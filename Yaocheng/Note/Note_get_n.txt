package TEST3

import java.util

import org.apache.spark.sql.SparkSession
import com.hankcs.hanlp

import scala.util.matching
import java.util._
import java.util.concurrent.ConcurrentMap
import java.util.function.{BiConsumer, BiFunction}

import scala.collection.immutable.Range.Int

object MAIN {
    def main(args: Array[String]): Unit ={
        //初始化SPARKSESSION
        val spark = SparkSession.builder().appName("MysqlDemo").master("local[2]").getOrCreate()
        //导入隐式转换方法
    //    import spark.implicits._

    //    val Mysql_url = "jdbc:mysql://localhost:3306/to_hdfs?useUnicode=true&characterEncoding=utf-8"

        val Hdfs_url = "hdfs://localhost:8020/a.txt/part-00000-8574c9d0-6646-44fe-b752-555ec6afbfff.csv"

        //取得MYSQL表DF
        val jdbcDF = spark.read.csv(Hdfs_url).toDF("Url","Text")

        val Text = jdbcDF.select("Text").take(10000)

        //compare with "n"
        val sing = "n".r

        //get all n list
        val DoneText=Text.map(i=>{
          hanlp.HanLP.segment(i.toString()).toArray().filter(x=>{!sing.findFirstIn(x.toString).isEmpty})
        }).flatten

        //get hashmap
        val DoneMap = Count.hashCount(DoneText)

        //重写BICONSUMER内部实现
        val D_p=new BiConsumer[String,Int] {
            override def accept(t: String, u: Int): Unit = {
                if(u>200) {
                    print(t + "\t" + u)
                    println()
                }
            }
        }

        DoneMap.forEach(D_p)

      //    .option("delimiter", ",") .option("heard", true)
      //    .option("url", "jdbc:mysql://localhost:3306/to_hdfs?useUnicode=true&characterEncoding=utf-8")
       //   .option("dbtable","baidu")
      //    .option("user", "root")
      //    .option("password", "123").load()
        //转存HDFS
     //   val res=jdbcDF.write.format("csv")
      //    .option("delimiter", ",")
      //    .option("heard", true)
      //    .save("hdfs://localhost:8020/a.txt")

        spark.stop()
    }
}

///////////////////////输入LIST[String]返回统计好的哈希表
package TEST3
import java.util.concurrent.ConcurrentHashMap

object Count {
  def hashCount(matalist:Array[AnyRef]): ConcurrentHashMap[String,Int] ={
    val datamap = new ConcurrentHashMap[String,Int](100)
    matalist.foreach(i=>{
      //when key not found insert (i,0) return null
      if(datamap.putIfAbsent(i.toString,1)!=null) {
        val oldvalue = datamap.get(i.toString)
        datamap.put(i.toString,oldvalue+1)
      }
    })
    datamap
  }
}
