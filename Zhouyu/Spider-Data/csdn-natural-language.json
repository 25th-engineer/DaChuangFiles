{"_id":{"$oid":"5d343ae262f717dc0659b251"},"title":"人工智能自然语言处理在机器学习上的运用","author":"_小陈同学_","content":"文章目录\n自然语言处理的概述\n人工智能自然语言处理的应用\n简单的自然语言处理模型\n感想\n自然语言处理的概述\n自然语言处理就是利用电子计算机为工具对人类特有的书面和口头形式的自然语言的信息进行各种类型处理和加工的技术。这种技术因为运用环境和发展潜力巨大，已经成为一门专门的边缘性交叉学科，涉及到语言学，计算机科学以及数学等。自然处理的目的在于建立各种自然语言处理系统，包括机器翻译系统、自然语言理解系统、自动检索系统、文字自动识别系统、数据库系统等。\n自然语言是人脑的高级功能之一，也是人类区别于其他动物的重要标志。人类可以借助于自然语言交流思想在人际交往及社会组成中发挥了巨大的作用，另外，人类还借助于自然语言进行思维活动，认识事物的本质和规律。所谓“脑海中的声音”，意味着人类无法脱离一个表达性质的工具来进行思考，每次人类进行思考的时候，其实是相当于自己和自己进行着对话。 可以说，离开了自然语言，人类社会和科学技术的发展不可能如此，井然有序的进行。\n自计算机诞生以来，人类与计算机交互只能通过编程语言编写的代码来实现，如使用basic pasical c lisp等计算机程序设计语言，对于计算机来说，他只能根据二进制的指令来作出不同的行为反应，而计算机程序设计者，则往往在这个过程中起到了翻译的功能，即将在自然语言表述下的功能需求，用程序设计语言表述，再由特定的编程转化为机器可以理解的二进制指令计算机能够做我们想要完成的事情，但是他并不真正理解我们的语言，从这个意义上来说，如果想要让计算机足够智能，让计算机能够直接理解我们的指令，显然是非常重要的，而在这其中，自然语言的处理及是人与计算机沟通的桥梁。\n人工智能自然语言处理的应用\n现在人工智能的飞速发展主要得益于统计学方法，即以从此诞生的机器学习的思想，而之前的的其学习的思想，机器学习的积极学习的发展，现在人们通过获取并输入海量的数据，让计算机能够在这些数据集中找到自己其中的规律，不同于以往使用逻辑严密的语言告诉计算机应当怎样做？现在我们所做的只是把基础的数据和正确的答案给计算机在中间的过程中，为什么计算机做出这样的判断其实并不重要？那么这样的思想对自然语言处理会有怎样的启发呢？\n接下来，我就通过一个实例来讲述一下人工智能自然语言在机器学习上的运用。——围棋AI的机器学习。\nAlphaGo在社会上引起巨大的反响。而且围棋和自然语言处理的复杂度都是极高的。AlphaGo项目的研究人员中并没有特别高水平的围棋选手，如果按照以往的人工智能的思路，研究人员必须要把如何更好地获胜采用逻辑较为严密的语言编写为程序，使得AlphaGo在对局的时候能够利用自己超高的运算能力在“获胜技巧”中不断穷举运算，从而做出最优解。这也是以往在象棋等棋牌类游戏中人工智能的思路。然而围棋之所以有挑战性，正是因为围棋的可能性有太多种，根本无法依靠穷举来计算下几步可能的结果，无法进行比较，也就是无从谈起所谓的“最优解”了。而之所以AlphaGo能够在之后的棋局中战胜世界围棋第一人，正是人工智能采取新的机器学习思路的胜利。\n简单的自然语言处理模型\n在具体涉及到自然语言处理时，学习数据从棋局变成了语料库。下面将以一个简单的自然语言问答系统为例，简要介绍其中的步骤。\n数据加载\n分为两部分：加载语料和预处理。加载语料可以认为是简单的数据存储在系统能够访问在数据库里，之后数据会从数据库中流向设计好的神经网络模型。预处理的目的是为了让数据更规范，一般而言，是把语料组成组合成输入需要的格式，从人类的角度，把数据变得更易于机器理解，从而增加机器学习的效率。\n训练过程\n机器学习模型一般使用神经网络输入是序列，从左侧进入输出序列为包含多个结果数值序列。在这个过程中，研究人员需要设置超参数模型的各项参数，如问题的最大程度神经元个数等在设置神经网络的激活函数，损失函数，损失大自然等思想，使得该神经网络能够在一定程度上进行忽略模拟，至此，一个简单的神经网络已经建成，再往后就是进行迭代训练，使数据不断的流向神经网络模型。\n观察效果\n因为超参数及模型的设置等，可能会有不合理之处，在训练过程中，可以观察损失函数和准确度的变化，根据这些变化，可以帮助优化模型，辅助超参数的设置。\n感想\n自然语言处理是计算机科学中的重要课题，但因为自然语言处理巨大的复杂性，使得以逻辑为基础的符号模型化，研究方法，男音取得较为基础的进展，然而，随着，人工智能技术的发展，特别是机器学习的应用，对于类似规则非逻辑严谨的问题有了较理想的解决方案，相信随着人工智能研究的不断发展，实现人类与机器的无障碍交流，也许就在不远的将来","data":"2019年06月20日 16:12:06"}
{"_id":{"$oid":"5d343ae362f717dc0659b253"},"title":"人工智能自然语言处理教程，如何进行字符串模糊匹配","author":"whale52hertz","content":"字符串模糊匹配是NLP自然语言处理中一项十分重要的研究项目。\n今天给大家介绍的就是字符串模糊匹配。\n文章链接：NLP教程：字符串模糊匹配","data":"2018年10月19日 14:44:26"}
{"_id":{"$oid":"5d343ae362f717dc0659b257"},"title":"UserCF和ItemCF用于Python人工智能自然语言处理","author":"小发猫","content":"最近做一个NLP项目，必须对很多自然语言处理算法进行了解，主要用Python，因为Python在人工智能上的支持相对比较好的，下面说说UserCF和ItemCF的区别和应用。\nUserCF算法的特点：\n较少的用户场合，否则用户相似度矩阵计算起来很昂贵\n适用于时效性强，用户友好性较低的地区\n对新用户不友好，对新项目友好，因为用户相似度矩阵不能实时计算\n很难提供用户强制的建议\n相应地，ItemCF算法的功能如下：\n\n适用于项目数量明显少于用户数量的场合，否则项目相似度矩阵计算起来很昂贵\n适用于长尾项目丰富且用户需求强烈的地区\n由于项目相似度矩阵不需要强实时性（cosα=（A * B）/（|| A || * || B ||）），所以对用户友好且对新项目不友好。\n使用用户历史记录行为作为推荐的解释让用户更加信服\n因此，可以看出，UserCF适用于项目快速增长且具有高实时性能的场合，例如新闻推荐。在图书，电子商务和电影领域，如京东，天猫和优酷，ItemCF可以充分利用其优势。在这些网站中，用户的兴趣更加固定和持久，并且这些网站上的更新不会特别快。\n为期一天的更新在公差范围内。\n\n\n\n模型评估指标：准确性，召回率，覆盖率，新颖性，惊喜性，实时性\n总结：人工智能伪原创工具 小发猫伪原创 采用这些技术开发。","data":"2018年04月18日 21:43:53"}
{"_id":{"$oid":"5d343ae362f717dc0659b259"},"title":"人工智能自然语言处理（六）-命名实体识别（Named Entity Recognition (NER)）","author":"waterfeeling","content":"目录\n摘要\nNER问题定义\n常用数据集\nCoNLL 2003\nOntoNotes 5.0 / CoNNLL 2012\nNLPBA2004\nEnron Emails\nBosonNLP\n人民日报1998\nSOTA算法在主流数据集上的表现\nCoNLL 2003\nOntoNotes 5.0 / CoNNLL 2012\n中文NER\nAPI接口资源\nGoogle NLP API\n百度NLP接口\nBOSON NER接口\n总结\n附录\n推荐论文资源\n推荐源代码资源\nTensorflow - Named Entity Recognition\nBLOG POSTS\n摘要\n命名实体识别的任务是识别句子中的实体，并且标注实体的类别，如人（PER），组织（ORG），位置（LOC）。本博文试图系统整理与命名实体识别技术相关的数据、算法及各类开源资源以便对命名实体识别技术有应用需求的个人和机构能够各取所需。\nNER问题定义\n命名实体识别的任务是识别句子中的实体，并且标注实体的类别，如人（PER），组织（ORG），位置（LOC）。此外，如果一个完整的实体是由多个单词构成，可将每个单词单独标注。\nDavid\nBeckham\nwas\nwith\nAC\nMilan\nB-PER\nI-PER\nO\nO\nI-ORG\nI-ORG\n以上：\nDavid标注为B-PER，B（Beginning）代表实体的起始部分；\nBeckham标注为I-PER，I代表实体的中间部分。\n关于单词的单独标注，有不同的惯例，如IOB（Inside，Outside，Beginning）体系和IOBES（Inside，Outside，Beginning，End，Singleton）体系。\n命名实体识别的标注实体类别和标注方式不存在统一的标准，可以根据具体的业务定制标注体系。例如在BosonNLP网站上提供的中文命名实体识别API中，标注的类型包括：product_name，time，person_name，org_name，location，company_name等，不同于常见的NER任务中只标注PER，ORG，LOC，用于标准的字符串格式也不一样（如person_name vs. PER）。另外其标注的方式是整个实体，即整个单次或短句为单位进行标注，并未针对每个字去标注他在实体中的位置。\n常用数据集\nCoNLL 2003\nCoNLL 2003应该算是很经典的NER数据集了。想2018年横空出世的Google BERT论文中也用CoNLL 2003的NER任务来作为BERT性能的炼金石（参见另一篇博文《Google BERT概览（一） -它解决了哪些问题？》）。\n以下是官网上提供的数据样本。数据一共包含四列，第一列为单次，第二列为a part-of-speech (POS) 标签，第三列为syntactic chunk标签，第四列为NER标注。\n\n关于CoNLL 2003数据集的其他信息，读者可移步该数据集的官网介绍，或者在下载了完整数据集后自行探索。\nOntoNotes 5.0 / CoNNLL 2012\n以下是官网上提供的中文数据样本。这里不得不吐槽一下，选取样本的时候就不能上点心，挑些文明得体的样本吗？可以看到OntoNotes 5.0数据中，除了标注NER，也包含了如POS标签等其他标签，可以用于其他类型的以单词或短语为单位的NLP文字识别任务。\n\n关于OntoNotes 5.0数据集的其他信息，读者可移步该数据集的官网介绍，或者在下载了完整数据集后自行探索。\nNLPBA2004\n关于NLPBA2004数据集的信息，读者可移步该数据集的官网介绍。\nEnron Emails\nEnron Emails数据集包含了Enron公司内部158名员工相互间发送的60多万Email. Enron Emails数据集的详细信息，读者可移步维基百科上对该数据集的介绍。\nBosonNLP\nBosonNLP中文命名实体识别数据集包含了2000条句子（下载地址）。以下展示了其原始数据中对命名实体的标注方式。\n{{product_name:浙江在线杭州}}{{time:4月25日}}讯（记者{{person_name: 施宇翔}} 通讯员 {{person_name:方英}}）毒贩很“时髦”，用{{product_name:微信}}交易毒品。没料想警方也很“潮”，将计就计，一举将其擒获。记者从{{org_name:杭州江干区公安分局}}了解到，经过一个多月的侦查工作，{{org_name:江干区禁毒专案组}}抓获吸贩毒人员5名，缴获“冰毒”400余克，毒资30000余元，扣押汽车一辆。{{location:黑龙江}}籍男子{{person_name:钱某}}长期落脚于宾馆、单身公寓，经常变换住址。他有一辆车，经常半夜驾车来往于{{location:杭州主城区}}的各大宾馆和单身公寓，并且常要活动到{{time:凌晨6、7点钟}}，{{time:白天}}则在家里呼呼大睡。\n人民日报1998\n以1998年人民日报语料为对象，由北京大学计算语言学研究所和富士通研究开发中心有限公司共同制作的标注语料库。该语料库对600多万字节的中文文章进行了分词及词性标注。以下展示了其原始数据的大致样式。\n19980101-01-001-007/m １９９７年/t ，/w 是/v 中国/ns 发展/vn 历史/n 上/f 非常/d 重要/a 的/u 很/d 不/d 平凡/a 的/u 一/m 年/q 。/w 中国/ns 人民/n 决心/d 继承/v 邓/nr 小平/nr 同志/n 的/u 遗志/n ，/w 继续/v 把/p 建设/v 有/v 中国/ns 特色/n 社会主义/n 事业/n 推向/v 前进/v 。/w [中国/ns 政府/n]nt 顺利/ad 恢复/v 对/p 香港/ns 行使/v 主权/n ，/w 并/c 按照/p “/w 一国两制/j ”/w 、/w “/w 港人治港/l ”/w 、/w 高度/d 自治/v 的/u 方针/n 保持/v 香港/ns 的/u 繁荣/an 稳定/an 。/w [中国/ns 共产党/n]nt 成功/a 地/u 召开/v 了/u 第十五/m 次/q 全国/n 代表大会/n ，/w 高举/v 邓小平理论/n 伟大/a 旗帜/n ，/w 总结/v 百年/m 历史/n ，/w 展望/v 新/a 的/u 世纪/n ，/w 制定/v 了/u 中国/ns 跨/v 世纪/n 发展/v 的/u 行动/vn 纲领/n 。/w\nSOTA算法在主流数据集上的表现\n注：2019年初，一个专门收集人工智能领域优秀论文及开源代码的网站https://paperswithcode.com/被公之于众，读者可以移步这里查看在NER领域的最新研究成果。本文提供部分主流数据集的leaderboards（截图时间：2019年2月）。\n从以下leaderboards来看，在NER领域表现比较突出的深度学习技术包括BiLSTM+CRF，Transformer（如BERT就是基于Transformer的）等。而BERT，Flair则提供了非常通用的NLP框架，这些框架不但在NER问题上取得了不错的成绩，由于其构建了非常抽象的NLP底层框架，他们在NLI，QA等问题上也有不错的表现。\nCoNLL 2003\nOntoNotes 5.0 / CoNNLL 2012\n中文NER\n中文NER方面统一的评估平台似乎并不多。在paperwithcode网站上罗列了数个leaderboard，但其中每个leaderboard的参与者也只有一两个。以下为相关的截图。\nAPI接口资源\nGoogle NLP API\n在Google Cloud NLP API官网上直接进行测试（测试时间2019年2月）。测试结果如下。\n\n再来一段中文的测试。结果如下。\n\n百度NLP接口\n在百度AI开放平台上直接进行测试（测试时间2019年2月）。测试结果如下。\n\nBOSON NER接口\n参考BOSON官方API文档说明中的Python调用示例对进行命名实体识别测试（测试时间2019年1月）：\nimport json import requests NER_URL = 'http://api.bosonnlp.com/ner/analysis' s = ['最近，一则名叫《啥是佩奇》的短视频在网上刷屏。' '该视频讲述的是一个生活在大山里的留守老人为给城里的孙子' '准备新年礼物问遍全村啥是佩奇的故事。' '老人广寻佩奇最终亲手打造了一个“硬核佩奇”。'] data = json.dumps(s) headers = { 'X-Token': KeyManager.BOSON_API_KEY, 'Content-Type': 'application/json' } resp = requests.post(NER_URL, headers=headers, data=data.encode('utf-8')) for item in resp.json(): for entity in item['entity']: print(''.join(item['word'][entity[0]:entity[1]]), entity[2])\n输出结果为：\n《啥是佩奇》 product_name 佩奇 person_name 佩奇 person_name\n总结\n命名实体识别是自然语言处理中的一个常见课题。主流的AI公司也通过API的形式提供各类命名实体识别服务。当前主流的技术包括BiLSTM+CRF、Transformer等。在BERT，FLAIR等NLP开源框架中也有针对NER的解决方案。国外的NER研究主要集中于利用CoNLL 2003，OntoNotes 5.0等数据集，研究成果相对集中。国内的NER研究常用的数据集包括人民日报1998数据集。但是作者在发文的时候还没有看到类似于CoNLL 2003那样比较集中和权威的性能测评和对比。\n附录\n推荐论文资源\nBidirectional LSTM-CRF Models for Sequence Tagging by Huang, Xu and Yu\nNeural Architectures for Named Entity Recognition by Lample et al.\nEnd-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF by Ma et Hovy\n其他优秀论文可以参看paperwithcode网站上关于NER相关资源的总结。\n推荐源代码资源\nTensorflow - Named Entity Recognition\n项目地址：https://github.com/guillaumegenthial/tf_ner\n其他优秀论文可以参看paperwithcode网站上关于NER相关资源的总结。\nBLOG POSTS\nhttps://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html\nhttps://guillaumegenthial.github.io/introduction-tensorflow-estimator.html","data":"2019年02月16日 18:43:58"}
{"_id":{"$oid":"5d343ae462f717dc0659b25b"},"title":"人工智能-自然语言处理-知识解构","author":"YSSY6886","content":"自然语言处理包含4个部分：\n1 语言识别，语音合成；\n2 自然语言理解，对话理解，知识获取和问答，任务理解；\n3 底层的机器学习（TennsorFlow和other）\n4 个性化信息的获取和利用；\n其中两个重要的竞品分析：\ngoogle的产品：各种语言场景；\nAlexa亚马逊产品：基于云计算的对话机器人产品，NLP定制化场景；\ngoogle NLP开源项目：\nBERT：Bidirectional Encoder Representations from Transformers 预训练语言表示的方法；\n可以在大型文本语料库（如维基百科）上训练通用的“语言理解”模型，然后将该模型用于下游NLP任务，比如机器翻译、问答。\n第一个无监督的用于预训练NLP的深度双向系统。\n无监督意味着BERT仅使用文本语料库进行训练，也就是说网络上有大量多种语言文本数据可供使用。\nNLP中的3个关键概念：\n1 文本嵌入（字符串的矢量表示）；\n2 机器翻译（使用神经网络翻译语言）；\n3 以及Dialogue和Conversations（可以实时与人进行对话的技术）；\n还涉及到的技术：\n技术1：情绪分析\n情绪分析是通过较小元素的语义组成来解释较大文本单元（实体、描述性术语、事实、论据、故事）的含义的过程；\n用于情感分析的现代深度学习方法可用于形态学、语法和逻辑语义，其中最有效的是递归神经网络。\n迄今为止用于情感分析的最强大的RNN模型是递归神经张量网络，其在每个节点处具有神经网络的树结构。\n技术2：问答系统\n问答（QA）系统的想法是直接从文档、对话、在线搜索和其他地方提取信息，以满足用户的信息需求；\nQA系统不是让用户阅读整个文档，而是更喜欢简短而简洁的答案；\nQA系统可以非常容易地与其他NLP系统结合使用，并且一些QA系统甚至超越了对文本文档的搜索，\n并且可以从图片集合中提取信息；\n强大的深度学习架构（称为动态内存网络（DMN））已针对QA问题进行了专门开发和优化；\n给定输入序列（知识）和问题的训练集，它可以形成情节记忆，并使用它们来产生相关答案；\n该体系结构具有以下组件：\n语义内存模块（类似于知识库）被用来创建从输入句子的嵌入字序列预先训练手套载体；\n输入模块处理与问题有关的输入矢量称为事实；\n问题模块逐字处理疑问词，并且使用输出相同权重的GRU输入模块的向量；\n情景记忆模块接收从输入中提取和编码的嵌入事实和问题载体；\n答案生成模块，通过适当的响应，情景记忆应该包含回答问题所需的所有信息；\nDMN不仅在质量保证方面做得非常好，而且在情感分析和词性标注方面也优于其他架构。\n技术3：文本摘要 人类很难手动汇总大型文本文档；\n文本摘要是NLP为源文档创建简短、准确和流畅的摘要问题；\n随着推送通知和文章摘要获得越来越多的注意力，为长文本生成智能且准确摘要的任务每天都在增长；\n技术4：注意力机制 神经网络中的注意力机制是基于人类的视觉注意机制；\n研究人员不得不处理各种障碍：算法的局限性、模型的可扩展性、对人类语言的模糊理解；\n好消息是，大量的开源存在；\nNLP中已经解决的主要障碍：\n没有单一的模型架构，跨任务具有一致的最新结果；\n机器学习中一种强大的方法是多任务学习，它共享相关任务之间的表示，以使模型能够更好地概括原始任务；\n另一个挑战是重复字表示的问题，其中模型中编码器和解码器的不同编码导致重复的参数/含义；\n另一个障碍是，与诸如卷积神经网络或前馈神经网络相比，任何Deep NLP技术的基本构建块Recurrent Neural Networks相当慢；\n准递归神经网络采用RNN和CNN的最佳部分来提高训练速度，使用卷积跨越时间的并行性和跨越信道的并行性的元素级门控递归；\n在NLP中，架构搜索使用机器学习自动化人工神经网络设计的过程 非常缓慢，使用Google Brain进行强化学习的神经架构搜索是迄今为止开发的最可行的解决方案；\n对话系统-场景：\nTask-bot任务型对话系统建立；\nIR-BOT:检索型对话系统；\nChitchat-bot：闲聊系统；\nNLU中设计概念：\n域确认；用户意图甄别；填充槽点；\n第一个是语法分析，可以通过语法规则去分析一句话，得到这句活是疑问句还是肯定句，继而分析出用户意图。\n第二种方法是生成模式，主要两个代表性的HMM，CRF, 这样就需要标注数据。\n第三种方法是分类思想，先对一句话提取特征，再根据有多少个槽值或意图训练多少个分类器，输入一句话分别给不同的分类器，最终得到包含槽值的概率有多大，最终得到这个槽值。\n还有一种采用深度学习方式，使用LSTM+CRF两种组合的方式进行实体识别，现在也是首选的方法 ，\n一般轻量型的对话系统还是通过语法分析或分类方式或序列标注来做。\n自然语言生成也有多种方法。这里举三个方法：基于模板，基于语法规则和基于生成模型方法；\nDM涉及的概念：\n对话状态追踪DST；\n对话策略；\n意图识别的准确度跟两方面有关：\n1 关键字在当前意图中出现的频率；\n2 关键字在整个文件中出现的频率；\n自然语言理解后，需要有状态追踪，策略优化等对话管理模块；\n一般用传统的三元组方式即：action, slot , value。action就是意图，slot是需要填充的槽值，value是对应的值；\n语音识别：孤立词语音识别，连续词语音识别，大词连续语音识别；\n语音合成：语言处理，声学处理，韵律处理，情感处理；\n语义理解：中文分词，序列标注，实体识别，意图识别；\n语言生成：预定义模板，问答语料库，知识图谱，深度学习；\n对话状态模型：对话表示模型，对话推理模型，对话学习模型；\n对话策略模型：通用对话策略，领域对话策略；\n语料库资源：预制模板，问答语料，知识图谱，生成模型，百科问答，搜索引擎；","data":"2019年03月23日 16:41:11"}
{"_id":{"$oid":"5d343ae562f717dc0659b25d"},"title":"人工智能工程师学习路线/自然语言处理算法工程师","author":"最小森林","content":"人工智能工程师学习路线/自然语言处理算法工程师学习路径\n人工智能工程师学习路线自然语言处理算法工程师学习路径\n1入门级别\n1 数据结构\n2 算法重点\n3python\n2进阶阶段\n1 机器学习算法\n2深度学习算法\n3深度学习框架\n4 大数据计算框架\n3高阶\n1 强化学习\n2 迁移学习\n3自然语言处理\n1入门级别\n1.1 数据结构\n1.2 算法（重点）\n面试必考。参考学习地址：\n麻省理工学院公开课：算法导论 http://open.163.com/special/opencourse/algorithms.html\n1.3python\n包括python基础、面向对象要懂。\n2进阶阶段\n2.1 机器学习算法\n特征工程、特征分析\n监督学习算法\n非监督学习算法\n参考学习地址：\n1. Coursera 斯坦福吴恩达课程❤❤❤\n2. 能使用sklearn解决一些小的机器学习任务。\n参考书本：《西瓜书》\n2.2深度学习算法\n视频：\n1. Andrew Ng (吴恩达) 深度学习专项课程 by Coursera and deeplearning.ai❤❤❤\n2. 或者Hinton 大神的coursera 面向机器学习的神经网络\n3. Udacity 深度学习（中/英）by Google。你将通过项目和任务接触完整的机器学习系统 TensorFlow。\n书：《AI圣经 深度学习》\n2.3深度学习框架\nkeras\ntensorflow\n掌握好编程的利器，参考视频资料：\n1. 斯坦福大学深度学习课程: CS 20SI: Tensorflow for Deep Learning Research。准确的说，这门课程主要是针对深度学习工具Tensorflow的❤❤❤\n2.4 大数据计算框架\nhadoop\nspark\n因为深度学习工程师一般面对的是大数据，所以公司的分布式计算平台要熟悉会用。\n3高阶\n3.1 强化学习\n理论与实践\n3.2 迁移学习\n理论与实践\n3.3自然语言处理\n斯坦福课程深度学习应用课程。这门课程融合了两位授课者之前在斯坦福大学的授课课程，分别是自然语言处理课程 cs224n (Natural Language Processing)和面向自然语言处理的深度学习课程 cs224d( Deep Learning for Natural Language Processing).\n牛津大学Deep Learning for Natural Language Processing: 2016-2017\u003c深度NLP\u003ehttp://study.163.com/course/introduction/1004336028.htm ❤❤❤","data":"2018年06月10日 19:43:01","date":"2018年01月08日 15:12:30"}
{"_id":{"$oid":"5d343ae562f717dc0659b25f"},"title":"人工智能 | 自然语言处理研究报告（人才篇）","author":"冲动的MJ","content":"博主github：https://github.com/MichaelBeechan\n博主CSDN：https://blog.csdn.net/u011344545\n============================================\n概念篇：https://blog.csdn.net/u011344545/article/details/89525801\n技术篇：https://blog.csdn.net/u011344545/article/details/89526149\n人才篇：https://blog.csdn.net/u011344545/article/details/89556941\n应用篇：https://blog.csdn.net/u011344545/article/details/89574915\n下载链接：https://download.csdn.net/download/u011344545/11147085\n============================================\n清华AMiner团队 AMiner.org\n自然语言处理论文发表情况（来自维普智立方）\n\n1、国外实验室及人才介绍\nAMiner 基于发表于国际期刊会议的学术论文，对自然语言处理领域全球 h-index 排序top1000 的学者进行计算分析，绘制了该领域顶尖学者全球分布地图。\n\n根据上图，我们可以得出以下结论——从国家来看，美国是自然语言处理研究学者聚集最多的国家，英国、德国、加拿大和意大利紧随其后；从地区来看，美国东部是自然语言处理人才的集中地，而西欧、美国西部等其他先进地区也吸引了大量自然语言处理的研究者。\n全球自然语言处理顶尖学者的 h-index 平均数为 59，h-index 指数大于 60 的学者最多占41%，h-index 指数在 40 到 60 之间的学者次之，占比 40%。\n自然语言处理领域顶尖学者男性占比 91%，女性占比 9%，男女比例不均衡。\n\nAMiner 对顶尖人才的迁徙路径做了分析。由上图可以看出，各国自然语言处理顶尖人才的流失和引进是相对比较均衡的，其中美国是自然语言处理领域人才流动大国，人才输入和输出幅度都大幅度领先，且从数据来看人才流入略大于流出。英国、德国、加拿大和中国等国落后于美国，其中英国和加拿大有轻微的顶尖人才流失现象。\n以下选取在 ACL、EMNLP、NAACL、COLING 等 4 个会议在近 5 年累计发表 10 次以上论文的国外学者及其所在实验室做简要介绍。\nChris Dyer\n\nChris Dyer，卡内基梅隆大学助理教授，2010 年在马里兰大学获语言学博士学位。主要兴趣领域是机器学习、自然语言处理和语言学的交叉研究。比较感兴趣的一些课题有：机器翻译、用于语言处理的神经网络模型、语言建模、特征归纳和表示学习、大数据算法、音乐概率模型等。\n\n卡内基梅隆大学语言技术研究所主要研究内容包括自然语言处理、计算语言学、信息提取、信息检索、文本挖掘分析、知识表示、机器学习、机器翻译、多通道计算和交互、语音处理、语音界面和对话处理等。\nChristopher D. Manning\n\nChristopher D. Manning，斯坦福大学计算机科学与语言学习的教授，1994 年在斯坦福大学获得博士学位。他致力于研究能够智能处理、理解和生成人类语言材料的计算机。Manning 在自然语言处理的深度学习领域有着深入研究，包括递归神经网络、情感分析、神经网络依赖分析等。\nManning 曾获 ACL、CILING、EMNLP 的最佳论文奖。\n\n斯坦福大学自然语言处理小组包括了语言学和计算机科学系的成员，是斯坦福人工智能实验室的一部分。主要研究计算机处理和理解人类语言的算法，工作范围从计算语言学的基本研究到语言处理的关键应用技术均有涉猎，涵盖句子理解、自动问答、机器翻译、语法解析和标签、情绪分析和模型的文本和视觉场景等。该小组的一个显著特征是将复杂和深入的语言建模和数据分析与 NLP 的创新概率、机器学习和深度学习方法有效地结合在一起。\nDan Klein\n\n\nDan Klein，伯克利大学自然语言处理小组负责人。2004 年在斯坦福大学取得计算机科学的博士学位。主要研究重点是自然语言信息的自组织，兴趣领域包括无监督的语言学习、机器翻译、NLP 的高效算法、信息提取、语言丰富的语言模型、NLP 的符号和统计方法的集成以及历史语言学等。多次在国际顶级会议上发表论文并获奖，如在 2012 年 EMNLP 上获得 Distinguished Paper “Training Factored PCFGs with Expectation Propagation”\n2017 年 Dan Klein 在 ACL、EMNLP、NAACL、COLING 等会议发表的论文有：\n\n伯克利大学自然语言处理小组分属于加州大学伯克利分校计算机科学部。主要从事以下几方面的研究工作，语言分析、机器翻译、计算机语言学、基于语义的方法、无监督学习等，多次在顶级国际会议（ACL、EMNLP、AAAI、IJCAI、COLING 等）上发表多篇论文，下表是 2018 年最新被选用的论文：\n\nNatural Language Processing Group at University of Notre Dame\n圣母大学自然语言处理小组主要关注机器翻译领域，并有多个项目的研究，如由 DARPALORELEI 和 Google 赞助的无监督多语言学习模型和算法研究；由亚马逊学术研究奖和谷歌教师研究奖赞助的研究，主要研究课题方向包括基于神经网络的机器翻译模型，以及使用神经网络进行翻译和语言建模的算法等。多次在国际顶级期刊和会议上发表论文。目前该小组主要负责人是 David Chiang\n\nDavid Chiang，美国圣母大学教授，在宾夕法尼亚大学计算机与信息科学获得博士学位。主要研究领域是自然语言处理，同时在语言翻译、句法分析等方面也有研究。David Chiang在 2005 年提出的基于短语的翻译模型，对机器翻译来说是一个巨大的进步，他把机器翻译从平面结构建模引向了层次结构建模。\nThe Harvard Natural Language Processing Group\n哈佛自然语言处理小组主要通过机器学习的方法处理人类语言，主要兴趣集中在数列生成的数学模型，以人类语言为基础的人工智能挑战以及用统计工具对语言结构进行探索等方面。该小组的研究出版物和开源项目集中在文本总结、神经机器翻译、反复神经网络的可视化、收缩神经网络的算法、文档中实体跟踪的模型、多模态文本生成、语法错误修正和文本生成的新方法等方面。Stuart Shieber 是该小组的主要负责人。\nStuart Shieber\nJames O. Welch, Jr. and Virginia B. Welch Professor of Computer Science\nFaculty Director, Harvard Office for Scholarly Communication\nHarvard University\n\nStuart Shieber，美国计算机协会（Association for Computing Machinery）Fellow 和美国人工智能协会（American Association for Artificial Intelligence）Fellow。他综合语言学、理论计算机科学、计算机系统以及人工智能等领域的知识，研究计算机语言学，从计算机科学的角度研究自然语言，在该领域的研究以科学和工程目标，以基础形式和数学工具为基础。具体研究领域包括计算语言学、数学语言学、基于语法的形式、自然语言生成、计算语义、机器翻译以及人机交互等。\nNatural Language Processing group of Columbia University\n哥伦比亚大学自然语言处理研究室是在计算机科学系、计算学习系统中心和生物医学信息系的支持下进行的，将语言洞察力与严谨前沿的机器学习方法和其他计算方法结合起来进行研究。在语言资源创造如语料库、词典等，阿拉伯语 NLP，语言和社交网络，机器翻译，信息提取，数据挖掘，词汇语义、词义消除歧义等方面有着比较深入的研究。现在该实验室计算机方面的主要负责人为 Michael Collins。\nMichael Collins\n\nMichael Collins，哥伦比亚大学计算机科学系教授，谷歌 NYC 研究科学家。1998 年在宾夕法尼亚大学获得计算机科学博士学位。主要研究兴趣是自然语言处理和机器翻译。多次在国际顶级会议上发表文章，例如在 EMNLP 2010，CoNLL 2008，UAL 2055 等会议上都获得最佳论文奖，同时还是 ACL 的研究员，获 NSF 生涯奖。\n2、国内实验室及人才介绍\nAMiner 基于论文数据整理了自然语言处理华人专家库，其中包括了来自 NUS、HKUS、THU、PKU、FDU 等知名高校以及百度、科大讯飞、微软等公司的 367 位专家学者。下面基于自然语言处理华人库中的数据对其进行分析。\n\n自然语言处理领域中华人专家在中国最多，美国次之。从地区来看，中国大陆是自然语言处理华人人才的最主要聚集地，尤其是北京、哈尔滨及东南沿海地区等具有自然语言处理学术基础的地区。美国东部和西部等其他地区排在其后。由图 11 可以看出，华人专家在中国流出量大于流入量，美国则正好相反，这也说明就自然领域而言，中国对人才的吸引力要小于美国。\nAMiner 自然语言处理华人库中专家 h-index 指数的平均数为 14，这一数值是远远低于自然语言处理全球 top1000 学者 h-index 指数平均数的。而且，在华人库中，h-index 指数\u003c10的专家人数最多，占比 60%；10-19 次之，占比 17%；\u003e60 的专家占比仅占 9%。这也说明，自然语言处理的华人专家整体水平低于自然语言处理领域全球 top1000 的学者，尤其是在 h-index 指数\u003e60 的学者方面有所欠缺。\nAMiner 自然语言处理华人库 367 位专家中，男性专家占 98%，女性专家仅占 2%，二者比例约为 49:1。\n以下选取在 ACL、EMNLP、NAACL、COLING 等 4 个会议在近 5 年累计发表 10 次以上论文的国内学者包括刘群、刘挺、周明、常宝宝、黄萱菁、刘洋、孙茂松、李素建、万小军、邱锡鹏、穗志方等。以下按照发表论文的多少为序，对这些学者及其所在实验室做简要介绍。\n刘群\n\n（来自百度百科）\n中科院计算所自然语言处理研究组\n刘群，中国科学院自然语言处理研究组组长，都柏林大学自然语言处理组组长、项目负责人。主要研究方向是中文自然语言处理，具体包括汉语词法分析、汉语句法分析、语义处理、统计语言模型、辞典和语料库、机器翻译、信息提取、中文信息处理和智能交互中的大规模资源建设、中文信息处理以及智能交互中的评测技术等。曾负责 863 重点项目“机器翻译新方法的研究”和“面向跨语言搜索的机器翻译关键技术研究”等。\n自然语言处理研究组隶属于中国科学院计算技术研究所智能信息处理重点实验室。研究组教师有刘群、冯洋等人。研究组主要从事自然语言处理和机器翻译相关的研究工作，研究方向包括机器翻译、人机对话、多语言词法分析、句法分析和网络信息挖掘等。研究组已完成和正在承担的国家自然科学基金、863 计划、科技支撑计划、国际合作等课题 40 余项，在自然语言处理和机器翻译领域取得了多项创新性研究成果。研究组自 2004 年重点开展统计机器翻译方面的研究并取得重大突破，并于 2015 年起转向神经机器翻译并取得很大进展。2018 年 7 月，正式加入华为诺亚方舟实验室，任语音语义首席科学家，主导语音和自然语言处理领域的前沿研究和技术创新。\n在自然语言处理的顶级国际刊物 CL、AI 和顶级国际学术会议 ACL、IJCAI、AAAI、EMNLP、COLING 上发表高水平论文 70 余篇，取得发明专利 10 余项。研究组已经成功将自主开发的统计机器翻译和神经机器翻译技术推广到汉语、维吾尔语、藏语、蒙古语、英语、韩语、泰语、日语、阿拉伯语等多种语言。部分语种的翻译系统已经在相关领域得到了实际应用，获得用户的好评。\n刘挺\n（来自雷锋网）\n刘挺，哈尔滨工业大学教授，国家“万人计划”科技创新领军人才。多次担任国家 863重点项目总体组专家、基金委会评专家。中国计算机学会理事，中国中文信息学会常务理事/社会媒体处理专委会（SMP）主任，曾任国际顶级会议 ACL、EMNLP 领域主席。主要研究方向为人工智能、自然语言处理和社会计算，是国家 973 课题、国家自然科学基金重点项目负责人。2012-2017 年在自然语言处理领域顶级会议发表的论文数量列世界第8 位（据剑桥大学统计），主持研制“语言技术平台 LTP”、“大词林”等科研成果被业界广泛使用。曾获国家科技进步二等奖、省科技进步一等奖、钱伟长中文信息处理科学技术一等奖等。\n刘挺领导的 哈工大社会计算与信息检索研究中心：http://ir.hit.edu.cn/\nhttps://github.com/HIT-SCIR/ltp\n哈工大社会计算与信息检索研究中心（HIT-SCIR）成立于 2000 年 9 月，隶属于计算机科学与技术学院。研究中心成员有主任刘挺教授，副主任秦兵教授，教师包括张宇、车万翔、陈毅恒、张伟男等。研究方向包括语言分析、信息抽取、情感分析、问答系统、社会媒体处理和用户画像 6 个方面。已完成或正在承担的国家 973 课题、国家自然科学基金重点项目、国家 863 重点项目、国际合作、企业合作等课题 60 余项。在这些项目的支持下打造出“语言技术平台 LTP”，提供给百度、腾讯、华为、金山等企业使用，获 2010 年钱伟长中文信息处理科学技术一等奖。\n研究中心近年来发表论文 100 余篇，其中在 ACL、SIGIR、IJCAI、EMNLP 等顶级国际学术会议上发表 20 余篇论文，参加国内外技术评测，并在国际 CoNLL’2009 七国语言句法语义分析评测总成绩第一名。研究中心通过与企业合作，已将多项技术嵌入企业产品中，为社会服务。双语例句检索等一批技术嵌入金山词霸产品中，并因此获得 2012 年黑龙江省技术发明二等奖。\n周明\n\n周明，微软亚洲研究院自然语言计算组的首席研究员，机器翻译和自然语言处理领域的专家。他的研究兴趣包括搜索引擎、统计和神经机器翻译、问答、聊天机器人、计算机诗歌和文本挖掘等。\n1989 年，他设计了“CEMT-I 机器翻译系统”，这是汉英机器翻译的第一个实验，获得了中国大陆政府的科学技术进步奖。1998 年，他设计了著名的中日文机器翻译软件产品 J-Beijing，并获得了日本机械翻译协会 2008 年颁发的机器翻译产品的最高荣誉称号。\n周明团队也为 Bing 搜索引擎提供了重要的技术支持，包括单词 breaker、情感分析、speller、解析器和 QnA 等 NLP 技术。他的团队创建了汉英、粤语的机器翻译引擎，为译者和 Skype 翻译。最近，周明团队与微软产品团队紧密合作，在中国（小冰）、日本（Rinna）和美国（Tay）创建了知名的 chat-bot 产品，拥有 4000 万用户。他在顶级会议（包括 45+ACL论文）和 NLP 期刊上发表并发表了 100 多篇论文，获得了 38 项国际专利。\n周明所属实验室为**微软亚洲研究院自然语言计算组**\n黄萱菁\n黄萱菁，复旦大学计算机科学技术学院教授、博士生导师。在 SIGIR、ACL、ICML、IJCAI、AAAI、NIPS、CIKM、ISWC、EMNLP、WSDM 和 COLING 等多个国际学术会议上发表论文数十篇。\n曾任 2014 年 CIKM 会议竞赛主席，2015 年 WSDM 会议组织者，2015 年全国社会媒体处理大会程序委员会主席，2016 年全国计算语言学会议程序委员会副主席，2017 年自然语言处理与中文计算国际会议程序委员会主席。\n多次在人工智能、自然语言处理和信息检索的国际学术会议IJCAI、ACL、SIGIR、WWW、EMNLP、COLING、CIKM、WSDM 担任程序委员会委员和资深委员。兼任中国中文信息学会常务理事，社会媒体专委会副主任，中国计算机学会中文信息处理专委会委员，中国人工智能学会自然语言理解专委会委员，ACM 和 ACL 会员，《中文信息学报》编委，国家自然科学基金、教育部高校博士点基金和 863 计划同行评议专家。\n黄萱菁领导的**复旦大学自然语言处理研究组**\n复旦大学自然语言与信息检索实验室，致力于社会媒体海量多媒体信息处理的前沿技术研究。主要研究方向包括：自然语言处理、非规范化文本分析、语义计算、信息抽取、倾向性分析、文本挖掘等方面。实验室开发了 NLP 工具包 FudanNLP，FudanNLP 提供了一系列新技术，包括中文分词、词性标注、依赖解析、时间表达式识别和规范化等。实验室先后承担和参与了国家科技重大专项、国家 973 计划、863 计划、国家自然科学基金课题、上海市科技攻关计划等，并与国内外多所重点大学、公司保持着良好的合作关系。研究成果持续发表在国际权威期刊和一流国际会议（TPAMI、TKDE、ICML、ACL、AAAI、IJCAI、SIGIR、CIKM、EMNLP、COLING 等）。\n孙茂松\n\n孙茂松，清华大学计算机科学与技术系教授。2007-2010 年任该系系主任，主要研究领域为自然语言处理、互联网智能、机器学习、社会计算和计算教育学。国家重点基础研究发展计划（973 计划）项目首席科学家，国家社会科学基金重大项目首席专家。在国际刊物、国际会议、国内核心刊物上发表论文 160 余篇，主持完成文本信息处理领域 ISO 国际标准 2 项。2007 年获全国语言文字先进工作者，2016 年获全国优秀科技工作者以及首都市民学习之星。多次担任相关领域国际会议和全国性学术会议大会主席或程序委员会主席。\n孙茂松领导的**清华大学自然语言处理与社会人文计算实验室**\n清华大学计算机系自然语言处理课题组在 20 世纪 70 年代末，就在黄昌宁教授的带领下从事这方面的研究工作，是国内开展相关研究最早、深具影响力的科研单位，同时也是中国中文信息学会计算语言学专业委员会的挂靠单位。现任学科带头人孙茂松教授任该专业委员会的主任（同时任中国中文信息学会副理事长），其余教师还有刘洋、刘知远等人。目前该课题组对以中文为核心的自然语言处理中的若干前沿课题，进行系统、深入的研究，研究领域的涵盖面正逐步从计算语言学的核心问题扩展到社会计算和人文计算。该课题组多篇论文被 ACL 2018、IJCAI-ECAI 2018、WWW 2018 录用，内容涉及问答系统、信息检索、机器翻译、诗歌生成、查询推荐等多个领域。\n万小军\n\n万小军，北京大学计算机科学技术研究所教授，博士生导师，语言计算与互联网挖掘实验室负责人。研究方向为自然语言处理与文本挖掘，兴趣领域包括自动文摘与文本生成、情感分析与观点挖掘、语义计算与信息推荐等，在国际重要学术会议与期刊上发表高水平学术论文上百篇。担任计算语言学顶级国际期刊 Computational Linguistics 编委，TACL 常务评审委员（Standing Reviewing Committee），多次担任自然语言处理领域重要国际会议领域主席或SPC（包括 ACL、NAACL、IJCAI、IJCNLP 等），以及相关领域多个国际顶级学术会议（ACL、SIGIR、CIKM、EMNLP、NAACL、WWW、AAAI 等）程序委员会委员。研制了自动文摘开源平台 PKUSUMSUM，与今日头条合作推出 AI 写稿机器人小明（Xiaomingbot），与南方都市报合作推出写稿机器人小南等应用系统。\n万小军所属实验室为**北京大学语言计算与互联网挖掘研究组**\n语言计算与互联网挖掘研究室从属于北京大学计算机科学技术研究所，成立于 2008 年7 月，负责人为万小军老师。研究室以自然语言处理技术、数据挖掘技术与机器学习技术为基础，对互联网上多源异质的文本大数据进行智能分析与深度挖掘，为互联网搜索、舆情与情报分析、写稿与对话机器人等系统提供关键技术支撑，并从事计算机科学与人文社会科学的交叉科学研究。\n研究室当前研究内容包括：1）语义理解：研制全新的语义分析系统实现对人类语言（尤其是汉语）的深层语义理解；2）机器写作：综合利用自动文摘与自然语言生成等技术让机器写出高质量的各类稿件；3）情感计算：针对多语言互联网文本实现高精度情感、立场与幽默分析；4）其他：包括特定情境下的人机对话技术等。\n穗志方\n\n\n穗志方，北京大学信息科学技术学院计算语言学实验室主任，教授、博士生导师。2011年度国家科技进步二等奖、“综合型语言知识库”项目第二完成人。长期从事自然语言处理方面的研究。在计算语言学国际顶级会议 ACL 2000、COLING 2008、CONLL 2008、ACL 2009、EMNLP2009、AIRS 2008 上发表多篇学术论文。作为课题负责人主持的科研项目有：国家自然科学基金项目“汉语动词子语类框架自动获取技术研究”、“基于结构化学习的语义角色标注研究”、“基于 Web 的概念实例及其属性值提取方法研究”，国家社科基金项目“面向文本内容提取的生成性组件库研究及建设”等。\n穗志方所属实验室为**北京大学计算语言学教育部重点实验室**\n计算语言学教育部重点实验室依托北京大学建设。实验室研究人员由北京大学信息科学技术学院计算语言学研究所、中文系、软件与微电子学院语言信息工程系、计算机技术研究所、心理系和外语学院的相关研究人员构成。主要研究方向包括：中文计算的基础理论与模型；大规模多层次语言知识库构建的方法；国家语言资源整理与语音数据库建设；海量文本内容分析与动态监控；多语言信息处理和机器翻译。\n宗成庆\n\n宗成庆，模式识别国家重点实验室研究员、博士生导师。主要从事自然语言处理、机器翻译和文本数据挖掘等相关领域的研究。主持国家自然科学基金项目、863 计划项目和重点研发计划重点专项等 10 余项，发表论文 150 余篇，出版专著和译著各一部。2013 年当选国际计算语言学委员会（ICCL）委员。目前担任亚洲自然语言处理学会（AFNLP）候任主席、中国中文信息学会副理事长、学术期刊 ACM TALLIP 副主编（AssociateEditor）、《自动化学报》副主编、IEEE Intelligent Systems 编委、MachineTranslation 编委和JCST 编委。2013 年获国务院颁发的政府特殊津贴，2014 年获“钱伟长中文信息处理科学技\n术奖”一等奖，2015 年获国家科技进步奖二等奖，2017 年获北京市优秀教师荣誉称号。\n赵军\n\n赵军，中科院研究员，博士生导师。1998 年在清华大学计算机科学与技术系获得博士学位。1998 年-2002 年在香港科技大学计算机科学系做博士后、访问学者。2002 年 5 月至今在中科院自动化所模式识别国家重点实验室工作。主持国家自然科学基金重点项目、973 计划等国家级项目。研究方向为信息提取和问答系统等。在 IEEE TKDE、JMLR 等顶级国际期刊和 ACL、SIGIR、EMNLP、COLING 等顶级国际会议上发表论文六十余篇，获 COLING-2014 最佳论文奖，获 KDD-CUP2011 亚军（2/1297）。研发了汉语文本分析、信息抽取和知识工程、百科问答等软件工具和平台，在中国大百科全书出版社、华为公司、讯飞公司等得到应用。\n宗成庆和赵军所属实验室为中科院模式识别国家重点实验室\n中科院模式识别国家重点实验室自然语言处理组主要成员有宗成庆、赵军、周玉、刘康、张家俊、汪昆、陆征等。该小组主要从事自然语言处理基础、机器翻译、信息抽取和问答系统等相关研究工作，力图在自然语言处理的理论模型和应用系统开发方面做出创新成果。目前研究组的主要方向包括：自然语言处理基础技术（汉语词语切分、句法分析、语义分析和篇章分析等）、多语言机器翻译、信息抽取（实体识别、实体关系抽取、观点挖掘等）和智能问答系统（基于知识库的问答系统、知识推理、社区问答等）。\n近年来，研究组注重于自然语言处理基础理论和应用基础的相关研究，承担了一系列包括国家自然科学基金项目、973 计划课题、863 计划项目和支撑计划项目等在内的基础研究和应用基础研究类项目，以及一批企业应用合作项目。在自然语言处理及相关领域顶级国际期刊（CL、TASLP、TKDE、JMLR、TACL、Information Sciences、Intelligent Systems 等）和学术会议（AAAI、IJCAI、ACL、SIGIR、WWW 等）上发表了一系列论文。2009 年获得第 23 届亚太语言、信息与计算国际会议（PACLIC）最佳论文奖，2012 年获得第一届自然语言处理与中文计算会议（NLPCC）最佳论文奖，2014 年获得第 25 届国际计算语言学大会（COLING）最佳论文奖。获得了 10 余项国家发明专利。\n国内学者在国际会议获得 Best paper 的有以下两个：\nPengcheng Yang 、 Xu Sun 、 Wei Li 、 Shuming Ma 、 Wei Wu 、 Houfeng Wang 的 SGM:\nSequence Generation Model for Multi-label Classification 在 2018 COLING 会议中被评为 Best\nerror analysis 和 Best evaluation。\n王厚峰\n\n王厚峰，北京大学信息科学技术学院教授，北京大学计算语言学研究所所长。主要研究兴趣包括情感分析、问答与会话、自然语言语言语篇分析等，曾作为首席专家主持过国家 863项目、国家社科基金重大项目，负责国家自然科学基金重大研究计划等。在 ACL、EMNLP、COLING、AAAI、IJCAI、ICML 等会议以及 Computational Linguistics 等期刊发表论文 70 余篇。\nFan Bu、Xiaoyan Zhu、Ming Li 等的 Measuring the Non-compositionality of MultiwordExpressions 在 2010 年 COLING 会议上被评为 best paper。\n\n朱小燕\n\n朱小燕，清华计算机系教授，博士生导师，智能技术与系统国家重点实验室主要负责人。\n主要研究领域为智能信息处理，其中包括：模式识别、神经元网络、机器学习、自然语言处理、信息提取和智能问答系统等。近年研究工作主要集中于生物领域文本信息处理和新一代智能信息获取的研究。作为项目负责人先后承担国家 863，973 项目，自然科学基金项目、国际合作项目多项。\n1997 年获国家教委科技进步二等奖，2003 年获北京市科技进步二等奖。获得国家发明专利3项。在各种国际刊物和会议上发表论文近100篇。其中包括国际刊物Genome Biology，Bioinformatics、BMC Bioinformatics、Medical informatics、IEEE Transactions.on SMC、IEEElectronics Letters、Neural Parallel \u0026 Science Computations、Document Analysis and Recognition，以及国际会议 SIG KDD、ACL、COLING、CIKM 等。\n朱小燕所属实验室为 清华大学智能技术与系统国家重点实验室\n智能技术与系统国家重点实验室依托在清华大学，1987 年 7 月开始筹建。1990 年 2 月通过国家验收，并正式对外开放运行。从 1990 年至 2003 年这十三年间，实验室顺利通过国家自然科学基金委受科技部委托组织的全部三次专家组评估，并被评估为 A（优秀实验室）。1994 年 10 月在庆祝国家重点实验室建设十周年表彰大会上，智能技术与系统国家重点实验室获集体“金牛奖”。1997 年被科技部列为试点实验室。2004 年庆祝国家重点实验室建设二十周年表彰大会上，本实验室再次荣获集体“金牛奖”。从 2004 年开始，实验室参与筹建清华信息科学与技术国家实验室。实验室学术委员会由 17 名国内外著名专家组成。实验室学术委员会名誉主任为中科院院士张钹教授，主任为应明生教授、副主任为邓志东教授。\n除此之外，活跃在自然语言领域的中国学者还有：\n周国栋\n周国栋，苏州大学计算机科学与技术学院教授，苏州大学 NLP 实验室负责人。主要研究兴趣是自然语言处理、中文计算、信息抽取和自然语言认知等。自 1999 年起，一直是 ACM、ACL、IEEE computer society 的会员，负责了多项国家 863 项目、国家重点研究项目等。近 5 年来发表国际著名 SCI 期刊论文 20 多篇和国际顶级会议论文 80 多篇，主持 NSFC项目 4 个(包括重点项目 1 个)。曾担任国际自然语言理解领域顶级 SCI 期刊 ComputationalLinguistics 编委，目前担任 ACM TALLIP 副主编、《软件学报》责任编委、CCF 中文信息技术专委会副主任委员、苏州大学校学术委员会委员。\n李涓子\n李涓子，清华大学教授，博士生导师。中国中文信息学会语言与知识计算专委会主任、中国计算机学会术语委员会执行委员。研究兴趣是语义 Web，新闻挖掘与跨语言知识图谱构建。多篇论文发表在重要国际会议（WWW、IJCAI、SIGIR、SIGKDD）和学术期刊（TKDE、TKDD）。主持多项国家级、部委级和国际合作项目研究,包括国家自然科学项目重点，欧盟第七合作框架、新华社等项目。获得 2013 年人工智能学会科技进步一等奖，2013 年电子学会自然科学二等奖。\n张民\n张民，苏州大学计算机科学与技术学院副院长。2003 年 12 月，他加入新加坡信息通信研究所并于 2007 年在研究所建立了统计机器翻译团队。2012 年加入苏州大学，并于 2013年在该大学成立智能计算研究所。\n目前的研究兴趣包括机器翻译、自然语言处理、信息提取、社交网络计算、互联网智能、智能计算和机器学习。近年来在国际顶级学报和顶级会议发表学术论文 150 余篇，Springer出版英文专著两部，主编 Springer 和 IEEE CPS 出版英文书籍十本。他一直积极地为研究界做贡献，组织多次会议并在许多会议和讲座中进行演讲。\n黄河燕\n黄河燕，语言智能处理与机器翻译领域专家，北京理工大学计算机学院院长、教授，北京市海量语言信息处理与云计算应用工程技术研究中心主任。长期从事语言智能处理的理论及应用研究，主持多项国家自然科学基金重点项目、国家重点研发计划项目、973 计划课题等重要科研项目，曾获国家科技进步一等奖、二等奖等奖项，被授予“全国优秀科技工作者”称号。\n孙乐\n孙乐，中国科学院软件研究所，研究员，博士生导师。中国中文信息学会副理事长兼秘书长。《中文信息学报》副主编。2003 至 2005 年，先后在英国 Birmingham 大学、加拿大Montreal 大学做访问学者，从事语料库和信息检索研究。目前主要研究兴趣：基于知识的语言理解、信息抽取、问答系统、信息检索等。在国内外主要刊物和会议上共发表论文 80 多篇。曾任 2008 和 2009 国际测评 NTCIR MOAT 中文\n简体任务的组织者、国际计算语言学大会（COLING 2010）组织委员会联席主席、机器翻译峰会（MT Summit 2011）组织委员会联席主席、中文语言评测国际会议（CLP2010、2012、2014）大会主席、国际计算语言学年会（ACL 2015）组织委员会联席主席等。\n3、ACL2018奖项介绍\n2018 年 7 月 15 在墨尔本开幕的 ACL 公布了其最佳论文名单，包括 3 篇最佳长论文和2 篇最佳短论文以及 1 篇最佳 demo 论文，值得一提的是 Amazon Door Prize 中北京大学和哈尔滨大学上榜，ACL2018 终身成就奖为爱丁堡大学 Mark Steedman 获得。\n最佳长论文：\nFinding syntax in human encephalography with beam search\n用波束搜索在人脑成像中寻找句法\n作者：John Hale，Chris Dyer，Adhiguna Kuncoro，Jonathan R.Brennan\n论文摘要：循环神经网络文法（RNNGs）是对于树-字符串对的生成式模型，它们依靠神经网络来评价派生的选择。用束搜索对它们进行解析可以得到各种不同复杂度的评价指标，比如单词惊异数（word surprisal count）和解析器动作数（parser action count）。当把它们用作回归因子，解析人类大脑成像图像中对于自然语言文本的电生理学响应时，它们可以带来两个增幅效果：一个早期的峰值以及一个类似 P600 的稍迟的峰值。相比之下，一个不具有句法结构的神经语言模型无法达到任何可靠的增幅效果。通过对不同模型的对比，早期峰值的出现可以归功于 RNNG 中的句法组合。结果中体现出的这种模式表明 RNNG+束搜索的组合可以作为正常人类语言处理中的语法处理的一个不错的机理解释模型。\n论文地址：https://arxiv.org/abs/1806.04127\nLearning to Ask Good Questions: Ranking Clarification Questions using Neural\nExpected Value of Perfect Information\n学习如何问好的问题：通过完全信息下的期待值为追问问题排序\n作者：Sudha Rao，Hal Daumé III\n论文摘要：在沟通中，提问是一大基本要素：如果机器不知道如何问问题，那它们也就无法高效地与人类合作。在这项研究中，作者们构建了一个神经网络用于给追问的问题做排名。作者们模型设计的启发来源于完全信息情况下的期待值：一个可以期待获得有用的答案的问题就是一个好问题。作者们根据 StackExchange 上抓取的数据研究了这个问题；StackExchange 是一个内容丰富的在线咨询平台，其中有人发帖咨询以后，别的用户会在下面追问起到解释澄清作用的问题，以便更好地了解状况、帮助到发帖人。论文作者们创建了一个由这样的追问问题组成的数据集，其中包含了 StackExchange 上 askubuntu、unix、superuser 这三个领域的约 77k 组发帖+追问问题+问题的回答。作者们在其中的 500 组样本上评估了自己的模型，相比其他基准模型有显著的提高；同时他们也与人类专家的判断进行了对比。\n论文地址：https://arxiv.org/abs/1805.04655\nLet’s do it “again”: A First Computational Approach to Detecting Adverbial\nPresupposition Triggers\n让我们“再”做一次：首个检测假定状态触发副词的计算性方法\n作者：Andre Cianflone，Yulan Feng，Jad Kabbara，Jackie Chi Kit Cheung\n论文摘要：这篇论文中，作者们介绍了一种新的研究课题——预测副词词性的假定状态触发语（adverbial presupposition triggers），比如 also 和 again。完成这样的任务需要在对话上下文里寻找重复出现的或者相似的内容；这项任务的研究成果则可以在文本总结或者对话系统这样的自然语言生成任务中起到帮助。作者们为这项任务创造了两个新的数据集，分别由 Penn Treebank 和 AnnotatedEnglish Gigaword 生成，而且也专为这项任务设计了一种新的注意力机制。作者们设计的注意力机制无需额外的可训练网络参数就可以增强基准 RNN 模型的表现，这最小化了这一注意力机制带来的额外计算开销。作者们在文中表明，他们的模型相比多个基准模型都有统计显著的更高表现，其中包括基于 LSTM 的语言模型。\n论文地址：https://www.cs.mcgill.ca/~jkabba/acl2018paper.pdf\n最佳短论文\nKnow What You Don’t Know: Unanswerable Questions for SQuAD\n知道你不知道的：SQuAD 中无法回答的问题\n作者：Pranav Rajpurkar，Robin Jia，Percy Liang\n论文摘要：提取式的阅读理解系统一般都能够在给定的文档内容中找到正确的内容来回答问题。不过对于正确答案没有明示在阅读文本中的问题，它们就经常会做出不可靠的猜测。目前现有的阅读理解问答数据集，要么只关注了可回答的问题，要么使用自动生成的无法回答的问题，很容易识别出来。为了改善这些问题，作者们提出了 SQuAD2.0 数据集，这是斯坦福问答数据集 SQuAD 的最新版本。SQuAD2.0 在现有的十万个问题-答案对的基础上增加了超过五万个无法回答的问题，它们由人类众包者对抗性地生成，看起来很像可以回答的问题。一个问答系统如果想要在 SQuAD2.0 上获得好的表现，它不仅需要在问题能够回答时给出正确的答案，还要在给定的阅读材料中不包含答案时做出决定、拒绝回答这个问题。\nSQuAD2.0 也设立了新的人类表现基准线，EM86.831，F189.452。对于现有模型来说SQuAD2.0 是一个具有挑战性的自然语言理解任务，一个强有力的基于神经网络的系统可以在 SQuAD1.1 上得到 86%的 F1 分数，但在 SQuAD2.0 上只能得到 66%。\n论文地址：https://arxiv.org/abs/1806.03822\n‘Lighter’ Can Still Be Dark: Modeling Comparative Color Descriptions\n“更浅的颜色”也可能仍然是黑暗的：建模比较性的颜色描述\n作者：Olivia Winn，Smaranda Muresan\n论文摘要：我们提出了一种在颜色描述领域内建立基准比较性形容词的新范式。给定一个参考 RGB 色和一个比较项（例如更亮、更暗），我们的模型会学习建立比较项的基准，将其作为 RGB 空间中的一个方向，这样颜色就会沿着向量植根于比较色中。我们的模型产生了比较形容词的基本表示形式，在期望的改变方向上，平均精确度为 0.65 余弦相似性。与目标颜色相比，依据向量的颜色描述方法 Delta-E 值小于 7，这表明这种方法与人类感知的差异非常小。这一方法使用了一个新创建的数据集，该数据集来自现有的标记好的颜色数据。\n论文地址：http://aclweb.org/anthology/P18-2125\n最佳 demo 论文\nOut-of-the-box Universal Romanization Tool\n开箱即用的通用罗马化工具\n作者：Ulf Hermjakob，Jonathan May，Kevin Knight\n论文摘要：我们想介绍 uroman，这个工具可以把五花八门的语言和文字（如中文、阿拉伯语、西里尔文）转换为普通拉丁文。该工具基于 Unicode 数据以及其他表，可以处理几乎所有的字符集（包括一些晦涩难懂的语言比如藏文和提非纳文）。uroman 还可以将不同文本中的数字转换为阿拉伯数字。罗马化让比较不同文本的字符串相似性变得更加容易，因为不再需要将两种文字翻译成中间文字再比较。本工具作为一个 Perl 脚本，可以免费提供，可用于数据处理管道和交互式演示网页。\n论文地址：http://aclweb.org/anthology/P18-4003\n\nACL 终身成就奖由 Mark Steedman 获得。\nMark Steedman 出生于 1946 年，1968 年毕业于苏塞克斯大学（University of Sussex），1973 年，获得爱丁堡大学人工智能博士学位（论文：《音乐知觉的形式化描述》）。此后，他曾担任华威大学心理学讲师，爱丁堡大学计算机语言学讲师，宾夕法尼亚大学计算机与信息科学学院副教授，也曾在德克萨斯大学奥斯汀分校，奈梅亨马克斯普朗克心理语言研究所和费城宾夕法尼亚大学担任过访问学者。\n目前他任爱丁堡大学信息学院认知科学系主任，主要研究领域有计算语言学、人工智能和认知科学、AI 会话的有意义语调生成、动画对话、手势交流以及组合范畴语法（Combinatorycategorial grammar，CCG）等。此外，他对计算音乐分析和组合逻辑等领域也很感兴趣。","data":"2019年04月26日 18:27:10"}
{"_id":{"$oid":"5d343ae562f717dc0659b261"},"title":"人工智能如何有效地运用于自然语言处理","author":"小发猫","content":"周末大家都出去玩，而我居然被人工智能深深吸引，其实AI还处于非常初级的发展阶段，今天看的是人工智能如何有效地运用于自然语言处理，1962年，Hubel和Wiesel通过猫视皮层细胞的研究提出了感受野的概念。 1984年，日本学者福岛基于感受野概念提出的神经认知机器（neocognitron）可以看作是卷积神经网络的第一个实现网络也是第一个将接受场概念应用于人工神经领域网络。神经认知机器将视觉图案划分为多个子图案（特征），然后进入用于处理的分层等级链接特征平面。它试图对视觉系统进行建模，使其即使物体被移位也可能会扭曲或稍微变形。同时，它可以完成识别。\n\n如何把人工智能运用到伪原创技术上？通常，神经认知机器包含两种类型的神经元，即携带特征提取的S元素和抵抗变形的C元素。 S元素包含两个重要参数，即感受野和阈值参数。前者确定输入连接的数量，后者控制对特征子模式的响应。许多学者一直致力于提高神经认知机器的性能：在传统的神经认知机器中，C-变形引起的视觉模糊通常分布在每个S元件的光敏区域中。如果由光感应区域的边缘产生的模糊效果大于中心的模糊效果，则S元件将接受由这种非正常模糊造成的较大变形容差。我们希望获得的是，接受场边缘处的训练模式和变形刺激模式之间的差异以及在中心处产生的效果变得越来越大。为了有效地形成这种非正常的模糊性，福岛提出了一种改进的具有双C元件层的神经认知机器。\n\n\nVan Ooyen和Niehuis引入了一个新参数来提高神经认知机器的识别能力。事实上，这个参数作为抑制信号抑制神经元对重复激励特性的激励。大多数神经网络记住权重中的训练信息。根据Hebb学习规则，特征被训练的次数越多，在后面的识别过程中检测它越容易。一些学者还将进化计算理论与神经认知机器相结合，削弱了对重复刺激特征进行训练和学习的能力，并使网络关注这些不同的特征来提高识别能力。所有这些都是神经认知机器的发展过程，而卷积神经网络可以看作是神经认知机器的一种普遍形式。神经认知机器是卷积神经网络的特例。\n对人工智能感兴趣的朋友可以看看我其他的总结文章：\nNLP神经网络实现在伪原创方面的运用\nNLP伪原创技术早期并不是很受欢迎\n基于主动学习的伪原创句法识别研究\n小发猫-人工智能的伪原创工具\n小发猫与普通伪原创工具的区别","data":"2018年03月31日 16:06:53"}
{"_id":{"$oid":"5d343ae662f717dc0659b263"},"title":"人工智能 | 自然语言处理（NLP）研究团队","author":"冲动的MJ","content":"博主github：https://github.com/MichaelBeechan\n博主CSDN：https://blog.csdn.net/u011344545\nNatural Language Processing\n自然语言处理（NLP）团队\n1、The Berkeley NLP Group\n加州大学伯克利分校\nhttp://nlp.cs.berkeley.edu/index.shtml\n\n2、The Stanford NLP Group\n斯坦福大学\nhttps://nlp.stanford.edu/\n\n\nChris Manning\nstatistical natural language processing, natural language understanding and deep learning\nDan Jurafsky\nnatural language understanding, conversational speech and dialog, NLP for the behavioral and social sciences\nPercy Liang\nsemantic parsing, probabilistic models for NLP, machine learning, program induction\n3、The Language Technologies Institute at Carnegie Mellon\n卡内基梅隆大学\nhttps://www.lti.cs.cmu.edu/\n\n4、Natural Language Processing Group Notre Dame\n圣母大学\nhttps://nlp.nd.edu/\n\n5、Harvard NLP\n哈佛大学\nhttp://nlp.seas.harvard.edu/\nhttp://nlp.seas.harvard.edu/code/\nUnsupervised Recurrent Neural Network Grammars\nLearning Neural Templates for Text Generation\n\n6、中科院计算所自然语言处理研究组\nhttp://nlp.ict.ac.cn/2017/introduction.php\n自然语言处理研究组隶属于中国科学院计算技术研究所智能信息处理重点实验室。研究组主要从事自然语言处理和机器翻译相关的研究工作，主要研究方向包括机器翻译、人机对话、多语言词法分析、句法分析等。研究组自2004年开展统计机器翻译方面的研究并取得重大突破，并于2015年起转向神经机器翻译并取得很大进展。研究组在自然语言处理的顶级国际刊物CL、AI和顶级国际学术会议ACL、IJCAI、AAAI、EMNLP、COLING上发表高水平论文90余篇，并获ACL颁发的“亚洲自然语言处理优秀论文奖”，多次在CWMT评测和IWSLT评测中获得第一名，并获得最权威的国家机器翻译评测NIST评测国内最好成绩,。部分研究成果获北京市科技进步奖、钱伟长中文信息处理科学技术奖。研究组已完成和正在承担的国家自然科学基金、863计划、科技支撑计划、国际合作等课题40余项，在自然语言处理和机器翻译领域取得了多项创新性研究成果。研究组已经成功将自主开发的统计机器翻译和神经机器翻译技术推广到汉语、维吾尔语、藏语、蒙古语、英语、韩语、泰语、日语、阿拉伯语等多种语言，并与腾讯、华为、中移动、三星等公司以及部分政府机关开展合作，部分语种的翻译系统已经在相关领域得到了实际应用，获得用户的好评。\n\n\n7、哥伦比亚大学\nhttp://www.cs.columbia.edu/people/faculty/\nhttp://www.cs.columbia.edu/areas/speech/\n\n\n8、哈工大社会计算与信息检索研究中心\nhttp://ir.hit.edu.cn/teams\n哈工大社会计算与信息检索研究中心 (HIT-SCIR) 成立于2000 年9月，隶属于计算机科学与技术学院。研究中心主任刘挺教授，副主任秦兵教授，教师包括张宇教授、车万翔教授、刘铭副教授/博导、张伟男副教授、丁效博士（讲师）、冯骁骋博士（助理研究员）、行政主管李冰老师、李艺雯老师。还有多位校内其他院系的老师参与实验室的研究工作，包括赵妍妍副教授/博导、张紫琼教授/博士后、景东讲师等。\n研究方向包括语言分析、信息抽取、情感分析、问答系统、社会媒体处理和用户画像6个方面。已完成或正在承担的国家973课题、国家自然科学基金重点项目、国家863重点项目、国际合作、企业合作等课题60余项。在这些项目的支持下打造出“语言技术平台LTP”，并免费共享给400多家研究机构，百度、腾讯、华为、金山等企业付费使用，获2010年钱伟长中文信息处理科学技术一等奖。\n9、自然语言处理研究小组\nhttp://nlpr-web.ia.ac.cn/cip/introduction.htm\n自然语言处理是利用计算机技术处理人类自然语言的一门交叉型学科，涉及计算机科学、数学、逻辑学、语言学和认知科学等多个领域。模式识别国家重点实验室自然语言处理组主要从事自然语言处理基础、机器翻译、信息抽取和问答系统等相关研究工作，力图在自然语言处理的理论模型和应用系统开发方面做出创新成果。目前研究组的主要方向包括：自然语言处理基础技术（汉语词语切分、句法分析、语义分析和篇章分析等）、多语言机器翻译、信息抽取（实体识别、实体关系抽取、观点挖掘等）和智能问答系统（基于知识库的问答系统、知识推理、社区问答等）。\n近年来，研究组注重于自然语言处理基础理论和应用基础的相关研究，取得了一批优秀成果，承担了一系列包括国家自然科学基金项目、“973”计划课题、“863”计划项目和支撑计划项目等在内的基础研究和应用基础研究类项目，以及一批企业应用合作项目。在自然语言处理及相关领域顶级国际期刊（CL、TASLP、TKDE、JMLR、TACL、Information Sciences、Intelligent Systems等）和学术会议（AAAI、IJCAI、ACL、SIGIR、WWW等）上发表了一系列高水平的研究论文。2009年获得第23届亚太语言、信息与计算国际会议（PACLIC）最佳论文奖，2012年获得第一届自然语言处理与中文计算会议（NLPCC）最佳论文奖，2014年获得第25届国际计算语言学大会（COLING）最佳论文奖。获得了10余项国家发明专利。\n\n10、Natural Language Computing（微软自然语言处理计算）\nhttps://www.microsoft.com/en-us/research/group/natural-language-computing/","data":"2019年06月22日 15:04:19"}
{"_id":{"$oid":"5d343ae662f717dc0659b265"},"title":"人工智能 | 自然语言处理研究报告（技术篇）","author":"冲动的MJ","content":"博主github：https://github.com/MichaelBeechan\n博主CSDN：https://blog.csdn.net/u011344545\n============================================\n概念篇：https://blog.csdn.net/u011344545/article/details/89525801\n技术篇：https://blog.csdn.net/u011344545/article/details/89526149\n人才篇：https://blog.csdn.net/u011344545/article/details/89556941\n应用篇：https://blog.csdn.net/u011344545/article/details/89574915\n下载链接：https://download.csdn.net/download/u011344545/11147085\n============================================\n清华AMiner团队 AMiner.org\n自然语言处理的研究领域极为广泛，各种分类方式层出不穷，各有其合理性，我们按照中国中文信息学会 2016 年发布的《中文信息处理发展报告》，将自然语言处理的研究领域和技术进行以下分类，并选取其中部分进行介绍。\n\n\n1、自然语言处理基础技术\n词法、句法及语义分析\n词法分析的主要任务是词性标注和词义标注。词性是词汇的基本属性，词性标注就是在给定句子中判断每个词的语法范畴，确定其词性并进行标注。解决兼类词和确定未登录词的词性问题是标注的重点。进行词性标注通常有基于规则和基于统计的两种方法。一个多义词往往可以表达多个意义，但其意义在具体的语境中又是确定的，词义标注的重点就是解决如何确定多义词在具体语境中的义项问题。标注过程中，通常是先确定语境，再明确词义，方法和词性标注类似，有基于规则和基于统计的做法。\n判断句子的句法结构和组成句子的各成分，明确它们之间的相互关系是句法分析的主要任务。句法分析通常有完全句法分析和浅层句法分析两种，完全句法分析是通过一系列的句法分析过程最终得到一个句子的完整的句法树。句法分析方法也分为基于规则和基于统计的方法，基于统计的方法是目前的主流方法，概率上下文无关文法用的较多。完全句法分析存在两个难点，一是词性歧义；二是搜索空间太大，通常是句子中词的个数 n 的指数级。浅层句法分析又叫部分句法分析或语块分析，它只要求识别出句子中某些结构相对简单的成分如动词短语、非递归的名词短语等，这些结构被称为语块。一般来说，浅层语法分析会完成语块的识别和分析、语块之间依存关系的分析两个任务，其中语块的识别和分析是浅层语法分析的主要任务。\n语义分析是指根据句子的句法结构和句子中每个实词的词义推导出来能够反映这个句子意义的某种形式化表示，将人类能够理解的自然语言转化为计算机能够理解的形式语言。句子的分析与处理过程，有的采用“先句法后语义”的方法，但“句法语义一体化”的策略还是占据主流位置。语义分析技术目前还不是十分成熟，运用统计方法获取语义信息的研究颇受关注，常见的有词义消歧和浅层语义分析。\n自然语言处理的基础研究还包括语用语境和篇章分析。语用是指人对语言的具体运用，研究和分析语言使用者的真正用意，它与语境、语言使用者的知识涵养、言语行为、想法和意图是分不开的，是对自然语言的深层理解。情景语境和文化语境是语境分析主要涉及的方面，篇章分析则是将研究扩展到句子的界限之外，对段落和整篇文章进行理解和分析。\n除此之外，自然语言的基础研究还涉及词义消歧、指代消解、命名实体识别等方面的研究。\n知识图谱\n2012 年 5 月，Google 推出 Google 知识图谱，并将其应用在搜索引擎中增强搜索能力，改善用户搜索质量和搜索体验，这是“知识图谱”名称的由来，也标志着大规模知识图谱在互联网语义搜索中的成功应用。搜索关键词，google 会在右侧给出与关键词相关的搜索结果。\n知识图谱，是为了表示知识，描述客观世界的概念、实体、事件等之间关系的一种表示形式。这一概念的起源可以追溯至语义网络——提出于 20 世纪五六十年代的一种知识表示形式。语义网络由许多个“节点”和“边”组成，这些“节点”和“边”相互连接，“节点”表示的是概念或对象，“边”表示各个节点之间的关系，如下图。\n\n知识图谱在表现形式上与语义网络比较类似，不同的是，语义网络侧重于表示概念与概念之间的关系，而知识图谱更侧重于表述实体之间的关系。现在的知识网络被用来泛指大规模的知识库，知识图谱中包含的节点有以下几种：\n实体：指独立存在且具有某种区别性的事物。如一个人、一种动物、一个国家、一种植物等。具体的事物就是实体所代表的内容，实体是知识图谱中的最基本元素，不同的实体间\n有不同的关系。\n语义类：具有同种特性的实体构成的集合，如人类、动物、国家、植物等。概念主要指集合、类别、对象类型、事物的种类，例如人物、地理等。内容：通常是实体和语义类的名字、描述、解释等，变现形式一般有文本、图像、音视频等。\n属性（值）：主要指对象指定属性的值，不同的属性类型对应于不同类型属性的边。关系：在知识图谱上，表现形式是一个将节点（实体、语义类、属性值）映射到布尔值的函数。\n除语义网络之外，70 年代的专家系统以及 Tim Berners Lee 提出的语义网和关联数据都可以说是知识图谱的前身。\n\n知识图谱表示、构建和应用涉及很多学科，是一项综合的复杂技术。知识图谱技术既涉及自然语言处理中的各项技术，从浅层的文本向量表示、到句法和语义结构表示被适用于资源内容的表示中，分词和词性标注、命名实体识别、句法语义结构分析、指代分析等技术被应用于自然语言处理中。同时，知识图谱的研究也促进了自然语言处理技术的研究，基于知识图谱的词义排岐和语义依存关系分析等知识驱动的自然语言处理技术得以建立。\n2、自然语言处理应用技术\n机器翻译\n机器翻译（Machine Translation）是指运用机器，通过特定的计算机程序将一种书写形式或声音形式的自然语言，翻译成另一种书写形式或声音形式的自然语言。机器翻译是一门交叉学科（边缘学科），组成它的三门子学科分别是计算机语言学、人工智能和数理逻辑，各自建立在语言学、计算机科学和数学的基础之上。\n机器翻译的方法总体上可以分为基于理性的研究方法和基于经验的研究方法两种。\n所谓“理性主义”的翻译方法，是指由人类专家通过编撰规则的方式，将不同自然语言之间的转换规律生成算法，计算机通过这种规则进行翻译。这种方法理论上能够把握语言间深层次的转换规律，然而理性主义方法对专家的要求极高，不仅要求其了解源语言和目标语言，还要具备一定的语言学知识和翻译知识，更要熟练掌握计算机的相关操作技能。这些因素都使得研制系统的成本高、周期长，面向小语种的翻译更是人才匮乏非常困难。因此，翻译知识和语言学知识的获取成为基于理性的机器翻译方法所面临的主要问题。\n所谓“经验主义”的翻译方法，指的是以数据驱动为基础，主张计算机自动从大规模数据中学习自然语言之间的转换规律。由于互联网文本数据不断增长，计算机运算能力也不断加强，以数据驱动为基础的统计翻译方法逐渐成为机器翻译的主流技术。但是同时统计机器翻译也面临诸如数据稀疏、难以设计特征等问题，而深度学习能够较好的缓解统计机器翻译所面临的挑战，基于深度学习的机器翻译现在正获得迅速发展，成为当前机器翻译领域的热点。\n机器翻译技术较早的被广泛应用在计算机辅助翻译软件上，更好地辅助专业翻译人员提升翻译效率，近几年机器翻译研究发展更为迅速，尤其是随着大数据和云计算技术的快速发展，机器翻译已经走进人们的日常生活，在很多特定领域为满足各种社会需求发挥了重要作用。按照媒介可以将机器翻译分为文本翻译、语音翻译、图像翻译以及视频和 VR 翻译等。\n目前，文本翻译最为主流的工作方式依然是以传统的统计机器翻译和神经网络翻译为主。Google、Microsoft 与国内的百度、有道等公司都为用户提供了免费的在线多语言翻译系统。将源语言文字输入其软件中，便可迅速翻译出目标语言文字。Google 主要关注以英语为中心的多语言翻译，百度则关注以英语和汉语为中心的多语言翻译。另外，即时通讯工具如Googletalk、Facebook 等也都提供了即时翻译服务。速度快、成本低是文本翻译的主要特点，而且应用广泛，不同行业都可以采用相应的专业翻译。但是，这一翻译过程是机械的和僵硬的，在翻译过程中会出现很多语义语境上的问题，仍然需要人工翻译来进行补充。\n语音翻译可能是目前机器翻译中比较富有创新意思的领域，吸引了众多资金和公众的注意力。亚马逊的 Alexa、苹果的 Siri、微软的 Cortana 等，我们越来越多的通过语音与计算机进行交互。应用比较好的如语音同传技术。同声传译广泛应用于国际会议等多语言交流的场景，但是人工同传受限于记忆、听说速度、费用偏高等因素门槛较高，搜狗推出的机器同传技术主要在会议场景出现，演讲者的语音实时转换成文本，并且进行同步翻译，低延迟显示翻译结果，希望能够取代人工同传，实现不同语言人们低成本的有效交流。科大讯飞、百度等公司在语音翻译方面也有很多探索。如科大讯飞推出的“讯飞语音翻译”系列产品，以及与新疆大学联合研发的世界上首款维汉机器翻译软件，可以准确识别维吾尔语和汉语，实现双语即时互译等功能。\n图像翻译也有不小的进展。谷歌、微软、Facebook 和百度均拥有能够让用户搜索或者自动整理没有识别标签照片的技术。图像翻译技术的进步远不局限于社交类应用。医疗创业公司可以利用计算机阅览 X 光照片、MRI（核磁共振成像）和 CT（电脑断层扫描）照片，阅览的速度和准确度都将超过放射科医师。而且更图像翻译技术对于机器人、无人机以及无人驾驶汽车的改进至关重要，福特、特斯拉、Uber、百度和谷歌均已在上路测试无人驾驶汽车的原型。\n除此之外还有视频翻译和 VR 翻译也在逐渐应用中，但是目前的应用还不太成熟。机器翻译这一话题 AMnier 研究报告系列第五期《人工智能之机器翻译研究报告》中有详细阐述，具体内容可查看：https://static.aminer.cn/misc/article/translation.pdf。\n信息检索\n信息检索是从相关文档集合中查找用户所需信息的过程。先将信息按一定的方式组织和存储起来，然后根据用户的需求从已经存储的文档集合当中找出相关的信息，这是广义的信息检索。信息检索最早提出于 20 世纪 50 年代，90 年代互联网出现以后，其导航工具——搜索引擎可以看成是一种特殊的信息检索系统，二者的区别主要在于语料库集合和用户群体的不同，搜索引擎面临的语料库是规模浩大、内容繁杂、动态变化的互联网，用户群体不再是具有一定知识水平的科技工作者，而是兴趣爱好、知识背景、年龄结构差异很大的网民群体。\n信息检索包括“存”与“取”两个方面，对信息进行收集、标引、描述、组织，进行有序的存放是“存”。按照某种查询机制从有序存放的信息集合（数据库）中找出用户所需信息或获取其线索的过程是“取”。信息检索的基本原理是将用户输入的检索关键词与数据库中的标引词进行对比，当二者匹配成功时，检索成功。检索标识是为沟通文献标引和检索关键词而编制的人工语言，通过检索标识可以实现“存”“取”的联系一致。检索结果按照与提问词的关联度输出，供用户选择，用户则采用“关键词查询+选择性浏览”的交互方式获\n取信息。\n以谷歌为代表的“关键词查询+选择性浏览”交互方式，用户用简单的关键词作为查询提交给搜索引擎，搜索引擎并非直接把检索目标页面反馈给用户，而是提供给用户一个可能的检索目标页面列表，用户浏览该列表并从中选择出能够满足其信息需求的页面加以浏览。这种交互方式对于用户来说查询输入是简单的事，但机器却难以通过简单的关键词准确的理解用户的真正查询意图，因此只能将有可能满足用户需求的结果集合以列表的形式提供给用户。\n目前互联网是人们获取信息的主要来源，网络上存放着取之不尽、用之不竭的信息，网络信息有着海量、分布、无序、动态、多样、异构、冗余、质杂、需求各异等特点。人们不再满足于当前的搜索引擎带来的查询结果，下一代搜索引擎的发展方向是个性化（精确化）、智能化、商务化、移动化、社区化、垂直化、多媒体化、实时化等。\n情感分析\n情感分析又称意见挖掘，是指通过计算技术对文本的主客观性、观点、情绪、极性的挖掘和分析，对文本的情感倾向做出分类判断。情感分析是自然语言理解领域的重要分支，涉及统计学、语言学、心理学、人工智能等领域的理论与方法。情感分析在一些评论机制的 App中应用较为广泛，比如某酒店网站，会有居住过的客人的评价，通过情感分析可以分析用户评论是积极还是消极的，根据一定的排序规则和显示比例，在评论区显示。这个场景同时也适用于亚马逊、阿里巴巴等电商网站的商品评价。\n除此之外，在互联网舆情分析中情感分析起着举足轻重的作用，话语权的下降和网民的大量涌入，使得互联网的声音纷繁复杂，利用情感分析技术获取民众对于某一事件的观点和意见，准确把握舆论发展趋势，并加以合理引导显得极为重要。\n同时，在一些选举预测、股票预测等领域情感分析也体现着越来越重要的作用。\n自动问答\n自动问答是指利用计算机自动回答用户所提出的问题以满足用户知识需求的任务。问答系统是信息服务的一种高级形式，系统反馈给用户的不再是基于关键词匹配排序的文档列表，而是精准的自然语言答案，这和搜索引擎提供给用户模糊的反馈是不同的。在自然语言理解领域，自动问答和机器翻译、复述和文本摘要一起被认为是验证机器是否具备自然理解能力的四个任务。\n自动问答系统在回答用户问题时，首先要正确理解用户所提出的问题，抽取其中关键的信息，在已有的语料库或者知识库中进行检索、匹配，将获取的答案反馈给用户。这一过程涉及了包括词法句法语义分析的基础技术，以及信息检索、知识工程、文本生成等多项技术。传统的自动问答基本集中在某些限定专业领域，但是伴随着互联网的发展和大规模知识库语料库的建立，面向开放领域和开放性类型问题的自动问答越来越受到关注。\n根据目标数据源的不同，问答技术大致可以分为检索式问答、社区问答以及知识库问答三种。检索式问答和搜索引擎的发展紧密联系，通过检索和匹配回答问题，推理能力较弱。社区问答是 web2.0 的产物，用户生成内容是其基础，Yahoo！、Answer、百度知道等是典型代表，这些社区问答数据覆盖了大量的用户知识和用户需求。检索式问答和社区问答的核心是浅层语义分析和关键词匹配，而知识库问答则正在逐步实现知识的深层逻辑推理。\n纵观自动问答发展历程，基于深度学习的端到端的自动问答将是未来的重点关注，同时，多领域、多语言的自动问答，面向问答的深度推理，篇章阅读理解以及对话也会在未来得到更广阔的发展。\n自动文摘\n自动文摘是运用计算机技术，依据用户需求从源文本中提取最重要的信息内容，进行精简、提炼和总结，最后生成一个精简版本的过程。生成的文摘具有压缩性、内容完整性和可读性。\n从 1955 年 IBM 公司 Luhn 首次进行自动文摘的实验至今的几十年中，自动文摘经历了基于统计的机械式文摘和基于意义的理解式文摘两种。机械式方法简单容易实现，是目前主要被采用的方法，但是结果不尽如人意。理解式文摘是建立在对自然语言的理解的基础之上的，接近于人提取摘要的方法，难度较大。但是随着自然语言处理技术的发展，理解式文摘有着长远的前景，应用于自动文摘的方法也会越来越多。\n\n作为解决当前信息过载的一项辅助手段，自动文摘技术的应用已经不仅仅限于自动文摘系统软件，在信息检索、信息管理等各领域都得到了广泛应用。同时随着深度学习等技术的发展，自动文摘也出现了许多新的研究和领域，例如多文本摘要、多语言摘要、多媒体摘要等。\n社会计算\n社会计算也称计算社会学，是指在互联网的环境下，以现代信息技术为手段，以社会科学理论为指导，帮助人们分析社会关系，挖掘社会知识，协助社会沟通，研究社会规律，破解社会难题的学科。社会计算是社会行为与计算系统交互融合，是计算机科学、社会科学、管理科学等多学科交叉所形成的研究领域。它用社会的方法计算社会，既是基于社会的计算，也是面向社会的计算。\n社会媒体是社会计算的主要工具和手段，它是一种在线交互媒体，有着广泛的用户参与性，允许用户在线交流、协作、发布、分享、传递信息、组成虚拟的网络社区等等。近年来，社会媒体呈现多样化的发展趋势，从早期的论坛、博客、维基到风头正劲的社交网站、微博和微信等，正在成为网络技术发展的热点和趋势。社会媒体文本属性特点是其具有草根性，字数少、噪声大、书写随意、实时性强；社会属性特点是其具有社交性，在线、交互。它赋予了每个用户创造并传播内容的能力，实施个性化发布，社会化传播，将用户群体组织成社会化网络，目前典型的社会媒体是 Twitter 和 Facebook，在我国则是微博和微信。社会媒体是一种允许用户广泛参与的新型在线媒体，通过社会媒体用户之间可以在线交流，形成虚拟的网络社区，构成了社会网络。社会网络是一种关系网络，通过个人与群体及其相互之间的关系和交互，发现它们的组织特点、行为方式等特征，进而研究人群的社会结构，以利于他们之间的进一步共享、交流与协作。\n社会计算应用广泛，近年来围绕社会安全、经济、工程和军事领域得到了长足发展。金融市场采用社会计算方法探索金融风险和危机的动态规律，例如美国圣塔菲研究所建立了首个人工股票市场的社会计算模型。许多发达国家都在政府资助下开展了研究项目，例如美国的 ASPEN，欧盟的 EURACE 等，并且在国家相应的经济政策制定中发挥着越来越重要的作用。通过社交媒体来把握舆情、引导舆论也是社会计算在社会安全方面发挥的一个重要作用。军事方面，许多国家更是加大投入力度扶持军事信息化的发展。\n信息抽取\n信息抽取技术可以追溯到 20 世纪 60 年代，以美国纽约大学开展的 Linguish String 项目和耶鲁大学 Roger Schank 及其同时开展的有关故事理解的研究为代表。信息抽取主要是指从文本中抽取出特定的事实信息，例如从经济新闻中抽取新发布产品情况，如公司新产品名、发布时间、发布地点、产品情况等，这些被抽取出来的信息通常以结构化的形式直接存入数据库，可以供用户查询及进一步分析使用，为之后构建知识库、智能问答等提供数据支撑。\n信息抽取和上文提到的信息检索关系密切，但是二者之间仍存在着很大的不同。首先是二者要实现的功能不同，信息检索是要从大量的文档中找到用户所需要的文档，信息抽取则是用在文本中获取用户感兴趣或所需要的事实信息。其次是二者背后的处理技术也不同，信息检索依靠的主要是以关键字词匹配以及统计等技术，不需要对文本进行理解和分析，而信息则需要利用自然语言处理的技术，包括命名实体识别、句法分析、篇章分析与推理以及知识库等，对文本进行深入理解和分析后才能完成信息抽取工作。除了以上的不同之外，信息检索和信息抽取又可以相互补充，信息检索的结果可以作为信息抽取的范围，提高效率，信息抽取用于信息检索可以提高检索质量，更好地满足用户的需求。\n信息抽取技术对于构建大规模的知识库有着重要的意义，但是目前由于自然语言本身的复杂性、歧义性等特征，而且信息抽取目标知识规模巨大、复杂多样等问题，使得信息抽取技术还不是很完善。但我们相信，在信息抽取技术经历了基于规则的方法、基于统计的方法、以及基于文本挖掘的方法等一系列技术演变之后，随着 web、知识图谱、深度学习的发展，可以为信息抽取提供海量数据源、大规模知识资源，更好地机器学习技术，信息抽取技术的问题会得到进一步解决并有长足的发展。","data":"2019年04月26日 16:59:05"}
{"_id":{"$oid":"5d343ae762f717dc0659b268"},"title":"人工智能科普｜自然语言处理（NLP）","author":"喜欢打酱油的老鸟","content":"https://www.toutiao.com/a6651845517565231620/\n2019-01-29 17:17:53\n今天晚上咱们要学习的课程比之前的难度要稍微大点，也是目前人工智能领域最难的研究方向之一——自然语言处理（NLP）。\n自然语言处理——AI领域“第一团宠”\nNLP作为AI领域的认知智能，其动态一直都是业内专家学者关注的重点，尤其是随着深度学习的不断进步，通过深度学习技术让NLP得到长足发展，让机器早日理解人类丰富多变的语言，成为了众多AI爱好者和开发者的期待。\n接下来童鞋们就跟着班主任一起来认识下被称为AI领域“第一团宠”的NLP，它能在日常中解决哪些问题，以及实操中会遇到的困难等。\n为了让大家更直观地理解自然语言处理，班主任画了一幅图：\n上图表明，计算机理解用户输入的各种语言的“谢谢”的过程。\n由用户输入不同语言中“谢谢”的不同文本，计算机根据不同文本处理出不同语言的“谢谢”，最后再将这些结果反馈给用户。\n其中这个过程包含了句法分析、自然语言处理和自然语言生成等相关技术。\nNLP解决的日常问题\n自然语言处理实际应用有如下几点：\nA、检验和提取不同类别的反馈\n通俗来讲，就是重点文本分析。例如通过一条微博、一篇新闻、一条朋友圈，研究不同的人对某件事的看法，通过研究对象正面或者负面的评论，采取进一步的决策。\nB、精准识别指代内容\n不同的目标群体在交流过程中会有不同的表达以及指代的方式，适当使用指代会使文本更加简练而且并不影响本意的阐述。\n例如，遇到生僻字“燚”不知道拼音的时候，大多会求助一些搜索引擎：“四个火是什么？”，搜索引擎一定会告诉你“燚”念什么，而不是告诉我们这几个词表面的匹配结果。由此可见，计算机能够“理解”这些指代内容。\nC、对给定文本进行分类\n对给定的文本，给出预定义的一个或多个分类标签，再进行高效、准确的分类。其实这就是一个简单的特征提取过程，通过不同的特征进行不同的分类。\n自然语言处理引起的歧义\nA、自然语言的二义性引起的歧义\n自然语言的二义性，其实说的就是自然语言中广泛存在的歧义现象。\n比如：“兵乓球拍卖完了”可切分为“乒乓球/拍卖/完了”又可以切分为“乒乓球拍/卖/完了”。对于这两种切分都是正确的。也就是说，就算是人工分词也会产生歧义。\n通过这两个例子我们可以看出，由于自然语言的二义性，句子存在着多种可能的组合方式或者句意，计算机在处理这些句子的时候就会费很大的劲。\nB、上下文理解引起的歧义\n所谓上下文，就是当前这句话所处的语言环境，这句话指代的主语、省略的部分、前后联系等等，都非常重要以及影响着这句话，因此上下文的理解是自然语言处理复杂性的一大体现。\n即使是同一概念，不同的人也有不同的解读，所以人们在日常对话中也会有语句理解歧义。例如看下面一段对话：\nA：今天一起吃饭吗？B：我妈今天从老家回来。\n如果仅仅按照字面理解，B的语句是无法回答A的。实际上，B是告诉A，今天我妈来了，不能和A一起吃饭了，这是人际交往中的一种间接拒绝。\n这就表明，相互之间的语言理解要借助语境推理，上下文理解不正确就会产生歧义。那么在计算机的自然语言处理中，要让计算机尽可能多的模拟人的智能，让机器具备人的上下文理解的功能。\n消除歧义\n由以上自然语言处理的歧义可以看出，NLP的关键在于消除歧义问题。而正确的消除歧义需要大量的知识，包括训练集的标注与添加、词典资源的建立。\n下面班主任来介绍三种消除歧义的方法：\nA、基于词典的消歧\n拿词典中的定义和歧义词出现的上下文环境进行对比，选择覆盖度最大的作为该词的词义。\n这种消歧方法思想很简单，但是消歧的准确率不是很高。\nB、有监督消歧\n让机器学习使用人工标记的数据，并与字典中的词语所代表的典型含义匹配。\n例如，在「I often play with my friends near the bank」一句中，「bank」一词需要机器判断是银行还是河边。我们希望机器能够匹配句中单词最有可能表达的含义，让机器更深刻地理解自然语言。\nC、无监督消歧\n不管是基于词典的消歧还是有监督消歧，都需要训练集，而无监督消歧不需要这些预先知道的资源。\n最简单的理解方式，就是把它比作考试。一般情况，每道题都有一个固定答案，对错代表分数的高低。那像作文只有题目没有固定答案，打分情况就要酌情而定。","data":"2019年02月01日 08:31:10","date":"2019年02月01日 08:31:10"}
{"_id":{"$oid":"5d343ae862f717dc0659b26b"},"title":"自然语言处理技术：让人工智能“听懂人话”","author":"OReillyData","content":"编者注： 更多人工智能业务方面重要的发展请关注2018年4月10-13日人工智能北京大会。\n人工智能的奇妙之处在于，它能让机器像人类一样拥有理解能力，完成智能任务。而它的难解之处在于，如何让人工智能拥有理解力，甚至让机器可以像人一样思考。让人工智能“听懂人话”，是近几年数据科学家们一直在做的努力，也收获了很多欣喜：\n为用户提供实时应答服务；\n为用户提供精确的搜索、推荐等个性化服务；\n辅助医生对患者进行综合诊疗；\n在无人驾驶、新闻传媒、在线娱乐、金融、教育等领域还将有许多令人期待的美好应用前景。\n可这些离人工智能像人的“小目标”还远着呢！尽管机器学习、深度学习、神经网络的发展推动自然语言技术的进步，但想要让机器学会理解，首先需要攻克人工智能的核心领域——自然语言处理技术（NLP）。关于它的技术痛点，应用难题，你都会在AI Conference 2018北京站 “自然语言处理与语音技术”板块中茅塞顿开。\n\n\n\n这里汇集了传媒/新闻、机器翻译、电信和教育行业的应用案例，先进的模型与算法,这里有我们耳熟能详的小冰，还有百度、微软、Intel、谷歌这些人工智能巨头，分享他们这些年在人工智能上翻过的山，趟过的河。\n在进入NLP的世界探究其奥秘之前，我迫不及待的要分享给你一些很有价值的内部消息：\n1. 你将会成为少数深入了解微软小冰的一员\n方向：与人工智能交互\n主题：小冰从人类与AI之间的对话中学到的经验教训\n主讲人：周力（微软中国）\n语音识别是人机交互的入口，经过四年的探索，小冰已经成为科技史上最大规模的人工智能情感计算框架系统。她当过歌手、诗人、主持人、评论员、客服，与中国、日本和美国超过1亿用户进行互动，从中学习人类特有的情感。小冰的每一次演进都让我们对机器能做什么产生了非常多的联想，这次又是什么呢？\n4月12日，微软小冰首席架构师周力博士将亲自带着小冰来到AI Conference 2018北京站，与大家分享在过去四年中研究微软小冰的感悟。希望在不久的将来，人类的生活会因为与人工智能的直接交流，变得更加美好。\n❖\n2. 使用Intel AI技术的NLP企业案例让你醍醐灌顶\n方向：模型与方法\n主题：深度学习时代的数据科学和自然语言处理\n主讲人：Yinyin Liu（Intel Nervana）\n2016年起，Intel逐渐将自己的战略重心转移到了数据科学和人工智能领域，向业界提供 AI 解决方案。最近几年主要的AI推动力是由深度学习产生的，NLP利用深度学习最新算法发展例如文档理解之类的应用，使公司能够筛查海量文本，分类并找到相关信息。\nIntel人工智能产品事业部数据科学主任Yinyin Liu将会与你讨论深度学习最新发展如何影响处理文本、语言及基于对话应用，并启发了利用数据的新方向。另外，还毫不吝啬的为大家分享一些使用Intel® AI技术的NLP企业案例。\n❖\n3. NLP落地金融了\n方向：模型与方法\n主题：使用AI来分析财务新闻的影响\n主讲人：ZhefuShi (密苏里大学)\nAI的领域在不断地进展之中，越来越多的公司认识到NLP对于金融行业分析的重要作用。在金融领域，AI技术对分析金融新闻的影响是有帮助的，将非结构化数据结构化处理，从中探寻影响市场变动的线索。比如通过历史金融新闻预测价格趋势，评估市场风险；为监管人员提供企业监管、市场监管、舆情监控；应用知识图谱和图谱计算技术来进行风险管理、供应链金融管理和投融资管理等。\n但自然语言处理技术，目前是人工智能进行场景落地时的一大难点重点。密苏里大学的Zhefu Shi博士带着自己的知识宝库与你分享如何使用AI来分析财务新闻的影响，如何提取金融实体信息并将其用于分析业务影响，敬请关注。\n❖\n4. 提升深度学习的表现有技巧\n方向：企业人工智能, 实施人工智能, 模型与方法\n主题：深度学习在文本挖掘中的应用\n主讲人：Emmanuel Ameisen (Insight DataScience), Jeremy Karnowski (Insight Data Science)\n深度学习在自然语言处理中的应用非常广泛，可以说横扫自然语言处理的各个应用，从底层的分词、语言模型、句法分析等到高层的语义理解、对话管理、知识问答等方面都几乎都有深度学习的模型，并且取得了不错的效果。多数公司已经开始利用文本数据支持部分业务运营，但也遇到了一系列挑战，其中包括如何验证和解释模型性能，以及模型复杂性如何影响部署它们的简便性。\nEmmanuel Ameisen和Jeremy Karnowski通过对google，Facebook，Amazon，Twitter，Salesforce，Airbnb等超过75个团队的对话进行分析，得到了很有价值的研究成果。分享他们如何从传统的机器学习算法转变成更有表现力的深度学习模型，如卷积神经网络和回归神经网络。这些新技术使公司能够改进许多关键业务操作，您将学习不同模型在不同项目中的应用，并了解如何选择最适合您项目的模型。\nGartner认为，未来10年，人工智能将成为最具破坏性级别的技术，主要是因为卓越的计算能力、漫无边际的数据集、深度神经网络领域的超乎寻常的进步。与其停留观望不如赶快行动，跟着大咖学习人工智能领域NLP的新知识，借鉴他们在人工智能布局的新思路。\n自然语言处理技术与语音技术\n15个议题   6大方向\n企业人工智能、模型与方法\n实施人工智能、人工智能交互\n……\n4月10-13日，AI Conference 2018北京站已经准备好了，你呢？","data":"2018年03月06日 00:00:00"}
{"_id":{"$oid":"5d343ae962f717dc0659b26d"},"title":"数据挖掘，机器学习，自然语言处理，人工智能？？？？？？","author":"jim_cainiaoxiaolang","content":"1.数据挖掘(DM)、机器学习(ML)、自然语言处理(NPL)这三者是什么关系？\n首先要认识到这三项并不是独立的选项，机器学习需要数据挖掘和自然语言处理的支撑，自然语言处理需要数据挖掘的支撑，数据挖掘需要大数据的支撑。最终所有的根源都要落实在大数据上，而这一切的顶点就是人工智能。从这个层面上来看数据挖掘是比较基础的部分，目前也有比较成熟的解决方案，只要你有数据不愁找不到工具。各种数据库（mongodb，Hive，Pig，HBase，RedShift），分布式系统（Hadoop， Spark），编程语言（Python和R）都是为其开发的或者擅长处理大数据。所谓学习数据挖掘已经逐渐变成熟练掌握这些工具的过程了。当然如果有兴趣，也可以参与各种分布式系统的开发，不过基本上你能想到的所有好用的算法，前人都已经写好了集成进去了。\n\n自然语处理，在这个世界上除了谷歌，苹果，微软，IBM还没有其他能够挑战此领域并且获得受人瞩目的成就的公司。因为现在自然语言处理就是方法很落后，手段很暴力。基本上常用的技术在10几20年前就出现了，只不过那时候没有谁拥有上万台计算机来处理自然语，现在倒是有了。可离实用还有很长的路要走（可以看一下IBM的沃特森，基本上也就代表现阶段最强的自然语处理的水平了）。\n\n最后就是机器学习了，这一点除了我之外已经有很多人强调过了——“机器学习只是被过度神话了！”。说白了现在的机器学习技术就是“战五渣”，谁上谁后悔。目前除了以“深度学习”为代表的人工神经网络之外其他的大部分常用的学习方法都是统计学习。不仅要喂足了料，还要精心调教，还不一定出货，出了也基本上不准。如果恰好结果符合预期，只能说“运气真好”。不过也正是因为这样，机器学习才作为一项前沿学科，很多科学家去研究，据我目测，这一波深度学习热应该已经过去了吧。按这个节奏，不知道10年之后又会有什么技术点燃机器学习的热情也说不定。\n\n2.数据挖掘（DM）,机器学习（ML）和自然语言处理（NPL）这几个怎么入门啊？\n（1）数据挖掘\n见上一篇\n\n（2）机器学习\nNG的课程我以前看过一部分，讲的风格我觉得在干货之前都比较好懂。但是学子接受起来可能有困难。台湾大学的林轩田老师的machine learning至少在本科生教育上做的很好。他们有个team经常去各种比赛上刷奖。我目前在修他的机器学习课程，觉得质量不错。现在coursera上也有同步课程。\n\n传送门：Coursera.org\n\n个人觉得机器学习的很多方法都是从统计学上借鉴过来的，所以现在在补统计学的知识。同时作为一个理论性比较强的领域，线性代数和高等数学的知识起码是要具备的（至少人家用矩阵写个公式再做梯度下降你要看明白是在干嘛）。\n\n（3）自然语言处理\n\n首推资料以及唯一的资料：\n\nColumbia University, Micheal Collins教授的自然语言课程\n链接\u003e\u003e Michael Collins\n\nMichael Collins，绝对的大牛，这门课是我见过讲NLP最最最清楚的！尤其是他的讲义！\nCollins的讲义，没有跳步，每一步逻辑都无比自然。最关键的是，Collins的语言措辞真是超级顺畅，没有长难句，没有装逼句，没有语法错误以及偏难怪的表示（学术圈大都是死理工科宅，语文能这么好真实太难得了）。《数学之美》的作者吴军博士在书中评价Collins的博士论文语言如小说般流畅，其写作功底可见一般。\n举两个例子，如果有时间，不妨亲自体验下，静下心来读一读，我相信即使是零基础的人也是能感受到大师的魅力的。\n1.语言模型（Language Model）\nhttp://www.cs.columbia.edu/~mc ... 3.pdf\n2.隐马尔可夫模型与序列标注问题(Tagging Problems and Hidden Markov Models)\nhttp://www.cs.columbia.edu/~mc ... 3.pdf\n现在Michael Collins在coursera上也开了公开课，视频免费看\n链接\u003e\u003e Coursera\n比看讲义更清晰，虽然没有字幕，但是不妨一试，因为讲的真的好清楚。\n其在句法分析与机器翻译部分的讲解是绝对的经典。\n如果能把Collins的课跟下来，讲义看下来，那么你已经掌握了NLP的主要技术与现状了。\n应该可以看懂部分论文了，你已经入门了。\n\n----------NLP进阶----------\nCollins的NLP课程虽然讲的清晰，不过有些比较重要的前沿的内容没有涉及（应该是为了突出重点做了取舍），比如语言模型的KN平滑算法等。\n此外，Collins的课程更注重于NLP所依赖的基础算法，而对于这些算法的某些重要应用并没涉及，比如虽然讲了序列标注的算法隐马尔可夫模型，条件随机场模型，最大熵模型，但是并没有讲如何用这些算法来做命名实体识别、语义标注等。\nStanford NLP组在coursera的这个课程很好的对Collins的课进行了补充。\n本课程偏算法的应用，算法的实现过的很快，不过上完Collins的课后再上感觉刚刚好~\n（这两门课是Coursera上仅有的两门NLP课，不得不佩服Coursera上的课都是精品啊！）\n----------进阶前沿----------\n上完以上两个课后，NLP的主要技术与实现细节就应该都清楚了， 离前沿已经很近了，读论文已经没问题了。\n想要继续进阶前沿，就要读论文了。\nNLP比起其它领域的一个最大的好处，此时就显现出来了，NLP领域的所有国际会议期刊论文都是可以免费下载的！而且有专人整理维护，每篇论文的bibtex也是相当清晰详细。\n关于NLP都有哪些研究方向，哪些比较热门，可以参考：当前国内外在自然语言处理领域的研究热点\u0026难点？ - White Pillow 的回答\nNLP是会议主导，最前沿的工作都会优先发表在会议上。关于哪个会议档次比较高，可以参考谷歌给出的会议排名：\nTop conference页面\n也可以参考各个会议的录稿率（一般来说越低表示会议档次越高）：\nConference acceptance rates\n基本上大家公认的NLP最顶级的会议为ACL，可以优先看ACL的论文。\n最后简单谈一下这三者哪个更有发展潜力……作为一个NLP领域的研究生，当然要说NLP领域有潜力啦！\n这里YY几个未来可能会热门的NLP的应用：\n语法纠错\n目前文档编辑器（比如Word）只能做单词拼写错误识别，语法级别的错误还无能为力。现在学术领域最好的语法纠错系统的正确率已经可以接近50%了，部分细分错误可以做到80%以上，转化成产品的话很有吸引力吧~无论是增强文档编辑器的功能还是作为教学软件更正英语学习者的写作错误。\n结构化信息抽取\n输入一篇文章，输出的是产品名、售价，或者活动名、时间、地点等结构化的信息。NLP相关的研究很多，不过产品目前看并不多，我也不是研究这个的，不知瓶颈在哪儿。不过想象未来互联网信息大量的结构化、语义化，那时的搜索效率绝对比现在翻番啊~\n语义理解\n这个目前做的并不好，但已经有siri等一票语音助手了，也有watson这种逆天的专家系统了。继续研究下去，虽然离人工智能还相去甚远，但是离真正好用的智能助手估计也不远了。那时生活方式会再次改变。即使做不到这么玄乎，大大改进搜索体验是肯定能做到的~搜索引擎公司在这方面的投入肯定会是巨大的。\n机器翻译\n这个不多说了，目前一直在缓慢进步中~我们已经能从中获益，看越南网页，看阿拉伯网页，猜个大概意思没问题了。此外，口语级别的简单句的翻译目前的效果已经很好了，潜在的商业价值也是巨大的。\n不过……在可预见的近几年，对于各大公司发展更有帮助的估计还是机器学习与数据挖掘，以上我YY的那些目前大都还在实验室里……目前能给公司带来实际价值的更多还是推荐系统、顾客喜好分析、股票走势预测等机器学习与数据挖掘应用~","data":"2016年08月25日 16:23:43"}
{"_id":{"$oid":"5d343ae962f717dc0659b270"},"title":"为什么说自然语言处理是人工智能的核心","author":"机器学习算法与Python学习","content":"微信公众号\n关键字全网搜索最新排名\n【机器学习算法】：排名第一\n【机器学习】：排名第一\n【Python】：排名第三\n【算法】：排名第四\n如果一台计算机能够欺骗人类，让人相信它是人类，那么该计算机就应当被认为是智能的。\n——阿兰·图灵\n机器能跟我们人类交流吗，能像我们人类一样理解文本吗，这是大家对人工智能最初的幻想。如今，它已成为人工智能的核心领域——自然语言处理（简称：NLP）。自然语言处理是一门融语言学、计算机科学、人工智能于一体的科学，解决的是“让机器可以理解自然语言”——这一到目前为止都还只是人类独有的特权，因此，被誉为人工智能皇冠上的明珠。\n如今，这门学科受到了国家政府、各大企业的普遍关注。国务院《新一代人工智能发展规划》，明确指出建立新一代人工智能关键共性技术体系，自然语言处理技术作为八大共性技术之一，被重点强调和扶持。\n无处不在的自然语言处理\n我们每天都在使用或受益于“自然语言处理”的技术，举个例子，微软小冰是中国微博上的一款将对话带入我们日常生活的聊天机器人。百万年轻中国用户通过小冰交换信息，与他人分手、丢了工作或感觉沮丧时，人们经常会和小冰聊天。到目前，小冰已经累积了上亿用户，平均聊天的回数23轮，平时聊天时长大概是25分钟左右。自然语言处理技术更广泛使用，可见下面的案例：\n机器翻译\n\n\n去年秋天，谷歌翻译推出了一个全新升级的人工智能翻译引擎。这样一来，曾以产出语言生硬但又可用的翻译而闻名的谷歌翻译，已开始产出语言流畅、精确度高的翻译文本。对未经专业翻译训练的人来说，这种文本输出几乎与人工翻译并未有区别。我们将上面这段文字输入到谷歌翻译中（中译英），输出的英文句子，让人惊叹！\n\n图一 谷歌翻译示意图\n垃圾邮件检测\n\n\n在自动垃圾邮件检测等一些应用中，分类只有两个：垃圾邮件和非垃圾邮件。在其它情况下，分类器可以有多个分类，比如按主题组织新闻报道或按领域组织学术论文。而要是一篇博客文章谈论的是体育和娱乐又会怎样？一个分类器如何在多个选项之间选择正确的分类？那依赖于具体应用：它可以简单地选择最有可能的选项，但有时候为一个文本分配多个分类是有意义的。\n\n图二 邮件自动分类\n\n问答系统\n\n\n从2011年Siri诞生，到Google Now，再到Cortana和Alexa，作为语音助手，其实它们本质上都是问答系统。这几个都是面向公开领域的问答系统，在我们的日常生活中帮忙定闹钟、打电话、导航、搜索问题，偶尔还能讲讲笑话，也正让我们的生活越来越方便。\n图三 苹果Siri示意图\n尤其是2010年后，深度学习应用于自然语言处理领域，一系列的产品功能逐渐走进我们的生活。各大企业也在纷纷布局相关产业，重金招揽相关领域人才。我国在语言文字信息处理方面就诞生了三家上市公司，从上市的顺序来说，最早是汉王，做模式识别，后来科大讯飞做语音识别，然后是拓而思的信息检索和文本挖掘。\n\n图四 知名招聘网站岗位图\n作为人工智能的一大热门研究领域，如何从基础开始入门，并学习到最新的技术呢？自然语言处理领域知名青年学者、国际顶级会议作者周教授，推出《自然语言处理基础与算法实践》、《基于深度学习的自然语言处理》两门在线直播课程（基础课+提高课），课程优秀学员可直接推荐至百度、搜狗、今日头条等知名企业实习就业。\n课程讲师\n周老师，教授、硕士生导师，中科院自动化所博士，主要从事自然语言处理以及深度学习等方面的研究工作。在相关领域国际期刊以及国际顶级学术会议ACL等发表论文20余篇，先后两次获得国际会议最佳论文奖。目前承担国家自然科学基金、973子课题等10余项。\n课程特色\n1. 顶级会议作者主讲，洞悉技术前沿；\n\n2. 理论结合实践，基础搭配强化课程；\n3. 课上在线直播答疑，课下微信群答疑；\n4. 优秀学员推荐名企实习就业；\n5. 课程PPT、数据集和源程序，均向学员公开。\n课程目录\nPart I：基础课程（10学时）\n1. 句法分析与语义分析（2学时）\n1.1  依存句法分析\n1.2  语义角色标注\n1.3  相关数据集、工具介绍\n2. 观点挖掘与情感分析（2学时）\n2.1  句子级情感分析\n2.2  文档级情感分析\n2.3  跨语言情感分析\n2.4  跨领域情感分析\n2.5  相关数据集、工具介绍\n3. 信息抽取：part 1（2学时）\n3.1  命名实体识别与抽取\n3.2  实体消歧\n3.3  相关数据集、工具介绍\n4. 信息抽取：part 2 （2学时）\n4.1  实体关系抽取\n4.2  事件抽取\n4.3  相关数据集、工具介绍\n5. 问答系统（2学时）\n5.1  检索式问答\n5.2  社区问答\n5.3  知识库问答\n5.4  相关数据集、工具介绍\nPart II：基于深度学习的NLP实战（24学时）\n\n6. 基于深度学习的词法分析（4学时）\n6.1 基于深度学习的中文分词\n6.2 基于深度学习的词性标注\n6.3 基于深度学习的命名实体识别\n6.4 代码模块演示、常用工具和公共数据集\n7. 基于深度学习的句法与语义分析（4学时）\n7.1 基于图的依存句法分析\n7.2 基于转移的依存句法分析\n7.3 浅层语义角色标注\n7.4 代码模块演示、常用工具和公共数据集\n8. 基于深度学习的情感分析（4学时）\n8.1 基于深度学习的情感词典构建\n8.2 基于深度学习的句子级情感分析\n8.3 基于深度学习的文档级情感分析\n8.4 基于深度学习的跨语言情感分析\n8.5 代码模块演示、常用工具和公共数据集\n9. 基于深度学习的信息抽取：Part 1（4学时）\n9.1 基于深度学习的实体关系抽取\n9.2 基于深度学习的实体消歧\n9.3 代表性系统模块演示、常用工具和公共数据集\n10. 基于深度学习的信息抽取：Part 2（4学时）\n10.1 基于深度学习的事件抽取\n10.2 基于深度学习的知识库表示\n10.3 基于深度学习的知识库补全\n10.4 代码模块演示、常用工具和公共数据集\n11. 基于深度学习的问答系统（4学时）\n11.1 基于深度学习的社区问答\n11.2 基于深度学习的复杂问句解析\n11.3 基于深度学习的知识库问答\n11.4 代码模块演示、常用工具和公共数据集\n报名\n前100名可领取 150元优惠券 ，1月6日正式开课，每周六、周日晚 19点到 21 点在线直播授课，一年内可以无限次在线回放。\n\n注：基础课程1月6号开课，适合小白用户入门，实践环节采用开源工具，无需编程；提高课程3月中旬开课，主要实践领域内最新最前沿的技术，需要深度学习基础，采用Python 2.8与深度学习TensorFlow框架。\n\n\n请添加工作人员「深蓝学院」助教报名","data":"2017年12月07日 00:00:00"}
{"_id":{"$oid":"5d343aea62f717dc0659b272"},"title":"人工智能、深度学习、计算机视觉、自然语言处理、机器学习视频教程","author":"jinhuan_hit","content":"1.CS224D\n2.NLP到Word2vec\n3.Opencv3图像处理\n4.Tensorflow视频教程\n5.机器学习视频教程\n6.七月在线所有人工智能课程\n7.聊天机器人视频教程\n8.自然语言处理视频教程\n链接如下，需要的可以在百度网盘下载。\n链接1:https://pan.baidu.com/s/1uqqYMQ3J4Vk1kSCH-7D4dA 密码:28s7 链接2:https://pan.baidu.com/s/1KdRYyI0Yta5gWNDwQge7sw 密码:0smi 链接3:https://pan.baidu.com/s/1gzSHZ52kCdNrrgtLyy3f1w 密码:8dl0 链接4:https://pan.baidu.com/s/1KrGVuhICot9GRLa-XFRsAA 密码:qa88 链接5:https://pan.baidu.com/s/16xwLarpVSQp6-ZIAAp7mOQ 密码:ixdk","data":"2018年08月02日 20:34:15"}
{"_id":{"$oid":"5d343aea62f717dc0659b274"},"title":"自然语言处理专题","author":"GitChat的博客","content":"NLP，全名 Natural Language Processing（自然语言处理）。从 1949 年的机器翻译设计方案到如今比尔盖茨认为“自然语言理解是人工智能皇冠上的明珠” ，NLP 成为了人工智能领域的重要战略目标。有读者曾留言问道“自然”指的是什么？在这里“自然”指的是随文化自然而然演化的过程。一千个人里就有一千个哈姆雷特，更何况还要让机器去理解我们的语言。\n这期我们精心挑选了 8 篇 NLP 相关的原理、应用和中文文本处理项目文章供大家学习。\n微信模式识别中心的高级研究员张金超博士不仅介绍了自然语言处理的基本概念和任务，还结合项目经验讲解使用深度学习解决 NLP 的方法和应用。文末尾也给想技能进阶的同学提供了“智者”建议。点此阅读全文\n如果你是一位 NLP or 机器学习爱好者，那你不能错过 Quora 上回答过万的热门问题；如果你恰好英语还可以，那小编觉得你最好别错过。点此阅读全文\n作者运用 Python 对《红楼梦》进行了中文分词，效果如何你不妨去看看。点此阅读全文\n用 WordCloud 制作词云、朴素贝叶斯算法和 SVM 分别对文本分类、LDA 主题模型获取文本关键词这一系列中文文本处理手段，都能在这篇文章看到！点此阅读全文\n学会了文本处理基础后，这篇将用集成学习和深度学习的两个应用实例来详细解说文本分类的原理。点此阅读全文\n除了文本分类，文本相似度的度量和计算也是必不可少的技能。即使不掌握但也可以通过了解相关的原理来避免文章被洗稿。点此阅读全文\n要是想学着设计一个文本相似度系统，可以来看看这篇。点此阅读全文\n基于在线 LU 工具无法本地部署等问题，微软工程师从基础开始讲解语言理解模块的工程实现。点此阅读全文","data":"2019年01月21日 11:49:28","date":"2019年01月21日 11:49:28"}
{"_id":{"$oid":"5d343aeb62f717dc0659b277"},"title":"人工智能自然语言处理技术处理专业领域的运用","author":"qq_44937469","content":"自然语言处理（NLP）是现代计算机科学和人工智能领域的一个重要分支，是一门融合了语言学、数学、计算机科学的科学。这一领域的研究将涉及自然语言，即人们日常使用的语言，所以它与语言学的研究有着密切的联系，但又有重要的区别。自然语言处理并不是一般地研究自然语言，而在于研制能有效地实现自然语言通信的计算机系统，特别是其中的软件系统。因而它是计算机科学的一部分。\n词法分析\n基于大数据和用户行为，对自然语言进行中文分词、词性标注、命名识体识别，定位基本语言元素，消除歧义，支撑自然语言的准确理解。\n中文分词 —— 将连续的自然语言文本，切分成具有语义合理性和完整性的词汇序列\n词性标注 —— 将自然语言中的每个词，赋予一个词性，如动词、名词、副词\n命名实体识别 —— 即专有名词识别，识别自然语言文本中具有特殊意义的实体，如人名、机构名、地名\n依存句法分析\n利用句子中词与词之间的依存关系，来表示词语的句法结构信息，并用树状结构来表示整句的结构。依存句法分析主要有几大作用：\n精准理解用户意图。当用户搜索时输入一个query，通过依存句法分析，抽取语义主干及相关语义成分，实现对用户意图的精准理解。\n知识挖掘。对大量的非结构化文本进行依存句法分析，从中抽取实体、概念、语义关系等信息，构建领域知识。\n语言结构匹配。基于句法结构信息，进行语言的匹配计算，提升语言匹配计算的准确率。\n词向量表示\n词向量计算是通过训练的方法，将语言词表中的词映射成一个长度固定的向量。词表中的所有词向量构成了一个向量空间，每一个词都是这个向量空间中的一个点。利用这种方法，实现文本的可计算。主要应用在：\n快速召回结果。不同于传统的倒排索引结构，构建基于词向量的快速索引技术，直接从语义相关性的角度召回结果。\n个性化推荐。基于用户的过去行为，通过词向量计算，学习用户的兴趣，实现个性化推荐。\nDNN语言模型\n语言模型是通过计算给定词组成的句子的概率，从而判断所组成的句子是否符合客观语言表达习惯。通常用于机器翻译、拼写纠错、语音识别、问答系统、词性标注、句法分析和信息检索等。\n词义相似度\n用于计算两个给定词语的语义相似度，基于自然语言中的分布假设，即越是经常共同出现的词之间的相似度越高。词义相似度是自然语言处理中的重要基础技术，是专名挖掘、query改写、词性标注等常用技术的基础之一。主要应用：\n专名挖掘 —— 通过词语间语义相关性计算寻找人名、地名、机构名等词的相关词，扩大专有名词的词典，更好的辅助应用\nquery改写 —— 通过寻找搜索query中词语的相似词，进行合理的替换，从而达到改写query的目的，提高搜索结果的多样性。\n短文本相似度\n短文本相似度计算服务能够提供不同短文本之间相似度的计算，输出的相似度是一个介于-1到1之间的实数值，越大则相似度越高。这个相似度值可以直接用于结果排序，也可以作为一维基础特征作用于更复杂的系统。\n评论观点抽取\n自动分析评论关注点和评论观点，并输出评论观点标签及评论观点极性，包括美食、酒店、汽车、景点等，可帮助商家进行产品分析，辅助用户进行消费决策。\n情感倾向分析\n针对带有主观描述的中文文本，可自动判断该文本的情感极性类别并给出相应的置信度。情感极性分为积极、消极、中性。情感倾向分析能帮助企业理解用户消费习惯、分析热点话题和危机舆情监控，为企业提供有力的决策支持。","data":"2019年06月16日 21:11:01"}
{"_id":{"$oid":"5d343aec62f717dc0659b27b"},"title":"一分钟看懂人工智能/机器学习/计算机视觉/自然语言处理等应用 图解","author":"独步天秤","content":"之前看到的一篇文章，不知道原文在哪，如有侵权请告知。","data":"2018年12月19日 11:20:21"}
{"_id":{"$oid":"5d343aed62f717dc0659b27e"},"title":"深度学习在自然语言处理中的应用","author":"FigthingForADream","content":"综述的大体部分\n自然语言处理的基础研究主要包括词法分析、句法分析、语义分析、语用语境与篇章分析等的研究。\n词向量(Word embedding 或Word representation) 方法,可以将词映射转换到一个独立的向量空间\n自然语言处理技术中采用深度学习知识的原因可以总结为以下几点：\n1、自然语言处理任务中首先要解决的问题是处理对象的表示形式，为了表示对象，通常必须抽取一些特征，如文本的处理中，常常用词集合来表示一个文档。传统依赖手工的方式抽取特征，费时费力，不仅获取过程比较随意，且完备性较差，同时，根据处理任务或领域的不同，特征提取工作要重复进行，无法实现表示共享。深度学习中的特征提取，即指可以自动从数据中学习获取特征，这也是考虑在自然语言处理技术中采用深度学习知识的主要原因。\n2、目前大多数效果较好的自然语言处理任务和机器学习方法都依赖于标注数据，实际应用而言，自然语言中大量存在的是未标注数据。深度神经网络采用无监督方式完成预训练过程，恰恰提供了合适的训练模型。\n3、深度学习结构一般由多层神经网络结点组成，其预训练过程通常需要高性能计算的支持，硬件及软件技术的发展，都为当前采用深度学习结构的自然语言处理提供了良好支撑环境。\n如何将深度学习与现有自然语言处理具体任务结合：\n比较直接简单的做法是，以词或短语作为原始输入，构建向量类型的表达方式，经过深度学习分层学习后得到的特征可以添加进现有基于特征的半监督学习系统中进行处理。\n中文自然语言处理的难点：\n首先由于每个汉字都包含不同的含义，需要为每个含义获取相应的表示。另外使用同音词或者多义词来为词语学习单一表征反而可能会影响最终的表征结果，由于多个含义之间的相互影响，不能准确表示任何一个含义。其次，需要进一步考虑训练语料问题，如何保证系统的鲁棒性、通用性，保证能够在不同领域都得到较好的效果，另外需要考虑新生词、网络用语等的识别问题。最后，需要考虑语料是否是越多越好，在训练学习的过程中，需要能够检测训练情况，避免过大的数据训练，破坏汉字的分布式表示。并且英文分词可以根据空格分词，而中文则不能简单的按照标点符号划分，需要联系上下文，考虑词性，词意。\n参考文献\n[1]竺宝宝, 张娜. 基于深度学习的自然语言处理[J]. 无线互联科技, 2017(10):25-26.\n[2]吴轲. 基于深度学习的中文自然语言处理[D]. 东南大学, 2014.\n[3]朱国进, 沈盼宇. 基于深度学习的算法知识实体识别与发现[J]. 智能计算机与应用, 2017, 7(1):17-21.\n[4] 冯志伟. 自然语言处理的历史与现状[J]. 中国外语, 2008, 5(1):14-22.\n[5] 姜倩盼. 自然语言处理的挑战与未来[J]. 信息与电脑:理论版, 2013(7):219-221.\n[6] 翟剑锋. 深度学习在自然语言处理中的应用[J]. 电脑编程技巧与维护, 2013(18):74-76.","data":"2018年04月09日 17:20:45","date":"2018年04月09日 17:20:45"}
{"_id":{"$oid":"5d343aed62f717dc0659b280"},"title":"人工智能 之 自然语言处理（NLP）算法分类总结","author":"seek_dreamer","content":"目录\n文章目录\n目录\n一、人工智能学习算法分类\n1. 纯算法类\n2.建模方面\n二、详细算法\n1.分类算法\n2.回归算法\n3.聚类算法\n4.降维算法\n5.概率图模型算法\n6.文本挖掘算法\n7.优化算法\n8.深度学习算法\n三、建模方面\n1.模型优化·\n2.数据预处理\n一、人工智能学习算法分类\n人工智能算法大体上来说可以分类两类：基于统计的机器学习算法(Machine Learning)和深度学习算法(Deep Learning)\n总的来说，在sklearn中机器学习算法大概的分类如下：\n1. 纯算法类\n(1).回归算法\n(2).分类算法\n(3).聚类算法\n(4)降维算法\n(5)概率图模型算法\n(6)文本挖掘算法\n(7)优化算法\n(8)深度学习算法\n2.建模方面\n(1).模型优化\n(2).数据预处理\n二、详细算法\n1.分类算法\n(1).LR (Logistic Regression，逻辑回归又叫逻辑分类)\n(2).SVM (Support Vector Machine，支持向量机)\n(3).NB (Naive Bayes，朴素贝叶斯)\n(4).DT (Decision Tree，决策树)\n1).C4.5\n2).ID3\n3).CART\n(5).集成算法\n1).Bagging\n2).Random Forest (随机森林)\n3).GB(梯度提升,Gradient boosting)\n4).GBDT (Gradient Boosting Decision Tree)\n5).AdaBoost\n6).Xgboost\n(6).最大熵模型\n2.回归算法\n(1).LR (Linear Regression，线性回归)\n(2).SVR (支持向量机回归)\n(3). RR (Ridge Regression，岭回归)\n3.聚类算法\n(1).Knn\n(2).Kmeans 算法\n(3).层次聚类\n(4).密度聚类\n4.降维算法\n(1).SGD (随机梯度下降)\n5.概率图模型算法\n(1).贝叶斯网络\n(2).HMM\n(3).CRF (条件随机场)\n6.文本挖掘算法\n(1).模型\n1).LDA (主题生成模型，Latent Dirichlet Allocation)\n2).最大熵模型\n(2).关键词提取\n1).tf-idf\n2).bm25\n3).textrank\n4).pagerank\n5).左右熵 :左右熵高的作为关键词\n6).互信息：\n(3).词法分析\n1).分词\n– ①HMM (因马尔科夫)\n– ②CRF (条件随机场)\n2).词性标注\n3).命名实体识别\n(4).句法分析\n1).句法结构分析\n2).依存句法分析\n(5).文本向量化\n1).tf-idf\n2).word2vec\n3).doc2vec\n4).cw2vec\n(6).距离计算\n1).欧氏距离\n2).相似度计算\n7.优化算法\n(1).正则化\n1).L1正则化\n2).L2正则化\n8.深度学习算法\n(1).BP\n(2).CNN\n(3).DNN\n(3).RNN\n(4).LSTM\n三、建模方面\n1.模型优化·\n(1).特征选择\n(2).梯度下降\n(3).交叉验证\n(4).参数调优\n(5).模型评估：准确率、召回率、F1、AUC、ROC、损失函数\n2.数据预处理\n(1).标准化\n(2).异常值处理\n(3).二值化\n(4).缺失值填充： 支持均值、中位数、特定值补差、多重插补","data":"2019年04月28日 13:42:59"}
{"_id":{"$oid":"5d343aed62f717dc0659b282"},"title":"人工智能、机器学习、模式识别、数据挖掘、自然语言处理","author":"beck_zhou","content":"人工智能（Artificial Intelligence，AI）、机器学习(Machine Leaining，ML)、模式识别（Pattern Recognition，PR）、数据挖掘（Data Mining, DM），他们要解决的核心问题不同，但是运用的数学模型如出一撤，主要是统计学方法。\n人工智能：就是计算机自动做决策，包含其他几个。\n机器学习：研究重点是算法的学习过程，强调的是一个反馈的、自我完善的框架，是人工智能的一个分支。\n模式识别：就是分类问题，是机器学习的一个方面，包括监督法和非监督法分类。\n数据挖掘：就是在大型数据库上的机器学习应用，偏重于从大型数据库中找规律的应用方面。\n自然语言处理NLP：\n主要的统计学方法：\n回归分析、决策树、贝叶斯学习、支持向量机SVM、PageRank 、K-Means、CRF条件随机场、隐马尔可夫模型。\n开源软件包：\nWeka\nCRF++","data":"2011年12月04日 14:22:25"}
{"_id":{"$oid":"5d343aee62f717dc0659b284"},"title":"人工智能 | 自然语言处理研究报告（概念篇）","author":"冲动的MJ","content":"博主github：https://github.com/MichaelBeechan\n博主CSDN：https://blog.csdn.net/u011344545\n============================================\n概念篇：https://blog.csdn.net/u011344545/article/details/89525801\n技术篇：https://blog.csdn.net/u011344545/article/details/89526149\n人才篇：https://blog.csdn.net/u011344545/article/details/89556941\n应用篇：https://blog.csdn.net/u011344545/article/details/89574915\n下载链接：https://download.csdn.net/download/u011344545/11147085\n============================================\n清华AMiner团队 AMiner.org\n摘要：\n自然语言处理是人工智能的一个重要应用领域，也是新一代计算机必须研究的课题。它的主要目的是克服人机对话中的各种限制，使用户能用自己的语言与计算机对话。\n1、自然语言处理概念\n自然语言是指汉语、英语、法语等人们日常使用的语言，是自然而然的随着人类社会发展演变而来的语言，而不是人造的语言，它是人类学习生活的重要工具。概括说来，自然语言是指人类社会约定俗成的，区别于如程序设计的语言的人工语言。在整个人类历史上以语言文字形式记载和流传的知识占到知识总量的 80%以上。就计算机应用而言，据统计，用于数学计算的仅占 10%，用于过程控制的不到 5%，其余 85%左右都是用于语言文字的信息处理。\n处理包含理解、转化、生成等过程。自然语言处理，是指用计算机对自然语言的形、音、义等信息进行处理，即对字、词、句、篇章的输入、输出、识别、分析、理解、生成等的操作和加工。实现人机间的信息交流，是人工智能界、计算机科学和语言学界所共同关注的重要问题。自然语言处理的具体表现形式包括机器翻译、文本摘要、文本分类、文本校对、信息抽取、语音合成、语音识别等。可以说，自然语言处理就是要计算机理解自然语言，自然语言处理机制涉及两个流程，包括自然语言理解和自然语言生成。自然语言理解是指计算机能够理解自然语言文本的意义，自然语言生成则是指能以自然语言文本来表达给定的意图。\n\n自然语言的理解和分析是一个层次化的过程，许多语言学家把这一过程分为五个层次，可以更好地体现语言本身的构成，五个层次分别是语音分析、词法分析、句法分析、语义分析和语用分析。\n\n在人工智能领域或者是语音信息处理领域中，学者们普遍认为采用图灵试验可以判断计算机是否理解了某种自然语言，具体的判别标准有以下几条：\n第一， 问答，机器人能正确回答输入文本中的有关问题；\n第二， 文摘生成，机器有能力生成输入文本的摘要；\n第三， 释义，机器能用不同的词语和句型来复述其输入的文本；\n第四， 翻译，机器具有把一种语言翻译成另一种语言的能力。\n2、自然语言处理发展历程\n自然语言处理是包括了计算机科学、语言学心理认知学等一系列学科的一门交叉学科，这些学科性质不同但又彼此相互交叉。因此，梳理自然语言处理的发展历程对于我们更好地了解自然语言处理这一学科有着重要的意义。\n\n1950 年图灵提出了著名的“图灵测试”，这一般被认为是自然语言处理思想的开端，20 世纪 50 年代到 70 年代自然语言处理主要采用基于规则的方法，研究人员们认为自然语言处理的过程和人类学习认知一门语言的过程是类似的，所以大量的研究员基于这个观点来进行研究，这时的自然语言处理停留在理性主义思潮阶段，以基于规则的方法为代表。但是基于规则的方法具有不可避免的缺点，首先规则不可能覆盖所有语句，其次这种方法对开发\n者的要求极高，开发者不仅要精通计算机还要精通语言学，因此，这一阶段虽然解决了一些简单的问题，但是无法从根本上将自然语言理解实用化。\n70 年代以后随着互联网的高速发展，丰富的语料库成为现实以及硬件不断更新完善，自然语言处理思潮由理性主义向经验主义过渡，基于统计的方法逐渐代替了基于规则的方法。贾里尼克和他领导的 IBM 华生实验室是推动这一转变的关键，他们采用基于统计的方法，将当时的语音识别率从 70%提升到 90%。在这一阶段，自然语言处理基于数学模型和统计的方法取得了实质性的突破，从实验室走向实际应用。\n从 2008 年到现在，在图像识别和语音识别领域的成果激励下，人们也逐渐开始引入深度学习来做自然语言处理研究，由最初的词向量到 2013 年的 word2vec，将深度学习与自然语言处理的结合推向了高潮，并在机器翻译、问答系统、阅读理解等领域取得了一定成功。深度学习是一个多层的神经网络，从输入层开始经过逐层非线性的变化得到输出。从输入到输出做端到端的训练。把输入到输出对的数据准备好，设计并训练一个神经网络，即可执行预想的任务。RNN 已经是自然语言护理最常用的方法之一，GRU、LSTM 等模型相继引发了一轮又一轮的热潮。\n3、我国自然语言处理现状\n20 世纪 90 年代以来，中国自然语言处理研究进入了高速发展期，一系列系统开始了大规模的商品化进程，自然语言处理在研究内容和应用领域上不断创新。\n目前自然语言处理的研究可以分为基础性研究和应用性研究两部分，语音和文本是两类研究的重点。基础性研究主要涉及语言学、数学、计算机学科等领域，相对应的技术有消除歧义、语法形式化等。应用性研究则主要集中在一些应用自然语言处理的领域，例如信息检索、文本分类、机器翻译等。由于我国基础理论即机器翻译的研究起步较早，且基础理论研究是任何应用的理论基础，所以语法、句法、语义分析等基础性研究历来是研究的重点，而且随着互联网网络技术的发展，智能检索类研究近年来也逐渐升温。\n从研究周期来看，除语言资源库建设以外，自然语言处理技术的开发周期普遍较短，基本为 1-3 年，由于涉及到自然语言文本的采集、存储、检索、统计等，语言资源库的建设较为困难，搭建周期较长，一般在 10 年左右，例如北京大学计算语言所完成的《现代汉语语法信息词典》以及《人民日报》的标注语料库，都经历了 10 年左右的时间才研制成功。\n自然语言处理的快速发展离不开国家的支持，这些支持包括各种扶持政策和资金资助。国家的资金资助包括国家自然科学基金、社会科学基金、863 项目、973 项目等，其中国家自然科学基金是国家投入资金最多、资助项目最多的一项。国家自然科学基金在基础理论研究方面的投入较大，对中文的词汇、句子、篇章分析方面的研究都给予了资助，同时在技术方面也给予了大力支持，例如机器翻译、信息检索、自动文摘等。除了国家的资金资助外，一些企业也进行了资助，但是企业资助项目一般集中在应用领域，针对性强，往往这些项目开发周期较短，更容易推向市场，实现由理论成果向产品的转化。\n4、自然语言处理业界发展\n\n微软亚洲研究院\nGoogle\nFacebook\n百度\n阿里巴巴\n腾讯\n京东\n科大讯飞","data":"2019年04月25日 21:00:29"}
{"_id":{"$oid":"5d343aee62f717dc0659b286"},"title":"人工智能-Python的自然语言处理视频课程","author":"itxueyuan88","content":"人工智能-Python的自然语言处理视频课程\n共33课时 共6小时25分钟 更新时间：2018-08-02\n本套课程是针对人工智能领域–自然语言理解的视频讲解，介绍了python语言对自然语言处理的工具包以及自然语言处理的方法使用。本套课程真对具有python编程基础的同学，在有python编程的基础上学习本套视频课程，会比较轻松的掌握python对自然语言处理的过程以及使用的方法，成为NLP工程。适用人群有英文基础，有python编程基础课程简介\npython版本：2.7\n课程目标处理语言：英语（English）\nNLP自然语言工程师： 初级课程\n所谓“自然语言”，是指人们日常交流使用的语言。\n本套课程是针对人工智能领域–自然语言理解的入门视频讲解，介绍了python语言对自然语言处理的工具包以及自然语言处理的方法使用。本套课程真对具有python编程基础的同学，在有python编程的基础上学习本套视频课程，会比较轻松的掌握python对自然语言处理的过程以及使用的方法。该套课程能够帮助想要进入人工智能—自然语言处理（NLP）行业的程序员对自然语言处理有初步的掌握与理解。\n成为一名出色的自然语言工程师，我相信这套课程会是你的开始。加入我们人工智能的大家庭吧，祝同学们学有所成。\n课程介绍：\n课时1-1：人工智能基本概念介绍 课时1-2：自然语言处理导论 课时2-1：NLTK函数使用（一） 课时2-2：NLTK函数使用（二） 课时2-3：计算机的简单统计 课时2-4：细粒度划分基础 课时2-5：细粒度划分条件详解 课时2-6：自动理解自然语言概念 课时3-1：nltk操作语料库的函数介绍 课时3-2:古腾堡语料库 课时3-3:网络和聊天文本库 课时3-4:布朗语料库 课时3-5:路透社语料库 课时3-6:就职演说语料库 课时3-7:标注文本语料库 课时3-8:文本语料库的结构与获取自己的语料库 课时3-9:条件频率分布于图表绘制 课时4-1：网络读取文件处理HTML 课时4-2：对搜索引擎与RSS订阅的处理 课时4-3：读取本地文件与NLP流程介绍 课时4-4：使用Unicode进行文本处理 课时4-5：使用正则表达式进行词干提取 课时4-6：NLTK词干提取器 课时4-7：使用正则表达式为文本分词 课时5-1：使用词性标注器 课时5-2：对语料库进行词性标注 课时5-3：自动标注-默认标注 课时5-4：自动标注-正则表达式标注 课时5-5：N-gram标注 课时6-1：信息提取介绍 课时6-2：名词短语分块 课时6-3：标记模式 课时6-4：理解自然语言 全套视频更新完毕 [百度云下载地址](http://itxuexiweb.com/thread-104-1-1.html)","data":"2018年08月07日 23:28:27","date":"2018年08月07日 23:28:27"}
{"_id":{"$oid":"5d343aef62f717dc0659b289"},"title":"自然语言处理技术的一些应用","author":"feng98ren","content":"自然语言处理技术的一些应用\n转载：https://zhuanlan.zhihu.com/p/31388720\n\n\n自然语言处理（NLP）是现代计算机科学和人工智能领域的一个重要分支，是一门融合了语言学、数学、计算机科学的科学。这一领域的研究将涉及自然语言，即人们日常使用的语言，所以它与语言学的研究有着密切的联系，但又有重要的区别。自然语言处理并不是一般地研究自然语言，而在于研制能有效地实现自然语言通信的计算机系统，特别是其中的软件系统。因而它是计算机科学的一部分。\n近段时间，笔者由于做了一些信息流内容平台相关的工作，对NLP的一些应用有了一定了解，所以和大家分享一下。\n\n\n1. 词法分析\n基于大数据和用户行为，对自然语言进行中文分词、词性标注、命名识体识别，定位基本语言元素，消除歧义，支撑自然语言的准确理解。\n中文分词 —— 将连续的自然语言文本，切分成具有语义合理性和完整性的词汇序列\n词性标注 —— 将自然语言中的每个词，赋予一个词性，如动词、名词、副词\n命名实体识别 —— 即专有名词识别，识别自然语言文本中具有特殊意义的实体，如人名、机构名、地名\n举例：\n\n\n2. 依存句法分析\n利用句子中词与词之间的依存关系，来表示词语的句法结构信息，并用树状结构来表示整句的结构。依存句法分析主要有几大作用：\n精准理解用户意图。当用户搜索时输入一个query，通过依存句法分析，抽取语义主干及相关语义成分，实现对用户意图的精准理解。\n知识挖掘。对大量的非结构化文本进行依存句法分析，从中抽取实体、概念、语义关系等信息，构建领域知识。\n语言结构匹配。基于句法结构信息，进行语言的匹配计算，提升语言匹配计算的准确率。\n举例：\n\n\n3. 词向量表示\n词向量计算是通过训练的方法，将语言词表中的词映射成一个长度固定的向量。词表中的所有词向量构成了一个向量空间，每一个词都是这个向量空间中的一个点。利用这种方法，实现文本的可计算。主要应用在：\n快速召回结果。不同于传统的倒排索引结构，构建基于词向量的快速索引技术，直接从语义相关性的角度召回结果。\n个性化推荐。基于用户的过去行为，通过词向量计算，学习用户的兴趣，实现个性化推荐。\n举例：\n\n\n\n\n4. DNN语言模型\n语言模型是通过计算给定词组成的句子的概率，从而判断所组成的句子是否符合客观语言表达习惯。通常用于机器翻译、拼写纠错、语音识别、问答系统、词性标注、句法分析和信息检索等。\n举例：\n\n\n5. 词义相似度\n用于计算两个给定词语的语义相似度，基于自然语言中的分布假设，即越是经常共同出现的词之间的相似度越高。词义相似度是自然语言处理中的重要基础技术，是专名挖掘、query改写、词性标注等常用技术的基础之一。主要应用：\n专名挖掘 —— 通过词语间语义相关性计算寻找人名、地名、机构名等词的相关词，扩大专有名词的词典，更好的辅助应用\nquery改写 —— 通过寻找搜索query中词语的相似词，进行合理的替换，从而达到改写query的目的，提高搜索结果的多样性\n举例：\n\n\n\n\n6. 短文本相似度\n短文本相似度计算服务能够提供不同短文本之间相似度的计算，输出的相似度是一个介于-1到1之间的实数值，越大则相似度越高。这个相似度值可以直接用于结果排序，也可以作为一维基础特征作用于更复杂的系统。\n举例：\n\n\n\n\n7. 评论观点抽取\n自动分析评论关注点和评论观点，并输出评论观点标签及评论观点极性，包括美食、酒店、汽车、景点等，可帮助商家进行产品分析，辅助用户进行消费决策。\n举例：\n\n\n\n\n\n8. 情感倾向分析\n针对带有主观描述的中文文本，可自动判断该文本的情感极性类别并给出相应的置信度。情感极性分为积极、消极、中性。情感倾向分析能帮助企业理解用户消费习惯、分析热点话题和危机舆情监控，为企业提供有力的决策支持。\n举例：","data":"2018年02月26日 16:05:17"}
{"_id":{"$oid":"5d343af062f717dc0659b28b"},"title":"读懂人工智能、机器学习、深度学习、大数据，自然语言处理……","author":"leishao_csdn","content":"从机器学习谈起\n在本篇文章中，我将对机器学习做个概要的介绍。本文的目的是能让即便完全不了解机器学习的人也能了解机器学习，并且上手相关的实践。这篇文档也算是EasyPR开发的番外篇，从这里开始，必须对机器学习了解才能进一步介绍EasyPR的内核。当然，本文也面对一般读者，不会对阅读有相关的前提要求。\n在进入正题前，我想读者心中可能会有一个疑惑：机器学习有什么重要性，以至于要阅读完这篇非常长的文章呢？\n我并不直接回答这个问题前。相反，我想请大家看两张图，下图是图一：\n\n图1 机器学习界的执牛耳者与互联网界的大鳄的联姻\n这幅图上上的三人是当今机器学习界的执牛耳者。中间的是Geoffrey Hinton, 加拿大多伦多大学的教授，如今被聘为“Google大脑”的负责人。右边的是Yann LeCun, 纽约大学教授，如今是Facebook人工智能实验室的主任。而左边的大家都很熟悉，Andrew Ng，中文名吴恩达，斯坦福大学副教授，如今也是“百度大脑”的负责人与百度首席科学家。这三位都是目前业界炙手可热的大牛，被互联网界大鳄求贤若渴的聘请，足见他们的重要性。而他们的研究方向，则全部都是机器学习的子类--深度学习。\n\n下图是图二：\n图2 语音助手产品\n这幅图上描述的是什么？Windows Phone上的语音助手Cortana，名字来源于《光环》中士官长的助手。相比其他竞争对手，微软很迟才推出这个服务。Cortana背后的核心技术是什么，为什么它能够听懂人的语音？事实上，这个技术正是机器学习。机器学习是所有语音助手产品(包括Apple的siri与Google的Now)能够跟人交互的关键技术。\n通过上面两图，我相信大家可以看出机器学习似乎是一个很重要的，有很多未知特性的技术。学习它似乎是一件有趣的任务。实际上，学习机器学习不仅可以帮助我们了解互联网界最新的趋势，同时也可以知道伴随我们的便利服务的实现技术。\n机器学习是什么，为什么它能有这么大的魔力，这些问题正是本文要回答的。同时，本文叫做“从机器学习谈起”，因此会以漫谈的形式介绍跟机器学习相关的所有内容，包括学科(如数据挖掘、计算机视觉等)，算法(神经网络，svm)等等。本文的主要目录如下：\n\n1.一个故事说明什么是机器学习\n\n2.机器学习的定义\n\n3.机器学习的范围\n\n4.机器学习的方法\n\n5.机器学习的应用--大数据\n\n6.机器学习的子类--深度学习\n\n7.机器学习的父类--人工智能\n\n8.机器学习的思考--计算机的潜意识\n\n9.总结\n\n10.后记\n\n\n1.一个故事说明什么是机器学习\n机器学习这个词是让人疑惑的，首先它是英文名称Machine Learning(简称ML)的直译，在计算界Machine一般指计算机。这个名字使用了拟人的手法，说明了这门技术是让机器“学习”的技术。但是计算机是死的，怎么可能像人类一样“学习”呢？\n\n传统上如果我们想让计算机工作，我们给它一串指令，然后它遵照这个指令一步步执行下去。有因有果，非常明确。但这样的方式在机器学习中行不通。机器学习根本不接受你输入的指令，相反，它接受你输入的数据! 也就是说，机器学习是一种让计算机利用数据而不是指令来进行各种工作的方法。这听起来非常不可思议，但结果上却是非常可行的。“统计”思想将在你学习“机器学习”相关理念时无时无刻不伴随，相关而不是因果的概念将是支撑机器学习能够工作的核心概念。你会颠覆对你以前所有程序中建立的因果无处不在的根本理念。\n\n下面我通过一个故事来简单地阐明什么是机器学习。这个故事比较适合用在知乎上作为一个概念的阐明。在这里，这个故事没有展开，但相关内容与核心是存在的。如果你想简单的了解一下什么是机器学习，那么看完这个故事就足够了。如果你想了解机器学习的更多知识以及与它关联紧密的当代技术，那么请你继续往下看，后面有更多的丰富的内容。\n\n这个例子来源于我真实的生活经验，我在思考这个问题的时候突然发现它的过程可以被扩充化为一个完整的机器学习的过程，因此我决定使用这个例子作为所有介绍的开始。这个故事称为“等人问题”。\n\n我相信大家都有跟别人相约，然后等人的经历。现实中不是每个人都那么守时的，于是当你碰到一些爱迟到的人，你的时间不可避免的要浪费。我就碰到过这样的一个例子。\n\n对我的一个朋友小Y而言，他就不是那么守时，最常见的表现是他经常迟到。当有一次我跟他约好3点钟在某个麦当劳见面时，在我出门的那一刻我突然想到一个问题：我现在出发合适么？我会不会又到了地点后，花上30分钟去等他？我决定采取一个策略解决这个问题。\n\n要想解决这个问题，有好几种方法。第一种方法是采用知识：我搜寻能够解决这个问题的知识。但很遗憾，没有人会把如何等人这个问题作为知识传授，因此我不可能找到已有的知识能够解决这个问题。第二种方法是问他人：我去询问他人获得解决这个问题的能力。但是同样的，这个问题没有人能够解答，因为可能没人碰上跟我一样的情况。第三种方法是准则法：我问自己的内心，我有否设立过什么准则去面对这个问题？例如，无论别人如何，我都会守时到达。但我不是个死板的人，我没有设立过这样的规则。\n\n事实上，我相信有种方法比以上三种都合适。我把过往跟小Y相约的经历在脑海中重现一下，看看跟他相约的次数中，迟到占了多大的比例。而我利用这来预测他这次迟到的可能性。如果这个值超出了我心里的某个界限，那我选择等一会再出发。假设我跟小Y约过5次，他迟到的次数是1次，那么他按时到的比例为80%，我心中的阈值为70%，我认为这次小Y应该不会迟到，因此我按时出门。如果小Y在5次迟到的次数中占了4次，也就是他按时到达的比例为20%，由于这个值低于我的阈值，因此我选择推迟出门的时间。这个方法从它的利用层面来看，又称为经验法。在经验法的思考过程中，我事实上利用了以往所有相约的数据。因此也可以称之为依据数据做的判断。\n\n依据数据所做的判断跟机器学习的思想根本上是一致的。\n\n刚才的思考过程我只考虑“频次”这种属性。在真实的机器学习中，这可能都不算是一个应用。一般的机器学习模型至少考虑两个量：一个是因变量，也就是我们希望预测的结果，在这个例子里就是小Y迟到与否的判断。另一个是自变量，也就是用来预测小Y是否迟到的量。假设我把时间作为自变量，譬如我发现小Y所有迟到的日子基本都是星期五，而在非星期五情况下他基本不迟到。于是我可以建立一个模型，来模拟小Y迟到与否跟日子是否是星期五的概率。见下图：\n图3 决策树模型\n这样的图就是一个最简单的机器学习模型，称之为决策树。\n\n当我们考虑的自变量只有一个时，情况较为简单。如果把我们的自变量再增加一个。例如小Y迟到的部分情况时是在他开车过来的时候(你可以理解为他开车水平较臭，或者路较堵)。于是我可以关联考虑这些信息。建立一个更复杂的模型，这个模型包含两个自变量与一个因变量。\n\n再更复杂一点，小Y的迟到跟天气也有一定的原因，例如下雨的时候，这时候我需要考虑三个自变量。\n\n如果我希望能够预测小Y迟到的具体时间，我可以把他每次迟到的时间跟雨量的大小以及前面考虑的自变量统一建立一个模型。于是我的模型可以预测值，例如他大概会迟到几分钟。这样可以帮助我更好的规划我出门的时间。在这样的情况下，决策树就无法很好地支撑了，因为决策树只能预测离散值。我们可以用节2所介绍的线型回归方法建立这个模型。\n\n如果我把这些建立模型的过程交给电脑。比如把所有的自变量和因变量输入，然后让计算机帮我生成一个模型，同时让计算机根据我当前的情况，给出我是否需要迟出门，需要迟几分钟的建议。那么计算机执行这些辅助决策的过程就是机器学习的过程。\n\n机器学习方法是计算机利用已有的数据(经验)，得出了某种模型(迟到的规律)，并利用此模型预测未来(是否迟到)的一种方法。\n\n通过上面的分析，可以看出机器学习与人类思考的经验过程是类似的，不过它能考虑更多的情况，执行更加复杂的计算。事实上，机器学习的一个主要目的就是把人类思考归纳经验的过程转化为计算机通过对数据的处理计算得出模型的过程。经过计算机得出的模型能够以近似于人的方式解决很多灵活复杂的问题。\n\n下面，我会开始对机器学习的正式介绍，包括定义、范围，方法、应用等等，都有所包含。\n2.机器学习的定义\n\n从广义上来说，机器学习是一种能够赋予机器学习的能力以此让它完成直接编程无法完成的功能的方法。但从实践的意义上来说，机器学习是一种通过利用数据，训练出模型，然后使用模型预测的一种方法。\n让我们具体看一个例子。\n\n\n图4 房价的例子\n拿国民话题的房子来说。现在我手里有一栋房子需要售卖，我应该给它标上多大的价格？房子的面积是100平方米，价格是100万，120万，还是140万？\n很显然，我希望获得房价与面积的某种规律。那么我该如何获得这个规律？用报纸上的房价平均数据么？还是参考别人面积相似的？无论哪种，似乎都并不是太靠谱。\n我现在希望获得一个合理的，并且能够最大程度的反映面积与房价关系的规律。于是我调查了周边与我房型类似的一些房子，获得一组数据。这组数据中包含了大大小小房子的面积与价格，如果我能从这组数据中找出面积与价格的规律，那么我就可以得出房子的价格。\n\n对规律的寻找很简单，拟合出一条直线，让它“穿过”所有的点，并且与各个点的距离尽可能的小。\n通过这条直线，我获得了一个能够最佳反映房价与面积规律的规律。这条直线同时也是一个下式所表明的函数：\n房价 = 面积 * a + b\n上述中的a、b都是直线的参数。获得这些参数以后，我就可以计算出房子的价格。\n假设a = 0.75,b = 50，则房价 = 100 * 0.75 + 50 = 125万。这个结果与我前面所列的100万，120万，140万都不一样。由于这条直线综合考虑了大部分的情况，因此从“统计”意义上来说，这是一个最合理的预测。\n\n在求解过程中透露出了两个信息：\n1.房价模型是根据拟合的函数类型决定的。如果是直线，那么拟合出的就是直线方程。如果是其他类型的线，例如抛物线，那么拟合出的就是抛物线方程。机器学习有众多算法，一些强力算法可以拟合出复杂的非线性模型，用来反映一些不是直线所能表达的情况。\n2.如果我的数据越多，我的模型就越能够考虑到越多的情况，由此对于新情况的预测效果可能就越好。这是机器学习界“数据为王”思想的一个体现。一般来说(不是绝对)，数据越多，最后机器学习生成的模型预测的效果越好。\n通过我拟合直线的过程，我们可以对机器学习过程做一个完整的回顾。首先，我们需要在计算机中存储历史的数据。接着，我们将这些 数据通过机器学习算法进行处理，这个过程在机器学习中叫做“训练”，处理的结果可以被我们用来对新的数据进行预测，这个结果一般称之为“模型”。对新数据 的预测过程在机器学习中叫做“预测”。“训练”与“预测”是机器学习的两个过程，“模型”则是过程的中间输出结果，“训练”产生“模型”，“模型”指导 “预测”。\n\n让我们把机器学习的过程与人类对历史经验归纳的过程做个比对。\n图5 机器学习与人类思考的类比\n\n人类在成长、生活过程中积累了很多的历史与经验。人类定期地对这些经验进行“归纳”，获得了生活的“规律”。当人类遇到未知的问题或者需要对未来进行“推测”的时候，人类使用这些“规律”，对未知问题与未来进行“推测”，从而指导自己的生活和工作。\n机器学习中的“训练”与“预测”过程可以对应到人类的“归纳”和“推测”过程。通过这样的对应，我们可以发现，机器学习的思想并不复杂，仅仅是对人类在生活中学习成长的一个模拟。由于机器学习不是基于编程形成的结果，因此它的处理过程不是因果的逻辑，而是通过归纳思想得出的相关性结论。\n这也可以联想到人类为什么要学习历史，历史实际上是人类过往经验的总结。有句话说得很好，“历史往往不一样，但历史总是惊人的相似”。通过学习历史，我们从历史中归纳出人生与国家的规律，从而指导我们的下一步工作，这是具有莫大价值的。当代一些人忽视了历史的本来价值，而是把其作为一种宣扬功绩的手段，这其实是对历史真实价值的一种误用。\n3.机器学习的范围\n上文虽然说明了机器学习是什么，但是并没有给出机器学习的范围。\n其实，机器学习跟模式识别，统计学习，数据挖掘，计算机视觉，语音识别，自然语言处理等领域有着很深的联系。\n从范围上来说，机器学习跟模式识别，统计学习，数据挖掘是类似的，同时，机器学习与其他领域的处理技术的结合，形成了计算机视觉、语音识别、自然语言处理等交叉学科。因此，一般说数据挖掘时，可以等同于说机器学习。同时，我们平常所说的机器学习应用，应该是通用的，不仅仅局限在结构化数据，还有图像，音频等应用。\n\n在这节对机器学习这些相关领域的介绍有助于我们理清机器学习的应用场景与研究范围，更好的理解后面的算法与应用层次。\n\n下图是机器学习所牵扯的一些相关范围的学科与研究领域。\n\n图6 机器学习与相关学科\n模式识别\n模式识别=机器学习。两者的主要区别在于前者是从工业界发展起来的概念，后者则主要源自计算机学科。在著名的《Pattern Recognition And Machine Learning》这本书中，Christopher M. Bishop在开头是这样说的“模式识别源自工业界，而机器学习来自于计算机学科。不过，它们中的活动可以被视为同一个领域的两个方面，同时在过去的10年间，它们都有了长足的发展”。\n\n数据挖掘\n数据挖掘=机器学习+数据库。这几年数据挖掘的概念实在是太耳熟能详。几乎等同于炒作。但凡说数据挖掘都会吹嘘数据挖掘如何如何，例如从数据中挖出金子，以及将废弃的数据转化为价值等等。但是，我尽管可能会挖出金子，但我也可能挖的是“石头”啊。这个说法的意思是，数据挖掘仅仅是一种思考方式，告诉我们应该尝试从数据中挖掘出知识，但不是每个数据都能挖掘出金子的，所以不要神话它。一个系统绝对不会因为上了一个数据挖掘模块就变得无所不能(这是IBM最喜欢吹嘘的)，恰恰相反，一个拥有数据挖掘思维的人员才是关键，而且他还必须对数据有深刻的认识，这样才可能从数据中导出模式指引业务的改善。大部分数据挖掘中的算法是机器学习的算法在数据库中的优化。\n\n统计学习\n统计学习近似等于机器学习。统计学习是个与机器学习高度重叠的学科。因为机器学习中的大多数方法来自统计学，甚至可以认为，统计学的发展促进机器学习的繁荣昌盛。例如著名的支持向量机算法，就是源自统计学科。但是在某种程度上两者是有分别的，这个分别在于：统计学习者重点关注的是统计模型的发展与优化，偏数学，而机器学习者更关注的是能够解决问题，偏实践，因此机器学习研究者会重点研究学习算法在计算机上执行的效率与准确性的提升。\n\n计算机视觉\n计算机视觉=图像处理+机器学习。图像处理技术用于将图像处理为适合进入机器学习模型中的输入，机器学习则负责从图像中识别出相关的模式。计算机视觉相关的应用非常的多，例如百度识图、手写字符识别、车牌识别等等应用。这个领域是应用前景非常火热的，同时也是研究的热门方向。随着机器学习的新领域深度学习的发展，大大促进了计算机图像识别的效果，因此未来计算机视觉界的发展前景不可估量。\n\n语音识别\n语音识别=语音处理+机器学习。语音识别就是音频处理技术与机器学习的结合。语音识别技术一般不会单独使用，一般会结合自然语言处理的相关技术。目前的相关应用有苹果的语音助手siri等。\n\n自然语言处理\n自然语言处理=文本处理+机器学习。自然语言处理技术主要是让机器理解人类的语言的一门领域。在自然语言处理技术中，大量使用了编译原理相关的技术，例如词法分析，语法分析等等，除此之外，在理解这个层面，则使用了语义理解，机器学习等技术。作为唯一由人类自身创造的符号，自然语言处理一直是机器学习界不断研究的方向。按照百度机器学习专家余凯的说法“听与看，说白了就是阿猫和阿狗都会的，而只有语言才是人类独有的”。如何利用机器学习技术进行自然语言的的深度理解，一直是工业和学术界关注的焦点。\n\n可以看出机器学习在众多领域的外延和应用。机器学习技术的发展促使了很多智能领域的进步，改善着我们的生活。\n4.机器学习的方法\n通过上节的介绍我们知晓了机器学习的大致范围，那么机器学习里面究竟有多少经典的算法呢？在这个部分我会简要介绍一下机器学习中的经典代表方法。这部分介绍的重点是这些方法内涵的思想，数学与实践细节不会在这讨论。\n\n1、回归算法\n在大部分机器学习课程中，回归算法都是介绍的第一个算法。原因有两个：一.回归算法比较简单，介绍它可以让人平滑地从统计学迁移到机器学习中。二.回归算法是后面若干强大算法的基石，如果不理解回归算法，无法学习那些强大的算法。回归算法有两个重要的子类：即线性回归和逻辑回归。\n线性回归就是我们前面说过的房价求解问题。如何拟合出一条直线最佳匹配我所有的数据？一般使用“最小二乘法”来求解。“最小二乘法”的思想是这样的，假设我们拟合出的直线代表数据的真实值，而观测到的数据代表拥有误差的值。为了尽可能减小误差的影响，需要求解一条直线使所有误差的平方和最小。最小二乘法将最优问题转化为求函数极值问题。函数极值在数学上我们一般会采用求导数为0的方法。但这种做法并不适合计算机，可能求解不出来，也可能计算量太大。\n计算机科学界专门有一个学科叫“数值计算”，专门用来提升计算机进行各类计算时的准确性和效率问题。例如，著名的“梯度下降”以及“牛顿法”就是数值计算中的经典算法，也非常适合来处理求解函数极值的问题。梯度下降法是解决回归模型中最简单且有效的方法之一。从严格意义上来说，由于后文中的神经网络和推荐算法中都有线性回归的因子，因此梯度下降法在后面的算法实现中也有应用。\n\n逻辑回归是一种与线性回归非常类似的算法，但是，从本质上讲，线型回归处理的问题类型与逻辑回归不一致。线性回归处理的是数值问题，也就是最后预测出的结果是数字，例如房价。而逻辑回归属于分类算法，也就是说，逻辑回归预测结果是离散的分类，例如判断这封邮件是否是垃圾邮件，以及用户是否会点击此广告等等。\n实现方面的话，逻辑回归只是对对线性回归的计算结果加上了一个Sigmoid函数，将数值结果转化为了0到1之间的概率(Sigmoid函数的图像一般来说并不直观，你只需要理解对数值越大，函数越逼近1，数值越小，函数越逼近0)，接着我们根据这个概率可以做预测，例如概率大于0.5，则这封邮件就是垃圾邮件，或者肿瘤是否是恶性的等等。从直观上来说，逻辑回归是画出了一条分类线，见下图。\n\n图7 逻辑回归的直观解释\n假设我们有一组肿瘤患者的数据，这些患者的肿瘤中有些是良性的(图中的蓝色点)，有些是恶性的(图中的红色点)。这里肿瘤的红蓝色可以被称作数据的“标签”。同时每个数据包括两个“特征”：患者的年龄与肿瘤的大小。我们将这两个特征与标签映射到这个二维空间上，形成了我上图的数据。\n\n当我有一个绿色的点时，我该判断这个肿瘤是恶性的还是良性的呢？根据红蓝点我们训练出了一个逻辑回归模型，也就是图中的分类线。这时，根据绿点出现在分类线的左侧，因此我们判断它的标签应该是红色，也就是说属于恶性肿瘤。\n\n逻辑回归算法划出的分类线基本都是线性的(也有划出非线性分类线的逻辑回归，不过那样的模型在处理数据量较大的时候效率会很低)，这意味着当两类之间的界线不是线性时，逻辑回归的表达能力就不足。下面的两个算法是机器学习界最强大且重要的算法，都可以拟合出非线性的分类线。\n\n2、神经网络\n神经网络(也称之为人工神经网络，ANN)算法是80年代机器学习界非常流行的算法，不过在90年代中途衰落。现在，携着“深度学习”之势，神经网络重装归来，重新成为最强大的机器学习算法之一。\n\n神经网络的诞生起源于对大脑工作机理的研究。早期生物界学者们使用神经网络来模拟大脑。机器学习的学者们使用神经网络进行机器学习的实验，发现在视觉与语音的识别上效果都相当好。在BP算法(加速神经网络训练过程的数值算法)诞生以后，神经网络的发展进入了一个热潮。BP算法的发明人之一是前面介绍的机器学习大牛Geoffrey Hinton(图1中的中间者)。\n\n具体说来，神经网络的学习机理是什么？简单来说，就是分解与整合。在著名的Hubel-Wiesel试验中，学者们研究猫的视觉分析机理是这样的。\n\n\n图8 Hubel-Wiesel试验与大脑视觉机理\n比方说，一个正方形，分解为四个折线进入视觉处理的下一层中。四个神经元分别处理一个折线。每个折线再继续被分解为两条直线，每条直线再被分解为黑白两个面。于是，一个复杂的图像变成了大量的细节进入神经元，神经元处理以后再进行整合，最后得出了看到的是正方形的结论。这就是大脑视觉识别的机理，也是神经网络工作的机理。\n\n让我们看一个简单的神经网络的逻辑架构。在这个网络中，分成输入层，隐藏层，和输出层。输入层负责接收信号，隐藏层负责对数据的分解与处理，最后的结果被整合到输出层。每层中的一个圆代表一个处理单元，可以认为是模拟了一个神经元，若干个处理单元组成了一个层，若干个层再组成了一个网络，也就是\"神经网络\"。\n\n图9 神经网络的逻辑架构\n\n在神经网络中，每个处理单元事实上就是一个逻辑回归模型，逻辑回归模型接收上层的输入，把模型的预测结果作为输出传输到下一个层次。通过这样的过程，神经网络可以完成非常复杂的非线性分类。\n下图会演示神经网络在图像识别领域的一个著名应用，这个程序叫做LeNet，是一个基于多个隐层构建的神经网络。通过LeNet可以识别多种手写数字，并且达到很高的识别精度与拥有较好的鲁棒性。\n图10 LeNet的效果展示\n右下方的方形中显示的是输入计算机的图像，方形上方的红色字样“answer”后面显示的是计算机的输出。左边的三条竖直的图像列显示的是神经网络中三个隐藏层的输出，可以看出，随着层次的不断深入，越深的层次处理的细节越低，例如层3基本处理的都已经是线的细节了。LeNet的发明人就是前文介绍过的机器学习的大牛Yann LeCun(图1右者)。\n进入90年代，神经网络的发展进入了一个瓶颈期。其主要原因是尽管有BP算法的加速，神经网络的训练过程仍然很困难。因此90年代后期支持向量机(SVM)算法取代了神经网络的地位。\n\n3、SVM（支持向量机）\n支持向量机算法是诞生于统计学习界，同时在机器学习界大放光彩的经典算法。\n支持向量机算法从某种意义上来说是逻辑回归算法的强化：通过给予逻辑回归算法更严格的优化条件，支持向量机算法可以获得比逻辑回归更好的分类界线。但是如果没有某类函数技术，则支持向量机算法最多算是一种更好的线性分类技术。\n但是，通过跟高斯“核”的结合，支持向量机可以表达出非常复杂的分类界线，从而达成很好的的分类效果。“核”事实上就是一种特殊的函数，最典型的特征就是可以将低维的空间映射到高维的空间。\n例如下图所示：\n图11 支持向量机图例\n\n\n我们如何在二维平面划分出一个圆形的分类界线？在二维平面可能会很困难，但是通过“核”可以将二维空间映射到三维空间，然后使用一个线性平面就可以达成类似效果。也就是说，二维平面划分出的非线性分类界线可以等价于三维平面的线性分类界线。于是，我们可以通过在三维空间中进行简单的线性划分就可以达到在二维平面中的非线性划分效果。\n\n图12 三维空间的切割\n\n\n支持向量机是一种数学成分很浓的机器学习算法（相对的，神经网络则有生物科学成分）。在算法的核心步骤中，有一步证明，即将数据从低维映射到高维不会带来最后计算复杂性的提升。于是，通过支持向量机算法，既可以保持计算效率，又可以获得非常好的分类效果。因此支持向量机在90年代后期一直占据着机器学习中最核心的地位，基本取代了神经网络算法。直到现在神经网络借着深度学习重新兴起，两者之间才又发生了微妙的平衡转变。\n\n4、聚类算法\n前面的算法中的一个显著特征就是我的训练数据中包含了标签，训练出的模型可以对其他未知数据预测标签。在下面的算法中，训练数据都是不含标签的，而算法的目的则是通过训练，推测出这些数据的标签。这类算法有一个统称，即无监督算法(前面有标签的数据的算法则是有监督算法)。无监督算法中最典型的代表就是聚类算法。\n\n让我们还是拿一个二维的数据来说，某一个数据包含两个特征。我希望通过聚类算法，给他们中不同的种类打上标签，我该怎么做呢？简单来说，聚类算法就是计算种群中的距离，根据距离的远近将数据划分为多个族群。\n\n聚类算法中最典型的代表就是K-Means算法。\n5、降维算法\n降维算法也是一种无监督学习算法，其主要特征是将数据从高维降低到低维层次。在这里，维度其实表示的是数据的特征量的大小，例如，房价包含房子的长、宽、面积与房间数量四个特征，也就是维度为4维的数据。可以看出来，长与宽事实上与面积表示的信息重叠了，例如面积=长 × 宽。通过降维算法我们就可以去除冗余信息，将特征减少为面积与房间数量两个特征，即从4维的数据压缩到2维。于是我们将数据从高维降低到低维，不仅利于表示，同时在计算上也能带来加速。\n\n刚才说的降维过程中减少的维度属于肉眼可视的层次，同时压缩也不会带来信息的损失(因为信息冗余了)。如果肉眼不可视，或者没有冗余的特征，降维算法也能工作，不过这样会带来一些信息的损失。但是，降维算法可以从数学上证明，从高维压缩到的低维中最大程度地保留了数据的信息。因此，使用降维算法仍然有很多的好处。\n\n降维算法的主要作用是压缩数据与提升机器学习其他算法的效率。通过降维算法，可以将具有几千个特征的数据压缩至若干个特征。另外，降维算法的另一个好处是数据的可视化，例如将5维的数据压缩至2维，然后可以用二维平面来可视。降维算法的主要代表是PCA算法(即主成分分析算法)。\n\n6、推荐算法\n推荐算法是目前业界非常火的一种算法，在电商界，如亚马逊，天猫，京东等得到了广泛的运用。推荐算法的主要特征就是可以自动向用户推荐他们最感兴趣的东西，从而增加购买率，提升效益。推荐算法有两个主要的类别：\n一类是基于物品内容的推荐，是将与用户购买的内容近似的物品推荐给用户，这样的前提是每个物品都得有若干个标签，因此才可以找出与用户购买物品类似的物品，这样推荐的好处是关联程度较大，但是由于每个物品都需要贴标签，因此工作量较大。\n另一类是基于用户相似度的推荐，则是将与目标用户兴趣相同的其他用户购买的东西推荐给目标用户，例如小A历史上买了物品B和C，经过算法分析，发现另一个与小A近似的用户小D购买了物品E，于是将物品E推荐给小A。\n两类推荐都有各自的优缺点，在一般的电商应用中，一般是两类混合使用。推荐算法中最有名的算法就是协同过滤算法。\n\n7、其他\n除了以上算法之外，机器学习界还有其他的如高斯判别，朴素贝叶斯，决策树等等算法。但是上面列的六个算法是使用最多，影响最广，种类最全的典型。机器学习界的一个特色就是算法众多，发展百花齐放。\n\n下面做一个总结，按照训练的数据有无标签，可以将上面算法分为监督学习算法和无监督学习算法，但推荐算法较为特殊，既不属于监督学习，也不属于非监督学习，是单独的一类。\n监督学习算法：\n线性回归，逻辑回归，神经网络，SVM\n\n无监督学习算法：\n聚类算法，降维算法\n\n特殊算法：\n推荐算法\n\n除了这些算法以外，有一些算法的名字在机器学习领域中也经常出现。但他们本身并不算是一个机器学习算法，而是为了解决某个子问题而诞生的。你可以理解他们为以上算法的子算法，用于大幅度提高训练过程。其中的代表有：梯度下降法，主要运用在线型回归，逻辑回归，神经网络，推荐算法中；牛顿法，主要运用在线型回归中；BP算法，主要运用在神经网络中；SMO算法，主要运用在SVM中。\n\n5.机器学习的应用--大数据\n\n说完机器学习的方法，下面要谈一谈机器学习的应用了。无疑，在2010年以前，机器学习的应用在某些特定领域发挥了巨大的作用，如车牌识别，网络攻击防范，手写字符识别等等。但是，从2010年以后，随着大数据概念的兴起，机器学习大量的应用都与大数据高度耦合，几乎可以认为大数据是机器学习应用的最佳场景。\n\n譬如，但凡你能找到的介绍大数据魔力的文章，都会说大数据如何准确准确预测到了某些事。例如经典的Google利用大数据预测了H1N1在美国某小镇的爆发。\n图13 Google成功预测H1N1\n\n百度预测2014年世界杯，从淘汰赛到决赛全部预测正确。\n\n\n图14 百度世界杯成功预测了所有比赛结果\n这些实在太神奇了，那么究竟是什么原因导致大数据具有这些魔力的呢？简单来说，就是机器学习技术。正是基于机器学习技术的应用，数据才能发挥其魔力。\n大数据的核心是利用数据的价值，机器学习是利用数据价值的关键技术，对于大数据而言，机器学习是不可或缺的。相反，对于机器学习而言，越多的数据会越 可能提升模型的精确性，同时，复杂的机器学习算法的计算时间也迫切需要分布式计算与内存计算这样的关键技术。因此，机器学习的兴盛也离不开大数据的帮助。 大数据与机器学习两者是互相促进，相依相存的关系。\n机器学习与大数据紧密联系。但是，必须清醒的认识到，大数据并不等同于机器学习，同理，机器学习也不等同于大数据。大数据中包含有分布式计算，内存数据库，多维分析等等多种技术。单从分析方法来看，大数据也包含以下四种分析方法：\n\n1.大数据，小分析：即数据仓库领域的OLAP分析思路，也就是多维分析思想。\n2.大数据，大分析：这个代表的就是数据挖掘与机器学习分析法。\n3.流式分析：这个主要指的是事件驱动架构。\n4.查询分析：经典代表是NoSQL数据库。\n\n也就是说，机器学习仅仅是大数据分析中的一种而已。尽管机器学习的一些结果具有很大的魔力，在某种场合下是大数据价值最好的说明。但这并不代表机器学习是大数据下的唯一的分析方法。\n\n机器学习与大数据的结合产生了巨大的价值。基于机器学习技术的发展，数据能够“预测”。对人类而言，积累的经验越丰富，阅历也广泛，对未来的判断越准确。例如常说的“经验丰富”的人比“初出茅庐”的小伙子更有工作上的优势，就在于经验丰富的人获得的规律比他人更准确。而在机器学习领域，根据著名的一个实验，有效的证实了机器学习界一个理论：即机器学习模型的数据越多，机器学习的预测的效率就越好。见下图：\n图15 机器学习准确率与数据的关系\n通过这张图可以看出，各种不同算法在输入的数据量达到一定级数后，都有相近的高准确度。于是诞生了机器学习界的名言：成功的机器学习应用不是拥有最好的算法，而是拥有最多的数据！\n\n在大数据的时代，有好多优势促使机器学习能够应用更广泛。例如随着物联网和移动设备的发展，我们拥有的数据越来越多，种类也包括图片、文本、视频等非结构化数据，这使得机器学习模型可以获得越来越多的数据。同时大数据技术中的分布式计算Map-Reduce使得机器学习的速度越来越快，可以更方便的使用。种种优势使得在大数据时代，机器学习的优势可以得到最佳的发挥。\n\n\n6.机器学习的子类--深度学习\n\n近来，机器学习的发展产生了一个新的方向，即“深度学习”。\n\n虽然深度学习这四字听起来颇为高大上，但其理念却非常简单，就是传统的神经网络发展到了多隐藏层的情况。\n\n在上文介绍过，自从90年代以后，神经网络已经消寂了一段时间。但是BP算法的发明人Geoffrey Hinton一直没有放弃对神经网络的研究。由于神经网络在隐藏层扩大到两个以上，其训练速度就会非常慢，因此实用性一直低于支持向量机。2006年，Geoffrey Hinton在科学杂志《Science》上发表了一篇文章，论证了两个观点：\n1.多隐层的神经网络具有优异的特征学习能力，学习得到的特征对数据有更本质的刻画，从而有利于可视化或分类；\n2.深度神经网络在训练上的难度，可以通过“逐层初始化” 来有效克服。\n\n\n图16 Geoffrey Hinton与他的学生在Science上发表文章\n\n通过这样的发现，不仅解决了神经网络在计算上的难度，同时也说明了深层神经网络在学习上的优异性。从此，神经网络重新成为了机器学习界中的主流强大学习技术。同时，具有多个隐藏层的神经网络被称为深度神经网络，基于深度神经网络的学习研究称之为深度学习。\n\n由于深度学习的重要性质，在各方面都取得极大的关注，按照时间轴排序，有以下四个标志性事件值得一说：\n\n2012年6月，《纽约时报》披露了Google Brain项目，这个项目是由Andrew Ng和Map-Reduce发明人Jeff Dean共同主导，用16000个CPU Core的并行计算平台训练一种称为“深层神经网络”的机器学习模型，在语音识别和图像识别等领域获得了巨大的成功。Andrew Ng就是文章开始所介绍的机器学习的大牛(图1中左者)。\n2012年11月，微软在中国天津的一次活动上公开演示了一个全自动的同声传译系统，讲演者用英文演讲，后台的计算机一气呵成自动完成语音识别、英中机器翻译，以及中文语音合成，效果非常流畅，其中支撑的关键技术是深度学习；\n2013年1月，在百度的年会上，创始人兼CEO李彦宏高调宣布要成立百度研究院，其中第一个重点方向就是深度学习，并为此而成立深度学习研究院(IDL)。\n2013年4月，《麻省理工学院技术评论》杂志将深度学习列为2013年十大突破性技术(Breakthrough Technology)之首。\n\n\n\n图17 深度学习的发展热潮\n文章开头所列的三位机器学习的大牛，不仅都是机器学习界的专家，更是深度学习研究领域的先驱。因此，使他们担任各个大型互联网公司技术掌舵者的原因不仅在于他们的技术实力，更在于他们研究的领域是前景无限的深度学习技术。\n\n目前业界许多的图像识别技术与语音识别技术的进步都源于深度学习的发展，除了本文开头所提的Cortana等语音助手，还包括一些图像识别应用，其中典型的代表就是下图的百度识图功能。\n图18 百度识图\n深度学习属于机器学习的子类。基于深度学习的发展极大的促进了机器学习的地位提高，更进一步地，推动了业界对机器学习父类人工智能梦想的再次重视。\n7.机器学习的父类--人工智能\n人工智能是机器学习的父类。深度学习则是机器学习的子类。如果把三者的关系用图来表明的话，则是下图：\n\n图19 深度学习、机器学习、人工智能三者关系\n毫无疑问，人工智能(AI)是人类所能想象的科技界最突破性的发明了，某种意义上来说，人工智能就像游戏最终幻想的名字一样，是人类对于科技界的最终梦想。从50年代提出人工智能的理念以后，科技界，产业界不断在探索，研究。这段时间各种小说、电影都在以各种方式展现对于人工智能的想象。人类可以发明类似于人类的机器，这是多么伟大的一种理念！但事实上，自从50年代以后，人工智能的发展就磕磕碰碰，未有见到足够震撼的科学技术的进步。\n\n总结起来，人工智能的发展经历了如下若干阶段，从早期的逻辑推理，到中期的专家系统，这些科研进步确实使我们离机器的智能有点接近了，但还有一大段距离。直到机器学习诞生以后，人工智能界感觉终于找对了方向。基于机器学习的图像识别和语音识别在某些垂直领域达到了跟人相媲美的程度。机器学习使人类第一次如此接近人工智能的梦想。\n事实上，如果我们把人工智能相关的技术以及其他业界的技术做一个类比，就可以发现机器学习在人工智能中的重要地位不是没有理由的。\n人类区别于其他物体，植物，动物的最主要区别，作者认为是“智慧”。而智慧的最佳体现是什么？\n是计算能力么，应该不是，心算速度快的人我们一般称之为天才。\n是反应能力么，也不是，反应快的人我们称之为灵敏。\n是记忆能力么，也不是，记忆好的人我们一般称之为过目不忘。\n是推理能力么，这样的人我也许会称他智力很高，类似“福尔摩斯”，但不会称他拥有智慧。\n是知识能力么，这样的人我们称之为博闻广，也不会称他拥有智慧。\n想想看我们一般形容谁有大智慧？圣人，诸如庄子，老子等。智慧是对生活的感悟，是对人生的积淀与思考，这与我们机器学习的思想何其相似？通过经验获取规律，指导人生与未来。没有经验就没有智慧。\n图20 机器学习与智慧\n那么，从计算机来看，以上的种种能力都有种种技术去应对。\n例如计算能力我们有分布式计算，反应能力我们有事件驱动架构，检索能力我们有搜索引擎，知识存储能力我们有数据仓库，逻辑推理能力我们有专家系统，但是，唯有对应智慧中最显著特征的归纳与感悟能力，只有机器学习与之对应。这也是机器学习能力最能表征智慧的根本原因。\n\n让我们再看一下机器人的制造，在我们具有了强大的计算，海量的存储，快速的检索，迅速的反应，优秀的逻辑推理后我们如果再配合上一个强大的智慧大脑，一个真正意义上的人工智能也许就会诞生，这也是为什么说在机器学习快速发展的现在，人工智能可能不再是梦想的原因。\n\n人工智能的发展可能不仅取决于机器学习，更取决于前面所介绍的深度学习，深度学习技术由于深度模拟了人类大脑的构成，在视觉识别与语音识别上显著性的突破了原有机器学习技术的界限，因此极有可能是真正实现人工智能梦想的关键技术。无论是谷歌大脑还是百度大脑，都是通过海量层次的深度学习网络所构成的。也许借助于深度学习技术，在不远的将来，一个具有人类智能的计算机真的有可能实现。\n最后再说一下题外话，由于人工智能借助于深度学习技术的快速发展，已经在某些地方引起了传统技术界达人的担忧。真实世界的“钢铁侠”，特斯拉CEO马斯克就是其中之一。最近马斯克在参加MIT讨论会时，就表达了对于人工智能的担忧。“人工智能的研究就类似于召唤恶魔，我们必须在某些地方加强注意。”\n图21 马斯克与人工智能\n尽管马斯克的担心有些危言耸听，但是马斯克的推理不无道理。“如果人工智能想要消除垃圾邮件的话，可能它最后的决定就是消灭人类。”马斯克认为预防此类现象的方法是引入政府的监管。在这里作者的观点与马斯克类似，在人工智能诞生之初就给其加上若干规则限制可能有效，也就是不应该使用单纯的机器学习，而应该是机器学习与规则引擎等系统的综合能够较好的解决这类问题。因为如果学习没有限制，极有可能进入某个误区，必须要加上某些引导。正如人类社会中，法律就是一个最好的规则，杀人者死就是对于人类在探索提高生产力时不可逾越的界限。\n在这里，必须提一下这里的规则与机器学习引出的规律的不同，规律不是一个严格意义的准则，其代表的更多是概率上的指导，而规则则是神圣不可侵犯，不可修改的。规律可以调整，但规则是不能改变的。有效的结合规律与规则的特点，可以引导出一个合理的，可控的学习型人工智能。\n8.机器学习的思考--计算机的潜意识\n\n最后，作者想谈一谈关于机器学习的一些思考。主要是作者在日常生活总结出来的一些感悟。\n\n回想一下我在节1里所说的故事，我把小Y过往跟我相约的经历做了一个罗列。但是这种罗列以往所有经历的方法只有少数人会这么做，大部分的人采用的是更直接的方法，即利用直觉。那么，直觉是什么？其实直觉也是你在潜意识状态下思考经验后得出的规律。就像你通过机器学习算法，得到了一个模型，那么你下次只要直接使用就行了。那么这个规律你是什么时候思考的？可能是在你无意识的情况下，例如睡觉，走路等情况。这种时候，大脑其实也在默默地做一些你察觉不到的工作。\n这种直觉与潜意识，我把它与另一种人类思考经验的方式做了区分。如果一个人勤于思考，例如他会每天做一个小结，譬如“吾日三省吾身”，或者他经常与同伴讨论最近工作的得失，那么他这种训练模型的方式是直接的，明意识的思考与归纳。这样的效果很好，记忆性强，并且更能得出有效反应现实的规律。但是大部分的人可能很少做这样的总结，那么他们得出生活中规律的方法使用的就是潜意识法。\n举一个作者本人关于潜意识的例子。作者本人以前没开过车，最近一段时间买了车后，天天开车上班。我每天都走固定的路线。有趣的是，在一开始的几天，我非常紧张的注意着前方的路况，而现在我已经在无意识中就把车开到了目标。这个过程中我的眼睛是注视着前方的，我的大脑是没有思考，但是我手握着的方向盘会自动的调整方向。也就是说。随着我开车次数的增多，我已经把我开车的动作交给了潜意识。这是非常有趣的一件事。在这段过程中，我的大脑将前方路况的图像记录了下来，同时大脑也记忆了我转动方向盘的动作。经过大脑自己的潜意识思考，最后生成的潜意识可以直接根据前方的图像调整我手的动作。假设我们将前方的录像交给计算机，然后让计算机记录与图像对应的驾驶员的动作。经过一段时间的学习，计算机生成的机器学习模型就可以进行自动驾驶了。这很神奇，不是么。其实包括Google、特斯拉在内的自动驾驶汽车技术的原理就是这样。\n\n除了自动驾驶汽车以外，潜意识的思想还可以扩展到人的交际。譬如说服别人，一个最佳的方法就是给他展示一些信息，然后让他自己去归纳得出我们想要的结论。这就好比在阐述一个观点时，用一个事实，或者一个故事，比大段的道理要好很多。古往今来，但凡优秀的说客，无不采用的是这种方法。春秋战国时期，各国合纵连横，经常有各种说客去跟一国之君交流，直接告诉君主该做什么，无异于自寻死路，但是跟君主讲故事，通过这些故事让君主恍然大悟，就是一种正确的过程。这里面有许多杰出的代表，如墨子，苏秦等等。\n\n基本上所有的交流过程，使用故事说明的效果都要远胜于阐述道义之类的效果好很多。为什么用故事的方法比道理或者其他的方法好很多，这是因为在人成长的过程，经过自己的思考，已经形成了很多规律与潜意识。如果你告诉的规律与对方的不相符，很有可能出于保护，他们会本能的拒绝你的新规律，但是如果你跟他讲一个故事，传递一些信息，输送一些数据给他，他会思考并自我改变。他的思考过程实际上就是机器学习的过程，他把新的数据纳入到他的旧有的记忆与数据中，经过重新训练。如果你给出的数据的信息量非常大，大到调整了他的模型，那么他就会按照你希望的规律去做事。有的时候，他会本能的拒绝执行这个思考过程，但是数据一旦输入，无论他希望与否，他的大脑都会在潜意识状态下思考，并且可能改变他的看法。\n\n如果计算机也拥有潜意识(正如本博客的名称一样)，那么会怎么样？譬如让计算机在工作的过程中，逐渐产生了自身的潜意识，于是甚至可以在你不需要告诉它做什么时它就会完成那件事。这是个非常有意思的设想，这里留给各位读者去发散思考吧。\n\n9.总结\n\n本文首先介绍了互联网界与机器学习大牛结合的趋势，以及使用机器学习的相关应用，接着以一个“等人故事”展开对机器学习的介绍。介绍中首先是机器学习的概念与定义，然后是机器学习的相关学科，机器学习中包含的各类学习算法，接着介绍机器学习与大数据的关系，机器学习的新子类深度学习，最后探讨了一下机器学习与人工智能发展的联系以及机器学习与潜意识的关联。经过本文的介绍，相信大家对机器学习技术有一定的了解，例如机器学习是什么，它的内核思想是什么(即统计和归纳)，通过了解机器学习与人类思考的近似联系可以知晓机器学习为什么具有智慧能力的原因等等。其次，本文漫谈了机器学习与外延学科的关系，机器学习与大数据相互促进相得益彰的联系，机器学习界最新的深度学习的迅猛发展，以及对于人类基于机器学习开发智能机器人的一种展望与思考，最后作者简单谈了一点关于让计算机拥有潜意识的设想。\n\n机器学习是目前业界最为Amazing与火热的一项技术，从网上的每一次淘宝的购买东西，到自动驾驶汽车技术，以及网络攻击抵御系统等等，都有机器学习的因子在内，同时机器学习也是最有可能使人类完成AI dream的一项技术，各种人工智能目前的应用，如微软小冰聊天机器人，到计算机视觉技术的进步，都有机器学习努力的成分。作为一名当代的计算机领域的开发或管理人员，以及身处这个世界，使用者IT技术带来便利的人们，最好都应该了解一些机器学习的相关知识与概念，因为这可以帮你更好的理解为你带来莫大便利技术的背后原理，以及让你更好的理解当代科技的进程。\n\n10.后记\n\n这篇文档花了作者两个月的时间，终于在2014年的最后一天的前一天基本完成。通过这篇文章，作者希望对机器学习在国内的普及做一点贡献，同时也是作者本人自己对于所学机器学习知识的一个融汇贯通，整体归纳的提高过程。作者把这么多的知识经过自己的大脑思考，训练出了一个模型，形成了这篇文档，可以说这也是一种机器学习的过程吧(笑)。\n作者所在的行业会接触到大量的数据，因此对于数据的处理和分析是平常非常重要的工作，机器学习课程的思想和理念对于作者日常的工作指引作用极大，几乎导致了作者对于数据价值的重新认识。想想半年前，作者还对机器学习似懂非懂，如今也可以算是一个机器学习的Expert了(笑)。但作者始终认为，机器学习的真正应用不是通过概念或者思想的方式，而是通过实践。只有当把机器学习技术真正应用时，才可算是对机器学习的理解进入了一个层次。正所谓再“阳春白雪”的技术，也必须落到“下里巴人”的场景下运用。目前有一种风气，国内外研究机器学习的某些学者，有一种高贵的逼格，认为自己的研究是普通人无法理解的，但是这样的理念是根本错误的，没有在真正实际的地方发挥作用，凭什么证明你的研究有所价值呢？作者认为必须将高大上的技术用在改变普通人的生活上，才能发挥其根本的价值。一些简单的场景，恰恰是实践机器学习技术的最好地方。\n\n最后，作者很感谢能够阅读到这里的读者。如果看完觉得好的话，还请轻轻点一下赞，你们的鼓励就是作者继续行文的动力。\n对EasyPR做下说明：EasyPR，一个开源的中文车牌识别系统，代码托管在github。其次，在前面的博客文章中，包含EasyPR至今的开发文档与介绍。在后续的文章中，作者会介绍EasyPR中基于机器学习技术SVM的应用即车牌判别模块的核心内容，欢迎继续阅读。\n\n\n版权说明：\n本文中的所有文字，图片，代码的版权都是属于作者和博客园共同所有。欢迎转载，但是务必注明作者与出处。任何未经允许的剽窃以及爬虫抓取都属于侵权，作者和博客园保留所有权利。\n转载自http://www.cnblogs.com/subconscious/p/4107357.html\n参考文献：\n1.Andrew Ng Courera Machine Learning\n2.LeNet Homepage\n\n3.pluskid svm","data":"2018年09月21日 14:08:47"}
{"_id":{"$oid":"5d343af062f717dc0659b28d"},"title":"自然语言处理与我的计算机专业的联系","author":"qq_44929755","content":"我是一名计算机学院的大二学生，本学期选修了大数据与人工智能这门课，通过一学期的学习，让我对大数据与人工智能有了更多的了解，也让我深深地感受到人工智能在当今社会的广泛应用，技术正在快速崛起，不断延伸到各个行业当中。人工智能发展到现在以及拥有很多的分支，包括机器学习，神经网络，自然语言处理，深度学习等等，所以接下来我想谈谈人工智能自然语言处理在计算机领域的应用。\n人工智能技术主要研究的目的是使一台计算机或者是一台机器完成一些需要我们人类亲自动手或者动脑来完成的工作。因此，人工智能的发展历史和计算机科学与技术的发展历史密不可分。自然语言处理（NLP）是人工智能的一个分支，用于分析、理解和生成自然语言，以方便人和计算机设备进行交流，以及人与人之间的交流。NLP是计算机科学，人工智能，语言学关注计算机和人类（自然）语言之间相互作用的领域。因此，自然语言处理是与人机交互的领域有关的。那么NLP和我的专业又有哪些联系呢?\n1.帮助我完成学习任务。前段时间和同学要完成一个班的小论文相似度的分析，这里面就需要运用NLP的相关知识，我们首先把每篇文章进行分词等一系列操作，最后再进行文本聚类得到结果，分词就是nlp的一个应用。除此之外，只要我一直在计算机领域学习下去，那么用到自然语言处理技术的地方也会越来越多，像数据挖掘，深度学习等等，NLP是一种处理数据的手段，它让我们看到得到更直观的结果。\n2.找工作。随着大数据的发展，人工智能得到了全新的发展机遇，人工智能技术越来越多的应用到我们的工作和日常生活中，人工智能相关职业的发展前景还是非常值得期待的，相关的产业也会得到进一步的发展，相关的人才需求会得到进一步的释放，所以从事人工智能的相关工作是一个不错的选择。但是，不管从事什么行业，打铁还需自身硬，现在我的主要任务就是学习，不断充实自己。\n尽管NLP不如大数据，机器学习听起来那么火，但实际上我们每天都在使用它。文本语音翻译，信息检索，家里的聊天机器人等等。现在人工智能领域应用广泛，我们也要把握时代，不断前进。","data":"2019年06月20日 17:09:29"}
{"_id":{"$oid":"5d343af162f717dc0659b28f"},"title":"人工智能-语音交互-NLP自然语言(一) 词法分析","author":"杨易","content":"NLP自然语言处理(一) 词法分析\n\n\n1.什么是自然语言处理NLP(natural language processing)?\n\n\n如图所示: 我希望计算机对房间的评论结果是,可以欣赏日出。\n\n\n\n\n因此我理解的NLP实际上就是让计算机和人一样理解语言.\n\n\n\n\n2.词法分析：\n法分析向用户提供分词、词性标注、命名实体识别三大功能。该服务能够识别出文本串中的基本词汇（分词），对这些词汇进行重组、标注组合后词汇的词性，并进一步识别出命名实体.\n1)分词 中文分词是将连续的自然语言文本，切分成具有语义合理性和完整性的词汇序列的过程。\n\n\n\n\n致毕业和尚未毕业的同学,我们更希望计算机得到的结果是\"和\"与\"尚未\"分开查询,而不是\"和尚\"与\"未\"分开查询.\n2)词性标注 词性标注（Part-of-Speech tagging 或POS tagging）是指为自然语言文本中的每个词汇赋予一个词性 的过程。\n\n\n\n\n3)命名实体 命名实体识别（Named Entity Recognition 简称NER），又称“专名识别”，是指识别自然语言文本中具有特定意义的实体，主要包括人名、地名、机构名、时间日期等。","data":"2017年10月27日 22:36:57"}
{"_id":{"$oid":"5d343af162f717dc0659b291"},"title":"自然语言处理、人工智能、机器学习、深度学习和神经网络之间的介绍","author":"duozhishidai","content":"人工智能：建立能智能化处理事物的系统。\n自然语言处理：建立能够理解语言的系统，人工智能的一个分支。\n机器学习：建立能从经验中进行学习的系统，也是人工智能的一个分支。\n神经网络：生物学启发出的人工神经元网络。\n深度学习：在大型数据集上，建立使用深度神经网络的系统，机器学习的一个分支。\n1.如何快速入门NLP自然语言处理概述\nhttp://www.duozhishidai.com/article-11742-1.html\n2.人工智能，机器学习和深度学习之间，主要有什么差异？\nhttp://www.duozhishidai.com/article-15858-1.html\n3.人工智能、机器学习、数据挖掘以及数据分析有什么联系？\nhttp://www.duozhishidai.com/article-13135-1.html","data":"2018年12月12日 20:33:24"}
{"_id":{"$oid":"5d343af162f717dc0659b293"},"title":"30行代码消费腾讯人工智能开放平台提供的自然语言处理API","author":"weixin_34248487","content":"腾讯人工智能AI开放平台上提供了很多免费的人工智能API，开发人员只需要一个QQ号就可以登录进去使用。\n腾讯人工智能AI开放平台的地址：https://ai.qq.com/\n里面的好东西很多，以自然语言处理的人工智能API为例。\n假设我们有一个句子：腾讯AI人工智能开放平台。我们希望用腾讯的人工智能开放平台里提供的自然语言处理API对这个句子进行智能分词。\n用您的QQ号登录腾讯人工智能开放平台，创建一个新的应用：\nhttps://ai.qq.com/\n根据您的实际需要选择自然语言处理的具体类别：\n文本朗读（Text to speech）/语音合成（Speech synthesis）\n语音识别（Speech recognition）\n中文自动分词（Chinese word segmentation）\n词性标注（Part-of-speech tagging）\n句法分析（Parsing）\n自然语言生成（Natural language generation）\n文本分类（Text categorization）\n信息检索（Information retrieval）\n信息抽取（Information extraction）\n文字校对（Text-proofing）\n问答系统（Question answering）\n机器翻译（Machine translation）\n自动摘要（Automatic summarization）\n文字蕴涵（Textual entailment）\n创建应用之后生成的app id和app key要记下来，在代码里要使用。\n新建一个js文件，输入如下代码：\nvar md5 = require('md5'); var app_id = \"2107823355\"; var time_stamp = Date.now() / 1000; var nonce_str = Date.now(); var text = \"腾讯AI人工智能开放平台\"; var app_key = \"LHGNH0usjUTRRRSA\"; var input = \"app_id=\" + app_id + \"\u0026nonce_str=\" + nonce_str + \"\u0026text=\" + encodeURI(text) + \"\u0026time_stamp=\" + time_stamp + \"\u0026app_key=\" + app_key; var upper = md5(input).toUpperCase(); console.log(upper); input = input + \"\u0026sign=\" + upper; var request = require('request'); var oOptions = { url: \"https://api.ai.qq.com/fcgi-bin/nlp/nlp_wordseg\", method: \"POST\", headers: { \"content-type\": \"application/x-www-form-urlencoded\", }, body: input }; console.log(\"request sent: \" + oOptions.body); var action = new Promise(function(resolve,reject){ request(oOptions,function(error,response,body){ console.log(\"response: \" + body); }); // end of request });\n通过nodejs里的request组件, 使用HTTP POST调用https://api.ai.qq.com/fcgi-bin/nlp/nlp_wordseg去消费腾讯人工智能开放平台的自然语言处理的分词API：\n这些代码的详细解释，我已经在我之前的NLP版本里介绍过了：\n[](https://www.toutiao.com/i6588311167087673869/?group_id=6588311167087673869)\n使用命令行 node nlp.js即可消费该API并查看结果：\n要获取更多Jerry的原创技术文章，请关注公众号\"汪子熙\"或者扫描下面二维码:","data":"2018年09月28日 20:35:35"}
{"_id":{"$oid":"5d343af262f717dc0659b295"},"title":"Google BERT概览（一） -它解决了哪些问题？","author":"waterfeeling","content":"目录\nGoogle BERT自然语言处理框架\nGoogle BERT都能解决哪些问题\nGoogle BERT自然语言处理框架\n2018之秋，一篇《谷歌新发布的BERT模型突破11项纪录》的文章一出来，重燃大家对人工智能自然语言处理领域的热情。借此热点，在这里整理一下自然语言处理最新发展状况。首先需要注明的一点的是，严格意义上来说，BERT模型更加针对的是自然语言处理（NLP）中的自然语言理解（NLU）分支的问题，而自然语言处理（NLP）是一个更加宽泛的研究领域，包含更多的算法，模型和应用场景。\nGoogle BERT都能解决哪些问题\n此次所谓Google的BERT(Bidirectional Encoder Representations from Transformers)模型号称打破11项纪录，那么我们就顺藤摸瓜，看看能引起Google注意的这11项挑战，由此找到NLU研究领域大家所关注的重点。\n在《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》【原文地址】中，作者将BERT模型应用于以下实验中：\nGLUE (General Language Understanding Evaluation) 数据集【数据集地址】。如同GLUE的创建者在论文【论文地址】中介绍到的，GLUE实际上是为了能够标准化的和综合性的评估NLU（自然语言理解）算法和模型而构造出来的一套包含数据集，在线评估平台的工具，其中包含了以下九个数据集：\n单句分析类\nCoLA （Corpus of Linguistic Acceptability）【链接地址】 从23本语言学发表物中抽取的10657句话并就每句话是否从语法角度成立进行了标注。整个数据集提供了9594条句子作为训练集，以及1063条句子作为测试集。以下是该数据集中的一些样本条目。\n\nSST-2（Stanford Sentiment Treebank）【链接地址】摘取了电影评论并标注了是正面还是负面评论以用于情感分析。\n相似度分析和转述类\nMRPC（Microsoft Research Paraphrase Corpus）【链接地址】收录了5800对句子并标注每对句子是否在语义上等价。\nQQP（Quora Question Pairs）【链接地址】收集了Quora网站上的问题和答案，并就两个问题是否在语义上等价进行了标注。\nSTS-B（Semantic Textual Similarity Benchmark）【链接地址】收集了来自于图片注释，新闻头条，社区论坛等不同来源的8628对句子，并就每对句子的相似度给与1分到5分的相似度评分标注。\n推断类\nMNLI（Multi-Genre Natural Language Inference Corpus）【链接地址】包含了433k对句子，每对句子分别包含premise和hypothesis，并标注了两者间是entail（正向关联），controdict（相互矛盾），或是neutral（中立）的关系标签。\nQNLI（Question-answering NLI）是基于SQuAD（Stanford Question Answering Dataset）【链接地址】构建的数据集。此数据集中收集了问题和文字，并标注了文字中是否包含与问题匹配的答案。\nRTE（Recognizing Textual Entailment）来源于年度的RTE竞赛，RTE-1，RTE-2，RTE-3，RTE-5【链接地址】。数据集收集了句子对，每对句子分为premise和hypothesis并标注两者间是否为entailment关系。\nWNLI （Winograd NLI）是基于WSC（Winograd Schema Challenge）【链接地址】中的数据构建的数据集。原始数据是用来训练和测试阅读理解模型、算法的，每个句子中包含有一个代词，算法、模型需要根据上下文获知此代词指代的事句子中的哪个名词。GLUE的作者基于原数据集将数据改造成句子对的形式，一是保留原始句子，二是将原始句子中的代词用其具体指代的名词替换，并且针对每对句子标注出两者间是否为entailment关系。\nSQuAD（Standford Question Answering Dataset）【链接地址】为斯坦福大学构建的阅读理解数据集。数据集的第一个版本SQuAD1.1中【文章地址】提供了100k问题和回答配对。每个问题的答案包含于一段维基百科的文字。\n\n为了进一步接近现实的阅读理解场景，2018年发布的QUaAD 2.0【文章地址】中额外增加了50k条新增的问题，并且问题不一定有答案。这要求模型和算法不但能找出答案，并且在没有答案的时候能给出正确的判断，而非“凑”答案。而在Google公布其BERT算法性能的论文中，作者采用的仍然是SQuAD1.1版本的数据集。\nNER（Named Entity Recognition）【链接地址】包含200k单词并且每个被标注为：Person, Organization, Location, Miscellaneous, 或Other。\nSWAG（Situations With Adversarial Generations）【链接地址】包含113k完形填空的句子。","data":"2019年02月06日 21:24:01"}
{"_id":{"$oid":"5d343af262f717dc0659b297"},"title":"神经网络、自然语言处理将如何赋能人工智能+的发展","author":"stupid_shadiao","content":"摘要 在信息网络高速发展的时代，神经网络已经成为人工智能领域不可缺少的部分，神经网络的推广使得更多的人了解到人工智能，推动了控制理论的不断前进。以深度神经网络算法为例，推动识别技术新业务的功能突破，人脸识别率达到99.5%，助力人工智能的应用和发展。与此同时，在自然语言的处理研究过程中，多义词的辨识以及短文本的情感分析等方面的研究使得人工智能具有更强大脑。随着深度神经网络和自然语言处理等基础技术的进步，人工智能具有了更广的应用领域，将逐步进入到各行各业中，最终革新人们的生产和生活方式。\n关键字 神经网络、自然语言处理、人脸识别、多义词辨识、短文本情感分析、人工智能\n一、 神经网络在人工智能中的应用\n1、 典型的神经网络\n1.1感知机\n感知机可以称为第一代的神经网络，主要包括输入的多个特征单元（人工定义或程序寻找），中间由学习权重连接，最后由决策单元输出。典型的感知机单元遵循前馈模型，输入通过权重处理后直接连接到输出单元上。\n图表 1感知机\n感知机可以使用的前提是感知机已经训练完毕，训练是为了调整它的权值。\n1.2卷积神经网络\n在感知机和多层感知机的基础上，人们提出了一种新的网络结构——卷积神经网络。利用卷积神经网络可以对一些特征的检测进行共享，并在尺度和位置和方向上具有一定的不变性。1998年Yann LeCun提出的一个称为LeNet的网络进行手写字符识别获得了巨大的成功。下图是LeNet的主要结构：一个包括卷积、池化和全连接的六层网络结构。\n图表 2LeNet网络\n它利用反向传播算法来对隐藏层的单元权重进行训练，并在每个卷积层中实现了卷积操作的（卷积核）权值共享，并引入池化层实现了特征的缩聚，最后通过全连接层来实现输出。\n2012年ILSVRC比赛，由ImageNet提供了120万张的高清训练数据，目的是训练一个可以分类出图像属于一千类中每一类的概率的模型，并以此来进行图像的识别。Hinton的学生Alex Krizhevsky，在LeNet的基础上改进了神经网络，训练出了一个具有7个隐藏层深度网络，更深更强大的AlexNet，并引入了GPU进行并行训练，极大的提高了深度学习模型的训练效率。\n图表 3AlexNet网络\n1.3循环神经网络\n循环神经网络主要用于处理序列数据。在机器学习领域，序列模型一般利用序列数据作为输入，来训练序列模型用于预测序列数据的下一项。在循环神经网络之前主要使用无记忆模型处理这类任务。\n图表 4循环神经网络\n循环神经网络包含了两个重要的特点。首先拥有一系列隐含状态的分布可以高效的存储过去的信息；其次它具有非线性动力学可以允许它以复杂的方式更新隐藏状态。在足够的时间和神经元数量下，RNN甚至可以计算出计算机能计算的任何东西。它们甚至会表现出振动、牵引和混沌的行为。\n目前主要有四种有效的方式实现循环神经网络，主要包括长短时记忆（Long Short Term Memory），海森自由优化方法（Hessian Free Optimization)，回声状态网络（Echo State Networks）以及利用动量的初始化（Good initialization with momentum）\n2、 深度神经网络在人工智能应用中的识别过程\n深度神经网络广泛应用与语音识别，大数据分析，图像识别，行为识别等领域。在各自的领域中，如果一个深度神经网络识别过程示意图如下，那么我们称之为一个较好的深度神经网络\n图表 5一个好的深度神经网络识别过程\n在识别领域，深度神经网络发挥这极其重要的作用，它可以解决某些我们人类很难解决的识别问题，比如对于狗和狼大多数人都无法将他们正确辨认，但是深度神经网络可以通过训练机器，用标记好的图片训练它，让它学会分类。.对于给定的一张图神经网络做出基本型反应，进一步计算神经网络做复杂结构反应，近而神经网络做抽象概念反应，最终输出预测结果。识别狗和狼的过程如下图所示；\n图表 6神经网络识别狗\n3、 神经网络算法\n基于多层神经网络的新的深度学习算法。是一种新的算法和结构，新的网络结构中最著名的就是CNN，它解决了传统较深的网络参数太多，很难训练的问题，使用了“局部感受野”和“权植共享”的概念，大大减少了网络参数的数量。这种结构很符合视觉类任务在人脑上的工作原理。为了解决传统的多层神经网络的一些不足：梯度消失，过拟合等。产生了一些新的方法：新的激活函数：ReLU，新的权重初始化方法（逐层初始化，XAVIER等），新的损失函数，新的防止过拟合方法（Dropout, BN等）。\n神经网络解决了早期人工智能的一些遗留问题，在大数据和大算力的加持下，使得人工智能重新进入到大众的视野。广泛应用与视觉识别，图像识别，语音识别，棋类AI中，提升了人工智能的深度和广度\n二、 自然语言处理中的人工智能\n1、 自然语言处理的多义词辨识\n造成语言模糊性的一个重要原因是词的多义性，词义辨识不准确，使语法、语义的分析存在偏差。运用神经网络模型根据词的搭配关系、句内语法、语义约束、前后文的语义逻辑关系确定词义，实现机器翻译中多义词的正确辨识。例如我们通过网络模型根据文中关键词创建语义场，然后根据当前语义场对多义词进行解释，实现汉英翻译中多义词语义的正确辨识。\n正确辨识汉英翻译中的多义词语义的处理方法：我们只有词形信息的书面汉语文本提供给机器，这些词形信息隐含了语法关系、逻辑关系、修辞关系以及文本的主题和作者的写作风格等，算法的核心是如何利用这些信息。在机器翻译系统中，记录词到词义项映射关系的是双语语义词典知识库。在双语语义词典中一词形X有m个语义项，具体选择哪一个语义项由所处的语言环境决定。根据上下文很容易获取多义词的具体意义。从我们人脑处理语言的过程中可以总结出机器翻译的多义词辨识分为两个阶段：第一阶段是当前语义场的建立，假设语义场是由相继出现的一系列关键词共同创建。第二阶段是利用当前语义场决定词的词义。由此可见机器翻译过程是一个约束满足问题。在此基础上可建立约束满足的神经网络模型。\n模型学习的例句为\n1）汤挺热乎。\n2）他待人总是很热乎。\n在双语词典中热乎有两个词义，一个词义在“温度”场中解释为warm，另一个在“情感”场中解释为warm and friendly。在例子1中创建“温度”场的关键字是具有温度属性的汤，在例子2中创建“情感”场的是与情感有关的他、待人关键字。\n多义词识别不仅应用于机器翻译，还广泛应用与人机交互的语音识别系统，以及智能机器人。在一定成都上多义词识别问题的解决推动的人工智能在智能机器人的发展\n2、 自然语言处理的短文本情感分析\n在自然语言处理中的又一重要难题是能否正确表达出句子的情感，是自然语言处理关键性任务的前提基础。\n2.1情感分析\n情感分析（SA）又称为倾向性分析和意见挖掘，它是对带有情感色彩的主观性文本进行分析、处理、归纳和推理的过程，其中情感分析还可以细分为情感极性（倾向）分析，情感程度分析，主客观分析等。情感极性分析的目的是对文本进行褒义、贬义、中性的进行判断。情感程度分析主要是对同一情感极性中再进行划分或者细分，以描述该极性的强度。例如“喜爱”和“敬爱”都是褒义词，但是“敬爱”相对来说褒义的程度更加强烈一些。\n主客观分析主要目的是识别文本中哪些部分是客观称述而不带情感色彩，哪些是带有情感的主管描述。在对于微博，或者商品评论时，这个分析步骤一般都忽略，因为微博和商品评论本身就一定存在强烈的情感色彩，而且客观描述的情感值理论上是为零，不影响最终的情感分析结果。\n2.2情感分析的主要方法：\n现阶段主要的情感分析方法主要有两类：基于词典的方法和基于机器学习的方法\n基于词典的方法主要通过制定一系列的情感词典和规则，对文本进行段落拆借、句法分析，计算情感值，最后通过情感值来作为文本的情感倾向依据。\n基于机器学习的方法大多将这个问题转化为一个分类问题来看待，对于情感极性的判断，将目标情感分类2类：正、负；对情感程度的分析则转化为回归问题看待。对训练文本进行人工标标注，然后进行有监督的机器学习过程。\n在还没有获得大量文本的情况下，使用基于词典的方法或者简单的机器学习方法是一个不错的选择。获得大量文本后，可以尝试使用一些复杂的机器学习方法甚至使用深度学习来进一步提升分析效果。\n2.2.1基于词典的情感分析：\n情感分析对象的粒度最小是词汇，但是表达一个情感的最基本的单位则是句子，词汇虽然能描述情感的基本信息，但是单一的词汇缺少对象，缺少关联程度，并且不同的词汇组合在一起所得到的情感程度不同甚至情感倾向都相反。所以以句子为最基本的情感分析粒度是较为合理的。篇章或者段落的情感可以通过句子的情感来计算。\n基于词典的情感分析大致步骤如下：\n分解文章段落：\n分解段落中的句子：\n分解句子中的词汇：\n探索情感并标注和计数：\n搜索情感词前的程度词，根据程度大小，赋予不同权值：\n搜索情感词前的否定词，赋予反转权值（-1）：\n计算句子情感得分：\n计算段落情感得分：\n计算文章情感得分：\n考虑到语句中的褒贬并非稳定分布，以上步骤对于积极和消极的情感词分开执行，最终的到两个分值，分别表示文本的正向情感值和负向情感值。\n进过以上的步骤，每篇文章的每个段落的每个句子都会有相应的情感分值，之后针对需求，可以针对句子的分值作统计计算，也可以针对段落的分值作统计计算，得到最后的文本的正向情感值和负向情感值。\n2.3基于情感分析的系统和应用\n总体来看，基于情感分析的系统和应用在商品/服务评论分析、社交网络分析、情感机器人这三方面被广泛应用。\n传统的情感分析应用聚焦于来自消费产品和服务的评论。基于产品评论的代表性平台有Google Shopping ，它还可以为用户提供在线购物平台的商品检索和比价服务；OpinionEQ 允许商业组织和个人按需定制产品分析服务。\n微博、Twitter等社交网络服务的爆炸式发展也为研究人员带来了极大的机遇，研究人员能够通过分析大量富情感的数据来分析公众的情绪变化，并对政府管理、经济、娱乐领域产生影响。从政府和管理者角度出发，联合国开发了针对全球情感波动监测的应用Global Pulse ，北航的研究小组推出了第一个针对中文微博的在线情感系统MoodLens ；2012年美国大选时罗姆尼和奥巴马在Twitter上展开了激烈宣传，借此影响普通民众及新闻从业者，成为互联网参与总统竞选典型案例。在金融应用方面，许多研究机构将情感分析技术应用于股票分析及预测系统，例如Stock Sonar 在每只股票的价格旁边展示了每天针对该股的积极和消极的情感信息，为投资者提供即时的参考，UIC开发的Twitter情感分析进行为股市的涨跌进行预测和追踪 。在娱乐领域，阿里云的人工智能系统“小Ai” 在《我是歌手》节目中成功预测李玟夺冠，也是依靠现场数据以及社交网络上的点评数据进行分析预测，这其中都运用了对海量文本情感分析技术。可以看到，对社交媒体的情感大数据的监测和分析预测，不断影响着政府决策和大众选择。\n除了在电商平台和社交网络得到广泛应用，情感分析技术还被引入到对话机器人领域。例如，微软的“小冰”机器人 可以通过分析用户的文本输入和表情贴图，理解用户当前的情绪状况，并据此回复文本或者语音等情感回应。部分研究机构还将情感分析技术融入实体机器人中。日本软银公司的Pepper机器人 依据常见的情感认知（喜怒哀惊）及对用户的面部表情、肢体语言和措辞的分析，了解用户的情绪并选择恰当的方式与用户交流。而香港Hanson Robotics公司开发的Han机器人 不仅可以理解用户的情感，它还可以将情感反馈以模拟的面部表情展现出来。国内的Gowild公司也推出了可以提供生活助理和年轻人强社交情感交流服务的“公子小白”机器人 。这些工作实际上并不是从认知机理出发，而是通过外在的形式（词语，表情，肢体）判断人类情感。\n3、 中国的自然语言处理领域的人工智能公司\n近年来，自然语言处理在工业界与学术界被广泛应用与人工智能领域从学术界来说，中国大陆地区除了微软亚洲研究院，越来越多的研究机构设立了自然语言处理实验室。据《互联网周刊》了解，清华大学自然语言处理与社会人文计算实验室、北京大学计算机科学研究所语言计算与互联网挖掘研究室、哈工大机器智能技术与自然语言处理实验室、中科院自然语言处理研究组、复旦大学自然语言处理研究组等都对自然语言处理有深入的研究。\n与此同时，随着自然语言处理领域的兴起，越来越多的自然语言处理领域的公司相继出现。《互联网周刊》整理了自然语言处理领域的代表性公司：\n图表 7自然语言处理领域的代表性公司\n三、总结\n随着神经网络算法的不断优化，将加速人工智能产业的发展，尤其是推动人机交互、万物相连的发展。物联网服务更加智能化。基础电信企业能快速的拓展新业务领域，搜寻到其他有价值的业务增长点。与此同时，各行业运营商积极人工智能领域的应用，通过神经网络建立合作开发的智能应用平台，联合创新，推动人工智能在居民的日常生活、交通、物流、家居、教育、医疗等行业的不断成长。将来随着情感分析研究不断突破，自然语言处理技术越来越成熟，其应用前途广大，尤其是和实体机器人结合，将多媒体技术融为一体，结合语音、图像处理技术，可以从语言、表情和行为方面理解人类情感并给出相应的情感回复，打造一个具有情感的机器人时代已经不远了！\n参考文献\n【1】《神经网络在人工智能中的应用》刘肖楠 信息技术与信息化\n【2】《神经网络算法在人工智能识别中的应用研究》张庆、刘中儒、郭华 江苏通信\n【3】《密集连接卷积神经网络:让人工智能拥有更强大脑》黄高 上海信息化\n【4】《基于卷积神经网络的自然语言处理相关技术研究》于彦秋 国防科学技术大学\n【5】《神经网络理论应用于自然语言处理》贺前华、徐秉铮 中文信息\n【6】《中国的自然语言处理领域的人工智能公司》轩中 互联网周刊\n【7】《人工智能在自然语言处理中的应用》李彦峰 襄阳职业技术学院学报\n【8】《面向自然语言处理的人工智能框架》蔡艳婧、程显毅、潘燕 微电子学与计算机\n【9】《文本情感分析：让机器读懂人类情感》黄高","data":"2019年04月15日 20:03:12"}
{"_id":{"$oid":"5d343af362f717dc0659b299"},"title":"推荐：深入浅出的自然语言处理书单！","author":"大圣众包平台","content":"DT时代，大数据、BI和人工智能均是十分火热的产业趋势，而自然语言处理作为人工智能领域和计算机科学领域中的一个重要方向，也随之火热了一把，得到不少IT人士的极大研究兴趣。现在，大圣众包威客平台推介若干本深入浅出的自然语言处理书籍。\n《Foundations of Statistical Natural Language Processing》\n用统计方法处理自然语言文本，在近年来已经占据了主导地位。《Foundations of Statistical Natural Language Processing》涵盖了搭配发现、词义消歧、概率解析、信息检索和其他应用等内容。同时，它也对统计自然语言处理（NLP）进行了全面的介绍，并且包含了所有开发NLP工具所需的理论和算法。此书不但提供了广泛且严格的数学和语言基础的内容，还包括详细的统计方法讨论，让学生和研究人员可以根据其实现自己的想法。\n2.《自然语言处理简明教程》\n系统地阐述了自然语言处理的基本方法的《自然语言处理简明教程》，描述了每一种方法的技术原理及操作过程。另外，此书还介绍了自然语言处理在各个领域的应用，让读者能够掌握第一手的自然语言处理的前沿动态。作为在本领域十分著名的书籍，《自然语言处理简明教程》不仅可供计算机科学工作者、人工智能领域工作者阅读，还可供语言学及应用语言学的师生阅读与参考。\n3.《Speech and Language Processing, 2nd Edition》\n《Speech and Language Processing, 2nd Edition》在古典自然语言处理、统计自然语言处理、语音识别、计算语言学和人类语言处理的本科或高级本科课程中，都有着十分崇高的地位。\n基于Web语言技术的爆炸式发展，以及多领域的合并等，使得语言处理渐渐成为让人深感兴趣的科目。它也是第一本在所有层次和所有现代技术层面上，全面覆盖语言技术的书，特别适用于大公司的应用统计方面以及其他机器学习算法领域。\n4.《自然语言处理原理与技术实现》\n《自然语言处理原理与技术实现》详细介绍了自然语言处理以Java 实现的各主要领域的原理，当中包括中文分词、词性标注、依存句法分析等，更对中文分词和词性标注的过程及相关算法，如隐马尔可夫模型等，进行了详细的介绍。本书内容丰富，它在自然语言处理的应用中主要介绍了信息抽取、自动文摘、文本分类等领域的基本理论和实现过程，另外，还有问答系统、语音识别等目前应用非常广泛的领域。值得注意的是，在问答系统的介绍中，《自然语言处理原理与技术实现》特地介绍了聊天机器人的实现过程，无论是从句子理解、句法分析、同义词提取等方面，都深刻地揭示了聊天机器人的实现原理。\n好的书单，不仅能够提升学习和工作的效率，还能节省进修成本。自然语言处理研究，是实现人与计算机之间用自然语言进行有效通信的各种理论和方法。希望广大对人工智能有兴趣的人士阅读此文后，略觉有所裨益。","data":"2017年01月06日 11:22:02","date":"2017年01月06日 11:22:02"}
{"_id":{"$oid":"5d343af362f717dc0659b29b"},"title":"什么是自然语言处理，自然语言处理目前的应用有哪些？","author":"人工智能和大数据时代","content":"自然语言处理大体包括了自然语言理解和自然语言生成两个部分，实现人机间自然语言通信意味着要使计算机既能理解自然语言文本的意义，也能以自然语言文本来表达给定的意图、思想等，前者称为自然语言理解，后者称为自然语言生成。\n自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。自然语言处理的终极目标是用自然语言与计算机进行通信，使人们可以用自己最习惯的语言来使用计算机，而无需再花大量的时间和精力去学习不很自然和习惯的各种计算机语言。\n\n针对一定应用，具有相当自然语言处理能力的实用系统已经出现，典型的例子有：多语种数据库和专家系统的自然语言接口、各种机器翻译系统、全文信息检索系统、自动文摘系统等。\n国内BAT、京东、科大讯飞都有涉及自然语言处理的业务，另外还出现了爱特曼、出门问问、思必驰、蓦然认知、三角兽科技、森亿智能、乂学教育、智齿客服等新兴企业。\n人工智能、大数据、云计算和物联网的未来发展值得重视，均为前沿产业，多智时代专注于人工智能和大数据的入门和科谱，在此为你推荐几篇优质好文：\n如何快速入门NLP自然语言处理概述\nhttp://www.duozhishidai.com/article-11742-1.html\n自然语言处理（NLP）知识结构总结\nhttp://www.duozhishidai.com/article-10036-1.html\nNLP自然语言处理技术，在人工智能法官中的应用是什么？\nhttp://www.duozhishidai.com/article-2325-1.html\n\n多智时代-\n人工智能\n和\n大数据\n学习入门网站|人工智能、大数据、\n物联网\n、\n云计算\n的学习交流网站","data":"2019年04月09日 16:54:59","date":"2019年04月09日 16:54:59"}
{"_id":{"$oid":"5d343af462f717dc0659b29d"},"title":"自然语言处理顶会 ACL 2018 参会见闻","author":"机器之心V","content":"关于作者：郑在翔，现为南京大学自然语言处理实验室二年级硕士生，将准备继续攻读自然语言处理方向的博士。当前主要研究方向为神经网络机器翻译。\n作者在本文记录了自己在自然语言处理顶会 ACL 2018 的参会经历，从个人的角度出发，介绍了会议内外的内容、感兴趣的工作和研究热点，并简单叙述了其以论文作者身份第一次参加学术会议的一些感想。\n2018 年 7 月 15 日至 20 日，自然语言处理领域的顶级会议 ACL 2018 在澳大利亚墨尔本举行。本次大会共收到了 1018 篇长文和 526 篇短文的提交，相比去年有显著的增长，在规模上是名副其实的学术界的盛会。其中，长文有 256 篇被录用，录用率为 25.1%；短文有 125 篇被录用，录用率为 23.8%；总体的录用率为 24.7%。\n本次会议还是产业界的盛会，共得到了来自全世界 28 家赞助商的大力赞助。近些年来，来自中国的企业对人工智能领域学术会议的赞助热情和规模逐年上涨，ACL 2018 的主要赞助商中，有 7 家来自中国，如字节跳动（ByteDance）、百度、京东、腾讯等。其中字节跳动公司与 Apple、Google 等公司同为本届 ACL 的顶级赞助商，这也反应了中国企业和市场对于人工智能学术前沿和产业化应用的关注。\n由于笔者有一篇发表在 TACL （Transaction of ACL）上的论文有机会在此次 ACL 展示，所以非常幸运地能来到墨尔本参加此次的大会。作为一个第一次参加学术会议的小白，笔者在墨尔本的这短短七天不仅经历了好多“第一次”，更得到了许许多多的收获。在会议期间，不仅有机会可以和来自世界各地优秀的研究者请教交流，还能近距离了解人工智能企业感兴趣的问题。笔者在此想结合自己的参会经历，和大家分享一下本次会议的见闻。\n精彩的主会场\n第一天是 Tutorial 环节。此次大会共设了 8 场 Tutorial，上午下午各 4 场。笔者参加了上午的Neural Approaches to Conversational AI 和下午的 Deep Reinforcement Learning for NLP。两个 Tutorial 深入浅出，非常精彩，并且都公开了报告的 slides。\n▲ Williams Wang 老师在报告《Deep Reinforcement Learning for NLP》Tutorial\n第二天日程由大会开幕式开始。在开幕式中，大会主席 Marti Hearst 宣布成立 AACL（The Asia-Pacific Chapter of the ACL）的决定，引发现场一片欢呼。区别于现有的针对于欧洲地区的 EACL（The European Chapter of the ACL）和北美地区的 NAACL（The North American Chapter of the Association for Computational Linguistics），此次成立的 AACL 会主要聚焦于亚太地区。\n新成立的 AACL 委员会由亚太地区该领域具有影响力的学者组成。其中，AACL 委员会主席（Chair）是来自百度公司的王海峰，候任主席（Chair-Elect）是来自台湾资讯科学研究所的 Keh-Yih Su，秘书（Secretary）是来自清华大学的刘洋等。相信 AACL 的成立将会给亚太地区的自然语言处理的研究者又一个促进交流和学习的机会。\n笔者在本次会议中看到，深度学习继续在自然语言处理的领域中发挥着重要作用。正式会议的上午和下午是 Oral 报告。笔者和大家一样，在各个不同 Track的会场奔波。Oral 的报告都非常有启发性，如来自 Google AI 的工作 The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation，通过将 RNN 的时序建模能力和 Transformer 中的“块”的概念及最新的训练技术结合起来，再次刷新了机器翻译上的最佳性能，同时也给笔者很大启发：虽然 Transformer 的提出使人们惊讶于 Self-Attention 赋予模型的性能，但是这不一定代表 RNN 的表达能力是弱于 Self-Attention 的；而在结合了 Transformer 中的一些最新的技术后，RNN 也表现出了更优越的性能，使人重新思考模型的表达能力与训练之间关系。\n▲ Google AI 的论文报告\n虽然笔者的研究方向主要是机器翻译，但也非常希望能从其他方向上的一些优秀论文也会中得到对自己研究的启发。例如来自 FAIR 的工作 What you can cram into a single $\u0026!#* vector: Probing sentence embeddings for linguistic properties，通过采用探测（probing）的方法，试图“探测”出30个不同模型学到神经网络句子表示中究竟蕴含了什么，比如句子的长度信息和句法树的深度等，给下游任务的研究者对句子表示的理解提供了新的视角。\n和 Oral 报告一样，在 Poster 环节中，笔者也更关注其他 NLP 方向的工作，希望能从其他得到启发。笔者看到本次会议有许多工作都开始探索基于 Self-Attention 的方法来建模文本中的句子表示和依赖关系。如来自 Berkeley 的工作 Constituency Parsing with a Self-Attention Encoder，通过引入 Transformer 中的 Self-Attention 机制，并且发现原本的 Self-Attention 机制中将内容信息和位置信息结合起来建模可能存在问题，提出了一种将内容和位置信息分解的方式，在成分句法分析（Constituency Parsing）上得到了 state-of-the-art 的性能。笔者还向作者询问了有关位置信息编码的问题，也得到了耐心的解答。\n除此之外，笔者还发现，生成对抗网络和强化学习在很多工作中得到应用，这也表现了未来一段时间的研究热点和趋势。另外，研究如何将人类总结的知识和自然语言处理中的深度学习模型结合起来，向模型提供更多的先验知识的工作也在逐渐增多。\n笔者有幸在此有一篇论文 Modeling Past and Future for Neural Machine Translation 作为 Poster 展示。这篇工作主要的动机是源于在通常的神经机器翻译的翻译结果中观察到的漏翻译和重复翻译现象。于是我们试图在解码阶段更显式地建模和学习到翻译的历史和未来信息的表示，希望能为当前一步翻译的注意力机制和单词预测提供更加丰富的目标端上下文，从而希望能避免翻译模型做出遗漏或者重复的决策。\n由于是作为小白的笔者第一次在学术会议上做 Poster，并且需要使用英语，对口语不是很自信的我一度非常紧张。当天中午笔者连午饭都来不及吃，就匆匆守到了我们的展板，内心十分忐忑，担心自己不能很好地向对我们工作感兴趣的研究者解答他们的疑惑和表达自己的观点。\n本文的共同第一作者、现在在字节跳动人工智能实验室工作的师兄看出了笔者的紧张，就向我介绍了他的经验。在实验设计和论文撰写时，师兄就提出了非常多有深度的见解，为这篇工作做出了主要贡献。在他的鼓励下，我的情绪得以稍稍缓解。当真正开始向源源不断过来的人讲解自己的工作的时候，笔者反而就忘记了紧张这回事了，投入到了和大家的交流过程中。面对各种各样的问题，笔者只好不停地“freestyle”。幸运的是，每个人都很有耐心地和我进行反复的讨论。通过和大家的交流，不仅收获了很多启发和建议，也锻炼了自己的表达和胆量，为自己积累了宝贵的经验。\n此次会议评选出了 3 篇最佳长论文和 2 篇最佳短论文，共计 5 篇，并在闭幕式前进行报告。其中，最佳长论文为：\n1. Finding syntax in human encephalography with beam search. John Hale, Chris Dyer, Adhiguna Kuncoro and Jonathan Brennan.\n2. Learning to Ask Good Questions: Ranking Clarification Questions using Neural Expected Value of Perfect Information. Sudha Rao and Hal Daumé III.\n3. Let’s do it “again”: A First Computational Approach to Detecting Adverbial Presupposition Triggers. Andre Cianflone, Yulan Feng, Jad Kabbara and Jackie Chi Kit Cheung.\n最佳短论文为：\n1. Know What You Don’t Know: Unanswerable Questions for SQuAD. Pranav Rajpurkar, Robin Jia and Percy Liang\n2. Lighter’ Can Still Be Dark: Modeling Comparative Color Descriptions. Olivia Winn and Smaranda Muresan\n在闭幕式上，大会将 ACL 终身成就奖（Lifetime Achievement Award）颁发给了 University of Edinburgh 的 Mark Steedman 教授，感谢其对计算语言学、人工智能和认知科学作出的贡献。\n▲ ACL终身成就奖获得者Mark Steedman教授\n会场之外\n在休息时走出报告的会场，可以看到有很多企业的展台，其中许多企业通过各种形式的 DEMO 来展示自己的技术和业务，让参会者们有机会了解到产业界的状况和关心的问题。 例如旗下有今日头条、抖音等产品的字节跳动（ByteDance）公司，他们以小明机器人（Xiaoming Bot） 为世界杯比赛自动写稿为例，展示了计算机视觉与自然语言处理技术的结合。其中，小明机器人首先通过基于计算机视觉的足球比赛理解技术能对视频中的球员、足球甚至人物的表情进行实时的追踪、分割和理解，而后结合自然语言处理技术自动生成图文并茂的新闻稿。\n除此之外还看到相关人员在演示字节跳动支持多个语种的机器翻译系统。作为近些年来国内发展迅猛的 AI 企业，它的产品和机构也在积极地进行全球化运作。除了短视频软件抖音等在海外市场的惊人扩张外，它在美国硅谷、西雅图等地都设立了国际化的 AI 实验室，从而希望能招揽世界各地的行业精英。\n笔者还有幸分别参加了百度公司和 CCF 青工委，以及字节跳动组织的晚宴。在百度公司和青工委组织的晚宴上，微软亚洲研究院副院长周明老师回顾了中国自然语言处理发展的历史，让我们这些晚辈对从那个年代的艰难起一路坚持走来的学术先行者们的敬意油然而生。展望未来，周明老师说：“在 NLP 的顶会上，中国和美国发表的论文数量很接近，我们下一步的目标，不光做更多的工作，还要做更好的工作，做更多有挑战的问题，为人类的文明作出贡献。”字节跳动晚宴邀请了很多国内外知名学者，李航老师现在在字节跳动人工智能实验室工作，这次晚宴是由他主持。从演讲了解到这是一个飞速发展的公司，在 NLP 领域已经有很多的应用，同时也正在招募更多技术人才加入他们。\n除了在大会现场搭建展台和学者们交流外，笔者也看到多家中国企业，如阿里巴巴、百度、腾讯、京东、讯飞等，在主会场和最后两天的 Workshop 中积极展示来自企业的工作发表及研究成果。一段时间以来，这些 AI 相关的企业都在大力布局人工智能，通过对产业界中发现的实际问题进行研究和解决，表现出对前沿学术研究的空前热情，也使得学术界能更直接地了解到产业界关心的问题，对整个人工智能领域的健康发展起到了重要的作用。\n写在最后\n短短几日的 ACL 2018 之旅就要结束了，这几日经笔者历了无数的“第一次”：第一次参加学术会议，第一次在会议上作 Poster 展示，第一次鼓起勇气向 Christopher Manning 教授请教问题，第一次走在路上被墨尔本突如其来的冰雹袭击等。我感觉对我来说最大的收获除了各个学术上的报告外，就是学会倾听别人的想法和努力表达自己的想法。\n相信很多小伙伴一开始的时候也向笔者一样，对自己的听力和表达不自信，担心无法和来自世界各地的研究者交流。但当笔者真正专注进这个事情，渴望就自己感兴趣的话题交换想法时，就发现这些都不再成为沟通的障碍了。在开会期间遇见了太多太多优秀的同龄人，他们对研究理解很深入，对问题的看法颇有远见，这也激励笔者要多多提高自己，努力向优秀的同龄人看齐。以上是笔者参加 ACL 2018 的一些见闻和浅薄的见解，希望自己将来也能做出有意义的工作，再次有机会和世界各地的学者交流。\n入门 ACL 2018 NLP\n1","data":"2018年07月25日 12:25:00"}
{"_id":{"$oid":"5d343af462f717dc0659b29f"},"title":"中文自然语言处理工具介绍","author":"xieyan0811","content":"自然语言处理是人工智能领域中的一个重要方向。它研究能人机之间通讯的方式，并涉及机器对人类知识体系的学习和应用．从分词，相似度计算，情感分析，文章摘要，到学习文献，知识推理，都涉及自然语言分析．下面介绍一些中文语言语义分析的资源．（以下只讨论能嵌入到我们程序里的资源）\n1.      同义词词林\n《同义词词林》是80年代出版的一本词典，这提供了词的归类，相关性信息，起始主要用于翻译，哈工大对它进行了细化和扩充，出了《词林扩展版》，其中含有7万多词，17000多种语义，五层编码．12大类，94中类，1428小类，形如：\n\nAa01A01= 人 士 人物 人士 人氏 人选\n\n\n每一个条目对应一种语义，根据分类编号：第一位大写表示大类，第二位小写表示中类…其中涉及了一词多义和一义多词．\n《词林扩展版》网上的下载很多，大小不到1M，可以直接load到程序中，用于简单的分词，文章分类，模糊查找，统计，情感分析（不同感情色彩对应不同类别号）等等．\n2.      哈工大语言云(LTP)\n中文的语义分析工具，大多数都像LTP这样，提供一个在线的分析器，一组API，比较简单稳定的功能．LTP是其中做得比较好的．\n它提供了中文分词、词性标注、命名实体识别、依存句法分析、语义角色标注等等功能．但对于进一步语义方面的深入的开发，用处不大，而且需要连网使用，速度和处理数量上都有一些限制．\n详见：http://www.ltp-cloud.com/demo\n3.      结巴分词\n结巴是一个Python的中文分词组件．它提供了分词和词性标注功能．能在本地自由使用, 是Python实现的, 可以很好的和其它Python工具相结合，使用方法如下：\n#encoding=utf-8 import jieba.posseg as pseg import jieba seg_list = jieba.cut(\"我爱北京天安门\", cut_all=True) print \"Full Mode:\", \"/ \".join(seg_list) words = pseg.cut(\"我爱北京天安门\") for w in words: print w.word,w.flag\n执行结果是:\n\nFull Mode: 我/ 爱/ 北京/ 天安/ 天安门 我 r 爱 v 北京 ns 天安门 ns\n详见: http://www.oschina.net/p/jieba/\n4.      知网 HowNet\n对于语言的理解, 人们更关注语义，即研究文字真正的含义是什么，并希望机器能像人脑一样把知识组织成体系．\n中文语义库开放的资源非常少，《现代汉语语义词典》，《中文概念辞书》这些都是听说过没见过，总之人家是不开放. 就算能去书店买一本, 也用不到程序里. 我在网上只找到了HowNet (可以在csdn下载, 压缩包1.5M左右). 形如:\n\nNO.=069980 W_C=群众 G_C=N E_C= W_E=the masses G_E=N E_E= DEF=human|人,mass|众\n\n\n可以看到它包含：编号, 中文词, 对应英文词, 词性, 约12万多项.\nHowNet在2013年后就不更新了, 以上版本差不多是能在网上找到的比较全的数据了. 它还提供了一些库, 可用于判断相似度等．\n详见：http://www.keenage.com/html/c_index.html\n5.      NLTK与WordNet (sentiwordnet)\nWordNet是一个语义词典, NLTK是Python的一个自然语言处理工具，它提供了访问WordNet各种功能的函数。WordNet形如:\nn 03790512 0 0 motorcycle#1 bike#1 a motor vehicle with two wheels and a strong frame\n其中含有词性, 编号, 语义, 词汇间的关系(同义/反义,上行/下行,整体/部分…), 大家都觉得＂它很棒, 只可惜没有中文支持＂. 其实也不是没中文支持. WordNet有中文以及其它更多语言的支持, 可以从以下网址下载:\n\nhttp://globalwordnet.org/wordnets-in-the-world/\n\n其中的数据文件形如：\n\n03790512-n cmn:lemma 摩托车\n可以看到，它与sentiwordnet的词条编号一致，尽管对应可能不是特别完美，但理论上是：对英文能做的处理，对中文也能做．\nNLTK+WordNet功能非常丰富，强烈推荐《PYTHON自然语言处理NLTK Natural LanguageProcessing with Python》这本书，它已由爱好者译成中文版，可从网上下载．里面不但讨论了具体的实现方法，还讨论了一些研究方向，比如＂从自然语言到一阶逻辑＂…\n6.      随想\n对语言的处理，首先是分词，然后是消歧, 判断词在句中的成份, 识别语义．形成知识网络．．．希望最终机器能像人类一样，学习，思考和创造．\n语言处理在不同的层次有不同的应用：从文章分类，内容提取，到自动诊断病情（IBM Watson），或者存在更通用的逻辑，使机器成为比搜索引擎更智能的各个行业的专家系统．\n自然语言和语义看似多对多的关系，我觉得本质上语义转换成语言是从高维到低的投影．从词林的分类看，真正核心的概念并不太多，但是语义的关系和组合很复杂，再深层次还涉及知识线等等．而语言只是它的表象．在分析过程中，越拟合那表象，差得越多．\n另外，这一领域已经有几十年的历史了，学习时尽可能利用现有工具，把精力集中在目标而非具体过程．多参考人家都实现了什么功能，人家的数据是怎么组织的．","data":"2017年03月09日 16:54:02"}
{"_id":{"$oid":"5d343af462f717dc0659b2a1"},"title":"自然语言处理NLP概述","author":"资深架构师","content":"自然语言处理NLP概述\n版权声明：本文为博主chszs的原创文章，未经博主允许不得转载。\n自然语言处理（Natural language processing，NLP）是计算机和人类语言之间的关系纽带。更具体地说，自然语言处理是计算机对自然语言的理解、分析、操纵和/或生成。计算机程序能否将一段英文文本转换成程序员友好的数据结构来描述自然语言文本的含义？不幸的是，这种数据结构的形式是否存在并没有形成共识。在解决这些基本的人工智能问题之前，计算机科学家必须解决提取描述文本信息有限方面的简单表示的简化目标。\n概述自然语言处理\n自然语言处理（NLP）可以被定义为人类语言的自动（或半自动）处理。“NLP”这个术语有时被用于比这更窄的范围，通常不包括信息检索，有时甚至不包括机器翻译。NLP有时还与“计算语言学”相对立，NLP被认为更适用。如今，往往首选使用替代术语，如“语言技术（Language Technology）”或“语言工程（Language Engineering）”。语言（Language）经常与演讲（Speech）（比如演讲技术和语言技术）相对照。但是我将简单地提到NLP并广义地使用这个术语。NLP本质上是多学科的：它与语言学密切相关（尽管NLP公然借鉴语言理论的程度差异很大）。\n什么是自然语言处理？\nNLP是计算机以一种聪明而有用的方式分析，理解和从人类语言中获取意义的一种方式。通过利用NLP，开发者可以组织和构建知识来执行自动摘要，翻译，命名实体识别，关系提取，情感分析，语音识别和话题分割等任务。NLP用于分析文本，使机器了解人的说话方式。这种人机交互使现实世界的应用，如自动文摘，情感分析，主题提取，命名实体识别，零部件，词性标注，关系提取，词干，等等。NLP通常用于文本挖掘，机器翻译和自动问答。\n\n图1：NLP技术\nNLP的重要性\n早期的NLP方法涉及更基于规则的方法，在这种方法中，简单的机器学习算法被告知要在文本中查找哪些单词和短语，并在这些短语出现时给出特定的响应。但深度学习是一个更灵活，直观的方法，在这个方法中，算法学会从许多例子中识别说话者的意图，就像孩子如何学习人类语言一样。\n在考虑以下两个陈述时，可以看到自然语言处理的优势：“云计算保险应该成为每个服务级别协议的一部分”和“良好的SLA确保夜间睡眠更加容易 - 即使在云端”。如果您使用国家语言处理的搜索，程序将认识到云计算是一个实体，云是云计算的缩写形式，并且SLA是服务级别协议的行业首字母缩略词。\n自然语言处理的术语\n这些分区与语言学的一些标准分支松散地相对应：\n形态学（Morphology）：词的结构。例如，不寻常的可以被认为是由一个前缀un-，一个词干和一个词缀-ly组成。构成是构成加上屈折词缀：拼写规则意味着我们结束而不是组成。\n语法（Syntax）：单词用于形成短语的方式。例如，它是英语语法的一部分，诸如意志之类的确定者会在名词前面出现，而且确定者对于某些单数名词是强制性的。\n语义（Semantics）：构成语义是基于语法的意义的建构（通常表示为逻辑）。这与词汇语义（即单词的含义）形成对照。\n自然语言处理的应用\n以下是目前使用NLP的几种常用方法：\nMicrosoft Word中的拼写检查功能是最基本和最知名的应用程序。\n文本分析也称为情感分析，是NLP的一个关键用途。企业可以使用它来了解他们的客户感受到的情绪，并使用这些数据来改善他们的服务。\n通过使用电子邮件过滤器分析流经其服务器的电子邮件，电子邮件提供商可以使用朴素贝叶斯垃圾邮件过滤来计算电子邮件基于其内容的可能性。\n呼叫中心代表经常听到来自客户的相同的具体投诉，问题和问题。挖掘这些数据的情绪可以产生令人难以置信的可操作的情报，可以应用于产品布局，消息传递，设计或其他一系列的用途。\nGoogle，Bing和其他搜索系统使用NLP从文本中提取条件来填充其索引并解析搜索查询。\nGoogle Translate将机器翻译技术应用于翻译单词，而且还用于理解句子的含义以改善翻译。\n金融市场使用NLP，通过明文公告和提取相关信息的格式进行算法交易决策。例如，公司之间合并的消息可能会对交易决策产生重大影响，并且合并细节（例如，参与者，价格，谁获得谁）的速度可以被纳入到交易算法中，在数百万美元。\n自然语言处理的例子\n使用Summarizer自动总结一个文本块，严格的主题句子，并忽略其余的。\n生成关键字话题标签文档使用LDA（隐含狄利克雷分布），它从一个确定最相关的词文件。该算法是自动标记和自动标记URL微服务的核心\n基于Stanford NLP的情感分析可以用来辨别一个陈述的感觉，观点或信念，从非常消极，中立到非常积极。\n参考\n[1] Ann Copestake, “Natural Language Processing”, 2004, 8 Lectures, available online at: https://www.cl.cam.ac.uk/teaching/2002/NatLangProc/revised.pdf\n[2] Ronan Collobert and Jason Weston, “Natural Language Processing (Almost) from Scratch”, Journal of Machine Learning Research 12 (2011) pp. 2493-2537\n[3] “Top 5 Semantic Technology Trends to look for in 2017”, available online at: https://ontotext.com/top-5-semantic-technology-trends-2017/","data":"2019年05月08日 22:15:03","date":"2018年01月04日 21:39:10"}
{"_id":{"$oid":"5d343af562f717dc0659b2a3"},"title":"利用NLTK在Python下进行自然语言处理","author":"白马负金羁","content":"自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。自然语言工具箱（NLTK，Natural Language Toolkit）是一个基于Python语言的类库，它也是当前最为流行的自然语言编程与开发工具。在进行自然语言处理研究和应用时，恰当利用NLTK中提供的函数可以大幅度地提高效率。本文就将通过一些实例来向读者介绍NLTK的使用。\n\n\n\n开发环境：我所使用的Python版本是最新的3.5.1，NLTK版本是3.2。Python的安装不在本文的讨论范围内，我们略去不表。你可以从NLTK的官网上http://www.nltk.org/ 获得最新版本的NLTK。Anyway，使用pip指令来完成NLTK包的下载和安装无疑是最简便的方法。\n当然，当你完成这一步时，其实还不够。因为NLTK是由许多许多的包来构成的，此时运行Python，并输入下面的指令（当然，第一条指令还是要导入NLTK包）\n\n\n\n\u003e\u003e\u003e import nltk \u003e\u003e\u003e nltk.download()\n\n\n然后，Python Launcher会弹出下面这个界面，建议你选择安装所有的Packages，以免去日后一而再、再而三的进行安装，也为你的后续开发提供一个稳定的环境。某些包的Status显示“out of date”，你可以不必理会，它基本不影响你的使用与开发。\n\n\n既然你已经安装成功，我们来小试牛刀一下。当然本文涉及的主要任务都是自然语言处理中最常用，最基础的pre-processing过程，结合机器学习的高级应用我们会在后续文章中再进行介绍。\n\n\n1、 Sentences Segment（分句）\n也就是说我们手头有一段文本，我们希望把它分成一个一个的句子。此时可以使用NLTK中的 punkt sentence segmenter。来看示例代码\n\n\n\u003e\u003e\u003e sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle') \u003e\u003e\u003e paragraph = \"The first time I heard that song was in Hawaii on radio. ... I was just a kid, and loved it very much! What a fantastic song!\" \u003e\u003e\u003e sentences = sent_tokenizer.tokenize(paragraph) \u003e\u003e\u003e sentences ['The first time I heard that song was in Hawaii on radio.', 'I was just a kid, and loved it very much!', 'What a fantastic song!']\n\n\n由此，我们便把一段话成功分句了。\n\n\n\n2、Tokenize sentences （分词）\n\n接下来我们要把每个句话再切割成逐个单词。最简单的方法是使用NLTK 包中的 WordPunct tokenizer。来看示例代码\n\n\n\u003e\u003e\u003e from nltk.tokenize import WordPunctTokenizer \u003e\u003e\u003e sentence = \"Are you old enough to remember Michael Jackson attending ... the Grammys with Brooke Shields and Webster sat on his lap during the show?\" \u003e\u003e\u003e words = WordPunctTokenizer().tokenize(sentence) \u003e\u003e\u003e words ['Are', 'you', 'old', 'enough', 'to', 'remember', 'Michael', 'Jackson', 'attending', 'the', 'Grammys', 'with', 'Brooke', 'Shields', 'and', 'Webster', 'sat', 'on', 'his', 'lap', 'during', 'the', 'show', '?']\n\n\n\n\n我们的分词任务仍然完成的很好。除了WordPunct tokenizer之外，NLTK中还提供有另外三个分词方法，\n\nTreebankWordTokenizer，PunktWordTokenizer和WhitespaceTokenizer，而且他们的用法与WordPunct tokenizer也类似。然而，显然我们并不满足于此。对于比较复杂的词型，WordPunct tokenizer往往并不胜任。此时我们需要借助正则表达式的强大能力来完成分词任务，此时我所使用的函数是regexp_tokenize()。来看下面这段话\n\n\n\u003e\u003e\u003e text = 'That U.S.A. poster-print costs $12.40...'\n\n\n目前市面上可以参考的在Python下进行自然语言处理的书籍是由Steven Bird、Ewan Klein、Edward Loper编写的《Python 自然语言处理》。但是该书的编写时间距今已有近十年的时间，由于软件包更新等语言，在新环境下进行开发时，书中的某些代码并不能很正常的运行。最后，我们举一个书中代码out of date的例子（对上面这就话进行分词），并给出相应的解决办法。首先来看书中的一段节录\n\n\n\u003e\u003e\u003e text = 'That U.S.A. poster-print costs $12.40...' \u003e\u003e\u003e pattern = r'''(?x) # set flag to allow verbose regexps ... ([A-Z]\\.)+ # abbreviations, e.g. U.S.A. ... | \\w+(-\\w+)* # words with optional internal hyphens ... | \\$?\\d+(\\.\\d+)?%? # currency and percentages, e.g. $12.40, 82% ... | \\.\\.\\. # ellipsis ... | [][.,;\"'?():-_`] # these are separate tokens; includes ], [ ... ''' \u003e\u003e\u003e nltk.regexp_tokenize(text, pattern)\n\n\n\n我们预期得到输出应该是这样的\n\n\n['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']\n\n\n\n但是我们实际得到的输出却是这样的（注意我们所使用的NLTK版本）\n\n\n\n[('', '', ''), ('A.', '', ''), ('', '-print', ''), ('', '', ''), ('', '', '.40'), ('', '', '')]\n\n\n会出现这样的问题是由于nltk.internals.compile_regexp_to_noncapturing()在V3.1版本的NLTK中已经被抛弃（尽管在更早的版本中它仍然可以运行），为此我们把之前定义的pattern稍作修改\n\n\npattern = r\"\"\"(?x) # set flag to allow verbose regexps (?:[A-Z]\\.)+ # abbreviations, e.g. U.S.A. |\\d+(?:\\.\\d+)?%? # numbers, incl. currency and percentages |\\w+(?:[-']\\w+)* # words w/ optional internal hyphens/apostrophe |\\.\\.\\. # ellipsis |(?:[.,;\"'?():-_`]) # special characters with meanings \"\"\"\n\n\n再次执行前面的语句，便会得到\n\n\n\n\u003e\u003e\u003e nltk.regexp_tokenize(text, pattern) ['That', 'U.S.A.', 'poster-print', 'costs', '12.40', '...']\n\n\n以上便是我们对NLTK这个自然语言处理工具包的初步探索，日后主页君将结合机器学习中的方法再来探讨一些更为深入的应用。最后，我想说《Python 自然语言处理》仍然是当前非常值得推荐的一本讲述利用NLTK和Python进行自然语言处理技术的非常值得推荐的书籍。","data":"2016年04月03日 20:14:50","date":"2016年04月03日 20:14:50"}
{"_id":{"$oid":"5d343af562f717dc0659b2a5"},"title":"自然语言处理简介及主要研究方向","author":"苏叶biu","content":"百度词条：\n自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。自然语言处理是一门融语言学、计算机科学、数学于一体的科学。因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，所以它与语言学的研究有着密切的联系，但又有重要的区别。自然语言处理并不是一般地研究自然语言，而在于研制能有效地实现自然语言通信的计算机系统，特别是其中的软件系统。因而它是计算机科学的一部分。\n自然语言处理（NLP）是计算机科学，人工智能，语言学关注计算机和人类（自然）语言之间的相互作用的领域。\nNatural language processing (NLP) is a field of computer science, artificial intelligence and computational linguistics concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with programming computers to fruitfully process large natural language corpora. Challenges in natural language processing frequently involve natural language understanding（NLU）, natural language generation (frequently from formal, machine-readable logical forms), connecting language and machine perception, dialog systems, or some combination thereof.\n随着深度学习的发展，LSTM的应用取得的突破，极大地促进了NLP的发展。\n自然语言处理的主要范畴有以下:\n文本朗读（Text to speech）/语音合成（Speech synthesis）\n语音识别（Speech recognition）\n中文自动分词（Chinese word segmentation）\n词性标注（Part-of-speech tagging）\n句法分析（Parsing）\n自然语言生成（Natural language generation）\n文本分类（Text categorization）\n信息检索（Information retrieval）\n信息抽取（Information extraction）\n文字校对（Text-proofing）\n问答系统（Question answering）\n给一句人类语言的问定，决定其答案。 典型问题有特定答案 (像是加拿大的首都叫什么?)，但也考虑些开放式问句(像是人生的意义是是什么?)\n机器翻译（Machine translation）\n将某种人类语言自动翻译至另一种语言\n自动摘要（Automatic summarization）\n产生一段文字的大意，通常用于提供已知领域的文章摘要，例如产生报纸上某篇文章之摘要\n文字蕴涵（Textual entailment）\n自然语言处理目前研究的难点\n单词的边界界定\n在口语中，词与词之间通常是连贯的，而界定字词边界通常使用的办法是取用能让给定的上下文最为通顺且在文法上无误的一种最佳组合。在书写上，汉语也没有词与词之间的边界。\n词义的消歧\n许多字词不单只有一个意思，因而我们必须选出使句意最为通顺的解释。\n句法的模糊性\n自然语言的文法通常是模棱两可的，针对一个句子通常可能会剖析（Parse）出多棵剖析树（Parse Tree），而我们必须要仰赖语意及前后文的资讯才能在其中选择一棵最为适合的剖析树。\n有瑕疵的或不规范的输入\n例如语音处理时遇到外国口音或地方口音，或者在文本的处理中处理拼写，语法或者光学字元识别（OCR）的错误。\n语言行为与计划\n句子常常并不只是字面的意思；例如，“你能把盐递过来吗”，一个好的回答应当是动手把盐递过去；在大多数上下文环境中，“能”将是糟糕的回答，虽说回答“不”或者“太远了我拿不到”也是可以接受的。再者，如果一门课程去年没开设，对于提问“这门课程去年有多少学生没通过？”回答“去年没开这门课”要比回答“没人没通过”好。\n当前自然语言处理研究的发展趋势：\n第一，传统的基于句法-语义规则的理性主义方法受到质疑，随着语料库建设和语料库语言学的崛起，大规模真实文本的处理成为自然语言处理的主要战略目标。\n第二，统计数学方法越来越受到重视，自然语言处理中越来越多地使用机器自动学习的方法来获取语言知识。\n第三，浅层处理与深层处理并重，统计与规则方法并重，形成混合式的系统。\n第四，自然语言处理中越来越重视词汇的作用，出现了强烈的“词汇主义”的倾向。词汇知识库的建造成为了普遍关注的问题。\n第五，统计自然语言处理\n统计自然语言处理运用了推测学、机率、统计的方法来解决上述，尤其是针对容易高度模糊的长串句子，当套用实际文法进行分析产生出成千上万笔可能性时所引发之难题。处理这些高度模糊句子所采用消歧的方法通常运用到语料库以及马可夫模型（Markov models）。统计自然语言处理的技术主要由同样自人工智能下与学习行为相关的子领域：机器学习及资料采掘所演进而成。 ——转自维基百科。","data":"2018年09月09日 22:24:36"}
{"_id":{"$oid":"5d343af662f717dc0659b2a8"},"title":"《自然语言处理（哈工大 关毅 64集视频）》学习笔记：第一章 自然语言处理概论","author":"miniAI学堂","content":"前言\n关毅老师，现为哈工大计算机学院语言技术中心教授，博士生导师。通过认真学习了《自然语言处理（哈工大 关毅 64集视频）》1（来自互联网）的课程，受益良多，在此感谢关毅老师的辛勤工作！为进一步深入理解课程内容，对部分内容进行了延伸学习2 3，在此分享，期待对大家有所帮助，欢迎加我微信（验证：NLP），一起学习讨论，不足之处，欢迎指正。\n\n视频列表：\n01自然语言处理绪论 一\n02自然语言处理绪论 二\n03自然语言处理绪论 三\n04自然语言处理绪论 四\n05自然语言处理绪论 五\n06自然语言处理概论 六\n07自然语言处理概论 七\n08自然语言处理概论 八\n09自然语言处理概论 九\n哈工大（LTP）开始使用\n01自然语言处理绪论 一\n第一章 自然语言处理概论\n自然语言处理团队\n\n\n自然语言处理的重要应用\n机器翻译\n数据库技术\n语音识别\n主要研究室\n语言技术研究中心\n语音处理实验室\n关毅老师主要工程项目\n微软拼音输入法\nBOPOMOFO汉字输入系统\n关毅老师主要科学贡献\n关于相似的研究，提出系统相似度测度的理论模型\n02自然语言处理绪论 二\n简史\n崛起于80年代初的统计自然语言处理技术，已经成为自然语言处理的主流技术，本课程重点介绍统计语言处理技术，特别是基于统计的汉语词法分析技术。\n自然语言处理技术起源于人们对机器翻译技术的研究，从1946年算起至今，已有60多年的历史了。\n“目前一些试用过的用户表示，改进后的翻译服务在质量方面令人惊讶。对于那些从未使用机器翻译的用户来说，他们完全可以通过翻译后的文本理解原文的意思，一些细微的错误并不会引起太大的麻烦。” --Franz Josef Och\n\n董振东：JSCL 2005 ”拿统计机器翻译说事”\n还需要多少年才能实现计算机与人类无障碍地沟通？\n\n1968年的美国影片《2001太空奥德赛》\n机器人HAL和Dave进行了如下对话：\nDave Bownman: Open the pod bay doors, HAL. (Dave Bownman：HAL, 请你打开太空舱的分离门。)\nHAL: I’m sorry Dave, I am afraid I can’t do that. (HAL：对不起，Dave，我恐怕不能这样做。)\n自然语言处理是一个“AI complete”问题\n自然语言理解与计算分子生物学有着深刻的渊源\n生物学中有着至少500年也解决不完的有趣问题 --Donald E. Knuth\n自然语言与人工语言的最大区别在于“歧义”问题\n\n03自然语言处理绪论 三\n歧义问题\n词法分析歧义\n分词\n严守一把手机关了\n严守/ 一把手/ 机关/ 了\n严守一/ 把/ 手机/ 关/ 了\n词性标注\n我/pro 计划/v 考/v 研/n\n我/pro 完成/v 了/aux 计划/n\n语法分析歧义\n咬死了猎人的狗\n那只狼咬死了猎人的狗\n咬死了猎人的狗失踪了\n语义分析歧义\nAt last, a computer that understands you like your mother.\n– 1985 McDonnell-Douglas ad\n含义1：计算机会象你的母亲那样很好地理解你（的语言）\n含义2：计算机理解你喜欢你的母亲\n含义3：计算机会象很好地理解你的母亲那样理解你\n语义用分析歧义\n“你真坏”\n\n音字转换例\nji qi fan yi ji qi ying yong ji qi le ren men ji qi nong hou de xing qu\n几点感性认识\n有点繁琐枯燥\n充满乐趣\n团队合作\n独创精神\n“一只美丽的小花猫”\n04自然语言处理绪论 四\n名言\n取法其上，仅得其中；取法其中，仅得其下；取法其众，得其上。-中国古代思想家\nEvery important idea is simple -列夫.托尔斯泰\nThe grand aim of all science is to cover the greatest number of empirical facts by logical deduction from the smallest number of hypotheses or axioms -爱因斯坦\n工具\nMindjet MindManager （ver 8.0）\nBiblioscape（ver 7.0）\n教材\nChristopher Manning and Hinrich Schutze: Foundations of Statistical Language Processing, MIT press, 1999（有中译本，译者 苑春法 等）\n自然语言处理综论 Daniel Jurafsky \u0026James H. Martin著 冯志伟 孙乐 译\n王晓龙、关毅 《计算机自然语言处理》 清华大学出版社 2005年\n什么是自然语言处理\n定义1：研究在人与人交际中以及在人与计算机交际中的语言问题的一门学科。自然语言处理要研制表示语言能力和语言应用的模型，建立计算框架来实现这样的语言模型，提出相应的方法来不断地完善这样的语言模型，根据这样的语言模型设计各种实用系统，并探讨这些实用系统的评测技术。—Bill Manaris\n本学科的主题与背景：“自然语言处理可以定义为研究在人与人交际中以及在人与计算机交际中的语言问题的一门学科。”\n人人交际中的语言问题\n例如语言不通的问题，促进了机器翻译这一语言处理中最重要的应用之一的发展\n人机交际中的语言问题\n例如语言文字的输入输出问题，促进了智能化人机接口技术的研究\n研究自然语言处理的意义\n从科学研究的角度：探寻人类通过语言来交互信息的奥秘，更好地理解语言本身的内在规律\n从实际应用的角度：构建更加有效的人机交互方式\n05自然语言处理绪论 五\n两类不同的语言处理模型\n能力模型\n基于语言学规则的模型。\n建模步骤\n语言学知识形式化\n形式化规则算法化\n算法实现\n应用模型\n根据不同的语言处理应用而建立的特定语言模型，通常是基于统计的模型。又称“经验主义的”语言模型。上世纪80年代崛起的统计自然语言模型可以归入。\n建模步骤\n大规模真实语料库中获得语言各级语言单位上的统计信息\n依据较低级语言单位上的统计信息运用相关的统计推理技术计算较高级语言单位上的统计信息\n用人工智能等相应的方法来不断地完善这样的语言模型\n规则与统计相结合\n评测技术\n自然语言处理的重要研究专题之一。\n国际公认的自然语言研究竞技场：\nSighan\nConll\nTREC\n什么是自然语言处理\n定义2：是人工智能和语言学的交叉学科，研究自然语言的自动生成与理解。\n图灵实验\n让机器模仿人来回答某些问题，通过实验和观察来判断机器是否具备智能。\n为人工智能确定了奋斗的目标，并指明了前进的方向\n人工智能自诞生之日起就和自然语言理解结下了不解之缘\n06自然语言处理概论 六\n交叉性学科\n自然语言处理是人工智能的重要分支，也是应用语言学的分支。\n语言学\n计算机科学\n数学\n心理学\n信息论\n中文信息处理\n中文语言处理\n计算语言学\n自然语言理解\n…\n07自然语言处理概论 七\n知识内容\n基础\n应用\n资源\n评测\n基础内容\n音位学\n描述音位的结合规律，说明音位怎样形成语素\n举例：“delete file x”-\u003edilet’#fail#eks\n形态学\n描述语素的结合规律，说明语素怎样形成单词\n举例： dilet’#fail#eks-\u003e”delete” “file” “x”\n词汇学\n描述词汇系统的规律，说明单词本身固有的语义特性和语法特性\n举例：\n\n句法学\n描述单词或词组之间的结构规则，说明单词或词组怎样构成句子\n举例：\n\n语义学\n描述句子中各个成分之间的语义关系，怎样从构成句子的各个成分推导出整个句子的语义\n举例：\n\n语用学\n描述与情景有关的情景语义，说明怎样推导出句子具有的与周围话语有关的各种涵义\n举例：delete-file(‘x’)-\u003erm -i x\n应用系统\n常用的中文资源\n北京大学人民日报语料库\n《现代汉语语法信息词典》\n概念层次网络\n知网\n评测内容\n评测方法\n评测量度\n08自然语言处理概论 八\n中文语言处理的发展概况\n从汉字信息处理到汉语信息处理\n汉字信息处理已经基本解决\n汉语信息处理遭遇瓶颈\n从单机信息处理到网络信息处理\n单机信息处理系统\n网络信息处理系统\n汉字排版系统\n词处理\n词是自然语言中最小的有意义的构成单位。\n研究内容\n分词\n词性标注\n名实体识别\n词义消歧等等\n语句处理\n应用\n音字转换\n文本校对\n语音合成\n机器翻译\n篇章处理\n09自然语言处理概论 九\n中文的主要特点\n汉语是大字符集的意音文字\n汉语词与词之间没有空格\n汉语的同音词较多\n汉语没有形态变化\n中文的主要困难\n汉语的语法研究尚未规范化\n汉语的语言学知识的量化与形式化工作滞后\n中文语言处理研究力量分散\n中文语言处理缺乏规范\n科学的评测机制尚未建立\n自然语言处理的主要课题\n基础理论\n概率与统计理论\n统计机器学习理论\n人工智能基本理论\n认知科学理论\n词法分析\n分词\n词性标注\n命名实体识别\n新词发现\n句法分析\n上下文无关文法（概率）\n语义分析\n语义表示\n概念语义网络\n词义消歧\n语用分析\n自然语言生成\n语段分析，对话\n机器翻译\n自然语言处理的主要应用\n哈工大（LTP）开始使用\n开始使用LTP\n本文实验环境为64位win7系统，64位python3.5.2\n哈工大LTP官方主页：\nhttp://ltp.ai/\n哈工大LTP使用文档：\nhttps://ltp.readthedocs.io/zh_CN/latest/install.html\nLTP下载\n（1）LTP项目文件ltp-3.3.1-win-x86.zip\nhttps://github.com/HIT-SCIR/ltp/releases\n（2）LTP模型文件ltp_data_v3.3.1\nhttp://pan.baidu.com/share/link?shareid=1988562907\u0026uk=2738088569\n文件夹放置\n（1）新建一个项目文件夹：C:\\projects\\ltp；\n（2）将模型文件解压后的ltp_data文件夹放入项目文件夹；\n（3）将ltp-3.3.1-win-x86.zip解压后的dll、exe文件全部拷入项目文件夹。\n\n将路径C:\\projects\\ltp 添加到Windows系统环境变量Path中\nPython使用ltp_test\nLTP提供的模型包括：（在ltp_data文件夹）\ncws.model 分句模型，单文件\npos.model 词性标注模型，单文件\nner.model 命名实体识别模型，单文件\nparser.model 依存句法分析模型，单文件\nsrl_data/ 语义角色标注模型，多文件（文件夹srl）\nltp_test是一个整合LTP中各模块的命令行工具。它完成加载模型，依照指定方法执行分析的功能。\n主要参数：线程数、最终步骤、输入文件路径、模型路径、词典路径等。具体可通过CMD运行ltp_test.exe查看。\npython程序简单调用\ntest.txt\n#coding=utf-8 txtName = \"C:\\\\projects\\\\ltp\\\\file\\\\test.txt\" f = open(txtName,'w'，,encoding='utf-8') f.write('我爱北京天安门!') f.close()\n调用ltp_test\n# -*- coding: utf-8 -*- import os project_path = \"C:\\\\projects\\\\ltp\\\\\" # 项目文件夹目录 # 可设置（cws、pos、par、ner）_cmdline，但是注意各自能用的参数，没有的参数请置空\"\" model_exe = \"ltp_test\" # 又如cws_cmdline threads_num = \" --threads \"+str(2) # 更改线程数 input_path = \" --input \"+\"C:\\\\projects\\\\ltp\\\\file\\\\test.txt\" # 输入文件 seg_lexicon = \"\" # 分词用户词典 pos_lexicon = \"\" # 词性标注用户词典 output_path = \"C:\\\\projects\\\\ltp\\\\result\\\\out.txt\" # 输出文件 command = \"cd \"+project_path+\" \u0026 \"+model_exe+threads_num+input_path+seg_lexicon+\" \u003e \"+output_path os.system(command)\n运行过程\n\nout.txt:\n\u003c?xml version=\"1.0\" encoding=\"utf-8\" ?\u003e \u003cxml4nlp\u003e \u003cnote sent=\"y\" word=\"y\" pos=\"y\" ne=\"y\" parser=\"y\" wsd=\"n\" srl=\"y\" /\u003e \u003cdoc\u003e \u003cpara id=\"0\"\u003e \u003csent id=\"0\" cont=\"﻿我爱北京天安门!\"\u003e \u003cword id=\"0\" cont=\"﻿\" pos=\"v\" ne=\"O\" parent=\"-1\" relate=\"HED\" /\u003e \u003cword id=\"1\" cont=\"我\" pos=\"r\" ne=\"O\" parent=\"2\" relate=\"SBV\" /\u003e \u003cword id=\"2\" cont=\"爱\" pos=\"v\" ne=\"O\" parent=\"0\" relate=\"COO\"\u003e \u003carg id=\"0\" type=\"7\u0026#x07;\" beg=\"1\" end=\"1\" /\u003e \u003carg id=\"1\" type=\"\" beg=\"3\" end=\"4\" /\u003e \u003c/word\u003e \u003cword id=\"3\" cont=\"北京\" pos=\"ns\" ne=\"B-Ns\" parent=\"4\" relate=\"ATT\" /\u003e \u003cword id=\"4\" cont=\"天安门\" pos=\"ns\" ne=\"E-Ns\" parent=\"2\" relate=\"VOB\" /\u003e \u003cword id=\"5\" cont=\"!\" pos=\"wp\" ne=\"O\" parent=\"0\" relate=\"WP\" /\u003e \u003c/sent\u003e \u003c/para\u003e \u003c/doc\u003e \u003c/xml4nlp\u003e\nPython使用xxx_cmdline\n（1）cws_cmdline ：分词命令行\n（2）pos_cmdline ：词性标注命令行\n（3）par_cmdline ：句法分析命令行\n（4）ner_cmdline ：命名实体识别命令行\npyltp使用\n安装 pyltp\nhttps://pyltp.readthedocs.io/zh_CN/latest/\n使用 pip 安装\npip install pyltp\n从源代码编译安装\n$ git clone https://github.com/HIT-SCIR/pyltp $ git submodule init $ git submodule update $ python setup.py install\n注意：有时候装python库的时候，会出现Microsoft visual c++ 14.0 is required的问题，欢迎加我微信（验证：NLP），一起研究解决。\n\n-下载 LTP 模型文件\n当前模型版本 - 3.4.0\n使用 pyltp 进行分句示例\n# -*- coding: utf-8 -*- from pyltp import SentenceSplitter sents = SentenceSplitter.split('我爱北京天安门？天安门上太阳升！') # 分句 print( '\\n'.join(sents))\n结果如下\n我爱北京天安门？ 天安门上太阳升！\n参考文献\n《自然语言处理（哈工大 关毅 64集视频）》（来自互联网） ↩︎\n王晓龙、关毅 《计算机自然语言处理》 清华大学出版社 2005年 ↩︎\n哈工大语言技术平台云官网：http://ltp.ai/ ↩︎","data":"2019年01月08日 05:29:20","date":"2019年01月08日 05:29:20"}
{"_id":{"$oid":"5d343af662f717dc0659b2aa"},"title":"自然语言处理-哈工大笔记","author":"wang2008start","content":"文章目录\n词处理\n语句处理\n篇章处理\n当前热点\n统计语言模型\n分词\n语料库\n词性标注\n句法分析\n语料库多机加工系统\n词语搭配识别技术\nN-Gram统计模型\n平滑方法\n动态-自适应-基于缓存的语言模型\n马尔科夫模型\n隐马尔科夫模型\n基于HMM的词性标注\n句法分析\n\n自然语言处理（计算机语言学、自然语言理解）\n涉及：字处理，词处理，语句处理，篇章处理\n词处理\n分词、词性标注、实体识别、词义消歧\n语句处理\n句法分析（Syntactic Analysis）、语义分析（Senmantic Analysis）、机器翻译、语音合成\n篇章处理\n自动文摘\n当前热点\n信息抽取、文本分类、问答系统\n统计语言模型\n分词\n字串均分为词串。难点：未登录词\n最大匹配法／逆向最大匹配法／双向匹配法／最佳匹配法／最少分词法／词网格算法\n语料库\n“生语料“-\u003e自动分词-\u003e语法标注-\u003e句法分析-\u003e语义/语法分析-\u003e语言知识库\n词性标注\n基于规则的词性标注\n基于隐马尔科夫模型HMM的词性标注\n基于转移的词性标注\n基于转移与隐马尔科夫模型相结合的词性标注\n句法分析\n总体结构：\n输入句子-\u003e短语界定自动预测-\u003e括号匹配区间限制-\u003e句法分析-\u003e人工校队-\u003e分析树表示\n自动短语定界：\n确定短语左边界、右边界\n根据上下文信息，把开括号与其相应的比括号对应起来\n根据歧义消解规则和统计信息，消解短语定界的歧义\n生成表示句子结构的成分结构树\n语料库多机加工系统\n自动切词和词性标注子系统\n自动短语定界和句法标注子系统\n自动语义标注子系统\n词语搭配识别技术\n基于频率方法\n基于均值和方差的方法\n基于假设检验的方法\n基于互信息方法\nN-Gram统计模型\nN-Pos 考虑词类，当整个模型只有一个词类时，那么前N-1个词类没有提供任何上下文信息，退化为Unigram模型，如果每一个词都有一个各不相同的词类，N-pos模型等价于N-gram模型。\n语言模型的评价-\u003eKL距离（某一语言的真实概率分布与构造的概率模型的KL距离）\n平滑方法\n拉普拉斯定律（加1平滑）降低已出现的N-gram条件概率分布，以使未出现N-gram条件概率分布非0\nGood-Turing平滑\n简单线性插值平滑\n其他常用平滑方法：Heldout、Back-off、witten-Bell\n动态-自适应-基于缓存的语言模型\n将N个最近出现过的词，存于一个缓存中，作为独立的训练数据。通过这些数据，计算动态频度分布数据，将动态频度分布数据与静态分布数据（由大规模性预料训练得到），通过线性差值的方法相结合。\n马尔科夫模型\n有限历史假设\n时间不变性假设（不随时间改变而改变，稳定性假设）\n（S，pai，A）S：状态集合；pai：初始状态概率；A：转移概率\n隐马尔科夫模型\n(S, K, pai, A, B) S:状态集合；K：输出字符集合；pai：初始状态概率；A：状态转移概率；B：状态转移时输出字符的概率\n\n\n\n\n\n三个基本问题：\n评价。给定一个模型，如果搞笑计算某一输出字符序列的概率\n给定一个输出字符序列o和模型u，如何确定产生这一序列概率最大的状态序列\n\n\n给定一个输出字符的序列o，如何调整模型的参数，使得产生这一序列的概率最大。\n基于HMM的词性标注\n句法分析\n判断输入的次序列嫩否构成一个合乎语法的句子，运用句法规则和其他知识，将输入句子中词之间的线性词库变成一个非线性的结构\n概率上下文无关文法（Probabilistic Stochastic Context Free Grammar） 位置无关、上下文无关、祖先无关\n完全句法分析-\u003e 浅层句法分析（部分分析/组块分析）","data":"2018年05月01日 21:21:40","date":"2018年05月01日 21:21:40"}
{"_id":{"$oid":"5d343af762f717dc0659b2ac"},"title":"初识自然语言处理---自然语言处理研究报告（概述篇）","author":"进击的cxy","content":"主要目的是克服人机对话中的各种限制，使用户能用自己的语言与计算机对话。\n相关概念：\n自然语言是指汉语、英语、法语等人们日常使用的语言，是自然而然的随着人类社会发展演变而来的语言，而不是人造的语言（如程序设计的语言），它是人类学习生活的重要工具。\n自然语言处理，是指用计算机对自然语言的形、音、义等信息进行处理，即对字、词、句、篇章的输入、输出、识别、分析、理解、生成等的操作和加工。\n自然语言处理的具体表现形式包括机器翻译、文本摘要、文本分类、文本校对、信息抽取、语音合成、语音识别等。\n自然语言处理机制涉及两个流程，包括自然语言理解和自然语言生成。自然语言理解是指计算机能够理解自然语言文本的意义，自然语言生成则是指能以自然语言文本来表达给定的意图。\n图 1 自然语言理解层次\n语音分析是要根据音位规则，从语音流中区分出一个个独立的音素，再根据音位形态规则找出音节及其对应的词素或词。\n词法分析的目的是找出词汇的各个词素，从中获得语言学的信息。\n句法分析是对句子和短语的结构进行分析，目的是要找出词、短语等的相互关系以及各自在句中的作用。\n语义分析的目的是找出词义、结构意义及其结合意义，从而确定语言所表达的真正含义或概念。\n语用分析则是研究语言所存在的外界环境对语言使用者所产生的影响。\n判断计算机是否理解了某种自然语言的具体判别标准：问答（正确回答输入文本中的有关问题）；文摘生成（有能力生成输入文本的摘要）；释义（用不同的词语和句型来复述其输入的文本）；翻译（把一种语言翻译成另一种语言）。\n发展历程：\n时间\n标志\n意义\n局限\n1950年\n图灵测试\n自然语言处理思想的开端\n20世纪50年代到70年代\n基于规则的方法\n（认为自然语言处理的过程和人类学习认知一门语言的过程是类似的）\n理性主义思潮阶段\n规则不可能覆盖所有语句\n开发者不仅要精通计算机还要精通语言学\n70年代以后\n基于统计的方法逐渐代替了基于规则的方法\n向经验主义过渡\n2008年到现在\n引入深度学习来做 NLP 研究\n深度学习与自然语言处理的结合推向了高潮\n我国现状\n目前自然语言处理的研究可以分为基础性研究和应用性研究两部分，语音和文本是两类研究的重点。基础性研究主要涉及语言学、数学、计算机学科等领域，相对应的技术有消除歧义、语法形式化等。应用性研究则主要集中在一些应用自然语言处理的领域，例如信息检索、文本分类、机器翻译等。由于我国基础理论即机器翻译的研究起步较早，且基础理论研究是任何应用的理论基础，所以语法、句法、语义分析等基础性研究历来是研究的重点，而且随着互联网网络技术的发展，智能检索类研究近年来也逐渐升温。\n业界发展\n微软亚洲研究院\n微软亚洲研究院1998年成立自然语言计算组，研究内容包括多国语言文本分析、机器翻译、跨语言信息检索和自动问答系统等。这些研究项目研发了一系列实用成果，如 IME、对联游戏、Bing 词典、Bing 翻译器、语音翻译、搜索引擎等，为微软产品做出了重大的贡献，并且在 NLP 顶级会议，例如 ACL，COLING 等会议上发表了许多论文。\n神经网络机器翻译，人机对话-----小娜，聊天机器人小冰。。。。。。\nGoogle\nGoogle 对自然语言处理的研究侧重于应用规模、跨语言和跨领域的算法，其成果在 Google 的许多方面都被使用，提升了用户在搜索、移动、应用、广告、翻译等方面的体验。\n机器翻译，知识图谱，语音识别。。。。。\nFacebook\nFacebook 语言技术小组不断改进自然语言处理技术以改善用户体验，致力于机器翻译、语音识别和会话理解。2016年，Facebook 首次将29层深度卷积神经网络用于自然语言处理，2017年，Facebook 团队使用全新的卷积神经网络进行翻译，以9倍于以往循环神经网络的速度实现了目前最高的准确率。\n语音识别，文本处理。。。。。\n百度\n百度自然语言处理部是百度最早成立的部门之一，研究涉及深度问答、阅读理解、智能写作、对话系统、机器翻译、语义计算、语言分析、知识挖掘、个性化、反馈学习等。其中，百度自然语言处理在深度问答方向经过多年打磨，积累了问句理解、答案抽取、观点分析与聚合等方面的一整套技术方案，目前已经在搜索、度秘等多个产品中实现应用。篇章理解通过篇章结构分析、主体分析、内容标签、情感分析等关键技术实现对文本内容的理解，目前，篇章理解的关键技术已经在搜索、资讯流、糯米等产品中实现应用。百度翻译目前支持全球28种语言，覆盖756个翻译方向，支持文本、语音、图像等翻译功能，并提供精准人工翻译服务，满足不同场景下的翻译需求，在多项翻译技术取得重大突破，发布了世界上首个线上神经网络翻译系统，并获得2015年度国家科技进步奖。\n阿里巴巴\n阿里自然语言处理为其产品服务，在电商平台中构建知识图谱实现智能导购，同时进行全网用户兴趣挖掘，在客服场景中也运用自然语言处理技术打造机器人客服，例如蚂蚁金融智能小宝、淘宝卖家的辅助工具千牛插件等，同时进行语音识别以及后续分析。阿里的机器翻译主要与其国家化电商的规划相联系，可以进行商品信息翻译、广告关键词翻译、买家采购需求以及即时通信翻译等，语种覆盖中文、荷兰语、希伯来语等语种，2017年初阿里正式上线了自主开发的神经网络翻译系统，进一步提升了其翻译质量。\n腾讯\nAI Lab 是腾讯的人工智能实验室，研究领域包括计算机视觉、语音识别、自然语言处理、机器学习等。其研发的腾讯文智自然语言处理基于并行计算、分布式爬虫系统，结合独特的语义分析技术，可满足自然语言处理、转码、抽取、数据抓取等需求，同时，基于文智 API 还可以实现搜索、推荐、舆情、挖掘等功能。在机器翻译方面，2017年腾讯宣布翻译君上线“同声传译”新功能，用户边说边翻的需求得到满足，语音识别+NMT 等技术的应用保证了边说边翻的速度与精准性。\n京东\n京东 AI 开放平台基本上由模型定制化平台和在线服务模块构成，其中在线服务模块包括计算机视觉、语音交互、自然语言处理和机器学习等。京东 AI 开放平台计划通过建立算法技术、应用场景、数据链间的连接，构建京东 AI 发展全价值链，实现 AI 能力平台化。","data":"2018年12月28日 21:16:00","date":"2018年12月28日 21:16:00"}
{"_id":{"$oid":"5d343af762f717dc0659b2ae"},"title":"自然语言处理语料库","author":"张大鹏的博客","content":"搜狗实验室数据资源\nhttp://www.sogou.com/labs/resource/list_pingce.php\n自然语言处理与信息检索共享平台\nhttp://www.nlpir.org/?action-category-catid-28\nChinese Word Vectors：目前最全的中文预训练词向量集合\nhttps://www.toutiao.com/a6555683864981799427/?tt_from=mobile_qq\u0026utm_campaign=client_share\u0026timestamp=1526372845\u0026app=news_article\u0026utm_source=mobile_qq\u0026iid=32264431851\u0026utm_medium=toutiao_android\n图像分类数据集：\nCIFAR10(The Canadian Institute For Advanced Research)是衡量机器学习模型好坏的一个公共数据集，主要目的是将32x32的RGB图片分类成以下的10个类型：\nairplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck\nhttp://academictorrents.com/details/463ba7ec7f37ed414c12fbb71ebf6431eada2d7a\n参考：\nhttp://blog.csdn.net/u012052268/article/details/78035272\n其它语料：\nhttp://mp.weixin.qq.com/s?__biz=MzIxODM4MjA5MA==\u0026mid=2247486225\u0026idx=1\u0026sn=080dfcedbdc9522f919e67e32d550250\u0026chksm=97ea2174a09da8623b8f94466d495cd98e54259891a3eee58586d3ec1f963e4e68df80387e88\u0026mpshare=1\u0026scene=23\u0026srcid=0408i5wriTkN6FUyZpOXovPo#rd","data":"2018年03月22日 14:24:28"}
{"_id":{"$oid":"5d343af762f717dc0659b2b0"},"title":"自然语言处理研究报告","author":"GitChat的博客","content":"内容简介\n自然语言处理是人工智能的一个重要应用领域，也是新一代计算机必须研究的课题。它的主要目的是克服人机对话中的各种限制，使用户能用自己的语言与计算机对话。因此，本研究报告对自然语言进行了简单梳理，包括以下内容：\n自然语言处理概念\n自然语言处理研究情况\n自然语言处理领域专家介绍\n自然语言处理的应用及趋势预测\n作者简介\nAMiner 平台，由清华大学计算机系研发，拥有我国完全自主知识产权。平台包含了超过2.3亿学术论文/专利和1.36亿学者的科技图谱，提供学者评价、专家发现、智能指派、学术地图等科技情报专业化服务。\n系统2006年上线，吸引了全球220个国家/地区1000多万独立 IP 访问，数据下载量230万次，年度访问量超过1100万，成为学术搜索和社会网络挖掘研究的重要数据和实验平台。\n系统相关核心技术申请专利50余项，发表论文500余篇，其中 SCI 论文110篇，编著英文论著两部，Google 引用超过11000次，SCI 他引超过2200次。\n项目成果及核心技术应用于工程院、科技部、自然基金委、华为、腾讯、阿里巴巴、百度等国内外20多家企事业单位，为各单位的系统建设及产品升级提供了重要数据及技术支撑。\nAMiner 唯一官方微信公众号：学术头条（ID：SciTouTiao）。\nAMiner 官方网站 https://www.aminer.cn/\n版权声明\nAMiner 研究报告提供给订阅用户使用，仅限于用户内部使用。未获得 AMiner 团队授权，任何人和单位不得以任何方式在任何媒体上（包括互联网）公开发布、复制，且不得以任何方式将研究报告的内容提供给其他单位或个人使用。如引用、刊发，需注明出处为“AMiner.org”，且不得对本报告进行有悖原意的删节与修改。\nAMiner 研究报告是基于 AMiner 团队及其研究员认可的研究资料，所有资料源自 AMiner 后台程序对大数据的自动分析得到，本研究报告仅作为参考，AMiner 团队不保证所分析得到的准确性和完整性，也不承担任何投资者因使用本产品与服务而产生的任何责任。\n本书内容\n摘要\n自然语言处理是人工智能的一个重要应用领域，也是新一代计算机必须研究的课题。它的主要目的是克服人机对话中的各种限制，使用户能用自己的语言与计算机对话。因此，本研究报告对自然语言进行了简单梳理，包括以下内容：\n自然语言处理概念。首先对自然语言处理进行定义，接着对自然语言的发展历程进行了梳理，对我国自然语言处理现状进行了简单介绍，对自然语言处理业界情况进行介绍。\n自然语言处理研究情况。依据2016年中文信息学会发布的中文信息处理发展报告对自然语言处理研究中的重要技术进行介绍。\n自然语言处理领域专家介绍。利用 AMiner 大数据对自然语言处理领域专家进行深入挖掘，对国内外自然语言处理知名实验室及其主要负责人进行介绍。\n自然语言处理的应用及趋势预测。自然语言处理在现实生活中应用广泛，目前的应用集中在语言学、数据处理、认知科学以及语言工程等领域，在介绍相关应用的基础上，对机器翻译未来的发展趋势做出了相应的预测。\n第一章 概述篇\n第二章 技术篇\n第三章 技术篇\n第四章 应用篇\n第五章 趋势篇\n参考文献\n阅读全文: http://gitbook.cn/gitchat/geekbook/5b988b4eca9910654c0823f5","data":"2018年11月06日 11:55:09"}
{"_id":{"$oid":"5d343af862f717dc0659b2b2"},"title":"智能语音助手的工作原理是？先了解自然语言处理(NLP)与自然语言生成(NLG)","author":"数据星河","content":"智能语音助手的工作原理是？先了解自然语言处理(NLP)与自然语言生成(NLG)\n语音助手越来越像人类了，与人类之间的交流不再是简单的你问我答，不少语音助手甚至能和人类进行深度交谈。在交流的背后，离不开自然语言处理（NLP）和自然语言生成（NLG）这两种基础技术。机器学习的这两个分支使得语音助手能够将人类语言转换为计算机命令，反之亦然。\n\n这两种技术有什么差异？工作原理是什么？\nNLP vs NLG：了解基本差异\n什么是NLP？\nNLP指在计算机读取语言时将文本转换为结构化数据的过程。简而言之，NLP是计算机的阅读语言。可以粗略地说，在NLP中，系统摄取人语，将其分解，分析，确定适当的操作，并以人类理解的语言进行响应。\n\nNLP结合了计算机科学、人工智能和计算语言学，涵盖了以人类理解的方式解释和生成人类语言的所有机制：语言过滤、情感分析、主题分类、位置检测等。\n什么是NLG？\n自然语言处理由自然语言理解（NLU）和自然语言生成（NLG）构成。NLG是计算机的“编写语言”，它将结构化数据转换为文本，以人类语言表达。即能够根据一些关键信息及其在机器内部的表达形式，经过一个规划过程，来自动生成一段高质量的自然语言文本。\n\nNLP vs NLG：聊天机器人的工作方式\n人类谈话涉及双向沟通的方式，聊天机器人也一样，只是沟通渠道略有不同——您是与机器交谈。当给机器人发送消息时，它会将其拾取并使用NLP，机器将文本转换为自身的编码命令。然后将该数据发送到决策引擎。\n在整个过程中，计算机将自然语言转换为计算机理解的语言，处理，识别语音。语音识别系统常用的是Hidden Markov模型（HMM），它将语音转换为文本以确定用户所说的内容。通过倾听您所说的内容，将其分解为小单元，并对其进行分析以生成文本形式的输出或信息。\n此后的关键步骤是自然语言理解（NLU），如上文所说，它是NLP的另一个子集，试图理解文本形式的含义。重要的是计算机要理解每个单词是什么，这是由NLU执行的部分。在对词汇、语法和其他信息进行筛选时，NLP算法使用统计机器学习、应用自然语言的语法规则，并确定所说的最可能的含义。\n\n另一方面，NLG是一种利用人工智能和计算语言学生成自然语言的系统。它还可以将该文本翻译成语音。NLP系统首先确定要翻译成文本的信息，然后组织表达结构，再使用一组语法规则，NLG就能系统形成完整的句子并读出来。\n应用\n语音助手只是NLP众多应用程序之一。它还可用于网络安全文章、白皮书、科研等领域。例如，NLP对在线内容进行情绪分析，以改进服务并为客户提供更好的产品。\n而NLG通常用于Gmail，它可以为您自动创建答复。创建公司数据图表的描述说明时，NLG也是很好的工具。\n说NLP和NLG完全不相关，也不正确，因为NLP和NLG相当于学习中的阅读、写作过程，还是有内在关联的。","data":"2018年10月26日 16:21:47","date":"2018年10月26日 16:21:47"}
{"_id":{"$oid":"5d343af862f717dc0659b2b4"},"title":"Python 自然语言处理（基于jieba分词和NLTK）","author":"HuangZhang_123","content":"----------欢迎加入学习交流QQ群：657341423\n自然语言处理是人工智能的类别之一。自然语言处理主要有那些功能？我们以百度AI为例\n\n从上述的例子可以看到，自然语言处理最基本的功能是词法分析，词法分析的功能主要有：\n分词分句\n词语标注\n词法时态（适用于英文词语）\n关键词提前（词干提取）\n由于英文和中文在文化上存在巨大的差异，因此Python处理英文和中文需要使用不同的模块，中文处理推荐使用jieba模块，英文处理推荐使用nltk模块。模块安装方法可自行搜索相关资料。\n英文处理\nimport nltk f = open('aa.txt','r',encoding='utf-8') text = f.read() f.close() ---------- # sent_tokenize 文本分句处理，text是一个英文句子或文章 value = nltk.sent_tokenize(text) print(value) # word_tokenize 分词处理,分词不支持中文 for i in value: words = nltk.word_tokenize(text=i) print(words) ---------- # pos_tag 词性标注,pos_tag以一组词为单位，words是列表组成的词语列表 words = ['My','name','is','Lucy'] tags = nltk.pos_tag(words) print(tags) ---------- # 时态，过去词，进行时等 # 词语列表的时态复原，如果单词是全变形的无法识别 from nltk.stem import PorterStemmer data = nltk.word_tokenize(text=\"worked presumably goes play,playing,played\",language=\"english\") ps = PorterStemmer() for w in data: print(w,\":\",ps.stem(word=w)) # 单个词语的时态复原，如果单词是全变形的无法识别 from nltk.stem import SnowballStemmer snowball_stemmer = SnowballStemmer('english') a = snowball_stemmer.stem('plays') print(a) # 复数复原，如果单词是全变形的无法识别 from nltk.stem import WordNetLemmatizer wordnet_lemmatizer = WordNetLemmatizer() a = wordnet_lemmatizer.lemmatize('leaves') print(a) ---------- # 词干提取,提前每个单词的关键词，然后可进行统计，得出词频 from nltk.stem.porter import PorterStemmer porter = PorterStemmer() a = porter.stem('pets insurance') print(a) ---------- from nltk.corpus import wordnet word = \"good\" # 返回一个单词的同义词和反义词列表 def Word_synonyms_and_antonyms(word): synonyms = [] antonyms = [] list_good = wordnet.synsets(word) for syn in list_good: # 获取同义词 for l in syn.lemmas(): synonyms.append(l.name()) # 获取反义词 if l.antonyms(): antonyms.append(l.antonyms()[0].name()) return (set(synonyms), set(antonyms)) # 返回一个单词的同义词列表 def Word_synonyms(word): list_synonyms_and_antonyms = Word_synonyms_and_antonyms(word) return list_synonyms_and_antonyms[0] # 返回一个单词的反义词列表 def Word_antonyms(word): list_synonyms_and_antonyms = Word_synonyms_and_antonyms(word) return list_synonyms_and_antonyms[1] print(Word_synonyms(word)) print(Word_antonyms(word)) ---------- # 造句 print(wordnet.synset('name.n.01').examples()) # 词义解释 print(wordnet.synset('name.n.01').definition()) ---------- from nltk.corpus import wordnet # 词义相似度.'go.v.01'的go为词语，v为动词 # w1 = wordnet.synset('fulfil.v.01') # w2 = wordnet.synset('finish.v.01') # 'hello.n.01'的n为名词 w1 = wordnet.synset('hello.n.01') w2 = wordnet.synset('hi.n.01') # 基于路径的方法 print(w1.wup_similarity(w2))# Wu-Palmer 提出的最短路径 print(w1.path_similarity(w2))# 词在词典层次结构中的最短路径 print(w1.lch_similarity(w2))# Leacock Chodorow 最短路径加上类别信息 # 基于互信息的方法 from nltk.corpus import genesis # 从语料库加载信息内容 # brown_ic = wordnet_ic.ic（'ic-brown.dat'） # nltk自带的语料库创建信息内容词典 genesis_ic = wordnet.ic(genesis,False,0.0) print(w1.res_similarity(w2,genesis_ic)) print(w1.jcn_similarity(w2,genesis_ic)) print(w1.lin_similarity(w2,genesis_ic))\n由于上述的方法是建立在语料库中，有时候一些不被记录的单词可能无法识别或标注。这时候需要自定义词性标注器，词性标注器的类型有几种，具体教程可以看——\u003e自定义词性标注器\n中文处理\nimport jieba import jieba.analyse f = open('aa.txt','r',encoding='utf-8') text = f.read() f.close() ---------- # 分词 seg_list = jieba.cut(text, cut_all=True) print(\"Full Mode: \" + \"/ \".join(seg_list)) # 全模式 seg_list = jieba.cut(text, cut_all=False) print(\"Default Mode: \" + \"/ \".join(seg_list)) # 精确模式 seg_list = jieba.cut_for_search(text) # 搜索引擎模式 print(\", \".join(seg_list)) ---------- # 关键字提取 # 基于TF-IDF算法的关键词抽取 # sentence 为待提取的文本 # topK 为返回几个 TF/IDF 权重最大的关键词，默认值为 20 # withWeight 为是否一并返回关键词权重值，默认值为 False # allowPOS 仅包括指定词性的词，默认值为空，即不筛选 keywords = jieba.analyse.extract_tags(sentence=text, topK=20, withWeight=True, allowPOS=('n','nr','ns')) # 基于TextRank算法的关键词抽取 # keywords = jieba.analyse.textrank(text, topK=20, withWeight=True, allowPOS=('n','nr','ns')) for item in keywords: print(item[0],item[1]) ---------- # 词语标注 import jieba.posseg # 新建自定义分词器，tokenizer 参数可指定内部使用的 jieba.Tokenizer 分词器。jieba.posseg.dt 为默认词性标注分词器。 posseg = jieba.posseg.POSTokenizer(tokenizer=None) words = posseg.cut(text) for word, flag in words: print('%s %s' % (word, flag))\njieba分词也是基于语料库，我们可以对原有的语料库添加词语，或者导入自定义的语料文件，如下所示：\n# 对原有的语料库添加词语 jieba.add_word(word, freq=None, tag=None) # 导入语料文件 jieba.load_userdict('disney.txt')\n语料文件格式如下：每行分三个部分（用空格隔开），词语 词频（可省） 词性（可省）。ns是词语标记，词语和标注之间用空格隔开，txt文件格式为uft-8\n\njieba更多教程——\u003ejieba教程","data":"2018年05月11日 11:39:48"}
{"_id":{"$oid":"5d343af962f717dc0659b2b7"},"title":"如何成为一名自然语言处理工程师","author":"csdn人工智能","content":"作者 | 兰红云\n责编 | 何永灿\n自然语言处理和大部分的机器学习或者人工智能领域的技术一样，是一个涉及到多个技能、技术和领域的综合体。\n所以自然语言处理工程师会有各种各样的背景，大部分都是在工作中自学或者是跟着项目一起学习的，这其中也不乏很多有科班背景的专业人才，因为技术的发展实在是日新月异，所以时刻要保持着一种强烈的学习欲望，让自己跟上时代和技术发展的步伐。本文作者从个人学习经历出发，介绍相关经验。\n一些研究者将自然语言处理（NLP，Natural Language Processing）和自然语言理解（NLU，Natural Language Understanding）区分开，在文章中我们说的NLP是包含两者的，并没有将两者严格分开。\n\n图1 自然语言处理工程师技能树\n自然语言处理学习路线\n数学基础\n数学对于自然语言处理的重要性不言而喻。当然数学的各个分支在自然语言处理的不同阶段也会扮演不同的角色，这里介绍几个重要的分支。\n代数\n代数作为计算数学里面很重要的一个分支，在自然语言处理中也有举足轻重的作用。这一部分需要重点关注矩阵处理相关的一些知识，比如矩阵的SVD、QR分解，矩阵逆的求解，正定矩阵、稀疏矩阵等特殊矩阵的一些处理方法和性质等等。\n对于这一部分的学习，既可以跟着大学的代数书一起学习，也可以跟着网上的各种公开课一起学习，这里既可以从国内的一些开放学习平台上学，也可以从国外的一些开放学习平台上学。这里放一个学习的链接，网易公开课的链接：https://c.open.163.com/search/search.htm?query=线性代数#/search/all。（其他的资料或者平台也都OK）。\n概率论\n在很多的自然语言处理场景中，我们都是算一个事件发生的概率。这其中既有特定场景的原因，比如要推断一个拼音可能的汉字，因为同音字的存在，我们能计算的只能是这个拼音到各个相同发音的汉字的条件概率。也有对问题的抽象处理，比如词性标注的问题，这个是因为我们没有很好的工具或者说能力去精准地判断各个词的词性，所以就构造了一个概率解决的办法。\n对于概率论的学习，既要学习经典的概率统计理论，也要学习贝叶斯概率统计。相对来说，贝叶斯概率统计可能更重要一些，这个和贝叶斯统计的特性是相关的，因其提供了一种描述先验知识的方法。使得历史的经验使用成为了可能，而历史在现实生活中，也确实是很有用的。比如朴素贝叶斯模型、隐马尔卡模型、最大熵模型，这些我们在自然语言处理中耳熟能详的一些算法，都是贝叶斯模型的一种延伸和实例。\n这一部分的学习资料，也非常丰富，这里也照例对两种概率学习各放一个链接，统计学导论http://open.163.com/movie/2011/5/M/O/M807PLQMF_M80HQQGMO.html，贝叶斯统计：https://www.springboard.com/blog/probability-bayes-theorem-data-science/。\n信息论\n信息论作为一种衡量样本纯净度的有效方法。对于刻画两个元素之间的习惯搭配程度非常有效。这个对于我们预测一个语素可能的成分（词性标注），成分的可能组成（短语搭配）非常有价值，所以这一部分知识在自然语言处理中也有非常重要的作用。\n同时这部分知识也是很多机器学习算法的核心，比如决策树、随机森林等以信息熵作为决策桩的一些算法。对于这部分知识的学习，更多的是要理解各个熵的计算方法和优缺点，比如信息增益和信息增益率的区别，以及各自在业务场景中的优缺点。照例放上一个链接：http://open.163.com/special/opencourse/information.html。\n数据结构与算法\n这部分内容的重要性就不做赘述了。学习了上面的基础知识，只是万里长征开始了第一步，要想用机器实现对自然语言的处理，还是需要实现对应的数据结构和算法。这一部分也算是自然语言处理工程师的一个看家本领。这一部分的内容也是比较多的，这里就做一个简单的介绍和说明。\n首先数据结构部分，需要重点关注链表、树结构和图结构（邻接矩阵）。包括各个结构的构建、操作、优化，以及各个结构在不同场景下的优缺点。当然大部分情况下，可能使用到的数据结构都不是单一的，而是有多种数据结构组合。比如在分词中有非常优秀表现的双数组有限状态机就使用树和链表的结构，但是实现上采用的是链表形式，提升了数据查询和匹配的速度。在熟练掌握各种数据结构之后，就是要设计良好的算法了。\n伴随着大数据的不断扩张，单机的算法越来越难发挥价值，所以多数场景下都要研发并行的算法。这里面又涉及到一些工具的应用，也就是编程技术的使用。例如基于Hadoop的MapReduce开发和Spark开发都是很好的并行化算法开发工具，但是实现机制却有很大的差别，同时编程的便利程度也不一样。\n当然这里面没有绝对的孰好孰坏，更多的是个人使用的习惯和业务场景的不同而不同。比如两个都有比较成熟的机器学习库，一些常用的机器学习算法都可以调用库函数实现，编程语言上也都可以采用Java，不过Spark场景下使用Scala会更方便一些。因为这一部分是偏实操的，所以我的经验会建议实例学习的方法，也就是跟着具体的项目学习各种算法和数据结构。最好能对学习过的算法和数据结构进行总结回顾，这样可以更好的得到这种方法的精髓。因为基础的元素，包括数据结构和计算规则都是有限的，所以多样的算法更多的是在不同的场景下，对于不同元素的一个排列组合，如果能够融会贯通各个基础元素的原理和使用，不管是对于新知识的学习还是对于新解决方案的构建都是非常有帮助的。\n对于工具的选择，建议精通一个，对于其他工具也需要知道，比如精通Java和MapReduce，对于Spark和Python也需要熟悉，这样可以在不同的场景下使用不同的工具，提升开发效率。这一部分实在是太多、太广，这里不能全面地介绍，大家可以根据自己的需求，选择合适的学习资料进行学习。这里给出一个学习基础算法（包含排序、图、字符串处理等）的课程链接：https://algs4.cs.princeton.edu/home/。\n语言学\n这一部分就更多是语文相关的知识，比如一个句子的组成成分包括：主、谓、宾、定、状、补等。对于各个成分的组织形式也是多种多样。比如对于主、谓、宾，常规的顺序就是：主语→谓语→宾语。当然也会有：宾语→主语→宾语（饭我吃了）。这些知识的积累有助于我们在模型构建或者解决具体业务的时候，能够事半功倍，因为这些知识一般情况下，如果要被机器学习，都是非常困难的，或者会需要大量的学习素材，或许在现有的框架下，机器很难学习到。如果把这些知识作为先验知识融合到模型中，对于提升模型的准确度都是非常有价值的。\n在先期的研究中，基于规则的模型，大部分都是基于语言模型的规则进行研究和处理的。所以这一部分的内容对于自然语言处理也是非常重要的。但是这部分知识的学习就比较杂一些，因为大部分的自然语言处理工程师都是语言学专业出身，所以对于这部分知识的学习，大部分情况都是靠碎片化的积累，当然也可以花一些精力，系统性学习。对于这部分知识的学习，个人建议可以根据具体的业务场景进行学习，比如在项目处理中要进行同义词挖掘，那么就可以跟着“百科”或者“搜索引擎”学习同义词的定义，同义词一般会有什么样的形式，怎么根据句子结构或者语法结构判断两个词是不是同义词等等。\n深度学习\n随着深度学习在视觉和自然语言处理领域大获成功，特别是随着AlphaGo的成功，深度学习在自然语言处理中的应用也越来越广泛，大家对于它的期望也越来越高。所以对于这部分知识的学习也几乎成为了一个必备的环节（实际上可能是大部分情况，不用深度学习的模型，也可以解决很多业务）。\n对于这部分知识，现在流行的几种神经网络都是需要学习和关注的，特别是循环神经网络，因为其在处理时序数据上的优势，在自然语言处理领域尤为收到追捧，这里包括单项RNN、双向RNN、LSTM等形式。同时新的学习框架，比如对抗学习、增强学习、对偶学习，也是需要关注的。其中对抗学习和对偶学习都可以显著降低对样本的需求，这个对于自然语言处理的价值是非常大的，因为在自然语言处理中，很重要的一个环节就是样本的标注，很多模型都是严重依赖于样本的好坏，而随着人工成本的上升，数据标注的成本越来越高，所以如果能显著降低标注数据需求，同时提升效果，那将是非常有价值的。\n现在还有一个事物正在如火如荼地进行着，就是知识图谱，知识图谱的强大这里就不再赘述，对于这部分的学习可能更多的是要关注信息的链接、整合和推理的技术。不过这里的每一项技术都是非常大的一个领域，所以还是建议从业务实际需求出发去学习相应的环节和知识，满足自己的需求，链接http://www.chinahadoop.cn/course/918。\n自然语言处理现状\n随着知识图谱在搜索领域的大获成功，以及知识图谱的推广如火如荼地进行中，现在的自然语言处理有明显和知识图谱结合的趋势。特别是在特定领域的客服系统构建中，这种趋势就更明显，因为这些系统往往要关联很多领域的知识，而这种知识的整合和表示，很适合用知识图谱来解决。随着知识图谱基础工程技术的完善和进步，对于图谱构建的容易程度也大大提高，所以自然语言处理和知识图谱的结合就越来越成为趋势。\n语义理解仍然是自然语言处理中一个难过的坎。目前各项自然语言处理技术基本已经比较成熟，但是很多技术的效果还达不到商用的水平。特别是在语义理解方面，和商用还有比较大的差距。比如聊天机器人现在还很难做到正常的聊天水平。不过随着各个研究机构和企业的不断努力，进步也是飞速的，比如微软小冰一直在不断的进步。\n对于新的深度学习框架，目前在自然语言处理中的应用还有待进一步加深和提高。比如对抗学习、对偶学习等虽然在图像处理领域得到了比较好的效果，但是在自然语言处理领域的效果就稍微差一些，这里面的原因是多样的，因为没有深入研究，就不敢妄言。\n目前人机对话、问答系统、语言翻译是自然语言处理中的热门领域，各大公司都有了自己的语音助手，这一块也都在投入大量的精力在做。当然这些上层的应用，也都依赖于底层技术和模型的进步，所以对于底层技术的研究应该说一直是热门，在未来一段时间应该也都还是热门。之前听一个教授讲过一个故事，他是做parser的，开始的时候很火，后来一段时间因为整个自然语言处理的效果差强人意，所以作为其中一个基础工作的parser就随之受到冷落，曾经有段时间相关的期刊会议会员锐减，但是最近整个行业的升温，这部分工作也随之而受到重视。不过因为他一直坚持在这个领域，所以建树颇丰，最近也成为热门领域和人物。\n所以在最后引用一位大牛曾经说过的话：“任何行业或者领域做到头部都是非常有前途的，即使是打球，玩游戏。”（大意）\n个人经验\n笔者是跟着项目学习自然语言处理的，非科班出身，所以的经验难免会有偏颇，说出来仅供大家参考, 有不足和纰漏的地方敬请指正。\n知识结构\n要做算法研究，肯定需要一定的知识积累，对于知识积累这部分，我的经验是先学数学理论基础，学的顺序可以是代数→概率论→随机过程。当然这里面每一科都是很大的一个方向，学的时候不必面面俱到，所有都深入理解，但是相对基础的一些概念和这门学科主要讲的是什么问题一定要记住。\n在学习了一些基础数学知识之后，就开始实现——编写算法。这里的算法模型，建议跟着具体的业务来学习和实践，比如可以先从识别垃圾邮件这样的demo进行学习实验，这样的例子在网上很容易找到，但是找到以后，一定不要看看就过去，要一步一步改写拿到的demo，同时可以改进里面的参数或者实现方法，看看能不能达到更好的效果。个人觉得学习还是需要下苦功夫一步一步模仿，然后改进，才能深入的掌握相应的内容。对于学习的资料，上学时期的各个教程即可。\n工具\n工欲善其事必先利其器，所以好的工具往往能事半功倍。在工具的选择上，个人建议，最高优先级的是Python，毕竟其的宣传口语是：人生苦短，请用Python。第二优先级的是Java，基于Java可以和现有的很多框架进行直接交互，比如Hadoop、Spark等等。对于工具的学习两者还是有很大的差别的，Python是一个脚本语言，所以更多的是跟着“命令”学，也就是要掌握你要实现什么目的来找具体的执行语句或者命令，同时因为Python不同版本、不同包对于同一个功能的函数实现差别也比较大，所以在学习的时候，要多试验，求同存异。\n对于Java就要学习一些基础的数据结构，然后一步一步的去编写自己的逻辑。对于Python当然也可以按照这个思路，Python本身也是一个高级编程语言，所以掌握了基础的数据结构之后，也可以一步一步的实现具体的功能，但是那样好像就失去了slogan的意义。\n紧跟时代\n自然语言处理领域也算是一个知识密集型的行业，所以知识的更新迭代非常的快，要时刻关注行业、领域的最新进展。这个方面主要就是看一些论文和关注一些重要的会议，对于论文的获取，Google Scholar、arxiv都是很好的工具和资源（请注意维护知识产权）。会议就更多了KDD、JIST、CCKS等等。\n作者简介：兰红云，滴滴算法工程师，负责算法策略相关工作。主要专注于机器学习和自然语言处理方向。著有《自然语言处理技术入门与实践》。\n本文来源公众号“人工智能头条”，未经允许不得转载。\n如何成为一名\n机器学习算法工程师\n推荐系统工程师\n对话系统工程师\n数据科学家\n异构并行计算工程师\n语音识别工程师\n求取技术突破：深度学习的专业路径\n实战路径：程序员的机器学习进阶方法","data":"2018年01月18日 12:24:18","date":"2018年04月12日 10:44:08"}
{"_id":{"$oid":"5d343af962f717dc0659b2b9"},"title":"浅谈自然语言与财务联系","author":"Kevin","content":"一、会计文本分析\n随着人工智能自然语言处理的发展，近年，文本信息逐渐成为国外会计实证研究的热点，许多学者开始致力于运用文本分析方法来解决会计与财务问题，并取得了众多有价值的研究成果。这里鄙人浅谈一下二者的联系以及如何运用。\n会计文本 一般指由公司发布的具有会计相关性的文本信息。关注对象还包括分析师研究报告、媒体新闻报道、互联网论坛上的帖子，具体来说，狭义会计文本还包括公司披露的年报、季报、招股说明书、季度盈余公告、管理层盈余预告以及电话会议纪要文本；广义的会计文本还包括分析师的研究报告、媒体的新闻报道以及投资者通过各种渠道发表的观点与评论。\n总结的现有的文献我们可以发现，已被量化的会计文本特征有九个，按照是否与内容相关可以分为两类显然，语调、可读性、重复性、管理者特征与文本内容无关，而风险、竞争、虚假性、融资约束则属于文本内容的一部分。\n语调\n语调是会计文本分析最基本的特征，有乐观或者悲观、正面或负面、积极或消极两种对立的感觉构成。中性语调可视为第三种语调，因为大部分词句既不乐观也不悲观。字典法是度量语调的基本方法，研究者通过对乐观和悲观两类单词进行词频统计和比较得到文本整体语调。另一种度量方法是朴素贝叶斯算法。\n文本分析方法\n一、字典法。字典法实质上是一种词频统计法，它基于预设的字典和规则将目标文档中的单词逐一映射到各个集合中，经过统计计算得到文本的量化特征。字典可分为通用、专用和自编三种类型。通用性字典广泛应用于众多研究领域，而不限于会计研究。由于某些词汇在会计用语有其特殊的含义，因此通用类字典的识别能力不强。\n二、机器学习算法。机器学习方法的本质是一种统计算法，具有类似于人工智能的自动学习能力。学习过程是利用培训样本进行反复训练，从而获得有文本处理功能的数学模型。研究者将目标文本输入该数学模型即可输入文本的量化特征。\n自然语言与会计的运用\n利用NLP技术来探索一种新型的会计信息系统核算和审计信息系统的新模式。企业会计信息主要如实反映企业的真实的经济活动状况，客观的核算以及预测未来的经济发展趋势。利用NLP技术来拓展审计电算化的新思路。自然语言会计核算原则和自然处理系统的基本框架，建立经济事项和会计语言对照信息 库，对会计经济事项自动生成会计分录，自动生成会计凭证以及相关重要事项。语言文字是人类社会信息的主要载体，自然语言处理系统包括自然语言入机接口、机器翻译、文献检索、自动文摘、自动校对、语音识别也合成、字符识别等等。\n计算机审计业务主要关注对审计单位电子数据的取得和分析、计算等数据处理的业务，还称不上信息系统审计。从财务报表审计的角度来看，这一阶段的主要业务内容是对交易金额和账户。报表余额进行检查，属于审计程序的实质性测试环节。","data":"2019年06月20日 01:02:47"}
{"_id":{"$oid":"5d343afa62f717dc0659b2bb"},"title":"快速了解什么是自然语言处理","author":"黄小斜","content":"快速了解什么是自然语言处理\n\n\n摘要：自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。自然语言处理是一门融语言学、计算机科学、数学等于一体的科学。因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，所以它与语言学的研究有着密切的联系，但又有重要的区别。自然语言处理并不是一般地研究自然语言，而在于研制能有效地实现自然语言通信的计算机系统，特别是其中的软件系统。因而它是计算机科学的一部分。（本文原创，分享供于学习，转载标明出处：快速了解什么是自然语言处理）\n相关文章\n【文本处理】自然语言处理在现实生活中运用\n【文本处理】多种贝叶斯模型构建及文本分类的实现\n【文本处理】快速了解什么是自然语言处理\n【文本处理】领域本体构建方法概述\n【文本挖掘（1）】OpenNLP：驾驭文本，分词那些事\n【文本挖掘（2）】【NLP】Tika 文本预处理：抽取各种格式文件内容\n【文本挖掘（3）】自己动手搭建搜索工具\n1 计算机对自然语言处理的过程\n1.1把需要研究是问题在语言上建立形式化模型，使其可以数学形式表示出来，这个过程称之为\"形式化\"\n1.2把数学模型表示为算法的过程称之为\"算法化\"\n1.3根据算法，计算机进行实现，建立各种自然语言处理系统，这个过程是\"程序化\"\n1.4对系统进行评测和改进最终满足现实需求，这个过程是\"实用化\"\n2 自然语言处理涉及的知识领域\n语言学、计算机科学（提供模型表示、算法设计、计算机实现）、数学（数学模型）、心理学（人类言语心理模型和理论）、哲学（提供人类思维和语言的更深层次理论）、统计学（提供样本数据的预测统计技术）、电子工程（信息论基础和语言信号处理技术）、生物学（人类言语行为机制理论）。故其为多边缘的交叉学科\n3 自然语言处理涉及的范围\n3.1语音的自动合成与识别、机器翻译、自然语言理解、人机对话、信息检索、文本分类、自动文摘等等，总之分为四大方向：\n语言学方向\n数据处理方向\n人工智能和认知科学方向\n语言工程方向\n3.2也可细分为13个方面\n口语输入：语音识别、信号表示、鲁棒的语音识别、语音识别中的隐马尔科夫模型方法、语言模型、说话人识别、口语理解\n书面语输入：文献格式识别、光学字符识别（OCR）:印刷体识别/手写体识别、手写界面、手写文字分析\n语言分析理解：小于句子单位的处理、语法的形式化、针对基于约束的语法编写的词表、计算语义学、句子建模和剖析技术、鲁棒的剖析技术\n语言生成：句法生成、深层生成\n口语输入技术：合成语音技术、语音合成的文本解释、口语生成\n话语分析与对话：对话建模、话语建模口语对话系统\n文献自动处理：文献检索、文本解释：信息抽取、文本内容自动归纳、文本写作和编辑的计算机支持、工业和企业中使用的受限语言\n多语问题的计算机处理：机器翻译、人助机译、机助人译、多语言信息检索、多语言语音识别、自动语种验证\n多模态的计算机处理：空间和时间表示方法、文本与图像处理、口语与手势的模态结合、口语与面部信息的模态结合：面部运动和语音识别\n信息传输和信息存储：语音压缩、语音品质的提升\n自然语言处理中的数学方法：统计建模和分类的数学理论、数字信号处理技术、剖析算法的数学基础研究、神经网络、有限状态分析技术、语音和语言处理中的最优化技术和搜索技术\n语言资源：书面语料库、口语语料库、机器词典与词网的建设、术语编撰和术语数据库、网络数据挖掘和信息提取\n自然语言处理系统的评测：面向任务的文本分析评测、机器翻译系统和翻译工具的评测、大覆盖面的自然语言剖析器的评测、语音识别：评估和评测、语音合成评测、系统的可用性和界面的评测、语音通信质量的评测、文字识别系统的评测\n4 自然语言处理的发展的几个特点\n基于句法-语义规则的理性主义方法受到质疑，随着语料库建设和语料库语言学 的崛起，大规模真实文本的处理成为自然语言处理的主要战略目标。\n自然语言处理中越来越多地使用机器自动学习的方法来获取语言知识。\n统计数学方法越来越受到重视。\n自然语言处理中越来越重视词汇的作用，出现了强烈的\"词汇主义\"的倾向。","data":"2017年09月15日 17:22:05","date":"2017年09月15日 17:22:05"}
{"_id":{"$oid":"5d343afb62f717dc0659b2be"},"title":"Atitit 人工智能体系树培训列表应用较为广泛的技术.docx Atitit 人工智能体系培训列表 目录 1. 1.NLP自然语言处理文本处理 2 1.1. 语言理解 分词 2 1.2. 抽取","author":"attilax","content":"Atitit 人工智能体系树培训列表应用较为广泛的技术.docx\nAtitit 人工智能体系培训列表\n目录\n1. 1.NLP自然语言处理文本处理 2\n1.1. 语言理解 分词 2\n1.2. 抽取 （压缩文档的读取 格式转换 2\n1.3. 索引处理  摘要提取 2\n1.4. 搜索（按照标题 内容 2\n1.5. 热词检索排行（词云可视化展示 2\n1.6. 专家系统 问答系统 2\n1.7. 智能搜索引擎 数据挖掘和知识发现 2\n2. 知识图谱 知识处理系统 2\n3. 2.机器视觉 图像处理 3\n3.1. ocr 文字识别 4\n3.2. 条码 二维码识别 4\n3.3. 人脸识别 4\n3.4. 目标识别 验证码识别 4\n3.5. 证件识别 银行卡识别 4\n3.6. 指纹识别 4\n3.7. 图像 视频内容分析 4\n3.8. 图像搜索（相似图片，小图搜大图，人脸搜类似 4\n3.9. 视频编解码五大类 4\n4. 3.机器人 在线机器人 自动化 4\n4.1. web自动化 webdriver 4\n4.2. 爬虫（信息采集与信息发布机器人 4\n4.3. gui自动化 4\n5. 生物特征识别(none 5\n6. 人机交互 5\n6.1. 键盘、鼠标、操纵杆 5\n6.2. 打印机、 、显示器、 、音箱等输出设备 5\n7. 4.语言识别 语言tts等 5\n8. Ar vr (none 5\n9. 5.机器学习（决策树 贝叶斯 knn 等 5\n10. Other 5\n11. ref 6\nNLP自然语言处理文本处理\n语言理解 分词\n抽取 （压缩文档的读取 格式转换\n索引处理  摘要提取\n搜索（按照标题 内容\n热词检索排行（词云可视化展示\n专家系统 问答系统\n智能搜索引擎\n数据挖掘和知识发现\n知识图谱 知识处理系统\n知识图谱本质上是结构化的语义知识库，是一种由节点和边组成的图数据结构，以符号形式描述物理世界中的概念及其相互关系，其基本组成单位是“实体—关系—实体”三元组，以及实体及其相关“属性—值”对。不同实体之间通过关系相互联结，构成网状的知识结构。在知识图谱中，每个节点表示现实世界的“实体”，每条边为实体与实体之间的“关系”。通俗地讲，知识图谱就是把所有不同种类的信息连接在一起而得到的一个关系网络，提供了从“关系”的角度去分析问题的能力。\n知识图谱可用于反欺诈、不一致性验证、组团欺诈等公共安全保障领域，需要用到异常分析、静态分析、动态分析等数据挖掘方法。特别地，知识图谱在搜索引擎、可视化展示和精准营销方面有很大的优势，已成为业界的热门工具。但是，知识图谱的发展还有很大的挑战，如数据的噪声问题，即数据本身有错误或者数据存在冗余。随着知识图谱应用的不断深入，还有一系列关键技术需要突破\n2.机器视觉 图像处理\n计算机视觉是使用计算机模仿人类视觉系统的科学，让计算机拥有类似人类提取、处理、理解和分析图像以及图像序列的能力\n根据解决的问题，计算机视觉可分为计算成像学、图像理解、三维视觉、动态视觉和视频编解码五大类\nocr 文字识别\n条码 二维码识别\n人脸识别\n目标识别 验证码识别\n证件识别 银行卡识别\n指纹识别\n图像 视频内容分析\n图像搜索（相似图片，小图搜大图，人脸搜类似\n视频编解码五大类\n3.机器人 在线机器人 自动化\nweb自动化 webdriver\n爬虫（信息采集与信息发布机器人\ngui自动化\n生物特征识别(none\n人机交互\n人机交互主要研究人和计算机之间的信息交换，主要包括人到计算机和计算机到人的两部分信息交换，是人工智能领域的重要的外围技术。人机交互是与认知心理学、人机工程学、多媒体技术、虚拟现实技术等密切相关的综合学科。传统的人与计算机之间的信息交换主要依靠交互设备进行，主要包括键盘、鼠标、操纵杆、数据服装、眼动跟踪器、位置跟踪器、数据手套、压力笔等输入设备，以及打印机、绘图仪、显示器、头盔式显示器、音箱等输出设备。人机交互技术除了传统的基本交互和图形交互外，还包括语音交互、情感交互、体感交互及脑机交互等技术。\n键盘、鼠标、操纵杆\n打印机、 、显示器、 、音箱等输出设备\n4.语言识别 语言tts等\nAr vr (none\n5.机器学习（决策树 贝叶斯 knn 等\nOther\n人工智能技术包含了机器学习、知识图谱、自然语言处理、人机交互、计算机视觉、生物特征识别、AR/VR七个关键技术\nref","data":"2019年05月10日 09:31:13"}
{"_id":{"$oid":"5d343afc62f717dc0659b2c1"},"title":"自然语言处理的十个发展趋势","author":"csdn_csdn__AI","content":"记者 | CSDN 苏靖芝\n7 月22 - 23 日，由中国人工智能学会、阿里巴巴集团 \u0026 蚂蚁金服主办，CSDN、中国科学院自动化研究所承办的第三届中国人工智能大会（CCAI 2017）在杭州国际会议中心盛大开幕。\n本次大会的第一场分论坛讨论是关于语言智能领域的八大问题。讨论期间，哈尔滨工业大学刘挺教授对自然语言处理的发展趋势做了一次精彩的归纳，他把这里的趋势分成了十个方面：\n趋势1：语义表示——从符号表示到分布表示\n自然语言处理一直以来都是比较抽象的，都是直接用词汇和符号来表达概念。但是使用符号存在一个问题，比如两个词，它们的词性相近但词形不匹配，计算机内部就会认为它们是两个词。举个例子，荷兰和苏格兰这两个国家名，如果我们在一个语义的空间里，用词汇与词汇组合的方法，把它表示为连续、低维、稠密的向量的话，就可以计算不同层次的语言单元之间的相似度。这种方法同时也可以被神经网络直接使用，是这个领域的一个重要的变化。\n从词汇间的组合，到短语、句子，一直到篇章，现在有很多人在做这个事，这和以前的思路是完全不一样的。\n有了这种方法之后，再用深度学习，就带来了一个很大的转变。原来我们认为自然语言处理要分成几个层次，但是就句法分析来说，它是人为定义的层次，那它是不是一定必要的？这里应该打一个问号。\n实际工作中，我们面临着一个课题——信息抽取。我之前和一个单位合作，初衷是我做句法分析，然后他们在我的基础上做信息抽取，相互配合，后来他们发表了一篇论文，与初衷是相悖的，它证明了没有句法分析，也可以直接做端到端的直接的实体关系抽取，\n这很震撼，不是说现在句法分析没用了，而是我们认为句法分析是人为定义的层次，在端到端的数据量非常充分，可以直接进行信息抽取的时候，那么不用句法分析，也能达到类似的效果。当端到端的数据不充分时，才需要人为划分层次。\n趋势2：学习模式——从浅层学习到深度学习\n浅层到深层的学习模式中，浅层是分步骤走，可能每一步都用了深度学习的方法，实际上各个步骤是串接起来的。直接的深度学习是一步到位的端到端，在这个过程中，我们确实可以看到一些人为贡献的知识，包括该分几层，每层的表示形式，一些规则等，但我们所谓的知识在深度学习里所占的比重确实减小了，主要体现在对深度学习网络结构的调整。\n趋势3：NLP平台化——从封闭走向开放\n以前我们搞研究的，都不是很愿意分享自己的成果，像程序或是数据，现在这些资料彻底开放了，无论是学校还是大企业，都更多地提供平台。NLP领域提供的开放平台越来越多，它的门槛也越来越降低。\n语音和语言其实有很大的差别，我认识的好几位国内外的进入NLP的学者，他们发现NLP很复杂，因为像语音识别和语音合成等只有有限的问题，而且这些问题定义非常清晰。但到了自然语言，要处理的问题变得纷繁复杂，尤其是NLP和其他的领域还会有所结合，所以问题非常琐碎。\n趋势4：语言知识——从人工构建到自动构建\nAlphaGo告诉我们，没有围棋高手介入他的开发过程,到AlphaGo最后的版本，它已经不怎么需要看棋谱了。所以AlphaGo在学习和使用过程中都有可能会超出人的想像，因为它并不是简单地跟人学习。\n美国有一家文艺复兴公司，它做金融领域的预测，但是这个公司不招金融领域的人，只是招计算机、物理、数学领域的人。这就给了我们一个启发，计算机不是跟人的顶级高手学，而是用自己已有的算法，去直接解决问题。\n但是在自然语言处理领域，还是要有大量的显性知识的，但是构造知识的方式也在产生变化。比如，现在我们开始用自动的方法，自动地去发现词汇与词汇之间的关系，像毛细血管一样渗透到各个方面。\n趋势5：对话机器人——从通用到场景化\n最近出现了各种图灵测试的翻版，就是做知识抢答赛来验证人工智能，从产学研应用上来讲就是对话机器人，非常有趣味性和实用价值。\n这块的趋势在哪里？我们知道，从Siri刚出来，国内就开始做语音助手了，后来语音助手很快下了马，因为它可以听得到但是听不懂，导致后面的服务跟不上。后来国内把难度降低成了聊天，你不是调戏Siri吗，我就做小冰就跟你聊。但是难度降低了，实用性却跟不上来，所以在用户的留存率上，还是要打个问号。\n现在更多的做法和场景结合，降低难度，然后做任务执行，即希望做特定场景时的有用的人机对话。在做人机对话的过程中，大家热情一轮比一轮高涨，但是随后大家发现，很多问题是由于自然语言的理解没有到位，才难以产生真正的突破。\n趋势6：文本理解与推理——从浅层分析向深度理解迈进\nGoogle等都已经推出了这样的测试机——以阅读理解作为一个深入探索自然语言理解的平台。就是说，给计算机一篇文章，让它去理解，然后人问计算机各种问题，看计算机是否能回答，这样做是很有难度的，因为答案就在这文章里面，人会很刁钻地问计算机。所以说阅读理解是现在竞争的一个很重要的点。\n趋势7：文本情感分析——从事实性文本到情感文本\n多年以前，很多人都在做新闻领域的事实性文本，而如今，搞情感文本分析的似乎更受群众欢迎，这一块这在商业和政府舆情上也都有很好地应用。\n趋势8：社会媒体处理——从传统媒体到社交媒体\n相应的，在社会媒体处理上，从传统媒体到社交媒体的过渡，情感的影响是一方面，大家还会用社交媒体做电影票房的预测，做股票的预测等等。\n但是从长远的角度看，社会、人文等的学科与计算机学科的结合是历史性的。比如，在文学、历史学等学科中，有相当一部分新锐学者对本门学科的计算机的大数据非常关心，这两者在碰撞，未来的前景是无限的，而自然语言处理是其中重要的、基础性的技术。\n趋势9：文本生成——从规范文本到自由文本\n文本生成这两年很火，从生成古诗词到生成新闻报道到再到写作文。这方面的研究价值是很大的，它的趋势是从生成规范性的文本到生成自由文本。比如，我们可以从数据库里面生成一个可以模板化的体育报道，这个模板是很规范的。然后我们可以再向自由文本过渡，比如写作文。\n趋势10：NLP+行业——与领域深度结合，为行业创造价值\n最后是谈与企业的合作。现在像银行、电器、医药、司法、教育、金融等的各个领域对NLP的需求都非常多。\n我预测NLP首先是会在信息准备的充分的，并且服务方式本身就是知识和信息的领域产生突破。还比如司法领域，它的服务本身也有信息，它就会首先使用NLP。NLP最主要将会用在以下四个领域，医疗、金融、教育和司法。\n\n\nCSDN AI热衷分享 欢迎扫码关注","data":"2017年08月10日 10:24:00","date":"2017年08月10日 10:24:00"}
{"_id":{"$oid":"5d343afc62f717dc0659b2c3"},"title":"自然语言处理笔记-哈工大 关毅","author":"Big_quant","content":"目录\n文章目录\n目录\n前言\n绪论课程\n课程定性\n课程应用\n个人经验\n统计自然语言处理\n自然语言处理绪论\n自然语言处理绪论 二\n自然语言处理绪论 三\n自然语言处理绪论 四\n前言\n硕士生涯结束，开始专心做一件自己觉得有用的工具，先做工程，后搞理论。\n自然语言处理是一个非常难的问题，同时是人工智能皇冠上的明珠。\n接下来会记录一系列自然语言处理的笔记，来自于哈工大老师关毅\n绪论课程\n课程定性\n本门课程关注于最基础的部分，只会讲述不容置疑的基本点，以及一些使用的技术。\n所以本门课程由两部分组成，1知识内容2动手实践。\n自然语言处理概论。\n一些段子，哈工大自然语言处理人超多，同时，工作原创性不够，其次，20年前的问题依旧是问题。\n课程应用\n机器翻译，数据库技术，语音识别。\n个人经验\n老师先做了一段时间的工程，后来发现做的东西都被时间所淘汰，于是，转而做持久的理论。\n统计自然语言处理\n本门课程主要专注于词法分析，句法分析，语义分析。\n自然语言处理的历史，1946，自然语言处理的诞生。\n目前人工智能陷入低谷，大家开始反思，究竟什么是智能。\n什么是理解。\n1高德纳的书，人和机器对于歧义的理解是不同的。\n人工语言不允许语法错误，自然语言则各种语法错误。\n自然语言处理绪论\n1分词歧义2词性标准3语法分析歧义。4语义分析歧义。5语用分析歧义。\n歧义很多，不好处理。\n老师靠一个关键问题，拿到了微软的offer。\n希望我们能掌握自己的核心技术。\n自然语言处理绪论 二\n取法其上，得乎其中；取法其众，得乎其上。\n博采众家之长。优于众人的模型，参考文献。\n建议两个工具\n1思维导图\n2endnote文献管理。\n推荐书籍\n自然语言处理综论，计算机自然语言处理。\n自然语言定义：\n研究人-人，人-机交互的语言的模型问题，提出一个语言处理的框架，理解语言本身的规律。\n研制出更好人机的交互系统。\n自然语言处理绪论 三\n语义学模型：\n1能力模型：1语言学规则2算法优化3将模型数学形式化。\n2应用模型：1语言库2统计技术3建立计算框架处理语言。\n自然语言处理的其他定义：\n用人工方法来处理语言学的相应问题。\n目前推荐的处理办法是：规则与统计相结合，提高处理效果。\n自然语言处理绪论 四\n资源介绍：\n1基础资源\n2应用资源\n3使用资源\n4评测标准\n自然语言设计的学科\n1音位学\n2形态学\n3词汇学\n4语用学\n5语义学\n6句法学\n自然语言处理的主要应用：\n1发现句子与句子间的关系\n2句子中的关系\n3单词结构与规则\n自然语言处理的资源与内容\n1北大人民日报语料库\n2现代汉语语法信息词典\n3概念层次网络\n4知网\n评判一个技术有没有使用前景的一个方法：\n1让工具为人服务，解放人类。","data":"2018年12月08日 20:53:01"}
{"_id":{"$oid":"5d343afd62f717dc0659b2c5"},"title":"奇虎360自然语言处理面试总结","author":"qq_28935065","content":"奇虎360面试主要考察的知识点：\n1.机器学习常用的分类算法，Logistic回归，SVM，Decision Tree，随机森林等相关分类算法的原理，公式推导，模型评价，模型调参。模型使用场景\n2.机器学习常用的聚类算法，Kmeans,BDSCAN，SOM（个人论文中使用的算法），LDA等算法的原理，算法（模型）中参数的确定，具体到确定的方法；模型的评价，例如LDA应该确定几个主题，Kmeans的k如何确定，DBSCAN密度可达与密度直达。模型使用场景\n3.特征工程：特征选择，特征提取，PCA降维方法中参数主成分的确定方法，如何进行特征选择\n4.Boosting和bagging的区别\n5.数据如何去除噪声，如何找到离群点，异常值，现有机器学习算法哪些可以去除噪声\n6.HMM与N-gram模型之间的区别\n7.梯度消失与梯度爆炸\n8.奥卡姆剃须刀原理\n9.TCP三次握手的原理，为什么是三次而不是其他次\n10.进行数据处理时，如何过滤无用的信息（例如利用正则表达式提取或者其他方法），数据乱码的处理\n11.交叉熵与信息熵，信息增益与信息增益率，gini系数，具体如何计算\n12.BIC准则（贝叶斯信息准则）与AIC（赤池信息准则）\n13.需要手写代码（此次面试：字符串的操作）\n14.前向传播与反向传播\n15.常见的损失函数\n\n最后面试官给的建议：1.多看（看文献，看别人的成果）2.视野要打开，需要进一步扩宽知识面，在校期间更多关注理论，工作时没有太多的时间关注理论而更多是偏向业务，理论：数学，统计学，同时需要多读多练\n向面试官提问时，尤其是技术类面试，尽量不要用短时间内这个词，毕竟技术是一个积累的过程。\n原创申明：此博文为博主原创，未经允许不得转载","data":"2018年01月16日 15:04:22"}
{"_id":{"$oid":"5d343afd62f717dc0659b2c7"},"title":"自然语言处理—学习笔记","author":"studyeboy","content":"自然语言处理简介\n自然语言处理（Natural Language Processing，简称NLP）就是用计算机来处理、理解以及运用人类语言(如中文、英文等)，它属于人工智能的一个分支，是计算机科学与语言学的交叉学科，又常被称为计算语言学。\n自然语言处理任务\n在得到字、句子表示之后，自然语言处理任务类型划分为：\n类别（对象）到序列：例如文本生成、图像描述生成\n序列到类别：文本分类、情感分析\n同步的序列到序列：中文分词、词性标注、语义角色标注\n异步的序列到序列：机器翻译、自动摘要\n参考资料\nAwesome-Chinese-NLP\n兜哥出品 \u003c一本开源的NLP入门书籍\u003e\nNLP研究入门之道\nYSDA course in Natural Language Processing\nnlp-tutorial：Natural Language Processing Tutorial for Deep Learning Researchers","data":"2019年05月06日 10:06:45"}
{"_id":{"$oid":"5d343afe62f717dc0659b2c9"},"title":"一个简单的自然语言处理例子","author":"Cvjark","content":"例子是我学习的教程的一个例子，收集了一些客户对于饭店的评价，目标是将他们进行分类，分成好评和差评。\n数据的前5项：\n\n这里用到了一个之前没用到的包NLTK对我们的文本数据进行必要的处理，转化，使其变成能够进入我们模型的数据，本文针对一个例子进行介绍，详细可以查看官方文档对于这个包的说明。\nNLTK的介绍：\nNLTK是一个高效的Python构建的平台，用来处理人类自然语言数据。它提供了易于使用的接口，通过这些接口可以访问超过50个语料库和词汇资源（如WordNet），还有一套用于分类、标记化、词干标记、解析和语义推理的文本处理库。\n实现的大致步骤：\n对文本进行处理（去除标点符号，单词提取，大小写转化…）\n提取出来的单词需要进一步的清理（去除虚词，单词词根化…）词根化处理是为了避免相同单词的不同形式影响到我们的模型。（因为最终模型对于结果的处理是通过统计词出现的频率所属的类别并依此来对新样本进行分类的）\n对提取结果进行稀疏矩阵化操作，转化为可进入模型的数据。\n代码：\nimport pandas as pd import numpy as np import re #正则用到的库 import nltk #文本处理用 from nltk.corpus import stopwords #处理文本中的虚词 nltk.download('stopwords') #将包含的虚词的加载下来，用于后面的比对，去除虚词 from nltk.stem.porter import PorterStemmer dataset = pd.read_csv('Restaurant_Reviews.tsv',delimiter='\\t') #加载数据 pocket = [] #用于存储处理后的文本\nfor i in range(dataset.shape[0]): review = dataset['Review'][i] review = re.sub('[^a-zA-Z]' , ' ' , review) #进行正则化处理，留下字母（不区分大小写），其余替换为空格，防止处理后单词连接在一起，破坏了词意 review = review.lower() #将大写的字母变成小写 review = review.split() ps = PorterStemmer() #词根化 review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))] ''' 这里的大致作用是遍历review中提取的单词，筛选出不在虚词包中的单词，并进行词根化。 ''' review = ' '.join(review) #对筛选结果重新组合成句子。 pocket.append(review)\n关于词根化：维基百科\nfrom sklearn.feature_extraction.text import CountVectorizer tool = CountVectorizer(max_features=1500) #将文本序列进行稀疏矩阵转化 x = tool.fit_transform(pocket).toarray() #shape(1000, 1565) ''' 这里是指我们的数据有1000个样本，1565个单词，CountVectorizer(max_features=1500)，这里限制了特征最大为1500 所以之后我们x.shape会是（1000，1500）也可以根据需求进行调整。 ''' y = dataset.iloc[:,1]\nfrom sklearn.model_selection import train_test_split x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state = 0) from sklearn.naive_bayes import GaussianNB classifer = GaussianNB() classifer = classifer.fit(x_train,y_train) y_pre = classifer.predict(x_test) #利用贝叶斯进行拟合训练集，并做出测试集的预测。\nfrom sklearn.metrics import confusion_matrix score = confusion_matrix(y_test,y_pre) print(score) point = (score[0,0]+score[1,1])/x_test.shape[0] print('分类器在测试集上的表现：'+str(point)) #利用混淆矩阵进行预测结果准确的检验。\n输出：\n[[55 42] [12 91]] 分类器在测试集上的表现：0.73\n使用随机森林试试：\nfrom sklearn.ensemble import RandomForestClassifier classifer = RandomForestClassifier(n_estimators=10000,random_state=0) classifer = classifer.fit(x_train,y_train)\ny_pre = classifer.predict(x_test) score = confusion_matrix(y_test,y_pre) print(score) point = (score[0,0]+score[1,1])/x_test.shape[0] print('分类器在测试集上的表现：'+str(point))\n输出：\n[[74 23] [38 65]] 分类器在测试集上的表现：0.695 #准确度不如贝叶斯。。\n至此例子结束。这个只是入门级别，更深层次的用法会对文本进行更加细化的处理，利用更好的模型。\n如有错误，还请指出，万分感谢。","data":"2018年09月20日 21:30:43"}
{"_id":{"$oid":"5d343afe62f717dc0659b2cb"},"title":"自然语言处理（NLP）知识结构总结","author":"喜欢打酱油的老鸟","content":"https://mp.weixin.qq.com/s/rmANEGi_a9k19Bxeidxf-Q\n作者简介：小郭，计算机专业在读硕士研究生，AI学习与爱好者，欢迎交流，留言或者邮箱guo_jc5@163.com。本文选自CSDN博客。\n自然语言处理知识太庞大了，网上也都是一些零零散散的知识，比如单独讲某些模型，也没有来龙去脉，学习起来较为困难，于是我自己总结了一份知识体系结构，内容来源主要参考黄志洪老师的自然语言处理课程，主要参考书为宗成庆老师的《统计自然语言处理》，可能很多内容写的不清楚，但好像中文NLP书籍就这一本全一些，如果想看好的英文资料，可以到我的GitHub上下载：\nhttp://github.com/lovesoft5/ml\n下面直接开始正文：\n▌一、自然语言处理概述\n1）自然语言处理：利用计算机为工具，对书面实行或者口头形式进行各种各样的处理和加工的技术，是研究人与人交际中以及人与计算机交际中的演员问题的一门学科，是人工智能的主要内容。\n2）自然语言处理是研究语言能力和语言应用的模型，建立计算机（算法）框架来实现这样的语言模型，并完善、评测、最终用于设计各种实用系统。\n3）研究问题（主要）：\n信息检索\n机器翻译\n文档分类\n问答系统\n信息过滤\n自动文摘\n信息抽取\n文本挖掘\n舆情分析\n机器写作\n语音识别\n研究模式：自然语言场景问题，数学算法，算法如何应用到解决这些问题，预料训练，相关实际应用\n自然语言的困难：\n场景的困难：语言的多样性、多变性、歧义性\n学习的困难：艰难的数学模型（hmm,crf,EM,深度学习等）\n语料的困难：什么的语料？语料的作用？如何获取语料？\n▌二、形式语言与自动机\n语言：按照一定规律构成的句子或者字符串的有限或者无限的集合。\n描述语言的三种途径：\n穷举法\n文法（产生式系统）描述\n自动机\n自然语言不是人为设计而是自然进化的，形式语言比如：运算符号、化学分子式、编程语言形式语言理论朱啊哟研究的是内部结构模式这类语言的纯粹的语法领域，从语言学而来，作为一种理解自然语言的句法规律，在计算机科学中，形式语言通常作为定义编程和语法结构的基础形式语言与自动机基础知识：\n集合论\n图论\n自动机的应用：\n单词自动查错纠正\n词性消歧（什么是词性？什么的词性标注？为什么需要标注？如何标注？）\n形式语言的缺陷：\n对于像汉语，英语这样的大型自然语言系统，难以构造精确的文法\n不符合人类学习语言的习惯\n有些句子语法正确，但在语义上却不可能，形式语言无法排出这些句子\n解决方向：基于大量语料，采用统计学手段建立模型\n▌三、语言模型\n1）语言模型（重要）：通过语料计算某个句子出现的概率（概率表示），常用的有2-元模型，3-元模型\n2）语言模型应用：\n语音识别歧义消除例如，给定拼音串：tashiyanyanjiusaunfade\n可能的汉字串：踏实烟酒算法的他是研究酸法的他是研究算法的，显然，最后一句才符合。\n3）语言模型的启示：\n开启自然语言处理的统计方法\n统计方法的一般步骤：\n收集大量语料\n对语料进行统计分析，得出知识\n针对场景建立算法模型\n解释和应用结果\n4）语言模型性能评价，包括评价目标，评价的难点，常用指标（交叉熵，困惑度）\n5）数据平滑：\n数据平滑的概念，为什么需要平滑？\n平滑的方法，加一法，加法平滑法，古德-图灵法，J-M法，Katz平滑法等。\n6）语言模型的缺陷：\n语料来自不同的领域，而语言模型对文本类型、主题等十分敏感。\nn与相邻的n-1个词相关，假设不是很成立。\n▌四、概率图模型，生成模型与判别模型，贝叶斯网络，马尔科夫链与隐马尔科夫模型（HMM）\n1）概率图模型概述（什么的概率图模型，参考清华大学教材《概率图模型》）\n2）马尔科夫过程（定义，理解）\n3）隐马尔科夫过程（定义，理解）\nHMM的三个基本问题（定义，解法，应用）\n注：第一个问题，涉及最大似然估计法，第二个问题涉及EM算法，第三个问题涉及维特比算法，内容很多，要重点理解，（参考书李航《统计学习方法》，网上博客，笔者github）\n▌五、马尔科夫网，最大熵模型，条件随机场（CRF）\n1)HMM的三个基本问题的参数估计与计算\n2）什么是熵\n3）EM算法（应用十分广泛，好好理解）\n4）HMM的应用\n5）层次化马尔科夫模型与马尔科夫网络\n提出原因，HMM存在两个问题\n6）最大熵马尔科夫模型\n优点：与HMM相比，允许使用特征刻画观察序列，训练高效\n缺点：存在标记偏置问题\n7）条件随机场及其应用(概念，模型过程，与HMM关系)\n参数估计方法（GIS算法，改进IIS算法）\nCRF基本问题：特征选取（特征模板）、概率计算、参数训练、解码（维特比）\n应用场景：\n词性标注类问题（现在一般用RNN+CRF）\n中文分词（发展过程，经典算法，了解开源工具jieba分词）\n中文人名，地名识别\n8）CRF++\n▌六、命名实体识别，词性标注，内容挖掘、语义分析与篇章分析（大量用到前面的算法）\n1）命名实体识别问题\n相关概率，定义\n相关任务类型\n方法（基于规程-\u003e基于大规模语料库）\n2）未登录词的解决方法(搜索引擎，基于语料)\n3）CRF解决命名实体识别（NER）流程总结：\n训练阶段：确定特征模板，不同场景（人名，地名等）所使用的特征模板不同，对现有语料进行分词，在分词结果基础上进行词性标注（可能手工），NER对应的标注问题是基于词的，然后训练CRF模型，得到对应权值参数值\n识别过程：将待识别文档分词，然后送入CRF模型进行识别计算（维特比算法），得到标注序列，然后根据标注划分出命名实体\n4）词性标注（理解含义，意义）及其一致性检查方法（位置属性向量，词性标注序列向量，聚类或者分类算法）\n▌七、句法分析\n1）句法分析理解以及意义\n1、句法结构分析\n完全句法分析\n浅层分析（这里有很多方法。。。）\n2、依存关系分析\n2）句法分析方法\n基于规则的句法结构分析\n基于统计的语法结构分析\n▌八、文本分类，情感分析\n1）文本分类，文本排重\n文本分类：在预定义的分类体系下，根据文本的特征，将给定的文本与一个或者多个类别相关联\n典型应用：垃圾邮件判定，网页自动分类\n2）文本表示，特征选取与权重计算，词向量\n文本特征选择常用方法：\n基于本文频率的特征提取法\n信息增量法\nX2（卡方）统计量\n互信息法\n3）分类器设计\nSVM，贝叶斯，决策树等\n4）分类器性能评测\n召回率\n正确率\nF1值\n5）主题模型（LDA）与PLSA\nLDA模型十分强大，基于贝叶斯改进了PLSA，可以提取出本章的主题词和关键词，建模过程复杂，难以理解。\n6）情感分析\n借助计算机帮助用户快速获取，整理和分析相关评论信息，对带有感情色彩的主观文本进行分析，处理和归纳例如，评论自动分析，水军识别。\n某种意义上看，情感分析也是一种特殊的分类问题\n7）应用案例\n▌九、信息检索，搜索引擎及其原理\n1）信息检索起源于图书馆资料查询检索，引入计算机技术后，从单纯的文本查询扩展到包含图片，音视频等多媒体信息检索，检索对象由数据库扩展到互联网。\n点对点检索\n精确匹配模型与相关匹配模型\n检索系统关键技术：标引，相关度计算\n2）常见模型：布尔模型，向量空间模型，概率模型\n3）常用技术：倒排索引，隐语义分析（LDA等）\n4）评测指标\n▌十、自动文摘与信息抽取，机器翻译，问答系统\n1）统计机器翻译的的思路，过程，难点，以及解决\n2）问答系统\n基本组成：问题分析，信息检索，答案抽取\n类型：基于问题-答案，基于自由文本\n典型的解决思路\n3）自动文摘的意义，常用方法\n4）信息抽取模型（LDA等）\n▌十一、深度学习在自然语言中的应用\n1）单词表示，比如词向量的训练（wordvoc）\n2）自动写文本\n写新闻等\n3）机器翻译\n4）基于CNN、RNN的文本分类\n5）深度学习与CRF结合用于词性标注\n...............\n原文地址：\nhttps://blog.csdn.net/meihao5/article/details/79592667\n——【完】——","data":"2018年08月23日 16:40:00","date":"2018年03月17日 18:04:35"}
{"_id":{"$oid":"5d343aff62f717dc0659b2cd"},"title":"思必驰俞凯：自然语言技术的畅想关键点不在交互，而是自然丨清华人工智能研习社","author":"大数据文摘","content":"大数据文摘作品\n\n大数据文摘记者 刘涵 魏子敏\n\n\n“自然语言技术的未来，其关键点是“自然”两个字。”\n\n\n11月最后一天，思必驰联合创始人、首席科学家俞凯博士在清华x-lab主办的人工智能研习社第七课上，如此评价自然语言处理，并与现场听众一起畅想了这一潜力巨大的技术将走向哪里。\n\n\n图：11月30日，思必驰联合创始人俞凯在清华做了题为《认知型口语对话智能》的讲座。 刘涵 摄\n\n\n在这场题为《认知型口语对话智能》的讲座上，俞凯认为认知交互面临的最主要的挑战一定不是语音，因为从语音识别的角度上来说，问题明确，只要专门向这个领域去做，绝大部分都可以优化的很好。\n\n\n他认为其最大的挑战还是对话的过程，例如针对抑郁症患者治疗的这类场景，语音对话更像是有目的的聊天，如果没有很强的数学背景在后面做支持，是很难的，只有在一个垂直领域积累更多的数据，才能做得更好。\n\n\n\n\n大数据文摘整理的俞凯博士本次讲座内容如下，在不改变原意的前提下有删改：\n\n\n今天的题目叫认知型口语对话智能，核心点是两个字：“对话”。\n\n\n这两个字不单单包含语音，还包含语言。从人机变迁讲起来，我在清华待了八年时间，在这八年当中，我们经历了人和机器在不同时代交互的几个变迁。\n\n\n我们为什么开始关心口语对话智能\n\n\n今天第一个要讲的问题，就是我们为什么开始关心口语对话智能。\n\n\n刚开始的时候我们使用的是Windows图形交互界面，通过机器图形交互界面使得人和信息可以进行交流，我们奇迹般的看到了打印出来很工整的排版。而到了现在，在2011年开始，手机变成智能手机，使用开始变的非常广泛，这个时代自然的语言（手动输入、语音）逐渐形成了我们现在的交互手段。再往后我们发现通过口语沟通是未来智能信息获取最核心的东西，而移动互联网的时代，最关键的是这一类沟通产生了一种新的模式，那就是交互。\n\n\n讲座现场图 刘涵 摄\n\n\n在上世纪出现Google、百度等搜索引擎的时候，交互还是单向的，但出现智能手机之后我们的交互变成了双向。比如苹果的交互史，在刚开始做出来第一代iPhone的时候并没有语音交互的能力，但经过市场调研之后发现有75%的用户都希望有语音控制。于是，在后面两代iPhone加入了语音控制，但到后面发现实际使用的用户竟然不到5%，苹果经过总结之后发现不仅仅是语音，还必须有自然语言交互。于是在iPhone4S上面出现了SiRi,再次经过市场调研之后发现，大概有87%的用户至少在一个月会使用一次SiRi。\n\n\n而且，他们还发现了一件事情，这87%的用户使用SiRi的时候基本上都是在调戏SiRi，并不做其它的事情，这导致苹果并不能赚到钱。这也促使了苹果在2015年收购了一家做统计对话交互的公司Vocallq,这会让技术语音识别和语义连在一起形成完整的闭环，SiRi就可以为我们提供新的功能了。\n\n\n讲座现场图 刘涵 摄\n\n\n现如今大家都说是互联网时代，那么如今的信息发展到什么程度了呢？有一个统计显示，到2017年年底，全世界物联网智能设备的总数将首次超过人类总数。而且这些智能设备绝大部分是没有或者拥有很小的屏幕，并没有办法进行很复杂的操作，这些设备如果想要去访问最核心抽象复杂的信息，只能是语音或者对话的形式。这也是众多巨头从2014年的音箱开始出现一系列智能音箱的原因。从技术上讲，这件事情不仅仅是要解决框架的问题，还包括了对话管理、识别、合成以及我们的理解。\n\n\n语音识别存在的问题和机遇\n\n\n我们会碰到什么样的问题，以及在这个过程中有多少和我们的应有相关的机会。\n\n\n首先是语音识别。\n\n\n语音识别是感知技术这一类里面前沿的技术，当许多人看到语音识别，第一个会想到的问题就是语音识别似乎已经被解决了，当我们使用一个包罗万象的语音识别系统的时候，我讲“疏影横斜水清浅，暗香浮动月黄昏”这样的东西都可以比较完整的出来。但尽管采用了深度学习的技术，仍然避免不了错误，它也会偶尔的有一些语音识别的错误出现，而我们的任务就是使得它像人一样，在有错误的时候，完整的去进行人机交互，修正错误，这需要感知技术和认知技术相互的帮助来实现。\n\n\n第二是计算能力。\n\n\n语音识别的解决是与计算能力有关的，举一个例子，刚才我在做演示的时候，这个演示的应用背后早期使用的深度神经网络，共有7层，每层有2048个节点，输入是1320，输出是将近1万，这大概有4500万的参数，在做语音识别的时候我们是把每秒钟的语音切成100份，每一份提取1320个向量，大家想象我在一秒钟要让特征向量经过100次深度神经网络计算，之后还要在数以亿计节点的搜索网络里再去搜它，所以这个运算是非常非常复杂的。曾经有过统计，整个语音识别会分成搜索的速度和做神经网络前向传递的速度，这两个速度的比例，在传统系统里面前向传递的速度占30%-40%，后面在各种各样的语言空间搜索的速度大体占60%-70%。所以，在技术上必须突破速度的问题。\n\n\n现场听众提问 刘涵 摄\n\n\n感知智能另外一件事是如何把它做得更小。整个信息技术的变化和推进一定是和技术基础的推进有关，性能抗噪能不能达到90%、能不能在手机手表上面也做到大词汇等新的挑战不断应运而生，随着在智能物联网方面我们做出各种各种的优化之后，这样的挑战开始被一个个的克服掉。\n\n\n认知这个事情更加麻烦。人机对话并不是大家想象那样，对话也是分成很多种形态的，有的可以很好的解决，有的却毫无头绪。如果以不同的轮回次数来分类，大概可以分为下面几种。第一种是模式最少的，单轮模式，既我说一句它回答一句，而且没有什么特定的结构化语义，这种情况基本上是命令式的，十分简单。复杂一点的则是问答，现在的经典深度学习技术很多是用来解决问答这个问题的，因为问答基本上是一问一答，你说一句它会给你一个答案，偶尔会带有一点上下文，这并不是真正意义上多轮的东西。还有一类是闲聊，比如微软小冰，你不停的说，它就不停的跟你聊天。闲聊的准则就是以聊得时间来定义的，曾经有一位用户，聊了好几个小时依然在继续。但这里面是没有什么目标意义的，所以闲聊要考虑的是如何把一些比较有趣的东西融入进去。\n\n\n但是里面究竟有什么意义，机器是不会去关注的，只要有用户黏性跟它一直聊下去，特点是多轮，没什么结构化的东西。偶尔会加一些知识，现在希望把这个东西融合起来，这是方向，本质上没有什么结构化的东西。所以闲聊这一类事情实际上更多的是怎么样能够把一些比较有趣的东西融进去。实事求是来讲目前还缺乏一套比较扎实的理论体系，能够让真正在理论上解决掉。\n\n\n最后一类是任务型的多轮对话，这类对话是有比较扎实的数学基础的，把对话看做是一个序列决策过程。\n\n\n这一技术的三个层面\n\n\n如果从认知层级的结算上来讲，我们会把认知技术分为三个层面。\n\n\n第一种是静态层面，我随便说一句话，自然语言能不能理解，能不能映射到正确的意思上面去。\n\n\n第二类是交互决策，意思是我在说话的时候如何进行反馈，比如我对一个机器说我要找到餐馆，它要明白我想去哪、吃什么。\n\n\n第三是进化，我想要便宜的东西，它却以为我想要贵的，当它发现错了之后下一次一定要更新自己的反馈策略，进化出自己的认知。\n\n\n聊一件和各位相关的事情：大规模可定制对话智能。在讲整个对话智能的时候，我们会发现在整个流程里面，每一个环境都看起来很美好，但一到专业领域的环节就会变得不一样了。比如做对话模式，做购物的场景与金融、家庭的场景所理解的东西完全不一样，这个时候就要看做出来的模型是否每一个场景都能识别，是否能很好的支持。在细节上面，还有很多个性化需求，例如唤醒。当我们喊小乐给我放一首歌的时候，这个小乐就是一种唤醒。但有的时候我们希望它有好几个名字，这种需要多唤醒词的需求在未来会出现更多。\n\n\n当我们真正去做的时候，会希望在我们所使用的口语对话系统上的支撑可以定制。而大规模可定制是我们提出的新概念，在2013年我们发布了一个叫“对话工场”的平台，2017年升级到大规模可定制的“Dialogue User Interface”，DUI，其本质上是把图形界面和语音界面在对话交互的框架下结合在一起。\n\n\n定制性的语音交互技术可以做什么？\n\n\n这时候，我们会好奇，这些定制技术能做什么呢？比如可以在做实时语音识别和大词汇语音识别的时候，做出来一个功能，当语义改变的时候，语音识别会对我们自动添加的词做自动识别，比如我们添加了“泷泽萝拉”四个字，语音识别系统能自动把它加入词表并具有识别的能力，继而在实现理解和交互。\n\n\n我们想要做一件事情，在一个车载的系统里面，自动选择一些声音添加进去，当想要林志玲甜甜声音的时候，喊一声林志玲出来，绝对不会再出来郭德纲的声音，让它回去它就会切换为原本的郭德纲声音。我们希望这样的事情可以很自由的来回切换。更进一步，我们要支持对理解和对话进行相应的定制。\n\n\n在这个过程里，在我们真正背后的技术上来说，已经不再是一般的语音的和对话的交互，不再仅仅是前面我们提到的感知和认知的独立框架。在这里要解决的问题是所谓大规模可定制的一些新技术。比如说在识别里，要解决所谓的自适应的问题。比如说话人和环境的自适应、领域主题的自适应等这些东西可以及时的去改变它，可以使得对话有很多的自适应。如果实现这些自适应规模化的话还需要有相应的系统支持。在这个过程里需要有具体的技术拆借、需要有模型定制，能够使得它规模化的扩展，并且在个性的基础之上去进行进化，这一类东西里会有很多新型的技术出现，但这些技术都需要技术基础的支撑。\n\n\n课程推荐\n使用keras快速构造深度学习模型实战\n微软\u0026谷歌数据科学家，带你每周案例实战\n\n\n史上最高性价比！\n两位顶尖的微软/谷歌数据科学家，直播互动分享珍贵学习经验，并详细讲解前沿实战案例！GPU云实验平台提供便捷的操作环境。还有原著大作免费送！\n七周时间，带你玩转Keras！\n很多即将毕业和渴望转型的小伙伴都加入了我们，你不来吗？\n\n\n往期精彩文章\n点击图片阅读\n沿着地铁买房怎样更划算？2017上海城市大数据报告发布","data":"2017年12月02日 00:00:00"}
{"_id":{"$oid":"5d343aff62f717dc0659b2cf"},"title":"人工智能","author":"我是波多","content":"文章目录\n人工智能的分类\n计算机视觉\n人工智能的分类\n计算机视觉（CV，Computer Vision）\n语音识别\n自然语言处理\n推荐系统、专家系统\n计算机视觉","data":"2018年10月31日 15:21:41"}
{"_id":{"$oid":"5d343aff62f717dc0659b2d1"},"title":"自然语言处理（NLP）","author":"端午过后的猪","content":"自然语言处理：\n1、什么是自然语言处理（NLP）\n自然语言处理是一门交叉学科，包括计算机科学，人工智能和语言学\n目标：让计算机去处理或“理解”自然语言, 完成一些有用的任务例如问答系统，机器翻译\n完全理解或者表示语言的意义（甚至去定义它）都是一个虚幻的目标\n完美的理解语言是一个“\nAI-complete\n”的问题\n\n\n2、自然语言处理的应用\n应用范围从简单到复杂\n拼写检查, 关键词提取\u0026搜索，同义词查找\u0026替换\n从网页中提取有用的信息例如产品价格，日期，地址，人名或公司名等\n分类，例如对教科书的文本进行分级，对长文本进行正负情绪判断\n机器翻译\n口语对话系统\n复杂的问答系统\n\n\n3、工业届里的NLP应用\n搜索引擎\n在线广告\n自动的或辅助的翻译技术\n市场营销或者金融交易领域的情感分析\n语音识别\n\n\n4、NLP为什么这么难\n语言在表达上就很复杂，使用的时候要综合考虑使用情境\nJane hit June and then she [fell/ran].\n歧义问题：“I made her duck”\n\n\n5、现有的中文自然语言处理工具：\nNLPIR\nfudanNLP\nLTP\npython-NLTK\n直接有纠错功能的：腾讯文智   大汉科技\n\n\n\n\n\n\nN-gram语言模型：\n目的：在给定语料库的情况下，计算一个字符串出现的概率\nN-gram是自然语言处理（NLP）中一个非常重要的概念，通常在NLP中，人们基于一定的语料库，可以利用N-gram来做以下几类事情：\n预计或者评估一个句子是否合理；\n评估两个字符串之间的差异程度，这也是模糊匹配中常用的一种手段；\n语音识别；\n机器翻译；\n文本分类；\n\n\n\n资料链接：\nn-gram_1（包含开源n-gram数据集）\nn_gram_2\nn-gram_3\nn-gram数据格式\n\n\n那么我们如何用N-gram来做篇章单元的分类器呢？其实很简单了，只要根据每个类别的语料库训练各自的语言模型，也就是上面的频率分布表，实质上就是每一个篇章单元的类别都有一个概率分布，当新来一个篇章单元的时候，只要根据各自的语言模型，计算出每个语言模型下这个篇章单元的发生概率，篇章单元在哪个模型的概率大，这篇文本就属于哪个类别了。\n\n\n数据稀疏和平滑技术：\n平滑\n\n\n\n\n中文纠错\n小孙纠错算法\n\n\ngithub上中文纠错项目：\nCn_Checker\nxmnlp\njcjc在线纠错\n\n\n结巴分词的词典：\n结巴分词词典\n\n\n高效的分词工具：\n结巴\nTHULAC","data":"2018年03月16日 14:17:06","date":"2018年03月16日 14:17:06"}
{"_id":{"$oid":"5d343b0062f717dc0659b2d4"},"title":"自然语言处理--序列标注模型","author":"热之决斗者","content":"在人工智能异常火爆的当下，自然语言处理技术因其具有广泛的应用领域、良好的计算性能等因素备受科研人员的青睐；而序列标注是自然语言处理领域的一个非常常见的问题，从分词、词性标注，到较深层的组块分析以至更为深层的完全句法分析、语义角色标注等任务，都可以看作是典型的序列标注问题。\n序列标注问题指对序列中每个元素进行标记，输出标记序列y = y1, . . . , yn，n是序列的长度。若yi 的取值范围定义为S = {si}，输出序列的可能组合数为Ci。变量yi 的不同取值也叫不同的状态。yi 所有可能取值的集合S，也被称为“状态空间”。因为一个序列状态的组合数非常多，也不能直接用传统的学习方法通过枚举y 来得到最佳的标记，需要用动态优化的方法来求解y。\n传统的单点分类器方法难以获得整个序列的最优标记。下图是两种线性链序列标注结构，每个元素标记只与相邻的元素相关，构成了线性链式结构。其中，图a是有向图结构，每个元素标记只与前一个元素标记相关，图b是无向图结构，每个元素标记与左右两个相邻元素标记相关。\n序列标注问题需要解决四个问题：\n1. 如何选择合适的序列标注模型？确定标记之间的关联关系。\n2. 怎样从序列上抽取特征？\n3. 如何进行求解？也就是解码问题。\n4. 如何进行参数学习？\n常用的序列标注模型有：线性模型、隐马尔可夫模型、最大熵马尔可夫模型、条件随机场等；\n马尔可夫链，简称马氏链，是由随机变量组成的一个序列x1, x2, x3, · · · ，xt 的值是在时间t 时的状态。如果xt+1 对于过去状态的条件概率分布仅是xt 的一个函数，即P(xt+1|x1, x2, · · · , xt) = P(xt+1|xt) 。\n序列标注模型可以分为两大类：一种是非统计方法，另一种是统计的方法：\n在非统计方法中，最有代表性的是线性分类器：y = arg max w · O(x, y)，\n在基于统计方法，比较主流的方法是用无向图来表示模型。\n对于中文分词的序列标注问题，可以定义y 属于 {B,O}，这里B 表示把当前字作为一个新词的开始，O表示当前字与前面的字构成一个词。例如：句子“他／说／的／确实／在理”转化为下面以字为基本元素构成的序列。\nx = 他  说  的  确  实  在  理\ny =  B   B   B   B   O   B   O\n假设状态空间大小为C，对于长度为n的y，其可能的组合数为Cn。因此，穷举不同的y已获得最佳序列是不可行的。通过观察公式，我们可以用动态优化方法来快速的求解。我们首先定义As,i 是输入序列x0, · · · , xi 且yi = s 的最佳标记序列。As,i 可以通过下面两个递归公式来计算：As,0 = 0, s 属于S ；As,i = maxAs′,i−1 + w · ϕ(x, s′, s)；这个方法也叫Viterbi 算法，可以保证找到得分最高的标记序列，有点动态规划的意思在里面。\n学习参数的方法也不同，一般为最大似然估计、最大边际距离或最小均方误差等。可以用传统的分类器训练算法，比如感知器、SVM、kNN等。\n隐马尔可夫模型（HMM）是常见的序列标注模型。\nHMM有三个典型问题：\n1 已知模型参数，计算某一特定输出序列的概率. 通常使用forward 算法解决.\n2 已知模型参数，寻找最可能的能产生某一特定输出序列的隐含状态的序列. 通常使用\nViterbi 算法解决.\n3 已知输出序列，寻找最可能的状态转移以及输出概率. 通常使用Baum-Welch 算法以及\nReversed Viterbi 算法解决\n隐马尔可夫模型是一个生成式模型，要对p(x, y) 进行建模。并且样本序列的观测值只与当前状态（标记）有关，这就限制了模型的能力。最大熵马尔可夫模型是一个判别式模型，直接对p(y|x) 进行建模，这样可以利用大量的冗余特征提高模型性能。\n最大熵马尔可夫模型是用局部信息去优化全局，会有标注偏置（Label Bias）的问题。条件随机场（Conditional Random Fields, CRF）图模型，它是在给定需要标记的观察序列x 的条件下计算整个标记序列y 的联合概率分布，而不是在给定当前状态条件下定义下一个状态的分布。即P(y|x) = exp(w · O(x, y))/Zx；隐马尔可夫模型中存在两个假设：输出独立性假设和马尔可夫性假设。其中，输出独立性假设要求序列数据严格相互独立才能保证推导的正确性，而事实上大多数序列数据不能被表示成一系列独立事件。而条件随机场则使用一种概率图模型，条件随机场没有隐马尔可夫模型那样严格的独立性假设条件，因而可以容纳任意的上下文信息，可以灵活地设计特征。同时，条件随机场具有表达长距离依赖性和交叠性特征的能力，而且所有特征可以进行全局归一化，能够求得全局的最优解，还克服了最大熵马尔可夫模型标记偏置的缺点。条件随机场模型作为一个整句联合标定的判别式概率模型，同时具有很强的特征融入能力，是目前解决自然语言序列标注问题最好的统计模型之一。条件随机场的缺点是\n训练的时间比较长。\n,      上面简单介绍了基本的序列标注的模型方法，以后会详细分析。","data":"2019年01月28日 00:34:11"}
{"_id":{"$oid":"5d343b0162f717dc0659b2d7"},"title":"【吴恩达deeplearning.ai】深度学习(10)：自然语言处理","author":"Hugsy19","content":"自然语言处理（Natural Language Processing，NLP)是人工智能和语言学领域的学科分支，它研究实现人与计算机之间使用自然语言进行有效通信的各种理论和方法。\n词嵌入\n前面介绍过，处理文本序列时，通常用建立字典后以one-hot的形式表示某个词，进而表示某个句子的方法。这种表示方法孤立了每个词，无法表现各个词之间的相关性，满足不了NLP的要求。\n词嵌入（Word Embedding）是NLP中语言模型与表征学习技术的统称，概念上而言，它是指把一个维数为所有词的数量的高维空间（one-hot形式表示的词）“嵌入”到一个维数低得多的连续向量空间中，每个单词或词组被映射为实数域上的向量。\n\n如上图中，各列分别组成的向量是词嵌入后获得的第一行中几个词的词向量的一部分。这些向量中的值，可代表该词与第一列中几个词的相关程度。\n使用2008年van der Maaten和Hinton在论文[Visualizing Data using t-SNE]中提出的t-SNE数据可视化算法，将词嵌入后获得的一些词向量进行非线性降维，可到下面的映射结果：\n\n其中可发现，各词根据它们的语义及相关程度，分别汇聚在了一起。\n对大量词汇进行词嵌入后获得的词向量，可用来完成命名实体识别（Named Entity Recognition)等任务。其中可充分结合迁移学习，以降低学习成本，提高效率。\n好比前面讲过的用Siamese网络进行人脸识别过程，使用词嵌入方法获得的词向量可实现词汇的类比及相似度度量。例如给定对应关系“男性（Man）”对“女性（Woman）”，要求机器类比出“国王（King）”对应的词汇，通过上面的表格，可发现词向量存在数学关系“Man - Woman\n≈\n≈\n\\approx King - Queen”，也可以从可视化结果中看出“男性（Man）”到“女性（女性）”的向量与“国王（King）”到“王后（Queen）”的向量相似。词嵌入具有的这种特性，在2013年Mikolov等发表的论文[Linguistic Regularities in Continuous Space Word Representations]中提出，成为词嵌入领域具有显著影响力的研究成果。\n上述思想可写成一个余弦（cos）相似度函数：\nsim(u,v)=uTv∣∣u∣∣2∣∣v∣∣2\ns\ni\nm\n(\nu\n,\nv\n)\n=\nu\nT\nv\n∣\n∣\nu\n∣\n∣\n2\n∣∣\nv\n∣\n∣\n2\nsim(u, v) = \\frac{u^T v}{\\mid\\mid u \\mid\\mid_2 \\mid\\mid v \\mid\\mid_2} 以此度量词向量的相似度。\n词嵌入方法\n词嵌入的方法包括人工神经网络、对词语同现矩阵降维、概率模型以及单词所在上下文的显式表示等。以词汇的one-hot形式作为输入，不同的词嵌入方法能以不同的方式学习到一个嵌入矩阵（Embedding Matrix），最后输出某个词的词向量。\n将字典中位置为\ni\ni\ni的词以one-hot形式表示为\noi\no\ni\no_i，嵌入矩阵用\nE\nE\nE表示，词嵌入后生成的词向量用\nei\ne\ni\ne_i表示，则三者存在数学关系：\nE⋅oi=ei\nE\n⋅\no\ni\n=\ne\ni\nE \\cdot o_i = e_i\n例如字典中包含10000个词，每个词的one-hot形式就是个大小为\n10000×1\n10000\n×\n1\n10000 \\times 1的列向量，采用某种方法学习到的嵌入矩阵大小为\n300×10000\n300\n×\n10000\n300 \\times 10000的话，将生成大小为\n300×1\n300\n×\n1\n300 \\times 1的词向量。\n神经概率语言模型\n采用神经网络建立语言模型是学习词嵌入的有效方法之一。2003年Bengio等人的经典之作[A Neural Probabilistic Language Model]中，提出的神经概率语言模型，是早期最成功的词嵌入方法之一。\n模型中，构建了了一个能够通过上下文来预测未知词的神经网络，在训练这个语言模型的同时学习词嵌入。例如将下图中上面的句子作为下面的神经网络的输入：\n经过隐藏层后，最后经Softmax将输出预测结果。其中的嵌入矩阵\nE\nE\nE与\nw\nw\nw、\nb\nb\nb一样，是该网络中的参数，需通过训练得到。训练过程中取语料库中的某些词作为目标词，以目标词的部分上下文作为输入，训练网络输出的预测结果为目标词。得到了嵌入矩阵，就能通过前面所述的数学关系式求得词嵌入后的词向量。\nWord2Vec\nWord2Vec（Word To Vectors）是现在最常用、最流行的词嵌入算法，它由2013年由Mikolov等人在论文[Efficient Estimation of Word Representations in Vector Space]中提出。\nWord2Vec中的Skip-Gram模型，所做的是在语料库中选定某个词（Context），随后在该词的正负10个词距内取一些目标词（Target）与之配对，构造一个用Context预测输出为Target的监督学习问题，训练一个如下图结构的网络：\n\n该网络仅有一个Softmax单元，输出Context下Target出现的条件概率：\np(t∣c)=exp(θTtec)∑mj=1exp(θTjec)\np\n(\nt\n∣\nc\n)\n=\ne\nx\np\n(\nθ\nt\nT\ne\nc\n)\n∑\nj\n=\n1\nm\ne\nx\np\n(\nθ\nj\nT\ne\nc\n)\np(t \\mid c) = \\frac{exp(\\theta_t^T e_c)}{\\sum_{j=1}^m exp(\\theta_j^T e_c)}\n上式中\nθt\nθ\nt\n\\theta_t是一个与输出的Target有关的参数，其中省略了用以纠正偏差的参数。训练过程中还是用交叉熵损失函数。\n选定的Context是常见或不常见的词将影响到训练结果，在实际中，Context并不是单纯地通过在语料库均匀随机采样得到，而是采用了一些策略来平衡选择。\nWord2Vec中还有一种CBOW（Continuous Bag-of-Words Model）模型，它的工作方式是采样上下文中的词来预测中间的词，与Skip-Gram相反。\n以上方法的Softmax单元中产生的计算量往往过大，改进方法之一是使用分级Softmax分类器（Hierarchical Softmax Classifier），采用霍夫曼树（Huffman Tree）来代替隐藏层到输出Softmax层的映射。\n此外，Word2Vec的作者在后续论文[Distributed Representations of Words and Phrases and their Compositionality]中提出了负采样（Negative Sampling）模型，进一步改进和简化了词嵌入方法。\n负采样模型中构造了一个预测给定的单词是否为一对Context-Target的新监督学习问题，采用的网络结构和前面类似：\n\n训练过程中，从语料库中选定Context，输入的词为一对Context-Target，则标签设置为1。另外任取\nk\nk\nk对非Context-Target，作为负样本，标签设置为0。只有较少的训练数据，\nk\nk\nk的值取5~20的话，能达到比较好的效果；拥有大量训练数据，\nk\nk\nk的取值取2~5较为合适。\n原网络中的Softmax变成多个Sigmoid单元，输出Context-Target（c,t）对为正样本（\ny=1\ny\n=\n1\ny = 1 )的概率：\np(y=1∣c,t)=σ(θTtec)\np\n(\ny\n=\n1\n∣\nc\n,\nt\n)\n=\nσ\n(\nθ\nt\nT\ne\nc\n)\np(y = 1 \\mid c, t) = \\sigma(\\theta_t^T e_c)\n其中的\nθt\nθ\nt\n\\theta_t、\nec\ne\nc\ne_c分别代表Target及Context的词向量。通过这种方法将之前的一个复杂的多分类问题变成了多个简单的二分类问题，而降低计算成本。\n模型中还包含了对负样本的采样算法。从本质上来说，选择某个单词来作为负样本的概率取决于它出现频率，对于更经常出现的单词，将更倾向于选择它为负样本，但这样会导致一些极端的情况。模型中采用一下公式来计算选择某个词作为负样本的概率：\np(wi)=f(wi)34∑mj=0f(wj)34\np\n(\nw\ni\n)\n=\nf\n(\nw\ni\n)\n3\n4\n∑\nj\n=\n0\nm\nf\n(\nw\nj\n)\n3\n4\np(w_i) = \\frac{f(w_i)^{\\frac{3}{4}}}{\\sum_{j=0}^m f(w_j)^{\\frac{3}{4}}}\n其中\nf(wi)\nf\n(\nw\ni\n)\nf(w_i)代表语料库中单词\nwi\nw\ni\nw_i出现的频率。\nGloVe\nGloVe（Global Vectors）是另一种现在流行的词嵌入算法,它在2014年由Pennington等人在论文[GloVe: Global Vectors for Word Representation]中提出。\nGlove模型中，首先基于语料库统计了词的共现矩阵\nX\nX\nX，\nX\nX\nX中的元素为\nXi,j\nX\ni\n,\nj\nX_{i,j}，表示整个语料库中单词\ni\ni\ni和单词\nj\nj\nj彼此接近的频率，也就是它们共同出现在一个窗口中的次数。之后要做的，就是优化以下代价函数：\nJ=∑i,jNf(Xi,j)(θTiej+bi+bj−log(Xi,j))2\nJ\n=\n∑\ni\n,\nj\nN\nf\n(\nX\ni\n,\nj\n)\n(\nθ\ni\nT\ne\nj\n+\nb\ni\n+\nb\nj\n−\nl\no\ng\n(\nX\ni\n,\nj\n)\n)\n2\nJ=\\sum_{i,j}^N f(X_{i,j})(\\theta_i^T e_j + b_i + b_j - log(X_{i,j}))^2\n其中\nθi\nθ\ni\n\\theta_i、\nej\ne\nj\ne_j分是单词\ni\ni\ni和单词\nj\nj\nj的词向量，\nbi\nb\ni\nb_i、\nbj\nb\nj\nb_j是两个偏差项，\nf()\nf\n(\n)\nf()是一个用以防止\nXi,j=0\nX\ni\n,\nj\n=\n0\nX_{i,j} = 0时\nlog(Xi,j)\nl\no\ng\n(\nX\ni\n,\nj\n)\nlog(X_{i,j})无解的权重函数，词汇表的大小为\nN\nN\nN。\n（以上优化函数的推导过程见参考资料中的“理解GloVe模型”）\n最后要说明的是，使用各种词嵌入方法学习到的词向量，并不像最开始介绍词嵌入时展示的表格中Man、Woman、King、Queen的词向量那样，其中的值能够代表着与Gender、Royal等词的的相关程度，实际上它们大都超出了人们的能够理解范围。\n词嵌入应用：情感分类器\nNLP中的情感分类，是对某段文字中所表达的情感做出分类，它能在很多个方面得到应用。训练情感分类模型时，面临的挑战之一可能是标记好的训练数据不够多。然而有了词嵌入得到的词向量，只需要中等数量的标记好的训练数据，就能构建出一个表现出色的情感分类器。\n\n如上图，要训练一个将左边的餐厅评价转换为右边评价所属星级的情感分类器，也就是实现\nx\nx\nx到\ny\ny\ny的映射。有了用词嵌入方法获得的嵌入矩阵\nE\nE\nE，一种简单的实现方法如下：\n方法中计算出句中每个单词的词向量后，取这些词向量的平均值输入一个Softmax单元，输出预测结果。这种简单的方法适用于任何长度的评价，但忽略了词的顺序，对于某些包含多个正面评价词的负面评价，很容易预测到错误结果。\n采用RNN能实现一个表现更加出色的情感分类器，此时构建的模型如下：\n\n这是一个“多对一”结构的循环神经网络，每个词的词向量作为网络的输入，由Softmax输出结果。由于词向量是从一个大型的语料库中获得的，这种方法将保证了词的顺序的同时能够对一些词作出泛化。\n词嵌入除偏\n在词嵌入过程中所使用的语料库中，往往会存在一些性别、种族、年龄、性取向等方面的偏见，从而导致获得的词向量中也包含这些偏见。比如使用未除偏的词嵌入结果进行词汇类比时，“男性（Man）”对“程序员（Computer Programmer）”将得到类似“女性（Woman）”对“家务料理人（Homemaker）”的性别偏见结果。2016年Bolukbasi等人在论文[Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings]中提出了一些消除词嵌入中的偏见的方法。\n这里列举消除词向量存在的性别偏见的过程，来说明这些方法。（摘自第二周课后作业）\n1.中和本身与性别无关词汇\n中和（Neutralize）“医生（doctor）”、“老师（teacher）”、“接待员（receptionist）”等本身与性别无关词汇中的偏见，首先计算\ng=ewoman−eman\ng\n=\ne\nw\no\nm\na\nn\n−\ne\nm\na\nn\ng = e_{woman}-e_{man}，用“女性（woman）”的词向量减去“男性（man）”的词向量，得到的向量\ng\ng\ng就代表了“性别（gender）”。假设现有的词向量维数为50，那么对某个词向量，将50维空间分成两个部分：与性别相关的方向\ng\ng\ng和与\ng\ng\ng正交的其他49个维度\ng⊥\ng\n⊥\ng_{\\perp}。如下左图：\n\n除偏的步骤，是将要除偏的词向量，左图中的\nereceptionist\ne\nr\ne\nc\ne\np\nt\ni\no\nn\ni\ns\nt\ne_{receptionist}，在向量\ng\ng\ng方向上的值置为\n0\n0\n0，变成右图所示的\nedebiasedreceptionist\ne\nr\ne\nc\ne\np\nt\ni\no\nn\ni\ns\nt\nd\ne\nb\ni\na\ns\ne\nd\ne_{receptionist}^{debiased}。所用的公式如下:\n\nebiascomponent=e⋅g||g||22×g\ne\nc\no\nm\np\no\nn\ne\nn\nt\nb\ni\na\ns\n=\ne\n⋅\ng\n|\n|\ng\n|\n|\n2\n2\n×\ng\ne^{bias}_{component} = \\frac{e \\cdot g}{||g||_2^2} \\times g\nedebiasedreceptionist=e−ebias_component\ne\nr\ne\nc\ne\np\nt\ni\no\nn\ni\ns\nt\nd\ne\nb\ni\na\ns\ne\nd\n=\ne\n−\ne\nb\ni\na\ns\n_\nc\no\nm\np\no\nn\ne\nn\nt\ne_{receptionist}^{debiased} = e - e^{bias}\\_{component}\n2.均衡本身与性别有关词汇\n对“男演员（actor）”、“女演员（actress）”、“爷爷（grandfather）”等本身与性别有关词汇，如下左图，假设“女演员（actress）”的词向量比“男演员（actor）”更靠近于“婴儿看护人（babysit）”。中和“婴儿看护人（babysit）”中存在的性别偏见后，还是无法保证它到“女演员（actress）”与到“男演员（actor）”的距离相等。对一对这样的词，除偏的过程是均衡（Equalization）它们的性别属性。\n\n均衡过程的核心思想是确保一对词（actor和actress）到\ng⊥\ng\n⊥\ng_{\\perp}的距离相等的同时，也确保了它们到除偏后的某个词（babysit）的距离相等，如上右图。\n对需要除偏的一对词\nw1\nw\n1\nw1、\nw2\nw\n2\nw2，选定与它们相关的某个未中和偏见的单词\nB\nB\nB之后，均衡偏见的过程如下公式：\nμ=ew1+ew22\nμ\n=\ne\nw\n1\n+\ne\nw\n2\n2\n\\mu = \\frac{e_{w1} + e_{w2}}{2}\nμB=μ⋅bias_axis||bias_axis||22×bias_axis\nμ\nB\n=\nμ\n⋅\nbias_axis\n|\n|\nbias_axis\n|\n|\n2\n2\n×\nbias_axis\n\\mu_{B} = \\frac {\\mu \\cdot \\text{bias_axis}}{||\\text{bias_axis}||_2^2} \\times \\text{bias_axis}\nμ⊥=μ−μB\nμ\n⊥\n=\nμ\n−\nμ\nB\n\\mu_{\\perp} = \\mu - \\mu_{B}\new1B=ew1⋅bias_axis||bias_axis||22×bias_axis\ne\nw\n1\nB\n=\ne\nw\n1\n⋅\nbias_axis\n|\n|\nbias_axis\n|\n|\n2\n2\n×\nbias_axis\ne_{w1B} = \\frac {e_{w1} \\cdot \\text{bias_axis}}{||\\text{bias_axis}||_2^2} \\times \\text{bias_axis}\new2B=ew2⋅bias_axis||bias_axis||22×bias_axis\ne\nw\n2\nB\n=\ne\nw\n2\n⋅\nbias_axis\n|\n|\nbias_axis\n|\n|\n2\n2\n×\nbias_axis\ne_{w2B} = \\frac {e_{w2} \\cdot \\text{bias_axis}}{||\\text{bias_axis}||_2^2} \\times \\text{bias_axis}\necorrectedw1B=|1−||μ⊥||22|−−−−−−−−−√×ew1B−μB||(ew1−μ⊥)−μB)||2\ne\nw\n1\nB\nc\no\nr\nr\ne\nc\nt\ne\nd\n=\n|\n1\n−\n|\n|\nμ\n⊥\n|\n|\n2\n2\n|\n×\ne\nw1B\n−\nμ\nB\n|\n|\n(\ne\nw\n1\n−\nμ\n⊥\n)\n−\nμ\nB\n)\n|\n|\n2\ne_{w1B}^{corrected} = \\sqrt{ |{1 - ||\\mu_{\\perp} ||^2_2} |} \\times \\frac{e_{\\text{w1B}} - \\mu_B} {||(e_{w1} - \\mu_{\\perp}) - \\mu_B)||_2}\necorrectedw2B=|1−||μ⊥||22|−−−−−−−−−√×ew2B−μB||(ew1−μ⊥)−μB)||2\ne\nw\n2\nB\nc\no\nr\nr\ne\nc\nt\ne\nd\n=\n|\n1\n−\n|\n|\nμ\n⊥\n|\n|\n2\n2\n|\n×\ne\nw2B\n−\nμ\nB\n|\n|\n(\ne\nw\n1\n−\nμ\n⊥\n)\n−\nμ\nB\n)\n|\n|\n2\ne_{w2B}^{corrected} = \\sqrt{ |{1 - ||\\mu_{\\perp} ||^2_2} |} \\times \\frac{e_{\\text{w2B}} - \\mu_B} {||(e_{w1} - \\mu_{\\perp}) - \\mu_B)||_2}\ne1=ecorrectedw1B+μ⊥\ne\n1\n=\ne\nw\n1\nB\nc\no\nr\nr\ne\nc\nt\ne\nd\n+\nμ\n⊥\ne_1 = e_{w1B}^{corrected} + \\mu_{\\perp}\ne2=ecorrectedw2B+μ⊥\ne\n2\n=\ne\nw\n2\nB\nc\no\nr\nr\ne\nc\nt\ne\nd\n+\nμ\n⊥\ne_2 = e_{w2B}^{corrected} + \\mu_{\\perp}\n参考资料\n吴恩达-序列模型-网易云课堂\nAndrew Ng-Sequence Model-Coursera\ndeeplearning.ai\nDeep Learning in NLP（一）词向量和语言模型\n从SNE到t-SNE再到LargeVis\nword2vec前世今生\nWord2Vec导学第二部分-负采样-csdn\n理解GloVe模型-csdn\n课程代码与资料-GitHub\n注：本文涉及的图片及资料均整理翻译自Andrew Ng的Deep Learning系列课程，版权归其所有。翻译整理水平有限，如有不妥的地方欢迎指出。\n更新历史：\n* 2018.03.08 完成初稿\n原文链接","data":"2018年03月22日 13:48:03"}
{"_id":{"$oid":"5d343b0162f717dc0659b2d9"},"title":"机器学习/自然语言处理方向面试","author":"qq_26919935","content":"360实习生面试过程：\n18年2月初去的360面试，这是第一次去大型互联网公司面试，也只是抱着试试看的念头，但是年后HR给我打电话说我通过了，然后就没再准备其他公司的，一心等3月多去公司。结果，一直没等到正式offer的我给公司打电话，跟我说人招满了，忘记通知我了。呵呵到无话可说，如果第一个电话告诉我说我没过也可以，关键是我都准备着去北京了，结果又是因为其他原因说忘记通知了。。。ε=(´ο｀*)))唉\n面试经过：共3面，第一面是个技术大牛，怼了我一顿，主要有这样几个问题：\n1.当场写代码：tensorflow的应用，比如写一个对图像进行分类的实际应用代码。\n2.keras的应用：不用写代码，但是要知道应用场景\n3.推荐算法的矩阵分解怎么做。\n4.最大似然和贝叶斯分类的区别（回答：先验的有无），分别适用于什么场景：贝叶斯适用于小数据集，因为先验好求。\n5.判别式和生成式的算法各有哪些，区别是什么，分别适用于什么场景。\n6.LR和最大似然的区别\n7.L1和L2范数的区别以及适用的场景\n8.各种算法的损失函数怎么写\n第二面应该也是一个技术，看起来像是管理层，问的比较简单，就是介绍一下自己简历上做过的项目\n三面是HR面试，一个孕妇，问了一下薪资的问题。\n总的2个多小时的时间，3个面试就结束了，可以说是比较有效率。\n滴滴电话面试\n一共是2面，都是技术面试，两个都是电话面试，中间隔了2天时间，大概每个都半个多小时时间。\n一面：面试官环境声音比较嘈杂，很多时候听不清楚，但是听起来脾气蛮好。开始面试官进行了自我介绍，然后我做自我介绍。\n问：选择一个最熟悉的项目进行讲解\n答：我讲的爬虫+文档分类+推荐算法，设计到word2vec,svm,贝叶斯，协同过滤，矩阵分解，冷启动。\n问：文档分类的准确度以及使用什么方法得到的准确度\n答：96%左右，k折交叉验证（面试官讲了他们做测试的时候，是16年的训练，17年的验证）\n问：对深度学习的理解\n答：略讲了CNN卷积，卷积层和pooling层的作用，CNN的主要目的。\n问：从头讲一下LR\n答：loss function+判别式函数+线性分类器（原因：决策平面是线性）+softmax+判别平面受所有数据影响。\n问：对强化学习的理解\n答：并不是很懂\n问：关于SVM的问题\n答：正则化（L1,L2范数），分类平面，拉格朗日\n二面：环境比较清晰，面试官声音很清楚，微信语音面试，也是先自我介绍\n问：指定的智能机器人那个项目讲解\n答：关键是cnn做图像分类算法\n问：对医疗那个项目讲解\n答：使用各种算法做测试，对数据的处理\n问：讲解最熟悉的算法\n答：讲的bp神经网络，梯度下降\n问：其他算法，如k-means\n答：讲了算法流程\n问：svm如何控制模型复杂度（防止过拟合）\n答：自带L2正则，减小W权重参数\n期间面试官给发了一个网址，石墨文档，一个杨辉三角的编程题，只需要写出算法原理就行。做的时候很紧张。\n中科院自动化所视频面试\n3.7号收到中科院的邮件，让看一篇强化学习的论文，Human-level control through deepreinforcement learning，谷歌发在nature上的DQN文章，两天之内给出理解报告。\n3.12号上午10点进行的视频面试，是北大的一个小哥哥面试的，很温柔。没有问太多问题，大部分都是让自己讲，我讲了自己两个项目，然后又讲了k-means,bp神经网络，tensorflow进行的图像分类，关于那篇论文的理解，就结束了。\n总共半个小时的时间。","data":"2018年02月01日 15:31:32"}
{"_id":{"$oid":"5d343b0262f717dc0659b2db"},"title":"ML笔记 - 自然语言处理常用技术","author":"般若Neo","content":"自然语言处理（NLP Natural Language Processing），是人工智能（AI Artificial Intelligence）的一部分，实现人与计算机之间的有效通信。\n自然语言处理属于计算机科学领域与人工智能领域，其研究使用计算机编程来处理与理解人类的语言。\n常见的自然语言处理有文本相似度匹配、情感分析、机器翻译、聊天机器人等。\n分词\n停用词过滤\n词干提取\n词形还原\n词干提取与词形还原\n词袋模型\n\n\nTF-IDF\n\n\n\nWord2Vec\n\n\n代码示例：https://github.com/yyhsong/iMLearning/tree/master/NLP","data":"2019年01月29日 17:23:40"}
{"_id":{"$oid":"5d343b0362f717dc0659b2df"},"title":"人工智能-语音交互-NLP自然语言(五) - 你不知道的机器翻译","author":"杨易","content":"NLP自然语言处理(五) 不可思议的机器翻译\n-除了震惊还是震惊的机器翻译.\n\n\n如果你认为机器翻译就是英译汉、汉译英，那么你落伍了。\n以下是基于神经网络机器翻译技术的机器人写的一首诗\n\n\n\n\n\n\n\n\n\n\n\n\n转载请注明出处,谢谢!","data":"2017年10月29日 17:58:58"}
{"_id":{"$oid":"5d343b0462f717dc0659b2e1"},"title":"自然语言处理相关技术与任务简介","author":"yuquanle","content":"更多学习笔记关注：\n公众号:StudyForAI\n知乎专栏:https://www.zhihu.com/people/yuquanle/columns\n\n\n自然语言处理(NLP)是人工智能的一个重要应用领域，由于本人主要研究方向为NLP，也由于最近学习的需要，特意搜罗资料，整理了一份简要的NLP的基本任务和研究方向，希望对大家有帮助。\n\n\n自然语言的发展： 一般认为1950 年图灵提出著名的“图灵测试”是自然语言处理思想的开端。20 世纪 50 年代到 70 年代自然语言处理主要采用基于规则的方法。基于规则的方法不可能覆盖所有语句，且对开发者的要求极高。这时的自然语言处理停留在理性主义思潮阶段。70 年代以后随着互联网的高速发展，语料库越来越丰富以及硬件更新完善，自然语言处理思潮由理性主义向经验主义过渡，基于统计的方法逐渐代替了基于规则的方法。从 2008 年到现在，由于深度学习在图像识别、语音识别等领域不断取得突破，人们也逐渐开始引入深度学习来做自然语言处理研究，由最初的词向量到 2013 年 word2vec，将深度学习与自然语言处理的结合推向了高潮，并且在机器翻译、问答系统、阅读理解等领域取得了一定成功。\n-----------------------------------------------------------分割线---------------------------------------------------\n\n\n\n\n先来看看自然语言处理的定义：\n自然语言是指汉语、英语等人们日常使用的语言，是随着人类社会发展自然而然的演变而来的语言，不是人造的语言，自然语言是人类学习生活的重要工具。或者说，自然语言是指人类社会约定俗成的，区别于人工语言，如程序设计的语言。\n\n\n处理包含理解、转化、生成等过程。自然语言处理，是指用计算机对自然语言的形、音、义等信息进行处理，即对字(如果是英文即为字符)、词、句、段落、篇章的输入、输出、识别、分析、理解、生成等的操作和加工。实现人机间的信息交流，是人工智能界、计算机科学和语言学界所共同关注的重要问题。所以自然语言处理也被誉为人工智能的掌上明珠。可以说，自然语言处理就是要计算机理解自然语言，自然语言处理机制涉及两个流程，包括自然语言理解和自然语言生成。自然语言理解是指计算机能够理解自然语言文本的意义，自然语言生成则是指能以自然语言文本来表达给定的意图。\n\n\n\n\n自然语言的理解和分析是一个层次化的过程，许多语言学家把这一过程分为五个层次，可以更好地体现语言本身的构成，五个层次分别是语音分析、词法分析、句法分析、语义分析和语用分析。\n\n\n语音分析是要根据音位规则，从语音流中区分出一个个独立的音素，再根据音位形态规则找出音节及其对应的词素或词。\n\n\n词法分析是找出词汇的各个词素，从中获得语言学的信息。\n\n\n句法分析是对句子和短语的结构进行分析，目的是要找出词、短语等的相互关系以及各自在句中的作用。\n\n\n语义分析是指运用各种机器学习方法，学习与理解一段文本所表示的语义内容。 语义分析是一个非常广的概念。\n\n\n语用分析是研究语言所存在的外界环境对语言使用者所产生的影响。\n\n\n\n\n这里根据自己的学习以及查阅相关资料的理解，简要的介绍一下自然语言处理(nlp)一些相关技术以及相关任务，nlp技术包括基础技术和应用技术。\n\n\n基础技术包括词法分析、句法分析、语义分析等。\n\n\n词法分析（lexical analysis）： 包括汉语分词（word segmentation 或 tokenization）和词性标注（part-of-speech tag）等。\n\n\n汉语分词：处理汉语(英文自带分词)首要工作就是要将输入的字串切分为单独的词语，这一步骤称为分词。\n\n\n词性标注：词性标注的目的是为每一个词赋予一个类别，这个类别称为词性标记。比如，名词（noun）、动词（verb）等。\n\n\n句法分析（syntactic parsing）：是对输入的文本句子进行分析得到句子的句法结构的处理过程。最常见的句法分析任务有下列几种：\n\n\n短语结构句法分析（phrase-structure syntactic parsing），该任务也被称作成分句法分析（constituent syntactic parsing），作用是识别出句子中的短语结构以及短语之间的层次句法关系；\n\n\n依存句法分析（dependency syntactic parsing），作用是识别句子中词汇与词汇之间的相互依存关系；\n\n\n深层文法句法分析，即利用深层文法，例如词汇化树邻接文法（Lexicalized Tree Adjoining Grammar， LTAG）、词汇功能文法（Lexical Functional Grammar， LFG）、组合范畴文法（Combinatory Categorial Grammar， CCG）等，对句子进行深层的句法以及语义分析。\n\n\n语义分析（Semantic Analysis）：语义分析的最终目的是理解句子表达的真实语义。但是，语义应该采用什么表示形式一直困扰着研究者们，至今这个问题也没有一个统一的答案。语义角色标注（semantic role labeling）是目前比较成熟的浅层语义分析技术。\n\n\n总而言之，自然语言处理系统通常采用级联的方式，即分词、词性标注、句法分析、语义分析分别训练模型。在使用过程中，给定输入句子，逐一使用各个模块进行分析，最终得到所有结果。近年来，研究者们提出了很多有效的联合模型，将多个任务联合学习和解码，如分词词性联合、词性句法联合、分词词性句法联合、句法语义联合等，取得了不错的效果。特别值得一提的是，今年EMNLP上有一个联合模型的教程，大家可以从这里下载：https://pan.baidu.com/s/1DxOqXxlK-1BCHqMCwr5_ZA。\n\n\n\n\n另一方面是自然语言处理的应用技术：这些任务往往会依赖基础技术，包括文本聚类（Text Clustering）、文本分类（Text Classification）、文本摘要（Text abstract）、情感分析（sentiment analysis）、自动问答（Question Answering, QA）、机器翻译（machine translation， MT）、信息抽取（Information Extraction）、信息推荐(Information Recommendation)、信息检索（Information Retrieval, IR）等。因为每一个任务都涉及的东西很多，因此在这里我知识简单总结介绍一下这些任务，等以后有时间随着我的学习深入，再分专题详细总结各种技术。\n\n\n文本分类：文本分类任务是根据给定文档的内容或主题，自动分配预先定义的类别标签。\n\n\n文本聚类：任务则是根据文档之间的内容或主题相似度，将文档集合划分成若干个子集，每个子集内部的文档相似度较高，而子集之间的相似度较低。\n\n\n文本摘要：文本摘要任务是指通过对原文本进行压缩、提炼，为用户提供简明扼要的文字描述。\n\n\n情感分析：情感分析任务是指利用计算机实现对文本数据的观点、情感、态度、情绪等的分析挖掘。\n\n\n自动问答：自动问答是指利用计算机自动回答用户所提出的问题以满足用户知识需求的任务。\n\n\n机器翻译：机器翻译是指利用计算机实现从一种自然语言到另外一种自然语言的自动翻译。被翻译的语言称为源语言（source language）， 翻译到的语言称作目标语言（target language）。\n\n\n信息抽取：信息抽取是指从非结构化/半结构化文本（如网页、新闻、论文文献、微博等）中提取指定类型的信息（如实体、属性、关系、事件、商品记录等），并通过信息归并、冗余消除和冲突消解等手段将非结构化文本转换为结构化信息的一项综合技术。\n\n\n信息推荐：信息推荐据用户的习惯、 偏好或兴趣， 从不断到来的大规模信息中识别满足用户兴趣的信息的过程。\n\n\n信息检索：信息检索是指将信息按一定的方式加以组织，并通过信息查找满足用户的信息需求的过程和技术。\n\n\n参考：\n1.中文信息处理发展报告(2016)","data":"2018年11月22日 18:45:47","date":"2018年11月22日 18:45:47"}
{"_id":{"$oid":"5d343b0462f717dc0659b2e3"},"title":"课程总结 -- 自然语言处理","author":"xtyang315","content":"这学期修了 Prof. Daniel Gildea 的 Statistical Speech and Language Processing 课程。作为 machine learning 的进阶课程，这门课的确难度不小。本文记录了这两天复习期末考所作的笔记，里面涵盖了本次课程涉及的主要内容（忽略了一些复杂、较为不重要的部分）。另外，列出了课上和复习期间领悟的一些心得，个人以为是上这门课最大的收获。水平有限，理解难免有误，请各位指正。\n另外，推荐大家去看Coursera上哥大的Natural Language Processing课程，讲的非常清楚，例子也很多。\n总结\nNLP 课程主要介绍了以下三类应用：\n\nPOS Tagging：给句子中的每个词标记相应的词性\nParsing：分析得到句子的句法结构（通常得到一个句法树）\nMachine Translation：将源语言表述的句子翻译为目标语言表述的句子\n其实这三类应用之间互有联系，方法也有相似之处。粗略地讲：Parsing 可以看作 Tagging 的进化版，因为它不仅需要对每个词进行标记，还需要标记出层次性的句法结构。因此虽然 Parsing 的算法同样是动态规划（或前向后向），但比 Tagging 要计算多一个维度的信息。MT 尤其是 Syntax based MT 与 Parsing 也有相似之处，只不过除了 Parse 源句子外还要注意两个平行语料库（parrallel corpus）的关系。\n动态规划（DP）在 NLP 中尤为重要。只要涉及到解码问题（decoding），即给定数据（句子）和 模型 （参数）求最优解（序列），基本都要用到 DP 来求解。这就很自然地延伸到两个问题：\n\n为什么要用DP？\n因为可行解的数量太多（对于长度为\nn\nn、状态个数为\nT\nT的解码问题，可行解有\nTn\nT^n个），穷举法效率太低。而DP可以给出多项式时间复杂度的解法。\n为什么可以用DP？\n个人认为这涉及到NLP中模型设计。动态规划适用于有重叠子问题和最优子结构性质的问题。而 NLP 中所用的模型（如HMM，PCFG）的性质使其优化问题具有类似 “\nmax∏\nmax \\prod” 的形式。而 “\nmax∏\nmax \\prod” 可以转化为 “\n∏max\n\\prod max” 来求解（准确来说，需要每一项\n∈ℜ+\n\\in\\Re^+，而概率一定\np≥0\np\\geq 0因此满足该限制），故产生了重叠子问题和最优子结构的性质，从而可以用DP来求解。换句话说，NLP中模型的设计很重要的一点是：可以使用动态规划进行高效求解。\n对于参数求解问题（parameters learning），一般会涉及到以下几种模型和算法，它们之间的优劣值得思考：\n\n隐马尔科夫模型（HMM）和期望最大化算法（EM）：EM常用于学习含有隐变量（hidden variables）的模型。它是一种迭代求解的算法，重复：E步—估计在当前模型参数下的数据分布，以及，M步—通过该分布利用最大似然法更新模型参数。HMM中的E步通过前向后向算法（其实也是动态规划）实现。HMM是最为经典的NLP统计模型，能够表示tags之间的时序依赖关系。个人觉得该算法的一个缺点是比较复杂，计算较慢\n感知机算法（Perceptron Algorithm）：由用于二分类的感知机算法延伸而来，若解码序列与真实序列不符，则更新相应参数的权重。感知机算法的优点是简单，并且可以使用任意定义的特征（相对的，HMM中只有两种特征\nf(yi|yi−1)\nf(y_i|y_{i-1}) 和\nf(xi|yi)\nf(x_i|y_i)）。感知机算法的缺点可能是由于模型简单效果并不是太好，而且是个确定性模型（deterministic）。\n条件随机场（CRF）：属于最大熵模型的一种，也可以看作指数线性模型。与生成式模型（generative）的HMM不同，CRF是判别式模型（discriminative），即HMM建模\nP(XN1,YN1)\nP(X_1^N,Y_1^N) 而CRF建模\nP(YN1|XN1)\nP(Y_1^N|X_1^N)。另外，CRF没有隐变量，而且是无向图。CRF的学习可以用梯度下降算法，但也需要用到前向后向算法来计算当前模型参数下特征的分布。CRF的优点是与感知机一样可以使用任意特征，但相比感知机而言它是一个概率模型。\n结构化支持向量机（Structured SVM）：属于最大间隔模型，同样从用于二分类的SVM延伸而来，其模型可以与感知机算法进行类比。SVM其实跟感知机算法没有太大区别，甚至可以看成加入了L2正则化的感知机算法。而Structured SVM还有一项改变是：在限制条件中加入了损失函数（loss function），导致解码时需要把损失函数考虑进来（还是动态规划）。因此Structured SVM具有一个优点：可以结合不同的损失函数。\n大部分模型学习的更新公式都具有相似的形式，即：加上真实的减去预测的（move forwards the y we saw and move away from the\nỹ\n\\tilde{y} we expect to see)。例子有：Perceptron、CRF、Structured SVM。至于原因嘛，还需要深入思考。\nQ：为什么有了维特比解码（Viterbi），还需要后验解码（即HMM中的前向后向，Parsing中的Inside-Outside）？\nA：首先，维特比算法是求解解码问题的正确方法，保证解码后的序列得到的score最大。但我们在评价performance的时候，除了看该序列是否正确（这对于一个序列只是对和错的二分问题），还要看每一位解码的正确与否（如真实序列为ABCD，解码后序列为ABBD，虽然是错的，但有其中三位都是对的）。这样就会导致另一种损失函数：\nL=∑iI(yi≠yi~)\nL=\\sum_iI(y_i\\neq\\tilde{y_i})。而后验解码正是相对应这种损失函数评价下的解码算法，因此也叫做Minimum Bayes Risk （MBR）算法。由于这种评价方法的存在，后验解码通常会得到比维特比解码更好的结果。那么反过来，为什么我们还需要维特比算法呢？因为它更加简单高效（对比前向后向算法），而且它本来就是对的，只是评价标准不同而已。\nHMM可以看作CFG的特列，在笔记中略有介绍。\nSCFG部分在笔记里懒得写了，感兴趣可以参考这篇文章：Introduction to synchronous grammar\n还有其他感悟，等我想起来再继续添加。。\n笔记\n地址：https://github.com/xyang35/course-NLP/blob/master/notes/nlp_note.pdf\n目录：\n1 Part of Speech (POS) Tagging\nPerception Algorithm\nHMM\nConditional Random Field (CRF)\nStructured SVM\n2 Parsing\nContext Free Grammar (CFG)\nCKY Parsing (Viterbi Decoding)\nPosterior Decoding (Inside-Outside Algorithm)\n3 Machine Translation\nWord Alignment (IBM Models)\nPhrase Based Translation\nSyntax Based Translation (SCFG)","data":"2015年12月21日 08:52:34"}
{"_id":{"$oid":"5d343b0462f717dc0659b2e5"},"title":"闲聊人工智能产品经理（AIPM）—人工智能产品体系","author":"暴走的鹏鹏哥哥","content":"如何了解人工智能产品体系\n我们从搭建一个人工智能产品需要一个怎样的基础架构，到剖析架构中每个组件的含义以及对整个系统起到的作用和扮演的角色，最后对每个组件展开讲起。\n1、人工智能产品实现逻辑\n通常的一款人工智能产品涉及了很多技术，包括语音识别、语音合成、机器视觉、自然语言处理、文本/语义理解等多项技术等交互集成。人工智能的目标是模拟和延伸人的感知、理解、决策、学习、交流、移动和操作物体的能力。感知是人工智能实现的第一步，目前已经有了实质性的进展。理解和决策需要机器学习和人类指导相结合的方式才能实现。\n目前阶段的人工智能还是弱人工智能，产品的流程可以概括为：海量数据训练和学习，从中识别规律和经验，新数据通过得到的经验用接近人的思维处理。\n通过对角色分工、处理过程、功能价值三个不同的角度，一个人工智能产品的体系包含四个重要角色：\n1、基础设施提供者。\n2、数据提供者。\n3、数据处理者。\n4、系统协调者。\n我们从数据流开始说起。\n人工智能的产品体系是一个动态流程，本质上是围绕数据采集、存储、计算展开的。\n1）数据提供者使用各种手段获得原始数据。\n2）数据处理者对数据进行加工。\n3）数据处理者进行模型训练，获得可以使用对模型。\n4）用模型对新数据进行预测。\n以上我们就完成了“数据--信息--知识--智慧”的过程，再随着动态循环，就是“训练--推断--再训练--再推断”的过程。产品经理需要完成系统集成、需求定义、资源协调、解决方案封装的保障工作。\n2、基础设施\n1）传感器：对信号模式进行转换。主要应用于可穿戴应用、高级辅助驾驶、健康监测、工业控制。举个例子，无人车对传感器有激光、毫米波、超声波、红外线等，产品经理需要对不同对传感器有自己对了解。\n2）芯片：完成训练和推断的强大计算能力的计算核心。模型训练：对神经网络和海量数据计算对核心部件应该有充足对了解。云端推断：服务器对CPU、GPU、TPU等计算单元。终端设备：手机、摄像头等。\n按照定制化程度，芯片又分为：\n通用芯片：CPU、GPU、TPU等，可以处理通用任务类型。\nFPGA半定制化芯片：延时低，用硬件实现软件算法。\nASIC：算法模型可以烧到芯片中，运行效率高。理论上先用FPGA在市场中试错，之后用ASIC量产。\n3）基础平台：\n1、大数据技术：算法虽好，数据决胜。\n2、云计算技术：降低了研发成本。\n3、数据收集\n数据收集类似于人类对各种感觉，没有感觉就无法判断。\n1、数据来源：直接购买行业数据和免费的数据源；自行采集和爬取；第三方合作。\n2、数据质量：（1）关联度；（2）时效性；（3）范围；（4）可信性。\n4、数据处理\n对原始数据对加工。可以概括为：数据  ---  机器学习给出规则  ---  新数据通过规则得到结果  ---  伴随着输入/输出的过程自我优化\n5、机器“大脑”处理过程：识别、理解和推理、决策\n1）识别：大量大量的数据存在计算机中计算得到一个模型，对于新数据判断。\n2）理解和推理：识别侧重于人对环境的感知，理解和推理强电深层次的理解和归纳能力，是对识别之后的数据的再次处理过程。\n3）做决策：通过对外界客观事物、环境、推理和理解来判断采取怎样的行动。\n6、系统配置统筹的关键环节：系统协调\n构建一个人工智能系统需要多方协调：包括基础设施提供者、信息提供者、信息处理者在内的各种公司或公司内部各个部门。系统协调者需要在人工智能的不同阶段：需求定义、设计开发、系统优化、运行保障、售后支持、监控和审计发回资源协调和统筹作用。\n人工智能产品体系最常见的发展规律是：一开始以项目交付解决单个场景的具体需求为主，看重个性化；当项目的技术和产品需求验证完毕后，就可以使产品走向千人千面的产品化；接下来是服务化，通过对外开放和输出各种服务能力，逐渐与终端用户具体业务解耦，统一数据中心和算法平台；最终实现平台化，帮助用户实现根据自身需求完成各种功能模块的在线快速封装和灵活配置。\n考虑到企业的发展速度、市场规模、技术实现瓶颈及业务特殊性多方面因素，需要人工智能产品经理具有成本意识、市场敏锐度、前瞻性和大局观等综合素质。\n7、不可逾越的红线：安全、隐私、伦理、道德\n（1）安全：人工智能产品认为可控；人工智能产品不会影响公共安全。\n（2）隐私：人工智能产品经理至少要评估一下四项：\n1、评估所有产品流程中涉及用户权利的风险。\n2、评估产品在设计或运行过程中的系统描述。\n3、基于产品设计或运行的目的，评估过程是否是必要的。\n4、针对识别出的风险，给出针对风险的管理措施。\n在涉及到隐私数据保护措施中，我们可以从三个方面着手：\n1、减少对训练数据量的需求：\n1）生成对抗网络（GAN）：通过轮流训练判别器和生成器，令其互相对抗，从复杂概率分布中取样，生成文字、图片、语音等。\n2）联合学习（Federal Learning）：部分训练过程放到用户手机，将模型传回服务器，不涉及用户敏感数据。\n3）迁移学习（Transfer Learning）：把一个场景学习到的模型举一反三迁移到类似的场景中的方法。\n2、在不减少数据的基础上保护隐私：\n1）差分隐私技术（Different Privacy）：在数据库检索时，加入满足某种分布的噪声，使查询结果随机化。\n2）同态加密技术（Homomorphic Encryption）：在密文上进行计算，生成加密结果，解密后的结果与对明文进行相同操作产生的结果一致。核心在于，支持在加密的数据上进行查询操作，解决数据委托给第三方如云计算公司时的安全问题。\n3）提高算法可解释性，避免黑盒子事件的发生。\n（3）伦理道德\n在产品设计时，主要从以下三个方面重点关注人工智能的特殊性所带来的伦理问题：\n1、人工智能产品算法的“可解释性差”、“不透明”，使得一旦发生伦理道德事故无法评判。\n2、人工智能代替人履行社会职能的时候，产品的“不可预见性”有可能导致伦理道德争议。\n3、人工智能产品的道德地位值得思考。\n8、运维管理\n人工智能产品的运维和传统IT运维的出发点都是让业务高效稳定的运行。评价标准：\n1、系统能否第一时间发现异常。\n2、发现异常后能否第一时间找出原因。\n3、从原因能否定义到具体问题。\n4、问题能否自动修复或者自我修复。\n5、未来出现类似问题能否提前预警。","data":"2019年06月19日 06:25:30"}
{"_id":{"$oid":"5d343b0562f717dc0659b2e7"},"title":"中文自然语言处理入门之Hanlp介绍","author":"adnb34g","content":"自然语言处理定义：\n自然语言处理是一门计算机科学、人工智能以及语言学的交叉学科。虽然语言只是人工智能的一部分（人工智能还包括计算机视觉等），但它是非常独特的一部分。这个星球上有许多生物拥有超过人类的视觉系统，但只有人类才拥有这么高级的语言。\n自然语言处理的目标是让计算机处理或说“理解”自然语言，以完成有意义的任务，比如订机票购物或QA等。完全理解和表达语言是极其困难的，完美的语言理解等效于实现人工智能。\n自然语言处理涉及的几个层次：\n自然语言处理涉及的层次\n作为输入一共有两个来源，语音与文本。所以第一级是语音识别和OCR或分词（事实上，跳过分词虽然理所当然地不能做句法分析，但字符级也可以直接做不少应用）。接下来是形态学，援引《统计自然语言处理》中的定义：形态学（morphology）：形态学（又称“词汇形态学”或“词法”）是语言学的一个分支，研究词的内部结构，包括屈折变化和构词法两个部分。由于词具有语音特征、句法特征和语义特征，形态学处于音位学、句法学和语义学的结合部位，所以形态学是每个语言学家都要关注的一门学科。\nHanlp自然语言处理开发包：\n从事大数据方面工作的人对自然语言处理必然都是不陌生的，在Github上用户量最多的开源汉语自然语言处理工具是HanLP。HanLP的初始版本是在2014年初开发的，3月份的时候开始在Github上开源。2015年的时候集成在了大快搜索的DKNLP中，目前大快已经把DKNLP技术成果已经开源，并且整体装如HanLP项目，HanLP的版本已经到了V1.50。\nHanlp自然语言处理技术优势：\n支持中文分词（N-最短路分词、CRF分词、索引分词、用户自定义词调、词性标注），命名实体识别（中国人民、音译人民、日本人民，地名，实体机构名识别），关键词提取，自动摘要，短语提取，拼音转换，简繁转换，文本推荐，依存句法分析（MaxEnt依存句法分析、神经网络依存句法分析）。提供Lucene查件，兼容Solr和ElasticSearch。\nHanlp自然语言处理应用领域：\nHanlp已经被广泛应用于Lucene、Solr、ElasticSearch、hadoop、android、Resin等平台，有大量开源作者开发各种查件与拓展，并且被包装或移植到Python、C#、R、JavaScript等语言上去。","data":"2018年07月24日 11:17:43","date":"2018年07月24日 11:17:43"}
{"_id":{"$oid":"5d343b0562f717dc0659b2e9"},"title":"《数学之美》—— 读后总结","author":"xing halo","content":"《数学之美》—— 读后总结\n这本书中的一个个小故事（知识点）是源自于吴军博士在Google的黑板报，所以整本书是由许多个小部分组成的。整本书主要的宗旨还是在讲述数学在自然语言处理、语音识别、搜索、通信等领域的作用，大致按照下面的流程讲述：\n语言的兴起\n人工智能\n自然语言处理\n中文分词\n隐马尔可夫\n信息熵\n贾里尼克\n布尔与搜索\n图论与爬虫\nPageRank——相关性与可信度\nTF-IDF\n余弦定理与分类\n矩阵运算与文本处理\n信息指纹\n密码学\n搜索引擎\n最大熵模型\n拼音输入法\n马库斯\n布隆过滤\n贝叶斯网络\n条件随机场\n维特比\nK均值与分类\n逻辑回归与广告\nMapReduce\n可以看到内容还是很多的，读者可以根据自己感兴趣的章节从前往后跳跃性的阅读。阅读之后，我想应该可以对搜索排名、文本分类、输入法优化等方面有一些收获。\nposted @ 2017-08-26 23:36 xingoo 阅读(...) 评论(...) 编辑 收藏","data":"2017年08月26日 23:36:00"}
{"_id":{"$oid":"5d343b0662f717dc0659b2eb"},"title":"【备忘】机器读心术之文本挖掘与自然语言处理人工智能视频","author":"qq_38472451","content":"01、自然语言处理与文本挖掘概述\n02、自动机及其应用，文稿自动校正，歧义消除\n03、语言模型，平滑方法。应用案例：语音识别，分词消岐\n04、概率图模型，生成式模型与判别式模型，贝叶斯网，马尔科夫链，隐马尔科夫模型HMM，应用案例：语音识别与分词\n05、马尔科夫网，最大熵模型，条件随机场CRF，实现HMM和CRF的软件。应用案例：使用最大熵消除歧义，使用CRF进行标注\n06、汉语分词专题。世界上最难的语言名不虚传\n07、命名实体识别，词性标注，从文本里挖出最重要的内容\n08、句法分析，找出句子的重点\n09、语义分析与篇章分析，让机器象语言学家那样思考\n10、文本分类，情感分析。应用案例：互联网自动门户，评论倾向性分析\n11、信息检索系统，搜索引擎原理，问答系统，应用案例：客服机器人是怎么造出来的？\n12、文本深度挖掘：自动文摘与信息抽取\n13、机器翻译与语音识别技术介绍 IBM Watson系统的认知智慧\n\n分词算法\n下载地址：百度网盘下载","data":"2018年01月15日 17:04:21"}
{"_id":{"$oid":"5d343b0662f717dc0659b2ed"},"title":"读书总结-《数学之美》","author":"爱淋雨的男人","content":"主要概要有：\n语言的兴起\n人工智能\n自然语言处理\n中文分词\n隐马尔可夫\n信息熵\n贾里尼克\n布尔与搜索\n图论与爬虫\nPageRank——相关性与可信度\nTF-IDF\n余弦定理与分类\n矩阵运算与文本处理\n信息指纹\n密码学\n搜索引擎\n最大熵模型\n拼音输入法\n马库斯\n布隆过滤\n贝叶斯网络\n条件随机场\n维特比\nK均值与分类\n逻辑回归与广告\nMapReduce\n关键内容有：\n1.信息度量\n信息就是不确定性的多少，信息就是要减少不确定性；\n熵: 信息的混杂程度，越大，信息越杂，越不纯；\n条件熵: 一个信息确定的条件下，另外一个信息不确定度的减少量；\n互信息: 在一个信息的条件下，为了是另外一个信息不确定度减少所需要提供的信息量；\n相对熵: 衡量两个函数值为正数的函数的相关性。\n2.指纹信息\n指纹: 每段信息包括文字，图片，音频，等都可以对应一组不太长的随机数\n伪随机数:压缩\n基于加密的伪随机数:密码\n集合的判定，文章，网页的判定，视频的判定\n指纹可能重复，但可能性很小\n相似哈希:词，权重，指纹，二进制的结合(提供了一种思路)\n3.最大熵模型\n最大熵原理: 保留全部的不确定性，让风险降到最小；\n最大熵模型: 在所有满足约束条件的模型中选出熵最大的模型；\n模型学习: 任何一组不自相矛盾的信息，最大熵模型存在并且唯一，都具有相同的形式，指数形式；\n特点: 能同时满足成千上万的中不同条件的模型(有效的组合很多特征)\n参数训练: 对数似然函数求极大\n4.期望最大\n如果模型的变量都是观测变量，用极大似然估计或贝叶斯估计\n如果存在隐含变量，用EM迭代，最大后验概率\n典型:kmeans聚类，隐马的参数训练，最大熵模型的训练\n特点: 局部最优，计算速度慢\n5.散列表与布隆过滤器\n散列表的核心:哈希函数hashcode(),equals()函数；\n散列表的特点:时间复杂度o(1),浪费空间，冲突；\n布隆过滤器核心: 一组二进制数和随机映射函数；\n布隆过滤器的特点: 时间复杂度o(1)，节约空间，到存在错误率\n6.文本分类\n相似性: 余弦定理，距离\n方法: k近邻思想，自底向上的两两合并，EM迭代，奇异值分解；\n技巧: 计算时存储重复计算的变量，只考虑非零元素，删除虚词\n余弦定理和奇异分解:余弦定理多次迭代，计算量大，消耗资源多；svd无需多次迭代，时间短，但存储空间需求大，适合超大规模分类；建议svd粗分类，余弦定理细分类\nTF-IDF解决两个重要问题:词的预测能力越强，权重越大；停止词的权重为零\n7.隐马尔可夫\n马尔可夫假设: t时刻的状态只取决于t-1时刻\n马尔可夫链: 状态链\n隐马模型: 初始概率分布，状态转移概率分布，观测概率分布(马尔可夫假设，观测独立)\n3个问题:\n参数估计-baum-uelch算法\n计算概率-直接，前向，后向算法\n预测状态-维特比算法(动态规划)\n8.贝叶斯网络\n是马尔可夫链的推广(链状-拓扑)\n又称信念网络: 弧+可信度\n训练: 结构和参数训练，交叉进行\n方法: 贪心算法，蒙卡，互信息\n9.条件随机场\n特点:观测值可能和前后的状态都有关\n条件随机场是无向图，贝叶斯网络是有向图\n核心:找到符合所有边缘分布的最大熵模型\n10.有限状态机和动态规划\n有限状态机: 开始，终止状态，有向弧，条件\n常见:  建立状态机，已知状态机匹配字符串\n区别: 基于概率的有限状态机和离散马尔可夫链等效\n动态规划: 把全程路径最短锁定到局部路径最短\n\n\n作者：哈得死\n链接：https://www.jianshu.com/p/0b997bd1c125\n来源：简书\n简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。","data":"2019年04月27日 19:55:55"}
{"_id":{"$oid":"5d343b0762f717dc0659b2ef"},"title":"对自然语言处理nlp的一点感想","author":"虎扑掌门人","content":"自然语言处理（nlp）作为计算机的一个研究方向存在已久，但是最近人工智能这一波热潮又让nlp重新得到巨大关注。由于处理对象是语言这一种人类特有的沟通工具以及其丰富巨大的信息量，给人一种错觉--似乎这是人工智能领域真正的皇冠，达到最终真正人工智能（强人工智能）的最近之路。但是事实是如何不敢随意断言，只是有点感慨一下，就是这一块很难做。\n语言作为人类的工具，一方面可以让我们与外界交互，可以说话，可以记录，可以标记，保存了巨大的信息。这样的信息，只有人类才能读懂并理解。我们是否可以理解为，如果个人心智是一个操作系统的话，语言和符号标记是操作系统上的一些接口函数，通过这些接口函数进行操作系统与操作系统之间的交互，即人与人的交流。\n然而，关于这个“操作系统”，医学科学家生物学家认知科学家都还没有完全弄清楚人的意识以及思维活动的具体过程，只能部分描述而已。在语言层面，也只能用语言来表达语言、用语言解释语言，不过正如递归那样，用初始的少量定义来描述全部情况也是很划算的。\n因此，对于自然语言处理的研究虽不断深入，现有水平下仍然无法触及人类或类人智能的核心问题。当然有人说不和人一样的智能也可以，不过这条路也是很难走。目前的研究，仍然是在语言内部做数据映射处理为主，是以encode和decode为主要内容的。这正如清华大学刘知远老师所说“自然语言处理是工具链”，只是工具链，而非信息的源头或终端。\n看以下这个例子：甲说：“今晚来我家吃饭”乙说“晚上我爸要回家”。仅从两句话单独的语义分析，就会觉得牛头不对马嘴，发觉毫无联系。计算机必须像人一样看到一句话具有联想推断等能力，具有分析对方这句话有哪几种含义的潜意识，才能提高文本的理解度。而这个过程，我们人类在潜意识（或者说在刚才提到的操作系统中）中已经处理好了。而这些正是让计算机具有语言智能的巨大难题，这可能需要多个学科的众多科学家精诚合作数十年甚至数百年数千年才能解决，但它终归是要被解决的。","data":"2018年02月20日 05:06:34","date":"2018年02月20日 05:06:34"}
{"_id":{"$oid":"5d343b0762f717dc0659b2f1"},"title":"自然语言处理与DuReader概述","author":"cyberspice","content":"自然语言处理与DuReader概述\n“自然语言处理是人工智能桂冠上的明珠”，这句话反应了NLP发展之艰巨。事实上，语言理解被我们认为是“AI的终极任务”，要解决这一难题，前提是要能解决全部人类水平人工智能的问题（https://www.yangfenzi.com/sousuo/61444.html）\n百度在自然语言处理上研究和推进，正式实际的应用出发的内部的迫切需求。请见下面链接文章https://www.leiphone.com/news/201805/0UtBTaxsxpOqEU3h.html. （插播一句，建议某些公司研究技术要应用的实际促进产品中去，搜索结果不能让人怀疑技术都是临时抱佛脚吧。）\n目前机器阅读理解国外做的比较多。例如在2018年伊始，阿里巴巴和微软亚洲研究院相继刷新了斯坦福大学发起的SQuAD（Stanford Question Answering Dataset）文本理解挑战赛成绩，机器阅读理解评分超过人类！这意味着机器阅读理解的能力已经开始在“指标”上超越人类，又是否能够引领自然语言处理（NLP）领域的下一场革命？（https://www.huxiu.com/article/233577.html?f=member_article）\nSQuAD是斯坦福大学于2016年推出的阅读理解数据集，也是行业内公认的机器阅读理解标准水平测试，该数据集包含来自维基百科的536篇文章及共计十万多个问题。在阅读数据集内的文章后，机器需要回答若干与文章内容相关的问题，通过与标准答案对比来获取得分。目前整体排名第一的是科大讯飞与哈工大联合实验室，得分为82.482或者89.281（标准不同。微软MARCO也应用在机器阅读理解领域，是由10万个问答和20万篇不重复的文档组成的数据集。相比SQuAD，其最大不同在于数据集中的问题来自微软自家必应搜索引擎。MARCO的挑战难度更大，它需要测试者提交的模型具备理解复杂文档、回答复杂问题的能力。目前最高得分为百度NLP团队获得，为（46.72 50.45 70.96，不同标准）；DuReader数据集，目前最好的模型与人的准确率接近，但是还是很低， 约是60%的准确率，相比英文阅读，还是有很大差距。\n机器阅读理解取得的成绩确实是一个突破性的进展，其可能是继机器翻译之后又一个取得重要进展的NLP领域；但机器阅读理解仍然是一种限定边界的任务(从某种意义上来讲，目前机器的阅读理解，是把问题词作为查找向量从文章中搜寻答案的词语位置，几乎没有变换及推断题)，机器阅读理解是一种边界限定的场景式机器理解，问题的前提条件和场景边界都比较清楚，所以机器阅读理解超过人类是以“设定文章集合、有限问题”为前提条件的，远远达不到真正的归纳和推理，因此对于人类的胜利更应该说是“指标”上的胜利。“以机器阅读理解任务来说，机器应该很快会从指标上超过人类的现有水平，但真正的阅读理解过程需要深层的推理和归纳，这恰恰是目前机器所欠缺的，还需要通过底层算法的突破才有可能实现机器在NLP领域的真正突破。”王士进谈到。 有人认为下一步推动NLP发展可能在知识图谱层面，通过知识图谱构建机器对任务的认知能力，再加以语义、交互等处理工具，通过应用才能更好推动一个行业的发展。其实知识图谱是源，不是机器阅读理解的钥匙，现在的核心是算法以及数据的自然性需要改进。\n目前机器阅读理解的技术现状，1956 年乔姆斯基借鉴香农的工作，把有限状态机用作刻画语法的工具，建立了自然语言的有限状态模型，具体来说就是用“代数”和“集合”将语言转化为符号序列，建立了一大堆有关语法的数学模型(https://www.sohu.com/a/204243606_500659)。90 年代以来，基于统计的自然语言处理就开始大放异彩了。大家的重心开始转向大规模真实文本了，传统的仅仅基于规则的自然语言处理显然力不从心了。如句法剖析、词类标注、参照消解、话语处理的算法几乎把“概率”与“数据”作为标准方法，成为了自然语言处理的主流。其实语言词汇，用法，上下下文环境，都会导致自然语言处理的难度。“当前中文自然语言处理发展还不甚成熟的时期，私以为基于统计的方法在很多方面并不完美，“理性主义”的作用空间还很大”（总成庆）\n深度学习掀开了机器学习的新篇章，目前深度学习应用于图像和语音已经产生了突破性的研究进展。(https://www.cnblogs.com/maybe2030/p/5427148.html)引用三年前一位网友的话来讲：　　“Steve Renals算了一下icassp录取文章题目中包含deep learning的数量，发现有44篇，而naacl则有0篇。有一种说法是，语言（词、句子、篇章等）属于人类认知过程中产生的高层认知抽象实体，而语音和图像属于较为底层的原始输入信号，所以后两者更适合做deep learning来学习特征。”\n研究难点：（https://baike.baidu.com/item/NLP/25220）\n单词的边界界定:在英语等语言中，使用空格等自然分词，这在有别于汉语等，需要使用者自己根据语义来自我划分词语边界。在汉语口语中，词与词之间通常是连贯的，而界定字词边界通常使用的办法是取用能让给定的上下文最为通顺且在文法上无误的一种最佳组合，这其中往往也有断句导致的沟通问题。在书写上，汉语也没有词与词之间的边界。在古汉语研究中，断句也是需要仔细的分析。\n词义的消歧：许多字词不单只有一个意思，因而我们必须选出使句意最为通顺的解释。\n句法的模糊性：自然语言的文法通常是模棱两可的，针对一个句子通常可能会剖析（Parse）出多棵剖析树（Parse Tree），而我们必须要仰赖语意及前后文的资讯才能在其中选择一棵最为适合的剖析树。\n语言行为与机动响应：句子常常并不只是字面上的意思；例如，“你能把盐递过来吗”，一个好的回答应当是把盐递过去；在大多数上下文环境中，“能”将是糟糕的回答，虽说回答“不”或者“太远了我拿不到”也是可以接受的。再者，如果一门课程去年没开设，对于提问“这门课程去年有多少学生没通过？”回答“去年没开这门课”要比回答“没人没通过”好。\n阅读库的完备性：自然语言，内容多样，语法语义，使用场景，都不能很好的界定数据库的特点。现在Squid，微软Marco，百度的dureader缺乏统一有效的标准，都是一种收集或者固定来源整理。\nhttps://blog.csdn.net/weixin_38440272/article/details/80239298\n机器阅读理解其实和人阅读理解面临的问题是类似的，不过为了降低任务难度，很多目前研究的机器阅读理解都将世界知识排除在外，采用人工构造的比较简单的数据集，以及回答一些相对简单的问题。\nhttps://zhuanlan.zhihu.com/p/22671467\n机器对语言的理解过程，可以分为几个步骤，其中很多的不确定性是逐渐明晰的（语音识别的不确定性更多，因为还要解决从声音到词的转换）。第一步是要把词分开，放到依存树上，看哪一个词是动词，对名词有哪些影响等等。随后，要理解每一个名字的含义。再次，再加入许多先验知识，即对这个世界的理解，因为很多句子只有使用了这些信息才能真正理解。如果足够幸运的话，到这就能得到清晰的理解了。https://www.yangfenzi.com/sousuo/61444.html:\n自然语言处理领域进化出了深度神经网络的一种新模式，该模式分为：embed、encode、attend、predict四部分https://blog.csdn.net/jdbc/article/details/53292414  然而，大多数NLP问题面对的不是单个词语，而是需要分析更长的文本内容。现在有一个简单而灵活的解决方案，它在许多任务上都表现出了卓越的性能，即RNN模型。\n高级词向量三部曲：https://blog.csdn.net/sinat_26917383/article/details/54850933\n1、NLP︱高级词向量表达（一）：“GloVe（理论、相关测评结果、R\u0026python实现、相关应用）\n2、NLP︱高级词向量表达（二）：FastText（简述、学习笔记）\n3、NLP︱高级词向量表达（三）：WordRank（简述）\n4、其他NLP词表示方法paper:从符号到分布式表示NLP中词各种表示方法综述\n如果用分类问题，用CNN。对于顺序建模，需要联系上下文，用RNN。深度学习是一个年轻的领域，理论建立并不完备，观点也会快速变化\n目前机器阅读理解研究领域出现了非常多的具体模型，如果对这些模型进行技术思路梳理的话，会发现本质上大多数模型都是论文“Teaching Machines to Read and Comprehend”提出的两个基础模型”Attentive Reader”和“Impatient Reader”的变体，当然很多后续模型在结构上看上去有了很大的变化，但是如果仔细推敲的话会发现根源和基础思路并未发生颠覆性的改变。（https://zhuanlan.zhihu.com/p/22671467深度学习解决机器阅读理解任务的研究进展。）\nDuReader，一个新的大型开放中文机器阅读理解数据集，其在中文应用中还是很有开创意义。实际上是”百度知道”和以及“百度搜索”，数据的全面性和权威性令人存疑。Dureader的基本内容介绍可以参见：\nhttps://blog.csdn.net/LiJiancheng0614/article/details/80866088\n由于规模大且问题类型复杂，基于DuReader数据集的分析工作相比以往数据集都要难得多。百度通过计算人工答案和文档的最小编辑距离来判断回答问题的困难度。编辑距离越大，对文档的编辑修改就更多，回答问题的复杂度也就越高。对于答案直接来源于原文的数据集（如SQuAD），它们的编辑距离应该是0。 下图展示了MS-MARCO和DuReader两个数据集答案与文档编辑距离分布情况。（https://zhuanlan.zhihu.com/p/36415104）\n图 MS-MARCO和DuReader两个数据集答案与文档编辑距离分布情况\n从图可以看出，在同为人工标注的数据集MS-MARCO中，77.1%的样本的编辑距离低于3，而在DuReader中51.3%的样本的编辑距离高于10，这说明DuReader更为复杂。\n百度基于DuReader构建了两个基线模型：Match-LSTM和BiDAF。\nMatch-LSTM是广泛应用的MRC模型，Match-LSTM为了在文章中找到答案，依次遍历文章，动态地将注意力权重与文章的每个标记进行匹配。最后，使用一个应答指针层来查找文章中的答案跨度。\nBiDAF既使用了语境对问题的注意，又使用了问题对上下文的注意，从而突出了问题和上下文中的重要部分。然后，利用注意流层融合所有有用的信息，从而得到每个位置的向量表示。\n下面对Match-LSTM进行分享：（https://www.imooc.com/article/25027)\n模型都分为三部分：\nEmbed 层使用词向量表示原文和问题；\nEncode 层使用单向 LSTM 编码原文和问题 embedding；\nInteraction层对原文中每个词，计算其关于问题的注意力分布，并使用该注意力分布汇总问题表示，将原文该词表示和对应问题表示输入另一个 LSTM编码，得到该词的 query-aware 表示；\n在反方向重复步骤 2，获得双向 query-aware 表示；\nAnswer 层基于双向 query-aware 表示使用 Sequence Model 或 Boundary Model预测答案范围。\nBiDAF[5]\n相比于之前工作，BiDAF（Bi-Directional Attention Flow）最大的改进在于 Interaction 层中引入了双向注意力机制，即首先计算一个原文和问题的 Alignment matrix，然后基于该矩阵计算 Query2Context 和 Context2Query 两种注意力，并基于注意力计算 query-aware 的原文表示，接着使用双向 LSTM 进行语义信息的聚合。\n另外，其 Embed 层中混合了词级 embedding 和字符级 embedding，词级 embedding 使用预训练的词向量进行初始化，而字符级 embedding 使用 CNN 进一步编码，两种 embedding 共同经过 2 层 Highway Network 作为 Encode 层输入。最后，BiDAF 同样使用 Boundary Model 来预测答案开始和结束位置。\n目前有很多的框架实现版本，例如Tensorflow, Pytorch, Paddlepadlle, QAnet等。尽管目前结果提升比较明显，但是离英文机器阅读理解取得的成绩尚有很大距离。\n目前笔者认为现在实现彻底的机器阅读理解，不妨从人类阅读理解的本质以及能从算法上实现渐进的路标和更加丰富完备的阅读理解数据集。","data":"2018年09月10日 01:33:50"}
{"_id":{"$oid":"5d343b0762f717dc0659b2f3"},"title":"14本人工智能技术入门书籍下载","author":"逆流的云","content":"书单详解与下载链接\n学习 OpenCV\n人工智能：一种现代的方法\n智能 Web 算法\n语音与语言处理\n模式识别与机器学习\n游戏人工智能编程案例精粹\n统计自然语言处理基础\n模式分类\n模式识别中的神经网络\n计算机视觉\n人工智能游戏编程真言\nJava 设计模式（第 2 版）\nPython 自然语言处理\n实用 Common Lisp 编程","data":"2018年01月22日 10:49:01"}
{"_id":{"$oid":"5d343b0862f717dc0659b2f5"},"title":"自然语言处理（NLP）学习路线总结","author":"Asia-Lee","content":"目录\n1、自然语言处理概述\n2、自然语言处理入门基础\n3、自然语言处理的主要技术范畴\n4、自然语言处理基本点\n5、特征处理\n6、模型选择\n7、NLP常用工具\n8、NLP语言模型\n9、快速入门NLP方法\n10、自然语言处理学习资料\n1、自然语言处理概述\n自然语言处理（Natural Language Processing，NLP）是计算机科学领域与人工智能领域中的一个重要方向。它研究人与计算机之间用自然语言进行有效通信的理论和方法。融语言学、计算机科学、数学等于一体的科学。旨在从文本数据中提取信息。目的是让计算机处理或“理解”自然语言，以执行自动翻译、文本分类和情感分析等。自然语言处理是人工智能中最为困难的问题之一。\n2、自然语言处理入门基础\n2.1 数学基础\n（1）线性代数\n向量、 矩阵、距离计算（余弦距离、欧式距离、曼哈顿距离、明可夫斯基距离、切比雪夫距离、杰卡德距离、汉明距离、标准欧式距离、皮尔逊相关系数）\n（2）概率论\n随机试验、条件概率、全概率、贝叶斯定理、信息论\n（3）统计学\n图形可视化（饼图、条形图、热力图、折线图、箱线图、散点图、雷达图、仪表盘）\n数据度量标准（平均数、中位数、众数、期望、方差、标准差）\n概率分布（几何分布、二项分布、正态分布、泊松分布）\n统计假设检验\n2.2 语言学基础\n语音、词汇、语法\n2.3 Python基础\n廖雪峰教程，Python从入门到实践\n2.4 机器学习基础\n统计学习方法、机器学习周志华、机器学习实战\n2.5 深度学习基础\nCNN、RNN、LSTM\n2.6 自然语言处理的理论基础\n统计自然语言处理（宗成庆第二版）、Python自然语言处理、数学之美（第二版）\n3、自然语言处理的主要技术范畴\n3.1 语义文本相似度分析\n语义文本相似度分析是对两段文本的意义和本质之间的相似度进行分析的过程。\n3.2 信息检索（Information Retrieval, IR）\n信息检索是指将信息按一定的方式加以组织，并通过信息查找满足用户的信息需求的过程和技术。\n3.3 信息抽取（Information Extraction）\n信息抽取是指从非结构化/半结构化文本（如网页、新闻、 论文文献、微博等）中提取指定类型的信息（如实体、属性、关系、事件、商品记录等），并通过信息归并、冗余消除和冲突消解等手段将非结构化文本转换为结构化信息的一项综合技术。\n3.4 文本分类（Text Categorization）\n文本分类的任务是根据给定文档的内容或主题，自动分配预先定义的类别标签。\n3.5 文本挖掘（Text Mining）\n文本挖掘是信息挖掘的一个研究分支，用于基于文本信息的知识发现。文本挖掘的准备工作由文本收集、文本分析和特征修剪三个步骤组成。目前研究和应用最多的几种文本挖掘技术有：文档聚类、文档分类和摘要抽取。\n3.6 文本情感分析（Textual Affective Analysis）\n情感分析是一种广泛的主观分析，它使用自然语言处理技术来识别客户评论的语义情感，语句表达的情绪正负面以及通过语音分析或书面文字判断其表达的情感等。\n3.7 问答系统（Question Answering, QA）\n自动问答是指利用计算机自动回答用户所提出的问题以满足用户知识需求的任务。不同于现有搜索引擎，问答系统是信息服务的一种高级形式，系统返回用户的不再是基于关键词匹配排序的文档列表，而是精准的自然语言答案。\n3.8 机器翻译（Machine Translation，MT）\n机器翻译是指利用计算机实现从一种自然语言到另外一种自然语言的自动翻译。被翻译的语言称为源语言（source language），翻译到的语言称作目标语言（target language）。\n机器翻译研究的目标就是建立有效的自动翻译方法、模型和系统，打破语言壁垒，最终实现任意时间、任意地点和任意语言的自动翻译，完成人们无障碍自由交流的梦想。\n3.9 自动摘要（Automatic Summarization）\n自动文摘（又称自动文档摘要）是指通过自动分析给定的一篇文档或多篇文档，提炼、总结其中的要点信息，最终输出一篇长度较短、可读性良好的摘要（通常包含几句话或数百字），该摘要中的句子可直接出自原文，也可重新撰写所得。\n3.10 语音识别（Speech Recognition）\n语言识别指的是将不同语言的文本区分出来。其利用语言的统计和语法属性来执行此任务。语言识别也可以被认为是文本分类的特殊情况。\n4、自然语言处理基本点\n4.1 语料库（Corpus）\n语料库中存放的是在语言的实际使用中真实出现过的语言材料；语料库是以电子计算机为载体承载语言知识的基础资源；真实语料需要经过加工（分析和处理），才能成为有用的资源。\n4.2 中文分词（Chinese Word egmentation）\n（1）中文分词指的是将一个汉字序列切分成一个一个单独的词。分词就是将连续的字序列按照一定的规范重新组合成词序列的过程。\n（2）现有的分词方法可分为三大类：基于字符串匹配的分词方法、基于理解的分词方法和基于统计的分词方法。\n（3）比较流行的中文分词工具：jieba、StanfordNLP、HanLP、SnowNLP、THULAC、NLPIR\n4.3 词性标注（Part-of-speech tagging）\n（1）词性标注是指为给定句子中的每个词赋予正确的词法标记，给定一个切好词的句子，词性标注的目的是为每一个词赋予一个类别，这个类别称为词性标记（part-of-speech tag），比如，名词（noun）、动词（verb）、形容词（adjective）等。\n（2）词性标注是一个非常典型的序列标注问题。最初采用的方法是隐马尔科夫生成式模型， 然后是判别式的最大熵模型、支持向量机模型，目前学术界通常采用的结构是感知器模型和条件随机场模型。近年来，随着深度学习技术的发展，研究者们也提出了很多有效的基于深层神经网络的词性标注方法。\n4.4 句法分析（Parsing）\n（1）基于规则的句法结构分析\n（2）基于统计的语法结构分析\n4.5 词干提取（Stemming）\n词干提取是将词语去除变化或衍生形式，转换为词干或原型形式的过程。词干提取的目标是将相关词语还原为同样的词干。\n4.6 词形还原（Lemmatization）\n词形还原是将一组词语还原为词源或词典的词目形式的过程。\n4.7 停用词过滤\n停用词过滤是指在文本中频繁出现且对文本信息的内容或分类类别贡献不大甚至无贡献的词语，如常见的介词、冠词、助词、情态动词、代词以及连词等。\n4.8 词向量化（Word Vector）\n词向量化是用一组实数构成的向量代表自然语言的叫法。这种技术非常实用，因为电脑无法处理自然语言。词向量化可以捕捉到自然语言和实数间的本质关系。通过词向量化，一个词语或者一段短语可以用一个定维的向量表示。（word2vec）\nfrom gensim.models import Word2Vec\n4.9 命名实体消歧（Named Entity Disambiguation）\n命名实体消岐是对句子中的提到的实体识别的过程。\n例如，对句子“Apple earned a revenue of 200 Billion USD in 2016”，命名实体消岐会推断出句子中的Apple是苹果公司而不是指一种水果。一般来说，命名实体要求有一个实体知识库，能够将句子中提到的实体和知识库联系起来。\n4.10 命名实体识别（named entity recognition）\n命名实体识别是识别一个句子中有特定意义的实体并将其区分为人名，机构名，日期，地名，时间等类别的任务。\n三种主流算法：CRF，字典法和混合方法\n5、特征处理\n5.1 特征提取（Feature Extraction）\n特征提取是指将机器学习算法不能识别的原始数据转化为算法可以识别的特征的过程。\n举例（文本分类特征提取步骤）：\n（1）对训练数据集的每篇文章，我们进行词语的统计，以形成一个词典向量。词典向量里包含了训练数据里的所有词语（假设停用词已去除），且每个词语代表词典向量中的一个元素。\n（2）在经过第一步的处理后，每篇文章都可以用词典向量来表示。这样一来，每篇文章都可以被看作是元素相同且长度相同的向量，不同的文章具有不同的向量值。这也就是表示文本的词袋模型（bag of words）。\n（3）针对于特定的文章，如何给表示它的向量的每一个元素赋值呢？最简单直接的办法就是0-1法了。简单来说，对于每一篇文章，我们扫描它的词语集合，如果某一个词语出现在了词典中，那么该词语在词典向量中对应的元素置为1，否则为0。\n5.2 特征选择（ Feature Selection）\n当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。特征选择是指去掉无关特征，保留相关特征的过程，也可以认为是从所有的特征中选择一个最好的特征子集。特征选择本质上可以认为是降维的过程。\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n5.3 降维（Dimension Reduction）\n6、模型选择\n6.1 马尔可夫模型、隐马尔可夫模型、层次化隐马尔可夫模型、马尔可夫网络\n（1）应用：词类标注、语音识别、局部句法剖析、语块分析、命名实体识别、信息抽取等。应用于自然科学、工程技术、生物科技、公用事业、信道编码等多个领域。\n（2）马尔可夫链：在随机过程中，每个语言符号的出现概率不相互独立，每个随机试验的当前状态依赖于此前状态，这种链就是马尔可夫链。\n（3）多元马尔科夫链：考虑前一个语言符号对后一个语言符号出现概率的影响，这样得出的语言成分的链叫做一重马尔可夫链，也是二元语法。二重马尔可夫链，也是三元语法，三重马尔可夫链，也是四元语法\n6.2 条件随机场（CRF）\n（1）条件随机场用于序列标注，中文分词、中文人名识别和歧义消解等自然语言处理中，表现出很好的效果。原理是：对给定的观察序列和标注序列，建立条件概率模型。条件随机场可用于不同预测问题，其学习方法通常是极大似然估计。\n（2）条件随机场模型也需要解决三个基本问题：特征的选择、参数训练和解码。\n6.3 贝叶斯网络\n贝叶斯网络又称为信度网络或信念网络（belief networks）,是一种基于概率推理的数学模型，其理论基础是贝叶斯公式。\n6.4 最大熵模型\n7、NLP常用工具\n（1）Anaconda\nAnaconda是一个用于科学计算的Python开发平台，支持 Linux，Mac和Windows系统，提供了包管理与环境管理的功能，可以很方便地解决多版本Python并存、切换以及各种第三方包安装问题。Anaconda利用conda命令来进行package和environment的管理，并且已经包含了Python和相关的配套工具。Anaconda集成了大量的机器学习库以及数据处理必不可少的第三方库，比如NumPy，SciPy，Scikit-Learn以及TensorFlow等。\n（2）Scikit-learn\nScikit-learn是广受欢迎的入门级机器学习库，包含大量的机器学习算法和特征提取实现，使用非常简便。Scikit-learn实现的是浅层学习算法，神经网络仅实现了多层感知机。\n（3）TensorFlow\nTensorFlow是谷歌基于DistBelief进行研发的第二代人工智能学习系统,可被用于语音识别或图像识别等多项机器学习和深度学习领域。\n（4）Keras\nKeras是一个高级别的Python神经网络框架，能在TensorFlow或者 Theano 上运行。Keras的作者、谷歌AI研究员Francois Chollet宣布了一条激动人心的消息，Keras将会成为第一个被添加到TensorFlow核心中的高级别框架，这将会让Keras变成Tensorflow的默认API。\n（5）Gensim\nGensim是一款开源的第三方Python工具包，用于从原始的非结构化的文本中，无监督地学习到文本隐层的主题向量表达。它支持包括TF-IDF，LSA，LDA，和word2vec在内的多种主题模型算法，支持流式训练，并提供了诸如相似度计算，信息检索等一些常用任务的API接口。\n（6）NLTK\n在NLP领域中，NLTK是最常使用的一个Python库。\n（7）Jieba\nJieba，结巴分词是最受欢迎的中文分词工具。\n8、NLP语言模型\n（1）词的独热表示（one-hot representation）\n（2）Bag of Words\n（3）Bi-gram 和 N-gram\n（4）词的分布式表示（distributed representation）\n（5）共现矩阵（Cocurrence martrix）\n（6）神经网络语言模型（Neural Networ Language model，NNLM）\n（7）word2vec\n连续词袋模型（Continuous Bag of Words，CBOW）\nSkip-Gram模型\n9、快速入门NLP方法\n（1）认真看完一本NLP相关的书，坚持看完一部视频。\n（2）看这两年相关方向的综述论文，然后看一些经典的论文和最新论文。\n（3）独立实现一个小型的自然语言处理项目。\n（4）可以在Github上找到很多相关的开源代码，选一个自己感兴趣的方向进行研究。\n10、自然语言处理学习资料\n（1）我爱自然语言处理\n（2）一文读懂自然语言NLP\n（3）中文分词原理与工具\n（4）自然语言处理项目资源库汇总","data":"2019年01月03日 16:56:51","date":"2019年01月03日 16:56:51"}
{"_id":{"$oid":"5d343b0862f717dc0659b2f7"},"title":"初识自然语言处理---自然语言处理研究报告（技术篇）","author":"进击的cxy","content":"自然语言处理基础技术\n自然语言处理基础技术\n自然语言的基础技术包括词汇、短语、句子和篇章级别的表示，以及分词、句法分析和语义分析等。\n词法分析的主要任务是词性标注和词义标注。词性标注就是在给定句子中判断每个词的语法范畴，确定其词性并进行标注。解决兼类词和确定未登录词的词性问题是标注的重点。词义标注的重点就是解决如何确定多义词在具体语境中的义项问题。标注过程中，通常是先确定语境，再明确词义，方法和词性标注类似。\n判断句子的句法结构和组成句子的各成分，明确它们之间的相互关系是句法分析的主要任务。句法分析通常有完全句法分析和浅层句法分析两种，完全句法分析是通过一系列的句法分析过程最终得到一个句子的完整的句法树，存在两个难点，一是词性歧义；二是搜索空间太大，通常是句子中词的个数n的指数级。浅层句法分析又叫部分句法分析或语块分析，只要求识别出句子中某些结构相对简单的成分如动词短语、非递归的名词短语等，这些结构被称为语块。一般来说，浅层语法分析会完成语块的识别和分析，语块之间依存关系的分析两个任务，其中语块的识别和分析是浅层语法分析的主要任务。\n语义分析是指根据句子的句法结构和句子中每个实词的词义推导出来能够反映这个句子意义的某种形式化表示，即将人类能够理解的自然语言转化为计算机能够理解的形式语言。\n自然语言处理的基础研究还包括语用语境和篇章分析。语用是指人对语言的具体运用，研究和分析语言使用者的真正用意，它与语境、语言使用者的知识涵养、言语行为、想法和意图是分不开的，是对自然语言的深层理解。情景语境和文化语境是语境分析主要涉及的方面，篇章分析则是将研究扩展到句子的界限之外，对段落和整篇文章进行理解和分析。\n之外，自然语言的基础研究还涉及词义消歧、指代消解、命名实体识别等方面的研究。\n自然语言处理核心技术\n自然语言处理核心技术包括知识图谱、文本分类与聚类、自动文摘、自动问答、信息抽取、文字识别、语音技术、信息推荐与过滤、多模态信息处理等。\n知识图谱\n知识图谱，是为了表示知识，描述客观世界的概念、实体、事件等之间关系的一种表示形式。起源可以追溯至语义网络——提出于20世纪五六十年代的一种知识表示形式。语义网络由许多个节点和边组成，这些节点和边相互连接，节点表示的是概念或对象，边表示各个节点之间的关系，如下图。\n语义网络示意图\n知识图谱在表现形式上与语义网络比较类似，不同的是，语义网络侧重于表示概念与概念之间的关系，而知识图谱更侧重于表述实体之间的关系。现在的知识网络被用来泛指大规模的知识库，知识图谱中包含的节点有以下几种：\n实体：指独立存在且具有某种区别性的事物，如一个人、一种动物、一个国家、一种植物等。\n语义类：具有同种特性的实体构成的集合，如人类、动物、国家、植物等。\n内容：通常是实体和语义类的名字、描述、解释等，变现形式一般有文本、图像、音视频等。\n属性（值）：主要指对象指定属性的值，不同的属性类型对应于不同类型属性的边。\n关系：在知识图谱上，表现形式是一个将节点（实体、语义类、属性值）映射到布尔值的函数。\n文本分类与聚类\n自动文摘\n自动文摘是运用计算机技术，依据用户需求从源文本中提取最重要的信息内容，进行精简、提炼和总结，最后生成一个精简版本的过程。生成的文摘具有压缩性、内容完整性和可读性。\n自动问答\n自动问答是指利用计算机自动回答用户所提出的问题以满足用户知识需求的任务。系统反馈给用户的不再是基于关键词匹配排序的文档列表，而是精准的自然语言答案。\n在自然语言理解领域，自动问答和机器翻译、复述和文本摘要一起被认为是验证机器是否具备自然理解能力的四个任务\n。\n根据目标数据源的不同，问答技术大致可以分为检索式问答、社区问答以及知识库问答三种。检索式问答通过检索和匹配回答问题，推理能力较弱。社区问答是 web2.0 的产物，用户生成内容是其基础，Yahoo！Answer、百度知道等是典型代表，这些社区问答数据覆盖了大量的用户知识和用户需求。检索式问答和社区问答的核心是浅层语义分析和关键词匹配，而知识库问答则正在逐步实现知识的深层逻辑推理。\n信息抽取\n信息抽取主要是指从文本中抽取出特定的事实信息，例如从经济新闻中抽取新发布产品情况如公司新产品名、发布时间、发布地点、产品情况等，这些被抽取出来的信息通常以结构化的形式直接存入数据库，可以供用户查询及进一步分析使用，为之后构建知识库、智能问答等提供数据支撑。\n信息检索与信息\n信息检索\n信息抽取\n目的\n从大量的文档中找到用户所需要的文档\n在文本中获取用户感兴趣或所需要的事实信息\n技术\n以关键字词匹配以及统计等技术，不需要对文本进行理解和分析\n需要利用自然语言处理的技术，包括命名实体识别、句法分析、篇章分析与推理以及知识库等，对文本进行深入理解和分析后才能完成信息抽取工作\n联系\n信息检索的结果可以作为信息抽取的范围，提高效率\n信息抽取用于信息检索可以提高检索质量，更好地满足用户的需求\n社会计算\n社会计算也称计算社会学，指在互联网的环境下，以现代信息技术为手段，以社会科学理论为指导，帮助人们分析社会关系，挖掘社会知识，协助社会沟通，研究社会规律，破解社会难题的学科。\n社会媒体是社会计算的主要工具和手段，它是一种在线交互媒体，有着广泛的用户参与性，允许用户在线交流、协作、发布、分享、传递信息、组成虚拟的网络社区等等。\n文字识别\n语音技术\n信息推荐与过滤\n多模态信息处理\n自然语言处理应用技术\n自然语言处理应用技术包括机器翻译、信息检索、情感分析、社会媒体处理等。\n机器翻译\n运用机器，通过特定的计算机程序将一种书写形式或声音形式的自然语言，翻译成另一种书写形式或声音形式的自然语言。\n机器翻译的方法总体上可以分为基于理性的研究方法和基于经验的研究方法两种。\n“理性主义”的翻译方法\n“经验主义”的翻译方法\n特点\n由人类专家通过编撰规则的方式，将不同自然语言之间的转换规律生成算法，计算机通过这种规则进行翻译\n以数据驱动为基础，主张计算机自动从大规模数据中学习自然语言之间的转换规律\n优势\n理论上能够把握语言间深层次的转换规律\n互联网文本数据不断增长，计算机运算能力也不断加强，以数据驱动为基础的统计翻译方法逐渐成为机器翻译的主流技术\n局限性\n对专家的要求极高，不仅要求其了解源语言和目标语言，还要具备一定的语言学知识和翻译知识，更要熟练掌握计算机的相关操作技能\n统计机器翻译也面临诸如数据稀疏、难以设计特征等问题\n现实\n研制系统的成本高、周期长，面向小语种的翻译更是人才匮乏非常困难。因此，翻译知识和语言学知识的获取成为基于理性的机器翻译方法所面临的主要问题\n深度学习能够较好的缓解统计机器翻译所面临的挑战，基于深度学习的机器翻译现在正获得迅速发展，成为当前机器翻译领域的热点\n文本翻译最为主流的工作方式依然是以传统的统计机器翻译和神经网络翻译为主；语音翻译可能是目前机器翻译中比较富有创新意思的领域，吸引了众多资金和公众的注意力；图像翻译也有不小的进展；除此之外还有视频翻译和 VR 翻译也在逐渐应用中，但是目前的应用还不太成熟。\n信息检索\n信息检索是从相关文档集合中查找用户所需信息的过程。\n信息检索包括“存”与“取”两个方面，对信息进行收集、标引、描述、组织，进行有序的存放是“存”。按照某种查询机制从有序存放的信息集合（数据库）中找出用户所需信息或获取其线索的过程是“取”。信息检索的基本原理是将用户输入的检索关键词与数据库中的标引词进行对比，当二者匹配成功时，检索成功。检索标识是为沟通文献标引和检索关键词而编制的人工语言，通过检索标识可以实现“存”“取”的联系一致。检索结果按照与提问词的关联度输出，供用户选择，用户则采用“关键词查询+选择性浏览”的交互方式获取信息。\n情感分析\n情感分析又称意见挖掘，是指通过计算技术对文本的主客观性、观点、情绪、极性的挖掘和分析，对文本的情感倾向做出分类判断。","data":"2019年01月11日 19:53:03","date":"2019年01月11日 19:53:03"}
{"_id":{"$oid":"5d343b0962f717dc0659b2fa"},"title":"自然语言处理与工业设计","author":"weixin_44940911","content":"自然语言处理的加持与工业设计 学习了大数据与人工智能的课程，知道了自然语言处理是研究计算机处理人类语言的一门技术，身为一名工业设计专业的学生，我觉得自然语言处理能和工业设计达到一个质的结合。 随着科技水平的提高，各个领域都会往科技方面有所偏向，而工业设计旨在创新为人们提升生活的质量，因此便利和创新是在工业设计领域的每一个人的追求，工业设计涉及到产品、系统、服务、体验等等各个方面，而这些方面，都可以加入自然语言处理这门技术。 工业设计里如果融入了自然语言处理，设计出来的产品也会更加智能化、更加符合人们的需求。在设计出的产品中，可能不同情景下有着不同的表现，因此自然语言处理往往可以加入到产品的设计中去帮助用户理解产品的语言，人群的多样性使得产品的受众人群变得单一，但如果加入自然语言处理，产品的贴合度就会得到扩展。\n自然语言处理指在人与计算机之间，通过自然语言进行有效通信的各种技术和方法。但由于语言的复杂性，处理过程中常常会涉及理解，因此被认为是距离人工智能最近的任务。其理解在工业设计中也能得到充分应用。\n如产品可以加入语音智能，通过对用户语言的识别通过自然语言的处理转换为另一种自然语言反馈给用户，根据每一个用户的不同点给予每一个用户的最优的反馈，让产品的优势得以最大化的展示，这便是自然语言处理的在产品设计中的应用优势。与其相似的还有翻译，就是把输入的源语言文本通过自动翻译获得另外一种语言的文本，如文本翻译，语音翻译，手语翻译，图形翻译等，如果设计一台翻译机，加入自然语言处理，就能让其发挥最大优势。再者还有问答系统，就是对一个自然语言表达的问题，由问答系统给予一个精准的答案 ，这其中就是通过自然语言的处理转化的。还有就是对话系统，系统通过一系列的对话，跟用户进行聊天，回答，完成某项任务。这涉及到用户意图理解，通用聊天引擎，问答引擎，对话管理等技术的加持。这就可以与现在火热的私人订制相结合了，设计出一台私人订制机器。这些运用自然语言处理的方面，无不可以加载到工业产品上，与其产生完美结合。\n科学技术日新月异，每一天都有着巨大变化，技术与技术中有着紧密联系，而领域与领域间也可以相互加持结合，让世界多元化，让明天更美好！","data":"2019年06月20日 12:40:36"}
{"_id":{"$oid":"5d343b0b62f717dc0659b300"},"title":"NLP自然语言处理实例：预测天气冷暖","author":"csdn怀","content":"NLP：自然语言处理（Natural Language Processing）是人工智能和语言学领域的分支学科。主要包括自然语言理解和生成，自然语言理解系统把自然语言转化为计算机程序更易于处理的形式即让电脑懂人类的语言。自然语言生成系统把计算机数据转化自然语言。\n处理过程：形式化描述-\u003e数学模型算法化-\u003e程序化-\u003e实用化\n使用Python语言，首先需要安装numpy、matplotlib库（也可以安装Anaconda实现）\nKNN算法实现预测功能\nKNN（K-nearest Neighbor）邻近算法，或者说K最近邻(kNN，k-NearestNeighbor)分类算法是数据挖掘分类技术中最简单的方法之一。所谓K最近邻，就是k个最近的邻居的意思，说的是每个样本都可以用它最接近的k个邻居来代表。\nKNN算法思想\n计算一直类别中数据集的点与当前点的距离\n计算样本距离并排序\n选取距离样本最近的K个点\n确定K个点所在类别的出现频率\n返回K个点出现频率最高的类别作为预测结果\nKNN算法模型流程与实现\n1. 搜集数据：数据采集过程，分为非结构化数据和结构化数据，如网络爬虫，数据库，文件等\n2.准备数据：格式化处理，对不同类别的数据进行处理，如转为统一csv格式\n3.分析数据：主要看数据特点，有没有缺失，数据离散性还是连续性，进而选择不同模型\n跟着网上视频敲的代码，完整如下\n# coding:utf-8 \"\"\" NLP 自然语言学习 \"\"\" import numpy as np import matplotlib.pyplot as plt import matplotlib.font_manager as fm import math,operator # 中文乱码 myfont =fm.FontProperties(fname='C:\\Windows\\Fonts\\simsunb.ttf') #只支持后缀ttc plt.rcParams['font.sans-serif']=['SimHei'] \"数据保存到文件中\" def create_dataset(): datasets = np.array([[8,4,2],[7,1,1,],[1,4,4],[3,0,5]]) # 数据集 labels = ['非常热','非常热','一般热','一般热'] # 类标签 return datasets,labels def create_datasets(): datasets = np.array([[8,4,2],[7,1,1,],[1,4,4],[3,0,5],[3,0,4],[5,2,1],[5,3,2]]) # 数据集 labels = [0,0,1,1,0,0,1] #['非常热','非常热','一般热','一般热','一般热'] # 类标签 return datasets,labels \"可视化分析数据\" def analyze_data_plot(x,y): fig = plt.figure() ax = fig.add_subplot(111) ax.scatter(x,y) # plt.scatter(x,y) #设置散点图标题和横坐标 # plt.title('冷热感知图',fontsize=25,fontproperties=myfont) plt.title('冷热感知图',fontsize=25) # plt.xlabel('冰淇淋',fontsize=15,fontproperties=myfont) plt.xlabel('冰淇淋',fontsize=15) # plt.ylabel('喝水', fontsize=15, fontproperties=myfont) plt.ylabel('喝水',fontsize=15) # 自动保存 plt.savefig('result.png',bbox_inches='tight') plt.show() \"构造KNN分类器\" def knn_classifier(newV, datasets, labels, k): # 1.计算样本数据和样本库数据的距离 sqrtDist = EuclideanDis3(newV,datasets) # 2.根据距离排序,按照列向量排序 sortedDistIndexs = sqrtDist.argsort(axis=0) # 3.针对k个值，统计各个类别的数量 classCount = {} for i in range(k): # 根据距离排序，索引值找到类标签 votelabel = labels[sortedDistIndexs[i]] # 统计类标签的键值对 classCount[votelabel] = classCount.get(votelabel,0)+1 # 4.投票机制，少数服从多数原则 # 对各个分类字典进行排序，降序，按照值 sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(1),reverse=True) # print('结果预测：',sortedClassCount[0][0]) return sortedClassCount[0][0] \"欧式距离计算 d2=(x1-x2)2+(y1-y2)2\" def computeEuclideanDis(x1,x2,y1,y2): d = math.sqrt(math.pow(x1-x2,2)+math.pow(y1-y2,2)) return d \"欧式距离计算优化公式\" def EuclideanDis(instance1,instance2): d = 0 length = len(instance1) for x in range(length): d += math.pow(instance1[x]-instance2[x],2) return math.sqrt(d) \"欧式距离计算3:大量数据计算\" def EuclideanDis3(newV,datasets): # 获取向量维度 rowsize,colsize = datasets.shape # 各特征向量间做差值 diffMat = np.tile(newV,(rowsize,1))-datasets # 差值平方 sqDiffMat = diffMat ** 2 # 差值开方求和 sqrtDist = sqDiffMat.sum(axis=1) ** 0.5 return sqrtDist \"利用KNN随机预测访客天气感知度\" def predict_temperature(): # 创建数据集和类标签 datasets, labels = create_dataset() newV = [2, 4, 4] iceCream = float(input(\"Q:请问你今天吃了几个冰淇淋？\\n\")) drinkWater = float(input(\"Q:请问你今天喝了几瓶水？\\n\")) playHours = float(input(\"Q:请问你今天在户外玩了几个小时?\\n\")) newV = np.array([iceCream, drinkWater, playHours]) # vecs = np.array([[2, 4, 4], [3, 0, 0], [5, 7, 2]]) # for i in vecs: res = knn_classifier(newV, datasets, labels, 3) print('KNN天气预测结果', res) \"使用机器学习库sklearn实现预测\" from sklearn import neighbors def knn_sklearn_predict(): # 调用机器学习库knn分类器算法 knn = neighbors.KNeighborsClassifier() datasets,labels = create_datasets() # 传入参数，特征数据和分类标签 print(datasets) knn.fit(datasets,labels) # knn预测 predictRes = knn.predict([[2,4,0]]) print(\"天气：\\t\",\"非常热\" if predictRes[0]==0 else '一般热') return predictRes if __name__ == '__main__': # predict_temperature() knn_sklearn_predict()","data":"2018年11月17日 19:29:23"}
{"_id":{"$oid":"5d343b0b62f717dc0659b302"},"title":"人工智能 | 自然语言处理（NLP）（国内外研究组）","author":"冲动的MJ","content":"博主github：https://github.com/MichaelBeechan\n博主CSDN：https://blog.csdn.net/u011344545\n============================================\n概念篇：https://blog.csdn.net/u011344545/article/details/89525801\n技术篇：https://blog.csdn.net/u011344545/article/details/89526149\n人才篇：https://blog.csdn.net/u011344545/article/details/89556941\n应用篇：https://blog.csdn.net/u011344545/article/details/89574915\n下载链接：https://download.csdn.net/download/u011344545/11147085\n============================================\n清华AMiner团队 AMiner.org\n国外：\nThe Language Technologies Institute (LTI) at Carnegie Mellon University\n卡内基梅隆大学语言技术研究所主要研究内容包括自然语言处理、计算语言学、信息提取、信息检索、文本挖掘分析、知识表示、机器学习、机器翻译、多通道计算和交互、语音处理、语音界面和对话处理等。\nThe Stanford Natural Language Processing Group\n斯坦福大学自然语言处理小组包括了语言学和计算机科学系的成员，是斯坦福人工智能实验室的一部分。主要研究计算机处理和理解人类语言的算法，工作范围从计算语言学的基本研究到语言处理的关键应用技术均有涉猎，涵盖句子理解、自动问答、机器翻译、语法解析和标签、情绪分析和模型的文本和视觉场景等。该小组的一个显著特征是将复杂和深入的语言建模和数据分析与 NLP 的创新概率、机器学习和深度学习方法有效地结合在一起。\nThe Berkeley NLP Group\n伯克利大学自然语言处理小组分属于加州大学伯克利分校计算机科学部。主要从事以下几方面的研究工作，语言分析、机器翻译、计算机语言学、基于语义的方法、无监督学习等，多次在顶级国际会议（ACL、EMNLP、AAAI、IJCAI、COLING 等）上发表多篇论文\nNatural Language Processing Group at University of Notre Dame\n圣母大学自然语言处理小组主要关注机器翻译领域，并有多个项目的研究，如由 DARPALORELEI 和 Google 赞助的无监督多语言学习模型和算法研究；由亚马逊学术研究奖和谷歌教师研究奖赞助的研究，主要研究课题方向包括基于神经网络的机器翻译模型，以及使用神经网络进行翻译和语言建模的算法等。多次在国际顶级期刊和会议上发表论文\nThe Harvard Natural Language Processing Group\n哈佛自然语言处理小组主要通过机器学习的方法处理人类语言，主要兴趣集中在数列生成的数学模型，以人类语言为基础的人工智能挑战以及用统计工具对语言结构进行探索等方面。该小组的研究出版物和开源项目集中在文本总结、神经机器翻译、反复神经网络的可视化、收缩神经网络的算法、文档中实体跟踪的模型、多模态文本生成、语法错误修正和文本生成的新方法等方面。\nNatural Language Processing group of Columbia University\n哥伦比亚大学自然语言处理研究室是在计算机科学系、计算学习系统中心和生物医学信息系的支持下进行的，将语言洞察力与严谨前沿的机器学习方法和其他计算方法结合起来进行研究。在语言资源创造如语料库、词典等，阿拉伯语 NLP，语言和社交网络，机器翻译，信息提取，数据挖掘，词汇语义、词义消除歧义等方面有着比较深入的研究。\n国内：\n中科院计算所自然语言处理研究组\n自然语言处理研究组隶属于中国科学院计算技术研究所智能信息处理重点实验室。研究组教师有刘群、冯洋等人。研究组主要从事自然语言处理和机器翻译相关的研究工作，研究方向包括机器翻译、人机对话、多语言词法分析、句法分析和网络信息挖掘等。研究组已完成和正在承担的国家自然科学基金、863 计划、科技支撑计划、国际合作等课题 40 余项，在自然语言处理和机器翻译领域取得了多项创新性研究成果。研究组自 2004 年重点开展统计机器翻译方面的研究并取得重大突破，并于 2015 年起转向神经机器翻译并取得很大进展。\n2018 年 7 月，正式加入华为诺亚方舟实验室，任语音语义首席科学家，主导语音和自然语言处理领域的前沿研究和技术创新。在自然语言处理的顶级国际刊物 CL、AI 和顶级国际学术会议 ACL、IJCAI、AAAI、EMNLP、COLING 上发表高水平论文 70 余篇，取得发明专利 10 余项。研究组已经成功将自主开发的统计机器翻译和神经机器翻译技术推广到汉语、维吾尔语、藏语、蒙古语、英语、韩语、泰语、日语、阿拉伯语等多种语言。部分语种的翻译系统已经在相关领域得到了实际应用，获得用户的好评。\n哈工大社会计算与信息检索研究中心\n哈工大社会计算与信息检索研究中心（HIT-SCIR）成立于 2000 年 9 月，隶属于计算机科学与技术学院。研究中心成员有主任刘挺教授，副主任秦兵教授，教师包括张宇、车万翔、陈毅恒、张伟男等。研究方向包括语言分析、信息抽取、情感分析、问答系统、社会媒体处理和用户画像 6 个方面。已完成或正在承担的国家 973 课题、国家自然科学基金重点项目、国家 863 重点项目、国际合作、企业合作等课题 60 余项。在这些项目的支持下打造出“语言技术平台 LTP”，提供给百度、腾讯、华为、金山等企业使用，获 2010 年钱伟长中文信息处理科学技术一等奖。\n研究中心近年来发表论文 100 余篇，其中在 ACL、SIGIR、IJCAI、EMNLP 等顶级国际学术会议上发表 20 余篇论文，参加国内外技术评测，并在国际 CoNLL’2009 七国语言句法语义分析评测总成绩第一名。研究中心通过与企业合作，已将多项技术嵌入企业产品中，为社会服务。双语例句检索等一批技术嵌入金山词霸产品中，并因此获得 2012 年黑龙江省技术发明二等奖。\n微软亚洲研究院自然语言计算组\n复旦大学自然语言处理研究组\n复旦大学自然语言与信息检索实验室，致力于社会媒体海量多媒体信息处理的前沿技术研究。主要研究方向包括：自然语言处理、非规范化文本分析、语义计算、信息抽取、倾向性分析、文本挖掘等方面。实验室开发了 NLP 工具包 FudanNLP，FudanNLP 提供了一系列新技术，包括中文分词、词性标注、依赖解析、时间表达式识别和规范化等。实验室先后承担和参与了国家科技重大专项、国家 973 计划、863 计划、国家自然科学基金课题、上海市科技攻关计划等，并与国内外多所重点大学、公司保持着良好的合作关系。研究成果持续发表在国际权威期刊和一流国际会议（TPAMI、TKDE、ICML、ACL、AAAI、IJCAI、SIGIR、CIKM、EMNLP、COLING 等）\n清华大学自然语言处理与社会人文计算实验室\n清华大学计算机系自然语言处理课题组在 20 世纪 70 年代末，就在黄昌宁教授的带领下从事这方面的研究工作，是国内开展相关研究最早、深具影响力的科研单位，同时也是中国中文信息学会计算语言学专业委员会的挂靠单位。现任学科带头人孙茂松教授任该专业委员会的主任（同时任中国中文信息学会副理事长），其余教师还有刘洋、刘知远等人。目前该课题组对以中文为核心的自然语言处理中的若干前沿课题，进行系统、深入的研究，研究领域的涵盖面正逐步从计算语言学的核心问题扩展到社会计算和人文计算。该课题组多篇论文被 ACL 2018、IJCAI-ECAI 2018、WWW 2018 录用，内容涉及问答系统、信息检索、机器翻译、诗歌生成、查询推荐等多个领域。\n清华大学智能技术与系统国家重点实验室\n智能技术与系统国家重点实验室依托在清华大学，1987 年 7 月开始筹建。1990 年 2 月通过国家验收，并正式对外开放运行。从 1990 年至 2003 年这十三年间，实验室顺利通过国家自然科学基金委受科技部委托组织的全部三次专家组评估，并被评估为 A（优秀实验室）。1994 年 10 月在庆祝国家重点实验室建设十周年表彰大会上，智能技术与系统国家重点实验室获集体“金牛奖”。1997 年被科技部列为试点实验室。2004 年庆祝国家重点实验室建设二十周年表彰大会上，本实验室再次荣获集体“金牛奖”。从 2004 年开始，实验室参与筹建清华信息科学与技术国家实验室。实验室学术委员会由 17 名国内外著名专家组成。实验室学术委员会名誉主任为中科院院士张钹教授，主任为应明生教授、副主任为邓志东教授。\n北京大学语言计算与互联网挖掘研究组\n语言计算与互联网挖掘研究室从属于北京大学计算机科学技术研究所，成立于 2008 年7 月，负责人为万小军老师。研究室以自然语言处理技术、数据挖掘技术与机器学习技术为基础，对互联网上多源异质的文本大数据进行智能分析与深度挖掘，为互联网搜索、舆情与情报分析、写稿与对话机器人等系统提供关键技术支撑，并从事计算机科学与人文社会科学的交叉科学研究。\n研究室当前研究内容包括：1）语义理解：研制全新的语义分析系统实现对人类语言（尤其是汉语）的深层语义理解；2）机器写作：综合利用自动文摘与自然语言生成等技术让机器写出高质量的各类稿件；3）情感计算：针对多语言互联网文本实现高精度情感、立场与幽默分析；4）其他：包括特定情境下的人机对话技术等。\n北京大学计算语言学教育部重点实验室\n计算语言学教育部重点实验室依托北京大学建设。实验室研究人员由北京大学信息科学技术学院计算语言学研究所、中文系、软件与微电子学院语言信息工程系、计算机技术研究所、心理系和外语学院的相关研究人员构成。主要研究方向包括：中文计算的基础理论与模型；大规模多层次语言知识库构建的方法；国家语言资源整理与语音数据库建设；海量文本内容分析与动态监控；多语言信息处理和机器翻译。\n中科院模式识别国家重点实验室\n中科院模式识别国家重点实验室自然语言处理组主要成员有宗成庆、赵军、周玉、刘康、张家俊、汪昆、陆征等。该小组主要从事自然语言处理基础、机器翻译、信息抽取和问答系统等相关研究工作，力图在自然语言处理的理论模型和应用系统开发方面做出创新成果。目前研究组的主要方向包括：自然语言处理基础技术（汉语词语切分、句法分析、语义分析和篇章分析等）、多语言机器翻译、信息抽取（实体识别、实体关系抽取、观点挖掘等）和智能问答系统（基于知识库的问答系统、知识推理、社区问答等）。\n近年来，研究组注重于自然语言处理基础理论和应用基础的相关研究，承担了一系列包括国家自然科学基金项目、973 计划课题、863 计划项目和支撑计划项目等在内的基础研究和应用基础研究类项目，以及一批企业应用合作项目。在自然语言处理及相关领域顶级国际期刊（CL、TASLP、TKDE、JMLR、TACL、Information Sciences、Intelligent Systems 等）和学术会议（AAAI、IJCAI、ACL、SIGIR、WWW 等）上发表了一系列论文。2009 年获得第 23 届亚太语言、信息与计算国际会议（PACLIC）最佳论文奖，2012 年获得第一届自然语言处理与中文计算会议（NLPCC）最佳论文奖，2014 年获得第 25 届国际计算语言学大会（COLING）最佳论文奖。获得了 10 余项国家发明专利。\n哈工大机器智能与翻译研究室\n哈尔滨工业大学计算机学院机器智能与翻译研究室（Machine Intelligence \u0026 Translation Laboratory，MI\u0026T Lab），自1985年以来一直致力于机器翻译研究与系统开发。本研究室最初完成的汉英机器翻译系统CEMT-I于1989年5月鉴定，成为我国第一个通过技术鉴定的汉英机器翻译系统，获部级科技进步二等奖。\n2000年6月哈工大-微软机器翻译技术联合实验室的建立，为本研究室进一步发展创造了机会。2004年6月，基于哈工大-微软机器翻译技术联合实验室几年来所作出的成绩，联合实验室扩大为哈工大-微软自然语言处理及语音技术联合实验室。2004年11月联合实验室进一步提升为教育部-微软语言语音重点实验室。目前，机器智能与翻译研究室有教师6名、博士15名、硕士30余名及本科生等。学术带头人为博士生导师李生教授和赵铁军教授。\n本研究室与微软亚洲研究院、富士通研究中心、东芝研究中心、香港理工大学、美国马里兰大学、瑞典林雪平大学、英国阿伯丁大学、新加坡通讯与信息研究所以及北大、清华、中科院、北外、复旦等学校及研究机构均建立了较密切的合作与交流关系，并有机会被交换到上述大学或研究机构学习或进修。\n哈尔滨工业大学智能技术与自然语言处理实验室（ITNLP）\n哈工大智能技术与自然语言处理研究室(Intelligent Technology \u0026 Natural Language Processing Lab，ITNLP Lab)是国内较早从事自然语言处理研究的科研团体之一。自八十年代初期以来，先后开展了俄汉机器翻译、固定段落问答、自动文摘、文本纠错、汉字智能输入、语音识别与合成、语料库多级加工、语言模型、信息检索、问答系统等多项研究。\n研究室的代表性成果是开创性地提出了汉字语句输入的思想并实现了国内外第一个语句级汉字键盘输入系统。目前共获得部科技进步级一等奖1项，二等奖4项，获得国家专利3项。先后在IEEE Transaction on SMC、中国科学等国内外重要学术刊物和会议上发表论文200余篇，编著书8部。1990年以来完成的国家自然科学基金重点/面上项目、国家863重点/面上项目、中美、中日国际合作等重要科研项目20多项。\n研究室目前的主要研究方向包括：网络信息处理、自然语言处理、智能人机接口、计算分子生物学等。近期实验室牵头或独立承担的主要科研项目包括：国家自然科学基金重点项目“问答式信息检索的理论与方法研究”、国家863计划目标导向类课题“基于NLP的智能搜索引擎”、以及多项国家自然科学基金面上项目和863计划探索类项目。\n哈工大语言语音教育部-微软重点实验室\n哈工大语言语音教育部-微软重点实验室以哈工大计算机学院语言技术研究中心为主要依托，由机器智能与翻译实验室、智能技术与自然语言处理实验室、信息检索实验室和语音处理实验室联合组成，由教育部和微软亚洲研究院联合支持并资助。实验室主任是李生教授（哈工大）和马维英主任研究员（微软研究院）。研究内容涵盖自然语言理解、网络信息处理、机器翻译、信息检索、问答系统、智能汉字输入、文景转换、生物信息识别、语音识别与合成技术等多个研究领域。实验室的研究目标是在3-5年内使得以上各领域研究达到国内领先和国际先进水平。到目前为止，实验室已完成或正在实施的重要科研项目已达100多项，包括国家自然科学基金（重点）项目、863计划（重点）项目、省部级攻关项目、国际合作项目等；近3年发表论文400多篇。目前实验室研究人员包括博士生导师7人、教授8人、具有博士学位的副教授10人、博士研究生50余人、硕士研究生70余人。实验室还聘请了多位国际知名专家担任兼职博士生导师和兼职教授。\n东北大学自然语言处理实验室\n\n南京大学自然语言处理研究组\n南京大学自然语言处理研究组从事自然语言处理领域的研究工作始于20世纪80年代。曾先后承担过该领域的18项国家科技攻关项目、863项目、国家自然科学基金和江苏省自然科学基金以及多项对外合作项目的研制。其中，承担的国家七五科技攻关项目“日汉机译系统研究”获七五国家科技攻关重大成果奖、教委科技进步二等奖以及江苏省科技进步三等奖。\n分析理解人类语言是人工智能的重要问题之一。近年来，本研究组在自然语言处理的多个方向上做了大量、深入的工作，近年来集中关注文本分析、机器翻译、社交媒体分析推荐、知识问答等多个热点问题，结合统计方法和深度学习方法进行问题建模和求解，取得了丰富的成果。近期，本研究组在自然语言处理顶级国际会议ACL上连续三年发表共5篇论文，也在人工智能顶级国际会议IJCAI和AAAI上发表论文多篇，相关系统在机器翻译、中文分词、命名实体识别、情感计算等多个国际国内评测中名列前茅。\n厦门大学智能科学与技术系自然语言处理实验室\n自然语言处理实验室是厦门大学信息学院一支具有30多年研究历史的科研创新团队。团队面向信息社会中不同语言间的国内外交流的重大需求，以实现高度智能化的语言处理为目标，开展分词、命名实体识别、句法分析、多语词语对齐、语言模型、隐喻理解等基础研究，规则和统计相结合的机器翻译、辅助翻译、嵌入式翻译、新型语篇语义翻译模型、云翻译平台、神经机器翻译等机器翻译研究，知识图谱、信息抽取、关系和事件抽取、跨语言信息检索、舆情分析等网络信息处理应用基础研究。同时和外文系、中文系、两岸关系和平发展协创中心等单位有密切的交叉学科合作关系。目前团队的三个主要研究方向是多语种机器翻译、隐喻计算、信息抽取和检索。实验室现有全职研究人员9人，含教授1人，副教授4人，助理教授4人。\n\n郑州大学自然语言处理实验室\n自然语言处理是计算机应用研究领域的热点之一。生活在信息网络时代的现代人，几乎都要与互联网打交道，都要或多或少地使用自然语言处理的研究成果来帮助他们获取或挖掘在广阔无边的互联网上的各种知识和信息。\n在时代潮流发展趋势下，郑州大学于2004年10月成立了自然语言处理实验室，实验室隶属于郑州大学信息工程学院，实验室现有教授2人，副教授2人，讲师3人，在读硕士近20人。\n在昝红英教授的带领下，实验室从成立至今在现代汉语广义虚词库建设、文本自动分类、中文文本的褒贬评价、中英文双语术语抽取等方向进行了深入研究，拥有扎实的工作基础，并在该领域积累了丰富的阶段性成果。\n实验室积极营造浓厚科研氛围，在与各种团体的交流、学习中不断发展。我们曾与北京大学、香港城市大学、香港慧科讯业有限公司、日本富士通研究开发中心有限公司等多家研究机构进行项目合作与交流工作，并在进行基础研究的同时，努力将科研成果转化为生产力，与国内外企业进行横向应用项目的合作。\n苏州大学自然语言处理实验室\n苏州大学人类语言技术研究所","data":"2019年04月26日 21:36:38"}
{"_id":{"$oid":"5d343b0c62f717dc0659b305"},"title":"ODE网络：一场颠覆RNN的革命即将到来","author":"串行并行","content":"递归神经网络是当今最常见的人工智能应用程序的核心，但我们很快就发现，它们并不适合用来解决广义时间序列问题。现在已经有几个在使用中的替代解决方案，其中有一个是刚刚出现的——ODE网络，它与我们思考解决方案的方式截然不同。\n\\n\n递归神经网络及其近亲LSTM是人工智能自然语言处理应用程序的核心。与其他形式的人工智能相比，RNN-NLP在现实世界中的应用要多得多，包括使用卷积神经网络识别和处理图像。\n\\n\n从某种意义上说，数据科学家的队伍已经分成了两组，每一组都在追求使用这两种技术开发独立的应用。从应用角度来看，这两种技术基本上不会发生重叠，因为图像处理处理的是静态数据，而RNN-NLP是将语音和文本解释为时间序列数据。\n\\n\n虽然RNN/LSTM仍然是大多数NLP的首选技术，但我们越是试图扩展时间序列应用，遇到的麻烦就越多。即将出现的技术可能不只是RNN的修改版本，而是对其他几种创新人工智能方法的硬分支。\n\\n\n第一个分支：将CNN与RNN组合使用\n\\n\n第一个分支是我们去年提出的将CNN和RNN结合在一个神经网络中（详见《将CNN与RNN组合使用，天才还是错乱？》）。需要解决的问题与时间序列上的图像有关，即视频，而最常见的任务是视频场景标记。事实证明，这种技术对于识别和标记视频中的情感以及根据之前在视频中见过的人来识别某些类型的人也很有用。\n\\n\n第二个分支：时间卷积神经网络TCN\n\\n\n去年，谷歌和Facebook都解决了RNN的第二类问题。因为要分析的数据扩展到DNN中的多个层，所以，在开始计算之前必须等待所有这些层都完成。这也意味着MPP实际上并不可行。虽然这个过程仍然很快，但不足以快到可以让实时语言翻译应用程序避免明显的延迟。\n\\n\n第二个分支导致这两家公司放弃了RNN，转而采用一种他们称之为时间卷积神经网络（TCN）的CNN变体来进行实时翻译。这看起来很像添加了“Attention”功能的CNN。因为它们的结构与CNN类似，所以可以应用MPP，于是延迟就消失了。\n\\n\n第三个分支：不规则时间序列\n\\n\n还有一些其他类型的时间序列问题是RNN无法完美解决的。它们的主要是具有连续值或者希望将具有不同频率、持续时间和起始点的时间序列数据组合在一起系统。\n\\n\n最后这一个分支看起来并没有那么神秘。它描述的是这样的一种情况，在你去看不同的医生时，你会看到自己的医疗记录，你有不同的预约时间间隔，有不同剂量和时间间隔的用药情况，对这些药品等有不同的身体反应，并且你的身体在以某种可测量的方式变老、变强、变好或变坏。\n\\n\n这就是为什么人工智能的绝大多数医疗应用都只与图像识别有关。我们在使用不规则时序AI能力方面确实存在不足，无法很好地基于不规则时间序列数据得出预测结果。\n\\n\n一种解决方案是将并行的医疗记录分为几星期、几天甚至是几小时的离散步骤。理论上，这样可以满足RNN所要求的离散化。但问题是，为了获得最大的收益，你必须使用非常合适的时间桶，这样会增加计算成本和复杂性。还有一个问题，那就是很多时间桶可能不包含任何数据。\n\\n\n因此，预测社区和医疗社区都需要一个人工智能解决方案，其性能要优于目前的RNN。\n\\n\nODE网络\n\\n\n去年12月在蒙特利尔举行的神经信息处理系统（NIPS）大会上，来自加拿大向量研究所的研究人员提出了人工智能时间序列建模的全新概念，并被评为大会四篇最佳论文之一。\n\\n\n他们的系统的名字叫作“ODE网络”，是Ordinary Differential Equation Net（常微分方程网络）的缩写。但不要被误导了，ODE网络看起来一点也不像DNN，它没有节点、层或互连。这是一种使用带有反向传播的黑盒微分方程解算器的方法，在连续和离散时间序列问题上都优于RNN。换句话说，它更像是一个坚实的计算板，而不是可以被可视化为神经网络的东西。\n\\n\n这种方法带来了思维方式上的几个有趣的变化。例如，在使用RNN时，你可以指定层和其他超参数，然后运行实验，并查看所获得的准确性。\n\\n\n而在使用ODE网络时，在准确性和训练时间之间存在一个权衡。你指定了准确性级别，ODE网络将会找到实现这一目标的最佳方法，但训练时间是变化的。如果训练时间长得让人无法接受，可以指定一个较低的准确性，以便加快训练过程。一个有趣的结果可能是在训练时指定高准确性，但在测试时可以指定较低的准确性。\n\\n\n这篇论文（https://arxiv.org/abs/1806.07366）的内容非常全面，并提供了几个实验的结果，其中的结果明显优于RNN。但它仍处于研究阶段，但与数据科学中的大多数东西一样，这并不需要很长时间就能走向应用。\n\\n\n英文原文：\n\\n\nhttps://www.datasciencecentral.com/profiles/blogs/the-coming-revolution-in-recurrent-neural-nets-rnns\n\\n\n\\n","data":"2017年01月22日 16:48:01"}
{"_id":{"$oid":"5d343b0d62f717dc0659b307"},"title":"自然语言处理之动手学词向量（word embedding）-杨帅-专题视频课程","author":"AI壹号堂","content":"自然语言处理之动手学词向量（word embedding）—88人已学习\n课程介绍\n\n词向量（Word embedding）是深入学习技术在自然语言处理中应用的基础，因此掌握好词向量是学习深度学习技术在自然语言处理用应用的重要环节。\n课程收益\n本课程从One-hot编码开始，word2vec、fasttext到glove讲解词向量技术的方方面面，每个技术点环节都有相应的小案例，以增加同学们学习兴趣。同时在课程最后，以整合案例的方式给大家展示词向量技术在相似度计算中的典型应用。希望我们的课程能帮助更多的NLPper。\n讲师介绍\n杨帅更多讲师课程\n长期从事机器学习深度学习研究，在自然语言处理领域有一定认知\n课程大纲\n第1章:One hot编码\n1.课程整体介绍及大纲剖析  10:24\n2.什么是one-hot编码  5:03\n3.one-hot在提取文本特征上的应用  4:45\n4.one-hot编码手动实现  18:59\n5.ont-hot编码keras中实现  9:33\n第2章:word2vec预备基础知识及相关概念\n1.word2vec的前世今生  7:27\n2.word2vec需要注意的关键点  8:23\n3.sigmoid与softmax函数讲解  4:32\n4.二叉树相关知识讲解  6:38\n5.Huffman树讲解  11:07\n6.Huffman编码讲解  7:46\n7.语言模型讲解  11:30\n8.神经网络语言模型概念讲解  6:39\n9.神经网络语言模型数学理论部分讲解  9:08\n第3章:word2vec实现及优化方式\n1.word2vec中Skip-Gram实现方式讲解  13:30\n2.word2vec中CBOW实现方式讲解  11:15\n3.word2vec训练方式负采样讲解  13:09\n4.word2vec训练方式层序softmax讲解  12:57\n第4章:word2vec之Tensorflow实现\n1.读取停用词  10:36\n2.文本预处理上  15:51\n3.文本预处理下  12:40\n4.文本编码处理讲解  18:30\n5.批量数据生成讲解  25:01\n6.遗留问题解决讲解  7:02\n7.word2vec模型实现讲解  24:35\n8.word2vec模型训练讲解  23:37\n9.word2vec可视化展示  19:32\n第5章:word2vec之gensim工具包使用\n1.gensim中word2vec参数讲解  8:26\n2.gensim-word2vec实战之加载停用词  6:06\n3.gensim-word2vec实战之文本预处理  15:10\n4.gensim-word2vec实战之模型训练  9:46\n5.gensim-word2vec实战之模型保存与加载  7:48\n6.gensim-word2vec实战之应用讲解  13:31\n第6章:fasttext理论部分\n1.fasttext之Subword n-gram讲解  10:47\n2.fasttext之分层softmax讲解  11:55\n第7章:fasttext之文本分类实战及词向量训练\n1.fasttext实战之数据集简介及停用词加载  6:37\n2.fasttext实战之文本预处理  14:24\n3.fasttext实战之文本分类模型训练  9:33\n4.fasttext实战之模型使用讲解  12:59\n5.fasttext实战之训练词向量  9:34\n第8章:Glove理论部分\n1.什么是Glove讲解  7:12\n2.Glove如何实现讲解  9:20\n3.Glove如何训练讲解  12:30\n4.Glove数学原理讲解上  10:01\n5.Glove数学原理讲解下  7:50\n第9章:Glove实战部分\n1.Glove实战是初识Glove  13:18\n2.Glove实战之求近义词  7:07\n3.Glove实战之求类比词  8:40\n第10章:综合案例-短文本标题相似度检测及计算\n1.项目实战之项目简介及数据集介绍  5:22\n2.项目实战之GrobalParament模块编写  10:40\n3.项目实战之utils模块中读取停用词方法编写  9:57\n4.项目实战之utils模块中分词方法封装  9:12\n5.项目实战之utils模块中文本预处理方法编写  14:39\n6.项目实战之utils模块中文本预处理优化  11:41\n7.项目实战之train_model模块之word2vec训练  9:28\n8.项目实战之训练好的word2vec模型剖析  10:26\n9.项目实战之word2vec整体训练  9:59\n10.项目实战之相似度计算上  10:59\n11.项目实战之相似度计算中  18:13\n12.项目实战之相似度计算下  13:01\n13.项目实战之结果输出  18:54\n14.项目实战整体总结  10:17\n大家可以点击【查看详情】查看我的课程","data":"2018年09月17日 09:42:01"}
{"_id":{"$oid":"5d343b0d62f717dc0659b309"},"title":"基于python+opencv的图像目标区域自动提取（本项目为提取纸张中的内容）","author":"小亮 Play NLP","content":"要点：\n该教程为基于python+opencv的图像目标区域自动提取，实现自动提取一张照片中的纸张内容\n环境配置：\nWn10+CPU i7-6700\nPycharm2018\nopencv-python 3.4.2.17\nnumpy 1.14.5\n笔者信息：Next_Legend QQ:1219154092 人工智能 自然语言处理 图像处理 神经网络\n——2018.8.12于天津大学\n该项目的代码在笔者的资源仓库中，代码地址：\n基于python+opencv的图像目标区域自动提取\n一、项目背景\n一张照片中的感兴趣区域总是沿着x,y,z三个轴都有一定倾斜（如下图），要想把照片翻转到平行位置，需要进行透视变换，而透视变换需要同一像素点变换前后的坐标。由此可以想到，提取矩形区域四个角的坐标作为变换前的坐标，变换后的坐标可以设为照片的四个角落，经过投影变换，矩形区域将会翻转并充满图像。\n因此我们要解决的问题变为：提取矩形的四个角落、进行透视变换。\n二、提取矩形角落坐标\n矩形的检测主要是提取边缘，图片显示部分的亮度通常高于周围环境，我们可以将图片阈值化，将图片部分与周围环境明显的分别开来，这对后边的边缘检测非常有帮助。\n检测矩形并提取坐标需要对图像进行预处理、边缘检测、提取轮廓、检测凸包、角点检测。\n1、预处理转为灰度图\n由于手机拍摄的照片像素可能会很高，为了加快处理速度，我们首先将图像转化为灰度图\nimage = cv2.imread(Config.src) gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) srcWidth, srcHeight, channels = image.shape print(srcWidth, srcHeight)\n2、中值滤波\nbinary = cv2.medianBlur(gray,7)\n3、转化为二值图像\nret, binary = cv2.threshold(binary, Config.threshold_thresh, 255, cv2.THRESH_BINARY) cv2.imwrite(\"1-threshold.png\", binary, [int(cv2.IMWRITE_PNG_COMPRESSION), 9])\n此时图片已经变成了这个样子：\n\n可见纸张页面部分已经与背景环境分离开来。\n4、边缘检测与轮廓处理\n我们用Canny算子边缘检测，提取轮廓\n# canny提取轮廓 binary = cv2.Canny(binary, 0, 60, apertureSize = 3) cv2.imwrite(\"3-canny.png\", binary, [int(cv2.IMWRITE_PNG_COMPRESSION), 9])\n\n提取轮廓后，拟合外接多边形（矩形）\n# 提取轮廓后，拟合外接多边形（矩形） _,contours,_ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) print(\"len(contours)=%d\"%(len(contours)))\n5、提取面积最大的轮廓并用多边形将轮廓包围\nfor idx,c in enumerate(contours): if len(c) \u003c Config.min_contours: continue epsilon = Config.epsilon_start while True: approx = cv2.approxPolyDP(c,epsilon,True) print(\"idx,epsilon,len(approx),len(c)=%d,%d,%d,%d\"%(idx,epsilon,len(approx),len(c))) if (len(approx) \u003c 4): break if math.fabs(cv2.contourArea(approx)) \u003e Config.min_area: if (len(approx) \u003e 4): epsilon += Config.epsilon_step print(\"epsilon=%d, count=%d\"%(epsilon,len(approx))) continue else: #for p in approx: # cv2.circle(binary,(p[0][0],p[0][1]),8,(255,255,0),thickness=-1) approx = approx.reshape((4, 2)) # 点重排序, [top-left, top-right, bottom-right, bottom-left] src_rect = order_points(approx) cv2.drawContours(image, c, -1, (0,255,255),1) cv2.line(image, (src_rect[0][0],src_rect[0][1]),(src_rect[1][0],src_rect[1][1]),color=(100,255,100)) cv2.line(image, (src_rect[2][0],src_rect[2][1]),(src_rect[1][0],src_rect[1][1]),color=(100,255,100)) cv2.line(image, (src_rect[2][0],src_rect[2][1]),(src_rect[3][0],src_rect[3][1]),color=(100,255,100)) cv2.line(image, (src_rect[0][0],src_rect[0][1]),(src_rect[3][0],src_rect[3][1]),color=(100,255,100)) # 获取最小矩形包络 rect = cv2.minAreaRect(approx) # rect = cv2.maxAreaRect(approx) box = cv2.boxPoints(rect) box = np.int0(box) box = box.reshape(4,2) box = order_points(box) print(\"approx-\u003ebox\") print(approx) print(src_rect) print(box) w,h = point_distance(box[0],box[1]), \\ point_distance(box[1],box[2]) print(\"w,h=%d,%d\"%(w,h))\n6、 透视变换\ndst_rect = np.array([ [0, 0], [w - 1, 0], [w - 1, h - 1], [0, h - 1]], dtype=\"float32\") M = cv2.getPerspectiveTransform(src_rect, dst_rect) warped = cv2.warpPerspective(image, M, (w, h)) cv2.imwrite(\"transfer%d.png\"%idx, warped, [int(cv2.IMWRITE_PNG_COMPRESSION), 9]) break\n总结\n本项目利用了照片背景亮度较高的特点，通过二值化突出轮廓提取坐标，进行透视变换。但是局限性在于，如果矩形的亮度与背景相差不大，就很难用这种方法检测到轮廓还需要算法优化。该项目的代码在笔者的资源仓库中，代码地址：\n基于python+opencv的图像目标区域自动提取","data":"2018年08月12日 20:02:34"}
{"_id":{"$oid":"5d343b0e62f717dc0659b30b"},"title":"人工智能语言与伦理2019尔雅答案100分","author":"zt3355624460","content":"1.1\n1对人工智能常见的误解有哪些?()AD\nA、人工智能就是机器学习\nB、机器学习只是人工智能中的一个方向\nC、人工智能最近十年受到深度学习的驱动较多\nD、人工智能就是深度学习\n2哲学思维对于人工智能的重要性表现在,哲学所强调的批判性思维有助于认清人工智能发展中的问题。()对\n3深度学习在人工智能领域的表现并不突出。()X\n1.2\n1\n计算机之父是()。C\nA、约翰·麦卡锡\nB、艾伦·图灵\nC、冯▪诺依曼\nD、马文·明斯基\n2\n人工智能与计算机学科的关系是()。C\nA、计算机学科的主要驱动力是人工智能研究\nB、计算机是人工智能研究的一个领域\nC、人工智能是计算机学科的一个分支\nD、人工智能与计算机学科没有联系\n3\n人工智能作为一门学科的建立时间是()。A\nA、1956年\nB、1930年\nC、1960年\nD、1952年\n4下列哪些选项是符号AI的技术路线()?AD\nA、通用问题求解器\nB、深度学习\nC、机器学习\nD、贝叶斯网络\n5符号AI是将人的思维通过逻辑语言制成流形图让计算机去执行。()对\n6通用问题求解器需要寻找全局最优解。()X\n7符号AI无法面对人类经验的变动性。()对\n1.3\n1\n()是现在新出现的人工智能的研究方向。D\nA、深度学习\nB、人工神经元网络\nC、贝叶斯网络\nD、类脑人工智能\n2\n深度学习中的“深度”是指()。B\nA、计算机理解的深度\nB、中间神经元网络的层次很多\nC、计算机的求解更加精准\nD、计算机对问题的处理更加灵活\n3人工神经元网络与深度学习的关系是()。AC\nA、人工神经元网络是深度学习的前身\nB、深度学习是人工神经元网络的一个分支\nC、深度学习是人工神经元网络的一个发展\nD、深度学习与人工神经元网络无关\n4人工神经元网络的运作可以粗略分为()三个层面。ACD\nA、输入层\nB、映射机制\nC、中间处理层\nD、输出层\n5符号AI不是人工智能的正统。()X\n6人工神经元网络是对人类的神经元运作进行一种非常粗糙的数学模拟。()对\n7相比于人工神经元网络和深度学习,类脑人工智能对人类大脑的神经回路具有更深入的了解。()对\n1.4\n1\n深度学习的实质是()。B\nA、推理机制\nB、映射机制\nC、识别机制\nD、模拟机制\n2符号AI的问题在于()。BCD\nA、缺少推理必要的信息\nB、把推理所依赖的公理系统全部锁死\nC、缺少推理的灵活性\nD、会遭遇“框架问题”\n3推理的本质是在信息不足的情况下能够最大程度的得到最靠谱的结论。()对\n4计算机具有触类旁通的能力,可以根据具体语境对事件进行分类。()X\n5人工神经元网络会遭遇“框架问题”。()X\n1.5\n1\n日本五代计算机泡沫关注的核心问题是( )。D\nA、人工神经元网络\nB、符号AI\nC、贝叶斯网络\nD、自然语言处理\n2制造人工智能的规划、计划和方案本身应该能根据情况的变化进行自我调整。( )正确\n2.1\n1目前对人工智能的发展所持有的观点有( )。ACD\nA、乌托邦论\nB、模块论\nC、末世论\nD、泡沫论\n2现在的人工智能系统都是专用人工智能而非通用人工智能。( )正确\n2.2\n1\n一个真正的通用人工智能系统应具备处理( )问题的能力。A\nA、全局性\nB、局部性\nC、专业性\nD、统一性\n2\n目前的人工智能研发的动力主要来源于( )。B\nA、科学\nB、商业\nC、学术\nD、军事\n3现有的人工神经元网络或深度学习无法处理全局性问题。( )正确\n4人工神经元网络只需要很少的数据便可掌握处理特定问题的能力。( )错误\n2.3\n1\n能够推进人工智能智能的研究最好方法是( )。C\nA、继续完善深度学习\nB、提升计算机处理数据的能力\nC、研究人类自己的智能\nD、研发通用人工智能\n2下列哪些选项属于通用智力因素?( )ABCD\nA、短期记忆\nB、流体智力\nC、晶体智力\nD、反应速度\n3类脑人工智能是指模拟人类大脑的人工智能。( ) 错误\n4人类自己的智能体现了通用性。( )正确\n2.4\n1以下哪些选项属于自然智能?( )ABC\nA、植物\nB、动物\nC、细菌\nD、机器\n2智能的特点是( )。AC\nA、能对环境进行灵活的应对\nB、能够不断创新\nC、具有十分牢固的记忆力\nD、经济高效\n3智能与神经元网络的存在具有必然关系。( )错误\n4类脑人工智能及人工神经元网络只是智能展现的一种形式。( )正确\n2.5\n1\n提出强人工智能与弱人工智能的人是( )。A\nA、约翰·塞尔\nB、彼得卡鲁瑟斯\nC、杰瑞·佛多\nD、埃隆·马斯克\n2通用人工智能就是强人工智能。( )错误\n3.1\n1\n深度学习的数据材料来源于( )。D\nA、人工搜集\nB、已有数据库\nC、抽样调查\nD、互联网\n2\n大数据技术的样本空间是( )。C\nA、针对所有相关数据\nB、需要确立样本范围\nC、不做样本控制\nD、以上都不对\n3统计学研究首先要确立样本空间,进行合理抽样,然后估测出相关的情况。( )正确\n4当前的主流人工智能是通向真正的通用人工智能的康庄大道。( )错误","data":"2019年01月22日 12:35:30"}
{"_id":{"$oid":"5d343b0e62f717dc0659b30d"},"title":"自然语言处理笔记9-哈工大 关毅","author":"Big_quant","content":"目录\n文章目录\n目录\n前言\n课堂总结（一）\n课堂总结（二）\n课堂总结（三）\n课堂总结（四）\n前言\n硕士生涯结束，开始专心做一件自己觉得有用的工具，先做工程，后搞理论。\n自然语言处理是一个非常难的问题，同时是人工智能皇冠上的明珠。\n接下来会记录一系列自然语言处理的笔记，来自于哈工大老师关毅。\n课堂总结（一）\n问答系统总结，数据层，搜索引擎控制，信息采集，文本分类，信息索引。\n\n课堂总结（二）\n文本分类系统新的结构。国家863项目。\n处理精度，鲁棒性达到相应的目标。\n新一代学习检索机制，持续学习的能力。\n统计词法分析，外界反馈来学习，机制研究。\n实体信息抽取，电子病历信息抽取。电子健康。基于最大熵的识别系统，CRF模型。\ntransfer learning模型。句法分析，补偿学习，增量学习，主动式学习，在线学习，强化学习。\n文本聚类，自组织映射文本系统。\n做一个有人用的东西。\n课堂总结（三）\n把自己做的东西的应用率作为自己的追求目标，忠实的fans。\n领域知识的自动构建，单词变体，缩略语的研究，难度极大，非常有用的应用。\nmindmanager 推荐。\n思维导图构成，结构清晰，思路连贯。\n开始回顾：\n语言-多类文档。字处理-编码，输入输出。\n分词的难点：every great idea is simple。\n频度统计，很多工作。科学的定量方法。\n语料库的多级加工，n-grams语言模型。\n\n课堂总结（四）\nn-gram噪声信道模型，平滑。\n平滑的原则。\n隐码句法浅层句法分析问题。\n总结的话：规则+统计结合的思想。\n一般性问题和特殊性问题。语义不能这么弄。\n分个类：\n1抢占高地的研究，先做式。\n2解决问题的研究，Hownet。\n3填补空白的研究，成熟方法+新事物处理，语义信息在神经中的机制。","data":"2019年01月14日 16:25:54","date":"2019年01月14日 16:25:54"}
{"_id":{"$oid":"5d343b0f62f717dc0659b30f"},"title":"CS224n 笔记1 深度自然语言处理","author":"刀口木","content":"前言\n该系列博客是对斯坦福CS224n系列课程的学习笔记，主要用于记录课程主要知识，加深个人理解。另外此系列博客只记录每一讲中个人认为重要的内容，不包含全部内容。文章若有错误之处，请各路大神批评指正。\n\n文章目录\n前言\n什么是自然语言处理（NLP）\nNLP的层次\nNLP的应用\n什么是深度学习\n为什么发展深度学习\n为什么NLP很难\n关于Deep NLP\n总结\n什么是自然语言处理（NLP）\n自然语言处理是计算机科学、人工智能以及语言学的交叉学科，其目标是为了让计算机能够处理或者理解人类语言，完成有意义的任务，如订票、购物等。\nNLP的层次\n\nNLP任务的输入主要有语音和文本，其对应的第一级分别就是语音识别和OCR或分词。接下来是形态学，然后就是句法分析和语义理解。由于仅仅理解单词或词语的意思无法准确理解句子的含义，所以句法分析和语义理解是NLP学习中的重点。\n该课程主要关注图中画圈的两个部分，语音信号分析是NLP领域第一个大显身手的领域，但该课程的重中之重还是在第二部分。\nNLP的应用\nNLP中的应用多种多样，大概可以从难易程度进行划分如下：\n初级任务：拼写检查、关键词搜索、查找同义词等\n中级任务：阅读信息，理解文本，提取特定信息；文本分类，比如判断文章阅读难度或目标受众，推文是正面还是负面的\n高级任务：机器翻译，口语对话系统，智能问答等\n什么是深度学习\n深度学习是机器学习的一个分支，其基本思想就是让电脑自动学习，而不是手工写代码告诉电脑该做什么。\n在传统机器学习中，面对一个应用问题，人类需要具备专业的背景知识，然后人工设计特征，把设计好的特征交给机器学习算法，让机器完成权值优化的工作。在这个过程中，所谓的“机器学习”并没有让机器真正的学习，而只是帮助人类完成了最后的优化工作，反而是人类学习到更多领域知识。课程中给出的一张图很好的表示了这种关系：\n\n而深度学习作为表征学习的一部分，其学习的是原始输入的多层特征表示：\n为什么发展深度学习\n深度学习是在上世纪八九十年代提出的，但是在2010年才逐渐崛起，究其原因主要有两个方面，一方面是随着社会的发展，出现了能够发挥深度学习优势的海量数据和支撑深度学习计算的硬件能力，另一方面是深度学习不需要人工设计特征，同时在各种任务上取得了突飞猛进的成果。\n为什么NLP很难\n人类语言模棱两可，不像编程语言一样具有明确的变量，另外人类语言会省略大量信息，但是人类在接受语言时会根据自身的背景知识进行补充。\n课程中举了几个很有意思的例子，此处列出一个:\nThe Pope’s baby steps on gays\n人类对于这句话的理解通常为“教皇在同性恋问题上裹足不前”，但是对于没有背景知识的机器来说，可能将这句话理解成“教皇的孩子踩了基佬”，这就是语言的歧义性所在。\n所以说，自然语言处理是人工智能皇冠上的明珠，其困难性不言而喻。\n关于Deep NLP\nDeep NLP就是将深度学习的思想用于自然语言处理的任务中，比如机器翻译、情感分析、客服系统等，用于解决实际中存在的问题，目前深度学习在自然语言处理任务中表现很好。另外课程中还简单介绍了NLP领域常见的词向量，这在后续的课程中还会详细讲解，本篇博客不再赘述。\n总结\nCS224n第一讲主要从顶至下的介绍了NLP，算是为初学者打开一扇大门的入门课程。下一讲是对向量表示的讲解，难度相较于此篇博客深入，希望通过博客的记录，能够加深自己对NLP领域知识的掌握和理解。","data":"2018年10月29日 16:19:03"}
{"_id":{"$oid":"5d343b0f62f717dc0659b311"},"title":"人工智能，机器学习","author":"palawind","content":"人工智能研究的各个分支，包括专家系统、机器学习、进化计算、模糊逻辑、计算机视觉、自然语言处理、推荐系统\n机器学习：一种实现人工智能的方法\n机器学习最基本的做法，是使用算法来解析数据、从中学习，然后对真实世界中的事件做出决策和预测。与传统的为解决特定任务、硬编码的软件程序不同，机器学习是用大量的数据来“训练”，通过各种算法从数据中学习如何完成任务。\n举个简单的例子，当我们浏览网上商城时，经常会出现商品推荐的信息。这是商城根据你往期的购物记录和冗长的收藏清单，识别出这其中哪些是你真正感兴趣，并且愿意购买的产品。这样的决策模型，可以帮助商城为客户提供建议并鼓励产品消费。\n机器学习直接来源于早期的人工智能领域，传统的算法包括决策树、聚类、贝叶斯分类、支持向量机、EM、Adaboost等等。从学习方法上来分，机器学习算法可以分为监督学习（如分类问题）、无监督学习（如聚类问题）、半监督学习、集成学习、深度学习和强化学习。\n传统的机器学习算法在指纹识别、基于Haar的人脸检测、基于HoG特征的物体检测等领域的应用基本达到了商业化的要求或者特定场景的商业化水平，但每前进一步都异常艰难，直到深度学习算法的出现。\n深度学习：一种实现机器学习的技术\n深度学习本来并不是一种独立的学习方法，其本身也会用到有监督和无监督的学习方法来训练深度神经网络。但由于近几年该领域发展迅猛，一些特有的学习手段相继被提出（如残差网络），因此越来越多的人将其单独看作一种学习的方法。\n最初的深度学习是利用深度神经网络来解决特征表达的一种学习过程。深度神经网络本身并不是一个全新的概念，可大致理解为包含多个隐含层的神经网络结构。为了提高深层神经网络的训练效果，人们对神经元的连接方法和激活函数等方面做出相应的调整。其实有不少想法早年间也曾有过，但由于当时训练数据量不足、计算能力落后，因此最终的效果不尽如人意。\n深度学习摧枯拉朽般地实现了各种任务，使得似乎所有的机器辅助功能都变为可能。无人驾驶汽车，预防性医疗保健，甚至是更好的电影推荐，都近在眼前，或者即将实现。","data":"2019年04月05日 23:44:03"}
{"_id":{"$oid":"5d343b1062f717dc0659b313"},"title":"计算机视觉+自然语言处理=强AI？","author":"ziliwangmoe","content":"所谓强AI是指和人一样能够用一种通用算法实现不同功能的AI。\n现在任何有点常识的人一听到强AI，就会皱起眉头：那还是现在技术瓶颈以外的能力。但我一直在思考这个通向强AI的那把金钥匙究竟在哪里？最近突然觉得也许强AI需要的技术已经成熟，只是我们不知道怎么把现有的技术结合在一起。\n计算机视觉是指通过处理视频信息，提取出摄像头周围的各种场景信息，比如什么位置有一个什么物体，有多大，我们把这些场景信息叫做地图，这个过程叫建图。因为利用这些场景信息，可以反过来计算出摄像头在场景中的位置，这个过程叫做定位。并且对于场景中的物体，还会识别他们大概是什么东西。其实这正是是人的视觉系统负责的任务：当我来到一个新的环境中时，我们环顾四周，然后来回走动一下，大概能知道周围有些什么物体，有多大，有多远，有些什么属性。然后我们就可以自如的在新环境中穿梭而不会到处碰撞。\n目前计算机视觉的水平大概是：对于地图和定位的精度可以达到人的水平，并且可以分割出不同的物体。然后再结合现有识别算法，可以知道每个物体的名字。如果我们还有一套名字和属性的数据库，我们就能知道每个物体大概有些什么特点和功能。但这里的瓶颈是如果想要万能的识别任何物体，需要极其大量的人工物体标注和训练，且不说现在的算法能否支持如此大量的数据。计算机视觉方面的强AI瓶颈正式在这里。\n那么这里出现了两个问题：\n1，强AI是否真的需要万能的识别世界上所有的物体。\n我们认为即使是才出生的婴儿的智力也是能够达到强AI的要求。但是婴儿能够识别的物体非常有限，基本就是屋里那几样东西。所以当我们把使用机器人的场景限定一下，比如只在室内，只是可移动的物体。这样需要标注和训练的量就打打减少了。其实一个成年人能够识别出无以计数的物体，是通过几十年的标注和训练过程才达到的。\n2，能否找到一种激励机制，让人自发的去标注物体并训练AI。\n有这样一个数据，全人类花在玩魔兽世界的时间累计起来达到了593万年，对于人类来说593万年前人类的祖先刚刚学会直立行走。所以劳动力这个资源，只要有合适的激励，几乎是无限的。其实我们把全人类用在教婴儿理解世界的时间加起来应该远远大于万魔兽世界的时间。\n再来说下自然语言处理。自然语言处理是把人类使用的语言翻译成计算机使用的语言。比如使用自然语言处理分析一段文字后，可以提取出这段话涉及到多少个物件，他们的关系是什么等等信息。这些信息可以使用计算机擅长的方式存储和使用。\n目前自然语言处理已经能够翻译几乎所有逻辑关系的文字。但其瓶颈在于如果要把很所有从文字中提取出来的物体和现实中的物体对应起来需要大量的标注和训练，并且算法不一定支持。\n同样我也提出两个问题：\n1，对于抽象的非物体的词语怎么让机器去理解。\n抽象的非物体的词是不能被简单的标记的，比如愤怒，失望，成就等等。这些词是建立在人类大量的具体词和价值观的基础上形成的。关于价值观的AI解释又会是一大篇文章。这里只是探究最简单的强AI，所以就不展开说AI中的价值观了。我的观点是：即使是不需要这些抽象词，也能实现强AI。比如我们可以教会婴儿从一堆物体里面拿出我们要求的东西，而不需要借助任何抽象的表达。\n2，能否找到一种激励机制，让人自发的去建立现实中的物体和自然语言处理得到的物体的关系\n这个问题的答案和上面的第二个问题一样。下面我用场景描述的方法来说明。\n假如我们有一个机器人，这个机器人具备这样几个功能：\n1，视觉：建图，定位和分割物体\n2，人的手势识别：可以判断人的手所指的方向\n3，自然语言处理：能够分析最简单的逻辑，比如这是什么，那是什么，把什么东西拿到哪里去。\n我们把这个机器人放到一个陌生的房间里面，就好像第一次把小婴儿带回家。我们让机器人自己在屋里到处闲逛，慢慢的它就能知道什么地方有几个物体，并且随时知道自己在房间中的位置。然后我们指着一个板凳对着机器人说：这是小板凳。通过手势识别和自然语言处理，我们指向的这个物体被标记为小板凳并被训练了一次。然后我们指着旁边一个大一点的凳子说：这是大板凳，然后第二个物体被标注为大板凳了。我们还可以用多种方式来训练机器人：我们可以说到小板凳旁边去。通过自然语言处理识别出“去”这个次的含义，并且去这个动作已经是预先写入到机器人的程序中，就像人类的某些行为并不是后天学习的，而是被预先写在DNA里面一样。如果之前学习小板凳成功了，机器人就能自己跑到小板凳旁边。反之我们给一个失望的手势，机器人识别出来后，又可以进行一次标记和学习。\n通过这样的方式我们可以教会所有房子里机器人需要了解的物体的标记以及位置。机器人不需要了解更多的物体，除非我们需要他完成新的功能。其实整个过程和我们教小婴儿的方式一模一样，而实现这一切需要的技术我们现在都实现了。\n如果我们给机器人装上一个可以拾取物体的设备，比如一个钳子或者吸盘。然后在机器人的预程序中写入拿过来这个表达对应的行为。那么我们还可以对着机器人说把小板凳拿过来。于是机器人就能移动到小板凳旁，再把小板凳拿过来。同理我们可以教会机器人拿任何房子里的东西。\n同理，还有很多事情可以教会机器人。比如把黄色的鞋子放到门边，把脏衣服扔到桶里。之后当我们回家随手把鞋子一脱，衣服一扔，机器人都能自动帮我们收拾好。\n慢慢的这个机器人就像是自己的小婴儿一样慢慢成长，难道我们不愿意花费一些时间在教育这个因为自己而独一无二的“小婴儿”吗？这正是我说的那种训练强AI的激励机制。\n最后的结论是，也许强AI会在家庭小型机器人的应用中最先实现。","data":"2018年08月05日 17:28:27"}
{"_id":{"$oid":"5d343b1062f717dc0659b315"},"title":"自然语言处理之神经网络基础","author":"herosunly","content":"1 前馈神经网络\n前馈神经网络也称为是深度前馈网络或者多层感知机，它是最基础的深度学习模型。\n1.1 基本概念\n前馈神经网络的目标是在函数空间中寻找相对正确的函数。函数空间是由所选的architecture决定的，而函数空间中的具体函数是由parameters决定的。\n前馈神经网络之所以称作前馈的，是因为信息从输入\nx\n⃗\n\\vec{x}\nx\n到输出\ny\ny\ny是单向流动的，并没有从输出到模型本身的反馈连接。\n前馈神经网络通常使用许多不同的函数复合而成，这些函数如何复合则由一个有向无环图来描述。最简单的情况：有向无环图是链式结构。\n假设有三个函数\nf\n1\n,\nf\n2\n,\nf\n3\nf_1,f_2,f_3\nf1 ,f2 ,f3 组成链式复合结构，则：\nf\n(\nx\n⃗\n)\n=\nf\n3\n(\nf\n2\n(\nf\n1\n(\nx\n⃗\n)\n)\n)\nf(\\vec{x})=f_3(f_2(f_1(\\vec{x})))\nf(x\n)=f3 (f2 (f1 (x\n))) 。其中：\nf\n1\nf_1\nf1 被称作网络的第一层，\nf\n2\nf_2\nf2 为网络第二层，\nf\n3\nf_3\nf3 称为网络第三层。链的全长称作模型的深度或者神经网络的层数。\n\n深度前馈网络的最后一层也称作输出层。\n给定训练样本\n(\nx\n⃗\n,\ny\n)\n(\\vec{x},y)\n(x\n,y)，要求输出层的输出\nf\n(\nx\n⃗\n)\n≈\ny\nf(\\vec{x}) \\approx y\nf(x\n)≈y ，但是对于其他层并没有任何要求。\n因为无法观测到除了输出层以外的那些层的输出，因此那些层被称作隐藏层(hidden layer) 。\n学习算法必须学习如何利用隐层来配合输出层来产生想要的结果。\n通常每个隐层的输出都是一个向量而不是标量，这些隐层的输出向量的维数决定了前馈神经网络的宽度。\n也可以将每一层想象成由许多并行的单元组成，每个单元表示一个向量到标量的函数：每个单元的输入来自于前一层的许多单元，单元根据自己的激活函数来计算单元的输出。激活函数是前馈神经网络具有非线性表达能力的核心因素。因此每个单元类似于一个神经元。\n\n1.2 特征学习\n线性模型简单高效，且易于求解。但是它有个明显的缺陷：模型的能力被局限在线性函数中，因此它无法理解任意两个输入变量间的非线性相互作用 。\n解决线性模型缺陷的方法是：采用核技巧，将线性模型作用在\nϕ\n(\nx\n⃗\n)\n\\phi(\\vec{x})\nϕ(x\n)上，而不是原始输入\nx\n⃗\n\\vec{x}\nx\n上。其中\nϕ\n\\phi\nϕ是一个非线性变换。\n可以认为：通过\nϕ\n\\phi\nϕ，提供了\nx\n⃗\n\\vec{x}\nx\n的一个新的representation。\n有三种策略来选择这样的非线性变换 。\n使用一个通用的\nϕ\n\\phi\nϕ，如无限维的\nϕ\n\\phi\nϕ（采用基于 RBF核的核技巧）。\n当\nϕ\n\\phi\nϕ具有足够高的维数，则总是有足够的能力来适应训练集，但是对于测试集的泛化往往不佳。这是因为：通用的\nϕ\n\\phi\nϕ通常只是基于局部平滑的原则，并没有利用足够多的先验知识来解决高级问题。\n手动设计\nϕ\n\\phi\nϕ 。\n这种方法对于专门的任务往往需要数十年的努力（如语音识别任务）。\n通过模型自动学习\nϕ\n\\phi\nϕ。\n这是深度学习采用的策略。以单层隐层的深度前馈网络为例：\ny\n=\nf\n(\nx\n⃗\n,\nθ\n⃗\n,\nw\n⃗\n)\n=\nϕ\n(\nx\n⃗\n;\nθ\n⃗\n)\nT\nw\n⃗\ny=f(\\vec{x},\\vec{\\theta},\\vec{w})=\\phi(\\vec{x};\\vec{\\theta})^T\\vec{w}\ny=f(x\n,θ\n,w\n)=ϕ(x\n;θ\n)Tw\n。此时有两个参数：\n参数\nθ\n⃗\n\\vec{\\theta}\nθ\n：从一族函数中学习\nϕ\n\\phi\nϕ，其中\nϕ\n\\phi\nϕ定义了一个隐层。\n参数\nw\n⃗\n\\vec{w}\nw\n：将\nϕ\n(\nx\n⃗\n)\n\\phi(\\vec{x})\nϕ(x\n)映射到所需输出。\n深度学习中，将representation参数化为\nϕ\n(\nx\n⃗\n,\nθ\n⃗\n)\n\\phi(\\vec{x},\\vec{\\theta})\nϕ(x\n,θ\n)，并使用优化算法来寻找\nθ\n⃗\n\\vec{\\theta}\nθ\n从而得到一个很好的 representation。\n如果使用一个非常宽泛的函数族\nϕ\n(\nx\n⃗\n,\nθ\n⃗\n)\n\\phi(\\vec{x},\\vec{\\theta})\nϕ(x\n,θ\n)，则能获得第一种方案的好处：适应能力强。\n如果将先验知识编码到函数族\nϕ\n(\nx\n⃗\n,\nθ\n⃗\n)\n\\phi(\\vec{x},\\vec{\\theta})\nϕ(x\n,θ\n)中，则能获得第二种方案的好处：有人工先验知识。\n因此深度学习的方案中，只需要寻找合适的、宽泛的函数族 ，而不是某一个映射函数 。\n通过特征学习来改善模型不仅仅适用于前馈神经网络，也适用于几乎所有的深度学习模型。\n1.3 训练\n训练一个深度前馈网络和训练一个线性模型的选项相同：选择优化算法、代价函数、输出单元的形式。\n除此之外还需要给出下列条件：\n由于深度前馈网络引入了隐层的概念，因此需要选择适用于隐层的激活函数。激活函数接受隐层的输入值，给出了隐层的输出值。\n深度前馈网络的网络结构也需要给出，其中包括：有多少层网络、每层网络有多少个单元、层级网络之间如何连接。\n深度神经网络训练时需要计算复杂函数的梯度，通常这采用反向传播算法(back propagation)和它的现代推广来完成。\n2. 使用pytorch定义简单神经网络\n假设输入样本为64个，输入层维度为1000，只包括一层隐藏层，隐藏层维度为100，输出层维度为10个。\n使用链式法则求导的代码如下所示：\nnum_samples = 64 # N dim_in, dim_hid, dim_out = 1000, 100, 10 # IN H OUT x = torch.randn(num_samples, dim_in) # N * IN y = torch.randn(num_samples, dim_out) # N * OUT w1 = torch.randn(dim_in, dim_hid) # IN * H w2 = torch.randn(dim_hid, dim_out) # H * OUT eta = 1e-6 for i in range(1000): #Forward pass h = x @ w1 # N * H h_relu = h.clamp(min = 0) # N * H y_pred = h_relu @ w2 # N * OUT #Loss loss = (y_pred - y).pow(2).sum().item() print('times is {}, loss is {}'.format(i, loss)) #Backward pass grad_y_pred = 2.0 * (y_pred - y) # N * OUT grad_w2 = (h_relu.t()) @ (grad_y_pred) #H * OUT = (H * N) * (N * OUT)，其中(H * N) = (N * H).T grad_h_relu = grad_y_pred @ ((w2.t()))# N * H = (N * OUT) * (OUT * H)，其中(OUT * H) = (H * OUT).T grad_h = grad_h_relu.clone() grad_h[h \u003c 0] = 0 grad_w1 = (x.t()) @ (grad_h) # IN * H = （IN * N） * (N * H) w1 = w1 - eta * grad_w1 w2 = w2 - eta * grad_w2\n使用pytorch自动求导代码如下所示：\nnum_samples = 64 # N dim_in, dim_hid, dim_out = 1000, 100, 10 # IN H OUT x = torch.randn(num_samples, dim_in) # N * IN y = torch.randn(num_samples, dim_out) # N * OUT w1 = torch.randn(dim_in, dim_hid, requires_grad=True) # IN * H w2 = torch.randn(dim_hid, dim_out, requires_grad=True) # H * OUT eta = 1e-6 for i in range(1000): #Forward pass h = x @ w1 # N * H h_relu = h.clamp(min = 0) # N * H y_pred = h_relu @ w2 # N * OUT #Loss loss = (y_pred - y).pow(2).sum() print('times is {}, loss is {}'.format(i, loss.item())) loss.backward() #Backward pass with torch.no_grad(): w1 -= eta * w1.grad #如果写成w1 = w1 - eta * w1.grad就会报错 w2 -= eta * w2.grad w1.grad.zero_() w2.grad.zero_()\n3. 激活函数\n激活函数的设计是一个非常活跃的研究领域，并且目前还没有明确的指导性理论，难以决定何时采用何种类型的激活函数是最佳方案。\n通常不能预先判断哪种类型的激活函数工作的最好，所以设计过程中需要反复试错，通过测试集评估其性能来选择合适的激活函数。\n一般默认采用的激活函数是修正线性单元(relu)，但是仍然有许多其他类型的激活函数。\n某些激活函数可能并不是在所有的输入上都是可微的。如：修正线性单元\ng\n(\nz\n)\n=\nmax\n⁡\n{\n0\n,\nz\n}\ng(z)=\\max{\\{0,z\\}}\ng(z)=max{0,z}在\nz\n=\n0\nz=0\nz=0处不可微，这使得在该点处梯度失效。\n事实上梯度下降法对这些隐单元的表现仍然足够好，原因是：\n神经网络的训练算法通常并不会达到代价函数的局部最小值，而仅仅是显著地降低代价函数的值即可。因此实际上训练过程中一般无法到达代价函数梯度为零的点，所以代价函数取最小值时梯度未定义是可以接受的。\n不可微的点通常只是在有限的、少数的点上不可微，在这些不可微的点通常左导数、右导数都存在。\n神经网络训练的软件实现通常返回左导数或者右导数其中的一个，而不是报告导数未定义或者产生一个错误。这是因为计算机计算 0 点的导数时，由于数值误差的影响实际上不可能计算到理论上 0 点的导数，而是一个微小的偏离：向左侧偏离就是左导数，向右侧偏离就是右导数。\n大多数激活函数的工作过程都可以描述为下面三步：\n接受输入向量\nx\n⃗\n\\vec{x}\nx\n。\n计算仿射变换\nz\n=\nw\n⃗\nT\nx\n⃗\n+\nb\nz =\\vec{w}^{T}\\vec{x}+b\nz=w\nTx\n+b。\n激活函数也称作是隐单元。\n3.1 线性激活函数\n可以使用单位函数\ng\n(\nz\n)\n=\nz\ng(z)=z\ng(z)=z作为激活函数 。但如果网络的每一层都是由线性变换组成，则网络作为整体也是线性的。这会降低网络的表达能力，因此线性激活函数较少使用。\n3.2 修正线性单元（relu）\n修正线性单元采用激活函数\ng\n(\nz\n)\n=\nm\na\nx\n{\n0\n,\nz\n}\ng(z)=max\\{0, z\\}\ng(z)=max{0,z}，它和线性单元非常类似，区别在于：修正线性单元在左侧的定义域上输出为零。\n优点：采用基于梯度的优化算法时，非常易于优化。当修正线性单元处于激活状态时，导数为常数1 ；当修正线性单元处于非激活状态时，导数为常数0 。修正线性单元的二阶导数几乎处处为零。\n缺点：无法通过基于梯度的方法学习那些使得修正线性单元处于非激活状态的参数，因为此时梯度为零。\n对于修正线性单元\nh\n⃗\n=\ng\n(\nW\nT\nx\n⃗\n+\nb\n⃗\n)\n\\vec{h}=g(W^{T}\\vec{x}+\\vec{b})\nh\n=g(WTx\n+b\n)，初始化时可以将\nb\n⃗\n\\vec{b}\nb\n的所有元素设置成一个小的正值（如0.1），从而使得修正线性单元在初始时尽可能的对训练集中大多数输入呈现激活状态。\n有许多修正线性单元的扩展存在，这些扩展保证了它们能在各个位置都保持非零的梯度。大多数扩展的表现与修正线性单元相差无几，偶尔表现的更好。\n3.3 sigmoid / tanh\n在引入修正线性单元之前，大多数神经网络使用sigmoid函数\ng\n(\nz\n)\n=\nσ\n(\nz\n)\ng(z)=\\sigma(z)\ng(z)=σ(z)，或者双曲正切函数\ng\n(\nz\n)\n=\nt\na\nn\nh\n(\nz\n)\ng(z)=tanh(z)\ng(z)=tanh(z)作为激活函数。这两个激活函数密切相关，因为\nt\na\nn\nh\n(\nz\n)\n=\n2\nσ\n(\n2\nz\n)\n−\n1\ntanh(z)=2\\sigma(2z)-1\ntanh(z)=2σ(2z)−1 。\n与修正线性单元不同，sigmoid单元和tanh单元在其大部分定义域内都饱和，仅仅当\nz\nz\nz在 0 附近才有一个较高的梯度，这会使得基于梯度的学习变得非常困难。因此，现在不鼓励将这两种单元用作前馈神经网络中的激活函数。\n如果选择了一个合适的代价函数（如对数似然函数）来抵消了sigmoid的饱和性，则这两种单元可以用作输出单元（而不是隐单元）。\n如果必须选用sigmoid激活函数时，tanh激活函数通常表现更佳。因为tanh函数在 0点附近近似于单位函数\ng\n(\nz\n)\n=\nz\ng(z)=z\ng(z)=z。\n\nsigmoid激活函数在前馈神经网络之外的神经网络中更为常见。\n有一些网络不能使用修正线性单元，因此sigmoid激活函数是个更好的选择，尽管它存在饱和问题。\n循环神经网络：修正线性单元会产生信息爆炸的问题。\n一些概率模型：要求输出在 0~1 之间。\n3.3 激活函数对比\nsigmoid主要缺点：\n容易饱和从而使得梯度消失。当激活函数取值在接近0或者1时会饱和，此时梯度为近似为0。\n函数输出不是零中心的。这会导致后续神经元的输出数值总是正数。\ntanh ：\n优点：函数输出是零中心的。\n缺点：容易饱和从而使得梯度消失。\ntanh 激活函数几乎在所有场合都是优于sigmoid 激活函数的。但是有一种情况例外：如果要求函数输出是0~1 之间（比如表征某个概率），则二者之间必须用sigmoid。\nrelu：\n优点：对随机梯度下降的收敛有巨大的加速作用，而且非常容易计算。\n缺点：可能导致神经元死掉。\n当一个很大的梯度流过 relu 神经元时，可能导致梯度更新到一种特别的状态：在这种状态下神经元无法被其他任何数据点再次激活。此后流过这个神经元的梯度将变成 0，该单元在训练过程中不可逆的死亡。\n如果学习率设置的过高，可能会发现网络中大量神经元都会死掉。整个训练过程中，这些神经元都不会被激活。\nleaky relu ：为了解决 relu 死亡神经元的问题的尝试，但是效果并不明显。\n4. 正则化\n正则化常用于缓解模型过拟合。过拟合发生的原因是模型的容量过大，而正则化可以对模型施加某些限制，从而降低模型的有效容量。\n目前有多种正则化策略。\n有些正则化策略是向模型添加额外的约束，如增加对参数的限制。这是对参数的硬约束。\n有些正则化策略是向目标函数增加额外项。这是对参数的软约束。\n正则化策略代表了某种先验知识，即：倾向于选择简单的模型。\n在深度学习中，大多数正则化策略都是基于对参数进行正则化。正则化以偏差的增加来换取方差的减少，而一个有效的正则化能显著降低方差，并且不会过度增加偏差。\n在深度学习的实际应用中，不要因为害怕过拟合而采用一个小模型，推荐采用一个大模型并使用正则化。\n4.1 参数范数正则化\n一些正则化方法通过对目标函数\nJ\nJ\nJ添加一个参数范数正则化项\nΩ\n(\nθ\n⃗\n)\n\\Omega(\\vec{\\theta})\nΩ(θ\n)来限制模型的容量capacity 。\n正则化之后的目标函数为\nJ\n⃗\n(\nθ\n⃗\n;\nX\n,\ny\n⃗\n)\n=\nJ\n⃗\n(\nθ\n⃗\n;\nX\n,\ny\n⃗\n)\n+\nα\nΩ\n(\nθ\n⃗\n)\n\\vec{J}(\\vec{\\theta};X,\\vec{y})=\\vec{J}(\\vec{\\theta};X,\\vec{y}) +\\alpha\\Omega(\\vec{\\theta})\nJ\n(θ\n;X,y\n)=J\n(θ\n;X,y\n)+αΩ(θ\n)。\nα\n\\alpha\nα为正则化项的系数，它衡量正则化项\nΩ\n(\nθ\n⃗\n)\n\\Omega(\\vec{\\theta})\nΩ(θ\n)和标准目标函数\nJ\n⃗\n\\vec{J}\nJ\n的比重。\nα\n=\n0\n\\alpha=0\nα=0则没有正则化。\nα\n\\alpha\nα越大则正则化项越重要。如果最小化\nJ\n⃗\n\\vec{J}\nJ\n，则会同时降低J和参数\nθ\n⃗\n\\vec{\\theta}\nθ\n的规模。\n参数范数正则化可以缓解过拟合。\n如果\nα\n\\alpha\nα设置的足够大，则参数\nθ\n⃗\n\\vec{\\theta}\nθ\n就越接近零。这意味着模型变得更简单，简单的模型不容易过拟合（但是可能欠拟合）。\n对于神经网络，这意味着很多隐单元的权重接近0，于是这些隐单元在网络中不起任何作用。此时大的神经网络会变成一个小的网络。\n在\nα\n\\alpha\nα从零逐渐增加的过程中存在一个中间值，使得参数\nθ\n⃗\n\\vec{\\theta}\nθ\n的大小合适，即一个合适的模型。\n选择不同的\nΩ\n\\Omega\nΩ的形式会产生不同的解，常见的形式有\nL\n2\nL_2\nL2 正则化和\nL\n1\nL_1\nL1 正则化。\n4.1.1 L2正则化\nL\n2\nL_2\nL2 正则化通常被称作岭回归或者Tikhonov正则化。\n正则化项为\nΩ\n(\nθ\n⃗\n)\n=\n1\n2\n∣\n∣\nθ\n∣\n∣\n2\n\\Omega(\\vec{\\theta})=\\frac{1}{2}||\\theta||^{2}\nΩ(θ\n)=21 ∣∣θ∣∣2。系数\n1\n2\n\\frac{1}{2}\n21 是为了使得导数的系数为 1。\n该正则化形式倾向于使得参数\nθ\n⃗\n\\vec{\\theta}\nθ\n更接近零。\n假设\nθ\n⃗\n\\vec{\\theta}\nθ\n参数就是权重\nw\n⃗\n\\vec{w}\nw\n，没有偏置参数，则\nJ\n⃗\n(\nθ\n⃗\n;\nX\n,\ny\n⃗\n)\n=\nJ\n⃗\n(\nθ\n⃗\n;\nX\n,\ny\n⃗\n)\n+\nα\n2\nw\n⃗\nT\nw\n⃗\n\\vec{J}(\\vec{\\theta};X,\\vec{y})=\\vec{J}(\\vec{\\theta};X,\\vec{y}) +\\frac{\\alpha}{2}\\vec{w}^{T}\\vec{w}\nJ\n(θ\n;X,y\n)=J\n(θ\n;X,y\n)+2α w\nTw\n，对应的梯度为\n∇\nw\n⃗\nJ\nˇ\n(\nw\n⃗\n;\nX\n,\ny\n⃗\n)\n=\n∇\nw\n⃗\nJ\nˇ\n(\nw\n⃗\n;\nX\n,\ny\n⃗\n)\n+\nα\nw\n⃗\n\\nabla_{\\vec{w}} \\check { J }(\\vec{w};X,\\vec{y}) = \\nabla_{\\vec{w}} \\check { J }(\\vec{w};X,\\vec{y})+\\alpha \\vec{w}\n∇w\nJˇ(w\n;X,y\n)=∇w\nJˇ(w\n;X,y\n)+αw\n。\n正则化对于梯度更新的影响是：每一步执行梯度更新之前，会对权重向量乘以一个常数因子来收缩权重向量。因此L2 正则化也被称作“权重衰减”。\nL\n2\nL_2\nL2 正则化表明：\n只有显著减小目标函数\nJ\nJ\nJ的那个方向的参数会相对保留下来。\n无助于减小目标函数\nJ\nJ\nJ的方向（该方向上\nH\nH\nH特征值较小，或者说该方向上\nJ\nJ\nJ的曲率较小，或者说该方向上\nJ\nJ\nJ的曲线更接近于直线），因为在这个方向上移动不会显著改变梯度，因此这个不重要方向上的分量会因为正则化的引入而被衰减掉。\n4.1.2 L1正则化\n模型参数\nw\n⃗\n\\vec{w}\nw\n的\nL\n1\nL_1\nL1 的正则化形式为：\nΩ\n(\nθ\n⃗\n)\n=\n∣\n∣\nw\n⃗\n∣\n1\n=\n∑\ni\n∣\nw\ni\n∣\n\\Omega(\\vec{\\theta})=||\\vec{w}|_1=\\sum \\limits_{i} |w_i|\nΩ(θ\n)=∣∣w\n∣1 =i∑ ∣wi ∣ 。即各个参数的绝对值之和。\nL\n1\nL_1\nL1 正则化后的目标函数：\nJ\n⃗\n(\nθ\n⃗\n;\nX\n,\ny\n⃗\n)\n=\nJ\n⃗\n(\nθ\n⃗\n;\nX\n,\ny\n⃗\n)\n+\nα\n∣\n∣\n∣\nw\n∣\n∣\n1\n\\vec{J}(\\vec{\\theta};X,\\vec{y})=\\vec{J}(\\vec{\\theta};X,\\vec{y}) +\\alpha|||w||_{1}\nJ\n(θ\n;X,y\n)=J\n(θ\n;X,y\n)+α∣∣∣w∣∣1 。\n对应的梯度为\n∇\nw\n⃗\nJ\nˇ\n(\nw\n⃗\n;\nX\n,\ny\n⃗\n)\n=\n∇\nw\n⃗\nJ\nˇ\n(\nw\n⃗\n;\nX\n,\ny\n⃗\n)\n+\nα\ns\ni\ng\nn\n(\nw\n⃗\n)\n\\nabla_{\\vec{w}} \\check { J }(\\vec{w};X,\\vec{y}) = \\nabla_{\\vec{w}} \\check { J }(\\vec{w};X,\\vec{y})+\\alpha sign( \\vec{w})\n∇w\nJˇ(w\n;X,y\n)=∇w\nJˇ(w\n;X,y\n)+αsign(w\n)。其中\ns\ni\ng\nn\n(\n⋅\n)\nsign(\\cdot)\nsign(⋅)函数取自变量的符号：\n如果自变量大于零，则取值为 1；如果自变量小于零，则取值为 -1；如果自变量为零，则取值为零。\nL\n1\nL_1\nL1 正则化对于梯度更新的影响是：不再是线性地缩放每个\nw\ni\nw_i\nwi （\nL\n2\nL_2\nL2 正则化项的效果），而是减去与\ns\ni\ng\nn\n(\nw\ni\n)\nsign(w_i)\nsign(wi )同号的常数因子。\nL\n1\nL_1\nL1 正则化项更容易产生稀疏(sparse)解，而 正则化并不会导致稀疏解。\n在\nL\n1\nL_1\nL1 正则化中，\nw\ni\n∗\nw_i^*\nwi∗ 的绝对值越小，该维的特征越容易被稀疏化。\nL\n1\nL_1\nL1 正则化的这一性质已经被广泛地用作特征选择：\nL\n1\nL_1\nL1 正则化使得部分特征子集的权重为零，表明相应的特征可以被安全地忽略。\n4.2 数据集增强\n提高模型泛化能力的一个最直接的方法是采用更多的数据来训练。但是通常在现实任务中，我们拥有的数据量有限。解决该问题的一种方法是：创建一些虚拟的数据用于训练。\n数据集增强仅仅用于模型的训练，而不是用于模型的预测。即：不能对测试集、验证集执行数据集增强。\n当比较机器学习算法基准测试的结果时，必须考虑是否采用了数据集增强。通常情况下，人工设计的数据集增强方案可以大大减少模型的泛化误差。当两个模型的泛化性能比较时，应该确保这两个模型使用同一套人工设计的数据集增强方案。\n注意数据集增强和预处理的区别：数据集增强会产生更多的输入数据，而数据预处理产生的输入数据数量不变。\n4.2.1 线性变换\n对于某些任务来说，创建虚拟数据非常困难。如：在密度估计任务中，除非预先知道了密度函数，否则无法产生新的虚拟数据。\n对于分类问题来说，创建虚拟数据非常简单。对于一个分类器，它将高维的输入\nx\n⃗\n\\vec{x}\nx\n映射到类别\ny\ny\ny。这意味着这种映射规则是不随坐标系的改变而改变的。因此可以通过线性变换，将训练集中的\n(\nx\n⃗\n,\ny\n)\n(\\vec{x}, y)\n(x\n,y)变换为\n(\nx\n⃗\n′\n,\ny\n)\n(\\vec{x}^ { \\prime }, y)\n(x\n′,y)从而产生了新的数据\n(\nx\n⃗\n′\n,\ny\n)\n(\\vec{x}^ { \\prime }, y)\n(x\n′,y) 。对图像分类问题，数据集增强特别有效。数据集增强也可以应用于语音识别任务。\n常见的图片数据集增强方法：\n将训练图像沿着每个方向平移几个像素产生新的图像。\n对训练图像进行旋转、翻转或者缩放。\n对训练图像进行随机裁剪。实际上，随机裁剪图像的操作也可以被认为是预处理步骤，而不是数据集增强。\n对训练图像进行颜色抖动：调整饱和度、调整亮度、调整对比度、调整锐度。\n对比度：图像画面的明暗反差程度。对比度越高，则图片亮的地方更亮，暗的地方越暗。\n亮度：图像的明暗程度。亮度越高，则图像整体越亮。\n饱和度：图像颜色种类的多少。饱和度越高，则图像的颜色种类越多，图像越鲜艳。\n锐度：图像的边缘轮廓的锐利程度。锐度越高，则图像的边缘越清晰。\n在使用线性变换执行数据集增强时需要注意：\n某些线性变换会改变正确的类别。如：字符识别任务中， b/d以及6/9的图像， 不能执行水平翻转变换和旋转 180 度变换。\n某些线性变换难以执行。如：平面外的绕轴旋转（类似于翻页）难以通过简单的几何运算在输入图片上实现。\n4.3 噪声添加\n有两种添加噪声的策略：输入噪声注入、权重噪声注。\n输入噪声注入是将噪声作用于输入的数据集，这也是一种数据集增强方法。对于某些模型，在输入上注入方差极小的噪音等价于对权重施加参数范数正则化（Bishop,1995a,b）。但是输入噪声注入远比简单地收缩参数强大，尤其是噪声被添加到隐单元的输入上时。\n权重噪声注入是将噪音作用于权重。这项技术主要用于循环神经网络。权重噪声注入可以解释为：将权重视作不确定的随机变量（拥有某个概率分布），向权重注入噪声是对该随机变量采样得到的一个随机值。\n4.4 早停\n当训练一个容量较大的模型时会经常发现：训练误差逐渐降低，但是验证误差先下降后上升。当验证误差没有进一步改善时，算法就提前终止。这种策略被称作早停early stopping。\n\n早停是深度学习中最常用的正则化形式，因为它简单、有效。\n当训练终止时，返回的不是最新的模型参数，而是验证误差最小的模型参数，因此需要频繁存储模型参数。\n4.4.1 早停算法\n早停算法：\n输入：当前验证集的误差非最小值的次数、验证集验证的间隔 、初始参数。\n输出：最佳参数、获得最佳参数时迭代的步数。\n算法步骤：先进行初始化。然后迭代直至满足条件停止。\n可以认为早停是一个非常高效的超参数选择算法：训练步数是一个超参数，该超参数在验证误差上具有 U形曲线。\n早停策略通过控制训练步数来控制模型的有效容量capacity 。\n早停策略只需要跑一轮训练就能够得到很多的超参数（即：训练步数）及其对应的验证误差。\n早停是正则化的一种非常不起眼的形式，其优点有：\n它几乎不需要干涉基本的训练过程，适合任何模型。\n可以单独使用，或者与其他的正则化策略相结合。\n早停不仅有正则化的好处，还有降低计算成本的好处。\n4.5 dropout\ndropout：在前向传播过程中，对网络中的每个隐层，每个隐单元都以一定的概率\np\nd\nr\no\np\np_{drop}\npdrop 被删除，最后得到一个规模更小的网络。在反向传播过程中，仅仅针对该小网络进行权重更新。\n所谓的删除，即指定该该隐单元的输出都为 0。一旦隐单元的权重为0，则该隐单元对后续神经元的影响均为 0 。\n输入层和输出层的神经元不会被删除，因为这两个层的神经元的数量是固定的。理论上可以对输入层应用dropout ，使得可以有机会删除一个或者多个输入特征。但实际工程中，通常不会这么做。\n隐单元删除发生在一个训练样本的训练期间。\n不同的训练样本，其删除的隐单元的集合是不同的，因此裁剪得到的小网络是不同的。\n不同的训练样本，隐单元被删除的概率 都是相同的。\n在不同batch 之间的同一个训练样本，其删除的隐单元的集合也是不同的。\n在不同的梯度更新周期，会从完整的网络中随机删除不同的神经元，因此裁剪得到的小网络是不同的。但是在这个过程中，隐单元被删除的概率是相同的。\n可以指定某一个隐层或者某几个隐层执行dropout，而没有必要针对所有的隐层执行dropout 。\n可以对网络的每个隐单元指定不同的删除概率，但实际工程中，通常不会这么做。\n定义一个掩码向量\nμ\n⃗\n\\vec{\\mu}\nμ\n，它给出了哪些隐单元被保留哪些隐单元被删除：掩码为 0 的位置对应的隐单元被删除，掩码为1 的位置对应的隐单元被保留。定义\nJ\n(\nθ\n,\nμ\n⃗\n⃗\n)\nJ(\\vec{\\theta,\\vec{\\mu}})\nJ(θ,μ\n)为参数\nθ\n⃗\n\\vec{\\theta}\nθ\n和掩码\nu\n⃗\n\\vec{u}\nu\n共同定义的模型代价，dropout的目标是最小化\nE\nμ\n⃗\nJ\n(\nθ\n⃗\n,\nμ\n⃗\n)\nE_{\\vec{\\mu}}J(\\vec{\\theta}, \\vec{\\mu})\nEμ\nJ(θ\n,μ\n)。\n这里采用期望，因为掩码向量\nu\n⃗\n\\vec{u}\nu\n是一个随机向量，对于每个训练样本\nu\n⃗\n\\vec{u}\nu\n都可能不同。\n因为掩码向量具有指数多个，因此期望包含了指数多项。实际应用中，可以通过抽样\nu\n⃗\n\\vec{u}\nu\n来获得期望的无偏估计。\n5.深度模型的优化\n5.1 参数初始化策略\n有些优化算法是非迭代的，可以直接解析求解最优解；有些优化算法是迭代的，但是它们是初始值无关的。深度学习不具有这两类性质，通常是迭代的，且与初始值相关。\n深度学习中，大多数算法都受到初始值的影响。初始值能够决定：算法最终是否收敛、以及收敛时的收敛速度有多快、以及收敛到一个代价函数较高还是较低的值。\n深度学习中，初始值也会影响泛化误差，而不仅仅是目标函数的最优化。因为如果选择一个不好的初始值，则最优化的结果会落在参数空间的一个较差的区域。此时会导致模型一个较差的泛化能力。\n目前深度学习中，选择初始化策略是启发式的。\n大多数初始化策略使得神经网络初始化时实现一些良好的性质。但是这些性质能否在学习过程中保持，难以保证。\n有些初始化点从最优化的观点是有利的，但是从泛化误差的观点来看是不利的。\n设定一个好的初始化策略是困难的，因为神经网络最优化任务至今都未被很好理解。\n对于初始点如何影响泛化误差的理论是空白的，几乎没有任何指导。\n通常的参数初始化策略为：随机初始化权重，偏置通过启发式挑选常数，额外的参数也通过启发式挑选常数。\n也可以使用机器学习来初始化模型的参数。在同样的数据集上，即使是用监督学习来训练一个不相关的任务，有时也能够得到一个比随机初始化更好的初始值。原因是：监督学习编码了模型初始参数分布的某些信息。\n5.1.1 权重初始化\n通常权重的初始化是从高斯分布或者均匀分布中挑选出来的值。\n从高斯分布还是均匀分布中挑选，看起来似乎没有很大差别，实际上也没有被仔细研究。\n该分布的范围（如均匀分布的上、下限）对优化结果和泛化能力有很大的影响。\n初始权重的大小很重要，下面的因素决定了权重的初始值的大小：\n更大的初始权重具有更强的破坏对称性的作用，有助于避免冗余的单元。\n更大的初始权重也有助于避免梯度消失。\n更大的初始权重也容易产生梯度爆炸。\n循环神经网络中，更大的初始权重可能导致混沌现象：对于输入中的很小的扰动非常敏感，从而导致确定性算法给出了随机性结果。\n关于如何初始化网络，正则化和最优化有两种不同的角度：\n从最优化角度，建议权重应该足够大，从而能够成功传播信息。\n从正则化角度，建议权重小一点（如 正则化），从而提高泛化能力。\n实践中，通常需要将初始权重范围视作超参数。如果计算资源允许，可以将每层权重的初始数值范围设置为一个超参数，然后使用超参数搜索算法来挑选这些超参数。\n5.1.2 偏置初始化\n偏置的初始化通常更容易。大多数情况下，可以设置偏置初始化为零。\n有时可以设置偏置初始化为非零，这发生在下面的三种情况：\n如果偏置是作为输出单元，则初始化偏置为非零值。假设初始权重足够小，输出单元的输出仅由初始化偏置决定，则非零的偏置有助于获取正确的输出边缘统计。\n有时选择偏置的初始值以免初始化引起激活函数饱和。如：ReLU 激活函数的神经元的偏置设置为一个小的正数，从而避免ReLU 初始时就位于饱和的区域。\n有时某个单元作为开关来决定其他单元是使用还是不使用。此时偏置应该非零，从而打开开关。\n6. 优化算法\n6.1 动量法（Momentum）\n该适用于隧道型曲面，梯度下降法在狭长的隧道型函数上表现不佳，如下图所示\n函数主体缓缓向右方下降\n在主体方向两侧各有一面高墙，导致垂直于主体方向有更大的梯\n度\n梯度下降法会在隧道两侧频繁震荡\n而动量法每次更新都吸收一部分上次更新的余势。这样主体方向的更新就得到了更大的保留，从而效果被不断放大。物理上这就像是推一个很重的铁球下山，因为铁球保持了下山主体方向的动量，所以在隧道上沿两侧震荡测次数就会越来越少。\n\nv\nt\n=\nγ\nv\nt\n−\n1\n+\nη\n∇\nθ\nJ\n(\nθ\n)\nv_{t} = \\gamma v_{t-1} + \\eta \\nabla_{\\theta}J(\\theta)\nvt =γvt−1 +η∇θ J(θ)\n\nθ\nt\n=\nθ\nt\n−\n1\n−\nv\nt\n\\theta_{t} = \\theta_{t-1} - v_{t}\nθt =θt−1 −vt\n6.2 Adagrad\n该算法的特点是自动调整学习率，适用于稀疏数据。梯度下降法在每一步对每一个参数使用相同的学习率，这种一刀切的做法不能有效的利用每一个数据集自身的特点。\nAdagrad 是一种自动调整学习率的方法：\n随着模型的训练，学习率自动衰减\n对于更新频繁的参数，采取较小的学习率\n对于更新不频繁的参数，采取较大的学习率\n6.3 Adadelta(Adagrad的改进算法)\nAdagrad的一个问题在于随着训练的进行，学习率快速单调衰减。Adadelta则使用梯度平方的移动平均来取代全部历史平方和。\n定义移动平均：\nE\n[\ng\n2\n]\nt\n=\nγ\nE\n[\ng\n2\n]\nt\n−\n1\n+\n(\n1\n−\nγ\n)\ng\nt\n2\nE[g^{2}]_{t} = \\gamma E[g^{2}]_{t-1} + (1-\\gamma)g_{t}^{2}\nE[g2]t =γE[g2]t−1 +(1−γ)gt2\nAdadelta 的第一个版本也叫做 RMSprop，是Geoff Hinton独立于Adadelta提出来的。\n6.4 Adam\n如果把Adadelta里面梯度的平方和看成是梯度的二阶矩，那么梯度本身的求和就是一阶矩。Adam算法在Adadelta的二次矩基础之上又引入了一阶矩。而一阶矩，其实就类似于动量法里面的动量。\n7. Normalization\n7.1 batch normalization\nbatch normalization是优化神经网络的一大创新。\n它并不是一个优化算法，而是一个自适应的、调整参数模型的方法。\n它试图解决训练非常深的神经网络的困难。\n深度神经网络训练困难的一个重要原因是：深度神经网络涉及很多层的叠加，而每一层的参数更新会导致上一层的输入数据分布发生变化。这会带来两个问题：\n下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止。\n通过层层叠加，高层的输入分布变化会非常剧烈。这就使得高层需要不断去适应底层的参数更新变化。这就要求我们需要非常谨慎的设定学习率、初始化权重、参数更新策略。\n7.2 layer normalization\n与 BN 不同，LN 是对单个样本的同一层的神经元进行归一化，同层神经元使用相同的均值和方差。对于该层神经元，不同样本可以使用的均值和方差不同。\n与之相比，BN 是对每个神经元在mini batch 样本之间计算均值和方差。对每个神经元，mini batch 中的所有样本在该神经元上都使用相同的均值和方差。但是不同神经元使用不同的均值和方差。\n因此LN 不依赖于batch size，也不依赖于网络深度。因此它适合在线学习，也适合于RNN 网络。","data":"2019年04月21日 13:32:02"}
{"_id":{"$oid":"5d343b1162f717dc0659b317"},"title":"深度学习和自然语言处理中的attention和memory机制","author":"仲浩","content":"Attention机制是最近深度学习的一个趋势。在一次采访中，OpenAI的研究总监Ilya Sutskever说attention机制是最令人兴奋的进步之一，而且已经广为使用。听起来激动人心吧。但attention机制究竟是什么呢？\n神经网络里的attention机制是（非常）松散地基于人类的视觉注意机制。人类的视觉注意机制已经被充分地研究过了，而且提出了多个不同的模型，所有的模型归根结底都是按照“高分辨率”聚焦在图片的某个特定区域并以“低分辨率”感知图像的周边区域的模式，然后不断地调整聚焦点。\nAttention在神经网络领域有着很长的历史，尤其是在图像识别领域。相关的论文有Learning to combine foveal glimpses with a third-order Boltzmann machine和Learning where to Attend with Deep Architectures for Image Tracking。但直到最近，attention机制才被引入NLP界常用的（视觉领域也逐步使用的）递归神经网络结构中。这正是我们这篇文章的主要关注点。\nattention解决了什么问题？\n我们以神经机器翻译（Neural Machine Translation，NMT）为例，来理解attention能为我们做什么。传统的机器翻译系统通常依赖于基于文本统计特性的复杂特征工程。简而言之，这些系统非常复杂，需要投入大量工程来搭建它们。神经机器翻译系统则有所区别。在NMT系统里，我们把一句话的意思映射为一个固定长度的表征向量，然后基于此向量生成翻译文本。由于不依赖于类似n-gram计数，而是捕捉文本更高层次的含义，NMT系统生成的翻译语句比大多数其它方法都要好。更重要的是，NMT系统的搭建和训练过程更方便，它们不需要任何手工的特征工程。事实上，TensorFlow只需要几百行代码就能实现一个简单版本。\n大多数NMT系统使用递归神经网络（RNN）将源语句（比如，一句德语）编码为一个向量，然后同样用RNN将其解码为英语句子。\n如上图所示，“Echt”、“Dicke”和“Kiste”依次输入到编码器中，一个特殊字符标志输入结束（图中未显示），然后解码器开始生成翻译的语句。解码器持续逐词地生成，直到生成句子的终止符。这里的h向量表示了编码器的内部状态。\n如果你仔细观察，你会发现解码器在翻译时仅依赖编码器最后的隐藏状态（上图的h3）。h3向量必须对源句子的所有内容都进行编码。它必须充分地捕捉含义。用专业术语来说，这个向量就是一个sentence embedding。事实上，如果你用PCA或者t-SNE降维之后将不同句子的embedding绘制出来，你将看到语义相近的句子彼此很接近。真是令人觉得神奇。\n然而，我们似乎无法把一个很长的句子所包含的所有信息编码成一个向量，然后解码器仅根据这个向量生成完美的翻译，这种假设显得不可理喻。我们假设原文句子长度有50个单词。英文译文的第一个单词可能与原文的第一个单词高度相关。但这意味着解码器必须考虑50步之前的信息，而且那段信息需要以某种形式已经被编入向量中。众所周知，RNN在处理这类长距离依赖关系时会出现问题。理论上，LSTM这类结构能够处理这个问题，但在实践中，长距离依赖关系仍旧是个问题。例如，研究人员发现将原文倒序（将其倒序输入编码器）产生了显著改善的结果，因为从解码器到编码器对应部分的路径被缩短了。同样，两次输入同一个序列似乎也有助于网络更好地记忆。\n我认为倒序句子这种方法属于“hack”手段。它属于被实践证明有效的方法，而不是有理论依据的解决方法。大多数翻译的基准都是用法语、德语等语种，它们和英语非常相似（即使汉语的词序与英语也极其相似）。但是有些语种（像日语）句子的最后一个词语在英语译文中对第一个词语有高度预言性。那么，倒序输入将使得结果更糟糕。还有其它办法吗？那就是Attention机制。\n有了Attention机制，我们不再需要将完整的原文句子编码为固定长度的向量。相反，我们允许解码器在每一步输出时“参与（attend）”到原文的不同部分。尤为重要的是我们让模型根据输入的句子以及已经产生的内容来决定参与什么。因此，在形式非常相似的语种之间（如英语与德语），解码器可能会选择顺序地参与事情。生成第一个英语词语时参与原文的第一个词语，以此类推。这正是论文Neural Machine Translation by Jointly Learning to Align and Translate的成果，如下图所示：\ny’是编码器生成的译文词语，x’是原文的词语。上图使用了双向递归网络，但这并不是重点，你先忽略反向的路径吧。重点在于现在每个解码器输出的词语yt取决于所有输入状态的一个权重组合，而不只是最后一个状态。a’是决定每个输入状态对输出状态的权重贡献。因此，如果a3,2的值很大，这意味着解码器在生成译文的第三个词语时，会更关注与原文句子的第二个状态。a’求和的结果通常归一化到1（因此它是输入状态的一个分布）。\nAttention机制的一个主要优势是它让我们能够解释并可视化整个模型。举个例子，通过对attention权重矩阵a的可视化，我们能够理解模型翻译的过程。\n我们注意到当从法语译为英语时，网络模型顺序地关注每个输入状态，但有时输出一个词语时会关注两个原文的词语，比如将“la Syrie”翻译为“Syria”。\nAttention的成本\n如果再仔细观察attention的等式，我们会发现attention机制有一定的成本。我们需要为每个输入输出组合分别计算attention值。50个单词的输入序列和50个单词的输出序列需要计算2500个attention值。这还不算太糟糕，但如果你做字符级别的计算，而且字符序列长达几百个字符，那么attention机制将会变得代价昂贵。\n其实它和我们的直觉恰恰相反。人类的注意力是节省计算资源的。当专注于一件事时，我们能忽略其它事情。但这并不是我们上一个模型的作法。我们在决定专注于某个方面之前先仔细观察每件事。直观地说，这相当于输出一个翻译后的词语，然后遍历记忆里所有文本再决定下一个输出什么。这似乎是一种浪费，而且没人会这么干。事实上，它更类似于内存访问，不是attention，在我看来有点儿用词不当（下文会继续讨论）。不过，这并没有阻碍attention机制的流行传播。\nattention的另一种替代方法是用强化学习（Reinforcement Learning）来预测关注点的大概位置。这听起来更像是人的注意力，这也是Recurrent Models of Visual Attention文中的作法。然而，强化学习模型不能用反向传播算法端到端训练，因此它在NLP的应用不是很广泛。\n机器翻译之外领域的Attention机制\n到目前为止，我们已经见识了attention在机器翻译领域的应用。但上述的attention机制同样也能应用于递归模型。让我们再来看几个例子。\n在Show，Attend and Tell一文中，作者将attention机制应用于生成图片的描述。他们用卷积神经网络来“编码”图片，并用一个递归神经网络模型和attention机制来生成描述。通过对attention权重值的可视化（就如之前机器翻译的例子一样），在生成词语的同时我们能解释模型正在关注哪个部分。\n在Grammar as a Foreign Language论文中，作者用递归神经网络模型和attention机制的来生成语法分析树。可视化的attention矩阵让人深入地了解网络模型如何生成这些树：\n在Teaching Machines to Read and Comprehend论文里，作者利用RNN模型读入文本，先读入一个（合成的）问题，然后产生一个答案。通过将attention可视化，我们可以看到网络模型在试图寻找问题答案的时候关注哪些方面：\nATTENTION = (FUZZY) MEMORY?\nattention机制解决的根本问题是允许网络返回到输入序列，而不是把所有信息编码成固定长度的向量。正如我在上面提到，我认为使用attention有点儿用词不当。换句话说，attention机制只是简单地让网络模型访问它的内部存储器，也就是编码器的隐藏状态。在这种解释中，网络选择从记忆中检索东西，而不是选择“注意”什么。不同于典型的内存，这里的内存访问机制是弹性的，也就是说模型检索到的是所有内存位置的加权组合，而不是某个独立离散位置的值。弹性的内存访问机制好处在于我们可以很容易地用反向传播算法端到端地训练网络模型（虽然有non-fuzzy的方法，其中的梯度使用抽样方法计算，而不是反向传播）。\n记忆机制本身的历史更久远。标准递归网络模型的隐藏状态本身就是一种内部记忆。RNN由于存在梯度消失问题而无法从长距离依赖学习。LSTM通过门控机制对此做了改善，它允许显式的记忆删除和更新。\n更复杂的内存结构的趋势还在延续。End-To-End Memory Networks一文中的方法允许网络在输出内容前多次读入相同的序列，每一步都更新记忆内容。举个例子，输入一个故事，在经过多步推理之后回答一个问题。然而，当网络参数的权重以某种特定方式被绑定，端到端记忆网络的记忆机制就和这里所介绍的attention机制一样了，只是它是多跳的记忆（因为它试图整合多个句子信息）。\n神经图灵机器使用类似的记忆机制，但有一个更复杂的解决方案，它同时基于内容（如在这里）和位置，使网络模型通过学习模式来执行简单的计算机程序，比如排序算法。\n在将来，我们很可能看到记忆机制和attention机制之间有更清晰的区别，也许是沿着Reinforcement Learning Neural Turing Machines，它尝试学习访问模式来处理外部接口。\n原文地址：ATTENTION AND MEMORY IN DEEP LEARNING AND NLP（译者/赵屹华 审校/刘翔宇 责编/仲浩）\n译者简介：赵屹华，计算广告工程师@搜狗，前生物医学工程师，关注推荐算法、机器学习领域。","data":"2016年01月12日 01:15:54"}
{"_id":{"$oid":"5d343b1162f717dc0659b319"},"title":"自然语言处理之中文分词算法","author":"sysu63","content":"关于分词\n目前有三大主流分词方法：基于字符串匹配的分词方法、基于理解的分词方法和基于统计的分词方法。\n1、基于字符串匹配的分词方法\n基于字符串匹配的分词方法又称为机械分词方法，它需要有一个初始的充分大的词典，然后将待分词的字符串与词典中的元素进行匹配，若能成功匹配，则将该词切分出来。\n按扫描方向的不同，字符串匹配分词方法可以分为正相匹配和逆向匹配；按照不同长度的匹配优先度可以划分为最大匹配和最小匹配。\n1.1正向最大匹配\n1.从左到右将待切分句子的m个字符作为匹配字符，m为初始词典中最长词条的长度。\n2.将字符与字典中元素进行匹配：\n2.1.若匹配成功，则将这个字符作为一个词切分出来\n2.2.若匹配不成功，则将这个字符的最后一个字去掉，再进行匹配，重复上述过程，知道切分完整个文本为止。\n举个例子吧：\n假设我们要切分的句子为“南京市长江大桥”，字典中最长的元素长度为5，则先取待切分句子的前5个字符“南京市长江”。字典中没有元素与之匹配，长度减一，则变成“南京市长”，匹配成功。\n对剩余三个字“江大桥”再次进行正向最大匹配，会切成“江”、“大桥”；\n整个句子切分完成为：南京市长、江、大桥；\n1.2逆向最大匹配\n逆向最大匹配思想与正向最大匹配基本相同，不同的是将扫描方向变成了从右往左，匹配不成功时，去掉最左边的字符。\n实验表明，逆向最大匹配算法效果要优于正向最大匹配算法。\n“南京市长江大桥”的逆向最大匹配：\n1.取出“南京市长江大桥”的后5个字“市长江大桥”，字典中无匹配元素，将字符“市”去掉，发现词典中有匹配，切割下来；\n2.对剩余的“南京市”进行分词，整体结果为：南京市、长江大桥\n1.3双向最大匹配\n双向最大匹配法是将正向最大匹配法得到的分词结果和逆向最大匹配法得到的结果进行比较，从而决定正确的分词方法。\n还是上面的例子，双向最大匹配的划分结果为：南京市长、南京市、长江大桥、江、大桥。\n这类算法的优点是速度快，时间复杂度为O（n），实现简单；但是对于歧义和未登录词表现不佳。\n2、基于理解的分词方法\n其基本思想就是在分词的同时进行句法、语义分析，利用句法信息和语义信息来处理歧义现象。它通常包括三个部分：分词子系统、句法语义子系统、总控部分。由于汉语语言知识的笼统、复杂性，难以将各种语言信息组织成机器可直接读取的形式，因此目前基于理解的分词系统还处在试验阶段。\n3、基于统计的分词方法\n主要思想：每个字都是词的最小单元，如果相连的字在不同的文本中出现的频率越多，这就越有可能是一个词。因此我们可以用相邻字出现的频率来衡量组词的可能性，当频率高于某个阈值时，我们可以认为这些字可能会构成一个词。\n主要统计模型： N元文法模型（N-gram），隐马尔可夫模型（Hidden Markov Model，HMM），最大熵模型（ME），条件随机场（Conditional Random Fields，CRF）等\n优势：在实际运用中常常将字符串匹配分词和统计分词结合使用，这样既体现了匹配分词速度快、效率高的优点，同时又能运用统计分词识别生词、自动消除歧义等方面的特点。\n3.1 N-gram模型思想\n该模型基于这样一种假设，第n个词出现只与前面n-1个词相关，而与其他词都不相关。整句话的概率就是各个词出现概率的乘积。\n对于一个句子T，假设它由n个词\nw1,w2,w3,⋯,wn\nw\n1\n,\nw\n2\n,\nw\n3\n,\n⋯\n,\nw\nn\n{w_1},{w_2},{w_3}, \\cdots ,{w_n}组成的，则\np(T)=p(w1w2w3⋯wn)=p(w1)p(w2|w1)p(w3|w1w2)⋯p(wn|w1w2⋯wn−1)\np\n(\nT\n)\n=\np\n(\nw\n1\nw\n2\nw\n3\n⋯\nw\nn\n)\n=\np\n(\nw\n1\n)\np\n(\nw\n2\n|\nw\n1\n)\np\n(\nw\n3\n|\nw\n1\nw\n2\n)\n⋯\np\n(\nw\nn\n|\nw\n1\nw\n2\n⋯\nw\nn\n−\n1\n)\np\\left( T \\right) = p\\left( {{w_1}{w_2}{w_3} \\cdots {w_n}} \\right) = p\\left( {{w_1}} \\right)p\\left( {{w_2}\\left| {{w_1}} \\right.} \\right)p\\left( {{w_3}\\left| {{w_1}} \\right.{w_2}} \\right) \\cdots p\\left( {{w_n}\\left| {{w_1}} \\right.{w_2} \\cdots {w_{n - 1}}} \\right)，计算这个式子很麻烦，我们引入马尔科夫假设：一个词的出现仅依赖于它前面有限的几个词。如果一个词的出现仅依赖于它前面出现的一个词，我们就称之为bigram。则上式变为：\np(T)=p(w1)p(w2|w1)p(w3w2)⋯p(wnwn−1)\np\n(\nT\n)\n=\np\n(\nw\n1\n)\np\n(\nw\n2\n|\nw\n1\n)\np\n(\nw\n3\nw\n2\n)\n⋯\np\n(\nw\nn\nw\nn\n−\n1\n)\np\\left( T \\right) = p\\left( {{w_1}} \\right)p\\left( {{w_2}\\left| {{w_1}} \\right.} \\right)p\\left( {{w_3}{w_2}} \\right) \\cdots p\\left( {{w_n}{w_{n - 1}}} \\right)\n如果一个词的出现仅依赖于它前面出现的两个词，那么我们就称之为trigram。\n以此类推，N元模型就是假设当前词的出现概率只同它前面的N-1个词有关。（实际中通常只用到二元模型）\n3.2 隐马尔可夫模型（HMM）\n3.2.1 隐马尔可夫模型简介\n隐马尔可夫模型中的变量有两组。一组为状态变量{y1, y2, …, yn}，其中yi表示第i时刻所处的状态，这些状态是隐藏的、不可观测的，因此又称为隐变量，隐变量的取值通常是离散的。第二组是观测变量{x1, x2, …, xn}，其中xi表示第i时刻的观测值。\n在任一时刻，观测变量的取值只与该时刻的状态变量有关，即xi由yi决定。而当前状态只与前一时刻的状态有关，与其他状态无关。\n3.2.2 隐马尔可夫模型的三大问题\n一般的，一个HMM可以表示为u=（S, K, A, B, π）， 其中S是状态集合，K是输出符号也就是观察集合，A是状态转移概率，B是符号发射概率，π是初始状态的概率分布。HMM主要解决三个基本问题：\n估计问题，给定一个观察序列O=O1,O2,O3,… ,Ot和模型u=(A,B,π)，计算观察序列的概率;\n序列问题，给定一个观察序列O=O1,O2,O3… Ot和模型μ=(A, B, π)，计算最优的状态序列Q=q1,q2,q3…qt;\n参数估计问题，给定一个观察序列O=O1,O2,O3… Ot，如何调节模型μ=(A,B, π)的参数，使得P(O|μ)最大。\n三类问题的求解在这里略去。\n3.2.3 隐马尔可夫模型分词方法\n隐马尔可夫的三大问题分别对应了分词中的几个步骤。参数估计问题就是分词的学习阶段，通过海量的预料数据来学习归纳出分词模型的各个参数。状态序列问题是分词的执行阶段，通过观测变量（待分词句子的序列）来预测出最优的状态序列（分词结构）。\n设状态集合S=（B.M,E,S），每个状态代表的是这个字在词语中的位置，B代表该字是词语中的起始字，M代表是词语中的中间字，E代表是词语中的结束字，S则代表是单字成词;观察值集合K =(所有的汉字);则中文分词的问题就是通过观察序列来预测出最优的状态序列。\n比如观察序列为：\nO = 南京市长江大桥\n预测的状态序列为：\nQ = BMEBMME\n根据这个状态序列我们可以进行切词：\nBME/BMME/\n所以切词结果如下：\n南京市/长江大桥/\n因为HMM分词算法是基于字的状态(BEMS)来进行分词的，因此很适合用于新词发现，某一个新词只要标记为如“BMME”，就算它没有在历史词典中出现过，HMM分词算法也能将它识别出来。\n中文分词工具介绍\npython常用的分词包有jieba分词、SnowNLP、THULAC、NLPIR 等。\n1、jieba分词\njieba分词是国内使用人数最多的中文分词工具。\n1.1、jieba分词的三种模式\n（1）精确模式：试图将句子最精确地切分，适合文本分析；\n（2）全模式：把句子中所有可以成词的词语都扫描出来，速度非常快，但是不能解决歧义；\n（3）搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。\n1.2、jieba分词涉及的算法\njieba分词过程中主要涉及如下几种算法：\n（1）基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG)；\n（2）采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合；\n（3）对于未登录词，采用了基于汉字成词能力的 HMM 模型，采用Viterbi 算法进行计算；\n（4）基于Viterbi算法做词性标注；\n（5）基于tf-idf和textrank模型抽取关键词；\njieba分词测试如下\nimport jieba u=\"我来到北京清华大学\" #全模式 test1 = jieba.cut(u, cut_all=True) print(\"全模式: \" + \"| \".join(test1)) #精确模式 test2 = jieba.cut(u, cut_all=False) print(\"精确模式: \" + \"| \".join(test2)) #搜索引擎模式 test3= jieba.cut_for_search(u) print(\"搜索引擎模式:\" + \"| \".join(test3))\n全模式: 我| 来到| 北京| 清华| 清华大学| 华大| 大学\n精确模式: 我| 来到| 北京| 清华大学\n搜索引擎模式:我| 来到| 北京| 清华| 华大| 大学| 清华大学\n2、SnowNLP\nSnowNLP可以方便的处理中文文本内容，是受到了TextBlob的启发而写的。SnowNLP主要包括如下几个功能：\n（1）中文分词（Character-Based Generative Model）；\n（2）词性标注（3-gram HMM）；\n（3）情感分析（简单分析，如评价信息）；\n（4）文本分类（Naive Bayes）\n（5）转换成拼音（Trie树实现的最大匹配）\n（6）繁简转换（Trie树实现的最大匹配）\n（7）文本关键词和文本摘要提取（TextRank算法）\n（8）计算文档词频（TF，Term Frequency）和逆向文档频率（IDF，Inverse Document Frequency）\n（9）Tokenization（分割成句子）\n（10）文本相似度计算（BM25）\nSnowNLP的最大特点是特别容易上手，用其处理中文文本时能够得到不少有意思的结果，但不少功能比较简单，还有待进一步完善。\n3、THULAC\nTHULAC由清华大学自然语言处理与社会人文计算实验室研制推出的一套中文词法分析工具包，具有中文分词和词性标注功能。THULAC具有如下几个特点：\n（1）能力强。利用我们集成的目前世界上规模最大的人工分词和词性标注中文语料库（约含5800万字）训练而成，模型标注能力强大。\n（2）准确率高。该工具包在标准数据集Chinese Treebank（CTB5）上分词的F1值可达97.3％，词性标注的F1值可达到92.9％，与该数据集上最好方法效果相当。\n（3）速度较快。同时进行分词和词性标注速度为300KB/s，每秒可处理约15万字。只进行分词速度可达到1.3MB/s。\n4、NLPIR\nNLPIR分词系统是由北京理工大学张华平博士研发的中文分词系统，经过十余年的不断完善，拥有丰富的功能和强大的性能。NLPIR是一整套对原始文本集进行处理和加工的软件，提供了中间件处理效果的可视化展示，也可以作为小规模数据的处理加工工具。主要功能包括：中文分词，词性标注，命名实体识别，用户词典、新词发现与关键词提取等功能。","data":"2018年05月03日 22:57:25"}
{"_id":{"$oid":"5d343b1262f717dc0659b31b"},"title":"自然语言处理基础 一 及 sklearn实现 分析词代码实现","author":"R戎","content":"自然语言概念\n自然语言，即我们人类日常所使用的语言，是人类交际的重要方式，也是人类区别于其他动物的本质特征。\n我们只能使用自然语言与人进行交流，而无法与计算机进行交流。\n自然语言处理\n自然语言处理（NLP Natural Language Processing），是人工智能（AI Artificial Intelligence）的一部分，实现人与计算机之间的有效通信。\n自然语言处理属于计算机科学领域与人工智能领域，其研究使用计算机编程来处理与理解人类的语言。\n应用场景\n自然语言处理，具有非常广泛的应用场景，例如：\n情感分析\n机器翻译\n文本相似度匹配\n智能客服\n通用技术\n分词\n停用词过滤\n词干提取\n词形还原\n词袋模型\nTF-IDF\nWord2Vec\n说明：\nscikit-learn库中实现的tf-idf转换，与标准的公式略有不同。并且，tf-idf结果会使用L2范数进行规范化处理。\n\nimport numpy as np # 对语料库中出现的词汇进行词频统计，相当于词袋模型。 # 操作方式：将语料库当中出现的词汇作为特征，将词汇在当前文档中出现的频率（次数） # 作为特征值。 from sklearn.feature_extraction.text import CountVectorizer count = CountVectorizer() # 语料库 docs = np.array([ \"Where there is a will, there is a way.\", \"There is no royal road to learning.\", ]) # bag是一个稀疏的矩阵。因为词袋模型就是一种稀疏的表示。 bag = count.fit_transform(docs) # 输出单词与编号的映射关系。 print(count.vocabulary_) # 调用稀疏矩阵的toarray方法，将稀疏矩阵转换为ndarray对象。 print(bag.toarray())\n{'where': 8, 'there': 5, 'is': 0, 'will': 9, 'way': 7, 'no': 2, 'royal': 4, 'road': 3, 'to': 6, 'learning': 1} [[2 0 0 0 0 2 0 1 1 1] [1 1 1 1 1 1 1 0 0 0]]\nfrom sklearn.feature_extraction.text import TfidfTransformer tfidf = TfidfTransformer() print(tfidf.fit_transform(count.fit_transform(docs)).toarray())\n[[0.53594084 0. 0. 0. 0. 0.53594084 0. 0.37662308 0.37662308 0.37662308] [0.29017021 0.4078241 0.4078241 0.4078241 0.4078241 0.29017021 0.4078241 0. 0. 0. ]]\n评论情感分析\n项目背景¶\n公司活动，新闻，微博，影评，商品评价等。\n加载数据集\nimport pandas as pd import numpy as np data = pd.read_csv(r\"movie.csv\") data.head()\nlabel comment 0 pos 此英雄完全自给自足，没有任何超能力，自己造就自己，酷！科技以人为本，发展才是硬道理！ 1 pos 如果一个男人嫌女人太聪明了，那一定是因为他自己还不够牛逼 2 pos 这是一个会搞笑，爱臭屁又喋喋不休的英雄噢！结尾那句“I am Iron Man”太帅了~~ 3 pos 没想到会比这侠那侠的都要好看！2个多小时的片子并不觉长，紧凑，爽快，意犹未尽…… 4 pos 想起一个人，铁臂阿木童.\n数据预处理\n数据清洗\n# 缺失值探索。 data.isnull().sum(axis=0) # 异常值探索。 data[\"label\"].value_counts() # 重复值 # data.duplicated().sum() # data[data.duplicated()] data.drop_duplicates(inplace=True)\n数据转换\n将label与comment列转换为数值类型。\ndata[\"label\"] = data[\"label\"].map({\"pos\": 1, \"neg\": 0}) data[\"label\"].value_counts()\n1 11555 0 3928 Name: label, dtype: int64\n# 用于进行中文分词的库。安装： # pip install jieba import jieba import re # 获取停用词列表 def get_stopword(): # 默认情况下，在读取文件时，双引号会被解析为特殊的引用符号。双引号中的内容会正确解析，但是双引号不会解析为文本内容。 # 在这种情况下，如果文本中仅含有一个双引号，会产生解析错误。如果需要将双引号作为普通的字符解析，将quoting参数设置为3。 stopword = pd.read_csv(r\"stopword.txt\", header=None, quoting=3, sep=\"a\") # 转换为set，这样可以比list具有更快的查询速度。 return set(stopword[0].tolist()) # 清洗文本数据 def clear(text): return re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……\u0026*（）]+\", \"\", text) # 进行分词的函数。 def cut_word(text): return jieba.cut(text) # 去掉停用词函数。 def remove_stopword(words): # 获取停用词列表。 stopword = get_stopword() return [word for word in words if word not in stopword] def preprocess(text): # 文本清洗。 text = clear(text) # 分词。 word_iter = cut_word(text) # 去除停用词。 word_list = remove_stopword(word_iter) return \" \".join(word_list) # 对文本数据（评论数据）的处理。步骤： # 1 文本清洗。去掉一些特殊无用的符号，例如@，#。 # 2 分词，将文本分解为若干单词。 # 3 去除停用词。 # 以上步骤通过调用preprocess方法来实现。 data[\"comment\"] = data[\"comment\"].apply(lambda text: preprocess(text))\n# 调用cut方法可以对文本进行分词，返回结果。cut方法返回的是生成器对象。 # jieba.cut(\"今天我们学习自然语言处理。\") # lcut方法返回的是列表。 # jieba.lcut(\"今天我们学习自然语言处理。\") data[\"comment\"].head()\n0 英雄 自给自足 超能力 造就 酷 科技 以人为本 发展 硬道理 1 男人 嫌 女人 太 聪明 是因为 牛 逼 2 这是 搞笑 爱 臭屁 喋喋不休 英雄 噢 结尾 那句 IamIronMan 太帅 3 没想到 这侠 那侠 要好看 小时 片子 不觉 长 紧凑 爽快 意犹未尽 4 想起 铁臂 阿木童 Name: comment, dtype: object\n# 使用TfidfVectorizer来进行文本向量化，具有一个局限（不足）：就是语料库中存在多少个单词，就会具有多少个特征， # 这样会造成特征矩阵非常庞大，矩阵非常稀疏。 from sklearn.feature_extraction.text import TfidfVectorizer tfidf = TfidfVectorizer() tfidf.fit_transform(data[\"comment\"].tolist())\ndata[\"comment\"]\n0 英雄 自给自足 超能力 造就 酷 科技 以人为本 发展 硬道理 1 男人 嫌 女人 太 聪明 是因为 牛 逼 2 这是 搞笑 爱 臭屁 喋喋不休 英雄 噢 结尾 那句 IamIronMan 太帅 3 没想到 这侠 那侠 要好看 小时 片子 不觉 长 紧凑 爽快 意犹未尽 4 想起 铁臂 阿木童 5 个会 飞 锅炉 6 美国 人真 幼稚 喜欢 大力士 7 超级 英雄 电影 差 很远 RobertDowneyJr 天才 演员 电影 娱乐性 十足 话... 8 腐朽 堕落 资本主义 垃圾 9 爱看 商业片 动作片 科幻片 动画片 喜剧片 10 太拉风 装备 蝙蝠侠 摩托 拉风 11 资本主义 钢铁 炮弹 纸老虎 12 蜘蛛侠 哈里波特 功夫 骇客帝国 攻壳 II 吐 死 13 真的 男生 喜欢 看吧 14 高科技 份 15 中规中矩 超级 英雄 片 16 侠中 算是 17 期望值 高 不免有些 失望 当作 minitransformer 斯坦 李 作品 名单 蜘蛛... 18 唐尼 goodluck 19 复仇者 连门 回来 补 真的 片子 复仇者 联盟 啊啊啊 20 Jarvis 21 主角 高 大帅 情节 老套 经不起 推敲 起伏跌宕 噱头 幽默 烂 装备 代表 票房 烂片 22 完 复仇者 联盟 倒 回来 第一部 发现 罗伯特 童孩 演了 福尔摩斯 特别 喜欢 咔 咔 ... 23 RobertDowneyJr 24 SEXYIRONMAN 25 电影院 看一看 图个 爽 片子 26 想 一套 钢铁 侠 铠甲 27 想起 铁臂 阿木童 28 视效 噱头 情节 弱得 要死 女一号 不错 29 不错 人造 强 ... 15541 听 搞笑 研究 阶段 蛮牛 15542 影片 没什么 兴趣 一部 超过 预期 值得 收藏 15543 迅雷 抢先 版画 质 差 总 找个 高清 版 弥补 情节 空洞 带给 失望 15544 昨天 钢铁 侠 感觉 不错 15545 灰常想 一套 钢铁 侠 衣服 15546 APPLESMOM 15547 构思 错 动作 错 造型 很酷 故事 单调 15548 comiccharactersuperhero fetish 15549 爱 marvel superhero 题材 电影 ironman 心理变态 小萝卜头 糖泥 ... 15550 赞 年 极力推荐 这部 15551 喜欢 美国 漫画 接触 机会 非常少 铁人 MARVEL 二线 英雄 电影 发现 错 仅次于... 15552 好看 我会 想起 奥特曼 15553 特效 不错 剧情 15554 美国式 英雄 场面 不错 15555 特效 不错 男人 心中 想 飞 冲动 男人 想 英雄 记住 喜欢 IRONMANSAY IA... 15556 TONYSTARK 诙诣 语言 不错 那句 事实上 钢铁 侠帅 15557 电影 里 道具 科幻 构思新颖 15558 一部 好看 superheromovie 15559 好看 电影 特地去 电影院 花 80 块钱 15560 08 年度 最棒 英雄 科幻片 15561 atypicalHolleywoodblockbusterNosurprise 15562 钢铁 侠 生活 完美 男人 生活 15563 不错 想象力 主角 牛 牛 15564 每次 美国 超级 英雄 电影 影虫们 失望 钢铁 侠 剧情 不谈 确实 场 视觉 盛宴 15565 不错 特别 组装 装甲 帅呆了 15566 诺 极力推荐 真的 失望 近期 好看 15567 目前为止 喜欢 超级 英雄 电影 15568 西 老公 喜欢 15569 功夫 之王 烂片 强奸 强档 好片 回到 人间 15570 2008 04 30cathyAMK Name: comment, Length: 15483, dtype: object\nfrom sklearn.feature_extraction.text import TfidfVectorizer from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.metrics import classification_report X_train, X_test, y_train, y_test = train_test_split(data[\"comment\"], data[\"label\"], test_size=0.25, random_state=0) # TfidfVectorizer可以看做是CountVectorizer与TfidfTransformer两个类型的合体。 tfidf = TfidfVectorizer() lr = LogisticRegression(class_weight=\"balanced\") # lr = LogisticRegression() steps = [(\"tfidf\", tfidf), (\"model\", lr)] pipe = Pipeline(steps=steps) pipe.fit(X_train, y_train) y_hat = pipe.predict(X_test) print(pipe.score(X_train, y_train)) print(pipe.score(X_test, y_test)) print(classification_report(y_test, y_hat))\n0.8405959352394075 0.672436063032808 precision recall f1-score support 0 0.40 0.51 0.45 1004 1 0.81 0.73 0.77 2867 micro avg 0.67 0.67 0.67 3871 macro avg 0.60 0.62 0.61 3871 weighted avg 0.70 0.67 0.68 3871\nprint(pd.Series(y_test).value_counts()) print(pd.Series(y_hat).value_counts())\n1 2867 0 1004 Name: label, dtype: int64 1 2585 0 1286 dtype: int64\nfrom sklearn.ensemble import RandomForestClassifier rf = RandomForestClassifier(n_estimators=100, n_jobs=-1, class_weight=\"balanced\") # rf = RandomForestClassifier(n_estimators=100, n_jobs=-1) pipe.set_params(model=rf) pipe.fit(X_train, y_train) y_hat = pipe.predict(X_test) print(pipe.score(X_train, y_train)) print(pipe.score(X_test, y_test)) print(classification_report(y_test, y_hat))\n0.9889769204271444 0.706277447687936 precision recall f1-score support 0 0.38 0.21 0.27 1004 1 0.76 0.88 0.82 2867 micro avg 0.71 0.71 0.71 3871 macro avg 0.57 0.55 0.54 3871 weighted avg 0.66 0.71 0.68 3871\nfrom sklearn.ensemble import BaggingClassifier from sklearn.tree import DecisionTreeClassifier b = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, n_jobs=-1) pipe.set_params(model=b) pipe.fit(X_train, y_train) y_hat = pipe.predict(X_test) print(pipe.score(X_train, y_train)) print(pipe.score(X_test, y_test)) print(classification_report(y_test, y_hat))\n0.9770065449534964 0.7091190906742444 precision recall f1-score support 0 0.41 0.26 0.32 1004 1 0.77 0.86 0.81 2867 micro avg 0.71 0.71 0.71 3871 macro avg 0.59 0.56 0.57 3871 weighted avg 0.68 0.71 0.69 3871\nfrom sklearn.ensemble import AdaBoostClassifier ada = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=100) pipe.set_params(model=ada) pipe.fit(X_train, y_train) y_hat = pipe.predict(X_test) print(pipe.score(X_train, y_train)) print(pipe.score(X_test, y_test)) print(classification_report(y_test, y_hat))\n0.7695487426799862 0.7377938517179023 precision recall f1-score support 0 0.48 0.13 0.21 1004 1 0.76 0.95 0.84 2867 micro avg 0.74 0.74 0.74 3871 macro avg 0.62 0.54 0.52 3871 weighted avg 0.69 0.74 0.68 3871","data":"2019年04月03日 16:54:21"}
{"_id":{"$oid":"5d343b1262f717dc0659b31d"},"title":"自然语言的智能识别简介","author":"朱宏 宏","content":"Word2vec在自然语言词向量中基本原理和应用的研究\n（云南大学软件学院软件工程班，云南昆明，650500，朱宏）\n摘 要：现代计算机自然语言的智能识别技术成为了时代发展的重要方向，可以说智能化的生活已经走入了大众的生活。本文研究了Word2vec在自然语言的单词向量的基本原理和应用。Word2vec是Google公司在2013年在研究自然语言的识别模型的背景下所推出的软件工具。本文探讨了自然语言的智能识别的里程碑，并且重点研究现在最为成熟的Word2vec所采用的CBOW和Skip-gram模型对于自然语言的智能识别的作用和这两种模型的异同。最后，本文还探讨了Word2vec对于自然语言智能识别的应用。\n关键词：自然语言  智能识别  Word2vec  CBOW  Skip-gram\n1. 引言\n计算机在现代社会各个方面中起着必不可少的作用，以计算机智能识别应用技术为核心的自然语言智能识别更是飞速发展，慢慢地走入我们大众的视线[]。智能化的普及是着力于解放人工传统的工作方式，提高效率，甚至达成传统人工所不能完成的工作。人们所处的时代每天产生海量的数据。如何高效的利用这些数据就依赖智能化的工作方式。\n本文主要探究了自然语言的智能识别，如何让计算机能够正确的识别和处理自然语言，消除语言歧义，致力于代替人工的工作模式是自然语言智能识别的一大核心研究方向,而且自然语言的智能识别从基于规则的转变到了基于统计的转变[]。本文探究了自然语言智能识别的发展历程和重点介绍了Google公司2003年推出的Word2vec[]的基本原理和实际应用。其中又主要介绍了Word2vec的CBOW和Skip-gram模型的基本原理和实现方式。最后总结了自然语言智能识别的重要性和发展前景。\n2. 自然语言智能识别的发展\n2.1统计语言模型\n2.1.1语言句子的二元模型\n对于计算机来说，怎样判断一个句子是否合理是自然语言智能识别的重点。对于计算机，需要建立相应的语言模型才能很好的识别自然语言。统计语言模型是一个概率分布，就是已知一个词出现的条件下前后位置出现的词的概率，公式：\nP(w1,w2,...,wn)=P(w1)P(w2|w1)P(w3|w1,w2)...P(wn|w1,w2,...,wn−1)[]\n其中wi是一句话的第i个词，P表示条件概率。\n后来俄国科学家马尔科夫假设该词出现的概率只与前面出现的词相关，这就是二元模型，于是公式变成：\nP(w1,w2,...,wn)=P(w1)P(w2|w1)P(w3|w2)...P(wn|wn−1)\n利用贝叶斯公式最后得出计算单词出现合理的概率公式：\nP(wi−1,wi)=N(wi−1,wi)/N(wi−1)\n2.1.2N-gram模型\n基于上面的二元模型，假定文本中的每一个词wiwi和前面的N-1个词有关，而与更前面的词无关，这样当前词wiwi的概率只取决于前面N-1个词P(wi−N+1,wi−N+2,...,wi−1)P(wi−N+1,wi−N+2,...,wi−1)，因此：\nP(wi|w1,w2,...,wi−1)\n=P(wi|wi−N+1,wi−N+2,...,wi−1)P(wi|w1,w2,...,wi−1)\n=P(wi|wi−N+1,wi−N+2,...,wi−1)\n\n上式对应的就是N元模型[]（N-Gram Model）不过这样做的弊端就是当N\u003e3时可能词的相关性就不是那么大了，计算结果准确性差.\n2.2词向量\n2.2.1词向量简介\n词向量就是给每个单词用纬度的方式去描述[]，把词语抽象出来交给计算机处理的数据结构[]，每个词都有自己的定位空间和大小，可以进行比较和加减获得词语相似度[]。\n传统的单词特征的抽取把单词作为一个不可以再分基本组成部分，认为不相同的符号之间没有任何的关系，比如：“计算机”和“电脑”被认为两个没有关系的字符串，但是这样显然不符合我们人类的思维。由此而生，人们提出了空间词向量的方法来解决这个问题，空间词向量可以从大量的数据中统计和训练得到[]，1954年Harris提出分布式表示的假说，利用词语的上下文相似假说该词相似，上下文相似的词向量来自于在大量的语料中利用神经网络模型[]训练得来，使用最广泛的是Word2vec使用的连续词袋模型（CBOW (Continuous Bag-Of-Words)）和Skip-gram模型。其他公开的模型还有HLBL、SENNA、Turian’s、Huang’s、Glove等。Word2vec在语义上有较好的联系性。所以Word2vec的大部分测试指标较为准确。\n2.2.2 One hot representation矩阵\nOne hot representation矩阵就是每一个词在一个N维的数组中有唯一的标识，每一个单词有唯一的id，其余纬度标识成0.\n例如：\n苹果 [0，0，0，1，0，0，0，0，0，……]\n那么一句话就是由多维的数组组成，便构成了矩阵，这么做的好处是处理维度越高的自然语言可以用线性方法来表示和计算，但是同时也会造成纬度灾难和语义关联性差的不良后果，由于数据稀疏，可能出现语义关联度为0的情况。\n2.2.3 Distributed representation\n为了解决One hot representation的问题，采用分布式的表示方法就会更好。分布式表示就是利用多维度来表示词的属性。\n例如：\n苹果 [2，0，7，1，0，0，8，0，0，……]\n这样的表达方式可以大大减少词向量的维度，解决了Distributed representation语义关联为0的问题，并且更好的，多方面的描述了词向量的特征。但是同时也没有从根本上解决维度灾难的问题和数据稀疏的问题。\n2.3神经网络语言模型\n语言模型是计算词语在语句中出现的概率来统计自然语言的正确性，是NLP研究的基础方向。神经网络语言模型的发展经过了很长的时间，期间取得了很大的研究成果。开始由Bengio等人提出NNLM（Neutwork language model）最为被人们认可。这个模型为之后的模型研究打下了坚实的基础。其模型见图一：\n图一 神经网络语言模型\nNNLM根据语料库C生成对应的词汇表V，V中的每一个词汇对应一个编号i，将语料库作为样本输入[]。\n经过不断的模型优化，出现了现在人们应用最为广泛的，相对于NNLM简单的连续词袋模型CBOW （Continuous Bag-Of-Words）和Skip-gram模型。在训练方法上，也出现了Hierarchical Softmax[]等优秀的算法。本文主要介绍的是Word2vec所使用的连续词袋模型CBOW （Continuous Bag-Of-Words）和Skip-gram模型。\n3. Word2vec的核心架构模型\n3.1Word2vec简介\n2013年，Google开源了一款用于词向量计算的工具--word2vec,引起了业界的极大关注。他的强大在于可以在百万级的词典和上亿的数据上快速的进行训练。最终得到词向量，并且能很好地体现出词语之间的相关性和相似性。Google开源的Word2vec使用的就是现在最有权威的连续词袋模型CBOW （Continuous Bag-Of-Words）和Skip-gram模型。Word2vec是一款软件，不是一种训练算法。这款强大的软件的开源对于自然语言的智能研究起着很大的作用。也为我们省去了搭建模型的时间。目前也有很多的编程语言API实现Word2vec的功能。比如：python和java中都可以调用相应的包来实现文本的模型训练和词向量的统计。Word2vec的训练框架[]为如图二：\n图二 Word2vec的训练框架\n3.2深度神经网络模型\n深度神经网络模型是一个具有多层感知机的模型，在未来，出现上千层的深度神经网络模型也不奇怪。深度神经网络模型在自然语言的处理中取得了很大的成效。深度神经网络模型在词性识别，命名识别和语快的识别中应用广泛。深度神经网络模型如图一所示：\n图三 深度神经网络模型\n图一中的深度神经网络模型由输出层，隐藏层和输入层组成。输入层是最底层，对应上下文的特征结合状态转移概率求得句子的最佳标注序列[]。\n3.3CBOW模型\nCBOW模型的核心思想就是上下文来确定该词的出现概率来匹配词语的相似度，可以设定上下文影响的距离度，上下文对于该词的影响力度是一样的，之所以叫词袋模型，就是从袋子中取出词语，取出的顺序对于结果没有影响，但是要求就是要有足够的词语来训练，在大量的训练资料中的出我们所需要的特殊实例。这也是深度学习的方向，不再要求具体的特定模型，而是统计模型[]。CBOW模型主要包含了输入层，投影层，输出层，当前词语，词语上下文，上下文累加和。从输入层输入已经数据化的词向量，在投影层对这些向量求累加和，输出层对应一个二叉树，各词出现次数的当权值构造的赫夫曼树[]，叶子节点对应词典中的词。从而构成模型。CBOW的模型复杂度为：\nQ=N*D+D*|V|\n其中N为输入层的窗口长度，D为发射层纬度，|V|为训练语料的词典大小（不同的词语个数）。复杂度就是简单的矩阵计算，主要的计算量为D*|V|[]。CBOW的模型示意图如图二：\n图四 CBOW模型示意图\n其中输入层就是从词袋中取出的词语，W(i)中的i表示取出的词语相对于目标词语的位置。\n3.4 Skip-gram模型\nSkip-gram模型的提出很好的解决了语料经过模型训练后怎样体现出语义的问题，在统计语言模型的应用中，大量的语料是确定语义相似性的基本保证，语言的重复次数决定了语义模仿的程度，模型的模仿程度才会更高。其次，处理方式也是影响最终模型的学习度的一大重要因素，如果断章取义，那最后结果可能会和我们的理解偏差很大。而Skip-gram模型的提出很好的解决这些问题。Skip-gram模型是允许跳过几个词来确定上下文的对应关系，中间跳过的词数就在一定程度上解决了断章取义的问题。因为Skip-gram模型识别的不全是相邻词的出现概率，更多是附近的词出现的概率。Skip-Gram模型的图与CBOW相反，CBOW是从原始语句推测目标字词，Skip-Gram是从目标字词推测原始语句，CBOW适合小型数据库，而Skip-Gram在大型数据中更加合适。N-gram中N=2和N=3的实例如图四[]：\n图五 N-gram中N=2和N=3实例\n最终效果是“中国足球踢得太烂了”，可以看到这种表达方式是能够体现出这个语义的，同时也可以看出N的取值不同，效果也会有差异。一般N\u003e3以上效果会有偏差，所以一般N取值不大于3的。Skip-gram模型如图五：\n图六 Skip-gram模型\n4. Word2vec在语言智能识别的应用\n4.1Word2vec的应用领域\n自然语言处理技术在语音智能上的应用主要包括同声传译、智能机械的聊天以及特定人群的辅助系统等方面。特别是同声传译涉及到包括语音建模、合理的语义转换及语言的精确翻译等，是自然语言处理技术应用的直接领悟，同时同声传译中的语言翻译还要求语音和语义的转化，即音似字如何从特定的语言情景中译出，这种情况通过自然语言处理技术对其进行算法验证和语义的情景化处理，从而提高语言语义的转换质量; 智能机械的聊天，如机器人的聊天系统，也是语音智能的一个应用分支，它主要处理的是比较广泛的如自动回答互动对话系统，在这样的一个语音智能中，机器人聊 天不可避免地要具有语音和文本的转化以及逻辑规范性的自动答话，通过对智能识别自然语言处理技术的应用，做到在语义上的理解、逻辑的正确推断和具体知识的应用等得到质的提高，进而将应答 互动和聊天的准确性提高到一个较被普遍认可的程度，增强机器人之类的智能机器系统在实际中的应用; 而自然语言处理技术在自动场景和特定人群提供辅助方面的应用，将为一些特定的需要帮助的群体———如盲人提供便利的帮助，帮助他们在生活中解决一些诸如交通出行的智能提示方面的困难，对他们来说将是非常必要的。\n所以，基于智能识别的自然语言处理技术对于实际的语音智能方面的应用，将会帮助社会生活的方方面面、各个领域的群体，它的价值在不远的将来，是不可估量的，因为智能识别的自然语言处理技术，是一个多学科交叉、多技术应用、多领域合作的一个技术整合[]。\n4.1Word2vec工作目录\nGoogle官方推出的软件包工作目录如图七：\n图七 Word2vec的工作目录\n其中，demo-word.sh是词库训练模型的各项参数，text8是自动采用的训练样本，可自行设定自己的训练集。Distance.c可以计算词向量的余弦值。在Linux的系统下可以在命令提示符接运行demo-word.sh进行使用。Window系统需要安装Linux的虚拟环境。\n4.2Word2vec的训练参数\nWord2vec的训练参数表[]如图八：\n图八 Word2vec的训练参数\n4.3Google Word2vec软件包的使用\n（1）去Google官网或者相关平台上下载Word2vec的文件夹包。\n图九 word2vec包\n（2）打开Linux运行环境。\n（3）切换到trunk目录下执行make命令。\n图十 切换到trunk工作目录\n（4）执行demo-word.sh并下载再带的text8训练集或者自己的分词训练集。\n图十一 下载自带的text8训练集\n（5）加载完毕输入单词。\n（6）得到输出结果。\n图十二 Word2vec输出结果\n5. 结语\n相比于上个世纪的两次工业革命，计算机智能时代来的更为快速，对我们的生活也起到了很大的提高作用，在这个充满智能化的时代，自然语言的智能识别是尤为重要的研究方向，也是我们需要全力突破的瓶颈，Google推出的Word2vec是代表现在最为先进的自然语言处理技术，实用性比较大，Word2vec应用了CBOW和Skip-gram模型，结合使用了hierarchy softmax和negative sampling等优化技术，在自然语言智能化表达方面起了非常大的推动作用。在未来的自然语言的智能识别还会有长足的进步，同时也是值得研究的方向和值得涉足的领域。\n参考文献：\n[] 李翠霞.现代计算机智能识别技术处理自然语言研究的应用与进展[A].中国知网.2012\n[] 周练.Word2vec的工作原理及应用探究[A].中国知网.2015.吴军.数学之美[M].北京：人民邮电出版社.2012\n[] Tomas Mikolov.Word2vec project [EB/OL].[2014-09-18].https://code.google.com/p/ word2vec/.\n[]维基百科.语言模型 [OL].[2013-3-12]\n[] 一夜了.自然语言处理-统计语言模型（数学之美）.CSDN博客.2017\n[] Huang Xian-ying, Chen Hong-yang, Liu Ying-tao, et al.A novel feature word selecting method of micro-blog short text[J].Computer Engineering\u0026Science, 2015, 37 (9) :1761-1767. (in Chinese)\n[] 涂楚成.基于CUDA的Word2Vec设计与实现[D]西安：西安电子科技大学，2015.12.29\n[] 刘培磊,唐晋韬,王挺,谢松县,岳大鹏,刘海池.基于词向量语义聚类的微博热点挖掘方法[J].计算机工程与科学,2018,40(02):313-319.\n[] Mikolov T, Yih W, Zweig G.Linguistic regularities in continuous space word representations[C]∥Proc of NAACLHLT’13, 2013:746-751.\n[] 路佳佳.神经网络语言模型[D]山西：山西大学，2017.2.9\n[] 周练.Word2vec的工作原理及应用探究[A].中国知网.2015.吴军.数学之美[M].北京：人民邮电出版社.2012\n[] Frederic M.Yoshua B.Hierarchical probabilistic neural network language model[C]//Proceedings of the International Work-shop on Artificial Intelligence and Statistics.Barbados:MIT Press.2005:246-252\n[] 周练.Word2vec的工作原理及应用探究[A].中国知网.2015.吴军.数学之美[M].北京：人民邮电出版社.2012\n[] 游飞,张激,邱定,于铭华.基于深度神经网络的武器名称识别[J].计算机系统应用,2018,27(01):239-243.\n[] 熊富林，邓怡豪，唐晓晟.Word2vec的核心架构及其应用.中国知网.2015\n[] 许莹.赫夫曼树遍历算法的优化[D]安徽：安徽电子信息职业技术学院，2009\n[] 熊富林，邓怡豪，唐晓晟.Word2vec的核心架构及其应用.中国知网.2015\n[] 熊富林，邓怡豪，唐晓晟.Word2vec的核心架构及其应用.中国知网.2015\n[] 李翠霞.现代计算机智能识别技术处理自然语言研究的应用与进展[J].科学技术与工程,2012,12(36):9912-9918.\n[] 周练.Word2vec的工作原理及应用探究[A].中国知网.2015.吴军.数学之美[M].北京：人民邮电出版社.2012","data":"2018年05月18日 10:25:42"}
{"_id":{"$oid":"5d343b1262f717dc0659b321"},"title":"自然语言处理基础技术工具篇之Stanfordcorenlp","author":"yuquanle","content":"更多实时更新的个人学习笔记分享，请关注：\n知乎：https://www.zhihu.com/people/yuquanle/columns\n微信订阅号：人工智能小白入门学习\nID: StudyForAI\nStanfordcorenlp简介\nStanford CoreNLP提供了一套人类语言技术工具。 支持多种自然语言处理基本功能，Stanfordcorenlp是它的一个python接口。\n官网地址：https://stanfordnlp.github.io/CoreNLP/\nGithub地址：https://github.com/stanfordnlp/CoreNLP\nStanfordcorenlp主要功能包括分词、词性标注、命名实体识别、句法结构分析和依存分析等等。\nStanfordcorenlp工具Demo\n安装：pip install stanfordcorenlp\n先下载模型，下载地址：https://nlp.stanford.edu/software/corenlp-backup-download.html\n支持多种语言，这里记录一下中英文使用方法\nfrom stanfordcorenlp import StanfordCoreNLP zh_model = StanfordCoreNLP(r'stanford-corenlp-full-2018-02-27', lang='zh') en_model = StanfordCoreNLP(r'stanford-corenlp-full-2018-02-27', lang='en')\nzh_sentence = '我爱自然语言处理技术！' en_sentence = 'I love natural language processing technology!'\n1.分词(Tokenize)\nprint ('Tokenize:', zh_model.word_tokenize(zh_sentence)) print ('Tokenize:', en_model.word_tokenize(en_sentence))\nTokenize: ['我爱', '自然', '语言', '处理', '技术', '！'] Tokenize: ['I', 'love', 'natural', 'language', 'processing', 'technology', '!']\n2.词性标注(Part of Speech)\nprint ('Part of Speech:', zh_model.pos_tag(zh_sentence)) print ('Part of Speech:', en_model.pos_tag(en_sentence))\nPart of Speech: [('我爱', 'NN'), ('自然', 'AD'), ('语言', 'NN'), ('处理', 'VV'), ('技术', 'NN'), ('！', 'PU')] Part of Speech: [('I', 'PRP'), ('love', 'VBP'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('technology', 'NN'), ('!', '.')]\n3.命名实体识别(Named Entity)\nprint ('Named Entities:', zh_model.ner(zh_sentence)) print ('Named Entities:', en_model.ner(en_sentence))\nNamed Entities: [('我爱', 'O'), ('自然', 'O'), ('语言', 'O'), ('处理', 'O'), ('技术', 'O'), ('！', 'O')] Named Entities: [('I', 'O'), ('love', 'O'), ('natural', 'O'), ('language', 'O'), ('processing', 'O'), ('technology', 'O'), ('!', 'O')]\n4.句法成分分析(Constituency Parse)\nprint ('Constituency Parsing:', zh_model.parse(zh_sentence) + \"\\n\") print ('Constituency Parsing:', en_model.parse(en_sentence))\nConstituency Parsing: (ROOT (IP (IP (NP (NN 我爱)) (ADVP (AD 自然)) (NP (NN 语言)) (VP (VV 处理) (NP (NN 技术)))) (PU ！))) Constituency Parsing: (ROOT (S (NP (PRP I)) (VP (VBP love) (NP (JJ natural) (NN language) (NN processing) (NN technology))) (. !)))\n5.依存句法分析(Dependency Parse)\nprint ('Dependency:', zh_model.dependency_parse(zh_sentence)) print ('Dependency:', en_model.dependency_parse(en_sentence))\nDependency: [('ROOT', 0, 4), ('nsubj', 4, 1), ('advmod', 4, 2), ('nsubj', 4, 3), ('dobj', 4, 5), ('punct', 4, 6)] Dependency: [('ROOT', 0, 2), ('nsubj', 2, 1), ('amod', 6, 3), ('compound', 6, 4), ('compound', 6, 5), ('dobj', 2, 6), ('punct', 2, 7)]","data":"2019年01月05日 20:46:11"}
{"_id":{"$oid":"5d343b1362f717dc0659b323"},"title":"获7000万元融资之后，一知智能要把自然语言处理技术吃透","author":"机器之心V","content":"日前，杭州一知智能科技有限公司宣布在 2018 年 7 月完成 A 轮融资，融资金额 7000 万元人民币。本轮融资由启赋资本领投、金沙江联合资本等机构跟投。资金主要用于进一步加强人工智能 NLP 人才引进和核心技术科研投入，并推出基于 NLP 技术的智能外呼机器人。\n此次融资是继 2017 年 9 月获金沙江天使投资 2000 万元后再获投资。公司累计已获近 1 亿元投资，成为近两年国内 NLP 领域获最多融资的初创企业之一。\n据悉，杭州一知智能科技成立于 2017 年 8 月，是一家专注于 NLP（Natural Language Processing/自然语言处理）的技术型人工智能公司，主攻中文自然语义理解与人机交互的前沿底层技术研发，包括阅读理解、语义识别、多轮对话、精准问答、知识图谱等综合 NLP 技术，在业界处于领先水平，并被机器之心评选为「2017 十大最具潜力早期 AI 公司」。\n公司核心创始团队及研发人员来自香港科技大学、浙江大学、清华大学、卡耐基梅隆大学、南洋理工大学、中科院等顶尖院校，在 AAAI、IJCAI、TKDE、ACL、KDD 等国际顶级会议及期刊发表数十篇论文，横跨机器学习、深度学习、自然语言处理、语义识别、知识图谱、语义纠错、语音合成等。\n团队在 2017 世界顶级机器阅读理解大赛 SQuAD 获世界排名第二的优异成绩。SQuAD 被誉为机器阅读理解界的 ImageNet，该挑战赛不仅有微软、Google、Facebook、腾讯、阿里巴巴、IBM、科大讯飞等科技公司参与，还有斯坦福、哈佛、卡内基梅隆、新加坡国立、清华大学、北京大学、哈工大等国内外知名大学和科研院所参与其中。\n在一知智能创始人、首席科学家赵洲博士看来，NLP 技术是人工智能从计算智能、感知智能走向认知智能的关键领域，也是中美等大国在 AI 前沿科技竞争的焦点。\n关于公司的发展定位，赵洲博士希望一知智能作为 NLP 领域前沿科研高地，持续引进 NLP 尖端人才，科研成果走在业界领先，同时加快 NLP 技术应用落地，在金融、政务、教育等领域打造通用型产品。当前，赵洲及其带领的博士团队与微软、腾讯、阿里、网易等均有项目合作。\n过去五年，人工智能企业大量涌现，出现了讯飞、商汤、旷视、寒武纪等独角兽科技企业。然而，在认知智能领域，由于理论和底层技术局限，仍处于探索初期。\nNLP 是人工智能「皇冠上的明珠」，是一个极其复杂的研究领域。全球来看, 美国从 2016 年起在 NLP 领域投资案例大幅增多，国内仍少见。据 Tractica 报告预估，到 2025 年，全球 NLP 领域的软硬件市场规模将达 223 亿美金，服务市场规模有望突破 1000 亿美金。\n启赋资本投资总监刘永佳认为，「NLP 实际应用的最大困难，来自语义识别的复杂性，中文语义尤为困难。但 NLP 技术价值巨大，长期来看 NLP 会推动人机交互和非结构化信息应用的巨量蓝海市场，在金融、商业、教育等行业有很大应用空间。机器阅读、语义分析、多轮对话、知识图谱、认知推理等多项 NLP 技术综合突破，应用产品才能成熟。」\n同时，刘永佳表示，本轮投资看好一知智能主要基于三方面考虑。一是 NLP 当前最好投顶尖科技人才驱动的团队，赵洲博士团队拥有极强的创新基因，能从根本上实现 NLP 底层理论和前沿技术创造；二是公司虽在初创期，但其团队综合实力强，由一群年轻的技术天才和工程产品团队构成，平均年龄不到 30 岁，企业创新文化类似 Google，能夜以继日推进底层技术和产品研发；三是他们以市场和客户为驱动，高度重视产品化和商业落地，非常务实，公司人才引进和产品研发速度很快。\n展望人工智能未来发展，金沙江联合资本董事总经理王国成表示，「AI 科技内涵和外延很大，未来长达几十年发展期，AI 在认知领域才刚起步。一知智能拥有一支年轻的科研团队，工程能力持续加强，公司会高速成长。」\n在启赋资本董事长傅哲宽看来，「NLP 是人工智能领域继语音、计算机视觉、芯片之后，最有机会诞生一批优秀科技企业的领域。希望一知智能把握先机，加大人才引进力度，成为人工智能 NLP 领域国际前沿的科研阵地和人才集聚地，早日成长为 NLP 领域独角兽企业。」\n产业融资NLP创业公司\n1","data":"2018年07月16日 00:00:00"}
{"_id":{"$oid":"5d343b1462f717dc0659b325"},"title":"自然语言处理 数据集","author":"守望者白狼","content":"自然语言处理\nYelp Open Dataset：Yelp 数据集是用于 NLP 的 Yelp 业务、评论和用户数据的子集。\nNLP Chinese Corpus：大规模中文自然语言处理语料\n腾讯中文词NLP数据集：该数据包含800多万中文词汇，其中每个词对应一个200维的向量。相比现有的公开数据集，在覆盖率、新鲜度及准确性上大幅提高。在对话回复质量预测、医疗实体识别等自然语言处理方向的业务应用方面，腾讯内部效果提升显著。\nNarrativeQA：DeepMind机器阅读理解数据集，是第一个基于整本书或整个剧本的大规模问答数据集。数据集中该有的所有文档\n非正式汉语数据集：收集了3700万条图书评论和5万条bbs回帖，作为大型非正式汉语数据集（LSICC）。内容来源分别是“豆瓣读书”和Chiphell论坛。豆瓣读书评论：Chiphell回帖：\nSQuAD：一个最新的阅读理解数据集。该数据集包含 10 万个（问题，原文，答案）三元组，原文来自于 536 篇维基百科文章。\n安然数据集：安然集团高级管理层的电子邮件数据。\nGoogle Books Ngram：来自Google书籍的词汇集合。\n博客语料库：从blogger.com收集的681，288篇博客文章。每个博客至少包含200个常用的英语单词。\n维基百科链接数据（Wikipedia Links data）：维基百科全文。该数据集包含来自400多万篇文章，近19亿字。你可以对字、短语或段落本身的一部分进行搜索。\nGutenberg电子图书列表：Project Gutenberg的附加注释的电子书列表。\nHansards加拿大议会的文本块（Hansards text chunks of Canadian Parliament）：来自第36届加拿大议会记录的130万对文本。\n危险边缘 (Jeopardy)：来自问答游戏节目《危险边缘》(Jeopardy) 的超过 20 万个问题的存档。\n英文SMS垃圾邮件收集（SMS Spam Collection in English）：包含5，574条英文垃圾邮件的数据集。\nYelp评论（Yelp Reviews）：Yelp发布的一个开放数据集，包含超过500万次评论。\nUCI的垃圾邮件库（UCI’s Spambase）：一个大型垃圾邮件数据集，用于垃圾邮件过滤。\n亚马逊评论：包含18年来亚马逊上的大约3500万条评论，数据包括产品和用户信息，评级和文本审核。\n问答\nTopical Chat数据集：亚马逊将公布超过最大会话和知识数据集，超410万单词21万句子的语料库将于2019年9月17日发布。主题聊天数据集将包含超过210,000个句子（超过4,100,000个单词），可支持高质量，可重复的研究，将成为研究界公开可用的最大社交对话和知识数据集\n数学题海数据集：DeepMind 发布，包含大量不同类型的数学问题（练习题级别），旨在考察模型的数学学习和代数推理能力。包含 200 万（问题答案）对和 10000 个预生成测试样本，问题的长度限制为 160 字符，答案的长度限制为 30 字符。每个问题类型中的训练数据被分为「容易训练」、「中等训练难度」和「较难训练」三个级别。\nGQA图像场景图问答数据集：斯坦福大学教授 Christopher Manning 及其学生 Drew Hudson 一同打造的，旨在推动场景理解与视觉问答研究领域的进步。包含高达 20M 的各种日常生活图像，主要源自于 COCO 和 Flickr。每张图像都与图中的物体、属性与关系的场景图（scene graph）相关，创建上基于最新清洁版本的 Visual Genome。此外，每个问题都与其语义的结构化表示相关联，功能程序上指定必须采取一定的推理步骤才能进行回答。\nNatural Questions数据集：Google发布一个新的大规模训练和评估开放领域超难问答数据集「自然问题」，能够训练AI阅读维基百科，并找到各种开放领域问题的答案。1、超过30万组问答，其中训练集有307,372组问答，包含152,148组长答案问答和110,724组短答案问答；2、开发示例问答，包含有7830组“一问五答”的问答，也就是同一个问题，找五个人分别从维基百科中寻找答案，以此来衡量QA问答系统的表现；3、测试集有7842组问答。\nGQA图像场景图问答数据集 ：GQA 是斯坦福大学教授 Christopher Manning 及其学生 Drew Hudson 一同打造的全新图像场景图问答数据集，旨在推动场景理解与视觉问答研究领域的进步。该数据集包含高达 20M 的各种日常生活图像，主要源自于 COCO 和 Flickr。每张图像都与图中的物体、属性与关系的场景图（scene graph）相关，创建上基于最新清洁版本的 Visual Genome。此外，每个问题都与其语义的结构化表示相关联，功能程序上指定必须采取一定的推理步骤才能进行回答。\nNLPCC2016KBQA数据集：基于知识图谱的问答系统，其包含 14,609 个问答对的训练集和包含 9870 个问答对的测试集。并提供一个知识库，包含 6,502,738 个实体、 587,875 个属性以及 43,063,796 个三元组。知识库文件中每行存储一个事实（fact），即三元组 ( 实体、属性、属性值) 。原数据中本只有问答对（question-answer），并无标注三元组（triple)，本人所用问答对数据来自该比赛第一名的预处理。\nHotpotQA：面向自然语言和多步推理问题，新型问答数据集，具有自然、多跳问题的问答数据集，具有支持事实的强大监督，以实现更易于解释的问答系统。\nCoQA：斯坦福最新问答数据集，囊括来自 7 个不同领域的文本段落里 8000 个对话中的 127,000 轮问答。\n推荐系统\nyf_amazon 数据集：52 万件商品，1100 多个类目，142 万用户，720 万条评论/评分数据\nyf_dianping 数据集：24 万家餐馆，54 万用户，440 万条评论/评分数据\ndmsc_v2 数据集：28 部电影，超 70 万 用户，超 200 万条 评分/评论 数据\nez_douban 数据集：5 万多部电影（3 万多有电影名称，2 万多没有电影名称），2.8 万 用户，280 万条评分数据\n亚马逊评论：3500万条来自亚马逊的评论，时间长度为18年。数据包括产品和用户信息、评级等。\n情感/观点/评论 倾向性分析\nyf_amazon 数据集：52 万件商品，1100 多个类目，142 万用户，720 万条评论/评分数据\nyf_dianping 数据集：24 万家餐馆，54 万用户，440 万条评论/评分数据\ndmsc_v2 数据集：28 部电影，超 70 万 用户，超 200 万条 评分/评论 数据\nsimplifyweibo_4_moods 数据集：36 万多条，带情感标注 新浪微博，包含 4 种情感， 其中喜悦约 20 万条，愤怒、厌恶、低落各约 5 万条\nweibo_senti_100k 数据集：10 万多条，带情感标注 新浪微博，正负向评论约各 5 万条\nonline_shopping_10_cats 数据集：10 个类别，共 6 万多条评论数据，正、负向评论各约 3 万条， 包括书籍、平板、手机、水果、洗发水、热水器、蒙牛、衣服、计算机、酒店\nwaimai_10k 数据集：某外卖平台收集的用户评价，正向 4000 条，负向 约 8000 条\nChnSentiCorp_htl_all 数据集：7000 多条酒店评论数据，5000 多条正向评论，2000 多条负向评论\n多域情感分析数据集（Multidomain sentiment analysis dataset）：一个比较有历史的数据集，里面还有一些来自亚马逊的产品评论。\nIMDB评论： 影评，也是比较有历史的二元情绪分类数据集、数据规模相对较小，里面有 25,000 条电影评论。\n斯坦福情感树银行（Stanford Sentiment Treebank）：带有情感注释的标准情绪数据集。\nSentiment140：一个流行的数据集，它使用16万条推文，并把表情等等符号剔除了。\nTwitter 美国航空公司情绪数据集 (Twitter US Airline Sentiment)：自 2015 年 2 月以来美国航空公司的 Twitter 数据，分类为正面、负面和中性推文。\n中文命名实体识别\ndh_msra 数据集：5 万多条中文命名实体识别标注数据（包括地点、机构、人物）","data":"2019年01月13日 15:47:06"}
{"_id":{"$oid":"5d343b1462f717dc0659b328"},"title":"零基础入门自然语言处理的学习建议","author":"胖胖的飞象","content":"在入门的阶段最适合做的事情：\n（1）阅读和学习自然语言处理（natural language processing，nlp）综述类文章和图书，对nlp有一个基本的认识，梳理nlp研究内容的演变，包括nlp从诞生到多次繁荣发展和多次停滞不前的原因，正确认识nlp与人工智能、机器学习、自然语言理解、计算语言学、文本挖掘等概念之间的区别与联系（除了我下面推荐的图书、文章和代码，一定要多百度和google寻找学习资料）；\n（2）做一些非常简单的nlp入门小任务，通过小任务理解自然语言处理的流程（包括中英文nlp处理流程的差别）；\n（3）快速学习一门开发nlp技术的编程语言（基本就是python了，不需要学习网络编程等部分，重点掌握python基本语法、文件读写与编码、正则表达式、gensim、numpy、pandas、matplotlib等的使用）；\n（4）学习机器学习的相关概念，如：模型评估与选择、有监督学习、半监督学习、无监督学习、强化学习、迁移学习（只需要学习和认识其原理，做到心中有数即可）；\n（5）关注各大内容大V，如：微信公众号（paperweekly、新智元、AI科技大本营、机器之心、人工智能头条等），知乎（一搜自然语言处理或者nlp就能看到好多大V），博主（我爱自然语言处理等），因为每天各大V都会发布很多的新闻和内容，初入门的小白不建议全篇深入阅读，感兴趣的可以收藏起来以后看，大部分就了解一下（1）“谁”（2）“干了啥，咋干的”（3）“啥效果”，然后读了这个文章的报道（4）“自己有啥想法”，就可以了（注：有一个小细节上的建议，就是在平日里有任何idea最好都要在手机或者任何地方的备忘录里记一下，也要尽快多实践以验证idea是否有效）；\n（6）了解国内外nlp的协会组织（如：中文信息学会、中文信息学会青年工作委员会、ACL等，主要查找和阅读协会开辟的专栏、组织撰写的综述、看看最近都组织了什么会议和比赛等）、主要的大型nlp科研团队（斯坦福、多伦多、清华、北大、哈工大、复旦、中科、deepmind、google brain、openAI等高校、院所和科研机构的自然语言处理小组，看看他们都在研究什么）；\n（7）关注“中国中文信息学会”的微信公众号，时刻关注ACL、IJCAI、ACML、SIGIR等顶会论文报告会（由中文信息学会组织，报告的人都是当年被顶会录取的论文的国内作者和大佬，来自各大高校和院所，整个报告会和国际会议的日程都是一样的，都有coffee break可以在茶歇的时候近距离与大佬交流，是一个近距离与国内各大NLP大组老师、同学交流的机会，看看大厂们都在干什么，推荐有机会的话一定要去听一下报告，但一定要保证已经看过很多论文、并且有一定基础再去听，完全零基础不建议去，会议比较火爆和受欢迎，建议时刻关注尽快报名）。\nnlp非常容易入门的原因是这是一门非常开放的学科，各大高校、学者都有一颗开放的心，源码经常开源、而且有很多优秀的老师有写博客、博文的习惯，大家都喜欢一有研究成果就立马放到arxiv或者researchgate上，nlp的研究日新月异、变化的非常快的原因就是“开放”，所以有什么问题尽量面向百度、google查询，注意关键词（毕竟是搞nlp的，可以先学习一下检索的原理，然后就知道怎么检索能尽快找到你想要的的东西），检索的结果重点关注reddit、medium、csdn、arxiv、researchgate、知乎、stackoverflow、github等上面的内容。\n可参考学习和实践的链接如下：\n复旦大学邱锡鹏组实验室新生一般完成的五个NLP练习上手实验（NLP四大类任务：分类、序列标注、文本匹配、文本生成，都需要完整实现一遍）。\nhttps://www.zhihu.com/question/324189960/answer/682130580?utm_source=wechat_session\u0026utm_medium=social\u0026utm_oi=952466020582064128\n自动化所宗成庆研究员：读懂NLP的过去与现在（梳理的非常好，把各个概念之间的关系和NLP的发展都梳理清楚了）\nhttps://mp.weixin.qq.com/s/xgySwq2m-mHT7XG1zZGpzw\n中文自然语言处理入门实战\nhttps://mp.weixin.qq.com/s/5z7Xy4NL-buUkpBmv4iIpw\n自然语言处理全家福：纵览当前NLP中的任务、数据、模型与论文\nhttps://mp.weixin.qq.com/s/sQ903WNSR4v367t78_VG1Q\n中文信息处理发展报告（综述由中文信息学会统筹，国内各大NLP专家撰写，非常适合入门了解NLP）\nhttp://cips-upload.bj.bcebos.com/cips2016.pdf\nJumping NLP Curves: A Review of Natural Language Processing Research [Review Article]\nhttps://ieeexplore.ieee.org/document/6786458\nNatural Language Processing: A Review\nhttps://www.researchgate.net/publication/309210149_Natural_Language_Processing_A_Review\nA Review of the Neural History of Natural Language Processing\nhttp://ruder.io/a-review-of-the-recent-history-of-nlp/\n邓力和刘洋大神合著的图书《Deep Learning in Natural Language Processing》\n（就不给链接了，百度或google搜索，有中文连载、英文原版的原版图书购买、pdf分享或者课程）\n宗成庆研究员所著《统计自然语言处理》（经典之作）\nSteven Bird所著《Python自然语言处理》（快速掌握python开发nlp技术的各种能力）\n机器学习 → 推荐周志华教授所著的《机器学习》\n深度学习 → 推荐Ian Goodfellow等人合著的《Deep Learning》\n开发框架 → 首推Pytorch，推荐陈云的《深度学习框架Pytorch入门与实践》或者廖星宇的《深度学习入门之Pytorch》\nTensorflow学习 → 推荐黄文坚、唐源的《Tensorflow实战》\n（注意有些情况需要“科学上网”，一定要掌握google和google scholar的使用）","data":"2019年06月01日 14:31:53"}
{"_id":{"$oid":"5d343b1562f717dc0659b32a"},"title":"自然语言处理发展缓慢，主要有6条原因","author":"duozhishidai","content":"理解人类语言，在人工智能领域称为自然语言处理，所谓的自然语言处理，就是用计算机处理人类在日常生活串所使用的自然语言的能力。\n\n让机算机理解自然语言是十分艰难的任务，无法理解计算机语言的原因，主要存在语义、语法、语音问题，归纳起来主要有6条原因：\n\n1.句子的正确词序规则和概念，难以理解不含规则的句子。\n\n2.词语的确切含义、形式、词类及构词法。\n\n3.词的语义分类以及词的多义性和岐义性。\n\n4.指定和不定特性及所有隶属特性。\n\n5.问题领域的结构知识和时间概念。\n\n6.有关语言表达形式的文学知识\n\n语言的理解与交流需要一个相当庞大和复杂的知识体系，自然语言理解最大的困难就在于对知识不完整性、不确定性、模糊性的处理，了解了以下学习难点，才可以说是真正的入门。\n如何快速入门NLP自然语言处理概述\nhttp://www.duozhishidai.com/article-11742-1.html\n从语言学到深度学习NLP，一文概述自然语言处理\nhttp://www.duozhishidai.com/article-1120-1.html\n改变世界的七大NLP技术\nhttp://www.duozhishidai.com/article-8918-1.html","data":"2019年01月23日 15:35:16"}
{"_id":{"$oid":"5d343b1562f717dc0659b32e"},"title":"自然语言处理笔记5-哈工大 关毅","author":"Big_quant","content":"目录\n文章目录\n目录\n前言\nn-gram语言模型（一）\nn-gram语言模型（二）\nn-gram语言模型（三）\nn-gram语言模型（四）\nn-gram语言模型（五）\nn-gram语言模型（六）\nn-gram语言模型（七）\n前言\n硕士生涯结束，开始专心做一件自己觉得有用的工具，先做工程，后搞理论。\n自然语言处理是一个非常难的问题，同时是人工智能皇冠上的明珠。\n接下来会记录一系列自然语言处理的笔记，来自于哈工大老师关毅。\nn-gram语言模型（一）\nn元词序列，通分词一元频度，语料库加工，最大熵模型基本信息，噪声信道模型，信源s发出信息，一系列01序列。输入和输出完全匹配一致，信息转变。\nin-\u003eprocess-\u003eout 贝叶斯公式是统计的核心地位，一个声学信号对应于一个语句。\nT=argmax（p（T/A））\n求的是使其概率最大的T。\n语音识别的应用，信源的应用：手写体汉字识别，文字作用信源。\n以概率p输出字符串。\n目标，翻译，输出。\n一段语音文字出现的概率（P（T）），语言模型，完成特定功能的数据结构。实现字符串结构的模型概率，信源字符序列。\n香农游戏给定前n个词，求下一个词。\nn-gram语言模型（二）\n全概率模型0-1规则，力量较强。\n参数统计模型，空间大，稀疏。\n马尔可夫假设：\n下一个词依赖于前一个词：\nP（s|t）=P(S|t-1)\nTrigram模型：\n\nP\n(\nI\n)\nP\n(\nW\n1\n)\nP\n(\nW\n2\n∣\nW\n1\n)\nP(I)~P(W_1)P(W_2|W_1)\nP(I) P(W1 )P(W2 ∣W1 )\n还可以无限延伸变成ngram模型。\n约减参数空间，可靠辨别，一个参数。\n数据平滑技术，只统计他的一元频度\n某语料库词汇分布图，最大相似度估计分布图，期望概率分布图。\nn-gram语言模型（三）\n数据平滑技术，discounting技术，分给小的validation，拉普拉斯定律，加一平滑法。\n大家同加一，解决数据稀疏问题。\n\nP\nl\na\np\n=\nW\n1\nN\n+\nB\nB\n=\n∣\nU\n∣\nn\nP_{lap}=\\frac{W_1}{N+B} B=|U|^n\nPlap =N+BW1 B=∣U∣n\nGood-Turing 估计 ：如果\nC\n(\nW\n1\n,\nW\n2\n,\n.\n.\n.\nW\nN\n)\n=\nr\n\u0026gt;\n0\nC(W_1,W_2,...W_N)=r\u0026gt;0\nC(W1 ,W2 ,...WN )=r\u003e0\n\nP\na\nt\n(\nW\n1\n,\n.\n.\nW\nn\n)\n=\nr\n∗\n/\nN\nPat(W_1,..W_n)=r*/N\nPat(W1 ,..Wn )=r∗/N\n此处R*=（r+1）S(r+1)/s®\n这里S®是Nr的期望平滑估计。Nr=arb，估计整体分布参数估计的一种。\n图灵估计，线性插值平滑。构造高鲁棒性语言模型2规模小，效果显著3规模大效果不显著。\n技术上实行。\nn-gram语言模型（四）\n只依赖前n-1个词的词性，n-pose模型。\n动态估计和静态估计合力解决词汇问题\n统计语言模型的评价方法：实用方法。\n基于交叉熵与迷惑度的方法。\n\nH\n(\nx\n)\n=\n−\n∑\nq\n(\nx\n)\nl\no\ng\np\n(\nx\n)\nq\n(\nx\n)\nH(x)=-\\sum q(x) log \\frac{p(x)}{q(x)}\nH(x)=−∑q(x)logq(x)p(x)\nn-gram语言模型（五）\nargmax（P（T|s））语言模型的实例\n考虑数据的加载与注入，高压缩比数据。\nN-gram 语言模型的构造\nn-gram语言模型（六）\n理解骨架，基本模型，隐马，极大熵。\n生成/条件判别模型。\n最大熵原理是指在一定的限制条件下，尽可能地选择熵最大的概率分布（均匀分布），作为预测结果，而对不知道（限制条件以外）的情形不做任何假设。\n如何设计正负的概率。\n假设在语料库中，有如下词性标记及次数，估计在限定条件下的概率，选择满足限定条件的P。\n使H（p）为最大\n\nH\n(\nx\n)\n=\n−\n∑\nP\n(\nx\n)\nl\no\ng\np\n(\nx\n)\nH(x)=-\\sum P(x)logp(x)\nH(x)=−∑P(x)logp(x)a\u003cA且b\u003cB.\n在最大熵模型中，特征是一个关于事件的二值函数。\n\nf\nj\n:\nx\n−\n》\n0\n,\n1\n,\nx\n=\nA\n∗\nB\nf_j:x-》{0,1},x=A*B\nfj :x−》0,1,x=A∗B,原子级特征。\nn-gram语言模型（七）\n限制条件，模型特征的期望值等于训练语料库中观察到的特征的期望值。\n\nE\np\nf\nj\n=\nE\np\nf\nj\nE_pf_j=Ep^~f_j\nEp fj =Ep fj\n训练语料库非常关键，从训练数据到可观测事件，解的存在且唯一，拉格朗日解。\n最大熵模型的使用方法（rf条件随机域）\n文本数据，数据缺失，HMM/EM。","data":"2019年01月04日 19:25:25"}
{"_id":{"$oid":"5d343b1662f717dc0659b330"},"title":"Python 自然语言处理（基于SnowNLP）","author":"HuangZhang_123","content":"----------欢迎加入学习交流QQ群：657341423\nSnowNLP是一个python写的类库,可以方便的处理中文文本内容。如\n中文分词\n词性标注\n情感分析\n文本分类\n提取文本关键词\n文本相似度计算\n安装：pip install snownlp\n完成snownlp安装后，查看模块的目录结构，如图所示\n\nnormal：文字转换成拼音\nseg：中文分词\nsentiment：情感分析\nsim：文本相似度\nsummary：提取文本摘要\ntag：词性标注\n__init__.py：整个模块的函数方法\n想了解snownlp，可以打开 __init__.py 查看snownlp提供的方法函数\n# -*- coding: utf-8 -*- from __future__ import unicode_literals from . import normal from . import seg from . import tag from . import sentiment from .sim import bm25 from .summary import textrank from .summary import words_merge class SnowNLP(object): def __init__(self, doc): self.doc = doc self.bm25 = bm25.BM25(doc) @property def words(self): return seg.seg(self.doc) @property def sentences(self): return normal.get_sentences(self.doc) @property def han(self): return normal.zh2hans(self.doc) @property def pinyin(self): return normal.get_pinyin(self.doc) @property def sentiments(self): return sentiment.classify(self.doc) @property def tags(self): words = self.words tags = tag.tag(words) return zip(words, tags) @property def tf(self): return self.bm25.f @property def idf(self): return self.bm25.idf def sim(self, doc): return self.bm25.simall(doc) def summary(self, limit=5): doc = [] sents = self.sentences for sent in sents: words = seg.seg(sent) words = normal.filter_stop(words) doc.append(words) rank = textrank.TextRank(doc) rank.solve() ret = [] for index in rank.top_index(limit): ret.append(sents[index]) return ret def keywords(self, limit=5, merge=False): doc = [] sents = self.sentences for sent in sents: words = seg.seg(sent) words = normal.filter_stop(words) doc.append(words) rank = textrank.KeywordTextRank(doc) rank.solve() ret = [] for w in rank.top_index(limit): ret.append(w) if merge: wm = words_merge.SimpleMerge(self.doc, ret) return wm.merge() return ret\n整个snownlp模块就提供这些方法函数给我们使用，具体的使用方式以官方文档为例\nfrom snownlp import SnowNLP s = SnowNLP(u'这个东西真心很赞') # 分词 s.words # [u'这个', u'东西', u'真心', # u'很', u'赞'] # 词语标注 s.tags # [(u'这个', u'r'), (u'东西', u'n'), # (u'真心', u'd'), (u'很', u'd'), # (u'赞', u'Vg')] # 情感分析 s.sentiments # 0.9769663402895832 positive的概率 # 转换拼音 s.pinyin # [u'zhe', u'ge', u'dong', u'xi', # u'zhen', u'xin', u'hen', u'zan'] s = SnowNLP(u'「繁體字」「繁體中文」的叫法在臺灣亦很常見。') # 转换简体 s.han # u'「繁体字」「繁体中文」的叫法 # 在台湾亦很常见。' text = u''' 自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。 它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。 自然语言处理是一门融语言学、计算机科学、数学于一体的科学。 因此，这一领域的研究将涉及自然语言，即人们日常使用的语言， 所以它与语言学的研究有着密切的联系，但又有重要的区别。 自然语言处理并不是一般地研究自然语言， 而在于研制能有效地实现自然语言通信的计算机系统， 特别是其中的软件系统。因而它是计算机科学的一部分。 ''' s = SnowNLP(text) # 提取关键字 s.keywords(3) # [u'语言', u'自然', u'计算机'] # 提取摘要 s.summary(3) # [u'因而它是计算机科学的一部分', # u'自然语言处理是一门融语言学、计算机科学、 # 数学于一体的科学', # u'自然语言处理是计算机科学领域与人工智能 # 领域中的一个重要方向'] # 文本分句处理 temp_list = s.sentences s = SnowNLP([['这篇', '文章'], ['那篇', '论文'], ['这个']]) # TF-IDF算法 s.tf s.idf # 文本相似度。从s对象中找出与sim(['文章'])相似的文本 s.sim(['文章'])# [0.3756070762985226, 0, 0]\n关于训练\n训练是更好地完善现有的语料库，现在提供训练的包括分词，词性标注，情感分析。以分词为例 分词在snownlp/seg目录下\n# 分词训练 from snownlp import seg seg.train('data.txt') seg.save('seg.marshal') # 词性标注训练 # from snownlp import tag # tag.train('199801.txt') # tag.save('tag.marshal') # 情感分析训练 # from snownlp import sentiment # sentiment.train('neg.txt', 'pos.txt') # sentiment.save('sentiment.marshal')\n这样训练好的文件就存储为seg.marshal了，之后修改snownlp/seg/init.py里的data_path指向刚训练好的文件即可","data":"2018年05月11日 16:22:13"}
{"_id":{"$oid":"5d343b1762f717dc0659b332"},"title":"R语言自然语言处理：文本分类","author":"R3eE9y2OeFcU40","content":"欢迎关注天善智能，我们是专注于商业智能BI，人工智能AI，大数据分析与挖掘领域的垂直社区，学习，问答、求职一站式搞定！\n对商业智能BI、大数据分析挖掘、机器学习，python，R等数据领域感兴趣的同学加微信：tstoutiao，邀请你进入数据爱好者交流群，数据爱好者们都在这儿\n作者：黄天元，复旦大学博士在读，目前研究涉及文本挖掘、社交网络分析和机器学习等。希望与大家分享学习经验，推广并加深R语言在业界的应用。\n邮箱：huang.tian-yuan@qq.com\n前文推送：\nR语言自然语言处理：中文分词\nR语言自然语言处理：词性标注与命名实体识别\nR语言自然语言处理：关键词提取（TF-IDF）\nR语言自然语言处理：关键词提取与文本摘要（TextRank）\nR语言自然语言处理：文本向量化——词嵌入（Word Embedding）\nR语言自然语言处理：情感分析\n不知不觉已经写了这么多，但是很多R语言自然语言处理的方法并没有展开来讲。这次希望尝试用简单的技术（TF-IDF和相似度矩阵）做一次实践，即文档分类。\n任务定义：对于任意给定的一个字符串，判断它与目前哪个文档最为相似，从而进行归类。首先要对当前的文档（数据见github.com/hope-data-sc）做词嵌入（就用最简单的TF-IDF模型），然后对于任意的新字符串，进行向量化之后，与先前的标准库做相似性的分析，看看与哪个文档相似性最近，就属于哪一个类别。\n1 读入文件\n1 library(pacman)\n2 p_load(tidyverse,data.table)\n3\n4 fread(\"classification_corpus_raw.csv\",encoding =\"UTF-8\") %\u003e%\n5 as_tibble() %\u003e%\n6 mutate(id=1:n())-\u003e raw\n这样，文件就在raw中了。\n2 计算TF-IDF\n这一部分参考R语言自然语言处理：关键词提取（TF-IDF），先进行分词，然后对所有的词计算TF-IDF。\n1 ## 快速分词\n2 p_load(jiebaR)\n3 worker() -\u003e wk\n4\n5 raw %\u003e%\n6 mutate(words = map(title,segment,jieba = wk)) %\u003e%\n7 select(id,words) -\u003e corpus\n8\n9 ## 计算TF-IDF\n10 corpus %\u003e%\n11 unnest() %\u003e%\n12 count(id,words) %\u003e%\n13 bind_tf_idf(term = words,document =id,n = n) -\u003e corpus_tf_idf\n仔细看，这个文档现在究竟有多少个词语呢？\n1 corpus_tf_idf%\u003e% distinct(words)\n2\n3 # A tibble: 1,510 x 1\n4 words\n5 \u003cchr\u003e\n6 1百年\n7 2办公室\n8 3筹备工作\n9 4校庆\n10 5保卫部\n11 6处\n12 7安全\n13 8管理\n14 9生产\n15 10保密\n16 # ... with 1,500 more rows\n一共1510个，不多，因此我决定不进行筛选了。本来常规套路要把这个TF-IDF的矩阵变为一个文档-词语矩阵（Document Term Matrix,DTM）。但是既然走了tidy的路线，我突然认为那是一个多余的步骤，做了一个高维稀疏的矩阵效率异常低，而进行连接（join）的速度可谓异常地快。\n下面我要写一个函数，它要完成一个这样的任务：对于任意给定的字符串，求这个字符串与当前所有文档的相似性，然后筛选出相似性最高的n个文档，显示出来。\n虽然不需要构造矩阵，但是我还是要构造一个类似的数据框。\n1 corpus_tf_idf%\u003e%\n2 select(id,tf_idf) -\u003e for_future_use\n3 举例尝试\n先假设给定的字符串为“大数据学院”，我们看看是否能够找到合理的相似文档。我们首先要明确，什么叫做相似？定义：1、字符串中包含相同的组分（相同的分词结果）；2、当包含组分数量一致的时候，如果包含重要表征组分，其得分更高（举例说明：我们给定的字符串是“物理学院”，分词之后是“物理”和“学院”，但是“物理”这个词能够表征的程度更高，因此它会得到更高的得分，这个得分在我们的模型中是以TF-IDF的形式存在的）。\n下面我们给出代码：\n1 string=\"大数据学院\"\n2\n3 string%\u003e%\n4 segment(jiebar = wk) %\u003e%\n5 enframe() %\u003e%\n6 transmute(words =value) -\u003e string_table\n7\n8 for_future_use %\u003e%\n9 inner_join(string_table) %\u003e%\n10 group_by(id) %\u003e%\n11 summarise(score = sum(tf_idf)) %\u003e%\n12 arrange(desc(score)) -\u003e sort_table\n13\n14 sort_table %\u003e%\n15 slice(1:5) %\u003e%\n16 inner_join(raw,by=\"id\")\n17\n18 # A tibble: 5 x 3\n19 id score title\n20\n21 1584.70大数据学院\n22 2572.86大数据研究院\n23 31091.84高级律师学院\n24 44361.84公共卫生学院\n25 54791.84管理学院\n我们可以看到，“大数据学院”被正确地筛选出来，而排名第二的是“大数据研究院”，因为“大数据”作为一个比“学院”拥有更高TF-IDF的关键词，更能够表征“大数据”这个特征。其他3个选项得分其实是一样的，它们都因为有“学院”而被筛选出来，但是没有匹配更多更有价值的词语了。现在我们就可以正式对函数进行构造：\n1 get_sim = function(string){\n2 string%\u003e%\n3 segment(jiebar = wk) %\u003e%\n4 enframe() %\u003e%\n5 transmute(words =value) -\u003e string_table\n6\n7 for_future_use %\u003e%\n8 inner_join(string_table,by=\"words\") %\u003e%\n9 group_by(id) %\u003e%\n10 summarise(score = sum(tf_idf)) %\u003e%\n11 arrange(desc(score)) -\u003e sort_table\n12\n13 sort_table %\u003e%\n14 slice(1:3) %\u003e%\n15 inner_join(raw,by=\"id\") -\u003eresult\n16\n17 ifelse(nrow(result)==0,\n18 NA,\n19 result %\u003e%\n20 pull(title) %\u003e%\n21 str_c(collapse =\",\"))\n22 }\n这个函数能够对任意的字符串进行识别，如果没有任何识别，就返回NA；如果识别到了，最多返回匹配度最高的3个分类，分类之间以“,”分隔（注意是英文的逗号，这个可以根据自己的洗好更改）。我们用两个例子看看结果如何：\n1 get_sim(\"稀奇古怪\")\n2 [1]NA\n3\n4 get_sim(\"大数据\")\n5 [1]\"大数据研究院,大数据学院,大数据试验场研究院（筹）\"\n显然，这个函数是有效的。\n——————————————\n往期精彩：\n今天，我改名了！\n不敢穷，不敢病，不敢死……我们是独生子女\nqkerntool使用说明\nR语言中文社区2018年终文章整理（作者篇）\nR语言中文社区2018年终文章整理（类型篇）","data":"2019年04月29日 13:11:00"}
{"_id":{"$oid":"5d343b1762f717dc0659b334"},"title":"自然语言处理笔记11-哈工大 关毅","author":"Big_quant","content":"目录\n文章目录\n目录\n前言\n一篇论文的诞生（1）\n一篇论文的诞生（2）\n一片论文的诞生（3）\n前言\n硕士生涯结束，开始专心做一件自己觉得有用的工具，先做工程，后搞理论。\n自然语言处理是一个非常难的问题，同时是人工智能皇冠上的明珠。\n接下来会记录一系列自然语言处理的笔记，来自于哈工大老师关毅。\n一篇论文的诞生（1）\n这是课堂额外福利。\n不同的研究专题不一样。论文的研究过程8年。\n整合现有相似模型，语义相似计算。\n论文创作阶段：\n草创： 2000.1-2001.5\n布局：2001.5-2002.12\n奋争：2003.1-2004.12\n沉沦与转折：2005\n发表之路：2006-2008.7\n2000.1博士毕业去挣钱。\n问一问智能搜索引擎。\n语义相似度，好好计算争取出结果做了4个月出结果了\n写了一个专利，因为第一作者和老板谈崩了，后来想明白了。\n自己是去挣钱的，还想着名，有点儿想太多了。\n2001.5月出局回国，哈工大任教。\n2001-2002，按照老师的方法做的idea是老师的，你只是负责了实现，工作不在你。\n一篇论文的诞生（2）\n反思，小公司不要去争这些名利，大公司还可以争一争。\n新阶段新目标，一个阶段一个目标，错过了进入微软亚洲研究院的机会。\n专心处理后面的遗留问题，一直在处理，争取加速解决。\n进行调研，发现相似系统工程和这个问题一样。\n没做实验就开始投文章被大会举办方使劲怼了。\n国内的优势：大量高素质低成本的人才，写作比一切都重要，擅长于调动资源。\n把idea实验做出来，然后去落实，在做的这个过程中产生了新的想法，句法结构，新的结构，认知系统的深层原理。基础性研究解决了问题。\n一片论文的诞生（3）\n两个压力，项目和职称逼迫，犯了一件蠢事。\n中英文文章投稿，算是沾边一稿多投这一学术问题。\n幸好没接受，什么时候适合写论文呢？\n如鲠在喉，不吐不快时，适合写论文。\n再写其他论文应付工作。\n问答系统，应为参加了比赛，结果也不错，所以写出来，期刊也比较接受。\n写文章之前要有明确投论文的目标，要有对科技问题的认识。\n要知道一些问题适用于一些问题。\n创新不难，难在让人接受。\n绝对不要违规。\n最后给大家一点儿建议。\n1心怀大志\n2始终如一\n3相信自己\n4不懈努力\n5坚持自信。","data":"2019年02月14日 22:04:11"}
{"_id":{"$oid":"5d343b1862f717dc0659b336"},"title":"深度学习—— 人工智能概述","author":"BofLee","content":"什么是人工智能\n人工智能是计算机科学的一个分支，她企图了解智能的实质，并产生一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统。\n机器学习\n机器主要通过大量的训练数据进行训练，程序不断地进行自我学习和修正来训练出一个模型，而模型的本质就是一堆参数用成千上万的参数来描述业务特点，从而接近人类的智力。\n深度学习\n深度学习是机器学习的一个子集。\n深度学习的前身是人工神经网络（ANN），它的基本特点就是模仿人脑神经元传递和处理信息的模式。\n有监督学习：输入的训练数据有特征、有标记，在学习中就是找到特征与标记之间的映射关系，通过标记不断纠正学习中的偏差，使预测率不断提高。这种训练数据有标记的学习称为有监督学习。\n无监督学习：让计算机自己去学习怎样做一些事情，所有训练数据没有标记，只有特征。无监督学习有两种思路：第一种，训练时不为其指定明确分类但数据会呈现聚群的结构，彼此相似的类型会聚集在一起。计算机把这些没有标记的数据分成一个个组合，就是聚类；第二种，在成功时采用某种激励制度，即强化学习.\n半监督学习：训练数据中有一部分有标记有一部分无标记，没有标记的数量远远大于有标记的数量（这也符合现实）。它的基本规律是：数据的分布必然不完全随机，通过结合有标记的局部特征，以及大量没标记的数据的整体分布，可以得到比较好的分类结果。","data":"2019年01月17日 11:26:49"}
{"_id":{"$oid":"5d343b1862f717dc0659b338"},"title":"Python和人工智能的关系","author":"lmseo5hy","content":"我们经常听到“Python”与“人工智能”这两个词，也很容易混淆这两个词，那么Python和人工智能有什么关系呢?\n首先我们先来说说人工智能\n人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。\n简单来说，人工智能是一种未来性的技术。\n再来说说Python\nPython是一门计算机程序语言，目前人工智能科学领域应用广泛，应用广泛就表明各种库，各种相关联的框架都是以Python作为主要语言开发出来的。\n谷歌的TensorFlow大部分代码都是Python，其他语言一般只有几千行。如果讲开发效率，用Python，谁会用Java这种高不成低不就的语言搞人工智能呢?\nPython虽然是脚本语言，但是因为容易学，迅速成为科学家的工具，从而积累了大量的工具库、架构，人工智能涉及大量的数据计算，用Python是很自然的，简单高效。\nPython有非常多优秀的深度学习库可用，现在大部分深度学习框架都支持Python，不用Python用谁?","data":"2018年05月29日 16:00:33"}
{"_id":{"$oid":"5d343b1962f717dc0659b33a"},"title":"自然语言处理-词向量与相关应用","author":"风一样的男人_","content":"计算机处理图像和文字的实质是在向量矩阵等基础上将其转化为数字，然后计算搜索的内容和库内容信息的匹配度\n文字---\u003e数值向量\n算法案例：\n词编码：N-gram\n权重：TF-IDF ----\u003eword2vec----\u003esense2vec\n-----------------------------------------------------------------------------------------------------\nNLP常见任务\n自动摘要（百度，google）\n指代消解 (代词理解指代是什么)\n机器翻译 （应用面很广， 但目前还不完善）\n词性标注\n分词（中文，英文，日文）\n主题识别\n文本分类\n.............\n-----------------------------------------------------------------------------------------------------\nNLP处理方法：\n传统：基于规则\n现代：基于统计机器学习\nHMM CRF SVM LDA  CNN......\n\"规则\"隐含在模型参数里\n-----------------------------------------------------------------------------------------------------\n\n\n\n\n\n-----------------------------------------------------------------------------------------------------\n\n数据决定结果上限\n算法将以多大程度接近结果上限\n\n\n词权重：（词在文档中的顺序没有被考虑）\nTF-IDF  信息检索\nBinary weighting 短文本相似性\n\n\n离散表示缺点：\n词表维度随着语料库增长膨胀\nn-gram词序列随语料库膨胀更快\n数据稀疏问题\n无法衡量词向量之间的关系\n\n\n分布式表示：\n用一个词附近的其他词来表示该词\n--》被称为现代统计自然语言处理中最有创见的想法之一\n共现矩阵：\n主要发现主题，用于主题模型，如LSA\n局域窗中的word-word共现矩阵可以挖掘语法和语义信息\n\n\n共现矩阵存的问题：\n向量维度随着词典大小线性增长\n存储整个词典的空间消耗非常大\n一些模型如文本分类模型会面临稀疏性问题\n模型会欠稳定\n\n\n构造低维稠密向量作为词的分布式表示（25维~1000维）！\nSVD降维\n\n\n-----------------------------------------------------------------------------------------------------\n\n\n\n-----------------------------------------------------------------------------------------------------","data":"2018年06月24日 12:09:32"}
{"_id":{"$oid":"5d343b1962f717dc0659b33c"},"title":"AI面试官产品说明【Android APP开发 自然语言处理 图像处理 深度学习 人脸检测 面相分析 智能面试】","author":"南七小僧","content":"1.注册登录系统\n2.招聘信息中心系统【用于获得招聘信息，查看招聘详情，查看公司信息】\n3.简历系统【用于新建与完成简历编辑工作】\n4.面试系统【采用人工智能，自然语言对话开发的面试系统，包含 基于上下文提问，语义理解，语音文字实时转写，基于人脸68特征点的面相分析，基于人脸的情绪分析，基于简历的画像生成，人岗匹配度语义计算算法等】\n5.正常招聘流程【简历投递-\u003e参与面试-\u003e等待HR处理反馈-\u003e结果】","data":"2019年04月08日 12:56:37"}
{"_id":{"$oid":"5d343b1a62f717dc0659b33e"},"title":"自然语言处理笔记3-哈工大 关毅","author":"Big_quant","content":"目录\n文章目录\n目录\n前言\n汉语的分词与频度统计（1）\n汉语词汇的特点\n汉语的分词与频度统计（2）\n汉语的分词与频度统计（3）\n汉语的分词与频度统计（4）\n汉语的分词与频度统计（5）\n汉语的分词与频度统计（6）\n汉语的分词与频度统计（7）\n前言\n硕士生涯结束，开始专心做一件自己觉得有用的工具，先做工程，后搞理论。\n自然语言处理是一个非常难的问题，同时是人工智能皇冠上的明珠。\n接下来会记录一系列自然语言处理的笔记，来自于哈工大老师关毅。\n汉语的分词与频度统计（1）\n语\n言\n分\n类\n{\n孤\n立\n语\nif\n没\n有\n附\n加\n词\n，\n如\n汉\n语\n黏\n着\n语\nif\n有\n附\n加\n词\n，\n如\n日\n语\n曲\n折\n语\nif\n形\n态\n变\n化\n，\n如\n英\n语\n语言分类\\begin{cases} 孤立语\u0026amp;\\text{if } 没有附加词，如汉语\\\\ 黏着语 \u0026amp;\\text{if } 有附加词 ，如日语 \\\\ 曲折语 \u0026amp;\\text{if } 形态变化，如英语 \\end{cases}\n语言分类⎩⎪⎨⎪⎧ 孤立语黏着语曲折语 if 没有附加词，如汉语if 有附加词，如日语if 形态变化，如英语\n词是自然语言处理中的最小单位。\n语速，词，短语，句子，语群。\n汉语词汇的特点\n结合紧密，使用频繁，汉语的词可以拆开。\n调换位置，有限度地展开。\n字串可以切分为词串。\n提出规划。\n汉语的自动分词是他的重要组成部分，对他分词很困难。\n新领域老方法，新瓶装旧酒。\n汉语的分词与频度统计（2）\nGB分词规划，提出了汉字的分词规则。\n四字词语，一律是词。切分歧义，未登录词，比较困难。\n比如提高中国人民生活水平比较困难。\n覆盖型切分容易出问题，真歧义同属切分型。\n如何排除歧义呢？\n蛋鸡问题先有蛋。\n分词做词切分，前驱字串和后驱字串。\n词法信息实例。\n歧义字串单切，句法规则调整。\n利用语义信息实例进行切分。\n新出现的词最困难，挂一漏万。\n上下文出现的条件，以及分词系统。\n互信息，极大方差，极大熵模型。\n汉语的分词与频度统计（3）\n主要分词方法，正向最大匹配方法，几个字符在一块儿。去掉一个词再试，逆向匹配方法。\n双向匹配法。\n最小分词方法：做的东西是给人看的。\n创造力最丰富：20-40岁的时候。\n不存在切分歧义的点：分段，计算最短路径。图的方法去理解这些东西。\n词网格方法：生成所有可能切分的方式。计算词的概率。\n汉语的分词与频度统计（4）\n哈工大2005年第一名，做到95%。\n语料库，平衡语料库。\n生语料库，半生不熟语料库，句法分析所困。\n语法分析十万级的词汇基本没用。\n共时语料库，历时语料库。\n发展时间一段时间以内，各种模型的正确率。\n统计机器翻译，统计翻译模型。\n中文信息语料库：英语：Brown corpus。\nPenn Treebank。句法树，数学化。\n双语语料库，法律文档语料库。\n词频统计，构建词汇模型的核心。词典收词的规律。\n汉语的分词与频度统计（5）\n《现代汉语频率词典》LJVAC华语共时语料库。\n建立了各地词典。双音节词最多，定量分析。\n用词相同率和地域相关。\n词频反映国家政策的变化。\n汉语的分词与频度统计（6）\n词频一个数表，高频虚，低频实词。定量分析，占90%的词低于10次。\nzipf定律，f正比于1/r。\n\ny\n=\nk\nx\nc\ny=kx^c\ny=kxc\n指数定理，同取对数。除特高频和特低频以外都符合。\n语料库规律，可以推测句式规律。\n1构语语言模型，模型多少词enough2heap’s law。\n反映了词表长度与语料库的关系。平滑算法更好的保障。\n汉语的分词与频度统计（7）\n其他的统计分布规律，频度和频度词个数，推荐大家看，创世纪的第八天。\n真正的科学，需要枯燥的处理一件事，需要把一件事情做到极致。","data":"2018年12月12日 21:07:14"}
{"_id":{"$oid":"5d343b1a62f717dc0659b340"},"title":"自然语言处理笔记6-哈工大 关毅","author":"Big_quant","content":"目录\n文章目录\n目录\n前言\nMarkov模型1\nMarkov模型2\nMarkov 模型3\nMarkov模型4\nMarkov模型（5）\n前言\n硕士生涯结束，开始专心做一件自己觉得有用的工具，先做工程，后搞理论。\n自然语言处理是一个非常难的问题，同时是人工智能皇冠上的明珠。\n接下来会记录一系列自然语言处理的笔记，来自于哈工大老师关毅。\nMarkov模型1\n设\nX\n=\n（\nX\n1\n,\nX\n2\n,\n.\n.\n.\nX\nt\n）\n是\n随\n机\n变\n量\n序\n列\n，\n其\n中\n每\n个\n随\n机\n变\n量\n的\n取\n值\n在\n有\n限\n集\nS\n=\ns\n1\n,\nS\n2\n,\n称\n为\n状\n态\n空\n间\n，\n时\n间\n不\n变\n性\n假\n设\nX=（X_1,X_2,...X_t）是随机变量序列，其中每个随机变量的取值在有限集S={s_1,S_2},称为状态空间，时间不变性假设\nX=（X1 ,X2 ,...Xt ）是随机变量序列，其中每个随机变量的取值在有限集S=s1 ,S2 ,称为状态空间，时间不变性假设\nN阶Markov模型，只需修改状态空间的定义S’={X}。定义新的变量\nX\ni\nb\ne\nl\no\nn\ng\nt\no\ns\n′\nX_i belong to s\u0026#x27;\nXi belongtos′\n使得\nX\nt\n=\n(\nS\ni\n−\n1\n,\nS\ni\n)\nX_t=(S_{i-1},S_i)\nXt =(Si−1 ,Si )并且约定：\n\nP\n(\nX\ni\n∣\nX\ni\n−\n1\n)\n=\nP\n(\n(\nS\ni\n−\n1\n,\nS\ni\n)\n∣\n(\nS\ni\n−\n2\n，\nS\ni\n−\n3\n)\n)\nP(X_i|X_{i-1})=P((S_{i-1},S_i)|(S_{i-2}，S_{i-3}))\nP(Xi ∣Xi−1 )=P((Si−1 ,Si )∣(Si−2 ，Si−3 ))\nMarkov模型的形式化表示，一个马尔可夫模型是一个三元组\n（\nS\n，\nπ\n,\nA\n）\n（S，\\pi,A）\n（S，π,A）,其中S是状态的集合，\nπ\n\\pi\nπ是初始状态的概率，A是状态间的转移概率。\n发射字符依赖于当前状态，不同状态，有不同输出。\nHMM：不同状态可以有相同输出，输出在状态转移中进行。\nMarkov模型2\nHMM模型：\n最大的灵活性在状态转移中以特定概率输出。\n##HMM模型：\nHMM是一个五元组（S,k,pi,a,b），其中s是状态的集合，k是输出字符的集合，pi是初始状态的概率，a是状态转移的概率。b是状态转移时输出字符的概率。\nt:=1\n以概率\np\ni\np_i\npi 在状态\nS\ni\nS_i\nSi 开始（ie，X1=i）\nforever do\nmove from state Si to state Sj with\nprobability\nA\ni\nj\n(\ni\n,\ne\n,\n.\n.\nX\nt\n+\n1\n=\nj\n)\nA_{ij}(i,e,..{X_{t+1}=j})\nAij (i,e,..Xt+1 =j)\nEmit observation symbol Ot=k\nwith probability b\nt:=t+1\nend\n##HMM的基本问题\n给定一个输出的字符序列。如何调整模型的参数使得产生这一序列的概率最大，IBM Watson医生。 隐马模型的基本问题：给定一个模型M=（S,k,pi,a,b），如何高效地计算某一输出字符序列的概率P（O|u）。\n给定一个输出字符序列O和一个模型u，如何确定产生这一序列概率最大的状态序列\n（X1，x2）\n词网格分类，音字转换。网格cell states。\n问题1：评价（evaluation）\n给定一个模型u=（s,k,pi,a,b）如何高效地计算某一输出字符序列的概率P（O|u）。\nO=（o1,o2,…,or）,u=(a,b,pi)\n计算P（O|u）。\n给定词网格最优路径\n方案一：直观方法。\nX1–\u003eo1\nP(o|x,u)=bx1oz=\n∑\nP\n(\nO\n∣\nX\n,\nU\n)\n∗\nP\n(\nX\n∣\nu\n)\n\\sum P(O|X,U)*P(X|u)\n∑P(O∣X,U)∗P(X∣u)\n动态规划，递推求解。\n\nα\ni\n(\nt\n)\n=\nP\n(\nO\n1\n,\n.\n.\nO\ni\n∣\nX\nt\n)\n\\alpha_i(t)=P(O1,..Oi|X_t)\nαi (t)=P(O1,..Oi∣Xt )\n方案2：向前过程\n=\n∑\ni\n=\n1\nα\ni\n(\nt\n)\n∗\nb\nj\nα\ni\nj\nb\nj\n∗\nα\nt\n+\n1\n\\sum_{i=1}\\alpha_i(t)*b_j\\alpha_{ij}b_j*\\alpha_{t+1}\ni=1∑ αi (t)∗bj αij bj ∗αt+1\nMarkov 模型3\n向前过程\nRRGB\n动态规划法\n向后过程概述：\nKaTeX parse error: Expected 'EOF', got '\\lmd' at position 5: P(O|\\̲l̲m̲d̲)=\\sum_{1\u003cj\u003cN}p…\n算法效率与前算法相同。\n用途：参数训练问题的一个重要组成部分。\n##解码\n确定产生概率最大的状态\ndelta为在t时刻到达状态j，输出字符Ot时，输出前面t-1\n个字符的最可能路径的概率。\ndelta_j(t)=max_{xi…xt+1}P(x1…xt+1,O1…Ot-1,Xt=1,Ot)\ndelta+{t+1}(j)=max_deltat(j)aijbij(ot+1)\nviterbi algorithm:\n初始化：\ndelta（i）=piibi（Oi）\nphi（i）=0\n递归：\n最优路径 qt=phi_t=1（Qt+1）\n把连乘变成加。\n参数统计\nargmax_uP(O|u)\nMarkov模型4\n设计更新计算更新值。basic思想。\n设定模型的初始值，U-old。\n基于U_old计算输出U_new和O的概率。\n如果P（o|u_new）-P(O|u_old)\u003c某个阈值\n停止\n否则，U_old\u003c-U_new返回step2.\nBaum-Welch算法。\n向前向后算法。\n基于HMM的词性标注。\n词性标注：\n作用句法分析的前期步骤\n难点：兼类词。\n词性标准应用：\nTbest=argmaxPr(T|s)=argmaxP(S|t)P(T)\n如何计算P(S|t)和P(T)\n简化：\n词wi的出现，仅仅依赖于它的词性标记，标记ti的出现仅仅条件依赖于它前面的标记t_i-1\n公式转化 计算P(S|T)和P(T)\nPr(S|t)Pr(t)=\\timr P(Wi|ti)P(Ti|ti-1)\n使用最大相似度估计：\nP(Ti|ti-1)=c(ti,tj)/c(ti)\n音字转换\n发射字符：状态是什么？\n发射字是什么？\n不是什么？转化为生产力的学习。\nMarkov模型（5）\nHMM评价，解码编码问题\nch6尾声，音字转换\nT=argmax（v|s）\n语言单位间的远距离约束\n递归模型\n规则与统计相结合\n采用规则的方法：\n短语结合规则：\nA+NP-\u003eNP\nA+‘的’+NP-\u003eNP\nM+‘枝’+NP-\u003eNP\n短语匹配算法。\n从词网格到元素网格\n颗粒度疏，工作量太大。\n规则匹配强度不够。\n做了几个宣传词，要有自己的优势项。\n还做了系统挂接问题。","data":"2019年01月11日 21:50:34","date":"2019年01月11日 21:50:34"}
{"_id":{"$oid":"5d343b1b62f717dc0659b342"},"title":"【自然语言处理入门】02：Kenlm语料库的制作与模型的训练","author":"闰土不用叉","content":"本文是《从自然语言处理到机器学习入门》系列课程的第二次作业，由于我的作业环境没有配好（配了n次了还是不行T_T），但是为了保证这一系列作业的完整性，于是经罗曜强律师同意，人工智能A7论坛授权，转载他的作业笔记。\n\n1 基本要求\n通过自己训练的语言模型编程，判断每句话中是否存在a an用错的问题(所谓用错 指a an用反了 比如 i have a apple是错误的； i have an apple 是正确的)\n2 准备工作\n（1）实验的环境Ubuntu16.04，Python 版本 2.7\n（2）使用kenlm训练一个语言模型，首先要准备kenlm所需要的语料，按照http://kheafield.com/code/kenlm/官方文档上使用说明，训练的文件会被训练成.arpa的格式。\n（3）训练模型：例如:我有名为test.txt的文件需要训练成kenlm指定的.arpa格式文件,训练后的文件为text.arpa，我需要在Ubuntu的Teminal终端使用如下命令进行训练：\nbin/lmplz -o 5 \u003ctest.txt \u003e text.arpa\n-o Required. Order of the language model to estimate\n-o 5 代表使用5ngram\n将arpa文件转换为binary文件，这样可以对arpa文件进行压缩，提高后续在python中加载的速度。\nbin/build_binary -s text.arpa text.bin\n3 具体实验\n做好上述前置准备工作后，接着就是在Python下运行text.arpa\n主要分为以下几个步骤：\n#导入训练所需要的包 import kenlm import nltk from itertools import combinations, permutations #将文件导入kenlm语言模型 model = kenlm.LanuageModel(text.bin) #判断a或者an在互换前的得分和互换后的得分，如果互换前的得分高于互换后的得分，则说明a或an没有错误，如果互换后的得分高于互换前的得分则说明a或者an语法错误 def judge_a_or_an(sentence): #创建一个空list，用于存放sentence s = [] #将句子进行分词 \"\"\" Model.score函数输出的是对数概率，bos=False, eos=False意思是不自动添加句首和句末标记符 \"\"\" pre_score = model.score(‘ ’.join(sentence), bos = True, eos = True) #通过循环的方式替换a或an然后进行评分对比 for word in sentence: #如果word里面有a，则把a换成an If word == ‘a’: s.append(‘an’) #如果word里面有an，则把an换成a elif word == ‘an’: s.append(‘a’) #如果word里面没有a或者an，按照原句输出 else: s.append(word) after_score = model.score(‘ ’.join(s), bos = True, eos = True) #对话置换前，置换后的得分，如果置换前得分高于置换后，则返回0，否则返回1 if pre_score \u003e after_score return 0 else: return 1 #打开文件 inputs = open(‘text.arpa’, ’r’) outputs = open(‘text_after.txt’, ‘r’) for line in inputs: data = nltk.tokenize.word_tokenize(line) #调用judge_a_or_an函数 label = judge_a_or_an(data) #格式化输出0或1 print(line + ‘\\t%d’ %(label)) #关闭IO流 outputs.close() inputs.close()\n4 常见错误\n（1）ModuleNotFoundError: No module named ‘kenlm’\nKenlm安装错误，导致无法正常调用kenlm、\n（2）打开文件后未关闭IO流致使文件无法正常输出\n（3）其他语法错误","data":"2017年12月18日 16:53:51","date":"2017年12月18日 16:53:51"}
{"_id":{"$oid":"5d343b1b62f717dc0659b344"},"title":"人工智能将改变世界 想学AI首选Python语言","author":"qq_43444478","content":"人工智能将改变世界，想学AI首选Python语言。大家知道：人工智能改变的不仅是商业运作的方式，还涉及到社会的方方面面。从使用图像识别技术增强公共安全，到借助自然语言处理提供自动的人性化服务，人工智能都起到了很大的作用。那么，人工智能培训就业前景如何?\n人工智能发展前景很好，中国正在产业升级，工业机器人和人工智能方面都会是强烈的热点，而且正好是在3~5年以后的时间。所以也可以很清楚地知道：现在学习人工智能真的很有必要。\n对于学习人工智能的人来说，学习质量的好坏直接影响到学习结果的好坏，所以选择一个不错的学校就比较重要。那么对于一个新手来说，最重要的应该是选择一家靠谱的人工智能学校。\n如果你想学习人工智能，但是却不知道哪个好。那么教你一个办法，不要去看那些便宜的，筛选出一批正常的，然后一个个去了解咨询，最后结合其他方面做一个对比，这样就能找到比较不错的人工智能学校。\n鉴别好坏的方法之一，就是去看下口碑如何，选择学校口碑不错的，深入了解，这样就可以找到比较好的人工智能学校。\n目前在保障就业的学校中，有的会保障就业薪资的水平，而有的不会，从学员角度来说，能够保障就业薪资的，肯定实力更加强，更好，不然也不会随便做出这样的承诺。所以如果条件允许，选择有薪资保障的会更好。\n我们还要先了解下它的课程内容是否全面，课程内容是否是比较新的，一般人工智能课程内容包含python、数据库、无监督学习、有监督学习、深度学习算法、TensorFlow、CNN，以及多个人工智能相关的项目内容，这样的是比较全面的。所以尽可能多了解，对比不同学校之前的课程内容，选择课程内容比较全面专业的。\nPython是人工智能的首选语言，因此大家知道掌握Python技术的重要性了吗?对于人工智能学习者来说，我们需要从多个方面观察了解对比之后，再进行决定，当然个人建议一定要上门实地考察下，同时试听下课程，这样你就可以知道好不好，适不适合自己。","data":"2018年12月25日 15:55:43"}
{"_id":{"$oid":"5d343b1c62f717dc0659b346"},"title":"自然语言处理笔记10-哈工大 关毅","author":"Big_quant","content":"目录\n文章目录\n目录\n前言\n基于认知科学原理的相似模型（五）\n基于认知科学原理的相似模型（六）\n面向旅游领域的问答系统实验\n前言\n硕士生涯结束，开始专心做一件自己觉得有用的工具，先做工程，后搞理论。\n自然语言处理是一个非常难的问题，同时是人工智能皇冠上的明珠。\n接下来会记录一系列自然语言处理的笔记，来自于哈工大老师关毅。\n基于认知科学原理的相似模型（五）\n高阶谓词逻辑结构\n1结构匹配优先于特征匹配\n2这个过程就像递归\n\n3说到权重大家重新认识\n4心理学原理推导权重\n\n\n5相似心理学模型洋气对齐\n\n心理学模型\n5高阶谓元结构，使用理论圆不了的东西，就靠假设圆了\n心理学模型\n属性是连续系统的对象，结构映射理论\n前提2：值域前提\n\n基于结构映射原理的系统相似度估算工程：\n1映射与推理。\n系统相似度计算的主要因素：\n个体相似度\n匹配对象对及其相似度\n对象的权重\n带比较对象的系统结构\n个体相似度，比较相似度。\n基于认知科学原理的相似模型（六）\n当对对象A和对象B进行相似度计算时，子对象Ai必须同时满足如下两个条件才与对象Bj构成匹配对象对。\n必须满足关系所约定的语义角色限制和序限制。\n他们的相似度必须大于某个阈值。\n比如，给是三元关系 给予人，被给人，物品。\n相似，简单，单调约束。\n递归算法，进行处理，算法总结。\n对象表示高阶谓词论元结构。\n模仿人类\n\n严格落实认知规律，满足所有条件。\n简单用户问句体验。\n面向旅游领域的问答系统实验\n肯定了结构主义语言学和认知语言学的含义。\n信息检索中广为应用的向量空间模型的基础性理论模型。\n图像语义检索来源于图像语义结构分析。","data":"2019年02月14日 15:23:35","date":"2019年02月14日 15:23:35"}
{"_id":{"$oid":"5d343b1c62f717dc0659b348"},"title":"自然语言处理笔记8-哈工大 关毅","author":"Big_quant","content":"目录\n文章目录\n目录\n前言\n问答系统基础一\n问答系统基础二\n问答系统术语\n问答系统基础三\n问答系统基础四\n问答系统基础五\n前言\n硕士生涯结束，开始专心做一件自己觉得有用的工具，先做工程，后搞理论。\n自然语言处理是一个非常难的问题，同时是人工智能皇冠上的明珠。\n接下来会记录一系列自然语言处理的笔记，来自于哈工大老师关毅。\n问答系统基础一\nIBM这个人，蓝色巨人，硅谷海盗。\nWatson 对话系统，doing。\n老师自我吐槽，扯淡时间多于工作的时间。\n问答系统基本概念。\n问答的简史：问答的所有问题？\n1机器翻译2语音识别3数据库\n自然语言控制机器人动作，积木世界。\nLUNAR系统。\n扩充知识转移网络进行句法分析。\nLIFER系统\nCHAT-80系统\nSTART系统\nMURAX系统\nAskFeeres系统\n人肉高科技，人工恢复问题\n十万篇相关文档分析，相关问题答案未实现。\n问答系统基础二\n在各个步骤，建立统计分类模型。\n问答式信息检索。\n\n会议检索，评测技术平台，check。\n1问句处理？\n2海量答案对应？\n3事实性陈述即可解决\n问答系统术语\n问题类型 question type\n\n\n答案类型 answer type\n\n\n问句焦点 question focus\n\n\n问句主题 question topic\n\n\n候选段落 candidate passage\n\n\n候选答案 candidate answer\n\n答案所属类别，最型问题，观点问题，因果类问题，事实类问题\n问句对应的目标类型。\n问答系统基础三\n2008年，认识到自己目前做的和想做的距离有多大。\n焦点：实体的属性。\n主题：讨论的实体。\n\n候选段落：由搜索引擎响应用户问句而检索得到的文本片段。\n候选答案：可能的答案\n{1找到候选段落2与问句匹配并检查段落的语义3抽取答案}\n{1相似段落2语义匹配法3语法匹配法}\n智能化信息检索结构图\n语法，语义，专业知识库\n元搜索，满足确切的未知的点。\n问答系统基础四\n网页重复太多（一些搜索引擎死了，百度活下来了）\n工程问题细节超多，这才是你的生命线。\n\n数据库索引技术，PAT树，B+树，哈希树，My SQL\n智能化信息，检索模型。\n{布尔，向量空间，概率模型}\n基于结构映射理论的新型信息，检索模型，系统相似模型。\n向量空间模型的本源理论模型，通用性理论模型。\n自然语言处理技术：各个技术的综合。\n完善自己的理论，做应用课题。\n原创理论时代。复杂性，精度是冲突的。\n问答式基础理论。\n问答系统基础五\n项目研究进展报告提纲：\n1开放域问答系统概要设计\n2工作进展与展望\n3主要阶段性成果\n4总结\n用户层：用户交互，语言分析识别，个性化信息。\n最终肯定要个性化发展。\n机器学习，ranking结果\nNLP+NLG。\n三层体系，四层系统。\n语义层，用户层，强化学习技术。","data":"2019年01月14日 11:36:18"}
{"_id":{"$oid":"5d343b1c62f717dc0659b34a"},"title":"自然语言处理笔记7-哈工大 关毅","author":"Big_quant","content":"目录\n文章目录\n目录\n前言\n句法分析技术1\n句法分析技术2\n句法分析技术3\n句法分析技术4\n前言\n硕士生涯结束，开始专心做一件自己觉得有用的工具，先做工程，后搞理论。\n自然语言处理是一个非常难的问题，同时是人工智能皇冠上的明珠。\n接下来会记录一系列自然语言处理的笔记，来自于哈工大老师关毅。\n句法分析技术1\n基于规则+统计结合的句法分析\n判定输入的词序列是否合法，短语结构树，有向无环图。\n句子:{主『定语，中心』}{谓语『状，谓{动宾【动，宾语（定语，中心词）】，补语}』}\n\n状语修饰，核心动作。\n\n提高语法分析结果，计算机的语法分析里面不明确。\n词性层级：两种句法分析的区别因子进入短语结合规则。\n句法分析和短语结合分析进入区别。\n语法歧义示例。\n汉语句法分析，句法分析细语，形式语法体系。\n匹配模式方法，“正则文法”。\n短语结构文法，信息处理系统。机器翻译运用，留下此路不通的牌子。\n科研有风险，不是一帆风顺，需要有挑战精神的人去做。\n\n扩充转移网络，状态转移机，树邻接语法\n句法分析技术2\n基于合一运算的语法，复杂描述集的语法，合一运算实现该方法，依存语法，上下文颗粒度太大，短语限定在词汇上，K+语法，依存文法，形式语法体系模式，正则匹配。\n短语结构语法分析很多方法。\n扩充转移网络\n回顾：Chomsky文法体系\n\nG\n=\n（\nN\n，\n∑\n,\nP\n,\nS\n）\nG=（N，\\sum,P,S）\nG=（N，∑,P,S）是一个文法，\n\nα\n−\n\u0026gt;\nβ\n∈\nP\n\\alpha-\u0026gt;\\beta\\in P\nα−\u003eβ∈P\n0型文法：对\nα\n−\n\u0026gt;\nβ\n不\n作\n任\n何\n限\n制\n\\alpha-\u0026gt;\\beta 不作任何限制\nα−\u003eβ不作任何限制\nI型文法：\n∣\nα\n∣\n≤\n∣\nβ\n∣\n|\\alpha|\\leq|\\beta|\n∣α∣≤∣β∣\nII型文法：\n上\n下\n文\n无\n关\n文\n法\n，\nα\n∈\nN\n上下文无关文法，\\alpha \\in N\n上下文无关文法，α∈N\nIII型文法：正则文法。\n一个字串的推导是一系列文化规则的应用。\n起始符推导到最好。强范式：基于词的语法。\n格里巴克：形式语言自动化机。\n一种语言\nL\ng\nL_g\nLg 是由某上下文无关文法推导出来的所有终结符号串的集合，其中的每个终结符串，称为合乎文法G，否则，称之为不合乎文法。上下文文法，扩充概率无关文法。\n一个随机上下文无关语法，PCFG的三个假设。\n1）位置无关2）上下文无关3）祖先无关。\n推出非总结串，隐码模型，推出问题。\nPCFG的三个基本问题。\n\n一个语句\nW\n=\nW\ni\nW\ni\n−\n1\nW\ni\n−\n2\nW\nn\nW=W_iW_{i-1}W_{i-2}W_n\nW=Wi Wi−1 Wi−2 Wn 的P(W|G)也就是产生语句W的概率？\n\n在语句W的句法结构有歧义的情况下，如何快速选择最佳的语法分析（parse）？\n如何从语料库中训练G的概率参数使得P(W|G)最大（类比之前的问题，评价，解码，编码问题）\n节点间的递推关系，叶节点到根节点的句法树。\n向内算法\n\n句法分析技术3\n随机上下文无关文法\n任何一个语句都可以视为一种语言模型。\n一个句法树中的结点词句法树开始推导，自顶向下，自下向上。\n某一部推导，对应于几个规则，开始推导，做出结果。\n登上算法，尝试去做，EM算法，优化前进，无指导学习算法，PCFG的优点。\n可以对句法分析的歧义，结果进行概率排序。\n提高文法的容错能力。\n词对结构分析，上下文对结构分析，随机上下文无关文法。\n向前算法，节点值增加提前。\n\nα\ni\nj\n(\nA\n)\n=\nP\n(\nW\ni\n,\nW\nj\n∣\nA\n)\n,\ni\n\u0026lt;\nj\n\\alpha_{ij}(A)=P(W_i,W_j|A),i\u0026lt;j\nαij (A)=P(Wi ,Wj ∣A),i\u003cj\n\n=\n∑\nB\n,\nC\n,\n∈\nR\nP\n(\nW\ni\n,\nW\nj\n,\nB\n,\nW\nr\n+\n1\n.\n.\nW\nj\n,\nC\n∣\nA\n)\n=\\sum_{B,C,\\in R}P(W_i,W_j,B,W_{r+1}..W_j,C|A)\n=B,C,∈R∑ P(Wi ,Wj ,B,Wr+1 ..Wj ,C∣A)\n\nα\ni\n,\nj\n=\nP\n(\nA\n−\n\u0026gt;\nW\ni\n)\ni\n=\nj\n\\alpha_{i,j}=P(A-\u0026gt;W_i)i=j\nαi,j =P(A−\u003eWi )i=j\n句法分析技术4\n浅层句法分析，形式合规分析，结构分析就行。\n部分分析，组块分析。\n例句：\n这一切已经引起世界各国的普遍关注。\nS-k,r,c,p.\n浅层专项研究。\n基于HMM的浅层分析技术，ACL会议。他识别的目标是非递归的NLP，浅层句法分析，隐码是五元组，浅层分析状态空间如何定义。输出一对词性标记，一个组块开始。\n照着看，任何阶段都可以用任何一个模型，不同的是标记的内容。\n级联式有限状态分析句法。\n# 句法分析技术5\n基于规则的方法，需要大量人力，不好迁移。\n总结：\n概率上下文无关文法，句法分析是目前语言处理技术瓶颈之一。发现问题比解决问题更重要。\n句法分析是必由之路，ACL每年关注，语法分析。\n强化学习技术：免疫机制分析合适吗？\n句法是形式，语义是内容。\n完整合法性，没有公认的内容。\n句法的强制性和语义的决定性，句法系统和语义系统是两个不同的系统，它们各自独立而又相互依存，彼此的对应关系十分复杂，统计规则之后讲应用。","data":"2019年01月13日 11:02:38","date":"2019年01月13日 11:02:38"}
{"_id":{"$oid":"5d343b1d62f717dc0659b34c"},"title":"","author":"","content":"","data":""}
{"_id":{"$oid":"5d343b1d62f717dc0659b34e"},"title":"循环神经网络综述 -语音识别与自然语言处理的利器","author":"SIGAI_csdn","content":"本文为SIGAI原创文章，仅供个人学习使用，未经允许，不能用于商业目的\n欢迎搜索关注微信公众号SIGAI获取更多原创干货\n\n\n导言\n循环神经网络是一种具有记忆功能的神经网络，适合序列数据的建模。它在语音识别、自然语言处理等领域取得\n了成功。是除卷积神经网络之外深度学习中最常用的一种网络结构。在本文中，SIGAI将和大家一起回顾循环神\n经网络的发展历程与在各个领域的应用。\n序列数据建模\n全连接网络和卷积网络在运行时每次接收的都是独立的输入数据，没有记忆能力。在有些应用中需要神经网络具有记忆功能，典型的是时间序列预测问题，时间序列可以抽象的表示为一个向量序列：\n这里的下标表示时刻，神经网络每个时刻接收一个向量输入。不同时刻的向量之间存在关系，每个时刻的向量与更早时刻的向量相关。例如，在说话时当前要说的词和之前所说的词之间相关，依赖于上下文语境。我们需要根据输入序列来产生输出向量。这类问题称为序列预测问题，输入序列的长度可能不固定。\n语音识别与自然语言处理的问题是这类序列预测问题的典型代表。前者的输入是一个时间序列的语音信号；后者是文字序列。下面我们用一个实际例子来说明序列预测问题。假设神经网络要用来完成汉语填空，考虑下面这个句子：\n现在已经半夜12点了，我非常困，想回家__。\n最佳答案是“睡觉”或者“休息”，这个答案需要根据上下文理解得到。在这里，神经网络每次的输入为一个词，最后要填出这个空，这需要网络能够理解语义，并记住之前输入的信息即语句上下文。这里需要神经网络具有记忆功能，能够根据之前的输入词序列计出当前使用哪个词的概率最大。如何设计一个神经网络满足上面的要求？答案就是我们接下来要介绍的循环神经网络。\n\n\n循环层的工作原理\n循环神经网络（简称RNN）[1]会记住网络在上一个时刻的输出值，并将该值用于当前时刻输出值的生成，这由循环层实现。RNN的输入为前面介绍的向量序列，每个时刻接收一个输入，网络会产生一个输出，而这个输出是由之前的序列共同作用决定的。假设t时刻循环层的状态值为  ，它由上一时刻的状态值以及当前时刻的输入值共同决定，即：\n这是一个递推关系式，现在的问题是确定这个表达式的具体形式，即将上一时刻的状态值与当前时刻的输入值整合到一起。在全连接神经网络中，神经元的输出值是对输入值进行加权，然后用激活函数进行作用，得到输出。在这里，我们可以对上一时刻的状态值，当前时刻的输入值进行类似的处理，即将它们分别都乘以权重矩阵，然后整合起来。整合可以采用加法，也可以采用乘法或者更复杂的运算，最简单的是加法，乘法在数值上不稳定，多次乘积之后数为变得非常大或者非常小。显然，这里需要两个权重矩阵，分别作用于上一时刻状态值，当前时刻的输入值，由此得到下面的递推关系式：\n其中W为权重矩阵，b为偏置向量。和全连接神经网络相比，这里只是多了一个项：\n\n\n它意味着在实现循环神经网络的时候需要用变量记住隐含层上次的输出值。使用激活函数的原因在SIGAI之前公众号的文章中介绍过，是为了确保非线性。下面我们用示意图来表示一个隐含层的变换：\n\n\n在上图中  和 共同决定  ，  体现了记忆功能，而它的值又是由  和 决定。因此的值实际上是由  决定的，它记住了之前完整的序列信息。需要强调的是，权重矩阵  并不会随着时间变化，而是固定的，即在每个时刻进行计算时使用的是同一个矩阵。这样做的好处一方面是减少了模型参数，另一方面也记住了之前的信息。\n如果把每个时刻的输入和输出值按照时间线展开，如下图所示：\n\n\n网络结构\n最简单的循环神经网络由一个输入层，一个循环层，一个输出层组成。输出层接收循环层的输出值作为输入并产生输出，它不具有记忆功能。输出层实现的变换为：\n\n\n函数g的类型根据任务而定，对于分类任务一般选用softmax函数，输出各个类的概率。结合循环层和输出层，循环神经网络完成的变换为：\n\n\n在这里只使用了一个循环层和一个输出层，实际使用时可以有多个循环层，即深度循环神经网络，在下一节中会详细介绍。\n\n\n深层网络\n上面我们介绍的循环神经网络只有一个输入层，一个循环层和一个输出层，这是一个浅层网络。和全连接网络以及卷积网络一样，我们可以把它推广到任意多个隐含层的情况，得到深度循环神经网络[11]。\n这里有3种方案，第一种方案为Deep Input-to-Hidden Function，在循环层之前加入多个普通的前馈层，将输入向量进行多层映射之后再送入循环层进行处理。\n第二种方案是Deep Hidden-to-Hidden Transition，它使用多个循环层，这和前馈型神经网络类似，唯一不同的是计算隐含层输出的时候需要利用本隐含层在上一个时刻的输出值。\n第三种方案是Deep Hidden-to-Output Function，它在循环层到输出层之间加入多前馈层，这和第一种情况类似。\n由于循环层一般用tanh作为激活函数，层次过多之后会导致梯度消失问题，和残差网络类似，可以采用跨层连接的方案。在语音识别、自然语言处理问题上，我们会看到深层循环神经网络的应用，实验结果证明深层网络比浅层网络有更好的精度。\n\n\n训练算法\n前面我们介绍了循环神经网络的结构，接下来要解决的问题是网络的参数如何通过训练确定。由于循环神经网络的输入是时间序列，因此每个训练样本是一个时间序列，包含多个相同维度的向量。解决循环神经网络训练问题的算法是Back Propagation Through Time算法，简称BPTT[2-4]，原理和标准的反向传播算法类似，都是建立误差项的递推公式，根据误差项计算出损失函数对权重矩阵、偏置向量的梯度值。不同的是，全连接神经网络中递推是在层之间建立的，而这里是沿着时间轴建立的。限于篇幅，在这里我们不详细介绍和推导BPTT的原理，如果有机会，SIGAI会在后续的公众号文章中给出。\n挑战与改进措施\n循环神经网络与其他类型的神经网络共同要面对的是梯度消失问题，对此出现了一些解决方案，如LSTM等。相比卷积神经网络，循环神经网络在结构上的改进相对要少一些。\n\n\n梯度消失问题\n和前馈型神经网络一样，循环神经网络在进行梯度反向传播时也面临着梯度消失和梯度爆炸问题，只不过这种消逝问题表现在时间轴上，即如果输入序列的长度很长，我们很难进行有效的梯度更新。对这一问题的解释和理论分析，SIGAI会在以后的文章中给出。文献[5]对循环神经网络难以训练的问题进行了分析。进一步的，文献[6]对这一问题作出了更深的解释，并给出了一种解决方案。\n\n\nLSTM\n长短期记忆模型（long short time memory，简称LSTM）由Schmidhuber等人在1997年提出[7]，与高速公路网络（highway networks）有异曲同工之妙。它对循环层进行改造，具体方法是使用输入门、遗忘门、输出门3个元件，通过另外一种方式由  计算\n。LSTM的基本单元称为记忆单元，它记住了上一个时刻的状态值。记忆单元在t时刻维持一个记忆值  ，循环层状态的输出值计算公式为：\n\n\n即输出门与状态值的乘积，在这里是向量对应元素相乘。其中  为输出门，是一个向量，按照如下公式计算：\n\n\n其中  为sigmoid函数，后面公式中含义相同。输出门控制着记忆单元中存储的记忆值有多大比例可以被输出。使用sigmoid函数这是因为它的值域是(0,1)，这样的所有分量的取值范围都在0和1之间，它们分别与另外一个向量的分量相乘，可以控制另外一个向量的输出比例。分别为输出门的权重矩阵和偏置向量。  是输出门的权重矩阵和偏置项，这里参数通过训练得到。\n记忆值  是循环层神经元记住的上一个时刻的状态值，随着时间进行加权更新，它的计算公式为：\n\n\n其中  是遗忘门，  是记忆单元在上一时刻的值，遗忘门决定了记忆单元上一时刻的值有多少会被传到当前时刻。上式表明，记忆单元当前值是上时刻值与当前输入值的加权和，记忆值只是个中间值。遗忘门的计算公式为：\n这里也使用了sigmoid函数，  分别为遗忘门的权重矩阵和偏置向量。  是输入门，控制着当前时刻的输入有多少可以进入记忆单元，其计算公式为：\n\n\n其中  分别为输入门的权重矩阵和偏置向量。这3个门的计算公式都是一样的，分别使用了自己的权重矩阵和偏置向量，这3个值的计算都用到了  和  ，它们起到了信息的流量控制作用。\n隐含层的状态值由遗忘门，记忆单元上一时刻的值，以及输入门，输出门共同决定。除掉3个门之外，真正决定  的只有  和  。总结起来，LSTM的计算思路为：\n输入门作用于输入信息，遗忘门作用于之前的记忆信息，二者加权和，得到汇总信息；最后通过输出门决定输出信息。\n所有的权重矩阵，偏置向量都通过训练得到，这和普通的循环神经网络没有区别，根据BPTT算法，我们可以得到这些参数的梯度值，在这里不详细介绍。\n\n\nGRU\n门控循环单元[8]（Gated Recurrent Units，简称GRU）是解决循环神经网络梯度消失的另外一种方法，它也是通过门来控制信息的流动。和LSTM不同的是，它只使用了两个门，把LSTM的输入门和遗忘门合并成更新门。在这里我们不详细介绍计算公式，感兴趣的读者可以阅读参考文献。\n\n\n双向网络\n前面介绍的循环神经网络是单向的，每一个时刻的输出依赖于比它早的时刻的输入值，这没有利用未来时刻的信息，对于有些问题，当前时刻的输出不仅与过去时刻的数据有关，还与将来时刻的数据有关，为此Schuster等人设计了双向循环神经网络[9]，它用两个不同的循环层分别从正向和反向对数据进行扫描。正向传播时的流程为：\n1.循环，对t=1,...T\n用正向循环层进行正向传播，记住每一个时刻的输出值\n结束循环\n2.循环，对对t=T,...1\n用反向循环层进行正向传播，记住每一个时刻的输出值\n结束循环\n3.循环，对所有的t，可以按照任意顺序进行计算\n用正向和反向循环层的输出值作为输出层的输入，计算最终的输出值\n结束循环\n下面用一个简单的例子来说明，假设双向循环神经网络的输入序列为  。首先用第一个循环层进行正向迭代，得到隐含层的正向输出序列：\n在这里由x1决定，由x1，x2决定，由x1 , . . . , x3决定，由x1 , . . . , x4 决定。即每个时刻的状态值由到当前时刻为止的所有输入值序列决定，这里利用的是序列的过去信息。然后用第二个循环层进行反向迭代，输入顺序是x4 , ..., x1，得到隐含层的反向输出序列：\n在这里，由x4决定，由x4, x3决定，由x4,...,x2 决定，由x4,...,x1决定。即每个时刻的状态值由它之后的输入序列决定，这里利用的是序列未来的信息。\n然后将每个时刻的隐含层正向输出序列和反向输出序列合并起来：\n\n\n送入神经网络中后面的层进行处理，此时，各个时刻的处理顺序是随意的，可以不用按照输入序列的时间顺序。\n\n\n多维网络\n循环神经网络的另外一个改进是拓展到多维的情况，得到多维网络，限于篇幅，在这里不详细介绍，感兴趣的读者可以阅读参考文献[18]。\n\n\n序列预测问题\n序列预测问题是一类问题的抽象，它的输入是一个序列，输出也是一个序列，而且输入和输出序列的长度是不固定的。这是循环神经网络最擅长处理的问题之一。\n\n\n序列标注问题\n序列标注问题[12]是一个抽象的概念，它泛指将一个序列数据映射成另外一个序列的任务，其本质是根据上下文信息对序列每个时刻的输入值进行预测。典型的序列标注问题包括语音识别，机器翻译，词性标注等。对于语音识别问题，输入数据是语音信号序列，输出是离散的文字序列；对于机器翻译问题，输入是一种语言的语句，即单词序列，输出是另外一种语言的单词序列；对于词性标注问题，输入是一句话的单词序列，输出是每个单词的词性，如名词、动词。\n与普通的模式分类问题相比，序列标注问题最显著的区别是输入序列数据的数据点之间存在相关性，输出的序列数据的数据点之间也存在相关性。例如，对于语音识别问题，一句话的语音信号在各个时刻显然是相关的；识别的结果单词序列组成，各个单词之间显然也具有相关性，它们必须符合词法和语法规则。\n序列标注问题的一个困难之处在于输入序列和输出序列之间的对齐关系是未知的。以语音识别问题为例，输入语音信号哪个时间段内的数据对应哪个单词的对应关系在进行识别之前并不知道，我们不知道一个单词在语音信号中的起始时刻和终止时刻。\n循环神经网络因为具有记忆功能，因此特别适合于序列标注任务。但是循环神经网络在处理这类任务时面临几个问题。第一个问题是标准的循环神经网络是单向的，但有些问题不仅需要序列过去时刻的信息，还需要未来时刻的信息。例如我们要理解一个句子中的某个词，它不仅与句子中前面的词有关，还和后门的词有关，即所谓的上下文语境。解决这个问题的方法是上面介绍的双向循环神经网络。\n第二个问题是循环神经网络的输出序列和输入序列之间要对齐。即每一个时刻的输出值与输入值对应，而有些问题中输入序列和输出序列的对应关系是未知的。典型的是语音识别问题，这在前面已经介绍。解决这个问题的经典方法是连接主义时序分类，即Connectionist Temporal Classification，简称CTC。\n根据输入序列和输出序列的对应关系，我们可以将序列标注问题分为三类。第一类为序列分类问题，它给输入序列赋予一个类别标签，即输出序列只有一个值，因此输出序列的长度为1。第二类问题为段分类问题，输入序列被预先分成了几段，每段为一个序列，为每一段赋予一个标签值，显然，第一种问题是第二种问题的一个特例。第三类问题为时序分类问题，对于这类问题，输入序列和输出序列的任何对齐方式都是允许的。显然，第二类问题是第三类问题的一个特例，因此这3类问题是层层包含关系。三类问题的关系如下图所示：\n\n\nCTC\n循环神经网络虽然可以解决序列数据的预测问题，但它要求输入的数据是每个时刻分割好并且计算得到的固定长度的特征向量。对于有些问题，对原始的序列数据进行分割并计算特征向量存在困难，典型的是语音识别。原始的声音信号我们很难先进行准确的分割，得到每个发音单元所对应的准确的时间区间。解决这类问题的一种典型方法是CTC技术。\nCTC[13]是一种解决从未分段的序列数据预测标签值的通用方法，在这里不要求将输入数据进行分割之后再送入循环神经网络中预测。2014年Graves等人将这一方法用于语音识别问题[14]，通过和循环神经网络整合来完成语音识别任务。CTC解决问题的关键思路是引入了空白符以及消除重复，以及用一个映射函数将循环神经网络的原始输出序列映射为最终需要的标签序列。\n假设训练样本集为S，训练样本服从概率分布  。输入空间是输入序列的集合，定义为：\n\n\n这是所有m维实向量序列的集合。目标空间是我们需要的预测结果序列的集合，定义为：\n\n\n这是建立在包含有限个字母集L之上的标签序列的集合，我们将L*中的元素称为标签序列。对于语音识别，L是文字字典，L*是识别出来的句子。训练样本集中的每个样本是一个序列对(x,z)。其中输入序列为：\n\n\n目标序列为：\n\n\n这有一个约束条件，目标序列的长度不大于输入序列的长度，即  。由于输出序列的长度与输入序列的长度可能不相等，因此无法用先验知识将它们对齐，即让输出序列的某些元素和输入序列的某一个元素对应起来。我们的目标是用训练样本集训练一个时序分类器：\n\n\n然后用它对新的输入序列进行分类。分类时，要让定义的某种误差最小化。要使用循环神经网络对时序数据进行分类，其中关键的步是将循环神经网络的输出值转换成某一个序列的条件概率值。这样，我们通过寻找使得这个条件概率最大化的输出序列来完成对输入序列的分类。\nCTC网络的输出层为softmax层，如果标签字母集中的字母个数为|L|，则这一层有|L|+1个神经元，其中前|L|个神经元表示在某一个时刻输出标签为每一个标签字母的概率，最后一个神经元的输出值为输出标签值为空的概率，即没有标签输出。这样，softmax层在各个时刻的输出值合并在一起，定义了各种可能的输出标签序列和输入序列进行对齐的方式的概率。任何一个标签序列的概率值可以通过对其所有不同的对齐方式的概率进行求和得到。\n假设输入序列的长度为T，循环神经网络的输入数据为m维，输出向量为n维，权重向量为w，它实现了如下的映射：\n\n\n我们将网络的映射写成  ，其中y是输出序列。在t时刻，网络第k个输出单元的值为  。在这里可以将解释为在t时刻观测标签k的概率。这个概率值定义了集合  中长度为T的序列所服从的概率分布，其中  {blank}，其中blank为空白符号，即：\n\n\n在这里我们将集合中的元素称为路径（path），记为  。接下来，我们定义一个多对一的映射，将神经网络的输出序列映射为最终需要的标签值序列：\n\n\n其中  是所有可能的输出标签序列的集合，即由字母集合中的字母组成的长度小于等于T的序列的集合。从神经网络的输出序列  得到目标标签序列的做法是消除空白符和连续的重复标签值。下面来看B函数作用于一个序列的例子：\n\n\n其中，-为空白符号。由于与一个标签序列对应的路径不止一个，因此目标标签序列的条件概率应该等于能得到它的所有路径的条件概率之和。我们借助映射B来定义一个标签序列  的条件概率，它等于所有映射后为l的路径  的概率之和：\n\n\n下面用一个简单的例子进行说明。如果标签字母集合为{a,b,c}，路径的序列长度为4，标签序列的长度为3。则标签序列l = abc所对应的所有可能路径  为：\n\n\n总共有7条路径和一个标签序列对应。基于上面的定义，CTC分类器的分类结果是给定输入序列，寻找上面的条件概率最大的那个输出序列：\n\n\n在这里，需要解决如何找到概率最大的输出序列的问题，而前面定义的框架只是计算给定的输出序列的条件概率。采用和隐马尔可夫模型类似的概念，我们称这一过程为解码，它们都是要得到概率最大的序列值。直接暴力枚举计算量太大，这里采用了动态规划建立递推公式进行计算，限于篇幅，我们不能详细介绍，在后面的文章中，SIGAI将对此展开讲解。\n\n\nseq2seq\n对有些问题，输入序列的长度和输出序列不一定相等，而且我们事先并不知道输出序列的长度，典型的是机器翻译问题。以机器翻译为例，将一种语言的句子翻译成另外一种语言之后，句子的长度即包括的单词数量一般是不相等的。以英译汉为例，英文句子“what's your name”是3个单词组成的序列，翻译成中文为“你叫什么名字”，由4个汉字词组成。标准的RNN没法处理这种输入序列和输出序列长度不相等的情况，解决这类问题的一种方法是序列到序列学习技术。\nSequence to Sequence Learning，即序列到序列的学习[15]，简称seq2seq，是用循环神经网络构建的一种框架，它能实现从一个序列到另外一个序列的映射，两个序列的长度可以不相等。seq2seq框架包括两部分，分别称为编码器和解码器，它们都是循环神经网络。这里要完成的是从一个序列到另外一个序列的预测：\n\n\n前者是源序列，后者是目标序列，两个序列的长度可能不相等。\n用于编码器的循环神经网络接受输入序列  ，最后时刻T产生的隐含层状态值\n作为序列的编码值，它包含了时刻1到T输入序列的所有信息，在这里我们将其简写为v，这是一个固定长度的向量。用于解码的RNN的初始隐含状态为v，它可以计算目标序列\n的条件概率：\n\n\n根据训练神经网络的输出值之间的关系，这个概率可以进一步写成：\n\n\n如果在输出层使用softmax函数映射，就可以到到上面每一个时刻的概率。实现时编码器和解码器同时训练，最大化上面的条件概率。在这里训练样本是成对的序列(A,B)，训练的目标是让序列A编码之后解码得到序列B的概率最大，即最大化如下的条件对数似然函数：\n\n\n其中N是训练样本数，  为要求解的参数。输入序列和对应的输出序列组合在一起为一个训练样本。下图是编码器对句子编码后的结果，在这里投影到2维平面上了：\n\n\nseq2seq框架有两种用法。第一种用法是为输入输出序列对打分，即计算条件概率值：\n\n\n第二种用法是根据输入序列生成对应的输出序列，由于seq2seq只有计算条件概率的功能，因此需要采用搜索技术得到条件概率最大的输出序列，可以使用集束搜索（beam search）技术。机器翻译问题采用的是第二种用法。seq2seq框架提供的是一种预测输出序列对输入序列的条件概率的手段。\n集束搜索通过在每一步对上一步的结果进行扩展来生成最优解。在每一步，选择一个词添加到之前的序列中，形成新的序列，并只保留概率最大的k个序列。在这里，k为人工设定的参数，称为集束宽度。\n下面用一个例子来说明集束搜索的原理。假设词典大小为3，包含的词为{a,b,c}。如果集束搜索的搜索宽度设置为2，则在选择第一个词的时候，寻找概率最大的两个词，假设为{a,b}。接下来，生成下一个词，对所有可能的组合{aa,ab,ac,ba,bb,bc}，保留概率最大的2个，假设为{ab,bb}。接下来，在这个基础上再选择第三个词，以此类推。最终得到概率最大的完整序列作为输出。\n\n\n典型应用\n循环神经网络被成功应用于各类时间序列数据的分析和建模，包括语音识别，自然语言处理，机器视觉中的目标跟踪、视频动作识别等。\n\n\n语音识别\n深度学习最早应用于语音识别问题时的作用是替代GMM-HMM框架中的高斯混合模型，负责声学模型的建模，即DNN-HMM结构。在这种结构里，深层神经网络负责计算音频帧属于某一声学状态的概率或者是提取出声音的特征，其余的部分和GMM-HMM结构相同。\n语音识别的困难之处在于输入语音信号序列中每个发音单元的起始位置和终止位置是未知的，即不知道输出序列和输入序列之间的对齐关系，这属于前面介绍的时序分类问题。\n深度学习技术在语音识别里一个有影响力的成果是循环神经网络和CTC的结合，和卷积神经网络、自动编码器等相比，循环神经网络具有可以接受不固定长度的序列数据作为输入的优势，而且具有记忆功能。文献[14]将CTC技术用于语音识别问题。语音识别中，识别出的字符序列或者音素序列长度一定不大于输入的特征帧序列。CTC在标注符号集中加上空白符号blank，然后利用循环神经网络进行标注，再把blank符号和预测出的重复符号消除。下图是CTC的原理：\n\n\n假设x为语音输入序列，l为识别出来的文字序列，  为循环神经网络的输出。可能有多个连续帧对应一个文字，有些帧可能没有任何输出，按照CTC的原理，用多对一的函数B把输出序列中重复的字符进行合并，形成一个唯一的序列：\n\n\n其中l为文字序列，  是带有冗余的循环神经网络输出。映射函数B将神经网络的输出序列映射成文字序列l。分类器的输出为对输入序列最可能的标签值：\n\n\n解码时采用的是前缀搜索技术。CTC在这里起到了对齐的作用，最显著的优势是实现了端到端的学习，无需人工对语音序列进行分割，这样做还带来了精度上的提升。\n在实现时循环神经网络采用了双向LSTM网络，简称BLSTM。训练样本集的音频数据被切分成10毫秒的帧，其中相邻帧之间有5毫秒的重叠，使用MFCC特征作为循环神经网络的输入向量。原始音频信号被转换成一个MFCC向量序列。特征向量为26维，包括了对数能量和一阶导数值。向量的每一个分量都进行了归一化。在解码时，使用最优路径和前缀搜索解码，解码的结果就是语音识别要得到的标记序列。\n\n\n文献[14]中的循环神经网络是一个浅层的网络，文献[17]提出了一种用深度双向LSTM网络和CTC框架进行语音识别的方法，这种方法主要的改进是使用了多个双向LSTM层，称为深度LSTM网络。。对于多层RNN网络，计算公式为：\n\n\n双向深度循环神经网络采用两套隐含层，分别正向、反向对输入序列进行处理，并把最后一个隐含层的输出值合并之后送到输出层，计算公式为：\n\n\n对于深度双向LSTM网络原理类似，只是把隐含层的变换换成LSTM结构的公式，在这里不再详细介绍。\n\n\n假设输入的声学序列数据为x，输出音素序列为y。第一步是给定输入序列和所有可能的输出序列，用循环神经网络计算出条件概率值p(y|x)。在训练时的样本为输入序列以及对应的输出序列。训练时的损失函数为对数似然函数：\n\n\n这里使用CTC来对序列z进行分类，对于一段输入的语音数据，分类的结果是一个音素序列。假设有k个音素，再加上一个空白符，是一个k+1类的分类问题。循环神经网络的最后一层为softmax层，输出k+1个概率值，在时刻t输出值为p(y|t)。\n神经网络在每一个时刻确定是输出一个音素，还是不输出即输出空白符。将所有时刻的输出值合并在一起，得到了一个输入和输出序列的对齐方案。CTC对所有的对齐方式进行概率求和得到p(z|x)。\n在使用CTC时，循环神经网络被设计成双向的，这样每个时刻的概率输出值为：\n\n\n其中N是隐含层的数量，y是神经网络的输出向量。上式用softmax映射根据神经网络的输出向量得到每一个音素的概率值。\n\n\n前面介绍的CTC框架输入是声学数据，输出是音素数据，只是一个声学模型。接下来还需要将音素序列转化成最终的文字序列作为识别结果，需要一个语言模型。在这里采用RNN transducer，一种集成了声学建模CTC和语言模型RNN的方法，后者负责将音素转化成文字，二者联合起来训练得到模型，我们称第一个网络为CTC网络，第二个网络为预测网络。\n假设和为CTC网络最后一个CTC最后一个隐含层的前向和后向输出序列，p为预测网络的隐含层输出序列。在每个时刻t，u为输出网络，它包含一个线性层，接受输入和，产生输出向量lt，另外还包含一个tanh隐含层，接受输入值lt和pu，产生输出值htu，最后将htu送入类的softmax层得到概率值p(k|t,u)。整个过程的计算公式为：\n\n\n\nRNN transducer只是给出了任何一个输出序列相对于输入序列的条件概率值，还需要解码算法得到概率最大的输出序列。在这里使用了集束搜索算法，算法给出n个最优的候选结果，选择的依据是概率值P(k|t)。\n整个系统的输入数据是对音频数据进行分帧后的编码向量，具体做法是对分帧后的音频数据进行傅里叶编码，然后40个傅里叶系数，加上能量，以及它们的一阶和二阶导数构成的向量，因此特征向量为123维。整个向量进行了归一化。在这里使用了61个音素，它们被映射为39个类。实验结果证明，更深的网络具有更高的准确率，双向LSTM比单向网络也有更高的精度。\n\n\n文献[19]提出了一种融合了卷积神经网络和循环神经网络的英语与汉语普通话语音识别算法。这也是一种完全端到端的方法，所有人工工程的部分都用神经网络替代，可以处理各种情况，包括噪声、各种语言。\n整个系统的输入为音频数据，使用20毫秒的窗口对原始音频数据分帧，然后计算对数谱，对功率进行归一化形成序列数据，送入神经网络中处理。首先是1D或者2D卷积层，然后是双向RNN，接下来全力连接的lookahead卷积层，最后是CTC分类器。整个模型也实现了端到端的训练。\n\n\n在每个时刻t神经网络的输出值为  。其中  为字母表中的符号或者是空格。对于英文为：\n{a,b,c,...,z,space,apotrohpe,blank}\n其中space为词之间的边界。对于中文输出值为简化的汉字字符。识别时CTC模型和语言模型结合起来使用。解码时使用集束搜索算法寻找输出序列y，最大化如下函数：\n\n\n第一部分为RNN的损失函数，第二部分为语言模型的损失函数，第三部分对英文为单词数，对汉语为字数，  和  为人工设定的权重参数。\n网络的最前端是卷积层，对输入的频谱向量执行1D或者2D卷积。实验结果证明2D卷积有更好的效果。\n整个网络包含多个循环层，循环层还使用了批量归一化技术，它可以作用于前一层和本层上一时刻状态值的线性加权和，也可以只作用于前一层的输入值。\n在所有循环层之前，加上了lookahead卷积层，计算公式为：\n\n\n其中d为前一层的神经元个数，h是前一层的输出值，W是  的权重矩阵，  为时间步长。除了上面介绍的这些论文，用循环神经网络进行语音识别的文章还很多，限于篇幅，不能一一列举，感兴趣的读者可以自己去阅读。\n\n\n自然语言处理\n自然语言处理的很多问题是时间序列问题，也是循环神经网络被广为应用的领域，下面介绍在一些典型问题上的使用情况。文献[30]为自然语言处理的很多问题提供了一个用循环神经网络解决的统一框架。这个框架用循环神经网络为句子序列进行编码，得到上下文语义信息，然后产生输出，如下图所示：\n\n\n中文分词\n汉语句子的词之间没有类似英文的空格，因此我们需要根据上下文来完成对句子的切分。分词的任务是把句子切分成词的序列，即完成我们通常所说的断句功能，它是解决自然语言处理很多问题的第一步，在搜索引擎等产品中都有应用。由于歧义和未登录词即词典里没有的新词的存在，中文分词并不是一件简单的任务。以下面的句子为例：\n乒乓球拍卖了\n显然这句话有歧义，对应于下面两种切分方案：\n乒乓球 拍卖 了\n乒乓球拍 卖 了\n句子中出现词典里没有的词也会影响我们的正确切分，例如下面的句子：\n李国庆节日在加班\n在这里李国庆是一个人名字，而国庆节也是一个合法的词，正确的分词需要程序知道李国庆是人名。\n最简单的分词算法是基于词典匹配，这又分为正向匹配，反向匹配和双向匹配3种策略。如果使用正向最大匹配，在分词时用词典中所有的词和句子中还未切分的部分进行匹配，如果存在多个匹配的词，则以长度最大的那个词作为匹配结果。反向最大匹配的做法和正向最大匹配类似，只是从后向前扫描句子。双向最大匹配则既进行正向最大匹配，也进行反向最大匹配，以切分的词较少的最为结果。显然，词典匹配无法有效的处理未登录词问题，对歧义切分也只能简单使用长度最大的词去匹配。词典匹配可以看作是解决分词问题的基于规则的方法。\n作为改进，可以采用全切分路径技术。这种技术列出一个句子所有切分的方案，然后选择出最佳的方案。随着句子的增长，这种方法的计算量将呈指数级增长。\n机器学习技术也被用于分词问题，采用序列标注的手段解决此问题。隐马尔可夫模型、条件随机场等方法为其中的代表。\n可以看成是序列标注问题，将一个句子中的每个字标记成各种标签。系统的输入是字序列，输出是一个标注序列，因此这是一个标准的序列到序列的问题。在这里，标注序列有这样几种类型：\n{B,N,E,S}\n其中B表示当前字为一个词的开始，M表示当前字为一个词的中间位置，E表示当前字为一个词的结束位置，S表示单字词。以下面的句子为例：\n我是中国人\n其分词结果为：\n我 是 中国人\n标注序列为：\n我/S 是/S 中/B 国/M 人/E\n同样的，我们可以用循环神经网络进行序列标注从而完成分词任务，在这里网络的输出是句子中的每个字，输出是每个字的类别标签。得到类别标签之后，我们就完成了对句子的切分。\n\n\n词性标注\n词性标注（POS Tagging）是确定一个句子中各个词的类别，它是和分词密切相关的一个问题。典型的分类有名词，动词，形容词和副词等。给定句子中的词序列，词性标注的结果是每个词的词类别。这也可以看成是一个序列标注问题，即给定一个句子，预测出句子中每个词的类别：\n\n\n最简单的是基于统计信息的模型，即从训练样本中统计出每种词性的词后面所跟的词的词性，然后计算最大的概率。除此之外，条件熵，隐马尔可夫模型，条件随机场等技术也被用于词性标注问题。\n同样的，词性标注问题可以看做是一个序列标注问题。将循环神经网络用于词性标注时，输入序列是一个句子的单词序列，每个时刻的输入向量是单词的one-hot编码向量，网络的输出为单词属于某一类次的概率，此时输出层可以采用softmax作为激活函数。在这里，典型的标注集合为：\n{v,n,a,...}\n其中v为动词，n为名字，a为形容词，其他词性在这里不详细列出。训练时，也使用端到端的方案，直接给定语句和对应的标签序列。神经网络的预测输出就是每个词的词性类别值。\n\n\n命名实体识别\n命名实体识别（Named Entity Recognition，简称NER）又称为专名识别，其目标是识别文本中有特定含义的实体，如人名、地名、机构名称、专有名词等，属于未登录词识别的范畴。命名实体识别和其他自然语言处理问题相比存在的一个困难是训练样本的缺乏，因为未登录词很少有重复的，基本上都是新词。\n如果直接用序列标注的方法解决命名实体识别，思路和分词类似，这里要识别出句子里所有的专名词。假设要识别的专有词包括人名，地名，组织机构名称，则标注集合为：\n{BN,MN,EN,BA,MA,EA,BO,MO,EO,O}\n其中BN表示这个字是人名的开始，BN表示人名的中间字，EN表示人名的结束；BA表示地名的开始，MA表示地名的中间字，EA表示地名的结束；BO表示机构名称的开始，MO表示机构名称的中间字，EO表示机构名称的结束；O表示这个字不是命名实体。给定所有训练样本句子的标注序列，我们就可以实现端到端的训练。预测时输入一个句子，输出标签序列，根据标签序列我们可以得到命名实体识别的结果。\n除了这种最直接的序列标记手段，还更复杂的方法。文献[31]提出了一种用LSTM和条件随机场CRF进行命名实体识别的方法。假设LSTM网络的输入序列是 ，输出序列是  。其中，输入序列是一个句子所有的单词，这些单词被编码为向量。\nLSTM在t时刻的输出向量是句子中第t个单词的左上下文。单词的右上下文也是非常重要的信息，也通过LSTM计算得到，具体做法是将整个句子颠倒过来送入LSTM中计算，第个时刻的输出向量即为右上下文。在这里，称第一个LSTM为前向LSTM，第二个为后向LSTM。它们是两个不同的神经网络，分别有各自的参数。这种结构也称为双向LSTM。\n每个词用它的左上下文和右上下文联合起来表示，即将两个向量拼接起来：\n\n\n接下来用条件随机场对句子中的所有词进行联合标注。对于一个句子，假设矩阵P是双向LSTM输出的得分矩阵。这是一个NxK的矩阵，其中k是不同的标记个数。元素\n为第i个单词被赋予第j个标记的概率。对于预测输出序列y，它的得分定义为：\n其中矩阵A是转移得分矩阵，其元素表示从标记i转移到标记j的得分。  和  是句子的开始和结束标记，我们把它们加入到标记集合中。因此矩阵A是一个k+2阶方阵。\n对所有可能的标记序列的softmax值定义了序列的概率：\n其中  为句子X所有可能的标记序列。在解码时将具有最大得分的序列作为预测输出：\n这可以通过动态规划算法得到。根据输出序列的值，我们就可以直接得到命名实体识别的结果。\n\n\n文本分类\n文本分类是自然语言处理中的重要问题，经典的机器学习算法如支持向量机、贝叶斯分类器等都曾被用于解决此问题。卷积神经网络在文本分类问题中也有应用。除了这些方法之外，循环神经网络也被成功的应用于文本分类问题。\n\n\n文献[34]设计了一种用分层注意力网络进行文本分类的方案。在这种方案里采用了分层的结构，首先建立句子的表示，然后将它们聚合，形成文档的表示。在文档中，不同的词和句子所蕴含的有用信息是不一样的，而且重要性和文档上下文有密切的关系。因此，采用了两层的注意力机制，第一个是单词级的，第二个是句子级的。在提取文档的表示特征时，会关注某些词和句子，也会忽略一些词和句子。\n整个网络由一个单词序列编码器，一个单词级注意力层，一个句子编码器，一个句子级注意力层组成。单词序列编码器由GRU循环神经网络实现。网络的输入是一个句子的单词序列，输出是句子的编码向量。\n假设一篇文档有L个句子  ，句子有  个词。  表示第i个句子中的第t个单词，其中  。HAN将文档投影为一个向量，然后对这个向量进行分类。\n第一步是采用词嵌入技术将一个句子的单词转换为一个向量。计算公式为：\n在这里  称为嵌入矩阵。然后用双向GRU网络对词序列进行编码：\n具体做法参考双向RNN和双向LSTM。得到隐含层的状态值：\n\n\n将这个状态值作为句子的表示。句子中的不同单词有不同的重要性，在这里采用了注意力机制。它的计算公式为：\n\n\n首先将  输入一个单层的MLP，得到它的隐含层表示  ，这个单词的重要性由向量与单词级上下文向量  的相似度来衡量。通过softmax函数，最后得到归一化的重要性权重值 。接下来计算句子向量  ，它是词向量的加权平均，加权值为每个词的重要性权重。在这里，上下文向量  被随机初始化，并且在训练过程中和神经网络一起训练得到。\n在得到句子向量之后，我们可以用类似的方式得到文档向量。在这里，使用双向GRU对句子进行编码：\n\n\n将这两个向量合并，得到句子的编码向量：\n\n\n这个编码综合第i个句子周围的句子，但还是聚焦于第i个句子。类似的，我们用句子级的注意力机制来形成文档的表示向量：\n\n\n在这里v是文档向量，它综合了文档中所有句子的信息。同样的，向量\n通过训练得到。\n最后用文档向量来对文档进行分类：\n\n\n训练时的损失函数采用负对数似然函数，定义为：\n\n\n其中j是第d个文档的类别标签值。采用注意力机制，可以直接把对分类有贡献的词和句子显示出来，便于理解和调试分析。\n\n\n自动摘要\n自动摘要的目标是给定一段文本，得到它的摘要信息，摘要信息浓缩了文本的内容，和输入文本有相同的语义，体现了文章的主要内容。在这里，输入文本可以是一句话或者多句话。摘要输出语句的词汇表和输入文本的词汇表相同。可以将自动摘要也看成是一个序列到序列的预测问题，输出序列的长度远小于输入序列的长度。\n\n\n文献[35]提出了一种使用注意力机制和seq2seq技术的新闻类文章标题生成算法。在这里，先用seq2seq的编码网络生成文本的抽象表示，解码器网络在生成摘要的每个单词的时候使用注意力机制关注文本中的重点词。\n首先，新闻文章的每个单词被依次输入编码网络，单词首先被送入嵌入层，生成概率分布表示。然后，被送入有多个隐含层组成的训练神经网络。所有词被输入网络处理之后，最后一个隐含层的状态值将用来作为解码器网络的输入。\n\n\n接下来将作为解码器网络的初始状态。首先将一个结束符end-of-sequences，简称EOS，输入解码器网络，用softmax层和注意力机制生成每一个摘要单词，最后以EOS结束。在生成每一个单词时，将生成的上一个单词作为解码器网络的输入。\n训练时的损失函数定义为：\n其中  是输入文本的单词序列，  是生成的摘要单词序列。训练时，解码器在每个时刻的输入为真实的标题中的单词，而不是上一时刻生成的单词。在测试时，则使用的是上一时刻生成的单词。但这样做会造成训练和预测时的脱节，作为补救，在训练时随机的使用真实的单词和上一时刻生成的单词作为输入。在预测时，使用集束搜索技术生成每一个输出单词。\n在解码器生成每个输出单词时使用了注意力机制。对于每一个输出单词，注意力机制为每个输入单词计算一个权重值，这个权重值决定了对每个输入单词的关注度。这些权重的和为1，并被用于计算最后一个隐含层的输出值的加权平均值，在这里，每次处理完一个输入单词，会产生一个输出值，最后是对这些输出值进行平均。这个加权平均值被看做是文档的上下文信息，接下来，它和解码器当前解码时最后一个隐含层的输出值一起被送入softmax层进行计算。\n\n\n机器翻译\n统计机器翻译采用大量的语料库进行学习，训练样本为源语言和目标语言的语句。得到模型之后，对于一个语句，算法直接使用这个模型得到目标语言的语句。如果用统计学习的方法，机器翻译要解决的问题是，给定一个输入句子a，对于另外一种语言所有可能的翻译结果b，计算条件概率：\n\n\n概率最大的句子就是翻译的结果。使用机器学习的翻译有基于词的翻译和基于短语的翻译两种方法。前者对词进行翻译，不考虑上下文语境和词之间的关联，后者对整个句子进行翻译，目前主流的是基于短语的翻译。\n我们可以将机器翻译问题抽象成一个序列  到另外一个序列  的预测：\n和语音识别之类的应用不同，这里的序列到序列映射并不是一个单调映射，也就是说，输出序列的顺序是按照输入序列的顺序来的。这很容易理解，将一种语言的句子翻译成另一一种语言的句子时，源语言种的每个单词的顺序和目标语言种每个单词的顺序不一定是一致的。\n训练时的目标是对所有的样本最大化下面的条件概率：\n因此我们需要在所有可能的输出序列中寻找到上面的条件概率值最大的那个序列作为机器翻译的输出。如果用神经网络来对机器翻译进行建模，称为神经机器翻译。当前，用循环神经网络解决机器翻译问题的主流方法是序列到序列学习技术。\n文献[15]提出了用seq2seq技术解决机器翻译问题。在这里，使用编码器对输入的输入序列进行特征编码，得到这句话的意义，然后用解码器对这个意义进行解码并得到概率最大的输出序列，这就得到了翻译的结果。\n先将源句子表示成向量序列\n，在这里每个向量是一个词的编码向量。通过第一个循环神经网络，当我们输入完这个序列之后得到最后时刻的隐含层状态值  ，在这里简记为v。这个值包含了整个句子的信息。\n接下来用解码器生成翻译序列。解码器循环神经网络银行层的初始状态值为v，它输出向量序列  。对于所有可能的输出序列，我们都可以用解码器计算出它的条件概率值，在这里要寻找概率值最大的那个序列。如果枚举所有可能的输出序列，计算量太大，显然是不现实的。在这里采用了集束搜索技术。\n训练样本是成对的句子，即源句子和它的翻译结果。训练的目标是最大化对数概率值：\n其中D是训练样本集，S是源句子，T是翻译的句子。训练完成之后，可以用这个模型来进行翻译，即寻找概率最大的输出序列：\n在这里采用了自左到右的集束解码器。它维持K个最有可能的部分结果，部分结果是整个翻译句子的前缀部分。在每一步，我们在词典的范围内用每一个可能的词扩展这个部分结果。然后用seq2seq模型计算这些部分结果的概率，保留概率最大的个部分结果。当输入结束符之后，整个翻译过程结束。\n在实现时，无论是在训练阶段还是测试阶段，都将句子反序输入，但是预测结果序列是正序而不是反序。另外，并没有采用单个隐含层的循环神经网络，而是采用了4层的LSTM网络。\n\n\n文献[36]提出了一种用编码器-解码器框架进行机器翻译的方法。在这里，编码器-解码器框架的结构和之前介绍的相同。\n不同的是，使用了一种新的隐藏单元，即循环层的激活函数。这种激活函数和LSTM类似，但计算更简单。在这里，使用了两个门来进行信息流的控制，分别称为更新门和复位门。复位门的计算公式为：\n更新门的计算公式为：\n隐含层的变换公式为：\n在这里，更新门用来控制新老信息的权重。其中：\n假设e为源语句，f为翻译后的目标语句。根据贝叶斯公式，机器翻译的目标是给定源语句，寻找使得如下条件概率最大的目标语句：\n上式右边的第一项为转换模型，第二项为语言模型，这和语音识别类似。大多数机器翻译算法将转换模型表示成对数线性模型：\n其中  为第n个特征，  为特征的权重，  为归一化因子。在这里编码器-解码器框架用于对对数线性模型的翻译候选结果短语进行评分。\n文献[37]提出了一种使用了双向循环神经网络的机器翻译算法，循环层也使用了重置门和更新门结构。\n解码器用循环神经网络实现，它根据当前状态，以及当前的输出词预测下一个输出词，计算公式为：\n其中  为解码器网络隐含层的状态。这个框架采用了注意力机制，计算方法和之前介绍的相同。\n文献[38]介绍了Google的机器翻译系统。他们的系统同样采用了编码器-解码器架构，两个网络都由深层双向LSTM网络实现，并采用了注意力机制。\n这里的深层双向LSTM网络和前面介绍的相同，不再重复讲述。为了克服深层带来的梯度消失问题，隐含层采用了残差网络结构，即跨层连接。\n训练时的目标是最大化对数似然函数，即对数条件概率值：\n在这里  是要求解的参数。同样的，解码时也使用了集束搜索算法。\n机器视觉\n对于机器视觉中的某些问题，循环神经网络也取得了很好的效果。在这些问题中，数据都被抽象成一个时间序列，如物体运动的动作，状态等。\n\n\n字符识别\n如果我们知道每个字符的笔画信息，即整个字的书写过程，则可以将手写字符识别看成是一个轨迹分类问题。每个手写字符是一个序列数据，每个时刻的坐标连接起来，在平面上构成一个字符的图像。手写字符识别属于序列标记问题中的序列分类问题，即给定一个字符的坐标点序列，预测这个字符的类别。在这里，循环神经网络的输入为坐标点序列，输出值为类别，为了达到这个目的，我们可以将最后一个时刻的循环层输出值映射为类别概率，这可以通过softmax层实现。\n另外，也可以直接以图像作为输入，在这里，将图像看作是一个序列，序列中的每一个向量是图像中的一个行的像素。依次将每一行输入循环神经网络，最后时刻的隐含层状态输出作为提取的字符特征，送入softmax层进行分类。\n\n\n目标跟踪\n运动跟踪可以抽象为已知目标在之前时刻的坐标，预测出它在当前时刻的坐标，这同样是一个序列预测问题。\n文献[42]提出了一种用循环神经网络进行目标跟踪的方法，称为RTT。RTT主要目标是解决目标遮挡问题。循环神经网络的作用是得到置信度图，即每个点处是目标的概率。下面介绍这种方法的处理流程。\n在对每一帧进行跟踪时，给定目标在上一帧中的矩形框，以目标的中心为中心，以目标宽高的2.5倍为宽高，即将目标矩形放大2.5倍，得到一个矩形的候选区域。然后，将这个候选区域划分成网格。然后对每个矩形框提取特征，可以使用HOG特征，也可以使用更复杂的卷积网络提取的特征。在这里，划分网格而不是对整个候选区域计算特征的原因是这样做能够更好的处理遮挡，以及目标外观的变化。最后我们得到候选区域的特征。\n然后，以这个特征作为输入，用多维RNN对特征进行处理，得到置信度图。最后根据置信度图完成对目标位置的预测。\n\n\n和单个目标跟踪不同，多目标跟踪需要解决数据关联问题，即上一帧的每个目标和下一帧的哪个目标对应，还要解决新目标出现，老目标消失问题。多目标的跟踪的一般流程为每一时刻进行目标检测，然后进行数据关联，为已有目标找到当前时刻的新位置，在这里，目标可能会消失，也可能会有新目标出现，另外目标检测结果可能会存在虚警和漏检测。联合概率滤波，多假设跟踪，线性规划，全局数据关联，MCMC马尔可夫链蒙特卡洛算法先后被用于解决数据关联问题来完成多个目标的跟踪。\n首先我们定义多目标跟踪的中的基本概念，目标是我们跟踪的对象，每个目标有自己的状态，如大小、位置、速度。观测是指目标检测算法在当前帧检测出的目标，同样的，它也有大小、位置、速度等状态值。在这里，我们要建立目标与观测之间的对应关系。下图是数据关联的示意图：\n在上图中，第一列圆形为跟踪的目标，即之前已经存在的目标；第二列圆为观测值，即当前帧检测出来的目标。在这里，第1个目标与第2个观察值匹配，第3个目标与第1个观测值匹配，第4个目标与第3个观测值匹配。第2个和第5个目标没有观测值与之匹配，这意味着它们在当前帧可能消失了，或者是当前帧被漏检，没有检测到这两个目标。类似的，第4个观测值没有目标与之匹配，这意味着它是新目标，或者虚警。\n文献[43]提出了一种用循环神经网络在线跟踪多个目标的算法。这种方法实现了完全端到端的训练。在这里，用LSTM循环神经网络同时解决数据关联、新目标出现、老目标消失问题。\n首先定义状态向量  ，这是一个NxD维向量，表示t时刻所有目标的状态值，其中，D为每个目标的状态个数，在这里值为4，分别为目标的位置和宽高。定义N为某一帧中能够同时跟踪的最大目标个数。  为第i个目标的状态。\n类似的定义观测向量  ，这是一个MxD维向量，表示t时刻所有观测值。其中M为每一帧中最大检测目标个数。需要注意的是，我们对模型能够处理的最大目标个数并没有限制。\n接下来定义分配概率矩阵A，这是一个Nx(M+1)的矩阵，元素取值0和1之间的实数。矩阵的每一行为一个目标的分配概率向量，即元素  表示将第i个目标分配给第j个观测的概率。分配概率矩阵满足约束条件：\n在这里矩阵的列数不是M而是M+1，这是因为一个目标可能不和任何一个观测向匹配。\n最后定义指示向量  ，这是一个N维向量，每个元素表示一个目标存在的概率值。\n跟踪问题被分成两个部分来解决：状态预测与更新，以及跟踪管理；数据关联。前一部分负责单个目标的状态跟踪；后一部分解决目标之间的对应关系。\n对于第一个问题，用一个时序循环神经网络来学习N个目标的运动模型，以及目标的指示变量，指示变量用于处理目标的出现与消失。在时刻t，循环神经网络输出四种值：\n1.包括所有目标的状态预测值  ，前面已经介绍过。\n2.所有目标状态的更新值  。\n3.指示向量  ，其每个元素的值位于(0,1)之间，表示目标是一个真实轨迹的概率。\n4.  ，这是与  的差值。\n神经网络的输入为前一个时刻的状态值  ，前一个时刻的指示向量值  ，当前时刻的观测值  ，以及当前时刻的数据关联矩阵  ，数据关联矩阵的计算方法将在后面介绍。\n这个功能模块有三个目标：\n1.预测。为指定数量的目标学习一个复杂的运动模型，这个模型包含了每个目标的运动参数，包括速度，加速度信息等。\n2.更新。根据当前的观测数据，对预测值进行校正，修正物体的状态值，包括运动状态值。\n3.目标的出现与消失。学习到如何根据目标的状态值、当前时刻的观测值，以及数据关联信息来处理新目标的出现，已有目标的消失问题。\n预测值  只取决于状态值  和循环神经网络隐含层的状态值  。一旦数据关联矩阵  已经确定，即已经知道了目标和观测之间的对应关系，我们就可以根据观测值来更新状态值，完成校正。接下来，将观测值和预测的状态值拼接在一起：\n然后乘以矩阵  。同时  也被计算出来。在确定了网络的输出和输出之后，我们需要定义训练时的损失函数。损失函数定义为：\n其中  为预测值，  为真实值。上面损失函数的第一部分为预测误差，第二部分为更新误差，第三和第四部分为目标消失、出现以及回归值误差。这里只是定义了某一个时刻的误差值，训练时需要将每一帧的误差值累加起来，然后计算平均值。第一部分误差的意义是在没有观察值的情况下，预测值要和目标的真实运动轨迹尽可能接近。第二部分的意义是得到观测值之后，要将预测值校正到和观测值尽可能接近。\n第三部分损失反应了目标的出现与消失。如果  ，表示一个目标存在，如果\n，表示这个目标不存在。为此我们定义交叉熵损失函数：\n最后一个问题是数据关联。数据关联的目标是为每个目标分配一个唯一的观测值，这是一个组合优化问题，直接求解的话是NP完全问题。在这里，采用LSTM网络通过学习来解决此问题。在这里，网络的输入是成对距离矩阵C，这是一个NxM的矩阵，矩阵元素定义为：\n即第i个目标的预测状态与第j个观察值之间的欧氏距离。当然，我们也可以使用更多的信息，如目标的外观或其他相似度。网络的输出值为概率向量  ，表示第i个目标与所有观测值之间的分配概率，这可以通过softmax层输出。这里的是数据关联矩阵的第i行。最后，我们定义网络训练时的损失函数为：\n其中  是一个标量，是目标i的真实分配值，即将目标i分配给观测\n视频分析\n视频动作识别是机器视觉领域的一个重要问题，它的目标是对运动物体的动作进行分类，如人的站立，坐下等动作。动作识别在诸多领域有实际的应用，如视频监控、人机交互、游戏控制等。这个问题可以抽象成一个时间序列分类问题。以人的动作识别为例，它的输入是目标关键点坐标序列，如人体一些关键点的2D或3D坐标，输出值为动作类别，即序列的标签值。\n\n\n文献[45]提出了一种整合了卷积神经网络和循环神经网络的框架进行人体动作分类的方法。整个系统包括一个3D卷积神经网络和一个循环神经网络。其中，3D卷积神经网络的输入为多张图像，用于提取一段视频的时空特征。然后将提取的特征序列送入循环神经网络中进行分类。\n在这里，卷积神经网络的输入为3D图像。整个视频被分成一系列的固定长度片段，每个片段包括相同数量的帧，被处理成固定大小的输入图像。第三个卷积层后面是两个全连接层，最后一个全连接层有6个神经元，即卷积网络的输出向量为6维。\n接下来将卷积得到的固定长度的特征向量序列送入LSTM循环神经网络。用循环神经网络的输出完成对视频的分类。\n文献[46]提出了一种用双向LSTM循环神经网络进行3D手势分类的方法。在这里，每个时刻用加速度计和陀螺仪测量出手在3D空间的加速度和角速度，形成一个6D的向量，作为循环神经网络的输入，这是一个序列数据。循环神经网络采用双向LSTM网络。循环神经网络的输出向量维数和要分类的手势类型数相同，最后通过softmax层产生概率输出用于分类。这些都是标准的做法，不再详细讲述。\n文献[47]提出了一种用分层循环神经网络进行人体动作识别的方法，在这里，利用了人体骨架的关键点信息，对骨架关键点的运动轨迹进行分析。\n整个人体被分成5个部分进行建模，分别为四肢和躯干。整个处理流程为：\n1.将5个部分分别送入5个子网络中进行处理\n2.将四肢和躯干在第一步中的处理结果分别进行融合，送入4个子网络中进行处理\n3.将两只胳膊，两条腿，躯干在第二步中的处理结果进行融合，送入2个子网络中进行处理\n4.将上一步中的两个结果融合，送入第4层子网络中进行处理\n5.将上一步的结果送入全连接层中进行处理\n6.最后用softmax层进行计算，得到分类概率\n在这里，所有循环层都使用双向循环结构，前面3个循环层都采用tanh激活函数，最后一个循环层采用LSTM单元。循环层和全连接层的计算方式和前面介绍的标准结构相同，在这里不详细讲述。\n全连接层在各个时刻的输出向量被累计起来，然后用softmax层进行概率输出。整个网络的输入为人体各个部位关键点的3D坐标，送入网络之前，对坐标进行了归一化处理；要识别的动作类型根据实际应用而定。\n文献[48]提出了一种整合卷积神经网络和循环神经网络的视频识别方法。在这里，用卷积网络提取单帧图像的特征，多个帧的特征依次被送入循环神经网络中进行处理。这种结构不仅在空间上具有深度，在时间上也具有深度，称为Long-term Recurrent Convolutional Networks，简称LRCNs。\n整个系统的输入是一系列的视频帧，对于每一帧，首先经过卷积网络的作用，产生固定长度的输出向量。经过这一步，我们得到一个固定长度的序列数据：\n这个序列数据被送入循环神经网络中进行处理，得到输出值。最后，经过softmax层，得到概率输出。这里的卷积网络和循环神经网络的变换和前面介绍的标准做法一致，不再重复介绍。\n假设循环神经网络的学习参数为V和W，训练时的损失函数定义为：\n这一框架可以用于以下三种情况：\n1.序列输入，固定长度输出。即实现映射  ，典型的是视频动作识别。在这里输入是多个视频帧，输出是动作类别。\n2.固定长度输入，序列输出。即实现映射  ，典型的是生成图像的描述，如给图像生成文字说明。\n3.序列输入，序列输出。即实现映射  ，典型的是视频描述，如为一段视频生成一段文字解说。\n\n\n参考文献\n[1] Mikael Boden. A guide to recurrent neural networks and backpropagation. 2001.\n[2] Ronald J Williams, David Zipser. A learning algorithm for continually running fully recurrent neural networks. 1989, Neural Computation.\n[3] Fernando J Pineda. Generalization of back-propagation to recurrent neural networks. 1987, Physical Review Letters.\n[4] Paul J Werbos. Backpropagation through time: what it does and how to do it. 1990, Proceedings of the IEEE.\n[5] Xavier Glorot, Yoshua Bengio. On the difficulty of training recurrent neural networks. 2013, international conference on machine learning.\n[6] Y. Bengio, P. Simard, P. Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks, 5(2):157-166, 1994.\n[7] S. Hochreiter, J. Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735-1780, 1997.\n[8] Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau , Fethi Bougares, Holge. Learning Phrase Representations using RNN Encoder--Decoder for Statistical Machine Translation. 2014, empirical methods in natural language processing.\n[9] M. Schuster and K. K. Paliwal. Bidirectional recurrent neural networks. IEEE Transactions on Signal Processing, 45(11):2673-2681, 1997.\n[10] Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio. Gated Feedback Recurrent Neural Networks. 2015, international conference on machine learning.\n[11] Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio. How to Construct Deep Recurrent Neural Networks. 2014, international conference on learning representations.\n[12] Alex Graves. Supervised Sequence Labelling with Recurrent Neural Networks.\n[13] Alex Graves, Santiago Fernandez, Faustino J Gomez, Jurgen Schmidhuber. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks.2006, international conference on machine learning.\n[14] Alex Graves, Navdeep Jaitly. Towards End-To-End Speech Recognition with Recurrent Neural Networks. 2014, international conference on machine learning.\n[15] Ilya Sutskever, Oriol Vinyals, Quoc V Le. Sequence to Sequence Learning with Neural Networks.2014, neural information processing systems.\n[16] Oriol Vinyals, Suman Ravuri, and Daniel Povey. Revisiting Recurrent Neural Networks for Robust ASR. ICASSP, 2012.\n[17] A. Graves, A.Mohamed, G. Hinton, Speech Recognition with Deep Recurrent Neural Networks, ICASSP 2013.\n[18] Alex Graves, Santiago Fernandez, Juergen Schmidhuber. Multi-dimensional recurrent neural networks. 2007, international conference on artificial neural networks.\n[19] Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric Battenberg. Deep speech 2: end-to-end speech recognition in English and mandarin. 2016, international conference on machine learning.\n[20] Hasim Sak, Andrew W Senior, Kanishka Rao, Francoise Beaufays. Fast and Accurate Recurrent Neural Network Acoustic Models for Speech Recognition. 2015, conference of the international speech communication association\n[21] Miao, Yajie, Mohammad Gowayyed, and Florian Metze. EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding. 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). IEEE, 2015.\n[22] Bahdanau, Dzmitry, et al. End-to-end attention-based large vocabulary speech recognition. 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016.\n[23] Chan, William, et al. Listen, attend and spell: A neural network for large vocabulary conversational speech recognition. 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016.\n[24] H. Sak, Hasim, Senior, Andrew, and Beaufays, Francoise. Long short-term memory recurrent neural network architectures for large scale acoustic modeling. In Inter speech, 2014.\n[25] Sainath, Tara, Vinyals, Oriol, Senior, Andrew, and Sak, Hasim. Convolutional, long short-term memory, fully connected deep neural networks. In ICASSP, 2015.\n[26] Chorowski, Jan, Bahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua. End-to-End continuous speech recognition using attention-based recurrent nn: First results. abs/1412.1602, 2015. http://arxiv.org/1412.1602\n[27] Hannun, Awni, Case, Carl, Casper, Jared, Catanzaro, Bryan, Diamos, Greg, Elsen, Erich, Prenger, Ryan, Satheesh, Sanjeev, Sengupta, Shubho, Coates, Adam, and Ng, Andrew Y. Deep speech: Scaling up end-to-end speech recognition. 1412.5567, 2014a. http://arxiv.org/abs/1412.5567.\n[28] A.Graves. Sequence transduction with recurrent neural networks. ICML Representation Learning Workshop, 2012.\n[29] Bahdanau, Dzmitry, Chorowski, Jan, Serdyuk, Dmitriy, Brakel, Philemon, and Bengio, Yoshua. End-to-end attention-based large vocabulary speech recognition. abs/1508.04395, 2015. http://arxiv.org/abs.1508.04395.\n[30] Tomas Mikolov, Martin Karafiat, Lukas Burget, Jan Cernocký, Sanjeev Khudanpur. Recurrent neural network based language model. 2010, conference of the international speech communication association.\n[31] Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, Chris Dyer. Neural architectures for named entity recognition. 2016, north american chapter of the association for computational linguistics.\n[32] Peilu Wang, Yao Qian, Frank K Soong, Lei He, Hai Zhao. Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Recurrent Neural Network. 2015, Computation and Language.\n[33] Siwei Lai, Liheng Xu, Kang Liu, Jun Zhao. Recurrent convolutional neural networks for text classification. 2015, national conference on artificial intelligence.\n[34] Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alexander J Smola, Eduard H Hovy. Hierarchical Attention Networks for Document Classification. 2016, north american chapter of the association for computational linguistics.\n[35] Konstantin Lopyrev. Generating News Headlines with Recurrent Neural Networks. 2015, arXiv: Computation and Language.\n[36] Kyunghyun Cho,Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares. Learning Phrase Representations using RNN Encoder--Decoder for Statistical Machine Translation. 2014, empirical methods in natural language processing.\n[37] Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio. Neural Machine Translation by Jointly Learning to Align and Translate. 2015, international conference on learning representations.\n[38] Yonghui Wu, et al. Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. Technical Report, 2016.\n[39] Graves, Alex. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850 (2013).\n[40] Shujie Liu, Nan Yang, Mu Li, Ming Zhou. A Recursive Recurrent Neural Network for Statistical Machine Translation. 2014, meeting of the association for computational linguistics.\n[41] Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling. 2014, arXiv: Neural and Evolutionary Computing.\n[42] Zhen Cui, Shengtao Xiao, Jiashi Feng, Shuicheng Yan. Recurrently Target-Attending Tracking. 2016, computer vision and pattern recognition.\n[43] Anton Milan, Seyed Hamid Rezatofighi, Anthony R Dick, Ian D Reid, Konrad Schindler. Online Multi-target Tracking using Recurrent Neural Networks. 2016, national conference on artificial intelligence.\n[44] Peter Ondruska, Ingmar Posner. Deep tracking: seeing beyond seeing using recurrent neural networks. 2016, national conference on artificial intelligence.\n[45] M. Baccouche, F. Mamalet, C. Wolf, C. Garcia, and A. Baskurt. Sequential deep learning for human action recognition. In Human Behavior Understanding, pages 29-39. Springer, 2011.\n[46] G. Lefebvre, S.Berlemont, F.Mamalet, and C.Garcia. Blstm-rnn based 3d gesture classification. In Artificial Neural Networks and Machine Learning, pages 381-388. Springer, 2013.\n[47] Y.Du, W.Wang and L.Wang. Hierarchical recurrent neural network for skeleton based action recognition. CVPR 2015.\n[48] J.Donahue, L.A.Hendricks, S.Guadarrama, M.Rohrbach, S.Venugopalan, K.Saenko, and T.Darrell. Long-term recurrent convolutional networks for visual recognition and description. arXiv preprint arXiv:1411.4389, 2014.\n[49] A.Grushin, D.D.Monner, J.A.Reggia, and A.Mishra. Robust human action recognition via long short-term memory. In International Joint Conference on Neural Networks, pages 1-8, IEEE, 2013.\n[50] Antoine Miech, Ivan Laptev, Josef Sivic. Learnable pooling with Context Gating for video classification. 2017, Computer Vision and Pattern Recognition.\n\n\n推荐阅读\n[1] 机器学习-波澜壮阔40年 SIGAI 2018.4.13.\n[2] 学好机器学习需要哪些数学知识？SIGAI 2018.4.17.\n[3] 人脸识别算法演化史 SIGAI 2018.4.20.\n[4] 基于深度学习的目标检测算法综述 SIGAI 2018.4.24.\n[5] 卷积神经网络为什么能够称霸计算机视觉领域？ SIGAI 2018.4.26.\n[6] 用一张图理解SVM的脉络 SIGAI 2018.4.28.\n[7] 人脸检测算法综述 SIGAI 2018.5.3.\n[8] 理解神经网络的激活函数 SIGAI 2018.5.5.\n[9] 深度卷积神经网络演化历史及结构改进脉络-40页长文全面解读 SIGAI 2018.5.8.\n[10] 理解梯度下降法 SIGAI 2018.5.11","data":"2018年06月15日 14:03:12"}
{"_id":{"$oid":"5d343b1e62f717dc0659b351"},"title":"自然语言处理笔记4-哈工大 关毅","author":"Big_quant","content":"目录\n文章目录\n目录\n前言\n汉语语料库的多级加工（1）\n汉语语料库的多级加工（2）\n汉语语料库的多级加工（3）\n汉语语料库的多级加工（4）\n汉语语料库的多级加工（5）\n汉语语料库的多级加工（6）\n汉语语料库的多级加工（7）\n前言\n硕士生涯结束，开始专心做一件自己觉得有用的工具，先做工程，后搞理论。\n自然语言处理是一个非常难的问题，同时是人工智能皇冠上的明珠。\n接下来会记录一系列自然语言处理的笔记，来自于哈工大老师关毅。\n汉语语料库的多级加工（1）\n两条路线：基于规则的和基于模型的。\n\n路\n线\n{\n基\n于\n规\n则\n的\nif\n基\n于\n模\n型\n的\nif\n路线\\begin{cases} 基于规则的\u0026amp;\\text{if } \\\\ 基于模型的 \u0026amp;\\text{if } \\end{cases}\n路线{基于规则的基于模型的 if if\n语料库语言学\n经常使用概率统计及信息论中的方法。\n语料库规模对效果影响很大。\n语料分布，语音识别，情况处理。\n句法分析和语料分析各项处理。\n句法分析加工停止\n语料多级加工停止。\n预料多级加工实例，语法树。规范语料库加工。\n切分词性标准，人民日报语料库，未登录词和命名实体。\n汉语语料库的多级加工（2）\n中文人名的识别方法，人名用字比较集中。定义和使用型识别。\n人名姓氏中文资料，人名识别，词性修剪。传统的规则方法。\n中文的黄可以做名词可以做形容词。\n有效的方法是两种东西的结合：\n以坚实的理论基础做架构从而实现的极大熵模型。\n英语译名手册，考察上下文信息。\n统计机器翻译，地名识别，得资源者得一切。中文信息的翻译。\n汉语语料库的多级加工（3）\n采用一种统计分类模型进行统计处理。定位词+中心词。\n识别命名实体，辅助规则，坚实的理论模型，统计规则，最大熵模型。\n特征模板，系数规则生成器。\n系统在满足约束的情况下，熵趋向于最大，任何原理都有适用范围。\n平常要多做应用，调参数调多了自然就有感觉了。\n统计分类模型，目前的效果超好。\n推荐一篇论文，A maximum entropy approach to natural language processing。\n下载源代码后，做一个软件包，进行实时的处理，好好写东西。\n汉语语料库的多级加工（4）\n汉语的兼类词，动名词，名形容词，动名兼类，37%。\n基于规则的词性标注，词性多重修改。词性相同类举，基于隐markov模型，效果超好。\n选择训练集，构建训练样本。训练集异常重要，garbage in，就会garbage out。\n关键数据，结果训练，机器学习，标记不同值。\n未经标注的文本-》 初始标准器-》已经标注的样本-》学习器-《纠错规则，黄金标注文本。《-转换规则。\n转发规则，原tag+环境-》目标tag。\n\n汉语语料库的多级加工（5）\n词性标注的模板。规则的颗粒度不同。转移数量-》标注精度。选择这样的工作。TBL，效果良好，精度有限，好的标注器。基于决策树的方法，效果良好。\n句法分析的总体结构如上图所示\n句法词性处理结构转换。自动短语定界和句法标注实例语义难，\n涉及到一个核心问题，意义的意义是什么？\n汉语语料库的多级加工（6）\n语义与语法的关系，语法是形式，语义是内容。自动语义标注。\n语法标注和语义标注。\n听到词汇后分词，再进行处理，词与词的关系。\n概念在头脑中正确联系，汉语语言判定容易。以单位词的词义，定义多义词的词义比较方便。\n莱斯克以单位词源定义多义词源。\n词义分析很难，利用上下文的搭配关系，确定该词的关系。\n深层语言结构，效果超好。\n词汇间的语义关系是词汇的灵魂，整体关系和上下级关系。\n汉语语料库的多级加工（7）\n语义标注实例，semantic tree。语义树。\n任何一个实体所有的属性比如他在书店看书，AGT（word_no=0,他，r，rrl）\n他就完全被定义了。\n众多学者号召做出千万级的语料库，计算语言学的基础理论。\n十万句句法休整。语义知识库，英语。\nframenet，语言架构，自动切词标注了系统。\n语料库多级加工系统，人的精力得到解放。","data":"2018年12月14日 19:53:07"}
{"_id":{"$oid":"5d343b1e62f717dc0659b353"},"title":"自然语言处理技术（NLP）在推荐系统中的应用","author":"starzhou","content":"自然语言处理技术（NLP）在推荐系统中的应用\nNLP 推荐系统 词袋模型\n阅读6145\n作者： 张相於，58集团算法架构师，转转搜索推荐部负责人，负责搜索、推荐以及算法相关工作。多年来主要从事推荐系统以及机器学习，也做过计算广告、反作弊等相关工作，并热衷于探索大数据和机器学习技术在其他领域的应用实践。\n责编：何永灿（heyc@csdn.net）\n本文为《程序员》原创文章，更多精彩文章请订阅《程序员》\n概述\n个性化推荐是大数据时代不可或缺的技术，在电商、信息分发、计算广告、互联网金融等领域都起着重要的作用。具体来讲，个性化推荐在流量高效利用、信息高效分发、提升用户体验、长尾物品挖掘等方面均起着核心作用。在推荐系统中经常需要处理各种文本类数据，例如商品描述、新闻资讯、用户留言等等。具体来讲，我们需要使用文本数据完成以下任务：\n候选商品召回。候选商品召回是推荐流程的第一步，用来生成待推荐的物品集合。这部分的核心操作是根据各种不同的推荐算法来获取到对应的物品集合。而文本类数据就是很重要的一类召回算法，具有不依赖用户行为、多样性丰富等优势，在文本信息丰富或者用户信息缺乏的场合中具有非常重要的作用。\n相关性计算。相关性计算充斥着推荐系统流程的各个步骤，例如召回算法中的各种文本相似度算法以及用户画像计算时用到的一些相关性计算等。\n作为特征参与模型排序（CTR/CVR）。在候选集召回之后的排序层，文本类特征常常可以提供很多的信息，从而成为重要的排序特征。\n但是相比结构化信息（例如商品的属性等），文本信息在具体使用时具有一些先天缺点。\n首先，文本数据中的结构信息量少。严格来说，文本数据通常是没有什么结构的，一般能够有的结构可能只是“标题”、“正文”、“评论”这样区分文本来源的结构，除此以外一般就没有更多的结构信息了。为什么我们要在意结构信息呢？因为结构代表着信息量，无论是使用算法还是业务规则，都可以根据结构化信息来制定推荐策略，例如“召回所有颜色为蓝色的长款羽绒服”这样一个策略里就用到了“颜色”和“款式”这两个结构化信息。但是如果商品的描述数据库中没有这样的结构化信息，只有一句“该羽绒服为蓝色长款羽绒服”的自由文本，那么就无法利用结构信息制定策略了。\n其次，文本内容的信息量不确定。与无结构化相伴随的，是文本数据在内容的不确定性，这种不确定性体现在内容和数量上，例如不同用户对同一件二手商品的描述可能差异非常大，具体可能在用词、描述、文本长短等方面都具有较大差异。同样的两个物品，在一个物品的描述中出现的内容在另外一个物品中并不一定会出现。这种差异性的存在使得文本数据往往难以作为一种稳定可靠的数据源来使用，尤其是在UGC化明显的场景下更是如此。\n再次，自由文本中的歧义问题较多。歧义理解是自然语言处理中的重要研究课题，同时歧义也影响着我们在推荐系统中对文本数据的使用。例如用户在描述自己的二手手机时可能会写“出售iPhone6一部，打算凑钱买iPhone7”这样的话，这样一句对人来说意思很明确的话，却对机器造成了很大困扰：这个手机究竟是iPhone6还是iPhone7？在这样的背景下如何保证推荐系统的准确率便成为了一个挑战。\n但是文本数据也不是一无是处，有缺点的同时也具有一些结构化数据所不具有的优点：\n数据量大。无结构化的文本数据一般来说是非常容易获得的，例如各种UGC渠道，以及网络爬取等方法，都可穿获得大量文本数据。\n多样性丰富。无结构化是一把双刃剑，不好的一面已经分析过，好的一面就是由于其开放性，导致具有丰富的多样性，会包含一些结构规定以外的数据。\n信息及时。在一些新名词，新事物出现之后，微博、朋友圈常常是最先能够反应出变化的地方，而这些都是纯文本的数据，对这些数据的合理分析，能够最快得到结构化、预定义数据所无法得到的信息，这也是文本数据的优势。\n综上所述，文本数据是一类量大、复杂、丰富的数据，对推荐系统起着重要的作用，本文将针对上面提到的几个方面，对推荐系统中常见的文本处理方法进行介绍。\n从这里出发：词袋模型\n词袋模型（Bag of Words，简称BOW模型）是最简单的文本处理方法，其核心假设非常简单，就是认为一篇文档是由文档中的词组成的多重集合（多重集合与普通集合的不同在于考虑了集合中元素的出现次数）构成的。这是一种最简单的假设，没有考虑文档中诸如语法、词序等其他重要因素，只考虑了词的出现次数。这样简单的假设显然丢掉了很多信息，但是带来的好处是使用和计算都比较简单，同时也具有较大的灵活性。\n在推荐系统中，如果将一个物品看作一个词袋，我们可以根据袋中的词来召回相关物品，例如用户浏览了一个包含“羽绒服”关键词的商品，我们可以召回包含“羽绒服”的其他商品作为该次推荐的候选商品，并且可以根据这个词在词袋中出现的次数（词频）对召回商品进行排序。\n这种简单的做法显然存在着很多问题：\n首先，将文本进行分词后得到的词里面，并不是每个词都可以用来做召回和排序，例如“的地得你我他”这样的“停用词”就该去掉，此外，一些出现频率特别高或者特别低的词也需要做特殊处理，否则会导致召回结果相关性低或召回结果过少等问题。\n其次，使用词频来度量重要性也显得合理性不足。以上面的“羽绒服”召回为例，如果在羽绒服的类别里使用“羽绒服”这个词在商品描述中的出现频率来衡量商品的相关性，会导致所有的羽绒服都具有类似的相关性，因为在描述中大家都会使用类似数量的该词汇。所以我们需要一种更为科学合理的方法来度量文本之间的相关性。\n除了上面的用法，我们还可以将词袋中的每个词作为一维特征加入到排序模型中。例如，在一个以LR为模型的CTR排序模型中，如果这一维特征的权重为w，则可解释为“包含这个词的样本相比不包含这个词的样本在点击率的log odds上要高出w”。在排序模型中使用词特征的时候，为了增强特征的区分能力，我们常常会使用简单词袋模型的一种升级版——N-gram词袋模型。\nN-gram指的就是把N个连续的词作为一个单位进行处理，例如：“John likes to watch movies.Mary likes movies too.”这句话处理为简单词袋模型后的结果为：\n[\"John\":1, \"likes\":2, \"to\":1, \"watch\":1, \"movies\":2, \"Mary\":1, \"too\":1]\n而处理为bigram（2-gram）后的结果为：\n[\"John likes\":1, \"likes to\":1, \"to watch\":1, \"watch movies\":1, \"Mary likes\":1, \"likes movies\":1, \"movies too\":1]\n做这样的处理有什么好处呢？如果将bigram作为排序模型的特征或者相似度计算的特征，最明显的好处就是增强了特征的区分能力，简单来讲就是：两个有N个bigram重合的物品，其相关性要大于有N个词重合的物品。从根本上来讲，是因为bigram的重合几率要低于1-gram（也就是普通词）的重合几率。那么是不是N-gram中的N越大就越好呢？N的增大虽然增强了特征的区分能力，但是同时也加大了数据的稀疏性，从极端情况来讲，假设N取到100，那么几乎不会有两个文档有重合的100-gram了，那这样的特征也就失去了意义。一般在实际应用中，bigram和trigram（3-gram）能够在区分性和稀疏性之间取到比较好的平衡，N如果继续增大，稀疏性会有明显增加，但是效果却不会有明显提升，甚至还会有降低。\n综合来看，虽然词袋模型存在着明显的弊端，但是只需要对文本做简单处理就可以使用，所以不失为一种对文本数据进行快速处理的使用方法，并且在预处理（常用的预处理包括停用词的去除，高频/低频词的去除或降权等重要性处理方法，也可以借助外部高质量数据对自由文本数据进行过滤和限定，以求获得质量更高的原始数据）充分的情况下，也常常能够得到很好的效果。\n统一度量衡：权重计算和向量空间模型\n从上文我们看到简单的词袋模型在经过适当预处理之后，可以用来在推荐系统中召回候选物品。但是在计算物品和关键词的相关性，以及物品之间的相关性时，仅仅使用简单的词频作为排序因素显然是不合理的。为了解决这个问题，我们可以引入表达能力更强的基于TF-IDF的权重计算方法。在TF-IDF方法中，一个词t在文档d中权重的计算方法为：\n其中tft,d代表t在d中出现的频次，而dft指的是包含t的文档数目，N代表全部文档的数目。\nTF-IDF以及其各种改进和变种（关于TF-IDF变种和改进的详细介绍，可参考《Introduction to Information Retrieval》的第六章。）相比简单的TF方法，核心改进在于对一个词的重要性度量，例如：\n原始TF-IDF在TF的基础上加入了对IDF的考虑，从而降低了出现频率高而导致无区分能力的词的重要性，典型的如停用词。\n因为词在文档中的重要性和出现次数并不是完全线性相关，非线性TF缩放对TF进行log缩放，从而降低出现频率特别高的词所占的权重。\n词在文档中出现的频率除了和重要性相关，还可能和文档的长短相关，为了消除这种差异，可以使用最大TF对所有的TF进行归一化。\n这些方法的目的都是使对词在文档中重要性的度量更加合理，在此基础之上，我们可以对基于词频的方法进行改进，例如，可以将之前使用词频来对物品进行排序的方法，改进为根据TF-IDF得分来进行排序。\n但是除此以外，我们还需要一套统一的方法来度量关键词和文档，以及文档和文档之间的相关性，这套方法就是向量空间模型（Vector Space Model，简称VSM）。\nVSM的核心思想是将一篇文档表达为一个向量，向量的每一维可以代表一个词，在此基础上，可以使用向量运算的方法对文档间相似度进行统一计算，而这其中最为核心的计算，就是向量的余弦相似度计算：\n其中V(d1)和V(d2)分别为两个文档的向量表示。这样一个看似简单的计算公式其实有着非常重要的意义。首先，它给出了一种相关性计算的通用思路，那就是只要能将两个物品用向量进行表示，就可以使用该公式进行相关性计算。其次，它对向量的具体表示内容没有任何限制——基于用户行为的协同过滤使用的也是同样的计算公式，而在文本相关性计算方面，我们可以使用TFIDF填充向量，同时也可以用N-gram，以及后面会介绍的文本主题的概率分布、各种词向量等其他表示形式。只要对该公式的内涵有了深刻理解，就可以根据需求构造合理的向量表示。再次，该公式具有较强的可解释性，它将整体的相关性拆解为多个分量的相关性的叠加，并且这个叠加方式可以通过公式进行调节，这样一套方法很容易解释，即使对非技术人员，也是比较容易理解的，这对于和产品、运营等非技术人员解释算法思路有很重要的意义。最后，这个公式在实际计算中可以进行一些很高效的工程优化，使其能够从容应对大数据环境下的海量数据，这一点是其他相关性计算方法很难匹敌的。\nVSM是一种“重剑无锋，大巧不工”的方法，形态简单而又变化多端，领会其精髓之后，可以发挥出极大的能量。\n透过现象看本质：隐语义模型\n前面介绍了文本数据的一些“显式”使用方法，所谓显式，是指我们将可读可理解的文本本身作为了相关性计算、物品召回以及模型排序的特征。这样做的好处是简单直观，能够清晰地看到起作用的是什么，但是其弊端是无法捕捉到隐藏在文本表面之下的深层次信息。例如，“羽绒服”和“棉衣”指的是类似的东西，“羽绒服”和“棉鞋”具有很强的相关性，类似这样的深层次信息，是显式的文本处理所无法捕捉的，因此我们需要一些更复杂的方法来捕捉，而隐语义模型（Latent Semantic Analysis，简称LSA）便是这类方法的鼻祖之一。\n隐语义模型中的“隐”指的是隐含的主题，这个模型的核心假设，是认为虽然一个文档由很多的词组成，但是这些词背后的主题并不是很多。换句话说，词不过是由背后的主题产生的，这背后的主题才是更为核心的信息。这种从词下沉到主题的思路，贯穿着我们后面要介绍到的其他模型，也是各种不同文本主体模型（Topic Model）的共同中心思想，因此理解这种思路非常的重要。\n在对文档做LSA分解之前，我们需要构造文档和词之间的关系，一个由5个文档和5个词组成的简单例子如下：\nLSA的做法是将这个原始矩阵C进行如下形式的SVD分解：\n其中U是矩阵CCT的正交特征向量矩阵，V是矩阵CTC的正交特征向量矩阵，∑k是包含前k个奇异值的对角矩阵，k是事先选定的一个降维参数。\n得到原始数据的一个低维表示，降低后的维度包含了更多的信息，可以认为每个维度代表了一个主题。\n降维后的每个维度包含了更丰富的信息，例如可以识别近义词和一词多义。\n可以将不在训练文档中的文档d通过变换为新向量空间内的一个向量（这样的变换无法捕捉到新文档中的信息，例如词的共现，以及新词的出现等等，所以该模型需要定期进行全量训练。），从而可以在降维后的空间里计算文档间相似度。由于新的向量空间包含了同义词等更深层的信息，这样的变换会提高相似度计算的准确率和召回率。\n为什么LSA能具有这样的能力？我们可以从这样一个角度来看待：CCT中每个元素CCTi,j代表同时包含词i和词j的文档数量，而CTC中每个元素CTCi,j代表文档i和文档j共享的词的数量。所以这两个矩阵中包含了不同词的共同出现情况，以及文档对词的共享情况，通过分解这些信息得到了类似主题一样比关键词信息量更高的低维度数据。\n从另外一个角度来看，LSA相当于是对文档进行了一次软聚类，降维后的每个维度可看做是一个类，而文档在这个维度上的取值则代表了文档对于这个聚类的归属程度。\nLSA处理之后的数据推荐中能做什么用呢？首先，我们可以将分解后的新维度（主题维度）作为索引的单位对物品进行索引，来替代传统的以词为单位的索引，再将用户对物品的行为映射为对新维度的行为。这两个数据准备好之后，就可以使用新的数据维度对候选商品进行召回，召回之后可以使用VSM进行相似度计算，如前文所述，降维后的计算会带来更高的准确率和召回率，同时也能够减少噪音词的干扰，典型的，即使两个文档没有任何共享的词，它们之间仍然会存在相关性，而这正是LSA带来的核心优势之一。此外，还可以将其作为排序模型的排序特征。\n简单来讲，我们能在普通关键词上面使用的方法，在LSA上面仍然全部可用，因为LSA的本质就是对原始数据进行了语义的降维，只需将其看作是信息量更丰富的关键词即可。\n可以看到LSA相比关键词来说前进了一大步，主要体现在信息量的提升，维度的降低，以及对近义词和多义词的理解。但是LSA同时也具有一些缺点，例如：\n训练复杂度高。LSA的训练时通过SVD进行的，而SVD本身的复杂度是很高的，在海量文档和海量词汇的场景下难以计算，虽然有一些优化方法可降低计算的复杂度，但该问题仍然没有得到根本解决。\n检索（召回）复杂度高。如上文所述，使用LSA做召回需要先将文档或者查询关键词映射到LSA的向量空间中，这显然也是一个耗时的操作。\nLSA中每个主题下词的值没有概率含义，甚至可能出现负值，只能反应数值大小关系。这让我们难以从概率角度来解释和理解主题和词的关系，从而限制了我们对其结果更丰富的使用。\n概率的魔力：概率隐语义模型\n为了进一步发扬隐语义模型的威力，并尽力克服LSA模型的问题，Thomas Hofmann在1999年提出了概率隐语义模型（probabilistic Latent Semantic Analysis，简称pLSA)。从前面LSA的介绍可以看出，虽然具体的优化方法使用的是矩阵分解，但是从另一个角度来讲，我们可以认为分解后的U和V两个矩阵中的向量，分别代表文档和词在隐语义空间中的表示，例如一个文档的隐向量表示为(1,2,0)T， 代表其在第一维隐向量上取值为1，第二维上取值为2，第三维上取值为0。如果这些取值能够构成一个概率分布，那么不仅模型的结果更利于理解，同时还会带来很多优良的性质，这正是pLSA思想的核心：将文档和词的关系看作概率分布，然后试图找出这个概率分布来，有了文档和词的概率分布，我们就可以得到一切我们想要得到的东西了。\n在pLSA的基本假设中，文档d和词w的生成过程如下：\n以 P(d) 的概率选择文档d。\n以 P(z|d) 的概率选择隐类z。\n以 P(w|z) 的概率从z生成w。\nP(z|d)和P(w|z) 均为多项式分布。\n将这个过程用联合概率进行表达得到：\n\n\n图1 pLSA的生成过程\n可以看到，我们将隐变量z作为中间桥梁，将文档和词连接了起来，形成了一个定义良好、环环相扣的概率生成链条（如图1所示）。虽然pLSA的核心是一种概率模型，但是同样可以用类似LSI的矩阵分解形式进行表达。为此，我们将LSI中等号右边的三个矩阵进行重新定义：\n在这样的定义下，原始的矩阵C仍然可以表述为C=U∑VT。这样的对应关系让我们更加清晰地看到了前面提到的pLSA在概率方面的良好定义和清晰含义，同时也揭示了隐语义概率模型和矩阵分解之间的密切关系（关于概率模型和矩阵分解的密切关系可参考这篇文档：http://www.cs.cmu.edu/~epxing/Class/10708-15/slides/LDA_SC.pdf）。在这样的定义，隐变量z所代表的主题含义更加明显，也就是说，我们可以明确的把一个z看作一个主题，主题里的词和文档中的主题都有着明确的概率含义。也正是由于这样良好的性质，再加上优化方法的便捷性，使得从pLSA开始，文本主题开始在各种大数据应用中占据重要地位。\n从矩阵的角度来看，LSA和pLSA看上去非常像，但是它们的内涵却有着本质的不同，这其中最为重要的一点就是两者的优化目标是完全不同的：LSA本质上是在优化SVD分解后的矩阵和原始矩阵之间的平方误差，而pLSA本质上是在优化似然函数，是一种标准的机器学习优化套路。也正是由于这一点本质的不同，导致了两者在优化结果和解释能力方面的不同。\n至此我们看到，pLSA将LSA的思想从概率分布的角度进行了一大步扩展，得到了一个性质更加优良的结果，但是pLSA仍然存在一些问题，主要包括：\n由于pLSA为每个文档生成一组文档级参数，模型中参数的数量随着与文档数成正比，因此在文档数较多的情况下容易过拟合。\npLSA将每个文档d表示为一组主题的混合，然而具体的混合比例却没有对应的生成概率模型，换句话说，对于不在训练集中的新文档，pLSA无法给予一个很好的主题分布。简言之，pLSA并非完全的生成式模型。\n而LDA的出现，就是为了解决这些问题。\n概率的概率：生成式概率模型\n为了解决上面提到的pLSA存在的问题，David Blei等人在2003年提出了一个新模型，名为“隐狄利克雷分配”（Latent Dirichlet Allocation，简称LDA），这个名字念起来颇为隐晦，而且从名字上似乎也看不出究竟是个什么模型，在这里我们试着做一种可能的解读：\nLatent：这个词不用多说，是说这个模型仍然是个隐语义模型。\nDirichlet：这个词是在说该模型涉及到的主要概率分布式狄利克雷分布。\nAllocation：这个词是在说这个模型的生成过程就是在使用狄利克雷分布不断地分配主题和词。\n上面并非官方解释，但希望能对理解这个模型能起到一些帮助作用。\nLDA的中心思想就是在pLSA外面又包了一层先验，使得文档中的主题分布和主题下的词分布都有了生成概率，从而解决了上面pLSA存在的“非生成式”的问题，顺便也减少了模型中的参数，从而解决了pLSA的另外一个问题。在LDA中为一篇文档di生成词的过程如下：\n从泊松分布中抽样一个数字N作为文档的长度（这一步并非必须，也不影响后面的过程）。\n从狄利克雷分布Dir(α)中抽样一个样本θi，代表该篇文档下主题的分布。\n从狄利克雷分布Dir(β)中抽样一组样本Φk，代表每个主题下词的分布。\n对于1到N的每个词wn：\n\n从多项式分布Multinomial(θi) 中抽样一个主题ci,j。\n从多项式分布Multinomial(Φi) 中抽样一个词wi,j。\n\n\n图2 LDA的生成过程\n忽略掉最开始选择文档长度的步骤，我们发现LDA的生成过程相比pLSA来讲，在文档到主题的分布和主题到词的分布上面都加了一层概率，使得这两者都加上了一层不确定性，从而能够很自然地容纳训练文档中没有出现过的文档和词，这使得LDA具有了比pLSA更好的概率性质。\nLDA的应用\n这部分我们介绍LDA在用作相似度计算和排序特征时需要注意的一些地方，然后介绍以LDA为代表的文本主题在推荐系统中更多不同角度的应用。\n相似度计算\n上面提到LSA可以直接套用到VSM中进行相似度计算，在LDA中也可以做类似的计算，具体方法是把文档的主题分布值向量化然后用余弦公式进行计算。但是把余弦相似度替换为KL divergence或Jensen–Shannon divergence效果更好，原因是LDA给出的主题分布是含义明确的概率值，用度量概率之间相似度的方法来进行度量更为合理。\n排序特征\n将物品的LDA主题作为排序模型的特征是一种很自然的使用方法，但并不是所有的主题都有用。物品上的主题分布一般有两种情况：\n有少数主题（三个或更少）占据了比较大的概率，剩余的主题概率加起来比较小。\n所有主题的概率值都差不多，都比较小。\n在第一种情况下，只有前面几个概率比较大的主题是有用的，而在第二种情况下，基本上所有的主题都没有用。那么该如何识别这两种情况呢？第一种方法，可以根据主题的概率值对主题做一个简单的K-Means聚类，K选为2，如果是第一种情况，那么两个类中的主题数量会相差较大——一个类中包含少量有用主题，另一个类包含其他无用主题；而第二种情况下主题数量则相差不大，可以用这种方法来识别主题的重要性。第二种方法，可以计算主题分布的信息熵，第一种情况对应的信息熵会比较小，而第二种情况会比较大，选取合适的阈值也可以区分这两种情况。\n物品打标签\u0026用户打标签\n为物品计算出其对应的主题，以及主题下面对应的词分布之后，我们可以选取概率最大的几个主题，然后从这几个主题下选取概率最大的几个词，作为这个物品的标签。在此基础上，如果用户对该物品发生了行为，则可以将这些标签传播到用户身上。\n这种方法打出的标签，具有非常直观的解释，在适当场景下可以充当推荐解释的理由。例如我们在做移动端个性化推送时，可供展示文案的空间非常小，可以通过上面的方式先为物品打上标签，然后再根据用户把标签传播到用户身上，在推送时将这些标签词同时作为召回源和推荐理由，让用户明白为什么给他做出这样的推荐。\n主题\u0026词的重要性度量\nLDA训练生成的主题中，虽然都有着同等的位置，但是其重要性却是各不相同的，有的主题包含了重要的信息，有的则不然。例如，一个主题可能包含“教育、读书、学校”等词，和这样主题相关的文档，一般来说是和教育相关的主题，那么这就是一个信息量高的主题；相反，有的主题可能会包含“第一册、第二册、第三册……”等词（如果在一个图书销售网站的所有图书上训练LDA，就有可能得到这样的主题，因为有很多套装图书都包含这样的信息），和这样主题相关的文档却有可能是任何主题，这样的主题就是信息量低的主题。\n如何区分主题是否重要呢？从上面的例子中我们可以得到启发：重要的主题不会到处出现，只会出现在小部分与之相关的文档中，而不重要的主题则可能在各种文章中都出现。基于这样的思想，我们可以使用信息熵的方法来衡量一个主题中的信息量。通过对LDA输出信息做适当的变换，我们可以得到主题θi在不同文档中的概率分布，然后我们对这个概率分布计算其信息熵，通俗来讲信息熵衡量了一个概率分布中概率值分散程度，越分散熵越大，越集中熵越小。所以在我们的问题中，信息熵越小的主题，说明该主题所对应的文档越少，主题的重要性越高。\n使用类似的方法，我们还可以计算词的重要性，在此不再赘述。\n更多应用\n除了上面提到的，LDA还有很多其他应用，甚至在文本领域以外的图像等领域也存在着广泛应用。LSA/pLSA/LDA这些主题模型的核心基础是词在文档中的共现，在此基础上才有了各种概率分布，把握住这个核心基础，就可以找到文本主体模型的更多应用。例如，协同过滤问题中，基础数据也是用户对物品的共同行为，这也构成了文本主题模型的基础，因此也可以使用LDA对用户对物品的行为进行建模，得到用户行为的主题，以及主题下对应的物品，然后进行物品/用户的推荐。\n捕捉上下文信息：神经概率语言模型\n以LDA为代表的文本主题模型通过对词的共现信息的分解处理，得到了很多有用的信息，但是pLSA/LDA有一个很重要的假设，那就是文档集合中的文档，以及一篇文档中的词在选定了主题分布的情况下都是相互独立，可交换的，换句话说，模型中没有考虑词的顺序以及词和词之间的关系，这种假设隐含了两个含义：\n在生成词的过程中，之前生成的词对接下来生成的词是没有影响的。\n两篇文档如果包含同样的词，但是词的出现顺序不同，那么在LDA看来他们是完全相同的。\n这样的假设使得LDA会丢失一些重要的信息，而近年来得到关注越来越多的以word2vec为代表的神经概率语言模型恰好在这方面和LDA形成了一定程度的互补关系，从而可以捕捉到LDA所无法捕捉到的信息。\nword2vector的中心思想用一句话来讲就是：A word is characterized by the company it keeps（一个词的特征由它周围的词所决定）。\n这是一句颇有哲理的话，很像是成语中的“物以类聚人以群分”。具体来讲，词向量模型使用“周围的词=\u003e当前词”或“当前词=\u003e周围的词”这样的方式构造训练样本，然后使用神经网络来训练模型，训练完成之后，输入词的输入向量表示便成为了该词的向量表示，如图3所示。\n这样的训练方式，本质上是在说，如果两个词具有类似的上下文（上下文由周围的词组成），那么这两个词就会具有类似的向量表示。有了词的向量表示之后，我们可以做很多事情，最常见的是将这一层向量表示作为更深层次模型的一个嵌入层。除了在深度学习中的使用以外，在推荐系统中还可以做很多其他的事情，其中之一就是做词的聚类，以及寻找相似词。我们知道LDA天然就可以做到词的聚类和相似词的计算，那么使用word2vec计算出来的结果和LDA有什么不同呢？它们之间的不同具体体现在两点：第一是聚类的粒度不同，LDA关注的主题级别的粒度，层次更高，而词向量关注的是更低层次的语法语义级别的含义。例如“苹果”，“小米”和“三星”这三个词，在LDA方法中很可能会被聚类在一个主题中，但是在词向量的角度来看，“苹果”和“小米”可能会具有更高的相似度，就像“乔布斯”和“雷军”在词向量下的关系一样，所以在词向量中可能会有：“vector（小米）- vector（苹果）+vector（乔布斯）= vector（雷军）”这样的结果。\n除此以外，由于word2vec有着“根据上下文预测当前内容”的能力，将其做适当修改之后，还可以用来对用户行为喜好做出预测。首先我们将用户的行为日志进行收集，进行session划分，得到类似文本语料的训练数据，在这个数据上训练word2vec模型，可以得到一个“根据上下文行为预测当前行为”的模型。但是原始的行为数据中行为的对象常常是id级的，例如商品、视频的id等等，如果直接放到模型中训练，会造成训练速度慢、泛化能力差等问题，因此需要对原始行为做降维，具体来说可以将行为映射到搜索词、LDA Topic、类别等等低维度特征上，然后再进行训练。例如，我们可以对用户的搜索词训练一个word2vec模型，然后就可以根据用户的历史搜索行为预测他的下一步搜索行为，并在此基础上进行推荐。这种方法考虑到了上下文，但是对前后关系并没有做最恰当的处理，因为word2vec的思想是“根据上下文预测当前内容”，但我们希望得到的模型是“根据历史行为预测下一步行为”，这两者之间有着微妙的差别。例如用户的行为序列为“ABCDE”，每个字母代表对一个物品（或关键词）的行为，标准的word2vec算法可能会构造出下面这些样本：AC→B, BD→C, CE→D… 但是我们希望的形式其实是这样的：AB→C, BC→D,CD→E…因此，需要对word2vec生成样本的逻辑进行修改，使其只包含我们需要的单方向的样本，方可在最终模型中得到我们真正期望的结果。\n下面是按照该方法生成的一些预测例子：\n可以看出，预测搜索词都与历史搜索词有着紧密的关系，是对历史搜索词的延伸（例如学生书桌和烤肠机的例子）或者细化（例如小龟王和西铁城手表的例子），具有比较好的预测属性，是非常好的推荐策略来源。沿着这样的思路，我们还可以对word2vec作进一步修改，得到对时序关系更为敏感的模型，以及尝试使用RNN、LSTM等纯时序模型来得到更好的预测结果，但由于篇幅所限，在此不做展开。\n行业应用现状\n文本主题模型在被提出之后，由于其良好的概率性质，以及对文本数据有意义的聚类抽象能力，在互联网的各个行业中都取得了广泛的应用。搜索巨头Google在其系统的各个方面都在广泛使用文本主题模型，并为此开发了大规模文本主题系统Rephil。例如在为用户搜索产生广告的过程中，就使用了文本主题来计算网页内容和广告之间的匹配度，是其广告产品成功的重要因素之一。此外，在匹配用户搜索词和网页间关系的时候，文本主题也可用来提高匹配召回率和准确性。Yahoo！也在其搜索排序模型中大量使用了LDA主题特征，还为此开源了著名的Yahoo!LDA工具。\n在国内，文本主题最著名的系统当属腾讯开发的Peacock系统，该系统可以捕捉百万级别的文本主题，在腾讯的广告分类、网页分类、精准广告定向、QQ群分类等重要业务上均起着重要的作用。该系统使用的HDP（Hierarchical Dirichlet Process）模型是LDA模型的一个扩展，可智能选择数据中主题的数量，还具有捕捉长尾主题的能力。除了腾讯以外，文本主题模型在各公司的推荐、搜索等业务中也已经在广泛使用，使用方法根据各自业务有所不同。\n以word2vec为代表的神经网络模型近年来的使用也比较广泛，典型的应用如词的聚类、近义词的发现、quer y的扩展、推荐兴趣的扩展等。Facebook开发了一种word2vec的替代方案FastText，该方案在传统词向量的基础上，考虑子词（subword）的概念，取得了比word2vec更好的效果 。\n总结和展望\n我们从简单的文本关键词出发，沿着结构化、降维、聚类、概率、时序的思路，结合推荐系统中候选集召回、相关性计算、排序模型特征等具体应用，介绍了推荐系统中一些常用的自然语言处理技术和具体应用方法。自然语言处理技术借着深度学习的东风，近年来取得了长足的进步，而其与推荐系统的紧密关系，也意味着推荐系统在这方面仍然有着巨大的提升空间，让我们拭目以待。","data":"2017年06月30日 08:43:16"}
{"_id":{"$oid":"5d343b1f62f717dc0659b355"},"title":"《Python进行自然语言处理》代码笔记（一）：第一章示例","author":"Pd-pony","content":"利用空余时间，将《用Python进行自然语言处理》的前面几章内容都敲了一遍，其中遇到与书中示例不太一致的地方也进行了修改。第一章示例如下：\n#!/usr/bin/env python # -*- coding: utf-8 -*- # @Author : Peidong # @Site : # @File : eg1.py # @Software: PyCharm \"\"\" the first example for nltk book \"\"\" from nltk.book import * # 查找特定词语上下文 text1.concordance(\"monstrous\") # 相关词查找 text1.similar(\"monstrous\") # 查找多个词语的共同上下文 text2.common_contexts([\"monstrous\", \"very\"]) # 画出词语的离散图 text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"]) # 产生随机文本 text3.generate() Traceback (most recent call last): File \"E:/nlp/eg1.py\", line 25, in \u003cmodule\u003e text3.generate() TypeError: generate() missing 1 required positional argument: 'words' # 单词数量 标识符总数 print(len(text3)) # 词汇的种类及数量 用集合set显示 print(sorted(set(text3))) print(len(set(text3))) # 测量平均每类词语被使用的次数 from __future__ import division #本命令必须放在文件的开始之初 print(len(text3)/len(set(text3))) # 统计特定单词在文本中出现的次数，并计算其占比 print(text3.count(\"smote\")) print(100*text4.count('a')/len(text4)) # # 词的频率分布 fdist1 = FreqDist(text1) # # 输出总的词数 print(fdist1) # In Python 3 dict.keys() returns an iteratable but not indexable object. vac1 = list(fdist1.keys()) # # 输出词数最多的前五十个词 print(vac1[:50]) # # 输出whale的次数 print(fdist1[\"whale\"]) # # 输出前五十个词的累积频率图 fdist1.plot(50) # 查找长度超过15个字符的词 V = set(text1) long_words = [w for w in V if len(w)\u003e15] print(sorted(long_words)) # 查找长度超过7的词且频率超过7 fdist5 = FreqDist(text5) print(sorted([ w for w in set(text5) if len(w)\u003e7 and fdist5[w]\u003e7])) # 双连词的使用 from nltk import bigrams # # 查了一下nltk官网上的函数说明，要加list()函数，结果才是书上的情况 print(list(bigrams(['more', 'is', 'said', 'than', 'done']))) # 文本中常用的连接词 print(text4.collocations()) print([len(w) for w in text1]) fdist = FreqDist([len(w) for w in text1]) print(fdist) print(fdist.keys()) print(fdist.items()) print(fdist.max()) print(fdist[3]) print(fdist.freq(3)) print(sorted([w for w in set(text1) if w.endswith('ableness')])) print(babelize_shell())","data":"2017年05月09日 15:38:54","date":"2017年05月09日 15:38:54"}
{"_id":{"$oid":"5d343b1f62f717dc0659b359"},"title":"cs224n自然语言处理与深度学习笔记 week1","author":"hongyesuifeng","content":"前言\n本节主要针对斯坦福大学CS224N的自然语言处理与深度学习课程所做笔记,将学习过程中的一些重难点进行记录,方便后续复习\n什么是自然语言处理\n自然语言处理是计算机科学,人工智能和语言学的集合,该技术的目的是为了使计算机能够理解语言.\n自然语言处理的一些应用\n拼写检查,关键词查询,语法检查,文本分类,对话系统…\n什么是深度学习\n深度学习是机器学习的一个分之,和传统方法的区别主要在于其端到端的形式,从raw input中自动提取特征,最后输出想要的结果分类或者回归.\n一些先修知识\nPython基础,线性代数,概率论,统计学,基本的机器学习理论\n希望教授的知识\n使用有效深度学习模型的能力,比如NLP中的一些重要技术,RNN,attention机制\n有NLP有个宏观的认知,了解该领域的一些难点\n有能力构建一些NLP中的系统来解决一些主要问题,比如单词的相似度,命名体识别,翻译系统,对话系统…\n为什么NLP难\n语言表示和学习的复杂性,语言的歧义性,人们语言的解读依赖于实际环境,比如场景和上下文.\nDeep NLP=Deeplearning + NLP\n词义的表示,向量化单词,且可进行可视化\n词义向量(Morphology)\n依存句法分析(Parsing for sentence structure)\n句子的含义(Semantic)\n情感分析(Sentiment Analysis)\n作业\n第4部分作业：Assignment 1.1-1.2（地址： https://github.com/learning511/cs224n-learning-camp/blob/master/Assignmnet.md ）\n1.1 Softmax 算法\n1.2 Neural Network Basics 神经网络基础实现\nWord Vector\n本节主要记录述词向量相关的原理和内容\n如何表示单词含义\n词义进行表示\n离散表示,Onehot\n基于单词上下文的分布式表示\n将单词表示为向量形式\nWord2vec 的主要思想\n两个方法:Skip-gram和CBOW\nSkip-gram:根据中心词,预测上下文\n\nCBOW:根据上下文预测中心词\n\n两种相对效率的训练方法:霍夫曼树,负采样法\n训练中同时更新迭代所有向量,每个词有两个向量的表示,一个是作为中心词时候的词向量,另一个是作为上下文的词向量\n更新迭代的过程\n\n\n\n更新计算所有梯度,在划窗过程中,更新计算的不止是中心词的词向量,在每一个划窗过程,对窗口中的中心词词向量,上下文词向量都进行更新.","data":"2018年12月03日 15:12:53"}
{"_id":{"$oid":"5d343b2062f717dc0659b35b"},"title":"NLP学习个人笔记 1 自然语言处理概述","author":"Dld_ML_Blog","content":"note：C++Boost库可以使用编译好的c++代码替换python代码块提升代码性能\n自然语言处理概述\n自然语言包括\n口语  语音 文本\n是人工智能 和 语言学的交叉学科\n基于 机器学习 深度学习\n文本实际上包含了十分丰富的信息\n语义理解-\u003e推理和推断      最后的语义推断是重点（后面的三个部分）\n目前就业市场中NLP的比重是十分重要的\n想要人工智能发挥作用就要让计算机理解语言之上的内容\n打开手机app之后会发现新闻媒体资讯类的app中呈现的信息大多数是文字信息的\n大多数信息是由文字去传递的\n自然语言处理不是编程语言，自言语言具有很强的逻辑性\n比喻性强的语言：“双击666”，“打call”\nNLP的核心的问题\n在海量数据中如何及时提取出核心的信息\n简单的自然语言分词\n在垂直领域中有简单的分词操作\n文本主体和标签分类\n摘要生成\n舆情监控\n搜索引擎\n语音识别\n对话机器人\n学术论文 新闻的标注 方便索引 翻遍做推荐\n自动对对联的系统   （神经网络）\n机器翻译（语言映射）\n情感分析与舆情监控（民众情绪的变化）\n语音识别应用（基于深度学习）\n聊天机器人、客服机器人\n深度学习的流程：\n数据准备 与清洗\n原始数据输入与表征（分布）学习\n网络设计与非线性表达\n模型拟合与预测\n（具有很强的非线性表达能力）\n深度学习的使用率逐年增加\n基于评论情感分析的酒店挑选\n传统机器学习的文本表述：词袋模型\n1基础知识以及项目背景：\n词嵌入算法（稠密表示）\n（每个词映射为一个超高维度的向量）\n2机器学习的解决方案\n3深度学习的解决方案\n项目经理  完整的项目经历的描述\n1.项目背景\n描述项目的具体背景  酒店。。。\n2.项目数据\n项目的数据是哪里来的   飞猪 携程\n3.数据处理方法\n脏的数据怎么定义的\n脏数据是怎么处理的\n类别不均衡的情况怎么做的处理  以及一些在数据预处理怎么表述\n数据 独热向量   聚集表示 -----\u003e为什么要使用这些东西\n如何调优性能\n如何识别判断是否是有问题的\n如何去优化项目\n模型选择与建模过程\n评估推测与效果\n模型优化与提升","data":"2018年12月20日 19:13:39"}
{"_id":{"$oid":"5d343b2162f717dc0659b35d"},"title":"斯坦福自然语言处理习题课1——绪论","author":"最老程序员闫涛","content":"对于技术人员来说，如果要问当前最热门的技术是什么？我想大家一定会回答是人工智能技术。而在人工智能技术中，哪个技术方向最火呢？大家肯定会回答是深度学习技术。如果我们要问在深度学习技术中，哪些应用方向最火呢？我想大家可能会不约而同地说是机器视觉和自然语言处理了。机器视觉自然就不必说了，比如说像商汤、旷视、Face++等独角兽级企业，是史上成长最快的独角兽级企业了，融资规模在几十亿以上，同时机器视觉工程师的平均年薪已经达到50万起，由此可见这个领域有多火爆。但是我们觉得，机器视觉已经发展接近顶峰了，未来几年很难再现像前几年的飞速发展期了，未来的发展的会比较平稳。但是自然语言处理领域则不同，目前处于刚刚开始起飞阶段，未来三到五年，将迎来进喷式发展，发展速度甚至比机器视觉还要快，因为自然语言处理应用领域比机器视觉更广泛。因为未来的人工智能要求更自然的人机交互、人机协同和人机融合，这就要求机器具有听、说、读、写能力，而这正是自然语言处理的研究领域。\n综上所述，目前是学习自然语言处理技术的黄金时期，一方面是目前自然语言处理技术工程师的年薪已经接近甚至超过机器视觉工程师的年薪，另一方面，我们认为未来2、3年或者3至5年将是自然语言处理飞速发展期，会对自然语言处理方面的人才产生巨大的需求，现在开始学习，才能够抓这一趋势，成为市场上抢手的人才。\n那么我们怎样来学习自然语言处理技术呢？我想大部分人可能会选择学习网络上可以公开获取到的斯坦福大学自然语言处理课程cs224n了。这门课出生名门，不仅讲述基础原论而且讲述业界最新进展，同时还沿袭了斯坦福大学理论联系实际的传统，这点从课程作业的选题中可以看出来。所以说这门课绝对是学习自然语言处理技术不可多得的资料，但是我们从公开渠道只能看到老师上课的视频、课程讲义PPT和作业资料，而这门课中更加重要的，由斯坦福大学这门课助教讲解作业实现技术的习题课，我们就看不到了。而对于初学者来说，这些助教讲的习题课才是这门课的干货，虽然老师讲的理论和最新进展也很重要，但是我们只有通过动手实践才能真正掌握和理解这些内容，但是非常遗憾，习题课是不公开的。\n基于这种情况，我们在网易云课堂上开设了《斯坦福自然语言处理习题课》这样一门课程，我们在这门课中，将像斯坦福的助教一样，向大家详细讲解作业的实现技术，使大家真正做到理论联系实际。讲到这里，可能有同学会问，对于CS224n这门课的作业，网上也有很多作业实现和博客文章来讲解实现细节，我们直接学习这些内容不就可以了吗？还用学习这门课吗？当然，直接看这些是可以的，但是由于同学们是初学者，对于网络上的这些资料，到底质量怎么样很难区分。另外，这些资料通常都是由在校生完成并分享出来的，而在校生由于没有大型工程项目的经验和教训，他们的实现容易陷入就事论事的状态。\n比如我们来看一个例子：\n\n\n这个还是一个比较著名的实现，实现的质量还是蛮高的。但是就是这样一个实现，也是有很大的问题的。先来说一下这个实现好的地方，在这个实现里面使用了lambda表达式、numpy apply_along_axis这样的高级函数。但是这个实现有一个小的问题，大家看老师给的作业要求里面，对于X的说明里，写了这样一句：You are allowed to modify x in place.这句话只是一个建议，所以很多同学都把这句给忽略了。绝大多数同学在实现时都采用x = np.exp(x)的形式，但是这样写的会复制一个x的数组并返回，这样既浪费了内存空间同时会浪费运行时间，虽然softmax一般只用于输出层，一般对MNIST手写数字识别只有10个类别，李飞飞开发的ImageNet竞赛也不过是1000类，计算量并不大，但是在训练阶段训练样本集通常有数百万的训练样本，而且会学习多轮，这样就浪费太大了。所以老师才会提醒你，你可以在x数组里进行修改，不必返回一个全新的数组，但是可惜被同学们忽略掉了。当然，可能也有同学尝试过老师提的in place方式，但是在老师给的代码情况下，直接使用是通不过的，需要进行一些小的修改，这点在我们课程中间会详细讲解。这就让我们想起了西游记里孙悟空学艺，菩提老祖在孙悟空头上敲三下，孙悟空就能悟到是老师让他三更去找老师，而其他弟子就悟不到了。所以在这门课里，我们会代领大家掌握自然语言处理技术的精髓，达到业界标准甚至更高的水平。\n所以说我们这门课的价值不仅仅告诉你怎么把CS224n这门课里面的作业做出来，而是要给大家展示业界的最佳实践，使同学们真正达到业界自然语言处理工程师的水平。\n这里面还有一点，就是即使像斯坦福自然语言处理CS224n这样的精品课程，也存在由于录课时间较久远，课上所用技术存在过时的情况。例如，在CS224n的作业中，还是基于python2的代码，而我们知道Python2已经停止更新了，业界已经基本完成从python2到python3的转化了，大家到实际工作中，基本都是使用python3了。这不得不说是一个遗憾。所以在这门课里面，我们将带领大家一起，把课程作业中的代码移植到python3。还有例如在作业1里面，老师给我们的神经元激活函数还是sigmoid，而我们知道，自从06年深度学习崛起之后，神经元特别是隐藏层神经元的缺省激活函数已经变为ReLU了，如果是近一两年，还会看到很多采用SeLU激活函数的，这一点也比较遗憾。关于这一点，我们在课程中也会向大家作一个简单的介绍，使同学们能够掌握最新的技术。还有一点，也是在作业1里面，在讲解随机梯度下降算法的时候，还采用的是传统的BP算法的方式，而我们知道，当前流行的深度学习框架，如TensorFlow、PyTorch和Theano等，都是基于计算图的实现方式了，这些内容也略微有些过时，我们在课程中也会以TensorFlow普遍采用的计算图方式，向大家讲解递度下降算法。当然了，这门课毕竟是CS224n配套的习题课，不可能向大家系统的讲解计算图模型，如果同学们对这方面内容感举趣，可以关注我们即将推出的《自己动手写TensorFlow》课程，在这门课中，我们将带领大家，模仿TensorFlow框架，使用C语言来实现深度学习的核心算法，使用Python语言来进行数据可视化、模型可视化和训练过程可视化工作。\n回到我们这门课程，我们会带领大家使用python+numpy的方式来实现所有的核心算法，虽然实际工作中，大家可能会使用TensorFlow、PyTorch这样的深度学习框架来实现算法，甚至很多人用更高层的Keras来写算法，但是这些框架和库会给我们屏蔽了底层的细节，虽然使我们更容易入门，可是也使我们很难搞懂底层的数学原理和代码实现技术，而我们通过python+numpy的方式，自己动手完全从头来实现这些核心算法，恰恰可以弥补大家在这方面的知识缺陷。\n如果大家对自然语言处理技术感兴趣，想掌握这些算法的底层实现原理，请关注我们的课程：斯坦福自然语言处理习题课（https://study.163.com/course/introduction/1006361019.htm?share=2\u0026shareId=400000000383016），课程目前处于连载状态，早学的同学有优惠呦！","data":"2018年11月06日 16:17:08"}
{"_id":{"$oid":"5d343b2162f717dc0659b35f"},"title":"python自然语言处理：有效信息的提取","author":"uinglin","content":"最近跑了3D CNN，实现MRI的分割及分类。训练过程中将准确率等自动保存成一个txt文件，该文件有64M多，想要从中提取有用信息比如Mean accuracy、sensitivity、specificity和mean dsc等，人工筛选太繁琐，所以想到用python编程来实现。\n去请教了一位研究自然语言处理的计算机系的师兄，实现代码如下：\n# -*- coding: utf-8 -*-\n# @Time : 18-4-18\n# @Author : sadscv\n# @File : textExtract.py\nimport json\nimport re\ndef readfile():\nsave = None\ncount = 0\nwith open(\"data/trainSessionDeepMedic.txt\", \"r\") as f:\ntmpfile = open(\"data/Epoch.txt\", \"wb\")\n# split file\nfor line in f:\nprint(save)\nhead = re.search(\"Starting new Epoch! Epoch\", line, flags=0)\nif head:\npos = re.search(\"#\", line).span()[1]\nnum = line[pos:pos + 2].rstrip(\"/\")\nsave = num\nopen(\"data/Epoch_{}.txt\".format(save), \"wb\")\ncount += 1\ntmpfile = open(\"data/Epoch_{}.txt\".format(save), \"a\")\nelif save:\ntmpfile.write(line)\nreturn count\ndef subfile_extract(epoch):\nwith open(\"data/Epoch_{}.txt\".format(epoch), \"r\") as f:\nclass_json = {}\nfor line in f:\n# 两行 whole epoch\nif re.search(\"finished. Reporting Accuracy over whole epoch.\", line):\ntmp = []\ntmp_flag = False\nfor i in range(4):\ncurrent_line = next(f)\nif re.search('Finished sampling segment', current_line):\ntmp_flag = True\nif re.search(\"VALIDATION\", current_line):\ntmp.append(current_line)\nif tmp_flag:\ntmp.append(next(f))\nclass_json.update(json_generater(tmp, \"whole\"))\n# 8行 subepoch\nif re.search(\"Reporting Accuracy over whole epoch for Class\", line):\ntmpval = []\ntmptrain = []\nfor i in range(8):\ncurrent_line = next(f)\nif re.search(\"VALIDATION\", current_line):\ntmpval.append(current_line)\nif re.search(\"TRAINING\", current_line):\ntmptrain.append(current_line)\nif tmpval:\nclass_json.update(json_generater(tmpval, \"validation\"))\nif tmptrain:\nclass_json.update(json_generater(tmptrain, \"training\"))\nreturn json.dumps(class_json, indent=4)\ndef json_generater(lines, key=None):\n# 送进来8行，输出一个json. 代表epoch*-validation(or training)-class* 包含的数据\n# 如果长度为2，则是overall validation\nif len(lines) == 2:\np0 = re.search(\"mean accuracy of epoch:\", lines[0]).span()[1]\np1 = re.search(\"mean accuracy of each subepoch:\", lines[1]).span()[1]\nmean_accuracy = lines[0][p0:].split(\"=\u003e\")[0].strip()\nsubepoch_accuracy = lines[1][p1:].strip()\noutput_dict = {\n\"mean accuracy of epoch\": mean_accuracy,\n\"mean accuracy of each subepoch\": subepoch_accuracy\n}\nreturn {\"overall\": output_dict}\nif len(lines) == 8:\naccuracy = {}\nsensitivity = {}\nspecificity = {}\ndice = {}\n# Todo 添加函数，重构重复代码。\nfor i in range(len(lines)):\nif re.search(\"accuracy\", lines[i]):\naccuracyidx = re.search(\"mean accuracy of epoch:\", lines[i])\naccuracysubidx = re.search(\"mean accuracy of each subepoch:\", lines[i])\nif accuracyidx:\naccuracy.update({\"epoch\": lines[i][accuracyidx.span()[1]:].split(\"=\u003e\")[0].strip()})\nif accuracysubidx:\naccuracy.update({\"subepoch\": lines[i][accuracysubidx.span()[1]:].strip()})\nelif re.search(\"sensitivity\", lines[i]):\nsen_idx = re.search(\"mean sensitivity of epoch:\", lines[i])\nsen_subidx = re.search(\"mean sensitivity of each subepoch:\", lines[i])\nif sen_idx:\nsensitivity.update({\"epoch\": lines[i][sen_idx.span()[1]:].split(\"=\u003e\")[0].strip()})\nif sen_subidx:\nsensitivity.update({\"subepoch\": lines[i][sen_subidx.span()[1]:].strip()})\nelif re.search(\"specificity\", lines[i]):\nspc_idx = re.search(\"mean specificity of epoch:\", lines[i])\nspc_subidx = re.search(\"mean specificity of each subepoch:\", lines[i])\nif spc_idx:\nspecificity.update({\"epoch\": lines[i][spc_idx.span()[1]:].split(\"=\u003e\")[0].strip()})\nif spc_subidx:\nspecificity.update({\"subepoch\": lines[i][spc_subidx.span()[1]:].strip()})\nelif re.search(\"dice\", lines[i]):\ndice_idx = re.search(\"mean dice of epoch:\", lines[i])\ndice_subidx = re.search(\"mean dice of each subepoch:\", lines[i])\nif dice_idx:\ndice.update({\"epoch\": lines[i][dice_idx.span()[1]:].strip()})\nif dice_subidx:\ndice.update({\"subepoch\": lines[i][dice_subidx.span()[1]:].strip()})\noutput = {\n\"accuracy\": accuracy,\n\"sensitivity\": sensitivity,\n\"specificity\": specificity,\n\"dice\": dice\n}\nl = lines[0][re.search(\"Class-\", lines[0]).span()[1]:re.search(\"Class-\", lines[0]).span()[1] + 1]\nreturn {key + '_class_' + l: output}\ndef main():\ncount = readfile()\nfor i in range(count):\nwith open('data/Epoch_{}.json'.format(i), 'w+') as f:\nf.write(subfile_extract(i))\nif __name__ == \"__main__\":\nmain()\ngithub地址：https://github.com/sadscv/gadgets/tree/master/ZJ_text_extract。这是师兄的github，大家可以关注一下呀\n最后，感谢师兄！！！！！！！","data":"2018年04月24日 09:22:06"}
{"_id":{"$oid":"5d343b2162f717dc0659b361"},"title":"Python与自然语言处理（一）搭建环境","author":"monkey131499","content":"参考书籍《Python自然语言处理》，书籍中的版本是Python2和NLTK2，我使用的版本是Python3和NLTK3\n\n实验环境Windows8.1，已有Python3.4，并安装了NumPy, Matplotlib，参考：http://blog.csdn.net/monkey131499/article/details/50734183\n\n安装NLTK3，Natural Language Toolkit，自然语言工具包，地址：http://www.nltk.org/\n安装命令：pip install nltk\n安装完成后测试：import nltk\n\n\n\n没有报错即表明安装成功。\nNLTK包含大量的软件、数据和文档，可以进行文本分析和语言结构分析等。数据资源可以自行下载使用。地址：http://www.nltk.org/data.html，数据列表：http://www.nltk.org/nltk_data/\n下载NLTK-Data，在Python中输入命令：\n\u003e\u003e\u003eimport nltk\n\u003e\u003e\u003enltk.download()\n弹出新的窗口，用于选择下载的资源\n\n点击File可以更改下载安装的路径。all表示全部数据集合，all-corpora表示只有语料库和没有语法或训练的模型，book表示只有书籍中例子或练习的数据。需要注意一点，就是数据的保存路径，要么在C盘中，要么在Python的根目录下，否则后面程序调用数据的时候会因为找不到而报错。\n\n【注意：软件安装需求：Python、NLTK、NLTK-Data必须安装，NumPy和Matplotlin推荐安装，NetworkX和Prover9可选安装】\n简单测试NLTK分词功能：\n但是在词性标注上就出现问题了，百度也没有明确的解决办法，若有大神知道是什么原因请不吝赐教！\n\n\n词性标注功能就先暂且放一放。\n下面看一下NLTK数据的几种方法：\n1.加载数据\nfrom nltk.book import *\n\n2.搜索文本\nprint(text1.concordance('monstrous'))\n\n3.相似文本\nprint(text1.similar('monstrous'))\n\n4.共用词汇的上下文\nprint(text2.common_contexts(['monstrous','very']))\n\n5.词汇分布图\ntext4.dispersion_plot(['citizens','democracy','freedom','duties','America'])\n6.词汇统计\n#encoding=utf-8 import nltk from nltk.book import * print('~~~~~~~~~~~~~~~~~~~~~~~~~') print('文档text3的长度：',len(text3)) print('文档text3词汇和标识符排序：',sorted(set(text3))) print('文档text3词汇和标识符总数：',len(set(text3))) print('单个词汇平均使用次数：',len(text3)*1.0/len(set(text3))) print('单词 Abram在text3中使用次数：',text3.count('Abram')) print('单词Abram在text3中使用百分率：',text3.count('Abram')*100/len(text3))\n\n\n暂时先练习到这里，基本上对NLTK-Data有了一定的了解，以及学会了其基本使用方法。","data":"2016年04月08日 14:46:30"}
{"_id":{"$oid":"5d343b2262f717dc0659b363"},"title":"华为诺亚方舟实验室主任李航：迎接自然语言处理新时代","author":"csdn_csdn__AI","content":"作者简介：李航，华为技术有限公司诺亚方舟实验室主任。主要研究方向为信息检索、自然语言处理、机器学习等。\n本文经李航博士授权发布，未经作者允许不得转载。\n欢迎人工智能领域技术投稿、约稿、给文章纠错，请发送邮件至heyc@csdn.net\n人类的语言具有什么特性？下面是几位最权威学者的看法。\n语言是草根现象，它像是维基百科，聚集了数以十万计的人的贡献。当人们要找到更好的表达自己思想方式的时候，就发明了术语、俚语、新说法，其中一部分积累到语言中，这就是我们得到语言的过程。\n——史蒂文·平克(Steven Pinker)\n如果语法没有递归结构，那么它将变得不可接受的复杂。因为它有了递归的工具，所以它能够产生无穷多的句子。\n——诺姆·乔姆斯基(Noam Chomsky)\n我们通常的概念系统的大部分都具有比喻性。我们的思考方式，我们所经历的，我们每天做的，都与比喻有关。\n——乔治·雷可夫(George Lakoff)\n当一个人听到或看到一句话的时候，他使用自己所有的知识和智能去理解。这不仅包括语法，也包括他的词汇知识、上下文知识，更重要的，是对相关事物的理解。\n——特里·威诺格拉德(Terry Winograd)\n语言看来是人的认知向外界环境扩展的核心手段。语言的进化也许就是为了扩展我们的认知与外界环境的积极交互。\n——安迪·克拉克(Andy Clark)\n总结起来，不完全规则性、递归性、比喻性、知识关联性、交互性是人类语言的主要特点。这些特性密切关联，体现了语言的本质。上述学者对这些语言特性的研究作出了卓越贡献，他们的论述是对这些特性的最佳诠释。\n本文从语言的特性出发，讨论为什么让计算机理解人类语言（自然语言）是极其困难的，提出自然语言处理研究应该采取的策略。\n为什么自然语言理解很难？\n自然语言理解\n你说一句话，如何判断别人（或者计算机）是否真正理解了你的意思？这是一个难解的问题。到目前为止，自然语言理解主要有两个定义，一个是基于表示的，一个是基于行为的。对于前者，如果你说“哈利·波特”，别人把它联系到了大脑中的哈利·波特的概念（表示），那么就认为他理解了你的意思。而对于后者，如果你说“给我拿一杯茶来”，别人真的按你说的做了（行为），就认为他理解了你的意思（图1）。\n\n\n图1 人通过语言给出命令，机器人若能正确执行，就认为它可以“理解”语言\n现在的人工智能研究中，人们开始倾向于采用后者的定义，因为这样更容易评价任务驱动、端到端的语言理解系统的能力。\n语言的特性\n下面结合语言学、认知科学、脑科学的最新研究成果，对语言的主要特性进行介绍。\n不完全规则性\n语言具有一定规范，语言的规范可以用语法来描述，但是，几乎所有的语法规则都存在例外。语法规则中一定有逻辑不一致、功能冗余的现象。正如语言学家爱德华·萨丕尔(Edward Sapir)所说，“所有语法都有漏洞(all grammars leak)”。这是为什么？\n其中一个重要原因是，语言不是一个人发明的，甚至不是一组人发明的，而是成千上万人经过成千上万年的时间不断建立起来的，而且在不断演化，这个过程跟人们构建维基百科的过程非常相似。这是认知学家平克等人的观点[1,2]，也被越来越多的人接受。\n语言的基本单元是词汇和语法规则。为了顺畅地交流，需要人们对词汇和语法有基本的共识及准确的使用。另一方面，词汇和语法又不是一成不变的。为了更好地表达自己的思想，人们会不断地去扩展已有词汇和语法规则的使用范围，或者增加新的词汇和语法规则。\n语言中不断有大量的新词汇涌现，但其中大部分会逐渐消失，只有真正有生命力的表达才能留存下来。每一个语言的词汇都在不断增加，随着文明的进步，这个趋势会越来越明显。\n语法是相对稳定的。在远古时代，语言曾经历过“语法大发明”的时期，后来逐渐趋于成熟。但是即使在现代，语法也不是一成不变的。首先，有一个趋势是语法变得越来越简单。比如，英语中以前说“We shall”、“I shall”，现在逐渐变成“We will”、“I will”。另外，受其他语言影响，语法也会发生变异。比如，非洲美国裔英语（也被称为黑人英语）是受非洲语言影响而形成的一种英语变种，在这个语言中，“I working”、“you working”是正确的说法，笔者猜测可能是受其他语言的影响。\n不完全规则性是语言作为人类交流手段而动态发展的必然结果。\n递归性\n现在普遍认为，词汇应该有100万年以上的历史，而语法大概只有7万年左右的历史。而正是在7万年前，智人(Homo Sapiens)，也就是现在人类的祖先，开始从非洲大陆迁移至欧亚大陆，与此同时开始发明各种语言。（语言学中， 只要有口头语就被认为是“语言”， 而不需要有书面语。）\n黑猩猩也能使用一些简单的词汇，但我们不认为黑猩猩拥有语言。因为它们不能把词汇组合起来构成句子。组合性、递归性是语言的重要特点。递归的例子如下：“她觉得很好”，“他认为她觉得很好”，“我想他认为她觉得很好”⋯⋯理论上可以无限扩展。\n1956年，语言学家乔姆斯基提出了文法体系，在人类历史上首次用数学模型对语法现象做出严谨的刻画。乔姆斯基特别指出，递归性属于语法的重要特性。只有有了递归这个工具，我们才能够生成无穷多的表达，语言才拥有丰富的表达能力[3]。\n比喻性\n比喻的本质是把表面不相关联的概念，通过它们背后的相似性联系起来。比如微信里的“潜水”。把“潜水”和在微信里“沉默不语”这两个概念联系起来，就是一个比喻。认知科学家雷可夫等认为比喻是语言的重要特性，语言中的发明基本都是基于比喻的[4-6]。\n比喻的使用是人类认知能力、语言能力的体现。中文说“开灯”，英语说“turn on the light”，应该始于比喻，开始有一个人或几个人同时发明了这些比喻，后来变成了固定说法，被广泛使用。据观察，一个英语母语的四岁男孩儿，有创意地说出“open the light”（直译就是开灯）。这个例子说明，人天生就有比喻、创造的能力。\n比喻是否能被接受并在语言中使用，具有一定的偶然性。一旦比喻变成固定用法，人们就开始习惯性地使用，而不考虑其缘由。比如，中文中所说的“上厕所”、“下厨房”。这些习惯用法都是比喻性的，但是随着时间的推移，已经很难考证当初为什么做出这样的比喻。（互联网上有许多关于“ 上厕所”、“ 下厨房” 语源的讨论。）\n比喻也依赖于语言使用的环境与文化。据说，在大多数语言里都有“温暖的爱”这个比喻，如英语中说“warm affection”，在日语中说“暖かい愛”。这些语言都是温带和寒带的语言，热带的语言里就看不到这样的比喻。\n知识关联性\n十几年前，脑科学研究中有一个有趣的发现。当把电极插到猴子的大脑前运动皮质(pre-motor cortex)时，有一个脑细胞会在猴子自己吃香蕉和看别人吃香蕉时，同样处于兴奋状态，也就是说对猴子来说这个脑细胞对应着“吃香蕉”的概念。（猴子和人的运动都是由小脑控制， 但大脑的前运动皮质也与运动有关。）\n后来对人脑做类似的实验，但使用功能磁共振。让人实际做和想象做各种动作，比如张嘴和想象张嘴，接球和想象接球。结果发现，对同一动作，实际做和想象做大脑的前运动皮质中发生反应的部位完全一致。\n现在一个得到广泛支持的理论认为，对于同一个概念，大脑用固定的脑细胞去记忆，人理解语言的过程，就是激活相关概念的脑细胞，并关联这些概念的过程[6]。\n表示同一个概念的脑细胞，可以通过不同的方式被激活。例如，有一个细胞表示人在喝水，当你看到人在喝水的时候，或者当你从书中读到人在喝水的时候，这个脑细胞同样会被激活。这也能解释为什么我们在读小说的时候常常有身临其境的感觉。\n每个人把自己经历的事件进行编码，存储记忆在脑细胞中，在与外界的交互中这些脑细胞被激活，相关的记忆被唤醒。所以，不同人对同样的语言会有不同的理解，因为他们的经历不同。但也有许多共性，因为大家在交流过程中，相互激活对方脑中的表示相同内容的细胞。\n发明比喻的时候，大脑中表示两个不同概念的部位都开始兴奋，相关的脑细胞之间产生新的连接，概念之间产生关联，这个过程被称为神经结合(neural binding)，是现在脑科学研究的重要课题[6]。\n语言的理解实际上动用了大脑中所有的相关知识，是一个非常复杂的过程。这一点在计算机学家威诺格拉德开发的著名的对话系统SHRDLU中也有充分体现[7]。\n交互性\n语言作为人类交流的工具，其重要特点就是交互。哲学家克拉克等人认为，与环境的交互是人或者动物作为智能体存在的必要条件，或者说，离开了与环境的交互，智能就无从谈起[8]。\n\n\n图2 主动猫与被动猫的实验\n脑科学家赫尔德(Richard Held)和海恩(Alan Hein)的实验能够很好地说明与环境的交互对智能体的重要性[9]。实验对象是一对刚出生的孪生猫，把其中一只当作“主动猫”，另一只当作“被动猫”。白天把它们放到转马上，主动猫脚能着地，可以行走；被动猫被放在篮子里，不能行走。主动猫走动时，转马被带动旋转，这时被动猫也跟着旋转（图2）。晚上把它们放到黑暗处，让它们吃睡。两个月以后，将它们放出去。主动猫和一般的猫没有什么不同，可以正常行走，但被动猫已经失去了行走的能力，走路时要么撞墙，要么跌倒。赫尔德和海恩对10对孪生猫做同样的实验，得到同样的结论。\n以上实验说明，对人或者动物来说，虽然拥有先天能力，但在成长的过程中如果不能在与环境的交互中使用，该能力也会丧失。这一点，语言能力也一样。当狼孩被发现时，他已不会说话，因为在他的成长阶段没有与人进行语言交互，没有学习语言。\n语言的理解需要在与环境（包括社会、文化）的交互中进行，这点可以在外语学习的过程中体会到。在外语使用环境中学习外语，最容易理解，提高也最快。严格地说，语言是不能翻译的，只能解释。语言必须在其环境中学习与使用。\n自然语言理解的困难\n人的语言理解是一个非常复杂的过程，现在科学对其有了非常粗浅的了解，离理解明了所有细节的程度还相差甚远。\n同时，让计算机“理解”人类的语言是极其困难的，因为当代计算机和人脑拥有完全不同的架构。在当代计算机上实现不完全规则性和递归性，意味着进行复杂的组合计算；实现比喻性、知识关联性、交互性，意味着进行全局的穷举计算。是否可行，仍存在巨大疑问。实现能像人一样理解语言的计算机，需要有全新的体系架构，意味着计算机科学发生革命性的进步。\n让计算机处理有限的语言表达，让它看似很智能，其实不难，只要写出有限的规则就有可能做到。这样的系统做出的演示往往具有一定的欺骗性，让人误以为实现了语言理解。其实一个系统能够理解语言意味着理论上能够理解无穷多的语言表达。例如，表1给出了“给我拿一杯茶来”的部分同义说法，理论上类似的表达是无穷多的，一个能理解语言的计算机应该能够判断这样的表达都是同一个意思。而这不是一件容易的事情。关键是要让计算机拥有强健的、通用的语言处理能力。\n人们的错觉\n人们通常认识不到计算机的自然语言理解极具困难这一事实，可能有以下几个原因。\n自然语言具有一定的规律。很多人以为只要写一些规则就可以实现自然语言理解系统，这只是看到了一些非常表面的现象。\n人脑的信息处理大部分都是在下意识中进行，有人说其比例高达98%。意识进行的是顺序处理，下意识进行的是并行处理。语言处理也一样。也就是说，人脑进行的大量的语言处理，我们自己是感受不到的。认为语言理解比较简单实际上是我们的错觉。正如彩虹、日出、日落，我们所能直观感受到的，只是现实中发生的很小一部分。\n绝大部分人可以在12岁之前几乎无障碍地学会自己的母语，在这个过程中，伴随着大脑的发育，可以在很短的时间内掌握大量的词汇和复杂的语法规则。这个现象是一种奇迹，仍然是认知科学研究的重要课题。\n自然语言处理的策略\n自然语言处理\n自然语言理解是困难的，但是“自然语言处理”却是可行的。现实中可以让计算机完成一些特定的语言处理任务，比如自动问答、机器翻译、多轮对话，为人们提供帮助，使计算机成为人类的智能助手。现在已部分实现，在可预见的未来可以基本实现，这也是现在自然语言处理研究的目标。\n自然语言处理之所以现实可行，主要是因为将人的语言理解过程进行了合理的简化或者限制，而这些简化与限制可以回避自然语言理解中的难题，让计算机表面上像人一样完成语言处理任务。下面以知识问答和多轮对话为例来说明。\n人的知识问答可能有这样的处理：得到问题以后，分析问题的内容，理解问题的意思，进行相关的推理，检索相关的知识，决定回答的内容，最后产生回答。现在计算机做知识问答，没有真正的自然语言理解，通常把其中的困难步骤省略简化。计算机的知识问答一般只有以下步骤：分析问题的内容，检索相关的知识，产生回答（见图3）。\n\n\n图3 计算机问答处理过程是人的问答处理过程的简化\n人的对话可能有这样的处理：对方发话以后，分析发话的内容，理解发话的意图，进行相关的推理，决定回话的内容，最后产生回话。如果对话是多轮，还有对话管理机制。现在计算机做多轮对话，没有真正的自然语言理解，通常把对话的领域固定，比如订机票、订酒店，并只能在这个领域内进行（见图4）。\n\n\n图4 计算机的对话处理过程与人的相似， 但适用范围有很大限制\n两大策略\n我们认为，自然语言处理可以采用任务驱动与混合模式两大策略。\n任务驱动的自然语言处理就是在具体的应用中构建系统。这是现在自然语言处理通常采用的策略，仍可以加强。任务驱动的好处是，可以帮助解决避开自然语言理解之后仍存在的一些问题，而这些问题在实际应用中也相对容易解决。\n可以认为自然语言处理经历了三代技术发展演进，第一代基于规则，第二代基于统计，第三代属于现在，基于深度学习。各自有优势和局限。未来的发展方向应该是将这些不同的技术有效地结合起来，即采用混合模式。\n任务驱动\n人工智能系统都遵循这样的规律，我们称作“人工智能闭环”（图5）。先有系统，后有用户，然后产生大量数据，机器学习算法可以基于数据构建模型，提高系统的性能，系统性能提高后又能更好地服务于用户，形成一个闭环。人工智能系统可以在这个闭环中不断改进，提升智能水平。自然语言处理也不例外。当任务确定时，就更容易开展基于人工智能闭环的技术开发。\n\n\n图5 人工智能闭环\n混合模式\n统计方法比起规则方法，能够更好地应对不确定性。人类的智能，包括语言能力，从数学角度来看，最大的特点就是拥有不确定性。事实证明，统计方法是应对不确定性的最有利工具。\n统计方法可以从数据中概括出概率统计规律，构建模型，拥有举一反三的泛化能力。规则方法则不具备这一能力。\n深度学习本质也是统计方法，其特点是复杂非线性模型的学习。相比之下，传统的统计方法的模型都是简单的。事实证明，相比传统的统计方法，深度学习有更强的模式学习能力，能够更好地处理复杂的模式识别问题。\n规则方法可以有效地利用人给定的知识，而统计方法和深度学习方法，至少是现在，还没有和知识推理有效地结合起来。\n统计方法、深度学习方法都依赖于数据。在没有数据或数据稀少的情况下，很难有用武之地。而规则方法，在这种情况下，至少可以派上一定用场。\n综上所述，规则、统计（即统计机器学习）、深度学习三种方法都各有优势和局限（见表2）。可以预见，将三者有效地结合，会使人工智能、自然语言处理的水平大幅度提升，这是自然语言处理未来的发展方向。\n华为研究团队最近提出了受教式人工智能(Educated AI,EAI)的想法，认为这是未来人工智能的范式。其核心思想是，人工智能系统拥有基本的处理以及学习能力，在用户的指导下不断提高智能水平[10]。受教式人工智能采用的就是混合模式，因为人的指导有时是以规则的形式呈现的。\n自然语言处理新时代\n表3总结了现在自然语言处理在各个任务上所能达到的水平，是从不同数据集上得到的实验结果。可以看出，自然语言处理距离人们的期待还有一定的差距，现实中这些任务也只是部分实现了实用化。\n可以预见，在不远的将来，随着自然语言处理技术的进步，这些性能指标会不断提升。事实上，近年深度学习在自然语言处理的应用，已使机器翻译、单轮对话有了令人惊喜的进步。计算机能够“自如地”进行自然语言处理的时代为期不远。人工智能闭环会推动技术的不断改进，规则、统计、深度学习的结合会产生更强大的技术。现在我们正在进入自然语言处理的一个全新的时代！\n参考文献\n[1] Pinker S. The Language Instinct, 1994.\n[2] Pinker S. Linguistics as a Window to Understanding the Brain. Big Think, 2013.\n[3] Chomsky N. Three models for the description of language [J]. IRE Transactions on Information Theory,1956, 2(3):113-124.\n[4] Taylor J. Linguistic Categorization: Prototypes in Linguistic Theory, 1996.\n[5] Lakoff G, Johnson M. Metaphors We Live by, 1980.\n[6] Lakoff G. What Studying the Brain Tells Us About Arts\nEducation, 2013.\n[7] Winograd T. Understanding Natural Language [J]. Cognitive Psychology, 1972, 3(1):1-191.\n[8] Clark A. Supersizing the Mind: Embodiment, Action, and Cognitive Extension, 2010.\n[9] Held R, Hein A. Movement-Produced Stimulation in Development of Visually Guided Behavior [J]. Journal of Comparative and Physiological Psychology, 1963, 56(5):872-6.\n[10]李航, 张宝峰, 霍大伟等. 华为研究的畅想： Educated AI. 中国计算机学会通讯, 2016, 12(1): 62-65.","data":"2017年02月28日 09:59:29"}
{"_id":{"$oid":"5d343b2362f717dc0659b366"},"title":"自然语言处理与临床医学的关系","author":"GGGAnk","content":"自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信的方法。在如今互联网、大数据与人工智能迅速发展的时代背景下，自然语言处理能够应用的场所极大地扩大。除开计算机领域，自然语言处理与大数据早已逐渐渗透到其他各个领域，医学方面也囊括在内。\n近年来医疗数据挖掘发展迅速，然而医疗数据结构化处于起步阶段，更多的医疗数据仍然以自然语言形式出现。我们可以通过自然语言处理技术研制出一套完整的系统辅助完成汇总医学领域知识的过程，将知识提炼出来，提取其中有用的诊疗信息，最终形成知识网络。比如说人体解剖，学生可以通过完整的系统了解人体的组织、器官以及各种骨骼，同时通过自然语言处理，学生还能了解他们之间的联系。然后，包括各种病症与病理，通过这项技术，学生也能系统地学习。自然语言处理为医学知识的线上学习提供了可能，当然这是建立在技术成熟的基础之上。\n自然语言处理在临床信息系统中起着作用，临床信息系统能将文本形式的病历报告转换成编码数据以供使用，医生与病人方面都能受益。通过完善医疗系统，病人能较早地知晓自己的病情，为救治提供了帮助。医生能够通过这项技术调动病人的病历以便查看，此外还能实时观察病人的病情，为治疗带来了便利。","data":"2019年06月20日 17:33:36"}
{"_id":{"$oid":"5d343b2362f717dc0659b368"},"title":"读冯志伟老前辈的《自然语言处理与现状》","author":"zhangbill555","content":"写在前面\n很早之前就接触过冯志伟老先生的一些有关自然语言处理的相关文字，彼时，我还是一个大三的文科外语专业的小罗罗，而此时的我已经冠上了为“高级软件工程师”的虚名。当时在直接阅读相关的文字时，因为还未有对技术有深入的实践，对自然语言的处理以及和计算机科学，数学之间的关系并没有足够清晰的了解。在经历了最初职场的迷惑和对工程技术的实践之后，似乎很多曾经脑中模糊的概念，通过了一个个软件的系统和一行行代码，都被串到了一起。今天又找了一篇08年，冯志伟老先生在《中国外语》上发表的一篇介绍自然语言处理的文章，重新开始组织脑中散落在各个角落的各种。\n关于早期的萌芽\n根据冯老先生的阐述，自然语言处理其实在二战时期，计算机技术出现之前就已经发生了极其重要的萌芽和分支发展。在图灵刚刚发明计算机的时候，战争的惨烈刚刚教了所有人类一堂课。二十世纪40年代到50年代之间，除了当时给世界带来极大震撼的计算机技术，在美国还有两个人，在进行着他们的工作：第一项就是乔姆斯基对形式语言的研究工作，另一个就是当时在工作中发展出来的香农的基于概率和信息论模型的研究。在这个时期，其实计算机就已经和自然语言产生了极大的交互，计算机在当时虽然更多的是一种代替人类进行大量重复计算的工具，然而，图灵自己发明“思维机器”的时候，内心就一直没有停止对智能，语言，数学之间关系的思考。而众所周知的图灵测试，也是在语言媒介的基础上对计算智能的一种评估而已。也正是图灵的工作，对后世现代计算机科学产生了直接的推动作用，也延续下去产生了神经元理论等各种计算框架。香农的信息论则在概率和统计的基础上对语言和计算机语言进行了相当的刻画。而乔姆斯基在其基础上，深入的研究直接产生了形式语言的模型框架。在这里，计算机的计算过程（计算机语言）和自然语言终于得到了统一，被放到了同一个水平进行研究。而其中产生的最重要的上下文无关语法，也在计算机语言的领域各自得到了应有的发展和成长。如果没有这些，之后产生的编译器估计也就无法产生，也就不用再提高级编程语言和复杂的计算机系统了。\n关于中期的发展\n从二十世纪60年代到80年代，自然语言处理，在计算机技术的飞速发展下，也取得了相当程度的成果。二十世纪60年代，法国格勒诺布尔理工大学的著名数学家沃古瓦开始了自动翻译系统的研制，沃古瓦也是计算语言学的创始人和第一届主席。在这一过程当中，不同的国家和组织对机器翻译都投入了大量的人力，物力和财力，人类历史上第一次可以通过技术尝试去把不同语言之间隔阂的通天之塔打通，因此大部分人对此应该还是抱着很大的希望的。但是在实际的过程中，机器翻译系统的研制，遇到了各种问题。这些问题的复杂度也远远超过了原来大家的预期，当然，在这过程中，因为希望解决这一问题，产生了各种各样的模型和解决方案。虽然，最后的结果并不是都尽如人意，但是却为后来的各个相关分支领域的发展，奠定了极其重要的基础。统计学，逻辑学，语言学，以及后来丘奇的计算理论等等。\n关于后期的繁荣\n二十世纪90年代之后，自然语言处理的发展进入了相当繁荣的一个时期。在这一阶段，有一个重要的东西，诞生了：万维网在这一时期开始澎湃的生长起来。94年，万维网协会成立，从此之后，似乎整个世界在一瞬间都被互联网攻陷了。从此之后，各种学科的发展，尤其是计算机科学的发展，在互联网的冲击下，产生了很多原来没有的计算模型，大数据和各种统计模型开始大行其道。自然语言处理在这段时间，也在大数据和概率统计模型下，得到了飞速的发展，同时也产生了一大批高科技公司，并对其发展起到了不同的推动作用。早期的雅虎搜索，后来的谷歌，中国的百度，大量的基于web的应用和各种社交工具，一切都让自然语言处理在飞速的发展中。在这个过程中，各种数学算法和计算模型越来越重要，最近刚兴起不久的神经网络，深度学习，各种数据挖掘算法，机器学习等等，都不断的在消除人和机器之间交流的限制。也许在不久的将来，在互联网的基础上，自然语言处理中遇到的问题不再是问题。不同语言的人之间的沟通可以畅通无阻，人和机器之间的沟通可以畅通无阻。\n关于未来\n在近期的可见的各种技术的发展过程中，似乎人和机器走到了一个相对对立的情形。如果继续这样发展下去，很多人开始担心：机器的智能如果真的实现了类似人类的智能，并远远超越了人类，这样的情况下，人类会不会被自己的发明灭绝呢？或者人们开始和自己的机器人情侣约会，人类这个种族还会继续延续么？\n似乎现在一切都不明朗，但是我个人还是抱着乐观的态度，正如在机器出现之初，人们对机器的噪音，造成的各种意外，以及其他危险的抱怨。现在人类已经无法离开机器，如果这个世界突然没有了各种机械和电子机器，也许人类都会疯掉吧。\n也希望我可以在这个领域可以继续下去，继续发挥我个人的热量，也许有一天，智能将不仅仅限于人类。","data":"2016年05月05日 00:34:27"}
{"_id":{"$oid":"5d343b2462f717dc0659b36a"},"title":"《自然语言处理（哈工大 关毅 64集视频）》学习笔记：第七章 句法分析技术","author":"miniAI学堂","content":"视频列表：\n43 句法分析技术（一）\n44 句法分析技术（二）\n45 句法分析技术（三）\n46 句法分析技术（四）\n47 句法分析技术（五）\n43 句法分析技术（一）\n第七章 句法分析技术\n什么是句法分析\n判断输入的词序列能否构成一个合乎语法的句子，确定合乎语法句子的句法结构\n运用句法规则和其他知识将输入句子中词之间的线性次序，变成一个非线性的数据结构（例如短语结构树或有向无环图）\n\n\n为什么要进行句法分析\n例一：音字转换例\n一只小花猫\n例二：机器翻译示例\nJan hit the girl with long hair\nJan hit the girl with a hammer\n例三：信息检索例\n哪个球队获得了亚洲杯冠军？\n日本队击败中国队获得亚洲杯冠军\n例四：语法歧义：一个句子对应着几种句法分析结果\n“咬死了猎人的狗”\n“那只狼咬死了猎人的狗”\n“那只咬死了猎人的狗失踪了”\n汉语句法分析的独特性\n根据朱德熙《语法答问》《语法讲义》\n汉语没有形态\n语序灵活\n词类和句法成分不存在一一对应的关系\n汉语句子的构造原则与词组的构造原则基本上是一致的\n汉语语法形式化工作滞后\n句法分析系统\n一个句法分析系统通常由两部分组成：\n形式语法体系\n匹配模式\n基于模板的方法\n短语结构语法\n句法规则\n特征制约\n语义解释\n扩充转移网络\n树邻接语法(TAG)\n44 句法分析技术（二）\n基于合一运算的语法（广义短语结构语法、词汇功能语法、功能合一语法、基于中心词驱动的短语结构语法(HPSG)）\n基于词的语法（链语法、依存语法、配价语法）\n分析控制机制\n模式匹配技术\n基于短语结构语法分析算法（厄尔利（ Earley ）分析算法、富田胜（ Tomida ）分析算法、线图（Chart）分析算法、确定性分析算法等等）\n基于扩充转移网络的分析算法\n链分析算法\n\n\nＧ\n=\n(\nN\n,\n∑\n,\nP\n,\nS\n)\nＧ = (N,\\sum ,P, S)\nＧ=(N,∑,P,S)是一个文法，α→β ∈ P\n0型文法\n对α→β不作任何限制\n1型文法\n|α|≤|β|\n2型文法：上下文无关文法\nα∈N\n3型文法：正则文法\nA→aB或A→a: G是右线性文法，L(G)是3型语言\nA→Ba或A→ａ: G是左线性文法，L(G)是3型语言\n在自然语言处理中研究和应用较多的是2型文法和3型文法\n推导\n一个字串的推导是一系列文法规则的应用\nS→NP VP →John V NP →John V NP PP →John ate fish P NP →John ate fish with bone\n这一推导的过程可以用分析树来表示\n\n根据某上文下无关文法从起始非终结符可能推导出的所有字串的集合称为由该CFG定义的语言\nCFG的形式化定义\n一个CFG是一个四元组\nＧ\n=\n\u0026lt;\nN\n,\n∑\n,\nP\n,\nS\n\u0026gt;\nＧ = \u0026lt;N,\\sum ,P, S\u0026gt;\nＧ=\u003cN,∑,P,S\u003e\nN是非终结符的集合\n\n∑\n\\sum\n∑是终结符的集合\nP是产生式的集合，其中每个产生式形如:\n\nA\n→\nα\nA\\rightarrow \\alpha\nA→α\nA是非终结符\n\nα\n\\alpha\nα是由终结符与非终结符构成的字串\nS是一个起始非终结符\n上下文无关文法示例（context free grammar）\n语言的合法性\n概率上下文无关文法（Probabilistic (Stochastic) Context Free Grammar）\n随机上下文无关语法可以直接统计语言学中词与词、词与词组以及词组与词组的规约信息，并且可以由语法规则生成给定句子的概率。\n定义\n定义：一个随机上下文无关语法（PCFG）由以下5部分组成：\n（1）一个非终结符号集N\n（2）一个终结符号集∑\n（3）一个开始非终结符S∈N\n（4）一个产生式集R\n（5）对于任意产生式r∈R，其概率为P®\n产生式具有形式X→Y，其中，X∈ N, Y ∈(N∪ ∑)*\n\n∑\nλ\nP\n(\nX\n→\nλ\n)\n=\n1\n{\\sum_{}^{\\lambda }}P(X\\rightarrow \\lambda )=1\n∑λ P(X→λ)=1\nPCFG的三个基本假设\nCFG的简单概率拓广\n∑\nλ\nP\n(\nX\n→\nλ\n)\n=\n1\n{\\sum_{}^{\\lambda }}P(X\\rightarrow \\lambda )=1\n∑λ P(X→λ)=1\n基本假设\n位置无关(Place invariance)\n上下文无关(Context-free)\n祖先无关(Ancestor-free)\n分析树的概率等于所有施用规则概率之积\n\nP(tree1)=1/22/32/3=2/9\nP(tree2)=1/21/31/3=1/18\nP(tree3)=1/21/2=1/4\nP(tree4)=1/21/2=1/4\nPCFG的三个基本问题\n1、一个语句\nW\n=\nw\n1\nw\n2\n…\n.\nw\nn\nW=w_{1}w_{2}….w_{n}\nW=w1 w2 ….wn 的P(W|G),也就是产生语句W的概率？\n\nP\n(\nW\n∣\nG\n)\nP(W|G)\nP(W∣G)\n2、在语句W的句法结构有歧义的情况下,如何快速选择最佳的语法分析(parse) ?\n\na\nr\ng\nm\na\nx\nt\nr\ne\ne\nP\n(\nt\nr\ne\ne\n∣\nW\n,\nG\n)\n\\underset{tree}{argmax}P(tree|W,G)\ntreeargmax P(tree∣W,G)\n3、如何从语料库中训练G的概率参数,使得P(W|G)最大\n\na\nr\ng\nm\na\nx\nG\nP\n(\nt\nr\ne\ne\n∣\nW\n,\nG\n)\n\\underset{G}{argmax}P(tree|W,G)\nGargmax P(tree∣W,G)\n-问题1\u00262解决思路\n\n向内（Inside）算法\n\n非终结符A的内部概率（Inside probability）\n定义为根据文法G从A推出词串\nw\ni\n.\n.\n.\nw\nj\nw_{i}...w_{j}\nwi ...wj 的概率，\n记为\nα\ni\n,\nj\n(\nA\n)\n\\alpha _{i,j}(A)\nαi,j (A),\n\ni\n≤\nj\ni\\leq j\ni≤j\n\nα\ni\n,\nj\n(\nA\n)\n\\alpha _{i,j}(A)\nαi,j (A)称为向内变量\n45 句法分析技术（三）\n向内概率公式\n\n向内算法计算示例:\nS→NP VP 1.0 NP→NP PP 0.4\nPP→P NP 1.0 NP→John 0.1\nVP→V NP 0.7 NP→bone 0.18\nVP→VP PP 0.3 NP→star 0.04\nP→with 1.0 NP→fish 0.18\nV→ate 1.0 NP→telescope 0.1\n\n\n\n问题2\n\n\nViterbi 算法\n输入： G=(S,N,∑,R,P),字符串\nW\n=\nw\n1\nw\n2\n…\n.\nw\nn\nW=w_{1}w_{2}….w_{n}\nW=w1 w2 ….wn\n输出：t* ( W在G下最可能的分析树)\nViterbi算法示例(自底向上)\n问题3 参数训练问题-有指导学习方法\n从树库直接统计——Treebank Grammar\n最大似然估计\n依赖于艰巨的工程：树库建设\nPCFG的优缺点\n优点\n可以对句法分析的歧义结果进行概率排序\n提高文法的容错能力（robustness）\n缺点\n没有考虑词对结构分析的影响\n没有考虑上下文对结构分析的影响\n许多当前的获得较高精度的句法分析系统以PCFG为基础\n46 句法分析技术（四）\n浅层句法分析技术\n从完全句法分析（complete parsing）到浅层句法分析（shallow parsing）\n真实语料的复杂性\n语言知识的不足\n提高分析的效率\n应用目标驱动\n浅层分析的其他名称：部分分析（partial parsing），组块分析（ chunking ）\n\n基于HMM的浅层分析技术\n识别目标：非递归的NP\n组块分析：在线性序列中插入括号，来标示组块边界\n[The/DT prosecutor/NN] said/VB in/IN [closing/NN] that/CS …\n\n\n级联式有限状态句法分析\n（1）从左向右扫描输入字符串，按照Li层级上的正则表达式模式进行归约，得到新的模式序列，对于输入串中无法归约的符号，直接输出；\n（2）i=i+1，在新的Li层级上，用正则表达式模式进行归约\n（3）不断进行上述步骤，直到无法归约为止；\n（4）如果归约过程中有多种选择，以覆盖范围最大的归约子串为输入结果\n\n\n47 句法分析技术（五）\n小结\n以PCFG为重点介绍了近年来句法分析技术的基本原理与方法\n句法分析是当前语言处理技术的瓶颈问题之一\n句法分析是语义分析（更深层次的语言理解）的必由之路\n句法是形式、语义是内容\n句法的强制性和语义的决定性\n句法系统和语义系统是两个不同的系统，它们各自独立而又相互依存，彼此的对应关系十分复杂\n致谢\n关毅老师，现为哈工大计算机学院语言技术中心教授，博士生导师。通过认真学习了《自然语言处理（哈工大 关毅 64集视频）》1（来自互联网）的课程，受益良多，在此感谢关毅老师的辛勤工作！为进一步深入理解课程内容，对部分内容进行了延伸学习2 3 456，在此分享，期待对大家有所帮助，欢迎加我微信（验证：NLP），一起学习讨论，不足之处，欢迎指正。\n\n参考文献\n《自然语言处理（哈工大 关毅 64集视频）》（来自互联网） ↩︎\n王晓龙、关毅 《计算机自然语言处理》 清华大学出版社 2005年 ↩︎\n哈工大语言技术平台云官网：http://ltp.ai/ ↩︎\nSteven Bird,Natural Language Processing with Python,2015 ↩︎\nClaude E. Shannon. “Prediction and Entropy of Printed English”, Bell System Technical Journal 30:50-64. 195 ↩︎\nAn Empirical Study of Smoothing Techniques for Language Modeling, Stanley F. Chen ↩︎","data":"2019年01月11日 20:22:15"}
{"_id":{"$oid":"5d343b2462f717dc0659b36c"},"title":"自然语言处理与信管","author":"weixin_44934492","content":"我之前一直觉得自然语言处理应该是一个很高深很难去学习的东西，如果我们不去学习人工智能，我们是绝对用不上它的。然而，我上了我们的选修课了解了它之后，我才知道我错了，我真的错了。\n学习之前，我并不怎么了解什么是自然语言处理。之后，我才懂。原来自然语言处理（Natural Language Processing，简称NLP）就是用计算机来处理、理解以及运用人类语言(如中文、英文等)，它属于人工智能的一个分支，是计算机科学与语言学的交叉学科，又常被称为计算语言学。由于自然语言是人类区别于其他动物的根本标志。没有语言，人类的思维也就无从谈起，所以自然语言处理体现了人工智能的最高任务与境界，也就是说，只有当计算机具备了处理自然语言的能力时，机器才算实现了真正的智能。\n我个人认为对于我们信息管理与信息系统专业来说，在庞大的数据流里面来找到我们需要的数据是我们专业的一大难点，但是如果我们结合大数据和这个自然语言，这个过程将会变得无比简单和精确，我们的学习也会变得十分容易。\n我们信息管理与信息系统专业本来就要与计算机经常打交道，所以自然语言处理也就似乎成为了我们所需要掌管的知识。但是，我们也面临着一些严峻的事实：自然语言处理这项技术虽然在进行着突飞猛进的发展，但是却并不是一帆风顺，它有过低谷，也有过高潮。现在我们正面临着新的挑战和机遇。不论是探究自然本质还是付诸实际应用，自然语言处理在将来必定会有令人期待的惊喜和异常快速的发展。 现在的我们正处黄金年龄，风华正茂，唯有努力学习方能不付我们璀璨的年华。","data":"2019年06月19日 16:57:45"}
{"_id":{"$oid":"5d343b2562f717dc0659b36f"},"title":"自然语言处理与通信工程专业的关系","author":"ZhANGZj99","content":"自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。自然语言处理是一门融语言学、计算机科学、数学于一体的科学。因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，所以它与语言学的研究有着密切的联系，但又有重要的区别。自然语言处理并不是一般地研究自然语言，而在于研制能有效地实现自然语言通信的计算机系统，特别是其中的软件系统。因而它是计算机科学的一部分。 用自然语言与计算机进行通信，这是人们长期以来所追求的。因为它既有明显的实际意义，同时也有重要的理论意义：人们可以用自己最习惯的语言来使用计算机，而无需再花大量的时间和精力去学习不很自然和习惯的各种计算机语言；人们也可通过它进一步了解人类的语言能力和智能的机制。 实现人机间自然语言通信意味着要使计算机既能理解自然语言文本的意义，也能以自然语言文本来表达给定的意图、思想等。前者称为自然语言理解，后者称为自然语言生成。因此，自然语言处理大体包括了自然语言理解和自然语言生成两个部分。历史上对自然语言理解研究得较多，而对自然语言生成研究得较少。但这种状况已有所改变。 通信工程的学生主要学习通信系统和通信网方面的基础理论、组成原理和设计方法，受到通信工程实践的基本训练，具备从事现代通信系统和网络的设计、开发、调测和工程应用的基本能力。 自然语言处理研究的内容包括但不限于如下分支领域：文本分类、信息抽取、自动摘要、智能问答、话题推荐、机器翻译、主题词识别、知识库构建、深度文本表示、命名实体识别、文本生成、文本分析（词法、句法、语法）、语音识别与合成等。 随着科技的发展，自然语言会在通信工程得到更多的应用。","data":"2019年06月17日 11:47:02"}
{"_id":{"$oid":"5d343b2562f717dc0659b371"},"title":"Natural 自然语言处理（NLP）「全解析」","author":"人工智能学家","content":"原文来源： 机器人圈\n概要：在自然语言处理方面的研究已经延续了五十多年，而随着计算机的兴起，它的发展也早已超出了语言学的范畴。\n\n\n提起AI，你可能会不假思索的想到自然语言处理、人脸识别、无人驾驶等。那么，你对这些真的了解吗？接下来，我们就以自然语言处理为例，来仔细说一说。\n\n\n\n自然语言处理（Natural Language Processing），简称NLP，广义上定义为通过软件对诸如语音和文本这样的自然语言进行自动操作。\n\n\n在自然语言处理方面的研究已经延续了五十多年，而随着计算机的兴起，它的发展也早已超出了语言学的范畴。\n\n\n而当你读完本篇文章之后，你就会明白什么是自然语言处理，以及它为什么那么重要：\n\n\n•什么是自然语言？它与其他类型的数据有什么不同？\n\n\n•什么使得处理自然语言的工作变得如此具有挑战性？\n\n\n•NLP的领域来自哪里？现代从业者是如何对其进行界定的？\n\n\n现在我们就来深入探索一下！\n\n\n自然语言\n\n\n自然语言是指我们作为人类来说，人与人之间相互沟通的方式，即语音和文本。而我们无时不刻地处在文本的包围之中。\n\n\n想象一下你每天将会看到多少文本：\n\n\n•符号\n\n\n•菜单\n\n\n•电子邮件\n\n\n•短信\n\n\n•网页\n\n\n•还有更多……\n\n\n甚至可以说，这个名单是无休无止的。\n\n\n现在我们先来考虑一下语音。\n\n\n人类作为一个物种，我们可以相互交流，而这要远远超过我们所需要进行的写作。我们不得不承认，相对写作来说，学会说话要简单得多。\n\n\n语音和文字是我们进行相互交流的方式。\n\n\n鉴于这种类型的数据的重要性，我们必须有方法来了解和理解自然语言，就像我们对其他类型的数据一样。\n\n\n自然语言的挑战\n\n\n使用自然语言数据所存在的问题还没有得到解决。\n\n\n该领域的专家已经对其进行了半个多世纪的研究，但必须要承认，它真的很难。\n\n\n对于那些必须花费好多年来学习获得一门语言的孩子来说，这是很难的；对于一个学习语言的成年人来说，这是很难的；对于那些尝试建模的科学家来说，这是很难的；同样，对于那些尝试构建处理自然语言输入或输出系统的工程师来说，这也是很难的。这些任务是如此的困难，以至于图灵将可以用自然语言进行流畅的交流作为他对智能测试的核心。\n\n\n——2010年《数学语言学》，第248页\n\n\n自然语言之所以难主要是因为它很混乱，几乎没有规则可循。\n\n\n不过大部分的时间里我们而已很容易地彼此相互理解。\n\n\n人类语言是非常模糊的……它也在不断变化和演变。从古至今，人类在创造语言、理解语言方面是非常擅长的，并且能够表达，感知和解读非常精细和细微的含义。同时，虽然我们人类是语言使用的极大群体者，但我们在形式理解和描述用以管理语言的规则方面还存在一定的欠缺。\n\n\n——2017年《自然语言处理中的神经网络方法》，第1页\n\n\n从语言学到自然语言处理\n\n\n语言学\n\n\n语言学是对语言进行的科学研究，包括语法、语义学和语音学。\n\n\n古典语言学涉及到语言规则的设计和评估，在语法和语义学的形式方法上取得了很大的进步，但在大多数情况下，自然语言理解中存在的很多有趣的问题遏制了清晰的数学形式。\n\n\n广义上来说，语言学家可以是学习语言的任何人，但更通俗地说，一个以语言学家自居的人可能更侧重于此领域之外的领域。\n\n\n数学是科学的工具。从事自然语言工作的数学家可能将他们的研究称为数学语言学，仅专注于离散数学形式学的使用和自然语言理论（例如形式语言和自动机理论）。\n\n\n计算语言学\n\n\n计算语言学是使用计算机科学工具对语言学进行的现代研究。昨天的语言学可能是今天的计算语言学家，因为计算工具的使用和思考方式的改变已然跨越了研究的大多数领域。\n\n\n计算语言学是对理解和产生自然语言的计算机系统的研究。计算语言学的一个本质上的功能将是对理论语言学家提出的语法进行测试。\n\n\n——1986年《计算语言学》导言，第4-5页\n\n\n大数据和计算机的发展意味着，通过编写和运行软件，可以从大量文本数据集中发现新的、不同的事物。\n\n\n在20世纪90年代，统计方法和统计机器学习开始盛行，并最终取代了经典的自上而下的基于规则的语言方法，而这主要得益于它们结果的优良性、速度的快捷性、以及鲁棒性。现在研究自然语言的统计方法主导了这一领域，它可以定义这个领域。\n\n\n现如今，数据驱动的自然语言处理方法受到了广泛的欢迎，以至于被认定是计算语言学的主流方法。导致这一发展的一个强有力的因素无疑是可用的电子存储数据的增加量，从而为这些处理方法的应用提供了充足的数据量。由于其观察到的脆性，另一个因素可能是在看到现存方法的脆弱性之后对过分依赖手工制动规则的觉醒。\n\n\n——2005年《牛津计算语言学手册》，第358页\n\n\n自然语言的统计方法不仅限于统计本身，而且还包括用于应用机器学习中的高级推理方法。\n\n\n理解自然语言并不是一件简单的事情，这需要大量的关于形态学、语法、语义和语用学知识，以及对世界的普遍认识。获取和对所有这些知识进行编码是开发具有良好有效性和鲁棒性的语言系统的根本障碍之一。就像统计方法一样，机器学习方法并没有做到这一点，即从带有注释或未注释的语言语料库中自动获取这种知识。\n\n\n——2005年《牛津计算语言学手册》，第377页\n\n\n统计自然语言处理\n\n\n计算语言学也被称为自然语言处理或NLP，以反映统计方法的更为基于工程师或经验的方法性一面。\n\n\n该领域的统计优势还常常导致NLP被描述为统计自然语言处理，也许是为了将其与经典计算语言学方法区别开来。\n\n\n我认为计算语言学既具有科学的一面又具有工程学的一面。称为工程学的这一面，通常称为自然语言处理（NLP），主要涉及构建计算工具，以便使用语言做有用的事情，例如机器翻译、总结、问答等。与任何工程学科一样，自然语言处理也涵盖了各种不同的科学学科。\n\n\n——2009年《统计变革是如何改变（计算）语言学的》\n\n\n\n\n语言学是一个很大的研究课题，虽然NLP的统计学方法在某些领域取得了巨大的成功，但从传统的自上而下的方法来看，仍然有很大的空间和巨大的收益。\n\n\n粗略地说，统计NLP将概率与在分析话语或文本过程中遇到的替代方案相关联，并将最可能的结果接受为正确的结果。 ......毫不奇怪的是，词语的名称现象在世界上都是密切相关的，或者我们对它的认知，在关于世界的事实反映在文本的一些模糊事实上，经常彼此接近。这个观点有很大的争论空间。\n——2005年《牛津计算语言学手册》，第19页\n自然语言处理\n作为对处理文本数据感兴趣的机器学习从业者，我们关注自然语言处理领域中的工具和方法。\n在前面的内容中，我们已经看到了从语言学到NLP的路径。现在，我们来看看现代研究人员和从业人员如何定义NLP的所有内容。\n在这一领域最顶尖研究人员撰写的教科书中，他们将这个学科称为“语言科学”，允许讨论古典语言学和现代统计学方法。\n\n\n语言科学的目的是能够描述和解释围绕在我们周围的大量语言观察，在对话、写作和其他媒体中。其中一部分与人类获取、产生和理解语言的认知范围有关，一部分与理解语言话语与世界的关系有关，一部分与了解用哪种语言沟通的语言结构有关。\n——1999年《统计自然语言处理基础》，第3页\n他们通过在自然语言处理中使用统计方法继续关注推理过程。\n\n\n统计NLP旨在对自然语言领域进行统计推理。统计推理通常包括采取一些数据（根据一些未知概率分布生成），然后对该分布进行一些推断。\n—— 1999年《统计自然语言处理基础》，第191页\n在应用自然语言处理的文本中，作者、NLP的知名NLPK Python库的贡献者将其广泛描述为使用计算机来处理自然语言数据。\n\n\n我们将采用自然语言处理（简称NLP），涵盖了对自然语言任何类型的计算机操作。一方面，它可以简单地计算单词频率来比较不同的写作风格。另一方面，NLP涉及“理解”完整的人类言语，至少在能够给予有效回应的程度上。\n——2009《用Python进行自然语言处理》，第9页\n统计NLP已经转向另一个角度，现在强调使用深度学习神经网络来对特定任务进行推理，并开发强大的端对端系统。\n在第一本专门针对这一新兴主题的教科书中，Yoav Goldberg简洁地将NLP定义为将自然语言作为输入或生成自然语言作为输出的自动方法。\n自然语言处理（NLP）是指人类语言的自动计算处理的总称。这包括将人类生成的文本作为输入的算法，以及生成自然文本作为输出的算法。\n\n\n——2017年《自然语言处理中的神经网络方法》，第17页\n进一步阅读\n如果你想更深入了解，本部分将提供有关该主题的更多资源。\n图书：\n《数学语言学》，2010，http://amzn.to/2tO1cOO\n\n\n《自然语言处理中的神经网络方法》，2017，http://amzn.to/2u0JtPl\n\n\n《计算语言学：导论》，1986，http://amzn.to/2h6U4qY\n\n\n《牛津计算语言学手册》，2005年，http://amzn.to/2uHeERE\n\n\n《统计自然语言处理基础》，1999，http://amzn.to/2uzwxDE\n\n\n《用Python进行自然语言处理》，2009，http://amzn.to/2uZMF27\n维基百科：\n维基百科上的语言学，https://en.wikipedia.org/wiki/Linguistics\n\n\n维基百科上的计算语言学，https://en.wikipedia.org/wiki/Computational_linguistics\n\n\n维基百科上的自然语言处理https://en.wikipedia.org/wiki/Natural_language_processing\n\n\n维基百科上的自然语言处理史https://en.wikipedia.org/wiki/History_of_natural_language_processing\n\n\n维基百科上的自然语言处理概要https://en.wikipedia.org/wiki/Outline_of_natural_language_processing","data":"2017年09月23日 00:00:00"}
{"_id":{"$oid":"5d343b2662f717dc0659b373"},"title":"Python自然语言处理-Day1 初识自然语言处理","author":"D0m1no","content":"Day1 初识自然语言处理\n在哲学层面上，构建智能机器人是人工智能长久以来的挑战，语言理解是智能行为的重要组成部分。这一目标多年来一直是被认为是太困难了。然而，随着NLP技术日趋成熟，用稳定的方法来分析非结构化文本越来越广泛，对自然语言理解的期望变成一个合理的目标再次浮现。\n今天介绍一些语言理解技术。\n词意消歧\n在词意消歧中，要分析出特定的上下文中的词被赋予的是哪个意思。思考存在歧义的词serve和dish。\na. serve: help with food or drink; hold an office; put ball into play\nb. dish: plate; course of a meal; communications devices\n在包含短话he served the dish 的句子中，你知道serve 和dish用的都是它们与食物相关的含义。仅仅三个词，讨论的话题，不太可能从体育转向陶器。这也许会迫使你眼前产生一幅怪异的画面:一个职业网球手，正把他的郁闷发泄到放在网球场边的陶瓷茶具上 。换句话说，自动消除歧义，需要使用上下文，利用相邻词汇的相近含义。\n指代消解\n更深刻的语言理解是解决“谁对谁做了什么” ，即检测动词的主语和宾语，虽然你也在小学已经学会了这些，但它比你想象的更难。在句子，the thieves stole the paintings 中，很容易分辨出谁做了偷窃的行为。考虑下面句子中的3种可能，尝试确定是什么被出售、被抓和被发现（其中一种情况是有歧义的）。\na. The thieves stole the paintings. They were subsequently sold.\nb. The thieves stole the paintings. They were subsequently caught.\nc. The thieves stole the paintings. They were subsequently found.\n要回答这个问题，涉及到寻找代词 they 的先行词或者paintings。处理这个问题的计算技术包括指代消解——确定代词或名词短语指的是什么——和语义角色标注——确定名词短语如何与动词相关联（如代理、受事、工具等）。\n自动生成语言\n如果能够自动的解决语言理解的问题，我们就能够继续进行那些包含自动生成语言的任务，如自动问答和机器翻译。在自动问答中，一台机器应该能够回答用户关于特定文本集的问题。\na. Text:… The thieves stole the paintings. They were subsequently sold…\nb. Human: Who or what was sold?\nc. Machine: The paintings.\n机械的回答表明，它已经正确的计算出，they 是指paintings，而不是thieves。在机器翻译中，机器应该能够把文本翻译成另一种语言文字，并准确传达原文的意思。所有这些例子中，弄清楚词的含义、动作的主语及代词的先行词是确定句子含义的步骤，也是希望语言理解系统能够做到的事情。\n机器翻译\n长久以来，机器翻译（MT）都是语言理解的圣杯，人们希望能找到从根本上提供高品质且符合语言习惯的任意两种语言之间的翻译。其历史可以追溯到冷战初期，当时自动翻译带来大量的政府赞助，它也是NLP本身的起源。今天，特定语言之间实用的翻译系统已经形成，而且有些已经集成到搜索引擎中了。但是，这些系统有一些严重的缺点。\n机器翻译是困难的,一方面原因是给定的单词可能有几种不同的解释,另一方面原因是必须改变词序，才能与目标语言的语法结构保持一致。如今，遇到的难题是，从新闻和政府网站发布的两种或两种以上的语言文档中可以搜集到大量的相似文本。如果给出一个德文和英文双语的文档或者一个双语词典，就可以自动配对组成句子，这个过程叫做文本对齐。一旦有100万个或更多的句子对，就可以检测出相应的单词和短语，并建立能用来翻译新文本的模型。\n人机对话系统\n在人工智能的历史上，主要的智能测试是一种语言学测试，叫做图灵测试：一个响应用户文本输入的对话系统能否表现得如此自然，以至于我们无法区分它是人工生成的响应？相比之下，今天的商业对话系统能力是非常有限的。下面这个图，展示了一个简单的语音对话系统的流程架构。沿途的顶部从左向右，是一些语言理解组件的管道。这些图是从语音输入，经过文法分析到某种意义的重现。图的中间，从右向左是这些组件将概念转换为语音的逆向流程。这些组件构成了系统的动态方面。在图的底部是一些有代表性的静态信息：处理组件在其上运作的语言相关数据的仓库。\nNLP的局限性\n尽管在很多如RTE(Recognizing Textual Entaliment)这样的任务研究中取得了进展，但在现实世界已经部署的语言理解系统仍然不能进行常识推理，或以一般的可靠的方式描述这个世界的知识。在等待这些困难的人工智能问题得到解决的同时，接受一些在推理和知识能力上存在严重限制的智能语音系统是有必要的。因此，从一开始，自然语言处理研究的重要目标，一直是使用浅显但强大的技术，代替无边无际的知识和推理能力。，","data":"2018年08月02日 20:27:21"}
{"_id":{"$oid":"5d343b2762f717dc0659b375"},"title":"自然语言处理与应用化学专业的关系","author":"林檎花火とソーダの海","content":"自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。自然语言处理是一门融语言学、计算机科学、数学于一体的科学。因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，所以它与语言学的研究有着密切的联系，但又有重要的区别。自然语言处理并不是一般地研究自然语言，而在于研制能有效地实现自然语言通信的计算机系统，特别是其中的软件系统。因而它是计算机科学的一部分。\n自然语言处理（NLP）是计算机科学，人工智能，语言学关注计算机和人类（自然）语言之间的相互作用的领域。\n自然语言处理是一门沟通的科学，但不是人与人之间的沟通，而是人与计算机之间的沟通，而且不是传统计算的输入数据，算式得到结果，而是输入条件，库，由关系分析得到结果，它最早来自机器翻译，最早的库是很小的，像词典那样，由A国词汇与B国词汇组成有一对一，一对多，多对一等联系，输入A国文字得到一个或多个检索条目，它的能力随着库的完善，条目联系的增多而增强，这是与传统的数学计算里给定的算式，输入一定数字，得到一个结果完全不一样的。\n自然语言处理是非常有开创性的，它为人与计算机之间的交流开发了一条新道路，这种思路为现在的大数据的发展打下了基础，虽然在当时无法实现对极大数据的处理，库也被限制在字典词条范围之内，但在对它的研究，在它的发展中人们认识到了，模糊输入，歧义，不规范输入造成的问题，它们是长期困扰人们的问题也使人们在解决了这些“独特事件”之后对计算科学有了进一步的推动。\n我的专业是应用化学专业，这是一门讲究实践的实验科学，但它与自然语言处理存在者深刻的联系。应用化学是一个理学分支，所有理学都建立于数学之上，而数学则是自然语言处理的基础，在新时代里，自然语言处理不局限于词典的制作，它被广泛的应用在各学科里，包括应用化学，利用它所制作的化学实验库，你能输入一个物质，就能了解到它的性质，常见的实验反应集群；输入一个反应，你能得到反应内所有物质的性质，所有同类反应，大量相似反应；你也可以只凭某少数特征来检索拥有这些特征的所有物质，通过某些现象检索到某类型反应，它的库来自大量化学实验室真实实验的积累，结果可靠而且丰富，可以说是关于化学应用实验的大百科。\n自然语言处理是一门计算学科，但更是一种求知的思想，直到现在我们都还在为它的发展中遇到的某些问题而头疼，但那并不阻止我们去实现它","data":"2019年06月20日 17:56:21"}
{"_id":{"$oid":"5d343b2762f717dc0659b377"},"title":"自然语言处理(python)环境配置-NLTK的安装","author":"AIAS编程有道","content":"自然语言处理(python)环境配置-NLTK的安装\n1.自然语言处理的介绍\nNLP (Natural Language Processing) 是人工智能（AI）的一个子领域。是机器真正能够理解人类说话的重要一环。自然语言处理也不是新的研究领域，早在上个世纪就开始研究，但是给予计算机环境等等因素导致这方面的发展一直停滞不前，再机器学习，统计学，计算机科学的快速发展下，NLP又迎来了新的春天，在将来的发展中也是非常重要的一环。具体介绍可以参见百度百科等（https://baike.baidu.com/item/nlp/25220）。\n2.自然语言处理语言工具\n在自然语言处理中，python也成了当仁不让的语言了，这种包的继承，有如站在巨人的肩膀上前进，但是，这仅是对当前已技术的使用，对于深层次的研究确实是需要花费功夫的。特别是在硕士研究生、博士研究生等等都是是需要真正地去思考语言的形成，这里又乔姆斯基的形式语言学说，中国的自然语言处理大家冯远炜教授的著作都是我们值得去思考和借鉴的，在结合当前的统计学，机器学习，计算机科学的发展，自然语言处理在python这种好用的编程工具的基础上会发展的更好。\n3.自然语言处理的第一步\n当然，博主是想从事这方面研究的小白，才刚刚起步，希望这是一个记录自己成长的平台，也希望把自己知道的，学习中遇到的问题分享出来。这是开始学习，使用的是比较出名的nltk包，当然对于汉字的分词处理等，据博主知道的还有jieba分词等。\n环境准备：\nSystem：window 10\nIDE：anaconda-spyder\n环境配置：似乎anaconda中已经把nltk集成了，当然自己也可以在命令行中输入：pip install nltk(前提是读者已经把环境都配好了)；之后就是打开IDE创建一个py文件\nimport nltk nltk.download()\n即可下载nltk的语料等资源\n如图：\n\n由于国内访问比较慢，所以需要下载的，博主已经下载好了：链接: https://pan.baidu.com/s/1WbNb-h9U8VKYQXSYZonbvQ 密码: dq4s\n更多信息也可查看官网：http://www.nltk.org/","data":"2018年04月08日 00:20:15"}
{"_id":{"$oid":"5d343b2862f717dc0659b37a"},"title":"自然语言处理项目之文档主题分类","author":"湾区人工智能","content":"#希拉里右键门，文档主题分类。LDA模型，数据读取还有点问题 #数据来源:请联系公众号：湾区人工智能 import numpy as np import pandas as pd import re import codecs #UnicodeEncodeError: 'mbcs' codec can't encode characters in position 0--1: invalid character df = pd.read_csv(\"D:/自然语言处理/Lecture_3 LDA 主题模型课件与资料/Lecture_3 LDA 主题模型课件与资料/主题模型课件与资料/input/HillaryEmails.csv\",encoding='utf-8') # 原邮件数据中有很多Nan的值，直接扔了。 df = df[['Id','ExtractedBodyText']].dropna() def clean_email_text(text): text = text.replace('\\n',\" \") #新行，我们是不需要的 text = re.sub(r\"-\", \" \", text) #把 \"-\" 的两个单词，分开。（比如：july-edu ==\u003e july edu） text = re.sub(r\"\\d+/\\d+/\\d+\", \"\", text) #日期，对主体模型没什么意义 text = re.sub(r\"[0-2]?[0-9]:[0-6][0-9]\", \"\", text) #时间，没意义 text = re.sub(r\"[\\w]+@[\\.\\w]+\", \"\", text) #邮件地址，没意义 text = re.sub(r\"/[a-zA-Z]*[:\\//\\]*[A-Za-z0-9\\-_]+\\.+[A-Za-z0-9\\.\\/%\u0026=\\?\\-_]+/i\", \"\", text) #网址，没意义 pure_text = '' # 以防还有其他特殊字符（数字）等等，我们直接把他们loop一遍，过滤掉 for letter in text: # 只留下字母和空格 if letter.isalpha() or letter==' ': pure_text += letter # 再把那些去除特殊字符后落单的单词，直接排除。 # 我们就只剩下有意义的单词了。 text = ' '.join(word for word in pure_text.split() if len(word)\u003e1) return text #新建一个colum docs = df['ExtractedBodyText'] docs = docs.apply(lambda s: clean_email_text(s)) #docs.head(1).values doclist = docs.values from gensim import corpora, models, similarities import gensim stoplist = ['very', 'ourselves', 'am', 'doesn', 'through', 'me', 'against', 'up', 'just', 'her', 'ours', 'couldn', 'because', 'is', 'isn', 'it', 'only', 'in', 'such', 'too', 'mustn', 'under', 'their', 'if', 'to', 'my', 'himself', 'after', 'why', 'while', 'can', 'each', 'itself', 'his', 'all', 'once', 'herself', 'more', 'our', 'they', 'hasn', 'on', 'ma', 'them', 'its', 'where', 'did', 'll', 'you', 'didn', 'nor', 'as', 'now', 'before', 'those', 'yours', 'from', 'who', 'was', 'm', 'been', 'will', 'into', 'same', 'how', 'some', 'of', 'out', 'with', 's', 'being', 't', 'mightn', 'she', 'again', 'be', 'by', 'shan', 'have', 'yourselves', 'needn', 'and', 'are', 'o', 'these', 'further', 'most', 'yourself', 'having', 'aren', 'here', 'he', 'were', 'but', 'this', 'myself', 'own', 'we', 'so', 'i', 'does', 'both', 'when', 'between', 'd', 'had', 'the', 'y', 'has', 'down', 'off', 'than', 'haven', 'whom', 'wouldn', 'should', 've', 'over', 'themselves', 'few', 'then', 'hadn', 'what', 'until', 'won', 'no', 'about', 'any', 'that', 'for', 'shouldn', 'don', 'do', 'there', 'doing', 'an', 'or', 'ain', 'hers', 'wasn', 'weren', 'above', 'a', 'at', 'your', 'theirs', 'below', 'other', 'not', 're', 'him', 'during', 'which'] texts = [[word for word in doc.lower().split() if word not in stoplist] for doc in doclist] #用词袋的方法，把每个单词用一个数字index指代，并把我们的原文本变成一条长长的数组： dictionary = corpora.Dictionary(texts) corpus = [dictionary.doc2bow(text) for text in texts] #建立模型 lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=20) #第10号分类，其中最常出现的单词是 #lda.print_topic(10, topn=5) #所有的主题打印出来看看 lda.print_topics(num_topics=20, num_words=5) ''' #可以把新鲜的文本/单词，分类成20个主题中的一个。文本和单词，都必须得经过同样步骤的文本预处理+词袋化，也就是说，变成数字表示每个单词的形式。 lda.get_document_topics(bow) '''","data":"2018年06月11日 11:21:07"}
{"_id":{"$oid":"5d343b2862f717dc0659b37c"},"title":"教你用Python进行自然语言处理（附代码）","author":"数据派THU","content":"原文题目：NLP in Python\n翻译： 陈之炎\n校对： 和中华\n本文共2700字，建议阅读6分钟。\n\n自然语言处理是数据科学中的一大难题。在这篇文章中，我们会介绍一个工业级的python库。\n\n\n自然语言处理（NLP）是数据科学中最有趣的子领域之一，越来越多的数据科学家希望能够开发出涉及非结构化文本数据的解决方案。尽管如此，许多应用数据科学家（均具有STEM和社会科学背景）依然缺乏NLP（自然语言处理）经验。\n\n\n在这篇文章中，我将探讨一些基本的NLP概念，并展示如何使用日益流行的Python spaCy包来实现这些概念。这篇文章适合NLP初学者阅读，但前提是假设读者具备Python的知识。\n\n\n你是在说spaCy吗？\n\n\nspaCy是一个相对较新的包，“工业级的Python自然语言工具包”，由Matt Honnibal在Explosion AI.开发。它在设计时目标用户以应用数据科学家为主，这也意味着它不需要用户来决定使用哪个算法来处理常见任务，而且它非常地快—快得难以置信（它用Cython来实现）。如果你熟悉Python数据科学栈，spaCy就是NLP的numpy，它虽然理所当然地位于底层，但是却很直观，性能也相当地高。\n\n\n那么，它能做什么呢？\n\n\nspaCy为任何NLP项目中常用的任务提供一站式服务.包括：\n\n\n符号化(Tokenizatioin)\n词干提取(Lemmatization)\n词性标注(Part-of-speech tagging)\n实体识别(Entity recognition)\n依存句法分析(Dependency parsing)\n句子的识别(Sentence recognition)\n字-向量变换(Word-to-vector transformation)\n许多方便的清除文本和标准化文本的方法(cleaning and normalizing text)\n\n\n我会对这些功能做一个高层次的概述，并说明如何利用spaCy访问它们。\n那我们就开始吧。\n\n\n首先，我们加载spaCy的管线，按照约定，它存储在一个名为nlp的变量中。需要花几秒钟时间声明该变量，因为spaCy预先将模型和数据加载到前端，以节省时间。实际上，这样做可以提前完成一些繁重的工作，使得nlp解析数据时开销不至于过大。 请注意，在这里，我们使用的语言模型是英语，同时也有一个功能齐全的德语模型，在多种语言中均可实现标记化（将在下面讨论）。\n\n\n我们在示例文本中调用NLP来创建Doc对象。Doc 对象是文本本身NLP任务容器，将文本切分成文字(Span 对象)和元素(Token 对象)，这些对象实际上不包含数据。值得注意的是Token 和 Span对象实际上没有数据。相反，它们包含Doc对象中的数据的指针，并且被惰性求值（即根据请求）。绝大多数spaCy的核心功能是通过对Doc (n=33), Span (n=29),和 Token (n=78)对象的方法来实现的。\n\n\nIn[1]:import spacy\n...: nlp = spacy.load(\"en\")\n...: doc = nlp(\"The big grey dog ate all of the chocolate, but fortunately he wasn't sick!\")\n\n\n分词(tokenization)\n\n\n分词是许多自然语言处理任务中的一个基本步骤。分词就是将一段文本拆分为单词、符号、标点符号、空格和其他元素的过程，从而创建token。这样做的一个简单方法是在空格上拆分字符串：\n\n\nIn[2]:doc.text.split()\n...: Out[2]: ['The', 'big', 'grey', 'dog', 'ate', 'all', 'of', 'the', 'chocolate,', 'but', 'fortunately', 'he', \"wasn't\", 'sick!']\n\n\n从表面上，直接以空格进行分词效果还不错。但是请注意， 它忽略了标点符号，且没有将动词和副词分开(\"was\", \"n't\")。换句话说，它太天真了，它无法识别出帮助我们（和机器）理解其结构和含义的文本元素。让我们来看看spaCy如何处理这个问题：\n\n\nIn[3]:[token.orth_ for token in doc]\n...:\nOut[3]: ['The', 'big', 'grey', 'dog', 'ate', 'all', 'of', 'the', 'chocolate', ',', 'but', 'fortunately', 'he', 'was', \"n't\", ' ', 'sick', '!']\n\n\n这里，我们访问的每个token的.orth_方法，它返回一个代表token的字符串，而不是一个SpaCytoken对象。这可能并不总是可取的，但值得注意。SpaCy能够识别标点符号，并能够将这些标点符号与单词的token分开。许多SpaCy的token方法为待处理的文字同时提供了字符串和整数的返回值：带有下划线后缀的方法返回字符串而没有下划线后缀的方法返回的是整数。例如:\n\n\nIn[4]:[(token, token.orth_, token.orth) for token in doc]\n...:\nOut[4]:[(The, 'The', 517), (big, 'big', 742), (grey, 'grey', 4623), (dog, 'dog', 1175), (ate, 'ate', 3469), (all, 'all', 516), (of, 'of', 471), (the, 'the', 466), (chocolate, 'chocolate', 3593), (,, ',', 416), (but, 'but', 494), (fortunately, 'fortunately', 15520), (he, 'he', 514), (was, 'was', 491), (n't, \"n't\", 479), ( , ' ', 483), (sick, 'sick', 1698), (!, '!', 495)]\nIn[5]: [token.orth_ for token in doc if not token.is_punct | token.is_space]\n...:\nOut[5]: ['The', 'big', 'grey', 'dog', 'ate', 'all', 'of', 'the', 'chocolate', 'but', 'fortunately', 'he', 'was', \"n't\", 'sick']\n\n\n很酷，对吧？\n\n\n词干提取\n\n\n和分词相关的任务是词干提取。词干提取是将一个单词还原成它的基本形式--母词的过程。不同用法的单词往往具有相同意义的词根。例如，practice（练习）, practiced（熟练的）,和 practising（实习）这三个单词实质上指的是同一件事情。通常需要将相似意义的单词进行标准化，标准化到其基本的形式。使用SpaCy，我们利用标记的.lemma_ 方法访问到每个单词的基本形式。\n\n\nIn[6]:practice = \"practice practiced practicing\"\n...: nlp_practice = nlp(practice)\n...: [word.lemma_ for word in nlp_practice]\n...:\nOut[6]: ['practice', 'practice', 'practice']\n\n\n为什么这个会有用？一个即时用例便是机器学习，特别是文本分类。例如：在创建“单词袋”之前需对文本进行词干提取，避免了单词的重复，因此，该模型可以更清晰地描述跨多个文档的单词使用模式。\n\n\n词性标注(POS Tagging)\n\n\n词性标注是将语法属性（如名词、动词、副词、形容词等）赋值给词的过程。共享相同词性标记的单词往往遵循类似的句法结构，在基于规则的处理过程中非常有用。\n\n\n例如，在给定的事件描述中，我们可能希望确定谁拥有什么。通过利用所有格，我们可以做到这一点（提供文本的语法）。SpaCy采用流行的Penn Treebank POS标记（参见这里)。利用SpaCy，可以分别使用.pos_ 和 .tag_方法访问粗粒度POS标记和细粒度POS标记。在这里，我访问细粒度的POS标记：\n\n\nIn[7]:doc2 = nlp(\"Conor's dog's toy was hidden under the man's sofa in the woman's house\")\n...: pos_tags = [(i, i.tag_) fori indoc2]\n...: pos_tags\n...:\nOut[7]:\n[(Conor,'NNP'),\n('s, 'POS'),\n(dog,'NN'),\n('s, 'POS'),\n(toy,'NN'),\n(was,'VBD'),\n(hidden,'VBN'),\n(under,'IN'),\n(the,'DT'),\n(man,'NN'),\n('s, 'POS'),\n(sofa,'NN'),\n(in,'IN'),\n(the,'DT'),\n(woman,'NN'),\n('s, 'POS'),\n(house,'NN')]\n\n\n我们可以看到，'s 的标签被标记为 POS.我们可以利用这个标记提取所有者和他们拥有的东西：\n\n\nIn[8]:owners_possessions = []\n...: for i in pos_tags: ...: if i[1] == \"POS\":\n...: owner = i[0].nbor(-1)\n...: possession = i[0].nbor(1)\n...: owners_possessions.append((owner, possession))\n...:\n...: owners_possessions\n...:\nOut[8]: [(Conor, dog), (dog, toy), (man, sofa), (woman, house)]\n\n\n\n\n这将返回所有者拥有元组的列表。如果你想在这件事上表现成为超级Python能手的话，你可以把它写成一个完整的列表（我认为这是最好的！）：\n\n\nIn[9]: [(i[0].nbor(-1), i[0].nbor(+1)) for i in pos_tags if i[1] == \"POS\"]\n...: Out[9]: [(Conor, dog), (dog, toy), (man, sofa), (woman, house)]\n\n\n在这里，我们使用的是每个标记的.nbor 方法，它返回一个和这个标记相邻的标记。\n\n\n实体识别\n\n\n实体识别是将文本中的指定实体分类为预先定义的类别的过程，如个人、地点、组织、日期等。spaCy使用统计模型对各种模型进行分类，包括个人、事件、艺术作品和国籍/宗教(参见完整列表文件）)\n\n\n例如，让我们从贝拉克·奥巴马的维基百科条目中选出前两句话。我们将解析此文本，然后使用Doc 对象的 .ents方法访问标识的实体。通过调用Doc 的这个方法，我们可以访问其他的标记方法 ，特别是 .label_ 和 .label两个方法:\n\n\nIn[10]:wiki_obama = \"\"\"Barack Obama is an American politician who served as\n...: the 44th President of the United States from 2009 to 2017.He is the first\n...: African American to have served as president,\n...: as well as the first born outside the contiguous United States.\"\"\"\n…：\n…：nlp_obama = NLP（wiki_obama）\n…：[(i, i.label_, i.label) for i in nlp_obama.ents]\n...:\n\n\nOut[10]: [(Barack Obama, 'PERSON', 346), (American, 'NORP', 347), (the United States, 'GPE', 350), (2009 to 2017, 'DATE', 356), (first, 'ORDINAL', 361), (African, 'NORP', 347), (American, 'NORP', 347), (first, 'ORDINAL', 361), (United States, 'GPE', 350)]\n\n\n您可以看到在本例中，模型所识别的实体以及它们的精确程度。PERSON 是不言自明的；NORP是国籍或宗教团体；GGPE标识位置（城市、国家等等）；DATE 标识特定的日期或日期范围， ORDINAL标识一个表示某种类型的顺序的单词或数字。\n\n\n在我们讨论Doc方法的主题时，值得一提的是spaCy的句子标识符。NLP任务希望将文档拆分成句子的情况并不少见。利用SpaCy访问Doc's.sents 方法并不难做到：\n\n\nIn[11]:for ix, sent in enumerate(nlp_obama.sents, 1):\n...: print(\"Sentence number {}: {}\".format(ix, sent))\n...: Sentence number 1: Barack Obama is an American politician who served as the 44th President of the United States from 2009 to 2017.Sentence number 2: He is the first African American to have served as president, as well as the first born outside the contiguous United States.\n\n\n目前就是这样。在以后的文章中，我将展示如何在复杂的数据挖掘和ML的任务中使用spaCy。\nTrueSight是一个AIOps平台,由机器学习和分析提供动力支持，它解决了多个云的复杂性，并且提高了数字转化的速度，从而提升了IT运­作的效率。\n\n\n原文链接：https://dzone.com/articles/nlp-in-python\n\n\n译者简介\n陈炎之，北京交通大学通信与控制工程专业毕业，获得工学硕士学位，历任长城计算机软件与系统公司工程师，大唐微电子公司工程师，现任北京吾译超群科技有限公司技术支持。目前从事智能化翻译教学系统的运营和维护，在人工智能深度学习和自然语言处理（NLP）方面积累有一定的经验。业余时间喜爱翻译创作，翻译作品主要有：IEC-ISO 7816、伊拉克石油工程项目、新财税主义宣言等等，其中中译英作品“新财税主义宣言”在GLOBAL TIMES正式发表。能够利用业余时间加入到THU 数据派平台的翻译志愿者小组，希望能和大家一起交流分享，共同进步。\n翻译组招募信息\n工作内容：需要一颗细致的心，将选取好的外文文章翻译成流畅的中文。如果你是数据科学/统计学/计算机类的留学生，或在海外从事相关工作，或对自己外语水平有信心的朋友欢迎加入翻译小组。\n\n你能得到：定期的翻译培训提高志愿者的翻译水平，提高对于数据科学前沿的认知，海外的朋友可以和国内技术应用发展保持联系，THU数据派产学研的背景为志愿者带来好的发展机遇。\n其他福利：来自于名企的数据科学工作者，北大清华以及海外等名校学生他们都将成为你在翻译小组的伙伴。\n\n\n点击文末“阅读原文”加入数据派团队~\n点击“阅读原文”拥抱组织","data":"2018年03月28日 00:00:00"}
{"_id":{"$oid":"5d343b2962f717dc0659b37e"},"title":"自然语言处理(一)NLP概述","author":"JN_rainbow","content":"NLP概述\nNLP是利用计算机为工具，对人类特有的书面形式和口头形式的自然语言的信息进行各种类型处理和加工的技术.\nNLP内容结构\nNLP基础技术\n词法分析\n词法分析目的是从句子中分出单词，找出词汇的各个词素，从中获得单词的语言学信息并确定单词的词性. 词法分析是很多中文信息处理任务的必要步骤.\n自动分词\n命名实体识别\n词性标注\n句法分析\n句法分析是对句子和短语结构进行分析，如句子的形式结构：主语、谓语、宾语等. 句法分析是语言学理论和实际的自然语言应用的一个重要桥梁. 一个实用的、完备的、准确的句法分析将是计算机真正理解自然语言的基础.\n短语结构分析(宾州树库)\n依存分析\n语义分析\n解释自然语言句子或篇章各部分(词、词组、句子、段落、篇章)的意义. 目前语义计算的理论、方法、模型尚不成熟.\n词义消歧(词)\n语义归纳、推理(词)\n语义角色标注(句子)\n篇章分析\n指超越单个句子范围的各种可能分析，包括句子（语段）之间的关系以及关系类型的划分，段落之间的关系的判断，跨越单个句子的词与词之间的关系分析，话题的继承与变迁等.\nNLP核心应用\n机器翻译(Machine translation, MT)\n信息检索(Information Retrieval)\n信息抽取(Information Extraction)\n自动文摘(Automatic summarization/abstracting)\n问答系统(Question-Answering system)\n阅读理解(Machine Reading)\n文档分类(Document categorization)\n情感分类(Sentimental classification)\n信息推荐与过滤(Formation Recommendation and Filtering)\nNLP技术及应用架构\nNLP领域的学术会议\nACL（Association of Computational Linguistics）\nColing（International Conference on Computational Linguistics）\nEMNLP（Conference on Empirical Methods in Natural language Processing）\nEACL(European Chapter of ACL)\nIJCNLP(International Joint Conference on Natural language Processing)\nSIGIR(SIG Information Retrieval)\nTREC(Text REtrievalConference)\nJSCL(全国计算语言学联合学术会议)\n国内NLP研究组\nTencent AI Lab\n苏州大学NLP实验室\n微软亚洲研究院自然语言计算组NaturalLanguageComputing(NLC)Group\n头条人工智能实验室\n清华大学自然语言处理与社会人文计算实验室\n清华大学智能技术与系统国家重点实验室信息检索组\n北京大学计算语言学教育部重点实验室\n北京大学计算机科学技术研究所语言计算与互联网挖掘研究室\n哈工大社会计算与信息检索研究中心\n哈工大机器智能与翻译研究室\n哈尔滨工业大学智能技术与自然语言处理实验室\n中科院计算所自然语言处理研究组\n中科院自动化研究所语音语言技术研究组\n南京大学自然语言处理研究组\n复旦大学自然语言处理研究组\n东北大学自然语言处理实验室\n厦门大学智能科学与技术系自然语言处理实验室\n参考资料\n中国科学院大学-NLP课程课件(IIE胡玥老师主讲)","data":"2018年11月27日 22:21:10"}
{"_id":{"$oid":"5d343b2962f717dc0659b380"},"title":"人工智能 1.概论","author":"lagoon_lala","content":"参考：http://abook.hep.com.cn/1865081\n智能的概念：知识+智力（获取并应用知识求解问题）\n智能的特征：感知、记忆、思维（处理信息）、学习、行为\n思维：逻辑、形象、顿悟（灵感）\n基本内容：知识表示、机器感知、机器思维、机器学习、机器行为\n研究领域：自动定理证明、博弈、模式\n识别\n、机器视觉、自然语言理解、智能信息检索、数据挖掘与知识发现（找出有意义的）、专家系统、自动程序设计、机器人、组合优化、人工神经网络、分布式人工智能与多智能体、智能控制（广义问题决策与规划）、智能仿真（建模、实验、结果分析）、智能CAD（设计）、智能CAI（教学）、智能管理决策、智能多媒体（综合处理文字图形）、智能操作系统（自动管理维护）、智能计算机系统、智能通信、智能网络（计网）、人工生命（模拟）","data":"2018年11月16日 16:26:52"}
{"_id":{"$oid":"5d343b2962f717dc0659b382"},"title":"人工智能时代下的数据安全治理","author":"shipinginfo","content":"上周，小编 惊喜地 收到网络安全十余年老兵——飞絮老师的投稿。大佬终于重出江湖了……\n一、人 工 智 能\n人工智能是一门以数学为基础，涉及到计算机科学、生物学、心理学、语言学和哲学等的交叉类学科。\n维 基 百 科\n人工智能就是机器展现出的“智能”，即只要是某种机器，具有某种或某些“智能”的特征或表现，都应该算作“人工智能”。人工智能三大核心要素：算法、算力、数据。\n根据中国电子技术标准化研究院的《人工智能标准化白皮书》（2018版）（下载点击 阅读原文）阐述，人工智能领域关键技术包括机器学习、知识图谱、自然语言处理、计算机视觉、人机交互、生物特征识别、虚拟现实/增强现实等。\n其实，人工智能已发展了半个多世纪，如今随着计算能力飞速发展及硬件成本的不断降低促使用人工智能发展到了第三波浪潮。\n2017年3月5日，“人工智能”正式写入2017政府工作报告，无人驾驶、个人助理、金融、电商、医疗、教育等各大领域大量应用了人工智能。预计2030年全球将达7万亿美元规模的市场。\n人工智能和机器学习有望彻底改变很多行业，但它们也带来了重大安全风险。\n比如“算法黑箱”或算法不透明性将引发算法安全管理困境，可能成为“隐形”恶意武器，操控决策致使算法权力诱导个人行为、影响政府决策和司法判决。\n剑桥分析助力特朗普总统竞选、携程差异化定价杀熟等一个个案例呈现在我们面前。\n除了算法、算力外，另一个核心因素是数据。实现人工智能有两个阶段，即准备数据与训练模型。数据准备工作量占比达 70% 以上，但更重要的数据背后的人工，即数据预处理、模型选择与参数调整。\n二、数据安全治理\n目前数据已成为资产、能源和基础设施的关键要素，数据安全市场呈井喷之势。\n据中商产业研究院分析，2016-2020年中国数据安全市场规模年增速30%以上，预计2020年市场规模将超70亿元。\n理解“数据安全保护” 的内涵，一般可以分为3个阶梯式层次：数据安全、个人数据保护、国家层面的数据保护。\n数据安全可以理解为信息或信息系统免受未经授权的访问、使用、披露、破坏、修改、销毁等。\n数据安全=保密性+完整性+可用性\n个人数据保护 = 数据安全 + 个人数据自决权利 + 数据控制者等相关方满足个人数据自决权利的义务\n国家层面的数据保护 = 数据安全 + 数据支配权 + 防止敏感数据遭恶意使用对国家安全的威胁\n目前数据安全面临的挑战如下：\n新的数据和隐私保护的合规要求；\n网络攻击造成的数据泄露破坏了组织声誉和客户信任；\n混合IT架构下缺乏数据安全策略；\n数据安全和身份管理产品不会整合甚至不共享通用策略。\n著名的咨询与研究机构Gartner在2018年5月发布了数据安全治理（Data Security Governance）框架，提供了一个如何通过数据保护和隐私声明的平衡方法来实现实际的安全性。\n数据安全治理不仅仅是工具或产品的解决方案，而是基于战略、业务、应用、人员的安全和风险管理的有机整体，从管理制度到工具支撑，从上层管理架构到下层技术实现，采取的一系列适合组织数据生命周期的措施。\nGartner指出了从数据的加密、监控审计、防泄露、用户身份证、用户行为等环节入手是一个错误的实践。\n数据安全治理的最佳实践是从考虑组织的经营战略与策略、面临的内外合规要求、整体的IT策略以及组织的安全风险容忍度开始，然后是对数据进行分级分类，再者是对不同级别的数据实行合理的安全手段。\n我们可以设计从“安全监控评估、安全技术加固、安全治理服务”三位一体的数据安全保障体系。\n同时，依据Gartner DSG的理念，在数据的全生命周期中，采用先从数据安全治理咨询为入口的阶梯式数据安全治理思路。详细步骤如下图所示：\n注：红色字体可大量应用人工智能技术\n三、人工智能应用于数据安全治理\n人工智能在机器学习和自然语言处理方面的应用一直受到业界的关注。\n依托人工智能引擎，通过对业务数据的获取、清洗、语义计算、数据挖掘、机器学习、知识图谱、认知计算等技术，将快速促进数据安全保障体系完善。\n应用机器学习、自然语言处理、和文本聚类分类技术，能对数据进行基于内容的实时精准分类分级，而数据的分类分级是数据安全治理的核心环节。\n数据分类引擎已成功应用在邮件内容过滤、保密文件管理、知识挖掘、情报分析、反欺诈、电子发现和归档、数据防泄露等领域。利用人工智能可实现对数据的“智”、“准”、“深”的识别、控制和价值挖掘。\n然而，人工智能需要海量的数据，人工智能技术的进步取决于各种来源数据的可用性，如何确保这些数据的安全性与保证用户数据的隐私性又是一个相生相伴的问题……\n想看到更多像飞絮老师这样的大佬们带来的干货，请长按下文二维码关注“世平信息”\n杭州世平信息有限公司（简称“世平信息”），致力于智能化数据管理与应用的深入开拓和持续创新，为用户提供数据安全、数据治理、数据共享和数据利用解决方案，帮助用户切实把握大数据价值与信息安全。\n近期热点\nBlack Hat 2018 | 会议概览+10个热门议题\n央采中标！世平数据防泄漏、数据脱敏系统C位出道\n信息安全自主可控，可信计算助飞等级保护2.0\n互利共赢 | 世平信息与盘州市区域政府开展友好交流活动\n助推数字经济，世平信息上榜2018大数据产业生态地图\n在老板曾是军人的公司工作是一种怎样的体验？\n网络安全执法检查？大数据安全整治？我该怎么办？！","data":"2018年08月14日 09:55:11"}
{"_id":{"$oid":"5d343b2a62f717dc0659b384"},"title":"R语言自然语言处理：词性标注与命名实体识别","author":"R3eE9y2OeFcU40","content":"欢迎关注天善智能，我们是专注于商业智能BI，人工智能AI，大数据分析与挖掘领域的垂直社区，学习，问答、求职一站式搞定！\n对商业智能BI、大数据分析挖掘、机器学习，python，R等数据领域感兴趣的同学加微信：tstoutiao，邀请你进入数据爱好者交流群，数据爱好者们都在这儿。\n作者：黄天元，复旦大学博士在读，目前研究涉及文本挖掘、社交网络分析和机器学习等。希望与大家分享学习经验，推广并加深R语言在业界的应用。\n邮箱：huang.tian-yuan@qq.com\n\n\n原理简介\n在之前的文章中（\nR语言自然语言处理：中文分词\n）介绍了如何利用jiebaR来做中文分词，这次希望研究如果利用R语言来做词性标注，并利用标注来做命名实体识别。 首先需要明确词性标注的概念，就是要把中文分词后的每一个词，确定其性质。是名词？动词？还是形容词？如果是名词，是人名、地名还是机构团体名称？对这些词性进行更为细致的标注，有助于我们对信息进行提取（有的时候动词和形容词其实不包含我们感兴趣的信息，但是名词却非常重要）。此外，也有利于我们了解作者的用词习惯（这个时候，名词又不一定重要了，一个人的行文习惯可以体现在他经常用的动词和形容词）。 因为我们是用jiebaR来做分词，根据官方文档说明，它的标注是根据北大《人民日报》语料库进行训练的，最后的标准整理为ICTPOS3.0词性标记集，内容如下：\nn 名词\nnr 人名\nnr1 汉语姓氏\nnr2 汉语名字\nnrj 日语人名\nnrf 音译人名\nns 地名\nnsf 音译地名\nnt 机构团体名\nnz 其它专名\nnl 名词性惯用语\nng 名词性语素\n\nt 时间词\ntg 时间词性语素\n\ns 处所词\n\nf 方位词\n\nv 动词\nvd 副动词\nvn 名动词\nvshi 动词“是”\nvyou 动词“有”\nvf 趋向动词\nvx 形式动词\nvi 不及物动词（内动词）\nvl 动词性惯用语\nvg 动词性语素\na 形容词\nad 副形词\nan 名形词\nag 形容词性语素\nal 形容词性惯用语\nb 区别词\nbl 区别词性惯用语\nz 状态词\nr 代词\nrr 人称代词\nrz 指示代词\nrzt 时间指示代词\nrzs 处所指示代词\nrzv 谓词性指示代词\nry 疑问代词\nryt 时间疑问代词\nrys 处所疑问代词\nryv 谓词性疑问代词\nrg 代词性语素\nm 数词\nmq 数量词\nq 量词\nqv 动量词\nqt 时量词\n\n\n\n词性标注实践\n话不多说，我们上代码来做词性标注分析。需要注意的是，我们要做词性标注的输入，既可以是一大段没有经过分词处理字符串，也可以是已经分词完毕的分词结果（也就是字符向量）。我们先介绍第一种情况，就是没有经过分词的大段字符串，要完成分词，然后对每个词都进行词性标注。\n1library(pacman)\n2p_load(jiebaR,tidyverse)\n3\n4cn = \"我想写一本书，名字叫做《R语言高效数据处理》。\"   #构造中文文本\n5tag_worker = worker(type = \"tag\")    #构造分词标注器\n6\n7tag_result = tagging(cn,tag_worker)   #进行分词标注\n8\n9tag_result            #查看结果\n10##          r          v          v          m          r          n\n11##       \"我\"       \"想\"       \"写\"       \"一\"     \"本书\"     \"名字\"\n12##          v        eng          a          n\n13##     \"叫做\"    \"R语言\"     \"高效\" \"数据处理\"\n\n我们得到的tag_result实质上是一个带属性的向量，这样其实不是特别好用。因此我要把它变成数据框的格式，方便以后利用。\n1str(tag_result)  #查看数据类型\n2##  Named chr [1:10] \"我\" \"想\" \"写\" \"一\" \"本书\" \"名字\" \"叫做\" \"R语言\" ...\n3##  - attr(*, \"names\")= chr [1:10] \"r\" \"v\" \"v\" \"m\" ...\n4enframe(tag_result) -\u003e tag_table  #转换数据存储格式\n5\n6tag_table\n7## # A tibble: 10 x 2\n8##    name  value\n9##    \u003cchr\u003e \u003cchr\u003e\n10##  1 r     我\n11##  2 v     想\n12##  3 v     写\n13##  4 m     一\n14##  5 r     本书\n15##  6 n     名字\n16##  7 v     叫做\n17##  8 eng   R语言\n18##  9 a     高效\n19## 10 n     数据处理\n\n其实这里分词效果还不是那么尽如人意，因为“本书”应该分为“本”、“书”，而这里被认定为代词，指代之前提过的一本书（然而我并没有指代任何词）。不过大体来说还算满意。注意“R语言”之所以能够被分出来，是因为我上次处理加了用户词库，因此这次自动地进行了识别。如果大家没有把“R语言”加入到用户自定义词库中，你们看到的应该是“R”、“语言”。关于如何定义用户词库，见上一篇文章R语言自然语言处理：\n中文分词\n。 如果已经分词完毕，需要对这些词进行词性标注，可以使用vector_tag函数。我们先按照正常流程进行分词：\n1#正常分词流程\n2\n3worker() -\u003e wk\n4segment(cn,wk) -\u003e seg_cn\n5\n6seg_cn\n7##  [1] \"我\"       \"想\"       \"写\"       \"一\"       \"本书\"     \"名字\"\n8##  [7] \"叫做\"     \"R语言\"    \"高效\"     \"数据处理\"\n\n然后我们利用函数进行标注。\n1vector_tag(seg_cn,tag_worker)\n2##          r          v          v          m          r          n\n3##       \"我\"       \"想\"       \"写\"       \"一\"     \"本书\"     \"名字\"\n4##          v        eng          a          n\n5##     \"叫做\"    \"R语言\"     \"高效\" \"数据处理\"\n\n这个结构与我们上面得到的tag_result是一致的。\n\n\n命名实体识别尝试\n现在我们尝试用词性标注的方法来进行命名实体识别。我们的目的是：对于既定的一套字符串，我们希望得到里面的名词，因为我们认为它会代表一些实际的实体对象。我非常喜欢一篇文章，是王小波的《一只特立独行的猪》，原谅我的任性，我要把这篇文章直接放在这里作为我们的中文语料对象。\n1cn = \"插队的时候，我喂过猪、也放过牛。假如没有人来管，这两种动物也完全知道该怎样生活。它们会自由自在地闲逛，饥则食渴则饮，春天来临时还要谈谈爱情；这样一来，它们的生活层次很低，完全乏善可陈。人来了以后，给它们的生活做出了安排：每一头牛和每一口猪的生活都有了主题。就它们中的大多数而言，这种生活主题是很悲惨的：前者的主题是干活，后者的主题是长肉。我不认为这有什么可抱怨的，因为我当时的生活也不见得丰富了多少，除了八个样板戏，也没有什么消遣。有极少数的猪和牛，它们的生活另有安排。以猪为例，种猪和母猪除了吃，还有别的事可干。就我所见，它们对这些安排也不大喜欢。种猪的任务是交配，换言之，我们的政策准许它当个花花公子。但是疲惫的种猪往往摆出一种肉猪（肉猪是阉过的）才有的正人君子架势，死活不肯跳到母猪背上去。母猪的任务是生崽儿，但有些母猪却要把猪崽儿吃掉。总的来说，人的安排使猪痛苦不堪。但它们还是接受了：猪总是猪啊。\n2对生活做种种设置是人特有的品性。不光是设置动物，也设置自己。我们知道，在古希腊有个斯巴达，那里的生活被设置得了无生趣，其目的就是要使男人成为亡命战士，使女人成为生育机器，前者像些斗鸡，后者像些母猪。这两类动物是很特别的，但我以为，它们肯定不喜欢自己的生活。但不喜欢又能怎么样？人也好，动物也罢，都很难改变自己的命运。\n3以下谈到的一只猪有些与众不同。我喂猪时，它已经有四五岁了，从名分上说，它是肉猪，但长得又黑又瘦，两眼炯炯有光。这家伙像山羊一样敏捷，一米高的猪栏一跳就过；它还能跳上猪圈的房顶，这一点又像是猫——所以它总是到处游逛，根本就不在圈里呆着。所有喂过猪的知青都把它当宠儿来对待，它也是我的宠儿——因为它只对知青好，容许他们走到三米之内，要是别的人，它早就跑了。它是公的，原本该劁掉。不过你去试试看，哪怕你把劁猪刀藏在身后，它也能嗅出来，朝你瞪大眼睛，噢噢地吼起来。我总是用细米糠熬的粥喂它，等它吃够了以后，才把糠对到野草里喂别的猪。其他猪看了嫉妒，一起嚷起来。这时候整个猪场一片鬼哭狼嚎，但我和它都不在乎。吃饱了以后，它就跳上房顶去晒太阳，或者模仿各种声音。它会学汽车响、拖拉机响，学得都很像；有时整天不见踪影，我估计它到附近的村寨里找母猪去了。我们这里也有母猪，都关在圈里，被过度的生育搞得走了形，又脏又臭，它对它们不感兴趣；村寨里的母猪好看一些。它有很多精彩的事迹，但我喂猪的时间短，知道得有限，索性就不写了。总而言之，所有喂过猪的知青都喜欢它，喜欢它特立独行的派头儿，还说它活得潇洒。但老乡们就不这么浪漫，他们说，这猪不正经。领导则痛恨它，这一点以后还要谈到。我对它则不止是喜欢——我尊敬它，常常不顾自己虚长十几岁这一现实，把它叫做“猪兄”。如前所述，这位猪兄会模仿各种声音。我想它也学过人说话，但没有学会——假如学会了，我们就可以做倾心之谈。但这不能怪它。人和猪的音色差得太远了。\n4后来，猪兄学会了汽笛叫，这个本领给它招来了麻烦。我们那里有座糖厂，中午要鸣一次汽笛，让工人换班。我们队下地干活时，听见这次汽笛响就收工回来。我的猪兄每天上午十点钟总要跳到房上学汽笛，地里的人听见它叫就回来——这可比糖厂鸣笛早了一个半小时。坦白地说，这不能全怪猪兄，它毕竟不是锅炉，叫起来和汽笛还有些区别，但老乡们却硬说听不出来。领导上因此开了一个会，把它定成了破坏春耕的坏分子，要对它采取专政手段——会议的精神我已经知道了，但我不为它担忧——因为假如专政是指绳索和杀猪刀的话，那是一点门都没有的。以前的领导也不是没试过，一百人也治不住它。狗也没用：猪兄跑起来像颗鱼雷，能把狗撞出一丈开外。谁知这回是动了真格的，指导员带了二十几个人，手拿五四式手枪；副指导员带了十几人，手持看青的火枪，分两路在猪场外的空地上兜捕它。这就使我陷入了内心的矛盾：按我和它的交情，我该舞起两把杀猪刀冲出去，和它并肩战斗，但我又觉得这样做太过惊世骇俗——它毕竟是只猪啊；还有一个理由，我不敢对抗领导，我怀疑这才是问题之所在。总之，我在一边看着。猪兄的镇定使我佩服之极：它很冷静地躲在手枪和火枪的连线之内，任凭人喊狗咬，不离那条线。这样，拿手枪的人开火就会把拿火枪的打死，反之亦然；两头同时开火，两头都会被打死。至于它，因为目标小，多半没事。就这样连兜了几个圈子，它找到了一个空子，一头撞出去了；跑得潇洒之极。以后我在甘蔗地里还见过它一次，它长出了獠牙，还认识我，但已不容我走近了。这种冷淡使我痛心，但我也赞成它对心怀叵测的人保持距离。\n5我已经四十岁了，除了这只猪，还没见过谁敢于如此无视对生活的设置。相反，我倒见过很多想要设置别人生活的人，还有对被设置的生活安之若素的人。因为这个原故，我一直怀念这只特立独行的猪。\"\n\n现在，我想识别这篇文章里面所有的名词。\n1tagging(cn,tag_worker) %\u003e%\n2  enframe() %\u003e%\n3  filter(name == \"n\") -\u003e tag_names\n\n现在我把文中的名词都筛选了出来。词性的列名称为name，词语的列名称为value。我要统计一下王小波在这篇文章中用到名词的词频。\n1tag_names %\u003e%\n2  count(value) %\u003e%   #对名词进行计数\n3  arrange(desc(n))   #降序排列\n4## # A tibble: 113 x 2\n5##    value     n\n6##    \u003cchr\u003e \u003cint\u003e\n7##  1 猪       17\n8##  2 人       12\n9##  3 母猪      8\n10##  4 汽笛      5\n11##  5 动物      4\n12##  6 领导      4\n13##  7 主题      4\n14##  8 狗        3\n15##  9 火枪      3\n16## 10 牛        3\n17## # ... with 103 more rows\n\n有意思，“猪”是出现最多的名词，其次是“人”，再到“母猪”。\n实际运用中，想必还是会有很多障碍。大家要记得，在用户自定义词库中，我们是可以给词性进行标注的！也就是我们的词想要识别成什么，我们自己可以说了算。这在垂直领域的运用中，是相当有用的。至于应该如何设置标注，大家可以观察原始词库的格式，然后对文本文件进行修饰。原始文件的位置在哪里？请直接键入DICTPATH，你会找到路径，然后用文本格式来查看这个文件即可。然后按照相应格式，来更改用户词典（同一个文件目录下的“user.dict.utf8”）。 我还是认为，算法是不可能超越词库的，多在词库下功夫，算法才能够发挥效用。应该想方设法构建更加优秀的自定义词库，并进行面向业务的精准标注，才能够在实际应用中获得好的效果。\n\n\n往期精彩：\nR语言自然语言处理：中文分词\n\n\n找工作难，面试失败的核心原因已经找到\n\n\nR语言中文社区2018年终文章整理（作者篇）\nR语言中文社区2018年终文章整理（类型篇）\n公众号后台回复关键字即可学习\n回复 爬虫            爬虫三大案例实战\n回复 Python       1小时破冰入门\n回复 数据挖掘     R语言入门及数据挖掘\n回复 人工智能     三个月入门人工智能\n回复 数据分析师  数据分析师成长之路\n回复 机器学习     机器学习的商业应用\n回复 数据科学     数据科学实战\n回复 常用算法     常用数据挖掘算法","data":"2019年03月06日 09:48:00"}
{"_id":{"$oid":"5d343b2a62f717dc0659b386"},"title":"什么是人工智能，机器学习和人工智能主要有什么关系？","author":"duozhishidai","content":"人工智能\n人工智能英文缩写为AI，它是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。人工智能是计算机科学研究领域的一个重要分支，又是众多学科的一个交叉学科，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括语音识别、图像识别、机器人、自然语言处理、智能搜索和专家系统等等，人工智能可以对人的意识、思维的信息过程的模拟。人工智能包括众多的分支领域，比如大家熟悉的机器学习、自然语言理解和模式识别等。\n机器学习\n机器学习属于人工智能研究与应用的一个分支领域。机器学习的研究更加偏向理论性，其目的更偏向于是研究一种为了让计算机不断从数据中学习知识，而使机器学习得到的结果不断接近目标函数的理论。\n机器学习，引用卡内基梅隆大学机器学习研究领域的着名教授TomMitchell的经典定义：\n如果一个程序在使用既有的经验E(Experience)来执行某类任务T(Task)的过程中被认为是“具备学习能力的”，那么它一定要展现出：利用现有的经验E，不断改善其完成既定任务T的性能(Performance)的特质。\n机器学习已经有了十分广泛的应用，例如：数据挖掘、计算机视觉、自然语言处理、生物特征识别、搜索引擎、医学诊断、检测信用卡欺诈、证券市场分析、DNA序列测序、语音和手写识别、战略游戏和机器人运用。在我们当下的生活中，语音输入识别、手写输入识别等技术，识别率相比之前若干年的技术识别率提升非常巨大，达到了将近97%以上，大家可以在各自的手机上体验这些功能，这些技术来自于机器学习技术的应用。\n人工智能、大数据、云计算和物联网的未来发展值得重视，均为前沿产业，多智时代专注于人工智能和大数据的入门和科谱，在此为你推荐几篇优质好文：\nAI vs 深度学习 vs 机器学习，人工智能的 12 大应用场景\nhttp://www.duozhishidai.com/article-15385-1.html\n人工智能全景图与发展趋势分析\nhttp://www.duozhishidai.com/article-15301-1.html\n在网络大时代背景下，人工智能技术是如何应用的\nhttp://www.duozhishidai.com/article-15277-1.html\n\n多智时代-\n人工智能\n和\n大数据\n学习入门网站|人工智能、大数据、\n物联网\n、\n云计算\n的学习交流网站","data":"2019年04月04日 13:43:31"}
{"_id":{"$oid":"5d343b2b62f717dc0659b388"},"title":"人工智能_简介","author":"North北","content":"人工智能：\nArtificial Intelligence，AI，它是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。\n人工智能 = 大数据 + 深度学习；\n应用场景：\n机器视觉，指纹识别，人脸识别，视网膜识别，虹膜识别，掌纹识别，专家系统，自动规划，智能搜索，定理证明，博弈，自动程序设计，智能控制，机器人学，语言和图像理解，遗传编程等。\n神经网络：\n神经元，神经网络，分层结构。\n人工神经网络（Artificial Neural Network，即ANN ），从信息处理角度对人脑神经元网络进行抽象， 建立某种简单模型，按不同的连接方式组成不同的网络。\n神经网络是一种运算模型，由大量的节点（或称神经元）之间相互联接构成。每个节点代表一种特定的输出函数，称为激励函数（activation function）。\n每两个节点间的连接都代表一个对于通过该连接信号的加权值，称之为权重，这相当于人工神经网络的记忆。\n网络的输出则依网络的连接方式，权重值和激励函数的不同而不同。而网络自身通常都是对自然界某种算法或者函数的逼近，也可能是对一种逻辑策略的表达。\n深度学习：\n深度学习是指多层神经网络上运用各种机器学习算法解决图像，文本等各种问题的算法集合。深度学习的核心是特征学习，旨在通过分层网络获取分层次的特征信息，从而解决以往需要人工设计特征的重要难题。深度学习是一个框架，包含多个重要算法。\n大数据：\n无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。\n大数据的5V特点（IBM提出）：Volume（大量）、Velocity（高速）、Variety（多样）、Value（低价值密度）、Veracity（真实性）。\n大数据（Big data）通常用来形容一个公司创造的大量非结构化数据和半结构化数据，这些数据在下载到关系型数据库用于分析时会花费过多时间和金钱。大数据分析常和云计算联系到一起，因为实时的大型数据集分析需要像MapReduce一样的框架来向数十、数百或甚至数千的电脑分配工作。\n弱人工智能：\n单一任务的计算，导航，汽车ABS。\n强人工智能：\n人类，多任务，多任务，复杂任务处理。\n构成理解：\n神经元网络：\ninput =\u003e {每个神经元（激活函数）@单层网络} * 多层 =\u003e 加权求和 =\u003e 求偏见 =\u003e output\n相关概念：\n目标函数，梯度下降，反向传播；","data":"2018年09月28日 14:12:12"}
{"_id":{"$oid":"5d343b2b62f717dc0659b38a"},"title":"自然语言处理-实际开发:用语义开放平台olami写一个翻译的应用","author":"Volcano__Liu","content":"自然语言处理-实际开发\n一个可以识别自然语言的翻译应用\n-----------------------------------------------------------------------------\n必不可少的开发环境\nEclipse4.5+JDK1.7+WindowBuilder插件\n其他资源\n语义平台：OLAMI\n源代码：https://github.com/volcanoliu/TranslateDemo\n可执行文件：http://download.csdn.net/detail/u011211290/9888544\n百度云地址：http://pan.baidu.com/s/1bQhH4U\n1.界面及使用\n这里介绍一下页面。\n整体分为三个部分，最上面的是对话框，中间的是回答框，最下面的比较大的显示的是从语义平台取得的语义数据。\n使用方式：把需要理解的语句输入到对话框中，点击发送，就可以得到结果。\n返回结果：\n2.代码简介\n这里先整体简单介绍一下。\nNLPJSON.java 里面是拿到语义返回JSON数据的关键字；\nAPIJSON.java 里面是拿到翻译返回JSON数据的关键字。\nApiLanguage.java 里面是翻译API接口需要的各国语言的缩写；\nEncrypt.java 功能是加密字符串，里面只有MD5加密的方法；\nFormat.java 功能是整理JSON内容，用于输出；\nGetModifier.java 功能是从OLAMI提供的API接口拿到语义；\nHttpRequestUtils.java 功能是发送HTTP请求,获得HTTP返回的数据；\nMainWindow.java 是主程序，做的是窗口的建立和主流程的控制；\nModifierProcess.java 功能是处理语义；\nTranslateByAPI.java 功能是从翻译API接口拿到翻译的结果；\n3.核心代码\n3.1 MainWindows.java\nButton btnNewButton = new Button(translateShell, SWT.NONE); translateShell.setDefaultButton(btnNewButton); btnNewButton.setLocation(319, 91); btnNewButton.setSize(80, 27); btnNewButton.addSelectionListener(new SelectionAdapter() { @Override public void widgetSelected(SelectionEvent e) { NLPText.setText(\"\"); String src = inputText.getText(); if (src == null || src.length() == 0) { answerText.setText(\"你还没有输入内容！\"); return; } // 把string用接口拿到语义 JSONObject nlp = GetModifier.GetNLI(src); NLPText.setText(Format.formatJson(nlp.toString())); // 处理语义 String answer = ModifierProcess.NLPProcess(nlp); answerText.setText(answer); if (errorFlag == 1) { answerText.setText(errorMessage); } else if (errorFlag == 2) { answerText.setText(\"遇到了错误，但这不是我的锅！\"); } resetError(); } }); btnNewButton.setText(\"发送\");\n这里是主程序的处理流程，发送按钮做了监听。\n先处理输入内容，然后把内容发送到语义平台拿到语义，然后去处理语义，输出结果。\n3.2 GetModifier.java\nprotected static JSONObject GetNLI(String src) { JSONObject data = new JSONObject(); data.put(\"input_type\", 1); data.put(\"text\", src); JSONObject send = new JSONObject(); send.put(\"data_type\", \"stt\"); send.put(\"data\", data); // 时间戳 long timestamp = new Timestamp(System.currentTimeMillis()).getTime(); // 签名 String sign = appSecret + \"api=\" + api + \"appkey=\" + appKey + \"timestamp=\" + timestamp + appSecret; sign = Encrypt.MD5(sign); // 参数 Map\u003cString, String\u003e post_data = new HashMap\u003cString, String\u003e(); post_data.put(\"appkey\", appKey); post_data.put(\"api\", api); post_data.put(\"timestamp\", String.valueOf(timestamp)); post_data.put(\"sign\", sign); post_data.put(\"rq\", send.toString()); post_data.put(\"cusid\", cusid); return HttpRequestUtils.httpPost(apiUrl, post_data); }\n3.3 ModifierProcess.java\nprotected static String NLPProcess(JSONObject nlpJson) { String status = nlpJson.getString(NLPJSON.JSON_STATUS); if (status == null || !NLPJSON.STATUS_OK.equalsIgnoreCase(status)) { MainWindow.setErrorFlag(2); return \"\"; } // Get info from JSON String result = \"\"; String modifier = null; String src_language = null; String dst_language = null; String src_code = null; String dst_code = null; String content = null; String resultLanguage = null; try { JSONObject nli = nlpJson.getJSONObject(NLPJSON.JSON_DATA).getJSONArray(NLPJSON.DATA_NLI).getJSONObject(0); JSONObject descobj = nli.getJSONObject(NLPJSON.NLI_DESCOBJ); if (!\"0\".equals(descobj.getString(NLPJSON.DESCOBJ_STATUS))) { MainWindow.setErrorFlag(1); MainWindow.setErrorMessage(descobj.getString(NLPJSON.DESCOBJ_RESULT)); return \"\"; } JSONObject semantic = nli.getJSONArray(NLPJSON.NLI_SEMANTIC).getJSONObject(0); modifier = semantic.getJSONArray(NLPJSON.SEMANTIC_MODIFIER).getString(0); JSONArray slotsArray = semantic.getJSONArray(NLPJSON.SEMANTIC_SLOTS); if (slotsArray != null \u0026\u0026 slotsArray.size() \u003e 0) { Map\u003cString, String\u003e slotsMap = new HashMap\u003cString, String\u003e(); for (int i = 0; i \u003c slotsArray.size(); i++) { JSONObject slots = slotsArray.getJSONObject(i); String name = slots.getString(NLPJSON.SLOTS_NAME); String value = slots.getString(NLPJSON.SLOTS_VALUE); slotsMap.put(name, value); } src_language = slotsMap.get(NLPJSON.S_SRCLANGUAGE); dst_language = slotsMap.get(NLPJSON.S_DSTLANGUAGE); content = slotsMap.get(NLPJSON.S_CONTENT); } } catch (Exception e) { MainWindow.setErrorFlag(2); return \"\"; } // Process the info if (modifier == null || modifier.length() \u003c 1) { MainWindow.setErrorFlag(2); return \"\"; }// 问能力 if (modifier.equalsIgnoreCase(NLPJSON.M_CAN)) { if (dst_language != null \u0026\u0026 dst_language.length() \u003e 0) { String language = ApiLanguage.language.get(dst_language); if (language != null \u0026\u0026 language.length() \u003e 0) { MainWindow.resetLastLanguage(); MainWindow.setLastDstLanguage(dst_language); result = \"没问题！\"; return result; } result = \"我还不懂\" + dst_language + \"，但我会慢慢学习的！\"; return result; } result = \"我可以翻译，问我吧，但最好不要太难哟！\"; return result; } else if (modifier.equalsIgnoreCase(NLPJSON.M_CANDOWHICH)) { // 问能翻译多少种语言 // map.keyset get 5 languages Set\u003cString\u003e set = ApiLanguage.language.keySet(); set.remove(ApiLanguage.AUTO); String[] language = set.toArray(new String[set.size()]); int start = 0; for (int i = 0; i \u003c outputLanguageNumber; i++) { Random random = new Random(); start += random.nextInt(language.length - outputLanguageNumber + i - start) + 1; result += language[start] + \"、\"; } MainWindow.resetLastLanguage(); result = \"我会翻译\" + result.substring(0, result.length() - 1) + \"...还有好多种语言！\"; return result; } else if (modifier.equalsIgnoreCase(NLPJSON.M_TRANSLATE)) { // 翻译内容 // If content exist, get languages, translate it by translateApi if (content != null \u0026\u0026 content.length() \u003e 0) { if (src_language == null || src_language.length() == 0) { String lastSrcLanguage = MainWindow.getLastSrcLanguage(); if (lastSrcLanguage == null || lastSrcLanguage.length() == 0) { src_code = ApiLanguage.language.get(ApiLanguage.AUTO); } else { src_code = ApiLanguage.language.get(lastSrcLanguage); } } else { src_code = ApiLanguage.language.get(src_language); if (src_code == null || src_code.length() == 0) { result = \"我还没有学会\" + src_language + \"，等我学会之后你再问我吧！\"; return result; } } if (dst_language == null || dst_language.length() == 0) { String lastDstLanguage = MainWindow.getLastDstLanguage(); if (lastDstLanguage == null || lastDstLanguage.length() == 0) { dst_code = ApiLanguage.language.get(ApiLanguage.ENGLISH); resultLanguage = ApiLanguage.ENGLISH; } else { dst_code = ApiLanguage.language.get(lastDstLanguage); resultLanguage = lastDstLanguage; } } else { dst_code = ApiLanguage.language.get(dst_language); resultLanguage = dst_language; if (dst_code == null || dst_code.length() == 0) { result = \"我还没有学会\" + dst_language + \"，等我学会之后你再问我吧！\"; return result; } } MainWindow.setLastSrcLanguage(src_language); MainWindow.setLastDstLanguage(dst_language); String transResult = TranslateByAPI.translateProcess(content, src_code, dst_code); if (transResult.equals(content)) { dst_code = ApiLanguage.language.get(ApiLanguage.CHINESE); resultLanguage = ApiLanguage.CHINESE; transResult = TranslateByAPI.translateProcess(content, src_code, dst_code); } if (\"\".equals(transResult)) { return \"\"; } result = \"【\" + content + \"】翻译成\" + resultLanguage + \"的结果是【\" + transResult + \"】\"; return result; } else { if (src_language != null \u0026\u0026 src_language.length() \u003e 0 \u0026\u0026 dst_language != null \u0026\u0026 dst_language.length() \u003e 0) { MainWindow.setLastSrcLanguage(src_language); MainWindow.setLastDstLanguage(dst_language); } result = \"你说，我来翻译！\"; return result; } } // 前面没有处理，设置错误标志 MainWindow.setErrorFlag(2); return result; }\n\n从语义数据拿到语义信息，然后判断返回的modifier和slot内容作出处理。\n3.4 TranslateByAPI.java\nprotected static String translateProcess(String src, String from, String to) { if (src == null || src.length() == 0) { MainWindow.setErrorFlag(2); return \"\"; } else if (src.length() \u003e 1500) { MainWindow.setErrorFlag(1); MainWindow.setErrorMessage(\"翻译的文字太多了！\"); return \"\"; } if (from == null) { MainWindow.setErrorFlag(1); MainWindow.setErrorMessage(\"输入的文字看不懂啊！\"); return \"\"; } if (to == null) { MainWindow.setErrorFlag(1); MainWindow.setErrorMessage(\"我还不会你想要的语言！\"); return \"\"; } int salt = new Random().nextInt(10000); // 签名 String sign = Encrypt.MD5(APPID + src + salt + key); if (sign == \"\") { return \"\"; } Map\u003cString, String\u003e params = new HashMap\u003cString, String\u003e(); params.put(\"appid\", APPID); params.put(\"salt\", String.valueOf(salt)); params.put(\"sign\", sign); params.put(\"from\", from); params.put(\"to\", to); params.put(\"q\", src); JSONObject translate = HttpRequestUtils.httpPost(apiUrl, params); String errorCode = translate.getString(APIJSON.ERROR); if (errorCode != null \u0026\u0026 errorCode.length() \u003e 0) { MainWindow.setErrorFlag(2); return \"\"; } JSONObject transResult = null; try { transResult = translate.getJSONArray(APIJSON.TRANS_RESULT).getJSONObject(0); } catch (Exception e) { MainWindow.setErrorFlag(2); return \"\"; } if (transResult == null) { MainWindow.setErrorFlag(2); return \"\"; } String result = transResult.getString(APIJSON.RESULT_DST); if (result == null) { MainWindow.setErrorFlag(2); return \"\"; } return result; }\n通过翻译API来翻译内容。API的调用方法大同小异。\n到此为止，翻译工作就完成了。\n4.总结\n总体来说，功能简单，代码简单，但是能做到的事情就很强大，总体还是归功于语义开放平台的语义解析。\n其他说明\nGetModifier.java 里面所使用到的调用语义API接口的关键信息，开发者需自行在OLAMI官网注册生成并更换，这样开发者可以自己定义所需的语法。\nTranslateByAPI.java 里所使用到的调用翻译API接口的关键信息，开发者也需自行更换。目前某度的翻译API处于量小免费量大收费的模式，开发者使用的话还是不用担心超出免费额度的情况。但是如果发现有被乱用的现象，作者将关闭翻译API的接口。\n语义开放平台\n1.注册登陆OLAMI官网；\n2.点击账号出现下拉菜单，点击“NLI系统”进入语法编辑系统（可以选择导入已有的模块，也可以创建模块自定义语法）；\n3.导入translate语义模块；\n4.进入语义模块，点击“发布”进入发布页面，点击“发布”按钮，启用刚才导入的语法；\n5.回到官网，点击账号出现菜单，点击“应用管理”，进入应用管理页面，点击“创建新应用”，新建一个应用；\n6.配置应用，点击“配置模块”，勾选模块，选择这个应用需要支持的模块，第一个选项卡是自定义的模块，第二个选项卡是系统自带模块；\n7.“查看Key”可以查询使用API调用应用所需要的key，“测试”可以测试应用的语法。\n更多的文档可以参照OLAMI文档中心\n最后\n这里只是做了一个可以运行的demo。如果说能把这些功能整合到个人网站，或者公众号、微信小程序之中，就能极大的提高逼格。\n[闲聊-智能对话：微信小程序详解]\n[“欢快”的小程序开发之路]\n[微信小程序IOS端showLoading之后showToast不显示]\n---------------\n**优秀自然语言理解博客文章推荐：**\n\n[根据OLAMI平台开发的日历Demo]\n\n[用olami开放语义平台做汇率换算应用]\n\n[自然语言处理-实际开发:用语义开放平台olami写一个翻译的应用]\n\n[自定义java.awt.Canvas—趣味聊天]\n\n[微信小程序+OLAMI自然语言API接口制作智能查询工具--快递、聊天、日历等]\n\n[热门自然语言理解和语音API开发平台对比]\n\n[使用OLAMI SDK和讯飞语音合成制作一个语音回复的短信小助手]\n\n[告诉你如何使用OLAMI自然语言理解开放平台API制作自己的智能对话助手]","data":"2017年07月04日 14:51:24"}
{"_id":{"$oid":"5d343b2d62f717dc0659b38d"},"title":"深度学习(10)：自然语言处理 2018-03-01","author":"langlanlacn3","content":"Witmart.com帮你担保，做网络兼职也能轻轻松松赚美元，提现无忧可靠安全。\n深度学习(10)：自然语言处理 2018-03-01\n自然语言处理（Natural Language Processing，NLP)是人工智能和语言学领域的学科分支，它研究实现人与计算机之间使用自然语言进行有效通信的各种理论和方法。\n词嵌入\n前面介绍过，处理文本序列时，通常用建立字典后以one-hot的形式表示某个词，进而表示某个句子的方法。这种表示方法孤立了每个词，无法表现各个词之间的相关性，满足不了NLP的要求。\n词嵌入（Word Embedding）是NLP中语言模型与表征学习技术的统称，概念上而言，它是指把一个维数为所有词的数量的高维空间（one-hot形式表示的词）“嵌入”到一个维数低得多的连续向量空间中，每个单词或词组被映射为实数域上的向量。\nWord Embedding\n如上图中，各列分别组成的向量是词嵌入后获得的第一行中几个词的词向量的一部分。这些向量中的值，可代表该词与第一列中几个词的相关程度。\n使用2008年van der Maaten和Hinton在论文[Visualizing Data using t-SNE]中提出的t-SNE数据可视化算法，将词嵌入后获得的一些词向量进行非线性降维，可到下面的映射结果：\nt-SNE映射\n其中可发现，各词根据它们的语义及相关程度，分别汇聚在了一起。\n对大量词汇进行词嵌入后获得的词向量，可用来完成命名实体识别（Named Entity Recognition)等任务。其中可充分结合迁移学习，以降低学习成本，提高效率。\n好比前面讲过的用Siamese网络进行人脸识别过程，使用词嵌入方法获得的词向量可实现词汇的类比及相似度度量。例如给定对应关系“男性（Man）”对“女性（Woman）”，要求机器类比出“国王（King）”对应的词汇，通过上面的表格，可发现词向量存在数学关系“Man - Woman ≈≈ King - Queen”，也可以从可视化结果中看出“男性（Man）”到“女性（女性）”的向量与“国王（King）”到“王后（Queen）”的向量相似。词嵌入具有的这种特性，在2013年Mikolov等发表的论文[Linguistic Regularities in Continuous Space Word Representations]中提出，成为词嵌入领域具有显著影响力的研究成果。\n上述思想可写成一个余弦（cos）相似度函数：\nsim(u,v)=uTv∣∣u∣∣2∣∣v∣∣2sim(u,v)=uTv∣∣u∣∣2∣∣v∣∣2\n以此度量词向量的相似度。\n词嵌入方法\n词嵌入的方法包括人工神经网络、对词语同现矩阵降维、概率模型以及单词所在上下文的显式表示等。以词汇的one-hot形式作为输入，不同的词嵌入方法能以不同的方式学习到一个嵌入矩阵（Embedding Matrix），最后输出某个词的词向量。\n将字典中位置为ii的词以one-hot形式表示为oioi，嵌入矩阵用EE表示，词嵌入后生成的词向量用eiei表示，则三者存在数学关系：\nE⋅oi=eiE⋅oi=ei\n例如字典中包含10000个词，每个词的one-hot形式就是个大小为10000×110000×1的列向量，采用某种方法学习到的嵌入矩阵大小为300×10000300×10000的话，将生成大小为300×1300×1的词向量。\n神经概率语言模型\n采用神经网络建立语言模型是学习词嵌入的有效方法之一。2003年Bengio等人的经典之作[A Neural Probabilistic Language Model]中，提出的神经概率语言模型，是早期最成功的词嵌入方法之一。\n模型中，构建了了一个能够通过上下文来预测未知词的神经网络，在训练这个语言模型的同时学习词嵌入。例如将下图中上面的句子作为下面的神经网络的输入：\n语言模型\n经过隐藏层后，最后经Softmax将输出预测结果。其中的嵌入矩阵EE与ww、bb一样，是该网络中的参数，需通过训练得到。训练过程中取语料库中的某些词作为目标词，以目标词的部分上下文作为输入，训练网络输出的预测结果为目标词。得到了嵌入矩阵，就能通过前面所述的数学关系式求得词嵌入后的词向量。\nWORD2VEC\nWord2Vec（Word To Vectors）是现在最常用、最流行的词嵌入算法，它由2013年由Mikolov等人在论文[Efficient Estimation of Word Representations in Vector Space]中提出。\nWord2Vec中的Skip-Gram模型，所做的是在语料库中选定某个词（Context），随后在该词的正负10个词距内取一些目标词（Target）与之配对，构造一个用Context预测输出为Target的监督学习问题，训练一个如下图结构的网络：\nSkip-Gram网络\n该网络仅有一个Softmax单元，输出Context下Target出现的条件概率：\np(t∣c)=exp(θTtec)∑mj=1exp(θTjec)p(t∣c)=exp(θtTec)∑j=1mexp(θjTec)\n上式中θtθt是一个与输出的Target有关的参数，其中省略了用以纠正偏差的参数。训练过程中还是用交叉熵损失函数。\n选定的Context是常见或不常见的词将影响到训练结果，在实际中，Context并不是单纯地通过在语料库均匀随机采样得到，而是采用了一些策略来平衡选择。\nWord2Vec中还有一种CBOW（Continuous Bag-of-Words Model）模型，它的工作方式是采样上下文中的词来预测中间的词，与Skip-Gram相反。\n以上方法的Softmax单元中产生的计算量往往过大，改进方法之一是使用分级Softmax分类器（Hierarchical Softmax Classifier），采用霍夫曼树（Huffman Tree）来代替隐藏层到输出Softmax层的映射。\n此外，Word2Vec的作者在后续论文[Distributed Representations of Words and Phrases and their Compositionality]中提出了负采样（Negative Sampling）模型，进一步改进和简化了词嵌入方法。\n负采样模型中构造了一个预测给定的单词是否为一对Context-Target的新监督学习问题，采用的网络结构和前面类似：\n负采样\n训练过程中，从语料库中选定Context，输入的词为一对Context-Target，则标签设置为1。另外任取kk对非Context-Target，作为负样本，标签设置为0。只有较少的训练数据，kk的值取5~20的话，能达到比较好的效果；拥有大量训练数据，kk的取值取2~5较为合适。\n原网络中的Softmax变成多个Sigmoid单元，输出Context-Target（c,t）对为正样本（y=1y=1)的概率：\np(y=1∣c,t)=σ(θTtec)p(y=1∣c,t)=σ(θtTec)\n其中的θtθt、ecec分别代表Target及Context的词向量。通过这种方法将之前的一个复杂的多分类问题变成了多个简单的二分类问题，而降低计算成本。\n模型中还包含了对负样本的采样算法。从本质上来说，选择某个单词来作为负样本的概率取决于它出现频率，对于更经常出现的单词，将更倾向于选择它为负样本，但这样会导致一些极端的情况。模型中采用一下公式来计算选择某个词作为负样本的概率：\np(wi)=f(wi)34∑mj=0f(wj)34p(wi)=f(wi)34∑j=0mf(wj)34\n其中f(wi)f(wi)代表语料库中单词wiwi出现的频率。\nGLOVE\nGloVe（Global Vectors）是另一种现在流行的词嵌入算法,它在2014年由Pennington等人在论文[GloVe: Global Vectors for Word Representation]中提出。\nGlove模型中，首先基于语料库统计了词的共现矩阵XX，XX中的元素为Xi,jXi,j，表示整个语料库中单词ii和单词jj彼此接近的频率，也就是它们共同出现在一个窗口中的次数。之后要做的，就是优化以下代价函数：\nJ=∑i,jNf(Xi,j)(θTiej+bi+bj−log(Xi,j))2J=∑i,jNf(Xi,j)(θiTej+bi+bj−log(Xi,j))2\n其中θiθi、ejej分是单词ii和单词jj的词向量，bibi、bjbj是两个偏差项，f()f()是一个用以防止Xi,j=0Xi,j=0时log(Xi,j)log(Xi,j)无解的权重函数，词汇表的大小为NN。\n（以上优化函数的推导过程见参考资料中的“理解GloVe模型”）\n最后要说明的是，使用各种词嵌入方法学习到的词向量，并不像最开始介绍词嵌入时展示的表格中Man、Woman、King、Queen的词向量那样，其中的值能够代表着与Gender、Royal等词的的相关程度，实际上它们大都超出了人们的能够理解范围。\n词嵌入应用：情感分类器\nNLP中的情感分类，是对某段文字中所表达的情感做出分类，它能在很多个方面得到应用。训练情感分类模型时，面临的挑战之一可能是标记好的训练数据不够多。然而有了词嵌入得到的词向量，只需要中等数量的标记好的训练数据，就能构建出一个表现出色的情感分类器。\n情感分类\n如上图，要训练一个将左边的餐厅评价转换为右边评价所属星级的情感分类器，也就是实现xx到yy的映射。有了用词嵌入方法获得的嵌入矩阵EE，一种简单的实现方法如下：\n简单方法\n方法中计算出句中每个单词的词向量后，取这些词向量的平均值输入一个Softmax单元，输出预测结果。这种简单的方法适用于任何长度的评价，但忽略了词的顺序，对于某些包含多个正面评价词的负面评价，很容易预测到错误结果。\n采用RNN能实现一个表现更加出色的情感分类器，此时构建的模型如下：\nRNN情感分类\n这是一个“多对一”结构的循环神经网络，每个词的词向量作为网络的输入，由Softmax输出结果。由于词向量是从一个大型的语料库中获得的，这种方法将保证了词的顺序的同时能够对一些词作出泛化。\n词嵌入除偏\n在词嵌入过程中所使用的语料库中，往往会存在一些性别、种族、年龄、性取向等方面的偏见，从而导致获得的词向量中也包含这些偏见。比如使用未除偏的词嵌入结果进行词汇类比时，“男性（Man）”对“程序员（Computer Programmer）”将得到类似“女性（Woman）”对“家务料理人（Homemaker）”的性别偏见结果。2016年Bolukbasi等人在论文[Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings]中提出了一些消除词嵌入中的偏见的方法。\n这里列举消除词向量存在的性别偏见的过程，来说明这些方法。（摘自第二周课后作业）\n1.中和本身与性别无关词汇\n中和（Neutralize）“医生（doctor）”、“老师（teacher）”、“接待员（receptionist）”等本身与性别无关词汇中的偏见，首先计算g=ewoman−emang=ewoman−eman，用“女性（woman）”的词向量减去“男性（man）”的词向量，得到的向量gg就代表了“性别（gender）”。假设现有的词向量维数为50，那么对某个词向量，将50维空间分成两个部分：与性别相关的方向gg和与gg正交的其他49个维度g⊥g⊥。如下左图：\n本身与性别无关\n除偏的步骤，是将要除偏的词向量，左图中的ereceptionistereceptionist，在向量gg方向上的值置为00，变成右图所示的edebiasedreceptionistereceptionistdebiased。所用的公式如下:\nebiascomponent=e⋅g||g||22×gecomponentbias=e⋅g||g||22×g\nedebiasedreceptionist=e−ebiascomponentereceptionistdebiased=e−ecomponentbias\n2.均衡本身与性别有关词汇\n对“男演员（actor）”、“女演员（actress）”、“爷爷（grandfather）”等本身与性别有关词汇，如下左图，假设“女演员（actress）”的词向量比“男演员（actor）”更靠近于“婴儿看护人（babysit）”。中和“婴儿看护人（babysit）”中存在的性别偏见后，还是无法保证它到“女演员（actress）”与到“男演员（actor）”的距离相等。对一对这样的词，除偏的过程是均衡（Equalization）它们的性别属性。\n本身与性别有关\n均衡过程的核心思想是确保一对词（actor和actress）到g⊥g⊥的距离相等的同时，也确保了它们到除偏后的某个词（babysit）的距离相等，如上右图。\n对需要除偏的一对词w1w1、w2w2，选定与它们相关的某个未中和偏见的单词BB之后，均衡偏见的过程如下公式：\nμ=ew1+ew22μ=ew1+ew22\nμB=μ⋅bias_axis||bias_axis||22×bias_axisμB=μ⋅bias_axis||bias_axis||22×bias_axis\nμ⊥=μ−μBμ⊥=μ−μB\new1B=ew1⋅bias_axis||bias_axis||22×bias_axisew1B=ew1⋅bias_axis||bias_axis||22×bias_axis\new2B=ew2⋅bias_axis||bias_axis||22×bias_axisew2B=ew2⋅bias_axis||bias_axis||22×bias_axis\necorrectedw1B=|1−||μ⊥||22|−−−−−−−−−√×ew1B−μB||(ew1−μ⊥)−μB)||2ew1Bcorrected=|1−||μ⊥||22|×ew1B−μB||(ew1−μ⊥)−μB)||2\necorrectedw2B=|1−||μ⊥||22|−−−−−−−−−√×ew2B−μB||(ew1−μ⊥)−μB)||2ew2Bcorrected=|1−||μ⊥||22|×ew2B−μB||(ew1−μ⊥)−μB)||2\ne1=ecorrectedw1B+μ⊥e1=ew1Bcorrected+μ⊥\ne2=ecorrectedw2B+μ⊥e2=ew2Bcorrected+μ⊥\n参考资料\n吴恩达-序列模型-网易云课堂\nAndrew Ng-Sequence Model-Coursera\ndeeplearning.ai\nDeep Learning in NLP（一）词向量和语言模型\n从SNE到t-SNE再到LargeVis\nword2vec前世今生\nWord2Vec导学第二部分-负采样-csdn\n理解GloVe模型-csdn\n课程代码与资料-GitHub\n注：本文涉及的图片及资料均整理翻译自Andrew Ng的Deep Learning系列课程，版权归其所有。翻译整理水平有限，如有不妥的地方欢迎指出。","data":"2018年06月05日 10:02:41"}
{"_id":{"$oid":"5d343b2d62f717dc0659b38f"},"title":"自然语言处理大纲及正则表达式","author":"zhuchangbo18","content":"1.自然语言处理要解决哪些任务？\n（1）解剖类：分词、词性标注、命名实体识别、word2vec\n（2）生成类：文本分类、主题识别、关键词提取、自动摘要、情感分析、文本生成\n（3）情感分析、智能问答系统和知识图谱\n为了直观理解这些任务，推荐这个网站：http://ictclas.nlpir.org/nlpir/\n比较商业成熟一些的网站：http://www.datagrand.com/\n2.对应 不同的任务所需要掌握的技能树有哪些？\n数据清洗：正则表达式匹配\n基础处理：one hot,bag of words(文本数字化)，tf-idf\n分词：英文nltk,spacy, 中文jieba\n词性标注：英文：nltk,spacy，中文:jieba,CRF(条件随机场)，HMM(隐马)\n命名实体识别：英文: nltk,spacy; 中文：CRF、Stanford CoreNLP\n主题识别：plsa和LDA\n文本分类：Word2vec+CNN\n文本生成：RNN, LSTM\n情感分析:关键词打分机制，比如AFINN-111\n3.正则表达式匹配：https://blog.csdn.net/qq_28633249/article/details/77686976\n这篇博客讲得挺全的,另外附上规则表。","data":"2018年08月25日 21:28:34"}
{"_id":{"$oid":"5d343b2d62f717dc0659b391"},"title":"NLP自然语言处理的开发环境搭建","author":"村雨1943","content":"NLP的开发环境搭建主要分为以下几步：\nPython安装\nNLTK系统安装\nPython3.5下载安装\n下载链接：https://www.python.org/downloads/release/python-354/\n安装步骤：\n\n双击下载好的python3.5的安装包，如下图；\n\n选择默认安装还是自定义安装，一般默认安装就好，直接跳到步骤5，自定义的接着看步骤3，PS：Add Python3.5 to PATH勾选上，免去再去配置环境变量的麻烦；\n\n选择一些需要的设置；\n\n勾选一些高级选项；\n\n等待安装完成；\n\n安装结束；\n\n测试安装是否成功，控制台输入python，出现下列提示则表示安装成功；\n\nNLTK系统安装\n利用pip安装，控制台输入pip install nltk，则会开始安装，可能安装速度会很慢，耐心等待，因为笔者已经安装过，所以提示已经安装，PS:安装过程中可能会报错，这是由于需要安装依赖包，把报错信息中的依赖包安装上之后再继续安装NLTK就可以了；\n\n测试安装是否成功，打开控制台进入python环境中之后，导入nltk包，下载nltk数据包；\n\n\n等待所有数据包缓存结束之后，环境就搭建完成了，是不是So easy！","data":"2018年07月11日 08:51:23"}
{"_id":{"$oid":"5d343b2e62f717dc0659b393"},"title":"人工智能-计算机视觉-图像处理-模式识别的关系","author":"szfhy","content":"图像处理是将输入图像转换为输出图像的过程，人是图像处理的效果的最终解释者；\n在计算机视觉中，计算机是图像的解释者；图像处理仅仅是计算机视觉系统中的一个模块；\n计算机图形学的主要工作是从三维描述到二维图像显示的过程；\n计算机视觉则是从二维图像数据到三维描述的过程，计算机视觉是计算机图形学的逆问题。\n模式识别主要解决分类的问题，是计算机视觉中的一个模块；\n\n\n总体来说他们有如下的关系：\n不要把几个相关的概念混为一谈","data":"2016年03月30日 14:18:12"}
{"_id":{"$oid":"5d343b2e62f717dc0659b395"},"title":"Kaggle竞赛项目--自然语言处理","author":"一勇之夫","content":"视频地址:  https://www.bilibili.com/video/av55504443/\n相关资料： 待审核\nKaggle竞赛题：https://www.kaggle.com/c/home-depot-product-search-relevance\n这里将使用多种处理库，对比一下Python NLP领域各个库的优缺点。\nStep1：导入所需\n所有要用到的库，读入训练/测试集，及介绍\nimport numpy as np import pandas as pd from sklearn.ensemble import RandomForestRegressor, BaggingRegressor from nltk.stem.snowball import SnowballStemmer df_train = pd.read_csv('../input/train.csv', encoding=\"ISO-8859-1\") df_test = pd.read_csv('../input/test.csv', encoding=\"ISO-8859-1\") df_desc = pd.read_csv('../input/product_descriptions.csv')\n查看数据\ndf_train.head()\ndf_desc.head()\n看来不要做太多的复杂处理，我们于是直接合并测试/训练集，以便于统一做进一步的文本预处理\ndf_all = pd.concat((df_train, df_test), axis=0, ignore_index=True) df_all.head()\ndf_all = pd.merge(df_all, df_desc, how='left', on='product_uid') df_all.head()\nStep 2: 文本预处理\n我们这里遇到的文本预处理比较简单，因为最主要的就是看关键词是否会被包含。\n所以我们统一化我们的文本内容，以达到任何term在我们的数据集中只有一种表达式的效果。\n我们这里用简单的Stem做个例子：（有兴趣再选用其他预处理方式：去掉停止词，纠正拼写，去掉数字，去掉各种emoji，等等）\nstemmer = SnowballStemmer('english') def str_stemmer(s): return \" \".join([stemmer.stem(word) for word in s.lower().split()]) def str_common_word(str1, str2): return sum(int(str2.find(word)\u003e=0) for word in str1.split()) df_all['search_term'] = df_all['search_term'].map(lambda x:str_stemmer(x)) df_all['product_title'] = df_all['product_title'].map(lambda x:str_stemmer(x)) df_all['product_description'] = df_all['product_description'].map(lambda x:str_stemmer(x))\nStep 3: 自制文本特征\n一般属于一种脑洞大开的过程，想到什么可以加什么。\n当然，特征也不是越丰富越好，稍微靠谱点是肯定的。\n关键词的长度：\ndf_all['len_of_query'] = df_all['search_term'].map(lambda x:len(x.split())).astype(np.int64)\n描述中有多少关键词重合\ndf_all['commons_in_desc'] = df_all.apply(lambda x:str_common_word(x['search_term'],x['product_description']), axis=1)\n等等等等。。变着法子想出些数字能代表的features，一股脑放进来~\n搞完之后，我们把不能被『机器学习模型』处理的column给drop掉\ndf_all = df_all.drop(['search_term','product_title','product_description'],axis=1)\nStep 4: 重塑训练/测试集\n舒淇说得好，要把之前脱下的衣服再一件件穿回来\n数据处理也是如此，搞完一圈预处理之后，我们让数据重回原本的样貌\n分开训练和测试集\ndf_train = df_all.loc[df_train.index] df_test = df_all.loc[df_test.index]\n记录下测试集的id   留着上传的时候 能对的上号\ntest_ids = df_test['id']\n分离出y_train\ny_train = df_train['relevance'].values\n把原集中的label给删去  否则就是cheating了\nX_train = df_train.drop(['id','relevance'],axis=1).values X_test = df_test.drop(['id','relevance'],axis=1).values\nStep 5: 建立模型\n我们用个最简单的模型：Ridge回归模型\nfrom sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import cross_val_score\n用CV结果保证公正客观性；并调试不同的alpha值\nparams = [1,3,5,6,7,8,9,10] test_scores = [] for param in params: clf = RandomForestRegressor(n_estimators=30, max_depth=param) test_score = np.sqrt(-cross_val_score(clf, X_train, y_train, cv=5, scoring='neg_mean_squared_error')) test_scores.append(np.mean(test_score))\n画个图来看看：\nimport matplotlib.pyplot as plt %matplotlib inline plt.plot(params, test_scores) plt.title(\"Param vs CV Error\");\n大概6~7的时候达到了最优解\nStep 6: 上传结果\n用我们测试出的最优解建立模型，并跑跑测试集\nrf = RandomForestRegressor(n_estimators=30, max_depth=6) rf.fit(X_train, y_train)\nRandomForestRegressor(bootstrap=True, criterion='mse', max_depth=6, max_features='auto', max_leaf_nodes=None, min_impurity_split=1e-07, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=1, oob_score=False, random_state=None, verbose=0, warm_start=False)\ny_pred = rf.predict(X_test) pd.DataFrame({\"id\": test_ids, \"relevance\": y_pred}).to_csv('submission.csv',index=False)\n总结：\n虽然都是用的最简单的方法，但是基本框架是很完整的。\n后续可以尝试修改/调试/升级的部分是：\n文本预处理步骤: 你可以使用很多不同的方法来使得文本数据变得更加清洁\n自制的特征: 相处更多的特征值表达方法（关键词全段重合数量，重合比率，等等）\n更好的回归模型: 根据之前的课讲的Ensemble方法，把分类器提升到极致","data":"2019年06月22日 01:01:25"}
{"_id":{"$oid":"5d343b2e62f717dc0659b397"},"title":"自然语言处理项目之新闻主题分类Python实现","author":"湾区人工智能","content":"''' #2018-06-10 June Sunday the 23 week, the 161 day SZ 数据来源：链接:https://pan.baidu.com/s/1_w7wOzNkUEaq3KAGco19EQ 密码:87o0 朴素贝叶斯与应用 文本分类问题 经典的新闻主题分类，用朴素贝叶斯做。 #还有点问题。无法正确读取数据。UnicodeDecodeError: 'charmap' codec can't decode byte 0x90 in position 41: character maps to \u003cundefined\u003e folder_path = 'D:/自然语言处理/第2课/Lecture_2/Lecture_2/Naive-Bayes-Text-Classifier/Database/SogouC/Sample' all_words_list, train_data_list, test_data_list, train_class_list, test_class_list = text_processing(folder_path,test_size=0.2) ''' import os import time import random import codecs import jieba #处理中文 #import nltk #处理英文 import sklearn from sklearn.naive_bayes import MultinomialNB import numpy as np import pylab as pl import matplotlib.pyplot as plt import sys #reload(sys) #sys.setdefaultencoding('utf8') #粗暴的词去重 def make_word_set(words_file): words_set = set() with open(words_file, 'r') as fp: for line in fp.readlines(): word = line.strip().decode(\"utf-8\") if len(word)\u003e0 and word not in words_set: # 去重 words_set.add(word) return words_set # 文本处理，也就是样本生成过程 def text_processing(folder_path, test_size=0.2): folder_list = os.listdir(folder_path) data_list = [] class_list = [] # 遍历文件夹 for folder in folder_list: new_folder_path = os.path.join(folder_path, folder) files = os.listdir(new_folder_path) # 读取文件 j = 1 for file in files: if j \u003e 100: # 怕内存爆掉，只取100个样本文件，你可以注释掉取完 break with open(os.path.join(new_folder_path, file), 'r') as fp: raw = fp.read() ## 是的，随处可见的jieba中文分词 #jieba.enable_parallel(4) # 开启并行分词模式，参数为并行进程数，不支持windows word_cut = jieba.cut(raw, cut_all=False) # 精确模式，返回的结构是一个可迭代的genertor word_list = list(word_cut) # genertor转化为list，每个词unicode格式 #jieba.disable_parallel() # 关闭并行分词模式 data_list.append(word_list) #训练集list class_list.append(folder.decode('utf-8')) #类别 j += 1 ## 粗暴地划分训练集和测试集 data_class_list = zip(data_list, class_list) random.shuffle(data_class_list) index = int(len(data_class_list)*test_size)+1 train_list = data_class_list[index:] test_list = data_class_list[:index] train_data_list, train_class_list = zip(*train_list) test_data_list, test_class_list = zip(*test_list) #其实可以用sklearn自带的部分做 #train_data_list, test_data_list, train_class_list, test_class_list = sklearn.cross_validation.train_test_split(data_list, class_list, test_size=test_size) # 统计词频放入all_words_dict all_words_dict = {} for word_list in train_data_list: for word in word_list: if all_words_dict.has_key(word): all_words_dict[word] += 1 else: all_words_dict[word] = 1 # key函数利用词频进行降序排序 all_words_tuple_list = sorted(all_words_dict.items(), key=lambda f:f[1], reverse=True) # 内建函数sorted参数需为list all_words_list = list(zip(*all_words_tuple_list)[0]) return all_words_list, train_data_list, test_data_list, train_class_list, test_class_list def words_dict(all_words_list, deleteN, stopwords_set=set()): # 选取特征词 feature_words = [] n = 1 for t in range(deleteN, len(all_words_list), 1): if n \u003e 1000: # feature_words的维度1000 break if not all_words_list[t].isdigit() and all_words_list[t] not in stopwords_set and 1\u003clen(all_words_list[t])\u003c5: feature_words.append(all_words_list[t]) n += 1 return feature_words # 文本特征 def text_features(train_data_list, test_data_list, feature_words, flag='nltk'): def text_features(text, feature_words): text_words = set(text) ## ----------------------------------------------------------------------------------- if flag == 'nltk': ## nltk特征 dict features = {word:1 if word in text_words else 0 for word in feature_words} elif flag == 'sklearn': ## sklearn特征 list features = [1 if word in text_words else 0 for word in feature_words] else: features = [] ## ----------------------------------------------------------------------------------- return features train_feature_list = [text_features(text, feature_words) for text in train_data_list] test_feature_list = [text_features(text, feature_words) for text in test_data_list] return train_feature_list, test_feature_list # 分类，同时输出准确率等 def text_classifier(train_feature_list, test_feature_list, train_class_list, test_class_list, flag='nltk'): ## ----------------------------------------------------------------------------------- if flag == 'nltk': ## 使用nltk分类器 train_flist = zip(train_feature_list, train_class_list) test_flist = zip(test_feature_list, test_class_list) classifier = nltk.classify.NaiveBayesClassifier.train(train_flist) test_accuracy = nltk.classify.accuracy(classifier, test_flist) elif flag == 'sklearn': ## sklearn分类器 classifier = MultinomialNB().fit(train_feature_list, train_class_list) test_accuracy = classifier.score(test_feature_list, test_class_list) else: test_accuracy = [] return test_accuracy print (\"start\") ## 文本预处理 folder_path = 'D:/自然语言处理/第2课/Lecture_2/Lecture_2/Naive-Bayes-Text-Classifier/Database/SogouC/Sample' all_words_list, train_data_list, test_data_list, train_class_list, test_class_list = text_processing(folder_path,test_size=0.2) # 生成stopwords_set stopwords_file = 'D:\\\\自然语言处理\\\\第2课\\\\Lecture_2\\\\Lecture_2\\\\Naive-Bayes-Text-Classifier\\\\stopwords_cn.txt' stopwords_set = make_word_set(stopwords_file) ## 文本特征提取和分类 # flag = 'nltk' flag = 'sklearn' deleteNs = range(0, 1000, 20) test_accuracy_list = [] for deleteN in deleteNs: # feature_words = words_dict(all_words_list, deleteN) feature_words = words_dict(all_words_list, deleteN, stopwords_set) train_feature_list, test_feature_list = text_features(train_data_list, test_data_list, feature_words, flag) test_accuracy = text_classifier(train_feature_list, test_feature_list, train_class_list, test_class_list, flag) test_accuracy_list.append(test_accuracy) print (test_accuracy_list) # 结果评价 #plt.figure() plt.plot(deleteNs, test_accuracy_list) plt.title('Relationship of deleteNs and test_accuracy') plt.xlabel('deleteNs') plt.ylabel('test_accuracy') plt.show() #plt.savefig('result.png') print (\"finished\")","data":"2018年06月11日 11:12:00"}
{"_id":{"$oid":"5d343b2f62f717dc0659b399"},"title":"自然语言处理（资源篇）","author":"守望者白狼","content":"理论\n动态 | FAIR 最新论文：一种不需要训练就能探索句子分类的随机编码器\nNLP 中评价文本输出都有哪些方法？为什么要小心使用 BLEU？\nAAAI 2019教程—361页PPT带你回顾最新词句Embedding技术和应用\n嘘，这里有千展价值提高95%的秘密\n句法敏感的实体表示用于神经网络关系抽取\n命名实体识别中，众包标注能否优于专家标注？\n针对商品标题冗长问题，阿里工程师怎么解决？\n如何生成你的专属推荐文案？智能文案在1688平台的应用\nFacebook最新论文：跨语言模型预训练，三大任务刷新最高性能\n跨语言版BERT：Facebook提出跨语言预训练模型XLM\n\u003c命名实体识别（NER）综述\n用可视化解构BERT，我们从上亿参数中提取出了6种直观模式\nCMU、谷歌提出Transformer-XL：学习超长上下文关系\n知识图谱最新论文清单，高阶炼丹师为你逐一解读\nBERT大火却不懂Transformer？读这一篇就够了\nNLP 的巨人肩膀（下）：从 CoVe 到 BERT\n深度长文：NLP的巨人肩膀（上）\n博客 | 谷歌最强 NLP 模型 BERT 解读\n深度学习文本分类实战报告：CNN, RNN \u0026 HAN\n微软小冰首席科学家武威解读 EMNLP 论文：聊天机器人的深度学习模型\nCCKS 2018 | 最佳论文：南京大学提出DSKG，将多层RNN用于知识图谱补全\n从经典到端到端，详解问答系统和机器阅读理解\n消除NLP中的刻板印象：程序员之于男性=家政人员之于女性？\n让AI触类旁通93种语言：Facebook最新多语种句嵌入来了\n在大规模数据集上应用潜在语义分析的三种方式\n从语言学角度看词嵌入模型\n谷歌Quoc Le这篇NLP预训练模型论文值得一看\n博客 | 总结+paper分享 | 任务型对话中的跨领域\u0026个性化\u0026迁移学习\n\u003c博客 | 任务型对话系统公式建模\u0026\u0026实例说明\n项目\n推荐系统\nGithub项目推荐 | RecQ - Python推荐系统框架\nICME 2019短视频内容理解与推荐竞赛正式启动（附baseline方法）\n从KDD 2018最佳论文看Airbnb实时搜索排序中的Embedding技巧\n如何增加用户的参与感？交互式推荐来了！\n\u003c强化学习与推荐系统的强强联合\n\u003c蚂蚁金服核心技术：百亿特征实时推荐算法揭秘\n\u003c大众点评搜索基于知识图谱的深度学习排序实践\n双11商品怎样凑？\n于品类关系，虚拟类目如何建设？\n如何解决移动电商平台中的“伪曝光”？\n凑单这个技术活，阿里工程师怎么搞？\n可视化理解深度神经网络CTR预估模型\n人群优选算法模型，如何挖掘品牌潜客？\nJUMP： 一种点击和停留时长的协同预估器\n数十亿商品中，长尾和新品怎么找到新主人？\n为电商而生的知识图谱，如何感应用户需求？\n一种端到端的模型：基于异构内容流的动态排序\n火箭发射：点击率预估界的“神算子”是如何炼成的？\n为什么短视频会让人刷不停？背后也许用了这套技术\n机器如何“猜你喜欢”？深度学习模型在1688的应用实践\n让机器帮你做决策！强化学习在智能交互搜索的应用分享\n基于改进注意力循环控制门，品牌个性化排序升级系统来了\n基于快速GeoHash，如何实现海量商品与商圈的高效匹配？\n打破传统搜索排序，阿里首次提出商品间相互影响的全局排序法\nGithub项目推荐 | Sentence Classification - 神经网络句子分类(陈述/疑问/感叹/祈使)\nGithub项目推荐 | Chatito - 使用简单的DSL为AI聊天机器人、NLP任务、命名实体识别或文本分类模型生成数据集\n一文看懂虚假新闻检测（附数据集 \u0026 论文推荐）\n基于CNN和序列标注的对联机器人 | 附数据集 \u0026 开源代码\n搞定NLP领域的“变形金刚”！手把手教你用BERT进行多标签文本分类\n为了写春联，我用Transformer训练了一个“对穿肠”\nGithub项目推荐 | PAAG - 电子商务问答中的产品感知应答生成\n谷歌升级版Transformer官方解读：更大、更强，解决长文本问题（开源）\n贼好理解，这个项目教你如何用百行代码搞定各类NLP模型\n谷歌的机器翻译模型 Transformer，现在可以用来做任何事了\nCMU和谷歌联手放出XL号Transformer！提速1800倍 | 代码+预训练模型+超参数\n谷歌开源BERT不费吹灰之力轻松训练自然语言模型\n谷歌最强NLP模型BERT如约开源，12小时GitHub标星破1500，即将支持中文\n自然语言处理是如何工作的？一步步教你构建 NLP 流水线\n基于 Tensorflow eager 的文本生成，注意力，图像注释的完整代码\n自动文本摘要\n博客 | 一次LDA的项目实战(附GibbsLDA++代码解读）\n斯坦福发布重磅NLP工具包StanfordNLP，支持中文等53种语言\nFacebook开源增强版LASER库，包含93种语言工具包\n40个中文NLP词库\n北大开源了中文分词工具包，准确度远超Jieba，提供三个预训练模型\nFacebook开源新NLP框架：简化部署流程，大规模应用也OK\n深度学习的NLP工具\n资讯\nAI人必看！89页全网最全清华知识图谱报告\n对话清华NLP实验室刘知远：NLP搞事情少不了知识库与图神经网络\n谷歌、CMU重磅论文：Transformer升级版，评估速度提升超1800倍！\n请收好这份NLP热门词汇解读：预训练、Transformer、无监督机器翻译\nBERT霸榜问答任务，谷歌新基准模型缩小AI与人类差距50%\n横扫13项中文NLP任务：香侬科技提出汉语字形表征向量Glyce+田字格CNN\nTransformer在进化！谷歌大脑用架构搜索方法找到Evolved Transformer\n现有模型还「不懂」自然语言：20多位研究者谈NLP四大开放性问题\n放弃幻想，全面拥抱Transformer：自然语言处理三大特征抽取器（CNN/RNN/TF）比较\n史上最强NLP知识集合：知识结构、发展历程、导师名单\n不只有BERT！盘点2018年NLP令人激动的10大想法\n清华大学发布10大机器翻译学习必读论文清单 | 资源\n一文概述 2018 年深度学习 NLP 十大创新思路\n博客 | 斯坦福大学—自然语言处理中的深度学习（CS 224D notes-1）\nCMU课程上新：Neural Networks for NLP（18年视频课件放出）\n斯坦福大学2019年NLP课程上线，下周二开课 | 附PPT+视频\n时隔两年，斯坦福NLP标准公开课CS224N将再次开放视频\n这套GitHub 1300星的NLP课程即将完结，视频授课，在线答疑丨课程","data":"2019年01月07日 15:00:16"}
{"_id":{"$oid":"5d343b2f62f717dc0659b39b"},"title":"人工智能一键生成原创爆文","author":"小发猫","content":"自人工智能（AI）“小发猫写作”以来，机器生成原创进入了一个新的快速发展渠道，并开始引起广泛的社会关注。\n近日，美国微软公司宣布其首次开发的机器生成原创系统达到了人文专业水平，从原稿到仿写生成原创一般新闻，实现了自然语言处理的又一里程碑式突破。\n与此相关的另一个消息是，在2018年的谷歌亚洲论坛上，腾讯向会议的某些论坛提供了人工智能的免费解释。嘉宾的演讲通过人工智能一键生成原创爆文确定，并生成原创成原稿和仿写。字幕被筛选。结果，人工智能通过了“小发猫写作”，并且该网站暴露了许多问题和低级生成原创错误。\n\n即便如此，在人工智能生成原创技术发生迅速变化的时代，高校外语专业的教育工作者仍然需要思考一些问题：大学生成原创专业是否已经面临生存危机？高校如何改善现有的人才发展计划，以适应未来20年的行业发展？\n●机器生成原创尚未达到理解人类自然语言的水平\n在谈论人工智能生成原创之前，让我们先快速了解机器生成原创。\n首先，机器生成原创的优势在于交付速度快、可以在短时间内生成原创大量文本，无需传闻。其次，成本低，与专业人工生成原创相比，机器生成原创的单位成本相对较低。特别是在处理大量、术语时，更多、语法规范、技术信息没有个人感受，这一点尤为明显。\n机器生成原创的优点也体现在即时性和可控性上。机器生成原创不受时间和地点的限制。无需预约，可随时提供服务，流程简单快捷。在、生成原创上花费的总时间是高度可控的。在质量方面，机器生成原创的生成原创是一致的，技术术语是高度准确的。\n当然，目前的机器生成原创仍然存在瓶颈。虽然发展势头很快，但目前的生成原创质量仍然不尽如人意。导致这些瓶颈的因素不仅是技术上的，而且是语言上的哲学。因为人类语言在实际使用中很复杂。生成原创不是简单的字面意义转换，而是需要深入理解源文本、风格样式、语言风格、人际意义所涉及的语义深层结构。目前的机器生成原创还没有达到理解人类自然语言的水平。它还没有能够表达对、概念、文化的立体理解，这在文学生成原创中尤为明显。\n●人工智能生成原创对生成原创服务行业提出了挑战，就像无人司机一样\n但是，我们不能低估人工智能在生成原创领域的潜在和革命性意义。英国《自然》杂志近年来梳理了科技领域的十大突破，将人工智能列为首位。我们已经看到，机器生成原创系统经历了不断的演变和升级，从规则和例子到基于语料库和统计，再到当今基于人工智能，机器生成原创的“基于深度学习的神经网络机器生成原创”。与过去相比，生成原创质量得到了显着提高，并且变得越来越“人性化”。人工智能模拟人类意识和思维的信息处理过程，而深度学习技术模仿人脑神经系统的运作，并扩展传统人工神经网络的内置水平，使系统具有更复杂的学习行为。“基于深度学习的神经网络机器生成原创”已被谷歌使用、 Facebook、微软、 Apple、IBM、百度、有一条路、搜狗、分行大学新闻、腾讯等科技公司。该算法依赖于两种基本的神经网络架构，即递归神经网络和卷积神经网络，它们可以合成上下文上下文信息并通过时间递归或分层来加深句子。理解，完成句子的编码和解码，产生更多完整性、更高精度、更多逻辑、话语更流畅、阅读体验更友好的生成原创。\n但是，这些成就并非机器生成原创的终结。人工智能可以基于人工勘误的结果，学习更符合人类语言习惯，并通过云计算不断优化和升级生成原创能力。如果有一天，机器生成原创达到一定程度的“情感理解”，那并非完全不可能。\n在可预见的未来，随着人工智能技术的升级，机器生成原创将逐渐占据低端到低端口生成原创市场，生成原创精度要求不那么严格。人工智能生成原创将对现有的生成原创服务行业产生巨大影响，就像无人驾驶技术对专业驾驶员构成挑战一样。\n●机器生成原创和人工生成原创，将来会被放错地方\n那么，外语院校生成原创专业的毕业生是否真的面临“这些同时生成原创的人可能在未来几年没有工作的情况”？\n人工智能生成原创技术与主要语言服务提供商积累的语料库相结合，使生成原创服务便宜，便捷，而智能基础设施需要大规模的资金投入和技术积累，这将导致至少在低端生成原创。在市场上，现有的小型、个体生成原创公司正面临被淘汰的命运，而市场份额将逐渐集中在少数技术巨头手中。\n不难想象，一个只能掌握语言技能的写手无法与高质量的、高效率的低成本人工智能生成原创竞争，因为它的成本和效率，就像低技能一样英国人在十九世纪的工业革命中。体力劳动者无法与蒸汽机竞争。然而，正如工业革命同时创造了新的就业机会一样，人工智能生成原创必将为语言服务行业带来新的机遇。\n根据作者的观点，生成原创既是一种技巧，也是一种技巧；技术工作使机器完成，并解放人才来雕刻工艺。首先，人工智能生成原创是生成原创科学毕业生的综合学科。它为生成原创专业的毕业生开辟了新的职位。跨语言学的发展、数学、生成原创科学、计算机科学、脑神经科学等综合学科。无论是语料库还是“深度学习”还是“情感模拟”，语言学和生成原创研究的基础理论研究都需要作为学科支持。因此，高校外语专业应在“互联网+”的背景下与科技公司合作进行跨行业合作，实现语言资产共享，开展语料库建设，形成生产一体化，学习和研究，掌握核心技术。同时，高校还应修改现有的生成原创课程体系、，以完善相应的人工智能生成原创技术应用课程。\n其次，要加强生成原创专业的“专业化”。外语人才的人工智能生成原创提出了更高的专业要求。未来，人类生成原创的目标市场和机器生成原创的目标市场将越来越明显。、变得越来越清晰。在低端生成原创市场，机器生成原创将在未来占主导地位，而人工生成原创将专注于需要严格生成原创准确性的高端市场。机器生成原创和人工生成原创将占据市场的不同利基，实现错位竞争。\n例如，人们不会让机器生成原创重要的法律文件，也不会允许机器作为重要商务谈判的生成原创。人工生成原创将为差异化的市场需求提供差异化 ，完善的专业生成原创服务。许多领域的生成原创需要专业生成原创人员的操作。具有相关专业知识的特殊高级生成原创人员始终不可替代机器生成原创。因此，在高校生成原创课程体系中，有必要尝试将“医学生成原创”，“法律生成原创”，“商务生成原创”和“政府生成原创”等选修课程加入“外语+专业”，这样学生就可以根据自己的兴趣转移。教学，丰富了毕业后学生的职业选择。\n最后但并非最不重要的是，生成原创不仅是客观信息的转换和传播，也是人类交流的重要形式。生成原创不仅是技术，也是“人性”的技巧。从某种角度来看，机器生成原创与语言发展是冲突的，因为机器生成原创为了提高准确性，必然会消除语言模糊性，而语言丰富度、的表现力往往来自于这种模糊性。可以说，在中国经典生成原创的文学生成原创领域，机器生成原创从未能够取代人类，因为人文领域的这些生成原创本质上是人类无与伦比的理解和创造力的表达和实现。生成原创专业的培养方案应注重“人性化”，“人文”是未来人类生成原创的核心竞争力。","data":"2018年11月29日 22:59:58"}
{"_id":{"$oid":"5d343b2f62f717dc0659b39d"},"title":"Python 自然语言处理（基于Gensim）","author":"HuangZhang_123","content":"欢迎加入学习交流QQ群：657341423\nGensim是一款开源的第三方Python工具包，用于从原始的非结构化的文本中，无监督地学习到文本隐层的主题向量表达。它支持包括TF-IDF，LSA，LDA，和word2vec在内的多种主题模型算法，支持流式训练，并提供了诸如相似度计算，信息检索等一些常用任务的API接口。\n简单地说，Gensim主要处理文本数据，对文本数据进行建模挖掘。\n语料（Corpus）：一组原始文本的集合，用于无监督地训练文本主题的隐层结构。语料中不需要人工标注的附加信息。在Gensim中，Corpus通常是一个可迭代的对象（比如列表）。每一次迭代返回一个可用于表达文本对象的稀疏向量。\n向量（Vector）：由一组文本特征构成的列表。是一段文本在Gensim中的内部表达。\n稀疏向量（Sparse Vector）：通常，我们可以略去向量中多余的0元素。此时，向量中的每一个元素是一个(key, value)的tuple。\n模型（Model）：是一个抽象的术语。定义了两个向量空间的变换（即从文本的一种向量表达变换为另一种向量表达）。\n文件准备：\naa.txt。文件内容为某新闻报道\n\nfrom gensim import corpora, models, similarities import jieba import re from snownlp import SnowNLP # 读取文本内容 f = open('aa.txt','r',encoding='utf-8') text = f.read() f.close() # 分句 s = SnowNLP(text) text_list = s.sentences seg_list = [] # 循环句子列表，对每个句子做分词处理 for i in text_list: temp_list = jieba.cut(i,cut_all=False) results = re.sub('[（）：:？“”《》，。！()·、.\\d ]+', ' ', ' '.join(temp_list)) seg_list.append(results) # 将分词写入文件 f = open('data.txt','w',encoding='utf-8') f.write(' '.join(seg_list)) f.close() #…………………我是分割线………………………# # **********字典的使用********** # gensim的字典是将分词好的数据转换成gensim能处理的数据格式 seg_dict = [x.split(' ') for x in seg_list] dict1 = corpora.Dictionary(seg_dict,prune_at=2000000) print(dict1.token2id) # 手动添加字典 dict2 = corpora.Dictionary() dict2.token2id = {'computer': 0, 'human': 1, 'response': 2, 'survey': 3} print(dict2.token2id) # 合并字典 dict2 = corpora.Dictionary(seg_dict,prune_at=2000000) dict2_to_dict1 = dict1.merge_with(dict2) # 获取字典中某词语的词袋向量 new_doc = '生态环境 政府 非法' new_vec = dict1.doc2bow(new_doc.split()) print(new_vec) # [(14, 1), (22, 1), (66, 1)] -\u003e 14代表生态环境在字典dict1的ID，1代表出现次数 # 获取整个dict1的词袋向量 bow_corpus = [dict1.doc2bow(text) for text in seg_dict] print(bow_corpus) # **********字典的使用********** #…………………我是分割线………………………# #…………………我是分割线………………………# # **********模型的使用********** # 模型对象的初始化，实现词向量化 tfidf = models.TfidfModel(bow_corpus) # 计算new_vec的权重 string_tfidf = tfidf[new_vec] print(string_tfidf) # 基于Tf-Idf计算相似度，参考https://radimrehurek.com/gensim/tutorial.html index = similarities.SparseMatrixSimilarity(bow_corpus, num_features=10) sims = index[string_tfidf] print(sims) # 输出[(14, 0.5862218816946012), (22, 0.4809979876921243), (66, 0.6519086141926397)] # 14代表生态环境在字典dict1的ID，0.5862218816946012代表相似性分数 # ****建模**** # 参考https://radimrehurek.com/gensim/tut2.html # LSI建模，models.LsiModel(corpus=tfidf[bow_corpus], id2word=dict1, num_topics=50, chunksize=10000) # HDP建模，models.HdpModel(corpus=tfidf[bow_corpus], id2word=dict1,chunksize=10000) # RP建模，models.RpModel(corpus=tfidf[bow_corpus], id2word=dict1, num_topics=50) lda = models.LdaModel(corpus=tfidf[bow_corpus], id2word=dict1, num_topics=50, update_every=1, chunksize=10000, passes=1) for i in range(0, 3): print(lda.print_topics(i)[0]) # 利用模型获取文档的主题概率分布 doc_lda = lda[new_vec] print(doc_lda) # 根据模型计算相似度 # 参考https://radimrehurek.com/gensim/tut3.html index = similarities.MatrixSimilarity(bow_corpus) sims = index[new_vec] print(list(enumerate(sims))) # ****建模**** # **********模型的使用********** #…………………我是分割线………………………# #…………………我是分割线………………………# # **********word2vec的使用********** # 通过word2vec的“skip-gram和CBOW模型”生成深度学习的单词向量 # 读取已分词的文件 sentences = models.word2vec.LineSentence('data.txt') # 建立模型，实现词向量化，第一个参数是训练语料，min_count是小于该数的单词会被踢出，默认值为5；size是神经网络的隐藏层单元数，在保存的model.txt中会显示size维的向量值。默认是100。默认window=5 model = models.word2vec.Word2Vec(sentences, size=100, window=25, min_count=5, workers=4) # 根据语料，计算某个词的相关词列表 sim = model.wv.most_similar('生态环境', topn=10) # 计算一个词d（或者词表），使得该词的向量v(d)与v(a=\"政府\")-v(c=\"生态环境\")+v(b=\"街道\")最近 # sim = model.most_similar(positive=['政府','街道'],negative=['生态环境'], topn=10) for s in sim: print(\"word:%s,similar:%s \" %(s[0],s[1])) # 根据语料，计算两个词的相似度 / 相关程度 print(str(model.similarity('政府','生态环境'))) # 计算文本的相似度 similarity_matrix = model.wv.similarity_matrix(dict1) # MatrixSimilarity:指数相似性（密集与余弦距离）。 # SparseMatrixSimilarity:索引相似度（带余弦距离的稀疏）。 # SoftCosineSimilarity:指数相似性（具有软余弦距离）。 # WmdSimilarity:索引相似度（与字移动距离）。 index = similarities.SoftCosineSimilarity(bow_corpus, similarity_matrix, num_best=10) sims = index[dict1.doc2bow(new_doc.split())] print(sims) # 保存模型方法一 model.save(\"test_01.model\") # 保存模型方法二 # model.wv.save_word2vec_format(\"test_01.model.bin\",binary=True) # model= models.KeyedVectors.load_word2vec_format(\"test_01.model.bin\", binary=True) # **********word2vec的使用********** #…………………我是分割线………………………#\n参考资料：官方文档","data":"2018年05月15日 17:39:56"}
{"_id":{"$oid":"5d343b3062f717dc0659b39f"},"title":"自然语言处理和计算机科学的联系","author":"晚风胧月","content":"自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。自然语言处理是一门融语言学、计算机科学、数学于一体的科学。\n语言是人类区别于其他动物的本质特征，人类的多种智能都与语言有关，语言是人工智能的一个重要部分，甚至是核心部分。但是迄今为止，语法都限于分析一个独立的句子，上下文关系存在约束作用不能完全考虑，文章存在歧义的一系列问题得不到系统的解决；另一个方面就是，人们理解一个句子不是单凭语法就行，还有和生活阅历相关的大量知识，这些知识肯定不能全部储存在计算机里面，且对于不同的人也许会有不同的解释。因此，只有当计算机的储存量和运转速度大大提高才能广泛使用，但这也是如今有关自然语言处理的一个巨大难题。\n我的专业是信息与计算科学，一个结合数学和计算机的专业，看似数学和计算机并没有什么关系，其实不然。数学是一切人类文明的基础，数学的逻辑性很强，一步步的算法必须通过严谨的认证后往下算，一开始的编程也许很少用到数学，但是到写程序时就要用到高中数学，大学高等数学。计算机学习离不开数学，很多计算机学科的建立在数学模型的基础之上的，编程需要逻辑思维能力，而恰好逻辑思维能力是数学所培养。所以编程和数学有很大的关系。我们专业也会学到很多算法并且也要有自己能设计算法的能力，当然，本科学习并不会上升到这种高度，但随着学习的进一步加深，对于算法设计以达到优化模型优化算法的目的，现在本专业本科学习既是为了有一定的数学基础，是为了锻炼思维能力，能为今后的学习和发展添砖加瓦。我们专业结合了计算机和数学，虽暂时还没到机器学习和数据挖掘那种高度，但却为以后学习打下坚实基础，其实也可以就机器语言，数据挖掘和自然语言处理的关系来说 更加明显的对比出本专业和自然语言处理的关系，因为基本的机器学习和数据挖掘我们专业也会涉及其中一部分。机器学习就像内力一样，是一个武者的基础，而自然语言和数据挖掘就是。如果你内功深厚，招式什么的其实就很简单了。机器学习、数据挖掘、自然语言处理这三项并不是独立的选项，机器学习需要数据挖掘和自然语言处理的支撑，自然语言处理需要数据挖掘的支撑，数据挖掘需要大数据的支撑。最终所有的根源都要落实在大数据上，而这一切的顶点就是人工智能。","data":"2019年06月20日 18:46:22"}
{"_id":{"$oid":"5d343b3162f717dc0659b3a1"},"title":"人工智能语言 PROLOG 很智能","author":"ria4com","content":"人工智能语言是一类适应于人工智能和知识工程领域的、具有符号处理和逻辑推理能力的计算机程序设计语言,其中Prolog是当代最有影响的人工智能语言之一。\n一、什么是人工智能语言\n人工智能（AI）语言是一类适应于人工智能和知识工程领域的、具有符号处理和逻辑推理能力的计算机程序设计语言。能够用它来编写程序求解非数值计算、知识处理、推理、规划、决策等具有智能的各种复杂问题。\n典型的人工智能语言主要有LISP、Prolog、Smaltalk、C++等。\n一般来说，人工智能语言应具备如下特点：\n•具有符号处理能力（即非数值处理能力）；\n•适合于结构化程序设计，编程容易；\n•具有递归功能和回溯功能；\n•具有人机交互能力；\n•适合于推理；\n•既有把过程与说明式数据结构混合起来的能力，又有辨别数据、确定控制的模式匹配机制。\n人们可能会问，用人工智能语言解决问题与传统的方法有什么区别呢？\n传统方法通常把问题的全部知识以各种的模型表达在固定程序中，问题的求解完全在程序制导下按着预先安排好的步骤一步一步（逐条）执行。解决问题的思路与冯.诺依曼式计算机结构相吻合。当前大型数据库法、数学模型法、统计方法等都是严格结构化的方法。\n对于人工智能技术要解决的问题，往往无法把全部知识都体现在固定的程序中。通常需要建立一个知识库（包含事实和推理规则），程序根据环境和所给的输入信息以及所要解决的问题来决定自己的行动，所以它是在环境模式的制导下的推理过程。这种方法有极大的灵活性、对话能力、有自我解释能力和学习能力。这种方法对解决一些条件和目标不大明确或不完备，（即不能很好地形式化，不好描述）的非结构化问题比传统方法好，它通常采用启发式、试探法策略来解决问题。\n二、Prolog语言及其基本结构\nProlog是当代最有影响的人工智能语言之一，由于该语言很适合表达人的思维和推理规则，在自然语言理解、机器定理证明、专家系统等方面得到了广泛的应用，已经成为人工智能应用领域的强有力的开发语言。\n尽管Prolog语言有许多版本，但它们的核心部分都是一样的。Prolog的基本语句仅有三种，即事实、规则和目标三种类型的语句，且都用谓词表示，因而程序逻辑性强，文法简捷，清晰易懂。另一方面，Prolog是陈述性语言，一旦给它提交必要的事实和规则之后，Prolog就使用内部的演绎推理机制自动求解程序给定的目标，而不需要在程序中列出详细的求解步骤。\n１、事实\n事实用来说明一个问题中已知的对象和它们之间的关系。在Prolog程序中，事实由谓词名及用括号括起来的一个或几个对象组成。谓词和对象可由用户自己定义。\n例如，谓词likes(bill，book).\n是一个名为like的关系，表示对象bill和book之间有喜欢的关系。\n２、规则\n规则由几个互相有依赖性的简单句（谓词）组成，用来描述事实之间的依赖关系。从形式上看，规则由左边表示结论的后件谓词和右边表示条件的前提谓词组成。\n例如，规则 bird(X):-animal(X),has(X,feather).\n表示凡是动物并且有羽毛，那么它就是鸟。\n３、目标（问题）\n把事实和规则写进Prolog程序中后，就可以向Prolog询问有关问题的答案，询问的问题就是程序运行的目标。目标的结构与事实或规则相同，可以是一个简单的谓词，也可以是多个谓词的组合。目标分内、外两种，内部目标写在程序中，外部目标在程序运行时由用户手工键入。\n例如问题 ?-student(john).\n表示“john是学生吗？”\n三、Prolog程序的简单例子\n以下两个例子在Turbo Prolog 2.0环境下运行通过。\n[ 注：一个Turbo Prolog程序至少包括谓词段、子句段和目标段三项。目标可以包含在程序中，也可以在程序运行时给出。]\n例1 谁是john的朋友？\npredicates /*谓词段，对要用的谓词名和参数进行说明*/\nlikes(symbol, symbol)\nfriend(symbol, symbol)\nclauses /*子句段，存放所有的事实和规则*/\nlikes(bell,sports). /*前4行是事实*/\nlikes(mary,music).\nlikes(mary,sports).\nlikes(jane,smith).\nfriend(john,X):-likes(X,sports),likes(X,music). /*本行是规则*/\n当上述事实与规则输入计算机后，运行该程序，用户就可以进行询问，如输入目标：\nfriend(john,X)\n即询问john的朋友是谁,,这时计算机的运行结果为：\nX=mary （mary是john的朋友）\n1 Solution （得到了一个结果）\n程序运行界面如下图所示:\n例2 汉诺塔问题：\n有N个有孔的盘子，最初这些盘子都叠放在柱a上（如图1），要求将这N个盘子借助柱b从柱a移到柱c（如图2），移动时有以下限制：每次只能移动一个盘子；大盘不能放在小盘上。问如何移动？\n该问题可以采用递归法思想来求解,其源程序为:\npredicates /*谓词段*/\nhanoi(integer)\nmove(integer,symbol,symbol,symbol)\ninform(symbol,symbol).\nclauses /*子句段*/\nhanoi(N):-move(N,a,b,c).\nmove(1,A,_,C):-inform(A,C),!.\nmove(N,A,B,C):-N1=N-1,move(N1,A,C,B),\ninform(A,C),move(N1,B,A,C).\ninform(Loc1,Loc2):-nl,write(\"移动1个盘子从柱\" ,Loc1,\"到柱\",Loc2).\ngoal /*目标段，问移动3个盘子的方法*/\nhanoi(3).\n这个例子的目标包含在程序里面，因此运行时程序将直接输出所有结果。\n程序运行界面如下图所示：\n四、Prolog语言的常用版本\nProlog语言最早是由法国马赛大学的Colmerauer和他的研究小组于1972年研制成功。早期的Prolog版本都是解释型的，自1986年美国Borland公司推出编译型Prolog,即Turbo Prolog以后，Prolog便很快在PC机上流行起来。后来又经历了PDC PROLOG、Visual Prolog不同版本的发展。并行的逻辑语言也于80年代初开始研制，其中比较著名的有PARLOG、Concurrent PROLOG等。\n1、Turbo Prolog\n由美国Prolog开发中心（Prolog Development Center, PDC）1986年开发成功、Borland公司对外发行，其1.0，2.0，2.1版本取名为Turbo Prolog，主要在IBM PC系列计算机，MS-DOS环境下运行。\n2、PDC Prolog\n1990年后，PDC推出新的版本，更名为PDC Prolog 3.0，3.2，它把运行环境扩展到OS/2操作系统，并且向全世界发行。它的主要特点是:\n•速度快。编译及运行速度都很快，产生的代码非常紧凑。\n•用户界面友好。提供了图形化的集成开发环境。\n•提供了强有力的外部数据库系统。\n•提供了一个用PDC Prolog编写的Prolog解释起源代码。用户可以用它研究Prolog的内部机制，并创建自己的专用编程语言、推理机、专家系统外壳或程序接口。\n•提供了与其他语言（如C、Pascal、Fortran等）的接口。Prolog和其他语言可以相互调用对方的子程序。\n•具有强大的图形功能。支持Turbo C、Turbo Pascal同样的功能。\n3、Visual Prolog\nVisual Prolog是基于Prolog语言的可视化集成开发环境，是PDC推出的基于Windows环境的智能化编程工具。目前，Visual Prolog在美国、西欧、日本、加拿大、澳大利亚等国家和地区十分流行，是国际上研究和开发智能化应用的主流工具之一。\nVisual Prolog具有模式匹配、递归、回溯、对象机制、事实数据库和谓词库等强大功能。它包含构建大型应用程序所需要的一切特性：图形开发环境、编译器、连接器和调试器，支持模块化和面向对象程序设计，支持系统级编程、文件操作、字符串处理、位级运算、算术与逻辑运算，以及与其它编程语言的接口。\nVisual Prolog包含一个全部使用Visual Prolog语言写成的有效的开发环境，包含对话框、菜单、工具栏等编辑功能。\nVisual Prolog与SQL数据库系统、C++开发系统、以及Visual Basic、Delphi或Visual Age等编程语言一样，也可以用来轻松地开发各种应用。\nVisual Prolog软件的下载地址为：http://www.visual-prolog.com","data":"2014年07月15日 02:02:35"}
{"_id":{"$oid":"5d343b3162f717dc0659b3a4"},"title":"浅谈人工智能","author":"xyfu66","content":"浅谈人工智能\n人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。\n为什么要学人工智能\n学习范围广泛，内容多，开发平台高，就业机会多，发展方向多元化。\n怎么学习人工智能\nLinux系统、C/C++语言、python语言、计算机架构，图像处理、模式识别。\n人工智能研究内容\n人工智能涉及信息论、控制论、自动化、仿生学、生物学、心理学、数理逻辑、语言学、医学和哲学等多门学科。\n人工智能学科研究的主要内容包括：知识表示、自动推理和搜索方法、机器学习和知识获取、知识处理系统、自然语言理解、计算机视觉、智能机器人、自动程序设计等方面。","data":"2018年11月14日 13:44:29"}
{"_id":{"$oid":"5d343b3262f717dc0659b3a7"},"title":"自然语言处理与专业的资料收集","author":"AaLinxl","content":"自然语言处理是现代技术最重要的组成部分之一，自然语言是指汉语、英语、法语等人们日常使用的语言，是自然而然的随着人类社会发展演变而来的语言，而不是人造的语言，他是人类学习生活的重要工具。概括来说，自然语言是指人类社会约定俗成的，区别于人工语言，如设计程序的语言。自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。\n因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，所以它与语言学的研究有着密切的联系，但又有重要的区别。自然语言处理并不是一般地研究自然语言，而在于研制能有效地实现自然语言通信的计算机系统，特别是其中的软件系统。因而它是计算机科学的一部分。自然语言处理是计算机科学，人工智能，语言学关注计算机和人类语言之间的相互作用的领域。\n自然语言处理的基础是各类自然语言处理数据集，如面向文本分类研究的中英文新闻分类语料、以IG卡方等特征词选择方法生成的多维度ARFF格式中文VSM模型、万篇随机抽取论文中文DBLP资源、用于非监督中文分词算法的中文分词词库、UCI评价排序数据、带有初始化说明的情感分析数据集等。\n最早的自然语言理解方面的研究工作是机器翻译。1949年，美国人威弗首先提出了机器翻译设计方案。20世纪60年代，国外对机器翻译曾有大规模的研究工作，耗费了巨额费用，但人们当时显然是低估了自然语言的复杂性，语言处理的理论和技术均不成热，所以进展不大。主要的做法是存储两种语言的单词、短语对应译法的大辞典，翻译时一一对应，技术上只是调整语言的同条顺序。但日常生活中语言的翻译远不是如此简单，很多时候还要参考某句话前后的意思。\n自然语言处理的具体表现形式包括机器翻译、文本摘要、文本分类、文本校对、信息抽取、语音合成、语音识别等。机器翻译是指运用机器，通过特定的计算机程序将一种书写形式 或声音形式的自然语言，翻译成另一种书写形式或声音形式的自然语言。语音翻译可能是目前机器翻译中比较富有创新意思的领域，搜狗推出的机器同传 技术主要在会议场景出现，演讲者的语音实时转换成文本，并且进行同步翻译，低延迟显示 翻译结果，希望能够取代人工同传，实现不同语言人们低成本的有效交流。信息检索是从相关文档集合中查找用户所需信息的过程。信息检索的基本原理是将用户输入的检索关键词与数据库 中的标引词进行对比，当二者匹配成功时，检索成功。自动问答是指利用计算机自动回答用户所提出的问题以满足用户知识需求的任务。自动问答系统在回答用户问题时，首先要正确理解用户所提出的问题，抽取其中关键的信息，在已有的语料库或者知识库中进行检索、匹配，将获取的答案反馈给用户。","data":"2019年06月17日 16:10:03"}
{"_id":{"$oid":"5d343b3462f717dc0659b3ab"},"title":"刘小瑜 自然语言处理与专业的关系","author":"Takenaka Kumiko","content":"自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。无论实现自然语言理解，还是自然语言生成，都远不如人们原来想象的那么简单，而是十分困难的。从现有的理论和技术现状看，通用的、高质量的自然语言处理系统，仍然是较长期的努力目标。\n本人的专业是电气工程及其自动化。其中电气工程的自动化就涉及到自然语言处理，如果在不久的将来能够将自然语言处理做的更加成熟，计算机能够通过自然语言处理实现真正意义上的人机交互，那么便可以将这一成果投入到电气工程的智能化中，提高人们的工作效率和生产效率。","data":"2019年06月18日 20:42:24"}
{"_id":{"$oid":"5d343b3562f717dc0659b3ae"},"title":"人工智能简介","author":"liubin5620","content":"人工智能简介\n人工智能[Artificial Intelligence],英文简称AI,它是计算机科学的一个分支，了解和探索智能的实质，并以人类智能相似的方式做出反应的智能机器，人工智能主要研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等，这是一门极具挑战的科学，从事这项工作的人必须懂得计算机知识。\n它是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。人工智能在计算机学科领域可以细分为五大学科。\n一、机器学习\n机器学习(Machine Learning, ML)是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。\n它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演绎。\n二、人机对话\n人机对话是计算机的一种工作方式，即计算机操作员或用户与计算机之间，通过控制台或终端显示屏幕，以对话方式进行工作。操作员可用命令或命令过程告诉计算机执行某一任务。\n\n三、深度学习\n深度学习是机器学习研究中的一个新的领域，其动机在于建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，例如图像，声音和文本。\n\n四、图像处理\n用计算机对图像进行分析，以达到所需结果的技术。\n\n五、网络爬虫\n网络爬虫是一个自动提取网页的程序，它为搜索引擎从万维网上下载网页，是搜索引擎的重要组成。","data":"2018年01月02日 15:17:11"}
{"_id":{"$oid":"5d343b3562f717dc0659b3b0"},"title":"机器学习、深度学习、计算机视觉、自然语言处理及应用案例——干货分享（持续更新......）","author":"jason_ql","content":"机器学习、深度学习、计算机视觉、自然语言处理及应用案例——干货分享（持续更新……）\nauthor@jason_ql\nhttp://blog.csdn.net/lql0716\nGitChat提问码：\n\n1、机器学习/深度学习\n1.1 对抗生成网络GAN\n【2017.04.21】\n对抗生成网络GAN变种大集合\n【链接】\n资源 | 生成对抗网络及其变体的论文汇总\n【链接】\n生成对抗网络(GAN)图片编辑\n【链接】\nCycleGAN失败案例\n【链接】\n【2017.04.22】\n用条件生成对抗网络玩转中文书法\n【链接】\n《Gang of GANs: Generative Adversarial Networks with Maximum Margin Ranking》F Juefei-Xu, V N Boddeti, M Savvides [CMU \u0026 Michigan State University] (2017)\n【链接】\n【2017.04.23】\nTP-GAN 让图像生成再获突破，根据单一侧脸生成正面逼真人脸\n【链接】【GitHub】\n【2017.04.26】\n【对抗生成网络GAN教程】\n《Tutorial on GANs》by Adit Deshpande\n【链接】【GitHub】\n【2017.05.07】\n【GAN相关资源与实现】’Resources and Implementations of Generative Adversarial Nets: GAN, DCGAN, WGAN, CGAN, InfoGAN’ by YadiraF\n【链接】【GitHub】\n【PyTorch实现的CoGAN】《Coupled Generative Adversarial Networks》M Liu, O Tuzel [Mitsubishi Electric Research Labs (MERL)] (2016)\n【链接】【GitHub】\n【利用CGAN生成Sketch漫画】《Auto-painter: Cartoon Image Generation from Sketch by Using Conditional Generative Adversarial Networks》Y Liu, Z Qin, Z Luo, H Wang [Beihang University \u0026 Samsung Telecommunication Research Institute] (2017)\n【链接】【GitHub】\n《Adversarial Feature Learning》J Donahue, P Krähenbühl, T Darrell [UC Berkeley]\n【链接】【GitHub】\n【PyTorch实现的DCGAN、pix2pix、DiscoGAN、CycleGAN、BEGAN VAE、Neural Style Transfer、Char RNN等】’Paper Implementations - Use PyTorch to implement some classic frameworks’ by SunshineAtNoon\n【链接】【GitHub】\n【GAN画风迁移】《Generative Adversarial Networks for Style Transfer (LIVE) - YouTube》by Siraj Raval\n【链接】【GitHub】【video】\n【2017.05.08】\n生成对抗网络（GAN）研究年度进展评述\n【链接】【GitHub】\n【对抗生成网络(Gan)深入研究(文献/教程/模型/框架/库等)】《Delving deep into GANs》by Grigorios Kalliatakis\n【链接】【GitHub】\n【对抗式机器翻译】《Adversarial Neural Machine Translation》L Wu, Y Xia, L Zhao, F Tian, T Qin, J Lai, T Liu [Sun Yat-sen University \u0026 University of Science and Technology of China \u0026 Microsoft Research Asia] (2017)\n【链接】【GitHub】\n【CycleGAN生成模型：熊变熊猫】’Models generated by CycleGAN’ by Tatsuya\n【链接】【GitHub】\n【对抗生成网络(GAN)】《Generative Adversarial Networks (LIVE) - YouTube》by Siraj Raval\n【链接】【GitHub】【video】\n【Keras实现的ACGAN/DCGAN】’Implementation of some basic GAN architectures in Keras’ by Batchu Venkat Vishal\n【链接】【GitHub】\n【2017.05.09】\n【策略梯度SeqGAN】《SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient》L Yu, W Zhang, J Wang, Y Yu [Shanghai Jiao Tong University \u0026 University College London] (2016)\n【链接】【GitHub】\n【2017.05.10】\n《Improved Training of Wasserstein GANs》I Gulrajani, F Ahmed, M Arjovsky, V Dumoulin, A Courville [Montreal Institute for Learning Algorithms \u0026 Courant Institute of Mathematical Sciences] (2017)\n【链接】【GitHub】【GitHub2】\n《Geometric GAN》J H Lim, J C Ye [ETRI \u0026 KAIST] (2017)\n【链接】【GitHub】\n【PyTorch实现的CycleGAN/SGAN跨域迁移(MNIST-to-SVHN \u0026 SVHN-to-MNIST)】’PyTorch Implementation of CycleGAN and SGAN for Domain Transfer (Minimal)’ by yunjey GitHub:\n【链接】【GitHub】\n1.2 神经网络\n【2017.04.24】\n如何用PyTorch实现递归神经网络？\n【链接】【GitHub】\n【2017.04.25】\n一个基于TensorFlow的简单故事生成案例：带你了解LSTM\n【链接】【GitHub】\n【2017.05.07】\n深度学习10大框架对比分析\n【链接】【GitHub】\n深度学习之CNN卷积神经网络\n【链接】【GitHub】\n【Keras教程：Python深度学习】《Keras Tutorial: Deep Learning in Python》by Karlijn Willems\n【链接】【GitHub】\nTensorFlow 官方解读：如何在多系统和网络拓扑中构建高性能模型\n【链接】【GitHub】\n从自编码器到生成对抗网络：一文纵览无监督学习研究现状\n【链接】【GitHub】\n《Residual Attention Network for Image Classification》F Wang, M Jiang, C Qian, S Yang, C Li, H Zhang, X Wang, X Tang [SenseTime Group Limited \u0026 Tsinghua University \u0026 The Chinese University of Hong Kong] (2017)\n【链接】【GitHub】\n-【基于OpenAI Gym/Tensorflow/Keras的增强学习实验平台】’OpenAI Lab - An experimentation system for Reinforcement Learning using OpenAI Gym, Tensorflow, and Keras.’ by Wah Loon Keng\n【链接】【GitHub】\n【基于生成卷积网络的潜在指纹重建】《Generative Convolutional Networks for Latent Fingerprint Reconstruction》J Svoboda, F Monti, M M. Bronstein [USI Lugano] (2017)\n【链接】【GitHub】\n【TensorFlow入门代码集锦】’tensorflow-resources - Curated Tensorflow code resources to help you get started’ by Skcript\n【链接】【GitHub】\n入门级攻略：机器学习 VS. 深度学习\n【链接】【GitHub】\n《Gabor Convolutional Networks》S Luan, B Zhang, C Chen, X Cao, J Han, J Liu [Beihang University \u0026 University of Central Florida Orlando \u0026 Northumbria University \u0026 Huawei Company] (2017)\n【链接】【GitHub】\nTensorFlow基准：图像分类模型在各大平台的测试研究\n【链接】【GitHub】\n谷歌开源深度学习街景文字识别模型：让地图随世界实时更新\n【链接】【GitHub】\n《Geometric deep learning: going beyond Euclidean data》M M. Bronstein, J Bruna, Y LeCun, A Szlam, P Vandergheynst [USI Lugano \u0026 NYU \u0026 Facebook AI Research] (2016)\n【链接】【GitHub】\n【利用强化学习设计神经网络架构】《Designing Neural Network Architectures using Reinforcement Learning》B Baker, O Gupta, N Naik, R Raskar [MIT] (2016)\n【链接】【GitHub】\n【神经网络：三万英尺高空纵览入门】《Neural Networks : A 30,000 Feet View for Beginners | Learn OpenCV》by Satya Mallick\n【链接】【GitHub】\nTop100论文导读：深入理解卷积神经网络CNN（Part Ⅰ）\n【链接】【GitHub】\nTop100论文导读：深入理解卷积神经网络CNN（Part Ⅱ）\n【链接】【GitHub】\n-【深度神经网络权值初始化的研究】《On weight initialization in deep neural networks》S K Kumar (2017)\n【链接】【GitHub】\n【2017.05.08】\n【提升结构化特征嵌入深度度量学习】《Deep Metric Learning via Lifted Structured Feature Embedding》H Oh Song, Y Xiang, S Jegelka, S Savarese (2016)\n【链接】【GitHub】\n【图的深度特征学习】《Deep Feature Learning for Graphs》R A. Rossi, R Zhou, N K. Ahmed [Palo Alto Research Center (Xerox\nPARC) \u0026 Intel Labs] (2017)\n【链接】【GitHub】\n【用于性能分析、模型优化的神经网络生成器】’Perceptron - A flexible artificial neural network builder to analysis performance, and optimise the best model.’ by Caspar Wylie\n【链接】【GitHub】\n【TensorFlow最佳实践之文件、文件夹与模型架构实用建议】《TensorFlow: A proposal of good practices for files, folders and models architecture》by Morgan\n【链接】【GitHub】\n【带有快速局部滤波的图CNN】《Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering》M Defferrard, X Bresson, P Vandergheynst [EPFL] (2016)\n【链接】【GitHub】\n【(Tensorflow/TFLearn)RNN命名实体识别】“Named Entity Recognition using Recurrent Neural Networks in Tensorflow and TFLearn” by Dhwaj Raj\n【链接】【GitHub】\n【深度学习的局限性】《Failures of Deep Learning》S Shalev-Shwartz, O Shamir, S Shammah [The Hebrew University \u0026 Weizmann Institute] (2017)\n【链接】【GitHub】【video】\n【基于矩阵乘法的并行多通道卷积】《Parallel Multi Channel Convolution using General Matrix Multiplication》A Vasudevan, A Anderson, D Gregg [Trinity College Dublin] (2017)\n【链接】【GitHub】\n【在手机上进行深度学习训练】《Migrate Deep Learning Training onto Mobile Devices!》by Saman BigManborn\n【链接】【GitHub】\n【TensorFlow实现的RNN(LSTM)序列预测】’tensorflow-lstm-regression - Sequence prediction using recurrent neural networks(LSTM) with TensorFlow’ by mouradmourafiq\n【链接】【GitHub】\n【TensorFlow 1.1.0发布】”TensorFlow 1.1.0 Released”\n【链接】【GitHub】\n【CNN到图结构数据的推广】《A Generalization of Convolutional Neural Networks to Graph-Structured Data》Y Hechtlinger, P Chakravarti, J Qin [CMU] (2017)\n【链接】【GitHub】\nMomenta研发总监任少卿：From Faster R-CNN to Mask R-CNN\n【链接】【GitHub】\n《Deep Multitask Learning for Semantic Dependency Parsing》H Peng, S Thomson, N A. Smith [CMU] (2017)\n【链接】【GitHub】\n【利用整流单元稀疏性加快卷积神经网络】《Speeding up Convolutional Neural Networks By Exploiting the Sparsity of Rectifier Units》S Shi, X Chu [Hong Kong Baptist University] (2017)\n【链接】【GitHub】\n【深度学习之CNN卷积神经网络】《Deep Learning #2: Convolutional Neural Networks》by Rutger Ruizendaal\n【链接】【GitHub】\n【PyTorch试炼场：提供各主流预训练模型】’pytorch-playground - Base pretrained model and datasets in pytorch (MNIST, SVHN, CIFAR10, CIFAR100, STL10, AlexNet, VGG16, VGG19, ResNet, Inception, SqueezeNet)’ by Aaron Chen\n【链接】【GitHub】\n从自编码器到生成对抗网络：一文纵览无监督学习研究现状\n【链接】【GitHub】\n【2017.05.09】\nLearning Deep Learning with Keras\n【链接】【GitHub】\n【TensorFlow生成模型库】’A Library for Generative Models’\n【链接】【GitHub】\n【深度学习的过去、现在和未来】《Deep Learning – Past, Present, and Future》by Henry H. Eckerson\n【链接】【GitHub】\n正在涌现的新型神经网络模型：优于生成对抗网络\n【链接】【GitHub】\n【贝叶斯深度学习文献列表】’A curated list of resources dedicated to bayesian deep learning’ by Rabindra Nath Nandi\n【链接】【GitHub】\n【面向推荐系统的深度学习文献列表】’Deep-Learning-for-Recommendation-Systems - Deep Learning based articles , paper and repositories for Recommender Systems’ by Rabindra Nath Nandi\n【链接】【GitHub】\n【2017.05.10】\n【深度学习职位面试经验分享】《My deep learning job interview experience sharing》by Justin Ho\n【链接】【GitHub】\n《Convolutional Sequence to Sequence Learning》J Gehring, M Auli, D Grangier, D Yarats, Y N. Dauphin [Facebook AI Research] (2017)\n【链接】【GitHub】\n【VGG19的TensorFlow实现/详解】’VGG19_with_tensorflow - An easy implement of VGG19 with tensorflow, which has a detailed explanation.’ by Jipeng Huang\n【链接】【GitHub】\n【Keras实现的深度聚类】“Keras implementation of Deep Clustering paper” by Eduardo Silva\n【链接】【GitHub】\n1.3 机器学习\n【2017.05.07】\n【无监督学习纵览】《Navigating the Unsupervised Learning Landscape》by Eugenio Culurciello\n【链接】【GitHub】\n【(Python)机器学习导论课程资料】’Materials for the “Introduction to Machine Learning” class’ by Andreas Mueller\n【链接】【GitHub】\n【Newton ADMM快速准平滑牛顿法】’A Newton ADMM based solver for Cone programming.’\n【链接】【GitHub】\n【超大规模机器学习工具集MaTEx】’Machine Learning Toolkit for Extreme Scale (MaTEx) - a collection of high performance parallel machine learning and data mining (MLDM) algorithms, targeted for desktops, supercomputers and cloud computing systems’\n【链接】【GitHub】\n关于迁移学习的一些资料\n【链接】【GitHub】\n《Clustering with Adaptive Structure Learning: A Kernel Approach》Z Kang, C Peng, Q Cheng [Southern Illinois University] (2017)\n【链接】【GitHub】\n【(R)稀疏贝叶斯网络学习】’sparsebn - Software for learning sparse Bayesian networks’ by Bryon Aragam\n【链接】【GitHub】\n【Node.js机器学习/自然语言处理/情感分析工具包】’salient - Machine Learning, Natural Language Processing and Sentiment Analysis Toolkit for Node.js’ by Thomas Holloway\n【链接】【GitHub】\nExplaining the Success of AdaBoost and Random Forests as Interpolating Classifiers\n【链接】【GitHub】\n机器学习中容易犯下的错\n【链接】【GitHub】\n【2017.05.08】\n【(C/C++ and MATLAB/Octave)互信息函数工具箱】’MIToolbox - Mutual Information functions for C and MATLAB’ by Adam Pocock\n【链接】【GitHub】\n【Criteo 1TB数据集上多机器学习算法Benchmark】’Benchmark of different ML algorithms on Criteo 1TB dataset’ by Rambler Digital Solutions\n【链接】【GitHub】\n机器学习十大常用算法\n【链接】【GitHub】\n【加速随机梯度下降】《Accelerating Stochastic Gradient Descent》P Jain, S M. Kakade, R Kidambi, P Netrapalli, A Sidford [Microsoft Research \u0026 University of Washington \u0026 Stanford University] (2017)\n【链接】【GitHub】\n【(C++)大规模稀疏矩阵分解包】“LIBMF - library for large-scale sparse matrix factorization” by cjlin1\n【链接】【GitHub】\n【(C/Python/Matlab)求解大规模正则线性分类与回归的简单包】“LIBLINEAR - simple package for solving large-scale regularized linear classification and regression” by cjlin1\n【链接】【GitHub】\n【批量归一化(Batch Norm)概述】《Appendix: A Batch Norm Overview》by alexirpan\n【链接】【GitHub】\n【2017.05.09】\n谱聚类\n【链接】【GitHub】\n【2017.05.10】\n【学习非极大值抑制】《Learning non-maximum suppression》J Hosang, R Benenson, B Schiele [Max Planck Institut für Informatik] (2017)\n【链接】【GitHub】\n【(Python)机器学习工作流框架】’AlphaPy - Machine Learning Pipeline for Python’ by ScottFree Analytics\n【链接】【GitHub】\n【如何解释机器学习模型和结果】《Ideas on interpreting machine learning | O’Reilly Media》by Patrick HallWen Phan, SriSatish Ambati\n【链接】【GitHub】\n2、计算机视觉\n【2017.04.21】\nOpenCV/Python/dlib人脸关键点实时标定\n【paper】【GitHub】\n【2017.04.22】\n【高效的卷积神经网络在手机中的应用】MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\n【paper】【GitHub】\n【生成式人脸补全】《Generative Face Completion》Y Li, S Liu, J Yang, M-H Yang [Univerisity of California, Merced \u0026 Adobe Research] (2017)\n【paper】【GitHub】\n《Computer Vision for Autonomous Vehicles: Problems, Datasets and State-of-the-Art》J Janai, F Güney, A Behl, A Geiger [Max Planck Institute for Intelligent Systems \u0026 ETH Zurich] (2017)\n【paper】【GitHub】\n《Tracking the Trackers: An Analysis of the State of the Art in Multiple Object Tracking》L Leal-Taixé, A Milan, K Schindler, D Cremers, I Reid, S Roth [Technical University Munich \u0026 University of Adelaide \u0026 ETH Zurich \u0026 TU Darmstadt] (2017)《译：多目标追踪的现状分析》\n【paper】【GitHub】\n《CNN-SLAM: Real-time dense monocular SLAM with learned depth prediction》K Tateno, F Tombari, I Laina, N Navab [CAMP - TU Munich] (2017)\n【paper】【GitHub】\n《Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields》Z Cao, T Simon, S Wei, Y Sheikh [CMU] (2016)《译：基于PAF的实时二维姿态估计》\n【paper】【GitHub】\n《Virtual to Real Reinforcement Learning for Autonomous Driving》Y You, X Pan, Z Wang, C Lu [Shanghai Jiao Tong University \u0026 UC Berkeley \u0026 Tsinghua University] (2017)\n【paper】【GitHub】\n《Semantic3D.net: A new Large-scale Point Cloud Classification Benchmark》T Hackel, N Savinov, L Ladicky, J D. Wegner, K Schindler, M Pollefeys [ETH Zurich] (2017)\n【paper】【GitHub】\n《Learning Video Object Segmentation with Visual Memory》P Tokmakov, K Alahari, C Schmid [Inria] (2017)\n【paper】【GitHub】\n【2017.04.23】\n《A Brief History of CNNs in Image Segmentation: From R-CNN to Mask R-CNN》by Dhruv Parthasarathy\n【paper】【GitHub】\n《Stacked Hourglass Networks for Human Pose Estimation》A Newell, K Yang, J Deng [University of Michigan] (2016)\n【paper】【GitHub】\n自动驾驶计算机视觉研究综述：难题、数据集与前沿成果（附67页论文下载）\n【paper】【GitHub】\n谷歌推出最新“手机版”视觉应用的卷积神经网络—MobileNets\n【paper】【GitHub】\n《Deep Learning for Photo Editing》by Malte Baumann\n【paper】【GitHub】\n【2017.04.24】\nTensorFlow Implementation of conditional variational auto-encoder (CVAE) for MNIST by hwalsuklee\n【paper】【GitHub】\n【2017.04.26】\n【单目视频深度帧间运动估计无监督学习框架】’SfMLearner - An unsupervised learning framework for depth and ego-motion estimation from monocular videos’ by T Zhou\n【paper】【GitHub】\n“U-Nets(Caffe)”\n【paper】【GitHub】\n《U-Net: Convolutional Networks for Biomedical Image Segmentation》(2015)\n【paper】【GitHub】\n3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation\n【paper】【GitHub】\n【2017.05.07】\n【(C++/Matlab)视频/图片序列人脸标定】’Find Face Landmarks - C++ \\ Matlab library for finding face landmarks and bounding boxes in video\\image sequences.’ by Yuval Nirkin\n【paper】【GitHub】\n【(Keras)UNET图像分割】’ZF_UNET_224 Pretrained Model - Modification of convolutional neural net “UNET” for image segmentation in Keras framework’ by ZFTurbo\n【paper】【GitHub】\n【复杂条件下的深度人脸分割】”Deep face segmentation in extremely hard conditions” by Yuval Nirkin\n【paper】【GitHub】\n【基于单目RGB图像的实时3D人体姿态估计】《VNect: Real-time 3D Human Pose Estimation with a Single RGB Camera》D Mehta, S Sridhar, O Sotnychenko… [Max Planck Institute for Informatics \u0026 Universidad Rey Juan Carlos] (2017)\n【paper】【paper2】【GitHub】\n【衣服检测与识别】《DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations》Z Liu, P Luo, S Qiu, X Wang, X Tang (CVPR 2016)\n【paper】\n【paper2】【GitHub】\nSLAM 学习与开发经验分享\n【paper】【GitHub】\n【大规模街道级图片(分割)数据集】《Releasing the World’s Largest Street-level Imagery Dataset for Teaching Machines to See》by Peter Kontschieder\n【paper】【GitHub】【dataset】\n【基于深度增强学习的交叉路口车辆自动导航】《Navigating Intersections with Autonomous Vehicles using Deep Reinforcement Learning》D Isele, A Cosgun, K Subramanian, K Fujimura [University of Pennsylvania \u0026 Honda Research Institute \u0026 Georgia Institute of Technology] (2017)\n【paper】【GitHub】\n十分钟看懂图像语义分割技术\n【paper】【GitHub】\n【(C++)实时多人关键点检测】’OpenPose: A Real-Time Multi-Person Keypoint Detection And Multi-Threading C++ Library’\n【paper】【GitHub】\n计算机视觉、机器学习相关领域论文和源代码大集合\n【paper】【GitHub】\n【(Tensorflow)RPN+人体检测】’RPNplus - RPN+(Tensorflow) for people detection’ by Shiyu Huang\n【paper】【GitHub】\n【(C++/OpenCV3)实时可变人脸追踪】’Real time deformable face tracking in C++ with OpenCV 3.’ by Kyle McDonald\n【paper】【GitHub】\n【图片快速标记】《How to Label Images Quickly 》by Pete Warden\n【paper】【paper2】【GitHub】\n【基于深度图像类比的视觉要素迁移】《Visual Attribute Transfer through Deep Image Analogy》J Liao, Y Yao, L Yuan, G Hua, S B Kang [Microsoft Research \u0026 Shanghai Jiao Tong University] (2017)\n【paper】【GitHub】\n【基于深度学习的质谱成像中的肿瘤分类】《Deep Learning for Tumor Classification in Imaging Mass Spectrometry》J Behrmann, C Etmann, T Boskamp, R Casadonte, J Kriegsmann, P Maass [University of Bremen \u0026 Proteopath GmbH] (2017)\n【paper】【link2】【GitHub】\n【Andorid手机上基于TensorFlow的人体行为识别】《Deploying Tensorflow model on Andorid device for Human Activity Recognition》by Aaqib Saeed\n【paper】【paper2】【GitHub】\n【TensorFlow图像自动描述】《Caption this, with TensorFlow | O’Reilly Media》by Raul Puri, Daniel Ricciardelli\n【paper】【paper2】【GitHub】\n【基于CNN (InceptionV1) + STFT的Kaggle鲸鱼检测竞赛方案】’CNN (InceptionV1) + STFT based Whale Detection Algorithm - A whale detector design for the Kaggle whale-detector challenge!’ by Tarin Ziyaee\n【paper】【GitHub】\n【TensorFlow实现的摄像头pix2pix图图转换】’webcam-pix2pix-Tensorflow - Source code and pretrained model for webcam pix2pix’ by Memo Akten\n【paper】【GitHub】\n【图像分类的大规模进化】《Large-Scale Evolution of Image Classifiers》E Real, S Moore, A Selle, S Saxena, Y L Suematsu, Q Le, A Kurakin [Google Brain \u0026 Google Research] (2017)\n【paper】【paper2】【GitHub】\n【2017.05.08】\n人脸检测与识别的趋势和分析\n【paper】【GitHub】\n【全局/局部一致图像补全】《Globally and Locally Consistent Image Completion》S Iizuka, E Simo-Serra, H Ishikawa (2017)\n【paper】【GitHub】\n【基于CNN的面部表情识别】《Convolutional Neural Networks for Facial Expression Recognition》S Alizadeh, A Fazel [Stanford University] (2017)\n【paper】【GitHub】\n计算机视觉识别简史：从 AlexNet、ResNet 到 Mask RCNN\n【paper】【GitHub】\n【脸部识别与聚类】《Face Identification and Clustering》A Dhingra [The State University of New Jersey] (2017)\n【paper】【GitHub】\n【(TensorFlow)通用U-Net图像分割】’Tensorflow Unet - Generic U-Net Tensorflow implementation for image segmentation’ by Joel Akeret\n【paper】【GitHub】\n【深度学习介绍之文本图像生成】《How to Convert Text to Images - Intro to Deep Learning #16 - YouTube》by Siraj Raval\n【paper】【GitHub】\n【一个深度神经网络如何对自动驾驶做端到端的训练】《Explaining How a Deep Neural Network Trained with End-to-End Learning Steers a Car》M Bojarski, P Yeres, A Choromanska, K Choromanski, B Firner, L Jackel, U Muller [NVIDIA Corporation \u0026 New York University \u0026 Google Research] (2017)\n【paper】【GitHub】\n【基于深度卷积网络的动态场景关节语义与运动分割】《Joint Semantic and Motion Segmentation for dynamic scenes using Deep Convolutional Networks》N Haque, N D Reddy, K. M Krishna [International Institute of Information Technology \u0026 Max Planck Institute For Intelligent Systems] (2017)\n【paper】【GitHub】\n【高分辨率图像的实时语义分割】《ICNet for Real-Time Semantic Segmentation on High-Resolution Images》H Zhao, X Qi, X Shen, J Shi, J Jia [The Chinese University of Hong Kong \u0026 SenseTime Group Limited] (2017)\n【paper】【GitHub】【GitHub2】【video】\n【深度学习应用到语义分割的综述】《A Review on Deep Learning Techniques Applied to Semantic Segmentation》A Garcia-Garcia, S Orts-Escolano, S Oprea, V Villena-Martinez, J Garcia-Rodriguez [University of Alicante] (2017)\n【paper】【GitHub】\n【医学图像的深度迁移学习的原理】《Understanding the Mechanisms of Deep Transfer Learning for Medical Images》H Ravishankar, P Sudhakar, R Venkataramani, S Thiruvenkadam, P Annangi, N Babu, V Vaidya [GE Global Research] (2017)\n【paper】【GitHub】\n【(Torch)基于循环一致对抗网络的非配对图到图翻译】\n【paper】【GitHub】\n【深度网络光流估计的演化】《FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks》E Ilg, N Mayer, T Saikia, M Keuper, A Dosovitskiy, T Brox [University of Freiburg] (2016)\n【paper】【GitHub】【video】\n【基于p-RNN的目标实例标注】《Annotating Object Instances with a Polygon-RNN》L Castrejon, K Kundu, R Urtasun, S Fidler [University of Toronto] (2017)\n【paper】【GitHub】\n《Dataset Augmentation for Pose and Lighting Invariant Face Recognition》D Crispell, O Biris, N Crosswhite, J Byrne, J L. Mundy [Vision Systems, Inc \u0026 Systems and Technology Research] (2017)\n【paper】【GitHub】\n【人脸的分割、交换与感知】《On Face Segmentation, Face Swapping, and Face Perception》Y Nirkin, I Masi, A T Tran, T Hassner, G Medioni [The Open University of Israel \u0026 USC] (2017)\n【paper】【GitHub】\n【面向视频运动估计的几何感知神经网络SfM-Net】《SfM-Net: Learning of Structure and Motion from Video》S Vijayanarasimhan, S Ricco, C Schmid, R Sukthankar, K Fragkiadaki [Google \u0026 Indri \u0026 CMU] (2017)\n【paper】【GitHub】\n【基于深度自学习的弱监督目标定位】《Deep Self-Taught Learning for Weakly Supervised Object Localization》Z Jie, Y Wei, X Jin, J Feng, W Liu [Tencent AI Lab \u0026 National University of Singapore] (2017)\n【paper】【GitHub】\n【单个图像的手部关键点检测】《Hand Keypoint Detection in Single Images using Multiview Bootstrapping》T Simon, H Joo, I Matthews, Y Sheikh [CMU] (2017)\n【paper】【GitHub】\n《Hierarchical 3D fully convolutional networks for multi-organ segmentation》H R. Roth, H Oda, Y Hayashi, M Oda, N Shimizu, M Fujiwara, K Misawa, K Mori [Nagoya University \u0026 Nagoya University Graduate School of Medicine \u0026 Aichi Cancer Center] (2017)\n【paper】【GitHub】\n《Towards Large-Pose Face Frontalization in the Wild》X Yin, X Yu, K Sohn, X Liu, M Chandraker [Michigan State University \u0026 NEC Laboratories America \u0026 University of California, San Diego] (2017)\n【paper】【paper2】【GitHub】\n【通过观察目标运动迁移学习特征】《Learning Features by Watching Objects Move》D Pathak, R Girshick, P Dollár, T Darrell, B Hariharan [Facebook AI Research \u0026 UC Berkeley] (2016)\n【paper】【GitHub】\n【面向深度学习训练的视频标记工具】’BeaverDam - Video annotation tool for deep learning training labels’ by Anting Shen\n【paper】【GitHub】\n【生成对抗网络(GAN)图片编辑】《Photo Editing with Generative Adversarial Networks | Parallel Forall》by Greg Heinrich\n【paper】【paper2】【GitHub】\n解读Keras在ImageNet中的应用：详解5种主要的图像识别模型\n【paper】【GitHub】\n《Adversarial PoseNet: A Structure-aware Convolutional Network for Human Pose Estimation》Y Chen, C Shen, X Wei, L Liu, J Yang [Nanjing University of Science and Technology \u0026 The University of Adelaide \u0026 Nanjing University] (2017)\n【paper】【GitHub】\n【结构感知卷积网络的人体姿态估计】《Adversarial PoseNet: A Structure-aware Convolutional Network for Human Pose Estimation》Y Chen, C Shen, X Wei, L Liu, J Yang [Nanjing University of Science and Technology \u0026 The University of Adelaide \u0026 Nanjing University] (2017)\n【paper】【GitHub】\n【基于神经网络的鲁棒多视角行人跟踪】《Robust Multi-view Pedestrian Tracking Using Neural Networks》M Z Alom, T M. Taha [University of Dayton] (2017)\n【paper】【GitHub】\n【视频密集事件描述】”Dense-Captioning Events in Videos”\n【paper】【GitHub】【data】\n【受Siraj Raval深度学习视频启发的每周深度学习实践挑战】’Deep-Learning Challenges - Codes for weekly challenges on Deep Learning by Siraj’ by Batchu Venkat Vishal\n【paper】【GitHub】\n《SLAM with Objects using a Nonparametric Pose Graph》B Mu, S Liu, L Paull, J Leonard, J How [MIT] (2017)\n【paper】【GitHub】\n【医学图像分割中迭代估计的归一化输入】《Learning Normalized Inputs for Iterative Estimation in Medical Image Segmentation》M Drozdzal, G Chartrand, E Vorontsov, L D Jorio, A Tang, A Romero, Y Bengio, C Pal, S Kadoury [Universite de Montreal \u0026 Imagia Inc] (2017)\n【paper】【GitHub】\n《An Analysis of Action Recognition Datasets for Language and Vision Tasks》S Gella, F Keller [University of Edinburgh] (2017)\n【paper】【GitHub】\n【2017.05.09】\nTensorflow实现卷积神经网络，用于人脸关键点识别\n【paper】【GitHub】\n【FRCN(faster-rcnn)文字检测】’Text-Detection-using-py-faster-rcnn-framework’ by jugg1024\n【paper】【GitHub】\n【手机单目视觉状态估计器】’VINS-Mobile - Monocular Visual-Inertial State Estimator on Mobile Phones’ by HKUST Aerial Robotics Group\n【paper】【GitHub】\n【R-FCN目标检测】R-FCN: Object Detection via Region-based Fully Convolutional Networks\n【paper】【GitHub】\n行人检测、跟踪与检索领域年度进展报告\n【paper】【GitHub】\n【(TensorFlow)点云(Point Cloud)分类、分割、场景语义理解统一框架PointNet】’PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation’\n【paper】【paper2】【GitHub】【GitHub2】\n【深度视频去模糊】《Deep Video Deblurring》by Shuochen Su(2016)\n【paper】【paper2】【GitHub】【video】\n【中国的Infervision及其肺癌诊断AI工具】《Chinese startup Infervision emerges from stealth with an AI tool for diagnosing lung cancer | TechCrunch》by Jonathan Shieber\n【paper】【paper2】【GitHub】\n【基于医院大量胸部x射线数据库的弱监督分类和常见胸部疾病定位的研究】《ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases》X Wang, Y Peng, L Lu, Z Lu… [National Institutes of Health] (2017)\n【paper】【paper2】【GitHub】\n目标跟踪方法的发展概述\n【paper】【GitHub】\n【(Caffe)实时交互式图片自动着色】《Real-Time User-Guided Image Colorization with Learned Deep Priors》[UC Berkeley] (2017)\n【paper】【paper2】【GitHub】【video】\n相术的新衣】《Physiognomy’s New Clothes》by Blaise Aguera y Arcas\n【paper】【GitHub】\n【2017.05.10】\n快速生成人脸模型\n【paper】【paper2】【GitHub（预计八月开源）】\nVALSE2017系列之二: 边缘检测领域年度进展报告\n【paper】【GitHub】\n【(GTC2017)Stanford发布0.5PB大规模放射医疗图像ImageNet数据集】“Stanford gave the world ImageNet. Now it’s giving the world Medical ImageNet—a 0.5PB dataset for diagnostic radiology” via:James Wang\n【paper】【GitHub】\n【医疗图像深度学习】《Medical Image Analysis with Deep Learning》by Taposh Dutta-Roy\nPart1\nPart2\nPart3\n【激光雷达(LIDAR)：自驾车关键传感器】《An Introduction to LIDAR: The Key Self-Driving Car Sensor》by Oliver Cameron\n【paper】【GitHub】\n【根据目标脸生成带语音的视频】《You said that?》J S Chung, A Jamaludin, A Zisserman [University of Oxford] (2017)\n【paper】【GitHub】\n【用于图像生成和数据增强的生成协作网】《Generative Cooperative Net for Image Generation and Data Augmentation》Q Xu, Z Qin, T Wan [Beihang University \u0026 Alibaba Group] (2017)\n【paper】【GitHub】\n【COCO像素级标注数据集】’The official homepage of the COCO-Stuff dataset.’\n【paper】【GitHub】\n《COCO-Stuff: Thing and Stuff Classes in Context》 (2017) 【paper】【GitHub】\n【LinkNet：基于编码器表示的高效语义分割】《(LinkNet)Feature Forwarding: Exploiting Encoder Representations for Efficient Semantic Segmentation》A Chaurasia, E Culurciello\n【paper】【GitHub】【GitHub2】\n3、自然语言处理\n【2017.04.22】\n《Semantic Instance Segmentation via Deep Metric Learning》A Fathi, Z Wojna, V Rathod, P Wang, H O Song, S Guadarrama, K P. Murphy [Google Inc \u0026 UCLA] (2017)\n【paper】【GitHub】\n【2017.04.26】\n【对话语料集】’chat corpus collection from various open sources’ by Marsan-Ma\n【paper】【GitHub】\n【2017.05.07】\n【从文本中提取特征的神经网络技术综述】《A Survey of Neural Network Techniques for Feature Extraction from Text》V John [University of Waterloo] (2017)\n基於向量匹配的情境式聊天機器人’ by Justin Yang\n【paper】【GitHub】\n【PyTorch实践：序列到序列Attention法-英翻译】《Practical PyTorch: Translation with a Sequence to Sequence Network and Attention》by Sean Robertson\n【paper】【GitHub】\n【PyTorch实践：探索GloVe词向量】《Practical PyTorch: Exploring Word Vectors with GloVe》by Sean Robertson\n【paper】【GitHub】\n【自然语言生成(NLG)系统评价指标】《How to do an NLG Evaluation: Metrics》by Ehud Reiter\n【paper】【paper2】【GitHub】\n【看似靠谱的文本分类对抗样本】’textfool - Plausible looking adversarial examples for text classification’ by Bogdan Kulynych \u003e【paper】【GitHub】\n【基于bidirectional GRU-CRF的联合中文分词与词性标注】’A Joint Chinese segmentation and POS tagger based on bidirectional GRU-CRF’ by yanshao9798\n【paper】【GitHub】\n【自然语言处理(NLP)入门指南】《How to get started in NLP》by Melanie Tosik\n【paper】【GitHub】\n【2017.05.08】\n【(TensorFlow)面向文本相似度检测的Deep LSTM siamese网络】’Deep LSTM siamese network for text similarity - Tensorflow based implementation of deep siamese LSTM network to capture phrase/sentence similarity using character embeddings’ by Dhwaj Raj\n【paper】【GitHub】\n【Keras/TensorFlow语种检测】《Deep Learning: Language identification using Keras \u0026 TensorFlow》by Lucas KM\n【paper】【GitHub】\n-【(C++)神经网络语种检测工具】“Compact Language Detector v3 (CLD3) - neural network model for language identification” by Google\n【paper】【GitHub】\n【用于文本分类的端到端多视图网络】《End-to-End Multi-View Networks for Text Classification》H Guo, C Cherry, J Su [National Research Council Canada] (2017)\n【paper】【GitHub】\n【理解非结构化文本数据】《Making Sense of Unstructured Text Data》L Li, W M. Campbell, C Dagli, J P. Campbell [MIT Lincoln Laboratory] (2017)\n【paper】【GitHub】\n【非本族语者英语写作风格检测】《Detecting English Writing Styles For Non Native Speakers》Y Chen, R Al-Rfou’, Y Choi [Stony Brook University] (2017)\n【paper】【GitHub】\n【2017.05.10】\nFacebook提出全新CNN机器翻译：准确度超越谷歌而且还快九倍（已开源）\n【paper1】【paper2】【GitHub】\n4、应用案例\n【2017.04.21】\n深度学习入门实战（一）-像Prisma一样算法生成梵高风格画像\n【paper】【GitHub】\n【2017.04.22】\n我们教电脑识别视频字幕\n【paper】【GitHub】\n【2017.04.24】\n《Data Sciencing Motorcycles: Lean Assist》by Josh Peng\n【paper】【GitHub】\n【2017.04.26】\n【PhotoScan新增的去除翻拍反光功能】《PhotoScan: Taking Glare-Free Pictures of Pictures | Google Research Blog》by Ce Liu, Michael Rubinstein, Mike Krainin, Bill Freeman\n【paper】【GitHub】\n【2017.05.08】\n【假新闻的实时检测】《How to Detect Fake News in Real-Time 》by Krishna Bharat\n【paper】【GitHub】\n5、综合\n5.1 教程\n【2017.04.21】\n30 Free Courses: Neural Networks, Machine Learning, Algorithms, AI\n【paper】【GitHub】\n【2017.04.22】\n【Deep Learning】\n英文原文：【link】\n中文译文：【link】\n中文译文说明：【link】\n【2017.04.23】\n机器学习(Machine Learning)\u0026深度学习(Deep Learning)资料(Chapter 1)\n【paper】【GitHub】\n【2017.05.07】\n【台大李宏毅中文深度学习课程(2017)】”NTUEE Machine Learning and having it Deep and Structured(MLDS) (2017)”\n【paper】【GitHub】【video】\nTensorFlow教程\n【paper】【GitHub】\n【2017.05.08】\n【Keras教程：Python深度学习】《Keras Tutorial: Deep Learning in Python》by Karlijn Willems\n【paper】【GitHub】\n【2017.05.09】\n【用Anaconda玩转深度学习】《Deep Learning with Anaconda(AnacondaCON 2017) - YouTube》by Stan Seibert \u0026 Matt Rocklin\n【paper】【GitHub】【video】\n5.2 其它\n【2017.04.23】\n哥伦比亚大学与Adobe提出新方法，可将随机梯度下降用作近似贝叶斯推理\n【paper】【GitHub】\n英特尔深度学习产品综述：如何占领人工智能市场\n【paper】【GitHub】\n【2017.04.24】\n28款GitHub最流行的开源机器学习项目：TensorFlow排榜首\n【paper】【GitHub】\n【2017.04.26】\n英国皇家学会百页报告：机器学习的力量与希望（豪华阵容参与完成）\n【paper】【GitHub】\n深度学习在推荐算法上的应用进展\n【paper】【GitHub】\n周志华教授gcForest（多粒度级联森林）算法预测股指期货涨跌\n【paper】【GitHub】\n【2017.05.07】\n市值250亿的特征向量——谷歌背后的线性代数\n【paper】【GitHub】\n【可重现/易分享数据科学项目框架】’DVC - Data Version Control: Make your data science projects reproducible and shareable\n【paper】【GitHub】\n《Fast k-means based on KNN Graph》C Deng, W Zhao [Xiamen University] (2017)\n【paper】【GitHub】\n【信息检索人工神经网络模型】《Neural Models for Information Retrieval》B Mitra, N Craswell [Microsoft] (2017)\n【paper】【GitHub】\n地平线机器人杨铭：深度神经网络在图像识别应用中的演化\n【paper】【GitHub】\n【(Python)Facebook的开源AI对话研究框架】’ParlAI - A framework for training and evaluating AI models on a variety of openly available dialog datasets.’\n【paper】【GitHub】\n【(Python)深度神经网络多标签文本分类框架】’magpie - Deep neural network framework for multi-label text classification’ by inspirehep\n【paper】【GitHub】\n【(300万)Instacart在线杂货购物数据集】《3 Million Instacart Orders, Open Sourced》by Jeremy Stanley\n【paper】【GitHub】\n【基于语言/网络结构的推荐系统GraphNet】《GraphNet: Recommendation system based on language and network structure》R Ying, Y Li, X Li [Stanford University] (2017)\n【paper】【GitHub】\n【2017.05.08】\n【将Python 3.x代码转换成Python2.x代码的Python-Python编译器】’Py-backwards - Python to python compiler that allows you to use Python 3.6 features in older versions.’ by Vladimir Iakovlev\n【paper】【GitHub】\n【2017.05.09】\n【Xgboost新增GPU加速建树算法】”Xgboost GPU - CUDA Accelerated Tree Construction Algorithm”\n【paper】【GitHub】\n【独立开发者赚钱资料集锦】’awesome-indie - Resources for independent developers to make money’ by Joan Boixadós\n【paper】【GitHub】\n【基于MAPD/Anaconda/H2O的GPU数据分析框架】’GPU Data Frame with a corresponding Python API’\n【paper】【GitHub】\n从文本到视觉：各领域最前沿的论文集合\n【paper】【GitHub】\n【2017.05.10】\n【(C++)信息检索框架库Trinity】’Trinity IR Infrastructure’ by Phaistos Networks GitHub:\n【paper】【GitHub】\n参考\n爱可可-爱生活\n机器之心synced\n机器学习研究会","data":"2017年04月23日 00:55:15"}
{"_id":{"$oid":"5d343b3662f717dc0659b3b2"},"title":"Tensorflow-自然语言处理-唐宇迪-专题视频课程","author":"迪哥有点愁了","content":"Tensorflow-自然语言处理—331人已学习\n课程介绍\n\n课程以Tensorflow作为核心武器，基于自然语言处理热点话题进行案例实战。选择当下热门模型，使用真实数据集进行实战演示，通俗讲解整个算法模型并使用tensorflow进行实战，详解其中的原理与代码实现。\n课程收益\n掌握如何使用Tensorflow进行自然语言处理任务实战\n讲师介绍\n唐宇迪更多讲师课程\n计算机博士，专注于机器学习与计算机视觉领域，深度学习领域一线实战讲师。在图像识别领域有着丰富经验，实现过包括人脸识别，物体识别，关键点检测等多种应用的新算法。 参与多个国家级计算机视觉项目，多年数据领域培训经验，丰富的教学讲解经验，出品多套机器学习与深度学习系列课程，课程生动形象，风格通俗易懂。\n课程大纲\n第1章:词向量模型\n1.词向量任务简介  10:36\n2.数据源制作  6:34\n3.输入数据制作  12:24\n4.模型整体架构  12:36\n5.模型结果  6:19\n第2章:相似度判别\n1.相似度分析模型  5:40\n2.数据源分析  5:53\n3.数据输入制作  8:41\n4.数据前期整理  7:49\n5.整体模型架构  11:24\n6.单词训练方法  9:19\n7.整体相似度判别  5:52\n第3章:聊天机器人构造\n1.对话展示  8:27\n2.数据制作与配置  12:55\n3.输入数据处理  8:52\n4.向量层与输出层  9:18\n5.编码解码网络  8:23\n6.整体模型实现  6:55\n7.整体模型概述  6:51\n8.数据处理与架构模型  13:17\n9.模型实现  8:47\n大家可以点击【查看详情】查看我的课程","data":"2018年04月02日 09:07:21"}
{"_id":{"$oid":"5d343b3662f717dc0659b3b4"},"title":"干货分享 | 自然语言处理及词向量模型介绍（附PPT）","author":"量子位","content":"云脑科技机器学习训练营第二期，对自然语言处理及词向量模型进行了详细介绍，量子位作为合作媒体为大家带来本期干货分享~\n本期讲师简介\n樊向军\n云脑科技核心算法工程师，清华大学学士，日本东京大学与美国华盛顿州立大学双硕士\n第33届亚洲、国际物理奥赛双料金牌得主，在美国硅谷高通等公司有着多年超高性能计算仿真软件设计开发经验，获得高通Qualstar Diamond杰出贡献奖，目前作为云脑科技算法团队的主要成员进行金融、通信、能源大数据领域的核心人工智能算法研发与系统设计工作。\n分享内容实录\n自然语言处理Natural Language Processing是一个非常大的topic，在本节课程中，我们仅做非常概要性的介绍。下面这张图可以给你一个感觉，NLP技术能够做些什么。\nNLP应用在自然语言处理中主要分为以下几类：第一是Classifying Words ，即需要去研究一下词是什么意思。第二是Classifying Documents，即整个文章有一些什么操作，怎么去分类。第三个比较难也比较热门的是Understand Documents，即理解文章是在讲什么。这些是NLP比较热门的几个方面。前半段我们讲介绍比较传统的NLP方法，后面会讲NLP和Deep Learning 的结合。\nClassifying Words\n也就是把每个词分类，词分为哪几类或者是能不能把它group起来？比如说维基百科上很多信息放在一起，或者你拿到一本字典、百科全书，再或者许多文章放在一起，怎么去分类这些字？NLP产生了许多分支去研究各种各样的里面的问题，比如： Stemming，找到一个词的词根，根据词根把相同的词尽量的放在一起。\n另外一个是Splitting Words分词，根据里面的字母把词分成许多块，做字母级别的k-grams或者n-grams，再做分类。这两种方法比较偏重拉丁文、英文语系的文章，对词根或者字母进行分解，但是对中文不是很合适。\nClassifying Documents\n分类文本本身，词我们可以找词根或者分词，文本分类又提高了一个难度。它有一些应用，比如说我们想知道读一篇文章需要多久，最简单的办法是规定某一个人每分钟读多少词，统计一下这篇文章有多少词，做一下除法，就得到了时间。\n这可能是最直截了当的方法，但是精度可能很差，因为每个人读的速度不一样，文章本身的难度也不一样等各种各样的原因。如果应用没有特别的要求，就可以这样简单的用一下，但如果某些应用或研究中希望得到一个高的精度，比如你的研究是有阅读障碍的人遇到各种各样的文本会怎么样，则希望会得到一个精度比较高的阅读时间的估计结果。NLP本身有许多研究，也产生了许多好的方法，在这里就不细讲了，有兴趣可以关注一下相关研究文献。\nIdentifying a Language\n这也是比较有趣的应用，你输入给计算机一段语言，绝大部分要做翻译，比如说把日语翻译成中文，或者把中文翻译成日语，你需要选一下对应语言的选项才可以，那现在怎么让计算机自己去发现你的输入属于哪一种语言？这也是NLP的一个研究方向。\n有一种方法是Words in a Vocabulary，先建一个词典，看你输入的属于哪一个词典里，当然这也是低效率，精度也不一定高的办法。还有一个方法是Frequencies of Groups of Letters，比如说中文，用中文的语言建立一个语言模型，然后再把新输入的东西带入你的模型作比较，得出是不是属于中文的结论。具体语言模型要怎么建立，后面会提到。\nUnderstanding Documents\n刚刚我们有了词、有了文本，它们都是做分类，但文本具体是在讲什么呢？这又是一个一直想要解决，但没有非常好的、完全被解决的方向，很多人在这个方向上做各种各样的研究。现在有一些比较好的、可以实际应用的，比如document summary，之前英国有个学生写了对news做summary的系统，后来被谷歌收去了。所以像这种应用很多互联网公司已经有了很好的处理能力了，但还是希望有更好的算法进来。\n关于summary，TextRank比较有意思，大家都知道谷歌最早的算法叫 PageRank，根据两个document之间的connection去links、去排序它语义的重要性。TextRank比它更拓展一点，不是简单的有关系或者没有关系，如果两个文本间部分有关系怎么办？它把两个文本间部分有关系部分copy进来，做一个语言模型。\n第二个是Sentiment analysis 语义分析，简单的看一下这个文本是positive 还是 negative? 现在也很常用，比如说对亚马逊或者京东商品review，分析一下是好的还是坏的，好在哪里坏在哪里？或者是苹果app、安卓app或者是谷歌的app的评论，有多好多坏？好在哪里，差在哪里？这些都可以通过语义分析出来，帮助客户做更好的选择。\n接下来是Parsing，也是很常用的，一个文本是有结构的，有章节，句子有主谓宾，有的段落的首句或者尾句是总结，那这种结构怎么去认识它。比如说股票公司的报告，上市公司的年报、财报，法律文件，合同等，这些报告你怎么去理解它？这些文本都比较结构化了，从人的角度都很好了解到，如第一章讲的是什么？合同的甲方乙方是谁？合同的成交价格是多少？这些人都可以快速的找到它，但对机器来说是不是有个比较好的办法去识别呢？而不是规定具体的哪一章节要讲什么。\n有些公司试图把规范化化报告的描述，其实这些是很难完全做到的，其实有更好的办法：用NLP的方法找到关键点。\n比如大家写code都需要有具体的格式，比如说缩进、每一行后面加分号等，这些都是规定好的，所以计算机很容易compare你的language把它变成机器语言，但真正的自然语言没有严格的语法，那怎么做Parsing也是需要做但也比较难的问题。\n最后是Translate语言翻译，显而易见大家都看到了最近几年有很大的进步，应用也是越来越广。\nNLP应用十分的广，这里主要是讲一下大概，如果大家对某个方向或应用感兴趣可以关注、阅读一些相关应用的研究结果。\n比较常用的NLP Libraries: Apache OpenNLP，The Classical Language Toolkit (CLTK) ，FreeLing，Moses，NLTK，Pattern，Polyglot，Sentiment，SpaCy，CoreNLP，Parser\nNLP与Deep Learning\n之前讲的这些NLP都是和语言处理相关的。举几个例子，看看如何将NLP和Deep Learning 相结合？如何通过引入deep learning 把NLP做到更好的效果。\n词分类\n根据相似性词分类。这里例子是摘自斯坦福的讲义，如果大家对NLP感兴趣，建议看一下斯坦福的课程，里面讲了大量应用Deep Learning的例子，且都是更新比较快的。如果输入是一个青蛙，会输出一些和青蛙有关的词，比如toad等。如果你给他一个文本，它自己能够把它学出来。\n语法的分析\n词本身是有词性或是语法的，比如拉丁文、英文前缀后缀，如加un变反义，这都是根据词根来定。Deep learning的思路是：把每个词义变成一个vector，然后分成两个或多个vector，把它们combine起来，然后去学它的vector就可以了。对于句子做一些句法的分析，句子中有一些词，有主谓宾这种词要怎么认出来，这是做语义理解的一个基础，那怎么用Deep Learning 来做句法的分析？\n语义的分析\n传统的做法需要对语言本身做许多工程，需要做一些知识的积累，然后root放进来，然后用tree记录下来，最后来分析新进来的文本。Deep Learning的做法是把字和词变成一个向量，然后用neural network去学这个vector。\nQuestion Answering也是有传统的做法，首先要做许多的feature engineering 把knowledge 先build起来，通过build起来的knowledge 再做一个分解。从deep learning角度来看，你Question Answering 需要的一些facts,需要的一些结果，需要的一些answer都会存在vector 里面，然后再通过vector再做问答的matching.\nTranslation谷歌、Facebook把之前传统的Translation方法都推翻了，使用Machine Learning 提高的很多的精度。这里有一个Neural Machine Translation的示意图，ML的做法都是把它变成了一个vector。\nVector\n我们前面提到的应用都用vector 来做，下面我们主要讲vector是什么？怎么去用vector？怎么得到vector？\nWord Embedding\n简单的讲vector就是一个一维的数组，每一个词都变成一个vector。比如说先把一个词变到一个多维空间中，然后把所有的词都放在这个多维空间中。最大的好处是，这些词对计算机来说是categorical feature，像one-hot一样，两个词放在不同位置完全没有关系。如果用vector来表示，词与词之间的关系就可以用距离来表现。也就是说这些词对计算机来说本来是没有关系的，但通过vector转换之后，它们的距离代表了它们的关系，这也是比较好的帮助计算机去理解词之间关系的方法。\nWord Embedding 实际上就是把词从词本身或从one-hot本身变成一个vector 的过程，Embedding就是你怎么去变换这个向量。\nHow Do We Represent the Meaning of a Word?\nMeaning本身在字典的定义是：词背后的想法，或是某个人、文章、艺术品想要表达的想法。Meaning本身是个idea，它在大脑里面怎么存储的我们不知道，这个idea怎么让计算机系统去理解它，比较好的办法是把它变成一个vector。\n词本身如果不做向量的变化，那计算机看起来是什么？如果两个词不一样，那就是一个分类的feature, 那我们就直接做one-hot，就是在出现的位置记为1，其他位置记为0，这样做显然是可以的，但是维度是十分大的，尤其是英文。比如说，你搜索电视大小其实和电视容量是一个意思，那计算机怎么知道电视大小和电视容量是同一个意思？包括你要查hotel和motel，其实是一个意思，如果用one-hot它们将在两个维度上，完全没有关系。如果把它变到一个比one-hot低维的，但每个位置上都是有浮点数的vector，而且这些浮点数的数值是有意义的，比如说两个词的浮点数值大小非常靠近，那这两个词就比较靠近，那这样学出来的vector 也是非常有意义的。\nHow to Represent Word as Vector？\n怎么去学vector？每一个词都做一个one-hot encoding，变成一个很长的vector。通过Neural network，首先input vector然后通过 Hidden Layer加工，再marking到另一个词上面，再进行训练。如果这两个词是相近的就是1，如果不相近就是0。如果能准备到这样的数据，让神经网络去学，所有的词都表征为向量之后，那这两个向量距离之间就比较接近，因为和train数据是有关系的。如果进去是一个词，让它比较和另一个词的距离，这时需要有一个label才可以去train，那怎么样得到这个label呢？\nSkip-Gram Model\n下面举一个Skip-Gram Model的例子，它的主要思想是：如果你能够拿到一些文本，可能是维基百科、百度百科的文章，很自然的有一些词就会出现在另一些词的附近，那我们在做Skip-Gram的过程实际就是在create　一个train data的过程，我们把文本拿来，把中间词作为ｘ，两边的词作为label或是topic words，这两个词如果同时出现在附近，可以记为１，如果没有记为０。\n这样的就可以得到一个train sample，这样的train sample都是一个pairs，这样就可以把文本变成很多个train sample，再返回刚才的模型，能很好的把Hidden Layer学出来。学到Hidden Layer之后这就一个embedding了，通过word Paris建立语言模型，然后每一个词再回来，本身还是一个one-hot encoding，再经过Hidden Layer weight matrix，会变成一些的word vector。\n回溯总结一下，vector就是把词本身变成一个向量，怎么得到这个向量？刚才举到了用神经网络，Skip-Gram 建立train 数据，然后学到这个数据，然后Embedding实际上就是Hidden Layer weight matrix，通过Embedding就得到了向量。这是一个比较直接的，事实证明也是一个比较有效的办法。\nNegative Sampling\n刚刚有讲到每个词用one-hot encoding，然后用weight matrix与它相乘，假设我们想要得到的vector的size是300，输入字典的维度是10000，可以看到weight matrix 有300*10000个parameter，只有一个FC Layer就非常大了，所以不仅是weight matrix 这么大，每一次迭代都要把matrix更新一遍，这样整个学习过程的效率是十分低的。\n比如说在10000个词中有很多和它是词义相近的，但绝大部分和它是没有关系的，数学的角度是正交的，所以它不需要每次都进行更新。所以Negative Sampling的核心：大量减少更新的内容，而且可以大量的减少训练损失，实际测量下来的结果也是非常好的。当然还要做一个词频，高频的词需要放到训练的过程当中去，低频的就不需要做了。\n总结一下Word2Vec 是怎样一个流程：\nCollect text data\nProcess text\nSkip-Gram to generate word pair\nTraining embedding\nWord2Vec\nVector Space Visualization\n把它们全都变成vector之后下一步需要做什么？下一步最简单的做法就是把它们画出来。\n当然之前例子中说到把每一个词变到300维，300维是人的肉眼是看不出来的，大家的物理世界只有３维，需要一些降维的方法，降维之后可以看到本来一些词是没有关系的，最后自动的group到一起。\n比如说左上角12345678910的英语都到一起了，左下角和时间相关的都在一起了，右下角语文数学化学高考科目都到一起了，这是个比较有意思的事情，明显的看到这个学习的过程是比较有意义的，意思相近的词都在一起了。\n还有一个点，word vector不仅仅是把词进行分类，而且词和词之间的距离也是有很强的关系。\n比如说英语里面有基本级、比较级、最高级，如果看成一个向量，一个的比较级减去最高级和另外一个比较级减去另外一个最高级，下面的向量还是一样的。它不仅把词之间的分类学会了，词之间的关系也学会了。还有就是男人对应国王，女人对应什么？是皇后。这个也能学出来。所以Word2Vec的fundamental idea不是很难，但效果也是非常好的。\n相关学习资源\n以上就是此次课程的相关内容，在量子位微信公众号（QbitAI）对话界面回复“171202”，可获得完整版PPT。\n上期课程回顾：干货分享 | 详解特征工程与推荐系统及其实践\nP.S.云脑科技对人才如饥似渴，如有兴趣请移步http://cloudbrain.ltd/join.html ，简历发送至 job@cloudbrain.ai ，据说邮件主题添加注明“来源量子位”，通过率会更高噢~\n— 完 —\n活动报名\n加入社群\n量子位AI社群11群开始招募啦，欢迎对AI感兴趣的同学，加小助手微信qbitbot4入群；\n\n\n此外，量子位专业细分群(自动驾驶、CV、NLP、机器学习等)正在招募，面向正在从事相关领域的工程师及研究人员。\n\n\n进群请加小助手微信号qbitbot4，并务必备注相应群的关键词~通过审核后我们将邀请进群。（专业群审核较严，敬请谅解）\n诚挚招聘\n量子位正在招募编辑/记者，工作地点在北京中关村。期待有才气、有热情的同学加入我们！相关细节，请在量子位公众号(QbitAI)对话界面，回复“招聘”两个字。\n量子位 QbitAI · 头条号签约作者\nվ'ᴗ' ի 追踪AI技术和产品新动态","data":"2017年12月02日 00:00:00"}
{"_id":{"$oid":"5d343b3762f717dc0659b3b6"},"title":"自然语言处理_分词_停用词整理[哈工大、四川大学机器智能实验室停用词库、百度停用词库、中文停用词词表]","author":"舟雨","content":"最近在研究自然语言处理，最基础的内容之一是分词处理，但是分词的结果并非均是有效的信息，按照普遍说法，存在‘停用词’这样的尴尬信息。\n所谓‘停用词’，即是在自然语言处理时，与文章包含的情感信息，或文章主题信息关系性不强的词语，所以如果进行筛选过滤之后，更便于主题分析，或者情感分析。\n这里，我在网上找到了：\n\n结合哈工大停用词表、四川大学机器智能实验室停用词库、百度停用词表、以及网络上较大的一份无名称停用词表，\n并整理了一下，做了去重处理，最终得到了一份较全的停用词表，在此分享出来给大家，希望对各位有用。\n整合的停用词表下载\n后续可能即需更新其他相关文章，逐步积累，哈哈。","data":"2018年01月02日 15:55:00"}
{"_id":{"$oid":"5d343b3762f717dc0659b3b8"},"title":"机器学习 人工智能 博文链接汇总","author":"Alice熹爱学习","content":"115\n[入门问题]\n[TensorFlow]\n[深度学习]\n[好玩儿的算法应用实例]\n[聊天机器人]\n[神经网络]\n[机器学习]\n[机器学习算法应用实例]\n[自然语言处理]\n[数据科学]\n[Python]\n[Java]\n[机器学习－－初期的笔记]\n[路线]\n[软件安装]\n[面试]\n入门问题\n简单粗暴地入门机器学习\n机器学习的技术栈及应用实例脑洞\n深度学习相关最新图书推荐\nTensorFlow\nTensorFlow-11-策略网络\nTensorFlow-10-基于 LSTM 建立一个语言模型\nTensorFlow-9-词的向量表示\nTensorFlow-8-详解 TensorBoard 如何调参\nTensorFlow-7-TensorBoard Embedding可视化\nTensorFlow-6-TensorBoard 可视化学习\nTensorFlow－5: 用 tf.contrib.learn 来构建输入函数\nTensorFlow-4: tf.contrib.learn 快速入门\nTensorFlow－3: 用 feed-forward neural network 识别数字\nTensorFlow-2: 用 CNN 识别数字\nTensorFlow－1: 如何识别数字\nTensorFlow 入门\n一文学会用 Tensorflow 搭建神经网络\n用 Tensorflow 建立 CNN\n深度学习\n深度学习的主要应用举例\n[Keras]\n对比学习用 Keras 搭建 CNN RNN 等常用神经网络\n[强化学习]\n强化学习是什么\n一文了解强化学习\n好玩儿的算法应用实例\n5分钟构建一个自己的无人驾驶车\n自己动手写个聊天机器人吧\n自己写个 Prisma\n用 TensorFlow 创建自己的 Speech Recognizer\n用 TensorFlow 让你的机器人唱首原创给你听\n如何自动生成文章摘要\n聊天机器人\n开启聊天机器人模式\n用 TensorFlow 做个聊天机器人\n神经网络\n神经网络\n神经网络的前世\n神经网络 之 感知器的概念和实现\n神经网络 之 线性单元\n什么是神经网络\n手写，纯享版反向传播算法公式推导\n常用激活函数比较\n什么是 Dropout\nCNN\n图解何为CNN\n用 Tensorflow 建立 CNN\n按时间轴简述九大卷积神经网络\nRNN\n详解循环神经网络(Recurrent Neural Network)\n图解RNN\nCS224d－Day 5: RNN快速入门\n用深度神经网络处理NER命名实体识别问题\n用 RNN 训练语言模型生成文本\n用 Recursive Neural Networks 得到分析树\nRNN的高级应用\nLSTM\n详解 LSTM\n用 LSTM 来做一个分类小问题\n用 LSTM 做时间序列预测的一个小例子\nseq2seq\nseq2seq 入门\nseq2seq 的 keras 实现\n机器学习\n[Kaggle]－－由此来看实战是什么样的\n一个框架解决几乎所有机器学习问题\n通过一个kaggle实例学习解决机器学习问题\n从 0 到 1 走进 Kaggle\nKaggle 神器 xgboost\n[基础]－－一些基本概念和小技巧\n轻松看懂机器学习十大常用算法\n特征工程怎么做\n机器学习算法应用中常用技巧-1\n机器学习算法应用中常用技巧-2\n关于凸优化\n如何选择优化器 optimizer\n为什么要用交叉验证\n用学习曲线 learning curve 来判别过拟合问题\n用验证曲线 validation curve 选择超参数\n用 Grid Search 对 SVM 进行调参\n用 Pipeline 将训练集参数重复应用到测试集\nPCA 的数学原理和可视化效果\n机器学习中常用评估指标汇总\n什么是 ROC AUC\n[算法]－－通俗易懂讲算法\n决策树的python实现\nCART 分类与回归树\nBagging 简述\nAdaboost 算法\n浅谈 GBDT\n用ARIMA模型做需求预测\n推荐系统\n[Sklearn]\nSklearn 快速入门\n了解 Sklearn 的数据集\n自然语言处理\n[cs224d]\nDay 1. 深度学习与自然语言处理 主要概念一览\nDay 2. TensorFlow 入门\nDay 3. word2vec 模型思想和代码实现\nDay 4. 怎样做情感分析\nDay 5. CS224d－Day 5: RNN快速入门\nDay 6. 一文学会用 Tensorflow 搭建神经网络\nDay 7. 用深度神经网络处理NER命名实体识别问题\nDay 8. 用 RNN 训练语言模型生成文本\nDay 9. RNN与机器翻译\nDay 10. 用 Recursive Neural Networks 得到分析树\nDay 11. RNN的高级应用\n一个隐马尔科夫模型的应用实例：中文分词\n数据科学\n1.［图解DS基础概念］AB Testing, Type 1 / 2 Error\n2.［图解DS基础概念］Critical value，Alpha，Z－score，P－value 关系\nPython\nPandas常用命令－1\nPandas常用命令－2\nPandas QQ聊天记录分析\nPython 爬虫 1 快速入门\nPython 爬虫 2 爬取多页网页\nJava\n入门 Java 系列汇总：\n2 天入门 Java－Day 1\nDay 1-Java-imooc－2.变量常量\nDay 1-Java-imooc－3.运算符\nDay 1-Java-imooc－4.流程控制语句\nDay 1-Java-imooc－5.数组\nDay 1-Java-imooc－6.方法\n2 天入门 Java－Day 2\nDay 2-Java－imooc－8-封装\nDay 2-Java－imooc－9-继承\nDay 2-Java－imooc－10-多态\n机器学习－－初期的笔记很粗糙\n机器学习－多元线性回归\nUdacity-Machine Learning纳米学位－学习笔记1\nMachine Learning Notes-Decision Trees-Udacity\nMachine Learning Notes-Linear Regression-Udacity\n支持向量机\n神经网络\nInstance Based Learning\nEnsemble Learners\n路线\n数据科学家养成路线\n纯粹的数学之美\nPython很强大\n一张图带你看懂何为数据分析\n如何成为一名数据科学家并得到一份工作\n软件安装\n［MySQL］\n5分钟入门MySQL Workbench\n图解Mac下如何安装管理MySQL\n［Virtualenv］\n详解Mac配置虚拟环境Virtualenv，安装Python科学计算包\n面试\n面试官是怎么看你的Github profile\n［Leetcode］\nLEETCODE - Linked List 题目思路汇总\n欢迎关注公众号：极客X养成计划\n人工智能时代，学点机器学习，一起持续迭代，Run With AI ！","data":"2017年05月13日 23:41:07"}
{"_id":{"$oid":"5d343b3862f717dc0659b3ba"},"title":"[自然语言处理] (6) 主题提取 + 文本实体标注","author":"LeYOUNGER","content":"主题提取\n参考知乎的回答： 《“关键词”提取都有哪些方案？》\n《word2vec词向量训练及中文文本相似度计算》\n简单的LDA实现： 《NLP 主题抽取 Topic LDA代码实践 gensim包 代码》\n命名实体识别\n参考：http://spaces.ac.cn/archives/3942/\nIIS 推导：\nhttp://blog.csdn.net/xueyingxue001/article/details/50773917\nBi-LSTM + CRF 进行NER：\nhttps://www.cnblogs.com/Determined22/p/7238342.html\n使用CRF++进行命名实体识别\nhttps://zhuanlan.zhihu.com/p/27597790","data":"2017年12月12日 16:25:14"}
{"_id":{"$oid":"5d343b3862f717dc0659b3bc"},"title":"人工智能 5.搜索树求解","author":"lagoon_lala","content":"简单搜索:AI作为内核（算法、算力、大数据）。包括输入、输出、训练、搜索好坏评价。\n自然语言处理\n是搜索引擎最核心的基础技术。\n包括了输入和输出，每一次的搜索行为都可以看做是对搜索引擎的一次训练，用户的点击来告诉搜索结果的好坏，从而展示出相对应的搜索排名。在这个过程中，搜索引擎不仅提高了推荐的准确性，还越来越懂得判断所收录结果的好与坏，渐渐学会了像人类一样去分辨网页。\n在求解一个问题时，涉及到两个方面：\n问题的表示。\n相对合适的求解方法。（搜索法、归纳法、归结法、推理法和产生式等）。\n5.1  搜索的概念\n5.2  状态空间的搜索策略\n5.3  盲目的图搜索策略（Uninformed Search）\n5.4  启发式图搜索策略（ Informed Search ）\n补：其他搜索策略（局部搜索法、爬山搜索法、局部剪枝搜索、模拟退火法等）\n搜索中需要解决的基本问题：\n（1）是否一定能找到一个解。——完备性\n（2）找到的解是否是最佳解。——最优性\n（3）时间与空间复杂性如何。\n（4）是否终止运行或是否会陷入一个死循环。\n搜索的主要过程（三要素）：\n（1）状态空间 （state space）\n从初始或目的状态出发，并将它作为当前状态。（双向？）\n(2)  后继函数（successor function with actions and costs）\n扫描操作算子集，将适用当前状态的一些操作算子作用于当前状态而得到新的状态，并建立指向其父结点的指针 。\n(3)  初始状态和目标测试（start state and goal test）\n解 是一个行动序列，将初始状态转换成目标状态\n搜索问题是对原问题的建模!\n扩展出潜在的行动 (tree nodes)\n维护所考虑行动的边缘(fringe)节点\n试图扩展尽可能少的树节点\n搜索策略：\n1.  搜索方向：\n(1) 数据驱动：从初始状态出发的正向搜索。用给定数据中约束知识指导搜索\n(2) 目的驱动：从目的状态出发的逆向搜索。哪些操作算子能产生该目的，产生目的时需要哪些条件\n(3)双向搜索：直到两条路径在中间的某处汇合为止。\n2.  盲目搜索与启发式搜索:\n（1）盲目搜索：在不具有对特定问题的任何有关信息的条件下，按固定的步骤（依次或随机调用操作算子）进行的搜索。\n（2）启发式搜索：考虑可应用的知识，动态地确定调用操作算子的步骤，优先选择较适合的操作算子，尽量减少不必要的搜索，以求尽快地到达结束状态。\n状态空间的搜索策略\n状态空间表示法\n状态空间的图描述\n状态：表示系统状态、事实等叙述型知识的一组变量或数组.（环境细节）\n操作：表示引起状态变化的过程型知识的关系或函数：\nProblem: Pathing\nStates: (x,y) location\nActions: NSEW\nSuccessor: update location\nGoal test: is (x,y)=END\nProblem: Eat-All-Dots\nStates: {(x,y), dot booleans}\nActions: NSEW\nSuccessor: update location and  dot boolean\nGoal test: dots all false\n状态空间：利用状态变量和操作符号，表示系统或问题的有关知识的符号体系，状态空间四元组：\nS ：状态集合。\nO ：操作算子的集合。\nS0 ：包含问题的初始状态是 S的非空子集。\nG：若干具体状态或满足某些性质的路径信息描述。\n求解路径：从S0结点到G结点的路径。\n状态空间的一个解：一个有限的操作算子序列。\n八数码问题的状态空间。\n状态集S：所有摆法9!\n操作算子：4\n将空格向上移Up\n将空格向左移Left\n将空格向下移Down\n将空格向右移Right\n状态空间的图描述\n状态空间的有向图描述（搜索树）\n状态空间图中，每个状态只出现一次！（搜索树可出现多次）\n几乎不在内存中构建完整的状态空间图（太大了），但是有用的。\n盲目的图搜索策略\n回溯策略\n宽度优先搜索策略\n深度优先搜索策略\n带回溯策略的搜索：\n从初始状态出发寻找路径，直到它到达\n目的\n或“\n不可解结点\n”为止。若它遇到\n不可解结点就回溯到路径中最近的父结点\n上，查看该结点是否还有其他的子结点未被扩展。如果找到目标，就成功\n退出搜索\n，返回解题路径。\n回溯搜索的算法\n(1) PS（\npath\nstates）表：保存当前搜索路径上的状态。如果找到了目的，PS就是解路径上的状态有序集。\n(2) NPS（new path states）表：新的路径状态表。它包含了\n等待搜索的\n状态，其后裔状态还未被搜索到，即未被生成扩展 。\n(3) NSS（no solvable states）表：\n不可解状态集，列出了找不到解题路径的状态\n。如果在搜索中扩展出的状态是\n它的元素，则可立即将之排除\n，不必沿该状态继续搜索。\n图搜索算法（深度优先、宽度优先、最好优先搜索等）的回溯思想：\n（1）用未处理状态表（NPS）使算法能返回（回溯）到其中任一状态。 ？\n（2）用一张“死胡同”状态表（NSS）来避免算法重新搜索无解的路径。\n（3）在PS 表中记录当前搜索路径的状态，当满足目的时可以将它作为结果返回。\n（4）为\n避免陷入死循环必须对新生成的子状态进行检查\n，看它是否在该三张表中 。\n宽度优先搜索策略\nopen表（NPS表）：已经生成出来但其子状态未被搜索的状态。(FIFO)\nclosed表（ PS表和NSS表的合并）：记录了已被生成扩展过的状态。\n操作算子为MOVE（X，Y）：把积木X搬到Y（积木或桌面）上面。\n操作算子可运用的先决条件：\n（1）被搬动积木的顶部必须为空。\n（2）如果 Y 是积木，则积木 Y 的顶部也必须为空。\n（3）同一状态下，运用操作算子的次数不得多于一次。\n生成扩展完N层的所有结点后才转向N+1层，总能找到最好的解。\n当图分支数太多，即状态的后裔数平均值较大，这种组合爆炸会使算法耗尽资源。\n为了保证找到解，应选择合适的深度限制值，或采取不断加大深度限制值的办法，反复搜索，直到找到解。\n深度优先搜索并不能保证第一次搜索到的是到这个状态的最短路径。\n如果路径的长度对解题很关键的话，当算法多次搜索到同一个状态时，它应该保留最短路径。\nOpen表是一个堆栈结构，使搜索偏向最后生成状态。\n特点：\n深度优先搜索在搜索有大量分支的状态空间时有高效率，不需要把某层上所有结点进行扩展。\n但会找不到通向目的的更短路径或陷入不通往目的的无限长的路径中。\n所有的搜索算法都是相同的，除了对边缘的处理策略\n从概念上说，所有的边缘是优先队列 (即附加优先级的节点集合)\n对于DFS, BFS，可以通过使用栈或队列代替优先队列，从而减少log(n) 的开支\n结合DFS的空间优势与BFS的时间优势——迭代深入搜索\n启发式图搜索策略（图知识表示、图搜索）\n启发式策略\n“启发”（heuristic）：关于发现和发明操作算子及搜索方法的研究。\n在状态空间搜索中，启发式被定义成一系列操作算子，并能从状态空间中选择最有希望到达问题解的路径。\n启发式策略：利用与问题有关的启发信息进行搜索。按照什么顺序考察状态空间图的节点。\n启发式搜索应用于博弈、机器学习、数据挖掘和智能检索等。\n在状态空间搜索中，启发式被定义成一系列操作算子，并能从状态空间中选择\n最有希望\n到达问题解的路径。\n启发式策略：利用与问题有关的启发信息进行搜索。按照什么\n顺序\n考察状态空间图的节点。\n运用启发式策略的两种基本情况：\n（1）由于问题陈述和数据获取方面固有的\n模糊性\n（模糊理论），可能会使它没有一个确定的解。\n（2）虽然一个问题可能有确定解，但是其\n状态空间特别大\n，搜索中生成扩展的状态数会随着搜索的深度呈指数级增长。（穷尽式搜索、无解）\n（3）两部分：\n启发方法（剪枝）和搜索状态空间的算法\n。\n启发式策略的运用：剪枝（棋盘对称性）以减少状态空间的大小。\n棋局走法 9！—— 3*8！（第一步3种走法）——3+12*7！\n启发性知识：与被求解问题自身特性相关的知识，包括被求解问题的\n解特性、解分布规律\n和实际求解问题的经验和技巧等，对应问题求解的控制性知识。\n启发函数：实现启发式搜索，需要把启发性知识函数表示，通过函数计算评价选择价值大小，指导搜索过程。\n求解问题中能利用的大多是非完备的启发信息\n启发信息的分类：\n（1）陈述性启发信息：精准描述状态，缩小问题状态空间。\n（2）过程性启发信息：以规律性知识构造操作算子。\n（3）控制性启发信息：搜索策略、控制结构等知识。\n利用控制性的启发信息的情况：\n（1）没有任何控制性知识作为搜索的依据，因而搜索的每一步完全是随意的。\n（2）有充分的控制知识作为依据，因而搜索的每一步选择都是正确的，但这是不现实的。\n启发函数的设计：\n在实际设计过程中，启发函数是用来\n估计搜索树节点\nx\n与目标节点接近程度\n的一种函数，通常记为h(x)。启发函数可以是：\n（1）一个结点到目标结点的某种\n距离或差异\n的量度；\n（2）一个结点处\n在最佳路径上的概率\n；\n启发式搜索：用启发函数来导航，其搜索算法就要在状态图一般搜索算法基础上再\n增加启发函数值的计算与传播过程，并且由启发函数值来确定节点的扩展顺序\n。分为全局\n择优搜索\n和局部择优搜索。\n全局择优搜索基本思想：\n在OPEN表中保留所有已生成而未考察的节点，并用启发函数h(x)对它们\n全部进行估价，从中选出最优节点进行扩展\n，而不管这个节点出现在搜索树的什么地方。\n局部(子节点)择优搜索基本思想：\n在启发性知识导航下的深度优先搜索，在OPEN表中保留所有已生成而未考察的结点，对其中\n新生成的每个子结点\nx\n计算启发函数\n，从全部\n子结点中选出最优结点\n进行扩展，其选择下一个要考察结点的范围是刚刚生成的全部子结点.\n步1  把附有f（ S0 ）的初始结点S0放入OPEN表中；\n步2  若OPEN表为空，则搜索失败，退出；\n步3  否则，移出OPEN表中第一个结点N放入CLOSED表中，顺序编号n；\n步4  若目标结点Sg＝N，则搜索成功，利用CLOSED表中的返回指针找出S0到N的路径即为所求解，退出。\n步5  若N不可扩展，则转步2；\n6 扩展N，计算N的每个子结点x的函数值，并将N所有\n子结点\nx\n配以指向N\n的返回指针后放入OPEN\n表\n中，依据启发函数对结点的计算，再对OPEN表中所有\n结点\n/\n子结点按其启发函数值的大小以升序排列\n，转步2。移出OPEN表中第一个结点N放入CLOSED表中\n在全局择优和局部择优搜索算法中，没有考虑从初始结点到当前结点已经付出的实际代价。在很多实际问题中，已经付出的实际代价是必须考虑的，如TSP问题等。将两者同时考虑，用于指导搜索的算法称为A算法和A*算法。\n启发信息和估价函数：估价函数的任务就是估计待搜索结点的“有希望”程度，并依次给它们排定次序（在open表中）。\n从初始结点经过n结点到达目的结点的路径的最小代价估计值\ng(n)代价函数表示从\n初始结点到\nn\n结点\n的实际代价，越小越靠近初始结点，利于搜索的\n横向\n发展，可提高搜索\n完备性\n，但影响搜索效率。\nh(n)启发函数表示从\nn\n结点到目的结点的最佳路径的估计\n代价，越小越靠近目标结点，利于搜索的\n纵向\n，可提高搜索\n效率\n，影响完备性。\n一般地，在f（n）中，g的比重越大，越倾向于宽度优先搜索方式，而h的比重越大，表示启发性能越强。f(n)=g(n)+w h(n),调整w的值，使结果偏重效率或完备性\n对估价函数\nf（x） ＝g（x）＋h（x）\n令其中\nh\n（x\n）=0\n时，得到代价树的\n非启发式\n搜索算法。\n按对节点的\n考察范围\n不同，可分为两种搜索策略：\n分支界限法\n将全局择优搜索算法中的h(x)替换为g(x)，可得到分支界限法。\n瞎子爬山法\n将局部择优搜索算法中的h(x)替换为g(x) , 可得到瞎子爬山法\nA搜索算法\n启发式图搜索法的基本特点：如何寻找并设计一个与问题有关的h(n)及构出f(n)=g(n)+ h(n),，  然后以f(n)的大小来排列待扩展状态的次序，每次选择 f(n)值最小者进行扩展。\nopen表：保留所有已生成而未扩展的状态。\nclosed表：记录已扩展过的状态。\n进入open表的状态是根据其估值的大小插入到表中合适的位置，每次从表中优先取出启发估价函数值最小的状态加以扩展。\nopen：=[start]；closed：=[ ]；f(s)：=g(s)+h(s)；     *初始化\nwhile open≠[ ] do\nbegin\n从open表中删除第一个状态，称之为n；\nif n=目的状态  then return(success)；\n生成n的所有子状态；\nif  n没有任何子状态 then continue；\nfor n的每个子状态do\ncase子状态is not already on open表or closed表；\nbegin\n计算该子状态的估价函数值；\n将该子状态加到open表中；\nend；\ncase子状态is already on open表：\nif该子状态是沿着一条比在open表已有的更短路径而到达\nthen 记录更短路径走向及其估价函数值；\ncase子状态is already on closed表：\nif该子状态是沿着一条比在closed表已有的更短路径而到达then\nbegin\n将该子状态从closed表移到open表中；\n记录更短路径走向及其估价函数值；\nend；\ncase end；\n将n放入closed表中；\n根据估价函数值，从小到大重新排列open表；\nend；                                         *open表中结点已耗尽\nreturn(failure)；\nend.\n八数码：以“不在位”的将牌数作为启发信息的度量。\nh*(n)：为状态 到目的状态的最优路径的代价\nA*搜索算法及其特性分析:\n如果某一问题有解，那么利用A*搜索算法对该问题进行搜索则一定能搜索到解，并且一定能搜索到最优的解而结束。\n上例中的八数码A搜索树也是A*搜索树，所得的解路（s，B，E，I，K，L）为最优解路，其步数为状态L（5）上所标注的5 。\n1. 可采纳性\n当一个搜索算法在最短路径存在时能保证找到它，就称它是可采纳的。\n2. 单调性\n搜索算法的单调性：在整个搜索空间都是局部可采纳的。一个状态和任一个子状态之间的差由该状态与其子状态之间的实际代价所限定。\n3. 信息性\n在两个A*启发策略的h1h2中，如果对搜索空间中的任一状态n都有h1(n)\u003c=h2(n)，就称策略h1有更多的信息性。","data":"2018年12月20日 20:02:48"}
{"_id":{"$oid":"5d343b3962f717dc0659b3be"},"title":"word2vec 在 非 自然语言处理 (NLP) 领域的应用","author":"爱跑咪","content":"word2vec 本来就是用来解决自然语言处理问题的，它在 NLP 中的应用是显然的。\n比如，你可以直接用它来寻找相关词、发现新词、命名实体识别、信息索引、情感分析等；你也可以将词向量作为其他模型的输入，用于诸如文本分类、聚类等各种自然语言处理问题。\n事实上，word2vec 的思想和工具，还可以应用于自然语言处理之外的其他领域。一个词，无非就是个符号；句子是词的序列，无非也就是个符号序列。如果我们能够在其他的应用场景中，构造出一些符号，还有这些符号形成的序列，那我们就可以试一把 word2vec。\n下面是，根据网络上的资料，整理的 word2vec 在自然语言处理领域之外的一些应用。\n【社交网络】\n应用场景：在社交网络中，给当前用户推荐 他/她 可能关注的大V\n映射关系：每一个大V 就是一个词；将每个用户关注的大V，按照关注的顺序排列，形成文章\n【App 商店】\n\n应用场景：App 商店中，向用户推荐感兴趣的 App\n映射关系：每个 App 就是一个词；将每个用户下载的 App，按照下载的顺序排列，形成文章\n【广告系统】\n应用场景：广告主在媒体网站上打广告，媒体网站提供一个后台管理系统，可以让广告主自行决定要将广告推荐给哪些目标人群。\n映射关系：每一个页面就是一个词；将每个用户浏览的页面，按照浏览的顺序排列，形成文章。\n这样，根据训练后的词向量，就可以计算出页面之间的相关程度。\n那目标用户怎么计算呢？浏览与广告主的广告页 相关的页面 的用户 就是广告主潜在的 目标用户。把这些用户推荐给广告主就可以了。\n应用场景：广告系统中广告主上线了一支新广告，如何估算用户对新广告的 CTR（Click-Through-Rate），即点击通过率。\n映射关系：和上面给广告主推荐目标用户一样的做法，可以计算出每个广告页对应的向量\n然后，对这些广告页做一个聚类，把相似的广告页聚在一个簇中。用新广告所在簇的 CTR 来近似新广告的 CTR。\n【向量快速检索】\n综合以上各种应用，将各种文档转换成向量之后，常见一个基本操作就是输入一个文档（对应的向量），寻找和它最相关的 top k 个文档（对应的向量）。如果要所有文档都比对一遍的话，那时间复杂度就是 O(n)。这在实际的工程应用中就太慢了。因此，需要借助 redis，或者引入 kd-tree, simhash, 聚类等算法来加速检索。\n参考：\nword2vec在工业界的应用场景\n深度学习word2vec笔记之应用篇\nword2vec有什么应用\nA non-NLP application of Word2Vec\nWord2Vec with Non-Textual Data\n【原文链接】http://www.ipaomi.com/2017/09/22/word2vec-在-非-自然语言处理-nlp-领域的应用/","data":"2017年10月17日 17:37:17"}
{"_id":{"$oid":"5d343b3962f717dc0659b3c0"},"title":"如何自信地回答别人问你什么是人工智能","author":"小发猫","content":"程序员如何自信的回答普通人问你什么是人工智能，因为我被问过，答不出来感觉很丢脸，当然，只是个人觉得，不是说程序员就要懂。人工智能试图理解智能的本质，并产生一种新的智能机器，它可以以类似于人类智能的方式作出反应。该领域的研究包括机器人技术，语音识别，图像识别，自然语言处理和专家系统。自人工智能诞生以来，理论和技术日趋成熟，应用领域不断扩大。可以预见，在未来几年内，它将进入“人工智能时代”。\n\n人工智能现在遍布全世界，并在日常生活中经历了巨大的变化。这些AI不是科幻电影中的机器人，它们具有自我意识并计划摧毁世界的邪恶。相反，我们的智能手机，智能家居，银行信用卡管理员和智能汽车等产品在我们每天生活的产品和服务中使用AI。\n人工智能将通过促进自动驾驶汽车的发展，改进医学图像分析，促进更好的医疗诊断和个性化医疗，带来重大的社会转型。人工智能也将成为支持未来技术发展的基本资源，就像电力和网络一样。但对于大多数人来说，人工智能仍然非常奇怪，充满了神秘色彩。\n那么让我们来谈谈今天人工智能最重要的功能。——模式识别有效。我希望通过简短而简洁的介绍帮助您理解这一领域。\n人工智能是一门严谨的科学，而不是一种无所不能的神话。媒体过度夸大报道人工智能的功能，提倡威胁论是不负责任的。人工智能的目标是设计一个具有智能的机器，其中算法和技术基于人脑的当前研究结果。今天许多流行的AI系统使用人工神经网络来模拟非常简单的互连元素的网络，有点像大脑中的神经元。这些网络可以通过调整单元之间的连接来学习经验，这个过程类似于通过修改神经元之间的连接而学习的人和动物大脑。神经网络可以学习模式识别，翻译语言，学习简单的逻辑推理，甚至创建图像或形成新的设计。其中，模式识别是一项特别重要的功能，因为AI非常擅长识别海量数据中隐藏的模式，这对于依赖经验和知识的人来说并不容易。这些程序运行在具有数百万单位和数十亿连接的神经网络上。我们现在可以创建的“智能”由这些电子神经元网络组成。\n机器没有人体器官和大脑，它们可以很好地协同工作。例如，当我们看到一只狗时，我们会很快判断它是什么动物以及它是什么类型的动物。这个看似简单的过程对于机器来说非常困难。人类获得这种力量的能力也来源于数亿年来生物学的进化过程。机器了解世界的方式是通过模型，需要通过复杂的算法和数据建立模型，这样机器就可以获得简单的感知和判断能力。以下描述了深度学习系统中最重要的算法之一，即——卷积神经网络。如果你以前对AI有一些了解，那么你一定听说过这个概念。该算法涉及人类和其他动物大脑视觉皮层结构的研究。简要介绍这种特殊类型的人工神经网络，它使用感知器，机器学习单元算法来监督数据的学习和分析。适用于图像处理，自然语言处理和其他类型的认知任务。与其他类型的人工神经网络一样，卷积神经网络具有输入层，输出层和各种隐藏层。其中一些层是复卷的，并使用数学模型将结果传递给连续的层。该过程模仿人类视觉皮层中的一些动作，因此称为卷积神经网络或CNN。例如，当我们人类看到猫和狗时，虽然它们的大小相似，但我们可以立即将它们与猫和狗区分开来。对于计算机，图像只是一堆数据。通过神经网络的第一层中的特征来检测对象的轮廓。神经网络的下一层将检测由这些简单模式组合形成的简单形状，例如动物眼睛和耳朵。下一层将检测由这些形状组合形成的物体的某些部分，例如猫和狗的头部或腿部。神经网络的最后一层将检测这些部分的组合：完整的猫，完整的狗，等等。神经网络的每一层将执行图像组合分析和特征检测，以判断和组合并将结果传递给下一层神经网络。所使用的神经网络的实际深度将比该示例深得多，因此神经网络可以以这种分层方式执行复杂的模式识别。\n只要有大量标记的样本数据库，就可以在神经网络上进行特征训练。它对于识别图像，视频，语音，音乐甚至文本特别有用。为了很好地训练AI的机器视觉，我们需要提供这些神经网络所标记的大量图像数据。神经网络学习将每个图像与其对应的标签相关联。您还可以将之前从未见过的图像与相应的标签配对。这样的系统可以对各种图像进行分类并识别照片中的元素。同时，神经网络在语音识别和文本识别中也非常有用。自动驾驶汽车和最新的医学图像分析系统也是关键组件，因此您可以看到神经网络的使用非常广泛和有效。最初，有必要依靠手工标记大量有效数据来完成知识输入。现在，通过运行海量数据，神经网络可以自学。大大增强的人工智能应用范围降低了使用门槛。人类的大脑与动物大不相同，在进化过程中具有高度的专业性和适应性。目前的人工智能系统远没有人类拥有的看似普遍的智能。人工智能的更先进的发展将在后面讨论，我们仍然关注现在实施的人工智能的基本原则。\n人工智能相关阅读：\n游戏 的人工智能时代悄悄来临\nAI已经发展61年人工智能已经到了春天\n人工智能NLP需要了解这些东西\nnlg 自然语言生成有写什么项目\n“写作神器”人工智能写作软件有哪些？\n2018年NLP技术学习总结\n伪原创文章的软件","data":"2019年03月19日 13:51:53"}
{"_id":{"$oid":"5d343b3a62f717dc0659b3c2"},"title":"Python自然语言处理实战（3）：中文分词技术","author":"CopperDong","content":"3.1、中文分词简介\n在英文中，单词本身就是“词”的表达，一篇英文文章就是“单词”加分隔符（空格）来表示的，而在汉语中，词以字为基本单位的，但是一篇文章的语义表达却仍然是以词来划分的。\n自中文自动分词被提出以来，历经将近30年的探索，提出了很多方法，可主要归纳为“规则分词”、“统计分词”和“混合分词”这三个主要流派。\n3.2、规则分词\n基于规则的分词是一种机械分词方法，主要是通过维护词典，在切分语句时，将语句的每个字符串与词表中的词进行逐一匹配，找到则切分，否则不予切分。按照匹配切分的方式，主要有正向最大匹配法、逆向最大匹配法以及双向最大匹配法三种方法。\n正向最大匹配法（Maximum Match Method, MM法）：假定分词词典中的最长词有i个汉字字符，则用被处理文档的当前字串中的前i个字作为匹配字段，查找字典。若字典中存在这样的一个i字词，则匹配成功，匹配字段被作为一个词切分出来。如果词典中找不到这样的一个i字词，则匹配失败，将匹配字段中的最后一个字去掉，对剩下的字串重新进行匹配处理。如此进行下去，直到匹配成功，即切分出一个词或剩余字串的长度为零为止。这样就完成了一轮匹配，然后取下一个i字字串进行匹配处理，直到文档被扫描完为止。\n逆向最大匹配（Reverse Maxinum Match Method，RMM法）的基本原理和MM法相同，不同的是分词切分的方向与MM法相反。\n双向最大匹配法（Bi-direction Matching method）是将正向最大匹配法得到的分词结果和逆向最大匹配法得到的结构进行比较，然后按照最大匹配原则，选取词数切分最少的作为结构。据SumM.S.和Benjamin K.T.（1995）的研究表明，中文中90.0%的句子两种切分方法得到的结果不一样，但其中必有一个是正确的（歧义检测成功），只有不到1.0%的句子，使用正向最大匹配法和逆向最大匹配法的切分虽重合却是错的，或者正向最大匹配法和逆向最大匹配法切分不同但两个都不对（歧义检测失败）。这正是双向最大匹配法在实用中文信息处理系统中得以广泛使用的原因。\n# 逆向最大匹配 class IMM(object): def __init__(self, dic_path): self.dictionary = set(); self.maximum = 0 # 读取词典 with open(dic_path, 'r', encoding=\"utf8\") as f: for line in f: line = line.strip() if not line: continue self.dictionary.add(line) self.maximum = max(self.maximum, len(line)) def cut(self, text): result = [] index = len(text) while index \u003e 0: word = None for size in range(self.maximum, 0, -1): if index - size \u003c 0: continue piece = text[(index - size):index] if piece in self.dictionary: word = piece result.append(word) index -= size break if word is None: index -= 1 return result[::-1] def main(): text = \"南京市长江大桥\" tokenizer = IMM('./data/imm_dic.utf8') print(tokenizer.cut(text))\n3.3 统计分词\n其主要思想是把每个词看做是由词的最小单位的各个字组成的，如果相连的字在不同的文本中出现的次数越多，就证明这相连的字很可能就是一个词。一般要做如下两步操作：\n1）建立统计语言模型：为长度为m的字符串确定其概率分布P(w1, w2, ... , wm)。当文本过长时，右部从第三项起的每一项计算难度都很大。为了解决该问题，有人提出n元模型（n-gram model）降低该计算难度。所谓n元模型就是在估算条件概率时，忽略距离大于等于n的上文词的影响，因此P(wi | w1, w2, ... , wi-1）的计算可简化为：P(wi | w1, w2, ... , wi-1) ~= P(wi | wi-(n-1), ... , wi-1)。\n显然当n\u003e=2时，该模型是可以保留一定的词序信息的，而且n越大，保留的词序信息越丰富，但计算成本也呈指数级增长。一般使用频率计数的比例来计算n元条件概率：\nP(wi | wi-(n-1) , ... , wi-1) = count(wi-(n-1), ... , wi-1, wi) / count(wi-(n-1), ... , wi-1 )\n由于会出现分子分母为零的情况，一般在n元模型中需要配合相应的平滑算法解决，如拉普拉斯平滑算法等。\n2）对句子进行单词划分，然后对划分结果进行概率计算，获得概率最大的分词方式。这里就用到了统计学习算法，如隐含马尔可夫（HMM）、条件随机场（CRF）等。\nHMM是将分词作为字在字串中的序列标注任务来实现的。其基本思路是：每个字在构造一个特定的词语时，都占据着一个确定的构词位置（即词位）。现规定每个字最多只有四个构词位置，即B（词首）、M（词中）、E（词尾）和S（单独成词）。\nmax = max P(o1o2...on| r1r2...rn)\n假设每个字的输出仅仅与当前字有关，就能得到：P(o1o2...on| r1r2...rn)=P(o1|r1)P(o2|r2)...P(on|rn)，但该方法完全没有考虑上下文，且会出现不合理的情况。\nHMM就是用来解决该问题的一种方法。P(o | r) = P(o,r)/P(r) = P(r|o)P(o)/P(r)，其中P(r)为常数，因此求最大化P(r|o)P(o)。\n在HMM中，求解max(P(r|o)P(o))的常用方法是Veterbi算法。它是一种动态规划方法，核心思想是：如果最终的最优路径经过某个oi，那么从初始节点到oi-1点的路径必然也是一个最优路径---因此每个节点oi只会影响前后两个P(oi-1 | oi) 和 P(oi | oi+1)。\n# -*- coding: utf-8 -*- class HMM(object): def __init__(self): pass def try_load_model(self, trained): pass def train(self, path): pass def viterbi(self, text, states, start_p, trans_p, emit_p): pass def cut(self, text): pass class HMM(object): def __init__(self): import os # 主要是用于存取算法中间结果，不用每次都训练模型 self.model_file = \"./data/hmm_model.pkl\" self.state_list = ['B', 'M', 'E', 'S'] self.load_para = False # 用于加载已计算的中间结果，当需要重新训练时，需初始化清空结果 def try_load_model(self, trained): if trained: import pickle with open(self.model_file, 'rb') as f: self.A_dic = pickle.load(f) self.B_dic = pickle.load(f) self.Pi_dic = pickle.load(f) self.load_para = True else: # 状态转移概率（状态-\u003e状态的条件概率） self.A_dic = {} # 发射概率（状态-\u003e词语的条件概率） self.B_dic = {} # 状态的初始概率 self.Pi_dic = {} self.load_para = False # 采用人民日报的分词语料，通过统计，得到HMM所需的初始概率、转移概率以及发射概率 def train(self, path): self.try_load_model(False) Count_dic = {} #求p(o) # 初始化参数 def init_parameters(): for state in self.state_list: self.A_dic[state] = {s: 0.0 for s in self.state_list} self.Pi_dic[state] = 0.0 self.B_dic[state] = {} Count_dic[state] = 0 def makeLabel(text): out_text = [] if len(text) == 1: out_text.append('S') else: out_text += ['B'] + ['M'] * (len(text) - 2) + ['E'] return out_text init_parameters() line_num = -1 # 观察者集合，主要是字以及标点等 words = set() with open(path, encoding=\"utf-8\") as f: for line in f: line_num += 1 line = line.strip() if not line: continue word_list = [i for i in line if i != ' '] words |= set(word_list) # 更新字的集合 linelist = line.split() line_state = [] for w in linelist: line_state.extend(makeLabel(w)) #print(word_list) #print(line_state) assert len(word_list) == len(line_state) for k, v in enumerate(line_state): Count_dic[v] += 1 if k==0: self.Pi_dic[v] += 1 #每个句子的第一个字的状态，用于计算初始状态概率 else: self.A_dic[line_state[k-1]][v] += 1 #计算转移概率 # 计算发射概率 self.B_dic[line_state[k]][word_list[k]] = self.B_dic[line_state[k]].get(word_list[k], 0) + 1.0 self.Pi_dic = {k: v*1.0/line_num for k, v in self.Pi_dic.items()} self.A_dic = {k: {k1: v1 / Count_dic[k] for k1, v1 in v.items()} for k, v in self.A_dic.items() } # 加1平滑 self.B_dic = {k: {k1: (v1 + 1) / Count_dic[k] for k1, v1 in v.items()} for k,v in self.B_dic.items()}#序列化 import pickle with open(self.model_file, 'wb') as f: pickle.dump(self.A_dic, f) pickle.dump(self.B_dic, f) pickle.dump(self.Pi_dic, f) return self def viterbi(self, text, states, start_p, trans_p, emit_p): print(start_p) #print(trans_p) #print(emit_p) V = [{}] path = {} for y in states: V[0][y] = start_p[y] * emit_p[y].get(text[0], 0) path[y] = [y] for t in range(1, len(text)): V.append({}) newpath = {} print(text[t]) #检验训练的发射概率矩阵中是否有该字 neverSeen = text[t] not in emit_p['S'].keys() and \\ text[t] not in emit_p['M'].keys() and \\ text[t] not in emit_p['E'].keys() and \\ text[t] not in emit_p['B'].keys() for y in states: emitP = emit_p[y].get(text[t], 0) if not neverSeen else 1.0 #设置未知字单独成词 (prob, state) = max( [(V[t - 1][y0] * trans_p[y0].get(y, 0) * emitP, y0) for y0 in states if V[t - 1][y0] \u003e 0]) V[t][y] = prob newpath[y] = path[state] + [y] path = newpath if emit_p['M'].get(text[-1], 0) \u003e emit_p['S'].get(text[-1], 0): (prob, state) = max([(V[len(text) - 1][y], y) for y in ('E', 'M')]) else: (prob, state) = max([(V[len(text) - 1][y], y) for y in states]) return (prob, path[state]) def cut(self, text): import os if not self.load_para: self.try_load_model(os.path.exists(self.model_file)) prob, pos_list = self.viterbi(text, self.state_list, self.Pi_dic, self.A_dic, self.B_dic) begin, next = 0, 0 for i, char in enumerate(text): pos = pos_list[i] if pos == 'B': begin = i elif pos == 'E': yield text[begin: i+1] next = i+1 elif pos == 'S': yield char next = i+1 print(next) if next \u003c len(text): yield text[next:] hmm = HMM() hmm.train('./data/trainCorpus.txt_utf8') text = '这是一个非常棒的方案！' res = hmm.cut(text) print(text) print(str(list(res)))\n这是一个非常棒的方案！ {'M': 0.0, 'S': 0.41798844132394497, 'E': 0.0, 'B': 0.5820149148537713} 是 一 个 非 常 棒 的 方 案 ！ 0 2 2 4 4 6 7 8 8 10 11 ['这是', '一个', '非常', '棒', '的', '方案', '！']\n3.5 中文分词工具---jieba\njieba分词结合了基于规则和基于统计这两类方法。首先基于前缀词典进行词图扫描，前缀词典是指词典中的词按照前缀包含的顺序排列，可以快速构建包含全部可能分词结果的有向无环图，这个图中包含多条分词路径，有向是指全部的路径都始于第一个字、止于最后一个字，无环是指节点之间不构成闭环。基于标注语料，使用动态规划的方法可以找出最大概率路径，并将其作为最终的分词结果。对于未登录词，jieba使用了基于汉字成词的HMM模型，采用了Viterbi算法进行推导。\n实战之高频词提取：高频词一般是指文档中出现频率较高且非无用的词语，其一定程度上代表了文档的焦点所在。针对单篇文档，可以作为一种关键词来看。对于如新闻这样的多篇文档，可以将其作为热词，发现舆论焦点。需要去掉标点符号和停用词。\n下面对搜狗实验室的新闻数据，进行高频词的提取\n# read data def get_content(path): with open(path, 'r', encoding='gbk', errors='ignore') as f: content = '' for l in f: l = l.strip() content += l return content def get_TF(words, topK=10): tf_dic = {} for w in words: tf_dic[w] = tf_dic.get(w, 0) + 1 return sorted(tf_dic.items(), key=lambda x: x[1], reverse=True)[:topK] def stop_words(path): with open(path) as f: return [l.strip() for l in f] def main(): import glob import random import jieba files = glob.glob('./data/news/C000013/*.txt') corpus = [get_content(x) for x in files] sample_inx = random.randint(0, len(corpus)) #split_words = list(jieba.cut(corpus[sample_inx])) #停用词 split_words = [x for x in jieba.cut(corpus[sample_inx]) if x not in stop_words('./data/stop_words.utf8')] print('yangben 1: ' + corpus[sample_inx]) print('/ '.join(split_words)) print('topK(10): ' + str(get_TF(split_words)))\ntopK(10): [('前列腺', 34), ('食品', 7), ('做', 7), ('男人', 6), ('排尿', 6), ('充血', 5), ('引起', 5), ('前列腺癌', 5), ('导致', 5), ('压力', 4)]\n有时需要定制自己的领域词典，用以提升分词的效果。\njieba.load_userdict('./data/user_dict.utf8')\n要求格式一般为：词语、词频（可省略）、词性（可省略），用空格隔开，顺序不可颠倒，需为utf8编码","data":"2018年07月15日 17:49:18"}
{"_id":{"$oid":"5d343b3a62f717dc0659b3c4"},"title":"人工智能（Artificial Intelligence）","author":"qq_35774189","content":"今天一个完全不懂人工智能和计算机的小伙伴问我自然语言处理的工作怎么样？我说这个是属于人工智能的一部分，他就搞不清楚这个人工智能和计算机是什么关系？我告诉他计算机是工具，人工智能一般是有落地产品的，细节上的我也说不好，因此，我就对具体的人工智能重新认识了下。\n人工智能是一门基于计算机科学，生物学，心理学，神经科学，数学和哲学等学科的科学和技术。人工智能的一个主要推动力要开发与人类智能相关的计算机功能，例如推理，学习和解决问题的能力。\n人工智能的主要研究方向有：\n计算机视觉（图像识别，视频识别），具体应用有人脸识别，步态识别，无人驾驶汽车等等\n自然语言处理技术（机器翻译，语音识别，文本挖掘），具体siri，谷歌翻译\n数据挖掘（推荐和预测），具体电子商务的商品推荐，计算广告，社交网络分析，预测一些趋势（股市走向，天气变化）\n人工智能的特点：\n1、人工智能是人为创造的只能机械产品。将人工智能拆开解读，可以分为两部分，分别是“人工”和“智能”，即是指人为创造的智能机械产品。现阶段的人工智能产品非常依赖人，因为它只能够被人们创造出来，不能自动生成，除此以外，在使用的过程中还必须接受人为的指令，才能够有效准确地执行各种动作，准确有效地运行。\n2、现阶段的人工智能只具备机械思维。人类拥有情感 ，而人工智能产品并不具备情感。人工智能产品之所以能够有效有序地代替人们完成各种工作，是因为人工智能产品具备机械思维，拥有强大的计算、分析和决策能力，这是人类所不具备的强大能力。\n3、人工智能机器人能够高效地代替人们工作。传统的机器人或只能产品没有办法高效地代替人们工作，因为他们的智能程度极低，而装载了人工智能系统的机器人能够高效地代替人们的工作。只是因为人工智能机器人具备用户友好性和环境适应性，能够更加了解用户的需求，也能够根据周围环境的变化做出最正确最恰当的反应。\n人工智能历史：\n1940-1950：\n一帮来自数学，心理学，工程学，经济学和政治学领域的科学家在一起讨论人工智能的可能性，当时已经研究出了人脑的工作原理是神经元电脉冲工作。\n1950-1956：\n伦·图灵（Alan Turing）发表了一篇具有里程碑意义的论文，其中他预见了创造思考机器的可能性。\n重要事件： 曼彻斯特大学的Christopher Strachey使用Ferranti Mark 1 机器写了一个跳棋程序， Dietrich Prinz写了一个国际象棋程序。\n1956：\n达特茅斯会议，人工智能诞生。约翰麦卡锡创造了人工智能一词并且演示了卡内基梅隆大学首个人工智能程序。\n1956-1974：\n推理研究，主要使用推理算法，应用在棋类等游戏中。自然语言研究，目的是让计算机能够理解人的语言。日本，早稻田大学于1967年启动了WABOT项目，并于1972年完成了世界上第一个全尺寸智能人形机器人 WABOT-1 。\n1974-1980：\n由于当时的计算机技术限制，很多研究迟迟不能得到预期的成就，这时候AI处于研究低潮。\n1980-1987：\n在20世纪80年代，世界各地的企业采用了一种称为“ 专家系统 ” 的人工智能程序，知识表达系统成为主流人工智能研究的焦点。在同一年，日本政府通过其第五代计算机项目积极资助人工智能。1982年，物理学家John Hopfield发明了一种神经网络可以以全新的方式学习和处理信息。\n1987-1993：\n第二次AI研究低潮。\n1993-2011 ：\n出现了智能代理，它是感知周围环境，并采取最大限度提高成功的机会的系统。这个时期自然语言理解和翻译，数据挖掘，Web爬虫出现了较大的发展。\n里程碑的事件：1997年深蓝击败了当时的世界象棋冠军Garry Kasparov。2005年，斯坦福大学的机器人在一条没有走过的沙漠小路上自动驾驶131英里。\n2011年至今：\n在深度学习，大数据和强人工智能的发展迅速。\n人工智能的发展对人类的影响：\n1、人工智能给人类带来新生\n人工智能需要人类操作或下达命令，才能执行相应地动作，完成相应的任务，如果没有人类，那么人工智能仅仅是一堆破铜烂铁。\n2、 人工智能将进一步发展，更好地帮助人类\n人工智能可能有助于太空殖民或者地球上的数字社会\n3、人工智能将丰富人类的精神世界\n人工智能能够帮助人们完成更多的工作，全面解放人手，人类的生活将变得悠闲而惬意，不再需要进行大量繁重的工作。\n同时参考：https://baijiahao.baidu.com/s?id=1611014130425035698\u0026wfr=spider\u0026for=pc","data":"2018年12月26日 10:34:16"}
{"_id":{"$oid":"5d343b3b62f717dc0659b3c6"},"title":"个人开发者如何通过人工智能盈利？","author":"GitChat的博客","content":"人工智能大背景和历史。\n关于人工智能的知识网络拓扑图及学习路线。\n阿尔法狗原理算法深入解析包含：\n阿尔法狗各模块详解：价值判断、专家网络、反向更新、强化学习、快速响应\n卷积神经网络（分层拆分计算，求无限接近值）+蒙特卡洛树搜索（选重要节点向后推断，得到最优值）\n阿尔法狗适用于哪些应用场景以及如何拿来用。\n个人如何开发一款人工智能应用。\n个人如何利用免费的人工智能工具与平台赚钱。\n实录提要：\n人工智能发展到什么程度会取代程序猿或者其他行业？\n现在自然语言处理达到什么地步了？\n虽然接触了 3 个月 AI，但是学的比较散，可以推荐一套系统的学习路线吗？\n前端开发转人工智能开发需要从哪方面入手，需要掌握什么技能？\n人工智能怎么检验自己的能力到哪种等级了？\n适合个人开发者的具体的盈利模式及发展方向有哪些？\n学习人工智能开发要求数学掌握到什么程度，没学过高数的同学应该怎样学习？\n移动开发与人工智能结合的场景，因为移动设备计算能力有限，有哪些应用点？\n前端开发和 AI 有什么结合点吗？\n人工智能是否会带来新的商业模式？\n人工智能与硬件、无线电、APP 或小程序有哪些结合应用前景？\n关于自然语言处理，目前世界上有什么前沿的论文或书籍推荐吗？\n盈利模式的话，数据能否成为盈利点，成功的AI项目中，是如何获取训练数据的？\n除了经典的书籍，AI 有哪些高质量论文和论坛可供持续学习？\n阅读全文: http://gitbook.cn/gitchat/activity/59ed5e07991df70ecd5a01e2\n您还可以下载 CSDN 旗下精品原创内容社区 GitChat App ，阅读更多 GitChat 专享技术内容哦。","data":"2018年04月12日 10:43:18"}
{"_id":{"$oid":"5d343b3b62f717dc0659b3c8"},"title":"《Python自然语言处理-雅兰·萨纳卡(Jalaj Thanaki)》学习笔记：09 NLU和NLG问题中的深度学习","author":"miniAI学堂","content":"09 NLU和NLG问题中的深度学习\n9.1　人工智能概览\n9.1.1　人工智能的基础\n9.1.2　人工智能的阶段\n9.1.3　人工智能的种类\n9.1.4　人工智能的目标和应用\n9.2　NLU和NLG之间的区别\n9.2.1　自然语言理解\n9.2.2　自然语言生成\n9.3　深度学习概览\n9.4　神经网络基础\n9.4.1　神经元的第一个计算模型\n9.4.2　感知机\n9.4.3　理解人工神经网络中的数学概念\n9.5　实现神经网络\n9.5.1　单层反向传播神经网络\n9.5.2　练习\n9.6　深度学习和深度神经网络\n9.6.1　回顾深度学习\n9.6.2　深度神经网络的基本架构\n9.6.3　NLP中的深度学习\n9.6.4　传统NLP和深度学习NLP技术的区别\n9.7　深度学习技术和NLU\n9.7.1 机器翻译\nMLT (EN to FR ) TensorFlow\nBrief Overview of the Contents\nData preprocessing\nBuild model\nTraining\nPrediction\n9.8　深度学习技术和NLG\n9.8.1　练习\n9.8.2　菜谱摘要和标题生成\n9.9　基于梯度下降的优化\n9.9.1 基本梯度下降\n9.9.2 随机梯度下降\n9.9.3 小批量梯度下降\n9.9.4 动量\n9.9.5 Nesterov加速梯度\n9.9.6 Adagrad\n9.9.7 adadelta\n9.9.8 Adam\n9.10　人工智能与人类智能\n9.11　总结\n\n在前面的章节中，我们已经看到了基于规则的方法和各种机器学习技术来解决NLP任务。在本章中，我们将看到机器学习技术,称为深度学习（DL）子集。在过去的四到五年里，神经网络和深度学习技术在人工智能领域引起了广泛的关注，因为许多技术巨头使用这些尖端技术来解决现实生活中的问题，这些技术的成果令人印象深刻。谷歌、苹果、亚马逊、OpenAI等科技巨头花费大量时间和精力为现实生活中的问题创造创新的解决方案。这些努力主要是为了发展人工通用智能，使世界成为人类更好的地方。\n我们首先要了解整个人工智能，总的来说，给你一个的概念，为什么深度学习现在正在高速发展。我们将在本章中讨论以下主题：\nNLU和NLG之间的区别\n神经网络基础\n使用各种深度学习技术构建NLP和NLG应用程序\n在了解了DL的基础知识之后，我们将接触到在深度学习领域中发生的一些最新的创新。那么，让我们开始吧！\n9.1　人工智能概览\n在本节中，我们将看到人工智能的各个方面以及深度学习与人工智能的关系。我们将看到人工智能的组成部分、人工智能的不同阶段和不同类型的人工智能；在本节的最后，我们将讨论为什么深度学习是实现人工智能最有希望的技术之一。\n9.1.1　人工智能的基础\n当我们谈论人工智能时，我们想到的是智能机器，这是人工智能的基本概念。人工智能是一个科学领域，它不断朝着使机器具有人类水平智能的方向发展。人工智能背后的基本思想是在机器中启用智能，以便它们也可以执行一些仅由人类执行的任务。我们正在尝试使用一些很酷的算法技术来实现机器中的人类级智能；在这个过程中，机器获取的任何类型的智能都是人工生成的。各种用于为机器生成人工智能的算法技术主要是机器学习技术的一部分。在进入核心机器学习和深度学习部分之前，我们将了解与人工智能相关的其他事实。人工智能受许多分支的影响；在图9.1中，我们将把那些严重影响人工智能的分支视为单个分支：\n\n首先，我们将看到人工智能的关键组成部分。这些组成部分对于我们理解世界的发展方向是非常有用的。据我所知，有两个组件，如图9.2所示：\n自动化\n自动化是人工智能的一个著名组成部分。全世界的人们都在高度自动化方面工作，我们在机器执行的自动化任务方面取得了巨大的成功。我们将查看一些足够直观的例子，以便您理解人工智能中的自动化概念。\n在汽车行业，我们使用自动机器人制造汽车。这些机器人遵循一套指令，执行特定的任务。在这里，这些机器人不是智能机器人，它们可以与人类互动、提问或回应人类。但这些机器人只是遵循一套指令来实现高速制造的高精度和高效率。所以这些机器人就是人工智能领域自动化的例子。\n另一个例子是DevOps领域。现在，DevOps正在使用机器学习来自动化许多人类密集型的过程，例如，为了维护内部服务器，DevOps团队在分析各种服务器日志后获得一系列建议，在获得建议后，另一个机器学习模型优先处理警报和建议。这种应用程序确实为DevOps团队节省了时间来按时交付大量工作。这些应用程序确实帮助我们理解自动化是人工智能的一个非常重要的组成部分。\n智力\n当我们说智力，作为人类，我们的期望真的很高。我们的目标是让机器了解我们的行为和情绪。我们还希望机器根据人类的行为做出智能的反应，机器产生的所有反应都应该是模仿人类智能的。我们希望从20世纪90年代中期开始实现这一目标。在全球范围内，许多研究人员、科学家团体和社区正在进行大量的酷的研究，以使机器像人类一样智能化。\n我们希望在获得智能之后，机器能够以更好的精度为人类完成大部分任务，这是一个单一的广泛的期望。在过去的45年中，我们已经开始成功地实现这一广泛的目标，因此，经过多年的努力，谷歌最近宣布，谷歌助手可以从人类身上听到自然语言，并能像人类一样准确地解释语音信号。另一个例子是，Facebook的研究小组进行了一项非常强大的研究，以建立一个善于对问题和答案进行推理的系统。特斯拉和谷歌的自动驾驶汽车是一个复杂的人工智能系统，但非常有用和智能。自动驾驶汽车和聊天机器人是窄人工智能的一部分。你也可以在网上找到很多其他的例子，这些例子时不时会出现。有些子组件可以作为信息的一部分。参见图9.3：\n\n智能是前面图中描述的所有组件的组合。所有这些成分——推理、学习、从经验中学习、解决问题、感知和语言智能——都是人类的天性，而不是机器的天性。所以我们需要能够为机器提供智能的技术。\n在学习本章后面将要使用的技术名称之前，让我们先了解人工智能的各个阶段。\n9.1.2　人工智能的阶段\n人工智能系统有三个主要阶段。我们将详细介绍以下几个阶段：\n机器学习\n机器智能\n机器意识\n在了解人工智能各个阶段的详细信息之前，请参阅图9.4：\n\n我们将从下到上，因此我们将首先了解机器学习阶段，然后了解机器智能，最后了解机器意识。\n机器学习\n在前面的章节中，您已经学习了很多关于机器学习的知识，但是我想在本章中给您一个人工智能的视角。\nML技术是一组解释如何生成或达到定义的输出的算法。这种算法被试图从经验中学习的智能系统所使用。使用ML算法的系统热衷于从历史数据或实时数据中学习。因此，在人工智能的这个阶段，我们关注学习模式或特定的算法\n使用我们提供给ML系统的特性从数据中得到的结构。为了说明这一点，让我们举个例子。\n假设您想要构建一个情绪分析应用程序。我们可以使用历史标记数据、手工制作的特性和朴素的Bayes ML算法。因此，我们可以拥有一个从其学习示例中学习到的智能系统——如何为看不见的新数据实例提供情感标签。\n机器智能\n机器智能又是一套算法，但大多数算法都严重受人脑学习和思考方式的影响。利用神经科学、生物学和数学，人工智能研究人员提出了一套高级算法，帮助机器从数据中学习，而不提供手工制作的特征。在此阶段，算法使用未标记或标记的数据。在这里，您只需定义最终目标，高级算法就可以找到实现预期结果的方法。\n如果您将我们在这个阶段使用的算法与传统的ML算法进行比较，那么主要的区别在于，在机器智能阶段，我们不会将手工制作的特征作为任何算法的输入。当这些算法受到人脑的启发时，算法本身就学习特征和模式并生成输出。目前，人工智能的世界正处于这个阶段。全世界的人们都使用这些先进的算法，似乎很有希望为机器实现类似人类的智能。\n利用人工神经网络和深度学习技术实现机器智能。\n机器意识\n机器意识是人工智能中讨论最多的主题之一，因为我们的最终目标是达到这里。\n我们希望机器学习人类的学习方式。作为人类，我们不需要太多的数据；我们不需要太多时间来理解抽象概念。我们从少量数据或没有数据中学习。大多数时候，我们从经验中学习。如果我们想建立一个和人类一样有意识的系统，那么我们应该知道如何为机器产生意识。然而，我们是否完全知道我们的大脑是如何工作和反应的，以便把这些知识转移到机器上，使它们像我们一样有意识？不幸的是，现在我们还没有意识到这一点。我们期望在这一阶段，机器在没有数据或数据量非常小的情况下学习，并利用自己的经验来实现定义的输出。\n9.1.3　人工智能的种类\n人工智能有三种类型，如下所示：\n弱人工智能\n通用人工智能\n人工超级智能\n弱人工智能\n弱人工智能（ANI）是一种人工智能，它涵盖了一些基本任务，如基于模板的聊天机器人、基本的个人助理应用程序，如苹果公司的Siri初始版本。\n这种智能主要集中在应用程序的基本原型设计上。这种类型的智能是任何应用程序的起点，然后您可以改进基本原型。您可以通过添加人工通用智能来添加下一层智能，但前提是最终用户确实需要这种功能。我们在第7章，NLP的基于规则的系统中也看到了这种基本聊天机器人。\n通用人工智能\n通用人工智能（AGI）是一种人工智能，用于构建能够执行人级任务的系统。我所说的人工级别的任务是什么意思？建造自动驾驶汽车等任务。谷歌自动驾驶汽车和特斯拉自动驾驶仪是最著名的例子。类人机器人也尝试使用这种人工智能。\nNLP级的例子是复杂的聊天机器人，它们忽略拼写错误和语法错误，并理解您的查询或问题。深度学习技术对于人类理解自然语言似乎非常有希望。我们现在正处在一个世界各地的人们和社区使用基本概念的阶段，通过相互参照对方的研究成果，尝试构建具有敏捷性的系统。\n人工超级智能\n实现人工超级智能（ASI）的方法对我们来说有点困难，因为在这种人工智能中，我们期望机器比人类更聪明，以便学习特定的任务，并且能够像人类在生活中一样执行多个任务。这种超级智能现在是我们的梦想，但我们正试图在这样一个机器和系统始终是人类技能的补充，不会对人类造成威胁。\n9.1.4　人工智能的目标和应用\n这是我们需要了解各个领域人工智能的目标和应用程序的时间和部分。这些目标和应用程序只是为了让您了解启用人工智能的应用程序的当前状态，但是如果您可以在任何领域想到一些疯狂但有用的应用程序，那么您应该尝试将其包括在这个列表中。您应该尝试在该应用程序中实现各种类型和阶段的人工智能。\n现在，让我们看看我们想要集成人工智能各个阶段并使这些应用程序启用人工智能的领域：\n推理\n机器学习\n自然语言处理\n机器人学\n实施一般情报\n计算机视觉\n自动化学习和调度\n语音分析\n您可以参考图9.5，它显示了许多不同的领域和相关的应用程序：\n\n支持人工智能的应用程序\n在这里，我将向您简要介绍启用人工智能的应用程序。一些应用程序也与NLP域相关：\n对任何系统进行推理都是非常令人兴奋的事情。在这方面，我们可以建立一个Q/A系统，利用推理得出问题的答案。\n如果我们能够对基于人工智能的系统进行推理，那么这些系统将非常擅长决策，并将改进现有的决策系统。\n在机器学习中，我们需要一个基于ML的应用程序的完美架构，它可以由机器自己决定。据我所知，这是一个支持人工智能的ML应用程序。\n当我们谈论人工智能的NLP应用程序时，我们真的需要能够理解人类自然语言的上下文并作出反应，表现得更像人类的NLP系统。\n类人机器人是描述人工智能系统的最佳应用。机器人应该获得感知，这是一个长期的人工智能目标。我认为，当我们谈论一般智能时，系统的反应应该更像人类。尤其是机器反应应该与人类的真实行为相匹配。在分析了某些情况后，机器的反应应该比人类相同或更好\n如今，计算机视觉有许多应用，为我们提供了可靠的证据，证明人工智能将很快在这一领域实现。这些应用包括物体识别、图像识别、使用图像识别技术检测皮肤癌、从机器生成面部图像、为图像生成文本（反之亦然）等。所有这些应用程序给出了人工智能驱动计算机视觉的具体证明。\n自动学习和日程安排是一种为您个人提供帮助并管理日程安排的构建系统。关于人工智能部分，我们真的希望系统的每个用户都能获得个性化的体验，因此自动化学习一个人的个人选择对于人工智能驱动的调度非常重要。为了实现这一目标，自动化学习系统还应该学习如何为特定用户选择最适合的模型。\n语言分析是nl的另一种形式，但不幸的是，我们在本书中没有讨论这个概念。在这里，我们讨论的是一个语音识别系统的潜在人工智能启用领域。通过使用这个语音识别区域启用人工智能，我们可以了解生成的人类环境和思维过程。\n一个人在的社会学、心理学和哲学的影响下。我们也可以预测他们的性格。在看到所有这些迷人的应用程序之后，我们想到了三个真正有趣的问题：什么是导致我们生产人工智能驱动系统的原因，为什么时间如此适合我们构建人工智能驱动系统，以及我们如何构建一个人工智能系统？\n自20世纪90年代中期以来，我们一直在尝试将智能引入机器。在这一阶段，研究人员和科学家给出了许多很酷的概念。例如，人工神经元，也被称为McCulloch-Pitts模型（MCP），受人脑的启发，这个概念的目的是理解人脑的生物工作过程，并从数学和物理的角度来表示这个过程。因此，对机器实现人工智能有一定的帮助。\n他们成功地给出了单个神经元如何工作的数学表示，但是这个模型有一个结果不适合用于训练目的。因此，研究人员Frank Rosenblatt在1958年的论文中提出了感知器，引入了动态权重和阈值概念。在此之后，许多研究者在早期概念的基础上发展了诸如反向传播和多层神经网络等概念。研究团体希望在实际应用中实现已开发的概念，第一位研究员Geoffrey Hinton演示了使用广义反向传播算法训练多层神经网络。从那时起，研究人员和社区开始使用这种通用模型，但在20世纪末，数据量比现在少，计算设备既慢又昂贵。所以我们没有得到预期的结果。然而，随着当时取得的成果，研究人员相信这些概念将被用来实现人工智能驱动的世界。现在我们有了大量的数据和计算设备，这些设备速度快、价格便宜，并且能够处理大量的数据。当我们在当前时代应用人工神经网络的旧概念开发通用机器翻译系统、语音识别系统、图像识别系统等应用时，我们得到了非常有前途的结果。让我们举个例子。谷歌正在利用人工神经网络开发一个通用的机器翻译系统，该系统将翻译多种语言。这是因为我们有大量可用的数据集和快速的计算能力，可以帮助我们使用ANN处理数据集。我们使用的神经网络不是一层或两层，而是多层的。取得的成果令人印象深刻，以至于每一个大型科技巨头都在使用深度学习模型来开发一个人工智能系统。据我所知，数据、计算能力和可靠的基础概念是开发人工智能驱动系统的关键组件。您可以参考图9.6了解神经网络的简要历史：\n\n图9.7将向您介绍神经网络的长期历史：\n\n现在让我们进入下一个问题：我们如何启用人工智能？答案是深度学习。这是使人工智能适用于非人工智能系统的最常用技术之一。在少数情况下，深度学习不用于启用人工智能，但在NLP领域，深度学习主要用于启用人工智能。为了发展一般智力，我们可以利用深度学习。我们从这项技术中得到了非常有希望的结果。在嘈杂的环境中，诸如生成人脸的机器能更准确地理解人类的语言，自动驾驶汽车、问答系统的推理等实验只是其中的一小部分。深度学习技术利用大量的数据和高计算能力来训练系统对给定数据的学习。当我们在大量数据上应用正确的深度学习模型时，我们会得到一个神奇的、令人印象深刻的、有希望的结果。这就是为什么深度学习在当今引起了很多轰动。所以我想现在你知道为什么深度学习是人工智能世界的流行词了。\n9.2　NLU和NLG之间的区别\n在第3章，我们已经看到了NLU和NLG的定义，细节，以及句子理解结构中的差异。在本节中，我们将从启用人工智能的应用程序的角度比较NLP的这两个子区域。\n9.2.1　自然语言理解\n早些时候，我们已经看到，NLU更多的是处理对语言结构的理解，无论是单词、短语还是句子。NLU更多的是在已经生成的NL上应用各种ML技术。在NLU中，我们关注语法和语义。我们还试图解决与语法和语义相关的各种类型的歧义。我们已经看到了词汇歧义、句法歧义、语义歧义和语用歧义。\n现在让我们看看在哪里我们可以使用人工智能，帮助机器更准确、更有效地理解语言结构和含义。人工智能和人工智能技术在解决本地语言的这些方面并不落后。举个例子，深入学习使我们在机器翻译方面取得了令人印象深刻的成果。现在，当我们讨论解决句法歧义和语义歧义时，我们可以使用深度学习。假设您有一个命名实体识别工具，它将使用深度学习和word2vec，那么我们就可以解决语法上的歧义。这只是一个应用程序，但您也可以改进解析器结果和词性标注器。\n现在我们来谈谈语用歧义，我们真正需要的是AGI和ASI。这种歧义发生在你试图理解一个句子与其他先前写的或说的句子的长距离上下文时，它还取决于说话者的说话或写作意图。\n让我们来看一个语用歧义的例子。你和你的朋友正在谈话，你的朋友很久以前就告诉你，她加入了一个非政府组织，会为贫困学生做一些社会活动。现在你问她社交活动怎么样了。在这种情况下，你和你的朋友知道你在谈论什么社会活动。这是因为作为人类，我们的大脑存储信息，并知道何时获取这些信息，如何解释这些信息，以及获取的信息与您当前与朋友的对话有何关联。你和你的朋友都能理解对方问题和答案的上下文和相关性，但是机器没有这种理解上下文和说话者意图的能力。\n这就是我们对智能机器的期望。我们希望机器也能理解这种复杂的情况。支持这种解决语用歧义的能力包含在MSI中。这在将来肯定是可能的，但现在，我们正处于机器试图采用AGI并使用统计技术来理解语义的阶段。\n9.2.2　自然语言生成\nNLG是一个我们试图教机器如何以合理的方式生成NL的领域。这本身就是一项具有挑战性的人工智能任务。深入学习真的帮助我们完成了这类具有挑战性的任务。让我举个例子。如果你正在使用谷歌的新收件箱，那么你可能会注意到，当你回复任何邮件时，你会得到三个最相关的回复，以句子的形式回复给你的邮件。谷歌使用了数百万封电子邮件，并制作了一个NLG模型，该模型通过深度学习来生成或预测任何给定邮件的最相关回复。您可以参考图9.8：\n\n除了这个应用程序之外，还有另一个应用程序：看到图像后，机器将提供特定图像的标题。这也是一个使用深度学习的NLG应用程序。生成语言的任务比生成nl要简单，也就是说，连贯性，这就是我们需要AGI的地方。\n我们已经讨论了很多关于“深度学习”这个词的内容，但它实际上是如何工作的？为什么它如此有前途？我们将在本章的下一节中看到这一点。我们将解释NLU和NLG应用程序的编码部分。我们还将从头开始开发NLU和NLG应用程序。在此之前，你必须了解ANN和深度学习的概念。我将在接下来的部分中加入数学，并尽我所能保持简单。\n9.3　深度学习概览\n机器学习是人工智能的一个分支，深度学习是ML的一个分支，参见图9.9：\n\n深度学习使用的人工神经网络不仅是一个或两个层次，而是许多层次的深度，称为深度神经网络（DNN）。当我们使用DNN通过预测同一个问题的可能结果来解决给定的问题时，它被称为深度学习。深度学习可以使用标记的数据或未标记的数据，因此我们可以说深度学习可以用于有监督的技术，也可以用于无监督的技术。使用深度学习的主要思想是，使用DNN和大量的数据，我们希望机器概括特定的任务，并为我们提供一个我们认为只有人类才能生成的结果。深度学习包括一系列的技术和算法，可以帮助我们解决NLP中的各种问题，如机器翻译、问答系统、总结等。除了NLP，您还可以找到其他应用领域，如图像识别、语音识别、对象识别、手写数字识别、人脸检测和人工人脸生成。\n深度学习对我们来说似乎是有希望的，以便建立AGI和ASI。您可以在图9.10中看到一些使用了深度学习的应用程序：\n\n9.4　神经网络基础\n神经网络的概念是ML中最古老的技术之一。神经网络源于人脑。在这一部分中，我们将看到人脑的组成部分，然后推导出神经网络。\n为了理解神经网络，我们首先需要了解人脑的基本工作流程。您可以参考图9.11：\n\n人脑由几千亿个被称为神经元的神经细胞组成，每个神经元执行以下三个任务：\n接收信号：它从树突接收一组信号\n决定将信号传递给细胞体：将这些信号整合在一起，决定是否应将信息传递给细胞体。\n发送信号：如果一些信号通过某个阈值，它会通过轴突将这些称为动作电位的信号发送给下一组神经元。您可以参考图9.12，它演示了用于在生物神经网络中执行这三项工作的组件：\n\n这是我们大脑如何学习和处理某些决定的一个非常简短的概述。现在的问题是：我们能建立一个使用像硅或其他金属这样的非生物基底的人工神经网络吗？我们可以构建它，然后通过提供大量的计算机电源和数据，我们可以比人类更快地解决问题。人工神经网络是一种生物启发的算法，学习识别数据集中的模式。\n9.4.1　神经元的第一个计算模型\n1943年年中，研究人员McCulloch-Pitts发明了第一个神经元计算模型，他们的模型相当简单。该模型有一个接收二进制输入的神经元，对其求和，如果总和超过某个阈值，则输出为1，如果不是，则输出为零。您可以在图9.13中看到图示：\n\n它看起来很简单，但是就像人工智能早期发明的那样，这种模型的发明是一件非常大的事情。\n9.4.2　感知机\n在发明了第一个神经元计算模型的几年后，心理学家Frank Rosenblatt发现，McCulloch-Pitts模型没有从输入数据中学习的机制。所以他发明了神经网络，建立在第一个神经元计算模型的基础上。Frank Rosenblatt称这个模型为感知器。它也被称为单层前馈神经网络。我们称这个模型为前馈神经网络，因为在这个神经网络中，数据只朝一个方向流动——正向。\n现在让我们来理解感知器的工作，它包含了在给定输入上拥有权重的思想。如果您提供一些输入输出示例的训练集，它应该根据给定输入示例的输出，通过不断增加和减少每个训练示例的权重来从中学习函数。这些权重值在数学上应用于输入，以便在每次迭代之后，输出预测变得更准确。整个过程称为训练。参考图9.14了解Rosenblatt的感知器原理图：\n\n9.4.3　理解人工神经网络中的数学概念\n这一节非常重要，因为ML、ANN和DL使用了一系列数学概念，我们将看到其中一些最重要的概念。这些概念将真正帮助您优化ML、ANN和DL模型。我们还将看到不同类型的激活函数和一些关于您应该选择哪个激活函数的提示。我们将看到以下数学概念：\n梯度下降\n激活函数\n损失函数\n梯度下降\n梯度下降是一种非常重要的优化技术，已被几乎所有的神经网络所采用。为了解释这些技术，我想举个例子。我有一个学生成绩和学习时间的数据集。我们想通过学生的学习时间来预测他的考试成绩。您会说这看起来像一个ML线性回归示例。你说得对，我们用线性回归来做预测。为什么是线性回归，与梯度下降有什么关系？让我回答这个问题，然后我们将看到代码和一些很酷的可视化效果。\n线性回归是使用统计方法的ML技术，它允许我们研究两个连续定量变量之间的关系。在这里，这些变量是学生的分数和学习时间。通常在线性回归中，我们试图得到一条最适合我们数据集的线，这意味着无论我们做什么计算，都只是为了得到一条最适合给定数据集的线。得到这条最佳拟合线是线性回归的目标。\n我们来谈谈线性回归与梯度下降的关系。梯度下降是我们用来优化线性回归精度和最小化损失或误差函数的最常用的优化技术。梯度下降是使误差函数最小化、预测精度最大化的技术，其数学定义是一阶迭代优化算法。该算法利用梯度下降法求函数的局部极小值。每一步都与当前点的函数梯度的负值成正比。您可以使用这个实际例子来考虑梯度下降。假设你在山顶上，现在你想到达一个有美丽湖泊的底部，所以你需要开始下降它。现在你不知道该往哪个方向走。在这种情况下，你观察你附近的土地，并试图找到土地倾向于下降的方式。这会让你知道你应该朝什么方向走。如果你沿着下降的方向迈出第一步，并且每次都遵循同样的逻辑，那么你很可能会到达湖边。这正是我们使用梯度下降的数学公式所做的。在ML和DL中，我们从优化的角度考虑所有问题，因此梯度下降是一种用于随时间最小化损失函数的技术。\n另一个例子是你有一个深碗，你把一个小球从它的一端放进去，你可以观察到，过了一段时间后，球会减速并试图到达碗的底部。参见图9.15：\n\n这个图显示了使用梯度下降获得最佳拟合线的过程或步骤。它只是可视化，让您全面了解我们将在代码中做什么。顺便说一下，损失函数、误差函数和成本函数是彼此的同义词。梯度下降也称为最陡下降。首先，让我们了解数据集。它是学生考试成绩和学习时间的数据集。我们知道，在这两个属性之间，应该有一种关系——你学习的数量越少，学生的分数越差，你学习的越多，分数就越好。我们将用线性回归证明这一关系，x值表示数据集的第一列，即学生学习的小时数，y值表示第二列，即考试分数。参见图9.16：\n\n我们调用了一个函数，用来计算误差和实际的梯度下降。\nfrom numpy import * # y = mx + b # m is slope, b is y-intercept # here we are calculating the sum of squared error by using the equation which we have seen in the book. def compute_error_for_line_given_points(b, m, points): totalError = 0 for i in range(0, len(points)): x = points[i, 0] y = points[i, 1] totalError += (y - (m * x + b)) ** 2 return totalError / float(len(points)) def step_gradient(b_current, m_current, points, learningRate): b_gradient = 0 m_gradient = 0 N = float(len(points)) for i in range(0, len(points)): x = points[i, 0] y = points[i, 1] # Here we are coding up out partial derivatives equations and # generate the updated value for m and b to get the local minima b_gradient += -(2/N) * (y - ((m_current * x) + b_current)) m_gradient += -(2/N) * x * (y - ((m_current * x) + b_current)) # we are multiplying the b_gradient and m_gradient with learningrate # so it is important to choose ideal learning rate if we make it to high then our model learn nothing # if we make it to small then our training is to slow and there are the chances of over fitting # so learning rate is important hyper parameter. new_b = b_current - (learningRate * b_gradient) new_m = m_current - (learningRate * m_gradient) return [new_b, new_m] def gradient_descent_runner(points, starting_b, starting_m, learning_rate, num_iterations): b = starting_b m = starting_m for i in range(num_iterations): # we are using step_gradient function to calculate the actual partial derivatives for error function b, m = step_gradient(b, m, array(points), learning_rate) return [b, m]\n让我们读取数据集和执行运算\n# Step 1 : Read data # genfromtext is used to read out data from data.csv file. points = genfromtxt(\"./gradientdescentexample/data.csv\", delimiter=\",\") # Step2 : Define certain hyperparameters # how fast our model will converge means how fast we will get the line of best fit. # Converge means how fast our ML model get the optimal line of best fit. learning_rate = 0.0001 # Here we need to draw the line which is best fit for our data. # so we are using y = mx + b ( x and y are points; m is slop; b is the y intercept) # for initial y-intercept guess initial_b = 0 # initial slope guess initial_m = 0 # How much do you want to train the model? # Here data set is small so we iterate this model for 1000 times. num_iterations = 1000 # Step 3 - print the values of b, m and all function which calculate gradient descent and errors # Here we are printing the initial values of b, m and error. # As well as there is the function compute_error_for_line_given_points() # which compute the errors for given point print(\"Starting gradient descent at b = {0}, m = {1}, error = {2}\".format(initial_b, initial_m, compute_error_for_line_given_points(initial_b, initial_m, points))) print(\"Running...\") # By using this gradient_descent_runner() function we will actually calculate gradient descent [b, m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, num_iterations) # Here we are printing the values of b, m and error after getting the line of best fit for the given dataset. print(\"After {0} iterations b = {1}, m = {2}, error = {3}\".format(num_iterations, b, m, compute_error_for_line_given_points(b, m, points)))\nStarting gradient descent at b = 0, m = 0, error = 5565.107834483211 Running... After 1000 iterations b = 0.08893651993741346, m = 1.4777440851894448, error = 112.61481011613473\n我们调用了两个函数：compute_error_for_line_points(),它将计算实际值和预测值之间的误差，以及gradient_descent_runner()，它将为我们计算梯度。\n计算误差或损失\n有许多方法可以计算ML算法的误差，但在本章中，我们将使用最流行的技术之一：平方距离误差之和。现在我们直接讨论细节，这个误差函数对我们有什么作用？回想一下我们的目标：我们希望得到最适合我们的数据集的行。参考图9.19，这是线路坡度方程。这里，m是直线的斜率，b是y的截距，x和y是数据点——在我们的例子中，x是学生学习的小时数，y是测试分数。参见图9.19：\n\n利用前面的方程，我们画出直线，从斜率m和y截距b的随机值开始，用第一列数据点作为x的值，得到y的值。在训练数据中，我们已经得到y的值，这意味着我们知道每个学生的考试分数。所以对于每个学生，我们需要计算出误差。让我们以一个非常直观的例子为例，注意我们正在使用虚拟值进行解释。假设您通过放置m和b的随机值得到y值41.0。现在您得到y的实际值，即52.5，那么预测值和实际值之间的差为11.5。这只是一个数据点，但我们需要计算每个数据点。因此，为了进行这种误差计算，我们使用的是平方距离误差之和。\n现在我们如何计算平方距离误差之和，为什么要使用平方距离误差之和？\n那么让我们从第一个问题开始，计算平方距离误差和的公式如图9.20所示：\n\n如您所见，最后一部分mxi+b是我们通过选择m和b的随机值绘制的线，我们实际上可以将y替换为mxi+b。因此，我们在这里计算原始y值与生成y值之间的差。我们将减去原始Y值和生成的Y值，并将每个数据点的该值平方。我们之所以对值进行平方，是因为我们不想处理负值，因为我们在计算平方后进行求和，并且我们想测量整体的大小。我们不需要实际的价值，因为我们正试图最小化这个整体的规模。现在回到方程，我们已经计算了原始y值和生成y值之差的平方。现在我们对所有这些点执行求和；我们将使用sigma表示法来指示数据集中所有数据点的求和操作。此时，我们有一个指示误差大小的和值，我们将用这些值除以数据点的总数。之后，我们将得到我们想要的实际错误值。您可以看到，为了生成最适合我们的数据集的行，行正在为每个迭代移动。我们正在根据误差值更新m和b的值。现在，对于每个时间戳，行是静态的，我们需要计算误差。参照图9.21：\n\n现在我们需要根据给定的方程，用技术的方式来表达直观的例子和方程。在这里，我们计算从每个数据点到我们画的直线的距离，将它们平方，求和，然后除以总点数。所以，在每次迭代或时间戳之后，我们可以计算我们的错误值，并了解我们的行有多糟糕，或者我们的行有多好。如果我们的行是差的，那么为了得到最适合的行，我们更新m和b的值。因此，错误值为我们提供了指示是否有改进的可能性，以生成最佳匹配的行。因此，我们最终想要最小化我们在这里得到的错误值，以便生成最佳拟合线。我们如何将这个错误最小化并生成最佳拟合线？下一步称为梯度下降。\n平方和误差的原因有两个：对于线性回归，这是最常用的计算误差的方法。如果您有一个大的数据集，也可以使用它。\n让我们看一下编码部分，然后我们将跳到计算梯度下降的核心部分。def compute_error_for_line_given_points(b, m, points):\ntotalError = 0\nfor i in range(0, len(points)):\nx = points[i, 0]\ny = points[i, 1]\ntotalError += (y - (m * x + b)) ** 2\nreturn totalError / float(len(points))\n计算梯度\n使用错误函数，我们知道是否应该更新行以生成最佳匹配的行，但是如何更新行将在本节中看到。我们如何将这个错误最小化并生成最佳拟合线？为了回答这个问题，首先，让我们对梯度下降和编码部分有一些基本的了解，我们只剩下最后一个函数，gradient_descent_runner()，参考图9.23\n![Alt](https://img-blog.csdnimg.cn/20190204071829188.png)\n如图9.23所示，这是一个三维图。这两个图是相同的，它们的视角是不同的。这些图显示了斜率m，y截距b和误差的所有可能值。这是三个值的对，包括m、b和error。这里，x轴是一个斜率值，y轴是y轴截距，z轴是误差值。我们试图找出错误最少的地方。如果你仔细看图表，那么你可以观察到在曲线的底部，误差值是最小的。值最小的点称为ml中的局部极小值。在复杂的数据集中，可以找到多个局部极小值；这里我们的数据集很简单，因此我们有一个局部极小值。如果您有一个复杂的高维数据集，其中有多个局部极小值，那么您需要进行二阶优化来决定应该选择哪个局部极小值以获得更好的精度。我们不会在这本书中看到二阶优化。现在让我们回顾一下我们的图表，在这里我们可以直观地识别出给我们最小误差值的点，同样的点也给出了y截距的理想值，即b和斜率值，即m。当我们得到b和m的理想值时，我们将把这些值放入我们的y=mx+c方程中，然后魔法就会发生，我们将得到最佳拟合线。这不是获得最佳拟合线的唯一方法，但我的目的是让您对梯度下降有一个深入的了解，以便我们以后可以在DL中使用这个概念。现在从视觉上看，你可以看到误差最小的点，但是如何达到这个点呢？答案是通过计算梯度。梯度也称为坡度，但这不是坡度值m，因此不要混淆。我们讨论的是斜坡的方向，使我们到达那个最小的误差点。所以我们有一些b值和m值，在每次迭代之后，我们更新这些b值和m值，这样我们就可以达到最小的误差值点。所以从三维图像的角度来看，如果你在曲线的顶部，每次迭代，我们计算梯度和误差，然后更新m和b的值，到达曲线的底部。我们需要到达曲线的底部，通过计算梯度值，我们得到了我们下一步应该采取的方向。所以梯度是一条切线，它不断地告诉我们，我们需要移动的方向，无论是向上还是向下，以达到最小的误差点，并获得理想的B和M值来生成最佳拟合线。参见图9.24：\n\n现在让我们看看最后一个但不是最不重要的计算梯度下降的方程。在图9.25中，你可以看到梯度下降方程，它只是我们误差函数的偏导数。我们采用平方误差和方程，对m和b进行偏导数，计算梯度下降。结果如图9.25所示：\n\n左侧符号是偏导数的符号。这里，我们有两个方程，因为我们取了误差函数，生成了关于变量m的偏导数，在第二个方程中，我们生成了关于变量b的偏导数。通过这两个方程，我们将得到b和m的更新值。为了计算梯度，我们需要导出偏导数。误差函数,对于ml和dl中的一些问题，我们不知道误差函数的偏导数，这意味着我们找不到梯度。所以我们不知道如何处理这种函数。你的误差函数应该是可微的，这意味着你的误差函数应该有偏导数。这里的另一件事是我们使用的是线性方程，但是如果你有高维的数据，那么你可以使用非线性函数，如果你知道误差函数的话。当我们第一次开始时，梯度下降并没有给我们最小值。梯度只是告诉我们如何更新m和b值，无论我们应该更新为正值还是负值。所以梯度给了我们一个如何更新m和b值的概念，也就是说，通过计算梯度，我们得到了方向，并试图达到我们得到m和b的最小误差值和最佳值的点。def step_gradient(b_current, m_current, points, learningRate):\nb_gradient = 0\nm_gradient = 0\nN = float(len(points))\nfor i in range(0, len(points)):\nx = points[i, 0]\ny = points[i, 1]\n# Here we are coding up out partial derivatives equations and\n# generate the updated value for m and b to get the local minima\nb_gradient += -(2/N) * (y - ((m_current * x) + b_current))\nm_gradient += -(2/N) * x * (y - ((m_current * x) + b_current))\n# we are multiplying the b_gradient and m_gradient with learningrate\n# so it is important to choose ideal learning rate if we make it to high then our model learn nothing\n# if we make it to small then our training is to slow and there are the chances of over fitting\n# so learning rate is important hyper parameter.\nnew_b = b_current - (learningRate * b_gradient)\nnew_m = m_current - (learningRate * m_gradient)\nreturn [new_b, new_m]在代码中，我们将m_gradient和b_gradient与学习速率相乘，因此学习速率是一个重要的超参数。选择它的值时要小心。如果您选择一个非常高的值，您的模型可能根本不会训练。如果你选择了一个非常低的值，那么训练会花费很多时间，而且也有可能过度拟合。请参阅图9.27，它为您提供了关于良好学习率的直觉：\n\n激活函数\n让我们先看看激活函数。我想给你一个概念，在什么阶段的神经网络，我们将使用这个激活函数。在我们对感知器的讨论中，我们说，如果超过某个阈值，神经网络将生成一个输出；否则，输出将为零。整个机制计算阈值并生成基于此阈值的输出由激活函数负责。\n激活函数能够为我们提供介于0和1之间的值。之后，使用阈值，我们可以生成输出值1或输出值0。假设我们的阈值是0.777，我们的激活函数输出是0.457，那么我们的结果输出是0；如果我们的激活函数输出是0.852，那么我们的结果输出是1。所以，下面是激活函数在ANN中的工作原理。通常，在神经网络中，每个神经元都有一定的权重和输入值。我们正在求和并生成加权和值。当我们通过非线性函数传递这些值时，这个非线性函数激活一定数量的神经元以获得复杂任务的输出；这个神经元的激活过程使用一定的非线性数学函数被称为激活函数或传递函数。激活函数将输入节点映射到输出节点。以某种方式使用某些数学运算。\n在人工神经网络中具有激活函数的目的是在网络中引入非线性。让我们一步一步地了解这一点。\n让我们集中讨论一下ANN的结构。该神经网络结构可进一步分为三个部分：\n架构：架构就是决定神经网络中神经元和层的排列。\n激活：为了生成复杂任务的输出，我们需要看到神经元的活动——一个神经元如何响应另一个神经元以生成复杂行为。\n学习规则：当ANN生成输出时，我们需要在每个时间戳更新ANN权重，以使用误差函数优化输出。\n激活函数是激活部分的一部分。如前所述，我们将把非线性引入神经网络。其背后的原因是，没有非线性，神经网络不能产生复杂的行为来解决复杂的任务。在数字语言中，大多数情况下，我们使用非线性激活函数来获得复杂的行为。除此之外，我们还希望以非线性方式将输入映射到输出。\n如果您不使用非线性激活函数，那么对于复杂的任务，ANN将不会为您提供大量有用的输出，因为您正在传递矩阵，并且如果您在ANN中使用多个具有线性激活函数的层，则会得到一个输出，该输出是输入值、权重和偏差的总和。所有层。这个输出给你另一个线性函数，这意味着这个线性函数将多层人工神经网络的行为转换为单层人工神经网络。这种行为对于解决复杂的任务一点都不没有用。\n我想强调一下连接主义的概念。神经网络中的连接主义是利用相互连接的神经元产生复杂的行为，就像人脑一样，如果不在神经网络中引入非线性，我们就无法实现这种行为。参见图9.29了解激活功能：\n\n这里，我们将介绍上图中提到的这些功能：\nTransfer potential：这是一个集合输入和权重的函数。更具体地说，此函数执行输入和权重的总和\nActivation function：该函数将传递势函数的输出作为输入，并使用激活函数进行非线性数学变换。\nThreshold function：基于激活功能，阈值功能激活神经元或不激活。传递势是一个简单的求和函数，它将输入的内积和连接的权值相加。如图9.30所示：\n\n这种传递势通常是点积，但它可以使用任何数学方程，如多二次函数。\n另一方面，激活函数应该是任何可微的非线性函数。它必须是可微的，这样我们才能计算误差梯度，而且这个函数必须具有非线性特性才能从神经网络中获得复杂的行为。通常，我们使用sigmoid函数作为激活函数，这需要将潜在输出值作为输入，计算最终输出，然后计算实际输出和生成输出之间的误差。然后，我们将利用误差梯度的计算概念以及应用反向传播优化策略来更新神经网络连接的权重。\n图9.31用theta表示传递势函数，也称为logit，我们将在logistic-sigmoid激活函数方程中使用该函数：\n\n您可以在图9.32中看到logistic-sigmoid函数的方程：\n\n激活函数背后的整个想法大致模拟了神经元在大脑中相互交流的方式。每一个都是通过它的动作电位被激活的，如果达到某个阈值，那么我们就知道是否激活一个神经元。激活功能模拟大脑动作电位的峰值。深度神经网络（DNN）被称为通用近似函数，因为它们可以在任何情况下计算任何函数。它们可以计算任何可微的线性函数和非线性函数。现在您可能会问我何时使用这个激活函数。我们将在下一段中看到这一点。有多种激活函数可用。使用时要小心。我们不应该仅仅因为它们听起来很新很酷就使用它们。在这里，我们将讨论您如何知道应该使用哪一个。我们将看到三个主要的激活函数，因为它们在DL中的广泛使用，尽管还有其他的激活函数可以使用。\nSigmoid\nTanh\nReLU 及其变体\nSigmoid\n就其数学概念而言，Sigmoid函数很容易理解。其数学公式如图9.33所示：\n\n如图9.33所示，sigmoid函数将采用给定的方程，取一个数字，并在0和1的范围内挤压该数字。它产生一条S形曲线。这个函数是第一个在神经网络中作为激活函数使用的函数，因为它可以解释为神经元的激活率——零意味着没有激活，一个是完全饱和的激活。当我们在DNN中使用这个激活函数时，我们了解到这个激活函数的一些局限性，这使得它现在不那么流行。\n这些功能的一些基本问题如下：\n它有梯度消失问题\n它的收敛速度很慢\n它不是一个以零为中心的函数\n让我们详细了解每个问题：消失梯度问题：\n当使用基于梯度的方法训练某些神经网络时，尤其是在使用反向传播的神经网络中，可以发现这个问题。这个问题使得学习和调整神经网络早期层的参数变得非常困难。当你向你的人工神经网络添加更多的层时，这就变得更加困难了。如果我们明智地选择激活函数，那么这个问题就可以得到解决。我想先给你详细介绍一下这个问题，然后我们再讨论它背后的原因。\n基于梯度的方法通过了解输入参数和权重的微小变化如何影响神经网络的输出来学习参数值。如果这个梯度太小，那么参数的变化将导致神经网络输出的变化非常小。在这种情况下，经过一些迭代后，神经网络不能有效地学习参数，并且不会以我们想要的方式收敛。这就是梯度消失问题中发生的情况。网络输出相对于早期层参数的梯度变得非常小。可以说，即使输入层和权重的参数值发生了很大的变化，也不会对输出产生很大的影响。\n我给你所有这些细节，因为你也可以面对同样的问题与sigmoid函数。最基本的问题是这个消失梯度问题取决于你的激活函数的选择。sigmoid函数以非线性方式将输入压缩成一个小范围的输出。如果您给sigmoid函数一个实数，它将把这个数压缩到[0,1]的范围内。所以输入空间中有很大的区域被映射到一个非常小的范围。即使输入参数发生很大的变化，也会导致因为这个区域的梯度很小，所以输出变化很小。对于sigmoid函数，当一个神经元饱和接近零或一时，这个区域的梯度非常接近零。在反向传播过程中，这个局部梯度将乘以每个层的输出的梯度。因此，如果第一层映射到一个大的输入区域，我们得到一个非常小的梯度以及第一层输出的非常小的变化。这一微小的变化传递到下一层，并在第二层的输出中产生更小的变化。如果我们有一个DNN，那么在某些层之后输出就不会发生变化。这是sigmoid函数的问题。\n低收敛速度：\n由于这个消失梯度问题，有时具有sigmoid函数的神经网络收敛非常慢。\n非零中心功能：\nsigmoid函数不是零中心激活功能。这意味着，sigmoid函数的输出范围是[0,1]，这意味着函数的输出值将始终为正，从而使权重的梯度变为全部正或全部负。这使得梯度更新在不同的方向上走得太远，这使得优化更加困难。\n由于这些限制，sigmoid函数最近没有在DNN中使用。虽然您可以使用其他函数来解决这些问题，但是您也可以只在您的神经网络的最后一层使用sigmoid函数。\nTanH\n为了克服sigmoid函数的问题，我们将引入一个激活函数，叫做双曲正切函数（tanh）。tanh方程如图9.34所示：\n\n此函数将输入区域压缩到范围[-1,1]内，因此其输出是以零为中心的，这使得优化对我们来说更加容易。这个函数也有消失梯度问题，所以我们需要看到其他的激活函数。\nReLU 及其变体\n整流线性单元（ReLU ）是工业上最流行的函数。看它的方程式在图9.35中：\n\n如果你会看到ReLU数学方程，那么你就会知道它只是max（0，x），这意味着当x小于零时，值为零，当x大于或等于零时，值与斜率1成线性关系。一位名叫Krizhevsky的研究人员发表了一篇关于图像分类的论文，并说他们使用ReLU作为激活函数，可以更快地收敛六倍。你可以点击阅读这篇研究论文http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf.这个函数很简单，没有任何复杂的计算，而且比Sigmoid和Tanh更简单。这就是这个函数学习更快的原因。除此之外，它也没有消失梯度问题。\n我们曾经在DNN中存在的每一层中应用激活函数。目前，ReLU已被广泛应用于大部分的DNN，但它被应用于DNN的隐藏层。如果要解决分类问题，输出层应该使用SoftMax，因为SoftMax函数为我们提供了每个类的概率。我们在word2vec算法中使用了softmax激活函数。对于回归问题，输出层应该使用线性函数，因为信号通过不变。除了ReLU的所有这些优点外，它还有一个问题：在训练过程中，神经网络的某些单元可能会脆弱并死亡，这意味着通过ReLU神经元的大梯度可能会导致权重更新，使其不再在任何数据点上激活。所以流过它的梯度，从那一点开始总是零。为了克服ReLU的这种局限性，引入了一种ReLU的变体——Leaky ReLU。当x小于0（x\u003c0）时，Leaky ReLU的负斜率较小，而不是函数为零。参见图9.36：\n\n还有一种变体叫做maxout，它是ReLU和Leaky ReLU的广义形式，但它使每个神经元的参数加倍，这是一个缺点。\n现在您已经对激活函数有了足够的了解，那么应该使用哪个函数呢？答案是ReLU，但是如果太多的神经元死亡，那么使用Leaky ReLU或maxout。此激活功能应用于隐藏层。对于输出层，如果要解决分类问题，请使用SoftMax函数；如果要解决回归问题，请使用线性激活函数。Sigmoid和Tanh不应用于DNNS。这是一个非常有趣的研究领域，有很大的空间来提出伟大的激活功能。\n还有其他的激活函数可以检查：标识函数、二进制步进函数、arctan等等。在这里，我们将检查第三个重要概念——损失函数\n损失函数\n有时，损失函数也被称为成本函数或误差函数。损失函数给我们提供了一个关于给定训练示例的神经网络性能的概念。因此，首先，我们定义了误差函数，当我们开始训练我们的神经网络时，我们将得到输出。我们将生成的输出与作为训练数据一部分给出的预期输出进行比较，并计算该误差函数的梯度值。我们反向传播网络中的误差梯度，以便更新现有的权重和偏差值，以优化生成的输出。误差函数是训练的主要部分。有多种误差函数可用。如果你问我选择哪个错误函数，那么就没有具体的答案，因为所有的人工神经网络训练和优化都是基于这个损失函数。所以这取决于你的数据和问题陈述。如果你问某人你在你的神经网络中使用了哪个误差函数，那么你间接地问他们训练算法的整个逻辑。无论您将使用什么误差函数，请确保该函数必须是可微的。我列出了一些最流行的错误函数：\n二次成本函数,又称均方误差或和方误差\n交叉熵成本函数,也称为伯努利负对数似然或二进制交叉熵\nKullback-Leibler散度也称为信息散度、信息增益、相对熵或klic。\n除此之外，还有许多其他的损失函数，如指数成本、海林格距离、广义Kullback-Leibler散度和Itakura-Saito距离。\n一般来说，我们在回归中使用平方和误差，在分类数据和分类任务中使用交叉熵。\n9.5　实现神经网络\n在本节中，我们将使用numpy作为依赖项在python中实现我们的第一个ANN，在这个实现过程中，您可以将梯度下降、激活函数和损失函数集成到我们的代码中。除此之外，我们将看到反向传播的概念。\n我们将看到使用反向传播的单层NN的实现。\n9.5.1　单层反向传播神经网络\n在一个单层神经网络中，我们有输入，我们把它输入到第一层。这些层连接有一些权重。我们使用输入、权重和偏差，并对它们求和。这个和通过激活函数并生成输出。这是一个重要的步骤；无论生成什么输出，都应该与实际的预期输出进行比较。根据误差函数计算误差。现在使用误差函数的梯度并计算误差梯度。这个过程和我们在梯度下降部分看到的一样。这个误差梯度给出了如何优化生成的输出的指示。误差梯度流回神经网络并开始更新权重，以便在下一个迭代中得到更好的输出。在人工神经网络中，为了产生更精确的输出，通过返回误差梯度来更新权重的过程称为反向传播。总之，后向传播是一种通过梯度下降更新权值来训练神经网络的常用训练技术。\n计算和数学的所有其他方面将显示在编码部分。所以让我们用反向传播来编写我们自己的单层前馈神经网络。首先，我们将定义主要功能和抽象步骤。这里，我们将给出输入和输出值。因为我们的数据是标记的，所以它是一个有监督的学习示例。第二步是训练，我们将重复训练，重复10000次。我们将首先从随机权重开始，并根据激活函数和误差函数调整权重。\nfrom numpy import exp, array, random, dot class NeuralNetwork(): def __init__(self): # Seed the random number generator, so it generates the same numbers # every time the program runs. random.seed(1) # We model a single neuron, with 3 input connections and 1 output connection. # We assign random weights to a 3 x 1 matrix, with values in the range -1 to 1 # and mean 0. self.synaptic_weights = 2 * random.random((3, 1)) - 1 # The Sigmoid function, which describes an S shaped curve. # We pass the weighted sum of the inputs through this function to # normalise them between 0 and 1. def __sigmoid(self, x): return 1 / (1 + exp(-x)) # The derivative of the Sigmoid function. # This is the gradient of the Sigmoid curve. # It indicates how confident we are about the existing weight. def __sigmoid_derivative(self, x): return x * (1 - x) # We train the neural network through a process of trial and error. # Adjusting the synaptic weights each time. def train(self, training_set_inputs, training_set_outputs, number_of_training_iterations): for iteration in range(number_of_training_iterations): # Pass the training set through our neural network (a single neuron). output = self.think(training_set_inputs) # Calculate the error (The difference between the desired output # and the predicted output). error = training_set_outputs - output # Multiply the error by the input and again by the gradient of the Sigmoid curve. # This means less confident weights are adjusted more. # This means inputs, which are zero, do not cause changes to the weights. adjustment = dot(training_set_inputs.T, error * self.__sigmoid_derivative(output)) # Adjust the weights. self.synaptic_weights += adjustment # The neural network thinks. def think(self, inputs): # Pass inputs through our neural network (our single neuron). return self.__sigmoid(dot(inputs, self.synaptic_weights))\n#Intialise a single neuron neural network. neural_network = NeuralNetwork() print(\"Random starting synaptic weights: \") print(neural_network.synaptic_weights) # The training set. We have 4 examples, each consisting of 3 input values # and 1 output value. training_set_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]]) # Python store output in horizontally so we have use transpose training_set_outputs = array([[0, 1, 1, 0]]).T # Train the neural network using a training set. # Do it 10,000 times and make small adjustments each time. neural_network.train(training_set_inputs, training_set_outputs, 10000) print(\"New synaptic weights after training: \") print(neural_network.synaptic_weights) # Test the neural network with a new situation. print(\"Considering new situation [1, 0, 0] -\u003e ?: \") print(neural_network.think(array([1, 0, 0])))\nRandom starting synaptic weights: [[-0.16595599] [ 0.44064899] [-0.99977125]] New synaptic weights after training: [[ 9.67299303] [-0.2078435 ] [-4.62963669]] Considering new situation [1, 0, 0] -\u003e ?: [ 0.99993704]\n这里，我们使用Sigmoid作为激活函数。我们将使用Sigmoid导数来计算Sigmoid曲线的梯度。我们的损失函数是从生成的输出中减去实际输出。我们用这个误差值乘以梯度得到误差梯度，这有助于我们调整神经网络的权重。新更新的权值和输入再次通过神经网络，计算出Sigmoid曲线的梯度下降和误差梯度，并调整权值直到得到最小误差。\n9.5.2　练习\n使用numpy作为依赖项构建三层深度人工神经网络。（提示：在单层人工神经网络中，我们使用单层，但在这里，您将使用三层。反向传播通常使用递归取导数，但在我们的单层演示中，没有递归。所以需要应用递归导数。）\n9.6　深度学习和深度神经网络\n现在，从ANN到DNN。在接下来的部分中，我们将看到深度学习、DNN的体系结构，并比较DL用于NLP和ML用于NLP的方法。\n9.6.1　回顾深度学习\n我们已经看到了有关DL的一些基本细节。在这里，目的只是为了让你回忆起一些事情。不是两层或三层而是多层深的人工神经网络称为DNN。当我们在大量数据上使用多层深度神经网络时，使用大量的计算能力，我们称之为深度学习过程。让我们看看深层神经网络的结构。\n9.6.2　深度神经网络的基本架构\n在本节中，我们将看到DNN的体系结构。图形表示看起来很简单，并用一些很酷的数学公式定义，如激活函数、隐层激活函数、损失函数等。在图9.41中，您可以看到DNN的基本架构：\n\n现在为什么我们要使用多层深度神经网络，有什么原因可以这样做吗？有多层的意义是什么？\n让我解释一下为什么我们使用多层DNN。假设，作为一名编码人员，您希望开发一个识别水果图像的系统。现在你有了一些橙子和苹果的图像，你开发了一个逻辑，比如我可以使用水果的颜色来识别图像，你还添加了形状作为识别参数。您进行了一些编码，并准备好了结果。现在，如果有人告诉你，我们也有黑白图像。现在您需要重做编码工作。有些种类的图像对你来说太复杂了，作为一个人，无法编码，尽管你的大脑非常擅长识别水果的实际名称。因此，如果你有一个如此复杂的问题，你不知道如何编码，或者你不太了解那些有助于机器解决问题的特征或参数，那么你就使用一个深度神经网络。原因有以下几种：\nDNN是用人脑工作原理的抽象概念推导出来的。\n使用DNN，我们改变了编码的方法。最初，我们为机器提供颜色、形状等功能，以识别给定图像中的水果名称，但通过DNN和DL，我们为机器提供了许多示例，机器将自己了解这些功能。之后，当我们为机器提供一个新的水果图像时，它将预测水果的名称。\n现在，您真的想知道dnn如何自己学习特性，所以让我们重点强调以下几点：\nDNN使用多层非线性处理单元的级联，用于特征提取和转换。每个连续的DNN层都使用前一层的输出作为输入，这个过程与人脑如何将信息从一个神经元传输到另一个神经元非常相似。所以我们试图在DNN的帮助下实现相同的结构。\n在DL中，在DNN的帮助下，使用多个表示级别学习了特性。更高级别的特征或表示是从较低级别的特征派生而来的。所以我们可以说，在DNN中派生特征或表示的概念是分层的。我们用这个较低层次的想法学习一些新东西，并尝试学习一些额外的东西。我们的大脑也以层次结构的方式使用和派生概念。这种不同级别的特性或表示与不同级别的抽象相关。\nDNN的多层有助于机器导出层次表示，这是将多层作为体系结构的一部分的重要性。\n借助DNN和数学概念，机器能够模拟人脑的某些过程。\nDL可以应用于有监督和无监督的数据集，以开发NLP应用程序，如机器翻译、总结、问答系统、论文生成、图像标题标记等。\n9.6.3　NLP中的深度学习\nNLP的早期是基于基于规则的系统，对于许多应用来说，早期的原型是基于基于规则的系统，因为我们没有大量的数据。现在，我们正在应用ML技术来处理自然语言，使用基于统计和概率的方法，在这种方法中，我们以一种热编码格式或共现矩阵的形式表示单词。在这种方法中，我们得到的主要是句法表示，而不是语义表示。当我们尝试基于词汇的方法时，例如词袋、n-grams等等，我们无法区分特定的上下文。\n我们希望所有这些问题都能通过DNN和DL解决，因为现在我们有大量的数据可以使用。为了捕捉自然语言的语义方面，我们开发了好的算法，如word2vec、glove等。除此之外，DNN和DL还提供了一些很酷的功能，如下所示：\n可表达性：这种能力表示机器对通用函数的逼近程度\n可训练性：这种能力对于NLP应用程序非常重要，它表明一个DL系统能够很好、快速地了解给定的问题并开始生成大量的输出。\n可归纳性：这表示机器能够很好地归纳给定的任务，以便它能够预测或生成未知数据的准确结果。\n除了上述三种功能外，DL还为我们提供了其他功能，如可解释性、模块性、可传输性、延迟、对抗稳定性和安全性。\n我们知道语言是复杂的事情，有时我们也不知道如何解决某些NLP问题。这背后的原因是，世界上有如此多的语言，它们有自己的句法结构、单词用法和意义，你不能用同样的方式用其他语言表达。所以我们需要一些帮助我们概括问题并给出良好结果的技术。所有这些原因和因素都引导我们朝着将dnn和dl用于nlp应用的方向发展。\n现在让我们看看经典的NLP技术和dl nlp技术之间的区别，因为这将连接我们的点，说明dl在解决与nlp域相关的问题上是如何更有用的。\n9.6.4　传统NLP和深度学习NLP技术的区别\n在这一节中，我们将比较经典的NLP技术和用于NLP的DL技术。参见图9.42：\n\n在经典的NLP技术中，我们在数据生成特征之前对数据进行了早期的预处理。在下一阶段，我们使用手工制作的特性，这些特性是使用NER工具、POS标记器和解析器生成的。我们将这些特性作为ML算法的输入并训练模型。我们将检查精度，如果精度不好，我们将优化算法的一些参数，并尝试生成更精确的结果。根据NLP应用程序的不同，可以包括检测语言并生成功能的模块。\n![Alt](https://img-blog.csdnimg.cn/20190204072924841.png)\n现在，让我们看看NLP应用程序的深入学习技术。在这种方法中，我们对我们拥有的数据进行一些基本的预处理。然后我们将文本输入数据转换为密集向量的形式。为了产生密集向量，我们将使用word2vec、glove、doc2vec等嵌入技术，并将这些密集向量嵌入到dnn中。这里，我们不使用手工制作的特性，而是根据NLP应用程序使用不同类型的dnn，例如机器翻译，我们使用的是dnn的变体，称为序列到序列模型。总而言之，我们使用的是另一种变体，即长短期内存单元（lstms）。DNN的多层概括了目标，并学习了实现定义目标的步骤。在这个过程中，机器学习了层次表示，并给出了根据需要对模型进行验证和优化的结果。下一节是本章最有趣的部分。我们将构建两个主要应用程序：一个用于NLU，另一个用于NLG。我们使用TensorFlow和Keras作为主要依赖项来编写示例代码。我们将理解dnn的变体，例如sequence-to-sequence和LSTM，因为我们对它们进行编码以更好地理解它们。\n猜猜我们要建造什么？我们将构建一个机器翻译程序作为NLP应用程序的一部分，我们将从中生成一个摘要。那么让我们跳到编码部分！我会给你做一些有趣的练习！\n9.7　深度学习技术和NLU\n人类说、写或读的语言太多了。你试过学一门新语言吗？如果是的话，那么你就知道掌握说一种新语言或写一种新语言的技能是多么困难。你有没有想过谷歌翻译是如何被用来翻译语言的？如果您好奇，那么让我们开始使用深度学习技术开发机器翻译应用程序。不要担心我们将使用哪种类型的DNN，因为我正在向您详细解释。我们来翻译一下吧！\n注意，DL需要大量的计算能力，所以我们不打算实际训练模型，尽管我将向您详细介绍训练代码，但我们将使用训练模型来复制最终的结果。给你一个想法：谷歌连续一周使用100 GPU来训练语言翻译模型。所以我们通过代码，理解概念，使用一个已经训练过的模型，并看到结果。\n9.7.1 机器翻译\n机器翻译（MT）是NLU领域中一个广泛应用的领域。研究人员和科技巨头们正在进行大量的实验，以制造一个可以翻译任何语言的单一机器翻译系统。这种机器翻译系统被称为通用机器翻译系统。因此，我们的长期目标是建立一个单一的机器翻译系统，可以把英语翻译成德语，同样的机器翻译系统也应该把英语翻译成法语。我们正试图建立一个能帮助我们翻译任何语言的系统。让我们来谈谈迄今为止研究人员为建立一个通用的机器翻译系统所做的努力和实验。\n1954年，进行了第一次机器翻译演示，翻译了250个俄语和英语单词。这是一种基于词典的方法，这种方法使用源语言和目标语言的单词映射。在这里，翻译是逐字进行的，它不能捕获句法信息，这意味着准确性不好。\n下一个版本是中间语言；它使用源语言并生成中间语言来编码和表示源语言语法、语法等的特定规则，然后从中间语言生成目标语言。与第一种方法相比，这种方法很好，但很快就被统计机器翻译（SMT）技术所取代。\nIBM使用了这种SMT方法；他们将文本分成若干段，然后将其与对齐的双语语料库进行比较。之后，使用统计技术和概率，选择最有可能的翻译。\n世界上使用最多的SMT是Google翻译，最近，Google发表了一篇论文，指出他们的机器翻译系统使用深度学习来产生巨大的效果。我们正在使用TensorFlow库，这是一个由Google提供的用于深度学习的开源库。我们将通过编码了解如何使用深度学习进行机器翻译。\n我们使用电影字幕作为数据集。此数据集包括德语和英语。我们正在建立一个模型，将德语翻译成英语，反之亦然。您可以从http://opus.lingfil.uu.se/opensubitles.php下载数据。这里，我使用的是pickle格式的数据。使用pickle（依赖于python），我们可以序列化数据集。\n首先，我们使用的是用于记住长期和短期依赖关系的LSTMS网络。我们正在使用TensorFlow的内置数据实用程序类对数据进行预处理。然后我们需要定义训练模型的词汇大小，这里，我们的数据集有一个很小的词汇大小，所以我们考虑数据集中的所有单词，但是我们定义词汇（词汇）大小，比如30000个单词，也就是说，一个小的训练数据集。我们将使用data_utils类从数据目录中读取数据。这个类提供两种语言的标记化和格式化单词。然后我们定义TensorFlow的占位符，它是输入的编码器和解码器。这两者都是表示离散值的整数张量。它们被嵌入到密集的表示中。我们将把词汇输入编码器，并将学习到的编码表示输入解码器。以下非原书内容，请参考：\nClone the GitHub Repository:\nhttps://github.com/deep-diver/EN-FR-MLT-tensorflow\nMLT (EN to FR ) TensorFlow\nIn this project, I am going to build language translation model called seq2seq model or encoder-decoder model in TensorFlow. The objective of the model is translating English sentences to French sentences. I am going to show the detailed steps, and they will answer to the questions like how to preprocess the dataset, how to define inputs, how to define encoder model, how to define decoder model, how to build the entire seq2seq model, how to calculate the loss and clip gradients, and how to train and get prediction. Please open the IPython notebook file to see the full workflow and detailed descriptions.\nThis is a part of Udacity’s Deep Learning Nanodegree. Some codes/functions (save, load, measuring accuracy, etc) are provided by Udacity. However, majority part is implemented by myself along with much richer explanations and references on each section.\nYou can find only the model part explained in my medium post. https://medium.com/@parkchansung/seq2seq-model-in-tensorflow-ec0c557e560f\nBrief Overview of the Contents\nData preprocessing\nIn this section, you will see how to get the data, how to create lookup table, and how to convert raw text to index based array with the lookup table.\nBuild model\nIn short, this section will show how to define the Seq2Seq model in TensorFlow. The below steps (implementation) will be covered.\n(1) define input parameters to the encoder model\nenc_dec_model_inputs\n(2) build encoder model\nencoding_layer\n(3) define input parameters to the decoder model\nenc_dec_model_inputs, process_decoder_input, decoding_layer\n(4) build decoder model for training\ndecoding_layer_train\n(5) build decoder model for inference\ndecoding_layer_infer\n(6) put (4) and (5) together\ndecoding_layer\n(7) connect encoder and decoder models\nseq2seq_model\n(8) train and estimate loss and accuracy\nTraining\nThis section is about putting previously defined functions together to build an actual instance of the model. Furthermore, it will show how to define cost function, how to apply optimizer to the cost function, and how to modify the value of the gradients in the TensorFlow’s optimizer module to perform gradient clipping.\nPrediction\nNothing special but showing the prediction result.\n9.8　深度学习技术和NLG\n在本节中，我们将为NLG构建一个非常简单但直观的应用程序。我们将从shot文章生成一行摘要。我们将在本节中看到有关汇总的所有详细信息。\n这个应用程序花费了大量的训练时间，所以您可以将您的模型放到CPU上进行训练，同时，您还可以执行一些其他任务。如果你没有其他任务，我给你一个。\n9.8.1　练习\n试着通过提供一些起始字符序列来找出如何生成维基百科文章。别误会我！我是认真的！你真的需要好好想想。\n这是您可以使用的数据集：\nhttps://einstein.ai/research/the-wikitext-long-term-dependency-language-modelin\nG数据集。跳转到下载部分，下载这个名为下载wikitext-103字级（181MB）的数据集。\n（提示：请参阅此链接，https://github.com/kumikokashii/lstm文本生成器。）\n别担心，在理解了总结的概念之后，你可以尝试一下。让我们开始总结之旅吧！\n9.8.2　菜谱摘要和标题生成\n在开始编写代码之前，我想简单介绍一下总结的背景知识。架构和其他技术部分将被理解为我们的代码。\n语义在NLP中是一个非常重要的问题。随着数据在文本密度上的增加，信息也会增加。现在，你周围的人真的希望你能在短时间内有效地说出最重要的事情。\n文本总结始于90年代，加拿大政府建立了一个名为天气预报生成器（FOG）的系统，该系统使用天气预报数据并生成摘要。这是基于模板的方法，机器只需要填写某些值。让我举个例子，星期六是晴天，有10%的机会下雨。单词阳光和10%实际上是由雾产生的。\n其他领域包括金融、医疗等。近年来，医生发现总结病人的病史非常有用，能够有效地诊断病人。\n总结有两种类型，如下所示：\nExtractive\nAbstractive过去的大多数摘要工具都是抽取式的；它们从文章中选择现有的一组单词来创建文章摘要。作为人类，我们做更多的事情；也就是说，当我们总结时，我们构建了一个我们所读内容的内部语义表示。使用这种内部语义表达，我们可以对文本进行总结，这种总结称为抽象总结。因此，让我们使用KERA构建一个抽象的总结工具。\nKeras是TensorFlow和Theano的高级包装。这个例子需要多个GPU超过12小时。如果你想在你的终端重现结果，那么它需要大量的计算能力。1. Clone the GitHub Repository:\nhttps://github.com/jalajthanaki/recipe-summarization\n2. Initialized submodules:\ngit submodule update --init -recursive\n3. Go inside the folder:\npython src/config.p\n4. Install dependencies:\npip install -r requirements.txt\n5. Set up directories:\npython src/config.py\n6. Scrape recipes from the web or use the existing one at this link:\nwget -P recipe-box/data https://storage.googleapis.com/recipebox/\nrecipes_raw.zip; unzip recipe-box/data/recipes_raw.zip -d recipebox/\ndata\n7. Tokenize the data:\npython src/tokenize_recipes.py\n8. Initialize word embeddings with GloVe vectors:\nGet the GloVe vectors trained model:\nwget -P data http://nlp.stanford.edu/data/glove.6B.zip;\nunzip data/glove.6B.zip -d data\nInitialize embeddings:\npython src/vocabulary-embedding.py\nTrain the model:\npython src/train_seq2seq.py\nMake predictions:\nuse src/predict.ipynb在这里，对于矢量化，我们使用Glove是因为我们需要一个用于总结的单词的全局级表示，并且我们使用Sequence-to-Sequence模型（seq2seq模型）来训练我们的数据。seq2seq与我们在机器翻译部分讨论的模型相同。我知道总结示例需要大量的计算能力，并且可能会出现本地计算机没有足够内存（RAM）来运行此代码的情况。在这种情况下，不用担心；您可以使用各种云选项。您可以使用Google Cloud、Amazon Web Services（AWS）或任何其他服务。现在您对NLU和NLG应用程序有了足够的了解。我还在此Github链接上放置了一个与NLG域相关的应用程序：\nhttps://github.com/tensorflow/models/tree/master/im2xt\n此应用程序生成图像标题；这是一种计算机视觉和NLG的组合应用程序。Github上有必要的详细信息，所以也可以查看这个例子。\n在下一节中，我们将看到基于梯度下降的优化策略。\nTensorFlow为我们提供了梯度下降算法的一些变体。一旦我们了解了所有这些变体是如何工作的，以及它们各自的缺点和优势是什么，那么我们就可以很容易地为我们的dl算法的优化选择最佳选项。让我们来了解基于梯度下降的优化。\n9.9　基于梯度下降的优化\n在本节中，我们将讨论TensorFlow提供的基于梯度下降的优化选项。最初，您不清楚应该使用哪种优化选项，但是当您了解DL算法的实际逻辑时，它会变得更加清晰。\n我们使用基于梯度下降的方法来开发一个智能系统。使用此算法，机器可以学习如何从数据中识别模式。在这里，我们的最终目标是获得局部最小值，目标函数是机器将要做出的最终预测或机器生成的结果。在基于梯度下降的算法中，我们并不是集中在如何在第一步实现目标函数的最佳最终目标上，而是通过迭代或反复的小步骤，选择中间的最佳选项，使我们获得最终的最佳选项，即我们的局部极小值。这种有教育意义的猜测和检验方法很好地获得了局部极小值。当dl算法得到局部极小值时，可以得到最佳的结果。我们已经看到了基本的梯度下降算法。如果面临过拟合和欠拟合的情况，可以使用不同类型的梯度下降来优化算法。有各种各样的梯度下降方式，可以帮助我们生成理想的局部极小值，控制算法的方差，更新我们的参数，并引导我们收敛我们的ML或DL算法。让我们举个例子。如果函数y=x2，那么给定函数的偏导数是2x。当我们随机猜测状态值时，我们从x=3开始，然后y=2（3）=6，为了得到局部极小值，我们需要朝负方向迈出一步，所以y=6。在第一次迭代之后，如果你猜到x=2.3，那么y=2（2.3）=4.6，我们需要再次向负方向移动——y=-4.6——因为我们得到了一个正值。如果我们得到一个负值，那么我们就朝着正值移动。经过一定的迭代，y的值非常接近于零，这就是我们的局部极小值。现在我们从基本梯度下降开始。让我们开始探索各种梯度下降。\n9.9.1 基本梯度下降\n在基本梯度下降中，我们根据整个训练数据集中存在的参数计算损失梯度函数，我们需要计算整个数据集中的梯度来执行单个更新。对于单个更新，我们需要考虑整个训练数据集以及所有参数，因此速度非常慢。如图9.58所示：\n\n您可以在图9.59中找到用于理解目的的示例逻辑代码：\n由于该方法速度较慢，我们将引入一种新的技术，称为随机梯度下降。\n9.9.2 随机梯度下降\n在这种技术中，我们更新每个训练示例和标签的参数，因此我们只需要为我们的训练数据集添加一个循环，这种方法更新参数的速度比基本梯度下降更快。如图9.60所示：\n\n您可以在图9.61中找到用于理解目的的示例逻辑代码：\n\n这种方法也有一些问题。这种方法使得收敛复杂，有时参数更新太快。该算法可以超越局部极小值并保持运行。为了避免这个问题，另一种方法被称为小批量梯度下降。\n9.9.3 小批量梯度下降\n在这种方法中，我们将从基本梯度下降和随机梯度下降两个方面得到最好的部分。我们将把训练数据集的一个子集作为批处理，并从中更新参数。这种梯度下降用于神经网络的基本类型，如图9.62所示：\n\n您可以在图9.63中找到用于理解目的的示例逻辑代码：\n\n如果我们有一个高维数据集，那么我们可以使用其他的梯度下降方法；让我们从动量开始。\n9.9.4 动量\n如果所有可能的参数值的曲面曲线在一个维度上比在另一个维度上陡得多，那么在这种情况下，这在局部最优中非常常见。在这些情况下，SGD在斜坡上振荡。为了解决这个振荡问题，我们将使用动量法。如图9.64所示：\n\n如果你看到这个方程，我们将从上一个时间步到当前步的梯度方向的一个分数相加，然后我们在正确的方向放大参数更新，从而加快收敛速度并减少振荡。在这里，动量的概念与物理学中动量的概念相似。当获得局部极小值时，这种变化不会减慢，因为此时动量很高。在这种情况下，我们的算法可以完全忽略局部极小值，而这个问题可以通过Nesterov加速梯度来解决。\n9.9.5 Nesterov加速梯度\n这种方法是Yurii Nesterov发明的。他试图解决动量技术中出现的问题。他发表了一篇论文，你可以在这个链接上看到：你可以在图9.65中看到等式：\n\n正如你所看到的，我们做的计算和动量的计算是一样的，但是我们改变了计算的顺序。在动量中，我们计算梯度使动量放大的那个方向上的跳跃，而在Nesterov加速梯度法中，我们首先根据先前的动量进行跳跃，然后计算梯度，然后添加修正并生成参数的最终更新。这有助于我们更动态地提供参数值。\n9.9.6 Adagrad\nAdagrad代表自适应梯度。该方法允许学习率根据参数进行调整。该算法对不常用参数进行大更新，对常用参数进行小更新。如图9.66所示：\n\n该方法根据对该参数计算的过去梯度，为给定时间戳的每个参数提供不同的学习速率。在这里，我们不需要手动调整学习速度，尽管它有限制。根据方程，学习率总是随着分母中平方梯度的累积总是正的而下降，并且随着分母的增长，整个学期都会下降。有时，学习率变得非常小，以至于ML模型停止学习。解决这个问题。图中出现了名为adadelta的方法。\n9.9.7 adadelta\nadadelta是adagrad的扩展。在Adagrad中，我们不断地将平方根加到和上，导致学习率下降。我们没有求和所有过去的平方根，而是将窗口限制为一个固定大小的累积过去梯度。您可以看到图9.67中的公式：\n![Alt](https://img-blog.csdnimg.cn/20190204073501658.png)\n正如你在方程中看到的，我们将使用梯度之和作为所有过去平方梯度的衰减平均值。在这里，给定时间戳的运行平均值e[g2]t取决于以前的平均值和当前的梯度。\n在看过所有的优化技术之后，你知道我们如何计算每个参数的单个学习率，如何计算动量，以及如何防止学习率的衰减。尽管如此，通过应用一些自适应动量还有改进的空间，这将引导我们找到最终的优化方法Adam。\n9.9.8 Adam\nAdam代表自适应动量估计。当我们计算每个参数的学习速率时，我们也可以分别存储每个参数的动量变化。您可以看到图9.68中的方程式：\n![Alt](https://img-blog.csdnimg.cn/20190204073526350.png)\n首先，我们将计算梯度的平均值，然后我们将计算梯度的非中心方差，并使用这些值更新参数。就像一个三角洲。你可以在图9.69中看到Adam的方程：\n![Alt](https://img-blog.csdnimg.cn/20190204073547830.png)\n所以现在你想知道我们应该使用哪种方法；根据我的说法，亚当是最好的整体选择，因为它优于其他方法。您也可以使用adadelta和adagrad。如果您的数据是稀疏的，那么您不应该使用sgd、momentum或nesterov。\n9.10　人工智能与人类智能\n从过去的一年开始，你可能听到过这种问题。在人工智能领域，这类问题已经变得普遍。人们大肆宣传人工智能将使人类消失，机器将夺走我们所有的力量。现在让我告诉你，这不是事实。这些威胁听起来像科幻小说。据我所知，人工智能正处于高速发展阶段，但其目的是为了补充人类，使人类生活更容易。我们仍在研究宇宙中一些复杂和未知的真理，这些真理可以帮助我们对如何构建辅助系统提供更多的见解。所以人工智能将完全帮助我们。人工智能肯定会让我们的生活大吃一惊，但它不会很快被它的发明所淹没。因此，享受这个人工智能阶段，以积极的方式为人工智能生态系统做出贡献。\n人们担心人工智能会夺走我们的工作。它不会夺走你的工作。这会使你的工作更容易。如果你是一名医生，想在一些癌症报告上发表最后一句话，人工智能将帮助你。在信息技术（IT）行业，有一种担忧，即人工智能将取代编码器。如果你相信，研究人员和科技公司很快就能制造出比人类更强大的机器，而且人工智能的转变很快就会发生，机器将夺走我们的工作，那么你最好能获得ML、DL和AI相关技能集来工作，也许你是这个星球上最后一个拥有S的人。我要做的工作！我们假设人工智能会夺走一些就业机会，但这种人工智能生态系统也会创造许多新的就业机会。所以别担心！这个讨论可以继续进行，但我想给你们一些时间来思考这个问题。\n9.11　总结\n恭喜你们！我们已经读到最后一章了！我非常感谢你的努力。在这一章中，你学到了很多东西，比如人工智能方面，它们帮助你理解为什么深度学习是当今的流行词。我们已经看到了人工神经网络的概念。我们已经看到了诸如梯度下降、各种激活函数和损失函数等概念。我们已经看到了dnn和dl生命周期的结构。我们还讨论了sequence-to-sequence模型的基础知识，并开发了机器翻译、标题生成和摘要等应用程序。我们还看到了基于梯度下降的优化技术。\n接下来的部分是附录A到C，它将向您提供有关框架（如Hadoop、Spark等）的概述。您还可以看到这些框架以及其他工具和库的安装指南。除此之外，如果您不熟悉Python，您还可以找到许多Python库的备忘表，这些备忘表非常方便。如果你真的想提高你的数据科学和NLP技能，我有一些建议。我还在附录中提供了Gitter链接，您可以使用它与我联系，以防您有任何问题。\n致谢\n《Python自然语言处理》1 2 3，作者：【印】雅兰·萨纳卡(Jalaj Thanaki），是实践性很强的一部新作。为进一步深入理解书中内容，对部分内容进行了延伸学习、练习，在此分享，期待对大家有所帮助，欢迎加我微信（验证：NLP），一起学习讨论，不足之处，欢迎指正。\n\n参考文献\nhttps://github.com/jalajthanaki ↩︎\n《Python自然语言处理》,（印）雅兰·萨纳卡（Jalaj Thanaki） 著 张金超 、 刘舒曼 等 译 ,机械工业出版社,2018 ↩︎\nJalaj Thanaki ，Python Natural Language Processing ，2017 ↩︎","data":"2019年02月04日 09:28:49"}
{"_id":{"$oid":"5d343b3b62f717dc0659b3ca"},"title":"800G人工智能全套学习资料，不可错过！","author":"yatou2016","content":"人工智能一直是个很火的词，被称为新的“风口”、未来的趋势，总之就是很有前瞻性、很未来的概念。但其实，它并不那么“未来”，我们生活中其实每天都在用人工智能。\n\n\n我们先明确下“人工智能”的定义：\n\n\n人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。\n\n\n所以，人工智能其实就是计算机科学的一个分支，将来也是会成为人类社会基础设施的一部分。\n\n\n现在让我们从头开始，学习人工智能。\n\n\n这里有800G的人工智能学习资料，如果你想站在时代的转折点上成为历史的见证者，请认真学习这份学习资料！\n\n\n以下是资料概览\n\n\n\n\n40G人工智能入门课\n\n\n\n\n\n\n\nPython语言入门课\n\n\n\n\n\n\n\n25G机器学习教程\n\n\n\n\n资料还包括谷歌人工智能学习系统TensorFlow教程、华盛顿大学规模系统和算法的数据操作课、1.8G斯坦福NLP课程...........\n\n\n学习人工智能资料基本都在这儿了！\n\n\n下载链接：https://pan.baidu.com/s/1JqitYJlsY1h8Jt6zJvWPVQ 密码：o2bu\n\n\n更多资料欢迎关注公众号：OFweek机器人网（ofweekrobot）","data":"2018年03月06日 15:28:45"}
{"_id":{"$oid":"5d343b3c62f717dc0659b3cd"},"title":"自然语言处理(PyTorch)-Natural Language Processing with PyTorch - 2019 pdf+中文笔记+源代码下载","author":"Mr.Jk.Zhang","content":"自然语言处理（NLP）为人工智能中的一个有趣问题，是深度学习的应用程序的最新前沿。如果您是一名开发人员或研究人员，准备深入研究这个快速发展的人工智能领域，这本实用的书籍将向您展示如何使用PyTorch深度学习框架来实现最近发现的NLP技术。首先，您需要的是机器学习背景和使用Python编程的经验。\n作者Delip Rao和Goku Mohandas为您提供了PyTorch的坚实基础，以及深度学习算法，用于构建涉及文本语义表示的应用程序。每章包括几个代码示例和插图\n获得对NLP，深度学习和PyTorch介绍\n了解传统的NLP方法，包括NLTK，SpaCy和gensim\n探索嵌入：语言中单词的高质量表示\n使用递归神经网络（RNN）学习语言序列中的表示\n通过复杂的神经架构改进RNN结果，例如长期短期记忆（LSTM）和门控递归单位\n探索读取一个序列并产生另一个序列的序列到序列模型（用于翻译）\n资源下载:\n链接：https://pan.baidu.com/s/1tiUiwSXQGlhZHJrmB1rCHQ\n提取码:点击查看","data":"2019年06月17日 19:10:45"}
{"_id":{"$oid":"5d343b3c62f717dc0659b3cf"},"title":"人工智能与伦理道德","author":"Excef","content":"人工智能：\n“人工智能（Artificial Intelligence）”即AI，指的是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。这是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。\n人工智能”一词最初是在1956 年Dartmouth学会上提出的。从那以后，研究者们发展了众多理论和原理，人工智能的概念也随之扩展。\nAI是否会取代人类？\n关于这个问题，许多学者都表达出了他们的忧虑。早在1993年，计算机科学家Vernon Vinge推广了技术新概念: AI驱动的计算机或机器人能重新设计并改进自身，或者能设计比自身更先进的AI。这个概念让很多人认为，这将导致AI超出人类智慧、理解和控制，从而导致人类时代的终结。近来，许多权威科学家，包括Stuart Russell, Max Tegmark, 和 Frank Wilczek 都曾警告过，AI太过“聪明”的潜在后果就是机器将摆脱人类的控制，起身反抗，同治甚至是消灭人类。而权威物理学家与宇宙学家史蒂芬·霍金也曾说过：“人工智能的崛起可能是人类文明的终结”（出自霍金的主题演讲《让人工智能造福人类及其赖以生存的家园》）\n霍金表示，人工智能的威胁分短期和长期两种。短期威胁包括自动驾驶、智能性自主武器，以及隐私问题；长期担忧主要是人工智能系统失控带来的风险，如人工智能系统可能不听人类指挥。（《终结者》就是这种危险的放大）\n不过，对于这个话题，也有一些人持不同的意见。微软Azure公司副总裁茱莉亚·怀特就曾在悉尼举办的微软峰会上说过，“人工智能将取代人类”的说法忽略了一个前提，那就是人类是“不断学习、成长、改变自己以适应环境的”。怀特认为，人是智慧的生物，能够在不断学习的过程中利用人工智能，控制人工智能，而不是被机器取代。而英国物理学家罗杰·彭罗斯甚至认为，作为一种算法确定性的系统，当前的电子计算机无法产生智能。\n对于这个问题，不同的人有不同的观点，但唯一不变的一点就是：Vernon Vinge提出的观点在未来某天终会到来，人工智能终会实现真正意义上的智能。\n国内第一起机器人伤人事件：\n2016年11月18日，网络盛传深圳高交会上发生“全国首例机器人伤人事件”，名叫“小胖”的机器人突发故障“杀伤力爆棚”，在没有指令的情况下自行打砸展台玻璃、砸伤路人，一位路人全身多处划伤后被担架抬走。这让许多人认为机器人自我意识觉醒对抗人类的危险越来越可能了。但是，18日下午，高交会组委会在其官网发布公告，称事故是由于工作人员操作反了方向键所致：\n\n公告声明，11月17日13时30分左右，在1号展馆D32号展位（小胖的投影机供应商）旁通道内，参展人员试图将一台面罩打开的小胖机器人移动到展位内时，误将后退键按成前进键，并未及时按停，导致另一侧展台的玻璃被部分碰倒摔碎，玻璃划伤了另一侧展台内的一名观众，全过程持续约10余秒。\n虽然最终并非机器人自我意识觉醒。但许多人依然强调，人工智能不能脱离伦理道德。这一点并非仅仅针对AI，还针对AI工作者。AI工作者应该富有责任感，用伦理道德约束自己，在可能的情况下，用伦理道德约束AI.\n\n△ 今日头条创始人、CEO张一鸣\n“作为人工智能的企业，应该永远恪守一条原则：必须对整个人类的未来充满责任感，充满善意。”\n（资料皆来自网络检索）","data":"2017年12月06日 00:16:45"}
{"_id":{"$oid":"5d343b3d62f717dc0659b3d1"},"title":"花非花，物非物，AI岂是池中物(人工智能篇)","author":"奥特曼超人Dujinyang","content":"转载请标明出处：\nhttp://blog.csdn.net/DJY1992/article/details/74979436\n本文出自:【奥特曼超人的博客】\nAI（Artificial Intelligence），即人工智能。人工智能领域的研究包括机器人、语音识别、图像识别、自然语言处理和专家系统等。人工智能从诞生以来，理论和技术日益成熟，应用领域也不断扩大，可以设想，未来人工智能带来的科技产品，将会是人类智慧的“容器”。\n可观看上篇文章：CSDN观点：人工智能会不会取代开发它的人？\n所谓的人工智能，是可以对人的意识、思维的信息过程的模拟。\n\n\n\nQ：你会为 AI 转型？学习代价有多大？\n我想大部分不会All-In ， 毕竟每个人都有家庭、有消费，当然，也有自己的职业生涯顾虑。职业发展稍有不慎，将掉进坑里，不说其它的，光是入行所学习的代价就是巨大的，如果你不是在校研究生、硕士生，我想学习上的优势并不是很大，而且不同的行业所涉及的学科都是不同的，如认知科学，数学，神经生理学，心理学，计算机科学，信息论，控制论，不定性论，仿生学，社会结构学与科学发展观，也就是说，如果真的从事这行业，一个人也只能单方面的针对某个行业的某个领域。\n\n\n\nQ：AI 人工智能领域的状况？\n微软小冰、人机围棋大战、亦或是昨天的百度 AI 开发者大会，无一不代表着人工智能的火热，光是百度养的那几支团队，耗费都是“惊人”的，两个字，太贵。\n\n\nQ：AI 人工智能的实现方法？用什么工具或语言？\n我们大部分都是用传统的编程语言，如JAVA\\C#\\C\\C++\\ANDROID\\IOS 等，那很多人不是太理解人工智能要怎么操作？用什么语言？\n先来看看实现方法，百度上已给出很好的解释，这里精简一下，AI 在计算机上实现时有2种不同的方式：\n一种是采用传统的编程技术，使系统呈现智能的效果，而不考虑所用方法是否与人或动物机体所用的方法相同。这种方法叫工程学方法（ENGINEERING APPROACH），它已在一些领域内作出了成果，如文字识别、电脑下棋等。\n另一种是模拟法（MODELING APPROACH），它不仅要看效果，还要求实现方法也和人类或生物机体所用的方法相同或相类似。\n遗传算法（GENERIC ALGORITHM，简称GA）和人工神经网络（ARTIFICIAL NEURAL NETWORK，简称ANN） 均属后一类型。\n为了得到相同智能效果，两种方式通常都可使用。\n采用前一种方法，需要人工详细规定程序逻辑，如果游戏简单，还是方便的。如果游戏复杂，角色数量和活动空间增加，相应的逻辑就会很复杂（按指数式增长），人工编程就非常繁琐，容易出错。而一旦出错，就必须修改原程序，重新编译、调试，最后为用户提供一个新的版本或提供一个新补丁，非常麻烦。\n采用后一种方法时，编程者要为每一角色设计一个智能系统（一个模块）来进行控制，这个智能系统（模块）开始什么也不懂，就像初生婴儿那样，但它能够学习，能渐渐地适应环境，应付各种复杂情况。这种系统开始也常犯错误，但它能吸取教训，下一次运行时就可能改正，至少不会永远错下去，用不到发布新版本或打补丁。利用这种方法来实现人工智能，要求编程者具有生物学的思考方法，入门难度大一点。但一旦入了门，就可得到广泛应用。由于这种方法编程时无须对角色的活动规律做详细规定，应用于复杂问题，通常会比前一种方法更省力。\n至于用什么工具和语言，就看个人了，学习的前期是，你数学 Very good ，反正博主我是不行了。\n\n\nQ：个人对AI远景的看法？\n人工智能是一门边沿学科，也是一门交叉学科，虽然说冲击着某些行业，如无人驾驶汽车等，但大部分大多数的实现方式都是基于大数据和模拟预测的方式，至目前而言，研究院内还无法模拟出机器的专属情感与自我意识，当然，这就是为何未来还是需要编程人员，因为无法自主实现，还有一点就是基因遗传学，这里所谓的基因遗传学并不是类似人类的DNA，而是RB的自主创造性思维能力，到目前为止，计算机或者人工智能的智能水平在不断提高，但是人工智能的意识还是零。\n换句话说，我们现在没有任何指标证明或者任何迹象表明计算机和人工智能 在未来 能获得这种意识。其实在这一块的技术发展上，我们还是属于慢动作的，期待未来的科技能更智能化。\n\n\nQ：建议大家去学习这一块？\n要看个人兴趣了，上面也提及了这块的学习成本，如果有兴趣的去了解下也无妨，如果基础不是太扎实的，想要深入的建议先去补下数学方面，还有相关的算法知识，如果基础不是太好又想往这方面的职业生涯走，恐怕要吃不少苦头，还不如在自己的专属下深入，如果基础底子不错的，也不建议All-In，可以抽80%的精力去学习，保留20%作为后路。\n||\n版权声明：本文为博主杜锦阳原创文章，转载请注明出处。","data":"2017年07月11日 17:03:56"}
{"_id":{"$oid":"5d343b3d62f717dc0659b3d3"},"title":"人工智能语音如何实现？","author":"luobogen666","content":"语音识别是以语音为研究对象，通过语音信号处理和模式识别让机器自动识别和理解人类口述的语言。语音识别技术就是让机器通过识别和理解过程把语音信号转变为相应的文本或命令的高技术。语音识别是一门涉及面很广的交叉学科，它与声学、语音学、语言学、信息理论、模式识别理论以及神经生物学等学科都有非常密切的关系。语音识别技术正逐步成为计算机信息处理技术中的关键技术，语音技术的应用已经成为一个具有竞争性的新兴高技术产业。\n1、语音识别的基本原理\n语音识别系统本质上是一种模式识别系统，包括特征提取、模式匹配、参考模式库等三个基本单元，它的基本结构如下图所示：\n未知语音经过话筒变换成电信号后加在识别系统的输入端，首先经过预处理，再根据人的语音特点建立语音模型，对输入的语音信号进行分析，并抽取所需的特征，在此基础上建立语音识别所需的模板。而计算机在识别过程中要根据语音识别的模型，将计算机中存放的语音模板与输入的语音信号的特征进行比较，根据一定的搜索和匹配策略，找出一系列最优的与输入语音匹配的模板。然后根据此模板的定义，通过查表就可以给出计算机的识别结果。显然，这种最优的结果与特征的选择、语音模型的好坏、模板是否准确都有直接的关系。\n语音识别系统构建过程整体上包括两大部分：训练和识别。训练通常是离线完成的，对预先收集好的海量语音、语言数据库进行信号处理和知识挖掘，获取语音识别系统所需要的“声学模型”和“语言模型”;而识别过程通常是在线完成的，对用户实时的语音进行自动识别。识别过程通常又可以分为“前端”和“后端”两大模块：“前端”模块主要的作用是进行端点检测(去除多余的静音和非说话声)、降噪、特征提取等;“后端”模块的作用是利用训练好的“声学模型”和“语言模型”对用户说话的特征向量进行统计模式识别(又称“解码”)，得到其包含的文字信息，此外，后端模块还存在一个“自适应”的反馈模块，可以对用户的语音进行自学习，从而对“声学模型”和“语音模型”进行必要的“校正”，进一步提高识别的准确率。\n语音识别是模式识别的一个分支，又从属于信号处理科学领域，同时与语音学、语言学、数理统计及神经生物学等学科有非常密切的关系。语音识别的目的就是让机器“听懂”人类口述的语言，包括了两方面的含义：其一是逐字逐句听懂非转化成书面语言文字;其二是对口述语言中所包含的要求或询问加以理解，做出正确响应，而不拘泥于所有词的正确转换。\n自动语音识别技术有三个基本原理：首先语音信号中的语言信息是按照短时幅度谱的时间变化模式来编码;其次语音是可以阅读的，即它的声学信号可以在不考虑说话人试图传达的信息内容的情况下用数十个具有区别性的、离散的符号来表示;第三语音交互是一个认知过程，因而不能与语言的语法、语义和语用结构割裂开来。\n声学模型\n语音识别系统的模型通常由声学模型和语言模型两部分组成，分别对应于语音到音节概率的计算和音节到字概率的计算。声学建模;语言模型\n搜索\n连续语音识别中的搜索，就是寻找一个词模型序列以描述输入语音信号，从而得到词解码序列。搜索所依据的是对公式中的声学模型打分和语言模型打分。在实际使用中，往往要依据经验给语言模型加上一个高权重，并设置一个长词惩罚分数。\n系统实现\n语音识别系统选择识别基元的要求是，有准确的定义，能得到足够数据进行训练，具有一般性。英语通常采用上下文相关的音素建模，汉语的协同发音不如英语严重，可以采用音节建模。系统所需的训练数据大小与模型复杂度有关。模型设计得过于复杂以至于超出了所提供的训练数据的能力，会使得性能急剧下降。\n听写机：大词汇量、非特定人、连续语音识别系统通常称为听写机。其架构就是建立在前述声学模型和语言模型基础上的HMM拓扑结构。训练时对每个基元用前向后向算法获得模型参数，识别时，将基元串接成词，词间加上静音模型并引入语言模型作为词间转移概率，形成循环结构，用Viterbi算法进行解码。针对汉语易于分割的特点，先进行分割再对每一段进行解码，是用以提高效率的一个简化方法。\n对话系统：用于实现人机口语对话的系统称为对话系统。受目前技术所限，对话系统往往是面向一个狭窄领域、词汇量有限的系统，其题材有旅游查询、订票、数据库检索等等。其前端是一个语音识别器，识别产生的N-best候选或词候选网格，由语法分析器进行分析获取语义信息，再由对话管理器确定应答信息，由语音合成器输出。由于目前的系统往往词汇量有限，也可以用提取关键词的方法来获取语义信息。\n二：语音识别技术原理-工作原理解读\n首先，我们知道声音实际上是一种波。常见的mp3等格式都是压缩格式，必须转成非压缩的纯波形文件来处理，比如Windows PCM文件，也就是俗称的wav文件。wav文件里存储的除了一个文件头以外，就是声音波形的一个个点了。下图是一个波形的示例。\n图中，每帧的长度为25毫秒，每两帧之间有25-10=15毫秒的交叠。我们称为以帧长25ms、帧移10ms分帧。\n分帧后，语音就变成了很多小段。但波形在时域上几乎没有描述能力，因此必须将波形作变换。常见的一种变换方法是提取MFCC特征，根据人耳的生理特性，把每一帧波形变成一个多维向量，可以简单地理解为这个向量包含了这帧语音的内容信息。这个过程叫做声学特征提取。实际应用中，这一步有很多细节，声学特征也不止有MFCC这一种，具体这里不讲。\n至此，声音就成了一个12行(假设声学特征是12维)、N列的一个矩阵，称之为观察序列，这里N为总帧数。观察序列如下图所示，图中，每一帧都用一个12维的向量表示，色块的颜色深浅表示向量值的大小。\n接下来就要介绍怎样把这个矩阵变成文本了。首先要介绍两个概念：\n音素：单词的发音由音素构成。对英语，一种常用的音素集是卡内基梅隆大学的一套由39个音素构成的音素集，参见The CMU Pronouncing DicTIonary?。汉语一般直接用全部声母和韵母作为音素集，另外汉语识别还分有调无调，不详述。\n状态：这里理解成比音素更细致的语音单位就行啦。通常把一个音素划分成3个状态。\n语音识别是怎么工作的呢?实际上一点都不神秘，无非是：\n第一步，把帧识别成状态(难点)。\n第二步，把状态组合成音素。\n第三步，把音素组合成单词。\n如下图所示：\n图中，每个小竖条代表一帧，若干帧语音对应一个状态，每三个状态组合成一个音素，若干个音素组合成一个单词。也就是说，只要知道每帧语音对应哪个状态了，语音识别的结果也就出来了。\n那每帧音素对应哪个状态呢?有个容易想到的办法，看某帧对应哪个状态的概率最大，那这帧就属于哪个状态。比如下面的示意图，这帧在状态S3上的条件概率最大，因此就猜这帧属于状态S3。\n那这些用到的概率从哪里读取呢?有个叫“声学模型”的东西，里面存了一大堆参数，通过这些参数，就可以知道帧和状态对应的概率。获取这一大堆参数的方法叫做“训练”，需要使用巨大数量的语音数据，训练的方法比较繁琐，这里不讲。\n但这样做有一个问题：每一帧都会得到一个状态号，最后整个语音就会得到一堆乱七八糟的状态号，相邻两帧间的状态号基本都不相同。假设语音有1000帧，每帧对应1个状态，每3个状态组合成一个音素，那么大概会组合成300个音素，但这段语音其实根本没有这么多音素。如果真这么做，得到的状态号可能根本无法组合成音素。实际上，相邻帧的状态应该大多数都是相同的才合理，因为每帧很短。\n解决这个问题的常用方法就是使用隐马尔可夫模型(Hidden Markov Model，HMM)。这东西听起来好像很高深的样子，实际上用起来很简单：\n第一步，构建一个状态网络。\n第二步，从状态网络中寻找与声音最匹配的路径。\n这样就把结果限制在预先设定的网络中，避免了刚才说到的问题，当然也带来一个局限，比如你设定的网络里只包含了“今天晴天”和“今天下雨”两个句子的状态路径，那么不管说些什么，识别出的结果必然是这两个句子中的一句。\n那如果想识别任意文本呢?把这个网络搭得足够大，包含任意文本的路径就可以了。但这个网络越大，想要达到比较好的识别准确率就越难。所以要根据实际任务的需求，合理选择网络大小和结构。\n搭建状态网络，是由单词级网络展开成音素网络，再展开成状态网络。语音识别过程其实就是在状态网络中搜索一条最佳路径，语音对应这条路径的概率最大，这称之为“解码”。路径搜索的算法是一种动态规划剪枝的算法，称之为Viterbi算法，用于寻找全局最优路径。\n这里所说的累积概率，由三部分构成，分别是：\n观察概率：每帧和每个状态对应的概率\n转移概率：每个状态转移到自身或转移到下个状态的概率\n语言概率：根据语言统计规律得到的概率\n其中，前两种概率从声学模型中获取，最后一种概率从语言模型中获取。语言模型是使用大量的文本训练出来的，可以利用某门语言本身的统计规律来帮助提升识别正确率。语言模型很重要，如果不使用语言模型，当状态网络较大时，识别出的结果基本是一团乱麻。\n这样基本上语音识别过程就完成了,这就是语音识别技术的原理。\n三：语音识别技术原理-语音识别系统的工作流程\n一般来说，一套完整的语音识别系统其工作过程分为7步：\n①对语音信号进行分析和处理，除去冗余信息。\n②提取影响语音识别的关键信息和表达语言含义的特征信息。\n③紧扣特征信息，用最小单元识别字词。\n④按照不同语言的各自语法，依照先后次序识别字词。\n⑤把前后意思当作辅助识别条件，有利于分析和识别。\n⑥按照语义分析，给关键信息划分段落，取出所识别出的字词并连接起来，同时根据语句意思调整句子构成。\n⑦结合语义，仔细分析上下文的相互联系，对当前正在处理的语句进行适当修正。\n音识别系统基本原理框图\n语音识别系统基本原理结构如图所示。语音识别原理有三点：①对语音信号中的语言信息编码是按照幅度谱的时间变化来进行;②由于语音是可以阅读的，也就是说声学信号可以在不考虑说话人说话传达的信息内容的前提下用多个具有区别性的、离散的符号来表示;③语音的交互是一个认知过程，所以绝对不能与语法、语义和用语规范等方面分裂开来。\n预处理，其中就包括对语音信号进行采样、克服混叠滤波、去除部分由个体发音的差异和环境引起的噪声影响，此外还会考虑到语音识别基本单元的选取和端点检测问题。反复训练是在识别之前通过让说话人多次重复语音，从原始语音信号样本中去除冗余信息，保留关键信息，再按照一定规则对数据加以整理，构成模式库。再者是模式匹配，它是整个语音识别系统的核心部分，是根据一定规则以及计算输入特征与库存模式之间的相似度，进而判断出输入语音的意思。\n前端处理，先对原始语音信号进行处理，再进行特征提取，消除噪声和不同说话人的发音差异带来的影响，使处理后的信号能够更完整地反映语音的本质特征提取，消除噪声和不同说话人的发音差异带来的影响，使处理后的信号能够更完整地反映语音的本质特征。\n四：语音识别技术原理-发展历程\n早在计算机发明之前，自动语音识别的设想就已经被提上了议事日程，早期的声码器可被视作语音识别及合成的雏形。而1920年代生产的“Radio Rex”玩具狗可能是最早的语音识别器，当这只狗的名字被呼唤的时候，它能够从底座上弹出来。最早的基于电子计算机的语音识别系统是由AT\u0026T贝尔实验室开发的Audrey语音识别系统，它能够识别10个英文数字。其识别方法是跟踪语音中的共振峰。该系统得到了98%的正确率。到1950年代末，伦敦学院(Colledge of London)的Denes已经将语法概率加入语音识别中。\n1960年代，人工神经网络被引入了语音识别。这一时代的两大突破是线性预测编码Linear PredicTIve Coding (LPC)， 及动态时间弯折Dynamic Time Warp技术。\n语音识别技术的最重大突破是隐含马尔科夫模型Hidden Markov Model的应用。从Baum提出相关数学推理，经过Labiner等人的研究，卡内基梅隆大学的李开复最终实现了第一个基于隐马尔科夫模型的大词汇量语音识别系统Sphinx。此后严格来说语音识别技术并没有脱离HMM框架。\n实验室语音识别研究的巨大突破产生于20世纪80年代末：人们终于在实验室突破了大词汇量、连续语音和非特定人这三大障碍，第一次把这三个特性都集成在一个系统中，比较典型的是卡耐基梅隆大学(CarnegieMellonUniversity)的Sphinx系统，它是第一个高性能的非特定人、大词汇量连续语音识别系统。\n这一时期，语音识别研究进一步走向深入，其显著特征是HMM模型和人工神经元网络(ANN)在语音识别中的成功应用。HMM模型的广泛应用应归功于AT\u0026TBell实验室Rabiner等科学家的努力，他们把原本艰涩的HMM纯数学模型工程化，从而为更多研究者了解和认识，从而使统计方法成为了语音识别技术的主流。\n20世纪90年代前期，许多著名的大公司如IBM、苹果、AT\u0026T和NTT都对语音识别系统的实用化研究投以巨资。语音识别技术有一个很好的评估机制，那就是识别的准确率，而这项指标在20世纪90年代中后期实验室研究中得到了不断的提高。比较有代表性的系统有：IBM公司推出的ViaVoice和DragonSystem公司的NaturallySpeaking，Nuance公司的NuanceVoicePlatform语音平台，Microsoft的Whisper，Sun的VoiceTone等。","data":"2017年09月19日 19:31:26"}
{"_id":{"$oid":"5d343b3e62f717dc0659b3d5"},"title":"《自然语言处理（哈工大 关毅 64集视频）》学习笔记：第五章 n-gram语言模型","author":"miniAI学堂","content":"视频列表：\n31 n-gram语言模型（一）\n32 n-gram语言模型（二）\n33 n-gram语言模型（三）\n34 n-gram语言模型（四）\n35 n-gram语言模型（五）\n36 n-gram语言模型（六）\n37 n-gram语言模型（七）\n31 n-gram语言模型（一）\n第五章 n-gram语言模型\n噪声信道模型\n\n\nI\n=\na\nr\ng\nm\na\nx\nI\nP\n(\nI\n∣\nO\n)\n)\n=\na\nr\ng\nm\na\nx\nI\nP\n(\nO\n∣\nI\n)\nP\n(\nI\n)\nP\n(\nO\n)\n)\n=\na\nr\ng\nm\na\nx\nI\nP\n(\nO\n∣\nI\n)\nP\n(\nI\n)\nI=\\underset{I}{argmax}P(I|O))=\\underset{I}{argmax}\\frac{P(O|I)P(I)}{P(O)})=\\underset{I}{argmax}P(O|I)P(I)\nI=Iargmax P(I∣O))=Iargmax P(O)P(O∣I)P(I) )=Iargmax P(O∣I)P(I)\n目标:通过有噪声的输出信号试图恢复输入信号\n噪声信道模型的应用\n噪声信道模型是一种普适性的模型，通过修改噪声信道的定义，可以将如下应用纳入到这一模型的框架之中\n语音识别\n一个声学信号对应于一个语句，一个语音识别器需找到其对应的可能性最大的语言文本\n\nT\n=\na\nr\ng\nm\na\nx\nT\nP\n(\nT\n∣\nA\n)\n)\nT=\\underset{T}{argmax}P(T|A))\nT=Targmax P(T∣A))\n根据贝叶斯公式 :\n\nT\n=\na\nr\ng\nm\na\nx\nT\nP\n(\nA\n∣\nT\n)\nP\n(\nT\n)\nP\n(\nA\n)\n)\n=\na\nr\ng\nm\na\nx\nI\nP\n(\nA\n∣\nT\n)\nP\n(\nT\n)\nT=\\underset{T}{argmax}\\frac{P(A|T)P(T)}{P(A)})=\\underset{I}{argmax}P(A|T)P(T)\nT=Targmax P(A)P(A∣T)P(T) )=Iargmax P(A∣T)P(T)\n信息源对应于以概率\nP\n(\nT\n)\nP(T)\nP(T)生成语句文本，噪声信道对应于以概率分布\nP\n(\nA\n∣\nT\n)\nP(A|T)\nP(A∣T)将语句文本转换成声音信号。语音识别的目的就是由通过噪声信道而输出的声音信号恢复其原始的语句文本。\n其他应用\n信源以概率\nP\n(\nT\n)\nP(T)\nP(T)生成语句文本，信道为\nP\n(\nO\n∣\nT\n)\nP(O|T)\nP(O∣T)，语音/图像/翻译文本/字音转换模型\n手写体汉字识别\n文本－〉书写(或者打印、扫描)－〉图像\n文本校错\n文本－〉输入编辑－〉带有错误的文本\n机器翻译\n目标语言的文本－〉翻译－〉源语言文本\n音字转换\n文本－〉字音转换－〉汉字（拼音）编码\n词性标注\n词性标注序列－〉词性词串转换－〉词串\n香农游戏1\n给定前n-1个词(或者字母),预测下一个词(字母)\n32 n-gram语言模型（二）\n语言模型\nP\n(\nT\n)\nP(T)\nP(T)语言模型，如何计算P(T)?\n根据链规则:\n\nP\n(\nT\n)\n=\nP\n(\nS\n)\n=\nP\n(\nw\n1\nw\n2\n.\n.\n.\nw\n3\n)\n=\nP\n(\nw\n1\n)\nP\n(\nw\n2\n∣\nw\n1\n)\nP\n(\nw\n3\n∣\nw\n1\nw\n2\n)\n.\n.\n.\nP\n(\nw\nn\n∣\nw\n1\nw\n2\n.\n.\n.\nw\nn\n−\n1\n)\nP(T)=P(S)=P(w_{1}w_{2}...w_{3})=P(w_{1})P(w_{2}|w_{1})P(w_{3}|w_{1}w_{2})...P(w_{n}|w_{1}w_{2}...w_{n-1})\nP(T)=P(S)=P(w1 w2 ...w3 )=P(w1 )P(w2 ∣w1 )P(w3 ∣w1 w2 )...P(wn ∣w1 w2 ...wn−1 )\n问题:\n1、参数空间过大，无法实用！\n2、数据稀疏问题\n马尔科夫假设\nP\n(\nT\n)\n=\nP\n(\nS\n)\n=\nP\n(\nw\n1\nw\n2\n.\n.\n.\nw\n3\n)\n=\nP\n(\nw\n1\n)\nP\n(\nw\n2\n∣\nw\n1\n)\nP\n(\nw\n3\n∣\nw\n1\nw\n2\n)\n.\n.\n.\nP\n(\nw\nn\n∣\nw\n1\nw\n2\n.\n.\n.\nw\nn\n−\n1\n)\nP(T)=P(S)=P(w_{1}w_{2}...w_{3})=P(w_{1})P(w_{2}|w_{1})P(w_{3}|w_{1}w_{2})...P(w_{n}|w_{1}w_{2}...w_{n-1})\nP(T)=P(S)=P(w1 w2 ...w3 )=P(w1 )P(w2 ∣w1 )P(w3 ∣w1 w2 )...P(wn ∣w1 w2 ...wn−1 )\nbiigram\n假设下一个词的出现依赖于它前面的一个词\n\n≈\nP\n(\nw\n1\n)\nP\n(\nw\n2\n∣\nw\n1\n)\nP\n(\nw\n3\n∣\nw\n2\n)\n.\n.\n.\nP\n(\nw\nn\n∣\nw\nn\n−\n1\n)\n\\approx P(w_{1})P(w_{2}|w_{1})P(w_{3}|w_{2})...P(w_{n}|w_{n-1})\n≈P(w1 )P(w2 ∣w1 )P(w3 ∣w2 )...P(wn ∣wn−1 )\ntrigram\n假设下一下一个词的出现依赖于它前面的两个词\n\n≈\nP\n(\nw\n1\n)\nP\n(\nw\n2\n∣\nw\n1\n)\nP\n(\nw\n3\n∣\nw\n1\nw\n2\n)\n.\n.\n.\nP\n(\nw\nn\n∣\nw\nn\n−\n2\nw\nn\n−\n1\n)\n\\approx P(w_{1})P(w_{2}|w_{1})P(w_{3}|w_{1}w_{2})...P(w_{n}|w_{n-2}w_{n-1})\n≈P(w1 )P(w2 ∣w1 )P(w3 ∣w1 w2 )...P(wn ∣wn−2 wn−1 )\nN-gram语言模型\n最大相似度估计（ Maximum Likelihood Estimate ）\n\nP\n(\nw\nn\n∣\nw\n1\nw\n2\n.\n.\n.\nw\nn\n−\n1\n)\n=\nC\n(\nw\n1\nw\n2\n.\n.\n.\nw\nn\n)\nC\n(\nw\n1\nw\n2\n.\n.\n.\nw\nn\n−\n1\n)\nP(w_{n}|w_{1}w_{2}...w_{n-1})=\\frac{C(w_{1}w_{2}...w_{n})}{C(w_{1}w_{2}...w_{n-1})}\nP(wn ∣w1 w2 ...wn−1 )=C(w1 w2 ...wn−1 )C(w1 w2 ...wn )\n“n-gram” = n个词构成的序列\nunigram\nbigram\ntrigram\nfour-gram(quadgram 4-gram)\n……\nN元文法对下一个单词的条件概率逼近的通用等式是：\n\nP\n(\nw\nn\n∣\nw\n1\nn\n−\n1\n)\n≈\nP\n(\nw\nn\n∣\nw\nn\n−\nN\n+\n1\nn\n−\n1\n)\nP(w_{n}|w_{1}^{n-1})\\approx P(w_{n}|w_{n-N+1}^{n-1})\nP(wn ∣w1n−1 )≈P(wn ∣wn−N+1n−1 )\n构造（训练）N-gram语言模型：在训练语料库中统计获得n-gram的频度信息\n举例\n假设语料库总词数为13,748词\nI\n3437\nwant\n1215\nto\n3256\neat\n938\nChinese\n213\nfood\n1506\nlunch\n459\n\\\nI\nwant\nto\neat\nChinese\nfood\nlunch\nI\n8\n1087\n0\n13\n0\n0\n0\nwant\n3\n0\n786\n0\n6\n8\n6\nto\n3\n0\n10\n860\n3\n0\n12\neat\n0\n0\n2\n0\n19\n2\n52\nChinese\n2\n0\n0\n0\n0\n120\n1\nfood\n19\n0\n17\n0\n0\n0\n0\nlunch\n4\n0\n0\n0\n0\n1\n0\nP(I want to eat Chinese food)\n=P(I)P(want|I)P(to|want)P(eat|to)P(Chinese|eat)P(food|Chinese)\n=0.251087/3437786/1215860/325619/938120/213\n= 0.000154171\nN的选择：可靠性 vs. 辨别力\n“我 正在 ________ ”\n讲课？图书馆？听课？学习？借书？……\n“我 正在 图书馆 ________”\n学习？ 借书？……\n更大的 n: 对下一个词出现的约束性信息更多，更大的辨别力\n更小的n: 在训练语料库中出现的次数更多，更可靠的统计结果，更高的可靠性\nN的选择方法\n词表中词的个数 |V| = 20,000 词\nn\n所有可能的n-gram的个数\n2 (bigrams)\n400,000,000\n3 (trigrams)\n8,000,000,000,000\n4 (4-grams)\n1.6\nx\n1\n0\n17\n1.6 x 10^{17}\n1.6x1017\n数据稀疏问题\n假设我们使用trigram模型\n\nP\n(\nS\n)\n=\nP\n(\nw\n1\n)\nP\n(\nw\n2\n∣\nw\n1\n)\nP\n(\nw\n3\n∣\nw\n1\nw\n2\n)\n.\n.\n.\nP\n(\nw\nn\n∣\nw\nn\n−\n2\nw\nn\n−\n1\n)\nP(S) = P(w_{1})P(w_{2}|w_{1})P(w_{3}|w_{1}w_{2})...P(w_{n}|w_{n-2}w_{n-1})\nP(S)=P(w1 )P(w2 ∣w1 )P(w3 ∣w1 w2 )...P(wn ∣wn−2 wn−1 )\n如果某个\nP\n(\nw\ni\n∣\nw\ni\n−\n2\nw\ni\n−\n1\n)\n=\nC\n(\nw\ni\n−\n2\nw\ni\n−\n1\nw\ni\n)\nC\n(\nw\ni\n−\n2\nw\ni\n−\n1\n)\n=\n0\nP(w_{i}|w_{i-2}w_{i-1})=\\frac{C(w_{i-2}w_{i-1}w_{i})}{C(w_{i-2}w_{i-1})}=0\nP(wi ∣wi−2 wi−1 )=C(wi−2 wi−1 )C(wi−2 wi−1 wi ) =0\n那么\nP\n(\nS\n)\n=\n0\nP(S)=0\nP(S)=0\n数据稀疏问题\n必须保证\nC\n≠\n0\nC\\neq 0\nC̸ =0从而使\nP\n≠\n0\nP\\neq 0\nP̸ =0\n假设某语料库词汇分布如下:\n\n最大相似度估计\n\n期望概率分布\n\n\n33 n-gram语言模型（三）\n数据平滑技术\n降低已出现的n-gram条件概率分布，以使未出现n-gram条件概率分布非0\n又可称为“折扣方法” (Discounting methods)\n（确认）“Validation” –特指使用两个不同的训练语料库的平滑方法\n拉普拉斯定律\n加一平滑法\n\n\nJeffreys-Perks Law\n\nGood-Turing估计\n\n\nGood-Turing估计示例\n建立频度bigram个数表(词表中词数14585，语料库中出现的各不相同的bigram总数199252个，bigram总数为617091个)\n\n对于未出现的bigram\n\n假设语料库中，某bigram 出现了1次，\n\nP=0.3663/617091=5.94E-7\n简单 Good-Turing\n对于比较大的 r,Nr=arb (b \u003c -1) 用线性回归的方法估算 a 和 b :log Nr= log a + b log r, 对于比较小的 r, 直接使用Nr\n关于Good-Turing平滑的两个问题2\n1、Good-Turing 估计的理论依据是什么？\n2、Good-Turing 估计是完备的吗？\n其他常用平滑方法\nBack-off 平滑\n线性插值平滑\nWitten-Bell平滑\n平滑的效果\n数据平滑的效果与训练语料库的规模有关\n数据平滑技术是构造高鲁棒性语言模型的重要手段\n训练语料库规模越小,数据平滑的效果越显著,\n训练语料库规模越大,数据平滑的效果越不显著,甚至可以忽略不计\n34 n-gram语言模型（四）\n一元模型,N-gram模型与N-pos模型之间的关系\n考察N-pos模型的极端情况,即当整个模型只有一个词类,与每一个词都有一个各自不同的词类的情况:如果N-pos模型只有一个词类,那么前N-1个词类没有提供任何上下文信息,于是N-pos模型退化为Unigram模型;如果每一个词都有一个各不相同的词类,那么这样的N-pos模型等价于N-gram模型\n\n统计语言模型的评价\n实用方法\n通过查看该模型在实际应用中的表现来评价统计语言模型\n优点： 直观，实用\n缺点：缺乏针对性，不够客观\n理论方法\n交叉熵与迷惑度\n\n\n\n\n\nKullback-Leibler (KL)距离\nKullback-Leibler (KL)距离（相关熵）\n两个概率密度函数p(x)与q(x)它们的相关熵由下式给出：\n\n描述了两个概率分布之间的差异\n（D(p||q)==0 iff p=q）\n非量度（不满足交换率和三角不等式）\n语言与其模型的交叉熵\n我们构造的语言模型为q(x)，如何评价它的好坏?\nIdea:如果q(x)与正确的语言模型p(x)的相关熵越小，模型越好\n问题是我们并不知道p(x)\n可以借助交叉熵\n\n某语言L，其真实的概率分布为p(x)，我们构造的该语言的概率模型为q(x)，那么该语言与其模型的交叉熵为：\n\n如果我们将语言视为平稳各态可遍历的随机过程：\n那么\n\n迷惑度\n\n举例：150万词WSJ语料库得到的不同n-gram语言模型的迷惑度：\nUnigram:962\nBigram:170\nTrigram:109\n35 n-gram语言模型（五）\n音字转换系统\n\n附录1 语言模型构造实例\nN-gram语言模型构造举例\n36 n-gram语言模型（六）\n附录2 最大熵模型基础\n最大熵模型\n一种基于最大熵原理的统计预测模型。\n最大熵原理\n在一定的限制条件下，尽可能地选择熵最大的概率分布（均匀分布）作为预测结果\n对不知道（限制条件以外）的情形，不做任何假设\n举例-1\n抛一枚硬币： p(H)=p1, p(T)=p2.\n限制条件: p1 + p2 = 1\n问题：如何估计概率分布 p=(p1, p2)?\n基于最大熵原理的答案: 选择使H§最大的那个p\n\n举例-2\n\n\n\n\n最大熵模型\n目的：估计在限定条件下的概率p\n选择满足限定条件的p，使H§为最大\n\nH\n(\nx\n)\n=\n−\n∑\nx\n∈\nX\np\n(\nx\n)\nlog\n⁡\np\n(\nx\n)\nH(x)=-{\\sum_{}^{x\\in X}}p(x)\\log p(x)\nH(x)=−∑x∈X p(x)logp(x)\n\nx\n=\n(\na\n,\nb\n)\n,\na\n∈\nA\n∧\nb\n∈\nB\nx=(a,b),a\\in A\\wedge b\\in B\nx=(a,b),a∈A∧b∈B\n\nA\nA\nA 为上下文特征集合， 为待预测标记的集合\n37 n-gram语言模型（七）\n- 如何获得这样的模型 从训练数据中统计 (a, b) 对: a: 上下文 b:预测标记（观察值） (a,b)称为一个事件 举例: 词性标注 a=在某个文本窗口中的词 b=NN - 学习得到每个 (a, b)的概率值：p(a, b) - 问题：如何表示限制条件 - 特征 - 在最大熵模型中，特征是一个关于事件二值函数\nf\nj\n:\nX\n→\n{\n0\n,\n1\n}\n,\nX\n=\nA\n×\nB\nf_{j}:X\\rightarrow \\left \\{ 0, 1\\right \\},X=A\\times B\nfj :X→{0,1},X=A×B\n举例：\n\n特征(事件)举例\n(title caps, NNP)： Citibank, Mr.\n(sufix -ing, VBG)： running, cooking\n(POS tag DT, I-NP)： the bank, a thief\n(current word from, I-PP)： from the bank\n(next word Inc., I-ORG)： Lotus Inc.\n(previous word said, I-PER)： said Mr. Vinken\n复杂特征\n文档级特征\n(document category = 篮球\u0026 current word = “火箭”, I-ORG)\n可能将“火箭”标为 I-ORG 而非普通名词\n原子级（词）特征\n(current word = “李宁” \u0026 next word = 公司, I-ORG)\n可能将“李宁”标为 I-ORG而非I-PER\n限制条件\n最大熵模型的使用\n根据局部概率值直接标注\n计算全局最优解\nViterbi search\nBeam search\n最大熵模型总结\n原理：找到满足所有限制的熵最大的概率分布\n训练:通过 GIS 或者 IIS，收敛速度可能较慢\n对交叉性的特征也能很好的处理\n对自然语言处理的许多问题都能提供很好的解决方案\n致谢\n关毅老师，现为哈工大计算机学院语言技术中心教授，博士生导师。通过认真学习了《自然语言处理（哈工大 关毅 64集视频）》3（来自互联网）的课程，受益良多，在此感谢关毅老师的辛勤工作！为进一步深入理解课程内容，对部分内容进行了延伸学习4 5 612，在此分享，期待对大家有所帮助，欢迎加我微信（验证：NLP），一起学习讨论，不足之处，欢迎指正。\n\n参考文献\nClaude E. Shannon. “Prediction and Entropy of Printed English”, Bell System Technical Journal 30:50-64. 195 ↩︎ ↩︎\nAn Empirical Study of Smoothing Techniques for Language Modeling, Stanley F. Chen ↩︎ ↩︎\n《自然语言处理（哈工大 关毅 64集视频）》（来自互联网） ↩︎\n王晓龙、关毅 《计算机自然语言处理》 清华大学出版社 2005年 ↩︎\n哈工大语言技术平台云官网：http://ltp.ai/ ↩︎\nSteven Bird,Natural Language Processing with Python,2015 ↩︎","data":"2019年01月10日 09:07:00"}
{"_id":{"$oid":"5d343b3e62f717dc0659b3d7"},"title":"论人工智能背后的伦理问题","author":"yuxy36","content":"论人工智能背后的伦理问题\n\n\n\n\n人工智能（Artificial Intelligence），英文缩写为AI。它是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。\n\n人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。人工智能从诞生以来，理论和技术日益成熟，应用领域也不断扩大，可以设想，未来人工智能带来的科技产品，将会是人类智慧的“容器”。\n人工智能可以对人的意识、思维的信息过程的模拟。人工智能不是人的智能，但能像人那样思考、也可能超过人的智能。\n人工智能会有感情吗？ 是的\nkismet还是麻省理工学院研制出的世界上第一个有感情的机器人。\n\n情感和社交技能对于一个智能AGENT是很重要的。 首先，通过了解他们的动机和情感状态，代理人能够预测别人的行动(这涉及要素 博弈论、决策理论以及能够塑造人的情感和情绪感知能力检测)。此外，为了良好的人机互动，智慧代理人也需要表现出情绪来。至少它必须出现礼貌地和人类打交道。至少，它本身应该有正常的情绪。\n如果人工智能拥有了情感，它是否会吸引人类呢？人类与人工智能之间的爱情又符合伦理道德吗？\n\n很多爱恋都是从朋友发展而来的。试想一下，如果你每天都能接触到你的好朋友，他懂你，他关心你，他幽默风趣，他带给你无限的正能量和快乐，他让你变得积极。这样的关系发展下去，真的就很容易发展成爱恋吧，特别是当你有需要的时候。又抑或是闺蜜、兄弟、死党。即使对方不是传统意义上的生物人。这样的话，又牵涉到伦理问题。传统意义上的爱情是指，是人与人之间的强列的依恋、亲近、向往，以及无私专一并且无所不尽的情感,还有向往未来的生活。现代科学解析爱情是通过激素作用的生物程序，有关爱情的行为都是由进化的力量主导，通过激素起作用，所有疯狂的行为只为了把基因传递给后代。以此为标准，人机恋爱是违背常理的。\n影视作品中也有类似的桥段。人机相恋的结局有好有坏，我们当然应该思考如何规避风险，应对未来这一天的到来。\n《西部世界》里，机器人向人类开枪。\n《她》讲述了作家西奥多在结束了一段令他心碎的爱情长跑之后，他爱上了电脑操作系统里的女声，这个叫“萨曼莎”的姑娘不仅有着一把略微沙哑的性感嗓音，并且风趣幽默、善解人意，让孤独的男主泥足深陷。\n《机械姬》讲述了老板邀请员工到别墅进行对智能机器人进行“图灵测试”的故事，最终机械姬逃出实验室，混入人类社会。\n人工智能伦理和监管基金\nMIT媒体实验室(MIT Media Lab)和哈佛大学伯克曼克莱恩互联网及社会研究中心(Berkman Klein Center)共同成为了“人工智能伦理和监管基金”的管理机构。该基金将用于解决人工智能所带来的人文及道德问题。\n我们了解到，除了LinkedIn创始人Reid Hoffman、eBay创始人Pierre Omidya的非盈利组织Omidyar Network各捐出的1000万美金之外，Knight基金会也捐助了500万美元，此外，还有William and Flora Hewlett基金会和Raptor集团创始人Jim Pallotta各捐出了100万美元。这笔总计2700万美元的基金将由MIT媒体实验室和哈佛大学Berkman Klein中心共同管理。将来，也会有更多的捐款者加入，基金的规模还将扩大。\n\n“人工智能将影响我们每一个地球人，对社会的各个领域意义重大。”Knight基金会CEO Alberto Ibargüen 表示。尽管人工智能帮助人类干了不少事儿，比如用自动驾驶技术来减少大量的交通事故，扫描医学影像来发现癌症等等。\n\n但是有不少业界人士也在发出警告称AI很有可能在给人类带来帮助的同时，也带给人不小的威胁。\n\n人工智能系统能分析大量数据，但其中也可能存在偏见。\n\n“AI的飞速发展带来了很多严峻的挑战，”麻省理工学院媒体实验室主任伊藤穰一(Joi Ito)解释道，“人工智能给社会以及人类带来了不可忽视的影响，机器变得越来越智能，以至于有时候我们会担心它们是否会脱离我们的控制。其中最关键的挑战之一就是，我们如何确保自己培训的机器永远不会产生并且放大人类的偏见，以此来困扰社会？我们该如何针对这项技术展开更广泛，深入的讨论？社会该如何与AI 共同演变？人们如何将计算机科学和社会科学连接在一起，开发出不仅‘聪明’，更重要的是对社会负责的智能机器？”\n\n人工智能伦理和监管基金将用于研究AI应当如何承担社会责任，例如教育、交通运输和司法等领域的计算机程序怎样确保公平性问题。该基金还希望探索出AI是以何种方式与公众展开对话的，帮助公众理解AI的复杂性和多样性。此外，建立这项基金的必要性在于它将会跨越学科之间的障碍，打破不同领域的孤岛。MIT媒体实验室、哈佛Berkman Klein中心以及其他潜在合作者，将作为一种共同机制，加强基金组织的跨学科工作，并鼓励交叉学科的并行对话和协作。\n\n据了解，该组织预计在未来数年以阶段式来达成目标，比如在MIT媒体实验室原定于7月10日举办的AI研讨会上将进行一定的补充。此外，该基金还将监督AI奖学金计划，对一些相关的合作项目进行支持，引导AI向利于社会的方向发展。\n\n协作型网络\n\nMIT媒体实验室和哈佛Berkman Klein中心在网络社区以及多学科人才的帮助下，使用机器学习从数据中学习伦理和法律规范，并且使用数据驱动技术来量化人工智能对劳动力市场的潜在影响。\n据悉，这项工作已经在两个机构中开始进行。其中一个是自动驾驶汽车的合作小组，由Iyad Rahwan领导，旨在讨论汽车中相关的AI道德复杂性;另一个是机器人小组，由Cynthia Breazeal领导，旨在研究人类与机器人互动过程中所涉及的道德问题。\n\n“正如18、19世纪，工业革命对整个世界产生的影响那样，AI的影响力也是类似的。”Rahwan表示，他将运输系统和就业列为最可能受到自动化和人工智能影响的领域之一。“我们需要的是让整个社会处在这些系统的控制环路中，包括技术专家、工程师、伦理学家、认知科学家、经济学家、法律学者、人类学家、信仰领袖、政府监管者等等，以达到公众效益。“\n\nBreazeal表示，“人工智能为所有年龄和阶段的人提供了深度个性化的学习体验，”她强调人工智能需要接触发展中国家和少数派人群。同时，她补充到，“人工智能也是一把双刃剑，这种技术该如何惠及大众，又该如何保护人类的隐私和安全？这些问题都需要慢慢思考。”\n\n共同的目标\n\n哈佛大学Berkman Klein中心一直致力于研究以公共利益导向的解决方案，比如知识共享和美国数字公共图书馆。此外，目前它还和MIT媒体实验室合作，将一部分高水平的开发人员和高科技行业的专业人士汇集在哈佛大学进行为期三周的严格培训，随后还将展开12周的协同研究，让各路人才共同探索网络安全问题。\n\n哈佛大学Berkman Klein中心的联合创始人兼计算机科学教授Jonathan Zittrain表示：“这些研究会帮助我们判断 AI 事业更深层次的目标。虽然有时人工智能让人担忧，但是人类应该做的工作是确定和培养自身在技术面前的自主性和尊严，而不是去削弱技术。”\n\n据了解，“人工智能伦理和监管基金”将由一个小型董事会进行管理。此外，董事会在选举出一组专家顾问，其中包括Daniela Rus、Andrew，以及Max Tegmark等MIT CSAIL实验室的大牛。\n\n麻省理工学院媒体实验室主任伊藤穰一表示，现在对各个领域进行覆盖是至关重要的。“我们决定创建一个动态网络，而不是建立一个机构。解决问题最好的方法就是展开跨学科、交叉领域的研究。我们这个项目只是一个开始。”","data":"2017年12月01日 23:56:32"}
{"_id":{"$oid":"5d3447e362f717dc0659b54b"},"title":"人工智能自然语言处理在文学方面的应用","author":"Qearl_8","content":"人工智能自然语言处理(NLP)是一种新颖的人机交互方式，也就是让计算机去分析和理解人类的语言，实现从自然语言到机器语言的转换。不同于人与人之间基于长久以来约定俗成的社会情境的交流，计算机与人类的对话有更多的限制，比如自然语言中的歧义，有时候人类自身也会在理解上出现偏差，更遑论是计算机。为了准确了解人类的语言，也为了更加流畅的交流，计算机不得不去汇集大规模的数据，以此来减少失误。\n以上是关于人工智能自然语言处理的粗略介绍，下面谈论NLP在文学方面的应用。作为文学系的一员，长期以来，我一直在同文字打交道，比如说学习我国的语言文字、不同时期的文学作品以及外国的作家作品等等，这些都需要大量的阅读和写作，而这些一般在计算机上进行，于是NLP能够帮助我的，大概就是缩减一下工作量了。\n文字的阅读于我而言，是长久而连续的工作。阅读的开始，需要选取一本书或者一份资料，但由于出版社的不同和名字的重叠，往往会出现错误。为了避免这种情况，人需要进行一定的筛选，尽管工作量不大，还是有些消耗时间。NLP可以包揽这项工作，只需要输入名字和相应的信息，就可以很快的找到需要的书或者资料。比如网上图书馆的信息检索功能，就是NLP应用在现实生活的一个表现。另外一方面，当在阅读过程中遇到感兴趣的名词或者句子时，也能通过信息检索快速而有效的寻找到相应的信息，这在生活中应用得更为广泛。\n这里提一下NLP的问答系统，现在大部分软件和电子设备上都设有小机器人，当工作人员不在时，仍然可以与用户进行对话，在解决问题上也不算鸡肋。网上图书馆也有这种小机器人，可以解决一些简单的事情，比如某种书籍是否在馆、能否借阅之类的小事。\n有时候，因为没有充裕的时间，我不能将文字完全阅览，但又需要马上了解文字所表达的内容，NLP同样可以派上用场。它能使计算机在研读文字后，自动提取其核心内容，并生成相应的信息反馈给我。这样的信息通常简明扼要，能够让我短时间内了解文本的内容。\n如果文字看累了，现在的阅读软件都会提供语音朗读服务，只需要点开，就会出现人声为你朗读，让眼睛休息一会。与这相联系的，由于经常需要写作，要求的字数也挺多，经常写到一半就不想再打字了，这种时候会通过语音输入来实现。机器能够识别你的语言，将语音转变为文本进行呈现。上述两种情形，都与NLP的应用有关系。\n关于写作，NLP还有其他令人愉快的应用——手写体和印刷体的识别。这个我用得不多，大多时候是为了搜索不清楚发音的繁体字，有时候也会用印刷体识别的功能将图片上的文字转变为文本文字，方便修改。\n当输入一个汉字或者英文字母时，输入列表里往往会出现一些相关的文字，有的恰好是你接下来需要写下来的。同时，依照你输入的文字的长短，列表里的内容也会不断变换，与你想要输进去的文字越发契合。因为每个人的常用文字不尽相同，相应的每个人的电子设备上出现的联想性文字也存在区别。对于经常需要写作的人来说，NLP的这个功能是最实用的。\n除此之外，值得一提的是NLP的机器翻译功能，在对外国文学的学习上有很大效用。外国对他们本国文学的研究，大体上要比我国学者更深刻和全面，再者，多了解一个消息总归不是坏事。新鲜出炉的学者论文是不大容易找到翻译的，哪怕是寻到了可靠的翻译人员，这也不是一项轻松的活儿。NLP解决了这个困扰，通过机器翻译，我可以及时到了解外国文学新的研究动向，讯息流通更加便捷。\n文本分类也是NLP的一个应用，比较常见的是其中的情感分析，给出几个文本，计算机会自动为你归类，例如依照文本所表现出来的情绪分为“积极”和“消极”，这个分类标准可以依据个人需要去删改，当接收文本过多、人工分类太麻烦的时候可以用到。我通常是阅读的时候使用，因为习惯一次性下载许多文章，又不想费时间一个个翻阅后归类，就干脆交由计算机识别了。\n以上是我个人对于人工智能自然语言处理在文学方面应用的总结，如果表述存在问题或者不足之处，还请多多指正，非常感谢。","data":"2019年06月16日 21:18:21"}
{"_id":{"$oid":"5d34480262f717dc0659b555"},"title":"人工智能 | 自然语言处理研究报告（应用篇）","author":"冲动的MJ","content":"博主github：https://github.com/MichaelBeechan\n博主CSDN：https://blog.csdn.net/u011344545\n============================================\n概念篇：https://blog.csdn.net/u011344545/article/details/89525801\n技术篇：https://blog.csdn.net/u011344545/article/details/89526149\n人才篇：https://blog.csdn.net/u011344545/article/details/89556941\n应用篇：https://blog.csdn.net/u011344545/article/details/89574915\n下载链接：https://download.csdn.net/download/u011344545/11147085\n============================================\n清华AMiner团队 AMiner.org\n从知识产业角度来看，自然语言处理软件占有重要的地位，专家系统、数据库、知识库，计算机辅助设计系统（CAD）、计算机辅助教学系统（Cal）、计算机辅助决策系统、办公室自动化管理系统、智能机器人等，全都需要自然语言做人机界面。长远看来，具有篇章理解能力的自然语言理解系统可用于机器自动翻译、情报检索、自动标引及自动文摘等领域，有着广阔的应用前景。\n随着自然语言处理研究的不断深入和发展，应用领域越来越广。\n文本方面的应用主要有：基于自然语言理解的智能搜索引擎和智能检索、智能机器翻译、自动摘要与文本综合、文本分类与文件整理、智能自动作文系统、自动判卷系统、信息过滤与垃圾邮件处理、文学研究与古文研究、语法校对、文本数据挖掘与智能决策以及基于自然语言的计算机程序设计等。\n语音方面的应用主要有：机器同声传译、智能远程教学与答疑、语音控制、智能客户服务、机器聊天与智能参谋、智能交通信息服务（ATIS）、智能解说与体育新闻实时解说、语音挖掘与多媒体挖掘、多媒体信息提取与文本转化以及对残疾人智能帮助系统等。\n此外,建立在自然语言处理技术基础之上的心理学、认知学、哲学、混沌学说的共同发展，将使人们对智能的起源问题有新的认识。如果把计算机网络和未来的网格看作是由机器组成的机器社会，那么一种属于机器的智能可能会因为人类的参与以及机器社会中各元素的相互作用而自然诞生。这样，机器必将能够通过“图灵测试”，达到“会思考”的层次。而有关智能机器的研究也会诞生一系列新的领域，比如，机器心理学和机器认知学等。\n其中，机器心理学主要研究机器的心理反应和意图。美国圣迭戈神经科学研究所研制的机器人 DarwinV II，能够根据其感知对外部事物进行分类，并根据经验和知识采取相应的对策。然而，机器心理学的研究不能局限于此，人们还需要对机器的意识、知觉、思想、情感、情绪、创造力、机器社会、机器交流等方面进行研究，而这一切还需要计算机科学、心理学、神经科学的同步发展。\n我们选取一些自然语言处理应用较为频繁的场景进行介绍。\n1、知识图谱\n知识图谱能够描述复杂的关联关系，它的应用极为广泛，最为人所知的就是被用在搜索引擎中丰富搜索结果，并为搜索结果提供结构化结果体现关联，这也是 google 提出知识图谱的初衷。同时微软小冰、苹果 siri 等聊天机器人中也加入了知识图谱的应用，IBM Watson是问答系统中应用知识图谱较为典型的例子。按照应用方式，可以将知识图谱的应用分为语义搜索、知识问答、以及基于知识的大数据分析和决策等。\n语义搜索利用建立大队莫知识库对搜索关键词和文档内容进行语义标注，改善搜索结果，如谷歌、百度等在搜索结果中嵌入知识图谱。知识问答是基于知识库的问答，通过对提问句子的语义分析，再将其解析为结构化的询问，在已有的知识库中获取答案。在大数据的分析和决策方面，知识图谱起到了辅助作用，典型应用是美国 Netflix 公司利用其订阅用户的注册信息以及观看行为构建的知识图谱反映出英剧版《纸牌屋》很受欢迎，于是拍摄了美剧《纸牌屋》，大受追捧。\n\n2、机器翻译\n机器翻译是自然语言处理最为人知的应用场景，一般是将机器翻译作为某个应用的组成部分，例如跨语言的搜索引流等。目前以 IBM、谷歌、微软为代表的国外科研机构和企业均相继成立机器翻译团队，专门从事智能翻译研究。如 IBM 于 2009 年 9 月推出 ViaVoiceTranslator 机器翻译软件，为自动化翻译奠定了基础；2011 年开始，伴随着语音识别、机器翻译技术、DNN（深度神经网络）技术的快速发展和经济全球化的需求，口语自动翻译研究成为当今信息处理领域新的研究热点；Google 于 2011 年 1 月正式在其 Android 系统上推出了升级版的机器翻译服务；微软的 Skype 于 2014 年 12 月宣布推出实时机器翻译的预览版、支持英语和西班牙语的实时翻译，并宣布支持 40 多种语言的文本实时翻译功能。\n尤其值得之注意的是，在“一带一路”这一发展背景下，合作沟通会涉及 60 多个国家、53 种语言，此时机器翻译的技术应用显得尤为重要，语言的畅通是“一带一路”战略得以实施的重要基础。而机器翻译涉及到语义分析、上下文环境等诸多挑战，其发展道路还有很长一段路要走。\n3、聊天机器人\n聊天机器人是指能通过聊天 app、聊天窗口或语音唤醒 app 进行交流的计算机程序，是被用来解决客户问题的智能数字化助手，其特点是成本低、高效且持续工作。例如 siri，小娜等对话机器人是一个应用场景。除此之外，聊天机器人在一些电商网站有着很实用的价值，可以充当客服角色，例如京东客服 jimi，有很多基本的问题，其实并不需要真的联系人工客服来解决。通过应用智能问答系统，可以排除掉大量的用户问题，比如商品的质量投诉、商品的基本信息查询等程式化问题，在这些特定的场景中，特别是会被问到高度可预测的问题中，利用聊天机器人可以节省大量的人工成本。\n\n4、文本分类\n文本分类是指根据文档的内容或者属性，将大量的文档归到一个或多个类别的过程。这一技术的关键问题是如何构建一个分类函数或分类模型，并利用这一分类模型将未知文档映射到给定的类别空间。\n按照其领域分类不同的期刊、新闻报道，甚至多文档分类也是可能的。文本分类的一个重要应用之处是垃圾电子邮件检测，除此之外，腾讯、新浪、搜狐之类的门户网站每天产生的信息分繁杂多，依靠人工整理分类是一项耗时巨大的工作且很不现实，此时文本分类技术的应用就显得极为重要。\n5、搜索引擎\n自然语言处理技术例如词义消歧、句法分析、指代消解等技术在搜索引擎中常常被使用。搜索引擎的职责不单单是帮助用户找到答案，还能帮助用户找到所求，连接人与实体世界的服务。搜索引擎最基本的模式是自动化地聚合足够多的内容，对之进行解析、处理和组织，响应用户的搜索请求找到对应结果返回。每一个环节，都需要用到自然语言处理。用百度举例，比如用户可以搜“天气”、“日历”、“机票”及“汇率”这样的模糊需求，会直接在搜索结果呈现结果。用户还可以搜索“范冰冰演过的电视剧”这样的复杂问题，百度都可以准确地回答。\n一方面，有了自然语言处理技术才使得搜索引擎能够快速精准的返回用户的搜索结果，几乎所有的自然语言处理技术都在搜索引擎中有应用的影子；另一方面，搜索引擎（例如谷歌商业帝国和百度巨头）在商业上的成功，也促进了自然语言处理技术的进步。\n6、推荐系统\n第一个推荐系统是 1992 年 Goldberg 提出的 Tapestry，这是一个个性化邮件推荐系统，第一次提出了协同过滤的思想，利用用户的标注和行为信息对邮件进行重排序。推荐系统依赖数据、算法、人机交互等环节的相互配合，应用了数据挖掘技术、信息检索技术以及计算统计学等技术使用推荐系统的目的是联系用户和信息，帮助用户发现对自己有价值的信息，同时让信息能够展示在对它感兴趣的用户面前，精准推荐，用来解决信息过载和用户无明确需求的问题。\n推荐系统在音乐电影的推荐、电子商务产品推荐、个性化阅读、社交网络好友推荐等场景发挥着重要的作用，美国 Netflix 2/3 的电影是因为被推荐而观看，Google news 利用推荐系统提升了 38%的点击率，Amazon 的销售中推荐占比高达 35%。\n7、发展趋势\n随着深度学习时代的来临，神经网络成为一种强大的机器学习工具，自然语言处理取得了许多突破性发展，情绪分析、自动问答、机器翻译等领域都飞速发展。\n下图分别是 AMiner 计算出的自然语言处理近期热点和全球热点。通过对 1994-2017 年间自然语言处理领域有关论文的挖掘，总结出二十多年来，自然语言处理的领域关键词主要集中在计算机语言、神经网络、情感分析、机器翻译、词义消歧、信息提取、知识库和文本分析等领域。旨在基于历史的科研成果数据的基础上，对自然语言处理热度甚至发展趋势进行研究。图中，每个彩色分支表示一个关键词领域，其宽度表示该关键词的研究热度，各关键词在每一年份（纵轴）的位置是按照这一时间点上所有关键词的热度高低进行排序。\n\n\n图 14 显示，情绪分析、词义消歧、知识库和计算机语言学将是最近的热点发展趋势。\n图 15 显示词义消歧、词义理解、计算机语言学、信息检索和信息提取将是自然语言处理全球热点。\n参考文献\n［1］ 中文信息处理发展报告 2016\n［2］ 李涓子，侯磊 知识图谱研究综述.[J]山西大学学报 2017\n［3］ 冯志伟.机器翻译研究.[M].北京：中国对外翻译出版社.2004\n［4］ 冯志伟.自然语言处理的形式模型[M].北京：中国科学技术大学出版社 2010\n［5］ 吴军，数学之美[M].北京：人民邮电出版社 2012\n［6］ 2006-2020 年国家信息化发展战略[Z] 中共中央办公厅、国务院办公厅 2006\n［7］ 刘奕群，马少平，洪涛等 搜索引擎技术基础[M] 北京：清华大学出版社 2010\n［8］ 韩家炜等，数据挖掘：概念与技术[M] 北京：机械工业出版社 2012","data":"2019年04月26日 18:58:28"}
{"_id":{"$oid":"5d34483b62f717dc0659b562"},"title":"xmnlp — 轻量级中文自然语言处理工具","author":"seanlee97","content":"python下NLP工具有很多 jieba, nltk, ltp 等， 虽然他们很强大，但是提供的功能比较分散，而且通常模型比较大。为了方便平时的处理工作，我尝试找了一些集成工具包，发现snownlp还可以，它的分词是基于TnT的，总得来说分词效果逊色于基于词典的分词（比如jieba）。所以决定自己写一个包xmnlp，主打轻量快捷。\n功能\n\n中文分词 \u0026 词性标注： 基于词典构建DAG图，然后采用动态规划的思想求得最大概率路径（jieba分词采用了反向输出，我采用了正向加权反向输出的方式，使得正反向共同影响分词效果），对于未登录词采用HMM+Viterbi处理\n文本纠错：采用了bi-gram + levenshtein实现\n文本摘要 \u0026 关键词提取：textrank\n情感分析：naive bayes\n文本转拼音：Trie 树检索\n以下展示xmnlp的功能效果，不同模块的原理之后的文章会补上。\n分词\u0026词性标注\n[ In ]\n自然语言处理: 是人工智能和语言学领域的分支学科。\n在这此领域中探讨如何处理及运用自然语言；自然语言认知则是指让电脑“懂”人类的语言。\n自然语言生成系统把计算机数据转化为自然语言。\n自然语言理解系统把自然语言转化为计算机程序更易于处理的形式。\n[ 分词 ]\n自然语言处理 / : / 是 / 人工智能 / 和 / 语言学 / 领域 / 的 / 分支 / 学科 / 。 / 在 / 这此 / 领域 / 中 / 探讨 / 如何 / 处理 / 及 / 运用 / 自然 / 语言 / ； / 自然 / 语言 / 认知 / 则 / 是 / 指让 / 电脑 / “ / 懂 / ” / 人类 / 的 / 语言 / 。 / 自然 / 语言 / 生成 / 系统 / 把 / 计算机 / 数据 / 转化 / 为 / 自然 / 语言 / 。 / 自然 / 语言 / 理解 / 系统 / 把 / 自然 / 语言 / 转化 / 为 / 计算机程序 / 更 / 易于 / 处理 / 的 / 形式 / 。\n[ 标注 ]\n自然语言处理 un / : un / 是 v / 人工智能 nw / 和 c / 语言学 n / 领域 n / 的 uj / 分支 n / 学科 n / 。 un / 在 p / 这此 un / 领域 n / 中 f / 探讨 v / 如何 r / 处理 v / 及 c / 运用 vn / 自然 d / 语言 n / ； un / 自然 d / 语言 n / 认知 v / 则 d / 是 v / 指让 un / 电脑 n / “ un / 懂 v / ” un / 人类 n / 的 uj / 语言 n / 。 un / 自然 d / 语言 n / 生成 v / 系统 n / 把 p / 计算机 n / 数据 n / 转化 v / 为 p / 自然 d / 语言 n / 。 un / 自然 d / 语言 n / 理解 v / 系统 n / 把 p / 自然 d / 语言 n / 转化 v / 为 p / 计算机程序 n / 更 d / 易于 v / 处理 v / 的 uj / 形式 n / 。 un\n\n\n文本纠错\n[ In ]\n这理风景绣丽，而且天汽不错，我的心情各外舒畅!\n[ Out ]\n这里风景秀丽，而且天气不错，我的心情格外舒畅!\n\n\n文本摘要\u0026关键词\n[ In ]\n自然语言处理: 是人工智能和语言学领域的分支学科。\n在这此领域中探讨如何处理及运用自然语言；自然语言认知则是指让电脑“懂”人类的语言。\n自然语言生成系统把计算机数据转化为自然语言。\n自然语言理解系统把自然语言转化为计算机程序更易于处理的形式。\n[ 关键词 ]\n('自然语言', 2.2069266136741321),\n('处理', 1.5572478858429686),\n('是', 1.4182222157079281),\n('系统', 1.2431338210535401),\n('转化', 1.1532093387566391)\n[ 摘要 ]\n自然语言理解系统把自然语言转化为计算机程序更易于处理的形式\n\n\n情感分析\n[ In ]\n这件衣服的质量也太差了吧！一穿就烂！\n[ Out ]\n0.009959694621645698\n\n\n文本转拼音\n[ In ]\n面朝大海，春暖花开\n[ Out ]\n['mian', 'zhao', 'da', 'hai', '，', 'chun', 'nuan', 'hua', 'kai']\n\n\n前往 - \u003e 项目github地址","data":"2018年02月16日 08:52:24"}
{"_id":{"$oid":"5d34485b62f717dc0659b568"},"title":"NLPIR智能完美融合人工智能和自然语言处理","author":"weixin_34261415","content":"数据发展到今天，已不再是一个新的概念，基于大数据技术的应用也层出不穷，但作为一项发展前景广阔的技术，其很多作用还有待挖掘，比如为人们的生活带来方便，为企业带来更多利益等。现今，互联网上每日产生的数据已由曾经的TB级发展到了今天的PB级、EB级甚至ZB级。如此爆炸性的数据怎样去使用它，又怎样使它拥有不可估量的价值呢?这就需要不断去研究开发，让每天的数据“砂砾”变为“黄金”。那么如何才能将大量的数据存储起来，并加以分析利用呢，大数据技术应运而生。\n在大数据时代，数据挖掘是最关键的工作。大数据的挖掘是从海量、不完全的、有噪声的、模糊的、随机的大型数据库中发现隐含在其中有价值的、潜在有用的信息和知识的过程，也是一种决策支持过程。其主要基于人工智能，机器学习，模式学习，统计学等。通过对大数据高度自动化地分析，做出归纳性的推理，从中挖掘出潜在的模式，可以帮助企业、商家、用户调整市场政策、减少风险、理性面对市场，并做出正确的决策。\n数据管理理念不断变革，大数据成为信息技术发展的必然选择。随着现代信息传播技术手段和方式不断丰富，信息获取、信息传递、信息处理、信息再生、信息利用等功能应用日益多样化，智能化信息系统逐渐形成一个信息网络体系，人类社会的生产方式、工作方式、学习方式、交往方式、生活方式、思维方式等发生了极其深刻的变革，互动化、即时性、全媒体等，成为常态性的信息生态环境，传统的数据库组织架构和信息服务模式已经难以适应信息社会现实需要，整个信息技术架构的革命性重构势在必行，大数据成为信息技术发展的必由之路。\n灵玖软件NLPIR大数据语义智能分析平台针对中文数据挖掘的综合需求,融合了网络精准采集、自然语言理解、文本挖掘和语义搜索的研究成果,先后历时十八年,服务了全球四十万家机构用户,是大时代语义智能分析的一大利器。\nNLPIR大数据语义智能分析平台平台针对互联网内容处理的需要，融合了自然语言理解、网络搜索和文本挖掘的技术，提供了用于技术二次开发的基础工具集。\nNLPIR能够全方位多角度满足应用者对大数据文本的处理需求，包括大数据完整的技术链条：网络采集、正文提取、中英文分词、词性标注、实体抽取、词频统计、关键词提取、语义信息抽取、文本分类、情感分析、语义深度扩展、繁简编码转换、自动注音、文本聚类等。\n“大数据”的本质实际上是数据生产的社会化，其对统计尤其是政府统计的冲击是重大的，不仅涉及到整个统计流程，更加对当前的政府统计管理体制、机构设置、数据价值等方面形成了挑战。可以大胆预测，未来政府统计的政府角色会被统计专业性取代，经济分析的职能会被更为专业的经济分析部门取代，宏观数据的重要性会让位于更有信息价值的微观数据。","data":"2018年08月29日 11:24:52"}
{"_id":{"$oid":"5d34489d62f717dc0659b575"},"title":"从CNN视角看在自然语言处理上的应用","author":"csdn人工智能","content":"作者 | 卞书青\n\n\n\n\n卷积神经网络（Convolutional Neural Network）最早是应用在计算机视觉当中，而如今CNN也早已应用于自然语言处理（Natural Language Processing）的各种任务。本文主要以CMU CS 11-747（Neural Networks for NLP）课程中Convolutional Networks for Text这一章节的内容作为主线进行讲解。\n\n\n本文主要包括了对如下几块内容的讲解，第一部分是对于常见的语言模型在进行文本表示时遇到的问题以及引入卷积神经网络的意义，第二部分是对于卷积神经网络模块的介绍，第三部分主要是介绍一些卷积神经网络应用于自然语言处理中的论文，第四部分主要是对这一篇综述进行总结。\n引例\n我们首先来看这么一个问题，假设我们需要对句子做情感上的分类。\n\n传统的词袋模型或者连续词袋模型都可以通过构建一个全连接的神经网络对句子进行情感标签的分类，但是这样存在一个问题，我们通过激活函数可以让某些结点激活（例如一个句子里”not”,”hate”这样的较强的特征词），但是由于在这样网络构建里，句子中词语的顺序被忽略，也许同样两个句子都出现了not和hate但是一个句子（I do not hate this movie）表示的是good的情感，另一个句子（I hate this movie and will not choose it）表示的是bad的情感。其实很重要的一点是在刚才上述模型中我们无法捕获像not hate这样由连续两个词所构成的关键特征的词的含义。\n在语言模型里n-gram模型是可以用来解决，想法其实就是将连续的两个词作为一个整体纳入到模型中，这样确实能够解决我们刚才提出的问题，加入bi-gram，tri-gram可以让我们捕捉到例如“don’t love”，“not the best”。但是问题又来了，如果我们使用多元模型，实际训练时的参数是一个非常大的问题，因为假设你有20000个词，加入bi-gram实际上你就要有400000000个词，这样参数训练显然是爆炸的。另外一点，相似的词语在这样的模型中不能共享例如参数权重等，这样就会导致相似词无法获得交互信息。\n卷积神经网络结构的认识\n利用卷积神经网络实际上是可以解决上述的两个问题。在讲卷积神经网络前，我们先来看两个简单的例子。\n\n假设我去识别出左边这个方框里的猫，实际上在一张图片中猫所处的位置并不重要，它在左边，在右边，还是在底部，其实对于猫来说，它的特征是不变的，我需要在这一部分位置学习的特征也能用在另一部分位置上，所以对于这个图像上的所有位置，我们都能使用同样的学习特征。而在右边的例子中，假设一句话中是谈论猫咪的，猫咪这个词的意义是否会随它在第一句话还是第二句话而发生改变呢，大部分情况是不变的，所以我们当我们使用一个文本网络时，网络能够学习到什么是猫咪并且可以重复使用，而不是每一次见到它就要重新学习。\n接下来我们先来介绍卷积神经网络中各个重要的环节。\n卷积\n所以这里我们首先去理解卷积神经网络中卷积的运算。这里我们以图像作为输入。比较容易理解卷积的方法是把卷积想象成作用于矩阵的一个滑动窗口函数。如下面这张图的表示。\n滑动窗口又称作卷积核、滤波器或是特征检测器。图中使用3x3的卷积核，将卷积核与矩阵对应的部分逐元素相乘，然后求和。对于卷积的运算可以看下面这幅图的解释。\n在不改变卷积核权重的情况下，就像拿着一只刷子一样对整个图水平垂直滑动进行卷积运算，这样输出就是经过卷积运算后的输出层。这里有一个对卷积操作的动画演示，可以加深对其的理解（CS231n Convolutional Neural Networks for Visual Recognition）\n什么是卷积神经网络\n卷积神经网络其实就是多层卷积运算，然后对每层的卷积输出用非线性激活函数做转换（后面会讲到）。卷积过程中每块局部的输入区域与输出的一个神经元相连接。对每一层应用不同的卷积核，每一种卷积核其实可以理解为对图片的一种特征进行提取，然后将多种特征进行汇总，以下面这幅图为例，原始的input为一幅图片，第一层卷积过后输出层变为6@28*28，所以这里的卷积核实际上用了6个，6个卷积核代表了对这一张原始图片的六种不同角度的特征提取（例如提取图片左上方的边缘线条，右下方的边缘线条等等）。feature map实际上的含义就是特征通道（或者理解为一个图片的不同特征），也可以说就是输出层的深度，这里就是6，然后后面每一次做卷积操作是都是要对所有的特征通道进行卷积操作以便提取出更高级的特征。这里也涉及到池化层，在下一小节进行讲解。在训练阶段，卷积神经网络会基于你想完成的任务自动学习卷积核的权重值。\n例如，在上面这幅图中，第一层CNN模型也许学会从原始像素点中检测到一些边缘线条，然后根据边缘线条在第二层检测出一些简单的形状（例如横线条，左弯曲线条，竖线条等），然后基于这些形状检测出更高级的特征，比如一个A字母的上半部分等。最后一层则是利用这些组合的高级特征进行分类。\n卷积神经网络中的卷积计算实际上体现了：位置不变性和组合性。位置不变性是因为卷积核是在全图范围内平移，所以并不用关心猫究竟在图片的什么位置。组合性是指每个卷积核对一小块局部区域的低级特征组合形成更高级的特征表示。当然这两点对于句子的建模也是很多的帮助，我们会在后面的例子中提到。\n卷积是如何应用到自然语言处理中\n在图像中卷积核通常是对图像的一小块区域进行计算，而在文本中，一句话所构成的词向量作为输入。每一行代表一个词的词向量，所以在处理文本时，卷积核通常覆盖上下几行的词，所以此时卷积核的宽度与输入的宽度相同，通过这样的方式，我们就能够捕捉到多个连续词之间的特征，并且能够在同一类特征计算时中共享权重。下面这张图很好地诠释了刚才的讲解。\n\n\n图片引用自《A Sensitivity Analysis of (and Practitioners’ Guide to) ConvolutionalNeural Networks for Sentence Classification》Ye Zhang, Byron Wallace\n池化层\n卷积神经网络的一个重要概念就是池化层，一般是在卷积层之后。池化层对输入做降采样。池化的过程实际上是对卷积层分区域求最大值或者对每个卷积层求最大值。例如，下图就是2x2窗口的最大值池化（在自然语言处理中，我们通常对整个输出做池化，每个卷积层只有一个输出值）。\n\n\n图片来自于http://cs231n.github.io/convolutional-networks/#pool\n\n\n为什么要进行池化操作？\n池化首先是可以输出一个固定大小的矩阵，这对于自然语言处理当中输入句子的长度不一有非常大的作用。例如，如果你用了200个卷积核，并对每个输出使用最大池化，那么无论卷积核的尺寸是多大，也无论输入数据的维度或者单词个数如何变化，你都将得到一个200维的输出。这让你可以应对不同长度的句子和不同大小的卷积核，但总是得到一个相同维度的输出结果，用作最后的分类。\n另外池化层在降低数据维度的同时还能够保留显著的特征。每一种卷积核都是用来检测一种特定的特征。在以句子分类中，每一种卷积核可以用来检测某一种含义的词组，如果这种类型的含义的词语出现了，该卷积核的输出值就会非常大，通过池化过程就能够尽可能地将该信息保留下来。\n关于池化层几种池化方式会在下面的内容里讲解。\n激活函数\n有关激活函数很多细节的讲述在最后的总结会提到。\n\n\n卷积神经网络结构在NLP的应用\n首先我们来介绍第一篇论文《Natural Language Processing (almost) from Scratch》，该论文主要是针对原来那种man-made 的输入特征和人工特征，利用神经网络的方法自动抽取出文本句子更高级的特征用来处理自然语言处理里的各项任务，例如本文中输入是一个句子序列，输出是对句子中各个词的词性的预测。该文提出了两种方法，一种是滑动窗口的方法（window approach），另一种就是将整个句子作为输入（sentence approach）的方法，两种方法就分别对应着局部和全局的特征。模型结构如下图所示：\n\n\nwindow approach\nsentence approach\nwindow approach 是根据某一个单词以及其附近固定长度范围内的单词对应的词向量来为单词预测标签。需要注意的是，当处理到一个句子的开始或者结尾的单词的时候，其前后窗口或许不包含单词，这时候我们需要填充技术，为前面或者后面填充象征开始或者结束的符号。\n实际上基于窗口的方法已经可以解决很多常见的任务，但是如果一个单词如果非常依赖上下文的单词，且当时这个单词并不在窗口中，这时就需要sentence approach，这里所使用的卷积操作与卷积神经网络中的卷积操作基本相同。这里需要对句子中的每一个单词进行一次卷积操作，这里池化过程选择最大池化，这里认为句子中大部分的词语对该单词的意义不会有影响。\n刚才这篇论文实际上是在池化层中直接选择了最大池化，接下来的这篇论文《A Convolutional Neural Network for Modelling Sentences》对句子级别特征的池化过程进行了改进并且提出了DCNN动态卷积网络（Dynamic Convolutional Neural Network），在介绍该论文前首先先来介绍一下常见的几种池化方式。\nMax-pooling最为常见，最大池化是取整个区域的最大值作为特征，在自然语言处理中常用于分类问题，希望观察到的特征是强特征，以便可以区分出是哪一个类别。Average-pooling通常是用于主题模型，常常是一个句子不止一个主题标签，如果是使用Max-pooling的话信息过少，所以使用Average的话可以广泛反映这个区域的特征。最后两个K-max pooling是选取一个区域的前k个大的特征。Dynamic pooling是根据网络结构动态调整取特征的个数。最后两个的组合选取，就是该篇论文的亮点。\n该论文的亮点首先对句子语义建模，在底层通过组合邻近的词语信息，逐步向上传递，上层则又组合新的语义信息，从而使得句子中相离较远的词语也有交互行为（或者某种语义联系）。从直观上来看，这个模型能够通过词语的组合，再通过池化层提取出句子中重要的语义信息。\n另一个亮点就是在池化过程中，该模型采用动态k-Max池化，这里池化的结果不是返回一个最大值，而是返回k组最大值，这些最大值是原输入的一个子序列。池化中的参数k可以是一个动态函数，具体的值依赖于输入或者网络的其他参数。该模型的网络结构如下图所示：\n这里重点介绍k-max池化和动态k-max池化。K-max的好处在于，既提取除了句子中不止一个重要信息，同时保留了它们的顺序。同时，这里取k的个数是动态变化的，具体的动态函数如下。\n这里需要注意的是s代表的是句子长度，L代表总的卷积层的个数，l代表的是当前是在几个卷积层，所以可以看出这里的k是随着句子的长度和网络深度而改变，我们的直观的感受也能看出初始的句子层提取较多的特征，而到后面提取的特征将会逐渐变少，同时由于  代表最顶层的卷积层需要提取的个数。\n这里的网络结构大多与通常的卷积网络层，但需要注意的是这里有一个Folding层（折叠操作层）。这里考虑相邻的两行之间的某种联系，将两行的词向量相加。\n该模型亮点很多，总结如下，首先它保留了句子中词序和词语之间的相对位置，同时考虑了句子中相隔较远的词语之间的语义信息，通过动态k-max pooling较好地保留句子中多个重要信息且根据句子长度动态变化特征抽取的个数。\n刚才这篇论文是对池化过程进行改进，接下来的两篇论文是对卷积层进行了改进。第三篇论文是《Neural Machine Translation in Linear Time》，该论文提出了扩张卷积神经网络（Dilated Convolution）应用于机器翻译领域。Dilated convolution实际上要解决的问题是池化层的池化会损失很多信息（无论该信息是有用还是无用）。Dilated convolution的主要贡献就是，如何在去掉池化操作的同时，而不降低网络的感受野。下图理解起来更加容易，卷积的输入像素的间距由1-2-4-8，虽然没有池化层，但是随着层数越深覆盖的原始输入信息依旧在增加。也就是我们通常卷积核与输入的一个区域的维度大小保持一致，但是去掉池化层后，我们随着深度增加，卷积核的所能覆盖的输入区域扩展一倍。\n在该模型中，句子建模时输入是以句子的字符级别开始的，之后随着卷积核所能覆盖的范围扩展，不断地去交互信息，同时还能够保证原始的输入信息不被丢失。\n之前的论文中主要是对卷积层和池化层从本身结构上进行改造，下面的这篇论文主要考虑到了本身句子已有依存句法树信息，将其融入到句子的建模中来。论文《Dependency-based Convolutional Neural Networks for Sentence Embedding》便是提出这一想法，模型的想法是，不仅仅是利用句子中相邻的词信息作为特征信息，一个依存句法树的实际上将句子的语义信息关系真正地提取出来，由于整个卷积的过程，句子的语序关系仍然会丢失，通过将依存句法树中父子节点的语序信息和兄弟语序信息一起作为输入，可以更加有效地抽取句子的特征。\n最后要介绍的一篇论文是有关于句子匹配(Sentence Matching)的问题，基础问题仍然是句子建模。首先，文中提出了一种基于CNN的句子建模网络，卷积的作用是从句子中提取出局部的语义组合信息，而多个Feature Map则是从多种角度进行提取，也就是保证提取的语义组合的多样性。分别单独地对两个句子进行建模（使用上文中的句子模型），从而得到两个相同且固定长度的向量，然后，将这两个向量作为一个多层感知机(MLP)的输入，最后计算匹配的分数。\n这个模型比较简单，但是有一个较大的缺点，两个句子在建模过程中是完全独立的，没有任何交互行为，一直到最后生成抽象的向量表示后才有交互行为，这样做使得句子在抽象建模的过程中会丧失很多语义细节，因此，推出了第二种模型结构。\n这种结构提前了两个句子间的交互行为,第一层中，首先取一个固定的卷积窗口\n，然后遍历中所有组合的二维矩阵进行卷积，每一个二维矩阵输出一个值，构成Layer-2，然后进行2×2的Max-pooling，后续的卷积层均是传统的二维卷积操作。\n总结/Q\u0026A\n本篇综述中具体介绍了卷积神经网络的结构以及应用于自然语言处理中的场景，最后再做一个简单地归纳总结。\n\n还有一些有关卷积神经网络细节上的问题与答案，与大家分享。\n卷积层和池化层有什么区别？\n首先可以从结构上可以看出，卷积之后输出层的维度减小，深度变深。但池化层深度不变。同时池化可以把很多数据用最大值或者平均值代替。目的是降低数据量。降低训练的参数。对于输入层，当其中像素在邻域发生微小位移时，池化层的输出是不变的，从而能提升鲁棒性。而卷积则是把数据通过一个卷积核变化成特征，便于后面的分离。\n采用宽卷积的好处有什么？\n通过将输入边角的值纳入到滑窗中心进行计算，以便损失更少的信息。\n卷积输出的深度与哪个部件的个数相同？\n输出深度（通道）与卷积核（过滤器）的个数相等。\n激活函数通常放在卷积神经网络的那个操作之后？\n通常放在卷积层之后。\n为什么激活函数通常都是采用非线性的函数？\n如果网络中都采用线性函数的组合，那么线性的组合还是线性，那么使用多次线性组合就等同于使用了一次线性函数。因此采用非线性函数可以来逼近任意函数。\n非线性激活函数中sigmod函数存在哪些不足？\nSigmod函数存在饱和状态，尤其是值过大时，当进入饱和状态时，进行梯度下降计算时，很容易出现梯度消失的情况，求导的精确值不能保证。\nReLU和SoftPlus激活函数有哪些优势？\n与sigmod相比，不存在指数计算，求导计算量变小，同时缓解了过拟合的情况，一部分输出为0，减少了参数的相互依存。\n参考文献：\n《Neural Machine Translation in Linear Time》阅读笔记\nhttps://zhuanlan.zhihu.com/c_51425207\n卷积神经网络(CNN)在句子建模上的应用\n\nhttp://www.jeyzhang.com/cnn-apply-on-modelling-sentence.html\n卷积神经网络在自然语言处理的应用\nhttps://zhuanlan.zhihu.com/p/30268946\n相关参考资料链接：\n1. 一个很好的卷积操作的动画演示\nhttp://cs231n.github.io/convolutional-networks/\n2. 宽/窄卷积的动画演示\nhttp://deeplearning.net/software/theano_versions/dev/tutorial/conv_arithmetic.html\n3. Udacity deep learning 课程\nhttps://cn.udacity.com/course/deep-learning--ud730\n4. Github上一个有关深度学习入门的教程/代码\nhttps://github.com/CreatCodeBuild/TensorFlow-and-DeepLearning-Tutorial\n\n\n作者简介：卞书青，2017级研究生，目前研究方向为信息抽取、深度学习，来自中国人民大学大数据管理与分析方法研究北京市重点实验室。\n\n来源：https://zhuanlan.zhihu.com/p/30268946\n\n\n\n\n10 月 28 日（本周六），SDCC 2017“人工智能技术实战线上峰会”将在CSDN学院以直播互动的方式举行。\n\n\n作为SDCC系列技术峰会的一部分，来自阿里巴巴、微软、商汤科技、第四范式、微博、出门问问、菱歌科技的AI专家，将针对机器学习平台、系统架构、对话机器人、芯片、推荐系统、Keras、分布式系统、NLP等热点话题进行分享。\n\n\n先行者们正在关注哪些关键技术？如何从理论跨越到企业创新实践？你将从本次峰会找到答案。每个演讲时段均设有答疑交流环节，与会者和讲师可零距离互动。\n扫描下方二维码，入群交流","data":"2017年10月24日 00:00:00"}
{"_id":{"$oid":"5d3448d862f717dc0659b582"},"title":"中文自然语言处理入门实战","author":"Null__er","content":"转载自：https://blog.csdn.net/valada/article/details/80892583\n第01课：中文自然语言处理的完整机器处理流程\n有机器学习相关经验的人都知道，中文自然语言处理的过程和机器学习过程大体一致，但又存在很多细节上的不同点，下面我们就来看看中文自然语言处理的基本过程有哪些呢？\n获取语料\n语料，即语言材料。语料是语言学研究的内容。语料是构成语料库的基本单元。所以，人们简单地用文本作为替代，并把文本中的上下文关系作为现实世界中语言的上下文关系的替代品。我们把一个文本集合称为语料库（Corpus），当有几个这样的文本集合的时候，我们称之为语料库集合(Corpora)。（定义来源：百度百科）按语料来源，我们将语料分为以下两种：\n1.已有语料\n很多业务部门、公司等组织随着业务发展都会积累有大量的纸质或者电子文本资料。那么，对于这些资料，在允许的条件下我们稍加整合，把纸质的文本全部电子化就可以作为我们的语料库。\n2.网上下载、抓取语料\n如果现在个人手里没有数据怎么办呢？这个时候，我们可以选择获取国内外标准开放数据集，比如国内的中文汉语有搜狗语料、人民日报语料。国外的因为大都是英文或者外文，这里暂时用不到。也可以选择通过爬虫自己去抓取一些数据，然后来进行后续内容。\n语料预处理\n这里重点介绍一下语料的预处理，在一个完整的中文自然语言处理工程应用中，语料预处理大概会占到整个50%-70%的工作量，所以开发人员大部分时间就在进行语料预处理。下面通过数据洗清、分词、词性标注、去停用词四个大的方面来完成语料的预处理工作。\n1.语料清洗\n数据清洗，顾名思义就是在语料中找到我们感兴趣的东西，把不感兴趣的、视为噪音的内容清洗删除，包括对于原始文本提取标题、摘要、正文等信息，对于爬取的网页内容，去除广告、标签、HTML、JS 等代码和注释等。常见的数据清洗方式有：人工去重、对齐、删除和标注等，或者规则提取内容、正则表达式匹配、根据词性和命名实体提取、编写脚本或者代码批处理等。\n2.分词\n中文语料数据为一批短文本或者长文本，比如：句子，文章摘要，段落或者整篇文章组成的一个集合。一般句子、段落之间的字、词语是连续的，有一定含义。而进行文本挖掘分析时，我们希望文本处理的最小单位粒度是词或者词语，所以这个时候就需要分词来将文本全部进行分词。\n常见的分词算法有：基于字符串匹配的分词方法、基于理解的分词方法、基于统计的分词方法和基于规则的分词方法，每种方法下面对应许多具体的算法。\n当前中文分词算法的主要难点有歧义识别和新词识别，比如：“羽毛球拍卖完了”，这个可以切分成“羽毛 球拍 卖 完 了”，也可切分成“羽毛球 拍卖 完 了”，如果不依赖上下文其他的句子，恐怕很难知道如何去理解。\n3.词性标注\n词性标注，就是给每个词或者词语打词类标签，如形容词、动词、名词等。这样做可以让文本在后面的处理中融入更多有用的语言信息。词性标注是一个经典的序列标注问题，不过对于有些中文自然语言处理来说，词性标注不是非必需的。比如，常见的文本分类就不用关心词性问题，但是类似情感分析、知识推理却是需要的，下图是常见的中文词性整理。\n\n常见的词性标注方法可以分为基于规则和基于统计的方法。其中基于统计的方法，如基于最大熵的词性标注、基于统计最大概率输出词性和基于 HMM 的词性标注。\n4.去停用词\n停用词一般指对文本特征没有任何贡献作用的字词，比如标点符号、语气、人称等一些词。所以在一般性的文本处理中，分词之后，接下来一步就是去停用词。但是对于中文来说，去停用词操作不是一成不变的，停用词词典是根据具体场景来决定的，比如在情感分析中，语气词、感叹号是应该保留的，因为他们对表示语气程度、感情色彩有一定的贡献和意义。\n特征工程\n做完语料预处理之后，接下来需要考虑如何把分词之后的字和词语表示成计算机能够计算的类型。显然，如果要计算我们至少需要把中文分词的字符串转换成数字，确切的说应该是数学中的向量。有两种常用的表示模型分别是词袋模型和词向量。\n词袋模型（Bag of Word, BOW)，即不考虑词语原本在句子中的顺序，直接将每一个词语或者符号统一放置在一个集合（如 list），然后按照计数的方式对出现的次数进行统计。统计词频这只是最基本的方式，TF-IDF 是词袋模型的一个经典用法。\n词向量是将字、词语转换成向量矩阵的计算模型。目前为止最常用的词表示方法是 One-hot，这种方法把每个词表示为一个很长的向量。这个向量的维度是词表大小，其中绝大多数元素为 0，只有一个维度的值为 1，这个维度就代表了当前的词。还有 Google 团队的 Word2Vec，其主要包含两个模型：跳字模型（Skip-Gram）和连续词袋模型（Continuous Bag of Words，简称 CBOW），以及两种高效训练的方法：负采样（Negative Sampling）和层序 Softmax（Hierarchical Softmax）。值得一提的是，Word2Vec 词向量可以较好地表达不同词之间的相似和类比关系。除此之外，还有一些词向量的表示方式，如 Doc2Vec、WordRank 和 FastText 等。\n特征选择\n同数据挖掘一样，在文本挖掘相关问题中，特征工程也是必不可少的。在一个实际问题中，构造好的特征向量，是要选择合适的、表达能力强的特征。文本特征一般都是词语，具有语义信息，使用特征选择能够找出一个特征子集，其仍然可以保留语义信息；但通过特征提取找到的特征子空间，将会丢失部分语义信息。所以特征选择是一个很有挑战的过程，更多的依赖于经验和专业知识，并且有很多现成的算法来进行特征的选择。目前，常见的特征选择方法主要有 DF、 MI、 IG、 CHI、WLLR、WFO 六种。\n模型训练\n在特征向量选择好之后，接下来要做的事情当然就是训练模型，对于不同的应用需求，我们使用不同的模型，传统的有监督和无监督等机器学习模型， 如 KNN、SVM、Naive Bayes、决策树、GBDT、K-means 等模型；深度学习模型比如 CNN、RNN、LSTM、 Seq2Seq、FastText、TextCNN 等。这些模型在后续的分类、聚类、神经序列、情感分析等示例中都会用到，这里不再赘述。下面是在模型训练时需要注意的几个点。\n1.注意过拟合、欠拟合问题，不断提高模型的泛化能力。\n过拟合：模型学习能力太强，以至于把噪声数据的特征也学习到了，导致模型泛化能力下降，在训练集上表现很好，但是在测试集上表现很差。\n常见的解决方法有：\n增大数据的训练量；\n增加正则化项，如 L1 正则和 L2 正则；\n特征选取不合理，人工筛选特征和使用特征选择算法；\n采用 Dropout 方法等。\n欠拟合：就是模型不能够很好地拟合数据，表现在模型过于简单。\n常见的解决方法有：\n添加其他特征项；\n增加模型复杂度，比如神经网络加更多的层、线性模型通过添加多项式使模型泛化能力更强；\n减少正则化参数，正则化的目的是用来防止过拟合的，但是现在模型出现了欠拟合，则需要减少正则化参数。\n2.对于神经网络，注意梯度消失和梯度爆炸问题。\n评价指标\n训练好的模型，上线之前要对模型进行必要的评估，目的让模型对语料具备较好的泛化能力。具体有以下这些指标可以参考。\n1.错误率、精度、准确率、精确度、召回率、F1 衡量。\n错误率：是分类错误的样本数占样本总数的比例。对样例集 D，分类错误率计算公式如下：\n\n精度：是分类正确的样本数占样本总数的比例。这里的分类正确的样本数指的不仅是正例分类正确的个数还有反例分类正确的个数。对样例集 D，精度计算公式如下：\n\n对于二分类问题，可将样例根据其真实类别与学习器预测类别的组合划分为真正例（True Positive）、假正例（False Positive）、真反例（True Negative)、假反例（False Negative）四种情形，令 TP、FP、TN、FN 分别表示其对应的样例数，则显然有 TP+FP++TN+FN=样例总数。分类结果的“混淆矩阵”（Confusion Matrix）如下：\n\n准确率，缩写表示用 P。准确率是针对我们预测结果而言的，它表示的是预测为正的样例中有多少是真正的正样例。定义公式如下：\n\n精确度，缩写表示用 A。精确度则是分类正确的样本数占样本总数的比例。Accuracy 反应了分类器对整个样本的判定能力（即能将正的判定为正的，负的判定为负的）。定义公式如下：\n\n召回率，缩写表示用 R。召回率是针对我们原来的样本而言的，它表示的是样本中的正例有多少被预测正确。定义公式如下：\n\nF1 衡量，表达出对查准率/查全率的不同偏好。定义公式如下：\n\n2.ROC 曲线、AUC 曲线。\nROC 全称是“受试者工作特征”（Receiver Operating Characteristic）曲线。我们根据模型的预测结果，把阈值从0变到最大，即刚开始是把每个样本作为正例进行预测，随着阈值的增大，学习器预测正样例数越来越少，直到最后没有一个样本是正样例。在这一过程中，每次计算出两个重要量的值，分别以它们为横、纵坐标作图，就得到了 ROC 曲线。\nROC 曲线的纵轴是“真正例率”（True Positive Rate, 简称 TPR)，横轴是“假正例率”（False Positive Rate,简称FPR），两者分别定义为：\n\n\nROC 曲线的意义有以下几点：\nROC 曲线能很容易的查出任意阈值对模型的泛化性能影响；\n有助于选择最佳的阈值；\n可以对不同的模型比较性能，在同一坐标中，靠近左上角的 ROC 曲所代表的学习器准确性最高。\n如果两条 ROC 曲线没有相交，我们可以根据哪条曲线最靠近左上角哪条曲线代表的学习器性能就最好。但是实际任务中，情况很复杂，若两个模型的 ROC 曲线发生交叉，则难以一般性的断言两者孰优孰劣。此时如果一定要进行比较，则比较合理的判断依据是比较 ROC 曲线下的面积，即AUC（Area Under ROC Curve）。\nAUC 就是 ROC 曲线下的面积，衡量学习器优劣的一种性能指标。AUC 是衡量二分类模型优劣的一种评价指标，表示预测的正例排在负例前面的概率。\n前面我们所讲的都是针对二分类问题，那么如果实际需要在多分类问题中用 ROC 曲线的话，一般性的转化为多个“一对多”的问题。即把其中一个当作正例，其余当作负例来看待，画出多个 ROC 曲线。\n模型上线应用\n模型线上应用，目前主流的应用方式就是提供服务或者将模型持久化。\n第一就是线下训练模型，然后将模型做线上部署，发布成接口服务以供业务系统使用。\n第二种就是在线训练，在线训练完成之后把模型 pickle 持久化，然后在线服务接口模板通过读取 pickle 而改变接口服务。\n模型重构（非必须）\n随着时间和变化，可能需要对模型做一定的重构，包括根据业务不同侧重点对上面提到的一至七步骤也进行调整，重新训练模型进行上线。\n参考文献\n周志华《机器学习》\n李航《统计学习方法》\n伊恩·古德费洛《深度学习》\n第02课：简单好用的中文分词利器 jieba 和 HanLP\n前言\n从本文开始，我们就要真正进入实战部分。首先，我们按照中文自然语言处理流程的第一步获取语料，然后重点进行中文分词的学习。中文分词有很多种，常见的比如有中科院计算所 NLPIR、哈工大 LTP、清华大学 THULAC 、斯坦福分词器、Hanlp 分词器、jieba 分词、IKAnalyzer 等。这里针对 jieba 和 HanLP 分别介绍不同场景下的中文分词应用。\njieba 分词\njieba 安装\n（1）Python 2.x 下 jieba 的三种安装方式，如下：\n全自动安装：执行命令 easy_install jieba 或者 pip install jieba / pip3 install jieba，可实现全自动安装。\n半自动安装：先下载 jieba，解压后运行 python setup.py install。\n手动安装：将 jieba 目录放置于当前目录或者 site-packages 目录。\n安装完通过 import jieba 验证安装成功与否。\n（2）Python 3.x 下的安装方式。\nGithub 上 jieba 的 Python3.x 版本的路径是：https://github.com/fxsjy/jieba/tree/jieba3k。\n通过 git clone https://github.com/fxsjy/jieba.git 命令下载到本地，然后解压，再通过命令行进入解压目录，执行 python setup.py install 命令，即可安装成功。\njieba 的分词算法\n主要有以下三种：\n基于统计词典，构造前缀词典，基于前缀词典对句子进行切分，得到所有切分可能，根据切分位置，构造一个有向无环图（DAG）；\n基于DAG图，采用动态规划计算最大概率路径（最有可能的分词结果），根据最大概率路径分词；\n对于新词(词库中没有的词），采用有汉字成词能力的 HMM 模型进行切分。\njieba 分词\n下面我们进行 jieba 分词练习，第一步首先引入 jieba 和语料:\nimport jieba content = \"现如今，机器学习和深度学习带动人工智能飞速的发展，并在图片处理、语音识别领域取得巨大成功。\"\n（1）精确分词\n精确分词：精确模式试图将句子最精确地切开，精确分词也是默认分词。\nsegs_1 = jieba.cut(content, cut_all=False)print(“/”.join(segs_1))\n其结果为：\n现如今/，/机器/学习/和/深度/学习/带动/人工智能/飞速/的/发展/，/并/在/图片/处理/、/语音/识别/领域/取得/巨大成功/。\n（2）全模式\n全模式分词：把句子中所有的可能是词语的都扫描出来，速度非常快，但不能解决歧义。\nsegs_3 = jieba.cut(content, cut_all=True) print(\"/\".join(segs_3))\n结果为：\n现如今/如今///机器/学习/和/深度/学习/带动/动人/人工/人工智能/智能/飞速/的/发展///并/在/图片/处理///语音/识别/领域/取得/巨大/巨大成功/大成/成功//\n（3）搜索引擎模式\n搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。\nsegs_4 = jieba.cut_for_search(content) print(\"/\".join(segs_4))\n结果为：\n如今/现如今/，/机器/学习/和/深度/学习/带动/人工/智能/人工智能/飞速/的/发展/，/并/在/图片/处理/、/语音/识别/领域/取得/巨大/大成/成功/巨大成功/。\n（4）用 lcut 生成 list\njieba.cut 以及 jieba.cut_for_search 返回的结构都是一个可迭代的 Generator，可以使用 for 循环来获得分词后得到的每一个词语（Unicode）。jieba.lcut 对 cut 的结果做了封装，l 代表 list，即返回的结果是一个 list 集合。同样的，用 jieba.lcut_for_search 也直接返回 list 集合。\nsegs_5 = jieba.lcut(content) print(segs_5)\n结果为：\n['现如今', '，', '机器', '学习', '和', '深度', '学习', '带动', '人工智能', '飞速', '的', '发展', '，', '并', '在', '图片', '处理', '、', '语音', '识别', '领域', '取得', '巨大成功', '。']\n（5）获取词性\njieba 可以很方便地获取中文词性，通过 jieba.posseg 模块实现词性标注。\nimport jieba.posseg as psg print([(x.word,x.flag) for x in psg.lcut(content)])\n结果为：\n[('现如今', 't'), ('，', 'x'), ('机器', 'n'), ('学习', 'v'), ('和', 'c'), ('深度', 'ns'), ('学习', 'v'), ('带动', 'v'), ('人工智能', 'n'), ('飞速', 'n'), ('的', 'uj'), ('发展', 'vn'), ('，', 'x'), ('并', 'c'), ('在', 'p'), ('图片', 'n'), ('处理', 'v'), ('、', 'x'), ('语音', 'n'), ('识别', 'v'), ('领域', 'n'), ('取得', 'v'), ('巨大成功', 'nr'), ('。', 'x')]\n（6）并行分词\n并行分词原理为文本按行分隔后，分配到多个 Python 进程并行分词，最后归并结果。\n用法：\njieba.enable_parallel(4) # 开启并行分词模式，参数为并行进程数 。jieba.disable_parallel() # 关闭并行分词模式 。\n注意： 并行分词仅支持默认分词器 jieba.dt 和 jieba.posseg.dt。目前暂不支持 Windows。\n（7）获取分词结果中词列表的 top n\nfrom collections import Counter top5= Counter(segs_5).most_common(5) print(top5)\n结果为：\n[('，', 2), ('学习', 2), ('现如今', 1), ('机器', 1), ('和', 1)]\n（8）自定义添加词和字典\n默认情况下，使用默认分词，是识别不出这句话中的“铁甲网”这个新词，这里使用用户字典提高分词准确性。\ntxt = \"铁甲网是中国最大的工程机械交易平台。\" print(jieba.lcut(txt))\n结果为：\n['铁甲', '网是', '中国', '最大', '的', '工程机械', '交易平台', '。']\n如果添加一个词到字典，看结果就不一样了。\njieba.add_word(\"铁甲网\") print(jieba.lcut(txt))\n结果为：\n['铁甲网', '是', '中国', '最大', '的', '工程机械', '交易平台', '。']\n但是，如果要添加很多个词，一个个添加效率就不够高了，这时候可以定义一个文件，然后通过 load_userdict()函数，加载自定义词典，如下：\njieba.load_userdict('user_dict.txt') print(jieba.lcut(txt))\n结果为：\n['铁甲网', '是', '中国', '最大', '的', '工程机械', '交易平台', '。']\n注意事项：\njieba.cut 方法接受三个输入参数: 需要分词的字符串；cut_all 参数用来控制是否采用全模式；HMM 参数用来控制是否使用 HMM 模型。\njieba.cut_for_search 方法接受两个参数：需要分词的字符串；是否使用 HMM 模型。该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细。\nHanLP 分词\npyhanlp 安装\n其为 HanLP 的 Python 接口，支持自动下载与升级 HanLP，兼容 Python2、Python3。\n安装命令为 pip install pyhanlp，使用命令 hanlp 来验证安装。\npyhanlp 目前使用 jpype1 这个 Python 包来调用 HanLP，如果遇到：\nbuilding ‘_jpype’ extensionerror: Microsoft Visual C++ 14.0 is required. Get it with “Microsoft VisualC++ Build Tools”: http://landinghub.visualstudio.com/visual-cpp-build-tools\n则推荐利用轻量级的 Miniconda 来下载编译好的 jpype1。\nconda install -c conda-forge jpype1 pip install pyhanlp\n未安装 Java 时会报错：\njpype.jvmfinder.JVMNotFoundException: No JVM shared library file (jvm.dll) found. Try setting up the JAVAHOME environment variable properly.\nHanLP 主项目采用 Java 开发，所以需要 Java 运行环境，请安装 JDK。\n命令行交互式分词模式\n在命令行界面，使用命令 hanlp segment 进入交互分词模式，输入一个句子并回车，HanLP 会输出分词结果：\n\n可见，pyhanlp 分词结果是带有词性的。\n服务器模式\n通过 hanlp serve 来启动内置的 HTTP 服务器，默认本地访问地址为：http://localhost:8765 。\n\n\n也可以访问官网演示页面：http://hanlp.hankcs.com/。\n通过工具类 HanLP 调用常用接口\n通过工具类 HanLP 调用常用接口，这种方式应该是我们在项目中最常用的方式。\n（1）分词\nfrom pyhanlp import * content = \"现如今，机器学习和深度学习带动人工智能飞速的发展，并在图片处理、语音识别领域取得巨大成功。\" print(HanLP.segment(content))\n结果为：\n[现如今/t, ，/w, 机器学习/gi, 和/cc, 深度/n, 学习/v, 带动/v, 人工智能/n, 飞速/d, 的/ude1, 发展/vn, ，/w, 并/cc, 在/p, 图片/n, 处理/vn, 、/w, 语音/n, 识别/vn, 领域/n, 取得/v, 巨大/a, 成功/a, 。/w]\n（2）自定义词典分词\n在没有使用自定义字典时的分词。\ntxt = \"铁甲网是中国最大的工程机械交易平台。\" print(HanLP.segment(txt))\n结果为：\n[铁甲/n, 网/n, 是/vshi, 中国/ns, 最大/gm, 的/ude1, 工程/n, 机械/n, 交易/vn, 平台/n, 。/w]\n添加自定义新词：\nCustomDictionary.add(\"铁甲网\") CustomDictionary.insert(\"工程机械\", \"nz 1024\") CustomDictionary.add(\"交易平台\", \"nz 1024 n 1\") print(HanLP.segment(txt))\n结果为：\n[铁甲网/nz, 是/vshi, 中国/ns, 最大/gm, 的/ude1, 工程机械/nz, 交易平台/nz, 。/w]\n当然了，jieba 和 pyhanlp 能做的事还有很多，关键词提取、自动摘要、依存句法分析、情感分析等，后面章节我们将会讲到，这里不再赘述。\n参考文献：\nhttps://github.com/fxsjy/jieba\nhttps://github.com/hankcs/pyhanlp","data":"2018年09月05日 11:29:02"}
{"_id":{"$oid":"5d344b0562f717dc0659b5f6"},"title":"数据挖掘，机器学习，自然语言处理这三者是什么关系?","author":"小码哥kylin","content":"数据挖掘与机器学习是两个不同的概念； 数据挖掘中使用到机器学习的各种工具，而自然语言处理也是是一种机器学习的方式，属于数据挖掘的范畴。 数据挖掘（英语：Data mining），又译为资料探勘、数据采矿。它是数据库知识发现 （英语：Knowledge-Discovery in Databases，简称：KDD) 中的一个步骤。数据挖掘一般是指从大量的数据中自动搜索隐藏于其中的有着特殊关系性 （属于Association rule learning）的信息的过程。 数据挖掘通常与计算机科学有关，并通过统计、在线分析处理、情报检索、机器学习、 专家系统（依靠过去的经验法则）和模式识别等诸多方法来实现 上述目标。 机器学习(Machine Learning, ML)是一门多领域交叉学科，涉及概率论、统计学、逼近论、 凸分析、算法复杂度理论等多门学科。 专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构 使之不断改善自身的性能。 它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域， 它主要使用归纳、综合而不是演绎。 自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用 自然语言进行有效通信的各种理论和方法。 自然语言处理是一门融语言学、计算机科学、数学于一体的科学。因此，这一领域的研究将涉及自然语言， 即人们日常使用的语言， 所以它与语言学的研究有着密切的联系，但又有重要的区别。自然语言处理并不是一般地研究自然语言， 而在于研制能有效地实现自然语言通信的 计算机系统，特别是其中的软件系统。因而它是计算机科学的一部分。 自然语言处理(NLP)是计算机科学，人工智能，语言学关注计算机和人类(自然)语言之间的相互作用的领域。","data":"2017年10月30日 14:27:36"}
{"_id":{"$oid":"5d344b1b62f717dc0659b5fe"},"title":"中文自然语言处理可能是 NLP 中最难的？","author":"喜欢打酱油的老鸟","content":"现如今，在更多情况下，我们通过传感器和字节来与机器获得交流，而不是依靠交换情感，那如何让超级智能机器能够和人类正常交流沟通呢？\n在人工智能背景技术下，自然语言处理（NLP）技术被越来越多的人看好，并受到重视。\n其中，以微软小冰为代表的聊天机器人，如今却成了网红，迅速刷爆了微信和朋友圈，一个17岁纯情少女懂礼貌、有素质和会作诗，众多网友对她可是情有独钟！下面这幅图是小冰的一个简介。\n那什么是 NLP？\nNLP (NaturalLanguage Processing) 是人工智能（AI）的一个子领域。自然语言是人类智慧的结晶，自然语言处理是人工智能中最为困难的问题之一，它是能够让人类与智能机器进行沟通交流的重要技术手段。因此，自然语言处理的研究也是充满魅力和挑战的。\nNLP 的主要范畴有哪些？\nNLP 作为一种人工智能方法，能够处理机器和人类自然语言之间的交互，即 NLP 帮助计算机机器以各种形式使用自然人类语言进行交流，包括进行分析、理解、改变或生成自然语言。主要涉及的范畴如下（维基百科）：\n• 中文自动分词\n• 词性标注\n• 句法分析\n• 文本分类\n• 信息抽取\n• 知识图谱\n• 问答系统和自动聊天机器人\n• 机器翻译\n• 自动摘要\n为什么要学 NLP？\n人工智能的发展势不可挡，不可否认,当前从事互联网的人们已经制造出了海量的数据，未来还将继续持续，其中包括结构化数据、半结构化和非结构化数据。\n笔者发现，对于结构化数据而言，在大数据、云计算技术“上下齐心”的大力整合下，其技术基本趋向成熟和稳定，而半结构化、非结构化的数据，因其自身的复杂性，在当前和未来更多领域应用都具有很大的困难和挑战。\n而当前市场对于 NLP 技术人才的需求又非常急切，而且这种状态将持续5-10年，大部分企业需要懂 NLP 技术的人来处理海量非结构数据。\n对于大多数人来说，学完一门技术，最终的目的是找到自己满意的工作，包括自己感兴趣的领域、舒适的环境和高薪。\n单纯从高薪来看，不仅意味着很多 money，更是来证明自己优秀。下面是 BOSS 直聘上对 NLP 技术人员的待遇需求，可以看到仅仅是NLP开发工程师（当然要懂算法）薪资在30-60k。\n如何入门中文 NLP ？\n作为初学者，笔者当初也是走过很多弯路。其中很重要的一点是，我们常常遇到这样的尴尬。\n网上大部分自然语言处理内容都是英文为基础，大多数人先是学好了英语的处理，回头来再处理中文，却发现有很大的不同，这样不仅让中文自然语言处理学习者走了弯路，也浪费了大量时间和精力。\n中文的处理比英文复杂的多，网上中文相关资料少之又少，国内纯中文自然语言处理书籍只有理论方面的，却在实战方面比较空缺，这让中文自然语言处理的学习者感到举步维艰，很难下笔。\n对于这样的难点，是不是认为中文 NLP 就很难学呢？答案是：非也。相反笔者认为，入门中文 NLP 最快的捷径就是以小数量的实例，边学边实战。\nhttps://blog.csdn.net/qq_36330643/article/details/80772390","data":"2018年08月04日 07:52:13"}
{"_id":{"$oid":"5d344c2e62f717dc0659b631"},"title":"在自然语言处理领域，哪些企业的发展遥遥领先？（附报告）","author":"数据派THU","content":"后台回复关键词“NLP”下载研究报告（含人才分布图）\n\n\n目录\n\n\n第 1 章 自然语言处理概念篇\n第 2 章 自言语言处理技术篇\n第 3 章 自然语言处理人才篇\n第 4 章 自然语言处理应用篇\n第 5 章 自然语言处理趋势篇\n\n\n自然语言处理是包括了计算机科学、语言学心理认知学等一系列学科的一门交叉学科，这些学科性质不同但又彼此相互交叉。\n\n\n\n1950年图灵提出了著名的“图灵测试”，这一般被认为是自然语言处理思想的开端。\n\n\n\n\n20世纪50年代到70年代自然语言处理主要采用基于规则的方法。\n\n\n70年代以后随着互联网的高速发展，自然语言处理思潮由理性主义向经验主义过渡，基于统计的方法逐渐代替了基于规则的方法。\n\n\n从2008年到现在，在图像识别和语音识别领域的成果激励下，人们也逐渐开始引入深度学习来做自然语言处理研究。\n\n\n\n\n由最初的词向量到2013年word2vec，将深度学习与自然语言处理的结合推向了高潮，并在机器翻译、问答系统、阅读理解等领域取得了一定成功。\n\n\n接下来我们将为大家介绍自然语言处理的业界发展，涵盖了以下企业。\n\n\n\n\n微软亚洲研究院\n\n\n微软亚洲研究院1998年成立自然语言计算组，研究内容包括多国语言文本分析、机器翻译、跨语言信息检索和自动问答系统等。\n这些研究项目研发了一系列实用成果，如IME（Input Method Editors输入法编辑器，它是一种专门的应用程序， 用来输入代表东亚地区书面语言文字的不同字符。）、对联游戏、Bing词典、Bing翻译器、语音翻译、搜索引擎等，为微软产品做出了重大的贡献。\n\n\n微软IME\n\n\n微软对联游戏\n\n\n微软必应词典\n\n\n并且在自然语言处理顶级会议，例如ACL、COLING等会议上发表了许多论文。\n\n\n语音翻译\n\n\n2017年微软在语音翻译上全面采用了神经网络机器翻译，并新扩展了Microsoft Translator Live Feature。\n\n\n可以在演讲和开会时，实时同步在手机端和桌面端，同时把讲话者的话翻译成多种语言。\n\n\n\n\n其中最重要的技术是对于源语言的编码以及引进的语言知识，同时，微软还表示，将来要将知识图谱纳入神经网络机器翻译中规划语言理解的过程中。\n\n\n人机对话\n\n\n小娜现在已经拥有超过1.4亿用户，在数以十亿计的设备上与人们进行交流，并且覆盖了十几种语言。\n\n\n\n\n有聊天机器人小冰，正在试图把各国语言的知识融合在一起，实现一个开放语言自由聊天的过程，目前小冰实现了中文、日文和英文的覆盖，有上亿用户。\n\n\n\n\nGoogle\n\n\nGoogle是最早开始研究自然语言处理技术的团队之一，作为一个以搜索为核心的公司，Google对自然语言处理更为重视。\n\n\n\n\nGoogle拥有着海量数据，可以搭建丰富庞大的数据库，可以为其研究提供强大的数据支撑。\n\n\nGoogle对自然语言处理的研究侧重于应用规模、跨语言和跨领域的算法。\n\n\n机器翻译\n知识图谱\n\n\nGoogle的知识图谱更是遥遥领先，例如自动挖掘新知识的准确程度、文本中命名实体的识别、纯文本搜索词条到在知识图谱上的结构化搜索词条的转换等，效果都领先于其他公司，而且很多技术都实现了产品化。\n\n\n\n\n语音识别\n\n\nGoogle一直致力于投资语音搜索技术和苹果公司的siri竞争，自2012年以来将神经网络应用于这一领域，使语音识别错误率极大降低。\n\n\n2011年收购语言信息平台SayNow，把语音通信、点对点对话、以及群组通话和社交应用融合在一起。\n\n\n2014年收购了SR Tech Group的多项语音识别相关专利。\n\n\nFacebook\n\n\nFacebook涉猎自然语言处理较晚，2013年开始发展语音翻译，2015年开始语音识别的研发之路。\n\n\n语音翻译\n\n\n发展道路如下图所示\n\n\n\n\n语音识别\n\n\n2015年，Facebook相继建立语音识别和对话理解工具，开始了语音识别的研发之路。\n\n\n2016年Facebook开发了一个响应“Hey Oculus”的语音识别系统。\n\n\n并在2018年初开发了wav2letter，这是一个简单高效的端到端自动语音识别（ASR）系统。\n\n\n百度\n\n\n百度自然语言处理部是百度最早成立的部门之一，研究涉及以下方面。\n百度在深度问答方向经过多年打磨，积累了问句理解、答案抽取、观点分析与聚合等方面的一整套技术方案，目前已经在搜索、度秘等多个产品中实现应用。\n\n\n百度翻译目前支持全球28种语言，覆盖756个翻译方向，支持文本、语音、图像等翻译功能，并提供精准人工翻译服务，满足不同场景下的翻译需求，发布了世界上首个线上神经网络翻译系统，并获得2015年度国家科技进步奖。\n\n\n阿里巴巴\n\n\n阿里自然语言处理为其产品服务，在电商平台中构建知识图谱实现智能导购，同时进行全网用户兴趣挖掘，在客服场景中也运用自然语言处理技术打造机器人客服。\n\n\n例如蚂蚁金融智能小宝、淘宝卖家的辅助工具千牛插件等，同时进行语音识别以及后续分析。\n\n\n\n\n阿里的机器翻译主要与其国家化电商的规划相联系，2017年初阿里正式上线了自主开发的神经网络翻译系统，进一步提升了其翻译质量。\n\n\n腾讯\n\n\nAI Lab是腾讯的人工智能实验室，研究领域包括计算机视觉、语音识别、自然语言处理、机器学习等。\n\n\n\n\n其研发的腾讯文智自然语言处理基于并行计算、分布式爬虫系统，结合独特的语义分析技术，可满足自然语言处理、转码、抽取、数据抓取等需求。\n\n\n在机器翻译方面，2017年腾讯宣布翻译君上线“同声传译”新功能，用户边说边翻的需求得到满足，语音识别+NMT等技术的应用保证了边说边翻的速度与精准性。\n\n\n京东\n\n\n京东在人工智能的浪潮中也不甘落后。京东AI开放平台基本上由模型定制化平台和在线服务模块构成，其中在线服务模块包括计算机视觉、语音交互、自然语言处理和机器学习等。\n\n\n按照京东的规划，NeuHub平台将作为普惠性开放平台，不同角色均可找到适合自己的场景，例如用简单代码即可实现对图像质量的分析评估。\n\n\n\n\n从业务上说，平台可以支撑科研人员、算法工程师不断设计新的AI能力以满足用户需求。\n\n\n并深耕电商、供应链、物流、金融、广告等多个领域应用，探索试验医疗、扶贫、政务、养老、教育、文化、体育等多领域应用。聚焦于新技术和行业趋势研究，孵化行业最新落地项目。\n\n\n科大讯飞\n\n\n科大讯飞股份有限公司成立于1999年，是一家专业从事智能语音及语言技术、人工智能技术研究、软件及芯片产品开发、语音信息服务及电子政务系统集成的国家级骨干软件企业。\n\n\n\n\n科大讯飞作为中国智能语音与人工智能产业领导者，在语音合成、语音识别、口语评测、自然语言处理等多项技术上拥有国际领先的成果。\n\n\n科大讯飞成立之时就开始在语言和翻译领域布局项目。基于深度神经网络算法上的创新和突破，在翻译方面的发展如下图所示。\n\n\n后台回复关键词“NLP”下载研究报告（含人才分布图）\n\n\n版权声明\n\n\nAMiner属于清华-青岛数据科学研究院科技大数据研究中心。\n（www.ids.tsinghua.edu.cn）\n\n\nAMiner咨询产品版权为AMiner团队独家所有，拥有唯一著作权。AMiner咨询产品是AMiner团队的研究与统计成果，其性质是供客户内部参考的商业资料。\n\n\n\nAMiner咨询产品为有偿提供给购买该产品的客户使用，并仅限于该客户内部使用。未获得AMiner团队书面授权，任何人不得以任何方式在任何媒体上(包括互联网)公开发布、复制，且不得以任何方式将本产品的内容提供给其他单位或个人使用。如引用、刊发，需注明出处为“AMiner.org”，且不得对本报告进行有悖原意的删节与修改。否则引起的一切法律后果由该客户自行承担，同时AMiner团队亦认为其行为侵犯了AMiner团队著作权，AMiner团队有权依法追究其法律责任。\nAMiner咨询产品是基于AMiner团队及其研究员认为可信的公开资料，但AMiner团队及其研究员均不保证所使用的公开资料的准确性和完整性，也不承担任何投资者因使用本产品与服务而产生的任何责任。\n行业研究报告是AMiner团队智能服务体系的重要组成部分。如对有关信息或问题有深入需求的客户，欢迎使用AMiner团队专项研究智能服务。","data":"2018年07月26日 19:00:00"}
{"_id":{"$oid":"5d344c4762f717dc0659b639"},"title":"自然语言处理（NLP）与自然语言理解（NLU）的区别","author":"hlang8160","content":"自然语言处理主要步骤包括：\n\n1.分词（只针对中文，英文等西方字母语言已经用空格做好分词了）：将文章按词组分开\n\n2.词法分析：对于英文，有词头、词根、词尾的拆分，名词、动词、形容词、副词、介词的定性，多种词意的选择。比如DIAMOND，有菱形、棒球场、钻石3个含义，要根据应用选择正确的意思。\n\n3.语法分析：通过语法树或其他算法，分析主语、谓语、宾语、定语、状语、补语等句子元素。\n\n4.语义分析：通过选择词的正确含义，在正确句法的指导下，将句子的正确含义表达出来。方法主要有语义文法、格文法。\n\n但是以上的分析，仅适用于小规模的实验室研究，远不能应用到实际语言环境中，比如说语法，我们能总结出的语法是有限的，可是日常应用的句子，绝大部分是不遵守语法的，如果让语法包罗所有可能的应用，会出现爆炸的景象。\n\n自然语言处理的应用方向主要有：\n\n1.文本分类和聚类：主要是将文本按照关键字词做出统计，建造一个索引库，这样当有关键字词查询时，可以根据索引库快速地找到需要的内容。此方向是搜索引擎的基础，在早期的搜索引擎，比如北大开发的“天问系统”，采用这种先搜集资料、在后台做索引、在前台提供搜索查询服务。目前GOOGLE，百度的搜索引擎仍旧类似，但是采用了自动“蜘蛛”去采集网络上的信息，自动分类并做索引，然后再提供给用户。我曾经在我的文章中做过测试，当文章中有“十八禁”这样的字眼时，点击次数是我其他文章点击次数的几十倍，说明搜索引擎将“十八禁”这个词列为热门索引，一旦有一个“蜘蛛”发现这个词，其他“蜘蛛”也会爬过来。\n\n2.信息检索和过滤：这是网络瞬时检查的应用范畴，主要为网警服务，在大流量的信息中寻找关键词，找到了就要做一些其他的判断，比如报警。\n\n3.信息抽取：（抄书）信息抽取研究旨在为人们提供更有力的信息获取工具，以应对信息爆炸带来的严重挑战。与信息检索不同，信息抽取直接从自然语言文本中抽取事实信息。过去十多年来，信息抽取逐步发展成为自然语言处理领域的一个重要分支，其独特的发展轨迹——通过系统化、大规模地定量评测推动研究向前发展，以及某些成功启示，如部分分析技术的有效性、快速自然语言处理系统开发的必要性，都极大地推动了自然语言处理研究的发展，促进了自然语言处理研究与应用的紧密结合。回顾信息抽取研究的历史，总结信息抽取研究的现状，将有助于这方面研究工作向前发展。\n\n4.问答系统：目前仍局限于80年代的专家系统，就是按照LISP语言的天然特性，做逻辑递归。LISP语言是括号式的语言，比如A=（B，C，D），A=（B，E，F），提问：已知B，C，能得到什么样的结论？结论是A，D；若提问改为已知B，结论则是C，D，A或E，F，A。比如一个医疗用的专家系统，你若询问“感冒”的治疗方法，系统可能给出多种原因带来的感冒极其治疗方法，你若询问“病毒性感冒”的治疗方法，则系统会给出比较单一的、明确的治疗方法。你有没有用过AUTOCAD系统，这个就是建立在LISP语言上的括号系统，在用的时候会出现上述情况。\n\n5.拼音汉字转换系统：这应该是中文输入法应用范畴的东西，再多的东西我就没想过。\n\n6.机器翻译：当前最热门的应用方向，这方面的文章最多。国际上已经有比较好的应用系统，美国有个AIC公司推出过著名的实时翻译系统，欧共体的SYSTRAN系统可以将英、法、德、西、意、葡六种语言实时对译，美、日、德联合开发的自动语音翻译系统，成功进行了10多分钟对话。我国军事科学院、中科院也开发过此类系统。但是这里边的问题也很多，最主要的是“满篇洋文难不住，满篇译文看不懂”，就是脱离了人类智慧的机器翻译，总会搞出让人无法理解的翻译，比如多意词选择哪个意思合适、怎么组织出通顺的语句，等等。所以目前微软、GOOGLE的新趋势是：翻译+记忆，类似机器学习，将大量以往正确的翻译存储下来，通过检索，如果碰到类似的翻译要求，将以往正确的翻译结果拿出来用。GOOGLE宣称今后几年就可以推出商业化的网页翻译系统。\n\n7.新信息检测：这个我不知道，没思路。\n\n以上已经回答了自然语言发展方向的问题。我认为机器翻译是最有前途的方向，其难点在于机器翻译还不具备人类智能，虽然翻译已经达到90%以上的正确程度，然而还是不能象人类翻译那样，可以准确表达。为什么存在这样的难点？关键是自然语言处理做不到人类对自然语言的理解，“处理”和“理解”是天差地别的两个概念。“处理”好比控制眼睛、耳朵、舌头的神经，他们将接收的信息转化成大脑可以理解的内部信息，或者反过来，他们的功能就是这么多。而“理解”则是大脑皮层负责语言理解那部分，多少亿的脑细胞共同完成的功能。一个人因为其自身家庭背景、受教育程度、接触现实中长期形成的条件反射刺激、特殊的强列刺激、当时的心理状况，这么多的因素都会影响和改变“理解”的功能，比如我说“一个靓女开着BMW跑车”，有人心里会想这是二奶吧？有人心里会仇视她，联想到她会撞了人白撞；做汽车买卖的人则会去估量这部车的价值；爱攀比的人也许会想，我什么时候才能开上BWM？所以“理解”是更加深奥的东西，涉及更多神经学、心理学、逻辑学领域。\n\n还有上下文理解问题，比如这句：“我们90平方米以后会占的分量越来越大，那么这样他的价格本身比高档低很多，所以对于整体把这个价格水平给压下来了，这个确实非常好的。” 你能理解么？估计很难或者理解出多种意思，但是我把前文写出来：“去年国家九部委联合发布了《建设部等部门关于调整住房供应结构稳定住房价格意见的通知》，对90平方米以下住房须占总面积的70%以上作出了硬性规定，深圳市经过一年的调控，目前已做到每个项目的75%都是90平方米以内。深圳市国土资源和房产管理局官员说”看了后面的你才能知道是根据国家的通知，深圳做了相应的调整。\n\n“解决的大体思路”，很多自然语言处理目前还处于探索阶段，还根本没有好的解决办法，推荐几个网站：\n1.中国科学院计算技术研究所·数字化室＆软件室：http://www.nlp.org.cn/\n2.北大计算语言所：http://icl.pku.edu.cn/\n\n3.麻省理工人工智能实验室：http://www.csail.mit.edu/index.php\n转载自：https://blog.csdn.net/riverflowrand/article/details/51355238","data":"2018年05月08日 16:13:39"}
{"_id":{"$oid":"5d344c7a62f717dc0659b646"},"title":"python 自然语言处理(NLP)入门","author":"人鱼线","content":"本文简要介绍Python自然语言处理(NLP)，使用Python的NLTK库。NLTK是Python的自然语言处理工具包，在NLP领域中，最常使用的一个Python库。\n什么是NLP？\n简单来说，自然语言处理(NLP)就是开发能够理解人类语言的应用程序或服务。\n这里讨论一些自然语言处理(NLP)的实际应用例子，如语音识别、语音翻译、理解完整的句子、理解匹配词的同义词，以及生成语法正确完整句子和段落。\n这并不是NLP能做的所有事情。\nNLP实现\n搜索引擎: 比如谷歌，Yahoo等。谷歌搜索引擎知道你是一个技术人员，所以它显示与技术相关的结果；\n社交网站推送:比如Facebook News Feed。如果News Feed算法知道你的兴趣是自然语言处理，就会显示相关的广告和帖子。\n语音引擎:比如Apple的Siri。\n垃圾邮件过滤:如谷歌垃圾邮件过滤器。和普通垃圾邮件过滤不同，它通过了解邮件内容里面的的深层意义，来判断是不是垃圾邮件。\nNLP库\n下面是一些开源的自然语言处理库(NLP)：\nNatural language toolkit (NLTK);\nApache OpenNLP;\nStanford NLP suite;\nGate NLP library\n其中自然语言工具包(NLTK)是最受欢迎的自然语言处理库(NLP)，它是用Python编写的，而且背后有非常强大的社区支持。\nNLTK也很容易上手，实际上，它是最简单的自然语言处理(NLP)库。\n在这个NLP教程中，我们将使用Python NLTK库。\n安装 NLTK\n如果您使用的是Windows/Linux/Mac，您可以使用pip安装NLTK:\npip install nltk\n1\n打开python终端导入NLTK检查NLTK是否正确安装：\nimport nltk\n1\n如果一切顺利，这意味着您已经成功地安装了NLTK库。首次安装了NLTK，需要通过运行以下代码来安装NLTK扩展包:\nimport nltk nltk.download()\n1\n2\n3\n这将弹出NLTK 下载窗口来选择需要安装哪些包:\n\n您可以安装所有的包，因为它们的大小都很小，所以没有什么问题。\n使用Python Tokenize文本\n首先，我们将抓取一个web页面内容，然后分析文本了解页面的内容。\n我们将使用urllib模块来抓取web页面:\nimport urllib.request response = urllib.request.urlopen('http://php.net/') html = response.read() print (html)\n1\n2\n3\n4\n5\n从打印结果中可以看到，结果包含许多需要清理的HTML标签。\n然后BeautifulSoup模块来清洗这样的文字:\nfrom bs4 import BeautifulSoup import urllib.request response = urllib.request.urlopen('http://php.net/') html = response.read() soup = BeautifulSoup(html,\"html5lib\")\n1\n2\n3\n4\n5\n6\n这需要安装html5lib模块\ntext = soup.get_text(strip=True) print (text)\n1\n2\n现在我们从抓取的网页中得到了一个干净的文本。\n下一步，将文本转换为tokens,像这样:\nfrom bs4 import BeautifulSoup import urllib.request response = urllib.request.urlopen('http://php.net/') html = response.read() soup = BeautifulSoup(html,\"html5lib\") text = soup.get_text(strip=True) tokens = text.split() print (tokens)\n1\n2\n3\n4\n5\n6\n7\n8\n9\n统计词频\ntext已经处理完毕了，现在使用Python NLTK统计token的频率分布。\n可以通过调用NLTK中的FreqDist()方法实现:\nfrom bs4 import BeautifulSoup import urllib.request import nltk response = urllib.request.urlopen('http://php.net/') html = response.read() soup = BeautifulSoup(html,\"html5lib\") text = soup.get_text(strip=True) tokens = text.split() freq = nltk.FreqDist(tokens) for key,val in freq.items(): print (str(key) + ':' + str(val))\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n如果搜索输出结果，可以发现最常见的token是PHP。\n您可以调用plot函数做出频率分布图:\nfreq.plot(20, cumulative=False) # 需要安装matplotlib库\n1\n2\n这上面这些单词。比如of,a,an等等，这些词都属于停用词。\n一般来说，停用词应该删除，防止它们影响分析结果。\n处理停用词\nNLTK自带了许多种语言的停用词列表，如果你获取英文停用词:\nfrom nltk.corpus import stopwords stopwords.words('english')\n1\n2\n3\n现在，修改下代码,在绘图之前清除一些无效的token:\nclean_tokens = list() sr = stopwords.words('english') for token in tokens: if token not in sr: clean_tokens.append(token)\n1\n2\n3\n4\n5\n最终的代码应该是这样的: 下面代码应该是 if token not in sr:\nfrom bs4 import BeautifulSoup import urllib.request import nltk from nltk.corpus import stopwords response = urllib.request.urlopen('http://php.net/') html = response.read() soup = BeautifulSoup(html,\"html5lib\") text = soup.get_text(strip=True) tokens = text.split() clean_tokens = list() sr = stopwords.words('english') for token in tokens: if not token in sr: clean_tokens.append(token) freq = nltk.FreqDist(clean_tokens) for key,val in freq.items(): print (str(key) + ':' + str(val))\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n现在再做一次词频统计图，效果会比之前好些，因为剔除了停用词:\nfreq.plot(20,cumulative=False)\n1\n使用NLTK Tokenize文本\n在之前我们用split方法将文本分割成tokens，现在我们使用NLTK来Tokenize文本。\n文本没有Tokenize之前是无法处理的，所以对文本进行Tokenize非常重要的。token化过程意味着将大的部件分割为小部件。\n你可以将段落tokenize成句子，将句子tokenize成单个词，NLTK分别提供了句子tokenizer和单词tokenizer。\n假如有这样这段文本:\nHello Adam, how are you? I hope everything is going well. Today is a good day, see you dude\n1\n使用句子tokenizer将文本tokenize成句子:\nfrom nltk.tokenize import sent_tokenize mytext = \"Hello Adam, how are you? I hope everything is going well. Today is a good day, see you dude.\" print(sent_tokenize(mytext))\n1\n2\n3\n4\n输出如下:\n['Hello Adam, how are you?', 'I hope everything is going well.', 'Today is a good day, see you dude.']\n1\n这是你可能会想，这也太简单了，不需要使用NLTK的tokenizer都可以，直接使用正则表达式来拆分句子就行，因为每个句子都有标点和空格。\n那么再来看下面的文本:\nHello Mr. Adam, how are you? I hope everything is going well. Today is a good day, see you dude.\n1\n这样如果使用标点符号拆分,Hello Mr将会被认为是一个句子，如果使用NLTK:\nfrom nltk.tokenize import sent_tokenize mytext = \"Hello Mr. Adam, how are you? I hope everything is going well. Today is a good day, see you dude.\" print(sent_tokenize(mytext))\n1\n2\n3\n4\n输出如下:\n['Hello Mr. Adam, how are you?', 'I hope everything is going well.', 'Today is a good day, see you dude.']\n1\n这才是正确的拆分。\n接下来试试单词tokenizer:\nfrom nltk.tokenize import word_tokenize mytext = \"Hello Mr. Adam, how are you? I hope everything is going well. Today is a good day, see you dude.\" print(word_tokenize(mytext))\n1\n2\n3\n4\n输出如下:\n['Hello', 'Mr.', 'Adam', ',', 'how', 'are', 'you', '?', 'I', 'hope', 'everything', 'is', 'going', 'well', '.', 'Today', 'is', 'a', 'good', 'day', ',', 'see', 'you', 'dude', '.']\n1\nMr.这个词也没有被分开。NLTK使用的是punkt模块的PunktSentenceTokenizer，它是NLTK.tokenize的一部分。而且这个tokenizer经过训练，可以适用于多种语言。\n非英文Tokenize\nTokenize时可以指定语言:\nfrom nltk.tokenize import sent_tokenize mytext = \"Bonjour M. Adam, comment allez-vous? J'espère que tout va bien. Aujourd'hui est un bon jour.\" print(sent_tokenize(mytext,\"french\"))\n1\n2\n3\n4\n输出结果如下:\n['Bonjour M. Adam, comment allez-vous?', \"J'espère que tout va bien.\", \"Aujourd'hui est un bon jour.\"]\n1\n同义词处理\n使用nltk.download()安装界面，其中一个包是WordNet。\nWordNet是一个为自然语言处理而建立的数据库。它包括一些同义词组和一些简短的定义。\n您可以这样获取某个给定单词的定义和示例:\nfrom nltk.corpus import wordnet syn = wordnet.synsets(\"pain\") print(syn[0].definition()) print(syn[0].examples())\n1\n2\n3\n4\n5\n输出结果是:\na symptom of some physical hurt or disorder ['the patient developed severe pain and distension']\n1\n2\nWordNet包含了很多定义：\nfrom nltk.corpus import wordnet syn = wordnet.synsets(\"NLP\") print(syn[0].definition()) syn = wordnet.synsets(\"Python\") print(syn[0].definition())\n1\n2\n3\n4\n5\n6\n结果如下:\nthe branch of information science that deals with natural language information large Old World boas\n1\n2\n3\n可以像这样使用WordNet来获取同义词:\nfrom nltk.corpus import wordnet synonyms = [] for syn in wordnet.synsets('Computer'): for lemma in syn.lemmas(): synonyms.append(lemma.name()) print(synonyms)\n1\n2\n3\n4\n5\n6\n7\n输出:\n['computer', 'computing_machine', 'computing_device', 'data_processor', 'electronic_computer', 'information_processing_system', 'calculator', 'reckoner', 'figurer', 'estimator', 'computer']\n1\n反义词处理\n也可以用同样的方法得到反义词：\nfrom nltk.corpus import wordnet antonyms = [] for syn in wordnet.synsets(\"small\"): for l in syn.lemmas(): if l.antonyms(): antonyms.append(l.antonyms()[0].name()) print(antonyms)\n1\n2\n3\n4\n5\n6\n7\n8\n9\n输出:\n['large', 'big', 'big']\n1\n词干提取\n语言形态学和信息检索里，词干提取是去除词缀得到词根的过程，例如working的词干为work。\n搜索引擎在索引页面时就会使用这种技术，所以很多人为相同的单词写出不同的版本。\n有很多种算法可以避免这种情况，最常见的是波特词干算法。NLTK有一个名为PorterStemmer的类，就是这个算法的实现:\nfrom nltk.stem import PorterStemmer stemmer = PorterStemmer() print(stemmer.stem('working')) print(stemmer.stem('worked'))\n1\n2\n3\n4\n5\n输出结果是:\nwork work\n1\n2\n还有其他的一些词干提取算法，比如 Lancaster词干算法。\n非英文词干提取\n除了英文之外，SnowballStemmer还支持13种语言。\n支持的语言:\nfrom nltk.stem import SnowballStemmer print(SnowballStemmer.languages) 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish'\n1\n2\n3\n4\n5\n你可以使用SnowballStemmer类的stem函数来提取像这样的非英文单词：\nfrom nltk.stem import SnowballStemmer french_stemmer = SnowballStemmer('french') print(french_stemmer.stem(\"French word\"))\n1\n2\n3\n4\n5\n单词变体还原\n单词变体还原类似于词干，但不同的是，变体还原的结果是一个真实的单词。不同于词干，当你试图提取某些词时，它会产生类似的词:\nfrom nltk.stem import PorterStemmer stemmer = PorterStemmer() print(stemmer.stem('increases'))\n1\n2\n3\n4\n5\n结果:\nincreas\n1\n现在，如果用NLTK的WordNet来对同一个单词进行变体还原，才是正确的结果:\nfrom nltk.stem import WordNetLemmatizer lemmatizer = WordNetLemmatizer() print(lemmatizer.lemmatize('increases'))\n1\n2\n3\n4\n5\n结果:\nincrease\n1\n结果可能会是一个同义词或同一个意思的不同单词。\n有时候将一个单词做变体还原时，总是得到相同的词。\n这是因为语言的默认部分是名词。要得到动词，可以这样指定：\nfrom nltk.stem import WordNetLemmatizer lemmatizer = WordNetLemmatizer() print(lemmatizer.lemmatize('playing', pos=\"v\"))\n1\n2\n3\n4\n5\n结果:\nplay\n1\n实际上，这也是一种很好的文本压缩方式，最终得到文本只有原先的50%到60%。\n结果还可以是动词(v)、名词(n)、形容词(a)或副词(r)：\nfrom nltk.stem import WordNetLemmatizer lemmatizer = WordNetLemmatizer() print(lemmatizer.lemmatize('playing', pos=\"v\")) print(lemmatizer.lemmatize('playing', pos=\"n\")) print(lemmatizer.lemmatize('playing', pos=\"a\")) print(lemmatizer.lemmatize('playing', pos=\"r\"))\n1\n2\n3\n4\n5\n6\n7\n8\n输出:\nplay playing playing playing\n1\n2\n3\n4\n5\n词干和变体的区别\n通过下面例子来观察:\nfrom nltk.stem import WordNetLemmatizer from nltk.stem import PorterStemmer stemmer = PorterStemmer() lemmatizer = WordNetLemmatizer() print(stemmer.stem('stones')) print(stemmer.stem('speaking')) print(stemmer.stem('bedroom')) print(stemmer.stem('jokes')) print(stemmer.stem('lisa')) print(stemmer.stem('purple')) print('----------------------') print(lemmatizer.lemmatize('stones')) print(lemmatizer.lemmatize('speaking')) print(lemmatizer.lemmatize('bedroom')) print(lemmatizer.lemmatize('jokes')) print(lemmatizer.lemmatize('lisa')) print(lemmatizer.lemmatize('purple'))\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n输出:\nstone speak bedroom joke lisa\n1\n2\n3\n4\n5\npurpl\nstone speaking bedroom joke lisa purple\n1\n2\n3\n4\n5\n6\n词干提取不会考虑语境，这也是为什么词干提取比变体还原快且准确度低的原因。\n个人认为，变体还原比词干提取更好。单词变体还原返回一个真实的单词，即使它不是同一个单词，也是同义词，但至少它是一个真实存在的单词。\n如果你只关心速度，不在意准确度，这时你可以选用词干提取。\n在此NLP教程中讨论的所有步骤都只是文本预处理。在以后的文章中，将会使用Python NLTK来实现文本分析。\n我已经尽量使文章通俗易懂。希望能对你有所帮助。\n\u003cscript\u003e (function(){ function setArticleH(btnReadmore,posi){ var winH = $(window).height(); var articleBox = $(\"div.article_content\"); var artH = articleBox.height(); if(artH \u003e winH*posi){ articleBox.css({ 'height':winH*posi+'px', 'overflow':'hidden' }) btnReadmore.click(function(){ if(typeof window.localStorage === \"object\" \u0026\u0026 typeof window.csdn.anonymousUserLimit === \"object\"){ if(!window.csdn.anonymousUserLimit.judgment()){ window.csdn.anonymousUserLimit.Jumplogin(); return false; }else if(!currentUserName){ window.csdn.anonymousUserLimit.updata(); } } articleBox.removeAttr(\"style\"); $(this).parent().remove(); }) }else{ btnReadmore.parent().remove(); } } var btnReadmore = $(\"#btn-readmore\"); if(btnReadmore.length\u003e0){ if(currentUserName){ setArticleH(btnReadmore,3); }else{ setArticleH(btnReadmore,1.2); } } })() \u003c/script\u003e \u003c/article\u003e\n@[TOC](这里写自定义目录标题)","data":"2018年11月29日 15:52:46"}
{"_id":{"$oid":"5d344ca262f717dc0659b64c"},"title":"《从零开始学习自然语言处理(NLP)》-DeepPavlov框架解析（4）","author":"l7H9JA4","content":"作者：刘才权\n\n编辑：田   旭\n\n\n\n\n\n前  言\n\n\n在这个日新月异的信息时代，海量数据的积累，计算能力的不断提升，机器学习尤其是深度学习的蓬勃发展，使得人工智能技术在不同领域焕发出蓬勃的活力。自己经历了嵌入式开发，移动互联网开发，目前从事自然语言处理算法开发工作。从工程软件开发到自然语言处理算法开发，希望通过这个系列的文章，能够由浅入深，通俗易懂的介绍自然语言处理的领域知识，分享自己的成长，同大家一起进步。\n\n\n\n\n01\n问题描述\n\n\n在上一篇《从零开始学习自然语言处理(NLP)-NLP Framework开源方案梳理(3)》中梳理了目前流行的NLP开源框架，这里重点介绍下DeepPavlov框架。DeepPavlov框架的模型实用性很强，对实际的生产开发有很大的借鉴意义。\n\n\n\n\n02\n框架组成\n\n\nDeepPavlov是一个基于TensorFlow和Keras的，专门针对对话系统研究和实验部署的自然语言处理框架。\n\n项目地址：http://docs.deeppavlov.ai/en/master/#\n\n框架主要包括：\n\n常用的NLP模型（包括Pre-train模型）\n如词向量训练、分类、命名实体识别（NER）、相似度计算等；\n\n针对对话系统实现和评测的实验框架（Framework）\n基于Json文件进行开发流程和数据流pipeline配置；\n\n提供同第三方应用进行集成的工具\n如与Amazon Alexa和Microsoft Bot Framework的集成；\n\n为对话模型的评测提供Benchmark环境\nDeepPavlov的默认Pre-train模型和测评数据集主要基于英文和俄文，对于中文场景需要做适当的调整。\n\n\n\n\n03\n框架使用对象\n\n\n\n新模型开发者\n方便同已有Benchmark模型进行对比评测\n\n普通NLP任务处理者\n如针对内容审核任务，敏感信息增加掩码等任务，可以直接使用框架提供的分类和序列标注模型，完成业务服务的快速开发和测评；\n\n对话系统开发者\nDeepPavlov是为对话系统场景量身定制的。对话系统开发者，可以直接参考使用；\n\n对话系统应用开发者\nDeepPavlov框架为应用集成提供了专门的工具，可以直接与Amazon Alexa，Microsoft Bot Framework等平台进行对接。\n\n\n\n\n04\n框架使用\n\n\n规范开发流程\n框架将数据模型服务的开发和验证流程（如数据清洗、模型设计、模型训练、模型选优、模型评测），使用Json配置文件串联成pipeline。能够很好的规范开发流程；\n\n新模型对比评测\n为对话模型的评测提供Benchmark环境，方便新模型的对比评测。但环境主要基于英文和俄文，对于中文任务，需要重新训练对比；\n\n常用NLP模型使用\n框架内置了很多常用模型（以分类为例，就包含了cnn_model，bilstm_model，bilstm_attention_model，transformer_model等12种模型实现），在项目中可以直接使用；\n同时框架提供了Pre-train模型，但主要是基于英文和俄文的，对于中文场景需要自己进行重新训练；\n\n对话系统开发\nDeepPavlov是为对话系统场景量身定制的，对话系统开发者，可以直接参考使用；\n\n\n\n\n05\nDeepPavlov框架层次\n\n\nDeepPavlov从整体到局部，可分为如下三个层次：\n\n\n\n\n下面我们从外层到内层，逐层介绍DeepPavlov的框架设计。\n\n\n\n\n\n\n06\nDeepPavlov顶层框架\n\n\nAgent\n同用户直接交互的代理，直接接收用户输入的纯文本信息（raw text）\n\n\n\nSkill\n领域技能，如基于意图-词槽的任务型技能，基于Seq2Seq的闲聊技能，基于知识图谱的知识问答技能；\n\n\n\nSkill Manager\n确定用户query，选择使用哪些skill，并确定将哪一个skill的召回结果作为最终的回复；\n\n\n\nComponent\nSkill实现的组成部分，如针对任务型技能，包括数据预处理component、意图识别component、slotfilling component等；\n\n\n\nChainer\nChainer以Json配置文件的形式，将某个skill相关的所有component串联起来；\n\n\n\nData Storage\n框架本身包含的Pre-train模型和Benchmark评测数据集\n\n\n\n\n07\nSkill框架\n\n\nDeepPavlov内置的skill主要包括：\n\n任务型skill（Goal-Oriented Dialogue Bot）\n基于意图/词槽/对话管理等component实现的问答skill。\n\n\n\n\n\n\n阅读理解skill（Open-Domain Question Answering）\n基于阅读理解实现问答skill。相对于阅读理解component（Context Question Answering），skill还包含在多个召回结果中进行排序的能力。\n\n\n\n规则型skill（Pattern Matching）\n基于自定义规则实现问答skill。\n\nSeq2Seq skill（Sequence-To-Sequence Dialogue Bot）\n基于Seq2Seq实现问答skill。\n\n\n\n\n常见问题问答skill（Frequently Asked Questions Answering）\n先将句子嵌入为向量（使用词向量叠加），然后做分类处理（给每一个answer一个打分），选取打分最高的answer作为最终回复的skill。\n\n\n\n商品查询skill（eCommerce Bot）\n商品查询回复skill，支持多轮（添加过滤条件）。下面是场景示例，\n\n\n\n\n\n\n08\n基本能力框架\n\n\nDeepPavlov内置的基本能力主要包括：\n\n数据预处理component（Data processors）\n主要提供包括分词、嵌入向量化等预处理能力（主要基于俄文和英文）。\n\n阅读理解component（Context Question Answering）\n相对于阅读理解skill（Open-Domain Question Answering），component不包含对多个召回结果进行排序（rank）的能力。具体的处理场景示例如下，\n\n\n\n\n分类component（Classification）\n分类组件，可以用来做场景和意图的分类。\n\n\nMorphological Tagger component\n一种特殊的POS？\n\n\n\n命名实体识别component（Named Entity Recognition）\nNER能力组件。\n\n\n\n相似度计算component（Neural Ranking）\n通过基于孪生网络完成相似度计算，实现在标准问答库中标准答复的查找。\n\n\n\n词槽填充component（Slot filling）\n在NER的基础上，增加了词表限制。\n\n\n\n*拼写纠错component（Spelling Correction）\n提供了两种纠错方法：\n* levenshtein_corrector ：基于编辑距离\n* brillmoore：基于统计模型\n\n\nTF-IDF排序component（TF-IDF Ranking）\n基于TF-IDF的文档召回排序。\n\n\n\n流行度排序component（Popularity Ranking）\n将TF-IDF打分和流行度打分作为特征，通过逻辑回归计算流行度，最终实现排序。\n\n\n\n\n09\nJson配置文件解析\n\n\nDeepPavlov通过Json配置文件实现开发流程控制和数据流pipeline的控制。\n\n上面提到DeepPavlov主要分为Agent、Skill和Component三个层次。而Json配置文件主要应用在Skill和Component这两个层面。而对Agent的控制，框架通过直接的代码来实现，例如，\n\n\n\n\n其中，\n\nskills\nAgent支持的所有skill列表；\n\nskills_filter\n针对用户query，确定Agent使用哪些skill；\n\nskills_filter\n通过召回的不同skill结果，确定最后的回复内容给用户；\n下面我们具体的介绍下Json配置文件的具体结构。\n\n\n\n\n09\nJson配置文件结构\n\n\n\n\n如上图所示，Json配置文件主要由如下五个部分组成（参考分类component）：\n\ndataset_reader\n主要负责数据的读取。\n\n\n\n\ndataset_iterator\n数据迭代器，从dataset_reader中获得数据，然后按batch抽取数据，供后面的模型训练使用。\n\n\n\n\nchainer\n配置文件的核心，将数据预处理、模型选优和模型预测输出，通过pipeline（\"pipe\"字段内进行约束）的形式串联起来。\n\n\n\"chainer\": {\n\"in\": [\n\"x\"\n],\n\"in_y\": [\n\"y\"\n],\n\"pipe\": [\n{\n\"id\": \"classes_vocab\",\n\"class_name\": \"simple_vocab\",\n\"fit_on\": [\n\"y\"\n],\n\"save_path\": \"{MODELS_PATH}/classifiers/{PROJECT_NAME}_{MODEL_NAME}/classes.dict\",\n\"load_path\": \"{MODELS_PATH}/classifiers/{PROJECT_NAME}_{MODEL_NAME}/classes.dict\",\n\"in\": \"y\",\n\"out\": \"y_ids\"\n},\n{\n\"in\": \"x\",\n\"out\": \"x_tok\",\n\"id\": \"my_tokenizer\",\n\"class_name\": \"char_tokenizer\",\n\"tokenizer\": \"char_tokenizer\"\n},\n{\n\"in\": \"x_tok\",\n\"out\": \"x_ids\",\n\"id\": \"seq_to_emb_ids\",\n\"class_name\": \"seq_to_emb_ids\",\n\"emb_file_path\":\"{EMBED_PATH}\",\n\"text_size\":250\n},\n{\n\"in\": \"y_ids\",\n\"out\": \"y_onehot\",\n\"class_name\": \"one_hotter\",\n\"id\": \"my_one_hotter\",\n\"single_vector\": true,\n\"depth\": \"#classes_vocab.len\"\n},\n{\n\"in\": [\n\"x_ids\"\n],\n\"fit_on_batch_preprocess\": [\n\"x_ids\",\n\"y_onehot\"\n],\n\"out\": [\n\"y_pred_probas\"\n],\n\"main\": true,\n\"class_name\": \"keras_classifier_model\",\n\"graph_metrics\": \"multilabel_f1\",\n\"save_path\": \"{MODELS_PATH}/classifiers/{PROJECT_NAME}_{MODEL_NAME}/model\",\n\"load_path\": \"{MODELS_PATH}/classifiers/{PROJECT_NAME}_{MODEL_NAME}/model\",\n\"n_classes\": \"#classes_vocab.len\",\n\"train_emb\": false,\n\"kernel_sizes_cnn\": [\n1,\n2,\n3,\n4,\n5\n],\n\"filters_cnn\": 512,\n\"optimizer\": \"Adam\",\n\"learning_rate\": 0.001,\n\"learning_rate_decay\": 0.9,\n\"loss\": \"categorical_crossentropy\",\n\"embedding_matrix\": \"#seq_to_emb_ids.matrix\",\n\"text_size\": 250,\n\"last_layer_activation\": \"softmax\",\n\"coef_reg_cnn\": 0.0,\n\"coef_reg_den\": 0.0,\n\"dropout_rate\": 0.5,\n\"dense_size\": 30,\n\"model_name\": \"cnn_model\"\n},\n{\n\"in\": \"y_pred_probas\",\n\"out\": \"y_pred_ids\",\n\"class_name\": \"proba2labels\",\n\"max_proba\": true\n},\n{\n\"in\": \"y_pred_ids\",\n\"out\": \"y_pred_onehot\",\n\"ref\": \"my_one_hotter\"\n},\n{\n\"in\": \"y_pred_ids\",\n\"out\": \"y_pred_labels\",\n\"ref\": \"classes_vocab\"\n}\n],\n\"out\": [\n\"y_pred_ids\",\n\"y_pred_onehot\",\n\"y_pred_labels\",\n\"y_pred_probas\"\n]\n},\n\n\n官方文档中，将“pipe”字段内的每一对花括号({})中的内容成为一个component（注意这里的component和上面提到的框架component是不同的。为了方便区分，我们将\"pipe\"中的component标识为pipe-component）。\n\ntrain\n模型训练、模型选优和评测配置。\n\n\n\"train\": {\n\"epochs\": 10,\n\"batch_size\": 256,\n\"metrics\": [\n{\n\"name\": \"cal_confusion_matrix\",\n\"inputs\": [\n\"y\",\n\"y_pred_labels\"\n]\n},\n{\n\"name\": \"f1_micro\",\n\"inputs\": [\n\"y\",\n\"y_pred_labels\"\n]\n},\n{\n\"name\": \"recall_micro\",\n\"inputs\": [\n\"y_ids\",\n\"y_pred_ids\"\n]\n},\n{\n\"name\": \"precision_micro\",\n\"inputs\": [\n\"y_onehot\",\n\"y_pred_onehot\"\n]\n},\n{\n\"name\": \"f1_macro\",\n\"inputs\": [\n\"y_onehot\",\n\"y_pred_onehot\"\n]\n},\n{\n\"name\": \"precision_macro\",\n\"inputs\": [\n\"y\",\n\"y_pred_labels\"\n]\n},\n{\n\"name\": \"recall_macro\",\n\"inputs\": [\n\"y_ids\",\n\"y_pred_ids\"\n]\n},\n{\n\"name\": \"recall_group\",\n\"inputs\": [\n\"y_onehot\",\n\"y_pred_onehot\"\n]\n},\n{\n\"name\": \"precision_group\",\n\"inputs\": [\n\"y\",\n\"y_pred_labels\"\n]\n},\n{\n\"name\": \"f1_group\",\n\"inputs\": [\n\"y_ids\",\n\"y_pred_ids\"\n]\n}\n],\n\"validation_patience\": 5,\n\"val_every_n_epochs\": 1,\n\"log_every_n_epochs\": 1,\n\"show_examples\": true,\n\"validate_best\": true,\n\"test_best\": true,\n\"report_path\": \"{MODELS_PATH}/classifiers/{PROJECT_NAME}_{MODEL_NAME}/report.xlsx\"\n},\n\n\n其中，\n\"metric\"字段中排在最前面的指标，作为模型选优的标准。\n\n\nmetadata\n相关相关的常量配置。\n\n\n\n\n其中，\"imports\"是DeepPavlov框架之外自定义实现。\n\n\n\n\n10\nDeepPavlov存在的问题\n\n\n环境依赖\nDeepPavlov是基于TensorFlow和Keras实现的，不能继承其他计算框架的模型实现（如PyTorch）。\n\n\n\n语言支持\nPre-train模型和评测数据集主要基于英文和俄文，不支持中文。\n\n\n\n生产环境部署\nDeepPavlov在运行时需要依赖整个框架源码，开发环境对框架修改后，生产环境需要更新整个框架。同时，也不能直接将功能Component作为服务独立导出，不适合在生产环境的部署和发布。\n\n\n\n\n\n\nEND\n\n\n\n\n\n\n往期回顾之作者刘才权\n【1】《机器学习》笔记-神经网络（5）\n【2】《从零开始学习自然语言处理(NLP)》 -基础准备(0)\n\n【3】《机器学习》笔记-降维与度量学习（10）\n【4】《机器学习》笔记-聚类（9）\n【5】《机器学习》笔记-集成学习（8）\n【6】《机器学习》笔记-贝叶斯分类器（7）\n\n\n\n\n\n\n\n\n\n\n机器学习算法工程师\n一个用心的公众号\n长按，识别，加关注\n\n进群，学习，得帮助\n你的关注，我们的热度，\n我们一定给你学习最大的帮助\n\n\n\n\n你点的每个赞，我都认真当成了喜欢","data":"2019年03月05日 18:10:57"}
{"_id":{"$oid":"5d344cdb62f717dc0659b65b"},"title":"人工智能|机器学习|NLP 算法分类总结","author":"weixin_34096182","content":"一、人工智能学习算法分类\n人工智能算法大体上来说可以分类两类：基于统计的机器学习算法(Machine Learning)和深度学习算法(Deep Learning)\n总的来说，在sklearn中机器学习算法大概的分类如下：\n1. 纯算法类\n(1).回归算法\n(2).分类算法\n(3).聚类算法\n(4)降维算法\n(5)概率图模型算法\n(6)文本挖掘算法\n(7)优化算法\n(8)深度学习算法\n2.建模方面\n(1).模型优化\n(2).数据预处理\n二、详细算法\n1.分类算法\n(1).LR (Logistic Regression，逻辑回归又叫逻辑分类)\n(2).SVM (Support Vector Machine，支持向量机)\n(3).NB (Naive Bayes，朴素贝叶斯)\n(4).DT (Decision Tree，决策树)\n1).C4.5\n2).ID3\n3).CART\n(5).集成算法\n1).Bagging\n2).Random Forest (随机森林)\n3).GB(梯度提升,Gradient boosting)\n4).GBDT (Gradient Boosting Decision Tree)\n5).AdaBoost\n6).Xgboost\n(6).最大熵模型\n2.回归算法\n(1).LR (Linear Regression，线性回归)\n(2).SVR (支持向量机回归)\n(3). RR (Ridge Regression，岭回归)\n3.聚类算法\n(1).Knn\n(2).Kmeans 算法\n(3).层次聚类\n(4).密度聚类\n4.降维算法\n(1).SGD (随机梯度下降)\n(2).\n5.概率图模型算法\n(1).贝叶斯网络\n(2).HMM\n(3).CRF (条件随机场)\n6.文本挖掘算法\n(1).模型\n1).LDA (主题生成模型，Latent Dirichlet Allocation)\n4).最大熵模型\n(2).关键词提取\n1).tf-idf\n2).bm25\n3).textrank\n4).pagerank\n5).左右熵 :左右熵高的作为关键词\n6).互信息：\n(3).词法分析\n1).分词\n①HMM (因马尔科夫)\n②CRF (条件随机场)\n2).词性标注\n3).命名实体识别\n(4).句法分析\n1).句法结构分析\n2).依存句法分析\n(5).文本向量化\n1).tf-idf\n2).word2vec\n3).doc2vec\n4).cw2vec\n(6).距离计算\n1).欧氏距离\n2).相似度计算\n7.优化算法\n(1).正则化\n1).L1正则化\n2).L2正则化\n8.深度学习算法\n(1).BP\n(2).CNN\n(3).DNN\n(3).RNN\n(4).LSTM\n三、建模方面\n1.模型优化·\n(1).特征选择\n(2).梯度下降\n(3).交叉验证\n(4).参数调优\n(5).模型评估：准确率、召回率、F1、AUC、ROC、损失函数\n2.数据预处理\n(1).标准化\n(2).异常值处理\n(3).二值化\n(4).缺失值填充： 支持均值、中位数、特定值补差、多重插补","data":"2017年07月25日 17:20:00"}
{"_id":{"$oid":"5d344cf162f717dc0659b663"},"title":"人工智能和NLP的关键技术和应用领域","author":"perfectzq","content":"人工智能的概述\nAI 指代「人工智能」，是让机器能够像人类一样完成智能任务的技术。AI 使用智能完成自动化任务。\n\n人工智能包含两个关键点：\n1. 自动化\n2.智能\n人工智能的目标\n推理\n自动学习\u0026调度\n机器学习\n自然语言处理\n计算机视觉\n机器人\n通用智能\n\n\n\n\n人工智能三大阶段\n阶段 1——机器学习：智能系统使用一系列算法从经验中进行学习。\n阶段 2——机器智能：机器使用的一系列从经验中进行学习的高级算法，例如深度神经网络。\n阶段 3——机器意识：不需要外部数据就能从经验中自学习。\n目前处于第3阶段\n人工智能的类型\nANI（狭义人工智能）：它包含基础的、角色型任务，比如由 Siri、Alexa 这样的聊天机器人、个人助手完成的任务。\nAGI（通用人工智能）：通用人工智能包含人类水平的任务，它涉及到机器的持续学习。\nASI（强人工智能）：强人工智能指代比人类更聪明的机器。\n什么使得系统智能化？\n自然语言处理\n知识表示\n自动推理\n机器学习\n\n\n\n\nNLP、人工智能、机器学习、深度学习和神经网络之间的区别\n人工智能：建立能智能化处理事物的系统。\n自然语言处理：建立能够理解语言的系统，人工智能的一个分支。\n机器学习：建立能从经验中进行学习的系统，也是人工智能的一个分支。\n神经网络：生物学启发出的人工神经元网络。\n深度学习：在大型数据集上，建立使用深度神经网络的系统，机器学习的一个分支。\n\n\n\n\n下面是我关注的重点：\n\n\n自然语言处理的概念\n自然语言处理（NLP）是指机器理解并解释人类写作、说话方式的能力。NLP 的目标是让计算机／机器在理解语言上像人类一样智能。最终目标是弥补人类交流（自然语言）和计算机理解（机器语言）之间的差距。\n下面是三个不同等级的语言学分析：\n句法学：给定文本的哪部分是语法正确的。\n语义学：给定文本的含义是什么？\n语用学：文本的目的是什么？\nNLP 处理语言的不同方面\n音韵学：指代语言中发音的系统化组织。\n词态学：研究单词构成以及相互之间的关系。\nNLP 中理解语义分析的方法\n分布式：它利用机器学习和深度学习的大规模统计策略。\n框架式：句法不同，但语义相同的句子在数据结构（帧）中被表示为程式化情景。\n理论式：这种方法基于的思路是，句子指代的真正的词结合句子的部分内容可表达全部含义。\n交互式（学习）：它涉及到语用方法，在交互式学习环境中用户教计算机一步一步学习语言。\n我们为什么需要 NLP\n有了 NLP，有可能完成自动语音、自动文本编写这样的任务。由于大型数据（文本）的存在，我们为什么不使用计算机的能力，不知疲倦地运行算法来完成这样的任务，花费的时间也更少。这些任务包括 NLP 的其他应用，比如自动摘要（生成给定文本的总结）和机器翻译。\nNLP 流程\n如果要用语音产生文本，需要完成文本转语音任务\nNLP 的机制涉及两个流程：1. 自然语言理解\n2. 自然语言生成\n自然语言理解（NLU)\nNLU 是要理解给定文本的含义。文本内每个单词的特性与结构需要被理解。在理解结构上，NLU 要理解自然语言中的以下几个歧义性：\n词法歧义性：单词有多重含义\n句法歧义性：语句有多重解析树\n语义歧义性：句子有多重含义\n回指歧义性（Anaphoric Ambiguity）：之前提到的短语或单词在后面句子中有不同的含义。\n存在的问题：有些词有类似的含义（同义词），有些词有多重含义（多义词）。\n自然语言生成(NLG)\nNLG 是从结构化数据中以可读地方式自动生成文本的过程。自然语言生成的问题是难以处理。\n自然语言生成可被分为三个阶段：\n1. 文本规划：完成结构化数据中基础内容的规划。\n2. 语句规划：从结构化数据中组合语句，来表达信息流。\n3. 实现：产生语法通顺的语句来表达文本。\nNLP 与文本挖掘（或文本分析）之间的不同\n自然语言处理是理解给定文本的含义与结构的流程。\n文本挖掘或文本分析是通过模式识别提起文本数据中隐藏的信息的流程。\n自然语言处理被用来理解给定文本数据的含义（语义），而文本挖掘被用来理解给定文本数据的结构（句法）。\n\n\n\n大数据中的 NLP\n如今所有数据中的 80% 都可被用到，大数据来自于大公司、企业所存储的信息。例如，职员信息、公司采购、销售记录、经济业务以及公司、社交媒体的历史记录等。尽管人类使用的语言对计算机而言是模糊的、非结构化的，但有了 NLP 的帮助，我们可以解析这些大型的非结构化数据中的模式，从而更好地理解里面包含的信息。NLP 可使用大数据解决商业中的难题，比如零售、医疗、金融领域中的业务。下面主要谈谈聊天机器人。\n聊天机器人或自动智能代理\n指代你能通过聊天 app、聊天窗口或语音唤醒 app 进行交流的计算机程序。\n也有被用来解决客户问题的智能数字化助手，成本低、高效且持续工作。\n聊天机器人的重要性\n聊天机器人对理解数字化客服和频繁咨询的常规问答领域中的变化至关重要。\n聊天机器人在一些领域中的特定场景中非常有帮助，特别是会被频繁问到高度可预测的的问题时。\n聊天机器人的工作机制\n\n\n基于知识：包含信息库，根据客户的问题回应信息。\n数据存储：包含与用户交流的历史信息。\nNLP 层：它将用户的问题（任何形式）转译为信息，从而作为合适的回应。\n应用层：指用来与用户交互的应用接口。\nNLP 中为什么需要深度学习\n它使用基于规则的方法将单词表示为「one-hot」编码向量。\n传统的方法注重句法表征，而非语义表征。\n词袋：分类模型不能够分别特定语境。\n\n\n\n\n深度学习的三项能力\n可表达性：这一能力描述了机器如何能近似通用函数。\n可训练性：深度学习系统学习问题的速度与能力。\n可泛化性：在未训练过的数据上，机器做预测的能力。\n在深度学习中，当然也要考虑其他的能力，比如可解释性、模块性、可迁移性、延迟、对抗稳定性、安全等。但以上是主要的几项能力。\n\n\nNLP 中深度学习的常见任务\n\n\n传统 NLP 和深度学习 NLP 的区别\n\n\n日志分析与日志挖掘中的 NLP\n什么是日志？\n不同网络设备或硬件的时序信息集合表示日志。日志可直接存储在硬盘文档中，也可作为信息流传送到日志收集器。日志提供维持、追踪硬件表现、参数调整、紧急事件、系统修复、应用和架构优化的过程。\n什么是日志分析？\n日志分析是从日志中提取信息的过程，分析信息中的句法和语义，解析应用环境，从而比较分析不同源的日志文档，进行异常检测、发现关联性。\n什么是日志挖掘？\n日志挖掘或日志知识发现是提取日志中模式和关联性的过程，从而挖掘知识，预测日志中的异常检测。\n日志分析和日志挖掘中使用到的技术\n模式识别：将日志信息与模式薄中的信息进行对比，从而过滤信息的技术。\n标准化：日志信息的标准化是将不同的信息转换为同样的格式。当来自不同源的日志信息有不同的术语，但含义相同时，需要进行标准化。\n分类 \u0026 标签：不同日志信息的分类 \u0026 标签涉及到对信息的排序，并用不同的关键词进行标注。\nArtificial Ignorance：使用机器学习算法抛弃无用日志信息的技术。它也可被用来检测系统异常。\n日志分析 \u0026 日志挖掘中的 NLP\n自然语言处理技术被普遍用于日志分析和日志挖掘。词语切分、词干提取（stemming)、词形还原（lemmatization）、解析等不同技术被用来将日志信息转换成结构化的形式。一旦日志以很好的形式组织起来，日志分析和日志挖掘就能提取信息中有用的信息和知识。\nNLP的一个例子\n用户需要输入一个包含已写文本的文件；接着应该执行以下 NLP 步骤：\n\n\n\n\n\n语句分割 - 在给定文本中辨识语句边界，即一个语句的结束和另一个语句的开始。语句通常以标点符号「.」结束。\n标记化 - 辨识不同的词、数字及其他标点符号。\n词干提取 - 将一个词还原为词干。\n词性标注 - 标出语句中每一个词的词性，比如名词或副词。\n语法分析 - 将给定文本的部分按类划分。\n命名实体识别 - 找出给定文本中的人物、地点、时间等。\n指代消解 - 根据一个语句的前句和后句界定该句中给定词之间的关系。\nNLP 的其他关键应用领域\n除了在大数据、日志挖掘及分析中的应用，NLP 还有一些其他主要应用领域。\n自动摘要 - 在给定输入文本的情况下，摈弃次要信息完成文本摘要。\n情感分析 - 在给定文本中预测其主题，比如，文本中是否包含判断、观点或评论等。\n文本分类 - 按照其领域分类不同的期刊、新闻报道。多文档分类也是可能的。文本分类的一个流行示例是垃圾电子邮件检测。基于写作风格，可检测作者姓名。\n信息提取 - 建议电子邮件程序自动添加事件到日历。\n主要是翻译的这篇论文：Overview of Artificial Intelligence and Role of Natural Language Processing in Big Data\n\n\n参考文献：点击打开链接","data":"2017年05月08日 19:07:11"}
{"_id":{"$oid":"5d344d2a62f717dc0659b66d"},"title":"自然语言处理怎么最快入门","author":"赵志雄","content":"作者：微软亚洲研究院\n链接：https://www.zhihu.com/question/19895141/answer/149475410\n来源：知乎\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n\n\n自然语言处理（简称NLP），是研究计算机处理人类语言的一门技术，包括：\n1.句法语义分析：对于给定的句子，进行分词、词性标记、命名实体识别和链接、句法分析、语义角色识别和多义词消歧。\n2.信息抽取：从给定文本中抽取重要的信息，比如，时间、地点、人物、事件、原因、结果、数字、日期、货币、专有名词等等。通俗说来，就是要了解谁在什么时候、什么原因、对谁、做了什么事、有什么结果。涉及到实体识别、时间抽取、因果关系抽取等关键技术。\n3.文本挖掘（或者文本数据挖掘）：包括文本聚类、分类、信息抽取、摘要、情感分析以及对挖掘的信息和知识的可视化、交互式的表达界面。目前主流的技术都是基于统计机器学习的。\n4.机器翻译：把输入的源语言文本通过自动翻译获得另外一种语言的文本。根据输入媒介不同，可以细分为文本翻译、语音翻译、手语翻译、图形翻译等。机器翻译从最早的基于规则的方法到二十年前的基于统计的方法，再到今天的基于神经网络（编码-解码）的方法，逐渐形成了一套比较严谨的方法体系。\n5.信息检索：对大规模的文档进行索引。可简单对文档中的词汇，赋之以不同的权重来建立索引，也可利用1，2，3的技术来建立更加深层的索引。在查询的时候，对输入的查询表达式比如一个检索词或者一个句子进行分析，然后在索引里面查找匹配的候选文档，再根据一个排序机制把候选文档排序，最后输出排序得分最高的文档。\n6.问答系统： 对一个自然语言表达的问题，由问答系统给出一个精准的答案。需要对自然语言查询语句进行某种程度的语义分析，包括实体链接、关系识别，形成逻辑表达式，然后到知识库中查找可能的候选答案并通过一个排序机制找出最佳的答案。\n7.对话系统：系统通过一系列的对话，跟用户进行聊天、回答、完成某一项任务。涉及到用户意图理解、通用聊天引擎、问答引擎、对话管理等技术。此外，为了体现上下文相关，要具备多轮对话能力。同时，为了体现个性化，要开发用户画像以及基于用户画像的个性化回复。\n随着深度学习在图像识别、语音识别领域的大放异彩，人们对深度学习在NLP的价值也寄予厚望。再加上AlphaGo的成功，人工智能的研究和应用变得炙手可热。自然语言处理作为人工智能领域的认知智能，成为目前大家关注的焦点。很多研究生都在进入自然语言领域，寄望未来在人工智能方向大展身手。但是，大家常常遇到一些问题。俗话说，万事开头难。如果第一件事情成功了，学生就能建立信心，找到窍门，今后越做越好。否则，也可能就灰心丧气，甚至离开这个领域。这里针对给出我个人的建议，希望我的这些粗浅观点能够引起大家更深层次的讨论。\n建议1：如何在NLP领域快速学会第一个技能？\n我的建议是：找到一个开源项目，比如机器翻译或者深度学习的项目。理解开源项目的任务，编译通过该项目发布的示范程序，得到与项目示范程序一致的结果。然后再深入理解开源项目示范程序的算法。自己编程实现一下这个示范程序的算法。再按照项目提供的标准测试集测试自己实现的程序。如果输出的结果与项目中出现的结果不一致，就要仔细查验自己的程序，反复修改，直到结果与示范程序基本一致。如果还是不行，就大胆给项目的作者写信请教。在此基础上，再看看自己能否进一步完善算法或者实现，取得比示范程序更好的结果。\n建议2：如何选择第一个好题目？\n工程型研究生，选题很多都是老师给定的。需要采取比较实用的方法，扎扎实实地动手实现。可能不需要多少理论创新，但是需要较强的实现能力和综合创新能力。而学术型研究生需要取得一流的研究成果，因此选题需要有一定的创新。我这里给出如下的几点建议。\n先找到自己喜欢的研究领域。你找到一本最近的ACL会议论文集, 从中找到一个你比较喜欢的领域。在选题的时候，多注意选择蓝海的领域。这是因为蓝海的领域，相对比较新，容易出成果。\n充分调研这个领域目前的发展状况。包括如下几个方面的调研：方法方面，是否有一套比较清晰的数学体系和机器学习体系；数据方面，有没有一个大家公认的标准训练集和测试集；研究团队，是否有著名团队和人士参加。如果以上几个方面的调研结论不是太清晰，作为初学者可能不要轻易进入。\n在确认进入一个领域之后，按照建议一所述，需要找到本领域的开源项目或者工具，仔细研究一遍现有的主要流派和方法，先入门。\n反复阅读本领域最新发表的文章，多阅读本领域牛人发表的文章。在深入了解已有工作的基础上，探讨还有没有一些地方可以推翻、改进、综合、迁移。注意做实验的时候，不要贪多，每次实验只需要验证一个想法。每次实验之后，必须要进行分析存在的错误，找出原因。\n对成功的实验，进一步探讨如何改进算法。注意实验数据必须是业界公认的数据。\n与已有的算法进行比较，体会能够得出比较一般性的结论。如果有，则去写一篇文章，否则，应该换一个新的选题。\n建议3：如何写出第一篇论文？\n接上一个问题，如果想法不错，且被实验所证明，就可开始写第一篇论文了。\n确定论文的题目。在定题目的时候，一般不要“…系统”、“…研究与实践”，要避免太长的题目，因为不好体现要点。题目要具体，有深度，突出算法。\n写论文摘要。要突出本文针对什么重要问题，提出了什么方法，跟已有工作相比，具有什么优势。实验结果表明，达到了什么水准，解决了什么问题。\n写引言。首先讲出本项工作的背景，这个问题的定义，它具有什么重要性。然后介绍对这个问题，现有的方法是什么，有什么优点。但是（注意但是）现有的方法仍然有很多缺陷或者挑战。比如（注意比如），有什么问题。本文针对这个问题，受什么方法（谁的工作）之启发，提出了什么新的方法并做了如下几个方面的研究。然后对每个方面分门别类加以叙述，最后说明实验的结论。再说本文有几条贡献，一般写三条足矣。然后说说文章的章节组织，以及本文的重点。有的时候东西太多，篇幅有限，只能介绍最重要的部分，不需要面面俱到。\n相关工作。对相关工作做一个梳理，按照流派划分，对主要的最多三个流派做一个简单介绍。介绍其原理，然后说明其局限性。\n然后可设立两个章节介绍自己的工作。第一个章节是算法描述。包括问题定义，数学符号，算法描述。文章的主要公式基本都在这里。有时候要给出简明的推导过程。如果借鉴了别人的理论和算法，要给出清晰的引文信息。在此基础上，由于一般是基于机器学习或者深度学习的方法，要介绍你的模型训练方法和解码方法。第二章就是实验环节。一般要给出实验的目的，要检验什么，实验的方法，数据从哪里来，多大规模。最好数据是用公开评测数据，便于别人重复你的工作。然后对每个实验给出所需的技术参数，并报告实验结果。同时为了与已有工作比较，需要引用已有工作的结果，必要的时候需要重现重要的工作并报告结果。用实验数据说话，说明你比人家的方法要好。要对实验结果好好分析你的工作与别人的工作的不同及各自利弊，并说明其原因。对于目前尚不太好的地方，要分析问题之所在，并将其列为未来的工作。\n结论。对本文的贡献再一次总结。既要从理论、方法上加以总结和提炼，也要说明在实验上的贡献和结论。所做的结论，要让读者感到信服，同时指出未来的研究方向。\n参考文献。给出所有重要相关工作的论文。记住，漏掉了一篇重要的参考文献（或者牛人的工作），基本上就没有被录取的希望了。\n写完第一稿，然后就是再改三遍。\n把文章交给同一个项目组的人士，请他们从算法新颖度、创新性和实验规模和结论方面，以挑剔的眼光，审核你的文章。自己针对薄弱环节，进一步改进，重点加强算法深度和工作创新性。\n然后请不同项目组的人士审阅。如果他们看不明白，说明文章的可读性不够。你需要修改篇章结构、进行文字润色，增加文章可读性。\n如投ACL等国际会议，最好再请英文专业或者母语人士提炼文字。","data":"2017年05月30日 06:30:36"}
{"_id":{"$oid":"5d344d4362f717dc0659b671"},"title":"【自然语言处理】自然语言处理（NLP）知识结构总结","author":"CS正阳","content":"自然语言处理知识太庞大了，网上也都是一些零零散散的知识，比如单独讲某些模型，也没有来龙去脉，学习起来较为困难，于是我自己总结了一份知识体系结构，不足之处，欢迎指正。内容来源主要参考黄志洪老师的自然语言处理课程。主要参考书为宗成庆老师的《统计自然语言处理》，虽然很多内容写的不清楚，但好像中文NLP书籍就这一本全一些，如果想看好的英文资料，可以到我的GitHub上下载：\nhttp://github.com/lovesoft5/ml\n下面直接开始正文：\n一、自然语言处理概述\n1）自然语言处理：利用计算机为工具，对书面实行或者口头形式进行各种各样的处理和加工的技术，是研究人与人交际中以及人与计算机交际中的演员问题的一门学科，是人工智能的主要内容。\n2）自然语言处理是研究语言能力和语言应用的模型，建立计算机（算法）框架来实现这样的语言模型，并完善、评测、最终用于设计各种实用系统。\n3）研究问题（主要）：\n信息检索\n机器翻译\n文档分类\n问答系统\n信息过滤\n自动文摘\n信息抽取\n文本挖掘\n舆情分析\n机器写作\n语音识别\n研究模式：\n自然语言场景问题，数学算法，算法如何应用到解决这些问题，预料训练，相关实际应用\n自然语言的困难：\n\n场景的困难：语言的多样性、多变性、歧义性\n学习的困难：艰难的数学模型（hmm,crf,EM,深度学习等）\n语料的困难：什么的语料？语料的作用？如何获取语料？\n二、形式语言与自动机\n语言：按照一定规律构成的句子或者字符串的有限或者无限的集合。\n描述语言的三种途径：\n穷举法\n文法（产生式系统）描述\n自动机\n自然语言不是人为设计而是自然进化的，形式语言比如：运算符号、化学分子式、编程语言\n形式语言理论朱啊哟研究的是内部结构模式这类语言的纯粹的语法领域，从语言学而来，作为一种理解自然语言的句法规律，在计算机科学中，形式语言通常作为定义编程和语法结构的基础\n形式语言与自动机基础知识：\n集合论\n图论\n自动机的应用：\n1，单词自动查错纠正\n2，词性消歧（什么是词性？什么的词性标注？为什么需要标注？如何标注？）\n形式语言的缺陷：\n1、对于像汉语，英语这样的大型自然语言系统，难以构造精确的文法\n2、不符合人类学习语言的习惯\n3、有些句子语法正确，但在语义上却不可能，形式语言无法排出这些句子\n4、解决方向：基于大量语料，采用统计学手段建立模型\n三、语言模型\n1）语言模型（重要）：通过语料计算某个句子出现的概率（概率表示），常用的有2-元模型，3-元模型\n2）语言模型应用：\n语音识别歧义消除例如，给定拼音串：ta shi yan yan jiu saun fa de\n可能的汉字串：踏实烟酒算法的   他是研究酸法的      他是研究算法的，显然，最后一句才符合。\n3）语言模型的启示：\n1、开启自然语言处理的统计方法\n2、统计方法的一般步骤：\n收集大量语料\n对语料进行统计分析，得出知识\n针对场景建立算法模型\n解释和应用结果\n4） 语言模型性能评价，包括评价目标，评价的难点，常用指标（交叉熵，困惑度）\n5）数据平滑：\n数据平滑的概念，为什么需要平滑\n平滑的方法，加一法，加法平滑法，古德-图灵法，J-M法，Katz平滑法等\n6）语言模型的缺陷：\n语料来自不同的领域，而语言模型对文本类型、主题等十分敏感\nn与相邻的n-1个词相关，假设不是很成立。\n\n四、概率图模型，生成模型与判别模型，贝叶斯网络，马尔科夫链与隐马尔科夫模型（HMM）\n1）概率图模型概述（什么的概率图模型，参考清华大学教材《概率图模型》）\n2）马尔科夫过程（定义，理解）\n3）隐马尔科夫过程（定义，理解）\nHMM的三个基本问题（定义，解法，应用）\n注：第一个问题，涉及最大似然估计法，第二个问题涉及EM算法，第三个问题涉及维特比算法，内容很多，要重点理解，（参考书李航《统计学习方法》，网上博客，笔者github）\n五、马尔科夫网，最大熵模型，条件随机场（CRF）\n1)HMM的三个基本问题的参数估计与计算\n2）什么是熵\n\n3）EM算法（应用十分广泛，好好理解）\n4）HMM的应用\n5）层次化马尔科夫模型与马尔科夫网络\n提出原因，HMM存在两个问题\n6）最大熵马尔科夫模型\n优点：与HMM相比，允许使用特征刻画观察序列，训练高效\n缺点： 存在标记偏置问题\n7）条件随机场及其应用(概念，模型过程，与HMM关系)\n参数估计方法（GIS算法，改进IIS算法）\nCRF基本问题：特征选取（特征模板）、概率计算、参数训练、解码（维特比）\n应用场景：\n词性标注类问题（现在一般用RNN+CRF）\n中文分词（发展过程，经典算法，了解开源工具jieba分词）\n中文人名，地名识别\n8）  CRF++\n六、命名实体 识别，词性标注，内容挖掘、语义分析与篇章分析（大量用到前面的算法）\n1）命名实体识别问题\n相关概率，定义\n相关任务类型\n方法（基于规程-\u003e基于大规模语料库）\n2）未登录词的解决方法(搜索引擎，基于语料)\n3）CRF解决命名实体识别（NER）流程总结：\n训练阶段：确定特征模板，不同场景（人名，地名等）所使用的特征模板不同，对现有语料进行分词，在分词结                      果基础上进行词性标注（可能手工），NER对应的标注问题是基于词的，然后训练CRF模型，得到对应权值参数值\n识别过程：将待识别文档分词，然后送入CRF模型进行识别计算（维特比算法），得到标注序列，然后根据标                            注划分出命名实体\n4）词性标注（理解含义，意义）及其一致性检查方法（位置属性向量，词性标注序列向量，聚类或者分类算法）\n\n七、句法分析\n1）句法分析理解以及意义\n1、句法结构分析\n完全句法分析\n浅层分析（这里有很多方法。。。）\n2、 依存关系分析\n2）句法分析方法\n1、基于规则的句法结构分析\n2、基于统计的语法结构分析\n八、文本分类，情感分析\n1）文本分类，文本排重\n文本分类：在预定义的分类体系下，根据文本的特征，将给定的文本与一个或者多个类别相关联\n典型应用：垃圾邮件判定，网页自动分类\n2）文本表示，特征选取与权重计算，词向量\n文本特征选择常用方法：\n1、基于本文频率的特征提取法\n2、信息增量法\n3、X2（卡方）统计量\n4、互信息法\n3）分类器设计\nSVM，贝叶斯，决策树等\n4）分类器性能评测\n1、召回率\n2、正确率\n3、F1值\n5）主题模型（LDA）与PLSA\nLDA模型十分强大，基于贝叶斯改进了PLSA，可以提取出本章的主题词和关键词，建模过程复杂，难以理解。\n6）情感分析\n借助计算机帮助用户快速获取，整理和分析相关评论信息，对带有感情色彩的主观文本进行分析，处理和归纳例如，评论自动分析，水军识别。\n某种意义上看，情感分析也是一种特殊的分类问题\n\n7）应用案例\n\n九、信息检索，搜索引擎及其原理\n1）信息检索起源于图书馆资料查询检索，引入计算机技术后，从单纯的文本查询扩展到包含图片，音视频等多媒体信息检索，检索对象由数据库扩展到互联网。\n1、点对点检索\n2、精确匹配模型与相关匹配模型\n3、检索系统关键技术：标引，相关度计算\n2）常见模型：布尔模型，向量空间模型，概率模型\n3）常用技术：倒排索引，隐语义分析（LDA等）\n4）评测指标\n十、自动文摘与信息抽取，机器翻译，问答系统\n1）统计机器翻译的的思路，过程，难点，以及解决\n2）问答系统\n基本组成：问题分析，信息检索，答案抽取\n类型：基于问题-答案， 基于自由文本\n典型的解决思路\n3）自动文摘的意义，常用方法\n4）信息抽取模型（LDA等）\n\n十一、深度学习在自然语言中的应用\n1）单词表示，比如词向量的训练（wordvoc）\n2）自动写文本\n写新闻等\n3）机器翻译\n4）基于CNN、RNN的文本分类\n5）深度学习与CRF结合用于词性标注\n...............\n更多深度学习内容，可参考我之前的文章。","data":"2019年03月27日 17:38:49","date":"2019年03月27日 17:38:49"}
{"_id":{"$oid":"5d344d5762f717dc0659b677"},"title":"自然语言处理nlp全领域综述","author":"未济2019","content":"*************************\n精华总结，时间不够只看这个部分就行了\n1.书和课\nMichael Collins：COMS W4705: Natural Language Processing (Spring 2015)（重要）\nJason Eisner的Lecture Notes：600.465 - Natural Language Processing\ndan jurasfsky：Speech and Language Processing (3rd ed. draft)  https://web.stanford.edu/~jurafsky/slp3/ 中文名译为《自然语言处理综论》(重要)\nWardhaugh, Ronald: An introduction to sociolinguistics\n宗成庆: 《统计自然语言处理》\nYoav Goldberg: Neural Network Methods for Natural Language Processing\n《数学之美》--吴军，科普且生动形象，入门必备；\n《统计学习方法》--李航\nStanford nlp公开课-cs224n\nJacob Eisenstein:Natural Language Processing\n《中文信息处理报告2016》\nChris Manning和Dan jurafsky两尊大神的至尊课程：introduction to natural language processing\nzhengxiang zhai：Text Mining and Analytics https://zh.coursera.org/learn/text-mining\n2. 代码\n哈工大开源的那个工具包 LTP (Language Technology Platform)\npattern - simpler to get started than NLTK\nchardet - character encoding detection\npyenchant - easy access to dictionaries\nscikit-learn - has support for text classification\nunidecode - because ascii is much easier to deal with\nWord2Vec\nCRF++\nGluonNLP\nTensorflow.Tensor2Tensor\ngensim\nPyTorch\nFasttext\nMMXnet\nnltk\nTextBlob\nspaCy\nGensim\njieba\nkeras: sequence to sequence learning\n3.学术会议\n***ACL年会\n**EMNLP（Conference on Empirical Methods on Natural Language Processing）\n**CoNLL（Conference on Natural Language Learning）\n**COLING (International Conference on Computational Linguistics)\n**ACL Anthology ACL学会网站建立了称作ACL Anthology的页面（URL：ACL Anthology），支持该领域绝大部分国际学术会议论文的免费下载\n4.期刊\n***Computational Linguistics\n**Transactions of ACL\n**ACM Transactions on Speech and Language Processing\n**ACM Transactions on Asian Language Information Processing\n**Journal of Quantitative Linguistics\nACL、EMNLP、NAACL、COLING、LREC、Computational Linguistics位于前5位。\n当需要了解某个领域，如果能找到一篇该领域的最新研究综述。最方便的方法还是在Google Scholar中搜索“领域名称 + survey / review / tutorial / 综述”来查找\n6.网站\nKaggle 情感分析题\n*************************\n所有资料都来自知乎，序号加了方括号重点关注。\n1.https://www.zhihu.com/question/19895141/answer/149475410\n自然语言处理（简称NLP），是研究计算机处理人类语言的一门技术，包括：\n1.句法语义分析：对于给定的句子，进行分词、词性标记、命名实体识别和链接、句法分析、语义角色识别和多义词消歧。\n2.信息抽取：从给定文本中抽取重要的信息，比如，时间、地点、人物、事件、原因、结果、数字、日期、货币、专有名词等等。通俗说来，就是要了解谁在什么时候、什么原因、对谁、做了什么事、有什么结果。涉及到实体识别、时间抽取、因果关系抽取等关键技术。\n3.文本挖掘（或者文本数据挖掘）：包括文本聚类、分类、信息抽取、摘要、情感分析以及对挖掘的信息和知识的可视化、交互式的表达界面。目前主流的技术都是基于统计机器学习的。\n4.机器翻译：把输入的源语言文本通过自动翻译获得另外一种语言的文本。根据输入媒介不同，可以细分为文本翻译、语音翻译、手语翻译、图形翻译等。机器翻译从最早的基于规则的方法到二十年前的基于统计的方法，再到今天的基于神经网络（编码-解码）的方法，逐渐形成了一套比较严谨的方法体系。\n5.信息检索：对大规模的文档进行索引。可简单对文档中的词汇，赋之以不同的权重来建立索引，也可利用1，2，3的技术来建立更加深层的索引。在查询的时候，对输入的查询表达式比如一个检索词或者一个句子进行分析，然后在索引里面查找匹配的候选文档，再根据一个排序机制把候选文档排序，最后输出排序得分最高的文档。\n6.问答系统： 对一个自然语言表达的问题，由问答系统给出一个精准的答案。需要对自然语言查询语句进行某种程度的语义分析，包括实体链接、关系识别，形成逻辑表达式，然后到知识库中查找可能的候选答案并通过一个排序机制找出最佳的答案。\n7.对话系统：系统通过一系列的对话，跟用户进行聊天、回答、完成某一项任务。涉及到用户意图理解、通用聊天引擎、问答引擎、对话管理等技术。此外，为了体现上下文相关，要具备多轮对话能力。同时，为了体现个性化，要开发用户画像以及基于用户画像的个性化回复。随着深度学习在图像识别、语音识别领域的大放异彩，人们对深度学习在NLP的价值也寄予厚望。再加上AlphaGo的成功，人工智能的研究和应用变得炙手可热。自然语言处理作为人工智能领域的认知智能，成为目前大家关注的焦点。很多研究生都在进入自然语言领域，寄望未来在人工智能方向大展身手。但是，大家常常遇到一些问题。俗话说，万事开头难。如果第一件事情成功了，学生就能建立信心，找到窍门，今后越做越好。否则，也可能就灰心丧气，甚至离开这个领域。这里针对给出我个人的建议，希望我的这些粗浅观点能够引起大家更深层次的讨论。\n建议1：如何在NLP领域快速学会第一个技能？我的建议是：找到一个开源项目，比如机器翻译或者深度学习的项目。理解开源项目的任务，编译通过该项目发布的示范程序，得到与项目示范程序一致的结果。然后再深入理解开源项目示范程序的算法。自己编程实现一下这个示范程序的算法。再按照项目提供的标准测试集测试自己实现的程序。如果输出的结果与项目中出现的结果不一致，就要仔细查验自己的程序，反复修改，直到结果与示范程序基本一致。如果还是不行，就大胆给项目的作者写信请教。在此基础上，再看看自己能否进一步完善算法或者实现，取得比示范程序更好的结果。\n\n--\n\n1. 国际学术组织、学术会议与学术论文自然语言处理（natural language processing，NLP）在很大程度上与计算语言学（computational linguistics，CL）重合。与其他计算机学科类似，NLP/CL有一个属于自己的最权威的国际专业学会，叫做The Association for Computational Linguistics（ACL，URL：ACL Home Page），这个协会主办了NLP/CL领域最权威的国际会议，即ACL年会，\nACL学会还会在北美和欧洲召开分年会，分别称为NAACL和EACL。除此之外，ACL学会下设多个特殊兴趣小组（special interest groups，SIGs），聚集了NLP/CL不同子领域的学者，性质类似一个大学校园的兴趣社团。其中比较有名的诸如SIGDAT（Linguistic data and corpus-based approaches to NLP）、SIGNLL（Natural Language Learning）等。这些SIGs也会召开一些国际学术会议，\n其中比较有名的就是SIGDAT组织的EMNLP（Conference on Empirical Methods on Natural Language Processing）和\nSIGNLL组织的CoNLL（Conference on Natural Language Learning）。此外还有一个International Committee on Computational Linguistics的老牌NLP/CL学术组织，它每两年组织一个称为International Conference on Computational Linguistics (COLING)的国际会议，也是NLP/CL的重要学术会议。NLP/CL的主要学术论文就分布在这些会议上。作为NLP/CL领域的学者最大的幸福在于，ACL学会网站建立了称作ACL Anthology的页面（URL：ACL Anthology），支持该领域绝大部分国际学术会议论文的免费下载，甚至包含了其他组织主办的学术会议，例如COLING、IJCNLP等，并支持基于Google的全文检索功能，可谓一站在手，NLP论文我有。由于这个论文集合非常庞大，并且可以开放获取，很多学者也基于它开展研究，提供了更丰富的检索支持，具体入口可以参考ACL Anthology页面上方搜索框右侧的不同检索按钮。与大部分计算机学科类似，由于技术发展迅速，NLP/CL领域更重视发表学术会议论文，原因是发表周期短，并可以通过会议进行交流。\n当然NLP/CL也有自己的旗舰学术期刊，发表过很多经典学术论文，那就是Computational Linguistics（URL：MIT Press Journals）。该期刊每期只有几篇文章，平均质量高于会议论文，时间允许的话值得及时追踪。\n此外，ACL学会为了提高学术影响力，也刚刚创办了Transactions of ACL（TACL，URL：Transactions of the Association for Computational Linguistics (ISSN: 2307-387X)），值得关注。值得一提的是这两份期刊也都是开放获取的。\n此外也有一些与NLP/CL有关的期刊，如ACM Transactions on Speech and Language Processing，ACM Transactions on Asian Language Information Processing，Journal of Quantitative Linguistics等等。\n根据Google Scholar Metrics 2013年对NLP/CL学术期刊和会议的评价，ACL、EMNLP、NAACL、COLING、LREC、Computational Linguistics位于前5位，基本反映了本领域学者的关注程度。\nNLP/CL作为交叉学科，其相关领域也值得关注。主要包括以下几个方面：（1）信息检索和数据挖掘领域。相关学术会议主要由美国计算机学会（ACM）主办，包括SIGIR、WWW、WSDM等；（2）人工智能领域。相关学术会议主要包括AAAI和IJCAI等，相关学术期刊主要包括Artificial Intelligence和Journal of AI Research；（3）机器学习领域，相关学术会议主要包括ICML，NIPS，AISTATS，UAI等，相关学术期刊主要包括Journal of Machine Learning Research（JMLR）和Machine Learning（ML）等。例如最近兴起的knowledge graph研究论文，就有相当一部分发表在人工智能和信息检索领域的会议和期刊上。实际上国内计算机学会（CCF）制定了“中国计算机学会推荐国际学术会议和期刊目录”（CCF推荐排名），通过这个列表，可以迅速了解每个领域的主要期刊与学术会议。最后，值得一提的是，美国Hal Daumé III维护了一个natural language processing的博客（natural language processing blog），经常评论最新学术动态，值得关注。我经常看他关于ACL、NAACL等学术会议的参会感想和对论文的点评，很有启发。另外，ACL学会维护了一个Wiki页面（ACL Wiki），包含了大量NLP/CL的相关信息，如著名研究机构、历届会议录用率，等等，都是居家必备之良品，值得深挖。\n2. 国内学术组织、学术会议与学术论文与国际上相似，国内也有一个与NLP/CL相关的学会，叫做中国中文信息学会（URL：中国中文信息学会）。通过学会的理事名单（中国中文信息学会）基本可以了解国内从事NLP/CL的主要单位和学者。学会每年组织很多学术会议，例如全国计算语言学学术会议（CCL）、全国青年计算语言学研讨会（YCCL）、全国信息检索学术会议（CCIR）、全国机器翻译研讨会（CWMT），等等，是国内NLP/CL学者进行学术交流的重要平台。尤其值得一提的是，全国青年计算语言学研讨会是专门面向国内NLP/CL研究生的学术会议，从组织到审稿都由该领域研究生担任，非常有特色，也是NLP/CL同学们学术交流、快速成长的好去处。值得一提的是，2010年在北京召开的COLING以及2015年即将在北京召开的ACL，学会都是主要承办者，这也一定程度上反映了学会在国内NLP/CL领域的重要地位。此外，计算机学会中文信息技术专委会组织的自然语言处理与中文计算会议（NLP\u0026CC）也是最近崛起的重要学术会议。中文信息学会主编了一份历史悠久的《中文信息学报》，是国内该领域的重要学术期刊，发表过很多篇重量级论文。此外，国内著名的《计算机学报》、《软件学报》等期刊上也经常有NLP/CL论文发表，值得关注。过去几年，在水木社区BBS上开设的AI、NLP版面曾经是国内NLP/CL领域在线交流讨论的重要平台。这几年随着社会媒体的发展，越来越多学者转战新浪微博，有浓厚的交流氛围。如何找到这些学者呢，一个简单的方法就是在新浪微博搜索的“找人”功能中检索“自然语言处理”、 “计算语言学”、“信息检索”、“机器学习”等字样，马上就能跟过去只在论文中看到名字的老师同学们近距离交流了。还有一种办法，清华大学梁斌开发的“微博寻人”系统（清华大学信息检索组）可以检索每个领域的有影响力人士，因此也可以用来寻找NLP/CL领域的重要学者。值得一提的是，很多在国外任教的老师和求学的同学也活跃在新浪微博上，例如王威廉（Sina Visitor System）、李沐（Sina Visitor System）等，经常爆料业内新闻，值得关注。还有，国内NLP/CL的著名博客是52nlp（我爱自然语言处理），影响力比较大。总之，学术研究既需要苦练内功，也需要与人交流。所谓言者无意、听者有心，也许其他人的一句话就能点醒你苦思良久的问题。无疑，博客微博等提供了很好的交流平台，当然也注意不要沉迷哦。3. 如何快速了解某个领域研究进展最后简单说一下快速了解某领域研究进展的经验。你会发现，搜索引擎是查阅文献的重要工具，尤其是谷歌提供的Google Scholar，由于其庞大的索引量，将是我们披荆斩棘的利器。当需要了解某个领域，如果能找到一篇该领域的最新研究综述，就省劲多了。最方便的方法还是在Google Scholar中搜索“领域名称 + survey / review / tutorial / 综述”来查找。也有一些出版社专门出版各领域的综述文章，例如NOW Publisher出版的Foundations and Trends系列，Morgan \u0026 Claypool Publisher出版的Synthesis Lectures on Human Language Technologies系列等。它们发表了很多热门方向的综述，如文档摘要、情感分析和意见挖掘、学习排序、语言模型等。如果方向太新还没有相关综述，一般还可以查找该方向发表的最新论文，阅读它们的“相关工作”章节，顺着列出的参考文献，就基本能够了解相关研究脉络了。当然，还有很多其他办法，例如去http://videolectures.net上看著名学者在各大学术会议或暑期学校上做的tutorial报告，去直接咨询这个领域的研究者，等等。\n--\n开始推荐工具包：中文的显然是哈工大开源的那个工具包 LTP (Language Technology Platform) developed by HIT-SCIR(哈尔滨工业大学社会计算与信息检索研究中心).英文的(python)：pattern - simpler to get started than NLTKchardet - character encoding detectionpyenchant - easy access to dictionariesscikit-learn - has support for text classificationunidecode - because ascii is much easier to deal with希望可以掌握以下的几个tool：CRF++GIZAWord2Vec\n\n还记得小时候看过的数码宝贝，每个萌萌哒的数码宝贝都会因为主人身上发生的一些事情而获得进化能力，其实在自然语言处理领域我觉得一切也是这样~ 我简单的按照自己的见解总结了每个阶段的特征，以及提高的解决方案1.幼年体——自然语言处理好屌，我什么都不会但是好想提高建议。。。去看公开课~去做Kaggle的那个情感分析题。2.成长期——觉得简单模型太Naive，高大上的才是最好的这个阶段需要自己动手实现一些高级算法，或者说常用算法，比如LDA，比如SVM，比如逻辑斯蒂回归。并且拥抱Kaggle，知道trick在这个领域的重要性。3.成熟期——高大上的都不work，通过特征工程加规则才work大部分人应该都在这个级别吧，包括我自己，我总是想进化，但积累还是不够。觉得高大上的模型都是一些人为了paper写的，真正的土方法才是重剑无锋，大巧不工。在这个阶段，应该就是不断读论文，不断看各种模型变种吧，什么句子相似度计算word2vec cosine已经不再适合你了。4.完全体——在公开数据集上，把某个高大上的模型做work了~这类应该只有少数博士可以做到吧，我已经不知道到了这个水平再怎么提高了~是不是只能说不忘初心，方得始终。5.究极体——参见Micheal Jordan Andrew Ng.\n希望可以理解自然语言处理的基本架构~：分词=\u003e词性标注=\u003eParserQuora上推荐的NLP的论文（摘自Quora 我过一阵会翻译括号里面的解释）：Parsing（句法结构分析~语言学知识多，会比较枯燥）\nKlein \u0026 Manning: \"Accurate Unlexicalized Parsing\" ( )\nKlein \u0026 Manning: \"Corpus-Based Induction of Syntactic Structure: Models of Dependency and Constituency\" (革命性的用非监督学习的方法做了parser)\nNivre \"Deterministic Dependency Parsing of English Text\" (shows that deterministic parsing actually works quite well)\nMcDonald et al. \"Non-Projective Dependency Parsing using Spanning-Tree Algorithms\" (the other main method of dependency parsing, MST parsing)\nMachine Translation（机器翻译，如果不做机器翻译就可以跳过了，不过翻译模型在其他领域也有应用）\nKnight \"A statistical MT tutorial workbook\" (easy to understand, use instead of the original Brown paper)\nOch \"The Alignment-Template Approach to Statistical Machine Translation\" (foundations of phrase based systems)\nWu \"Inversion Transduction Grammars and the Bilingual Parsing of Parallel Corpora\" (arguably the first realistic method for biparsing, which is used in many systems)\nChiang \"Hierarchical Phrase-Based Translation\" (significantly improves accuracy by allowing for gappy phrases)Language Modeling (语言模型)\nGoodman \"A bit of progress in language modeling\" (describes just about everything related to n-gram language models 这是一个survey，这个survey写了几乎所有和n-gram有关的东西，包括平滑 聚类)\nTeh \"A Bayesian interpretation of Interpolated Kneser-Ney\" (shows how to get state-of-the art accuracy in a Bayesian framework, opening the path for other applications)\nMachine Learning for NLPSutton \u0026 McCallum \"An introduction to conditional random fields for relational learning\" (CRF实在是在NLP中太好用了！！！！！而且我们大家都知道有很多现成的tool实现这个，而这个就是一个很简单的论文讲述CRF的，不过其实还是蛮数学= =。。。)\nKnight \"Bayesian Inference with Tears\" (explains the general idea of bayesian techniques quite well)\nBerg-Kirkpatrick et al. \"Painless Unsupervised Learning with Features\" (this is from this year and thus a bit of a gamble, but this has the potential to bring the power of discriminative methods to unsupervised learning\n)Information ExtractionHearst. Automatic Acquisition of Hyponyms from Large Text Corpora. COLING 1992. (The very first paper for all the bootstrapping methods for NLP. It is a hypothetical work in a sense that it doesn't give experimental results, but it influenced it's followers a lot.)Collins and Singer. Unsupervised Models for Named Entity Classification. EMNLP 1999. (It applies several variants of co-training like IE methods to NER task and gives the motivation why they did so. Students can learn the logic from this work for writing a good research paper in NLP.)Computational SemanticsGildea and Jurafsky. Automatic Labeling of Semantic Roles. Computational Linguistics 2002. (It opened up the trends in NLP for semantic role labeling, followed by several CoNLL shared tasks dedicated for SRL. It shows how linguistics and engineering can collaborate with each other. It has a shorter version in ACL 2000.)Pantel and Lin. Discovering Word Senses from Text. KDD 2002. (Supervised WSD has been explored a lot in the early 00's thanks to the senseval workshop, but a few system actually benefits from WSD because manually crafted sense mappings are hard to obtain. These days we see a lot of evidence that unsupervised clustering improves NLP tasks such as NER, parsing, SRL, etc,其实我相信，大家更感兴趣的是上层的一些应用~而不是如何实现分词，如何实现命名实体识别等等。而且应该大家更对信息检索感兴趣。不过自然语言处理和信息检索还是有所区别的，So~~~我就不在这边写啦\n\n2.https://www.zhihu.com/question/52164329/answer/129930771\n本科大三，学过机器学习算法。假设你学过的算法都熟练的话，你已经有了不错的基础了。那么问题分解为：1.如何入门NLP；2.如何开始做NLP的研究。这两个我分别回答，但是你可以同时行动。入门NLP。就像你自学机器学习一样，你最好系统的看一本书，或者上一门公开课，来系统的梳理一遍NLP的基本知识，了解NLP的基本问题。这里我推荐Michael Collins的公开课：COMS W4705: Natural Language Processing (Spring 2015)，以及Jason Eisner的Lecture Notes：600.465 - Natural Language Processing。如果学有余力的话，可以看一下参考书：https://web.stanford.edu/~jurafsky/slp3/。 时间有限的情况下，公开课和Notes就够了。系统学习知识的同时（或之后），你可以开始着手复现一些经典的项目。这个过程非常重要：1.你可以巩固自己的知识（确定你真的正确理解了）；2.你可以进一步提高自己的科研和工程能力；3.你很可能在实现的过场中发现问题，产生灵感，做出自己的工作（发一篇paper）。那么复现什么项目呢？如果你的导师没有给你指定的话，不妨从历年NLP顶会（ACL，EMNLP，NAACL）的获奖论文中筛选你感兴趣又有能力完成的。由于full paper的工程量通常较大，你可以先从short paper中进行选择。下面是最近的ACL，EMNLP和NAACL的录取论文列表：ACL | Association for Computational LinguisticsEMNLP 2016Accepted Papers同时，再附上一些Jason Eisner为帮助本科生做研究而写的一些建议：Advice for Research Students (and others)希望你能enjoy NLP！\n3.https://zhuanlan.zhihu.com/p/34524452\n从目前的发展情况来看，NLP更多的是统计学（shallow NLP），机器学习(deep NLP)，深度学习(deep NLP)的field。甚至有NLP方面的学者认为自己每开除一个语言学家，NLP的准确率就会提升一个百分点。\nNLP的几个核心问题。第一个为什么基于语法的分析是不可行的（目前）。第二个是NLP，从浅（shallow）到深（deep）的四个维度：lexical analysis，syntactic analysis，pragmatic analysis以及semantic analysis的问题。当然。这也就牵扯出了第三个问题：NLP的两大方法：基于统计学（处理shallow NLP问题，一般准确率比较高，目前也相对通用）和基于机器学习，深度学习的deep NLP问题。\n3.https://www.zhihu.com/question/35381685/answer/342705218\nReference:（大方向书籍，我要是能全部买下来就好了...并没有全部看完，有的只是看过某一章节。Grammar和syntax知乎里面有很多问答跟这方面有关，在此不重复了。）\nCruse, Alan. \"Meaning in language: An introduction to semantics and pragmatics.\" (2011).\nKarttunen, Lauri (1974) [1]. Theoretical Linguistics 1 181-94. Also in Pragmatics: A Reader, Steven Davis (ed.), pages 406-415, Oxford University Press, 1991.\nKadmon, Nirit. \"Formal pragmatics semantics, pragmatics, presupposition, and focus.\" (2001).\nLevinson, Stephen C. Pragmatics.Cambridge: Cambridge University Press, 1983, pp. 181-184.\nWardhaugh, Ronald. An introduction to sociolinguistics. John Wiley \u0026 Sons, 2010. (这本书的影响力很大，有很多跟social science的讨论)(具体其他上面提到的，每一篇我都仔细读过的)Grice's MaximsMonroe, Will, and Christopher Potts. \"Learning in the rational speech acts model.\" arXiv preprint arXiv:1510.06807 (2015).(这篇是关于rsa如何被用于具体task上的)Farkas, Richárd, et al. \"The CoNLL-2010 shared task: learning to detect hedges and their scope in natural language text.\" Proceedings of the Fourteenth Conference on Computational Natural Language Learning---Shared Task. Association for Computational Linguistics, 2010. (上文提到的hedge and cues shared task,关于linguistics里的现象是如何被formulate成nlp问题的)Morante, Roser, and Eduardo Blanco. \"* SEM 2012 shared task: Resolving the scope and focus of negation.\" Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation. Association for Computational Linguistics, 2012. (negation 的shared task)最后附上两篇老爷爷对我影响最大的：Zadeh, Lotfi Asker. \"The concept of a linguistic variable and its application to approximate reasoning—I.\" Information sciences 8.3 (1975): 199-249.Zadeh, Lotfi A. \"The concept of a linguistic variable and its application to approximate reasoning—II.\" Information sciences 8.4 (1975): 301-357.（ 这系列work分两部。）Zadeh, Lotfi A. \"Toward a theory of fuzzy information granulation and its centrality in human reasoning and fuzzy logic.\" Fuzzy sets and systems 90.2 (1997): 111-127.\n\n按照题主的描述，Dan jurasfsky的Speech and Language Processing 应该是最好的选择了。Manning的Foundations of Statistical Natural Language Processing感觉相对比较旧了，中文的话可以考虑宗成庆的《统计自然语言处理》。很多人对宗老师这本书有负面评价，我觉得倒还好。我们实验室里做NLP的几个人，有忘记一些知识的时候都会把他作为工具书来翻翻。可能的确不适合于入门和精读。最后想强烈安利Yoav Goldberg的这本：Neural Network Methods for Natural Language Processing\n\n我最偏爱的还是大神dan jurasfsky的Speech and Language Processing，中文名译为《自然语言处理综论》。大神讲什么都很清楚，一点就通，而且还很贝叶斯。逻辑斯特回归我是先看的吴恩达和林轩田的课，统计学派是从优化角度用拉格朗日乘数法引入正则化L2,L1。要理解L1为何会导致参数稀疏化，还得去看sub-gradient。jurafksy从贝叶斯的角度讲正则化，其实只是贝叶斯公式里的先验概率，你用高斯分布就是L2正则，拉普拉斯分布就是L1正则。 要是看过深度学习训练后的参数分布，你会发现更直观，训练出来的参数部分就是高斯分布的样子~该书第三版正在撰写中，作者已经完成了不少章节的撰写，所完成的章节均可下载：Speech and Language Processing\n4. https://zhuanlan.zhihu.com/p/35423943\n自然语言数据集\n\n[5].https://www.zhihu.com/question/53959076/answer/262419812\n看到实验室有同学关注了这个问题，还挺有趣的，让我来开一下脑洞。现在网站应该是用关键字匹配，或者正则表达式来过滤恶意弹幕。这种很容易被破解的，改成拼音，谐音，或者加几个标点就没办法了。如果要用上nlp手段，可以先人工标注恶意弹幕，再用深度学习的方法，比如用LSTM学习句子语义，最后给出它属于恶意弹幕的score，其实就是sentence classification，二分类问题。还可以做成个性化弹幕屏蔽，转化成n分类问题，对弹幕进行多分类，可以让用户来设置屏蔽哪种弹幕，或者根据用户的历史信息来自动设置。比如你是个单纯的孩子，那你可以设置屏蔽掉污污的弹幕。但是啊我觉得中文是博大精深的，内涵段子也是博大精深的，机器是很难读懂的(像我们这些单纯的小朋友也是读不懂的对不对)。现阶段做问答多是标注答案，做文本生成也是根据所给文本生成语义匹配的另一段文本，还有根据查询语句生成SQL命令，都是有套路有模板的，推理领域我不太了解，目前还有很大的发展空间(也就是做得还不够好)。就算机器训练再多次，看遍各种段子，遇到真正的老司机还是要翻车的，因为老司机的套路深不可测。自古深情留不住，总是套路得人心啊。\n中文博大精深，简单的用词库或者正则，是肯定不够的。而有监督的深度学习方法一般需要大量的标注语料，会标到让你怀疑人生。这里安利一下实验室一个师兄的工作：Reading the Videos: Temporal Labeling for Crowdsourced Time-Sync Videos based on Semantic Embedding。文章可以利用无监督方法获得弹幕文本的embedding。idea是假设经常在相近时间一起出现的弹幕有着相似的语义和向量空间，然后将问题转换成监督问题，让相近时间经常一起出现的弹幕在向量空间尽可能相近，让不同时间的弹幕在向量空间尽可能远离，这样可以得到包含弹幕语义信息的embedding向量。后面只需要提供少量你要屏蔽的弹幕屏蔽列表，然后计算目标弹幕和弹幕屏蔽列表中弹幕的余弦相似度，根据阈值过滤即可。\n\n6.https://zhuanlan.zhihu.com/p/33366448\n自然语言处理主要技术\n自然语言处理大概有五类技术，分别是：\n分类：文字的序列，我们要打印标签，这是我们常做的最基本的自然语言处理。\n匹配：两个文字序列都匹配，看它们匹配的程度，最后输出一个非负的实数值，判断这两个文字序列它们的匹配程度。\n翻译：把一个文字序列，转换成另外一个文字序列。\n结构预测：你给我一个文字序列，让它形成内部结构的一个信息。\n序列决策过程：在一个复杂的动态变化环境里面，我们怎么样不断去决策。比如描述序列决策过程的马尔可夫随机过程，这是一个有效的、非常常用的数学工具。\n我们看自然语言处理的大部分问题，基本上做得比较成功、实用的都是基于这样的技术做出来的。比如：分类，有文本分类、情感分析；匹配，有搜索、问答、单轮对话、基于检索的单轮对话；翻译，有机器翻译、语音识别、手写体识别、基于生成方法的单轮对话；结构预测，有专名识别、词性标注、语意分析；序列决策过程，有多轮对话。\n\n资料推荐--开始旅程！\n关于书籍：《数学之美》--吴军，科普且生动形象，入门必备；《统计学习方法》--李航，这个讲述基础机器学习算法，这是值得看的；《统计自然语言处理》--宗成庆，经典好书，可以详细看。\n关于综述：深度学习NLP，这个综述主要是深度学习在NLP的应用和发展，值得一看的；自然语言生成综述，讲述自然语言生层的各种方式和应用。。\n关于教程：Stanford nlp公开课-cs224n，需要中文笔记的可以看下博文，比如word2vec，斯坦福CS224N深度学习自然语言处理（一）---note等等，\n关于其他资料：Recent Advances and New Frontiers，对话的综述（因为我是做对话的哈哈，其他方向不了解了）\n多看论文，做实验，多看论文，做实验.....\n\n7.https://zhuanlan.zhihu.com/p/36708892\nGluonNLP — 自然语言处理的深度学习工具包\n这天他看到时下最热门的一篇谷歌论文 “Attention Is All You Need” 介绍基于注意力机制的 Transformer 模型。小A上网搜了搜发现， Tensorflow 的 Tensor2Tensor 包里已经有了这篇论文的实现。 身 (she) 经 (shi) 百 (wei) 战 (shen) 的小 A，于是决定立刻就拿这个包跑一下，想在当天下午重现一下这个最新的黑科技。\n自然语言处理的模型重现之所以难，与数据处理和模型搭建中需要解决的茫茫多技术点有很大关系：从各种语言的文本文件编码解码 （encoding/decoding)，读取，分词 （tokenization)，词向量转化 （embedding)，到输入给神经网络前的填充位 （padding)，截长 （clipping)，再到神经网络模型里处理变长输入数据和状态，一直到模型预测解码后的输出的 BLEU score 等等表现评估方法，每处都会有坑。如果工具不到位，每次做新模型开发都要经历各种大坑小坑的洗礼。\n最近做新项目，发现一个新趋势是好的资源不集中。大家都知道预训练的词向量和语言模型对很多应用有帮助，而哪个预训练模型更有用则是需要实验来验证的。在做这些验证时，开发者常常需要装许多不同的工具。比如 Google 的 Word2vec 需要装 gensim ，而 Salesforce 做的 AWD 语言模型是用 PyTorch 实现的，且暂不提供预训练模型， Facebook 的 Fasttext 则是自己开发的一个独立的包。为了能把这些资源凑齐在自己心爱的框架里使用，用户往往需要花费大量的精力在安装上。\n\n[8].https://zhuanlan.zhihu.com/p/33797826\n手把手教您解决90%的自然语言处理问题\nNLP是一个非常大的领域，NLP有几个最常使用的关键应用：\n识别不同的用户/客户群。\n准确的检测和提取不同类别的反馈。\n根据意图对文本进行分类。\n本文将讲解如何从头开始有效地处理这些问题的指南和技巧：首先解释如何构建机器学习解决方案来解决上面提到的问题。然后转向更细致的解决方案，比如特性工程、单词向量和深度学习。\n第一步：收集你的数据\n每个机器学习问题都始于数据。本文中，我们将使用一个名为“社交媒体上的灾难”的数据集：投稿人查看了超过一万条的推文，然后指出每条推文是否提到了灾难事件。\n我们的任务是检测哪些推文是关于灾难事件的，因为有潜在的应用专门收集紧急事件并通知执法部门。这个任务的特殊挑战是两个类都包含用于查找推文的相同搜索条件，所以我们不得不用更微妙的差异来区分它们。\n在本文中，我们将有关灾难的推文称为“灾难”，其他推文称为“无关紧要的”。正如Richard Socher所描述的那样，查找和标记足够的数据来训练模型比试图优化复杂的无监督方法通常更快、更简单、更便宜。\n第二步：清理你的数据\n“你的模型只能和你的数据一样好”。一个干净的数据集能够使模型学习有意义的特征，所以应当是先查看数据然后再清理数据。\n以下是用来清理你的数据的清单(详见代码):\n1、删除所有不相关的字符，例如任何非字母数字字符。\n2、把你的文章分成一个个单独的单词。\n3、删除不相关的单词。\n4、将所有字符转换为小写。\n5、考虑将拼错的单词或拼写单词组合成一个单独的表示。\n6、考虑词形化。\n在遵循这些步骤并检查额外的错误之后，我们可以开始使用干净的、标记的数据来训练模型！\n第三步：找到一个好的数据表示\n机器学习模型以数值作为输入。我们的数据集是一个句子的列表，所以为了能够提取数据。我们首先要找到一种方法使我们的算法能理解它：也就是数字列表。\n\n一组以数据矩阵表示的笑脸\n独热编码（Bag of Words）\n计算机文本表示的一种方法是将每个字符单独编码为一个数字（例如ASCII）。这对于大多数数据集来说是不可能的，所以我们需要更高层次的方法。\n例如，我们可以在我们的数据集中建立一个所有的单词的词汇表，并将一个唯一的索引与词汇表中的每个单词联系起来。每个句子被表示为一个列表，只要我们的词汇表中有不同单词的数量。在这个列表中的每个索引中，我们标记出在我们的句子中出现了多少次给定的单词。这被称为Bag of Words模型，因为它是一种完全无视我们句子中词语顺序的表现形式。\n\n可视化嵌入\n为了查看嵌入是否捕获了与我们的问题相关的信息（例如，tweet是否与灾难有关），我们选择可视化并查看这些类这个方法，但是由于词汇表通常非常大，并且在20000个维度中可视化数据是不可能的，像PCA这样的技术将有助于将数据压缩到两个维度。如下图。\n\n嵌入后这两个类依旧不太好分开，仅仅是降低了维度。为了看Bag of Words特征是否有用，我们根据它们来训练一个分类器。\n第四步：分类\n当涉及到对数据进行分类时，逻辑回归是最简单可用的工具，训练简单，结果可解释，可以很容易的从模型中提取最重要的系数。将数据分成一个适用于我们的模型和测试集的训练集，以了解它如何推广到不可见的数据。训练结束后得到了75.4%的准确度，虽然这个精度足够满足我们的需求，但是我们还是应该试图去理解它是如何工作的。\n第五步：检查\n混淆矩阵\n第一步是了解我们模型的错误类型，以及哪种类型的错误是最不可取。在我们的例子中，误报是将不相关的tweet归为灾难，而漏报是将灾难归类为不相关的tweet。如果要优先处理每个潜在的事件，就要降低漏报率，如果受到资源的限制，那么会游戏那考虑降低误报率。将这些信息可视化的一个好方法是使用混淆矩阵，将我们的模型与真实标签的预测相比较。理想情况下，矩阵将是从左上角到右下角的对角线。\n\n混淆矩阵\n结果显示该分类器漏报率更高。换句话说，我们的模型最常见的错误是将灾难分类为不相关的。\n解释我们的模型\n验证我们的模型并解释它的预测结果，重要的是看它使用哪些词作出预测。在数据有偏差时，分类器能在样本数据中做出准确预测，但是这个模型在现实世界中不能很好地推广。在这里，我们为灾难和不相关的推文绘制了最关键的单词表。\n\n我们分类器的词汇库能够处理大量的词汇。然而，有些词是非常频繁的，而且只会对我们的预测造成干扰。所以接下来，我们将尝试用一种方法来表示能够解释单词频率的句子，看看我们是否能从我们的数据中获得更多的信息。\n第六步：掌握词汇结构\nTF-IDF\n为了帮助我们的模型更多地关注有意义的单词，我们可以在我们的单词模型包上使用TF-IDF评分，下图为新嵌入的PCA投影。\n\n可视化TF-IDF嵌入\n我们可以看到这两个颜色之间的区别更明显了，这使我们的分类器更容易区分。我们在新的嵌入式系统上培训另一个逻辑回归，并最终达到了76.2%的精确度。一个轻微的改善，提高了模型的性能，所以我们可以考虑升级这个模型了。\n\nTF-IDF：文字的重要性\n第七步：Leveraging semantics\nWord2Vec\n即使是最新的模型也没法将训练中没有遇到的单词进行分类，哪怕是非常相似的单词。为了解决这个问题，我们所用的工具叫做Word2Vec。\nWord2Vec是一种查找单词连续嵌入的技术。它可以从阅读大量的文本中学习，并记住在类似的语境中出现的单词。论文的作者开放了一个在非常大的语料库中预先训练的模型，预先训练的向量可以在与这个帖子相关的存储库中找到。\n语句级别的表示\n为我们的分类器获得一个句子嵌入的一个快速方法是平均Word2Vec得分。这跟以前一样是Bag of Words的方法，但是这次我们只丢掉句子的语法，同时保留一些语义信息。\n\nWord2Vec句子嵌入\n下图是我们使用以前的技术实现的新嵌入的可视化：\n\n可视化Word2Vec嵌入\n在训练了相同的模型三次（逻辑回归）后，我们得到了77.7%的精度分数，这是现阶段得到的最好的结果！\n复杂性/ Explainability trade-oG\n由于我们的嵌入没有像以前的模型那样被表示为每个单词一维的矢量，所以很难看出哪些单词与我们的分类最相关。虽然我们仍然可以访问我们的逻辑回归的系数，但它们与我们嵌入的300个维度相关，而不是词的索引。然而，对于更复杂的模型，我们可以利用LIME等黑盒解释器来了解我们的分类器如何工作。\nLIME\n在GitHub上可以获得开源的LIME。它是一种允许用户解释任何分类器决定的黑盒解释器。\n\n正确的灾难词语被识别为“相关的”\n在这里，词语对分类的贡献似乎不那么明显\n我们需要在一个有代表性的测试用例上运行LIME，看看那些词最关键，用这种方法可以得到像以前模型一样重要的分数，并验证我们模型的预测结果。\n\nWord2Vec：文字的重要性\n由上图可得，这个模型收集了相关性非常高的词，暗示它做出了可以解释的预测结果，所以可以放心的部署到生成中。\n第八步：使用端到端的方法来利用语法\n由于以上方法省略了单词的顺序，丢弃了句子的句法信息，所以这些方法不能提供足够准确的结果。为此您可以使用更复杂的模型，一种常见的方法是将一个句子作为单个单词向量的序列，使用Word2Vec或者如GloVe、CoVe这样的方法。\n\n一个高度eGective端到端架构（源）\n卷积神经网络用于句子分类的训练非常迅速，并且是入门级的深度学习体系结构。卷积神经网络在文本相关的任务中表现非常出色，而且通常比大多数复杂的NLP方法(例如LSTMs和编码器/解码架构)要快得多。这个模型保存了单词的顺序，并学习了有价值的信息，其中的单词序列可以预测我们的目标类。训练这个模型不会比之前的方法麻烦，并且能获得79.5%的准确性。所以下一步应该是使用我们描述的方法来探索和解释预测，以验证它确实是部署到用户的最佳模型。\n本文由阿里云云栖社区组织翻译。\n文章原标题《How to solve 90% of NLP problems: a step-by-step guide》\n\n9.https://zhuanlan.zhihu.com/p/32829048\n自然语言处理中N-Gram模型介绍\n10.https://zhuanlan.zhihu.com/p/37646689\n自然语言处理最新教材开放下载，乔治亚理工大学官方推荐\n机器之心昨日，乔治亚理工大学 Jacob Eisenstein 教授开放了自然语言处理领域的最新教材《Natural Language Processing》，该教材 2018 年 6 月第一版的 PDF 已经在 GitHub 上开放下载。这本书的内容主要分为四大章节，即 NLP 中监督与无监等学习问题、序列与解析树等自然语言的建模方式、语篇语义的理解，以及后这些技术最在信息抽取、机器翻译和文本生成等具体任务中的应用。\n[10] https://www.zhihu.com/question/266856019/answer/319002132\n建议先从传统方法学起，真没必要上来就学224n。这里我强烈推荐哥伦比亚大学 Michael Collins 的自然语言处理课程，以前coursera有这门课程视频的，但是自从改版后好像找不到了，可以网上百度云盘搜一搜。你可以去他的个人主页看他的讲义，看后会有一种如沐春风的感觉，写的真是太好了，我研一的寒假把他的讲义全部打印出来，看了好几遍，自此算是入了NLP的大门。学习NLP，我建议第一步学language model， 然后依次学POS tagging， 语法分析PCFG，接着接触NLP的第一个实际应用，学习机器翻译（机器翻译真是融合了各种NLP知识到里面），先从基于统计的机器翻译开始学，IBM model1， IBM model 2，再到phrase based machine translation，然后再学log linear model。 再往后就可以学习各种应用啦，情感分析，文本分类等，这个可以上斯坦福的那门NLP课程，也是非常棒的课程。\n对于入门而言 上来就看CS224并不好 现在这门课已经变成完全的讲授深度学习的方法了 固然深度学习在NLP领域取得了重大的发展 但一上来就看深度学习 难免忽视了NLP的一些基础问题我在此首先推荐Chris Manning和Dan jurafsky两尊大神的至尊课程：introduction to natural language processing还有宅成翔教授的经典课程：Text Mining and Analyticshttps://zh.coursera.org/learn/text-mining这两门课程都会让你有一种如沐春风的感觉 然后彻底的疯狂的爱上NLP\n\n[11] https://www.zhihu.com/question/26391679/answer/34169968\n题主的问题太多了，每个展开都可以讲很多~作为自然语言处理（NLP）方向的研究生，我来回答一下题主关于自然语言处理如何入门的问题吧，最后再YY一下自然语言处理的前途~有点话我想说在前头：不管学什么东西，都要跟大牛去学，真正的大牛可以把一件事解释的清清楚楚。If you can't explain it simply, you don't understand it well enough.跟大牛学东西，你不会觉得难，一切都觉得很自然，顺利成章的就掌握了整套的知识。不过很遗憾，大牛毕竟是少数，愿意教别人的大牛更少，所以如果遇到，就不要强求语言了吧~开始进入正题，我将介绍如何从零基础入门到基本达到NLP前沿：----------NLP零基础入门----------首推资料以及唯一的资料：Columbia University, Micheal Collins教授的自然语言课程链接\u003e\u003e Michael CollinsMichael Collins，绝对的大牛，我心目中的偶像，这门课是我见过讲NLP最最最清楚的！尤其是他的讲义！Collins的讲义，没有跳步，每一步逻辑都无比自然，所有的缩写在第一次出现时都有全拼，公式角标是我见过的最顺眼的（不像有的论文公式角标反人类啊），而且公式角标完全正确（太多论文的公式角标有这样那样的错标，这种时候真是坑死人了，读个论文跟破译密码似的），而且几乎不涉及矩阵表示……（初学者可能不习惯矩阵表示吧）。最关键的是，Collins的语言措辞真是超级顺畅，没有长难句，没有装逼句，没有语法错误以及偏难怪的表示（学术圈大都是死理工科宅，语文能这么好真实太难得了）。《数学之美》的作者吴军博士在书中评价Collins的博士论文语言如小说般流畅，其写作功底可见一般。举两个例子，如果有时间，不妨亲自体验下，静下心来读一读，我相信即使是零基础的人也是能感受到大师的魅力的。1.语言模型（Language Model）http://www.cs.columbia.edu/~mcollins/lm-spring2013.pdf2.隐马尔可夫模型与序列标注问题(Tagging Problems and Hidden Markov Models)http://www.cs.columbia.edu/~mcollins/hmms-spring2013.pdf现在Michael Collins在coursera上也开了公开课，视频免费看链接\u003e\u003e Coursera比看讲义更清晰，虽然没有字幕，但是不妨一试，因为讲的真的好清楚。其在句法分析与机器翻译部分的讲解是绝对的经典。如果能把Collins的课跟下来，讲义看下来，那么你已经掌握了NLP的主要技术与现状了。应该可以看懂部分论文了，你已经入门了。----------NLP进阶----------Collins的NLP课程虽然讲的清晰，不过有些比较重要的前沿的内容没有涉及（应该是为了突出重点做了取舍），比如语言模型的KN平滑算法等。此外，Collins的课程更注重于NLP所依赖的基础算法，而对于这些算法的某些重要应用并没涉及，比如虽然讲了序列标注的算法隐马尔可夫模型，条件随机场模型，最大熵模型，但是并没有讲如何用这些算法来做命名实体识别、语义标注等。Stanford NLP组在coursera的这个课程很好的对Collins的课进行了补充。链接\u003e\u003e Coursera本课程偏算法的应用，算法的实现过的很快，不过上完Collins的课后再上感觉刚刚好~（这两门课是Coursera上仅有的两门NLP课，不得不佩服Coursera上的课都是精品啊！）----------进阶前沿----------上完以上两个课后，NLP的主要技术与实现细节就应该都清楚了， 离前沿已经很近了，读论文已经没问题了。想要继续进阶前沿，就要读论文了。NLP比起其它领域的一个最大的好处，此时就显现出来了，NLP领域的所有国际会议期刊论文都是可以免费下载的！而且有专人整理维护，每篇论文的bibtex也是相当清晰详细。链接\u003e\u003e ACL Anthology关于NLP都有哪些研究方向，哪些比较热门，可以参考：当前国内外在自然语言处理领域的研究热点\u0026难点？ - White Pillow 的回答NLP是会议主导，最前沿的工作都会优先发表在会议上。关于哪个会议档次比较高，可以参考谷歌给出的会议排名：Top conference页面也可以参考各个会议的录稿率（一般来说越低表示会议档次越高）：Conference acceptance rates基本上大家公认的NLP最顶级的会议为ACL，可以优先看ACL的论文。-------------------------最后简单谈一下这三者哪个更有发展潜力……作为一个NLP领域的研究生，当然要说NLP领域有潜力啦！这里YY几个未来可能会热门的NLP的应用：语法纠错目前文档编辑器（比如Word）只能做单词拼写错误识别，语法级别的错误还无能为力。现在学术领域最好的语法纠错系统的正确率已经可以接近50%了，部分细分错误可以做到80%以上，转化成产品的话很有吸引力吧~无论是增强文档编辑器的功能还是作为教学软件更正英语学习者的写作错误。结构化信息抽取输入一篇文章，输出的是产品名、售价，或者活动名、时间、地点等结构化的信息。NLP相关的研究很多，不过产品目前看并不多，我也不是研究这个的，不知瓶颈在哪儿。不过想象未来互联网信息大量的结构化、语义化，那时的搜索效率绝对比现在翻番啊~语义理解这个目前做的并不好，但已经有siri等一票语音助手了，也有watson这种逆天的专家系统了。继续研究下去，虽然离人工智能还相去甚远，但是离真正好用的智能助手估计也不远了。那时生活方式会再次改变。即使做不到这么玄乎，大大改进搜索体验是肯定能做到的~搜索引擎公司在这方面的投入肯定会是巨大的。机器翻译这个不多说了，目前一直在缓慢进步中~我们已经能从中获益，看越南网页，看阿拉伯网页，猜个大概意思没问题了。此外，口语级别的简单句的翻译目前的效果已经很好了，潜在的商业价值也是巨大的。不过……在可预见的近几年，对于各大公司发展更有帮助的估计还是机器学习与数据挖掘，以上我YY的那些目前大都还在实验室里……目前能给公司带来实际价值的更多还是推荐系统、顾客喜好分析、股票走势预测等机器学习与数据挖掘应用~\n\n[12] https://zhuanlan.zhihu.com/p/25004227\n【译文】基于Python的自然语言处理指南\nsudo easy_install pip\n安装NLTK: 在终端中运行:\nsudo pip install -U nltk\n\nScikit-learn: 机器学习库\nNatural Language Toolkit (NLTK): 为各种NLP技术提供轮子\nPattern – 网页挖掘模块，和NLTK搭配使用\nTextBlob – 方便的NLP工具的API，基于NLTK和Pattern架构\nspaCy – 工业级的NLP库\nGensim – 可构建主题模型\nStanford Core NLP – 斯坦福NLP小组提供的各类服务\n14.https://zhuanlan.zhihu.com/p/34520785\n13个自然语言处理的深度学习框架\n15.https://zhuanlan.zhihu.com/p/26249110\n基于 Python 的简单自然语言处理实践\n16.https://www.zhihu.com/question/59282225/answer/168654529\n谢邀。从符号主义和连接主义的对立走向合作，从静态分析走向交互，从语法和浅层语义走向深层语义，从功能主义走向认知和情感体验。2016年是深度学习的大潮冲击NLP的一年，果实丰硕。从底层的pos tagging, word segmentation, NER，到高级的任务比如semantic analysis, machine translation, machine reading comprehension, QA system, natural language generation。。都是全面开花，Deep learning for NLP的架构越来越成熟。经典的Speech and Language Processing也出了第三版的draft（ http://web.stanford.edu/~jurafsky/slp3/ ）。那么在2017年，我们又有什么样的期待呢？我想对于这个问题最有发言权的应该是Christopher Manning——他在Computational Linguistics and Deep Learning （http://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00239） 中的一些论点到了2017年依然成立。NLP无疑依然是机器学习有待攻克的下一个重大领域。但是由于语言本身已经是一种高层次的表达，深度学习在NLP中取得的成绩并不如在视觉领域那样突出。尤其是在NLP的底层任务中，基于深度学习的算法在正确率上的提升并没有非常巨大，但是速度却要慢许多，这对于很多对NLP来说堪称基础的任务来说，是不太能够被接受的，比如说分词。在一些高级任务中，基于端到端学习的神经网络确实取得了令人瞩目的成就，尤其是机器翻译方面。由于复杂性太高，这样的高级任务在此前是非常难以攻克的，无论是基于常规的统计学习方法，还是基于规则的方法。深度神经网络强悍的“记忆”能力和复杂特征提取能力非常适合于这类问题。在完形填空类型的阅读理解（cloze-style machine reading comprehension）上，基于attention的模型也取得了非常巨大的突破（在SQuAD数据集上，2016年8月的Exact Match最好成绩只有60%，今年3月已经接近77%，半年时间提升了接近20个点，这是极其罕见的）。但同时，深度学习的不可解释的特性和对于数据的需求，也使得它尚未在要求更高的任务上取得突破，比如对话系统（虽然对话在2016年随着Echo的成功已经被炒得火热）。相比于机器翻译，对话系统并不是一个简单的“sequence-to-sequence”的问题（虽然很多paper尝试这样去做）。对话系统必须要能够准确地理解问题，并且基于自身的知识系统和对于对话目标的理解，去生成一个回复。这并不是简单地去寻找“word alignment”就可以做到的。当然更不必说对于上下文和情感的理解。而相比于完形填空类型的机器阅读理解，对话系统可能的回复是完全开放的，并不是仅限于“答案包含在文本中”这样的情形。而开放式的阅读理解，同样是一个AI-complete的难题。这就要求我们对于交互的过程有更深刻的理解，对于人类在交流的过程中的认知过程和情感变化有更好的模型。而这个方向上，深度学习暂时还没有更好的办法。在这个过程中，就像Chris Manning说的一样，我们需要更好的理解模型的组合（ compositionally in models）。很显然，从传统的语言学到我们现在的端到端的靠大量数据的训练结果，其间还有很大一块认知过程的坑没有被填上。有一个有意思的事情是，在大多数端到端的NLP应用中，在输入中包括一些语言学的特征（例如pos tag或dependency tree）并不会对结果有重大影响。我们的一些粗浅的猜测，是因为目前的NLP做的这些特征，其实对于语义的表示都还比较差，某种程度来说所含信息还不如word embedding来的多。对于极其复杂、需要非常深的语义理解的任务来说，这些语言学特征并没有太多作用。这并不一定是对的——在结合语言学的规则与深度学习方面，太多实验等着我们去做了。所以，我们需要解决的不仅仅是Semantic Role Labelling，甚至Semantic Parsing或是Abstract Meaning Representation（ http://amr.isi.edu/）;我们需要知道的是从符号到人类体验的一种映射——不仅仅是“红色”可以被翻译为“Red”——我们想知道人类在看到红色时的感受，以及红色所代表的情绪。我们想要复原的是文字完全无法记录下来的现场的气氛，情绪和心跳的感觉（embodied experience）。同样的文字，在不同的场景，应该有完全不同的表达力。我们相信，仅仅依赖word2vec（或其它distributed representation）或是先进的memory-augmented networks，或是传统的NLP方法，都还无法解决这些问题。在情感和体验的另一个极端，我们又希望语言能够展示它“如雕塑一样的美感”（罗素形容数学用语），可以精准地描述概念和逻辑。这要求我们在语言的模糊性上建立出来健壮的知识和推理体系——同样，现在的深度学习也还不能做到这一点。只有结合了符号逻辑，神经网络以及认知科学，才有可能让我们在对语言的理解和处理上更上一层楼。（硬广Bayersian Cognitive Science/PPL https://www.zhihu.com/question/59442141/answer/166358150）现在结合一些热门的领域（任务）来谈一谈具体的方向。 Dialogue是的，自然语言对话将会开创一个新的人机交互时代。但是2016年流行的seq2seq对话框架不会给我们太大的惊喜。虽然理论上，如果能够给足训练数据，它是可以表现得很好的。原因在于，对话不同于翻译，翻译的input和output肯定是一个domain的东西，这大大限制了可能的解的空间。更重要的是，对话中有大量的省略和指代，我们必须通过大量的上下文信息才能够理解对话。这样的后果就是训练对话系统对于训练数据有指数级别上升的要求。就算我们已经记录了这个世界上所有人类的对话，明天依然会有人在不同的场景下说出的话，根本没有在训练集中出现。所以，2017年的对话系统，一定是在限定的场景下发挥作用的。即便是限定场景下的对话，也存在以下的几个难点需要攻克。后面例举的文章只是抛砖引玉。1.    怎样评估对话的质量？必须要和标准答案回答得一模一样才算好吗？（Towards an automatic Turing test: Learning to evaluate dialogue responses https://openreview.net/pdf?id=HJ5PIaseg）2.    怎么利用对话中人类的反馈来帮助学习？（Dialogue Learning With Human-in-the-Loop: https://arxiv.org/pdf/1611.09823.pdf）3.    怎样keep track of dialogue state？怎么定义目标？怎么记住多个对话片段？（Frames: A Corpus for Adding Memory to Goal-Oriented Dialogue Systems：https://arxiv.org/abs/1704.00057）4.    如何去做对话的policy？（Towards Information-Seeking Agents：https://arxiv.org/abs/1612.02605）5.    如何结合记忆以及情感？（Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory：https://arxiv.org/abs/1704.01074）6.    上下文如何建模？（Improving Frame Semantic Parsing with Hierarchical Dialogue Encoders：https://arxiv.org/abs/1705.03455）7.    对话回复的生成如何变得可控？（Data Distillation for Controlling Specificity in Dialogue Generation https://arxiv.org/pdf/1702.06703.pdf）阅读理解（Open-domain QA）去年到今年初MRC取得的进展大家已经有目共睹了，最高表现的架构基本趋同。估计再刷下去就要达到super-human performance了（人类的 baseline是82 EM, 91 F1）。比较有意思的是大家基本上都放弃了multi-hop reasoning的结构，原因非常简单：Stanford的SQuAD跟FB的bAbI不一样，没有专门设立这种需要推理的项目（诸如John went to the hall; John putdown the ball; Where is the ball?这类问题），大部分的问题主要依赖Attention机制就可以抓得很好了。bAbI这样的伪推理看来大家也是受够了。但是SQuAD本身也存在很多问题，抛开细的面不说，cloze-style本来就有很大的问题。而且最近出现了海量的刷SQuAD的文章，品质老实说并不敢恭维。幸好Stanford的Chen Danqi大神的Reading Wikipedia to Answer Open-Domain Questions （http://cs.stanford.edu/people/danqi/papers/acl2017.pdf ）打开了很多的方向。通过海量阅读（“machine reading at scale”），这篇文章试图回答所有在wikipedia上出现的factoid问题。其中有大量的工程细节，在此不表，仅致敬意。Unsupervised Learning在分布式语义表示这个“传统”深度学习领域（2013年算是很“传统”了吧），主要的工作还是向下，向上和向周边扩展（不小心说了句废话）。向下是指sub-word level。(Enriching Word Vectors with Subword Information：https://arxiv.org/abs/1607.04606）向上当然就是句子／篇章级别了。（A Simple but Tough-to-Beat Baseline for Sentence Embeddings：https://openreview.net/pdf?id=SyK00v5xx）向周边呢？就是面向任务，譬如知识库里的entity-embedding，或者面向sentiment analysis的情感-embedding。。好吧，我承认这类的文章真的已经看得太多了，并没有太大新意（no offense:我知道无论如何“boring”的文章，背后都是大家不眠不休的心血）。NLG 通过RNN-language model来做语言生成已经很成熟了，这都已经出survey paper了——Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation：https://arxiv.org/pdf/1703.09902.pdf但是通过GAN／VAE来生成呢？当然，做这个方向的人也很多，比如MSRA的Adversarial Neural Machine Translation（https://arxiv.org/abs/1704.06933）和Li Jiwei的Adversarial Learning for Neural Dialogue Generation：https://arxiv.org/pdf/1701.06547.pdf不过认真地说，我同意Ian Goodfellow在Reddit里说的：“GANs have not been applied to NLP because GANs are only defined for real-valued data.” （https://www.reddit.com/r/MachineLearning/comments/40ldq6/generative_adversarial_networks_for_text/）当然，做一些twist当然是可以强行让它work的，或者用VAE——但是目前看来，这些生成模型在自然语言方面并没有在图像方面的显著疗效。更重要的是，目前NLG的重要课题不是生成的质量，而是要搞清楚想说什么——类比一下，就如同就算人脑的Broca区域没有问题，可是Wernicke区域出现了问题，那么病人会说一口流利的语言，可是每一句话都毫无意义——这样的生成当然也是毫无意义的了。所以这个领域，其实还是任重道远啊。目前的很多“自然语言生成”或“写稿机器人”，还是carefully-crafted的模版来的多。总结通过2016年的努力，deep learning在NLP领域已经站稳了脚跟。我们期待2017年语言学和机器学习的进一步结合，让机器更加聪明，更懂你。欢迎大家补充、讨论：）本回答来自竹间智能Emotibot机器学习科学家赵宁远。  编辑于 2017-08-23 赞同 331 44 条评论 分享 收藏 感谢收起更多回答知乎用户深度学习（Deep Learning） 话题的优秀回答者收录于知乎圆桌 · 68 人赞同了该回答深度学习目前已经在NLP领域站稳脚跟 ，但是还没有成熟到像语音和图像那样可以和人类PK的程度，所以目前还是上升期，有三个点非常值得关注：1. 深度学习最初进入NLP走的是端到端的路线，靠无需人工特征知识即可达到state-of-art的卖点在NLP站稳脚跟。但是后面的发展过程中大家逐渐发现，只靠端到端是不行的。这个不仅仅是因为很多NLP任务监督数据匮乏的问题，因为在机器翻译这种语料充足的任务中，纯端到端的方法有很快遇到了瓶颈。所以大家把目光重新投向传统方法，和传统方法结合，借助外部知识来提高端到端模型的表现逐渐成为主流。机器翻译作为NLP领域中深度学习应用最成熟的方向，这一点尤为明显。2017年，这种结合的思路应该会进一步发展并向对话、摘要等其他NLP任务扩展，同时也会有更多通用的结合方法出现。2. 强化学习开始在NLP发力。AlphaGo的成功带来了强化学习的一轮热潮，大家很自然的会考虑用强化学习解决NLP中的一些问题。多轮对话是强化学习非常自然的应用场景，而chatbot的火热又在里面添了一把柴。另外强化学习在信息检索这种传统场景中的应用也值得关注，如多轮搜索。还有一点就是文本生成，目前文本生成还是很初级的阶段，而文本生成是可以看做一个马尔可夫决策过程，用强化学习解决的，因此很期待后续强化学习在这方面的应用。3. GAN在NLP开始发力。GAN在图像领域取得巨大成功，大家很自然的会想到把对抗的思路引入到NLP领域。目前除了在文本生成任务中结合GAN和强化学习的应用之外，多任务学习中也有对抗思想的引入。目前GAN在NLP中的效果还很一般，这个主要受制于NLP离散特性带来的梯度学习困难，因此GAN算法针对离散场景的改进是一个很有价值的研究方向，而由此带来的NLP任务突破则很值得期待。另一方面，GAN由于刚刚进入NLP领域，它的生成模型部分一般直接套用现有的复杂模型，而判别模型则比较简单，因此有很大改进空间，例如从matching方向借鉴一些更复杂的匹配模型。先写到这里，后续想到了再补充。编辑于 2017-05-15 赞同 68 6 条评论 分享 收藏 感谢刘知远用户标识自然语言处理、深度学习（Deep Learning）、机器学习 话题的优秀回答者63 人赞同了该回答2016年回答过这个题目，现在看来似乎并没有完全答对。也许这就是科研创新的魅力所在，就像一盒巧克力，打开前永远不知道它的口味是什么。2017年已经将近过半，其实有一些迹象已经可以从ACL 2017等会议论文窥豹一斑。我觉得2017年的发展将体现在以下几个方面：先验语言知识与深度学习模型的有机融合。从ACL 2017上NMT的相关论文可以看到，学者们纷纷将各种语言知识（如句法等）应用到NMT模型中，进一步提升机器翻译效果。该思路应该具有一定普适性。对抗训练思想的应用。虽然GAN本身尚未在NLP各领域得到广泛验证，但对抗训练思想已经在NMT等模型中开始发挥重要作用，值得关注。其他稍后想到了继续补充。\n17. https://zhuanlan.zhihu.com/p/35041012\n注意力机制\nAttention 机制最早是在视觉图像领域提出来的，应该是在九几年思想就提出来了，但是真正火起来应该算是 2014 年 Google Mind 团队的这篇论文 Recurrent Models of Visual Attention，他们在 RNN 模型上使用了 Attention机制来进行图像分类。\n随后，Bahdanau 等人在论文 Neural Machine Translation by Jointly Learning to Align and Translate 中，使用类似 Attention 的机制在机器翻译任务上将翻译和对齐同时进行，他们的工作算是第一个将 Attention 机制应用到 NLP 领域中。\n接着 Attention 机制被广泛应用在基于 RNN/CNN 等神经网络模型的各种 NLP 任务中。2017 年，Google 机器翻译团队发表的 Attention is All You Need 中大量使用了自注意力（self-attention）机制来学习文本表示。自注意力机制也成为了大家近期的研究热点，并在各种 NLP 任务上进行探索。\n\n18.https://www.zhihu.com/question/24417961/answer/66872781\n自然语言处理有一套严整的理论体系，如果希望系统学习可以参考Stanford NLP Group几位教授的三本教科书，基本都有中文翻译版本。以下按照我心目中的浅易程度排序：Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. 2008.Introduction to Information Retrieval. Cambridge University Press.Christopher D. Manning and Hinrich Schütze. 1999. Foundations of Statistical Natural Language Processing. Cambridge, MA: MIT Press.Daniel Jurafsky and James H. Martin. 2008. Speech and Language Processing: An Introduction to Natural Language Processing, Speech Recognition, and Computational Linguistics. 2nd edition. Prentice-Hall.\n\n19. https://www.zhihu.com/question/57057613/answer/151471222\nNatural Language Toolkit\n和中文分词\nfxsjy/jieba\ngensim: word2vec\nnltk: tokenization, segmentation\nkeras: sequence to sequence learning\n\n20.https://zhuanlan.zhihu.com/p/25889937\n自然语言处理系列篇——关键词智能提取\n[21] https://zhuanlan.zhihu.com/p/30138012\n自然语言处理领域重要论文\u0026资源全索引\n22.https://zhuanlan.zhihu.com/p/25612011\n机器学习、深度学习与自然语言处理领域推荐的书籍列表\n23.https://zhuanlan.zhihu.com/p/28616862\n自然语言处理从入门到进阶资代码资源库汇总（随时更新）","data":"2018年07月18日 16:05:49"}
{"_id":{"$oid":"5d344ddf62f717dc0659b68d"},"title":"NLP 自然语言处理 中文任务列表","author":"DarrenXf","content":"table\nI translated it myself. It may not be authoritative.\nindex\nEnglish\nChinese\n1\nAutomatic speech recogniton\n自动语音识别\n2\nCCG supertagging\nCCG 超级标记\n3\nCommon sense\n常识\n4\nConstituency parsing\n选区分析\n5\nCoreference resolution\n共指消解\n6\nDependency parsing\n依存关系句法分析\n7\nDialogue\n对话\n8\nDomain adaptation\n领域自适应\n9\nEntity linking\n实体链接\n10\nGrammatical error corrrection\n语法错误更正\n11\nInformation extraction\n信息提取\n12\nLanguage modeling\n语言模型\n13\nLexical normalization\n词汇规范化\n14\nMachine translation\n机器翻译\n15\nMulti-task learning\n多任务学习\n16\nMulti-model\n多模态\n17\nNamed entity recognition\n命名实体识别\n18\nNatural language inference\n自然语言推理\n19\nPart-of-speech tagging\n词性标注\n20\nQuestion answering\n问答\n21\nRelation prediction\n关系预测\n22\nRelationship extraction\n关系提取\n23\nSemantic textual similarity\n语义文本相似性\n24\nSemantic parsing\n语义分析\n25\nSemantic role labeling\n语义角色标记\n26\nSentiment analysis\n情感分析\n27\nShallow syntax\n浅句法\n28\nSimplification\n简化\n29\nStance detection\n姿态检测\n30\nSummarization\n摘要\n31\nTaxonomy learning\n分类学习\n32\nTemporal processing\n时间处理\n33\nText classification\n文本分类\n34\nWord sense disambiguation\n词义消歧","data":"2019年02月26日 12:48:21"}
{"_id":{"$oid":"5d344e0462f717dc0659b694"},"title":"AI 人工智能学习路线","author":"微风--轻许--","content":"相关视频资料下载见：https://blog.csdn.net/qwxwaty/article/details/80800701\n阶段一、人工智能基础 －　高等数学必知必会\n本阶段主要从数据分析、概率论和线性代数及矩阵和凸优化这四大块讲解基础，旨在训练大家逻辑能力，分析能力。拥有良好的数学基础，有利于大家在后续课程的学习中更好的理解机器学习和深度学习的相关算法内容。同时对于AI研究尤为重要，例如人工智能中的智能很大一部分依托“概率论”实现的。\n一、数据分析\n1）常数e\n2）导数\n3）梯度\n4）Taylor\n5）gini系数\n6）信息熵与组合数\n7）梯度下降\n8）牛顿法\n二、概率论\n1）微积分与逼近论\n2）极限、微分、积分基本概念\n3）利用逼近的思想理解微分，利用积分的方式理解概率\n4）概率论基础\n5）古典模型\n6）常见概率分布\n7）大数定理和中心极限定理\n8）协方差(矩阵)和相关系数\n9）最大似然估计和最大后验估计\n三、线性代数及矩阵\n1）线性空间及线性变换\n2）矩阵的基本概念\n3）状态转移矩阵\n4）特征向量\n5）矩阵的相关乘法\n6）矩阵的QR分解\n7）对称矩阵、正交矩阵、正定矩阵\n8）矩阵的SVD分解\n9）矩阵的求导\n10）矩阵映射/投影\n四、凸优化\n1）凸优化基本概念\n2）凸集\n3）凸函数\n4）凸优化问题标准形式\n5）凸优化之Lagerange对偶化\n6）凸优化之牛顿法、梯度下降法求解\n阶段二、人工智能提升 － Python高级应用\n随着AI时代的到来以及其日益蓬勃的发展，Python作为AI时代的头牌语言地位基本确定，机器学习是着实令人兴奋，但其复杂度及难度较大，通常会涉及组装工作流和管道、设置数据源及内部和云部署之间的分流而有了Python库后，可帮助加快数据管道，且Python库也在不断更新发布中，所以本阶段旨在为大家学习后续的机器学习减负。\n一、容器\n1）列表:list\n2）元组:tuple\n3）字典: dict\n4）数组: Array\n5）切片\n6）列表推导式\n7）浅拷贝和深拷贝\n二、函数\n1）lambda表达式\n2）递归函数及尾递归优化\n3）常用内置函数/高阶函数\n4）项目案例：约瑟夫环问题\n三、常用库\n1）时间库\n2）并发库\n3）科学计算库\n4）Matplotlib可视化绘图库\n5）锁和线程\n6）多线程编程\n阶段三、人工智能实用 － 机器学习篇\n机器学习利用算法去分析数据、学习数据，随后对现实世界情况作出判断和预测。因此，与预先编写好、只能按照特定逻辑去执行指令的软件不同，机器实际上是在用大量数据和算法去“自我训练”，从而学会如何完成一项任务。\n所以本阶段主要从机器学习概述、数据清洗和特征选择、回归算法、决策树、随机森林和提升算法、SVM、聚类算、EM算法、贝叶斯算法、隐马尔科夫模型、LDA主题模型等方面讲解一些机器学习的相关算法以及这些算法的优化过程，这些算法也就是监督算法或者无监督算法。\n一、机器学习\n1）机器学习概述\n二、监督学习\n1）逻辑回归\n2）softmax分类\n3）条件随机场\n4）支持向量机svm\n5）决策树\n6）随机森林\n7）GBDT\n8）集成学习\n三、非监督学习\n1）高斯混合模型\n2）聚类\n3）PCA\n4）密度估计\n5）LSI\n6）LDA\n7）双聚类\n四、数据处理与模型调优\n1）特征提取\n2）数据预处理\n3）数据降维\n4）模型参数调优\n5）模型持久化\n6）模型可视化\n阶段四、人工智能实用 － 数据挖掘篇\n本阶段主要通过音乐文件分类和金融反欺诈模型训练等项目，帮助大家对于上阶段的机器学习做更深入的巩固，为后续深度学习及数据挖掘提供项目支撑。\n项目一：百度音乐系统文件分类\n音乐推荐系统就是利用音乐网站上的音乐信息，向用户提供音乐信息或者建议，帮助用户决定应该听什么歌曲。而个人化推荐则是基于音乐信息及用户的兴趣特征、听歌历史行为，向用户推荐用户可能会感兴趣的音乐或者歌手。推荐算法主要分为以下几种：基于内容的推荐、协同过滤推荐、基于关联规则推荐、基于效用推荐、基于知识推荐等；推荐系统常用于各个互联网行业中，比如音乐、电商、旅游、金融等。\n项目二：千万级P2P金融系统反欺诈模型训练\n目前比较火的互联网金融领域，实质是小额信贷，小额信贷风险管理，本质上是事前对风险的主动把控，尽可能预测和防范可能出现的风险。本项目应用GBDT、Randomforest等机器学习算法做信贷反欺诈模型，通过数据挖掘技术，机器学习模型对用户进行模型化综合度量，确定一个合理的风险范围，使风险和盈利达到一个平衡的状态。\n阶段五、人工智能前沿 －　深度学习篇\n深度学习是实现机器学习的技术，同时深度学习也带来了机器学习的许多实际应用，拓展了AI的使用领域，本阶段主要从TensorFlow、BP神经网络、深度学习概述、CNN卷积神经网络、递归神经网、自动编码机，序列到序列网络、生成对抗网络，孪生网络，小样本学习技术等方面讲解深度学习相关算法以，掌握深度学习前沿技术，并根据不同项目选择不同的技术解决方案。针对公司样本不足，采用小样本技术和深度学习技术结合，是项目落地的解决方案。\n1）TensorFlow基本应用\n2）BP神经网络\n3）深度学习概述\n4）卷积神经网络(CNN)\n5）图像分类(vgg,resnet)\n6）目标检测(rcnn,fast-rcnn,faster-rcnn,ssd)\n7）递归神经网络(RNN)\n8）lstm,bi-lstm,多层LSTM\n9）无监督学习之AutoEncoder自动编码器\n10）Seq2Seq\n11）Seq2Seq with Attension\n12）生成对抗网络\n13）irgan\n14）finetune及迁移学习\n15）孪生网络\n16）小样本学习\n阶段六、人工智能进阶 － 自然语言处理篇\n自然语言处理（NLP）是计算机科学领域与人工智能领域中的一个重要方向。它已成为人工智能的核心领域。自然语言处理解决的是“让机器可以理解自然语言”这一到目前为止都还只是人类独有的特权，被誉为人工智能皇冠上的明珠，被广泛应用。本阶段从NLP的字、词和句子全方位多角度的学习NLP，作为NLP的基础核心技术，对NLP为核心的项目，如聊天机器人，合理用药系统，写诗机器人和知识图谱等提供底层技术。通过学习NLP和深度学习技术，掌握NLP具有代表性的前沿技术。\n1）词（分词，词性标注）代码实战\n2）词（深度学习之词向量，字向量）代码实战\n3）词（深度学习之实体识别和关系抽取）代码实战\n4）词（关键词提取，无用词过滤）代码实战\n5）句（句法分析，语义分析）代码实战\n6）句（自然语言理解,一阶逻辑）代码实战\n7）句（深度学习之文本相似度）代码实战\n阶段七、人工智能进阶 － 图像处理篇\n数字图像处理(Digital Image Processing)是通过计算机对图像进行去除噪声、增强、复原、分割、提取特征等处理的方法和技术。广泛的应用于农牧业、林业、环境、军事、工业和医学等方面，是人工智能和深度学习的重要研究方向。深度学习作为当前机器学习领域最热门的技术之一，已经在图像处理领域获得了应用，并且展现出巨大的前景。本阶段学习了数字图像的基本数据结构和处理技术，到前沿的深度学习处理方法。掌握前沿的ResNet,SSD,Faster RCNN等深度学习模型，对图像分类，目标检测和模式识别等图像处理主要领域达到先进水平。实际工作中很多项目都可以转化为本课程的所学的知识去解决，如行人检测，人脸识别和数字识别。\n一、图像基础\n图像读，写，保存，画图（线，圆，多边形，添加文字）\n二、图像操作及算数运算\n图像像素读取，算数运算，ROI区域提取\n三、图像颜色空间运算\n图像颜色空间相互转化\n四、图像几何变换\n平移，旋转，仿射变换，透视变换等\n五、图像形态学\n腐蚀，膨胀，开/闭运算等\n六、图像轮廓\n长宽，面积，周长，外接圆，方向，平均颜色，层次轮廓等\n七、图像统计学\n图像直方图\n八、图像滤波\n高斯滤波，均值滤波，双边滤波，拉普拉斯滤波等\n阶段八、人工智能终极实战 － 项目应用\n本阶段重点以项目为导向，通过公安系统人脸识别、图像识别以及图像检索、今日头条CTR广告点击量预估、序列分析系统、聊天机器人等多个项目的讲解，结合实际来进行AI的综合运用。\n项目一：公安系统人脸识别、图像识别\n使用深度学习框架从零开始完成人脸检测的核心技术图像类别识别的操作，从数据预处理开始一步步构建网络模型并展开分析与评估，方便大家快速动手进行项目实践！识别上千种人靓，返回层次化结构的每个人的标签。\n项目二：公安系统图像检索\n本项目基于卷积神经网在训练过程中学习出对应的『二值检索向量』，对全部图先做了一个分桶操作，每次检索的时候只取本桶和临近桶的图片作比对，而不是在全域做比对，使用这样的方式提高检索速度，使用Tensorflow框架建立基于ImageNet的卷积神经网络，并完成模型训练以及验证。\n项目三：今日头条CTR广告点击量预估\n点击率预估是广告技术的核心算法之一，它是很多广告算法工程师喜爱的战场。广告的价值就在于宣传效果,点击率是其中最直接的考核方式之一,点击率越大,证明广告的潜在客户越多,价值就越大,因此才会出现了刷点击率的工具和技术。通过对于点击量的评估，完成对于潜在用户的价值挖掘。\n项目四：序列分析系统\n时间序列分析(Time Series Analysis)是一种动态数据处理的统计方法，主要基于随机过程理论和数理统计方法，研究随机数据序列所遵从的统计规律以便用于解决实际问题。主要包括自相关分析等一般的统计分析方法，构建模型从而进行业务推断。经典的统计分析是假定数据序列具有独立性，而时间序列分析则侧重于研究数据样本序列之间的依赖关系。时间序列预测一般反应了三种实际变化规律：趋势变化、周期性变化和随机性变化。时间序列预测常应用于国民经济宏观控制、企业经营管理、市场潜力量预测、天气预报、水文预报等方面，是应用于金融行业的一种核心算法之一。\n项目五：京东聊天机器人/智能客服\n聊天机器人/智能客服是一个用来模拟人类对话或者聊天的一个系统，利用深度学习和机器学习等NLP相关算法构建出问题和答案之间的匹配模型，然后可以将其应用到客服等需要在线服务的行业领域中，聊天机器人可以降低公司客服成本，还能够提高客户的体验友好性。 在一个完整的聊天机器人实现过程中，主要包含了一些核心技术，包括但不限于：爬虫技术、机器学习算法、深度学习算法、NLP领域相关算法。通过实现一个聊天机器人可以帮助我们队AI整体知识的一个掌握。\n项目六：机器人写诗歌\n机器人写诗歌/小说是一种基于NLP自然语言相关技术的一种应用，在实现过程中可以基于机器学习相关算法或者深度学习相关算法来进行小说/诗歌构建过程。人工智能的一个终极目标就是让机器人能够像人类一样理解文字，并运用文字进行创作，而这个目标大致上主要分为两个部分，也就是自然语言理解和自然语言生成，其中现阶段的主要自然语言生成的运用，自然语言生成主要有两种不同的方式，分别为基于规则和基于统计，基于规则是指首先了解词性及语法等规则，再依据这样的规则写出文章；而基于统计的本质是根据先前的字句和统计的结果，进而判断下一个子的生成，例如马尔科夫模型就是一种常用的基于统计的方法。\n项目七：机器翻译系统\n机器翻译又称自动翻译，是指利用计算机将一种自然语言转换为另外一种自然语言的过程，机器翻译是人工智能的终极目标之一，具有很高的研究价值，同时机器翻译也具有比较重要的实用价值，机器翻译技术在促进政治、经济、文化交流等方面起到了越来越重要的作用；机器翻译主要分为以下三个过程：原文分析、原文译文转换和译文生成；机器翻译的方式有很多种，但是随着深度学习研究取得比较大的进展，基于人工网络的机器翻译也逐渐兴起，特别是基于长短时记忆(LSTM)的循环神经网络(RDD)的应用，为机器翻译添了一把火。\n项目八：垃圾邮件过滤系统\n邮件主要可以分为有效邮件和垃圾邮件两大类，有效邮件指的邮件接收者有意义的邮件，而垃圾邮件转指那些没有任何意义的邮件，其内容主要包含赚钱信息、成人广告、商业或者个人网站广告、电子杂志等，其中垃圾邮件又可以发为良性垃圾邮件和恶性垃圾邮件，良性垃圾邮件指的就是对收件人影响不大的信息邮件，而恶性垃圾邮件指具有破坏性的电子邮件，比如包含病毒、木马等恶意程序的邮件。垃圾邮件过滤主要使用使用机器学习、深度学习等相关算法，比如贝叶斯算法、CNN等，识别出所接收到的邮件中那些是垃圾邮件。\n项目九：手工数字识别\n人认知世界的开始就是从认识数字开始的，深度学习也一样，数字识别是深度学习的一个很好的切入口，是一个非常经典的原型问题，通过对手写数字识别功能的实现，可以帮助我们后续对神经网络的理解和应用。选取手写数字识别的主要原因是手写数字具有一定的挑战性，要求对编程能力及神经网络思维能力有一定的要求，但同时手写数字问题的复杂度不高，不需要大量的运算，而且手写数字也可以作为其它技术的一个基础，所以以手写数字识别为基础，贯穿始终，从而理解深度学习相关的应用知识。\n项目十：癌症筛选检测\n技术可以改变癌症患者的命运吗，对于患有乳腺癌患者来说，复发还是痊愈影响这患者的生命，那么怎么来预测患者的患病结果呢，机器学习算法可以帮助我们解决这一难题，本项目应用机器学习logistic回归模型，来预测乳腺癌患者复发还是正常，有效的预测出医学难题。\n项目十一：葡萄酒质量检测系统\n随着信息科技的快速发展,计算机中的经典算法在葡萄酒产业中得到了广泛的研究与应用。其中机器学习算法的特点是运用了人工智能技术,在大量的样本集训练和学习后可以自动地找出运算所需要的参数和模型。\n项目十二：淘宝网购物篮分析推荐算法\n购物篮分析(Market Basket Analysis)即非常有名的啤酒尿布故事的一个反应，是通过对购物篮中的商品信息进行分析研究，得出顾客的购买行为，主要目的是找出什么样的物品会经常出现在一起，也就是那些商品之间是有很大的关联性的。通过购物篮分析挖掘出来的信息可以用于指导交叉销售、追加销售、商品促销、顾客忠诚度管理、库存管理和折扣计划等业务；购物篮分析的最常用应用场景是电商行业，但除此之外，该算法还被应用于信用卡商城、电信与金融服务业、保险业以及医疗行业等。\n项目十三：手工实现梯度下降回归算法\n梯度下降法（英语：Gradient descent）是一个一阶最优化算法，通常也称为最速下降法。 要使用梯度下降法找到一个函数的局部极小值，必须向函数上当前点对应梯度（或者是近似梯度）的反方向的规定步长距离点进行迭代搜索。如果相反地向梯度正方向迭代进行搜索，则会接近函数的局部极大值点；这个过程则被称为梯度上升法。\n项目十四：基于TensorFlow实现回归算法\n回归算法是业界比较常用的一种机器学习算法，通过应用于各种不同的业务场景，是一种成熟而稳定的算法种类；TensorFlow是一种常用于深度学习相关领域的算法工具；随着深度学习热度的高涨，TensorFlow的使用也会越来越多，从而使用TensorFlow来实现一个不存在的算法，会加深对TensorFlow的理解和使用；基于TensorFlow的回归算法的实现有助于后续的TensorFlow框架的理解和应用，并可以促进深度学习相关知识的掌握。\n项目十五：合理用药系统\n合理用药系统，是根据临床合理用药专业工作的基本特点和要求，运用NLP和深度学习技术对药品说明书，临床路径等医学知识进行标准化，结构化处理。如自动提取药品说明书文本里面的关键信息如：药品相互作用，禁忌，用法用量，适用人群等，实现医嘱自动审查，及时发现不合理用药问题，帮助医生、药师等临床专业人员在用药过程中及时有效地掌握和利用医药知识，预防药物不良事件的发生、促进临床合理用药工作。\n项目十六：行人检测\n行人检测是利用图像处理技术和深度学习技术对图像或者视频序列中是否存在行人并给予精确定位。学习完行人检测技术后，对类似的工业缺陷检测，外观检测和医疗影像检测等目标检测范畴类的项目可以一通百通。该技术可与行人跟踪，行人重识别等技术结合，应用于人工智能系统、车辆辅助驾驶系统、智能机器人、智能视频监控、人体行为分析、智能交通等领域。由于行人兼具刚性和柔性物体的特性 ，外观易受穿着、尺度、遮挡、姿态和视角等影响，使得行人检测成为计算机视觉领域中一个既具有研究价值同时又极具挑战性的热门课题。\n阶段九、人工智能实战 － 企业项目实战\n课程一、基于Python数据分析与机器学习案例实战教程\n课程风格通俗易懂，基于真实数据集案例实战。主体课程分成三个大模块(1)python数据分析，(2)机器学习经典算法原理详解,(3)十大经典案例实战。通过python数据科学库numpy,pandas,matplot结合机器学习库scikit-learn完成一些列的机器学习案例。算法课程注重于原理推导与流程解释，结合实例通俗讲解复杂的机器学习算法，并以实战为主，所有课时都结合代码演示。算法与项目相结合，选择经典kaggle项目，从数据预处理开始一步步代码实战带大家快速入门机器学习。旨在帮助同学们快速上手如何使用python库来完整机器学习案例。选择经典案例基于真实数据集，从数据预处理开始到建立机器学习模型以及效果评估，完整的讲解如何使用python及其常用库进行数据的分析和模型的建立。对于每一个面对的挑战，分析解决问题思路以及如何构造合适的模型并且给出合适评估方法。在每一个案例中，同学们可以快速掌握如何使用pandas进行数据的预处理和分析，使用matplotlib进行可视化的展示以及基于scikit-learn库的机器学习模型的建立。\n1）Python数据分析与机器学习实战课程简介\n2）Python快速入门\n3）Python科学计算库Numpy\n4）Python数据分析处理库Pandas\n5）Python可视化库Matplotlib\n6）回归算法\n7）模型评估\n8）K近邻算法\n9）决策树与随机森林算法\n10）支持向量机\n11）贝叶斯算法\n12）神经网络\n13）Adaboost算法\n14）SVD与推荐\n15）聚类算法\n16）案例实战：使用Python库分析处理Kobe Bryan职业生涯数据\n17）案例实战：信用卡欺诈行为检测\n18）案例实战：泰坦尼克号获救预测\n19）案例实战：鸢尾花数据集分析\n20）案例实战：级联结构的机器学习模型\n21）案例实战：员工离职预测\n22）案例实战：使用神经网络进行手写字体识别\n23）案例实战：主成分分析\n24）案例实战：基于NLP的股价预测\n25）案例实战：借贷公司数据分析\n课程二、人工智能与深度学习实战\n课程风格通俗易懂，必备原理，形象解读，项目实战缺一不可！主体课程分成四个大模块(1)神经网络必备基础知识点，(2)深度学习模型，(3)深度学习框架Caffe与Tensorflow，(4)深度学习项目实战。 课程首先概述讲解深度学习应用与挑战，由计算机视觉中图像分类任务开始讲解深度学习的常规套路。对于复杂的神经网络，将其展开成多个小模块进行逐一攻破，再挑战整体神经网络架构。对于深度学习模型形象解读卷积神经网络原理，详解其中涉及的每一个参数，对卷积网络架构展开分析与评估，对于现阶段火爆的对抗生成网络以及强化学习给出形象解读，并配合项目实战实际演示效果。 基于框架实战，选择两款深度学习最火框架，Caffe与Tensorflow，首先讲解其基本使用方法，并结合案例演示如何应用框架构造神经网络模型并完成案例任务。 选择经典深度学习项目实战，使用深度学习框架从零开始完成人脸检测，验证码识别，人脸关键点定位，垃圾邮件分类，图像风格转换，AI自己玩游戏等。对于每一个项目实战，从数据预处理开始一步步构建网络模型并展开分析与评估。 课程提供所涉及的所有数据，代码以及PPT，方便大家快速动手进行项目实践！\n1）深度学习概述与挑战\n2）图像分类基本原理门\n3）深度学习必备基础知识点\n4）神经网络反向传播原理\n5）神经网络整体架构\n6）神经网络案例实战图像分类任务\n7）卷积神经网络基本原理\n8）卷积参数详解\n9）卷积神经网络案例实战\n10）经典网络架构分析\n11）分类与回归任务\n12）三代物体检测算法分析\n13）数据增强策略\n14）TransferLearning\n15）网络架构设计\n16） 深度学习框架Caffe网络结构配置\n17）Caffe\n18）深度学习项目实战人脸检测\n19）人脸正负样本数据源制作\n20）人脸检测网络架构配置习模型\n21）人脸检测代码实战\n22）人脸关键点定位项目实战\n23）人脸关键点定位网络模型\n24）人脸关键点定位构建级联网络\n25）人脸关键点定位测试效果与分析\n26）Tensorflow框架实战\n27）Tensorflow构建回归模型\n28）Tensorflow构建神经网络模型\n29）Tensorflow深度学习模型\n30）Tensorflow打造RNN网络模型\n31）Tensorflow项目实战验证识别\n32）项目实战图像风格转换\n33）QLearning算法原理\n34）DQN网络架构\n35）项目实战DQN网络让AI自己玩游戏\n36）项目实战对抗生成网络等\n项目一、AI大数据互联网电影智能推荐（第一季）\n随着科技的发展，现在视频的来源和类型多样性，互联网视频内容充斥着整个网络，如果仅仅是通过翻页的方法来寻找自己想看的视频必然会感到疲劳，现在急需一种能智能推荐的工具，推荐系统通过分析用户对视频的评分分析，对用户的兴趣进行建模，从而预测用户的兴趣并给用户进行推荐。\nPython是一种面向对象的解释型计算机程序设计语言，Python具有丰富和强大的库。它常被昵称为胶水语言，而大数据是指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，企业面临海量数据的到来，大多选择把数据从本地迁移至云端，云端将成为最大的非结构化数据存储场所。本项目主要以客户咨询为载体，分析客户的群体，分布，旨在挖掘客户的内在需求，帮助企业实现更有价值的营销。\n一、教务管理系统业务介绍\n1）教务管理系统框架讲解\n2）系统业务逻辑介绍\n二、大数据需求分析\n1）明确数据需求\n2）大数据分析过程\n3）分析难点和解决方案\n4）大数据相关技术选型\n三、构建分布式大数据框架\n1）Hadoop分布式集群配置\n2）ZooKeeper高可用\n3）SQOOP数据转移\n4）ETL数据清洗\n5）HIVE数据分析\n6）HBase数据存储\n四、基于教务管理系统大数据分析\n1）业务数据分析指标设定\n2）操作MapReduce分而治之\n3）使用Hive进行数据整合抽离\n4）使用HBase存储非结构话数据\n五、大数据可视化\n1）可视化技术选型\n2）Echarts代码展示炫酷视图\n3）使用Tableau进行数据可视化展示\n项目二、电商大数据情感分析与AI推断实战项目（第一季）\n本项目从开发的角度以大数据、PHP技术栈为基础，使用真实商用表结构和脱敏数据，分三步构建商用系统、真实大数据环境、进行推断分析以及呈现结果。 项目课程的完整性、商业性，可以使学者尽可能完整地体会真实的商业需求和业务逻辑。完整的项目过程，使PHP技术栈的同学得以窥见和学到一个完整商业平台项目的搭建方法；真实大数据环境的搭建，使呈现、建立大数据的工具应用技术概念储备；基于大数据平台的分析需求的实现、呈现，将完整的一次大数据技术栈到分析结果的中线，平铺直述，为想要学习大数据并有开发基础的同学点亮新的能力。\n一、实践项目研发\n1）开发环境的安装配置\n2）表与数据\n3）LARAVEL的快速开发实践\n4）批量创建模型\n5）万能控制器与表配置\n6）统一视图的创建\n二、数据分析需求设立\n1）定义数据需求\n2）分析计算过程\n3）分析难点和解决方案\n4）大数据技术选型\n三、大数据平台搭建\n1）分布式环境的模拟建立\n2）网络环境的调通\n3）身份验证与集群控制\n4）Hadoop环境搭建和要点说明\n5）MapReduce与Yarn的搭建和说明\n四、大数据分析脚本编写\n1）MapReduce脚本编写\n2）拆解数据需求\n3）Map逻辑详写\n4）Reduce逻辑详写\n5）结果整理与输出\n五、结果可视化\n1）可视化需求和技术选型\n2）展示页面的快速铺设\n3）可视化JS上手\n4）使用可视化JS展示结果\n项目三、AI法律咨询大数据分析与服务智能推荐实战项目(第一季)\n本项目结合目前流行的大数据框架，在原有成熟业务的前提下，进行大数据分析处理，真实还原企业应用，让学员身临其境的感受企业大数据开发的整个流程。\n项目的业务系统底层主要采用JAVA架构，大数据分析主要采用Hadoop框架，其中包括Kettle实现ETL、SQOOP、Hive、Kibana、HBASE、Spark以及人工智能算法等框架技术；采用真实大数据集群环境的搭建，让学员切身感受企业项目的从0到1的过程。\n一、系统业务介绍\n1）底层业务实现框架讲解\n2）功能模块讲解\n二、系统架构设计\n1）总体架构分析\n2）数据流向\n3）各技术选型承载作用\n4）部署方案\n三、详尽实现\n1）原始数据处理\n2）ETL数据导入\n3）MR数据计算\n4）Hive数据分析\n四、数据可视化\n1）采用Highcharts插件展示客户偏好曲线图\n2）使用Tableau进行数据分析可视化展示\n五、项目优化\n1）ZooKeeper实现HA\n2）集群监控的整体联调\n项目四、AI大数据基站定位智能推荐商圈分析项目实战（第一季）\n随着当今个人手机终端的普及、出行人群中手机拥有率和使用率已达到相当高的比例，根据手机信号在真实地理空间的覆盖情况，将手机用户时间序列的手机定位数据，映射至现实地理位置空间位置，即可完整、客观地还原出手机用户的现实活动轨迹，从而挖掘出人口空间分布与活动联系特征信息。\n商圈是现代市场中企业市场活动的空间，同时也是商品和服务享用者的区域。商圈划分为目的之一是研究潜在顾客分布，以制定适宜的商业对策。\n本项目以实战为基础结合大数据技术Hadoop、.Net技术全栈为基础，采用真实商业数据，分不同环节构建商用系统、真实大数据环境、进行推断分析及呈现数据。\n一、分析系统业务逻辑讲解\n1）大数据基站定位智能推荐商圈分析系统介绍\n2）数据前期清洗和数据分析目标指标的设定等\n二、大数据导入与存储\n1）关系型数据库基础知识\n2）hive的基本语法\n3）hive的架构及设计原理\n4）hive安装部署与案例等\n5）Sqoop安装及使用\n6）Sqoop与关系型数据库进行交互等\n7）动手实践\n三、Hbase理论及实战\n1）Hbase简介、安装及配置\n2）Hbase的数据存储与数据模型\n3）Hbase Shell\n4）Hbase 访问接口\n5）Hbase数据备份与恢复方法等\n6）动手实践（数据转储与备份）\n四、基站数据分析与统计推断\n1）背景与分析推断目标\n2）分析方法与过程推断\n3）动手实践（分析既定指标数据）\n五、数据分析与统计推断结果的展示（大数据可视化）\n1）使用Tableau展示数据分析结果\n2）使用HighCharts、ECharts展示数据分析结果\n阶段十、阿里云认证\n课程一、云计算 - 网站建设：部署与发布\n阿里云网站建设认证课程教你如何掌握将一个本地已经设计好的静态网站发布到Internet公共互联网，绑定域名，完成工信部的ICP备案。\n课程二、云计算 - 网站建设：简单动态网站搭建\n阿里云简单动态网站搭建课程教你掌握如何快速搭建一个WordPress动态网站，并会对网站进行个性化定制，以满足不同的场景需求。\n课程三、云计算 - 云服务器管理维护\n阿里云服务器运维管理课程教你掌握快速开通一台云服务器，并通过管理控制台方便地进行服务器的管理、服务器配置的变更和升级、数据的备份，并保证其可以正常运转并按业务需求随时进行配置的变更。\n课程四、云计算 - 云数据库管理与数据迁移\n阿里云云数据库管理与数据迁移认证课程掌握云数据库的概念，如何在云端创建数据库、将自建数据库迁移至云数据库MySQL版、数据导入导出，以及云数据库运维的常用操作。\n课程五、云计算 - 云存储：对象存储管理与安全\n阿里云云储存认证课程教你掌握安全、高可靠的云存储的使用，以及在云端存储下载文件，处理图片，以及如何保护数据的安全。\n课程六、云计算 - 超大流量网站的负载均衡\n掌握如何为网站实现负载均衡，以轻松应对超大流量和高负载。\n课程七、大数据 - MOOC网站日志分析\n本课程可以帮助学员掌握如何收集用户访问日志，如何对访问日志进行分析，如何利用大数据计算服务对数据进行处理，如何以图表化的形式展示分析后的数据。\n课程八、大数据 - 搭建企业级数据分析平台\n模拟电商场景，搭建企业级的数据分析平台，用来分析商品数据、销售数据以及用户行为等。\n课程九、大数据 - 基于LBS的热点店铺搜索\n本课程可以帮助学员掌握如何在分布式计算框架下开发一个类似于手机地图查找周边热点（POI）的功能，掌握GeoHash编码原理，以及在地理位置中的应用，并能将其应用在其他基于LBS的定位场景中。\n课程中完整的演示了整个开发步骤，学员在学完此课程之后，掌握其原理，可以在各种分布式计算框架下完成此功能的开发，比如MapReduce、Spark。\n课程十、大数据 - 基于机器学习PAI实现精细化营销\n本课程通过一个简单案例了解、掌握企业营销中常见的、也是必需的精准营销数据处理过程，了解机器学习PAI的具体应用，指导学员掌握大数据时代营销的利器---通过机器学习实现营销。\n课程十一、大数据 - 基于机器学习的客户流失预警分析\n本课程讲解了客户流失的分析方法、流程，同时详细介绍了机器学习中常用的分类算法、集成学习模型等通用技能，并使用阿里云机器学习PAI实现流失预警分析。可以帮助企业快速、准确识别流失客户，辅助制定策略进行客户关怀，达到挽留客户的目的。\n课程十二、大数据 - 使用DataV制作实时销售数据可视化大屏\n帮助非专业工程师通过图形化的界面轻松搭建专业水准的实时可视化数据大屏，以满足业务展示、业务监控、风险预警等多种业务的展示需求。\n课程十三、大数据 - 使用MaxCompute进行数据质量核查\n通过本案例，学员可了解影响数据质量的因素，出现数据质量问题的类型，掌握通过MaxCompute（DateIDE）设计数据质量监控的方法，最终独立解决常见的数据质量监控需求。\n课程十四、大数据 - 使用Quick BI制作图形化报表\n阿里云Quick BI制作图形化报表认证课程教你掌握将电商运营过程中的数据进行图表化展现，掌握通过Quick BI将数据制作成各种图形化报表的方法，同时还将掌握搭建企业级报表门户的方法。\n课程十五、大数据 - 使用时间序列分解模型预测商品销量\n使用时间序列分解模型预测商品销量教你掌握商品销量预测方法、时间序列分解以及熟悉相关产品的操作演示和项目介绍。\n课程十六、云安全 - 云平台使用安全\n阿里云云平台使用安全认证课程教你了解由传统IT到云计算架构的变迁过程、当前信息安全的现状和形势，以及在云计算时代不同系统架构中应该从哪些方面利用云平台的优势使用安全风险快速降低90%。\n课程十七、云安全 - 云上服务器安全\n阿里云云上服务器安全认证课程教你了解在互联网上提供计算功能的服务器主要面临哪些安全风险，并针对这些风险提供了切实可行的、免费的防护方案。\n课程十八、云安全 - 云上网络安全\n了解网络安全的原理和解决办法，以及应对DDoS攻击的方法和防护措施，确保云上网络的安全。\n课程十九、云安全 - 云上数据安全\n了解云上数据的安全隐患，掌握数据备份、数据加密、数据传输安全的解决方法。\n课程二十、云安全 - 云上应用安全\n了解常见的应用安全风险，SQL注入原理及防护，网站防篡改的解决方案等，确保云上应用的安全。\n课程二十一、云安全 - 云上安全管理\n了解云上的安全监控方法，学会使用监控大屏来监控安全风险，并能够自定义报警规则，确保随时掌握云上应用的安全情况。\n\n---------------------\n作者：懒散的鱼与消失的猫\n原文：https://blog.csdn.net/qwxwaty/article/details/80793370","data":"2019年02月14日 11:45:08"}
{"_id":{"$oid":"5d344e1f62f717dc0659b69e"},"title":"自然语言处理学习笔记1：自然语言处理介绍","author":"腾阳","content":"给外行能看懂的科普：这就叫自然语言处理\n如何向文科同学科普自然语言处理（NLP）？\n刘知远，NLPer\n前几年曾经马少平老师的引荐，为某科普图书写过一篇短文介绍自然语言处理。如果只是介绍NLP的概念、任务和挑战，应该可以参考这篇小文。原文如下，仅供参考。\n\n\n自然语言处理\nNatural Language Processing\n\n\n一、什么是自然语言处理\n简单地说，自然语言处理（Natural Language Processing，简称NLP）就是用计算机来处理、理解以及运用人类语言(如中文、英文等)，它属于人工智能的一个分支，是计算机科学与语言学的交叉学科，又常被称为计算语言学。由于自然语言是人类区别于其他动物的根本标志。没有语言，人类的思维也就无从谈起，所以自然语言处理体现了人工智能的最高任务与境界，也就是说，只有当计算机具备了处理自然语言的能力时，机器才算实现了真正的智能。\n\n\n从研究内容来看，自然语言处理包括语法分析、语义分析、篇章理解等。从应用角度来看，自然语言处理具有广泛的应用前景。特别是在信息时代，自然语言处理的应用包罗万象，例如：机器翻译、手写体和印刷体字符识别、语音识别及文语转换、信息检索、信息抽取与过滤、文本分类与聚类、舆情分析和观点挖掘等，它涉及与语言处理相关的数据挖掘、机器学习、知识获取、知识工程、人工智能研究和与语言计算相关的语言学研究等。\n\n\n值得一提的是，自然语言处理的兴起与机器翻译这一具体任务有着密切联系。机器翻译指的是利用计算机自动地将一种自然语言翻译为另外一种自然语言。例如自动将英文“I like Beijing Tiananmen Square”翻译为“我爱北京天安门”，或者反过来将“我爱北京天安门”翻译为“I like Beijing Tiananmen Square”。由于人工进行翻译需要训练有素的双语专家，翻译工作非常耗时耗力。更不用说需要翻译一些专业领域文献时，还需要翻译者了解该领域的基本知识。世界上有超过几千种语言，而仅联合国的工作语言就有六种之多。如果能够通过机器翻译准确地进行语言间的翻译，将大大提高人类沟通和了解的效率。\n\n\n《圣经》里有一个故事说巴比伦人想建造一座塔直通天堂。建塔的人都说着同一种语言，心意相通、齐心协力。上帝看到人类竟然敢做这种事情，就让他们的语言变得不一样。因为人们听不懂对方在讲什么，于是大家整天吵吵闹闹，无法继续建塔。后来人们把这座塔叫作巴别塔，而“巴别”的意思就是“分歧”。虽然巴别塔停建了，但一个梦想却始终萦绕在人们心中：人类什么时候才能拥有相通的语言，重建巴别塔呢？机器翻译被视为“重建巴别塔”的伟大创举。假如能够实现不同语言之间的机器翻译，我们就可以理解世界上任何人说的话，与他们进行交流和沟通，再也不必为相互不能理解而困扰。\n\n\n事实上，“人工智能”被作为一个研究问题正式提出来的时候，创始人把计算机国际象棋和机器翻译作为两个标志性的任务，认为只要国际象棋系统能够打败人类世界冠军，机器翻译系统达到人类翻译水平，就可以宣告人工智能的胜利。四十年后的1997年，IBM公司的深蓝超级计算机已经能够打败国际象棋世界冠军卡斯帕罗夫。而机器翻译到现在仍无法与人类翻译水平相比，从此可以看出自然语言处理有多么困难！\n\n\n自然语言处理兴起于美国。第二次世界大战之后，二十世纪五十年代，当电子计算机还在襁褓之中时，利用计算机处理人类语言的想法就已经出现。当时，美国希望能够利用计算机将大量俄语材料自动翻译成英语，以窥探苏联科技的最新发展。研究者从破译军事密码中得到启示，认为不同的语言只不过是对“同一语义”的不同编码而已，从而想当然地认为可以采用译码技术像破译密码一样“破译”这些语言。\n\n\n1954年1月7日，美国乔治敦大学和IBM公司合作实验成功地将超过60句俄语自动翻译成英语。虽然当时的这个机器翻译系统非常简单，仅仅包含6个语法规则和250个词，但由于媒体的广泛报道，纷纷认为这是一个巨大的进步，导致美国政府备受鼓舞，加大了对自然语言处理研究的投资。实验完成者也当即自信地撰文称，在三到五年之内就能够完全解决从一种语言到另一种语言的自动翻译问题。他们认为只要制定好各种翻译规则，通过大量规则的堆砌就能够完美地实现语言间的自动翻译。\n\n\n然而，事实是理解人类语言远比破译密码要复杂得多，因此研究进展非常缓慢。1966年的一份研究报告总结发现，经过十年之久的研究，结果远远未能达到预期，因此支持资金急剧下降，使自然语言处理（特别是机器翻译）的研究陷入长达二十年的低潮。直到二十世纪八十年代，随着电子计算机的计算能力的飞速提高和制造成本的大幅下降，研究者又开始重新关注自然语言处理这个极富挑战的研究领域。三十年沧海桑田，此时研究者已经认识到简单的语言规则的堆砌无法实现对人类语言的真正理解。研究发现，通过对大量的文本数据的自动学习和统计，能够更好地解决自然语言处理问题，如语言的自动翻译。这一思想被称为自然语言处理的统计学习模型，至今方兴未艾。\n\n\n那么，自然语言处理到底存在哪些主要困难或挑战，吸引那么多研究者几十年如一日孜孜不倦地探索解决之道呢？\n\n\n二、自然语言处理的主要困难\n自然语言处理的困难可以罗列出来很多，不过关键在于消除歧义问题，如词法分析、句法分析、语义分析等过程中存在的歧义问题，简称为消歧。而正确的消歧需要大量的知识，包括语言学知识（如词法、句法、语义、上下文等）和世界知识（与语言无关）。这带来自然语言处理的两个主要困难。\n\n\n首先，语言中充满了大量的歧义，这主要体现在词法、句法及语义三个层次上。歧义的产生是由于自然语言所描述的对象――人类活动非常复杂，而语言的词汇和句法规则又是有限的，这就造成同一种语言形式可能具有多种含义。\n\n\n例如单词定界问题是属于词法层面的消歧任务。在口语中，词与词之间通常是连贯说出来的。在书面语中，中文等语言也没有词与词之间的边界。由于单词是承载语义的最小单元，要解决自然语言处理，单词的边界界定问题首当其冲。特别是中文文本通常由连续的字序列组成，词与词之间缺少天然的分隔符，因此中文信息处理比英文等西方语言多一步工序，即确定词的边界，我们称为“中文自动分词”任务。通俗的说就是要由计算机在词与词之间自动加上分隔符，从而将中文文本切分为独立的单词。例如一个句子“今天天气晴朗”的带有分隔符的切分文本是“今天|天气|晴朗”。中文自动分词处于中文自然语言处理的底层，是公认的中文信息处理的第一道工序，扮演着重要的角色，主要存在新词发现和歧义切分等问题。我们注意到：正确的单词切分取决于对文本语义的正确理解，而单词切分又是理解语言的最初的一道工序。这样的一个“鸡生蛋、蛋生鸡”的问题自然成了（中文）自然语言处理的第一条拦路虎。\n\n\n其他级别的语言单位也存在着各种歧义问题。例如在短语级别上，“进口彩电”可以理解为动宾关系（从国外进口了一批彩电），也可以理解为偏正关系（从国外进口的彩电）。又如在句子级别上，“做手术的是她的父亲”可以理解为她父亲生病了需要做手术，也可以理解为她父亲是医生，帮别人做手术。总之，同样一个单词、短语或者句子有多种可能的理解，表示多种可能的语义。如果不能解决好各级语言单位的歧义问题，我们就无法正确理解语言要表达的意思。\n\n\n另外一个方面，消除歧义所需要的知识在获取、表达以及运用上存在困难。由于语言处理的复杂性，合适的语言处理方法和模型难以设计。\n\n\n例如上下文知识的获取问题。在试图理解一句话的时候，即使不存在歧义问题，我们也往往需要考虑上下文的影响。所谓的“上下文”指的是当前所说这句话所处的语言环境，例如说话人所处的环境，或者是这句话的前几句话或者后几句话，等等。假如当前这句话中存在指代词的时候，我们需要通过这句话前面的句子来推断这个指代词是指的什么。我们以“小明欺负小亮，因此我批评了他”为例。在其中的第二句话中的“他”是指代“小明”还是“小亮”呢？要正确理解这句话，我们就要理解上句话“小明欺负小亮”意味着“小明”做得不对，因此第二句中的“他”应当指代的是“小明”。由于上下文对于当前句子的暗示形式是多种多样的，因此如何考虑上下文影响问题是自然语言处理中的主要困难之一。\n\n\n再如背景知识问题。 正确理解人类语言还要有足够的背景知识。举一个简单的例子，在机器翻译研究的初期，人们经常举一个例子来说明机器翻译任务的艰巨性。在英语中“The spirit is willing but the flesh is weak.”，意思是“心有余而力不足”。但是当时的某个机器翻译系统将这句英文翻译到俄语，然后再翻译回英语的时候，却变成了“The Voltka is strong but the meat is rotten.”，意思是“伏特加酒是浓的，但肉却腐烂了”。从字面意义上看，“spirit”（烈性酒）与“Voltka”（伏特加）对译似无问题，而“flesh”和“meat”也都有肉的意思。那么这两句话在意义上为什么会南辕北辙呢？关键的问题就在于在翻译的过程中，机器翻译系统对于英语成语并无了解，仅仅是从字面上进行翻译，结果自然失之毫厘，差之千里。\n\n\n从上面的两个方面的主要困难，我们看到自然语言处理这个难题的根源就是人类语言的复杂性和语言描述的外部世界的复杂性。人类语言承担着人类表达情感、交流思想、传播知识等重要功能，因此需要具备强大的灵活性和表达能力，而理解语言所需要的知识又是无止境的。那么目前人们是如何尝试进行自然语言处理的呢？\n\n\n三、自然语言处理的发展趋势\n目前，人们主要通过两种思路来进行自然语言处理，一种是基于规则的理性主义，另外一种是基于统计的经验主义。理性主义方法认为，人类语言主要是由语言规则来产生和描述的，因此只要能够用适当的形式将人类语言规则表示出来，就能够理解人类语言，并实现语言之间的翻译等各种自然语言处理任务。而经验主义方法则认为，从语言数据中获取语言统计知识，有效建立语言的统计模型。因此只要能够有足够多的用于统计的语言数据，就能够理解人类语言。然而，当面对现实世界充满模糊与不确定性时，这两种方法都面临着各自无法解决的问题。例如，人类语言虽然有一定的规则，但是在真实使用中往往伴随大量的噪音和不规范性。理性主义方法的一大弱点就是鲁棒性差，只要与规则稍有偏离便无法处理。而对于经验主义方法而言，又不能无限地获取语言数据进行统计学习，因此也不能够完美地理解人类语言。二十世纪八十年代以来的趋势就是，基于语言规则的理性主义方法不断受到质疑，大规模语言数据处理成为目前和未来一段时期内自然语言处理的主要研究目标。统计学习方法越来越受到重视，自然语言处理中越来越多地使用机器自动学习的方法来获取语言知识。\n\n\n迈进二十一世纪，我们已经进入了以互联网为主要标志的海量信息时代，这些海量信息大部分是以自然语言表示的。一方面，海量信息也为计算机学习人类语言提供了更多的“素材”，另一方面，这也为自然语言处理提供了更加宽广的应用舞台。例如，作为自然语言处理的重要应用，搜索引擎逐渐成为人们获取信息的重要工具，涌现出以百度、谷歌等为代表的搜索引擎巨头；机器翻译也从实验室走入寻常百姓家，谷歌、百度等公司都提供了基于海量网络数据的机器翻译和辅助翻译工具；基于自然语言处理的中文（输入法如搜狗、微软、谷歌等输入法）成为计算机用户的必备工具；带有语音识别的计算机和手机也正大行其道，协助用户更有效地工作学习。总之，随着互联网的普及和海量信息的涌现，自然语言处理正在人们的日常生活中扮演着越来越重要的作用。\n\n\n然而，我们同时面临着一个严峻事实，那就是如何有效利用海量信息已成为制约信息技术发展的一个全局性瓶颈问题。自然语言处理无可避免地成为信息科学技术中长期发展的一个新的战略制高点。同时，人们逐渐意识到，单纯依靠统计方法已经无法快速有效地从海量数据中学习语言知识，只有同时充分发挥基于规则的理性主义方法和基于统计的经验主义方法的各自优势，两者互相补充，才能够更好、更快地进行自然语言处理。\n\n\n自然语言处理作为一个年龄尚不足一个世纪的新兴学科，正在进行着突飞猛进的发展。回顾自然语言处理的发展历程，并不是一帆风顺，有过低谷，也有过高潮。而现在我们正面临着新的挑战和机遇。例如，目前网络搜索引擎基本上还停留在关键词匹配，缺乏深层次的自然语言处理和理解。语音识别、文字识别、问答系统、机器翻译等目前也只能达到很基本的水平。路漫漫其修远兮，自然语言处理作为一个高度交叉的新兴学科，不论是探究自然本质还是付诸实际应用，在将来必定会有令人期待的惊喜和异常快速的发展。\n\n\n参考文献\n[1] 张钹. 自然语言处理的计算模型. 中文信息学报, 2007, 21(3):3-7.\n[2] 冯志伟. 《统计自然语言处理》序言. 1版. 北京: 清华大学出版社, 2008.\n[3] 孙茂松. 语言计算:信息科学技术中长期发展的战略制高点. 语言文字应用, 2005, 3:38-40.\n\n\n查看知乎原文（12 条讨论）\n扫描二维码下载知乎日报\n支持 iOS 和 Android\n二维码下载知乎日报\n知乎网 · © 2018 知乎","data":"2018年05月19日 21:42:49"}
{"_id":{"$oid":"5d344e4062f717dc0659b6a2"},"title":"干货 | 自然语言处理入门资料推荐","author":"机器学习算法与Python学习","content":"微信公众号\n关键字全网搜索最新排名\n【机器学习算法】：排名第一\n【机器学习】：排名第一\n【Python】：排名第三\n【算法】：排名第四\n源 | AI深入浅出\n最近几个月小编遨游在税务行业的智能问答调研和开发中，里面涉及到了很多的自然语言处理NLP的功能点。虽然接触NLP也有近两年的时间了，现在真正要应用到问答中，避免不了还是需要再重新熟识并深入研究理解。\n\n\n下面是与NLP相关的一些书籍推荐、课件推荐和开源工具推荐。\n\n\n主要是记录下入门的资料，由于资料的存储位置没有做规整，所以本文没有附带资源下载链接。如果有同学需要其中的资源，可以在公众号上给我留言，回头我把资源链接反馈给您。\n\n\n\n\n部分开源工具和语料资源\n\n1、NLTK官方提供的语料库资源列表\n2、OpenNLP上的开源自然语言处理工具列表\n3、斯坦福大学自然语言处理组维护的“统计自然语言处理及基于语料库的计算语言学资源列表”\n4、LDC上免费的中文信息处理资源\n\n\n\n\n课件\n1、哈工大刘挺老师的“统计自然语言处理”课件；\n2、哈工大刘秉权老师的“自然语言处理”课件；\n3、中科院计算所刘群老师的“计算语言学讲义“课件；\n4、中科院自动化所宗成庆老师的“自然语言理解”课件；\n5、北大常宝宝老师的“计算语言学”课件；\n6、北大詹卫东老师的“中文信息处理基础”的课件及相关代码；\n7、MIT大牛Michael Collins的“Machine Learning Approaches for Natural Language Processing(面向自然语言处理的机器学习方法)”课件；\n8、Michael Collins的“Machine Learning （机器学习）”课件；\n9、SMT牛人Philipp Koehn “Advanced Natural Language Processing（高级自然语言处理）”课件；\n10、Philipp Koehn “Empirical Methods in Natural Language Processing”课件；\n11、Philipp Koehn“Machine Translation（机器翻译）”课件。\n\n\n书籍\n1、《自然语言处理综论》英文版第二版\n2、《统计自然语言处理基础》英文版\n3、《用Python进行自然语言处理》，NLTK配套书\n4、《Learning Python第三版》，Python入门经典书籍，详细而不厌其烦\n5、《自然语言处理中的模式识别》\n6、《EM算法及其扩展》\n7、《统计学习基础》\n8、《自然语言理解》英文版（似乎只有前9章）\n9、《Fundamentals of Speech Recognition》，质量不太好，不过第6章关于HMM的部分比较详细，作者之一便是Lawrence Rabiner；\n10、概率统计经典入门书：\n《概率论及其应用》（英文版，威廉*费勒著） 第一卷　　第二卷　　DjVuLibre阅读器（阅读前两卷书需要）\n11、一本利用Perl和Prolog进行自然语言处理的介绍书籍：《An Introduction to Language Processing with Perl and Prolog》\n12、国外机器学习书籍之：\n1) “Programming Collective Intelligence“，中文译名《集体智慧编程》，机器学习\u0026数据挖掘领域”近年出的入门好书，培养兴趣是最重要的一环，一上来看大部头很容易被吓走的”\n2) “Machine Learning“,机器学习领域无可争议的经典书籍，下载完毕将后缀改为pdf即可。\n豆瓣评论 by 王宁）：老书，牛人。现在看来内容并不算深，很多章节有点到为止的感觉，但是很适合新手（当然，不能”新”到连算法和概率都不知道）入门。比如决策树部分就很精彩，并且这几年没有特别大的进展，所以并不过时。另外，这本书算是对97年前数十年机器学习工作的大综述，参考文献列表极有价值。国内有翻译和影印版，不知道绝版否。\n3) “Introduction to Machine Learning”\n\n\n13、国外数据挖掘书籍之：\n1) “Data.Mining.Concepts.and.Techniques.2nd“，数据挖掘经典书籍。华裔科学家写的书，相当深入浅出。\n2) Data Mining:Practical Machine Learning Tools and Techniques\n3) Beautiful Data: The Stories Behind Elegant Data Solutions（ Toby Segaran, Jeff Hammerbacher）\n\n\n14、国外模式识别书籍之：\n1）“Pattern Recognition”\n2）“Pattern Recongnition Technologies and Applications”\n3）“An Introduction to Pattern Recognition”\n4）“Introduction to Statistical Pattern Recognition”\n5）“Statistical Pattern Recognition 2nd Edition”\n6）“Supervised and Unsupervised Pattern Recognition”\n7）“Support Vector Machines for Pattern Classification”\n\n\n15、国外人工智能书籍之：\n1）Artificial Intelligence: A Modern Approach (2nd Edition) 人工智能领域无争议的经典。\n2）“Paradigms of Artificial Intelligence Programming: Case Studies in Common LISP”\n\n\n16、其他相关书籍：\n1）Programming the Semantic Web，Toby Segaran , Colin Evans, Jamie Taylor\n2）Learning.Python第四版，英文\n\n\n加入微信机器学习交流群\n请添加微信：guodongwe1991\n备注姓名-单位-研究方向\n\n\n广告、商业合作\n请添加微信：guodongwe1991\n（备注：商务合作）","data":"2018年01月02日 00:00:00"}
{"_id":{"$oid":"5d344e5b62f717dc0659b6ac"},"title":"自然语言处理","author":"cnvlsi","content":"这里写自定义目录标题\n欢迎使用Markdown编辑器\n新的改变\n功能快捷键\n合理的创建标题，有助于目录的生成\n如何改变文本的样式\n插入链接与图片\n如何插入一段漂亮的代码片\n生成一个适合你的列表\n创建一个表格\n设定内容居中、居左、居右\nSmartyPants\n创建一个自定义列表\n如何创建一个注脚\n注释也是必不可少的\nKaTeX数学公式\n新的甘特图功能，丰富你的文章\nUML 图表\nFLowchart流程图\n导出与导入\n导出\n导入\n词表示是自然语言处理的基础，一个好的词向量在很大程度上决定了后续任务的上限。本文是我最近学习该部分内容的笔记，主要参照的是基于神经网络的词和文档语义向量表示方法研究一文，穿插了一些个人理解。内容较多，错误难免，请拍砖~\n分布表示(Distributional Representation)假说：上下文相似的词，其语义也相似。根据建模方式的不同，主要分为三类：基于矩阵的分布表示、基于聚类的分布表示和基于神经网络的分布表示。\n尽管不同的分布表示方法使用了不同的技术手段获取词表示，但由于这些方法均基于分布假说，它们的核心思想也都由两部分组成：\n选择一种方式描述上下文\n选择一种模型刻画目标词与其上下文之间的关系。\n基于矩阵的分布表示\n基于矩阵的分布表示通常又称为分布语义模型（distributional semantic models）。这类方法需要构建一个“词-上下文”矩阵，从矩阵中获取词的表示。在“词-上下文”矩阵中，每行对应一个词，每列表示一种不同的上下文，矩阵中的每个元素对应相关词和上下文的共现次数。在这种表示下，矩阵中的一行，就成为了对应词的表示，这种表示描述了该词的上下文的分布。由于分布假说认为上下文相似的词，其语义也相似，因此在这种表示下，两个词的语义相似度可以直接转化为两个向量的空间距离。这类方法具体可以分为三个步骤：\n选取上下文。最常见的有三种方法：第一种，将词所在的文档作为上下文，形成“词-文档”矩阵（term-document matrix）；第二种，将词附近上下文中的各个词（如上下文窗口中的5个词）作为上下文，形成“词-词”矩阵；第三种，将词附近上下文各词组成的n-gram作为上下文 。在这三种方法中，“词-文档”矩阵非常稀疏，而“词-词”矩阵相对较为稠密，效果一般好于前者。“词-n-gram”相对“词-词”矩阵保留了词序信息，建模更精确，但由于比前者更稀疏，实际效果不一定能超越前者。\n确定矩阵中各元素的值。“词-上下文”共现矩阵根据其定义，里面各元素的值应为词与对应的上下文的共现次数。然而直接使用原始共现次数作为矩阵的值在大多数情况下效果并不好，因此研究人员提出了多种加权和平滑方法，最常用的有tf-idf、PMI 和直接取log。\n矩阵分解（可选）。在原始的“词-上下文”矩阵中，每个词表示为一个非常高维（维度是不同上下文的总个数）且非常稀疏的向量，使用降维技术可以将这一高维稀疏向量压缩成低维稠密向量。降维技术可以减少噪声带来的影响，但也可能损失一部分信息。最常用的分解技术包括奇异值分解（SVD）、非负矩阵分解（NMF）、典型关联分析（Canonical Correlation Analysis，CCA）、Hellinger PCA（HPCA）。\n著名的 Global Vector模型（GloVe）就是基于矩阵的分布表示\n基于聚类的分布表示（分布聚类）\n基于聚类的分布表示又称作分布聚类（distributional clustering），这类方法通过聚类手段构建词与其上下文之间的关系。其中最经典的方法是布朗聚类（Brown clustering）。布朗聚类是一种层级聚类方法，聚类结果为每个词的多层类别体系。因此可以根据两个词的公共类别判断这两个词的语义相似度。\n这个方法似乎没有太多主流的应用，所以我没有做深入研究\n基于神经网络的分布表示（词向量）\n基于神经网络的分布表示一般称为词向量、词嵌入（word embedding）或分布式表示（distributed representation）。神经网络词向量表示技术通过神经网络技术对上下文，以及上下文与目标词之间的关系进行建模。由于神经网络较为灵活，这类方法的最大优势在于可以表示复杂的上下文。在前面基于矩阵的分布表示方法中，最常用的上下文是词。如果使用包含词序信息的n-gram作为上下文，当n增加时，n-gram的总数会呈指数级增长，此时会遇到维数灾难问题。而神经网络在表示n-gram时，可以通过一些组合方式对n个词进行组合，参数个数仅以线性速度增长。有了这一优势，神经网络模型可以对更复杂的上下文进行建模，在词向量中包含更丰富的语义信息。\n神经网络词向量模型与其它分布表示方法一样，均基于分布假说，核心依然是上下文的表示以及上下文与目标词之间的关系的建模。构建上下文与目标词之间的关系，最自然的一种思路就是使用语言模型。\n语言模型\n语言模型可以对一段文本的概率进行估计，对信息检索、机器翻译、语音识别等任务有着重要的作用。形式化讲，统计语言模型的作用是为一个长度为的字符串确定一个概率分布，表示其存在的可能性，其中到依次表示这段文本中的各个词。一般在实际求解过程中，通常采用下式计算其概率值：\n\n在实践中，如果文本的长度较长，上述公式右部的估算会非常困难。因此，研究者们提出使用一个简化模型：n元模型（n-gram model）。在n元模型中估算条件概率时，距离大于等于n的上文词会被忽略，也就是对上述条件概率做了以下近似：\n\n在元模型中，传统的方法一般采用频率计数的比例来估算元条件概率：\n\n其中，表示文本序列在语料中出现的次数。\n为了更好地保留词序信息，构建更有效的语言模型，我们希望在元模型中选用更大的。但是，当较大时，长度为序列出现的次数就会非常少，在按照上述公式估计元条件概率时，就会遇到数据稀疏问题，导致估算结果不准确。因此，一般在百万词级别的语料中，三元模型是比较常用的选择，同时也需要配合相应的平滑算法，进一步降低数据稀疏带来的影响。\n为了更好地解决元模型估算概率时遇到的数据稀疏问题，神经网络语言模型应运而生。\n神经网络语言模型（NNLM）\n神经网络语言模型（Neural Network Language Model ，NNLM）在学习语言模型的同时，也能得到词向量。\nNNLM 同样也是对n 元语言模型进行建模，估算的值。但与传统方法不同的是，NNLM 不通过计数的方法对元条件概率进行估计，而是直接通过一个神经网络结构，对其进行建模求解。下图展示了NNLM 的基本结构。\n\nNNLM模型\n具体而言，对语料中一段长度为的序列，元语言模型需要最大化以下概率：\n\n其中， 为需要通过语言模型预测的词（目标词）。对于整个模型而言，输入为条件部分的整个词序列：，输出为目标词的分布。而神经网络的目标就是要让输出中，(目标词)的概率最大。\n神经网络语言模型采用普通的三层前馈神经网络结构，其中第一层为输入层。Bengio提出使用各词的词向量作为输入以解决数据稀疏问题，因此输入层为词的词向量的顺序拼接：\n\n当输入层完成对上文的表示之后，模型将其送入剩下两层神经网络，依次得到隐藏层和输出层:\n\n\n其中,。表示词汇表的大小，表示词向量的维度，是隐层的维度。矩阵表示从输入层到输出层的直连边权重矩阵。如果使用该直连边，可以减少一半的迭代次数；但如果没有直连边，可以生成性能更好的语言模型。因此在后续工作中，很少有使用输入层到输出层直连边的工作。\n输出层一共有个元素，，依次对应下一个词为词表中某个词的可能性。这里使用softmax函数，将其转化为对应的概率。\n在NNLM模型中，词向量出现在两个地方，一个是输入层的词向量，另一是隐层的权重，的维度是，这可以看做是个维的行向量，其中的每一个向量，均可以看做某个词在模型中的另一个词向量，记为。在不考虑 的情况下，每个词在模型中有两套词向量。通常在实际工作中只是用第一个作为词向量。\n将展开，得到：\n\n被称为能量函数。\nlog双线性语言模型（LBL）\n2007 年，Mnih和Hinton在神经网络语言模型（NNLM）的基础上提出了log双线性语言模型（Log-Bilinear Language Model，LBL）。LBL模型的能量函数为：\n\nLBL模型的能量函数与NNLM的能量函数主要有两个区别。一、LBL 模型中，没有非线性的激活函数tanh，而由于NNLM 是非线性的神经网络结构，激活函数必不可少；二、LBL 模型中，只有一份词向量e，也就是说，无论一个词是作为上下文，还是作为目标词，使用的是同一份词向量。其中第二点（只有一份词向量），只在原版的LBL模型中存在，后续的改进工作均不包含这一特点。\n循环神经网络语言模型（RNNLM）\n循环神经网络语言模型（Recurrent Neural Network based Language Model，RNNLM）则直接对进行建模（注意不是）\n该模型就是把NNLM隐层变成RNN，每一个隐层包含此前所有上文信息\nRNNLM里面最厉害的就属ELMo了。该模型利用多层双向LSTM的加权和来表示词向量，其中权重可根据具体任务动态调节。\nC\u0026W模型\n与基于语言模型的词向量生成方法不同，C\u0026W以直接生成词向量为目标。\n\n\nC\u0026W模型\nC\u0026W模型没有去求解，而是直接对n元短语打分。对于语料中出现过的元短语，模型会对其打高分；而对于语料中没有出现的随机短语，模型会对其打低分。通过这种方式，C\u0026W 模型可以更直接地学习得到符合分布假说的词向量。\n具体而言，对于整个语料，C\u0026W模型需要最小化：\n\n其中，为从语料中选出的一个元短语，为序列中的中间词，也是目标词，即；表示的上下文；为字典中的某一个词。正样本来自语料，而负样本则是将正样本序列中的中间词替换成其它词。\n即:\n\nC\u0026W模型与NNLM相比，主要的不同点在于C\u0026W模型将目标词放到了输入层，同时输出层也从语言模型的个节点变为一个节点，这个节点的数值表示对这组n元短语的打分。这个区别使得C\u0026W模型成为神经网络词向量模型中最为特殊的一个，其它模型的目标词均在输出层，只有C\u0026W模型的目标词在输入层。\nCBOW模型\nCBOW模型\nCBOW模型的结构如上图，该模型一方面根据C\u0026W模型的经验，使用一段文本的中间词作为目标词；另一方面，又以NNLM作为蓝本，并在其基础上做了两个简化。一、CBOW没有隐藏层，去掉隐藏层之后，模型从神经网络结构直接转化为log线性结构，与Logistic回归一致。log线性结构比三层神经网络结构少了一个矩阵运算，大幅度地提升了模型的训练速度。二、CBOW去除了上下文各词的词序信息，使用上下文各词词向量的平均值，代替神经网络语言模型使用的上文各词词向量的拼接。形式化地，CBOW模型对于一段训练样本，输入为：\n\n由于没有隐藏层，CBOW模型的输入层直接就是上下文的表示。CBOW 模型根据上下文的表示，直接对目标词进行预测：\n\n对于整个语料而言，与神经网络语言模型类似，CBOW的优化目标为最大化：\n\nSkip-gram模型\nSkip-gram模型\nSkip-gram模型的结构如上图，与CBOW模型一样，Skip-gram模型中也没有隐藏层。和CBOW模型不同的是，Skip-gram模型每次从目标词的上下文中选择一个词，将其词向量作为模型的输入，也就是上下文的表示。Skip-gram模型同样通过上下文预测目标词，对于整个语料的优化目标为最大化：\n\n其中，\n\n\u003c/div\u003e\n欢迎使用Markdown编辑器\n你好！ 这是你第一次使用 Markdown编辑器 所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。\n新的改变\n我们对Markdown编辑器进行了一些功能拓展与语法支持，除了标准的Markdown编辑器功能，我们增加了如下几点新功能，帮助你用它写博客：\n全新的界面设计 ，将会带来全新的写作体验；\n在创作中心设置你喜爱的代码高亮样式，Markdown 将代码片显示选择的高亮样式 进行展示；\n增加了 图片拖拽 功能，你可以将本地的图片直接拖拽到编辑区域直接展示；\n全新的 KaTeX数学公式 语法；\n增加了支持甘特图的mermaid语法1 功能；\n增加了 多屏幕编辑 Markdown文章功能；\n增加了 焦点写作模式、预览模式、简洁写作模式、左右区域同步滚轮设置 等功能，功能按钮位于编辑区域与预览区域中间；\n增加了 检查列表 功能。\n功能快捷键\n撤销：Ctrl/Command + Z\n重做：Ctrl/Command + Y\n加粗：Ctrl/Command + B\n斜体：Ctrl/Command + I\n标题：Ctrl/Command + Shift + H\n无序列表：Ctrl/Command + Shift + U\n有序列表：Ctrl/Command + Shift + O\n检查列表：Ctrl/Command + Shift + C\n插入代码：Ctrl/Command + Shift + K\n插入链接：Ctrl/Command + Shift + L\n插入图片：Ctrl/Command + Shift + G\n合理的创建标题，有助于目录的生成\n直接输入1次#，并按下space后，将生成1级标题。\n输入2次#，并按下space后，将生成2级标题。\n以此类推，我们支持6级标题。有助于使用TOC语法后生成一个完美的目录。\n如何改变文本的样式\n强调文本 强调文本\n加粗文本 加粗文本\n标记文本\n删除文本\n引用文本\nH2O is是液体。\n210 运算结果是 1024.\n插入链接与图片\n链接: link.\n图片:\n带尺寸的图片:\n居中的图片:\n居中并且带尺寸的图片:\n当然，我们为了让用户更加便捷，我们增加了图片拖拽功能。\n如何插入一段漂亮的代码片\n去博客设置页面，选择一款你喜欢的代码片高亮样式，下面展示同样高亮的 代码片.\n// An highlighted block var foo = 'bar';\n生成一个适合你的列表\n项目\n项目\n项目\n项目1\n项目2\n项目3\n计划任务\n完成任务\n创建一个表格\n一个简单的表格是这么创建的：\n项目\nValue\n电脑\n$1600\n手机\n$12\n导管\n$1\n设定内容居中、居左、居右\n使用:---------:居中\n使用:----------居左\n使用----------:居右\n第一列\n第二列\n第三列\n第一列文本居中\n第二列文本居右\n第三列文本居左\nSmartyPants\nSmartyPants将ASCII标点字符转换为“智能”印刷标点HTML实体。例如：\nTYPE\nASCII\nHTML\nSingle backticks\n'Isn't this fun?'\n‘Isn’t this fun?’\nQuotes\n\"Isn't this fun?\"\n“Isn’t this fun?”\nDashes\n-- is en-dash, --- is em-dash\n– is en-dash, — is em-dash\n创建一个自定义列表\nMarkdown\nText-to-HTML conversion tool\nAuthors\nJohn\nLuke\n如何创建一个注脚\n一个具有注脚的文本。2\n注释也是必不可少的\nMarkdown将文本转换为 HTML。\nKaTeX数学公式\n您可以使用渲染LaTeX数学表达式 KaTeX:\nGamma公式展示\nΓ\n(\nn\n)\n=\n(\nn\n−\n1\n)\n!\n∀\nn\n∈\nN\n\\Gamma(n) = (n-1)!\\quad\\forall n\\in\\mathbb N\nΓ(n)=(n−1)!∀n∈N 是通过欧拉积分\nΓ\n(\nz\n)\n=\n∫\n0\n∞\nt\nz\n−\n1\ne\n−\nt\nd\nt\n\u0026ThinSpace;\n.\n\\Gamma(z) = \\int_0^\\infty t^{z-1}e^{-t}dt\\,.\nΓ(z)=∫0∞ tz−1e−tdt.\n你可以找到更多关于的信息 LaTeX 数学表达式here.\n新的甘特图功能，丰富你的文章\nMon 06\nMon 13\nMon 20\n已完成\n进行中\n计划一\n计划二\n现有任务\nAdding GANTT diagram functionality to mermaid\n关于 甘特图 语法，参考 这儿,\nUML 图表\n可以使用UML图表进行渲染。 Mermaid. 例如下面产生的一个序列图：:\n张三\n李四\n王五\n你好！李四, 最近怎么样?\n你最近怎么样，王五？\n我很好，谢谢!\n我很好，谢谢!\n李四想了很长时间,\n文字太长了\n不适合放在一行.\n打量着王五...\n很好... 王五, 你怎么样?\n张三\n李四\n王五\n这将产生一个流程图。:\n链接\n长方形\n圆\n圆角长方形\n菱形\n关于 Mermaid 语法，参考 这儿,\nFLowchart流程图\n我们依旧会支持flowchart的流程图：\nCreated with Raphaël 2.2.0\n开始\n我的操作\n确认？\n结束\nyes\nno\n关于 Flowchart流程图 语法，参考 这儿.\n导出与导入\n导出\n如果你想尝试使用此编辑器, 你可以在此篇文章任意编辑。当你完成了一篇文章的写作, 在上方工具栏找到 文章导出 ，生成一个.md文件或者.html文件进行本地保存。\n导入\n如果你想加载一篇你写过的.md文件或者.html文件，在上方工具栏可以选择导入功能进行对应扩展名的文件导入，\n继续你的创作。\nmermaid语法说明 ↩︎\n注脚的解释 ↩︎","data":"2019年03月05日 21:27:32","date":"2017年12月12日 20:52:35"}
{"_id":{"$oid":"5d344e7962f717dc0659b6b0"},"title":"自然语言处理之命名实体识别-tanfordcorenlp-NER(一)","author":"IT界的小小小学生","content":"转载请注明出处：https://blog.csdn.net/HHTNAN\n简介\nCoreNLP 项目是Stanford开发的一套开源的NLP系统。包括tokenize, pos , parse 等功能，与SpaCy类似。SpaCy号称是目前最快的NLP系统， 并且提供现成的python接口，但不足之处就是目前还不支持中文处理， CoreNLP则包含了中文模型，可以直接用于处理中文， 但CoreNLP使用Java开发，python调用稍微麻烦一点。\nStanford CoreNLP是一个比较厉害的自然语言处理工具，很多模型都是基于深度学习方法训练得到的。\n先附上其官网链接：\nhttps://stanfordnlp.github.io/CoreNLP/index.html\nhttps://nlp.stanford.edu/nlp/javadoc/javanlp/\nhttps://github.com/stanfordnlp/CoreNLP\n安装Installation\nwindows 10 环境\n安装依赖\n1.首先需要配置JDK，安装JDK 1.8及以上版本。。\n2.之后到 https://stanfordnlp.github.io/CoreNLP/history.html 下载对应的jar包。\n将压缩包解压得到目录，再将语言的jar包放到这个目录下即可。\n3.下载Stanford CoreNLP文件：http://stanfordnlp.github.io/CoreNLP/download.html\n\n4.下载中文模型jar包（注意一定要下载这个文件，否则它默认是按英文来处理的）。\n\n5.接下来py安装 stanfordcorenlp\n\n6. 解压配置\n下载完成后两个文件加起来1G+下载完成后两个文件加起来1G+\n\n把解压后的Stanford CoreNLP文件夹下载的Stanford-chinese-corenlp-2018—models.jar放在同一目录下(注意：一定要在同一目录下，否则执行会报错)\n\n7. 在Python中引用模型，执行下面语句：\nfrom stanfordcorenlp import StanfordCoreNLP\nnlp=StanfordCoreNLP(r’D:\\D:\\stanford_nlp\\stanford-corenlp-full-2018-10-05’,lang=‘zh’)\n应用\n#encoding=\"utf-8\" from stanfordcorenlp import StanfordCoreNLP import os if os.path.exists('D:\\\\stanford_nlp\\\\stanford-corenlp-full-2018-10-05'): print(\"corenlp exists\") else: print(\"corenlp not exists\") nlp=StanfordCoreNLP('D:\\\\stanford_nlp\\\\stanford-corenlp-full-2018-10-05',lang='zh') sentence = '王明是清华大学的一个研究生' print(nlp.ner(sentence))\n输出：\ncorenlp exists\n[(‘王明’, ‘PERSON’), (‘是’, ‘O’), (‘清华’, ‘ORGANIZATION’), (‘大学’, ‘ORGANIZATION’), (‘的’, ‘O’), (‘一’, ‘NUMBER’), (‘个’, ‘O’), (‘研究生’, ‘O’)]\n三、查看词性标注\n在浏览器中访问：http://localhost:9000/\n\n转载请注明出处：https://blog.csdn.net/HHTNAN","data":"2018年12月18日 11:27:40"}
{"_id":{"$oid":"5d344e9c62f717dc0659b6ba"},"title":"国内外有哪些自然语言处理的团队？","author":"搬砖小工053","content":"清华大学自然语言处理与社会人文计算实验室\n清华大学智能技术与系统国家重点实验室信息检索组\n北京大学计算语言学教育部重点实验室\n北京大学计算机科学技术研究所语言计算与互联网挖掘研究室\n哈工大社会计算与信息检索研究中心\n哈工大机器智能与翻译研究室\n哈尔滨工业大学智能技术与自然语言处理实验室\n中科院计算所自然语言处理研究组\n中科院自动化研究所语音语言技术研究组\n南京大学自然语言处理研究组\n东北大学自然语言处理实验室\n厦门大学智能科学与技术系自然语言处理实验室\n苏州大学自然语言处理实验室\n郑州大学自然语言处理实验室\n中科院自动化所 模式识别实验室（NLPR）\nHuawei Noah’s Ark Lab\nHuman Language Technology Center at Hong Kong University of Science \u0026 Technology\nNUS Natural Language Processing Group\nThe Stanford Natural Language Processing Group\nThe Berkeley NLP Group\nNatural Language Processing research at Columbia University\nNatural Language and Information Processing Research Group at University of Cambridge\nSpeech Research Group at University of Cambridge\nThe Language Technologies Institute (LTI) at Carnegie Mellon University\nThe Computational Linguistics Group at Oxford University\nHuman Language Technology and Pattern Recognition Group at the RWTH Aachen\nAlgorithms for Computational Linguistics at City University of New York\nRPI Blender Lab\nThe Natural Language Group at USC/ISI\nNatural Language Processing Group at University of Notre Dame\nArtificial Intelligence Research Group at Harvard\nNatural Language Processing - Research at Google\nThe Redmond-based Natural Language Processing group\nComputational Linguistics and Information Processing at Maryland\nLanguage and Speech Processing at Johns Hopkins University\nHuman Language Technology Center of Excellence at Johns Hopkins University\nStatistical Machine Translation Group at the University of Edinburgh\nUniversity of Sheffield NLP Group\nThe CNGL Centre for Global Intelligent Content\nCornell NLP group\nNatural Language Processing (NLP) group at University Of Washington\nNLP @ Illinois\n搜狗公司\n百度公司：现任副总裁王海峰先生是自然语言处理领域世界上影响力最大、也最具活力的国际学术组织ACL（Association for Computational Linguistics）50多年历史上唯一的华人主席。\n科大讯飞：国内专业做中文语音、文字产品研发的企业，是目前国内最大的智能语音技术提供商。\n参考\n知乎\nwarrioR_wx CSDN博客","data":"2016年09月06日 22:03:25"}
{"_id":{"$oid":"5d344eb162f717dc0659b6be"},"title":"深度学习与自然语言处理（二）","author":"wustjk124","content":"目录\n1.1 自然语言处理的挑战\n1.2 神经网络和深度学习\n1.3 自然语言处理中的深度学习\n1.1 自然语言处理的挑战\n自然语言处理是一个设计输入与输出为非结构化自然语言数据的方法和算法的研究领域。人类语言有很强的歧义性（如句子“I ate pizza with friends”（我和朋友一起吃披萨）和“I ate pizza with olives”(我吃了有橄榄的披萨)）和多样性（如“I ate pizza with friends”也可以说成“Friends and I shared some pizza”）。语言也一直在进化中。人善于产生和理解语言，并具有表达、感知、理解复杂且微妙信息的能力。与此同时，虽然人类是语言的伟大使用者，但是我们并不善于形式化地理解和描述支配语言的规则。\n使用计算机理解和产生语言因此极具挑战性。事实上，最为人所知的处理语言数据的方法是使用有监督机器学习算法，其试图从事先标注好的输入/输出集合中推导出使用的模式和规则。例如，一个将文本分为四类的任务，类别为：体育、政治、八卦、经济。显然，文本中的单词提供了很强的线索，但是到底哪些单词提供了什么线索呢？为该任务书写规则极具挑战性。然而，读者可以轻松地将一篇文档分到一个主题中，然后，基于几百篇认为分类的样例，可以让有监督机器学习产生用词的模式，从而帮助文本分类。机器学习方法擅长那些很难获得规则集，但是相对容易获得给定输入及相应输出样本的领域。\n除了使用不明确规则集处理歧义和多样输入的挑战外，自然语言展现了另外一些特性，其使得用包括机器学习在内的计算方法更具挑战性，即离散性（discrete）、组合性（compositional）和稀疏性（sparse）。\n语言是符号化和离散的。书面语义的基本单位是字符，字符构成了单词，单词再表示对象、概念、事件、动作和思想。字符和单词都是离散符号：如“hamburger”或“pizza”会唤起我们头脑中的某种表示，但是它们也是不同的符号，其含义是不相关的，待我们的大脑去理解。从符号自身看，“hamburger”和“pizza”之间没有内在的关系，从构成它们的字母看也一样。与机器视觉中普遍使用的如颜色的概念或声学信号相对比，这些概念都是连续的，如可以使用简单的数学运算从一幅彩色图像变为灰度图像，或者从色调、光强等内在性质比较两幅图像。对于单词，这些都不容易做到，如果不使用一个大的查找表或者词典，没有什么简单的运算可以从单词“red”变为单词“pink”.\n语言还具有组合性，即字母形成单词，单词形成短语和句子。短语的含义可以比包含的单词更大，并遵循复杂的规则集。为了理解一个文本，我们需要超越字母和单词，看到更长的单词序列，如句子甚至整篇文本。\n以上性质的组合导致了数据稀疏性（data sparseness）。单词（离散符号）组合并形成意义的方式实际上是无限的。可能合法的句子数是巨大的，我们从没指望能全部枚举出来。随便翻一本书，其中绝大部分句子是你之前从没看过和听过的。甚至，很有可能很多四个单词构成的序列对你都是新鲜的。如果你看一下过去10年的报纸或者想像一下未来10年的报纸，许多单词，特别是人名、品牌和公司以及俚语和术语都将是新的。我们也不清楚如何从一个句子生成另一个句子或者定义句子之间的相似性，也不依赖于它们的意思——对我们是不可观测的。当我们要从实例中学习时也是挑战重重，即使有非常大的实例集合，我们仍然很容易观测到实例集合中从没有出现过的事件，其与曾出现过的所有实例都非常不同。\n1.2 神经网络和深度学习\n深度学习是机器学习的一个分支，是神经网络的重命名。神经网络是一系列学习技术，历史上曾受模拟脑计算工作的启发，可被看作学习参数可微的数学函数。深度学习的名字源于许多曾被连接在一起的可微函数。\n虽然全部机器学习技术都可以被认为是基于过去的观测学习如何做出预测，但是深度学习方法不仅学习预测，而且学习正确地表示数据，以使其更有助于预测。给出一个巨大的输入-输出映射集合，深度学习方法将数据“喂”给一个网络，其产生输入的后继转换，直到用最终的转换来预测输出。网络产生的转换都学习自给定的输入-输出映射，以便每个转换都使得更易于将数据和期望的标签之间建立联系。\n开发者负责设计网络结构和训练方式，提供给网络合适的输入-输出实例集合，将输入数据恰当地编码，大量学习正确表示的工作则由网络自动执行，同时受到网络结构的支持。\n1.3 自然语言处理中的深度学习\n神经网络提供了强大的学习机制，对自然语言处理问题极具吸引力。将神经网络用于语言的一个主要组件是使用嵌入曾（embedding layer），即将离散的符号映射为相对低维的连续向量。当嵌入单词的时候，从不同的独立符号转换为可以运算的数学对象。特别地，向量之间的距离可以等价于单词之间的距离，这使得更容易从一个单词泛化到另一个单词。学习单词的向量表示成为训练过程的一部分。再往上层，网络学习单词向量的组合方式以更有利于观测，该能力减轻了离散和数据稀疏问题。\n有两种主要的神经网络结构，即前馈网络（feed-forward network）和循环/递归网络（recurrent/recursive network），它们可以以各种方式组合。\n前馈网络，也叫多层感知器（Multi-Layer Perceptron）,其输入大小固定，对于变化的输入长度，我们可以忽略元素的顺序。当将输入集合喂给网络时，网络学习用有意义的方式组合它们。之前线性模型所能应用的地方，多层感知器都能使用。网络的非线性以及易于整合预训练词嵌入的能力经常导致更高的分类精度。\n卷积前馈网络是一类特殊的结构，其善于抽取数据中有特殊意义的局部模式；将任意长度的输入“喂”给网络，网络能抽取有意义的局部模式，这些模式对单词顺序敏感，而忽略它们在输入中出现的位置。这些工作适合于识别长句子或者文本中有指示性的短语和惯用语。\n循环神经网络是适于序列数据的特殊模型，网络接收输入序列作为输入，产生固定大小的向量作为序列的摘要。对于不同的任务，“一个序列的摘要”意味着不同的东西（也就是说，用于回答一个句子情感所需的信息与回答其语法的信息并不相同）。循环网络很少被当做独立组件使用，其能力在于可能当做可训练的组件“喂”给其他网络组件，然后串联地训练它们。例如，循环网络的输出可以“喂”给前馈网络，用于预测一些值。循环网络被用作一个输入转换器，其被训练用于产生富含信息的表示，前馈网络将在其上进行运算。对于序列循环网络是非常引人注目的模型，可能也是神经网络用于自然语言最令人激动的成果。它们允许：打破自然语言处理中存在几十年的马尔科夫假设，设计能依赖整个句子的模型，并在需要的情况下考虑词的顺序，同时不太受由于数据稀疏造成的统计估计问题之苦。该能力是语言模型产生了令人印象深刻的收益，其中语言模型指的是预测序列中下一个单词的概率（等价于预测一个序列的概率），是许多自然语言处理应用的核心。递归神经网络将循环网络从序列扩展到树。\n自然语言处理的许多问题是结构化的，需要产生复杂的输出结构，如序列和树。神经网络模型能适应该需求，一方面可以改进已知的面向线性模型的结构化预测算法，另一方面可以使用新的结构，如序列到序列（编码器-解码器）模型，指的是条件生成模型。此类模型是目前公认的最好的机器翻译模型的核心。\n最后，许多自然语言预测任务互相关联，在某种意义上知道一种任务是如何执行的将对另一些任务有所帮助。另外，我们可能没有足够的有监督（带标签）训练数据，而只有足够的原始文本（无标签数据）。那我们能从相关的任务或者未标注数据中学习吗？对于多任务学习（Multi-Task Learning，即从相关问题中学习）和半监督（semi-supervised）学习（从额外的、未标注的数据中学习），神经网络方法提供了令人激动的机会。\n注:文章内容摘自Yoav Goldberg所著《Neural Network Methods for Natural Language Processing》的中文版《基于深度学习的自然语言处理》chapter 1 Introduction","data":"2018年07月20日 14:36:56"}
{"_id":{"$oid":"5d344ee462f717dc0659b6c8"},"title":"自然语言处理基础知识","author":"weixin_33826609","content":"1. 分词（Word Cut）\n英文：单词组成句子，单词之间由空格隔开\n中文：字、词、句、段、篇\n词：有意义的字组合\n分词：将不同的词分隔开，将句子分解为词和标点符号\n英文分词：根据空格\n中文分词：三类算法\n中文分词难点：歧义识别、未登录词\n中文分词的好坏：歧义词识别和未登录词的识别准确率\n分词工具：Jieba，SnowNLP，NlPIR，LTP，NLTK\n2. 词性标注（POS Tag）\n词性也称为词类或词汇类别。用于特定任务的标记的集合被称为一个标记集\n词性：词类，词汇性质，词汇的语义功能，词汇的所属类别\n词性取决于：1.选定的词的类别体系 2.词汇本身在语句中上下文的语法语义功能\n一个词汇有多个不同的词性，词性兼类现象\n词性唯一：单性词\n词性多于2个：兼类词\n词性标注：将单词按它们的词性分类并进行相应地标注的过程，称为词语性质标注、词性标注或简称标注。\n词性标注器：一个标注器能够正确识别一个句子的上下文中的这些词的标记\n词性标注方法：三类\n2.1 NLTK常用词性：\nCC Coordinating conjunction 连接词\nCD Cardinal number 基数词\nDT Determiner 限定词（如this,that,these,those,such，不定限定词：no,some,any,each,every,enough,either,neither,all,both,half,several,many,much,(a) few,(a) little,other,another.\nEX Existential there 存在句\nFW Foreign word 外来词\nIN Preposition or subordinating conjunction 介词或从属连词\nJJ Adjective 形容词或序数词\nJJR Adjective, comparative 形容词比较级\nJJS Adjective, superlative 形容词最高级\nLS List item marker 列表标示\nMD Modal 情态助动词\nNN Noun, singular or mass 常用名词 单数形式\nNNS Noun, plural 常用名词 复数形式\nNNP Proper noun, singular 专有名词，单数形式\nNNPS Proper noun, plural 专有名词，复数形式\nPDT Predeterminer 前位限定词\nPOS Possessive ending 所有格结束词\nPRP Personal pronoun 人称代词\nPRP$ Possessive pronoun 所有格代名词\nRB Adverb 副词\nRBR Adverb, comparative 副词比较级\nRBS Adverb, superlative 副词最高级\nRP Particle 小品词\nSYM Symbol 符号\nTO to 作为介词或不定式格式\nUH Interjection 感叹词\nVB Verb, base form 动词基本形式\nVBD Verb, past tense 动词过去式\nVBG Verb, gerund or present participle 动名词和现在分词\nVBN Verb, past participle 过去分词\nVBP Verb, non-3rd person singular present 动词非第三人称单数\nVBZ Verb, 3rd person singular present 动词第三人称单数\nWDT Wh-determiner 限定词（如关系限定词：whose,which.疑问限定词：what,which,whose.）\nWP Wh-pronoun 代词（who whose which）\nWP$ Possessive wh-pronoun 所有格代词\nWRB Wh-adverb 疑问代词（how where when）\n通用词性标记集\n标记 含义 英文示例\nADJ 形容词 new, good, high, special, big, local\nADP 介词 on, of, at, with, by, into, under\nADV 副词 really, already, still, early, now\nCONJ 连词 and, or, but, if, while, although\nDET 限定词，冠词 the, a, some, most, every, no, which\nNOUN 名词 year, home, costs, time, Africa\nNUM 数词 twenty-four, fourth, 1991, 14:24\nPRT 小品词 at, on, out, over per, that, up, with\nPRON 代词 he, their, her, its, my, I, us\nVERB 动词 is, say, told, given, playing, would\n. 标点符号 . , ; !\nX 其它 ersatz, esprit, dunno, gr8, univeristy\nNLTK读取已经标注的语料库：一个已标注的词符使用一个由词符和标记组成的元组来表示。str2tuple（）\n一旦我们开始做词性标注，我们将会创建分配一个标记给一个词的程序，标记是在给定上下文中最可能的标记。我们可以认为这个过程是从词到标记的映射。在Python中最自然的方式存储映射是使用所谓的字典数据类型（在其他的编程语言又称为关联数组或哈希数组）\nNLTK标记形式：（word，tag）和字典\n将字典转换成列表：list（），sorted（）\n按值排序一个字典的习惯用法，sorted()的第一个参数是要排序的项目，它是由一个词性标记和一个频率组成的元组的列表。第二个参数使用函数itemgetter()指定排序的键。在一般情况下，itemgetter(n)返回一个函数，这个函数可以在一些其他序列对象上被调用获得这个序列的第n个元素。\nfrom operator import itemgetter\nsorted(counts.items(), key=itemgetter(1), reverse=True)\n一个词的标记依赖于这个词和它在句子中的上下文\n3.自动标注\n3.1默认标注器\n1.最简单的标注器是为每个词符分配同样的标记。这似乎是一个相当平庸的一步，但它建立了标注器性能的一个重要的底线。为了得到最好的效果，我们用最有可能的标记标注每个词。让我们找出哪个标记是最有可能的\ntags = [tag for (word, tag) in brown.tagged_words(categories='news')]\nnltk.FreqDist(tags).max()\n'NN'\n2.创建一个将所有词都标注成NN的标注器\nraw = 'I do not like green eggs and ham, I do not like them Sam I am!'\ntokens = word_tokenize(raw)\ndefault_tagger = nltk.DefaultTagger('NN')\ndefault_tagger.tag(tokens)\n[('I', 'NN'), ('do', 'NN'), ('not', 'NN'), ('like', 'NN'), ('green', 'NN'),\n('eggs', 'NN'), ('and', 'NN'), ('ham', 'NN'), (',', 'NN'), ('I', 'NN'),\n('do', 'NN'), ('not', 'NN'), ('like', 'NN'), ('them', 'NN'), ('Sam', 'NN'),\n('I', 'NN'), ('am', 'NN'), ('!', 'NN')]\n3.不出所料，这种方法的表现相当不好。在一个典型的语料库中，它只标注正确了八分之一的标识符，正如我们在这里看到的：\ndefault_tagger.evaluate(brown_tagged_sents)\nOut[13]: 0.13089484257215028\n默认的标注器给每一个单独的词分配标记，即使是之前从未遇到过的词。碰巧的是，一旦我们处理了几千词的英文文本之后，大多数新词都将是名词。正如我们将看到的，这意味着，默认标注器可以帮助我们提高语言处理系统的稳定性。\n3.2正则表达式标注器\n正则表达式标注器基于匹配模式分配标记给词符。例如，我们可能会猜测任一以ed结尾的词都是动词过去分词，任一以's结尾的词都是名词所有格。可以用一个正则表达式的列表表示这些：\npatterns = [\n... (r'.ing$', 'VBG'), # gerunds\n... (r'.ed$', 'VBD'), # simple past\n... (r'.es$', 'VBZ'), # 3rd singular present\n... (r'.ould$', 'MD'), # modals\n... (r'.'s$', 'NN$'), # possessive nouns\n... (r'.s$', 'NNS'), # plural nouns\n... (r'^-?[0-9]+(.[0-9]+)?$', 'CD'), # cardinal numbers\n... (r'.*', 'NN') # nouns (default)\n... ]\n请注意，这些是顺序处理的，第一个匹配上的会被使用。现在我们可以建立一个标注器，并用它来标记一个句子。做完这一步会有约五分之一是正确的。\nregexp_tagger.evaluate(brown_tagged_sents)\nOut[21]: 0.20326391789486245\n3.3查询标注器\n3.4N-gram标注\n一元标注器基于一个简单的统计算法：对每个标识符分配这个独特的标识符最有可能的标记。例如，它将分配标记\u003ctt class=\"doctest\"\u003eJJ\u003c/tt\u003e给词frequent的所有出现，因为frequent用作一个形容词（例如a frequent word）比用作一个动词（例如I frequent this cafe）更常见。一个一元标注器的行为就像一个查找标注器（4），除了有一个更方便的建立它的技术，称为训练。\n一个n-gram tagger标注器是一个一元标注器的一般化，它的上下文是当前词和它前面n-1个标识符的词性标记\n\n\n1-gram标注器是一元标注器另一个名称：即用于标注一个词符的上下文的只是词符本身。2-gram标注器也称为二元标注器，3-gram标注器也称为三元标注器。\n5.组合标注器\n尝试使用二元标注器标注标识符。\n如果二元标注器无法找到一个标记，尝试一元标注器。\n如果一元标注器也无法找到一个标记，使用默认标注器。\n大多数NLTK标注器允许指定一个回退标注器。回退标注器自身可能也有一个回退标注器：\nt0 = nltk.DefaultTagger('NN') t1 = nltk.UnigramTagger(train_sents, backoff=t0) t2 = nltk.BigramTagger(train_sents, backoff=t1) t2.evaluate(test_sents) 0.844513...\n4.文本分类\n4.1 词类分类\n在一般情况下，语言学家使用形态学、句法和语义线索确定一个词的类别\n形态学线索\n一个词的内部结构可能为这个词分类提供有用的线索。举例来说：-ness是一个后缀，与形容词结合产生一个名词，如happy → happiness, ill → illness。如果我们遇到的一个以-ness结尾的词，很可能是一个名词。同样的，-ment是与一些动词结合产生一个名词的后缀，如govern → government和establish → establishment。\n英语动词也可以是形态复杂的。例如，一个动词的现在分词以-ing结尾，表示正在进行的还没有结束的行动（如falling, eating）。-ing后缀也出现在从动词派生的名词中，如the falling of the leaves（这被称为动名词）。\n句法线索\n另一个信息来源是一个词可能出现的典型的上下文语境。例如，假设我们已经确定了名词类。那么我们可以说，英语形容词的句法标准是它可以立即出现在一个名词前，或紧跟在词be或very后。根据这些测试，near应该被归类为形容词：\ns(2)\na. the near window\nb. The end is (very) near.\n语义线索\n最后，一个词的意思对其词汇范畴是一个有用的线索。\n4.2 有监督分类\n分类是为给定的输入选择正确的类标签的任务。在基本的分类任务中，每个输入被认为是与所有其它输入隔离的，并且标签集是预先定义的。这里是分类任务的一些例子：\n判断一封电子邮件是否是垃圾邮件。\n从一个固定的主题领域列表中，如“体育”、“技术”和“政治”，决定新闻报道的主题是什么。\n决定词bank给定的出现是用来指河的坡岸、一个金融机构、向一边倾斜的动作还是在金融机构里的存储行为。\n\n\n有监督分类框架\n\n\n（a）在训练过程中，特征提取器用来将每一个输入值转换为特征集。这些特征集捕捉每个输入中应被用于对其分类的基本信息，我们将在下一节中讨论它。特征集与标签的配对被送入机器学习算法，生成模型。（b）在预测过程中，相同的特征提取器被用来将未见过的输入转换为特征集。之后，这些特征集被送入模型产生预测标签。\n4.2.1 性别鉴定\n男性和女性的名字有一些鲜明的特点。以a，e和i结尾的很可能是女性，而以k，o，r，s和t结尾的很可能是男性。\n创建一个分类器的第一步是决定输入的什么样的特征是相关的，以及如何为那些特征编码。\n特征提取函数\ndef gender_features(word):\n... return {'last_letter': word[-1]}\n这个函数返回的字典被称为特征集，映射特征名称到它们的值。特征名称是区分大小写的字符串，通常提供一个简短的人可读的特征描述，例如本例中的'last_letter'。特征值是简单类型的值，如布尔、数字和字符串。\n准备数据（一个例子和对应类标签的列表）\nfrom nltk.corpus import names\nlabeled_names = ([(name, 'male') for name in names.words('male.txt')] +\n... [(name, 'female') for name in names.words('female.txt')])\nimport random\nrandom.shuffle(labeled_names)\n使用特征提取器处理names数据，并划分特征集的结果链表为一个训练集和一个测试集。训练集用于训练一个新的“朴素贝叶斯”分类器。\nfeaturesets = [(gender_features(n), gender) for (n, gender) in labeled_names]\ntrain_set, test_set = featuresets[500:], featuresets[:500]\nclassifier = nltk.NaiveBayesClassifier.train(train_set)\n测试\nclassifier.classify(gender_features('Neo'))\n'male'\nclassifier.classify(gender_features('Trinity'))\n'female'\n准确度\nprint(nltk.classify.accuracy(classifier, test_set))\n检查分类器，确定哪些特征对于区分名字的性别是最有效的\nclassifier.show_most_informative_features(5)\nMost Informative Features\nlast_letter = 'a' female : male = 33.2 : 1.0\nlast_letter = 'k' male : female = 32.6 : 1.0\nlast_letter = 'p' male : female = 19.7 : 1.0\nlast_letter = 'v' male : female = 18.6 : 1.0\nlast_letter = 'f' male : female = 17.3 : 1.0\n4.2.2选择正确的特征\ndef gender_features2(name):\nfeatures = {}\nfeatures[\"first_letter\"] = name[0].lower()\nfeatures[\"last_letter\"] = name[-1].lower()\nfor letter in 'abcdefghijklmnopqrstuvwxyz':\nfeatures[\"count({})\".format(letter)] = name.lower().count(letter)\nfeatures[\"has({})\".format(letter)] = (letter in name.lower())\nreturn features\ngender_features2('John')\n{'count(j)': 1, 'has(d)': False, 'count(b)': 0, ...}\n然而，你要用于一个给定的学习算法的特征的数目是有限的——如果你提供太多的特征，那么该算法将高度依赖你的训练数据的特性，而一般化到新的例子的效果不会很好。这个问题被称为过拟合，当运作在小训练集上时尤其会有问题。\n一旦初始特征集被选定，完善特征集的一个非常有成效的方法是错误分析。首先，我们选择一个开发集，包含用于创建模型的语料数据。然后将这种开发集分为训练集和开发测试集。\n训练集用于训练模型，开发测试集用于进行错误分析。测试集用于系统的最终评估。\n\n\n用于训练有监督分类器的语料数据组织图。语料数据分为两类：开发集和测试集。开发集通常被进一步分为训练集和开发测试集。\n使用开发测试集，我们可以生成一个分类器预测名字性别时的错误列表\nerrors = []\nfor (name, tag) in devtest_names:\nguess = classifier.classify(gender_features(name))\nif guess != tag:\nerrors.append( (tag, guess, name) )\nfor (tag, guess, name) in sorted(errors):\n... print('correct={:\u003c8} guess={:\u003c8s} name={:\u003c30}'.format(tag, guess, name))\n浏览这个错误列表，它明确指出一些多个字母的后缀可以指示名字性别。例如，yn结尾的名字显示以女性为主，尽管事实上，n结尾的名字往往是男性；以ch结尾的名字通常是男性，尽管以h结尾的名字倾向于是女性。因此，调整我们的特征提取器包括两个字母后缀的特征：\ntrain_set = [(gender_features(n), gender) for (n, gender) in train_names]\ndevtest_set = [(gender_features(n), gender) for (n, gender) in devtest_names]\nclassifier = nltk.NaiveBayesClassifier.train(train_set)\nprint(nltk.classify.accuracy(classifier, devtest_set))\n这个错误分析过程可以不断重复，检查存在于由新改进的分类器产生的错误中的模式。每一次错误分析过程被重复，我们应该选择一个不同的开发测试/训练分割，以确保该分类器不会开始反映开发测试集的特质。\n4.3词性标注\n训练一个分类器来算出哪个后缀最有信息量\n定义一个特征提取器函数，检查给定的单词的这些后缀\n训练一个新的“决策树”的分类器\n决策树模型的一个很好的性质是它们往往很容易解释——我们甚至可以指示NLTK将它们以伪代码形式输出s\n4.4探索上下文语境\n通过增加特征提取函数，我们可以修改这个词性标注器来利用各种词内部的其他特征，例如词长、它所包含的音节数或者它的前缀。然而，只要特征提取器仅仅看着目标词，我们就没法添加依赖词出现的上下文语境特征。然而上下文语境特征往往提供关于正确标记的强大线索——例如，标注词\"fly\"，如果知道它前面的词是“a”将使我们能够确定它是一个名词，而不是一个动词。\n为了采取基于词的上下文的特征，我们必须修改以前为我们的特征提取器定义的模式。不是只传递已标注的词，我们将传递整个（未标注的）句子，以及目标词的索引。\n很显然，利用上下文特征提高了我们的词性标注器的准确性。\n4.5序列分类\n一种序列分类器策略，称为连续分类或贪婪序列分类，是为第一个输入找到最有可能的类标签，然后使用这个问题的答案帮助找到下一个输入的最佳的标签。\n首先，我们必须扩展我们的特征提取函数使其具有参数\u003ctt class=\"doctest\"\u003ehistory\u003c/tt\u003e，它提供一个我们到目前为止已经为句子预测的标记的列表\n[1]\n。\u003ctt class=\"doctest\"\u003ehistory\u003c/tt\u003e中的每个标记对应\u003ctt class=\"doctest\"\u003esentence\u003c/tt\u003e中的一个词。但是请注意，\u003ctt class=\"doctest\"\u003ehistory\u003c/tt\u003e将只包含我们已经归类的词的标记，也就是目标词左侧的词。因此，虽然是有可能查看目标词右边的词的某些特征，但查看那些词的标记是不可能的（因为我们还未产生它们）。\n4.6 其他有监督分类例子\n4.6.1句子分割\n句子分割可以看作是一个标点符号的分类任务：每当我们遇到一个可能会结束一个句子的符号，如句号或问号，我们必须决定它是否终止了当前句子。\n第一步是获得一些已被分割成句子的数据，将它转换成一种适合提取特征的形式\nsents = nltk.corpus.treebank_raw.sents() \u003e\u003e\u003e tokens = [] \u003e\u003e\u003e boundaries = set() \u003e\u003e\u003e offset = 0 \u003e\u003e\u003e for sent in sents: ... tokens.extend(sent) ... offset += len(sent) ... boundaries.add(offset-1) tokens是单独句子标识符的合并列表，boundaries是一个包含所有句子边界词符索引的集合。\n下一步，我们需要指定用于决定标点是否表示句子边界的数据特征\ndef punct_features(tokens, i): ... return {'next-word-capitalized': tokens[i+1][0].isupper(), ... 'prev-word': tokens[i-1].lower(), ... 'punct': tokens[i], ... 'prev-word-is-one-char': len(tokens[i-1]) == 1}\n基于这一特征提取器，我们可以通过选择所有的标点符号创建一个加标签的特征集的列表，然后标注它们是否是边界标识符\nfeaturesets = [(punct_features(tokens, i), (i in boundaries)) ... for i in range(1, len(tokens)-1) ... if tokens[i] in '.?!']\n训练并评估\n\u003e\u003e\u003e size = int(len(featuresets) * 0.1) \u003e\u003e\u003e train_set, test_set = featuresets[size:], featuresets[:size] \u003e\u003e\u003e classifier = nltk.NaiveBayesClassifier.train(train_set) \u003e\u003e\u003e nltk.classify.accuracy(classifier, test_set) 0.936026936026936\n4.6.2识别对话行为类型\n处理对话时，将对话看作说话者执行的行为是很有用的。对于表述行为的陈述句这种解释是最直白的，例如\"I forgive you\"或\"I bet you can't climb that hill\"。但是问候、问题、回答、断言和说明都可以被认为是基于语言的行为类型。识别对话中言语下的对话行为是理解谈话的重要的第一步。\n可以利用这些数据建立一个分类器，识别新的即时消息帖子的对话行为类型。第一步是提取基本的消息数据。\n下一步，我们将定义一个简单的特征提取器，检查帖子包含什么词\n最后，我们通过为每个帖子提取特征（使用post.get('class')获得一个帖子的对话行为类型）构造训练和测试数据，并创建一个新的分类器\n4.6.3识别文字蕴含\n识别文字蕴含（RTE）是判断文本T的一个给定片段是否蕴含着另一个叫做“假设”的文本\n迄今为止，已经有4个RTE挑战赛，在那里共享的开发和测试数据会提供给参赛队伍。这里是挑战赛3开发数据集中的文本/假设对的两个例子。标签True表示蕴含成立，False表示蕴含不成立。\n5.评估\n5.1测试集\n5.2准确度\n5.3召回率和F值\n5.4混淆矩阵\n5.5交叉验证\n6.从文本提取信息\n6.1信息提取\n从文本获取意义的方法被称为信息提取\n6.1.1信息提取的架构\n6.1.2词块划分\n用于实体识别的基本技术是词块划分，它分割和标注多词符的序列。小框显示词级分词和词性标注，大框显示高级别的词块划分。每个这种较大的框叫做一个词块。就像分词忽略空白符，词块划分通常选择词符的一个子集。同样像分词一样，词块划分器生成的片段在源文本中不能重叠。\n\n\n名词短语词块划分\n首先思考名词短语词块划分或NP词块划分任务，在那里我们寻找单独名词短语对应的词块\n词块信息最有用的来源之一是词性标记。这是在我们的信息提取系统中进行词性标注的动机之一。为了创建一个词块划分器，我们将首先定义一个词块语法，由指示句子应如何进行词块划分的规则组成。\n\n\n标记模式\n组成一个词块语法的规则使用标记模式来描述已标注的词的序列。一个标记模式是一个词性标记序列，用尖括号分隔，如\u003cDT\u003e?\u003cJJ\u003e*\u003cNN\u003e。\n用正则表达式进行词块划分\n要找到一个给定的句子的词块结构，RegexpParser词块划分器以一个没有词符被划分的平面结构开始。词块划分规则轮流应用，依次更新词块结构。一旦所有的规则都被调用，返回生成的词块结构。\n\n\n探索文本语料库\n7.分析句子结构\n《python自然语言处理》各章总结：\n1. 语言处理与Python\n2. 获得文本语料和词汇资源\n3. 处理原始文本\n4. 编写结构化的程序\n5. 分类和词汇标注\n6. 学习分类文本\n7. 从文本提取信息\n8. 分析句子结构\n9. 构建基于特征的文法\n10. 分析句子的含义\n11. 语言学数据管理\n未完待续......","data":"2018年01月20日 01:03:00"}
{"_id":{"$oid":"5d344f2c62f717dc0659b6d7"},"title":"自然语言处理(NLP)四步流程：Embed-\u003eEncode-\u003eAttend-\u003ePredict","author":"Scofield_Phil","content":"过去半年以来，自然语言处理领域进化出了一件神器。此神器乃是深度神经网络的一种新模式，该模式分为：embed、encode、attend、predict四部分。本文将对这四个部分娓娓道来，并且剖析它在两个实例中的用法。\n\n人们在谈论机器学习带来的提升时，往往只想到了机器在效率和准确率方面带给人们的提升，然而最重要的一点却是机器学习算法的通用性。如果你想写一段程序来识别社交媒体平台上的侮辱性帖子，就把问题泛化为“需要输入一段文本，预测出文本的类别ID”。这种分类与识别侮辱性帖子或是标记电子邮件类别之类的具体任务无关。如果两个问题的输入和输出类型都一致，那我们就应复用同一套模型的代码，两者的区别应该在于送入的训练数据不同，就像我们使用同一个游戏引擎玩不同的游戏。\n笔者用spaCy和Keras实现了自然语言推理的可分解注意力模型。代码已经上传到github\n假设你有一项强大的技术，可以预测实数稠密向量的类别标签。只要输入输出的格式相同，你就能用这项技术解决所有的问题。与此同时，你有另一项技术，可以用一个向量和一个矩阵预测出另一个向量。那么，现在你手里就握着三类问题的解决方案了，而不是两类。为什么是三类呢？因为如果第三类问题是通过矩阵和一个向量，得到一个类别标签，显然你可以组合利用前两种技术来解决。大多数NLP问题可以退化成输入一条或多条文本的机器学习问题。如果我们能将这些文本转化为向量，我们就可以复用现有的深度学习框架。接下来就是具体的做法。\n文本类深度学习的四部曲\n嵌入式词语表示，也被称为“词向量”，是现在最广泛使用的自然语言处理技术之一。词向量表示是一种既能表示词本身又可以考虑语义距离的表示方法。然而，大多数NLP问题面对的不是单个词语，而是需要分析更长的文本内容。现在有一个简单而灵活的解决方案，它在许多任务上都表现出了卓越的性能，即RNN模型。将文本用一个向量的序列表示之后，使用双向RNN模型将向量编码为一个句子向量矩阵。这个矩阵的每一行可以理解为词向量 —— 它们对句子的上下文敏感。最后一步被称为注意力机制。这可以将句子矩阵压缩成一个句子向量，用于预测。\n第一步：词向量\n词向量表将高维的稀疏二值向量映射成低维的稠密向量。举个例子，假设我们收到的文本是一串ASCII字符，共有256种可能值，于是我们把每一种可能值表示为一个256维的二值向量。字符’a’的向量只有在第97维的值等于1，其它维度的值都等于0。字符’b’的向量只有在第98维的值等于1，其它维度的值都等于0。这种表示方法称为’one hot’形式。不同字符的向量表示完全不一样。\n\n\n大部分神经网络模型首先都会把输入文本切分成若干个词语，然后将词语都用词向量表示。另一些模型用其它信息扩展了词向量表示。比如，除了词语的ID之外，还会输入一串标签。然后可以学习得到标签向量，将标签向量拼接为词向量。这可以让你将一些位置敏感的信息加入到词向量表示中。然而，有一个更强大的方式来使词语表示呈现出语境相关。\n第二步：编码\n假设得到了词向量的序列，编码这一步是将其转化为句子矩阵，矩阵的每一行表示每个词在上下文中所表达的意思。\n\n\n这一步用到了双向RNN模型。LSTM和GRU结构的模型效果都不错。每一行向量通过两部分计算得到：第一部分是正向计算，第二部分是逆向计算，然后拼接两部分得到完整的向量。计算过程如下图代码所示：\n\n\n我个人认为双向RNN会是今后的主流。RNN的主要应用是读入文本内容，然后从中预测出一些信息。而我们是用它来计算一个中间表达状态。最重要的一点是得到的表达能够反映词语在文中的意义。理论上应该学到“pick up”与“pick on”这两个词语的意义有区别。这一直是NLP模型的巨大弱点。现在我们有了一个解决方案。\n第三步：注意力机制\n这一步是将上一步的矩阵表示压缩为一个向量表示，因此可以被送入标准的前馈神经网络进行预测。注意力机制对于其它压缩方法的优势在于它输入一个辅助的上下文向量：\n\n\nYang等人在2016年发表的论文提出了一种注意力机制，输入一个矩阵，输出一个向量。区别于从输入内容中提取一个上下文向量，该机制的上下文向量是被当做模型的参数学习得到。这使得注意机制变成一个纯粹的压缩操作，可以替换任何的池化步骤。\n第四步：预测\n文本内容被压缩成一个向量之后，我们可以学习最终的目标表达 —— 一种类别标签、一个实数值或是一个向量等等。我们也可以将网络模型看做是状态机的控制器，如一个基于转移的解析器，来做结构化预测。\n\n\n有趣的是，大部分的NLP模型通常更青睐浅层的前馈网络。这意味着近期在机器视觉领域取得的重要技术至今为止并没有影响到NLP领域，比如residual connections 和 batch normalization。\n实例1：自然语言推测的可分解注意力模型\n自然语言推测是给一对句子预测类别标签的问题，类别标签则表示它们两者的逻辑关系。斯坦福自然语言预测文本集使用三种类别标签：\n1.推演(Entailment)：如果第一句话是真的，那么第二句话一定为真。\n2.矛盾(Contradiction)：如果第一句话是真的，那么第二句话一定为假。\n3.中性(Neutral)：上述两者都不是。\nBowman等人在论文中给出了几条例子：\n文本内容\n假设内容\n标签\n某人正在检查一位来自中亚国家人士的服装\n此人正在睡觉\n矛盾\n一位长者和一位青年在微笑\n两个人在笑，嘲笑地板上玩耍的猫\n中性\n一辆黑色赛车在人群前面启动\n一个男人正沿着一条孤独的路行驶\n矛盾\n一种多个男性玩的足球游戏\n几位男性正在进行体育运动\n推演\n一位微笑盛装打扮的女性拿着一把伞\n一位快乐的女性在一个童话服装会上握着一把伞\n中性\n\n这份语料库的目的之一是为我们提供一个新的、规模合适的语料库用于研发将句子编码为向量的模型。例如，Bowman在2016年发表的论文介绍了一种基于转移的模型，它依次读入句子，构建一种树形结构的内部表达。\n\nBowman他们的准确率达到了83.2%，比之前的工作成果提升了一大截。过了不到半年，Parikh的论文提出的模型取得了86.8%的准确率，而使用的模型参数数量只有Bowman模型的10%。不久之后，Chen等人发表的论文提出了一种效果更好的系统，准确率达到88.3%。当我第一次阅读Parikh的论文时，我无法理解他们的模型如何取得这么好的效果。原因在于他们的模型用独特的注意力机制融合了两个句子矩阵：\n\n\n关键的优势是他们讲句子转为向量的压缩步骤合并完成，而Bowman他们则是分别将两个句子转为向量。请记住Vapnik的原则：\n“当解决一个关键问题时，不要解决一个更一般的问题作为中间步骤” —— VLADIMIR VAPNIK\nParikh的论文将自然语言推测任务当做是关键问题。他们想办法直接解决这个问题，因此比单独给句子编码有巨大的优势。Bowman等人则更关注问题的泛化，也是针对此构建模型。他们的模型适用的场景也就比Parikh的模型更广泛。比如说，利用Bowman的模型，你可以缓存句子向量，使得计算句子相似度的效率更高。\n实例2：文档分类的分层注意力网络\n给文档分类是我接触到的第一个NLP项目。澳大利亚的类似证券交易所的机构资助了一个项目，爬取澳大利亚的网站页面，并且自动检测金融诈骗。尽管这个项目已经过去了一段时间，但是文档分类的方法在之后的十年中几乎没有变化。这也是我看到Yang等人发表的分层注意力网络模型之后如此兴奋的原因。这是继词袋模型之后，我看到的第一篇真正有通用性改进的论文。下面是它的原理。\n该模型接收一篇文档作为输入，文档由句子的序列组成，其中每个句子是一个词语的序列。每句话的每个词语分别编码，生成两个词向量序列，每个序列表示一个句子。这两个序列分别编码成两个句子矩阵。然后由注意力机制将句子矩阵压缩为句子向量，多个句子向量又组成文本矩阵。最后一步注意力操作将文本矩阵压缩为文本向量，然后送入最终的预测网络来预测类别标签。\n\n\n该模型使用注意机制作为一个纯粹的压缩步骤：它学会了把矩阵作为输入，然后将其概括成一个向量。这种学习过程是通过学习上下文向量的两个注意力转换，我们可以将这种转换理解为表示模型认为相关的词语或者句子，该模型会找到理想相关。或者，你也可以把整个压缩过程看做是特征提取的过程。按照这种观点，上下文向量只是另一个不透明的参数。\n作者\n方法\nYELP ‘13\nYELP ‘14\nYELP ‘15\nIMDB\nYang et al. (2016)\nHN-ATT\n68.2\n70.5\n71\n49.4\nYang et al. (2016)\nHN-AVE\n67\n69.3\n69.9\n47.8\nTang et al. (2015)\nParagraph Vector\n57.7\n59.2\n60.5\n34.1\nTang et al. (2015)\nSVM + Bigrams\n57.6\n61.6\n62.4\n40.9\nTang et al. (2015)\nSVM + Unigrams\n58.9\n60\n61.1\n39.9\nTang et al. (2015)\nCNN-word\n59.7\n61\n61.5\n37.6\n将yang等人的模型与卷积神经网络做比较，可以得到有意思的结果。两个模型都能自动提取位置敏感特征。然而，CNN模型既不通用，效率也较低。而双向RNN模型只需要对每个句子读入两次 ——正向一次，反向一次。LSTM编码还能提取任意长度的特征，因为句子上下文的任何信息都有可能被揉入词语的向量表示。将句子矩阵压缩成向量的步骤简单并且有效的。要构建文档向量，只需要对句子向量再进行一次同样的操作。\n提升模型准确率的主要因素是双向LSTM编码器，它创建了位置敏感的特点。作者通过将注意力机制替换为平均池化，证明了上述观点。使用平均池化的方法，该模型在所有测试数据上仍然优于以前的最好模型。然而，注意力机制进一步普遍地提高了性能。\n后续内容\n\n我已经用我们自己的NLP库spaCy实现了第一个例子，我正在实现文本分类的系统。我们还计划开发一个SpaCy版的通用双向LSTM模型，能够方便地将预训练的词向量用于实际问题中。","data":"2017年03月14日 19:42:53"}
{"_id":{"$oid":"5d344f4362f717dc0659b6df"},"title":"什么是自然语言处理技术","author":"adnb34g","content":"自然语言处理（NLP）是计算机科学，人工智能，语言学关注计算机和人类（自然）语言之间的相互作用的领域。自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。自然语言处理是一门融语言学、计算机科学、数学于一体的科学。因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，所以它与语言学的研究有着密切的联系，但又有重要的区别。自然语言处理并不是一般地研究自然语言，而在于研制能有效地实现自然语言通信的计算机系统，特别是其中的软件系统。因而它是计算机科学的一部分。\n自然语言处理技术是所有与自然语言的计算机处理有关的技术的统称，其目的是使计算机理解和接受人类用自然语言输入的指令，完成从一种语言到另一种语言的翻译功能，自然语言处理技术的研究，可以丰富计算机知识处理的研究内容，推动人工智能技术的发展。\n大快NLP模块是大快大数据一体化平台的一个组件，用户引用该组件可以有效进行自然语言的处理工作，如进行文章摘要，语义判别以及提高内容检索的精确度和有效性。\n自然语言处理如今不仅作为人工智能核心课题来研究，而且也作为新一代计算机的核心课题来研究。从知识产业角度看，专家系统，数据库，知识库，计算机辅助设计系统（CAD）、计算机辅助教学系统（CAI）、计算机辅助决策系统，办公室自动化管理系统，智能机器人等，都需要用自然语言处理，具有篇章理解能力的自然语言理解系统可用于机器自动翻译、情报检索、自动标引、自动文摘、自动写故事小说等领域，都可以用我们的工具类DKNLPBase来处理。\n标准分词\n方法签名：List\u003cTerm\u003e StandardTokenizer.segment(String txt);\n返回：分词列表。\n签名参数说明：txt:要分词的语句。\n范例：下例验证一段话第5个分词是阿法狗。\npublic void testSegment() throws Exception\n{\nString text = \"商品和服务\";\nList\u003cTerm\u003e termList = DKNLPBase.segment(text);\nassertEquals(\"商品\", termList.get(0).word);\nassertEquals(\"和\", termList.get(1).word);\nassertEquals(\"服务\", termList.get(2).word);\ntext = \"柯杰解说“李世石VS阿法狗第二局” 结局竟是这样\";\ntermList = DKNLPBase.segment(text);\nassertEquals(\"阿法狗\", termList.get(5).word);  // 能够识别\"阿法狗\"\n}\n关键词提取\n方法签名：List\u003cString\u003e  extractKeyword(String txt,int keySum);\n返回：关键词列表.\n签名参数说明：txt:要提取关键词的语句，keySum要提取关键词的数量\n范例：给出一段话提取一个关键词是“程序员”。\npublic void testExtractKeyword() throws Exception\n{\nString content = \"程序员(英文Programmer)是从事程序开发、维护的专业人员。\" +\n\"一般将程序员分为程序设计人员和程序编码人员，\" +\n\"但两者的界限并不非常清楚，特别是在中国。\" +\n\"软件从业人员分为初级程序员、高级程序员、系统\" +\n\"分析员和项目经理四大类。\";\nList\u003cString\u003e keyword = DKNLPBase.extractKeyword(content, 1);\nassertEquals(1, keyword.size());\nassertEquals(\"程序员\", keyword.get(0));\n}\n短语提取\n方法签名：List\u003cString\u003e extractPhrase(String txt, int phSum);\n返回：短语\n签名参数说明：txt:要提取短语的语句，phSum短语数量\n范例：给出一段文字，能代表文章的五个短语，第一个短语是算法工程师。\n迈进二十一世纪，我们已经进入了以互联网为主要标志的海量信息时代，这些海量信息大部分是以自然语言表示的。一方面，海量信息也为计算机学习人类语言提供了更多的“素材”，另一方面，这也为自然语言处理提供了更加宽广的应用舞台。例如，作为自然语言处理的重要应用，搜索引擎逐渐成为人们获取信息的重要工具，涌现出以百度、谷歌等为代表的搜索引擎巨头；机器翻译也从实验室走入寻常百姓家，谷歌、百度等公司都提供了基于海量网络数据的机器翻译和辅助翻译工具；基于自然语言处理的中文（输入法如搜狗、微软、谷歌等输入法）成为计算机用户的必备工具；带有语音识别的计算机和手机也正大行其道，协助用户更有效地工作学习。总之，随着互联网的普及和海量信息的涌现，自然语言处理正在人们的日常生活中扮演着越来越重要的作用。\n然而，我们同时面临着一个严峻事实，那就是如何有效利用海量信息已成为制约信息技术发展的一个全局性瓶颈问题。自然语言处理无可避免地成为信息科学技术中长期发展的一个新的战略制高点。同时，人们逐渐意识到，单纯依靠统计方法已经无法快速有效地从海量数据中学习语言知识，只有同时充分发挥基于规则的理性主义方法和基于统计的经验主义方法的各自优势，两者互相补充，才能够更好、更快地进行自然语言处理。\n自然语言处理作为一个年龄尚不足一个世纪的新兴学科，正在进行着突飞猛进的发展。回顾自然语言处理的发展历程，并不是一帆风顺，有过低谷，也有过高潮。而现在我们正面临着新的挑战和机遇。例如，目前网络搜索引擎基本上还停留在关键词匹配，缺乏深层次的自然语言处理和理解。语音识别、文字识别、问答系统、机器翻译等目前也只能达到很基本的水平。路漫漫其修远兮，自然语言处理作为一个高度交叉的新兴学科，不论是探究自然本质还是付诸实际应用，在将来必定会有令人期待的惊喜和异常快速的发展。","data":"2018年03月15日 10:25:48","date":"2018年03月15日 10:25:48"}
{"_id":{"$oid":"5d344f8362f717dc0659b6f1"},"title":"自然语言处理NLP基本知识小结","author":"默一鸣","content":"1.什么是NLP？\n人与人、人与计算机交互中的语言问题。\n能力模型，通常是基于语言学规则的模型，建立在人脑中先天存在语法通则这一假设的基础上，认为语言是人脑的语言能力推导出来的，建立语言模型就是通过建立人工编辑的语言规则集来模拟这种先天的语言能力。又称“理性主义的”语言模型。\n应用模型，根据不同的语言处理应用而建立的特定语言模型，通常是基于统计的模型。又称“经验主义的”语言模型，使用大规模真实语料库中获得语言各级语言单位上的统计信息，依据较低级语言单位上的统计信息运用相关的统计推理技术计算较高级语言单位上的统计信息\n2、分词\n词是最小的能够独立活动的有意义的语言成分，英文单词之间是以空格作为自然分界符的，而汉语是以字为基本的书写单位，词语之间没有明显的区分标记，因此，中文词语分析是中文信息处理的基础与关键。\n中文分词技术可分为三大类：基于字典、词库匹配的分词方法；基于词频度统计的分词方法和基于知识理解的分词方法。\nhttp://www.cnblogs.com/flish/archive/2011/08/08/2131031.html\n3、词性标注\n词性标注（Part-of-Speech tagging 或POS tagging)，又称词类标注或者简称标注，是指为分词结果中的每个单词标注一个正确的词性的程序，也即确定每个词是名词、动词、形容词或其他词性的过程。在汉语中，词性标注比较简单，因为汉语词汇词性多变的情况比较少见，大多词语只有一个词性，或者出现频次最高的词性远远高于第二位的词性。据说，只需选取最高频词性，即可实现80%准确率的中文词性标注程序。利用HMM即可实现更高准确率的词性标注\nhttp://blog.csdn.net/truong/article/details/18847549\n4、命名实体识别\n命名实体识别（Named Entity Recognition，简称NER），又称作“专名识别”，是指识别文本中具有特定意义的实体，主要包括人名、地名、机构名、专有名词等。\n（1）实体边界识别；（2） 确定实体类别（人名、地名、机构名或其他）\n命名实体识别是信息提取、问答系统、句法分析、机器翻译、面向Semantic Web的元数据标注等应用领域的重要基础工具。\n基于规则和词典的方法（ MUC-6 会议中几乎所有参赛成员都采用基于规则的方法），该方法需要专家制定规则，准确率较高，但依赖于特征领域，可移植性差；\n基于统计的方法，主要采用 HMM 、 MEMM 、 CRF, 难点在于特征选择上，该方法能获得好的鲁棒性和灵活性，不需太多的人工干预和领域限制，但需要大量的标注集。\n混合方法，采用规则与统计相结合，多种统计方法相结合等，是目前主流的方法。\n特征：上下文信息+构词法\n5、指代消解\n指代是一种常见的语言现象，一般情况下，指代分为2种：回指和共指。\n回指是指当前的照应语与上文出现的词、短语或句子(句群)存在密切的语义关联性，指代依存于上下文语义中，在不同的语言环境中可能指代不同的实体，具有非对称性和非传递性；\n共指主要是指2个名词(包括代名词、名词短语)指向真实世界中的同一参照体，这种指代脱离上下文仍然成立。\n目前指代消解研究主要侧重于等价关系，只考虑2个词或短语是否指示现实世界中同一实体的问题，即共指消解。\n中文的指代主要有3种典型的形式：\n(1)人称代词(pronoun)，例如：李明 怕高妈妈一人呆在家\n里寂寞，他 便将家里的电视搬了过来。\n(2)指示代词(demonstrative)，例如：很多人都想留下什么给孩子，这 可以理解，但不完全正确。\n(3)有定描述(definite description)，例如：，贸易制裁已经成为了美国政府对华的惯用大棒，这根 大棒 真如美国政府所希望的那样灵验吗?\n6、文本分类\n一个文本(以下基本不区分“文本”和“文档”两个词的含义) 分类问题就是将\n一篇文档归入预先定义的几个类别中的一个或几个，而文本的自动分类则是使用计算机程序来实现这样的分类。\n7、问答系统\n问答系统(Question Answering System, QA)是信息检索系统的一种高级形式，它能用准确、简洁的自然语言回答用户用自然语言提出的问题。\n依据问题类型可分为：限定域和开放域两种，依据数据类型可分为：结构型和无结构型（文本），依据答案类型可分为：抽取式和产生式两种。\n问句分析-》文档检索-》答案抽取（验证）","data":"2017年02月08日 23:29:26"}
{"_id":{"$oid":"5d344fa062f717dc0659b6f5"},"title":"【转载】自然语言处理（NLP）的历史、发展、成果和难题，以及在教育领域的应用情况","author":"光影流年925","content":"2017年4月21日-22日，由映魅咨询主办的TAB（Tech and Business）教育科技论坛在上海举行。十几位教育投资研究机构、国内外各类教育科技公司的嘉宾，围绕教育及教育科技投资、国际化的教育产业链、以及教育科技等市场关注的热门话题进行了热烈而深远的分享与探讨。在本次论坛上，校宝在线CTO孙琳围绕自然语言处理（NLP）的历史、发展、成果和难题，介绍了NLP目前在教育领域的应用情况。\n以下是孙博士的分享内容精选：\n大家好！我是孙琳，很高兴参加TAB教育科技论坛，今天分享的题目是“教育应用中的自然语言处理”。首先我先做一下自我介绍，我是剑桥大学计算机系的博士，博士研究的方向是自然语言处理， 2011年的时候，我们一起创立了校宝在线的前身，当时就想要把自然语言处理的技术用在教育当中。校宝在线的业务是为中国的民办学校提供SaaS解决方案，包括ERP、IMS等，同时把人工智能的技术应用在这些软件解决方案当中。我自己在业余时间还保持做研究，目前是剑桥大学语言实验室的研究员，每年还会坚持发Paper，同时也是很多学术杂志和国际会议的审稿人。\n今天为什么给大家讲“自然语言处理”和“教育”这个题目呢？其实大家都知道人工智能，特别在教育中的应用是最近的热点，相关的信息也非常多，相信大家也可以看到。但是作为人工智能当中一个非常重要的领域：自然语言处理跟教育结合的相关信息，却并不是特别多。我自己也找过，无论是中文还是英文都不多，我想这是一个非常好的机会能把我自己对于这方面的一些思考分享出来给供在座的各位大咖和各位创业者们做一个参考。\n一、自然语言处理（NLP）关注的核心是语言和文本\n自然语言处理的英文是Natural language processing，简称NLP。所以我下面说到NLP的时候大家应该能够反应出来NLP是自然语言处理。首先跟大家介绍一下NLP，然后说一下NLP在教育中的应用，最后我说一点自己的结论以及我自己对未来的一点展望。\n自然语言处理其实是人工智能里面一个非常重要的分支，其他的分支大家也非常了解，比如说计算机视觉、语音，包括机器学习、深度学习，这些都是人工智能的分支，它也常常被叫做计算语言学。它核心的目标就是把人的语言也就是自然语言转换成计算机可以执行的命令。简单来说就是让计算机读懂人的语言。所以说NLP关注的核心其实是语言或者更通俗一点来说是文本。\n二、自然语言处理（NLP）的难点：理解人的语言不能光靠逻辑，还要有非常强的知识库\n自然语言处理，我个人认为是人工智能领域里面最难的一个领域，它最大的难点在哪里？\n首先，因为自然语言处理相对于语音和视觉来说是高度抽象化的表现，它不是信号，而是一些非常抽象化的理念。\n大家都认为人类的语言有非常强的逻辑性，其实人类的语言逻辑性并不强。我给大家举一个例子，大家看这句话“我从来没说他偷过钱。”这句话有6种理解方法，我一一列出来了。\n比如说，我可以这么来说：“我从来没说他偷过钱。”这个意思就是可能别人说过，但是我没有说。\n第三个可以说“我从来没有说他偷过钱”，可能我确实没有说，但是我用其他的方式暗示过。\n除了这6种以外，如果把这个句子加长的话，变成“我从来没说他偷过我的钱。”那么就有7种解释，不光有1到6，还有第7种解释，这个句子可以变得更长，这个歧义就会更多。\n对于计算机来讲，如果单单给它这一句输入，要做到真正语境上的理解是不可能的事情。要做到真实语境上的理解可能需要更多的辅助信息和上下文的信息，不然是没有任何可能性的。\n其次，我们要理解人的语言不能光靠逻辑，还要有非常强的知识库，要有很多知识才能正确理解人类语言。\n我举个例子，下面两句话中，第一句话We gave monkeys the bananas because they were hungry。这个地方的they指猴子。第二句话We gave monkeys the bananas because they were over ripe。这个地方的they指香蕉。对于计算机来说这两句话看起来结构非常相似，句式也非常类似，所以计算机必须知道猴子饿了，香蕉不能饿，猴子不能烂的，香蕉才能烂，才能对这句话有一个正确的理解，不然是完全无法知道。\n再次，人的语言还有一个非常大的特性即组合性。\n我们通过字母组合成词，通过词组合成短语，短语组成句子、句子组成段落、段落组成文章。如果单单抽出里面一部分进行解析的话，比如说解析字母、解析词，我们就算理解了词的意思也不能表现出人本来的含义，因为单个抽出词是没有意义的，人的自然语言表达的含义往往就在这些组合当中，恰恰是学习这些复杂的组合对于计算机来说是一件非常难的事情。\n最后，人类语言是非常灵活和开放的。\n开放是什么意思？人的语言是随着时间而改变的，不停的有新词冒出来，以前词的意思也会随着时间有完全不同的意思。比如说“灌水”、“潜水”，这两个词在网络时代有了完全不同的含义。对于计算机来说怎么能够实时的学会这些新词、发现新的用法，也是非常有挑战性的。\n三、自然语言处理（NLP）常用的三种方法：（1）机器学习、（2）规则和逻辑、（3）语言学\n研究自然语言处理，通常有三种方法。\n第一种，机器学习的方法，也包括深度学习。简单来说我们收集海量的文本、数据，建立语言模型，解决自然语言处理的很多任务。\n第二种，规则和逻辑的方法。虽然人的语言不是完完全全有逻辑，但是里面还是有很强的逻辑性的，一些传统的逻辑、原理都可以用在上面，其实这也是人工智能最早主要的研究方法，只不过90年代之后大家逐渐的开始更多的采用机器学习的方法，而不是采用逻辑和规则的方法。现在基本上在自然语言处理研究当中，这两个占的比例是二八开。逻辑规则和机器学习的比例，20%是逻辑和规则，80%是机器学习，也有两者结合。\n第三种，语言学的方法。因为自然语言处理离不开语言学，我们可以把自然语言处理看成语言学下面的一个分支，不单单看成人工智能下面的一个分支。语言学一句话归纳起来就是对人的语言现象的研究。它不关心怎么写得好，关心的是你写了什么。所有人类语言现象的研究都可以归为语言学，对于语言学家来说他们是很多自然语言处理任务的设计师，由他们提出问题，把框架勾勒出来；当然解决问题则要靠研究人员用机器学习、规则和逻辑的方法把这个框架填上，把问题解决掉。\n四、自然语言处理（NLP）的成功应用领域：搜索引擎、机器翻译、语音识别和问答系统\n常见的比较成功的自然语言处理的应用包括搜索引擎、机器翻译、语音识别和问答系统。其中，语音识别技术传统上来说算是自然语言处理下面的一个任务，但是近些年已经单独列成一个研究领域，因为在目标和研究方法上和自然语言处理是迥异的，所以往往把语音识别单列成跟NLP并排的研究领域。\n五、自然语言处理（NLP）的历史：与深度学习关系密切，但受其改进不大\n下面这个图里的概念大家已经非常熟悉了，其实它们都是人工智能下面的子领域，两者是平行的。而深度学习是机器学习的一个子领域。也就是说对于自然语言处理来说，用非深度学习的方法来做自然语言处理的任务也是没有问题的。\n自然语言处理和深度学习之间是什么关系呢？深度学习为自然语言处理提供了很多新的模型和方法。因为深度学习最早在计算机视觉和计算机语音方面取得了非常重大的突破，所以很早就被用在NLP的各个研究领域当中了。到今天为止，可以说它基本上在所有NLP的任务当中都取得了成功。现在对于NLP的各种任务，能见到的最好的模型几乎都用到深度学习了。\n但是跟其他领域内不一样的是：NLP上面深度学习带来的改进并不大。比如说我们在视觉或者在语音上面错误率的降低可以达到40%、50%，但是在NLP上面超过10%的改进都是非常少见的，很多都是1%、2%的改进。另外还有一个非常要命的问题，其实深度学习都是非常复杂的非线性模型，这对于研究人员来说也是黑盒。所以说人类很难理解一个模型背后所代表的语言学现象以及怎样用语言学的理论去解释深度学习的模型。之所以做不到这一点，是因为我们没有办法把深度学习模型对于很多问题的解决方案放进传统的语言学框架里面，这对于研究人员来说是很大的一个困扰。\n六、自然语言处理（NLP）的现状：除了语音和机器翻译领域之外，很多方面的进展并不大\n目前我们已经有非常好的语音识别系统了，现在基本上达到了人类的水平，在理想环境里可以达到95%以上的正确率。同样我们也有比较正确的机器翻译系统，正确率换算过来也可以有70%到80%，虽然离人的水平还有一定的差距，但是已经是可用的状态。除了这两个以外，自然语言处理（NLP）的应用目前进展不大。举一个最简单的例子，比如词性标注，在一个句子当中，动词、名词、形容词，这个任务是非常简单、非常基础的任务。但是句子级别（一句话一个词不错才算对）目前的正确率只有57%，而且从2009年到2017年间正确率提高了不到1%，无论使用深度学习、各种模型、各种方法，花了八年时间也是只是提高了不到1%。\n另外一个例子是句法分析，就组合式句法分析来说，我们今天没有比十一年前做得更好，无论是用深度学习还是其他任何方法，十一年没有改进过。谷歌在去年推出了谷歌SyntaxNet，号称是世界上面最优秀的句法分析器，其实对比四年前最好的系统也只提了2%，当然谷歌用了目前最好的深度学习技术，也仅仅做到了这样。还有多轮对话系统，目前正确率最多只能做到60%，这其实是完全不可用的状态。深度学习的模型，其实在NLP的各个领域都取得了成功，不是说不成功，只是没有取得在视觉、语音领域那么大的成功。\n七、自然语言处理（NLP）在教育领域中的应用\n在讨论AI的时候，我心里面的第一反应其实是它跟教育是最契合的一个点，但大家好像提得比较少。我觉得语言是大家学习的对象，母语或外语都是对自然语言的研究。第二教师的授课、教材也都是自然语言，所以说我很惊讶的发现大家对AI展望的时候有时候比较忽略NLP方面的一些信息。这也是今天我为什么会讲这个主题的原因。\n我把NLP的教育应用分成三大类：\n（1）跟语言教学相关的应用。包括外语和母语教育（自动评分，辅导口语写作等）\n（2）教育文本处理。\n一是教材的编订。举个例子，在所有剑桥官方出版的英语教材的封皮上面都有黄色的小标志，估计大家买书的时候直接忽略掉了，那上面写的是什么意思呢？它表示这本书用剑桥国际语料库通过语言学和自然语言处理的方法来检测书本里面内容的正确性和适用性，而且是在非常大的大数据、语料库上面完成的。\n二是文本阅读分级，大家比较熟悉的是蓝思。\n三是文本简化，生成题目。\n（3）对话系统，使用自然语言进行教学。让每个学生都能够有一个个人学习助理，有问题可以问它。但是目前来说这方面的应用，见到的系统比较少，因为在基础研究上面还是需要更大的进步才能让它有更好的应用。\n下面看几个具体的应用:\nNLP和教育结合的第一个应用是作文打分，这是成熟的应用。\nETS E-Rater用在托福、GMAT、GRE考试当中了。现在考托福，写作里面一部分分数是电脑自动评分的。ETS E-Rater和人工的打分数据非常接近了。我们校宝在线1Course也可以达到ETS E-Rater水平，而且可以给出详细的反馈。我们不仅会给出分数，而且会给出非常详细的学习建议以及得分的要点。\n第二个应用是作文的纠错。\n比如学生作文当中拼写、语法以及其他的各种错误，通过计算机看了之后可以给出相关的修改建议，包括润色，会建议学生更高级的表达、更符合的表达。这方面的提供商蛮多的，我们最早在2011年的时候推出了一个完全免费的针对个人用户的产品1Checker，今天完全可以用，但是已经很多年没有更新过了。还有其他的供应商，包括国内有句酷批改网，国际上面也有Grammarly等等。我就说1Checker，原理是通过一个语言模型，用计算机阅读学生的作文，找出可能错的一些点，然后对这些点生成不同的建议，最后用模型根据用户不同的水平过虑和重新对建议进行排序，这是对于纠错方面基本的原理。我自己比较惊讶的是去年华南理工大学对于市面上面很多作文纠错的供应商做了一个对比实验，发现1Checker已经三年没有更新了，但是依然领先于其他的供应商。因为作文纠错是作文评分的基础，我相信如果他们采用我们非个人版的系统还会有更大的提升。作文纠错和作文打分，是NLP在教育当中的应用最成功也是最受人关注的两块。\n其他的应用包括简答题的评分，简答题的自动评分其实是只能针对于有固定答案的非开放性的简答题。\n什么叫做开放性的简答题？比如说你最难忘的一件事情，这是开放性的。非开放性的，指的有几套固定答案的，或者让你描述一个现象，这些都属于可以自动批改的简答题。原理上面跟机器翻译很相似，把学生的答案和正确的答案进行比较。目前来说国际上面有两套比较通行的简答题评分的引擎，一个是牛津的那套，精度非常高，对于每道题都要手写规则。还有一个非常成功的是ETS E-Rater，在某些任务当中可以达到人的水平。\n下面一个常见的应用是阅读分级。大家可能听说过蓝思（Lexile）阅读分级，这里面涉及到两个关键信息：词汇频率和平均句子长度。其实词的频度是词汇难度的表现，在大的语料库和文本当中，比如说所有的人民日报或其他报纸，如果一个词汇少见可能就是比较难的词。平均句长是语法复杂度的体现。大家觉得蓝思（Lexile）阅读分级的算法不难，但它的效果是非常好的，它可以给利用计算机给很多的文本、书籍进行自动处理、分析这些书籍的难度，然后对于不同水平的学习者给他们提供不同难度的学习资料。\n另外一个应用是“词汇测试”，我在国内看到的比较少，在欧洲、美国看得蛮多的，它是对于词汇自动生成选择题。给定一篇文章，计算机自动根据学习者的水平找到合适的句子，找到合适的词然后自动生成迷惑项，自动生成学生的练习题。这个好处是老师不需要提前对于阅读理解、阅读材料或者词汇掌握情况准备，只需要准备阅读材料就好了。\n原理和步骤：\n（1）找到学习者能够读懂的句子；\n（2）找到适合他水平的待测试的词；\n（3）生成迷惑项。迷惑项的生成很有讲究，迷惑项要足够迷惑才可以，它们在非常小的上下文里面都是可以讲得通的，但是放在整句当中正确的只有一个，最大化他的迷惑性，最大化测试的效果，这个应用在国内的见的不是特别的多。\n八、自然语言处理（NLP）和教育结合方面的研究方向：\n自动纠错、自动打分、问答系统、对话系统\n目前研究的方向还是主要集中于自动纠错和自动打分，我估算了一下，大体占到每年Paper发表量的70%。从目前自动纠错研究来看，只有40%到60%比例的错误是可以被检测并改正的，离人的水平、教师的水平依然是非常遥远的。\n从目前自动打分研究来看，特定任务上面，比如是托福、雅思这种应试作文上面基本上已经达到了人的水平，但是对于更有挑战性的文本目前也处于一个停滞不前的状态，也没有很大的突破。\n另外一个问答系统、对话系统，和学生的个人助手，类似这种研究相对来说并不是特别多，主要原因是由于这些方面需要基础研究层面有更大的突破，才能在教育应用中更好的找到自己的一席之地。目前主要的两个研究机构是ETC和Cambridge assesment。\n九、对自然语言处理（NLP）未来的预期：应用需求很广，但还有很多难题需要继续突破，尤其是黑盒问题\n最后给大家分享一点我自己的结论，通过我刚才跟大家说的，深度学习可以说在人工智能应用上面已经非常成功了，但是在NLP和教育结合的点上，不能通过深度学习在人工智能应用上的成功来推测NLP会在教育应用中或者深度学习通过NLP在教育中的应用就能成功，这个点我是完全看不到的。因为首先在NLP的研究领域上面，深度学习就没有带来像视觉、语音的突破。如果再应用到教育上面，那可能是更未来的事情，但我相信这也不是一个坏事，未来还是蛮有希望的，我希望深度学习包括机器学习，在对话系统、问答系统，有在视觉、语音上面那么大的突破。通过解决根本的问题，然后可以用在教育中，这是非常大的需求，个人的智能助理，可以给你一些必要的帮助，就像一个虚拟的老师一样。\n另外还有一个非常难受的问题即黑盒问题，这是教育行业一个非常特殊的需求。因为深度学习这种模型都是高度非线性的、非常复杂的模型，尤其现在流行的是端到端，你给我输入输出就行了，中间完全用模型搞定，人干预的地方很少。那问题来了，对于教育来说往往需要的不仅仅是一个准确的结果，还需要你推理的过程。比如说我做打分，分数正确是很重要，但是对于学生来说需要知道为什么得了这个分数，具体哪写的不好，怎么改进。对于全黑盒的模型来说，即便是深度学习最终革新了NLP，大大提高了NLP任务的准确度，可是对于老师还是学生来说还是很难读懂和解释的。这个黑盒问题怎么解决，也是需要研究人员想办法的。","data":"2018年01月08日 18:21:18"}
{"_id":{"$oid":"5d344fc562f717dc0659b6ff"},"title":"国内外自然语言处理(NLP)研究组","author":"CopperDong","content":"*排名不分先后。收集不全，欢迎留言完善。\n\n\n清华大学自然语言处理与社会人文计算实验室\nhttp://nlp.csai.tsinghua.edu.cn/site2/\n\n清华大学智能技术与系统国家重点实验室信息检索组\nhttp://www.thuir.cn/cms/\n北京大学计算语言学教育部重点实验室\n\nhttp://www.klcl.pku.edu.cn/\n\n北京大学计算机科学技术研究所语言计算与互联网挖掘研究室\n\nhttp://www.icst.pku.edu.cn/lcwm/index.PHP?title=%E9%A6%96%E9%A1%B5\n\n哈工大社会计算与信息检索研究中心\nhttp://ir.hit.edu.cn/\n\n哈工大机器智能与翻译研究室\nhttp://www.contem.org/\n\n哈尔滨工业大学智能技术与自然语言处理实验室\n\nhttp://www.insun.hit.edu.cn/home/\n\n中科院计算所自然语言处理研究组\nhttp://nlp.ict.ac.cn/index_zh.php\n\n中科院自动化研究所语音语言技术研究组\nhttp://nlpr-web.ia.ac.cn/cip/introduction.htm\n\n南京大学自然语言处理研究组\nhttp://nlp.nju.edu.cn/homepage/\n\n复旦大学自然语言处理研究组\nhttp://nlp.fudan.edu.cn/\n东北大学自然语言处理实验室\nhttp://www.nlplab.com/\n\n厦门大学智能科学与技术系自然语言处理实验室\nhttp://nlp.xmu.edu.cn/\n苏州大学自然语言处理实验室\nhttp://nlp.suda.edu.cn/\n苏州大学人类语言技术研究所\nhttp://hlt.suda.edu.cn/\n郑州大学自然语言处理实验室\n\nhttp://nlp.zzu.edu.cn/\n\nHuawei Noah’s Ark Lab\n\nhttp://www.noahlab.com.hk\nHuman Language Technology Center  at Hong Kong University of Science \u0026 Technology\n\nhttp://www.cse.ust.hk/~hltc/\n\nNUS Natural Language Processing Group\n\nhttp://www.comp.nus.edu.sg/~nlp/index.html\n\nThe Stanford Natural Language Processing Group\nhttp://nlp.stanford.edu/\nThe Berkeley NLP Group\nhttp://nlp.cs.berkeley.edu/index.shtml\nNatural Language Processing research at Columbia University\n\nhttp://www1.cs.columbia.edu/nlp/index.cgi\nNatural Language and Information Processing Research Group at University of Cambridge\nhttp://www.cl.cam.ac.uk/research/nl/\nSpeech Research Group at University of Cambridge\n\nhttp://mi.eng.cam.ac.uk/Main/Speech/\nThe Language Technologies Institute (LTI) at Carnegie Mellon University\nhttp://www.lti.cs.cmu.edu/\nThe Computational Linguistics Group at Oxford University\nhttp://www.clg.ox.ac.uk/\nHuman Language Technology and Pattern Recognition Group at the RWTH Aachen\nhttps://www-i6.informatik.rwth-aachen.de/\nAlgorithms for Computational Linguistics at City University of New York\n\nhttp://acl.cs.qc.edu/\n\n\nAlgorithms for Computational Linguistics at Oregon State University\nhttp://web.engr.oregonstate.edu/~huanlian/\nRPI Blender Lab\nhttp://nlp.cs.rpi.edu/\n\nThe Natural Language Group at USC/ISI\nhttp://nlg.isi.edu/\n\nNatural Language Processing Group at University of Notre Dame\n\nhttp://nlp.nd.edu/\n\nArtificial Intelligence Research Group at Harvard\nhttp://www.eecs.harvard.edu/ai/\n\nNatural Language Processing - Research at Google\nhttps://research.google.com/pubs/NaturalLanguageProcessing.html\nThe Redmond-based Natural Language Processing group\nhttp://research.microsoft.com/en-us/groups/nlp/\nComputational Linguistics and Information Processing at Maryland\nhttps://wiki.umiacs.umd.edu/clip/index.php/Main_Page\nLanguage and Speech Processing at Johns Hopkins University\n\nhttp://www.clsp.jhu.edu/about-clsp/\n\nHuman Language Technology Center of Excellence at Johns Hopkins University\n\nhttp://hltcoe.jhu.edu/\n\nStatistical Machine Translation Group at the University of Edinburgh\nhttp://www.statmt.org/ued/?n=Public.HomePage\nUniversity of Sheffield NLP Group\n\nhttp://nlp.shef.ac.uk/index.html\n\nThe CNGL Centre for Global Intelligent Content\n\nhttps://www.cngl.ie/\n\nCornell NLP group\n\nhttps://confluence.cornell.edu/display/NLP/Home/\n\nNatural Language Processing (NLP) group at University Of Washington\nhttps://www.cs.washington.edu/research/nlp\nNLP @ Illinois\n\nhttp://nlp.cs.illinois.edu/\n\n\n原文连接：http://blog.csdn.net/wangxinginnlp/article/details/44890553","data":"2017年12月14日 17:34:33"}
{"_id":{"$oid":"5d344ffb62f717dc0659b70c"},"title":"自然语言处理（二 RNN语言模型）","author":"zchenack","content":"RNN语言模型\nRNN语言模型\n语言模型\nRNN语言模型\n模型扩展\n语言模型\n语言模型就是指语言产生的规律，一般用来预测所使用语言语序的概率，或者是当前上下文使用某个词语的概率。换句话说，就是用来表示语言产生顺序的建模，用某个词是否恰当，这样的语序构造句子是否妥当这样的。于是，训练出一个语言模型就需要相当大的样本数据。语言模型可以分为：文法型的语言模型（就是定义相关的文法结构，例如主语+谓语+宾语构成陈述句这样的），统计模型，神经网络语言模型。\n其中统计类的语言模型包括N-gram，N-pos，隐马尔科夫链模型、最大熵模型等。就是给出前边的词，判断后面出现词的概率。\np(w3|w1w2)\np(w_3|w_1w_2)表示\nw3\nw_3在词语\nw1w2\nw_1w_2之后出现的概率。具体计算公式为\np(w3|w1w2)=p(w1w2w3)p(w1w2)=Count(w1w2w3)Count(w1w2)\np(w_3|w_1w_2)=\\frac{p(w_1w_2w_3)}{p(w_1w_2)}=\\frac{Count(w_1w_2w_3)}{Count(w_1w_2)}, Count(x)表示x在语料库中出现的频率。这种模型能给出后面单词发生的概率。但是会出现Count(x)=0的情况，为避免这种问题出现了很多平滑技术，例如Laplace平滑等。\n但是这种统计模型的计算非常消耗内存。\nRNN语言模型\nRNN语言模型就是利用RNN神经网络对语言建模，用于描述语言序列的产生过程。RNN神经网络就是循环神经网络，能很好地拟合序列数据。\n\n假设当前你有大量文本语料库C，根据这个预料你构建了词典V，然后你做分句，把每句话通过扩展变成等长的句子。句子开始以START标志，结束以EOS结束，使用PAD来进行短句子的填充。现在得到长度为L的sequence序列。每个词使用vector进行表示（1-of-N model）序列为\nx1,x2,...,xL\nx_1,x_2,...,x_L，假设\nx1\nx_1是词典V中的第一个词，V的大小为N，则\nx1=[1,0,0,...,0]\nx_1=[1,0,0,...,0].对于RNN输出数据对应的True Value这里选择使用\nx2,x3,...,xL−1,EOS\nx_2,x_3,...,x_{L-1},EOS, 使用符号表示为\ny1,y2,...,yL\ny_1,y_2,...,y_L 对于RNN预测数据表示为\ny′1,y′2,...,y′L\ny'_1,y'_2,...,y'_L。\n\nht=f(whh∗ht−1+wxh∗xt)\n\\begin{equation} h_t = f(w_{hh}*h_{t-1}+w_{xh}*x_t) \\end{equation}\n\ny′t=g(ht)\n\\begin{equation} y'_t = g(h_t) \\end{equation}\n\ny′t\ny'_t是一个N维（词典的大小）向量，表示一个概率分布，即下一个词语出现的概率在词典中的概率分布。\ny′t(n)\ny'_t(n)表示下一个词是词典中第n个词的概率大小。\n损失函数定义维：\n\nLoss=−1L∑t=1L∑j=1Nyt(j)log(y′t(j))\n\\begin{equation} Loss = -\\frac{1}{L}\\sum_{t=1}^{L}\\sum_{j=1}^{N}y_t(j)log(y'_t(j)) \\end{equation}\n求导根据BackPropogation+SGD进行训练。最小化损失函数。\n模型扩展\n一般对于RNN的训练采用BPTT的算法。\n当L较大时，模型的训练会出现梯度消失和梯度爆炸的问题。\n对于梯度爆炸可以采取Clipping的方法解决，具体就是设置门限，超过这个门限时，进行该梯度方向上的归一化。\n对于梯度消失，可以采用LSTM或GRU来替代SRNN；或者使用ReLU来替代Sigmoid激励函数。","data":"2017年11月26日 18:13:35"}
{"_id":{"$oid":"5d34501362f717dc0659b710"},"title":"自然语言处理学习9：NLTK中BigramCollocationFinder的使用","author":"zhuzuwei","content":"from nltk.collocations import BigramCollocationFinder\n1. nltk.collocations.BigramCollocationFinder(word_fd,bigram_fd,window_size=2)\n用于查找和排列bigram搭配或其他关联度量的工具。\n2. BigramCollocationFinder.from_words(words, window_size=2): 把词列表变为双词搭配。\n为给定序列中的所有bigrams构建一个BigramCollocationFinder。\n3.BigramCollocationFinder.score_ngram(score_fn, w1, w2):\n使用给定的评分函数返回给定二元组的分数。\n4. BigramCollocationFinder.apply_ngram_filter(fn):\n对任意n元词组合应用函数fn,如果fn(w1,...,wn)返回结果为True，则删除此n元词组合。\n5.BigramCollocationFinder.apply_word_filter(fn):\n对任意n元词组合，应用函数fn,如果(fn(w1),...,fn(wn))中有一个结果是True，\n则删除此n元词组合。\n6. BigramCollocationFinder.apply_freq_filter(min_freq):\n删除频数小于min_freq的候选项。\n操作实例如下：\nimport re import jieba text2 = re.sub('[.．：。；;！!?？%\\]\\[\\t\\n\\\"\\')(】【）（+\\-\\*/\u003c\u003e《》]', '', text1) word_list = jieba.lcut(text2) word_list = [word for word in word_list if len(word)\u003e1] from nltk.collocations import BigramCollocationFinder bigram_finder = BigramCollocationFinder.from_words(word_list) bigram_finder.ngram_fd\n去除频数小于4的二元词组\nbigram_finder.apply_freq_filter(4) bigram_finder.ngram_fd FreqDist({('占崩岗', '滑坡'): 4, ('崩岗', '滑坡'): 24, ('崩岗', '面积'): 4, ('森林', '植被'): 6, ('滑坡', '面积'): 6})\n去除含有'植被'的词组，去除含有 '占崩岗的词组'\ndef fn1(*words): return '植被' in words def fn2(word): return word == '占崩岗' bigram_finder.apply_ngram_filter(fn1) bigram_finder.ngram_fd FreqDist({('占崩岗', '滑坡'): 4, ('崩岗', '滑坡'): 24, ('崩岗', '面积'): 4, ('滑坡', '面积'): 6}) bigram_finder.apply_word_filter(fn2) bigram_finder.ngram_fd FreqDist({('崩岗', '滑坡'): 24, ('崩岗', '面积'): 4, ('滑坡', '面积'): 6})","data":"2018年06月11日 18:55:59"}
{"_id":{"$oid":"5d34503062f717dc0659b71a"},"title":"自然语言处理(NLP)入门","author":"SinGaln","content":"本文简要介绍Python自然语言处理(NLP)，使用Python的NLTK库。NLTK是Python的自然语言处理工具包，在NLP领域中，最常使用的一个Python库。\n什么是NLP？\n简单来说，自然语言处理(NLP)就是开发能够理解人类语言的应用程序或服务。\n这里讨论一些自然语言处理(NLP)的实际应用例子，如语音识别、语音翻译、理解完整的句子、理解匹配词的同义词，以及生成语法正确完整句子和段落。\n这并不是NLP能做的所有事情。\nNLP实现\n搜索引擎: 比如谷歌，Yahoo等。谷歌搜索引擎知道你是一个技术人员，所以它显示与技术相关的结果；\n社交网站推送:比如Facebook News Feed。如果News Feed算法知道你的兴趣是自然语言处理，就会显示相关的广告和帖子。\n语音引擎:比如Apple的Siri。\n垃圾邮件过滤:如谷歌垃圾邮件过滤器。和普通垃圾邮件过滤不同，它通过了解邮件内容里面的的深层意义，来判断是不是垃圾邮件。\nNLP库\n下面是一些开源的自然语言处理库(NLP)：\nNatural language toolkit (NLTK);\nApache OpenNLP;\nStanford NLP suite;\nGate NLP library\n其中自然语言工具包(NLTK)是最受欢迎的自然语言处理库(NLP)，它是用Python编写的，而且背后有非常强大的社区支持。\nNLTK也很容易上手，实际上，它是最简单的自然语言处理(NLP)库。\n在这个NLP教程中，我们将使用Python NLTK库。\n安装 NLTK\n如果您使用的是Windows/Linux/Mac，您可以使用pip安装NLTK:\npip install nltk\n打开python终端导入NLTK检查NLTK是否正确安装：\nimport nltk\n如果一切顺利，这意味着您已经成功地安装了NLTK库。首次安装了NLTK，需要通过运行以下代码来安装NLTK扩展包:\nimport nltk nltk.download()\n这将弹出NLTK 下载窗口来选择需要安装哪些包:\n\n您可以安装所有的包，因为它们的大小都很小，所以没有什么问题。\n使用Python Tokenize文本\n首先，我们将抓取一个web页面内容，然后分析文本了解页面的内容。\n我们将使用urllib模块来抓取web页面:\nimport urllib.request response = urllib.request.urlopen('http://php.net/') html = response.read() print (html)\n从打印结果中可以看到，结果包含许多需要清理的HTML标签。\n然后BeautifulSoup模块来清洗这样的文字:\nfrom bs4 import BeautifulSoup import urllib.request response = urllib.request.urlopen('http://php.net/') html = response.read() soup = BeautifulSoup(html,\"html5lib\")\n这需要安装html5lib模块\ntext = soup.get_text(strip=True) print (text)\n现在我们从抓取的网页中得到了一个干净的文本。\n下一步，将文本转换为tokens,像这样:\nfrom bs4 import BeautifulSoup import urllib.request response = urllib.request.urlopen('http://php.net/') html = response.read() soup = BeautifulSoup(html,\"html5lib\") text = soup.get_text(strip=True) tokens = text.split() print (tokens)\n统计词频\ntext已经处理完毕了，现在使用Python NLTK统计token的频率分布。\n可以通过调用NLTK中的FreqDist()方法实现:\nfrom bs4 import BeautifulSoup import urllib.request import nltk response = urllib.request.urlopen('http://php.net/') html = response.read() soup = BeautifulSoup(html,\"html5lib\") text = soup.get_text(strip=True) tokens = text.split() freq = nltk.FreqDist(tokens) for key,val in freq.items(): print (str(key) + ':' + str(val))\n如果搜索输出结果，可以发现最常见的token是PHP。\n您可以调用plot函数做出频率分布图:\nfreq.plot(20, cumulative=False) # 需要安装matplotlib库\n这上面这些单词。比如of,a,an等等，这些词都属于停用词。\n一般来说，停用词应该删除，防止它们影响分析结果。\n处理停用词\nNLTK自带了许多种语言的停用词列表，如果你获取英文停用词:\nfrom nltk.corpus import stopwords stopwords.words('english')\n现在，修改下代码,在绘图之前清除一些无效的token:\nclean_tokens = list() sr = stopwords.words('english') for token in tokens: if token not in sr: clean_tokens.append(token)\n最终的代码应该是这样的:\nfrom bs4 import BeautifulSoup import urllib.request import nltk from nltk.corpus import stopwords response = urllib.request.urlopen('http://php.net/') html = response.read() soup = BeautifulSoup(html,\"html5lib\") text = soup.get_text(strip=True) tokens = text.split() clean_tokens = list() sr = stopwords.words('english') for token in tokens: if not token in sr: clean_tokens.append(token) freq = nltk.FreqDist(clean_tokens) for key,val in freq.items(): print (str(key) + ':' + str(val))\n现在再做一次词频统计图，效果会比之前好些，因为剔除了停用词:\nfreq.plot(20,cumulative=False)\n使用NLTK Tokenize文本\n在之前我们用split方法将文本分割成tokens，现在我们使用NLTK来Tokenize文本。\n文本没有Tokenize之前是无法处理的，所以对文本进行Tokenize非常重要的。token化过程意味着将大的部件分割为小部件。\n你可以将段落tokenize成句子，将句子tokenize成单个词，NLTK分别提供了句子tokenizer和单词tokenizer。\n假如有这样这段文本:\nHello Adam, how are you? I hope everything is going well. Today is a good day, see you dude\n使用句子tokenizer将文本tokenize成句子:\nfrom nltk.tokenize import sent_tokenize mytext = \"Hello Adam, how are you? I hope everything is going well. Today is a good day, see you dude.\" print(sent_tokenize(mytext))\n输出如下:\n['Hello Adam, how are you?', 'I hope everything is going well.', 'Today is a good day, see you dude.']\n这是你可能会想，这也太简单了，不需要使用NLTK的tokenizer都可以，直接使用正则表达式来拆分句子就行，因为每个句子都有标点和空格。\n那么再来看下面的文本:\nHello Mr. Adam, how are you? I hope everything is going well. Today is a good day, see you dude.\n这样如果使用标点符号拆分,Hello Mr将会被认为是一个句子，如果使用NLTK:\nfrom nltk.tokenize import sent_tokenize mytext = \"Hello Mr. Adam, how are you? I hope everything is going well. Today is a good day, see you dude.\" print(sent_tokenize(mytext))\n输出如下:\n['Hello Mr. Adam, how are you?', 'I hope everything is going well.', 'Today is a good day, see you dude.']\n这才是正确的拆分。\n接下来试试单词tokenizer:\nfrom nltk.tokenize import word_tokenize mytext = \"Hello Mr. Adam, how are you? I hope everything is going well. Today is a good day, see you dude.\" print(word_tokenize(mytext))\n输出如下:\n['Hello', 'Mr.', 'Adam', ',', 'how', 'are', 'you', '?', 'I', 'hope', 'everything', 'is', 'going', 'well', '.', 'Today', 'is', 'a', 'good', 'day', ',', 'see', 'you', 'dude', '.']\nMr.这个词也没有被分开。NLTK使用的是punkt模块的PunktSentenceTokenizer，它是NLTK.tokenize的一部分。而且这个tokenizer经过训练，可以适用于多种语言。\n非英文Tokenize\nTokenize时可以指定语言:\nfrom nltk.tokenize import sent_tokenize mytext = \"Bonjour M. Adam, comment allez-vous? J'espère que tout va bien. Aujourd'hui est un bon jour.\" print(sent_tokenize(mytext,\"french\"))\n输出结果如下:\n['Bonjour M. Adam, comment allez-vous?', \"J'espère que tout va bien.\", \"Aujourd'hui est un bon jour.\"]\n同义词处理\n使用nltk.download()安装界面，其中一个包是WordNet。\nWordNet是一个为自然语言处理而建立的数据库。它包括一些同义词组和一些简短的定义。\n您可以这样获取某个给定单词的定义和示例:\nfrom nltk.corpus import wordnet syn = wordnet.synsets(\"pain\") print(syn[0].definition()) print(syn[0].examples())\n输出结果是:\na symptom of some physical hurt or disorder ['the patient developed severe pain and distension']\nWordNet包含了很多定义：\nfrom nltk.corpus import wordnet syn = wordnet.synsets(\"NLP\") print(syn[0].definition()) syn = wordnet.synsets(\"Python\") print(syn[0].definition())\n结果如下:\nthe branch of information science that deals with natural language information large Old World boas\n可以像这样使用WordNet来获取同义词:\nfrom nltk.corpus import wordnet synonyms = [] for syn in wordnet.synsets('Computer'): for lemma in syn.lemmas(): synonyms.append(lemma.name()) print(synonyms)\n输出:\n['computer', 'computing_machine', 'computing_device', 'data_processor', 'electronic_computer', 'information_processing_system', 'calculator', 'reckoner', 'figurer', 'estimator', 'computer']\n反义词处理\n也可以用同样的方法得到反义词：\nfrom nltk.corpus import wordnet antonyms = [] for syn in wordnet.synsets(\"small\"): for l in syn.lemmas(): if l.antonyms(): antonyms.append(l.antonyms()[0].name()) print(antonyms)\n输出:\n['large', 'big', 'big']\n词干提取\n语言形态学和信息检索里，词干提取是去除词缀得到词根的过程，例如working的词干为work。\n搜索引擎在索引页面时就会使用这种技术，所以很多人为相同的单词写出不同的版本。\n有很多种算法可以避免这种情况，最常见的是波特词干算法。NLTK有一个名为PorterStemmer的类，就是这个算法的实现:\nfrom nltk.stem import PorterStemmer stemmer = PorterStemmer() print(stemmer.stem('working')) print(stemmer.stem('worked'))\n输出结果是:\nwork work\n还有其他的一些词干提取算法，比如 Lancaster词干算法。\n非英文词干提取\n除了英文之外，SnowballStemmer还支持13种语言。\n支持的语言:\nfrom nltk.stem import SnowballStemmer print(SnowballStemmer.languages) 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish'\n你可以使用SnowballStemmer类的stem函数来提取像这样的非英文单词：\nfrom nltk.stem import SnowballStemmer french_stemmer = SnowballStemmer('french') print(french_stemmer.stem(\"French word\"))\n单词变体还原\n单词变体还原类似于词干，但不同的是，变体还原的结果是一个真实的单词。不同于词干，当你试图提取某些词时，它会产生类似的词:\nfrom nltk.stem import PorterStemmer stemmer = PorterStemmer() print(stemmer.stem('increases'))\n结果:\nincreas\n现在，如果用NLTK的WordNet来对同一个单词进行变体还原，才是正确的结果:\nfrom nltk.stem import WordNetLemmatizer lemmatizer = WordNetLemmatizer() print(lemmatizer.lemmatize('increases'))\n结果:\nincrease\n结果可能会是一个同义词或同一个意思的不同单词。\n有时候将一个单词做变体还原时，总是得到相同的词。\n这是因为语言的默认部分是名词。要得到动词，可以这样指定：\nfrom nltk.stem import WordNetLemmatizer lemmatizer = WordNetLemmatizer() print(lemmatizer.lemmatize('playing', pos=\"v\"))\n结果:\nplay\n实际上，这也是一种很好的文本压缩方式，最终得到文本只有原先的50%到60%。\n结果还可以是动词(v)、名词(n)、形容词(a)或副词(r)：\nfrom nltk.stem import WordNetLemmatizer lemmatizer = WordNetLemmatizer() print(lemmatizer.lemmatize('playing', pos=\"v\")) print(lemmatizer.lemmatize('playing', pos=\"n\")) print(lemmatizer.lemmatize('playing', pos=\"a\")) print(lemmatizer.lemmatize('playing', pos=\"r\"))\n输出:\nplay playing playing playing\n词干和变体的区别\n通过下面例子来观察:\nfrom nltk.stem import WordNetLemmatizer from nltk.stem import PorterStemmer stemmer = PorterStemmer() lemmatizer = WordNetLemmatizer() print(stemmer.stem('stones')) print(stemmer.stem('speaking')) print(stemmer.stem('bedroom')) print(stemmer.stem('jokes')) print(stemmer.stem('lisa')) print(stemmer.stem('purple')) print('----------------------') print(lemmatizer.lemmatize('stones')) print(lemmatizer.lemmatize('speaking')) print(lemmatizer.lemmatize('bedroom')) print(lemmatizer.lemmatize('jokes')) print(lemmatizer.lemmatize('lisa')) print(lemmatizer.lemmatize('purple'))\n输出:\nstone speak bedroom joke lisa\npurpl\nstone speaking bedroom joke lisa purple\n词干提取不会考虑语境，这也是为什么词干提取比变体还原快且准确度低的原因。\n个人认为，变体还原比词干提取更好。单词变体还原返回一个真实的单词，即使它不是同一个单词，也是同义词，但至少它是一个真实存在的单词。\n如果你只关心速度，不在意准确度，这时你可以选用词干提取。\n在此NLP教程中讨论的所有步骤都只是文本预处理。在以后的文章中，将会使用Python NLTK来实现文本分析。\n我已经尽量使文章通俗易懂。希望能对你有所帮助。","data":"2017年11月21日 20:32:26","date":"2017年11月21日 20:32:26"}
{"_id":{"$oid":"5d34504d62f717dc0659b720"},"title":"人工智能知识体系","author":"double_happiness","content":"阶段一、人工智能基础 － 高等数学必知必会\n本阶段主要从数据分析、概率论和线性代数及矩阵和凸优化这四大块讲解基础，旨在训练大家逻辑能力，分析能力。拥有良好的数学基础，有利于大家在后续课程的学习中更好的理解机器学习和深度学习的相关算法内容。同时对于AI研究尤为重要，例如人工智能中的智能很大一部分依托“概率论”实现的。\n一、数据分析\n1）常数e\n2）导数\n3）梯度\n4）Taylor\n5）gini系数\n6）信息熵与组合数\n7）梯度下降\n8）牛顿法\n二、概率论\n1）微积分与逼近论\n2）极限、微分、积分基本概念\n3）利用逼近的思想理解微分，利用积分的方式理解概率\n4）概率论基础\n5）古典模型\n6）常见概率分布\n7）大数定理和中心极限定理\n8）协方差(矩阵)和相关系数\n9）最大似然估计和最大后验估计\n三、线性代数及矩阵\n1）线性空间及线性变换\n2）矩阵的基本概念\n3）状态转移矩阵\n4）特征向量\n5）矩阵的相关乘法\n6）矩阵的QR分解\n7）对称矩阵、正交矩阵、正定矩阵\n8）矩阵的SVD分解\n9）矩阵的求导\n10）矩阵映射/投影\n四、凸优化\n1）凸优化基本概念\n2）凸集\n3）凸函数\n4）凸优化问题标准形式\n5）凸优化之Lagerange对偶化\n6）凸优化之牛顿法、梯度下降法求解\n阶段二、人工智能提升 － Python高级应用\n随着AI时代的到来以及其日益蓬勃的发展，Python作为AI时代的头牌语言地位基本确定，机器学习是着实令人兴奋，但其复杂度及难度较大，通常会涉及组装工作流和管道、设置数据源及内部和云部署之间的分流而有了Python库后，可帮助加快数据管道，且Python库也在不断更新发布中，所以本阶段旨在为大家学习后续的机器学习减负。\n一、容器\n1）列表:list\n2）元组:tuple\n3）字典: dict\n4）数组: Array\n5）切片\n6）列表推导式\n7）浅拷贝和深拷贝\n二、函数\n1）lambda表达式\n2）递归函数及尾递归优化\n3）常用内置函数/高阶函数\n4）项目案例：约瑟夫环问题\n三、常用库\n1）时间库\n2）并发库\n3）科学计算库\n4）Matplotlib可视化绘图库\n5）锁和线程\n6）多线程编程\n阶段三、人工智能实用 － 机器学习篇\n机器学习利用算法去分析数据、学习数据，随后对现实世界情况作出判断和预测。因此，与预先编写好、只能按照特定逻辑去执行指令的软件不同，机器实际上是在用大量数据和算法去“自我训练”，从而学会如何完成一项任务。\n所以本阶段主要从机器学习概述、数据清洗和特征选择、回归算法、决策树、随机森林和提升算法、SVM、聚类算、EM算法、贝叶斯算法、隐马尔科夫模型、LDA主题模型等方面讲解一些机器学习的相关算法以及这些算法的优化过程，这些算法也就是监督算法或者无监督算法。\n一、机器学习\n1）机器学习概述\n二、监督学习\n1）逻辑回归\n2）softmax分类\n3）条件随机场\n4）支持向量机svm\n5）决策树\n6）随机森林\n7）GBDT\n8）集成学习\n三、非监督学习\n1）高斯混合模型\n2）聚类\n3）PCA\n\u003cp\u003e4）密度估计\u003c/p\u003e \u003c/td\u003e \u003ctd\u003e \u003cp\u003e5）LSI\u0026nbsp;\u003cbr\u003e 6）LDA\u0026nbsp;\u003cbr\u003e 7）双聚类\u0026nbsp;\u003cbr\u003e 8）降维算法\u003c/p\u003e \u003c/td\u003e \u003c/tr\u003e\u003ctr\u003e\u003ctd colspan=\"2\"\u003e \u003ch3\u003e\u003ca name=\"t14\"\u003e\u003c/a\u003e四、数据处理与模型调优\u003c/h3\u003e \u003c/td\u003e \u003c/tr\u003e\u003ctr\u003e\u003ctd\u003e \u003cp\u003e1）特征提取\u003cbr\u003e 2）数据预处理\u003cbr\u003e 3）数据降维\u003cbr\u003e 4）模型参数调优\u003cbr\u003e 5）模型持久化\u003c/p\u003e \u003c/td\u003e \u003ctd\u003e \u003cp\u003e6）模型可视化\u003cbr\u003e 7）优化算法：坐标轴下降法和最小角回归法\u003cbr\u003e 8）数据挖掘关联规则算法\u003cbr\u003e 9）感知器模型\u003c/p\u003e \u003c/td\u003e \u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/div\u003e\u003ch1\u003e\u003ca name=\"t15\"\u003e\u003c/a\u003e阶段四、人工智能实用 － 数据挖掘篇\u003c/h1\u003e\n本阶段主要通过音乐文件分类和金融反欺诈模型训练等项目，帮助大家对于上阶段的机器学习做更深入的巩固，为后续深度学习及数据挖掘提供项目支撑。\n项目一：百度音乐系统文件分类\n音乐推荐系统就是利用音乐网站上的音乐信息，向用户提供音乐信息或者建议，帮助用户决定应该听什么歌曲。而个人化推荐则是基于音乐信息及用户的兴趣特征、听歌历史行为，向用户推荐用户可能会感兴趣的音乐或者歌手。推荐算法主要分为以下几种：基于内容的推荐、协同过滤推荐、基于关联规则推荐、基于效用推荐、基于知识推荐等；推荐系统常用于各个互联网行业中，比如音乐、电商、旅游、金融等。\n项目二：千万级P2P金融系统反欺诈模型训练\n目前比较火的互联网金融领域，实质是小额信贷，小额信贷风险管理，本质上是事前对风险的主动把控，尽可能预测和防范可能出现的风险。本项目应用GBDT、Randomforest等机器学习算法做信贷反欺诈模型，通过数据挖掘技术，机器学习模型对用户进行模型化综合度量，确定一个合理的风险范围，使风险和盈利达到一个平衡的状态。\n阶段五、人工智能前沿 － 深度学习篇\n深度学习是实现机器学习的技术，同时深度学习也带来了机器学习的许多实际应用，拓展了AI的使用领域，本阶段主要从TensorFlow、BP神经网络、深度学习概述、CNN卷积神经网络、递归神经网、自动编码机，序列到序列网络、生成对抗网络，孪生网络，小样本学习技术等方面讲解深度学习相关算法以，掌握深度学习前沿技术，并根据不同项目选择不同的技术解决方案。针对公司样本不足，采用小样本技术和深度学习技术结合，是项目落地的解决方案。\n1）TensorFlow基本应用\n2）BP神经网络\n3）深度学习概述\n4）卷积神经网络(CNN)\n5）图像分类(vgg,resnet)\n6）目标检测(rcnn,fast-rcnn,faster-rcnn,ssd)\n7）递归神经网络(RNN)\n8）lstm,bi-lstm,多层LSTM\n9）无监督学习之AutoEncoder自动编码器\n10）Seq2Seq\n11）Seq2Seq with Attension\n12）生成对抗网络\n13）irgan\n14）finetune及迁移学习\n15）孪生网络\n16）小样本学习\n阶段六、人工智能进阶 － 自然语言处理篇\n自然语言处理（NLP）是计算机科学领域与人工智能领域中的一个重要方向。它已成为人工智能的核心领域。自然语言处理解决的是“让机器可以理解自然语言”这一到目前为止都还只是人类独有的特权，被誉为人工智能皇冠上的明珠，被广泛应用。本阶段从NLP的字、词和句子全方位多角度的学习NLP，作为NLP的基础核心技术，对NLP为核心的项目，如聊天机器人，合理用药系统，写诗机器人和知识图谱等提供底层技术。通过学习NLP和深度学习技术，掌握NLP具有代表性的前沿技术。\n1）词（分词，词性标注）代码实战\n2）词（深度学习之词向量，字向量）代码实战\n3）词（深度学习之实体识别和关系抽取）代码实战\n4）词（关键词提取，无用词过滤）代码实战\n5）句（句法分析，语义分析）代码实战\n6）句（自然语言理解,一阶逻辑）代码实战\n7）句（深度学习之文本相似度）代码实战\n阶段七、人工智能进阶 － 图像处理篇\n数字图像处理(Digital Image Processing)是通过计算机对图像进行去除噪声、增强、复原、分割、提取特征等处理的方法和技术。广泛的应用于农牧业、林业、环境、军事、工业和医学等方面，是人工智能和深度学习的重要研究方向。深度学习作为当前机器学习领域最热门的技术之一，已经在图像处理领域获得了应用，并且展现出巨大的前景。本阶段学习了数字图像的基本数据结构和处理技术，到前沿的深度学习处理方法。掌握前沿的ResNet,SSD,Faster RCNN等深度学习模型，对图像分类，目标检测和模式识别等图像处理主要领域达到先进水平。实际工作中很多项目都可以转化为本课程的所学的知识去解决，如行人检测，人脸识别和数字识别。\n一、图像基础\n图像读，写，保存，画图（线，圆，多边形，添加文字）\n二、图像操作及算数运算\n图像像素读取，算数运算，ROI区域提取\n三、图像颜色空间运算\n图像颜色空间相互转化\n四、图像几何变换\n平移，旋转，仿射变换，透视变换等\n五、图像形态学\n腐蚀，膨胀，开/闭运算等\n六、图像轮廓\n长宽，面积，周长，外接圆，方向，平均颜色，层次轮廓等\n七、图像统计学\n图像直方图\n八、图像滤波\n高斯滤波，均值滤波，双边滤波，拉普拉斯滤波等\n阶段八、人工智能终极实战 － 项目应用\n本阶段重点以项目为导向，通过公安系统人脸识别、图像识别以及图像检索、今日头条CTR广告点击量预估、序列分析系统、聊天机器人等多个项目的讲解，结合实际来进行AI的综合运用。\n项目一：公安系统人脸识别、图像识别\n使用深度学习框架从零开始完成人脸检测的核心技术图像类别识别的操作，从数据预处理开始一步步构建网络模型并展开分析与评估，方便大家快速动手进行项目实践！识别上千种人靓，返回层次化结构的每个人的标签。\n项目二：公安系统图像检索\n本项目基于卷积神经网在训练过程中学习出对应的『二值检索向量』，对全部图先做了一个分桶操作，每次检索的时候只取本桶和临近桶的图片作比对，而不是在全域做比对，使用这样的方式提高检索速度，使用Tensorflow框架建立基于ImageNet的卷积神经网络，并完成模型训练以及验证。\n项目三：今日头条CTR广告点击量预估\n点击率预估是广告技术的核心算法之一，它是很多广告算法工程师喜爱的战场。广告的价值就在于宣传效果,点击率是其中最直接的考核方式之一,点击率越大,证明广告的潜在客户越多,价值就越大,因此才会出现了刷点击率的工具和技术。通过对于点击量的评估，完成对于潜在用户的价值挖掘。\n项目四：序列分析系统\n时间序列分析(Time Series Analysis)是一种动态数据处理的统计方法，主要基于随机过程理论和数理统计方法，研究随机数据序列所遵从的统计规律以便用于解决实际问题。主要包括自相关分析等一般的统计分析方法，构建模型从而进行业务推断。经典的统计分析是假定数据序列具有独立性，而时间序列分析则侧重于研究数据样本序列之间的依赖关系。时间序列预测一般反应了三种实际变化规律：趋势变化、周期性变化和随机性变化。时间序列预测常应用于国民经济宏观控制、企业经营管理、市场潜力量预测、天气预报、水文预报等方面，是应用于金融行业的一种核心算法之一。\n项目五：京东聊天机器人/智能客服\n聊天机器人/智能客服是一个用来模拟人类对话或者聊天的一个系统，利用深度学习和机器学习等NLP相关算法构建出问题和答案之间的匹配模型，然后可以将其应用到客服等需要在线服务的行业领域中，聊天机器人可以降低公司客服成本，还能够提高客户的体验友好性。 在一个完整的聊天机器人实现过程中，主要包含了一些核心技术，包括但不限于：爬虫技术、机器学习算法、深度学习算法、NLP领域相关算法。通过实现一个聊天机器人可以帮助我们队AI整体知识的一个掌握。\n项目六：机器人写诗歌\n机器人写诗歌/小说是一种基于NLP自然语言相关技术的一种应用，在实现过程中可以基于机器学习相关算法或者深度学习相关算法来进行小说/诗歌构建过程。人工智能的一个终极目标就是让机器人能够像人类一样理解文字，并运用文字进行创作，而这个目标大致上主要分为两个部分，也就是自然语言理解和自然语言生成，其中现阶段的主要自然语言生成的运用，自然语言生成主要有两种不同的方式，分别为基于规则和基于统计，基于规则是指首先了解词性及语法等规则，再依据这样的规则写出文章；而基于统计的本质是根据先前的字句和统计的结果，进而判断下一个子的生成，例如马尔科夫模型就是一种常用的基于统计的方法。\n项目七：机器翻译系统\n机器翻译又称自动翻译，是指利用计算机将一种自然语言转换为另外一种自然语言的过程，机器翻译是人工智能的终极目标之一，具有很高的研究价值，同时机器翻译也具有比较重要的实用价值，机器翻译技术在促进政治、经济、文化交流等方面起到了越来越重要的作用；机器翻译主要分为以下三个过程：原文分析、原文译文转换和译文生成；机器翻译的方式有很多种，但是随着深度学习研究取得比较大的进展，基于人工网络的机器翻译也逐渐兴起，特别是基于长短时记忆(LSTM)的循环神经网络(RDD)的应用，为机器翻译添了一把火。\n项目八：垃圾邮件过滤系统\n邮件主要可以分为有效邮件和垃圾邮件两大类，有效邮件指的邮件接收者有意义的邮件，而垃圾邮件转指那些没有任何意义的邮件，其内容主要包含赚钱信息、成人广告、商业或者个人网站广告、电子杂志等，其中垃圾邮件又可以发为良性垃圾邮件和恶性垃圾邮件，良性垃圾邮件指的就是对收件人影响不大的信息邮件，而恶性垃圾邮件指具有破坏性的电子邮件，比如包含病毒、木马等恶意程序的邮件。垃圾邮件过滤主要使用使用机器学习、深度学习等相关算法，比如贝叶斯算法、CNN等，识别出所接收到的邮件中那些是垃圾邮件。\n项目九：手工数字识别\n人认知世界的开始就是从认识数字开始的，深度学习也一样，数字识别是深度学习的一个很好的切入口，是一个非常经典的原型问题，通过对手写数字识别功能的实现，可以帮助我们后续对神经网络的理解和应用。选取手写数字识别的主要原因是手写数字具有一定的挑战性，要求对编程能力及神经网络思维能力有一定的要求，但同时手写数字问题的复杂度不高，不需要大量的运算，而且手写数字也可以作为其它技术的一个基础，所以以手写数字识别为基础，贯穿始终，从而理解深度学习相关的应用知识。\n项目十：癌症筛选检测\n技术可以改变癌症患者的命运吗，对于患有乳腺癌患者来说，复发还是痊愈影响这患者的生命，那么怎么来预测患者的患病结果呢，机器学习算法可以帮助我们解决这一难题，本项目应用机器学习logistic回归模型，来预测乳腺癌患者复发还是正常，有效的预测出医学难题。\n项目十一：葡萄酒质量检测系统\n随着信息科技的快速发展,计算机中的经典算法在葡萄酒产业中得到了广泛的研究与应用。其中机器学习算法的特点是运用了人工智能技术,在大量的样本集训练和学习后可以自动地找出运算所需要的参数和模型。\n项目十二：淘宝网购物篮分析推荐算法\n购物篮分析(Market Basket Analysis)即非常有名的啤酒尿布故事的一个反应，是通过对购物篮中的商品信息进行分析研究，得出顾客的购买行为，主要目的是找出什么样的物品会经常出现在一起，也就是那些商品之间是有很大的关联性的。通过购物篮分析挖掘出来的信息可以用于指导交叉销售、追加销售、商品促销、顾客忠诚度管理、库存管理和折扣计划等业务；购物篮分析的最常用应用场景是电商行业，但除此之外，该算法还被应用于信用卡商城、电信与金融服务业、保险业以及医疗行业等。\n项目十三：手工实现梯度下降回归算法\n梯度下降法（英语：Gradient descent）是一个一阶最优化算法，通常也称为最速下降法。 要使用梯度下降法找到一个函数的局部极小值，必须向函数上当前点对应梯度（或者是近似梯度）的反方向的规定步长距离点进行迭代搜索。如果相反地向梯度正方向迭代进行搜索，则会接近函数的局部极大值点；这个过程则被称为梯度上升法。\n项目十四：基于TensorFlow实现回归算法\n回归算法是业界比较常用的一种机器学习算法，通过应用于各种不同的业务场景，是一种成熟而稳定的算法种类；TensorFlow是一种常用于深度学习相关领域的算法工具；随着深度学习热度的高涨，TensorFlow的使用也会越来越多，从而使用TensorFlow来实现一个不存在的算法，会加深对TensorFlow的理解和使用；基于TensorFlow的回归算法的实现有助于后续的TensorFlow框架的理解和应用，并可以促进深度学习相关知识的掌握。\n项目十五：合理用药系统\n合理用药系统，是根据临床合理用药专业工作的基本特点和要求，运用NLP和深度学习技术对药品说明书，临床路径等医学知识进行标准化，结构化处理。如自动提取药品说明书文本里面的关键信息如：药品相互作用，禁忌，用法用量，适用人群等，实现医嘱自动审查，及时发现不合理用药问题，帮助医生、药师等临床专业人员在用药过程中及时有效地掌握和利用医药知识，预防药物不良事件的发生、促进临床合理用药工作。\n项目十六：行人检测\n行人检测是利用图像处理技术和深度学习技术对图像或者视频序列中是否存在行人并给予精确定位。学习完行人检测技术后，对类似的工业缺陷检测，外观检测和医疗影像检测等目标检测范畴类的项目可以一通百通。该技术可与行人跟踪，行人重识别等技术结合，应用于人工智能系统、车辆辅助驾驶系统、智能机器人、智能视频监控、人体行为分析、智能交通等领域。由于行人兼具刚性和柔性物体的特性 ，外观易受穿着、尺度、遮挡、姿态和视角等影响，使得行人检测成为计算机视觉领域中一个既具有研究价值同时又极具挑战性的热门课题。\n项目十七：时间序列算法模型\n拿到一个观察序列后，首先要对它的平稳性和纯随机性进行检验，这两个重要的检验称为序列的预处理。根据检验的结果可以将序列分为不同的类型，对不同的类型我们采用不同的分析方法。\n1）移动平均法 (MA)\n2）自回归模型(AR)\nAR模型是一种线性预测，即已知N个数据，可由模型推出第N点前面或后面的数据（设推出P点）。\n本质类似于插值，其目的都是为了增加有效数据，只是AR模型是由N点递推，而插值是由两点（或少数几点）去推导多点，所以AR模型要比插值方法效果更好。\n3）自回归滑动平均模型(ARMA)\n其建模思想可概括为：逐渐增加模型的阶数，拟合较高阶模型，直到再增加模型的阶数而剩余残差方差不再显著减小为止。\n4）指数平滑法\n移动平均法的预测值实质上是以前观测值的加权和，且对不同时期的数据给予相同的加权。这往往不符合实际情况。\n指数平滑法则对移动平均法进行了改进和发展，其应用较为广泛。\n基本思想都是：预测值是以前观测值的加权和，且对不同的数据给予不同的权，新数据给较大的权，旧数据给较小的权。\n根据平滑次数不同，指数平滑法分为：一次指数平滑法、二次指数平滑法和三次指数平滑法等\n项目十八：PySpark大数据机器学习框架\nSpark由AMPLab实验室开发，其本质是基于内存的快速迭代框架，“迭代”是机器学习最大的特点，因此非常适合做机器学习。得益于在数据科学中强大的表现，Python是一种解释型、面向对象、动态数据类型的高级程序设计语言，结合强大的分布式内存计算框架Spark，两个领域的强者走到一起，自然能碰出更加强大的火花（Spark可以翻译为火花）。\nSpark的Python API几乎覆盖了所有Scala API所能提供的功能，只有极少数的一些特性和个别的API方法，暂时还不支持。但通常不影响我们使用Spark Python进行编程。\n项目十九：天池、kaggle比赛\n2014年3月，阿里巴巴集团董事局主席马云在北京大学发起“天池大数据竞赛”。首届大赛共有来自全球的7276支队伍参赛，海外参赛队伍超过148支。阿里巴巴集团为此开放了5.7亿条经过严格脱敏处理的数据。2014年赛季的数据提供方为贵阳市政府，参赛者根据交通数据模拟控制红绿灯时间，寻找减轻道路拥堵的方法。\nKaggle是一个数据分析的竞赛平台，网址：https://www.kaggle.com/企业或者研究者可以将数据、问题描述、期望的指标发布到Kaggle上，以竞赛的形式向广大的数据科学家征集解决方 案，类似于KDD-CUP（国际知识发现和数据挖掘竞赛）。Kaggle上的参赛者将数据下载下来，分析数据，然后运用机 器学习、数据挖掘等知识，建立算法模型，解决问题得出结果，最后将结果提交，如果提交的结果符合指标要求并且在参赛者中排名第一，将获得比赛丰厚的奖金。\n项目二十：量化交易\n量化交易(Quantitative Trading)是指借助现代统计学和数学的方法，利用计算机技术来进行交易的证券投资方式。量化交易从庞大的历史数据中海选能带来超额收益的多种“大概率”事件以制定策略，用数量模型验证及固化这些规律和策略，然后严格执行已固化的策略来指导投资，以求获得可以持续的、稳定且高于平均收益的超额回报。\n量化交易起源于上世纪七十年代的股票市场，之后迅速发展和普及，尤其是在期货交易市场，程序化逐渐成为主流。有数据显示，国外成熟市场期货程序化交易已占据总交易量的70%-80%，而国内则刚刚起步。手工交易中交易者的情绪波动等弊端越来越成为盈利的障碍，而程序化交易天然而成的精准性、100%执行率则为它的盈利带来了优势。\n阶段九、百度云实战体系\n课程一、深入理解百度云计算基础产品/基于百度云弹性计算服务实现基础架构解决方案\n全面介绍BCC（CDS 、EIP）、BLB、RDS、BOS、VPC等百度云弹性计算服务，介绍百度云的安全防护方案，深入介绍传统架构下如何通过百度云弹性计算服务快速构建更稳定、安全的应用；\n认证培训专家将通过深入浅出，理论和实践相结合的课程帮助学员深入掌握百度云弹性计算服务。\n1）快速体验百度云服务器BCC的功能全貌\n2）基于BCC的云磁盘CDS的操作与管理\n3）基于BCC的磁盘快照、自定义镜像的操作与管理\n4）基于自定义镜像快速生成BCC的实验\n5）基于磁盘快照实现数据备份与恢复的最佳实践\n6）基于百度云安全组完成定义IP＋端口的入站和出站访问策略\n7）快速体验百度云私有网络VPC的功能全貌\n8）基于百度云VPC+VPN快速搭建Stie-to-Stie的混合云架构\n9）在百度云VPC网络下实现NAT地址映射的实践\n10）快速体验百度云数据库RDS的功能全貌\n11）云数据库RDS的备份与恢复操作体验\n12）熟悉数据传输服务DTS的使用\n13）快速体验百度云负载均衡BLB的功能全貌\n14）快速体验百度云存储BOS的功能全貌\n15）快速体验百度云数据库RDS的功能全貌\n16）快速体验百度云内容分发网络CDN\n17）基于BLB、BCC、RDS、BOS和CDN快速部署Discuz论坛实现弹性架构综合实验\n18）快速体验百度云安全BSS和DDOS防护服务\n19）快速体验百度云监控BCM\n课程二、基于百度云的迁移上云实战\n基于百度云弹性计算服务的基础产品，实现传统IT架构迁移到百度云上的实战，为客户业务上云提升能力，提升客户上云前的信心，上云中和上云后的技术能力。以真实的客户案例，结合设计好的动手实验课提升实战经验，介绍了业务上云的过程、方法、工具以及案例等。\n1）基于BCC快速部署LNMP基础环境\n2）基于BCC快速部署LAMP基础环境\n3）基于BCC快速部署MySQL数据库\n4）基于BCC快速部署MS SQL数据库服务\n5）基于BCC快速部署Tomcat基础环境\n6）云数据库RDS结合数据传输服务DTS实现数据迁移上云的最佳实践\n7）基于BOS桌面实现BOS的可视化管理\n8）基于BOS FS实现BOS服务挂载到本地文件系统\n9）基于BOS-Util实现BOS的批量文件操作的演示\n10）基于BOS CLI实现BOS文件的单机操作\n课程三、在百度云平台上进行开发\n全面介绍使用百度云产品进行应用开发，理解百度云主要产品特性，包括BCC、BOS、RDS、SCS在应用开发中的使用，结合实际应用开发案例全面的介绍整个开发流程和百度云产品使用方法，以提升学员开发技能和了解百度云产品开发特点，根据一天或者两天的课程，提供多个实际动手实验，认证讲师指导实验，真正做到学以致用，为学员实现上云开发保驾护航。\n1）基于百度云OpenAPI实现简化版控制台的综合实验\n2）基于百度云BOS OpenAPI实现简化版的百度网盘\n课程四、百度云“天工 · 智能物联网”与“天像· 智能多媒体”服务平台介绍与案例分析\n百度天工物联平台是“一站式、全托管”的物联网服务平台，依托百度云基础产品与服务，提供全栈物联网核心服务，帮助开发者快速搭建、部署物联网应用。通过全面介绍天工的IoT Hub、IoT Parser、Rule Engine、IoT Device、BML、BMR、OCR和语音识别等产品与服务，解析天工典型的产品架构方案，应用到工业4.0、车联网、能源、物流和智能硬件等各行业解决方案。\n1）基于百度云LSS快速搭建音视频直播平台最佳实践\n2）基于百度云VOD快速搭建音视频点播平台最佳实践\n3）体验百度云音视频转码MCT的转码计算服务\n4）基于百度云文档服务DOC体验文档存储、转码、分发播放一站式服务体验\n5）基于百度云物接入IoT Hub实现智能设备与百度云端之间建立安全的双向连接\n6）体验百度云的物管理IoT Device端到端配置实践\n课程五、百度云“天智·人工智能”服务平台介绍与实战\n天智是基于世界领先的百度大脑打造的人工智能平台，提供了语音技术、文字识别、人脸识别、深度学习和自然语言NLP等一系列人工智能产品及解决方案，帮助各行各业的客户打造智能化业务系统。本课程力求对百度人工智能服务平台进行整体、全面的介绍，包括天智平台与解决方案介绍、主要产品（百度语音、人脸识别、文字识别、百度深度学习、百度机器学习 BML、自然语言NLP等）的介绍、客户案例分享等。\n1）百度机器学习BML-广告点击率预估\n2）百度识别-文字识别\n3）百度识别-人脸识别\n4）百度自然语言处理-短文本相似度\n5）百度语音-朗读者\n6）百度深度学习-预测用户感兴趣的电影\n阶段十、人工智能实战 － 企业项目实战\n课程一、基于Python数据分析与机器学习案例实战教程\n课程风格通俗易懂，基于真实数据集案例实战。主体课程分成三个大模块(1)python数据分析，(2)机器学习经典算法原理详解,(3)十大经典案例实战。通过python数据科学库numpy,pandas,matplot结合机器学习库scikit-learn完成一些列的机器学习案例。算法课程注重于原理推导与流程解释，结合实例通俗讲解复杂的机器学习算法，并以实战为主，所有课时都结合代码演示。算法与项目相结合，选择经典kaggle项目，从数据预处理开始一步步代码实战带大家快速入门机器学习。旨在帮助同学们快速上手如何使用python库来完整机器学习案例。选择经典案例基于真实数据集，从数据预处理开始到建立机器学习模型以及效果评估，完整的讲解如何使用python及其常用库进行数据的分析和模型的建立。对于每一个面对的挑战，分析解决问题思路以及如何构造合适的模型并且给出合适评估方法。在每一个案例中，同学们可以快速掌握如何使用pandas进行数据的预处理和分析，使用matplotlib进行可视化的展示以及基于scikit-learn库的机器学习模型的建立。\n1）Python数据分析与机器学习实战课程简介\n2）Python快速入门\n3）Python科学计算库Numpy\n4）Python数据分析处理库Pandas\n5）Python可视化库Matplotlib\n6）回归算法\n7）模型评估\n8）K近邻算法\n9）决策树与随机森林算法\n10）支持向量机\n11）贝叶斯算法\n12）神经网络\n13）Adaboost算法\n14）SVD与推荐\n15）聚类算法\n16）案例实战：使用Python库分析处理Kobe Bryan职业生涯数据\n17）案例实战：信用卡欺诈行为检测\n18）案例实战：泰坦尼克号获救预测\n19）案例实战：鸢尾花数据集分析\n20）案例实战：级联结构的机器学习模型\n21）案例实战：员工离职预测\n22）案例实战：使用神经网络进行手写字体识别\n23）案例实战：主成分分析\n24）案例实战：基于NLP的股价预测\n25）案例实战：借贷公司数据分析\n课程二、人工智能与深度学习实战\n课程风格通俗易懂，必备原理，形象解读，项目实战缺一不可！主体课程分成四个大模块(1)神经网络必备基础知识点，(2)深度学习模型，(3)深度学习框架Caffe与Tensorflow，(4)深度学习项目实战。 课程首先概述讲解深度学习应用与挑战，由计算机视觉中图像分类任务开始讲解深度学习的常规套路。对于复杂的神经网络，将其展开成多个小模块进行逐一攻破，再挑战整体神经网络架构。对于深度学习模型形象解读卷积神经网络原理，详解其中涉及的每一个参数，对卷积网络架构展开分析与评估，对于现阶段火爆的对抗生成网络以及强化学习给出形象解读，并配合项目实战实际演示效果。 基于框架实战，选择两款深度学习最火框架，Caffe与Tensorflow，首先讲解其基本使用方法，并结合案例演示如何应用框架构造神经网络模型并完成案例任务。 选择经典深度学习项目实战，使用深度学习框架从零开始完成人脸检测，验证码识别，人脸关键点定位，垃圾邮件分类，图像风格转换，AI自己玩游戏等。对于每一个项目实战，从数据预处理开始一步步构建网络模型并展开分析与评估。 课程提供所涉及的所有数据，代码以及PPT，方便大家快速动手进行项目实践！\n1）深度学习概述与挑战\n2）图像分类基本原理门\n3）深度学习必备基础知识点\n4）神经网络反向传播原理\n5）神经网络整体架构\n6）神经网络案例实战图像分类任务\n7）卷积神经网络基本原理\n8）卷积参数详解\n9）卷积神经网络案例实战\n10）经典网络架构分析\n11）分类与回归任务\n12）三代物体检测算法分析\n13）数据增强策略\n14）TransferLearning\n15）网络架构设计\n16） 深度学习框架Caffe网络结构配置\n17）Caffe\n18）深度学习项目实战人脸检测\n19）人脸正负样本数据源制作\n20）人脸检测网络架构配置习模型\n21）人脸检测代码实战\n22）人脸关键点定位项目实战\n23）人脸关键点定位网络模型\n24）人脸关键点定位构建级联网络\n25）人脸关键点定位测试效果与分析\n26）Tensorflow框架实战\n27）Tensorflow构建回归模型\n28）Tensorflow构建神经网络模型\n29）Tensorflow深度学习模型\n30）Tensorflow打造RNN网络模型\n31）Tensorflow项目实战验证识别\n32）项目实战图像风格转换\n33）QLearning算法原理\n34）DQN网络架构\n35）项目实战DQN网络让AI自己玩游戏\n36）项目实战对抗生成网络等\n项目一、AI大数据互联网电影智能推荐（第一季）\n随着科技的发展，现在视频的来源和类型多样性，互联网视频内容充斥着整个网络，如果仅仅是通过翻页的方法来寻找自己想看的视频必然会感到疲劳，现在急需一种能智能推荐的工具，推荐系统通过分析用户对视频的评分分析，对用户的兴趣进行建模，从而预测用户的兴趣并给用户进行推荐。\nPython是一种面向对象的解释型计算机程序设计语言，Python具有丰富和强大的库。它常被昵称为胶水语言，而大数据是指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，企业面临海量数据的到来，大多选择把数据从本地迁移至云端，云端将成为最大的非结构化数据存储场所。本项目主要以客户咨询为载体，分析客户的群体，分布，旨在挖掘客户的内在需求，帮助企业实现更有价值的营销。\n一、教务管理系统业务介绍\n1）教务管理系统框架讲解\n2）系统业务逻辑介绍\n二、大数据需求分析\n1）明确数据需求\n2）大数据分析过程\n3）分析难点和解决方案\n4）大数据相关技术选型\n三、构建分布式大数据框架\n1）Hadoop分布式集群配置\n2）ZooKeeper高可用\n3）SQOOP数据转移\n4）ETL数据清洗\n5）HIVE数据分析\n6）HBase数据存储\n四、基于教务管理系统大数据分析\n1）业务数据分析指标设定\n2）操作MapReduce分而治之\n3）使用Hive进行数据整合抽离\n4）使用HBase存储非结构话数据\n五、大数据可视化\n1）可视化技术选型\n2）Echarts代码展示炫酷视图\n3）使用Tableau进行数据可视化展示\n项目二、电商大数据情感分析与AI推断实战项目（第一季）\n本项目从开发的角度以大数据、PHP技术栈为基础，使用真实商用表结构和脱敏数据，分三步构建商用系统、真实大数据环境、进行推断分析以及呈现结果。 项目课程的完整性、商业性，可以使学者尽可能完整地体会真实的商业需求和业务逻辑。完整的项目过程，使PHP技术栈的同学得以窥见和学到一个完整商业平台项目的搭建方法；真实大数据环境的搭建，使呈现、建立大数据的工具应用技术概念储备；基于大数据平台的分析需求的实现、呈现，将完整的一次大数据技术栈到分析结果的中线，平铺直述，为想要学习大数据并有开发基础的同学点亮新的能力。\n一、实践项目研发\n1）开发环境的安装配置\n2）表与数据\n3）LARAVEL的快速开发实践\n4）批量创建模型\n5）万能控制器与表配置\n6）统一视图的创建\n二、数据分析需求设立\n1）定义数据需求\n2）分析计算过程\n3）分析难点和解决方案\n4）大数据技术选型\n三、大数据平台搭建\n1）分布式环境的模拟建立\n2）网络环境的调通\n3）身份验证与集群控制\n4）Hadoop环境搭建和要点说明\n5）MapReduce与Yarn的搭建和说明\n四、大数据分析脚本编写\n1）MapReduce脚本编写\n2）拆解数据需求\n3）Map逻辑详写\n4）Reduce逻辑详写\n5）结果整理与输出\n五、结果可视化\n1）可视化需求和技术选型\n2）展示页面的快速铺设\n3）可视化JS上手\n4）使用可视化JS展示结果\n项目三、AI法律咨询大数据分析与服务智能推荐实战项目(第一季)\n本项目结合目前流行的大数据框架，在原有成熟业务的前提下，进行大数据分析处理，真实还原企业应用，让学员身临其境的感受企业大数据开发的整个流程。\n项目的业务系统底层主要采用JAVA架构，大数据分析主要采用Hadoop框架，其中包括Kettle实现ETL、SQOOP、Hive、Kibana、HBASE、Spark以及人工智能算法等框架技术；采用真实大数据集群环境的搭建，让学员切身感受企业项目的从0到1的过程。\n一、系统业务介绍\n1）底层业务实现框架讲解\n2）功能模块讲解\n二、系统架构设计\n1）总体架构分析\n2）数据流向\n3）各技术选型承载作用\n4）部署方案\n三、详尽实现\n1）原始数据处理\n2）ETL数据导入\n3）MR数据计算\n4）Hive数据分析\n四、数据可视化\n1）采用Highcharts插件展示客户偏好曲线图\n2）使用Tableau进行数据分析可视化展示\n五、项目优化\n1）ZooKeeper实现HA\n2）集群监控的整体联调\n项目四、AI大数据基站定位智能推荐商圈分析项目实战（第一季）\n随着当今个人手机终端的普及、出行人群中手机拥有率和使用率已达到相当高的比例，根据手机信号在真实地理空间的覆盖情况，将手机用户时间序列的手机定位数据，映射至现实地理位置空间位置，即可完整、客观地还原出手机用户的现实活动轨迹，从而挖掘出人口空间分布与活动联系特征信息。\n商圈是现代市场中企业市场活动的空间，同时也是商品和服务享用者的区域。商圈划分为目的之一是研究潜在顾客分布，以制定适宜的商业对策。\n本项目以实战为基础结合大数据技术Hadoop、.Net技术全栈为基础，采用真实商业数据，分不同环节构建商用系统、真实大数据环境、进行推断分析及呈现数据。\n一、分析系统业务逻辑讲解\n1）大数据基站定位智能推荐商圈分析系统介绍\n2）数据前期清洗和数据分析目标指标的设定等\n二、大数据导入与存储\n1）关系型数据库基础知识\n2）hive的基本语法\n3）hive的架构及设计原理\n4）hive安装部署与案例等\n5）Sqoop安装及使用\n6）Sqoop与关系型数据库进行交互等\n7）动手实践\n三、Hbase理论及实战\n1）Hbase简介、安装及配置\n2）Hbase的数据存储与数据模型\n3）Hbase Shell\n4）Hbase 访问接口\n5）Hbase数据备份与恢复方法等\n6）动手实践（数据转储与备份）\n四、基站数据分析与统计推断\n1）背景与分析推断目标\n2）分析方法与过程推断\n3）动手实践（分析既定指标数据）\n五、数据分析与统计推断结果的展示（大数据可视化）\n1）使用Tableau展示数据分析结果\n2）使用HighCharts、ECharts展示数据分析结果\n阶段十一、区块链\n区块链(Blockchain)是分布式数据存储、点对点传输、共识机制、加密算法等计算机技术的新型应用模式。所谓共识机制是区块链系统中实现不同节点之间建立信任、获取权益的数学算法。\n区块链是比特币的底层技术，像一个数据库账本，记载所有的交易记录。这项技术也因其安全、便捷的特性逐渐得到了银行与金融业的关注。\n一、课程介绍\n1）区块链的发展\n2）课程安排\n3）学习目标\n二、区块链的技术架构\n1）数据层 创世区块 交易记录 私钥，公钥和钱包地址\n2）数据层 \u0026 通讯层 记账原理 Merkle 树和简单支付验证（SPV） P2P通讯 数据通信和验证\n3）共识层\n4）激励层 拜占庭将军问题与POW Pos DPos PBFT 挖矿 交易费 图灵完备和非完备\n5）合约层 比特币脚本 以太坊智能合约 fabic智能合约 RPC远程调用\n6）应用层\n7）总结 接口调用 DAPP的使用 应用场景的部署 重要概念和原理\n三、环境搭建\n1）以太坊 以太坊介绍 以太坊开发过程 图形界面客户端使用 供应链的应用 保险领域的应用 DAO的介绍和应用\n2）以太坊 以太坊本地开发环境的搭建 以太坊分布式集群环境的搭建\n3）hyperledger项目fabric介 fabric介绍 fabric本地开发环境搭建 fabric分布式集群环境搭建\n四、案例和DEMO\n1）案例讲解 支付和清结算 公益行业的应用 供应链的应用 保险领域的应用 DAO的介绍和应用\n2）Demo介绍 发币和交易Demo\n3）Demo介绍 数据资产的确权和追溯\n阶段十二、用人工智能预测金融量化交易投资系列课程\n程序化交易：又称程式交易,发源于上世纪80年代的美国,其最初的定义是指在纽约股票交易所(NYSE)市场上同时买卖超过15只以上的股票组合；像高盛、摩根士丹利及德意志银行都是在各大交易市场程序化交易的最活跃参与会员。\n本课程主要面向意愿从事金融量化交易人员、金融行业从业人员、金融策略开发人员及投资经验丰富而想实现计算机自动下单人员；主要讲解了证券期货程序化实现原理及过程，通过本课程的学习，您可以根据自己的意愿打造属于自己的量化投资交易系统； 本课程主要用到的技术手段有：Python、Pandas、数据分析、数据挖掘机器学习等。\n一、程序化交易数据获取与清洗讲解\n1）数据的清洗与合成\n2）K线图绘制\n3）技术指标开发讲解\n4）数据的获取\n二、回测框架搭建讲解\n1）回测框架搭建背景及基本流程讲解\n2）回测框架实现及收益指标讲解\n三、程序化交易部分实现讲解\n1）CTP技术讲解\n2）程序化API讲解\n3）程序化交易具体实现讲解\n阶段十三、阿里云认证\n课程一、云计算 - 网站建设：部署与发布\n阿里云网站建设认证课程教你如何掌握将一个本地已经设计好的静态网站发布到Internet公共互联网，绑定域名，完成工信部的ICP备案。\n课程二、云计算 - 网站建设：简单动态网站搭建\n阿里云简单动态网站搭建课程教你掌握如何快速搭建一个WordPress动态网站，并会对网站进行个性化定制，以满足不同的场景需求。\n课程三、云计算 - 云服务器管理维护\n阿里云服务器运维管理课程教你掌握快速开通一台云服务器，并通过管理控制台方便地进行服务器的管理、服务器配置的变更和升级、数据的备份，并保证其可以正常运转并按业务需求随时进行配置的变更。\n课程四、云计算 - 云数据库管理与数据迁移\n阿里云云数据库管理与数据迁移认证课程掌握云数据库的概念，如何在云端创建数据库、将自建数据库迁移至云数据库MySQL版、数据导入导出，以及云数据库运维的常用操作。\n课程五、云计算 - 云存储：对象存储管理与安全\n阿里云云储存认证课程教你掌握安全、高可靠的云存储的使用，以及在云端存储下载文件，处理图片，以及如何保护数据的安全。\n课程六、云计算 - 超大流量网站的负载均衡\n掌握如何为网站实现负载均衡，以轻松应对超大流量和高负载。\n课程七、大数据 - MOOC网站日志分析\n本课程可以帮助学员掌握如何收集用户访问日志，如何对访问日志进行分析，如何利用大数据计算服务对数据进行处理，如何以图表化的形式展示分析后的数据。\n课程八、大数据 - 搭建企业级数据分析平台\n模拟电商场景，搭建企业级的数据分析平台，用来分析商品数据、销售数据以及用户行为等。\n课程九、大数据 - 基于LBS的热点店铺搜索\n本课程可以帮助学员掌握如何在分布式计算框架下开发一个类似于手机地图查找周边热点（POI）的功能，掌握GeoHash编码原理，以及在地理位置中的应用，并能将其应用在其他基于LBS的定位场景中。\n课程中完整的演示了整个开发步骤，学员在学完此课程之后，掌握其原理，可以在各种分布式计算框架下完成此功能的开发，比如MapReduce、Spark。\n课程十、大数据 - 基于机器学习PAI实现精细化营销\n本课程通过一个简单案例了解、掌握企业营销中常见的、也是必需的精准营销数据处理过程，了解机器学习PAI的具体应用，指导学员掌握大数据时代营销的利器---通过机器学习实现营销。\n课程十一、大数据 - 基于机器学习的客户流失预警分析\n本课程讲解了客户流失的分析方法、流程，同时详细介绍了机器学习中常用的分类算法、集成学习模型等通用技能，并使用阿里云机器学习PAI实现流失预警分析。可以帮助企业快速、准确识别流失客户，辅助制定策略进行客户关怀，达到挽留客户的目的。\n课程十二、大数据 - 使用DataV制作实时销售数据可视化大屏\n帮助非专业工程师通过图形化的界面轻松搭建专业水准的实时可视化数据大屏，以满足业务展示、业务监控、风险预警等多种业务的展示需求。\n课程十三、大数据 - 使用MaxCompute进行数据质量核查\n通过本案例，学员可了解影响数据质量的因素，出现数据质量问题的类型，掌握通过MaxCompute（DateIDE）设计数据质量监控的方法，最终独立解决常见的数据质量监控需求。\n课程十四、大数据 - 使用Quick BI制作图形化报表\n阿里云Quick BI制作图形化报表认证课程教你掌握将电商运营过程中的数据进行图表化展现，掌握通过Quick BI将数据制作成各种图形化报表的方法，同时还将掌握搭建企业级报表门户的方法。\n课程十五、大数据 - 使用时间序列分解模型预测商品销量\n使用时间序列分解模型预测商品销量教你掌握商品销量预测方法、时间序列分解以及熟悉相关产品的操作演示和项目介绍。\n课程十六、云安全 - 云平台使用安全\n阿里云云平台使用安全认证课程教你了解由传统IT到云计算架构的变迁过程、当前信息安全的现状和形势，以及在云计算时代不同系统架构中应该从哪些方面利用云平台的优势使用安全风险快速降低90%。\n课程十七、云安全 - 云上服务器安全\n阿里云云上服务器安全认证课程教你了解在互联网上提供计算功能的服务器主要面临哪些安全风险，并针对这些风险提供了切实可行的、免费的防护方案。\n课程十八、云安全 - 云上网络安全\n了解网络安全的原理和解决办法，以及应对DDoS攻击的方法和防护措施，确保云上网络的安全。\n课程十九、云安全 - 云上数据安全\n了解云上数据的安全隐患，掌握数据备份、数据加密、数据传输安全的解决方法。\n课程二十、云安全 - 云上应用安全\n了解常见的应用安全风险，SQL注入原理及防护，网站防篡改的解决方案等，确保云上应用的安全。\n课程二十一、云安全 - 云上安全管理\n了解云上的安全监控方法，学会使用监控大屏来监控安全风险，并能够自定义报警规则，确保随时掌握云上应用的安全情况。\n阶段十四、IT高级开发者职场生存规则 － 职业素养\n本课程主要为广大毕业生或者工作经验较少的学员而设立，主要是为了在职业素养方面给大家提供辅导，为更加顺利走向职场而提供帮助。\n为什么有些同学在技能方面过关，却还是给予别人一种书生气的感觉？\n为什么简历已经通过了，却还是没有通过HR的面试？\n为什么入职后，与同事的沟通总是存在问题?\n为什么每天的时间都不够用，无法兼顾生活学习和工作?\n为什么学习一段时间后，对工作对职场没有方向感?\n为什么遇到事情，别人总是能够保持良好心态游刃有余，而我总是问题百出？\nCOT课程正是引领大家一起来探索其中的奥秘和方法，让大家一起在学习过程中不断深思和进步，让大家的职场路越走越顺畅！\n1）团队协作\n2）心态管理\n3）目标管理\n4）时间管理 ","data":"2019年04月04日 13:18:47"}
{"_id":{"$oid":"5d34506862f717dc0659b726"},"title":"自然语言处理简介","author":"数据科学爱好者","content":"一、定义\n实现人机间自然语言通信意味着要使计算机既能理解自然语言文本的意义，也能以自然语言文本表达给定的意图、思想等。前者称为自然语言理解，后者称为自然语言生成。自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。自然语言处理是一门融语言学、计算机科学、数学于一体的科学。（百度百科定义）\n二、自然语言处理的测试要求\n判断计算机是否“理解”了某种自然语言，具体的判别标准至少有如下四条：\n1、回答问题：机器能正确回答输入文本中的有关问题；\n2、文摘生成：机器有能力产生输入文本的摘要；\n3、释义：机器能用不同的词语和句型来复述其输入文本；\n4、翻译：机器具有把一种语言（源语）翻译成另一种语言（目标语）的能力。\n三、自然语言处理需要解决的问题\n自然语言处理的首要任务是将语言学知识在计算机中表示出来，在此基础上，才能完成文本意义的计算，也就是文本意义的解释（理解）。\n另外，歧义消解是自然语言理解的一个基本问题。因为在词的层面有一词多义和多词同义的问题，一个句子在不同语言环境中也有不同的含义，对篇章的理解更是仁者见仁，智者见智。\n四、发展历程\n20世纪60年代以关键词匹配为主流的早期；\n70年代以语法-语义分析为主流的中期；\n80年代开始走向实用化和工程化的近期。\n自然语言处理主要分为两大派别：1、基于规则的语法-语义分析 2、基于统计学方法的语料库语言学。\n五、中文语言处理存在的障碍\n1、输入问题，汉字不是拼音文字，而是象形文字或音形结合的文字；\n2、分词问题，多数中文句子是一长串连续的汉字（而不是以空格或其他分隔标记分开的单词），而且词汇缺少明显的形态变化；\n3、句法分析问题。\n六、关于自动分词\n自动分词是汉语特有的研究课题，也是中文信息处理技术中最基础，最重要的一个问题。就是把一个句子按照其中词的含义进行切分。\n分词单位：指汉语信息处理使用的、具有确定的语义或语法功能的基本单位，包括词和少量词组。\n词：最小的能独立运用的语言单位。\n词组：由两个或两个以上的词，按一定的语法规则组成，以表达一定意义的语言单位。\n注：为了实现机器自动分词，首先需要建立高效准确的分词词典，需要有快速的字符串匹配算法，由于汉语的广泛的歧义性，消歧算法的研究显得尤为重要，最后还要解决未登录词的识别问题。\n\n\n参考文献：\n1、苗夺谦,卫志华.《中文文本信息处理的原理与应用》.2007.\n2、百度百科-自然语言处理https://baike.baidu.com/item/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/365730?fr=aladdin\n3、自然语言处理学习相关书籍推荐 http://www.52nlp.cn/%e4%b9%a6%e7%b1%8d","data":"2018年01月08日 20:29:13","date":"2018年01月08日 20:29:13"}
{"_id":{"$oid":"5d34508062f717dc0659b72e"},"title":"报告 | 自然语言处理到底哪家强？这些企业上榜了","author":"网易智能","content":"关注网易智能，聚焦AI大事件，读懂下一个大时代！\n\n\n选自 | 学术头条（SciTouTiao）\n\n作者 | AMiner\n\n\n自然语言处理是包括了计算机科学、语言学心理认知学等一系列学科的一门交叉学科，这些学科性质不同但又彼此相互交叉。\n\n\n1950年图灵提出了著名的“图灵测试”，这一般被认为是自然语言处理思想的开端。\n\n\n\n\n20世纪50年代到70年代自然语言处理主要采用基于规则的方法。\n\n\n70年代以后随着互联网的高速发展，自然语言处理思潮由理性主义向经验主义过渡，基于统计的方法逐渐代替了基于规则的方法。\n\n\n从2008年到现在，在图像识别和语音识别领域的成果激励下，人们也逐渐开始引入深度学习来做自然语言处理研究。\n\n\n\n\n由最初的词向量到2013年word2vec，将深度学习与自然语言处理的结合推向了高潮，并在机器翻译、问答系统、阅读理解等领域取得了一定成功。\n\n\n接下来AMiner将为大家介绍自然语言处理的业界发展，涵盖了以下企业。\n\n\n\n\n微软亚洲研究院\n\n\n微软亚洲研究院1998年成立自然语言计算组，研究内容包括多国语言文本分析、机器翻译、跨语言信息检索和自动问答系统等。\n\n\n\n\n\n这些研究项目研发了一系列实用成果，如IME（Input Method Editors输入法编辑器，它是一种专门的应用程序， 用来输入代表东亚地区书面语言文字的不同字符。）、对联游戏、Bing词典、Bing翻译器、语音翻译、搜索引擎等，为微软产品做出了重大的贡献。\n\n\n微软IME\n\n\n\n微软对联游戏\n\n\n微软必应词典\n\n\n\n并且在自然语言处理顶级会议，例如ACL、COLING等会议上发表了许多论文。\n\n\n语音翻译\n\n\n2017年微软在语音翻译上全面采用了神经网络机器翻译，并新扩展了Microsoft Translator Live Feature。\n\n\n可以在演讲和开会时，实时同步在手机端和桌面端，同时把讲话者的话翻译成多种语言。\n\n\n\n\n其中最重要的技术是对于源语言的编码以及引进的语言知识，同时，微软还表示，将来要将知识图谱纳入神经网络机器翻译中规划语言理解的过程中。\n\n\n人机对话\n\n\n小娜现在已经拥有超过1.4亿用户，在数以十亿计的设备上与人们进行交流，并且覆盖了十几种语言。\n\n\n\n\n有聊天机器人小冰，正在试图把各国语言的知识融合在一起，实现一个开放语言自由聊天的过程，目前小冰实现了中文、日文和英文的覆盖，有上亿用户。\n\n\n\n\nGoogle\n\n\nGoogle是最早开始研究自然语言处理技术的团队之一，作为一个以搜索为核心的公司，Google对自然语言处理更为重视。\n\n\n\n\n\nGoogle拥有着海量数据，可以搭建丰富庞大的数据库，可以为其研究提供强大的数据支撑。\n\n\nGoogle对自然语言处理的研究侧重于应用规模、跨语言和跨领域的算法。\n\n\n机器翻译\n\n\n\n\n知识图谱\n\n\nGoogle的知识图谱更是遥遥领先，例如自动挖掘新知识的准确程度、文本中命名实体的识别、纯文本搜索词条到在知识图谱上的结构化搜索词条的转换等，效果都领先于其他公司，而且很多技术都实现了产品化。\n\n\n\n\n\n语音识别\n\n\nGoogle一直致力于投资语音搜索技术和苹果公司的siri竞争，自2012年以来将神经网络应用于这一领域，使语音识别错误率极大降低。\n\n\n2011年收购语言信息平台SayNow，把语音通信、点对点对话、以及群组通话和社交应用融合在一起。\n\n\n2014年收购了SR Tech Group的多项语音识别相关专利。\n\n\nFacebook\n\n\nFacebook涉猎自然语言处理较晚，2013年开始发展语音翻译，2015年开始语音识别的研发之路。\n\n\n\n语音翻译\n\n\n发展道路如下图所示：\n\n\n\n\n语音识别\n\n\n2015年，Facebook相继建立语音识别和对话理解工具，开始了语音识别的研发之路。\n\n\n2016年Facebook开发了一个响应“Hey Oculus”的语音识别系统。\n\n\n并在2018年初开发了wav2letter，这是一个简单高效的端到端自动语音识别（ASR）系统。\n\n\n百度\n\n\n百度自然语言处理部是百度最早成立的部门之一，研究涉及以下方面。\n\n\n\n\n\n百度在深度问答方向经过多年打磨，积累了问句理解、答案抽取、观点分析与聚合等方面的一整套技术方案，目前已经在搜索、度秘等多个产品中实现应用。\n\n\n百度翻译目前支持全球28种语言，覆盖756个翻译方向，支持文本、语音、图像等翻译功能，并提供精准人工翻译服务，满足不同场景下的翻译需求，发布了世界上首个线上神经网络翻译系统，并获得2015年度国家科技进步奖。\n\n\n阿里巴巴\n\n\n阿里自然语言处理为其产品服务，在电商平台中构建知识图谱实现智能导购，同时进行全网用户兴趣挖掘，在客服场景中也运用自然语言处理技术打造机器人客服。\n\n\n\n例如蚂蚁金融智能小宝、淘宝卖家的辅助工具千牛插件等，同时进行语音识别以及后续分析。\n\n\n\n\n阿里的机器翻译主要与其国家化电商的规划相联系，2017年初阿里正式上线了自主开发的神经网络翻译系统，进一步提升了其翻译质量。\n\n\n\n\n腾讯\n\n\nAI Lab是腾讯的人工智能实验室，研究领域包括计算机视觉、语音识别、自然语言处理、机器学习等。\n\n\n\n\n\n其研发的腾讯文智自然语言处理基于并行计算、分布式爬虫系统，结合独特的语义分析技术，可满足自然语言处理、转码、抽取、数据抓取等需求。\n\n\n在机器翻译方面，2017年腾讯宣布翻译君上线“同声传译”新功能，用户边说边翻的需求得到满足，语音识别+NMT等技术的应用保证了边说边翻的速度与精准性。\n\n\n京东\n\n\n京东在人工智能的浪潮中也不甘落后。京东AI开放平台基本上由模型定制化平台和在线服务模块构成，其中在线服务模块包括计算机视觉、语音交互、自然语言处理和机器学习等。\n\n\n\n按照京东的规划，NeuHub平台将作为普惠性开放平台，不同角色均可找到适合自己的场景，例如用简单代码即可实现对图像质量的分析评估。\n\n\n\n\n从业务上说，平台可以支撑科研人员、算法工程师不断设计新的AI能力以满足用户需求。\n\n\n并深耕电商、供应链、物流、金融、广告等多个领域应用，探索试验医疗、扶贫、政务、养老、教育、文化、体育等多领域应用。聚焦于新技术和行业趋势研究，孵化行业最新落地项目。\n\n\n科大讯飞\n\n\n科大讯飞股份有限公司成立于1999年，是一家专业从事智能语音及语言技术、人工智能技术研究、软件及芯片产品开发、语音信息服务及电子政务系统集成的国家级骨干软件企业。\n\n\n\n\n\n科大讯飞作为中国智能语音与人工智能产业领导者，在语音合成、语音识别、口语评测、自然语言处理等多项技术上拥有国际领先的成果。\n\n\n科大讯飞成立之时就开始在语言和翻译领域布局项目。基于深度神经网络算法上的创新和突破，在翻译方面的发展如下图所示。\n\n\n关于AMiner：\n\n以科研人员为中心，提供在线实时的人才、科技评估报告的情报。追踪、关注人工智能+20领域的发展动态。\n\n\n点击阅读原文下载《自然语言处理研究报告》完整版。\n\n\n- 加入社群吧 -\n网易智能AI社群（AI专家群、AI黑板报）火热招募中，对AI感兴趣的小伙伴，添加智能菌微信 kaiwu_club，说明身份即可加入。","data":"2018年07月27日 12:02:02"}
{"_id":{"$oid":"5d34509d62f717dc0659b737"},"title":"人工智能简史","author":"GitChat的博客","content":"内容简介\n本书全面讲述人工智能的发展史，几乎覆盖人工智能学科的所有领域，包括人工智能的起源、自动定理证明、专家系统、神经网络、自然语言处理、遗传算法、深度学习、强化学习、超级智能、哲学问题和未来趋势等，以宏阔的视野和生动的语言，对人工智能进行了全面回顾和深度点评。\n本书作者和书中诸多人物或为师友或相熟相知，除了详实的考证还有有趣的轶事。本书既适合专业人士了解人工智能鲜为人知的历史，也适合对人工智能感兴趣的大众读者作为入门的向导。\n名人推荐\n“《人工智能简史》确实是本难得的好书。它既是一本严肃的信史，又通俗易懂，带有科普的性质，更难得的是妙趣横生，使人拿起来一读就放不下手。要把人工智能的历史和背后的哲理讲得既准确又明白易懂，进一步还要有趣，那又是一项十分艰巨的任务。可是尼克做到了。”\n——毛德操（计算机专家、浙大网新科技首席科学家）\n“《人工智能简史》这本书应该是这两年出版的此类书籍中最好的一本，对我们建立对 AI 的全景式理解很有裨益。对于人工智能的来龙去脉，成败得失的原因，尼克老师娓娓道来，既不失深度，又不枯燥难懂。在最后两章，他讨论了不可避免的 AI 的未来和哲学问题，从能源、自动机器、社会的综合历史的角度，进行了发人深省的探讨。”\n——鲍捷（文因互联 CEO）\n“这本书不错！把各位人工智能大师的思想、成果、师承、恩怨都串起来讲，就像我们在与各位大神一起工作、生活一样。\n——孔华威（中科院计算所上海分所所长，起点资本合伙人）\n“尼克以灵活、轻松的笔调写出了人工智能发展历程中的人物、故事以及思潮起伏。他特别重视对具体的思想家、科学家、学者的刻画，轶事、趣闻像撒胡椒面一样为整本书“提味”，与一般人的预期相反，就叙事的有趣程度而言，《人工智能简史》简直不像是一本讲科学技术的书。”\n——《南方都市报》\n作者简介\n尼克，乌镇智库理事长，国家“千人计划”专家。毕业于中科院，美国麻省大学。早年曾任职哈佛和惠普；后创业投资，往返于大陆和硅谷。无论忙闲不忘读书写字，作品多发表于《上海书评》，并有著作《UNIX 系统 V 内核剖析》和《哲学评书》。\n本书内容\n前言\n历史素有两种写法：以人为主和以事为主。所有的传记都是以人为主的；而各种专史，如战争史，则多以事为主。所谓历史是人民创造的还是英雄创造的，我个人的偏好还是以人为本。八卦的历史，读者自然喜欢，对作者也有好处，就像一战后英国首相劳合 · 乔治对他的耶路撒冷总督说的那样：有争执，咱们政治家才派得上用场，如果他们停下来不打了，你就失业了。\n人工智能到底是什么？给一门学科界定范围很难，尤其是这门学科还在快速变化中。即使是数学这样的成熟学科，有时我们也理不清边界，而像人工智能这样朝令夕改的，更是不容易闹清楚了。人工智能的定义素无共识。在大学里，机械系、电子系、计算机系，甚至哲学系都有人干人工智能。让这些人对这门学科取得共识谈何容易。从实用主义（哲学的“实用主义”，不是日常用语“实用主义”）看，一个学科就是学科共同体共同关注的东西。有些毛边可以宽容，演变。这种外延式的定义要比从上帝视角给一个内涵式定义更为实用。\n一般认为，人工智能起源于1956年在达特茅斯学院召开的夏季研讨会。国内关于达特茅斯会议和神经网络早期历史的各种段子很多源于我几年前的两篇博客，后来被《上海书评》转发。经过修订，我把它们重新编为本书的两章：“达特茅斯会议：人工智能的缘起”和“神经网络简史”。“计算机下棋”一章的大部分也在《南方周末》发表过。“自动定理证明兴衰纪”的核心内容在《中国计算机学会通讯》连载过。\n明尼苏达大学的查尔斯 · 巴贝奇研究所一直在做计算机科学的口述历史，采访了很多对计算机科学有影响的人，其中有相当一批是人工智能学者。大部分的采访都有录音。除了翻阅各种文献外，我听了近100小时的采访录音，许多人工智能老一代革命家临终时话都说不利索，听这种东西除了兴趣，还得有体力。\n图灵大概是第一个对智能做出深刻思考的智者。他1936年的文章“可计算的数”奠定了计算机科学的理论和实践基础，也把相关的哲学思考推进了一大步，以至于哲学家蒙克（Ray Monk）把他列为有史以来最伟大的十位哲学家之一。图灵1950年在哲学杂志《心》（Mind）上发表的文章“计算机与智能”是传世之作，但这篇文章没有靠谱的中文翻译，我将我的译文和一篇图灵小传附在书后作为附录。图灵小传的一个早期版本曾出现在我的《哲学评书》一书中，但新版本融入了一些近几年关于图灵研究的新成果。“人工智能”这个词组的出现和达特茅斯会议有关。但英国学术圈在1956之前和之后的很长一段时间一直在用“机器智能”的说法，这和图灵1950年的文章有关。一般认为，这篇文章是这个学科的源头。但后来发现图灵1948年在英国国家物理实验室（NPL）写过一个内部报告，题为“智能机器”，其中提到了“肉体智能”（embodied intelligence）和“无肉体智能”（disembodied intelligence）的区分。机器人学家布鲁克斯（Rodney Brooks）认为图灵1948年的报告比图灵1950年的文章更加重要，它从某种意义上预示了后来符号派和统计派之争。这段历史我也列在附录里，放在图灵小传之后，因为我觉得先读读图灵的生平也许会有助于理解他的思想。\n本书每一章几乎都可单独阅读，大部分内容，对于受过高中教育的人，应该都不难懂。但第10章是个例外，这一章企图以严肃的态度探讨人工智能。我以一种浓缩的方式讲述了图灵机、丘奇?图灵论题、相似性原则和超计算。没有计算理论，很多人工智能的基础问题实在是拎不清。如果读者觉得吃力，可以跳过这一章。\n我常用的一种历史研究工具是谷歌的 Ngram。谷歌扫描了三千多万本书，把书中出现的词组的词频统计结果公布。以时间为横轴、词频为纵轴画一条曲线，就可看出特定的词在不同历史时间段的兴衰，从而得出某些洞察。例如，通过比较“United States are”和“United States is”在历史上出现的频率，就可看出美国人是何时开始认同美国作为一个统一的国家的。很明显，南北战争之后，“United States is”开始变得更常用。我们通过统计若干人工智能中关键词的 Ngram，可以感知人工智能在不同阶段的宏观发展。我曾经写过一篇“计算历史学”（见《哲学评书》）介绍 Ngram。大数据为历史学提供了有力而令人信服的工具。\n科普有一种写法：用一些貌似通俗的语言去解释复杂的原理。我一直不大相信这种方法，无论作者是内行还是专业科普作家。我压根就没见过一本可以把量子力学解释清楚的科普书。即使简单如图灵机，也鲜有适当的普及读物。倒是那些讲历史和八卦的书引人入胜，安德鲁 · 霍奇斯的《艾伦 · 图灵传：如谜的解谜者》是内行写作的典范，而数学家所罗门 · 费佛曼的太太安妮塔 · 费佛曼的两本逻辑学家传记是我心目中的标杆。戴森（Freeman Dyson）一直是我喜欢的作者，他也时不时为《纽约书评》写写八卦，趣味和我接近，我总是从阅读他的文章的过程中收获良多。即使我不懂他的数学和物理的领域，也能时有洞察。我的书单上还有蒙克的所有传记，它们既高级又有趣。就像蒙克所说，历史可以帮助内行了解知识的进化并获得新的视角，同时也为外行人了解专业知识提供入门的台阶或向导。\n读大科学家写的科普著作，最有意思的倒不是那些对成熟思想的通俗叙述，而是那些对不成熟看法的披露，还有不好意思写到正经学术论文里的自负和牢骚。恰因为这个原因，我也喜欢多依奇（David Deutsch）的几本书。\n我们很少有机会在学科发展之初就能把学科脉络梳理清楚。过去有过几个这样的时间段，例如1900年到1950年的逻辑学，1945年到2000年的分子生物学和1950年到当下的语言学。本书除了想梳理始于20世纪40年代的人工智能的历史外，还有一个作者隐含的心愿：作为人工智能的科普。哈代曾说科学（尤其是数学和理论物理，也许还有理论计算机科学）和艺术的原创需要一等的智力，解释和欣赏（例如乐评家和书评家）是二等的智力活儿。本书假想的对象是那些有能力但又是外行的人。丘成桐曾说（大意）：要想做大学问，必须先培养对学问的感情。除了科普，我还希望能帮助一小撮内行人或准内行人培养感情。我尽可能地列出了相关的参考文献供进一步学习。人工智能毕竟不是超弦理论，凭着一些智力还是可以自学的。\n本书写作得到白硕、陈利人、宫力、洪涛、刘江、马少平、毛德操、施水才和赵伟等诸位师友的帮助和指点，特此致谢。乌镇智库的同仁为本书提供了必要的数据，我的助理冰冰为我提供了多方面的支持，一并谢过。\n第1章　达特茅斯会议：人工智能的缘起\nWhat is past is prologue.\n过去只是序幕。\n——William Shakespeare（莎士比亚）\n1. 背景\n现在一说起人工智能的起源，公认是1956年的达特茅斯会议。殊不知还有个前戏，1955年，美国西部计算机联合大会（Western Joint Computer Conference）在洛杉矶召开，会中还套了个小会：学习机讨论会（Session on Learning Machine）。讨论会的参加者中有两个人参加了第二年的达特茅斯会议，他们是塞弗里奇（Oliver Selfridge）和纽厄尔（Allen Newell）。塞弗里奇发表了一篇模式识别的文章，而纽厄尔则探讨了计算机下棋，他们分别代表两派观点。讨论会的主持人是神经网络的鼻祖之一皮茨（Walter Pitts），他最后总结时说：“（一派人）企图模拟神经系统，而纽厄尔则企图模拟心智（mind）……但殊途同归。”这预示了人工智能随后几十年关于“结构与功能”两个阶级、两条路线的斗争。\n开聊达特茅斯会议之前，先说说6个最关键的人。首先，会议的召集者麦卡锡（John McCarthy）当时是达特茅斯学院的数学系助理教授。1954年，达特茅斯学院数学系同时有4位教授退休，这对达特茅斯这样的小学校而言真是不可承受之轻。刚上任的年轻系主任克门尼（John Kemeny）之前两年才在普林斯顿大学逻辑学家丘奇（Alonzo Church）门下取得了逻辑学博士，于是跑到母校求援。这么说起来，克门尼算是图灵的师弟，他战时和物理学家费曼一起工作，还一度当过爱因斯坦的数学助理，后来一头扎在计算机研究里，和麦卡锡一起琢磨出了分时系统。他1955年在《科学美国人》杂志上写过一篇文章“把人看作机器”（Man Viewed as a Machine），介绍了图灵机和冯诺伊曼[1]的细胞自动机（最早叫“自生机”），文章的简介提到“肌肉机器”（muscle machine）和“大脑机器”（brain machine）。所谓“大脑机器”就是人工智能的另一种说法而已。克门尼最为人知的工作应该是发明了老少咸宜的编程语言 BASIC。现在估计已经没人知道 BASIC 语言发明人曾是 LISP 语言发明人的老板。克门尼是天生的官僚，后来位居达特茅斯学院数学系主任和校长，美国三里岛核电站出事时，总统委托他当调查委员会主席，这是后话。\n克门尼从母校数学系带回了刚毕业的4位博士前往达特茅斯学院任教，麦卡锡是其中之一。麦卡锡后来发明的 LISP 语言中最重要的功能 Eval 实际就是丘奇的 λ 演算，而且他后半生致力于用数理逻辑把常识形式化。大家由此猜测他可能也是丘奇的学生，但其实不是，他学的压根就不是逻辑。他的老师是失去双手的代数拓扑学家莱夫谢茨（Lefschetz）。但麦卡锡对逻辑和计算理论一直有强烈兴趣，他1948年本科毕业于加州理工学院，在学校主办的 Hixon 会议上听到冯诺伊曼关于细胞自动机的讲座，后来他刚到普林斯顿大学读研究生时就结识了冯诺伊曼，在老冯影响下开始对在计算机上模拟智能产生兴趣。\n麦卡锡（1927—2011）\n达特茅斯会议的另一位积极的参加者是明斯基。他也是普林斯顿大学的数学博士，和麦卡锡在读书时就相熟。他的主业也不是逻辑，尽管他后来写过一本很有影响力的计算理论的书，还培养过好几个计算理论的博士，其中就有图灵奖获得者布鲁姆（Manual Blum）。布鲁姆目前和他老婆（Lenor Blum，就是实数计算模型 BSS 的 B）、儿子一家三口都在卡内基梅隆大学任教。明斯基的理论情结和丘奇关系也不大，他的老师塔克（Albert Tucker）是莱夫谢茨的学生，主要做非线性规划和博弈论，多年来担任普林斯顿大学数学系主任，出身数学世家，儿子、孙子也都是数学家。按辈分论，麦卡锡还是明斯基的师叔。塔克的另一名出色的学生后来得了诺贝尔经济学奖，他就是心灵美丽的纳什。纳什比明斯基小一岁，但比他早4年拿到博士学位，也算是明斯基的师兄了。明斯基的博士论文是关于神经网络的，他在麻省理工学院150周年纪念会议上回忆说是冯诺伊曼和麦卡洛克（Warren McCulloch）启发他做了神经网络。有人还找过他麻烦，质疑说神经网络的研究算数学吗，倒是老冯力挺说：现在不算，但很快就得算。倒是明斯基自己后来和神经网络结下梁子，那段故事见本书第5章“神经网络简史”。明斯基的熟人都认为他是无所不通的天才，他的忘年交沃尔弗拉姆（Stephen Wolfram）称，他晚年计划写本神学的书，但去世时书还没影子。\n塞弗里奇被后人提及不多，但他真是人工智能学科的先驱，他在麻省理工学院时一直和神经网络的开创人之一麦卡洛克一起在维纳（Norbert Wiener）手下工作，他是维纳最喜欢的学生，但没读完博士学位。维纳《控制论》一书的第一个读者就是塞弗里奇。塞弗里奇是模式识别的奠基人，他写了第一个可工作的 AI 程序。他后来在麻省理工学院参与领导 MAC 项目，这个项目后来一分为二：计算机科学实验室和人工智能实验室。但分久必合，现在这两个项目又合并了，变成了 MIT CSAIL。顺便给女读者添点料：塞弗里奇的爷爷就是英国第二大百货店塞尔福里奇（Selfridges）的创始人。所谓“顾客永远是对的”（The customer is always right.）就出自塞尔福里奇，他本是美国人，后到英国创业，发财后老婆就死了，于是勾搭上一对匈牙利双胞胎歌舞演员，出入赌场，赔光了家业。他的故事2013年还被有意思的英国人拍成了电视剧。塞尔福里奇百货几经周转，现在的主人是美国百货公司希尔斯（Sears）。塞尔福里奇百货和隔壁的哈罗德百货支撑着牛津街的零售业，现在大概一半顾客来自中国。\n信息论的创始人香农（Claude Shannon）被麦卡锡拉大旗做虎皮也请到会上打酱油。其实麦卡锡和香农的观点并不一致，平日相处也不睦。香农的硕士、博士论文都是讲怎么实现布尔代数的，当时麻省理工学院校长布什（Bush）亲自指导。博士毕业后他去了普林斯顿高等研究院，曾和数学家外尔（Hermann Weyl）、爱因斯坦、哥德尔等共事。战争中，他一直在贝尔实验室做密码学的工作，图灵在1943年曾秘访美国，和同行交流破解德国密码的经验，其间和香农曾有会晤，一起聊过通用图灵机。战后香农去英国还回访过图灵，一起讨论过计算机下棋。香农内向，从没说过这段往事，直到1982年接受一次采访时才提起。1950年香农在《哲学杂志》发表过一篇讲计算机下棋的文章，为计算机下棋奠定了理论基础。香农比其他几位年长十岁左右，当时已是贝尔实验室的大佬。\n香农（1916—2001）\n另外两位重量级参与者是纽厄尔和司马贺（Herbert Simon）。纽厄尔是麦卡锡和明斯基的同龄人，他硕士也是在普林斯顿大学数学系读的，按说普林斯顿大学数学系很小，他们应有机会碰面，但那时纽厄尔和他俩还真不认识。他们的第一次见面，纽厄尔回忆是在 IBM，而麦卡锡回忆是在兰德公司。纽厄尔的硕士导师就是冯诺伊曼的合作者、博弈论先驱摩根斯顿，纽厄尔硕士毕业后就迁往西部加入著名智库兰德公司。他在兰德开会时认识了塞弗里奇，并受到对方做的神经网络和模式识别的工作的启发，但方法论走的却完全是另一条路。\n纽厄尔（1927—1992）与司马贺（1916—2001）\n司马贺比他们仨都大11岁（怀特海比罗素也大11岁），那时是卡内基理工学院（卡内基梅隆大学的前身）工业管理系的年轻系主任，他在兰德公司学术休假时认识了纽厄尔。司马贺后来把纽厄尔力邀到卡内基梅隆大学，并给纽厄尔发了个博士学位，开始了他们终生的合作。\n纽厄尔和司马贺的合作是平等的，司马贺是纽厄尔的老师，但他们合作的文章署名都是按字母顺序纽在前司马在后，每次他们受邀去演讲，都是轮流。司马贺每次见到别人把他名字放到纽厄尔之前时都纠正。他们共享了1975年的图灵奖，三年后司马贺再得诺贝尔经济学奖。纽厄尔和司马贺代表了人工智能的另一条路线：符号派。他们后来把他们的哲学思路命名为“物理符号系统假说”。简单地说就是：智能是对符号的操作，最原始的符号对应于物理客体。这个思路和英美的经验主义哲学传统接近。他们和当时的数学系主任、第一届图灵奖获得者珀里思（Alan Perlis）一起创立了卡内基梅隆大学的计算机系，从此，卡内基梅隆大学成为计算机学科的重镇。\n2. 达特茅斯会议\n会议原址：达特茅斯楼\n1953年夏天，麦卡锡和明斯基都在贝尔实验室为香农打工。香农那时的兴趣是图灵机以及是否可用图灵机作为智能活动的理论基础。麦卡锡向香农建议编一本文集，请当时做智能研究的各位大佬贡献文章，这本文集直到1956年才以《自动机研究》（Automata Studies）为名出版，这个书名最后是香农起的，他不想花里胡哨，但麦卡锡认为这没有反映他们的初衷。\n文集的作者有两类人，一类是逻辑学家（后来都变成计算理论家了），如丘奇的两位杰出学生戴维斯和克里尼，后者的名著《元数学导论》在国内有逻辑学家莫绍揆先生的译本。明斯基、麦卡锡也都有论文录入，香农本人贡献了一篇讲只有两个内部状态的通用图灵机的文章，文集录入的一篇冯诺伊曼的论文后来开创了容错计算。文集的另一类作者几乎都是维纳的信徒，如阿什比（Ross Ashby）等，以控制论为基础。麦卡锡素不喜控制论和维纳，既不想把维纳当老大，也不愿和他见面争执，其中原因不详，或许和维纳与麦卡洛克吵翻了有关。麦卡洛克和皮茨这两位为维纳《控制论》思想贡献多多的人物，在维纳的自传里压根没被提及。麦卡锡同时又觉得香农太理论，当时他想自立门户，只对用计算机实现智能感兴趣，于是他筹划再搞一次活动。从香农后来接受的采访来看，他对维纳也没有多少尊重，他觉得自己创立的信息论和维纳一点关系也没有。但维纳却认为香农受到他的影响，香农认为维纳的这种错觉来源于维纳根本不了解信息论。\n1955年夏天，麦卡锡到 IBM 打工（美国教授都是9个月工资，如果没有研究经费，夏天要自己觅食），他的老板是罗切斯特（Nathaniel Rochester），罗切斯特是 IBM 第一代通用机701的主设计师，对神经网络素有兴趣。他们两人倒是挺对脾气，决定第二年夏天在达特茅斯搞一次活动，遂说动了香农和当时在哈佛做初级研究员（Junior Fellow[2]的明斯基一起给洛克菲勒基金会写了个项目建议书，希望得到资助。美国富豪还是有文化传统的，至少知道要资助好东西，值得中国土豪的后代学习。\n麦卡锡给这个第二年的活动起了个当时看来别出心裁的名字：人工智能夏季研讨会（Summer Research Project on Artificial Intelligence）。普遍的误解是“人工智能”这个词是麦卡锡想出来的，其实不是。麦老晚年回忆也承认这个词最早是从别人那里听来的，但记不清是谁了。后来英国数学家伍德华（Philip Woodward）给《新科学家》杂志写信说他是 AI 一词的原创者，麦卡锡最早是听他说的，因为他1956年曾去麻省理工学院访问，见过麦卡锡并交流过。但麦卡锡的建议书1955年就开始用“人工智能”了，人老了回忆真不靠谱。当事人都已仙逝，这事恐怕要成悬案了。其实英国人最早的说法是“机器智能”（Machine Intelligence），这大概和图灵那篇“计算机与智能”有关。\n大家对“人工智能”这个词一开始并没取得完全共识。很多人认为啥事一加“人工”就变味了。纽厄尔和司马贺一直主张用“复杂信息处理”这个词，以至他们发明的语言就叫 IPL（Information Processing Language）。他们从某种意义上说偏功能学派，也就是说找到智能的功能不一定非得依靠结构相同或相似。图灵机和递归函数等价，但结构完全不同，所以他们强调“信息处理”。他们俩一开始颇不喜“人工智能”几个字。1958年，在英国国家物理试验室（NPL）召开了“思维过程机器化”（Mechanization of Thought Process）会议，达特茅斯会议的与会者麦卡锡、明斯基、塞弗里奇都参加了，此外还有致力于神经网络研究的麦卡洛克，以及英国的控制论代表人物阿什比。两位编程语言的先驱也出席了：巴克斯（John Warner Backus）发表了一篇关于他新发明的语言 Fortran 的论文，但他后来一直是函数式语言的倡导者；美国海军女少将哈泊（Grace Hopper）的文章是讲第一个编译器的，这项工作导致了 COBOL 语言的诞生。中国也有女少将，也是码农。他俩论文的题目里都有 Automatic Programming 的说法，这在当时就是指高级语言编程，不能和后来人工智能中的自动编程搞混了。这次会上有人再提“人工思维”（Artificial Thinking）的说法。司马贺等人由此也逐渐接受了 AI 的说法，他晚年还写了本书《人工的科学》，倒是把 Artificial 这个词更加放大了。\n3. AI 历史的方法论\n历史研究方法有基于事件的和基于课题（issue）的。纽厄尔在1981年为一本颇为有料的文集《信息研究》贡献的一篇文章“AI 历史的智力课题”走了第二条路线。他的方法也挺有意思。他把 AI 历史当作斗争史，把历史分为两个阶级、两条路线的斗争，于是历史成了一串儿对立的议题，如模拟与数字，串行与并行，取代与增强，语法与语义，机械论与目的论，生物学与活力论，工程与科学，符号与连续，逻辑与心理等，在每一议题下有进一步可分的子议题，如在逻辑与心理下又有定理证明与问题求解等。\n被提到最多的是人工智能与控制论。在 Google Ngram 里试试 Cybernetics和Artificial Intelligence 两个词在 Google Books 里出现的词频，可以看出学科的跌宕起伏。\n“人工智能”与“控制论”词频对比\n美国最早办的一批计算机相关的系科都创办于20世纪60年代中期，那时有些系直接叫“计算机科学系”，而有些则叫“计算机与信息科学系”，带“信息”的都有些“控制论”的背景，如麻省大学计算机与信息系的创办人就有维纳的学生阿比卜（Michael Arbib）。而密歇根大学则叫计算机与通讯科学系。这些系后来都改名叫计算机系了。而原来的图书馆系现在都纷纷改名叫信息科学系，如加州大学伯克利分校和华盛顿大学的图书馆学院都改名叫信息学院（School of Information），连“科学”都省了。但现在计算机系又有加载信息的趋势，麻省大学和加州大学尔湾分校近年又改名叫信息与计算机科学学院了。大概和现在深度学习及神经网络又峰回路转有关吧。倒是中国的学科简单，一直都有计算机和自动化之分，老死不相往来罢了。\n“人工智能”这个词真正被共同体广泛认可是在十年后的1965年，在加州大学伯克利分校的欧陆派哲学家德雷弗斯（Hubert Dreyfus）发表了“炼金术与人工智能”一文之后。这篇文章一开始只是针对纽厄尔和司马贺的工作，几年后这篇文章演变成了那本著名的（或者被 AI 圈子称为“臭名昭著”的）《计算机不能干什么》一书，则是把整个 AI 当作靶子。欧陆派哲学家被人诟病数学和科学不通，但德雷弗斯有个数学家的兄弟，和他同一年在哈佛得了应用数学博士，后来又同在加州大学伯克利分校教书，是动态规划的大家，还带过神经网络的博士。哥俩一个立场。有时一个共同体的形成并不是靠内部的团结，而是靠外部的反对。有意思的是，“炼金术与人工智能”一文是德雷弗斯在兰德公司工作时写就的。司马贺后来撰文猛批德雷弗斯，说他滥用兰德公司的标签。德雷弗斯后来抱怨他在麻省理工学院和哈佛食堂吃饭，所有做 AI 的人都躲他远远的。学术争执哪儿都一样。\n麦卡锡和明斯基的建议书里罗列了他们计划研究的7个领域：(1) 自动计算机，所谓“自动”指的是可编程；(2) 编程语言；(3) 神经网络；(4) 计算规模的理论（theory of size of a calculation），这说的是计算复杂性，明斯基后来一直认为计算理论是人工智能的一部分，他早期对理论问题时不时会动动手，后来一手组建了麻省理工学院的计算理论队伍；(5) 自我改进，这个是说机器学习；(6) 抽象；(7) 随机性和创见性。\n麦卡锡的原始预算是一万三千五百美元，但洛克菲勒基金会只批了七千五百美元。麦卡锡预计会有6位学界的人出席，会议应该支付每人两个月的薪水一千两百美元，由此可推算出麦卡锡、明斯基当时的年薪在八千美元左右，考虑通货膨胀和购买力，大概相当于2016年的七万多美元，真不算多，现在随便一个美国大学计算机系的教授薪水都远不止这个数。这个学科真是今非昔比啊。作为对比，司马贺1949年去卡内基梅隆大学的前身卡内基理工学院担任新成立的工业管理系系主任时的年薪是一万美元。\n除了那六君子外，另外还有4人也参加了达特茅斯会议。他们是来自 IBM 的塞缪尔（Arthur Samuel）和伯恩斯坦，他们一个研究跳棋，一个研究象棋。达特茅斯的教授摩尔（Trenchard More）也参与了，他后来在工业界混的时间长，少为外人所知。达特茅斯会议中一位被后人忽视的“先知”是所罗门诺夫（Solomonoff）。\n和其他来来往往的人不同，所罗门诺夫在达特茅斯严肃地待了整整一个暑假。他1951年在芝加哥大学跟随费米得了物理硕士就到了麻省理工学院。但在芝加哥对他影响最大的是哲学家卡尔纳普（Paul Carnap）。有意思的是，神经网络的奠基者之一皮茨也受惠于卡尔纳普。司马贺的回忆录里也讲到自己在芝加哥时听卡尔纳普的课开始启蒙逻辑，从而开始对智能相关的问题感兴趣，但后来由于和定理证明逻辑派之间的冲突，司马贺就说自己的方法是在批判过度数学化和形式化。这么说来，人工智能的两大派——逻辑和神经网络——都发源于老卡。卡尔纳普那时的兴趣是归纳推理，这成为所罗门诺夫毕生的研究方向。所罗门诺夫后来结识了明斯基和麦卡锡，在他们的影响下研究逻辑和图灵机。达特茅斯会议时，他受麦卡锡“反向图灵机”和乔姆斯基文法的启发，发明了“归纳推理机”。他的工作后来被万能的苏联数学家柯尔莫格罗夫（Kolmogorov）独立地发明了一遍，就是现在俗称“柯尔莫格罗夫复杂性”和“算法信息论”的东西。中国的计算理论学者李明现在是这个领域的大牛，曾有专著。柯尔莫格罗夫1968年开始引用所罗门诺夫的文章，使得后者在苏联的名声比在西方更加响亮。所罗门诺夫的另一个观点“无限点”（Infinity Point）后来被未来学家库兹韦尔改名“奇点”窃为己有。目前 AI 中广泛用到的贝叶斯推理也有着所罗门诺夫的开创性痕迹。他一生并没有大富大贵，大部分时间都是在自己的咨询公司 Oxbridge（牛津+剑桥，相当于汉语俗称“清北”）拿政府（空军、海军、ARPA 和 NIH——NIH 资助了很多 AI 研究）的研究经费，那公司只有他自己一个雇员。伦敦大学皇家哈洛威学院（Royal Holloway）后来在苏联学者领导下搞柯尔莫格罗夫奖，他是第一届获奖人，并在那里兼职教授。他的学术自传1997年发表在计算理论杂志《计算机与系统科学》上。明斯基所谓 AI 孵化出计算理论的说法不无道理。\n按照麦卡锡和明斯基的说法，这十个人参加了达特茅斯会议，但现在有证据表明会议还有其他的列会者。后来一直做神经网络硬件研究从而躲过 AI 几十年过山车的斯坦福大学电机系教授维德罗（Bernard Widrow）后来回忆他也去了达特茅斯并且在那儿待了一周。麦卡锡原来的计划是两个月闭门研讨，但并非所有人都对那个事那么上心。纽厄尔和司马贺只待了一周。纽厄尔后来回忆说达特茅斯会议对他和司马贺没什么影响。\n尽管是“十仙过海”，但给所有人留下最深印象的是纽厄尔和司马贺的报告，他们公布了一款程序“逻辑理论家”（Logic Theorist），这个程序可以证明怀特海和罗素《数学原理》中命题逻辑部分的一个很大子集。司马贺回忆录里说自己学术生涯最重要的两年就是1955年和1956年。这篇文章后来成了 AI 历史上最重要的文章之一。\n值得注意的是，“逻辑理论家”对人工智能后来的一个分支“机器定理证明”的影响并不大。哲学家王浩1958年夏天在一台 IBM-704 机上，只用9分钟就证明了《数学原理》中一阶逻辑的全部定理。当然《数学原理》中罗列的一阶逻辑定理只是一阶逻辑的一个子集。目前，一阶逻辑的机器定理证明比起20世纪50年代已有长足进展，但仍然没有高效的办法。毕竟，王浩证明的是一阶逻辑，而“逻辑理论家”只能处理命题逻辑。数学家戴维斯和哲学家普特南合作，沿着王浩的思路进一步提出了戴维斯-普特南（DP）证明过程，后来进一步发展为 DPLL。王浩对“逻辑理论家”一直持鄙视的态度，认为这是一个不专业的东西。王浩在1983年被授予定理证明里程碑大奖，被认为是定理证明的开山鼻祖。司马贺在他的回忆录里则对此表示不满，认为王浩的工作抵消了“逻辑理论家”的原创性，他们的初衷并不是要有效地证明定理，而是研究人的行为。这是后话，见第2章“自动定理证明兴衰纪”。\n麦卡锡多年后回忆说：他从纽厄尔和司马贺的 IPL 语言中学到了表处理，这成为他后来发明 LISP 的基础。明斯基后来接受采访时说他对纽厄尔和司马贺的“逻辑理论家”印象深刻，因为那是第一个可工作的 AI 程序。但事实上，明斯基在当时为大会写的总结里对“逻辑理论家”只是轻描淡写。麦卡锡和明斯基明显是一伙的，会议是他们发动的，旨在创立一门新学科。但纽厄尔和司马贺却抢了他们的风头。美国20世纪50年代的学术氛围不免浮躁，这一帮人又都是年轻气盛、野心十足。\n4. 会议之后\n达特茅斯会议后不久，1956年9月 IRE（后来改名 IEEE）在麻省理工学院召开信息论年会，麦卡锡受邀做一个对一个月前达特茅斯会议的总结报告。这引起了纽厄尔尤其是司马贺的不满，他们认为麦卡锡只能聊，没干货，而达特茅斯会议唯一的干货是纽厄尔和司马贺的程序“逻辑理论家”。打了一圈架，最后纽厄尔和司马贺做了妥协：麦卡锡先做总结报告，但最后还是由纽厄尔和司马贺讲他们的“逻辑理论家”并发表一篇题为“逻辑理论机器”（Logic Theory Machine）的文章。明斯基认为是他的协调起了作用，但纽厄尔晚年则只对香农的邀请有印象，而司马贺的回忆录则说是大会的主席罗森布拉特和司马贺散了很长一圈步才了断。明斯基机敏异常，讲话时带幽默，但在对这段历史的重构中，却给人印象有点太“刁滑”（cynical），原因也不难猜出。研究历史有时必须得全方位，空间或时间上的接近不见得就真实。太接近时，当事人还都活着，还在一个圈子里混，不方便互相揭短。但在接近生命末期，或者功成名就，或者人之将死，或者对头已死无所顾忌，敞开了说，有时虽有夸张，但一不留神就会流露真话，纽厄尔属于后者。明斯基“刁滑”可能和他身体好有关系，偌大岁数也没不惑，觉得还有好长的路要走。\n科学达人戴森（Freeman Dyson）在他的《一面多彩的镜子》一书中借鉴过伯林（Isaiah Berlin）“刺猬与狐狸”的比喻：刺猬是那些构建理论体系的人，而狐狸则是那些解决问题的人。在他眼里，爱因斯坦、哥德尔是刺猬，而费米、冯诺伊曼属狐狸。科学史有时刺猬得势，有时狐狸当道。是不是可以说纽厄尔和司马贺更像刺猬，而麦卡锡和明斯基更像狐狸呢？具体到 AI 的源头和达特茅斯会议，麦卡锡认为他和明斯基是发起人，纽厄尔和司马贺是“外人”，是搅局者。明斯基的解释是纽厄尔和司马贺一开始的出发点是心理学，这与麦卡锡和他本人的背景不符。但在随后的十年里，他本人更多地走向心理学，而纽厄尔和司马贺更靠近 AI，也没什么矛盾。麦卡锡除了和明斯基关系紧密外，和其他 AI 群体的交流并不多，在所谓其他群体中，最有影响的当属卡内基梅隆那一派了。麦卡锡晚年回忆说那时群体之间的沟通主要是通过研究生，研究生就像大佬们的大使。后来斯坦福大学、卡内基梅隆大学、麻省理工学院的学生确实互为教授，门户之见随着时间的推移逐渐被抹平了。\n总之，1956年 IRE 信息论年会是个值得纪念的会议，除了纽厄尔和司马贺发表的那篇文章之外，心理学家米勒（George Miller）发表了“人类记忆和对信息的储存”（Human Memory and the Storage of Information），这是那篇著名的文章“魔力数字七”（The Magic Number Seven）的另一个版本，不知算不算一稿多发。同在此会上，伟大的乔姆斯基则发表了“语言描述的三种模型”（Three Models for the Description of Language），该文证明了有限状态句法不能表达某类语言，这是乔姆斯基分层的起源，文中引用了还没出版的不朽名著《句法结构》。乔姆斯基当时刚刚到 MIT 现代语言学系（该系后来演变为语言学与哲学系）出任助理教授并在 MIT 电子实验室做机器翻译的研究。尽管乔老爷后来是“反政府斗士”，但有点反讽的是他早期的研究经费都来自美国空军和海军。\n从参与者的角度看，大家会认为这次 IRE 的信息论年会比达特茅斯会议更重要，影响也更深远。米勒回忆说，他当时直觉认识到实验心理学、理论语言学、认知过程的计算机模拟，都是一个“大家伙”里面的组成部分。这个所谓的“大家伙”就是现在的人工智能加认知科学吧。\n明斯基回忆自己在达特茅斯会议期间，在纸上画了一个几何定理证明器的设计，并手动模拟证明了等腰三角形的一个定理。会后的1956年9月，IBM 招了新毕业的物理博士格兰特（Herb Gelernter）实现明斯基的几何定理证明器。麦卡锡此时受到纽厄尔和司马贺的影响，建议在 Fortran 里实现表处理语言，作为实现语言。这个项目在1959年实现后，IBM 削减了对 AI 的投入，把这个项目砍掉了，理由是 IBM 不想给人以机器可以替代人的印象。IBM 再次资助 AI 是20多年后的1983年了，现在好像 IBM 百年老店只能靠 AI 系统沃森（Watson）翻身了。\n麦卡锡1958年离开达特茅斯学院去了 MIT，帮助创立了 MIT 的 MAC 项目。他和明斯基一起领导了 MAC 项目中的 AI 实验室，1962年他再次跳槽到斯坦福大学。之后明斯基又和佩珀特（Seymour Papert）合作。计算机操作系统里“分时”的概念是由麦卡锡在 MAC 项目中首创的。他回忆说当时机器太少，但等着上机的学生很多，于是就发明了分时系统。按说分时系统的贡献要比麦卡锡后来的 AI 贡献彰显得多，但麦卡锡得图灵奖可不是靠“分时”，这就像爱因斯坦得诺贝尔奖没靠相对论一样。从这个意义上 AI 有点像哲学：由此衍生出很多问题，而对这些问题的解决产生出许多子学科；一旦这些子学科独立，就不再待见 AI 了。另一个例子是卡内基梅隆大学的微核心操作系统 MACH，其最早的发源是在卡内基梅隆大学的雷蒂（Raj Reddy）搞的分布式传感网络，MACH 领导者拉希德（Rick Rashid）后来加入微软，MACH 变成微软后来操作系统的基础，他本人也变成微软负责技术的决策者之一。\n现在计算机科学已成为成熟的学科，每个计算机系大都有三拨人：理论、系统和 AI。20年前的美国计算机圈子曾有一种说法：理论和系统的人互相看不起，但又同时看不起 AI 的人。AI 这几年火了，但曾几何时，AI 的人是被压迫者。哲学曾经孕育了科学，但一旦问题被确定，就分离成为单独的科学。最新的例子是逻辑学，现在的逻辑学家都在数学系和计算机系，哲学系被彻底空洞化。哲学家丹尼特（Daniel Dennett）曾说：AI 就是哲学。按照明斯基的说法，人工智能就是先锋派的计算机科学。MAC 项目孕育了计算机科学中很多原创的概念。以至于明斯基后来认为 UNIX 系统是落后的东西，因为他们丢掉了很多 Multics 中的精华。\n利克莱德（Joseph Licklider）是信息时代的预言家和布道者，他20世纪60年代初期在美国国防部“先进研究项目局”（ARPA）创办“指挥与控制”（C2）办公室，后来演变为“行为科学及指挥与控制”办公室，最终变成有权有势的“信息科技办公室”（IPTO）。正是利克莱德最早想到了“人机协同”“计算机网络”“未来图书馆”等先进概念。而他的“行为科学”计划也曾资助过监控项目，不知那是不是受到奥威尔的启发。\n1968年，参议院多数党领袖曼斯菲尔德对 ARPA 的资助方向不满，他认为国防部的钱不能被用于军事目的之外，非军事目的的项目应该由美国国家科学基金会 NSF 负责，ARPA 改名 DARPA，更强调“国防”。利克莱德遂于1968年离开 ARPA，去了 MIT 担任 MAC 项目负责人，统筹 MIT 的计算机科学实验室和人工智能实验室。人们认识到利克莱德的贡献太晚了，他于1990年过世。计算机科学最重要的实验室之一施乐 PARC 的创始人泰勒（Robert Taylor）曾称利克莱德是 Johnny Appleseed，这是美国18世纪到19世纪的园丁查普曼（John Chapman）的外号，他把“苹果树”的种子遍撒美国。\n利克莱德（1915—1990）\n20世纪70年代初期在海尔梅尔（George Heilmeirer）任内，DARPA 大砍 AI 预算。协调政府和 AI 实验室的工作变得头绪繁多，明斯基决定从 AI 实验室退位，让他刚毕业的学生温斯顿（Patrick Winston）接手。\n尽管明斯基说他不喜事务性工作，但他的采访和回忆中触及的话题总是和联邦政府的资助有关。温斯顿后来回忆时说，管理一个成功的实验室要管理好三个圈的交集：出资人（主要是政府）、科学上有创建、有国计民生的价值。他试图说服几任 ARPA 的头儿别把 AI 当作一个几年一次的项目，而是长期而独立的一门学科。另外他对比了早期 ARPA 和 NSF 的不同，NSF 是20世纪80年代才开始资助 AI 研究的，且给钱少，而且都是同行评议制，结果是越有成就的拿的钱越多，但很少会有根本性的原创性贡献，ARPA 早期都是头儿们说了算，好处是如果管事的头儿们品味好，肯定会支持好东西。这一点也值得一些科技人借鉴：大型项目决策者的品味可以超越“透明计算”吗？\n再说回海尔梅尔，他以 AI 不能帮助造武器打仗为理由，削减了对 AI 的大规模经费，但同时却重金资助了隐形飞机和空间武器技术，使美国在相关领域一直保持领先。ARPA 资助的这类项目要是通过同行评议是很难实施的。ARPA 几乎在同时也支持了 ARPANET，后来演变成互联网。有意思的是，海尔梅尔从 ARPA 离任后去了德州仪器（TI）做 CTO，在 TI 却大力提倡 AI。ARPA 对 AI 的资助在克柔克（Steve Crocker）手里才逐步恢复。大家知道克柔克是互联网的先驱之一。再后来的 ARPA 信息技术办公室（IPTO）的负责人中还有图灵奖获得者萨瑟兰（Ivan Edward Sutherland），也对 AI 继续投入。精英制风格的 ARPA，更适合做大型开创性项目，成功取决于少数决策者；而以民主制为基础的 NSF，历来就是小规模资助基础研究。\n5. 预测未来：会有奇点吗？\n司马贺1957年曾预言十年内计算机下棋会击败人。1968年麦卡锡和象棋大师列维（David Levy）打赌说十年内下棋程序会战胜列维，最后赔了列维两千块。乐观的预言总会给对手留下把柄：德雷弗斯后来每年都拿此事嘲讽 AI，说计算机下下跳棋还行，下象棋连十岁的孩子都干不过。这便宜话一直说到1997年，IBM 的下棋程序“深蓝”击败了卡斯帕罗夫。这真是“四十年太久，只争朝夕”啊。在1995年卡斯帕罗夫还在批评计算机下棋缺乏悟性（insights），但1996年时他已经开始意识到“深蓝”貌似有悟性了。而两年间“深蓝”的计算能力只不过提高了一倍而已。机器有没有悟性的边界其实就是人的解释能力的极限。量变到质变的临界点就是人的解释能力，人解释不了的东西就有悟性，解释了的东西就没有悟性。司马贺和日本计算机科学家宗像俊则（Toshinori Munakata）合写了篇解气的文章“人工智能的教训”（AI Lessons）登在《ACM 通讯》上。\n当然，德雷弗斯们还可以将“计算机仍然不能干什么”加上若干个“仍然”接着批评。明斯基1968年在库布里克的电影《2001太空漫游》的新闻发布会上曾大放厥词说30年内机器智能可以和人有一拼，1989年又预言20年可以解决自然语言处理。现在我们恐怕还不能说机器翻译器令人满意吧。过分乐观的另一个原因，照明斯基自己的说法是，一门年轻的学科，一开始都需要一点“过度销售”（excessive salesmanship）。但是过头了不免被人当作狗皮膏药或炼金术。\n2006年，达特茅斯会议50周年时，当时的10位与会者中有5位仙逝，活着的5位：摩尔、麦卡锡、明斯基、塞弗里奇和所罗门诺夫在达特茅斯团聚，忆往昔展未来。\n2006年，会议50年后，当事人重聚达特茅斯（左起：摩尔、麦卡锡、明斯基、塞弗里奇、所罗门诺夫）\n参会人之一霍维茨（Horvitz）现在是微软实验室的头目，他和他老婆拿出一笔钱在斯坦福大学捐助了一个“AI100”[3]的活动：在下面100年里各路豪杰聚会，每5年出个 AI 进展报告。第一期出版于2016年，但里面并无什么干货。\n乔姆斯基晚年边做学问边做斗士。2015年3月他和物理学家克劳斯对话时被问及“机器可以思维吗？”，他套用计算机科学家戴客斯特拉（Dijkstra）的说法反问：“潜艇会游泳吗？”如果机器人可以有意识（consciousness）的性质，机器人可以被认为有意识吗？他进一步说“意识”是相对简单的，而“前意识”（preconsciousness）是困难的问题。他把 AI 分成工程的和科学的。工程的，如自动驾驶车等，能做出对人类有用的东西；科学的一面，乔老爷明显不认可。他引用图灵的话：这问题没有讨论的意义（too meaningless to deserve discussion）。当一帮奇点理论的粉丝带着正面的期望采访乔姆斯基时，他却对人工智能这个被他深刻影响过的学科没太当回事，他认为气候和毁灭性武器是比奇点更紧迫的问题。这算有意回避吧。\n明斯基在2012年接受他的学生、预言家、奇点理论炮制者库兹韦尔的采访时说，他相信奇点的到来，可能就在我们的有生之年。两位“斯基”在麻省理工学院150周年纪念会上分在一个小组讨论里，却只打了下太极，并没有针锋相对。明斯基2016年1月24日在波士顿去世，据说为了等奇点，他老人家把自个儿冷冻了。\n明斯基和乔姆斯基在麻省理工学院150周年纪念会上同室不操戈，并没针锋相对\n参考文献指南\n人工智能是一门新学科，历史的读物并不多。波登的《认知科学历史》（Boden 2008）和尼尔森的《人工智能探究》（Nilsson 2010）是两本严肃的读物。麦克达克（Pamela McCorduck）曾是费根鲍姆的御用作家，她1979年写的《能思考的机器》（Machines Who Think）一书，无论是取材还是立意，从今天的角度看都略微过时。尼尔森是人工智能学科的早期参与者，也一直是领导者之一，他多年担任 SRI 的人工智能部门负责人和斯坦福大学计算机系主任，是圈里人。\n纽厄尔1981年的文章探讨了如何研究人工智能的历史，他总结了人工智能历史中不同思想的对立，他的方法也可以用来研究更广义的计算机科学，甚至可以拓展到不同科学领域和哲学。尽管这是30多年前的文章，但今天读来仍有启发。\n明尼苏达大学的巴贝奇研究所是专门研究计算机科学历史的机构。主持工作的诺伯格采访了多名计算机科学家，并做了录音。这些被采访的人中也有不少人工智能学者，例如纽厄尔、麦卡锡、明斯基、温斯顿、布坎南等。听这些人的录音采访和阅读正儿八经的文章完全是两种不同的体验。采访中的语调幽默，包含了很多文章不可能有的微妙细节。除了录音采访，麦卡锡还有个西蒙斯基金会的更正式的视频采访。\n雅各布森（Annie Jacobsen）的《五角大楼大脑》（Pentagon's Brain）是关于 ARPA 的详实而有趣的历史。从这本书中我们可以看到信息科技一直不是 ARPA 的主打方向，但互联网这个 ARPA 歪打正着的项目却是它最好的投资。\n[1] 我故意没有在“冯”和“诺伊曼”之间加那个讨厌的点儿，因为在更多时候，查找参考文献时，他的姓是列在 V 下，而不是 N 下。\n[2] 哈佛的 Fellow 还是挺值钱的，历史上人数不多，蒯因、王浩、库恩在变成正式教授之前都做过。乔姆斯基几乎在同时也是哈佛的 Fellow。\n[3] AI100 活动在斯坦福有个网站：https://ai100.stanford.edu/。::: hljs-center\n第2章　自动定理证明兴衰纪\n第3章　从专家系统到知识图谱\n第4章　第五代计算机的教训\n第5章　神经网络简史\n第6章　计算机下棋简史：机定胜人，人定胜天\n第7章　自然语言处理\n第8章　向自然学习：从遗传算法到强化学习\n第9章　哲学家和人工智能\n第10章　人是机器吗？——人工智能的计算理论基础\n第11章　智能的进化\n第12章　当我们谈论生死时，我们在谈论什么？\n附录1　图灵小传\n附录2　人工智能前史：图灵与人工智能\n附录3　冯诺伊曼与人工智能\n附录4　计算机与智能\n参考文献\n人名对照\n阅读全文: http://gitbook.cn/gitchat/geekbook/5b5e8fd791833538d3944d9f","data":"2018年08月14日 00:44:37"}
{"_id":{"$oid":"5d34514e62f717dc0659b75a"},"title":"自然语言处理 五","author":"qq_27678431","content":"自然语言处理（五）\n传统机器学习\n1. 朴素贝叶斯的原理\n1.1 朴素贝叶斯相关的统计学知识\n1.2基本定义\n2. 利用朴素贝叶斯模型进行文本分类\n2.1模型原理与训练\n3. SVM的原理\n3.1快速理解SVM原理\n4. 利用SVM模型进行文本分类\n5. pLSA、共轭先验分布；LDA主题模型原理\n6. 使用LDA生成主题特征，在之前特征的基础上加入主题特征进行文本分类\n传统机器学习\n1. 朴素贝叶斯的原理\n1.1 朴素贝叶斯相关的统计学知识\n贝叶斯学派很古老，但是从诞生到一百年前一直不是主流。主流是频率学派。频率学派的权威皮尔逊和费歇尔都对贝叶斯学派不屑一顾，但是贝叶斯学派硬是凭借在现代特定领域的出色应用表现为自己赢得了半壁江山。\n贝叶斯学派的思想可以概括为先验概率+数据=后验概率。也就是说我们在实际问题中需要得到的后验概率，可以通过先验概率和数据一起综合得到。数据大家好理解，被频率学派攻击的是先验概率，一般来说先验概率就是我们对于数据所在领域的历史经验，但是这个经验常常难以量化或者模型化，于是贝叶斯学派大胆的假设先验分布的模型，比如正态分布，beta分布等。这个假设一般没有特定的依据，因此一直被频率学派认为很荒谬。虽然难以从严密的数学逻辑里推出贝叶斯学派的逻辑，但是在很多实际应用中，贝叶斯理论很好用，比如垃圾邮件分类，文本分类。\n我们先看看条件独立公式，如果X和Y相互独立，则有：\nP(X,Y)=P(X)P(Y)\nP(X,Y)=P(X)P(Y)\n我们接着看看条件概率公式：\nP(Y|X)=P(X,Y)/P(X)\nP(Y|X)=P(X,Y)/P(X)\nP(X|Y)=P(X,Y)/P(Y)\nP(X|Y)=P(X,Y)/P(Y)\n或者说:\nP(Y|X)=P(X|Y)P(Y)/P(X)\nP(Y|X)=P(X|Y)P(Y)/P(X)\n接着看看全概率公式\nP(X)=∑kP(X|Y=Yk)P(Yk)其中∑kP(Yk)=1\nP(X)=∑kP(X|Y=Yk)P(Yk)其中∑kP(Yk)=1\n从上面的公式很容易得出贝叶斯公式：\nP(Yk|X)=P(X|Yk)P(Yk)∑kP(X|Y=Yk)P(Yk)\n\n基于朴素贝叶斯公式，比较出后验概率的最大值来进行分类，后验概率的计算是由先验概率与类条件概率的乘积得出，先验概率和类条件概率要通过训练数据集得出，即为朴素贝叶斯分类模型，将其保存为中间结果，测试文档进行分类时调用这个中间结果得出后验概率。\n1.2基本定义\n朴素贝叶斯分类是一种十分简单的分类算法，叫它朴素贝叶斯分类是因为这种方法的思想真的很朴素，朴素贝叶斯的思想基础是这样的：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。\n朴素贝叶斯分类的正式定义如下：\n1、设     为一个待分类项，而每个a为x的一个特征属性。\n2、有类别集合。\n3、计算。\n\n4、如果 ，则。\n那么现在的关键就是如何计算第3步中的各个条件概率。我们可以这么做：\n1、找到一个已知分类的待分类项集合，这个集合叫做训练样本集。\n2、统计得到在各类别下各个特征属性的条件概率估计。即\n。\n3、如果各个特征属性是条件独立的，则根据贝叶斯定理有如下推导：\n\n因为分母对于所有类别为常数，因为我们只要将分子最大化皆可。又因为各特征属性是条件独立的，所以有：\n2. 利用朴素贝叶斯模型进行文本分类\n2.1模型原理与训练\n朴素贝叶斯分类器是一种有监督学习，常见有两种模型，多项式模型(multinomial model)即为词频型和伯努利模型(Bernoulli model)即文档型，还有一种高斯模型。\n前二者的计算粒度不一样，多项式模型以单词为粒度，伯努利模型以文件为粒度，因此二者的先验概率和类条件概率的计算方法都不同。计算后验概率时，对于一个文档d，多项式模型中，只有在d中出现过的单词，才会参与后验概率计算，伯努利模型中，没有在d中出现，但是在全局单词表中出现的单词，也会参与计算，不过是作为“反方”参与的。\n这里暂不考虑特征抽取、为避免消除测试文档时类条件概率中有为0现象而做的取对数等问题。\n\n3. SVM的原理\n3.1快速理解SVM原理\n很多讲解SVM的书籍都是从原理开始讲解，如果没有相关知识的铺垫，理解起来还是比较吃力的，以下的一个例子可以让我们对SVM快速建立一个认知。\n给定训练样本，支持向量机建立一个超平面作为决策曲面，使得正例和反例的隔离边界最大化。\n决策曲面的初步理解可以参考如下过程，\n1）如下图想象红色和蓝色的球为球台上的桌球，我们首先目的是找到一条曲线将蓝色和红色的球分开，于是我们得到一条黑色的曲线。\n\n2） 为了使黑色的曲线离任意的蓝球和红球距离（也就是我们后面要提到的margin）最大化，我们需要找到一条最优的曲线。如下图，\n3） 想象一下如果这些球不是在球桌上，而是被抛向了空中，我们仍然需要将红色球和蓝色球分开，这时就需要一个曲面，而且我们需要这个曲面仍然满足跟所有任意红球和蓝球的间距的最大化。需要找到的这个曲面，就是我们后面详细了解的最优超平面。\n4) 离这个曲面最近的红色球和蓝色球就是Support Vector。","data":"2019年05月20日 20:20:32"}
{"_id":{"$oid":"5d34517b62f717dc0659b769"},"title":"中文自然语言处理数据集：ChineseNLPCorpus（附链接）","author":"数据派THU","content":"来源：AINLP\n本文约1300字，建议阅读5分钟。\n本文为你推荐中文自然语言处理数据集。\n\n\n推荐一个Github项目：ChineseNLPCorpus，该项目收集了一批中文自然语言处理数据集的相关链接，可以用来练手，点击阅读原文可以直达该项目链接：\nhttps://github.com/InsaneLife/ChineseNLPCorpus\n\n\n以下来自该项目介绍页\n\n\n中文自然语言处理数据集，平时做做实验的材料。欢迎补充提交合并。\n\n\n文本分类\n\n\n新闻分类\n\n\n今日头条中文新闻（短文本）分类数据集：https://github.com/fateleak/toutiao-text-classfication-dataset\n数据规模：共38万条，分布于15个分类中。\n采集时间：2018年05月。\n以0.7 0.15 0.15做分割 。\n清华新闻分类语料：\n根据新浪新闻RSS订阅频道2005~2011年间的历史数据筛选过滤生成。\n数据量：74万篇新闻文档（2.19 GB）\n小数据实验可以筛选类别：体育, 财经, 房产, 家居, 教育, 科技, 时尚, 时政, 游戏, 娱乐\nhttp://thuctc.thunlp.org/#%E8%8E%B7%E5%8F%96%E9%93%BE%E6%8E%A5\nrnn和cnn实验：https://github.com/\ngaussic/text-classification-cnn-rnn\n中科大新闻分类语料库：http://www.nlpir.org/?action-viewnews-itemid-145\n\n\n情感/观点/评论 倾向性分析\n\n\n数据集\n数据概览\nChnSentiCorp_htl_all\n7000 多条酒店评论数据，5000 多条正向评论，2000 多条负向评论\nwaimai_10k\n某外卖平台收集的用户评价，正向 4000 条，负向 约 8000 条\nonline_shopping_10_cats\n10 个类别，共 6 万多条评论数据，正、负向评论各约 3 万条， 包括书籍、平板、手机、水果、洗发水、热水器、蒙牛、衣服、计算机、酒店\nweibo_senti_100k\n10 万多条，带情感标注 新浪微博，正负向评论约各 5 万条\nsimplifyweibo_4_moods\n36 万多条，带情感标注 新浪微博，包含 4 种情感， 其中喜悦约 20 万条，愤怒、厌恶、低落各约 5 万条\ndmsc_v2\n28 部电影，超 70 万 用户，超 200 万条 评分/评论 数据\nyf_dianping\n24 万家餐馆，54 万用户，440 万条评论/评分数据\nyf_amazon\n52 万件商品，1100 多个类目，142 万用户，720 万条评论/评分数据\n\n\n实体识别\u0026词性标注\n\n\n微博实体识别。\nhttps://github.com/hltcoe/golden-horse\nboson数据。\n包含6种实体类型。\nhttps://github.com/InsaneLife/ChineseNLPCorpus/tree/master/NER/boson\n1998年人民日报数据集。\n人名、地名、组织名三种实体类型\nhttps://github.com/InsaneLife/ChineseNLPCorpus/tree/master/NER/renMinRiBao\nMSRA微软亚洲研究院数据集。\n5 万多条中文命名实体识别标注数据（包括地点、机构、人物）\nhttps://github.com/InsaneLife/ChineseNLPCorpus/tree/master/NER/MSRA\nSIGHAN Bakeoff 2005：一共有四个数据集，包含繁体中文和简体中文，下面是简体中文分词数据。\nMSR: http://sighan.cs.uchicago.edu/bakeoff2005/\nPKU ：http://sighan.cs.uchicago.edu/bakeoff2005/\n\n\n搜索匹配\n\n\nOPPO手机搜索排序\n\n\nOPPO手机搜索排序query-title语义匹配数据集。\n下载链接：\nhttps://pan.baidu.com/s/1Obm8oRVZEIh76-cpPc0qZw\n\n\n网页搜索结果评价(SogouE)\n\n\n用户查询及相关URL列表\nhttps://www.sogou.com/labs/resource/e.php\n\n\n推荐系统\n\n\n数据集\n数据概览\nez_douban\n5 万多部电影（3 万多有电影名称，2 万多没有电影名称），2.8 万 用户，280 万条评分数据\ndmsc_v2\n28 部电影，超 70 万 用户，超 200 万条 评分/评论 数据\nyf_dianping\n24 万家餐馆，54 万用户，440 万条评论/评分数据\nyf_amazon\n52 万件商品，1100 多个类目，142 万用户，720 万条评论/评分数据\n\n百科数据\n\n\n\n维基百科\n\n\n维基百科会定时将语料库打包发布：\n数据处理博客\nhttps://dumps.wikimedia.org/zhwiki/\n\n\n百度百科\n\n\n只能自己爬，爬取得链接：https://pan.baidu.\ncom/share/init?surl=i3wvfil提取码 neqs 。\n\n\n指代消歧\n\n\nCoNLL 2012 ：http://conll.cemantix\n.org/2012/data.html\n\n\n预训练：（词向量or模型）\n\n\nBERT\n\n\n开源代码:https://github.com/\ngoogle-research/bert\n模型下载：BERT-Base, Chinese: Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110M parameters\n\n\nELMO\n\n\n开源代码：\nhttps://github.com/allenai/bilm-tf\n预训练的模型：https://allennlp.org/elmo\n\n\n腾讯词向量\n\n\n腾讯AI实验室公开的中文词向量数据集包含800多万中文词汇，其中每个词对应一个200维的向量。\n下载地址：https://ai.tencent.com\n/ailab/nlp/embedding.html\n\n\n上百种预训练中文词向量\n\n\n下载地址：https://github.com/\nEmbedding/Chinese-Word-Vectors\n\n\n中文完形填空数据集\n\n\n下载地址：https://github.com/\nymcui/Chinese-RC-Dataset\n\n\n中华古诗词数据库\n\n\n最全中华古诗词数据集，唐宋两朝近一万四千古诗人, 接近5.5万首唐诗加26万宋诗. 两宋时期1564位词人，21050首词。\n\n\n下载地址：https://github.com/chinese-poetry/chinese-poetry\n\n\n保险行业语料库\n\n\n下载地址：https://github.com/\nSamurais/insuranceqa-corpus-zh\n\n\n汉语拆字字典\n\n\n英文可以做char embedding，中文不妨可以试试拆字\n下载地址：https://github.com/kfcd/chaizi\n\n\n中文数据集平台\n\n\n搜狗实验室\n\n\n搜狗实验室提供了一些高质量的中文文本数据集，时间比较早，多为2012年以前的数据。\nhttps://www.sogou.com/labs/resource/list_pingce.php\n中科大自然语言处理与信息检索共享平台\nhttp://www.nlpir.org/?action-category-catid-28\n中文语料小数据\n包含了中文命名实体识别、中文关系识别、中文阅读理解等一些小量数据。\nhttps://github.com/crownpku/Small-Chinese-Corpus\n维基百科数据集\nhttps://dumps.wikimedia.org/\n\n\nNLP工具\n\n\nTHULAC：https://github.com/thunlp/THULAC ：包括中文分词、词性标注功能。\nHanLP：https://github.com/hankcs/HanLP\n哈工大LTP： https://github.com/HIT-SCIR/ltp\nNLPIR ：https://github.com/NLPIR-team/NLPIR\njieba ：https://github.com/yanyiwu/cppjieba\n\n\n编辑：于腾凯","data":"2019年06月23日 19:00:00"}
{"_id":{"$oid":"5d3451b262f717dc0659b776"},"title":"不懂自然语言处理技术，怎么才能做一个人工智能产品？","author":"duozhishidai","content":"一、选择第三方NLP开放平台\nNLP技术沉淀周期过长，投入会很大，选择第三方开放平台想必是小公司最好的选择，推荐三个AI语音开放平台：\n科大讯飞开放平台；\n百度AI开放平；\n搜狗云知音。\n二、明确技术分工\n没有NLP技术背景，如何造一款AI产品？\n上图是引入单个NLP的对接方案，通过任务分解，可以很清楚知道，哪些是第三方平台做的，哪些是我们要做的。\nNLP底层识别交给第三方开放平台：\nASR(AutomaticSpeechRecognition,自动语音识别)：作用是将语音输入转化为文本文字\nNLU后台(NaturalLanguageUnderstanding,自然语言理解)：开放给使用者的一套自定义语义系统\nTTS(TextToSpeech,文本转语音)：用于文本转语音\n唤醒模型：预置唤醒词，当用户发出该语音指令时，设备便从休眠状态中被唤醒，并作出指定响应，唤醒词需要反复训练提升唤醒率，降低误唤醒。\nOS（OperatingSystem）：OS在执行层面发挥的巨大作用，比如：正在执行播放音乐，你想关闭、切换歌曲，这时候OS就显示出他的作用了\n系统垂类：开放平台所带的系统技能\nNLU补充、执行干预、运营系统是我们需要做的。\n三、谈谈我们要做的内容\n底层工作交给开放平台之后，我们需要搭建自己的运营管理系统，开发自己想要的技能。\n技能\n相当于垂类，简单的说就是某个应用程序，语音作为入口打开应用，像音乐、新闻、天气、笑话等都属于技能，比如：讲个笑话，语音产品执行打开了“笑话”应用，给你返回一条笑话内容。\n技能决定了产品内容的广度，技能可以是自制，比如：闹钟，也可以从第三方合作引进，像“抖音”、“微信”这样自带流量的第三方估计想必都想接入吧，对于一个智能产品来说，技能自然多多益善。至于需要多少，看公司的产品定位、业务、成本等因素综合考虑。\n自定义NLU\n给你的技能配置语义，基于开放平台下建立自己产品的自定义NLU语义内容，NLU主要由三个方面构成，语义文本、意图、参数。\n语义文本（Text）\n语义文本设计目的是为了能听得懂用户声音，同一个请求，每个用户说法都不一样。举个简单的例子，比如：帮我放首周杰伦的歌，来点周杰伦音乐，周杰伦的音乐有没有。设计语义文本时，既要使用正规的主谓宾结构，又要考虑到特殊的说法，语义要尽量覆盖全。\n意图（Intent）\n意图指用户的具体请求或目的，一个意图可以包含多个语义文本。举例：明天早上8点叫我起床，定明天早上8点钟的闹钟，都属于新增闹钟意图。通常意图依赖于技能，举例的意图就属于闹钟技能。\n详细参数（Detail）\n读懂用户说什么后，需要根据用户的意图作出相应的反馈，参数设计就显得特别重要了。NLP平台做法是当语义文本输入命中意图后，通过接口将自定义NLU的参数传达给后台。参数存在的目的是要告诉后台，接下来你要做什么。\n还是用歌曲的例子来说明：\n没有NLP技术背景，如何造一款AI产品？\n语义告诉后台，命中MUSIC意图，执行音乐技能，播放作者为“周杰伦”的歌曲。\n产品交互规则\n拿到了NLP传达的参数指令，接下来系统要做的是给用户反馈结果。\n命中到NLP系统自带的技能，如果你不做干预的话，系统可以直接给出结果。\n命中不是系统技能意图或干预系统自带技能，需要根据参数开发相应的功能。\n最后\n没有语音识别技术同样可以打造一款智能语音产品，它可以成为你的产品体系里的一部分。因为出身决定了它的造价成本会很高，如果脱离产品体系，将该语音产品单独为投入市场，至少在价格上缺乏竞争力。\n人工智能、大数据、云计算和物联网的未来发展值得重视，均为前沿产业，多智时代专注于人工智能和大数据的入门和科谱，在此为你推荐几篇优质好文：\n改变世界的七大NLP技术，你了解多少？\nhttp://www.duozhishidai.com/article-8918-1.html\nNLP自然语言处理技术，在人工智能法官中的应用是什么？\nhttp://www.duozhishidai.com/article-2325-1.html\n如何快速入门NLP自然语言处理概述\nhttp://www.duozhishidai.com/article-11742-1.html\n[多智时代-\n人工智能\n和\n大数据\n学习入门网站|人工智能、大数据、\n物联网\n、\n云计算\n的学习交流网站](http://www.duozhishidai.com)","data":"2019年03月11日 22:07:47"}
{"_id":{"$oid":"5d3451c962f717dc0659b77c"},"title":"自然语言处理学习笔记（一）","author":"刘石奇","content":"笔者是一名刚刚打开自然语言处理潘多拉魔盒的探路青年，在此写下一些学习笔记，作为总结。\n自然语言处理、语音识别、计算机视觉，是当下人工智能领域最为火热的三个领域。自然语言处理为的是让计算机理解和处理人类的语言，图灵测试中对机器智能的判断标准就是语言，语言是人类智慧的最高体现。\n我喜欢探索人类在对话中，知识的传递和积累，如何产生令人激动地aha时刻。引用一位本领域的学者：\n我的研究计划侧重于更好地理解对话的社会和实用性质，并利用这种理解建立可提高人与人之间以及人与计算机之间对话效率的计算系统。为了实现这些目标，我从计算语篇分析和文本挖掘，会话代理和计算机支持的协作学习中调用方法。\n利用自动监控自然语言沟通的技术，最后阶段是构建能够带来真实世界效益的干预措施。\nCarolyn Penstein Rose\n学习基础\n这个学科入门，一方面需要语言学的知识，另一方面是计算机建模的能力。年初开始接触自然语言处理，导师给我发了数篇文章和数学资料，要我仔细看看，把英语、编程、数学、研究方法的知识补起来。这是一个庞大的工程，我认为学习的本质，是将知识在脑海中形成一张纵横交错的大网。图结构并不能有效地存储人类知识，（对计算机而言知识图谱或许更好）。树形结构是人所倾向的。\n形成知识结构，包括学习的最好方法就是输出，输出包括：做题、解决问题、讲授和表达、写作。\n只要是将知识从大脑中得以显化，都是好的学习方式，因为这样会加深大脑中神经元的连接。\n数学基础\n在面对海量的数据时，如何让计算机有效地提取主题、理解用户的兴趣变化、情绪变化，就得用到机器学习的方法。使用无监督学习或者半监督学习，对照特征库，挖掘用户的特征，并对其进行建模。\n机器学习是指可以自主进化，不断增强的计算机算法。算法是自动化解决问题的步骤设计。人类脑袋中140亿个神经元的树突和轴突，连接起来的大脑，正是一种启发极强的计算系统，神经元的运作本质上就是计算。未来计算机会成为人类的左膀右臂。\n这些都要建立在数学建模的基础上，其中大部分得用到概率论和数理统计方法，诸如参数估计gama、beta函数、马尔科夫链、贝叶斯回归分析等，通过大量的数据集训练数学模型，使它的智能达到理想的程度。学好概率论的关键，则是打好高等数学的基础，一元微积分、多元微积分、无穷级数等，否则一头雾水。\n参考书目\n学习网站\n我爱自然语言处理\nwww.52nlp.com\n数学基础\n在考研时，我看的是同济版高等数学、浙大版概率数理统计、同济版线性代数，配合文都的参考书和课程，总觉得他们偏向于做题，有些抽象，自学起来非常费力（可能笔者理解能力有限），汤家凤老师也提到过这个问题。如果是计算机领域的数学，笔者认为最好能使用MATLAB进行模拟，结合实际中的案例。数学做题和应用是非常重要的，如王阳明所讲的知行合一，或晚清湖湘文化提倡的经世致用，知识要与实践相结合才能体现出他的价值。以下书籍以国外教材为主，建议购买国内影印的原版，还原度更高，和以后读英文文献有帮助，避免了许多错误。\n《概率论与数理统计》 陈希儒\n此书我通过知乎、lda数学八卦了解到，语言通俗易懂，配合中科大的国家精品课程学习效果更佳，做做后面的习题，有助于消化理解，灵活运用。\n\n\n《概率导论》Dimitri P bertsekas\n从直观、自然的角度阐述概率，是理工科学生入门的不二选择。本书编排和国内的书籍不同，先从样本空间入手，穿插贯序模型，配图丰富，讲解生动，比国内一出来就给个空间和公式轻松许多。\n《托马斯大学微积分》joel hass\n这本书是托马斯微积分的大学版，配图丰富，讲解生动，比国内苏联文风的写作方式亲切不少。单从配图都能感觉到此书的excited！\n《线性代数及其应用》或《线性代数引理》\n本书结合应用数学软件，强调了计算机对科学和T程学中线性代数的发展和实践的影响登录Davidc.Lay教授的网页，可以链接到琳琅满目的学习指导、数据库、应用实例等材料，无论是学生还是研究人员，阅读这本教材后，一定会被线性代数的理论和应用材料所吸引，并从中找到学习线性代数的乐趣，体会到线性代数教学改革的世界潮流和方向。\n主题模型\n2005-Parameterestimationfortextanalysis-LDA.pdf\nLDA\u0026Gibbs-Sampling-yangliuy.pdf\nlda数学八卦.pdf\n主题模型、情感识别领域，依旧是统计自然语言处理的天下，统计的方法简洁、算法鲁棒性强。基于深层次语义网络的技术在人机对话适用，笔者看了网上的介绍，建议阅读中文的《lda数学八卦》和国外的《参数估计的文本分析》，也可以看看2002年斯坦福大学吴恩达（NG）发的潜在狄利克雷主题分布的论文，这是整个lda的鼻祖。\n编程基础\n对于计算机专业学生，这个是立足之本，笔者本科时学过C和C++，懂一些皮毛的php，面向对象的编程是由面向过程的编程语言发展过来的，前者一般用在底层的硬件上，如驱动程序、单片机等他，后者在大数据、服务后台的重用性、代码维护性方面由于前者。\n导师建议我学习java作为语言，大部分语言的语法都相关。\n学姐向我推荐了毕向东的java课程，我看了两个星期，总觉得实际操作太少了，没有一本书在手，心里面没有底。在学习视频课程的同时，我买了《java编程思想》，并且请老师推荐了几个开源项目。\n目前从基础的分词项目着手，elips作为编辑器，中文的分词项目以中科院的分词系统为优。\n后记\n从猿类到人类，文化中两个关键特征，显然是人类所独有。这两个特征一个是宗教，另一个是讲故事。这两种特征需要语言进行传递。\n《人类的演化》 罗宾-邓巴上海文艺出版社","data":"2018年06月19日 06:25:23","date":"2018年06月19日 06:25:23"}
{"_id":{"$oid":"5d3451e262f717dc0659b784"},"title":"cnn用于自然语言处理","author":"芮芮杰","content":"转自：https://blog.csdn.net/diye2008/article/details/53105652\n所周知，卷积神经网络（CNN）在计算机视觉领域取得了极大的进展，但是除此之外CNN也逐渐在自然语言处理（NLP）领域攻城略地。本文主要以文本分类为例，介绍卷积神经网络在NLP领域的一个基本使用方法，由于本人是初学者，而且为了避免东施效颦，所以下面的理论介绍更多采用非数学化且较为通俗的方式解释。\n0.文本分类\n所谓文本分类，就是使用计算机将一篇文本分为a类或者b类，属于分类问题的一种，同时也是NLP中较为常见的任务。\n一.词向量\n提到深度学习在NLP中的应用就不得不提到词向量，词向量（Distributed Representation）在国内也经常被翻译为词嵌入等等，关于词向量的介绍的文章已经有很多，比如这位大神的博客：http://blog.csdn.net/zhoubl668/article/details/23271225 本文则用较为通俗的语言帮助大家了解词向量。\n所谓词向量就是通过神经网络来训练语言模型，并在训练过程钟生成一组向量，这组向量将每个词表示为一个n维向量。举个例子，假如我们要将\"北京\"表示为一个2维向量，可能的一种结果如 北京=（1.1,2.2）,在这里北京这个词就被表示为一个2维的向量。但是除了将词表示为向量以外，词向量还要保证语义相近的词在词向量表示方法中的空间距离应当是相近的。比如 '中国' -  '北京'  ≈ '英国' - '伦敦' 。上述条件可在下列词向量分布时满足，'北京'=（1.1,2.2），'中国'=（1.2,2.3） ，'伦敦'=（1.5,2.4），'英国'=(1.6,2.5)。 一般训练词向量可以使用google开源word2vec程序。\n二.卷积神经网络与词向量的结合\n有关CNN的博客非常之多，如果不了解CNN的基本概念可以参见这位大神的博客如下：http://blog.csdn.net/zhoubl668/article/details/23271225 这里就不在赘述。\n通常卷积神经网络都是用来处理类似于图像这样的二维（不考虑rgb）矩阵，比如一张图片通常都可以表示为一个2维数组比如255*255，这就表示该图片是一张255像素宽，255像素高的图片。那么如何将CNN应用到文本中呢，答案就是词向量。\n我们刚刚介绍了词向量的概念，下面介绍下如何将文本通过词向量的方式转换成一个类图像类型的格式。一般来说一篇文本可以被视为一个词汇序列的组合，比如有篇文本内容是 '书写代码，改变世界'。可以将其转换为（'书写'，'代码'，'改变'，'世界'）这样一个文本序列，显然这个序列是一个一维的向量，不能直接使用cnn进行处理。\n但是如果使用词向量的方式将其展开，假设在某词向量钟 '书写' =（1.1,2.1），'代码' = （1.5,2.9），'改变' = （2.7,3.1） ，'世界' = （2.9,3.5）,那么（'书写'，'代码'，'改变'，'世界'）这个序列就可以改写成（（1.1,2.1），（1.5,2.9），（2.7,3.1），（2.9,3.5）），显然原先的文本序列是4*1的向量，改写之后的文本可以表示为一个4*2的矩阵。 推而广之任何以文本序列都可以表示为m*d的数组，m维文本序列的词数，d维词向量的维数。\n三.用于文本分类的神经网络结构设计\n本文前面介绍了词向量、卷积神经网络等概念，并提出可以将文本转换成一个由词序列和词向量嵌套而成的二维矩阵，并通过CNN对其进行处理，下面以文本分类任务为例，举例说明如何设计该神经网络的样式。\n3.1 文本预处理部分的流程\n这部分主要是分3步，共4种状态。1.将原始文本分词并转换成以词的序列 2.将词序列转换成以词编号（每个词表中的词都有唯一编号）为元素的序列 3.将词的编号序列中的每个元素（某个词）展开为词向量的形式。下面通过一张图（本人手画简图。。。。囧）来表示这个过程，如下图所示：\n上述图片，以'书写代码，改变世界' 这一文本为例，介绍了将其转换成词向量为元素的序列表示，最后得到了一个2维矩阵，该矩阵可用于后续神经网络的训练等操作。\n3.2 神经网络模块的设计\n本文关于神经网络设计的思想来自于以下博文：\nhttp://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/  由于该文章是纯英文的，某些读者可能还不习惯阅读这类文献，我下面结合一张神经网络设计图，来说明本文中所使用的神经网络，具体设计图（又是手画图，囧）如下：\n简要介绍下上面的图，第一层数据输入层，将文本序列展开成词向量的序列，之后连接 卷积层、激活层、池化层 ，这里的卷积层因为卷积窗口大小不同，平行放置了三个卷积层，垂直方向则放置了三重（卷积层、激活层、池化层的组合）。之后连接全脸阶层和激活层，激活层采用softmax并输出 该文本属于某类的概率。\n3.3 编程实现所需要的框架和数据集等\n3.3.1 框架：本文采用keras框架来编写神经网络，关于keras的介绍请参见莫言大神翻译的keras中文文档：http://keras-cn.readthedocs.io/en/latest/ 。\n3.3.2 数据集：文本训练集来自20_newsgroup,该数据集包括20种新闻文本，下载地址如下：http://www.qwone.com/~jason/20Newsgroups/\n3.3.3 词向量：虽然keras框架已经有embedding层，但是本文采用glove词向量作为预训练的词向量，glove的介绍和下载地址如下（打开会比较慢）：\nhttp://nlp.stanford.edu/projects/glove/\n3.4 代码和相应的注释\n在3.2部分已经通过一张图介绍了神经网络的设计部分，但是考虑到不够直观，这里还是把所使用的代码，罗列如下，采用keras编程，关键部分都已经罗列注释，代码有部分是来源自keras文档 中的example目录下的：pretrained_word_embeddings.py,但是该程序我实际运行时出现了无法训练的bug，所以做了诸多改变，最主要的是我把原文中的激活层从relu改成了tanh，整体的设计结构也有了根本性的改变。对keras原始demo有兴趣的可以参见：\nhttp://keras-cn.readthedocs.io/en/latest/blog/word_embedding/\n下面就是本文中所使用的文本分类代码：\n'''本程序将训练得到一个20类的文本分类器，数据来源是 20 Newsgroup dataset\nGloVe词向量的下载地址如下:\nhttp://nlp.stanford.edu/data/glove.6B.zip\n20 Newsgroup数据集来自于:\nhttp://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html\n'''\nfrom __future__ import print_function\nimport os\nimport numpy as np\nnp.random.seed(1337)\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical\nfrom keras.layers import Dense, Input, Flatten\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding\nfrom keras.models import Model\nfrom keras.optimizers import *\nfrom keras.models import Sequential\nfrom keras.layers import Merge\nimport sys\nBASE_DIR = '.' # 这里是指当前目录\nGLOVE_DIR = BASE_DIR + '/glove.6B/' # 根据实际目录名更改\nTEXT_DATA_DIR = BASE_DIR + '/20_newsgroup/' # 根据实际目录名更改\nMAX_SEQUENCE_LENGTH = 1000 # 每个文本的最长选取长度，较短的文本可以设短些\nMAX_NB_WORDS = 20000 # 整体词库字典中，词的多少，可以略微调大或调小\nEMBEDDING_DIM = 50 # 词向量的维度，可以根据实际情况使用，如果不了解暂时不要改\nVALIDATION_SPLIT = 0.4 # 这里用作是测试集的比例，单词本身的意思是验证集\n# first, build index mapping words in the embeddings set\n# to their embedding vector 这段话是指建立一个词到词向量之间的索引，比如 peking 对应的词向量可能是（0.1,0,32,...0.35,0.5)等等。\nprint('Indexing word vectors.')\nembeddings_index = {}\nf = open(os.path.join(GLOVE_DIR, 'glove.6B.50d.txt')) # 读入50维的词向量文件，可以改成100维或者其他\nfor line in f:\nvalues = line.split()\nword = values[0]\ncoefs = np.asarray(values[1:], dtype='float32')\nembeddings_index[word] = coefs\nf.close()\nprint('Found %s word vectors.' % len(embeddings_index))\n# second, prepare text samples and their labels\nprint('Processing text dataset') # 下面这段代码，主要作用是读入训练样本，并读入相应的标签，并给每个出现过的单词赋一个编号，比如单词peking对应编号100\ntexts = [] # 存储训练样本的list\nlabels_index = {} # 词到词编号的字典，比如peking对应100\nlabels = [] # 存储训练样本，类别编号的文本，比如文章a属于第1类文本\nfor name in sorted(os.listdir(TEXT_DATA_DIR)):\npath = os.path.join(TEXT_DATA_DIR, name)\nif os.path.isdir(path):\nlabel_id = len(labels_index)\nlabels_index[name] = label_id\nfor fname in sorted(os.listdir(path)):\nif fname.isdigit():\nfpath = os.path.join(path, fname)\nif sys.version_info \u003c (3,):\nf = open(fpath)\nelse:\nf = open(fpath, encoding='latin-1')\ntexts.append(f.read())\nf.close()\nlabels.append(label_id)\nprint('Found %s texts.' % len(texts)) # 输出训练样本的数量\n# finally, vectorize the text samples into a 2D integer tensor,下面这段代码主要是将文本转换成文本序列，比如 文本'我爱中华' 转化为[‘我爱’，'中华']，然后再将其转化为[101,231],最后将这些编号展开成词向量，这样每个文本就是一个2维矩阵，这块可以参加本文‘二.卷积神经网络与词向量的结合’这一章节的讲述\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\ndata = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\nlabels = to_categorical(np.asarray(labels))\nprint('Shape of data tensor:', data.shape)\nprint('Shape of label tensor:', labels.shape)\n# split the data into a training set and a validation set,下面这段代码，主要是将数据集分为，训练集和测试集（英文原意是验证集，但是我略有改动代码）\nindices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]\nnb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\nx_train = data[:-nb_validation_samples] # 训练集\ny_train = labels[:-nb_validation_samples]# 训练集的标签\nx_val = data[-nb_validation_samples:] # 测试集，英文原意是验证集\ny_val = labels[-nb_validation_samples:] # 测试集的标签\nprint('Preparing embedding matrix.')\n# prepare embedding matrix 这部分主要是创建一个词向量矩阵，使每个词都有其对应的词向量相对应\nnb_words = min(MAX_NB_WORDS, len(word_index))\nembedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\nfor word, i in word_index.items():\nif i \u003e MAX_NB_WORDS:\ncontinue\nembedding_vector = embeddings_index.get(word)\nif embedding_vector is not None:\n# words not found in embedding index will be all-zeros.\nembedding_matrix[i] = embedding_vector # word_index to word_embedding_vector ,\u003c20000(nb_words)\n# load pre-trained word embeddings into an Embedding layer\n# 神经网路的第一层，词向量层，本文使用了预训练glove词向量，可以把trainable那里设为False\nembedding_layer = Embedding(nb_words + 1,\nEMBEDDING_DIM,\ninput_length=MAX_SEQUENCE_LENGTH,\nweights=[embedding_matrix],\ntrainable=True)\nprint('Training model.')\n# train a 1D convnet with global maxpoolinnb_wordsg\n#left model 第一块神经网络，卷积窗口是5*50（50是词向量维度）\nmodel_left = Sequential()\n#model.add(Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32'))\nmodel_left.add(embedding_layer)\nmodel_left.add(Conv1D(128, 5, activation='tanh'))\nmodel_left.add(MaxPooling1D(5))\nmodel_left.add(Conv1D(128, 5, activation='tanh'))\nmodel_left.add(MaxPooling1D(5))\nmodel_left.add(Conv1D(128, 5, activation='tanh'))\nmodel_left.add(MaxPooling1D(35))\nmodel_left.add(Flatten())\n#right model 第二块神经网络，卷积窗口是4*50\nmodel_right = Sequential()\nmodel_right.add(embedding_layer)\nmodel_right.add(Conv1D(128, 4, activation='tanh'))\nmodel_right.add(MaxPooling1D(4))\nmodel_right.add(Conv1D(128, 4, activation='tanh'))\nmodel_right.add(MaxPooling1D(4))\nmodel_right.add(Conv1D(128, 4, activation='tanh'))\nmodel_right.add(MaxPooling1D(28))\nmodel_right.add(Flatten())\n#third model 第三块神经网络，卷积窗口是6*50\nmodel_3 = Sequential()\nmodel_3.add(embedding_layer)\nmodel_3.add(Conv1D(128, 6, activation='tanh'))\nmodel_3.add(MaxPooling1D(3))\nmodel_3.add(Conv1D(128, 6, activation='tanh'))\nmodel_3.add(MaxPooling1D(3))\nmodel_3.add(Conv1D(128, 6, activation='tanh'))\nmodel_3.add(MaxPooling1D(30))\nmodel_3.add(Flatten())\nmerged = Merge([model_left, model_right,model_3], mode='concat') # 将三种不同卷积窗口的卷积层组合 连接在一起，当然也可以只是用三个model中的一个，一样可以得到不错的效果，只是本文采用论文中的结构设计\nmodel = Sequential()\nmodel.add(merged) # add merge\nmodel.add(Dense(128, activation='tanh')) # 全连接层\nmodel.add(Dense(len(labels_index), activation='softmax')) # softmax，输出文本属于20种类别中每个类别的概率\n# 优化器我这里用了adadelta，也可以使用其他方法\nmodel.compile(loss='categorical_crossentropy',\noptimizer='Adadelta',\nmetrics=['accuracy'])\n# =下面开始训练，nb_epoch是迭代次数，可以高一些，训练效果会更好，但是训练会变慢\nmodel.fit(x_train, y_train,nb_epoch=3)\nscore = model.evaluate(x_train, y_train, verbose=0) # 评估模型在训练集中的效果，准确率约99%\nprint('train score:', score[0])\nprint('train accuracy:', score[1])\nscore = model.evaluate(x_val, y_val, verbose=0) # 评估模型在测试集中的效果，准确率约为97%，迭代次数多了，会进一步提升\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n上述代码和注释较为详细的描述了该神经网络的结构，但是实际使用代码时最好去除中文注释部分，否则可能会有一些编码问题。\n四.总结\n本文描述了如何使用深度学习和keras框架构建一个文本分类器的全过程，并给出了相应的代码实现，为了方便大家使用，下面给出本文代码的下载地址一（简单版）：\nhttps://github.com/894939677/deeplearning_by_diye/blob/master/pretrain_text_class_by_diye.py\n下面给出本文代码的下载地址二（完整版）：\nhttps://github.com/894939677/deeplearning_by_diye/blob/master/pre_merge_3.py\n五.后记\n本文描述的是使用类似于googlenet的网络结构，实际上也可以使用类似与resnet的网络结构来做这个事情","data":"2018年09月01日 20:56:59"}
{"_id":{"$oid":"5d3451fd62f717dc0659b788"},"title":"自然语言处理NLP（一）","author":"村雨1943","content":"NLP\n自然语言：指一种随着社会发展而自然演化的语言，即人们日常交流所使用的语言；\n自然语言处理：通过技术手段，使用计算机对自然语言进行各种操作的一个学科；\nNLP研究的内容\n词意消歧；\n指代理解；\n自动生成语言；\n机器翻译；\n人机对话系统；\n文本含义识别；\nNLP处理\n语料读入\n网络\n本地\n分词\n\n#!/usr/bin/env python # -*- coding: utf-8 -*- # @Time : 2018-9-28 22:21 # @Author : Manu # @Site : # @File : python_base.py # @Software: PyCharm import urllib from nltk import word_tokenize from bs4 import BeautifulSoup # 在线文档下载 url = 'http://www.gutenberg.org/files/2554/2554-0.txt' res = urllib.request.urlopen(url) raw = res.read().decode('utf8') print('length', len(raw)) print('type', type(raw)) print(raw[:100]) # 分词 tokens = word_tokenize(raw) print(tokens[:50]) print('length:' + str(len(tokens))) print('type:', type(tokens)) # 创建文本 text = nltk.Text(tokens) print('type', type(text)) print('length', len(text)) print(text)\n基于此单位的文本分析\n正则表达式\n\n分割\n断句\n分词\n规范化输出\n中文分词及相应算法\n基于字典、词库匹配；\n正向最大匹配；\n逆向最大匹配；\n双向最大匹配；\n设立切分表执法；\n最佳匹配；\n基于词频度统计；\nN-gram模型；\n隐马尔科夫模型；\n基于字标注的中文分词方法；\n基于知识理解；\n分词方法比较\n结巴分词\n安装\n在控制台使用pip install jieba即可安装；\n\n功能\n分词；\njieba.cut、jieba.cut_for_search；\n添加自定义词典；\njieba.load_userdict(file_name)、add_word(word, freq=None, tag=None)、jieba.del_word(word)、jieba.suggest_freq(segmen, tune=True)；\n关键词提取；\njieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())；\njieba.analyse.set_idf_path(file_name)；\njieba.analuse.set_stop_words(file_name)；\n词性标注；\njieba.tokenize()；\njieba.posseg.cut()；\n并行分词；\n词汇搜索；","data":"2018年09月30日 16:13:27","date":"2018年09月30日 16:13:27"}
{"_id":{"$oid":"5d34523062f717dc0659b795"},"title":"2019年上半年收集到的人工智能自然语言处理方向干货文章","author":"喜欢打酱油的老鸟","content":"2019年上半年收集到的人工智能自然语言处理方向干货文章\n自然语言（NLP）发展史及相关体系\n读了这篇文字，做年薪百万的NLP工程师\n聚焦机器“读、写、说、译”，探寻NLP未来之路\nNLP接下来黄金十年-----周明等谈值得关注的NLP技术\n人工智能科普｜自然语言处理（NLP\n）\n为什么要学习NLP\nAI\n研究员收集NLP\n数据的四种创意方法（大牛分享）\n自然语言处理中注意力机制综述\n8个方法解决90％的NLP问题\n周明：NLP进步将如何改变搜索体验\n赋能行业发展，NLP如何避免走入“死胡同”？\n中文的NLP\n什么样的NLP\n库，可以支持53\n种语言？\n万字长文概述NLP\n中的深度学习技术（上）\nNLP\n接下来黄金十年-----\n周明等谈值得关注的NLP\n技术\n读了这篇文字，做年薪百万的NLP\n工程师\nNLP\n中的词向量及其应用\n为什么NLP\n相对来说这么困难？\n8\n种优秀预训练模型大盘点，NLP\n应用so easy\n！\n让机器听懂人话的\"自然语言处理技术\"究竟神奇在哪里？\n动态记忆网络：向通用 NLP 更近一步\n【精读】自然语言处理基础之RNN\n纯干货|目前看到的BERT比较透彻的文章，强烈推荐\nBert时代的创新：Bert应用模式比较及其它\n为何BERT在 NLP 中的表现如此抢眼？\nBERT面向语言理解的深度双向变换预训练\nBERT大火却不懂Transformer？读这一篇就够了\n1亿参数4万样本BERT仍听不懂人话，我们离通用NLP能还有多远？\nFlair：一款简单但技术先进的NLP库！\nTensorflow实现的深度NLP模型集锦（附资源）\n深度学习：自然语言处理（五）NLTK的经典应用\n阿里自然语言处理部总监分享：NLP技术的应用及思考\n现有模型还「不懂」自然语言：20多位研究者谈NLP四大开放性问题\n学界 | 和清华大学自然语言处理与社会人文计算实验室一起读机器翻译论文\n2019-06-23 写于苏州市","data":"2019年06月26日 08:37:53"}
{"_id":{"$oid":"5d34524862f717dc0659b79b"},"title":"人工智能学习之路","author":"红叶骑士之初","content":"课程体系\n阶段一、人工智能基础 － 高等数学必知必会\n本阶段主要从数据分析、概率论和线性代数及矩阵和凸优化这四大块讲解基础，旨在训练大家逻辑能力，分析能力。拥有良好的数学基础，有利于大家在后续课程的学习中更好的理解机器学习和深度学习的相关算法内容。同时对于AI研究尤为重要，例如人工智能中的智能很大一部分依托“概率论”实现的。\n一、数据分析\n1）常数e\n2）导数\n3）梯度\n4）Taylor\n5）gini系数\n6）信息熵与组合数\n7）梯度下降\n8）牛顿法\n二、概率论\n1）微积分与逼近论\n2）极限、微分、积分基本概念\n3）利用逼近的思想理解微分，利用积分的方式理解概率\n4）概率论基础\n5）古典模型\n6）常见概率分布\n7）大数定理和中心极限定理\n8）协方差(矩阵)和相关系数\n9）最大似然估计和最大后验估计\n三、线性代数及矩阵\n1）线性空间及线性变换\n2）矩阵的基本概念\n3）状态转移矩阵\n4）特征向量\n5）矩阵的相关乘法\n6）矩阵的QR分解\n7）对称矩阵、正交矩阵、正定矩阵\n8）矩阵的SVD分解\n9）矩阵的求导\n10）矩阵映射/投影\n四、凸优化\n1）凸优化基本概念\n2）凸集\n3）凸函数\n4）凸优化问题标准形式\n5）凸优化之Lagerange对偶化\n6）凸优化之牛顿法、梯度下降法求解\n阶段二、人工智能提升 － Python高级应用\n随着AI时代的到来以及其日益蓬勃的发展，Python作为AI时代的头牌语言地位基本确定，机器学习是着实令人兴奋，但其复杂度及难度较大，通常会涉及组装工作流和管道、设置数据源及内部和云部署之间的分流而有了Python库后，可帮助加快数据管道，且Python库也在不断更新发布中，所以本阶段旨在为大家学习后续的机器学习减负。\n一、容器\n1）列表:list\n2）元组:tuple\n3）字典: dict\n4）数组: Array\n5）切片\n6）列表推导式\n7）浅拷贝和深拷贝\n二、函数\n1）lambda表达式\n2）递归函数及尾递归优化\n3）常用内置函数/高阶函数\n4）项目案例：约瑟夫环问题\n三、常用库\n1）时间库\n2）并发库\n3）科学计算库\n4）Matplotlib可视化绘图库\n5）锁和线程\n6）多线程编程\n阶段三、人工智能实用 － 机器学习篇\n机器学习利用算法去分析数据、学习数据，随后对现实世界情况作出判断和预测。因此，与预先编写好、只能按照特定逻辑去执行指令的软件不同，机器实际上是在用大量数据和算法去“自我训练”，从而学会如何完成一项任务。\n所以本阶段主要从机器学习概述、数据清洗和特征选择、回归算法、决策树、随机森林和提升算法、SVM、聚类算、EM算法、贝叶斯算法、隐马尔科夫模型、LDA主题模型等方面讲解一些机器学习的相关算法以及这些算法的优化过程，这些算法也就是监督算法或者无监督算法。\n一、机器学习\n1）机器学习概述\n二、监督学习\n1）逻辑回归\n2）softmax分类\n3）条件随机场\n4）支持向量机svm\n5）决策树\n6）随机森林\n7）GBDT\n8）集成学习\n三、非监督学习\n1）高斯混合模型\n2）聚类\n3）PCA\n4）密度估计\n5）LSI\n6）LDA\n7）双聚类\n8）降维算法\n四、数据处理与模型调优\n1）特征提取\n2）数据预处理\n3）数据降维\n4）模型参数调优\n5）模型持久化\n6）模型可视化\n7）优化算法：坐标轴下降法和最小角回归法\n8）数据挖掘关联规则算法\n9）感知器模型\n阶段四、人工智能实用 － 数据挖掘篇\n本阶段主要通过音乐文件分类和金融反欺诈模型训练等项目，帮助大家对于上阶段的机器学习做更深入的巩固，为后续深度学习及数据挖掘提供项目支撑。\n项目一：百度音乐系统文件分类\n音乐推荐系统就是利用音乐网站上的音乐信息，向用户提供音乐信息或者建议，帮助用户决定应该听什么歌曲。而个人化推荐则是基于音乐信息及用户的兴趣特征、听歌历史行为，向用户推荐用户可能会感兴趣的音乐或者歌手。推荐算法主要分为以下几种：基于内容的推荐、协同过滤推荐、基于关联规则推荐、基于效用推荐、基于知识推荐等；推荐系统常用于各个互联网行业中，比如音乐、电商、旅游、金融等。\n项目二：千万级P2P金融系统反欺诈模型训练\n目前比较火的互联网金融领域，实质是小额信贷，小额信贷风险管理，本质上是事前对风险的主动把控，尽可能预测和防范可能出现的风险。本项目应用GBDT、Randomforest等机器学习算法做信贷反欺诈模型，通过数据挖掘技术，机器学习模型对用户进行模型化综合度量，确定一个合理的风险范围，使风险和盈利达到一个平衡的状态。\n阶段五、人工智能前沿 － 深度学习篇\n深度学习是实现机器学习的技术，同时深度学习也带来了机器学习的许多实际应用，拓展了AI的使用领域，本阶段主要从TensorFlow、BP神经网络、深度学习概述、CNN卷积神经网络、递归神经网、自动编码机，序列到序列网络、生成对抗网络，孪生网络，小样本学习技术等方面讲解深度学习相关算法以，掌握深度学习前沿技术，并根据不同项目选择不同的技术解决方案。针对公司样本不足，采用小样本技术和深度学习技术结合，是项目落地的解决方案。\n1）TensorFlow基本应用\n2）BP神经网络\n3）深度学习概述\n4）卷积神经网络(CNN)\n5）图像分类(vgg,resnet)\n6）目标检测(rcnn,fast-rcnn,faster-rcnn,ssd)\n7）递归神经网络(RNN)\n8）lstm,bi-lstm,多层LSTM\n9）无监督学习之AutoEncoder自动编码器\n10）Seq2Seq\n11）Seq2Seq with Attension\n12）生成对抗网络\n13）irgan\n14）finetune及迁移学习\n15）孪生网络\n16）小样本学习\n阶段六、人工智能进阶 － 自然语言处理篇\n自然语言处理（NLP）是计算机科学领域与人工智能领域中的一个重要方向。它已成为人工智能的核心领域。自然语言处理解决的是“让机器可以理解自然语言”这一到目前为止都还只是人类独有的特权，被誉为人工智能皇冠上的明珠，被广泛应用。本阶段从NLP的字、词和句子全方位多角度的学习NLP，作为NLP的基础核心技术，对NLP为核心的项目，如聊天机器人，合理用药系统，写诗机器人和知识图谱等提供底层技术。通过学习NLP和深度学习技术，掌握NLP具有代表性的前沿技术。\n1）词（分词，词性标注）代码实战\n2）词（深度学习之词向量，字向量）代码实战\n3）词（深度学习之实体识别和关系抽取）代码实战\n4）词（关键词提取，无用词过滤）代码实战\n5）句（句法分析，语义分析）代码实战\n6）句（自然语言理解,一阶逻辑）代码实战\n7）句（深度学习之文本相似度）代码实战\n\n阶段七、人工智能进阶 － 图像处理篇\n数字图像处理(Digital Image Processing)是通过计算机对图像进行去除噪声、增强、复原、分割、提取特征等处理的方法和技术。广泛的应用于农牧业、林业、环境、军事、工业和医学等方面，是人工智能和深度学习的重要研究方向。深度学习作为当前机器学习领域最热门的技术之一，已经在图像处理领域获得了应用，并且展现出巨大的前景。本阶段学习了数字图像的基本数据结构和处理技术，到前沿的深度学习处理方法。掌握前沿的ResNet,SSD,Faster RCNN等深度学习模型，对图像分类，目标检测和模式识别等图像处理主要领域达到先进水平。实际工作中很多项目都可以转化为本课程的所学的知识去解决，如行人检测，人脸识别和数字识别。\n一、图像基础\n图像读，写，保存，画图（线，圆，多边形，添加文字）\n二、图像操作及算数运算\n图像像素读取，算数运算，ROI区域提取\n三、图像颜色空间运算\n图像颜色空间相互转化\n四、图像几何变换\n平移，旋转，仿射变换，透视变换等\n五、图像形态学\n腐蚀，膨胀，开/闭运算等\n六、图像轮廓\n长宽，面积，周长，外接圆，方向，平均颜色，层次轮廓等\n七、图像统计学\n图像直方图\n八、图像滤波\n高斯滤波，均值滤波，双边滤波，拉普拉斯滤波等\n阶段八、人工智能终极实战 － 项目应用\n本阶段重点以项目为导向，通过公安系统人脸识别、图像识别以及图像检索、今日头条CTR广告点击量预估、序列分析系统、聊天机器人等多个项目的讲解，结合实际来进行AI的综合运用。\n项目一：公安系统人脸识别、图像识别\n使用深度学习框架从零开始完成人脸检测的核心技术图像类别识别的操作，从数据预处理开始一步步构建网络模型并展开分析与评估，方便大家快速动手进行项目实践！识别上千种人靓，返回层次化结构的每个人的标签。\n项目二：公安系统图像检索\n本项目基于卷积神经网在训练过程中学习出对应的『二值检索向量』，对全部图先做了一个分桶操作，每次检索的时候只取本桶和临近桶的图片作比对，而不是在全域做比对，使用这样的方式提高检索速度，使用Tensorflow框架建立基于ImageNet的卷积神经网络，并完成模型训练以及验证。\n项目三：今日头条CTR广告点击量预估\n点击率预估是广告技术的核心算法之一，它是很多广告算法工程师喜爱的战场。广告的价值就在于宣传效果,点击率是其中最直接的考核方式之一,点击率越大,证明广告的潜在客户越多,价值就越大,因此才会出现了刷点击率的工具和技术。通过对于点击量的评估，完成对于潜在用户的价值挖掘。\n项目四：序列分析系统\n时间序列分析(Time Series Analysis)是一种动态数据处理的统计方法，主要基于随机过程理论和数理统计方法，研究随机数据序列所遵从的统计规律以便用于解决实际问题。主要包括自相关分析等一般的统计分析方法，构建模型从而进行业务推断。经典的统计分析是假定数据序列具有独立性，而时间序列分析则侧重于研究数据样本序列之间的依赖关系。时间序列预测一般反应了三种实际变化规律：趋势变化、周期性变化和随机性变化。时间序列预测常应用于国民经济宏观控制、企业经营管理、市场潜力量预测、天气预报、水文预报等方面，是应用于金融行业的一种核心算法之一。\n项目五：京东聊天机器人/智能客服\n聊天机器人/智能客服是一个用来模拟人类对话或者聊天的一个系统，利用深度学习和机器学习等NLP相关算法构建出问题和答案之间的匹配模型，然后可以将其应用到客服等需要在线服务的行业领域中，聊天机器人可以降低公司客服成本，还能够提高客户的体验友好性。 在一个完整的聊天机器人实现过程中，主要包含了一些核心技术，包括但不限于：爬虫技术、机器学习算法、深度学习算法、NLP领域相关算法。通过实现一个聊天机器人可以帮助我们队AI整体知识的一个掌握。\n项目六：机器人写诗歌\n机器人写诗歌/小说是一种基于NLP自然语言相关技术的一种应用，在实现过程中可以基于机器学习相关算法或者深度学习相关算法来进行小说/诗歌构建过程。人工智能的一个终极目标就是让机器人能够像人类一样理解文字，并运用文字进行创作，而这个目标大致上主要分为两个部分，也就是自然语言理解和自然语言生成，其中现阶段的主要自然语言生成的运用，自然语言生成主要有两种不同的方式，分别为基于规则和基于统计，基于规则是指首先了解词性及语法等规则，再依据这样的规则写出文章；而基于统计的本质是根据先前的字句和统计的结果，进而判断下一个子的生成，例如马尔科夫模型就是一种常用的基于统计的方法。\n项目七：机器翻译系统\n机器翻译又称自动翻译，是指利用计算机将一种自然语言转换为另外一种自然语言的过程，机器翻译是人工智能的终极目标之一，具有很高的研究价值，同时机器翻译也具有比较重要的实用价值，机器翻译技术在促进政治、经济、文化交流等方面起到了越来越重要的作用；机器翻译主要分为以下三个过程：原文分析、原文译文转换和译文生成；机器翻译的方式有很多种，但是随着深度学习研究取得比较大的进展，基于人工网络的机器翻译也逐渐兴起，特别是基于长短时记忆(LSTM)的循环神经网络(RDD)的应用，为机器翻译添了一把火。\n项目八：垃圾邮件过滤系统\n邮件主要可以分为有效邮件和垃圾邮件两大类，有效邮件指的邮件接收者有意义的邮件，而垃圾邮件转指那些没有任何意义的邮件，其内容主要包含赚钱信息、成人广告、商业或者个人网站广告、电子杂志等，其中垃圾邮件又可以发为良性垃圾邮件和恶性垃圾邮件，良性垃圾邮件指的就是对收件人影响不大的信息邮件，而恶性垃圾邮件指具有破坏性的电子邮件，比如包含病毒、木马等恶意程序的邮件。垃圾邮件过滤主要使用使用机器学习、深度学习等相关算法，比如贝叶斯算法、CNN等，识别出所接收到的邮件中那些是垃圾邮件。\n项目九：手工数字识别\n人认知世界的开始就是从认识数字开始的，深度学习也一样，数字识别是深度学习的一个很好的切入口，是一个非常经典的原型问题，通过对手写数字识别功能的实现，可以帮助我们后续对神经网络的理解和应用。选取手写数字识别的主要原因是手写数字具有一定的挑战性，要求对编程能力及神经网络思维能力有一定的要求，但同时手写数字问题的复杂度不高，不需要大量的运算，而且手写数字也可以作为其它技术的一个基础，所以以手写数字识别为基础，贯穿始终，从而理解深度学习相关的应用知识。\n项目十：癌症筛选检测\n技术可以改变癌症患者的命运吗，对于患有乳腺癌患者来说，复发还是痊愈影响这患者的生命，那么怎么来预测患者的患病结果呢，机器学习算法可以帮助我们解决这一难题，本项目应用机器学习logistic回归模型，来预测乳腺癌患者复发还是正常，有效的预测出医学难题。\n项目十一：葡萄酒质量检测系统\n随着信息科技的快速发展,计算机中的经典算法在葡萄酒产业中得到了广泛的研究与应用。其中机器学习算法的特点是运用了人工智能技术,在大量的样本集训练和学习后可以自动地找出运算所需要的参数和模型。\n项目十二：淘宝网购物篮分析推荐算法\n购物篮分析(Market Basket Analysis)即非常有名的啤酒尿布故事的一个反应，是通过对购物篮中的商品信息进行分析研究，得出顾客的购买行为，主要目的是找出什么样的物品会经常出现在一起，也就是那些商品之间是有很大的关联性的。通过购物篮分析挖掘出来的信息可以用于指导交叉销售、追加销售、商品促销、顾客忠诚度管理、库存管理和折扣计划等业务；购物篮分析的最常用应用场景是电商行业，但除此之外，该算法还被应用于信用卡商城、电信与金融服务业、保险业以及医疗行业等。\n项目十三：手工实现梯度下降回归算法\n梯度下降法（英语：Gradient descent）是一个一阶最优化算法，通常也称为最速下降法。 要使用梯度下降法找到一个函数的局部极小值，必须向函数上当前点对应梯度（或者是近似梯度）的反方向的规定步长距离点进行迭代搜索。如果相反地向梯度正方向迭代进行搜索，则会接近函数的局部极大值点；这个过程则被称为梯度上升法。\n项目十四：基于TensorFlow实现回归算法\n回归算法是业界比较常用的一种机器学习算法，通过应用于各种不同的业务场景，是一种成熟而稳定的算法种类；TensorFlow是一种常用于深度学习相关领域的算法工具；随着深度学习热度的高涨，TensorFlow的使用也会越来越多，从而使用TensorFlow来实现一个不存在的算法，会加深对TensorFlow的理解和使用；基于TensorFlow的回归算法的实现有助于后续的TensorFlow框架的理解和应用，并可以促进深度学习相关知识的掌握。\n项目十五：合理用药系统\n合理用药系统，是根据临床合理用药专业工作的基本特点和要求，运用NLP和深度学习技术对药品说明书，临床路径等医学知识进行标准化，结构化处理。如自动提取药品说明书文本里面的关键信息如：药品相互作用，禁忌，用法用量，适用人群等，实现医嘱自动审查，及时发现不合理用药问题，帮助医生、药师等临床专业人员在用药过程中及时有效地掌握和利用医药知识，预防药物不良事件的发生、促进临床合理用药工作。\n项目十六：行人检测\n行人检测是利用图像处理技术和深度学习技术对图像或者视频序列中是否存在行人并给予精确定位。学习完行人检测技术后，对类似的工业缺陷检测，外观检测和医疗影像检测等目标检测范畴类的项目可以一通百通。该技术可与行人跟踪，行人重识别等技术结合，应用于人工智能系统、车辆辅助驾驶系统、智能机器人、智能视频监控、人体行为分析、智能交通等领域。由于行人兼具刚性和柔性物体的特性 ，外观易受穿着、尺度、遮挡、姿态和视角等影响，使得行人检测成为计算机视觉领域中一个既具有研究价值同时又极具挑战性的热门课题。\n项目十七：时间序列算法模型\n拿到一个观察序列后，首先要对它的平稳性和纯随机性进行检验，这两个重要的检验称为序列的预处理。根据检验的结果可以将序列分为不同的类型，对不同的类型我们采用不同的分析方法。\n1）移动平均法 (MA)\n2）自回归模型(AR)\nAR模型是一种线性预测，即已知N个数据，可由模型推出第N点前面或后面的数据（设推出P点）。\n本质类似于插值，其目的都是为了增加有效数据，只是AR模型是由N点递推，而插值是由两点（或少数几点）去推导多点，所以AR模型要比插值方法效果更好。\n3）自回归滑动平均模型(ARMA)\n其建模思想可概括为：逐渐增加模型的阶数，拟合较高阶模型，直到再增加模型的阶数而剩余残差方差不再显著减小为止。\n4）指数平滑法\n移动平均法的预测值实质上是以前观测值的加权和，且对不同时期的数据给予相同的加权。这往往不符合实际情况。\n指数平滑法则对移动平均法进行了改进和发展，其应用较为广泛。\n基本思想都是：预测值是以前观测值的加权和，且对不同的数据给予不同的权，新数据给较大的权，旧数据给较小的权。\n根据平滑次数不同，指数平滑法分为：一次指数平滑法、二次指数平滑法和三次指数平滑法等\n项目十八：PySpark大数据机器学习框架\nSpark由AMPLab实验室开发，其本质是基于内存的快速迭代框架，“迭代”是机器学习最大的特点，因此非常适合做机器学习。得益于在数据科学中强大的表现，Python是一种解释型、面向对象、动态数据类型的高级程序设计语言，结合强大的分布式内存计算框架Spark，两个领域的强者走到一起，自然能碰出更加强大的火花（Spark可以翻译为火花）。\nSpark的Python API几乎覆盖了所有Scala API所能提供的功能，只有极少数的一些特性和个别的API方法，暂时还不支持。但通常不影响我们使用Spark Python进行编程。\n项目十九：天池、kaggle比赛\n2014年3月，阿里巴巴集团董事局主席马云在北京大学发起“天池大数据竞赛”。首届大赛共有来自全球的7276支队伍参赛，海外参赛队伍超过148支。阿里巴巴集团为此开放了5.7亿条经过严格脱敏处理的数据。2014年赛季的数据提供方为贵阳市政府，参赛者根据交通数据模拟控制红绿灯时间，寻找减轻道路拥堵的方法。\nKaggle是一个数据分析的竞赛平台，网址：https://www.kaggle.com/企业或者研究者可以将数据、问题描述、期望的指标发布到Kaggle上，以竞赛的形式向广大的数据科学家征集解决方 案，类似于KDD-CUP（国际知识发现和数据挖掘竞赛）。Kaggle上的参赛者将数据下载下来，分析数据，然后运用机 器学习、数据挖掘等知识，建立算法模型，解决问题得出结果，最后将结果提交，如果提交的结果符合指标要求并且在参赛者中排名第一，将获得比赛丰厚的奖金。\n项目二十：量化交易\n量化交易(Quantitative Trading)是指借助现代统计学和数学的方法，利用计算机技术来进行交易的证券投资方式。量化交易从庞大的历史数据中海选能带来超额收益的多种“大概率”事件以制定策略，用数量模型验证及固化这些规律和策略，然后严格执行已固化的策略来指导投资，以求获得可以持续的、稳定且高于平均收益的超额回报。\n量化交易起源于上世纪七十年代的股票市场，之后迅速发展和普及，尤其是在期货交易市场，程序化逐渐成为主流。有数据显示，国外成熟市场期货程序化交易已占据总交易量的70%-80%，而国内则刚刚起步。手工交易中交易者的情绪波动等弊端越来越成为盈利的障碍，而程序化交易天然而成的精准性、100%执行率则为它的盈利带来了优势。\n阶段九、百度云实战体系\n课程一、深入理解百度云计算基础产品/基于百度云弹性计算服务实现基础架构解决方案\n全面介绍BCC（CDS 、EIP）、BLB、RDS、BOS、VPC等百度云弹性计算服务，介绍百度云的安全防护方案，深入介绍传统架构下如何通过百度云弹性计算服务快速构建更稳定、安全的应用；\n认证培训专家将通过深入浅出，理论和实践相结合的课程帮助学员深入掌握百度云弹性计算服务。\n1）快速体验百度云服务器BCC的功能全貌\n2）基于BCC的云磁盘CDS的操作与管理\n3）基于BCC的磁盘快照、自定义镜像的操作与管理\n4）基于自定义镜像快速生成BCC的实验\n5）基于磁盘快照实现数据备份与恢复的最佳实践\n6）基于百度云安全组完成定义IP＋端口的入站和出站访问策略\n7）快速体验百度云私有网络VPC的功能全貌\n8）基于百度云VPC+VPN快速搭建Stie-to-Stie的混合云架构\n9）在百度云VPC网络下实现NAT地址映射的实践\n10）快速体验百度云数据库RDS的功能全貌\n11）云数据库RDS的备份与恢复操作体验\n12）熟悉数据传输服务DTS的使用\n13）快速体验百度云负载均衡BLB的功能全貌\n14）快速体验百度云存储BOS的功能全貌\n15）快速体验百度云数据库RDS的功能全貌\n16）快速体验百度云内容分发网络CDN\n17）基于BLB、BCC、RDS、BOS和CDN快速部署Discuz论坛实现弹性架构综合实验\n18）快速体验百度云安全BSS和DDOS防护服务\n19）快速体验百度云监控BCM\n课程二、基于百度云的迁移上云实战\n基于百度云弹性计算服务的基础产品，实现传统IT架构迁移到百度云上的实战，为客户业务上云提升能力，提升客户上云前的信心，上云中和上云后的技术能力。以真实的客户案例，结合设计好的动手实验课提升实战经验，介绍了业务上云的过程、方法、工具以及案例等。\n1）基于BCC快速部署LNMP基础环境\n2）基于BCC快速部署LAMP基础环境\n3）基于BCC快速部署MySQL数据库\n4）基于BCC快速部署MS SQL数据库服务\n5）基于BCC快速部署Tomcat基础环境\n6）云数据库RDS结合数据传输服务DTS实现数据迁移上云的最佳实践\n7）基于BOS桌面实现BOS的可视化管理\n8）基于BOS FS实现BOS服务挂载到本地文件系统\n9）基于BOS-Util实现BOS的批量文件操作的演示\n10）基于BOS CLI实现BOS文件的单机操作\n课程三、在百度云平台上进行开发\n全面介绍使用百度云产品进行应用开发，理解百度云主要产品特性，包括BCC、BOS、RDS、SCS在应用开发中的使用，结合实际应用开发案例全面的介绍整个开发流程和百度云产品使用方法，以提升学员开发技能和了解百度云产品开发特点，根据一天或者两天的课程，提供多个实际动手实验，认证讲师指导实验，真正做到学以致用，为学员实现上云开发保驾护航。\n1）基于百度云OpenAPI实现简化版控制台的综合实验\n2）基于百度云BOS OpenAPI实现简化版的百度网盘\n课程四、百度云“天工 · 智能物联网”与“天像· 智能多媒体”服务平台介绍与案例分析\n百度天工物联平台是“一站式、全托管”的物联网服务平台，依托百度云基础产品与服务，提供全栈物联网核心服务，帮助开发者快速搭建、部署物联网应用。通过全面介绍天工的IoT Hub、IoT Parser、Rule Engine、IoT Device、BML、BMR、OCR和语音识别等产品与服务，解析天工典型的产品架构方案，应用到工业4.0、车联网、能源、物流和智能硬件等各行业解决方案。\n1）基于百度云LSS快速搭建音视频直播平台最佳实践\n2）基于百度云VOD快速搭建音视频点播平台最佳实践\n3）体验百度云音视频转码MCT的转码计算服务\n4）基于百度云文档服务DOC体验文档存储、转码、分发播放一站式服务体验\n5）基于百度云物接入IoT Hub实现智能设备与百度云端之间建立安全的双向连接\n6）体验百度云的物管理IoT Device端到端配置实践\n课程五、百度云“天智·人工智能”服务平台介绍与实战\n天智是基于世界领先的百度大脑打造的人工智能平台，提供了语音技术、文字识别、人脸识别、深度学习和自然语言NLP等一系列人工智能产品及解决方案，帮助各行各业的客户打造智能化业务系统。本课程力求对百度人工智能服务平台进行整体、全面的介绍，包括天智平台与解决方案介绍、主要产品（百度语音、人脸识别、文字识别、百度深度学习、百度机器学习 BML、自然语言NLP等）的介绍、客户案例分享等。\n1）百度机器学习BML-广告点击率预估\n2）百度识别-文字识别\n3）百度识别-人脸识别\n4）百度自然语言处理-短文本相似度\n5）百度语音-朗读者\n6）百度深度学习-预测用户感兴趣的电影\n阶段十、人工智能实战 － 企业项目实战\n课程一、基于Python数据分析与机器学习案例实战教程\n课程风格通俗易懂，基于真实数据集案例实战。主体课程分成三个大模块(1)python数据分析，(2)机器学习经典算法原理详解,(3)十大经典案例实战。通过python数据科学库numpy,pandas,matplot结合机器学习库scikit-learn完成一些列的机器学习案例。算法课程注重于原理推导与流程解释，结合实例通俗讲解复杂的机器学习算法，并以实战为主，所有课时都结合代码演示。算法与项目相结合，选择经典kaggle项目，从数据预处理开始一步步代码实战带大家快速入门机器学习。旨在帮助同学们快速上手如何使用python库来完整机器学习案例。选择经典案例基于真实数据集，从数据预处理开始到建立机器学习模型以及效果评估，完整的讲解如何使用python及其常用库进行数据的分析和模型的建立。对于每一个面对的挑战，分析解决问题思路以及如何构造合适的模型并且给出合适评估方法。在每一个案例中，同学们可以快速掌握如何使用pandas进行数据的预处理和分析，使用matplotlib进行可视化的展示以及基于scikit-learn库的机器学习模型的建立。\n1）Python数据分析与机器学习实战课程简介\n2）Python快速入门\n3）Python科学计算库Numpy\n4）Python数据分析处理库Pandas\n5）Python可视化库Matplotlib\n6）回归算法\n7）模型评估\n8）K近邻算法\n9）决策树与随机森林算法\n10）支持向量机\n11）贝叶斯算法\n12）神经网络\n13）Adaboost算法\n14）SVD与推荐\n15）聚类算法\n16）案例实战：使用Python库分析处理Kobe Bryan职业生涯数据\n17）案例实战：信用卡欺诈行为检测\n18）案例实战：泰坦尼克号获救预测\n19）案例实战：鸢尾花数据集分析\n20）案例实战：级联结构的机器学习模型\n21）案例实战：员工离职预测\n22）案例实战：使用神经网络进行手写字体识别\n23）案例实战：主成分分析\n24）案例实战：基于NLP的股价预测\n25）案例实战：借贷公司数据分析\n课程二、人工智能与深度学习实战\n课程风格通俗易懂，必备原理，形象解读，项目实战缺一不可！主体课程分成四个大模块(1)神经网络必备基础知识点，(2)深度学习模型，(3)深度学习框架Caffe与Tensorflow，(4)深度学习项目实战。 课程首先概述讲解深度学习应用与挑战，由计算机视觉中图像分类任务开始讲解深度学习的常规套路。对于复杂的神经网络，将其展开成多个小模块进行逐一攻破，再挑战整体神经网络架构。对于深度学习模型形象解读卷积神经网络原理，详解其中涉及的每一个参数，对卷积网络架构展开分析与评估，对于现阶段火爆的对抗生成网络以及强化学习给出形象解读，并配合项目实战实际演示效果。 基于框架实战，选择两款深度学习最火框架，Caffe与Tensorflow，首先讲解其基本使用方法，并结合案例演示如何应用框架构造神经网络模型并完成案例任务。 选择经典深度学习项目实战，使用深度学习框架从零开始完成人脸检测，验证码识别，人脸关键点定位，垃圾邮件分类，图像风格转换，AI自己玩游戏等。对于每一个项目实战，从数据预处理开始一步步构建网络模型并展开分析与评估。 课程提供所涉及的所有数据，代码以及PPT，方便大家快速动手进行项目实践！\n1）深度学习概述与挑战\n2）图像分类基本原理门\n3）深度学习必备基础知识点\n4）神经网络反向传播原理\n5）神经网络整体架构\n6）神经网络案例实战图像分类任务\n7）卷积神经网络基本原理\n8）卷积参数详解\n9）卷积神经网络案例实战\n10）经典网络架构分析\n11）分类与回归任务\n12）三代物体检测算法分析\n13）数据增强策略\n14）TransferLearning\n15）网络架构设计\n16） 深度学习框架Caffe网络结构配置\n17）Caffe\n18）深度学习项目实战人脸检测\n19）人脸正负样本数据源制作\n20）人脸检测网络架构配置习模型\n21）人脸检测代码实战\n22）人脸关键点定位项目实战\n23）人脸关键点定位网络模型\n24）人脸关键点定位构建级联网络\n25）人脸关键点定位测试效果与分析\n26）Tensorflow框架实战\n27）Tensorflow构建回归模型\n28）Tensorflow构建神经网络模型\n29）Tensorflow深度学习模型\n30）Tensorflow打造RNN网络模型\n31）Tensorflow项目实战验证识别\n32）项目实战图像风格转换\n33）QLearning算法原理\n34）DQN网络架构\n35）项目实战DQN网络让AI自己玩游戏\n36）项目实战对抗生成网络等\n项目一、AI大数据互联网电影智能推荐（第一季）\n随着科技的发展，现在视频的来源和类型多样性，互联网视频内容充斥着整个网络，如果仅仅是通过翻页的方法来寻找自己想看的视频必然会感到疲劳，现在急需一种能智能推荐的工具，推荐系统通过分析用户对视频的评分分析，对用户的兴趣进行建模，从而预测用户的兴趣并给用户进行推荐。\nPython是一种面向对象的解释型计算机程序设计语言，Python具有丰富和强大的库。它常被昵称为胶水语言，而大数据是指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，企业面临海量数据的到来，大多选择把数据从本地迁移至云端，云端将成为最大的非结构化数据存储场所。本项目主要以客户咨询为载体，分析客户的群体，分布，旨在挖掘客户的内在需求，帮助企业实现更有价值的营销。\n一、教务管理系统业务介绍\n1）教务管理系统框架讲解\n2）系统业务逻辑介绍\n二、大数据需求分析\n1）明确数据需求\n2）大数据分析过程\n3）分析难点和解决方案\n4）大数据相关技术选型\n三、构建分布式大数据框架\n1）Hadoop分布式集群配置\n2）ZooKeeper高可用\n3）SQOOP数据转移\n4）ETL数据清洗\n5）HIVE数据分析\n6）HBase数据存储\n四、基于教务管理系统大数据分析\n1）业务数据分析指标设定\n2）操作MapReduce分而治之\n\n3）使用Hive进行数据整合抽离\n4）使用HBase存储非结构话数据\n五、大数据可视化\n1）可视化技术选型\n2）Echarts代码展示炫酷视图\n3）使用Tableau进行数据可视化展示\n项目二、电商大数据情感分析与AI推断实战项目（第一季）\n本项目从开发的角度以大数据、PHP技术栈为基础，使用真实商用表结构和脱敏数据，分三步构建商用系统、真实大数据环境、进行推断分析以及呈现结果。 项目课程的完整性、商业性，可以使学者尽可能完整地体会真实的商业需求和业务逻辑。完整的项目过程，使PHP技术栈的同学得以窥见和学到一个完整商业平台项目的搭建方法；真实大数据环境的搭建，使呈现、建立大数据的工具应用技术概念储备；基于大数据平台的分析需求的实现、呈现，将完整的一次大数据技术栈到分析结果的中线，平铺直述，为想要学习大数据并有开发基础的同学点亮新的能力。\n一、实践项目研发\n1）开发环境的安装配置\n2）表与数据\n3）LARAVEL的快速开发实践\n4）批量创建模型\n5）万能控制器与表配置\n6）统一视图的创建\n二、数据分析需求设立\n1）定义数据需求\n2）分析计算过程\n3）分析难点和解决方案\n4）大数据技术选型\n三、大数据平台搭建\n1）分布式环境的模拟建立\n2）网络环境的调通\n3）身份验证与集群控制\n4）Hadoop环境搭建和要点说明\n5）MapReduce与Yarn的搭建和说明\n四、大数据分析脚本编写\n1）MapReduce脚本编写\n2）拆解数据需求\n3）Map逻辑详写\n4）Reduce逻辑详写\n5）结果整理与输出\n五、结果可视化\n1）可视化需求和技术选型\n2）展示页面的快速铺设\n3）可视化JS上手\n4）使用可视化JS展示结果\n项目三、AI法律咨询大数据分析与服务智能推荐实战项目(第一季)\n本项目结合目前流行的大数据框架，在原有成熟业务的前提下，进行大数据分析处理，真实还原企业应用，让学员身临其境的感受企业大数据开发的整个流程。\n项目的业务系统底层主要采用JAVA架构，大数据分析主要采用Hadoop框架，其中包括Kettle实现ETL、SQOOP、Hive、Kibana、HBASE、Spark以及人工智能算法等框架技术；采用真实大数据集群环境的搭建，让学员切身感受企业项目的从0到1的过程。\n一、系统业务介绍\n1）底层业务实现框架讲解\n2）功能模块讲解\n二、系统架构设计\n1）总体架构分析\n2）数据流向\n3）各技术选型承载作用\n4）部署方案\n三、详尽实现\n1）原始数据处理\n2）ETL数据导入\n3）MR数据计算\n4）Hive数据分析\n四、数据可视化\n1）采用Highcharts插件展示客户偏好曲线图\n2）使用Tableau进行数据分析可视化展示\n五、项目优化\n1）ZooKeeper实现HA\n2）集群监控的整体联调\n项目四、AI大数据基站定位智能推荐商圈分析项目实战（第一季）\n随着当今个人手机终端的普及、出行人群中手机拥有率和使用率已达到相当高的比例，根据手机信号在真实地理空间的覆盖情况，将手机用户时间序列的手机定位数据，映射至现实地理位置空间位置，即可完整、客观地还原出手机用户的现实活动轨迹，从而挖掘出人口空间分布与活动联系特征信息。\n商圈是现代市场中企业市场活动的空间，同时也是商品和服务享用者的区域。商圈划分为目的之一是研究潜在顾客分布，以制定适宜的商业对策。\n本项目以实战为基础结合大数据技术Hadoop、.Net技术全栈为基础，采用真实商业数据，分不同环节构建商用系统、真实大数据环境、进行推断分析及呈现数据。\n一、分析系统业务逻辑讲解\n1）大数据基站定位智能推荐商圈分析系统介绍\n2）数据前期清洗和数据分析目标指标的设定等\n二、大数据导入与存储\n1）关系型数据库基础知识\n2）hive的基本语法\n3）hive的架构及设计原理\n4）hive安装部署与案例等\n5）Sqoop安装及使用\n6）Sqoop与关系型数据库进行交互等\n7）动手实践\n三、Hbase理论及实战\n1）Hbase简介、安装及配置\n2）Hbase的数据存储与数据模型\n3）Hbase Shell\n4）Hbase 访问接口\n5）Hbase数据备份与恢复方法等\n6）动手实践（数据转储与备份）\n四、基站数据分析与统计推断\n1）背景与分析推断目标\n2）分析方法与过程推断\n3）动手实践（分析既定指标数据）\n五、数据分析与统计推断结果的展示（大数据可视化）\n1）使用Tableau展示数据分析结果\n2）使用HighCharts、ECharts展示数据分析结果\n阶段十一、区块链\n区块链(Blockchain)是分布式数据存储、点对点传输、共识机制、加密算法等计算机技术的新型应用模式。所谓共识机制是区块链系统中实现不同节点之间建立信任、获取权益的数学算法。\n区块链是比特币的底层技术，像一个数据库账本，记载所有的交易记录。这项技术也因其安全、便捷的特性逐渐得到了银行与金融业的关注。\n一、课程介绍\n1）区块链的发展\n2）课程安排\n3）学习目标\n二、区块链的技术架构\n1）数据层 创世区块 交易记录 私钥，公钥和钱包地址\n2）数据层 \u0026 通讯层 记账原理 Merkle 树和简单支付验证（SPV） P2P通讯 数据通信和验证\n3）共识层\n4）激励层 拜占庭将军问题与POW Pos DPos PBFT 挖矿 交易费 图灵完备和非完备\n5）合约层 比特币脚本 以太坊智能合约 fabic智能合约 RPC远程调用\n6）应用层\n7）总结 接口调用 DAPP的使用 应用场景的部署 重要概念和原理\n三、环境搭建\n1）以太坊 以太坊介绍 以太坊开发过程 图形界面客户端使用 供应链的应用 保险领域的应用 DAO的介绍和应用\n2）以太坊 以太坊本地开发环境的搭建 以太坊分布式集群环境的搭建\n3）hyperledger项目fabric介 fabric介绍 fabric本地开发环境搭建 fabric分布式集群环境搭建\n四、案例和DEMO\n1）案例讲解 支付和清结算 公益行业的应用 供应链的应用 保险领域的应用 DAO的介绍和应用\n2）Demo介绍 发币和交易Demo\n3）Demo介绍 数据资产的确权和追溯\n阶段十二、用人工智能预测金融量化交易投资系列课程\n程序化交易：又称程式交易,发源于上世纪80年代的美国,其最初的定义是指在纽约股票交易所(NYSE)市场上同时买卖超过15只以上的股票组合；像高盛、摩根士丹利及德意志银行都是在各大交易市场程序化交易的最活跃参与会员。\n本课程主要面向意愿从事金融量化交易人员、金融行业从业人员、金融策略开发人员及投资经验丰富而想实现计算机自动下单人员；主要讲解了证券期货程序化实现原理及过程，通过本课程的学习，您可以根据自己的意愿打造属于自己的量化投资交易系统； 本课程主要用到的技术手段有：Python、Pandas、数据分析、数据挖掘机器学习等。\n一、程序化交易数据获取与清洗讲解\n1）数据的清洗与合成\n2）K线图绘制\n3）技术指标开发讲解\n4）数据的获取\n二、回测框架搭建讲解\n1）回测框架搭建背景及基本流程讲解\n2）回测框架实现及收益指标讲解\n三、程序化交易部分实现讲解\n1）CTP技术讲解\n2）程序化API讲解\n3）程序化交易具体实现讲解\n阶段十三、阿里云认证\n课程一、云计算 - 网站建设：部署与发布\n阿里云网站建设认证课程教你如何掌握将一个本地已经设计好的静态网站发布到Internet公共互联网，绑定域名，完成工信部的ICP备案。\n课程二、云计算 - 网站建设：简单动态网站搭建\n阿里云简单动态网站搭建课程教你掌握如何快速搭建一个WordPress动态网站，并会对网站进行个性化定制，以满足不同的场景需求。\n课程三、云计算 - 云服务器管理维护\n阿里云服务器运维管理课程教你掌握快速开通一台云服务器，并通过管理控制台方便地进行服务器的管理、服务器配置的变更和升级、数据的备份，并保证其可以正常运转并按业务需求随时进行配置的变更。\n课程四、云计算 - 云数据库管理与数据迁移\n阿里云云数据库管理与数据迁移认证课程掌握云数据库的概念，如何在云端创建数据库、将自建数据库迁移至云数据库MySQL版、数据导入导出，以及云数据库运维的常用操作。\n课程五、云计算 - 云存储：对象存储管理与安全\n阿里云云储存认证课程教你掌握安全、高可靠的云存储的使用，以及在云端存储下载文件，处理图片，以及如何保护数据的安全。\n课程六、云计算 - 超大流量网站的负载均衡\n掌握如何为网站实现负载均衡，以轻松应对超大流量和高负载。\n课程七、大数据 - MOOC网站日志分析\n本课程可以帮助学员掌握如何收集用户访问日志，如何对访问日志进行分析，如何利用大数据计算服务对数据进行处理，如何以图表化的形式展示分析后的数据。\n课程八、大数据 - 搭建企业级数据分析平台\n模拟电商场景，搭建企业级的数据分析平台，用来分析商品数据、销售数据以及用户行为等。\n课程九、大数据 - 基于LBS的热点店铺搜索\n本课程可以帮助学员掌握如何在分布式计算框架下开发一个类似于手机地图查找周边热点（POI）的功能，掌握GeoHash编码原理，以及在地理位置中的应用，并能将其应用在其他基于LBS的定位场景中。\n课程中完整的演示了整个开发步骤，学员在学完此课程之后，掌握其原理，可以在各种分布式计算框架下完成此功能的开发，比如MapReduce、Spark。\n课程十、大数据 - 基于机器学习PAI实现精细化营销\n本课程通过一个简单案例了解、掌握企业营销中常见的、也是必需的精准营销数据处理过程，了解机器学习PAI的具体应用，指导学员掌握大数据时代营销的利器---通过机器学习实现营销。\n课程十一、大数据 - 基于机器学习的客户流失预警分析\n本课程讲解了客户流失的分析方法、流程，同时详细介绍了机器学习中常用的分类算法、集成学习模型等通用技能，并使用阿里云机器学习PAI实现流失预警分析。可以帮助企业快速、准确识别流失客户，辅助制定策略进行客户关怀，达到挽留客户的目的。\n课程十二、大数据 - 使用DataV制作实时销售数据可视化大屏\n帮助非专业工程师通过图形化的界面轻松搭建专业水准的实时可视化数据大屏，以满足业务展示、业务监控、风险预警等多种业务的展示需求。\n课程十三、大数据 - 使用MaxCompute进行数据质量核查\n通过本案例，学员可了解影响数据质量的因素，出现数据质量问题的类型，掌握通过MaxCompute（DateIDE）设计数据质量监控的方法，最终独立解决常见的数据质量监控需求。\n课程十四、大数据 - 使用Quick BI制作图形化报表\n阿里云Quick BI制作图形化报表认证课程教你掌握将电商运营过程中的数据进行图表化展现，掌握通过Quick BI将数据制作成各种图形化报表的方法，同时还将掌握搭建企业级报表门户的方法。\n课程十五、大数据 - 使用时间序列分解模型预测商品销量\n使用时间序列分解模型预测商品销量教你掌握商品销量预测方法、时间序列分解以及熟悉相关产品的操作演示和项目介绍。\n课程十六、云安全 - 云平台使用安全\n阿里云云平台使用安全认证课程教你了解由传统IT到云计算架构的变迁过程、当前信息安全的现状和形势，以及在云计算时代不同系统架构中应该从哪些方面利用云平台的优势使用安全风险快速降低90%。\n课程十七、云安全 - 云上服务器安全\n阿里云云上服务器安全认证课程教你了解在互联网上提供计算功能的服务器主要面临哪些安全风险，并针对这些风险提供了切实可行的、免费的防护方案。\n课程十八、云安全 - 云上网络安全\n了解网络安全的原理和解决办法，以及应对DDoS攻击的方法和防护措施，确保云上网络的安全。\n课程十九、云安全 - 云上数据安全\n了解云上数据的安全隐患，掌握数据备份、数据加密、数据传输安全的解决方法。\n课程二十、云安全 - 云上应用安全\n了解常见的应用安全风险，SQL注入原理及防护，网站防篡改的解决方案等，确保云上应用的安全。\n课程二十一、云安全 - 云上安全管理\n了解云上的安全监控方法，学会使用监控大屏来监控安全风险，并能够自定义报警规则，确保随时掌握云上应用的安全情况。\n阶段十四、IT高级开发者职场生存规则 － 职业素养\n本课程主要为广大毕业生或者工作经验较少的学员而设立，主要是为了在职业素养方面给大家提供辅导，为更加顺利走向职场而提供帮助。\n为什么有些同学在技能方面过关，却还是给予别人一种书生气的感觉？\n为什么简历已经通过了，却还是没有通过HR的面试？\n为什么入职后，与同事的沟通总是存在问题?\n为什么每天的时间都不够用，无法兼顾生活学习和工作?\n为什么学习一段时间后，对工作对职场没有方向感?\n为什么遇到事情，别人总是能够保持良好心态游刃有余，而我总是问题百出？\nCOT课程正是引领大家一起来探索其中的奥秘和方法，让大家一起在学习过程中不断深思和进步，让大家的职场路越走越顺畅！\n1）团队协作\n2）心态管理\n3）目标管理\n4）时间管理","data":"2018年06月08日 10:22:19"}
{"_id":{"$oid":"5d34527a62f717dc0659b7a3"},"title":"自然语言处理-从规则到统计","author":"伐木场的博客","content":"1.总述\n人类对机器理解语言的认识走了一条大弯路。早期的研究集中采用基于规则的方法，虽然解决了一些简单的问题，但是无法从根本上将自然语言理解实用化。直到20多年后，人们开始尝试用基于统计的方法进行自然语言处理，才有了突破性的进展和实用的产品。\n\n\n2.前文回顾\n上一篇讲到，语言的出现是为了人类之间的通信。字母（或者中文的笔画）、文字和数字实际上是信息编码的不同单位。任何一种语言都是一种编码的方式，而语言的语法规则是编码的算法。我们把一个要表达的意思，通过某种语言的一句话表达出来，就是用这种语言的编码方式对头脑中的信息做了一次编码，编码的结果就是一串文字。而如果对方懂得这门语言，他或者她就可以用这门语言的解码方法获得说话人要表达的信息。这就是语言的数学本质。虽然动物也能做到传递信息，但是利用语言来传递信息是人类的特质。\n编码                        解码\n信息（信息源）------\u003e信息（信道）-------\u003e信息（接收者）\n\n\n\n3.两个问题\na.计算机能否处理自然语言？\nb.如果能，那么它处理自然语言的方法是否和人类一致？\n对这两个问题，答案都是肯定的。\n\n\n4.机器智能\na.图灵测试（Turing Test）\n让人和机器进行交流，如果人无法判断自己交流的对象是人还是机器，就说明这个机器有智能了。\n\n\nb.弯路阶段\n从20世纪50年代到70年代，是科学家们走弯路的阶段，全世界的科学家对计算机处理语言的认识都局限在人类学习语言的方式上，也就是说，用电脑模拟人脑（“鸟飞派”），这20多年的成果近乎为零。\n\n\nc.第二阶段\n直到20世纪70年代，一些自然语言处理的先驱开始重新认识这个问题，找到了基于数学模型和统计的方法，自然语言处理进入第二个阶段。30多年来，这个领域取得了实质性的突破，自然语言处理也在很多产品中得到广泛应用。今天，机器翻译和语音识别已经做得不错，并且有上亿人使用过，但是这个领域之外的大部分人已然错误地以为这两种应用是靠近计算机理解了自然语言才实现的。事实上，它们全都靠的是数学，更准确地说是靠统计学。\n\n\n5.理解自然语言\n\na.分析语句和获取语义\n应用层  语音识别  机器翻译  自动回答  自动摘要\n认知层  自然语言理解\n基础层  句法分析  语义分析\n\n\nb.从规则到统计\n\n在上个世纪70年代，基于规则的句法分析（包括文法分析或者语义分析）很快走到了尽头。而对于语义的处理则遇到了更大的麻烦。\n首先，自然语言中词的多义性很难用规则来描述，而是严重依赖于上下文，甚至是常识。\n第二点，也很有意思，用基于统计的方法代替传统的方法，需要等原有的一批语言学家退休。\n1970年以后统计语言学家的出现使得自然语言处理重获新生，并取得了今天非凡的成就。推动这个技术路线转变的关键人物是贾里尼克和他领导的IBM华生实验室。最初，他们也没有想解决整个自然语言处理的各种问题，而只是希望解决语音识别的问题。采用基于统计的方法，IBM将当时的语音识别率从70%提升到90%，同时语音识别的规模从几百单词上升到几万单词，这样语音识别就有了从实验室走向实际应用的可能。\n\n\n6.小结\n基于统计的自然语言处理方法，在数学模型和通信是相通的，甚至是相同的。因此，在数学意义上自然语言处理又和语言的初衷-通信联系在一起了。但是，科学家们用了几十年才认识到这个联系。","data":"2016年12月29日 08:56:56","date":"2016年12月29日 08:56:56"}
{"_id":{"$oid":"5d34529662f717dc0659b7b2"},"title":"一文读懂自然语言处理NLP","author":"长空飞鹰","content":"前言\n\n\n\n自然语言处理是文本挖掘的研究领域之一，是人工智能和语言学领域的分支学科。在此领域中探讨如何处理及运用自然语言。\n\n\n对于自然语言处理的发展历程，可以从哲学中的经验主义和理性主义说起。基于统计的自然语言处理是哲学中的经验主义，基于规则的自然语言处理是哲学中的理性主义。在哲学领域中经验主义与理性主义的斗争一直是此消彼长，这种矛盾与斗争也反映在具体科学上，如自然语言处理。\n\n\n早期的自然语言处理具有鲜明的经验主义色彩。如 1913 年马尔科夫提出马尔科夫随机过程与马尔科夫模型的基础就是“手工查频”，具体说就是统计了《欧根·奥涅金》长诗中元音与辅音出现的频度；1948 年香农把离散马尔科夫的概率模型应用于语言的自动机，同时采用手工方法统计英语字母的频率。\n\n\n然而这种经验主义到了乔姆斯基时出现了转变。\n\n\n1956 年乔姆斯基借鉴香农的工作，把有限状态机用作刻画语法的工具，建立了自然语言的有限状态模型，具体来说就是用“代数”和“集合”将语言转化为符号序列，建立了一大堆有关语法的数学模型。这些工作非常伟大，为自然语言和形式语言找到了一种统一的数学描述理论，一个叫做“形式语言理论”的新领域诞生了。这个时代，“经验主义”被全盘否定，“理性主义”算是完胜。\n\n\n然而在 20 世纪 50 年代末到 60 年代中期，经验主义东山再起了。多数学者普遍认为只有详尽的历史语料才能带来靠谱的结论。于是一些比较著名的理论与算法就诞生了，如贝叶斯方法（Bayesian Method）、隐马尔可夫、最大熵、Viterbi 算法、支持向量机之类。世界上第一个联机语料库也是在那个时候的 Brown University 诞生的。\n\n\n但是总的来说，这个时代依然是基于规则的理性主义的天下，经验主义虽然取得了不俗的成就，却依然没有受到太大的重视。但是金子总会发光的。\n\n\n90 年代以来，基于统计的自然语言处理就开始大放异彩了。首先是在机器翻译领域取得了突破，因为引入了许多基于语料库的方法（哈钦斯，英国著名学者）。1990 年在芬兰赫尔辛基举办的第 13 届国际计算语言学会议确定的主题是“处理大规模真实文本的理论、方法与工具”，大家的重心开始转向大规模真实文本了，传统的仅仅基于规则的自然语言处理显然力不从心了。学者们认为，大规模语料至少是对基于规则方法有效的补充。\n\n\n到了 1994~1999 年，经验主义就开始空前繁荣了。如句法剖析、词类标注、参照消解、话语处理的算法几乎把“概率”与“数据”作为标准方法，成为了自然语言处理的主流。\n\n\n总之，理性主义在自然语言处理的发展史上是有重要地位的，也辉煌了几十年，历史事物常常是此消彼长的，至于谁好谁坏，不是固定的，取决于不同时代的不同历史任务。总的来说，基于规则的理性主义在这个时代被提及得比较少，用的也比较少，主要是由于以下几个缺陷：\n\n\n• 鲁棒性差，过于严格的规则导致对非本质错误的零容忍（这一点在最近的一些新的剖析技术上有所改善）；\n\n\n• 研究强度大，泛化能力差。一个研究要语言学家、语音学家和各种领域的专家配合，在当前大规模文本处理的时间、资源要求下太不划算。且机器学习的方法很难应用，难以普及；\n\n\n• 实践性差。基于统计的经验主义方法可以根据数据集不断对参数进行优化，而基于规则的方法就不可以，这在当前数据量巨大的情况下，影响是致命的，因为前者常常可以通过增大训练集来获得更好的效果，后者则死板许多，结果往往不尽人意。\n\n\n\n但理性主义还是有很多优点的，同样经验主义也有很多缺陷，算是各有所长、各有所短。不同学科有不同学科的研究角度，只能说某些角度在某个特定的历史时期对提高生产力“更有用”，所以重视的人更多。但“有用”不代表胜利，暂时的“无用”更不能说是科学层面上的“失败”。尤其是在当前中文自然语言处理发展还不甚成熟的时期，私以为基于统计的方法在很多方面并不完美，“理性主义”的作用空间还很大，需要更多的人去关注、助力。\n——《统计自然语言处理》宗成庆\n\n\n自然语言处理涉及的范畴如下（维基百科）：\n\n\n• 中文自动分词（Chinese word segmentation）\n• 词性标注（Part-of-speech tagging）\n• 句法分析（Parsing）\n• 自然语言生成（Natural language generation）\n• 文本分类（Text categorization）\n• 信息检索（Information retrieval）\n• 信息抽取（Information extraction）\n• 文字校对（Text-proofing）\n• 问答系统（Question answering）\n• 机器翻译（Machine translation）\n• 自动摘要（Automatic summarization）\n\n\n本文针对其中几个主要领域的研究现状和进展，通过论文、博客等资料，结合自身的学习和实践经历进行浅显地介绍。由于个人实践经验不足，除中文分词、自动文摘、文本分类、情感分析和话题模型方面进行过实际业务的实践，其他方面经验欠缺，若有不当之处，欢迎童鞋们批评指正！\n目录\n\n\n一. 中文分词\n\n\n中文分词主要包括词的歧义切分和未登录词识别，主要可以分为基于词典和基于统计的方法，最新的方法是多种方法的混合。从目前汉语分词研究的总体水平看，F1 值已经达到 95% 左右，主要分词错误是由新词造成的，尤其对领域的适应性较差。下面主要介绍一下中文分词存在的主要问题和分词方法。\n\n\n1. 问题\n\n\n1.1 歧义切分\n\n\n切分歧义处理包括两部分内容：\n\n\n• 切分歧义的检测；\n\n\n• 切分歧义的消解。\n\n\n这两部分在逻辑关系上可分成两个相对独立的步骤。\n\n\n• 切分歧义的检测。“最大匹配法”（精确的说法应该叫“最长词优先匹配法”） 是最早出现、同时也是最基本的汉语自动分词方法。依扫描句子的方向，又分正向最大匹配 MM（从左向右）和逆向最大匹配 RMM（从右向左）两种。\n\n\n最大匹配法实际上将切分歧义检测与消解这两个过程合二为一，对输入句子给出唯一的切分可能性，并以之为解。从最大匹配法出发导出了“双向最大匹配法”，即 MM＋ RMM。双向最大匹配法存在着切分歧义检测盲区。\n\n\n针对切分歧义检测，另外两个有价值的工作是“最少分词法”，这种方法歧义检测能力较双向最大匹配法要强些，产生的可能切分个数仅略有增加；和“全切分法”，这种方法穷举所有可能的切分，实现了无盲区的切分歧义检测，但代价是导致大量的切分“垃圾”。\n\n\n• 切分歧义的消解。典型的方法包括句法统计和基于记忆的模型。句法统计将自动分词和基于 Markov 链的词性自动标注技术结合起来，利用从人工标注语料库中提取出的词性二元统计规律来消解切分歧义，基于记忆的模型对伪歧义型高频交集型歧义切分，可以把它们的正确（唯一）切分形式预先记录在一张表中，其歧义消解通过直接查表即可实现。\n\n\n1.2 未登录词识别\n\n\n未登录词大致包含两大类：\n\n\n• 新涌现的通用词或专业术语等；\n\n\n• 专有名词。如中国人名、外国译名、地名、机构名（泛指机关、团体和其它企事业单位）等。\n\n\n前一种未登录词理论上是可预期的，能够人工预先添加到词表中（但这也只是理想状态，在真实环境下并不易做到）；后一种未登录词则完全不可预期，无论词表多么庞大，也无法囊括。\n\n\n真实文本中（即便是大众通用领域），未登录词对分词精度的影响超过了歧义切分。未登录词处理在实用型分词系统中占的份量举足轻重。\n\n\n• 新涌现的通用词或专业术语。对这类未登录词的处理，一般是在大规模语料库的支持下，先由机器根据某种算法自动生成一张候选词表（无监督的机器学习策略），再人工筛选出其中的新词并补充到词表中。\n\n\n鉴于经过精加工的千万字、甚至亿字级的汉语分词语料库目前还是水月镜花，所以这个方向上现有的研究无一不以从极大规模生语料库中提炼出的 n 元汉字串之分布（n≥2）为基础。其中汉字之间的结合力通过全局统计量包括互信息、t- 测试差、卡方统计量、字串频等来表示。\n\n\n• 专有名词。对专有名词的未登录词的处理，首先依据从各类专有名词库中总结出的统计知识 （如姓氏用字及其频度）和人工归纳出的专有名词的某些结构规则，在输入句子中猜测可能成为专有名词的汉字串并给出其置信度，之后利用对该类专有名词有标识意义的紧邻上下文信息（如称谓），以及全局统计量和局部统计量（局部统计量是相对全局统计量而言的，是指从当前文章得到且其有效范围一般仅限于该文章的统计量，通常为字串频），进行进一步的鉴定。\n\n\n已有的工作涉及了四种常见的专有名词：中国人名的识别、外国译名的识别、中国地名的识别及机构名的识别。\n\n\n从各家报告的实验结果来看，外国译名的识别效果最好，中国人名次之，中国地名再次之，机构名最差。而任务本身的难度实质上也是遵循这个顺序由小增大。 沈达阳、孙茂松等（1997b）特别强调了局部统计量在未登录词处理中的价值。\n\n\n2. 方法\n\n\n2.1 基于词典的方法\n\n\n在基于词典的方法中，对于给定的词，只有词典中存在的词语能够被识别，其中最受欢迎的方法是最大匹配法（MM），这种方法的效果取决于词典的覆盖度，因此随着新词不断出现，这种方法存在明显的缺点。\n\n\n2.2 基于统计的方法\n\n\n基于统计的方法由于使用了概率或评分机制而非词典对文本进行分词而被广泛应用。这种方法主要有三个缺点：\n\n\n一是这种方法只能识别 OOV（out-of-vocabulary）词而不能识别词的类型，比如只能识别为一串字符串而不能识别出是人名；二是统计方法很难将语言知识融入分词系统，因此对于不符合语言规范的结果需要额外的人工解析；三是在许多现在分词系统中，OOV 词识别通常独立于分词过程。\n\n\n二. 词性标注\n\n\n词性标注是指为给定句子中的每个词赋予正确的词法标记，给定一个切好词的句子，词性标注的目的是为每一个词赋予一个类别，这个类别称为词性标记（part-of-speech tag），比如，名词（noun）、动词（verb）、形容词（adjective）等。\n\n\n它是自然语言处理中重要的和基础的研究课题之一，也是其他许多智能信息处理技术的基础，已被广泛的应用于机器翻译、文字识别、语音识别和信息检索等领域。\n\n\n词性标注对于后续的自然语言处理工作是一个非常有用的预处理过程，它的准确程度将直接影响到后续的一系列分析处理任务的效果。\n\n\n长期以来，兼类词的词性歧义消解和未知词的词性识别一直是词性标注领域需要解决的热点问题。当兼类词的词性歧义消解变得困难时，词性的标注就出现了不确定性的问题。而对那些超出了词典收录范围的词语或者新涌现的词语的词性推测，也是一个完整的标注系统所应具备的能力。\n\n\n1. 词性标注方法\n词性标注是一个非常典型的序列标注问题。最初采用的方法是隐马尔科夫生成式模型， 然后是判别式的最大熵模型、支持向量机模型，目前学术界通常采用结构感知器模型和条件随机场模型。\n\n\n近年来，随着深度学习技术的发展，研究者们也提出了很多有效的基于深层神经网络的词性标注方法。\n\n\n迄今为止，词性标注主要分为基于规则的和基于统计的方法。\n\n\n• 规则方法能准确地描述词性搭配之间的确定现象，但是规则的语言覆盖面有限，庞大的规则库的编写和维护工作则显得过于繁重，并且规则之间的优先级和冲突问题也不容易得到满意的解决。\n\n\n• 统计方法从宏观上考虑了词性之间的依存关系，可以覆盖大部分的语言现象，整体上具有较高的正确率和稳定性，不过其对词性搭配确定现象的描述精度却不如规则方法。\n\n\n针对这样的情况，如何更好地结合利用统计方法和规则处理手段，使词性标注任务既能够有效地利用语言学家总结的语言规则，又可以充分地发挥统计处理的优势成为了词性标注研究的焦点。\n\n\n2. 词性标注研究进展\n\n\n• 词性标注和句法分析联合建模：研究者们发现，由于词性标注和句法分析紧密相关，词性标注和句法分析联合建模可以同时显著提高两个任务准确率。\n\n\n• 异构数据融合：汉语数据目前存在多个人工标注数据，然而不同数据遵守不同的标注规范，因此称为多源异构数据。近年来，学者们就如何利用多源异构数据提高模型准确率，提出了很多有效的方法，如基于指导特征的方法、基于双序列标注的方法、以及基于神经网络共享表示的方法。\n\n\n• 基于深度学习的方法：传统词性标注方法的特征抽取过程主要是将固定上下文窗口的词进行人工组合，而深度学习方法能够自动利用非线性激活函数完成这一目标。进一步，如果结合循环神经网络如双向 LSTM，则抽取到的信息不再受到固定窗口的约束，而是考虑整个句子。\n\n\n除此之外，深度学习的另一个优势是初始词向量输入本身已经刻画了词语之间的相似度信息，这对词性标注非常重要。\n三. 句法分析\n\n\n语言语法的研究有非常悠久的历史，可以追溯到公元前语言学家的研究。不同类型的句法分析体现在句法结构的表示形式不同，实现过程的复杂程度也有所不同。因此，科研人员采用不同的方法构建符合各个语法特点的句法分析系统。其主要分类如下图所示：\n\n\n\n\n下文主要对句法分析技术方法和研究现状进行总结分析：\n\n\n1. 依存句法分析\n\n\n依存语法存在一个共同的基本假设：句法结构本质上包含词和词之间的依存（修饰）关系。一个依存关系连接两个词，分别是核心词（head）和依存词（dependent）。依存关系可以细分为不同的类型，表示两个词之间的具体句法关系。\n\n\n目前研究主要集中在数据驱动的依存句法分析方法，即在训练实例集合上学习得到依存句法分析器，而不涉及依存语法理论的研究。数据驱动的方法的主要优势在于给定较大规模的训练数据，不需要过多的人工干预，就可以得到比较好的模型。因此，这类方法很容易应用到新领域和新语言环境。\n\n\n数据驱动的依存句法分析方法主要有两种主流方法：基于图（ graph-based）的分析方法和基于转移（ transition-based）的分析方法。\n\n\n2.1 基于图的依存句法分析方法\n\n\n基于图的方法将依存句法分析问题看成从完全有向图中寻找最大生成树的问题。一棵依存树的分值由构成依存树的几种子树的分值累加得到。\n\n\n根据依存树分值中包含的子树的复杂度，基于图的依存分析模型可以简单区分为一阶和高阶模型。高阶模型可以使用更加复杂的子树特征，因此分析准确率更高，但是解码算法的效率也会下降。\n\n\n基于图的方法通常采用基于动态规划的解码算法，也有一些学者采用柱搜索（beam search）来提高效率。学习特征权重时，通常采用在线训练算法，如平均感知器（averaged perceptron）。\n\n\n2.2 基于转移的依存句法分析方法\n\n\n基于转移的方法将依存树的构成过程建模为一个动作序列，将依存分析问题转化为寻找最优动作序列的问题。早期，研究者们使用局部分类器（如支持向量机等）决定下一个动作。近年来，研究者们采用全局线性模型来决定下一个动作，一个依存树的分值由其对应的动作序列中每一个动作的分值累加得到。\n\n\n特征表示方面，基于转移的方法可以充分利用已形成的子树信息，从而形成丰富的特征，以指导模型决策下一个动作。模型通过贪心搜索或者柱搜索等解码算法找到近似最优的依存树。和基于图的方法类似，基于转移的方法通常也采用在线训练算法学习特征权重。\n\n\n2.3 多模型融合的依存句法分析方法\n\n\n基于图和基于转移的方法从不同的角度解决问题，各有优势。基于图的模型进行全局搜索但只能利用有限的子树特征，而基于转移的模型搜索空间有限但可以充分利用已构成的子树信息构成丰富的特征。详细比较发现，这两种方法存在不同的错误分布。\n\n\n因此，研究者们使用不同的方法融合两种模型的优势，常见的方法有：stacked learning；对多个模型的结果加权后重新解码（re-parsing）；从训练语料中多次抽样训练多个模型（bagging）。\n\n\n2. 短语结构句法分析\n\n\n分词，词性标注技术一般只需对句子的局部范围进行分析处理，目前已经基本成熟，其标志就是它们已经被成功地用于文本检索、文本分类、信息抽取等应用之中，而句法分析、语义分析技术需要对句子进行全局分析，目前，深层的语言分析技术还没有达到完全实用的程度。\n\n\n短语结构句法分析的研究基于上下文无关文法（Context Free Grammar，CFG）。上下文无关文法可以定义为四元组，其中 T 表示终结符的集合（即词的集合），N 表示非终结符的集合（即文法标注和词性标记的集合），S 表示充当句法树根节点的特殊非终结符，而 R 表示文法规则的集合，其中每条文法规则可以表示为 Ni®g ，这里的 g 表示由非终结符与终结符组成的一个序列（允许为空）。\n\n\n根据文法规则的来源不同，句法分析器的构建方法总体来说可以分为两大类：\n\n\n• 人工书写规则\n\n\n• 从数据中自动学习规则\n\n\n人工书写规则受限于规则集合的规模：随着书写的规则数量的增多，规则与规则之间的冲突加剧，从而导致继续添加规则变得困难。\n\n\n与人工书写规模相比，自动学习规则的方法由于开发周期短和系统健壮性强等特点，加上大规模人工标注数据，比如宾州大学的多语种树库的推动作用，已经成为句法分析中的主流方法。\n\n\n而数据驱动的方法又推动了统计方法在句法分析领域中的大量应用。为了在句法分析中引入统计信息，需要将上下文无关文法扩展成为概率上下文无关文法（Probabilistic Context Free Grammar，PCFG），即为每条文法规则指定概率值。\n\n\n概率上下文无关文法与非概率化的上下文无关文法相同，仍然表示为四元组，区别在于概率上下文无关文法中的文法规则必须带有概率值。\n\n\n获得概率上下文无关文法的最简单的方法是直接从树库中读取规则，利用最大似然估计（Maximum Likelihood Estimation，MLE）计算得到每条规则的概率值。使用该方法得到的文法可以称为简单概率上下文无关文法。在解码阶段，CKY 10 等解码算法就可以利用学习得到的概率上下文无关文法搜索最优句法树。\n\n\n虽然基于简单概率上下文无关文法的句法分析器的实现比较简单，但是这类分析器的性能并不能让人满意。\n\n\n性能不佳的主要原因在于上下文无关文法采取的独立性假设过强：一条文法规则的选择只与该规则左侧的非终结符有关，而与任何其它上下文信息无关。文法中缺乏其它信息用于规则选择的消歧。因此后继研究工作的出发点大都基于如何弱化上下文无关文法中的隐含独立性假设。\n\n\n3. 总结\n\n\n分词，词性标注技术一般只需对句子的局部范围进行分析处理，目前已经基本成熟，其标志就是它们已经被成功地用于文本检索、文本分类、信息抽取等应用之中，而句法分析、语义分析技术需要对句子进行全局分析，目前，深层的语言分析技术还没有达到完全实用的程度。\n\n\n四. 文本分类\n\n\n文本分类是文本挖掘的核心任务，一直以来倍受学术界和工业界的关注。文本分类（Text Classification）的任务是根据给定文档的内容或主题，自动分配预先定义的类别标签。\n\n\n对文档进行分类，一般需要经过两个步骤：\n\n\n• 文本表示\n\n\n• 学习分类\n\n\n文本表示是指将无结构化的文本内容转化成结构化的特征向量形式，作为分类模型的输入。在得到文本对应的特征向量后，就可以采用各种分类或聚类模型，根据特征向量训练分类器或进行聚类。因此，文本分类或聚类的主要研究任务和相应关键科学问题如下：\n\n\n1. 任务\n\n\n1.1 构建文本特征向量\n\n\n构建文本特征向量的目的是将计算机无法处理的无结构文本内容转换为计算机能够处理的特征向量形式。文本内容特征向量构建是决定文本分类和聚类性能的重要环节。\n\n\n为了根据文本内容生成特征向量，需要首先建立特征空间。其中典型代表是文本词袋（Bag of Words）模型，每个文档被表示为一个特征向量，其特征向量每一维代表一个词项。所有词项构成的向量长度一般可以达到几万甚至几百万的量级。\n\n\n这样高维的特征向量表示如果包含大量冗余噪音，会影响后续分类聚类模型的计算效率和效果。\n\n\n因此，我们往往需要进行特征选择（Feature Selection）与特征提取（Feature Extraction），选取最具有区分性和表达能力的特征建立特征空间，实现特征空间降维；或者，进行特征转换（Feature Transformation），将高维特征向量映射到低维向量空间。特征选择、提取或转换是构建有效文本特征向量的关键问题。\n\n\n1.2 建立分类或聚类模型\n\n\n在得到文本特征向量后，我们需要构建分类或聚类模型，根据文本特征向量进行分类或聚类。\n\n\n其中，分类模型旨在学习特征向量与分类标签之间的关联关系，获得最佳的分类效果； 而聚类模型旨在根据特征向量计算文本之间语义相似度，将文本集合划分为若干子集。 分类和聚类是机器学习领域的经典研究问题。\n\n\n我们一般可以直接使用经典的模型或算法解决文本分类或聚类问题。例如，对于文本分类，我们可以选用朴素贝叶斯、决策树、k-NN、逻辑回归（Logistic Regression）、支持向量机（Support Vector Machine, SVM）等分类模型。\n\n\n对于文本聚类，我们可以选用 k-means、层次聚类或谱聚类（spectral clustering）等聚类算法。 这些模型算法适用于不同类型的数据而不仅限于文本数据。\n\n\n但是，文本分类或聚类会面临许多独特的问题，例如，如何充分利用大量无标注的文本数据，如何实现面向文本的在线分类或聚类模型，如何应对短文本带来的表示稀疏问题，如何实现大规模带层次分类体系的分类功能，如何充分利用文本的序列信息和句法语义信息，如何充分利用外部语言知识库信息，等等。这些问题都是构建文本分类和聚类模型所面临的关键问题。\n\n\n2. 模型\n\n\n2.1 文本分类模型\n\n\n近年来，文本分类模型研究层出不穷，特别是随着深度学习的发展，深度神经网络模型 也在文本分类任务上取得了巨大进展。我们将文本分类模型划分为以下三类：\n\n\n• 基于规则的分类模型\n\n\n基于规则的分类模型旨在建立一个规则集合来对数据类别进行判断。这些规则可以从训练样本里自动产生，也可以人工定义。给定一个测试样例，我们可以通过判断它是否满足某 些规则的条件，来决定其是否属于该条规则对应的类别。\n\n\n典型的基于规则的分类模型包括决策树（Decision Tree）、随机森林（Random Forest）、 RIPPER 算法等。\n\n\n• 基于机器学习的分类模型\n\n\n典型的机器学习分类模型包括贝叶斯分类器（Naïve Bayes）、线性分类器（逻辑回归）、 支持向量机（Support Vector Machine, SVM）、最大熵分类器等。\n\n\nSVM 是这些分类模型中比较有效、使用较为广泛的分类模型。它能够有效克服样本分布不均匀、特征冗余以及过拟合等问题，被广泛应用于不同的分类任务与场景。通过引入核函数，SVM 还能够解决原始特征空间线性不可分的问题。\n\n\n除了上述单分类模型，以 Boosting 为代表的分类模型组合方法能够有效地综合多个弱分类模型的分类能力。在给定训练数据集合上同时训练这些弱分类模型，然后通过投票等机制综合多个分类器的预测结果，能够为测试样例预测更准确的类别标签。\n\n\n• 基于神经网络的方法\n\n\n以人工神经网络为代表的深度学习技术已经在计算机视觉、语音识别等领域取得了巨大成功，在自然语言处理领域，利用神经网络对自然语言文本信息进行特征学习和文本分类，也成为文本分类的前沿技术。\n\n\n前向神经网络：多层感知机（Multilayer Perceptron, MLP）是一种典型的前向神经网络。它能够自动学习多层神经网络，将输入特征向量映射到对应的类别标签上。\n\n\n通过引入非线性激活层，该模型能够实现非线性的分类判别式。包括多层感知机在内的文本分类模型均使用了词袋模型假设，忽略了文本中词序和结构化信息。对于多层感知机模型来说，高质量的初始特征表示是实现有效分类模型的必要条件。\n\n\n为了更加充分地考虑文本词序信息，利用神经网络自动特征学习的特点，研究者后续提出了卷积神经网络（Convolutional Neural Network, CNN）和循环神经网络（Recurrent Neural Network, RNN）进行文本分类。\n\n\n基于 CNN 和 RNN 的文本分类模型输入均为原始的词序列，输出为该文本在所有类别上的概率分布。这里，词序列中的每个词项均以词向量的形式作为输入。\n\n\n卷积神经网络（CNN）：卷积神经网络文本分类模型的主要思想是，对词向量形式的文本输入进行卷积操作。CNN 最初被用于处理图像数据。与图像处理中选取二维域进行卷积操作不同，面向文本的卷积操作是针对固定滑动窗口内的词项进行的。\n\n\n经过卷积层、 池化层和非线性转换层后，CNN 可以得到文本特征向量用于分类学习。CNN 的优势在于在计算文本特征向量过程中有效保留有用的词序信息。\n\n\n针对 CNN 文本分类模型还有许多改进工作， 如基于字符级 CNN 的文本分类模型、将词位置信息加入到词向量。\n\n\n循环神经网络（RNN）：循环神经网络将文本作为字符或词语序列{x0 , … , xN}，对于第 t时刻输入的字符或词语 xt，都会对应产生新的低维特征向量 st。如图 3 所示，st 的取值会受到 xt 和上个时刻特征向量 st-1 的共同影响，st 包含了文本序列从 x0 到 xt 的语义信息。因此，我们可以利用 sN 作为该文本序列的特征向量，进行文本分类学习。\n\n\n与 CNN 相比，RNN 能够更自然地考虑文本的词序信息，是近年来进行文本表示最流行的方案之一。\n\n\n为了提升 RNN 对文本序列的语义表示能力，研究者提出很多扩展模型。\n\n\n例如，长短时记忆网络（LSTM）提出记忆单元结构，能够更好地处理文本序列中的长程依赖，克服循环神经网络梯度消失问题。如图 4 是 LSTM 单元示意图，其中引入了三个门（input gate, output gate, forget gate）来控制是否输入输出以及记忆单元更新。\n\n\n提升 RNN 对文本序列的语义表示能力的另外一种重要方案是引入选择注意力机制 (Selective Attention)，可以让模型根据具体任务需求对文本序列中的词语给予不同的关注度。\n\n\n3. 应用\n\n\n文本分类技术在智能信息处理服务中有着广泛的应用。例如，大部分在线新闻门户网站（如新浪、搜狐、腾讯等）每天都会产生大量新闻文章，如果对这些新闻进行人工整理非常耗时耗力，而自动对这些新闻进行分类，将为新闻归类以及后续的个性化推荐等都提供巨大帮助。\n\n\n互联网还有大量网页、论文、专利和电子图书等文本数据，对其中文本内容进行分类，是实现对这些内容快速浏览与检索的重要基础。此外，许多自然语言分析任务如观点挖掘、垃圾邮件检测等，也都可以看作文本分类或聚类技术的具体应用。\n\n\n对文档进行分类，一般需要经过两个步骤：（1）文本表示，以及（2）学习。文本表示是指将无结构化的文本内容转化成结构化的特征向量形式，作为分类模型的输入。在得到文本对应的特征向量后，就可以采用各种分类或聚类模型，根据特征向量训练分类器\n\n\n五. 信息检索\n\n\n信息检索（Information Retrieval, IR）是指将信息按一定的方式加以组织，并通过信息查找满足用户的信息需求的过程和技术。\n\n\n1951 年，Calvin Mooers 首次提出了“信息检索”的概念，并给出了信息检索的主要任务：协助信息的潜在用户将信息需求转换为一张文献来源列表，而这些文献包含有对其有用的信息。\n\n\n信息检索学科真正取得长足发展是在计算机诞生并得到广泛应用之后，文献数字化使得信息的大规模共享及保存成为现实，而检索就成为了信息管理与应用中必不可少的环节。\n\n\n互联网的出现和计算机硬件水平的提高使得人们存储和处理信息的能力得到巨大的提高，从而加速了信息检索研究的进步，并使其研究对象从图书资料和商用数据扩展到人们生活的方方面面。\n\n\n伴随着互联网及网络信息环境的迅速发展，以网络信息资源为主要组织对象的信息检索系统：搜索引擎应运而生，成为了信息化社会重要的基础设施。\n\n\n2016 年初，中文搜索引擎用户数达到 5.66 亿人，这充分说明搜索引擎在应用层次取得的巨大成功，也使得信息检索，尤其是网络搜索技术的研究具有了重要的政治、经济和社会价值。\n\n\n1. 内容结构\n\n\n检索用户、信息资源和检索系统三个主要环节组成了信息检索应用环境下知识获取与信息传递的完整结构，而当前影响信息获取效率的因素也主要体现在这几个环节，即：\n\n\n• 检索用户的意图表达\n\n\n• 信息资源（尤其是网络信息资源）的质量度量\n\n\n• 需求与资源的合理匹配\n\n\n具体而言，用户有限的认知能力导致其知识结构相对大数据时代的信息环境而言往往存在缺陷，进而影响信息需求的合理组织和清晰表述；数据资源的规模繁杂而缺乏管理，在互联网“注意力经济”盛行的环境下，不可避免地存在欺诈作弊行为，导致检索系统难以准确感知其质量；用户与资源提供者的知识结构与背景不同，对于相同或者相似事物的描述往往存在较大差异，使得检索系统传统的内容匹配技术难以很好应对，无法准确度量资源与需求的匹配程度。\n\n\n上述技术挑战互相交织，本质上反映了用户个体有限的认知能力与包含近乎无限信息的数据资源空间之间的不匹配问题。\n\n\n概括地讲，当前信息检索的研究包括如下四个方面的研究内容及相应的关键科学问题：\n\n\n1.1 信息需求理解\n\n\n面对复杂的泛在网络空间，用户有可能无法准确表达搜索意图；即使能够准确表达，搜索引擎也可能难以正确理解；即使能够正确理解，也难以与恰当的网络资源进行匹配。这使得信息需求理解成为了影响检索性能提高的制约因素，也构成了检索技术发展面临的第一个关键问题。\n\n\n1.2 资源质量度量\n\n\n资源质量管理与度量在传统信息检索研究中并非处于首要的位置，但随着互联网信息资源逐渐成为检索系统的主要查找对象，网络资源特有的缺乏编审过程、内容重复度高、质量参差不齐等问题成为了影响检索质量的重要因素。\n\n\n目前，搜索引擎仍旧面临着如何进行有效的资源质量度量的挑战，这构成了当前信息检索技术发展面临的第二个关键问题。\n\n\n1.3 结果匹配排序\n\n\n近年来，随着网络技术的进步，信息检索系统（尤其是搜索引擎）涉及的数据对象相应 的变得多样化、异质化，这也造成了传统的以文本内容匹配为主要手段的结果排序方法面临着巨大的挑战。\n\n\n高度动态繁杂的泛在网络内容使得文本相似度计算方法无法适用；整合复杂异构网络资源作为结果使得基于同质性假设构建的用户行为模型难以应对；多模态的交互方式则使得传统的基于单一维度的结果分布规律的用户行为假设大量失效。\n\n\n因此，在大数据时代信息进一步多样化、异质化的背景下，迫切需要构建适应现代信息资源环境的检索结果匹配排序方法，这是当前信息检索技术发展面临的第三个关键问题。\n\n\n1.4 信息检索评价\n\n\n信息检索评价是信息检索和信息获取领域研究的核心问题之一。信息检索和信息获取系统核心的目标是帮助用户获取到满足他们需求的信息，而评价系统的作用是帮助和监督研究开发人员向这一核心目标前进，以逐步开发出更好的系统，进而缩小系统反馈和用户需求之间的差距，提高用户满意度。\n\n\n因此，如何设计合理的评价框架、评价手段、评价指标，是当前信息检索技术发展面临的第四个关键问题。\n\n\n2. 个性化搜索\n\n\n现有的主要个性化搜索算法可分为基于内容分析的算法、基于链接分析的方法和基于协作过滤的算法。\n\n\n• 基于内容的个性化搜索算法通过比较用户兴趣爱好和结果文档的内容相似性来对文档的用户相关性进行判断进而对搜索结果进行重排。\n\n\n用户模型一般表述为关键词或主题向量或层次的形式。个性化算法通过比较用户模型和文档的相似性，判断真实的搜索意图，并估计文档对用户需求的匹配程度。\n\n\n• 基于链接分析的方法主要是利用互联网上网页之间的链接关系，并假设用户点击和访问过的网页为用户感兴趣的网页，通过链接分析算法进行迭代最终计算出用户对每个网页的喜好度。\n\n\n• 基于协作过滤的个性化搜索算法主要借鉴了基于协作过滤的推荐系统的思想，这种方法考虑到能够收集到的用户的个人信息有限，因此它不仅仅利用用户个人的信息，还利用与用户相似的其它用户或群组的信息，并基于用户群组和相似用户的兴趣偏好来个性化当前用户的搜索结果。用户之间的相似性可以通过用户的兴趣爱好、历史查询、点击过的网页等内容计算得出。\n\n\n3. 语义搜索技术\n\n\n随着互联网信息的爆炸式增长，传统的以关键字匹配为基础的搜索引擎，已越来越难以满足用户快速查找信息的需求。同时由于没有知识引导及对网页内容的深入整理，传统网页搜索返回的网页结果也不能精准给出所需信息。\n\n\n针对这些问题，以知识图谱为代表的语义搜索（Semantic Search）将语义 Web 技术和传统的搜索引擎技术结合，是一个很有研究价值 但还处于初期阶段的课题。\n\n\n在未来的一段时间，结合互联网应用需求的实际和技术、产品运营能力的实际发展水平，语义搜索技术的发展重点将有可能集中在以各种情境的垂直搜索资源为基础，知识化推理为检索运行方式，自然语言多媒体交互为手段的智能化搜索与推荐技术。\n\n\n首先将包括各类垂直搜索资源在内的深度万维网数据源整合成为提供搜索服务的资源池；随后利用广泛分布在公众终端计算设备上的浏览器作为客户端载体，通过构建的复杂情境知识库来开发多层次查询技术，并以此管理、调度、整合搜索云端的搜索服务资源，满足用户的多样化、多模态查询需求；最后基于面向情境体验的用户行为模型构建，以多模态信息推荐的形式实现对用户信息需求的主动满足。\n\n\n六. 信息抽取\n\n\n信息抽取（Information Extraction）是指从非结构化/半结构化文本（如网页、新闻、 论文文献、微博等）中提取指定类型的信息（如实体、属性、关系、事件、商品记录等）， 并通过信息归并、冗余消除和冲突消解等手段将非结构化文本转换为结构化信息的一项综合技术。例如:\n\n\n• 从相关新闻报道中抽取出恐怖事件信息：时间、地点、袭击者、受害人、袭击 目标、后果等；\n\n\n• 从体育新闻中抽取体育赛事信息：主队、客队、赛场、比分等；\n\n\n• 从论文和医疗文献中抽取疾病信息：病因、病原、症状、药物等\n\n\n被抽取出来的信息通常以结构化的形式描述，可以为计算机直接处理，从而实现对海量非结构化数据的分析、组织、管理、计算、 查询和推理，并进一步为更高层面的应用和任务（如自然语言理解、知识库构建、智能问答系统、舆情分析系统）提供支撑。\n\n\n目前信息抽取已被广泛应用于舆情监控、网络搜索、智能问答等多个重要领域。与此同时，信息抽取技术是中文信息处理和人工智能的核心技术，具有重要的科学意义。\n\n\n一直以来，人工智能的关键核心部件之一是构建可支撑类人推理和自然语言理解的大规模常识知识库。然而，由于人类知识的复杂性、开放性、多样性和巨大的规模，目前仍然无法构建满足上述需求的大规模知识库。\n\n\n信息抽取技术通过结构化自然语言表述的语义知识，并整合来自海量文本中的不同语义知识，是构建大规模知识库最有效的技术之一。\n\n\n每一段文本内所包含的寓意可以描述为其中的一组实体以及这些实体相互之间的关联和交互，因此抽取文本中的实体和它们之间的语义关系也就成为了理解文本意义的基础。\n\n\n信息抽取可以通过抽取实体和实体之间的语义关系，表示这些语义关系承载的信息，并基于这些信息进行计算和推理来有效的理解一段文本所承载的语义。\n\n\n1. 命名实体识别\n\n\n命名实体识别的目的是识别文本中指定类别的实体，主要包括人名、地名、机构名、专有名词等的任务。\n\n\n命名实体识别系统通常包含两个部分：实体边界识别和实体分类。\n\n\n其中实体边界识别判断一个字符串是否是一个实体，而实体分类将识别出的实体划分到预先给定的不同类别中去。\n\n\n命名实体识别是一项极具实用价值的技术，目前中英文上通用命名实体识别（人名、地名、机构名）的 F1 值都能达到 90% 以上。命名实体识别的主要难点在于表达不规律、且缺乏训练语料的开放域命名实体类别（如电影、歌曲名）等。\n\n\n2. 关系抽取\n\n\n关系抽取指的是检测和识别文本中实体之间的语义关系，并将表示同一语义关系的提及（mention）链接起来的任务。关系抽取的输出通常是一个三元组（实体 1，关系类别，实体 2），表示实体 1 和实体 2 之间存在特定类别的语义关系。\n\n\n例如，句子“北京是中国的首都、政治中心和文化中心”中表述的关系可以表示为（中国，首都，北京），（中国，政治中心，北京）和（中国，文化中心，北京）。语义关系类别可以预先给定（如 ACE 评测中的七大类关系），也可以按需自动发现（开放域信息抽取）。\n\n\n关系抽取通常包含两个核心模块：关系检测和关系分类。\n\n\n其中关系检测判断两个实体之间是否存在语义关系，而关系分类将存在语义关系的实体对划分到预先指定的类别中。\n\n\n在某些场景和任务下，关系抽取系统也可能包含关系发现模块，其主要目的是发现实体和实体之间存在的语义关系类别。例如，发现人物和公司之间存在雇员、CEO、CTO、创始人、董事长等关系类别。\n\n\n3. 事件抽取\n\n\n事件抽取指的是从非结构化文本中抽取事件信息，并将其以结构化形式呈现出来的任务。\n\n\n例如，从“毛泽东 1893 年出生于湖南湘潭”这句话中抽取事件{类型：出生， 人物：毛泽东，时间：1893 年，出生地：湖南湘潭}。\n\n\n事件抽取任务通常包含事件类型识别和事件元素填充两个子任务。\n\n\n事件类型识别判断一句话是否表达了特定类型的事件。事件类型决定了事件表示的模板，不同类型的事件具有不同的模板。\n\n\n例如出生事件的模板是{人物， 时间，出生地}，而恐怖袭击事件的模板是{地点，时间，袭击者，受害者，受伤人数,…}。 事件元素指组成事件的关键元素，事件元素识别指的是根据所属的事件模板，抽取相应的元素，并为其标上正确元素标签的任务。\n\n\n4. 信息集成\n\n\n实体、关系和事件分别表示了单篇文本中不同粒度的信息。在很多应用中，需要将来自不同数据源、不同文本的信息综合起来进行决策，这就需要研究信息集成技术。\n\n\n目前，信息抽取研究中的信息集成技术主要包括共指消解技术和实体链接技术。\n\n\n共指消解指的是检测同一实体/关系/事件的不同提及，并将其链接在一起的任务，例如，识别“乔布斯是苹果的创始人之一，他经历了苹果公司几十年的起落与兴衰”这句话中的“乔布斯”和“他”指的是同一实体。\n\n\n实体链接的目的是确定实体名所指向的真实世界实体。例如识别上一句话中的“苹果”和“乔布斯”分别指向真实世界中的苹果公司和其 CEO 史蒂夫·乔布斯。\n\n\n七. 问答系统\n\n\n自动问答（Question Answering, QA）是指利用计算机自动回答用户所提出的问题以满足用户知识需求的任务。不同于现有搜索引擎，问答系统是信息服务的一种高级形式，系统返回用户的不再是基于关键词匹配排序的文档列表，而是精准的自然语言答案。\n\n\n近年来，随着人工智能的飞速发展，自动问答已经成为倍受关注且发展前景广泛的研究方向。自动问答的研究历史可以溯源到人工智能的原点。\n\n\n1950 年，人工智能之父阿兰图灵（Alan M. Turing）在《Mind》上发表文章《Computing Machinery and Intelligence》，文章开篇提出通过让机器参与一个模仿游戏（Imitation Game）来验证“机器”能否“思考”，进而提出了经典的图灵测试（Turing Test），用以检验机器是否具备智能。\n\n\n同样，在自然语言处理研究领域，问答系统被认为是验证机器是否具备自然语言理解能力的四个任务之一（其它三个是机器翻译、复述和文本摘要）。\n\n\n自动问答研究既有利于推动人工智能相关学科的发展，也具有非常重要的学术意义。从应用上讲，现有基于关键词匹配和浅层语义分析的信息服务技术已经难以满足用户日益增长的精准化和智能化信息需求，已有的信息服务范式急需一场变革。\n\n\n2011 年，华盛顿大学图灵中心主任 Etzioni 在 Nature 上发表的《Search Needs a Shake-Up》中明确指出：在万维网诞生 20 周年之际，互联网搜索正处于从简单关键词搜索走向深度问答的深刻变革的风口浪尖上。以直接而准确的方式回答用户自然语言提问的自动问答系统将构成下一代搜索引擎的基本形态。\n\n\n同一年，以深度问答技术为核心的 IBM Watson 自动问答机器人在美国智力竞赛节目 Jeopardy 中战胜人类选手，引起了业内的巨大轰动。Watson 自动问答系统让人们看到已有信息服务模式被颠覆的可能性，成为了问答系统发展的一个里程碑。\n\n\n此外，随着移动互联网崛起与发展，以苹果公司 Siri、Google Now、微软 Cortana 等为代表的移动生活助手爆发式涌现，上述系统都把以自然语言为基本输入方式的问答系统看作是下一代信息服务的新形态和突破口，并均加大人员、资金的投入，试图在这一次人工智能浪潮中取得领先。\n\n\n1. 关键问题\n\n\n自动问答系统在回答用户问题时，需要正确理解用户所提的自然语言问题，抽取其中的关键语义信息，然后在已有语料库、知识库或问答库中通过检索、匹配、推理的手段获取答案并返回给用户。\n\n\n上述过程涉及词法分析、句法分析、语义分析、信息检索、逻辑推理、知识工程、语言生成等多项关键技术。传统自动问答多集中在限定领域，针对限定类型的问题进行回答。伴随着互联网和大数据的飞速发展，现有研究趋向于开放域、面向开放类型问题的自动问答。概括地讲，自动问答的主要研究任务和相应关键科学问题如下。\n\n\n1.1 问句理解\n\n\n给定用户问题，自动问答首先需要理解用户所提问题。用户问句的语义理解包含词法分析、句法分析、语义分析等多项关键技术，需要从文本的多个维度理解其中包含的语义内容。\n\n\n在词语层面，需要在开放域环境下，研究命名实体识别（Named Entity Recognition）、术语识别（Term Extraction）、词汇化答案类型词识别（Lexical Answer Type Recognition）、 实体消歧（Entity Disambiguation）、关键词权重计算（Keyword Weight Estimation）、答案集中词识别（Focused Word Detection）等关键问题。\n\n\n\n在句法层面，需要解析句子中词与词之间、短语与短语之间的句法关系，分析句子句法结构。在语义层面，需要根据词语层面、句法层面的分析结果，将自然语言问句解析成可计算、结构化的逻辑表达形式（如一阶谓词逻辑表达式）。\n\n\n1.2 文本信息抽取\n\n\n给定问句语义分析结果，自动问答系统需要在已有语料库、知识库或问答库中匹配相关的信息，并抽取出相应的答案。\n\n\n传统答案抽取构建在浅层语义分析基础之上，采用关键词匹配策略，往往只能处理限定类型的答案，系统的准确率和效率都难以满足实际应用需求。为保证信息匹配以及答案抽取的准确度，需要分析语义单元之间的语义关系，抽取文本中的结构化知识。\n\n\n早期基于规则模板的知识抽取方法难以突破领域和问题类型的限制，远远不能满足开放领域自动问答的知识需求。为了适应互联网实际应用的需求，越来越多的研究者和开发者开始关注开放域知识抽取技术，其特点在于：\n\n\n• 文本领域开放：处理的文本是不限定领域的网络文本\n\n\n• 内容单元类型开放：不限定所抽取的内容单元类型，而是自动地从网络中挖掘内容单元的类型，例如实体类型、事件类型和关系类型等。\n\n\n1.3 知识推理\n\n\n自动问答中，由于语料库、知识库和问答库本身的覆盖度有限，并不是所有问题都能直 接找到答案。这就需要在已有的知识体系中，通过知识推理的手段获取这些隐含的答案。\n\n\n例如，知识库中可能包括了一个人的“出生地”信息，但是没包括这个人的“国籍”信息，因此无法直接回答诸如“某某人是哪国人?”这样的问题。但是一般情况下，一个人的“出生地”所属的国家就是他（她）的“国籍”。\n\n\n在自动问答中，就需要通过推理的方式学习到这样的模式。传统推理方法采用基于符号的知识表示形式，通过人工构建的推理规则得到答案。\n\n\n但是面对大规模、开放域的问答场景，如何自动进行规则学习，如何解决规则冲突仍然是亟待解决的难点问题。目前，基于分布式表示的知识表示学习方法能够将实体、概念以及它们之间的语义关系表示为低维空间中的对象（向量、矩阵等），并通过低维空间中的数值计算完成知识推理任务。\n\n\n虽然这类推理的效果离实用还有距离，但是我们认为这是值得探寻的方法，特别是如何将已有的基于符号表示的逻辑推理与基于分布式表示的数值推理相结合，研究融合符号逻辑和表示学习的知识推理技术，是知识推理任务中的关键科学问题。\n\n\n2. 技术方法\n\n\n根据目标数据源的不同，已有自动问答技术大致可以分为三类：\n\n\n• 检索式问答；\n• 社区问答;\n• 知识库问答。\n\n\n以下分别就这几个方面对研究现状进行简要阐述。\n\n\n2.1 检索式问答\n\n\n检索式问答研究伴随搜索引擎的发展不断推进。1999 年，随着 TREC QA 任务的发起， 检索式问答系统迎来了真正的研究进展。TREC QA 的任务是给定特定 WEB 数据集，从中找到能够回答问题的答案。这类方法是以检索和答案抽取为基本过程的问答系统，具体过程包括问题分析、篇章检索和答案抽取。\n\n\n根据抽取方法的不同，已有检索式问答可以分为基于模式匹配的问答方法和基于统计文本信息抽取的问答方法。\n\n\n• 基于模式匹配的方法往往先离线地获得各类提问答案的模式。在运行阶段，系统首先判断当前提问属于哪一类，然后使用这类提问的模式来对抽取的候选答案进行验证。同时为了提高问答系统的性能，人们也引入自然语言处理技术。由于自然语言处理的技术还未成熟，现有大多数系统都基于浅层句子分析。\n\n\n• 基于统计文本信息抽取的问答系统的典型代表是美国 Language Computer Corporation 公司的 LCC 系统。该系统使用词汇链和逻辑形式转换技术，把提问句和答案句转化成统一的逻辑形式（Logic Form），通过词汇链，实现答案的推理验证。\n\n\nLCC 系统在 TREC QA Track 2001 ~ 2004 连续三年的评测中以较大领先优势获得第一名的成绩。 2011 年，IBM 研发的问答机器人 Watson 在美国智力竞赛节目《危险边缘 Jeopardy!》中战胜人类选手，成为问答系统发展的一个里程碑。\n\n\nWatson 的技术优势大致可以分为以下三个方面：\n\n\n• 强大的硬件平台：包括 90 台 IBM 服务器，分布式计算环境；\n\n\n• 强大的知识资源：存储了大约 2 亿页的图书、新闻、电影剧本、辞海、文选和《世界图书百科全书》等资料；\n\n\n• 深层问答技术（DeepQA）：涉及统计机器学习、句法分析、主题分析、信息抽取、 知识库集成和知识推理等深层技术。\n\n\n然而，Watson 并没有突破传统问答式检索系统的局限性，使用的技术主要还是检索和匹配，回答的问题类型大多是简单的实体或词语类问题，而推理能力不强。\n\n\n2.2 社区问答\n\n\n随着 Web2.0 的兴起，基于用户生成内容（User-Generated Content, UGC）的互联网服务越来越流行，社区问答系统应运而生，例如 Yahoo! Answers、百度知道等。\n\n\n问答社区的出现为问答技术的发展带来了新的机遇。据统计 2010 年 Yahoo! Answers 上已解决的问题量达到 10 亿，2011 年“百度知道”已解决的问题量达到 3 亿，这些社区问答数据覆盖了方方面面的用户知识和信息需求。\n\n\n此外，社区问答与传统自动问答的另一个显著区别是：社区问答系统有大量的用户参与，存在丰富的用户行为信息，例如用户投票信息、用户评价信息、回答者的问题采纳率、用户推荐次数、页面点击次数以及用户、问题、答案之间的相互关联信息等等，这些用户行为信息对于社区中问题和答案的文本内容分析具有重要的价值。\n\n\n一般来讲，社区问答的核心问题是从大规模历史问答对数据中找出与用户提问问题语义相似的历史问题并将其答案返回提问用户。\n\n\n假设用户查询问题为 q0,用于检索的问答对数据为 SQ,A = {(q1 , a1 ), (q2 , a2 )}, … , (qn, an)}}，相似问答对检索的目标是从 SQ,A 中检索出能够解答问题 q0 的问答对 (qi , ai)。 针对这一问题，传统的信息检索模型，如向量空间模型、语言模型等，都可以得到应用。\n\n\n但是，相对于传统的文档检索，社区问答的特点在于：用户问题和已有问句相对来说都非常短，用户问题和已有问句之间存在“词汇鸿沟”问题，基于关键词匹配的检索模型很难达到较好的问答准确度。\n\n\n目前，很多研究工作在已有检索框架中针对这一问题引入单语言翻译概率模型，通过 IBM 翻译模型，从海量单语问答语料中获得同种语言中两个不同词语之间的语义转换概率，从而在一定程度上解决词汇语义鸿沟问题。\n\n\n例如和“减肥”对应的概率高的相关词有“瘦身”、“跑步”、“饮食”、“健康”、“远动”等等。 除此之外，也有许多关于问句检索中词重要性的研究和基于句法结构的问题匹配研究。\n\n\n2.3 知识库问答\n\n\n检索式问答和社区问答尽管在某些特定领域或者商业领域有所应用，但是其核心还是关键词匹配和浅层语义分析技术，难以实现知识的深层逻辑推理，无法达到人工智能的高级目标。\n\n\n因此，近些年来，无论是学术界或工业界，研究者们逐步把注意力投向知识图谱或知识库（Knowledge Graph）。其目标是把互联网文本内容组织成为以实体为基本语义单元（节点）的图结构，其中图上的边表示实体之间语义关系。\n\n\n目前互联网中已有的大规模知识库包括 DBpedia、Freebase、YAGO 等。这些知识库多是以“实体-关系-实体”三元组为基本单元所组成的图结构。\n\n\n基于这样的结构化知识，问答系统的任务就是要根据用户问题的语义直接在知识库上查找、推理出相匹配的答案，这一任务称为面向知识库的问答系统或知识库问答。要完成在结构化数据上的查询、匹配、推理等操作，最有效的方式是利用结构化的查询语句，例如：SQL、SPARQL 等。\n\n\n然而，这些语句通常是由专家编写，普通用户很难掌握并正确运用。对普通用户来说，自然语言仍然是最自然的交互方式。因此，如何把用户的自然语言问句转化为结构化的查询语句是知识库问答的核心所在，其关键是对于自然语言问句进行语义理解。\n\n\n目前，主流方法是通过语义分析，将用户的自然语言问句转化成结构化的语义表示，如范式和 DCS-Tree。相对应的语义解析语法或方法包括组合范畴语法（ Category Compositional Grammar, CCG ）以 及 依 存 组 合 语 法（ Dependency-based Compositional Semantics, DCS）等。\n八. 机器翻译\n\n\n1. 理论应用\n\n\n机器翻译（machine translation，MT）是指利用计算机实现从一种自然语言到另外一种自然语言的自动翻译。被翻译的语言称为源语言（source language），翻译到的语言称作目标语言（target language）。\n\n\n\n简单地讲，机器翻译研究的目标就是建立有效的自动翻译方法、模型和系统，打破语言壁垒，最终实现任意时间、任意地点和任意语言的自动翻译，完成人们无障碍自由交流的梦想。\n\n\n人们通常习惯于感知（听、看和读）自己母语的声音和文字，很多人甚至只能感知自己的母语，因此，机器翻译在现实生活和工作中具有重要的社会需求。\n\n\n从理论上讲，机器翻译涉及语言学、计算语言学、人工智能、机器学习，甚至认知语言学等多个学科，是一个典型的多学科交叉研究课题，因此开展这项研究具有非常重要的理论意义，既有利于推动相关学科的发展，揭示人脑实现跨语言理解的奥秘，又有助于促进其他自然语言处理任务，包括中文信息处理技术的快速发展。\n\n\n从应用上讲，无论是社会大众、政府企业还是国家机构，都迫切需要机器翻译技术。特别是在“互联网+”时代，以多语言多领域呈现的大数据已成为我们面临的常态问题，机器翻译成为众多应用领域革新的关键技术之一。\n\n\n例如，在商贸、体育、文化、旅游和教育等各个领域，人们接触到越来越多的外文资料，越来越频繁地与持各种语言的人通信和交流，从而对机器翻译的需求越来越强烈；在国家信息安全和军事情报领域，机器翻译技术也扮演着非常重要的角色。\n\n\n可以说离开机器翻译，基于大数据的多语言信息获取、挖掘、分析和决策等其他应用都将成为空中楼阁。\n\n\n尤其值得提出的是，在未来很长一段时间里，建立于丝绸之路这一历史资源之上的“一带一路”将是我国与周边国家发展政治、经济，进行文化交流的主要战略。据统计，“一带一路”涉及 60 多个国家、44 亿人口、53 种语言，可见机器翻译是“一带一路”战略实施中不可或缺的重要技术。\n\n\n2. 技术现状\n\n\n基于规则的机器翻译方法需要人工设计和编纂翻译规则，统计机器翻译方法能够自动获取翻译规则，但需要人工定义规则的形式，而端到端的神经网络机器翻译方法可以直接通过编码网络和解码网络自动学习语言之间的转换算法。\n\n\n从某种角度讲，其自动化程度和智能化程度在不断提升，机器翻译质量也得到了显著改善。机器翻译技术的研究现状可从欧盟组织的国际机器翻译评测（WMT）的结果中窥得一斑。\n\n\n该评测主要针对欧洲语言之间的互译，2006 年至 2016 年每年举办一次。对比法语到英语历年的机器翻译评测结果可以发现，译文质量已经在自动评价指标 BLEU 值上从最初小于 0.3 到目前接近 0.4（大量的人工评测对比说明，BLEU 值接近 0.4 的译文能够达到人类基本可以理解的程度）。\n\n\n另外，中国中文信息学会组织的全国机器翻译评测（CWMT）每两年组织一次， 除了英汉、日汉翻译评测以外，CWMT 还关注我国少数民族语言（藏、蒙、维）和汉语之间的翻译。\n\n\n相对而言，由于数据规模和语言复杂性的问题，少数民族与汉语之间的翻译性能要低于汉英、汉日之间的翻译性能。虽然机器翻译系统评测的分值呈逐年增长的趋势，译文质量越来越好，但与专业译员的翻译结果相比，机器翻译还有很长的路要走，可以说，在奔向“信、达、雅”翻译目标的征程上，目前的机器翻译基本挣扎在“信”的阶段，很多理论和技术问题仍有待于更深入的研究和探索。\n\n\n九. 自动摘要\n\n\n自动文摘（又称自动文档摘要）是指通过自动分析给定的一篇文档或多篇文档，提炼、总结其中的要点信息，最终输出一篇长度较短、可读性良好的摘要（通常包含几句话或数百字），该摘要中的句子可直接出自原文，也可重新撰写所得。\n\n\n简言之，文摘的目的是通过对原文本进行压缩、提炼，为用户提供简明扼要的文字描述。用户可以通过阅读简短的摘要而知晓原文中所表达的主要内容，从而大幅节省阅读时间。\n\n\n自动文摘研究的目标是建立有效的自动文摘方法与模型，实现高性能的自动文摘系统。近二十年来，业界提出了各类自动文摘方法与模型，用于解决各类自动摘要问题，在部分自动摘要问题的研究上取得了明显的进展，并成功将自动文摘技术应用于搜索引擎、新闻阅读 等产品与服务中。\n\n\n例如谷歌、百度等搜索引擎均会为每项检索结果提供一个短摘要，方便用 户判断检索结果相关性。在新闻阅读软件中，为新闻事件提供摘要也能够方便用户快速了解 该事件。2013 年雅虎耗资 3000 万美元收购了一项自动新闻摘要应用 Summly，则标志着自动文摘技术的应用走向成熟。\n\n\n自动文摘的研究在图书馆领域和自然语言处理领域一直都很活跃，最早的应用需求来自于图书馆。图书馆需要为大量文献书籍生成摘要，而人工摘要的效率很低，因此亟需自动摘要方法取代人工高效地完成文献摘要任务。\n\n\n随着信息检索技术的发展，自动文摘在信息检索系统中的重要性越来越大，逐渐成为研究热点之一。经过数十年的发展，同时在 DUC 与 TAC 等自动文摘国际评测的推动下，文本摘要技术已经取得长足的进步。国际上自动文摘方面比较著名的几个系统包括 ISI 的 NeATS 系统，哥伦比亚大学的 NewsBlaster 系统，密歇根大学的 NewsInEssence 系统等。\n1. 方法\n\n\n自动文摘所采用的方法从实现上考虑可以分为抽取式摘要（extractive summarization） 和生成式摘要（abstractive summarization）。\n\n\n抽取式方法相对比较简单，通常利用不同方法对文档结构单元（句子、段落等）进行评价，对每个结构单元赋予一定权重，然后选择最重要的结构单元组成摘要。而生成式方法通常需要利用自然语言理解技术对文本进行语法、 语义分析，对信息进行融合，利用自然语言生成技术生成新的摘要句子。\n\n\n目前的自动文摘方法主要基于句子抽取，也就是以原文中的句子作为单位进行评估与选取。抽取式方法的好处是易于实现，能保证摘要中的每个句子具有良好的可读性。\n\n\n为解决如前所述的要点筛选和文摘合成这两个关键科学问题，目前主流自动文摘研究工作大致遵循如下技术框架： 内容表示 → 权重计算 → 内容选择 → 内容组织。\n\n\n首先将原始文本表示为便于后续处理的表达方式，然后由模型对不同的句法或语义单元 进行重要性计算，再根据重要性权重选取一部分单元，经过内容上的组织形成最后的摘要。\n\n\n1.1 内容表示与权重计算\n\n\n原文档中的每个句子由多个词汇或单元构成，后续处理过程中也以词汇等元素为基本单位，对所在句子给出综合评价分数。\n\n\n以基于句子选取的抽取式方法为例，句子的重要性得分由其组成部分的重要性衡量。由于词汇在文档中的出现频次可以在一定程度上反映其重要性， 我们可以使用每个句子中出现某词的概率作为该词的得分，通过将所有包含词的概率求和得到句子得分。\n\n\n也有一些工作考虑更多细节，利用扩展性较强的贝叶斯话题模型，对词汇本身的话题相关性概率进行建模。一些方法将每个句子表示为向量，维数为总词表大小。通常使用加权频数作为句子向量相应维上的取值。加权频数的定义可以有多种，如信息检索中常用的词频-逆文档频率 （TF-IDF）权重。\n\n\n也有研究工作考虑利用隐语义分析或其他矩阵分解技术，得到低维隐含语义表示并加以利用。得到向量表示后计算两两之间的某种相似度（例如余弦相似度）。随后根据计算出的相似度构建带权图，图中每个节点对应每个句子。\n\n\n在多文档摘要任务中，重要的句子可能和更多其他句子较为相似，所以可以用相似度作为节点之间的边权，通过迭代求解基于图的排序算法来得到句子的重要性得分。\n\n\n也有很多工作尝试捕捉每个句子中所描述的概念，例如句子中所包含的命名实体或动词。\n\n\n出于简化考虑，现有工作中更多将二元词（bigram）作为概念。近期则有工作提出利用频繁图挖掘算法从文档集中挖掘得到深层依存子结构作为语义表示单元。\n\n\n另一方面，很多摘要任务已经具备一定数量的公开数据集，可用于训练有监督打分模型。\n\n\n例如对于抽取式摘要，我们可以将人工撰写的摘要贪心匹配原文档中的句子或概念，从而得到不同单元是否应当被选作摘要句的数据。然后对各单元人工抽取若干特征，利用回归模型或排序学习模型进行有监督学习，得到句子或概念对应的得分。\n\n\n文档内容描述具有结构性，因此也有利用隐马尔科夫模型（HMM）、条件随机场（CRF）、结构化支持向量机（Structural SVM）等常见序列标注或一般结构预测模型进行抽取式摘要有监督训练的工作。\n\n\n所提取的特征包括所在位置、包含词汇、与邻句的相似度等等。对特定摘要任务一般也会引入与具体设定相关的特征，例如查询相关摘要任务中需要考虑与查询的匹配或相似程度。\n\n\n1.2 内容选择\n\n\n无论从效果评价还是从实用性的角度考虑，最终生成的摘要一般在长度上会有限制。在获取到句子或其他单元的重要性得分以后，需要考虑如何在尽可能短的长度里容纳尽可能多的重要信息，在此基础上对原文内容进行选取。内容选择方法包括贪心选择和全局优化。\n\n\n2. 技术现状\n\n\n相比机器翻译、自动问答、知识图谱、情感分析等热门领域，自动文摘在国内并没有受到足够的重视。\n\n\n国内早期的基础资源与评测举办过中文单文档摘要的评测任务，但测试集规模比较小，而且没有提供自动化评价工具。2015 年 CCF 中文信息技术专委会组织了 NLPCC 评测，其中包括了面向中文微博的新闻摘要任务，提供了规模相对较大的样例数据和测试数据，并采用自动评价方法，吸引了多支队伍参加评测，目前这些数据可以公开获得。\n\n\n但上述中文摘要评测任务均针对单文档摘要任务，目前还没有业界认可的中文多文档摘要数据，这在事实上阻碍了中文自动摘要技术的发展。\n\n\n近些年，市面上出现了一些文本挖掘产品，能够提供中文文档摘要功能（尤其是单文档 摘要），例如方正智思、拓尔思（TRS），海量科技等公司的产品。百度等搜索引擎也能为检索到的文档提供简单的单文档摘要。这些文档摘要功能均被看作是系统的附属功能，其实现方法均比较简单。","data":"2017年11月30日 09:44:32"}
{"_id":{"$oid":"5d34531962f717dc0659b7c5"},"title":"“拨开迷雾看人工智能”-3分钟看懂自然语言处理","author":"weixin_34363171","content":"上一期，我们为你介绍了语音识别是人机交互的入口，这一期我们介绍什么是自然语言处理，以及自然语言处理的难点。\n《圣经》里有一个故事讲巴比伦人想建造一座塔直通天堂。建塔的人都说着同一种语言，心意相通、齐心协力。上帝看到人类竟然敢做这种事情，就让他们的语言变得不一样。因为人们听不懂对方在讲什么，于是大家整天吵吵闹闹，无法继续建塔。后来人们把这座塔叫作“巴别塔”，而“巴别”的意思就是“分歧”。\n虽然巴别塔没有建成，但让全世界拥有相通的语言一直是萦绕在人们心中的梦想。但人工智能技术实现了用机器翻译不同的语言，从最初只能翻译单词到现在可以整句或通篇翻译，近几年用语音都可以直接进行翻译。有了它你可以行走到世界上任何一个国家，即使看不懂文字，听不懂语言，也能够借助机器翻译与他人进行交流和沟通，不必再为相互不能理解而困扰。\n然而，机器翻译的核心，就是自然语言处理（Natural Language Processing），简称：NLP。\n什么是自然语言处理？\n简单地说，自然语言处理就是用人工智能来处理、理解以及运用人类语言。它体现了真正意义上的“人工智能”，百度机器学习专家余凯说过“听与看，说白了就是阿猫和阿狗都会的，而只有语言才是人类独有的”，也就是说只有当计算机具备了处理自然语言的能力时，才算实现了真正的智能。\n自然语言处理技术在生活中应用广泛，例如机器翻译、手写体和印刷体字符识别、语音识别后实现文字转换、信息检索、抽取与过滤、文本分类与聚类、舆情分析和观点挖掘等。它们分别应用了自然语言处理当中的语法分析、语义分析、篇章理解等技术，是人工智能界最前沿的研究领域。时至今日AI在这些技术领域的发展已经把识别准确率从70%提高到了90%以上，但只有当准确率提高到99%及以上时，才能被认定为自然语言处理的技术达到人类水平，这仍然是巨大的困难和挑战。\n自然语言处理存在哪些主要困难？\n自然语言处理的困难关键在于消除歧义问题，如词法分析、句法分析、语义分析等过程中存在的歧义问题，简称为消歧。而正确的消歧需要大量的知识，包括语言学知识（如词法、句法、语义、上下文等）和世界知识（与语言无关）。由于歧义的存在给自然语言处理带来两个主要困难。\n首先，当语言中充满了大量的歧义，分词难度很大，同一种语言形式可能具有多种含义。特别是在处理中文单词的过程中，由于中文词与词之间缺少天然的分隔符，因此文字处理比英文等西方语言多一步确定词边界的工序，即“中文自动分词”任务。通俗地说就是要由计算机在词与词之间自动加上分隔符，从而将中文文本切分为独立的单词。例如 “昨天有沙尘暴”这句话带有分隔符的切分文本是“昨天|有|沙尘暴”。自动分词处于中文自然语言处理的底层，意味着它是理解语言的第一道工序，但正确的单词切分又需要取决于对文本语义的正确理解。这形成了一个“鸡生蛋、蛋生鸡”的问题，成为自然语言处理的第一条拦路虎。\n除了在单个词级别分词和理解存在难度外，在短语和句子级别也容易存在歧义。例如 “出口冰箱”可以理解为动宾关系（从国内出口了一批冰箱），也可以理解为偏正关系（从国内出口的冰箱）；又如在句子级别，“做化疗的是她的妈妈”可以理解为她妈妈生病了需要做化疗，也可以理解为她妈妈是医生，帮别人做化疗。\n其次，消除歧义所需要的知识在获取、表达以及运用上存在困难。由于语言处理的复杂性，合适的语言处理方法和模型难以设计。\n在试图理解一句话的时候，即使不存在歧义问题，我们也往往需要考虑上下文的影响。所谓的“上下文”指的是当前所说这句话所处的语言环境，包括说话人所处的环境，或者是这句话的前几句话或者后几句话等。以“小A打了小B，因此我惩罚了他”为例。在其中的第二句话中的“他”是指代“小A”还是“小B”呢？要正确理解这句话，我们就要理解上句话“小A打了小B”意味着“小A”做得不对，因此第二句中的“他”应当指代的是“小A”。由于上下文对于当前句子的暗示形式是多种多样的，因此如何考虑上下文影响问题是自然语言处理中的主要困难之一。\n此外，正确理解人类语言还要有足够的背景知识，特别是对于成语和歇后语的理解。比如在英语中“The spirit is willing but the flesh is weak.”是一句成语，意思是“心有余而力不足”。但是曾经某个机器翻译系统将这句英文翻译到俄语，然后再翻译回英语的时候，却变成了“The Voltka is strong but the meat is rotten.”，意思是“伏特加酒是浓的，但肉却腐烂了”。导致翻译偏差的根本问题，在于机器翻译系统对于英语成语并无了解，仅仅是从字面上进行翻译，结果失之毫厘，谬之千里。\n小结：\n自然语言处理就是用人工智能来处理、理解以及运用人类语言。它在生活中具有广泛的应用，今天在一些领域（比如机器翻译）其处理准确率已经超过90%，但要达到人类水平，仍然存在较大难度。\n消除歧义是目前自然语言处理的最大困难，它的根源是人类语言的复杂性和语言描述的外部世界的复杂性。人类语言承担着人类表达情感、交流思想、传播知识等重要功能，因此需要具备强大的灵活性和表达能力，而理解语言所需要的知识又是无止境的。那么目前人们是如何尝试进行自然语言处理的呢？\n预告：\n下一篇，我们将结合2017年自然语言处理的最新发展趋势来介绍对抗神经网络——Gans。","data":"2017年03月07日 14:21:00"}
{"_id":{"$oid":"5d34536262f717dc0659b7d2"},"title":"自然语言处理(NLP)基础理解","author":"自信哥","content":"人工智能的目标\n- 推理 - 自动学习\u0026调度 - 机器学习 - 自然语言处理 - 计算机视觉 - 机器人 - 通用智能\n人工智能三大阶段\n阶段 1——机器学习：智能系统使用一系列算法从经验中进行学习。 阶段 2——机器智能：机器使用的一系列从经验中进行学习的高级算法，例如深度神经网络。人工智能目前处于此阶段。 阶段 3——机器意识：不需要外部数据就能从经验中自学习。\nimage.png\n人工智能的类型\nANI（狭义人工智能）：它包含基础的、角色型任务，比如由 Siri、Alexa 这样的聊天机器人、个人助手完成的任务。 AGI（通用人工智能）：通用人工智能包含人类水平的任务，它涉及到机器的持续学习。 ASI（强人工智能）：强人工智能指代比人类更聪明的机器。\n什么使得系统智能化？\nimage.png\n自然语言处理 | 知识表示 | 自动推理 | 机器学习\n什么是自然语言处理？\n自然语言处理（NLP）是指机器理解并解释人类paralyzes写作、说话方式的能力。\nNLP 的目标是让计算机／机器在理解语言上像人类一样智能。最终目标是弥补人类交流（自然语言）和计算机理解（机器语言）之间的差距。\nimage.png\n下面是三个不同等级的语言学分析：\n句法学：给定文本的哪部分是语法正确的。\n语义学：给定文本的含义是什么？\n语用学：文本的目的是什么？\nNLP 处理语言的不同方面，例如：\n音韵学：指代语言中发音的系统化组织。\n词态学：研究单词构成以及相互之间的关系。\nNLP 中理解语义分析的方法：\n分布式：它利用机器学习和深度学习的大规模统计策略。 框架式：句法不同，但语义相同的句子在数据结构（帧）中被表示为程式化情景。 理论式：这种方法基于的思路是，句子指代的真正的词结合句子的部分内容可表达全部含义。 交互式（学习）：它涉及到语用方法，在交互式学习环境中用户教计算机一步一步学习语言。\n为什么需要 NLP ?\n有了 NLP，有可能完成自动语音、自动文本编写这样的任务。\n由于大型数据（文本）的存在，我们为什么不使用计算机的能力，不知疲倦地运行算法来完成这样的任务，花费的时间也更少。\n这些任务包括 NLP 的其他应用，比如自动摘要（生成给定文本的总结）和机器翻译。\nNLP流程\n如果要用语音产生文本，需要完成ASR任务。\nNLP 的机制涉及两个流程：\n自然语言理解\n自然语言生成\n自然语言理解（NLU)\nNLU 是要理解给定文本的含义。本内每个单词的特性与结构需要被理解。在理解结构上，NLU 要理解自然语言中的以下几个歧义性：\n词法歧义性：单词有多重含义 句法歧义性：语句有多重解析树 语义歧义性：句子有多重含义 回指歧义性（Anaphoric Ambiguity）：之前提到的短语或单词在后面句子中有不同的含义。\n接下来，通过使用词汇和语法规则，理解每个单词的含义。\n然而，有些词有类似的含义（同义词），有些词有多重含义（多义词）。\n自然语言生成(NLG)\nNLG 是从结构化数据中以可读地方式自动生成文本的过程。难以处理是自然语言生成的主要问题。\n自然语言生成可被分为三个阶段：\n文本规划：完成结构化数据中基础内容的规划。\n语句规划：从结构化数据中组合语句，来表达信息流。\n实现：产生语法通顺的语句来表达文本。\nNLP 与文本挖掘（或文本分析）之间的不同\n自然语言处理是理解给定文本的含义与结构的流程。\n文本挖掘或文本分析是通过模式识别提起文本数据中隐藏的信息的流程。\n自然语言处理被用来理解给定文本数据的含义（语义），而文本挖掘被用来理解给定文本数据的结构（句法）。\nimage.png\n例如，在 \"I found my wallet near the bank \"一句中，NLP 的任务是理解句尾「bank」一词指代的是银行还是河边。\n大数据中的 NLP：The next Big Thing\n如今所有数据中的 80% 都可被用到，大数据来自于大公司、企业所存储的信息。例如，职员信息、公司采购、销售记录、经济业务以及公司、社交媒体的历史记录等。\n尽管人类使用的语言对计算机而言是模糊的、非结构化的，但有了 NLP 的帮助，我们可以解析这些大型的非结构化数据中的模式，从而更好地理解里面包含的信息。\nNLP 可使用大数据解决商业中的难题，比如零售、医疗、金融领域中的业务。\n聊天机器人\n聊天机器人或自动智能代理指代你能通过聊天 app、聊天窗口或语音唤醒 app 进行交流的计算机程序。\n也有被用来解决客户问题的智能数字化助手，成本低、高效且持续工作。\n聊天机器人的重要性\n聊天机器人对理解数字化客服和频繁咨询的常规问答领域中的变化至关重要。 聊天机器人在一些领域中的特定场景中非常有帮助，特别是会被频繁问到高度可预测的的问题时。\n聊天机器人的工作机制\nimage.png\n基于知识：包含信息库，根据客户的问题回应信息。 数据存储：包含与用户交流的历史信息。 NLP 层：它将用户的问题（任何形式）转译为信息，从而作为合适的回应。 应用层：指用来与用户交互的应用接口。\n聊天机器人每次与用户交流时都能进行学习，使用机器学习回应信息库中的信息。\nNLP 中为什么需要深度学习\n它使用基于规则的方法将单词表示为「one-hot」编码向量。 传统的方法注重句法表征，而非语义表征。 词袋：分类模型不能够分别特定语境。\nimage.png\n深度学习的三项能力\n可表达性：这一能力描述了机器如何能近似通用函数。 可训练性：深度学习系统学习问题的速度与能力。 可泛化性：在未训练过的数据上，机器做预测的能力。\n在深度学习中，当然也要考虑其他的能力，比如可解释性、模块性、可迁移性、延迟、对抗稳定性、安全等。但以上是主要的几项能力。\nNLP 中深度学习的常见任务\nimage.png\n传统 NLP 和深度学习 NLP 的区别\nimage.png\n日志分析与日志挖掘中的 NLP\n什么是日志？\n不同网络设备或硬件的时序信息集合表示日志。日志可直接存储在硬盘文档中，也可作为信息流传送到日志收集器。\n日志提供维持、追踪硬件表现、参数调整、紧急事件、系统修复、应用和架构优化的过程。\n什么是日志分析？\n日志分析是从日志中提取信息的过程，分析信息中的句法和语义，解析应用环境，从而比较分析不同源的日志文档，进行异常检测、发现关联性。\n什么是日志挖掘？\n日志挖掘或日志知识发现是提取日志中模式和关联性的过程，从而挖掘知识，预测日志中的异常检测。\n日志分析和日志挖掘中使用到的技术，下面介绍了完成日志分析的不同技术：\n模式识别：将日志信息与模式薄中的信息进行对比，从而过滤信息的技术。 标准化：日志信息的标准化是将不同的信息转换为同样的格式。当来自不同源的日志信息有不同的术语，但含义相同时，需要进行标准化。 分类 \u0026 标签：不同日志信息的分类 \u0026 标签涉及到对信息的排序，并用不同的关键词进行标注。 Artificial Ignorance：使用机器学习算法抛弃无用日志信息的技术。它也可被用来检测系统异常。\n日志分析 \u0026 日志挖掘中的 NLP\n自然语言处理技术被普遍用于日志分析和日志挖掘。\n词语切分、词干提取（stemming)、词形还原（lemmatization）、解析等不同技术被用来将日志信息转换成结构化的形式。\n一旦日志以很好的形式组织起来，日志分析和日志挖掘就能提取信息中有用的信息和知识。\n深度自然语言处理\n自然语言处理是一个复杂的领域，处于人工智能、计算语言学和计算机科学的交叉领域。\n从 NLP 开始\n用户需要输入一个包含已写文本的文件；接着应该执行以下 NLP 步骤：\nimage.png\nimage.png\n语句分割 - 在给定文本中辨识语句边界，即一个语句的结束和另一个语句的开始。语句通常以标点符号「.」结束。 标记化 - 辨识不同的词、数字及其他标点符号。 词干提取 - 将一个词还原为词干。 词性标注 - 标出语句中每一个词的词性，比如名词或副词。 语法分析 - 将给定文本的部分按类划分。 命名实体识别 - 找出给定文本中的人物、地点、时间等。 指代消解 - 根据一个语句的前句和后句界定该句中给定词之间的关系。\nNLP 的其他关键应用领域\n除了在大数据、日志挖掘及分析中的应用，NLP 还有一些其他主要应用领域。\n尽管 NLP 不如大数据、机器学习听起来那么火，但我们每天都在使用它：\n自动摘要 - 在给定输入文本的情况下，摈弃次要信息完成文本摘要。 情感分析 - 在给定文本中预测其主题，比如，文本中是否包含判断、观点或评论等。 文本分类 - 按照其领域分类不同的期刊、新闻报道。多文档分类也是可能的。文本分类的一个流行示例是垃圾电子邮件检测。基于写作风格，可检测作者姓名。 信息提取 - 建议电子邮件程序自动添加事件到日历。\nimage.png\n参考： https://www.jiqizhixin.com/articles/2017-05-07-3 https://www.xenonstack.com/blog/overview-of-artificial-intelligence-and-role-of-natural-language-processing-in-big-data\n\n\n作者：郭少悲\n链接：https://www.jianshu.com/p/b627cb31aab7\n來源：简书\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","data":"2018年01月03日 17:39:23"}
{"_id":{"$oid":"5d3453ac62f717dc0659b7de"},"title":"ACL 2017自然语言处理精选论文","author":"csdn_csdn__AI","content":"作者简介：洪亮劼，Etsy数据科学主管，前雅虎研究院高级经理。长期从事推荐系统、机器学习和人工智能的研究工作，在国际顶级会议上发表论文20余篇，长期担任多个国际著名会议及期刊的评审委员会成员和审稿人。\n责编：何永灿（heyc@csdn.net）\n本文为《程序员》原创文章，未经允许不得转载，更多精彩文章请订阅《程序员》\n涉及自然语言处理、人工智能、机器学习等诸多理论以及技术的顶级会议——第55届计算语言学年会（The 55th Annual Meeting of the Association for Computational Linguistics，简称ACL会议）于今年7月31日-8月4日在加拿大温哥华举行。从近期谷歌学术（Google Scholar）公布的学术杂志和会议排名来看，ACL依然是最重要的自然语言处理相关的人工智能会议。因为这个会议的涵盖面非常广泛，且理论文章较多，一般的读者很难从浩如烟海的文献中即刻抓取到有用信息，这里笔者从众多文章中精选出5篇有代表性的文章，为读者提供思路。\nMultimodal Word Distributions\n摘要：本文的核心思想为如何用Gaussian Mixture Model来对Word Embedding进行建模，从而可以学习文字的多重表达。这篇文章值得对Text Mining有兴趣的读者泛读。\n文章作者Ben Athiwaratkun是康奈尔大学统计科学系的博士生。Andrew Gordon Wilson是新加入康奈尔大学Operation Research以及Information Engineering的助理教授，之前在卡内基梅隆大学担任研究员，师从Eric Xing和Alex Smola教授，在之前，其在University of Cambridge的Zoubin Ghahramani手下攻读博士学位。\n这篇文章主要研究Word Embedding，其核心思想是想用Gaussian Mixture Model表示每一个Word的Embedding。最早的自然语言处理（NLP）是采用了One-Hot-Encoding的Bag of Word的形式来处理每个字。这样的形式自然是无法抓住文字之间的语义和更多有价值的信息。那么，之前Word2Vec的想法则是学习一个每个Word的Embedding，也就是一个实数的向量，用于表示这个Word的语义。当然，如何构造这么一个向量又如何学习这个向量成为了诸多研究的核心课题。\n在ICLR 2015会议上，来自UMass的Luke Vilnis和Andrew McCallum在“Word Representations via Gaussian Embedding”文章中提出了用分布的思想来看待这个实数向量的思想。具体说来，就是认为这个向量是某个高斯分布的期望，然后通过学习高斯分布的参数（也就是期望和方差）来最终学习到Word的Embedding Distribution。这一步可以说是扩展了Word Embedding这一思想。然而，用一个分布来表达每一个字的最直接的缺陷则是无法表达很多字的多重意思，这也就带来了这篇文章的想法。文章希望通过Gaussian Mixture Model的形式来学习每个Word的Embedding。也就是说，每个字的Embedding不是一个高斯分布的期望了，而是多个高斯分布的综合。这样，就给了很多Word多重意义的自由度。在有了这么一个模型的基础上，文章采用了类似Skip-Gram的来学习模型的参数。具体说来，文章沿用了Luke和Andrew的那篇文章所定义的一个叫Max-margin Ranking Objective的目标函数，并且采用了Expected Likelihood Kernel来作为衡量两个分布之间相似度的工具。这里就不详细展开了，有兴趣的读者可以精读这部分细节。\n\n\n图1 Skip-Gram\n文章通过UKWAC和Wackypedia数据集学习了所有的Word Embedding。所有试验中，文章采用了K=2的Gaussian Mixture Model（文章也有K=3的结果）。比较当然有之前Luke的工作以及其他各种Embedding的方法，比较的内容有Word Similarity以及对于Polysemous的字的比较。总之，文章提出的方法非常有效果。这篇文章因为也有源代码（基于Tensorflow），推荐有兴趣的读者精读。\nTopically Driven Neural Language Model\n摘要：文章的核心思想，也是之前有不少人尝试的，就是把话题模型（Topic Model）和语言模型（Language Model）相结合起来。这里，两种模型的处理都非常纯粹，是用“地道”的深度学习语言构架完成。用到了不少流行的概念（比如GRU、Attention等），适合文字挖掘的研究人员泛读。\n文章的作者是来自于澳大利亚的研究人员。第一作者Jey Han Lau目前在澳大利亚的IBM进行Topic Model以及NLP方面的研究，之前也在第二作者Timothy Baldwin的实验室做过研究。第二作者Timothy Baldwin和第三作者Trevor Cohn都在墨尔本大学长期从事NLP研究的教授。\n这篇文章的核心思想是想彻底用Neural的思想来结合Topic Model和Language Model。当然，既然这两种模型都是文字处理方面的核心模型，自然之前就有人曾经想过要这么做。不过之前的不少尝试都是要么还想保留LDA的一些部件或者往传统的LDA模型上去靠，要么是并没有和Language Model结合起来。\n文章的主要卖点是完全用深度学习的“语言”来构建整个模型，并且模型中的Topic Model模型部分的结果会成为驱动Language Model部分的成分。概括说来，文章提出了一个有两个组成部分的模型的集合（文章管这个模型叫tdlm）。\n\n\n图2 tdlm模型\n第一个部分是Topic Model的部分。我们已经提过，这里的Topic Model和LDA已相去甚远。思路是这样的，首先，从一个文字表达的矩阵中（有可能就直接是传统的Word Embedding），通过Convolutional Filters转换成为一些文字的特征表达（Feature Vector）。文章里选用的是线性的转换方式。这些Convolutional Filters都是作用在文字的一个Window上面，所以从概念上讲，这一个步骤很类似Word Embedding。得到这些Feature Vector以后，作者们又使用了一个Max-Over-Time的Pooling动作（也就是每一组文字的Feature Vector中最大值），从而产生了文档的表达。注意，这里学到的依然是比较直接的Embedding。然后，作者们定义了一组Topic的产生形式。首先，是有一个“输入Topic矩阵”。这个矩阵和已经得到的文档特征一起，产生一个Attention的向量。这个Attention向量再和“输出Topic矩阵”一起作用，产生最终的文档Topic向量。这也就是这部分模型的主要部分。\n最终，这个文档Topic向量通过用于预测文档中的每一个字来被学习到。有了这个文档Topic向量以后，作者们把这个信息用在了一个基于LSTM的Language Model上面。这一部分，其实就是用了一个类似于GRU的功能，把Topic的信息附加在Language Model上。文章在训练的时候，采用了Joint训练的方式，并且使用了Google发布的Word2Vec已经Pre-trained的Word Embedding。所采用的种种参数也都在文章中有介绍。\n文章在一些数据集上做了实验。对于Topic部分来说，文章主要和LDA做比较，用了Perplexity这个传统的测量，还比较了Topic Coherence等。总体说来，提出的模型和LDA不相上下。从Language Model的部分来说，提出的模型也在APNews、IMDB和BNC上都有不错的Perplexity值。总体说来，这篇文章值得文字挖掘的研究者和NLP的研究者泛读。\nTowards End-to-End Reinforcement Learning of Dialogue Agents for Information Access\n摘要：文章介绍如何进行端到端（End-to-End）的对话系统训练，特别是有数据库或者知识库查询步骤的时候，往往这一步“硬操作”阻止了端到端的训练流程。这篇文章介绍了一个“软查询”的步骤，使得整个流程可以能够融入训练流程。不过从文章的结果来看，效果依然很难说能够在实际系统中应用。可以说这篇文章有很强的学术参考价值。\n文章作者群来自于微软研究院、卡内基梅隆大学和台湾国立大学。文章中还有Lihong Li和Li Deng（邓力）这样的著名学者的影子。第一作者Bhuwan Dhingra是在卡内基梅隆大学William W. Cohen和Ruslan Salakhutdinov的博士学生，两位导师都十分有名气。而这个学生这几年在NLP领域可以说是收获颇丰：在今年的ACL上已经发表2篇文章，在今年ICLR和AAAI上都有论文发表。\n文章的核心思想是如何训练一个多轮（Multi-turn）的基于知识库（Knowledge Base）的对话系统。这个对话系统的目的主要是帮助用户从这个知识库中获取一些信息。那么，传统的基于知识库的对话系统的主要弊病在于中间有一个步骤是对于“知识库的查询”。也就是说，系统必须根据用户提交的查询（Query），进行分析并且产生结果。这一步，作者们称为“硬查询”（Hard-Lookup）。虽然这一步非常自然，但是阻断了（Block）了整个流程，使得整个系统没法“端到端”（End-to-End）进行训练。并且，这一步由于是“硬查询”，并没有携带更多的不确定信息，不利于系统的整体优化。\n这篇文章其实就是想提出一种“软查询”，从而让整个系统得以“端到端”（End-to-End）进行训练。这个新提出的“软查询”步骤，和强化学习（Reinforcement Learning）相结合，共同完成整个回路，从而在这个对话系统上达到真正的“端到端”。这就是整个文章的核心思想。那么，这个所谓的“软查询”是怎么回事？其实就是整个系统保持一个对知识库中的所有本体（Entities）所可能产生的值的一个后验分布（Posterior Distribution）。也就是说，作者们构建了这么一组后验分布，然后可以通过对这些分布的更新（这个过程是一个自然获取新数据，并且更新后验分布的过程），来对现在所有本体的确信度有一个重新的估计。这一步的转换，让对话系统从和跟知识库直接打交道，变成了如何针对后验分布打交道。\n显然，从机器学习的角度来说，和分布打交道往往容易简单很多。具体说来，系统的后验分布是一个关于用户在第T轮，针对某个值是否有兴趣的概率分布。整个对话系统是这样运行的。首先，用户通过输入的对话（Utterance）来触发系统进行不同的动作（Action）。动作空间（Action Space）包含向用户询问某个Slot的值，或者通知用户目前的结果。\n整个系统包含三个大模块： Belief Trackers、Soft-KB Lookup，以及Policy Network。Belief Trackers的作用是对整个系统现在的状态有一个全局的掌握。这里，每一个Slot都有一个Tracker，一个是根据用户当前的输入需要保持一个对于所有值的Multinomial分布，另外的则是需要保持一个对于用户是否知道这个Slot的值的置信值。文章中介绍了Hand-Crafted Tracker和Neural Belief Tracker（基于GRU）的细节，这里就不复述了。有了Tracker以后，Soft-KB Lookup的作用是保持一个整个对于本体的所有值得后验分布。最后，这些后验概率统统被总结到了一个总结向量（Summary Vector）里。这个向量可以认为是把所有的后验信息给压缩到了这个向量里。而Policy Network则根据这个总结向量，来选择整个对话系统的下一个动作。这里文章也是介绍了Hand-Crafted的Policy和Neural Policy两种情况。整个模型的训练过程还是有困难的。\n虽然作者用了REINFORCE的算法，但是，作者们发现根据随机初始化的算法没法得到想要的效果。于是作者们采用了所谓的Imitation Learning方法，也就是说，最开始的时候去模拟Hand-Crafted Agents的效果。\n在这篇文章里，作者们采用了模拟器（Simulator）的衡量方式。具体说来，就是通过与一个模拟器进行对话从而训练基于强化学习的对话系统。作者们用了MovieKB来做数据集。总体说来整个实验部分都显得比较“弱”。没有充足的真正的实验结果。整个文章真正值得借鉴主要是“软查询”的思想，整个流程也值得参考。但是训练的困难可能使得这个系统作为一个可以更加扩展的系统的价值不高。本文值得对对话系统有研究的人泛读。\nLearning to Skim Text\n摘要：这篇文章主要介绍如何在LSTM的基础上加入跳转机制，使得模型能够去略过不重要的部分，而重视重要的部分。模型的训练利用了强化学习。这篇文章建议对文字处理有兴趣的读者精读。\n作者群来自Google。第一作者来自卡内基梅隆大学的Adams Wei Yu在Google实习的时候做的工作。第三作者Quoc V. Le曾是Alex Smola和Andrew Ng的高徒，在Google工作期间有很多著名的工作，比如Sequence to Sequence Model来做机器翻译（Machine Translation）等。\n文章想要解决的问题为“Skim Text”。简单说来，就是在文字处理的时候，略过不重要的部分，对重要的部分进行记忆和阅读。要教会模型知道在哪里需要略过不读，哪里需要重新开始阅读的能力。略过阅读的另外一个好处则是对文字整体的处理速度明显提高，而且很有可能还会带来质量上的提升（因为处理的噪声信息少了、垃圾信息少了）。\n\n\n图3 用上述模型处理文本的一个示例\n具体说来，文章是希望在LSTM的基础上加入“跳转”功能，从而使得这个时序模型能够有能力判读是否要略过一部分文字信息。简单说来，作者们是这么对LSTM进行改进的。首先，有一个参数R来确定要读多少文字。然后模型从一个0到K的基于Multinomial分布的这一个跳转机制中决定当前需要往后跳多少文字（可以是0，也就是说不跳转）。这个是否跳转的这一个步骤所需要的Multinomial分布，则也要基于当期LSTM的隐参数信息（Hidden State）。跳转决定以后，根据这个跳转信息，模型会看一下是否已经达到最大的跳转限制N。如果没有则往后跳转。当所有的这些步骤都走完，达到一个序列（往往是一个句子）结尾的时候，最后的隐参数信息会用来对最终需要的目标（比如分类标签）进行预测。\n文章的另一个创新点，就是引入了强化学习（Reinforcement Learning）到模型的训练中。最终从隐参数到目标标签（Label）的这一步往往采用的是Cross Entropy的优化目标函数。这一个选择很直观，也是一个标准的步骤。然而，如何训练跳转的Multinomial分布，因为其离散（Discrete）特质，则成为文章的难点。原因是Cross Entropy无法直接应用到离散数据上。那么，这篇文章采取的思路是把这个问题构造成为强化学习的例子，从而使用最近的一些强化学习思路来把这个离散信息转化为连续信息。具体说来，就是采用了Policy Gradient的办法，在每次跳转正确的时候，得到一个为+1的反馈，反之则是-1。这样就把问题转换成为了学习跳转策略的强化学习模式。文章采用了REINFORCE的算法来对这里的离散信息做处理。从而把Policy Gradient的计算转换为了一个近似逼近。这样，最终的目标函数来自于三个部分：第一部分是Cross Entropy，第二部分是Policy Gradient的逼近，第三部分则是一个Variance Reduction的控制项（为了优化更加有效）。整个目标函数就可以完整得被优化了。\n文章在好多种实验类型上做了实验，主要比较的就是没有跳转信息的标准的LSTM。其实总体上来说，很多任务（Task）依然比较机械和人工。比如最后的用一堆句子，来预测中间可能会出现的某个词的情况，这样的任务其实并不是很现实。但是，文章中提到了一个人工（Synthetic）的任务还蛮有意思，那就是从一个数组中，根据下标为0的数作为提示来跳转取得相应的数作为输出这么一个任务。这个任务充分地展示了LSTM这类模型，以及文章提出的模型的魅力：第一，可以非常好的处理这样的非线性时序信息，第二，文章提出的模型比普通的LSTM快不少，并且准确度也提升很多。\n总体说来，这篇文章非常值得对时序模型有兴趣的读者精读。文章的“Related Work”部分也很精彩，对相关研究有兴趣的朋友可以参考这部分看看最近都有哪些工作很类似。\nFrom Language to Programs: Bridging Reinforcement Learning and Maximum Marginal Likelihood\n摘要：这篇文章要解决的问题是如何从一段文字翻译成为“程序”的问题，文章适合对Neural Programming有兴趣的读者泛读。\n作者群来自斯坦福大学。主要作者来自Percy Liang的实验室。最近几年Percy Liang的实验室可以说收获颇丰，特别是在自然语言处理和深度学习的结合上都有不错的显著成果。\n这篇文章里有好一些值得关注的内容。首先从总体上来说，这篇文章要解决的问题是如何从一段文字翻译成为“程序”的问题，可以说是一个很有价值的问题。如果这个问题能够可以容易解决，那么我们就可以教会计算机编写很多程序，而不一定需要知道程序语言的细微的很多东西。从细节上说，这个问题就是给定一个输入的语句，一个模型需要把目前的状态转移到下一个目标状态上。难点在于，对于同一个输入语句，从当前的状态到可能会到达多种目标状态。这些目标状态都有可能是对当前输入语句的一种描述。但是正确的描述其实是非常有限的，甚至是唯一的。那么，如何从所有的描述中，剥离开不正确的，找到唯一的或者少量的正确描述，就成为了这么一个问题的核心。\n文章中采用了一种Neural Encoder-Decoder模型架构。这种模型主要是对序列信息能够有比较好的效果。具体说来，是对于现在的输入语句，首先把输入语句变换成为一个语句向量，然后根据之前已经产生的程序状态，以及当前的语句向量，产生现在的程序状态。在这个整个的过程中，对于Encoder作者们采用了LSTM的架构，而对于Decoder作者们采用了普通的Feed-forward Network（原因文章中是为了简化）。\n另外一个比较有创新的地方就是作者们把过于已经产生程序状态重新Embedding化（作者们说是叫Stack）。这有一点模仿普通数据结构的意思。那么，这个模型架构应该是比较经典的。文章这时候引出了另外一个本文的主要贡献，那就是对模型学习的流程进行了改进。为了引出模型学习的改进，作者们首先讨论了两种学习训练模式的形式，那就是强化学习（Reinforcement Learning）以及MML（Maximum Marginal Likelihood）的目标函数的异同。文章中提出两者非常类似，不过比较小的区别造成了MML可以更加容易避开错误程序这一结果。文章又比较了基于REINFORCE算法的强化学习以及基于Numerical Integration以及Beam Search的MML学习的优劣。总体说来，REINFORCE算法对于这个应用来说非常容易陷入初始状态就不太优并且也很难Explore出来的情况。MML稍微好一些，但依然有类似问题。文章这里提出了Randomized Beam Search来解决。也就是说在做Beam Search的时候加入一些Exploration的成分。另外一个情况则是在做Gradient Updates的时候，当前的状态会对Gradient有影响，也就是说，如果当前状态差强人意，Gradient也许就无法调整到应该的情况。这里，作者们提出了一种叫Beta-Meritocratic的Gradient更新法则，来解决当前状态过于影响Gradient的情况。\n实验的部分还是比较有说服力的，详细的模型参数也一应俱全。对于提出的模型来说，在三个数据集上都有不错的表现。当然，从准确度上来说，这种从文字翻译到程序状态的任务离真正的实际应用还有一段距离。这篇文章适合对于最近所谓的Neural Programming有兴趣的读者泛读。对怎么改进强化学习或者MML有兴趣的读者精读。文章的“Related Work”部分也是非常详尽，有很多工作值得参考。\n论文下载链接：\nMultimodal Word Distributions\nTopically Driven Neural Language Model\nTowards End-to-End Reinforcement Learning of Dialogue Agents for Information Access\nLearning to Skim Text\nFrom Language to Programs: Bridging Reinforcement Learning and Maximum Marginal Likelihood\n相关阅读：\nWWW 精选论文\nWSDM 精选论文解读\nNIPS 十大机器学习精选论文\nICML 精选论文\nSIGIR 信息检索精选论文\nWWW 2017 精选论文\n知人知面需知心——论人工智能技术在推荐系统中的应用\n\n\nCSDN AI热衷分享 欢迎扫码关注","data":"2017年09月13日 16:08:20","date":"2017年09月13日 16:08:20"}
{"_id":{"$oid":"5d3453d562f717dc0659b7e8"},"title":"Salesforce公布自然语言处理重大进展，一个模型搞定十项任务","author":"人工智能观察","content":"本文由人工智能观察编译\n译者：Sandy\n几年前，通过语音对手机提问来在互联网上找到答案基本是不可能的，因为计算机在理解人类语言方面并不是很出色。\n如今，由于机器学习方面的进步，我们开始逐渐意识到谷歌助理或苹果Siri在对我们的问题进行应答方面基本已经没有太大的问题了。不过，不可否认，这一进展是极其艰难的，需要对非常具体的自然语言处理任务进行强化培训，比如将文本翻译成语音，对感叹词或者理解代词的引用进行分析，等等。这也是Salesforce的研究人员正在着手解决的问题。\n本周三，他们发布了一篇论文，概述了一种可以同时处理10个独立自然语言处理（NLP）任务的单一模型的方法。从本质上说，这一研究是充满挑战的。据了解，这一模型被称为“自然语言十项全能（Natural Language Decathlon）”，简称“decaNLP”。\n通过将该模型与“多任务问题应答网络（Multitask Question Answering Network；MQAN）”进行结合，可以同时学习10项任务，包括问答、机器翻译、摘要、自然语言推理、情感分析、语义角色标注、关系抽取、目标导向对话、语义分析、常识代词消解。\nSalesforce的首席科学家Richard Socher在接受采访时说：“我们的模型好比是NLP领域的瑞士军刀。”换句话说，研究人员和开发人员实际上只需使用一种工具，而不必为那些任务中的每一项使用一种工具，那些任务需要无法用于其他任何任务的超级定制的模型。最终，这一模型可能会带来更有能力的聊天机器人，让它们可以更自然地与人进行交谈。\nSocher与ImageNet做了一个类比，这是一个标签图像数据库，被认为开启了深度学习的革命，使得图像识别方面取得了突破。但是对于NLP，并不能通过一项任务就能进行定义。对此，Socher称：“在NLP领域，其实不存在所有研究人员都认为：如果你在这个方面取得进展，它就会整体上改善NLP的单一任务。”\nSalesforce的研究人员，包括Bryan McCann，Nitish Shirish Keskar和Caiming Xiong，提出的方法就是将这每一项任务都视作一个回答问题的问题。Socher解释说，“问题回答其实是非常宽泛的——你可以随便问任何问题——该研究相当于提供了解决几个任务的单一模型。”\n另外，MQAN允许进行所谓的“零样本”（zero-shot）学习，这意味着该模型可以处理以前未见过的任务或未经过培训的任务。“把它应用到一个全新的任务上，这是以前从未有过的，”McCann表示。“遇到换个方式来表述或意义上略有变化，大多数模型就无能为力，现在我们的模型做到了。”\n他补充说，具体到实际应用，面对并不完全是已经学会的短语，聊天机器人可以做出极准确的答复，更像是人们平常交谈那样。\n事实上，Salesforce的这一研究也代表了著名人工智能研究员Yoshua Bengio的观点，他是蒙特利尔大学计算机科学与运筹学系教授，与Socher在机器学习领域展开过合作。\n他表示：“自从大约二十年前我开始致力于表示自然语言的词汇嵌入以来，我的目标就是，同样的表示法应该可用于所有自然语言任务。这篇论文中将所有这些任务表示为回答问题的想法至关重要，但这还不够。论文作者搞出了自然语言十项全能，为这个目标定义一个基准，并引入了最终使这个梦想有可能实现的架构上的创新。”\n（文中图片来自网络）\n投稿、约访、合作，联系邮箱aiobservation@qq.com\n添加微信aixiaozhijun，更多交流或进AI观察团","data":"2018年06月21日 20:06:32"}
{"_id":{"$oid":"5d34541d62f717dc0659b7f8"},"title":"自然语言处理入门","author":"weixin_34072458","content":"自然语言处理\n分类\n自然语言理解是个综合的系统工程，涉及了很多细分的学科。\n代表声音的 音系学：语言中发音的系统化组织。\n代表构词法的 词态学：研究单词构成以及相互之间的关系。\n代表语句结构的 句法学：给定文本的那部分是语法正确的。\n代表理解的语义 句法学 和 语用学 ：给定文本的含义和目的是什么。\n语言理解涉及语言、语境和各种语言形式的学科。但总的来说，自然语言理解又可以分为三个方面：\n词义分析\n句法分析\n语义分析\n自然语言的生成则是从结构化的数据（可以通俗理解为自然语言理解分析后的数据）以读取的方式自动生成文本。主要有三个阶段：\n文本规划：完成结构化数据中的基础内容规划。\n语句规划：从结构化数据中组合语句来表达信息流。\n实现：产生语法通顺的语句来表达文本。\n中文文本分类\n做一个中文文本分类任务，首先要做的是文本的预处理，对文本进行分词和去停用词操作，来把字符串分割成词与词组合而成的字符串集合并去掉其中的一些非关键词汇（像是：的、地、得等）。再就是对预处理过后的文本进行特征提取。最后将提取到的特征送进分类器进行训练。\n研究与应用\nNLP 在现在大火的 AI 领域有着十分丰富的应用。总体来说，自然语言处理的研究问题（主要）有下面几种：\n信息检索：对大规模文档进行索引。\n语音识别：识别包含口语在内的自然语言的声学信号转换成符合预期的信号。\n机器翻译：将一种语言翻译成另外一种语言。\n智能问答：自动回答问题。\n对话系统：通过多回合对话，跟用户进行聊天、回答、完成某项任务。\n文本分类：将文本自动归类。\n情感分析：判断某段文本的情感倾向\n文本生成：根据需求自动生成文本\n自动文摘：归纳，总结文本的摘要。\n术语\n分词\n词性标注\n命名实体消歧\n词义消歧\n句法分析\n指代消解\nHMM应用与分词\n规定每个字在一个词语当中有着4个不同的位置，词首 B，词中 M，词尾 E，单字成词 S。我们通过给一句话中的每个字标记上述的属性，最后通过标注来确定分词结果。\n考虑到独立输出假设，有限历史性假设，用来求解HMM的算法可以用\n维特比算法\n一种动态规划算法。嗯。\n文本分类\n词袋模型\n把整个文档集的所有出现的词都丢进袋子里面，然后无序的排出来（去掉重复的）。对每一个文档，按照词语出现的次数来表示文档。\nTF-IDF模型\n这种模型主要是用词汇的统计特征来作为特征集。TF-IDF由两部分组成，TF（Term frequency），IDF（Inverse document frequency）\nTF：\n\\[tf_{ij} = \\frac{n_{ij}}{\\sum_{k}n_{kj}}\\]\n其中分子 \\(n_{ij}\\) 表示词 \\(i\\) 在文档 \\(j\\) 中出现的频次。分母则是所有词频次的总和，也就是所有词的个数。\nIDF：\n\\[idf_{i} = log\\left ( \\frac{\\left | D \\right |}{1+\\left | D_{i} \\right |} \\right )\\]\n其中 \\(\\left | D \\right |\\) 代表文档的总数，分母部分 \\(\\left | D_{i} \\right |\\) 则是代表文档集中含有 \\(i\\) 词的文档数。原始公式是分母没有 \\(+1\\) 的，这里 \\(+1\\) 是采用了拉普拉斯平滑，避免了有部分新的词没有在语料库中出现而导致分母为零的情况出现。\n\\[tf*idf(i,j)=tf_{ij}*idf_{i}= \\frac{n_{ij}}{\\sum_{k}n_{kj}} *log\\left ( \\frac{\\left | D \\right |}{1+\\left | D_{i} \\right |} \\right )\\]\n使用方法\n加载词袋类：\n调整类的参数：\n建立文本库：\n训练数据获得词袋特征，转换为array\n加载TF-IDF类\n调整类参数，并训练\n中文邮件分类\n数据准备\n转化为对应列表，拼接\n划分测试集和训练集\n预处理（去停用词）\n训练fit_transform(), 测试transform()\n将特征和标签喂入SVM，测试集验证结果\n转载于:https://www.cnblogs.com/xFANx/p/10203479.html","data":"2018年12月31日 22:26:00"}
{"_id":{"$oid":"5d3454bc62f717dc0659b81a"},"title":"自然语言处理深度学习的7个应用","author":"钱曙光","content":"原文：7 Applications of Deep Learning for Natural Language Processing\n作者：Jason Brownlee\n翻译：无阻我飞扬\n摘要：在这篇文章中，作者详细介绍了自然语言处理深度学习的7种应用，以下是译文。\n自然语言处理领域正在从统计方法转变为神经网络方法。\n自然语言中仍有许多具有挑战性的问题需要解决。然而，深度学习方法在一些特定的语言问题上取得了最新的成果。这不仅仅是深度学习模型在基准问题上的表现，基准问题也是最有趣的；事实上，一个单一的模型可以学习词义和执行语言任务，从而消除了对专业手工制作方法渠道的需要。\n在这篇文章中，你会发现7个有趣的自然语言处理任务，也会了解深度学习方法取得的一些进展。\n1、 文本分类\n2、 语言建模\n3、 语音识别\n4、 字幕生成\n5、 机器翻译\n6、 文档摘要\n7、 问答（Q\u0026A）\n我试图专注于你可能感兴趣的各种类型的终端用户问题，而不是更多的学术或语言的子问题，在有些方面深度学习已经做的很好，如词性标注，程序分块，命名实体识别，等等。\n每个示例提供了一个问题描述，示例，对演示方法和结果的文档引用。大多数参考来自2015年的Goldberg’s 的优秀的NLP研究人员深度学习入门文献 。\n你有没有一个深度学习中最受欢迎的NLP应用没有被列出？请在下面的评论中告诉我。\n1、 文本分类\n给出一个文本实例，预测一个预定义的类标签。\n文本分类的目的是对文档的标题或主题进行分类。\n—575页，自然语言处理的基础统计，1999\n一个流行的分类示例是情感分析，类标签代表源文本的情感基调，比如“积极的”或“消极的”。\n下面是另外三个例子：\n垃圾邮件过滤，将电子邮件文本分类为垃圾邮件或正常邮件。\n语言识别，对源文本的语言进行分类。\n体裁分类，对小说故事体裁进行分类。\n此外，这个问题可以用某种方式加以解决，将多个类分配给一个文本，即所谓的多标签分类。如给一个源tweet预测多个#标签。\n更多相关主题的内容，请参见：\nScholarpedia的文本分类\n维基百科的文档分类\n下面是3个文本分类深度学习的论文例子：\n烂片评论的情感分析\n文本分类的DUCR结构方法，2015\n亚马逊产品评价的情感分析，IMDB电影评论和新闻文章的主题分类。\n有效使用词序进行基于卷积神经网络的文本分类，2015\n影评的情感分析，将句子分类为主观的和客观的，分类问题类型，产品评论的情感及更多。\n基于卷积神经网络的句子分类，2014\n2、 语言建模\n语言建模真的是更有趣的自然语言问题的一个子任务，特别是那些在其它输入条件下的语言模型。\n…问题是根据给出的前一个词来预测下一个词。这项任务是语音或光学字符识别的基础，也用于拼写矫正，手写识别和统计机器翻译。\n—191页，统计自然语言处理基础，1999.\n除了对语言建模的学术兴趣外，它也是许多自然语言处理体系结构深度学习的一个重要组成部分。\n一个语言模型学习词与词之间的概率关系，这样以来，新的词的序列可以生成与源文本统计学上一致的文本内容。\n单独地，语言模型可用于文本或语音生成；例如：\n生成新的文章标题。\n生成新的句子，段落，或文件。\n生成一个句子的建议延续的句子。\n有关语言建模的更多信息，请参见：\n维基百科上的语言模型\n循环神经网络的不可思议的效用，2015\n生成基于模型的合成文本语音，第十讲，牛津，2017\n下面是深度学习语言建模（仅有）的一个例子：\n英语课文、书籍和新闻文章的的语言模型。\n一种神经概率语言模型，2003\n3、 语音识别\n语音识别是理解说了什么的问题。\n…语音识别的任务是将包含自然语言话语的语音映射成说话人想要表达的对应的词。（传统的语音识别模型是通过人工建立一张语音词表，将相似发音的字母划分为一类；并借助一个分类模型实现语音到字母的转译。）\n—458页，深度学习，2016.\n给定作为音频数据的文本的发声，该模型必须生成可读的文本。\n自动给出自然语言的处理，这个问题也可被称为自动语音识别（ASR）.\n语言模型用于创建以音频数据为条件的文本输出。\n包含的一些例子：\n录制语音。\n为电影或电视节目创建文本字幕。\n开车的时候向无线电发出指令。\n有关语音识别的更多信息，请参见：\n维基百科上的语音识别\n以下是用于语音识别深度学习的3个例子：\n英语语音到文字。\n连接时间分类：循环神经网络的不分段标签序列数据，2006。\n英语语音到文字。\n深度循环神经网络的语音识别，2013。\n英语语音到文字。\n用于语音识别的卷积神经网络结构的研究和优化技术，2014。\n4、字幕生成\n字幕生成是描述图像内容的问题。\n给定一个数字图像，如一张图片，生成关于这个图像内容的文本描述。\n语言模型用于创建符合图像内容的字幕。\n包含的一些例子：\n描述一个场景的内容。\n为照片创建标题。\n描述一个视频。\n这不仅仅是对听障者的一个应用程序，还可以为图像和视频数据生成可读的文本，将来可以搜索，比如在网上。\n以下是字幕生成深度学习的3个例子：\n为照片生成字幕。\n展示，出席和讲述：视觉注意力的神经图像字幕生成，2016.\n为照片生成字幕。\n展示和讲述：神经图像字幕生成器，2015.\n为视频生成字幕。\n片段到片段—视频到文本，2015.\n5、机器翻译\n机器翻译是把源文本从一种语言转换成另外一种语言的问题。\n…机器翻译，文本或语音从一种语言到另外一种语言的自动翻译，它是NLP最重要的应用。\n—463页，统计自然语言处理基础，1999.\n考虑到深度神经网络的使用，该领域被称为神经机器翻译。\n在一个机器翻译任务中，输入由一些语言中的一系列符号组成，计算机程序必须把它转换成另一种语言中的符号序列。这通常用于自然语言，比如从英语到法语的翻译。深度学习最近开始对这种任务产生重要影响。\n—98页，深度学习，2016.\n语言模型用于输出翻译以后语言的目标文本，以源文本为基础。\n包含的一些例子：\n将一个文本文件从法语翻译成英语。\n将西班牙音频翻译成德语文本。\n将英语文本翻译成意大利音频。\n更多关于神经机器翻译，请参见：\n维基百科上的神经机器翻译。\n下面是机器翻译深度学习的3个例子：\n从英语到法语的文本翻译。\n基于神经网络的片段到片段的学习，2014\n从英语到法语的文本翻译。\n联合学习对齐和翻译的神经机器翻译，2014\n从英语到法语的文本翻译。\n基于循环神经网络组合语言和翻译模型，2013\n6、文档摘要\n文档摘要是对创建的文本文档进行简短描述的任务。\n如上所述，语言模型用于基于完整文档的摘要输出。\n一些文档摘要的例子：\n为一篇文档创建一个标题。\n为一篇文档创建一个摘要。\n更多关于这个话题的信息，请参见：\n维基百科上的自动摘要。\n深度学习已经被应用于自动文本摘要（成功）了吗？\n下面是文档摘要深度学习的3个例子：\n新闻文章中的句子摘要\n一个抽象概括的神经注意力模型，2015\n新闻文章中的句子摘要\n使用片段到片段RNN(循环神经网络)的抽象总结及更多，2015\n新闻文章中的句子摘要\n通过提取句子和单词的神经摘要，2016\n7、 问答\n回答问题就是给定一个主题，如文本文件，回答关于这个主题的一个特定问题。\n…问答系统尝试回答用户以问题形式表述的疑问，它返回适当的短语，如位置，人员，或者日期。例如，问题是总统肯尼迪为什么被刺杀？可能回答的短语是：Oswald（“凶手”奥司华德）。\n—377页，统计自然语言处理基础，1999\n包含的一些例子：\n维基百科上的问答\n更多关于问答的信息，请参见：\n关于维基百科文章的问答\n关于新闻文章的问答\n关于医疗记录的问答\n下面是问答深度学习的3个例子：\n新闻文章中的问答\n阅读和理解的机器教学，2015\n回答关于Freebase文章的一般知识性问题\n用多列卷积神经网络回答关于Freebase的问题，2015\n回答给定文件的事实型问题\n深度学习回答选择句，2015\n扩展阅读\n如果你需要更深入的了解，本节提供更多用于NLP深度学习应用程序的资源。\n自然语言处理的优先神经网络模型，2015\n从零（几乎）开始自然语言处理，2011\n自然语言处理深度学习，实践概述，牛津，2017\n深度学习或神经网络的NLP问题已成功应用？\n深度学习能像自然语言处理在视觉和语音处理领域一样取得类似的突破吗？\n2017年10月14日，SDCC 2017之大数据技术实战线上峰会即将召开，邀请圈内顶尖的布道师、技术专家和技术引领者，共话大数据平台构建、优化提升大数据平台的各项性能、Spark部署实践、企业流平台实践、以及实现应用大数据支持业务创新发展等核心话题，七位大牛与你相聚狂欢，详情查看所有嘉宾和议题，以及注册参会。","data":"2017年09月27日 15:33:25"}
{"_id":{"$oid":"5d3454e462f717dc0659b826"},"title":"第 34、35、36 集：机器学习与人工智能；计算机视觉；自然语言处理","author":"djch0319","content":"第 34 集：机器学习与人工智能\n01:23 分类 Classification\n01:25 分类器 Classifier\n01:34 特征 Feature\n02:03 标记数据 Labeled data\n02:38 决策边界 Decision boundaries\n03:00 混淆矩阵 Confusion matrix\n03:39 未标签数据 Unlabeled data\n03:49 决策树 Decision tree\n04:25 支持向量机 Support Vector Machines\n05:52 人工神经网络 Artificial Neural Network\n08:34 深度学习 Deep learning\n09:21 弱AI, 窄AI Weak AI, Narrow AI\n09:43 强AI Strong AI\n10:42 强化学习 Reinforcement Learning\n分类器：做分类的算法叫做分类器。“特征”是用来帮助“分类”的值。\n标记数据不仅要记录特征值，还要记录种类\n机器学习算法的目的：是最大化正确分类+最小化错误分类\n决策树：生成决策树的机器学习算法\n多个决策树组成的算法叫“决策森林”\n支持向量机：本质上是用任意线段来切分“决策空间”，而且线段不一定是直线，可以是多项式或其他数学函数\n人工神经网络：“决策树” 和 “支持向量机”这样的技术发源自统计学。但也有不用统计学的算法，比如人工神经网络\n神经元常见处理流程：加权、求和、偏置(加或减一个固定值)、激活函数\n激活函数，也叫传递函数。应用与输出，对结果执行最后一次数学修改\n做神经网络时，这些偏差和权重，一开始会设计成随机值，然后将最后算出的结果跟样本数据进行对比，不断调整和【训练】，直到获得 让结果最接近真实数据的 偏差值和权重值。\n输入层：主要用于样本数据输入\n隐藏层：可以有很多层，用于对数据进行加权、求和等各种处理\n输出层：输出最后处理的结果\n弱AI：只能做特定任务\n强AI：像人一样聪明的AI\n强化学习：通过反复试错，自己发现成功的策略\n人工智能的处理逻辑：(个人总结的认识，仅供参考)\n1.有庞大的样本数据(正确性，真实性)\n2.根据推测的关联因素作为数据【特征】\n3.然后利用神经网络算法得出的结果，与样本数据进行对比。得到最优算法的各种值(如某个特征的权重)\n4.然后输入真实的数据，根据最优算法进行事实的提前预测\n第 35 集：计算机视觉\n02:41 检测垂直边缘的算法\n03:26 核/过滤器 kernel or filter\n03:56 卷积 convolution\n04:23 Prewitt 算子 Prewitt Operators\n05:34 维奥拉·琼斯 人脸检测 Viola-Jones Face Detection\n05:35 卷积神经网络 Convolutional Neural Networks\n07:33 识别出脸之后，可以进一步用其他算法定位面部标志，如眼睛和眉毛具体位置，从而判断心情等信息\n08:52 跟踪全身的标记点，如肩部，手臂等\nRGB：三原色\n颜色跟踪算法—最简单的计算机视觉算法：跟踪一个颜色物体，比如一个白色的球。\n1.颜色跟踪算法是一个个像素搜索，因为颜色是在一个像素里。 首先记下球的颜色，保存最中心像素的RGB值。然后让程序在图像中找最接近这个颜色的像素。可以在视频的每一帧图片跑这个算法，跟踪球的位置。\n卷积算法：核  、  2个边缘增强的核\n卷积神经网络：\n第 36 集：自然语言处理\n01:50 词性 Parts of speech\n02:15 短语结构规则 Phrase structure rules\n02:32 分析树 Parse tree\n05:30 语音识别 Speech recognition\n07:26 谱图 Spectrogram\n07:44 快速傅立叶变换 Fast Fourier Transform\n08:42 音素 Phonemes\n09:29 语音合成 Speech Synthesis\nNLP 自然语言处理(Natural Language Processing)\n快速傅利叶变换(FFT)：一种将声音的波形转换成频率图形的算法\n语音识别：声音频率的”共振峰“特征—音素识别—组成单词—识别句首和句尾—语言模型修正口音和发音错误—转换成文字\n语音合成技术：人机交互，正向反馈","data":"2019年06月02日 20:31:37"}
{"_id":{"$oid":"5d3454fd62f717dc0659b82a"},"title":"机器学习之NLP自然语言处理","author":"jingChenGauss","content":"本文将分八大步骤来介绍如何用机器学习处理文本数据。从最简单的方法开始，逐一讲解，然后分析更具体的方案细节，如特征工程、词向量和深度学习。你可以把本文看作是标准方法的高度概括。\n代码链接： https://github.com/hundredblocks/concrete_NLP_tutorial/blob/master/NLP_notebook.ipynb\n一、收集数据\n每一个机器学习问题都始于数据，比如一组邮件、帖子或是推文。文本信息的常见来源包括：\n商品评价（来自 Amazon、Yelp 以及其他 App 商城）\n用户产出的内容（推文、Facebook 的帖子、StackOverflow 的提问等）\n问题解决（客户请求、技术支持、聊天记录）\n“社交媒体中的灾难”数据集\n在这篇文章中，我们将使用 CrowdFlower 提供的一个数据集，名为“社交媒体中的灾难（Disasters on Social Media）”。\n贡献者们查看了超过 10000 条具有类似“着火”、“隔离”、“混乱”等搜索关键词的推文，然后标记这个推文是否和灾难事件有关（与之相反的是一些玩笑、电影点评或是一些非灾难性的事件）。\n我们的任务是分辨出哪些推文是真正和灾难事件相关的，而不是一些类似电影描述的不相关话题。为什么呢？一个潜在的应用是针对突发事件对执法人员进行专门的提醒，而不会被其他无关信息，比如 Adam Sandler 新上映的电影所干扰。这项任务中一个特别的挑战是这两种情况在搜索推文的时候都用到了相同的检索词，所以我们只能通过细微的差别去区分他们。\n在下面的文章中，我们将把与灾难事件相关的推文称为“灾难”，将其他推文称为“不相关的”。\n标签\n我们已经标注过数据，所以知道推文是如何分类的。比起优化一个复杂的无监督学习方法，寻找和标记足够多的数据来训练模型会更加快捷、简单和廉价。\n二、数据清洗\n数据科学家的一个必备技能是知道自己的下一步操作是处理模型还是数据。有一个好的经验法则是先观察数据然后进行数据清洗。一个干净的数据集能使模型学习到有意义的特征而不会被一些不相关的噪声影响。\n可以借鉴下方的列表来进行数据清洗：（查看代码获取更多信息）\n去除一切不相关的字符，比如任何非字母数字的字符\n标记你的文本，将他们拆分为独立的单词\n去除不相关的词语，比如 @这类提醒或是 url 链接\n将所有字母转换成小写，这样“hello”，“Hello”，“HELLO”就会被当做同样的单词处理\n将拼错的单词或是多种拼法的单词与某个特定的表达绑定（比如：“cool”/“kewl”/“cooool”）\n考虑词形还原（比如将“am”,“are”,“is”都看做“be”）\n完成这些步骤并检查完其他错误后，我们就可以使用这些干净的、标记过的数据进行模型训练了！\n代码:https://github.com/hundredblocks/concrete_NLP_tutorial/blob/master/NLP_notebook.ipynb：\n三、找到一种好的数据表达方式\n机器学习模型通常以数值作为输入。我们这里的数据集是句子列表，为了让模型可以从数据中学到句子的特征模式，我们首先要找到一种方法来把它转换成模型能理解的形式，即数字列表。\n独热编码（One-hot encoding）- 词袋模型（Bag of Words）\n通常为计算机解释文本的方法是将每一个字符都编为一个独立的数字（例如 ASCII 码）。如果使用这种简单的表达来做分类器，需要我们的数据从头开始学习词语的结构，这对大多数数据集来说是很难实现的。所以我们需要一种更上层的方法。\n例如，我们可以为数据集中的所有单词制作一张词表，然后将每个单词和一个唯一的索引关联。每个句子都是由一串数字组成，这串数字是词表中的独立单词对应的个数。通过列表中的索引，我们可以统计出句子中某个单词出现的次数。这种方法叫做 词袋模型，它完全忽略了句子中单词的顺序。如下图所示：\n用词袋模型表示句子。句子在左边，模型表达在右边。向量中的每一个索引代表了一个特定的单词。\n嵌入可视化\n在“社交媒体中的灾难”样本词表中大概会有 20000 个单词，这意味着每句句子都会用一个长度为 20000 的向量来表示。向量的 大部分会被 0 填充，因为每句话只包含了词表中很小的一个子集。\n为了看出嵌入的工作是否真正抓住了和问题相关的信息（比如推文是否与灾难相关），有一个好方法是将它们可视化，然后观察结果是否有很好的分布。考虑到词表通常很大，而且用 20000 维的数据做可视化是基本不可能的，所以我们使用了 PCA 这种技术将数据降到二维。绘制如下：\n词袋嵌入模型的可视化结果\n两个分类看起来没有很好的分离，这可能是我们选择的嵌入方法的特征或是单纯因为维度的减少引起的。为了了解词袋模型的特征是否会起一些作用，我们可以试着基于它训练一个分类器。\n四、分类\n当初次接触一个问题，通常来说最好的方法是先挑选一个能解决问题的最简单的工具。当提到数据分类时，一般最受欢迎的是通用性和可解释性兼具的逻辑回归算法。这种算法很容易训练而且结果也是可解释的，你可以很轻松地从模型中提取出最重要的一些系数。\n我们将数据分为两个集合，训练集用于匹配模型，测试集用于观察应用在未知数据上的效果。训练后我们得到了 75.4% 的精确度。结果还不错！推测出现最多的类（“不相关”）只能达到 57%。但是，即使是 75% 的精确度也已经足够好了，我们决不能在还没有理解模型的情况下就开始应用它。\n五、检验混淆矩阵\n理解模型的第一步，是了解模型产生的错误分类，以及最不应该出现的错误。在我们的例子中，“误报”是指将不相关的推文分类为“灾难事件”，“漏报”是指将与灾难有关的推文归类为“与灾难无关的事件”。如果要优先处理潜在的灾难事件，那就要降低“漏报”。而如果资源受限，就要优先降低“误报”，减少错误的提醒。使用混淆矩阵可以很好地可视化这些信息，并将模型预测的结果与数据的真是标签进行比较。理想情况下，模型的预测结果与真实情况（人工标注）完全相符，这时候混淆矩阵是一条从左上角到右下角的对角矩阵。\n混淆矩阵（绿色部分所占比例较高，蓝色部分的比例较低）\n相比假阳性结果，我们的分类器产生了更多的假阴性结果。换句话说，模型中最常见的错误是将灾难性推文错误归类为不相关推文。如果假阳性结果的执法成本很高的话，那么我们分类器的这种偏差就是良性的。\n解释和说明模型\n为了验证模型并解释它的预测结果，我们需要明确模型用以进行判断的那些词汇。如果我们的数据有偏差，而分类器在样本数据中却能做出准确预测，那这样的模型就无法在现实世界中很好地推广。\n在这里，我们可以用图表来表示灾难性推文与不相关推文两类预测中最重要的词汇。由于我们可以对模型的预测系数进行提取和排序，用词袋模型(bag-of-words)和Logistic回归模型很容易就能计算出单词的重要性。\n词袋模型(bag-of-words)：单词的重要性\n我们的分类器能够正确识别出一些模式（如广岛、大屠杀等），但在一些毫无意义的词汇（如heyoo、x1392等）上还是出现了过拟合。词袋模型（bag-of-words）仅能处理庞大词汇表内的不同词汇，并对所有的词汇分配相同的权重。然而，其中一些词汇出现得非常频繁，但却只是预测结果的噪音数据。接下来，我们将试着找到一种能够表示词汇在句子中出现频率的方法，尽量让模型从数据中获取更多的信号。\n六、词汇结构的统计\nTF-IDF嵌入模型\n为了让模型专注于学习更有意义的词汇，我们可以在词袋模型上面使用TF-IDF评分（术语频率，逆文档频率）。TF-IDF通过词汇在数据集中的稀有程度来评估它的重要性，适度削弱出现过于频繁的单词。下图是TF-IDF嵌入模型的PCA映射：\n可视化TF-IDF嵌入模型\n从中可以看出，两种颜色之间有了更清晰的区分，使这两类数据更易于被分类器分开。在新模型上训练Logistic回归，我们得到了76.2％的准确度，说明TF-IDF确实有助于提高识别性能。\n尽管只是非常微小的改进，但我们的模型能否就此学到更重要的词汇呢？如果能得到更好的结果，同时还能避免模型在无关词汇上的过拟合，那TF-IDF嵌入模型就可以被认为是真正的“升级版”模型。\n\nTF-IDF嵌入模型：单词的重要性\n可以看到，新模型学到的词汇看起来相关度更高！尽管测试集的指标只是略有增加，但是我们对模型的识别性能更有把握，因此部署新模型的交互系统会让用户体验更为舒适。\n七、语义信息的利用Word2Vec\nTF-IDF嵌入模型能够学习到信号更高频的词汇。然而，如果部署该模型后，我们很可能会遇到一些训练集中从未出现过的词汇。先前的模型均无法正确分类这样的新数据，即便其中的词汇与训练集非常相似。\n要解决这个问题，我们就要捕捉词汇的语义，这就意味着模型需要理解“好”与“积极”在语义上的距离要比“杏”和“大陆”更接近。这里的工具就是Word2Vec。\n使用预训练的嵌入模型\nWord2Vec是一种为单词查找连续嵌入的技术。通过阅读大量的文字，它能够学习并记忆那些倾向于在相似语境中出现的词汇。经过足够的数据训练之后，它会为词汇表中的每个单词都生成一个300维的向量，用以记录语义相近的词汇。\nWord2Vec作者在一个非常大的语料库上预训练并开源了该模型。利用这一语料库，我们可以将一些语义知识纳入到我们的模型内。预训练好的词向量可以在本文的GitHub代码库中找到。\nGitHub地址：https://github.com/hundredblocks/concrete_NLP_tutorial\n句子分级表示\n让分类器快速得到句子嵌入的方法，是先将句中所有词汇Word2Vec得分的平均化。这与此前词袋模型的做法类似，但这里我们在保留语义信息的同时只丢弃句法。\nWord2vec模型的句子嵌入\n利用前面的可视化技术对新模型绘图，结果如下：\nWord2Vc嵌入模型的可视化结果\n在这里，两组颜色的分离程度更大一些，这就意味着Word2Vec能够帮助分类器更好地分离这两种类别。再一次使用Logistic回归，得到77.7％的准确率，是我们迄今最好的结果！\n复杂性/可解释性权衡取舍\n与先前的模型不同，新模型无法将每个单词都表示成一维向量，因此很难看出哪些词汇与我们的分类结果相关度最高。尽管我们仍可使用Logistic回归的系数，但它们仅与嵌入的300个维度相关，而与词汇索引值并不相关。\n模型准确率确实提高了，但完全做不了可解释性分析就有点得不偿失了。不过，对于更复杂的模型，我们可以利用LIME这样的“黑盒解释器”来稍微解释一下分类器具体是如何工作的。\nLIME\nLIME是Github上的一个开源软件包，它允许用户通过观察输入的扰动（比如在我们的例子中，从句中移除单词）来分析一个特定分类器的预测结果是如何变化的。\n从下图来看它对我们数据集中几个句子的解释：\n正确分类的灾难性词汇被归类为“相关”\n这个词对分类的影响似乎不太明显\n不过，我们没有时间去逐一探索数据集中的数千个样本。我们要做的是在代表性的测试样本上运行LIME，以此来分析哪些词汇对于分类预测的影响更大。这样，我们就可以像前面一样获取到单词的重要性分数，以验证模型的预测结果。\nWord2Vec：单词的重要性\n模型能够提取高度相关的词，这意味着它做出了可解释的决定。这些词汇的相关度是最高的，因此我们更愿意在实际生产中部署这样的模型。\n八、使用端到端的方式训练语法特征\n我们已经介绍过如何用快速有效的办法来生成紧凑的句子嵌入。然而，通过省略词汇的顺序，我们也放弃了语句的所有句法信息。如果简单的方法给不出令人满意的结果，那我们就用更为复杂的模型：将整个句子作为输入并预测标签，同时无需建立中间表示。一种常见的做法是把句子视为词向量的序列，如使用Word2Vec，或是GloVe、CoVe等更先进的方法。接下来我们详细讨论。\n高效的端到端的训练体系结构（源）\n用于句子分类的卷积神经网络（https://arxiv.org/abs/1408.5882）训练速度很快。它作为一种入门级的深度学习架构，能够很好地解决分类问题。尽管CNN声名主要源自它在图像处理方面的出色能力，但在文本相关任务上，它所提供的结果也相当优异。且相比多数复杂的NLP方法（如LSTM、Encoder/Decoder架构等），CNN训练速度也更快。它能够保留单词的顺序，很好地学习单词的序列特征以及其他有用信息。相对于先前的模型，它可以区分出“Alex eats plants”与“Plants eat Alex”之间差异。\n相比先前的方法，该模型的训练不需更多的工作，但效果却好得多，准确率高达79.5％！与前面的步骤一样，下一步也要继续探索并可视化该模型的预测结果，以验证它是否为最佳模型。做到这一步，你应该能自己完成这里的操作。\n写在最后\n简单回顾一下，我们在各个步骤中所用的方法是这样的：\n从一个简单的模型快速开始\n解释模型的预测\n理解模型分类中的错误样本\n使用这些知识来决定下一步的部署。\n上述八大步骤所用的模型是我们处理短文本时的几个特定实例，但其背后的解决方法已经广泛被用在各类NLP问题的实际处理上。\n原文链接：\nhttps://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e","data":"2018年08月25日 21:59:27"}
{"_id":{"$oid":"5d34551662f717dc0659b830"},"title":"人工智能AI面试常用技术-自然语言NLP-LSTM","author":"jackiewang777","content":"先介绍一下我自己，我有过5年以上机器学习的工作经验，主要工作内容有图像分析,自然语言，模式识别。我认为该领域最稀缺的人才是NLP专业，然后是图像分析（CV），我准备做一个系列的文章，把我在面试过程中遇到的各种技术性问题，每个问题分别讲解。\n1.我常常会遇到问LSTM的问题： 现在详细讲解下\n理解LSTM前要先理解： RNN\n（Recurrent Neural Networks)这种神经网络带有环，可以将信息持久化。\n\n在上图所示的神经网络AA中，输入为XtXt，输出为htht。AA上的环允许将每一步产生的信息传递到下一步中。环的加入使得RNN变得神秘。不过，如果你多思考一下的话，其实RNN跟普通的神经网络也没有那么不同。一个RNN可以看作是同一个网络的多份副本，每一份都将信息传递到下一个副本。RNN在一系列的任务中都取得了令人惊叹的成就，比如语音识别，图片标题等等。\nLSTM是这一系列成功中的必要组成部分。LSTM(Long Short Term Memory)是一种特殊的循环神经网络，在许多任务中，LSTM表现得比标准的RNN要出色得多。几乎所有基于RNN的令人赞叹的结果都是LSTM取得的，接下来将着重介绍LSTM。\n长期依赖(Long Term Dependencies)的问题\nRNN的一个核心思想是将以前的信息连接到当前的任务中来，例如，通过前面的视频帧来帮助理解当前帧。如果RNN真的能够这样做的话，那么它们将会极其有用。但是事实真是如此吗？未必。\n有时候，我们只需要看最近的信息，就可以完成当前的任务。比如，考虑一个语言模型，通过前面的单词来预测接下来的单词。如果我们想预测句子“the clouds are in the sky”中的最后一个单词，我们不需要更多的上下文信息——很明显下一个单词应该是sky。在这种情况下，当前位置与相关信息所在位置之间的距离相对较小，RNN可以被训练来使用这样的信息。\n然而，有时候我们需要更多的上下文信息。比如，我们想预测句子“I am a tall man, .....i can play basketball”中的最后一个单词。最近的信息告诉我们，最后一个单词可能是某种语言的名字，然而如果我们想确定到底是哪种语言的话，我们需要basket这个更远的上下文信息。实际上，相关信息和需要该信息的位置之间的距离可能非常的远。\n随着距离的增大，RNN对于如何将这样的信息连接起来无能为力。\n\n\nLSTM，全称为长短期记忆网络(Long Short Term Memory networks)，是一种特殊的RNN，能够学习到长期依赖关系。LSTM由Hochreiter \u0026 Schmidhuber (1997)提出，许多研究者进行了一系列的工作对其改进并使之发扬光大。LSTM在许多问题上效果非常好，现在被广泛使用。\nLSTM在设计上明确地避免了长期依赖的问题。记住长期信息是小菜一碟！所有的循环神经网络都有着重复的神经网络模块形成链的形式。在普通的RNN中，重复模块结构非常简单，例如只有一个tanh层。\n我会在专栏和视频中免费给大家具体讲解细节的技术。","data":"2018年05月12日 14:00:56"}
{"_id":{"$oid":"5d34553c62f717dc0659b838"},"title":"深度学习 自然语言处理 资料推荐","author":"算法学习者","content":"本次首先推荐邱锡鹏老师的两个报告：\n1. Deep learning for natural language processing\nhttp://nlp.fudan.edu.cn/xpqiu/slides/20160618_DL4NLP@CityU.pdf\n主要讨论了深度学习在自然语言处理中的应用。其中涉及的模型主要有卷积神经网络，递归神经网络，循环神经网络网络等，应用领域主要包括了文本生成，问答系统，机器翻译以及文本匹配等。\n\n\n卷积神经网络结构示意图\n\n\n递归神经网络结构示意图\n循环神经网络示意图\n\n\n2. 神经网络与深度学习\nhttp://nlp.fudan.edu.cn/xpqiu/slides/20151226_CCFADL_NNDL.pdf\n这个报告可以看作上个报告的简短中文版，其中涉及的主要模型有卷积神经网络，循环神经网络以及前馈神经网络等。\n\n\n前馈神经网络示意图\n\n\n\n\n另外推荐两篇来自ACL 2016的tutorial\n1. 语义表示相关的Tutorial\n\n链接如下：\nhttp://wwwusers.di.uniroma1.it/~collados/Slides_ACL16Tutorial_SemanticRepresentation.pdf\n\n\n2. 短文本理解相关的tutorial\n\n链接如下：\nhttp://www.wangzhongyuan.com/tutorial/ACL2016/Understanding-Short-Texts/\n\n\n最后分享几篇搜索意图识别相关的论文\n1. Query Intent Detection using Convolutional Neural Networks\n这篇论文利用卷积神经网络来检测查询意图。\n链接如下：\nhttp://people.cs.pitt.edu/~hashemi/papers/QRUMS2016_HBHashemi.pdf\npeople.cs.pitt.edu/~hashemi/papers/QRUMS2016_slides.pdf\n\n\n2. Deep LSTM based Feature Mapping for Query Classification\n基于深度学习中的LSTM用于查询分类\n链接如下：\nhttps://aclweb.org/anthology/N/N16/N16-1176.pdf\n\n\n长短时记忆网络结构示意图\n\n\n\n\n3. Understanding User’s Query Intent with Wikipedia\n这篇论文利用维基百科来理解用户查询意图\nhttp://wwwconference.org/www2009/proceedings/pdf/p471.pdf","data":"2017年03月22日 01:14:07"}
{"_id":{"$oid":"5d34558862f717dc0659b847"},"title":"【自然语言处理】良心资源，不点开会后悔的那种","author":"alicelmx","content":"ACL文章链接\nhttp://www.aclweb.org/anthology/\n2016年ACL-WMT机器翻译数据集\nPaperWeekly\nhttp://rsarxiv.github.io/\n中国NLP联盟（墙裂推荐）\nhttps://github.com/NLPchina\n中文NLP工具大全\nhttps://github.com/NLPchina/Awesome-Chinese-NLP\n中文Embedding\nhttps://github.com/Embedding/Chinese-Word-Vectors\n更多优秀文章\nhttp://blog.csdn.net/leyounger/article/details/78085905\nNLP 工具\n(1) NLTK : http://www.nltk.org\n提供了常用的文本预处理函数和算法\n(2) HIT LTP汉语分词系统\n模型库百度Pan：\nhttps://pan.baidu.com/share/link?shareid=1988562907\u0026uk=2738088569#list/path=%2Fltp-models\n(3) Sklearn :\n用于分类相关的还是很好用的， 不仅提供小数据集搜索，对于大数据环境下也提供了支持partial_fit方法的模型来获得相对好的结果\nhttp://scikit-learn.org/stable/modules/scaling_strategies.html\nNLP工作\nhttp://www.nlpjob.com/\nStanford cs224d\nhttp://blog.csdn.net/neighborhoodguo/article/details/46868143\n可爱的数据集\nhttps://zhuanlan.zhihu.com/p/25138563\nhttps://blog.csdn.net/tMb8Z9Vdm66wH68VX1/article/details/78153519\n优秀NLP系统\n(1) 腾讯Peacock推荐系统\nhttp://www.flickering.cn/nlp/2015/03/peacock：大规模主题模型及其在腾讯业务中的应用/\n神经网络相关\n(1) 深度学习与计算机视觉系列\nhttp://blog.csdn.net/xiaopihaierletian/article/details/74688643\n(2) 用深度学习（CNN RNN Attention）解决大规模文本分类问题 - 综述和实践\nhttps://zhuanlan.zhihu.com/p/25928551\n深度学习相关\n(1) 深度学习中文版书籍\nhttps://github.com/exacity/deeplearningbook-chinese\n(2) 深度学习CrossEntropy导数推导\nhttp://www.cnblogs.com/python27/p/MachineLearningWeek05.html\nNLP比赛经验\n(1) 知乎“看山杯” 夺冠记\nhttps://zhuanlan.zhihu.com/p/28923961\n还有一个用TensorFlow做的版本：\nhttp://blog.csdn.net/jerr__y/article/details/77751885","data":"2018年10月31日 12:54:52"}
{"_id":{"$oid":"5d3455b862f717dc0659b84f"},"title":"自然语言处理(4)之中文文本挖掘流程详解（小白入门必读）","author":"机器学习算法与Python学习","content":"微信公众号\n关键字全网搜索最新排名\n【机器学习算法】：排名第一\n【机器学习】：排名第一\n【Python】：排名第三\n【算法】：排名第四\n\n\n前言\n在对文本做数据分析时，一大半的时间都会花在文本预处理上，而中文和英文的预处理流程稍有不同，本文对中文文本挖掘的预处理流程做一个总结。\n\n\n中文文本挖掘预处理特点\n首先看中文文本挖掘预处理与英文文本挖掘预处理的不同点。\n\n\n首先，中文文本是没有像英文的单词空格那样隔开的，因此不能直接像英文一样可以直接用最简单的空格和标点符号完成分词。所以一般需要用分词算法来完成分词，在（干货 | 自然语言处理(1)之聊一聊分词原理）已经讲到了中文的分词原理。\n\n\n第二，中文的编码不是utf8，而是unicode。这样会导致在分词时，需要处理编码的问题。\n\n\n上述两点构成了中文分词相比英文分词的一些不同点，后面也会重点讲述这部分的处理。了解了中文预处理的一些特点后，通过实践总结下中文文本挖掘预处理流程。\n\n\n1. 数据收集\n在文本挖掘之前，需要得到文本数据，文本数据的获取方法一般有两种：使用别人做好的语料库和自己用爬虫去在网上去爬自己的语料数据。\n\n\n对于第一种方法，常用的文本语料库在网上有很多，如果大家只是学习，则可以直接下载下来使用，但如果是某些特殊主题的语料库，比如“机器学习”相关的语料库，则这种方法行不通，需要我们自己用第二种方法去获取。\n\n\n对于第二种使用爬虫的方法，开源工具有很多，通用的爬虫我一般使用beautifulsoup。但是我们需要某些特殊的语料数据，比如上面提到的“机器学习”相关的语料库，则需要用主题爬虫（也叫聚焦爬虫）来完成，一般使用ache。 ache允许我们用关键字或者一个分类算法来过滤出我们需要的主题语料，比较强大。\n\n\n2. 除去数据中非中文部分\n这一步主要是针对用爬虫收集的语料数据，由于爬下来的内容中有很多html的一些标签，需要去掉。少量的非文本内容的可以直接用Python的正则表达式(re)删除, 复杂的则可以用beautifulsoup来去除。去除掉这些非文本的内容后，就可以进行真正的文本预处理了。\n\n\n3. 处理中文编码问题\n由于Python2.x不支持unicode的处理，因此使用Python2.x做中文文本预处理时需要遵循的原则是，存储数据都用utf8，读出来进行中文相关处理时，使用GBK之类的中文编码，在下一节的分词再用例子说明这个问题。\n\n\n4. 中文分词\n常用的中文分词软件有很多，比较推荐结巴分词。安装也很简单，比如基于Python的，用\"pip install jieba\"就可以完成。下面我们就用例子来看看如何中文分词。\n\n\n首先准备两段文本，内容分别如下：\nnlp_test0.txt\n沙瑞金赞叹易学习的胸怀，是金山的百姓有福，可是这件事对李达康的触动很大。易学习又回忆起他们三人分开的前一晚，大家一起喝酒话别，易学习被降职到道口县当县长，王大路下海经商，李达康连连赔礼道歉，觉得对不起大家，他最对不起的是王大路，就和易学习一起给王大路凑了5万块钱，王大路自己东挪西撮了5万块，开始下海经商。没想到后来王大路竟然做得风生水起。沙瑞金觉得他们三人，在困难时期还能以沫相助，很不容易。\nnlp_test2.txt\n沙瑞金向毛娅打听他们家在京州的别墅，毛娅笑着说，王大路事业有成之后，要给欧阳菁和她公司的股权，她们没有要，王大路就在京州帝豪园买了三套别墅，可是李达康和易学习都不要，这些房子都在王大路的名下，欧阳菁好像去住过，毛娅不想去，她觉得房子太大很浪费，自己家住得就很踏实。\n\n\n首先将文本从第一个文件中读取进来，并使用中文GBK编码，再调用结巴分词，最后把分词结果用uft8格式存在另一个文本nlp_test1.txt中。代码如下：\n# -*- coding: utf-8 -*-\n\n\nimport jieba\n\n\nwith open('./nlp_test0.txt') as f:\ndocument = f.read()\ndocument_decode = document.decode('GBK')\ndocument_cut = jieba.cut(document_decode)\nresult = ' '.join(document_cut)\nresult = result.encode('utf-8')\nwith open('./nlp_test1.txt', 'w') as f2:\nf2.write(result)\nf.close()\nf2.close()\n\n\n输出的文本内容如下：\nnlp_test1.txt\n沙 瑞金 赞叹 易 学习 的 胸怀 ， 是 金山 的 百姓 有福 ， 可是 这件 事对 李达康 的 触动 很大 。 易 学习 又 回忆起 他们 三人 分开 的 前一晚 ， 大家 一起 喝酒 话别 ， 易 学习 被 降职 到 道口 县当 县长 ， 王 大路 下海经商 ， 李达康 连连 赔礼道歉 ， 觉得 对不起 大家 ， 他 最 对不起 的 是 王 大路 ， 就 和 易 学习 一起 给 王 大路 凑 了 5 万块 钱 ， 王 大路 自己 东挪西撮 了 5 万块 ， 开始 下海经商 。 没想到 后来 王 大路 竟然 做 得 风生水 起 。 沙 瑞金 觉得 他们 三人 ， 在 困难 时期 还 能 以沫 相助 ， 很 不 容易 。\n\n\n可以发现对于一些人名和地名，jieba处理不好，不过可以帮jieba加入词汇如下：\njieba.suggest_freq('沙瑞金', True)\njieba.suggest_freq('易学习', True)\njieba.suggest_freq('王大路', True)\njieba.suggest_freq('京州', True)\n\n\n\n现在再重新进行读文件，编码，分词，编码和写文件，代码如下：\nwith open('./nlp_test0.txt') as f:\ndocument = f.read()\ndocument_decode = document.decode('GBK')\ndocument_cut = jieba.cut(document_decode)\nresult = ' '.join(document_cut)\nresult = result.encode('utf-8')\nwith open('./nlp_test1.txt', 'w') as f2:\nf2.write(result)\nf.close()\nf2.close()\n\n\n输出的文本内容如下：\nnlp_test1.txt\n沙瑞金 赞叹 易学习 的 胸怀 ， 是 金山 的 百姓 有福 ， 可是 这件 事对 李达康 的 触动 很大 。 易学习 又 回忆起 他们 三人 分开 的 前一晚 ， 大家 一起 喝酒 话别 ， 易学习 被 降职 到 道口 县当 县长 ， 王大路 下海经商 ， 李达康 连连 赔礼道歉 ， 觉得 对不起 大家 ， 他 最 对不起 的 是 王大路 ， 就 和 易学习 一起 给 王大路 凑 了 5 万块 钱 ， 王大路 自己 东挪西撮 了 5 万块 ， 开始 下海经商 。 没想到 后来 王大路 竟然 做 得 风生水 起 。 沙瑞金 觉得 他们 三人 ， 在 困难 时期 还 能 以沫 相助 ， 很 不 容易 。\n\n\n以同样的方法对第二段文本nlp_test2.txt进行分词和写入文件nlp_test3.txt。\nwith open('./nlp_test2.txt') as f:\ndocument2 = f.read()\ndocument2_decode = document2.decode('GBK')\ndocument2_cut = jieba.cut(document2_decode)\n#print  ' '.join(jieba_cut)\nresult = ' '.join(document2_cut)\nresult = result.encode('utf-8')\nwith open('./nlp_test3.txt', 'w') as f2:\nf2.write(result)\nf.close()\nf2.close()\n\n\n输出的文本内容如下：\nnlp_test3.txt\n沙瑞金 向 毛娅 打听 他们 家 在 京州 的 别墅 ， 毛娅 笑 着 说 ， 王大路 事业有成 之后 ， 要 给 欧阳 菁 和 她 公司 的 股权 ， 她们 没有 要 ， 王大路 就 在 京州 帝豪园 买 了 三套 别墅 ， 可是 李达康 和 易学习 都 不要 ， 这些 房子 都 在 王大路 的 名下 ， 欧阳 菁 好像 去 住 过 ， 毛娅 不想 去 ， 她 觉得 房子 太大 很 浪费 ， 自己 家住 得 就 很 踏实 。\n\n\n5. 引入停用词\n上面解析的文本中有很多无效的词，比如“着”，“和”，还有一些标点符号，这些我们不想在文本分析时引入，因此需要去掉，这些词就是停用词。常用的中文停用词表是1208个（下载地址：https://pan.baidu.com/s/1gfMXMl9）。\n\n\n现在将停用词表从文件读出，并切分成一个数组备用：\n#从文件导入停用词表\nstpwrdpath = \"stop_words.txt\"\nstpwrd_dic = open(stpwrdpath, 'rb')\nstpwrd_content = stpwrd_dic.read()\n#将停用词表转换为list\nstpwrdlst = stpwrd_content.splitlines()\nstpwrd_dic.close()\n\n\n6. 特征处理\n现在可以用scikit-learn来对文本特征进行处理，在（）中讲到了两种特征处理的方法，向量化与Hash Trick。而向量化是最常用的方法，因为它可以接着进行TF-IDF的特征处理。在（）中也讲到了TF-IDF特征处理的方法，这里使用scikit-learn的TfidfVectorizer类来进行TF-IDF特征处理。\n\n\nTfidfVectorizer类可以完成向量化，TF-IDF和标准化三步。当然，还可以处理停用词。现在把上面分词好的文本载入内存：\nwith open('./nlp_test1.txt') as f3:\nres1 = f3.read()\nprint res1\nwith open('./nlp_test3.txt') as f4:\nres2 = f4.read()\nprint res2\n\n\n\n\n现在可以进行向量化，TF-IDF和标准化三步处理（这里引入了上面的停用词表）。\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ncorpus = [res1,res2]\nvector = TfidfVectorizer(stop_words=stpwrdlst)\ntfidf = vector.fit_transform(corpus)\nprint tfidf\n\n\n\n部分输出如下：\n(0, 44)0.154467434933\n(0, 59)0.108549295069\n(0, 39)0.308934869866\n(0, 53)0.108549295069\n....\n(1, 27)0.139891059658\n(1, 47)0.139891059658\n(1, 30)0.139891059658\n(1, 60)0.139891059658\n\n\n\n看看每个词与TF-IDF的对应关系：\nwordlist = vector.get_feature_names()#获取词袋模型中的所有词\n# tf-idf矩阵 元素a[i][j]表示j词在i类文本中的tf-idf权重\nweightlist = tfidf.toarray()\n#打印每类文本的tf-idf词语权重，第一个for遍历所有文本，第二个for便利某一类文本下的词语权重\nfor i in range(len(weightlist)):\nprint \"-------第\",i,\"段文本的词语tf-idf权重------\"\nfor j in range(len(wordlist)):\nprint wordlist[j],weightlist[i][j]\n\n\n部分输出如下：\n\n\n-------第 0 段文本的词语tf-idf权重------\n一起 0.217098590137\n万块 0.217098590137\n三人 0.217098590137\n三套 0.0\n下海经商 0.217098590137\n.....\n-------第 1 段文本的词语tf-idf权重------\n.....\n李达康 0.0995336411066\n欧阳 0.279782119316\n毛娅 0.419673178975\n沙瑞金 0.0995336411066\n没想到 0.0\n没有 0.139891059658\n浪费 0.139891059658\n王大路 0.29860092332\n.....\n\n\n\n7. 建立分析模型\n有了每段文本的TF-IDF的特征向量，就可以利用这些数据建立分类或者聚类模型了，或者进行主题模型的分析。此时的分类聚类模型和之前讲的非自然语言处理的数据分析没有什么两样。因此对应的算法都可以直接使用。\n\n\n小结\n本文对中文文本挖掘预处理的过程做了一个总结，希望可以帮助到大家。需要注意的是这个流程主要针对一些常用的文本挖掘，并使用了词袋模型，对于某一些自然语言处理的需求则流程需要修改。比如我们涉及到词上下文关系的一些需求，此时不能使用词袋模型。而有时候我们对于特征的处理有自己的特殊需求，因此这个流程仅供自然语言处理入门者参考。\n\n\n欢迎分享给他人让更多的人受益\n参考：\n宗成庆《统计自然语言处理》 第2版\n博客园\nhttp://www.cnblogs.com/pinard/p/6744056.html\n\n\n近期热文\n普通程序员转型深度学习指南\n\n机器学习(33)之局部线性嵌入(LLE)【降维】总结\n\n干货 | 自然语言处理(3)之词频-逆文本词频（TF-IDF）详解\n\n机器学习(32)之典型相关性分析(CCA)详解 【文末有福利......】\n\n干货 | 自然语言处理(2)之浅谈向量化与Hash-Trick\n\n干货 | 自然语言处理(1)之聊一聊分词原理\n加入微信机器学习交流群\n请添加微信：guodongwe1991\n备注姓名-单位-研究方向\n\n\n广告、商业合作\n请添加微信：guodongwe1991\n（备注：商务合作）","data":"2017年12月23日 00:00:00"}
{"_id":{"$oid":"5d3455d062f717dc0659b857"},"title":"百度AI利用NLP自然语言处理技术发力智能写作","author":"李文哲_AI","content":"百度上线的智能写作平台集合了百度领先的自然语言处理技术（NLP）和知识图谱技术（KG），内置百度丰富的数据和素材，给您提供自动写作和辅助写作的能力，帮您全面提升内容创作效率，旨在成为最懂你的智能写作助手。\n自动写作：\n自动写作技术能够让机器自主的完成文章写作。当前计算机已经能够自动的撰写新闻快讯、热点组稿、春联等类型的文章。\n百度自动写作的财经新闻，这类自动写作通常以结构化数据为输入，智能写作算法按照人类习惯的方式描述数据中蕴含的主要信息，非常擅长完成时效性新闻的报道任务。这种自动写作的典型例子包括地震快讯、财经快讯、体育战报等。\n热点组稿写作，这类自动写作通常以海量素材为基础，按照应用需求线索筛选合适的内容，并基于对内容的分析抽取关注的信息，最后按照写作逻辑组织为篇章结果，非常擅长挖掘大数据中蕴含的分布、关联等信息。这种自动写作的典型例子包括热点组稿、事件脉络、排行盘点等。\n百度 NLP 的智能春联，在这类自动写作任务中，机器基于充分的训练数据、训练模型并得到创作能力，可以根据人类的指令，产出符合特定格式要求的创作结果。这种自动写作的典型例子包括智能写诗、智能对联等。\n辅助写作：\n提供领域热点事件发现、热点事件脉络、文本纠错和自动摘要能力，从素材收集、文章撰写、文章检查三个角度辅助您的创作，提升写作效率。\n辅助写作的目标是为人类的写作过程提供辅助，按照人的写作步骤，辅助写作主要从四个角度提供帮助：写什么、如何写、如何写好、如何更好地分发。\n写作之前，算法可以通过分析当前热点事件和话题，推荐适合创作的热门话题；写作过程中，算法可以提供写作素材、写作风格、写作内容建议等多角度的辅助；写作完成后，算法可以从纠错、配图、排版等多个角度提供改进建议，帮助人类作者完善写作结果。\n智能写作的核心技术\n1.经典自然语言生成算法\n从篇章规划（写什么）—到微观规划（如何写）—再到表层实现（转换为自然语言）来逐步按照“流水线”进行生成算法。\n2.神经网络序列生成算法\n深度神经网络技术为人工智能带来的技术变革，在智能写作技术中的集中体现是神经网络序列生成算法。这种算法能够有效利用语料中包含的统计规律，按特定要求产出符合人类语言特性的文本结果。智能写诗是机器创作的常用例子，也是序列生成算法的一个典型例子。\n在生成每一句诗歌时，关键词和上一句的信息会经过循环神经网络结构计算，作为生成诗歌中每一个字的依据。模型在学习过大量诗歌语料之后，能够具备概率统计意义上输出“像诗歌的字序列”的能力，这种能力即对应机器创作型智能写作，能够根据需求生成诗歌。\n虽然机器的创作“思路”和人类有本质的不同，但是机器生成的诗歌与人写的诗歌效果相当，因此能够帮助人类分担相应的工作量。\n标题生成是在辅助写作中有广泛的应用：完成写作之后，如果能够快速确定一个优质的标题，不仅节省作者的人力投入，也有利于写作结果的分发，让写作结果更好地触及相对应需求和兴趣的读者。\n3. 文本分析技术\n文本分析技术主要是关注作为智能写作素材的“输入”。对于各类素材，需要利用文本分析技术抽取关键词、标签、情感倾向、摘要等用于智能写作的特征。\n文章来至：百度AI","data":"2019年04月19日 16:52:25"}
{"_id":{"$oid":"5d34561562f717dc0659b868"},"title":"自然语言处理NLP（四）","author":"村雨1943","content":"实体识别\n实体识别–分块类型：\n名词短语分块；\n标记模式分块；\n正则表达式分块；\n分块的表示方法：标记和树状图；\n分块器评估；\n命名实体识别；\n命名实体定义：指特定类型的个体，是一些确切的名词短语，如组织、人、日期等；\n命名实体识别定义：指通过识别文字中所提及的命名实体，然后确定NE的边界和类型；\n命名实体关系提取；\n文法分析\n文法定义： 即就是文章的书写规则，一般用来指以文字、词语、短句、句子编排而成的完整语句和文章的合理性组织；\n文法用途：\n1、性能超越n-grams；\n2、确定句子成分结构；\n形式语法：一个四元组G=(N, ∑, P, S)，各个符号代表的意义如下：\nN：非终结符的有限集合（有事也称为变量级戒句法种类集）；\n∑：终结符号的有限集合；\nV：总词汇表，N∪∑；\nP：一组重写规则的有限集合，P=｛α→β｝，其中α，β是V种元素所构成的串，α种至少应该含有一个非终结符号；\nS：S∈N，叫做句子的符戒初始符；\n上下文无关文法：\n解析器：\n定义：根据文法产生式处理输入的矩阵，同时建立一个或多个符号文法的组成结构；\n分类：\n递归下降解析器：自上而下模式；\n移近-规约解析器：自下而上模式；\n左角落解析器：自上而下和自下而上两种模式相结合；\n递归下降和左角落解析都存在一定的缺陷，因此可以才用动态规划的方法进行解析；\n依存关系与依存文法：\n依存文法：关注词与其他词之间的关系；\n依存关系：中心词与其他从属直接的二元非对称关系；\n当前的一些语法困境\n语言数据与无限可能性；\n句子构造；\n句子歧义问题；\n自然语言理解\n智能问答系统；\n一阶逻辑；\n补充运算；\n句子语义理解；\n段落语义理解；\n图灵测试\n阿兰·图灵与1950年提出，测试在测试者和被测试者相互隔开的情况下，通过一些简单的装置向被测试者随意提问。通过一些问题之后，若被测试者的答复有超过30%的部分无法让测试者确认出是人还是机器的回答，则此时这台机器通过测试， 且被认为具有人工智能；\n命题逻辑\n一阶逻辑\n语法\n独立变量；\n独立常量；\n带不同参数的谓词；\n非逻辑常量；\n逻辑常量；\n存在量词；\n全称量词；\n采取约定：\u003cen，t\u003e是由n个e类型的参数所组成而产生一个类型为t的表达式的谓词的类型，此类情况下，则称n为谓词元数；\n语句的语义\n组合原则：整体含义是部分含义与他们的句法相结合方式的函数；\n语料库结构\nTIMIT的结构\n内容覆盖：方言，说话者，材料；\nTIMIT的设计特点\n包含语音与字形标注层；\n在多个维度的变化与方言地区和二元音覆盖范围中找到一个平衡点；\n将原始语音学时间作为录音来捕捉和标注来捕捉之间的区别；\n层次结构清晰，结构是树状结构，使用时目的性；\nTIMIT的基本数据类型\n词典\n文本\n语料库的生命周期\n创建语料库的方案\n研究过程中逐步形成；\n实验研究过程中收集；\n特定语音的参考语料；\n质量控制\nKappa系数：衡量两个人的判断类别，然后修正其期望一致性，越大一致性越好；\nwindowdiff打分器：衡量两个句子分词的一致性；\n维护与演变\n数据采集\n采集方式\n网上获取；\n文字处理器文件获取；\n电子表格和数据库中获取；\n通过数据格式转换获取；\n使用Toolbox数据；\n标注层\n分词；\n断句；\n分段；\n词性；\n句法结构；\n浅层语义；\n对话与段落；","data":"2018年10月03日 21:35:50"}
{"_id":{"$oid":"5d34563662f717dc0659b86c"},"title":"自然语言处理简洁自用代码合集","author":"Joliph","content":"记录文字处理的各种简介的代码表示\n1.快速去除中文标点（read的时候要以utf8格式）\ndef clean_str(string): string = re.sub(\"[^\\u4e00-\\u9fff]\", \" \", string) string = re.sub(r\"\\s{2,}\", \" \", string)#合并多个空格为一个 return string.strip()\n2.快速分词,默认一行为一样本\ndef seperate_line(string): return ''.join([word + ' ' for word in jieba.cut(string)]) f=open(\"xxx\",'r',encoding=\"utf8\") lines = list(f.readlines()) lines = [clean_str(seperate_line(line)) for line in lines]\n3.分行，使得一行为一句\nfor line in lines line.replace('\\n','').replace('，','\\n').replace('。','\\n').replace('！','\\n').replace('？','\\n') 重新写入\n4.语料训练集生成\ndef load_positive_negative_data_files(positive_data_file_path, negative_data_file_path): positive_example_lists = read_and_clean_zh_file(positive_data_file_path) #positive_example_lists ---\u003e 0维度上为样本有多少句句子，1维度上为每句的string，单词间空格隔开 negative_example_lists = read_and_clean_zh_file(negative_data_file_path) #positive_example_lists ---\u003e 形式同上 # Combine data x_text = positive_example_lists + negative_example_lists # Generate labels positive_labels = [[1] for _ in positive_example_lists] negative_labels = [[0] for _ in negative_example_lists] y = np.concatenate([positive_labels, negative_labels], 0) return [x_text, y]\n5.句子填充\ndef padding_sentences(input_sentences, padding_token, padding_sentence_length = None): sentences = [sentence.split(' ') for sentence in input_sentences] if padding_sentence_length !=None: max_sentence_length=padding_sentence_length else: max_sentence_length=max([len(sentence) for sentence in sentences]) for i,sentence in generate(sentences): if len(sentence) \u003e max_sentence_length: sentences[i] = sentence[:max_sentence_length] else: sentence.extend([padding_token] * (max_sentence_length - len(sentence))) return (sentences, max_sentence_length)\n6.从gensim训练模型拿词向量\nmodel加载 all_vectors = [] embeddingDim = w2vModel.vector_size embeddingUnknown = [0 for i in range(embeddingDim)] for sentence in sentences: this_vector = [] for word in sentence: if word in w2vModel.wv.vocab: this_vector.append(w2vModel[word]) else: this_vector.append(embeddingUnknown) all_vectors.append(this_vector) return all_vectors\n7.打乱np矩阵的方法\nx=[0,1,2,3,4,5,6] x=np.array(x) np.random.seed(10) shuffle_indices = np.random.permutation(np.arange(len(x))) print(shuffle_indices) x_shuffled = x[shuffle_indices] print(x_shuffled) 输出 [2 6 0 3 4 5 1] [2 6 0 3 4 5 1]\n8.分离部分样本为训练集和验证集\n1.打乱样本顺序（参考上面代码） 2.按比例截断","data":"2018年02月11日 16:31:43"}
{"_id":{"$oid":"5d3456b762f717dc0659b884"},"title":"自然语言处理（NLP）各任务最新研究进展，包括数据集和优秀论文","author":"Joyeishappy","content":"整理NLP-Progress上的东西。\n目录\nEnglish\nCommon Sense 知识推理\nEvent2Mind\nSWAG\nWinograd Schema Challenge\nConstituency parsing\nPenn Treebank\nEnglish\nCommon Sense 知识推理\nCommon sense reasoning tasks are intended to require the model to go beyond pattern recognition. Instead, the model should use “common sense” or world knowledge to make inferences.\n常识推理任务旨在要求模型超越模式识别。 相反，知识推理模型应该使用“常识”或世界知识来做出推论。\nEvent2Mind\nEvent2Mind is a crowdsourced corpus of 25,000 event phrases covering a diverse range of everyday events and situations. Given an event described in a short free-form text, a model should reason about the likely intents and reactions of the event’s participants. Models are evaluated based on average cross-entropy (lower is better).\nEvent2Mind是一个包含25,000个活动短语的众包语料库，涵盖各种日常事件和情境。 鉴于在简短的自由格式文本中描述的事件，模型应该推断事件的参与者可能的意图和反应。 基于平均交叉熵评估模型（越低越好）。\nModel\nDev\nTest\nPaper / Source\nCode\nBiRNN 100d (Rashkin et al., 2018)\n4.25\n4.22\nEvent2Mind: Commonsense Inference on Events, Intents, and Reactions\nConvNet (Rashkin et al., 2018)\n4.44\n4.40\nEvent2Mind: Commonsense Inference on Events, Intents, and Reactions\nSWAG\nSituations with Adversarial Generations (SWAG) is a dataset consisting of 113k multiple choice questions about a rich spectrum of grounded situations.\nSituations with Adversarial Generations（SWAG）是一个由113k多项选择问题组成的数据集，这些问题涉及丰富的基础情境。\nModel\nDev\nTest\nPaper / Source\nCode\nBERT Large (Devlin et al., 2018)\n86.6\n86.3\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\nBERT Base (Devlin et al., 2018)\n81.6\n-\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\nESIM + ELMo (Zellers et al., 2018)\n59.1\n59.2\nSWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference\nESIM + GloVe (Zellers et al., 2018)\n51.9\n52.7\nSWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference\nWinograd Schema Challenge\nThe Winograd Schema Challenge is a dataset for common sense reasoning. It employs Winograd Schema questions that require the resolution of anaphora: the system must identify the antecedent of an ambiguous pronoun in a statement. Models are evaluated based on accuracy.\nExample:\nThe trophy doesn’t fit in the suitcase because it is too big. What is too big? Answer 0: the trophy. Answer 1: the suitcase\nWSC是常识推理的数据集。 它使用了需要解决回指的Winograd Schema问题：系统必须识别句子中的模糊的代词。 模型基于准确性进行评估。\n\n例：\n\n奖杯不适合行李箱，因为它太大了。 什么太大了？\n回答0：奖杯。 答案1：行李箱\nModel\nScore\nPaper / Source\nWord-LM-partial (Trinh and Le, 2018)\n62.6\nA Simple Method for Commonsense Reasoning\nChar-LM-partial (Trinh and Le, 2018)\n57.9\nA Simple Method for Commonsense Reasoning\nUSSM + Supervised DeepNet + KB (Liu et al., 2017)\n52.8\nCombing Context and Commonsense Knowledge Through Neural Networks for Solving Winograd Schema Problems\nConstituency parsing 句法解析\nConsituency parsing aims to extract a constituency-based parse tree from a sentence that represents its syntactic structure according to a phrase structure grammar.\nExample:\nSentence (S) | +-------------+------------+ | | Noun (N) Verb Phrase (VP) | | John +-------+--------+ | | Verb (V) Noun (N) | | sees Bill\nRecent approaches convert the parse tree into a sequence following a depth-first traversal in order to be able to apply sequence-to-sequence models to it. The linearized version of the above parse tree looks as follows: (S (N) (VP V N)).\nPenn Treebank\nThe Wall Street Journal section of the Penn Treebank is used for evaluating constituency parsers. Section 22 is used for development and Section 23 is used for evaluation. Models are evaluated based on F1. Most of the below models incorporate external data or features. For a comparison of single models trained only on WSJ, refer to Kitaev and Klein (2018).\nModel\nF1 score\nPaper / Source\nSelf-attentive encoder + ELMo (Kitaev and Klein, 2018)\n95.13\nConstituency Parsing with a Self-Attentive Encoder\nModel combination (Fried et al., 2017)\n94.66\nImproving Neural Parsing by Disentangling Model Combination and Reranking Effects\nIn-order (Liu and Zhang, 2017)\n94.2\nIn-Order Transition-based Constituent Parsing\nSemi-supervised LSTM-LM (Choe and Charniak, 2016)\n93.8\nParsing as Language Modeling\nStack-only RNNG (Kuncoro et al., 2017)\n93.6\nWhat Do Recurrent Neural Network Grammars Learn About Syntax?\nRNN Grammar (Dyer et al., 2016)\n﻿93.3\nRecurrent Neural Network Grammars\nTransformer (Vaswani et al., 2017)\n92.7\nAttention Is All You Need\nSemi-supervised LSTM (Vinyals et al., 2015)\n92.1\nGrammar as a Foreign Language\nSelf-trained parser (McClosky et al., 2006)\n92.1\nEffective Self-Training for Parsing\nDomain adaptation\nSentiment analysis\nThe Multi-Domain Sentiment Dataset is a common evaluation dataset for domain adaptation for sentiment analysis. It contains product reviews from Amazon.com from different product categories, which are treated as distinct domains. Reviews contain star ratings (1 to 5 stars) that are generally converted into binary labels. Models are typically evaluated on a target domain that is different from the source domain they were trained on, while only having access to unlabeled examples of the target domain (unsupervised domain adaptation). The evaluation metric is accuracy and scores are averaged across each domain.\n多域情感数据集是用于情绪分析的域适应的通用评估数据集。 它包含来自Amazon.com的不同产品类别的产品评论，这些评论被视为不同的域。 评论包含星级（1至5星），通常转换为二进制标签。 模型通常在目标域上进行评估，该目标域与它们所训练的源域不同，而只能访问目标域的未标记示例（无监督域适应）。 评估指标是准确性，并且每个域的平均得分。\nModel\nDVD\nBooks\nElectronics\nKitchen\nAverage\nPaper / Source\nMulti-task tri-training (Ruder and Plank, 2018)\n78.14\n74.86\n81.45\n82.14\n79.15\nStrong Baselines for Neural Semi-supervised Learning under Domain Shift\nAsymmetric tri-training (Saito et al., 2017)\n76.17\n72.97\n80.47\n83.97\n78.39\nAsymmetric Tri-training for Unsupervised Domain Adaptation\nVFAE (Louizos et al., 2015)\n76.57\n73.40\n80.53\n82.93\n78.36\nThe Variational Fair Autoencoder\nDANN (Ganin et al., 2016)\n75.40\n71.43\n77.67\n80.53\n76.26\nDomain-Adversarial Training of Neural Networks\nMulti-task learning\nMulti-task learning aims to learn multiple different tasks simultaneously while maximizing performance on one or all of the tasks.\nDecaNLP\nThe Natural Language Decathlon (decaNLP) is a benchmark for studying general NLP models that can perform a variety of complex, natural language tasks. It evaluates performance on ten disparate natural language tasks.\nResults can be seen on the public leaderboard.\nGLUE\nThe General Language Understanding Evaluation benchmark (GLUE) is a tool for evaluating and analyzing the performance of models across a diverse range of existing natural language understanding tasks. Models are evaluated based on their average accuracy across all tasks.\nThe state-of-the-art results can be seen on the public GLUE leaderboard.","data":"2018年12月05日 11:22:00"}
{"_id":{"$oid":"5d3456cf62f717dc0659b88c"},"title":"自然语言处理基础知识HR","author":"湾区人工智能","content":"1.假设句子按单词顺序为w1,w2,...,wn ，那么这个句子的概率公式为？\n句子S在语料库中出现的概率P(S)=P(w1,w2,w3...wn)。根据条件概率公式P(w1,w2,w3...wn)=P(w1)*P(w2|w1)*p(w3|w1,w2)...P(wn|w1,w2...w(n-1))。\n2.是否句子越长概率就越低？为什么？\n3.一个语言模型的困惑度是怎么计算的？是什么意义？\n理论方法：迷惑度/困惑度/混乱度（preplexity），其基本思想是给测试集的句子赋予较高概率值的语言模型较好,当语言模型训练完之后，测试集中的句子都是正常的句子，那么训练好的模型就是在测试集上的概率越高越好，公式如下：\n由公式可知，迷惑度越小，句子概率越大，语言模型越好\n4.神经网络的语言模型相对N-Gram模型有哪些改进的地方？实际的应用情况如何？\n5.Word2Vec中skip-gram,cbow两者比较的优缺点是哪些？\n6.HMM, CRF两者比较的优缺点是哪些？\n7.Blue评价指标是干嘛用的？他考虑了哪些因素？缺点是什么？\n8.做一个翻译模型，如果输出的词典很大，例如有100万个词，要怎么解决这个计算量问题？\n9.什么是交叉熵？和KL距离有什么关系？\n10.sgd, momentum, adagrad, adam这些优化算法之间的关系和区别是怎样的？分别适用于什么场景\n11.理论上两层的神经网络可以拟合任意函数，为什么现在大多数是用多层的神经网络？\n12.生成模型和判别模型两者差别是啥？分别适用于什么场景？\n13.AUC的评估指标是怎么定义的？如果计算的AUC结果\u003c0.5，主要是什么原因导致的？\n14.逻辑回归和线性回归的区别是啥？适用场景分别是？\n15.编码实现softmax","data":"2018年11月18日 11:41:13"}
{"_id":{"$oid":"5d3456f562f717dc0659b892"},"title":"【NLP】自然语言处理 完整流程","author":"__盛夏光年__","content":"自然语言处理 完整流程\n第一步：获取语料\n1、已有语料\n2、网上下载、抓取语料\n第二步：语料预处理\n1、语料清洗\n2、分词\n3、词性标注\n4、去停用词\n三、特征工程\n1、词袋模型（BoW）\n2、词向量\n第四步：特征选择\n第五步：模型训练\n1、模型\n2、注意事项\n（1）过拟合\n（2）欠拟合\n（3）对于神经网络，注意梯度消失和梯度爆炸问题。\n第一步：获取语料\n语料，即语言材料，是构成语料库的基本单元。 所以，人们简单地用文本作为替代，并把文本中的上下文关系作为现实世界中语言的上下文关系的替代品。我们把一个文本集合称为\n语料库（Corpus）\n，当有几个这样的文本集合的时候，我们称之为\n语料库集合(Corpora)\n。（定义来源：百度百科）按语料来源，我们将语料分为以下两种：\n1、已有语料\n纸质或者电子文本资料==》电子化==》语料库。\n2、网上下载、抓取语料\n国内外标准开放数据集（比如国内的中文汉语有搜狗语料、人民日报语料） 或 通过爬虫。\n第二步：语料预处理\n语料预处理大概会占到整个50%-70%的工作量。\n基本过程： 数据清洗==》分词==》词性标注==》去停词\n1、语料清洗\n语料清洗：在语料中找到感兴趣的内容，将不感兴趣、视为噪音的内容清洗删除。包括：对于原始文本提取标题、摘要、正文等信息，对于爬虫，去除广告、标签、HTML、JS等代码和注释。\n常见数据清洗方式：人工去重、对齐、删除和标注等，或规则提取内容、正则表达式匹配、根据词性和命名实体提取，编写脚本或代码批处理等。\n2、分词\n分词：将短文本和长文本处理为最小单位粒度是词或词语的过程。\n常见方法：基于字符串匹配的分词方法、基于理解的分词方法、基于统计的分词方法和基于规则的分词方法，其中每种方法下面对应许多具体的方法。\n难点：歧义识别 和 新词识别。 eg：“羽毛球拍卖完了”，这个可以切分成“羽毛 球拍 卖 完 了”，也可切分成“羽毛球 拍卖 完 了”==》上下文信息\n3、词性标注\n词性标注：对每个词或词语打词类标签，是一个经典的序列标注问题。eg：形容词、动词、名词等。有助于在后面的处理中融入更多有用的语言信息。\n词性标注不是非必需的。比如，常见的文本分类就不用关心词性问题，但是类似情感分析、知识推理却是需要的，下图是常见的中文词性整理。\n\n常见方法：基于规则和基于统计的方法。\n基于统计的方法：基于最大熵的词性标注、基于统计最大概率输出词性和基于 HMM 的词性标注。\n4、去停用词\n停用词：对文本特征没有任何贡献的字词，eg：标点符号、语气、人称等。\n注意：根据具体场景决定。eg：在情感分析中，语气词、感叹号是应该保留的，因为他们对表示语气程度、感情色彩有一定的贡献和意义。\n三、特征工程\n如何把分词之后的字和词语表示成计算机能够计算的类型。\n思路：中文分词的字符串 ==》 向量\n两种常用表示模型：\n词袋模型（BoW）\n词向量\n1、词袋模型（BoW）\n词袋模型（Bag of Word, BOW)：不考虑词语原本在句子中的顺序，直接将每一个词语或者符号统一放置在一个集合（如 list），然后按照计数的方式对出现的次数进行统计。统计词频这只是最基本的方式，\nTF-IDF\n是词袋模型的一个经典用法。\n2、词向量\n词向量：将字、词语转换为向量矩阵的计算模型。\n常用的词表示方法：\nOne-Hot：把每个词表示为一个很长的向量。这个向量的维度是词表大小，其中绝大多数元素为 0，只有一个维度的值为 1，这个维度就代表了当前的词。eg: [0 0 0 0 0 0 0 0 1 0 0 0 0 ... 0]\nWord2Vec：其主要包含两个模型：跳字模型（Skip-Gram）和连续词袋模型（Continuous Bag of Words，简称 CBOW），以及两种高效训练的方法：负采样（Negative Sampling）和层序 Softmax（Hierarchical Softmax）。值得一提的是，Word2Vec 词向量可以较好地表达不同词之间的相似和类比关系。\nDoc2Vec\nWordRank\nFastText\n第四步：特征选择\n关键：如何构造好的特征向量？\n==》要选择合适的、表达能力强的特征。\n常见的特征选择方法：DF、 MI、 IG、 CHI、WLLR、WFO 六种。\n第五步：模型训练\n1、模型\n对于不同的应用需求，我们使用不同的模型\n传统的有监督和无监督等机器学习模型： KNN、SVM、Naive Bayes、决策树、GBDT、K-means 等模型；\n深度学习模型： CNN、RNN、LSTM、 Seq2Seq、FastText、TextCNN 等。\n2、注意事项\n（1）过拟合\n过拟合：模型学习能力太强，以至于把噪声数据的特征也学习到了，导致模型泛化能力下降，在训练集上表现很好，但是在测试集上表现很差。\n常见的解决方法有：\n增大数据的训练量；\n增加正则化项，如 L1 正则和 L2 正则；\n特征选取不合理，人工筛选特征和使用特征选择算法；\n采用 Dropout 方法等。\n（2）欠拟合\n欠拟合：就是模型不能够很好地拟合数据，表现在模型过于简单。\n常见的解决方法有：\n添加其他特征项；\n增加模型复杂度，比如神经网络加更多的层、线性模型通过添加多项式使模型泛化能力更强；\n减少正则化参数，正则化的目的是用来防止过拟合的，但是现在模型出现了欠拟合，则需要减少正则化参数。\n（3）对于神经网络，注意梯度消失和梯度爆炸问题。","data":"2019年03月06日 17:48:19"}
{"_id":{"$oid":"5d34571662f717dc0659b89a"},"title":"【序列模型】第二课--自然语言处理与词嵌入","author":"sinat_33761963","content":"今日感想：\n窗外是瓢泼大雨。来到杭州后，最大的变化就是说话少了，白天上班部门里的IT男神们只顾打代码如飞，闭口不语；晚上回到出租屋，也只剩一个人的独处。要么看书看论文，要么写写博客和日志，要么弹弹古筝哼个跑调的小曲，或是去舞蹈室跳1小时舞，满背大汗得走过太寂静的紫荆文路，把所有的期待藏进独自妖艳的夜色。然而，又总是满心欢喜，欢喜这日复一日枯燥与无味背后，沉默着的，成长与坚持。\n课程来源：吴恩达 深度学习课程 《序列模型》\n笔记整理：王小草\n时间：2018年5月5日\n本文记录的是自然语言处理中扮演重要觉得的词嵌入向量，它几乎是许多NLP项目的底层基础，对于词向量的深入学习，将帮助你在解决其他NLP问题上有非常大的提升，一起来看看吧～\n1.词汇表征\n1.1 one-hot词编码的缺陷\n回顾上一周的词向量表示方式：one-hot编码。即根据拥有的尽可能多的语料，整理一份词典，词典长度为n，使得每个词对应一个n*1的词向量，其中该词索引所在的位置为1，其余位置为0.比如，如下图，woman这个词在索引为9853的位置上是1，其余位置为0，这就是one-hot方式的word representation.\n\none-hot的词汇表征很简单，但是也有致命缺点，它无法表示词与词之间的相似性。举个简单的例子：\nI want a glass of orange __.\n假设我先告诉你空格是填的是juice。然后再给你下面这个句子：\nI want a glass of apple __.\n聪明的宝宝你肯定马上说，诶？也可以填juice啊,因为你知道orange与apple都是水果，它们在某种意义上有相似性。但是！如果我们使用的是one-hot形式对词进行编码的话，我们完全无法根据词向量来计算词与词之间的相似性，而两个one-hot词向量的内积永远也等于0\n1.2 特征化表征featurized repredentation:word embedding\n什么叫做特征化的表示，比如选一个特征是“gender”， 于是每个词都可以评估出一个与gender相似性的值,”man”为-1，”woman”为1， 而anpple与gender完全无关，为0，以此类推，如下表示：\n\n然后又可以再选第二个特征,比如”royal”,”age”…在每个特征维度，每一个词都可以有一个对应的值，以表示该词在该特征维度上的信息相关度。\n\n假设，有300个特征，那么每个词就会形成一个300*1的词向量量，向量的每个维度都有特定特征的含义。由于”man”和”woman”是很相近的词，它们在很多特征维度上都有相近的值，因此这两个向量的距离会很近，即内积获得的相似性会很高：\n\n因此再拿这个例子来说，由于orange与apple的词向量相似性高，因此可以根据 orange后面填juice推到出apple后面也可以填juice.\nI want a glass of orange __.\nI want a glass of apple __.\n总之，特征化的表示能比one-hot更好得表示不同的词\n但是要注意的是，实际上的词向量，并不是有清晰直观的特征，告诉你第一维是性别，第二维是高贵等等，而是比这复杂得多，但我们可以去这样理解，就是向量中的每一维都代表着某个特征。\n1.3可视化词潜入 visualizing word embedding\n假设我们已经获得了300维的词向量，那么可以将它降维到2维空间，并且画在二维坐标上，如下，可见相似的词会被聚在一起\n\n常见的可视化算法有t-SNE算法，来自于laurens van der maaten和Geoff Hinton的论文。\n最后说一说为什么这个方法叫做embedding嵌入，想象一个300维的空间，一个词对应多300维的向量，就像是嵌在这个空间中的一个点，因此取名为嵌入。嘿嘿嘿。\n2.词嵌入的应用\n2.1 词嵌入在命名实体识别中的应用\n知道了词嵌入是个什么东东，那么就来看看词嵌入可以如何使用，用起来到底爽在哪里。\n还是以实体命名的例子来做介绍，假设有这样一个句子：\n\n其中，Sally Johnson是一个人名，因此它们对应的预测应该是1，其余词的预测为0.我们之所以判断Sally Johnson是一个人名，而非公司名，是因为这句话的后面说了Sally Johnson是一个farmer(农民），fammer自然是一个人了。\n假设用以上句子进行训练模型，并对以下句子进行预测：\n\n由于训练中已经知道了，后面出现了farmer，那么前面的实体应该是人名，因此Robert Lin的预测为1.在这个例子中，使用one-hot或者词嵌入或许都能正确识别出。\n但是，假设把”apple farmer”改成”durian cultivator（榴莲培育家）”呢？训练集中从未出现过durian和cultivator这两个词，于是one-hot方式就傻眼了，但是词嵌入的方式却仍然游刃有余。因为词嵌入的表征可以体现词与词之间的相似关系，而apple与durain, farmer与cultivator有很大的相似性，因此虽然训练集中压根就没有学到过这两个词，模型也可以预测出durian cultivator也是一个人。\n以上可以看到，就算我们的训练样本比较少，没有覆盖尽可能多的词或样本类型，模型还是可以根据词嵌入向量来做更准确的预测。这里词嵌入的表征方式简直功不可没。那么词嵌入是如何得来的呢？你可以考察很大的数据集，可是是一亿或这100亿个词（来自于不需要标注的文本），然后对文进行学习，获得词嵌入的向量（别急，怎么学后面会详细讲述），这个大文本自然是越大越好，尽可能得包含所有的词，其中就有durain和cultivator,于是你就可以发现durain和apple等水果很相近。\n用大量的文本训练出词嵌入，然后将词嵌入运用到只有小量样本的模型中，这就是运用了“迁移学习”。\n另一点要注意的是，上图画的是一个单向的RNN，实际上做命名实体识别，一般使用的是双向RNN。\n2.2总结词嵌入做迁移学习的步骤\n（1）从大量的文本语料中学习词嵌入的向量（1-100亿词），或者直接从网上下载别人与训练好的向量。\n（2）将词嵌入迁移到你只有少量样本的任务中，使得用几百维的向量代替之前上万维的one-hot向量。\n（3）在新的数据上微调词嵌入向量。但是若你的样本数据很少，一般就不做微调了。\n对迁移学习再多说一句，当有两个任务A，B。在A任务中你有大量的标注的数据，而B中却只有少量，于是可以将在A中学习到的东西迁移到B中，以弥补B因为样本少而导致的缺陷。\n2.3 词嵌入与人脸编码\n词嵌入与人脸编码有些些奇妙的关系。\n在使用卷积神经网络进行人脸对比时，过程如下图，输入一张图片，一层一层计算后最后会得到一个向量（比如128维），然后去比较两张图片的这两个向量的相似性，即对图片进行了编码。\n\n词嵌入也差不多，对词进行了编码，因此两者有相似之处。\n两者的不同之处是：\n输入任何一张图片，都能得到一个图像编码的向量；而词嵌入是需要事先确定词库，假设有1亿个词参与了训练，如果出现另一个新词，那么将无法得到新词的词向量。（如果没有看懂这句话，别急，看完接下去的内容就会一目了然了）\n3.词嵌入的特性\n3.1 词嵌入的类比推理特性\n词嵌入还有一个很迷人的特性，那就是帮助类比推理，尽管类比推理在NLP的应用中不是最重要的角色，不过它帮助人们认识词嵌入到底做了什么。\n假设我们已经获取了以下这些词的特征表示，并且假设词嵌入就是以下4个维度的向量。\n\n现在告诉你man对应的是woman:man---\u003ewoman,问king对应的是什么词：king---\u003e？。聪明的宝宝你肯定会说king当然对应的是queen了，是的，你知道，但是计算机不知道啊，因此我们可以借助词嵌入的特性去通过类比找到这个对应的词。\n正如上图，我们知道了man,woman的词向量，分别用e_man, e_woman表示，将这两个词向量相减得到一个向量：\n\n同理，将e_king与e_queen相减：\n\n以上现象可见，通过词嵌入可以进行类比推理，根据man---\u003ewoman找到king---\u003e？\n\n于是，我们现在来把上述过程写成算法的形式：\n\n也就是找出一个词w， 使得它的词向量e_w与e_king-e_man+e_woman的向量最相近。\n3.2 余弦相似性\n前面讲了辣么多词相似性相似性相似性，但是相似性到底如何计算呢？常用的词向量之间的相似性一般用余弦相似性计算：\n\nu,v是两个向量，其实就是根据他们的夹角的来判断相似性：\n\n通过夹角来计算余弦，当夹角=0，余弦=1，两个向量完全一致；当夹角维90余弦维0，不相似；当夹角为180，则余弦为-1， 两个向量完全相反。\n\n当然咯，还有很多计算相似性的方法，比如欧式距离：\n\n4.嵌入矩阵Embedding matrix\n上面几节分别讲了什么是词嵌入，词嵌入的应用与特性，现在开始，要具体讲一讲，我们到底是如何得到这个神奇的词嵌入的。获取词嵌入，其实就是去求一个嵌入矩阵(Embedding matrix)。于是这一节先来介绍下什么是嵌入矩阵。\n假设有10000个词的词典，若按字母排，就是从a, aaron,......,orange,......zulu, \u003cUNK\u003e，我们要做的是学习一个嵌入矩阵，大小是300*10000，这个矩阵的每一列代表的是每个词的向量：\n\n要得到每个词在次嵌入矩阵中对应的向量，使嵌入矩阵乘以这个词的one-hot向量即可。\n嵌入矩阵记为E，维度是（300*10000）\n某个词one-hot词向量（比如orange是排在词典的6257位，记为O_6257)\n因此它们的积:E * O_6257，会得到一个300 * 1的向量，其实就是这个词在嵌入举证对应的那一列向量。\n推而广之，某个词Oj的嵌入向量（j是该词在词典中的位置），就是嵌入矩阵乘以该词的one-hot向量。\n因此这一节，你只需要知道我们需要去训练这样一个嵌入矩阵，然后用这个嵌入矩阵与one-hot向量相乘，可以得到嵌入向量。\n但需要注意的是，上面我们好像看似轻而易举得得到了嵌入矩阵与one-hot向量相乘的结果，但在实际计算中，由于向量的维数巨大，相乘的操作会带来巨大的计算量，因此实际中，往往直接根据词的索引去取出嵌入矩阵中对应的那一列向量。\n5.学习词嵌入\n好了，现在我们真的要开始去一步一步了解模型到底是如何学习出词嵌入的。\n在深度学习的历史上，人们曾经用很复杂的模型结构来训练词嵌入，随着不断得探索，现在我们已经可以用很简单的模型结构来训练出非常好的效果，特别是在大数据样本的情况下。但是我们仍然从最初的复杂模型开始讲起，这样你才能更深入得理解简单的模型到底为什么会取得好的效果。真的超爱吴恩达这种把一个知识点从头到尾串起来系统讲解的姿势。\n假如你要构建一个语言模型，要根据前面的单词预测出下面这句话空格中的单词（单词下方的数字是该词在词典中的索引位置）：\n\n实践证明，建立一个语言模型，是学习词嵌入的好方法。因此我们现在来建立一个神经网络预测序列中的下一个单词。\n(1)首先空格前面的每个单词都匹配上对应的one-hot向量；\n(2)然后去乘以一个嵌入矩阵E（一开始这个E是一个随机初始化的300*10000的矩阵）；\n(3)接着相乘后得到每个词的嵌入向量（如e_4343)。\n(4)所有词嵌入向量都作为神经网络的输入\n(5)经过一个隐层之后，再输入softmax层，做10000（词典的长度）的分类，即输出层有10000个神经元，输出后可形成1*10000的向量，最好的预期是这个向量中，只有该词所在的索引位置为1，其他位置为0.\n过程如下：\n\n将每次输出的1*10000维向量与真实期望的词向量（上面是juice)计算损失，通过梯度下降法去调整嵌入矩阵中的值，使得潜入矩阵越来越能优秀地表征对应的词。\n在实际中，往往会设置一个固定长度的窗口，比如5，意思是用前4个词去预测后一个词，这样就可以去适应非常长的句子了。\n不单单可以设置不同长度的窗口，还可以使用前后文，比如用该词的前面4个词后4个词来预测该词；也可以只用前面一个词，或者前后一个词等。但实践证明，如果你的目的是训练一个语言模型，那么使用前后4个次可能效果更好，若你等目的是得到词嵌入矩阵，那么使用前后1个词也会很好。\n因此，综上所述，通过以上训练语言模型的过程，就可以顺便得到了词嵌入矩阵。这是早期最成功的词嵌入学习算法之一。\n6.word2vec\n上一节介绍了一个复杂版本的词嵌入算法，现在来介绍一个更简单更灵活的模型来获得词嵌入,其中一种叫做Skip-grames。本节内容的大多数思想来自与Tomas Mikolov, Kai Chen,Greg Corrado 和Jeff Dean.\n6.1 Skip-grames\n假设给你这样一个句子：\n\n在skip-grames模型种，要做的是抽取上下文context与目标词target配对，来构造一个监督学习问题。上下文并不一定要是前一个词或者离得最近的四个单词之类，而是随机选择一个词作为上下文词，比如随机选择一个context词：orange.接着再随机选择一个在一定词距中的target词,比如随机选到了:juice,或者随机选到了前面的词：glass（词距是一开始认为设定的，比如前后10个词中随机选）。\n显然，这不是一个简单的监督学习，因为context词前后n个词距中许多不同词。但构造这个监督学习模型，并不是去解决模型本身的问题，而是想通过这个训练过程，去得到中间的词嵌入矩阵。\n现在来讲讲模型的细节，假设仍然使用一个10000词的词表（当然实际上要大得多）。并且已经随机取了一对context和target词，比如分别是context:orange–\u003etarget:juice.模型的过程和上一节一样：\n输入context词的one-hot词向量–\u003e乘以初始化的词嵌入矩阵E–\u003e得到词嵌入向量–\u003e经过softmax层–\u003e输出词汇表大小长度的词向量y\n\nsoftmax层的计算公式如下：\n\n损失函数如下：是两个one-hot词向量的差值之和\n\ny是context词的one-hot词向量,y帽是模型softmax层的输出，也是一个长度与前者相当的词向量。\n这个模型种有两类参数，一个是词嵌入矩阵E中的值，一个somtmax层中的参数，随着损失函数的最小化，这两类参数都会得到优化，并且越来越准确，从而我们就得到了我们最终想要的词嵌入矩阵啦～以上就是skip-gram模型～\n6.2 problems with softmax classification\n但是上面讲述的Skip-grames模型有一个很大的缺点，就是计算量太大了。来看softmax的计算公式：\n\n分母部分需要对词汇表中的每个词都计算后求和，一般情况下词汇表都会很大，因此求和操作是相当慢的。那么如何解决呢，下面就来讲一讲。\n6.2.1 分级softmax 分类器\n在一些文献中你会看到hierarchical softmax classider。什么意思呢？也就是在sofmax层不一次到位求出每个词的概率，而是通过分类的方式，第一个分类器告诉你这个词是在词汇表的5000前还是后，第二个分类器告诉你是在2500前还是后，以此类推，直到找到那个准确的词。额，像不像我们平时玩的猜数字游戏，一个人先在纸上写好一个数字，然后开始让大家猜，然后一步一步逼近真实数字，直到猜中的人接受真心话大冒险（别告诉我你没玩过，那你不是70后就是00后。。我们有代沟）。\n画出来的形状是树状的，每个节点是一个分类器：\n\n这就是分级softmax 分类器\n7.负采样Nagtive sampling\n上面一节讲述了用分级softmax 分类器去降低softmax层的计算复杂度，这一节讲述一个更好的方法，，叫做负采样，来一起看看吧～\n7.1 过程详述\n（1）准备样本\n还是这句话：\n\n和上一节一样，随机采出一个词作为context词，再在给定的词距下随机获取该词的target词，形成一组样本：orange--\u003ejuice。这组样本是一个正样本。\n有了正样本，就肯定需要负样本，负样本是这样得到的：context词不变，然后随机从词典中采样出k个词，这些词可以是句子中没有的词，也允许是句子中有的词，总之随缘就好，不强求，于是context词就与这些随机从词典中采样的词形成了几对负样本：\n\n关于k的数目，如果你的数据集很小，那么k在5-20之间，如果你有大数据集，那么k在2-5之间\n（2）训练模型\n有了样本之后，就可以训练一个监督模型了。\n模型的输入x是词对，也就是我们上面准备好的正样本与负样本；模型的输出y是一个二分类，若正样本则为1，负样本则为0。显而易见，这个模型对目的是去学习两个词是否是临近词，临近词为正样本，非临近词为负样本。\n这样的二分类我们选择用逻辑回归模型去构造：\n\n公式里有两个参数，一个是目标词的参数向量θ(t)，一个是上下文词e(c),即每个contxt word的词嵌入向量。利用上面公示预测处t,c共现时(y=1)时的概率\n纵观整个神经网络模型，前面的套路不变：\n（1）输入context word的one-hot向量\n（2）乘以嵌入矩阵E\n（3）得到context word的词嵌入\n（4）进入神经网络，输出10000维向量（10000是词典长度）\n要注意的是第（4）步，这个输出并不是之前的softmax的10000个概率，而是10000个逻辑回归二分类器，表示词典中每个索引上的词是否与context临近。\n但！并不是每次训练都要训练全部10000个逻辑回归，我们只训练其中5个，分别是，那个正样本的target词所在位置的逻辑回归模型，和另外四个采样的负样本所在位置的模型（假设我们设置了k=4）。\n如此以来，原来复杂的要计算10000次的softmax层变成了计算相对简单的10000个逻辑回归二分类模型，且每次训练只需要训练k+1个logistic unit，是不是大大减小了计算量呢～\n7.2如何选取负样本\n那么如何进行更优的负采样呢？\n论文的作者Mikoolov等人根据经验认为根据一下经验值采样会更好：\n\nwi表示第i个词，f(wi)表示第i个词在所有语料中的词频。但是这是针对英文单词的分布的，中文的不知道适不适用呢～\n8.GloVe词向量\n前面讲了word2vec算法进行词嵌入的学习，这一节将介绍另一种也表现很好且更简单的算法：GloVe算法（global vectors for word representatiom）。虽然它并没有word2vec那么火，但是也有人热衷于它。\n又是这句话：\n\nword2vec中获取了词对：context–\u003etarget。在glove中使词对的关系明确化。\nX_ij表示词i出现在j的上下文的次数，这里用ij来表示tc，因此X_ij等同于X_tc（t表示target, c表示context)。\n实际上，X_ij也经常与X_ji对称，比如当你将窗口设定为前后10个词时。\n也就是说word2vec中判断的是两个是否相邻，GloVe关注的是两个词相邻出现对次数\n因此，GloVe model的具体做法是酱紫的：\n其目标函数是最小化以下公式：\n\nθ(i)和e(j)分别表示target word与context word的词嵌入向量；\nf(xij)是一个权重项，对于像the, a, an,of等停用词会给予较小权重，对于durain这种稀有词但有蛮重要点词给予增加权重；\nlog(Xij)表示的是i词与j词的相似程度。\n由于此处是i和j是对称的，因此最终词嵌入e(w)_final可以是θ(w)与e(w)的均值\n9.情感分类Sentiment classification\n情感分类是指对一个文本（一篇文章，新闻，微博评论等等）预测出笔者对所描述的东西的情感是正向的还是负向的（喜欢还是讨厌），是NLP中一个应用很普遍，业务需求很旺盛的一个部分。在没有词嵌入向量之前，我们需要标注大量的数据去训练这个有监督的分类模型，但是现在有了词嵌入后，需要的样本量就大大减少了哦～\n来感受一下情感分类算法点过程吧。\n9.1 输入与输出\n首先明确输入输出\n首先输入是一段文本；输出是要预测的相应的情感，可以是正负的二分类，也可以是评级的多分类（比如影评和淘宝评价又5个等级）\n9.2简单的模型\na.先来说说一个简单的模型。 此时假设我们已经训练好了一个优先的嵌入矩阵E， b.将输入文本中的每个词都在E中找到对应的词嵌入e，可以用上文介绍过的方法，即用该词的one-hot词向量去乘以E。\nc.将所有词嵌入求均值或者加和，将n个向量变成一个向量。这里n的大小其实就文本中词的个数假设词向量是300维的，那么最终求均值或和之后，就生成一个新的300维的向量。 d.将这个300维的向量做为神经网络的输入（即输入层又300个神经元），经过一个softmax分类层，输出情感的分类，若又5类情感，则输出层又5个神经元。\n\n但这个模型有巨大的缺点，就是不考虑词的顺序，假设评论如下：\n“completely lacking in good taste, good service, and good ambience”\n这句话说是lacking， 但是却又3个good,因此直接将词向量均值或求和，就会认为是good,并没有捕捉到前面说的是lacking good XXX.\n9.3 RNN for sentiment classification\n要捕捉顺序上的信息，此时果断就需要RNN来闪亮登场了！\n同样是输入每个词的词嵌入向量，在最后一个时刻的输出情感的分类结果，具体结构如下：\n\n因为RNN的详细教程在前边的笔记中已经讲， 因此此处不在对以上结构做过多解释。\n10.词嵌入除偏Debiasing word embeddings\n10.1 什么偏？\n这里，除偏的偏，不是机器学习里技术上的bias,而是偏见的偏，话不多说，举几个例子你就懂。\n\n比如上面这句，男人之于程序员 就像 女人之于家庭主妇\n中国同胞们肯定就疑惑了，这句话好像没啥偏见啊，而且还挺准的。咳咳，我大中华真是直男成灾啊。你说说凭啥女性就不能做程序员，男性就不能在家带孩子。再咳咳，比如像本人这种以外能代码程序赚钱养家，内能貌美如花贤惠顾家为终极目标的新时代女汉子，就第一个不服这句带有性别歧视的话哈哈哈。\n\n再比如上面这句，父亲之于医生 就像 母亲之于护士\n是的，的确在国内，医生男的居多，护士大多都是女性，因此直男又要反驳这哪里有bais，明明是社会常态啊。\n不同社会下的语料训练出的词向量，会反应当下的性别，种族，年龄等偏见，这与当下的社会，经济，政治，文化状态都相关。也许某些观念在我们骨子里已经根深蒂固，且无法与之相抗，也不能要求整个社会的改变，但至少将被广泛应用于人类社会方方面面的机器学习与人工智能，能杜绝掉被这些传统意识形态的束缚，能真正成功不带任何偏见，没有有色眼镜，公正公平平等得去效力于各行各业的业务场景中。\n10.2 如何除偏\n假设有这样几个词：\n\n第一步：Identify bias direction\n以除去性别偏见为例，将性别词相减，然后再求均值\ne_he - e_she\ne_male - e_femal\n…\n–\u003eaverage\n从上图的分布可见,横轴代表了偏见的方向，给它1个维度；纵轴代表了无偏见的方向，给它299个维度（这里讲述地比实际论文中要简单，论文中的偏见方向不只1维，而且以上也不是简单得求均值，而是用起一只分解的方法。\n第二步：Neutralize\n中和偏见。有一些词本身就有性别上的信息，比如he,she,father,mother;而有一些词本身和性别并无关系，如doctor, nurse, homemaker, computer-programer等，即在性别上是中立等，因此需要对这些词做消除偏见（以上坐标中doctot是在男性那一侧，应该调整到在男性女性中间。\n第三步：Equalze pair\n均衡。比如使得babysiter能够到gramdfather和grandermother的距离一样近。做法就是将gramdfather和grandermother移动到根据纵轴对称，而将babysiter移动到纵轴上。\n那么如何找出哪些词是中立词呢，论文的作者建立了二分类到监督模型进行中立与非中立的分类。\nEND\n错别字太多请忽略，之后会详细检查与更正到哈～\n欢迎关注王小草的微信公众号，推送大数据，机器学习，深度学习，NLP等原创文章，欢迎交流与指正：","data":"2018年05月05日 17:45:58"}
{"_id":{"$oid":"5d34576662f717dc0659b8ad"},"title":"NLP-自然语言处理入门（持续更新）","author":"苏叶biu","content":"NLP-自然语言处理入门\n1.书籍-理论篇\n吴军老师的的《数学之美》\n《统计自然语言处理(第2版)》（宗成庆）蓝皮版\n《统计学习方法》（李航）\n《自然语言处理简明教程》（冯志伟）\n《自然语言处理综论》（Daniel Jurafsky）\n《自然语言处理的形式模型》（冯志伟）\n2.书籍——实践篇\npython基础教程（翻译版）+python入门博客推荐：廖雪峰的python教程\n《机器学习实战》哈林顿 (Peter Harrington)\n西瓜书《机器学习》（周志华）\n《集体智慧编程》—[美] 西格兰 著，莫映，王开福 译\n《python自然语言处理》—伯德 (Steven Bird)（主要讲NLTK这个包的使用）\n3.视频——辅助篇\n自然语言处理-宗庆成\n自然语言处理-关毅\n计算语言学概论_侯敏\n计算语言学_冯志伟\n语法分析_陆俭明\n哥伦比亚大学https://class.coursera.org/nlan+他人的博客自然语言处理大菜鸟\nmooc学院-机器学习-大牛Andrew Ng\n网易公开课-机器学习-Andrew Ng\n慕课网-初识机器学习\n台湾大学林轩田机器学习\n斯坦福的nlp课程Video Listing\n4.优秀参考博客\n我爱自然语言处理专门记录nlp的\n北京大学中文系 应用语言学专业\n5.国际学术组织、学术会议与学术论文\n国际机器学习会议（ICML）\nACL，URL：http://aclweb.org/\n国际神经信息处理系统会议（NIPS）\n国际学习理论会议（COLT）\n欧洲机器学习会议（ECML）\n亚洲机器学习会议（ACML）\nEMNLP：http://emnlp2017.net/ 丹麦哥本哈根 9.7-9.11\nCCKS http://www.ccks2017.com/index.php/att/ 成都 8月26-8月29\nSMP http://www.cips-smp.org/smp2017/ 北京 9.14-9.17\nCCL http://www.cips-cl.org:8080/CCL2017/home.html 南京 10.13-10.15\nNLPCC http://tcci.ccf.org.cn/conference/2017/ 大连 11.8-11.12\nNCMMSC http://www.ncmmsc2017.org/index.html 连云港 11.11 － 11.13\n6.知名国际学术期刊\nJournal of Machine Learning Research\nComputational Linguistics（URL：http://www.mitpressjournals.org/loi/coli）\nTACL，URL：http://www.transacl.org/\nMachine Learning\nIJCAI\nAAAI\nArtificial Intelligence\nJournal of Artificial Intelligence Research\n7.工具包推荐\n中文的显然是哈工大开源的那个工具包 LTP (Language Technology Platform) developed by HIT-SCIR(哈尔滨工业大学社会计算与信息检索研究中心).\n英文的(python)：\npattern - simpler to get started than NLTK\nchardet - character encoding detection\npyenchant - easy access to dictionaries\nscikit-learn - has support for text classification\nunidecode - because ascii is much easier to deal with\n8.Quora上推荐的NLP的论文\nParsing（句法结构分析~语言学知识多，会比较枯燥）\nKlein \u0026 Manning: \"Accurate Unlexicalized Parsing\" (克莱因与曼宁：“精确非词汇化句法分析” )\nKlein \u0026 Manning: \"Corpus-Based Induction of Syntactic Structure: Models of Dependency and Constituency\" (革命性的用非监督学习的方法做了parser)\nNivre \"Deterministic Dependency Parsing of English Text\" (shows that deterministic parsing actually works quite well)\nMcDonald et al. \"Non-Projective Dependency Parsing using Spanning-Tree Algorithms\" (the other main method of dependency parsing, MST parsing)\nMachine Translation（机器翻译，如果不做机器翻译就可以跳过了，不过翻译模型在其他领域也有应用）\nKnight \"A statistical MT tutorial workbook\" (easy to understand, use instead of the original Brown paper)\nOch \"The Alignment-Template Approach to Statistical Machine Translation\" (foundations of phrase based systems)\nWu \"Inversion Transduction Grammars and the Bilingual Parsing of Parallel Corpora\" (arguably the first realistic method for biparsing, which is used in many systems)\nChiang \"Hierarchical Phrase-Based Translation\" (significantly improves accuracy by allowing for gappy phrases)\nLanguage Modeling (语言模型)\nGoodman \"A bit of progress in language modeling\" (describes just about everything related to n-gram language models 这是一个survey，这个survey写了几乎所有和n-gram有关的东西，包括平滑 聚类)\nTeh \"A Bayesian interpretation of Interpolated Kneser-Ney\" (shows how to get state-of-the art accuracy in a Bayesian framework, opening the path for other applications)\nMachine Learning for NLP\nSutton \u0026 McCallum \"An introduction to conditional random fields for relational learning\" (CRF实在是在NLP中太好用了！！！！！而且我们大家都知道有很多现成的tool实现这个，而这个就是一个很简单的论文讲述CRF的，不过其实还是蛮数学= =。。。)\nKnight \"Bayesian Inference with Tears\" (explains the general idea of bayesian techniques quite well)\nBerg-Kirkpatrick et al. \"Painless Unsupervised Learning with Features\" (this is from this year and thus a bit of a gamble, but this has the potential to bring the power of discriminative methods to unsupervised learning)\nInformation Extraction\nHearst. Automatic Acquisition of Hyponyms from Large Text Corpora. COLING 1992. (The very first paper for all the bootstrapping methods for NLP. It is a hypothetical work in a sense that it doesn't give experimental results, but it influenced it's followers a lot.)\nCollins and Singer. Unsupervised Models for Named Entity Classification. EMNLP 1999. (It applies several variants of co-training like IE methods to NER task and gives the motivation why they did so. Students can learn the logic from this work for writing a good research paper in NLP.)\nComputational Semantics\nGildea and Jurafsky. Automatic Labeling of Semantic Roles. Computational Linguistics 2002. (It opened up the trends in NLP for semantic role labeling, followed by several CoNLL shared tasks dedicated for SRL. It shows how linguistics and engineering can collaborate with each other. It has a shorter version in ACL 2000.)\nPantel and Lin. Discovering Word Senses from Text. KDD 2002. (Supervised WSD has been explored a lot in the early 00's thanks to the senseval workshop, but a few system actually benefits from WSD because manually crafted sense mappings are hard to obtain. These days we see a lot of evidence that unsupervised clustering improves NLP tasks such as NER, parsing, SRL, etc,","data":"2018年08月13日 00:44:24"}
{"_id":{"$oid":"5d3457e662f717dc0659b8c2"},"title":"自然语言处理（NLP）解决方案","author":"SZ laoluo","content":"目录\n自然语言处理（NLP）应用程序的示例\n技术资产支持自然语言处理（NLP）\n端到端自然语言处理（NLP）解决方案\n在当今的企业世界中，单独分析结构化数据已经不足以进行复杂的业务分析，预测和决策。非结构化内容（自然语言通信），例如电子邮件，社交媒体，视频，客户评论等，可以帮助发现巨大的洞察力。通过自然语言处理（NLP）解决方案，您的组织可以更深入地了解非结构化或半结构化内容，从而提供增强的BI和分析。\n自然语言处理（NLP）应用程序的示例\n问答系统 - 增强企业中的语义搜索并将员工连接到业务数据，图表，信息和资源\n商务聊天机器人和客户支持应用程序 - 回答问题，指导用户使用手册或产品，以及自动回复/重新路由请求\n电子商务 - 提高搜索相关性，提供有针对性的响应，并根据查询意图提供个性化结果\nPhamacovigilance - 从ADR中提取实体（药物不良反应）报告，以发现见解并减少手动操作\n客户支持 - 自动处理大量支持请求，降低成本并增加追加销售机会\n供应商/法律合同分析 - 识别差距或违约\n留置权/贷款合同事实提取 - 取代手动流程并获得更好的贷款信息以实现更好的目标定位\n招聘 - 自动将工作与候选人匹配，以提高填充率并缩短填写时间\n石油和天然气 - 从高度非结构化的日常钻井报告（DDRs）中提取信息\n技术资产支持自然语言处理（NLP）\n洞察力驱动的企业越来越多地寻求利用庞大的非结构化数据来加速和改善业务成果。但是现有的自然语言处理技术并不能满足企业的需求 - 它们太狭隘（聊天机器人），太浅薄和通用（基于云的自然语言处理解决方案），或者开发，部署和维护成本太高。\n作为技术资产收集的一部分，Saga Natural Language Understanding（NLU）是一个可扩展，经济高效且易于使用的框架，填补了现有NLP / NLU技术的空白。了解有关Saga的更多信息并请求演示。\n端到端自然语言处理（NLP）解决方案\n我们的自然语言处理解决方案涵盖了一系列需求。无论您是要解决非结构化内容处理挑战还是开发自定义NLP解决方案，我们都可以与您合作：\n完整的解决方案：项目规划，架构设计，实施以及对NLP驱动的应用程序的支持\n数据采集 ：使用我们的预构建和定制连接器获取非结构化或半结构化数据\n原始语言处理和统计语言处理：为搜索和分析提供最高质量的结果\n文本分析解决方案：文本挖掘，文本提取，实体提取，内容分类，内容聚类，事实提取和关系提取\n查询理解：用于搜索相关性和个性化","data":"2019年03月26日 15:25:55","date":"2019年03月26日 15:25:55"}
{"_id":{"$oid":"5d34583962f717dc0659b8d3"},"title":"自然语言处理之知识图谱","author":"zourzh123","content":"1. 引言\n最早接触知识图谱是在一篇分析人工智能的文章，文章提出一个很有意思的观点：“在感知层面，人工智能进步很大，在更高级的认知层面，我们现在了解的仍然很少。”　我对这句话的粗浅理解是，人工智能在学习数据的内在表示（无监督学习），或者对数据的输出结果判别方面表现出了强大的能力，甚至在计算机视觉、语音识别、机器翻译等方面接近或超过人类的表现水平，但这些都还停留在对数据内容的归纳和感知层面，对于需要复杂背景知识和前后上下文的认知和推理层面了解仍然不够，例如我有一堆数据，我想让机器自己学习和推理出正确的知识，以及知识和知识的联系。当然知识图谱也知识在认知计算领域走出了一步，远未达到人们对认知的期望。\n具体到知识图谱，简单理解就是一个知识库，我们能利用这个知识库，给定你要查询的内容，然后到知识库中去进行关联分析和推理，试图让机器了解你的意图，反馈和你查询相关内容的更多关联信息。举一个简单例子，我们用所有的菜谱构建知识图谱，然后问“夏天西红柿怎么做汤”，知识图谱会查询“夏天”、“‘西红柿”和“汤”在所有菜谱中的直接和间接关系，进而推荐给你几个最匹配的菜谱。就我的总结，知识图谱有两大类主要应用：a) 搜索和问答类型的场景；b)自然语言理解类的场景。典型的应用场景如下：\n\n\n那知识图谱是怎么表示的呢？大多数知识图谱用RDF(Resource Description Framework)表示，RDF表征了实体和实体的关系，这种关系有两种：一种是属性关系，即一个实体是另一个实体的属性；另一种是外部关系，表明两个实体之间存在外部关联。。RDF形式上表示为SPO（Subject Predicate Object）三元组，所以实体通过关系链接成无向的网络。例如：\n\n\n2. 知识图谱的架构体系\n可以用知名的知识图谱平台PlantData为例，介绍知识图谱的架构体系：\n\n\n从图中我们可以看出知识图谱的体系分成４个过程：数据采集、知识抽取、知识链接和融合、知识的应用。\n首先说数据采集，构建知识图谱是以大量的数据为基础的，需要进行大规模的数据采集，采集的数据来源一般是：网络上的公开数据、学术领域的已整理的开放数据、商业领域的共享和合作数据，这些数据可能是结构化的、半结构化的或者非结构化的，数据采集器要适应不同类型的数据。\n知识抽取是对数据进行粗加工，将数据提取成实体－关系三元组，根据数据所在的问题领域，抽取方法分成开放支持抽取和专有领域知识抽取。\n知识链接和融合，由于表征知识的实体－关系三元组抽取自不同来源的数据，可能不同的实体可以进一步融合成新的实体，实现在抽象层面的融合；根据融合之后的新实体，三元组集合可以进一步学习和推理，将表达相同或相似含义的不同关系合并成相同关系，检测相同实体对之间的关系冲突等。\n\n知识图谱构建完成之后，形成了一个无向图网络，可以运用一些图论方法进行网络关联分析，将其用于文档、检索以及智能决策等领域。例如，阿里的知识图谱以商品、标准产品、 标准品牌、 标准条码、标准分类为核心， 利用实体识别、实体链指和语义分析技术，整合关联了例如舆情、百科、国家行业标准等9大类一级本体，包含了百亿级别的三元组，形成了巨大的知识网，然后将商品知识图谱广泛地应用于搜索、前端导购、平台治理、智能问答、品牌商运营等核心、创新业务。\n\n3. 知识图谱的构建\n知识图谱的构建有两大类方法：如果知识领域比较贴近开放领域，可以先从网络上找一个开放知识图谱，然后以此为基础进行扩充；如果知识领域只某个专有行业的，例如信息安全领域，则开发知识图谱图谱中可直接使用的知识表示相对较少，需要花更多的精力构建专业的知识图谱，一个典型的工具是Deepdive允许通过机器学习和人工参与的方式不断迭代提升知识图谱。\n不管构建哪一类的知识图谱，都要经历：数据收集、信息抽取、链接和融合数据、数据可视化以及分析等过程。目前中国的知识图谱从业者们建立了一个非常好的开放知识图谱共享网站：OpenKG.CN，网址是：http://www.openkg.cn/，网站的主要内容如下：\n\n\n其中，“数据”栏目里给出了开源知识图谱或者用于构建知识图谱的专业数据集。“工具”栏目里给出了几十种用于自然语言处理、知识抽取、知识存储、知识表示、知识链接、知识推理、知识查询、对话系统等用于构建知识图谱和应用知识图谱的工具。“成员”里列出了参与的科研机构和知识图谱从业企业单位。\n我们可以利用OpenKG.CN里提供的数据集和工具帮助我们构建知识图谱。数据集可以帮助我们建立一个知识图谱的初始版本，即从里面获得初始的知识表示：SPO三元组，然后根据我们收集的真实业务数据再进行知识抽取和知识推理。构建知识图谱的前提是收集数据，收集的数据越全面，则可供提取的知识表示越丰富，知识图谱的用处越大。\n3.1 数据收集\n收集数据的方法包括：\na) 收集通用的百科知识，包括百度百科、维基百科等；\nb)收集自然语言处理或者类似OpenKG.CN这类网站提供的公开数据集，例如自然语言处理的语料库、同义词近义词库，OpenKG.CN提供的疾病、菜谱、人物、商品、音乐、企业年报、突发事件、脑科学、中文地理、中医药等领域的数据集；\nc) 业务领域的数据，从业者所在的企业或者机构所能获取的问题领域的数据。\n\n以上数据的规模较大，需要一个大数据平台来支撑数据的收集、存储和查询，例如利用Hadoop系统或者单独的非关系数据库（Redis、Mongodb、Hbase和postgresql等数据库）进行存储。\n\n3.2 知识抽取（生成SPO三元组）\n收集数据之后需要对数据进行处理，这里面最有价值的首先是文本数据，因此要用到自然语言处理，基本的过程是：语言分词、词性标注、命名实体识别、句法分析，更高级写的应用还包括语义依存分析。对于构建知识库而言，自然语言处理的目的是获取命名实体，再根据命名实体和句法分析抽取知识三元组SPO。自然语言处理有两个强大的工具NLTK和Standford NLP，由于Standford NLP提供了开放信息抽取OpenIE功能用于提取三元组SPO，所以使用Standford NLP更贴合知识图谱构建任务，比较麻烦的一点是Standford NLP需要的计算资源和内存较大（推荐内存4GB），启动时间较长，分析效率低于NLTK，不过支持文件列表的输入方式，实现一次多文件输入得到多个文件的输出结果，总体效率还好。当然研究者也开发和共享了更多的知识抽取工具，例如OpenKG.CN里除了Standford NLP还提供了Reverb: 开放三元组抽取、SOFIE: 抽取链接本体及本体间关系、OLLIE：开放三元组知识抽取等工具。\n3.2.1 DeepDive\n以上知识抽取工具有一个共同的缺点是：利用别人训练好的模型、按照给定的模式进行抽取，对于开放领域的知识抽取，可能是足够的。但对于专业领域，例如某个特定行业的知识抽取，可能提供的工具并没有覆盖到该行业领域，此时该工具进行知识抽取的准确率是比较低的，需要一个能够根据你自己收集的业务数据，自适应的更新知识抽取的模型，通过不断迭代的方式逐渐提升知识抽取的准确性，这个迭代过程要允许人工参与。Deepdive是一款被广泛使用的知识抽取开源工具，DeepDive (http://deepdive.stanford.edu/) 是斯坦福大学开发的信息抽取系统，能处理文本、表格、图表、图片等多种格式的无结构数据，从中抽取结构化的信息。系统集成了文件分析、信息提取、信息整合、概率预测等功能。Deepdive在OpenKG.CN上有一个中文的教程：http://openkg1.oss-cn-beijing.aliyuncs.com/478e0087-8dd6-417c-9a49-4ce12f5ec22c/tutorial.pdf\nDeepDive系统的基本输入包括：\n1) 无结构数据，如自然语言文本\n2) 现有知识库或知识图谱中的相关知识\n3) 若干启发式规则\nDeepDive系统的基本输出包括：\n1) 规定形式的结构化知识，可以为关系（实体1，实体2）或者属性（实体，属性值）等形式\n2) 对每一条提取信息的概率预测\nDeepDive系统运行过程中还包括一个重要的迭代环节，即每轮输出生成后，用户需要对运行结果进行错误分析，通过特征调整、更新知识库信息、修改规则等手段干预系统的学习，这样的交互与迭代计算能使得系统的输出不断得到改进。\n\nDeepDive系统架构和工作流程：\n\n\n\nDeepDive主要针对关系抽取，在指定的关系抽取中效果比较理想，在实体确定后可以很好地进行关系抽取。同时也支持中文关系抽取，仅需要引入中文相关的基础处理工具即可(详情参考：http://www.openkg.cn/tool/cn-deepdive)。不足之处在于未提供专门的针对概念、实体和事件抽取的支持，同时需要大量的标注语料支持，并通过人工设置标注规则。\n\n\n\n\n总结一下，知识三元组的抽取，对于开放领域的信息抽取直接使用现有OpenIE工具，对于特定行业领域内的信息抽取，需要使用类似Deepdive这样的工具，在内部集成自然语言处理工具、实体识别工具、实体对之间的关系抽取、人工标注修正错误等步骤。实体识别工具可以直接用资源语言处理领域的命名实体识别NER工具，也可以根据从外部或者人工提取的知识库进行实体匹配，最难做的是实体对之间的关系抽取。Deepdive对实体对之间的关系通过弱监督训练和预测的方法，具体步骤是：\na) 先通过启发式规则的方式标注一部分实体对之间的关系作为监督学习的标记;\nb) 对每个实体对所在的文本进行特征提取生成监督学习的特征向量；\nc) 根据启发式规则的标注对所有已标实体对所在文本的特征向量进行监督学习的训练过程生成预测模型，再根据预测模型预测未标注实体对的关系标签，得到所有候选实体对的关系标签；\nd) 导出所有候选实体对及其关系标签，然后对SPO三元组做人工确认，将人工修改后实体对的关系标记重新导入启发式规则作为监督学习的已标注样本。\n重复以上监督学习的训练、预测过程和人工确认过程，迭代式的实现实体对关系的更新。\n\nDeepdive中的监督学习是一种远程监督学习技术。为了打破有监督学习中人工数据标注的局限性，Mintz等人提出了远程监督（Distant Supervision）算法，该算法的核心思想是将文本与大规模知识图谱进行实体对齐，利用知识图谱已有的实体间关系对文本进行标注。远程监督基于的基本假设是：如果从知识图谱中可获取三元组R（E1，E2）（注：R代表关系，E1、E2代表两个实体），且E1和E2共现与句子S中，则S表达了E1和E2间的关系R，标注为训练正例。\n远程监督算法是目前主流的关系抽取系统广泛采用的方法，也是该领域的研究热点之一。该算法很好地解决了数据标注的规模问题，但它基于的基本假设过强，会引入大量噪音数据，出现 the wrong label problem 的问题，原因是远程监督假设一个实体对只对应一种关系，但实际上实体对间可以同时具有多种关系，如上例中还存在CEO（乔布斯，苹果公司）的关系，实体对间也可能不存在通常定义的某种关系，而仅因为共同涉及了某个话题才在句中共现。\n为了减小 the wrong label problem 的影响，学术界陆续提出了多种改进算法，主要包括：\na) 基于规则的方法：通过对wrong label cases的统计分析，添加规则，将原本获得正例标注的wrong label cases直接标为负例，或通过分值控制，抵消原有的正标注。\nb) 基于图模型的方法：构建因子图（factor graph）等能表征变量间关联的图模型，通过对特征的学习和对特征权重的推算减小wrong label cases对全局的影响。\n\nc) 基于多示例学习（multi-instance learning）的方法：将所有包含（E1，E2）的句子组成一个bag，从每个bag对句子进行筛选来生成训练样本。\n除了Deepdive的关系抽取技术，基于深度学习的关系技术也很流行。两种方法相辅相成，各有优势：DeepDive系统较多依赖于自然语言处理工具和基于上下文的特征进行抽取，在语料规模的选择上更为灵活，能进行有针对性的关系抽取，且能方便地在抽取过程中进行人工检验和干预；而深度学习的方法主要应用了词向量和卷积神经网络，在大规模语料处理和多关系抽取的人物中有明显的优势。\n４. 知识图谱的应用\n4.1 进行图分析\n列举一些我们常用的图算法：\n图遍历：广度优先遍历、深度优先遍历\n最短路径查询： Dijkstra（迪杰斯特拉算法）、Floyd（弗洛伊德算法）\n路径探寻：给定两个或多个节点，发现它们之间的关联关系\n权威节点分析：PageRank算法\n族群发现：最大流算法\n相似节点发现：基于节点属性、关系的相似度算法\n其中，权威节点分析做过社交网络分析的人应该都知道，可以用来做社交网络里的权威人物分析，我们在创投知识图谱中用来做权威投资机构的发现。族群发现算法一般用来在社交网络中主题社区的发现，在这里我们同样可以用来识别企业知识图谱中的派系（阿里系、腾讯系）。相似节点发现应用就更加广泛了，在企业知识图谱中可以做相似企业的发现，这里有个很重要的实际应用场景，可以利用相似企业进行精准的获客营销。\n4.2 基于本体的推理\n基于本体的知识推理应用也非常的多，比如我们在实际场景中的冲突检测。因为不管是手动构建，还是自动构建知识图谱，都会碰到这样一个问题：或者数据来源不同，或者构建的人员不同、方法不同，这就会不可避免的导致一些冲突，这些冲突自身很难直观的去发现，但是可以利用知识图谱里面的冲突检测去发现存在的有矛盾的、有冲突的知识。\n本体推理基本方法包括：\n基于表运算及改进的方法：FaCT++、Racer、 Pellet Hermit等\n基于一阶查询重写的方法（Ontology based data access，基于本体的数据访问）\n基于产生式规则的算法（如rete）：Jena 、Sesame、OWLIM等\n基于Datalog转换的方法如KAON、RDFox等\n回答集程序 Answer set programming\nOpenKG.CN上有一些知识推理的工具，例如：http://www.openkg.cn/tool?tags=%E7%9F%A5%E8%AF%86%E6%8E%A8%E7%90%86\n\n\n4.3 基于规则的推理\n基于规则的推理是在知识图谱基础知识的基础上，专家依据行业应用的业务特征进行规则的定义，这在业务应用中是非常常见的。基于规则的推理是在知识图谱基础知识的基础上，专家依据行业应用的业务特征进行规则的定义，这在业务应用中是非常常见的。介绍一下我们常用的Drools（因被JBOSS收购，现已更名为JBoss Rules），它是为Java量身定制的基于Charles Forgy的RETE算法的规则引擎的实现，使用了OO接口的RETE,使得商业规则有了更自然的表达，其推理的效率也比较高。结合规则引擎工具，基于基础知识与所定义的规则，执行推理过程给出推理结果。\n4.4 可视化辅助决策\n首先介绍两个比较常见的可视化工具D3.js和ECharts。D3.js全称Data-Driven Documents，是一个用动态图形显示数据的JavaScript库，一个数据可视化工具，它提供了各种简单易用的函数，大大方便了数据可视化的工作。\nECharts是一款由百度前端技术部开发的，同样基于Javascript的数据可视化图标库。它提供大量常用的数据可视化图表。对于出入门的知识图谱使用者，推荐两个入门级别的开源的知识图谱展示工具：\na) 知识图谱Demo，Demo的详细介绍：\nhttps://zhuanlan.zhihu.com/p/29332977?group_id=891668221558661120\n\n开源代码网址：\n\nhttps://github.com/Shuang0420/knowledge_graph_demo\n\n生成的图谱展示结果如下：\n\n\n\nb) 农业知识图谱(KG)：农业领域的信息检索，命名实体识别，关系抽取，分类树构建，数据挖掘。\n\n开源代码网址：\nhttps://github.com/qq547276542/Agriculture_KnowledgeGraph\n\n知识图谱的Demo展示网址：\n\nhttp://ecnukg.vicp.io/\n\n展示效果如下：\n\n\n\n更深入的应用或则展示可以参考商业知识图谱平台PlantData，网址是：https://wx.jdcloud.com/shop/shopDetail/HiKnowledge\n\n4.5 问答系统\n这里介绍一个OpenKG.CN的问答demo：基于 REfO 的 KBQA 实现及示例\n网址是：http://www.openkg.cn/tool/refo-kbqa，这是一个基于 Python 模块 REfO 实现的知识库问答初级系统. 该问答系统可以解析输入的自然语言问句生成 SPARQL 查询，进一步请求后台基于 TDB 知识库的 Apache Jena Fuseki 服务, 得到结果。这是一个入门级的例子. 内含介绍此项目的 README.pdf. 方便用户快速把握这个项目的想法. 希望用户体会默认的 3 类 5 个问题. 不同的表述能够用统一的\"对象正则表达式\"匹配得到结果, 进而生成对应 SPARQL 查询语句。\n知识库由大量的三元组组成，并且这些三元组的实体和实体关系都是形式化的语言。给定一个自然语言的问题“Where was Obama born？”　我们面临的第一个挑战，就是如何建立问题到知识库的映射？语义解析KB-QA的思路是通过对自然语言进行语义上的分析，转化成为一种能够让知识库“看懂”的语义表示即逻辑形式（Logic Form），进而通过知识库中的知识，进行推理（Inference）查询（Query），得出最终的答案。KB-QA的详细介绍，可以参考知乎专栏：“揭开知识库问答KB-QA的面纱”。\n由于个人对知识图谱的理解也比较浅显，本文只是记录自己的一些平时整理的知识和经验，方便自己以后查询和深入学习。","data":"2018年07月12日 10:38:29"}
{"_id":{"$oid":"5d34585d62f717dc0659b8d9"},"title":"自然语言处理1 -- 分词","author":"谢杨易","content":"系列文章，请多关注\nTensorflow源码解析1 – 内核架构和源码结构\n带你深入AI（1） - 深度学习模型训练痛点及解决方法\n自然语言处理1 – 分词\n自然语言处理2 – jieba分词用法及原理\n自然语言处理3 – 词性标注\n自然语言处理4 – 句法分析\n自然语言处理5 – 词向量\n自然语言处理6 – 情感分析\n1 概述\n分词是自然语言处理的基础，分词准确度直接决定了后面的词性标注、句法分析、词向量以及文本分析的质量。英文语句使用空格将单词进行分隔，除了某些特定词，如how many，New York等外，大部分情况下不需要考虑分词问题。但中文不同，天然缺少分隔符，需要读者自行分词和断句。故在做中文自然语言处理时，我们需要先进行分词。\n2 中文分词难点\n中文分词不像英文那样，天然有空格作为分隔。而且中文词语组合繁多，分词很容易产生歧义。因此中文分词一直以来都是NLP的一个重点，也是一个难点。难点主要集中在分词标准，切分歧义和未登录词三部分。\n分词标准\n比如人名，有的算法认为姓和名应该分开，有的认为不应该分开。这需要制定一个相对统一的标准。又例如“花草”，有的人认为是一个词，有的人认为应该划分开为两个词“花/草”。某种意义上，中文分词可以说是一个没有明确定义的问题。\n切分歧义\n不同的切分结果会有不同的含义，这又包含如下几种情况\n组合型歧义：分词粒度不同导致的不同切分结果。比如“中华人民共和国”，粗粒度的分词结果为“中华人民共和国”，细粒度的分词结果为“中华/人民/共和国”。这种问题需要根据使用场景来选择。在文本分类，情感分析等文本分析场景下，粗粒度划分较好。而在搜索引擎场景下，为了保证recall，细粒度的划分则较好。jieba分词可以根据用户选择的模式，输出粗粒度或者细粒度的分词结果，十分灵活。\n另外，有时候汉字串AB中，AB A B可以同时成词，这个时候也容易产生组合型歧义。比如“他/将/来/网商银行”，“他/将来/想/应聘/网商银行”。这需要通过整句话来区分。\n组合型歧义描述的是AB A B均可以同时成词的汉字串，它是可以预测的，故也有专家称之为“固有型歧义”\n交集型歧义：不同切分结果共用相同的字，前后组合的不同导致不同的切分结果。比如“商务处女干事”，可以划分为“商务处/女干事”，也可以划分为“商务/处女/干事”。这也需要通过整句话来区分。交集型歧义前后组合，变化很多，难以预测，故也有专家称之为“偶发型歧义”。\n真歧义：本身语法或语义没有问题，即使人工切分也会产生歧义。比如“下雨天留客天天留人不留”，可以划分为“下雨天/留客天/天留/人不留”，也可以划分为“下雨天/留客天/天留人不/留”。此时通过整句话还没法切分，只能通过上下文语境来进行切分。如果是不想留客，则切分为前一个。否则切分为后一个。\n有专家统计过，中文文本中的切分歧义出现频次为1.2次/100汉字，其中交集型歧义和组合型歧义占比为12：1。而对于真歧义，一般出现的概率不大。\n未登录词\n也叫新词发现，或者生词，未被词典收录的词。未登录词分为如下几种类型\n新出现的词汇，比如一些网络热词，如“超女”“给力”等\n专有名词，主要是人名 地名 组织机构，比如“南苏丹”“特朗普” “花呗”“借呗”等。\n专业名词和研究领域词语，比如“苏丹红” “禽流感”\n其他专有名词，比如新出现的电影名、产品名、书籍名等。\n未登录词对于分词精度的影响远远超过歧义切分。未登录词识别难度也很大，主要原因有\n未登录词增长速度往往比词典更新速度快很多，因此很难利用更新词典的方式解决未登录词问题。不过词典越大越全，分词精度也会越高。因此一个大而全的词典还是相当重要的。\n未登录词都是由普通词汇构成，长度不定，也没有明显的边界标志词\n未登录词还有可能与上下文中的其他词汇构成交集型歧义。\n未登录词中还有可能夹杂着英语字母等其他符号，这也带来了很大难度。比如“e租宝”。\n对于词典中不包含的未登录词，我们无法基于字符串匹配来进行识别。此时基于统计的分词算法就可以大显身手了，jieba分词采用了HMM隐马尔科夫模型和viterbi算法来解决未登录词问题。下一篇文章我们会详细分析这个算法过程。\n3 中文分词算法\n当前的分词算法主要分为两类，基于词典的规则匹配方法，和基于统计的机器学习方法。\n基于词典的分词算法\n基于词典的分词算法，本质上就是字符串匹配。将待匹配的字符串基于一定的算法策略，和一个足够大的词典进行字符串匹配，如果匹配命中，则可以分词。根据不同的匹配策略，又分为正向最大匹配法，逆向最大匹配法，双向匹配分词，全切分路径选择等。\n__最大匹配法__主要分为三种：\n正向最大匹配法，从左到右对语句进行匹配，匹配的词越长越好。比如“商务处女干事”，划分为“商务处/女干事”，而不是“商务/处女/干事”。这种方式切分会有歧义问题出现，比如“结婚和尚未结婚的同事”，会被划分为“结婚/和尚/未/结婚/的/同事”。\n逆向最大匹配法，从右到左对语句进行匹配，同样也是匹配的词越长越好。比如“他从东经过我家”，划分为“他/从/东/经过/我家”。这种方式同样也会有歧义问题，比如“他们昨日本应该回来”，会被划分为“他们/昨/日本/应该/回来”。\n双向匹配分词，则同时采用正向最大匹配和逆向最大匹配，选择二者分词结果中词数较少者。但这种方式同样会产生歧义问题，比如“他将来上海”，会被划分为“他/将来/上海”。由此可见，词数少也不一定划分就正确。\n全切分路径选择，将所有可能的切分结果全部列出来，从中选择最佳的切分路径。分为两种选择方法\nn最短路径方法。将所有的切分结果组成有向无环图，切词结果作为节点，词和词之间的边赋予权重，找到权重和最小的路径即为最终结果。比如可以通过词频作为权重，找到一条总词频最大的路径即可认为是最佳路径。\nn元语法模型。同样采用n最短路径，只不过路径构成时会考虑词的上下文关系。一元表示考虑词的前后一个词，二元则表示考虑词的前后两个词。然后根据语料库的统计结果，找到概率最大的路径。\n基于统计的分词算法\n基于统计的分词算法，本质上是一个序列标注问题。我们将语句中的字，按照他们在词中的位置进行标注。标注主要有：B（词开始的一个字），E（词最后一个字），M（词中间的字，可能多个），S（一个字表示的词）。例如“网商银行是蚂蚁金服微贷事业部的最重要产品”，标注后结果为“BMMESBMMEBMMMESBMEBE”，对应的分词结果为“网商银行/是/蚂蚁金服/微贷事业部/的/最重要/产品”。\n我们基于统计分析方法，得到序列标注结果，就可以得到分词结果了。这类算法基于机器学习或者现在火热的深度学习，主要有HMM，CRF，SVM，以及深度学习等。\nHMM，隐马尔科夫模型。隐马尔科夫模型在机器学习中应用十分广泛，它包含观测序列和隐藏序列两部分。对应到NLP中，我们的语句是观测序列，而序列标注结果是隐藏序列。任何一个HMM都可以由一个五元组来描述：观测序列，隐藏序列，隐藏态起始概率，隐藏态之间转换概率（转移概率），隐藏态表现为观测值的概率（发射概率）。其中起始概率，转移概率和发射概率可以通过大规模语料统计来得到。从隐藏态初始状态出发，计算下一个隐藏态的概率，并依次计算后面所有的隐藏态转移概率。我们的序列标注问题就转化为了求解概率最大的隐藏状态序列问题。jieba分词中使用HMM模型来处理未登录词问题，并利用viterbi算法来计算观测序列（语句）最可能的隐藏序列（BEMS标注序列）。\nCRF，条件随机场。也可以描述输入序列和输出序列之间关系。只不过它是基于条件概率来描述模型的。详细的这儿就不展开了。\n深度学习。将语句作为输入，分词结果作为标注，可以进行有监督学习。训练生成模型，从而对未知语句进行预测。\n4 分词质量和性能\n中文分词对于自然语言处理至关重要，评价一个分词引擎性能的指标主要有分词准确度和分词速度两方面。分词准确度直接影响后续的词性标注，句法分析，文本分析等环节。分词速度则对自然语言处理的实时性影响很大。下图为几种常用分词引擎在准确度和速度方面的对比。\n由上可见，想要做准确度很高的通用型分词引擎是多么的困难。如果对准确度要求很高，可以尝试开发特定领域的分词引擎。比如专门针对金融领域。同时从图中可见，作为一款开源的通用型分词引擎，jieba分词的准确度和速度都还是不错的。后面我们会详细讲解jieba分词的用法及其原理。\n5 总结\n中文分词是中文自然语言处理中的一个重要环节，为后面的词向量编码，词性标注，句法分析以及文本分析打下了坚实的基础。同时，由于中文缺少空格等分隔符，并且汉字间的组合特别多，很容易产生歧义，这些都加大了中文分词的难度。基于词典的字符串匹配算法和基于统计的分词算法，二者各有优缺点，我们可以考虑结合使用。随着深度学习的兴起，我们可以考虑利用深度学习来进行序列标注和中文分词。\n系列文章，请多关注\nTensorflow源码解析1 – 内核架构和源码结构\n带你深入AI（1） - 深度学习模型训练痛点及解决方法\n自然语言处理1 – 分词\n自然语言处理2 – jieba分词用法及原理\n自然语言处理3 – 词性标注\n自然语言处理4 – 句法分析\n自然语言处理5 – 词向量\n自然语言处理6 – 情感分析","data":"2018年08月14日 19:33:26"}
{"_id":{"$oid":"5d3458a762f717dc0659b8ec"},"title":"自然语言处理（NLP）的基本原理及应用","author":"inter_peng","content":"本文由Markdown语法编辑器编辑完成。\n自然语言处理要解决的主要问题有：\n（1）垃圾邮件识别\n（2）中文输入法\n（3）机器翻译\n（4）自动问答、客服机器人\n这里简单罗列了一些NLP的常见领域：分词，词性标注，命名实体识别，句法分析，语义识别，垃圾邮件识别，拼写纠错，词义消歧，语音识别，音字转换,机器翻译，自动问答……\n腾讯文智中文语义平台：\n平台链接地址：http://nlp.qq.com/index.cgi。\n参考链接：\n1. 从破译外星人文字浅谈自然语言处理的基础\nhttp://blog.csdn.net/han_xiaoyang/article/details/50545650\n2. 腾讯文智中文语义平台\nhttp://nlp.qq.com/index.cgi","data":"2016年12月03日 00:05:08","date":"2016年12月03日 00:05:08"}
{"_id":{"$oid":"5d3458cd62f717dc0659b8f0"},"title":"「自然语言处理」如何快速理解？有这篇文章就够了！","author":"人工智能学家","content":"原文来源：codeburst.io\n作者：Pramod Chandrayan\n「雷克世界」编译：嗯~阿童木呀、我是卡布达\n\n\n现如今，在更多情况下，我们是以比特和字节为生，而不是依靠交换情感。我们使用一种称之为计算机的超级智能机器在互联网上进行交易和沟通。因此，我们觉得有必要让机器明白我们在说话时是如何对其进行理解的，并且试图用人工智能，一种称之为NLP——自然语言处理技术为它们提供语言。作为一种研究结果，聊天机器人正在成为一种可靠的聊天工具，使用这种非人为依赖的智能工具与人类进行交流。\n我强烈的感受到：\n\n\n直到我们的机器学会了解行为和情绪，数据科学家和工程师的工作才完成了一半。与深度学习（ML学科领域）融合的NLP将对这种计算机语言的使用起到关键作用。\n\n\n什么是NLP\n\n\n这是一种人工智能方法，给定机器一些人类语言从而使得它们能够与人类进行沟通交流。它涉及使用NLP技术对书面语言进行智能分析，以获取对一组文本数据的见解，如：\n\n\n1.情绪分析\n\n\n2.信息提取和检索\n\n\n3.智能搜索等\n\n\n它是人工智能和计算语言学的交汇点，能够处理机器和人类自然语言之间的交互，即计算机需要对其进行分析、理解、改变或生成自然语言。NLP帮助计算机机器以各种形式使用自然人类语言进行交流，包括但不限于语音、印刷、写作和签名。\n\n\nNLP机器学习和深度学习：它们是如何连接的\n\n\n\n\nNLP与机器学习和深度学习密切相关，所有这些都是人工智能领域的分支，如下图所示：它是一个致力于使机器智能化的计算机科学领域。深度学习是一种流行的机器学习技术之一，如回归，K-means等。\n\n\n机器学习的类型很多，像无监督机器学习这样的经常用于NLP技术中，如LDA（潜在狄利克雷分布，一种主题模型算法）。\n\n\n为了能够执行任何一个NLP，我们需要深入理解人类使如何处理语言的情感和分析方面。还有各种各样像社交媒体这样的语言数据源，人们直接或间接地分享他们感受到的内容，而这必须通过使用NLP的机器进行智能分析。NLP机器需要建立一个人类推理系统，借助ML技术，它们可以自动执行NLP过程并对其进行扩展。\n\n\n简而言之，“深度学习与自然语言处理”是相互联系、相互依存的，以构建一个能够像人类一样思考、说话和行动的智能计算机。\n\n\nMeltwater Group的NLP专家John Rehling在《自然语言处理是如何帮助揭示社交媒体情绪》一文中说，\n\n\n“通过分析语言的含义，NLP系统扮演着非常重要的角色，如纠正语法，将语音转换为文本，以及在多语言之间自动翻译。”\n\n\nNLP如何工作\n\n\n理解NLP的工作原理是非常重要的，因为这样的话，我们就可以将NLP作为一个整体来理解。NLP一般有两个主要组成部分：\n\n\n1.NLU：自然语言理解\n\n\n2.NLG：自然语言生成\n\n\n让我们深入理解NLU\n\n\n自然语言理解：它涉及的是一种方法论，试图了解如何对馈送给计算机的自然语言赋予一定的相关意义。\n\n\n在开始时，计算机获得自然语言的输入（自然语言可以是任何语言，它们通过使用和重复在人类中自然进化，而不是有意识的计划或预谋，自然语言可以采用不同的形式，例如语音或签名）。\n\n\n计算机之后将它们转换成人工语言，如语音识别和/或语音转换文本。在这里我们把数据转换成一个文本形式， NLU过程来理解其中的含义。\n\n\nHMM：隐马尔可夫模型（NLU示例）\n\n\n来源：wikipedia\n\n\n它是一种统计语音识别模型，它可以在预先构建的数学技术的帮助下，将你的语音转换成文本，并试图推断出你所说的语言。\n\n\n它试图理解你所说的，通过将语音数据分解成一小段特定的时间段，大多数情况下时间是20-20 ms。这些数据集将进一步与预馈语音进行比较，从而进一步解读你在每个语音单位中所说的内容。这里的目的是找到音素（一个最小的语音单位）。然后，机器对一系列这样的音素进行观察，并统计了最可能说出的单词和句子。\n\n\n不仅如此，NLU会深刻理解每个单词，试图理解它是一个名词还是动词，什么是时态（过去或未来）等。这个过程被定义为POS：词性标注部分（Part Of Speech Tagging）。NLP具有内置的词典和一套与语法预编码相关的协议，这些协议被预编码到它们的系统中，并在处理自然语言数据集时使用它，从而在NLP系统处理人类语音时，编译所说的内容。\n\n\nNLP系统也有一个词典（词汇表）和一套编码到系统中的语法规则。现代NLP算法使用统计机器，学习将这些规则应用于自然语言，并推断所说话语背后最可能的含义。在考虑诸如具有多个含义的词语（多义词）或具有相似含义的词语（同义词）时，存在一些挑战，但软件开发者在他们的NLU系统中建立了自己的规则，可以通过适当的训练和学习来处理这类问题。\n\n\n自然语言生成：\n\n\n与第一阶段（NLU做了大量的努力以理解人类的话语）相比，NLG可以很容易的进行翻译工作，即将计算机的人工语言翻译为有意义的文本，并可以通过文字转语音（tex-to-speech）技术将其转化为可听语音。文本转语音（（tex-to-speech））技术通过韵律模型（prosody model）来分析文本，从而确定语言的断句、长短和音调。然后，利用语音数据库，将记录的所有音素汇集在一起，形成一个连贯的语音串。\n\n\n简而言之，NLP采用NLU和NLG来处理人类自然语言，尤其是处理语音识别领域的人类自然语言，并试图将传递字符串或可听语言作为输出，来理解、编译并推断所说的内容。\n\n\nNLP在现代语境中的应用：\n\n\n在这个处于数字革命的电脑时代中，大部分任务需要由人类利用链接物联网的机器来完成。NLP在为媒体、出版、广告、医疗、银行和保险等行业领域建立强大的软件工具方面，发挥了重要作用，从而帮助他们高效快捷地运作。\n\n\nNLP的一些现代用法：\n\n\n1.聊天机器人\n\n\n这是一个被称为机器人的成熟软件，它可以处理任何场景的人物对话。api.ai、微软语音理解智能服务（LUIS）等一些热门的NLP和机器学习平台，可用于研发你的商业聊天机器人。\n\n\n\n\n2.垃圾邮件过滤\n\n\n来源：yhat\n你们中的大多数人一定对垃圾邮件并不陌生。Google使用基于NLP的技术来保障你的收件箱清洁、无垃圾邮件。贝叶斯垃圾邮件过滤（Bayesian spam filtering）是一种备受瞩目的技术，它是一种统计技术，基于此，电子邮件中词语的审核通过率根据其在垃圾和非垃圾邮件语料库中的典型事例来确定。\n\n\n3.机器翻译\n\nNLP被越来越多的应用于机器翻译程序当中，这使得一种语言被自动翻译成另一种语言，谷歌是一个将你的文本翻译为所需语言的先驱者。\n\n\n机器翻译技术所面临的挑战不在于翻译单词，而在于保留句子的含义，这是一个复杂的技术问题，也是NLP的核心。\n\n\n4.命名实体提取（Named entity extraction）\n\n\n它用于从给定的项目集合中分离出具有相似性质和属性的项目。例如名字、姓氏、年龄、地理位置、地址、电话号码、电子邮件地址和公司名称等等。命名实体提取（亦称命名实体识别）使挖掘数据变得更加容易。\n\n\n5.自动汇总\n\n\n自然语言处理可用于从大段文本中提取可读摘要。例如，我们可以自动总结出一份长篇学术文章的简短摘要。\n接下来我们将深入介绍一些NLP的技术细节。\n当自然界与人工相逢的时候，机器就像是一个真正具有生命力的人类一样进入了生活中。\nNLP技术术语\nNLP术语\n•语音体系——关于系统性地组织语音的研究。\n•形态学——这是一个从基本意义单位中进行单词构建的研究。\n•语素——语言中意义的基本单位。\n•语法——它是指单词经过组合排列构成句子，它还涉及在句子和短语中确定单词结构的作用。\n•语义——它涉及的是单词的含义，以及该如何将单词组合成有意义的短语和句子。\n•语用学——它涉及的是在不同情况下使用和理解句子以及对句子的解释是如何受到影响的。\n•话语——它指的是前面的句子如何影响对于下一句的解释的。\n•常识性知识——它涉及的是对于世界的一般性认识。\n自然语言处理库（对于开发者而言）\nNLP库：\n有许多通用的第三方开源库，开发人员可以使用它们来构建基于NLP的Projects Viz .。\n•自然语言工具包（NLTK）\n•Apache OpenNLP\n•斯坦福大学NLP套件\n•Gate NLP库\n自然语言工具包（NLTK）是最通用的自然语言处理（NLP）库。它是用Python编写的，背后有一个很大的社区。\nNLP实施所涉及的步骤：\n来源：mediterra-soft\n\n\n它涵盖了5个主要步骤：\n\n•词法分析——它对给定单词的结构进行识别和分析，其中整个文本数据块在词法分析中被分解成段落、句子和词汇。\n\n\n•解析（句法分析）——它涉及以一种显示单词之间的关系的方式对分析句子中的单词进行语法和单词排列分析，在这个阶段，任何不符合语法正确的句子都被拒绝，例如，“building lives in sita”将不会被语法分析器所接受\n\n\n•语义分析——对给定的文本进行分析以从中提取意义。它通过对任务域中的语法结构和目标进行分析来完成。语义分析器拒绝不相关的句子，如“hot banana”。\n\n\n•话语整合——正如我们所知，每个句子都与前一句话相互联系，基于倒数第二句的意义而言，任何句子都变得有意义。同样，它也使得后一句话变得有意义。\n\n\n•语用分析——在此期间，常识性知识被重新定义了，解释了它们的真实意义到底是什么，它涉及到那些需要常识性知识的语言方面。\n用图片来解读NLP（点击图片放大）：\n\n\n\n\nNLP应用程序：\n\n\n1.光学字符识别\n\n\n2.语音识别\n\n\n3.机器翻译\n\n\n4.自然语言生成\n\n\n5.情绪分析\n\n\n6.语义搜索\n\n\n7.自然语言编程\n\n\n8.情感计算\n\n\n9.开发聊天机器人\n\n\n未来智能实验室致力于研究互联网与人工智能未来发展趋势，观察评估人工智能发展水平，由互联网进化论作者，计算机博士刘锋与中国科学院虚拟经济与数据科学研究中心石勇、刘颖教授创建。\n\n\n未来智能实验室的主要工作包括：建立AI智能系统智商评测体系，开展世界人工智能智商评测；开展互联网（城市）云脑研究计划，构建互联网（城市）云脑技术和企业图谱，为提升企业，行业与城市的智能水平服务。\n如果您对实验室的研究感兴趣，欢迎支持和加入我们。扫描以下二维码或点击本文左下角“阅读原文”","data":"2017年12月16日 00:00:00"}
{"_id":{"$oid":"5d3458ee62f717dc0659b8fd"},"title":"NLP之自然语言处理简述","author":"miner_zhu","content":"什么是自然语言处理？\n自然语言处理是研究在人与人交际中以及人与计算机交际中的语言问题的一门学科。自然语言处理要研制表示语言能力（linguistic competence）和语言应用（linguistic performance）的模型，建立计算框架来实现这样的语言模型，提出相应的方法来不断完善这样的语言模型，根据这样的语言模型设计各种实用系统，并探讨这些实用系统的评测技术。\n根据这个定义，自然语言处理要研究 “在人与人交际中以及人与计算机交际中的语言问题”，既要研究语言，又要研究计算机，因此，它是一门交叉学科，它涉及语言学、计算机科学、数学、自动化等不同学科。\n以宗成庆所著《统计自然语言处理》为例，其在统计自然语言处理的理论方面，首先介绍相关的基础知识，例如概率论和信息论的基本概念、形式语言和自动机的基本概念。由于统计自然语言处理是以语料库和词汇知识库为语言资源的，因此接下来本书讲解了语料库和词汇知识库的基本原理。语言模型和隐马尔可夫模型是统计自然语言处理的基础理论，在统计自然语言处理中具有重要地位。因此本书介绍了语言模型的基本概念，并讨论了各种平滑方法和自适应方法，又介绍了隐马尔可夫模型和参数估计的方法。接着，本书分别论述了在词法分析与词性标注中的统计方法，在句法分析中的统计方法，在词汇语义中的统计方法。\n基于统计的自然语言处理的理论基础是哲学中的经验主义，基于规则的自然原因处理的理论基础是哲学中的理性主义。说到底，这个问题是关于如何处理经验主义和理论主义关系的问题。\n自然语言处理研究的内容？\n机器翻译（machine translation,MT）：实现一种语言到另一种语言的自动翻译\n自动文摘（automatic abstracting）：将文档的主要内容和含义自动归纳、提炼，形成摘要\n信息检索（information retrieval）：从海量文档中找到符合用户需要的相关文档\n文档分类（document categorization/classification）：对大量的文档按照一定的分类标准（例如，根据主题或内容划分等）实现自动归类。\n文档分类也称文本分类（text categorization/classification）或信息分类（information categorization/classification），近年来，情感分类（sentiment classification）或称文本倾向性识别（text orientation identification）成为本领域研究的热点。\n问答系统（question-answering system）：对用户提出的问题的理解，利用自动推理等手段，在有关知识资源中自动求解答案并做出相应的回答。\n信息过滤（information filtering）：自动识别和过滤那些满足特定条件的文档信息。\n信息抽取（information extraction）：指从文本中抽取出特定的事件（event）或事实信息，有时候又称事件抽取（event extraction）。\n信息抽取与信息检索不同，信息抽取直接从自然语言文本中抽取信息框架，一般是用户感兴趣的事实信息，而信息检索主要是从海量文档集合中找到与用户需求（一般通过关键词表达）相关的文档列表，而信息抽取则是希望直接从文本中获得用户感兴趣的事实信息。当然，信息抽取与信息检索也有密切的关系，信息抽取系统通常以信息检索系统（如文本过滤）的输出作为输入，而信息抽取技术又可以用来提高信息检索系统的性能。\n信息抽取与问答系统也有密切的联系。一般而言，信息抽取系统要抽取的信息是明定的、事先规定好的，系统只是将抽取出来的事实信息填充在给定的框架槽里，而问答系统面对的用户问题往往是随机的、不确定的，而且系统需要将问题的答案生成自然语言句子，通过自然、规范的语句准确地表达出来，使系统与用户之间形成一问一答的交互过程。\n文本挖掘（text mining）：从文本（多指网络文本）中获取高质量信息的过程。\n文本挖掘技术一般涉及文本分类、文本聚类（text clustering）、概念或实体抽取（concept/entity extraction）、粒度分类、情感分析（sentiment analysis）、自动文摘和实体关系建模（entity relation modeling）等多种技术。\n舆情分析（public opinion analysis）：舆情是较多群众关于社会中各种现象、问题所表达的信念、态度、意见和情绪等等表现的总和。显然，舆情分析是一项十分复杂、涉及问题众多的综合性技术，它涉及网络文本挖掘、观点（意见）挖掘（opinion mining）等各方面的问题。\n隐喻计算（metaphorical computation）：研究自然语言语句或篇章中隐喻修辞的理解方法。\n文字编辑和自动校对（automatic proofreading）：对文字拼写、用词，甚至语法、文档格式等进行自动检查、校对和编排。\n作文自动评分：对作文质量和写作水平进行自动评价和打分\n语音识别（speech recognition）：将输入的语音信号识别转换成书面语表示。\n文语转换（text-to-speech conversion）：将书面文本自动转换成对应的语音表征，又称语音合成（speech synthesis）。\n说话人识别／认证／验证（speaker recognition/identification/verification）：对说话人的言语样本做声学分析，依此推断（确定或验证）说话人的身份。\n自然语言处理涉及的几个层次？\n如果撇开语音学研究的层面，自然语言处理研究的问题一般会涉及自然语言的形态学、语法学、语义学和语用学等几个层次。\n形态学（morphology）：形态学（又称“词汇形态学”或“词法”）是语言学的一个分支，研究词的内部结构，包括屈折变化和构词法两个部分。由于词具有语音特征、句法特征和语义特征，形态学处于音位学、句法学和语义学的结合部位，所以形态学是每个语言学家都要关注的一门学科。\n语法学（syntax）：研究句子结构成分之间的相互关系和组成句子序列的规则。其关注的中心是：为什么一句话可以这么说，也可以那么说？\n语义学（semantics）：语义学的研究对象是语言的各级单位（词素、词、词组、句子、句子群、整段整篇的话语和文章，乃至整个著作）的意义，以及语义与语音、语法、修辞、文字、语境、哲学思想、社会环境、个人修养的关系，等等。其重点在探明符号与符号所指的对象之间的关系，从而指导人们的言语活动。它所关注的重点是：这个语言单位到底说了什么？\n语用学（pragmatics）：是现代语言学用来指从使用者的角度研究语言，特别是使用者所作的选择、他们在社会互动中所受的制约、他们的语言使用对信递活动中其他参与者的影响。目前还缺乏一种连贯的语用学理论，主要是因为它必须说明的问题是多方面的，包括直指、会话隐含、预设、言语行为、话语结构等。部分原因是由于这一学科的范围太宽泛，因此出现多种不一致的定义。从狭隘的语言学观点看，语用学处理的是语言结构中有形式体现的那些语境。相反，语用学最宽泛的定义是研究语义学未能涵盖的那些意义。因此，语用学可以是集中在句子层次上的语用研究，也可以是超出句子，对语言的实际使用情况的调查研究，甚至与会话分析、语篇分析相结合，研究在不同上下文中的语句应用，以及上下文对语句理解所产生的影响。其关注的重点在于：为什么在特定的上下文中要说这句话？\n在实际问题的研究中，上述几方面的问题，尤其是语义学和语用学的问题往往是相互交织在一起的。语法结构的研究离不开对词汇形态的分析，句子语义的分析也离不开对词汇语义的分析、语法结构和语用的分析，它们之间往往互为前提。\n自然语言处理面临的困难？\n根据上面的介绍，自然语言处理涉及形态学、语法学、语义学和语用学等几个层面的问题，其最终应用目标包括机器翻译、信息检索、问答系统等非常广泛的应用领域。其实，如果进一步归结，实现所有这些应用目标最终需要解决的关键问题就是歧义消解（disambiguation）问题和未知语言现象的处理问题。\n一方面，自然语言中大量存在的歧义现象，无论在词法层次、句法层次，还是在语义层次和语用层次，无论哪类语言单位，其歧义性始终都是困扰人们实现应用目标的一个根本问题。因此，如何面向不同的应用目标，针对不同语言单位的特点，研究歧义消解和未知语言现象的处理策略及实现方法，就成了自然语言处理面临的核心问题。\n另一方面，对于一个特定系统来说，总是有可能遇到未知词汇、未知结构等各种意想不到的情况，而且每一种语言又都随着社会的发展而动态变化着，新的词汇（尤其是一些新的人名、地名、组织机构名和专用词汇）、新的词义、新的词汇用法（新词类），甚至新的句子结构都在不断出现，尤其在口语对话或计算机网络对话、微博、博客等中，稀奇古怪的词语和话语结构更是司空见惯。因此，一个实用的自然语言处理系统必须具有较好的未知语言现象的处理能力，而且有足够的对各种可能输入形式的容错能力，即我们通常所说的系统的鲁棒性（robustness）问题。当然，对于机器翻译、信息检索、文本分类等特定的自然语言处理任务来说，还存在若干与任务相关的其他问题，诸如如何处理不同语言的差异、如何提取文本特征等。\n总而言之，目前的自然语言处理研究面临着若干问题的困扰，既有数学模型不够奏效、有些算法的复杂度过高、鲁棒性太差等理论问题，也有数据资源匮乏、覆盖率低、知识表示困难等知识资源方面的问题，当然，还有实现技术和系统集成方法不够先进等方面的问题。","data":"2018年09月15日 17:04:06","date":"2018年09月15日 17:04:06"}
{"_id":{"$oid":"5d34590f62f717dc0659b903"},"title":"人工智能？AI？都是什么 四句话就让你彻底明白","author":"greencabin","content":"第一句：AI顾名思义就是英文单词Artificial intelligenc，即人工智能。\n其实人工智能并不是什么触不可及的东西，\n包括苹果Siri、百度度秘、微软小冰等智能助理和智能聊天类应用\n以及美图秀秀的自动美化功能，都属于人工智能。\n甚至一些简单的，套路固定的资讯类新闻，也是由人工智能来完成的。\n当然，现在的主流搜索引擎以及翻译技术也都在尝试\n利用人工智能来为广大网友提供更为精准的搜索服务。\n至于以实物存在的人工智能，当属现在物流仓库的小黄机器人了。\n他们正代替人类完成繁重的商品摆放、整理，快速出库、入库等操作。\n第二句：现在人工智能并没有发展到像电影中的机器人一样，那么高智能化的程度，今天的家庭机器人还远无法像大家奢望的那样， 以人形外貌出现在主人面前。现在的人工智能分三个级别：弱人工智能、强人工智能、超人工智能。\n1、弱人工智能\n也称限制领域人工智能或应用型人工智能，指的是专注于且只能解决特定领域问题的人工智能。毫无疑问，今天我们看到的所有人工智能算法和应用都属于弱人工智能。Alpha Go其实也是一个弱人工智能。\n2、强人工智能\n强人工智能又称通用人工智能或完全人工智能， 指的是可以胜任人类所有工作的人工智能。一个可以称得上强人工智能的程序， 大概需要具备以下几方面的能力：存在不确定因素时进行推理，使用策略，解决问题，制定决策的能力；知识表示的能力，包括常识性知识的表示能力；规划能力；学习能力；使用自然语言进行交流沟通的能力；将上述能力整合起来实现既定目标的能力。\n3、超人工智能\n假设计算机程序通过不断发展，可以比世界上最聪明、最有天赋的人类还聪明，那么由此产生的人工智能系统就可以被称为超人工智能。超人工智能的定义最为模糊，因为没人知道， 超越人类最高水平的智慧到底会表现为何种能力。如果说对于强人工智能，我们还存在从技术角度进行探讨的可能性的话，那么，对于超人工智能，今天的人类大多就只能从哲学或科幻的角度加以解析了。\n第三句：人工智能的主要技术：深度学习+大数据\n简单地说， 深度学习就是把计算机要学习的东西看成一大堆数据， 把这些数据丢进一个复杂的、包含多个层级的数据处理网络，然后检查经过这个网络处理得到的结果数据是不是符合要求——如果符合， 就保留这个网络作为目标模型，如果不符合，就一次次地、锲而不舍地调整网络的参数设置，直到输出满足要求为止。\n这就好比输入一股水流，计算机只要调节中间层层阀门，如果可以在预期的管道出口看到水流，那么就说明这个管道符合要求。而我们要做的，只是告诉计算机输入和预期的结果，让他自己找规律。当然，新的输入进入时，我们也要保证已经调节好的管道不变化。\n也就是说，深度学习算法是有计算机自己凑出来的模型。这样反倒更加实用，更能够从本质上解决问题。\n当然，搭建好的“管道”只有通过各种类型“水流”的检验，才能变得越来越接近真实的世界，值得一提的是，大数据正是为这些“管道”提供了源源不断的“水流”。\n我们知道深度学习、大规模计算、大数据都是在2010年前后逐渐步入成熟的。\n第四句：人工智能主要用在以下领域：自动驾驶、智慧生活、智慧医疗\n1、自动驾驶：最大的应用场景\n自动驾驶是现在逐渐发展成熟的一项智能应用。可以想象，自动驾驶一旦实现，可以带来如下改变：\n1、完全意义上的共享汽车成为可能。大多数汽车可以用共享经济的模式，随叫随到。因为不需要司机，这些车辆可以保证24小时待命，可以在任何时间、任何地点提供高质量的租用服务。\n2、汽车本身的形态也会发生根本性的变化。一辆不需要方向盘、不需要司机的汽车，可以被设计成前所未有的样子。\n3、未来的道路发生变化。它们也会按照自动驾驶汽车的要求来重新设计，专用于自动驾驶的车道可以变得更窄，交通信号可以更容易被自动驾驶汽车识别。\n2、智慧生活\n目前的机器翻译水平， 大概相当于一个刚学某种外语两三年的中学生做出的翻译作业。 对于多数非专业类的普通文本内容， 机器翻译的结果已经可以做到基本表达原文语意， 不影响理解与沟通。但假以时日，不断提高翻译准确度的人工智能系统，极有可能像下围棋的Alpha Go那样悄然越过了业余译员和职业译员之间的技术鸿沟， 一跃而成为翻译大师。\n那时候，不只是手机会和人智能对话，我们每个家庭里的每一件家用电器，都会拥有足够强大的对话功能，为我们提供方便的服务。\n3、智慧医疗：AI将成为医生的好帮手\n大数据和基于大数据的人工智能， 为医生辅助诊断疾病提供了最好的支持。事实证明，就在今年2月，经过深度学习的神经网络在诊断某些皮肤病方面的表现比大部分医生还要好。在AI的帮助下， 我们看到的不会是医生失业， 而是同样数量的医生可以服务几倍、 数十倍甚至更多的人群。医疗资源分布不均衡的地区， 会因为AI的引入，让绝大多数病人享受到一流的医疗服务。","data":"2018年07月18日 16:12:40"}
{"_id":{"$oid":"5d34595062f717dc0659b90d"},"title":"2018 全球人工智能与机器人峰会（CCF-GAIR）－－探讨自然语言处理的商业落地","author":"refresh\u0026grow","content":"本文对参加圆桌会议嘉宾的观点进行了整理。\n云孚科技--专注于为企业提供自然语言处理技术解决方案。\n创始人兼 CEO 张文斌：商业化的本质首先一定要盈利，其次要做到规模化盈利。\n人工智能这一块的创业公司要盈利，大方向有两个，做 toC 直接面对消费者，和做 toB 面对企业。云孚科技选择了 toB，周期相对可控，比较容易把营收做起来。\n规模化盈利又有两个方向，创业公司选择最多的是深入行业做垂直行业的应用，这样可以规模化做特定行业用户，而且可以做大订单，也是投资人比较认可的方向。我们还看到另外一个方向，就是做基础技术平台，因为它足够基础，可以面向多个行业的客户提供产品，订单相对比较小，但客户数更广。\n竹间智能 --主要是做情感计算，不只做文本情感，还做语音情绪和表情。目前主要在金融、电商、IOT 领域、运营商等领域落地。\nCTO 翁嘉颀：目前人工智能必须从单个到单独的领域去突破，去那个领域先收集语料，以及到底要解决什么问题，针对这一类问题我能解决哪些？\n神州泰岳大数据 VP 张瑞飞：讲到商业落地，我们要解决几个矛盾，第一个矛盾是人工智能现在处于初级阶段，尤其在认知科学领域。在初级阶段要落地，就意味着你要管客户收钱。第二个矛盾是我们要解决成本和实际达成成效之间的平衡，理论上讲，只要你投入足够多的成本，人工智能的效果就会更好一些，但是它又有最佳值，我们要找到最佳值在哪儿。第三是我们要解决在算法工程和基础算法之间的选择，我们在算法工程中要解决的问题和我们在基础算法中进行的优化研究结合起来。\n准是你能使用的非常高的影响力，它也是能要到钱的基础\n薄言 RSVP.ai --公司的初衷就是希望让机器了解语言，自动帮人做一些关于语言的事情。\n落地的技术难点和应用难点\nNLP 技术目前处于相对不太成熟的阶段，也是因为它的任务非常多且复杂。分词相对比较成熟，测试语料上准确率可以做到 97% 以上，就算分词这么成熟的技术，落到特定行业，面对一些新词，效果还是不足够理想，还需要花很多精力去做针对特定领域去做优化。\n目前 NLP 在短文本、短句 15 个字以内，意图理解、意思理解可能没有什么问题，长文本目前还不太行。\n自然语言处理属于认知智能范畴，所以自然语言处理的终极目标是理解人类的思维和想法，在这个过程中，我们需要拿捏尺度，这个尺度即把机器智能和人类智慧融合在一起，而不是用机器智能取代人类智慧。\n目前自然语言处理的问题是如果我把算法做得特别深，往往有普适性的问题，如果做平台，往往做深入就会有困难，这是第一点。第二点，拿语义相似度短文本来讲，一些领域它们的训练语量没有那么多，但又有各自领域自己的知识和特点，这个时候通用的数据集怎么达到更好的效果？第三点是在商业化中，自然语言处理跟 CV 领域还有一个差别，大家很多时候是用大量的 LSTM、Model 等，哪怕 Attention 才是你真正需要的东西，但实际上它还是时序模型，如果我把它放到线上系统，时延是有问题的，而且成本非常高，所以自然语言处理商业化也要考虑成本问题。\n需要重点解决的问题\n短文本语义相似度\n实际上在语义上Q\u0026A两句话不是完全相等的。此外，我们在方法上还有一个维度的区别，现在很多服务是基于搜索的技术，现在业内也有一些新的评价方式，即在平行文本做评价，两种评价都达到很好的效果，但是如果做交叉，总会有损失，这个问题对我们来讲是比较棘手的问题。\n要解决标注好的数据，用户使用过程中反馈的数据。\n如何做好对话控制，如何限制用户讲话方式，这真的是一个技巧，因为用户乱讲一通，你是没有办法理解的。\n怎么样在没有标注语料或很少的标注语料的情况下就能把他们想要的结构化信息抽取出来，抽取完再构建这个行业的知识图谱。我们也积累了一些经验，一般可以先用已有的通用系统结合基于规则的方法先做一版系统出来，这样可以先跑一个初步结果，从中挑一部分比较严重的 badcase 出来，人工标注语料，再重新训练模型，如此反复迭代。最终可以花比较小的人力标注成本把系统迁移到其他领域。当然，刚才说的这个过程还比较理想，怎么样用尽可能少的标注语料，可以快速迁移领域？这是我们实际工作中碰到的一个非常实际的问题。\n学术界的前沿研究对于企业的产业化落地能得到哪些借鉴和思考？\n没有一个单一的算法能够解决好问题，可能要看四、五十篇 Paper，从里面融合出一个方法，所以每一个算法，每一个 Paper 都有它可取的地方，比如其中四个算法告诉我他要退货，一个算法告诉我他要换货，我会用投票的方式，比较有机会真正落地解决问题，因为算法有弱点，用多个算法去做，能互相弥补缺陷。\n不能评价哪个算法好与坏，因为算法要看适用场地，适用你的应用场景的方法就是最好的方法。\n学术界的诉求跟工业界还是不一样，学术界追求理论上、模型上的创新，如果有重复了就需要构思下一个新的模型。而创业公司是把他们探索出来的模型拿过来试，我们的核心是效果导向。其次，真正用这些算法做预测时，我们还得考虑它的性能，在正式场合，包括它需要的硬件条件是否符合业务需求，这也是我们落地时要考虑的因素，有的算法虽然非常高大上，可能高出 0.1 或 0.2 个点，但它的速度慢了很多，对硬件要求特别高，我们就会有所取舍，采用更加实用的算法。\n论文中的数据集往往跟我们面临的问题是不一样的，所以非常重要的是公司内部要有自己的测试集和标准，对于新的方法能够快速适应和尝试。\n参考链接：https://www.leiphone.com/news/201807/JHAwVgSYvCfKZQLm.html","data":"2018年07月19日 00:11:36"}
{"_id":{"$oid":"5d34598162f717dc0659b918"},"title":"浅谈自然语言处理（NLP）和 自然语言理解（NLU）","author":"IT_xiao_bai","content":"自然语言处理主要步骤包括：\n1. 分词（只针对中文，英文等西方字母语言已经用空格做好分词了）：将文章按词组分开\n\n2. 词法分析：对于英文，有词头、词根、词尾的拆分，名词、动词、形容词、副词、介词的定性，多种词意的选择。比如DIAMOND，有菱形、棒球场、钻石3个含义，要根据应用选择正确的意思。\n3. 语法分析：通过语法树或其他算法，分析主语、谓语、宾语、定语、状语、补语等句子元素。\n4. 语义分析：通过选择词的正确含义，在正确句法的指导下，将句子的正确含义表达出来。方法主要有语义文法、格文法。\n但是以上的分析，仅适用于小规模的实验室研究，远不能应用到实际语言环境中，比如说语法，我们能总结出的语法是有限的，可是日常应用的句子，绝大部分是不遵守语法的，如果让语法包罗所有可能的应用，会出现爆炸的景象。\n自然语言处理的应用方向主要有：\n1. 文本分类和聚类：主要是将文本按照关键字词做出统计，建造一个索引库，这样当有关键字词查询时，可以根据索引库快速地找到需要的内容。此方向是搜索引擎的基础，在早期的搜索引擎，比如北大开发的“天问系统”，采用这种先搜集资料、在后台做索引、在前台提供搜索查询服务。目前GOOGLE，百度的搜索引擎仍旧类似，但是采用了自动“蜘蛛”去采集网络上的信息，自动分类并做索引，然后再提供给用户。我曾经在我的文章中做过测试，当文章中有“十八禁”这样的字眼时，点击次数是我其他文章点击次数的几十倍，说明搜索引擎将“十八禁”这个词列为热门索引，一旦有一个“蜘蛛”发现这个词，其他“蜘蛛”也会爬过来。\n2. 信息检索和过滤：这是网络瞬时检查的应用范畴，主要为网警服务，在大流量的信息中寻找关键词，找到了就要做一些其他的判断，比如报警。\n3. 信息抽取：（抄书）信息抽取研究旨在为人们提供更有力的信息获取工具，以应对信息爆炸带来的严重挑战。与信息检索不同，信息抽取直接从自然语言文本中抽取事实信息。过去十多年来，信息抽取逐步发展成为自然语言处理领域的一个重要分支，其独特的发展轨迹——通过系统化、大规模地定量评测推动研究向前发展，以及某些成功启示，如部分分析技术的有效性、快速自然语言处理系统开发的必要性，都极大地推动了自然语言处理研究的发展，促进了自然语言处理研究与应用的紧密结合。回顾信息抽取研究的历史，总结信息抽取研究的现状，将有助于这方面研究工作向前发展。\n4. 问答系统：目前仍局限于80年代的专家系统，就是按照LISP语言的天然特性，做逻辑递归。LISP语言是括号式的语言，比如A=（B，C，D），A=（B，E，F），提问：已知B，C，能得到什么样的结论？结论是A，D；若提问改为已知B，结论则是C，D，A或E，F，A。比如一个医疗用的专家系统，你若询问“感冒”的治疗方法，系统可能给出多种原因带来的感冒极其治疗方法，你若询问“病毒性感冒”的治疗方法，则系统会给出比较单一的、明确的治疗方法。你有没有用过AUTOCAD系统，这个就是建立在LISP语言上的括号系统，在用的时候会出现上述情况。\n5. 拼音汉字转换系统：这应该是中文输入法应用范畴的东西，再多的东西我就没想过。\n6. 机器翻译：当前最热门的应用方向，这方面的文章最多。国际上已经有比较好的应用系统，美国有个AIC公司推出过著名的实时翻译系统，欧共体的SYSTRAN系统可以将英、法、德、西、意、葡六种语言实时对译，美、日、德联合开发的自动语音翻译系统，成功进行了10多分钟对话。我国军事科学院、中科院也开发过此类系统。但是这里边的问题也很多，最主要的是“满篇洋文难不住，满篇译文看不懂”，就是脱离了人类智慧的机器翻译，总会搞出让人无法理解的翻译，比如多意词选择哪个意思合适、怎么组织出通顺的语句，等等。所以目前微软、GOOGLE的新趋势是：翻译+记忆，类似机器学习，将大量以往正确的翻译存储下来，通过检索，如果碰到类似的翻译要求，将以往正确的翻译结果拿出来用。GOOGLE宣称今后几年就可以推出商业化的网页翻译系统。\n7. 新信息检测：这个我不知道，没思路。\n以上已经回答了自然语言发展方向的问题。我认为机器翻译是最有前途的方向，其难点在于机器翻译还不具备人类智能，虽然翻译已经达到90%以上的正确程度，然而还是不能象人类翻译那样，可以准确表达。为什么存在这样的难点？关键是自然语言处理做不到人类对自然语言的理解，“处理”和“理解”是天差地别的两个概念。“处理”好比控制眼睛、耳朵、舌头的神经，他们将接收的信息转化成大脑可以理解的内部信息，或者反过来，他们的功能就是这么多。而“理解”则是大脑皮层负责语言理解那部分，多少亿的脑细胞共同完成的功能。一个人因为其自身家庭背景、受教育程度、接触现实中长期形成的条件反射刺激、特殊的强列刺激、当时的心理状况，这么多的因素都会影响和改变“理解”的功能，比如我说“一个靓女开着BMW跑车”，有人心里会想这是二奶吧？有人心里会仇视她，联想到她会撞了人白撞；做汽车买卖的人则会去估量这部车的价值；爱攀比的人也许会想，我什么时候才能开上BWM？所以“理解”是更加深奥的东西，涉及更多神经学、心理学、逻辑学领域。\n还有上下文理解问题，比如这句：“我们90平方米以后会占的分量越来越大，那么这样他的价格本身比高档低很多，所以对于整体把这个价格水平给压下来了，这个确实非常好的。” 你能理解么？估计很难或者理解出多种意思，但是我把前文写出来：“去年国家九部委联合发布了《建设部等部门关于调整住房供应结构稳定住房价格意见的通知》，对90平方米以下住房须占总面积的70%以上作出了硬性规定，深圳市经过一年的调控，目前已做到每个项目的75%都是90平方米以内。深圳市国土资源和房产管理局官员说”看了后面的你才能知道是根据国家的通知，深圳做了相应的调整。\n自然语言理解\n1. 语义表示\n自然语言理解的结果，就是要获得一个语义表示（semantic representation）：\n语义表示主要有三种方式：\n1. 分布语义，Distributional semantics\n2. 框架语义，Frame semantics\n3. 模型论语义，Model-theoretic semantics\n1.1 分布语义表示（Distributional semantics）\n说distributional semantics大家比较陌生，但如果说word2vec估计大家都很熟悉，word2vec的vector就是一种distributional semantics。distributional semantics就是把语义表示成一个向量，它的理论基础来自于Harris的分布假设：语义相似的词出现在相似的语境中（Semantically similar words occur in similar contexts）。具体的计算方法有多种，比如LSA（Latent Semantic Analysis）、LDA（Latent Dirichlet Allocation）及各种神经网络模型（如LSTM）等。\n这种方法的优点在于，它完全是数据驱动的方法，并且能够很好的表示语义，但一个很大的缺点在于，它的表示结果是一个整体，没有进一步的子结构。\n1.2 框架语义表示（Frame semantics）\n顾名思义，这种方法把语义用一个frame表示出来，比如我们一开始举得例子：“订一张明天北京去杭州的机票，国航头等舱”，表示如下：\n在计算方法上，典型的比如语义角色标注（Semantic Role Labeling），具体可以分为两个步骤：frame identification和argument identification，frame identification用于确定frame的类型，argument identification用于计算各个属性的具体值。这种方法和distributional semantics相比，能够表达丰富的结构。\n1.3 模型论语义表示（Model-theoretic semantics）\n模型轮语义表示的典型框架是把自然语言映射成逻辑表达式（logic form）。比如对于下图中的“中国面积最大的省份是哪个？”,将其表示成逻辑表达式就是图中红色字体部分，进一步那这个逻辑表达式去知识库中查询，就得到了答案。在计算方法上，典型的就是构建一个semantic parser。\n模型论语义表示是对世界知识的完整表示，比前两种方法表达的语义更加完整，但是缺点是semantic parser的构建比较困难，这大大限制了该方法的应用。\n1.4 目前采用的语义表示\n目前常用的是frame semantics表示的一种变形：采用领域（domain）、意图（intent）和属性槽（slots）来表示语义结果。\n其中，领域是指同一类型的数据或者资源，以及围绕这些数据或资源提供的服务，比如“餐厅”，“酒店”，“飞机票”、“火车票”、“电话黄页”等；意图是指对于领域数据的操作，一般以动宾短语来命名，比如飞机票领域中，有“购票”、“退票”等意图；属性槽用来存放领域的属性，比如飞机票领域有“时间”“出发地”“目的地”等；\n对于飞机票领域，我们的语义表示结构如下图所示：\n进一步，我们对于世界的语义描述（又称为domain ontology）如下：\n2. 自然语言理解技术难点\n在确定了自然语言理解的语义表示方法后，我们把技术方案抽象为如下两步：\n\n这和前文提到的语义角色标注把过程分为frame identification和argument identification类似，领域分类和意图分类对应frame identification，属性抽取对应argument identification。无论对于分类还是对于抽取来说，都需要有外部知识的支持。在实现的过程中，我们面临着如下的困难：\n（1）如何构建知识库\n“总参”除了表示总参谋部外，还是南京一家很火的火锅店；“中华冷面”除了是一种面条，还是一首歌名；“王菲的红豆”是指王菲唱的红豆这首歌，但如果说“韩红的红豆”就不对了，因为韩红没有唱过红豆这首歌。要想把这些知识都理解对，就需要一个庞大的知识库，这个知识库中的实体词数以千万计，怎么挖掘，怎么清洗噪音，都是很大的挑战。\n（2）如何理解用户语句的意图\n“东三环堵吗”这句话意图是查询路况，“下水道堵吗”就不是查路况了；“今天的天气”是想问天气状况，“今天的天气不错”则无此意；“附近哪儿可以喝咖啡”是想找咖啡馆，但“牛皮癣能喝咖啡吗”就是一个知识问答了。类似上述的例子举不胜举，更别说语言理解还受时间、位置、设备、语境等等问题的影响。\n（3）如何构建可扩展的算法框架\n现实世界包含众多的领域，而我们不可能一次性的把所有领域都定义清楚并且实现之，那我们就需要一个可扩展的算法框架，每当修改或者新增某个领域的时候，不会对其他领域造成干扰。\n（4）如何构建数据驱动的计算流程\n大数据时代，如果一个算法或者流程不是数据驱动的，不是随着数据的增加而自动提升效果，那这个算法框架就没有持续的生命力。\n（5）如何融入上下文知识\n在对话场景中，每句话都有对话上下文，同样的句子在不同的上下文中理解结果是不一样的，比如如下的例子，同样的一句话“今天天气好吗”在左侧图中属于天气领域，而在右侧图中则属于音乐领域。","data":"2018年07月24日 11:10:30","date":"2018年07月24日 11:10:30"}
{"_id":{"$oid":"5d345afb62f717dc0659b94e"},"title":"NLP第1课：中文自然语言处理的完整机器处理流程","author":"Mr愚先森","content":"2016年全球瞩目的围棋大战中，人类以失败告终，更是激起了各种“机器超越、控制人类”的讨论，然而机器真的懂人类吗？机器能感受到人类的情绪吗？机器能理解人类的语言吗？如果能，那它又是如何做到呢？带着这样好奇心，本文将带领大家熟悉和回顾一个完整的自然语言处理过程，后续所有章节所有示例开发都将遵从这个处理过程。\n首先我们通过一张图来了解 NLP 所包含的技术知识点，这张图从分析对象和分析内容两个不同的维度来进行表达，个人觉得内容只能作为参考，对于整个 AI 背景下的自然语言处理来说还不够完整。\nenter image description here\n有机器学习相关经验的人都知道，中文自然语言处理的过程和机器学习过程大体一致，但又存在很多细节上的不同点，下面我们就来看看中文自然语言处理的基本过程有哪些呢？\n获取语料\n语料，即语言材料。语料是语言学研究的内容。语料是构成语料库的基本单元。所以，人们简单地用文本作为替代，并把文本中的上下文关系作为现实世界中语言的上下文关系的替代品。我们把一个文本集合称为语料库（Corpus），当有几个这样的文本集合的时候，我们称之为语料库集合(Corpora)。（定义来源：百度百科）按语料来源，我们将语料分为以下两种：\n1.已有语料\n很多业务部门、公司等组织随着业务发展都会积累有大量的纸质或者电子文本资料。那么，对于这些资料，在允许的条件下我们稍加整合，把纸质的文本全部电子化就可以作为我们的语料库。\n2.网上下载、抓取语料\n如果现在个人手里没有数据怎么办呢？这个时候，我们可以选择获取国内外标准开放数据集，比如国内的中文汉语有搜狗语料、人民日报语料。国外的因为大都是英文或者外文，这里暂时用不到。也可以选择通过爬虫自己去抓取一些数据，然后来进行后续内容。\n语料预处理\n这里重点介绍一下语料的预处理，在一个完整的中文自然语言处理工程应用中，语料预处理大概会占到整个50%-70%的工作量，所以开发人员大部分时间就在进行语料预处理。下面通过数据洗清、分词、词性标注、去停用词四个大的方面来完成语料的预处理工作。\n1.语料清洗\n数据清洗，顾名思义就是在语料中找到我们感兴趣的东西，把不感兴趣的、视为噪音的内容清洗删除，包括对于原始文本提取标题、摘要、正文等信息，对于爬取的网页内容，去除广告、标签、HTML、JS 等代码和注释等。常见的数据清洗方式有：人工去重、对齐、删除和标注等，或者规则提取内容、正则表达式匹配、根据词性和命名实体提取、编写脚本或者代码批处理等。\n2.分词\n中文语料数据为一批短文本或者长文本，比如：句子，文章摘要，段落或者整篇文章组成的一个集合。一般句子、段落之间的字、词语是连续的，有一定含义。而进行文本挖掘分析时，我们希望文本处理的最小单位粒度是词或者词语，所以这个时候就需要分词来将文本全部进行分词。\n常见的分词算法有：基于字符串匹配的分词方法、基于理解的分词方法、基于统计的分词方法和基于规则的分词方法，每种方法下面对应许多具体的算法。\n当前中文分词算法的主要难点有歧义识别和新词识别，比如：“羽毛球拍卖完了”，这个可以切分成“羽毛 球拍 卖 完 了”，也可切分成“羽毛球 拍卖 完 了”，如果不依赖上下文其他的句子，恐怕很难知道如何去理解。\n3.词性标注\n词性标注，就是给每个词或者词语打词类标签，如形容词、动词、名词等。这样做可以让文本在后面的处理中融入更多有用的语言信息。词性标注是一个经典的序列标注问题，不过对于有些中文自然语言处理来说，词性标注不是非必需的。比如，常见的文本分类就不用关心词性问题，但是类似情感分析、知识推理却是需要的，下图是常见的中文词性整理。\nenter image description here\n常见的词性标注方法可以分为基于规则和基于统计的方法。其中基于统计的方法，如基于最大熵的词性标注、基于统计最大概率输出词性和基于 HMM 的词性标注。\n4.去停用词\n停用词一般指对文本特征没有任何贡献作用的字词，比如标点符号、语气、人称等一些词。所以在一般性的文本处理中，分词之后，接下来一步就是去停用词。但是对于中文来说，去停用词操作不是一成不变的，停用词词典是根据具体场景来决定的，比如在情感分析中，语气词、感叹号是应该保留的，因为他们对表示语气程度、感情色彩有一定的贡献和意义。\n特征工程\n做完语料预处理之后，接下来需要考虑如何把分词之后的字和词语表示成计算机能够计算的类型。显然，如果要计算我们至少需要把中文分词的字符串转换成数字，确切的说应该是数学中的向量。有两种常用的表示模型分别是词袋模型和词向量。\n词袋模型（Bag of Word, BOW)，即不考虑词语原本在句子中的顺序，直接将每一个词语或者符号统一放置在一个集合（如 list），然后按照计数的方式对出现的次数进行统计。统计词频这只是最基本的方式，TF-IDF 是词袋模型的一个经典用法。\n词向量是将字、词语转换成向量矩阵的计算模型。目前为止最常用的词表示方法是 One-hot，这种方法把每个词表示为一个很长的向量。这个向量的维度是词表大小，其中绝大多数元素为 0，只有一个维度的值为 1，这个维度就代表了当前的词。还有 Google 团队的 Word2Vec，其主要包含两个模型：跳字模型（Skip-Gram）和连续词袋模型（Continuous Bag of Words，简称 CBOW），以及两种高效训练的方法：负采样（Negative Sampling）和层序 Softmax（Hierarchical Softmax）。值得一提的是，Word2Vec 词向量可以较好地表达不同词之间的相似和类比关系。除此之外，还有一些词向量的表示方式，如 Doc2Vec、WordRank 和 FastText 等。\n特征选择\n同数据挖掘一样，在文本挖掘相关问题中，特征工程也是必不可少的。在一个实际问题中，构造好的特征向量，是要选择合适的、表达能力强的特征。文本特征一般都是词语，具有语义信息，使用特征选择能够找出一个特征子集，其仍然可以保留语义信息；但通过特征提取找到的特征子空间，将会丢失部分语义信息。所以特征选择是一个很有挑战的过程，更多的依赖于经验和专业知识，并且有很多现成的算法来进行特征的选择。目前，常见的特征选择方法主要有 DF、 MI、 IG、 CHI、WLLR、WFO 六种。\n模型训练\n在特征向量选择好之后，接下来要做的事情当然就是训练模型，对于不同的应用需求，我们使用不同的模型，传统的有监督和无监督等机器学习模型， 如 KNN、SVM、Naive Bayes、决策树、GBDT、K-means 等模型；深度学习模型比如 CNN、RNN、LSTM、 Seq2Seq、FastText、TextCNN 等。这些模型在后续的分类、聚类、神经序列、情感分析等示例中都会用到，这里不再赘述。下面是在模型训练时需要注意的几个点。\n1.注意过拟合、欠拟合问题，不断提高模型的泛化能力。\n过拟合：模型学习能力太强，以至于把噪声数据的特征也学习到了，导致模型泛化能力下降，在训练集上表现很好，但是在测试集上表现很差。\n常见的解决方法有：\n增大数据的训练量；\n增加正则化项，如 L1 正则和 L2 正则；\n特征选取不合理，人工筛选特征和使用特征选择算法；\n采用 Dropout 方法等。\n欠拟合：就是模型不能够很好地拟合数据，表现在模型过于简单。\n常见的解决方法有：\n添加其他特征项；\n增加模型复杂度，比如神经网络加更多的层、线性模型通过添加多项式使模型泛化能力更强；\n减少正则化参数，正则化的目的是用来防止过拟合的，但是现在模型出现了欠拟合，则需要减少正则化参数。\n2.对于神经网络，注意梯度消失和梯度爆炸问题。\n评价指标\n训练好的模型，上线之前要对模型进行必要的评估，目的让模型对语料具备较好的泛化能力。具体有以下这些指标可以参考。\n1.错误率、精度、准确率、精确度、召回率、F1 衡量。\n错误率：是分类错误的样本数占样本总数的比例。对样例集 D，分类错误率计算公式如下：\nenter image description here\n精度：是分类正确的样本数占样本总数的比例。这里的分类正确的样本数指的不仅是正例分类正确的个数还有反例分类正确的个数。对样例集 D，精度计算公式如下：\nenter image description here\n对于二分类问题，可将样例根据其真实类别与学习器预测类别的组合划分为真正例（True Positive）、假正例（False Positive）、真反例（True Negative)、假反例（False Negative）四种情形，令 TP、FP、TN、FN 分别表示其对应的样例数，则显然有 TP+FP++TN+FN=样例总数。分类结果的“混淆矩阵”（Confusion Matrix）如下：\nenter image description here\n准确率，缩写表示用 P。准确率是针对我们预测结果而言的，它表示的是预测为正的样例中有多少是真正的正样例。定义公式如下：\nenter image description here\n精确度，缩写表示用 A。精确度则是分类正确的样本数占样本总数的比例。Accuracy 反应了分类器对整个样本的判定能力（即能将正的判定为正的，负的判定为负的）。定义公式如下：\nenter image description here\n召回率，缩写表示用 R。召回率是针对我们原来的样本而言的，它表示的是样本中的正例有多少被预测正确。定义公式如下：\nenter image description here\nF1 衡量，表达出对查准率/查全率的不同偏好。定义公式如下：\nenter image description here\n2.ROC 曲线、AUC 曲线。\nROC 全称是“受试者工作特征”（Receiver Operating Characteristic）曲线。我们根据模型的预测结果，把阈值从0变到最大，即刚开始是把每个样本作为正例进行预测，随着阈值的增大，学习器预测正样例数越来越少，直到最后没有一个样本是正样例。在这一过程中，每次计算出两个重要量的值，分别以它们为横、纵坐标作图，就得到了 ROC 曲线。\nROC 曲线的纵轴是“真正例率”（True Positive Rate, 简称 TPR)，横轴是“假正例率”（False Positive Rate,简称FPR），两者分别定义为：\nenter image description here\nenter image description here\nROC 曲线的意义有以下几点：\nROC 曲线能很容易的查出任意阈值对模型的泛化性能影响；\n有助于选择最佳的阈值；\n可以对不同的模型比较性能，在同一坐标中，靠近左上角的 ROC 曲所代表的学习器准确性最高。\n如果两条 ROC 曲线没有相交，我们可以根据哪条曲线最靠近左上角哪条曲线代表的学习器性能就最好。但是实际任务中，情况很复杂，若两个模型的 ROC 曲线发生交叉，则难以一般性的断言两者孰优孰劣。此时如果一定要进行比较，则比较合理的判断依据是比较 ROC 曲线下的面积，即AUC（Area Under ROC Curve）。\nAUC 就是 ROC 曲线下的面积，衡量学习器优劣的一种性能指标。AUC 是衡量二分类模型优劣的一种评价指标，表示预测的正例排在负例前面的概率。\n前面我们所讲的都是针对二分类问题，那么如果实际需要在多分类问题中用 ROC 曲线的话，一般性的转化为多个“一对多”的问题。即把其中一个当作正例，其余当作负例来看待，画出多个 ROC 曲线。\n模型上线应用\n模型线上应用，目前主流的应用方式就是提供服务或者将模型持久化。\n第一就是线下训练模型，然后将模型做线上部署，发布成接口服务以供业务系统使用。\n第二种就是在线训练，在线训练完成之后把模型 pickle 持久化，然后在线服务接口模板通过读取 pickle 而改变接口服务。\n模型重构（非必须）\n随着时间和变化，可能需要对模型做一定的重构，包括根据业务不同侧重点对上面提到的一至七步骤也进行调整，重新训练模型进行上线。\n\n\n作者：米饭超人\n链接：https://www.jianshu.com/p/b87e01374a65\n来源：简书\n简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。","data":"2019年03月16日 10:11:18"}
{"_id":{"$oid":"5d345b3762f717dc0659b95b"},"title":"自然语言处理太难？按这个方式走，就是砍瓜切菜！","author":"Python开发者","content":"人工智能的研究和应用越发炙手可热，其中“机器学习、自动驾驶、语音识别、计算机视觉、自然语言处理、知识推理”这6个方向热度最为火爆。\n自然语言处理（简称NLP）就是用计算机来处理、理解以及运用人类语言(如中文、英文等)，作为人工智能的一个分支，它站在机遇与挑战并存的十字路口。\n在信息时代，自然语言处理的应用是包罗万象的，比如：机器翻译、手写体和印刷体字符识别、语音识别及文语转换等，自然语言处理将会通过人工智能的方式出现在生活的方方面面；但它也面临着技术上的挑战，比如目前网络搜索引擎基本上还停留在关键词匹配，缺乏深层次的自然语言处理和理解。这是亟待解决的问题。\n而正是这种机遇与挑战并存的现状更加深刻的印证了自然语言处理作为一个高度交叉的新兴学科的无限发展空间。\n国内顶尖的科技公司几乎同时组建团队，争夺自然语言处理人才。\n给出的岗位薪资如此之高，更印证了技术人才的短缺。\n去年领英发布的全球 AI 人才调研报告显示，中国人工智能人才缺口达 500 万，供求比例仅为 1：10，并预计 AI 岗位高薪的状况仍将持续很长一段时间。自然语言处理自然是逃不掉的人才缺失重地。\n俗话说，万事开头难。如果第一件事情成功了，学生就能建立信心，找到窍门，今后越做越好。否则，也可能就灰心丧气，甚至离开这个领域。所以，我们建议你跟着大牛学习。跟着大牛学，自然语言处理，不过就是砍瓜切菜！\n《自然语言处理 第一期》\n主讲老师：秦曾昌\n英国布里斯托大学硕士、博士；\n美国加州大学伯克利分校博士后；\n牛津大学和卡内基梅隆大学访问学者；\n目前主要研究方向为数据挖掘、跨媒体检索与自然语言理解。出版英文专著一本、编辑论文集一本和专业论文或章节90余篇。同时在IT工业界做机器学习、大数据、人工智能等专业技术咨询工作。\n通过这次学习，你可以掌握","data":"2018年08月12日 19:50:00"}
{"_id":{"$oid":"5d345b6762f717dc0659b963"},"title":"自然语言处理2——THUCNews中文数据集与IMDB英文数据集","author":"Growing_Snake","content":"文章目录\n1. THUCNews中文数据集\n1.1 数据下载\n1.2 数据探索\n2. IMDB英文数据集\n2.1 数据下载\n2.2 数据探索\n3. 常用评估方式\n1. THUCNews中文数据集\nTHUCNews是根据新浪新闻RSS订阅频道2005~2011年间的历史数据筛选过滤生成，包含74万篇新闻文档（2.19 GB），均为UTF-8纯文本格式。在原始新浪新闻分类体系的基础上，重新整合划分出14个候选分类类别：财经、彩票、房产、股票、家居、教育、科技、社会、时尚、时政、体育、星座、游戏、娱乐。\n1.1 数据下载\n官网链接 http://thuctc.thunlp.org/#获取链接， 提供个人信息后可下载。\n1.2 数据探索\n数据集中包含四个文本文件：cnews.test.txt，cnews.train.txt，cnews.val.txt，cnews.vocab.txt。\ncnews.train.txt为训练数据集，cnews.test.txt为测试数据集，cnews.val.txt为验证数据集，cnews.vocab.txt是所有数据集中出现的汉字、字母与标点符号汇集成的词典，其中是词汇表中添加的辅助Token，用来补齐句子长度。\n简单建立一个数据字典观察一下，可以看到包含的中文汉字还是挺多的，基本上常用字都包含了，附部分截图：\n\n2. IMDB英文数据集\n数据集包含电影评论及其关联的二进制标签，旨在作为情感分类的基准。核心数据集包含50,000个评论，均匀分为25k训练集和25k测试集。\n标签的整体分布是平衡的（25k pos和25k neg），还包括另外50,000个未标记文档，用于无监督学习。\n2.1 数据下载\nhttp://ai.stanford.edu/~amaas/data/sentiment/ 进入后直接点击Large Movie Review Dataset v1.0开始下载即可。\n2.2 数据探索\n下载后会得到一个aclImdb_v1.tar.gz压缩包，解压之后可以看到，文件夹中包含train训练数据集的文件夹和test测试数据集文件夹。\n在训练数据集中主要包括两个已标记情感类别的影评数据集pos和neg和一个未标记的用于无监督学习的数据集unsup，还有一个imdb的词汇表字典，包含了训练集中出现的所有单词。\n测试集中主要包括两个已标记情感类别的影评数据集pos和neg。\n同样简单建立一个数据字典观察一下，这个…英文单词果然是庞然大物，太多了，密集恐惧…附部分截图：\n\n3. 常用评估方式\n首先要提出混淆矩阵：\n混淆矩阵\nPositive\nNegative\nPositive\nTP\nFP\nNegative\nFN\nTN\nTrue Positive(真正, TP)：将正类预测为正类数\nTrue Negative(真负 , TN)：将负类预测为负类数\nFalse Positive(假正, FP)：将负类预测为正类数 → 误报 (Type I error)\nFalse Negative(假负 , FN)：将正类预测为负类数 → 漏报 (Type II error)\n准确率(accuracy) 预测准确的在所有样本中的比例， accuracy=\n(\nT\nP\n+\nT\nN\n)\nT\nP\n+\nF\nN\n+\nF\nP\n+\nT\nN\n\\frac{(TP+TN)}{TP+FN+FP+TN}\nTP+FN+FP+TN(TP+TN)\n精确率（precision）：precision=\nT\nP\nT\nP\n+\nF\nP\n\\frac{TP}{TP+FP}\nTP+FPTP\n对于给定的测试数据集，分类器正确分类的样本数与正样本数之比。（简单点：给出的结果有多少是正确的）；精确率是针对预测结果而言的，它表示的是预测为正的样本中有多少是对的。那么预测为正就有两种可能了，一种就是把正类预测为正类(TP)，另一种就是把负类预测为正类(FP)。\n召回率（recall）： recall =\nT\nP\nT\nP\n+\nF\nN\n\\frac{TP}{TP+FN}\nTP+FNTP （正确的结果有多少被给出了）\n召回率是针对我们原来的样本而言的，它表示的是样本中的正例有多少被预测正确了。那也有两种可能，一种是把原来的正类预测成正类(TP)，另一种就是把原来的正类预测为负类(FN)。\nROC 关注两个指标:一个是TPR（也就是召回率），另一个是将负例错分为正例的概率（FPR=\nF\nP\nT\nP\n+\nT\nR\n\\frac{FP}{TP+TR}\nTP+TRFP ）。直观上，TPR 代表能将正例分对的概率，FPR 代表将负例错分为正例的概率。在 ROC 空间中，每个点的横坐标是 FPR，纵坐标是 TPR，\nAUC（Area Under Curve）被定义为ROC曲线下的面积。可以综合衡量一个预测模型的好坏，这一个指标综合了precision和recall两个指标。简单说：AUC值越大的分类器，正确率越高。\nAUC=1，完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。\n0.5\u003cAUC\u003c1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。\nAUC=0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。\nAUC\u003c0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测，因此不存在 AUC\u003c0.5 的情况。","data":"2019年04月08日 16:46:16"}
{"_id":{"$oid":"5d345bd062f717dc0659b977"},"title":"自然语言处理中的Transformer和BERT","author":"_zhang_bei_","content":"2018年马上就要过去，回顾深度学习在今年的进展，让人印象最深刻的就是谷歌提出的应用于自然语言处理领域的BERT解决方案，BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding（https://arxiv.org/abs/1810.04805）。BERT解决方案刷新了各大NLP任务的榜单，在各种NLP任务上都做到state of the art。这里我把BERT说成是解决方案，而不是一个算法，因为这篇文章并没有提出新的算法模型，还是沿用了之前已有的算法模型。BERT最大的创新点，在于提出了一套完整的方案，利用之前最新的算法模型，去解决各种各样的NLP任务，因此BERT这篇论文对于算法模型完全不做介绍，以至于在我直接看这篇文章的时候感觉云里雾里。但是本文中，我会从算法模型到解决方案，进行完整的诠释。本文中我会分3个部分进行介绍，第一部分我会大概介绍一下NLP的发展，第二部分主要讲BERT用到的算法，最后一部分讲BERT具体是怎么操作的。\n一，NLP的发展\n要处理NLP问题，首先要解决文本的表示问题。虽然我们人去看文本，能够清楚明白文本中的符号表达什么含义，但是计算机只能做数学计算，需要将文本表示成计算机可以处理的形式。最开始的方法是采用one hot，比如，我们假设英文中常用的单词有3万个，那么我们就用一个3万维的向量表示这个词，所有位置都置0，当我们想表示apple这个词时，就在对应位置设置1，如图1.1所示。这种表示方式存在的问题就是，高维稀疏，高维是指有多少个词，就需要多少个维度的向量，稀疏是指，每个向量中大部分值都是0。另外一个不足是这个向量没有任何含义。\n图1.1\n后来出现了词向量，word embedding，用一个低维稠密的向量去表示一个词，如图1.2所示。通常这个向量的维度在几百到上千之间，相比one hot几千几万的维度就低了很多。词与词之间可以通过相似度或者距离来表示关系，相关的词向量相似度比较高，或者距离比较近，不相关的词向量相似度低，或者距离比较远，这样词向量本身就有了含义。文本的表示问题就得到了解决。词向量可以通过一些无监督的方法学习得到，比如CBOW或者Skip-Gram等，可以预先在语料库上训练出词向量，以供后续的使用。顺便提一句，在图像中就不存在表示方法的困扰，因为图像本身就是数值矩阵，计算机可以直接处理。\n图1.2\nNLP中有各种各样的任务，比如分类（Classification），问答（QA），实体命名识别（NER）等。对于这些不同的任务，最早的做法是根据每类任务定制不同的模型，输入预训练好的embedding，然后利用特定任务的数据集对模型进行训练，如图1.3所示。这里存在的问题就是，不是每个特定任务都有大量的标签数据可供训练，对于那些数据集非常小的任务，恐怕就难以得到一个理想的模型。\n图1.3\n我们看一下图像领域是如何解决这个问题的。图像分类是计算机视觉中最基本的任务，当我要解决一个小数据集的图像分类任务时，该怎么做？CV领域已经有了一套成熟的解决方案。我会用一个通用的网络模型，比如Vgg，ResNet或者GoogleNet，在ImageNet上做预训练（pre-training）。ImageNet有1400万张有标注的图片，包含1000个类别，这样的数据规模足以训练出一个规模庞大的模型。在训练过程中，模型会不断的学习如何提取特征，底层的CNN网络结构会提取边缘，角，点等通用特征，模型越往上走，提取的特征也越抽象，与特定的任务更加相关。当完成预训练之后，根据我自己的分类任务，调整最上层的网络结构，然后在小数据集里对模型进行训练。在训练时，可以固定住底层的模型参数只训练顶层的参数，也可以对整个模型进行训练，这个过程叫做微调（fine-tuning），最终得到一个可用的模型。总结一下，整个过程包括两步，拿一个通用模型在ImageNet上做预训练（pre-training），然后针对特定任务进行微调（fine-tuning），完美解决了特定任务数据不足的问题。还有一个好处是，对于各种各样的任务都不再需要从头开始训练网络，可以直接拿预训练好的结果进行微调，既减少了训练计算量的负担，也减少了人工标注数据的负担。\nNLP领域也引入了这种做法，用一个通用模型，在非常大的语料库上进行预训练，然后在特定任务上进行微调，BERT就是这套方案的集大成者。BERT不是第一个，但目前为止，是效果最好的方案。BERT用了一个已有的模型结构，提出了一整套的预训练方法和微调方法，我们在后文中再进行详细的描述。\n二，算法\nBERT所采用的算法来自于2017年12月份的这篇文章，Attenion Is All You Need（https://arxiv.org/abs/1706.03762），同样来自于谷歌。这篇文章要解决的是翻译问题，比如从中文翻译成英文。这篇文章完全放弃了以往经常采用的RNN和CNN，提出了一种新的网络结构，即Transformer，其中包括encoder和decoder，我们只关注encoder。这篇英文博客（https://jalammar.github.io/illustrated-transformer/）对Transformer介绍得非常详细，有兴趣的读者可以看一下，如果不想看英文博客也可以看本文，本文中的部分图片也截取自这篇博客。\n图2.1\n图2.1是Transformer encoder的结构，后文中我们都简称为Transformer。首先是输入word embedding，这里是直接输入一整句话的所有embedding。如图2.1所示，假设我们的输入是Thinking Machines，每个词对应一个embedding，就有2个embedding。输入embedding需要加上位置编码（Positional Encoding），为什么要加位置编码，后文会做详细介绍。然后经过一个Multi-Head Attention结构，这个结构是算法单元中最重要的部分，我们会在后边详细介绍。之后是做了一个shortcut的处理，就是把输入和输出按照对应位置加起来，如果了解残差网络（ResNet）的同学，会对这个结构比较熟悉，这个操作有利于加速训练。然后经过一个归一化normalization的操作。接着经过一个两层的全连接网络，最后同样是shortcut和normalization的操作。可以看到，除了Multi-Head Attention，都是常规操作，没有什么难理解的。这里需要注意的是，每个小模块的输入和输出向量，维度都是相等的，比如，Multi-Head Attention的输入和输出向量维度是相等的，否则无法进行shortcut的操作；Feed Forward的输入和输出向量维度也是相等的；最终的输出和输入向量维度也是相等的。但是Multi-Head Attention和Feed Forward内部，向量维度会发生变化。\n图2.2\n我们来详细看一下Multi-Head Attention的结构。这个Multi-Head表示多头的意思，先从最简单的看起，看看单头Attention是如何操作的。从图2.1的橙色方块可以看到，embedding在进入到Attention之前，有3个分叉，那表示说从1个向量，变成了3个向量。具体是怎么算的呢？我们看图2.3，定义一个WQ矩阵（这个矩阵随机初始化，通过训练得到），将embedding和WQ矩阵做乘法，得到查询向量q，假设输入embedding是512维，在图3中我们用4个小方格表示，输出的查询向量是64维，图3中用3个小方格以示不同。然后类似地，定义WK和WV矩阵，将embedding和WK做矩阵乘法，得到键向量k；将embeding和WV做矩阵乘法，得到值向量v。对每一个embedding做同样的操作，那么每个输入就得到了3个向量，查询向量，键向量和值向量。需要注意的是，查询向量和键向量要有相同的维度，值向量的维度可以相同，也可以不同，但一般也是相同的。\n图2.3\n接下来我们计算每一个embedding的输出，以第一个词Thinking为例，参看图2.4。用查询向量q1跟键向量k1和k2分别做点积，得到112和96两个数值。这也是为什么前文提到查询向量和键向量的维度必须要一致，否则无法做点积。然后除以常数8，得到14和12两个数值。这个常数8是键向量的维度的开方，键向量和查询向量的维度都是64，开方后是8。做这个尺度上的调整目的是为了易于训练。然后把14和12丢到softmax函数中，得到一组加和为1的系数权重，算出来是大约是0.88和0.12。将0.88和0.12对两个值向量v1和v2做加权求和，就得到了Thinking的输出向量z1。类似的，可以算出Machines的输出z2。如果一句话中包含更多的词，也是相同的计算方法。\n图2.4\n通过这样一系列的计算，可以看到，现在每个词的输出向量z都包含了其他词的信息，每个词都不再是孤立的了。而且每个位置中，词与词的相关程度，可以通过softmax输出的权重进行分析。如图2.5所示，这是某一次计算的权重，其中线条颜色的深浅反映了权重的大小，可以看到it中权重最大的两个词是The和animal，表示it跟这两个词关联最大。这就是attention的含义，输出跟哪个词关联比较强，就放比较多的注意力在上面。上面我们把每一步计算都拆开了看，实际计算的时候，可以通过矩阵来计算，如图2.6所示。\n图2.5\n图2.6\n讲完了attention，再来讲Multi-Head。对于同一组输入embedding，我们可以并行做若干组上面的操作，例如，我们可以进行8组这样的运算，每一组都有WQ，WK，WV矩阵，并且不同组的矩阵也不相同。这样最终会计算出8组输出，我们把8组的输出连接起来，并且乘以矩阵WO做一次线性变换得到输出，WO也是随机初始化，通过训练得到，计算过程如图2.7所示。这样的好处，一是多个组可以并行计算，二是不同的组可以捕获不同的子空间的信息。\n图2.7\n到这里就把Transformer的结构讲完了，同样都是做NLP任务，我们来和RNN做个对比。图2.8是个最基本的RNN结构，还有计算公式。当计算隐向量h4时，用到了输入x4，和上一步算出来的隐向量h3，h3包含了前面所有节点的信息。h4中包含最多的信息是当前的输入x4，越往前的输入，随着距离的增加，信息衰减得越多。对于每一个输出隐向量h都是如此，包含信息最多得是当前的输入，随着距离拉远，包含前面输入的信息越来越少。但是Transformer这个结构就不存在这个问题，不管当前词和其他词的空间距离有多远，包含其他词的信息不取决于距离，而是取决于两者的相关性，这是Transformer的第一个优势。第二个优势在于，对于Transformer来说，在对当前词进行计算的时候，不仅可以用到前面的词，也可以用到后面的词。而RNN只能用到前面的词，这并不是个严重的问题，因为这可以通过双向RNN来解决。第三点，RNN是一个顺序的结构，必须要一步一步地计算，只有计算出h1，才能计算h2，再计算h3，隐向量无法同时并行计算，导致RNN的计算效率不高，这是RNN的固有结构所造成的，之前有一些工作就是在研究如何对RNN的计算并行化。通过前文的介绍，可以看到Transformer不存在这个问题。通过这里的比较，可以看到Transformer相对于RNN有巨大的优势，因此我看到有人说RNN以后会被取代。\n图2.8\n关于上面的第三点优势，可能有人会不认可，RNN的结构包含了序列的时序信息，而Transformer却完全把时序信息给丢掉了。为了解决时序的问题，Transformer的作者用了一个绝妙的办法，这就是我在前文提到的位置编码（Positional Encoding）。位置编码是和word embedding同样维度的向量，将位置embedding和词embedding加在一起，作为输入embedding，如图2.9所示。位置编码可以通过学习得到，也可以通过设置一个跟位置或者时序相关的函数得到，比如设置一个正弦或者余弦函数，这里不再多说。\n图2.9\n我们把图2.1的结构作为一个基本单元，把N个这样的基本单元顺序连起来，就是BERT的算法模型，如图2.10所示。从前面的描述中可以看到，当输入有多少个embedding，那么输出也就有相同数量的embedding，可以采用和RNN采用相同的叫法，把输出叫做隐向量。在做具体NLP任务的时候，只需要从中取对应的隐向量作为输出即可。\n图2.10\n三，BERT\n在介绍BERT之前，我们先看看另外一套方案。我在第一部分说过，BERT并不是第一个提出预训练加微调的方案，此前还有一套方案叫GPT，这也是BERT重点对比的方案，文章在这，Improving Language Understanding by Generative Pre-Training（https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf）。GPT的模型结构和BERT是相同的，都是图2.10的结构，只是BERT的模型规模更加庞大。GPT是这么预训练的，在一个8亿单词的语料库上做训练，给出前文，不断地预测下一个单词。比如这句话，Winter is coming，当给出第一个词Winter之后，预测下一个词is，之后再预测下一个词coming。不需要标注数据，通过这种无监督训练的方式，得到一个预训练模型。\n我们再来看看BERT有什么不同。BERT来自于Bidirectional Encoder Representations from Transformers首字母缩写，这里提到了一个双向（Bidirectional）的概念。BERT在一个33亿单词的语料库上做预训练，语料库就要比GPT大了几倍。预训练包括了两个任务，第一个任务是随机地扣掉15%的单词，用一个掩码MASK代替，让模型去猜测这个单词；第二个任务是，每个训练样本是一个上下句，有50%的样本，下句和上句是真实的，另外50%的样本，下句和上句是无关的，模型需要判断两句的关系。这两个任务各有一个loss，将这两个loss加起来作为总的loss进行优化。下面两行是一个小栗子，用括号标注的是扣掉的词，用[MASK]来代替。\n正样本：我[MASK]（是）个算法工程师，我服务于WiFi万能钥匙这家[MASK]（公司）。\n负样本：我[MASK]（是）个算法工程师，今天[MASK]（股票）又跌了。\n我们来对比下GPT和BERT两种预训练方式的优劣。GPT在预测词的时候，只预测下一个词，因此只能用到上文的信息，无法利用到下文的信息。而BERT是预测文中扣掉的词，可以充分利用到上下文的信息，这使得模型有更强的表达能力，这也是BERT中Bidirectional的含义。在一些NLP任务中需要判断句子关系，比如判断两句话是否有相同的含义。BERT有了第二个任务，就能够很好的捕捉句子之间的关系。图3.1是BERT原文中对另外两种方法的预训练对比，包括GPT和ELMo。ELMo采用的还是LSTM，这里我们不多讲ELMo。这里会有读者困惑，这里的结构图怎么跟图2.10不一样？如果熟悉LSTM的同学，看到最右边的ELMo，就会知道那些水平相连的LSTM其实只是一个LSTM单元。左边的BERT和GPT也是一样，水平方向的Trm表示的是同一个单元，图中那些复杂的连线表示的是词与词之间的依赖关系，BERT中的依赖关系既有前文又有后文，而GPT的依赖关系只有前文。\n图3.1\n讲完了这两个任务，我们再来看看，如何表达这么复杂的一个训练样本，让计算机能够明白。图3.2表示“my dog is cute, he likes playing.”的输入形式。每个符号的输入由3部分构成，一个是词本身的embedding；第二个是表示上下句的embedding，如果是上句，就用A embedding，如果是下句，就用B embedding；最后，根据Transformer模型的特点，还要加上位置embedding，这里的位置embedding是通过学习的方式得到的，BERT设计一个样本最多支持512个位置；将3个embedding相加，作为输入。需要注意的是，在每个句子的开头，需要加一个Classification（CLS）符号，后文中会进行介绍，其他的一些小细节就不说了。\n图3.2\n完成预训练之后，就要针对特定任务就行微调了，这里描述一下论文中的4个例子，看图3.4。首先说下分类任务，分类任务包括对单句子的分类任务，比如判断电影评论是喜欢还是讨厌；多句子分类，比如判断两句话是否表示相同的含义。图3.4（a）（b）是对这类任务的一个示例，左边表示两个句子的分类，右边是单句子分类。在输出的隐向量中，取出CLS对应的向量C，加一层网络W，并丢给softmax进行分类，得到预测结果P，计算过程如图3.3中的计算公式。在特定任务数据集中对Transformer模型的所有参数和网络W共同训练，直到收敛。新增加的网络W是HxK维，H表示隐向量的维度，K表示分类数量，W的参数数量相比预训练模型的参数少得可怜。\n图3.3\n图3.4\n我们再来看问答任务，如图3.4（c），以SQuAD v1.1为例，给出一个问题Question，并且给出一个段落Paragraph，然后从段落中标出答案的具体位置。需要学习一个开始向量S，维度和输出隐向量维度相同，然后和所有的隐向量做点积，取值最大的词作为开始位置；另外再学一个结束向量E，做同样的运算，得到结束位置。附加一个条件，结束位置一定要大于开始位置。最后再看NER任务，实体命名识别，比如给出一句话，对每个词进行标注，判断属于人名，地名，机构名，还是其他。如图3.4（d）所示，加一层分类网络，对每个输出隐向量都做一次判断。可以看到，这些任务，都只需要新增少量的参数，然后在特定数据集上进行训练即可。从实验结果来看，即便是很小的数据集，也能取得不错的效果。\n到此，本文对BERT做了完整的介绍，如有疑问，欢迎留言~","data":"2018年12月23日 23:54:42"}
{"_id":{"$oid":"5d345c0462f717dc0659b986"},"title":"十分钟学习自然语言处理概述","author":"starzhou","content":"十分钟学习自然语言处理概述\n\n摘要：近来自然语言处理行业发展朝气蓬勃，市场应用广泛。笔者学习以来写了不少文章，文章深度层次不一，今天因为某种需要，将文章全部看了一遍做个整理，也可以称之为概述。关于这些问题，博客里面都有详细的文章去介绍，本文只是对其各个部分高度概括梳理。（本文原创，转载注明出处：十分钟学习自然语言处理概述  )\n1 什么是文本挖掘？\n文本挖掘是信息挖掘的一个研究分支，用于基于文本信息的知识发现。文本挖掘的准备工作由文本收集、文本分析和特征修剪三个步骤组成。目前研究和应用最多的几种文本挖掘技术有：文档聚类、文档分类和摘要抽取。\n2 什么是自然语言处理？\n自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。它研究人与计算机之间用自然语言进行有效通信的理论和方法。融语言学、计算机科学、数学等于一体的科学。\n自然语言处理原理：形式化描述-数学模型算法化-程序化-实用化\n语音的自动合成与识别、机器翻译、自然语言理解、人机对话、信息检索、文本分类、自动文摘等。\n3 常用中文分词？\n中文文本词与词之间没有像英文那样有空格分隔，因此很多时候中文文本操作都涉及切词，这里整理了一些中文分词工具。\nStanford（直接使用CRF 的方法，特征窗口为5。）\n汉语分词工具（个人推荐）\n哈工大语言云\n庖丁解牛分词\n盘古分词  ICTCLAS（中科院）汉语词法分析系统\nIKAnalyzer（Luence项目下，基于java的）\nFudanNLP(复旦大学)\n4 词性标注方法？句法分析方法？\n原理描述：标注一篇文章中的句子，即语句标注，使用标注方法BIO标注。则观察序列X就是一个语料库（此处假设一篇文章，x代表文章中的每一句，X是x的集合），标识序列Y是BIO，即对应X序列的识别，从而可以根据条件概率P(标注|句子)，推测出正确的句子标注。\n显然，这里针对的是序列状态，即CRF是用来标注或划分序列结构数据的概率化结构模型，CRF可以看作无向图模型或者马尔科夫随机场。   用过CRF的都知道，CRF是一个序列标注模型，指的是把一个词序列的每个词打上一个标记。一般通过，在词的左右开一个小窗口，根据窗口里面的词，和待标注词语来实现特征模板的提取。最后通过特征的组合决定需要打的tag是什么。\n5 命名实体识别？三种主流算法，CRF，字典法和混合方法\n1 CRF：在CRF for Chinese NER这个任务中，提取的特征大多是该词是否为中国人名姓氏用字，该词是否为中国人名名字用字之类的，True or false的特征。所以一个可靠的百家姓的表就十分重要啦~在国内学者做的诸多实验中，效果最好的人名可以F1测度达到90%，最差的机构名达到85%。\n2 字典法：在NER中就是把每个字都当开头的字放到trie-tree中查一遍，查到了就是NE。中文的trie-tree需要进行哈希，因为中文字符太多了，不像英文就26个。\n3 对六类不同的命名实体采取不一样的手段进行处理，例如对于人名，进行字级别的条件概率计算。   中文：哈工大（语言云）上海交大    英文：stanfordNER等\n7 基于主动学习的中医文献句法识别研究\n7.1 语料库知识？\n语料库作为一个或者多个应用目标而专门收集的，有一定结构的、有代表的、可被计算机程序检索的、具有一定规模的语料的集合。\n语料库划分：① 时间划分② 加工深度划分：标注语料库和非标注语料库③ 结构划分⑤ 语种划分⑥ 动态更新程度划分：参考语料库和监控语料库\n语料库构建原则：①   代表性②   结构性③   平衡性④   规模性⑤   元数据：元数据对\n语料标注的优缺点\n①   优点： 研究方便。可重用、功能多样性、分析清晰。\n②   缺点： 语料不客观（手工标注准确率高而一致性差，自动或者半自动标注一致性高而准确率差）、标注不一致、准确率低\n7.2 条件随机场解决标注问题？\n条件随机场用于序列标注，中文分词、中文人名识别和歧义消解等自然语言处理中，表现出很好的效果。原理是：对给定的观察序列和标注序列，建立条件概率模型。条件随机场可用于不同预测问题，其学习方法通常是极大似然估计。\n我爱中国，进行序列标注案例讲解条件随机场。（规则模型和统计模型问题）\n条件随机场模型也需要解决三个基本问题：特征的选择（表示第i个观察值为“爱”时，相对yi,yi-1的标记分别是B，I），参数训练和解码。\n7.3 隐马尔可夫模型\n应用：词类标注、语音识别、局部句法剖析、语块分析、命名实体识别、信息抽取等。应用于自然科学、工程技术、生物科技、公用事业、信道编码等多个领域。\n马尔可夫链：在随机过程中，每个语言符号的出现概率不相互独立，每个随机试验的当前状态依赖于此前状态，这种链就是马尔可夫链。\n多元马尔科夫链：考虑前一个语言符号对后一个语言符号出现概率的影响，这样得出的语言成分的链叫做一重马尔可夫链，也是二元语法。二重马尔可夫链，也是三元语法，三重马尔可夫链，也是四元语法\n隐马尔可夫模型思想的三个问题\n问题1（似然度问题）：给一个HMM λ=（A,B） 和一个观察序列O，确定观察序列的似然度问题 P(O|λ) 。（向前算法解决）\n问题2（解码问题）：给定一个观察序列O和一个HMM λ=（A,B），找出最好的隐藏状态序列Q。（维特比算法解决）\n问题3（学习问题）：给定一个观察序列O和一个HMM中的状态集合，自动学习HMM的参数A和B。（向前向后算法解决）\n7.4 Viterbi算法解码\n思路：\n1 计算时间步1的维特比概率\n2 计算时间步2的维特比概率，在（1） 基础计算\n3 计算时间步3的维特比概率，在（2） 基础计算\n4 维特比反向追踪路径\n维特比算法与向前算法的区别：\n（1）维特比算法要在前面路径的概率中选择最大值，而向前算法则计算其总和，除此之外，维特比算法和向前算法一样。\n（2）维特比算法有反向指针，寻找隐藏状态路径，而向前算法没有反向指针。\nHMM和维特比算法解决随机词类标注问题，利用Viterbi算法的中文句法标注\n7.5 序列标注方法       参照上面词性标注\n7.6 模型评价方法\n模型：方法=模型+策略+算法\n模型问题涉及：训练误差、测试误差、过拟合等问题。通常将学习方法对未知数据的预测能力称为泛化能力。\n模型评价参数：\n准确率P=识别正确的数量/全部识别出的数量\n错误率 =识别错误的数量/全部识别出的数量\n精度=识别正确正的数量/识别正确的数量\n召回率R=识别正确的数量/全部正确的总量（识别出+识别不出的）\nF度量=2PR/(P+R)\n数据正负均衡适合准确率    数据不均适合召回率，精度，F度量\n几种模型评估的方法：\nK-折交叉验证、随机二次抽样评估等    ROC曲线评价两个模型好坏\n8 基于文本处理技术的研究生英语等级考试词汇表构建系统\n完成对2002--2010年17套GET真题的核心单词抽取。其中包括数据清洗，停用词处理，分词，词频统计，排序等常用方法。真题算是结构化数据，有一定规则，比较容易处理。此过程其实就是数据清洗过程）最后把所有单词集中汇总，再去除如：a/an/of/on/frist等停用词（中文文本处理也需要对停用词处理，诸如：的，地，是等）。处理好的单词进行去重和词频统计，最后再利用网络工具对英语翻译。然后根据词频排序。\n8.1 Apache Tika？\nApache Tika内容抽取工具，其强大之处在于可以处理各种文件，另外节约您更多的时间用来做重要的事情。\nTika是一个内容分析工具，自带全面的parser工具类，能解析基本所有常见格式的文件\nTika的功能:•文档类型检测   •内容提取  •元数据提取  •语言检测\n8.2 文本词频统计？词频排序方法？\n算法思想：\n1 历年（2002—2010年）GET考试真题，文档格式不一。网上收集\n2 对所有格式不一的文档进行统计处理成txt文档，格式化（去除汉字/标点/空格等非英文单词）和去除停用词（去除891个停用词）处理。\n3 对清洗后的单词进行去重和词频统计，通过Map统计词频，实体存储：单词-词频。（数组也可以，只是面对特别大的数据，数组存在越界问题）。排序：根据词频或者字母\n4 提取核心词汇，大于5的和小于25次的数据，可以自己制定阈值。遍历list\u003c实体\u003e列表时候，通过获取实体的词频属性控制选取词汇表尺寸。\n5 最后一步，中英文翻译。\n9 朴素贝叶斯模型的文本分类器的设计与实现\n9.1 朴素贝叶斯公式\n0：喜悦  1：愤怒 2：厌恶 3：低落\n9.2 朴素贝叶斯原理\n--\u003e训练文本预处理，构造分类器。（即对贝叶斯公式实现文本分类参数值的求解，暂时不理解没关系，下文详解）\n--\u003e构造预测分类函数\n--\u003e对测试数据预处理\n--\u003e使用分类器分类\n对于一个新的训练文档d，究竟属于如上四个类别的哪个类别？我们可以根据贝叶斯公式，只是此刻变化成具体的对象。\n\u003e P( Category | Document)：测试文档属于某类的概率\n\u003e P( Category))：从文档空间中随机抽取一个文档d，它属于类别c的概率。（某类文档数目/总文档数目）\n\u003e (P ( Document | Category )：文档d对于给定类c的概率（某类下文档中单词数/某类中总的单词数）\n\u003e P(Document)：从文档空间中随机抽取一个文档d的概率（对于每个类别都一样，可以忽略不计算。此时为求最大似然概率）\n\u003e  C(d)=argmax {P(C_i)*P(d|c_i)}：求出近似的贝叶斯每个类别的概率，比较获取最大的概率，此时文档归为最大概率的一类，分类成功。\n综述\n1.  事先收集处理数据集（涉及网络爬虫和中文切词，特征选取）\n2.  预处理：（去掉停用词，移除频数过小的词汇【根据具体情况】）\n3.  实验过程：\n数据集分两部分（3:7）：30%作为测试集，70%作为训练集\n增加置信度：10-折交叉验证（整个数据集分为10等份，9份合并为训练集，余下1份作为测试集。一共运行10遍，取平均值作为分类结果）优缺点对比分析\n4. 评价标准：\n宏评价\u0026微评价\n平滑因子\n9.3 生产模型与判别模型区别\n1）生产式模型：直接对联合分布进行建模，如：隐马尔科夫模型、马尔科夫随机场等\n2）判别式模型：对条件分布进行建模，如：条件随机场、支持向量机、逻辑回归等。\n生成模型优点：1）由联合分布2）收敛速度比较快。3）能够应付隐变量。 缺点：为了估算准确，样本量和计算量大，样本数目较多时候不建议使用。\n判别模型优点：1）计算和样本数量少。2）准确率高。缺点：收敛慢，不能针对隐变量。\n9.4 ROC曲线\nROC曲线又叫接受者操作特征曲线，比较学习器模型好坏可视化工具，横坐标参数假正例率，纵坐标参数是真正例率。曲线越靠近对角线（随机猜测线）模型越不好。\n好的模型，真正比例比较多，曲线应是陡峭的从0开始上升，后来遇到真正比例越来越少，假正比例元组越来越多，曲线平缓变的更加水平。完全正确的模型面积为1\n10 统计学知识\n信息图形化（饼图，线形图等）\n集中趋势度量（平均值 中位数 众数 方差等）\n概率\n排列组合\n分布（几何二项泊松正态卡方）\n统计抽样\n样本估计\n假设检验\n回归\n11 stanfordNLP\n句子理解、自动问答系统、机器翻译、句法分析、标注、情感分析、文本和视觉场景和模型， 以及自然语言处理数字人文社会科学中的应用和计算。\n12 APache OpenNLP\nApache的OpenNLP库是自然语言文本的处理基于机器学习的工具包。它支持最常见的NLP任务，如断词，句子切分，部分词性标注，命名实体提取，分块，解析和指代消解。\n句子探测器:句子检测器是用于检测句子边界\n标记生成器:该OpenNLP断词段输入字符序列为标记。常是这是由空格分隔的单词，但也有例外。\n名称搜索:名称查找器可检测文本命名实体和数字。\nPOS标注器:该OpenNLP POS标注器使用的概率模型来预测正确的POS标记出了标签组。\n细节化:文本分块由除以单词句法相关部分，如名词基，动词基的文字，但没有指定其内部结构，也没有其在主句作用。\n分析器:尝试解析器最简单的方法是在命令行工具。该工具仅用于演示和测试。请从我们网站上的英文分块\n13 Lucene\nLucene是一个基于Java的全文信息检索工具包，它不是一个完整的搜索应用程序，而是为你的应用程序提供索引和搜索功能。Lucene 目前是 Apache Jakarta(雅加达) 家族中的一个 开源项目。也是目前最为流行的基于Java开源全文检索工具包。\n目前已经有很多应用程序的搜索功能是基于 Lucene ，比如Eclipse 帮助系统的搜索功能。Lucene能够为文本类型的数 据建立索引，所以你只要把你要索引的数据格式转化的文本格式，Lucene 就能对你的文档进行索引和搜索。\n14 Apache Solr\nSolr它是一种开放源码的、基于 Lucene Java 的搜索服务器。Solr 提供了层面搜索(就是统计)、命中醒目显示并且支持多种输出格式。它易于安装和配置， 而且附带了一个基于HTTP 的管理界面。可以使用 Solr 的表现优异的基本搜索功能，也可以对它进行扩展从而满足企业的需要。\nSolr的特性包括：\n•高级的全文搜索功能\n•专为高通量的网络流量进行的优化\n•基于开放接口（XML和HTTP）的标准\n•综合的HTML管理界面\n•可伸缩性－能够有效地复制到另外一个Solr搜索服务器\n•使用XML配置达到灵活性和适配性\n•可扩展的插件体系 solr中文分词\n15 机器学习降维\n主要特征选取、随机森林、主成分分析、线性降维\n16 领域本体构建方法\n1 确定领域本体的专业领域和范畴\n2 考虑复用现有的本体\n3 列出本体涉及领域中的重要术语\n4 定义分类概念和概念分类层次\n5 定义概念之间的关系\n17 构建领域本体的知识工程方法：\n主要特点：本体更强调共享、重用，可以为不同系统提供一种统一的语言，因此本体构建的工程性更为明显。\n方法：目前为止，本体工程中比较有名的几种方法包括TOVE 法、Methontology方法、骨架法、IDEF-5法和七步法等。（大多是手工构建领域本体）\n现状： 由于本体工程到目前为止仍处于相对不成熟的阶段，领域本体的建设还处于探索期，因此构建过程中还存在着很多问题。\n方法成熟度： 以上常用方法的依次为:七步法、Methontology方法、IDEF-5法、TOVE法、骨架法。","data":"2017年07月23日 08:02:56"}
{"_id":{"$oid":"5d345c5662f717dc0659b995"},"title":"flask 第六章 人工智能 百度语音合成 识别 NLP自然语言处理+simnet短文本相似度 图灵机器人...","author":"weixin_34121304","content":"百度智能云文档链接 : https://cloud.baidu.com/doc/SPEECH/index.html\n1.百度语音合成\n概念: 顾名思义,就是将你输入的文字合成语音,例如:\nfrom aip import AipSpeech \"\"\" 你的 APPID AK SK \"\"\" APP_ID = '16027154' API_KEY = '5a8u0aLf2SxRGRMX3jbZ2VH0' SECRET_KEY = 'UAaqS13z6DjD9Qbjd065dAh0HjbqPrzV' #上面这些东西,都在我们的百度语音的应用列表中 client=AipSpeech(APP_ID,API_KEY,SECRET_KEY) result=client.synthesis('大噶好,吾系渣渣辉,系兄弟就来砍我','zh',1,{ 'spd' :4, 'vol' :5, 'pit' :8, 'per' :4 }) #识别征求返回语音二进制,错误则返回dict if not isinstance(result,dict): with open('audio.mp3','wb') as f: f.write(result)\n百度语音生成\n参数:\ntex : 合成的文本,使用UTF-8编码,注意文本长度必须小于1024字节   　　　　　　　　　   必须有\nculid :用户唯一标识,用来区分用户,填写机器的MAC地址或IMEI码, 长度60以内    　　　　不必须有\nspd :  语速 ,取值0-15,默认为5(中语速)                                           　　　　　　 　 不必须有\npit :   音调,取值0-15,默认为5(中语调)　　　　　　　　　　　　 　　　　　　  　　　　 不必须有\nvol : 音量,取值0-15,默认为5(中音量)　　　　　　　　　　　　　　　　  　　　　　　　不必须有\nper:发音人选择,0为女声,1位男声,3为情感合成-度逍遥,4为感情合成-度丫丫,默认为0  　　 不必须有\n2.百度语音识别\n概念: 同上,就是将你的音频文件的内容读出来,相当于电子书\nimport os from aip import AipSpeech \"\"\" 你的 APPID AK SK \"\"\" APP_ID = '16027160' API_KEY = 'uzx4SWZuimPqbE4LvxYScEhi' SECRET_KEY = '3HBy8yi11ID9T4yyxkADuGYOGyavxPdG' client = AipSpeech(APP_ID, API_KEY, SECRET_KEY) #语音合成,通过语音生成文字 def get_file_content(filePath): os.system(f'ffmpeg -y -i {filePath} -acodec pcm_s16le -f s16le -ac 1 -ar 16000 {filePath}.pcm') with open(f'{filePath}.pcm', 'rb') as fp: return fp.read() # 识别本地文件 ret = client.asr(get_file_content('nszm.m4a'), 'pcm', 16000, { 'dev_pid': 1536, })\n#得出音频文件中的内容,打印出来 print(ret.get('result')[0])\n3.百度NLP自然语言处理 -- simnet短文本相似度\nfrom aip import AipNlp\n\n\"\"\" 你的 APPID AK SK \"\"\"\nAPP_ID = '16027160'\nAPI_KEY = 'uzx4SWZuimPqbE4LvxYScEhi'\nSECRET_KEY = '3HBy8yi11ID9T4yyxkADuGYOGyavxPdG'\n\nclient_nlp = AipNlp(APP_ID, API_KEY, SECRET_KEY)\ntext='大噶好,吾系渣渣辉,是兄弟就来砍我'\n\n#这里算出来的是相似度(score)\nscore=client_nlp.simnet('你叫什么名字呀',text) if score \u003e=0.58: filename=执行某个函数 os.system(filename)\n4.对接图灵机器人 实现智能问答\n这里我说一下我的逻辑:\n我先通过语音合成弄了一个音频,然后通过语音识别读取出我音频的内容,最后通过连接图灵机器人,进行智能问答\nimport os from aip import AipSpeech, AipNlp \"\"\" 你的 APPID AK SK \"\"\" APP_ID = '16027160' API_KEY = 'uzx4SWZuimPqbE4LvxYScEhi' SECRET_KEY = '3HBy8yi11ID9T4yyxkADuGYOGyavxPdG' client = AipSpeech(APP_ID, API_KEY, SECRET_KEY) client_nlp = AipNlp(APP_ID, API_KEY, SECRET_KEY) # 语音识别,将你输入的文字转化为语音 def AI_voice(file): result = client.synthesis(file, 'zh', 1, { 'spd': 5, 'vol': 5, 'pit': 5, 'per': 2 }) if not isinstance(result, dict): with open('audio.mp3', 'wb') as f: f.write(result) return 'audio.mp3' # 语音合成,通过语音生成文字,在这里只是读出文字,并没有写出来,下面这一步才是将语音中的文字return出来 def get_file_content(file): os.system(f\"ffmpeg -y -i {file} -acodec pcm_s16le -f s16le -ac 1 -ar 16000 {file}.pcm\") with open(f'{file}.pcm', 'rb') as fp: return fp.read() # 返回的是你语音中的消息 def voice_content(file): result = client.asr(get_file_content(file), 'pcm', 16000, { 'dev_pid': 1536, }) # print(result.get('result')[0]) return result.get('result')[0] def goto_tl(text, uid): URL = \"http://openapi.tuling123.com/openapi/api/v2\" import requests data = { \"perception\": { \"inputText\": { \"text\": \"你叫什么名字\" } }, \"userInfo\": { \"apiKey\": \"be41cf8596a24aec95b0e86be895cfa9\", \"userId\": \"123\" } } data[\"perception\"][\"inputText\"][\"text\"] = text data[\"userInfo\"][\"userId\"] = uid res = requests.post(URL, json=data) print(res.content) # print(res.text) # print(res.json()) return res.json().get(\"results\")[0].get(\"values\").get(\"text\") text = voice_content(\"nszm.m4a\") # 自然语言的处理,比较low版 # 获取相似度 score = client_nlp.simnet('你叫什么名字', text).get('score') print(score) if score \u003e= 0.58: filename = AI_voice('我是你爸爸,我喜欢你妈妈') os.system(filename) # 将我语音中的内容识别出来,并进行返回, answer = goto_tl(text, 'XiaoQiang') name = AI_voice(answer) os.system(name)\n转载于:https://www.cnblogs.com/zty1304368100/p/10719949.html","data":"2019年04月16日 20:52:00"}
{"_id":{"$oid":"5d345c7862f717dc0659b99d"},"title":"自然语言处理怎么最快入门？","author":"starzhou","content":"自然语言处理怎么最快入门？\n最好是方法与教程\n关注者\n5552\n被浏览\n252054\n3 条评论\n分享\n邀请回答\n关注问题写回答\n27 个回答\n默认排序\n微软亚洲研究院\n专注科研18年，盛产黑科技\n收录于 编辑推荐、知乎圆桌 ·  524 人赞同了该回答\n谢邀。\n针对这个问题，我们邀请了微软亚洲研究院首席研究员周明博士为大家解答。\n周明博士于2016年12月当选为全球计算语言学和自然语言处理研究领域最具影响力的学术组织——计算语言学协会（ACL， Association for Computational Linguistics）的新一届候任主席。此外，他还是中国计算机学会中文信息技术专委会主任、中国中文信息学会常务理事、哈工大、天津大学、南开大学、山东大学等多所学校博士导师。他1985年毕业于重庆大学，1991年获哈工大博士学位。1991-1993年清华大学博士后，随后留校任副教授。1996-1999访问日本高电社公司主持中日机器翻译研究。他是中国第一个中英翻译系统、日本最有名的中日机器翻译产品J-北京的发明人。1999年加入微软研究院并随后负责自然语言研究组，主持研制了微软输入法、对联、英库词典、中英翻译等著名系统。近年来与微软产品组合作开发了小冰(中国)、Rinna（日本）等聊天机器人系统。他发表了100余篇重要会议和期刊论文。拥有国际发明专利40余项。\n\n\n————这里是正式回答的分割线————\n\n\n自然语言处理（简称NLP），是研究计算机处理人类语言的一门技术，包括：\n1.句法语义分析：对于给定的句子，进行分词、词性标记、命名实体识别和链接、句法分析、语义角色识别和多义词消歧。\n2.信息抽取：从给定文本中抽取重要的信息，比如，时间、地点、人物、事件、原因、结果、数字、日期、货币、专有名词等等。通俗说来，就是要了解谁在什么时候、什么原因、对谁、做了什么事、有什么结果。涉及到实体识别、时间抽取、因果关系抽取等关键技术。\n3.文本挖掘（或者文本数据挖掘）：包括文本聚类、分类、信息抽取、摘要、情感分析以及对挖掘的信息和知识的可视化、交互式的表达界面。目前主流的技术都是基于统计机器学习的。\n4.机器翻译：把输入的源语言文本通过自动翻译获得另外一种语言的文本。根据输入媒介不同，可以细分为文本翻译、语音翻译、手语翻译、图形翻译等。机器翻译从最早的基于规则的方法到二十年前的基于统计的方法，再到今天的基于神经网络（编码-解码）的方法，逐渐形成了一套比较严谨的方法体系。\n5.信息检索：对大规模的文档进行索引。可简单对文档中的词汇，赋之以不同的权重来建立索引，也可利用1，2，3的技术来建立更加深层的索引。在查询的时候，对输入的查询表达式比如一个检索词或者一个句子进行分析，然后在索引里面查找匹配的候选文档，再根据一个排序机制把候选文档排序，最后输出排序得分最高的文档。\n6.问答系统： 对一个自然语言表达的问题，由问答系统给出一个精准的答案。需要对自然语言查询语句进行某种程度的语义分析，包括实体链接、关系识别，形成逻辑表达式，然后到知识库中查找可能的候选答案并通过一个排序机制找出最佳的答案。\n7.对话系统：系统通过一系列的对话，跟用户进行聊天、回答、完成某一项任务。涉及到用户意图理解、通用聊天引擎、问答引擎、对话管理等技术。此外，为了体现上下文相关，要具备多轮对话能力。同时，为了体现个性化，要开发用户画像以及基于用户画像的个性化回复。\n随着深度学习在图像识别、语音识别领域的大放异彩，人们对深度学习在NLP的价值也寄予厚望。再加上AlphaGo的成功，人工智能的研究和应用变得炙手可热。自然语言处理作为人工智能领域的认知智能，成为目前大家关注的焦点。很多研究生都在进入自然语言领域，寄望未来在人工智能方向大展身手。但是，大家常常遇到一些问题。俗话说，万事开头难。如果第一件事情成功了，学生就能建立信心，找到窍门，今后越做越好。否则，也可能就灰心丧气，甚至离开这个领域。这里针对给出我个人的建议，希望我的这些粗浅观点能够引起大家更深层次的讨论。\n建议1：如何在NLP领域快速学会第一个技能？\n我的建议是：找到一个开源项目，比如机器翻译或者深度学习的项目。理解开源项目的任务，编译通过该项目发布的示范程序，得到与项目示范程序一致的结果。然后再深入理解开源项目示范程序的算法。自己编程实现一下这个示范程序的算法。再按照项目提供的标准测试集测试自己实现的程序。如果输出的结果与项目中出现的结果不一致，就要仔细查验自己的程序，反复修改，直到结果与示范程序基本一致。如果还是不行，就大胆给项目的作者写信请教。在此基础上，再看看自己能否进一步完善算法或者实现，取得比示范程序更好的结果。\n建议2：如何选择第一个好题目？\n工程型研究生，选题很多都是老师给定的。需要采取比较实用的方法，扎扎实实地动手实现。可能不需要多少理论创新，但是需要较强的实现能力和综合创新能力。而学术型研究生需要取得一流的研究成果，因此选题需要有一定的创新。我这里给出如下的几点建议。\n先找到自己喜欢的研究领域。你找到一本最近的ACL会议论文集, 从中找到一个你比较喜欢的领域。在选题的时候，多注意选择蓝海的领域。这是因为蓝海的领域，相对比较新，容易出成果。\n充分调研这个领域目前的发展状况。包括如下几个方面的调研：方法方面，是否有一套比较清晰的数学体系和机器学习体系；数据方面，有没有一个大家公认的标准训练集和测试集；研究团队，是否有著名团队和人士参加。如果以上几个方面的调研结论不是太清晰，作为初学者可能不要轻易进入。\n在确认进入一个领域之后，按照建议一所述，需要找到本领域的开源项目或者工具，仔细研究一遍现有的主要流派和方法，先入门。\n反复阅读本领域最新发表的文章，多阅读本领域牛人发表的文章。在深入了解已有工作的基础上，探讨还有没有一些地方可以推翻、改进、综合、迁移。注意做实验的时候，不要贪多，每次实验只需要验证一个想法。每次实验之后，必须要进行分析存在的错误，找出原因。\n对成功的实验，进一步探讨如何改进算法。注意实验数据必须是业界公认的数据。\n与已有的算法进行比较，体会能够得出比较一般性的结论。如果有，则去写一篇文章，否则，应该换一个新的选题。\n建议3：如何写出第一篇论文？\n接上一个问题，如果想法不错，且被实验所证明，就可开始写第一篇论文了。\n确定论文的题目。在定题目的时候，一般不要“…系统”、“…研究与实践”，要避免太长的题目，因为不好体现要点。题目要具体，有深度，突出算法。\n写论文摘要。要突出本文针对什么重要问题，提出了什么方法，跟已有工作相比，具有什么优势。实验结果表明，达到了什么水准，解决了什么问题。\n写引言。首先讲出本项工作的背景，这个问题的定义，它具有什么重要性。然后介绍对这个问题，现有的方法是什么，有什么优点。但是（注意但是）现有的方法仍然有很多缺陷或者挑战。比如（注意比如），有什么问题。本文针对这个问题，受什么方法（谁的工作）之启发，提出了什么新的方法并做了如下几个方面的研究。然后对每个方面分门别类加以叙述，最后说明实验的结论。再说本文有几条贡献，一般写三条足矣。然后说说文章的章节组织，以及本文的重点。有的时候东西太多，篇幅有限，只能介绍最重要的部分，不需要面面俱到。\n相关工作。对相关工作做一个梳理，按照流派划分，对主要的最多三个流派做一个简单介绍。介绍其原理，然后说明其局限性。\n然后可设立两个章节介绍自己的工作。第一个章节是算法描述。包括问题定义，数学符号，算法描述。文章的主要公式基本都在这里。有时候要给出简明的推导过程。如果借鉴了别人的理论和算法，要给出清晰的引文信息。在此基础上，由于一般是基于机器学习或者深度学习的方法，要介绍你的模型训练方法和解码方法。第二章就是实验环节。一般要给出实验的目的，要检验什么，实验的方法，数据从哪里来，多大规模。最好数据是用公开评测数据，便于别人重复你的工作。然后对每个实验给出所需的技术参数，并报告实验结果。同时为了与已有工作比较，需要引用已有工作的结果，必要的时候需要重现重要的工作并报告结果。用实验数据说话，说明你比人家的方法要好。要对实验结果好好分析你的工作与别人的工作的不同及各自利弊，并说明其原因。对于目前尚不太好的地方，要分析问题之所在，并将其列为未来的工作。\n结论。对本文的贡献再一次总结。既要从理论、方法上加以总结和提炼，也要说明在实验上的贡献和结论。所做的结论，要让读者感到信服，同时指出未来的研究方向。\n参考文献。给出所有重要相关工作的论文。记住，漏掉了一篇重要的参考文献（或者牛人的工作），基本上就没有被录取的希望了。\n写完第一稿，然后就是再改三遍。\n把文章交给同一个项目组的人士，请他们从算法新颖度、创新性和实验规模和结论方面，以挑剔的眼光，审核你的文章。自己针对薄弱环节，进一步改进，重点加强算法深度和工作创新性。\n然后请不同项目组的人士审阅。如果他们看不明白，说明文章的可读性不够。你需要修改篇章结构、进行文字润色，增加文章可读性。\n如投ACL等国际会议，最好再请英文专业或者母语人士提炼文字。\n\n\n————这里是回答结束的分割线————\n\n\n感谢大家的阅读。\n本帐号为微软亚洲研究院的官方知乎帐号。本帐号立足于计算机领域，特别是人工智能相关的前沿研究，旨在为人工智能的相关研究提供范例，从专业的角度促进公众对人工智能的理解，并为研究人员提供讨论和参与的开放平台，从而共建计算机领域的未来。\n微软亚洲研究院的每一位专家都是我们的智囊团，你在这个帐号可以阅读到来自计算机科学领域各个不同方向的专家们的见解。请大家不要吝惜手里的“邀请”，让我们在分享中共同进步。\n编辑于 2017-03-06\n524 23 条评论\n分享\n收藏 感谢 收起\n刘知远\n自然语言处理、深度学习（Deep Learning）、机器学习 话题的优秀回答者\n收录于 知乎圆桌 ·  463 人赞同了该回答\n曾经写过一篇小文，初学者如何查阅自然语言处理（NLP）领域学术资料_zibuyu_新浪博客，也许可以供你参考。\n\n\n昨天实验室一位刚进组的同学发邮件来问我如何查找学术论文，这让我想起自己刚读研究生时茫然四顾的情形：看着学长们高谈阔论领域动态，却不知如何入门。经过研究生几年的耳濡目染，现在终于能自信地知道去哪儿了解最新科研动态了。我想这可能是初学者们共通的困惑，与其只告诉一个人知道，不如将这些Folk Knowledge写下来，来减少更多人的麻烦吧。当然，这个总结不过是一家之谈，只盼有人能从中获得一点点益处，受个人认知所限，难免挂一漏万，还望大家海涵指正。\n\n\n1. 国际学术组织、学术会议与学术论文\n自然语言处理（natural language processing，NLP）在很大程度上与计算语言学（computational linguistics，CL）重合。与其他计算机学科类似，NLP/CL有一个属于自己的最权威的国际专业学会，叫做The Association for Computational Linguistics（ACL，URL：ACL Home Page），这个协会主办了NLP/CL领域最权威的国际会议，即ACL年会，ACL学会还会在北美和欧洲召开分年会，分别称为NAACL和EACL。除此之外，ACL学会下设多个特殊兴趣小组（special interest groups，SIGs），聚集了NLP/CL不同子领域的学者，性质类似一个大学校园的兴趣社团。其中比较有名的诸如SIGDAT（Linguistic data and corpus-based approaches to NLP）、SIGNLL（Natural Language Learning）等。这些SIGs也会召开一些国际学术会议，其中比较有名的就是SIGDAT组织的EMNLP（Conference on Empirical Methods on Natural Language Processing）和SIGNLL组织的CoNLL（Conference on Natural Language Learning）。此外还有一个International Committee on Computational Linguistics的老牌NLP/CL学术组织，它每两年组织一个称为International Conference on Computational Linguistics (COLING)的国际会议，也是NLP/CL的重要学术会议。NLP/CL的主要学术论文就分布在这些会议上。\n作为NLP/CL领域的学者最大的幸福在于，ACL学会网站建立了称作ACL Anthology的页面（URL：ACL Anthology），支持该领域绝大部分国际学术会议论文的免费下载，甚至包含了其他组织主办的学术会议，例如COLING、IJCNLP等，并支持基于Google的全文检索功能，可谓一站在手，NLP论文我有。由于这个论文集合非常庞大，并且可以开放获取，很多学者也基于它开展研究，提供了更丰富的检索支持，具体入口可以参考ACL Anthology页面上方搜索框右侧的不同检索按钮。\n与大部分计算机学科类似，由于技术发展迅速，NLP/CL领域更重视发表学术会议论文，原因是发表周期短，并可以通过会议进行交流。当然NLP/CL也有自己的旗舰学术期刊，发表过很多经典学术论文，那就是Computational Linguistics（URL：MIT Press Journals）。该期刊每期只有几篇文章，平均质量高于会议论文，时间允许的话值得及时追踪。此外，ACL学会为了提高学术影响力，也刚刚创办了Transactions of ACL（TACL，URL：Transactions of the Association for Computational Linguistics (ISSN: 2307-387X)），值得关注。值得一提的是这两份期刊也都是开放获取的。此外也有一些与NLP/CL有关的期刊，如ACM Transactions on Speech and Language Processing，ACM Transactions on Asian Language Information Processing，Journal of Quantitative Linguistics等等。\n根据Google Scholar Metrics 2013年对NLP/CL学术期刊和会议的评价，ACL、EMNLP、NAACL、COLING、LREC、Computational Linguistics位于前5位，基本反映了本领域学者的关注程度。\nNLP/CL作为交叉学科，其相关领域也值得关注。主要包括以下几个方面：（1）信息检索和数据挖掘领域。相关学术会议主要由美国计算机学会（ACM）主办，包括SIGIR、WWW、WSDM等；（2）人工智能领域。相关学术会议主要包括AAAI和IJCAI等，相关学术期刊主要包括Artificial Intelligence和Journal of AI Research；（3）机器学习领域，相关学术会议主要包括ICML，NIPS，AISTATS，UAI等，相关学术期刊主要包括Journal of Machine Learning Research（JMLR）和Machine Learning（ML）等。例如最近兴起的knowledge graph研究论文，就有相当一部分发表在人工智能和信息检索领域的会议和期刊上。实际上国内计算机学会（CCF）制定了“中国计算机学会推荐国际学术会议和期刊目录”（CCF推荐排名），通过这个列表，可以迅速了解每个领域的主要期刊与学术会议。\n最后，值得一提的是，美国Hal Daumé III维护了一个natural language processing的博客（natural language processing blog），经常评论最新学术动态，值得关注。我经常看他关于ACL、NAACL等学术会议的参会感想和对论文的点评，很有启发。另外，ACL学会维护了一个Wiki页面（ACL Wiki），包含了大量NLP/CL的相关信息，如著名研究机构、历届会议录用率，等等，都是居家必备之良品，值得深挖。\n\n\n2. 国内学术组织、学术会议与学术论文\n与国际上相似，国内也有一个与NLP/CL相关的学会，叫做中国中文信息学会（URL：中国中文信息学会）。通过学会的理事名单（中国中文信息学会）基本可以了解国内从事NLP/CL的主要单位和学者。学会每年组织很多学术会议，例如全国计算语言学学术会议（CCL）、全国青年计算语言学研讨会（YCCL）、全国信息检索学术会议（CCIR）、全国机器翻译研讨会（CWMT），等等，是国内NLP/CL学者进行学术交流的重要平台。尤其值得一提的是，全国青年计算语言学研讨会是专门面向国内NLP/CL研究生的学术会议，从组织到审稿都由该领域研究生担任，非常有特色，也是NLP/CL同学们学术交流、快速成长的好去处。值得一提的是，2010年在北京召开的COLING以及2015年即将在北京召开的ACL，学会都是主要承办者，这也一定程度上反映了学会在国内NLP/CL领域的重要地位。此外，计算机学会中文信息技术专委会组织的自然语言处理与中文计算会议（NLP\u0026CC）也是最近崛起的重要学术会议。中文信息学会主编了一份历史悠久的《中文信息学报》，是国内该领域的重要学术期刊，发表过很多篇重量级论文。此外，国内著名的《计算机学报》、《软件学报》等期刊上也经常有NLP/CL论文发表，值得关注。\n过去几年，在水木社区BBS上开设的AI、NLP版面曾经是国内NLP/CL领域在线交流讨论的重要平台。这几年随着社会媒体的发展，越来越多学者转战新浪微博，有浓厚的交流氛围。如何找到这些学者呢，一个简单的方法就是在新浪微博搜索的“找人”功能中检索“自然语言处理”、 “计算语言学”、“信息检索”、“机器学习”等字样，马上就能跟过去只在论文中看到名字的老师同学们近距离交流了。还有一种办法，清华大学梁斌开发的“微博寻人”系统（清华大学信息检索组）可以检索每个领域的有影响力人士，因此也可以用来寻找NLP/CL领域的重要学者。值得一提的是，很多在国外任教的老师和求学的同学也活跃在新浪微博上，例如王威廉（Sina Visitor System）、李沐（Sina Visitor System）等，经常爆料业内新闻，值得关注。还有，国内NLP/CL的著名博客是52nlp（我爱自然语言处理），影响力比较大。总之，学术研究既需要苦练内功，也需要与人交流。所谓言者无意、听者有心，也许其他人的一句话就能点醒你苦思良久的问题。无疑，博客微博等提供了很好的交流平台，当然也注意不要沉迷哦。\n\n\n3. 如何快速了解某个领域研究进展\n最后简单说一下快速了解某领域研究进展的经验。你会发现，搜索引擎是查阅文献的重要工具，尤其是谷歌提供的Google Scholar，由于其庞大的索引量，将是我们披荆斩棘的利器。\n当需要了解某个领域，如果能找到一篇该领域的最新研究综述，就省劲多了。最方便的方法还是在Google Scholar中搜索“领域名称 + survey / review / tutorial / 综述”来查找。也有一些出版社专门出版各领域的综述文章，例如NOW Publisher出版的Foundations and Trends系列，Morgan \u0026 Claypool Publisher出版的Synthesis Lectures on Human Language Technologies系列等。它们发表了很多热门方向的综述，如文档摘要、情感分析和意见挖掘、学习排序、语言模型等。\n如果方向太新还没有相关综述，一般还可以查找该方向发表的最新论文，阅读它们的“相关工作”章节，顺着列出的参考文献，就基本能够了解相关研究脉络了。当然，还有很多其他办法，例如去http://videolectures.net上看著名学者在各大学术会议或暑期学校上做的tutorial报告，去直接咨询这个领域的研究者，等等。\n编辑于 2015-07-16\n463 17 条评论\n分享\n收藏 感谢 收起\n知乎用户\n自然语言处理 话题的优秀回答者\n收录于 编辑推荐 ·  1267 人赞同了该回答\n推荐《数学之美》，这个书写得特别科普且生动形象，我相信你不会觉得枯燥。这个我极力推荐，我相信科研的真正原因是因为兴趣，而不是因为功利的一些东西。\n\n接下来说，《统计自然语言处理基础》这本书，这书实在是太老了，但是也很经典，看不看随意了。\n\n现在自然语言处理都要靠统计学知识，所以我十分十分推荐《统计学习方法》，李航的。李航老师用自己课余时间7年写的，而且有博士生Review的。自然语言处理和机器学习不同，机器学习依靠的更多是严谨的数学知识以及推倒，去创造一个又一个机器学习算法。而自然语言处理是把那些机器学习大牛们创造出来的东西当Tool使用。所以入门也只是需要涉猎而已，把每个模型原理看看，不一定细致到推倒。\n\n宗成庆老师 的统计自然语言处理第二版非常好~《中文信息处理丛书：统计自然语言处理（第2版）》 蓝色皮的~~~\n然后就是Stanford公开课了，Stanford公开课要求一定的英语水平。| Coursera 我觉得讲的比大量的中国老师好~\n举例：\nhttp://www.ark.cs.cmu.edu/LS2/in...\n或者\nhttp://www.stanford.edu/class/cs...\n\n如果做工程前先搜索有没有已经做好的工具，不要自己从头来。做学术前也要好好的Survey！\n\n开始推荐工具包：\n中文的显然是哈工大开源的那个工具包 LTP (Language Technology Platform) developed by HIT-SCIR(哈尔滨工业大学社会计算与信息检索研究中心).\n\n英文的(python)：\n\npattern - simpler to get started than NLTK\nchardet - character encoding detection\npyenchant - easy access to dictionaries\nscikit-learn - has support for text classification\nunidecode - because ascii is much easier to deal with\n\n希望可以掌握以下的几个tool：\nCRF++\nGIZA\nWord2Vec\n\n还记得小时候看过的数码宝贝，每个萌萌哒的数码宝贝都会因为主人身上发生的一些事情而获得进化能力，其实在自然语言处理领域我觉得一切也是这样~ 我简单的按照自己的见解总结了每个阶段的特征，以及提高的解决方案\n1.幼年体——自然语言处理好屌，我什么都不会但是好想提高\n建议。。。去看公开课~去做Kaggle的那个情感分析题。\n2.成长期——觉得简单模型太Naive，高大上的才是最好的\n这个阶段需要自己动手实现一些高级算法，或者说常用算法，比如LDA，比如SVM，比如逻辑斯蒂回归。并且拥抱Kaggle，知道trick在这个领域的重要性。\n3.成熟期——高大上的都不work，通过特征工程加规则才work\n大部分人应该都在这个级别吧，包括我自己，我总是想进化，但积累还是不够。觉得高大上的模型都是一些人为了paper写的，真正的土方法才是重剑无锋，大巧不工。在这个阶段，应该就是不断读论文，不断看各种模型变种吧，什么句子相似度计算word2vec cosine已经不再适合你了。\n4.完全体——在公开数据集上，把某个高大上的模型做work了~\n这类应该只有少数博士可以做到吧，我已经不知道到了这个水平再怎么提高了~是不是只能说不忘初心，方得始终。\n5.究极体——参见Micheal Jordan Andrew Ng.\n好好锻炼身体，保持更长久的究极体形态\n\n\n\n\n希望可以理解自然语言处理的基本架构~：分词=\u003e词性标注=\u003eParser\n\nQuora上推荐的NLP的论文（摘自Quora 我过一阵会翻译括号里面的解释）：\nParsing（句法结构分析~语言学知识多，会比较枯燥）\n\nKlein \u0026 Manning: \"Accurate Unlexicalized Parsing\" ( )\nKlein \u0026 Manning: \"Corpus-Based Induction of Syntactic Structure: Models of Dependency and Constituency\" (革命性的用非监督学习的方法做了parser)\nNivre \"Deterministic Dependency Parsing of English Text\" (shows that deterministic parsing actually works quite well)\nMcDonald et al. \"Non-Projective Dependency Parsing using Spanning-Tree Algorithms\" (the other main method of dependency parsing, MST parsing)\n\nMachine Translation（机器翻译，如果不做机器翻译就可以跳过了，不过翻译模型在其他领域也有应用）\n\nKnight \"A statistical MT tutorial workbook\" (easy to understand, use instead of the original Brown paper)\nOch \"The Alignment-Template Approach to Statistical Machine Translation\" (foundations of phrase based systems)\nWu \"Inversion Transduction Grammars and the Bilingual Parsing of Parallel Corpora\" (arguably the first realistic method for biparsing, which is used in many systems)\nChiang \"Hierarchical Phrase-Based Translation\" (significantly improves accuracy by allowing for gappy phrases)\n\nLanguage Modeling (语言模型)\n\nGoodman \"A bit of progress in language modeling\" (describes just about everything related to n-gram language models 这是一个survey，这个survey写了几乎所有和n-gram有关的东西，包括平滑 聚类)\nTeh \"A Bayesian interpretation of Interpolated Kneser-Ney\" (shows how to get state-of-the art accuracy in a Bayesian framework, opening the path for other applications)\n\nMachine Learning for NLP\n\nSutton \u0026 McCallum \"An introduction to conditional random fields for relational learning\" (CRF实在是在NLP中太好用了！！！！！而且我们大家都知道有很多现成的tool实现这个，而这个就是一个很简单的论文讲述CRF的，不过其实还是蛮数学= =。。。)\nKnight \"Bayesian Inference with Tears\" (explains the general idea of bayesian techniques quite well)\nBerg-Kirkpatrick et al. \"Painless Unsupervised Learning with Features\" (this is from this year and thus a bit of a gamble, but this has the potential to bring the power of discriminative methods to unsupervised learning)\nInformation Extraction\n\nHearst. Automatic Acquisition of Hyponyms from Large Text Corpora. COLING 1992. (The very first paper for all the bootstrapping methods for NLP. It is a hypothetical work in a sense that it doesn't give experimental results, but it influenced it's followers a lot.)\nCollins and Singer. Unsupervised Models for Named Entity Classification. EMNLP 1999. (It applies several variants of co-training like IE methods to NER task and gives the motivation why they did so. Students can learn the logic from this work for writing a good research paper in NLP.)\nComputational Semantics\nGildea and Jurafsky. Automatic Labeling of Semantic Roles. Computational Linguistics 2002. (It opened up the trends in NLP for semantic role labeling, followed by several CoNLL shared tasks dedicated for SRL. It shows how linguistics and engineering can collaborate with each other. It has a shorter version in ACL 2000.)\nPantel and Lin. Discovering Word Senses from Text. KDD 2002. (Supervised WSD has been explored a lot in the early 00's thanks to the senseval workshop, but a few system actually benefits from WSD because manually crafted sense mappings are hard to obtain. These days we see a lot of evidence that unsupervised clustering improves NLP tasks such as NER, parsing, SRL, etc,\n其实我相信，大家更感兴趣的是上层的一些应用~而不是如何实现分词，如何实现命名实体识别等等。而且应该大家更对信息检索感兴趣。不过自然语言处理和信息检索还是有所区别的，So~~~我就不在这边写啦\n编辑于 2015-10-06\n1.3K 29 条评论\n分享\n收藏 感谢 收起\n知乎用户\n228 人赞同了该回答\n不请自来，语言学背景，研二。废话不说，直接上货。\n书籍篇：\n入门书籍挺多的，我也看过不少。\n1）《数学之美》（吴军）\n这是我看的第一本关于NLP的书。现在第二版出来了，貌似新增了两章内容，还没看过。第一版写的挺好，科普性质。看完对于nlp的许多技术原理都有了一点初步认识。现在没事还会翻翻的。\n2）《自然语言处理简明教程》（冯志伟）\n冯志伟老师这本书，偏向于语言学，书略厚。关于语言学的东西很多。都是很容易理解的东西。建议没有学过理工科们翻一翻，毕竟nlp这东西未来趋势可能会融合不少语言学的东西。\n3）《自然语言处理综论》（Daniel Jurafsky）\n这本书也是冯志伟老师翻译的，翻译的挺棒，看了差不多一半。综论性质的，选感兴趣的章节翻翻就行。作者是Daniel Jurafsky，在coursera上面有他的课程，后面视频篇里集中谈。\n4）《自然语言处理的形式模型》（冯志伟）\n这本书还是冯志伟老师写的。很佩服冯志伟老师，文理兼修，而且都很厉害。内容许多是从他以前的著作里面摘取的。算是一本各种语言模型和统计模型的大集合吧。放在桌面，没事翻翻也能是极好的。\n5）《统计自然语言处理(第2版)》（宗成庆）\n这本书我觉得写的不错。虽然我是语言学背景，但读起来也没有太吃力。它也是综论性质的，可以跳着看。\n6）《统计学习方法》（李航）\n自然语言处理需要些机器学习的知识。我数学基础还是太薄弱，有的内容还是有些吃力和困惑的。\n7）《机器学习实战》哈林顿 (Peter Harrington)\n《Python自然语言处理》\n《集体智慧编程》\n这些书都是python相关的。中间那本就是将NLTK的。网上都有电子版，需要的时候翻一番看一看就行。\n视频篇：\n\n@吴俣\n上面提到的，斯坦福的nlp课程Video Listing，哥伦比亚大学的https://class.coursera.org/nlangp-001，两个都是英文的，无中文字幕，现在还可以下载视频和课件。\n另外超星学术视频：\n1）自然语言理解_宗成庆\n我觉得讲的还是不错的，第一次听的时候有些晕乎。该课程网上有ppt讲义。讲义后来被作者写成了书，就是上面提到的《统计自然语言处理》。拿着书就是教材，还有课程ppt和视频可以看，这种感觉还是很好的。\n2）自然语言处理_关毅\n感觉讲的一般，听了几节，跳跃太多，有时候让人摸不着头脑。多听听还是很有益处的吧。\n3）计算语言学概论_侯敏\n这个就是语言学内容为主了，作者也是语言学背景下在nlp比较活跃的。讲的很浅。老师讲课很啰嗦，说话太慢，我都是加速看的。\n4)计算语言学_冯志伟\n冯志伟老师这个课，一如他的著作，语言学和统计都会涉及到一些。冯志伟老师说话有些地方听不大清，要是有字幕就好了。\n5）语法分析_陆俭明\n这是纯语言学的课程。陆剑明也是当代语言学的大师。我觉得既然是自然语言处理，语言学的东西，还是多少要了解的。\n其他篇：\n1）博客的话，我爱自然语言处理专门记录nlp的，很不错，再有就是csdn上一些比较琐碎的了。\n2）北京大学中文系 应用语言学专业这个刚开始的时候也看了看，又不少干货。\n3）《中文信息学报》说这个，不会被大神喷吧。英语不佳，英文文献实在看的少。这个学报，也是挑着看看就行。\n好像就是这些内容了。如果有，日后再补。\n虽然自己写了这么多，但不少书和视频都没有完整的看完。现在水平仍很菜，仍在进阶的路上。希望各路大神多多指点，该拍砖就拍吧。\n编辑于 2014-12-19\n228 21 条评论\n分享\n收藏 感谢 收起\n何史提\n物理学、理论物理、量子物理 话题的优秀回答者\n10 人赞同了该回答\n看Coursera相关的课程，或参考：Manning and Shcutze, Foundations of Statistical Natural Language Processing\n但更重要的还是实战经验！\n发布于 2014-07-08\n10 4 条评论\n分享\n收藏 感谢\n陈见耸\nRokid A-Lab 自然语言处理、人工智能、机器学习\n23 人赞同了该回答\n大家回答的都挺不错了，只好来强答。\n一、独立实现一个小型的自然语言处理项目。\n要找一个合适的的自然语言处理相关的开源项目。这个项目可以是与自己工作相关的，也可以是自己感兴趣的。项目不要太大，以小型的算法模块为佳，这样便于独立实现。像文本领域的文本分类、分词等项目就是比较合适的项目。 运行程序得到项目所声称的结果。然后看懂程序，这期间一般需要阅读程序实现所参考的文献。最后，自己尝试独立实现该算法，得到与示例程序相同的结果。再进一步的，可以调试参数，了解各参数对效果的影响，看是否能得到性能更好的参数组合。\n这一阶段主要是学习快速上手一个项目，从而对自然语言处理的项目有比较感性的认识——大体了解自然语言处理算法的原理、实现流程等。\n当我们对自然语言处理项目有了一定的认识之后，接下来就要深入进去。任何自然语言处理应用都包含算法和所要解决的问题两方面，要想深入进去就需要从这两方面进行着手。\n二、对问题进行深入认识\n对问题的深入认识通常来源于两个方面，一是阅读当前领域的文献，尤其是综述性的文献，理解当前领域所面临的主要问题、已有的解决方案有哪些、有待解决的问题有哪些。这里值得一提的是，博士生论文的相关文献介绍部分通常会对本问题做比较详细的介绍，也是比较好的综述类材料。\n除了从文献中获取对问题的认识外，另一种对问题进行深入认识的直观方法就是对算法得出的结果进行bad case分析，总结提炼出一些共性的问题。对bad case进行分析还有一个好处，可以帮助我们了解哪些问题是主要问题，哪些问题是次要问题，从而可以帮助我们建立问题优先级。如果有具体任务的真实数据，一定要在真实数据上进行测试。这是因为，即使是相同的算法，在不同的数据集上，所得到的结果也可能相差很大。\n三、对算法进行深入理解\n除了具体的问题分析，对算法的理解是学习人工智能必须要过的关。经过这么多年的发展，机器学习、模式识别的算法已经多如牛毛。幸运的是，这方面已经有不少好的书籍可供参考。这里推荐华为李航的蓝宝书《统计学习方法》和周志华的西瓜书《机器学习》，这两本都是国内顶级的机器学习专家撰写的书籍，思路清晰，行文流畅，样例丰富。\n如果觉得教科书稍感乏味，那我推荐吴军的《数学之美》，这是一本入门级的科普读物，作者以生动有趣的方式，深入浅出的讲解了很多人工智能领域的算法，相信你一定会有兴趣。\n国外的书籍《Pattern Recognition and Machine Learning》主要从概率的角度解释机器学习的各种算法，也是不可多得的入门教材。如果要了解最新的深度学习的相关算法，可以阅读被誉为深度学习三架马车之一Bengio所著的《Deep Learning》。 在学习教材时，对于应用工程师来说，重要的是理解算法的原理，从而掌握什么数据情况下适合什么样的数据，以及参数的意义是什么。\n四、深入到领域前沿\n自然语言处理领域一直处在快速的发展变化当中，不管是综述类文章还是书籍，都不能反映当前领域的最新进展。如果要进一步的了解领域前沿，那就需要关注国际顶级会议上的最新论文了。下面是各个领域的一些顶级会议。这里值得一提的是，和其他人工智能领域类似，自然语言处理领域最主要的学术交流方式就会议论文，这和其他领域比如数学、化学、物理等传统领域都不太一样，这些领域通常都以期刊论文作为最主要的交流方式。 但是期刊论文审稿周期太长，好的期刊，通常都要两三年的时间才能发表，这完全满足不了日新月异的人工智能领域的发展需求，因此，大家都会倾向于在审稿周期更短的会议上尽快发表自己的论文。 这里列举了国际和国内文本领域的一些会议，以及官网，大家可以自行查看。\n国际上的文本领域会议：\nACL：http://acl2017.org/ 加拿大温哥华 7.30-8.4\nEMNLP：http://emnlp2017.net/ 丹麦哥本哈根 9.7-9.11\nCOLING：没找到2017年的\n国内会议：\nCCKS http://www.ccks2017.com/index.php/att/ 成都 8月26-8月29\nSMP http://www.cips-smp.org/smp2017/ 北京 9.14-9.17\nCCL http://www.cips-cl.org:8080/CCL2017/home.html 南京 10.13-10.15\nNLPCC http://tcci.ccf.org.cn/conference/2017/ 大连 11.8-11.12\nNCMMSC http://www.ncmmsc2017.org/index.html 连云港 11.11 － 11.13\n像paperweekly，机器学习研究会，深度学习大讲堂等微信公众号，也经常会探讨一些自然语言处理的最新论文，是不错的中文资料。\n五、当然，工欲善其事，必先利其器。我们要做好自然语言处理的项目，还需要熟练掌握至少一门工具。当前，深度学习相关的工具已经比较多了，比如：tensorflow、mxnet、caffe、theano、cntk等。这里向大家推荐tensorflow，自从google推出之后，tensorflow几乎成为最流行的深度学习工具。究其原因，除了google的大力宣传之外，tensorflow秉承了google开源项目的一贯风格，社区力量比较活跃，目前github上有相当多数量的以tensorflow为工具的项目，这对于开发者来说是相当大的资源。\n以上就是对于没有自然语言处理项目经验的人来说，如何学习自然语言处理的一些经验，希望对大家能有所帮助。\n\n\n发布于 2017-05-10\n23 2 条评论\n分享\n收藏 感谢 收起\n杨智\n互联网\n44 人赞同了该回答\n说说自己的历程吧。\n我是一名非科班的自然语言，机器学习，数据挖掘关注者。\n因工作关系，5年前需要做与自然语言处理的项目。当时的项目老大先是扔给我一本书《统计自然语言处理》，直接给我看蒙了。不能说一点都不懂，但是看的云里雾里，不知道get几层。\n但看这本书的过程中，我狂搜了些自然语言处理的课件，有北大的，中科院的，都写的很好，从语言模型开始。从分词，标注，语法树，语意等等。也大体知道自然语言处理，分词法，语法，语义。然后是各种应用，信息检索，机器翻译等自然语言经典应用问题。\n断断续续做了些小项目，基于语言模型的拼音输入法，仿照sun'pinyin写的，他们的blog写的很详细，从模型建模，到平滑处理，很详细，我也用python实现了一遍，当时这个输入法配合上一个简单的ui还在部门内部推广了，搞了个基于云的拼音输入法，获得个小奖品，很是洋洋得意。这个过程中，我看着sunpinyin的blog, 回过头又去看课件，去了解很细节的问题，如拉普拉斯平滑，回退平滑的细节等，收获很多。\n后来老大告诉我，看自然语言问题时，可以找博士论文先看，因为博士论文一般都会来龙去脉讲的非常详细，看完一遍之后基本上这个问题就了解的差不多，然后就是follow业界的进度，那就是关注各种会议和期考，可自行百度和谷歌。\n搞好这个拼音输入法，进入实际项目，做一套中文自然语言的基础处理引擎，好在不是让我一个人来，公司开始找大学合作，我做企业项目负责跟进的，大学负责具体算法，我跟着自己调查分词标注算法，了解了有基于词典的，语言模型的，hmm,crf的，那个crf的，我始终搞不大明白，后来先了解了hmm的vertbe算法，em算法，大学的博士给我讲了一遍crf，终于豁然开朗。还把解码过程写到了http://52nlp.cn上，关注的人还可以。从那以后我感觉我就真入门了。在来一个什么问题，我基本上也有套路来学习和研究了。\n总结下，\n1.先各种课件，加那本自然语言的书，搞清楚自然语言大概都有哪些问题，主要是为了解决什么问题的。\n2.基于某个问题看博士论文，了解来龙去脉。然后follow业界进度。\n3.找各种资源，会议的，期刊的，博客http://52nlp.cn(不是打广告，我不是博主，不过博客真心不错)\n4.微博上关注各种这个领域的大牛，他们有时候会推荐很多有用的资料。\n当然，数学之美 我也读了，确实不错。","data":"2017年07月23日 08:05:37"}
{"_id":{"$oid":"5d345d0c62f717dc0659b9b5"},"title":"自然语言处理如何助力人机共鸣","author":"52AI人工智能","content":"欢迎关注我们微信公众号，可以加入我们QQ人工智能行业交流群626784247.\n\n\n\n\n01\n\n\n\n\n在当前飞速发展的创新步伐中，科技似乎正在积极地解决人类最紧迫的难题。在某些方面，我们取得了很大的进步。但是，当涉及到解决如员工多样性、无意识偏见、员工和客户满意度等等以人为本的挑战时，技术并未达到预期效果。\n图片来自“123rf.com.cn”\n本文来自venturebeat，作者刘敏.\n在当前飞速发展的创新步伐中，科技似乎正在积极地解决人类最紧迫的难题。\n在某些方面，我们取得了很大的进步。在可再生能源、疾病预防和灾后重建等领域作出了重大突破。但是，当涉及到解决如员工多样性、无意识偏见、员工和客户满意度等等以人为本的挑战时，技术并未达到预期效果。\n这是因为像喷气推进或GPS这样的技术性问题在很大程度上是与数学和物理相关的，这也是计算机(和程序员)擅长的领域。但是，解决像员工投入度这类人情问题时通常需要同理心，这是很难用代码编写出来的。人类是情感动物，尤其是在做决定的时候。首先我们用心感受，然后利用逻辑思维帮助自己选择正确的情感反应，最后我们采取行动。因此，任何帮助人们做出“更好的”决定而不考虑情感因素的尝试都注定要失败。\n然而，随着人工智能的发展，尤其是自然语言处理(NLP)的最新进展，我们终于掌握了利用人类情感力量和复杂性的技术工具。这种方法对我们如何设计系统有着重要的影响，而且它也会带来更加人性化的解决方案。\n编程的差异\n语言极其复杂。人与人之间，一个人的经历或生活环境，一点细微的差别就可以影响他们表达自己的方式。方言，性别，地点，甚至季节都可以改变我们用来表达想法的词汇。\n人们很善于解释这些细微的差别。然而，对于计算机来说，这是一个巨大的挑战。为了达到接近人类水平的理解，他们需要一套庞大而丰富的语言训练数据，这些数据跨越了人口统计学、经验和背景的差距。\n要想了解这在现实生活中是如何运作的，只需想想加州的一个十几岁的少年在给新智能手机评论时使用了“lit”这个词(意思是“激动”)，而来自马萨诸塞州的一位老人作出的评论中，同样的词可能意味着屏幕亮度。\n体会言外之意\n这是第一次,我们能够教计算机不仅要通过计算单词或寻找特定短语来理解人们的基本意思,而是聪明地“体会言外之意”,理解我们的言语背后的真正意图和意义。当然，这是随着时间的推移人们获得的一项重要技能——移情。\n常见的“满意度调查”是一个典型的例子，人们能够感受到机器在解决最基础的问题时的局限性。从原则上讲，这是了解人们对产品或服务的看法的有效方法。但在实际操作中，它显得十分笨拙、不准确，而且早就应该修改一下了。\n回想一下大多数商店收据上的调查提示:“请为我们的服务打分（1-10），并分享原因。”比较一下，同样情况下——简单地问一句“你对这次体验的看法是什么?”然后从使用的语言和整个上下文推断出“得分”。虽然人们不需要明确的评分，但机器确实需要。\n镜像效应\n除了帮助我们更好地了解彼此之外，NLP还能让我们更好地了解自己。语言是我们表达思想感情的窗户。当技术可以开始理解我们的时候(不是它希望我们如何)，它可以成为一个真正的合作伙伴，帮助我们发现成长进步的最佳方式。\n以可怕的绩效评估和各种各样的偏见来折磨它。当你问工作环境中的人他们是否会有偏见，即使是下意识的，他们也会极力否认。然而，对绩效评估的研究显示出人们持有普遍的、无意识的偏见。\n我的团队分析显示，当男性审视其他男性时，他们中绝大多数都使用被动的语言(“他们可以更加积极主动”)。然而，当这些男性对女性进行审查时，他们通常会“指指点点”(“你应该注意细节”)。通过使用数据驱动的技术，我们能够进一步深入了解这些隐藏的偏见，而人们往往意识不到它们的存在。幸运的是，人工智能让我们直面这些偏见，并一步步纠正它们。\n为了解决世界上最具挑战性的“人性问题”，无论是通过开发更好的产品，还是在工作中获得更多理解和公正，我们都需要技术来表达同理心。让心与心结合在一起，我们就可以进一步发展并提倡人们应得的“以人为本”的解决方案。\n\n\n02\n—\n\n52AI\n\n\n\n52AI，专注服务于普通人的AI 学习和发展，让大众受益于人工智能就是我们的愿望。我们坚信只有对大众收益的科技才是有意义的，也是我们追求的方向。","data":"2017年11月01日 00:00:00"}
{"_id":{"$oid":"5d345d2462f717dc0659b9bf"},"title":"什么是自然语处理，自然语言处理主要有什么用","author":"duozhishidai","content":"一．什么是NLP\nNLP，中文叫自然语言处理，简单来说，是一门让计算机理解、分析以及生成自然语言的学科，大概的研究过程是：研制出可以表示语言能力的模型——提出各种方法来不断提高语言模型的能力——根据语言模型来设计各种应用系统——不断地完善语言模型。\nNLP理解自然语言目前有两种处理方式：\n1.基于规则来理解自然语言，即通过制定一些系列的规则来设计一个程序，然后通过这个程序来解决自然语言问题。输入是规则，输出是程序；\n2.基于统计机器学习来理解自然语言，即用大量的数据通过机器学习算法来训练一个模型，然后通过这个模型来解决自然语言问题。输入是数据和想要的结果，输出是模型。\n接下来简单介绍NLP常见的任务或应用。\n二．NLP能做什么：\n1.分词\n中文可以分为字、词、短语、句子、段落、文档这几个层面，如果要表达一个意思，很多时候通过一个字是无法表达的一个含义的，至少一个词才能更好表达一个含义，所以一般情况是以“词”为基本单位，用“词”组合来表示“短语、、句子、段落、文档”，至于计算机的输入是短语或句子或段落还是文档就要看具体的场景。由于中文不像英文那样词与词之间用空格隔开，计算机无法用区分一个文本有哪些词，所以要进行分词。目前分词常用的方法有两种：\n（1）基于规则：Heuristic（启发式）、关键字表\n（2）基于机器学习/统计方法：HMM（隐马尔科夫模型）、CRF（条件随机场）\n（注：在这里就不具体介绍方法的原理和实现过程了，大家感兴趣，可以自行百度了解）\n现状分词这项技术非常成熟了，分词的准确率已经达到了可用的程度，也有很多第三方的库供我们使用，比如jieba，所以一般在实际运用中我们会采用“jieba+自定义词典”的方式进行分词。\n2.词编码\n现在把“我喜欢你”这个文本通过分词分成“我”、“喜欢”、“你”三个词，此时把这三词作为计算机的输入，计算机是无法理解的，所以我们把这些词转换成计算机能理解的方式，即词编码，现在普遍是将词表示为词向量，来作为机器学习的输入和表示空间。目前有两种表示空间：\n（1）离散表示：\nA.One-hot表示\n假设我们的语料库是：\n我喜欢你你对我有感觉吗\n词典{“我”：1，“喜欢”：2，“你”:3,“对“：4，“有”：5，“感觉”：6，“吗”：7}。一共有七个维度。\n所以用One-hot表示：\n“我”：[1,0,0,0,0,0,0]\n“喜欢”：[0,1,0,0,0,0,0]\n········\n“吗”：[0,0,0,0,0,0,1]\n即一个词用一个维度表示\nB.bagofword：即将所有词的向量直接加和作为一个文档的向量。\n所以“我喜欢你”就表示为：“[1,1,1,0,0,0,0]”。\nC.Bi-gram和N-gram（语言模型）：考虑了词的顺序，用词组合表示一个词向量。\n这三种方式背后的思想是：不同的词都代表着不同的维度，即一个“单位”（词或词组合等）为一个维度。\n（2）分布式表示：word2vec，表示一个共现矩阵向量。其背后的思想是“一个词可以用其附近的词来表示”。\n离散式或分布式的表示空间都有它们各自的优缺点，感兴趣的读者可以自行查资料了解，在这里不阐述了。这里有一个问题，当语料库越大时，包含的词就越多，那词向量的维度就越大，这样在空间储存和计算量都会指数增大，所以工程师在处理词向量时，一般都会进行降维，降维就意味着部分信息会丢失，从而影响最终的效果，所以作为产品经理，跟进项目开发时，也需要了解工程师降维的合理性。\n3.自动文摘\n自动文摘是指在原始文本中自动摘要出关键的文本或知识。为什么需要自动文摘？有两个主要的原因：（1）信息过载，我们需要在大量的文本中抽出最有用、最有价值的文本；（2）人工摘要的成本非常高。目前自动文摘有两种解决思路：第一种是extractive（抽取式），从原始文本中找到一些关键的句子，组成一篇摘要；另一种方式是abstractive（摘要式），计算机先理解原始文本的内容，再用自己的意思将其表达出来。自动文摘技术目前在新闻领域运用的最广，在信息过载的时代，用该技术帮助用户用最短的时间了解最多、最有价值的新闻。此外，如何在非结构的数据中提取结构化的知识也将是问答机器人的一大方向。\n4.实体识别\n实体识别是指在一个文本中，识别出具体特定类别的实体，例如人名、地名、数值、专有名词等。它在信息检索、自动问答、知识图谱等领域运用的比较多。实体识别的目的就是告诉计算机这个词是属于某类实体，有助于识别出用户意图。比如百度的知识图谱：\n“周星驰多大了”识别出的实体是“周星驰”（明星实体），关系是“年龄”，搜索系统可以知道用户提问的是某个明星的年龄，然后结合数据“周星驰出生时间1962年6月22日”以及当前日期来推算出周星驰的年龄，并把结果直接把这个结果显示给用户，而不是显示候选答案的链接。\n此外，NLP常见的任务还有：主题识别、机器翻译、文本分类、文本生成、情感分析、关键字提取、文本相似度等，以后有时间再为大家做简单介绍。\n人工智能、大数据、云计算和物联网的未来发展值得重视，均为前沿产业，有兴趣的朋友，可以查阅多智时代，在此为你推荐几篇优质好文：\n自然语言理解过程主要有哪些层次，各层次的功能是怎么样？\nhttp://www.duozhishidai.com/article-1726-1.html\n如何快速入门NLP自然语言处理概述\nhttp://www.duozhishidai.com/article-11742-1.html\n什么是人脸识别，主要的应用于哪些场景？\nhttp://www.duozhishidai.com/article-1246-1.html\n多智时代-\n人工智能\n和\n大数据\n学习入门网站|人工智能、大数据、\n物联网\n、\n云计算\n的学习交流网站","data":"2019年02月13日 15:07:31"}
{"_id":{"$oid":"5d345d3a62f717dc0659b9c3"},"title":"自然语言处理 资源合集","author":"你的微笑依然那样灿烂","content":"声明：本文转载自此链接。\n维护者：Dibya Chakravorty\nContributions\nFeel free to send pull requests, or email me (dibyachakravorty@gmail.com)\nHow this list got started\nOn November 10, 2016, a Hacker News (HN) user aarohamankad asked the HN community for suggestions on beginner NLP resources. This Ask HN thread became popular and stayed in the front page for some time. In this time, it gathered plenty of community generated suggestions about beginner NLP resources. This list is an attempt to summarize this discussion into a coherent list of resources. I also wrote a blog post on this.\nTable of Contents\nBooks\nMOOCs\nYouTube Videos\nOnline University Courses\nPackages to Play With\nAcademic Papers\nLearning by Doing\nOpen Source Projects\nFun Ideas\nAPIs\nUser Groups\nOther Guides\nBooks\nSpeech and Language Processing : Classic and Standard textbook in NLP. Pre publication draft of 3rd edition available here.\nNatural Language Processing with Python : Application oriented book. Examples are in Python (NLTK). Free online version here.\nTaming Text : Application oriented book. Examples are in JAVA.\nFoundations of Statistical Natural Language Processing : Classic text on Statistical NLP. Goes deep into the implementation of parsers, taggers etc.\nHandbook of Natural Language Processing : A complete treatment of NLP that starts from the historical roots and ends with the modern methods of NLP.\nStatistical Machine Translation : Learn how to make a service like Google Translate\nIntroduction to Information Retrieval : Learn the nuts and bolts of services like Google Search and Google News (search, text classification, clustering etc.)\nProlog and Natural Language Analysis : Implement NLP algortihms in Prolog.\nMOOCs\nCoursera course offered by University of Michigan : Introductory course that covers all prerequisite materials. Favored programming language is Python.\nDicontinued Coursera course offered by Comlumbia University, available on Academic torrents : Theory and concept oriented course. Only the course materials are available at this point.\nYouTube Videos\nVideo series by Jurafsky and Martin : Jurafsky and Martin are both professors at Stanford, and they have written multiple classic textbooks on NLP.\nStanford CS224D : Deep Learning in NLP : Applicatin of Deep Learning in NLP\nNLP with Python and NLTK : Application oriented video series using Python and NLTK.\nOnline University Courses\nMachine Translation course at the University of Pennsylvania\nPackages to Play With\nNLTK : Most popular NLP library in Python. Excellent documentation in the form of a book/free online version. Powerful and extensible.\nStanford CoreNLP : Fast and feature rich NLP library, written in JAVA. An online demo is available here.\nSpacy : Another emerging NLP library in Python. Fast and state of the art. Tries to maintain an uniform API while implementing state of the art algorithms. They have a blog and an online demo.\nApache Tika : Offers an unified interface for extracting text data and meta data from many different file formats (PPT, PDF etc.) and analysis.\nAcademic Papers\nDeep Learning in NLP : A GitHub repo that collects papers on Deep Learning in NLP.\nLearning by Doing\nOften the best way to learn is to contribute to an existing open source NLP project or implementing a fun idea.\nOpen Source Projects\nBetty : Betty is a open source project with both real-life use and practical NLP considerations, and is looking for new maintainers.\nFun Ideas\nInteractive Fiction/Parser Based Fiction : A video game where the player's interactions primarily involve text. Listen to this illuminating FLOSS podcast on the topic.\nAPIs\nIBM Watson Cloud : From the makers of IBM Watson. It lets you integrate NLP functionality in your app via an API. There's a free tier/free trial.\nUser Groups\nACM Special Interest Group in AI : If you are craving for some face to face human contact.\nOther Guides\nQuora question on how to get into NLP\nawesome-nlp on GitHub : A GitHub repo containing a curated list of NLP resources.\n声明：本文转载自此链接。\n转载请注明：《 自然语言处理 资源合集 | 我爱计算机 》","data":"2018年01月16日 15:00:37"}
{"_id":{"$oid":"5d345d7262f717dc0659b9d0"},"title":"我的人工智能学习之路--NLP方向（开篇）","author":"jeque","content":"我的人工智能学习之路--NLP方向（开篇）\n什么是机器学习，什么是深度学习\n机器学习\n机器学习环境及所需工具\n机器学习十大算法\n深度学习\n深度学习中的函数类型\n深度学习中的常见概念\nNLP（自然语言处理）\n数学基础\n分词和统计分布规律\n基于数学统计的语言模型\n\n对于人工智能的学习，我主要侧重于自然语言处理方向，基于这个大方向，将我的学习脉络梳理成一套体系。按照总分原则，开篇先从总体上介绍机器学习、深度学习以及NLP的相关知识。在以后的博客中，我会边学边完善，同时我还会写入我参加各算法大赛的比赛经验，做到理论与实践相结合。期望我在这条道路上越走越深，排除万难，砥砺前行。\n接下来，开启我的人工智能之旅吧！\n什么是机器学习，什么是深度学习\n以下关于人工智能、机器学习的定义来自《百面机器学习》\n进入2018年以来，人工智能 机器学习 深度学习 神经网络等关键词已经成为人们茶余饭后的谈资，而且更会成为软件工程师的必备技能。\n人工智能泛指机器具有人的智力的技术。这项技术的目的是使机器像人一样感知、思考、做事、解决问题。人工智能是一个宽泛的技术领域，包括自然语言理解、计算机视觉、机器人、逻辑和规划等。\n机器学习\n机器学习指计算机通过观察环境，与环境交互，在吸取信息中学习、自我更新的进步。简而言之，机器学习可以揭示数据背后的真是含义。大多数机器学习算法可以分成训练和测试两个步骤，这两个步骤可以重叠进行。\n训练包括监督学习和无监督学习两类。其中，监督学习关注对事物未知表现的预测，一般包括分类问题和回归问题；无监督学习则倾向于对事物本身特性的分析，常用的技术包括数据降维和聚类问题等。\n分类：顾名思义，便是对其所在的类别进行预测。类别既是离散的，同时也是预先知道数量的。\n回归：同样是预测问题，只是预测的目标往往是连续变量。\n数据降维：是对事物的特性进行压缩和筛选，这项任务相对比较抽象。\n聚类：是依赖于数据的相似性，把相似的数据样本划分为一个簇。不同于分类问题，我们在大多数情况下不会预先知道簇的数量和每个簇的具体含义。\n机器学习环境及所需工具\n我习惯使用Python进行机器学习任务，同时利用里面强大的库资源来参加算法竞赛。\n为什么使用Python及优势\nPython是一种兼顾可读性和易用性的编程语言。同时，Python具有免费使用和跨平台执行的特性。作为一门解释型语言，也非常便于调试代码。\nPython机器学习的优势：\n1）方便调试的解释型语言\n2）跨平台执行作业\n3）广泛的应用编程接口\n4）丰富完备的开源工具包\nNumPy \u0026 SciPy\nNumPy除了提供一些高级的数学运算机制以外，还具备非常高效的向量和矩阵运算功能。\nSciPy是在NumPy的基础上构建更为强大，应用领域也更为广泛的科学计算包。它需要依赖NumPy的支持进行安装和运行。\nMatplotlib\n免费使用的绘图工具包。\nScikit-learn\n封装了大量经典以及最新的机器学习模型。\nPandas\n一款针对于数据处理和分析的Python工具包。\nAnaconda\n一个可以一次性获得300多种用于科学和工程计算相关任务的编程库的平台。\n机器学习十大算法\nC4.5决策树\nK-均值（K-mean）\n支持向量机（SVM）\nApriori\n最大期望算法（EM）\nPageRank算法\nAdaBoost算法\nk-近邻算法（kNN）\n朴素贝叶斯算法（NB）\n分裂回归树算法（CART）\n深度学习\n深度学习本身是传统神经网络算法的延伸。一般来说，深度学习适合解决数据量大、数据比较规范，但是决策函数高度非线性的问题。常见的深度学习应用非常成功的领域有图像识别、语音识别、文字生成、自然语言理解等。神经网络模型的发展大致经历了四个不同的阶段：\n基本的感知器\n传统的神经网络模型历史可以追溯到20世纪50年代，现在公认的鼻祖是Rosenblatt在1957年提出的感知器算法。\n多层感知器\n20世纪70年代到80年代，多层感知器被发现，其逼近高度非线性函数的能力使得科学界对它的兴趣大增，甚至有神经网络能解决一切问题的论调。\n传统神经网络比较沉寂的时期\n20世纪90年代到21世纪早些时候，传统神经网络模型比较沉寂，但却是核方法大行其道的时候。主要原因是计算能力跟不上。\n神经网络模型\n大约在2006年以后到现在，几个重要的技术进步促进了以深度学习为代表的神经网络的大规模应用。\n首先是廉价的并行计算；其次是深度网络结构的持续研究，使得模型训练效率大大增加；最后是互联网的出现，为大规模数据的生成和获取提供了极大的便利。\n深度学习中的函数类型\n大多数神经网络中都包含四类函数：组合函数、激活函数、误差函数和目标函数。\n组合函数\n激活函数\n误差函数\n目标函数\n深度学习中的常见概念\n批量\n在线学习和离线学习\n偏移/阈值\n标准化数据\n深度递减算法\n反向传播算法\nNLP（自然语言处理）\n从广义上讲，“自然语言处理”（Natural Language Processing 简称NLP）包含所有用计算机对自然语言进行的操作，从最简单的通过计数词出现的频率来比较不同的写作风格，到最复杂的完全“理解”人所说的话，至少要能达到对人的话语作出有效反应的程度。\n自然语言处理是用计算机通过可计算的方法对自然语言的各级语言单位（字、词、语句、篇章等等）进行转换、传输、存贮、分析等加工处理的科学。是一门与语言学、计算机科学、数学、心理学、信息论、声学相联系的交叉性学科。\n在词处理技术方面，词是自然语言中最小的有意义的构成单位，是自然语言处理中最基本的研究内容，也是其他研究的先行和基础。\n词处理的主要内容包括分词、词性标注、词义消歧三个主要的内容。分词常用的方法包括正向最大匹配和反向最大匹配以及基于词网格的统计方法。困扰分词的主要问题就是歧义消解和新词识别，由于语言本身的复杂性，目前这两个问题并没有得到根本性的解决。\n词性标注常用的方法就是基于隐马尔科夫模型的词性标注方法。常用的词性标注的方法包括基于词典知识库的方法，还有一些常用基于统计的分类方法，包括贝叶斯方法和最大熵模型。分词和词性标注是所有自然语言应用的基础，广泛的应用于机器翻译，信息检索等各个领域。\n数学基础\n概率论\n信息论：信息熵、联合熵、条件熵\n粗糙集\n分词和统计分布规律\n常用的分词方法：\n1）正向最大匹配分词\n2）反向最大匹配分词\n3）基于统计的词网格分词\n基于数学统计的语言模型\n现有的主要统计语言模型\n1）上下文无关模型\n2）N元文法模型\n3）N-pos模型\n4）基于决策树的语言模型\n5）动态、自适应、基于缓存的语言模型\n6）隐马尔科夫模型\n7）最大熵模型","data":"2018年10月10日 23:46:26"}
{"_id":{"$oid":"5d345d9362f717dc0659b9da"},"title":"facebook自然语言处理(NLP)平台Pytext简介","author":"rejames","content":"自然语言处理(NLP)在现代深度学习生态中越来越常见。从流行的深度学习框架到云端API的支持，例如Google云、Azure、AWS或Bluemix，NLP是深度学习平台不可或缺的部分。尽管已经取得了令人难以置信的进步，但构建大规模的NLP应用依然还有极大的挑战，在学习研究和生产部署之间还存在很多摩擦。作为当前市场上最大的会话环境之一，Facebook已经面对构建大规模NLP应用的挑战有一些年头了，最近，Facebook的工程团队开源了第一个版本的Pytext，一个基于PyTorch的NLP框架，可以用来构建高效的NLP解决方案。\nPyText的最终目标是简化端对端的NLP工作流实现。为了实现这一目标，PyText需要解决当前NLP流程中的一些问题，其中最令人头疼的就是NLP应用在实验环境和生产环境的不匹配问题。\n更好地平衡NLP实验和生产部署\n现代NLP解决方案通常包含非常重的实验环节，在这个阶段数据科学家们将借鉴研究文件快速测试新的想法和模型，以便达成一定的性能指标。在实验阶段，数据科学家倾向于使用容易上手、界面简单的框架，以便快速实现高级、动态的模型，例如PyTorch或TensorFlow Eager。当需要部署到生产环境时，动态图模型的固有局限性就带了新的挑战，这一阶段的深度学习技术需要使用静态计算图，并且需要为大规模计算进行优化。TensorFlow、Caffe2或MxNet都属于这一类型的技术栈。结果是大型数据科学团队不得不为实验和生产部署使用不同的技术栈。\nimage\nPyTorch是最早解决了快速实验与规模化部署之间冲突的深度学习框架之一。基于PyTorch构建的PyText为NLP领域应用了这些解决实验环境与生产部署之间冲突的优化原则。\n理解PyText\n从概念角度触发，PyText被设计为实现以下四个基本目标：\n尽可能简单、快速的实现新模型\n简化将预构建模型应用于新数据的工作量\n同时为研究者和工程师定义清晰的工作流，以便构建和评估模型，并以最小的代价上线模型\n确保部署的模型在推理时具有高性能：低延迟、高吞吐量\nPyText的处理容量最终打造的建模框架，可供研究者和工程师构建端到端的训练或推理流水线。当前的PyText实现涵盖了NLP工作流声明周期中的基本环节，为快速实验、原始数据处理、指标统计、训练和模型推理提供了必要的接口。一个高层级的PyText架构图可以清晰地展示这些环节如何封装了框架的原生组件：\nimage\n如上图所示，PyText的架构包含以下组成部分：\nTask：将多个用于训练或推理的组件拼装为一个流水线\nData Handler：处理原始输入数据，贮备张量批数据，以便送入模型\nModel：定义神经网络的架构\nOptimizer：封装模型参数优化过程，基于模型的前馈损失进行优化\nMetric Reporter：实现模型相关指标的计算和报表提供\nTrainer： 使用数据处理器、模型、损失和优化器来训练和筛选模型\nPredictor：使用数据处理器和模型对给定的数据集进行推理\nExporter： ONNX8导出训练好的PyTorch模型到Caffe2图\n你可以看到，PyText利用ONNX(Open Neural Network Exchange Format)将模型从实验环境的PyTorch格式转换为生产环境的Caffe2运行模型。\nPyText预置了众多NLP任务组件，例如文本分类、单词标注、语义分析和语言模型等，可以快速实现NLP工作流。类似的，PyText使用上下文模型介入语言理解领域，例如使用SeqNN模型用于意图标注任务，或者使用一个上下文相关的意图槽模型用于多个任务的联合训练。\n从NLP工作流的角度来说，PyText可以快速将一个思路从实验阶段转换为生产阶段。一个PyText应用的典型工作流包含如下的步骤：\nimage\n用PyText实现模型，确保测试集上的离线指标正确\n将模型发布到打包的基于PyTorch的推理服务，在实时样本上执行小规模评估\n自动导出到Caffe2网络，不过在有些情况下，例如当使用复杂的流程控制逻辑时，或者使用自定义数据结构式，PyTorch 1.0还不支持\n如果第3步不支持，那么使用Py-Torch C++ API9重写模型，并封装为一个Caffe2操作符\n将模型发布为生产就绪的Caffe2预测服务并启动\n使用PyText\n上手PyText非常简单，按标准python包的方法安装框架：\n$ pip install pytext-nlp\n然后，我们就可以使用一个任务配置来训练NLP模型了：\n(pytext) $ cat demo/configs/docnn.json { \"task\": { \"DocClassificationTask\": { \"data_handler\": { \"train_path\": \"tests/data/train_data_tiny.tsv\", \"eval_path\": \"tests/data/test_data_tiny.tsv\", \"test_path\": \"tests/data/test_data_tiny.tsv\" } } } } $ pytext train \u003c demo/configs/docnn.json\nTask是PyText应用中的用来定义模型的核心部件。每一个任务都有一个嵌入的配置，它定义了不同组件之间的关系，如下面代码所示：\nfrom word_tagging import ModelInputConfig, TargetConfig class WordTaggingTask(Task): class Config(Task.Config): features: ModelInputConfig = ModelInputConfig() targets: TargetConfig = TargetConfig() data_handler: WordTaggingDataHandler.Config = WordTaggingDataHandler.Config() model: WordTaggingModel.Config = WordTaggingModel.Config() trainer: Trainer.Config = Trainer.Config() optimizer: OptimizerParams = OptimizerParams() scheduler: Optional[SchedulerParams] = SchedulerParams() metric_reporter: WordTaggingMetricReporter.Config = WordTaggingMetricReporter.Config() exporter: Optional[TextModelExporter.Config] = TextModelExporter.Config()\n一旦模型训练完毕，我们就可以对模型进行评估，也可以导出为Caffe2格式：\n(pytext) $ pytext test \u003c \"$CONFIG\" (pytext) $ pytext export --output-path exported_model.c2 \u003c \"$CONFIG\"\n需要指出的是，PyText提供了可扩展的架构，可以定制、扩展其中任何一个构建模块。\nPyText代表了NLP开发的一个重要里程碑，它是最早解决实验与生产匹配问题的框架之一。基于Facebook和PyTorch社区的支持，PyText可能有机会称为深度学习生态中最重要的NLP技术栈之一。\n汇智网翻译整理，转载请标明出处。Pytext简介","data":"2018年12月26日 09:27:00"}
{"_id":{"$oid":"5d345dad62f717dc0659b9de"},"title":"目前自然语言处理的实际应用方法总结","author":"miner_zhu","content":"自然语言处理的方法\n分词\n分词的任务定义为：输入一个句子，输出一个词语序列的过程。如将「严守一把手机关了。」输出为「严守一/把/手机/关/了。」\n目前的两种主流方法包括基于离散特征的 CRF 和 BILSTM-CRF。\n挑战包括交叉歧义、新词识别、领域移植、多源异构数据融合及多粒度分词等。\n命名实体\n现在的主流方法包括：\n1. 规则系统\n2. 基于机器学习的学习系统\n目前的挑战包括新领域旧实体类别识别、新实体类别识别等，解决办法包括利用构词知识、领域知识，使用强化学习、跨领域学习、半监督学习、众包、远程监督等机器学习方法。\n句法分析\n句法分析的任务定义为：输入一个句子的词语序列，输出为句子结构表示的过程。依存句法分析输出的是依存句法树，下面以依存句法分析为例。\n目前采用的方法包括：\n基于图的方法，即从图中搜索得到句法树，主要的任务在于确定每个依存弧的分值；\n基于转移的方法：即通过一系列移进规约的动作得到句法树，主要任务在于基于当前状态，确定每个动作的分值。\n现在的主流做法是在上述两者的基础上加入深度学习的方法。\n语义分析\n定义是将文本转换为可计算的知识表示。目前学术界语义表达方法包括：1）浅层语义分析；2）逻辑语义分析；3）抽象语义表示分析。\n篇章分析\n篇章的定义指的是一系列连续的语段或句子构成的语言整体单位，核心问题是篇章结构和篇章特征，其所基于的语言学基本理论包括中心理论、脉络理论、RST 等多种语言学基本理论。\n基本结构分析\n篇章结构指的是篇章内部关系的不同结构化表达形式，主要包括逻辑语言结构、指代结构、话题结构、功能结构、事件结构等范畴。\n基本特征的研究\n包括连接性、连贯性、意图、可接受性、信息性、情景性和跨篇章等七个基本特征。\n自然语言生成\n张民教授总结了在基于规则、基于知识的检索及基于深度学习等三种自然语言生成方法的优缺点对比及适用场景。\n基于规则\n它的一大优势在于具体领域的能做到精准回答；但相应地，在可移植性及可扩展性上则存在不足；适用的场景以个人助理为主，和任务驱动型的对话。\n基于知识的检索\n它的优点在于知识库易于扩充，答案没有语法错误；但对话连续性差，容易出现答非所问的情况；适用场景以问答系统、娱乐聊天为主。\n基于深度学习\n基于数据驱动的方法能够省去显示语言理解等过程，但需要大量语料支持；适用场景以虚拟影像、智能聊天机器人为主的有丰富领域语料的场景。\n自然语言处理的应用\n1. 情感和情绪分析\n在业界研究和应用，情感一般包括正面、负面和中性，而情绪一般表现为喜、怒、哀、乐、惊、恐、思等。情绪和情感都是人对客观事物所持的态度体验，只是情绪更倾向于个体基本需求欲望上的态度体验，而情感则更倾向于社会需求欲望上的态度体验。情感和情绪分析包括问题驱动和模型驱动两个方面，在工业界和学术界都已经有着广泛的应用和研究。\n2. 问答\n智能问答主要有三方面的要求：一是理解人类语言的内涵；二是推敲知识获取的意图；三是挖掘精确贴切的知识。\n相应地，问答系统需要解决三个问题：\n1. 问题分类、分析和理解（一阶逻辑、二阶逻辑）\n2. 答案的匹配、检索\n3. 答案生成\n问答的四个难点及解决方法\n1）多源异构大数据背景下开放域问答的瓶颈。在效率与覆盖率的权衡下，数据大小与知识占比的关系是每个研究者需要考虑的问题；而结构化数据与非结构化数据的混杂，导致知识挖掘与存储存在相应的难点；此外，数据时效性的变化也给新旧知识的应用带来了挑战。\n以往是用 IR 或 RC 的方法，但目前流行采用对检索所得的多个段落排序，也就是在 IR 和 RC 中加入了排序的操作，进而进行面向多段落的提取/生成答案。\n2）深度语义理解的问答技术。以 Watson 为代表的系统采用的是抽取与置信度计算的方法；目前则是阅读理解抽取/生成式方法推动了技术发展。\n3）知识库与知识图谱。以往的知识库存在可靠性、包容性低，存在通用性不高的问题，目前研究者们更多考虑用当下热门问题自动生成来实现知识图谱的自动更新和扩展。\n4）多模态场景下的问答。问题的对象往往潜藏于多媒体，且答案的判断需要参考其它媒体的数据资源。目前出现了以语言处理 RNN 与图像处理的 CNN 的有机结合方法，实现跨媒体的特征共享、独立和抗依赖。\n对话\n根据应用场景的不同，可分为开放域及封闭域对话系统。高准确率的上下文篇章建模、对话状态转移模型和领域知识建模是目前对话亟待解决的问题。\n知识图谱\n包括知识建模、知识图谱构建、知识融合、知识推理计算以及知识赋能等主要任务。知识图谱构建是目前学术界和产业界研究热点，包括实体及其属性识别、事件抽取、实体事件关系抽取、概念实例化和规则学习等。\n机器翻译\n机器翻译目前已经取得较大进展，未来机器翻译可以从如下领域做发展：\n知识建模和翻译引擎，从词序列到语义到知识，利用知识图谱和各类知识（语言学知识、领域知识、常识知识等）进一步延伸机器翻译的边界；\n研究新的翻译模型，从广度（篇章）和深度（深度理解）进一步推进机器翻译的理解能力。此外，还需要适应产业化的需求和国家战略需求。\n转自：2018中国人工智能大会专题论坛","data":"2018年09月19日 16:22:54"}
{"_id":{"$oid":"5d36a89e6734bd8e681d5dc7"},"title":"【自然语言处理】论述自然语言处理的技术范畴","author":"贾继康","content":"文章目录\n论述自然语言处理的技术范畴\n一、前言\n二、主要技术范畴\n1、语音合成(Speech Synthesis)\n2、语音识别(Speech Recognition)\n3、中文自动分词\n4、词性标注\n5、句法分析\n6、文本分类\n7、文本挖掘\n8、信息抽取\n9、 问答系统\n10、机器翻译\n11、文本情感分析\n12、自动摘要\n13、文字蕴涵\n三、自然语言处理的难点\n1、语言环境复杂\n2、文本结构形式多样\n3、边界识别限制\n4、词义消岐\n5、指代消解\n四、展望自然语言处理\n论述自然语言处理的技术范畴\n一、前言\n本片博文主要是介绍说明自然语言处理的全貌，一些主要的技术范畴。\n自然语言处理(NLP)这个是一个很大的话题,，它是一个人机交互的一个过程，它涉及的学科比较广泛譬如如下所示：\n1：语言学\n2：计算机科学(提供模型表示，算法设计，计算机实现):\n3：当然还有数学以此来提供数学模型\n4：心理学(人类言语心理模型和理论)\n5：哲学(提供人类思维和语言的更高层次理论)\n6：统计学(提供样本数据的预测统计技术)\n7：电子工程(信息论基础和语言型号处理技术)\n8：生物学(人类言语行为机制理论)\n总之那涉及的学科范围广泛。不言而喻在自然语言处理研究工作中是十分艰难的，博主现在也只是学习它的一个小小的分支罢了，看到此篇博文的小伙伴希望能抛出你们的建议和意见，要是如此博主甚是感激，开心呀！！！\n二、主要技术范畴\n1、语音合成(Speech Synthesis)\n所谓的语音合成\n就是指用人工的方式产生人类语音。\n语音合成器\n，就是利用计算机系统作用在语音合成上。而语音合成器可以用软/硬件实现。\n\n文字转语音（Text-To-Speech，TTS）\n系统则是将一般语言的文字转换为语音，其他系统可以描绘语言符号的表示方式，就像音标转换至语音一样。\n\n语音合成器的质量:\n通常取决于人声的相似度及语义是否能被了解。举个例子，对于个瞎子看不到文字，只能通过语音合成器很清楚的听到文字转换成语音的效果。\n\n语音合成的应用\n包括智能仪表、智能玩具、电子地图、电子导游、电子词典等。\n总结：\n用大白话来讲使用语音合成器可以实现文字转换为语音，音标转化为语音,并且效果如同非瞎看文字，瞎子听语音同一个效果为最好。\n2、语音识别(Speech Recognition)\n语音识别（Speech Recognition）技术\n也被称为\n语音转文本识别（Speech to Text，STT）\n，\n目标\n是让计算机自动将人类的语音内容转换为相应的文字。\n\n语音识别技术的应用\n包括语音拨号、语音导航、室内设备控制、语音文档检索、简单的听写数据录入等。语音识别技术与其他自然语言处理技术如机器翻译及语音合成技术相结合，可以构建出更加复杂的应用，例如，语音到语音的翻译。\n\n总结：\n用大白话来讲语音识别就是借助计算机工具来识别人类说的话转化为可视化的东东(也就是文字啦)。\n3、中文自动分词\n中文自动分词指的是\n-----\u003e\n使用计算机\n-----\u003e\n自动对中文文本\n-----\u003e\n进行词语的切分。就像英文那样使得中文句子中的词之间有空格以标识。中文自动分词也是中文自然语言处理中的 最底层的一个环节。\n现有的方法：\n\n⊚ 基于词典的匹配：前向最大匹配、后向最大匹配。\n⊚ 基于字的标注：最大熵模型、条件随机场模型、感知器模型。\n⊚ 其他方法：与词性标注结合、与句法分析结合。\n\n\n例如以下是博主写的一个简单的测试\n\n\n代码\n\"\"\" author:jjk datetime:2018/11/1 coding:utf-8 project name:Pycharm_workstation Program function: 中文分词 结巴分词 \"\"\" import jieba # 导入结巴分词包 import jieba.posseg as pseg import time # 时间 time_Start = time.time() #f=open(\"t_with_splitter.txt\",\"r\")#读取文本 #string=f.read().decode(\"utf-8\") string = '中文自动分词指的是使用计算机自动对中文文本进行词语的切分，' + \\ '即像英文那样使得中文句子中的词之间有空格以标识。' + \\ '中文自动分词被认为是中文自然语言处理中的一个最基本的环节' words = pseg.cut(string) # 进行分词 result = \"\" #记录最终结果的变量 for w in words: result += str(w.word) + \"/\" + str(w.flag) # 加词性标注 print(result) # 输出结果 f = open(\"result.txt\",\"w\") #将结果保存到另一个文档中 f.write(result) f.close() time_Stop = time.time() print(\"分词及词性标注完成，耗时：\"+str(time_Stop-time_Start)+\"秒。\")# 输出结果\n结果\n\n\n4、词性标注\n词性标注（Part-of-Speech tagging 或POS tagging) 又称词类标注或者简称标注，是指在\n词性标记集已确定，并且词典中每个词都有确定词性\n的基础上，将一个输入词串转换成相应词性标记串的过程。如上 3、中文自动分词 中举的例子的结果所示。\n在汉语中，因为汉语词汇词性多变的情况比较少见，大多词语只有一个词性，或者出现次最高的词性远远高于第二位的词性，相对比较简单。同时，它也受到一些条件约束。比如：兼类词在具体语境中的词性判定问题、未登录词即新词词性问题、兼类词问题等。\n\n词性标注方法\n包括概率方法、隐马尔可夫模型的词性标注方法、机器学习规则的方法等。\n5、句法分析\n句法分析\n句法分析（Parsing）就是指对句子中的\n词语语法功能\n进行分析。比如“欢迎大家使用演示平台”就可以表示为\"欢迎\\VV 大家\\PN 使用\\VV 演示\\NN 平台\\NN\"。\n\n句法分析在中文信息处理中的主要应用\n包括机器翻译、命名实体识别等。\n自然语言生成\n自然语言生成研究使计算机具有人一样的表达和写作功能，即能够根据一些关键信息及其在机器内部的表达形式，经过一个规划过程，自动生成一段高质量的自然语言文本。自然语言处理包括\n自然语言理解和自然语言生成\n。\n自然语言生成是人工智能和计算语言学的分支\n，相应的语言生成系统是基于语言信息处理的计算机模型，其工作过程与自然语言分析相反，从抽象的概念层次开始，通过选择并执行一定的语义和语法规则来生成文本。\n6、文本分类\n文本分类用计算机对文本集按照\n一定的分类器模型\n进行自动分类标记。文本分类的总体过程如下（引用自 NLPIR 汉语分词系统）。\n（1） 预处理：将原始语料格式化为同一格式，便于后续的统一处理。\n（2） 索引：将文档分解为基本处理单元，同时降低后续处理的开销。\n（3） 统计：词频统计，项（单词、概念）与分类的相关概率。\n（4） 特征抽取：从文档中抽取出反映文档主题的特征。\n（5） 分类器：分类器的训练。\n（6） 评价：分类器的测试结果分析。\n文本分类常用算法包括\n决策树、朴素贝叶斯、神经网络、支持向量机、线性最小平方拟合、KNN、遗传算法、最大熵\n等，广泛应用于垃圾过滤、新闻分类、词性标注等。\n7、文本挖掘\n文本挖掘一般指在\n文本处理过程中产生高质量的信息\n。高质量的信息通常通过\n分类和预测\n来产生，如模式识别。文本挖掘通常涉及输入文本的处理过程，产生结构化数据，并最终评价和解释输出。\n例如博主的这篇文章中对微信朋友圈个性签名生成词云的分析,就是一个文本挖掘。\n典型的\n文本挖掘方法\n包括\n文本分类、文本聚类、信息抽取、概念/实体挖掘、情感分析和观点分析等。\n8、信息抽取\n信息抽取（Information Extraction）是从\n大量文字数据中自动为访问数据库而抽取特定消息的技术\n。\n\n简单点来说\n从给定文本中抽取重要的信息，比如时间、地点、人物、事件、原因、结果、数字、日期、货币、专有名词等。\n大白话就是\n，就是要了解谁在什么时候、什么原因、对谁、做了什么事、有什么结果，涉及实体识别、时间抽取、因果关系抽取等关键技术。\n9、 问答系统\n问答系统（Question Answering）是当下自然语言处理研究的热点，也是未来自然语言处理的重点问题。从问答系统的外部行为来看，其与目前主流资讯检索技术有两点不同：\n首先\n是查询方式为完整而口语化的问句，\n再者\n是其回传的为高精准度网页结果或明确的答案字串。\n至此不知道小伙伴你有没有想到聊天机器人呀！！！\n10、机器翻译\n机器翻译（Machine Translation，经常简写为MT）属于计算语言学的范畴，是计算机程序将文字或演说从一种自然语言翻译成另一种自然语言 。简单来说，机器翻译是通过将一个自然语言的字辞取代成另一个语言的字辞来实现的。借由使用语料库的技术，可达成更加复杂的自动翻译，包阔可更佳地处理不同的文法结构、辞汇辨识、惯用语的对应等。\n这里用博主自己的大白话的理解就是：将一种语言(比如中文) 翻译成\n11、文本情感分析\n文本情感分析（也称为意见挖掘）是指用自然语言处理、文本挖掘及计算机语言学等方法来识别和提取原素材中的主观信息\n。通常来说，情感分析的目的是为了找出说话者/作者在某些话题上或者针对一个文本两极的观点的态度。这个态度或许是他的个人判断或评估，或许是他当时的情感状态（也就是说，作者在做出这个言论时的情绪状态），或是作者有意向的情感交流（就是作者想要读者所体验的情绪）等。\n总结：就是作者规定一些代表文本的态度词，然后使用可视化进行表现出来从而达到客户情感交流。\n12、自动摘要\n所谓自动摘要就是利用计算机自动地从原始文献中提取文摘，文摘是全面准确地反映某一文献中心内容的连贯短文。常用方法是自动摘要将文本作为句子的线性序列，将句子视为词的线性序列。\n自动摘要可以按照\n技术类型和信息提取分类。\n\n⊚ 技术应用类型：自动提取给定文章的摘要信息，自动计算文章中词的权重，自动计算\n文章中句子的权重。\n⊚ 信息提取：单篇文章的摘要自动提取，大规模文档的摘要自动提取，基于分类的摘要\n自动提取。\n举例如下所示：\n\"\"\" author:jjk datetime:2018/10/15 coding:utf-8 project name:Pycharm_workstation Program function: 查找关键词 思路： 1：加载已有的文档数据集 2：加载停用词表 3：对数据集中的文档进行分词 4：根据停用词表，过来干扰词 5：根据数据集训练算法 \"\"\" import math import jieba import jieba.posseg as psg from gensim import corpora, models from jieba import analyse import functools import numpy as np # 停用词加载方法 def get_stopword_list(): stop_word_path = './data/stopword.txt' # 遍历txt文档，剔除'' stopword_list = [sw.replace('\\n', '') for sw in open(stop_word_path, encoding='utf-8').readlines()] return stopword_list # 分词方法，调用结巴接口 # pos是判断是否采用词性标注的参数 def seg_to_list(sentence, pos=False): if not pos: # 不进行词性标注的分词方法 seg_list = jieba.cut(sentence) else: # 进行词性标注的分词方法 seg_list = psg.cut(sentence) return seg_list # 去除干扰词，根据pos判断是否过滤除名词外的其他词性，再判断词是否在停用词表中，长度是否大于等于2等。 def word_filter(seg_list, pos=False): stopword_list = get_stopword_list() filter_list = [] # 根据pos参数选择是否词性过滤 # 不进行词性过滤，则将词性都标记为n,表示全部保留 for seg in seg_list: if not pos: word = seg flag = 'n' else: word = seg.word flag = seg.flag if not flag.startswith('n'): continue # 过滤高停用词表中的词，以及长度为\u003c2的词 if not word in stopword_list and len(word) \u003e 1: filter_list.append(word) return filter_list # 数据加载 # corpus.txt为数据集 def load_data(pos=False, corpus_path='./data/corpus.txt'): # 调用上面 方式对数据集进行处理，处理之后的数据集仅保留非干扰词 doc_list = [] for line in open(corpus_path, 'rb'): content = line.strip() seg_list = seg_to_list(content, pos) filter_list = word_filter(seg_list, pos) doc_list.append(filter_list) return doc_list # idf值统计方法 def train_idf(doc_list): idf_dic = {} # 总文档数 tt_count = len(doc_list) # 每个词出现的文档数 for doc in doc_list: for word in set(doc): idf_dic[word] = idf_dic.get(word, 0.0) + 1.0 # 按公式转换为idf值，分母加1进行平滑处理 for k, v in idf_dic.items(): idf_dic[k] = math.log(tt_count / (1.0 + v)) # 对于没有在字典中的词，默认其尽在一个文档出现，得到默认idf值 default_idf = math.log(tt_count / (1.0)) return idf_dic, default_idf # topK # cmp()函数是为了输出top关键词时，先按照关键词的计算分值排序，在得分相同时，根据关键词进行排序时 def cmp(e1, e2): # import numpy as np res = np.sign(e1[1] - e2[1]) if res != 0: return res else: a = e1[0] + e2[0] b = e2[0] + e1[0] if a \u003e b: return 1 elif a == b: return 0 else: return -1 # TF-IDF类 class TfIdf(object): # 训练好的idf字典，默认idf值，处理后的待提取文本，关键词数量 def __init__(self, idf_dic, default_idf, word_list, keyword_num): self.word_list = word_list self.idf_dic, self.default_idf = idf_dic, default_idf self.tf_dic = self.get_tf_dic() self.keyword_num = keyword_num # 统计tf值 def get_tf_dic(self): tf_dic = {} for word in self.word_list: tf_dic[word] = tf_dic.get(word, 0.0) + 1.0 tt_count = len(self.word_list) for k, v in tf_dic.items(): tf_dic[k] = float(v) / tt_count return tf_dic # 按公式计算tf-idf def get_tfidf(self): tfidf_dic = {} for word in self.word_list: idf = self.idf_dic.get(word, self.default_idf) tf = self.tf_dic.get(word, 0) tfidf = tf * idf tfidf_dic[word] = tfidf # 根据tf-idf排序，取排名前keyword_num的词作为关键词 for k, v in sorted(tfidf_dic.items(), key=functools.cmp_to_key(cmp), reverse=True)[:self.keyword_num]: print(k + \"/\", end='') print() # 主题模型 class TopicModel(object): # def __init__(self, doc_list, keyword_num, model=\"LSI\", num_topics=4): # 使用gensim接口，将文本转为向量化表示 self.dictionary = corpora.Dictionary(doc_list) # 使用BOW模型向量化 corpus = [self.dictionary.doc2bow(doc) for doc in doc_list] # 对每个词，根据tf-idf进行加权，得到加权后的向量表示 self.tfidf_model = models.TfidfModel(corpus) self.corpus_tfidf = self.tfidf_model[corpus] self.keyword_num = keyword_num self.num_topics = num_topics # 选择加载的模型 if model == 'LSI': self.model = self.train_lsi() else: self.model = self.train_lda() # 得到数据集的主题-词分布 word_dic = self.word_dictionary(doc_list) self.wordtopic_dic = self.get_wordtopic(word_dic) def train_lsi(self): lsi = models.LsiModel(self.corpus_tfidf, id2word=self.dictionary, num_topics=self.num_topics) return lsi def train_lda(self): lda = models.LdaModel(self.corpus_tfidf, id2word=self.dictionary, num_topics=self.num_topics) return lda def get_wordtopic(self, word_dic): wordtopic_dic = {} for word in word_dic: single_list = [word] wordcorpus = self.tfidf_model[self.dictionary.doc2bow(single_list)] wordtopic = self.model[wordcorpus] wordtopic_dic[word] = wordtopic return wordtopic_dic # 词空间构建方法和向量化方法，在没有gensim接口时的一般处理方法 def word_dictionary(self, doc_list): dictionary = [] for doc in doc_list: dictionary.extend(doc) dictionary = list(set(dictionary)) return dictionary def doc2bowvec(self, word_list): vec_list = [1 if word in word_list else 0 for word in self.dictionary] return vec_list # 计算词的分布和文档的分布的相似度，取相似度最高的keyword_num个词作为关键词 def get_simword(self, word_list): sentcorpus = self.tfidf_model[self.dictionary.doc2bow(word_list)] senttopic = self.model[sentcorpus] # 余弦相似度计算 def calsim(l1, l2): a, b, c = 0.0, 0.0, 0.0 for t1, t2 in zip(l1, l2): x1 = t1[1] x2 = t2[1] a += x1 * x1 b += x1 * x1 c += x2 * x2 sim = a / math.sqrt(b * c) if not (b * c) == 0.0 else 0.0 return sim # 计算输入文本和每个词的主题分布相似度 sim_dic = {} for k, v in self.wordtopic_dic.items(): if k not in word_list: continue sim = calsim(v, senttopic) sim_dic[k] = sim for k, v in sorted(sim_dic.items(), key=functools.cmp_to_key(cmp), reverse=True)[:self.keyword_num]: print(k + \"/ \", end='') print() def tfidf_extract(word_list, pos=False, keyword_num=10): doc_list = load_data(pos) idf_dic, default_idf = train_idf(doc_list) tfidf_model = TfIdf(idf_dic, default_idf, word_list, keyword_num) tfidf_model.get_tfidf() def textrank_extract(text, pos=False, keyword_num=10): textrank = analyse.textrank keywords = textrank(text, keyword_num) # 输出抽取出的关键词 for keyword in keywords: print(keyword + \"/\") # print() def topic_extract(word_list, model, pos=False, keyword_num=10): doc_list = load_data(pos) topic_model = TopicModel(doc_list, keyword_num, model=model) topic_model.get_simword(word_list) if __name__ == '__main__': # 获取测试文本 text1 = 'test.txt' text = open(text1, encoding='utf-8').read() print(text) pos = True seg_list = seg_to_list(text, pos) filter_list = word_filter(seg_list, pos) print('\\nTF-IDF模型结果：') tfidf_extract(filter_list) print('\\nTextRank模型结果：') textrank_extract(text) print('\\nLSI模型结果：') topic_extract(filter_list, 'LSI', pos) print('\\nLDA模型结果：') topic_extract(filter_list, 'LDA', pos)\n结果\n\n13、文字蕴涵\n文字蕴涵（Textual Entailment，TE）\n文字蕴涵在自然语言处理中主要指一个\n文字片段之间的定向关系。\n\n⊚ 正向蕴涵\n文本T：日本时间2011 年3 日11 日，日本宫城县发生里氏震级9.0 震，造成死伤失踪约3 万多人。\n假设H：日本时间2011 年3 日11 日，日本宫城县发生里氏震级9.0 强震。\n⊚ 矛盾蕴涵\n文本T：张学友在1961 年7 月10 日，生于香港，祖籍天津。\n假设H：张学友生于1960 年。\n⊚ 独立蕴涵\n文本T：黎姿与“残障富豪”马廷强结婚。\n假设H：马廷强为香港“东方报业集团”创办人之一马惜如之子。\n三、自然语言处理的难点\n1、语言环境复杂\n自然语言处理的语言环境较为复杂，以\n命名实体识别进行分析，对于同一个汉字某些情况下可以看作实体处理，某些情况则不能看作实体。\n\n例如，天龙八部中的“竹剑”小姐姐 在有些情况下可能就是指的是竹子做得剑。还有“湖北” 有可能指定是地点 “湖北”，也有可能指定是“湖的北边”。可见字自然语言处理过程中语言环境(根据上下文才能究其表达的意思)的复杂。\n2、文本结构形式多样\n文本内部结构形式多样。还是以自然语言处理中的\n命名实体识别任务\n为例子，例如：\n\n⊚ 人名\n，人名由姓和名构成。其中姓氏包括单姓和复姓（如赵、钱、孙、李、慕容、东方、西门等），名由若干个汉字组成。姓氏的用字范围相对有限，比较容易识别。然而名就比较灵活，既可以用名、字、号表示，也可以使用职务名和用典。比如：“李白、李十二、李翰林、李供奉、李拾遗、李太白、青莲居士，谪仙人”都是同一个人。\n\n⊚ 地名\n，\n一般由若干个字组成地名，可以为作为后缀关键字或者别名，都是指代一个地方\n。比如：“成都、蓉城、锦城、芙蓉城、锦官城、天府之国”，其中“蓉城、锦城、芙蓉城、锦官城、天府之国”为别名。除了全称的名称，还有地理位置代表地名的。比如：“河南、河南省、豫”都是指的一个省份，其中“豫”是简称。\n⊚\n组织机构名\n，组织机构命名方式比较复杂，有些是修饰性的命名，有些表示历史典故，有些表示地理方位，有些表示地名，有些表示风俗习惯和关键字等。例如：组织名“广州恒大淘宝足球俱乐部”中，“广州”表示地名的成分，“恒大”“淘宝”表示公司名称成分，“足球”是一项体育赛事成分，“俱乐部”是关键字的成分。比如：“四川大学附属中学”（四川省成都市第十二中学）中包括另一个机构名“四川大学”。机构名还可以以简称形式表示，比如：“四川大学附属中学”简称“川大附中”，“成都信息工程大学”简称“成信大\"。\n3、边界识别限制\n在自然语言处理任务中，边界识别最广泛应用于\n命名识别\n当中。边界识别可以分解为\n两大任务：\n如何去识别实体的边界；如何去判定实体的类别（诸如人名、地名、机构名）。\n中文命名实体识别要比英文命名实体识别更为复杂，\n一是\n受中文自身语言特性的限制，不同于英语文本中词间有空格界定；\n二是\n英文中的实体一般首字母大写容易区分，例如：‘Jobs wasadopted at birth in San Francisco，and raised in a hotbed of counterculture’ 中，人名乔布斯Jobs的首字母大写，地名旧金山San Francisco 的首字母也是大写,而中文不具备这样的特征。\n4、词义消岐\n词义消歧\n词义消歧是一个自然语言处理和本体论的开放问题。\n歧义与消歧\n是自然语言理解中最核心的问题，在词义、句义、篇章含义层次都会出现语言根据上下文语义而产生不同含义的现象。\n消歧即指根据上下文\n确定对象语义\n的过程\n，\n词义消歧即在词语层次上的语义消歧。\n语义消歧/词义消歧是自然语言处理任务的一个核心与难点，影响了几乎所有任务的性能，比如搜索引擎、意见挖掘、文本理解与产生、推理等。\n词性标注和词义消岐\n词性标注与词义消歧是相互关联的两个问题，在语言使用者身上它们往往同时能得到满足。但是目前的\n计算机系统\n一般并不能让\n二者共用参数并同时输出。\n语义理解包括分词、词性标注、词义消歧、句法解析、语义解析等。它们并不是前馈的，是相互依赖并存在反馈的。\n词性标注与语义消歧都要依赖上下文来标注\n，\n但是词性标注比语义消歧处理起来要更简单\n，最终结果也往往较好。\n主要原因\n是词性标注的标注集合是确定的，而语义消歧并没有，并且量级上词性标注要大得多；词性标注的上下文依赖比语义消歧要短。\n\n举例说明\n\n许多字词不单只有一个意思，因而我们必须选出使句意最为通顺的解释。看下面歧义的句子，词义消歧就是要分析出特定上下文的词被赋予的到底是哪个意思。\n（1） 川大学生上网成瘾如患绝症。歧义在于“川大学生”——四川大学的学生；四川的大学生。\n（2） 两代教授，人格不同。歧义：“两代”——两位代理教授；两个时代的教授。\n（3） 被控私分国有资产，专家总经理成了被告人。歧义：“专家总经理”——专家和总经理；有专家身份的总经理。\n（4） 新生市场苦熬淡季。歧义：“新生”——新学生的市场；新产生的市场。\n（5） 朝鲜十年走近国际社会一步。歧义：“十年走近国际社会一步”——每十年就向国际社会走近一步；最近十年间向国际社会走近了一步\n（6） 新汽车牌照。歧义：“新”——新的汽车；新的牌照。\n（7） 咬死了猎人的狗。歧义：——猎人的狗被咬死了；把猎人咬死了的那条狗。\n（8） 菜不热了。歧义：“热”——指菜凉了；指菜不加热了。\n（9） 还欠款四万元。歧义：“还”——读huai；读hai。\n（10） 北京人多。歧义：——北京/人多；北京人/多。\n5、指代消解\n定义\n指代消解（Anaphora Resolution）是自然语言处理的重要内容，在信息抽取时就用到了指代消解技术\n中文的三种典型指代\n\n（1） 人称代词：\n李明怕高妈妈一个人呆在家里寂寞，【他】便将家里的电视搬了过来。\n\n（2） 指示代词：\n很多人都想创造一个美好的世界留给孩子，【这】可以理解，但不完全正确。\n\n（3） 有定描述：\n贸易制裁似乎成了美国政府在对华关系中惯用的大棒。然而，这【大棒】果真如美国政府所希望的那样灵验吗？\n典型指代消解\n\n⊚ 显性代词消解\n\n所谓显性代词消解，就是指在\n篇章中确定显性代词指向哪个名词短语的问题\n，代词称为指示语或照应语（Anaphor），其所指向的名词短语一般被称为先行语（Antecedent）。根据二者之间的先后位置，可分为回指（Anaphora）与预指（Cataphora），其中：如果先行语出现在指示语之前，则称为回指，反之则称为预指。\n\n⊚ 零代词消解\n\n所谓零代词消解，是代词消解中针对零指代（Zero Anaphora）现象的一类特殊的消解。\n\n⊚ 共指消解\n\n所谓共指消解，\n是将篇章中指向同一现实世界客观实体（Entity）的词语划分到同一个等价集的过程\n，其中被划分的词语称为表述或指称语（Mention），形成的等价集称为共指链（Coreference Chain）。在共指消解中，指称语包含普通名词、专有名词和代词，因此可以将显性代词消解看作共指消解针对代词的子问题。共指消解与显性代词消解不同，它更关注在指称语集合上进行的等价划分，评测方法与显性代词消解也不尽相同，通常使用 MUC、 B-CUBED、CEAF 和 BLANC 等评价方法。\n\n指代消解的研究方法大致可以分为\n基于启发式规则的、基于统计的和基于深度学习的方法\n。目前看来，基于有\n监督统计机器学习\n的消解算法仍然是主流算法。\n典型例子\n指代消解是解决\n“谁对谁做了什么”，\n处理如上所述的自然语言的问题，下面看看例子：\n（1） 美国政府表示仍然支持强势美元，但这到底只是嘴上说说还是要采取果断措施，经济学家对此的看法是否定的。\n（2） 今天老师又在班会上表扬了自己，但是我觉得还需要继续努力。\n（3） 三妹拉着葛姐的手说，她老家在偏远的山区，因为和家里赌气才跑到北京打工的，接着她又哭泣起自己的遭遇来。\n（4） 当他把证书发给小钱时，他对他笑了。\n（5） 小明和肖华去公园玩，他摔了一跤，他急忙把他扶起来。\n（6） 星期天, 小雨和小英到田老师家补习功课，她一早就打电话给她约好在红旗饭店吃早餐。\n四、展望自然语言处理\n关于在2017年第三届中国人工智能大会上来自哈尔滨工业大学的刘挺教授对自然语言处理的一个发展趋势的一个总结归纳。\n归纳链接：http://www.sohu.com/a/163742617_610522","date":"2018年11月05日 00:11:43"}
{"_id":{"$oid":"5d36a89f6734bd8e681d5dc9"},"title":"自然语言处理入门读物","author":"江中舟","content":"自然语言处理入门读物\n本文目前研二，已经接触自然语言处理有一年的时间（半路出家），下面写一点自己关于自然语言处理的心得（纯属个人见解），先从入门学习开始写吧。\n书籍-理论篇\n书籍是人类进步的阶梯，这个一点不假，自己刚开始接触自然语言处理是从吴军老师的的《数学之美》开始的，这里再次感谢吴军老师。这门书写的通俗易懂，内容非常的吸引人，读起来不会感觉枯燥，每次读都会有新的体会。\n本书可以作为自然语言处理入门的第一本书，书中的知识一定要查阅其他文献和博客，因为本书并没有对内容讲的特别的细，所以每个知识都包含了大量的拓展内容。例如：隐马尔可夫的内容，其实远比书中将的多，所以拓展是配合本书的不二法门。\n第二本书推荐《统计自然语言处理(第2版)》（宗成庆）蓝皮版，这本书是宗老师的心血之作，内容较《数学之美》的内容更加的偏特定领域，对自然语言处理领域有宗老师自己独特的见解。书本内容涉及自然语言处理的大部分方向，且对重要的知识点给出了较为详细的理论推导，语言通俗易懂。\n第三本书推荐《统计学习方法》（李航），这本书的内容就更加偏数学化，主要对自然语言处理，特别是统计自然语言处理中应用的模型给出了非常详尽的数学公式推导，建议有一定数学功底的同学阅读（因为我读起来很吃力）。\n此外，还有很多业界推荐的好书：《自然语言处理简明教程》（冯志伟），《自然语言处理综论》（Daniel Jurafsky），《自然语言处理的形式模型》（冯志伟），但是这些书因为时间和个人精力有限，尚未曾阅读，此处仅列出。\n书籍——实践篇\n自然语言处理领域使用较多的语言是python，所以建议使用python来处理自然语言处理领域的相关内容；另外自然语言处理领域，特别是基于统计的自然语言处理以及当前大热的深度学习下的自然语言处理，往往使用大量的机器学习知识和深度学习知识。\n书籍推荐1：python基础教程（翻译版），本书的内容已经足够入门了，书本的内容不必大而全的全部阅读，抓住主要的，想要精通以后慢慢来。\npython入门博客推荐2：廖雪峰的python教程 非常的不错，也是抓住主要的。\n以上两个已经足够我们python入门啦。\npython练习环境推荐3：强烈推荐ipython和ipython notebook，不知道的百度哦，谁用谁知道。\n机器学习书籍推荐4：《机器学习实战》，这本书中的内容既有理论说明也有代码讲解，并且代码可以在书本提供的网站上下载，但是不推荐一上来就看这本书，因为如果原理没搞懂，直接上代码，感觉不理解；此外书本中用到了numpy等python库，若之前对其没有了解，直接学习，比较痛苦。\n机器学习书籍推荐5：西瓜书《机器学习》（周志华），本人有周老师亲笔签名的书籍，想想都开心。本书的内容介绍非常全面，知识讲解也非常的到位，理论知识较多，代码不多，非常适合阅读。主要是国内的，国内的，国内的，没错这本书是国内的，不是翻译的，支持，必须支持。\n既然学习了python和机器学习，那总不能python停留在练习上，机器学习停留在理论上吧，所以：\n书籍推荐6：《集体智慧编程》，《python自然语言处理》，前一本是使用python语言，编写代码实现一些现实的问题，通过学习可以切实的感受到原来他一直都在，只是我不知道。。。。。后者介绍了python的一个自然语言处理库NLTK，使用该库解决自然语言处理中的任务。\n至此，理论也有了，实践也有了，可以说非常完美了。神马，还不够，你觉得读书太累，一读就困。。。。。。那你就看点视频吧\n视频\n推荐1自然语言处理-宗庆成\n不要问我怎么样，因为我没看过，因为我不可能什么都看过。但是宗老师的课那是没问题的。\n推荐2自然语言处理-关毅\n这个课我看过，感觉不错。\n推荐3计算语言学概论_侯敏\n本人没看过。\n推荐4 哥伦比亚大学的自然语言处理，英文的。具体的课程地址已经变动，可以百度一下。也可以看一下他人的博客自然语言处理大菜鸟 ，自己英语不好，就看看别人的心得喽。当然coursera上还有一些其他的视频资源，如果你的英语还可以，可以去平台上搜索一下。\n自然语言处理已经有视频来了，那么机器学习有什么视频看吗？当然有\n推荐5 mooc学院-机器学习 这个视频是大牛Andrew Ng讲的，非常的不错，毕竟是大牛嘛，比我等凡人理解的深太多了。\n推荐6 这个推荐就是众多的mooc网站了，因为推荐5是我自己看的视频，但是很多慕课网站都存在机器学习课程，如网易公开课-机器学习，也是Andrew Ng讲的；慕课网-初识机器学习 ；台湾大学林轩田机器学习 林老师的机器学习讲解，非常的不错。\n现在好啦，我们可以拿本书，一边学习书本知识，一边看视频讲解，我想这会让自己很快的入门，想想都开心。。。。\n机器学习领域重要的会议\n国际机器学习会议（ICML）\n国际神经信息处理系统会议（NIPS）\n国际学习理论会议（COLT）\n欧洲机器学习会议（ECML）\n亚洲机器学习会议（ACML）\n重要的国际学术期刊\nJournal of Machine Learning Research\nMachine Learning\nIJCAI\nAAAI\nArtificial Intelligence\nJournal of Artificial Intelligence Research\n参考：\n1 自然语言处理怎么最快入门 知乎上的大神","date":"2016年12月13日 19:23:31"}
{"_id":{"$oid":"5d36a8a06734bd8e681d5dcd"},"title":"自然语言处理期刊","author":"weixin_33851604","content":"国内自然语言处理期刊\n现代语言学(汉斯出版社)\n汉斯出版社（Hans Publishers, www.hanspub.org) 聚焦于国际开源 (Open Access) 中文期刊的出版发行, 覆盖以下领域: 数学物理、生命科学、化学材料、地球环境、医药卫生、工程技术、信息通讯、人文社科、经济管理等。秉承着传播文化，促进交流的理念，本社将积极探索中文学术期刊国际化道路，并积极推进中国学术思想走向世界。目前，汉斯出版社的所有期刊均被知网（CNKI Scholar）等数据库收录。其中，23本被美国《化学文摘Chemical Abstracts》收录，30本被EBSCO收录。\n计算机学报\n《计算机学报》刊登的内容覆盖计算机领域的各个学科，以论文、技术报告、短文、研究简报、综论等形式报道以下方面的科研成果：计算机科学理论、计算机硬件体系结构、计算机软件、人工智能、数据库、计算机网络与多媒体、计算机辅助设计与图形学以及新技术应用等。\n计算机研究与发展\n刊登内容：计算机科学技术领域高水平的学术论文、最新科研成果和重大应用成果。刊登内容：综述、软件技术、信息安全、计算机网络、体系结构、人工智能、计算机应用技术（图形图象、自然语言处理、信息检索）、数据库技术、存储技术及计算机计算机基础理论等相关领域。\n《软件学报》\n《软件学报》注重刊登反映计算机科学和计算机软件新理论、新方法和新技术以及学科发展趋势的文章,主要涉及理论计算机科学、算法设计与分析、系统软件与软件工程、模式识别与人工智能、数据库技术、计算机网络、信息安全、计算机图形学与计算机辅助设计、多媒体技术及其他相关的内容.\n中国中文信息学会\n学会的学术研究内容是利用计算机对汉语的音、形、义等语言文字信息进行的加工和操作，包括对字、词、短语、句、篇章的输入、输出、识别、转换、压缩、存储、检索、分析、理解和生成等各方面的处理技术。中文信息处理学科是在语言文字学、计算机应用技术、人工智能、认知心理学和数学等相关学科的基础上形成的一门新兴的边缘学科。\n中国中文信息学会2018年学术活动计划\n国际自然语言处理及中文计算会议\n中文信息学报\n《中文信息学报》刊登内容有：计算语言学，包括：音位学、词法、句法、语义、知识本体和语用学；语言资源，包括：计算词汇学、术语、电子词典和语料库；机器翻译（MT）或机器辅助翻译（MAT）；汉语和少数民族语言文字输入输出和处理；中文手写和印刷体识别（OCR）；中文语音识别与合成以及文语转换（TTS）；信息检索（IR）信息抽取（IE）及相关的语言技术；网上搜索引擎；数据挖掘、知识获取、神经网络、机器学习、专家系统、知识工程和其他人工智能（AI）技术。\n国外自然语言处理期刊\n【2018年自然语言处理及相关国际会议重要日期整理】\nNLP会议\n会议名称\n截稿日期\n通知日期\n会议日期\n举办地点\nACL 2018\n2.22\n4.20\n7.15-7.20\n墨尔本，澳大利亚\nMNLP 2018‍\n5.22\n8.6\n10.31-11.04\n布鲁塞尔，比利时\nNAACL HLT 2018\n已过\n已过\n6.01-6.06\n新奥尔良，美国\nCOLING 2018\n3.16\n5.17\n8.20-8.25\n圣达菲，美国\nCICLING 2018\n已过\n已过\n3.18-3.24\n河内,越南\n相关会议\n会议名称\n截稿日期\n通知日期\n会议日期\n举办地点\nIJCAI-ECAI 2018\n已过\n4.16\n7.13-7.19\n斯德哥尔摩，瑞典\nAAAI 2018\n已过\n已过\n2.02-2.07\n新奥尔良，美国\nNIPS 2018\n待定\n待定\n12.03-12.08\n蒙特利尔，加拿大\nICML 2018\n已过\n5.11\n7.10-7.15\n斯德哥尔摩，瑞典\nSIGIR 2018\n已过\n4.11\n7.08-7.12\n安娜堡，美国\nKDD 2018\n已过\n5.06\n8.19-8.23\n伦敦，英国\nWSDM 2018\n已过\n已过\n2.06-2.08\n洛杉矶，美国\nCIKM 2018\n5.15\n8.06\n10.22-10.26\n灵格托，意大利\nWWW 2018\n已过\n已过\n4.23-4.27\n里昂，法国","date":"2018年04月01日 14:18:00","data":"2018年04月01日 14:18:00"}
{"_id":{"$oid":"5d36a8a06734bd8e681d5dcf"},"title":"自然语言处理综合","author":"luv_dusk","content":"目录\n一、自然语言处理介绍\n概念\n子领域\n数据集\n工具包\n二、API\nJieba (分词)\nPyltp (分词、词性标注、命名实体识别、句法依存树、语义角色标注)\nNLTK (词性、词性标注、提取词频、提取词根、词形还原、编辑距离)\nPre-trained BERT (特征提取)\nGloVe (词嵌入向量)\nSpacy (词嵌入向量)\nGensim (词嵌入训练)\n其他\n三、相关算法\n一、自然语言处理介绍\n概念\n通俗而言，自然语言处理 (Natural Language Processing) 即为处理与人类语言相关的各项任务。与计算机视觉类似，是一个由来已久，却在近几年被神经网络颠覆的传统领域，在人机对话、搜索引擎、后台广告推荐、机器翻译、语音识别等领域有广泛应用。传统的自然语言处理以统计学为根基，发展出了各具特色的优异模型，其中最为著名的包括 朴素贝叶斯、隐马尔科夫模型 (HMM)、条件随机场 (CRF)。神经网络的出现使得 NLP 领域得到空前的发展，从 Word2Vec (2013) 到 Attention机制 (2014)、Transformer (2017)、号称开启 NLP 新纪元的集大成者 BERT (2018)，再到近期卡内基梅隆大学的团队研发的 XLNet (2019)，NLP 在不断树立新的里程碑，走在人工智能的前沿。\n子领域\n语音文本\n文本朗读（Text to Speech）\n语音合成（Speech Synthesis）\n语音识别（Speech Recognition）\n自然语言理解\n中文分词（Chinese Word Segmentation）\n词性标注（Part-of-Speech Tagging）\n句法分析（Parsing）\n情绪分析（Sentiment Analysis）\n文字蕴涵（Textual Entailment）\n自然语言生成（Natural Language Generation）\n问答系统（Question Answering）\n人机对话（Man-Machine Interaction）\n文字校对（Text-Proofing）\n机器翻译（Machine Translation）\n大型文本分析\n信息抽取（Information Extraction）\n自动摘要（Automatic Summarization）\n文本分类（Text Categorization）\n信息检索（Information Retrieval）\n字符串处理\n模式匹配（Pattern Matching）\n文本相似度（Text Similarity）\n文本压缩（Text Compression）\n数据集\n数据集\n内容\n领域\n语言\n数量\nIWSLT\nTED演讲多国语言字幕\n机器翻译\n中英\n不限\nSQuAD\n维基百科词条\n文档问答\n英文\n150,000+\nDuReader\n用户日志\n文档问答\n中文\n-\nCoQA\n人为对话\n对话问答\n英文\n127,000+\nLOB\n历史文献\n词性标注\n英文\n1,000,000\n这里只呈现笔者自己熟悉的数据集，网上有很多关于开源数据集的总结博文，这里推荐几篇：https://www.jiqizhixin.com/articles/2018-09-05-2\nhttps://blog.csdn.net/enohtzvqijxo00atz3y8/article/details/80163069\n工具包\n中文 NLP 领域著名的 Python 工具包列示如下：\nPython Language Technology Platform (Pyltp)\nPyltp 是 LTP 的 Python 封装，提供了分词，词性标注，命名实体识别，依存句法分析，语义角色标注的功能。\nJieba\n专业提供分词功能的工具包。\nStandford NLP\n除可以实现 Pyltp 的功能以外，还能进行情绪分析，但安装较为复杂，需要通过 Java 安装并设置 Python 接口。\n英文 NLP 领域有：\nNatural Language Toolkit (NLTK)\nA Python library that provides modules for processing text, classifying, frequency analyzing, tokenizing, stemming, part-of-speech tagging, parsing, and more.\nApache OpenNLP\nA machine learning toolkit that provides tokenizers, sentence segmentation, part-of-speech tagging, named entity extraction, chunking, parsing, coreference resolution, and more.\nStandford NLP\nA suite of NLP tools that provide part-of-speech tagging, the named entity recognizer, coreference resolution system, sentiment analysis, and more.\nGlounNLP\nProvides implementations of the state-of-the-art (SOTA) deep learning models in NLP, and build blocks for text data pipelines and models. It is designed for engineers, researchers, and students to fast prototype research ideas and products based on these models.\n二、API\n笔者将分词、词性标注、命名实体识别、句法分析、语义角色标注等应用领域底层的 NLP 任务定义为基础 NLP 任务。在实际的应用研究与开发时，由于语料库的准备成本较高，预训练通常也耗时过长。在研究时，这一部分任务通常可以通过调用第三方专业机构预训练好的模型实现，将更多的注意力集中到上层模型的设计和搭建。特别地，当预训练的模型无法满足实际的业务需求时，可以通过定义用户词典修正模型结果。本章详细列示基础 NLP 任务的 API 实现。\n由于中文的特殊性，中文 NLP 与 英文 NLP 的一大不同在于中文文本处理需要借助语料库预先对语句进行分词，而英文只需要通过空格即可完成。在 Python 语言环境下，运用 Nshort 中文分词算法的 Jieba 出于杰出的分词效果，以及安装和使用方便，成为最为著名的中文分词工具。在其他的基础 NLP 任务上，哈尔滨工业大学开发的 Pyltp 库更为专业和全面，词库储备也更为丰富，在学术界和工业界得到广泛应用。\nJieba (分词)\n关于 Jieba，以下仅列示分词相关代码：\nimport jieba sentence = '里约热内卢的奶牛拿榴莲牛奶以折足之姿跑到委内瑞拉拿了蜂花护发素送给红鲤鱼与绿鲤鱼与驴' wordlist = jieba.cut(sentence) #精确模式 wordlist = jieba.cut(sentence, cut_all=True) #全模式 wordlist = jieba.cut_for_search(sentence) #搜索引擎模式 jieba.load_userdict(open(r'D:\\NLP resources\\jieba userdict.txt',encoding='gbk')) #导入用户词典\n精确模式：试图将句子最精确地切开，适合文本分析；\n全模式：把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义；\n搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词；\n使用用户词典：由用户自行选择地址新建词典 txt 文档，需要满足每行’词语 词频 词性’的编写要求，例：‘榴莲牛奶 5 n’。\nPyltp (分词、词性标注、命名实体识别、句法依存树、语义角色标注)\nfrom pyltp import * sentence = '里约热内卢的奶牛拿榴莲牛奶，以折足之姿跑到委内瑞拉拿了蜂花护发素，送给红鲤鱼与绿鲤鱼与驴' # 分词 segmentor = Segmentor() segmentor.load(r'D:\\NLP resources\\cws.model') words = segmentor.segment(sentence) print(\"|\".join(words)) # 词性标注 pos_tagger = Postagger() pos_tagger.load(r'D:\\NLP resources\\pos.model') pos_tags = pos_tagger.postag(words) for word,pos_tag in zip(words,pos_tags): print(word+'/'+pos_tag) # 命名实体识别 recognizer = NamedEntityRecognizer() recognizer.load(r'D:\\NLP resources\\ner.model') ne_tags = recognizer.recognize(words,pos_tags) for word,pos_tag,ne_tag in zip(words,pos_tags,ne_tags): print(word+' / '+pos_tag+' / '+ne_tag) # 句法依存树 import nltk from nltk.tree import Tree from nltk.grammar import DependencyGrammar from nltk.parse import * import re parser = Parser() parser.load(r'D:\\NLP resources\\parser.model') arcs = parser.parse(words,pos_tags) conll = '' for i in range(len(arcs)): if arcs[i].head == 0: arcs[i].relation = 'ROOT' conll += '\\n' + words[i] + '(' + pos_tags[i] + ')\\t' + pos_tags[i] + '\\t' + str(arcs[i].head) + '\\t' + arcs[i].relation print(conll) conlltree = DependencyGraph(conll) tree = conlltree.tree() tree.draw() # 语义角色标注 labeller = SementicRoleLabeller() labeller.load(r'D:\\NLP resources\\pisrl_win.model') roles = labeller.label(words, pos_tags, arcs) for role in roles: print([words[role.index], ' '.join(['%s:%s'%(arg.name, ''.join(words[arg.range.start:arg.range.end+1])) for arg in role.arguments])])\nNLTK (词性、词性标注、提取词频、提取词根、词形还原、编辑距离)\nimport nltk article = \"Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R\u0026B girl-group Destiny's Child.\" # 常用功能 tokens = nltk.word_tokenize(article) #分词 nltk.pos_tag(tokens) #词性标注 nltk.FreqDist(w.lower() for w in tokens) #提取词频 nltk.PorterStemmer().stem('lying') #提取词根 nltk.stem.WordNetLemmatizer().lemmatize('dancing','v') #词形还原 nltk.edit_distance('humble','dumpy') #编辑距离 # 查看nltk词性标注分类 nltk.help.upenn_tagset() # 下载功能包 nltk.download()\nPre-trained BERT (特征提取)\n当前最受欢迎的预训练 BERT 库，需要预先下载参数文件，Github 地址。 名为 BERT，实则同时包含了 GPT、Transformer-XL、GPT-2 的预训练参数。提取隐藏状态后可直接嫁接于任何下游任务。\nimport torch from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM # tokens text = \"[CLS] Who was Henson? [SEP] Jim [MASK] was a puppeteer [SEP]\" tokenizer = BertTokenizer.from_pretrained(r'D:\\NLP\\BERT\\pytorch-pretrained-BERT\\bert-tokenization-vocabulary.txt') #这里文本改为\"bert-base-uncased\"将自动下载参数文件(下同) tokens = tokenizer.tokenize(text) indicies = tokenizer.convert_tokens_to_ids(tokens) tokens_tensor = torch.tensor([indicies]) tokens_tensor = tokens_tensor.to('cuda') #迁移至GPU运行 # segments segments_ids = [0,0,0,0,0,0,1,1,1,1,1,1,1] segments_tensor = torch.tensor([segments_ids]) segments_tensor = segments_tensor.to('cuda') # model model = BertModel.from_pretrained(r'D:\\NLP\\BERT\\pytorch-pretrained-BERT\\bert-base-uncased.tar.gz') model.to('cuda') # forward with torch.no_grad(): encoded_layers,_ = model(tokens_tensor,segments_tensor)\nGloVe (词嵌入向量)\nGloVe 是斯坦福大学提供的专业词嵌入算法，在官网同时开放源码和预训练词向量供免费下载。\n# 使用预训练词向量模型 from tqdm import tqdm import numpy as np X = np.empty((400000,300)) word_to_id, id_to_word, idx = {},{},0 with open(r'D:\\NLP\\glove.6B\\glove.6B.300d.txt', 'r', encoding='utf-8') as f: #载入前需提前下载 for line in tqdm(f, total=400000): line = line.strip().split(' ') vector = list(map(float, line[1:])) X[idx,:] = vector word_to_id[line[0]] = idx id_to_word[idx] = line[0] idx += 1 # 查看词向量空间分布 import pandas as pd import matplotlib.pyplot as plt def pca(X): #PCA将词向量降至二维 X = pd.DataFrame(X) X = (X - X.mean()) / X.std() X = np.matrix(X) cov = (X.T * X) / X.shape[0] U, S, V = np.linalg.svd(cov) return U U = pca(X) #提取正交矩阵 Y = np.dot(X, U[:,:2]) #获取降维数据 ax = plt.subplot(111) for word in list(word_to_id.keys())[:20]: coordinate = Y[word_to_id[word],:] ax.scatter(coordinate[0,0],coordinate[0,1]) ax.annotate(word, xy=(coordinate[0,0], coordinate[0,1]), #坐标点 xycoords='data', #坐标点类型 xytext=(+5, +5), #标注文字相对位置 textcoords='offset points', #标注文字类型 fontsize=16) #标注文字大小 plt.show()\nSpacy (词嵌入向量)\nSpacy 是一个相较于 NLTK 执行效率更高，各基础任务准确度也更高的专业 NLP 工具包，在这里仅列示词嵌入向量的代码，感兴趣的读者可自行检索。\n# 使用预训练词向量模型 import spacy import numpy as np library = spacy.load('en_core_web_lg') #载入前需提前下载 article = 'let coward father mother brother sister juice milk is are be to 2013 2014 2015 2016 2017 2018' tokens = library(article) X = np.empty((0,300)) word_to_id, id_to_word, idx = {},{},0 for token in tokens: X = np.vstack((X,token.vector)) word_to_id[str(token)] = idx id_to_word[idx] = str(token) idx += 1 # 查看词向量空间分布 import pandas as pd import matplotlib.pyplot as plt def pca(X): #PCA将词向量降至二维 X = pd.DataFrame(X) X = (X - X.mean()) / X.std() X = np.matrix(X) cov = (X.T * X) / X.shape[0] U, S, V = np.linalg.svd(cov) return U U = pca(X) #提取正交矩阵 Y = np.dot(X, U[:,:2]) #获取降维数据 ax = plt.subplot(111) for word in word_to_id.keys(): coordinate = Y[word_to_id[word],:] ax.scatter(coordinate[0,0],coordinate[0,1]) ax.annotate(word, xy=(coordinate[0,0], coordinate[0,1]), #坐标点 xycoords='data', #坐标点类型 xytext=(+5, +5), #标注文字相对位置 textcoords='offset points', #标注文字类型 fontsize=16) #标注文字大小 plt.show()\nGensim (词嵌入训练)\n# 自行训练词向量 from gensim.models import Word2Vec sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]] model = Word2Vec(min_count=1) model.build_vocab(sentences) #搭建语料库 model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs) #训练 model.wv['cat'] #查看词向量 model.save('/word2vec') #保存模型 model.vocabulary.load('/word2vec') #读取模型\n其他\n# 拼写相似度 import difflib difflib.SequenceMatcher(None,'sequence','sequential').ratio()\n三、相关算法\n每一种应用都有经过长期考验，效果最佳的算法，列示如下。部分算法笔者提供代码实现，其余请读者自行搜索开放源码。\n算法\n应用\n链接\n朴素贝叶斯 (Naive Bayes)\n文本分类\n代码\n隐马尔科夫模型 (HMM)\n语音识别\n代码\n最大熵模型 (MEM)\n词性标注\n代码\n条件随机场 (CRF)\n中文分词、语义组块\n-\nTF-IDF + BM25\n搜索引擎、广告推荐\n-\nLDA (Latent Dirichelt Allocation)\n文本相似度\n-\nARC-1/ARC-2\n文本相似度\n-\nbi-LSTM + CRF\n命名实体识别、IOB/BIE序列标注\n-\nSeq2Seq\n机器翻译、文本会话、图像字幕、自然语言生成\n-\nDCN\n文档问答系统\n-\nBERT\n特征提取\n-\nGPT-2\n特征提取\n-\nXLNet\n特征提取\n-","date":"2019年07月06日 00:21:11"}
{"_id":{"$oid":"5d36a8a16734bd8e681d5dd1"},"title":"自然语言处理语言资源项目","author":"liuhuanyong_iscas","content":"项目地址：https://github.com/liuhuanyong/LanguageResources\n致力于利用web公开信息,采用爬虫脚本,加工处理形成语言资源包括词汇知识库,领域语料等语言资源,该资源可用于自然语言处理任务.\n1、　corpus_resources.py:词库，包括：\nname:人民日报语料 link:https://pan.baidu.com/s/1_E2YA7u61s_ZSSFV0IrHJA pwd:ux12 desc:人民日报199801语料 name:领域小说文本语料 link:https://pan.baidu.com/s/1JC3UyOu8PuJrnn_JUyF9UQ pwd:bguf desc:13个领域的小说文本集合，5000+小说文本 name:字幕文本语料 link:https://pan.baidu.com/s/19BI81W7rFwvLKEjVBPXaUA pwd:mpfz desc:基于字幕网抓取，70W字幕文本语料 name:段子文本语料 link:https://pan.baidu.com/s/1go84Pt8O-AHJJOgJhkG89Q pwd:eju6 desc:基于内涵段子等短文本网站抓取，约50W name:歌词文本语料 link:https://pan.baidu.com/s/1IOCH9EfZInTdI_GvnuedJA pwd:nq69 desc:基于歌词网站抓取，歌词数量约20W\n2 word_resources.py:领域语料\n包括：\nname:语义词库 link:https://pan.baidu.com/s/1b663-MVQ2UG69wvmKg912g pwd:flg8 desc:语法信息词典，知网义原、程度副词、现代汉语词典、否定词、同义词词林等 name:领域词库 link:https://pan.baidu.com/s/1fzwE94sC77PDo-36IKCkWg pwd:x57t desc:33个领域词词库 name:情感词库 link:https://pan.baidu.com/s/10KECT0kxiRDt43vuSBOdeA pwd:mn5u desc:通用、微博、食物、财经等领域情感词，以及公开情感词（清华、台湾大学、大连理工等） name:敏感词词库 link:https://pan.baidu.com/s/1DIkV-RyiEVaNMPNYiiKVsA pwd:asol desc:敏感词词库，可用于敏感信息检测 name:搜狗输入法词库 link:https://pan.baidu.com/s/11H8L0021TgnWEs8p4cjGkQ pwd:wpr8 desc:基于搜狗输入法抓取与转换生成，1W+个词库文本\n3 wordvector_resource.py:预训练词向量文件\nname:多领域词向量 link:https://pan.baidu.com/s/10j2Ozt9rOspVDsn_UNIfdw pwd:cw04 desc:基于腾讯历时滚动新闻训练的多领域词向量，包括财经、军事、体育、科技等领域 *********** name:中文字向量 link:https://pan.baidu.com/s/1m7E86igkOglQsl7hwn0QVw pwd:b2mg desc:基于维基百科生成的字向量\n资源已经共享至百度网盘,详细见相应的.py文件\nIf any question about the project or me ,see https://liuhuanyong.github.io/\n项目地址：https://github.com/liuhuanyong/LanguageResources","date":"2018年10月07日 20:48:50"}
{"_id":{"$oid":"5d36a8a16734bd8e681d5dd5"},"title":"百度自然语言处理","author":"Harrytsz","content":"新建 AipNlp:\nAipNlp 是自然语言处理的 Python SDK 客户端，为使用自然语言处理的开发人员提供了一系列的交互方法。参考如下代码新建一个 AipNlp:\nfrom aip import AipNlp \"\"\" 你的 APPID AK SK \"\"\" APP_ID = '##########' #'你的 APP ID' API_KEY = '##########' #'你的 Api key' SECRET_KEY = '##########' #'你的 Secret key' client = AipNlp(APP_ID, API_KEY, SECRET_KEY)\n配置AipNlp:\n如果用户需要配置 AipNlp 的网络请求参数（一般不需要配置），可以在构造 AipNlp 之后调用接口设置参数，目前只支持以下参数：\n接口\n说明\nsetConnectionTimeoutInMillis\n建立连接的超时时间（单位：毫秒）\nsetSocketTimeoutInMillis\n通过打开的连接传输数据的超时时间（单位：毫秒）\n接口说明：\n词法分析：\n词法分析接口向用户提供分词、词性标注、专名识别三大功能；能够识别出文本串中的基本词汇（分词），对这些词汇进行重组、标注组合后词汇的词性，并进一步识别出命名实体。\ntext = \"百度是一家高科技公司\" \"\"\" 调用词法分析 \"\"\" client.lexer(text)\n{'log_id': 3174179683102561622, 'text': '百度是一家高科技公司', 'items': [{'loc_details': [], 'byte_offset': 0, 'uri': '', 'pos': '', 'ne': 'ORG', 'item': '百度', 'basic_words': ['百度'], 'byte_length': 4, 'formal': ''}, {'loc_details': [], 'byte_offset': 4, 'uri': '', 'pos': 'v', 'ne': '', 'item': '是', 'basic_words': ['是'], 'byte_length': 2, 'formal': ''}, {'loc_details': [], 'byte_offset': 6, 'uri': '', 'pos': 'm', 'ne': '', 'item': '一家', 'basic_words': ['一', '家'], 'byte_length': 4, 'formal': ''}, {'loc_details': [], 'byte_offset': 10, 'uri': '', 'pos': 'n', 'ne': '', 'item': '高科技', 'basic_words': ['高', '科技'], 'byte_length': 6, 'formal': ''}, {'loc_details': [], 'byte_offset': 16, 'uri': '', 'pos': 'n', 'ne': '', 'item': '公司', 'basic_words': ['公司'], 'byte_length': 4, 'formal': ''}]}\n词法分析（定制版）\ntext = \"百度是一家高科技公司\" \"\"\" 调用词法分析（定制版）\"\"\" client.lexerCustom(text)\n{'log_id': 1030687273146384758, 'items': [{'loc_details': [], 'byte_offset': 0, 'uri': '', 'ne': 'ORG', 'basic_words': ['百度'], 'item': '百度', 'pos': '', 'byte_length': 4, 'formal': ''}, {'loc_details': [], 'byte_offset': 4, 'uri': '', 'ne': '', 'basic_words': ['是'], 'item': '是', 'pos': 'v', 'byte_length': 2, 'formal': ''}, {'loc_details': [], 'byte_offset': 6, 'uri': '', 'ne': '', 'basic_words': ['一', '家'], 'item': '一家', 'pos': 'm', 'byte_length': 4, 'formal': ''}, {'loc_details': [], 'byte_offset': 10, 'uri': '', 'ne': '', 'basic_words': ['高', '科技'], 'item': '高科技', 'pos': 'n', 'byte_length': 6, 'formal': ''}, {'loc_details': [], 'byte_offset': 16, 'uri': '', 'ne': '', 'basic_words': ['公司'], 'item': '公司', 'pos': 'n', 'byte_length': 4, 'formal': ''}], 'text': '百度是一家高科技公司'}\n依存句法分析\n依存句法分析接口可自动分析文本中的依存句法结构信息，哦拥句子中词与词之间的依存关系来表示词语的句法结构信息（如“主谓”、“动宾”、“定中”等结构关系），并用树状结构来表示整句的结构（如“主谓宾”、“定状补”等）。\ntext = \"今天天气怎么样\" \"\"\" 调用依存句法分析 \"\"\" client.depParser(text) \"\"\" 如果有可选参数 \"\"\" options = {} options[\"mode\"] = 1 \"\"\" 带参数调用依存句法分析 \"\"\" client.depParser(text, options)\n{'log_id': 6738947376011839670, 'text': '今天天气怎么样', 'items': [{'postag': 't', 'head': 2, 'word': '今天', 'id': 1, 'deprel': 'ATT'}, {'postag': 'n', 'head': 3, 'word': '天气', 'id': 2, 'deprel': 'SBV'}, {'postag': 'r', 'head': 0, 'word': '怎么样', 'id': 3, 'deprel': 'HED'}]}\n词向量表示\n词向量表示接口提供中文词向量的查询功能。\nword = \"张飞\" \"\"\" 调用词向量表示 \"\"\" client.wordEmbedding(word)\n{'log_id': 1696656248514338902, 'word': '张飞', 'vec': [-0.290384, -0.276273, 0.302719, 0.7209, 0.108958, 0.553115, -0.0877021, 0.359806, 0.0880146, -0.189588, 0.244222, -0.0651301, 0.0638421, 0.533272, -0.00821664, 0.0375696, -0.327892, -0.46532, 0.865607, 0.623493, -0.178252, -0.0400714, 0.25975, 0.11109, 0.0953429, 0.101911, -0.535927, -0.0933478, 0.601825, -0.321298, 0.631975, 0.0875886, 0.870735, -0.269735, -0.585102, 0.319081, 0.184684, -0.720537, -0.383718, -0.0765072, 0.31901, 0.270633, 0.795086, -0.203823, -0.125412, 0.45416, -0.172919, 0.295541, -0.216173, -0.430564, 0.0180166, 0.138979, -0.277238, 0.741072, 0.190484, -0.030923, -0.0943274, 0.591492, -0.418138, -0.523783, -0.227849, 0.366404, -0.443689, -0.125983, 0.0810465, -0.40937, -0.1809, -0.391663, 0.184682, 0.176599, 0.296323, 0.263794, 0.148703, 0.121896, 0.267335, -0.20897, -0.000618858, -0.258487, 0.284275, 0.115589, -0.28355, 0.150706, -0.220889, -0.591039, 0.0290777, -0.201643, 0.0797944, 0.488941, 0.831331, -0.379756, -0.139497, 0.2703, 0.504657, -0.440968, -0.1447, -0.110457, -0.0163559, 0.767792, 0.491371, -0.549788, 0.205589, 0.362547, 0.445447, 0.114256, -0.390303, 0.355757, -0.35865, 0.309228, -0.0702368, 0.0218542, -0.20673, 0.18002, 0.0739457, 0.230891, 0.014336, 0.18294, 0.660368, 0.771709, 0.210481, -0.366585, -0.487737, -0.392698, 0.165913, 0.0634584, 0.327222, 0.170312, 0.16333, -0.0126046, 0.139614, 0.41918, -0.151494, 0.317118, -0.391317, -0.673394, -0.430471, 0.0830508, -0.270076, 0.336409, -0.218263, 0.417467, 0.595822, -0.114509, 0.323514, 0.405187, -0.144482, -0.179517, 0.185674, -0.161061, 0.0338107, -0.290429, -0.187511, 0.131024, 0.0655593, -0.0429835, 0.249348, 0.470223, 0.439866, 0.191249, -0.551478, -0.0530808, 0.220113, 0.21264, 0.4053, 0.000986318, 0.431895, -0.266691, 0.387755, -0.176948, 0.790972, -0.186954, 0.311339, -0.847612, 0.0591855, 0.217022, -0.40963, 0.0388994, 0.258638, -0.0700524, -0.517052, 0.0738539, -0.0278234, -0.0207165, -0.64623, -0.397078, -0.512611, 0.240432, 0.631851, -0.266089, 0.23193, -0.335795, 0.48978, 0.101472, 0.112899, 0.0119656, 0.205143, 0.59687, -0.139228, 0.2366, -0.0448019, -0.463323, 0.136911, 0.245667, -0.531107, -0.203959, 0.437006, 0.0385832, -0.475222, 0.152122, -0.183256, 0.147781, 0.976636, -0.268798, 0.0467436, 0.398612, 0.726595, 0.0641848, 0.442981, 0.392992, 0.277279, 0.191023, 0.540712, 0.041807, 0.521223, 0.494714, -0.114315, -0.623037, 0.503307, 0.16223, -0.0109138, -0.0030869, -0.0127418, 0.0324629, 0.257331, -0.724175, 0.071035, 0.293041, -0.142676, 0.216268, 0.217721, 0.150594, 0.524261, 0.136377, -0.26703, 0.143736, 0.377088, 0.0852308, -0.248864, -0.2864, 0.336949, 0.0106289, 0.142447, 0.0830073, 0.00827009, 0.170654, -0.0537858, 0.66666, -0.167388, -0.00478372, 0.370992, -0.420722, -0.0163072, -0.224316, 0.900274, -0.0618271, 0.0933983, -0.138376, 0.0352047, 0.133874, -0.274968, -0.1037, 0.056145, 0.283046, -0.222181, 0.0843009, 0.201509, 0.0759472, 0.430465, 0.279714, -0.0762712, 0.0291045, 0.0666021, 0.389999, -0.0268815, 0.35655, 0.167335, 0.555981, 0.277015, 0.370779, -0.249201, -0.153099, 0.15063, 0.59068, 0.144961, -0.36857, 0.38433, -0.627967, 0.460143, 0.207135, -0.270095, -0.175896, 0.132773, 0.260412, -0.0316362, -0.511945, -0.014644, -0.338383, 0.513172, 0.273772, -0.245957, -0.484812, 0.479638, -0.781593, -0.692486, 0.269043, 0.48944, 0.151724, -0.109521, 0.0716606, 0.454819, -0.641453, -0.28264, -0.0844294, 0.0127063, -0.0473483, -0.0599927, 0.0715608, -0.562256, 0.215818, -0.207625, -0.0960898, 0.0344254, -0.0852497, -0.119984, 0.296039, -0.595229, 0.253829, -0.111723, 0.411277, 0.101737, -0.0322796, 0.345638, 0.0965107, 0.083087, 0.291633, -0.091778, -0.0279783, -0.108174, -0.300271, -0.541914, 0.197143, 0.631338, 0.479441, 0.0369768, 0.451288, -0.127012, -0.639879, 0.0512995, 0.273387, -0.418342, -0.45064, -0.1239, -0.595654, 0.31378, -0.35008, -0.0134738, 0.476063, 0.0309964, -0.0264222, -0.4704, 0.201462, 0.967353, -0.0587739, -0.221851, -0.221493, -0.319194, 0.321394, 0.176416, 0.0173751, -0.0174415, 0.339173, -0.0516278, -0.255842, -0.283161, -0.017094, -0.138473, 0.271638, 0.496162, 0.519359, -0.00602108, 0.459303, 0.295921, 0.27062, 0.753482, 0.0583323, 0.181312, -0.106313, 0.646242, -0.00311025, -0.163957, 0.182659, -0.0996339, 0.272461, 0.301206, 0.35085, 0.37463, -0.155242, 0.281236, -0.294234, 0.00533482, -0.00310824, 0.0731524, -0.394956, 0.452704, 0.000153456, -0.0800992, -0.0785606, -0.439399, -0.575366, -0.216206, -0.212303, -0.624662, 0.0487097, -0.15867, 0.278319, -0.21006, 0.786678, 0.23844, 0.189342, 0.108299, -0.511393, 0.405482, -0.161949, 0.212671, -0.379168, -0.0637337, 0.13583, 0.0522022, 0.072762, -0.11513, -0.647886, 0.112957, 0.147099, -0.156163, -0.127035, 0.145647, 0.182698, 0.482085, -0.0702394, -0.0172681, -0.24563, -0.0392392, -0.491031, -0.19934, 0.132408, 0.285179, 0.40498, 0.134263, 0.262012, 0.142867, -0.147229, -0.268257, 0.1726, 0.476211, -0.836967, 0.568796, 0.077607, -0.510508, 0.0675741, -0.681589, 0.100888, -0.326709, 0.266345, -0.397411, -0.644215, -0.13274, -0.354817, -0.558334, -0.114178, -0.0940336, 0.235152, -0.554642, 0.382976, -0.274543, -0.105513, -0.409024, -0.0281389, -0.350335, -0.773656, 0.602614, 0.0406916, -0.566817, 0.100671, 0.0793555, 0.176259, 0.218086, 0.654524, -0.109966, 0.157835, -0.214399, 0.166806, 0.297687, -0.526347, 0.330715, -0.223834, 0.354683, 0.164879, -0.060529, 0.208646, -0.347635, -0.386788, -0.434064, -0.448538, 0.106584, -0.137211, -0.821776, 0.448596, 0.55277, -0.486275, 0.0597583, 0.108438, 0.0167387, -0.205475, -0.367478, 0.0528088, 0.191489, 0.308181, 0.124091, 0.0241138, 0.332369, -0.418433, 0.609042, -0.564987, -0.0275926, -0.190715, 0.114899, 0.0137452, 0.00163973, 0.0747787, 0.219737, 0.0336625, 0.0256406, -0.14083, -0.0510848, 0.280421, -0.0751052, -0.195839, 0.217633, -0.110681, -0.692188, -0.516287, 0.0406127, 0.514706, 0.461349, 0.31112, -0.505281, -0.209302, -0.478191, -0.159178, 0.262902, 0.215158, -0.0384547, -0.0301001, -0.68696, 0.333097, 0.387189, -0.397549, -0.389793, -0.326927, -0.426165, -0.249444, -0.287807, -0.358692, 0.344935, -0.22274, -0.12828, -0.0673532, -0.0972766, -0.227617, -0.248091, -0.0705791, 0.63178, -0.759731, -0.368149, 0.578806, 0.280523, -0.0312885, -0.516321, -0.308148, -0.463663, -1.11399, 0.299133, 0.324969, -0.0922515, -0.223782, 0.0757393, 0.0956187, 0.307651, 0.274788, -0.495276, 0.305883, 0.0228269, 0.437532, -0.260021, -0.36529, -0.122708, -0.175827, 0.146148, 0.143242, -0.142164, -0.0918094, -0.415535, -0.0301366, -0.295545, -0.618801, 0.175826, -0.756559, -0.128965, 0.0491931, 0.733814, -0.0347257, -0.460981, -0.540235, 0.138612, -0.353038, -0.0671316, 0.0149887, -0.503586, 0.0874566, 0.441919, 0.0776407, -0.272449, -0.0997288, -0.44766, -0.216144, -0.00963199, 0.0527866, -0.0218697, 0.180018, 0.164696, 0.724876, 0.136289, 0.225619, -0.161481, 0.165889, 0.857903, -0.15784, 0.186857, -0.662843, -0.558884, -0.0192077, 0.00818205, -0.0243429, -0.217057, -0.455544, 0.00163086, -0.466992, 0.113344, -0.174208, 0.251834, -0.0775733, 0.102453, 0.258227, -0.145805, 0.00610516, -0.173767, 0.129026, -0.132582, -0.148301, -0.458603, 0.367434, -0.382593, 0.116882, -0.0928457, 0.276499, 0.180621, 0.351536, 0.353009, -0.31789, -0.0245226, -0.189822, -0.705618, -0.0623819, -0.68237, 0.027945, 0.0396841, -0.081132, 0.414828, 0.251657, -0.193545, -0.0149343, 0.0925272, -0.12489, -0.458534, 0.55974, 0.277349, 0.113657, 0.574713, -0.198563, 0.905217, 0.101096, 0.0367823, -0.120045, 0.278173, -0.191525, -0.0414615, -0.105125, -0.78052, -0.448668, 0.30789, 0.497319, -0.398035, -0.55494, -0.272399, -0.102899, -0.281833, -0.262621, 0.138731, -0.444618, 0.497306, -0.275449, -0.0123345, -0.120426, 0.491484, -0.402516, -0.288962, 0.387392, -0.144125, 0.838843, -0.236083, 0.227957, 0.418015, 0.510442, 0.0841282, -0.544343, -0.0525509, -0.0398014, 0.381329, 0.281488, -0.403923, -0.210186, -0.53414, 0.0852807, -0.345891, -0.294183, 1.17415, -0.023307, -0.828112, 0.0523113, -0.0824572, 0.317031, -0.543952, -0.699134, -0.278506, -0.576854, 0.434733, -0.267847, -0.570456, -0.017377, 0.645807, -0.917205, 0.441665, -0.393248, 0.0631595, -0.386241, 0.0413631, 0.0191933, -0.474338, -0.113288, 0.400757, -0.0247571, -0.348845, -0.0123555, 0.25809, 0.427283, 0.245173, -0.294317, 0.159206, 0.118759, 0.273828, 0.643573, 0.0927131, -0.265129, 0.233232, -0.138332, -0.136015, -0.673727, 0.684253, -0.0585586, -0.327816, -0.716404, -0.58116, 0.0275417, -0.0388521, 0.0237589, -0.277684, 0.0602299, 0.209622, 0.0348703, 0.327143, 0.24981, -0.251077, -0.455329, 0.396863, -0.0570048, -0.265072, -0.0683558, 0.0132361, 0.273579, -0.366049, 0.615134, -0.103124, 0.481334, -0.746339, -0.0640788, -0.484396, -0.00114065, 0.366753, 0.0240541, 0.439156, 0.159546, -0.0506753, 0.0468946, 0.43076, 0.602602, 0.0107401, 1.19797, 0.44314, -0.698443, -0.336827, 0.0258312, 0.172399, 0.305746, -0.150144, 0.0203008, 0.326867, -0.644517, 0.0156665, 0.13351, -0.23441, -0.293748, -0.0695886, -0.477291, 0.281291, -0.755484, 0.74025, -0.552702, 0.381103, 0.164566, -0.15145, -0.728736, 0.448275, 0.0725737, 0.116212, 0.210402, 0.691626, 0.0265872, -0.448584, 0.244172, -0.245309, 0.139035, 0.0288716, -0.364476, -0.0426868, -0.21928, -0.742586, -0.0932949, -0.193005, 0.0303013, -0.76493, 0.0455655, -0.608174, 0.255099, 0.0151615, 0.0139608, 0.0158675, -0.3893, 0.373225, 0.250462, 0.0276716, -0.0752877, -0.0127418, -0.435184, -0.0627005, -0.400453, -0.147969, 0.235518, 0.181853, -0.339577, 0.553451, 0.00837407, -0.248918, -0.136399, -0.354747, -0.350052, 0.220699, -0.183795, 0.784734, 0.395384, -0.315588, 0.0276707, 0.0840118, 0.254402, 0.0226935, -0.483695, -0.075312, 0.402732, -0.0151023, 0.166692, 0.65539, 0.467999, 0.192916, -0.429285, -0.349553, 0.626268, 0.153931, 0.0643198, 0.292859, 0.156136, -0.064216, 0.0490229, 0.147063, 0.151404, -0.701247, -0.0486219, 0.0359798, -0.307433, -0.254073, -0.0960998, 0.386864, -0.100606, -0.0278402, 0.27646, -0.373706, 0.244237, 0.445031, -0.0736471, 0.681565, -0.361913, 0.107957, -0.0310045, -0.0797901, -0.0512583, -0.560119, 0.0451696, -0.112058, 0.010503, 0.456464, 0.180504, 0.187385, -0.492449, 0.0517042, -0.269497, -0.0741519, -0.134895, -0.102614, 0.0668148, -0.498746, 0.386095, -0.131642, -0.208304, -0.0341324, -0.151889, 0.341949, 0.0420371, -0.116241, 0.440811, -0.108852, 0.134327, 0.0777457, 0.488344, 0.0472591, 0.697291, -0.580174, 0.101828, 0.131381, -0.192425, -0.317998, 0.122801, 0.0694366, 0.21801, -0.0429734, -0.136425, 0.437184, -0.11753, 0.344893, 0.24043, 0.0306901, -0.422333, -0.146097, 0.520181, 0.0972754, -0.186103, -0.0766742, -0.745162, 0.364611, 0.186148, -0.250859, 0.243429, -0.251991, -0.424686, ...]}\nDNN 语言模型\n中文 DNN 语言模型接口用于输出切词结果并给出每个词在句子中的概率值，判断一句话是否符合语言表达习惯。\ntext = \"床前明月光\" \"\"\" 调用 DNN 语言模型 \"\"\" client.dnnlm(text)\n{'log_id': 8461893498410162902, 'text': '床前明月光', 'items': [{'word': '床', 'prob': 3.85273e-05}, {'word': '前', 'prob': 0.0289018}, {'word': '明月', 'prob': 0.0284406}, {'word': '光', 'prob': 0.808029}], 'ppl': 79.0651}\n词意相似度\n输入两个词，得到两个词的相似度结果。\nword1 = \"北京\" word2 = \"上海\" \"\"\" 调用词义相似度 \"\"\" client.wordSimEmbedding(word1, word2) \"\"\" 如果有可选参数 \"\"\" options = {} options[\"mode\"] = 0 \"\"\" 带参数调用词义相似度 \"\"\" client.wordSimEmbedding(word1, word2, options)\n{'log_id': 1841062063069490934, 'error_code': 282004, 'error_msg': 'invalid parameter(s)'}\n短文本相似度\ntext1 = \"浙富股份\" text2 = \"万事通自考网\" \"\"\" 调用短文本相似度 \"\"\" client.simnet(text1, text2) \"\"\" 如果有可选参数 \"\"\" options = {} options[\"model\"] = \"CNN\" \"\"\" 带参数调用短文本相似度 \"\"\" client.simnet(text1, text2, options)\n{'log_id': 8759613961966585046, 'texts': {'text_2': '万事通自考网', 'text_1': '浙富股份'}, 'score': 0.0549339}\n评论观点抽取\n评论观点抽取接口用来提取一条评论句子的关注点和评论观点，并输出评论观点标签以及评论观点极性。\ntext = \"三星电脑电池不给力\" \"\"\" 调用评论观点抽取 \"\"\" client.commentTag(text) \"\"\" 如果有可选参数 \"\"\" options = {} options[\"type\"] = 13 \"\"\" 带参数调用评论观点抽取 \"\"\" client.commentTag(text, options)\n{'log_id': 8426923826378164630, 'items': [{'sentiment': 0, 'abstract': '三星电脑\u003cspan\u003e电池不给力\u003c/span\u003e', 'prop': '电池', 'begin_pos': 8, 'end_pos': 18, 'adj': '不给力'}]}\n情感倾向分析\n对包含主观观点信息的文本进行情感极性类别（积极、消极、中性）的判断，并给出相应的置信度。\ntext = \"苹果是一家伟大公司\" \"\"\" 调用情感倾向分析 \"\"\" client.sentimentClassify(text)\n{'log_id': 7415487462125078582, 'text': '苹果是一家伟大公司', 'items': [{'positive_prob': 0.691839, 'confidence': 0.315198, 'negative_prob': 0.308161, 'sentiment': 2}]}\n文章标签\n文章标签服务能够针对网络各类媒体文章进行快速的内容理解，根据输入含有标题的文章，输出多个内容标签以及对应的置信度，用于个性化推荐、相似文章聚合、文本内容分析等场景。\ntitle = \"iphone手机出现“白苹果”原因及解决办法，用苹果手机的可以看下\" content = \"如果下面的方法还是没有解决你的问题建议来我们门店看下成都市锦江区红星路三段99号银石广场24层01室。\" \"\"\" 调用文章标签 \"\"\" client.keyword(title, content)\n{'log_id': 4313909132996888022, 'items': [{'score': 0.99775, 'tag': 'iphone'}, {'score': 0.862602, 'tag': '手机'}, {'score': 0.845657, 'tag': '苹果'}, {'score': 0.837886, 'tag': '苹果公司'}, {'score': 0.811601, 'tag': '白苹果'}, {'score': 0.797911, 'tag': '数码'}]}\n文章分类\n对文章按照内容类型进行自动分类，首批支持娱乐、体育、科技等26个主流内容类型，文本内容分析等应用提供基础技术支持。\ntitle = \"欧洲冠军杯足球赛\" content = \"欧洲冠军联赛是欧洲足球协会联盟主办的年度足球比赛，代表欧洲俱乐部足球最高荣誉和水平，被认为是全世界最高素质、最具影响力以及最高水平的俱乐部赛事，亦是世界上奖金最高的足球赛事和体育赛事之一。\" \"\"\" 调用文章分类 \"\"\" client.topic(title, content)\n{'log_id': 2207187729196380118, 'item': {'lv2_tag_list': [{'score': 0.915631, 'tag': '足球'}, {'score': 0.803507, 'tag': '国际足球'}, {'score': 0.77813, 'tag': '英超'}], 'lv1_tag_list': [{'score': 0.830915, 'tag': '体育'}]}}\n文本纠错\n识别输入文本中有错误的片段，提示错误并给出正确的文本结果。支持短文本、长文本、语音等内容的错误识别，纠错是搜索引擎、语音识别、内容审查等功能更好运行的基础模块之一。\ntext = \"百度是一家仁工智能公司\" \"\"\" 调用文本纠错 \"\"\" client.ecnet(text)\n{'log_id': 4819268271360271574, 'item': {'vec_fragment': [{'ori_frag': '仁工', 'begin_pos': 10, 'correct_frag': '人工', 'end_pos': 14}], 'score': 0.529867, 'correct_query': '百度是一家人工智能公司'}, 'text': '百度是一家仁工智能公司'}\n对话情绪识别接口\n针对用户日常沟通文本背后所蕴含情绪的一种直观检测，可自动识别出当前会话者所表现出的情绪类别及其置信度，可以帮助企业更全面地把握产品服务质量、监控客户服务质量。\ntext = \"本来今天高高兴兴\" \"\"\" 调用对话情绪识别接口 \"\"\" client.emotion(text) \"\"\" 如果有可选参数 \"\"\" options = {} options[\"scene\"] = \"talk\" \"\"\" 带参数调用对话情绪识别接口 \"\"\" client.emotion(text, options)\n{'log_id': 901856600521512694, 'text': '本来今天高高兴兴', 'items': [{'subitems': [{'prob': 0.501008, 'label': 'happy'}], 'replies': ['你的笑声真欢乐'], 'prob': 0.501008, 'label': 'optimistic'}, {'subitems': [], 'replies': [], 'prob': 0.49872, 'label': 'neutral'}, {'subitems': [], 'replies': [], 'prob': 0.000272128, 'label': 'pessimistic'}]}\n新闻摘要接口\n自动抽取新闻文本中的关键信息，进而生成指定长度的新闻摘要。\ncontent = \"麻省理工学院的研究团队为无人机在仓库中使用RFID技术进行库存查找等工作，创造了一种...\" maxSummaryLen = 300 \"\"\" 调用新闻摘要接口 \"\"\" client.newsSummary(content, maxSummaryLen); \"\"\" 如果有可选参数 \"\"\" options = {} options[\"title\"] = \"标题\" \"\"\" 带参数调用新闻摘要接口 \"\"\" client.newsSummary(content, maxSummaryLen, options)\n{'error_code': 6, 'error_msg': 'No permission to access data'}","date":"2018年12月22日 16:53:14"}
{"_id":{"$oid":"5d36a8a46734bd8e681d5dd8"},"title":"自然语言处理与信管这一专业的关系","author":"fw1784892153","content":"自然语言处理与信管这一专业的关系\n我们首先说说什么是自然语言处理。现在世界上所有的语种语言，都属于自然语言。自然语言处理并不是通过人来人工处理，而是通过计算机来进行处理。自然语言输入至计算机后，计算机用定义好了的算法进行处理得到人所期待的结果。\n信息管理和信息系统简称信管，这一专业要求掌握管理信息系统的分析方法、设计方法和实现技术，具有信息收集、组织、分析研究、传播与综合利用的基本能力 ，掌握文献检索、资料查询、收集的基本方法。这些都是与计算机结合，通过计算机作为工具，使得信息管理更加有效和实用。这些在生活中有很多的应用。例如铁路订票系统，就是对车票这种信息的查询和管理系统。还有电子病历，也是对信息收集后的处理与反馈，大大节省了时间，提高了效率。\n自然语言处理在生活中有着广泛的应用。其中最常见的就是在线翻译，很多人出国随身带着翻译笔，即使语言不通，通过翻译笔也可以正常交流。很多app都有评论功能，通过感情分析可以看出用户的态度，负责人以此可以达到自己想要的效果。同时，在一些选举预测、股票预测等领域情感分析也逐渐体现着越来越重要的作用。随着大数据和人工智能的兴起，自然语言处理也将会在各个方面都有所作为。信息管理和信息系统在收集与分析数据的知识的应用，与自然语言处理可以说有一定的关系的。而自然语言处理能为信管服务，它可以缩短搜集的信息范围，并对其进行一定的分析，使我们得到我们想要的结果。未来的自然语言处理说不定会使得这些过程变得更加的简洁迅速，甚至是使其实现自动化的处理。","date":"2019年06月20日 19:53:56"}
{"_id":{"$oid":"5d36a8a46734bd8e681d5dda"},"title":"自然语言处理(NLP)相关资料汇总","author":"LeeTioN","content":"杂谈\n自然语言处理怎么最快入门？ - 刘知远的回答 - 知乎\nhttps://www.zhihu.com/question/19895141/answer/24710071\n刘老师主要是从ACL等著名NLP相关会议的角度来介绍如何跟进NLP领域的研究\n国内大陆部分NLP团队\nNLP（自然语言处理）界有哪些神级人物？ - jiangfeng的回答 - 知乎\nhttps://www.zhihu.com/question/32318281/answer/55588123\n有关NLP的比赛 - 砍手豪的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/33901181\n系列教程\n李航的《统计学习方法》第4、10、11章\nCoursera的一门课——Natural Language Processing(已经下架，有人整理)\nhttp://www.52nlp.cn/tag/nlp%E5%85%A5%E9%97%A8\nCS224 深度学习自然语言处理教程(B站上爱可可老师已分享带英文字幕的视频)\nhttps://www.bilibili.com/video/av13383754from=search\u0026seid=4721962932083536913\n工具(包)\n著名工具：NLTK、结巴、Word2Vec、Gensim、SpaCy\n目前常用的自然语言处理开源项目/开发包有哪些？ - 刘知远的回答 - 知乎\nhttps://www.zhihu.com/question/19929473/answer/90201148\n待更新","date":"2018年05月12日 16:07:30"}
{"_id":{"$oid":"5d36a8a56734bd8e681d5ddd"},"title":"自然语言处理中主题模型的发展","author":"hello_pig1995","content":"自然语言处理中主题模型的发展\n强烈建议直接看论文，看一些博客对于入门并没有什么太大帮助。\n[1]徐戈,王厚峰. 自然语言处理中主题模型的发展[J]. 计算机学报,2011,08:1423-1436.\n摘要：\n主题——词项的概率分布\n主题模型——文档从词项空间转换到主题空间，降维表达\n主要内容：\n1.对LSI PLSI LDA等主题模型进行介绍比较\n2.LDA派生模型介绍\n3.对EM算法生成主题的词项概率分布和文档的主题概率分布进行分析\n1.引言\n主题可以看作是词项的概率分布，一篇文章使用bag of words进行表示，长度较长，映射到主题空间之后，由于通常主题数K远远小于词项的数目，因此可以通过主题模型进行降维。\n隐性语义索引LSI (latent semantic indexing)不是一个概率模型\nDeerwester, Scott, et al. \"Indexing by latent semantic analysis.\" Journal of the American society for information science 41.6 (1990): 391.\n概率隐性语义索引pLSI真正意义上的主题模型\nHofmann, Thomas. \"Probabilistic latent semantic indexing.\" Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 1999.\nLDA(latent Dirichlet Allocation)Blei\nBlei, David M., Andrew Y. Ng, and Michael I. Jordan. \"Latent dirichlet allocation.\" Journal of machine Learning research 3.Jan (2003): 993-1022.\nLSI - PLSI - LDA - 各种LDA\n2.主题模型的主要内容\n五大组成部分：输入、模型假设、表示、参数估计、新样本推测\n2.1主题模型的输入\n主题模型的输入是文档集合，由于交换性的假设，所以等价于term-document矩阵。\nterm \\ document\ndocument 1\ndocument 2\ndocument 3\nterm 1\n1\n0\n2\nterm 2\n0\n3\n1\n另一个输入是主题数目K,通常K是经验决定，最简单的方法是使用不同的K重复实验。\n评价指标：困惑度、语料似然值、分类正确率等估计K\nBlei, David M., Andrew Y. Ng, and Michael I. Jordan. \"Latent dirichlet allocation.\" Journal of machine Learning research 3.Jan (2003): 993-1022. Griffiths, Thomas L., and Mark Steyvers. \"Finding scientific topics.\" Proceedings of the National academy of Sciences 101.suppl 1 (2004): 5228-5235. Blei, David M. \"Probabilistic topic models.\" Communications of the ACM 55.4 (2012): 77-84. CAO, Juan, et al. \"A method of adaptively selecting best LDA model based on density.\" Chinese Journal of Computer 31 (2008): 1780-1787.\n非参数贝叶斯估计K\nTeh, Yee Whye, et al. \"Hierarchical dirichlet processes.\" Journal of the american statistical association (2012). Shi, Jin, et al. \"Text segmentation based on model LDA.\" Chinese Journal of Computers 31.10 (2008): 1865-1873.\n2.2主题模型中的基本假设\nbag of words假设，即文档内词的顺序与模型结果无关。\n但是在LDA的派生模型中，有的交换性会被打破\n2.3主题模型的表示\n分别是图模型和生成模型，注意其中有两个超参数\nα\n\\alpha 和\nβ\n\\beta。\n2.4参数估计过程\n参数估计的结果是训练的最终结果。\n首先要选择优化的目标函数，通常就是整个语料的概率值。\n以LDA模型为例，根据图模型可以比较容易得到概率值的大小。（参看我的EM算法的文章）\n其实计算语料的概率就是计算整体的可能期望\n2.5新样本的推断\n其实新样本的推断就是将其映射到低维度的主题空间即可。可以用于信息检索中。\n3.期望最大化算法和参数估计\n（参看我的EM算法文章）\n期望最大化算法，对于隐变量通过概率模型寻找极大似然估计的一般方法。能够不断迭代，从而修改现有模型的参数。使用现有模型推断隐变量的后验概率分布，然后对于参数重新估计。\n不能保证全剧最优解，不过可以通过多次试验取得最好的结果。\n概率模型包括：1.隐变量集合Z 2.观测集合X 3.参数集合\nθ\n\\theta\n目标：得到P(X|\nθ\n\\theta)最大化时候的\nθ\n\\theta\nEM算法过程：\n初始化\nθ\n\\theta\nE步骤：使用当前\nθ\n\\theta对于P(Z|X,\nθ\n\\theta)进行估计\nM步骤：利用前一步的结果，最大化期望\n4.隐性语义索引\n隐性语义索引主要包含奇异值分解（SVD）和主成分分析（PCA）。\n4.1主成分分析\n4.2隐性语义索引\n隐性语义索引是通过奇异值分解构造新的隐性语义空间，即SVD分解。\n对于过于大的矩阵来说，可以通过EM算法来求得SVD分解的结果，其实SVD的U和V矩阵都可以看作是对于两个矩阵做了主成分分析，这两个矩阵的特征值和特征向量都可以通过EM算法计算出来。\n对于EM计算PCA,参见PRML。\n缺项矩阵：\nRoweis, Sam. \"EM Algorithms for PCA and SPCA.\"\n5.概率隐性语义索引\n概率隐性语义索引也是从词项空间到主题空间的变换。\n但是pLSI是一个概率生成模型。而且选择了不同的优化目标函数。\n就是两张二维参数表，分别是p(w|z)和p(z|d)，可以理解为和LSI中的类似。U对应p(w|z) V对应p(d|z) ，而中间矩阵对应着z的概率分布。\n对应于EM算法，可以对应于本模型，即w，d为观测值，z是隐变量，p(w|z) p(z|d)为待估计的参数。\npLSI模型和LSI的效果比较：\nHofmann, Thomas. \"Probabilistic latent semantic indexing.\" Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 1999. Hofmann, Thomas. \"Unsupervised learning by probabilistic latent semantic analysis.\" Machine learning 42.1-2 (2001): 177-196.\n和pLSI等价的NMF（非负矩阵分解）\nLee, Daniel D., and H. Sebastian Seung. \"Learning the parts of objects by non-negative matrix factorization.\" Nature 401.6755 (1999): 788-791. Lee, Daniel D., and H. Sebastian Seung. \"Algorithms for Non-negative Matrix Factorization.\"\n6.。。。从这里开始看不懂了，待我补完prml的概率部分再来看看","date":"2016年07月09日 19:41:24"}
{"_id":{"$oid":"5d36a8a56734bd8e681d5de0"},"title":"用Python进行自然语言处理 读书笔记 第一章","author":"Radiumm","content":"用Python进行自然语言处理（第一章）\n搜索文本\ntext1.concordance(\"monstrous\")#搜索文章中的词语 text3.concordance(\"lived\") text1.similar(\"monstrous\")#近义词 text2.common_contexts([\"monstrous\",\"very\"])#两个词共同的上下文 text4.dispersion_plot(['citizens','democracy','freedom','duties','America'])#该函数需要依赖numpy和matplotlib库\n计数词汇\nset(text3)#text3中所有标点，单词的集合，去重 sorted(set(text3))#text3中的所有标点、单词排序之后，去重 len(set(text3))#text3的独一无二的标点、单词类型个数，称为唯一项目类型 print(len(text3) / len(set(text3)))#每个字平均被使用的次数 print(text3.count(\"smote\"))#统计一个词语在一个文本中出现的次数 print(100 * text4.count('a') / len(text4))#'统计一个词语占全部词语的百分比是多少\n函数\ndef关键字定义,lexical_diversity为函数名，text为参数 def lexical_diversity(text):    return len(text) / len(set(text)) print(lexical_diversity(text3)); def percentage(count, total):    return  100 * count / total\n将文本当作词链表\na = ['Call','me','Ishmael','.'] print(a[1])#索引是从0开始的 print(text4[173]);#找到索引处的元素 print(text4.index('awaken'));#找到元素第一次出现的索引 print(text5[16715:16735])#获取链表中任意片段中的元素 sent = ['word1','word2','word3','word4','word5','word6','word7','word8','word9','word10']; print(sent[5:8])#sent[m:n] m:n-1 m represents index print(sent[:3])#from the first to index 3(exclude index3) print(sent[3:])#from index3 to the end sent[0] = 'First'#replace 'word1' to 'First' sent[1:9] = ['First','Last']#replace index 1~index 9 to the designated two words\n变量\n#assignment(赋值):variation = expression ,words ahead ,numbers and '_' is permitted my_sent = ['a','wwe','eee'] noun = my_sent[0:3] print(sorted(noun))#capital is ahead of lowercase\n字符串\n#operation on character string a = 'Monty' print(a[0])#'M' print(a[:4])#'Mont' #connect character string print(''.join(['Monty','Python','asd'])) #split two character string print('Monty Pytho n'.split())\n统计分布\n找出文本中最常见的50个词\nfdist = FreqDist(text1) vocabulary = fdist.keys() voc = list(vocabulary)#必须得转换成list才能用 print(fdist)#print the number of words print(voc[:50])#分片前50个 print(fdist['whale'])#the number of 'whale' fdist.plot(50,cumulative = True)#画出图案，需要安装pyqt\n找出text1中长度超过15个字符的词并排序\nV = set(text1) long_words = [w for w in V if len(w) \u003e 15] print(sorted(long_words))\n找出长度超过7个字符并且出现次数超过7次的词\nfdist5 = FreqDist(text5) print(sorted([w for w in set(text5) if len(w) \u003e 7 and fdist5[w] \u003e 7]))\n搭配：经常出现的词的序列，词对、双连词：找到在一起出现的两个词\nprint(bigrams(['more','is','said','than','done']))#找词对，有问题 text4.collocations()#找到出现频繁的双连词\n输出text1中每个词的长度\nprint([len(w) for w in text1])\n文本中的词的长度的性质统计\nfdist = FreqDist(len(w) for w in text1)#FreqDist计数链表中每个长度出现的次数 print(fdist.keys())#输出的是包含的元素的不同词长 print(fdist.items())#以（a,b)输出a长度的词出现了b次 print(fdist.max())#输出出现次数最多的长度 print(fdist[3])#输出长度为3的词出现的次数 print(fdist.freq(3))#输出长度为3的词出现的次数占总词数的比例 print(fdist.N())#输出样本总数 fdist.plot()#绘制频率分布图 fdist.plot(cumulative=True)#绘制累积频率分布图\n决策\n[w for w in sent7 if len(w) \u003c 4]#输出sent7中长度小于4的元素 sorted([w for w in set(text1) if w.endswith('ableness')])#输出以ableness结尾的单词并排序 s.startswith(t) #测试s是否以t开头 s.endswith(t) #测试s是否以t结尾 t in s #测试s是否包含t s.islower() #测试s中所有字符是否都是小写字母 s.isupper() #测试 s 中所有字符是否都是大写字母 s.isalpha() #测试 s 中所有字符是否都是字母 s.isalnum() #测试 s 中所有字符是否都是字母或数字 s.isdigit() #测试 s 中所有字符是否都是数字 s.istitle() #测试 s 是否首字母大写（s 中所有的词都首字母大写）\n控制\n对每个元素进行操作\n[len(w)for w in text1] [w.upper()for w in text1] #这些表达式形式为[f(w) for ...]或[w.f() for ...]，其中 f是一个函数，用来计算词长或把字母转换为大写 len(set(word.lower()for word in text1))#由于我们不重复计算像This和this这样仅仅大小写不同的词，就已经从词汇表计数中抹去了2,000个！ len(set(word.lower()for word in text1 if word.isalpha()))#通过过滤掉所有非字母元素，从词汇表中消除数字和标点符号\n嵌套代码块\n控制结构：\nif len(word) \u003c 5:#（注意冒号）     print('word length is less than 5')#注意段首空tab ...#该行空出来，再下一行输出 #如果不满足if成立条件，没有输出 for word in ['Call','me','Ishmael','.']:     print(word) ...#空一行，下一行输出 sent1 = ['Call','me','Ishmael','.'] for word in sent1:     if word.endswith('l'):             print(word) ... Call Ishmael #在 if 和for语句所在行末尾——缩进开始之前——有一个冒号 #所有的Python控制结构都以冒号结尾。冒号表示当前语句与后面的缩进块有关联。 for token in sent1: ...     if token.islower(): ...             print(token,'is a lowercase word') ...     elif token.istitle(): ...             print(token,'is a titlecase word') ...     else : ...             print (token,'is punctuation') ... Call is a titlecase word me is a lowercase word Ishmael is a titlecase word . is punctuation tricky = sorted([w for w in set(text2) if 'cei' in w or 'cie' in w]) for word in tricky: ...     print(word) ...\n关于NLP-自动理解自然语言\n词意消歧\n我们要算出特定上下文中的词被赋予的是哪个意思\n自动消除歧义需要使用上下文，利用相邻词汇有相近含义这样一个简单的事实\n指代消解\n一种更深刻的语言理解是解决“谁对谁做了什么”，即检测主语和动词的宾语\na. The thieves stole the paintings. They were subsequently sold .b. The thieves stole the paintings. They were subsequently caught .c. The thieves stole the paintings. They were subsequently found .要回答这个问题涉及到寻找代词they的先行词thieves或者paintings。处理这个问题的计算技术包括指代消解（anaphora resolution）——确定代词或名词短语指的是什么——和语义角色标注（semantic role labeling）——确定名词短语如何与动词相关联（如施事，受事，工具等）。\n自动生成语言\n如果我们能够解决自动语言理解等问题，我们将能够继续那些包含自动生成语言的任务，如自动问答和机器翻译。在自动问答中，一台机器要能够回答用户关于特定文本集的问题：\na.Text : ... The thieves stole the paintings. They were subsequently sold. ...b.Human : Who or what was sold?c.Machine : The paintings. 机器的回答表明，它已经正确的计算出they是指paintings，而不是thieves。在机器翻译中，机器要能够把文本翻译成另一种语言文字，并准确传达原文的意思。正确的翻译实际上取决于对代词的正确理解。所有这些例子中，弄清楚词的含义、动作的主语以及代词的先行词是理解句子含义的步骤，也是我们希望语言理解系统能够做到的事情。\n机器翻译\n长久以来，机器翻译（MT）都是语言理解的圣杯，人们希望能找到从根本上提供高品质的符合语言习惯的任意两种语言之间的翻译。其历史可以追溯到冷战初期，当时自动翻译的许诺带来大量的政府赞助，它也是NLP本身的起源。\n今天，特定语言之间实用的翻译系统已经存在，有些已经集成到搜索引擎中了。但是，这些系统有一些严重的缺点，例如babelize_shell() 该函数在nltk3.0中已经不再可用机器翻译是困难的，因为一个给定的词可能有几种不同的解释（取决于它的意思），也因为必须改变词序才能与目标语言的语法结构保持一致。今天，这些困难遇到新情况，从 闻和政府网站发布的两种或两种以上的语言文档中可以收集到大量的相似文本。给出一个德文和英文双语的文档或者一个双语词典，我们就可以自动配对组成句子，这个过程叫做文本对齐。一旦我们有一百万或更多的句子对，就可以检测出相应的词和短语，并建立一个能用来翻译新文本的模型。\n人机对话系统\n在人工智能的历史，主要的智能测试是一个语言学测试，叫做图灵测试：一个响应用户文本输入的对话系统能否表现的自然到我们无法区分它是人工生成的响应？相比之下，今天的商业对话系统能力是非常有限的，但在较小的给定领域仍然有些作用\nS: How may I help you?U: When is Saving Private Ryan playing? S: For what theater? U: The Paramount theater. S: Saving Private Ryan is not playing at the Paramount theater, but it’s playing at the Madison theater at 3:00, 5:30, 8:00, and 10:30.你不能要求这个系统提供驾驶指示或附近餐馆的细节，除非所需的信息已经被保存并且合适的问题答案对已经被纳入语言处理系统。 请看，这个系统似乎了解用户的目标：用户询问电影上映的时间，系统正确的判断出用户是想要看电影。这一推断看起来如此明显，你可能都没有注意到它，一个自然语言系统需要被赋予这种自然的交互能力。没有它，当问到：“ 你知道拯救大兵瑞恩什么时候上映？ ”时，系统可能只会回答一个冷冷的毫无用处的“ 是的 ”。然而，商业对话系统的开发者使用上下文语境假设和业务逻辑确保在用户以不同方式表达需求或提供信息时对特定应用都能有效处理。因此，如果你输入 When is ...或者 I want to know when ...或者Can you tell me whe n ...时，这些简单的规则总是对应着放映时间，这就足够系统提供有益的服务了。举例：原始的对话系统import nltknltk.chat.chatbots()Which chatbot would you like to talk to? 1: Eliza (psycho-babble) 2: Iesha (teen anime junky) 3: Rude (abusive bot) 4: Suntsu (Chinese sayings) 5: Zen (gems of wisdom)\nEnter a number in the range 1-5: Error: bad chatbot number\nEnter a number in the range 1-5: 1\n文本的含义\n近年来，一个叫做文本含义识别(Recognizing Textual Entailment 简称 RTE)的公开的“共享任务”使语言理解所面临的挑战成为关注焦点。基本情形很简单：假设你想找到证据来支持一个假设：Sandra Goudie 被Max Purnell击败了。而你有一段简短的文字似乎是有关的， 例如：Sandra Goudie 在2002 年国会选举首次当选，通过击败工党候选人 Max Purnell 将现 任绿党下院议员 Jeanette Fitzsimons 推到第三位，以微弱优势赢得了 Coromandel 席位。文本 是否为你接受假说提供了足够的证据呢？在这种特殊情况下，答案是“否”。你可以很容易得 出这样的结论，但使用自动方法做出正确决策是困难的。RTE 挑战为竞赛者开发他们的系统提供数据，但这些数据对“蛮力”机器学习技术（我们将在第 6章讲述这一主题）来说是不 够的。因此，一些语言学分析是至关重要的。在前面的例子中，很重要的一点是让系统知道 Sandra Goudie 是假设中被击败的人，而不是文本中击败别人的人。思考下面的文本-假设对， 这是任务困难性的另一个例证：a. Text: David Golinkin is the editor or author of 18 books, and over 150 responsa, articles, sermons and books b. Hypothesis: Golinkin has written 18 books\n为了确定假说是否得到文本的支持，该系统需要以下背景知识： （一）如果有人是一本书的作者，那么他/她写了这本书; （二）如果有人是一本书的编辑，那么他/她（完全）没有写这本书; （三）如果有人是18 本书的编辑或作者，则无法断定他/她是18 本书的作者\nNLP的局限性\n尽管在很多如 RTE这样的任务中研究取得了进展，但在现实世界的应用中已经部署的语言理解系统仍不能进行常识推理或以一种一般的可靠的方式描绘这个世界的知识。我们在等待这些困难的人工智能问题得到解决的同时，接受一些在推理和知识能力上存在严重限制的自然语言系统是有必要的。因此，从一开始，自然语言处理研究的一个重要目标一直是使用浅显但强大的技术代替无边无际的知识和推理能力，促进构建“语言理解”技术的艰巨任务的不断取得进展。事实上，这是本书的目标之一，我们希望你能掌握这些知识和技能，构建有效的自然语言处理系统，并为构建智能机器这一长期的理想做出贡献。","date":"2018年04月04日 23:13:04"}
{"_id":{"$oid":"5d36a8a66734bd8e681d5de3"},"title":"自然语言处理笔记2-哈工大 关毅","author":"Big_quant","content":"目录\n文章目录\n目录\n前言\n自然语言处理概论（七）\n自然语言处理概论（八）\n数学基础和语言学基础（1）\n数学基础和语言学基础（2）\n数学基础和语言学基础（3）\n数学基础和语言学基础（4）\n数学基础和语言学基础（5）\n前言\n硕士生涯结束，开始专心做一件自己觉得有用的工具，先做工程，后搞理论。\n自然语言处理是一个非常难的问题，同时是人工智能皇冠上的明珠。\n接下来会记录一系列自然语言处理的笔记，来自于哈工大老师关毅\n自然语言处理概论（七）\n人工智能经典实验，图灵实验,想象人和一块机器隔着屏幕讲话，如果，人无法判断对面是机器还是人，那就说明这个机器通过了图灵测试。\n强调一点，人工智能的发展还是要依赖于对人大脑机理的了解，做出真正的人工智能。\n涉及的学科：计算语言学，应用语言学，计算机科学。\n可计算的方法来自于数学的理论基础和人的心理学模型。\n将人理解自然语言的步骤反着来一遍，就可以做出真正的自然语言理解。\n现在想要在理论上做出大的创新，需要的是交叉学科，需要的是共同创新。\n计算语言学侧重于语言处理的基础。自然语言理解侧重于智能化人机接口。\n自然语言处理概论（八）\n汉语的特性：\n大字符集的意音文字，同义多，词态无变化，语法研究不规范。\n汉语语言形式化和量化工作滞后。\n力量较分散，分词评测系统很难。\n基础理论讲解：1概率统计2统计机器学习3人工智能4认知科学理论。\n人工智能理论：1组合优化方法2逻辑方法\n可研究的内容：1词法分析2句法分析3上下文无关分析4语义分析5概念网络6机器翻译。\n数学基础与语言学基础（一）\n从小规模语料库统计出语料信息然后在大规模语料库里面使用。\n个人的感受：\n工程开发经验以后，必须做一个精密的系统，收集分析情况，构造推断模型。\n概率论是我们的研究基础，它研究的是随机现象的规律，词汇的分布也符合幂律。\n数学基础和语言学基础（1）\n1948年，熵出现，\n\nH\n(\nP\n)\n=\n∑\nx\n属\n于\nΩ\n−\np\n(\nx\n)\n∗\nl\no\ng\np\n(\nx\n)\nH(P)=\\sum_{x 属于 \\Omega }-p(x)*log_p(x)\nH(P)=x属于Ω∑ −p(x)∗logp (x)\n不确定性的信息熵最大，完全确定的信息，信息熵最小。\n冯志伟汉语信息量最大。\n条件概率复习：\nP(A|B)，B为真时A发生的概率。\n数学基础和语言学基础（2）\n贝叶斯定理\n\nP\n(\nA\n∣\nB\n)\n=\nP\n(\nA\n,\nB\n)\nP\n(\nB\n)\n=\nP\n(\nB\n∣\nA\n)\n∗\nP\n(\nA\n)\nP\n(\nB\n)\n=\na\nr\ng\nm\na\nx\nA\nP\n(\nB\n∣\nA\n)\n∗\nP\n(\nA\n)\nP(A|B)=\\frac{P(A,B)}{P(B)}=\\frac{P(B|A)*P(A)}{P(B)}=argmax_AP(B|A)*P(A)\nP(A∣B)=P(B)P(A,B) =P(B)P(B∣A)∗P(A) =argmaxA P(B∣A)∗P(A)\n应用，音字替换，贝叶斯定理，将一个大问题分解成两个小的问题的乘积。\n随机变量，数学期望与方差。\n无参数分布以及有参数分布。\n极大似然估计和贝叶斯统计。\n语言学基础：\n1汉语的分类：可以分为实，虚，叹。\n属于黏着语。\n数学基础和语言学基础（3）\n语法分类，句法分析特点。\n汉语句法分析的特殊性。\n一个词可以在句中担任多种成分，切勿形态变化。\n语言知识库，一个关键核心部分，调整知识库。\n现代汉语语法信息词典，语用层提示到语义层。\n数学基础和语言学基础（4）\n贝叶斯公式和一个核心。\noncology是核心。\nhownet搭建了一个意元为基础的框架。\n定义意元很难，派生整个系统。\n数学基础和语言学基础（5）\n搜索系统，人性化提问。\n大规模文本抽取答案。\n语义相似度的计算，基于库或者基于统计的方法。\n机器可读词典。\n二进制文件构造你的词典，保护知识产权。","date":"2018年12月10日 19:13:19"}
{"_id":{"$oid":"5d36a8a66734bd8e681d5de5"},"title":"腾讯自然语言处理实习岗面经","author":"沉香屑_","content":"人生第一次面BAT，记录一下，为明年找工作赞经验，毕竟网上很少自然语言处理相关的面经。\n先来点干货！\n面试流程\n1. 自我介绍\n2. 谈谈项目\n3. 上黑板写代码\n按照上述流程分为三块，如下。\n简单的自我介绍了一下\n我：我做的项目是关于自然语言生成，…\n面试官1：为什么不用生成式的方法来做呢？\n我：我尝试了char-rnn和seq2seq的方法，…\n面试官1：rnn是怎么运行的你能说一下吗？\n我：巴拉巴拉…\n面试官1：什么是char-rnn，你能画一下rnn的结构图吗？\n我：OK，（开始画画画，画完解释了一通）\n面试官2：那如果是seq2seq的结构是什么样的你能画一下吗？\n我：OK，（继续画，此时我给自己挖了一个坑）这是不加attention的seq2seq，加上attention，encoder后的编码向量就不是固定的了（此刻的我觉得自己回答的还蛮好）。\n面试官1：那你能画一下attention的结构吗？是怎么做的？\n我：我有点忘记了，能翻下笔记本吗？（思考了一下，大脑一片空白，想想前两周每天都在研究attention啊，年纪大了记性不好，出来翻了下笔记，这么熟悉的attention啊，那一刻怎么就不记得了！！！）\n面试官1：没事，那来道代码题吧。\n面试官2出题，一个struct含有start,end属性，现在有一个这样结构的数据集，统计这个数据集中具有相同start和end的数据有多少个？【 简化一下就是统计{[0,2),[0,2),[2,4)…}中[0,2)、[2,4)…分别有多少个?】\n解题思路：输入格式为一个存放string的list\ndata = [\"0 2\", \"2 4\", \"3 6\", \"0 2\", \"2 4\", \"2 4\"] ret = dict() for d in data: if d in ret: ret[d] = ret[d] + 1 else: ret[d] = 1 for d in ret: print(d+\":\"+str(ret[d]))\n当时跟面试官应该没沟通彻底，隐约感觉他想要的输入是struct类型。\n然后就结束了。\n基于本次面试，未来需要做的准备：\n1）准备一个完美的自我介绍\n2）刷LeetCode\n3）熟记主流框架公式\n4）项目深入了解","date":"2017年12月22日 19:22:08"}
{"_id":{"$oid":"5d36a8a86734bd8e681d5de8"},"title":"自然语言处理复习汇总(南京大学)","author":"冬虫夏草1993","content":"自然语言处理复习汇总(南京大学)\n标签（空格分隔）： 自然语言处理\n参考书籍:统计自然语言处理–宗成庆\n该文档用markdown编写,github地址为https://github.com/lyfadvance/nlp/blob/master/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%A4%8D%E4%B9%A0%E6%96%87%E6%A1%A3.md\n如果想继续编写，可以fork\n自然语言处理复习汇总南京大学\n统计语言模型\nN-Gram\nNeural language model\nword2vector\n文本分类\n朴素贝叶斯模型\nBernoulli document model伯努利文档模型\nMultinomial document model\n训练句向量\n文本或句子向量化\n词袋模型\nN-Gram Bag-of-Words\nTF-IDF\nTF词频\nIDF逆文档频率\n特征过滤\nDistributional similarity-based representations\n词性标注与隐马尔科夫模型\n隐马尔科夫模型的三个基本问题\n问题1\n问题2\n问题3\n统计语义分析\nPCFG概率上下文无关文法\n问题1\nTreebank\nChomsky Normal Form\n统计机器翻译\n统计语言模型\nN-Gram\nN-1阶马尔可夫链我们称之为N元语言模型\n\nP(wi|wi−1)=P(wi−1wi)P(wi−1)=Count(wi−1wi)∑wCount(wi−1w)\nP(w_i|w_{i-1})=\\dfrac{P(w_{i-1}w_i)}{P(w_{i-1})}=\\dfrac{Count(w_{i-1}w_i)}{\\sum_wCount(w_{i-1}w)}\nCount(wi−1wi)\nCount(w_{i-1}w_i)由于稀疏性，值可能等于0．从而导致整个句子的概率都等于0\n进行平滑处理:\n线性平滑:\nP(wi|wi−1)=P(wi−1wi)P(wi−1)=Count(wi−1wi)+α∑w(Count(wi−1w)+α)\nP(w_i|w_{i-1})=\\dfrac{P(w_{i-1}w_i)}{P(w_{i-1})}=\\dfrac{Count(w_{i-1}w_i)+\\alpha}{\\sum_w(Count(w_{i-1}w)+\\alpha)}\nlaplace 平滑:\nP(wi|wi−1)=P(wi−1wi)P(wi−1)=Count(wi−1wi)+kP(w)(∑wCount(wi−1w))+k\nP(w_i|w_{i-1})=\\dfrac{P(w_{i-1}w_i)}{P(w_{i-1})}=\\dfrac{Count(w_{i-1}w_i)+kP(w)}{(\\sum_wCount(w_{i-1}w))+k}\n简单线性插值平滑:\nNeural language model\nword2vector\n文本分类\n朴素贝叶斯模型\nD为待分类的文档，\nck\nc_k指第k个类别\nargmaxckP(ck|D)=argmaxckP(D|ck)P(ck)P(D)=argmaxckP(D|ck)P(ck)\nargmax_{c_k}P(c_k|D)=argmax_{c_k}\\dfrac{P(D|c_k)P(c_k)}{P(D)}=argmax_{c_k}P(D|c_k)P(c_k)\n1. Bernoulli document model(伯努利文档模型)\n一个文档被表示成01向量.向量中每一个元素表示相应的单词是否在文档中出现了\n令\nDi\nD_i表示第i个文档的01向量\n令\nDit\nD_it表示第i个文档的01向量中第t个元素的值，即单词\nwt\nw_t是否在文档i中出现了\n\nP(wt|ck)\nP(w_t|c_k)表示单词\nwt\nw_t在类别\nck\nc_k中出现的文档数的占比.\n则\n\nP(wt|ck)=ck中wt出现的文档个数ck中所有文档的个数\nP(w_t|c_k)=\\dfrac{c_k中w_t出现的文档个数}{c_k中所有文档的个数}\nP(Dit|ck)=DitP(wt|ck)+(1−Dit)(1−P(wt|ck))\nP(D_{it}|c_k)=D_{it}P(w_t|c_k)+(1-D_{it})(1-P(w_t|c_k))\nP(Di|Ck)=∏|V|t=1P(Dit|ck)\nP(D_i|C_k)=\\prod_{t=1}^{|V|}P(D_{it}|c_k)\n2. Multinomial document model\n一个文档被表示成整数向量.向量中每一个元素表示相应的单词在文档中出现了多少次\n令\nDi\nD_i表示第i个文档的向量\n令\nDit\nD_{it}表示第i个文档的向量中第t个元素的值\nP(wt|ck)\nP(w_t|c_k)表示单词\nwt\nw_t在类别\nck\nc_k中出现的文档数的占比.\n训练句向量\n一般来讲每一个类别\nck\nc_k也可以看成一个向量,记为\nf(ck)\nf(c_k)。\n文本\nDi\nD_i也表示成向量\nw\nw。\n训练句向量也就是训练打分模型\nscore(w,f(ck))\nscore(w,f(c_k))\n可以根据这个设计各种loss函数。用SVM的loss函数训练\n文本或句子向量化\n词袋模型\n0-1向量\nN-Gram Bag-of-Words\nVocab = set of all n-grams in corpus\nDocument = n-grams in document w.r.t vocab with multiplicity\nFor bigram:\nSentence 1: “The cat sat on the hat”\nSentence 2: “The dog ate the cat and the hat”\nVocab = { the cat, cat sat, sat on, on the, the hat, the dog, dog ate, ate the, cat and, and the}\nSentence 1: { 1, 1, 1, 1, 1, 0, 0, 0, 0, 0}\nSentence 2 : { 1, 0, 0, 0, 0, 1, 1, 1, 1, 1}\nTF-IDF\nTF(词频)\n词频(TF)=某个词在文章中的出现次数文章中出现最多词的个数\n词频(TF)=\\dfrac{某个词在文章中的出现次数}{文章中出现最多词的个数}\nIDF(逆文档频率)\n逆文档频率(IDF)=log语料库的文档总数包含该词的文档数+1\n逆文档频率(IDF)=\\log \\dfrac{语料库的文档总数}{包含该词的文档数+1}\n特征过滤\n停用词\n基于文档频率(DF)的特征提取法\n从训练预料中统计出包含某个特征的文档的频率(个数),然后根据设定的阈值，当该特征项的DF值小于某个阈值时，从特征空间中去掉该特征项，因为该特征项使文档出现的频率太低，没有代表性；当该特征项的DF值大于另外一个阈值时，从特征空间中也去掉该特征项，因为该特征项使文档出现的频率太高，没有区分度\n信息增益法\n信息增益(IG)法依据某特征项\nti\nt_i为整个分类所能提供的信息量多少来衡量该特征项的重要程度，从而决定对该特征项的取舍。某个特征项\nti\nt_i的信息增益是指有该特征或没有该特征时，为整个分类所能提供的信息量的差别，其中，信息量的多少由熵来衡量。因此，信息增益即不考虑任何特征时文档的熵和考虑该特征后文档的熵的差值:\n\nGain(ti)=Entropy(S)−Expected Entropy(Sti)={−∑j=1MP(Cj)⋅logP(Cj)}−{P(ti)⋅[−∑j=1MP(Cj|ti)⋅logP(Cj|ti)]   +P(ti¯)⋅[−∑j=1MP(Cj|ti¯)⋅logP(Cj|ti¯)]}\n\\begin{aligned} Gain(t_i) \u0026=Entropy(S)-Expected\\ Entropy(S_{t_i})\\\\ \u0026=\\{-\\sum_{j=1}^MP(C_j)\\cdot\\log P(C_j)\\}-\\{ P(t_i)\\cdot[-\\sum_{j=1}^{M}P(C_j|t_i)\\cdot\\log P(C_j|t_i)]\\\\ \u0026\\ \\ \\ +P(\\bar{t_i})\\cdot[-\\sum_{j=1}^{M}P(C_j|\\bar{t_i})\\cdot\\log P(C_j|\\bar{t_i})]\\} \\end{aligned}\n其中\nP(Cj)\nP(C_j)表示\nCj\nC_j类文档在预料中出现的概率，\nP(ti)\nP(t_i)表示语料中包含特征项\nti\nt_i的文档的概率，\nP(Cj|ti)\nP(C_j|t_i)表示文档包含特征项\nti\nt_i时属于\nCj\nC_j类的条件概率，\nP(ti¯)\nP(\\bar{t_i})表示语料中不包含特征项\nti\nt_i的文档的概率，\nP(Cj|ti¯)\nP(C_j|\\bar{t_i})表示文档不包含特征项\nti\nt_i时属于\nCj\nC_j的条件概率,\nM\nM表示类别数\nmutual information(互信息法)\nχ2\n\\chi^2统计量\nDistributional similarity-based representations\nLSI\nFirst Propose\nWord2vec\nDoc2Vec\n词性标注与隐马尔科夫模型\n维特比算法和算法\nA\nA是状态转移概率矩阵\nB\nB是观测概率矩阵\nπ\n\\pi是初始状态概率向量\n隐马尔科夫模型的三个基本问题\n概率计算问题。给定模型\nλ=(A,B,π)\n\\lambda=(A,B,\\pi)和观测序列\nO=(o1,o2,...,oT)\nO=(o_1,o_2,...,o_T),计算在模型\nλ\n\\lambda下观测序列\nO\nO出现的概率\nP(O|λ)\nP(O|\\lambda)\n学习问题.已知观测序列\nO=(o1,o2,...,oT)\nO=(o_1,o_2,...,o_T).估计模型\nλ=(A,B,π)\n\\lambda=(A,B,\\pi)参数,使得在该模型下观测序列概率\nP(O|λ)\nP(O|\\lambda)最大.即用极大似然估计的方法估计参数.\n预测问题，也称为解码(decoding)问题。已知模型\nλ=(A,B,π)\n\\lambda=(A,B,\\pi)和观测序列\nO=(o1,o2,...,oT)\nO=(o_1,o_2,...,o_T),求对给定观测序列条件概率\nP(I|O)\nP(I|O)最大的状态序列\nI=(i1,i2,...,iT)\nI=(i_1,i_2,...,i_T).即给定观测序列，求最有可能的对应的状态序列.\n问题1:\n前向算法.\n定义前向概率:\n给定隐马尔科夫模型\nλ\n\\lambda,定义到时刻\nt\nt部分观测序列为\no1,o2,...,ot\no_1,o_2,...,o_t且状态为\nqi\nq_i的概率为前向概率,记作\n\nαt(i)=P(o1,o2,...,ot,it=qi|λ)\n\\alpha_t(i)=P(o_1,o_2,...,o_t,i_t=q_i|\\lambda)\n输入:隐马尔科夫模型\nλ\n\\lambda,观测序列\nO\nO\n输出:观测序列概率\nP(O|λ)\nP(O|\\lambda)\n(1) 初值\nα1(i)=πibi(o1),i=1,2,...,N\n\\alpha_1(i)=\\pi_ib_i(o_1), i=1,2,...,N\n(2) 递推　对t=1,2,…,T-1\nαt+1(i)=⎡⎣∑j=1Nαt(j)aji⎤⎦bi(ot+1),i=1,2,...N\n\\alpha_{t+1}(i)=\\left[ \\sum_{j=1}^N\\alpha_t(j)a_{ji}\\right]b_i(o_{t+1}), i=1,2,...N\n(3) 终止\nP(O|λ)=∑i=1NαT(i)\nP(O|\\lambda)=\\sum_{i=1}^{N}\\alpha_T(i)\n(4)最优路径回溯\n后向算法:\n定义后向概率:\n给定隐马尔科夫模型\nλ\n\\lambda,定义在时刻\nt\nt状态为\nqi\nq_i的条件下，从\nt+1\nt+1到\nT\nT的部分观测序列为\not+1,ot+2,...,oT\no_{t+1},o_{t+2},...,o_T的概率为后向概率，记作\n\nβt(i)=P(ot+1,ot+2,...,oT|it=qi,λ)\n\\beta_t(i)=P(o_{t+1},o_{t+2},...,o_T|i_t=q_i,\\lambda)\n输入:隐马尔可夫模型\nλ\n\\lambda,观测序列\nO\nO:\n输出:观测序列概率\nP(O|λ)\nP(O|\\lambda)\n(1)\nβT(i)=1,i=1,2,...,N\n\\beta_T(i)=1,i=1,2,...,N\n(2)对\nt=T−1,T−2,...,1\nt=T-1,T-2,...,1\nβt(i)=∑j=1Naijbj(ot+1)βt+1(j),i=1,2...N\n\\beta_t(i)=\\sum_{j=1}^{N}a_{ij}b_j(o_{t+1})\\beta_{t+1}(j),i=1,2...N\n(3)\nP(O|λ)=∑i=1Nπibi(o1)β1(i)\nP(O|\\lambda)=\\sum_{i=1}^N\\pi_ib_i(o_1)\\beta_1(i)\n问题2\nBaum-Welch算法(无监督学习方法)\n假设给定训练数据只包含\nS\nS个长度为\nT\nT的观测序列\nO1,O2,...,OS\n{O_1,O_2,...,O_S}而没有对应的状态序列，目标是学习隐马尔科夫模型\nλ=(A,B,π)\n\\lambda=(A,B,\\pi)的参数。我们将观测序列数据看做观测数据\nO\nO,状态序列数据看做不可观测的隐数据\nI\nI,那么隐马尔科夫模型事实上是一个含有隐变量的概率模型\n\nP(O|λ)=∑IP(O|I,λ)P(I|λ)\nP(O|\\lambda)=\\sum_IP(O|I,\\lambda)P(I|\\lambda)\n它的参数学习可以由\nEM\nEM算法实现\n参数估计问题是HMM面临的第三个问题，即给定一个观察序列\nO=O1O2...OT\nO=O_1O_2...O_T,如何调节模型\nu=(A,B,π)\nu=(A,B,\\pi)的参数，使得\nP(O|u)\nP(O|u)最大化:\n\nargmaxuP(Otraining|u)\narg \\underset{u}{max} P(O_{training}|u)\n模型的参数是指构成\nu\nu的\nπi,aij,bj(k)\n\\pi_i,a_{ij},b_j(k).最大似然估计方法可以作为HMM参数估计的一种选择。如果产生观察序列\nO\nO的状态序列\nQ=q1q2...qT\nQ=q_1q_2...q_T已知，根据最大似然估计,HMM的参数可以通过如下公式计算:\n\nπ¯i=δ(q1,si)\n\\bar{\\pi}_i=\\delta(q_1,s_i)\n\na¯ij=Q中从状态qi转移到qj的次数Q中所有从状态qi转移到另一状态(包括qi自身)的次数=∑T−1t=1δ(qt,si)∗δ(qt+1,sj)∑T−1t=1δ(qt,si)\n\\begin{aligned} \\bar{a}_{ij}\u0026=\\dfrac{Q中从状态q_i转移到q_j的次数}{Q中所有从状态q_i转移到另一状态(包括q_i自身)的次数}\\\\ \u0026=\\dfrac{\\sum_{t=1}^{T-1}\\delta(q_t,s_i)*\\delta(q_{t+1},s_j)}{\\sum_{t=1}^{T-1}\\delta(q_t,s_i)} \\end{aligned}\n\nb¯j(k)=Q中从状态qj输出符号vk的次数Q到达qj的次数\n\\bar{b}_j(k)=\\dfrac{Q中从状态q_j输出符号v_k的次数}{Q到达q_j的次数}\n但实际上，由于HMM中的状态序列Q是观察不到的(隐变量),因此，这种最大似然估计的方法不可行。所幸的是，期望最大化(expectation maximization,EM)算法可以用于含有隐变量的统计模型的参数最大似然估计。其基本思想是，初始时随机地给模型的参数赋值，该复制遵循模型对参数的限制，例如，从某一状态出发的所有转移概率的和为1。给模型参数赋初值以后，得到模型\nu0\nu_0,然后，根据\nu0\nu_0可以得到模型中隐变量的期望值。例如，从u_0得到从某一状态转移到另一状态的期望次数，用期望次数来替代上式中的实际次数，这样可以得到模型参数的新估计值，由此得到新的模型\nu1\nu_1.从\nu1\nu_1又可以得到模型中隐变量的期望值，然后，重新估计模型的参数，执行这个迭代过程，知道参数收敛于最大似然估计值.\n问题3\n维特比算法:\n其实就是前向算法的变种形式\n输入:隐马尔科夫模型\nλ\n\\lambda,观测序列\nO\nO\n输出:最优路径\nI∗=(i∗1,i∗2,...,i∗T)\nI^*=(i_1^*,i_2^*,...,i_T^*)\n(1) 初值\nα1(i)=πibi(o1),i=1,2,...,N\n\\alpha_1(i)=\\pi_ib_i(o_1), i=1,2,...,N\n\nψ1(i)=0\n\\psi_1(i)=0\n(2) 递推　对t=1,2,…,T-1\nαt+1(i)=max1≤j≤N⎡⎣∑j=1Nαt(j)aji⎤⎦bi(ot+1),i=1,2,...N\n\\alpha_{t+1}(i)=\\underset{1\\le j\\le N}{max}\\left[ \\sum_{j=1}^N\\alpha_t(j)a_{ji}\\right]b_i(o_{t+1}), i=1,2,...N\n\nψt+1(i)=argmax1≤j≤N⎡⎣∑j=1Nαt(j)aji⎤⎦,i=1,2,...N\n\\psi_{t+1}(i)=arg \\underset{1\\le j\\le N}{max}\\left[ \\sum_{j=1}^N\\alpha_t(j)a_{ji}\\right], i=1,2,...N\n(3) 终止\nP∗=max1≤i≤NαT(i)\nP^*=\\underset{1\\le i\\le N}{max}\\alpha_T(i)\n\ni∗T=argmaxi≤i≤NαT(i)\ni_T^*=arg \\underset{i \\le i\\le N}{max}\\alpha_T(i)\n统计语义分析\nPCFG，概率上下文无关文法\n三个基本问题\n- 给定一个句子\nW=w1w2...wn\nW=w_1w_2...w_n和文法\nG\nG,如何快速计算概率\nP(W|G)\nP(W|G)\n- 给定一个句子\nW=w1w2...wn\nW=w_1w_2...w_n和文法\nG\nG,如何选择该句子的最佳结构?即选择句法结构树\nt\nt使其具有最大概率:\nargmaxtP(t|W,G)\nargmax_tP(t|W,G)\n- 给定PCFG G和句子\nW=w1w2...wn\nW=w_1w_2...w_n,如何调节G的概率参数，使句子的概率最大?即求解\nargmaxGP(W|G)\nargmax_GP(W|G)\n问题1:\n内向算法和外向算法:\n内向算法的基本思想是:利用动态规划算法计算非终结符\nA\nA推导出\nW\nW中子串\nwiwi+1...wj\nw_iw_{i+1}...w_j的概率\naij(A)\na_{ij}(A)\n有递推公式如下:\n\naii(A)=P(A−\u003ewi)\na_{ii}(A)=P(A-\u003ew_i)\n\naij(A)=∑B,C∑i≤k≤j−1P(A−\u003eBC)⋅aik(B)⋅a(k+1)j(C)\na_{ij}(A)=\\sum_{B,C}\\sum_{i\\le k\\le j-1}P(A-\u003eBC)\\cdot a_{ik}(B)\\cdot a_{(k+1)j}(C)\n算法如下:\n输入:PCFG G(S)和句子\nW=w1w2...wn\nW=w_1w_2...w_n\n输出:\naij(A),1≤i≤j≤n\na_{ij}(A),1\\le i \\le j\\le n\n步1 初始化:\naii(A)=P(A→wi),1≤i≤n\na_{ii}(A)=P(A\\rightarrow w_i),1\\le i\\le n\n步2 归纳计算:\nj=1...n,i=1...n−j\nj=1...n,i=1...n-j,重复下列计算:\nai(i+j)(A)=∑B,C∑i≤k≤i+j−1P(A→BC)∗aik(B)∗a(k+1)(i+j)(C)\na_{i(i+j)}(A)=\\sum_{B,C}\\sum_{i\\le k \\le i+j-1}P(A\\rightarrow BC)*a_{ik}(B)*a_{(k+1)(i+j)}(C)\n步3 终结:\nP(S→w1w2...wn)=a1n(S)\nP(S\\rightarrow w_1w_2...w_n)=a_{1n}(S)\n外向算法的基本思想是:\n定义外向变量\nβij(A)\n\\beta_{ij}(A)为初始非终结符\nS\nS在推导出语句\nW=w1w2...wn\nW=w_1w_2...w_n的过程中，产生符号串\nw1...wi−1Awj+1...wn\nw_1...w_{i-1}Aw_{j+1}...w_n的概率\n有如下递推公式:\n\nβ1n(A)={10A=SA≠S\n\\beta_{1n}(A)=\\left\\{ \\begin{array}{rcl} 1 \u0026 \u0026 {A=S}\\\\ 0 \u0026 \u0026 {A\\neq S}\\\\ \\end{array} \\right.\n\nβij(A)=∑B,C∑k\u003ejP(B→AC)α(j+1)k(C)βik(B)   +∑B,C∑k\u003ciP(B→CA)αk(i−1)(C)βkj(B)\n\\begin{aligned} % requires amsmath; align* for no eq. number \\beta_{ij}(A) \u0026 =\\sum_{B,C}\\sum_{k\\gt j}P(B\\rightarrow AC)\\alpha_{(j+1)k}(C)\\beta_{ik}(B) \\\\ \u0026 \\ \\ \\ +\\sum_{B,C}\\sum_{k\\lt i}P(B\\rightarrow CA)\\alpha_{k(i-1)}(C)\\beta_{kj}(B)\\\\ \\end{aligned}\n问题2:\n就是将内向算法的递推式取最大\n\naii(A)=P(A→wi)\na_{ii}(A)=P(A\\rightarrow w_i)\naij(A)=argmaxB,C∈N;i≤k≤i+jP(A→BC)⋅aik(B)⋅a(k+1)j(C)\na_{ij}(A)=arg\\underset{B,C\\in N;i\\le k\\le i+j}{max}P(A\\rightarrow BC)\\cdot a_{ik}(B)\\cdot a_{(k+1)j}(C)\n然后用变量\nβij\n\\beta_{ij}记忆子串\nwi...wj\nw_i...w_j的维特比句法分析树\n\nβij(A)=argmaxB,C∈N;i≤k≤i+jP(A→BC)⋅aik(B)⋅a(k+1)j(C)\n\\beta_{ij}(A)=arg\\underset{B,C\\in N;i\\le k\\le i+j}{max}P(A\\rightarrow BC)\\cdot a_{ik}(B)\\cdot a_{(k+1)j}(C)\nTreebank\nChomsky Normal Form\n统计机器翻译","date":"2018年01月02日 13:08:24"}
{"_id":{"$oid":"5d36a8a86734bd8e681d5dea"},"title":"自然语言处理综述","author":"吾苏踵","content":"一、前言\n1、前人研究\n图灵的图灵机\n关于算法计算模型的研究；图灵机是一种抽象的数学模型；\n香农的信息论\n噪声声道，解码；把熵作为测量信道的信息能力或者语言的信息量的一种方法，用概率测定；噪声信道与解码模型；\n信息：文字和语言/数字和信息；信息冗余是信息安全的保障/语料对翻译至关重要。\n信息的度量：信息熵是对一个信息系统不确定性的度量；熵；冗余度；条件熵；互信息；相对熵；相对熵，利用它可以得到词频率-逆向文档频率TF-IDF；香农第一定理：对于一个信息，任何编码的长度都不小于它的信息熵；信息的作用就是消除不确定性，自然语言处理的大量问题就是找相关的信息。\n2、发展历史\n\u003c90年代：规则系统：专家系统和知识工程；\n1990-2014:概率系统：规则从数据中抽取/规则是有概率的；流程设计-手机训练数据-预处理-抽取特征-分类器-预测-评估；特征和流程都是专家设计的，存在大量独立的子任务\n2014之后：深度学习。\n3、形式模型\n1、基于短语结构语法的形式模型、基于合一运算的形式模型、基于依存和配价的形式模型、基于格语法的形式模型、基于词汇主义的形式模型；\n2、基于概率和统计的形式模型：n-gram，隐马尔科夫模型、最大熵模型，条件随机场，查理亚克的概率上下文无关语法和词汇化的概率上下文无关语法、贝叶斯公式、动态规划算法、噪声信道模型、最小编辑距离算法、决策树模型、加权自动机、维特比算法、向内向外算法、向前向后算法。\n3、语义自动处理的形式模型\n4、语用自动处理的形式模型\n4、基本介绍\n自然语言处理：是机器理解人类语言和表达方式并作出回应；\n句法分析和语义消歧（依赖上下文，消除歧义性）；\n语言信息、世界信息和视觉信息；\n主要任务：文本处理、文本生成、文本翻译；\n层次分类：语音学，形态学，语法学，语义学，语用学；\n研究方法：理性主义，经验主义；\n统计方法：隐马尔可夫模型，上下文无关文法，噪声信道模型；\n关键问题：歧义消解问题和未知语言现象；\n挑战：一词多义、新词、不规范用语；领域隔离，只有封闭环境可用；数据获取难；效果评估难；\n过去25年来，自动问答的需求被网页搜索和数据挖掘替代，新的应用越来越依靠数据的作用和浅层的自然语言处理。研究者们也从单纯的句法分析和语义理解，转变到了对机器翻译、语音识别、文本生成、数据挖掘和知识获取等方向。\n二、形式语言与自动机\n语言：按照一定规律构成的句子或者字符串的有限或者无限的集合。\n描述语言的三种途径：穷举法、文法描述和自动机。\n基础知识：集合论/图论\n1、基本概念\n图、树和字符串\n2、形式语言\n缺陷：对于像汉语英语这样的大型自然语言系统，难以构造精确的文法/不符合人类学习语言的习惯/有些句子语法正确，但在语义上却不可能，形式语言无法排出这些句子；\n解决方向：基于大量语料，采用统计学手段建立模型。\n形式语法：正则文法，上下文无关文法，上下文相关文法和无约束文法；\n3、自动机\n有限自动机，下推自动机，线性带限自动机和图灵机\n应用：单词自动查错纠正/词性消歧。\n三、语料库与词汇知识库\n1、语料库\n语料库，基于语料的统计方法；\n2、词汇知识库\n3、本体论\n4、知网\n定义了各种关系/动态演化认知架构系统：概念对象和动作对象。\n概念之间定义了两种关系：扩展和属性。\n动作接受一些概念对象，然后产出一些新的概念对象，动作接受的概念对象有两类：一类是必须要有的，没有动作就没法执行；另一类是可选的，可有可无，类似提供了默认参数。\n四、统计语言模型\n语言模型就是给某句语言打分，给某个话题打分；狗叫模型；球星模型；电影模型；\n概率系统：基本分类器；经典序列模型（HMM/CRF/EM、自动机、语言模型）；\n概率语言模型：核心就是通过分数告诉机器怎么说话；\n概率模型：语言模型、翻译模型、文本对齐、seq2seq模型；\n语言模型：文法语言模型、统计语言模型。统计语言模型：n-gram模型；深度学习：神经序列模型LSTM\n相似度计算：篇章表示、编辑距离；\ncomputing device：自动机、规则系统、分类器；\n搜索技术：关键词匹配、Beam Search、Local Sensitive Hashing、倒排索引；\n语言相关技术：stemming、同义词识别替换、中文分词、语法分析、语义意图理解；\nseq2seq模型：基本分类器、诗歌生成、情感分析、机器翻译、序列标注 ；\n人能否解决，如果能就自己解决，然后考虑机器能否模仿；人如果不能解决，就尝试从计算机的角度思考；\nBash Script：wc/sed/awk/grep/sort/uniq/paste/cat/head/tail(学会linux下的基本命令)；python：处理稍微复杂的问题；\nStanford Core NLP（语义分析）；NLTK（句子划分、读取语义树）；Tensorflow；\n有向网络图：首先将问句进行词法分析，得到语义组块序列，然后对其进行意图识别，意图分为两部分：目标概念对象和条件概念对象。Viv的核心技术就是利用DECAS找到从条件概念对象到目标概念对象的联通路径，称之为计划。\n1、n-gram模型\n统计语言模型：根据前面的所有词测算当前词出现的概率，最后累积相乘，得到整句话出现的概率。/马尔可夫假设当前词出现的概率只与前一个词有关，二元模型，也可以假设由前面n-1个词决定，为n-gram模型；结合语料库计算相对频度，根据大数定理估算最终条件概率。\nn-gram模型：利用链式法则计算每句话的概率P（w1.w2.,,,wn）；引入马尔可夫假设：无记忆性：未来的事件，只取决于有限的历史；unigram\\bigram\\trigram分别对应1，2，3个参考事件；\n模型评估\n外在评估：能不能抓到老鼠；语音识别的准确性；特点：接近业务场景但比较慢和复杂；\n内在评估：颜色速度和力量；预测测试集的能力；特点：与真正的目标有偏差但快和简单；\nperplexity（ppx）指标。\n新词\n新词：未登录词，ovv;\np(wi|wi-1,wi-2)=count(wi-2,wi-1,wi)/count(wi-2,wi-1);\n最大似然估计方法:log p(Td);拉格朗日法工具：KenLM(Modified Kneser-Ney Smoothing)\n应用案例：完形填空，加入使用3 gram LM\n数据平滑\n模型的参数：模型中所有的条件概率。\n模型的训练：通过对语料的统计，得到这些参数的过程。由于大数定理的局限，需要增加数据量，但不可避免，因此出现了概率估计：古德-图灵估计；\nZipf定律：出现次数少的词总比出现次数多的词要多；\n数据平滑，解决零概率，下调出现频率很低的词的概率,卡茨退避法。\nn-gram平滑：\n本质上是贫富分化的问题；\n+1平滑方法：政府给大家每人发一点钱，没用；\nBack-off回退法：自己有钱自己出，自己没钱爸爸出，爸爸没钱爷爷出；\ninterpolate插值法：自己，爸爸，爷爷各出一笔钱，Development Set,EM最大期望值算法解决；\nAbsolute Discounting\"绝对折扣\"：有钱的，每个人叫固定的税D，建立一个基金；没钱的，根据自己爸爸有多少钱来分了这个基金；\nKneser-Ney:有钱人缴固定税，按爸爸人脉分配；词的适配度；\nModified KN:有钱人缴阶梯税，按爸爸人脉分配；阶梯税率，最好的方法！\n自适应\n训练用户特定的语言模型的步骤如下：将训练语言模型的文本按照主题分成很多不同的类别/对于每个类，找到它们的特征向量/统计某个人输入的文本，得到他输入词的特征向量/余弦定理测相似度/选择距离最近的类对应的文本，作为这个特定用户语言模型的训练数据/训练处一个用户特定的语言模型。特定模型在特定领域内效果比通用模型好，但相对偏僻的内容，就比不上通用模型。因此需要采用最大熵模型来综合两个模型的特征，简化来做的话，采用线性插值的模型。\n2、其他语言模型（指数）\n概率图模型\n马尔可夫模型\n隐马尔可夫模型\n通信模型：信息，上下文-编码-信道-解码-接收；\n独立输出假设；\n三个基本问题：\n1.给定一个模型，如何计算某个特定的输出序列的概率（前向-后向算法）；\n2.给定一个模型和某个特定的输出序列，如何找到最可能产生这个输出的状态序列（维特比算法）；\n3.给定足够量的观测数据，如何估计隐含马尔可夫模型的参数（模型训练）。\n模型训练：计算转移概率和生成概率。\n有监督训练：人工标注；\n无监督训练：鲍姆-韦尔奇算法；\nEM过程，期望值最大化，保证算法迭代到最优。\n条件随机场\n条件随机场是隐马尔可夫模型的扩展，是一种特殊的概率图模型，变量之间要遵循马尔可夫假设，即每个状态的转移概率只取决于相邻的概率。与贝叶斯网络不同的是，条件随机场是无向图。根据最大熵原则，希望找到一个符合所有边缘分布，同时使得熵最大的模型，就是指数函数。浅层句法分析：看到的东西是词、词性，要推导的东西是语法成分。条件随机场是一个非常灵活的用于预测的统计模型。\n对于给定的句子，进行分词、词性标记、命名实体识别和链接、句法分析、语义角色识别和多义词消歧。\n前后向算法及参数估计\n维特比算法：解码算法，动态规划算法，可以解决最短路径的问题，凡是使用隐马尔可夫模型描述的问题都可以用它来解码，可以概括成以下三点：\n1、如果最短路径经过某个点，那么这一条路径上的子路径，一定也是最短路径，否则用另一段最短路径来代替它，便构成更短的路径，这明显是矛盾的。\n2、路径上必定经过某个时刻的某个状态，嘉假定在某个时刻有很多状态，那么如果记录了从起始到这个状态所有节点的最短路径，最终最短路径一定是这其中的一条。\n3、假定状态变更，最短路径已经找到，并且记录在这些节点上，那么考虑最短路径时，只需要考虑前一个状态的最短路径，以及这个节点的距离即可。维特比算法是和长度成正比的。\n五、神经网络语言模型\n5种神经网络语言模型：\na) Neural Network Language Model ，NNLM\nb) Log-Bilinear Language Model， LBL\nc) Recurrent Neural Network based Language Model，RNNLM\nd) Collobert 和 Weston 在2008 年提出的 C\u0026W 模型\ne) Mikolov 等人提出了 CBOW（ Continuous Bagof-Words）和 Skip-gram 模型\n词向量\nword2vec的神经网络是浅层的，GloVe实现了一种计数方法，借助两者进行训练的模型通常用作深度学习NLP方法的输入数据。fastText避免了OOV问题，在小数据集上效果更优。主要好处是不用自己积累语料库，只需要爬取网络数据即可。\n为什么要用向量表示？因为单词编码是任意的，很难表示出之间的任意关系，还会带来数据系数问题，使用向量进行词的表示可以克服一些障碍。上下文相似的词，其语义也相似；词的语义由其上下文决定。选择一种方式描述上下文-选择一种模型刻画目标词与上下文之间的关系。\nGloVe：Global Vector模型，是一种基于矩阵的分布式表示模型\n词向量：基于神经网络的分布式表示，word embedding\nword2vec模型通常用于预处理阶段的词向量表示。\n一种方法是构建共现矩阵，其中包含着语料库中每一个单词同出现在它后一个单词的统计信息。\n相同语境中的词语具有相似的语义，基于这一原则，有两类方法：计数的方法（隐性语义分析）和预测方法（神经概率语言模型）。两者的区别在于前者计算出共同出现的概率然后映射到每个词的小而密集的向量。预测模型直接尝试根据学习到的近邻单词的小密集嵌入向量（模型参数）来预测单词。一种用于从原始文本中学习词嵌入的模型，具有很高的计算效率。两种实现方式：连续词袋模型（CBOW），skip-gram模型。两者相似，唯一的区别在于前者从源上下文单词中预测目标单词，而后者根据目标单词预测源上下文单词。\n简单的窗口分类器：softmax，将元素变成概率值\n最重要的3个环节是分词、锁定关键词、文本相似度计算。\n1:词义， 相似度计算，词义向量，微积分，lamanda，为向量的短语赋予意义，情绪分析，邮件建议回复，机器翻译\n2:skip-gram模型，给定概率，向量表示，最大化概率分布，损失函数，目标函数，成本函数，单词序列，theta模型，中心向量和上下文向量，更改参数，期望向量，加权，梯度下降，SGD\n3：编码会话，职业展览，项目建议，哈希表，语料库，损失函数【0，1】，随机抽样，超参数，unigram，hacky，最大化概率，最小化成本，连续词汇模型，PCA主成分分析\n语义组合\n简单的加权组合；\n卷积神经网络；\n循环神经网络；\n递归神经网络。\n六、基本方法\n1、词法分析\n中文：字/词/短语/句子/段落/文档。\n相关概率，定义/相关任务模型/方法；\n词性标注及其一致性检查方法。\n分词方法/未登录词处理/词性标注/自动校对/命名实体识别；\n中文分词：\n词是表达语义的最小单位：中国/航天/历史/已经/有/100/年。\n查字典/动态规划，利用维特比算法快速地找到最佳分词（分词器）。\n分词的一致性/词的颗粒度和层次。\n矩阵运算和文本处理中的两个分类问题：\nSVD奇异值分解\n就是把一个大矩阵，分解成三个小矩阵相乘，第一个矩阵X是对词进行分类的一个结果；最后一个矩阵Y是对文本的分类结果；中间矩阵B表示词的类和问这个的类之间的相关性。使用矩阵的特征值和数值分析中的各种算法就可以进行奇异值分解。奇异值分解不可迭代，适合处理超大规模文本的粗分类。信息指纹及其应用/相似哈希。\nSPAM反作弊：\n1、从信息源出发，加强通信（编码）自身的抗干扰能力；\n2、从传输来看，过滤掉噪音，还原信息。最大熵模型/原理：保留全部的不确定性，将风险降到最小。\n贝叶斯网络\n马尔可夫链的扩展，很多事物的相互关系显然不能用一条链来表示，他们之间的关系可能是一个有向网络。状态和关系，可信度用概率来描述，可以有附加的权重。它虽然也是依赖前一个的状态，但不受链状结构的限制，可以更准确滴描述事件之间的相关性。\n贝叶斯网络在文本分类-主题模型中的应用：\n把文本和关键词的关联矩阵扭转90度，进行奇异值分解，或者对每一个词以文本为维度，建立一个向量，再进行向量的聚类，那么得到的是对词的分类而不是文本的分类，分出来的每一类我们成为一个概念。贝叶斯网络是一个加权的有向图，是马尔可夫链的扩展。它克服了马尔可夫链那种机械的线性约束，把任何有关联的事件统一到了它的框架下面。\n期望最大化算法\n文本自动分类算法，不需要预定义类别，也不需要合并聚类。只要随机的挑出一些类的中心，然后优化这个中心，是他们和真实的聚类中心尽可能一致。\n分类的步骤如下：\n1、随机挑选一些点，作为起始的中心。\n2、计算所有点到这些聚类中心的距离，将这些点归到最近的一类中。\n3、重新计算每一类的中心，新的聚类中心和原先的相比会有一个位移。\n4、重复上述过程，直到新的中心和旧的中心之间偏移非常非常小，即过程收敛。\n1、根据现有的聚类结果，对所有数据进行重新划分。如果把最终的分类结果看作是一个数学的模型，那么这些聚类的中心，以及每一个点和聚类的隶属关系，可以看作是这个模型的参数。\n2、根据重新划分的结果，得到新的聚类。最大化目标函数。EM算法：E过程：期望值计算过程；M过程：最大化过程。如果我们优化的目标函数是一个凸函数，那么一定保证能得到全局最优解。\n2、句法分析\n句法结构分析：完全句法分析、浅层分析；依存关系分析。\n分析方法：基于规则；基于统计。\n3、词义消歧\n七、处理流程\n1、处理流程\n根据要求，将自然语言处理成query，再加以形式化，建立语言模型，称之为算法和计算模型；对计算模型的研究，是一个强不适定问题，因为难以满足存在性、唯一性和稳定性的要求，所以应当加入约束条件，使在一定范围内编程适定问题。\n1、形式化表示为数学形式\n2、形式化表示为算法，表现为模型\n3、编写程序，在计算机上加以实现\n4、评测，不断改进和优化，以满足需求。\n数据基本处理流程：获取数据-数据预处理（观察数据-分词-去除停用词）-特征工程-机器学习工程。\n2、指标类别\nTP:true positives 真正：判断为真的正确率；\nTN:True negatives 真负：判断为假的正确率；\nFP:false positives 假正：判断为正的误报率；\nFN:false negatives 假负：判断为负的漏报率；\naccuracy准确率：反映了分类器对整个样本的判定能力，也就是说能将正的判定为正，负的判定为负；\nA=(TP+TN)/(TP+FN+FP+TN)。\nprecision精准度：被分类器判定正例中的正样本的比重；\nP=TP/(TP+FP)。\nrecall召回率：被预测为正例的占总的正例的比重；\nR=TP/(TP+FN)。\nF-measure:precision和recall调和均值的2倍；\nF=(a2+1)P*R/a2(P+R),取参数a=1。当F1较高时，说明实验结果比较理想。 以具体场景为例：假定某个班级有男生80人，女生20人，共计100人。目标是找出所有女生。现在某人挑选出了50人，其中20人是女生，把其余30人错认为是女生。请你来评估一下他的工作。\n假定目标女生为正类P，男生为负类N，则TP=20，FP=30，TN=50，FN=0，A=70/100，P=20/50，R=20/20。\n八、技术应用\n1、应用方向\n自然语言生成/文本分类/信息检索/信息抽取/文字校对/问答系统/机器翻译/自动摘要/文字蕴涵/对话系统/文本挖掘。\n信息抽取\n从给定文本中抽取重要信息，如时间、地点、人物、事件、原因、结果、数字、日期、货币、专有名词等，涉及到实体识别、时间抽取、因果关系抽取等。\n文本挖掘\n包括文本聚类、分类、信息抽取、摘要、情感分析以及对所挖掘信息、知识的可视化和交互式的表达界面，基于统计机器学习。\n机器翻译\n输入一种语言，输出另外一种语言。根据输入媒介不同，可以分为文本翻译、语音翻译、手语翻译、图形翻译等。机器翻译最早基于规则，后来基于统计，到近年基于神经网络，发展至今。\n信息检索\n对大规模的文档进行检索。可简单对文档中的词汇，赋之以不同的权重来建立索引，也可利用123的技术来建立更深层的索引。在查询的时候，对输入的查询表达式比如一个检索词或者句子进行分析，然后在索引里面查找匹配的候选文档，再根据一个排序机制把候选文档排序，最后输出排序得分最高的文档。 4.3.6问答系统 对一个自然语言表达的问题，由问答系统给出一个精准的答案。需要对自然语言查询语句进行某种程度的语义分析，包括实体链接、关系识别，形成逻辑表达式，然后道知识库中查找可能的候选答案并通过一个排序机制找出最佳答案。\n对话系统\n系统通过一系列的对话，跟用户聊天、回答、完成某一项任务。涉及到用户意图理解、通用聊天引擎、问答引擎、对话管理等技术。为了体现上下文关联，需要具备多伦对话能力。同时为了体现个性化，要开发用户画像以及基于用户画像的个性化回复。\n2、相关项目\n自动生成天气预报；自动翻译和自动问答；饭馆咨询服务；图像到语音的转换；残疾人增强交际；旅行咨询服务；语音地理导航；语音资料搜索；跨语言信息检索和翻译；作文自动评分；自动阅读家庭教师；个性化市场服务。\n输入法的应用：语言模型和自动机；\n自动拼写更正：语言模型、自动机和编辑距离；\n机器翻译：中文分词、文本对齐、翻译模型、语言模型、Beam Search\nQuery意图理解：模板匹配、分类器\nEvernote推荐系统：篇章表示、相似度计算、Local Sensitive Hashing、文本分类、倒排索引\n小黄鸡：关键词匹配、倒排索引\n英文写作助手：语法分析、倒排索引、stem（找词根）\n重大事件监测：模板匹配、分类器、\n医疗诊断书自动生成：规则系统、深度学习\n体育报道自动生成：模板填充、同义词替换、文本对齐\n法律专利生成：模板匹配、分类器\n聊天互动：seq2seq\n邮件自动回复：seq2seq模型、语义意图理解\n行业：办公自动化、文体娱乐行业、财经、法律、医疗\n3、相关会议\nACL、EMNLP、EACL、NAACL\n4、其他\n自然语言处理与知识图谱的区别\n自然语言处理的研究对象是计算机和人类语言的交互，其任务是理解人类语言并将其转换为机器语言。在目前的商业场景中，NLP技术用于分析源自邮件、音频、文件、网页、论坛、社交媒体中的大量数据。知识图谱是通过将应用数学、图形学、信息可视化技术、信息科学等学科的理论与方法与计量学引文分析、共现分析等方法结合，并利用可视化的图谱形象地展示学科的核心结构、发展历史、前沿领域以及整体知识架构达到多学科融合目的的现代理论。它把复杂的知识领域通过数据挖掘、信息处理、知识计量和图形绘制而显示出来，揭示知识领域的动态发展规律，为学科研究提供切实的、有价值的参考。\n自然语言处理与机器学习的区别\n自然语言处理都需要依赖统计学知识，而且它和机器学习不同，机器学习依靠的更多是严谨的数学知识以及推导，去创造一个又一个机器学习算法，而自然语言处理是把那些机器学习大牛们创造出来的东西当工具使用。所以入门也只是需要涉猎而已，把每个模型原理看看就行了。\n数学模型的重要性\n1、一个正确的数学模型应当在形式上是简单的；\n2、一个正确的模型一开始可能还不如一个精细雕琢过的错误模型来的准确，但是，如果我们认定大方向是对的，就应该坚持下去。\n3、大量准确的数据对研发很重要。\n4、正确的模型也可能受噪音干扰，而显得不准确；这时不应该用一种凑合的方法来弥补它，而要找到噪音的根源，这也许能通往重大的发现。","date":"2018年12月25日 11:08:16","data":"2018年08月15日 22:01:30"}
{"_id":{"$oid":"5d36a8a86734bd8e681d5dec"},"title":"基于统计的自然语言处理和基于规则的自然语言处理的一些个人看法","author":"北海仔","content":"基于规则的自然语言处理在自然语言处理刚开始发展的时候，是处在领先的地位，因为人们觉得自然语言处理的过程和人类学习和认知一门语言的过程应该相似的，所以大量的研究员也是从这个点出发来研究自然语言处理。\n但是后来发现根据规则开发出来的系统不是特别好，有的研究员就换一个方向，觉得应该从基于统计的角度来研究自然语言处理，其实基于统计的方法有点像通信的编码与解码，原始语句就是一个已经编码的信息，然后我们根据统计的概率，解码处原始信息。这个方法是自然语言处理的效果有了很大的提升，慢慢地，该方法也占据了该方向的主导地位。基于统计的自然语言处理为什么能发展这么好，有它的历史原因：互联网的高速发展，使得人们可以得到丰富的语料，而完善的语料库是统计自然语言处理成功地一个先决条件，还有就是硬件的发展也给该方向提供了强大的硬件支撑。\n虽然目前基于统计的自然语言处理比基于规则的自然语言处理有很大优势，但是历史是发展的，我们谁也不能保障，在将来的某一天，基于规则的自然语言处理会不会因为具备了某种条件而再一次重获得历史的主角。","date":"2014年11月13日 16:14:17"}
{"_id":{"$oid":"5d36a8a96734bd8e681d5dee"},"title":"自然语言处理技术之HanLP介绍","author":"adnb34g","content":"这段时间一直在接触学习hadoop方面的知识，所以说对自然语言处理技术也是做了一些了解。网络上关于自然语言处理技术的分享文章很多，今天就给大家分享一下HanLP方面的内容。\n自然语言处理技术其实是所有与自然语言的计算机处理相关联的技术的统称，自然语言处理技术应用的目的是为了能够让计算机理解和接收我们用自然语言输入的指令，实现从将我们人类的语言翻译成计算机能够理解的并且不会产生歧义的一种语言。接合目前的大数据以及人工智能，自然语言处理技术的快速发展能够很好的助力人工智能的发展。\n（大快DKhadoop技术架构图）\n这里要分享的HanLP是我在学习使用大快DKhadoop大数据一体化平台时使用到的自然语言处理技术，使用这个组建可以很高效的进行自然语言的处理工作，比如进行文章摘要，语义判别以及提高内容检索的精确度和有效性等。\n本想找个通俗的案例来介绍一下HanLP,一时间也没想到什么好的案例，索性就从HanLp数据结构HE 分词简单介绍下吧。\n首先我们来看了解下HanLP的数据结构：\n二分tire树：Tire树是一种前缀压缩结构，可以压缩存大量字符串，并提供速度高于Map的get操作。HanLP中的trie树采用有序数组储存子节点，通过二分搜索算法检索，可以提供比TreeMap更快的查询速度。\n不同于父节点储存子节点引用的普通trie树，双数组trie树将节点的从属关系转化为字符内码的加法与校验操作\n对于一个接收字符c从状态s移动到t的转移，需满足条件是：\nbase[s] + c = t\ncheck[t] = s比如：base[一号] + 店 = 一号店\ncheck[一号店] = 一号\n相较于trie树的前缀压缩（success表），AC自动机还实现了后缀压缩（output表）\n在匹配失败时，AC自动机会跳转到最可能成功的状态（fail指针）\n关于HanLP分词\n1、词典分词\n基于双数组trie树或ACDAT的词典最长分词(即从词典中找出所有可能的词，顺序选择最长的词语)\n输出:[HanLP/名词, 是不是/null, 特别/副词, 方便/形容词, ？/null]\n2、NGram分词\n统计语料库中的BiGram，根据转移概率，选出最可能的句子，达到排除歧义的目的\n3、HMM2分词\n这是一种由字构词的生成式模型，由二阶隐马模型提供序列标注\n被称为TnT Tagger，特点是利用低阶事件平滑高阶事件，弥补高阶模型的数据稀疏问题\n4、CRF分词\n这是一种由字构词的生成式模型，由CRF提供序列标注\n相较于HMM，CRF的优点是能够利用更多特征、对OOV分词效果好，缺点是占内存大、解码慢。","date":"2018年04月12日 14:54:17"}
{"_id":{"$oid":"5d36a8a96734bd8e681d5df0"},"title":"自然语言处理入门（一）","author":"美美王子","content":"概念\n（Natural Language Processing, 简称NLP）就是利用电子计算机为工具对人类特有的书面形式和口头形式的自然语言的信息进行各种类型处理和加工的技术，这种技术现在已经形成一门专门的边缘性交叉性学科，它涉及语言学、数学和计算机科学，横跨文科、理科和工科三大知识领域。自然语言处理的目的在于建立各种自然语言处理系统，如机器翻译系统、自然语言理解系统、信息自动检索系统、信息自动抽取系统、文本信息挖掘系统、术语数据库系统、计算机辅助教学系统、语音自动识别系统、语音自动合成系统、文字自动识别系统等。\n自然语言是人类区别于其它动物的重要标志之一。人借助于自然语言交流思想，达到互相了解，组成人类社会生活；人还借助于自然语言进行思维活动，认识事物的本质和规律，创造了人类的物质文明和精神文明。\n自然语言起码在下面四个方面与人工语言大相径庭：\n（1） 自然语言中充满着歧义，而人工语言中的歧义则是可以控制的；\n（2） 自然语言的结构复杂多样，而人工语言的结构则相对简单；\n（3） 自然语言的语义表达千变万化，迄今还没有一种简单而通用的途径来描述它，而人工语言的语义则可以由人来直接定义；\n（4） 自然语言的结构和语义之间有着千丝万缕的、错综复杂的联系，一般不存在一一对应的同构关系，而人工语言则常常可以把结构和语义分别进行处理，人工语言的结构和语义之间有着整齐的一一对应的同构关系。\n自然语言处理的发展\n基于句法-语义规则的理性主义方法受到质疑，随着语料库建设和语料库语言学的崛起，大规模真实文本的处理成为自然语言处理的主要战略目标，概率和数据驱动的方法几乎成为了自然语言处理的标准方法。\n自然语言处理越来越多的使用机器自动学习的方法来获取语言知识\n统计数学越来越受到重视\n自然语言处理中越来越重视词汇的作用，出现了强烈的“词汇主义”的倾向。\n多语言在线自然语言处理技术迅猛发展。随着网络技术的发展，互联网（Web）逐渐变成一个多语言的网络世界，互联网上的机器翻译、信息检索和信息抽取等自然语言处理的需要变得更加紧迫。\n自然语言处理的常见运用\n信息提取\n如下面的这段话:\nHi Dan, we’ve now scheduled the curriculummeeting. It will be in Gates 159 tomorrow from10:00-11:30.-Chris\n我们通过自然语言处理能够得出如下信息:\nEvent： Curriculum mtg Date： Dec-18-2017 Start： 10:00am End: 11:30am where: Gates 159\n语义分析\n比如淘宝某个商品的评价,我们能够提取信息,并且根据语义来进行测评.\n比如对于一个照相机\n提取出如下的主要特征:\nzoom ,affordability, size and weight, flash ,ease of use\n我们根据语义分析,搜集到关于大小和重量的如下三个评价:\n1. 拿起来很好很舒适\n2. 好轻的照相机,我再也不用拿着又大又笨的机器到处跑了.\n3. 这个照相机太娇嫩了,拿在手上必须非常小心.\n再进一步的,通过NLP,我们可以得出前两个是好的评价,最后一个是不好的.\n通过这些,我们就可以对这款相机做一些基于NLP的测评指标。\n自动翻译.\n谷歌翻译、百度翻译、网易云翻译等等就是实例.\n工欲善其事，必先利其器\n自己最近爱上了自然语言处理、机器翻译、人工智能，看了老师推荐的《计算机自然语言处理》，真的是云里雾里，不能说都看不懂，但是没有get到有层次的东西，所以自己在网站各个博客、社区，知乎、github上看了一些关于自然语言处理入门的讲解或者简述，了解了一些之后，我其实觉得，自然语言处理就是机器学习。自己根据了解的情况写了这篇杂记，也安排了自己的一个学习计划：\n《计算机自然语言处理》——上面分词规范特别学习，跟着看一些python在自然语言处理上的应用源码，多看源码在github上，多在练习中更加深刻理解自然语言处理的思想；\n编程语言我用的是python\n现阶段先学习这些写吧，眼看就要考试了，期间看看《数学之美》，应该会很不错。\n感觉还是云里雾里。~~~~！","date":"2017年12月19日 15:04:05"}
{"_id":{"$oid":"5d36a8aa6734bd8e681d5df2"},"title":"自然语言处理关注博客和文章","author":"小小她爹","content":"整理的部分网络资源文章，后续会持续补充。\n\n\n1、综述博客\n我爱自然语言处理\nhttp://www.52nlp.cn/\n\n码农场\nhttp://www.hankcs.com/\n\n\n\n\n\n2、书籍和入门\n自然语言相关资源入门\nhttp://www.52nlp.cn/about\n\n\n自然语言处理书籍推荐\nhttp://www.52nlp.cn/%E4%B9%A6%E7%B1%8D\n\n\n水木论坛自然语言处理版\n\nhttp://www.newsmth.net/bbsdoc.php?board=NLP\n\n\n我爱自然语言处理整理的资源\nhttp://www.52nlp.cn/resources\n\n\n\n知乎上推荐入门\nhttp://www.zhihu.com/question/19895141\n\n\n\n北京大学中文系应用语言学专业课程安排\n\nhttp://ccl.pku.edu.cn/all/info.asp?item=2\u0026page=1\u0026expand=6\n\n\n\n3、研究主题\n分词\nAho-Corasick算法的Java实现与分析\nhttp://www.hankcs.com/program/algorithm/implementation-and-analysis-of-aho-corasick-algorithm-in-java.html\n\n\nTrie树分词\nhttp://www.hankcs.com/program/java/tire-tree-participle.html\n\n\nAnsj分词双数组Trie树实现与arrays.dic词典格式\nhttp://www.hankcs.com/nlp/ansj-word-pairs-array-tire-tree-achieved-with-arrays-dic-dictionary-format.html\n\n\n\n双数组Trie树(DoubleArrayTrie)Java实现\nhttp://www.hankcs.com/program/java/%E5%8F%8C%E6%95%B0%E7%BB%84trie%E6%A0%91doublearraytriejava%E5%AE%9E%E7%8E%B0.html\n\n\n基于深度学习的分词系统：采用tensorflow\nhttps://github.com/koth/kcws   训练\n\n\n\n主题分类：\nLDA入门与Java实现\nhttp://www.hankcs.com/nlp/lda-java-introduction-and-implementation.html\n\n\nLDA数学八卦\n\nhttp://www.52nlp.cn/lda-math-%E6%B1%87%E6%80%BB-lda%E6%95%B0%E5%AD%A6%E5%85%AB%E5%8D%A6\n\n\n深度学习\ntensorflow相关：\n英文官方网站：\nhttp://tensorflow.org/\n\n\n官方GitHub仓库：\nhttps://github.com/tensorflow/tensorflow\n\n\n中文版 GitHub 仓库：\nhttps://github.com/jikexueyuanwiki/tensorflow-zh\n\n\n\n4、语料库\n【汇总】语料库资源\n\nhttps://www.douban.com/note/269081724/\n\n\n\n\n\n中文文本语料库整理(不定时更新2015-10-24).md\n\nhttp://www.jianshu.com/p/206caa232ded\n\n\n\n中文文本分类的新闻语料库\n\nhttp://www.52nlp.cn/opencorpus\n\n\n\n搜狗实验室语料库\nhttp://www.sogou.com/labs/resource/list_yuliao.php\n\n\n\n\n\n5、论坛\n语料库语言学在线\n\nhttp://www.corpus4u.org/\n\n\n\n\n\n国内语料库建设一览表\nhttp://blog.csdn.net/yujun00/article/details/541633\n\n\n\n6、新增加的地址\n自然语言一个微信号\nhttps://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==\u0026mid=2247484260\u0026idx=1\u0026sn=3df8f915c7aed6038d50436381cc8e5b\u0026chksm=eb5017f7dc279ee12a6f324e332aa54d1b01102725add8cf1bac2d620870200d1dce0288a749\u0026mpshare=1\u0026scene=23\u0026srcid=0412DvqQp9Nj5OC8925qDLRs#rd\n中文信息学报核定刊物：\nhttps://item.taobao.com/item.htm?spm=a230r.1.14.16.b6e73fc2I1NRvB\u0026id=567299675084\u0026ns=1\u0026abbucket=5#detail\n中文信息学报高引用文章： http://jcip.cipsc.org.cn/CN/column/column35.shtml\noxford-cs-deepnlp-2017/lectures  https://github.com/oxford-cs-deepnlp-2017/lectures\n深度与自然语言处理书： https://zhuanlan.zhihu.com/p/25612011\nNLP进阶书籍说明： http://www.sohu.com/a/211831610_473283","date":"2016年09月02日 22:27:03"}
{"_id":{"$oid":"5d36a8ab6734bd8e681d5df7"},"title":"统计自然语言处理梳理四：篇章分析","author":"alihonglong","content":"进行统计自然语言处理系统梳理，学习资料《统计自然语言处理.宗成庆》\n篇章分析的最终目标是从整体上理解篇章，最重要的任务是分析篇章结构。篇章结构包括：语义结构，话题结构，指代结构等。\n一、基本理论\n概念依存理论。\nBeaugrandeand Dressler(1981)认为篇章有7个基本特征：衔接性，连续性，意图性，信息性，可接受性，情景性和跨篇章性。其中衔接性，连续性，意图性和信息性对自然语言产生了很多影响。\n言语行为理论。\n中心理论。\n修辞结构理论。\n脉络理论。\n篇章表示理论。\n二、篇章衔接性\n衔接又称为外部联结，主要表现为整个篇章范围内词汇或短语之间的关联。当语篇中一个成分的含义依赖于另一个成分的解释时，便产生衔接关系。\nHallidayand Hasan(1980)将衔接分为五种情况：一般指代，替换，省略，连接和词汇衔接。目前的研究主要集中在指代消解方面和词汇衔接方面的衔接性相关研究。\n指代一般包括两种情况：回指和共指。二者之间有很大的交集，但并不严格地彼此地包含。\n词汇衔接方面。\n整体上篇章衔接性研究还处于初步或停留于问题本身。\n三、篇章连贯性\n连贯性又称内部联结，主要通过句子之间的语义关联来表示篇章不同部分之间的关联关系。这方面主要针对信息性和意图性进行展开。","date":"2016年08月27日 08:16:25"}
{"_id":{"$oid":"5d36a8ab6734bd8e681d5dfa"},"title":"自然语言处理(1)——文本分词","author":"VioletCherry","content":"一想到每次被同学问到“你在做什么？”，我回答“自然语言处理。”，同学就会说“这么难的东西你也做？”，我就觉得，很多人觉得自然语言处理很难，其实是对自然语言处理的误解，至少入门没大家想象的那么难，其实自然语言处理不是难在算法复杂，而是语句的结构。因为不管是在商品评论或是社交平台上，人的发言。最近闲来无事，正好自己也在做一个微博话题情感分析的工具。所以准备将整个过程记录下来，以供想入门的同学参考。\n所有的处理都要在词性标注的文件上进行，所以要进行文本分析，首先要进行分词，现有的分词工具有不少。我自己用的是中科院开发的ICTCLAS，目前更新到2016版，可以在http://ictclas.nlpir.org/下载。下面讲一下如何来应用这个工具来进行分词。\n我平时写程序是用C/C++，所以在这里以C/C++为例。下载好解压之后如图1，下边的Data文件夹复制到自己工程下，该Data文件夹下存放了词典、配置文件和许可证信息。同时，在lib文件夹下选择与自己系统相关的NLPIR.dll文件和NLPIR.lib文件复制到自己工程下，然后将include文件夹下的NLPIR.h复制到工程下，现在就可以开始在自己的工程中编码进行分词了。\n\n\n图1\n开发环境我选择的是VS2012，如何建工程这里就不说了，如何建工程这里就不说了。要想使用NLPIR动态链接库，还有引入头文件，也就是刚才我们复制到工程下边的NLPIR.h，此外，用#pargma comment指定要连接的库。\n#include\"NLPIR.h\" #pragma comment(lib,\"NLPIR.lib\");\n\n\n现在我们可以调用它的分词函数进行分词。我们写一个函数来调用该库中的函数。\nint participle(char* src_file,char* result_file,char* userdict){ if(!NLPIR_Init()) return -1; NLPIR_SetPOSmap(ICT_POS_MAP_FIRST); NLPIR_ImportUserDict(userdict); NLPIR_FileProcess(src_file,result_file,1); NLPIR_Exit(); return 0; }\n其中NLPIR_Init()为初始化函数，NLPIR_SetPOSmap()设置标注集：\nICT_POS_MAP_FIRST 计算所一级标注集\nICT_POS_MAP_SECOND 计算所二级标注集\nPKU_POS_MAP_SECOND 北大二级标注集\nPKU_POS_MAP_FIRST 北大一级标注集\nNLPIR_ImportUserDict()导入用户词典来提高分词精度，NLPIR_FileProcess(src_file,result_file,1)进行分词，其中src_file为需要分词的文件，result_file为分词结果文件。\n现在文本经过分词已经是标注好的文本，如图2所示。\n\n\n图2\n可以看到在分词结果中每个词后面都是跟/在跟词性，为了方便后面处理，我们将/过滤掉，具体程序如下\n\nint resultFilter(char* src_file,char* result_file){ int i=0; ifstream ifs_tfilter_file; ofstream ofs_tfilter_result_file; char cixing; ifs_tfilter_file.open(src_file,ios::in); ofs_tfilter_result_file.open(result_file,ios::ate); while (!ifs_tfilter_file.eof()) { ifs_tfilter_file\u003e\u003ecixing; if(cixing=='\\/') { ofs_tfilter_result_file\u003c\u003c\" \"; i=1; } else { ofs_tfilter_result_file\u003c\u003ccixing; if(i==1) { ofs_tfilter_result_file\u003c\u003c\" \"; i=0; } } } return 0; }\n过滤之后的结果如图3所示\n\n\n图3\n过滤/后的文本就比较方便我们后续的其他处理了。\n\n至此，自然语言处理的第一步分词就结束了，尽管ICTCLAS分词系统精度已经很高，但在某一领域中，其分词也有不准确的时候。所以，在分词完成后，应该人工检验一下是否有和自己比较相关的词被切分错误的情况，然后筛选出来作为添加词典，这样便可以保证其准确了。","date":"2016年01月13日 21:56:13"}
{"_id":{"$oid":"5d36a8ac6734bd8e681d5dfc"},"title":"使用Mahout实现自然语言处理","author":"wbj0110","content":"cestella/NLPWithMahout · GitHub是一个使用Mahout实现自然语言处理(NLP:Natural Language Processing)的开源项目。\n\nNLP一词来自于统计自然语言处理，来自google的研究主管Peter Norvig评价这本书：如果有人告诉我在一年内能赚一百万，那么就只有这本书能做到，我复制了这本书，并开始启动一个web 文本处理公司。\n\nApache Mahout is 一个能够运行在Hadoop上的分布式机器学习算法高性能库，可用算法如下：\n\n\n类型：算法 Linear Algebra : Stochastic Gradient Descent Linear Algebra : Stochastic Singular Value Decomposition Classification: Random Forests Classification : Naïve Bayesian Classification :Hidden Markov Models Clustering :Normal and Fuzzy K-Means Clustering :Expectation Maximization Clustering :Dirichlet Process Clustering Clustering :Latent Dirichlet Allocation Clustering :Spectral Clustering Clustering :MinHash Clustering Pattern Mining: Parallel FP Growth\n\nMahout 提供了很多工具库，允许从hadoop中以ML算法格式获取数据。\n\n基本模式有：\n1.将文本转为序列文件 SequenceFiles，通过 seqdirectory\n命令。\n2.将序列文件转为一系列稀疏向量，使用seq2sparse，根据选择使用 word-integer和feature-weight.\n3.转换与稀疏向量关联的Key到使用rowid命令的可增量整数。\n\n另外，可在Mahout中使用Latent Dirichlet Allocation(LDA)，通过客户一遍一遍购买记录，能够猜测其购买偏好。Mahout是LDA原始实现的性能15倍。\n\nLDA在Mahout使用方式：\n输入数据作为一个稀疏向量；\n建立来自文档字段的管道，如下三个步骤：\n1. seqdirectory ! 转换包含每行一个文档的系列文档到序列文件\n2. seq2sparse ! 将序列文件作为条目字典转为稀疏向量。\n3. rowid !转为稀疏向量的key到整数\ncvb工具能够运行LDA算法。输入是字数加权频率的序列文件，输出是topic 模型。\nhttp://www.jdon.com/45591","date":"2014年07月08日 11:58:34"}
{"_id":{"$oid":"5d36a8ad6734bd8e681d5dfe"},"title":"如何学习自然语言处理：一本书和一门课","author":"梅逊雪","content":"关于“如何学习自然语言处理”，有很多同学通过不同的途径留过言，这方面虽然很早之前写过几篇小文章：《如何学习自然语言处理》和《几本自然语言处理入门书》，但是更推崇知乎上这个问答：自然语言处理怎么最快入门，里面有微软亚洲研究院周明老师的系统回答和清华大学刘知远老师的倾情奉献：初学者如何查阅自然语言处理（NLP）领域学术资料，当然还包括其他同学的无私分享。\n不过，对于希望入门NLP的同学来说，推荐你们先看一下这本书: Speech and Language Processing，第一版中文名译为《自然语言处理综论》，作者都是NLP领域的大大牛：斯坦福大学 Dan Jurafsky 教授和科罗拉多大学的 James H. Martin 教授。这也是我当年的入门书，我读过这本书的中文版（翻译自第一版英文版）和英文版第二版，该书第三版正在撰写中，作者已经完成了不少章节的撰写，所完成的章节均可下载：Speech and Language Processing (3rd ed. draft)。从章节来看，第三版增加了不少和NLP相关的深度学习的章节，内容和篇幅相对于之前有了更多的更新：\nChapter\nSlides\nRelation to 2nd ed.\n1:\nIntroduction\n[Ch. 1 in 2nd ed.]\n2:\nRegular Expressions, Text Normalization, and Edit Distance\nText [pptx] [pdf]\nEdit Distance [pptx] [pdf]\n[Ch. 2 and parts of Ch. 3 in 2nd ed.]\n3:\nFinite State Transducers\n4:\nLanguage Modeling with N-Grams\nLM [pptx] [pdf]\n[Ch. 4 in 2nd ed.]\n5:\nSpelling Correction and the Noisy Channel\nSpelling [pptx] [pdf]\n[expanded from pieces in Ch. 5 in 2nd ed.]\n6:\nNaive Bayes Classification and Sentiment\nNB [pptx] [pdf]\nSentiment [pptx] [pdf]\n[new in this edition]\n7:\nLogistic Regression\n8:\nNeural Nets and Neural Language Models\n9:\nHidden Markov Models\n[Ch. 6 in 2nd ed.]\n10:\nPart-of-Speech Tagging\n[Ch. 5 in 2nd ed.]\n11:\nFormal Grammars of English\n[Ch. 12 in 2nd ed.]\n12:\nSyntactic Parsing\n[Ch. 13 in 2nd ed.]\n13:\nStatistical Parsing\n14:\nDependency Parsing\n[new in this edition]\n15:\nVector Semantics\nVector [pptx] [pdf]\n[expanded from parts of Ch. 19 and 20 in 2nd ed.]\n16:\nSemantics with Dense Vectors\nDense Vector [pptx] [pdf]\n[new in this edition]\n17:\nComputing with Word Senses: WSD and WordNet\nIntro, Sim [pptx] [pdf]\nWSD [pptx] [pdf]\n[expanded from parts of Ch. 19 and 20 in 2nd ed.]\n18:\nLexicons for Sentiment and Affect Extraction\nSentLex [pptx] [pdf]\n[new in this edition]\n19:\nThe Representation of Sentence Meaning\n20:\nComputational Semantics\n21:\nInformation Extraction\n[Ch. 22 in 2nd ed.]\n22:\nSemantic Role Labeling and Argument Structure\nSRL [pptx] [pdf]\nSelect [pptx] [pdf]\n[expanded from parts of Ch. 19 and 20 in 2nd ed.]\n23:\nNeural Models of Sentence Meaning (RNN, LSTM, CNN, etc.)\n24:\nCoreference Resolution and Entity Linking\n25:\nDiscourse Coherence\n26:\nSeq2seq Models and Summarization\n27:\nMachine Translation\n28:\nQuestion Answering\n29:\nConversational Agents\n30:\nSpeech Recognition\n31:\nSpeech Synthesis\n另外该书作者之一斯坦福大学 Dan Jurafsky 教授曾经在Coursera上开设过一门自然语言处理课程：Natural Language Processing，该课程目前貌似在Coursera新课程平台上已经查询不到，不过我们在百度网盘上做了一个备份，包括该课程视频和该书的第二版英文，两个一起看，效果更佳：\n链接: https://pan.baidu.com/s/1kUCrV8r 密码: jghn 。\n对于一直寻找如何入门自然语言处理的同学来说，先把这本书和这套课程拿下来才是一个必要条件，万事先有个基础。\n同时欢迎大家关注我们的公众号：NLPJob，回复\"slp\"获取该书和课程最新资源。\n本条目发布于2017年07月24号。属于自然语言处理分类，被贴了 Dan Jurafsky、James H. Martin、NLP书籍、NLP入门、NLP课程、Speech and Language Processing、斯坦福大学，科罗拉多大学、深度学习、自然语言处理、自然语言处理书籍、自然语言处理入门、自然语言处理综论、自然语言处理课程 标签。作者是52nlp。\n\n\n\n\n\n\n\n\n这里推荐一批学习自然语言处理相关的书籍，当然，不止是自然语言处理，国内的书籍相对比较便宜，值得购买。\n1、《自然语言处理综论》，当年的入门书，不过翻译的是第一版，英文名《Speech and Language Processing\u003e, 第三版据说很快就要出版（2016年），有条件的同学建议直接看英文版第二版。\n2、《统计自然语言处理基础》，另一本入门书籍，这本书的英文版貌似没有更新，但是中文版貌似也不再发售了，当然，优先推荐读英文版。\n3、《Python自然语言处理》，NLTK配套丛书，有了上面两本书的介绍，再加上一些Python基础，通过这本书进行相关的文本挖掘实战，很不错的一个路径。\n4、宗成庆老师的《统计自然语言处理（第2版）》，当年读书的时候大致看过第一版，作为入门书籍不错。\n5、国内青年学者刘知远老师等合著的《互联网时代的机器学习和自然语言处理技术大数据智能》，没有仔细看过，仅供参考。\n6、南大周志华老师的西瓜书《机器学习》，最近出版的书籍，国内难得学习机器学习的高质量书籍，评价非常高，强烈推荐。\n7、CMU机器学习系主任Tom Mitchell院士的 《机器学习》，机器学习老牌经典书籍，历久弥新。\n华章引进的英文版也不贵，不过貌似没货：《机器学习（英文版》\n8、比较新的一本机器学习书籍，被誉为内容全面的机器学习教程 Machine Learning期刊主编力作：《机器学习》\n9、李航老师的这本《统计学习基础》挺不错的，简洁明了：《统计学习基础》\n10、王斌老师翻译的《大数据 互联网大规模数据挖掘与分布式处理（第2版）》，质量挺不错的，对应的英文书籍是《Mining of Massive Datasets》，有相应的官方主页，提供相应的英文PDF，课程和课件资源。\n————————————————————————————\n自然语言处理中的若干问题（http://blog.csdn.net/yueyedeai/article/details/14524151）\n\n\n\n一、语言模型\n（一）N元语言模型\n（二）语言模型性能评价\n（三）数据平滑\n（四）语言模型自适应方法\n二、汉语自动分词和词性标注\n（一）基本分词方法\n（二）未登陆词处理方法\n（三）基于多特征的命名实体模型\n（四）词性标注\n（五）词性标注的一致性检查和自动校对\n三、句法分析\n（一）统计句法分析以及句法分析的检查\n（二）层次化汉语长句结构分析\n（三）浅层句法分析\n（四）依据句法理论与依存句法分析\n四、语义消歧\n（一）有监督的语义消歧\n（二）基于词典的语义消歧\n（三）无监督的语义消歧\n（四）语义消歧系统评测\n五、文本分类\n（一）文本表示\n（二）文本 特征选择方法\n（三）特征权重计算方法\n（四）分类器设计\n（五）文本分类器性能评估方法\n六、自动文摘和信息抽取\n（一）多文档摘要\n（二）单文档摘要\n（三）信息抽取\n七、文档聚类\n（一）聚类算法\n（二）聚类结果评估\n八、自然语言处理的主要范畴\n1．文本朗读（Text to speech）/语音合成（Speech synthesis）\n2．语音识别（Speech recognition）\n3．中文自动分词（Chinese word segmentation）\n4．词性标注（Part-of-speech tagging）\n5．句法分析（Parsing）\n6．自然语言生成（Natural language generation）\n7．文本分类（Text categorization）\n8．信息检索（Information retrieval）\n9．信息抽取（Information extraction）\n10．文字校对（Text-proofing）\n11．问答系统（Question answering）\n12．机器翻译（Machine translation）\n13．自动摘要（Automatic summarization）\n14．文字蕴涵（Textual entailment）\n九、自然语言处理研究的难点\n1． 单词的边界界定\n在口语中，词与词之间通常是连贯的，而界定字词边界通常使用的办法是取用能让给定的上下文最为通顺且在文法上无误的一种最佳组合。在书写上，汉语也没有词与词之间的边界。\n2．词义的消歧\n许多字词不单只有一个意思，因而我们必须选出使句意最为通顺的解释。\n3．句法的模糊性\n自然语言的文法通常是模棱两可的，针对一个句子通常可能会剖析（Parse）出多棵剖析树（Parse Tree），而我们必须要仰赖语意及前后文的资讯才能在其中选择一棵最为适合的剖析树。\n4．有瑕疵的或不规范的输入\n例如语音处理时遇到外国口音或地方口音，或者在文本的处理中处理拼写，语法或者光学字符识别（OCR）的错误。\n5．语言行为与计划\n句子常常并不只是字面上的意思；例如，“你能把盐递过来吗”，一个好的回答应当是把盐递过去；在大多数上下文环境中，“能”将是糟糕的回答，虽说 回答“不”或者“太远了我拿不到”也是可以接受的。再者，如果一门课程去年没开设，对于提问“这门课程去年有多少学生没通过？”回答“去年没开这门课”要 比回答“没人没通过”好。","date":"2017年10月12日 10:47:53"}
{"_id":{"$oid":"5d36a8ae6734bd8e681d5e01"},"title":"统计自然语言处理学习（概论）","author":"continueOo","content":"定义\n书中定义的统计自然语言处理由所有的自动语言处理的定量方法组成，包括概率模型，信息论，线性代数。代表自然语言处理中非符号化和非逻辑的工作。\n语言的非绝对性，需要利用统计观察来考察问题。\n个人思考\n因为生活中充满了不确定和不完整的信息，为了能和世界有效的相互作\n用，我们需要处理这类信息，所以概率论和随机过程给我么一个可以处理不确定和不完整信息架构的量化框架\n这里只是因为想到认知是随机的，所以推广到语言，但是我认为问题是需要针对特定问题的，在这里我认为语言处理的第一步就是需要让机器知道我们的某些想法，并且完成某些事情。如果阶段性的去划分这样一个过程，我觉得应该是这样。\n下命令-\u003e执行命令\n1.给出具体某一条命令，电脑执行某一条命令：开机/关机\n2.给出命令，电脑反馈所有能执行的命令：明天天气真好-\u003e各个游玩地点信息，日程安排等。。。\n3.给出任意命令，准确知道我要干什么。\n那么以上的问题首先就是机器要能做某些事情。\n语言就是信息，信息就是一定要传达某种内容，目标就是解析内容嘛！\n关于歧义自己的想法\n其实我还没有觉得有什么歧义的问题，首先你必须知道这个句子中的每个词语和字，如果这个都不知道肯定分析不出来嘛，我还是那个观点，每个人心中对词都有一个词网，“南京市长江大桥”如果我知道这个地方，OK，那么这个词就是一个固定词，如果不知道，那么我就要用已知的方式去猜测，长江大桥我有概念，南京市我也知道。如果能精确的让我去划分这个短语，我肯定不知道江大桥是个什么玩意，所以我能很好的判断出来，也就是说我需要建立这样一套系统，我要给出我对每个词语了解程度的多少。关键就成了如何去联系词语和现实世界。\n语法规则问题：\n关于语法这一块的话，我现在是这么考虑的，所有的内容都是词语，不需要。必要，一定是必要的。以我语文语法水平为基准，我知道的基本我认为是必要的。名词，动词，形容词。词语中人物，时间，地点等基本少许的概念一定是要有的吧。既然是我知道的，那么规则也肯定不多，肯定是够用的。\n一些法则\n由于语料库使用中绝大部分词语出现极少，而常用词出现频率极高，这样很难预测行为，最初认为使用更大的语料库就可以解决这个问题，但是愿望是无法证实的，下面提出语料库语言学中最著名的早期结论：Zipf法则，这个法则针对的问题就是这些稀有词汇。\nZipf我们能够统计一种语言中所有的词在一个大型语料库中出现的次数，并且按照其出现次数排列，发现f x r =k","date":"2017年05月13日 23:30:47"}
{"_id":{"$oid":"5d36a8ae6734bd8e681d5e04"},"title":"自然语言处理——中英文分词工具（还可做词性标注与命名实体识别）","author":"南木Sir","content":"更多内容请至南木博客主页查看哦\n中文分词工具\nJieba\nSnowNLP\nTHULAC\nNLPIR\nNLPIR\nStanfordCoreNLP\nHanLP\n英文分词工具\nnltk\nnltk\nnltk\nSpacy\nSpacy\nStanfordCoreNLP\n更多关于自然语言处理的内容，请转至\n自然语言处理（一）——中英文分词\n自然语言处理（二）——词性标注与命名实体识别\n自然语言处理（三）——句法分析与依存句法分析\n哈工大依存句法分析工具——LTP的使用与安装\n进行查看\n同时也欢迎各位关注我的微信公众号 南木的下午茶","date":"2019年05月24日 21:33:08"}
{"_id":{"$oid":"5d36a8af6734bd8e681d5e06"},"title":"SQuad2.0自然语言处理界最重量级的数据集","author":"BBlue-Sky","content":"追赶ImageNet ，发力自动问答领域\n这个数据集文章展现了着斯坦福做一个自然语言处理的ImageNet的野心，他很可能成为自然语言学术界未来至少一年内最流行的数据集。模型在这个数据集上做出好成绩，可以让自己的文章加分不少，被顶会录取的几率大大增加。如果读者想发顶会，且目前没有明确的研究方向，那么刷这个数据集是一条很好的道路。\n于此同时，这个数据集也会为工业界做出贡献。之所以说会为工业界做出贡献，因为自然语言处理的研究风气和图像相比差一些，任务较多，且没有在paper里面附带代码的行业规则，导致很多工作无法重现，甚至有些人会连实验都不做，直接往图和表里面填数造一篇文章。而这个数据集学习了Imagenet，不给测试集，这样你就没法作弊，把代码交上来，我来给你跑，之后把测试集合上的水平评测出来，这样大家都公平，谁也别吹牛，谁也别作弊。此种环境有利于真正大贡献的工作得以浮现，例如Residual Network在去年席卷图像领域，在一个公平的环境下，以比其他对手好很多的效果呈现在了世人的面前。而SQuAD则是斯坦福在自然语言处理上，意图构建一个类似“ImageNet”的测试集合，分数实时在leaderboard上显示。\n这就让这个数据集有如下优势：\n1.测试出真正的好算法。尤其对于工业界，这个数据集是十分值得关注的，因为他可以告诉大家现在各个算法在“阅读理解”或者说“自动问答”这个任务上的排名。我们可以光看分数排名，就知道世界上哪个算法最好，不会再怀疑是作者做假了还是实现的不对。\n2.提供一个阅读理解的大规模数据集。由于之前的阅读理解数据集规模太小或者十分简单，用一个普通的深度学习算法就可以刷到90%度，所以并不能很好的体现不同算法优劣。\n纵使SQuAD不会像ImageNet有那么大的影响力，但绝对也会在接下来的几年内对自动问答领域产生深远的影响，并且是各大巨头在自动问答这个领域上的兵家必争之地（IBM已经开始了）。","date":"2018年12月13日 12:39:43"}
{"_id":{"$oid":"5d36a8af6734bd8e681d5e09"},"title":"如何检索自然语言处理领域相关论文","author":"毛球饲养员","content":"如何检索自然语言处理领域相关论文\n前言\n针对自身的情况，发现个人查找论文的能力，看论文的能力有些薄弱。特此进行如果检索合适的论文写一个博客。\n本文主要是摘自刘知远 老师的新浪博客和南京理工大学文本挖掘研究组博客\n综述\n要快速地熟悉一个领域，更加深刻地了解这该领域的发展，就必须查阅这个领域的相关论文。本文主要讲述自然语言处理领域（NLP）相关论文的检索。\n与其他领域一样，自然语言处理领域每年都有大量的论文发表在各种期刊、会议上，然而人的时间和精力是有限的，如何能在有限的时间内，检索出该领域的高影响力、高质量的论文，是我们所关注的。对于这个问题，首先我们应当了解一下自然语言知名的学术组织、学术会议及学术论文，其次是在了解上述信息基础上的论文检索手段。\n下面，本文将从国内外自然语言处理领域知名的学术组织、学术会议及学术论文及相关论文检索和筛选的经验两方面内容，介绍一些关于自然语言处理领域的知识和论文检索的经验。\n本文第一部分引用清华大学刘知远老师新浪博客上的一篇博文，针对国内外自然语言处理领域知名的学术组织、学术会议及学术论文的介绍。第二部分将分享一些前一段时间，在论文调研过程中关于论文查找和筛选的一些经验，希望对大家有所帮助。\n正文\n1. 初学者如何查阅自然语言处理（NLP）领域学术资料（作者：刘知远）\n昨天实验室一位刚进组的同学发邮件来问我如何查找学术论文，这让我想起自己刚读研究生时茫然四顾的情形：看着学长们高谈阔论领域动态，却不知如何入门。经过研究生几年的耳濡目染，现在终于能自信地知道去哪儿了解最新科研动态了。我想这可能是初学者们共通的困惑，与其只告诉一个人知道，不如将这些Folk Knowledge写下来，来减少更多人的麻烦吧。当然，这个总结不过是一家之谈，只盼有人能从中获得一点点益处，受个人认知所限，难免挂一漏万，还望大家海涵指正。\n1.1 国际学术组织、学术会议与学术论文\n自然语言处理（natural language processing，NLP）在很大程度上与\n计算语言学\n（computational linguistics，CL）重合。与其他计算机学科类似，NLP/CL有一个属于自己的最权威的国际专业学会，叫做The Association for Computational Linguistics（\nACL\n，URL：http://aclweb.org/），这个协会主办了NLP/CL领域最权威的国际会议，即ACL年会，ACL学会还会在北美和欧洲召开分年会，分别称为\nNAACL\n和\nEACL\n。除此之外，ACL学会下设多个特殊兴趣小组（special interest groups，SIGs），聚集了NLP/CL不同子领域的学者，性质类似一个大学校园的兴趣社团。其中比较有名的诸如SIGDAT（Linguistic data and corpus-based approaches to NLP）、SIGNLL（Natural Language Learning）等。这些SIGs也会召开一些国际学术会议，其中比较有名的就是SIGDAT组织的\nEMNLP\n（Conference on Empirical Methods on Natural Language Processing）和SIGNLL组织的\nCoNLL\n（Conference on Natural Language Learning）。此外还有一个International Committee on Computational Linguistics的老牌NLP/CL学术组织，它每两年组织一个称为\nInternational Conference on Computational Linguistics (COLING)\n的国际会议，也是NLP/CL的重要学术会议。NLP/CL的主要学术论文就分布在这些会议上。\n作为NLP/CL领域的学者最大的幸福在于，ACL学会网站建立了称作\nACL Anthology\n,支持该领域绝大部分国际学术会议论文的免费下载，甚至包含了其他组织主办的学术会议，例如COLING、IJCNLP等，并支持基于Google的全文检索功能，可谓一站在手，NLP论文我有。由于这个论文集合非常庞大，并且可以开放获取，很多学者也基于它开展研究，提供了更丰富的检索支持，具体入口可以参考ACL Anthology页面上方搜索框右侧的不同检索按钮。\n与大部分计算机学科类似，由于技术发展迅速，NLP/CL领域更重视发表学术会议论文，原因是发表周期短，并可以通过会议进行交流。当然NLP/CL也有自己的旗舰学术期刊，发表过很多经典学术论文，那就是\nComputational Linguistics\n。该期刊每期只有几篇文章，平均质量高于会议论文，时间允许的话值得及时追踪。此外，ACL学会为了提高学术影响力，也刚刚创办了\nTransactions of ACL\n，值得关注。值得一提的是这两份期刊也都是开放获取的。此外也有一些与NLP/CL有关的期刊，如ACM Transactions on Speech and Language Processing，ACM Transactions on Asian Language Information Processing，Journal of Quantitative Linguistics等等。\n根据Google Scholar Metrics 2013年对NLP/CL学术期刊和会议的评价，\nACL、EMNLP、NAACL、COLING、LREC、Computational Linguistics\n位于前5位，基本反映了本领域学者的关注程度。\nNLP/CL作为交叉学科，其相关领域也值得关注。主要包括以下几个方面：\n（1）信息检索和数据挖掘领域。相关学术会议主要由美国计算机学会（ACM）主办，包括SIGIR、WWW、WSDM等；\n（2）人工智能领域。相关学术会议主要包括AAAI和IJCAI等，相关学术期刊主要包括Artificial Intelligence和Journal of AI Research；\n（3）机器学习领域，相关学术会议主要包括ICML，NIPS，AISTATS，UAI等，相关学术期刊主要包括Journal of Machine Learning Research（JMLR）和Machine Learning（ML）等。例如最近兴起的knowledge graph研究论文，就有相当一部分发表在人工智能和信息检索领域的会议和期刊上。实际上国内计算机学会（CCF）制定了“中国计算机学会推荐国际学术会议和期刊目录”（http://www.ccf.org.cn/sites/ccf/aboutpm.jsp?contentId=2567814757463），通过这个列表，可以迅速了解每个领域的主要期刊与学术会议。\n最后，值得一提的是，美国Hal Daumé III维护了一个- （1）信息检索和数据挖掘领域。相关学术会议主要由美国计算机学会（ACM）主办，包括natural language processing的博客 (http://nlpers.blogspot.com/)，经常评论最新学术动态，值得关注。我经常看他关于ACL、NAACL等学术会议的参会感想和对论文的点评，很有启发。另外，ACL学会维护了一个Wiki页面（http://aclweb.org/aclwiki/），包含了大量NLP/CL的相关信息，如著名研究机构、历届会议录用率，等等，都是居家必备之良品，值得深挖。\n1.2 国内学术组织、学术会议与学术论文\n与国际上相似，国内也有一个与NLP/CL相关的学会，叫做\n中国中文信息学会\n（URL：http://www.cipsc.org.cn/）。通过学会的理事名单（http://www.cipsc.org.cn/lingdao.php）基本可以了解国内从事NLP/CL的主要单位和学者。学会每年组织很多学术会议，例如全国计算语言学学术会议（CCL）、全国青年计算语言学研讨会（YCCL）、全国信息检索学术会议（CCIR）、全国机器翻译研讨会（CWMT），等等，是国内NLP/CL学者进行学术交流的重要平台。尤其值得一提的是，全国青年计算语言学研讨会是专门面向国内NLP/CL研究生的学术会议，从组织到审稿都由该领域研究生担任，非常有特色，也是NLP/CL同学们学术交流、快速成长的好去处。值得一提的是，2010年在北京召开的COLING以及2015年即将在北京召开的ACL，学会都是主要承办者，这也一定程度上反映了学会在国内NLP/CL领域的重要地位。此外，计算机学会中文信息技术专委会组织的自然语言处理与中文计算会议（NLP\u0026CC）也是最近崛起的重要学术会议。中文信息学会主编了一份历史悠久的《中文信息学报》，是国内该领域的重要学术期刊，发表过很多篇重量级论文。此外，国内著名的《计算机学报》、《软件学报》等期刊上也经常有NLP/CL论文发表，值得关注。\n1.3 如何快速了解某个领域研究进展\n最后简单说一下快速了解某领域研究进展的经验。你会发现，搜索引擎是查阅文献的重要工具，尤其是谷歌提供的Google Scholar，由于其庞大的索引量，将是我们披荆斩棘的利器。\n当需要了解某个领域，如果能找到一篇该领域的最新研究综述，就省劲多了。最方便的方法还是在Google Scholar中搜索“领域名称 + survey / review / tutorial / 综述”来查找。也有一些出版社专门出版各领域的综述文章，例如NOW Publisher出版的Foundations and Trends系列，Morgan \u0026 Claypool Publisher出版的Synthesis Lectures on Human Language Technologies系列等。它们发表了很多热门方向的综述，如文档摘要、情感分析和意见挖掘、学习排序、语言模型等。\n如果方向太新还没有相关综述，一般还可以查找该方向发表的最新论文，阅读它们的“相关工作”章节，顺着列出的参考文献，就基本能够了解相关研究脉络了。当然，还有很多其他办法，例如去videolectures.net上看著名学者在各大学术会议或暑期学校上做的tutorial报告，去直接咨询这个领域的研究者，等等。\n2.补充\n参考：南京理工大学文本挖掘研究组博客\n另，附上南京理工大学文本挖掘研究组博客实现的一款论文调研工具。该工具基于Python的爬虫技术，可根据论文发表年份、关键字、发表会议等信息，自动批量抓取主题相关论文的标题，然后，从Google Scholar获取引用次数、下载链接、论文作者、论文摘要信息并按指定的格式保存在EXCEL文档中。\ngithub链接\n总论","date":"2017年08月27日 19:38:49"}
{"_id":{"$oid":"5d36a8b06734bd8e681d5e0b"},"title":"《pyhton自然语言处理》学习笔记（一）","author":"beckyUp","content":"前言\n最近在做的一个项目，非结构化数据处理，然后从自然语言处理入手，开始学习一下如果使用python进行自然语言的处理\n参考资料：https://github.com/wnma3mz/Nltk_Study\nhttps://wnma3mz.github.io/hexo_blog/2018/05/13/《Python自然语言处理》阅读笔记（一）/\n首先下载了anaconda，然后按照说明下载了数据文件\n这个数据文件，一开始并不知道要拷到哪里去，但是尝试了输入了\nimport nltk from nltk.book import *\n之后，发现了报错\n然后我在download之后顺其根源发现了需要拷贝的地址\n\n这样以后，找到了需要拷贝的目录，将数据文件复制进去，这样就可以运行我需要的操作了~\n然后再运行以上代码\n# 导入nltk模块 import nltk # 导入基本语料集(不需要额外下载)，包含text1到text9变量，可以直接输出这些变量 from nltk.book import * # 搜索文本。这里表示找到\"monstrous\"所包含的句子，并且输出上下文 text1.concordance(\"monstrous\") # 搜索文本出现在相似的上下文中 text1.similar(\"monstrous\") # 搜索两个及两个以上共同词的上下文 text2.common_contexts([\"monstrous\", \"very\"]) # 画一张离散图表示这些词出现在文本中的位置，输出见下图 text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"]) # 基于文本，随机生成一些文本 text3.generate()\n最后一句话 又遇到了报错\n\n看上去是缺少参数\n于是我在括号里面添加参数，没有任何变化…\n\n并不知道发生了什么。。。。愣住。。。。\n接下来继续分析text\n# 有序字典，按词频从高到低排序 fdist1 = FreqDist(text1) # 选出词频最高的50个词 fdist1.keys()[:50] # 某个词出现的频数 fdist1['whale'] # text1中词频最高的50个单词，进行绘图，输出见下图 fdist1.plot(50, cumulative=True) # text1中只出现过一次的单词 fdist1.hapaxes()\n然后其实只要修改添加一个list就行了\nlist(fdist1.keys())[:50]\n可能是因为版本的缘故，又一次遇到需要list的地方\n\n修改如下\n\n对于机器人对话的命令，发现根本停不下来\n\n…崩溃\n\n好吧，今天的学习到此结束\n每日一吹，咖啡鸡天下第一！","date":"2018年11月08日 17:06:57"}
{"_id":{"$oid":"5d36a8b16734bd8e681d5e0e"},"title":"讲解基本自然语言处理NLP","author":"Clifnich","content":"简介\n前几天用自然语言处理技术学习了一下习主席的十九大报告，发布到朋友圈以后反响比较大，很多同事和朋友都好奇我是怎么做到的；由于学习的算法比较简单，所以我基本上两三句话都给他们解释清楚了。我觉得很多人也会对类似的话题感兴趣，所以这里要写这么篇博文，来document一下我的这个非常简单的自然语言处理程序，给大家揭开一点迷雾。\n原理分析\n这个算法来自斯坦福的抽象编程课，若干年前叫cs106b, 不知道现在还上不上这门课。主要的方法就是对文章中出现的每一个字都建立一个索引，这个索引里包含了所有紧跟在它后面出现的字及其出现的次数，自动生成文章的时候会使用一个seed，不断生成随机数，按照紧跟字出现的概率和随机数来决定下一个字。\n先说索引，索引里包含了所有紧跟在它后面出现的字及其出现的次数。比如说字“同“，我们发现文章里有同学这个词，于是“同“的索引里就有“学“这个字，又发现文章里有同胞这个词，于是“同“的索引里就又有了“胞“这个字。索引不光是紧跟字的集合，也记录了每个紧跟字的出现次数。就拿习主席的报告为例，学习完成后，发现“培“字后面分别紧跟“育“、“养“和“训“三个字，他们的出现次数是11、10和2。\n当我们有了一个种子（seed）的时候，我们可以查看它的紧跟列表，按照概率来选出下一个字。就拿“培“打个比方，后一个字有\n1123\n\\frac{11}{23} 的几率会是“育“字，\n1023\n\\frac{10}{23} 的几率是“养“字，\n223\n\\frac{2}{23} 的几率是训字。选出下一个字以后，这个字又做为新的种子重复上述的过程。\n有趣的是逗号、句号这种标点符号也被当成一个字，所以新生成的文章也会有明显的断句。\n程序实现\n先说我封装的一个重要的类：Token. 我们先来看一下类的UML图：\n\n最底层的类叫做CharacterNOccurrence，这是一个字符（Character）和它的出现次数（int）组成的类，它有一个公有方法叫做addOneOccurrence(), 就是增加一次出现次数。\nToken类aggregate了这个CharacterNOccurrence类，这是对于现实中每个字都有对应的索引的抽象，我把一个字（一个Character对象）和它的索引（List\u003cCharacterNOccurrence\u003e) 封装在同一个对象里，方便使用。\n主程序的学习部分就是读取一个文本文件，为每个字生成一个Token实例，并用一个Map把文字本身和它对应的Token实例存储起来，以便自动写文章的时候使用。Map是一个Map\u003cString, Token\u003e 的形式。\n主程序的书写部分就是hardcode一个种子，到Map当中去找对应Token，按照概率选出下一个字，再重复之前的操作。\n代码在Github . 有问题欢迎提问。","date":"2017年10月22日 23:23:28"}
{"_id":{"$oid":"5d36a8b26734bd8e681d5e11"},"title":"自然语言处理-Stanford中文实体识别","author":"yaoyaoyao2","content":"自然语言处理\n关于斯坦福自然语言处理NLP工具资料收集\n斯坦福分词链接：\nhttps://nlp.stanford.edu/software/segmenter.shtml\nChinese is standardly written withoutspaces between words (as are some other languages). This software will splitChinese text into a sequence of words, defined according to some wordsegmentation standard. It is a Java implementation of the CRF-based Chinese Word Segmenter described in:\n斯坦福做的中文分词是基于条件随机场实现的。\n斯坦福大学自然语言处理组是世界知名的NLP研究小组，他们提供了一系列开源的Java文本分析工具，包括分词器(Word Segmenter)，词性标注工具（Part-Of-Speech Tagger），命名实体识别工具（Named Entity Recognizer），句法分析器（Parser）等，可喜的事，他们还为这些工具训练了相应的中文模型，支持中文文本处理。\n摘抄于：\nhttp://www.52nlp.cn/python自然语言处理实践-在nltk中使用斯坦福中文分词器#more-6763\n使用Stanford NLP工具实现中文命名实体识别\nhttp://m.blog.csdn.net/article/details?id=49497231\n按照上面的链接步骤，也下载了分词器stanford-segmenter-2016-10-31\ndata目录下有两个gz压缩文件，分别是ctb.gz和pku.gz，其中CTB：宾州大学的中国树库训练资料 ，PKU：中国北京大学提供的训练资料。\nNER实体识别在edu.stanford.nlp.ie.crf\njava -mx600m -cp \"*;lib\\*\"edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifierclassifiers/english.all.3class.distsim.crf.ser.gz -textFile sample.txt\njava -mx600m -cp \"*;lib/*\"edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifierclassifiers/english.all.3class.distsim.crf.ser.gz -outputFormat tabbedEntities-textFile sample.txt \u003e sample.tsv\nChinese\nWe also provideChinese models built from the Ontonotes Chinese named entity data. There aretwo models, one using distributional similarity clusters and one without. Theseare designed to be run on word-segmented Chinese. So, if you wantto use these on normal Chinese text, you will first need to run Stanford Word Segmenter orsome other Chinese word segmenter, and then run NER on the output of that!\n3.7.0 Chinesemodels\nstanford-chinese-corenlp-2016-10-31-models.jar\n使用斯坦福中文实体标注之前，必须先完成分词的任务。\n实体识别的Demo\nhttp://nlp.stanford.edu/software/ner-example/NERDemo.java\n参考文章：\nhttp://blog.csdn.net/yangyangrenren/article/details/54709925\n\n\n中文实体识别的代码：\nimport edu.stanford.nlp.ie.AbstractSequenceClassifier; import edu.stanford.nlp.ie.crf.*; import edu.stanford.nlp.io.IOUtils; import edu.stanford.nlp.ling.CoreLabel; import edu.stanford.nlp.ling.CoreAnnotations; import edu.stanford.nlp.sequences.DocumentReaderAndWriter; import edu.stanford.nlp.util.Triple; import java.util.List; /** This is a demo of calling CRFClassifier programmatically. * \u003cp\u003e * Usage: {@code java -mx400m -cp \"*\" NERDemo [serializedClassifier [fileName]] } * \u003cp\u003e * If arguments aren't specified, they default to * classifiers/english.all.3class.distsim.crf.ser.gz and some hardcoded sample text. * If run with arguments, it shows some of the ways to get k-best labelings and * probabilities out with CRFClassifier. If run without arguments, it shows some of * the alternative output formats that you can get. * \u003cp\u003e * To use CRFClassifier from the command line: * \u003c/p\u003e\u003cblockquote\u003e * {@code java -mx400m edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier [classifier] -textFile [file] } * \u003c/blockquote\u003e\u003cp\u003e * Or if the file is already tokenized and one word per line, perhaps in * a tab-separated value format with extra columns for part-of-speech tag, * etc., use the version below (note the 's' instead of the 'x'): * \u003c/p\u003e\u003cblockquote\u003e * {@code java -mx400m edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier [classifier] -testFile [file] } * \u003c/blockquote\u003e * * @author Jenny Finkel * @author Christopher Manning */ public class NERDemo { public static void main(String[] args) throws Exception { String serializedClassifier = \"classifiers/chinese.misc.distsim.crf.ser.gz\"; if (args.length \u003e 0) { serializedClassifier = args[0]; } AbstractSequenceClassifier\u003cCoreLabel\u003e classifier = CRFClassifier.getClassifier(serializedClassifier); /* For either a file to annotate or for the hardcoded text example, this demo file shows several ways to process the input, for teaching purposes. */ if (args.length \u003e 1) { /* For the file, it shows (1) how to run NER on a String, (2) how to get the entities in the String with character offsets, and (3) how to run NER on a whole file (without loading it into a String). */ String fileContents = IOUtils.slurpFile(args[1]); List\u003cList\u003cCoreLabel\u003e\u003e out = classifier.classify(fileContents); for (List\u003cCoreLabel\u003e sentence : out) { for (CoreLabel word : sentence) { System.out.print(word.word() + '/' + word.get(CoreAnnotations.AnswerAnnotation.class) + ' '); } System.out.println(); } System.out.println(\"---\"); out = classifier.classifyFile(args[1]); for (List\u003cCoreLabel\u003e sentence : out) { for (CoreLabel word : sentence) { System.out.print(word.word() + '/' + word.get(CoreAnnotations.AnswerAnnotation.class) + ' '); } System.out.println(); } System.out.println(\"---\"); List\u003cTriple\u003cString, Integer, Integer\u003e\u003e list = classifier.classifyToCharacterOffsets(fileContents); for (Triple\u003cString, Integer, Integer\u003e item : list) { System.out.println(item.first() + \": \" + fileContents.substring(item.second(), item.third())); } System.out.println(\"---\"); System.out.println(\"Ten best entity labelings\"); DocumentReaderAndWriter\u003cCoreLabel\u003e readerAndWriter = classifier.makePlainTextReaderAndWriter(); classifier.classifyAndWriteAnswersKBest(args[1], 10, readerAndWriter); System.out.println(\"---\"); System.out.println(\"Per-token marginalized probabilities\"); classifier.printProbs(args[1], readerAndWriter); // -- This code prints out the first order (token pair) clique probabilities. // -- But that output is a bit overwhelming, so we leave it commented out by default. // System.out.println(\"---\"); // System.out.println(\"First Order Clique Probabilities\"); // ((CRFClassifier) classifier).printFirstOrderProbs(args[1], readerAndWriter); } else { /* For the hard-coded String, it shows how to run it on a single sentence, and how to do this and produce several formats, including slash tags and an inline XML output format. It also shows the full contents of the {@code CoreLabel}s that are constructed by the classifier. And it shows getting out the probabilities of different assignments and an n-best list of classifications with probabilities. */ String[] example = {\"5月 8日 下午 ， 李克强 考察 河南 新乡 封丘县 黄河 滩区 后 ， 随即 在 当地 居民 迁建 指挥部 主持 召开 现场会 ， 专题 研究 河南 、 山东 两 省 黄河 滩区 居民 迁建 工作 。 除 陪同 总理 考察 的 国务院 领导 及 发改委 、 财政部 、 水利部 、 黄河 水利 委员会 、 河南省 负责人 外 ， 山东省 省长 也 专程 赶来 参会 。 窗外 一直 下 着 雨 。 会前 ， 李克强 结束 开封 考察 后 ， 专程 驱车 一 小时 赴 新乡市 封丘县 黄河 滩区 ， 冒雨 踩 着 泥泞 小路 实地 察看 黄河 滩区 ， 并 入户 探望 滩区 居民 。 “ 黄河 滩区 问题 是 多年来 历史 形成 的 ， 现在 到 了 该 解决 的 时候 了 ！ ” 李克强 面色 凝重 地说 ， “ 滩区 迁建 关乎 近 200万 滩区 居民 的 生活 和 发展 ， 也 关系 黄河 的 长治久安 ， 黄河 的 事 是 天下 大 事 ！\" }; for (String str : example) { System.out.println(classifier.classifyToString(str)); } System.out.println(\"---\"); for (String str : example) { // This one puts in spaces and newlines between tokens, so just print not println. System.out.print(classifier.classifyToString(str, \"slashTags\", false)); } System.out.println(\"---\"); for (String str : example) { // This one is best for dealing with the output as a TSV (tab-separated column) file. // The first column gives entities, the second their classes, and the third the remaining text in a document System.out.print(classifier.classifyToString(str, \"tabbedEntities\", false)); } System.out.println(\"---\"); for (String str : example) { System.out.println(classifier.classifyWithInlineXML(str)); } System.out.println(\"---\"); for (String str : example) { System.out.println(classifier.classifyToString(str, \"xml\", true)); } System.out.println(\"---\"); for (String str : example) { System.out.print(classifier.classifyToString(str, \"tsv\", false)); } System.out.println(\"---\"); // This gets out entities with character offsets int j = 0; for (String str : example) { j++; List\u003cTriple\u003cString,Integer,Integer\u003e\u003e triples = classifier.classifyToCharacterOffsets(str); for (Triple\u003cString,Integer,Integer\u003e trip : triples) { System.out.printf(\"%s over character offsets [%d, %d) in sentence %d.%n\", trip.first(), trip.second(), trip.third, j); } } System.out.println(\"---\"); // This prints out all the details of what is stored for each token int i=0; for (String str : example) { for (List\u003cCoreLabel\u003e lcl : classifier.classify(str)) { for (CoreLabel cl : lcl) { System.out.print(i++ + \": \"); System.out.println(cl.toShorterString()); } } } System.out.println(\"---\"); } } }\n\n\n结果图：","date":"2017年05月12日 14:07:14"}
{"_id":{"$oid":"5d36a8b36734bd8e681d5e15"},"title":"Python自然语言处理工具汇总","author":"搬砖小工053","content":"Python 自然语言处理（NLP）工具汇总\nNLTK\n简介：\nNLTK 在使用 Python 处理自然语言的工具中处于领先的地位。它提供了 WordNet 这种方便处理词汇资源的接口，以及分类、分词、词干提取、标注、语法分析、语义推理等类库。\n网站：\nNatural Language Toolkit\n安装：\n安装 NLTK:\n[root@master ~]# pip install nltk Collecting nltk Downloading nltk-3.2.1.tar.gz (1.1MB) 100% |████████████████████████████████| 1.1MB 664kB/s Installing collected packages: nltk Running setup.py install for nltk ... done Successfully installed nltk-3.2.1\n注意事项：\n安装完以后还要下载nltk语料库才可以使用,下载的是压缩文件,需要解压到nltk_data下面。目录结构如下：\nzang@ZANG-PC D:\\nltk_data \u003e ls -al total 44 drwxrwx---+ 1 Administrators None 0 Oct 25 2015 . drwxrwx---+ 1 SYSTEM SYSTEM 0 May 30 10:55 .. drwxrwx---+ 1 Administrators None 0 Oct 25 2015 chunkers drwxrwx---+ 1 Administrators None 0 Oct 25 2015 corpora drwxrwx---+ 1 Administrators None 0 Oct 25 2015 grammers drwxrwx---+ 1 Administrators None 0 Oct 25 2015 help drwxrwx---+ 1 Administrators None 0 Oct 25 2015 stemmers drwxrwx---+ 1 Administrators None 0 Oct 25 2015 taggers drwxrwx---+ 1 Administrators None 0 Oct 25 2015 tokenizers\nPattern\n简介：\nPattern是基于web的Python挖掘模块，包含如下工具：\n* 数据挖掘：Web服务接口(Google,Twitter,Wikipedia),网络爬虫,HTML DOM 解析。\n* 自然语言处理：POS词性标注,n-gram搜索,情感分析,词云。\n* 机器学习：向量空间模型(VSM),聚类,分类(KNN,SVM,Perceptron)。\n* 网络分析：图中心和可视化。\n网站：\nGitHub主页\n安装:\n[root@master ~]# pip install pattern Collecting pattern Downloading pattern-2.6.zip (24.6MB) 100% |████████████████████████████████| 24.6MB 43kB/s Installing collected packages: pattern Running setup.py install for pattern ... done Successfully installed pattern-2.6 [root@master ~]#\nTextBlob\n简介：\nTextBlob 是基于NLTK和pattern的工具, 有两者的特性。如下：\n名词短语提前\nPOS标注\n情感分析\n分类 (Naive Bayes, Decision Tree)\n谷歌翻译\n分词和分句\n词频和短语频率统计\n句法解析\nn-grams模型\n词型转换和词干提取\n拼写校正\n通过词云整合添加新的语言和模型\n网站：\nTextBlob: Simplified Text Processing\n安装：\n[root@master ~]# pip install -U textblob Collecting textblob Downloading textblob-0.11.1-py2.py3-none-any.whl (634kB) 100% |████████████████████████████████| 634kB 1.1MB/s Requirement already up-to-date: nltk\u003e=3.1 in /usr/lib/python2.7/site-packages (from textblob) Installing collected packages: textblob Successfully installed textblob-0.11.1 [root@master ~]# python -m textblob.download_corpora [nltk_data] Downloading package brown to /root/nltk_data... [nltk_data] Unzipping corpora/brown.zip. [nltk_data] Downloading package punkt to /root/nltk_data... [nltk_data] Unzipping tokenizers/punkt.zip. [nltk_data] Downloading package wordnet to /root/nltk_data... [nltk_data] Unzipping corpora/wordnet.zip. [nltk_data] Downloading package averaged_perceptron_tagger to [nltk_data] /root/nltk_data... [nltk_data] Unzipping taggers/averaged_perceptron_tagger.zip. [nltk_data] Downloading package conll2000 to /root/nltk_data... [nltk_data] Unzipping corpora/conll2000.zip. [nltk_data] Downloading package movie_reviews to /root/nltk_data... [nltk_data] Unzipping corpora/movie_reviews.zip. Finished.\nGensim\n简介：\nGensim 是一个 Python 库，用于对大型语料库进行主题建模、文件索引、相似度检索等。它可以处理大于内存的输入数据。作者说它是“纯文本上无监督的语义建模最健壮、高效、易用的软件。”\n网站：\nGensim HomePage\nGitHub - piskvorky/gensim: Topic Modelling for Humans\n安装：\n[root@master ~]# pip install -U gensim Collecting gensim Downloading gensim-0.12.4.tar.gz (2.4MB) 100% |████████████████████████████████| 2.4MB 358kB/s Collecting numpy\u003e=1.3 (from gensim) Downloading numpy-1.11.0-cp27-cp27mu-manylinux1_x86_64.whl (15.3MB) 100% |████████████████████████████████| 15.3MB 66kB/s Collecting scipy\u003e=0.7.0 (from gensim) Downloading scipy-0.17.1-cp27-cp27mu-manylinux1_x86_64.whl (39.5MB) 100% |████████████████████████████████| 39.5MB 27kB/s Requirement already up-to-date: six\u003e=1.5.0 in /usr/lib/python2.7/site-packages/six-1.10.0-py2.7.egg (from gensim) Collecting smart_open\u003e=1.2.1 (from gensim) Downloading smart_open-1.3.3.tar.gz Collecting boto\u003e=2.32 (from smart_open\u003e=1.2.1-\u003egensim) Downloading boto-2.40.0-py2.py3-none-any.whl (1.3MB) 100% |████████████████████████████████| 1.4MB 634kB/s Requirement already up-to-date: bz2file in /usr/lib/python2.7/site-packages (from smart_open\u003e=1.2.1-\u003egensim) Collecting requests (from smart_open\u003e=1.2.1-\u003egensim) Downloading requests-2.10.0-py2.py3-none-any.whl (506kB) 100% |████████████████████████████████| 512kB 1.4MB/s Installing collected packages: numpy, scipy, boto, requests, smart-open, gensim Found existing installation: numpy 1.10.1 Uninstalling numpy-1.10.1: Successfully uninstalled numpy-1.10.1 Found existing installation: scipy 0.12.1 DEPRECATION: Uninstalling a distutils installed project (scipy) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project. Uninstalling scipy-0.12.1: Successfully uninstalled scipy-0.12.1 Found existing installation: boto 2.38.0 Uninstalling boto-2.38.0: Successfully uninstalled boto-2.38.0 Found existing installation: requests 2.8.1 Uninstalling requests-2.8.1: Successfully uninstalled requests-2.8.1 Found existing installation: smart-open 1.3.1 Uninstalling smart-open-1.3.1: Successfully uninstalled smart-open-1.3.1 Running setup.py install for smart-open ... done Found existing installation: gensim 0.12.3 Uninstalling gensim-0.12.3: Successfully uninstalled gensim-0.12.3 Running setup.py install for gensim ... done Successfully installed boto-2.40.0 gensim-0.12.4 numpy-1.11.0 requests-2.6.0 scipy-0.17.1 smart-open-1.3.3\nPyNLPI\n简介：\n它的全称是：Python 自然语言处理库（Python Natural Language Processing Library，音发作: pineapple） 是一个用于自然语言处理任务库。它集合了各种独立或松散互相关的，那些常见的、不常见的、对NLP 任务有用的模块。PyNLPI 可以用来处理 N 元搜索，计算频率表和分布，建立语言模型。它还可以处理向优先队列这种更加复杂的数据结构，或者像 Beam 搜索这种更加复杂的算法。\n网站：\nGithub\nPyNLPI HomePage\n安装：\n从Github上下载源码，解压以后编译安装。\n[root@master pynlpl-master]# python setup.py install Preparing build running install running bdist_egg running egg_info creating PyNLPl.egg-info writing requirements to PyNLPl.egg-info/requires.txt writing PyNLPl.egg-info/PKG-INFO writing top-level names to PyNLPl.egg-info/top_level.txt writing dependency_links to PyNLPl.egg-info/dependency_links.txt writing manifest file 'PyNLPl.egg-info/SOURCES.txt' reading manifest file 'PyNLPl.egg-info/SOURCES.txt' writing manifest file 'PyNLPl.egg-info/SOURCES.txt' installing library code to build/bdist.linux-x86_64/egg running install_lib running build_py creating build creating build/lib creating build/lib/pynlpl copying pynlpl/tagger.py -\u003e build/lib/pynlpl ...... byte-compiling build/bdist.linux-x86_64/egg/pynlpl/__init__.py to __init__.pyc byte-compiling build/bdist.linux-x86_64/egg/pynlpl/mt/__init__.py to __init__.pyc byte-compiling build/bdist.linux-x86_64/egg/pynlpl/mt/wordalign.py to wordalign.pyc byte-compiling build/bdist.linux-x86_64/egg/pynlpl/statistics.py to statistics.pyc creating build/bdist.linux-x86_64/egg/EGG-INFO copying PyNLPl.egg-info/PKG-INFO -\u003e build/bdist.linux-x86_64/egg/EGG-INFO copying PyNLPl.egg-info/SOURCES.txt -\u003e build/bdist.linux-x86_64/egg/EGG-INFO copying PyNLPl.egg-info/dependency_links.txt -\u003e build/bdist.linux-x86_64/egg/EGG-INFO copying PyNLPl.egg-info/not-zip-safe -\u003e build/bdist.linux-x86_64/egg/EGG-INFO copying PyNLPl.egg-info/requires.txt -\u003e build/bdist.linux-x86_64/egg/EGG-INFO copying PyNLPl.egg-info/top_level.txt -\u003e build/bdist.linux-x86_64/egg/EGG-INFO creating dist creating 'dist/PyNLPl-0.9.2-py2.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it removing 'build/bdist.linux-x86_64/egg' (and everything under it) Processing PyNLPl-0.9.2-py2.7.egg creating /usr/lib/python2.7/site-packages/PyNLPl-0.9.2-py2.7.egg Extracting PyNLPl-0.9.2-py2.7.egg to /usr/lib/python2.7/site-packages Adding PyNLPl 0.9.2 to easy-install.pth file Installed /usr/lib/python2.7/site-packages/PyNLPl-0.9.2-py2.7.egg Processing dependencies for PyNLPl==0.9.2 Searching for httplib2\u003e=0.6 Reading https://pypi.python.org/simple/httplib2/ Best match: httplib2 0.9.2 Downloading https://pypi.python.org/packages/ff/a9/5751cdf17a70ea89f6dde23ceb1705bfb638fd8cee00f845308bf8d26397/httplib2-0.9.2.tar.gz#md5=bd1b1445b3b2dfa7276b09b1a07b7f0e Processing httplib2-0.9.2.tar.gz Writing /tmp/easy_install-G32Vg8/httplib2-0.9.2/setup.cfg Running httplib2-0.9.2/setup.py -q bdist_egg --dist-dir /tmp/easy_install-G32Vg8/httplib2-0.9.2/egg-dist-tmp-IgKi70 zip_safe flag not set; analyzing archive contents... httplib2.__init__: module references __file__ Adding httplib2 0.9.2 to easy-install.pth file Installed /usr/lib/python2.7/site-packages/httplib2-0.9.2-py2.7.egg Searching for numpy==1.11.0 Best match: numpy 1.11.0 Adding numpy 1.11.0 to easy-install.pth file Using /usr/lib64/python2.7/site-packages Searching for lxml==3.2.1 Best match: lxml 3.2.1 Adding lxml 3.2.1 to easy-install.pth file Using /usr/lib64/python2.7/site-packages Finished processing dependencies for PyNLPl==0.9.2\nspaCy\n简介：\n这是一个商业的开源软件。结合了Python 和Cython 优异的 NLP 工具。是快速的，最先进的自然语言处理工具。\n网站：\nHomePage\nGitHub\n安装：\n[root@master pynlpl-master]# pip install spacy Collecting spacy Downloading spacy-0.101.0-cp27-cp27mu-manylinux1_x86_64.whl (5.7MB) 100% |████████████████████████████████| 5.7MB 161kB/s Collecting thinc\u003c5.1.0,\u003e=5.0.0 (from spacy) Downloading thinc-5.0.8-cp27-cp27mu-manylinux1_x86_64.whl (1.4MB) 100% |████████████████████████████████| 1.4MB 287kB/s Collecting murmurhash\u003c0.27,\u003e=0.26 (from spacy) Downloading murmurhash-0.26.4-cp27-cp27mu-manylinux1_x86_64.whl Collecting cloudpickle (from spacy) Downloading cloudpickle-0.2.1-py2.py3-none-any.whl Collecting plac (from spacy) Downloading plac-0.9.1.tar.gz (151kB) 100% |████████████████████████████████| 153kB 3.2MB/s Requirement already satisfied (use --upgrade to upgrade): numpy\u003e=1.7 in /usr/lib64/python2.7/site-packages (from spacy) Requirement already satisfied (use --upgrade to upgrade): six in /usr/lib/python2.7/site-packages/six-1.10.0-py2.7.egg (from spacy) Collecting cymem\u003c1.32,\u003e=1.30 (from spacy) Downloading cymem-1.31.2-cp27-cp27mu-manylinux1_x86_64.whl (66kB) 100% |████████████████████████████████| 71kB 4.3MB/s Collecting preshed\u003c0.47,\u003e=0.46.1 (from spacy) Downloading preshed-0.46.4-cp27-cp27mu-manylinux1_x86_64.whl (223kB) 100% |████████████████████████████████| 225kB 2.4MB/s Collecting sputnik\u003c0.10.0,\u003e=0.9.2 (from spacy) Downloading sputnik-0.9.3-py2.py3-none-any.whl Collecting semver (from sputnik\u003c0.10.0,\u003e=0.9.2-\u003espacy) Downloading semver-2.5.0.tar.gz Installing collected packages: murmurhash, cymem, preshed, thinc, cloudpickle, plac, semver, sputnik, spacy Running setup.py install for plac ... done Running setup.py install for semver ... done Successfully installed cloudpickle-0.2.1 cymem-1.31.2 murmurhash-0.26.4 plac-0.9.1 preshed-0.46.4 semver-2.5.0 spacy-0.101.0 sputnik-0.9.3 thinc-5.0.8\nPolyglot\n简介：\nPolyglot 支持大规模多语言应用程序的处理。它支持165种语言的分词，196中语言的辨识，40种语言的专有名词识别，16种语言的词性标注，136种语言的情感分析，137种语言的嵌入，135种语言的形态分析，以及69种语言的翻译。特性如下：\nTokenization (165 Languages)\nLanguage detection (196 Languages)\nNamed Entity Recognition (40 Languages)\nPart of Speech Tagging (16 Languages)\nSentiment Analysis (136 Languages)\nWord Embeddings (137 Languages)\nMorphological analysis (135 Languages)\nTransliteration (69 Languages)\n网站：\nGithub\n安装：\n[root@master pynlpl-master]# pip install polyglot Collecting polyglot Downloading polyglot-15.10.03-py2.py3-none-any.whl (54kB) 100% |████████████████████████████████| 61kB 153kB/s Collecting pycld2\u003e=0.3 (from polyglot) Downloading pycld2-0.31.tar.gz (14.3MB) 100% |████████████████████████████████| 14.3MB 71kB/s Collecting wheel\u003e=0.23.0 (from polyglot) Downloading wheel-0.29.0-py2.py3-none-any.whl (66kB) 100% |████████████████████████████████| 71kB 4.2MB/s Collecting futures\u003e=2.1.6 (from polyglot) Downloading futures-3.0.5-py2-none-any.whl Requirement already satisfied (use --upgrade to upgrade): six\u003e=1.7.3 in /usr/lib/python2.7/site-packages/six-1.10.0-py2.7.egg (from polyglot) Collecting PyICU\u003e=1.8 (from polyglot) Downloading PyICU-1.9.3.tar.gz (179kB) 100% |████████████████████████████████| 184kB 2.9MB/s Collecting morfessor\u003e=2.0.2a1 (from polyglot) Downloading Morfessor-2.0.2alpha3.tar.gz Installing collected packages: pycld2, wheel, futures, PyICU, morfessor, polyglot Running setup.py install for pycld2 ... done Running setup.py install for PyICU ... done Running setup.py install for morfessor ... done Successfully installed PyICU-1.9.3 futures-3.0.5 morfessor-2.0.2a3 polyglot-15.10.3 pycld2-0.31 wheel-0.29.0\nMontyLingua\n简介：\nMontyLingua 是一个免费的、功能强大的、端到端的英文处理工具。在 MontyLingua 输入原始英文文本 ，输出就会得到这段文本的语义解释。它适用于信息检索和提取，请求处理，问答系统。从英文文本中，它能提取出主动宾元组，形容词、名词和动词短语，人名、地名、事件，日期和时间等语义信息。\n网站：\nHomePage\nGithub\n安装：\n无\nUsage\nWebservice\npython server.py\nThe webservice runs on port 8001 at /service by default. For parameters etc see the NIF spec.\nTherefore you can curl your query like this\ncurl “http://localhost:8001/service?nif=true\u0026input-type=text\u0026input=This%20is%20a%20city%20called%20Berlin.”\nor simply use your browser to query the target.\nConsole\npython nif.py\nBut this method is mainly for debugging purposes and supports only hardcoded options.\nBLLIP Parser\n简介：\nBLLIP Parser（也叫做 Charniak-Johnson parser）是一个集成了生成成分分析器和最大熵排序的统计自然语言分析器。它包括命令行和python接口。\n网站：\nGitHub\nHomePage\n安装：\n[root@master pynlpl-master]# pip install --user bllipparser Collecting bllipparser Downloading bllipparser-2015.12.3.tar.gz (548kB) 100% |████████████████████████████████| 552kB 1.2MB/s Requirement already satisfied (use --upgrade to upgrade): six in /usr/lib/python2.7/site-packages/six-1.10.0-py2.7.egg (from bllipparser) Building wheels for collected packages: bllipparser Running setup.py bdist_wheel for bllipparser ... done Stored in directory: /root/.cache/pip/wheels/6f/7a/d8/037a4aa0fa275f43e1129008eb7834dc8522ef158d2e96534b Successfully built bllipparser Installing collected packages: bllipparser Successfully installed bllipparser\nQuepy\n简介：\nQuepy 是一个 Python 框架，提供了将自然语言问题转换成为数据库查询语言中的查询。它可以方便地自定义自然语言中不同类型的问题和数据库查询。所以，通过 Quepy，仅仅修改几行代码，就可以构建你自己的自然语言查询数据库系统。\n网站：\nGitHub - machinalis/quepy: A python framework to transform natural language questions to queries in a database query language.\nQuepy: A Python framework to transform natural language questions to queries.\n安装\n[root@master pynlpl-master]# pip install quepy Collecting quepy Downloading quepy-0.2.tar.gz (42kB) 100% |████████████████████████████████| 51kB 128kB/s Collecting refo (from quepy) Downloading REfO-0.13.tar.gz Requirement already satisfied (use --upgrade to upgrade): nltk in /usr/lib/python2.7/site-packages (from quepy) Collecting SPARQLWrapper (from quepy) Downloading SPARQLWrapper-1.7.6.zip Collecting rdflib\u003e=4.0 (from SPARQLWrapper-\u003equepy) Downloading rdflib-4.2.1.tar.gz (889kB) 100% |████████████████████████████████| 890kB 823kB/s Collecting keepalive\u003e=0.5 (from SPARQLWrapper-\u003equepy) Downloading keepalive-0.5.zip Collecting isodate (from rdflib\u003e=4.0-\u003eSPARQLWrapper-\u003equepy) Downloading isodate-0.5.4.tar.gz Requirement already satisfied (use --upgrade to upgrade): pyparsing in /usr/lib/python2.7/site-packages (from rdflib\u003e=4.0-\u003eSPARQLWrapper-\u003equepy) Collecting html5lib (from rdflib\u003e=4.0-\u003eSPARQLWrapper-\u003equepy) Downloading html5lib-0.9999999.tar.gz (889kB) 100% |████████████████████████████████| 890kB 854kB/s Requirement already satisfied (use --upgrade to upgrade): six in /usr/lib/python2.7/site-packages/six-1.10.0-py2.7.egg (from html5lib-\u003erdflib\u003e=4.0-\u003eSPARQLWrapper-\u003equepy) Building wheels for collected packages: quepy, refo, SPARQLWrapper, rdflib, keepalive, isodate, html5lib Running setup.py bdist_wheel for quepy ... done Stored in directory: /root/.cache/pip/wheels/c8/04/bf/495b88a68aa5c1e9dd1629b09ab70261651cf517d1b1c27464 Running setup.py bdist_wheel for refo ... done Stored in directory: /root/.cache/pip/wheels/76/97/81/825976cf0a2b9ad759bbec13a649264938dffb52dfd56ac6c8 Running setup.py bdist_wheel for SPARQLWrapper ... done Stored in directory: /root/.cache/pip/wheels/50/fe/25/be6e98daa4f576494df2a18d5e86a182e3d7e0735d062cc984 Running setup.py bdist_wheel for rdflib ... done Stored in directory: /root/.cache/pip/wheels/fb/93/10/4f8a3e95937d8db410a490fa235bd95e0e0d41b5f6274b20e5 Running setup.py bdist_wheel for keepalive ... done Stored in directory: /root/.cache/pip/wheels/16/4f/c1/121ddff67b131a371b66d682feefac055fbdbb9569bfde5c51 Running setup.py bdist_wheel for isodate ... done Stored in directory: /root/.cache/pip/wheels/61/c0/d2/6b4a10c222ba9261ab9872a8f05d471652962284e8c677e5e7 Running setup.py bdist_wheel for html5lib ... done Stored in directory: /root/.cache/pip/wheels/6f/85/6c/56b8e1292c6214c4eb73b9dda50f53e8e977bf65989373c962 Successfully built quepy refo SPARQLWrapper rdflib keepalive isodate html5lib Installing collected packages: refo, isodate, html5lib, rdflib, keepalive, SPARQLWrapper, quepy Successfully installed SPARQLWrapper-1.7.6 html5lib-0.9999999 isodate-0.5.4 keepalive-0.5 quepy-0.2 rdflib-4.2.1 refo-0.13\nMBSP\n简介：\nMBSP is a text analysis system based on the TiMBL and MBT memory based learning applications developed at CLiPS and ILK. It provides tools for Tokenization and Sentence Splitting, Part of Speech Tagging, Chunking, Lemmatization, Relation Finding and Prepositional Phrase Attachment.\nThe general English version of MBSP has been trained on data from the Wall Street Journal corpus.\n网站：\nHomePage\nGithub\n安装：\n下载，解压，编译安装：\n[root@master MBSP]# python setup.py install .....编译的信息..... .....2分钟左右.....\n参考：\n李岩知乎回答：目前常用的自然语言处理开源项目/开发包有哪些？\n数盟：用Python做自然语言处理必知的八个工具","date":"2016年07月04日 19:24:11"}
{"_id":{"$oid":"5d36a8b46734bd8e681d5e17"},"title":"RNN在自然语言处理中的应用","author":"clayanddev","content":"前言\n跳过废话，直接看正文\n循环神经网络(Recurrent Neural Networks，RNNs)目前在自然语言处理领域中的格外受欢迎。\n很多简单的自然语言处理任务可以直接由RNN来完成。\n这里列出几种RNN在自然语言处理领域的应用算法，以供参考。（目前只列出了参考代码，后续会补上相关说明。）\n正文\n中文分词算法\n具体代码参考github\n命名实体识别算法\n具体代码参考github\n文本生成算法\n这里内容比较多，详见后一篇博客。\n后记\n分词算法的关键有两个，算法和词典。缺了其中一个，效果都不会太好，可惜现在互联网上公布的标注语料库太少了，希望将来能做一份贡献吧。\n语言处理领域中的很多问题（分词、命名实体识别等）都可以转换为序列标注问题，而序列标注这样的上下文关系较紧密的问题由RNN来处理再适合不过了。","date":"2016年12月31日 14:38:41"}
{"_id":{"$oid":"5d36a8b46734bd8e681d5e1a"},"title":"各种开源NLP自然语言处理工具集锦","author":"樱夕夕","content":"开源NLP自然语言处理工具集锦\n现状\n首先看看目前常用的分词系统：\nNo\nName\nFeature\n1\nBosonNLP\nhttp://bosonnlp.com/\n2\nIKAnalyzer\nhttp://git.oschina.net/wltea/IK-Analyzer-2012FF\n3\nNLPIR\nhttp://ictclas.nlpir.org/\n4\nSCWS\nhttp://www.xunsearch.com/scws/\n5\n结巴分词\nhttp://www.oschina.net/p/jieba\n6\n盘古分词\nhttp://pangusegment.codeplex.com/\n7\n庖丁解牛\nhttp://zengzhaoshuai.iteye.com/blog/986314\n8\n搜狗分词\nhttp://www.sogou.com/labs/webservice/\n9\n腾讯文智\nhttp://nlp.qq.com/\n10\n新浪云\nhttp://www.sinacloud.com/doc/sae/php/storage.html\n11\n语言云\nhttp://www.ltp-cloud.com/demo/\n博主也是刚开始接触分词，使用的不多，目前看来市场上用的比较多的是中科院的NLPIR分词系统，大家可以在官网上下载试用（貌似是一个月 (～￣▽￣)），然后就被无情的提示license过期。这时只需要在git上下载新的license替换旧license就好啦~\nps.每次更新license有效期一个月，所以大家勤动手吧！\nBosonNLP\n和大多数的NLP工具一样，玻森的处理能力大概就以上几种。\n分词与词性标注\n大家可以点击链接浏览词性分析的文档。博主摘取部分关键信息如下：\n1）分词和词性标注联合枚举的方法\n2）开放API接口\n3）基于序列标注实现的，以词为单位对句子进行词边界和词性的标注，即基于字符串匹配的方法。\n4）结合上下文识别生词\n5）加入了对url、email等特殊词的识别\n6）对词性标签进行调整和优化，实现了更细的标签划分（22个大类，69个标签）\n7）对训练语料进行修正\n8）加入繁简转化，可以处理繁体中文或者繁简混合的中文句子\n9）多种分词选项：\n空格保留选项\n新词枚举强度选项\n繁简转换选项\n特殊字符转换选项\n下面看一下玻森的免费使用次数：\n可见除了词性分析比较多以外，其他的均为500次。(；′⌒`)\n我们这里额外讲解一下rest api——表述性状态转移（Representational State Transfer），它是一种设计风格而非标准，通常基于使用http、uri、xml、以及html这些现有的广泛流行的协议和标准。\n想深入了解的童鞋可以查看下面的链接：\nRest API开发学习笔记——by spring yang\nRest——维基百科\n情感分析\n这是情感分析返回的结果，可见我们查询了两句话，每句话的前面是正面概率，后面是消极概率。\n\n这里提供一个curl的下载链接：\nCURL官方网址\n新闻分类\n\n时间转换\n这个在博主看来还是很有意思的，它可以将中文描述的时间短语转换为三种标准的时间格式字符串—：\n1) 时间点（timestamp，表示某一具体时间时间描述）;\n2) 时间量（timedelta，表示时间的增量的时间描述）;\n3)时间区间（timespan，大于一天的有具体起始和结束时间点的时间描述）\n新闻摘要\n摘要系统提供4个输入选项：\n- 新闻标题\n- 新闻正文\n- 字数限制\n- 是否为严格字数限制\n文本聚类引擎\n看到可以文本聚类的时候，博主是很激动的，因为毕设就一直在折腾这个！\n文档中说：该引擎能够对给定的文本进行话题聚类，将语义上相似的文章归为一类\n\nIKAnalyzer\n点击 IKAnalyzer 链接，可以看到最新的版本也是2012年的，实现的功能比较单一，感兴趣的童鞋可以看看。\n总之，玻森使用比较方便，个人认为界面简介明了，易于初学者使用。","date":"2016年07月19日 17:05:28"}
{"_id":{"$oid":"5d36a8b56734bd8e681d5e1c"},"title":"自然语言处理（一）","author":"cimoko","content":"1、计算jieba和thula的P，R，F值。基于文本‘express.txt’，标准文本是人工切分。\n基本计算公式：\n精度（Precision）、召回率（Recall）、F值（F-mesure）。\nN ：标准分割的单词数e ：分词器错误标注的单词数c ：分词器正确标注的单词数\nP = c/N     R = c/(c+e)   F = 2*R*P/(R+P)\n结巴分词的使用函数：\ns1 = list(jieba.cut(f))\n清华分词的使用函数：\nthu1=thulac.thulac(seg_only=True) s_2 = thu1.cut(f, text=True)\n\n\n编程思路\n读取标准文本，建立词典（数据类型为list），去除文本中的标点符号，计算n值\n通过jieba进行分词，jieba分词后可直接生成list，删除文本中的标点符号，将jieba分词的结果与词典进行对比，遍历jieba分词结果中的每个词，若词典中有，则c+1，若没有，则e+1，最后计算P、R、F值，输出结果\n通过清华分词，清华分词后的结果为一个字符串，用空格隔开，先通过一个循环将字符串中的词分割开，存入list中，再删除list中的标点符后，其余步骤与jieba分词相同\n将P、R、F的计算过程写成函数，简化代码\n结果\n\n\n\n\n\n\n代码\n#!/usr/bin/env python # -*- coding:utf-8 -*- #Date:2018/3/26 19:07 #__Author__:cimoko #File Name:lesson_3_1.py import jieba import re import thulac def P_R_F(n, c, e): R = round(c / n, 4) P = round(c / (c + e), 4) F = round(2*P*R / (P + R), 4) print(\"精度P为：\", P * 100, '%') print(\"召回率R为：\", R * 100, '%') print(\"F值为：\", F * 100, '%') return P, R, F f = str(open(\"express.txt\").readlines()) #print(f) #标准文本 raw = open('express_cut.txt').readlines() d=[re.split(r' |\\n',w)[0] for w in raw] dict = [] for w in d: if w == r'，' or w == r'。' or w == r'（'or w == r'）': pass else: dict.append(w) print('*************标准分词文本*************') print(dict) n = len(dict) #结巴 s1 = list(jieba.cut(f)) s_jieba = [] for w in s1: if w == r'，' or w == r'。' or w == r'（'or w == r'）'or w == r'['or w == r']'or w == r\"'\": pass else: s_jieba.append(w) #print(s_jieba) e_jieba = 0 c_jieba = 0 for i in range(len(s_jieba)): if s_jieba[i] in dict: c_jieba += 1 else: e_jieba += 1 print('*************结巴分词结果*************') print(s_jieba) print('c:',c_jieba) print('e:',e_jieba) print('n:',n) P_R_F(n, c_jieba, e_jieba) #清华 thu1=thulac.thulac(seg_only=True) s_2 = thu1.cut(f, text=True) s2=[] #print(s_2) s_qinghua = [] a=0 for i in range(len(s_2)): if s_2[i] == ' ': s2.append(s_2[a:i]) a = i+1 else: continue for w in s2: if w == r'，' or w == r'。' or w == r'（'or w == r'）'or w == r'['or w == r']'or w == r\"'\": pass else: s_qinghua.append(w) #print(s_qinghua) e_qinghua = 0 c_qinghua = 0 for i in range(len(s_qinghua)): if s_qinghua[i] in dict: c_qinghua += 1 else: e_qinghua += 1 print('*************清华分词结果*************') print(s_qinghua) print('c:',c_qinghua) print('e:',e_qinghua) print('n:',n) P_R_F(n, c_qinghua, e_qinghua)","date":"2017年07月14日 18:33:23","data":"2018年03月30日 08:52:53"}
{"_id":{"$oid":"5d36a8b56734bd8e681d5e1f"},"title":"自然语言处理学习路线图","author":"忧郁一休","content":"路线图请戳这里","date":"2017年01月22日 15:52:05"}
{"_id":{"$oid":"5d36a8b66734bd8e681d5e22"},"title":"自然语言处理发展历程自我总结","author":"邵可佳","content":"自然语言处理(NLP)历史悠久，从上个世纪初，便有人开始提出自然语言相关的规律和假设，但本人阅读了若干自然语言相关的书籍后，发现自然语言处理的方法论在长达近1个世纪的时间内并无半点实质上的进展。\n自然语言处理的方法体系目前大致可分为两个方向：\n1.形式化语言处理方向\n这个方向吸引了众多学者，体系非常庞杂，其中诞生了很多处理主义，但都并未有革命性的变化，基本上属于盲人摸象，其中就包括了如下理论：范畴语法、语言串分析、语言集合论、有限状态语法、短语结构语法、线图分析、汉字结构、左结合、合一运算、依存配价、格语法、词汇模型……\n不一一列举了，防止被绕晕，其实都是文字概念上的变化，很多概念都有重复的嫌疑，总结一下，就是基于语言规则的形式化模型，各有各的细微变化，但都没有从根本上解决问题，用人力可以实现有限状态机的“有限度的”智能。\n2.数字化语言处理方向\n这个方向似乎才是沿着科学的道路在前进，但发展似乎也很慢，主要包括如下理论：概率语法、Bayes动态规划、HMM、CRF、LSTM、CNN，这些方法将语言看作数字信号，使用概率论的方法对其处理，但也没有真正实现语义理解。\n\n\n针对以上两个方向，我个人认为，数字化语言处理才是正确的，但对形式化语言处理的认识越深，才能更好的设计自然语言处理模型，现阶段想要做出自动学习语言并生成语言认知模型还比较困难，但形式化方向上很多先驱提出的算法、语言规律和语言现象，有助于网络结构和参数的设计。","date":"2017年06月04日 23:14:56"}
{"_id":{"$oid":"5d36a8b86734bd8e681d5e26"},"title":"入行 AI 必看的《中文自然语言处理入门》","author":"GitChat的博客","content":"限时活动 | 入行 AI 必看《中文自然语言处理入门》\n作为\nAI 初学者\n，我们时常面临这样的尴尬：\n市面上自然语言处理内容大抵为英文；\n中文的处理比英文复杂的多，网上的相关资料少之又少；\n国内纯中文自然语言处理书籍只有理论方面的，却在实战方面比较空缺。\n而我们的达人课《中文自然语言处理入门》专门应对这些困境：\n21 节精品内容带你边学边实战\n6 个极简案例快速掌握基本能力\n9 步制作自己的中文聊天机器人\nNeo4j 从入门到构建知识图谱\n原价 39 元，限时促销，只要3.99！！！\n赶快扫码领取你的专属学习福利带你重新认识 NLP！","date":"2019年05月08日 17:06:32"}
{"_id":{"$oid":"5d36a8b86734bd8e681d5e28"},"title":"统计自然语言处理（统计推理：稀疏数据集上的n元语法模型）","author":"continueOo","content":"概述\n统计自然语言处理的目的就是针对自然语言领域进行统计推理。作为一个常用的统计估计的例子，我们将考察经典建模问题，即当前词预测下一个词。词汇预测任务是一项技术可以解决的简单明了的问题。\nBins：构造等价类\n利用历史词汇预测词汇，我们构造这样一个模型。模型中所有历史都是前n-1个已经出现的词，那么我们就有一个（n-1）阶马尔可夫模型，或者称N元语法模型。随着n的增加和词表数量的增加，我们把数据划分到太多的类别中，有大量的参数要去估计。所以有一些方法如“词干化”来减小词语表数量，使用2-3元语言模型来预测等。另外还有很多模型能比较好的进行预测，比如我们可以想象如果我们知道句子的主谓宾等一节结构，我们可以基于谓语来识别下一个词。但这里只介绍n元语法模型。\n构建n元语法模型\n对于n元语法模型的例子，我们感兴趣的是概率P（w1…wn） 和预测任务P(wn | w1….wn-1)\n\n我们可以利用MLE，最大似然估计去预测。\n\n这个里面存在的问题就是，对于没有出现的n元组，我们统统给了0概率，这个问题是普遍存在的，没有如此大的一个数据集能让我们满足不出现稀疏的情况。虽然有些办法试图去解决这个问题，比如我们动态调整n的大小，然后在超大的数据集上去跑。但是终究这些方法是不完备的。我们需要尝试平滑的去处理那些没有在历史中出现的情况，并且给这些情况赋予一定的概率。\nLaplace法则 Lidstone法则 和 Jeffreys-Perks 法则\n\n这种处理方式常常被非正式的称为加1法，它把一小部分概率有效地转移到了未知事件上。这里的假设是有统一的先验证概率（每个n元组都有相同的可能性），事实上就是一个贝叶斯估计。（个人觉得这个假设不太成立，但是这是一种平滑的方法吧）\n\n这些方法有一些缺点，如在预测句子中概率都被打了折扣（我认为相对比较而言，这个折扣关系不是那么大）。并且结果证明“差的上下文不如没有”。这里可能是平滑时，将0概率比低出现的更大，导致了这种情况。这种建模是可以对概率估计进行排序的。下面放一张图解释一下相对性问题。证明该建模概率折扣与相对有效性。\n\n留存估计\n这里的留存估计与机器学习中讨论的比较接近，是一种自我验证的方法，一定程度上防止训练或者决策的过拟合。书中介绍了讲训练集划分成两部分的方法，也介绍了交叉验证法。Leaving-one-Out的方法应该就是我们机器学习中常用的N折叠法。这里就不再描述。\nGood-Turing\nGood根据图灵机原理提出了一种确定时间频率或者概率估计的方法，假设事件是二项分布的。这种方法适用于从大此表得来的大量数据观察，而且，尽管词汇和n-gram不服从二项式分布。（因为概率论没学太好，不太能直观体会到不同分布的感觉-_-!）,该方法利用了一个调整后的频率，参见如下。\n\n这里具体的算法本人理解不是很透彻，但是大致思路是这样，这些方法都重新归一化了所有的概率估计，以确保得到合理的概率分布结果。比如我们调整转移到未知事件上的概率大小，或者很好的方法似乎是保持转移到未知事件上的概率N1/N不变，并且重新归一化所有的已知事件的概率估计。（Gale and Sampson 1995提出）\n简单总结\n为了处理空类或者说是数据集中不存在的元组，我们采用折扣的的方法将频率增益均分到未知事件上，然后有几种均分方案\n绝对折扣\n线性折扣\n\n等等吧，各有各的特性。\n组合估计法\n对于ｎ-gram模型来说，找到合适的组合不同阶模型的方法是成功的关键。一种合并不同ｎ值的ＭＬＥ ｎ-gram估计（对于未知词有一定的概率转移）的方法使用了简单的线性插值技术，得出了一个非常好的语言模型。\n简单的线性模型\n解决trigram模型中稀疏问题的一种方法是，把bigram模型和unigram模型组合到trigram模型中，这两个模型容忍稀疏数据问题的能力比较强。\n\n回退法\n当没有ｎ-gram时，回退到低阶的模型。同时这里也是需要注意，需要讲概率转移到未知词语上。\n贝叶斯估计是什么？","date":"2017年06月03日 16:25:28"}
{"_id":{"$oid":"5d36a8b96734bd8e681d5e2a"},"title":"自然语言处理（NLP） vs 自然语言理解（NLU）","author":"光影流年925","content":"自然语言处理，是Natural Language Processing， 简称NLP\n自然语言理解，是Natural Language Understanding，简称NLU。\n（图1）\n\n（图2）\n\n1、概括来说，NLP，除了NLU（图中红框部分），还包含理解之前的处理阶段、和理解之后的应用阶段。也就是说，NLU是NLP的子集——他们不是并集、更不是等价的概念。这里，是很多AI从业人员都容易混淆的，大家可以先记住这个概念关系：NLU是NLP的子集。\n2、其他：\n1）左边最底部，是最基础的大数据、机器学习和语言学（Linguistics）；\n2）往上看，是知识图谱（Knowledge Graph），其中包含了实体图谱、注意力图谱和意图图谱。\n3）再往上，左侧是语言理解（Language Understanding），右侧是语言生成（Language Generation）——\n语言理解，包含了Query理解、文本理解、情感分析（Sentiment Analysis）等，还有词法（Lexical）、句法（Syntax）和语义（Semantic）等不同层次的分析。\n语言生成，包含了写作、阅读理解等等。\n4）最上方，是系统层面，包含了问答系统、机器翻译和对话系统。\n5）最右侧，是各种应用场景，包含搜索、feeds流、O2O、广告等等。\n（图3）NLP的难点\n\n（图4~6）NLP的解决方法：规则—\u003e统计—\u003e深度学习\n（图4）\n\n（图5）\n\n（图6）\n\n（图7~9）NLP是AI的最大瓶颈；语言生成是NLP的最前沿。——这2点，大家知道就可以了。\n（图7）\n\n（图8）\n\n（图9）\n\n以上内容，来自饭团“AI产品经理大本营”，点击这里可关注：http://fantuan.guokr.net/groups/219/ （如果遇到支付问题，请先关注饭团的官方微信服务号“fantuan-app”）\n作者：黄钊hanniman，图灵机器人-人才战略官，前腾讯产品经理，5年AI实战经验，8年互联网背景，微信公众号/知乎/在行ID“hanniman”，饭团“AI产品经理大本营”，分享人工智能相关原创干货，200页PPT《人工智能产品经理的新起点》被业内广泛好评，下载量1万+。","date":"2018年01月08日 10:27:03"}
{"_id":{"$oid":"5d36a8ba6734bd8e681d5e2f"},"title":"自然语言处理简介及研究方向","author":"IT狂人-jawi","content":"百度词汇\n自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。自然语言处理是一门融语言学、计算机科学、数学于一体的科学。因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，所以它与语言学的研究有着密切的联系，但又有重要的区别。自然语言处理并不是一般地研究自然语言，而在于研制能有效地实现自然语言通信的计算机系统，特别是其中的软件系统。因而它是计算机科学的一部分。\n自然语言处理（NLP）是计算机科学，人工智能，语言学关注计算机和人类（自然）语言之间的相互作用的领域。\n随着深度学习的发展，LSTM的应用取得的突破，极大地促进了NLP的发展。\n\n自然语言处理的主要范畴有以下\n文本朗读（Text to speech）/语音合成（Speech synthesis）\n语音识别（Speech recognition）\n中文自动分词（Chinese word segmentation）\n词性标注（Part-of-speech tagging）\n句法分析（Parsing）\n自然语言生成（Natural language generation）\n文本分类（Text categorization）\n信息检索（Information retrieval）\n信息抽取（Information extraction）\n文字校对（Text-proofing）\n问答系统（Question answering）\n给一句人类语言的问定，决定其答案。 典型问题有特定答案 (像是加拿大的首都叫什么?)，但也考虑些开放式问句(像是人生的意义是是什么?)\n机器翻译（Machine translation）\n将某种人类语言自动翻译至另一种语言\n自动摘要(Automatic summarization)\n产生一段文字的大意，通常用于提供已知领域的文章摘要，例如产生报纸上某篇文章之摘要\n文字蕴含（Textual entailment）\n自然语言处理目前研究的难点\n单词的边界界定\n在口语中，词与词之间通常是连贯的，而界定字词边界通常使用的办法是取用能让给定的上下文最为通顺且在文法上无误的一种最佳组合。在书写上，汉语也没有词与词之间的边界。\n词义的消岐\n许多字词不单只有一个意思，因而我们必须选出使句意最为通顺的解释。\n句法的模糊性\n自然语言的文法通常是模棱两可的，针对一个句子通常可能会剖析（Parse）出多棵剖析树（Parse Tree），而我们必须要仰赖语意及前后文的资讯才能在其中选择一棵最为适合的剖析树。\n有瑕疵的或不规范的输入\n例如语音处理时遇到外国口音或地方口音，或者在文本的处理中处理拼写，语法或者光学字元识别（OCR）的错误。\n语言行为与计划\n句子常常并不只是字面的意思；例如，“你能把盐递过来吗”，一个好的回答应当是动手把盐递过去；在大多数上下文环境中，“能”将是糟糕的回答，虽说回答“不”或者“太远了我拿不到”也是可以接受的。再者，如果一门课程去年没开设，对于提问“这门课程去年有多少学生没通过？”回答“去年没开这门课”要比回答“没人没通过”好。\n当前自然语言处理研究的发展趋势：\n第一，传统的基于句法-语义规则的理性主义方法受到质疑，随着语料库建设和语料库语言学的崛起，大规模真实文本的处理成为自然语言处理的主要战略目标。\n第二，统计数学方法越来越受到重视，自然语言处理中越来越多地使用机器自动学习的方法来获取语言知识。\n第三，浅层处理与深层处理并重，统计与规则方法并重，形成混合式的系统。\n第四，自然语言处理中越来越重视词汇的作用，出现了强烈的“词汇主义”的倾向。词汇知识库的建造成为了普遍关注的问题。\n第五，统计自然语言处理\n统计自然语言处理运用了推测学、机率、统计的方法来解决上述，尤其是针对容易高度模糊的长串句子，当套用实际文法进行分析产生出成千上万笔可能性时所引发之难题。处理这些高度模糊句子所采用消歧的方法通常运用到语料库以及马可夫模型（Markov models）。统计自然语言处理的技术主要由同样自人工智能下与学习行为相关的子领域：机器学习及资料采掘所演进而成。 ——转自维基百科。","date":"2019年06月12日 02:27:52"}
{"_id":{"$oid":"5d36a8bc6734bd8e681d5e33"},"title":"自然语言处理里程碑","author":"weixin_39012047","content":"自然语言是人类独有的智慧结晶。自然语言处理（Natural Language Processing，NLP）是计算机科学领域与人工智能领域中的一个重要方向，旨在研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。用自然语言与计算机进行通信，有着十分重要的实际应用意义，也有着革命性的理论意义。\n由于理解自然语言，需要关于外在世界的广泛知识以及运用操作这些知识的能力，所以自然语言处理，也被视为解决人工智能完备（AI-complete）的核心问题之一。对自然语言处理的研究也是充满魅力和挑战的。\n本文是来自自然语言处理领域从业人员、知名博主 Sebatian Ruder的一篇文章，主要从神经网络技术方法的角度，讨论自然语言处理领域近 15 年来的重大进展，并总结出与当下息息相关的 8 大里程碑事件。文章内容难免会省略了一些其它重要的相关工作，同时，这份总结偏向于神经网络相关技术，这并不意味着在这段时间内其它技术领域就没有重要的进展。值得注意的是，文中提及的很多神经网络模型都是建立在同一时期非神经网络技术的里程碑之上的，在文章的最后，我们强调了这些打下坚实基础的重要成果。\n2001年——神经语言模型（Neurallanguage models）\n语言模型解决的是在给定已出现词语的文本中，预测下一个单词的任务。这可以算是最简单的语言处理任务，但却有许多具体的实际应用，例如智能键盘、电子邮件回复建议等。当然，语言模型的历史由来已久。经典的方法基于 n-grams 模型（利用前面 n 个词语预测下一个单词），并利用平滑操作处理不可见的 n-grams。\n第一个神经语言模型，前馈神经网络（feed-forward neuralnetwork），是 Bengio 等人于 2001 年提出的。如图 1 所示。\n图 1 | 前馈神经网络语言模型(Bengio et al., 2001; 2003)\n这个模型以某词语之前出现的 n 个词语作为输入向量。今天，这样的向量被称为大家熟知的词嵌入（word embeddings）。这些词嵌入在级联后进入一个隐藏层，该层的输出然后通过一个 softmax 层。\n近年来，用于构建语言模型的前馈神经网络已经被循环神经网络（RNNs）和长短期记忆神经网络（LSTMs）取代。虽然后来提出的许多新模型在经典的 LSTM 上进行了扩展，但它仍然是强有力的基础模型。甚至 Bengio 等人的经典前馈神经网络在某些设定下也和更复杂的模型效果相当，因为这些任务只需要考虑邻近的词语。更好地理解语言模型究竟捕捉了哪些信息也是当今一个活跃的研究领域。\n语言模型的建立是一种无监督学习（unsupervised learning），Yann LeCun 也将其称之为预测学习（predictive learning），是获得世界如何运作常识的先决条件。关于语言模型最引人注目的是，尽管它很简单，但却与后文许多核心进展息息相关。\n反过来，这也意味着自然语言处理领域的许多重要进展都可以简化为某种形式的语言模型构建。但要实现对自然语言真正意义上的理解，仅仅从原始文本中进行学习是不够的，我们需要新的方法和模型。\n2008年——多任务学习（Multi-tasklearning）\n多任务学习是在多个任务下训练的模型之间共享参数的方法，在神经网络中可以通过捆绑不同层的权重轻松实现。多任务学习的思想在 1993 年由 Rich Caruana 首次提出，并应用于道路追踪和肺炎预测。多任务学习鼓励模型学习对多个任务有效的表征描述。这对于学习一般的、低级的描述形式、集中模型的注意力或在训练数据有限的环境中特别有用。\n多任务学习于 2008 年被Collobert 和 Weston 等人首次在自然语言处理领域应用于神经网络。在他们的模型中，词嵌入矩阵被两个在不同任务下训练的模型共享，如图 2 所示。\n图 2 | 词嵌入矩阵共享(Collobert \u0026 Weston, 2008; Collobert et al., 2011)\n共享的词嵌入矩阵使模型可以相互协作，共享矩阵中的低层级信息，而词嵌入矩阵往往构成了模型中需要训练的绝大部分参数。Collobert 和 Weston 发表于 2008 年的论文，影响远远超过了它在多任务学习中的应用。它开创的诸如预训练词嵌入和使用卷积神经网络处理文本的方法，在接下来的几年被广泛应用。他们也因此获得了 2018 年机器学习国际会议（ICML）的 test-of-time 奖。\n如今，多任务学习在自然语言处理领域广泛使用，而利用现有或“人工”任务已经成为 NLP 指令库中的一个有用工具。虽然参数的共享是预先定义好的，但在优化的过程中却可以学习不同的共享模式。当模型越来越多地在多个任务上进行测评以评估其泛化能力时，多任务学习就变得愈加重要，近年来也涌现出更多针对多任务学习的评估基准。\n2013年——词嵌入\n通过稀疏向量对文本进行表示的词袋模型，在自然语言处理领域已经有很长的历史了。而用稠密的向量对词语进行描述，也就是词嵌入，则在 2001 年首次出现。2013 年Mikolov 等人工作的主要创新之处在于，通过去除隐藏层和近似计算目标使词嵌入模型的训练更为高效。尽管这些改变在本质上是十分简单的，但它们与高效的 word2vec（word to vector，用来产生词向量的相关模型）组合在一起，使得大规模的词嵌入模型训练成为可能。\nWord2vec 有两种不同的实现方法：CBOW（continuous bag-of-words）和 skip-gram。它们在预测目标上有所不同：一个是根据周围的词语预测中心词语，另一个则恰恰相反。如图 3 所示。\n图 3 | CBOW 和skip-gram 架构（Mikolov et al., 2013a; 2013b）\n虽然这些嵌入与使用前馈神经网络学习的嵌入在概念上没有区别，但是在一个非常大语料库上的训练使它们能够获取诸如性别、动词时态和国际事务等单词之间的特定关系。如下图 4 所示。\n图 4 | word2vec 捕获的联系（Mikolov et al., 2013a; 2013b）\n这些关系和它们背后的意义激起了人们对词嵌入的兴趣，许多研究都在关注这些线性关系的来源。然而，使词嵌入成为目前自然语言处理领域中流砥柱的，是将预训练的词嵌入矩阵用于初始化可以提高大量下游任务性能的事实。\n虽然 word2vec 捕捉到的关系具有直观且几乎不可思议的特性，但后来的研究表明，word2vec 本身并没有什么特殊之处：词嵌入也可以通过矩阵分解来学习，经过适当的调试，经典的矩阵分解方法 SVD 和 LSA 都可以获得相似的结果。\n从那时起，大量的工作开始探索词嵌入的不同方面。尽管有很多发展，word2vec 仍然是目前应用最为广泛的选择。Word2vec 的应用范围也超出了词语级别：带有负采样的 skip-gram——一个基于上下文学习词嵌入的方便目标，已经被用于学习句子的表征。它甚至超越了自然语言处理的范围，被应用于网络和生物序列等领域。\n一个激动人心的研究方向是在同一空间中构建不同语言的词嵌入模型，以达到（零样本）跨语言转换的目的。通过无监督学习构建这样的映射变得越来越有希望（至少对于相似的语言来说），这也为语料资源较少的语言和无监督机器翻译的应用程序创造可能。\n2013年——用于自然语言处理的神经网络\n2013年 和 2014 年是自然语言处理领域神经网络时代的开始。其中三种类型的神经网络应用最为广泛：循环神经网络（recurrent neural networks）、卷积神经网络（convolutionalneural networks）和结构递归神经网络（recursive neural networks）。\n循环神经网络是 NLP 领域处理动态输入序列最自然的选择。Vanilla 循环神经网络很快被经典的长短期记忆网络（long-shortterm memory networks，LSTM）代替，该模型能更好地解决梯度消失和梯度爆炸问题。在 2013 年之前，人们仍认为循环神经网络很难训练，直到 Ilya Sutskever 博士的论文改变了循环神经网络这一名声。双向的长短期记忆记忆网络通常被用于同时处理出现在左侧和右侧的文本内容。LSTM 结构如图 5 所示。\n图 5 | LSTM 网络（来源：ChrisOlah）\n应用于文本的卷积神经网络只在两个维度上进行操作，卷积层只需要在时序维度上移动即可。图6 展示了应用于自然语言处理的卷积神经网络的典型结构。\n图 6 | 卷积神经网络（Kim,2014）\n与循环神经网络相比，卷积神经网络的一个优点是具有更好的并行性。因为卷积操作中每个时间步的状态只依赖于局部上下文，而不是循环神经网络中那样依赖于所有过去的状态。卷积神经网络可以使用更大的卷积层涵盖更广泛的上下文内容。卷积神经网络也可以和长短期记忆网络进行组合和堆叠，还可以用来加速长短期记忆网络的训练。\n循环神经网络和卷积神经网络都将语言视为一个序列。但从语言学的角度来看，语言是具有层级结构的：词语组成高阶的短语和小句，它们本身可以根据一定的产生规则递归地组合。这激发了利用结构递归神经网络，以树形结构取代序列来表示语言的想法，如图 7 所示。\n图 7 | 结构递归神经网络(Socher et al., 2013)\n结构递归神经网络自下而上构建序列的表示，与从左至右或从右至左对序列进行处理的循环神经网络形成鲜明的对比。树中的每个节点是通过子节点的表征计算得到的。一个树也可以视为在循环神经网络上施加不同的处理顺序，所以长短期记忆网络则可以很容易地被扩展为一棵树。\n不只是循环神经网络和长短期记忆网络可以扩展到使用层次结构，词嵌入也可以在语法语境中学习，语言模型可以基于句法堆栈生成词汇，图形卷积神经网络可以树状结构运行。\n2014年——序列到序列模型（Sequence-to-sequencemodels）\n2014 年，Sutskever 等人提出了序列到序列学习，即使用神经网络将一个序列映射到另一个序列的一般化框架。在这个框架中，一个作为编码器的神经网络对句子符号进行处理，并将其压缩成向量表示；然后，一个作为解码器的神经网络根据编码器的状态逐个预测输出符号，并将前一个预测得到的输出符号作为预测下一个输出符号的输入。如图 8 所示。\n图 8 | 序列到序列模型(Sutskever et al., 2014)\n机器翻译是这一框架的杀手级应用。2016 年，谷歌宣布他们将用神经机器翻译模型取代基于短语的整句机器翻译模型。谷歌大脑负责人 Jeff Dean 表示，这意味着用 500 行神经网络模型代码取代 50 万行基于短语的机器翻译代码。\n由于其灵活性，该框架在自然语言生成任务上被广泛应用，其编码器和解码器分别由不同的模型来担任。更重要的是，解码器不仅可以适用于序列，在任意表示上均可以应用。比如基于图片生成描述（如图 9）、基于表格生成文本、根据源代码改变生成描述，以及众多其他应用。\n图 9 | 基于图像生成标题（Vinyalset al., 2015）\n序列到序列的学习甚至可以应用到自然语言处理领域常见的结构化预测任务中，也就是输出具有特定的结构。为简单起见，输出就像选区解析一样被线性化（如图 10）。在给定足够多训练数据用于语法解析的情况下，神经网络已经被证明具有产生线性输出和识别命名实体的能力。\n图 10 | 线性化选区解析树（Vinyalset al., 2015）\n序列的编码器和解码器通常都是基于循环神经网络，但也可以使用其他模型。新的结构主要都从机器翻译的工作中诞生，它已经成了序列到序列模型的培养基。近期提出的模型有深度长短期记忆网络、卷积编码器、Transformer（一个基于自注意力机制的全新神经网络架构）以及长短期记忆依赖网络和的 Transformer 结合体等。\n2015年——注意力机制\n注意力机制是神经网络机器翻译 (NMT) 的核心创新之一，也是使神经网络机器翻译优于经典的基于短语的机器翻译的关键。序列到序列学习的主要瓶颈是，需要将源序列的全部内容压缩为固定大小的向量。注意力机制通过让解码器回顾源序列的隐藏状态，以此为解码器提供加权平均值的输入来缓解这一问题，如图 11 所示。\n图 11 | 注意力机制(Bahdanau et al., 2015)\n之后，各种形式的注意力机制涌现而出。注意力机制被广泛接受，在各种需要根据输入的特定部分做出决策的任务上都有潜在的应用。它已经被应用于句法分析、阅读理解、单样本学习等任务中。它的输入甚至不需要是一个序列，而可以包含其他表示，比如图像的描述（图 12）。\n注意力机制一个有用的附带作用是它通过注意力权重来检测输入的哪一部分与特定的输出相关，从而提供了一种罕见的虽然还是比较浅层次的，对模型内部运作机制的窥探。\n图 12 | 图像描述模型中的视觉注意力机制指示在生成”飞盘”时所关注的内容（Xu etal., 2015）\n注意力机制也不仅仅局限于输入序列。自注意力机制可以用来观察句子或文档中周围的单词，获得包含更多上下文信息的词语表示。多层的自注意力机制是神经机器翻译前沿模型 Transformer 的核心。\n2015年——基于记忆的神经网络\n注意力机制可以视为模糊记忆的一种形式，其记忆的内容包括模型之前的隐藏状态，由模型选择从记忆中检索哪些内容。与此同时，更多具有明确记忆单元的模型被提出。他们有很多不同的变化形式，比如神经图灵机（Neural Turing Machines）、记忆网络（Memory Network）、端到端的记忆网络（End-to-end Memory Newtorks）、动态记忆网络（DynamicMemory Networks）、神经可微计算机（Neural Differentiable Computer）、循环实体网络（RecurrentEntity Network）。\n记忆的存取通常与注意力机制相似，基于与当前状态且可以读取和写入。这些模型之间的差异体现在它们如何实现和利用存储模块。比如说，端到端的记忆网络对输入进行多次处理并更新内存，以实行多次推理。神经图灵机也有一个基于位置的寻址方式，使它们可以学习简单的计算机程序，比如排序。基于记忆的模型通常用于需要长时间保留信息的任务中，例如语言模型构建和阅读理解。记忆模块的概念非常通用，知识库和表格都可以作为记忆模块，记忆模块也可以基于输入的全部或部分内容进行填充。\n2018——预训练的语言模型\n预训练的词嵌入与上下文无关，仅用于初始化模型中的第一层。近几个月以来，许多有监督的任务被用来预训练神经网络。相比之下，语言模型只需要未标记的文本，因此其训练可以扩展到数十亿单词的语料、新的领域、新的语言。预训练的语言模型于 2015 年被首次提出，但直到最近它才被证明在大量不同类型的任务中均十分有效。语言模型嵌入可以作为目标模型中的特征，或者根据具体任务进行调整。如下图所示，语言模型嵌入为许多任务的效果带来了巨大的改进。\n图 13 | 改进的语言模型嵌入（Peterset al., 2018）\n使用预训练的语言模型可以在数据量十分少的情况下有效学习。由于语言模型的训练只需要无标签的数据，因此他们对于数据稀缺的低资源语言特别有利。\n其他里程碑\n一些其他进展虽不如上面提到的那样流行，但仍产生了广泛的影响。\n基于字符的描述（Character-based representations）\n在字符层级上使用卷积神经网络和长短期记忆网络，以获得一个基于字符的词语描述，目前已经相当常见了，特别是对于那些语言形态丰富的语种或那些形态信息十分重要、包含许多未知单词的任务。据目前所知，基于字符的描述最初用于序列标注，现在，基于字符的描述方法，减轻了必须以增加计算成本为代价建立固定词汇表的问题，并使完全基于字符的机器翻译的应用成为可能。\n对抗学习（Adversarial learning）\n对抗学习的方法在机器学习领域已经取得了广泛应用，在自然语言处理领域也被应用于不同的任务中。对抗样例的应用也日益广泛，他们不仅仅是探测模型弱点的工具，更能使模型更具鲁棒性（robust）。（虚拟的）对抗性训练，也就是最坏情况的扰动，和域对抗性损失（domain-adversariallosses）都是可以使模型更具鲁棒性的有效正则化方式。生成对抗网络 (GANs) 目前在自然语言生成任务上还不太有效，但在匹配分布上十分有用。\n强化学习（Reinforcement learning）\n强化学习已经在具有时间依赖性的任务上证明了它的能力，比如在训练期间选择数据和对话建模。在机器翻译和概括任务中，强化学习可以有效地直接优化”红色”和”蓝色”这样不可微的度量，而不必去优化像交叉熵这样的代理损失函数。同样，逆向强化学习（inverse reinforcement learning）在类似视频故事描述这样的奖励机制非常复杂且难以具体化的任务中，也非常有用。","date":"2018年10月23日 14:28:22"}
{"_id":{"$oid":"5d36a8bd6734bd8e681d5e38"},"title":"自然语言处理资源","author":"黄俊东","content":"NLP领域\n自然语言处理　　计算语言学　　自然语言理解　　自然语言生成　　机器翻译　　文本分类　　语音识别　　语音合成\n中文分词　　信息检索　　信息抽取　　句法分析　　问答系统　　自动摘要　　拼写检查　　统计机器翻译\nNLP专题\n隐马尔科夫模型　　最大熵模型　　条件随机场　　数学之美　　支持向量机　　机器学习　　SRILM　　Moses　　知网\nIRSTLM　　NLTK\nNLP人物\n冯志伟　　俞士汶　　董振东　　黄昌宁　　黄曾阳　　周明　　姚天顺　　刘群　　宗成庆　　赵铁军　　詹卫东　　常宝宝\n刘挺　　王海峰\n哈工大中文信息处理人物谱　　中文信息学会人物谱\nFranz Josef Och\nNLP会议\nACL　　COLING　　TREC　　EMNLP　　其他会议　　NLP会议档次\nNLP书籍\n自然语言处理相关书籍\n取自\"http://wiki.52nlp.cn/首页\"\n特别推荐：\n1、HMM学习最佳范例全文文档\n2、无约束最优化全文文档\n一、书籍：\n1、《自然语言处理综论》英文版第二版\n2、《统计自然语言处理基础》英文版\n3、《用Python进行自然语言处理》，NLTK配套书\n4、《Learning Python第三版》，Python入门经典书籍，详细而不厌其烦\n5、《自然语言处理中的模式识别》\n6、《EM算法及其扩展》\n7、《统计学习基础》\n8、《自然语言理解》英文版（似乎只有前9章）\n9、《Fundamentals of Speech Recognition》，质量不太好，不过第6章关于HMM的部分比较详细，作者之一便是Lawrence Rabiner；\n10、概率统计经典入门书：《概率论及其应用》（英文版，威廉*费勒著）\n第一卷　　第二卷　　DjVuLibre阅读器（阅读前两卷书需要）\n11、一本利用Perl和Prolog进行自然语言处理的介绍书籍：《An Introduction to Language Processing with Perl and Prolog》\n12、国外机器学习书籍之：\n1) “Programming Collective Intelligence“，中文译名《集体智慧编程》，机器学习\u0026数据挖掘领域”近年出的入门好书，培养兴趣是最重要的一环，一上来看大部头很容易被吓走的”\n2) “Machine Learning“,机器学习领域无可争议的经典书籍，下载完毕将后缀改为pdf即可。豆瓣评论 by 王宁）：老书，牛人。现在看来内容并不算深，很多章节有点到为止的感觉，但是很适合新手（当然，不能”新”到连算法和概率都不知道）入门。比如决策树部分就很精彩，并且这几年没有特别大的进展，所以并不过时。另外，这本书算是对97年前数十年机器学习工作的大综述，参考文献列表极有价值。国内有翻译和影印版，不知道绝版否。\n3) “Introduction to Machine Learning”\n13、国外数据挖掘书籍之：\n1) “Data.Mining.Concepts.and.Techniques.2nd“，数据挖掘经典书籍作者 : Jiawei Han/Micheline Kamber 出版社 : Morgan Kaufmann 评语 : 华裔科学家写的书，相当深入浅出。\n2) Data Mining:Practical Machine Learning Tools and Techniques\n3) Beautiful Data: The Stories Behind Elegant Data Solutions（ Toby Segaran, Jeff Hammerbacher）\n14、国外模式识别书籍之：\n1）“Pattern Recognition”\n2）“Pattern Recongnition Technologies and Applications”\n3）“An Introduction to Pattern Recognition”\n4）“Introduction to Statistical Pattern Recognition”\n5）“Statistical Pattern Recognition 2nd Edition”\n6）“Supervised and Unsupervised Pattern Recognition”\n7）“Support Vector Machines for Pattern Classification”\n15、国外人工智能书籍之：\n1）Artificial Intelligence: A Modern Approach (2nd Edition) 人工智能领域无争议的经典。\n2）“Paradigms of Artificial Intelligence Programming: Case Studies in Common LISP”\n16、其他相关书籍：\n1）Programming the Semantic Web，Toby Segaran , Colin Evans, Jamie Taylor\n2）Learning.Python第四版，英文\n二、课件：\n1、哈工大刘挺老师的“统计自然语言处理”课件；\n2、哈工大刘秉权老师的“自然语言处理”课件；\n3、中科院计算所刘群老师的“计算语言学讲义“课件；\n4、中科院自动化所宗成庆老师的“自然语言理解”课件；\n5、北大常宝宝老师的“计算语言学”课件；\n6、北大詹卫东老师的“中文信息处理基础”的课件及相关代码；\n7、MIT Regina Barzilay教授的“自然语言处理”课件，52nlp上翻译了前5章；\n8、MIT大牛Michael Collins的“Machine Learning Approaches for Natural Language Processing(面向自然语言处理的机器学习方法)”课件；\n9、Michael Collins的“Machine Learning （机器学习）”课件；\n10、SMT牛人Philipp Koehn “Advanced Natural Language Processing（高级自然语言处理）”课件；\n11、Philipp Koehn “Empirical Methods in Natural Language Processing”课件；\n12、Philipp Koehn“Machine Translation（机器翻译）”课件；\n三、语言资源和开源工具：\n1、Brown语料库：\na) XML格式的brown语料库，带词性标注；\nb) 普通文本格式的brown语料库，带词性标注；\nc) 合并并去除空行、行首空格，用于词性标注训练：browntest.zip\n2、NLTK官方提供的语料库资源列表\n3、OpenNLP上的开源自然语言处理工具列表\n4、斯坦福大学自然语言处理组维护的“统计自然语言处理及基于语料库的计算语言学资源列表”\n5、LDC上免费的中文信息处理资源\n6、中文分词相关工具：\n1）Java版本的MMSEG：mmseg-v0.3.zip，作者为solol，详情可参见：《中文分词入门之篇外》\n2）张华平老师的ICTCLAS2010，该版本非商用免费一年，下载地址：\nhttp://cid-51de2738d3ea0fdd.skydrive.live.com/self.aspx/.Public/ICTCLAS2010-packet-release.rar\n7、热心读者“finallyliuyu”提供的一批新闻语料库，包括腾讯，新浪，网易，凤凰等，目前放在CSDN上：http://finallyliuyu.download.csdn.net/\n另外finalllyliuyu在2010年9月又提供了一批文本文类语料，详情见：献给热衷于自然语言处理的业余爱好者的中文新闻分类语料库之二\n四、文献：\n1、ACL-IJCNLP 2009论文全集：\na) 大会论文Full Paper第一卷\nb) 大会论文Full Paper第二卷\nc) 大会论文Short Paper合集\nd) ACL09之EMNLP-2009合集\ne) ACL09 所有workshop论文合","date":"2013年03月26日 22:23:47"}
{"_id":{"$oid":"5d36a8be6734bd8e681d5e3a"},"title":"自然语言处理相关学习资料","author":"enoying","content":"自然语言处理相关学习资料（转）\nbook\n宗成庆. 统计自然语言处理. 清华大学出版社. 2008. 此书为统计观点，适合CS背景做NLP的人读。\n2.Manning, C. D Foundations of Statistical Natural Language Processing. MIT Press. 1999.\n冯志伟. 自然语言处理的形式模型. 中国科技大学出版社. 2010. 此书讲涵盖句法、语义各个层面 ps：作者是从Linguistic角度去分析自然语言处理\nModel:\nYoshua Bengio. A Neural Probabilistic Language Model. JMLR(2003). 2003. 神经网络语言模型的开山之作，MileStone论文，引用率634(Google Scholar)。\nFrederic Morin, Yoshua Bengio. Hierarchical Probabilistic Neural Network Language Model. Innovations in Machine Learning(2006). 2006.提出了Hierarchical NPLM\nAndriy Mnih, Geoffrey Hinton. Three New Graphical Models for Statistical Language Modelling. ICML(2007). 2007. 提出了三个Model，其中提的较多的是A Log-Bilinear Language Model，后续论文多引用此模型\nAndriy Mnih, Geoffrey Hinton. A Scalable Hierarchical Distributed Language Model. NIPS(2008). 2008. 提出HLBL\nRonan Collobert, Jason Weston. A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning. ICML(2008). 2008. 旧瓶新酒-TDNN Multitask Learning\nRonan Collobert Jason Weston et al.Natural Language Processing (Almost) from Scratch. JMLR(2011). 2011. 对SENNA进行解释的论文，注意SENNA要区别[5]中的C\u0026W embedding.\nEric H. Huang, Richard Socher, etc. ImprovingWord Representations via Global Context and MultipleWord Prototypes. ACL(2012). 2012. 此篇paper把全局信息加入模型，模型求解用了[5]中的方法\nword2vec系列paper：\nDistributed Representations ofWords and Phrases and their Compositionality\nEfficient Estimation of Word Representations in Vector Space\nword2vec Explained: Deriving Mikolov et al.’s Negative\nSampling Word-Embedding Method 解释性的paper 发布arxiv上的，和有道那个可以一起看\nNitish Srivastava, Ruslan Salakhutdinov,Geoffrey Hinton. Modeling Documents with a Deep Boltzmann Machine. UAI(2013). 类似于LDA的一种topic model\nRNN系列, Recurrent NN能model long term dependency, 训练出的结果比Feed Forward NN结果更好 但训练复杂度更大 这个系列word2vec作者Mikolov研究较多，比如其博士论文\nLinguistic Regularities in Continuous SpaceWord Representations\nRecurrent neural network based language model\nRecursive NN这个主要用在句法分析上，model自然语言存在的递归结构 这个主要是Richard Socher的paper\nRecursive Deep Models for Semantic Compositionality Over a Sentiment Treebank\nParsing Natural Scenes and Natural Language with Recursive Neural Networks\nJoseph Turian, Lev Ratinov, Yoshua Bengio. Word representations: A simple and general method for semi-supervised learning. ACL(2010) 对现有的word Representation做了对比 提供一个新的word embedding 读者可以自行复现（见Section 13）。\nJeffrey Pennington，Richard Socher, Chris Manning. GloVe: Global Vectors for Word Representation. EMNLP(2014)\nGloVe与word2vec对比的效果曾经被质疑过 其实word2vec效果差不多\nOmer Levy, Yoav Goldberg.Neural Word Embedding as Implicit Matrix Factorization. NIPS. 2014.\n将SGNS(Skip Gram with Negative Sampling)和矩阵分解等价分析，SGNS等价于分解PMI矩阵。文中作者基于谱方法（SVD）分解shifted PPMI的矩阵，得到了不错的效果（word sim上和word2vec类似）。作者还在arxiv提交了一个分析SGNS的note，结合看更加。\nQ.V. Le, T. Mikolov.Distributed Representations of Sentences and Documents.ICML(2014). 2014. 文中各个实验都体现了好的效果，但是可复现性一直遭到质疑，最近在word2vec的google group上公布了复现方法，已经有人复现出92.6%的结果。\nTutorial：\nTomas Mikolov. Statistical Language Models Based on Neural Networks\nRichard Socher. Recursive Deep Learning for Modeling Semantic Compositionality\nRuchard Socher, Christpher Manning. Deep Learning for Natural Language Processing (without Magic)\nEvaluation：\nYanqing Chen, etc. The Expressive Power of Word Embeddings. ICML(2013). 实验评价了四个model–HLBL[4],SENNA[11],Turian’s[12], Huang’s[6].","date":"2017年12月28日 14:50:57"}
{"_id":{"$oid":"5d36a8bf6734bd8e681d5e3c"},"title":"精通Python自然语言处理 PDF ——带完整书签","author":"blue-white","content":"下载链接：精通Python自然语言处理\n带完整书签：","date":"2019年05月09日 14:17:10"}
{"_id":{"$oid":"5d36a8bf6734bd8e681d5e3f"},"title":"自然语言处理（NLP） vs 自然语言理解（NLU）补充篇","author":"光影流年925","content":"（图1~2）微软亚洲研究院副院长周明老师，关于NLP的7个重要领域的详细解释。\n\n（图3）自然语言处理的重要应用领域之一：机器翻译。\n图中对比了3种翻译方式的效果，红色是基于短语，天蓝色是基于神经网络，黄色是人类水平。\n可以看到，英文和西班牙语/法语的（双向）翻译，通过应用神经网络，效果已经是比较完美了（perfect），接近于人类水平。但英文和中文的翻译，效果还比较普通。\n\n（图4）介绍了机器翻译的2种方法（SMT、NMT），并引出了“注意力机制”。\n\n（图5）用图像领域的一张非常经典的图，解释注意力机制。\n左侧，问题是“外套的颜色是什么？”传统的VQA方法，会给出错误答案“棕色”，因为图中大部分区域是砖块。而基于注意力机制的VQA方法，会先找到“外套”，然后再给出回答“黄色”\n类似的，右侧，问题是“雨伞的颜色是什么？”基于注意力的VQA方法，也会先找到“雨伞”，然后回答“红色”，而不是之前方法的错误判断“绿色”（因为雨伞背后更大区域是绿色）。\n\n（图6）语言生成的6个应用方向\n\n（图7）语言生成-对话的4个难题\n\n（图8）知乎上的一个问题“百度NLP部门怎么样？”，有匿名用户回答“NLP是现在人工智能的瓶颈，有志于从事NLP工作的同学要明白这一点”，下面还有同学问“能不能多解释一下”：）\n其实匿名用户的意思是，如果想在NLP领域做出真正有突破性的成就，是非常难的，如果不能忍受长时间的煎熬，还不如去其他相对更容易做出成果的AI领域……\n\n以上内容，来自饭团“AI产品经理大本营”，点击这里可关注：http://fantuan.guokr.net/groups/219/ （如果遇到支付问题，请先关注饭团的官方微信服务号“fantuan-app”）\n作者：黄钊hanniman，图灵机器人-人才战略官，前腾讯产品经理，5年AI实战经验，8年互联网背景，微信公众号/知乎/在行ID“hanniman”，饭团“AI产品经理大本营”，分享人工智能相关原创干货，200页PPT《人工智能产品经理的新起点》被业内广泛好评，下载量1万+。","date":"2018年01月09日 15:17:56"}
{"_id":{"$oid":"5d36a8c06734bd8e681d5e43"},"title":"自然语言处理 学习笔记（五）","author":"VinceLim","content":"个人学习nlp笔记：学习材料CS124、COSC572和《Speech and Language Processing》第三版\n\n\n自然语言处理 学习笔记（五）\n1.矢量语义（Vector Semantics）\n1.1词汇语义\n1.2 语义的矢量表达\n2. 信息抽取和命名实体抽取\n2.1 关系抽取\n2.1.1 手写规则\n2.1.2 监督式关系抽取\n2.1.3 非监督式和半监督式关系抽取\n3.问答系统QA\n3.1 信息检索型问答\n3.1.1 答案类型检测\n3.1.2 构建query\n3.1.3 章节检索\n3.1.4 答案提取\n3.2 模型评估\n3.4 使用知识（knowledge）的QA系统\n3.5 更复杂的问题\n1.矢量语义（Vector Semantics）\n分布式假说 distributional hypothesis，若词的分布相似，则词的意思也相似。\n1.1词汇语义\n有两个重要的概念，词根和词义，词根lemma，也叫citation form，词sung,sang的词根就是sing，而sung这些词也称wordform\n\n词义sense就更好理解了，比如mouse能表示老鼠和鼠标，这就说明mouse有多个词义\n\n多个词义也带来了，诸如同义词synonyms的问题\ncouch/sofa\nvomit/throw up\nfilbert/hazelnut\ncar/automobile\n当然，我们也有反义词Antonyms\nlong/short\nbig/little\nfast/slow\ncold/hot dark/light\n同样，我们也有很多相似的词（word similarity），比如cat和dog，显然他们不是同义词，但是他们都很相似，指代着名词和一种动物。我们很多种方法来测量这种相似度，比如很多手工标注的数据库。\n同时还有词相关度Word Relatedness，比如词coffee和cup，一个指一种饮料或植物吗，而另一种指一个餐具或者形状，但是毫无疑问，他们在现实世界中会经常在同一个语义场景下出现。诸如hospitals(surgeon, scalpel, nurse, anaesthetic, hospital), restaurants (waiter, menu, plate,food, chef), or houses (door, roof, kitchen, family, bed)。语义场景又和主题模型topic models有联系。\n语义框架，比如在交易的场景下，词代表着不同的事件，比如买，是由买家发出。而支付是着重于货币方面的…比如我卖了本书给B，则B就为买家，我为卖家，若机器能理解这些，会在QA系统或翻译上有很大帮助\n\n分类关系Taxonomic Relations，比如把车归到交通工具，芒果归到水果，给每个详细的词一个上位词。\n\n此外，词还有很多情感的含义，比如sad和terrible有不同程度的感情色彩\n\n1.2 语义的矢量表达\n人类可以从一句话中的其他信息获得对不认识的词的信息\n\n矢量语义的直觉是来自分布式假说和前文的connotation中用向量表达一个词结合\n\n\n矢量语义就算把一个词映射到多维的语义空间中，我们能看见，褒义的和贬义的词是明显分布在不同的地方的。同样我们在矢量化的单词中能执行很多操作，比如计算相似度和语义分析。在前文的语义分析中（贝叶斯），只能在足够多的关键词同时出现在训练和测试集中才行。而在矢量化后，我们可以用相似度高的词替代在训练集中未出现的词。总而言之，是一个很实用的非监督的方法。\n\n比如前文的tf-idf和word2vec\n2. 信息抽取和命名实体抽取\n信息抽取的目的主要有，抽取有用的信息，把信息整合为数据框等易处理的形式\n\n也可以用来抽取重要信息，比如一句话的发生地点和主要人物\n\n这些信息可以用来做一些应用，比如自动建立日程等等\n\n命名实体识别，也就是把一些词分为人名，地点，时间，组织等。要先find，才能classify\n\n2.1 关系抽取\n\n抽取文中的关系，能让QA系统更加智能\n\n抽取的关系\n\n不同的任务可能由不同的需要抽取的信息。比如在医疗场景：\n\n原理维基百科上的这个条目是由关系抽取得来的啊\n‘\n2.1.1 手写规则\n\n\n\n\n\n准确率高，召回率低（漏掉的多）\n\n2.1.2 监督式关系抽取\n第一步，找到命名实体，第二步，判断2个命名实体是否存在关系，如果binary分类器返回存在，开始对关系进行分类。因为有二分类这一步，很多不必要的（显然没关系的）命名实体关系就不用进行分类，同时我们也可以采用两套特征来分别解决问题。\n\n监督学习索要用到的特征：\n命名实体的中心词，以及把两者结合。M1和M2的unigram和bigram词袋模型，M1和M2左边和右边的单词，M1和M2之间的词的unigram和bigram词袋模型\n\n命名实体的类型，类型组合，实体层面(entity level,名字、名词还是代词)\n\n语法特征：\n\n亲属词，地名索引和上级关系，比如海南在中国\n\n\n准确度高，但是泛化能力一般\n\n2.1.3 非监督式和半监督式关系抽取\n使用手头已有的少数数据和准确度高的模式，自动找其他模式\n\n用找到的pair周围的内容再生成新的模式来找新的pair\n\n键入已有的seed tuple，找到符合的内容，学习模式，再用学习来的模式去找新的tuple\n\n\nsnowball的方法加上了判断是否为命名实体\n\n一种结合了半监督和非监督的的方法，就是用半监督的方法，把标记的数据（命名实体），放着更大的语料库（google）找到相应语句，抽取其的关系，再训练\n\n\n\n非监督：使用一个小的有语法信息的数据训练一个分类器，看得到的关系或者tuple是否值得相信。放入大语料中，抽取关系，若这是个值得相信的关系或tuple，保留。最后把得到的关系根据出现频率排序\n\n评估方式：抽取前1000个新关系，手动评价\n\n3.问答系统QA\n一种更智能的检索或者知识管理方法\n\n一般问题有两种，第一种factoid question，易回答，商业系统（已使用）\n\n另一种是复杂的问题，在research system中常见\n\n常见的方案，第一个是信息检索的方法，直接在网上找答案；第二种是通过对问题解析，并用混合的方法构建回答，比如用部分数据库，再结合信息检索的方法，也是比较modern的方法。\n\n\n\n\n\n3.1 信息检索型问答\n3.1.1 答案类型检测\n第一步，理解问题问了什么\n\n?x表示unknown\n\n以答案类型检测为例子展开：\n\n\n\nJeopardy知识竞赛中的答案类型，然而最频繁的200各类型包含了50%的数据\n\n答案类型检测的方法有三种\n\n手写规则，在一些场景很有用\n\n机器学习方法：\n\n\n3.1.2 构建query\n第二步，构成query，决定把什么词传输给信息检索系统\n一个关键词选择的算法：\n\n引号中是非停顿词，则排第一，而名词应排六（ppt应该有误），剩下的动词排七；我们可以输入前4个也可以只输入rank1的作为关键词。\n\n3.1.3 章节检索\n前两步等同前文的检索系统，不过把文档换成按段落来检索了。\n\n而章节的重排序需要其他方法，比如按规则或者监督学习\n\n3.1.4 答案提取\n对排序得到的passage按答案类型做提取，但也有问题\n\n比如人名问题中，存在多个名字，我们需要对其进行再次排序\n\n\n比如IBM的Waston用了50多个变量\n\n3.2 模型评估\n可以对前M个得分高的答案进行对比，若有回答对的，返回\n1\nr\na\nn\nk\ni\n\\frac{1}{rank_{i}}\nranki 1 ，则最高得分的candidate若正确就是1，若M个答案都错就是0。\n\n3.4 使用知识（knowledge）的QA系统\n比如whose granddaughter starred in E.T.\n分解为谁在et中出演，和她又是谁的孙女\n\n通过人物时间判断回答准确性\n\n通过地理知识\n\n3.5 更复杂的问题\n整合答案\n\n抽取专业文献的答案\n\n两种主要的办法：\n\n主要谈了第二种，根据不同问题\n\n\n比如在回答定义的类型中，设定20个文档，并返回8句话的答案。先将其检索，发现1000余个与Hajj有关的句子，再用一个分类器判断是否这是一个下定义的句子。再将两种（定义语句和非定义语句）句子聚类和重要性排序。","date":"2019年01月01日 16:29:30"}
{"_id":{"$oid":"5d36a8c06734bd8e681d5e46"},"title":"自然语言处理技术框架","author":"IT狂人-jawi","content":"自然语言处理涉及到的相关技术，可以按照不同的分类标准、基于不同的观察视角进行划分。基于不同的分类原则，自然语言处理相关技术的分类结果也有所不同。在这里，我们主要采用两个分类原则进行划分，其一、基于分析对象语言单位粒度的不同：词汇级、句子级级和篇章级；其二、基于分析内容性质的不同：词法分析、语法分析、语义分析和语用分析。按照以上的分类标准，自然语言处理的主要技术分类结果如下图所示：","date":"2019年06月12日 01:50:25"}
{"_id":{"$oid":"5d36a8c16734bd8e681d5e49"},"title":"自然语言处理 学习笔记（四）","author":"VinceLim","content":"个人学习nlp笔记：学习材料CS124、COSC572和《Speech and Language Processing》第三版\n\n\n自然语言处理 学习笔记（四）\n1. 信息检索\n2. 词汇-文本关联矩阵\n3.倒排索引(Inverted Index)\n2.1 倒排索引的结构\n2.2 用倒排索引的查询处理（Query processing）\n2.3 布尔检索模型\n4.短语查询\n4.1 双词索引（biwords indexes）\n4.2 带位置的索引\n5.排序检索\n5.1 Jaccard系数\n5.2 加权词频\n5.3 逆文档频率加权（Invrse document frequency weighting/IDF）\n6.TF-IDF\n7. 向量空间模型\n8.TF-IDF的cosine得分\n1. 信息检索\n从文档中提取需要的信息\n\n\ninfo need步骤里，把我们想要的信息翻译为搜索框能够理解的形式\nquery里翻译为搜索引擎能理解的形式。这个过程中主要会出现两种错误，本课主要关注第二种，即怎样才能正确组织文字来送到搜索引擎。我们选择 how trap mice alive; how trap mice without killing或者是加上引号，都有不同效果。\n\n如何评价是否很好地检索到文件\n\n2. 词汇-文本关联矩阵\n比如在所有文档种，我们想检索A and B, but NOT C，我们可以用正则的方法，但是这对大的语料库很慢，而且很多复杂操作不能用或不灵活，而且我们还要能对文档进行排序。\n\n我们可以引入词汇-文本关联矩阵来解决上诉的那个要求，因为从矩阵种我们知道这些词汇是否在某个文档种存在。\n\n用其的二进制形式表示，若为NOT，则取反，即101111就表示Calpurnia是否在这6个文档中出现，因为前面是NOT，所以010000取反。\n\n在大文档中，100w个1000字的文档，而我们的term，有500k个，\n\n得到一个巨大的文档，其中绝大多数都是0，因此需要更好的，比如只记录1的数据结构。\n\n3.倒排索引(Inverted Index)\n若只是最普通的数据结构，因为文字出现的频率不同，每个列表包含的内容长度不同。同时因为列表是有序的，所以插入数据时候也会很麻烦。\n\n\n2.1 倒排索引的结构\n\n第一步：\n\n得到一个token和其的documentID\n\n由词汇进行排序（字母表中顺序）\n\n把同个文档中重复的token只考虑一个，映射到dictionary和postings中，同时在dictionary中记录出现频率（也就是这个词在几个文档中出现了）\n\n2.2 用倒排索引的查询处理（Query processing）\n利用倒排索引完成查询操作 AND，抽取Brutus和Caesar的postings，并合并\n\n用指针进行元素间的对比，如Brutus出现第一个在2，而caesar在1，不同，则两者小的指针向前一格。此时brutus在2，caesar也在2，两者都前进一格。\n\n伪代码\n\n2.3 布尔检索模型\n一个法律领域的检索模型例子，所以说这个方法过时，但还是在一些地方适用：\n\n\n在a and b and c的 query中，从短的开始，把全部都遍历了\n\n\n若存在多个or操作，先估计or的尺寸，先对小的（预测的，也就是直接or两端的频数相加）进行操作\n\n如果是not呢\n\n4.短语查询\n我们经常把stanford university或者san francisco当成一个短语，也就是一个不被分割的整体。这样倒排索引就解决不了了。\n\n4.1 双词索引（biwords indexes）\n第一步的尝试，就算建立一个两个词的逆序索引：\n\n在大于2的短语中，比如stanford university palo alto，可以分为stanford university AND university palo AND palo alto，这样就等同于前面单个词的逆序索引了。但是也有一个问题，上文这4个词若只找到同时连续出现才有意义（长短语），那文档不同地方分别出现就没意义了，这也就是会导致positive falsely。不过其实问题不大\n\n一种扩展的双词方法：\n对编入索引的文本进行词性标注，若词（term）为名词(N) 或冠词/介词(X)，称具有NX*N形式的词项序列为扩展双词(extended biword)，将这样扩展词对作为一个词项放入词典中。\n比如索引catcher in the rye (麦田守望者)时，N X X N，符合NX*N，将查询分析成N和X序列，将查询切分成扩展双词，在索引中查找catcher rye\n\n字典太大，存在false positive问题，但是可以作为综合的索引策略第一部分。\n\n4.2 带位置的索引\n第二种方法，带位置的索引\n\n先对比是否同时出现在一个文档，然后进行对比，此处to要为be的位置-1。\n\n这个方法同样可以用到模糊搜索上\n\n带位置信息的索引特点，储存要求大，不过很灵活而且可压缩。\n\n大小和文档长度有关，若文档不长，那和普通的posting差不多大小，若很大，比如书等大约100000词，那就是其100倍。\n\n总的来说，比无位置信息大2~4倍，不过仍是原文本的35%~50%，无位置信息的大致是10%\n\n一种将biword和位置信息的索引结合的办法，省时，但费内存。\n\n5.排序检索\n普通的布尔检索没能满足用户的需要，尤其是同时返回上千个结果时候，这是我们需要对索引进行排序，给用户最早看到最重要的内容。\n\n\n5.1 Jaccard系数\n\n例子：\n\n但也存在问题，比如没用单词的出现次数，而且标准化的方式不大对\n\n5.2 加权词频\n\n但是这样的模型没有位置信息，这样 A is better than B和B is better than A的词袋模型其实是一样的：\n\nintuition是词频和相关性是相关的，但不是线性的\n\n评分就\n\n5.3 逆文档频率加权（Invrse document frequency weighting/IDF）\nintuition: 检索中，不常见的词应当有更多信息，应当赋予更高的权重。像it, and 这类停用词，基本没有什么信息。\n\n所以使用词在多少文档中出现，来表示其是否常见。\n\n\nd\nf\nt\ndf_{t}\ndft 表示词在多少文档中出现，N表示语料库中的文档个数，则\ni\nd\nf\nt\n∈\n[\n0\n,\nl\no\ng\n10\nN\n]\nidf_{t}\\in[0,log_{10}N]\nidft ∈[0,log10 N]，若词在每个文档中都出现，那其的逆文档频率/\ni\nd\nf\nidf\nidf为0。\n\n且若语料库是不变的，那么我们得到的idf也是不变的，是一个对应每个词的值\n\n单词检索时候，idf只是一个点值（scalar），对检索没影响。但是在多个单词的索引时，idf可以给诸如capricous person两个词赋权，少见的前者赋予更高的权重。\n\ntry和insurance虽然出现的频率都差不多，但是try出现地很广，而insurance出现的文章较少，因此虽然总频数差不多，根据idf的原理，因赋予insurance更高权重\n\n6.TF-IDF\nTF-IDF是信息检索领域中最重要的加权方法之一。第一个系数\n1\n+\nl\no\ng\nt\nf\nt\n,\nd\n1+logtf_{t,d}\n1+logtft,d 表示的是前文对词出现频率的加权方法log-frequency，其大小与词出现次数成正相关，而后方的\nl\no\ng\n10\n(\nN\n/\nd\nf\nt\n)\nlog_{10}(N/df_{t})\nlog10 (N/dft )指的是单词在多少文档出现的反比，也就是前文的idf\n\n总得分，所有单词的tf-idf加合\n\n7. 向量空间模型\n前文我们把文档当作了一个向量，这是很稀疏的向量，同时占据了很大的内存空间。\n\n在查询处，我们也将问题，转换为向量，并于文档进行相似度对比，排序。相似度，约等于距离的倒数（越近越相似）\n\n但通常使用的欧式距离有很多问题，比如向量的长度对距离大小影响很大，如下图，尽管q和d2看似最相近，但是因此向量长度，导致查询向量q和d1或d3最相似。\n\n取而代之，我们可以使用cosines来计算，其在\n[\n0\n,\nπ\n]\n[0,\\pi]\n[0,π]中为减函数（值从1到-1），能满足我们的需求\n\nL2 正则，使文档文档大小的影响变小。\n‘若文档和查询向量都已标准化，直接使用下列式子\n\nemmm，好像也可以用log词频来替代词频\n\n8.TF-IDF的cosine得分\n综上所述，我们有很多种方法来做加权，这就像是一种组合。若我们使用log词频，逆文档词频加上L2 norm，则我们用的为ltc（smart notation），log词频适应于长文档\n\n计算实例，注意，标准化时候，若前方使用的是加权词频，则标准化时候使用的就是加权后的词频。","date":"2018年12月25日 17:05:15"}
{"_id":{"$oid":"5d36a8c26734bd8e681d5e4e"},"title":"自然语言处理入门以及TensorFlow官网教程Vector Representations of words简介","author":"玛莎鱼","content":"前言\nTensorFlow官网教程 Vector Representation of words主要是介绍了谷歌2013开源的 word2vec 工具包中的两个模型 CBOW(Continuous Bag-of-words Model)和 Skip-gram 模型（Continuous Skip-gram Model）。\n在讲具体的模型之前，先要介绍一点自然语言处理的基础知识\n什么是语言模型？\n通俗的讲，语言模型就是某种自然语言的模型，它用来判断给定的语句是否属于该自然语言范畴，即判断语句是否符合该自然语言所属的语义、语法和逻辑。\n那放到机器学习里该怎样做呢？\n首先进行一些定义操作。\n假设某自然语言为Ω，给定一个属于该自然语言的文本数据集，记为T，T就是自然语言Ω的样本集。然后我们把文本按单词拆解，拆解后所有单词组成的集合叫做语料库，记为C，再把语料库中的单词去重复，称为词典，记为D。\n\n那我们来简单算一下，我们事先要统计多少个值？\n但是这类算法有个问题，就是避免不了还是要提前反复扫描文本，并且存储统计值，而且对于样本集没有遇见的情况，统计的概率为0，因此对没有遇到的情况泛化能力很差。\n什么是神经概率语言模型？\n神经概率语言模型（Neural Probabilistic Languange Model）的想法很简单，就是使用神经网络来拟合函数F(*)。\n该神经网络包含四层结构，输入层，映射层、隐层和输出层。\n输入层神经元的个数是 context(w)的单词数量，一般定义的context(w)是固定长度，对于context(w)不足固定长度的情况，使用填充向量进行填充，此处按下不表。\n映射层负责将输入层的输入映射为词向量，这里插播一下什么是词向量。\n什么是词向量？\n词向量是 vector representation of words 又叫做 word embeddings，就是把语料库中的单词映射为固定长度的向量的技术。\n由于机器只能处理二进制数据，不能直接识别和处理文本字符，因此我们首先要将文本数据进行编码。\n传统的编码方式叫做one-hot-reprensentation。实现方式很简单，假设你的语料库中的词典数量为n，则将这n个词按一定的顺序排列。每次词对应一个序号i，然后将词表示为长度为n的向量，该向量的第i个元素为1，其余元素全部为0。\n这个方式虽然简单，但存在很多问题：\n1、向量的长度n等于语料库的词汇量，而一般的语料库的词汇量成千上万的，都非常大，所以这样构造的数据集维度很大，过于稀疏，容易造成维数灾难。\n2、这种做法无法表示出单词和单词之间的关联性。比如dog和cat应该是非常类似的两个单词，但被映射为与其他单词没有区别差异的0,1组成的向量\n因此 word embeddings 的目的在于将所有的词汇映射到m维空间中，将语法或者语义相近的词表示为空间中邻近的点。m的大小一般可以人为指定。word embedding一般是所有自然语言处理过程的第一步，也是最重要的一步。\n我们再把话题转回到神经概率语言模型中来。\n映射层负责将文本字符映射成词向量，至于怎样实现映射的，事先是不知道的，我们需要把这种映射关系也放到神经网络中进行训练（将映射矩阵定义为未知参数）。因此词向量变为了神经网络语言模型的附属产物。\n隐层是全连接结构，神经元的数量可以人为指定，一般还会对隐层的输出再作用一个tanh激活函数。\n假设训练集的词典数目为N，则定义输出层的神经元个数为N，输出层的神经元与词典D中的单词一一对应。输出层也是全连接结构。因此整个神经网络的计算过程如下\n输入层：Input\n映射层：P = Prpject(Input)\n隐层：H = tanh(W*P +bias_p)\n输出层: Output = U*H + bias_o\nW、U是网络权重，bias_p、bias_o是偏置项\nCBOW和Skip-gram模型就是在此基础上衍生出来的。\nCBOW是用上下文context(w) 预测目标 w，而skip-gram则是逆向操作，用目标 w预测 context(w)。但二者的原理相同，因此这里以CBOW为例进行介绍。\nCBOW模型\n神经网络的结构保持不变，让我们换个角度来看待问题。\n在神经概率语言模型中，我们把问题看作是一个N分类问题，使用softmax regression进行分类，而现在我们现在把问题看成是N个二分类问题，每个单词 w_i∈D 对应一个二分类问题\ny = 1：某context input对应的目标单词是该单词\ny = 0：某context input对应的目标单词不是该单词\n\n\n因此在每一个样本对应的似然函数中计算项由原先的N降低为k+1，k的大小一般由人为指定，k\u003c\u003c N，从而大大减少了计算量\nTensorflow中Skip-gram模型的代码实现\n我们以word2vec_basic.py为例简单讲解一下语言模型的代码实现。\nword2vec_basic.py使用的模型是Skip-gram，具体的实现和之前的神经网络教程区别不大，我们这边只讲讲不同的地方。\n不同的地方在于：1、输入数据的构造；2、映射层的构造 3、目标函数的定义\n1、输入数据的构造\n输入数据的构造负责将自然语言文本转换成计算机能识别的数据。这是初级翻译操作，还不是word embedding。\n我们将字典D中的N个单词进行编号，编号范围0~(N-1)，编号方式可以任意选择。给定一段自然语言文本作为输入，假设文本固定长度为n，将其拆解为单词w_1,w_2,…,w_n，然后将每个单词对应的编号feed给tensor placeholder: train_inputs（shape = [batch_size, n]）\n2、映射层的构造\n创建映射矩阵tensor variable: embeddings, 其shape = [N, M]。然后对该矩阵进行随机初始化。\nM为人为规定的词向量的长度，则映射矩阵的embeddings[i, :]对应的是第i个单词的词向量。embeddings是variable类型的对象，属于神经网络的待优化参数。\n调用embed = tf.nn.embedding_lookup(embeddings, train_inputs)实现词向量映射。\n返回的embed是映射层的输出：输入数据的词向量映射结果，shape = [batch_size, n, M]\nembed[i, j, :] = embeddings[ train_inputs[i, j ], : ]\n可以理解为embeddings是词向量字典，train_inputs是索引，将train_inputs中每个元素替换为该元素值对应的embeddings中相应序号的向量。\n3、目标函数的定义\nloss = tf.reduce_mean(tf.nn.nce_loss(nce_weights, nce_biases, embed, train_labels, num_sampled, vocabulary_size))\n这里使用的是NEC loss是Negative Sampling的近似替代。\nnce_weights、 nce_biases：输出层的权重和偏置项\nembed：映射层的输出，PS：在这个例子中，神经网络没有隐层。\ntrain_labels ：训练集的真实标注\nnum_sampled 指定随机负采样 negative samplings的数量\nvocabulary_size ：词典数量，也是输出层神经元的数量。\n然后剩下的工作就交给tf.nn.nce_loss来完成了。\n由此可见，虽然CBOW和skip-gram模型稍微有点复杂，但在实际的代码实现中tensorflow已经把大部分的工作都提前封装好了，我们只需要一行简单的代码调用即可以实现。但理解一下它内部的原理肯定还是益处多多的。\n注：自然语言处理入门级小白，上述如有不妥之处，欢迎批评指正","date":"2016年09月29日 15:03:37"}
{"_id":{"$oid":"5d36a8c36734bd8e681d5e50"},"title":"自然语言处理（一 神经网络背景介绍）","author":"zchenack","content":"神经网络NLP\n神经网络结构\n文本特征表示\n前馈神经网络\n损失函数\nCNN应用于文本\nRNN\nRecursive NN\n神经网络NLP\n对于自然语言处理技术，传统机器学习算法例如SVM、LR等，对映射到高维空间的文本特征进行处理，大部分应用在文本分类、情感分析等。近年来，一些非线性模型在自然语言处理来领域取得了极大的成功，这里简单介绍一些神经网络的背景知识以及在文本处理中的应用。\n神经网络结构\n常用于自然语言处理领域的神经网络结构包括：Feed-Forward network, Recurrent network, Recursive network等。\n其中Feed Forward神经网络包括全连接的MLP、具有卷积层和池化层的CNN网络。RNN网络包括S-RNN、LSTM、GRU等，Recursive network是结合树模型的神经网络。\n文本特征表示\n使用神经网络最重要的就是明确输入输出是什么，输入需要量化成数学表示的向量或矩阵，便于神经网络计算。对于文本而言，最传统的表示方式是One-hot表示，即建立一个N维的词典，然后文本中出现某个单词，就用1表示，从而表示这个特征。但这种表示方式不能明确词语之间的相似性，另外还导致特征空间过大，不便于计算。现在出现了各种embedded的方法，就是把主要特征映射到某一特征空间，用向量表示，相似词语在该空间中距离较近，能够明确词语之间的相似性。常见的表示工具有谷歌的word2vec工具。\n得到文本的特征表示后，使用神经网络进行文本分类步骤：\n（1）针对文本需要划分的输出类，进行相关特征提取；\n（2）针对提取的特征（可能是词语、词性、语言模型等），将其数学表示、向量化；\n（3）把所有特征的向量表示拼接；\n（4）将拼接得到的特征表示输入到神经网络，使用已标记的训练数据集合进行神经网络参数训练\n前馈神经网络\n全连接MLP网络\n1. 网络框架\n\n2.输入输出对应关系\n每一个计算单元都是一个感知机，感知机的输入一般表示为\n\nInput=∑wi∗xi+b\nInput = \\sum w_i*x_i +b，\n感知机的输出可以表示为：\n\nOutput=g(Input)\nOutput = g(Input)\n因此对于上图中输出可以表示为：\n\n3.常见的激励函数\n（1）Sigmoid函数\n\nδ(x)=11+e−x\n\\begin{equation} \\delta(x)=\\frac{1}{1+e^{-x}} \\end{equation}\n（2）tanh函数\n\ntanh(x)=e2x−1e2x+1\n\\begin{equation} tanh(x)=\\frac{e^{2x}-1}{e^{2x}+1} \\end{equation}\n（3）ReLU函数\n\nReLU(x)=max(0,x)\n\\begin{equation} ReLU(x) = max(0,x) \\end{equation}\n4.对于神经网络的输出进行再次变换\n例如是一个多分类问题，输出可以表示为y1,y2,…,yN，N分类的问题，想要知道最终是属于哪一类的，需要做一个Softmax。softmax函数可以表示为：\n\nsoftmax(yi)=exi∑kj=1exj\n\\begin{equation} softmax(y_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{k}{e^{x_j}}} \\end{equation}\n谁的输出最大，就取那一类为输出类\n损失函数\n对于神经网络的训练，最重要的就是要明确目标函数，训练的目标是什么，就是要使得模型尽可能地取拟合训练数据样本，目标函数就是去最小化模型输出与样本标签之间的差距。损失函数就是这一用途。常见的损失函数有：(\ny′\ny' 是模型输出，\ny\ny表示数据的真是标签)\nHinge函数：\n\nLoss=max(0,1−y∗y′)\n\\begin{equation} Loss = max(0,1-y*y') \\end{equation}\n对数损失函数\n\nLoss=log(1+exp(−(y′−y)))\n\\begin{equation} Loss = log(1+exp(-(y'-y))) \\end{equation}\n交叉熵\n\nLoss=−∑yilog(y′i)\n\\begin{equation} Loss = -\\sum{y_i}log(y'_i) \\end{equation}\nCNN应用于文本\n传统对文本的表示为CBOW(Bag of words)，没有语序的概念，只能说明拥有某个词和某个词出现的频率这样的。例如：“他不好，他相当坏”与“他不坏，他相当好”可能就区分不开。CNN的结构，就帮助模型记住了大量的局部信息，能够保存位置顺序。基本的convolution + pooling的结构如下所示：\n\n将文本编程小的phrase，然后分别训练前向神经网络，再使用pooling将多个神经网络整合输出。能够辅助记住局部文本信息。\nRNN\nRNN 在自然语言处理中得到广泛使用，保留句子序列信息。应用于机器翻译、问答系统等领域。\n\n递归调用，前一时刻的信息输出给后一个序列。\nRNN有很多种变形，例如LSTM就是解决RNN训练过程中梯度消失问题产生的长短时记忆网络，GRU就是解决LSTM训练过于复杂产生的神经网络。 Bi-RNN就是考虑未来序列信息用于增强模型的变形RNN等。\nRecursive NN\n区别于RNN，它是将句法树引入的神经网络，具体介绍参考http://www.jianshu.com/p/403665b55cd4","date":"2017年11月18日 16:39:06"}
{"_id":{"$oid":"5d36a8c36734bd8e681d5e52"},"title":"自然语言处理相关介绍","author":"nicolas_chang","content":"文章目录\n自然语言处理基本概念\n语言的数学本质\n统计语言模型\nN-Gram Model\n分词\n信息度量\n信息熵\n信息的作用\n互信息\n相对熵\n信息熵的应用 -- 决策树\nFeature Extraction and Preprocessing\nOne-hot encoding\nBag Of Words Model\nSparse Vectors\nStop-word filter\nLemmatization vs Stemming\nTF-IDF\nTF-IDF的信息论依据\n文本分类样例\nSummary\n自然语言处理基本概念\n语言的数学本质\n语言的出现是为了通信，通信的本质是为了传递信息。字母，文字，数字都是信息编码的不同单元。任何一种语言都是一种编解码算法。\n我们通过语言把要表达的意思传递出来，实际上就是用语言将大脑中的信息进行了一次编码，形成了一串文字。懂得这种语言的接收方就能够使用这种语言进行解码，然后获取到里面的信息。这就是语言的数学本质。\n统计语言模型\n机器是不懂得任何一种语言的，早期的自然语言处理方式是让计算机学习理解语言的语义，语法，然后据此判断一个句子是否合理，含义是什么。但最终证明这种研究方向和学习方式是行不通的。\n现在的自然语言处理是基于统计语言模型，它根本不需要计算机理解人类的语言，它要做的就是判断一个句子是否合理，就看这个句子在语料库中出现的概率如何。\n假定S表示某一个有意义的句子，由一连串的词\nw\n1\n,\nw\n2\n,\n⋯\n\u0026ThinSpace;\n,\nw\nn\nw_1, w_2, \\cdots, w_n\nw1 ,w2 ,⋯,wn 组成，\nn\nn\nn是句子的长度。如果想知道S在文本中出现的概率\nP\n(\nS\n)\nP(S)\nP(S)，那就需要把有史以来人类讲过的话统计一下，然后计算出出现的概率。这种方法很显然是行不通的。因此，需要一个模型来估算。由于\nS\n=\nw\n1\n,\nw\n2\n,\n⋯\n\u0026ThinSpace;\n,\nw\nn\nS = w_1, w_2, \\cdots, w_n\nS=w1 ,w2 ,⋯,wn ，那么\nP\n(\nS\n)\n=\nP\n(\nw\n1\n,\nw\n2\n,\n⋯\n\u0026ThinSpace;\n,\nw\nn\n)\nP(S) = P(w_1, w_2, \\cdots, w_n)\nP(S)=P(w1 ,w2 ,⋯,wn )，利用条件概率公式，S出现的概率等于每一个词出现的条件概率的乘积\n\nP\n(\nw\n1\n,\nw\n2\n,\n⋯\n\u0026ThinSpace;\n,\nw\nn\n)\n=\nP\n(\nw\n1\n)\n⋅\nP\n(\nw\n2\n∣\nw\n1\n)\n⋅\nP\n(\nw\n3\n∣\nw\n1\n,\nw\n2\n)\n⋯\nP\n(\nw\nn\n∣\nw\n1\n,\nw\n2\n,\n⋯\n\u0026ThinSpace;\n,\nw\nn\n−\n1\n)\nP(w_1, w_2, \\cdots, w_n) = P(w_1) \\cdot P(w_2|w_1) \\cdot P(w_3|w_1, w_2) \\cdots P(w_n|w_1, w_2, \\cdots, w_{n-1})\nP(w1 ,w2 ,⋯,wn )=P(w1 )⋅P(w2 ∣w1 )⋅P(w3 ∣w1 ,w2 )⋯P(wn ∣w1 ,w2 ,⋯,wn−1 )\n其中\nP\n(\nw\n2\n∣\nw\n1\n)\nP(w_2|w_1)\nP(w2 ∣w1 )表示在已知以一个词出现的前提下，第二个词出现的概率，以此类推，\nw\nn\nw_n\nwn 的出现概率取决于它前面所有的词。但这种条件概率的可能性太多，非常难以计算。俄国数学家马尔科夫提出了一个偷懒但是有效的做法，即马尔科夫假设模型来简化这种计算：任意一个词\nw\ni\nw_i\nwi 出现的概率只同它前面的词\nw\ni\n−\n1\nw_{i-1}\nwi−1 有关，简化后S出现的概率为：\n\nP\n(\nS\n)\n=\nP\n(\nw\n1\n)\n⋅\nP\n(\nw\n2\n∣\nw\n1\n)\n⋅\nP\n(\nw\n3\n∣\nw\n2\n)\n⋯\nP\n(\nw\nn\n∣\nw\nn\n−\n1\n)\nP(S) = P(w_1) \\cdot P(w_2|w_1) \\cdot P(w_3|w_2) \\cdots P(w_n|w_{n-1})\nP(S)=P(w1 )⋅P(w2 ∣w1 )⋅P(w3 ∣w2 )⋯P(wn ∣wn−1 )\n该公式对应的统计语言模型为二元模型(Bigram Model)。\n以上是理论，那么在实际的机器学习中是如何操作的呢？\n首先计算\nP\n(\nw\ni\n∣\nw\ni\n−\n1\n)\nP(w_i|w_{i-1})\nP(wi ∣wi−1 )，根据条件概率的定义\nP\n(\nw\ni\n∣\nw\ni\n−\n1\n)\n=\nP\n(\nw\ni\n,\nw\ni\n−\n1\n)\nP\n(\nW\ni\n−\n1\n)\nP(w_i|w_{i-1}) = \\frac{P(w_i, w_{i-1})}{P(W_{i-1})}\nP(wi ∣wi−1 )=P(Wi−1 )P(wi ,wi−1 ) ，只需估计联合概率\nP\n(\nw\ni\n,\nw\ni\n−\n1\n)\nP(w_i, w_{i-1})\nP(wi ,wi−1 )和边缘概率\nP\n(\nw\ni\n−\n1\n)\nP(w_{i-1})\nP(wi−1 )，就变得很简单。基于大量的语料库(Corpus)，只需要统计\nw\ni\n−\n1\n,\nw\ni\nw_{i-1}, w_i\nwi−1 ,wi 这对词在统计的文本中出现的次数\n#\n(\nw\ni\n−\n1\n,\nw\ni\n)\n\\#(w_{i-1}, w_i)\n#(wi−1 ,wi )以及\nw\ni\n−\n1\nw_{i-1}\nwi−1 本身在同样的文本中出现的次数\n#\n(\nw\ni\n)\n\\#(w_i)\n#(wi )，然后用这两个数分别除以语料库的大小\n#\n\\#\n#，即可得到这些词的相对频度：\nf\n(\nw\ni\n−\n1\n,\nw\ni\n)\n=\n#\n(\nw\ni\n−\n1\n,\nw\ni\n)\n#\nf(w_{i-1}, w_i) = \\frac{\\#(w_{i-1}, w_i)}{\\#}\nf(wi−1 ,wi )=##(wi−1 ,wi )\nf\n(\nw\ni\n−\n1\n)\n=\n#\n(\nw\ni\n−\n1\n)\n#\nf(w_{i-1}) = \\frac{\\#(w_{i-1})}{\\#}\nf(wi−1 )=##(wi−1 )\n然后根据大数原理，只要统计量足够，相对频度就等于概率，即\nP\n(\nw\ni\n,\nw\ni\n−\n1\n)\n≈\n#\n(\nw\ni\n−\n1\n,\nw\ni\n)\n#\nP(w_i, w_{i-1}) \\approx \\frac{\\#(w_{i-1}, w_i)}{\\#}\nP(wi ,wi−1 )≈##(wi−1 ,wi )\nP\n(\nw\ni\n−\n1\n)\n≈\n#\n(\nw\ni\n−\n1\n)\n#\nP(w_{i-1}) \\approx \\frac{\\#(w_{i-1})}{\\#}\nP(wi−1 )≈##(wi−1 )\n最终简化后，\n#\n\\#\n#约掉，因此\nP\n(\nw\ni\n∣\nw\ni\n−\n1\n)\n≈\n#\n(\nw\ni\n−\n1\n,\nw\ni\n)\n#\n(\nw\ni\n−\n1\n)\nP(w_i|w_{i-1}) \\approx \\frac{\\#(w_{i-1}, w_i)}{\\#(w_{i-1})}\nP(wi ∣wi−1 )≈#(wi−1 )#(wi−1 ,wi )\nN-Gram Model\n马尔科夫假设中只定义和前面一个词有关，称之为二元模型。当和其前面N个词有关的情况，则成为N元模型，这就是文本处理中经常见到的N-Gram Model。实际应用最多的是N=3的三元模型，之所以不用更高阶的原因主要是：\n空间复杂度。N元模型的大小是N的指数，即\nO\n(\n∣\nV\n∣\nN\n)\nO(|V|^N)\nO(∣V∣N)，V为一种语言字典的词汇量\n时间复杂度。N元模型的速度也是N的指数，即\nO\n(\n∣\nV\n∣\nN\n−\n1\n)\nO(|V|^{N-1})\nO(∣V∣N−1)\n因此，N不能太大，而且N从1-2,2-3的效果提升显著，但是3-4时效果就不明显了。而且N即使更高阶，也无法覆盖所有的语言，因为语言的上下文的相关性跨度可能非常大，比如跨段落，这是马尔科夫假设无法解决的。\n分词\n统计语言模型是建立在词的基础上的，词是表达语义的最小单位。对于西方拼音语言来说，词之间是有分界符，因此分词很简单。但是对于东方语言，词之间没有分界符，因此，进行自然语言处理前，首先要对句子进行分词。\n查字典法\n把句子从左到右扫描，遇到字典里面有的词就标识出来，遇到复合词就找最长匹配，遇到不认识的字串就分割成单字词。比如“上海大学”，“上”是单字词，遇到“海”时，发现可以和前面的“上”组成更长的词，分割点就放在“上海”后面。后面它还能发现“上海大学”其实是个复合词，那么最后把分割点再移到“大学”后面。\n统计语言模型分词法\n虽然查字典法可以解决70-80%的分词问题，但是中文中有很多二义性的词语，比如“发展中国家”，按照查字典的方法，得到的分词结果是“发展-中国-家”，而正确的分词结果应该是“发展-中-国家”。又比如长匹配带来的问题，“北京大学生”，正确的应该是“北京-大学生”，而不是“北京大学-生”。\n最终解决这个问题的方法还是依赖统计语言模型，原理如下。\n假设一个句子S可以有以下几种分词方法：\n\nA\n1\n,\nA\n2\n,\n⋯\n\u0026ThinSpace;\n,\nA\nx\nA_1, A_2, \\cdots, A_x\nA1 ,A2 ,⋯,Ax\n\nB\n1\n,\nB\n2\n,\n⋯\n\u0026ThinSpace;\n,\nB\ny\nB_1, B_2, \\cdots, B_y\nB1 ,B2 ,⋯,By\n\nC\n1\n,\nC\n2\n,\n⋯\n\u0026ThinSpace;\n,\nC\nz\nC_1, C_2, \\cdots, C_z\nC1 ,C2 ,⋯,Cz\n那么最好的一种分词方法，应该保证分词后的句子出现的概率最大。如果\nA\n1\n,\nA\n2\n,\n⋯\n\u0026ThinSpace;\n,\nA\nx\nA_1, A_2, \\cdots, A_x\nA1 ,A2 ,⋯,Ax 最好，那么需要满足\n\nP\n(\nA\n1\n,\nA\n2\n,\n⋯\n\u0026ThinSpace;\n,\nA\nx\n)\n\u0026gt;\nP\n(\nB\n1\n,\nB\n2\n,\n⋯\n\u0026ThinSpace;\n,\nB\ny\n)\nP(A_1, A_2, \\cdots, A_x) \u0026gt; P(B_1, B_2, \\cdots, B_y)\nP(A1 ,A2 ,⋯,Ax )\u003eP(B1 ,B2 ,⋯,By )，且\nP\n(\nA\n1\n,\nA\n2\n,\n⋯\n\u0026ThinSpace;\n,\nA\nx\n)\n\u0026gt;\nP\n(\nC\n1\n,\nC\n2\n,\n⋯\n\u0026ThinSpace;\n,\nC\nz\n)\nP(A_1, A_2, \\cdots, A_x) \u0026gt; P(C_1, C_2, \\cdots, C_z)\nP(A1 ,A2 ,⋯,Ax )\u003eP(C1 ,C2 ,⋯,Cz )\n分词粒度，对于不同的应用场景，可以有不同的分词粒度。比如机器翻译中，粒度大效果好。而在网页搜索中，粒度小的效果好。\n以统计预言模型为基础的中文分词基本可以看做是一个已经解决了的问题，提升空间微乎其微。分词器好坏的差别在于数据的使用工程实现的精度。\n信息度量\n信息是一个比较抽象的概念，比如50万字的《史记》信息量是多少？直到香农1948年提出“信息熵”的概念，才解决了信息的度量问题。\n信息熵\n一条信息的信息量与其不确定性有着直接的关系。比如2018年世界杯冠军是谁，不确定性就大，因此需要了解大量的信息才能推断。又比如，中国队能否进入世界杯，不确定性就很小，基本不需要什么信息量就能确定。前面的信息量大，后面的信息量小。因此，可以认为，信息量就等于不确定性的多少。\n香农使用bit来度量信息量。比如32只球队比赛，每个球队夺冠的概率相等，那么谁是冠军的信息量是5bit。它的算法如下：\nH\n=\n−\n(\np\n1\n⋅\nlog\n⁡\np\n1\n+\np\n2\n⋅\nlog\n⁡\np\n2\n+\n⋯\n+\np\n32\n⋅\nlog\n⁡\np\n32\n)\nH = -(p_1 \\cdot \\log p_1 + p_2 \\cdot \\log p_2 + \\cdots + p_{32} \\cdot \\log p_{32})\nH=−(p1 ⋅logp1 +p2 ⋅logp2 +⋯+p32 ⋅logp32 )\n其中，\np\n1\n,\n⋯\n\u0026ThinSpace;\n,\np\n32\np_1, \\cdots, p_{32}\np1 ,⋯,p32 分别是这32支球队夺冠的概率，H为信息熵（Entropy），单位是bit。当32支球队的夺冠概率相等时，H为5bit。\n对于任意一个随机变量X（比如得冠的球队），它的信息熵定义如下：\n\nH\n(\nx\n)\n=\n−\n∑\nx\n∈\nX\nP\n(\nx\n)\nlog\n⁡\nP\n(\nx\n)\nH(x) = -\\sum_{x \\in X}P(x) \\log P(x)\nH(x)=−x∈X∑ P(x)logP(x)\n变量的不确定性越大，熵就越大。比如P(x)越小，熵就越大。\n案例：一本50万字的中文书平均信息量为多少？\n中文常用汉字7000左右，假如每个汉字概率相等，那么大约每个汉字的信息熵需要13bit。但是汉字的使用频率是不等的，基本10%左右的常用字占据整个文本的95%，那么每个汉字的信息量10bit就够了。如果再考虑上下文，每个汉字的信息熵5-6bit就够了。所以一本50万字的书的信息量大约是250万-300万bit。\n但这只是一个平均数，同样长度的书所含的信息量是不同的。如果一本书重复的内容很多，它的信息量就会很少，冗余度就很大。而且不同语言的冗余度差别也很大，汉语的冗余度是比较小的，一般认为汉语是最简洁的语言。\n信息的作用\n信息是消除系统不确定性的唯一方法。假如一个系统的不确定性为\nU\nU\nU，从外部消除这个不确定性的唯一方法是引入信息\nI\nI\nI，如果\nI\n\u0026gt;\nU\nI \u0026gt; U\nI\u003eU，那么就消除了不确定性，如果\nI\n\u0026lt;\nU\nI \u0026lt; U\nI\u003cU，只是部分消除了，但仍遗留了新的不确定性：\nU\n′\n=\nU\n−\nI\nU^{\\prime} = U - I\nU′=U−I。\n自然语言处理的过程就是一个消除不确定性的过程。比如在一元模型就是通过单个词的概率分布消除不确定性因素，二元模型使用了上下文信息，就能消除更多的不确定性，提高准确率。通过上下文信息可以消除不确定性可以用数学的方法证明。这里使用了条件熵。\n假定X和Y是两个随机变量，如果知道了X的随机分布\nP\n(\nx\n)\nP(x)\nP(x)，那么也就知道了X的熵：\n\nH\n(\nx\n)\n=\n−\n∑\nx\n∈\nX\nP\n(\nx\n)\nlog\n⁡\nP\n(\nx\n)\nH(x) = -\\sum_{x \\in X}P(x) \\log P(x)\nH(x)=−∑x∈X P(x)logP(x)\n假定还知道Y的一些情况，包括Y和X一起出现的概率（联合概率）以及Y在取不同值的前提下X的概率分布（条件概率）。则在Y的条件下的条件熵为：\n\nH\n(\nX\n∣\nY\n)\n=\n−\n∑\nx\n∈\nX\n,\ny\n∈\nY\nP\n(\nx\n,\ny\n)\nlog\n⁡\nP\n(\nx\n,\ny\n)\nH(X|Y) = - \\sum_{x \\in X, y \\in Y}P(x, y) \\log P(x, y)\nH(X∣Y)=−∑x∈X,y∈Y P(x,y)logP(x,y)\n数学上可以证明\nH\n(\nx\n)\n≥\nH\n(\nX\n∣\nY\n)\nH(x) \\ge H(X|Y)\nH(x)≥H(X∣Y)，也就是在知道了Y的信息后，关于X的不确定性降低了。那么由此可以得出二元模型的不确定性小于一元模型。同理，三元模型的不确定性小于二元模型。\n总之，信息的作用就是消除不确定性。\n互信息\n上节讲到的有上下文关系的随机变量能够帮忙消除不确定性，但是这个有关系是个模糊的说法，能不能把这种关系也量化呢？香农提出的互信息就是对两个随机变量的相关性做的度量量化。\n假定有两个随机事件X和Y，它们的互信息定义：\nI\n(\nX\n;\nY\n)\n=\n∑\nx\n∈\nX\n,\ny\n∈\ny\nP\n(\nx\n,\ny\n)\nlog\n⁡\nP\n(\nx\n,\ny\n)\nP\n(\nx\n)\nP\n(\ny\n)\nI(X;Y) = \\sum_{x \\in X, y \\in y}P(x, y) \\log{\\frac { P(x, y)}{P(x)P(y)}}\nI(X;Y)=x∈X,y∈y∑ P(x,y)logP(x)P(y)P(x,y)\n实际上，\nI\n(\nX\n;\nY\n)\n=\nH\n(\nX\n)\n−\nH\n(\nX\n∣\nY\n)\nI(X;Y) = H(X) - H(X|Y)\nI(X;Y)=H(X)−H(X∣Y)，就是上节里面提到的X的熵与条件熵的差。所谓两个事件相关性的量化度量，就是在了解其中一个Y的前提下，对消除另一个X不确定性所提供的信息量。互信息的范围是0到min(H(X), H(Y))，当X和Y完全相关时，取值为1，完全无关时，取值为0\n相对熵\n也称为交叉熵（Kullback-Leibler Divergence），也用来衡量相关性，但和变量的互信息不同，它用来衡量两个取值为正数的函数的相似性，它的定义如下：\n\nK\nL\n(\nf\n(\nx\n)\n∣\n∣\ng\n(\nx\n)\n)\n=\n∑\nx\ni\nn\nX\nf\n(\nx\n)\n⋅\nlog\n⁡\nf\n(\nx\n)\ng\n(\nx\n)\nKL(f(x)||g(x)) = \\sum_{x in X}f(x) \\cdot \\log {\\frac{f(x)}{g(x)}}\nKL(f(x)∣∣g(x))=xinX∑ f(x)⋅logg(x)f(x)\n公式不重要，结论记住就好：\n对于两个完全相同的函数，它们的相对熵等于零\n相对熵越大，两个函数差异越大，反之越小\n对于概率分布或者概率密度函数，如果取值均大于零，相对熵可以度量两个随机分布的差异性\n相对熵之前用于信号处理，两个随机信号，相对熵越小，说明两个信号越接近。后来也把它用来衡量两端信息的相似度，比如一篇文章照抄或者改写另一篇，那么这两篇文章中的词频分布的相对熵就非常小。\n信息熵的应用 – 决策树\n决策树由结点（node）和有向边（directed edge）组成。结点有两种类型：内部结点（internal node）和叶结点（leaf node）。内部结点表示一个特征，叶结点表示一个类。\n用决策树分类，从根结点开始，对实例的每一个特征进行测试，根据测试结果，将实例分配到其子结点，每个子结点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直到达到叶结点，最后将实例分配到叶结点。\n下图为决策树示意图，圆和方框分别表示内部结点和叶结点：\n\n如下14个训练样本，其中8只猫，6只狗。采用决策树算法，如何高效地分类。\n\n我们的目标是希望每一次的特征测试分出来的子类要么包括所有的猫或者所有的狗，而不是两者都有。每一次的测试都能最大的降低不确定性（这样就能提高决策树的效率），而不确定性的度量就是用信息熵。\nH\n(\nx\n)\n=\n−\n∑\nx\n=\n1\nN\nP\n(\nx\ni\n)\nlog\n⁡\nP\n(\nx\ni\n)\nH(x) = -\\sum_{x=1}^{N}P(x_i) \\log P(x_i)\nH(x)=−∑x=1N P(xi )logP(xi )\n对于训练样本，除了知道里面有6只狗和8只猫，其他信息一无所知，那么对于猫狗分类这件事的信息熵为：\nH\n(\nx\n)\n=\n−\n(\n6\n14\nl\no\ng\n(\n6\n14\n)\n+\n8\n14\nl\no\ng\n(\n8\n14\n)\n)\n=\n0.98523\nH(x) = -(\\frac {6}{14}log(\\frac{6}{14}) + \\frac {8}{14}log(\\frac{8}{14})) = 0.98523\nH(x)=−(146 log(146 )+148 log(148 ))=0.98523\n现在有3个特征：play fetch，is grumpy，favorite food，我们希望选择一个用来测试，能够最大的降低信息熵（不确定性）。比如，选择play fetch，分类后结果如下：\n决策树经常使用上图可视化的方式来查看分类逻辑和效果：在根节点中的信息熵是0.985，然后我们使用“Play fetch”这个特征分成2类后，一类中有9个样本，其中7只猫2只狗；另一类5个样本，其中1只猫4只狗。结果不是很理想，每个子类里面都同时包括了猫和狗。那么对于这两个子类，其信息熵为：\nH\n(\nx\n)\n=\n−\n(\n7\n9\nl\no\ng\n(\n7\n9\n)\n+\n2\n9\nl\no\ng\n(\n2\n9\n)\n)\n=\n0.7642\nH(x) = -(\\frac {7}{9}log(\\frac{7}{9}) + \\frac {2}{9}log(\\frac{2}{9})) = 0.7642\nH(x)=−(97 log(97 )+92 log(92 ))=0.7642\n\nH\n(\nx\n)\n=\n−\n(\n1\n5\nl\no\ng\n(\n1\n5\n)\n+\n4\n5\nl\no\ng\n(\n4\n5\n)\n)\n=\n0.7219\nH(x) = -(\\frac {1}{5}log(\\frac{1}{5}) + \\frac {4}{5}log(\\frac{4}{5})) = 0.7219\nH(x)=−(51 log(51 )+54 log(54 ))=0.7219\n如果我们使用is grumy作为特征测试，结果为：\n使用cat food：\n\n那么对于这3种结果，哪一种性能最好呢？\n实际上，这是一个最优特征选择的问题。这里，我们引入information gain(信息增益)来解决这个问题。\nInformation Gain：特征A对训练集D的信息增益g(D, A)，定义为集合D的信息熵H(D)与在特征A给定的条件下D的条件熵H(D|A)之差，即：\ng\n(\nD\n,\nA\n)\n=\nH\n(\nD\n)\n−\nH\n(\nD\n∣\nA\n)\ng(D, A) = H(D) - H(D|A)\ng(D,A)=H(D)−H(D∣A)\n信息熵H(D)表示对于数据集D进行分类的不确定性。条件熵H(D | A)表示在特征A给定的条件下对数据集D进行分类的不确定性，它们的差即为信息增益。它表示：由于特征A而使得对数据集D的分类不确定性减少的程度。显然，对于数据集D而言，信息增益依赖特征，不同的特征具有不同的信息增益。信息增益大的特征具有更强的分类能力。\n根据信息增益的特征选择方法是：对训练集（或者子集）D，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。\n之前已经讲过，信息熵H(X)与条件熵H(X|Y)之差为互信息：\nI\n(\nX\n;\nY\n)\n=\nH\n(\nX\n)\n−\nH\n(\nX\n∣\nY\n)\nI(X;Y) = H(X) - H(X|Y)\nI(X;Y)=H(X)−H(X∣Y)。\n实际上，决策树中的信息增益等价于训练数据集中类与特征的互信息。决策树算法其实就是使用互信息来选择最优特征。\n下图即为各个特征条件下的信息增益：\n通过计算IG，我们发现cat food这个特征IG最大，所以它是最有的特征。\n注意：在这里，在计算IG时，\nI\nG\n=\nP\na\nr\ne\nn\nt\n′\ns\nE\nn\nt\nr\no\np\ny\n−\nW\ne\ni\ng\nh\nt\ne\nd\nA\nv\ne\nr\na\ng\ne\nIG = Parent\u0026#x27;s Entropy - Weighted Average\nIG=Parent′sEntropy−WeightedAverage\n以上只是选择除了第一级的最优特征，后面第二级的最右特征选择，方法一样，需要在剩余的特征中，递归找到IG最大的那个特征\n但是这里IG最大值有两个，在ID3算法中，它是随机选择一种的。后面第三级，第四级等等都是采用这种决策方法。最终我们会得到如下一张决策树的图：\n决策树算法C4.5是基于ID3的变种，它可以修建分支，也是最流行的决策树方法。\n决策树使用了信息论中的信息熵，互信息，在接下里的文本处理中还将看到对于交叉熵的使用。\n代码实例：internet ads\nFeature Extraction and Preprocessing\nOne-hot encoding\n计算机是读不懂人类的文字的，它本质上只能做快速计算。为了让计算机能够处理文字，就要求我们先把文字变成一组可计算的数字，然后设计一个算法来算出这些文字的关系。\nOne-Hot编码，又称为一位有效编码，主要是采用N位状态寄存器来对N个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候只有一位有效。该编码用在文字处理上面，举例如下：\nfrom sklearn.feature_extraction import DictVectorizer onehot_encoder = DictVectorizer() instances = [ {'city': 'Shanghai'}, {'city': 'Beijing'}, {'city': 'Shenzhen'} ] print(onehot_encoder.fit_transform(instances).toarray()) [[0. 1. 0.] [1. 0. 0.] [0. 0. 1.]]\n从上面输出的feature vector可以看出：\n通过DictVectorizer，将字符串转换成了一个字典向量\n3个城市的名字作为了3个元素，但是城市名字的顺序不是按照定义排序的，而是按照字母顺序排序的。分别为：‘Beijing’ ‘Shanghai’ ‘Shenzhen’\n表示‘Shanghai‘时，其’对应位置的元素为1，[0, 1, 0]\n通过这个简单的例子说明，文本在用来机器学习前，一定要先将其向量化，将人类可读的文本，转换为机器可算的数字。\n这里只是处理几个简单的变量，那么计算机如何处理大量的文本呢？这就需要通过下面的模型来实现。\nBag Of Words Model\n我们已经知道，文本在机器学习前，一定要将其向量化，而且需要尽可能多得保证文本的原意。其中最为通用的一个模型是Bag of words，中文为词袋模型。\n该模型可以看做是one-hot编码的扩展，特点如下：\n忽略词序\n忽略语法\n创建一个特征向量，里面包含了文本中的每个单词\n词袋模型的动机是为了说明包含相似单词的文本应该有着相似的意思，它可以高效地处理文本分类并能从编码后的向量恢复对应的文本。\nCorpus：包含所有文本的集合称为语料库\nVocabulary：由语料库中所有不重复的单词组成，称为词表\nDimension：组成feature vector的元素数量称为维度\nfrom sklearn.feature_extraction.text import CountVectorizer corpus = [ 'UNC played Duke in basketball', 'Duke lost the basketball game' ] vectorizer = CountVectorizer() print(vectorizer.fit_transform(corpus).todense()) print(vectorizer.vocabulary_) [[1 1 0 1 0 1 0 1] [1 1 1 0 1 0 1 0]] {'unc': 7, 'played': 5, 'duke': 1, 'in': 3, 'basketball': 0, 'lost': 4, 'the': 6, 'game': 2}\n上例中：\ncorpus中包含了两个文本\n每个文本的feature vector采用的one-hot编码，该feature vector的维度是8\nVocabulary为{‘unc’: 7, ‘played’: 5, ‘duke’: 1, ‘in’: 3, ‘basketball’: 0, ‘lost’: 4, ‘the’: 6, ‘game’: 2}，单词后面的数字表示该单词在feature vector中的位置。由此词表，它可以快速地将feature vector恢复为文本。\n使用CountVectorizer，它默认会自动最小化字母，自动去重，自动分词，自动去空格等符号，改过程称为Tokenization，即将string分段为tokens。Tokens大多数为单词，当然也可以分段为短语(可以包括标点符号)，也支持自定义正规表达式分段\nSparse Vectors\nSparse vectors：稀疏矩阵，这个在使用词袋模型时很常见。比如\ncorpus = [ 'UNC played Duke in basketball', 'Duke lost the basketball game', 'I ate a sandwich' ] [0 1 1 0 1 0 1 0 0 1] [0 1 1 1 0 1 0 0 1 0] [1 0 0 0 0 0 0 1 0 0]\n新加了一条文本，在意思上面与前两条没有关系，它的向量与上面两条也无交集。这也体现了词袋模型的特点：能够发现相似意思的文本。但同时这也带来一个问题，向量中出现了大量的零元素。\n如果语料库有几千万条文本，它的feature vector维度可能也有几百万，那么语义无关的文本肯定也会出现大量的零元素。这种含有大量零元素的高维矩阵称为稀疏矩阵。它带来两个严重的问题：\n占用大量的memory\n维数灾难，随着维度地不断增加，这就要求样本数据需要更多有效的feature保持文本的意思，否则会被稀释掉；而且高维度的情况下，距离计算也会困难。\n维数灾难是机器学习中常见的问题，也有一些降维的方案，接下来介绍几个文本降维的方法。\n代码实例：20NewsGroups_Classification\nStop-word filter\nStop word：停用词。这个过滤原理很简单，比如针对英文和中文，各自维护了一个停用词列表，里面收集了一些不能表示语义的词语，比如，a, the, I, you, do, be, is, will, …。中文也类似，比如，我，你，她，啊，呢 …通过这种方法，可以达到降维的目标。比如：\ncorpus = [ 'UNC played Duke in basketball', 'Duke lost the basketball game', 'I ate a sandwich' ] vectorizer = CountVectorizer(stop_words='english') data = vectorizer.fit_transform(corpus) print(data.todense()) print(data.shape) print(vectorizer.vocabulary_) {'unc': 7, 'played': 5, 'duke': 2, 'basketball': 1, 'lost': 4, 'game': 3, 'ate': 0, 'sandwich': 6}\n你会发现，它去掉了’I’, ‘a’, ‘in’, ‘the’。现在也能支持自定义停用词，用来应对不同的应用场景。\nLemmatization vs Stemming\nStop word是一个很简单的降维策略，但是stop word list里面只有几百个单词。对于一个很大的语料库来讲仍然是杯水车薪。下面针对英文文本，还有2个相似的降维方法：\nLemmatization：词性还原\nStemming：词根化\n这是两个相似的用来降维的策略。一篇高维的文档向量里面，可能有很多同一个词的各种时态，但是它们也是作为feature vector里面一个独立的元素。Stemming和Lemmatization的作用就是将这些词简化成为一个向量元素。\nfrom sklearn.feature_extraction.text import CountVectorizer corpus = [ 'He ate the sandwiches', 'Every sandwich was eaten by him' ] vectorizer = CountVectorizer(stop_words='english') data = vectorizer.fit_transform(corpus) print(data.todense()) print(vectorizer.vocabulary_) [[1 0 0 1] [0 1 1 0]] {'ate': 0, 'sandwiches': 3, 'sandwich': 2, 'eaten': 1}\n这两条文本有着相似的意思，但是他们的feature vector却没有任何交集。理想情况下，有着相似意思的文本，应该有相似的feature vector。而造成这个问题的原因就是eat和sandwich的不同形态被认为是不同的feature。解决这种问题就可以使用Lemmatization和Stemming。\nLemmatization的过程就是将单词还原为其原型的一个过程，它需要依赖词典资源，比如WordNet，来恢复出一个正确的单词原型\nStemming的目标和Lemmatization一样，它就是直接地去掉所有看起来是词缀样式的东西，而不在意恢复后的原型是不是一个有效的单词，它不依赖词典，只是依赖规则。\n对于上面的文本，处理后分别为：\nStemmed: [['He', 'ate', 'the', 'sandwich'], ['Everi', 'sandwich', 'wa', 'eaten', 'by', 'him']] Lemmatized: [['He', 'eat', 'the', 'sandwich'], ['Every', 'sandwich', 'be', 'eat', 'by', 'him']] [1 1] [1 1]\n由上述结果可知，通过Stemming和Lemmatization，除了可以降低feature vector的维度，还能够提高句子之间相似度。\nTF-IDF\n通过词袋模型向量化后的feature vector，忽略了语法，词序，词频。但是从直觉上来分析，如果一篇文档中一些词多次出现，那么它和该文档主题的关联性大于那些只出现过一次的词。为此，我们要在feature vector中，把词频信息考虑进来。通过TF-IDF来扩展词袋模型。\nTF-IDF：Term Frequency / Inverse Document Frequency\n首先介绍Term Frequency，这就是词频的意思。那么怎么把词频的信息添加到feature vector中呢？方法如下：\nfrom sklearn.feature_extraction.text import CountVectorizer corpus = ['The dog ate a sandwich, the wizard transfigured a sandwich, and I ate a sandwich'] vectorizer = CountVectorizer(stop_words='english') print(vectorizer.fit_transform(corpus).todense()) [[2 1 3 1 1]] {'dog': 1, 'ate': 0, 'sandwich': 2, 'wizard': 4, 'transfigured': 3}\nCountVectorizer就是一个能够记录词频的方法，feature vector中的元素值由之前的0或者1，变成了单词出现的频次。\n接下来举一个形象的例子进一步说明：\n短语“原子能的应用”可以分为3个词：原子能，的，应用。如果一篇文档中，这些词出现的次数多于其他文档的时候，那么可以说明该篇文档的主题和这些词的相关性高，这些关键词就是该文档主题的重要feature。但是这里也有个漏洞，那就是篇幅长的文档比短的占便宜。所以，根据文档的长度，对关键词的次数进行归一化。即，用关键词的数量除以文档总的词数，这才是真正的词频（Term Frequency）。\n比如一篇文档一共有1000个词，其中“原子”，‘的’，“应用”分别出现了2次，35次，5次，那么它们的词频分别是0.002，0.035，0.005。这里面“的”的词频数最高，但是看起来它对于主题没有什么价值，所以可以直接将其去掉，其实“的”即为中文的停止词。至此，这些关键词对于主题的贡献，“原子”为0.002，“应用”为0.005。\n这里还有一个小问题。在中文中，“应用”是个很普通的词，而“原子能”是个很专业的词，它和主题的相关性比前者重要。因此，我们有必要给这些关键词一个权重，来体现出它的重要性。这个权重的设定必须满足下面2个条件：\n一个词预测主题的能力越强，权重越大，反之，权重越小\n停止词的权重为零\n那么如何判断哪些次预测主题的能力强呢？\n如果一个关键词在语料库中少量的文档中出现，说明该词和这些文档关系密切，它的权重也就应该大。反之，如果一个词在大量的文档中出现，比如“应用”，它的权重就应该小。\n接下来的问题就是：如何量化权重？\n这里就需要IDF。假定一个关键词w在\nD\nw\nD_w\nDw 个文档中出现，那么\nD\nw\nD_w\nDw 越大，w的权重就越小。IDF的公式为：\nlog\n⁡\n(\nD\nD\nw\n)\n\\log (\\frac {D}{D_w})\nlog(Dw D )，其中D为全部文档数。比如，语料库总文档数D = 10亿，停止词“的”在所有文档中出现，它的\nD\nw\n=\n10\nD_w = 10\nDw =10亿，那么它的IDF = log (10亿 / 10 亿) = log(1) = 0。假如“原子能”在200万个文档中出现，\nD\nw\n=\n200\nD_w = 200\nDw =200万，则它的权重IDF = log(500) = 8.96。“应用”在5亿个网页中出现，IDF = log(2) = 1。\n利用IDF，关键词和文档主题的相关性公式变为：\n\nT\nF\n1\n⋅\nI\nD\nF\n1\n+\nT\nF\n2\n⋅\nI\nD\nF\n2\n+\n⋯\n+\nT\nF\nn\n⋅\nI\nD\nF\nn\nTF_1 \\cdot IDF_1 + TF_2 \\cdot IDF_2 + \\cdots + TF_n \\cdot IDF_n\nTF1 ⋅IDF1 +TF2 ⋅IDF2 +⋯+TFn ⋅IDFn\n上例中，该文档和“原子能的应用”的相关性为0.002 x 8.96 + 0.035 x 0 + 0.005 x 1 = 0.02292。\nTF-IDF是对搜索关键词的重要性的度量，并且具备很强的理论依据。因此，即使是对搜索不是很精通的人，直接采用TF-IDF，效果也会太差。\nTF-IDF的信息论依据\n衡量一个词的权重，一个简单的办法就是用这个词的信息量作为它的权重，即：\nI\n(\nw\n)\n=\n−\nP\n(\nw\n)\nl\no\ng\nP\n(\nw\n)\n=\n−\nT\nF\n(\nw\n)\nN\nl\no\ng\n(\nT\nF\n(\nw\n)\nN\n)\n=\nT\nF\n(\nw\n)\nN\nl\no\ng\n(\nN\nT\nF\n(\nw\n)\n)\nI(w) = -P(w)logP(w) = - \\frac {TF(w)}{N}log( \\frac {TF(w)}{N}) = \\frac {TF(w)}{N}log(\\frac{N}{TF(w)})\nI(w)=−P(w)logP(w)=−NTF(w) log(NTF(w) )=NTF(w) log(TF(w)N ) ，\n其中，N是整个语料库的大小，是个可以省略的常数，公式可以简化为：\nI\n(\nw\n)\n=\nT\nF\n(\nw\n)\nl\no\ng\n(\nN\nT\nF\n(\nw\n)\n)\nI(w) = TF(w)log(\\frac{N}{TF(w)})\nI(w)=TF(w)log(TF(w)N )\n这里存在一个缺陷：两个词出现的TF相同，比如一个是一篇文章的常见词，另一个是分散在多篇文章中，那么显然第一个词应该贡献更大，应该有更大的权重。为此，我们做一些理想的假设：\n每个文档的大小基本相同，均为M个词，即\nM\n=\nN\nD\n=\n∑\nw\nT\nF\n(\nw\n)\nD\nM = \\frac {N}{D} = \\frac {\\sum_wTF(w)}{D}\nM=DN =D∑w TF(w)\n一个关键词在文档中一旦出现，不论次数多少，贡献都相同。这样一个词要么在一个文献中出现\nc\n(\nw\n)\n=\nT\nF\n(\nw\n)\nD\n(\nw\n)\nc(w)=\\frac{TF(w)}{D(w)}\nc(w)=D(w)TF(w) 次，要么就是零。注意，\nc\n(\nw\n)\n\u0026lt;\nM\nc(w) \u0026lt; M\nc(w)\u003cM\n把这两个条件带入到上面的信息量公式后得出：\nT\nF\n(\nw\n)\nl\no\ng\nN\nT\nF\n(\nw\n)\n=\nT\nF\n(\nw\n)\nl\no\ng\nM\nD\nc\n(\nw\n)\nD\n(\nw\n)\n=\nT\nF\n(\nw\n)\nl\no\ng\n(\nD\nD\n(\nw\n)\nM\nc\n(\nw\n)\n)\n=\nT\nF\n(\nw\n)\nl\no\ng\n(\nD\nD\n(\nw\n)\n)\n+\nT\nF\n(\nw\n)\nl\no\ng\n(\nM\nc\n(\nw\n)\n)\nTF(w)log\\frac{N}{TF(w)}=TF(w)log\\frac{MD}{c(w)D(w)}=TF(w)log(\\frac{D}{D(w)}\\frac{M}{c(w)}) = TF(w)log(\\frac{D}{D(w)}) + TF(w)log(\\frac{M}{c(w)})\nTF(w)logTF(w)N =TF(w)logc(w)D(w)MD =TF(w)log(D(w)D c(w)M )=TF(w)log(D(w)D )+TF(w)log(c(w)M )\n其中\nT\nF\n−\nI\nD\nF\n(\nw\n)\n=\nT\nF\n(\nw\n)\nl\no\ng\n(\nD\nD\n(\nw\n)\n)\nTF-IDF(w) = TF(w)log(\\frac{D}{D(w)})\nTF−IDF(w)=TF(w)log(D(w)D )，最终：\nT\nF\n−\nI\nD\nF\n(\nw\n)\n=\nI\n(\nw\n)\n−\nT\nF\n(\nw\n)\nl\no\ng\n(\nM\nc\n(\nw\n)\n)\nTF-IDF(w) = I(w) - TF(w)log(\\frac{M}{c(w)})\nTF−IDF(w)=I(w)−TF(w)log(c(w)M )\n可以看出，当一个词的信息量\nI\n(\nw\n)\n越\n多\n，\nT\nF\n−\nI\nD\nF\n值\n越\n大\n，\n第\n二\n项\n值\n越\n小\n，\nT\nF\n−\nI\nD\nF\n也\n越\n大\nI(w)越多，TF-IDF值越大，第二项值越小，TF-IDF也越大\nI(w)越多，TF−IDF值越大，第二项值越小，TF−IDF也越大\n代码实现：\nfrom sklearn.feature_extraction.text import TfidfVectorizer corpus = [ 'The dog ate a sandwich and I ate a sandwich', 'The wizard transfigured a sandwich' ] vectorizer = TfidfVectorizer(stop_words='english') print(vectorizer.fit_transform(corpus).todense()) [[0.75458397 0.37729199 0.53689271 0. 0. ] [0. 0. 0.44943642 0.6316672 0.6316672 ]] {'dog': 1, 'ate': 0, 'sandwich': 2, 'wizard': 4, 'transfigured': 3}\n文本分类样例\n垃圾短消息分类\n新闻类别分类\nSummary\n理论原理：\n自然语言处理的数学本质\n统计语言模型\n中文分词\n信息论：信息熵，互信息，交叉熵\n文本feature抽取和预处理\n词袋模型\n降维方法：停止词，词性还原/词根化\nTF-IDF模型","date":"2018年10月21日 12:24:17"}
{"_id":{"$oid":"5d36a8c46734bd8e681d5e54"},"title":"自然语言处理 | (12)基于统计的语言模型构建","author":"CoreJT","content":"统计语言模型：自然语言从它产生开始，逐渐演变成一种上下文相关的信息表达和传递方式。因此让计算机处理自然语言，一个基本问题就是为自然语言这种上下文相关的特性建立数学模型，这个数学模型就是在自然语言处理中常说的统计语言模型(Statistical Language Model)。它是今天所有自然语言处理的基础，并且广泛应用于机器翻译、语音识别、印刷体或手写体识别、拼写纠错、汉字输入和文献查询。\n目录\n1.用数学的方法描述语言规律\n2. 高阶语言模型\n3.模型的训练、零概率问题和平滑方法\n1.用数学的方法描述语言规律\n统计语言模型产生的初衷是为了解决语音识别问题。在语音识别中，计算机需要知道一个文字序列是否能构成一个大家理解并且有意义的句子，然后显示或打印给使用者。\n比如：\n这句话就很通顺，意义也很明白。\n如果改变一些词的顺序，或者替换掉一些词，将这句话变成：\n意思就含混了，虽然多少还能猜到一点。\n但如果再换成：\n基本上读者就不知所云了。\n第一个句子合乎语法，词义清晰。第二个句子虽不合乎语法，但是词义还算清晰。而第三个句子则连词义都不清晰了。上世纪 70 年代以前，科学家们也是这样想的，他们试图判断这个文字序列是否合乎文法、含义是否正确等。但是语言的结构千变万化，要通过制定规则来覆盖所有的文法根本是不可能的。而弗里德里克·贾里尼克(Frederick Jelinek)换了一个角度，用一个简单的统计模型就很漂亮地搞定了这个问题。\n贾里尼克想法\n贾里尼克的出发点很简单：一个句子是否合理，就看它的可能性大小如何。上面的例子中，第一个句子出现的概率大致是，第二个句子出现的概率是 ，第三个句子出现的概率是 。因此第一个句子出现的可能性最大，是第二个句子的 10万倍，是第三个句子的一百亿亿亿亿亿亿倍。\n用更普遍而严格的描述是：\n假定 S 是一个有意义的句子，由一连串特定顺序排列的词 𝑤1,𝑤2,⋯,𝑤𝑛组成，n为句子的长度。那么 S 在文本中出现的可能性就是 S 的概率 P(S)。于是可以把 P(S) 展开表示为：\n利用条件概率公式，S 这个序列出现的概率等于每一个词出现的条件概率相乘，于是：\n其中表示句子第一个词为的概率；是在已知第一个词的前提下，第二个词出现的概率，以此类推。不难看出，词的出现概率取决于他前面的所有词。\n从计算上来看，第一个词的条件概率 𝑃(𝑤1) 很容易算，第二个词的条件概率 𝑃(𝑤2∣𝑤1)也还不太麻烦，但是从第三个词的条件概率 𝑃(𝑤3∣𝑤1,𝑤2)开始就非常难算了，因为它涉及到三个变量 𝑤1,𝑤2,𝑤3而每个变量的可能性/可能取值都是语言字典的大小。到了最后一个词 𝑤𝑛，条件概率 𝑃(𝑤𝑛∣𝑤1,𝑤2,⋯,𝑤𝑛−1) 的可能性太多，根本无法估算。\n二元模型与N元模型\n从 19 世纪到 20 世纪初，俄国有个数学家叫马尔可夫(Andrey Markov)，他提出了一种偷懒但还颇为有效的方法：假设任意一个词语 出现的概率只同它前面的词 有关。于是问题就变得很简单了，这种假设在数学上称为马尔可夫假设。\n现在，句子S出现的概率就变得简单了：\n上面的公式对应的统计语言模型是二元模型(Bigram Model)。当然，也可以假设一个词由前面的 N−1 个词决定，对应的模型稍微复杂些，被称为 N 元模型。\n接下来的问题就是如何估计条件概率 。根据它的定义:\n而估计联合概率和边缘概率很简单。根据大数定理，只要统计量足够，相对频率就等于概率，因而只需在语料库(corpus)的文本中统计一下这两个词前后相邻出现的次数,以及出现了多少次，然后把这两个数分别处以语料库大小N，即可得到这些词或2元组的概率：\n于是：\n更一般的，对于n-gram：\n这似乎有点难以置信，用这么简单的数学模型就能解决复杂的语音识别、机器翻译等问题，而用很复杂的文法规则和人工智能却做不到。其实很多语言学家都曾质疑过这种方法的有效性，但事实证明，统计语言模型比任何已知的借助某种规则的解决方法更有效。\n2. 高阶语言模型\n在基于一阶马尔可夫假设的二元模型(bi-gram/2-gram)中，句子中每个词只和前面一个词有关，这似乎过于简化了，或者说近似地过头了。比如说在句子“美丽的花朵”中，“花朵”其实是和“美丽”有关，也就是说是与前面的第二个词有关。因此，更普遍的假设是某个词和前面的若干个词有关。\n正如之前介绍的那样，N 元模型(n-gram)假设每个词 和前面的 N−1 个词有关，与更前面的词无关(不是与前面所有的词相关)，这样词  的概率只取决于前面的 N−1 个词 。因此:\n这种假设被称为 N−1 阶马尔可夫假设，对应的语言模型称为 N 元模型(N-Gram Model)。N=2时就是之前介绍的二元模型，而 N=1 的一元模型实际上是一个上下文无关模型，即假定当前词的出现概率与前面的词无关。在实际中应用最多的就是 N=3 的三元模型(trigram/3-gram)，更高阶的模型就很少使用了。\n为什么N取值那么小？\n我们之前在上一篇博客中曾经探讨过这个问题：\n1.首先，N 元模型的大小（空间复杂度）几乎是 N 的指数函数，即 ，这里 |V|是一种语言词典的词汇量，一般在几万到几十万个。其次，使用 N 元模型的速度（时间复杂度）也几乎是一个指数函数，即 。因此，N 不能很大。\n2.当 N 从 1 到 2，再从 2 到 3 时，模型的效果上升显著。而当模型从 3 到 4 时，效果的提升就不是很显著了，而资源的耗费却增加地非常快。所以，除非是为了做到极致不惜资源，很少有人会使用四元以上的模型。\n还有一个问题，三元、四元或更高阶的模型也并不能覆盖所有的语言现象。在自然语言处理中，上下文之间的相关性可能跨度非常大，甚至可以从一个段落跨到另一个段落。因此，即便再怎么提高模型的阶数，对这种情况也无可奈何，这就是马尔可夫模型的局限性，这时就需要采用其他一些长程的依赖性(Long Distance Dependency)来解决这个问题了,如之后学习的神经语言模型LSTM/GRU等可以很好的解决这个问题。\n3.模型的训练、零概率问题和平滑方法\n语言模型中所有的条件概率称为模型的参数，通过对语料的统计，得到这些参数的过程(计算这些条件概率)称为模型的训练。前面提到的二元模型训练方法似乎非常简单，只需计算一下前后相邻出现的次数 和 单独出现的次数 的比值即可。而的取值可能是词典中的任意一个单词，即考虑所有可能的组合，基于语料库计算频数、频率及条件概率，对于N元模型也是同理，这样做的话，的很多组合可能没有意义，在语料库中没有出现过，即=0。那么是否意味着条件概率=0？反之，如果，都在语料库中只出现一次，那么能否得到=1，这样非常绝对的结论？\n注意词典和语料库不是一个概念，词典基于语料库构建，对语料库分词，去重，调整顺序来构建词典。n-gram模型，可以理解为考虑词典中的所有可能组合，然后基于语料库进行统计，计算条件概率，存储起来，应用时直接查询计算即可。这样考虑所有可能组合，很多组合会没有意义，在语料库中也不会出现，就会存在0概率/数据稀疏的问题，此时需要使用平滑方法，对没有见过的gram赋于一个非0的概率值。\n还会面临统计可靠性或统计量不足的问题。在数理统计中，我们之所以敢用对采样数据进行观察的结果来预测概率，是因为有大数定理(Law of Large Number)在背后做支持，它的要求是有足够的观察值。但是在估计语言模型的概率时，很多人恰恰忘了这个道理，因此训练出来的语言模型“不管用”，然后回过头来怀疑这个方法是否有效。那么如何正确地训练一个语言模型呢？\n一个直接的办法就是增加数据量，但是即使如此，仍会遇到零概率或者统计量不足的问题。假定要训练一个汉语的语言模型，汉语的词汇量大致是 20 万这个数量级，训练一个三元模型就有 个不同参数。假设抓取 100 亿个有意义的中文网页，每个网页平均 1000 词，全部用作训练也依然只有 。因此，如果用直接的比值计算概率，大部分条件概率依然是零，这种模型我们称之为“不平滑”。\n训练统计语言模型的艺术就在于解决好统计样本不足时的概率估计问题。\n关于平滑技术的详细介绍，可以阅读这篇博客自然语言处理中N-Gram模型的Smoothing算法\n当然，如果对这些平滑算法不是很懂也不必太担心，平滑技术在统计自然语言处理时代，用得比较多；现代的神经网络对语言模型建模的方式，由于本身结构的原因，自动解决了这个问题，我们之后还会学习。","date":"2019年02月12日 12:10:54"}
{"_id":{"$oid":"5d36a8c46734bd8e681d5e56"},"title":"斯坦福自然语言处理工具python环境配置","author":"_compiling","content":"斯坦福自然语言处理工具python环境配置\n1. 简介\nStanford nlp group 是世界知名的自然语言处理研究组，该组的研究内容涵盖了从基本的计算语言原理研究到NLP的关键应用技术。其中，该组所开发的coreNLP工具被广泛应用，该工具提供了分词、词性标注、语法分析、共指消解、命名实体识别等操作。\nStanford coreNLP源码使用Java编写而成，但一些程序员将coreNLP进行了封装，从而可以便于在其他语言环境下使用该工具。本文对自己配置coreNLP的python环境的过程进行总结。\n2.过程\n首先，需要下载Stanford coreNLP的Java源码，该代码可以在斯坦福NLP组的下载页面进行下载（见此处）。标准的coreNLP为jar格式，可以通过Java程序引入、命令行等方式进行调用。若是需要处理中文，则还需在该页面上下载对应的中文处理jar文件。\n之后由于我们要使用python调用coreNLP，该主页上还提供了其他语言使用Stanford coreNLP的库（见此处）。 如我们在python环境下使用coreNLP，则需要安装一个可以调用coreNLP源码的库，可选的库有很多，包括pycorenlp、stanfordcorenlp、corenlp-pywrap等等。 每个库在GitHub上都有相应的说明，参考其介绍即可。 我本次使用的是stanfordcorenlp库（见此处）。使用pip安装好后，按照参考文档测试运行即完成了配置。\n# Simple usage from stanfordcorenlp import StanfordCoreNLP nlp = StanfordCoreNLP(r'G:/JavaLibraries/stanford-corenlp-full-2016-10-31/') sentence = 'Guangdong University of Foreign Studies is located in Guangzhou.' print 'Tokenize:', nlp.word_tokenize(sentence) print 'Part of Speech:', nlp.pos_tag(sentence) print 'Named Entities:', nlp.ner(sentence) print 'Constituency Parsing:', nlp.parse(sentence) print 'Dependency Parsing:', nlp.dependency_parse(sentence)`","date":"2017年10月28日 00:46:03"}
{"_id":{"$oid":"5d36a8c56734bd8e681d5e58"},"title":"机器学习-Python自然语言处理库","author":"gao8658","content":"自然语言处理的库非常多，下面列举一些对Python友好，简单易用，轻量，功能又全的库。\n1 中文\n中文自然语言处理工具评测：https://github.com/mylovelybaby/chinese-nlp-toolkit-test\nawesome: https://github.com/crownpku/Awesome-Chinese-NLP\nHanlp\n地址：https://github.com/hankcs/HanLP\nJieba\n地址：https://github.com/fxsjy/jieba\nsnownlp\nhttps://github.com/isnowfy/snownlp\n2 英文\nNLTK\n地址：https://www.nltk.org/\nTextblob\n地址：https://github.com/sloria/TextBlob\n3 实例\n3.1 中文自然语言处理Pipeline实例\n实例：https://github.com/JiaLiangShen/Chinese-Article-Classification-based-on-own-corpus-via-TextCNN-and-GBDT\n3.2 英文Pipeline实例\n实例：https://github.com/TiesdeKok/Python_NLP_Tutorial/blob/master/NLP_Notebook.ipynb\n友情推荐：ABC技术研习社\n为技术人打造的专属A(AI),B(Big Data),C(Cloud)技术公众号和技术交流社群。","date":"2018年08月06日 17:31:54"}
{"_id":{"$oid":"5d36a8c66734bd8e681d5e5b"},"title":"自然语言处理入门学习系列","author":"qq_36049695","content":"最近在学习和实践自然语言处理相关的知识，在这个文档从头到尾做个总结，防止自己忘记，也提供给新人来参考。本教程英文处理使用的是NLTK这个Python库，中文处理使用的是jieba这个Python库，主要是看July7月学习NLP视频学习而来，如有侵权，立即删除。Natural Language Processing(NLP)自然语言处理主要是处理以及理解自然语言的计算过程。整个自然语言处理的大致流程入下图所示：\n\n图1:自然语言处理流程\n一、自然语言处理流程\n第一步：Tokenize——分词，分词是将一个句子分成很多个单词，用一个word list存起来。如：\n英文：How are you today? 会分成 [“How”,”are”,”you”,”today”,”?”]\n中文：今天心情很好。 会分成[“今天”，”心情”，”很”，”好”，”。”]\n第二步: Stemming/Lemma——提取词干，是将英文的过去式，名词形式，复数形式全部转换为最原始单词。如：\napples =\u003e apple, went =\u003e go, watched =\u003e watch, watching =\u003e watch\n第三步：stopwords ——去除停止词，去掉单词列表中的停止词the,a等单词。如：\n英文: The school is beautiful. =\u003e [“school”,”beautiful”]，去掉了the,is等单词。\n有时会用到POS Tag —— 标注词性，即标注出单词是动词/名词/形容词/副词等。\n第四步：Get feature —— 提取特征，这个步骤的意思是用一个什么样的向量来表示这单词或者句子。如使用TF-IDF来表示一个单词：\nTF:Term Frequecy：衡量一个单词在文档中出现的次数\nTF(term) = (term出现在文档中的次数)/(文档中单词的总数)\nIDF:Inverse Document Frequecy,衡量一个单词的重要性\nIDF(term) = loge(文档总数/含有term的文档总数)\n如果一个单词在所有文档中都出现了，则IDF(term) = 0，表明这个单词不重要。\nTF-IDF = TF * IDF\n对每个单词进行统计和计算，就可以得到每个单词的TF-IDF的值，用这个值来代替这个单词，整个句子就变成了一个浮点数的List。\n当然这个是最简单的模型，这个模型有很多缺陷，现在流行的word2vect和fasttext，都是由google实习生写出来的，这两个模型生成的分布式向量可以有效的表达出两个单词之间的关系，这个在后续再做介绍。\n第五步：Machine Learning——机器学习，机器学习是表示得到特征向量之后，能根据训练集合来预测需要测试集合。这个部分也在后续再做专门的介绍。\n二、自然语言处理入门软件安装以及常见问题\n整个实验环境是在VMWare+Ubuntu 16.04 LTS下完成的，最好是能翻墙，我用的翻墙软件是LoCo加速器。\nNLTK 安装\n 安装pip\n安装pip,一个python第三方软件的库，apt-get是获得软件或者库\nsudo apt-get install python-pip python-dev build-essential\n 问题1，可能会碰到的问题: install的时候碰到Could not get lock /var/lib/dpkg/lock，无法Install\n解决方案：找到哪个线程锁住了这个资源，然后Kill掉，指令：\nsudo lsof /var/lib/dpkg/lock\nsudo kill -9 (get from lsof output)\n 更新pip这个库\nsudo pip install –upgrade pip\n 安装nltk库，用于自然语言处理。\nsudo pip install -U nltk\n 安装numpy库\nsudo pip install -U numpy\n 下载nltk所有相关东西（语料库，模型等）\npython\nimport nltk\nnltk.download(‘all’)\n Python Debug使用\nimport pdb\n在需要断点的地方 pdb.set_trace()\nh Help\nq Quit\np Print\nPp Prettyprint\nw Where+stack trace 执行到了什么地方\nl 断点前后的代码\nn 执行下一句\nb 35 在第35行断点\n变量名称 打印出变量的值是多少\nc continue until break\ns step inside\n 使用VIM做IDE，可能需要配置和Python相关的信息，在.vrmrc中进行配置，第一次需要新建这个文件，如果需要添加插件，则先在.vimrc中进行配置，然后使用vim，输入:PluginInstall来在Vim上安装指定插件。\n 问题2：运行pandas.test报错：\n运行numpy.test()或者pandas.test()出现如下错误\nImportError: Need nose \u003e= 1.0.0 for tests - see http://somethingaboutorange.com/mrl/projects/nose\n需要先安装nose,sudo pip install nose\n 问题3：pandas 从0.19.0开始不再支持pandas.io.wb，改用pandas_datareader,这个python库需要先行下载，sudo pip install pandas_datareader\nStarting in 0.19.0, pandas will no longer support pandas.io.data or pandas.io.wb, so you must replace your imports from pandas.io with those from pandas_datareader:\nfrom pandas.io import data, wb # becomes\nfrom pandas_datareader import data, wb\nMany functions from the data module have been included in the top level API.\nimport pandas_datareader as pdr\npdr.get_data_yahoo(‘AAPL’)\n 问题4：使用matplotlibc出现错误\nImportError: No module named _tkinter, please install the python-tk package\n需要安装python-tk安装包\nsudo apt-get install python-tk\n 问题5：vim添加Python支持\n1、Ctrl+Alt+T 打开命令终端，输入: vim –version |grep python 查看vim是否支持python我这个vim只支持python3，不支持python。\n2、安装py2包，在命令终端下输入: sudo apt-get install vim-nox-py2。\n3、可以再次用vim –version|grep python 查看此时vim是否支持python，若支持到此为止，若不支持，请执行第四步。\n4、在命令终端输入:sudo update-alternatives –config vim\n\n我这里是第三项属于python，第二项属于python3，想打开哪一项支持就输入它的编号就可以了（0，1，2，3）。\n其他资料的使用：\n matplotlin画图软件,画二维图可以使用这个工具，功能和matlab画图类似 sudo pip install matplotlib，结合pandas使用\nAPI使用文档：\nhttp://matplotlib.org/1.5.3/users/beginner.html\n pandas-datareader:从yahoo Finance,google API 上下载相关信息，具体使用API如下\nhttps://pandas-datareader.readthedocs.io/en/latest/\n python一些常见库的入门指导材料，可以加快学习的速度：\nhttps://pythonprogramming.net/\n 美国金融方面的数据，房价/股票等信息，已经格式化好了，容易处理的数据：\nhttps://www.quandl.com/\n pandas入门资料：\nhttp://pandas.pydata.org/pandas-docs/stable/10min.html\n三、自然语言处理实践\n实践的题目是Kaggle上的一道竞赛题目，\n链接：https://www.kaggle.com/c/home-depot-product-search-relevance， Home Depot是美国一家网上卖五金的公司，在用户输入修洗脸盆的时候，希望能提供给用户所有洗脸盆需要的五金和工具。\n给出了五组数据：\n产品属性：\n“product_uid”,”name”,”value”\n100001,”Bullet01”,”Versatile connector for various 90° connections and home repair projects”\n产品描述：\n“product_uid”,”product_description”\n100001,”Not only do angles make joints stronger, they also provide more consistent, straight corners. Simpson Strong-Tie offers a wide variety of angles in various sizes and thicknesses to handle light-duty jobs or projects where a structural connection is needed. Some can be bent (skewed) to match the project. For outdoor projects or those where moisture is present, use our ZMAX zinc-coated connectors, which provide extra resistance against corrosion (look for a “”Z”” at the end of the model number).Versatile connector for various 90 connections and home repair projectsStronger than angled nailing or screw fastening aloneHelp ensure joints are consistently straight and strongDimensions: 3 in. x 3 in. x 1-1/2 in.Made from 12-Gauge steelGalvanized for extra corrosion resistanceInstall with 10d common nails or #9 x 1-1/2 in. Strong-Drive SD screws”\n测试集：\n“id”,”product_uid”,”product_title”,”search_term”\n1,100001,”Simpson Strong-Tie 12-Gauge Angle”,”90 degree bracket”\n训练集合：\n“id”,”product_uid”,”product_title”,”search_term”,”relevance”\n2,100001,”Simpson Strong-Tie 12-Gauge Angle”,”angle bracket”,3\n提交的样本：\n“id”,”relevance”\n1,1\nRelevance是表示选出工具和输入搜索关键词之间的相关性，相关性=3表示非常相关，相关性=1表示不太相关。\n第一步：数据清洗\n 用pandas读取csv中的数据，因为数据过大，没法一次读取出来进行处理，每次处理10000条，处理完1次就放到另外一个csv文件中存储起来。\n 处理的过程是将英文用nltk的stemming方法对每个单词进行提取词干。\n 只有第一次写入csv文件的时候需要写入header,且不需要index。Header表示表头。\n\n\n处理完成之后产品描述变成如下的样子，训练数据和测试数据类似处理：\nproduct_uid,product_description\n100001,”not onli do angl make joint stronger, they also provid more consi stent, straight corners. simpson strong-ti offer a wide varieti of angl i n various size and thick to handl light-duti job or project where a struc tur connect is needed. some can be bent (skewed) to match the project. fo r outdoor project or those where moistur is present, use our zmax zinc-co at connectors, which provid extra resist against corros (look for a “”z”” at the end of the model number).versatil connector for various 90 connec t and home repair projectsstrong than angl nail or screw fasten alonehelp ensur joint are consist straight and strongdimensions: 3 in. x 3 in. x 1 -1/2 in.mad from 12-gaug steelgalvan for extra corros resistanceinstal wi th 10d common nail or #9 x 1-1/2 in. strong-driv sd screw”\n注意事项：\n 使用chunk_size进行分块读入\n 使用iterrows 一行一行读入数据\n 使用final_df[“column”] = A新增加一列，A可以是一个list\n to_csv的header设置来表示是否需要表格头\n 注意编码是ISO-8859-1\n第二步：提取特征\n假设使用搜索关键词在产品名字和产品描述中出现的最大次数来表示这个搜索关键词的两个主要特征，搜索关键词的长度表示另外一个特征。\n\n注意事项：\n 数据连接之后可能会出现NaN的字符，Python会默认为float类型的无穷大，需要通过pd.isnull(a)来判断下是否为空。\n第三步：使用机器学习来预测，加博士是用的随机森林来进行relevance的预测，后续章节会继续深入讨论。\n未完，待续。","date":"2016年12月06日 22:27:28"}
{"_id":{"$oid":"5d36a8c76734bd8e681d5e5e"},"title":"自然语言处理连载（1）- 自然语言和形式语言","author":"ZenGeek","content":"自然语言：人类语言\n无强加规则\n自然进化\n形式语言： 特别设计\n规则认为设计\n计算机代码\n数学符号等等","date":"2018年08月11日 01:22:53"}
{"_id":{"$oid":"5d36a8c86734bd8e681d5e61"},"title":"自然语言处理与分析(two)","author":"edagarli","content":"接着自然语言处理与分析（one）\n\n\n笔者现在推荐一款在线编辑器。\n我就是用来写代码的。\ndata:text/html, \u003cstyle type=\"text/css\"\u003e#e{position:absolute;top:0;right:0;bottom:0;left:0;}\u003c/style\u003e\u003cdiv id=%22e%22\u003e\u003c/div\u003e\u003cscript src=%22http://d1n0x3qji82z53.cloudfront.net/src-min-noconflict/ace.js%22 type=%22text/javascript%22 charset=%22utf-8%22\u003e\u003c/script\u003e\u003cscript\u003evar e=ace.edit(%22e%22);e.setTheme(%22ace/theme/monokai%22);e.getSession().setMode(%22ace/mode/java%22);\u003c/script\u003e\n\n这个粘帖到浏览器地址。\npublic void train() throws IOException { for (int i = 0; i \u003c categories.length; ++i) { String category = categories[i]; Classification classification = new Classification(category); //新建类别 File dir = new File(pDir, categories[i]); File[] trainFiles = dir.listFiles(); for (int j = 0; j \u003c trainFiles.length; ++j) { File trainFile = trainFiles[j]; if (isTrainingFile(trainFile)) { //判断一下是为了让一部分数据作为训练集、一部分作为测试集 String review = Files.readFromFile(trainFile, \"ISO-8859-1\"); Classified classified = new Classified( review, classification); //指定内容和类别 classifer.handle(classified); //训练 } } } }\n\n\n\n\n\n这里说明一下isTrainingFile方法。我们需要一份测试集和一个训练集，但是我们只有一个语料库，只有人为分割。我原本是每次随机数一下来干的，但是有点影响速度，这里直接用文件名作为判断依据了。\n\n\nboolean isTrainingFile(File file) { return file.getName().charAt(2) != '1'; //如果第2位为1就是测试集 }\n\n\n\n训练完成后使用classifer就可以进行极性分析了。\npublic void evaluate() throws IOException { int numTests = 0; int numCorrect = 0; for (int i = 0; i \u003c categories.length; ++i) { String category = categories[i]; File file = new File(pDir, categories[i]); File[] testFiles = file.listFiles(); for (int j = 0; j \u003c testFiles.length; ++j) { File testFile = testFiles[j]; if (!isTrainingFile(testFile)) { String review = Files.readFromFile(testFile, \"ISO-8859-1\"); ++numTests; Classification classification = classifer.classify(review); String resultCategory = classification.bestCategory(); if (resultCategory.equals(category)) ++numCorrect; } } } System.out.println(\"测试总数：\" + numTests); System.out.println(\"正确数：\" + numCorrect); System.out.println(\"正确率\" + ((double) numCorrect) / (double) numTests); }\n\n\n\n\n效果：\nlingpipe1\n将isTrainingFile修改一下\nboolean isTrainingFile(File file) { return file.getName().charAt(2) != '2'; //如果第2位为2就是测试集 }\n\n\n\nlingpipi2\n就正确率而言怎么划分训练集和测试集影响不大。\n还可以这样划分\nboolean isTrainingFile(File file) { return (file.getName().charAt(2) != '2')\u0026\u0026(file.getName().charAt(2) != '1'); }\n\n\n\n扩展\n基本极性分析只是文本倾向性分析一个很简单的部分，如果需要深入的话，LingPipe还可以实现主观性分析、层次极性分析等。\n如果需要支持中文的话，请下载words-zh-as.CompiledSpellChecker。\n最后附上三篇参考文献：\nBo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment Classification using Machine Learning Techniques. EMNLP Proceedings.\nBo Pang and Lillian Lee. 2004. A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts. ACL Proceedings.\nBo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. ACL Proceedings.","date":"2014年03月24日 12:39:37"}
{"_id":{"$oid":"5d36a8c96734bd8e681d5e63"},"title":"自然语言处理中传统词向量表示VS深度学习语言模型（二）","author":"Mr_Ru","content":"上一篇自然语言处理中传统词向量表示VS深度学习语言模型（一）主要介绍了关于语言表示的问题，今天在正式接触word2vec之前还是想啰嗦一下自然语言处理的基本问题以及语言模型等方面的知识。\n1. 语言模型\n语言模型（language model，LM）在自然语言处理中占有重要的地位，尤其在基于统计模型的语音识别、机器翻译、分词和句法分析等相关研究中得到广泛应用。语言模型，简单地说，就是判断一个句子的概率的模型，其基本任务就是使得一个句子更符合自然语言的叙述规则，使得符合这套叙述规则的句子的概率更大。即计算P(S)的概率，其中S为一句话。\n早期的自然语言处理系统主要是基于人工撰写的规则，这种方法费时费力，且不能覆盖各种语言现象。上个世纪80年代后期，机器学习算法被引入到自然语言处理中，这要归功于不断提高的计算能力。研究主要集中在统计模型上，这种方法采用大规模的训练语料（corpus）对模型的参数进行自动的学习，和之前的基于规则的方法相比，这种方法更具鲁棒性。\n2. 统计语言模型\n统计语言模型（Statistical Language Model），就是利用统计数据来求P(S)的大小，在NLP领域中，大部分的任务都是基于词语的细分粒度来构建模型，由词语构成句子，段落，文章等，那么统计语言模型的处理单元当然也多是已词语为单位而展开的。以词语为单位，对于序列S，可以由n个词语W1，W2，W3，...，Wn表示，因此，语言模型可以表示为，求概率P(S)的大小：\n在上式中，产生第i(1≤i≤n)个词的概率是由已经产生的i-1个词决定的，一般的我们将前i-1个词称为第i个词的“历史”或者“上下文”。在这种计算方法中，一方面，随着历史长度的增加，不同的词语的组合数目按照指数级增长。如果历史的长度为i-1，那么，就有种不同的历史情况存在，这样的话就会存在着参数空间过大的问题；另一方面，对于组合特征中，存在着大量的未出现的组合，这样就导致该组合出现的次数为0，最终导致数据稀疏严重的问题。\n2.1 N元模型（N-gram）\n由于长历史信息的组合数目过大的问题，导致概率无法计算，所以就需要一种替代的方案来近似这个概率，使之得以解决。N元模型就是在这种需求上产生的，N元模型是利用了马尔科夫假设来将求解进行转换，马尔科夫假设认为：随意一个词出现的概率只与它前面出现的有限的一个或者n个词有关。这样就将长历史信息转换为只关注于前面出现的k个历史信息，大大地简化了组合空间。\n通常情况下，n不能过大，否则组合空间过大的情况依然存在，无法解决根本问题。所以n一般取1,2,3。\n当n=1时，被称为一元语言模型，unigram，说明一个词的出现与它周围的词是独立的，\n\n\n当n=2时，被称为二元语言模型，bigram，也叫一阶马尔科夫链（Markov chain），说明当前词的出现只与其前一个词有关，\n\n\n当n=3时，被称为三元语言模型，trigram，也叫二阶马尔科夫链，说明当前词的出现与其前面的两个词有关，\n\n\n\n\n以上内容，我们大致的讲解了n-gram语言模型为什么存在，以及不同的n元gram的形式。总结：语言模型是为了求解语言模型中一个句子产生的概率大小，而n-gram是为了对求解过程中参数空间过大问题的一步优化假设，且基于不同的n有着不同的gram形式。\n3.神经网络语言模型\n在第2节中，我们知道语言模型可以使用n-gram模型来进行近似求解，可以解决一部分的自然语言处理领域的基础问题，其在词性标注、句法分析、机器翻译、信息检索等任务中起到了重要作用。然而随着深度学习的不断发展，神经网络相关研究越来越深入，神经网络语言模型（Neural Network Language Model，NNLM）越来越受到学术界和工业界的关注，接下来将系统介绍下NNLM。\n用神经网络来训练语言模型的思想最早由百度 IDL （深度学习研究院）的徐伟提出NNLM（Nerual Network Language Model）是这方面的一个经典模型，NNLM 依赖的一个核心概念就是词向量（Word Embedding）。词向量源于Hinton在Learning distributed representations of concepts提出的Distributed Representation，Bengio将其引入到语言模型建模中，提出了NNLM。\n模型的训练数据是一组词的序列W1,W2,...,Wn，Wn∈V，其中V为词典，Vi表示词典中的第i个单词。NNLM的训练目标也是训练如下模型：\n\n\n\n其中wt表示词序列中第 t个单词，w1,...,wt-1表示从第1个词到第t 个词组成的子序列。\n\n其模型的框架为：\n这里的模型主要分为两个部分，特征映射和计算条件概率分布。\n\n\n这里偷懒了，直接截图过来了。3333~\n\n需要注意的是：一般的神经网络模型不需要对输入进行训练，而该模型中的输入x=(C(wt−n+1),...,C(wt−1)) 是词向量，也是需要训练的参数。由此可见模型的权重参数与词向量是同时进行训练，模型训练完成后同时得到网络的权重参数和词向量。word2vec也是通过类似的方式来训练语言模型，最后顺便得到最后的词向量。接下来的博文中将会介绍下word2vec。\n特别感谢：\n1.[我们是这样理解语言的-3]神经网络语言模型\n2.宗成庆，《统计自然语言处理》（第二版）\n3.语言模型的基本概念\n4.Bengio Y, Ducharme R, Vincent P, et al. A neural probabilistic language model[J]. Journal of machine learning research, 2003, 3(Feb): 1137-1155.\n5.知乎：深入浅出讲解语言模型\n\n\n\n\n\n\n\n\n\n\nC∈R|V|×mC∈R|V|×m 将输入","date":"2018年06月05日 18:33:19"}
{"_id":{"$oid":"5d36a8c96734bd8e681d5e67"},"title":"暑期学习自然语言处理笔记","author":"icaoys","content":"暑期学习自然语言处理笔记\n一、 自然语言处理的应用\n自然语言处理（natural language processing，即 NLP）\n（1）拼写检查纠错，关键词搜索，垃圾邮件识别\n（2）文本挖掘，文本分类\n（3）机器翻译\n（4）自动问答、客服机器人\n（5）复杂对话系统：微软小冰\n二、 自然语言处理的模型\n深度学习网络应用于NLP，在于特征提取的优势。深度学习中的强化学习是无监督的模型。\n关于语言模型\n机器翻译 （语言搭配的概率）\n拼写纠错 （出现一句话的概率）\n智能问答\n什么是语言模型\n用来计算 一句话概率的 模型。\n几个词都出现的联合概率密度。在前 i-1 个词出现的条件下第 i 个词出现的概率称之为与之相关性。当词非常多的时候就会造成数据过于稀疏，参数空间太大。 如果 i 很大，参数空间过大容易过拟合，也无法实用。\n如何简化问题？近似上面的公式，效果又要求比独立性假设好。\nn-gram 模型\nn-gram公式如下图所示。下图公式②叫做三元语法（trigram，3-gram）：\n马尔科夫假设（Markov Assumption）：下一个词的出现仅依赖于它前面的一个或几个词。这对于联合概率链规则来说其实是相对粗糙的简化，位置离得较远而且关系比较弱的词语就简化省略掉了。\n概率的计算过程（P=0.33 = 927/2533）：\n参考网址：http://blog.csdn.net/yaoweijiao/article/details/52945186\n大概了解语言模型的计算处理过程。\n三、 词向量\n计算机理解文本的方式——word vector，就是指将单词向量化，将某个单词用特定的向量来表示。\n\n注意：转化的是一个词，而非一个字。如：\n假设/下/一个/词/没有/出现。=\u003e正确转化\n假/设/下/一/个/词/没/有/出/现。=\u003e错误转化\n如何构造有意义的向量？\n希望构造的词向量，对于意义相近的词对应的向量相关性大些。\n构造是基于上下文的语境构造，与语言拼写规则本身无关。如下图，虽然单词是不同国家的语言，但是转化为的向量分布想非常相似。\n关于NLP的神经网络模型。\n假设一个文本（“神经网络模型。”）交给这个模型，根据上下文的前 i-1 个词（【1】【2】）输入到input layer，经过网络，让模型自己找到后面的第 i 个词（【3】）是什么。这就是模型的任务。\n模型的架构：输入层=\u003e投影层=\u003e隐层=\u003e输出层。\n输入层：每个词的向量维数必须一致。\n投影层：输入层的多个向量连成一串变成一个大向量。\n输出层：类似于softmax，输出的是一组概率值。\n如何用一个向量更好地表示词？ 参考网址：\n\nhttp://blog.csdn.net/u013362975/article/details/53319002\n四、 Hierarchical Softmax 模型\n更新每个输出词向量在训练集上每个词的分布的问题是非常耗时耗力的。为了解决这个问题。两个方法：hierarchical softmax 和 negative sampling。\n\nHierarchical softmax：用一个二叉树代表词表中的所有词，这个词是叶子节点。对于每一个叶子节点，存在着从根到叶子的唯一路径，这个路径用来估计这个词的概率。\nNegative Sampling：每次只更新一个输出词，目标输出词应该一直在样本中得到更新，并且添加一些negative samples进去。\n1. CBOW模型\n拿一个词语的上下文作为输入，来预测这个词语本身。（基于上下文预测某词）（ContinuousBag Of Words Model）\n公式参考：\n\nhttp://blog.csdn.net/dream_catcher_10/article/details/51361328\n当结果分类较多（比如50分类）时，如何解决输出问题。\n哈夫曼树，最优二叉树（Huffman Tree）。\n路径长度是指一个结点到另外一个结点之间分支数。\n带权路径长度是指每个分支上有权值，一个结点到另外一个结点所有路径权值总和。树的带权路径长度是从根结点出发到每一个叶节点的带权路径长度总和。\n\n哈夫曼树的建立步骤：在给定的权值中选择两棵根结点权值最小的作为左右子树构造一棵新的二叉树，并将新二叉树的根结点的新权值再替代原来两个小权值放入原权值中重新挑选两棵根结点权值最小，不断迭代创建左右子树。\n哈夫曼编码 参考网址：\nhttp://blog.csdn.net/qq_19762007/article/details/50708573\n逻辑回归\n不属于回归分析，而是属于分类，差异主要在于变量不同。逻辑回归是无监督学习的一个重要算法，对某些数据与（事物的归属类别）及（分到某一类别的概率）进行评估。\nlogistic（即sigmoid）具体针对的是二分类问题，而softmax解决的是多分类问题。sigmoid函数在这里将得分值转化为概率。\n到输出层则利用上下文词向量的拼接和做为输入，输出的是窗口中心位置所有词出现的概率。利用softmax求中心词概率，当语料较大时，计算变的非常耗时。于是为了解决这个问题，利用哈夫曼树对词表进行分类，用一连串的二分类来近似多分类 。\n哈夫曼编码，一句话就是 频率越高编码越短。\n哈夫曼编码怎么用的，先将词表的词频统计好，词频高的放在接近于跟根节点的位置，词频低的放在叶子。\n训练不仅仅针对 θ 的更新，还有输入的词向量 x 要更新。\n损失函数中有2个待求参数：θ、x。在训练CBOW模型时，词向量只是个副产品。确切来说，是CBOW模型的一个参数。\n参考网址：\n\nhttp://blog.csdn.net/qwe11002698_ling/article/details/53888284\nhttp://blog.csdn.net/dream_catcher_10/article/details/51361328\n2. skip-gram模型\n用一个词语作为输入，来预测它周围的上下文。（基于当前词预测上下文）（Continuous Skip-gram Model）\n该模型与CBOW类似。参考资料：\n\nhttp://blog.csdn.net/qwe11002698_ling/article/details/53888284\nhttp://www.cnblogs.com/tina-smile/p/5204619.html\n五、 Negative Sampling 模型\nNegative Sampling：负采样\n\n\n已知一个词w，它的上下文是context(w)，那么词w就是一个正例，其他词就是一个负例。但是负例样本太多了，我们怎么去选取呢？\n\n\n\n在语料库C中，各个词出现的频率不一样，采样的时候要求高频词选中的概率较大，而低频词选中的概率较小。这就是一个带权采样的问题。 随机抽取负样本，随机数生成满足均匀分布，而取词概率可不是均匀分布，其概率应当随着词频大小变化。\n\n将词频转换为线段长度。选取负例样本的时候，取线段上的一个随机数，对应到相应词频区间上就可以了。\n优化求偏导过程类似于 Hierarchical Softmax。\n参考网址：\n\nhttp://blog.csdn.net/chunyun0716/article/details/51722230\nhttp://blog.csdn.net/suibianti/article/details/68483231#基于negative-sampling的模型","date":"2017年08月17日 12:18:55"}
{"_id":{"$oid":"5d36a8ca6734bd8e681d5e69"},"title":"统计自然语言处理——概率论基础","author":"Shingle_","content":"对于从事统计自然语言处理来说，了解概率论、信息论以及语言学知识都是很有必要的。\n下面内容主要介绍了在统计自然语言处理中需要了解的概率论基础。\n概率\n如果P(A)作为事件A的概率，Ω是试验的样本空间，则概率函数满足下面三条公理：\n非负性 P(A) \u003e= 0\n规范性 P(Ω) = 1\n可列可加性：对于不相交的集合Aj ∈F\n\n条件概率和独立性\n假设事件B的概率已知，那么事件A发生的条件概率为（P(B) \u003e 0）:\n\n\n\n在统计自然语言处理中，上面那个链式法则很有用处，比如推导马尔可夫模型的性质。\n贝叶斯定理\n由条件概率和链式规则推得：\n\n右边的分母P(A)可以看作是归一化常数，以保证其满足概率函数的性质。\n如果我们感兴趣的仅仅是事件发生的相对可能性，这时可以忽略分母：\n随机变量\n设X为一离散型随机变量，其全部可能的值为{a1,a2,···}。那么：\npi = P(X = ai), i = 1, 2, ····\n称为X的概率函数。\nP(X \u003c= x) = F(x), x∈R\n称为X的分布函数。\n\n期望和方差\n联合分布和条件分布\n设两个离散随机变量X和Y，它们的联合密度函数可以写为：\n\n描述其中单个随机变量的概率密度函数称为边缘密度函数：\n\n标准分布\n离散分布函数：二项分布\n重复一个只有两种输出的实验，并且每次实验之间相互独立时，我们认为实验结果服从二项分布（例如抛硬币实验）。\n在自然语言处理中，语料库中的句子间肯定不是完全相互独立的。但是为了简化问题的复杂性，我们通常可能会做独立性假设，假设一个句子的出现独立于它前面的其他句子，近似认为它们服从二项分布。\n当实验有两个以上结果时，二项分布问题就转化为多项式分布（multi-nomial distribution）。\n连续分布函数：正态分布","date":"2016年07月13日 12:14:09"}
{"_id":{"$oid":"5d36a8ca6734bd8e681d5e6d"},"title":"自然语言处理（二）——词性标注与命名实体识别","author":"南木Sir","content":"文章目录\n一、Jieba\n二、NLPIR\n三、nltk\n四、SnowNLP\n五、StandFordNLP\n六、thulac\n七、StandfordNLP\n八、结论\n微信公众号\n\n本次依然使用上篇博客（自然语言处理（一）——中英文分词）中我们使用过的工具，来对中英文文本进行词性标注与命名实体识别。\n一、Jieba\n词性标注与命名实体识别\n\n二、NLPIR\nNLPIR词性标注与命名实体识别\n\n三、nltk\nnltk词性标注与命名实体识别\n\n\n四、SnowNLP\nSnowNLP词性标注与命名实体识别\n\n五、StandFordNLP\nStandfordNLP词性标注与命名实体识别\n\n\n六、thulac\nthulac词性标注与命名实体识别\n\n七、StandfordNLP\n中文词性标注与命名实体识别\n\n\n八、结论\n词性标注与命名实体识别，和上一个实验的分词相比，难度又有所增加，在给出的实验文件中已经无法找到现成的代码，有些需要去百度找，有些需要自己写，不过好在都完成了。\n在词性标注中，斯坦福大学nlp库的效果最好，但是占用内存较大，而且需要下载较大的预装包，在对中文进行处理时需要中文版；此外，结巴的中文分词是轻量级中效果最好的。\n代码和文本太多，上传太麻烦，如果需要请点击这里下载\n微信公众号\n同时也欢迎各位关注我的微信公众号 南木的下午茶","date":"2019年05月24日 21:23:28"}
{"_id":{"$oid":"5d36a8ca6734bd8e681d5e6f"},"title":"《用Python进行自然语言处理》代码笔记（五）：第七章：从文本提取信息","author":"Pd-pony","content":"#!/usr/bin/env python # -*- coding: utf-8 -*- # @Author : Peidong # @Site : # @File : eg7.py # @Software: PyCharm \"\"\" 从文本提取信息 \"\"\" import nltk # 读取语料库的“训练”部分的100 个句子的例子 from nltk.corpus import conll2000 print(conll2000.chunked_sents('train.txt')[99]) # # 使用chunk_types 参数选择 print(conll2000.chunked_sents('train.txt', chunk_types=['NP'])[99]) # 访问一个已分块语料，可以评估分块器 cp = nltk.RegexpParser(\"\") test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP']) print(cp.evaluate(test_sents)) # 尝试一个初级的正则表达式分块器，查找以名词短语标记的特征字母（如CD、DT 和JJ）开头的标记。 grammar = r\"NP: {\u003c[CDJNP].*\u003e+}\" cp = nltk.RegexpParser(grammar) test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP']) print(cp.evaluate(test_sents)) # 使用unigram 标注器对名词短语分块。 class UnigramChunker(nltk.ChunkParserI): def __init__(self, train_sents): train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)] for sent in train_sents] self.tagger = nltk.UnigramTagger(train_data) def parse(self, sentence): pos_tags = [pos for (word,pos) in sentence] tagged_pos_tags = self.tagger.tag(pos_tags) chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags] conlltags = [(word, pos, chunktag) for ((word,pos),chunktag) in zip(sentence, chunktags)] return nltk.chunk.conlltags2tree(conlltags) # # 可以使用CoNLL2000 分块语料库训练它，并测试其性能 test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP']) train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP']) unigram_chunker = UnigramChunker(train_sents) print(unigram_chunker.evaluate(test_sents)) postags = sorted(set(pos for sent in train_sents for (word,pos) in sent.leaves())) print(unigram_chunker.tagger.tag(postags)) # 使用连续分类器对名词短语分块 class ConsecutiveNPChunkTagger(nltk.TaggerI): def __init__(self, train_sents): train_set = [] for tagged_sent in train_sents: untagged_sent = nltk.tag.untag(tagged_sent) history = [] for i, (word, tag) in enumerate(tagged_sent): featureset = npchunk_features(untagged_sent, i, history) train_set.append( (featureset, tag) ) history.append(tag) self.classifier = nltk.MaxentClassifier.train(train_set, algorithm='megam', trace=0) def tag(self, sentence): history = [] for i, word in enumerate(sentence): featureset = npchunk_features(sentence, i, history) tag = self.classifier.classify(featureset) history.append(tag) return zip(sentence, history) class ConsecutiveNPChunker(nltk.ChunkParserI): def __init__(self, train_sents): tagged_sents = [[((w, t), c) for (w, t, c) in nltk.chunk.tree2conlltags(sent)] for sent in train_sents] self.tagger = ConsecutiveNPChunkTagger(tagged_sents) def parse(self, sentence): tagged_sents = self.tagger.tag(sentence) conlltags = [(w, t, c) for ((w, t), c) in tagged_sents] return nltk.chunk.conlltags2tree(conlltags) # # 定义一个简单的特征提取器，它只是提供了当前标识符的词性标记 def npchunk_features(sentence, i, history): word, pos = sentence[i] return {\"pos\": pos} train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP']) chunker = ConsecutiveNPChunker(train_sents) print(chunker.evaluate(test_sents)) # 一个分块器，处理NP，PP，VP 和S grammar = r\"\"\" NP: {\u003cDT|JJ|NN.*\u003e+} # Chunk sequences of DT, JJ, NN PP: {\u003cIN\u003e\u003cNP\u003e} # Chunk prepositions followed by NP VP: {\u003cVB.*\u003e\u003cNP|PP|CLAUSE\u003e+$} # Chunk verbs and their arguments CLAUSE: {\u003cNP\u003e\u003cVP\u003e} # Chunk NP, VP \"\"\" cp = nltk.RegexpParser(grammar) sentence = [(\"Mary\", \"NN\"), (\"saw\", \"VBD\"), (\"the\", \"DT\"), (\"cat\", \"NN\"), (\"sit\", \"VB\"), (\"on\", \"IN\"), (\"the\", \"DT\"), (\"mat\", \"NN\")] print(cp.parse(sentence)) sentence = [(\"John\", \"NNP\"), (\"thinks\", \"VBZ\"), (\"Mary\", \"NN\"), (\"saw\", \"VBD\"), (\"the\", \"DT\"), (\"cat\", \"NN\"), (\"sit\", \"VB\"), (\"on\", \"IN\"), (\"the\", \"DT\"), (\"mat\", \"NN\")] print(cp.parse(sentence)) cp = nltk.RegexpParser(grammar, loop=2) print(cp.parse(sentence)) # 在NLTK 中，创建了一棵树，通过给一个节点添加标签和一个孩子链表： # tree1 = nltk.Tree('NP', ['Alice']) # print(tree1) # tree2 = nltk.Tree('NP', ['the', 'rabbit']) # print(tree2) # tree3 = nltk.Tree('VP', ['chased', tree2]) # tree4 = nltk.Tree('S', [tree1, tree3]) # print(tree4) # print(tree4[1]) # print(tree4.leaves()) # print(tree4[1].node) # print(tree4[1][1][1]) # 递归函数遍历树 # def traverse(t): # try: # t.node # except AttributeError: # print(t,) # else: # # Now we know that t.node is defined # print ('(', t.node,) # for child in t: # traverse(child) # print (')',) # # t = nltk.Tree('(S (NP Alice) (VP chased (NP the rabbit)))') # print(traverse(t)) sent = nltk.corpus.treebank.tagged_sents()[22] print(nltk.ne_chunk(sent, binary=True)) print(nltk.ne_chunk(sent))","date":"2017年05月09日 16:24:04"}
{"_id":{"$oid":"5d36a8cb6734bd8e681d5e73"},"title":"计算语言学（CL）与自然语言处理（NLP）","author":"hlang8160","content":"一般认为计算语言学（CL）是语言学的一个分支，自然语言处理（NLP）是计算机科学的一个子学科。但是现在由于CL和NLP之间的界限越来越模糊，甚至两个领域的人常常去参加同样的会议，交流起工作来也完全没有障碍，于是一个问题出现了：NLP是跨语言学和计算机科学的交叉学科吗？\n近日在NLP学术圈里，因为Twitter上的一个推文引发了对这个问题的一场小争论。\n一、The Beginning\n过程大概是这样的：\n华盛顿大学著名的语言学教授Emily M. Bender在审核一篇跨语言应用的论文时，为作者数据集的混乱不堪发愁，于是就发了个twetter：\n\n直接的评论到没有，转推也就两个。但两个转推却引来两场争论。一场主要关于怎么用数据集的，因为和本文无关，这里我们就按下不表了。\n另一场争论起点是这样的：\n推文转了四天，到了纽约大学计算科学与数据科学的助理教授Kyunghyun Cho这里，他抱怨说：“为什么我不用更多的语言？因为投稿ACL反馈回来的评审意见是：‘奇怪，作者竟然选择用土耳其语-英语数据集’。”\n\n这条推文本身也是对数据集问题的响应。Emily Bender教授是这么回复的：会不会是因为（由于ACL是跨学科的）人手不够，不能相互审查？\n\n我们知道，推文就像聊天，不一定看到的人联想到什么呢，所以推着推着话题就变了。接着“interdisciplinarity”这个词，约翰霍普金斯大学Jason Eisner教授（ nlp几个神牛之一）的博士生Ryan Cotterell加入了讨论：\n\n一个是著名的语言学教授，一个只是一个博士生，在国内可能Ryan早早缴械投降，说“我one-million-percent endorse Emily教授的观点”了。但Ryan毕竟也是大牛的学生，并不怯于和教授辩论。\n二、ACL是跨学科的吗？\n随后的内容就变得越来越复杂，也有越来越多的人加入争论。加上twitter 140个字符的限制，道理更是说不清。于是Ryan Cotterell决定做两件事情：\n1）在medium.com上写篇博文来澄清他的观点；\n2）通过定量的方法来考察语言学和NLP之间的关系。\n博文内容大致如下：\n我最近推了一些东西，没想到引起了一些争论。由于Twitter上140个字符长度的限制，可能会引起误解。我的观点是：在NLP上发表的工作并没有吸取语言学方面最新的进展，因此也没有被公认为是跨学科的。\n也许更坦率的观点例如这样\n当然我不认为语言学最近三十年没什么有意思的成果，但很明显，语言学和NLP已经分离开了。举一个例子，在NLP阅读群，例如Stanford、CLSP、Stony Brook和Arizona等，从他们的日程安排中我找不到一篇最近的语言学论文。如果两个学科有交叉的话，这是不应该的。所以我的观点的弱化版本是：NLP在过去10到20年的发展与近期语言学的研究无关。\n\n在深入这个观点之前，我想先说清楚两个概念：什么是计算语言学（CL）以及什么是自然语言处理（NLP）。内容来自我导师Jason Eisner在Quora上的回答。\n1、什么是计算语言学\n计算语言学（CL）类似于计算生物学或者任何计算XXX。它主要致力于用计算的方法来回答语言学的科学问题。\n在语言学中的核心问题包括语言表征和语言知识的性质，如何在语言的产生、理解中获得和运用语言学知识。对这类问题的回答，有助于描述人类的语言能力，也有助于解释我们实际记录的语言数据和行为的分布。\n在计算语言学中，我们用更形式化的答案来回答这些问题。语言学家关心人类计算了什么以及如何计算的。所以我们将语言表征和语法通过数学的形式来定义，我们研究它们的数学属性，并设计有效的算法来学习、生成和理解。只要这些算法可以实际运行，我们就可以测试我们的模型，看它们是否能作出合理的预测。\n语言学也考虑一些非核心的问题，例如社会语言学、历史语言学、生理语言学或者神经语言学等等。这些学科问题本质上和计算语言学是平等的，都是在用一套模型和算法来让数据看起来合理。\n从这个角度来说，计算语言学并不试图去对日常用语进行建模，而是将语言学家所作的推论自动化。这潜在地就使我们能够处理更大的数据集（甚至新的数据）并得出更准确的结论。同样的，计算语言学家可能会设计软件工具来帮助记录濒危语言。\n2、什么是自然语言处理（NLP）\n自然语言处理（NLP）是解决分析（或生成）自然语言文本的工程问题的艺术。 在这里，成功的标准不在于你设计了一个更好的科学理论，或者是证明了语言X和Y在历史上是相关的；它的标准是你是否在工程问题上得到了好的解决方案。\n例如，你不会去考虑谷歌翻译有没有解释翻译的“本质”是什么或者翻译人员如何工作的；你在意的是它能否给你产生出一个合理、精确、流畅的翻译结果。机器翻译社群有他们自己的衡量方法，他们致力于提高这些分数，而不是理解翻译的本质。\nNLP主要是用来帮助人们去理解和消化那些以文本形式存在的大量信息，当然也会被用来生成更好的用户接口以便于人类更好的与机器或人进行交流。\n我说NLP是工程性质的，并不意味着它只用来开发商业价值。NLP也会被拿来研究学术问题，例如政治科学（博客文章）、经济学（金融新闻和报道）、医学（医生的笔记）、数字人文（文学作品、历史资料）等。这些都是被作为“计算XX学”的工具来回答XX学家的科学问题，而不属于语言学家的科学问题。\n3、跟交叉学科有什么关系呢？\n计算语言学已经被定义为一个交叉学科。但NLP还没有，可能是，也可能不是。正如航空工程不需要从鸟类获得灵感一样，NLP也不必从人类如何处理语言中获得灵感。所以我认为应当认真考虑的一个问题是：目前还没有关于NLP是否是交叉学科的判断标准。相关的人员只是从他们的工具箱里选择一些工具解决他们的工程问题，在ACL会议中很多（绝大多数）工作认真来看都不能算是交叉学科的。\n4、交叉学科应该是什么样子？\nWilson and Hayes（2008）曾经做的一份工作可以很好的回答这个问题：首先他们借鉴NLP和ML的技术，提出了一些提高语言理解的方法；由此他们得出一些关于语言的科学结论，并通过实验验证了这些结论。\n学科的定义一直是在变化的，我认为所谓跨学科的工作其实就是两个（多个）群体兴趣的交集。\n一些人认为因为NLP里有“word”和“punctuation”这些语言学的概念就是跨学科了，这是很荒谬的。我们也使用对数的概念，能说NLP与数学也有交叉吗？我们所做的工作与数学期刊上的完全不同。\n争论中出现有两个容易混淆的论断\nClaim 1：没有语言学的理论，NLP就做不下去。\n这是Emily Bender教授的说法，我倾向于认同。但我没有做过面向人类的NLP任务，所以我实在不好下判断。\nClaim2：计算语言学的工作没有真正地呈现在*ACL会议上。\n我认为这基本上是对的，但也有一些例外，只不过很少见。原因有两面：当我对NLPer谈论问题时，他们会问“这有什么用”；而当我和语言学家讨论时，他们又完全听不懂，因为他们最后一节数学课还是在高中上的。基于这样经验，我觉得ACL并不是一个真正的跨学科的地方，而且是越来越如此。举三个例子：\n（1）许多以计算为导向的语言学家和面向语言的ACLers希望在2018年初成立一个新的会议。如果*ACL真的能够体现计算机科学和语言学之间的跨学科合作，为什么会有很多人需要另一个会议呢？我认为这主要是因为这两个学科之间基本上没有交叉。\n（2）我在EMNLP 2017上有一个海报展示，是关于多语言形态标记的（Cotterell and Heigold 2017）。我得到的第一个问题是来自工业界的一个NLP研究人员，他很真诚地问：现在已经可以通过端对端训练一切东西了，为什么还要进行词性标注呢？在一定程度上这也是许多有建树的研究人员的观点，例如Kyunghyun Cho。当然在模型中选不选用词性标注应该根据你的问题而定。我想强调的是，我们正处于这样一个时期，之前NLP的一些旧的辅助应该用新一代重新判断了。如果认真去考察的话，就会发现词性标注是句法理论中相对肤浅的部分。Fred Jelinek著名的讽刺是：每次他炒掉一个语言学家，性能就会提升一些而且这个咒语现在还在很多NLP领域有效。\n（3）另外一点就是很多NLPer并没有学过语言学。如Emily所说，交叉学科研究的本质上需要两个领域的专业知识。我认为，这些专业知识应该以某种形式来源于领域内的专家。而据我所知，这似乎并没有发生。\n\n5、定量化研究\n我现在正在尝试研究发表在语言学会议/期刊的论文与发表在NLP会议/期刊上的论文之间的引用情况，来定量化研究语言学与NLP之间关系。初步的结果表明，两者的重合率非常小。\n我非常欢迎任何能促进这份研究的建议。\n三、语言学、NLP和跨学科\n看到Ryan Cotterell的博文（还有些别的原因），Emily Bender教授也就此在medium上写了一篇博文来反驳Ryan（博文有一部分是针对另一场争论的反驳）\n首先，Emily教授回顾了一下事件的起因（前面已经说过了），然后针对第一波争论给出了一针见血的评论。随后话锋一转，说第一波争论周日晚上已经圆满结束，但又来了波新的——\n部分博文内容：\nRyan Cotterell花了很大力气试图说服每一个人相信NLP不是一个交叉学科，理由是他认为交叉学科必须建立在两个学科共同的工作基础上，而目前NLP的工作大部分不符合不符合这个定义。\n对此，我想做出以下回答：如果问题要求多个领域的专业知识有效地接近，一个研究领域原则上就是跨学科的。\n根据我的定义，NLP原则上就是跨学科的。我同意Ryan的观点，说NLP在实践中大多不是学科交叉的，但我觉得没有必要非要达到这么高的标准。同样的，我也不认为语言学的所有子领域都和NLP相关。\n我的观点是：学习语言如何工作以及（或者）与有相关经验的人合作，会让NLP发展地更好。\n对“交叉学科”如此高标准的定义我觉得是无益的：我不希望人们认为“如果我不能拿一个语言学学位，我就没法做交叉学科的工作”；同样我也不希望人们留下“语言学无关紧要”的印象。\n也许这个争论中最令人沮丧的部分是，它抹除了我在语言学领域和CL/NLP领域的工作。他们似乎还将“语言学”等同于“现代乔姆斯基语法”。另一方面，那些“通常NLP不使用语言学的工作”的论断等同于把包括我在内的一些人的工作都给排除在外了。\n所以，Hey world，语言学已经不是乔姆斯基时代的了。\n四、结语\n事情大致就是如此，说大不大，说小也确实小。它反映出几个问题：\n1、要不要认真地考察一下NLP是否是交叉学科？Ryan Cotterell在尝试用论文之间引用量的数据来定量化分析这个问题，也许会很有意思。\n2、另一方面，不管讨论的结果是什么，它都是有益的，因为讨论促使人们去反复地思考自己的观点。国外学术圈子的讨论风气很旺盛。","date":"2017年11月12日 17:13:53"}
{"_id":{"$oid":"5d36a8cc6734bd8e681d5e75"},"title":"《NLP汉语自然语言处理原理与实践》学习一","author":"一厘米1992","content":"中文语言的机器处理\n直观上，一个自然语言处理系统最少三个模块：语言的解析、语义的理解及语言的生成。\n计算机处理自然语言最早应用在机器翻译上，此后在信息检索、信息抽取、数据挖掘、舆情分析、文本摘要、自动问答系统等方面都获得了很广泛的应用。虽然已经产生了许多专业技术作用域语言理解的不同层面和不同任务，例如，这些技术包括完全句法分析、浅层句法分析、信息抽取、词义消歧、潜在语义分析、文本蕴含和指代消解，但是还不能完美或完全地译解出语言的本义。\n命名实体识别：主要用来识别语料中专有名词和未登录词的成词情况，如人名、地名\n组织机构名称等，也包括一些特别的专名。准确的命名实体识别一准确的分词和词性标注为前提。\n语义组块：用来确定一个以上的词汇构成的短语结构，即短语级别的标注，主要识别名词性短语、动词性短语、介词性短语等，以及其他类型的短语结构。语义组块的自动识别来源于中文分词、词性标注和命名实体识别的共同信息。语义组块的识别特征必须包含中文分词和词性标注两部分。\n语义角色标注：以句子中的谓语动词为中心预测出句子中各个语法成分的语义特征，是句子解析的最后一个环节，也是句子级别研究的重要里程碑。语义角色标注直接受到句法解析和语义组块的影响。\n词性标注（Part-of-Speech Tagging 或POS Tagging）：又称为词类标注，是指判断出在一个句子中每个词所扮演的语法角色。例如：表示人、事物、地点等的名称为名词，表示动作或状态变化的词为动词等。一个词可能具有多个词性。一般而言，中文的词性标注算法比较同意，大多数使用隐马尔科夫模型（HMM）或最大熵算法，如结巴分词的词性标注。为了获得更高的精度，也有使用条件随机场（CRF）算法的，如LTP3.3 中的词性标注。中文词性标签有两大类：北大词性标注集和宾州词性标注集。\n句法分析：是根据给定的语法体系自动推导出句子的语法结构，分析句子所包含的语法单元和这些语法单元之间的关系，将句子转化为一棵结构化的语法树。目前句法分析有两种不同的理论：一种是短语结构语法，另一种是依存语法。\n哈工大NLP平台\n哈工大语言技术平台（Language Technology Platform，LTP）是哈工大社会计算与信息检索研究中心研发的一整套中文语言处理系统。语言技术平台包括中文分词、词性标注、命名实体识别、依存句法分析、语义角色标准等丰富、高效、精准的自然语言处理技术，还可以通过可视化的图形输出，使用户一目了然。（P-10）\nStanford NLP 团队\n斯坦福自然语言处理团队（http://nlp.stanford.edu/）是一个由斯坦福大学的教师、科研人员、博士后、程序员组成的团队。该团队致力于研究计算机理解人类语言的工作，涵盖诸如句子的理解、机器翻译、概率解析和标注、生物医学信息抽取、语法归纳、词义消歧、自动问答及文本区域到3D场景的生成等。\n在某些中文NLP应用中局域卓越的性能，一些主要的中文NLP应用如下：\n（1）斯坦福句法解析器\n概率自然语言句法解析器包括PCFG（与概率的上下文无关的短语）和依存句法解析器，一个词汇的PCFG解析器，以及一个超快速的神经网络的依存句法解析器和深度学习重排序器。在线句法分析器演示：http://nlp.stanford.edu:8080/parser/index.jsp\n（2）斯坦福命名实体识别器\n该识别器基于条件随机场序列模型，用于英文、中文、德文、西班牙文的连同命名实体识别、以及一个在线NER演示。\n（3）斯坦福词性标注器\n基于最大熵（CMM）算法、词性标注（POS）系统包括英语、阿拉伯语、汉语、法语、德语和西班牙语。\n（4）斯坦福分析器\n基于CRF算法的分词器，支持阿拉伯语和汉语","date":"2018年09月09日 16:12:27"}
{"_id":{"$oid":"5d36a8cc6734bd8e681d5e77"},"title":"【读书笔记】《Python自然语言处理》第1章 语言处理与Python","author":"iMayday_hui","content":"1.1 语言计算：文本和词汇\n入门\nnltk下载地址 使用pip安装\n\u003e\u003e\u003eimport nltk 检验是否成功。\n\u003e\u003e\u003enltk.download() 选择语料下载\n使用python解释器加载book模块中的条目\n\u003e\u003e\u003efrom nltk.book import *\n输入名字如 \u003e\u003e\u003etext1 即可找到相应的文本\n搜索文本\n搜索文本中的某个词 \u003e\u003e\u003etext1.concordance(\"monstrous\")\n搜索文本中与指定词相似的词 \u003e\u003e\u003etext1.similar(\"monstrous\")\n研究同一文本中的两个及以上的词之间的关系 \u003e\u003e\u003etext2.common_contexts([\"monstrous\",\"very\"])\n(可以用来考察两次的用法是否相似)\n根据几个词在文本中出现位置的离散图观察单词分布\n\u003e\u003e\u003etext4.dispersion_plot([\"citizens\",\"democracy\",\"freedom\",\"duties\",\"America\"])\n生成文本 \u003e\u003e\u003etext3.generate()（书中nltk2.0.1版本可用，但是新版本不再支持）\n计数词汇\n使用len函数获取文本长度（包括单词及标点） \u003e\u003e\u003elen(text3)\n获取无重复的词汇表 \u003e\u003e\u003esorted(set(text3))\n计算文本词汇丰富度\n\u003e\u003e\u003efrom __feature__ import division \u003e\u003e\u003elen(text3) / len(set(text3))\n特定单词计数 \u003e\u003e\u003etext3.count(\"smote\")\n1.2 近观Python：将文本当做词链表\n主要介绍Python中链表的相关操作。（略）\n1.3 计算语言：简单的统计\n频率分布\n统计文本中词的词频，降序排列保存至map中\n\u003e\u003e\u003efdist1 = FreqDist(text1) \u003e\u003e\u003evocabulary1 = fdist1.keys() \u003e\u003e\u003evocabulary1[:50]\nTop50词频可视化 \u003e\u003e\u003efdist1.plot(50,cumulative=True)\n\n查看文本中只出现一次的词 \u003e\u003e\u003efdist1.hapaxes()\n细粒度选择词\n找出文本中长度超过15的词 :\n\u003e\u003e\u003eV = set(text1) \u003e\u003e\u003elong_words = [w for w in V if len(w) \u003e 15] \u003e\u003e\u003esorted(long_words)\n词语搭配和双连词\n提取文本词汇中的词对\n\u003e\u003e\u003elist(bigrams(['more', 'is', 'said', 'than', 'done']))\n\ncollocations() 函数在已知单个词的词频基础上，找到出现频繁的双连词\n\u003e\u003etext4.collocations()\n\n计算其他东西\n查看文本中词长的分布\n\u003e\u003e\u003efdist = FreqDist([len(w) for w in text1]) \u003e\u003e\u003efdist.keys()\n\n输出结果表明text1中最长的词是由20个字符组成。\n\u003e\u003e\u003efdist.items()\n\n\u003e\u003e\u003efdist.max() \u003e\u003e\u003efdist.freq(3)\n\n由结果可知，文本中长度为3的词最频繁，约占20%。\nExamples\nDescriptions\nfdist = FreqDist(samples)\n创建包含给定样本的频率分布\nfdist.inc(samples)\n增加样本\nfdist['monstrous']\n计数给定样本出现的次数\nfdist.freq('monstrous')\n给定样本的频率\nfdist.N()\n样本总数\nfdist.keys()\n以频率递减顺序排序的样本链接\nfor sample in fdist:\n以频率递减的顺序遍历样本\nfdist.max()\n数值最大的样本\nfdist.tabulate()\n绘制频率分布表\nfdist.plot()\n绘制频率分布图\nfdist.plot(cumulative=True)\n绘制累积频率分布图\nfdist1\u003cfdist2\n测试样本在fdist1中出现的频率是否小于fdist2\n1.4 回到Python：决策与控制\n主要介绍for循环与条件语句（略）\n1.5 自动理解自然语言\n词义消岐\n指代消解 anaphora resolution\n自动生成语言\n遗憾地发现在nltk3.2里，书中的babelize_shell()这个服务也不再提供了。\n人机对话系统\n\u003e\u003e\u003eimport nltk \u003e\u003e\u003enltk.chat.chatbots()\n文本的含义","date":"2017年12月22日 15:47:18"}
{"_id":{"$oid":"5d36a8cd6734bd8e681d5e79"},"title":"阿里自然语言处理部总监分享：NLP技术的应用及思考","author":"喜欢打酱油的老鸟","content":"https://www.toutiao.com/a6679610377992405507/\n本文整理自阿里巴巴iDST自然语言处理部总监郎君博士的题为“NLP技术的应用及思考”的演讲。本文从NLP背景开始谈起，重点介绍了AliNLP平台，接着分享了NLP相关的应用实例，最后对NLP的未来进行了思考。\n背景介绍\n阿里巴巴的生态系统下面有很多的计算平台，上面有各种各样的业务层，最中间是买家和卖家之间包括销售、支付等等之间的关系，外面建了一圈从娱乐到广告到金融到购物到物流等等各方面这样一个生态，中间有非常多的数据能够关联起来，所以对于阿里巴巴而言，这个图可以非常简练的概括我们在做什么，中间是最重要的数据，下面数据包含了最核心的也是阿里巴巴最早起家的来自于电商的数据，所以电商对于我们而言是非常重要的，后来又扩展出了金融、菜鸟物流、健康和娱乐，比如我们有大文娱事业群，去做了优酷土豆等各种各样的数据，数据当中包含了很多的文本。\n比如阿里的电商平台里面有数十亿的商品，每一个商品都包含详细的标题、副标题、详情页、评价区，甚至问答区，这里面的信息构成了一个非常丰富的商品信息，还有上亿的文章，阿里在两年前开始进入内容时代，比如现在各种各样的内容营销、直播还有一些问答的场景圆桌等等，文章里面可以包含各种各样的标题、正文和评论等大量的数据，这只是电商的例子，还有金融、物流、健康、娱乐，加在一起还会有海量的数据，就会孕育出大量文本处理的工作需求。\n自然语言处理是什么呢？\n语言是生物同类之间由于沟通需要而制定的具有统一编码解码标准的声音(图像)指令。包含手势、表情、语音等肢体语言，文字是显像符号。\n自然语言通常是指一种自然地随文化演化的语言。例如英语、汉语、日语等。有别于人造语言，例如世界语、编程语言等。\n自然语言处理包括自然语言理解和自然语言生成。自然语言理解是将自然语言变成计算机能够理解的语言，及非结构化文本转变为结构化信息。\nNLP的 四大经典“AI 完全 ” 难题：问答、复述、文摘、翻译，只要解决其中一个，另外三个就都解决了。问答就是让机器人很开放的回答你提的各种各样问题，就像真人一样；复述是让机器用另外一种方式表达出来；文摘就是告诉你一篇很长的文章，让你写一个100字的文摘，把它做出来是非常难做的；翻译也是很困难的，英语思维方式和中文思维方式转换过来，中间会涉及到很多复杂的问题。\n阿里巴巴需要什么样的自然语言处理技术？\n阿里的生态是非常复杂的，我们不能用一个简单的自然语言处理技术去解决所有的问题，以往自然语言处理是比较简单的，甚至一个词表放上去就解决所有问题了，随着电商生态的扩展，就需要非常复杂的技术，所以我们需要完备且高性能的自然语言处理技术，高性能体现在算法精度还有执行效率，IDST的定位如下：\n-引领技术前沿-赶超市场最佳的竞争者，完备和完善AliNLP平台的技术体系及服务能力；\n-赋能核心业务-帮助核心业务快速成长，寻找和解决业务方的最痛点；\n-创造商业机会-创造看似不可能的商业技术，深度理解语言，深度理解需求，变革产品体验。\nAliNLP 自然语言技术平台\n图为我们整个自然语言处理平台最核心的框图，底层是各种各样的基础数据，中间层包含基本的词法分析、句法分析、语义分析、文档分析，还有其他各种各样跟深度学习相关的一些技术；上层是自然语言处理能够直接掌控和变革的一些算法和业务，比如内容搜索、内容推荐、评价、问答、文摘、文本理解等等一系列问题，最上层我们直接支持大业务的单元，比如商品搜索、推荐、智能交互、翻译。商业翻译和普通机器翻译是不一样的，还有广告、风控、舆情监控等等。这个层次结构是比较传统的方式，为了让我们平台具有非常好的落地能力，右边有一列平台工程，专门解决如何让算法能够快速的用到业务里面去。\n将核心框图细化，底层有各种各样的数据，比如实体库、源学辞典、词性标注库、词性关系库、句法树库、情感分析标注库，还有情感词典、资讯库、图谱等等。这些是词法分析，包括分词、词性、实体识别，拼写检查等一些基础的组件，句法分析有结构句法分析、依存句法分析、语义分布表示等等，还有语义分析，包含词义消歧、语义角色标注、主题模型、行为表示等。还有文档分析，比如普通的文档聚类、文档分类、事件挖掘、层次聚类和意图分类，其他部分就是我们尝试比较多的偏深度学习的一些自然语言算法。\n右边的平台工程我们做了很多尝试。团队经过几年的发展，不停的去反思如何把我们的技术快速的跟业务对接起来，经过不停的尝试之后，我们做了很多的可视化、需求管理、用户中心、监控中心、系统运维，还有自动的标注平台、训练平台、评测中心等等，经过一系列的封装，才会使得平台越来越完善。\n图为阿里AliNLP系统架构图，左边是算法模块，包括知识库、语料库、算法模型，中间是服务化平台，比如我们的服务分为在线服务和离线服务。离线服务有阿里巴巴最大的计算平台ODPS，里面做了很多这方面的UDF操作，在线有HSF和HTTP服务，可以很好的对接各种各样的相关服务方；中间有用户中心、监控中心、测试中心、系统运维等比较复杂的一套体系。右边是我们对接的一套生态平台，上面可以通过我们的接口层直接对接各种各样的应用。我们迭代了很多轮才出现这样的结构，现在大概支持30多个业务方，平均每天的调用量在数百亿规模。\nAliNLP平台核心价值\nAliNLP平台核心价值就是解耦。我们希望通过做这样一个平台，去面对整个阿里巴巴的生态系统：\n算法超市。我们希望平台是NLP算法超市，业务方可以清晰看到分门别类的NLP算法；\n工程小白。我们希望平台解决一切工程问题，算法工程师可以是工程小白只需专注算法研发；\n系统生态。对于系统，以此为中心形成一个系统生态体系，从各个环节切入服务NLP算法和业务；\n服务底线。对于产品运营，平台只做底层模型的服务输出，不直接对接业务。\n经过各种各样的迭代、打磨、思考、反思，5月初会发布2.0版本，我们希望做持续的改进。我们平台中最核心的三个概念如下：\n1.模型：最基本的算法逻辑复用单元，如果用算法超市的概念解释，模型就是原材料，模型是算法工程师的主要产出成果；\n2.方案：是多个模型的组合，用于真正解决某一方向的具体问题，类似于待售的超市商品。方案是业务、算法的结合之处，我们负责“算法售卖”的同学会应用手头已有的模型通过不同的组合配置，产生出不同的商品供最终业务方的用户使用；\n3.场景：是多个方案在线上部署的最终形态，是最终服务的提供者，是业务方真正使用我们的算法大礼包的地方。按目前的设计，不同的业务方可以在相互隔离的多个场景中使用算法服务。\n只有理解这三个概念，才会知道平台怎么去很好的使用。\nNLP算法举例\n下面对我们的算法做一些比较简单的举例。\n1.词法分析（分词、词性、实体）：\n–算法：基于Bi-LSTM-CRF算法体系，以及丰富的多领域词表\n–应用：优酷、YunOS、蚂蚁金服、推荐算法、资讯搜索等\n2.句法分析（依存句法分析、成分句法分析）：\n–算法：Shift-reduce，graph-based，Bi-LSTM\n–新闻领域、商品评价、商品标题、搜索Query\n–应用：资讯搜索、评价情感分析\n3.情感分析（情感对象、情感属性、情感属性关联）：\n–算法：情感词典挖掘，属性级、句子级、篇章级情感分析\n–应用：商品评价、商品问答、品牌舆情、互联网舆情\n4.句子生成（句子可控改写、句子压缩）：\n–算法：Beam Search、Seq2Seq+Attention\n–应用：商品标题压缩，资讯标题改写，PUSH消息改写\n5.句子相似度（浅层相似度、语义相似度）：\n–算法：Edit Distance，Word2Vec，DSSM\n–应用：问大家相似问题、商品重发检测、影视作品相似等\n6.文本分类/聚类（垃圾防控、信息聚合）：\n–算法：ME，SVM，FastText\n–应用：商品类目预测、问答意图分析、文本垃圾过滤、舆情聚类、名片OCR后语义识别等\n7.文本表示（词向量、句子向量、篇章向量、Seq2Seq）：\n–Word2Vec、LSTM、DSSM、Seq2Seq为基础进行深入研究\n8.知识库\n–数据规模：电商同义词，通用同义词，电商上下位，通用上下位，领域词库（电商词、娱乐领域词、通用实体词），情感词库\n–挖掘算法：bootstrapping，click-through mining，word2vec，k-means，CRF\n–应用：语义归一、语义扩展、Query理解、意图理解、情感分析\n9.语料库\n–分词、词性标注数据，依存句法标注数据\n有这样一句话叫“我要买秋天穿的红色连衣裙”，这句是电商领域中比较常见的，词法分析结果会把中间“我要”拆开。分词要分的很准，它不是每个单字都是一个词，比如秋天是一个词，连衣裙是一个词。下面这一层标签是对应的词性。上面这一层就是句子树型结构，它会比较深入的把句子比较深度的结构化。只有把它结构化之后才能导到数据库里面去，才能做后续的各种机器学习研究和应用，这种叫结构句法分析。\n对于电商而言，光有句法分析是不够的，比如我要知道秋天的含义是说这是个适用季节，红色是一个颜色分类，连衣裙是一个产品，要做到这一步才会使得真正在电商里面用起来。\n比如我们用的是通用领域依存分析器，我们针对商品标题决定某一个依存句法分析器，假设某一个商品标题写的是“我要买秋天穿的红色连衣裙”，只需要把“秋天”、“红色”、“连衣裙”这几个关键的成分标出来，“我要买”和“穿的”对电商而言是没有意义的，但会去做进一步的组合。\n如果这个句子是一个query，对于某些核心成分一点都不需要，完全不用看，直接会把它输出“秋天”、“红色”、“连衣裙”三个串，中间依存关系标出就可以了。这样可以做很好的信息凝练。这是我们针对三种不同类型的文本做的很深入的底层自然语言处理分析。\n这个例子是一个买家对于某一个商品写的一个评论，“虽然有点贵，不是很修身，但是颜色很亮，布料摸起来挺舒服的，图案也好看。挺喜欢的。”，上图是我们的情感分析结果，我们情感分析不但要知道整句的信息，比如说整句有蓝色、淡蓝色，淡蓝色表示情感是正向的，整个句子表达的是一个比较褒义的结果，但不是非常满意。\n再下面我们做的更深入一点，比如说贵、修身、颜色等等，做了很细粒度的一个拆解，这种叫属性级的情感分析。情感词比如说“贵”它是一个形容词，贵表达的是相对的关系，有时候说黄金很贵，这时就是一个褒义。所以这个词语非常复杂，不同环境下褒贬不一。如修身，这个平台里面表达修身是一个很严重的反向关系，所以我们就把它识别出来是个很红色的关系，只要经过很深度的细致分析之后，后面可以做各种各样的玩法。\n应用实例\n图中显示商品标题和副标题。 “2015年秋冬毛衣连衣裙我是证人杨幂同款宽松显瘦时尚打底针织连衣裙”，它不是一个自然语言的原句子，是一堆词语拼凑在一起的，副标题就自然一点。因为搜索引擎以关键词为核心算法，关键词堆砌的话搜索结果不会往前面排，销量就不好，所以标题就变成这个样子了。而副标题没有应用这种算法，副标题不进索引库，不能搜索，只是一个营销的额外宣传语。所以电商的自然语言处理是很有意思的。\n对标题做深度理解和分析的时候，我们知道商品的产品词、款式、材质、风格、服务营销、适用季节等，做到这种结构化后，就可以把一个文本串变成一个数据库。\n这个摆件的标题也可以做很深入的分析，也可以变成一长串结果，如果你要建一个电子商务搜索引擎的话，或者电子商务推荐引擎的话，只有做到这一步，才会使你的引擎更加智能。\n标题分析主要分四步：\n第一步先做分词。把第一行变成第二行，打空格用了很多算法、词表、人工、优化的思路；\n第二步是实体打标。需要知道每个词语是什么含义，粉红大布娃娃是个品牌，泡泡袖是个袖型等等，这样你的搜索引擎就更加智能一点；\n第三步是热度计算。把热度分数识别出来，因为串里面每个词不是等价的，有些重要性非常高，有些重要性非常低；\n第四步是中心识别。我们用依存句法分析方法来做，表达这个句子的最核心关系就是春装连衣裙，这里面可以做进一步的简化，选取合适的某一个维度的信息。这样，你的数据库就非常好了，可以做很多深入的工作。\n如果买家写的原始标题非常长，在PC上显示一个标题，但是在手机上显示一长串的时候，就会把标题按照字数限制截断。你会发现很多截断本来不应该，截断之后末尾那一串信息其实也是蛮关键的，我们把它变成如图一种关系，当买家来看商品信息的时候，在窄屏的区域里面能够很好的显示出来，所以就会使得我们的销量包括购买体验都会提升。\n关于舆情文本分析，我们有文本的分类、标签和文档聚类技术。假如你在手机淘宝app评价写了一堆东西，就进入了我们的流程。我们的系统叫摩天轮，会自动的把你写的每一条评论做各种各样的分析和处理，包括聚类的和标签的很细粒度的解析。\n商品评价\n有关商品的评价，我们积累了几百亿条评论，这是非常海量的一个数据库。它通过商品的搜索推荐还有文章的引导，到商品详情页之后，有上亿的人每一天在看评价，通过看详情页之后，你可以去做要么收藏，要么放购物车，要么直接购买的决策，后面才有支付订单管理，最后还有评价。写下来评价之后，评价会经过我们的过滤挖掘和展现，再回到详情页里面来，这就是一个闭环。真实评价对购物决策有重要作用，评价作为淘宝最大的UGC，富含对商品的体验和知识，浏览评价与否对收藏、加购、下单、客单价均有显著影响。\n上图为商品详情页，下面是正常写的评论，我们会在上面做大家印象，会把所有的评论做一个综合的摘取和总结，点击某一个，下面就会变成一堆相关文本筛选出来，并且把那一段描述的文本高亮。\n图为我们的算法总架构图。如果要做某一个电商类或者某种服务体系的评价系统，可以采用这种模式。左边是一种溯源的机制，我们希望鼓励用户去写更多更好的评价，包括交互的优化，去提升有用评论的积累。有了数据之后，我们要去做去伪。去伪就是我们有一个评价雾霾工作，会把文本和图片的垃圾都去掉，做好之后才能保证信息是比较真实的。我们会对核心数据库做语义分析，会结合某一些类目来做，做完之后我们会考虑它的时效性和个性化，还有买卖家模型，再做排序折叠和大家印象的扶优。然后再做增值，我们会有一些比如优质内容库、推荐理由、评价有赏。通过评价去发现商品的品质好不好，是不是假货，物流满意度如何，这里面可以做很多很深入的分析。\n评价雾霾中间是非常复杂的一套体系，有很多工程很多算法迭代了很久，比如说广告的样本怎么采集，有全类目的和分类目的，还有正常广告的，怎么去做拆分，有一些基础特征库比如用户特征、文本特征、模型特征、行为特征等做融合，最后再用一个treelink模型，把maxent模型、贝叶斯模型和dbn模型总体做融合，然后再回流，一天一天迭代。\n问大家\n商品中有另外一个很有趣的产品叫问大家。以买奶粉为例，假如你有五个邻居，有三个邻居买过同一款奶粉，你要买奶粉可能希望多问两家，如果三个人都买过A奶粉，三个人的回答结果综合看一看，做最终的决策。我们把它做成产品化，那我们做一个问题的拆解分为四类：无效问题、相似问题、问答排序、智能分发。\n问大家3个问题解析如下：\n无效问题过滤\n–专业的外包同学标注无效问题，Active Learning筛选待标注样本\n–分类采用LR+GBDT，定制特征\n–无效问题会不断变种，算法和标注迭代推进\n相似问题识别\n–Doc2Vec然后计算相似度，人工评测\n页面问答排序\n–内容丰富度、点赞数、过滤词表匹配数等加权求和\n–Detail页透出的一条问大家CTR 提升\n内容资讯分析\n针对内容我们需要做大量的分析，比如说底层我们有各种各样的数据库要汇总，中间有一些文本算法，比如说相关性、时效和质量、CTR预估、个性化、分类、打标、质量和去重等等，中间也有一些系统工程，还有服务体系。上面是业务场景，比如淘秘籍、微淘、淘宝头条、知识卡片、社区问答等等，会让你迅速进入一个很好的购物背景知识状态，使你做更好的购物决策。你可以在手机淘宝搜索结果页的第四个Tab里看到我们的淘秘籍产品。\n思考\n自然语言处理难在哪呢？它涉及到人的认知，知识\u003c=\u003e语言\u003c=\u003e思考\u003c=\u003e行动，左边专注到知识，右边专注到思考和行动。它是非常复杂的，最难的问题有两个：第一就是歧义，自然语言与计算机语言是完全不可调和的，计算机语言是精确的、可枚举的、无歧义的。第二是变化，变化是非常剧烈的。它的语法是群体一致，个体有差异，语言每天都在发生变化，新词总在不断的产生，无法穷举， 不同上下文不同含义，甚至随时间推移，词义也在发生变化，例如Apple-\u003e公司，甚至词性也在发生变化，如Google -\u003eto google 。\n那么，NLP怎么走？\n在完全搞清人脑机制前，NLP研发永远是在模拟人类群体智慧在某些文字方面的表现；\n这种模仿的效果会越来越好，持续提升；\n更深入的模拟是，NLP会和语音、图像、视频、触觉等多维度信息融合学习。\n我们未来会做什么，我们在一年之内会继续把AliNLP平台做的完备和完善，开放更多的能力，服务好阿里的各种生态系统。我们希望调用量能超过千亿，两年之内我们争取能够对外开放，普惠大众，更好的开放融合，调用量希望达到万亿，我们希望做的更美好！\n上乘：阿里巴巴iDST 自然语言处理部总监，博士毕业于哈尔滨工业大学自然语言处理方向，曾在新加坡资讯技术研究院工作四年担任研究科学家负责统计机器翻译系统的研发和应用，2014年至今在阿里巴巴iDST担任资深专家，从零组建了自然语言处理部门，负责自然语言处理技术平台的研发和多项核心业务应用。","date":"2019年04月17日 08:30:27"}
{"_id":{"$oid":"5d36a8cd6734bd8e681d5e7b"},"title":"自然语言处理入门心得——书籍、课程推荐","author":"Shingle_","content":"MOOCs\nMIT 的 Natural Language Processing\nStanford 的cs224n Natural Language Processing\nStanford 的 CS224d: Deep Learning for Natural Language Processing ：讲述深度学习在自然语言处理方面比较成功的应用。\nBOOKs\n入门首选：吴军博士的《数学之美》：深入浅出的讲述了数学在计算机领域的应用，体现了数学的简单美，书中主要涉及了自然语言处理的一些内容。\n宗成庆的《统计自然语言处理》：全面介绍了统计自然语言处理的基本概念、理论方法和最新研究进展。尤其是讲述了中文的自然语言处理。\n《统计自然语言处理基础》：统计自然语言处理的一本著作。\n《Python自然语言处理》：主要讲NLTK这个包的使用。\nLibs\nNLTK：Python的自然语言处理包","date":"2016年07月25日 22:49:41"}
{"_id":{"$oid":"5d36a8cd6734bd8e681d5e7d"},"title":"自然语言处理之数据平滑方法","author":"Vincent-Yuan","content":"在自然语言处理中，经常要计算单词序列（句子）出现的概率估计。但是，算法训练的时候，预料库中不可能包含所有可能出现的序列，因此为了防止对训练样本中为出现的新序列概率估计值为零，人们发明了不少可以改善估计新序列出现的概率算法，即数据的平滑。最常见的数据平滑算法包括如下几种：\nAdd-one (Laplace) smoothing\nAdd-k smoothing\nBackoff回退法\nInterpolation插值法\nAbsolute discounting\nKneser-Ney smoothing\nModified Kneser-ney smoothing\n这几个方法实际上可以简单的理解为三种不同的方法：第一种类型为政府给大家每人一笔或者几笔钱（如1和2），第二种为找父母要（如3和4），最后一种就是劫富济贫（如5-7）。下面依次简单介绍上面的方法，具体详细的介绍，大家可以参阅相应的论文和书籍。\n数据预处理\n在介绍上面几种平滑的方法之前，这里先给出一个简单的的数据预处理的方法，特别是对于OOV（需要训练的词不在词袋里面）的情况特别有效，而且如果训练的时候，如果有几十万的词汇，一般不会对这几十万的词汇进行全部训练，而是需要预先做下面的处理后再进行数据的平滑和训练。\n假设训练数据集中出现了|N|个不同的词汇，那么可以根据词频对这些词汇进行排序，可以选择词频最高的M个词汇作为我们的词汇集合，这样在训练和测试数据集中，将不属于V的词汇都替换成特殊的词汇UNK，这样可以大大减少计算量，也可以提高计算的精度。\nAdd-one (Laplace) smoothing\nAdd-one 是最简单、最直观的一种平滑算法，既然希望没有出现过的N-gram的概率不再是0，那就直接规定在训练时任何一个N-gram在训练预料至少出现一次（即规定没有出现的，在语料中也出现一次），因此：Countnew(n-gram) = countold(n-gram)+1;\n于是对于n-gram的模型而言，假设V是所有可能的不同的N-gram的类型个数，那么根据贝叶斯公式有\n当然这里的n-gram的可以相应的改成uingram和bigram表达式，并不影响。其中C（x）为x在训练中出现的次数，wi为给定的训练数据中第i个单词。\n这样一来，训练语料库中出现的n-gram的概率不再为0，而是一个大于0的较小的概率值，Add-one平滑算法确实解决了我们的问题，但是显然它也并不完美，由于训练语料中未出现的n-gram数量太多，平滑后，所有未出现的占据了整个概率分布的一个很大的比例，因此，在自然语言处理中，Add-one给语料库中没有出现的n-gram分配了太多的概率空间。此外所有没有出现的概率相等是不是合理，这也是需要考虑的。\nAdd-k smoothing\n由Add-one衍生出来的另一种算法就是Add-k，既然我们认为加1有点过了，那么我们可以选择一个小于1的正数k，概率计算公式就可以变成如下表达式：\n它的效果通常会比Add-one好，但是依旧没有办法解决问题，至少在实践中，k必须认为的给定，而这个值到底多少该取多少都没有办法确定。\nBackoff回退法\n回退模型，思路实际上是：如果你自己有钱，那么就自己出钱，如果你自己没有钱，那么就你爸爸出，如果你爸爸没有钱，就你爷爷出，举一个例子，当使用Trigram的时候，如果Count（trigram）满足条件就使用，否则使用Bigram，再不然就使用Unigram.\n它也被称为：Katz smoothing，具体的可以去查看相应的书籍。\n它的表达式为：\n其中d，a和k分别为参数。k一般选择为0，但是也可以选其它的值。\nInterpolation插值法\n插值法和回退法的思想非常相似，设想对于一个trigram的模型，我们要统计语料库中“”“I like chinese food”出现的次数，结果发现它没有出现，则计数为0，在回退策略中们将会试着用低阶的gram来进行替代，也就是用“like chinese food”出现的次数来替代。在使用插值的时候，我们把不同阶层的n-gram的模型线性叠加组合起来之后再使用，简单的如trigram的模型，按照如下的方式进行叠加：\n参数可以凭借经验进行设定，也可以通过特定的算法来进行确定，比如EM算法。对于数据一般可以分为: traning set, development set, testing set. 那么P的概率使用training set进行训练得出，lamda参数使用development set得到。\nAbsolute discounting\n插值法使用的参数实际上没有特定的选择，如果将lamda参数根据上下文进行选择的话就会演变成Absolute discounting。对于这个算法的基本想法是，有钱的，每个人交固定的税D，建立一个基金，没有钱的根据自己的爸爸有多少钱分这个基金。比如对于bigram的模型来说，有如下公式。\nD为参数，可以通过测试优化设定。\nKneser-Ney smoothing\n这种算法是目前一种标准的而且是非常先进的平滑算法，它其实相当于前面讲过的几种算法的综合。它的思想实际上是：有钱的人，每个人交一个固定的税D，大家一起建立一个基金，没有钱的呢，根据自己的的爸爸的“交际的广泛”的程度来分了这个基金。这里交际的广泛实际上是指它爸爸会有多少种不同的类型，类型越多，这说明越好。其定义式为：\n其中max（c(X)-D,0）的意思是要保证最后的计数在减去一个D后不会变成一个负数，D一般大于0小于1。这个公式递归的进行，直到对于Unigram的时候停止。而lamda是一个正则化的常量，用于分配之前的概率值（也就是从高频词汇中减去的准备分配给哪些未出现的低频词的概率值（分基金池里面的基金））。其表达是为：\nPKN是在wi固定的情况下，unigram和bigram数目的比值，这里需要注意的是PKN是一个分布，它是一个非负的值，求和的话为1。\nModified Kneser-ney smoothing\n这一种方法是上一种方法的改进版，而且也是现在最优的方法。上一个方法，每一个有钱的人都交一个固定的锐，这个必然会出现问题，就像国家收税一样，你有100万和你有1个亿交税的量肯定不一样这样才是比较合理的，因此将上一种方法改进就是：有钱的每个人根据自己的收入不同交不同的税D，建立一个基金，没有钱的，根据自己的爸爸交际的广泛程度来分配基金。\n这里D根据c来设定不同的值，比如c为unigram，则使用D1，c位bigram，则使用D2，如果是大于等于3阶的使用D3.\n转自：微信公众号：自然语言处理技术\n参考书籍：\n[1] Speech and language processing, Daniel Jurafsky, et la.\n[2] 语音识别实践，俞栋等人。","date":"2018年08月10日 18:26:08"}
{"_id":{"$oid":"5d36a8ce6734bd8e681d5e7f"},"title":"PythonNLP学习进阶：第二章练习题（Python自然语言处理）","author":"txlCandy","content":"python自然语言处理.2014年7月第一版课后习题练习\n1.\n\u003e\u003e\u003e phrase=[\"Valentine's\"] \u003e\u003e\u003e phrase=[\"lonely\"]+phrase+[\"day\"] \u003e\u003e\u003e phrase ['lonely', \"Valentine's\", 'day'] \u003e\u003e\u003e phrase[1] \"Valentine's\" \u003e\u003e\u003e phrase[1][1] 'a' \u003e\u003e\u003e phrase.index('day') 2 \u003e\u003e\u003e sorted(phrase) [\"Valentine's\", 'day', 'lonely'] \u003e\u003e\u003e phrase[1:2] [\"Valentine's\"] \u003e\u003e\u003e phrase*3 ['lonely', \"Valentine's\", 'day', 'lonely', \"Valentine's\", 'day', 'lonely', \"Valentine's\", 'day']\n\n\n2.\n\u003e\u003e\u003e from nltk.corpus import gutenberg \u003e\u003e\u003e gutenberg.fileids() [u'austen-emma.txt', u'austen-persuasion.txt', u'austen-sense.txt', u'bible-kjv.txt', u'blake-poems.txt', u'bryant-stories.txt', u'burgess-busterbrown.txt', u'carroll-alice.txt', u'chesterton-ball.txt', u'chesterton-brown.txt', u'chesterton-thursday.txt', u'edgeworth-parents.txt', u'melville-moby_dick.txt', u'milton-paradise.txt', u'shakespeare-caesar.txt', u'shakespeare-hamlet.txt', u'shakespeare-macbeth.txt', u'whitman-leaves.txt'] \u003e\u003e\u003e persuasion=gutenberg.words('austen-persuasion.txt') \u003e\u003e\u003e len(persuasion) 98171 \u003e\u003e\u003e len(set(persuasion))//词类型，我不知道是不是指有多少个不一样的词 6132\n3.\n\u003e\u003e\u003e from nltk.corpus import brown \u003e\u003e\u003e brown.categories() [u'adventure', u'belles_lettres', u'editorial', u'fiction', u'government', u'hobbies', u'humor', u'learned', u'lore', u'mystery', u'news', u'religion', u'reviews', u'romance', u'science_fiction'] \u003e\u003e\u003e brown.words(categories='lore') [u'In', u'American', u'romance', u',', u'almost', ...] \u003e\u003e\u003e brown.words(categories='mystery') [u'There', u'were', u'thirty-eight', u'patients', ...]\n\u003e\u003e\u003e from nltk.corpus import webtext \u003e\u003e\u003e webtext.fileids() [u'firefox.txt', u'grail.txt', u'overheard.txt', u'pirates.txt', u'singles.txt', u'wine.txt'] \u003e\u003e\u003e webtext.words('firefox.txt') [u'Cookie', u'Manager', u':', u'\"', u'Don', u\"'\", u't', ...] \u003e\u003e\u003e webtext.words('grail.txt') [u'SCENE', u'1', u':', u'[', u'wind', u']', u'[', ...]\n4.\n\u003e\u003e\u003e from nltk.corpus import state_union as su \u003e\u003e\u003e su.fileids() [u'1945-Truman.txt', u'1946-Truman.txt', u'1947-Truman.txt', u'1948-Truman.txt', u'1949-Truman.txt', u'1950-Truman.txt', u'1951-Truman.txt', u'1953-Eisenhower.txt', u'1954-Eisenhower.txt', u'1955-Eisenhower.txt', u'1956-Eisenhower.txt', u'1957-Eisenhower.txt', u'1958-Eisenhower.txt', u'1959-Eisenhower.txt', u'1960-Eisenhower.txt', u'1961-Kennedy.txt', u'1962-Kennedy.txt', u'1963-Johnson.txt', u'1963-Kennedy.txt', u'1964-Johnson.txt', u'1965-Johnson-1.txt', u'1965-Johnson-2.txt', u'1966-Johnson.txt', u'1967-Johnson.txt', u'1968-Johnson.txt', u'1969-Johnson.txt', u'1970-Nixon.txt', u'1971-Nixon.txt', u'1972-Nixon.txt', u'1973-Nixon.txt', u'1974-Nixon.txt', u'1975-Ford.txt', u'1976-Ford.txt', u'1977-Ford.txt', u'1978-Carter.txt', u'1979-Carter.txt', u'1980-Carter.txt', u'1981-Reagan.txt', u'1982-Reagan.txt', u'1983-Reagan.txt', u'1984-Reagan.txt', u'1985-Reagan.txt', u'1986-Reagan.txt', u'1987-Reagan.txt', u'1988-Reagan.txt', u'1989-Bush.txt', u'1990-Bush.txt', u'1991-Bush-1.txt', u'1991-Bush-2.txt', u'1992-Bush.txt', u'1993-Clinton.txt', u'1994-Clinton.txt', u'1995-Clinton.txt', u'1996-Clinton.txt', u'1997-Clinton.txt', u'1998-Clinton.txt', u'1999-Clinton.txt', u'2000-Clinton.txt', u'2001-GWBush-1.txt', u'2001-GWBush-2.txt', u'2002-GWBush.txt', u'2003-GWBush.txt', u'2004-GWBush.txt', u'2005-GWBush.txt', u'2006-GWBush.txt'] \u003e\u003e\u003e fdist1=nltk.ConditionalFreqDist( ... (object,file[0:4]) ... for file in su.fileids() ... for w in su.words(file) ... for object in ['men','women','people'] ... if w.lower().startswith(object)) \u003e\u003e\u003e fdist1.plot()\n\n可以思考女权运动这个问题\n\n5.\n首先，复习一下2.5 WordNet\n（1）意义与同义词（类属关系 AKO）\nsynset--同义词     lemma--词条\n词条：motorcar  属于哪一个同义词集合\n\u003cspan style=\"color:#3333ff;\"\u003e\u003e\u003e\u003e from nltk.corpus import wordnet as wn \u003e\u003e\u003e a=wn.synsets(\"motorcar\") \u003e\u003e\u003e a [Synset('car.n.01')]\u003c/span\u003e\n该同义词集合有哪些词条，也可以仅仅显示词条的名称\n\u003cspan style=\"color:#3333ff;\"\u003e\u003e\u003e\u003e wn.synset('car.n.01').lemmas() [Lemma('car.n.01.car'), Lemma('car.n.01.auto'), Lemma('car.n.01.automobile'), Lemma('car.n.01.machine'), Lemma('car.n.01.motorcar')] \u003e\u003e\u003e wn.synset('car.n.01').lemma_names() [u'car', u'auto', u'automobile', u'machine', u'motorcar']\u003c/span\u003e\n\u003cspan style=\"color:#3333ff;\"\u003e也可以显示词条的定义和事例\u003c/span\u003e\n\u003cspan style=\"color:#3333ff;\"\u003e\u003e\u003e\u003e wn.synset('car.n.01').definition() u'a motor vehicle with four wheels; usually propelled by an internal combustion engine' \u003e\u003e\u003e wn.synset('car.n.01').examples() [u'he needs a car to get to work']\u003c/span\u003e\n\n\n提示：在课本上事例为：\n\u003cspan style=\"color:#3333ff;\"\u003e\u003e\u003e\u003e wn.synset('car.n.01').definition \u003cbound method Synset.definition of Synset('car.n.01')\u003e\u003c/span\u003e\n\n答案会显示异常\n(2) WordNet的层次结构\n2.1上下位词( hyponyms hypernyms)（类属关系中ISA的关系）\n\u003cspan style=\"color:#3333ff;\"\u003e\u003e\u003e\u003e a=wn.synset('car.n.01') \u003e\u003e\u003e print(a.hyponyms()) [Synset('ambulance.n.01'), Synset('beach_wagon.n.01'), Synset('bus.n.04'), Synset('cab.n.03'), Synset('compact.n.03'), Synset('convertible.n.01'), Synset('coupe.n.01'), Synset('cruiser.n.01'), Synset('electric.n.01'), Synset('gas_guzzler.n.01'), Synset('hardtop.n.01'), Synset('hatchback.n.01'), Synset('horseless_carriage.n.01'), Synset('hot_rod.n.01'), Synset('jeep.n.01'), Synset('limousine.n.01'), Synset('loaner.n.02'), Synset('minicar.n.01'), Synset('minivan.n.01'), Synset('model_t.n.01'), Synset('pace_car.n.01'), Synset('racer.n.02'), Synset('roadster.n.01'), Synset('sedan.n.01'), Synset('sport_utility.n.01'), Synset('sports_car.n.01'), Synset('stanley_steamer.n.01'), Synset('stock_car.n.01'), Synset('subcompact.n.01'), Synset('touring_car.n.01'), Synset('used-car.n.01')] \u003e\u003e\u003e print(a.hypernyms()) [Synset('motor_vehicle.n.01')]\u003c/span\u003e\n计算到car.n.01的路径数\n\u003cspan style=\"color:#3333ff;\"\u003e\u003e\u003e\u003e path=a.hypernym_paths() \u003e\u003e\u003e len(path) 2 \u003e\u003e\u003e [synset.name() for synset in path[0]] [u'entity.n.01', u'physical_entity.n.01', u'object.n.01', u'whole.n.02', u'artifact.n.01', u'instrumentality.n.03', u'container.n.01', u'wheeled_vehicle.n.01', u'self-propelled_vehicle.n.01', u'motor_vehicle.n.01', u'car.n.01'] \u003e\u003e\u003e [synset.name() for synset in path[1]] [u'entity.n.01', u'physical_entity.n.01', u'object.n.01', u'whole.n.02', u'artifact.n.01', u'instrumentality.n.03', u'conveyance.n.03', u'vehicle.n.01', u'wheeled_vehicle.n.01', u'self-propelled_vehicle.n.01', u'motor_vehicle.n.01', u'car.n.01']\u003c/span\u003e\n2.2 蕴含关系\n\u003cspan style=\"color:#3333ff;\"\u003e\u003e\u003e\u003e wn.synset('walk.v.01').entailments() [Synset('step.v.01')]\u003c/span\u003e\n2.3 反义关系——互斥\n\u003cspan style=\"color:#3333ff;\"\u003e\u003e\u003e\u003e wn.lemma('rush.v.01.rush').antonyms() [Lemma('linger.v.04.linger')]\u003c/span\u003e\n2.4 查看词条拥有哪些关系\n\u003cspan style=\"color:#3333ff;\"\u003e\u003e\u003e\u003e dir(wn.synset('harmony.n.02')) ['__class__', '__delattr__', '__dict__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_all_hypernyms', '_definition', '_examples', '_frame_ids', '_hypernyms', '_instance_hypernyms', '_iter_hypernym_lists', '_lemma_names', '_lemma_pointers', '_lemmas', '_lexname', '_max_depth', '_min_depth', '_name', '_needs_root', '_offset', '_pointers', '_pos', '_related', '_shortest_hypernym_paths', '_wordnet_corpus_reader', 'also_sees', 'attributes', 'causes', 'closure', 'common_hypernyms', 'definition', 'entailments', 'examples', 'frame_ids', 'hypernym_distances', 'hypernym_paths', 'hypernyms', 'hyponyms', 'instance_hypernyms', 'instance_hyponyms', 'jcn_similarity', 'lch_similarity', 'lemma_names', 'lemmas', 'lexname', 'lin_similarity', 'lowest_common_hypernyms', 'max_depth', 'member_holonyms', 'member_meronyms', 'min_depth', 'name', 'offset', 'part_holonyms', 'part_meronyms', 'path_similarity', 'pos', 'region_domains', 'res_similarity', 'root_hypernyms', 'shortest_path_distance', 'similar_tos', 'substance_holonyms', 'substance_meronyms', 'topic_domains', 'tree', 'unicode_repr', 'usage_domains', 'verb_groups', 'wup_similarity']\u003c/span\u003e\n回顾完毕！\n这一题属于以下关系\n2.5  整体部分关系\n整体与部分关系有三种：member_holonyms()   集合概念，把事物看成构成的一部分；part_meronyms()  肢解后的小部分；substance_meronyms() 事物构成的本质\n\u003e\u003e\u003e wn.synset('tree.n.01').member_holonyms()//树的集合是森林 [Synset('forest.n.01')]\n\u003e\u003e\u003e wn.synset('dog.n.01').member_holonyms()//第一个：狗是犬属 [Synset('canis.n.01'), Synset('pack.n.06')]\n\u003e\u003e\u003e wn.synset('hand.n.01').part_meronyms()//下面为hand的构成部分 [Synset('ball.n.10'), Synset('digital_arteries.n.01'), Synset('finger.n.01'), Synset('intercapitular_vein.n.01'), Synset('metacarpal_artery.n.01'), Synset('metacarpal_vein.n.01'), Synset('metacarpus.n.01'), Synset('palm.n.01')]\n\u003e\u003e\u003e wn.synset('tree.n.01').substance_meronyms()//树的实质是心材和边材 [Synset('heartwood.n.01'), Synset('sapwood.n.01')]\n6.\n不懂～\n7.\nwwe","date":"2016年02月13日 11:55:52"}
{"_id":{"$oid":"5d36a8ce6734bd8e681d5e81"},"title":"自然语言处理（NLP）服务-从非结构化数据中获得更好的见解","author":"SZ laoluo","content":"目录\n非结构化数据获取\n原语言处理\n文本挖掘，文本提取和查询，以便进行改进的搜索\n统计语言处理\n技术资产支持自然语言处理（NLP）\n问答系统开发\n自然语言处理（NLP）正迅速成为现代组织获得竞争优势的基本技能。它已成为许多新业务功能的重要工具，从聊天机器人和问答系统到情感分析，合规性监控，医疗见解以及非结构化和半结构化内容的BI和分析。\n考虑所有可以带来重要见解的非结构化内容 - 查询，电子邮件通信，社交媒体，视频，客户评论，客户支持请求等。自然语言处理（NLP）工具和技术有助于处理，分析和理解非结构化的“大数据” “为了有效和积极地运作。\n我们的 自然语言处理服务涵盖了从数据采集和处理到分析，实体提取，事实提取和问答系统（考虑专为您的企业构建的数字助理）的各种需求。\n非结构化数据获取\n十多年来，我们帮助组织从外部和内部资源中获取非结构化内容，以进行搜索和分析。我们的顾问在识别和提取数据方面经验丰富：\n可通过互联网上的付费和免费内容来源获取可下载的数据\nSearch Technologies 与流行业务内容存储库的安全连接器\n原语言处理\n由于原始数据因来自不同来源而异，因此一旦获取内容，我们会提供数据清理和格式化服务，以确保您的数据准备好以获得最高质量的结果。\n确定格式（例如PDF，XML，HTML等）\n提取文本内容\n识别并删除不相关的部分（常见的页眉，页脚，侧边栏，样板）\n识别差异和变化\n提取编码的元数据\n令牌提取，规范化和清理\n短语提取\n文本挖掘，文本提取和查询，以便进行改进的搜索\n在许多用例中，内容以自然语言（例如英语，中文，西班牙语等）写下，但不方便标记。我们有工具和技术来帮助您从此内容中提取信息。可以利用某些级别的文本挖掘，文本提取或可能的完整NLP。\n典型的全文提取包括：\n实体提取 - 例如公司，人员，金额，关键举措等。\n内容分类 - 正面或负面（例如情绪分析）; 按功能，意图或目的; 或按行业或其他类别进行分析和趋势分析\n内容聚类 - 识别话语的主要主题和/或发现新主题\n事实提取 - 使用结构化信息填充数据库，以进行分析，可视化，趋势分析和警报\n关系提取 - 填写图形数据库以探索现实世界的关系\n统计语言处理\n在许多NLP项目中，统计技术可以提供对整个文档的一般理解。我们工作过的示例统计处理用例包括：\n聚类\n分类\n相似\n主题分析\n词云\n概要\n技术资产支持自然语言处理（NLP）\n洞察力驱动的企业越来越多地寻求利用庞大的非结构化数据来加速和改善业务成果。但是现有的自然语言处理技术并不能满足企业的需求 - 它们太狭隘（聊天机器人），太浅薄和通用（基于云的自然语言处理解决方案），或者开发，部署和维护成本太高。\n作为技术资产收集的一部分，Saga Natural Language Understanding（NLU）是一个可扩展，经济高效且易于使用的框架，填补了现有NLP / NLU技术的空白。\n问答系统开发\n问答系统（也称为“Insight Engines” - 由Gartner创造的一个术语）解析自然语言问题的查询，然后与后端系统集成以提供直接答案，而不仅仅是包含关键字的结果列表。\n可以使用搜索技术的自然语言处理工具包结合一套先进且可扩展的自然语言处理工具来构建问答系统，该工具可以执行查询理解所需的所有必要功能。我们的NLP工具包括：\n符号化\n缩写标准化\n词形还原\n句子和短语边界\n实体提取（所有类型但不统计）\n统计短语提取\n问题模式识别\n统计消歧\n对行动回应的问答\n业务用户界面（见下文）\n\nNLP问答系统的好处：\n许多业务用户界面可用于输入和维护实体和模式。这些接口允许没有编程经验的业务用户输入和维护公共实体和问题/响应模式。\n程序员干预只需要与后端系统集成。\n答案可以从关系数据库，RESTful API到任何业务系统，或从搜索引擎结果中提取。\n根据您的要求，答案可以格式化为自然语言响应或图表，报告或交互式图形。","date":"2019年03月26日 16:42:42","data":"2019年03月26日 16:42:42"}
{"_id":{"$oid":"5d36a8cf6734bd8e681d5e83"},"title":"Python自然语言处理（NLP）工具小结","author":"qq_36981835","content":"Python 的几个自然语言处理工具\n自然语言处理（Natural Language Processing，简称NLP）是人工智能的一个子域。自然语言处理的应用包括机器翻译、情感分析、智能问答、信息提取、语言输入、舆论分析、知识图谱等方面。也是深度学习的一个分支。首先介绍一下Python的自然语言处理工具包：\n1.NLTK工具包\nNLTK 在用 Python 处理自然语言的工具中处于领先的地位。它提供了 WordNet 这种方便处理词汇资源的接口，还有分类、分词、除茎、标注、语法分析、语义推理等类库。\n2.Jieba工具包\n3.Pattern工具包\nPattern 工具包包括词性标注工具(Part-Of-Speech Tagger)，N元搜索(n-gram search)，情感分析(sentiment analysis)，WordNet。同时也支持机器学习的向量空间模型，聚类和支持向量机。\n4.TextBlob\nTextBlob 是一个处理文本数据的 Python 库。提供了一些简单的api解决一些自然语言处理的任务，例如词性标注、名词短语抽取、情感分析、分类、翻译等等。\n5.Gensim\nGensim 提供了对大型语料库的主题建模、文件索引、相似度检索的功能。它可以处理大于RAM内存的数据，作者说它是“实现无干预从纯文本语义建模的最强大、最高效、最无障碍的软件”。\n6.PyNLPI\nPython自然语言处理库（Python Natural Language Processing Library，音发作: pineapple） 这是一个各种自然语言处理任务的集合，PyNLPI可以用来处理N元搜索，计算频率表和分布，建立语言模型。他还可以处理向优先队列这种更加复杂的数据结构，或者像 Beam 搜索这种更加复杂的算法。\n7.spaCy\nspaCy是一个商业的开源软件，结合Python和Cython，自然语言处理能力达到了工业强度。是领域内速度最快、最先进的自然语言处理工具。\n8.Polyglot\nPolyglot 支持对海量文本和多语言的处理。它支持对165种语言的分词，对196种语言的辨识，40种语言的专有名词识别，16种语言的词性标注，136种语言的情感分析，137种语言的嵌入，135种语言的形态分析，以及69中语言的翻译。\n9.MontyLingua（英文）\nMontyLingua 是一个自由的、训练有素的、端到端的英文处理工具。输入原始英文文本到 MontyLingua ，就会得到这段文本的语义解释。适合用来进行信息检索和提取，问题处理，回答问题等任务。从英文文本中，它能提取出主动宾元组，形容词、名词和动词短语，人名、地名、事件，日期和时间等语义信息。\n10.BLLIP Parser\nBLLIP Parser（也叫做Charniak-Johnson parser）是一个集成了产生成分分析和最大熵排序的统计自然语言工具。包括 命令行 和 python接口 。\n11.Quepy\nQuepy是一个Python框架，提供将自然语言转换成为数据库查询语言，可以轻松地实现不同类型的自然语言和数据库查询语言的转化。所以，通过Quepy，仅仅修改几行代码，就可以实现你自己的自然语言查询数据库系统。\nGitHub:https://github.com/machinalis/quepy\n12.HanNLP\nHanLP是一个致力于向生产环境普及NLP技术的开源Java工具包，支持中文分词（N-最短路分词、CRF分词、索引分词、用户自定义词典、词性标注），命名实体识别（中国人名、音译人名、日本人名、地名、实体机构名识别），关键词提取，自动摘要，短语提取，拼音转换，简繁转换，文本推荐，依存句法分析（MaxEnt依存句法分析、神经网络依存句法分析）。\n文档使用操作说明：Python调用自然语言处理包HanLP 和 菜鸟如何调用HanNLP\n【参考文献】：\n1.Python自然语言处理工具小结","date":"2018年01月03日 14:00:51"}
{"_id":{"$oid":"5d36a8cf6734bd8e681d5e86"},"title":"自然语言处理快速入门","author":"芦金宇","content":"自然语言处理（简称NLP），是研究计算机处理人类语言的一门技术，包括：\n1.句法语义分析：对于给定的句子，进行分词、词性标记、命名实体识别和链接、句法分析、语义角色识别和多义词消歧。\n2.信息抽取：从给定文本中抽取重要的信息，比如，时间、地点、人物、事件、原因、结果、数字、日期、货币、专有名词等等。通俗说来，就是要了解谁在什么时候、什么原因、对谁、做了什么事、有什么结果。涉及到实体识别、时间抽取、因果关系抽取等关键技术。\n3.文本挖掘（或者文本数据挖掘）：包括文本聚类、分类、信息抽取、摘要、情感分析以及对挖掘的信息和知识的可视化、交互式的表达界面。目前主流的技术都是基于统计机器学习的。\n4.机器翻译：把输入的源语言文本通过自动翻译获得另外一种语言的文本。根据输入媒介不同，可以细分为文本翻译、语音翻译、手语翻译、图形翻译等。机器翻译从最早的基于规则的方法到二十年前的基于统计的方法，再到今天的基于神经网络（编码-解码）的方法，逐渐形成了一套比较严谨的方法体系。\n5.信息检索：对大规模的文档进行索引。可简单对文档中的词汇，赋之以不同的权重来建立索引，也可利用1，2，3的技术来建立更加深层的索引。在查询的时候，对输入的查询表达式比如一个检索词或者一个句子进行分析，然后在索引里面查找匹配的候选文档，再根据一个排序机制把候选文档排序，最后输出排序得分最高的文档。\n6.问答系统： 对一个自然语言表达的问题，由问答系统给出一个精准的答案。需要对自然语言查询语句进行某种程度的语义分析，包括实体链接、关系识别，形成逻辑表达式，然后到知识库中查找可能的候选答案并通过一个排序机制找出最佳的答案。\n7.对话系统：系统通过一系列的对话，跟用户进行聊天、回答、完成某一项任务。涉及到用户意图理解、通用聊天引擎、问答引擎、对话管理等技术。此外，为了体现上下文相关，要具备多轮对话能力。同时，为了体现个性化，要开发用户画像以及基于用户画像的个性化回复。随着深度学习在图像识别、语音识别领域的大放异彩，人们对深度学习在NLP的价值也寄予厚望。再加上AlphaGo的成功，人工智能的研究和应用变得炙手可热。自然语言处理作为人工智能领域的认知智能，成为目前大家关注的焦点。很多研究生都在进入自然语言领域，寄望未来在人工智能方向大展身手。\n但是，大家常常遇到一些问题。俗话说，万事开头难。如果第一件事情成功了，学生就能建立信心，找到窍门，今后越做越好。否则，也可能就灰心丧气，甚至离开这个领域。这里针对给出我个人的建议，希望我的这些粗浅观点能够引起大家更深层次的讨论。\n建议1：如何在NLP领域快速学会第一个技能？我的建议是：找到一个开源项目，比如机器翻译或者深度学习的项目。理解开源项目的任务，编译通过该项目发布的示范程序，得到与项目示范程序一致的结果。然后再深入理解开源项目示范程序的算法。自己编程实现一下这个示范程序的算法。再按照项目提供的标准测试集测试自己实现的程序。如果输出的结果与项目中出现的结果不一致，就要仔细查验自己的程序，反复修改，直到结果与示范程序基本一致。如果还是不行，就大胆给项目的作者写信请教。在此基础上，再看看自己能否进一步完善算法或者实现，取得比示范程序更好的结果。\n建议2：如何选择第一个好题目？工程型研究生，选题很多都是老师给定的。需要采取比较实用的方法，扎扎实实地动手实现。可能不需要多少理论创新，但是需要较强的实现能力和综合创新能力。而学术型研究生需要取得一流的研究成果，因此选题需要有一定的创新。我这里给出如下的几点建议。先找到自己喜欢的研究领域。你找到一本最近的ACL会议论文集, 从中找到一个你比较喜欢的领域。在选题的时候，多注意选择蓝海的领域。这是因为蓝海的领域，相对比较新，容易出成果。充分调研这个领域目前的发展状况。包括如下几个方面的调研：方法方面，是否有一套比较清晰的数学体系和机器学习体系；数据方面，有没有一个大家公认的标准训练集和测试集；研究团队，是否有著名团队和人士参加。如果以上几个方面的调研结论不是太清晰，作为初学者可能不要轻易进入。在确认进入一个领域之后，按照建议一所述，需要找到本领域的开源项目或者工具，仔细研究一遍现有的主要流派和方法，先入门。反复阅读本领域最新发表的文章，多阅读本领域牛人发表的文章。在深入了解已有工作的基础上，探讨还有没有一些地方可以推翻、改进、综合、迁移。注意做实验的时候，不要贪多，每次实验只需要验证一个想法。每次实验之后，必须要进行分析存在的错误，找出原因。对成功的实验，进一步探讨如何改进算法。注意实验数据必须是业界公认的数据。与已有的算法进行比较，体会能够得出比较一般性的结论。如果有，则去写一篇文章，否则，应该换一个新的选题。\n建议3：如何写出第一篇论文？接上一个问题，如果想法不错，且被实验所证明，就可开始写第一篇论文了。确定论文的题目。在定题目的时候，一般不要“…系统”、“…研究与实践”，要避免太长的题目，因为不好体现要点。题目要具体，有深度，突出算法。写论文摘要。要突出本文针对什么重要问题，提出了什么方法，跟已有工作相比，具有什么优势。实验结果表明，达到了什么水准，解决了什么问题。写引言。首先讲出本项工作的背景，这个问题的定义，它具有什么重要性。然后介绍对这个问题，现有的方法是什么，有什么优点。但是（注意但是）现有的方法仍然有很多缺陷或者挑战。比如（注意比如），有什么问题。本文针对这个问题，受什么方法（谁的工作）之启发，提出了什么新的方法并做了如下几个方面的研究。然后对每个方面分门别类加以叙述，最后说明实验的结论。再说本文有几条贡献，一般写三条足矣。然后说说文章的章节组织，以及本文的重点。有的时候东西太多，篇幅有限，只能介绍最重要的部分，不需要面面俱到。相关工作。对相关工作做一个梳理，按照流派划分，对主要的最多三个流派做一个简单介绍。介绍其原理，然后说明其局限性。然后可设立两个章节介绍自己的工作。第一个章节是算法描述。包括问题定义，数学符号，算法描述。文章的主要公式基本都在这里。有时候要给出简明的推导过程。如果借鉴了别人的理论和算法，要给出清晰的引文信息。在此基础上，由于一般是基于机器学习或者深度学习的方法，要介绍你的模型训练方法和解码方法。第二章就是实验环节。一般要给出实验的目的，要检验什么，实验的方法，数据从哪里来，多大规模。最好数据是用公开评测数据，便于别人重复你的工作。然后对每个实验给出所需的技术参数，并报告实验结果。同时为了与已有工作比较，需要引用已有工作的结果，必要的时候需要重现重要的工作并报告结果。用实验数据说话，说明你比人家的方法要好。要对实验结果好好分析你的工作与别人的工作的不同及各自利弊，并说明其原因。对于目前尚不太好的地方，要分析问题之所在，并将其列为未来的工作。结论。对本文的贡献再一次总结。既要从理论、方法上加以总结和提炼，也要说明在实验上的贡献和结论。所做的结论，要让读者感到信服，同时指出未来的研究方向。参考文献。给出所有重要相关工作的论文。记住，漏掉了一篇重要的参考文献（或者牛人的工作），基本上就没有被录取的希望了。写完第一稿，然后就是再改三遍。把文章交给同一个项目组的人士，请他们从算法新颖度、创新性和实验规模和结论方面，以挑剔的眼光，审核你的文章。自己针对薄弱环节，进一步改进，重点加强算法深度和工作创新性。然后请不同项目组的人士审阅。如果他们看不明白，说明文章的可读性不够。你需要修改篇章结构、进行文字润色，增加文章可读性。如投ACL等国际会议，最好再请英文专业或者母语人士提炼文字。","date":"2017年10月30日 23:58:47"}
{"_id":{"$oid":"5d36a8d06734bd8e681d5e88"},"title":"一、中文自然语言处理的完整机器处理流程 nlp笔记","author":"钻石王小二吼吼吼","content":"中文自然语言处理的完整机器处理流程\n1. 获取语料，读取原始数据：\n语言材料，文本集合。\n2. 语料预处理，数据清洗：\n1.数据清洗：整理出感兴趣的内容 2.分词：将文本全部进行分词，基于字符串匹配，统计的分词方法，规则的分词方法 3.词性标注：形容词，动词，名词等 4.去停用词：标点符号，人称，语气词等，由具体场景定\n3. 特征工程\n1.词袋模型：不考虑出现的顺序，直接放一个集合，统计出现的次数，频率 2.词向量 ：将字、词语转换成向量矩阵的计算模型\n4. 特征选择\n特征选择方法：DF、 MI、 IG、 CHI、WLLR、WFO 六种\n5. 模型训练\n1.注意过拟合、欠拟合问题，不断提高模型的泛化能力。 常见的解决方法有： 增大数据的训练量； 增加正则化项，如 L1 正则和 L2 正则； 特征选取不合理，人工筛选特征和使用特征选择算法； 采用 Dropout 方法等。 欠拟合：就是模型不能够很好地拟合数据，表现在模型过于简单。 常见的解决方法有： 添加其他特征项； 增加模型复杂度，比如神经网络加更多的层、线性模型通过添加多项式使模型泛化能力更强； 减少正则化参数，正则化的目的是用来防止过拟合的，但是现在模型出现了欠拟合，则需要减少正则化参数。 2.对于神经网络，注意梯度消失和梯度爆炸问题。\n6. 评价指标\n7. 模型上线应用\n8. 模型重构\n参考文献\n中文自然语言处理入门","date":"2019年04月10日 10:04:37"}
{"_id":{"$oid":"5d36a8d16734bd8e681d5e8a"},"title":"深度学习与自然语言处理 主要概念一览","author":"Alice熹爱学习","content":"CS224d－Day 1:\n要开始系统地学习 NLP 课程 cs224d，今天先来一个课程概览。\n课程一共有16节，先对每一节中提到的模型，算法，工具有个总体的认识，知道都有什么，以及它们可以做些什么事情。\n简介：\n1. Intro to NLP and Deep Learning\nNLP：\nNatural Language Processing (自然语言处理)的目的，就是让计算机能‘懂得’人类对它‘说’的话，然后去执行一些指定的任务。\n这些任务有什么呢？\nEasy:\n• Spell Checking－－拼写检查\n• Keyword Search－－关键词提取\u0026搜索\n• Finding Synonyms－－同义词查找\u0026替换\nMedium:\n• Parsing information from websites, documents, etc.－－从网页中提取有用的信息例如产品价格，日期，地址，人名或公司名等\nHard:\n• Machine Translation (e.g. Translate Chinese text to English)－－自动的或辅助的翻译技术\n• Semantic Analysis (What is the meaning of query statement?)－－市场营销或者金融交易领域的情感分析\n• Coreference (e.g. What does “he” or “it” refer to given a document?)\n• Question Answering (e.g. Answering Jeopardy questions).－－复杂的问答系统\nNLP的难点：\n情境多样\n语言歧义\nDeep Learning：\n深度学习是机器学习的一个分支，尝试自动的学习合适的特征及其表征，尝试学习多层次的表征以及输出。\n它在NLP的一些应用领域上有显著的效果，例如机器翻译，情感分析，问答系统等。\n和传统方法相比，深度学习的重要特点，就是用向量表示各种级别的元素，传统方法会用很精细的方法去标注，深度学习的话会用向量表示 单词，短语，逻辑表达式和句子，然后搭建多层神经网络去自主学习。\n这里有简明扼要的对比总结。\n向量表示：\n词向量：\n- One－hot 向量：\n记词典里有 |V| 个词，每个词都被表示成一个 |V| 维的向量，设这个词在字典中相应的顺序为 i，则向量中 i 的位置上为 1，其余位置为 0.\n词－文档矩阵：\n构建一个矩阵 X，每个元素 Xij 代表 单词 i 在文档 j 中出现的次数。\n词－词共现矩阵：\n构建矩阵 X，每个元素 Xij 代表 单词 i 和单词 j 在同一个窗口中出现的次数。\n模型算法：\n2. Simple Word Vector representations: word2vec, GloVe\nword2vec：\nword2vec是一套能将词向量化的工具，Google在13年将其开源，代码可以见 https://github.com/burness/word2vec ，它将文本内容处理成为指定维度大小的实数型向量表示，并且其空间上的相似度可以用来表示文本语义的相似度。\nWord2vec的原理主要涉及到统计语言模型（包括N-gram模型和神经网络语言模型），continuousbag-of-words 模型以及 continuous skip-gram 模型。\nN-gram的意思就是每个词出现只看其前面的n个词，可以对每个词出现的概率进行近似。\n比如当n=2的时候:\n神经网络语言模型（NNLM）用特征向量来表征每个词各个方面的特征。NNLM的基础是一个联合概率:\n其神经网络的目的是要学习：\nContinuous Bag-of-Words(CBOW) 模型与NNLM类似，结构如下:\nCBOW是通过上下文来预测中间的词，如果窗口大小为k，则模型预测:\n其神经网络就是用正负样本不断训练，求解输出值与真实值误差，然后用梯度下降的方法求解各边权重参数值的。\nContinuous skip-gram 模型与CBOW正好相反，是通过中间词来预测前后词，一般可以认为位置距离接近的词之间的联系要比位置距离较远的词的联系紧密。目标为最大化：\n结构为：\n应用：\n- 同义词查找，\n- 文本聚类，实现方法：用关键词来表征文本。关键词提取用TF-IDF，然后用word2vec训练得到关键词向量，再用k-means聚类，最后文本就能够以关键词的类别进行分类了。\n- 文本类别投递，实现方法：人工标记出该词属于各个类别的概率，出全体词属于各个类别的概率。\nGlove：\nGlobal Vectors 的目的就是想要综合前面讲到的 word-document 和 word-windows 两种表示方法，做到对word的表示即 sementic 的表达效果好，syntactic 的表达效果也好：\n3. Advanced word vector representations: language models, softmax, single layer networks\nsoftmax：\nsoftmax 模型是 logistic 模型在多分类问题上的推广， logistic 回归是针对二分类问题的，类标记为{0， 1}。在softmax模型中，label可以为k个不同的值。\n4. Neural Networks and backpropagation – for named entity recognition\n5. Project Advice, Neural Networks and Back-Prop (in full gory detail)\nNeural Networks：\n神经网络是受生物学启发的分类器，可以学习更复杂的函数和非线性决策边界。\n模型调优：\n6. Practical tips: gradient checks, overfitting, regularization, activation functions, details\n**UFLDL：**Unsupervised Feature Learning and Deep Learning\nGradient Checking（梯度检测）：\n反向传播因为细节太多，往往会导致一些小的错误，尤其是和梯度下降法或者其他优化算法一起运行时，看似每次 J(Θ) 的值在一次一次迭代中减小，但神经网络的误差可能会大过实际正确计算的结果。\n针对这种小的错误，有一种梯度检验（Gradient checking）的方法，通过数值梯度检验，你能肯定确实是在正确地计算代价函数（Cost Function）的导数。\nGC需要对params中的每一个参数进行check，也就是依次给每一个参数一个极小量。\noverfitting：\n就是训练误差Ein很小，但是实际的真实误差就可能很大，也就是模型的泛化能力很差(bad generalization)\n发生overfitting 的主要原因是：（1）使用过于复杂的模型(dvc 很大)；（2）数据噪音；（3）有限的训练数据。\nregularization：\n为了提高模型的泛化能力，最常见方法便是：正则化，即在对模型的目标函数（objective function）或代价函数（cost function）加上正则项。\n平台：\n7. Introduction to Tensorflow\nTensorflow：\nTensorflow 是 python 封装的深度学习库，非常容易上手，对分布式系统支持比 Theano 好，同时还是 Google 提供资金研发的\n在Tensorflow里：\n使用张量(tensor)表示数据.\n使用图(graph)来表示计算任务.\n在被称之为会话(Session)的上下文 (context)中执行图.\n通过变量 (Variable)维护状态.\n使用feed和fetch可以为任意的操作(arbitrary operation)赋值或者从其中获取数据.\nTensorFlow 算是一个编程系统，它使用图来表示计算任务，图中的节点被称之为operation(可以缩写成op)，一个节点获得0个或者多个张量(tensor，下文会介绍到)，执行计算，产生0个或多个张量。\n模型与应用：\n8. Recurrent neural networks – for language modeling and other tasks\nRNN：\n在深度学习领域，传统的前馈神经网络（feed-forward neural net，简称FNN）具有出色的表现。\n在前馈网络中，各神经元从输入层开始，接收前一级输入，并输入到下一级，直至输出层。整个网络中无反馈，可用一个有向无环图表示。\n不同于传统的FNNs，RNNs引入了定向循环，能够处理那些输入之间前后关联的问题。定向循环结构如下图所示：\n9. GRUs and LSTMs – for machine translation\n传统的RNN在训练 long-term dependencies 的时候会遇到很多困难，最常见的便是 vanish gradient problem。期间有很多种解决这个问题的方法被发表，大致可以分为两类：一类是以新的方法改善或者代替传统的SGD方法，如Bengio提出的 clip gradient；另一种则是设计更加精密的recurrent unit，如LSTM，GRU。\nLSTMs：\n长短期内存网络(Long Short Term Memory networks)是一种特殊的RNN类型，可以学习长期依赖关系。\nLSTMs 刻意的设计去避免长期依赖问题。记住长期的信息在实践中RNN几乎默认的行为，但是却需要很大的代价去学习这种能力。\nLSTM同样也是链式结构，但是重复的模型拥有不同的结构，它与单个的神经网层不同，它有四个， 使用非常特别方式进行交互。\nGRUs：\nGated Recurrent Unit 也是一般的RNNs的改良版本，主要是从以下两个方面进行改进。\n一是，序列中不同的位置处的单词(已单词举例)对当前的隐藏层的状态的影响不同，越前面的影响越小，即每个前面状态对当前的影响进行了距离加权，距离越远，权值越小。\n二是，在产生误差error时，误差可能是由某一个或者几个单词而引发的，所以应当仅仅对对应的单词weight进行更新。\n10. Recursive neural networks – for parsing\n11. Recursive neural networks – for different tasks (e.g. sentiment analysis)\nRecursive neural networks：\n和前面提到的 Recurrent Neural Network 相比：\nrecurrent: 时间维度的展开，代表信息在时间维度从前往后的的传递和积累，可以类比markov假设，后面的信息的概率建立在前面信息的基础上。\nrecursive: 空间维度的展开，是一个树结构，就是假设句子是一个树状结构，由几个部分(主语，谓语，宾语）组成，而每个部分又可以在分成几个小部分，即某一部分的信息由它的子树的信息组合而来，整句话的信息由组成这句话的几个部分组合而来。\n12. Convolutional neural networks – for sentence classification\nConvolutional neural networks：\n卷积神经网络是一种特殊的深层的神经网络模型，它的特殊性体现在两个方面，一方面它的神经元间的连接是非全连接的， 另一方面同一层中某些神经元之间的连接的权重是共享的（即相同的）。它的非全连接和权值共享的网络结构使之更类似于生物 神经网络，降低了网络模型的复杂度，减少了权值的数量。\n13. Guest Lecture with Andrew Maas: Speech recognition\n14. Guest Lecture with Thang Luong: Machine Translation\n大数据：\n15. Guest Lecture with Quoc Le: Seq2Seq and Large Scale DL\nSeq2Seq：\nseq2seq 是一个机器翻译模型，解决问题的主要思路是通过深度神经网络模型（常用的是LSTM，长短记忆网络，一种循环神经网络）将一个作为输入的序列映射为一个作为输出的序列，这一过程由编码输入与解码输出两个环节组成。\nEncoder:\nDecoder:\n注意机制是Seq2Seq中的重要组成部分:\n应用领域有：机器翻译，智能对话与问答，自动编码与分类器训练等。\nLarge Scale DL：\n为了让 Neural Networks 有更好的效果，需要更多的数据，更大的模型，更多的计算。\nJeff Dean On Large-Scale Deep Learning At Google\n未来方向：\n16. The future of Deep Learning for NLP: Dynamic Memory Networks\ndynamic memory network (DMN)：\n利用 dynamic memory network（DMN）框架可以进行 QA（甚至是 Understanding Natural Language）。\n这个框架是由几个模块组成，可以进行 end-to-end 的 training。其中核心的 module 就是Episodic Memory module，可以进行 iterative 的 semantic + reasoning processing。\n有了一个总体的了解，看的热血沸腾的，下一次开始各个击破！\n[cs224d]\nDay 1. 深度学习与自然语言处理 主要概念一览\nDay 2. TensorFlow 入门\nDay 3. word2vec 模型思想和代码实现\nDay 4. 怎样做情感分析\nDay 5. CS224d－Day 5: RNN快速入门\nDay 6. 一文学会用 Tensorflow 搭建神经网络\nDay 7. 用深度神经网络处理NER命名实体识别问题\nDay 8. 用 RNN 训练语言模型生成文本\nDay 9. RNN与机器翻译\nDay 10. 用 Recursive Neural Networks 得到分析树\nDay 11. RNN的高级应用\n推荐阅读\n历史技术博文链接汇总\n也许可以找到你想要的","date":"2017年05月14日 01:13:34"}
{"_id":{"$oid":"5d36a8d16734bd8e681d5e8c"},"title":"自然语言处理之语言模型综述","author":"海涛anywn","content":"一 文法型语言模型\n文法型语言模型是人工编制的语言学文法，文法规则来源于语言学家掌握的语言学知识和领域知识，但这种语言模型不能处理大规模真实文本。\n二 统计语言模型\n统计语言模型常用的思想是用一个词在句子中的neighborhood表示该词\n主要的统计语言模型有：\n1.上下文无关模型\n2.N-gram模型：考虑词形方面的特征\n（1）一元模型\n（2）二元模型\n（3）N元模型\n3.N-pos模型：考虑词类词性方面的特征，前一个词的词类决定下一个词出现的概率。\n4.基于决策树的语言模型\n5.最大熵模型\n6.动态、自适应、基于缓存的语言模型\n7.Hyperspace Analogue to Language method (HAL)\nHAL (Lund \u0026 Burgess, 1996）方法可以用一个co-occurrence matrix, 表示任意两个词相关性\n8.Latent Semantic Analysis (LSA)\nLSA (Deerwester et al., 1990; Landauer, Foltz, \u0026 Laham, 1998) 中， co-occurrence matrix是word-document矩阵，表示文档中出现某词的频率，统计后将其进行normalization\n将document从稀疏的高维Vocabulary空间映射到一个低维的向量空间，我们称之为隐含语义空间(Latent Semantic Space)。\n9.COALS （Rohde et al., 2009）\n在HAL上做了小改动， 将HAL所得co-occurrence matrix进行correlation normalization。\n三 严格匹配模型、概率模型\n严格匹配模型是给定一个查询,利用匹配函数,将文档集分为两个集合: 匹配集和非匹配集. 严格匹配模型中最简单并且常用的一种便是布尔模型.在布尔模型中要定义一个二值变量的集合,这些变量都对应文档的某个特征,称为特征变量.文档由这些特征变量组成的集合来表示,如果变量对文档的内容表示有贡献,则赋值为True,否则为False.查询语句则是由特征变量和操作符and, or和not组成的表达式. 匹配函数则遵循布尔逻辑的规则.\n概率模型是信息检索的又一主要模型,这种模型主要针对信息检索中相关性判断的不确定性以及查询信息表示的模糊性.基于概率排序原则: 对于给定的用户查询Q,对所有的文本D计算概率P(R|D,Q)并从大到小进行排序. 这里R 表示文本D与查询Q的相关性.如果以D=(d1,d2,…,dn)表示文本D,N为特征项个数,特征项i在文本中出现di=1,否则di=0.\n概率模型的缺点是对文本集的依赖性过强,而且处理问题过于简单.\n四 基于分布理论的独立检验模型\n基于分布理论的独立检验模型有关键要素，分别是互信息、t测试、相异度（t测试差）、相关度（i平方的统计量）\n五 基于规则的模型\n这种模型假设自然语言的知识可以用规则集来表示,而规则集的获取既可以人工编写(唯理主义) ,也可以有语料库中学习得到(经验主义) . 1956年乔姆斯基发表了《语言描述的三个模型》,由此兴起的短语结构语法、乔姆斯基语法体系和其他的一些语言描述模型,都可以看作是描述语言的规则模型,基于这些规则模型的语言处理技术就是句法分析技术和语义分析技术.\n六 语言模型变种\nClass-based N-gram Model\n该方法基于词类建立语言模型，以缓解数据稀疏问题，且可以方便融合部分语法信息。\nTopic-based N-gram Model\n该方法将训练集按主题划分成多个子集，并对每个子集分别建立N-gram语言模型，以解决语言模型的主题自适应问题。\nCache-based N-gram Model\n该方法利用cache缓存前一时刻的信息，以用于计算当前时刻概率，以解决语言模型动态自适应问题。\n应用：各种输入法（搜狗、QQ、微软等）\nSkipping N-gram Model\u0026Trigger-based N-gram Model\n二者核心思想都是刻画远距离约束关系。\n指数语言模型\n最大熵模型MaxEnt、最大熵马尔科夫模型MEMM、条件随机域模型CRF\n七 主题模型及其发展\n主题模型有两种：pLSA（ProbabilisticLatent Semantic Analysis）和LDA（Latent Dirichlet Allocation）\n主题模型的起源是隐性语义索引（LSI）\n隐性语义索引后来又发展为概率隐性语义索引（pLSI）\n主题的实现一般包括五部分的内容：输入、基本假设、表示、参数估计、新样本推断\n输入：主要是文档集合\n基本假设：是词袋（bag of words）假设，即一篇文档内的单词可以交换次序而不影响模型的训练结果。\n主题模型的表示：图模型和生成过程\nTopic Model主要可以分为四大类：\n1）无监督的、无层次结构的topic model；2）无监督的、层次结构的topic model；\n3）有监督的、无层次结构的topic model；4）有监督的、层次结构的topic model。\n对于1）主要有： PLSA, LDA, Correlated Topic Model, PAM，Concept Topic Model等\n对于2）主要有： HLDA， HDP，HPAM等\n对于3）主要有： S-LDA, Disc-LDA, MM-LDA, Author-Model, Labeled LDA, PLDA 等等\n对于4）主要有： hLLDA, HSLDA等","date":"2016年04月15日 10:35:13"}
{"_id":{"$oid":"5d36a8d16734bd8e681d5e8e"},"title":"nodejs在自然语言处理中的一些小应用","author":"攻城狮丶麦晓杰","content":"nodejs做自然语言处理是非常可行的，这次我做了一些小小的尝试，一起来体验一下吧。\n因为还保持着对自然语言处理的那份热爱，最近没事的时候会把毕业论文翻出来看（毕业论文的课题就是关于自然语言处理的），然后在我的新博客中加入了一些相关的处理，主要做了以下几个方面：\n对每一篇文章进行快速的内容理解，根据标题和内容，输出多个内容标签；\n对文章按照内容进行自动分类，为文章聚类、文本内容分析等提供基础；\n根据文章标题、用户自定义标签、以及人工智能获得到的标签，进行相似度计算；\n在阅读一篇文章的时候，通过相似度计算的结果，推荐相关的文章給用户。\n下面给出自动输出内容标签的结果图：\n博客系统\n运行环境：centos9 + docker\n开发语言：nodejs\n数据库：MariaDB\n开发框架：eggjs + nunjucks（模板引擎）\n这次也是我第一次做后端渲染的博客，ajax的网站，做seo是真的不好做…\n然后这次也是我第一次正儿八经的用了下阿里大佬们的eggjs，这种“洋葱模型”的框架，我真的是超级喜欢，不管是用es7优雅地处理js异步，还是经典的MVC，还是框架的插件机制等等。确实是超级赞的。如果有喜欢nodejs的同志，强力推荐此框架。\n推荐系统\n推荐系统是我们平时在用软件，或者网站中经常会遇见的，比如资讯类的，百度feed、头条、qq看点等；电商类的，阿里，京东等等，还有抖音什么的，很多很多。\n一个好的推荐系统可以带来更多的收益，but一个不好的推荐系统往往会得到别人的吐槽。之前在脉脉看到某公司CTO收到脉脉推荐的安卓工程师的推荐职位，遭到吐槽。百度李彦宏某天因为没有在feed收到一条重要的科技资讯信息，而吐槽自家员工。这样的事情通常会很多。\n我觉得一个好的推荐系统应该更“懂”人，假如我最近一个月前买了一部手机，我希望能给我推送一些手机配件，而不是在给我推送一部手机，这个时候我买手机配件的概率是远远大于在买一部手机的。现在很多推荐系统，都是通过用户画像，加上各种埋点，用户操作数据，从而进行分析推送的。我觉得未必不可以在此基础上加上情感分析，多一个维度，或许能够得到更准确的数据。\n说了这么多，我觉得还是有很多瓶颈存在的😐，现在的AI就像很多年前的移动互联网，正处于上升期。 我们还有很多事情可以做。\n下面进入今天的真题…\n这次做的文章推荐系统，分享一些细节给大家：\n图中右侧部分就是我们这个文章推送系统的推送结果，我们用不同的颜色标注了这篇文章和当前正在浏览的文章的关联度，颜色越深表示关联度越高，置信度越高，权重越大。\n这个推荐系统中主要使用了上面所说的第三点：相似度计算；使用的数学模型为空间向量模型，空间向量模型能够将非结构化的文本数据转换成向量形式，表示成向量形式之后能为之后的处理过程打下良好的数学基础。\n空间向量模型，帮助我们把每篇文档转化为一个多维的空间向量形式：\n![])(https://wx4.sinaimg.cn/large/8f29f10bgy1fwi8fl6laxj20du01pt8l.jpg)\n其中，向量 W1i表示第一个词占文档 Ci的比重，向量 W2i表示第二个词占文档 Ci的比重，依次类推，向量 Wti表示第 t 个词占文档 Ci的比重。\n那么两篇文章的相似度，我们就可以计算他们对应向量的夹角余弦值来进行计算：\n两个文档的余弦值越接近 1，这两个文档则越相似。\n下面给出计算相似度的关键代码：","date":"2018年11月14日 17:53:00"}
{"_id":{"$oid":"5d36a8d26734bd8e681d5e90"},"title":"《自然语言处理实战入门》---- 第1课：自然语言处理简介","author":"shiter","content":"本博客为《自然语言处理实战课程》---- 第一课：自然语言处理简介 讲稿\n文章大纲\n个人简介\n本节课程导览\n1.自然语言处理（NLP）简介\n1.1 基础技术\n1.2 Nlp 核心技术\n1.3 NlP+（高端技术）\n1.4 课程涵盖的主要内容总揽\n2.知名NLP服务系统与开源组件简介\n2.1 单一服务提供商\n2.1.1 汉语分词系统ICTCLAS\n2.1.2 哈工大语言云（Language Technology Platform，LTP）\n2.1.3 HanLP\n2.1.4 BosonNLP\n2.2 云服务提供商\n2.2.1 Amazon Comprehend\n2.2.2 阿里云NLP\n2.2.3 腾讯云NLP\n2.2.4 百度语言处理基础技术\n2.3 NLP开源组件简介\n2.3.1 NLTK\n2.3.2 Jieba分词\n2.3.3 ICTCLAS\n2.3.4 Gensim\n参考文献\n大家好，今天开始和大家分享，我在自然语言处理（Natural Language Processing，NLP）的一些学习经验和心得体会。\n随着人工智能的快速发展，自然语言处理和机器学习技术的应用愈加广泛。为使大家对该领域整体概况有一个系统、明晰的认识，同时入门一些工程实践，也借CSDN为NLP的学习，开发者们搭建一个交流的平台。\n个人简介\n王雅宁， 2016年毕业于陕西师范大学计算机软件与理论专业。\nCSDN博客专家，主要专注于大数据，计算机视觉，自然语言处理\n对大数据机器学习类软件开发技术都有比较浓厚的兴趣，熟悉数据分析，机器学习，计算机视觉等领域的研发工作。熟悉windows，Linux下的c/c++开发，OpenCV图形图像库的各类接口。熟悉大数据生态圈下的Python开发。\n曾参与并负责国家级安全项目相关POC验证与探索工作，在客户业务场景下验证产品的功能与性能。\n主要工作内容有：\n1、在客户现场搭建大数据产品平台，与客户沟通，根据客户的需求或业务场景在大数据平台上实现大数据平台软件的项目实施与安装部署。\n2、现场提供专业服务，包括系统、大数据集群故障分析与诊断，数据分析服务，业务应用对接迁移，完善提供整体解决方案。\n3、实现在单机与分布式环境下发掘等短文本的兴趣倾向和命名实体识别。该项目对结构化数据进行分词，停用词处理，命名实体识别，图计算等操作。\n目前在西安知盛数据科技有限公司主要负责大健康平台中医疗健康保险的部分内容构建与实施，主要负责包括数据理解，数据接入与清洗，描述性统计分析，大数据可视化等方面的工作与探索。对自然语言处理，保险数据异常检测方面有独到的探索经验。\n\n本节课程导览\n本小结主要介绍内容如下\n自然语言处理简介\n3W，发展历程、研究现状、\n课程涵盖的主要内容总览\n第一阶段\n第二阶段\n知名NLP服务系统与开源组件简介\n对汉语自然处理的服务提供商及其服务内容做一个简单的梳理，让大家能够更好的了解目前的技术手段，技术现状。\n本小节课程主要内容分为2大部分：\n第一部分，自然语言处理简介，用认知思维的方法，结合发展历程总揽自然语言处理.\n同时顺带介绍，本课程的主要内容，本课程的主要内容我们分成两个阶段 。第一个阶段如思维导图的右边，我们力求短时间内上手，完成爬虫、分词、可视化、文本分类4个自然语言处理实战中最经常碰到的问题，我首先通过爬虫爬取自己CSDN的博客积累语料，其次尝试通过一些解决方案的对比，比如不同的分词组件的对比，选择一个进行可视化词云，主题模型的生成。最后我们介绍一些文本分类的方法，文本分类的应用较广，如垃圾邮件检测，舆论分析，文本查重等场景都可以转化为文本分类问题。第二个阶段的课程，如果有时间的话，我们来共同探讨一些业界常用的NLP实战场景，如脑图左侧所示的，命名实体识别，问答机器人，知识图谱，基于深度学习的NLP 等\n第二部分介绍 ，NLP技术在我国的应用现状，以及一些我们经常用到的开源包。\n1.自然语言处理（NLP）简介\n上学的时候，老师经常使用这样提问的方式加深我们对于知识的理解和认知\nwhat is it？\n自然语言处理（Natural Language Processing，简称 NLP）是人工智能和语言学交叉领域下的分支学科。\n用于分析、理解和生成自然语言，以方便人和计算机设备进行交流，以及人与人之间的交流\nNLP 是人工智能和语言学领域的交叉学科，\n自然语言处理在广义上分为两大部分：\n第一部分为自然语言理解，是指让计算机懂人类的语言。\n第二部分为自然语言生成，是指把计算机数据转化为自然语言。\nNLP 技术按照由浅入深可以分为三个层次，分别为：\n基础技术\n核心技术\nNLP+\n1.1 基础技术\n这三个层次中，基础技术主要是对自然语言中的基本元素进行表示和分析，比如词汇，短语，句子。\n词汇短语分析中，大家熟知的分词技术，就是为了解决如下问题，比如：我去北京大学玩，北京大学独立成词，而不是分成北京和大学。\n句法语义分析：对于给定的句子，进行分词、词性标记、命名实体识别和链接、句法分析、语义角色识别和多义词消歧。\n1.2 Nlp 核心技术\nNLP 的核心技术是建立在基础技术之上的的技术产出，基础技术中如词法，句法的分析越准确，核心技术的产出才能越准确。核心技术主要包括以下几个方面：\n信息抽取\n从给定文本中抽取重要的信息，比如，时间、地点、人物、事件、原因、结果、数字、日期、货币、专有名词等等。通俗说来，就是要了解谁在什么时候、什么原因、对谁、做了什么事、有什 么结果。涉及到实体识别、时间抽取、因果关系抽取等关键技术。\n文本挖掘（或者文本数据挖掘）\n包括文本聚类、分类、信息抽取、摘要、情感分析以及对挖掘的信息和知识的可视化、交互式的表达界面。目前主流的技术都是基于统计机器学习的。\n机器翻译\n把输入的源语言文本通过自动翻译获得另外一种语言的文本。根据输入媒介不同，可以细分为文本翻译、语音翻译、手语翻译、图形翻译等。机器翻译从最早的基于规则的方法到二十年前的基于统计的方法，再到今天的基于神经网络（编码-解码）的方法，逐渐形成了一套比较严谨的方法体系。\n信息检索\n对大规模的文档进行索引。可简单对文档中的词汇，赋之以不同的权重来建立索引，也可利用（句法分析，信息抽取，文本发掘）来建立更加深层的索引。在查询的时候，对输入的查询表达式比如一个检索词或者一个句子进行分析，然后在索引里面查找匹配的候选文档，再根据一个排序机制把候选文档排序，最后输出排序得分最高的文档。\n1.3 NlP+（高端技术）\n能够真正影响我们生活的黑科技，能够通过图灵测试的机器问答系统，我们可以称之为NLP+\n问答系统\n对一个自然语言表达的问题，由问答系统给出一个精准的答案。需要对自然语言查询语句进行某种程度的语义分析，包括实体链接、关系识别，形成逻辑表达式，然后到知识库中查找可能的候选答案并通过一个排序机制找出最佳的答案。\n对话系统\n系统通过一系列的对话，跟用户进行聊天、回答、完成某一项任务。涉及到用户意图理解、通用聊天引擎、问答引擎、对话管理等技术。此外，为了体现上下文相关，要具备多轮对话能力。\nAI助手\n目前自然语言处理的前沿，已经与人类真假难辨\nhttps://v.qq.com/x/page/w0648xqraxj.html\n参考：\nhttps://www.zhihu.com/question/19895141/answer/149475410\n1.4 课程涵盖的主要内容总揽\n2.知名NLP服务系统与开源组件简介\n以下我们通过一些知名中文NLP服务提供商，包括我们熟知的云服务提供商BAT ,aws，以及两家科研院所的系统简介，来介绍以及宏观认识NLP的各种技术手段和应用场景。\n首先介绍的是两家NLP基础分析，准确率很高的科研院所 的产品，源自北理工和哈工大，之后我们介绍知名云服务提供商的产品。\n2.1 单一服务提供商\n2.1.1 汉语分词系统ICTCLAS\n主页：http://ictclas.nlpir.org/\n在线演示系统：http://ictclas.nlpir.org/\nPython版本：https://github.com/tsroten/pynlpir\n（需要频繁更新key）\nhttps://blog.csdn.net/sinat_26917383/article/details/77067515\n\n对于习总书记这篇新闻稿 的实体抽取结果\nhttp://news.163.com/18/0715/14/DMOTHJEK000189FH.html\n该系统为汉语自然语言处理领域顶尖大牛，北京理工大学张华平博士20年的专业技术积累，NShort 革命性分词算法的发明者。\n主要功能包括中文分词；英文分词；中英文混合分词，词性标注；命名实体识别；新词识别；关键词提取；支持用户专业词典与微博分析。NLPIR系统支持多种编码、多种操作系统、多种开发语言与平台。\n该平台的特点为：功能丰富，分词，语义，实体发现准确率高，近期发布了最新的2018版。\n（与熟知的jieba，ltp，清华thulac）\n2.1.2 哈工大语言云（Language Technology Platform，LTP）\nhttps://www.ltp-cloud.com/\n源自哈工大知名的分词插件ltp，准确率高\nPython版本：https://github.com/HIT-SCIR/pyltp\n语言技术平台（Language Technology Platform，LTP）是哈工大社会计算与信息检索研究中心历时十年开发的一整套中文语言处理系统。LTP制定了基于XML的语言处理结果表示，并在此基础上提供了一整套自底向上的丰富而且高效的中文语言处理模块（包括词法、句法、语义等6项中文处理核心技术），以及基于动态链接库（Dynamic Link Library, DLL）的应用程序接口、可视化工具，并且能够以网络服务（Web Service）的形式进行使用。\n“语言云”\n以哈工大社会计算与信息检索研究中心研发的 “语言技术平台（LTP）” 为基础，为用户提供高效精准的中文自然语言处理云服务。 使用 “语言云” 非常简单，只需要根据 API 参数构造 HTTP 请求即可在线获得分析结果，而无需下载 SDK 、无需购买高性能的机器，同时支持跨平台、跨语言编程等。 2014年11月，哈工大联合科大讯飞公司共同推出 “哈工大-讯飞语言云”，借鉴了讯飞在全国性大规模云计算服务方面的丰富经验，显著提升 “语言云” 对外服务的稳定性和吞吐量，为广大用户提供电信级稳定性和支持全国范围网络接入的语言云服务，有效支持包括中小企业在内开发者的商业应用需要。\n有关更多语言云API的使用方法，请参考：http://www.ltp-cloud.com/document/\nwindows 下安装pyltp的话，应该是需要安装visual studio, 由于LTP是用c++写的，pyltp也是基于它封装而成的，需要调用 cl.exe 完成源码的编译。然后下载源码，使用python setup.py install 的方式进行安装就可以了。\n2.1.3 HanLP\nHanLP是一系列模型与算法组成的NLP工具包，由大快搜索主导并完全开源，目标是普及自然语言处理在生产环境中的应用。HanLP具备功能完善、性能高效、架构清晰、语料时新、可自定义的特点。\nHanLP提供下列16大类功能：\n中文分词\n词性标注\n命名实体识别\n关键词提取\n自动摘要\n短语提取\n拼音转换\n简繁转换\n文本推荐\n依存句法分析\n文本分类\n情感分析\n文本聚类\nword2vec\n文档语义相似度计算\n语料库工具\n项目地址：https://github.com/hankcs/HanLP\npython 版本：https://github.com/hankcs/pyhanlp\nwindows 安装指南：https://github.com/hankcs/pyhanlp/wiki/Windows\n由于HanLP底层是java 版本的，所以对java 的支持比较好，python 版本中有一些功能没有实现，但可以通过调用java 实现。HanLP随v1.6.8发布了在一亿字的大型综合语料库上训练的分词模型，该语料是已知范围内全世界最大的中文分词语料库。在HanLP的在线演示中使用已久，现在无偿公开。语料规模决定实际效果\n，所以不用多说HanLP确实可以直接拿来做项目。有趣的是HanLP 有着非常多的衍生项目，其中docker 版和ES 版值得大家关注，这些衍生项目无疑更加提高了HanLP的可用性、灵活性。\n调用代码样例\nfrom pyhanlp import * print(HanLP.segment('你好，欢迎在Python中调用HanLP的API')) for term in HanLP.segment('下雨天地面积水'): print('{}\\t{}'.format(term.word, term.nature)) # 获取单词与词性 testCases = [ \"商品和服务\", \"结婚的和尚未结婚的确实在干扰分词啊\", \"买水果然后来世博园最后去世博会\", \"中国的首都是北京\", \"欢迎新老师生前来就餐\", \"工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作\", \"随着页游兴起到现在的页游繁盛，依赖于存档进行逻辑判断的设计减少了，但这块也不能完全忽略掉。\"] for sentence in testCases: print(HanLP.segment(sentence)) # 关键词提取 document = \"水利部水资源司司长陈明忠9月29日在国务院新闻办举行的新闻发布会上透露，\" \\ \"根据刚刚完成了水资源管理制度的考核，有部分省接近了红线的指标，\" \\ \"有部分省超过红线的指标。对一些超过红线的地方，陈明忠表示，对一些取用水项目进行区域的限批，\" \\ \"严格地进行水资源论证和取水许可的批准。\" print(HanLP.extractKeyword(document, 2)) # 自动摘要 print(HanLP.extractSummary(document, 3)) # 依存句法分析 print(HanLP.parseDependency(\"徐先生还具体帮助他确定了把画雄鹰、松鼠和麻雀作为主攻目标。\"))\n2.1.4 BosonNLP\nBosonNLP(界面，接口友好，准确率高)\nhttps://bosonnlp.com/demo\n\n如果你在网上搜索汉语分词评测，十有八九你会搜索到专注于汉语自然语言处理技术的这家公司，以及下面这张评测结果：\n\n2.2 云服务提供商\n2.2.1 Amazon Comprehend\nhttps://amazonaws-china.com/cn/comprehend/?nc2=h_a1\nAmazon Comprehend 是一项自然语言处理 (NLP) 服务，可利用机器学习发现文本中的见解和关系。Amazon Comprehend 可以识别文本语言，提取关键的短语、地点、人物、品牌或事件，了解文本的含义是肯定还是否定，还可以自动按主题整理一系列文本文件。\n您可使用 Amazon Comprehend API 分析文本，并将结果进行广泛应用，包括客户意见分析、智能文档搜索以及 Web 应用程序的内容个性化设置。\n该服务不断地通过各种信息来源 (包括世界上最大的自然语言数据集之一：Amazon.com 商品描述和买家评论) 学习和提升， 以跟上语言的发展演变。\n实例：利用 AWS Comprehend 打造近实时文本情感分析\nhttps://amazonaws-china.com/cn/blogs/china/realizing-near-real-time-text-sentiment-analysis-with-aws-comprehend/\n可以看到图中，aws 使用kibana 仪表盘和 Comprehend 服务组成了一个实时的电影评论实时分析系统，其实主要功能就是实现了分词和内容来源的地理位置统计，看起来很炫酷。\n2.2.2 阿里云NLP\n\nhttps://data.aliyun.com/product/nlp?spm=5176.8142029.388261.396.63f36d3eoZ8kNK\n阿里的NLP 服务简介为：\n自然语言处理是为各类企业及开发者提供的用于文本分析及挖掘的核心工具， 已经广泛应用在电商、文化娱乐、金融、物流等行业客户的多项业务中。 自然语言处理API可帮助用户搭建内容搜索、内容推荐、舆情识别及分析、文本结构化、对话机器人等智能产品， 也能够通过合作，定制个性化的解决方案。\n按量付费的基准价，在没有购买资源包或资源包用尽的情况下，将按基准价进行计费。\n其中，基础版对每个主帐号提供每日5万次的免费使用额度。商品评价解析没有免费额度。\n值得注意的是阿里云的nlp 服务刚发布不到1年，应该算是领域内的新手，语料库应该和aws 一样，主要为商品描述和评论，所以它有一项功能叫做商品评价解析\n\n时隔半年之后我们再来看一下这个产品名录发现，功能更加丰富了。整体来看受限于语料的积累，我认为没有什么亮点。\n\n2.2.3 腾讯云NLP\n\n\nhttps://cloud.tencent.com/product/nlp\n界面友好，功能丰富，语料库为海量综合性语料库\n腾讯云智在线演示系统\nhttp://nlp.qq.com/semantic.cgi\n2.2.4 百度语言处理基础技术\nhttp://ai.baidu.com/tech/nlp\n依托海量检索数据，并且搜索引擎本身就是NLP 最终的结果产出，所以在NLP领域，百度无论是语料库丰富程度，技术先进性，以及服务多样性等都是遥遥领先其他厂家，基本上可以算作是中文NLP服务提供商的业界最佳实践。\n功能丰富且技术领先\n词法分析\n词向量表示\n词义相似度\n评论观点抽取\n文章标签\n依存句法分析\nDNN语言模型\n短文本相似度\n情感倾向分析\n文章分类\n对话情绪识别\n文本纠错\n新闻摘要\n等13个大类的服务,对于个人开发者来说，配比了免费额度，对于词向量来说，每秒免费的额度是5个词，基本可以够用拿来做点有趣的事情了。\n从图中结果也可以看出，百度对词向量相似度的分析和我用余弦相似度的结果一样，可以推断出百度的算法比较接地气。\nDNN语言模型\nDeep Neural Network（DNN）模型是基本的深度学习框架，DNN语言模型是通过计算给定词组成的句子的概率，从而判断所组成的句子是否符合客观语言表达习惯\n通常用于机器翻译、拼写纠错、语音识别、问答系统、词性标注、句法分析和信息检索等\n百度这个模型是大厂中首个公开提供服务接口的深度学习语言模型。\n调用方式友好简单\n提供更加简单的调用方式：类似aws boto3\n如果已安装pip，执行pip install baidu-aip即可\nSdk 方式，安装\nfrom aip import AipNlp \"\"\" 你的 APPID AK SK \"\"\" APP_ID = '你的 App ID' API_KEY = '你的 Api Key' SECRET_KEY = '你的 Secret Key' client = AipNlp(APP_ID, API_KEY, SECRET_KEY) word = \"张飞\" \"\"\" 调用词向量表示 \"\"\" client.wordEmbedding(word);\n2.3 NLP开源组件简介\nNLP 领域有非常多的开源组件可以用来快速构建开发的原型，我来简单介绍以下四个知名开源组件\n2.3.1 NLTK\nhttp://www.nltk.org/\n最常用的自然语言处理库\nNLTK是一个高效的Python构建的平台，用来处理人类自然语言数据。基本包含了NLP 中需要用到的所有技术。\n它提供了易于使用的接口，通过这些接口可以访问超过50个语料库和词汇资源（如WordNet），还有一套用于分类、标记化、词干标记、解析和语义推理的文本处理库，以及工业级NLP库的封装器和一个活跃的讨论论坛。\n古腾堡项目（Project Gutenberg）\nNLTK 包含古腾堡项目（Project Gutenberg）中电子文本档案的经过挑选的一小部分文本。该项目大约有57,000 本免费电子图书，放在http://www.gutenberg.org/上。我们先要用Python 解释器加载NLTK 包，然后尝试nltk.corpus.gutenberg.fileids()，当然其中的中文语料也很丰富（都是没有版权的免费文档），比如李白文集，三字经，百家姓等等（要是用这些训练中文模型效果可想而知）\n2.3.2 Jieba分词\nhttps://github.com/fxsjy/jieba\n“结巴”中文分词：做最好的 Python 中文分词组件\n“Jieba” (Chinese for “to stutter”) Chinese text segmentation: built to be the best Python Chinese word segmentation module.\n实现基本功能的代码量在一千行左右，词典长度35w ，安装方式友好，简洁，高效，（但准确性已经跟不上时代！！！85%）\n2.3.3 ICTCLAS\nhttp://ictclas.nlpir.org/\n主要功能包括中文分词；词性标注；中英混合分词；命名实体识别；用户词典功能；支持GBK编码、UTF8编码、BIG5编码。新增微博分词、新词发现与关键词提取；张华平博士先后倾力打造20余年，内核升级10次。\n全球用户突破20万，先后获得了2010年钱伟长中文信息处理科学技术奖一等奖，2003年国际SIGHAN分词大赛综合第一名，2002年国内973评测综合第一名。\n2.3.4 Gensim\nhttps://radimrehurek.com/gensim/\n它的 slogan 是：Topic modelling for humans.\nGensim提供了一个发现文档语义结构的工具，用于从原始的非结构化的文本中，无监督地学习到文本隐层的主题向量表达。它将语料（Corpus）向量化表示后，主要能够实现以下三个功能：\n建立语言模型\n词嵌入模型的训练\n检索和语义分析的神器\n简介参考：https://www.cnblogs.com/iloveai/p/gensim_tutorial.html\n参考文献\n我爱自然语言处理\nhttp://www.52nlp.cn/\n深度学习与中文短文本分析总结与梳理\nhttps://blog.csdn.net/wangyaninglm/article/details/66477222\n分析了近5万首《全唐诗》，发现了这些有趣的秘密\nhttp://www.growthhk.cn/cgo/9542.html\n万字干货｜10款数据分析“工具”，助你成为新媒体运营领域的“增长黑客”\nhttp://www.woshipm.com/data-analysis/553180.html\njieba分词简介与解析\nhttps://www.cnblogs.com/baiboy/p/jieba2.html\n有哪些好的汉语分词方案\nhttps://www.zhihu.com/question/19578687\n基于分布式的短文本命题实体识别之----人名识别（python实现）\nhttps://blog.csdn.net/wangyaninglm/article/details/75042151\nNLP技术的应用及思考\nhttps://yq.aliyun.com/articles/78031","date":"2019年04月01日 22:25:53"}
{"_id":{"$oid":"5d36a8d26734bd8e681d5e94"},"title":"自然语言处理的通俗百科","author":"光影流年925","content":"以下内容转载自百度百科，如果没时间仔细看，可只看加粗部分即可！\n自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。自然语言处理是一门融语言学、计算机科学、数学于一体的科学。因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，所以它与语言学的研究有着密切的联系，但又有重要的区别。自然语言处理并不是一般地研究自然语言，而在于研制能有效地实现自然语言通信的计算机系统，特别是其中的软件系统。因而它是计算机科学的一部分。\n自然语言处理（NLP）是计算机科学，人工智能，语言学关注计算机和人类（自然）语言之间的相互作用的领域。\n1 详细介绍\n2 发展历史\n3 相关内容\n4 相关技术\n5 概述\n▪ 基础理论\n▪ 语言资源\n▪ 关键技术\n▪ 应用系统\n6 争论\n7 处理数据\n8 处理工具\n▪ OpenNLP\n▪ FudanNLP\n▪ 语言技术平台(LTP)\n9 自然语言处理技术难点\n▪ 单词的边界界定\n▪ 词义的消歧\n▪ 句法的模糊性\n▪ 有瑕疵的或不规范的输入\n▪ 语言行为与计划\n1 详细介绍\n语言是人类区别其他动物的本质特性。在所有生物中，只有人类才具有语言能力。人类的多种智能都与语言有着密切的关系。人类的逻辑思维以语言为形式，人类的绝大部分知识也是以语言文字的形式记载和流传下来的。因而，它也是人工智能的一个重要，甚至核心部分。\n用自然语言与计算机进行通信，这是人们长期以来所追求的。因为它既有明显的实际意义，同时也有重要的理论意义：人们可以用自己最习惯的语言来使用计算机，而无需再花大量的时间和精力去学习不很自然和习惯的各种计算机语言；人们也可通过它进一步了解人类的语言能力和智能的机制。\n实现人机间自然语言通信意味着要使计算机既能理解自然语言文本的意义，也能以自然语言文本来表达给定的意图、思想等。\n前者称为自然语言理解，后者称为自然语言生成。\n因此，自然语言处理大体包括了自然语言理解和自然语言生成两个部分。历史上对自然语言理解研究得较多，而对自然语言生成研究得较少。但这种状况已有所改变。\n无论实现自然语言理解，还是自然语言生成，都远不如人们原来想象的那么简单，而是十分困难的。从现有的理论和技术现状看，通用的、高质量的自然语言处理系统，仍然是较长期的努力目标，但是针对一定应用，具有相当自然语言处理能力的实用系统已经出现，有些已商品化，甚至开始产业化。典型的例子有：多语种数据库和专家系统的自然语言接口、各种机器翻译系统、全文信息检索系统、自动文摘系统等。\n自然语言处理，即实现人机间自然语言通信，或实现自然语言理解和自然语言生成是十分困难的。\n造成困难的根本原因是自然语言文本和对话的各个层次上广泛存在的各种各样的歧义性或多义性（ambiguity）。\n一个中文文本从形式上看是由汉字（包括标点符号等）组成的一个字符串。由字可组成词，由词可组成词组，由词组可组成句子，进而由一些句子组成段、节、章、篇。无论在上述的各种层次：字（符）、词、词组、句子、段，……还是在下一层次向上一层次转变中都存在着歧义和多义现象，即形式上一样的一段字符串，在不同的场景或不同的语境下，可以理解成不同的词串、词组串等，并有不同的意义。\n一般情况下，它们中的大多数都是可以根据相应的语境和场景的规定而得到解决的。也就是说，从总体上说，并不存在歧义。这也就是我们平时并不感到自然语言歧义，和能用自然语言进行正确交流的原因。但是一方面，我们也看到，为了消解歧义，是需要极其大量的知识和进行推理的。如何将这些知识较完整地加以收集和整理出来；又如何找到合适的形式，将它们存入计算机系统中去；以及如何有效地利用它们来消除歧义，都是工作量极大且十分困难的工作。这不是少数人短时期内可以完成的，还有待长期的、系统的工作。\n以上说的是，一个中文文本或一个汉字（含标点符号等）串可能有多个含义。它是自然语言理解中的主要困难和障碍。反过来，一个相同或相近的意义同样可以用多个中文文本或多个汉字串来表示。\n因此，自然语言的形式（字符串）与其意义之间是一种多对多的关系。其实这也正是自然语言的魅力所在。但从计算机处理的角度看，我们必须消除歧义，而且有人认为它正是自然语言理解中的中心问题，即要把带有潜在歧义的自然语言输入转换成某种无歧义的计算机内部表示。\n歧义现象的广泛存在使得消除它们需要大量的知识和推理，这就给基于语言学的方法、基于知识的方法带来了巨大的困难，因而以这些方法为主流的自然语言处理研究几十年来一方面在理论和方法方面取得了很多成就，但在能处理大规模真实文本的系统研制方面，成绩并不显著。研制的一些系统大多数是小规模的、研究性的演示系统。\n目前存在的问题有两个方面：\n一方面，迄今为止的语法都限于分析一个孤立的句子，上下文关系和谈话环境对本句的约束和影响还缺乏系统的研究，因此分析歧义、词语省略、代词所指、同一句话在不同场合或由不同的人说出来所具有的不同含义等问题，尚无明确规律可循，需要加强语用学的研究才能逐步解决。\n另一方面，人理解一个句子不是单凭语法，还运用了大量的有关知识，包括生活知识和专门知识，这些知识无法全部贮存在计算机里。因此一个书面理解系统只能建立在有限的词汇、句型和特定的主题范围内；计算机的贮存量和运转速度大大提高之后，才有可能适当扩大范围.\n以上存在的问题成为自然语言理解在机器翻译应用中的主要难题，这也就是当今机器翻译系统的译文质量离理想目标仍相差甚远的原因之一；而译文质量是机译系统成败的关键。\n中国数学家、语言学家周海中教授曾在经典论文《机器翻译五十年》中指出：\n要提高机译的质量，首先要解决的是语言本身问题而不是程序设计问题；\n单靠若干程序来做机译系统，肯定是无法提高机译质量的；\n另外在人类尚未明了大脑是如何进行语言的模糊识别和逻辑判断的情况下，机译要想达到“信、达、雅”的程度是不可能的。\n2 发展历史\n最早的自然语言理解方面的研究工作是机器翻译。1949年，美国人威弗首先提出了机器翻译设计方案。20世纪60年代，国外对机器翻译曾有大规模的研究工作，耗费了巨额费用，但人们当时显然是低估了自然语言的复杂性，语言处理的理论和技术均不成热，所以进展不大。主要的做法是存储两种语言的单词、短语对应译法的大辞典，翻译时一 一对应，技术上只是调整语言的同条顺序。但日常生活中语言的翻译远不是如此简单，很多时候还要参考某句话前后的意思。\n大约90年代开始，自然语言处理领域发生了巨大的变化。这种变化的两个明显的特征是：\n（1）对系统输入，要求研制的自然语言处理系统能处理大规模的真实文本，而不是如以前的研究性系统那样，只能处理很少的词条和典型句子。只有这样，研制的系统才有真正的实用价值。\n（2）对系统的输出，鉴于真实地理解自然语言是十分困难的，对系统并不要求能对自然语言文本进行深层的理解，但要能从中抽取有用的信息。例如，对自然语言文本进行自动地提取索引词，过滤，检索，自动提取重要信息，进行自动摘要等等。\n同时，由于强调了“大规模”，强调了“真实文本”，下面两方面的基础性工作也得到了重视和加强。\n（1）大规模真实语料库的研制。大规模的经过不同深度加工的真实文本的语料库，是研究自然语言统计性质的基础。没有它们，统计方法只能是无源之水。\n（2）大规模、信息丰富的词典的编制工作。规模为几万，十几万，甚至几十万词，含有丰富的信息（如包含词的搭配信息）的计算机可用词典对自然语言处理的重要性是很明显的。\n3 相关内容\n自然语言处理（NLP）是计算机科学，人工智能，语言学关注计算机和人类（自然）语言之间的相互作用的领域。因此，自然语言处理是与人机交互的领域有关的。在自然语言处理面临很多挑战，包括自然语言理解，因此，自然语言处理涉及人机交互的面积。在NLP诸多挑战涉及自然语言理解，即计算机源于人为或自然语言输入的意思，和其他涉及到自然语言生成。\n现代NLP算法是基于机器学习，特别是统计机器学习。机器学习范式是不同于一般之前的尝试语言处理。语言处理任务的实现，通常涉及直接用手的大套规则编码。\n许多不同类的机器学习算法已应用于自然语言处理任务。这些算法的输入是一大组从输入数据生成的“特征”。一些最早使用的算法，如决策树，产生硬的if-then规则类似于手写的规则，是再普通的系统体系。然而，越来越多的研究集中于统计模型，这使得基于附加实数值的权重，每个输入要素柔软，概率的决策。此类模型具有能够表达许多不同的可能的答案，而不是只有一个相对的确定性，产生更可靠的结果时，这种模型被包括作为较大系统的一个组成部分的优点。\n自然语言处理研究逐渐从词汇语义成分的语义转移，进一步的，叙事的理解。然而人类水平的自然语言处理，是一个人工智能完全问题。它是相当于解决中央的人工智能问题使计算机和人一样聪明，或强大的AI。自然语言处理的未来一般也因此密切结合人工智能发展。[1]\n4 相关技术\n略。\n5 概述\n基础理论\n自动机 形式逻辑 统计机器学习汉语语言学 形式语法理论\n语言资源\n语料库 词典\n关键技术\n汉字编码词法分析 句法分析 语义分析 文本生成 语音识别\n应用系统\n文本分类和聚类 信息检索和过滤 信息抽取问答系统 拼音汉字转换系统 机器翻译 新信息检测\n6 争论\n自然语言处理的基础是各类自然语言处理数据集，如tc-corpus-train（语料库训练集）、面向文本分类研究的中英文新闻分类语料、以IG卡方等特征词选择方法生成的多维度ARFF格式中文VSM模型、万篇随机抽取论文中文DBLP资源、用于非监督中文分词算法的中文分词词库、UCI评价排序数据、带有初始化说明的情感分析数据集等。\n7 处理数据\n8 处理工具\n▪ OpenNLP\n▪ FudanNLP\n▪ 语言技术平台(LTP)\n9 自然语言处理技术难点\n▪ 单词的边界界定\n▪ 词义的消歧\n▪ 句法的模糊性\n▪ 有瑕疵的或不规范的输入\n▪ 语言行为与计划\n单词的边界界定\n在口语中，词与词之间通常是连贯的，而界定字词边界通常使用的办法是取用能让给定的上下文最为通顺且在文法上无误的一种最佳组合。在书写上，汉语也没有词与词之间的边界。\n词义的消歧\n许多字词不单只有一个意思，因而我们必须选出使句意最为通顺的解释。\n句法的模糊性\n自然语言的文法通常是模棱两可的，针对一个句子通常可能会剖析(Parse)出多棵剖析树(Parse Tree)，而我们必须要仰赖语意及前后文的信息才能在其中选择一棵最为适合的剖析树。\n有瑕疵的或不规范的输入\n例如语音处理时遇到外国口音或地方口音,或者在文本的处理中处理拼写,语法或者光学字符识别(OCR)的错误。\n语言行为与计划\n句子常常并不只是字面上的意思；例如，“你能把盐递过来吗”，一个好的回答应当是把盐递过去；\n在大多数上下文环境中，“能”将是糟糕的回答，虽说回答“不”或者“太远了我拿不到”也是可以接受的。再者，如果一门课程上一年没开设，对于提问“这门课程去年有多少学生没通过？”回答“去年没开这门课”要比回答“没人没通过”好。","date":"2018年01月08日 11:50:58"}
{"_id":{"$oid":"5d36a8d36734bd8e681d5e97"},"title":"自然语言处理Java开源包FNLP(FudanNLP)的使用","author":"黄骨鱼骨","content":"自然语言处理是如今计算机科学领域比较火热的一个方向，其也确实有很大的应用场景。前面说过，我参加了微软编程之美的比赛，这个比赛其所基于的就是自然语言处理。我本身并不是学自然语言处理的，甚至连这门课也没有选过，可是为了完成资格赛我自己在网上找了一些资料。\n时间所限，也是能力所限，我当然不会自己去实现一些诸如中文分词与词性标注的自然语言处理算法。自然的，我想到了在网上找一些开源包。我主要考虑的是三个包，首先是斯坦福大学的一系列自然语言处理工具，作为国际知名高校自然人们都会认为斯坦福的技术会更高一点，可是我在网上并没有找到太多的使用文档，而且软件也比较大，简单使用也并不需要太高深的技术。然后呢，我又找到了哈工大的LTP，这个是我们自己学校的东西，而且也广受认可，所以天然的，我倾向于使用它。但是我发现LTP不是开源的，如果要索要代码还需要签署一些协议，哎。。。最后我选择使用复旦大学的自然语言处理开源包FNLP，它的优点在于获取方便，而且是比较轻量级的，简单使用比较方便。下面给出入门教程：\nFNLP入门\n1.下载跟编译\nGitHub上下载FNLP压缩包；\n下载相应的模型文件，放在第一步下载的文件里的models目录；\n下载Maven，并按照教程配置（其实就是解压缩和配置环境变量）\n在命令行中进入FNLP的源码目录（即“README.md”所在的目录），执行如下命令进行编译：\nmvn install -Dmaven.test.skip=true\n（更新2018.4.25：win10系统如果进入的是powershell， 则使用命令\nmvn install '-Dmaven.test.skip=true' ）\n这会编译四个Jar包：fnlp-core、fnlp-dev、fnlp-train、fnlp-app、fnlp-demo。它们分别位于源码目录中，各自对应目录中的”target”目录之中，例如fnlp-core的软件包位于： fnlp-core/target/fnlp-core-2.0-SNAPSHOT.jar\n2.命令行使用\nFNLP的源码目录下的命令行（可以Win+R 输入cmd，然后进入源码目录；也可以在源码目录下，按住shift右键进入命令行），执行maven命令：\nmvn dependency:copy-dependencies -DoutputDirectory=libs\n这样jar包都会copy到工程目录下的libs里面\n输入 java -Xmx1024m -Dfile.encoding=UTF-8 -classpath \".;fnlp-core/target/fnlp-core-2.1-SNAPSHOT.jar;libs/trove4j-3.0.3.jar;libs/commons-cli-1.2.jar\" org.fnlp.nlp.cn.tag.CWSTagger -s models/seg.m \"自然语言是人类交流和思维的主要工具，是人类智慧的结晶。\" 可以使用分词功能，也可以测试是否安装成功（windows系统下）。结果如图：\n\n3.调用FNLP库（eclipse下使用）\n在eclipse左侧的Project Explorer中选择项目名称，右击，在菜单中选择Build Path，Add External Archives…，则会弹出文件选择对话框，依次查找并添加下列文件（第二步获得的一些jar包，可以在lib等文件夹下找到）：\nfnlp-core-2.1-SNAPSHOT.jar\ntrove4j-3.0.3.jar\ncommons-cli-1.2.jar\n修改虚拟机最大内存量（网上有教程）；\n模型文件指词典、训练后的中文分词器、POS标注器等，它们位于FNLP源码目录下的“models”目录之中。将此目录复制到Eclipse项目目录之下即可。\n接下来就可以编程调用了。\nFNLP提供了一系列中文处理工具，其中中文分词、词性标注、实体名识别等基础功能已经封装在工厂类CNFactory之中。CNFactory位于org.fnlp.nlp.cn包之中，经过初始化后就可以使用其提供的全部功能：\nimport org.fnlp.nlp.cn.CNFactory; CNFactory factory = CNFactory.getInstance(\"models\");\n以上代码创建了一个CNFactory对象，并载入位于“models”目录下的模型文件。接下来就可以使用CNFactory的对象来进行各种中文语言处理任务。\n中文分词\npublic static void main(String[] args) throws Exception { // 创建中文处理工厂对象，并使用“models”目录下的模型文件初始化 CNFactory factory = CNFactory.getInstance(\"models\"); // 使用分词器对中文句子进行分词，得到分词结果 String[] words = factory.seg(\"关注自然语言处理、语音识别、深度学习等方向的前沿技术和业界动态。\"); // 打印分词结果 for(String word : words) { System.out.print(word + \" \"); } System.out.println(); }\n结果： 关注 自然 语言 处理 、 语音 识别 、 深度 学习 等 方向 的 前沿 技术 和 业界 动态 。\n中文词性标注\npublic static void main(String[] args) throws Exception { // 创建中文处理工厂对象，并使用“models”目录下的模型文件初始化 CNFactory factory = CNFactory.getInstance(\"models\"); // 使用标注器对中文句子进行标注，得到标注结果 String result = factory.tag2String(\"关注自然语言处理、语音识别、深度学习等方向的前沿技术和业界动态。\"); // 显示标注结果 System.out.println(result); }\n结果：关注/动词 自然/名词 语言/名词 处理/动词 、/标点 语音/名词 识别/名词 、/标点 。。。\n实体名识别\npublic static void main(String[] args) throws Exception { // 创建中文处理工厂对象，并使用“models”目录下的模型文件初始化 CNFactory factory = CNFactory.getInstance(\"models\"); // 使用标注器对包含实体名的句子进行标注，得到结果 HashMap\u003cString, String\u003e result = factory.ner(\"詹姆斯·默多克和丽贝卡·布鲁克斯 鲁珀特·默多克旗下的美国小报《纽约邮报》的职员被公司律师告知，保存任何也许与电话窃听及贿赂有关的文件。\"); // 显示标注结果 System.out.println(result); }\n结果：\n{詹姆斯·默多克=人名, 鲁珀特·默多克旗=人名, 丽贝卡·布鲁克斯=人名, 纽约=地名, 美国=地名}","date":"2017年05月21日 21:17:43"}
{"_id":{"$oid":"5d36a8d36734bd8e681d5e9b"},"title":"中文自然语言处理语言资源项目（ChineseNLPcorpus）","author":"liuhuanyong_iscas","content":"ChineseNLPcorpus\nAn collection of Chinese nlp corpus including basic Chinese syntactic wordset, semantic wordset, historic corpus and evaluate corpus. 中文自然语言处理的语料集合，包括语义词、领域共时、历时语料库、评测语料库等。本项目简单谈谈自己对语言资源的感想以及目前自己进行语言资源构建的现状。\n项目地址：https://github.com/liuhuanyong/ChineseNLPCorpus\n介绍\n语言资源，本身是一个宽泛的概念，即语言+资源，语言指的是资源的限定域，资源=资+源，是资料的来源或者汇总，加在一起，也就形成了这样一种界定：任何语言单位形成的集合，都可以称为语言资源。语言资源是自然语言处理任务中的一个必不可少的组成部分，一方面语言资源是相关语言处理任务的支撑，为语言处理任务提供先验知识进行辅助，另一方面，语言处理任务也为语言资源提出了需求，并能够对语言资源的搭建、扩充起到技术性的支持作用。因此，随着自然语言处理技术的不断发展，自然语言处理需求在各个领域的不断扩张、应用，相关语言资源的构建占据了越来越为重要的地位。作者硕士期间所在的研究机构为国家语言资源监测与研究平面媒体中心，深受导师所传授的语言资源观熏陶，并在实际的学习、工作过程中，动手实践，形成了自己的一些浅薄的语言资源认识，现在写出来，供大家一起讨论，主要介绍一些自己对语言资源的搜索，搭建过程中的一些心得以及自己目前在语言资源建设上的一些工作。\n语言资源的分类\n介绍中说到，任何语言单位的集合都可以称为语言资源，比如我有一个个人的口头禅集合，这个就可以称为一个语言资源库，在你实际生活中进行言语活动时，你其实就在使用这个语言资源库。再比如说，一个班级中的学生名单，其实也可以当作是一种语言资源，这个语言资源在进行班级学生点名、考核的时候也大有帮助。当然，此处所讨论的语言资源是从自然语言处理应用的角度上出发的。总的来说，我把它归为以下两种类型：\n1、领域语料库\n领域语料库，是从语料的这个角度来讲的，这里的语料，界定成文本级别（以自然语句为基础级别形成的文本集合，即可以是句子、段落、篇章等）。领域语料库，可以根据不同的划分规则而形成不同的语料类别：\n1）根据所属领域，可以进一步细化成不同领域的语料库。包括金融领域语料、医药领域语料、教育领域语料、文学领域语料等等。\n2）根据所属目的，可以进一步细化为：评测语料（为自然语言处理技术pk而人工构造的一些评测语料，如ACE,MUC等国际评测中所出现的如semeval2014,snli等）；工具语料（指供自然语言处理技术提供资源支撑的语料）\n3）根据语料加工程度的不同，可进一步分为：熟语料（指在自然语言单位上添加人工的标签标注，如经过分词、词性标注、命名实体识别、依存句法标注形成的语料），生语料（指直接收集而未经加工形成的语言资源集，如常见的微博语料，新闻语料等）\n4）根据语料语种的不同，可进一步分为：单语语料和多语语料，多语语料指的是平行语料，常见于机器翻译任务中的双语对齐语料（汉-阿平行语料库，汉-英平行语料库）等。\n5）根据语料规模的不同，可以进一步分为：小型语料库，中型语料库，大型语料库。至于小型、中型、大型的界定，可根据实际领域语料的规模而动态调整\n2、领域词库\n领域词库，指以句级以下语言单位形成的语言资源库，这个层级的语言单位可以是笔画、偏旁部首、字、词、短语等。同样的，领域词库也可以进一步细分。\n1）领域特征词库。这里所说的领域特征词库，指的是与领域强相关，具有领域区别能力形成的词语集合，如体育领域中常见的“篮球”、“足球”等词，文学领域常见的“令狐冲”、“鲁迅”等词，又如敏感词库等，这些词常常可作为分类特征而存在。\n2）语法语义词库。语义词库的侧重点在与语言的语法层面和语义层面：\na）语法词库：北大的语法信息词典，北大的实体概念词典、Hownet语义词典这三类词典，这几个语法词库，在对词的语法功能上都做了不同的工作，对词的内部结构信息进行了详细的标注，如北大的语法信息词典，以词类为划分标准讲汉语的常用词进行了划分，并对词性、搭配（前接成分和后接成分）进行了详细的标注；Hownet语义词典从义项的角度对词的义元进行了分解和注释。\nb）语义词库：这类语义词，侧重点不在词语的内部语法结构，而在词语的整体语义上。这类词库，常见的词库有哈工大发布的同义词词林扩展版，这个词库将同义词按照语义的相近程度进行了不同层次的聚类，可以作为同义词扩展提供帮助。另一个是情感分析任务中常用的情感词典，这类词典主要公开的词典包括大连理工大学信息检索实验室公开的情感本体词库、hownet、香港中文大学、台湾清华大学公开的情感词库（具体包括情感词库、否定词库、强度词库）等。另外，工业界，有boson公开的微博情感词库（词的规模比较大，但标注信息不是很精准）。还有的，则是中文的反义词库等，这个可以参考我的github项目，里面对这些词库也有一些涉及。\n语言资源的问题\n语言资源的搭建，指的是语言资源的整个搭建过程。其实是要解决四个问题，一个是语言资源的收集问题；二是语言资源的融合标准化问题；三是语言资源的动态更新问题；四是语言资源的共享与联盟问题。下面就这四点展开阐述：\n1、语言资源收集的问题。语言资源搜索过程中有三步走策略，在这个步骤完成之后，会得到一系列的词库。这些词库可能初期不会特别完善，往往还需要人工使用启发式规则进行人工去噪的工作。\n2，语言资源的融合标准化问题。通过不同方式收集起来的语言资源，往往会存在一个格式不对称的问题，这有点像知识图谱中的知识融合问题。因此，为了解决这个问题，我们通常需要制定一个标准化的语言资源格式，例如，在构建情感词表的过程当中，有的情感词表没有强度标记，有的强度值范围不一样，有的情感词表的标记不一，这个时候往往需要标准化，给定一个标准化的样式，再将不同来源的情感词按照这个标记做相应的调整。我在实际的工作过程中，常常把这种问题类别成知识图谱构建过程中的schema搭建问题，信息抽取过程中的slot-definition问题。先把规范和标准搭好，再去统一标准化。\n3，语言资源的动态更新问题。知识和信息的价值，在很大程度上都在于它的一种实时性，语言资源作为一种常识性知识库，能够保证自身的一种与时俱进，将能够最大限度地发挥自身的价值。而从实践的角度上来说，语言资源的动态更新，可以靠人工去维持，去动态及时更新，也可以建立一种动态监测和更新机制，让机器自动地去更新。这类其实可以参考知识图谱更新的相关工作。\n4，语言资源的共享与联盟问题。语言资源是否共享，其实是一个与业务敏感以及开源意识想结合的一种决策，有的资源因为某种业务敏感或者开源意识不够open而无法共享，当然还有其他因素成分在，不过，语言资源最好是需要共享的，这样能够最大力度的发挥语言资源在各个领域的应用。语言资源的联盟问题，更像是对开源语言资源的一种链接与互联。这类问题是对当前的资源零散、碎片化问题的一个思考，前面也说到，目前情感分析的词表有很多个，语法和语义词库也有很多个，但每个人在构建时的出发点不同，构建者也分布在不同的高校或机构当中，这些资源虽然在个数上会有增长，但随着时间的推移，这种零散化的现象将会越来越严重。\n语言资源的实践\n作者在学习和工作之余，根据语言资源搭建策略，构建起了语义词库、领域词库、领域语料库、评测语料库。种类约50种，具体如下：\n语义知识库\n类型\n名称\n介绍\n语义词库\n语法信息词典\n汉语词语的语法功能分类、词语的语法属性描述\n语义词库\nHownet义原词典\n董振东老师研制，汉语词语义原分类\n语义词库\n程度副词词典\n表示程度的词\n语义词库\n现代汉语词典\n现代汉语词典, txt版本\n语义词库\n否定词词典\n对意义进行反转的词典\n语义词库\n同义词词林词典\n哈工大同义词词典\n语义词库\n反义词词典\n反义词词表，1.5W对\n语义词库\n同义词词典\n同义词词典，5.5W对\n语义词库\nschema概念词典\n互动百科概念体系，百度百科概念体系\n语义词库\n停用词\n自然语言处理用停用词词表\n领域词库\n类型\n名称\n介绍\n领域词库\n搜狗输入法领域词库\n超过1W个领域的搜狗输入法词库txt版本\n领域词库\n职位词典\n基于百万级拉钩JD网抽取形成的职位词典\n领域词库\n敏感词词词库\n敏感词词库，包括政治、反动等词\n领域词库\n情感词词库\n大连理工、知网、港中大、台大、boson等公开情感词典\n领域语料库\n类型\n名称\n介绍\n领域语料库\n人民日报标注语料\n1998年人民日报分词语料库\n领域语料库\n20类小说文本集合\n20个领域(武侠、恐怖等)小说集合，7K+小说文本\n领域语料库\n字幕网70W字幕文本语料\n字幕网字幕文件解析，70W字幕文本语料\n领域语料库\n内涵段子50W等语料\n基于内涵段子采集，50W短文本\n领域语料库\n歌词14W语料\n基于公开歌词网采集，14W首歌曲歌词\n领域语料库\n职位JD语料\n基于公开职位采集，213W职位jd\n领域语料库\n古诗词语料\n唐诗宋词语料集合，10W篇\n领域语料库\n相声剧本语料\n基于公开相声剧本网站采集，6K篇\n领域语料库\n中文维基百科语料\n中文简体版，98W篇\n领域语料库\n法务问答语料\n法务咨询问答对，22W\n领域语料库\n股票问答语料\n股票相关咨询问答对，10W\n领域语料库\n携程攻略50W\n携程攻略文本集，50W篇\n领域语料库\n法律案例语料17W\n17W法律案例语料，带案例标签\n领域语料库\n人民日报历时语料库1946-2003\n1946-2003，133W篇\n领域语料库\n参考消息历时语料库1957-2002\n1957-2002，57W篇\n领域语料库\n腾讯滚动新闻历时语料库2009-2016\n腾讯历时滚动新闻(13板块)\n领域语料库\n酒店评论语料\n酒店评论数据7K条\n领域语料库\n外卖点评语料\n外卖评论数据1.2W条\n领域语料库\n京东商品评论语料\n10类商品6W条\n领域语料库\n新浪微博情感语料\n正文及评论10W条\n领域语料库\n细粒度微博情感语料\n喜悦、愤怒、厌恶、低落等标签共36W条\n领域语料库\n电影评论语料\n电影评分评论语料，200W+条\n领域语料库\n餐馆点评语料\n餐馆点评语料，440W条\n领域语料库\n亚马逊商品评论语料\n亚马逊商品评论语料，720W条\n评测语料库\n类型\n名称\n介绍\n评测语料库\n问句匹配\n英文question相似问句6.5W对，中文微众银行问句集1000对\n评测语料库\n命名实体识别\n中文电子病历命名实体识别、微软MSR命名实体识别5W条\n评测语料库\n情感分析\n斯坦福sentibank\n评测语料库\n实体关系抽取\n中文人物关系数据集、英文SEMEVAL2008评测数据集(NYT,NYTfilter)\n评测语料库\n文本蕴含\n英文snli,multinli数据集116W，中文文本蕴含数据集100W\n评测语料库\n音乐问句解析\n音乐问句解析数据集1.2W\n评测语料库\n幽默计算\n中文幽默计算数据集（幽默类型、幽默等级、隐喻类型、隐喻等级分类等)\n评测语料库\n阅读理解\nsquad数据集\n评测语料库\n知识图谱补全\n知识图谱链接数据集(FB15K, FB40K, Freebase, WN18,WordNet)\n评测语料库\n中文实体链接\n基于中文百科知识的实体链接数据集1.3K\n评测语料库\n中文自动问答\n中文智能问答数据集，两个任务(问句意图分类，航空、酒店、火车客服问答)\n评测语料库\n中文罪行分类\n法律智能评测数据集，288W\n总结\n1、本项目阐述了语言资源的相关感想，并给出了目前语言资源的构建现状，目前为止收集了四个大类共50小类的语言资源数据集。\n2、本项目中所涉及到的报告内容均来源于网上公开资源，对此免责声明。\n3、如果有需要用到以上作者收集到的这些语料库，可以联系作者获取。\n4、自然语言处理，是人工智能皇冠上的一颗明珠，懂语言者得天下，语言资源在自然语言处理中扮演着举足轻重的作用，懂语言资源者，分得天下。目前开放的网络环境，对语言资源的大繁荣提供了很大的契机。语言资源构建是一门学问，也是一种手段，现在自然语言处理技术也对语言资源的构建提供了技术上的支持，如何把握语言资源搜索策略，搭建策略，重点解决语言资源的动态更新、共享与联盟问题，将是语言资源建设未来需要解决的问题。\n本项目地址：https://github.com/liuhuanyong/ChineseNLPCorpus\n如有自然语言处理、知识图谱、事理图谱、社会计算、语言资源建设等问题或合作，可联系我：\n1、我的github项目介绍：https://liuhuanyong.github.io\n2、我的csdn博客：https://blog.csdn.net/lhy2014\n3、刘焕勇，中国科学院软件研究所，lhy_in_blcu@126.com","date":"2018年12月16日 11:09:04"}
{"_id":{"$oid":"5d36a8d56734bd8e681d5e9e"},"title":"读《自然语言处理》--序及诸论","author":"xiaopihaierletian","content":"1、自然语言处理是一门集语言学、数学、计算机科学、认知科学于一体的综合性交叉学科。\n2、自然语言处理主要有两种方法，基于规则的方法和基于统计的方法；\n基于统计的方法，属于哲学中的经验主义，主要采用归纳法；同样假设大脑中存在某些认知的能力，该方法和理性主义方法的区别是程度上的区别， 而非本质上的区别。\n基于规则的方法，属于哲学中的理性主义，主要采用演绎法；人们相信人类大脑中的重要知识不是由感官得到的，而是提前固定在头脑中，由遗传基因决定。\n3、培根主张理性主义与经验主义相结合。\n4、语言障碍是制约21世纪社会全球化的一个重要因素。\n5、自然语言处理NLP也称为自然语言理解NLU。\n6、图灵测试——计算机被误认为是人的几率就是智能程度。\n7、部分研究方向：\n机器翻译（Machine Translation，MT）：一种语言到另一种语言；\n自动文摘（automatic summarizing/abstracting）：提炼出原文档的主要内容；\n信息检索（information retrieval，IR）：从海量文档中找到符合用户要求的相关文档，面向多语言的信息检索叫做跨语言信息检索（cross-language IR）；\n文档分类（document categorization/classification，也称文本分类，信息分类）：把大量的文档按照一定的标准进行自动归类；\n问答系统（question-answering system）：计算机对人提出的问题进行理解、推理、分析，在有关的知识资源中自动求解答案并回答。与其他技术构成人机对话系统（human-computer dialogue system）；\n文字编辑和自动校对：对文字的拼写、用词、语法、格式等检查、校对、编排；\n信息过滤：自动过滤那些满足特定要求条件的文档，主要用于信息安全和防护；\n文字识别：对印刷体或手写体文字进行自动识别，将其转换成计算机可以处理的电子文本。其主要内容属于字符图像识别；\n语音识别：把语音信号转换成书面形式；\n文语转换：将文本转换成语音，又叫语音合成；\n说话人识别/验证：对一个人说话的言语样本进行声学分析，推断说话人的身份；\n8、自然语言形态学、语法学、语义学、语用学：\n形态学：研究词的内部结构，包括屈折变化和构词法；\n语法学：研究句子结构成分之间的关系，中心就是为什么一句话可以这么说，也可以那么说。\n语义学：研究语言各级单位的意义以及语义与语音、语法、修辞、文字、语境、哲学思想、社会环境、个人修养的关系等等，中心就是：这个语言单位到底说了什么。\n语用学：涉及方面较多，中心就是：为什么在特定的上下文中要说这句话。\n9、面临的问题：\n【1】歧义消解问题：无论在词法层次、句法层次、语义层次、语用层次，无论哪类语言单位，其歧义性始终都是困扰人们实现应用目标的根本问题；\n例句1：put the block in the box on the table.可以是理解为：\n（1）       put the block [in the box on the table]\n（2）       put [the block in the box] on the table\n实际上，英文中歧义结构分析结果的数量是随着借此短语数目的增加呈指数上升的，其歧义组合的复杂程度随着借此短语个数的增加不断加深，这个歧义结构的组合数成为开塔兰数（Catalan numbers，记作Cn），如果句子中存在n个介词短语，那么Cn可以表示为：\n汉语尽管不像英语那样由于多个介词结构成分而导致大量歧义，但是汉语中也普遍存在有歧义现象。\n例句2：喜欢乡下的孩子。\n例句3：关于鲁迅的著作。\n这都是句法歧义，而词汇的词类歧义、词义歧义、句子的语义歧义也同样是NPL中普遍存在的现象。\n例句4：我的头像牛逼吗？\n例句5：今天中午吃食堂。\n例句6：“火烧圆明园”与“驴肉火烧”。\n例句7：打鼓、打架、打球、打电话、打毛衣、打伞、一打铅笔、自打今天起。。。\n例句8：\n他说：“她这个人真有意思funny。”\n她说：“他这个人也怪有意思的funny。”\n人们以为他俩有了意思wish，就让他向她意思意思express。\n他急了：“我根本没那个意思thought！”\n她也生气了：“你们这么说是什么意思intention？！”\n有人觉得这个段子很有意思funny，但是也有人觉得这个段子并没有意思sense。\n当然，像这个段子中这么复杂的用词方法，在实际生活中几乎没有人使用。这个段子的目的呢，只是说明自然语言中的歧义是普遍存在的现象，并不是说一个自然语言处理系统必须具备如此复杂的歧义消解能力才能算得上是真正实用的系统。\n【2】未知语言现象处理：随着社会的发展，新的词汇、新的词义、新的用法、新的句子用法都在不断出现。\n例如9：元芳，你怎么看？\n例如10：灌水、盖楼、沙发、童鞋、盆友、驴友。\n10、实践证明，除了语音识别和机器翻译以外，很多自然语言处理的研究任务，包括汉语自动分词和词性标注、文字识别、拼音法汉字输入等，都可以用噪声信道模型来描述和实现。\n11、研究现状\n（1）很多技术已经达到或者基本达到实用程度；例如，文字输入、编辑、排版，文字识别，电子词典，语音合成；\n（2）许多新的研究方向不断出现；受实际应用驱动，将NLP技术与其他相关技术融合，用于研究和开发更多实用的技术。例如，网络内容管理，网络信息监控，有害信息过滤等，这些技术不仅与NLP技术密切相关，还设计图像处理、情感计算、网络技术等多种技术。此外、还有语音自动翻译、语音自动文摘、语音检索、基于图像内容及文字说明的图像理解技术研究；\n（3）许多理论问题尚未得到根本性解决，整个NLP领域也尚未建立起一套完整、系统的理论框架体系；很多方法已经得到实际应用，比如上下文无关问法、HMM、噪声信道模型等；很多重要问题也尚未彻底、有效的解决，例如语义的形式化与计算问题、句法分析问题、指代歧义消解问题、汉语自动分词中的未登录词识别问题等；","date":"2017年03月31日 14:30:44"}
{"_id":{"$oid":"5d36a8d56734bd8e681d5ea0"},"title":"自然语言处理的一些算法研究和实现(NLTK)","author":"GodBMW","content":"递归\n如果要计算n个词有多少种组合方式按照阶乘定义n nn-11\n如果要寻找word下位词的大小并且将他们加和\n构建一个字母查找树\n贪婪算法不确定边界自然语言的分割问题退火算法的非确定性搜索\n动态规划\n首先用递归的方式编写一下找到任意音节的函数\n使用动态规划来实现找到任意音节的函数\nNLTK自带装饰符默记\n其他的应用\n词汇多样性\n文体差异性\n随机语句生成\n词谜问题解决\n时间和空间权衡全文检索系统\n自然语言处理中算法设计有两大部分：分而治之 和 转化 思想。一个是将大问题简化为小问题，另一个是将问题抽象化，向向已知转化。前者的例子：归并排序；后者的例子：判断相邻元素是否相同（与排序）。\n这次总结的自然语言中常用的一些基本算法，算是入个门了。\n递归\n使用递归速度上会受影响，但是便于理解算法深层嵌套对象。而一些函数式编程语言会将尾递归优化为迭代。\n如果要计算n个词有多少种组合方式？按照阶乘定义：n! = n*(n-1)*…*1\ndef func(wordlist): length = len(wordlist) if length==1: return 1 else: return func(wordlist[1:])*length\n如果要寻找word下位词的大小，并且将他们加和。\nfrom nltk.corpus import wordnet as wn def func(s):#s是WordNet里面的对象 return 1+sum(func(child) for child in s.hyponyms()) dog = wn.synset('dog.n.01') print(func(dog))\n构建一个字母查找树\n建立一个嵌套的字典结构，每一级的嵌套包含既定前缀的所有单词。而子查找树含有所有可能的后续词。\ndef WordTree(trie,key,value): if key: first , rest = key[0],key[1:] if first not in trie: trie[first] = {} WordTree(trie[first],rest,value) else: trie['value'] = value WordDict = {} WordTree(WordDict,'cat','cat') WordTree(WordDict,'dog','dog') print(WordDict)\n贪婪算法：不确定边界自然语言的分割问题(退火算法的非确定性搜索)\n爬山法是完完全全的贪心法，每次都鼠目寸光的选择一个当前最优解，因此只能搜索到局部的最优值。模拟退火其实也是一种贪心算法，但是它的搜索过程引入了随机因素。模拟退火算法以一定的概率来接受一个比当前解要差的解，因此有可能会跳出这个局部的最优解，达到全局的最优解。\nimport nltk from random import randint #text = 'doyou' #segs = '01000' def segment(text,segs):#根据segs，返回切割好的词链表 words = [] last = 0 for i in range(len(segs)): if segs[i]=='1':#每当遇见1,说明是词分界 words.append(text[last:i+1]) last = i+1 words.append(text[last:]) return words def evaluate(text,segs): #计算这种词分界的得分。作为分词质量，得分值越小越好(分的越细和更准确之间的平衡) words = segment(text,segs) text_size = len(words) lexicon_size = len(' '.join(list(set(words)))) return text_size + lexicon_size ###################################以下是退火算法的非确定性搜索############################################ def filp(segs,pos):#在pos位置扰动 return segs[:pos]+str(1-int(segs[pos]))+segs[pos+1:] def filp_n(segs,n):#扰动n次 for i in range(n): segs = filp(segs,randint(0,len(segs)-1))#随机位置扰动 return segs def anneal(text,segs,iterations,cooling_rate): temperature = float(len(segs)) while temperature\u003e=0.5: best_segs,best = segs,evaluate(text,segs) for i in range(iterations):#扰动次数 guess = filp_n(segs,int(round(temperature))) score = evaluate(text,guess) if score\u003cbest: best ,best_segs = score,guess score,segs = best,best_segs temperature = temperature/cooling_rate #扰动边界，进行降温 print( evaluate(text,segs),segment(text,segs)) print() return segs text = 'doyouseethekittyseethedoggydoyoulikethekittylikethedoggy' seg = '0000000000000001000000000010000000000000000100000000000' anneal(text,seg,5000,1.2)\n动态规划\n它在自然语言中运用非常广泛。首先他需要一张表，用来将每一次的子结果存放在查找表之中。避免了重复计算子问题！！！\n这里我们讨论一个梵文组合旋律的问题。短音节：S，一个长度；长音节：L，两个长度。所以构建长度为2的方式：{SS,L}。\n首先用递归的方式编写一下找到任意音节的函数\ndef func1(n): if n==0: return [\"\"] elif n==1: return [\"S\"] else: s = [\"S\" + item for item in func1(n-1)] l = [\"L\" + item for item in func1(n-2)] return s+l print(func1(4))\n使用动态规划来实现找到任意音节的函数\n之前递归十分占用时间，如果是40个音节，我们需要重复计算632445986次。如果使用动态规划，我们可以把结果存到一个表中，需要时候调用，而不是很坑爹重复计算。\ndef func2(n):#采用自下而上的动态规划 lookup = [[\"\"],[\"S\"]] for i in range(n-1): s = [\"S\"+ item for item in lookup[i+1]] l = [\"L\" + item for item in lookup[i]] lookup.append(s+l) return lookup print(func2(4)[4]) print(func2(4))\ndef func3(n,lookup={0:[\"\"],1:[\"S\"]}):#采用自上而下的动态规划 if n not in lookup: s = [\"S\" + item for item in func3(n-1)] l = [\"L\" + item for item in func3(n-2)] lookup[n] = s+l return lookup[n]#必须返回lookup[n].否则递归的时候会出错 print(func3(4))\n对于以上两种方法，自下而上的方法在某些时候会浪费资源，因为，子问题不一定是解决主问题的必要条件。\nNLTK自带装饰符:默记\n装饰器@memoize 会存储每次函数调用时的结果及参数，那么之后的在调用，就不用重复计算。而我们可以只把精力放在上层逻辑，而不是更关注性能和时间（被解决了）\nfrom nltk import memoize @memoize def func4(n): if n==0: return [\"\"] elif n==1: return [\"S\"] else: s = [\"S\" + item for item in func4(n-1)] l = [\"L\" + item for item in func4(n-2)] return s+l print(func4(4))\n其他的应用\n这里主要介绍一下除了上述两种主要算法外，一些小的使用技巧和相关基础概念。\n词汇多样性\n词汇多样性主要取决于：平均词长（字母个数/每个单词）、平均句长（单词个数/每个句子）和文本中没歌词出现的次数。\nfrom nltk.corpus import gutenberg for fileid in gutenberg.fileids(): num_chars = len(gutenberg.raw(fileid)) num_words = len(gutenberg.words(fileid)) num_sents = len(gutenberg.sents(fileid)) num_vocab = len(set(w.lower() for w in gutenberg.words(fileid))) print(int(num_chars/num_words),int(num_words/num_sents),int(num_words/num_vocab),'from',fileid)\n文体差异性\n文体差异性可以体现在很多方面：动词、情态动词、名词等等。这里我们以情态动词为例，来分析常见情态动词的在不同文本的差别。\nfrom nltk.corpus import brown from nltk import FreqDist,ConditionalFreqDist cfd = ConditionalFreqDist(( genere,word) for genere in brown.categories() for word in brown.words(categories=genere)) genres=['news','religion','hobbies'] models = ['can','could','will','may','might','must'] cfd.tabulate(conditions = genres,samples=models)\n随机语句生成\n从《创世纪》中得到所有的双连词，根据概率分布，来判断哪些词最有可能跟在给定词后面。\nimport nltk def create_sentence(cfd,word,num=15): for i in range(num): print(word,end=\" \") word = cfd[word].max()#查找word最有可能的后缀 text= nltk.corpus.genesis.words(\"english-kjv.txt\") bigrams = nltk.bigrams(text) cfd = nltk.ConditionalFreqDist(bigrams) print(create_sentence(cfd,'living'))\n词谜问题解决\n单词长度\u003e=3,并且一定有r,且只能出现’egivrvonl’中的字母。\npuzzle_word = nltk.FreqDist('egivrvonl') base_word = 'r' wordlist = nltk.corpus.words.words() result = [w for w in wordlist if len(w)\u003e=3 and base_word in w and nltk.FreqDist(w)\u003c=puzzle_word] #通过FreqDist比较法（比较键对应的value），来完成字母只出现一次的要求！！！ print(result)\n时间和空间权衡:全文检索系统\n除了研究算法，分析内部实现外。构造辅助数据结构，可以显著加快程序执行。\nimport nltk def raw(file): contents = open(file).read() return str(contents) def snippet(doc,term):#查找doc中term的定位 text = ' '*30+raw(doc)+' '*30 pos = text.index(term) return text[pos-30:pos+30] files = nltk.corpus.movie_reviews.abspaths() idx = nltk.Index((w,f) for f in files for w in raw(f).split()) #注意nltk.Index格式 query = 'tem' while query!='quit' and query: query = input('\u003e\u003e\u003e input the word:') if query in idx: for doc in idx[query]: print(snippet(doc,query)) else: print('Not found')\n欢迎进一步交流本博文相关内容：\n博客园地址 : http://www.cnblogs.com/AsuraDong/\nCSDN地址 : http://blog.csdn.net/asuradong\n也可以致信进行交流 : xiaochiyijiu@163.com\n欢迎转载 , 但请指明出处  :  )","date":"2017年06月12日 21:34:26"}
{"_id":{"$oid":"5d36a8d66734bd8e681d5ea5"},"title":"自然语言处理相关书籍","author":"赵志雄","content":"这里推荐一批学习自然语言处理相关的书籍，当然，不止是自然语言处理，国内的书籍相对比较便宜，值得购买。\n1、《自然语言处理综论》，当年的入门书，不过翻译的是第一版，英文名《Speech and Language Processing\u003e, 第三版据说很快就要出版（2016年），有条件的同学建议直接看英文版第二版。\n2、《统计自然语言处理基础》，另一本入门书籍，这本书的英文版貌似没有更新，但是中文版貌似也不再发售了，当然，优先推荐读英文版。\n3、《Python自然语言处理》，NLTK配套丛书，有了上面两本书的介绍，再加上一些Python基础，通过这本书进行相关的文本挖掘实战，很不错的一个路径。\n4、宗成庆老师的《统计自然语言处理（第2版）》，当年读书的时候大致看过第一版，作为入门书籍不错。\n5、国内青年学者刘知远老师等合著的《互联网时代的机器学习和自然语言处理技术大数据智能》，没有仔细看过，仅供参考。\n6、南大周志华老师的西瓜书《机器学习》，最近出版的书籍，国内难得学习机器学习的高质量书籍，评价非常高，强烈推荐。\n7、CMU机器学习系主任Tom Mitchell院士的 《机器学习》，机器学习老牌经典书籍，历久弥新。\n华章引进的英文版也不贵，不过貌似没货：《机器学习（英文版》\n8、比较新的一本机器学习书籍，被誉为内容全面的机器学习教程 Machine Learning期刊主编力作：《机器学习》\n9、李航老师的这本《统计学习基础》挺不错的，简洁明了：《统计学习基础》\n10、王斌老师翻译的《大数据 互联网大规模数据挖掘与分布式处理（第2版）》，质量挺不错的，对应的英文书籍是《Mining of Massive Datasets》，有相应的官方主页，提供相应的英文PDF，课程和课件资源。","date":"2017年05月30日 18:37:54"}
{"_id":{"$oid":"5d36a8d76734bd8e681d5eaa"},"title":"自然语言处理入门知识","author":"寒暑假很重要啊","content":"1.《数学之美》吴军 这个书写得特别生动形象，没有太多公式，科普性质。看完对于nlp的许多技术原理都会有初步认识。可以说是自然语言处理最好的入门读物。链接: https://pan.baidu.com/s/1eSphCSa 密码: 59je.\n2.如何在NLP领域第一次做成一件事 by 周明 微软亚洲研究院首席研究员、自然语言处理顶会ACL候任主席,http://www.msra.cn/zh-cn/news/features/nlp-20161124\n3.深度学习基础 by 邱锡鹏 复旦大学 2017年8月17日, 206页PPT带你全面梳理深度学习要点。http://nlp.fudan.edu.cn/xpqiu/slides/20170817-CIPS-ATT-DL.pdf,https://nndl.github.io/\n4.Deep learning for natural language processing 自然语言处理中的深度学习 by 邱锡鹏\n主要讨论了深度学习在自然语言处理中的应用。其中涉及的模型主要有卷积神经网络，递归神经网络，循环神经网络网络等，应用领域主要包括了文本生成，问答系统，机器翻译以及文本匹配等。http://nlp.fudan.edu.cn/xpqiu/slides/20160618_DL4NLP@CityU.pdf\n5.Deep Learning, NLP, and Representations （深度学习，自然语言处理及其表达)来自著名的colah’s blog，简要概述了DL应用于NLP的研究，重点介绍了Word Embeddings。http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/ 翻译： http://blog.csdn.net/ycheng_sjtu/article/details/48520293\\\n6.《中文信息发展报告》 by 中国中文信息学会 2016年12月是一份非常好的中文NLP总览性质的文档，通过这份报告可以了解中文和英文NLP主要的技术方向。链接：http://cips-upload.bj.bcebos.com/cips2016.pdf\n7.Deep Learning in NLP （一）词向量和语言模型 by Lai Siwei(来斯惟) 中科院自动化所 2013比较详细的介绍了DL在NLP领域的研究成果，系统地梳理了各种神经网络语言模型。链接：http://licstar.net/archives/328\n8.语义分析的一些方法(一，二，三) by 火光摇曳 腾讯广点通 链接：http://www.flickering.cn/ads/2015/02/\n9.我们是这样理解语言的-3 神经网络语言模型 by 火光摇曳 腾讯广点通 总结了词向量和常见的几种神经网络语言模型。链接：http://www.flickering.cn/nlp/2015/03/\n10.深度学习word2vec笔记之基础篇 by falao_beiliu http://blog.csdn.net/mytestmy/article/details/26961315\n11.Understanding Convolutional Neural Networks for NLP 卷积神经网络在自然语言处理的应用 by WILDML 链接：http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp 翻译：http://www.csdn.net/article/2015-11-11/2826192\n12.The Unreasonable Effectiveness of Recurrent Neural Networks. 循环神经网络惊人的有效性 by Andrej Karpathy 链接：http://karpathy.github.io/2015/05/21/rnn-effectiveness/ 翻译：https://zhuanlan.zhihu.com/p/22107715\n13.Understanding LSTM Networks 理解长短期记忆网络（LSTM NetWorks） by colah 链接：http://colah.github.io/posts/2015-08-Understanding-LSTMs/ 翻译：http://www.csdn.net/article/2015-11-25/2826323?ref=myread\n14.注意力机制（Attention Mechanism）在自然语言处理中的应用 by robert_ai 链接：http://www.cnblogs.com/robert-dlut/p/5952032.html\n15.初学者如何查阅自然语言处理（NLP）领域学术资料 刘知远 链接：http://blog.sina.com.cn/s/blog_574a437f01019poo.html\\","date":"2017年11月24日 20:07:26"}
{"_id":{"$oid":"5d36a8d76734bd8e681d5eac"},"title":"自然语言处理学习笔记6：向量距离之高级的词向量表示","author":"腾阳","content":"斯坦福大学在三月份开设了一门“深度学习与自然语言处理”的课程：CS224d: Deep Learning for Natural Language Processing，授课老师是青年才俊 Richard Socher，以下为相关的课程笔记。\n第三讲：高级的词向量表示（Advanced word vector representations: language models, softmax, single layer networks）\n推荐阅读材料：\nPaper1：[GloVe: Global Vectors for Word Representation]\nPaper2：[Improving Word Representations via Global Context and Multiple Word Prototypes]\nNotes：[Lecture Notes 2]\n第三讲Slides [slides]\n第三讲视频 [video]\n以下是第三讲的相关笔记，主要参考自课程的slides，视频和其他相关资料。\n\n回顾：简单的word2vec模型\n代价函数J\n其中的概率函数定义为：\n我们主要从内部向量(internal vector)$v_{w_I}$导出梯度\n计算所有的梯度\n我们需要遍历每一个窗口内的中心向量(center vector)的梯度\n我们同时需要每一个外部向量（external vectors)$v^'$的梯度\n通常的话在每一个窗口内我们将更新计算所有用到的参数\n例如在一个窗口长度为1的句子里：I like learning\n第一个窗口里计算的梯度包括:内部向量$v_{like}$, 外部向量$v^'_I$及$v^'_{learning}$\n同理更新计算句子的下一个窗口里的参数\n计算所有的向量梯度\n我们经常在一个模型里把所有的参数集合都定义在一个长的向量$\\theta$里\n在我们的例子里是一个d维度的向量及一个长度为V的词汇集：\n梯度下降\n要在整个训练集上最小化代价函数$J(\\theta)$需要计算所有窗口里的参数梯度\n对于参数向量$\\theta$来说需要更新其中每一个元素:\n这里$\\alpha$是步长从矩阵的角度来看参数更新:\n梯度下降相关代码\n随机梯度下降(SGD)\n对于上述梯度下降的方法，训练集语料库有可能有400亿（40B）的token和窗口\n一轮迭代更新需要等待很长的时间\n对于非常多的神经网络节点来说这不是一个好方法\n所以这里我们使用随机梯度下降（SGD）：在每一个窗口计算完毕后更新所有的参数\n词向量的随机梯度下降\n但是在每一个窗口里，我们仅有2c-1个词，这样的话$\\delta_{\\theta}J_t(\\theta)$非常稀疏\n我们也许仅仅应该只更新那些确实存在的词向量解决方案：或者保留词向量的哈稀或者更新词嵌入矩阵L和$L^'$的固定列\n很重要的一点是如果你有上百万个词向量并且在做分布式训练的话就不需要发送大量的更新信息了\nPSet1\n归一化因子的计算代价很大\n因此在PSet1你们将实现skip-gram模型主要的思路：对一对实际的词对（一个中心词及一个窗口内的其他词）和一些随机的词对（一个中心词及一个随机词）训练二元逻辑回归模型\nPSet1: The skip-gram model and negative sampling\n来源论文：Distributed Representations of Words and Phrases and their Compositionality（Mikolov et al. 2013）\n这里k是我们所使用的负例采样(negative sampling)的样本数Sigmoid函数：\n\n\n所以我们最大化第一个log处两个词的共现概率更进一步比较清晰的公式：\n最大化在中心词周边真实词对的概率；最小化中心词周边随机词对的概率\n这里unigram分布U(w)被赋予了3/4幂次方，这样可以保证一些出现比较少的词可以被尽可能多的抽样\nWhat to do with the two sets of vectors?\n我们从所有的向量v和$v^'$中得到了L和$L^'$\n这两个都获得了相似的共现信息，如何有效的利用着两个向量集？一个简单有效的方法，对它们进行加和\n在GloVe中对许多超参数进行了探究: Global Vectors for Word Representation (Pennington et al. (2014))\n如何评测词向量\n和一般的NLP评测任务相似：内部 vs 外部(Intrinsic vs extrinsic)\n内部评测：\n在一个特定的子任务中进行评测\n计算迅速\n有助于理解相关的系统\n不太清楚是否有助于真实任务除非和实际的NLP任务的相关性已经建立起来\n外部评测：\n在一个真实任务中进行评测\n需要花很长的实际来计算精度\n不太清楚是否是这个子系统或者其他子系统引起的问题\n如果用这个子系统替换原有的系统后获得精度提升--\u003e有效(Winning!)\n词向量的内部评测：\n词向量类比:语法和语义\n通过评测模型在一些语义或语法类比问题上的余弦相似度距离的表现来评测词向量去除一些来自于搜索的输入词问题：如果信息符合但不是线性的怎么办?\n词向量的内部评测例一\n词向量类比：以下语法和语义例子来源于：https://code.google.com/p/word2vec/source/browse/trunk/questions-words.txt\n存在的问题：不同的城市可能存在相同的名字\n词向量的内部评测例二\n词向量类比：以下语法和语义例子来源于：https://code.google.com/p/word2vec/source/browse/trunk/questions-words.txt\n词向量的内部评测例三\n词向量类比：以下语法和语义例子来源于：https://code.google.com/p/word2vec/source/browse/trunk/questions-words.txt\n词向量的内部评测例四\n词向量类比：以下语法和语义例子来源于：https://code.google.com/p/word2vec/source/browse/trunk/questions-words.txt\n类比评测和超参数\n目前为止最细致的评测: GloVe 词向量\n非对称上下文（仅有左侧的单词）并不是很好\n最佳的向量维度：300左右，之后变化比较轻微但是对于不同的“下游”任务来说最佳的维度也会不同对于GloVe向量来说最佳的窗口长度是8训练的时间约长是否有帮助：对于GloVe来说确实有助于\n更多的数据是否有帮助？维基百科的数据比新闻数据更相关\n词向量的内部评价\n评测任务：词向量距离以及和人工评价的相关性\n评测集：WordSim353(http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/)\n相关性评测结果：\n如何应对歧义问题（But what about ambiguity?）\n也许你寄希望于一个词向量能捕获所有的语义信息（例如run即是动车也是名词），但是这样的话词向量会被辣向两个方向\n这篇论文对此有相应的描述：Improving Word Representations Via Global Context And Multiple Word Prototypes(Huang et al. 2012)\n解决思路：对词窗口进行聚类，并对每个单词词保留聚类标签，例如$bank_1$, $bank_2$等\n词向量的外部评价\n一个例子NER(named entity recognition)：好的词向量会对实际任务有直接的帮助\n命名实体识别(NER)：找到人名，地名和机构名\n下一步：如何在神经网络模型中使用词向量\n简单的单个词的分类问题\n从深度学习的词向量中最大的获益是什么？\n有能力对单词进行精确的分类\n国家类的单词可以聚和到一起--\u003e因此可以通过词向量将地名类的单词区分出来\n可以在其他的任务中将单词的任意信息融合进来\n可以将情感分析（Sentiment)问题映射到单词分类中：在语料库中寻找最具代表性的正/负例单词\nThe Softmax\n逻辑回归 = Softmax分类在给定词向量x的情况下获得y类的概率\nThe Softmax - 细节\n术语：损失函数（Loss function) = 代价函数(Cost function) = 目标函数（Objective function)\nSoftmax的损失（Loss): 交叉熵（Cross Entropy)\n如何计算p(y|x): 取W的$y^'$行乘以含x的行\n计算所有的$f_c$, c = 1, 2, ... , C归一化计算Softmax函数的概率\nSoftmax和交叉熵误差\n目标是最大化正确分类y的概率\n因此，我们可以最小化改函数负的对数概率\n因此，如果有多个类别我们可以在总的误差函数中叠加多个交叉熵误差\n背景：交叉熵 \u0026 KL散度\n假设分布是：p = [0,...,0,1,0,...0], 对应计算的概率分布是q，则交叉熵是：\n因为p是one-hot的缘故，则上述公式剩余的则是真实类的负的对数概率交叉熵可以写成熵和两个分布的KL散度之和\n在我们的case里p是0（即使不是0也会因为它是固定的对梯度没有固定），最小化交叉熵等价于最小化KL散度KL散度并非是一个距离函数而是一个对于两个概率分布差异的非对称的度量\n维基百科：KL散度是两个概率分布P和Q差别的非对称性的度量。 KL散度是用来 度量使用基于Q的编码来编码来自P的样本平均所需的额外的位元数。 典型情况下，P表示数据的真实分布，Q表示数据的理论分布，模型分布，或P的近似分布。\n简单的单个单词分类\n例子：情感分析\n两个选择：仅仅训练softmax权重W或者同时训练词向量\n问题：训练词向量的优点和缺点是什么\nPro: 更好的适应训练数据\nCon: 更差的泛化能力\n训练的词向量的情感分享可视化\n\n继续“打怪升级”：窗口分类（Window classification)\n单个的单词没有上下文信息\n通过对窗口中的中心词进行分类同时考虑窗口中的上下文\n可能性：Softmax 和 交叉熵误差 或者 最大边界损失（max-margin loss）\n我们将在下一讲中探索这些问题(next class)\n注：原创文章，转载请注明出处及保留链接“我爱自然语言处理”：http://www.52nlp.cn\n本文链接地址：斯坦福大学深度学习与自然语言处理第三讲：高级的词向量表示\n相关文章:\n斯坦福大学深度学习与自然语言处理第二讲：词向量\n斯坦福大学深度学习与自然语言处理第四讲：词窗口分类和神经网络\n斯坦福大学深度学习与自然语言处理第一讲：引言\n维基百科语料中的词语相似度探索\n自然语言处理工具包spaCy介绍\n中英文维基百科语料上的Word2Vec实验\nCoursera公开课笔记: 斯坦福大学机器学习第七课“正则化(Regularization)”\nPRML读书会第五章 Neural Networks\nAndrew Ng 深度学习课程小记\nCoursera公开课笔记: 斯坦福大学机器学习第二课“单变量线性回归(Linear regression with one variable)”\n本条目发布于2015年07月15号。属于机器学习、深度学习、自然语言处理分类，被贴了 Deep Learning、Deep Learning公开课、Deep NLP、DL、glove、KL散度、KL距离、NER、Richard Socher、SGD、softmax、word vectors、word2vec、wordnet、交叉熵、公开课、共现矩阵、单词分类、命名实体识别、情感分析、斯坦福大学、机器学习、梯度下降、深度学习、深度学习与自然语言处理、深度学习技术、深度学习模型、神经网络、自然语义处理、自然语言处理、计算机视觉、词向量、词向量评测、词嵌入、语义词典、随机梯度下降 标签。作者是52nlp。","date":"2018年06月02日 21:46:41"}
{"_id":{"$oid":"5d36a8d86734bd8e681d5eae"},"title":"深度学习下的自然语言处理综述（一）","author":"baohuaii","content":"摘要\n深度学习方法使用多个处理层来学习数据的层次表示，并在许多领域产生了最先进的结果。近年来，在自然语言处理(NLP)的背景下，各种模型设计和方法得到了蓬勃发展。在这篇综述中，我们回顾了许多NLP任务中所使用的重要的深度学习相关模型和方法。在综述中，我们提到了序列生成方法，神经机器翻译，对话系统的模型，并叙述了它们的发展过程。与此我们还对各种模型进行了总结、比较和对比，并对NLP中深度学习的过去、现在和未来进行了详细的了解。同时，我们也深入探讨了现在主流的几种算法。\n关键词：深度学习，自然语言处理，序列生成技术，机器翻译，对话系统\n1．研究背景及意义\n深度学习体系结构和算法在计算机视觉和模式识别等领域已经取得了令人瞩目的进展。随着这一趋势，近年来的关于NLP研究越来越多地使用新的深度学习方法。针对NLP问题的机器学习方法是基于浅层模型的（例SVM和逻辑回归），大多数的语言信息用稀疏表示（高维特征）表示，这导致诸如维数灾难之类的问题。近年来，基于密集向量表示的神经网络在各种NLP任务中取得了较好的效果。这一趋势是由词嵌入和深度学习方法的成功引发的。深度学习可以实现多层次的自动特征表示学习。相比之下，传统的基于机器学习的NLP系统在很大程度上依赖于手工构建的特征。这种手工构建的特征常耗大量的时间和成本，并且不同任务所需要的特征又是不同的。\n自然语言处理(NLP)是一种基于理论的计算技术，用于人类语言的自动分析和表达。NLP的研究已经从打卡和批处理的时代(一个句子的分析时间可以长达7分钟)发展过来，到现在可以在不到一秒的时间内可以处理数百万个网页。NLP使计算机能够在所有级别上执行广泛的自然语言相关任务，从解析和词性标记到机器翻译和对话系统。\n自然语言处理（NLP）通过计算技术学习、理解然后产生人类语言。NLP中有许多有意义的研究方向。其中包括序列生成，机器翻译，对话系统。NLP还有一些其他的主题。计算机视觉与NLP的集成，如视觉字幕、视觉对话、视觉关系和属性检测。\n在过去的几年中，深度学习（DL）架构和算法在图像识别和语音处理等领域取得了令人瞩目的进步。它们在自然语言处理（NLP）中的应用起初并不那么令人印象深刻，但现在已经证明可以做出重大贡献，为一些常见的NLP任务提供最先进的结果。命名实体识别（NER），词性（POS）标记或情感分析是神经网络模型优于传统方法的一些问题。机器翻译的进步可能是最引人注目的。\nR. Collobert 等人[1]，证实了一个简单的深度学习框架在几个NLP任务(如命名实体识别(NER)、语义角色标注(SRL)和词性标注（POS），具体分析见附录)方面的性能优于最先进的方法。此后，针对复杂的NLP任务，提出了大量基于深度学习的复杂算法。这其中包括应用于自然语言任务的主要深度学习相关模型和方法，如卷积神经网络(CNNs)、循环神经网络(RNNs)和递归神经网络。还包括增强记忆策略、注意力机制以及无监督模型、强化学习方法以及最近的深度生成模型是如何应用于与语言相关任务的。\n2．当前研究现状\n在过去几年中，深度学习改变了整个景观。每天，有更多的应用程序依赖于医疗保健，金融，人力资源，零售，地震检测和自动驾驶汽车等领域的深度学习技术。至于现有的应用，结果一直在稳步提高。\n在学术层面，机器学习领域变得如此重要，以至于每20分钟就会出现一篇新的科学文章。\n而且，深度学习已经渗透到NLP的许多子领域，并帮助取得重大进展。对于深度学习算法和非深度学习算法，以及基于无领域知识的方法和基于语言学知识的方法，NLP似乎仍然是一个关于协同而不是竞争的领域。一些非深度学习算法是有效且表现良好的，如word2vec 和fastText等。\n在2018年，深度学习也取得了一些主要进展。但最受关注的是Google AI语言团队发表了一篇BERT语言模型论文[1]。在自然语言处理（NLP）中，语言模型是可以估计一组语言单元（通常是单词序列）的概率分布的模型。这些是有趣的模型，因为它们可以以很低的成本构建，并且显著改进了几个NLP任务，例如机器翻译，语音识别和解析。之前，最著名的方法是马尔可夫模型和n-gram模型。随着深度学习的出现，出现了一些基于长短期记忆网络（LSTM）的更强大的模型。虽然高效，但现有模型通常是单向的，这意味着只有单词的左（或右）的有序序列才会被考虑。但在18年10月份，Google AI语言团队提出的深度双向变换器模型，不仅解决了单向模型的问题。而且该模型可实现11种NLP任务的最先进性能，包括斯坦福问答（SQUAD）数据集。\n强化学习也在自然语言处理上取得了巨大的进步，因为目前的自然语言处理大多都是一个离散空间的自然语言处理、生成或者是序列决策，这时，我们很天然地可以利用到强化学习去拟合和运作。强化学习下的几种分类在自然语言处理的任务中，也都取得了非常优异的成果。例如value-based RL（基于价值函数），policy-based RL（基于策略的函数），model-based RL（基于模型的函数）。\n2018年，在各种领域被搜索的最多的词条是自然语言处理（NLP）和生成对抗网络。在Github上最流行的生成对抗网络方法包括：vid2vid, DeOldify, CycleGAN and faceswaps。最流行的NLP方法包括：BERT, HanLP, jieba, AllenNLP and fastText。\n\n3.基本理论分析\n3.1 序列生成技术\n序列可以采用文本、音乐、分子等形式。序列生成技术可以应用于多个领域，包括与音乐旋律有关的实验和计算分子生成[4]。本节主要研究文本生成问题，因为文本生成是会话响应生成、机器翻译、抽象摘要等许多NLP问题的基础。\n最开始，文本生成模型通常基于n-gram、前馈神经网络或递归神经网络，训练它们根据前面的基础真词作为输入，预测下一个单词，生成的模型用于整个序列；然后在测试中，使用训练的模型逐字生成序列，并将生成的单词作为输入。再对这些模型进行词级损失的训练，如交叉熵，使下一个词的概率最大化。不过这些模型在用于生成文本时存在两个主要缺陷。首先，他们被训练预测下一个单词时，给定之前的基础真词作为输入。但是，在测试时，通过每次预测一个单词，并在下一次步骤中将生成的单词作为输入返回，生成的模型用于生成整个序列。这个过程非常脆弱，因为模型是在不同的输入分布上进行训练的，即从数据分布中提取的单词，而不是从模型分布中提取的单词。因此，在此过程中所犯的错误将迅速累积。我们把这种差异称为暴露偏差，即一个模型只暴露于训练数据的分配，而不是它自己的预测。其次，用于训练这些模型的损失函数是字级的。一个流行的选择是交叉熵损失，用来最大化下一个正确单词的概率。然而，这些模型的性能通常使用离散的度量来评估。\n针对这一问题，现在有一种新的序列级训练算法[5]，为混合增量交叉熵增强（MIXER），因为它结合了XENT和增强，还有增量学习。第一个关键思想是改变增强的初始策略，以确保模型能够有效地处理文本生成的巨大操作空间。而不是从一个糟糕的随机策略开始训练模型收敛到最优策略，作者从最优策略开始，然后慢慢偏离它，让模型去探索和利用它自己的预测。第二个关键思想为在退火过程中引入模型预测，以逐步教会模型生成稳定的序列。\n这种方法可以直接优化测试时使用的度量，如BLEU或ROUGE。在三个不同的任务上，方法优于贪婪生成的几个强大基线。当这些基线使用波束搜索时，这种方法也很有竞争力，而且速度要快几倍。\nBahdanau et al.(2017)提出了一种序列预测的actor-critic算法[6]，以改进序列级训练算法[5]。作者利用一个评估网络来预测一个token的值，即，序列预测策略下的期望得分，由行动者网络定义，通过训练来预测tokens的值。采用了一些技术来提高性能:用SARSA而不是蒙特卡罗方法来减少估计值函数的方差;稳定的目标网络;这种训练神经网络的方法是使用来自强化学习的 actor-critic 方法来生成序列。结果显示，这种方法提升了在合成任务（synthetic task）以及德英机器翻译任务上的表现。作者的分析为这样的方法在自然语言生成任务上的应用铺平了道路，比如机器翻译、图片描述生成、对话模型。\nBahdanau提出的论文有两个重要的贡献，首先它描述了强化学习中像 actor-critic 方法这样的方法能被应用于带有结构化输出的监督学习问题上，然后调查了新方法在合成任务以及机器翻译这样的真实世界任务上的表现与行为，展示了由 actor- critic 带来的在最大似然方法以及 REINFORCE 方法上的改进。\n生成对抗网(GAN)是一种新的生成模型训练方法，它利用判别模型来指导生成模型的训练，在生成实际数据方面取得了很大的成功。GAN 网络在计算机视觉上已经得到了很好的应用，然而，其在自然语言处理上并不是很有效，最初的 GAN 仅仅定义在实数领域，GAN 通过训练出的生成器来产生合成数据，然后在合成数据上运行判别器，判别器的输出梯度将会告诉你，如何通过略微改变合成数据而使其更加现实。然而，当目标是生成离散tokens序列时，它有局限性。主要原因是生成模型的离散输出使得判别模型的梯度更新难以传递到生成模型。另外，判别模型只能对一个完整的序列进行评估，而对于一个部分生成的序列，一旦生成了整个序列，要平衡其当前的分数和未来的分数是很重要的。\n例如，如果你输出了一张图片，其像素值是1.0，那么接下来你可以将这个值改为1.0001。如果输出了一个单词“penguin”，那么接下来就不能将其改变为“penguin + .001”，因为没有“penguin +.001”这个单词。 因为所有的自然语言处理（NLP）的基础都是离散值，如“单词”、“字母”或者“音节”，NLP 中应用 GANs 是非常困难的。\n随后，Yu等人也提出了SeqGAN，解决了上述问题。即具有策略梯度的序列生成式对抗网络[7]，来解决这些问题，将数据生成器建模为强化学习(RL)中的随机策略，SeqGAN通过直接执行梯度策略更新来绕过生成器的微分问题。RL激励信号来自基于完整序列判断的GAN 鉴别器，并通过蒙特卡罗搜索返回到中间状态动作步骤。在合成数据和真实任务上的大量实验表明，与强大的基线相比，它们有了显著的改进。SeqGAN的结构图，如图3.1所示\n\n右:D是训练完毕真实数据和G是生成的数据。左:G是受过训练根据提供最终激励信号的政策梯度，并通过它传递回中间操作值蒙特卡罗搜索。\n参考文献\n[1].R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa, “Natural language processing (almost)from scratch,” Journal of Machine Learning Research, vol. 12, no. Aug, pp. 2493–2537, 2011.\n[2]. Devlin J, Chang M W, Lee K, et al. Bert: Pre-training of deep bidirectional transformers for language understanding[J]. arXiv preprint arXiv:1810.04805, 2018.\n[3]Young T, Hazarika D, Poria S, et al. Recent trends in deep learning based natural language processing[J]. ieee Computational intelligenCe magazine, 2018, 13(3): 55-75.\n[4]Jaques N, Gu S, Bahdanau D, et al. Sequence tutor: Conservative fine-tuning of sequence generation models with kl-control[J]. arXiv preprint arXiv:1611.02796, 2016.\n[5] Ranzato M A, Chopra S, Auli M, et al. Sequence level training with recurrent neural networks[J]. arXiv preprint arXiv:1511.06732, 2015.\n[6] Bahdanau D, Brakel P, Xu K, et al. An actor-critic algorithm for sequence prediction[J]. arXiv preprint arXiv:1607.07086, 2016\n[7] Yu L, Zhang W, Wang J, et al. SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient[C]//AAAI. 2017: 2852-2858.","date":"2019年01月25日 17:10:56"}
{"_id":{"$oid":"5d36a8d86734bd8e681d5eb0"},"title":"【自然语言处理】英文开源工具汇总（）","author":"qq280929090","content":"本博客主要是对网络上的一些关于英文自然语言处理开源工具的博客进行整理、汇总，如果有涉及到您的知识产权等，请联系本人已进行修改，也欢迎广大读者进行指正以及补充。\n\n\n本博客将尽量从工具的使用语言、功能等方面进行汇总介绍。\n1 斯坦福大学\n语言：Java\n功能：分词、词性标注、命名实体识别、语法解析、分类。\n介绍：Stanford NLP Group是斯坦福大学自然语言处理的团队，开发了多个NLP工具，官网网址。由于该团队将该开源分为多个子模块，以下将逐一进行介绍。\n1.1 Stanford Word Segmenter\n介绍：采用CRF（条件随机场）算法进行分词，也是基于Java开发的，同时可以支持中文和Arabic，官方要求Java版本1.6以上，推荐内存至少1G。下载地址。\n\n\n示例代码：\n[java] view plain copy\n//设置分词器属性。\nProperties props = new Properties();\n//字典文件地址，可以用绝对路径，如d:/data\nprops.setProperty(\"sighanCorporaDict\", \"data\");\n//字典压缩包地址，可以用绝对路径\nprops.setProperty(\"serDictionary\",\"data/dict-chris6.ser.gz\");\n//输入文字的编码；\nprops.setProperty(\"inputEncoding\", \"UTF-8\");\nprops.setProperty(\"sighanPostProcessing\", \"true\");\n//初始化分词器，\nCRFClassifier classifier = new CRFClassifier(props);\n//从持久化文件中加载分词器设置；\nclassifier.loadClassifierNoExceptions(\"data/ctb.gz\", props);\n// flags must be re-set after data is loaded\nclassifier.flags.setProperties(props);\n//分词\nList words = classifier.segmentString(\"语句内容\");\n1.2 Stanford POS Tagger\n\n介绍：采用Java编写的面向英文、中文、法语、阿拉伯语、德语的命名实体识别工具，下载地址。\n1.3 Stanford Named Entity Recognizer\n介绍：采用条件随机场模型的命名实体工具，下载地址。\n1.4 Stanford Parser\n介绍：进行语法分析的工具，支持英文、中文、阿拉伯文和法语。下载地址。\n1.5 Stanford Classifier\n介绍：采用Java编写的分类器，下载地址。\n最后附上关于中文分词器性能比较的一篇文章(2014.05.27更新)\n1.6 Stanford CoreNLP\n功能：分词、词性标注、命名实体识别、语法分析\n介绍：采用Java编写的面向英文的处理工具，下载网址。\n用户评价：采用它进行英语单词的词性还原，具体应用详见文章《采用Stanford CoreNLP实现英文单词词形还原》。","date":"2018年03月24日 10:17:06"}
{"_id":{"$oid":"5d36a8d86734bd8e681d5eb2"},"title":"自然语言处理6 -- 情感分析","author":"谢杨易","content":"系列文章，请多关注\nTensorflow源码解析1 – 内核架构和源码结构\n带你深入AI（1） - 深度学习模型训练痛点及解决方法\n自然语言处理1 – 分词\n自然语言处理2 – jieba分词用法及原理\n自然语言处理3 – 词性标注\n自然语言处理4 – 句法分析\n自然语言处理5 – 词向量\n自然语言处理6 – 情感分析\n1 概述\n情感分析是自然语言处理中常见的场景，比如淘宝商品评价，饿了么外卖评价等，对于指导产品更新迭代具有关键性作用。通过情感分析，可以挖掘产品在各个维度的优劣，从而明确如何改进产品。比如对外卖评价，可以分析菜品口味、送达时间、送餐态度、菜品丰富度等多个维度的用户情感指数，从而从各个维度上改进外卖服务。\n情感分析可以采用基于情感词典的传统方法，也可以采用基于深度学习的方法，下面详细讲解\n2 基于情感词典的传统方法\n2.1 基于词典的情感分类步骤\n基于情感词典的方法，先对文本进行分词和停用词处理等预处理，再利用先构建好的情感词典，对文本进行字符串匹配，从而挖掘正面和负面信息。如下图\n2.2 情感词典\n情感词典包含正面词语词典、负面词语词典、否定词语词典、程度副词词典等四部分。如下图\n词典包含两部分，词语和权重，如下\n正面： 很快 1.75 挺快 1.75 还好 1.2 很萌 1.75 服务到位 1 负面： 无语 2 醉了 2 没法吃 2 不好 2 太差 5 太油 2.5 有些油 1 咸 1 一般 0.5 程度副词： 超级 2 超 2 都 1.75 还 1.5 实在 1.75 否定词： 不 1 没 1 无 1 非 1 莫 1 弗 1 毋 1\n情感词典在整个情感分析中至关重要，所幸现在有很多开源的情感词典，如BosonNLP情感词典，它是基于微博、新闻、论坛等数据来源构建的情感词典，以及知网情感词典等。当然我们也可以通过语料来自己训练情感词典。\n2.3 情感词典文本匹配算法\n基于词典的文本匹配算法相对简单。逐个遍历分词后的语句中的词语，如果词语命中词典，则进行相应权重的处理。正面词权重为加法，负面词权重为减法，否定词权重取相反数，程度副词权重则和它修饰的词语权重相乘。如下图\n利用最终输出的权重值，就可以区分是正面、负面还是中性情感了。\n2.4 缺点\n基于词典的情感分类，简单易行，而且通用性也能够得到保障。但仍然有很多不足\n精度不高。语言是一个高度复杂的东西，采用简单的线性叠加显然会造成很大的精度损失。词语权重同样不是一成不变的，而且也难以做到准确。\n新词发现。对于新的情感词，比如给力，牛逼等等，词典不一定能够覆盖\n词典构建难。基于词典的情感分类，核心在于情感词典。而情感词典的构建需要有较强的背景知识，需要对语言有较深刻的理解，在分析外语方面会有很大限制。\n3 基于深度学习的算法\n近年来，深度学习在NLP领域内也是遍地开花。在情感分类领域，我们同样可以采用深度学习方法。基于深度学习的情感分类，具有精度高，通用性强，不需要情感词典等优点。\n3.1 基于深度学习的情感分类步骤\n基于深度学习的情感分类，首先对语句进行分词、停用词、简繁转换等预处理，然后进行词向量编码，然后利用LSTM或者GRU等RNN网络进行特征提取，最后通过全连接层和softmax输出每个分类的概率，从而得到情感分类。\n3.2 代码示例\n下面通过代码来讲解这个过程。下面是我周末写的，2018年AI Challenger细粒度用户评论情感分析比赛中的代码。项目数据来源于大众点评，训练数据10万条，验证1万条。分析大众点评用户评论中，关于交通，菜品，服务等20个维度的用户情感指数。分为正面、负面、中性和未提及四类。代码在验证集上，目前f1 socre可以达到0.62。\n3.2.1 分词和停用词预处理\n数据预处理都放在了PreProcessor类中，主函数是process。步骤如下\n读取原始csv文件，解析出原始语句和标注\n错别字，繁简体，拼音，语义不明确等词语的处理\nstop words停用词处理\n分词，采用jieba分词进行处理。分词这儿有个trick，由于分词后较多口语化的词语不在词向量中，所以对这部分词语从jieba中del掉，然后再进行分词。直到只有为数不多的词语不在词向量中为止。\n构建词向量到词语的映射，并对词语进行数字编码。这一步比较常规。\nclass PreProcessor(object): def __init__(self, filename, busi_name=\"location_traffic_convenience\"): self.filename = filename self.busi_name = busi_name self.embedding_dim = 256 # 读取词向量 embedding_file = \"./word_embedding/word2vec_wx\" self.word2vec_model = gensim.models.Word2Vec.load(embedding_file) # 读取原始csv文件 def read_csv_file(self): reload(sys) sys.setdefaultencoding('utf-8') print(\"after coding: \" + str(sys.getdefaultencoding())) data = pd.read_csv(self.filename, sep=',') x = data.content.values y = data[self.busi_name].values return x, y # todo 错别字处理，语义不明确词语处理，拼音繁体处理等 def correct_wrong_words(self, corpus): return corpus # 去掉停用词 def clean_stop_words(self, sentences): stop_words = None with open(\"./stop_words.txt\", \"r\") as f: stop_words = f.readlines() stop_words = [word.replace(\"\\n\", \"\") for word in stop_words] # stop words 替换 for i, line in enumerate(sentences): for word in stop_words: if word in line: line = line.replace(word, \"\") sentences[i] = line return sentences # 分词，将不在词向量中的jieba分词单独挑出来，他们不做分词 def get_words_after_jieba(self, sentences): # jieba分词 all_exclude_words = dict() while (1): words_after_jieba = [[w for w in jieba.cut(line) if w.strip()] for line in sentences] # 遍历不包含在word2vec中的word new_exclude_words = [] for line in words_after_jieba: for word in line: if word not in self.word2vec_model.wv.vocab and word not in all_exclude_words: all_exclude_words[word] = 1 new_exclude_words.append(word) elif word not in self.word2vec_model.wv.vocab: all_exclude_words[word] += 1 # 剩余未包含词小于阈值，返回分词结果，结束。否则添加到jieba del_word中，然后重新分词 if len(new_exclude_words) \u003c 10: print(\"length of not in w2v words: %d, words are:\" % len(new_exclude_words)) for word in new_exclude_words: print word, print(\"\\nall exclude words are: \") for word in all_exclude_words: if all_exclude_words[word] \u003e 5: print \"%s: %d,\" % (word, all_exclude_words[word]), return words_after_jieba else: for word in new_exclude_words: jieba.del_word(word) raise Exception(\"get_words_after_jieba error\") # 去除不在词向量中的词 def remove_words_not_in_embedding(self, corpus): for i, sentence in enumerate(corpus): for word in sentence: if word not in self.word2vec_model.wv.vocab: sentence.remove(word) corpus[i] = sentence return corpus # 词向量，建立词语到词向量的映射 def form_embedding(self, corpus): # 1 读取词向量 w2v = dict(zip(self.word2vec_model.wv.index2word, self.word2vec_model.wv.syn0)) # 2 创建词语词典，从而知道文本中有多少词语 w2index = dict() # 词语为key，索引为value的字典 index = 1 for sentence in corpus: for word in sentence: if word not in w2index: w2index[word] = index index += 1 print(\"\\nlength of w2index is %d\" % len(w2index)) # 3 建立词语到词向量的映射 # embeddings = np.random.randn(len(w2index) + 1, self.embedding_dim) embeddings = np.zeros(shape=(len(w2index) + 1, self.embedding_dim), dtype=float) embeddings[0] = 0 # 未映射到的词语，全部赋值为0 n_not_in_w2v = 0 for word, index in w2index.items(): if word in self.word2vec_model.wv.vocab: embeddings[index] = w2v[word] else: print(\"not in w2v: %s\" % word) n_not_in_w2v += 1 print(\"words not in w2v count: %d\" % n_not_in_w2v) del self.word2vec_model, w2v # 4 语料从中文词映射为索引 x = [[w2index[word] for word in sentence] for sentence in corpus] return embeddings, x # 预处理，主函数 def process(self): # 读取原始文件 x, y = self.read_csv_file() # 错别字，繁简体，拼音，语义不明确，等的处理 x = self.correct_wrong_words(x) # stop words x = self.clean_stop_words(x) # 分词 x = self.get_words_after_jieba(x) # remove不在词向量中的词 x = self.remove_words_not_in_embedding(x) # 词向量到词语的映射 embeddings, x = self.form_embedding(x) # 打印 print(\"embeddings[1] is, \", embeddings[1]) print(\"corpus after index mapping is, \", x[0]) print(\"length of each line of corpus is, \", [len(line) for line in x]) return embeddings, x, y\n3.2.2 词向量编码\n词向量编码步骤主要有：\n加载词向量。词向量可以从网上下载或者自己训练。网上下载的词向量获取简单，但往往缺失特定场景的词语。比如大众点评菜品场景下的鱼香肉丝、干锅花菜等词语，而且往往这些词语在特定场景下还十分重要。而自己训练则需要几百G的语料，在高性能服务器上连续训练好几天，成本较高。可以将两种方法结合起来，也就是加载下载好的词向量，然后利用补充语料进行增量训练。\n建立词语到词向量的映射，也就是找到文本中每个词语的词向量\n对文本进行词向量编码，可以通过keras的Embedding函数，或者其他深度学习库来搞定。\n前两步在上面代码中已经展示了，词向量编码代码示例如下\nEmbedding(input_dim=len(embeddings), output_dim=len(embeddings[0]), weights=[embeddings], input_length=self.max_seq_length, trainable=False, name=embeddings_name))\n3.2.3 构建LSTM网络\nLSTM网络主要分为如下几层\n两层的LSTM。\ndropout，防止过拟合\n全连接，从而可以输出类别\nsoftmax，将类别归一化到[0, 1]之间\nLSTM网络是重中之重，这儿可以优化的空间很大。比如可以采用更优的双向LSTM，可以加入注意力机制。这两个trick都可以提高最终准确度。另外可以建立分词和不分词两种情况下的网络，最终通过concat合并。\nclass Model(object): def __init__(self, busi_name=\"location_traffic_convenience\"): self.max_seq_length = 100 self.lstm_size = 128 self.max_epochs = 10 self.batch_size = 128 self.busi_name = busi_name self.model_name = \"model/%s_seq%d_lstm%d_epochs%d.h5\" % (self.busi_name, self.max_seq_length, self.lstm_size, self.max_epochs) self.yaml_name = \"model/%s_seq%d_lstm%d_epochs%d.yml\" % (self.busi_name, self.max_seq_length, self.lstm_size, self.max_epochs) def split_train_data(self, x, y): x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.1) # 超长的部分设置为0，截断 x_train = sequence.pad_sequences(x_train, self.max_seq_length) x_val = sequence.pad_sequences(x_val, self.max_seq_length) # y弄成4分类，-2未提及，-1负面，0中性，1正面 y_train = keras.utils.to_categorical(y_train, num_classes=4) y_val = keras.utils.to_categorical(y_val, num_classes=4) return x_train, x_val, y_train, y_val def build_network(self, embeddings, embeddings_name): model = Sequential() model.add(Embedding(input_dim=len(embeddings), output_dim=len(embeddings[0]), weights=[embeddings], input_length=self.max_seq_length, trainable=False, name=embeddings_name)) model.add(LSTM(units=self.lstm_size, activation='tanh', return_sequences=True, name='lstm1')) model.add(LSTM(units=self.lstm_size, activation='tanh', name='lstm2')) model.add(Dropout(0.1)) model.add(Dense(4)) model.add(Activation('softmax')) return model def train(self, embeddings, x, y): model = self.build_network(embeddings, \"embeddings_train\") model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]) # 训练，采用k-folder交叉训练 for i in range(0, self.max_epochs): x_train, x_val, y_train, y_val = self.split_train_data(x, y) model.fit(x_train, y_train, batch_size=self.batch_size, validation_data=(x_val, y_val)) # 保存model yaml_string = model.to_yaml() with open(self.yaml_name, 'w') as outfile: outfile.write(yaml.dump(yaml_string, default_flow_style=True)) # 保存model的weights model.save_weights(self.model_name) def predict(self, embeddings, x): # 加载model print 'loading model......' with open(self.yaml_name, 'r') as f: yaml_string = yaml.load(f) model = model_from_yaml(yaml_string) # 加载权重 print 'loading weights......' model.load_weights(self.model_name, by_name=True) model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]) # 预测 x = sequence.pad_sequences(x, self.max_seq_length) predicts = model.predict_classes(x) # 得到分类结果，它表征的是类别序号 # 转换 classes = [0, 1, -2, -1] predicts = [classes[item] for item in predicts] np.set_printoptions(threshold=np.nan) # 全部打印 print(np.array(predicts)) return predicts\n3.2.4 softmax输出类别\n这一部分上面代码已经讲到了，不在赘述。softmax只是一个归一化，讲数据归一化到[0, 1]之间，从而可以得到每个类别的概率。我们最终取概率最大的即可。\n3.3 基于深度学习的情感分析难点\n基于深度学习的情感分析难点也很多\n语句长度太长。很多用户评论都特别长，分词完后也有几百个词语。而对于LSTM，序列过长会导致计算复杂、精度降低等问题。一般解决方法有进行停用词处理，无关词处理等，从而缩减文本长度。或者对文本进行摘要，抽离出语句主要成分。\n新词和口语化的词语特别多。用户评论语句不像新闻那样规整，新词和口语化的词语特别多。这个问题给分词和词向量带来了很大难度。一般解决方法是分词方面，建立用户词典，从而提高分词准确度。词向量方面，对新词进行增量训练，从而提高新词覆盖率。\n4. 总结\n文本情感分析是NLP领域一个十分重要的问题，对理解用户意图具有决定性的作用。通过基于词典的传统算法和基于深度学习的算法，可以有效的进行情感分析。当前情感分析准确率还有待提高，任重而道远！\n系列文章，请多关注\nTensorflow源码解析1 – 内核架构和源码结构\n带你深入AI（1） - 深度学习模型训练痛点及解决方法\n自然语言处理1 – 分词\n自然语言处理2 – jieba分词用法及原理\n自然语言处理3 – 词性标注\n自然语言处理4 – 句法分析\n自然语言处理5 – 词向量\n自然语言处理6 – 情感分析","date":"2018年09月09日 15:30:17","data":"2018年09月09日 15:30:17"}
{"_id":{"$oid":"5d36a8d96734bd8e681d5eb4"},"title":"Pyhon 自然语言处理（一）NLTK及语料库下载","author":"慕白","content":"Python 自然语言处理（一）NLTK及语料库下载\n参考网站 http://www.nltk.org/\nNLTK是用来进行自然语言处理很强大的包，本文介绍Python下安装NLTK及语料下载\n1. 安装 NLTK\npip install nltk\n如果已经安装了 Anaconda 则默认安装了nltk，但是没有安装语料库\n2. 自动安装语料库\n如果在引入nltk包后，发现没有安装语料库，则可以自动下载安装，命令:\nimport nltk nltk.download() showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\nTrue\n3. 手动导入语料库\n由于自动安装语料库会耗费很大时间，可以手动导入语料库。\n语料库下载地址百度云盘：http://pan.baidu.com/s/1hswoU5u\n下载后的语料库可以导入到以下目录：\n- ‘/home/zhanghc/nltk_data’\n- ‘/usr/share/nltk_data’\n- ‘/usr/local/share/nltk_data’\n- ‘/usr/lib/nltk_data’\n- ‘/usr/local/lib/nltk_data’\n4. NLTK安装包及语料库安装完成\nimport nltk\n# NLTK自带的语料库展示 from nltk.corpus import brown\nbrown.categories()\n[u'adventure', u'belles_lettres', u'editorial', u'fiction', u'government', u'hobbies', u'humor', u'learned', u'lore', u'mystery', u'news', u'religion', u'reviews', u'romance', u'science_fiction']\nlen(brown.sents())\n57340\nlen(brown.words())\n1161192\n5. NLTK 常用函数","date":"2017年01月11日 15:48:51"}
{"_id":{"$oid":"5d36a8d96734bd8e681d5eb6"},"title":"自然语言处理中的符号表征","author":"yuanmengxinglong","content":"自然语言处理中的符号表征\n0. Preface\n自然语言处理中的符号表征\nAuthor: Cao Shengming\nEmail: caoshengming@trio.ai\nCompany: Trio 北京（三角兽）科技有限公司\n0. Preface\n这部分将探讨一下，自然语言处理中的符号表征问题。","date":"2018年07月27日 14:35:40"}
{"_id":{"$oid":"5d36a8da6734bd8e681d5eb8"},"title":"自然语言处理之维特比(Viterbi)算法","author":"lovive","content":"维特比算法 (Viterbi algorithm) 是机器学习中应用非常广泛的动态规划算法，在求解隐马尔科夫、条件随机场的预测以及seq2seq模型概率计算等问题中均用到了该算法。实际上，维特比算法不仅是很多自然语言处理的解码算法，也是现代数字通信中使用最频繁的算法。在介绍维特比算法之前，先回顾一下隐马尔科夫模型，进而介绍维特比算法的计算步骤。\n\n\n以下为一个简单的隐马尔科夫模型，如下图所示：\n\n\n\n\n\n其中x = (x1, x2, ..., xN) 为隐状态序列，y = (y1, y2, ..., yN) 为观测序列，要求的预测问题为：\n\n\n\n依据马尔科夫假设，上式等价于：\n\n\n\n在隐马尔科夫链中，任意时刻t下状态的值有多个，以拼音转汉字为例，输入拼音为“yike”可能有的值为一棵，一刻或者是一颗等待，用符号xij表示状态xi的第j个可能值，将状态序列按值展开，就得到了一个篱笆网了，这也就是维特比算法求解最优路径的图结构：\n\n\n\n隐马尔科夫的预测问题就是要求图中的一条路径，使得该路径对应的概率值最大。 对应上图来讲，假设每个时刻x可能取的值为3，如果直接求的话，有3^N的组合数，底数3为篱笆网络宽度，指数N为篱笆网络的长度，计算量非常大。维特比利用动态规划的思想来求解概率最大路径（可理解为求图最短路径），使得复杂度正比于序列长度，复杂度为O(N⋅D⋅D), N为长度，D为宽度，从而很好地解决了问题的求解。\n\n\n维特比算法的基础可以概括为下面三点（来源于吴军：数学之美）：\n\n\n1、如果概率最大的路径经过篱笆网络的某点，则从开始点到该点的子路径也一定是从开始到该点路径中概率最大的。\n\n\n2、假定第i时刻有k个状态，从开始到i时刻的k个状态有k条最短路径，而最终的最短路径必然经过其中的一条。\n\n\n3、根据上述性质，在计算第i+1状态的最短路径时，只需要考虑从开始到当前的k个状态值的最短路径和当前状态值到第i+1状态值的最短路径即可，如求t=3时的最短路径，等于求t=2时的所有状态结点x2i的最短路径加上t=2到t=3的各节点的最短路径。\n\n\n为了纪录中间变量，引入两个变量sigma和phi，定义t时刻状态为i的所有单个路径 (i1, i2, ..., it) 中最大概率值（最短路径）为（前文小修已经有介绍隐马尔科夫相关的概念，如果不清楚可以看一下前面的详解隐马尔可夫模型 (HMM) ）:\n\n\n其中it表示最短路径，Ot表示观测符号，lamda表示模型参数，根据上式可以得出变量sigma的递推公式：\n\n\n其中i = 1, 2, ..., N; t = 1, 2, ... , T-1，定义在时刻t状态为i的所有单个路径 (i1, i2, ..., it, i) 中概率最大的路径的第t－1个结点为：\n\n\n根据上面的两个定义下面给出维特比算法具体内容：\n输入为模型和观测状态分别为：\n，\n\n输出为求出最优路径：\n\n\n步骤为：(1) 初始化各参数：\n\n\n(2) 根据上式进行递推，对t＝2, 3, ..., T\n\n\n(3) 最后计算终止状态：\n\n\n\n\n最优路径的回溯，对t＝T-1, T－2，..., 1\n\n\n最后求得最优路径：\n\n\n以上就是维特比算法的主要过程和内容，下面介绍一个个例子。在自然语言处理技术中的seq2seq模型中，如下图所示：\n\n\n其实seq2seq模型的核心就是：\n\n\n其中e和f就是相应的输出和输入序列，在进行解码的时候，如果词袋中的个数为V个，那么那么输出长度为N的序列，则需要的总共搜索V^N次，如果N的个数非常之大，这样的搜索非常耗时间，那么这个时候使用维特比算法就会大大的降低搜索的时间。这里假设词袋中只有a和b，而且它们之间的转变概率为：\n\n\n\n\n在这时使用维特比算法，其主要的思想为：\n\n\n其中s(v,n) 表示的是以v结尾的最大概率的序列的概率，t(i, j, n)为第n－1步从i跳到第n步的j的概率。根据算法可以得到：\n\n\n\n\n因此最终输出的序列为bba，\n\n\n\n在这里最后说一点就是，其实对于seq2seq有相应的算法对整个序列的输出进行搜索计算 (beam search算法)，其思想和维特比算法非常相似，这里不做介绍，下次有机会给大家介绍。\n\n\n参考书目：\n[1] 统计学习方法，李航\n文章来源于微信公众号：言处理技术，更多内容请访问该公众号。\n\n\n欢迎关注公众号学习","date":"2017年12月04日 19:51:07"}
{"_id":{"$oid":"5d36a8da6734bd8e681d5eba"},"title":"Attention Model 在自然语言处理里的应用","author":"YoungDreamNJU","content":"本文的目标是介绍Attention Model在自然语言处理里的应用，本文的结构是：先介绍两篇经典之作，一篇NMT，一篇是Image Caption；之后介绍Attention在不同NLP Task上的应用，在介绍时有详有略。\n经典之作\n有两篇文章被Attention的工作广泛引用，这里单拎出来介绍：\nNEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE\nNMT通常用encoder-decoder family的方法，把句子编码成一个定长向量，再解码成译文。作者推测定长向量是encoder-decoder架构性能提升的瓶颈，因此让模型自动寻找（与预测下一个词相关的）部分原文。\n\nEncoder部分，作者使用了Bidirectional RNN for annotating sequences\n这是PPT介绍\nhttp://www.iclr.cc/lib/exe/fetch.php?media=iclr2015:bahdanau-iclr2015.pdf\n\n\n\nShow, Attend and Tell: Neural Image Caption Generation with Visual Attention\n这篇文章的任务是给图片起个标题，我自己做了一页PPT总结了文章思路\n\n\n接下来介绍自然语言处理各种Task中的Attention应用。\nAttention in Word Embedding\nNot All Contexts Are Created Equal: Better Word Representations with Variable Attention\nThe general intuition of the model is that some words are only relevant for predicting local context (e.g. function words), while other words are more suited for determining global context, such as the topic of the document.\nIn CBOW:\n\np(v0|w[−b,b]−{0})=expvT0Oc∑v∈VexpvTOc\np(v_0|w_{[-b,b]-\\{0\\}})=\\frac{\\exp v_0^TO_c}{\\sum_{v \\in V} \\exp v^TO_c}\nIn this paper:\n\nc=∑i∈[−b,b]−{0}ai(wi)wi\nc = \\sum_{i \\in [-b,b]-\\{0\\}} a_i(w_i)w_i\n\nai(w)=expkw,i+si∑j∈[−b−b]−{0}expkw,i+si\na_i(w)=\\frac{\\exp k_{w,i}+s_i}{\\sum_{j \\in [-b-b]-\\{0\\}} \\exp k_{w,i}+s_i}\neach word\nwi\nw_i at relative position\ni\ni is attributed an attention level representing how much the attention model believes this it is important to look at in order to predict the center word\nGradients of the loss function with respect to the parameters\n(W,O,K,s)\n(W, O, K, s) are computed with backpropagation, and parameters are updated after each training instance using a fixed learning rate.\n\n\n\n\nAttention in Machine Translation\nEffective Approaches to Attention-based Neural Machine Translation\nGlobal Attention\nat(s)=align(ht,h¯s)=exp(score(ht,h¯s))∑s′exp(ht,hs′¯)\na_t(s)=align(h_t,\\bar h_s) =\\frac{\\exp(score(h_t,\\bar h_s))}{\\sum_{\\mathop{s'}} \\exp(h_t,\\bar{h_{\\mathop s'}})}\n这里的\nat\na_t是Global Align Weights，它的size是由the number of time steps on the source side决定的，之后的\nct\nc_t是由source hidden states\nh¯s\n\\bar h_s和\nat\na_t的weighted average计算出的。\n\n这里的score function可以有多种：\n\nscore(ht,h¯s)=⎧⎩⎨⎪⎪hTth¯shTtWah¯sWa[ht;h¯s]dot,general,concat.\nscore(h_t,\\bar h_s)=\\begin{cases}h_t^\\mathrm{T}\\bar h_s\u0026\\text{dot},\\\\ h_t^\\mathrm{T}W_a\\bar h_s\u0026\\text{general},\\\\ W_a[h_t;\\bar h_s]\u0026\\text{concat}.\\end{cases}\n除这些之外，作者还实验了\nat=softmax(Waht)\na_t = softmax(W_ah_t)\nLocal Attention\nThe context vector\nct\nc_t is then derived as a weighted average over the set of source hidden states within the window\n[pt−D,pt+D]\n[p_t−D, p_t+D];\nD\nD is empirically selected.\nUnlike the global approach, the local alignment vector\nat\na_t is now fixed-dimensional, i.e.,\n∈R2D+1\n\\in R^{2D+1}.(这是定义级的区别)\n接下来作者把模型做了两种变种：\nMonotonic alignment (local-m)\nsimply set\npt=t\np_t = t assuming that source and target sequences are roughly monotonically aligned.\nat\na_t的公式同global\nPredictive alignment (local-p)\n修改定义\n\nat(s)=align(ht,h¯s)exp(−(s−pt)22σ2)\na_t(s)=align(h_t,\\bar h_s)\\exp(-\\frac{(s-p_t)^2}{2\\sigma^2})\n其中\n\npt=S∙sigmoid(vTptanh(Wpht))\np_t=S\\bullet sigmoid(v_p^\\mathrm{T}\\tanh(W_ph_t))\nAs a result, attention model will favor alignment points near\npt\np_t.\nIn our proposed global and local approaches, the attentional decisions are made independently, which is suboptimal.\n在标准的MT中，有一个coverage set记录哪些词被翻译过了，在这个模型中attentional vectors\nh~t\n\\tilde h_tare concatenated with inputs at the next time steps. 作者把它称作input-feeding approach.\n\nThe effects of having such connections are two-fold: (a) we hope to make the model fully aware of previous alignment choices and (b) we create a very deep network spanning both horizontally and vertically.\n\n\n\nAttention in QA\nCharacter-Level Question Answering with Attention\nEncode the Entities and Predicates in the KB\nEncode the Query\nDecoding the KB Query\n3.1. An LSTM-based decoder with attention\n3.2. A pairwise semantic relevance function that measures the similarity between the hidden units of the LSTM and the embedding of an entity or predicate candidate\n\n\n小结\n通过以上具体的解释，我们可以看出：\nThe basic idea of attention mechanism is that it assigns a weight/importance to each lower position when computing an upper level representation.\n下面再看一些其他任务上Attention Model的应用。\n\n\nAttention in Document Classification\nHierarchical Attention Networks for Document Classification\n先用词表示、双向GRU、Attention生成句子表示，再用一样的方法生成文档表示v，最后softmax(Wv+b)用于文档分类。\n\n\n\nAttention in Language to Logical Form\nLanguage to Logical Form with Neural Attention\n本文要把自然语言转化成逻辑表达式，创造了2个模型：1）Sequence-to-Sequence Model把语义解析当做普通的序列转换任务；2）Sequence-to-Tree Model用层次树解码器获得逻辑形式的结构，先翻译第一层，再翻译下一层。最后在翻译的时候加入了Attention机制。\n\n\n\nAttention in Summarization\nA Neural Attention Model for Abstractive Sentence Summarization\n与以上类似\n\n\n就写这么多了，请各位批评指正，谢谢！\n\n\nReference\nBahdanau D, Cho K, Bengio Y. Neural Machine Translation by Jointly Learning to Align and Translate[J]. Computer Science, 2014.\nXu K, Ba J, Kiros R, et al. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention[J]. Computer Science, 2016:2048-2057.\nLing W, Tsvetkov Y, Amir S, et al. Not All Contexts Are Created Equal: Better Word Representations with Variable Attention[C]// Conference on Empirical Methods in Natural Language Processing. 2015:1367-1372.\nLuong M T, Pham H, Manning C D. Effective Approaches to Attention-based Neural Machine Translation[J]. Computer Science, 2015.\nGolub D, He X. Character-Level Question Answering with Attention[J]. 2016.\nYang Z, Yang D, Dyer C, et al. Hierarchical Attention Networks for Document Classification[C]// Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016:1480-1489.\nDong L, Lapata M. Language to Logical Form with Neural Attention[C]// Meeting of the Association for Computational Linguistics. 2016:33-43.\nRush A M, Chopra S, Weston J. A Neural Attention Model for Abstractive Sentence Summarization[J]. Computer Science, 2015.","date":"2017年02月06日 18:42:17"}
{"_id":{"$oid":"5d36a8dc6734bd8e681d5ec0"},"title":"自然语言处理：盘点一下数据平滑算法","author":"quicmous","content":"在自然语言处理中，经常要计算单词序列（句子）出现的概率估计。我们知道，算法在训练时，语料库不可能包含所有可能出现的序列。因此，为了防止对训练样本中未出现的新序列概率估计值为零，人们发明了好多改善估计新序列出现概率的算法，即数据平滑算法。\nLaplace 法则\n最简单的算法是Laplace法则，思路很简单，统计测试数据集中的元素在训练数据集中出现的次数时，计数器的初始值不要设成零，而是设成１。这样，即使该元素没有在训练集中出现，其出现次数统计值至少也是１。因此，其出现的概率估计值就不会是零了。\n假设测试集\nV\nV 中某个元素在训练集\nT\nT 中出现\nr\nr 次，经过Laplace法则调整后的统计次数为：\nr∗=r+1\nr^*=r+1\n当然这样做，纯粹是为了不出现零概率，并没有解决对未见过的实例进行有效预测的问题。因此，Laplace法则仅仅是一种非常初级的技术，有点太小儿科了。\nGood-Turing 估计\nLaplace方法一个很明显的问题是\n∑r∗≠∑r。\n\\sum{r^*} \\ne \\sum r。 Good-Turning 方法认为这是一个重大缺陷，需要给予改进。其实我觉得这真不算重要，只要能合理估计未见过的新实例的概率，总的统计次数发生变化又怎样呢？\n\nGood-Turing 修正后的计算公式还真的很巧妙，它在Laplace法则后面乘了一个修正系数，就可以保证总次数不变。这个拿出来炫一炫还是没问题的：\n\n\nr∗=(r+1)nr+1nr\nr^*=(r+1)\\frac{n_{r+1}}{n_r}\n其中，\nnr\nn_r表示测试集\nV\nV 中，一共有\nnr\nn_r个元素在训练集\nT\nT 中出现过\nnr\nn_r 次。\n虽然我觉得这个方法没啥用，但是它的确保证了测试集中元素在训练集中出现的总次数不变。即：\n\nN1=∑r=0∞rnr=0×n0+1×n1+2×n2+...N2=∑r=0∞r∗nr=1×n1n0×n0+2×n2n1×n1+...=1×n1+2×n2+...\n\\begin{matrix} N_1=\\sum_{r=0}^{\\infty}rn_r=0\\times n_0 + 1\\times n_1+2 \\times n_2 + ...\\\\ N_2=\\sum_{r=0}^{\\infty}r^*n_r = 1 \\times \\frac{n_1}{n_0}\\times {n_0} +2 \\times \\frac{n_2}{n_1}\\times{n_1}+...\\\\ =1\\times n_1+2 \\times n_2 + ... \\end{matrix}\n显然，\nN1=N2\nN_1=N_2。或许这个方法解决不了自然语言处理问题，而且\nnr=0\nn_r=0 时公式也会失效，但其思路应该还是很有价值的，或许解决其他问题能用得上。\n绝对折扣和线性折扣\n估计发明的作者受到 Good-Turing 的刺激了，认为这个方法就是“劫富济贫”，把数量较大的统计次数拿出一部分均给了较小的统计次数，减少贫富差距。只不过这个方法用了一个很有技巧的公式掩盖的其本质。\n\n与其羞羞答答“劫富济贫”，不如来个赤裸裸的方法，于是乎就出现了绝对折扣和线性折扣方法。\n\n问题是，“劫富济贫”并不是我们的目的，我们需要的是能够对语料库中从未出现过的句子做出概率判断。要得到正确的判断，需要“劫”多少？“济”多少？这个问题绝对折扣和线性折扣都回答不了。所以，无论Good-Turing方法，还是这两种折扣方法，本质上都没跳出 Laplace 法则的思路。\n\nWitten-Bell算法\nWitten-Bell算法终于从 Laplace 算法跳了出来，有了质的突破。这个方法的基本思想是：如果测试过程中一个实例在训练语料库中未出现过，那么他就是一个新事物，也就是说，他是第一次出现。那么可以用在语料库中看到新实例（即第一次出现的实例）的概率来代替未出现实例的概率。\n\n假设词汇在语料库出现的次数参见下表：\nr\nr\n1\n2\n3\n4\n5\nnr\nn_r\n50\n40\n30\n20\n10\n则\n\nN=1×50+2×40+3×30+4×20+5×10=350T=50+40+30+20+10=150\n\\begin{matrix} N = 1 \\times 50+2 \\times 40+3 \\times 30+4 \\times 20+5 \\times 10 = 350 \\\\ T = 50+40+30+20+10 = 150 \\\\ \\end{matrix}\n那么，我们可以用\n\nTN+T=150350+150=0.3\n\\frac{T}{N+T}=\\frac{150}{350+150}=0.3\n近似表示在语料库看到新词汇的概率。\n我不能说这个方法有多少道理，但与那些“劫富济贫”的方法相比，它至少提供了一个说得过去的理由。\n\n扣留估计和交叉检验\n\n扣留估计和交叉检验这两种方法估计是受到Witten-Bell算法启发了，但是思路没跳出该方法套路，而且手法比较卑劣。和Witten-Bell算法一样，对于所有遇到的新事物，都给出完全相同的概率预测。\n插值算法\n\n前面的平滑算法对于从来没出现的n-gram都给与相同的概率估计，有些情况下这并不合适。事实上我们可以考虑根据n-gram中的(n-1)gram的频率产生一个更好的概率估计。如果 (n-1)gram很少出现，就给n-gram一个较小的估计，反之给出一个较大的估计。\n例如，假定要在一批语料库上构建二元语法模型，其中有两对词的同现次数为0：\n\nC(send the)=0　　C(send thou)=0\n\\begin{matrix} 　　C(send \\ the)=0\\\\ 　　C(send \\ thou) = 0\\\\ 　　\\end{matrix}\n那么，按照前面提到的任何一种平滑方法都可以得到：\n\np(the|send)=p(thou|send)\np(the|send)=p(thou|send)\n\n但是，直觉上我们认为应该有\n\np(the|send)\u003ep(thou|send)\np(the|send)\u003ep(thou|send)\n因为冠词the要比单词thou出现的频率要高得多。因此，可以通过组合不同信息资源的方法来产生一个更好的模型。\n基本思路\n一般来讲，使用低阶的ｎ元模型向高阶ｎ元模型插值是有效的，因为当没有足够的语料估计高阶模型时，低阶模型往往可以提供有用的信息。例如，bigram模型中的删除插值法，最基本的做法是：\n\nPinterp(wi|wi−1)=λPML(wi|wi−1)+(1−λ)PML(wi),  0≤λ≤1\nP_{interp}(w_i|w_{i-1}) = \\lambda P_{ML}(w_i|w_{i-1}) + (1-\\lambda) P_{ML}(w_i), \\ \\ 0\\le\\lambda\\le1\n由于\n\nPML(the|send)=PML(thou|send)=0\nP_{ML}(the|send)=P_{ML}(thou|send)=0\n而且\n\nPML(the)≫PML(thou)\nP_{ML}(the)\\gg P_{ML}(thou)\n所以\n\nPinterp(the|send)\u003ePinterp(thou|send)\nP_{interp}(the|send)\u003eP_{interp}(thou|send)\n定义\n在统计自然语言处理中，这种方法通常被称为线性插值法（Linear Interpolation），在其他的地方常常被称为混合模型（Mixture Model）。插值模型的递归定义如下：\nPinterp(wi|wi−(n−1)...wi−1)=λPML(wi|wi−(n−1)...wi−1)+(1−λ)Pinterp(wi|wi−(n−2)...wi−1)\nP_{interp}(w_i|w_{i-(n-1)}...w_{i-1})=\\lambda P_{ML}(w_i|w_{i-(n-1)}...w_{i-1})+(1-\\lambda)P_{interp}(w_i|w_{i-(n-2)}...w_{i-1})\n例子\n对于三元模型，有\n\nPinterp(wi|wi−(n−1)...wi−1)=λPML(wi|wi−(n−1)...wi−1)+(1−λ)Pinterp(wi|wi−1)\nP_{interp}(w_i|w_{i-(n-1)}...w_{i-1})=\\lambda P_{ML}(w_i|w_{i-(n-1)}...w_{i-1})+(1-\\lambda)P{interp}(w_i|w_{i-1})\n对于二元模型，有\n\nPinterp(wi|wi−1)=λPML(wi|wi−1)+(1−λ)PML(wi)\nP_{interp}(w_i|w_{i-1})=\\lambda P_{ML}(w_i|w_{i-1})+(1-\\lambda)P_{ML}(w_i)\n于是，\n\nPinterp=...=λ3PML(wi|wi−2wi−1)+λ2PML(wi|wi−1)+λ1PML(wi),  λ1+λ2+λ3=1\nP_{interp}=...=\\lambda_3P_{ML}(w_i|w_{i-2}w_{i-1})+\\lambda_2P_{ML}(w_i|w_{i-1})+\\lambda_1P_{ML}(w_i),\\ \\ \\lambda_1+\\lambda_2+\\lambda_3=1\n\nKatz回退算法\nkatz方法也是一种插值方法，不过它仅对语料库中未出现的数据进行预测。\n\n总起来说，这类插值方法与传统数学中用已知数据插值预测未知数据的方法思路相同，属于比较靠谱的方法。\n\n参考文献\n[1] 陈鄞，自然语言处理基本理论和方法，哈尔滨工业大学出版社， 第1版 (2013年8月1日)","date":"2016年08月13日 11:02:40"}
{"_id":{"$oid":"5d36a8dd6734bd8e681d5ec3"},"title":"自然语言处理入门（8）——TextRank","author":"飞鸟2010","content":"TextRank是自然语言处理领域一种比较常见的关键词提取算法，可用于提取关键词、短语和自动生成文本摘要。TextRank是由PageRank算法改进过来的，所以有大量借鉴PageRank的思想，其处理文本数据的过程主要包括以下几个步骤：\n（1）首先，将原文本拆分为句子，在每个句子中过滤掉停用词（可以不选），并只保留指定词性的单词，由此可以得到句子和单词的集合。\n（2）每个单词作为PageRank中的一个节点。设窗口大小为k，假设一个句子所组成的单词可以表示为w1,w2,w3,…, wn.\n则w1,w2, …, wk、w2,w3,…,wk+1、w3,w4,…,wk+2等都是一个窗口，在一个窗口内任意两个单词之间存在一条无向无权的边。\n（3）基于上面的节点和边构成图，可以据此计算出每个节点的重要性。最重要的若干单词可以作为区分文本类别和主题的关键词。\n\n\n基于荣耀V10手机评论数据的Python代码实现如下所示：\n# -*- coding: utf-8 -*- \"\"\" Created on Fri Feb  9 15:58:14 2018 @author: zch \"\"\" import codecs from textrank4zh import TextRank4Keyword, TextRank4Sentence #读取华为荣耀天猫旗舰店荣耀V10手机的评论文本数据 text = codecs.open('D://data/tmall/origin_tmall_review.txt', 'r', 'utf-8').read() tr4w = TextRank4Keyword() tr4w.analyze(text=text, lower=True, window=2) print( '关键词：' ) for item in tr4w.get_keywords(10, word_min_len=1):     print(\"{} 出现的频率为:{:.6f}\".format(item.word, item.weight)) print( '关键短语：' ) for phrase in tr4w.get_keyphrases(keywords_num=10, min_occur_num=5):     print(phrase) tr4s = TextRank4Sentence() tr4s.analyze(text=text, lower=True, source = 'all_filters') print() print( '摘要：' ) for item in tr4s.get_key_sentences(num=3):     #index是语句在文本中位置，weight是权重     print(\"第{}句出现的频率为:{:.6f},内容为:{}\".format(item.index, item.weight, item.sentence))\n输出的关键词如下图所示:\n输出的关键短语如下图所示：\n输出的摘要如下图所示：\n从上面的输出结果可以看出，华为荣耀V10的评论信息，大多数还是比较积极、正面的，能够基本反映出用户对这款手机产品的态度。","date":"2018年02月09日 16:27:16"}
{"_id":{"$oid":"5d36a8dd6734bd8e681d5ec6"},"title":"spark中自然语言处理的一些方法","author":"旭旭_哥","content":"spark中常用的一些自然语言处理方法，分词、tf-idf、word2vec、文本分类等看看代码吧：\npackage com.iclick.word2vec import org.apache.log4j.{ Level, Logger } import org.apache.spark.{ SparkConf, SparkContext } import org.apache.spark.sql.SQLContext import org.apache.spark.mllib.feature.{ Word2Vec, Word2VecModel } import org.apache.spark.mllib.linalg.{ SparseVector =\u003e SV } import org.apache.spark.mllib.feature.HashingTF import org.apache.spark.mllib.feature.IDF object Word2VecTest { def main(args: Array[String]): Unit = { Logger.getLogger(\"org.apache.spark\").setLevel(Level.ERROR) Logger.getLogger(\"org.eclipse.jetty.server\").setLevel(Level.OFF) val sc = new SparkContext(\"local\", \"mysql\") val sqlContext = new SQLContext(sc) val path = \"D:\\\\SPARKCONFALL\\\\Spark机器学习数据\\\\20news-bydate-train\\\\*\" val rdd = sc.wholeTextFiles(path).cache() val xxx = rdd.map { case (file, text) =\u003e file.split(\"/\").takeRight(2).head }.map(n =\u003e (n, 1)).reduceByKey(_ + _).collect().sortBy(_._2).mkString(\"\\n\") println(\"文章主题的数目\") println(xxx) val newsgroups = rdd.map { case (file, text) =\u003e file.split(\"/\").takeRight(2).head } println(\"分词数目\") val text = rdd.map { case (file, text) =\u003e text } val whieteSpaceSplit = text.flatMap { t =\u003e t.split(\" \") }.map(_.toLowerCase()) println(whieteSpaceSplit.distinct().count) println(whieteSpaceSplit.sample(true, 0.3, 42).take(100).mkString(\",\")) println(\"改进分词\") val nonWordSplit = text.flatMap(t =\u003e t.split(\"\"\"\\W+\"\"\").map(_.toLowerCase)) println(nonWordSplit.distinct.count) println(nonWordSplit.distinct.sample(true, 0.3, 42).take(100).mkString(\",\")) val regex = \"\"\"[^0-9]*\"\"\".r val filterNumbers = nonWordSplit.filter(token =\u003e regex.pattern.matcher(token).matches) println(filterNumbers.distinct.count) println(filterNumbers.distinct.sample(true, 0.3, 42).take(100).mkString(\",\")) println(\"移除停用词\") val tokenCounts = filterNumbers.map(t =\u003e (t, 1)).reduceByKey(_ + _) val oreringDesc = Ordering.by[(String, Int), Int](_._2) //println(tokenCounts.top(20)(oreringDesc).mkString(\"\\n\")) val stopwords = Set( \"the\", \"a\", \"an\", \"of\", \"or\", \"in\", \"for\", \"by\", \"on\", \"but\", \"is\", \"not\", \"with\", \"as\", \"was\", \"if\", \"they\", \"are\", \"this\", \"and\", \"it\", \"have\", \"from\", \"at\", \"my\", \"be\", \"that\", \"to\") val tokenCountsFilteredStopwords = tokenCounts.filter { case (k, v) =\u003e !stopwords.contains(k) } //println(tokenCountsFilteredStopwords.top(20)(oreringDesc).mkString(\"\\n\")) val tokenCountsFilteredSize = tokenCountsFilteredStopwords.filter { case (k, v) =\u003e k.size \u003e= 2 } println(tokenCountsFilteredSize.top(20)(oreringDesc).mkString(\"\\n\")) println(\"移除低频词\") val oreringAsc = Ordering.by[(String, Int), Int](-_._2) //println(tokenCountsFilteredSize.top(20)(oreringAsc).mkString(\"\\n\")) val rareTokens = tokenCounts.filter { case (k, v) =\u003e v \u003c 2 }.map { case (k, v) =\u003e k }.collect.toSet val tokenCountsFilteredAll = tokenCountsFilteredSize.filter { case (k, v) =\u003e !rareTokens.contains(k) } println(tokenCountsFilteredAll.top(20)(oreringAsc).mkString(\"\\n\")) def tokenize(line: String): Seq[String] = { line.split(\"\"\"\\W+\"\"\") .map(_.toLowerCase) .filter(token =\u003e regex.pattern.matcher(token).matches) .filterNot(token =\u003e stopwords.contains(token)) .filterNot(token =\u003e rareTokens.contains(token)) .filter(token =\u003e token.size \u003e= 2) .toSeq } //println(text.flatMap(doc =\u003e tokenize(doc)).distinct.count) val tokens = text.map(doc =\u003e tokenize(doc)) println(tokens.first.take(20)) println(\"训练模型\") val dim = math.pow(2, 18).toInt val hashingTF = new HashingTF(dim) val tf = hashingTF.transform(tokens) tf.cache() val v = tf.first.asInstanceOf[SV] println(v.size) println(v.size) println(v.values.size) println(v.values.take(10).toSeq) println(v.indices.take(10).toSeq) println(\"fit \u0026 transform\") val idf = new IDF().fit(tf) val tfidf = idf.transform(tf) val v2 = tfidf.first.asInstanceOf[SV] println(v2.values.size) println(v2.values.take(10).toSeq) println(v2.indices.take(10).toSeq) // 分析权重 val minMaxVals = tfidf.map { v =\u003e val sv = v.asInstanceOf[SV] (sv.values.min, sv.values.max) } val globalMinMax = minMaxVals.reduce { case ((min1, max1), (min2, max2)) =\u003e (math.min(min1, min2), math.max(max1, max2)) } println(globalMinMax) //globalMinMax: (Double, Double) = (0.0,66155.39470409753) //常用词 val common = sc.parallelize(Seq(Seq(\"you\", \"do\", \"we\"))) val tfCommon = hashingTF.transform(common) val tfidfCommon = idf.transform(tfCommon) val commonVector = tfidfCommon.first.asInstanceOf[SV] println(commonVector.values.toSeq) //不常出现的单词 val uncommon = sc.parallelize(Seq(Seq(\"telescope\", \"legislation\", \"investment\"))) val tfUncommon = hashingTF.transform(uncommon) val tfidfUncommon = idf.transform(tfUncommon) val uncommonVector = tfidfUncommon.first.asInstanceOf[SV] println(uncommonVector.values.toSeq) // // 4 使用模型 //4.1 余弦相似度 import breeze.linalg._ val hockeyText = rdd.filter { case (file, text) =\u003e file.contains(\"hockey\") } val hockeyTF = hockeyText.mapValues(doc =\u003e hashingTF.transform(tokenize(doc))) val hockeyTfIdf = idf.transform(hockeyTF.map(_._2)) val hockey1 = hockeyTfIdf.sample(true, 0.1, 42).first.asInstanceOf[SV] val breeze1 = new SparseVector(hockey1.indices, hockey1.values, hockey1.size) val hockey2 = hockeyTfIdf.sample(true, 0.1, 43).first.asInstanceOf[SV] val breeze2 = new SparseVector(hockey2.indices, hockey2.values, hockey2.size) val cosineSim = breeze1.dot(breeze2) / (norm(breeze1) * norm(breeze2)) println(cosineSim) val graphicsText = rdd.filter { case (file, text) =\u003e file.contains(\"comp.graphics\") } val graphicsTF = graphicsText.mapValues(doc =\u003e hashingTF.transform(tokenize(doc))) val graphicsTfIdf = idf.transform(graphicsTF.map(_._2)) val graphics = graphicsTfIdf.sample(true, 0.1, 42).first.asInstanceOf[SV] val breezeGraphics = new SparseVector(graphics.indices, graphics.values, graphics.size) val cosineSim2 = breeze1.dot(breezeGraphics) / (norm(breeze1) * norm(breezeGraphics)) println(cosineSim2) val baseballText = rdd.filter { case (file, text) =\u003e file.contains(\"baseball\") } val baseballTF = baseballText.mapValues(doc =\u003e hashingTF.transform(tokenize(doc))) val baseballTfIdf = idf.transform(baseballTF.map(_._2)) val baseball = baseballTfIdf.sample(true, 0.1, 42).first.asInstanceOf[SV] val breezeBaseball = new SparseVector(baseball.indices, baseball.values, baseball.size) val cosineSim3 = breeze1.dot(breezeBaseball) / (norm(breeze1) * norm(breezeBaseball)) println(cosineSim3) //4.2 学习单词与主题的映射关系 //多分类映射 import org.apache.spark.mllib.regression.LabeledPoint import org.apache.spark.mllib.classification.NaiveBayes import org.apache.spark.mllib.evaluation.MulticlassMetrics val newsgroupsMap = newsgroups.distinct.collect().zipWithIndex.toMap val zipped = newsgroups.zip(tfidf) val train = zipped.map { case (topic, vector) =\u003e LabeledPoint(newsgroupsMap(topic), vector) } train.cache //朴素贝叶斯训练 val model = NaiveBayes.train(train, lambda = 0.1) //加载测试数据集 val testPath = \"D:\\\\SPARKCONFALL\\\\Spark机器学习数据\\\\20news-bydate-test\\\\*\" val testRDD = sc.wholeTextFiles(testPath) val testLabels = testRDD.map { case (file, text) =\u003e val topic = file.split(\"/\").takeRight(2).head newsgroupsMap(topic) } val testTf = testRDD.map { case (file, text) =\u003e hashingTF.transform(tokenize(text)) } val testTfIdf = idf.transform(testTf) val zippedTest = testLabels.zip(testTfIdf) val test = zippedTest.map { case (topic, vector) =\u003e LabeledPoint(topic, vector) } //计算准确度和多分类加权F-指标 val predictionAndLabel = test.map(p =\u003e (model.predict(p.features), p.label)) val accuracy = 1.0 * predictionAndLabel.filter(x =\u003e x._1 == x._2).count() / test.count() println(accuracy) val metrics = new MulticlassMetrics(predictionAndLabel) println(metrics.weightedFMeasure) //5 评估 val rawTokens = rdd.map { case (file, text) =\u003e text.split(\" \") } val rawTF = rawTokens.map(doc =\u003e hashingTF.transform(doc)) val rawTrain = newsgroups.zip(rawTF).map { case (topic, vector) =\u003e LabeledPoint(newsgroupsMap(topic), vector) } val rawModel = NaiveBayes.train(rawTrain, lambda = 0.1) val rawTestTF = testRDD.map { case (file, text) =\u003e hashingTF.transform(text.split(\" \")) } val rawZippedTest = testLabels.zip(rawTestTF) val rawTest = rawZippedTest.map { case (topic, vector) =\u003e LabeledPoint(topic, vector) } val rawPredictionAndLabel = rawTest.map(p =\u003e (rawModel.predict(p.features), p.label)) val rawAccuracy = 1.0 * rawPredictionAndLabel.filter(x =\u003e x._1 == x._2).count() / rawTest.count() println(rawAccuracy) val rawMetrics = new MulticlassMetrics(rawPredictionAndLabel) println(rawMetrics.weightedFMeasure) println(\"word2Vec模型训练\") val word2vec = new Word2Vec() word2vec.setSeed(42) // we do this to generate the same results each time val word2vecModel = word2vec.fit(tokens) println(\"寻找最相似的二十个單詞，\") word2vecModel.findSynonyms(\"hockey\", 20).foreach(println) word2vecModel.findSynonyms(\"legislation\", 20).foreach(println) } }","date":"2016年06月27日 12:22:23"}
{"_id":{"$oid":"5d36a8de6734bd8e681d5ec8"},"title":"自然语言处理——简单词袋模型","author":"I'm zm","content":"What Is Natural Language Processing?\n本文将学习自然语言处理，当给予计算机一篇文章，它并不知道这篇文章的含义。为了让计算机可以从文章中做出推断，我们需要将文章转化为数值表示。这个过程使得计算机能够凭语法规则去识别它。那么首先就要学会如何将文章变为数值表示。\nLooking At The Data\nHacker News网站是一个可以提交文章的社区网站，并且其他的人可以对文章进行投票。投票最高的文章会被放到首页，这样就有更多的人可以看到它。我们的数据集就是 Hacker News网站2006年到2015年提交的文章集合。Arnaud Drizard利用Hacker News API爬取到了这些数据。我们从中随机抽取了3000个样本，删除了所有多余的列，最终数据的属性如下：\nsubmission_time – when the article was submitted.\nupvotes – number of upvotes the article got.\nurl – the base url of the article.\nheadline – the headline of the article.\n我们将通过文章的标题来预测文章会收到多少投票（换句话就是哪种文章更受欢迎），首先将数据中的元素值为NA的行删除掉。\nimport pandas as pd submissions = pd.read_csv(\"sel_hn_stories.csv\") submissions.columns = [\"submission_time\", \"upvotes\", \"url\", \"headline\"] submissions = submissions.dropna()\nTokenization\n我们为了预测某个标题会得到多少个投票，那么首先需要将标题转换为数值表示。可以用词袋模型（ bag of words model）来完成这个转换，词袋模型中将每个文本表示为一个数值型向量。看个例子：\n词袋模型的第一步就是分词，将一个句子根据空格将其分散为一个个不相连的单词。\ntokenized_headlines = [] for item in submissions[\"headline\"]: tokenized_headlines.append(item.split(\" \"))\nPreprocessing\n由于大小写代表的意思相同，因此我们需要将所有的单词都转换为小写\n剔除掉标点符号\npunctuation = [\",\", \":\", \";\", \".\", \"'\", '\"', \"’\", \"?\", \"/\", \"-\", \"+\", \"\u0026\", \"(\", \")\"] clean_tokenized = [] for item in tokenized_headlines: tokens = [] for token in item: token = token.lower() for punc in punctuation: token = token.replace(punc, \"\") tokens.append(token) clean_tokenized.append(tokens)\nAssembling A Matrix\n现在获取了每个文本的词袋，下一步就是将这些词袋求并集。\n利用single_tokens 剔除掉了只出现一次的单词，这样的单词没有多大意义。unique_tokens 自然就是存储的大于一次的单词。\ncounts是个值全为0的DataFrame，其中列标签为unique_tokens 中的单词，行标签为标题序号。\nimport numpy as np unique_tokens = [] single_tokens = [] for tokens in clean_tokenized: for token in tokens: if token not in single_tokens: single_tokens.append(token) elif token in single_tokens and token not in unique_tokens: unique_tokens.append(token) counts = pd.DataFrame(0, index=np.arange(len(clean_tokenized)), columns=unique_tokens)\nCounting Tokens\n填充上面构造的全零DataFrame，遍历每个token 中的所有单词，进行计数：\nfor i, item in enumerate(clean_tokenized): for token in item: if token in unique_tokens: counts.iloc[i][token] += 1\nRemoving Extraneous Columns\n我们的属性高达2309，并且其中绝大部分取值为0，这样不便于分析。较多的属性只会让模型更加拟合噪音而不是真正的信息，因此容易导致过拟合问题。\nprint(len(unique_tokens)) ''' 2309 '''\n有两类特征会降低模型的精度\n第一种：只出现过几次，这样的特征会导致过拟合。因为模型没有更多的信息来精确的确定这个特征是否重要，因为它就只出现了几次。并且它们在训练集和测试集中对于目标变量的影响也会有很大的差异，因为出现太少，因此属性分布不平衡。\n第二种：出现的次数太多，比如像and和to这样的特征根本不能给模型带来任何有意义的信息，这些词被称为停顿词，应当剔除掉。\n因此最终确定保留那些属性值大于5小于100的属性：\nword_counts = counts.sum(axis=0) ''' word_counts Series (\u003cclass 'pandas.core.series.Series'\u003e) 418 and 289 for 298 as 47 you 100 is 158 ''' counts = counts.loc[:,(word_counts \u003e= 5) \u0026 (word_counts \u003c= 100)]\nSplitting The Data\nsklearn.cross_validation中有专门划分训练集和测试集的函数train_test_split。\ncounts中存储的是分类数据，而submissions[“upvotes”]是类标签数据，分别对其进行划分。\nfrom sklearn.cross_validation import train_test_split X_train, X_test, y_train, y_test = train_test_split(counts, submissions[\"upvotes\"], test_size=0.2, random_state=1)\nMaking Predictions\nfrom sklearn.linear_model import LinearRegression clf = LinearRegression() clf.fit(X_train, y_train) predictions = clf.predict(X_test)\nCalculating Error\n计算MSE，也就是平均平方误差（mean squared error(MSE)）\nmse = sum((y_test - predictions) ** 2) / len(predictions) print(mse) ''' 2652.6082512522867 '''\nNext Steps\n得到的模型的mse是2652.6082512522867，这是一个很大的值，但是关于什么是好的错误率这个没有硬性规定，因为它取决于具体的问题。在这个问题中，投票的平均值是10，标准差是39.5。MSE的平方根是大约是51。这意味着我们的平均误差是远离真正的值的，所以我们预测时有很大偏差的。\n可以采取以下措施来降低预测的偏差问题：\n利用整个数据集进行模型的创建，因为在这个实验中我们只是抽样了3000个文章。如果利用全部的数据集将大大减少出错率。\n添加元特征（ “meta” features），比如标题的长度，单词的平均长度等等。\n利用随机森林或者其它更强大的机器学习算法。\n在剔除那些少见或者常见单词的时候，要尝试不同的阈值，找到最佳的为止。","date":"2016年05月03日 16:12:00"}
{"_id":{"$oid":"5d36a8de6734bd8e681d5ecb"},"title":"pyhanlp 自然语言处理包","author":"ljtyxl","content":"pyhanlp 自然语言处理包\nhttps://github.com/hankcs/pyhanlp HanLP是一系列模型与算法组成的NLP工具包，由大快搜索主导并完全开源，目标是普及自然语言处理在生产环境中的应用。HanLP具备功能完善、性能高效、架构清晰、语料时新、可自定义的特点。\nHanLP提供下列功能：\nt 中文分词 HMM-Bigram（速度与精度最佳平衡；一百兆内存） 最短路分词、N-最短路分词 由字构词（侧重精度，全世界最大语料库，可识别新词；适合NLP任务） 感知机分词、CRF分词 词典分词（侧重速度，每秒数千万字符；省内存） 极速词典分词 所有分词器都支持： 索引全切分模式 用户自定义词典 兼容繁体中文 训练用户自己的领域模型 词性标注 HMM词性标注（速度快） 感知机词性标注、CRF词性标注（精度高） 命名实体识别 基于HMM角色标注的命名实体识别 （速度快） 中国人名识别、音译人名识别、日本人名识别、地名识别、实体机构名识别 基于线性模型的命名实体识别（精度高） 感知机命名实体识别、CRF命名实体识别 关键词提取 TextRank关键词提取 自动摘要 TextRank自动摘要 短语提取 基于互信息和左右信息熵的短语提取 拼音转换 多音字、声母、韵母、声调 简繁转换 简繁分歧词（简体、繁体、臺灣正體、香港繁體） 文本推荐 语义推荐、拼音推荐、字词推荐 依存句法分析 基于神经网络的高性能依存句法分析器 MaxEnt依存句法分析 文本分类 情感分析 word2vec 词向量训练、加载、词语相似度计算、语义运算、查询、KMeans聚类 文档语义相似度计算 语料库工具 部分默认模型训练自小型语料库，鼓励用户自行训练。所有模块提供训练接口，语料可参考OpenCorpus。 在提供丰富功能的同时，HanLP内部模块坚持低耦合、模型坚持惰性加载、服务坚持静态提供、词典坚持明文发布，使用非常方便。默认模型训练自全世界最大规模的中文语料库，同时自带一些语料处理工具，帮助用户训练自己的模型。\nIn [ ]:\n# encoding: utf-8 from pyhanlp import * # 中文分词 print(HanLP.segment('皇家盐湖城梅西煤球王c罗费城联合')) # 词性标注 for term in HanLP.segment('徐先生还具体帮助他确定了把画雄鹰、松鼠和麻雀作为主攻目标。'): print('{}\\t{}'.format(term.word, term.nature)) # 获取单词与词性 # 关键词提取 document = \"水利部水资源司司长陈明忠9月29日在国务院新闻办举行的新闻发布会上透露，\" \\ \"根据刚刚完成了水资源管理制度的考核，有部分省接近了红线的指标，\" \\ \"有部分省超过红线的指标。对一些超过红线的地方，陈明忠表示，对一些取用水项目进行区域的限批，\" \\ \"严格地进行水资源论证和取水许可的批准。\" print(HanLP.extractKeyword(document, 5)) # 自动摘要 print(HanLP.extractSummary(document, 3)) # 依存句法分析 print(HanLP.parseDependency(\"徐先生还具体帮助他确定了把画雄鹰、松鼠和麻雀作为主攻目标。\"))\n[皇家, 盐湖城, 梅西, 煤球王, c, 罗, 费城, 联合] 徐先生 nr 还 d 具体 a 帮助 v 他 rr 确定 v 了 ule 把 pba 画 v 雄鹰 n 、 w 松鼠 n 和 cc 麻雀 n 作为 p 主攻 vn 目标 n 。 w [水资源, 陈明忠, 进行, 红线, 部分] [严格地进行水资源论证和取水许可的批准, 有部分省超过红线的指标, 水利部水资源司司长陈明忠9月29日在国务院新闻办举行的新闻发布会上透露] 1 徐先生 徐先生 nh nr _ 4 主谓关系 _ _ 2 还 还 d d _ 4 状中结构 _ _ 3 具体 具体 a ad _ 4 状中结构 _ _ 4 帮助 帮助 v v _ 0 核心关系 _ _ 5 他 他 r r _ 4 兼语 _ _ 6 确定 确定 v v _ 4 动宾关系 _ _ 7 了 了 u u _ 6 右附加关系 _ _ 8 把 把 p p _ 15 状中结构 _ _ 9 画 画 v v _ 8 介宾关系 _ _ 10 雄鹰 雄鹰 n n _ 9 动宾关系 _ _ 11 、 、 wp w _ 12 标点符号 _ _ 12 松鼠 松鼠 n n _ 10 并列关系 _ _ 13 和 和 c c _ 14 左附加关系 _ _ 14 麻雀 麻雀 n n _ 10 并列关系 _ _ 15 作为 作为 v v _ 6 动宾关系 _ _ 16 主攻 主攻 v vn _ 17 定中关系 _ _ 17 目标 目标 n n _ 15 动宾关系 _ _ 18 。 。 wp w _ 4 标点符号 _ _\npyhanlp中的命名实体识别\n对于分词而言，命名实体识别是一项非常重要的功能，当然发现新词同样重要（这部分内容被我放在之后的“提取关键词、短语提取与自动摘要、新词识别”与再之后的案例中了。\n首先是一个简单的例子，展示一下命名实体识别的效果。之后是正式内容：\nIn [ ]:\nfrom pyhanlp import * \"\"\" HanLP开启命名实体识别 \"\"\" # 音译人名示例 CRFnewSegment = HanLP.newSegment(\"crf\") term_list = CRFnewSegment.seg(\"译智社的田丰要说的是这只是一个hanlp命名实体识别的例子\") print(term_list) print(\"\\n========== 命名实体开启与关闭对比试验 ==========\\n\") sentences =[ \"北川景子参演了林诣彬导演的《速度与激情3》\", \"林志玲亮相网友:确定不是波多野结衣？\", \"龟山千广和近藤公园在龟山公园里喝酒赏花\", ] # 通过HanLP 进行全局设置,但是部分分词器本身可能不支持某项功能 # 部分分词器本身对某些命名实体识别效果较好 HanLP.Config.japaneseNameRecognize = False viterbiNewSegment = HanLP.newSegment(\"viterbi\").enableJapaneseNameRecognize(True) CRFnewSegment_new = HanLP.newSegment(\"crf\").enableJapaneseNameRecognize(True) # segSentence # CRFnewSegment_2.seg2sentence(sentences) for sentence in sentences: print(\"crf : \",CRFnewSegment.seg(sentence)) print(\"crf_new : \",CRFnewSegment_new.seg(sentence)) print(\"viterbi : \",viterbiNewSegment.seg(sentence))\n[译智社/n, 的/u, 田丰/nr, 要/v, 说/v, 的/u, 是/v, 这/r, 只/d, 是/v, 一个/m, hanlp命名/vn, 实体/n, 识别/v, 的/u, 例子/n] ========== 命名实体开启与关闭对比试验 ========== crf : [北川/ns, 景子/n, 参演/v, 了/u, 林诣彬/nr, 导演/n, 的/u, 《/w, 速度/n, 与/c, 激情/n, 3/m, 》/w] crf_new : [北川/ns, 景子/n, 参演/v, 了/u, 林诣彬/nr, 导演/n, 的/u, 《/w, 速度/n, 与/c, 激情/n, 3/m, 》/w] viterbi : [北川景子/nrj, 参演/v, 了/ule, 林诣彬/nr, 导演/nnt, 的/ude1, 《/w, 速度/n, 与/cc, 激情/n, 3/m, 》/w] crf : [林志玲/nr, 亮相/v, 网友/n, :/w, 确定/v, 不/d, 是/v, 波多野/n, 结衣/n, ？/w] crf_new : [林志玲/nr, 亮相/v, 网友/n, :/w, 确定/v, 不/d, 是/v, 波多野/n, 结衣/n, ？/w] viterbi : [林志玲/nr, 亮相/vi, 网友/n, :/w, 确定/v, 不是/c, 波多野结衣/nrj, ？/w] crf : [龟/v, 山/n, 千/m, 广/q, 和/c, 近藤/a, 公园/n, 在/p, 龟山公园/ns, 里/f, 喝/v, 酒/n, 赏/v, 花/n] crf_new : [龟/v, 山/n, 千/m, 广/q, 和/c, 近藤/a, 公园/n, 在/p, 龟山公园/ns, 里/f, 喝/v, 酒/n, 赏/v, 花/n] viterbi : [龟山千广/nrj, 和/cc, 近藤公园/nrj, 在/p, 龟山/nz, 公园/n, 里/f, 喝酒/vi, 赏花/nz]\n中国人名识别\n说明\n目前分词器基本上都默认开启了中国人名识别，比如HanLP.segment()接口中使用的分词器等等，用户不必手动开启；上面的代码只是为了强调。 有一定的误命中率，比如误命中关键年，则可以通过在data/dictionary/person/nr.txt加入一条关键年 A 1来排除关键年作为人名的可能性，也可以将关键年作为新词登记到自定义词典中。 如果你通过上述办法解决了问题，欢迎向我提交pull request，词典也是宝贵的财富。 建议NLP用户使用感知机或CRF词法分析器，精度更高。\n算法详解 《实战HMM-Viterbi角色标注中国人名识别》\nIn [ ]:\n# 中文人名识别 def demo_chinese_name_recognition(sentences): segment = HanLP.newSegment().enableNameRecognize(True); for sentence in sentences: term_list = segment.seg(sentence) print(term_list) print([i.word for i in term_list]) sentences = [ \"签约仪式前，秦光荣、李纪恒、仇和等一同会见了参加签约的企业家。\", \"武大靖创世界纪录夺冠，中国代表团平昌首金\", \"区长庄木弟新年致辞\", \"朱立伦：两岸都希望共创双赢 习朱历史会晤在即\", \"陕西首富吴一坚被带走 与令计划妻子有交集\", \"据美国之音电台网站4月28日报道，8岁的凯瑟琳·克罗尔（凤甫娟）和很多华裔美国小朋友一样，小小年纪就开始学小提琴了。她的妈妈是位虎妈么？\", \"凯瑟琳和露西（庐瑞媛），跟她们的哥哥们有一些不同。\", \"王国强、高峰、汪洋、张朝阳光着头、韩寒、小四\", \"张浩和胡健康复员回家了\", \"王总和小丽结婚了\", \"编剧邵钧林和稽道青说\", \"这里有关天培的有关事迹\", \"龚学平等领导说,邓颖超生前杜绝超生\",] demo_chinese_name_recognition(sentences) print(\"\\n========== 中文人名 基本默认已开启 ==========\\n\") print(CRFnewSegment.seg(sentences[0]))\n[签约/vi, 仪式/n, 前/f, ，/w, 秦光荣/nr, 、/w, 李纪恒/nr, 、/w, 仇和/nr, 等/udeng, 一同/d, 会见/v, 了/ule, 参加/v, 签约/vi, 的/ude1, 企业家/nnt, 。/w] ['签约', '仪式', '前', '，', '秦光荣', '、', '李纪恒', '、', '仇和', '等', '一同', '会见', '了', '参加', '签约', '的', '企业家', '。'] [武大靖/nr, 创/v, 世界/n, 纪录/n, 夺冠/vi, ，/w, 中国/ns, 代表团/n, 平昌/ns, 首/q, 金/b] ['武大靖', '创', '世界', '纪录', '夺冠', '，', '中国', '代表团', '平昌', '首', '金'] [区长/nnt, 庄木弟/nr, 新年/t, 致辞/vi] ['区长', '庄木弟', '新年', '致辞'] [朱立伦/nr, ：/w, 两岸/n, 都/d, 希望/v, 共创/v, 双赢/n, /w, 习/v, 朱/ag, 历史/n, 会晤/vn, 在即/vi] ['朱立伦', '：', '两岸', '都', '希望', '共创', '双赢', ' ', '习', '朱', '历史', '会晤', '在即'] [陕西/ns, 首富/n, 吴一坚/nr, 被/pbei, 带走/v, /w, 与/cc, 令计划/nr, 妻子/n, 有/vyou, 交集/v] ['陕西', '首富', '吴一坚', '被', '带走', ' ', '与', '令计划', '妻子', '有', '交集'] [据/p, 美国之音/n, 电台/nis, 网站/n, 4月/t, 28/m, 日/b, 报道/v, ，/w, 8/m, 岁/qt, 的/ude1, 凯瑟琳/nr, ·/w, 克/q, 罗尔/nr, （/w, 凤甫娟/nr, ）/w, 和/cc, 很多/m, 华裔/n, 美国/nsf, 小朋友/n, 一样/uyy, ，/w, 小小/z, 年纪/n, 就/d, 开始/v, 学/v, 小提琴/n, 了/ule, 。/w, 她/rr, 的/ude1, 妈妈/n, 是/vshi, 位/q, 虎妈/nz, 么/y, ？/w] ['据', '美国之音', '电台', '网站', '4月', '28', '日', '报道', '，', '8', '岁', '的', '凯瑟琳', '·', '克', '罗尔', '（', '凤甫娟', '）', '和', '很多', '华裔', '美国', '小朋友', '一样', '，', '小小', '年纪', '就', '开始', '学', '小提琴', '了', '。', '她', '的', '妈妈', '是', '位', '虎妈', '么', '？'] [凯瑟琳/nr, 和/cc, 露西/nr, （/w, 庐瑞媛/nr, ）/w, ，/w, 跟/p, 她们/rr, 的/ude1, 哥哥/n, 们/k, 有/vyou, 一些/m, 不同/a, 。/w] ['凯瑟琳', '和', '露西', '（', '庐瑞媛', '）', '，', '跟', '她们', '的', '哥哥', '们', '有', '一些', '不同', '。'] [王国强/nr, 、/w, 高峰/n, 、/w, 汪洋/n, 、/w, 张朝阳/nr, 光/n, 着/uzhe, 头/n, 、/w, 韩寒/nr, 、/w, 小/a, 四/m] ['王国强', '、', '高峰', '、', '汪洋', '、', '张朝阳', '光', '着', '头', '、', '韩寒', '、', '小', '四'] [张浩/nr, 和/cc, 胡健康/nr, 复员/v, 回家/vi, 了/ule] ['张浩', '和', '胡健康', '复员', '回家', '了'] [王总/nr, 和/cc, 小丽/nr, 结婚/vi, 了/ule] ['王总', '和', '小丽', '结婚', '了'] [编剧/nnt, 邵钧林/nr, 和/cc, 稽道青/nr, 说/v] ['编剧', '邵钧林', '和', '稽道青', '说'] [这里/rzs, 有/vyou, 关天培/nr, 的/ude1, 有关/vn, 事迹/n] ['这里', '有', '关天培', '的', '有关', '事迹'] [龚学平/nr, 等/udeng, 领导/n, 说/v, ,/w, 邓颖超/nr, 生前/t, 杜绝/v, 超生/vi] ['龚学平', '等', '领导', '说', ',', '邓颖超', '生前', '杜绝', '超生'] ========== 中文人名 基本默认已开启 ========== [签约/vn, 仪式/n, 前/f, ，/w, 秦光荣/nr, 、/w, 李纪恒/nr, 、/w, 仇和/nr, 等/u, 一同/d, 会见/v, 了/u, 参加/v, 签约/v, 的/u, 企业家/n, 。/w]\n音译人名识别\n说明\n目前分词器基本上都默认开启了音译人名识别，用户不必手动开启；上面的代码只是为了强调。 算法详解\n《层叠隐马模型下的音译人名和日本人名识别》\nIn [ ]:\n# 音译人名识别 sentences = [ \"一桶冰水当头倒下，微软的比尔盖茨、Facebook的扎克伯格跟桑德博格、亚马逊的贝索斯、苹果的库克全都不惜湿身入镜，这些硅谷的科技人，飞蛾扑火似地牺牲演出，其实全为了慈善。\", \"世界上最长的姓名是简森·乔伊·亚历山大·比基·卡利斯勒·达夫·埃利奥特·福克斯·伊维鲁莫·马尔尼·梅尔斯·帕特森·汤普森·华莱士·普雷斯顿。\", ] segment = HanLP.newSegment().enableTranslatedNameRecognize(True) for sentence in sentences: term_list = segment.seg(sentence) print(term_list) print(\"\\n========== 音译人名 默认已开启 ==========\\n\") print(CRFnewSegment.seg(sentences[0]))\n[一桶/nz, 冰水/n, 当头/vi, 倒下/v, ，/w, 微软/ntc, 的/ude1, 比尔盖茨/nrf, 、/w, Facebook/nx, 的/ude1, 扎克伯格/nr, 跟/p, 桑德博格/nrf, 、/w, 亚马逊/nrf, 的/ude1, 贝索斯/nrf, 、/w, 苹果/nf, 的/ude1, 库克/nr, 全都/d, 不惜/v, 湿身/nz, 入镜/nz, ，/w, 这些/rz, 硅谷/ns, 的/ude1, 科技/n, 人/n, ，/w, 飞蛾/n, 扑火/vn, 似/vg, 地/ude2, 牺牲/v, 演出/vn, ，/w, 其实/d, 全/a, 为了/p, 慈善/a, 。/w] [世界/n, 上/f, 最长/d, 的/ude1, 姓名/n, 是/vshi, 简森/nr, ·/w, 乔伊/nr, ·/w, 亚历山大/nr, ·/w, 比基/nr, ·/w, 卡利斯/nr, 勒/v, ·/w, 达夫·埃利奥特·福克斯·伊维鲁莫·马尔尼·梅尔斯·帕特森·汤普森·华莱士·普雷斯顿/nrf, 。/w] ========== 音译人名 默认已开启 ========== [一桶/m, 冰水/n, 当头/d, 倒下/v, ，/w, 微软/a, 的/u, 比尔盖茨/n, 、/w, Facebook/l, 的/u, 扎克伯格/n, 跟/p, 桑德博格/n, 、/w, 亚马逊/nr, 的/u, 贝索斯/nr, 、/w, 苹果/n, 的/u, 库克/nr, 全都/d, 不惜/v, 湿身/n, 入镜/v, ，/w, 这些/r, 硅谷/n, 的/u, 科技/n, 人/n, ，/w, 飞蛾/v, 扑火似/v, 地/u, 牺牲/v, 演出/v, ，/w, 其实/d, 全/d, 为了/p, 慈善/a, 。/w] ---------------------\n日本人名识别\n说明\n目前标准分词器默认关闭了日本人名识别，用户需要手动开启；这是因为日本人名的出现频率较低，但是又消耗性能。 算法详解\n《层叠隐马模型下的音译人名和日本人名识别》\nIn [ ]:\n# 日语人名识别 def demo_japanese_name_recognition(sentences): segment = HanLP.newSegment().enableJapaneseNameRecognize(True) for sentence in sentences: term_list = segment.seg(sentence) print(term_list) print([i.word for i in term_list]) sentences =[ \"北川景子参演了林诣彬导演的《速度与激情3》\", \"林志玲亮相网友:确定不是波多野结衣？\", \"龟山千广和近藤公园在龟山公园里喝酒赏花\", ] demo_japanese_name_recognition(sentences) print(\"\\n========== 日文人名 标准分词器默认未开启 ==========\\n\") print(CRFnewSegment.seg(sentences[0]))\n[北川景子/nrj, 参演/v, 了/ule, 林诣彬/nr, 导演/nnt, 的/ude1, 《/w, 速度/n, 与/cc, 激情/n, 3/m, 》/w] ['北川景子', '参演', '了', '林诣彬', '导演', '的', '《', '速度', '与', '激情', '3', '》'] [林志玲/nr, 亮相/vi, 网友/n, :/w, 确定/v, 不是/c, 波多野结衣/nrj, ？/w] ['林志玲', '亮相', '网友', ':', '确定', '不是', '波多野结衣', '？'] [龟山千广/nrj, 和/cc, 近藤公园/nrj, 在/p, 龟山/nz, 公园/n, 里/f, 喝酒/vi, 赏花/nz] ['龟山千广', '和', '近藤公园', '在', '龟山', '公园', '里', '喝酒', '赏花'] ========== 日文人名 标准分词器默认未开启 ========== [北川/ns, 景子/n, 参演/v, 了/u, 林诣彬/nr, 导演/n, 的/u, 《/w, 速度/n, 与/c, 激情/n, 3/m, 》/w] ---------------------\n地名识别\n说明\n目前标准分词器都默认关闭了地名识别，用户需要手动开启；这是因为消耗性能，其实多数地名都收录在核心词典和用户自定义词典中。 在生产环境中，能靠词典解决的问题就靠词典解决，这是最高效稳定的方法。 建议对命名实体识别要求较高的用户使用感知机词法分析器。 算法详解\n《实战HMM-Viterbi角色标注地名识别》\nIn [ ]:\n# 演示数词与数量词识别 sentences = [ \"十九元套餐包括什么\", \"九千九百九十九朵玫瑰\", \"壹佰块都不给我\", \"９０１２３４５６７８只蚂蚁\", \"牛奶三〇〇克*2\", \"ChinaJoy“扫黄”细则露胸超2厘米罚款\", ] StandardTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.StandardTokenizer\") StandardTokenizer.SEGMENT.enableNumberQuantifierRecognize(True) for sentence in sentences: print(StandardTokenizer.segment(sentence)) print(\"\\n========== 演示数词与数量词 默认未开启 ==========\\n\") CRFnewSegment.enableNumberQuantifierRecognize(True) print(CRFnewSegment.seg(sentences[0]))\n[十九元/mq, 套餐/n, 包括/v, 什么/ry] [九千九百九十九朵/mq, 玫瑰/n] [壹佰块/mq, 都/d, 不/d, 给/p, 我/rr] [９０１２３４５６７８只/mq, 蚂蚁/n] [牛奶/nf, 三〇〇克/mq, */w, 2/m] [ChinaJoy/nx, “/w, 扫黄/vi, ”/w, 细则/n, 露/v, 胸/ng, 超/v, 2厘米/mq, 罚款/vi] ========== 演示数词与数量词 默认未开启 ========== [十九/m, 元/q, 套餐/n, 包括/v, 什么/r] ---------------------\n机构名识别\n说明\n目前分词器默认关闭了机构名识别，用户需要手动开启；这是因为消耗性能，其实常用机构名都收录在核心词典和用户自定义词典中。 HanLP的目的不是演示动态识别，在生产环境中，能靠词典解决的问题就靠词典解决，这是最高效稳定的方法。 建议对命名实体识别要求较高的用户使用感知机词法分析器。 算法详解\n《层叠HMM-Viterbi角色标注模型下的机构名识别》\nIn [ ]:\n# 机构名识别 sentences = [ \"我在上海林原科技有限公司兼职工作，\", \"我经常在台川喜宴餐厅吃饭，\", \"偶尔去开元地中海影城看电影。\", ] Segment = JClass(\"com.hankcs.hanlp.seg.Segment\") Term = JClass(\"com.hankcs.hanlp.seg.common.Term\") segment = HanLP.newSegment().enableOrganizationRecognize(True) for sentence in sentences: term_list = segment.seg(sentence) print(term_list) print(\"\\n========== 机构名 标准分词器已经全部关闭 ==========\\n\") print(CRFnewSegment.seg(sentences[0])) segment = HanLP.newSegment('crf').enableOrganizationRecognize(True)\n[我/rr, 在/p, 上海/ns, 林原科技有限公司/nt, 兼职/vn, 工作/vn, ，/w] [我/rr, 经常/d, 在/p, 台川喜宴餐厅/nt, 吃饭/vi, ，/w] [偶尔/d, 去/vf, 开元地中海影城/nt, 看/v, 电影/n, 。/w] ========== 机构名 标准分词器已经全部关闭 ========== [我/r, 在/p, 上海林原科技有限公司/nt, 兼职/vn, 工作/vn, ，/w] ---------------------\n地名识别\n说明\n目前标准分词器都默认关闭了地名识别，用户需要手动开启；这是因为消耗性能，其实多数地名都收录在核心词典和用户自定义词典中。 在生产环境中，能靠词典解决的问题就靠词典解决，这是最高效稳定的方法。 建议对命名实体识别要求较高的用户使用感知机词法分析器。 算法详解\n《实战HMM-Viterbi角色标注地名识别》\nIn [ ]:\n# 地名识别 def demo_place_recognition(sentences): segment = HanLP.newSegment().enablePlaceRecognize(True) for sentence in sentences: term_list = segment.seg(sentence) print(term_list) print([i.word for i in term_list]) sentences = [\"蓝翔给宁夏固原市彭阳县红河镇黑牛沟村捐赠了挖掘机\"] demo_place_recognition(sentences) print(\"\\n========== 地名 默认已开启 ==========\\n\") print(CRFnewSegment.seg(sentences[0]))\n[蓝翔/nr, 给/p, 宁夏/ns, 固原市/ns, 彭阳县/ns, 红河镇/ns, 黑牛沟村/ns, 捐赠/v, 了/ule, 挖掘机/n] ['蓝翔', '给', '宁夏', '固原市', '彭阳县', '红河镇', '黑牛沟村', '捐赠', '了', '挖掘机'] ========== 地名 默认已开启 ========== [蓝翔/v, 给/v, 宁夏/ns, 固原市/ns, 彭阳县/ns, 红河镇/ns, 黑牛沟村/ns, 捐赠/v, 了/u, 挖掘机/n] ---------------------\nURL 识别\n自动识别URL,该部分是在demo中发现的，但是原作者并没有在文档中提到这个，该部分可以发现URL，测试发现其他分类器应该是默认不开启这个的，而且config中并没有开启该功能的选项，因此这应该是一个额外的类。我建议如果有需要的，你可以尝试先利用URLTokenizer获取URL，然后添加进用户词典。或者直接使用其他工具或者自定义函数解决该问题。\nIn [ ]:\n# URL 识别 text = '''HanLP的项目地址是https://github.com/hankcs/HanLP， 发布地址是https://github.com/hankcs/HanLP/releases， 我有时候会在www.hankcs.com上面发布一些消息， 我的微博是http://weibo.com/hankcs/，会同步推送hankcs.com的新闻。 听说.中国域名开放申请了,但我并没有申请hankcs.中国,因为穷…… ''' Nature = SafeJClass(\"com.hankcs.hanlp.corpus.tag.Nature\") Term = SafeJClass(\"com.hankcs.hanlp.seg.common.Term\") URLTokenizer = SafeJClass(\"com.hankcs.hanlp.tokenizer.URLTokenizer\") term_list = URLTokenizer.segment(text) print(term_list) for term in term_list: if term.nature == Nature.xu: print(term.word)\nIn [ ]:\n[HanLP/nx, 的/ude1, 项目/n, 地址/n, 是/vshi, https://github.com/hankcs/HanLP/xu, ，/w, /w, /w, 发布/v, 地址/n, 是/vshi, https://github.com/hankcs/HanLP/releases/xu, ，/w, /w, /w, 我/rr, 有时候/d, 会/v, 在/p, www.hankcs.com/xu, 上面/f, 发布/v, 一些/m, 消息/n, ，/w, /w, /w, 我/rr, 的/ude1, 微博/n, 是/vshi, http://weibo.com/hankcs//xu, ，/w, 会/v, 同步/vd, 推送/nz, hankcs.com/xu, 的/ude1, 新闻/n, 。/w, /w, /w, 听说/v, ./w, 中国/ns, 域名/n, 开放/v, 申请/v, 了/ule, ,/w, 但/c, 我/rr, 并/cc, 没有/v, 申请/v, hankcs.中国/xu, ,/w, 因为/c, 穷/a, ……/w, /w, /w] https://github.com/hankcs/HanLP https://github.com/hankcs/HanLP/releases www.hankcs.com http://weibo.com/hankcs/ hankcs.com hankcs.中国","date":"2019年03月16日 22:45:06"}
{"_id":{"$oid":"5d36a8df6734bd8e681d5ecd"},"title":"自然语言处理4 -- 句法分析","author":"谢杨易","content":"系列文章，请多关注\nTensorflow源码解析1 – 内核架构和源码结构\n带你深入AI（1） - 深度学习模型训练痛点及解决方法\n自然语言处理1 – 分词\n自然语言处理2 – jieba分词用法及原理\n自然语言处理3 – 词性标注\n自然语言处理4 – 句法分析\n自然语言处理5 – 词向量\n自然语言处理6 – 情感分析\n1 概述\n句法分析也是自然语言处理中的基础性工作，它分析句子的句法结构（主谓宾结构）和词汇间的依存关系（并列，从属等）。通过句法分析，可以为语义分析，情感倾向，观点抽取等NLP应用场景打下坚实的基础。\n随着深度学习在NLP中的使用，特别是本身携带句法关系的LSTM模型的应用，句法分析已经变得不是那么必要了。但是，在句法结构十分复杂的长语句，以及标注样本较少的情况下，句法分析依然可以发挥出很大的作用。因此研究句法分析依然是很有必要的。\n2 句法分析分类\n句法分析分为两类，一类是分析句子的主谓宾 定状补的句法结构。另一类是分析词汇间的依存关系，如并列 从属 比较 递进等。下面详细讲解。\n2.1 句法结构分析\n句法结构分析，识别句子的主谓宾 定状补，并分析各成分之间的关系。如下图\n句子的核心HED为谓语“提出”，主语SBV为“李克强”，宾语VOB为“支持上海积极探索新机制”。这样我们就明确了句子的主干结构。再来看细节，对于主语“李克强”，其修饰定语ATT为“国务院总理”。对于谓语“提出”，其修饰状语ADV为“调研上海外高桥时”（这个状语内部还可以结构细分）。\n通过句法结构分析，我们就能够分析出语句的主干，以及各成分间关系。对于复杂语句，仅仅通过词性分析，不能得到正确的语句成分关系。比如动词谓语“提出”的主语，我们就可以知道是“李克强”，而不是离“提出”更近的同样是名词的“外高桥”了。\n句法结构分析的标注如下\n2.2 语义依存关系分析\n语义依存关系分析，识别词汇间的从属 并列 递进等关系，可以获得较深层的语义信息。如以下三个不同的表达方式，表达了同一个语义信息。可见语义依存关系不受句法结构的影响。\n语义依存关系偏向于介词等非实词的在语句中的作用，而句法结构分析则更偏向于名词 动词 形容词等实词。如张三 -\u003e 吃的关系为施加关系Agt，苹果-\u003e吃的关系为受事关系Pat。依存关系标注比较多，就不一一列举了。\n3 句法分析工具\n句法分析算法比较复杂，我们就不展开了。可以参考文章链接。介绍下几个句法分析工具。\n哈工大LTP，https://www.ltp-cloud.com/intro/。\n斯坦福句法分析工具Stanford Parser，https://nlp.stanford.edu/software/lex-parser.shtml\n当前句法分析难度还很大，准确度不高。哈工大的LTP也只能做到80%左右的准确率。\n4 深度学习和句法分析\n基于深度学习的RNN和LSTM序列模型，本身可以携带很多句法结构和依存关系等深层信息。同时，句法分析树结构也可以和深度学习结合起来。利用句法分析树可以构建LSTM网络（tree-lstm）, 从而对语句进行文本摘要，情感分析。那是否基于句法分析树的LSTM（tree-lstm）就一定比单纯的双向LSTM（bi-lstm）效果好吗？\n研究表明，很多情况下，单纯的bi-lstm，比基于句法分析树的tree-lstm效果更好\n这主要是因为当前句法分析准确度不高，只有90%左右。如果是句子成分关系很复杂，则准确率更低。因此给lstm网络带来了很大的噪声，从而导致了tree-lstm模型准确度的降低。但是tree-lstm可以使用较少的标注语料，而且在句子结构复杂的长语句上，表现更好。因此当语料较少且句子结构很复杂时，可以考虑使用tree-lstm。相关文章可以参考 哈工大车万翔：自然语言处理中的深度学习模型是否依赖于树结构？链接\n5 总结\n句法分析是自然语言处理中的基础性工作，在文本分析 观点抽取 情感分析等场景下可以广泛应用。句法分析当前难度还很高，准确率也有待提升。受制于句法分析准确率问题，基于句法结构树的LSTM深度学习网络的准确率还有待进一步提升。总之，句法分析，任重而道远。\n系列文章，请多关注\nTensorflow源码解析1 – 内核架构和源码结构\n带你深入AI（1） - 深度学习模型训练痛点及解决方法\n自然语言处理1 – 分词\n自然语言处理2 – jieba分词用法及原理\n自然语言处理3 – 词性标注\n自然语言处理4 – 句法分析\n自然语言处理5 – 词向量\n自然语言处理6 – 情感分析","date":"2018年08月23日 10:42:28"}
{"_id":{"$oid":"5d36a8df6734bd8e681d5ecf"},"title":"汉语自然语言处理工具包下载","author":"Font Tian","content":"Python Windows10\n汉语自然语言处理基本组件:\n\n20170703\n2018/01/16:Github长期更新,Windows,Linux通用","date":"2017年07月03日 15:22:47"}
{"_id":{"$oid":"5d36a8df6734bd8e681d5ed1"},"title":"数据挖掘之自然语言处理","author":"cold冷星辰","content":"NLTK是python上著名的自然语言处理库，自带语料库、词性分类库、自带分类、分词等等功能。\n安装语料库import nlk\nnlk.download()\n文本处理流程\n最后从文本转换为一组数字，这些数字就隐含了文本的意义。\nstopwords:对于注重理解文本意思的应用场景来说歧义太多，所以要去掉。\nNLTK在NLP上的经典应用有情感分析、文本相似度、文本分类。","date":"2019年04月06日 12:34:32"}
{"_id":{"$oid":"5d36a8e06734bd8e681d5ed4"},"title":"自然语言处理技术框架简要展示","author":"Paulzhao6518","content":"自然语言处理涉及到的相关技术，可以按照不同的分类标准、基于不同的观察视角进行划分。基于不同的分类原则，自然语言处理相关技术的分类结果也有所不同。在这里，我们主要采用两个分类原则进行划分，其一、基于分析对象语言单位粒度的不同：词汇级、句子级级和篇章级；其二、基于分析内容性质的不同：词法分析、语法分析、语义分析和语用分析。按照以上的分类标准，自然语言处理的主要技术分类结果如下图所示：","date":"2018年09月19日 18:05:08"}
{"_id":{"$oid":"5d36a8e16734bd8e681d5ed6"},"title":"自然语言处理(1)","author":"胖胖的票票","content":"自然语言处理-概述\n概述\n1.基本概念\n2.人类语言技术HLT发展简史\n3.HLT 研究内容\n4.基本问题和主要困难\n5.基本研究方法\n概述\n本系列文章计划总结整理中国科学院大学宗成庆老师《自然语言处理》课程相关知识，参考数目《统计自然语言处理》-第二版，宗成庆。\n1.基本概念\n语言学：(Linguistics) 研究语言本质、结构、和发展规律的科学。-商务印书馆，《现代汉语词典》，1996年\n自然语言： 人类特有的书面和口头形式的语言。\n自然语言理解(Natural Language Understanding,NLU)： 研究模仿人类语言认知过程的自然语言处理方法和实现技术的一门学科。 《计算机科学技术百科全书》第三版，P1223，宗成庆，黄昌宁\n计算语言学（Computation Linguistics,CL）： 通过建立形式化的计算模型来分析、理解和生成自然语言的学科，是人工智能和语言学的分支学科。计算语言学更加侧重基础理论和方法的研究《计算机科学技术百科全书》第三版，2018,5，P476，常宝宝\n自然语言处理（Natural Language Processing,NLP）： 自然语言处理是研究如何利用计算机技术对语言文本（句子、篇章或话语）等进行处理和加工的一门学科。 《计算机科学技术百科全书》第三版，P1223，宗成庆，黄昌宁\n人类语言技术（Human Language Technology,HLT）： 就字面意思理解，研究人类语言的技术。\n上个世纪五十年代，学术界对机器翻译产生了浓厚的兴趣；并得到了实业界的支持。因此国际上出现了研究机器翻译的热潮。随着机器翻译的发展，各种自然语言处理技术应运而生；并逐渐发展壮大，形成了这一语言学与计算机技术相结合的新兴学科。\n2.人类语言技术HLT发展简史\n1950s: 基于模板的NLP方法\n1960-1980s： 基于规则的方法\n1990-2013： 统计NLP方法\n2013~： 深度学习的方法\n3.HLT 研究内容\n机器翻译、信息检索、自动文摘、问答系统、信息过滤、信息抽取、文档文类、语音识别、说话人识别。有很多研究方向都密切相关。\n4.基本问题和主要困难\n基本问题： 形态学问题、句法问题、语义问题、语用学问题、语音学问题。\n主要困难：\n大量歧义现象：词法歧义、词性歧义、结构歧义、语义歧义、语音歧义（多音字歧义）。\n大量未知语言现象：随着社会生活的发展，每时每刻都会产生大量的具有新意义的词汇。\n5.基本研究方法\n1.理性主义会基于规则的分析方法建立符号处理系统。\n2.经验主义会基于大规模真实语料（语言真实数据）建立计算方法。","date":"2019年03月09日 22:25:30","data":"2019年03月09日 22:25:30"}
{"_id":{"$oid":"5d36a8e26734bd8e681d5eda"},"title":"自然语言处理基本过程理解","author":"lt77701","content":"接触自然语言已有两年，下面谈一谈自己的一些理解\n文本基本处理过程：\n\n\n1.获取数据。\n可以是任何文本类型的数据，自有的或者爬虫爬取的数据。曾经用几行代码写了一个爬虫，爬取了几万条商品评论还是很好用的。\n\n\n2.数据预处理。\n这一部分很重要！很重要！很重要！有可能会决定着你文本处理任务的最终质量！\n1)观察数据。尤其是网上的数据质量参差不齐，一定要先观察数据，有没有异常符号，有的时候有很多空格，或者会有换行，这些符号都要首先去掉。我观察的方式也很简单，把数据统一存储到一个csv文件里，正常情况下文本非常整齐。如果有特殊符号可能会让一段文本被分开成好多列。看到有分成好多列的情况那可能有特殊符号，处理了就行。\n2)分词，因为中文词语之间没有空格，所以一定要分词，分词的工具很多。做数据分析我一直用Python，推荐使用jieba分词，个人感觉很好用。\n3)去除停用词，停用词就是那些几乎出现在每一篇文本，数量很多但是没有区分度的词比如的，一，等等。如果做分类的话，这些词对处理结果有一定影响，所以可以考虑先去掉。停用词词表一般网上都有，可以找一个适合自己的用。\n\n\n3.特征工程\n做过比赛的人都知道特征工程有多重要了，文本特征需要自己构建，最简单的就是词袋模型，还可以用n-gram模型，ti-idf模型。但是这些模型共同的特点就是太稀疏了。一般情况下需要降维，比如SVD，其实很多模型也可以用来进行特征选择比如决策树，L1正则也可以用来进行特征选择，具体原理这里就不讲了。是不是很复杂，其实已有一个强大的工具帮我们做好了 sklearn，超级好用。\n\n\n4.正式干活。\n至此你的数据应该还算是比较干净了，可以开始做下一步工作。比如分类，聚类，命名实体识别，事件抽取，机器翻译。。。。太多了，还是很有意思的。欢迎交流，指导！","date":"2017年05月09日 20:00:16"}
{"_id":{"$oid":"5d36a8e26734bd8e681d5edc"},"title":"Python自然语言处理实战","author":"自然语言处理博客","content":"出版社： 机械工业出版社\nISBN：9787111597674\n出版时间：2018-06-01\n作者：涂铭，刘祥，刘树春\nPython自然语言处理实战","date":"2019年03月31日 17:46:20"}
{"_id":{"$oid":"5d36a8e26734bd8e681d5ede"},"title":"Python自然语言处理实战（1）：NLP基础","author":"CopperDong","content":"从建模的角度看，为了方便计算机处理，自然语言可以被定义为一组规则或符号的集合，我们组合集合中的符号来传递各种信息。自然语言处理研究表示语言能力、语言应用的模型，通过建立计算机框架来实现这样的语言模型，并且不断完善这样的语言模型，还需要根据语言模型来设计各种实用的系统，并且探讨这些实用技术的评测技术。从自然语言的角度出发，NLP基本可以分为两个部分：自然语言处理以及自然语言的生成，演化为理解和生成文本的任务。\n自然语言的理解是个综合的系统工程，它又包含了很多细分学科，有代表声音的音系学，代表构词法的词态学，代表语句结构的句法学，代表理解的语义句法学和语用学。\n音系学：指代语言中发音的系统化组织。\n词态学：研究单词构成以及相互之间的关系。\n句法学：给定文本的哪部分是语法正确的。\n语义学：给定文本的含义是什么？\n语用学：文本的目的是什么？\n自然语言生成恰恰相反，从结构化数据中以读取的方式自动生成文本。该过程主要包含三个阶段：文本规划（完成结构化数据中的基础内容规划）、语句规划（从结构化数据中组合语句来表达信息流）、实现（产生语法通顺的语句来表达文本）。\n1.2、NLP的研究任务\n机器翻译：计算机具备将一种语言翻译成另一种语言的能力。\n情感分析：计算机能够判断用户评论是否积极。\n智能问答：计算机能够正确回答输入的问题。\n文摘生成：能够准确归纳、总结并产生文本摘要。\n文本分类：能够采集各种文章，进行主题分析，从而进行自动分类。\n舆论分析：能够判断目前舆论的导向。\n知识图谱：知识点相互连接而成的语义网络。\n1.3、NLP相关知识的构成\n分词（segment）：词是最小的能够独立活动的有意义的语言成分，英文单词之间是以空格作为自然分界符的，而汉语是以字位基本的书写单位，词语之间没有明显的区分标记，因此，中文词语分析是中文分词的基础和关键。中文分词常用的手段是基于字典的最长串匹配，据说可以解决85%的问题，但是歧义分词很难。\n词性标注（part-of-speech tagging）：标注的目的是表征词的一种隐藏状态，隐藏状态构成的转移就构成了状态转移序列。\n命名实体识别（NER, Named Entity Recognition）：从文本中识别具有特定类别的实体（通常是名词）\n句法分析（syntax parsing）：往往是一种基于规则的专家系统。目的是解析句子中各个成分的依赖关系，可以解决传统词袋模型不考虑上下文的问题。\n指代消解（anaphora resolution）：中文中代词出现的频率很高\n情感识别（emotion recognition）：本质上是分类问题，通常可以基于词袋模型+分类器，或者现在流行的词向量模型+RNN。经过测试发现后者比前者准确率略有提升。\n纠错（correction）：具体做法有很多，可以基于N-Gram进行纠错，也可以通过字典树、有限状态机等方法进行纠错。\n问答系统（QA system）：往往需要语音识别、合成、自然语言理解、知识图谱等多项技术的配合才会实现得比较好。\n知识结构：NLP是研究人和机器之间用自然语言进行有效通信的理解和方法。这需要很多跨学科的知识，需要语言学、统计学、最优化理论、机器学习、深度学习以及自然语言处理相关理论模型知识做基础。\n句法语义分析：针对目标句子，进行各种句法分析，如分词、词性标记、命名实体识别及链接、句法分析、语义角色识别和多义词消歧等。\n关键词抽取：抽取目标文本中的主要信息，比如从一条新闻中抽取关机信息。主要是了解是谁、于何时、为何、对谁、做了何事、产生了有什么结果。涉及实体识别、时间抽取、因果关系抽取等多项关键技术。\n文本挖掘：主要包含了对文本的聚类、分类、信息抽取、摘要、情感分析以及对挖掘的信息和知识的可视化、交互式的呈现界面。\n机器翻译：将输入的源语言文本通过自动翻译转化为另一种语言的文本。根据输入数据类型的不同，可细分位文本翻译、语音翻译、手语翻译、图形翻译等。机器翻译从最早的基于规则到二十年前的基于统计的方法，再到今天的基于深度学习（编解码）的方法，逐渐形成了一套比较严谨的方法体系。\n信息检索：对大规模的文档进行索引。可简单对文档中的词汇，赋以不同的权重来建立索引，也可使用算法模型来建立更加深层的索引。查询时，首先对输入比进行分析，然后在索引里面查找匹配的候选文档，再根据一个排序机制把候选文档排序，最后输出排序得分最高的文档。\n问答系统：针对某个自然语言表达的问题，由问答系统给出一个精准的答案。需要对自然语言查询语句进行语义分析，包括实体链接、关系识别，形成逻辑表达式，然后到知识库中查找可能的候选答案并通过一个排序机制找出最佳的答案。\n对话系统：系统通过多回合对话，跟用户进行聊天、回答、完成某项任务。主要涉及用户意图理解、通用聊天引擎、问答引擎、对话管理等技术。此外，为了体现上下文相关，要具备多轮对话能力。同时，为了体现个性化，对话系统还需要基于用户画像做个性化回复。","date":"2018年07月14日 20:24:23"}
{"_id":{"$oid":"5d36a8e36734bd8e681d5ee0"},"title":"自然语言处理 (三) 之 word embedding","author":"周建丁","content":"文章介绍了著名的、通用的三种 word embedding 模型： LSI, PMI 和 skip-gram, 介绍了它们间的联系与区别，结论是： skip-gram 等价于分解一个 shifted PPMI 矩阵，它和SVD间没有本质的差异。\n原文链接：自然语言处理 (三) 之 word embedding","date":"2015年08月04日 09:22:40"}
{"_id":{"$oid":"5d36a8e36734bd8e681d5ee2"},"title":"Python调用哈工大语言云（LTP）API进行自然语言处理","author":"竹聿Simon","content":"哈工大语言云（语言技术平台云）是以哈工大社会计算与信息检索研究中心研发的“语言技术平台（LTP）” 为基础，提供高效精准的中文自然语言处理云服务。\n官网：http://www.ltp-cloud.com/\n使用python调用API实验，参考文档：http://www.ltp-cloud.com/document/\n\n\n1.注册：免费注册一个帐号\n注册网址：http://www.ltp-cloud.com/accounts/register/\n注册后获取调用语言云服务的token以及api_key（新版API的调用认证方式）。目前新注册用户将获得每月20G的免费流量。\n2.Python程序（注：32位 python 2.7.11，64位win7系统）\n（1）简单测试句子\n# -*- coding: utf-8 -*- \"\"\" 功能：哈工大语言云使用测试 时间：2016年4月9日 13:45:24 \"\"\" import urllib2 url_get_base = \"http://api.ltp-cloud.com/analysis/?\" api_key = '********替换为自己的API_KEY********’ # 输入注册API_KEY # 待分析的文本 text = \"这是一个测试文本\" format0 = 'xml' # 结果格式，有xml、json、conll、plain（不可改成大写） pattern = 'ws' # 指定分析模式，有ws、pos、ner、dp、sdp、srl和all result = urllib2.urlopen(\"%sapi_key=%s\u0026text=%s\u0026format=%s\u0026pattern=%s\" % (url_get_base, api_key, text, format0, pattern)) content = result.read().strip() print content\n\n\n（2）本地文本处理\n# -*- coding: utf-8 -*- \"\"\" 功能：哈工大语言云使用测试 时间：2016年4月12日 19:56:11 \"\"\" import urllib2 import codecs def ltp_cloud(par1): url_get_base = \"http://api.ltp-cloud.com/analysis/?\" api_key = '***********替换为自己的API_KEY***********' # 用户注册语言云服务后获得的认证标识 format0 = 'plain' # 结果格式，有xml、json、conll、plain（不可改成大写） pattern = 'ws' # 指定分析模式，有ws、pos、ner、dp、sdp、srl和all result1 = urllib2.urlopen(\"%sapi_key=%s\u0026text=%s\u0026format=%s\u0026pattern=%s\" % (url_get_base, api_key, par1, format0, pattern)) return result1.read().strip() f = open(r\"C:\\Users\\lenovo\\Desktop\\test.txt\", \"r\") # 待分析文本，已分句，每行一句。 savef = codecs.open(u\"C:\\\\Users\\\\lenovo\\\\Desktop\\\\out1.txt\", \"a\", \"utf-8\") # 结果存储 linenum = 0 newline = \"\" for line in f: linenum += 1 # 记录处理行数 newline += line.strip().replace(\"#\", \"\") # 删除行末空白符、干扰符号，以免影响URI if line[-1] != \"\\n\": # 如果处理到文本最后一行 if \" and \" and \" in \" in newline: print u\"需要更改单词in\" newline = newline.replace(\" in \", \" i.n \") print u\"已处理到文本最后一行：\", linenum savef.write(ltp_cloud(newline).decode(\"utf-8\") + \"\\n\") if len(newline) \u003e 6000: # 让文本足够长时再提交处理，最大值在8000左右 if \" and \" and \" in \" in newline: # 不能同时含有and和in两个词 print u\"需要更改单词in\" newline = newline.replace(\" in \", \" i.n \") print u\"处理到第\" + str(linenum) + u\"行\" savef.write(ltp_cloud(newline).decode(\"utf-8\") + \"\\n\") newline = \"\" savef.close() f.close()\n\n\n说明：\n[1]如果是本地文本，尽量一次提交尽可能多的文本，而不是一句一句提交，以提高请求效率。一次提交的文本有最大长度限制，在UTF-8编码下，单次解析的文本长度大约为2700个汉字（8100长度）。\n[2]提交的文本中不能有影响URI构造的特殊符号，目前已知的干扰符号有【# \u0026 ; +】四种；另外不知道为什么英文单词and和in不能同时存在于提交的文本中。\n[3]上述程序中读取的文本是已经分好句的，每行一句。不过语言云本身提供分句功能，因此可以直接提交没有分句的文本。其分句是根据中文标点符号【。？！；……】五种。","date":"2016年04月17日 17:17:07"}
{"_id":{"$oid":"5d36a8e46734bd8e681d5ee4"},"title":"自然语言处理基础技术之词性标注","author":"yuquanle","content":"声明：转载请注明出处，谢谢：https://blog.csdn.net/m0_37306360/article/details/84502176\n另外，更多实时更新的个人学习笔记分享，请关注：\n知乎：https://www.zhihu.com/people/yuquanle/columns\n公众号：StudyForAI\n今天总结一下自然语言处理之词性标注，后附现有比较好的开源实现工具(基于python实现包)~~~\n词性定义\n百度百科定义：词性指以词的特点作为划分词类的根据。词类是一个语言学术语，是一种语言中词的语法分类，是以语法特征（包括句法功能和形态变化）为主要依据、兼顾词汇意义对词进行划分的结果。\n维基百科定义：In traditional grammar, a part of speech (abbreviated form: PoS or POS) is a category of words (or, more generally, of lexical items) which have similar grammatical properties.\n从组合和聚合关系来说，一个词类是指：在一个语言中，众多具有相同句法功能、能在同样的组合位置中出现的词，聚合在一起形成的范畴。词类是最普遍的语法的聚合。词类划分具有层次性。如汉语中，词可以分成实词和虚词，实词中又包括体词、谓词等，体词中又可以分出名词和代词等。\n词性标注就是在给定句子中判定每个词的语法范畴，确定其词性并加以标注的过程，这也是自然语言处理中一项非常重要的基础性工作，所有对于词性标注的研究已经有较长的时间，在研究者长期的研究总结中，发现汉语词性标注中面临了许多棘手的问题。\n中文词性标注的难点\n汉语是一种缺乏词形态变化的语言，词的类别不能像印欧语那样，直接从词的形态变化上来判别。\n常用词兼类现象严重。《现代汉语八百词》收取的常用词中，兼类词所占的比例高达22.5%，而且发现越是常用的词，不同的用法越多。由于兼类使用程度高，兼类现象涉及汉语中大部分词类，因而造成在汉语文本中词类歧义排除的任务量大。\n研究者主观原因造成的困难。语言学界在词性划分的目的、标准等问题上还存在分歧。目前还没有一个统的被广泛认可汉语词类划分标准，词类划分的粒度和标记符号都不统一。词类划分标准和标记符号集的差异，以及分词规范的含混性，给中文信息处理带来了极大的困难。\n词性标注常见方法\n基于规则的词性标注方法\n基于规则的词性标注方法是人们提出较早的一种词性标注方法，其基本思想是按兼类词搭配关系和上下文语境建造词类消歧规则。早期的词类标注规则一般由人工构建。\n随着标注语料库规模的增大，可利用的资源也变得越来越多，这时候以人工提取规则的方法显然变得不现实，于是乎，人们提出了基于机器学习的规则自动提出方法。\n基于统计模型的词性标注方法\n统计方法将词性标注看作是一个序列标注问题。其基本思想是：给定带有各自标注的词的序列，我们可以确定下一个词最可能的词性。\n现在已经有隐马尔可夫模型（HMM）或条件随机域（CRF）等统计模型了，这些模型可以使用有标记数据的大型语料库进行训练，而有标记的数据则是指其中每一个词都分配了正确的词性标注的文本。\n基于统计方法与规则方法相结合的词性标注方法\n理性主义方法与经验主义相结合的处理策略一直是自然语言处理领域的专家们不断研究和探索的问题，对于词性标注问题当然也不例外。\n这类方法的主要特点在于对统计标注结果的筛选，只对那些被认为可疑的标注结果，才采用规则方法进行歧义消解，而不是对所有情况都既使用统计方法又使用规则方法。\n基于深度学习的词性标注方法\n可以当作序列标注的任务来做，目前深度学习解决序列标注任务常用方法包括LSTM+CRF、BiLSTM+CRF等。\n**词性标注任务数据集 **\n人民日报1998词性标注数据集：https://pan.baidu.com/s/1fW908EQmyMv0XB5i0DhVyQ\n词性标注工具推荐\nJieba：“结巴”中文分词：做最好的 Python 中文分词组件，可以进行词性标注。\nGithub地址：https://github.com/fxsjy/jieba\nSnowNLP：SnowNLP是一个python写的类库，可以方便的处理中文文本内容。\nGithub地址：https://github.com/isnowfy/snownlp\nTHULAC：THULAC（THU Lexical Analyzer for Chinese）由清华大学自然语言处理与社会人文计算实验室研制推出的一套中文词法分析工具包，具有中文分词和词性标注功能。\nGithub地址：https://github.com/thunlp/THULAC\n官网：http://thulac.thunlp.org/\nStanfordCoreNLP：斯坦福的，具备各种nlp功能，包括词性标注。\nGithub地址：https://github.com/Lynten/stanford-corenlp\n官网：https://stanfordnlp.github.io/CoreNLP/\nHanlp：HanLP是一系列模型与算法组成的NLP工具包，由大快搜索主导并完全开源，目标是普及自然语言处理在生产环境中的应用。\nGithub地址：https://github.com/hankcs/pyhanlp\n官网：http://hanlp.linrunsoft.com/\nNLTK：NLTK是一个高效的Python构建的平台,用来处理人类自然语言数据。\nGithub地址：https://github.com/nltk/nltk\n官网：http://www.nltk.org/\nSpaCy：工业级的自然语言处理工具，遗憾的是不支持中文。\nGihub地址：https://github.com/explosion/spaCy\n官网：https://spacy.io/\n最新研究进展看这里：https://github.com/sebastianruder/NLP-progress/blob/master/english/part-of-speech_tagging.md\n参考：\n1.统计自然语言处理","date":"2018年11月25日 20:35:18"}
{"_id":{"$oid":"5d36a8e46734bd8e681d5ee6"},"title":"自然语言处理资源备忘录","author":"kaiyuan_sjtu","content":"写在前面\n整理一些质量不错的自然语言处理资源，备忘录性质。\n持续更新~\nAwesome Github\nAwesome Artificial Intelligence (AI)\nAwesome Deep Learning\nAwesome Machine Learning\nawesome-nlp\nNLP-progress\nYSDA Natural Language Processing course\nnlp-datasets\nawesome-chinese-nlp\nAwesome Deep Learning for Natural Language Processing (NLP)\nawesome-sentence-embedding\nAwesome Artificial Intelligence use cases\nBooks \u0026 Videos\n数学之美: 吴军博士\n自然语言处理综论\n[统计自然语言处理]（宗成庆）\n神经网络与深度学习 : 复旦大学 邱锡鹏\nNLTK with Python 3 for Natural Language Processing\nCS224n: Natural Language Processing with Deep Learning：这个不用说了，斯坦福cs224n\nIntroduction to Natural Language Processing：密歇根大学 自然语言处理导论\nNatural Language Processing：哥伦比亚大学 NLP\nNatural Language Understanding and Computational Semantics :NYU\nInteresting blogs\ngeneralized language model 系列博客\nGoogle AI Blog\ncolah’s blog\nSebastian Ruder\nMostafa Dehghani\nAndrej Karpathy blog\nJay Alammar\nNLP Highlights hosted by Matt Gardner and Waleed Ammar\nnatural language processing blog\nDetermined22\n我爱自然语言处理\n徐阿衡\n立委NLP频道\n羊肉泡馍与糖蒜","date":"2019年05月26日 15:31:27"}
{"_id":{"$oid":"5d36a8e56734bd8e681d5ee8"},"title":"16 NLP 走近自然语言处理","author":"贾辛洪","content":"概念\nNatural Language Processing/Understanding，自然语言处理/理解\n日常对话、办公写作、上网浏览\n希望机器能像人一样去理解，以人类自然语言为载体的文本所包含的信息，并完成一些特定任务\n内容\n中文分词、词性标注、命名实体识别、关系抽取、关键词提取、信息抽取、依存分析、词嵌入……\n应用\n篇章理解、文本摘要、情感分析、知识图谱、文本翻译、问答系统、聊天机器人……","date":"2018年08月18日 15:51:46"}
{"_id":{"$oid":"5d36a8e56734bd8e681d5eeb"},"title":"自然语言处理实战之微博情感偏向分析","author":"白马负金羁","content":"自然语言处理（NLP）中一个很重要的研究方向就是语义的情感分析（Sentiment Analysis）。例如IMDB上有很多关于电影的评论，那么我们就可以通过Sentiment Analysis来评估某部电影的口碑，（如果它才刚刚上映的话）甚至还可以据此预测它是否能够卖座。与此相类似，国内的豆瓣上也有很多对影视作品或者书籍的评论内容亦可以作为情感分析的语料库。对于那些电子商务网站而言，针对某一件商品，我们也可以看到留言区里为数众多的评价内容，那么同类商品中，哪个产品最受消费者喜爱呢？或许对商品评论的情感分析可以告诉我们答案。\n在之前的文章中，笔者已经向读者介绍了在Python中利用NLTK进行自然语言处理的一些基本方法：\n利用NLTK在Python下进行自然语言处理\nPython自然语言处理：词干、词形与MaxMatch算法\n同时，我也介绍了在Python中利用Scikit-Learn进行机器学习，尤其是是利用LogisticRegression进行分类的基本方法：\nPython机器学习之Logistic回归\n下面本文将在这些文章的基础之上，尝试将机器学习和自然语言处理结合起来，以Tweet文为例，演示进行Sentiment Analysis的基本方法。首先需要说明的是内容有三点：\n1）下面的例子仍然主要使用Python中NLTK和Scikit-Learn两个函数库。\n2）SemEval 是NLP领域的带有竞赛性质的年度盛会，类似KDD-Cup。SemEval 创始于1998年，今年（2016）的活动主页为http://alt.qcri.org/semeval2016/  , 下面程序中所使用的数据即来自 SemEval 2016 的Task（当然在使用时我们已经完成了基本的预处理过程，而这并非本文的重点，我们略去不表）。\n\n\n\n3）我们所演示的方法，主要目的在于帮助大家熟悉Sentiment Analysis的基本内容，深化Scikit-Learn函数库的使用，而且我们所分析的数据来自于实际数据集，而非模拟数据集，所以最终的分析结果并不保证得到非常高的准确率。要得到更高的准确率，需要在模型构建和特征选择上做更深层次的思考。而这些“思考”已经超出本博文所讨论的范围。\n\n\n我们原始的数据是一条一条的Tweet，例如：\nTop 5 most searched for Back-to-School topics -- the list may surprise you http://t.co/Xj21uMVo0p  @bing @MSFTnews #backtoschool @Microsoft\n@taehongmin1 We have an IOT workshop by @Microsoft at 11PM on the Friday - definitely worth going for inspiration! #HackThePlanet\n当然，我们同时还拥有一个list of labels，即对每条Tweet的Polarity进行评定的标签，其中：+1表示positive, -1表示negative，0表示neutral。\n\n\n在预处理阶段，我对每条Tweet进行了分句和分词，然后：1）剔除了@***这样的内容；2）对于#引导的Topic，我们将其视为一个独立的句子进行处理；3）删除了由http引导的网络地址；4）统一了大小写。所以上述两个Tweet处理之后将得到下面两个结果\n\n[['top', '5', 'most', 'searched', 'for', 'back', '-', 'to', '-', 'school', 'topics', '--', 'the', 'list', 'may', 'surprise', 'you', '.'], ['back', 'to', 'school', '.']]\n[['we', 'have', 'an', 'iot', 'workshop', 'by', 'at', '11pm', 'on', 'the', 'friday', '-', 'definitely', 'worth', 'going', 'for', 'inspiration', '!'], ['.'], ['hack', 'the', 'planet', '.']]\n然后，我们根据训练数据集创建一个词袋（BOW，bag-of-word），这个词袋是一个字典，里面存储着所有训练数据集中出现过的词汇，以及它们在全文中出现的频数。这样做的目的，在于我们期望剔除那些在全部训练数据集中极少出现的词汇（生僻词），以及那些频繁出现但毫无意义的词汇（通常我们称之为停词 stop words，例如 the, of, a等）。\n\n\n\n在BOW基础之上，接下来就可以为每条Tweet创建创建 feature dictionaries了。特征字典是指每条Tweet中出现在BOW中的词（即剔除了罕见的生僻词和停词）以及它们在该条Tweet中出现的频数构成的字典。\n{'-': 2, '--': 1, '.': 2, '5': 1, 'back': 2, 'list': 1, 'may': 1, 'school': 2, 'searched': 1, 'surprise': 1, 'top': 1, 'topics': 1}\n\n{'!': 1, '-': 1,  '.': 2, '11pm': 1, 'definitely': 1, 'friday': 1, 'going': 1, 'hack': 1, 'inspiration': 1, 'iot': 1, 'planet': 1, 'workshop': 1, 'worth': 1}\n\n\n\n到此为止，所有的预处理工作都已经完成了。我们得到了一个list of dicts 形式的训练数据集（以及它对应的list of labels），和一个list of dicts形式的测试数据集（以及它对应的list of labels）。但是现在问题来了，这种形式的数据显然不能被直接使用。回忆一下我们在前篇介绍Logistic Regression的文章中所使用的鸢尾花数据集的样子，便不难发现与当前我们所拥有的数据形式大相径庭。这时就要借助于Scikit-Learn中提供的特征提取（Feature Extraction）模块。\nThe sklearn.feature_extraction module can be used to extract features in a format supported by machine learning algorithms from datasets consisting of formats such as text and image.\n更直接的说我们将有借助的函数是DictVectorizer，The class DictVectorizer can be used to convert feature arrays represented as lists of standard Python dict objects to the NumPy/SciPy representation used by scikit-learn estimators.\n\n\n如果你对Scikit-Learn文档中的这些描述感到困惑，那么下面的例子将让你很容易理解其作用。首先，我们给出它的定义原型：\n\nclass sklearn.feature_extraction.DictVectorizer(dtype=\u003cclass'numpy.float64'\u003e, separator='=', sparse=True, sort=True)\n其中sparse是一个布尔类型的参数，用于指示是否将结果转换成scipy.sparse matrices，即稀疏矩阵，缺省情况下其赋值为True。\n\n\n来看一个例子，measurements是一个list of dicts，我们把它转化成矩阵表示，当对应位置出现某个城市名时，其对应行的那一列就被置为1，否则就是0。\n\n\u003e\u003e\u003e from sklearn.feature_extraction import DictVectorizer \u003e\u003e\u003e measurements = [ {'city': 'Dubai', 'temperature': 33.}, {'city': 'London', 'temperature': 12.}, {'city': 'San Fransisco', 'temperature': 18.}, ] \u003e\u003e\u003e vec = DictVectorizer() \u003e\u003e\u003e vec.fit_transform(measurements).toarray() array([[ 1., 0., 0., 33.], [ 0., 1., 0., 12.], [ 0., 0., 1., 18.]]) \u003e\u003e\u003e vec.get_feature_names() ['city=Dubai', 'city=London', 'city=San Fransisco', 'temperature']\n再来一个补充例子\n\u003e\u003e\u003e measurements = [ {'city=Dubai': True, 'city=London': True, 'temperature': 33.}, {'city=London': True, 'city=San Fransisco': True, 'temperature': 12.}, {'city': 'San Fransisco', 'temperature': 18.},] \u003e\u003e\u003e vec.fit_transform(measurements).toarray() array([[ 1., 1., 0., 33.], [ 0., 1., 1., 12.], [ 0., 0., 1., 18.]])\n\n另外的一个常见问题是训练数据集和测试数据集的字典大小不一致，此时我们希望短的那个能够通过补零的方式来追平长的那个。这时就需要使用transform。还是来看例子：\n\u003e\u003e\u003e D = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}] \u003e\u003e\u003e v = DictVectorizer(sparse=False) \u003e\u003e\u003e X = v.fit_transform(D) \u003e\u003e\u003e X array([[ 2., 0., 1.], [ 0., 1., 3.]]) \u003e\u003e\u003e v.transform({'foo': 4, 'unseen_feature': 3}) array([[ 0., 0., 4.]]) \u003e\u003e\u003e v.transform({'foo': 4}) array([[ 0., 0., 4.]])\n\n可见当使用transform之后，后面的那个总是可以实现同前面的一个相同的维度。当然这种追平可以是补齐，也可以是删减，所以通常，我们都是用补齐短的这样的方式来实现维度一致。如果你不使用transform，而是继续fit_transform，则会得到下面的结果（这显然不能满足我们的要求）\n\u003e\u003e\u003e v.fit_transform({'foo': 4, 'unseen_feature': 3}) array([[ 4., 3.]])\n\n\n有了这样的认识，下面就可以为我们后续的Logistic Regression建立稀疏矩阵了，代码如下\nvec = DictVectorizer() sparse_matrix_tra = vec.fit_transform(feature_dicts_tra) sparse_matrix_dev = vec.transform(feature_dicts_dev)\n当然，这里你还可以用下面的代码来测试一下他们的维度是否按我们预想的那样\n\n\nprint(sparse_matrix_dev.shape) print(sparse_matrix_tra.shape)\n\n\n然后我们就可以利用之前用过的Logistic Regression来建立分类模型了。\n\n\n\nfrom sklearn import linear_model logreg = linear_model.LogisticRegression(C = 1) logreg.fit(sparse_matrix_tra, labels_t) prediction = logreg.predict(sparse_matrix_dev) print(logreg) print(\"accuracy score: \") print(accuracy_score(labels_d, prediction)) print(classification_report(labels_d, prediction))\n一同来看一下该模型对测试集的预测结果\nLogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1, penalty='l2', random_state=None, solver='liblinear', tol=0.0001, verbose=0, warm_start=False) accuracy score: 0.512848551121 precision recall f1-score support -1 0.41 0.28 0.33 360 0 0.46 0.69 0.55 700 1 0.68 0.46 0.55 769 avg / total 0.54 0.51 0.51 1829\n\n该Sentiment分类模型的准确率为51.28%。当然，正如我们前面所说，这个模型显然还有很大的改进空间。你完全可以通过引入新的feature，或者使用其他机器学习模型（或者调整模型参数）等多种途径来提升模型的准确率。但是本文旨在演示NLP中的Sentiment Analysis的基本步骤和策略，以及进一步演示利用Scikit Learn进行机器学习的更广泛的方法（例如基于字典的特征提取和引入稀疏矩阵）等方面的初衷已经完成了。有兴趣的读者完全可以在此基础上继续进行模型优化，以期实现更准确的分类能力。","date":"2016年04月14日 23:22:38"}
{"_id":{"$oid":"5d36a8e66734bd8e681d5ef0"},"title":"python自然语言处理课后答案","author":"Mr-Cat伍可猫","content":"持续更新中…\n使用的是《python自然语言处理》这本书，只给部分笔者做的答案，不敢保证都对，仅供参考\n\n我的目录\n持续更新中...\n第一章\n1.4\n1.6\n1.13\n1.14\n1.18\n1.21\n1.22\n2.23\n2.24\n2.25\n2.26\n2.28\n第一章\n1.4\nlen(text2) #先弄成都小写，去掉大小写区别，在求个数 len(set([w.lower() for w in set(a)]))\n1.6\ntext2.dispersion_plot(['Elinor','Marianne','Edward','Willoughby'])\n1.13\n\n\n1.14\n1.18\nsorted(set(sent1+sent2+sent3...))\n1.21\n1.22\nwords = sorted([w.lower() for w in text5 if len(w)==4]) fdist = FreqDist(words) fdist.most_common() #or plot the first 10 words fdist.plot(10)\n2.23\n列表：\n[w for w in text6 if w.isupper()]\n每行一个：\n\n2.24\n(a)\n[w for w in text6 if w.endswith('ize')]\n[]\n(b)\n\n\n可以看到上面有重复的单词，故将上面的text6改为set(text6)\n©\n\n(d)\n[w for w in set(text6) if w.istitle()]\n2.25\n2.26\n用来求全篇的字符/字母长度，如果要求字的平均长度可以如下\n\n2.28\ndef percent(word,text): return FreqDist(text)[word]/len(text)","date":"2019年05月02日 19:33:28"}
{"_id":{"$oid":"5d36a8e76734bd8e681d5ef2"},"title":"百度云 自然语言处理(Nlp)","author":"奔跑的蜗牛fzq","content":"这个自然语言处理功能十分强大,对一语话,可以进行类似我们以前的分词器分词效果,还能标记出可能倾向的搜索词.还有就是对语言的情感分析,文章的标签分类等等在商业场合应用都十分广泛的,来看这个小例子好像在微信小程序有看到\n\n\n\n\n以上是引自百度的一个ai体验中心,.....\n/** * 词法分析 */ @Test public void lexer(){ JavaAipNlp aipNlp = new JavaAipNlp(); HashMap\u003cString, Object\u003e options = new HashMap\u003cString, Object\u003e(); JSONObject lexer = aipNlp.lexer(\"好好学习天天向上!\", options); System.out.println(lexer.toString()); //{\"log_id\":664642641923030240,\"text\":\"好好学习天天向上!\",\"items\":[ // {\"formal\":\"\",\"loc_details\":[],\"item\":\"好好\",\"pos\":\"d\",\"ne\":\"\",\"basic_words\":[\"好\",\"好\"],\"byte_length\":4,\"byte_offset\":0,\"uri\":\"\"}, // {\"formal\":\"\",\"loc_details\":[],\"item\":\"学习\",\"pos\":\"v\",\"ne\":\"\",\"basic_words\":[\"学习\"],\"byte_length\":4,\"byte_offset\":4,\"uri\":\"\"}, // {\"formal\":\"\",\"loc_details\":[],\"item\":\"天天向上\",\"pos\":\"v\",\"ne\":\"\",\"basic_words\":[\"天天\",\"向上\"],\"byte_length\":8,\"byte_offset\":8,\"uri\":\"\"}, // {\"formal\":\"\",\"loc_details\":[],\"item\":\"!\",\"pos\":\"w\",\"ne\":\"\",\"basic_words\":[\"!\"],\"byte_length\":1,\"byte_offset\":16,\"uri\":\"\"}]}\n//词法分析（定制版） @Test public void lexerCustom(){ JavaAipNlp aipNlp = new JavaAipNlp(); HashMap\u003cString, Object\u003e options = new HashMap\u003cString, Object\u003e(); JSONObject lexer = aipNlp.lexer(\"广东省南山区科苑北清华信息港!\", options); System.out.println(lexer.toString());\n// {\"log_id\":2841336793035219062,\"text\":\"广东省南山区科苑北清华信息港!\",\"items\":[ // {\"formal\":\"\",\"loc_details\":[],\"item\":\"广东省\",\"pos\":\"\",\"ne\":\"LOC\",\"basic_words\":[\"广东\",\"省\"],\"byte_length\":6,\"byte_offset\":0,\"uri\":\"\"}, // {\"formal\":\"\",\"loc_details\":[],\"item\":\"南山区\",\"pos\":\"\",\"ne\":\"LOC\",\"basic_words\":[\"南山\",\"区\"],\"byte_length\":6,\"byte_offset\":6,\"uri\":\"\"}, // {\"formal\":\"\",\"loc_details\":[],\"item\":\"科苑\",\"pos\":\"n\",\"ne\":\"\",\"basic_words\":[\"科苑\"],\"byte_length\":4,\"byte_offset\":12,\"uri\":\"\"}, // {\"formal\":\"\",\"loc_details\":[],\"item\":\"北\",\"pos\":\"f\",\"ne\":\"\",\"basic_words\":[\"北\"],\"byte_length\":2,\"byte_offset\":16,\"uri\":\"\"}, // {\"formal\":\"\",\"loc_details\":[],\"item\":\"清华信息港\",\"pos\":\"\",\"ne\":\"ORG\",\"basic_words\":[\"清华\",\"信息\",\"港\"],\"byte_length\":10,\"byte_offset\":18,\"uri\":\"\"}, // {\"formal\":\"\",\"loc_details\":[],\"item\":\"!\",\"pos\":\"w\",\"ne\":\"\",\"basic_words\":[\"!\"],\"byte_length\":1,\"byte_offset\":28,\"uri\":\"\"}]} //LOC 地名\n//依存法 句法分析 @Test public void depParser (){ JavaAipNlp aipNlp = new JavaAipNlp(); HashMap\u003cString, Object\u003e options = new HashMap\u003cString, Object\u003e(); //模型选择。默认值为0，可选值mode=0（对应web模型）；mode=1（对应query模型 options.put(\"mode\",1); JSONObject lexer = aipNlp.depParser(\"我不想上班\", options); System.out.println(lexer.toString()); //{\"log_id\":8560515157495529056,\"text\":\"我不想上班\",\"items\":[ // {\"head\":2,\"deprel\":\"SBV\",\"postag\":\"n\",\"id\":1,\"word\":\"我不想\"}, // {\"head\":0,\"deprel\":\"HED\",\"postag\":\"v\",\"id\":2,\"word\":\"上班\"}]} }\n//DNN 语言模型 //中文DNN语言模型接口用于输出切词结果并给出每个词在句子中的概率值,判断一句话是否符合语言表达习惯。 //ppl float 描述句子通顺的值：数值越低，句子越通顺 resp_sample: //prob float 该词在句子中的概率值,取值范围[0,1] @Test public void dnnlmCn (){ // String words = \"我上下班飞机在河里漂浮\"; String words = \"我爱生活!\"; JavaAipNlp aipNlp = new JavaAipNlp(); HashMap\u003cString, Object\u003e options = new HashMap\u003cString, Object\u003e(); JSONObject lexer = aipNlp.dnnlmCn(words, options); System.out.println(lexer.toString()); //{\"log_id\":6371386997672135571,\"text\":\"我上下班飞机在河里漂浮\",\"items\":[ // {\"prob\":0.0161273,\"word\":\"我\"}, // {\"prob\":0.00229803,\"word\":\"上\"}, // {\"prob\":0.00197205,\"word\":\"下班\"}, // {\"prob\":1.35979E-5,\"word\":\"飞机\"}, // {\"prob\":0.0167389,\"word\":\"在\"}, // {\"prob\":3.04629E-4,\"word\":\"河里\"}, // {\"prob\":1.17134E-4,\"word\":\"漂浮\"}], // \"ppl\":1077.36} //===================================================================== // {\"log_id\":962095172634786721,\"text\":\"我爱生活!\",\"items\":[ // {\"prob\":0.0161273,\"word\":\"我\"}, // {\"prob\":0.0125896,\"word\":\"爱\"}, // {\"prob\":9.05624E-4,\"word\":\"生活\"}, // {\"prob\":0.0197345,\"word\":\"!\"}], }\n//词义相似度 // 输入两个词，得到两个词的相似度结果。 @Test public void wordSimEmbedding (){ String words1 = \"小\"; // 最大64kb String words2 = \"小\"; JavaAipNlp aipNlp = new JavaAipNlp(); HashMap\u003cString, Object\u003e options = new HashMap\u003cString, Object\u003e(); // options.put(\"mode\", 0); JSONObject lexer = aipNlp.wordSimEmbedding(words1,words2, options); System.out.println(lexer.toString()); //{\"log_id\":7955806838486346559,\"score\":1,\"words\":{\"word_1\":\"小\",\"word_2\":\"小\"}} //score 相似度的分数 1 为完全相似 }\n//短文本 相似度 // 输入两个短文本，得到两个词的相似度结果。 @Test public void simnet (){ String words1 = \"立马\"; String words2 = \"马上\"; JavaAipNlp aipNlp = new JavaAipNlp(); HashMap\u003cString, Object\u003e options = new HashMap\u003cString, Object\u003e(); options.put(\"model\", \"CNN\"); JSONObject lexer = aipNlp.simnet(words1,words2, options); System.out.println(lexer.toString()); //{\"log_id\":5656570856871633902,\"score\":0.580114,\"texts\":{\"text_1\":\"立马\",\"text_2\":\"马上\"}} }\n//评论观点抽取 // 评论观点抽取接口用来提取一条评论句子的关注点和评论观点，并输出评论观点标签及评论观点极性 /** * Type * 1 - 酒店 2 - KTV3 - 丽人 4 - 美食餐饮 5 - 旅游 6 - 健康 7 - 教育 8 - 商业 9 - 房产 10 - 汽车 11 - 生活 12 - 购物 13 - 3C */ @Test public void commentTag (){ JavaAipNlp aipNlp = new JavaAipNlp(); String text = \"这家餐馆味道很差\"; HashMap\u003cString, Object\u003e options = new HashMap\u003cString, Object\u003e(); JSONObject lexer = aipNlp.commentTag(text, ESimnetType.FOOD, options); System.out.println(lexer.toString()); // {\"log_id\":8456459865047604201,\"items\":[ // {\"sentiment\":0,\"adj\":\"差劲\",\"prop\":\"味道\",\"end_pos\":16,\"abstract\":\"这家餐馆\u003cspan\u003e味道很差\u003c\\/span\u003e\",\"begin_pos\":8}]} String hotel = \"喜来登酒店干净卫生\"; JSONObject result = aipNlp.commentTag(hotel, ESimnetType.HOTEL, options); System.out.println(result.toString()); //prop string 匹配上的属性词 //adj string 匹配上的描述词 // sentiment int 该情感搭配的极性（0表示消极，1表示中性，2表示积极） //begin_pos int 该情感搭配在句子中的开始位置 (干) //end_pos int 该情感搭配在句子中的结束位置 (生) // abstract string 对应于该情感搭配的短句摘要 //{\"log_id\":6206030619412743250,\"items\":[ // {\"sentiment\":2,\"adj\":\"卫生\",\"prop\":\"干净\",\"end_pos\":18,\"abstract\":\"喜来登酒店\u003cspan\u003e干净卫生\u003c\\/span\u003e\",\"begin_pos\":10}, // {\"sentiment\":2,\"adj\":\"干净\",\"prop\":\"卫生\",\"end_pos\":18,\"abstract\":\"喜来登酒店\u003cspan\u003e干净卫生\u003c\\/span\u003e\",\"begin_pos\":10}]} }\n/** * 情感倾向分析 * 对包含主观观点信息的文本进行情感极性类别（积极、消极、中性）的判断，并给出相应的置信度。 */ @Test public void sentimentClassify(){ JavaAipNlp aipNlp = new JavaAipNlp(); String text = \"淘宝上很多假货\"; HashMap\u003cString, Object\u003e options = new HashMap\u003cString, Object\u003e(); JSONObject lexer = aipNlp.sentimentClassify(text, options); System.out.println(lexer.toString()); // {\"log_id\":4774610278737884339,\n//\"text\":\"淘宝上很多假货\",\n//\"items\":[{\"positive_prob\":0.498948,\"sentiment\":1,\"confidence\":0.97895,\"negative_prob\":0.501053}]}\n/** * +sentiment 是 number 表示情感极性分类结果, 0:负向，1:中性，2:正向 +confidence 是 number 表示分类的置信度 +positive_prob 是 number 表示属于积极类别的概率 +negative_prob 是 number 表示属于消极类别的概率 */ }\n/* 文章标签 文章标签服务能够针对网络各类媒体文章进行快速的内容理解，根据输入含有标题的文章，输出多个内容标签以及对\n应的置信度，用于个性化推荐、相似文章聚合、文内容分析等场景。\n*/@Testpublic void keyword(){ String title = \"iphone手机出现“白苹果”原因及解决办法，用苹果手机的可以看下\"; String content = \"如果下面的方法还是没有解决你的问题建议来我们门店看下成都市锦江区红星路三段99号银石广场24层01室。\"; JavaAipNlp aipNlp = new JavaAipNlp(); HashMap\u003cString, Object\u003e options = new HashMap\u003cString, Object\u003e(); JSONObject lexer = aipNlp.keyword(title,content, options); System.out.println(lexer.toString()); /* {\"log_id\":3274746225884300396,\"items\":[ {\"score\":0.99775,\"tag\":\"iphone\"}, {\"score\":0.862602,\"tag\":\"手机\"}, {\"score\":0.845657,\"tag\":\"苹果\"}, {\"score\":0.837886,\"tag\":\"苹果公司\"}, {\"score\":0.811601,\"tag\":\"白苹果\"}, {\"score\":0.797911,\"tag\":\"数码\"}]} +tag 是 string 关注点字符串 +score 是 number 权重(取值范围0~1) */}\n/** * 文章分类 对文章按照内容类型进行自动分类，首批支持娱乐、体育、科技等26个主流内容类型，为文章聚类、\n文本内容分析等应用提供基础技术支持。 */ @Test public void topic (){ String title = \"欧洲冠军杯足球赛\"; String content = \"欧洲冠军联赛是欧洲足球协会联盟主办的年度足球比赛，\n代表欧洲俱乐部足球最高荣誉和水平，被认为是全世界最高素质、\" + \"最具影响力以及最高水平的俱乐部赛事，亦是世界上奖金最高的足球赛事和体育赛事之一。\"; JavaAipNlp aipNlp = new JavaAipNlp(); HashMap\u003cString, Object\u003e options = new HashMap\u003cString, Object\u003e(); JSONObject lexer = aipNlp.topic(title,content, options); System.out.println(lexer.toString()); /** * 返回参数说明 * +lv1_tag_list array of objects 一级分类结果 +lv2_tag_list array of objects 二级分类结果 实际返回参数: * {\"log_id\":6440401236167732852,\"item\":{\"lv2_tag_list\":[ * {\"score\":0.915631,\"tag\":\"足球\"}, * {\"score\":0.803507,\"tag\":\"国际足球\"}, * {\"score\":0.77813,\"tag\":\"英超\"}], * \"lv1_tag_list\":[{\"score\":0.830915,\"tag\":\"体育\"}]}} */ }\n像百度在大数据,人工智能这一块的业务做得已经很开了,就像调查问卷,你可以直接根据你想调查的行业和问题调用\n他们的接口数据,返回他们的真是调查结果给你.已经很厉害了,只是这个需要一元一份问卷,看似很贵,但是这是他们\n多年的数据积累啊.还有就是根据您的视频可以提取视频里面精彩部分作为帧图或者缩略图,等等很强大的接口,\n视频播放网站上那些我们看到的电影的缩略图是不是就是这样来的?","date":"2018年05月29日 02:33:11"}
{"_id":{"$oid":"5d36a8e76734bd8e681d5ef6"},"title":"自然语言处理之朴素贝叶斯小结","author":"VeeLe","content":"此篇博客为自然语言处理之朴素贝叶斯的总结\n更多详细信息参考超链接内容\n1.朴素贝叶斯=贝叶斯公式 + 条件独立假设\n2.朴素贝叶斯的效果好，尤其是在有大量语料的情况下。\n3.处理重复语句的三种方式\n4.处理未在训练集中覆盖的词语——平滑技术（赋予一个小概率，从而调低整体的概率）\n5.直接匹配关键词处理垃圾邮件，为何行不通。\n6.实际工程中的小技巧：\n取对数，把乘法变成加法.并预先把对应的概率求出来\n引入正常邮件出现词语的概率，把词语概率转化为权重，P垃圾邮件中的W/P正常邮件中的W.\n选取前N个关键词，需要经验\n分隔样本，因为样本长度不一，前N个关键词的占比不同\n给位单词所在置赋权\n7.如何处理多分类问题，忽略被判断的文本的概率，即用似然函数。\n8.先验概率是否准确或者相等的问题。\n贝叶斯方法，需要靠谱的先验概率，否则会在，最大似然法和基本的朴素贝叶斯得出不同地结果。作者给出的建议是，在处理多份类问题时，知道先验概率具体数值且不相等的情况下，考虑删除部分数据使得鲜艳概率相等，然后用最大似然法。（为什么不考虑基础的贝叶斯方法呢？） 如果不知道先验概率，就只能按等比例抽取样本，然后按先验概率相等的情况处理。\n9.朴素贝叶斯方法的常见应用：\n褒贬分析：\n(1)对否定句进行特别处理\n(2)最相关的情感词在片段中只出现一次，词频模型作用有限，用伯努利、多项式模型替代\n(3)考虑副词对情感的影响。（很不好，不是很好）\n难点：\n情绪的含蓄表达\n欲扬先抑等转折\n10.拼写纠错\n非词错误\u0026真词错误\n真词错误比较复杂。非词错误，可以直接采用贝叶斯方法\nP(候选词i|错误词)∝P(错误词|候选词i)P(候选词i)；i=1,2,3,\n一些小技巧：\n（1）经验发现，80%的瓶邪错误，编辑距离为1,几乎所有的拼写错误，编辑距离小于等于2.\n（2）键盘上临近按键，更容易拼写错误，可以按这个条件加权。","date":"2017年08月30日 21:51:53"}
{"_id":{"$oid":"5d36a8e86734bd8e681d5ef8"},"title":"某公司自然语言处理算法笔试题","author":"海天一树","content":"1 请列出几种文本特征提取算法\n答：文档频率、信息增益、互信息、X^2统计、TF-IDF\n2 简述几种自然语言处理开源工具包\n答：LingPipe、FudanNLP、OpenNLP、CRF++、Standord CoreNLP、IKAnalyzer\n3 简述无监督和有监督算法的区别\n答：\n（1）有监督学习：对具有概念标记（分类）的训练样本进行学习，以尽可能对训练样本集外的数据进行标记（分类）预测。这里，所有的标记（分类）是已知的。因此，训练样本的岐义性低。\n无监督学习：对没有概念标记（分类）的训练样本进行学习，以发现训练样本集中的结构性知识。这里，所有的标记（分类）是未知的。因此，训练样本的岐义性高。聚类就是典型的无监督学习\n（2）有监督学习的样本全部带标记，无监督学习的样本全部不带标记。\nPS:部分带标记的是半监督学习\n（3）训练集有输入有输出是有监督，包括所有的回归算法分类算法，比如线性回归、决策树、神经网络、KNN、SVM等；训练集只有输入没有输出是无监督，包括所有的聚类算法，比如k-means 、PCA、 GMM等\n4 请简述几种熟悉的分类算法\n答：kNN，kMeans，决策树，随机森林等\n5 以下代码是Java实现中文分词，请简述分词过程\npublic class SplitChineseCharacter { public static void main(String[] args) { String input = \"太好了，今天是星期六啊\"; new Split(input).start(); } } class Split { private String[] dictionary = {\"今天\", \"是\", \"星期\", \"星期六\"}; private String input = null; public Split(String input) { this.input = input; } public void start() { String temp = null; System.out.println(this.input.length()); for(int i = 0; i \u003c this.input.length(); i++) { temp = this.input.substring(i); if(this.isInDictionay(temp)) { System.out.println(temp); this.input = this.input.replace(temp, \"\"); i = - 1; } } if(null != this.input \u0026\u0026 !\"\".equals(this.input)) { this.input = this.input.substring(0, this.input.length() - 1); this.start(); } } public boolean isInDictionay(String temp) { for(int i = 0; i \u003c this.dictionary.length; i++) { if(temp.equals(this.dictionary[i])) { return true; } } return false; } }\n运行结果：\n星期六 是 今天\n\n\n\n更多内容请关注微信公众号","date":"2018年04月02日 22:59:57"}
{"_id":{"$oid":"5d36a8e96734bd8e681d5efc"},"title":"如何学习自然语言处理(52nlp)","author":"hello_pig1995","content":"如何学习自然语言处理(52nlp)\n1.几篇文章介绍相关技术\nnlper:\nhttp://nlpers.blogspot.my/\n2.标准书籍\n《统计自然语言处理基础》\n《自然语言处理综论》（涉猎广，有门槛）\n《自然语言理解》\nnltk工具包，《用python进行自然语言处理》 e－book:http://code.google.com/p/brishen/downloads/list\n3.ACL anthology\n或者到作者主页去进一步follow，或者去其导师主页看看是否有进一步相关文献\n4.多参加会议\n大体流程就是：博客入门－书籍系统了解－论文跟进－会议感受","date":"2016年05月30日 09:45:55"}
{"_id":{"$oid":"5d36a8ea6734bd8e681d5eff"},"title":"基于深度学习的自然语言处理","author":"自然语言处理博客","content":"基于深度学习的自然语言处理\n作者：约阿夫·戈尔德贝格（Yoav Goldberg）\n出版社： 机械工业出版社\nISBN：9787111593737\n出版时间：2018-05-01","date":"2019年04月01日 09:30:05"}
{"_id":{"$oid":"5d36a8eb6734bd8e681d5f02"},"title":"IBM Bluemix 自然语言处理初识","author":"sunfoot001","content":"今天注册了IBM Bluemix的30天免费账号，大概了解了下其提供的自然语言处理功能。\n这些自然语言处理还是浅层的处理，包括了识别概念、实体、关键字、类别， 观点、情绪、关系、语义角色，并不包含推理等深层处理。一个典型的自然语言处理pipline包括： 意图识别，实体识别，语气识别，上下文分析和知识扩展。\n\n\n基于Bluemix在云端实现一个领域内的chatbot不再是难事。\nBluemix这些开放NLP平台的出现，对于一些小公司的NLP团队来说不是好事了。\n一般的互联网开发人员掌握这些工具就可以开发一些NLP功能了。\n在云端，大公司有着各种优势；在嵌入式等端上语义理解，会是什么样的情景呢？","date":"2017年09月06日 22:02:48"}
{"_id":{"$oid":"5d36a8ec6734bd8e681d5f04"},"title":"统计学自然语言处理（语义消歧）","author":"continueOo","content":"概述\n本书本章描述自然语言处理中消除歧义的问题，并介绍几种重要的语义消歧算法，描述他们的资源需求和算法性能。消歧我们应该能直观的想象到就是一句话可能有几个意思。但是落实到具体细节中，我认为主要分以下几种：\n１．分词的消歧，这是很常见的一个例子(南京　市　长　江大桥)\n２．多义词的具体词义\n３．词性的判断\n对于词性的判断可以看做一个词性标注的问题词性标注的话，我们通常考虑邻近上下文。相反，如果是词义判决的话，可能会有相隔很远的词语来决定他的词性。因此大部分的词性标注模型简单地使用当前上下文，而语义消歧模型通常使用规模广泛一些的上下文中的实词。本章将会介绍３个方法：基于标注训练集的有监督消歧，基于词典的消歧，无监督消歧。\n性能上下界\n性能上界：相同情况下人工标注的性能，这里一定要强调相同判断情况，实际判决中，人往往会将该系统没有利用的特性加入进来，这里有几个思考：\n１．系统的性能是有上界的，我们判决时应该使用系统的视角来看待问题。不要全局来看，这样能意识到系统本身的缺陷。\n２．正是因为系统有了明显的上界缺陷，才使得我们有改进的方向，比如我们知道利用前后一个词不能判断语义，那么我们考虑的方向就变成了，看一句话我们能否进行这样的判断呢？我们做的是需要不断挖掘我们本能考虑但是系统没有考虑的内容和信息，讲其模型化。\n基于贝叶斯分类的语义消歧\n原理是，考虑一个上下文窗口中歧义词周围词的信息。通常我们这里使用一个特殊的分类器，即朴素贝叶斯分类器，这里使用朴素贝叶斯有两个假设：\n１．第一个是上下文中的所有结构和词语顺序都可以被忽略。\n２．可有重复的单词集中出现的词独立于其他词。\n虽然这两个假设不太成立，但是朴素贝叶斯能在这个任务中取得一定的效果。\n\n以下给出书中该方法得出的靠前的两个结果：\n\n一种基于信息论的方法\n贝叶斯采取的是利用上下文中所有的词来帮助消歧决策，而且事先还做了一个不太真是的独立假设。现在利用信息论的方法可以得到一个比较不同的策略。试图寻找一个单一的上下文特征，用来指示哪一种语义应该被使用。方法如下：（该方法适用于２语义词，多语义请看书中文献扩展）\n我们划分出语义集Ｐ１，Ｐ２，然后划分出指示集Ｑ1，Ｑ２，书中利用了一种叫flip-flop的方法来计算最大化的互信息量I(P;Q)。该方法是一个有效的线性时间算法，比暴力搜索效果要好。这里重点在于提出了指示器的集合，然后我们可以根据这个集合来判断我们想要的语义如何划分。下面给出书中集合结果：\n\n基于词典的消歧\n书中提供了一系列词典消歧的方法，准确率都不太高，而且思路也比较通俗，比如说利用语义不同定义里面的一些统计特性来区分，还有基于主题的分类。没有太多想细致去看的，大家有兴趣自己去看吧！\n无监督消歧\n书中仅仅提了一下无监督做法思路，模型与贝叶斯相似，随机初始化参数P(vj | sk)，然后ＥＭ算法重新估计，确认参数后利用贝叶斯判别规则决策。这里聚类我暂时不太清楚，以后补上～\n自己的体会\n这里插一下自己的想法，本章主要介绍了英文的语义消歧，这里语义更多的是在说词义。对于中文而言，词语不是硬性划分的。那么我们首先会存在的一个问题就是如何去划分词语。”南京市　长江大桥”，”南京　市长　江大桥”。在英文中是存在这个歧义的。所以以下介绍的内容，对于中文处理者来说仅仅提供一些基础方案吧。估计不太适用！","date":"2017年06月12日 11:20:31"}
{"_id":{"$oid":"5d36a8ec6734bd8e681d5f06"},"title":"第四课 kaggle自然语言处理","author":"gaoyishu91","content":"NLTK\n自然语言处理库，自带语料库，词性分类库。要记得安装语料库。\nimport nltk nltk.download()\nTokenize\n拆句子，拆小\n英文分词\nimport nltk sentence = 'hello world' tokens = nltk.word_tokenize(sentence) tokens\n社交语言的分词 表情符号需要用正则表达式去匹配\n中文分词 启发式Heuristic 或者 机器学习统计方法 可以用Jieba\nimport jieba seg_list = jieba.cut('啊啊啊',cut_all=Ture) #全模式 False是精准模式分词\n分词之后\n是一个数组，比如\nword = ['i','am','a','fool']\n英文单词比较复杂，为了降低复杂度，一般要经过词干提取(stemming)和词形归一(lemma)。\nwalking 变成 walk 去掉了ing的后缀，因为它不影响词性\nwent 变成 go 去掉了过去式模式，不影响词性\nfrom nlk.stem.porter import PorterStemmer porter_stemmer = PorterStemmer() porter_stemmer.stem('running') from nlk.stem.lancaster import LancasterStemmer lancaster_stemmer = LancasterStemmer() lancaster_stemmer.stem('probalility')\nfrom nltk.stem import WordNetLemmatizer\n为了更好的进行词性划分，因为一个单词有不同的意思，所以必须加入词性让它更加容易分离。\nNLTK有词性标注器，标注POS Tag\nimport nltk text = nltk.word_tokenize('i am a fool') text.pos_tag(text)\nStopwords 停止词\n以上就是对文本预处理的流程。\n文本预处理有什么用\n\n得到一个list，包含了句子中有意思的词，去掉了不需要的，然后进行feature化。\n进入竞赛内容","date":"2018年04月14日 17:30:48"}
{"_id":{"$oid":"5d36a8ed6734bd8e681d5f08"},"title":"NLTK02 《Python自然语言处理》code01 语言处理与Python","author":"longji","content":"01 语言处理与Python\n# -*- coding: utf-8 -*- # win10 python3.5.3/python3.6.1 nltk3.2.4 # 《Python自然语言处理》01 语言处理与Python # 安装nltk库 # pip3 install nltk==3.2.4 # 下载nltk数据，nltk_data ''' import nltk nltk.download() # 出现NLTK Downloader对话框后，设置[Download Directory]路径后，点击[Download]按钮开始下载，如果下载失败或者卡顿，重新下载。 ''' # 1.1 语言计算：文本和词汇 from __future__ import division from nltk.book import * ''' *** Introductory Examples for the NLTK Book *** Loading text1, ..., text9 and sent1, ..., sent9 Type the name of the text or sentence to view it. Type: 'texts()' or 'sents()' to list the materials. text1: Moby Dick by Herman Melville 1851 text2: Sense and Sensibility by Jane Austen 1811 text3: The Book of Genesis text4: Inaugural Address Corpus text5: Chat Corpus text6: Monty Python and the Holy Grail text7: Wall Street Journal text8: Personals Corpus text9: The Man Who Was Thursday by G . K . Chesterton 1908 ''' print(text1) '''\u003cText: Moby Dick by Herman Melville 1851\u003e''' print(text2) '''\u003cText: Sense and Sensibility by Jane Austen 1811\u003e''' # 搜索文本 result = text1.concordance(\"monstrous\") print(result) ''' Displaying 11 of 11 matches: ong the former , one was of a most monstrous size . ... This came towards us , ON OF THE PSALMS . \" Touching that monstrous bulk of the whale or ork we have r ll over with a heathenish array of monstrous clubs and spears . Some were thick d as you gazed , and wondered what monstrous cannibal and savage could ever hav that has survived the flood ; most monstrous and most mountainous ! That Himmal they might scout at Moby Dick as a monstrous fable , or still worse and more de th of Radney .'\" CHAPTER 55 Of the Monstrous Pictures of Whales . I shall ere l ing Scenes . In connexion with the monstrous pictures of whales , I am strongly ere to enter upon those still more monstrous stories of them which are to be fo ght have been rummaged out of this monstrous cabinet there is no telling . But of Whale - Bones ; for Whales of a monstrous size are oftentimes cast up dead u None ''' print(text1.similar(\"monstrous\")) ''' true contemptible christian abundant few part mean careful puzzled mystifying passing curious loving wise doleful gamesome singular delightfully perilous fearless None ''' text2.similar(\"monstrous\") ''' very so exceedingly heartily a as good great extremely remarkably sweet vast amazingly ''' text2.common_contexts([\"monstrous\", \"very\"]) '''a_pretty am_glad a_lucky is_pretty be_glad''' # 视图 #text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"]) text3.generate(\"very\") # 计数词汇 print(len(text3)) # 44764 print(sorted(set(text3))) ''' ['!', \"'\", '(', ')', ',', ',)', '.', '.)', ':', ';', ';)', '?', '?)', 'A', 'Abel', 'Abelmizraim', ... 'your', 'yourselves', 'youth'] ''' print(len(set(text3))) # 2789 print(len(text3)/len(set(text3))) # 16.050197203298673 print(text3.count(\"smote\")) # 5 print(100*text4.count('a')/len(text4)) # 1.4643016433938312 def lexical_diversity(text): return len(text)/len(set(text)) def percentage(count, total): return 100*count/total print(lexical_diversity(text3)) # 16.050197203298673 print(lexical_diversity(text5)) # 7.420046158918563 print(percentage(4, 5)) # 80.0 print(percentage(text4.count('a'), len(text4))) # 1.4643016433938312 # 1.2 近观Python：将文本当做词链表 # 链表 sent1 = ['Call', 'me', 'Ishamel', '.'] print(sent1, len(sent1)) '''['Call', 'me', 'Ishamel', '.'] 4''' print(lexical_diversity(sent1)) # 1.0 print(sent2) # ['The', 'family', 'of', 'Dashwood', 'had', 'long', 'been', 'settled', 'in', 'Sussex', '.'] print(sent3) # ['In', 'the', 'beginning', 'God', 'created', 'the', 'heaven', 'and', 'the', 'earth', '.'] l1 = ['Monty', 'Python'] + ['and', 'the', 'Holy', 'Grail'] print(l1) # ['Monty', 'Python', 'and', 'the', 'Holy', 'Grail'] l2 = sent4 + sent1 print(l2) # ['Fellow', '-', 'Citizens', 'of', 'the', 'Senate', 'and', 'of', 'the', 'House', 'of', 'Representatives', ':', 'Call', 'me', 'Ishamel', '.'] sent1.append(\"Some\") print(sent1) # ['Call', 'me', 'Ishamel', '.', 'Some'] # 索引列表 print(text4[173]) # awaken print(text4.index('awaken')) # 173 print(text5[16715:16735]) # ['U86', 'thats', 'why', 'something', 'like', 'gamefly', 'is', 'so', 'good', 'because', 'you', 'can', 'actually', 'play', 'a', 'full', 'game', 'without', 'buying', 'it'] print(text6[1600:1625]) # ['We', \"'\", 're', 'an', 'anarcho', '-', 'syndicalist', 'commune', '.', 'We', 'take', 'it', 'in', 'turns', 'to', 'act', 'as', 'a', 'sort', 'of', 'executive', 'officer', 'for', 'the', 'week'] sent = ['word1', 'word2', 'word3', 'word4', 'word5', 'word6', 'word7', 'word8', 'word9', 'word10'] print(sent[0], sent[9]) # word1 word10 # print(sent[10]) # IndexError: list index out of range print(sent[5:8]) # ['word6', 'word7', 'word8'] print(sent[5]) # word6 print(sent[6]) # word7 print(sent[7]) # word8 print(sent[:3]) # ['word1', 'word2', 'word3'] print(text2[141565:]) # ['themselves', ',', 'or', 'producing', 'coolness', 'between', 'their', 'husbands', '.', 'THE', 'END'] sent[0] = 'First' sent[9] = 'Last' print(len(sent)) # 10 sent[1:9] = ['Second', 'Third'] print(sent) # ['First', 'Second', 'Third', 'Last'] # print(sent[9]) # IndexError: list index out of range # 变量 sent1 = ['Call', 'me', 'Ismael', '.'] my_sent = ['Bravely', 'bold', 'Sir', 'Robin', ',', 'rode', 'forth', 'from', 'Camelot', '.'] noun_phrase = my_sent[1:4] print(noun_phrase) # ['bold', 'Sir', 'Robin'] wOrDs = sorted(noun_phrase) print(wOrDs) # ['Robin', 'Sir', 'bold'] # 关键字不能做变量使用 # not = 'Camelot' # SyntaxError: invalid syntax vocab = set(text1) vocab_size = len(vocab) print(vocab_size) # 19317 # 字符串 name = 'Monty' print(\"name[0]:\", name[0], \"\\nname:\", name, \"\\nname[:4]:\", name[:4]) ''' name[0]: M name: Monty name[:4]: Mont ''' print(name*2) # MontyMonty print(name + '!') # Monty! print(' '.join(['Monty', 'Python'])) # Monty Python print('Monty Python'.split()) # ['Monty', 'Python'] # 1.3 计算语言：简单的统计 saying = ['After', 'all', 'is', 'said', 'and', 'done', 'more', 'is', 'than', 'done'] tokens = set(saying) tokens = sorted(tokens) print(tokens[-2:]) # ['said', 'than'] # 概率分布 fdist1 = FreqDist(text1) print(fdist1) # \u003cFreqDist with 19317 samples and 260819 outcomes\u003e vocabulary1 = list(fdist1.keys()) print(vocabulary1[:5]) # ['[', 'Moby', 'Dick', 'by', 'Herman'] print(fdist1['whale']) # # fdist1.plot(50, cumulative=True) # 常用词累计频率图 # 细粒度的选择词 V = set(text1) long_words = [w for w in V if len(w) \u003e 15] print(sorted(long_words)) ''' ['CIRCUMNAVIGATION', 'Physiognomically', 'apprehensiveness', 'cannibalistically', 'characteristically', 'circumnavigating', 'circumnavigation', 'circumnavigations', 'comprehensiveness', 'hermaphroditical', 'indiscriminately', 'indispensableness', 'irresistibleness', 'physiognomically', 'preternaturalness', 'responsibilities', 'simultaneousness', 'subterraneousness', 'supernaturalness', 'superstitiousness', 'uncomfortableness', 'uncompromisedness', 'undiscriminating', 'uninterpenetratingly'] ''' fdist5 = FreqDist(text5) l5 = sorted([w for w in set(text5) if len(w) \u003e 7 and fdist5[w] \u003e 7]) print(l5) ''' ['#14-19teens', '#talkcity_adults', '((((((((((', '........', 'Question', 'actually', 'anything', 'computer', 'cute.-ass', 'everyone', 'football', 'innocent', 'listening', 'remember', 'seriously', 'something', 'together', 'tomorrow', 'watching'] ''' # 词语搭配与双连词 from nltk import bigrams bis = bigrams(['more', 'is', 'said', 'than', 'done']) print(list(bis)) # [('more', 'is'), ('is', 'said'), ('said', 'than'), ('than', 'done')] text4.collocations() ''' United States; fellow citizens; four years; years ago; Federal Government; General Government; American people; Vice President; Old World; Almighty God; Fellow citizens; Chief Magistrate; Chief Justice; God bless; every citizen; Indian tribes; public debt; one another; foreign nations; political parties ''' text8.collocations() ''' would like; medium build; social drinker; quiet nights; non smoker; long term; age open; Would like; easy going; financially secure; fun times; similar interests; Age open; weekends away; poss rship; well presented; never married; single mum; permanent relationship; slim build ''' # 计算其他东西 print([len(w) for w in text1]) ''' [1, 4, 4, 2, 6, ..., 5, 7, 6, 1] ''' fdist = FreqDist([len(w) for w in text1]) print(fdist) # \u003cFreqDist with 19 samples and 260819 outcomes\u003e print(list(fdist.keys())) # [1, 4, 2, 6, 8, 9, 11, 5, 7, 3, 10, 12, 13, 14, 16, 15, 17, 18, 20] print(fdist.items()) ''' dict_items([(1, 47933), (4, 42345), (2, 38513), (6, 17111), (8, 9966), (9, 6428), (11, 1873), (5, 26597), (7, 14399), (3, 50223), (10, 3528), (12, 1053), (13, 567), (14, 177), (16, 22), (15, 70), (17, 12), (18, 1), (20, 1)]) ''' print(fdist.max()) # 3 print(fdist[3]) # 50223 print(fdist.freq(3)) # 0.19255882431878046 # 1.4 回到Python决策与控制 print([w for w in sent7 if len(w) \u003c 4]) # [',', '61', 'old', ',', 'the', 'as', 'a', '29', '.'] print([w for w in sent7 if len(w) \u003c= 4]) # [',', '61', 'old', ',', 'will', 'join', 'the', 'as', 'a', 'Nov.', '29', '.'] print([w for w in sent7 if len(w) == 4]) # ['will', 'join', 'Nov.'] print([w for w in sent7 if len(w) != 4]) # ['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', '29', '.'] print(sorted([w for w in set(text1) if w.endswith('ableness')])) ''' ['comfortableness', 'honourableness', 'immutableness', 'indispensableness', 'indomitableness', 'intolerableness', 'palpableness', 'reasonableness', 'uncomfortableness'] ''' print(sorted([term for term in set(text4) if 'gnt' in term])) # ['Sovereignty', 'sovereignties', 'sovereignty'] print(sorted([item for item in set(text6) if item.istitle()])) ''' ['A', 'Aaaaaaaaah', 'Aaaaaaaah', 'Aaaaaah', 'Aaaah', 'Aaaaugh', 'Aaagh', 'Aaah', 'Aaauggh', 'Aaaugh', 'Aaauugh', 'Aagh', 'Aah', 'Aauuggghhh', 'Aauuugh', 'Aauuuuugh', 'Aauuuves', 'Action', 'Actually', 'African', 'Ages', 'Aggh', 'Agh', 'Ah', 'Ahh', 'Alice', 'All', 'Allo', 'Almighty', 'Alright', ... 'Yeah', 'Yes', 'You', 'Your', 'Yup', 'Zoot'] ''' print(sorted([item for item in set(sent7) if item.isdigit()])) # ['29', '61'] # 对每个元素进行操作 print([len(w) for w in text1]) # [1, 4, 4, 2, 6, ... 5, 7, 6, 1] print([w.upper() for w in text1]) # IN', 'THE', 'FORECASTLES', 'OF', ... 'FOUND', 'ANOTHER', 'ORPHAN', '.'] print(len(text1), len(set(text1)), len(set([word.lower() for word in text1]))) # 260819 19317 17231 print(len(set([word.lower() for word in text1 if word.isalpha()]))) # 16948 # 嵌套代码块 word = 'cat' if len(word) \u003c 5 : print('word length is less than 5') else: print('word \u003e= 5') '''word length is less than 5''' for word in ['Call', 'me', 'Ishmael', '.']: print(word) ''' Call me Ishmael . ''' # 条件循环 sent1 = ['Call', 'me', 'Ishmael', '.'] for xyzzy in sent1: if xyzzy.endswith('l'): print(xyzzy) ''' Call Ishmael ''' for token in sent1: if token.islower(): print(token, 'is a lowercase word') elif token.istitle(): print(token, 'is a titlecase word') else: print(token, 'is puncatuation') ''' Call is a titlecase word me is a lowercase word Ishmael is a titlecase word . is puncatuation ''' tricky = sorted([w for w in set(text2) if 'cie' in w or 'cei' in w]) for word in tricky: print(word) ''' ancient ceiling conceit ... sufficiently undeceive undeceiving ''' # 1.5 自动理解自然语言 # 词意消歧 # 指代消解 # 自动生成语言(自动问答、机器翻译) # 机器翻译 from nltk.book import* import nltk.misc.babelfish as babelfish babelfish.babelize_shell() # 这个没有实验成功 # Babelfish online translation service is no longer available. #Babel\u003ehow long before the next flight to Alice Springs? #Babel\u003egerman #Babel\u003erun # 人机对话系统 import nltk.chat as chat chat.chatbots() ''' Which chatbot would you like to talk to? 1: Eliza (psycho-babble) 2: Iesha (teen anime junky) 3: Rude (abusive bot) 4: Suntsu (Chinese sayings) 5: Zen (gems of wisdom) Enter a number in the range 1-5: 1 ''' ''' 表1-2 NLTK频率分布类中定义的函数 例子 描述 fdist = FreqDist(samples) 创建包含给定样本的频率分布 fdist.inc(sample) 增加样本 fdist['monstrous'] 计数给定样本monstrous出现的次数 fdist.freq('monstrous') 给定样本monstrous的频率 fdist.N() 样本总数 list(fdist.keys()) 以频率递减顺序排序的样本链表 for sample in fdist: 以频率递减的顺序遍历样本 fdist.max() 数值最大的样本 fdist.tabulate() 绘制频率分布表 fdist.plot() 绘制频率分布图 fdist.plot(cumulative=True) 绘制累计频率分布图 fdist1 \u003c fdist2 测试样本在fdist1中出现的频率是否小于fdist2 表1-4 词汇比较运算符 函数 含义 s.startswith(t) 测试s是否以t开头 s.endswith(t) 测试s是否以t结尾 t in s 测试s是否包含t s.islower() 测试s中所有字符是否都是小写字母 s.isupper() 测试s中所有字符是否都是大写字母 s.isalpha() 测试s中所有字符是否都是字母 s.isalnum() 测试s中所有字符是否都是字母或数字 s.isdigit() 测试s中所有字符是否都是数字 s.istitle() 测试s是否首字母大写(s中所有的词都是首字母大写) '''","date":"2017年08月29日 16:40:24"}
{"_id":{"$oid":"5d36a8ed6734bd8e681d5f0a"},"title":"自然语言处理中常用的文本清理流程","author":"人如墨","content":"在自然语言处理中，尽管文本清理受所做的任务影响比较大，但是有一些通用的清理流程标准是通用的，比如是否有必要替换URLS，时间，货币，姓名，地名，数字等。\n我们以英文文本为例，大致将文本处理流程分为以下几个步骤：\nNormalization\nTokenization\nStop words\nPart-of-speech Tagging\nNamed Entity Recognition\nStemming and Lemmatization\n下面是各个流程的具体介绍\nNormalization\n得到纯文本后，第一步通常要做就是Normalization。在英文中，所有句子第一个单词的首字母一般是大写，有的单词也会全部字母都大写用于表示强调和区分风格，这样更易于人类理解表达的意思，但是从计算机的角度来说是没法区别’Car’、‘car’、'CAR’是否是一个意思的，因此我们一般把文本中所有字母都转换为小写或大写(通常意义上是小写)，没歌词用一个唯一的词来表示。\n例如在下面的代码中，字符串文本调用lower()函数就可以将所有字母转换为小写形式。\npre_str = 'I Love My Family' after_str = pre_str.lower() print(after_str)\n输出结果为：\ni love my family\n你可能还想清楚文本中的句号、问号、感叹号等特殊字符，并且保留字母表中的字母和数字。文档分类和聚类等应用中若要将所有文本文档作为一个整体，那么正则表达式这个方法特别有效。用正则匹配小写’a’到’z’以及大写’A’到’Z’以及数字’0’到’9’的范围之外的所有字符并用空格代替。这个方法无需指定所有标点符号。当然，也可以采用其他正则表达式。\n在下面的代码中，使用re模块的sub正则匹配所有非a-z,A-Z,0-9的字母，并将其替换为空格。\nimport re text = 'the first time you see the second renaissance it may look boring.look at it at least and definitely watch part 2.it will??' text = re.sub(r'[^a-zA-Z0-9]', \" \", text) print(text)\n输出结果：\nthe first time you see the second renaissance it may look boring look at it at least and definitely watchpart 2 it will\n小写转换和标点移除是两个最常见的文本Normalization步骤，是否需要以及在哪个阶段使用这两个步骤取决于你的最终目标。\nTokenization\nToken是\"符号\"的高级表达， 一般值具有某种意义，无法再拆分的符号。在英文自然语言处理中，Tokens通常是单独的词，因此Tokenization就是将每个句子拆分为一系列的词。\n通常情况下，最简单的方法是使用split()方法返回词列表。\ntext = 'the first time you see the second renaissance it may look boring.look at it at least and definitely watch part 2.it will??' words = text.split() print(words)\n输出单词组成的列表：\n['the', 'first', 'time', 'you', 'see', 'the', 'second', 'renaissance', 'it', 'may', 'look', 'boring.look', 'at', 'it', 'at', 'least', 'and', 'definitely', 'watch', 'part', '2.it', 'will??']\n这里默认情况下是将一段话在空格字符处拆分，除了空格，也包括其他标签、新行等。这种方法还很智能，可以忽略一个序列中的两个或多个空格字符，因此不会返回空字符串。同样也可以使用可选参数对其进行控制。目前为止，我们只使用了Python内置的处理工具，当然我们也可以使用其他工具来完成相同的事情，比如NLTK，一种处理英文最常见的自然语言处理工具箱，某些运算会简单很多，在NLTK中拆分文本最常用的方法是使用nltk,tokenize中的word_tokenize()函数，这与split()函数的效果差不多，但是更加聪明一些，在尝试传入未标准化的原始文本时，会发现，根据标点符号位置的不同，对它们 的处理也不同。\n\n例如，头衔’Dr’后面的句号’.'与’Dr’保留在一起作为一个 Token。可想而知，NLTK 使用某种规则或模式决定如何处理每个标点符号。\n有时，我们可能需要将一段话分解成句子而不是单词。比如，如果你想翻译文本，可能需要将文本分拆成句子。\n\n这时，我们可以通过 NLTK 使用 sent_tokenize()实现这一点。然后可以根据需要将每个句子分拆成词，NLTK 提供多种 Token 解析器。包括基于正则表达式的令牌解析器，可以用于一步清除标点符号并将其 Tokenize。\nStop Word\nStop Word 是无含义的词，例如’is’/‘our’/‘the’/‘in’/'at’等。它们不会给句子增加太多含义，单停止词是频率非常多的词。 为了减少我们要处理的词汇量，从而降低后续程序的复杂度，需要清除停止词。\n\n在上述句子中，即使没有’are’和’the’，我们仍然能推断出人对狗的正面感情。你可以自己思考一下 NLTK 将英语中的哪些词作为停止词。\n\n这里，NLTK 是基于特定的文本语料库或文本集。不同的语料库可能有不同的停止词。在一个应用中， 某个词可能是停止词。而在另一个应用这个词就是有意义的词。要从文本中清除停止词，可以使用带过滤条件的 Python 列表理解。\n\n这里，我们将影评 Normalization 和 Tokenization 之后 清除其中的停止词。结果有点难懂，但现在输入量缩小了很多，并保留了比较重要的词汇。\n\nPart-of-Speech Tagging\n还记得在学校学过的词性吗？名词、代词、动词、副词等等。识别词在句子中的用途有助于我们更好理解句子内容。并且，标注词性还可以明确词之间的关系，并识别出交叉引用。同样地，NLTK给我们带来了很多便利。你可以将词传入 PoS tag 函数。然后对每个词返回一个标签，并注明不同的词性。\n\n这里函数正确地将出现的第一个’lie’标注为动词，将第二个标注为名词。关于标签含义的更多详细信息，请参阅 NLTK 文档。词性标注的一个典型应用是句子解析。\n\n上面的示例是 NLTK 手册中使用自定义语法解析歧义句的一个示例。实例中解析器返回了两种有效解释。我们也可以使用代码画出解析树，以便可以轻易地看出两者的区别。\n\n\"I / shot an elephant / in my pajamas\"(“我穿着睡衣杀了一头象”)以及 \"I / shot / an elephant in my pajamas\";（“我杀了一头穿着我睡衣的象”）\n另外，还有其他很多方法可以进行 PoS，比如 Hidden Markov Models (HMM)以及 Recurrent Neural Networks (RNNs)\nNamed Entity\nNamed Entity 一般是名词短语，又来指代某些特定对象、人、或地点 可以使用 ne_chunk()方法标注文本中的命名实体。在进行这一步前，必须先进行 Tokenization 并进行 PoS Tagging。\n\n如图，这是一个非常简单的示例。NLTK 还可以识别出不同的实体类型，分辨出人、组织和 GPE（地缘政治实体）。 另外，它还将’Udaticy’和’Inc’这两个词识别成一个实体，效果不错。Named Entity 并不是所有的情况都识别的很好，但如果是对大型语料库进行训练，却非常有效。命名实体识别通常用于对新闻文章建立索引和进行搜索。我们可以搜索自己感兴趣的公司的相关新闻。\nStemming and Lemmatization\n为了进一步简化文本数据，我们可以将词的不同变化和变形标准化。Stemming 提取是将词还原成词干或词根的过程。\n\n例如’brancing’/‘branched’/‘branches’等，都可以还原成’branch’。总而言之，它们都表达了分成多个路线或分支的含义。这有助于降低复杂度，并同时保留词所含的基本含义。Stemming 是利用非常简单的搜索和替换样式规则进行的。\n\n例如，后缀’ing’和’ed’可以丢弃；'ies’可以用’y’替换等等。这样可能会变成不是完整词的词干，但是只要这个词的所有形式都还原成同一个词干即可。因此 它们都含有共同的根本含义。\n\nNLTK 有几个不同的词干提取器可供选择，例如PorterStemmer()方法。上图例子中我们已经清除了 Stop Words，所以部分转换效果非常好。例如，‘started’还原成了’start’。但是像其它词，例如’people’末尾的’e’被删除，出现这样的原因是因为规则过于简单。\nLemmatization 是将词还原成标准化形式的另一种技术。在这种情况下，转换过程实际上是利用词典，将一个词的不同变形映射到它的词根。通过这种方法，我们能将较大的词形变化，如 is/was/were 还原成词根be。NLTK 中的默认词形还原器使用 nltk.stem.wordnet 数据库将词还原成词根。\n\n这里我们试一下像词干提取一样，将 WordNetLemmatizer() 的实例初始化，并将各个词传入 lemmatize()方法。结果中只有词ones被还原成了one，其它词并无任何变化。仔细读各个词，你会发现ones是这里唯一的复数名词。实际上，这就是它被转换的原因。\n\nLemmatization 需要知道每个词的词性。在这个例子中WordNetLemmatizer()默认词性是名词。但是我们可以指定 PoS 参数，修改这个默认设置。我们传入 ‘v’ 代表动词。现在，两个动词形式’boring’和’started’都被转换了。\n\n小结一下，在前面的示例中，可以看出 Stemming 有时会生成不是完整英语词的词干。Lemmatization 与 Stemming 类似，差别在于最终形式也是有含义的词。这就是说，Lemmatization 需要字典，而 Stemming 不需要字典。因此，根据你施加约束的不同，Stemming 是对内存要求较低的方案。","date":"2018年10月01日 09:14:20"}
{"_id":{"$oid":"5d36a8ee6734bd8e681d5f0c"},"title":"自然语言处理nlt文档集合","author":"san_yun","content":"Python+NLTK自然语言处理学习（一）：环境搭建\nPython+NLTK自然语言处理学习（二）：常用方法（similar、common_contexts、generate）\nPython+NLTK自然语言处理学习（三）：计算机自动学习机制","date":"2013年09月05日 16:47:30"}
{"_id":{"$oid":"5d36a8ef6734bd8e681d5f0e"},"title":"《Python自然语言处理-雅兰·萨纳卡(Jalaj Thanaki)》学习笔记：08 自然语言处理中的机器学习方法","author":"miniAI学堂","content":"08 自然语言处理中的机器学习方法\n8.1 机器学习的基本概念\n8.1.1 ML类型\n8.1.2 ML 监督学习\n8.1.3 无监督学习\n8.1.4 强化学习\n8.2 自然语言处理应用的开发步骤\n8.2.1 第一次迭代时的开发步骤\n8.2.2 从第二次到第N次迭代的开发步骤\n8.3 机器学习算法和其他概念\n8.3.1 有监督机器学习方法\n逻辑回归\n决策树\n随机森林\n朴素贝叶斯\n支持向量机\n8.3.2 无监督机器学习方法\nk-均值聚类\n8.3.3 半监督机器学习算法\n8.3.4 一些重要概念\n8.3.5 特征选择\n8.3.6 维度约减\n主成分分析\nt-SNE\n8.4 自然语言处理中的混合方法\n8.5 总结\n\n我们已经看到了特性工程的基本和高级水平。我们还看到了如何使用基于规则的系统来开发NLP应用程序。在本章中，我们将开发NLP应用程序，为了开发应用程序，我们将使用机器学习（ML）算法。我们将从ML的基本知识开始。之后，我们将看到使用ML的NLP应用程序的基本开发步骤。我们将主要了解如何在NLP域中使用ML算法。然后，我们将进入特性选择部分，我们还将了解混合模型和后处理技术。\n本章概述如下：\n了解机器学习的基础知识\nNLP应用程序的开发步骤\n了解ML算法和其他概念\nNLP应用的混合方法\n让我们来探索ML的世界！\n8.1 机器学习的基本概念\n首先，我们将了解什么是机器学习。传统上，编程就是定义所有的步骤以达到某个预先定义的结果。在这个编程过程中，我们使用一种编程语言来定义每一个微小的步骤，这有助于我们实现我们的结果。为了给你一个基本的理解，我举一个一般的例子，假设你想写一个程序来帮助你画一张脸。您可以先编写绘制左眼的代码，然后编写绘制右眼的代码，再编写绘制鼻子的代码，依此类推。这里，您正在为每个面部属性编写代码，但是ML会翻转这种方法。在ML中，我们定义结果，程序学习实现定义的输出的步骤。因此，我们不为每个面部属性编写代码，而是向机器提供数百个人脸样本。我们希望这台机器能够学习绘制人脸所需的步骤，以便绘制出新的人脸。除此之外，当我们提供新的人脸以及一些动物脸时，它应该能够识别出哪张脸看起来像人的脸。让我们举几个一般的例子。如果要识别某些状态的有效车牌，在传统编程中，需要编写代码，例如车牌的形状、颜色、字体等。如果您试图手动对车牌的每个属性进行编码，则这些编码步骤太长。使用ML，我们将向机器提供一些车牌示例，机器将学习步骤，以便识别新的有效车牌。假设你想制作一个程序来玩超级马里奥游戏并赢得比赛。所以，定义每个游戏规则对我们来说太困难了。我们通常定义一个目标，比如你需要在不死亡的情况下到达终点，机器会学习所有步骤来到达终点。有时，问题太复杂了，甚至我们不知道应该采取什么步骤来解决这些问题。例如，我们是一家银行，我们怀疑有一些欺诈活动正在发生，但我们不确定如何检测它们，或者我们甚至不知道要寻找什么。我们可以提供所有用户活动的日志，并查找行为与其他用户不同的用户。机器自行学习检测异常的步骤。ML在互联网上无处不在。每个大科技公司都在以某种方式使用它。当您看到任何YouTube视频时，YouTube会更新或向您提供您可能喜欢观看的其他视频的建议。甚至你的手机也使用ML为你提供诸如iPhone的Siri、Google援助等设施。ML目前进展非常快。研究人员使用旧的概念，改变其中的一些，或者使用其他研究人员，努力使其更有效和有用。让我们来看看ML的基本传统定义。1959年，一位名叫Arthur Samuel的研究人员给计算机提供了无需显式编程即可学习的能力。他从人工智能模式识别和计算学习理论的研究中发展了ML的概念。1997年，Tom Mitchell给了我们一个准确的定义，这个定义对那些能理解基础数学的人很有用。根据Tom Mitchell的定义，ML的定义是：一个计算机程序据说是从经验E中学习一些任务T和一些性能度量P，如果它在T上的性能，如经验P所测量的那样，随着经验E的提高而提高。让我们将前面的定义与前面的示例链接起来。识别车牌称为任务T。您将使用名为体验E的车牌示例运行一些ML程序，如果它成功学习，那么它可以预测下一个未知的车牌，称为性能度量P。现在是时候探索不同类型的ML以及它如何与人工智能相关了。\n8.1.1 ML类型\n在本节中，我们将介绍不同类型的ML以及一些有趣的分支和超级分支关系。\nML本身源于一个叫做人工智能的分支。ML也有一个分支，它正在制造许多流行的话题，叫做深度学习，但是我们将在第9章，NLP和NLG问题的深度学习中详细介绍人工智能和深度学习。\n学习技术可以分为不同的类型。在本章中，我们将重点放在ML上。参见图8.1：\n\nML技术可以分为三种不同类型，如图8.2所示：\n\n8.1.2 ML 监督学习\n在这种类型的ML中，我们将提供一个标记的数据集作为ML算法的输入，并且我们的ML算法知道什么是正确的，什么是不正确的。在这里，ML算法学习标签和数据之间的映射。它生成了ML模型，然后生成的ML模型可以用来解决某些给定的任务。假设我们有一些带有标签的文本数据，比如垃圾邮件和非垃圾邮件。数据集的每个文本流都有这两个标签中的任何一个。当我们应用监督的ML算法时，它使用标记的数据并生成一个ML模型，该模型预测标签为垃圾邮件或非垃圾邮件，用于看不见的文本流。这是一个监督学习的例子。\n8.1.3 无监督学习\n在这种类型的ML中，我们将提供一个未标记的数据集作为ML算法的输入。所以，我们的算法没有得到任何关于正确与否的反馈。它必须自己学习数据的结构来解决给定的任务。使用未标记的数据集比较困难，但更方便，因为并非每个人都有一个完全标记的数据集。大多数数据都是未标记、混乱和复杂的。\n假设我们正在开发一个汇总应用程序。我们可能还没有总结出与实际文档相对应的文档。然后，我们将使用原始文档和实际文本文档为给定的文档创建摘要。在这里，机器不会得到关于ML算法生成的摘要是对还是错的任何反馈。我们还将看到一个计算机视觉应用程序的例子。对于图像识别，我们将一些卡通人物的未标记的图像数据集输入机器，我们期望机器学习如何对每个角色进行分类。当我们提供一个卡通人物的未看到的图像时，它应该识别该角色并将该图像放入T中。他是由机器本身产生的适当的类。\n8.1.4 强化学习\n第三种类型的ML是强化学习。在这里，ML算法不会在每次预测之后立即给您反馈，但是如果ML模型实现了它的目标，它会生成反馈。这种类型的学习主要用于机器人领域，并开发智能机器人来玩游戏。强化学习与使用试错法与环境交互的思想相联系。为了学习基础知识，让我们举个例子。比如说你想做一个在象棋上打败人类的机器人。这种机器人只有在赢得比赛后才会收到反馈。最近，谷歌Alphago击败了世界上最好的围棋玩家。如果您想了解更多信息，请参阅以下链接：https://techcrunch.com/2017/05/24/alphago-beats-planets-best-human-go-player-ke\n-jie/.我知道你一定有兴趣了解每种类型的ML之间的区别。所以，在阅读下一段时要注意。\n对于有监督的学习，你会在每一步或每一个预测之后得到反馈。在强化学习中，只有当我们的模型达到目标时，我们才会收到反馈。在无监督的学习中，我们永远不会得到反馈，即使我们实现了我们的目标或者我们的预测是正确的。在强化学习中，它与现有的环境相互作用，使用试错法，而其他两种方法不适用试错法。在有监督的学习中，我们将使用有标记的数据，而在无监督的学习中，我们将使用无标记的数据，在强化学习中，涉及到一系列的目标和决策过程。您可以参考图8.4：\n\n从这一节开始，你将学到很多新的东西，如果你一开始不理解一些术语，那么不要担心！请放心，我将在本章中实际地解释每一个概念。所以，让我们开始了解使用ML的NLP应用程序的开发步骤。\n8.2 自然语言处理应用的开发步骤\n在本节中，我们将讨论使用ML算法开发NLP应用程序的步骤。这些步骤因域而异。对于NLP应用程序，数据可视化并没有发挥那么重要的作用，而对分析应用程序的数据可视化将给您带来很多洞察。因此，它将从应用程序更改为应用程序，从域更改为域。这里，我的重点是NLP域和NLP应用程序，当我们查看代码时，我肯定会记得我在这里描述的步骤，以便您可以连接这些点。我将开发步骤分为两个版本。第一个版本考虑到它是NLP应用程序开发的第一个迭代。第二个版本将帮助您完成NLP应用程序开发的第一次迭代之后可以考虑的可能步骤。参见图8.5：\n![Alt](https://img-blog.csdnimg.cn/20190202225147352.png)\n8.2.1 第一次迭代时的开发步骤\n首先，我们将了解在使用ML开发第一个版本的NLP应用程序时通常可以使用的步骤。在我的解释过程中，我将参考图8.6，以便您正确理解：\n\n1、这个版本的第一步是理解您的问题陈述、应用程序需求或者您正试图解决的目标。\n2、第二步是获取解决目标所需的数据，或者，如果有数据集，则尝试找出数据集包含的内容和构建NLP应用程序所需的内容。如果您需要一些其他数据，那么首先问问您自己；您能在可用数据集的帮助下派生出子数据属性吗？如果是，那么可能不需要获取另一个数据集，但是如果不是，那么尝试获取一个可以帮助您开发NLP应用程序的数据集。\n3、第三步是考虑您希望得到什么样的最终结果，并据此开始探索数据集。做一些基本的分析。\n4、第四步是在对数据进行一般性分析之后，可以对其应用预处理技术。\n5、第五步是从预处理数据中提取特征，作为特征工程的一部分。\n6、第六种是，使用统计技术，可以可视化特征值。这是NLP应用程序的可选步骤。\n7、第七步是为您自己的基准构建一个简单的基本模型。\n8、最后但并非最不重要的是，评估基本模型，如果它符合标准，那么很好；否则，您需要更多的迭代，并且需要遵循另一个版本，我将在下一节中描述这个版本。\n8.2.2 从第二次到第N次迭代的开发步骤\n我们已经看到了您在第一次迭代中可以采取的步骤；现在我们将看到如何执行第二次迭代，以便提高模型的准确性和效率。在这里，我们还试图使我们的模型尽可能简单。所有这些目标都将是这个开发版本的一部分。\n现在，我们将看到在第一次迭代之后可以遵循的步骤。有关基本理解，请参见图8.7：\n\n第二次迭代的一些基本步骤如下：1、在第一次迭代之后，您已经构建了一个模型，现在您需要改进它。我建议您尝试不同的ML算法来解决相同的NLP应用程序，并比较其准确性。根据准确度选择最佳的三种ML算法。这将是第一步。\n2、作为第二步，通常，您可以对每个选定的ML算法应用超参数调整，以获得更好的精度。\n3、如果参数优化对您没有太大的帮助，那么您需要真正专注于特性工程部分，这将是您的第三步。\n4、目前，特征工程主要有两个部分：特征提取和特征选择。所以在第一次迭代中，我们已经提取了特征，但是为了优化我们的ML模型，我们需要进行特征选择。我们将在本章后面介绍所有的特性选择技术。\n5、在特性选择中，您基本上选择那些特性、变量或数据属性，这些特性、变量或数据属性是非常关键的，或者对获得结果贡献很大。因此，我们只考虑重要的特性并删除其他特性。\n6、您还可以删除异常值，执行数据规范化，并对输入数据应用交叉验证，这将帮助您改进ML模型。\n7、在执行了所有这些技巧之后，如果您没有得到准确的结果，那么您需要花费一些时间来获得新特性并使用它们。\n8、您可以重复前面的所有步骤，直到获得满意的结果。这就是如何处理NLP应用程序的开发。您应该观察您的结果，然后在下一个迭代中采取明智的、必要的步骤。在你的分析中要聪明，考虑所有的问题，然后再重申解决它们。如果你不彻底分析你的结果，那么重复永远不会帮助你。所以保持冷静，明智地思考，并重申。不用担心，当我们使用ML算法开发NLP应用程序时，我们将看到前面的过程。如果你是研究方面的，那么我强烈建议你理解ML算法背后的数学知识，但是如果你是一个初学者并且不太熟悉数学，那么你可以阅读ML库的文档。那些介于这两个区域之间的人，试着找出数学知识，然后实现它。现在，是时候深入了解ML世界，学习一些真正伟大的算法了。\n8.3 机器学习算法和其他概念\n在这里，我们将介绍最广泛使用的NLP域的ML算法。我们将根据ML的类型来研究算法。首先，我们将从有监督的ML算法开始，然后是无监督的ML算法，最后是半监督的ML算法。在这里，我们将了解算法及其背后的数学。我会保持简单，让那些不是来自一个强大的数学背景可以理解算法背后的直观概念。之后，我们将看到如何实际地使用这些算法来开发一个NLP应用程序。我们将开发一个很酷的NLP应用程序，它将帮助您理解算法而不会产生任何混淆。\n那么，我们开始吧！\n8.3.1 有监督机器学习方法\n我们在本章前面看到了关于受监督机器学习的介绍。我们看到和使用的任何技术和数据集都包括数据集中已经给出的结果、结果或标签。所以，这意味着无论何时，只要有一个标记的数据集，就可以使用受监控的ML算法。\n在开始使用算法之前，我将介绍两个主要的监控ML算法概念。这也将帮助您决定选择哪种算法来解决NLP或任何其他与数据科学相关的问题：\n回归\n分类\n回归\n回归是一个统计过程，用来估计变量之间的关系。假设你有一堆变量，你想找出它们之间的关系。首先，你需要找出哪些是因变量，哪些是自变量。回归分析有助于理解因变量如何改变其行为或自变量给定值的值。在这里，因变量依赖于自变量的值，而自变量则采用不依赖于其他变量的值。让我们举一个例子来给你一个清晰的理解。如果你有一个数据集有一个人的身高，你需要根据身高来决定体重，那么这个数据集是有监控的ML，你的数据集中已经有年龄了。所以，你有两个属性，也称为变量：高度和重量。现在，你需要根据给定的高度来预测重量。所以，想几秒钟，让我知道哪个数据属性或变量是依赖的，哪个是独立的。我希望你有一些想法。所以，让我现在回答。这里，权重是依赖于可变高度的相关数据属性或变量。高度是自变量。自变量也称为预测器。因此，如果在因变量和自变量之间有某种映射或关系，那么您也可以预测任何给定高度的权重。\n请注意，当输出或因变量取连续值时，使用回归方法。在我们的示例中，重量可以是任何值，例如20千克、20.5千克、20.6千克、60千克等等。对于其他数据集或应用程序，因变量的值可以是任何实数。参见图8.8：\n\n分类\n在这一节中，我们将讨论受监督的ML的另一个主要概念，即分类技术。这也被称为统计分类。\n统计分类用于确定给定新观察的类别。所以，我们有许多类别可以把新的观察结果放在其中。但是，我们不会盲目地选择任何类别，但是我们将使用给定的数据集，并且基于此数据集，我们将尝试为新观察确定最适合的类别，并将这一类别或类别的观察。\n让我们以NLP域本身为例。您有一个包含大量电子邮件的数据集，这些电子邮件已经有了一个类标签，即垃圾邮件或非垃圾邮件。所以，我们的数据集分为两类——垃圾邮件和非垃圾邮件。现在，如果我们收到一封新的电子邮件，那么我们可以将特定的电子邮件分类为垃圾邮件类还是非垃圾邮件类？答案是肯定的。因此，为了对新的电子邮件进行分类，我们使用数据集和ML算法，\n为新邮件提供最适合的类。实现分类的算法称为分类器。有时，术语分类器也指由分类器算法实现的将输入数据映射到类别的数学函数。\n请注意，这一点有助于您识别回归和分类之间的差异。在分类中，输出变量采用基本上是离散或分类值的类标签。在回归中，我们的输出变量取一个连续值。参见图8.9：\n\n既然我们已经了解了回归和分类的概念，那么让我们经常使用的基本术语，同时解释专门用于分类的ML算法：\n实例：这被称为输入，通常是向量的形式，这些是属性的向量。在pos-tagger示例中，我们使用从每个单词派生的特性，并使用scikit-learns API dictvectorizer将它们转换为向量。向量值被输入到ML算法中，因此这些输入向量就是实例。\n概念：概念是指将输入映射到输出的函数。因此，如果我们有一个电子邮件内容，并且我们正在努力查明该电子邮件内容是垃圾邮件还是非垃圾邮件，那么我们必须关注实例或输入中的某些特定参数，然后生成结果。如何从某些输入中识别某些输出的过程称为概念。例如，你有一些关于人身高的数据。在看到数据后，你可以决定这个人是高还是矮。在这里，这个概念或函数帮助你找到一个给定输入或实例的输出。所以，如果我把它放在数学格式中，那么这个概念就是一个世界上的一个对象和一个集合中的成员之间的映射。\n目标概念：目标概念指的是实际的答案或具体的功能，或我们试图找到的某些特定的概念。作为人类，我们在头脑中已经了解了很多概念，例如通过阅读电子邮件，我们可以判断它是垃圾邮件还是非垃圾邮件，如果您的判断是正确的，那么您就可以得到实际的答案。你知道什么叫垃圾邮件，什么不是，但除非我们把它写在某个地方，否则我们不知道它是对还是错。如果我们注意到数据集中每个原始数据的这些实际答案，那么我们就更容易确定哪些电子邮件应被视为垃圾邮件，哪些不应被视为垃圾邮件。这将帮助您找到新实例的实际答案。\n假设类：是可以帮助我们对实例进行分类的所有可能函数的类。我们刚刚看到了目标概念，我们试图找出一个特定的函数，但是这里我们可以想到所有可能的和潜在的函数的子集，这些函数可以帮助我们找出分类问题的目标概念。在这里，我要指出的是，我们看到分类任务的这个术语，所以不要考虑x2函数，因为它是一个线性函数，我们执行的是分类而不是回归。\n训练数据集：在分类中，我们试图找到目标概念或实际答案。现在，我们如何才能得到最终的答案呢？为了使用ML技术得到最终的答案，我们将使用一些样本集、训练集或训练数据集来帮助我们找到实际的答案。让我们看看训练集是什么。训练集包含与标签配对的所有输入。监督分类问题需要用实际答案或实际输出标记的训练数据集。所以，我们不仅仅是将我们的知识传递给机器，告诉它什么是垃圾邮件，什么是非垃圾邮件；我们还向机器提供了很多例子，比如这是垃圾邮件，这是非垃圾邮件，等等。因此，对于机器来说，很容易理解目标概念。\nML模型：我们将使用训练数据集，并将这些数据输入到ML算法中，然后，ML算法将尝试使用大量的训练示例来学习这个概念，并生成输出模型。此输出模型稍后可用于预测或决定给定的新邮件是否为垃圾邮件。生成的输出称为ML模型。我们将使用一个生成的ML模型并将新邮件作为输入，这个ML模型将生成关于给定邮件是否属于垃圾邮件类别的答案。\n候选者：候选者是我们的ML模型为新示例告诉我们的潜在目标概念。所以，你可以说候选者是机器的预测目标概念，但是我们不知道这里的候选者的预测或生成的输出是否是正确的答案。那么，让我们举个例子。我们向机器提供了许多电子邮件示例。机器可以概括垃圾邮件的概念，而不是垃圾邮件。我们将提供一封新的电子邮件，我们的ML模型会说它不是垃圾邮件，但是，我们需要检查我们的ML模型的答案是对还是错。这个答案被称为候选人。如何检查ML模型生成的答案是否与目标概念匹配？为了回答您的问题，我将介绍下一个术语，即测试集。\n测试集：测试集类似于训练数据集。我们的训练数据集包含带有垃圾邮件或非垃圾邮件等标签的电子邮件。因此，我将采用被认为是候选者的答案，并且我们将检查我们的测试集是否是非垃圾邮件或垃圾邮件。我们将把我们的答案与测试集的答案进行比较，并试图找出候选人的答案是正确的还是错误的。假设不是垃圾邮件是正确的答案。现在，您将收到另一封电子邮件，ML模型将再次生成一个非垃圾邮件答案。我们将用我们的测试集再次检查这个问题，这次ML模型生成了一个错误的答案——邮件实际上是垃圾邮件，但是ML模型将其错误地分类为非垃圾邮件类别。因此，测试集帮助我们验证MLModel。请注意，训练和测试集不应相同。这是因为，如果您的机器使用训练数据集学习概念，并且在训练数据集上测试MLModel，那么您就没有公平地评估ML模型。这在ML中被认为是作弊。因此，您的训练数据集和测试集应该总是不同的；测试集是您的机器从未见过的数据集。我们这样做是因为我们需要检查机器的能力，看看给定的问题能推广多少。这里，广义的意思是ML模型如何对未知和未知的例子做出反应。如果你还是很困惑，那我再举一个例子。你是个学生，老师教你一些事实，给你举了一些例子。起初，你只是记住了事实。为了检查你是否有正确的概念，老师会给你一个测试，并给你一个新的例子，在那里你需要应用你的学习。如果您能够将您的学习完美地应用到测试中的新示例中，那么您实际上就得到了这个概念。这证明了我们可以概括出老师教过的概念。我们用机器做同样的事情。\nML算法\n我们已经充分了解了ML的基本概念，现在我们将探讨ML算法。首先，我们将看到在NLP域中主要使用的监控ML算法。我不打算在这里介绍所有受监控的ML算法，但我将解释在NLP领域中最广泛使用的那些算法。\n在NLP应用程序中，我们主要使用各种ML技术执行分类。所以，在这里，我们主要关注的是算法的分类类型。其他领域，如分析使用各种类型的线性回归算法，以及分析应用程序\n但是我们不会去看这些算法，因为这本书是关于NLP域的。由于线性回归的一些概念有助于我们理解深度学习技术，我们将通过第9章“NLP和NLG问题的深度学习”中的示例，详细介绍线性回归和梯度下降。\n我们将使用各种算法开发一些NLP应用程序，这样您就可以看到算法的工作原理以及NLP应用程序如何使用ML算法开发。我们将研究垃圾邮件过滤等应用程序。\n\n逻辑回归\n我知道你一定很困惑，为什么我把逻辑回归放在分类类别中。让我告诉你，这只是这个算法的名字，但它是用来预测离散输出的，所以这个算法属于分类类别。\n对于这个分类算法，我将给你一个逻辑回归算法是如何工作的概念，我们将看看与之相关的一些基础数学。然后，我们将查看垃圾邮件过滤应用程序。\n首先，我们将考虑二进制类，如垃圾邮件与否、好坏、得失、0或1等，以了解算法及其应用。假设我想将电子邮件分类为垃圾邮件，而不是垃圾邮件类别。垃圾邮件和非垃圾邮件是离散的输出标签或目标概念。我们的目标是预测新电子邮件是否是垃圾邮件。非垃圾邮件也被称为火腿。为了构建这个NLP应用程序，我们将使用逻辑回归。\n首先让我们了解算法的技术性。\n在这里，我以一种非常简单的方式陈述与数学和这个算法有关的事实。理解该算法的一般方法如下。如果你知道ml的某些部分，那么你可以把这些点连接起来，如果你是ml的新手，那么不要担心，因为我们将了解每个部分：\n我们正在定义我们的假设函数，帮助我们生成目标输出或目标概念。\n我们定义了成本函数或误差函数，我们选择误差函数的方式是，我们可以导出误差函数的偏导数，这样我们就可以方便地计算梯度下降。\n我们正努力将误差降到最低，以便生成更准确的标签，并对数据进行准确分类。在统计学中，逻辑回归又称为逻辑回归或逻辑模型。该算法主要是作为一个二元类分类器来使用的，这意味着应该有两个不同的类来对数据进行分类。二元逻辑模型用于估计二元响应的概率，它基于一个或多个预测因子或独立变量或特征生成响应。这是一个ML算法，在深度学习中也使用了基本的数学概念。\n首先，我想解释一下为什么这个算法被称为逻辑回归。原因是该算法使用了一个逻辑函数或sigmoid函数。逻辑功能和sigmoid函数是同义词。\n我们用sigmoid函数作为假设函数。你所说的假设函数是什么意思？好吧，正如我们前面看到的，机器必须学习数据属性和给定标签之间的映射，这样它才能预测新数据的标签。如果机器通过数学函数学习这种映射，就可以实现这一点。数学函数是机器用来分类数据和预测标签或目标概念的假设函数。我们想要构建一个二进制分类器，所以我们的标签要么是垃圾邮件，要么不是。所以，在数学上，我可以为正常指定0，或者不为垃圾邮件指定1，或者反之亦然。这些数学上指定的标签是我们的因变量。现在，我们需要输出标签为0或1。数学上，标签是y和yε0，1。所以我们需要选择一个假设函数，将我们的输出值转换为0或1。logistic函数或sigmoid函数就是这样做的，这也是logistic回归使用sigmoid函数作为假设函数的主要原因。\n\n从上图中，您可以发现以下事实： 如果z值大于或等于零，那么logistic函数给出输出值1。 如果z值小于零，那么logistic函数将生成输出0\n这个sigmoid函数如何表示为假设函数：\n\n使用假设方程，机器实际上尝试学习输入变量或输入特征和输出标签之间的映射。我们来谈谈这个假设函数的解释。你能想出预测类标签的最佳方法吗？根据我的观点，我们可以使用概率概念来预测目标类标签。我们需要为这两个类生成概率，并且任何具有高概率的类都将被分配给特性的特定实例。在二进制分类中，y或目标类的值要么是零，要么是一。如果您熟悉概率，那么可以表示图8.16中给出的概率方程：\n\n逻辑回归的成本或误差函数\n首先，让我们了解成本函数或错误函数。在ML中，成本函数、损失函数或误差函数是一个非常重要的概念，因此我们将了解成本函数的定义。成本函数用于检查我们的ML分类器执行的准确性。在我们的训练数据集中，我们有数据和标签。当我们使用假设函数并生成输出时，我们需要检查离实际预测的距离。如果我们预测实际的输出标签，那么我们的假设函数输出和实际标签之间的差异是零或最小的，如果我们的假设函数输出和实际标签不相同，那么我们在它们之间有很大的差异。如果电子邮件的实际标签是垃圾邮件，即垃圾邮件，并且我们的假设函数也生成结果1，那么实际目标值和预测输出值之间的差异为零，因此预测中的错误也为零。如果我们的预测输出为1，而实际输出为零，那么我们的实际目标概念和预测之间的误差最大。所以，在我们的预测中有最小的误差是很重要的。这是误差函数的基本概念。我们将在一段时间内学习数学。有几种类型的错误函数可用，如r2错误、平方和错误等。根据ML算法和假设函数，我们的误差函数也发生了变化。逻辑回归的误差函数是什么？什么是θ，如果我需要选择θ的某个值，我如何处理它？所以，在这里，我会给出所有的答案。\n让我给你一些线性回归的背景知识。在线性回归中，一般采用平方误差和或残差之和作为成本函数。在线性回归中，我们试图生成最适合我们的数据集的线。在前面的例子中，给定高度，我想预测重量。我们首先画一条线，测量从每个数据点到线的距离。我们将平方这些距离，求和并尽量减小误差函数。参见图8.17：\n\n您可以看到每个数据点与线之间的距离是用小的垂直线表示的。我们将把这些距离平方，然后求和。我们将使用这个错误函数。我们已经生成了关于m线和截距b的斜率的偏导数。在图8.17中，b约为0.9，m约为三分之二。每次计算误差，更新m和b的值，生成最佳拟合线。更新m和b的过程称为梯度下降。利用梯度下降法对m和b进行更新，使误差函数具有最小的误差值，从而生成最佳拟合线。梯度下降给我们一个需要画线的方向。您可以在第9章深入学习NLP和NLG问题中找到一个详细的例子。因此，通过定义误差函数并生成偏导数，我们可以应用梯度下降算法，帮助我们最小化误差或成本函数。现在回到主要问题：我们可以使用误差函数进行逻辑回归吗？如果你对函数和微积分很了解，那么你的答案可能是否定的，这是正确的答案。让我为那些不熟悉函数和微积分的人解释一下。在线性回归中，我们的假设函数是线性的，所以我们很容易计算平方误差之和，但是在这里，我们将使用一个非线性函数sigmoid函数。如果你应用我们在线性回归中使用的相同的函数，结果会不好，因为如果你使用的是sigmoid函数，输入平方误差函数之和，并尝试可视化所有可能的值，那么你会得到一条非凸曲线。\n\n在ML中，我们主要使用能够提供凸曲线的函数，因为我们可以使用梯度下降算法来最小化误差函数并达到全局最小值。如图8.18所示，非凸曲线有许多局部极小值，因此要达到全局极小值是非常困难和耗时的，因为需要应用二阶或第n阶优化来达到全局最小值，而在凸曲线中，您可以确定地快速地达到全局最小值。所以，如果我们把我们的sigmoid函数代入平方误差之和，就得到了非凸函数，这样我们就不会定义线性回归中使用的相同的误差函数。我们需要定义一个不同的凸优化函数，这样我们就可以应用梯度下降算法并生成一个全局最小值。我们将使用称为可能性的统计概念。为了推导似然函数，我们将使用图8.16中给出的概率方程，并考虑训练数据集中的所有数据点。所以，我们可以得到下面的方程，叫做似然函数。参见图8.19：\n\n现在，为了简化导数过程，我们需要将似然函数转换为单调递增函数。这可以通过取似然函数的自然对数来实现，称为对数似然。这个对数可能性是我们进行逻辑回归的成本函数。参考图8.20中的以下方程式：\n\n我们将绘制成本函数并了解它为我们提供的好处。在x轴上，我们有假设函数。假设函数的范围是0到1，所以我们在x轴上有这两点。从第一个案例开始，其中y=1。您可以在图8.21中看到右上方生成的曲线：\n\n如果你看任何一个对数函数图，它会像误差函数y=0的图。这里，我们翻转曲线，因为我们有一个负号，然后你得到我们为y=1值绘制的曲线。在图8.21中，您可以看到日志图，以及图8.22中的翻转图：\n\n这里，我们对值0和1感兴趣，因此我们考虑图8.21中描述的图的那部分。这个成本函数有一些有趣和有用的特性。如果预测标签或候选标签与实际目标标签相同，则成本为零。如果假设函数预测hθ（x）=1，那么成本=0；如果hθ（x）趋向于0，这意味着如果它更接近于零，那么成本函数就会放大到∞。对于y=0，您可以看到图8.21中左上方的图。这种情况下的条件也具有与我们之前看到的相同的优点和特性。当实际值为0且假设函数预测为1时，它将变为∞。如果假设函数预测为0，实际目标也为0，则成本=0。现在，我们来看看为什么要选择这个成本函数。原因是这个函数使我们的优化变得容易，因为我们将使用最大对数似然，因为它有一个凸曲线，可以帮助我们进行梯度下降。\n为了应用梯度下降，我们需要生成相对于θ的偏导数，并生成如图8.23所示的方程：\n\n该方程用于更新θ的参数值；定义学习率。这是可以用来设置算法学习或训练的速度或速度的参数。如果你把学习率设置得太高，那么算法就无法学习，如果你把它设置得太低，那么训练就需要很多时间。所以你需要明智地选择学习率。\n代码实现\nimport math import numpy as np import pandas as pd from pandas import DataFrame from sklearn import preprocessing from sklearn.linear_model import LogisticRegression from sklearn.cross_validation import train_test_split from numpy import loadtxt, where from pylab import scatter, show, legend, xlabel, ylabel\nd:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20. \"This module will be removed in 0.20.\", DeprecationWarning)\n# scale larger positive and values to between -1,1 depending on the largest # value in the data min_max_scaler = preprocessing.MinMaxScaler(feature_range=(-1,1)) df = pd.read_csv(\"data.csv\", header=0)\n# clean up data df.columns = [\"grade1\",\"grade2\",\"label\"]\nx = df[\"label\"].map(lambda x: float(x.rstrip(';'))) # formats the input data into two arrays, one of independant variables # and one of the dependant variable X = df[[\"grade1\",\"grade2\"]] X = np.array(X) X = min_max_scaler.fit_transform(X) Y = df[\"label\"].map(lambda x: float(x.rstrip(';'))) Y = np.array(Y)\n# if want to create a new clean dataset ##X = pd.DataFrame.from_records(X,columns=['grade1','grade2']) ##X.insert(2,'label',Y) ##X.to_csv('data2.csv') # creating testing and training set X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.33) # train scikit learn model clf = LogisticRegression() clf.fit(X_train,Y_train)\nLogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1, penalty='l2', random_state=None, solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\nprint('score Scikit learn: ', clf.score(X_test,Y_test))\nscore Scikit learn: 0.8484848484848485\n# visualize data, uncomment \"show()\" to run it pos = where(Y == 1) neg = where(Y == 0) scatter(X[pos, 0], X[pos, 1], marker='o', c='b') scatter(X[neg, 0], X[neg, 1], marker='x', c='r') xlabel('Exam 1 score') ylabel('Exam 2 score') legend(['Not Admitted', 'Admitted']) show()\n##The sigmoid function adjusts the cost function hypotheses to adjust the algorithm proportionally for worse estimations def Sigmoid(z): G_of_Z = float(1.0 / float((1.0 + math.exp(-1.0*z)))) return G_of_Z ##The hypothesis is the linear combination of all the known factors x[i] and their current estimated coefficients theta[i] ##This hypothesis will be used to calculate each instance of the Cost Function def Hypothesis(theta, x): z = 0 for i in range(len(theta)): z += x[i]*theta[i] return Sigmoid(z) ##For each member of the dataset, the result (Y) determines which variation of the cost function is used ##The Y = 0 cost function punishes high probability estimations, and the Y = 1 it punishes low scores ##The \"punishment\" makes the change in the gradient of ThetaCurrent - Average(CostFunction(Dataset)) greater def Cost_Function(X,Y,theta,m): sumOfErrors = 0 for i in range(m): xi = X[i] hi = Hypothesis(theta,xi) if Y[i] == 1: error = Y[i] * math.log(hi) elif Y[i] == 0: error = (1-Y[i]) * math.log(1-hi) sumOfErrors += error const = -1/m J = const * sumOfErrors print('cost is ', J ) return J ##This function creates the gradient component for each Theta value ##The gradient is the partial derivative by Theta of the current value of theta minus ##a \"learning speed factor aplha\" times the average of all the cost functions for that theta ##For each Theta there is a cost function calculated for each member of the dataset def Cost_Function_Derivative(X,Y,theta,j,m,alpha): sumErrors = 0 for i in range(m): xi = X[i] xij = xi[j] hi = Hypothesis(theta,X[i]) error = (hi - Y[i])*xij sumErrors += error m = len(Y) constant = float(alpha)/float(m) J = constant * sumErrors return J ##For each theta, the partial differential ##The gradient, or vector from the current point in Theta-space (each theta value is its own dimension) to the more accurate point, ##is the vector with each dimensional component being the partial differential for each theta value def Gradient_Descent(X,Y,theta,m,alpha): new_theta = [] constant = alpha/m for j in range(len(theta)): CFDerivative = Cost_Function_Derivative(X,Y,theta,j,m,alpha) new_theta_value = theta[j] - CFDerivative new_theta.append(new_theta_value) return new_theta ##The high level function for the LR algorithm which, for a number of steps (num_iters) finds gradients which take ##the Theta values (coefficients of known factors) from an estimation closer (new_theta) to their \"optimum estimation\" which is the ##set of values best representing the system in a linear combination model def Logistic_Regression(X,Y,alpha,theta,num_iters): m = len(Y) for x in range(num_iters): new_theta = Gradient_Descent(X,Y,theta,m,alpha) theta = new_theta if x % 100 == 0: #here the cost function is used to present the final hypothesis of the model in the same form for each gradient-step iteration Cost_Function(X,Y,theta,m) print('theta ', theta) print('cost is ', Cost_Function(X,Y,theta,m)) Declare_Winner(theta) ##This method compares the accuracy of the model generated by the scikit library with the model generated by this implementation def Declare_Winner(theta): score = 0 winner = \"\" #first scikit LR is tested for each independent var in the dataset and its prediction is compared against the dependent var #if the prediction is the same as the dataset measured value it counts as a point for thie scikit version of LR scikit_score = clf.score(X_test,Y_test) length = len(X_test) for i in range(length): prediction = round(Hypothesis(X_test[i],theta)) answer = Y_test[i] if prediction == answer: score += 1 #the same process is repeated for the implementation from this module and the scores compared to find the higher match-rate my_score = float(score) / float(length) if my_score \u003e scikit_score: print('You won!') elif my_score == scikit_score: print('Its a tie!') else: print('Scikit won.. :(') print('Your score: ', my_score) print('Scikits score: ', scikit_score )\ninitial_theta = [0,0] alpha = 0.1 iterations = 1000 Logistic_Regression(X,Y,alpha,initial_theta,iterations)\ncost is 0.6886958174712052 theta [0.015808968977217012, 0.014030982200249273] cost is 0.6886958174712052 cost is 0.6886958174712052 cost is 0.45043928326843835 theta [1.1446039323506159, 1.030383323481578] cost is 0.45043928326843835 cost is 0.45043928326843835 cost is 0.37210396400568835 theta [1.7920198800927762, 1.6251057941038252] cost is 0.37210396400568835 cost is 0.37210396400568835 cost is 0.33493174290971306 theta [2.2378078311381255, 2.0381775708737533] cost is 0.33493174290971306 cost is 0.33493174290971306 cost is 0.3134393548415864 theta [2.5764517180022444, 2.35358660097723] cost is 0.3134393548415864 cost is 0.3134393548415864 cost is 0.2995143683386589 theta [2.8487364478320787, 2.608155678935002] cost is 0.2995143683386589 cost is 0.2995143683386589 cost is 0.2898100759552151 theta [3.0758031030008572, 2.8210921909376734] cost is 0.2898100759552151 cost is 0.2898100759552151 cost is 0.2826976528686292 theta [3.2700162725064694, 3.0036648752998807] cost is 0.2826976528686292 cost is 0.2826976528686292 cost is 0.2772893938976962 theta [3.4392392975568247, 3.163057635787686] cost is 0.2772893938976962 cost is 0.2772893938976962 cost is 0.2730601259267772 theta [3.588788716304762, 3.3041402117668226] cost is 0.2730601259267772 cost is 0.2730601259267772 Its a tie! Your score: 0.8484848484848485 Scikits score: 0.8484848484848485\n这有一个逻辑回归的实现，您可以在SciKit学习库中找到它与给定实现的比较。\n垃圾邮件过滤\n垃圾邮件过滤是一种基本的NLP应用程序。使用此算法，我们希望建立一个ML模型，将给定邮件分类为垃圾邮件或正常类。所以，让我们制作一个垃圾邮件过滤应用程序。在垃圾邮件过滤中，我们将使用scikit-learn的CountVectorizer API学习生成特性，然后使用LogisticRegression进行训练。\nimport pandas as pd import numpy as np\n# read file into pandas using a relative path path = 'sms.tsv' sms = pd.read_table(path, header=None, names=['label', 'message'])\n# examine the shape sms.shape\n(5572, 2)\n# examine the first 10 rows sms.head(10)\nlabel\nmessage\n0\nham\nGo until jurong point, crazy.. Available only ...\n1\nham\nOk lar... Joking wif u oni...\n2\nspam\nFree entry in 2 a wkly comp to win FA Cup fina...\n3\nham\nU dun say so early hor... U c already then say...\n4\nham\nNah I don't think he goes to usf, he lives aro...\n5\nspam\nFreeMsg Hey there darling it's been 3 week's n...\n6\nham\nEven my brother is not like to speak with me. ...\n7\nham\nAs per your request 'Melle Melle (Oru Minnamin...\n8\nspam\nWINNER!! As a valued network customer you have...\n9\nspam\nHad your mobile 11 months or more? U R entitle...\n# examine the class distribution sms.label.value_counts()\nham 4825 spam 747 Name: label, dtype: int64\n# convert label to a numerical variable sms['label_num'] = sms.label.map({'ham':0, 'spam':1})\n# check that the conversion worked sms.head(10)\nlabel\nmessage\nlabel_num\n0\nham\nGo until jurong point, crazy.. Available only ...\n0\n1\nham\nOk lar... Joking wif u oni...\n0\n2\nspam\nFree entry in 2 a wkly comp to win FA Cup fina...\n1\n3\nham\nU dun say so early hor... U c already then say...\n0\n4\nham\nNah I don't think he goes to usf, he lives aro...\n0\n5\nspam\nFreeMsg Hey there darling it's been 3 week's n...\n1\n6\nham\nEven my brother is not like to speak with me. ...\n0\n7\nham\nAs per your request 'Melle Melle (Oru Minnamin...\n0\n8\nspam\nWINNER!! As a valued network customer you have...\n1\n9\nspam\nHad your mobile 11 months or more? U R entitle...\n1\n# how to define X and y (from the SMS data) for use with COUNTVECTORIZER X = sms.message y = sms.label_num print(X.shape) print(y.shape)\n(5572,) (5572,)\n# split X and y into training and testing sets from sklearn.cross_validation import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1) print(X_train.shape) print(X_test.shape) print(y_train.shape) print(y_test.shape)\n(4179,) (1393,) (4179,) (1393,)\n# import and instantiate CountVectorizer (with the default parameters) from sklearn.feature_extraction.text import CountVectorizer # instantiate the vectorizer vect = CountVectorizer() # learn training data vocabulary, then use it to create a document-term matrix vect.fit(X_train) X_train_dtm = vect.transform(X_train)\n# equivalently: combine fit and transform into a single step X_train_dtm = vect.fit_transform(X_train)\n# examine the document-term matrix X_train_dtm\n\u003c4179x7456 sparse matrix of type '\u003cclass 'numpy.int64'\u003e' with 55209 stored elements in Compressed Sparse Row format\u003e\n# transform testing data (using fitted vocabulary) into a document-term matrix X_test_dtm = vect.transform(X_test) X_test_dtm\n\u003c1393x7456 sparse matrix of type '\u003cclass 'numpy.int64'\u003e' with 17604 stored elements in Compressed Sparse Row format\u003e\nfrom sklearn import linear_model clf = linear_model.LogisticRegression(C=1e5)\n# train the model using X_train_dtm (timing it with an IPython \"magic command\") %timeit clf.fit(X_train_dtm, y_train)\n10 loops, best of 3: 54.7 ms per loop\n# make class predictions for X_test_dtm y_pred_class = clf.predict(X_test_dtm)\n# calculate accuracy of class predictions from sklearn import metrics metrics.accuracy_score(y_test, y_pred_class)\n0.9885139985642498\n我们执行一些基本的文本分析，以帮助我们理解数据。这里，我们使用scikit-learn API, CountVectorizer().将文本数据转换为矢量格式。此API在下面使用（tf-idf）。我们将数据集分为一个训练数据集和一个测试集，这样我们就可以检查分类器模型在测试数据集上的表现。\n# print the confusion matrix metrics.confusion_matrix(y_test, y_pred_class)\narray([[1205, 3], [ 13, 172]])\n# print message text for the false positives (ham incorrectly classified as spam) X_test[y_test \u003c y_pred_class]\n2340 Cheers for the message Zogtorius. I’ve been st... 4009 Forgot you were working today! Wanna chat, but... 1497 I'm always on yahoo messenger now. Just send t... Name: message, dtype: object\n# print message text for the false negatives (spam incorrectly classified as ham) X_test[y_test \u003e y_pred_class]\n1777 Call FREEPHONE 0800 542 0578 now! 763 Urgent Ur £500 guaranteed award is still uncla... 3132 LookAtMe!: Thanks for your purchase of a video... 1875 Would you like to see my XXX pics they are so ... 1893 CALL 09090900040 \u0026 LISTEN TO EXTREME DIRTY LIV... 4298 thesmszone.com lets you send free anonymous an... 4394 RECPT 1/3. You have ordered a Ringtone. Your o... 4949 Hi this is Amy, we will be sending you a free ... 761 Romantic Paris. 2 nights, 2 flights from £79 B... 19 England v Macedonia - dont miss the goals/team... 2821 INTERFLORA - “It's not too late to order Inter... 2247 Hi ya babe x u 4goten bout me?' scammers getti... 4514 Money i have won wining number 946 wot do i do... Name: message, dtype: object\n# example false negative X_test[3132]\n\"LookAtMe!: Thanks for your purchase of a video clip from LookAtMe!, you've been charged 35p. Think you can do better? Why not send a video in a MMSto 32323.\"\n# calculate predicted probabilities for X_test_dtm (poorly calibrated) y_pred_prob = clf.predict_proba(X_test_dtm)[:, 1] y_pred_prob\narray([9.90605780e-07, 4.03318013e-09, 1.38284780e-07, ..., 6.48404534e-06, 1.00000000e+00, 3.77161160e-09])\n# calculate AUC metrics.roc_auc_score(y_test, y_pred_prob)\n0.9932611419366387\n逻辑回归的优势：\n它能处理非线性效应\n它可以为每个类生成概率分数，这使得解释变得容易\n逻辑回归的缺点：\n此分类技术仅用于二进制分类。如果要将数据分类为两个以上的类别，我们可以使用其他算法。我们可以使用随机森林和决策树等算法将数据分类为两个以上的类别。\n如果你提供了大量的特征作为这个算法的输入，那么特征空间就会增加，这个算法的性能就不好了。\n过度拟合的可能性很高，这意味着分类器在训练数据集上表现良好，但不能进行足够的归纳，从而能够预测未知数据的正确目标标签。\n决策树\n决策树是最古老的ML算法之一。这个算法很简单，但很健壮。这个算法为我们提供了一个树结构来做任何决定。逻辑回归用于二进制分类，但如果有两个以上的类，则可以使用决策树。\n让我们通过一个例子来理解决策树。假设克里斯喜欢风帆冲浪，但他有自己的喜好——他通常喜欢晴天和有风的天气来享受它，也不喜欢在下雨天或阴天或风小的日子冲浪。参见图8.26：\n\n如你所见，O（圆点）是克里斯喜欢风浪的好天气条件，X（十字）是克里斯不喜欢风浪的坏天气条件。\n我所绘制的数据不是线性可分的，这意味着你不能仅仅用一条线来区分红色十字和蓝色圆点。你可能会认为，如果目标只是把蓝点和红十字分开，那么我可以用两条线来实现这一点。然而，一条线能把蓝点和红十字分开吗？答案是否定的，这就是为什么我告诉你这个数据集不能线性分离的原因。因此对于这种情况，我们将使用决策树。\n决策树实际上为您做了什么？用外行的术语来说，决策树学习实际上是关于提出多个线性问题。让我们理解我所说的线性问题。\n假设我们问一个问题：有风吗？你有两个答案：是或否。我们有一个与风有关的问题，所以我们需要集中在图8.26的X轴上。如果我们的答案是：是的，有风，那么我们应该考虑右手边有红十字和蓝点的区域；如果我们回答：不是，没有风，那么我们需要考虑左手边所有的红十字。为了更好地理解，您可以参考图8.27：\n\n如图8.27所示，我画了一条穿过x轴中点的线。我刚刚选择了一个中点，没有具体的原因。所以我画了一条黑线。行左侧的红色十字表示：不，没有风，行右侧的红色十字表示：有风。在这条线的左边，只有红色的十字，没有一个蓝色的点。如果你选择了答案，不，那么实际上你是用标记为“否”的分支进行遍历的。左侧的区域只有红色交叉，因此你最终得到了属于同一类的所有数据点，这些数据点用红色交叉表示，你不会再为该树的分支提出进一步的问题。现在，如果您选择答案，是的，那么我们需要将焦点放在右侧的数据点上。您可以看到有两种类型的数据点，蓝点和红十字。所以，为了对它们进行分类，你需要提出一个线性边界，这样由直线组成的部分只有一种数据点，我们将通过问另一个问题来实现这一点：天气晴朗吗？这一次，再一次，你有两个可能的答案-是或否。记住，我已经遍历了树的分支，它以是的形式回答了我们的第一个问题。所以我的重点是在数据点的右边，因为在那里我有以红十字和蓝点的形式表示的数据点。我们已经在y轴上描述了太阳，所以你需要看看这个轴，如果你画一条线穿过y轴的中点，那么线上面的部分代表答案，是的，这是一个晴天。线下的所有数据点代表答案，不，这不是晴天。当您绘制这样一条线并停止在第一条线之后延伸该线时，您可以成功地分离位于右侧的数据点。所以线上方的部分只包含蓝点，线下方的部分，红色十字。您可以看到水平线，如图8.28所示：\n\n我们可以观察到，通过问一系列问题或一系列线性问题，我们实际上将表示克里斯不冲浪的红色十字和表示克里斯冲浪的蓝色圆点分类。\n这是一个非常基本的示例，可以帮助您了解决策树如何处理分类问题。在这里，我们通过问一系列问题以及生成多个线性边界来分类数据点来构建树。让我们举一个数字例子，这样你就能更清楚地看到它了。参见图8.29：\n\n您可以看到给定的数据点。我们先从x轴开始。您希望选择X轴上的哪个阈值，以便获得这些数据点的最佳分割？想一想！我想选择一条在点3通过x轴的线。现在有两个部分。在数学上，我选择了给定数据点的最佳分割，即x\u003c=3和x\u003e3。参见图8.30：\n\n我们先看一下左侧部分。您希望选择Y轴上的哪个值，以便在绘制该线后在一个区域中只有一种类型的数据点？您选择的Y轴上的阈值是多少，以便在一个部分中有一种类型的数据集，而在另一个部分中有另一种类型的数据集？我将选择穿过Y轴上点2的线。因此，线上方的数据点属于一个类，线下方的数据点属于另一个类。数学上，y\u003c=2给你一个类，y\u003e2给你另一个类。参见图8.31：\n![Alt](https://img-blog.csdnimg.cn/20190202230725373.png)\n现在集中在右侧部分；对于该部分，我们还需要选择相对于Y轴的阈值。这里，分离边界的最佳阈值是y=4，因此截面y\u003c4只有红色十字，截面y\u003e=4只有蓝色点。最后，通过一系列线性问题，我们能够对数据点进行分类。参见图8.32：\n![Alt](https://img-blog.csdnimg.cn/20190202230755679.png)\n现在你对算法有了一个概念，但是你的脑子里可能有几个问题。我们对获取行进行了可视化，但是决策树算法如何选择最佳的方法来分割数据点并使用给定的特征生成决策边界？假设我有两个以上的特征，比如说十个特性；那么决策树如何知道它需要在第一次使用第二个特性而不是第三个特性？所以我将通过解释决策树背后的数学来回答所有这些问题。我们将查看一个与NLP相关的示例，这样您就可以了解如何在NLP应用程序中使用决策树。\n我有一些关于决策树的问题，让我们逐一回答。我们将使用可视化来获得一个线性边界，但是决策树如何识别使用哪些特征以及应该分割数据的哪些特征值？让我们看看熵这个数学术语。因此，决策树使用熵的概念来决定在哪里分割数据。让我们了解熵。熵是一个树分支中杂质的度量，因此，如果一个树分支中的所有数据点都属于同一类，那么熵e=0；否则，熵e\u003e0，e\u003c1。如果熵e=1，则表示树的分支高度不纯，或者数据点在所有可用类之间平均分配。让我们看一个例子，这样你就能理解熵和杂质的概念。我们正在开发一个垃圾邮件过滤应用程序，我们有一个特性，即单词和短语类型。现在我们将介绍另一个特性，即数据集中出现的短语的最小阈值计数。参见图8.33：\n\n现在关注右边的图表。在这个图中，右侧部分只有一种数据点，用红色十字表示。所以从技术上讲，所有的数据点都是同质的，因为它们属于同一个类。因此，没有杂质，熵的值约为零。现在，如果您将焦点放在左侧图表上并查看其右侧部分，您将找到属于其他类标签的数据点。这部分含有杂质，因此熵很高。因此，在实现决策树的过程中，您需要找出可用于定义分割点的变量以及变量。您需要记住的另一件事是，您正试图最小化数据中的杂质，因此请尝试根据这些数据拆分数据。我们将看到如何在一段时间内选择变量来执行拆分。现在，让我们先看看熵的数学公式。参见图8.34：\n\n让我们看看圆周率是什么。它是给定类的分数值。假设我是班上的学生。t是可用类的总值。您有四个数据点；如果两个点属于A类，另两个点属于B类，则t=2。我们在使用分数值生成日志值后执行求和。现在是时候对熵进行数学计算了，然后我将告诉您如何使用熵对变量或特性执行拆分。我们来看一个计算熵的例子。您可以找到图8.35中的示例：\n\n如果您关注过滤列，那么您有两个值为spam的标签和两个值为ssh的ham。现在回答以下几个问题：我们总共有多少数据行？答案是4\n数据标签在筛选列中出现多少次？答案是2\n数据标签H在筛选列中出现多少次？答案是2\n要为类标签s生成分数值，需要执行数学运算使用以下公式：pS = No. of time S occurred / Total no. of data rows = 2/4 = 0.5\n现在我们还需要计算h的p：pH = No. of time H occurred / Total no. of data rows = 2/4 = 0.5\n现在我们有了产生熵的所有必要值。\n专注于公式：Entropy = -pS* log2(pS) -pHlog2(pH) = -0.5 * log(0.5) -0.5log(0.5) = 1.0\n您可以使用python的数学模块进行计算。如你所见，我们得到熵e=1。这是最不纯净的状态，数据均匀地分布在可用的类中。因此，熵告诉我们数据的状态，不管类是否处于不纯状态。\n现在我们来看看最期待的问题：我们如何知道在哪一个变量上或者使用哪一个特性来执行分割？要理解这一点，我们需要了解信息获取。这是决策树算法的核心概念之一。让我介绍一下信息获取（ig）的公式：\n信息增益（ig）=熵（父节点）-[权重平均值]熵（子节点）现在我们来看看这个公式。我们正在计算父节点的熵，并减去子节点的加权熵。如果在父节点上执行拆分，决策树将尝试最大化信息获取。使用ig，决策树将选择我们需要对其执行拆分的功能。该计算针对所有可用的特性进行，因此决策树确切知道要在哪里进行拆分。您需要参考图8.33。\n我们计算了父节点的熵：e（父节点）=1。现在我们将关注单词并计算ig。让我们检查一下是否应该使用带ig的words执行split。在这里，我们将重点放在单词栏。所以，让我回答一些问题，以便您理解ig的计算：一共有多少个肯定意义的词？答案是3\n一共有多少个否定意义的词？答案是1\n所以，对于这个分支，熵e=0。当我们计算子节点的加权平均熵时，我们将使用这个方法。你可以看到，对于右边的节点，熵是零，所以分支中没有任何杂质，所以我们可以停在那里。但是，如果您查看左侧节点，它具有ssh类，因此我们需要计算每个类标签的熵。让我们一步一步地为左侧节点执行此操作：\nps=分支中S标签的数量/分支中示例的总数=2/3\nph=分支中H标签的数量/分支中示例的总数=1/3\n现在熵e=-2/3 log2（2/3）-1/3 log2（1/3）=0.918\n在下一步中，我们需要计算子节点的加权平均熵。\n我们有三个数据点作为左侧分支的一部分，一个数据点作为右侧分支，如图8.36所示。因此，值和公式如下：\n子级权重平均熵=左侧分支数据点/数据点总数*（该分支中的子级熵）+右侧分支数据点/数据点总数*（该分支中的子级熵）\n儿童体重平均熵=[体重平均]熵（儿童）=¾0.918+¼（0）=0.6885\n现在是时候获得免疫球蛋白了：\nig=熵（父节点）-[权重平均值]熵（子节点）。我们两个部分的us-e（父节点）=1和[权重平均]熵（子节点）=0.6885\n因此，最终计算如下：\nig=1-0.6885=0.3115\n让我们集中讨论短语出现计数列，并计算短语计数值3的熵，即ethree（3）=1.0，短语计数值4的熵为efour（4）=1.0；现在\n[权重平均]熵（子项）=1.0，ig=1.0-1.0=0。所以，我们并没有得到任何关于这个特性拆分的信息。所以，我们不应该选择这个特性。\n现在，让我们集中讨论短语列，其中我们提到了短语类别异常短语或常用短语。当我们使用此列拆分数据点时，我们得到一个分支中的垃圾邮件类和另一个分支中的ham类。所以这里，你需要自己计算ig，但是ig=1。我们得到最大的免疫球蛋白。因此，我们将为分割选择此功能。您可以在图8.37中看到决策树：\n![Alt](https://img-blog.csdnimg.cn/20190202231014684.png)\n如果您有大量的特性，那么决策树执行培训非常缓慢，因为它为每个特性计算ig，并通过选择提供最大ig的特性来执行拆分。现在是时候看一下使用决策树的NLP应用程序了。我们将重新开发垃圾邮件过滤，但这次，我们将使用决策树。我们只需更改垃圾邮件过滤应用程序的算法，我们采用了之前生成的相同功能集，这样您就可以比较逻辑回归和垃圾邮件过滤决策树的结果。在这里，我们将使用由scikit-learn.学习中的CountVectorizer API生成的相同功能。\nimport pandas as pd import numpy as np\n# read file into pandas using a relative path path = 'sms.tsv' sms = pd.read_table(path, header=None, names=['label', 'message'])\n# examine the shape sms.shape\n(5572, 2)\n# examine the first 10 rows sms.head(10)\nlabel\nmessage\n0\nham\nGo until jurong point, crazy.. Available only ...\n1\nham\nOk lar... Joking wif u oni...\n2\nspam\nFree entry in 2 a wkly comp to win FA Cup fina...\n3\nham\nU dun say so early hor... U c already then say...\n4\nham\nNah I don't think he goes to usf, he lives aro...\n5\nspam\nFreeMsg Hey there darling it's been 3 week's n...\n6\nham\nEven my brother is not like to speak with me. ...\n7\nham\nAs per your request 'Melle Melle (Oru Minnamin...\n8\nspam\nWINNER!! As a valued network customer you have...\n9\nspam\nHad your mobile 11 months or more? U R entitle...\n# examine the class distribution sms.label.value_counts()\nham 4825 spam 747 Name: label, dtype: int64\n# convert label to a numerical variable sms['label_num'] = sms.label.map({'ham':0, 'spam':1})\n# check that the conversion worked sms.head(10)\nlabel\nmessage\nlabel_num\n0\nham\nGo until jurong point, crazy.. Available only ...\n0\n1\nham\nOk lar... Joking wif u oni...\n0\n2\nspam\nFree entry in 2 a wkly comp to win FA Cup fina...\n1\n3\nham\nU dun say so early hor... U c already then say...\n0\n4\nham\nNah I don't think he goes to usf, he lives aro...\n0\n5\nspam\nFreeMsg Hey there darling it's been 3 week's n...\n1\n6\nham\nEven my brother is not like to speak with me. ...\n0\n7\nham\nAs per your request 'Melle Melle (Oru Minnamin...\n0\n8\nspam\nWINNER!! As a valued network customer you have...\n1\n9\nspam\nHad your mobile 11 months or more? U R entitle...\n1\n# how to define X and y (from the SMS data) for use with COUNTVECTORIZER X = sms.message y = sms.label_num print(X.shape) print(y.shape)\n(5572,) (5572,)\n# split X and y into training and testing sets from sklearn.cross_validation import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1) print(X_train.shape) print(X_test.shape) print(y_train.shape) print(y_test.shape)\n(4179,) (1393,) (4179,) (1393,)\n# import and instantiate CountVectorizer (with the default parameters) from sklearn.feature_extraction.text import CountVectorizer # instantiate the vectorizer vect = CountVectorizer() # learn training data vocabulary, then use it to create a document-term matrix vect.fit(X_train) X_train_dtm = vect.transform(X_train)\n# equivalently: combine fit and transform into a single step X_train_dtm = vect.fit_transform(X_train)\n# examine the document-term matrix X_train_dtm\n\u003c4179x7456 sparse matrix of type '\u003cclass 'numpy.int64'\u003e' with 55209 stored elements in Compressed Sparse Row format\u003e\n# transform testing data (using fitted vocabulary) into a document-term matrix X_test_dtm = vect.transform(X_test) X_test_dtm\n\u003c1393x7456 sparse matrix of type '\u003cclass 'numpy.int64'\u003e' with 17604 stored elements in Compressed Sparse Row format\u003e\nfrom sklearn import tree clf = tree.DecisionTreeClassifier(criterion='entropy')\n# train the model using X_train_dtm (timing it with an IPython \"magic command\") %timeit clf.fit(X_train_dtm, y_train)\n10 loops, best of 3: 169 ms per loop\n# make class predictions for X_test_dtm y_pred_class = clf.predict(X_test_dtm)\n# calculate accuracy of class predictions from sklearn import metrics metrics.accuracy_score(y_test, y_pred_class)\n0.9655419956927495\n# print the confusion matrix metrics.confusion_matrix(y_test, y_pred_class)\narray([[1182, 26], [ 22, 163]])\n# print message text for the false positives (ham incorrectly classified as spam) X_test[y_test \u003c y_pred_class]\n1827 Dude. What's up. How Teresa. Hope you have bee... 1973 Yes but can we meet in town cos will go to gep... 3242 Ok i've sent u da latest version of da project. 1791 Am not working but am up to eyes in philosophy... 2900 Aight, I should be there by 8 at the latest, p... 2497 HCL chennai requires FRESHERS for voice proces... 745 Men like shorter ladies. Gaze up into his eyes. 2340 Cheers for the message Zogtorius. I’ve been st... 1832 Hello- thanx for taking that call. I got a job... 566 Ill call u 2mrw at ninish, with my address tha... 858 Hai ana tomarrow am coming on morning. \u0026lt;DE... 3544 I'm e person who's doing e sms survey... 987 I'm in office now . I will call you \u0026lt;#\u0026gt;... 705 True dear..i sat to pray evening and felt so.s... 988 Geeee ... I miss you already, you know ? Your ... 5336 Sounds better than my evening im just doing my... 100 Please don't text me anymore. I have nothing e... 1364 Yetunde, i'm sorry but moji and i seem too bus... 4092 Hey doc pls I want to get nice t shirt for my ... 4766 if you text on your way to cup stop that shoul... 5094 Hi Shanil,Rakhesh here.thanks,i have exchanged... 3826 Hi. I'm always online on yahoo and would like ... 3237 Aight text me when you're back at mu and I'll ... 4814 i can call in \u0026lt;#\u0026gt; min if thats ok 4958 I'm vivek:)i got call from your number. 330 I'm reading the text i just sent you. Its mean... Name: message, dtype: object\n# print message text for the false negatives (spam incorrectly classified as ham) X_test[y_test \u003e y_pred_class]\n3642 You can stop further club tones by replying \"S... 1777 Call FREEPHONE 0800 542 0578 now! 2680 New Tones This week include: 1)McFly-All Ab..,... 763 Urgent Ur £500 guaranteed award is still uncla... 4574 URGENT! This is the 2nd attempt to contact U!U... 881 Reminder: You have not downloaded the content ... 3132 LookAtMe!: Thanks for your purchase of a video... 2514 U have won a nokia 6230 plus a free digital ca... 5 FreeMsg Hey there darling it's been 3 week's n... 3530 Xmas \u0026 New Years Eve tickets are now on sale f... 4768 Your unique user ID is 1172. For removal send ... 4298 thesmszone.com lets you send free anonymous an... 1734 Hi, this is Mandy Sullivan calling from HOTMIX... 4949 Hi this is Amy, we will be sending you a free ... 761 Romantic Paris. 2 nights, 2 flights from £79 B... 3230 Ur cash-balance is currently 500 pounds - to m... 579 our mobile number has won £5000, to claim call... 3564 Auction round 4. The highest bid is now £54. N... 2863 Adult 18 Content Your video will be with you s... 2247 Hi ya babe x u 4goten bout me?' scammers getti... 4514 Money i have won wining number 946 wot do i do... 789 5 Free Top Polyphonic Tones call 087018728737,... Name: message, dtype: object\n# example false negative X_test[761]\n'Romantic Paris. 2 nights, 2 flights from £79 Book now 4 next year. Call 08704439680Ts\u0026Cs apply.'\n# calculate predicted probabilities for X_test_dtm (poorly calibrated) y_pred_prob = clf.predict_proba(X_test_dtm)[:, 1] y_pred_prob\narray([0., 0., 0., ..., 0., 1., 0.])\n# calculate AUC metrics.roc_auc_score(y_test, y_pred_prob)\n0.929778951136567\n如你所见，与逻辑回归相比，我们的精确度较低。现在是时候看一些可以用来提高ML模型精度的调整参数了。\n可调参数\nscikit-learn学习中有一个参数，这就是标准。你可以把它设置为熵或者基尼。熵或基尼被用来计算ig。因此，它们都有一个相似的机制来计算ig，决策树将根据熵或基尼给出的ig计算来执行分割。\n有最小样本大小，其默认值为2。因此，决策树分支将被拆分，直到每个分支的数据元素多于或等于两个。有时，决策树会试图拟合最大的训练数据，并过度拟合训练数据点。为了防止过拟合，您需要将最小样本尺寸从2增加到50或60。\n我们可以使用树木修剪技术，为此我们将采用自下而上的方法。\n决策树的优点\n决策树简单易开发\n决策树很容易被人理解，是一种白盒算法\n它帮助我们确定不同情况下的最差、最佳和预期值\n决策树的缺点\n如果您有很多特性，那么决策树可能有过拟合问题。\n在训练过程中，你需要注意你通过的参数\n随机森林\n该算法是解决过拟合问题的决策树的一个变种。\n随机森林既能发展线性回归，又能发展分类任务，这里我们将重点放在分类任务上。它使用了一个非常简单的技巧，而且效果非常好。关键是随机森林使用投票机制来提高测试结果的准确性。\n随机森林算法从训练数据集中生成数据的随机子集，并使用该子集为每个数据子集生成决策树。所有这些生成的树都称为随机林。现在让我们了解投票机制。一旦我们生成了决策树，我们就检查类标签，每个树都是为特定的数据点提供的。假设我们生成了三个随机的森林决策树。其中两个表示某个特定数据点属于A类，第三个决策树预测该特定数据点属于B类，算法考虑了更高的投票数，并为该特定数据点分配了类标签A。对于随机森林，分类的所有计算都类似于决策树。\nimport nltk from nltk import word_tokenize import pprint from sklearn.tree import DecisionTreeClassifier from sklearn.feature_extraction import DictVectorizer from sklearn.pipeline import Pipeline\n#tagged_sentences = nltk.corpus.brown.tagged_sents() tagged_sentences = nltk.corpus.treebank.tagged_sents()\nprint(tagged_sentences[0])\n[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\nprint(\"Tagged sentences: \", len(tagged_sentences))\nTagged sentences: 3914\nprint(\"Tagged words:\", len(nltk.corpus.treebank.tagged_words()))\nTagged words: 100676\ndef features(sentence, index): \" sentence: [w1, w2, ...], index: the index of the word \" return { 'word': sentence[index], 'is_first': index == 0, 'is_last': index == len(sentence) - 1, 'is_capitalized': sentence[index][0].upper() == sentence[index][0], 'is_all_caps': sentence[index].upper() == sentence[index], 'is_all_lower': sentence[index].lower() == sentence[index], 'prefix-1': sentence[index][0], 'prefix-2': sentence[index][:2], 'prefix-3': sentence[index][:3], 'suffix-1': sentence[index][-1], 'suffix-2': sentence[index][-2:], 'suffix-3': sentence[index][-3:], 'prev_word': '' if index == 0 else sentence[index - 1], 'next_word': '' if index == len(sentence) - 1 else sentence[index + 1], 'has_hyphen': '-' in sentence[index], 'is_numeric': sentence[index].isdigit(), 'capitals_inside': sentence[index][1:].lower() != sentence[index][1:] }\npprint.pprint(features(['This', 'is', 'a', 'sentence'], 2))\n{'capitals_inside': False, 'has_hyphen': False, 'is_all_caps': False, 'is_all_lower': True, 'is_capitalized': False, 'is_first': False, 'is_last': False, 'is_numeric': False, 'next_word': 'sentence', 'prefix-1': 'a', 'prefix-2': 'a', 'prefix-3': 'a', 'prev_word': 'is', 'suffix-1': 'a', 'suffix-2': 'a', 'suffix-3': 'a', 'word': 'a'}\ndef untag(tagged_sentence): return [w for w, t in tagged_sentence] def transform_to_dataset(tagged_sentences): X, y = [], [] for tagged in tagged_sentences: for index in range(len(tagged)): X.append(features(untag(tagged), index)) y.append(tagged[index][1]) #print \"index:\"+str(index)+\"original word:\"+str(tagged)+\"Word:\"+str(untag(tagged))+\" Y:\"+y[index] return X, y\ncutoff = int(.75 * len(tagged_sentences)) training_sentences = tagged_sentences[:cutoff] test_sentences = tagged_sentences[cutoff:]\nprint(len(training_sentences))\n2935\nprint(len(test_sentences))\n979\nX, y = transform_to_dataset(training_sentences) clf = Pipeline([ ('vectorizer', DictVectorizer(sparse=False)), ('classifier', DecisionTreeClassifier(criterion='entropy')) ])\n%timeit clf.fit(X[:10000],y[:10000]) # Use only the first 10K samples if you're running it multiple times. It takes a fair bit :)\n1 loop, best of 3: 14.9 s per loop\nprint('Training completed')\nTraining completed\nX_test, y_test = transform_to_dataset(test_sentences)\nprint(\"Accuracy:\", clf.score(X_test, y_test))\nAccuracy: 0.8937409609513096\ndef pos_tag(sentence): tagged_sentence = [] tags = clf.predict([features(sentence, index) for index in range(len(sentence))]) return zip(sentence, tags)\nlist(pos_tag(word_tokenize('This is my friend, John.')))\n[('This', 'DT'), ('is', 'VBZ'), ('my', 'NN'), ('friend', 'NN'), (',', ','), ('John', 'NNP'), ('.', '.')]\n随机森林的优势\n它有助于我们防止过拟合\n它既可用于回归，也可用于分类\n随机森林的缺点\n随机森林模型可以很容易地生长，这意味着如果数据集的随机子集很高，那么我们将得到更多的决策树，因此，我们将得到一组树，也就是可能占用大量内存的决策树林。\n对于高维的特征空间，很难解释树的每个节点，尤其是当一个森林中有大量的树时。\n朴素贝叶斯\n在本节中，我们将了解在许多数据科学应用中大量使用的概率ML算法。我们将使用此算法开发最著名的NLP应用程序情感分析，但在进入应用程序之前，我们将了解朴素贝叶斯算法的工作原理。那么，我们开始吧！朴素的贝叶斯ML算法基于贝叶斯定理。根据这个定理，我们最重要的假设是事件是独立的，这是一个朴素的假设，这就是这个算法被称为朴素贝叶斯的原因。那么，让我给你一个独立事件的概念。在分类任务中，我们有许多特性。如果我们使用朴素的贝叶斯算法，那么我们假设我们要提供给分类器的每个特征都是相互独立的，这意味着类中某个特定特征的存在不会影响任何其他特征。让我们举个例子。你想找出这句话的感悟，很好！你有很多特点，比如词袋、形容词短语等等。即使所有这些特征相互依赖或依赖于其他特征的存在，这些特征所携带的所有属性都独立地贡献了这个句子带有积极情绪的可能性，这就是我们称之为朴素算法的原因。\n这个算法非常简单，而且非常强大。如果你有大量的数据，这就非常有效。它可以对两个以上的类进行分类，因此有助于构建一个多类分类器。那么，现在，让我们来看一些点，这些点将告诉我们朴素贝叶斯算法是如何工作的。让我们了解它背后的数学和概率定理。\n我们将首先理解贝叶斯规则。在非常简单的语言中，您有一些事件的先验概率，并且您在测试数据中发现了相同事件的一些证据，并将它们相乘。然后你得到后验概率，帮助你得到最终的预测。别担心术语，我们会详细讨论这些细节的。让我先给你一个方程，然后我们举一个例子，这样你就知道我们需要做的计算是什么。如图8.41所示\n\n这里，p（c|x）是C类的概率，c类是目标，x是特征或数据属性。p（c）是C类的先验概率，p（x|c）是对给定目标类的预测概率的似然估计，p（x）是预测概率的先验概率。让我们用这个方程来计算一个例子。假设有一个医学测试可以帮助确定一个人是否患有癌症。患这种特定类型癌症的人的先前概率只有1%，这意味着p（c）=0.01=1%，因此p（而非c）=0.99=99%。如果患者患有癌症，有90%的几率检测结果呈阳性。因此，p（阳性|c）=0.9=90%的先验概率，10%的概率，即使患者没有癌症，结果仍然显示阳性，所以p（阴性|c）=0.1=10%。现在，我们需要检查这个人是否真的得了癌症。如果结果为阳性，则概率写为p（c阳性），如果患者没有癌症，但结果仍然为阳性，则表示为p（c阴性）。我们需要计算这两个概率来推导后验概率。首先，我们需要计算联合概率：\nJoint P(c | Positive result) = P© * P(Positive result | c) = 0.01 x 0.9 =0.009\nJoint P(not c | Positive result) = P(not c) * P(Positive result | not c) = 0.99 x 0.1 = 0.099\n前面的概率称为联合概率。这将有助于推导最终后验概率。为了得到后验概率，我们需要应用归一化。\nP (Positive result) = P(c | Positive result) + P ( not c | Positive result) = 0.009 +0.099 = 0.108\n现在实际后验概率如下：\nPosterior probability of P( not c | Positive result) = joint probability of P(not c | Positive result) / P\n(Positive result) = 0.099 / 0.108 = 0.916\n如果你把p的后验概率P(c | Positive result) +加上p的后验概率P(not c| Positive result)，应为=1。在这种情况下，它的总和是1。\n有很多数学在进行，所以我会给你画一个图表，帮助你理解这些事情。参见图8.42：\n\n我们将把这个概念扩展到一个NLP应用程序。这里，我们将以一个基于NLP的基本示例为例。假设有两个人——克里斯和萨拉。我们有克里斯和萨拉的电子邮件详细信息。他们都使用诸如生活、爱情和交易之类的词。为了简单起见，我们只考虑三个词。他们都以不同的频率使用这三个词。\n克里斯在邮件中只使用了1%的时间“爱”这个词，而他使用“交易”这个词的时间占80%，而“生活”的时间占1%。另一方面，萨拉使用“爱”这个词的时间占50%，交易时间占20%，生活时间占30%。如果我们有新的电子邮件，那么我们需要决定它是由克里斯还是萨拉写的。p的先验概率（chris）=0.5，p（sara）=0.5。\n邮件中有句话“生命交易”，所以概率计算是针对p（Chris“生命交易”）=p（Life）*p（Deal）*p（Chris）=0.04，而计算p（Sara“生命交易”）=p（Life）*p（Deal）*p（Sara）=0.03。现在，让我们应用规范化并生成实际概率，为此，我们需要计算联合概率=P（Chris“Life Deal”）+P（Sara“Life Deal”）=0.07。以下是实际概率值：\nP(Chris| “Life Deal”) = 0.04 / 0.07 = 0.57\nP(Sara| “Life Deal”) = 0.03 / 0.07 = 0.43\n“生命交易”这个句子更有可能是克里斯写的。本例到此结束，现在是实际实施的时候了。在这里，我们正在开发最著名的NLP应用程序，即情感分析。我们将对文本数据进行情绪分析，这样我们就可以说情绪分析是对人类产生的观点进行的文本分析。情绪分析帮助我们分析客户对某个产品或事件的看法。\n对于情绪分析，我们将使用“词语袋”方法。你也可以使用人工神经网络，但我解释的是一种简单而基本的方法。\nimport sys import os import time from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.naive_bayes import MultinomialNB from sklearn.metrics import classification_report\ndata_dir = \"data\" classes = ['pos', 'neg']\n# Read the data train_data = [] train_labels = [] test_data = [] test_labels = [] for curr_class in classes: dirname = os.path.join(data_dir, curr_class) for fname in os.listdir(dirname): with open(os.path.join(dirname, fname), 'r') as f: content = f.read() if fname.startswith('cv9'): test_data.append(content) test_labels.append(curr_class) else: train_data.append(content) train_labels.append(curr_class)\n# Create feature vectors vectorizer = TfidfVectorizer(min_df=5, max_df = 0.8, sublinear_tf=True, use_idf=True) train_vectors = vectorizer.fit_transform(train_data) test_vectors = vectorizer.transform(test_data)\nd:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\nclf = MultinomialNB() %time clf.fit(train_vectors, train_labels)\nWall time: 7 ms MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n%time prediction = clf.predict(test_vectors)\nWall time: 1 ms\n# Print results in a nice table print(\"Results for NaiveBayes (MultinomialNB) \") print(classification_report(test_labels, prediction))\nResults for NaiveBayes (MultinomialNB) precision recall f1-score support neg 0.81 0.92 0.86 100 pos 0.91 0.78 0.84 100 avg / total 0.86 0.85 0.85 200\n可调参数\n对于此算法，有时需要应用平滑。现在，平滑是什么意思？让我给你一个简单的想法。训练数据中有一些词，我们的算法使用这些数据生成一个ML模型。如果ML模型看到的单词不在训练数据中，而是出现在测试数据中，那么此时，我们的算法不能很好地预测事情。我们需要解决这个问题。因此，作为一个解决方案，我们需要应用平滑，这意味着我们也在计算稀有词的概率，这是scikit-learn学习中的可调参数。它只是一个标志——如果启用它，它将执行平滑；如果禁用它，则不会应用平滑。\n朴素贝叶斯的优势\n可以使用朴素贝叶斯算法处理高维特征空间\n它可用于对两个以上的类进行分类\n朴素贝叶斯的缺点\n如果你有一个由不同的词组成的词组有不同的意思，那么这个算法将不会帮助你。你有一句话，古吉拉特狮子。这是板球队的名字，但古吉拉特邦是印度的一个州，狮子是一种动物。因此，朴素贝叶斯算法将单个单词单独解释，因此该算法无法正确解释古吉拉特邦狮子。\n如果一些分类数据只出现在测试数据集中，而不出现在训练数据中，那么朴素贝叶斯就不会对此提供预测。所以，为了解决这类问题，我们需要应用平滑技术。\n现在是时候看看最后的分类算法了，支持向量机。\n支持向量机\n这是我们将在本章中看到的最后一个但不是最不受监督的ML算法。它被称为支持向量机（SVM）。该算法用于分类任务和回归任务。该算法也适用于多类分类任务。\nSVM获取标记的数据，并试图通过使用一条称为超平面的线将数据点分离来对其进行分类。目标是获得一个最佳的超平面，用于对现有的和新的，未发现的例子进行分类。如何获得一个最佳超平面是我们将要理解的。\n首先让我们了解最优超平面这个术语。我们需要以这样一种方式获得超平面：获得的超平面最大化到所有类的最近点的距离，这个距离称为边界。这里，我们将讨论一个二进制分类器。边界是超平面（或直线）与两个类中任何一个类的最近点之间的距离。SVM试图使利润最大化。参见图8.45：\n在给定的图中，有三行A、B和C。现在，选择您认为最能分隔数据点的行。我会选择B行，因为它最大化了两个类的边界，而其他A行和C行不这样做。\n请注意，SVM首先尝试完美地执行分类任务，然后尝试最大化利润。因此，对于SVM来说，正确地执行分类任务是第一要务。支持向量机既能获得线性超平面，又能生成非线性超平面。那么，让我们理解这背后的数学原理。如果你有n个特征，那么使用SVM，你可以画出n-1维超平面。如果你有一个二维特征空间，那么你可以画一个一维的超平面。\n如果有三维特征空间，则可以绘制二维超平面。在任何ML算法中，我们实际上都尝试最小化损失函数，因此我们首先定义SVM的损失函数。SVM使用铰链损失功能。我们使用这个损失函数，试图将我们的损失降到最低，并获得超平面的最大裕度。铰链损失函数方程如下：\nC (x, y, f(x)) = (1 - y * f(x))+\n这里，x是样本数据点，y是真标签，f（x）是预测标签，c是损失函数。方程中的+符号表示的是，当我们计算y*f（x）并且它大于等于1时，我们试图从1中减去它，得到一个负值。我们不想这样，所以为了表示这一点，我们把+号放在：\n\n现在是时候定义接受损失函数的目标函数，以及一个名为正则化项的lambda项。我们会看到它对我们有什么作用。但是，它也是一个调整参数。数学方程见图8.46：\n\nSVM有两个我们需要注意的调整参数。其中一个术语是lambda，它表示正则化术语。如果正则化项太高，那么我们的ML模型就过拟合，不能推广未看到的数据。如果它太低，那么它的下溢，我们会得到一个巨大的训练误差。所以，我们也需要一个正则化项的精确值。我们需要注意有助于防止过度拟合的正则化条件，我们需要将损失最小化。因此，我们对这两个项都取偏导数，下面是正则化项和损失函数的导数，我们可以用它们来进行梯度下降，这样我们可以最小化损失，得到一个准确的正则化值。偏导数方程见图8.47：\n\n损失函数的偏导数如图8.48所示：\n我们需要计算偏导数的值，并相应地更新权重。如果我们错误地分类了数据点，那么我们需要使用下面的公式来更新权重。参见图8.49：\n因此，如果y\u003c1，那么我们需要使用图8.50中的以下方程：\n\n在这里，长n形被称为eta，它表示学习率。学习速率是一个调整参数，它可以显示算法的运行速度。这也需要一个准确的值，因为如果它太高，那么训练将完成得太快，算法将错过全局最小值。另一方面，如果速度太慢，那就需要太多的时间来训练，而且可能永远不会收敛。如果发生错误分类，那么我们需要更新我们的损失函数以及正则化项。\n现在，如果算法正确地对数据点进行分类呢？在这种情况下，我们不需要更新损失函数；我们只需要更新我们的正则化参数，您可以使用图8.51中给出的方程看到：\n\n当我们有一个适当的正则化值和全局极小值时，我们就可以对支持向量机中的所有点进行分类，此时，边缘值也成为最大值。\n如果要使用SVM进行非线性分类器，则需要应用内核技巧。简单地说，内核技巧就是将较低的特征空间转换为较高的特征空间，从而引入非线性属性，以便对数据集进行分类。如图8.52所示：\n![Alt](https://img-blog.csdnimg.cn/20190202231721536.png)\n为了对这些数据进行分类，我们有X，Y特征。我们引入了新的非线性特征x2+y2，这有助于我们绘制一个能够正确分类数据的超平面。\n所以，现在是时候实现SVM算法了，我们将再次开发情感分析应用程序，但这次，我使用的是SVM，看看在准确性上有什么不同。\nimport sys import os import time from sklearn.feature_extraction.text import TfidfVectorizer from sklearn import svm from sklearn.metrics import classification_report\ndata_dir = \"data\" classes = ['pos', 'neg'] # Read the data train_data = [] train_labels = [] test_data = [] test_labels = [] for curr_class in classes: dirname = os.path.join(data_dir, curr_class) for fname in os.listdir(dirname): with open(os.path.join(dirname, fname), 'r') as f: content = f.read() if fname.startswith('cv9'): test_data.append(content) test_labels.append(curr_class) else: train_data.append(content) train_labels.append(curr_class) # Create feature vectors vectorizer = TfidfVectorizer(min_df=5, max_df = 0.8, sublinear_tf=True, use_idf=True) train_vectors = vectorizer.fit_transform(train_data) test_vectors = vectorizer.transform(test_data) # Perform classification with SVM, kernel=rbf classifier_rbf = svm.SVC() t0 = time.time() classifier_rbf.fit(train_vectors, train_labels) t1 = time.time() prediction_rbf = classifier_rbf.predict(test_vectors) t2 = time.time() time_rbf_train = t1-t0 time_rbf_predict = t2-t1 # Perform classification with SVM, kernel=linear classifier_linear = svm.SVC(kernel='linear') t0 = time.time() classifier_linear.fit(train_vectors, train_labels) t1 = time.time() prediction_linear = classifier_linear.predict(test_vectors) t2 = time.time() time_linear_train = t1-t0 time_linear_predict = t2-t1 # Perform classification with SVM, kernel=linear classifier_liblinear = svm.LinearSVC() t0 = time.time() classifier_liblinear.fit(train_vectors, train_labels) t1 = time.time() prediction_liblinear = classifier_liblinear.predict(test_vectors) t2 = time.time() time_liblinear_train = t1-t0 time_liblinear_predict = t2-t1 # Print results in a nice table print(\"Results for SVC(kernel=rbf)\") print(\"Training time: %fs; Prediction time: %fs\" % (time_rbf_train, time_rbf_predict)) print(classification_report(test_labels, prediction_rbf)) print(\"Results for SVC(kernel=linear)\") print(\"Training time: %fs; Prediction time: %fs\" % (time_linear_train, time_linear_predict)) print(classification_report(test_labels, prediction_linear)) print(\"Results for LinearSVC()\") print(\"Training time: %fs; Prediction time: %fs\" % (time_liblinear_train, time_liblinear_predict)) print(classification_report(test_labels, prediction_liblinear))\nd:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float): Results for SVC(kernel=rbf) Training time: 8.710498s; Prediction time: 0.945054s precision recall f1-score support neg 0.86 0.75 0.80 100 pos 0.78 0.88 0.83 100 avg / total 0.82 0.81 0.81 200 Results for SVC(kernel=linear) Training time: 7.742443s; Prediction time: 0.762043s precision recall f1-score support neg 0.91 0.92 0.92 100 pos 0.92 0.91 0.91 100 avg / total 0.92 0.92 0.91 200 Results for LinearSVC() Training time: 0.075004s; Prediction time: 0.000000s precision recall f1-score support neg 0.92 0.94 0.93 100 pos 0.94 0.92 0.93 100 avg / total 0.93 0.93 0.93 200\n可调参数\nScikit Learn为内核技巧提供了一个非常有用的调优参数。您可以使用各种类型的内核，如线性、RBF等。\n还有其他参数称为c和gamma。\nc控制平滑决策边界和正确分类训练点之间的权衡。如果C值较大，则可以获得更正确的训练点。\n如果你想设置你的边界，gamma会很有用。如果为gamma设置了较高的值，则只考虑附近的数据点来绘制决策边界；如果gamma的值较低，则也会考虑远离决策边界的点来测量决策边界是否使边界最大化。\n支持向量机的优点\n它在复杂的数据集中表现良好\n它可用于多类分类器\n支持向量机的缺点\n当你有一个非常大的数据集时，它不会表现得很好，因为它需要大量的培训时间\n当数据太嘈杂时，它将无法有效工作\n8.3.2 无监督机器学习方法\n这是另一种机器学习算法。当我们没有任何标记的数据时，我们可以使用无监督的机器学习算法。在NLP域中，有一种常见的情况是您找不到标记的数据集，然后这种类型的ML算法就被我们拯救了。\n在这里，我们将讨论无监督的ML算法，称为k-均值聚类。这种算法有许多应用。谷歌已经为他们的许多产品使用了这种无监督的学习算法。YouTube视频建议使用聚类算法。\n下图将向您介绍如何在无监督的ML算法中表示数据点。参见图8.55：\n\n如图8.55所示，数据点没有与其关联的标签，但从视觉上看，可以看到它们形成了一些组或集群。实际上，我们将尝试使用无监督的ML算法来确定数据中的结构，这样我们就可以对看不见的数据点获得一些卓有成效的见解。\n在这里，我们将研究k-means聚类算法，并开发与NLP域相关的文档分类示例。那么，我们开始吧！\nk-均值聚类\n在本节中，我们将讨论k-均值聚类算法。我们将首先了解算法。k均值聚类采用迭代细化技术。\n让我们了解一些关于k均值算法的基础知识。k是指我们要生成多少个集群。现在，您可以选择一个随机点，并将质心放在这个点上。k-均值聚类中的形心数不大于k的值，即不大于聚类值k。\n该算法有以下两个步骤，我们需要重申：\n1、第一步是指定质心。\n2、第二步是计算优化步骤。\n为了理解k-means的步骤，我们将看一个例子。您有五个数据点，在表中给出，我们希望将这些数据点分组为两个集群，所以k=2。参见图8.56：\n\n我们选择了点A（1，1）和点C（0，2）来分配我们的质心。这是分配步骤的结束，现在让我们了解优化步骤。\n我们将计算从每个点到这个质心的欧几里得距离。欧几里得距离方程如图8.57所示：\n\n每次我们都需要计算两个质心之间的欧几里得距离。让我们检查一下计算结果。起始质心平均值为c1=（1,1）和c2=（0,2）。这里，我们要做两个星团，这就是我们取两个质心的原因。\n迭代1\nFor point A = (1,1):\nC1 = (1,1) so ED = Square root ((1-1)2 + (1-1)2) = 0\nC2 = (0,2) so ED = Square root ((1-0)2 + (1-2)2) = 1.41\nHere, C1 \u003c C2, so point A belongs to cluster 1.\nFor point B = (1,0):\nC1 = (1,1) so ED = Square root ((1-1)2 + (0-1)2) = 1\nC2 = (0,2) so ED = Square root ((1-0)2 + (0-2)2) = 2.23\nHere, C1 \u003c C2, so point B belongs to cluster 1.\nFor point C = (0,2):\nC1 = (1,1) so ED = Square root ((0-1)2 + (2-1)2) = 1.41\nC2 = (0,2) so ED = Square root ((0-0)2 + (2-2)2) = 0\nHere, C1 \u003e C2, so point C belongs to cluster 2.\nFor point D = (2,4):\nC1 = (1,1) so ED = Square root ((2-1)2 + (4-1)2) = 3.16\nC2 = (0,2) so ED = Square root ((2-0)2 + (4-2)2) = 2.82\nHere, C1 \u003e C2, so point C belongs to cluster 2.\nFor point E = (3,5):\nC1 = (1,1) so ED = Square root ((3-1)2 + (5-1)2)= 4.47\nC2 = (0,2) so ED = Square root ((3-0)2 + (5-2)2)= 4.24\nHere, C1 \u003e C2, so point C belongs to cluster 2.\n在第一次迭代之后，我们的集群看起来如下。cluster c1有点a和b，c2有点c、d和e。因此，这里我们需要根据新的cluster point重新计算质心平均值：\nC1 = XA + XB / 2 = (1+1) / 2 = 1\nC1 = YA + YB / 2 = (1+0) / 2 = 0.5\nSo new C1 = (1,0.5)\nC2 = Xc + XD + XE / 3 = (0+2+3) / 3 = 1.66\nC2 = Yc +YD + YE / 3 = (2+4+5) / 3 = 3.66\nSo new C2 = (1.66,3.66)\n我们需要以和迭代1相同的方式再次进行所有的计算。所以我们得到如下的值。\n迭代 2\nFor point A = (1,1):\nC1 = (1,0.5) so ED = Square root ((1-1)2 + (1-0.5)2) = 0.5\nC2 = (1.66,3.66) so ED = Square root ((1-1.66)2 + (1-3.66)2) = 2.78\nHere, C1 \u003c C2, so point A belongs to cluster 1.\nFor point B = (1,0):\nC1 = (1,0.5) so ED = Square root ((1-1)2 + (0-0.5)2) = 1\nC2 = (1.66,3.66) so ED = Square root ((1-1.66)2 + (0-3.66)2) = 3.76\nHere, C1 \u003c C2, so point B belongs to cluster 1.\nFor point C = (0,2):\nC1 = (1,0.5) so ED = Square root ((0-1)2 + (2-0.5)2)= 1.8\nC2 = (1.66, 3.66) so ED = Square root ((0-1.66)2 + (2-3.66)2)= 2.4\nHere, C1 \u003c C2, so point C belongs to cluster 1.\nFor point D = (2,4):\nC1 = (1,0.5) so ED = Square root ((2-1)2 + (4-0.5)2)= 3.6\nC2 = (1.66,3.66) so ED = Square root ((2-1.66)2 + (4-3.66)2)= 0.5\nHere, C1 \u003e C2, so point C belongs to cluster 2.\nFor point E = (3,5):\nC1 = (1,0.5) so ED = Square root ((3-1)2 + (5-0.5)2) = 4.9\nC2 = (1.66,3.66) so ED = Square root ((3-1.66)2 + (5-3.66)2) = 1.9\nHere, C1 \u003e C2, so point C belongs to cluster 2.\n在第二次迭代之后，我们的集群看起来如下。C1有点A、B和C，C2有点D和E：\nC1 = XA + XB + Xc / 3 = (1+1+0) / 3 = 0.7\nC1 = YA + YB + Yc / 3 = (1+0+2 ) / 3 = 1\nSo new C1 = (0.7,1)\nC2 = XD + XE / 2 = (2+3) / 2 = 2.5\nC2 = YD + YE / 2 = (4+5) / 2 = 4.5\nSo new C2 = (2.5,4.5)\n我们需要进行迭代，直到集群不会改变。这就是为什么这个算法被称为迭代算法的原因。这是k均值聚类算法的直觉。现在我们将查看文档分类应用程序中的一个实际示例。\n文档聚类\n文档集群可以帮助您使用推荐系统。假设你有很多研究论文，而且没有标签。您可以使用k-means聚类算法，它可以帮助您根据文档中出现的单词形成聚类。您可以构建一个新闻分类应用程序。所有来自同一类别的新闻都应该组合在一起；您有一个超集类别，例如体育新闻，而这个体育新闻类别包含关于板球、足球等的新闻。\n# We are going to generate 5 movie genre by using K-mena clustering. from IPython.display import Image Image(filename='./K_means_clustering/data/kmeanexample.png')\nimport numpy as np import pandas as pd import nltk from bs4 import BeautifulSoup import re import os import codecs from sklearn import feature_extraction import mpld3\n#import three lists: titles, links and wikipedia synopses titles = open('./K_means_clustering/data/title_list.txt',encoding='utf-8').read().split('\\n') #ensures that only the first 100 are read in titles = titles[:100] links = open('./K_means_clustering/data/link_list_imdb.txt',encoding='utf-8').read().split('\\n') links = links[:100] synopses_wiki = open('./K_means_clustering/data/synopses_list_wiki.txt',encoding='utf-8').read().split('\\n BREAKS HERE') synopses_wiki = synopses_wiki[:100] synopses_clean_wiki = [] for text in synopses_wiki: text = BeautifulSoup(text, 'html.parser').getText() #strips html formatting and converts to unicode synopses_clean_wiki.append(text) synopses_wiki = synopses_clean_wiki genres = open('./K_means_clustering/data/genres_list.txt',encoding='utf-8').read().split('\\n') genres = genres[:100] print(str(len(titles)) + ' titles') print(str(len(links)) + ' links') print(str(len(synopses_wiki)) + ' synopses') print(str(len(genres)) + ' genres')\n100 titles 100 links 100 synopses 100 genres\nsynopses_imdb = open('./K_means_clustering/data/synopses_list_imdb.txt',encoding='utf-8').read().split('\\n BREAKS HERE') synopses_imdb = synopses_imdb[:100] synopses_clean_imdb = [] for text in synopses_imdb: text = BeautifulSoup(text, 'html.parser').getText() #strips html formatting and converts to unicode synopses_clean_imdb.append(text) synopses_imdb = synopses_clean_imdb\nsynopses = [] for i in range(len(synopses_wiki)): item = synopses_wiki[i] + synopses_imdb[i] synopses.append(item)\n# generates index for each item in the corpora (in this case it's just rank) and I'll use this for scoring later ranks = [] for i in range(0,len(titles)): ranks.append(i)\n# load nltk's English stopwords as variable called 'stopwords' stopwords = nltk.corpus.stopwords.words('english')\n# load nltk's SnowballStemmer as variabled 'stemmer' from nltk.stem.snowball import SnowballStemmer stemmer = SnowballStemmer(\"english\")\n# here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed def tokenize_and_stem(text): # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)] filtered_tokens = [] # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation) for token in tokens: if re.search('[a-zA-Z]', token): filtered_tokens.append(token) stems = [stemmer.stem(t) for t in filtered_tokens] return stems def tokenize_only(text): # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)] filtered_tokens = [] # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation) for token in tokens: if re.search('[a-zA-Z]', token): filtered_tokens.append(token) return filtered_tokens\ntotalvocab_stemmed = [] totalvocab_tokenized = [] for i in synopses: allwords_stemmed = tokenize_and_stem(i) totalvocab_stemmed.extend(allwords_stemmed) allwords_tokenized = tokenize_only(i) totalvocab_tokenized.extend(allwords_tokenized)\nvocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\nfrom sklearn.feature_extraction.text import TfidfVectorizer tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000, min_df=0.2, stop_words='english', use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3)) %time tfidf_matrix = tfidf_vectorizer.fit_transform(synopses) print(tfidf_matrix.shape)\nWall time: 12.1 s (100, 563)\nterms = tfidf_vectorizer.get_feature_names()\nfrom sklearn.metrics.pairwise import cosine_similarity dist = 1 - cosine_similarity(tfidf_matrix)\nfrom sklearn.cluster import KMeans num_clusters = 5 km = KMeans(n_clusters=num_clusters) %time km.fit(tfidf_matrix) clusters = km.labels_.tolist()\nWall time: 679 ms\nfrom sklearn.externals import joblib #joblib.dump(km, 'doc_cluster.pkl') km = joblib.load('./K_means_clustering/data/doc_cluster.pkl') clusters = km.labels_.tolist()\nd:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:312: UserWarning: Trying to unpickle estimator KMeans from version pre-0.18 when using version 0.19.0. This might lead to breaking code or invalid results. Use at your own risk. UserWarning) d:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:4: DeprecationWarning: The file './K_means_clustering/data/doc_cluster.pkl' has been generated with a joblib version less than 0.10. Please regenerate this pickle file.\nimport pandas as pd films = { 'title': titles, 'rank': ranks, 'synopsis': synopses, 'cluster': clusters, 'genre': genres } frame = pd.DataFrame(films, index = [clusters] , columns = ['rank', 'title', 'cluster', 'genre'])\nframe['cluster'].value_counts()\n4 26 0 25 2 21 1 16 3 12 Name: cluster, dtype: int64\ngrouped = frame['rank'].groupby(frame['cluster']) grouped.mean()\ncluster 0 47.200000 1 58.875000 2 49.380952 3 54.500000 4 43.730769 Name: rank, dtype: float64\nprint(\"Top terms per cluster:\") print() order_centroids = km.cluster_centers_.argsort()[:, ::-1] for i in range(num_clusters): print(\"Cluster %d words:\" % i, end='') for ind in order_centroids[i, :6]: print(' %s' % vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',') print() print() print(\"Cluster %d titles:\" % i, end='') for title in frame.loc[i]['title'].values.tolist(): print(' %s,' % title, end='') print() print()\nTop terms per cluster: Cluster 0 words: b'family', b'home', b'mother', b'war', b'house', b'dies', Cluster 0 titles: Schindler's List, One Flew Over the Cuckoo's Nest, Gone with the Wind, The Wizard of Oz, Titanic, Forrest Gump, E.T. the Extra-Terrestrial, The Silence of the Lambs, Gandhi, A Streetcar Named Desire, The Best Years of Our Lives, My Fair Lady, Ben-Hur, Doctor Zhivago, The Pianist, The Exorcist, Out of Africa, Good Will Hunting, Terms of Endearment, Giant, The Grapes of Wrath, Close Encounters of the Third Kind, The Graduate, Stagecoach, Wuthering Heights, Cluster 1 words: b'police', b'car', b'killed', b'murders', b'driving', b'house', Cluster 1 titles: Casablanca, Psycho, Sunset Blvd., Vertigo, Chinatown, Amadeus, High Noon, The French Connection, Fargo, Pulp Fiction, The Maltese Falcon, A Clockwork Orange, Double Indemnity, Rebel Without a Cause, The Third Man, North by Northwest, Cluster 2 words: b'father', b'new', b'york', b'new', b'brothers', b'apartments', Cluster 2 titles: The Godfather, Raging Bull, Citizen Kane, The Godfather: Part II, On the Waterfront, 12 Angry Men, Rocky, To Kill a Mockingbird, Braveheart, The Good, the Bad and the Ugly, The Apartment, Goodfellas, City Lights, It Happened One Night, Midnight Cowboy, Mr. Smith Goes to Washington, Rain Man, Annie Hall, Network, Taxi Driver, Rear Window, Cluster 3 words: d:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: DeprecationWarning: .ix is deprecated. Please use .loc for label based indexing or .iloc for positional indexing See the documentation here: http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated b'george', b'dance', b'singing', b'john', b'love', b'perform', Cluster 3 titles: West Side Story, Singin' in the Rain, It's a Wonderful Life, Some Like It Hot, The Philadelphia Story, An American in Paris, The King's Speech, A Place in the Sun, Tootsie, Nashville, American Graffiti, Yankee Doodle Dandy, Cluster 4 words: b'killed', b'soldiers', b'captain', b'men', b'army', b'command', Cluster 4 titles: The Shawshank Redemption, Lawrence of Arabia, The Sound of Music, Star Wars, 2001: A Space Odyssey, The Bridge on the River Kwai, Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb, Apocalypse Now, The Lord of the Rings: The Return of the King, Gladiator, From Here to Eternity, Saving Private Ryan, Unforgiven, Raiders of the Lost Ark, Patton, Jaws, Butch Cassidy and the Sundance Kid, The Treasure of the Sierra Madre, Platoon, Dances with Wolves, The Deer Hunter, All Quiet on the Western Front, Shane, The Green Mile, The African Queen, Mutiny on the Bounty,\n#This is purely to help export tables to html and to correct for my 0 start rank (so that Godfather is 1, not 0) frame['Rank'] = frame['rank'] + 1 frame['Title'] = frame['title']\n#export tables to HTML print(frame[['Rank', 'Title']].loc[frame['cluster'] == 1].to_html(index=False))\n\u003ctable border=\"1\" class=\"dataframe\"\u003e \u003cthead\u003e \u003ctr style=\"text-align: right;\"\u003e \u003cth\u003eRank\u003c/th\u003e \u003cth\u003eTitle\u003c/th\u003e \u003c/tr\u003e \u003c/thead\u003e \u003ctbody\u003e \u003ctr\u003e \u003ctd\u003e5\u003c/td\u003e \u003ctd\u003eCasablanca\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd\u003e13\u003c/td\u003e \u003ctd\u003ePsycho\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd\u003e14\u003c/td\u003e \u003ctd\u003eSunset Blvd.\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd\u003e15\u003c/td\u003e \u003ctd\u003eVertigo\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd\u003e24\u003c/td\u003e \u003ctd\u003eChinatown\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd\u003e31\u003c/td\u003e \u003ctd\u003eAmadeus\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd\u003e57\u003c/td\u003e \u003ctd\u003eHigh Noon\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd\u003e64\u003c/td\u003e \u003ctd\u003eThe French Connection\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd\u003e77\u003c/td\u003e \u003ctd\u003eFargo\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd\u003e87\u003c/td\u003e \u003ctd\u003ePulp Fiction\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd\u003e91\u003c/td\u003e \u003ctd\u003eThe Maltese Falcon\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd\u003e92\u003c/td\u003e \u003ctd\u003eA Clockwork Orange\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd\u003e95\u003c/td\u003e \u003ctd\u003eDouble Indemnity\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd\u003e96\u003c/td\u003e \u003ctd\u003eRebel Without a Cause\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd\u003e98\u003c/td\u003e \u003ctd\u003eThe Third Man\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd\u003e99\u003c/td\u003e \u003ctd\u003eNorth by Northwest\u003c/td\u003e \u003c/tr\u003e \u003c/tbody\u003e \u003c/table\u003e\nK均值聚类的优点\n这是一个非常简单的NLP应用算法\n它解决了主要的问题，因为它不需要标记数据或结果标签，您可以将此算法用于无标记数据\nk-均值聚类的缺点\n集群中心的初始化是一个非常关键的部分。假设有三个簇，在同一簇中放置两个形心，在最后一个簇中放置另一个形心。在某种程度上，k-均值聚类使聚类中所有数据点的欧几里得距离最小化，并且它将变得稳定，因此实际上，一个聚类中有两个形心，而第三个聚类中有一个形心。在这种情况下，您最终只有两个集群。这被称为集群中的局部最小问题。这是无监督学习算法的终结。在这里，您学习了k均值聚类算法，并开发了文档分类应用程序。如果你想更多地了解这项技术，试试这个练习。\n练习\n您需要探索NLP域中的层次集群及其应用。\n下一节非常有趣。我们将研究半监督机器学习技术。在这里，我们将得到它们的概述。所以，让我们了解这些技术。\n8.3.3 半监督机器学习算法\n当您有一个训练数据集，该数据集中的某些数据具有目标概念或目标标签，而另一部分数据没有任何标签时，基本上使用半监督ML或半监督学习（SSL）。如果您有这种数据集，那么可以应用半监督的ML算法。当我们有非常少量的标记数据和大量的未标记数据时，我们可以使用半监督技术。如果您想为任何本地语言（除了英语）构建一个NLP工具，并且您有非常少量的标记数据，那么您可以使用半监督方法。在这种方法中，我们将使用一个使用标记数据并生成ML模型的分类器。此ML Model用于为未标记的数据集生成标签。分类器用于对未标记的数据集进行高置信度预测。您可以使用任何适当的分类器算法对标记的数据进行分类。\n半监督技术是一个重要的研究领域，特别是在NLP应用中。去年，Google Research开发了基于图形的半监督技术：\nhttps://research.googleblog.com/2016/10/graph-powered-machine-learning-at-google.html\n8.3.4 一些重要概念\n在这一节中，我们将看到那些帮助我们了解我们如何掌握我们的数据集的训练，你应该如何判断，你说过这些情况，你应该做什么？什么是新的应用评价矩阵？所以，让我们找到所有这些问题的答案。我将介绍以下主题。我们将逐一查看所有这些内容：\n偏差方差权衡\n欠拟合\n过度拟合\n评价矩阵\n偏差方差权衡\n在这里，我们将看到一个关于偏差-方差权衡的高级概念。让我们逐一理解每个术语。\n让我们先了解“偏见”这个词。当您使用ML算法执行训练时，您发现生成的ML模型在第一轮训练迭代中的性能没有不同，那么您可以立即识别出ML算法有很高的偏差。在这种情况下，ML算法没有能力从给定的数据中学习，因此它不会学习您期望ML算法学习的新内容。如果你的算法有很高的偏差，那么它最终会停止学习。假设您正在构建一个情绪分析应用程序，并且您已经提出了MLModel，现在您对ML模型的准确性不满意，希望改进模型。您将通过添加一些新特性和更改一些算法参数来进行训练。现在，这个新生成的模型在测试数据上不能很好地执行或以不同的方式执行，这表明您可能有很高的偏差。您的ML算法不会以预期的方式收敛，因此您可以改进ML模型结果。\n让我们理解第二个术语，方差。所以，您使用任何ML算法来训练您的模型，并且观察到您得到了非常好的训练精度。但是，您应用相同的ML模型来为一个看不见的测试数据集生成输出，并且您的ML模型不能很好地工作。这种情况下，你有很好的训练精度，而MLModel对于看不见的数据效果不佳，这被称为高方差情况。因此，在这里，ML模型只能复制它在训练数据中看到的预测或输出，并且没有足够的偏差来概括看不见的情况。换句话说，您可以说您的ML算法正试图记住每个训练示例，最后，它只是模仿您的测试数据集上的输出。如果您有一个高方差问题，那么您的模型将以这样一种方式聚合，即它试图将数据集的每个示例分类到一个特定的类别中。这种情况导致我们过度拟合。我会解释什么是过度拟合，所以别担心！我们几分钟后到。\n为了克服前面的两个坏情况，我们真的需要一些中间的东西，这意味着没有高偏差和高方差。对于ML算法，产生最大偏差和最佳方差的技术可以得到最佳的ML模型。您的ML模型可能并不完美，但它都是关于生成最佳偏差方差权衡。\n在下一节中，您将学习欠拟合和过拟合的概念，以及帮助您摆脱这些高偏差和高方差场景的技巧。\n欠拟合\n在本节中，我们将讨论欠拟合这个术语。什么是欠拟合，它与偏方差权衡有什么关系？假设您使用任何ML算法对数据进行训练，就会得到一个很高的训练错误。参见图8.60：\n\n前面的情况，我们得到一个非常高的训练错误，被称为欠拟合。ML算法不能很好地处理训练数据。现在，我们将尝试更高程度的多项式，而不是线性决策边界。参见图8.61：\n\n这张图有一条非常曲折的线，它不能很好地处理训练数据。换句话说，您可以说它与前一次迭代的执行情况相同。这表明，ML模型有很高的偏差，并且没有学到新的东西。\n过度拟合\n在本节中，我们将讨论“过度拟合”这一术语。当我解释上一节中的差异时，我把这个术语放在你面前。所以，是时候解释过拟合了，为了解释它，我想举个例子。假设我们有一个数据集，我们将所有数据点绘制在二维平面上，现在我们正试图对数据进行分类，我们的ML算法绘制一个决策边界来对数据进行分类。如图8.62所示：\n![Alt](https://img-blog.csdnimg.cn/20190202232208602.png)\n如果你看左边的图，你会看到作为决策边界的直线。现在，这个图显示了一个训练错误，所以在第二次迭代中调整参数，您将获得非常好的训练精度；请参见右侧图。您希望测试数据也能获得良好的测试精度，但是ML模型在测试数据预测方面做得非常糟糕。因此，这种算法具有很好的训练精度，但在测试数据上表现不佳的情况称为过度拟合。在这种情况下，ML模型具有很高的方差，并且不能概括未知的数据。既然你已经看到了欠拟合和过度拟合，那么有一些经验法则可以帮助你避免这些情况。始终将培训数据分为三部分：60%的数据集应视为训练数据集\n数据集的20%应被视为验证数据集或开发数据集，这将有助于获得ML算法的中间精度，以便您可以捕获意外的内容并根据此更改算法。\n应保留数据集的20%，以报告最终精度，这将是测试数据集。您还应该应用k-折叠交叉验证。k表示需要验证的次数。假设我们把它设为三。我们将训练数据分成三个相等的部分。在训练算法的第一个时间戳中，我们使用两个部分，对单个部分进行测试，因此从技术上讲，它将以66.66%的速度进行训练，并以33.34%的速度进行测试。然后，在第二个时间戳中，ML算法使用一个部分对两个部分进行测试，最后一个时间戳将使用整个数据集进行培训和测试。经过三次时间戳后，计算平均误差，找出最佳模型。通常，对于合理数量的数据集，k应取10。\n对于ML模型，您不能有100%的准确度，这背后的主要原因是您的输入数据中存在一些您不能真正删除的噪声，这被称为不可约错误。\n因此，ML算法的最终误差方程如下：\nTotal error = Bias + Variance + Irreducible Error\n你真的不能摆脱不可约误差，所以你应该集中精力在偏差和方差上。请参阅图8.63，这将有助于向您展示如何处理偏差和差异权衡：\n![Alt](https://img-blog.csdnimg.cn/20190202232251527.png)\n评价矩阵\n对于我们的代码，我们会检查准确性，但在评估一个ML模型时，我们真的不知道哪些属性起主要作用。因此，在这里，我们将考虑一个广泛用于NLP应用程序的矩阵。这种评价矩阵称为F1得分或F度量。它有三个主要组成部分；在此之前，让我们介绍一些术语：\n真阳性（TP）：这是一个由分类器标记为a的数据点，实际上是来自类a。\n真阴性（TN）：这是对分类器中任何类的适当拒绝，这意味着分类器不会将数据点随机分类为类A，但会拒绝错误的标签。\n假阳性（FP）：这也被称为I型错误。让我们通过一个例子来理解这个度量：一个人为癌症测试献血。他实际上没有癌症，但他的测试结果是阳性的。这就是所谓的FP。\n假阴性（FN）：这也被称为II型错误。让我们通过一个例子来理解这个度量：一个人为癌症测试献血。他得了癌症，但他的检查结果是阴性的。所以它实际上忽略了类标签。这叫做FN。\n精度：精度是精确性的度量；分类器标记为正且实际为正的数据点的百分比是多少：精度=TP/TP+FP\n召回：召回是完整性的度量；分类器将阳性数据点的百分比标记为阳性：召回=TP/TP+FN\nF-度量：这只是衡量精度和召回的衡量标准。见公式：F=2精度召回/精度+召回除此之外，您还可以使用混淆矩阵来了解TP、TN、FP和FN中的每一个。\n您可以使用ROC曲线下的区域，该区域指示您的分类器能够区分负类和正类的程度。roc=1.0表示模型正确预测了所有类；0.5表示模型只是进行随机预测。\n8.3.5 特征选择\n现在是时候理解我们如何在第一次迭代之后即兴创作我们的模型了，有时候，特性工程在这方面帮助了我们很多。在第5章，特征工程和NLP算法以及第6章，高级特征工程和NLP算法中，我们解释了如何使用各种NLP概念和统计概念从文本数据中提取特征作为特征工程的一部分。特征工程包括特征提取和特征选择。现在是时候探索作为特征选择一部分的技术了。特征提取和特征选择为我们的NLP应用提供了最重要的特征。一旦我们设置了这些特性，您就可以使用各种ML算法来生成最终结果。如前所述，特征提取和特征选择是特征工程的一部分，在本节中，我们将介绍特征选择。您可能想知道我们为什么要学习特性选择，但有一些原因，我们将研究其中的每一个。首先，我们将看到对特征选择的基本理解。\n特征选择也称为变量选择、属性选择或变量子集选择。特征选择是选择最佳相关特征、变量或数据属性的过程，这些特征、变量或数据属性可以帮助我们开发更高效的机器学习模型。如果可以确定哪些功能贡献很大，哪些功能贡献较小，则可以选择最重要的功能，并删除其他不重要的功能。\n只要后退一步，首先了解我们使用功能选择试图解决的问题是什么。使用功能选择技术，我们可以获得以下好处：\n选择相关和适当的特性将帮助您简化ML模型，这将帮助您轻松地解释ML模型，并降低ML模型的复杂性。\n使用特征选择技术选择适当的特征将帮助我们提高ML模型的准确性。\n特征选择有助于机器学习算法更快地训练。\n功能选择还可以防止过度拟合。\n它帮助我们摆脱维度的诅咒。\n维数之咒\n让我们理解我所说的维度性诅咒是什么意思，因为这个概念将帮助我们理解为什么我们需要特性选择技术。维数的诅咒是说，随着特征或维数的增加，这意味着我们的机器学习算法增加了新的特征，那么我们需要推广的数据量将以指数形式精确增长。让我们举个例子来看看。假设你有一条线，一维特征空间，我们在这条线上放置了五个点，你可以看到每一点都在这条线上占据了一些空间。每一点占直线上空间的五分之一。参见图8.64：\n\n如果你有二维特征空间，那么我们需要五个以上的数据点来填充这个空间。所以，我们需要25个数据点来表示这两个维度。现在，每个点都占据了空间的1/25。见图8.65：\n\n如果你有一个三维特征空间，这意味着我们有三个特征，那么我们需要填充立方体。如图8.66所示：\n\n但是，要填充立方体，您需要正好125个数据点，如图8.66所示（假设有125个点）。所以每次我们添加功能时，都需要更多的数据。我想你们都会同意数据点的增长从5、25、125等指数级增长。所以，一般来说，您需要x d特性空间，其中x是训练中的数据点数量，d是特性或维度数量。如果您只是盲目地添加越来越多的特性，以便您的ML算法更好地理解数据集，那么实际上您所做的就是强制您的ML算法用数据填充更大的特性空间。你可以用一个简单的方法来解决这个问题。在这种情况下，您需要为您的算法提供更多的数据，而不是特性。\n现在你真的认为我限制了你添加新功能。所以，让我为你澄清一下。如果需要添加功能，那么您可以；您只需要选择帮助您的ML算法从中学习的最佳和最小数量的功能。我真的建议您不要盲目地添加太多功能。\n现在，我们如何获得最佳特性集？为我正在构建的特定应用程序设置的最佳特性是什么？我如何知道我的ML算法将在这个特性集上运行良好？我将在下一节特性选择技术中提供所有这些问题的答案。在这里，我将给您一个关于特性选择技术的基本概念。我建议您在我们目前开发的NLP应用程序中实际实现它们。\n特征选择技术\n使一切尽可能简单但不简单\n阿尔伯特·爱因斯坦的这句话，在我们谈论特征选择技术时是非常正确的。我们已经看到为了摆脱维度性的诅咒，我们需要特征选择技术。我们将介绍以下功能选择技术：\n滤波法\n包装方法\n嵌入法- 滤波法\n特征选择完全是一个独立的活动，独立于ML算法。对于数字数据集，通常在预处理数据时使用此方法，对于NLP域，应在将文本数据转换为数字格式或矢量格式后执行此方法。让我们首先在图8.67中看到这个方法的基本步骤：\n这些步骤非常清楚，也不需要解释。在这里，我们使用统计技术来给我们打分，基于此，我们将决定是保留这个特性，还是只删除它。参见图8.68：\n如果特征和响应都是连续的，那么我们将执行相关性 如果特征和响应都是分类的，那么我们将使用Chi Square；在NLP中（我们主要使用这个） 如果特征是连续的，响应是分类的，那么我们将使用线性判别分析（LDA） 如果特征是分类的，响应是连续的，那么我们将使用方差分析。\n我将更加关注NLP领域，并解释LDA和Chi Square的基础知识。\nLDA通常用于寻找表征或分离一类以上分类变量的特征的线性组合，而与LDA相比，Chi Square主要用于NLP。将卡方应用到一组分类特征中，以了解使用频率分布的特征之间相关或关联的可能性。\n— 包装方法\n在这个方法中，我们正在寻找最佳特性集。这种方法计算上非常昂贵，因为我们需要为每次迭代搜索最佳特性子集。参见图8.69中的基本步骤：\n\n我们可以使用三种子方法来选择最佳功能子集：\n正向选择\n向后选择\n递归特征消除在正向选择中，我们从没有特性开始，并在每次迭代中添加改进MLModel的特性。我们继续这个过程，直到我们的模型不能进一步提高其精度。\n反向选择是另一种方法，我们从所有特性开始，在每次迭代中，我们找到最佳特性并删除其他不必要的特性，然后重复，直到在ML模型中观察到没有进一步的改进。\n递归特征消除使用贪婪的方法来找出性能最好的特征子集。它反复创建模型，并为每个迭代保留最佳或最差的性能特性。下一次，它使用最好的特性并创建模型，直到所有特性都用尽为止；最后，它根据消除它们的顺序对特性进行排序。\n- 嵌入法\n在这个方法中，我们结合了过滤器和包装器方法的特性。该方法由具有自己内置特征选择方法的算法实现，见图8.70：\n\n8.3.6 维度约减\n降维是机器学习中一个非常有用的概念。如果我们在开发我们的ML模型时包含了很多特性，那么有时我们会包含一些真正不需要的特性。有时我们需要高维的特征空间。有哪些方法可以让我们的功能空间有一定的意义？因此，我们需要一些技术来帮助我们去除不必要的特征，或者将我们的高维特征空间转换为二维或三维特征，以便我们能够看到所有发生的事情。顺便说一下，我们在第6章，高级功能工程和NLP算法中使用了这个概念，当时我们开发了一个应用程序，生成了word2vec用于权力游戏数据集。当时，我们使用t-分布随机邻域嵌入（t-sne）降维技术将我们的结果可视化到二维空间中。\n在这里，我们将看到最著名的两种技术，即主成分分析（PCA）和T-SNE，用于在二维空间中可视化高维数据。那么，我们开始吧。\n主成分分析\nPCA是一种使用正交变换将一组可能相关特征的数据点转换为一组线性不相关特征值的统计方法，称为主成分。主成分的数量小于或等于原始特征的数量。这种技术定义转换的方式是，第一个主要组件对每个后续特性具有最大的可能方差。\n\n这个图对理解PCA有很大帮助。我们取了两个主成分，它们彼此正交，并且使方差尽可能大。在C图中，我们通过在一条直线上投影，把尺寸从二维缩小到一维。\nPCA的缺点是，当你减少维度时，它就失去了数据点所代表的意义。如果解释性是维度减少的主要原因，那么就不应该使用PCA；可以使用T-SNE。\nt-SNE\n这是帮助我们可视化高维非线性空间的技术。T-SNE试图保留紧密相连的本地数据点组。当你想看到高维空间时，这项技术将帮助你。您可以使用它来可视化使用Word2vec、图像分类等技术的应用程序。有关详细信息，请参阅此链接：https://lvdmaaten.github.io/tsne/\n8.4 自然语言处理中的混合方法\n混合方法有时确实有助于我们改进NLP应用程序的结果。例如，如果我们正在开发一个语法修正系统，一个模块识别多字表达式，如kick the bucket，一个基于规则的模块识别错误的模式并生成正确的模式。这是一种混合方法。让我们为同一个NLP应用程序再举一个例子。您正在创建一个分类器，用于标识句子中名词短语的正确文章（限定词-a、an和the）。在这个系统中，您可以分为两类-a/an和the。我们需要开发一个分类器，它将生成限定符类别，无论是a/an还是the。一旦我们为名词短语生成了文章，我们就可以应用一个基于规则的系统来进一步确定第一类a/an的实际限定符。我们也知道一些英语语法规则，我们可以用它来决定我们应该用a还是an。这也是混合方法的一个例子。为了更好地进行情绪分析，我们还可以使用混合方法，包括基于词汇的方法、基于ML的方法或Word2vec或Glove预训练模型，以获得真正高的准确性。所以，要有创意，了解你的NLP问题，这样你就可以利用不同类型的技术，使你的NLP应用程序更好。后处理是一种基于规则的系统。假设您正在开发一个机器翻译应用程序，并且您生成的模型会犯一些特定的错误。你想让机器翻译（MT）模型避免这些错误，但要避免这些错误需要很多特性，这些特性会使训练过程变慢，使模型过于复杂。另一方面，如果你知道有一些简单的规则或近似值，一旦生成了输出，就可以帮助你使之更准确，然后我们可以使用后处理我们的机器翻译模型。混合模型和后处理有什么区别？让我来澄清你的困惑。在给定的示例中，我使用了单词近似。因此，您也可以应用近似值，例如应用阈值来调整结果，而不是使用规则，但只有当您知道近似值会给出准确的结果时，才应该应用近似值。这种近似应该对NLP系统进行足够的推广。\n8.5 总结\n在本章中，我们研究了ML的基本概念，以及NLP领域中使用的各种分类算法。在NLP中，与线性回归相比，我们主要使用分类算法。我们已经看到一些非常酷的例子，比如垃圾邮件过滤、情绪分析等等。我们还重新访问了词性标注示例为您提供更好的理解。我们研究了无监督的ML算法和重要概念，如偏差-方差权衡、欠拟合、过度拟合、评估矩阵等。我们还了解了特征选择和降维。我们还讨论了混合ML方法和后处理。因此，在本章中，我们主要了解如何开发和微调NLP应用程序。在下一章中，我们将看到一个机器学习的新时代——深度学习。我们将探索人工智能所需的基本概念。在此之后，我们将讨论深度学习的基础知识，包括线性回归和梯度下降。我们将看到为什么深度学习成为过去几年最流行的技术。我们将看到与深度学习相关的数学必要概念，探索深度神经网络的结构，并开发一些很酷的应用程序，如NLU域的机器翻译和NLG域的文本摘要。我们将使用TensorFlow、Keras和其他一些最新的工具来实现这一点。我们还将看到可以应用于传统ML算法和深度学习算法的基本优化技术。让我们在下一章深入了解深入学习的世界！\n致谢\n《Python自然语言处理》1 2 3，作者：【印】雅兰·萨纳卡(Jalaj Thanaki），是实践性很强的一部新作。为进一步深入理解书中内容，对部分内容进行了延伸学习、练习，在此分享，期待对大家有所帮助，欢迎加我微信（验证：NLP），一起学习讨论，不足之处，欢迎指正。\n\n参考文献\nhttps://github.com/jalajthanaki ↩︎\n《Python自然语言处理》,（印）雅兰·萨纳卡（Jalaj Thanaki） 著 张金超 、 刘舒曼 等 译 ,机械工业出版社,2018 ↩︎\nJalaj Thanaki ，Python Natural Language Processing ，2017 ↩︎","date":"2019年02月03日 12:06:18"}
{"_id":{"$oid":"5d36a8f06734bd8e681d5f10"},"title":"NLP 工具包 大调查 自然语言处理工具包合集","author":"Scofield_Phil","content":"NLP 工具包 大调查 自然语言处理工具包合集\n\n原创作品， 转载请注明出处：[ Mr.Scofield  http://blog.csdn.net/scotfield_msn/article/details/72904863  ]\nFrom RxNLP.\n\n\n\n\n\n\n可以想一想，如何你把NLP领域的所有的工具都能掌握的数如家珍，是不是很NB？必然的。\n只用过这里面的一部分。。。\n这份调查是基于使用语言差别来归纳的，别问我什么这么分类哈。。。\n\n\n\n\n一、多语言多环境编译\n\n1、THULAC{\n分词\n词性标注\n\n\n}c++/python/java\n\n\n2.NLPIR2016：{\n汉语分词系统NLPIR(前身ICTCLAS)\n\n\n分词标注、实体抽取、词频统计、关键词提取、Word2vec、文本分类、情感分析、依存文法、繁简编码转换、自动注音、摘要提取\n{\noriginal:https://github.com/NLPIR-team/NLPIR\njava JNI:http://blog.csdn.net/u010161379/article/details/50813012\npython:http://blog.csdn.net/junkichan/article/details/51883160\n}\n\n\n}c++/python/java\n\n\n3.crfsuite{\nA fast implementation of Conditional Random Fields (CRFs):\nFast training and tagging\n}c++/python\n\n\n4.哈工大LTP{一整套中文语言处理系统,以网络服务(Web Service)的形式进行使用。\n1、分词\n2、词性标注\n3、依存句法分析\n……\n\n\n}c++/java\n\n\n5.Libsvm{ Libsvm和Liblinear都是国立台湾大学的Chih-Jen Lin博士开发的\nLibsvm主要是用来进行非线性svm 分类器的生成\n\n\n}c++/python/java/matlab\n\n\n6.Liblinear{\nLiblinear则是去年才创建的，主要是应对large-scale的data classification，因为linear分类器的训练比非线性分类器的训练计算复杂度要低很多，时间也少很多，而且在large scale data上的性能和非线性的分类器性能相当，所以Liblinear是针对大数据而生的\n\n\n}c++/python/java/matlab\n\n\n\n\n二、Java\n1、IKAnalyzer {\n中文分词工具包\n\n\n}\n\n\n\n2、FNLP{综合性工具\n信息检索： 文本分类 新闻聚类\n中文处理： 中文分词 词性标注 实体名识别 关键词抽取 依存句法分析 时间短语识别\n结构化学习： 在线学习 层次分类 聚类\n}\n\n\n3.hanLP{综合性工具\n分词\n词典\n命名实体识别\n篇章理解\n简繁拼音转换\n依存句法解析\n智能推荐\n\n}\n\n\n4.openNLP{综合性工具\nApache OpenNLP\n句法检测器：Sentence Detector\n分词器：Tokenizer\n名字查找器：Name Finder\n文档分类器：Document Categorizer\n词性标注器：Part-of-Speech Tagger\n组块分析器：Chunker\n解析器：Parser\n指代消解：Coreference Resolution\n}\n\n\n5.stanfod NLP{综合性工具\nStanford CoreNLP：分词、词性标注、命名实体识别、语法分析\nStanford Word Segmenter：采用CRF（条件随机场）算法进行分词\nStanford POS Tagger：采用Java编写的面向英文、中文、法语、阿拉伯语、德语的命名实体识别工具\nStanford Named Entity Recognizer：采用条件随机场模型的命名实体工具\nStanford Parser：进行语法分析的工具\nStanford Classifier：\n}\n\n\n6、ansj{\n基于中科院的 ictclas 中文分词算法\n中文分词.\n实体识别\n用户自定义词典\n\n\n}\n\n\n\n7.lingpipe{\n主题分类（Top Classification）\n命名实体识别（Named Entity Recognition）\n词性标注（Part-of Speech Tagging）\n句题检测（Sentence Detection）\n查询拼写检查（Query Spell Checking）\n兴趣短语检测（Interseting Phrase Detection）\n聚类（Clustering）\n字符语言建模（Character Language Modeling）\n医学文献下载/解析/索引（MEDLINE Download, Parsing and Indexing）\n数据库文本挖掘（Database Text Mining）\n中文分词（Chinese Word Segmentation）\n情感分析（Sentiment Analysis）\n语言辨别（Language Identification）\n\n\n}\n\n\n8.GATE：The General Architecture for Text Engineering {\n针对不同的用例提供了一系列子项目\n\n\n}\n\n\n9.MALLET: Machine Learning for Language Toolkit{\n文档分类、聚类、主题建模和信息提取\n\n\n}\n\n\n\n\n三、C/C++\n1、CRF++{\n\ncrf实现工具，适用于序列标注问题\n\n\n}\n\n\n2. HTK ：Hidden Markov Model Toolkit{英国剑桥大学工程学院开发的隐马尔可夫模型\n做语音识别的\n\n}\n\n\n3.svm light{\nSVMlight is an implementation of Support Vector Machines (SVMs) in C\n\n\n}\n\n\n\n\n四、※Python\n\n\n调查除下列包之外：{\nscipy\nnumpy\nsklearn\npandas\nmatplt\niPython\nPyBrain\nPyML - machine learning in Python\nMilk：Machine learning toolkit in Python.\nPyMVPA: MultiVariate Pattern Analysis (MVPA) in Python\nPyrallel - Parallel Data Analytics in Python\nMonte - gradient based learning in Python\nxgboost\n}\n\n\n1.gensim{\nCorpora and Vector Spaces\nTopics and Transformations：LSA，LDA，TF-IDF，\nExperiments on the English Wikipedia\nDistributed Computing：word2vec\n\n\n}\n\n\n2.jieba{\n功能 1)：分词\n功能 2) ：添加自定义词典\n功能 3) ：关键词提取\n功能 4) : 词性标注\n功能 5) : 并行分词\n功能 6) : Tokenize：返回词语在原文的起始位置\n\n\n}\n\n\n3.NLTK{\nTokenize and tag some text:\nIdentify named entities:\nDisplay a parse tree:\n它提供了 WordNet 这种方便处理词汇资源的借口，还有分类、分词、除茎、标注、语法分析、语义推理等类库\n}\n\n\n4.TextBlob{\n词性标注，\n名词性成分提取，\n情感分析，\n文本翻译\n\n\n}\n\n\n5.PyNLPI{\n处理N元搜索，计算频率表和分布，建立语言模型。他还可以处理向优先队列这种更加复杂的数据结构，或者像 Beam 搜索这种更加复杂的算法。\nSegmenting Text\nGetting Key Words\n\n}\n\n\n6.spaCy{结合Python和Cython：是具有工业级强度的Python NLP工具包\n英文断句\n词干化（Lemmatize):\n词性标注(POS Tagging):\n命名实体识别（NER）：\n名词短语提取：\n\n\n}\n\n\n7.polyglot{\nTokenization\nLanguage detection\nMorphological analysis\nNamed Entity Recognition\nSentiment Analysis\nWord Embeddings\n}","date":"2017年06月07日 23:06:18"}
{"_id":{"$oid":"5d36a8f16734bd8e681d5f12"},"title":"自然语言处理-BM25相关度打分","author":"weixin_41090915","content":"哈尔滨工程大学-537\n自然语言处理-BM25相关度打分\n(注：文中大写Query、Document等代表集合，小写query、document等代表集合中的个体)\n一、优缺点\n适用于：在文档包含查询词的情况下，或者说查询词精确命中文档的前提下，如何计算相似度，如何对内容进行排序。\n不适用于：基于传统检索模型的方法会存在一个固有缺陷，就是检索模型只能处理 Query 与 Document 有重合词的情况，传统检索模型无法处理词语的语义相关性。\n白话举例：提出一个query：当下最火的女网红是谁？\n在Document集合中document1的内容为：[当下最火的男明星为鹿晗]；\ndocument2的内容为：[女网红能火的只是一小部分]。\n显然document1和document2中都包含[火]、[当下]、[网红]等词语。但是document3的内容可能是：[如今最众所周知的网络女主播是周二柯]。很显然与当前Query能最好匹配的应该是document3，可是document3中却没有一个词是与query中的词相同的（即上文所说的没有“精确命中”），此时就无法应用BM25检索模型。\n二、算法核心\nBM25算法是一种常见用来做相关度打分的公式，思路比较简单，主要就是计算一个query里面所有词\nq1,q2...qn\nq\n1\n,\nq\n2\n.\n.\n.\nq\nn\nq_1,q_2...q_n和文档的相关度，然后再把分数做累加操作。公式如下：\n\nScore(Q,d)=∑inWi⋅R(qi,d)(8)\n(8)\nS\nc\no\nr\ne\n(\nQ\n,\nd\n)\n=\n∑\ni\nn\nW\ni\n⋅\nR\n(\nq\ni\n,\nd\n)\nScore(Q,d)=\\sum_i^n{W_i}\\cdot{R(q_i,d)}\n其中\nR(qi,d)\nR\n(\nq\ni\n,\nd\n)\nR(q_i,d)是查询语句query中每个词\nqi\nq\ni\nq_i和文档d的相关度值，\nWi\nW\ni\nW_i是该词的权重，最后将所有词的\nWi∗R(qi,d)\nW\ni\n∗\nR\n(\nq\ni\n,\nd\n)\nW_i*R(q_i,d)相加。\nWi\nW\ni\nW_i一般情况下为\nIDF(InverseDocumentFrequency)\nI\nD\nF\n(\nI\nn\nv\ne\nr\ns\ne\nD\no\nc\nu\nm\ne\nn\nt\nF\nr\ne\nq\nu\ne\nn\nc\ny\n)\nIDF(Inverse Document Frequency)值，即逆向文档频率，公式如下：\n\nIDF(qi)=logN+0.5n(qi)+0.5(9)\n(9)\nI\nD\nF\n(\nq\ni\n)\n=\nl\no\ng\nN\n+\n0.5\nn\n(\nq\ni\n)\n+\n0.5\nIDF(q_i)=log\\frac{N+0.5}{n(q_i)+0.5}\n其中\nN\nN\nN是文档总数，\nn(qi)\nn\n(\nq\ni\n)\nn(q_i)是包含该词的文档数，0.5是调教系数，避免\nn(qi)=0\nn\n(\nq\ni\n)\n=\n0\nn(q_i)=0的情况。\nlog\nl\no\ng\nlog函数是为了让IDF的值受\nN\nN\nN和\nn(qi)\nn\n(\nq\ni\n)\nn(q_i)的影响更加平滑。\n从公式中显然能看出IDF值的含义：即总文档数越大，包含词\nqi\nq\ni\nq_i的文档数越小，则\nqi\nq\ni\nq_i的IDF值越大。\n白话举例就是：比如我们有1万篇文档，而单词basketball,Kobe Bryant几乎只在和体育运动有关的文档中出现，说明这两个词的IDF值比较大，而单词is, are, what几乎在所有文档中都有出现，那么这几个单词的IDF值就非常小。\n解决了\nWi\nW\ni\nW_i，现在再来解决\nR(qi,d)\nR\n(\nq\ni\n,\nd\n)\nR(q_i,d)。\nR(qi,d)\nR\n(\nq\ni\n,\nd\n)\nR(q_i,d)公式如下：\n\nR(qi,d)=fi⋅(k1+1)fi+K⋅qfi⋅(k2+1)qfi+k2(10)\n(10)\nR\n(\nq\ni\n,\nd\n)\n=\nf\ni\n⋅\n(\nk\n1\n+\n1\n)\nf\ni\n+\nK\n⋅\nq\nf\ni\n⋅\n(\nk\n2\n+\n1\n)\nq\nf\ni\n+\nk\n2\nR(q_i,d)=\\frac{{f_i}\\cdot{(k_1+1)}}{f_i+K}\\cdot{\\frac{{qf_i}\\cdot{(k_2+1)}}{qf_i+k_2}}\n其中\nk1,k2,b\nk\n1\n,\nk\n2\n,\nb\nk_1,k_2,b都是调节因子，一般\nk1=1,k2=1,b=0.75\nk\n1\n=\n1\n,\nk\n2\n=\n1\n,\nb\n=\n0.75\nk_1=1,k_2=1,b=0.75。\n式中\nqfi\nq\nf\ni\nqf_i为词\nqi\nq\ni\nq_i在查询语句\nquery\nq\nu\ne\nr\ny\nquery中的出现频率，\nfi\nf\ni\nf_i为\nqi\nq\ni\nq_i在文档\nd\nd\nd中的出现频率。由于绝大多数情况下一条简短的查询语句\nquery\nq\nu\ne\nr\ny\nquery中，词\nqi\nq\ni\nq_i只会出现一次，即\nqfi=1\nq\nf\ni\n=\n1\nqf_i=1，因此公式可化简为：\n\nR(qi,d)=fi⋅(k1+1)fi+K(11)\n(11)\nR\n(\nq\ni\n,\nd\n)\n=\nf\ni\n⋅\n(\nk\n1\n+\n1\n)\nf\ni\n+\nK\nR(q_i,d)=\\frac{{f_i}\\cdot{(k_1+1)}}{f_i+K}\n其中\nK=k1⋅(1−b+b⋅dlavgdl)(12)\n(12)\nK\n=\nk\n1\n⋅\n(\n1\n−\nb\n+\nb\n⋅\nd\nl\na\nv\ng\nd\nl\n)\nK={k_1}\\cdot{(1-b+b\\cdot{\\frac{dl}{avgdl}})}\n\ndl\nd\nl\ndl为文档\nd\nd\nd的长度，\navgdl\na\nv\ng\nd\nl\navgdl为所有文档的平均长度。意即该文档\nd\nd\nd的长度和平均长度比越大，则\nK\nK\nK越大，则相关度\nR(qi,d)\nR\n(\nq\ni\n,\nd\n)\nR(q_i,d)越小,\nb\nb\nb为调节因子，\nb\nb\nb越大，则文档长度所占的影响因素越大，反之则越小。\n白话举例就是：一个\nquery\nq\nu\ne\nr\ny\nquery为：诸葛亮在哪里去世的？\ndocument1的内容为：诸葛亮在五丈原积劳成疾，最终去世；\ndocument2的内容为：司马懿与诸葛亮多次在五丈原交锋；\n而document3为一整本中国历史书的内容。\n显然document3中肯定包含了大量[诸葛亮]、[哪里]、[去世]这些词语，可是由于document3文档长度太大，所以\nK\nK\nK非常大，所以和\nquery\nq\nu\ne\nr\ny\nquery中每个词\nqi\nq\ni\nq_i的相关度\nR(qi,d)\nR\n(\nq\ni\n,\nd\n)\nR(q_i,d)非常小。\n综上所述，可将BM25相关度打分算法的公式整理为：\n\nScore(Q,d)=∑inIDF(qi)⋅fi⋅(k1+1)fi+k1⋅(1−b+b⋅dlavgdl)(13)\n(13)\nS\nc\no\nr\ne\n(\nQ\n,\nd\n)\n=\n∑\ni\nn\nI\nD\nF\n(\nq\ni\n)\n⋅\nf\ni\n⋅\n(\nk\n1\n+\n1\n)\nf\ni\n+\nk\n1\n⋅\n(\n1\n−\nb\n+\nb\n⋅\nd\nl\na\nv\ng\nd\nl\n)\nScore(Q,d)=\\sum_i^nIDF(q_i)\\cdot\\frac{{f_i}\\cdot{(k_1+1)}}{f_i+k_1\\cdot{(1-b+b\\cdot{\\frac{dl}{avgdl}})}}","date":"2018年01月13日 20:37:18"}
{"_id":{"$oid":"5d36a8f16734bd8e681d5f15"},"title":"面向自然语言处理的深度学习","author":"自然语言处理博客","content":"面向自然语言处理的深度学习\n作者：[印]帕拉什·戈雅尔（Palash Goyal）苏米特·潘迪\n出版时间：2019-02-18\n出版社：机械工业出版社","date":"2019年04月04日 22:19:25"}
{"_id":{"$oid":"5d36a8f26734bd8e681d5f18"},"title":"自然语言处理NLP（2）——统计语言模型、语料库","author":"echoKangYL","content":"在上一部分中，我们已经了解了自然语言处理的基本知识：自然语言处理NLP（1）——概述。\n在这一部分中，我们将简要介绍NLP领域的基本模型——语言模型，我们还将对自然语言处理的基础——语料库的概念进行介绍。这些都是在学习自然语言处理之前所必备的知识。此外，我们默认大家有一定的信息论和概率论基础，在这里不对信息论和概率论知识进行赘述。\n接下来，我们进入正题。\n【一】语言模型\n在这一部分中，我们讨论的语言模型主要是统计语言模型，除此之外，我们在今后的文章中还会对神经网络语言模型进行介绍。\n所谓语言模型，就是利用数学的方法描述语言规律。而统计语言模型，就是用句子S出现的概率P(S)来刻画句子的合理性（而不进行语言学分析处理），这是统计自然语言处理的基础模型。\n假设句子S=w1,w2,...,wn，其中，wi可以暂时看作句子中的第i个词（在后面会进行具体介绍）。由于自然语言是上下文相关的信息传递方式，可以很自然地讲句子S出现的概率定义如下：\nP(S) = P(w1)P(w2|w1)...P(wn|w1,w2,...wn-1)\n特别地，当i=1时，P(w1|w0) = P(w1)，概率定义与条件概率相同。\n在统计语言模型中，输入是句子S，输出是句子S的概率P(S)，模型参数是各个P，即，P(wi|w1,w2,...,wi-1)。\nwi被称为统计基元，可以是字、词、短语、词类等等，通常以“词”代替；\nwi由w1,w2,...wi-1决定，由特定一组w1,w2,...wi-1构成的一个序列称为wi的历史。\n说到这里，相信大家已经发现了一个很现实却很重要的问题：在这个模型中，参数的数量也太多了！\n假设我们的统计基元个数为L（在这里可以将其理解为词汇表），那么一句话中的第i个基元就有L^(i-1)种不同的历史情况。我们必须考虑到这所有不同的历史情况下产生第i个基元的概率，于是，对于长度为m的句子，模型中有L^m个自由参数P(wm|w1…wm-1)。\n这样算来，这会是一个很可怕的参数数量，假设L = 5000（词汇表中的词数有5000不过分吧），m = 3，模型中的自由参数就达到了1250亿！这还仅仅是对应着一个三个词的句子，而汉语中平均每句话中有22个词，这将是一个天文数字。\n要解决这个问题，就要减少历史基元的个数，也就是减少决定wi的历史词的数目。\n等等！看到这里，是不是有点熟悉，这不就是大名鼎鼎的马尔可夫链嘛！（不熟悉马尔可夫链的朋友们可以自行百度一下~）\n没错，我们称其为马尔可夫方法：假设任意一个词wi出现的概率只与它前面的wi-1有关，将原模型简化为二元模型：\nP(S) = P(w1)P(w2|w1)...P(wi|wi-1)...P(wn|wn-1)\n在此基础上，提出n元文法（n-gram）：一个词由它前面的n-1个词决定（注意，是n-1哦，不是n）。\n一元文法（1-gram）：n=1，P(wi|w1,w2,...,wi-1) = P(wi)，出现在每一位上的基元wi独立于历史；\n二元文法（2-gram）：n=2，P(wi|w1,w2,...,wi-1) = P(wi|wi-1)，1阶马尔可夫链；\n三元文法（3-gram）：n=3，P(wi|w1,w2,...,wi-1) = P(wi|wi-2,wi-1)，2阶马尔可夫链；\n……\n以此类推。\n理论上讲，n元文法中的n越大越好（越能保留句子词之间的相关性），但是正如上文所说，n越大也就意味着需要估计的参数越多。一般来讲，n=3用得相对较多，n=4的时候参数就已经太多了。\n值得注意的是，即使采用n较大的高阶模型，也无法覆盖全部的语言现象（因为现阶段，对于人类来讲，语言依然是一个谜）。\n了解了语言模型之后，问题来了：如何估计语言模型的参数呢？（用于计算P(S)的各个P(wi|wi-n+1,...,wi-1)值）\n利用极大似然估计（Maximum Likelihood Evaluation，MLE）。\n从个人理解的角度出发，我认为利用极大似然估计在语料中统计语言模型参数的过程，可以形象化地概括为两个字：数（三声）数（四声）。\n极大似然估计的过程大致如下：\n假设我们的语料库中只有三句话（语料库的概念将在后面进行介绍）：\n1. John read Moby Dick\n2. Mary read a different book\n3. She read a book by Cher\n现在，我们想要估计2-gram模型中，‘John read a book’ 这句话出现的概率。\nP(‘John read a book’ ) = P(John|\u003cBOS\u003e)P(read|John)P(a|read)P(book|a)P(\u003cEOS\u003e| book)，其中\u003cBOS\u003e和\u003cEOS\u003e分别表示句子的开始符和结束符。\n我们可以从语料中（1，2，3一共三句话）统计得到：\nP(John|\u003cBOS\u003e) = 1/3，因为\u003cBOS\u003e出现了三次（每个句子的开始），后面接John的有一次——即，John出现在句首；\nP(read|John) = 1/1，因为John出现了一次，后面接read的有一次；\nP(a|read) = 2/3，因为read出现了3词，后面接a的有一次；\nP(book|a) = 1/2，因为a出现了2次，后面接book的有一次；\nP(\u003cEOS\u003e| book) = 1/2，因为book出现了2次，book出现在句子结尾（后面接\u003cEOS\u003e）的有1次。\n因此，P(S) = 1/18。\n通过这个小小的例子，我们可以发现，语言模型可以预测句子的下一个词（已知前面的词），也可以计算一句话出现的概率，决定哪一个词序列出现的概率大。\n下面，我们再举一个例子：\n还是上面的语料库，还是使用2-gram，我们想要求‘Cher read a book’的概率。\n看起来没什么问题啊，按着上面的套路做就可以了啊，但是在实际计算的时候我们发现：P(Cher|\u003cBOS\u003e) = 0，Cher这个词根本就没有出现在句首过！\n这也就意味着，无论后面几个概率值是多少，我们计算出来的整个句子出现的概率都是0。\n这个问题被称为零概率问题，是由数据稀疏（匮乏）造成的。\n那么遇到了这个问题，该如何解决呢？\n进行数据平滑。\n数据平滑的方法有很多种，在这里不一一进行赘述，有兴趣的朋友可以自行百度了解。\n总之，数据平滑之后，不会出现上面例子中出现的零概率问题，我们就可以安安心心地用语言模型计算一句话出现的概率啦~\n到这里，对语言模型的介绍就基本结束了，我们还剩下一些边边角角的问题需要解决：比如，如何评价语言模型。\n目前，语言模型的评价一般有两种方法：\n1.实用方法：通过查看该模型在实际应用（如机器翻译）中的表现来评价，优点是直观、实用，缺点是缺乏针对性、不够客观。\n2.理论方法：为了评价语言模型，提出了一个“困惑度”的概念，方法如下：\n平滑过后的n-gram模型句子的概率为（与语言模型中的描述相同）：P(S) = P(w1)...P(wi|wi-n+1,...,wi-1)...\n假设测试语料T由k个句子t1,t2,...,tk构成，那么整个测试集T的概率为：p(T) = P(t1)P(t2)...P(tk)\n定义模型对于测试预料的交叉熵为：\n其中，分母项为测试语料T中的词数（可以理解为测试文本集的单词表）。\n定义模型的困惑度为：\n困惑度越低，模型评价越高。\n在这里，多说几句题外话：\n对于一个模型而言，最重要的部分有四个：输入、输出、参数、对应关系（函数）。\n对于任何一个模型而言，把握住这四个基本要素，相信会帮助大家理解得更加透彻（特别是今后我们会提到的神经网络）。\n【二】语料库（corpus）\n语料库是什么？\n语料库就是，存放在计算机里的原始语料文本 或 经过加工后带有语言学信息标注的语料文本。为了方便理解，我们可以将其看作一个数据库，我们从中提取语言数据，以便对其进行分析、处理。\n语料库有三点特征：\n1.语料库中存放的是在实际使用中真是出现过的语言材料。\n2.语料库是以计算机为载体承载语言知识的基础资源，但并不等于语言知识。\n3.真实语料需要经过分析、处理和加工，才能成为有用的资源。（这一点尤为重要，在NLP知识的学习中，需要对语言原材料进行一系列的处理才能够使用）\n在这里，不对语料库进行系统的介绍，仅需要了解“我们可以从中提取语言数据来用”就可以啦~\n如果有朋友对语料库的分类和现阶段一些著名的语料库感兴趣，可以自行百度了解~\n现在，我们已经对统计语言模型和语料库有了基本的了解，在下一部分的内容中，我们将介绍神经网络语言模型和一些在NLP领域常用的基本的神经网络。\n如果本文中某些表述或理解有误，欢迎各位大神批评指正。\n谢谢！","date":"2019年01月24日 16:29:23"}
{"_id":{"$oid":"5d36a8f36734bd8e681d5f1a"},"title":"自然语言处理中的几种文本预处理的写法总结","author":"君的名字","content":"写在前面的话\nbiaji,\u003c(￣3￣)\u003e bia叽，嘎嘎，最近来教大家写点简单又迷人的自然语言处理的代码。\n不好意思，原谅我用词不当，毕竟我是菜鸟，也没得资格教别人，the main reason is that 我自己写了给自己看的，你看我就是这样一个正直，又不爱慕虚荣的小公主呢～\n感觉自己萌萌哒，啊哈哈哈，不要脸也确实是真的\n反正也没有啥子浏览量，估计就是自己每天看自己写的了呢，所以在我的地盘就听我的，啊哈哈哈\n这女的一定是刚刚从精神病院里跑出来的。\n哦，对了，今天知乎有个推送，笑死我了，怎么在精神病院里证明自己不是神经病！！！！\n我的天，好想怒答一波，问这个人的脑子一定刚刚被门挤了吧，啊哈？\n假装很正紧的正文\n其实个人觉得NLP 就是所谓的nature language processing 在计算机领域入门还真的是没有任何难度呢。当然了，个人意见，觉得你安装几个库，nltk,gensim，sklearn，textblob 什么standford parser 斯坦福解析器，然后自己捣鼓捣鼓，学一些分类算法，聚类算法，topic model 算法之后，我觉得你就入门了。\n就算你不学算法的各种基本原理，你知道怎么用，你也可以很快上手的。\n预处理的话我们要安装一个库，我们就用nltk 来做预处理把，textblob 可以用来修改一些拼写的错误，但是呢，感觉没有考虑语境，所以有时候改正的效果其实并不好，nltk 的处理效果本人觉得也就是那个样子啦。\n不过我们先学吧，你得会了才能评价，不会，听别人说怎么滴，那也是跟你没有半毛钱关系的。\n我一直用的都是Linux的系统Ubuntu14.04 所以安装也很简单\n就是下面这个样子的啦\nsudo pip install nltk\n如果你是用的是conda 那个就这样安装nltk这个库\nconda install nltk\n这里我们先讨论英文的文本处理\n对于自然语言处理的话，预处理其实就是有那么几个固定的步骤，分词，英文的话可能需要全部转换为小写，去除标点符号，提取词干，出去不是英文的单词，出去特殊的符号，修正错别字。\n这篇写的挺好的，这对英文和非英文的处理都在这里了。\nhttp://www.spiderpy.cn/blog/detail/30\n一些必须知道的基本概念\n在做预处理的时候，我们要知道一些基本的概念，什么叫做分词，什么叫做提取词干\n1.分词 （Tokenization）\nToken 是符号，包括了单词还有标点符号两种。 Tokenization 就是把一句话或者一段话分解成单个的单词和标点。\nI like my cat.\n这句话分词之后就变成了\n['I','like','my','cat','.']\n这样的一个五元组，注意最后的标点符号也是算的。\n2.提取词干（stemming）\n在英文中，常常可能会有一些英文单词的各种变化，比如第三人称的单数，时态等等的变化。\n比如 run 可以变成runnIng，ran,runs 等等，但是我们只要他们的基本态就是run. 这个就叫做提取词干。\n这么做的主要目的是用统一的特征形式，特征降维以减少计算量。在NLTK中提供了三种最常用的词干提取器接口，即 Porter stemmer, Lancaster Stemmer 和 Snowball Stemmer。抽取词的词干或词根形式（不一定能够表达完整语义）\n\u003e\u003e\u003e from nltk.stem.porter import PorterStemmer \u003e\u003e\u003e porter_stemmer = PorterStemmer() \u003e\u003e\u003e from nltk.stem.lancaster import LancasterStemmer \u003e\u003e\u003e lancaster_stemmer = LancasterStemmer() \u003e\u003e\u003e from nltk.stem import SnowballStemmer \u003e\u003e\u003e snowball_stemmer = SnowballStemmer(“english”) \u003e\u003e\u003e porter_stemmer.stem(‘maximum’) u’maximum’ \u003e\u003e\u003e lancaster_stemmer.stem(‘maximum’) ‘maxim’ \u003e\u003e\u003e snowball_stemmer.stem(‘maximum’) u’maximum’ \u003e\u003e\u003e porter_stemmer.stem(‘presumably’) u’presum’ \u003e\u003e\u003e snowball_stemmer.stem(‘presumably’) u’presum’ \u003e\u003e\u003e lancaster_stemmer.stem(‘presumably’) ‘presum’ \u003e\u003e\u003e porter_stemmer.stem(‘multiply’) u’multipli’ \u003e\u003e\u003e snowball_stemmer.stem(‘multiply’) u’multipli’ \u003e\u003e\u003e lancaster_stemmer.stem(‘multiply’) ‘multiply’ \u003e\u003e\u003e porter_stemmer.stem(‘provision’) u’provis’ \u003e\u003e\u003e snowball_stemmer.stem(‘provision’) u’provis’ \u003e\u003e\u003e lancaster_stemmer.stem(‘provision’) u’provid’ \u003e\u003e\u003e porter_stemmer.stem(‘owed’) u’owe’ \u003e\u003e\u003e snowball_stemmer.stem(‘owed’) u’owe’ \u003e\u003e\u003e lancaster_stemmer.stem(‘owed’) ‘ow’\n各有优劣，看具体文本情况。对于分类、聚类这样对于特征词语的具体形态没有要求的情况下，进行词干抽取虽然抽取后的词干可能无实际意义但是却会大大减少计算时间，提高效率。\n以上部分来自这篇博客https://zhangmingemma.github.io/2017/03/29/Python+NLTK-Natural-Language-Process.html，觉得写的很好的。\n词形还原\n词形还原 Lemmatization 是把任何形式的词汇还原为一般形式，能表达完整的语义。相对而言，词干提取是简单的轻量级的词形归并方式，最后获得的结果为词干，但是可能没有实际的意义。词形还原处理相对来说比较复杂，获得结果为词的原形，能够承载一定的意义，与词干的提取相比，更具有研究和应用的价值。\n比如说词干提取，假设这个词是provision得到的是provis这个没有什么实际的意义。\n不过在nltk 中的Lemmatization 算法很鸡肋，基本可以理解为只有复述还原为单数的形式，一些其他的非常态的复数形式转换为单数的形式也是可以实现的。但是形容词变成名词可能会失效。\n具体的例子如下所示：\n\u003e\u003e\u003e from nltk.stem import WordNetLemmatizer \u003e\u003e\u003e wordnet_lemmatizer = WordNetLemmatizer() \u003e\u003e\u003e word = wordnet_lemmatizer.lemmatize('birds') bird\n以上的例子也来自同一篇文章https://zhangmingemma.github.io/2017/03/29/Python+NLTK-Natural-Language-Process.html\n基本操作的代码\n针对这些基本操作我们给出了一些预处理的代码。\n大家之后可以直接拿来用，多看看别人怎么写的，自己再用，那也是极好的，一来提高效率，而来哈哈，积累经验吧。\nimport nltk from nltk.corpus import stopwords from nltk.stem.porter import PorterStemmer from nltk.stem import WordNetLemmatizer def Preprocessing(text): text = text.lower() # 将所有的单词转换成小写字母 for c in string.punctuation: text = text.replace(c,\" \") # 将标点符号转换成空格 wordList = nltk.word_tokenize(text) # 分词 filtered = [w for w in wordList if w not in stopwords.words('english')] # 删除停顿词 # stem ps = PorterStemmer() filtered = [ps.stem(w) for w in filtered] # 提取词干 wl = WordNetLemmatizer() filtered = [wl.lemmatize(w) for w in filtered] # 词形还原 return \" \".join(filtered)\n如果你要修改一些拼写的错误的话，就用textblob 这个包\n我现在只会用pip 来安装，conda 安装直接试了一下没有成功。所以就只能用系统自带的那个Python来运行这个自然语言的脚本啦。\n安装TextBlob\nsudo pip install -U textblob\n在使用的时候只要事先引进它就行，也可以用这个工具来做预处理\n拼写矫正的代码如下：\n\u003e\u003e\u003e b = TextBlob(\"I havv goood speling!\") \u003e\u003e\u003e print(b.correct())\n还看到了一些比较有意思的处理，决定收录一下：\nfrom nltk.corpus import stopwords from nltk.stem.porter import PorterStemmer from nltk.tokenize import word_tokenize stopset = stopwords.words('english') + list(string.punctuation) + ['will','also','said'] corpus = [] all_docs = [] vocab = set() stemmer = PorterStemmer() with open(filename) as f: try: doc = f.read().splitlines() doc = filter(None,doc) # remove empty string doc = '.'.join(doc) doc = doc.translate(None,string.punctuation) doc = doc.translate(None,'0123456789') doc = doc.decode(\"utf8\").encode(\"utf-8\",'ignore') all_docs.append(doc) tokens = word_tokenize(str(doc)) filtered = [] for w in tokens: w = stemmer.stem(w.lower()) if w in stopset: continue filtered.append(w) vocab.update(filtered) corpus.append(filtered) except UnicodeDecodeError: print \"Failed to load:\", filename\nReference\nhttps://zhangmingemma.github.io/2017/03/29/Python+NLTK-Natural-Language-Process.html\nhttp://www.voidcn.com/article/p-kwpvxxsc-bch.html","date":"2018年08月30日 15:51:48","data":"2018年08月30日 15:51:48"}
{"_id":{"$oid":"5d36a8f36734bd8e681d5f1c"},"title":"自然语言处理NLP之终极指南（Pytho…","author":"秦陇纪10数据简化DataSimp","content":"理解和使用\n自然语言处理\n之终极指南（\nPython\n编码）（\n经典收藏\n版\n12k\n字\n，\n附数据简化\n筹员\n2\n月\n17\n日\nFri\n新闻\n）\n秦陇纪\n10\n译\n编\n12k字\n：\n理解和使用\n自然语言处理\n之终极指南（\nPython编码）7k字；附数据简化DataSimp筹收\n技术\n简历\n414\n字、\n2月17日Fri新闻\n四则\n4k字\n。\n欢迎加入共建\n“数据简化DataSimp”\n学会及\n社区\n，\n关注、收藏、转发新媒\n体\n“\n数据简化\nDataSimp\n、科学\nSciences”微信号、头条号\n，\n转载请\n写\n出处：\n秦陇纪\n10“数据简化DataSimp/科学Sciences”公众号、头条号\n译编\n，\n投稿\nQinDragon2010@qq.com。\n目录\n理解和使用自然语言处理之终极指南（\nPython\n编码）（\n7.4k\n字）\n附\nA.\n数据简化\nDataSimp\n筹备收简历\n(414\n字\n)\n附\nB. 2017年2月17\n日周\n五\n（农历丁酉鸡年正月\n廿一\n）新闻四则\n汇编\n(4.8k\n字\n)\n\n理解和使用自然语言处理之终极指南（\nPython\n编码）\n秦陇纪\n10\n译\n编\n；\n来源：\n仕瓦姆\n·邦萨尔（Shivam Bansal\n）\n,2017年1月12\n日，威提亚分析学\n\n\n\n\n目录表\nTable of Contents\n1.\nIntroduction to NLP\n自然语言处理介绍\n2.\nText Preprocessing\n文本预处理\no\nNoise Removal\n噪声去除\no\nLexicon Normalization\n词汇规范化\n§\nLemmatization\n词变体归类\n§\nStemming\n词干提取\no\nObject Standardization\n对象规范化\n3.\nText to Features (Feature Engineering on text data)\n文本到特征（文本数据之特征工程）\no\nSyntactical Parsing\n句法\n解析\n§\nDependency Grammar\n依存语法\n§\nPart of Speech Tagging\n词性标注\no\nEntity Parsing\n实体解析\n§\nPhrase Detection\n短语检测\n§\nNamed Entity Recognition\n命名实体识别\n§\nTopic Modelling\n主题造型\n§\nN-Grams\nN\n元连续模型\no\nStatistical features\n统计特征\n§\nTF – IDF\n词频\n-\n逆文档词频\n§\nFrequency / Density Features\n频率\n/\n密度特征\n§\nReadability Features\n可读性特征\no\nWord Embeddings\n字嵌入\n4.\nImportant tasks of NLP\n自然语言处理\nNLP\n的重要任务\no\nText Classification\n文本分类\no\nText Matching\n文本匹配\n§\nLevenshtein Distance\n莱文斯坦距离\n§\nPhonetic Matching\n语音匹配\n§\nFlexible String Matching\n柔性字符串匹配\no\nCoreference Resolution\n共指消解\no\nOther Problems\n其他问题\n5.\nImportant NLP libraries\n重要\nNLP\n库\n据业内人士估计，只有\n21%\n可用数据以结构化形式\n存在。数据产生，正如我们所说的，来自于我们的推特、\nWhatsApp\n和其他各种交流活动中发送的信息。大多数这些数据存在于文本的形式，是高度非结构化的性质。一些臭名昭著的例子包括——在社交媒体上的推特\n/\n帖子、用户到用户的聊天对话、新闻、博客和文章、产品或服务审查和医疗部门里的病人记录。最近的一些例子包括聊天机器人和其他声音驱动的机器人程序。\n尽管具有高维数据，但其呈现的信息是\n不可以直接访问\n的，除非它被\n手动处理（读取和理解）\n或由\n自动化系统分析\n。为了从文本数据中产生明显的和可操作的洞察\n/\n见解，熟悉\n自然语言处理（\nNLP\n）的技术和原则\n显得非常重要。那么，如果你打算今年创建聊天机器人，或者你想使用非结构化文本的力量，本指南是正确的起点。本指南挖掘自然语言处理的概念、技术与实现。文章的目的是教会自然语言处理的概念，并将其应用于实际数据集。\n1. Introduction to Natural Language Processing 自然语言处理介绍\nNLP\n是数据科学\n的一个分支，包括智能和高效地从文本数据中分析、理解和导出信息的系统流程。通过\nNLP\n及其组成部分，企业可以组织海量\n文本数据块、执行许多自动化任务、并解决广泛问题，如自动摘要、机器翻译、命名实体识别、关系抽取、情感分析、语音识别、主题分割\n等。\n在进一步研究之前，我想解释一下文章中使用的一些术语：\n·\n标记化\n——\n转换文本到标记体的过程；\n·\n标记体\n——文本中存在的单词或实体；\n·\n文本对象\n——一个句子或一个短语或一个词或一篇文章\n安装\nNLTK\n及其数据的步骤（使用\nPython\n语言及环境）：\n安装\nPip\n：在终端中运行：\nsudo easy_install pip\n安装\nNLTK\n：在终端中运行：\nsudo pip install -U nltk\n下载\nNLTK数据\n：运行\nPython shell\n（在终端）和写下面的代码：\n``` import nltk nltk.download()```\n按照屏幕上的指令下载所需的包或集合。其他库可以直接使用\nPip\n安装。\n2. Text Preprocessing文本预处理\n因此，文本是所有可用数据的最具非结构化的形式，存在于其中的各种类型的噪声，并且没有预处理的数据是不容易分析的。文本清理和标准化的全过程，是一个去除其噪声和称为文本预处理的分析准备工作。\n它主要由三个步骤组成：\n·\nNoise Removal\n噪声去除\n·\nLexicon Normalization\n词汇规范化\n·\nObject Standardization\n对象标准化\n下图显示了文本预处理（清洁）流水线的体系结构。\n\n\n\n2.1 Noise Removal 噪声去除\n任何与数据上下文和最终输出无关的文本片段，都可以指定为\n噪声\n。例如\n——语言停用词（语言常用词\nis/am/the/of/in\n等），\nURL\n或链接，社会媒体实体（提示、哈希标签），标点符号和特定行业用词。此步骤处理移除文本中存在的所有类型噪声实体。\n去除噪声的一般方法是准备一个\n噪声实体字典\n，并通过\n标记符号（或文字）\n来迭代文本对象，消除这些噪声字典呈现出的标记符号。\n以下是\n实现\n相同目的\nPython\n代码。\n```\n# Sample code to remove noisy words from a text\nnoise_list = [\"is\", \"a\", \"this\", \"...\"]\ndef _remove_noise(input_text):\nwords = input_text.split()\nnoise_free_words = [word for word in words if word not in noise_list]\nnoise_free_text = \" \".join(noise_free_words)\nreturn noise_free_text\n_remove_noise(\"this is a sample text\")\n\u003e\u003e\u003e \"sample text\"\n```\n另一种方法是在处理特殊噪声模式时使用\n正则表达式\n。\n之前的\n一篇文章中\n，我们\n详细解释了正则表达式。以下的\nPython\n代码从输入文本\n中移除了\n一个正则表达式模式：\n```\n# Sample code to remove a regex pattern\nimport re\ndef _remove_regex(input_text, regex_pattern):\nurls = re.finditer(regex_pattern, input_text)\nfor i in urls:\ninput_text = re.sub(i.group().strip(), '', input_text)\nreturn input_text\nregex_pattern = \"#[A-Za-z0-9\\w]*\"\n_remove_regex(\"remove this #hashtag from analytics vidhya\", regex_pattern)\n\u003e\u003e\u003e \"remove this\nfrom analytics vidhya\"\n```\n2.2 Lexicon Normalization\n词汇规范化\n另一种文本\n式\n噪声是关于单个词所表现的\n多重表征\n。\n例如\n：\n“玩”、“玩家”，“玩\n过\n”，\n第三人称的\n“玩”和“\n正在\n玩\n”\n（\nplay, player, played, plays and playing\n）\n这\n些\n词\n是单词\n“玩”的不同变化，尽管他们\n的意思是\n不同的\n，但\n内容都是相似的。\n这个\n步骤\n是把\n一个词的所有差异转换\n成\n它们的标准化形式（也称为\nlemma\n引理）。规范化是文本特征工程\n的\n关键步骤，因为它转换的高维特征（\nN\n维度\n不同\n特征\n）到低维空间（\n1\n个特征），是任何\nML\n模型\n的\n一种理想\n解\n。\n最常见的词汇规范化做法是：\n·\n词干提取\n（\nStemming\n）\n：\n词干提取\n是一种基本的\n基于规则的从一个词剥离后缀的过程（后缀\ning, ly, es, s\n等）。\n·\n词变体归类\n（\nLemmatization\n）\n：\n词变体归类\n，从另一方面，是一个有组织且有步骤获得这个词的词根形式的过程，即词汇用法（单词的词典重要性）和形态逻辑分析（词汇结构和语法关系）。\n下面\n的\n示例代码\n是\n用\nPython\n主流库\nNLTK\n执行\n的\n词变体归类\n（\nLemmatization\n）和\n词干提取\n（\nStemming\n）\n。\n```\nfrom nltk.stem.wordnet import WordNetLemmatizer\nlem = WordNetLemmatizer()\nfrom nltk.stem.porter import PorterStemmer\nstem = PorterStemmer()\nword = \"multiplying\"\nlem.lemmatize(word, \"v\")\n\u003e\u003e \"multiply\"\nstem.stem(word)\n\u003e\u003e \"multipli\"\n```\n2.3 Object Standardization\n对象标准化\n文本数据通常包含\n一些\n任何标准\n语义\n字典中不存在的单词或短语。这些\n碎片\n是\n搜索引擎和模型不\n能\n识别\n的\n。\n这方面的\n一些例子是\n——首字母缩略词\n语\n、单词附属哈希\n标签和口语俚语。借助正则表达式和手工编写的数据字典，可以\n找到\n这种类型的噪声，下面的代码使用一个字典查找法从文本\n中\n代替\n社交\n媒体的俚语。\n```\nlookup_dict = {'rt':'Retweet', 'dm':'direct message', \"awsm\" : \"awesome\", \"luv\" :\"love\", \"...\"}\ndef _lookup_words(input_text):\nwords = input_text.split()\nnew_words = []\nfor word in words:\nif word.lower() in lookup_dict:\nword = lookup_dict[word.lower()]\nnew_words.append(word) new_text = \" \".join(new_words)\nreturn new_text\n_lookup_words(\"RT this is a retweeted tweet by Shivam Bansal\")\n\u003e\u003e \"Retweet this is a retweeted tweet by Shivam Bansal\"\n```\n除了讨论到目前为止\n的\n三个步骤，其他类型的\n文本预处理\n包括编码解码噪声\n、\n语法检查器\n、\n拼写校正等\n。\n详细的\n文本\n预处理\n及其\n方法在\n秦陇纪专著\n文章\n有\n。\n3.Text to Features (Feature Engineering on text data)\n文本到特征（文本数据之特征工程）\n对\n预处理数据\n做\n分析，需要将其转换成\n特征\n。根据使用情况，文本特征可用配套技术\n来\n构建\n——语义分析、实体\n/\n克\n/\n基于词的特征、统计特征、字的嵌入。\n实体\n/N\n元连续模型\n/\n基于词的特征、统计特征，和\n单词\n嵌入。\n下面来继续\n阅读\n，以\n详细了解这些技术。\n3.1 Syntactic Parsing\n句法解析\n句法解析\n涉及句中单词的\n语法\n和显示这些单词间关系的\n排列方式\n的分析。依存语法和部分语音标签是文本句法的重要属性。\n依存树\n–句子是由一些单词缝和在一起组成的。句子中词语间的关系由基本依存语法决定。\n依存语法\n是处理两个语义项之间的（标记的）非对称二元关系（单词）的一类语义文本分析法。每一种关系都可以用\n三元组（关系、监督、依存）\n来表示。例如：考虑句子\n“\nBills on ports and immigration were submitted by Senator Brownback, Republican of Kansas.\n”这些单词间的关系，可以用下图所示的\n树形表示\n形式观察到：\n这个\n树\n显示\n“submitted”\n是\n这\n个句子\n的根词，是由两个\n子树\n（主体与客体的子树）相连。每个\n子树\n本身\n一个诸如\n(“Bills” \u003c-\u003e “ports” “proposition” relation)\n、\n(“ports” \u003c-\u003e “immigration” “conjugation” relation)\n关系的\n依存关系树\n。\n这种类型的树，采用自上而下的方法递归解析时，给出了\n的\n语法关系\n三元组作为输出\n——可用于许多\nNLP\n问题的特征，像\n实体情感分析、\n演员和实体识别和文本分类\n。\nPython\n包\n组\n斯坦福\nCoreNLP\n（\n来自\nStanford NLP\n项目\n组，只有商业许可证\n版\n）和\nNLTK\n依存\n语法\n可以用来产生\n依存关系树\n.\n词性标注\n（\nPart of Speech tagging\n）\n–除了语法关系，在一个句子里每个词也\n和\n词性标签（\nPOS\n）\n（名词、动词、形容词、副词等）\n相关联\n。\nPOS\n标签\n定义一个词在句子中的用法和\n功能\n。这是宾夕法尼亚大学定义\n的\n一个所有可能\nPOS\n标签列表。下面的代码使用\nNLTK\n对\n输入文本进行\n词性标注\n注释。（它提供了多种实现方案，默认是\n感知\n标记\n器\n）\n```\nfrom nltk import word_tokenize, pos_tag\ntext = \"I am learning Natural Language Processing on Analytics Vidhya\"\ntokens = word_tokenize(text)\nprint pos_tag(tokens)\n\u003e\u003e\u003e [('I', 'PRP'), ('am', 'VBP'), ('learning', 'VBG'), ('Natural', 'NNP'),('Language', 'NNP'),\n('Processing', 'NNP'), ('on', 'IN'), ('Analytics', 'NNP'),('Vidhya', 'NNP')]\n```\n词性标注\n用于\nNLP\n自然语言处理中的许多重要用途：\nA.\n词义消歧：\n一些语言词汇根据其用法有多种含义。例如，在以下两个句子中：\nI. “Please book my flight for Delhi”\nII. “I am going to read this book in the flight”\n“\nBook\n”\n在\n不同语境使用，\n这\n两种情况下的\n词性标注词\n不同。句\nI\n中，\n“\nBook\n”作为动词，而\nII\n句中\n它\n被用作名词\n。（\nLesk\n算法也用于类似\n目的\n）\nB.提高基于词的\n特征值\n：\n学习模型\n在以\n一个词为特征时，学习词的不同情境，如果词性标注词与他们\n有\n联系，\n则\n上下文被保存，从而\n做出强壮的特征值\n。例如\n:\n句子\nSentence\n-“book my flight, I will read this book”\n标记词\nTokens\n– (“book”, 2), (“my”, 1), (“flight”, 1), (“I”, 1), (“will”, 1), (“read”, 1), (“this”, 1)\n词性标注\n标记\n词\nTokens with POS – (“book_VB”, 1), (“my_PRP$”, 1), (“flight_NN”, 1), (“I_PRP”, 1), (“will_MD”, 1), (“read_VB”, 1), (“this_DT”, 1), (“book_NN”, 1)\nC.\n标准\n化和词变体归类：\nPOS\n标签是词变体归类\n过程的\n基础\n，用于将一个词转换成它的基形式（\nlemma\n引理）\n。\nD.\n有效的停用词去除：\nPOS\n标签在高效去除\n停用词\n也有用。\n例如，有一些标签总是定义一个语言的低频\n/\n不重要的单词。例如：\n(\nIN\n– “\nwithin\n”\n,\n“\nupon\n”\n,\n“\nexcept\n”\n), (\nCD\n– “\none\n”\n,\n”\ntwo\n”\n,\n“\nhundred\n”\n), (\nMD\n– “\nmay\n”\n,\n“\nmu st\n”\netc)\n。\n3.2 Entity Extraction (Entities as features)\n实体提取（实体为特征值）\n实体\n被定义为句子中最重要的句块\n--\n名词短语、动词短语或两者。\n实体检测算法\n通常是基于规则解析\n、\n字典查找\n、\nPOS\n标签、依存句法分析的集成模型。\n实体检测的适用性\n可以在自动聊天机器人\n、\n内容分析\n器\n和消费者洞察\n中看见\n。\n主题\n模型\n和\n命名实体识别\n是\nNLP\n自然语言处理\n里\n两个\n主要的\n实体检测方法。\nA. Named Entity Recognition\n命名实体识别（\nNER）\n检测如人名、地名\n、公司名等\n命名实体的过程称为\nNER\n。例如\n:\n句子\nSentence\n– Sergey Brin, the manager of Google Inc. is walking in the streets of New York.\n命名实体\nNamed Entities\n– ( “person” : “Sergey Brin” ), (“org” : “Google Inc.”), (“location” : “New York”)\n一个典型\nNER\n模型由三块\n组成\n：\n名词短语识别：\n这一步涉及使用依存\n解析\n和词性标注从文本中提取所有名词短语。\n短语分类：\n这是\n将所有被提取名词短语划分为\n所属\n相应类别（位置、名称等）的分类步骤。谷歌地图\nAPI\n提供了消除歧义位置\n的一个\n好路径，然后，从\nDBpedia, wikipedia\n开放数据库可以用来识别个人姓名或公司名称。除此之外，结合来自不同信息源的\n查找表和词典\n可以精确查找。\n实体消歧：\n有时这是可能的，实体的误判分类的，因此\n随之\n创建\n分类\n结果\n之上的\n验证层是有用的。\n出于此目的可以运用\n知识图。流行的知识图\n有\n–\n谷歌知识图，\nIBM\n沃森和维基百科\n。\nB. Topic Modeling\n主题\n模型\n主题建模\n是一个存在\n于\n文本语料库中主题的自动识别过程，它以无监督方式推导出语料库中\n的\n隐含模式。主题被定义为\n“a repeating pattern of co-occurring terms in a corpus”\n。医疗保健\n为主题的\n一个好的\n主题模型结果\n有\n–“health”, “doctor”, “patient”, “hospital”\n（\n“健康”、“医生”、“病人”、“医院”\n）\n，农事\n为主体则有\n–“farm”, “crops”, “wheat”\n（\n“农场”、“庄稼”、“小麦”为话题“农业”\n）\n。\n隐含狄利克雷分配（\nLDA\n）\n是最受欢迎的主题建模技术，以下是\n使用\nLDA\n实现主题建模\n的\nPython\n代码\n。有关其工作和执行的详细说明，请检查这里\n的\n完整文章。\n```\ndoc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\"\ndoc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\ndoc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\ndoc_complete = [doc1, doc2, doc3]\ndoc_clean = [doc.split() for doc in doc_complete]\nimport gensim from gensim\nimport corpora\n# Creating the term dictionary of our corpus, where every unique term is assigned an index.\ndictionary = corpora.Dictionary(doc_clean)\n# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\ndoc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n# Creating the object for LDA model using gensim library\nLda = gensim.models.ldamodel.LdaModel\n# Running and Training LDA model on the document term matrix\nldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n# Results\nprint(ldamodel.print_topics())\n```\nC.\nN-Grams as Features\nN元连续模型N grams作为特征值\nN\n个单词在\n一起\n的组合\n被称为\nN\n元连续模型（\nN grams\n）\n。\n作为特征值，\n相比\n单词\n（一元\n1gram\n）\n，\nN\n元连续模型\n（\nn\n＞\n1\n）通常包含更多信息。另外，双\n单词元\n组（\nn=2\n）被认为是所有其他\n元模型更\n重要的特征。下面的代码生成一个文本二元\n模型实例\n。\n```\ndef generate_ngrams(text, n):\nwords = text.split()\noutput = []\nfor i in range(len(words)-n+1):\noutput.append(words[i:i+n])\nreturn output\n\u003e\u003e\u003e generate_ngrams('this is a sample text', 2)\n# [['this', 'is'], ['is', 'a'], ['a', 'sample'], , ['sample', 'text']]\n```\n3.3 Statistical Features\n统计特征\n文本数据也可以使用本节中描述的几种技术直接量化成数字：\n（欢迎转发声明：秦陇纪\n10\n公众号、头条号“数据简化\nDataSimp\n”科普文章。）\n\n\n附\n数据简化\nDataSimp\n筹备收简历\n(414\n字\n)\n北京数据简化有限责任公司（筹）愿景：\n①行业大数据采集处理分析管理系统，②企事业单位行政人事财物联网智能OA系统，③数据简化DataSimp\n学术组及\n开源社区（中英双语），\n④物联网大数据底层操作系统（整合Linux开源软件\n和通信模块\n）。\n现重点收集数据分析程序\n算法模型研发简历\n，成立前\n/每季度实习生\n在中关村集中面试。有意实习半年、工作一年以上的开发人员，请注明学历和工作简历、职务和职业规划、吃住薪酬预期、个人爱好等事项，投递邮箱\nQinDragon2010@qq.com主题注明：应聘\n数据简化\nDataSimp合伙人或XX岗位\n（研发岗参考本\n蚊及\n文本分析一文的二级标题）。\n1）技术研发部（重点收简历）：核心的数据分析DA、NLP、DL编程\n技能，\nWindows/Linux/Android/iOS平台、OA、App软件\n开发基础；\n2）市场客服部（研发部兼职）：搜集客户资料、面见客户、形成客户需求分析文档，跟踪反馈，面谈、电邮、电话、邮寄沟通服务；\n3）行政后勤部（合伙人兼职）：高级的全系列文档搜集编辑整理技能，OA软件界面和操作体验实验，公司法律财会物业文书基础\n。\n详情落地前发文宣传。\n\n\n（西安秦陇纪\n10数据简化DataSimp综合汇编，欢迎有志于数据简化之传媒、技术的实力伙伴加入全球“数据简化DataSimp”团队！）","date":"2017年02月27日 22:54:18"}
{"_id":{"$oid":"5d36b6e26734bd8e681d61e6"},"title":"手把手教你解决90%的自然语言处理问题","author":"机器学习算法与人工智能","content":"无论你是成熟的公司，还是想要推出一个新服务，都可以利用文本数据来验证、改进和扩展产品的功能。科学的从文本数据中提取语义并学习是自然语言处理(NLP)研究的一个课题。\nNLP每天都会产生新的令人兴奋的结果，并且它是一个非常大的领域。然而，在与数百家公司合作之后，Insight团队发现一些关键的实际应用程序比其他应用程序出现得更频繁，例如:\n识别不同的用户/客户群体(如预测客户流失、终身价值、产品偏好)；\n准确地检测和提取不同类别的反馈(积极和消极的评论/意见和特定属性,如衣服尺寸/是否合身)；\n根据意图对文本进行分类(例如，基本请求，紧急问题)。\n虽然有许多线上NLP文件和教程，但我们发现很难找到有效地从底层解决这些问题的指导方针和技巧。\n这篇文章解释了如何构建机器学习解决方案来解决上面提到的问题。我们将从最简单的方法开始，然后转向更细致的解决方案，比如特性工程、单词向量和深度学习。\n读完这篇文章，你会知道如何:\n收集、准备和检查数据。\n建立简单的模型，并在必要时向深度学习过渡。\n解释和理解你的模型，以确保你是在获取信息而不是噪音。\n我们把这篇文章作为一个分步指南;它还可以作为高度有效的标准方法的高级概述。\n这篇文章附带了一个交互式笔记本，演示和应用所有这些技术。\n交互式笔记本地址：https://github.com/hundredblocks/concrete_NLP_tutorial/blob/master/NLP_notebook.ipynb\n\n\n\n\nPart\n1\n收集数据\n\n\n示例数据来源\n每一个机器学习问题都是从数据开始的，比如电子邮件、帖子或推文。文本信息的来源包括:\n产品评论(在亚马逊，Yelp和各种应用商店)；\n用户生成内容(推文, Facebook帖子，StackOverflow问题)；\n故障排除(客户请求、支持票、聊天记录)。\n“社交媒体灾难”数据集\n对于这篇文章，我们将使用CrowdFlower提供的称为“社交媒体灾难”的数据集，其中:\n参与者查看了超过10,000条推文，其中包括“着火”、“隔离”和“防疫”等各种关键字的搜索，然后指出这条推文是否提到了灾难事件(而不是带有关键字的电影评论或笑话，和一些非灾难性的事件)。\n我们的任务是检测哪些推文是关于灾难性事件的，而不是像电影这样无关紧要的话题。这个任务的特别在于，两个类都包含相同搜索词，因此我们将不得不使用更微妙的差异来区分它们。\n在这篇文章的余下部分中，我们将把有关灾难的推文称为“灾难”，并把其他的推文称为“无关”。\n标签\n我们已经标记了数据，因此我们知道哪些推文属于哪个类别。正如Richard Socher所描述的那样，与试图优化复杂的无监督方法相比，用查找和标记足够的数据来训练模型，更快、更简单、成本更低。\nRichard Socher的观点\n\n\n\n\nPart\n2\n清洗数据\n我们遵循的第一条规则是:“数据的好坏影响着你的模型。”\n数据科学家的关键技能之一就是知道下一步应该是研究模型还是数据。经验告诉我们应该先查看数据然后再洗数据集。干净的数据集将允许模型学习有意义的特性，而不是过度拟合无关的噪音。\n以下是用来清洗你的数据的清单(详见代码):\n删除所有不相关的字符，例如任何非字母数字字符（non alphanumeric character）。\n把文本分成单独的单词来令牌化文本。\n删除不相关的单词，比如“@”或url。\n将所有字符转换为小写，如“hello”, “Hello”和“HELLO” 。\n考虑将拼错的单词组合成一个单独的表示(如“cool”“kewl”“cooool”)\n考虑lemmatization(减少诸如“am”、“are”和“is”这样的常见形式，例如“be”)\n代码地址：https://github.com/hundredblocks/concrete_NLP_tutorial/blob/master/NLP_notebook.ipynb\n令牌化地址:https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html\n在遵循这些步骤并检查其他错误之后，我们可以开始使用干净的、标记的数据来训练模型。\n\n\n\n\nPart\n3\n找到一个好的数据表示\n机器学习模型以数值作为输入。例如，对图像进行处理的模型，利用矩阵表示颜色通道中每个像素的强度。\n以数字矩阵表示的笑脸\n我们的数据集是句子的列表，为了让我们的算法从数据中提取模式，我们首先需要找到一种方法以算法能够理解的方式来表示它，也就是一个数字列表。\n独热编码(词袋)\n表示计算机文本的一种方法是将每个字符单独编码为一个数字(例如ASCII)。如果我们要将这个简单的表示输入到分类器中，那么它必须只根据我们的数据来学习单词的结构，这对于大多数数据集来说是不可能的。我们需要使用更高级的方法。\n例如，我们可以在我们的数据集中建立一个包含所有单词的词汇表，并为词汇表中的每个单词创建一个唯一索引。每个句子都被表示成一个列表，这个列表的长度取决于不同单词的数量。在这个列表中的每个索引中，我们标记出给定词语在句子中出现的次数。这被称为词袋模型，因为它是一种完全无视句子中词语顺序的表现形式。以下是插图说明：\n把句子表示为词袋。左边是句子，右边是数字表示。向量中的每一个索引都代表一个特定的单词。\n可视化嵌入\n在“社交媒体灾难”数据集中，我们大约有2万个单词，这意味着每个句子都将被表示成长度为20000的向量。这每个句子只包含了我们词汇量的一小部分。\n为了查看嵌入是否捕获了与我们问题相关的信息(例如，推文是否与灾难有关)，可视化它们并查看分类是否正确，是一个好方法。由于词汇表是非常大的，并且在20,000个维度中可视化数据是不可能的，像PCA这样的技术将有助于将数据压缩到两个维度。\n可视化词袋嵌入\n这两个类看起来并没有很好地分离，这可能是嵌入的一个特性，或者仅仅是维度缩减。为了了解这些词袋的特点是否有任何用途，我们可以用它们来训练分类器。\n\n\nPart\n4\n分类\n当第一次尝试时，最好的做法一般是从最简单的工具开始着手解决问题。每当提到数据分类时，人们最喜欢用的是逻辑回归。这是非常简单的训练，结果是可以解释的，你可以很容易地从模型中提取最重要的系数。\n我们将数据分解到一个训练集中，用于拟合我们的模型和测试集，以查看它对不可见的数据的概括程度。经过训练，我们的准确率达到75.4%。不是太糟糕。\n\n\nPart\n5\n检查\n混淆矩阵\n第一步是了解我们的模型所犯错误的类型，以及哪些错误是最不可取的。在我们的例子中，误报将一个无关的推文归类为灾难，而漏报则将灾难推文分类为“无关”。如果首要任务是对预测灾难事件，我们就要降低我们的漏报率。如果我们在资源方面受到限制，我们可能会优先考虑降低误报率以减少假警报。一个很好的可视化这个信息的方法是使用混淆矩阵，它比较了我们的模型预测和真实标签。理想情况下，矩阵将是一条从左上到右下的对角线(我们的预测完全符合事实)。\n混淆矩阵(绿色是高比例，蓝色是低比例)\n我们的分类器的漏报率高于误报率(比例)。换句话说，我们的模型最常见的错误是错误地将灾难分类为“无关”。\n解释模型\n为了验证我们的模型并解释它的预测，重要的是看一下它用哪些单词来做决策。如果我们的数据有偏差，我们的分类器会在样本数据中做出准确的预测，但是模型在现实世界中不会很好地泛化。在这里，我们为“灾难”和“无关”类找出最重要的单词。用词袋和逻辑回归来绘制单词的重要度是很简单的，因为我们可以提取和排列模型用于预测的系数。\n词袋:单词的重要度\n我们的分类器正确地选择了一些模式(广岛，大屠杀)，但显然似乎是过度拟合一些无意义的术语(heyoo, x1392)。现在，我们的词袋模型是处理大量的词汇，并对所有单词一视同仁。然而，有些词出现频率非常高，而且只会对我们的预测造成干扰。接下来，我们将尝试用一种方法来表示能够解释单词频率的句子，看看是否能从数据中获得更多的信号。\n\n\nPart\n6\n词汇结构\nTF-IDF\n为了帮助我们的模型更多地关注有意义的单词，我们可以在我们的词袋模型的顶部使用TF-IDF评分(术语频率，逆文档频率)。TF-IDF通过单词在数据集中出现的频率来衡量单词，在我们的数据集里，一些词是非常罕见的，而有些词太过频繁，只会增加噪音。这是我们新嵌入的PCA投影。\n可视化TF-IDF嵌入\n我们可以看到，这两种颜色之间有更明显的区别。这将使我们的分类器更容易区分两个组。让我们看看这会不会带来更好的性能。在我们新的嵌入式系统上训练另一个逻辑回归，我们得到了76.2%的精确度。\n一个轻微的改善。我们的模型是否开始研究更重要的词汇?如果我们得到了更好的结果，同时防止模型“欺骗”我们，那么我们就可以真正地考虑升级这个模型。\nTF-IDF:文字的重要度\n它挑选的单词看起来更有意义!虽然我们在测试集上的度量只稍微增加了一点，但是我们对我们的模型使用的术语有了更多的信心，因此在将它部署到与客户交互的系统中会更好。\n\n\n\n\nPart\n7\n利用语义\nWord2Vec\n我们的最新模型设法获得高信号单词。然而，很有可能的是，如果我们部署这个模型，我们将会遇到以前在我们的训练中没有看到的单词。之前的模型将无法准确地对这些推文进行分类，即使在训练过程中看到了非常相似的单词。\n为了解决这个问题，我们需要掌握词语的语义。用来帮助我们捕捉语义的工具叫做Word2Vec。\n使用预先训练的单词\nWord2Vec是一种查找单词连续嵌入的技术。它听过阅读大量的文本来学习，并记住在类似的语境中出现的单词。在对足够的数据进行训练之后，它会在词汇表中为每个单词生成一个300维的向量，这些单词之间的意思相近。\n该论文的作者开源了一个在非常大的语料库中预先训练的模型，我们可以利用它将一些语义的知识包含进我们的模型中。预先训练的向量可以在相关的资源库中找到。\n论文地址：https://arxiv.org/abs/1301.3781\n资源库地址：https://github.com/hundredblocks/concrete_NLP_tutorial\n句子层面上的表示\n让句子快速嵌入分类器的方法，是平均在我们的句子所有单词的Word2Vec分数。这是与以前方法类似的词袋，但是这次我们只去掉了句子的语法，同时保留一些语义信息。\nWord2Vec句子嵌入\n下面是我们使用以前的技术实现的新嵌入的可视化:\n可视化Word2Vec嵌入\n这两组颜色看起来更加分离，我们的新嵌入应该帮助分类器找到两个类之间的分离。在第三次(逻辑回归)训练了相同的模型后，我们的准确率为77.7%，这是我们最好的结果。是时候检查我们的模型了。\n复杂性/可解释性权衡\n由于我们的嵌入没有像我们以前的模型那样表示为每个单词的一维向量，所以很难看出哪些单词与我们的分类最相关。虽然我们仍然可以使用逻辑回归的系数，但它们与我们的嵌入的300个维度有关，而不是单词的索引。\n对于如此低的精确度，失去所有的解释能力似乎是一种苛刻的取舍。但是，对于更复杂的模型，我们可以利用像LIME这样的黑箱解释器来了解我们的分类器是如何工作的。\nLIME\nGithub通过开源软件包提供LIME。黑箱解释器允许用户通过扰动输入（在我们的例子中是从句子中移除单词）和观察预测如何改变来解释任何分类器在一个特定示例上的决定。\nGithub资源包地址：https://github.com/marcotcr/lime\n让我们来看看我们的数据集中的几个句子的解释。\n真正的灾难词被识别为“相关”\n词语对分类的贡献似乎不那么明显\n但是，我们没有时间去探索数据集中的数以千计的例子。我们要做的是在一个有代表性的测试示例样本上运行LIME，看看哪些词对于分类贡献度最高。使用这种方法，我们可以得到单词重要度分数，并验证我们模型的预测。\nWord2Vec:文字的重要性\n看起来模型提取出了高度相关的单词，这些单词暗示它做出可以理解的决定。这些看起来像是以前所有模型中最相关的词汇，因此我们更愿意部署到生产中。\n\n\n\n\nPart\n8\n使用端到端的方法利用语法\n我们已经介绍了快速有效的方法来生成紧凑的句子嵌入。然而，通过省略单词的顺序，我们放弃了句子的所有语法信息。如果这些方法不能提供足够的结果，则可以使用更复杂的模型，将整个句子作为输入并预测标签，而不需要建立中间表示。一种常见的方法是使用Word2Vec或其他方法，如GloVe或CoVe，将句子作为一个单词向量的序列。\n高效的端到端架构\n卷积神经网络的句子分类训练非常快，并且适用于作为入门级的深度学习架构。虽然卷积神经网络(CNN)主要以其在图像数据上的性能而著称，但它们在与文本相关的任务上的性能也非常好，而且通常比大多数复杂的NLP方法(例如LSTM和编码器/解码器架构)要快得多。这个模型保存了单词的顺序，并且学习了关于哪些单词序列可以预测目标类的有价值的信息。与以前的模式相反，它可以区分“Alex eats plants”和“Plants eat Alex.”。\n训练这个模型不需要比以前的方法做更多的工作(详见代码)，并且得到的模型会比以前的好得多，准确率高达79.5%。与上面的模型一样，下一步应该使用我们描述的方法来探索和解释预测，以验证它确实是最佳模型。\n代码地址：https://github.com/hundredblocks/concrete_NLP_tutorial/blob/master/NLP_notebook.ipynb\n最后\n成功方法的快速回顾:\n从一个快速简单的模型开始。\n解释其预测。\n理解所犯的错误。\n使用这些知识来提示下一步，无论是处理数据，还是一个更复杂的模型。\n这些方法被应用到一个特定的示例案例中，使用定制的模型来理解和利用诸如推文之类的短文本，但是这些想法广泛适用于各种问题。\n\n\n\n\nML \u0026 AI\n长按，识别二维码，加关注","data":"2018年02月10日 00:00:00"}
{"_id":{"$oid":"5d36b7cc6734bd8e681d626d"},"title":"自然语言处理中常见的10个任务简介及其资源","author":"shelley__huang","content":"from:http://www.datalearner.com/blog/1051509699533080\n简介\n现在很多公司和组织每天都要处理大量的文本信息，包括邮件、评论、客户的电话等。将这些数据变成有用的信息需要花费大量的时间。抽取这些信息的一个核心的技能就是自然语言处理（Natural Language Processing，NLP）。\n自然语言处理在现阶段变得越来越重要，不管你是做什么的，这篇博客都能给你一点帮助。\n为何写这篇博客？\n作者已经在NLP领域做过一段时间工作了。作者遇到过很多种情况，需要从各种资料如最新的论文、博客以及一些自然语言处理的任务中获得帮助。因此，作者希望将这些资源都写到一起来提供一站式的帮助。下面就是自然语言处理有关的一些资源。\n一、词干抽取（Stemming）\n词干抽取是去除词语的形态等，将词语变成它的词干形式的过程。它的目的是将不同形式的相同含义的词语统一起来（数据学习网站提醒一下：中文中一般没有词干抽取的工作，但是多了个分词，这是中英文文本处理的不同）。例如，有下面两个例子：\nbeautiful and beautifully are stemmed to beauti\ngood, better and best are stemmed to good, better and best respectively\n这是来自于论文的例子，原文：https://tartarus.org/martin/PorterStemmer/def.txt\n使用Python处理词干抽取：https://bitbucket.org/mchaput/stemming/src/5c242aa592a6d4f0e9a0b2e1afdca4fd757b8e8a/stemming/porter2.py?at=default\u0026fileviewer=file-view-default\n比如，我们可以用Porter2算法来实现词干抽取：\n#!pip install stemming\nfrom stemming.porter2 import stem\nstem(\"casually\")\n二、词形还原（Lemmatisation）\n词形还原是指将一组词语变成他们词干的形式的过程。例如在会话识别任务中，我们需要考虑这个单词在句子中的含义，也要考虑这个单词在相邻句子中的含义。例如，：\nbeautiful and beautifully are lemmatised to beautiful and beautifully respectively.\ngood, better and best are lemmatised to good, good and good respectively.\n有两篇论文讨论了词形还原：\n论文1：http://www.ijrat.org/downloads/icatest2015/ICATEST-2015127.pdf\n论文2：https://academic.oup.com/dsh/article-abstract/doi/10.1093/llc/fqw034/2669790/Lemmatization-for-variation-rich-languages-using\n数据集：https://catalog.ldc.upenn.edu/ldc99t42\n在Python中可以使用Spacy来做词形还原：\n#!pip install spacy\n#python -m spacy download en\nimport spacy\nnlp=spacy.load(\"en\")\ndoc=\"good better best\"\nfor token in nlp(doc):\nprint(token,token.lemma_)\n三、词嵌套（Word Embeddings）\n词嵌套是一种技术，它可以将自然语言变成实值向量的形式。由于计算机无法直接处理文本，所以这种转换很有用。这类技术使用实值向量来表示自然语言中词语的位置和相互关系（数据学习网站提醒一下：词嵌套最有名的论文应当属于word2vec这篇论文，它并没有说提供了新方法，但是提供了一种新工具，可以很方便的从文本中获取词向量的结果。这也是谷歌提出来的，谷歌真是个不错的公司）。例如，我们可以用100维向量表示词语或者短语。\n例如，用5-维的向量表示“男人”这个词语：\n\n\n\n\n这里的每个数字都是在特定方向上的大小。\n这篇博客详细介绍了词嵌套模型。\n这篇论文详细介绍了词向量，对于深入理解词嵌套模型必看。\n这个工具是一个基于浏览器的对词向量进行可视化展示的。\n这里是一个对294种语言的词向量的训练结果。\n实现方式：我们可以使用Python中gensim工具来训练。\n下载使用谷歌新闻训练好的词向量：\n#!pip install gensim\nfrom gensim.models.keyedvectors import KeyedVectors\nword_vectors=KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin',binary=True)\nword_vectors['human']\n用自己的数据集训练词向量：\nsentence=[['first','sentence'],['second','sentence']]\nmodel = gensim.models.Word2Vec(sentence, min_count=1,size=300,workers=4)\n四、词性标注（Part-Of-Speech Tagging）\n词性标注就是将句子中的单词标注成“名词”、“动词”等（数据学习网站提醒一下：中文的词性标注工具可以使用结巴分词或者是张华平分词，都是带有词性标注的功能）。例如，句子：\n“Ashok killed the snake with a stick”\n词性标注的结果是：\nAshok 代词\nkilled 动词\nthe DET\nsnake 名词\nwith 连词\na DET\nstick 名词\n. 标点符号\n这篇论文使用了Dynamic Feature Induction来得到高精度的词性标注结果。\n这篇论文使用Anchor Hidden Markove模型来训练无监督的词性标注结果。\n实现：我们可以使用spacy来实现词性标注：\n#!pip install spacy\n#!python -m spacy download en\nnlp=spacy.load('en')\nsentence=\"Ashok killed the snake with a stick\"\nfor token in nlp(sentence):\nprint(token,token.pos_)\n五、命名实体消歧（Named Entity Disambiguation）\n命名实体消岐是值识别句子中的实体的过程。例如，句子：\n“Apple earned a revenue of 200 Billion USD in 2016”\n命名实体消歧的目标是认出Apple是一个公司名字而不是水果名。\n命名实体一般需要一个实体库，它可以将句子中的实体链接到实体库中。\n这篇论文使用了基于深度神经网络的Deep Semantic Relatedness技术来进行命名实体消歧。效果不错。它使用了知识库。\n这篇论文则利用了词向量模型，使用 Local Neural Attention 来进行命名实体消歧。\n六、命名实体识别（Named Entity Recognition）\n命名实体识别是要识别出句子中的实体，并将实体划分到某个类别中，例如人、组织、日期等。例如，句子：\n“Ram of Apple Inc. travelled to Sydney on 5th October 2017”\n返回的结果是：\nRam\nof\nApple ORG\nInc. ORG\ntravelled\nto\nSydney GPE\non\n5th DATE\nOctober DATE\n2017 DATE2017 DATE\nORG表示组织，GPE表示地点。\n目前命名实体识别最大的问题是，当数据变了，即使是最好的NER技术也会表现不好。\n这篇论文使用了二向LSTM模型，在4种语言上，联合了有监督、无监督的模型实现了比较好的命名实体识别的效果。\n我们可以使用Spacy来实现这个技术：\nimport spacy\nnlp=spacy.load('en')sentence=\"Ram of Apple Inc. travelled to Sydney on 5th October 2017\"\nfor token in nlp(sentence):\nprint(token, token.ent_type_)\n七、情感分析\n情感分析的任务涉及的主题较多，一般是利用自然语言处理技术识别如客户评论中正向或者负向的情感等，或者是通过语音分析、写作分析得到情绪判别结果。例如：\n“I did not like the chocolate ice-cream” – 对冰激淋做负向的评价\n“I did not hate the chocolate ice-cream” – 可能是一个中立的评价\n情感分析的方法很多，开始的时候可以用LSTM模型与词向量模型一起，数一数句子中正负向情感词的个数得到。资源有：\n博客 1: 电影推文的情感分析\n博客 2: Chennai 洪水的情感分析\n论文 1: 使用朴素贝叶斯方法对IMDB评论的情感分类.\n论文 2: 使用LDA获得用户文本的主题，然后基于无监督的方法识别情感.\nRepository: 这里总结了大量的关于不同语言的情感处理的论文和实现方法.\nDataset 1: Multi-Domain sentiment dataset version 2.0\nDataset 2: Twitter Sentiment analysis Dataset\nCompetition: A very good competition where you can check the performance of your models on the sentiment analysis task of movie reviews of rotten tomatoes.\n八、文本语义相似性（Semantic Text Similarity）\n计算文本语义相似性就是计算两段文本之间含义相似性的任务。\n论文 1：详细说明了多种计算文本相似性的方法，必读\n论文 2：介绍如何使用CNN获得短文本之间相似性\n论文 3：使用基于树的LSTM方法获取语义分类\n九、语言识别\n就是识别出文本是什么语言写的，是文本分类的一种特殊情况。\n博客：给了一个新工具，可以识别170多种语言\n论文1：讨论了7种方法，识别285种语言\n论文2：讨论了如何使用深度神经网络获得语言识别结果\n十、文本摘要（Text Summarisation）\n文本摘要是通过识别文本重要的内容将一段文本缩减，并变成对这些点的总结。文本摘要的目标是最大限度保留原始文本的含义。\n论文1：使用 Neural Attention Model 获取文本摘要\n论文2：使用sequence-to-sequence的RNN获取摘要\nRepository：谷歌大脑团队提供的可以自动文本摘要的代码，基于Gigaword数据集训练\n应用：Reddit’s autotldr bot 使用文本摘要将帖子的评论总结成短文，这个特性在Reddit用户中很有名。\n我们可以使用gensim来获取文本摘要：\nfrom gensim.summarization import summarize\nsentence=\"Automatic summarization is the process of shortening a text document with software, in order to create a summary with the major points of the original document. Technologies that can make a coherent summary take into account variables such as length, writing style and syntax.Automatic data summarization is part of machine learning and data mining. The main idea of summarization is to find a subset of data which contains the information of the entire set. Such techniques are widely used in industry today. Search engines are an example; others include summarization of documents, image collections and videos. Document summarization tries to create a representative summary or abstract of the entire document, by finding the most informative sentences, while in image summarization the system finds the most representative and important (i.e. salient) images. For surveillance videos, one might want to extract the important events from the uneventful context.There are two general approaches to automatic summarization: extraction and abstraction. Extractive methods work by selecting a subset of existing words, phrases, or sentences in the original text to form the summary. In contrast, abstractive methods build an internal semantic representation and then use natural language generation techniques to create a summary that is closer to what a human might express. Such a summary might include verbal innovations. Research to date has focused primarily on extractive methods, which are appropriate for image collection summarization and video summarization.\"\nsummarize(sentence)","data":"2018年11月14日 16:09:27"}
{"_id":{"$oid":"5d36b7f06734bd8e681d627c"},"title":"五个非常实用的自然语言处理资源","author":"weixin_34392906","content":"如果你对自然语言处理方面的资源感兴趣，请仔细阅读本篇文章。\n\n\n运行数据科学POC的7个步骤\n网上有很多依靠深度学习方法的NLP资源，有一些资源理论深厚，十分经典，特别是斯坦福大学和牛津大学的NLP，其深度学习课程为：\nl自然语言处理与深度学习（斯坦福大学）\nl自然语言处理的深度学习（牛津大学）\n但是如果你已经完成了这些，或已经在NLP中获得了基础并想要转向一些实用资源，或者只是对其他方法感兴趣，希望这篇文章能对你有所帮助。\n\n\n\n1. 用Python进行自然语言处理—用自然语言工具包分析文本\n这是一本至少从实用性和Python生态系统的双重视角介绍自然语言处理的书，它可以用于个人学习或作为自然语言处理或计算语言学课程的教科书，或作为人工智能、文本挖掘或语料库语言学课程的补充。本书通过使用自然语言工具包（NLTK)来接近NLP。\nNLTK包含丰富的软件、数据和文档，可从http://nltk.org/免费下载。发行版本是由Windows，Macintosh和Unix平台提供的。我们强烈建议您下载Python和NLTP，并尝试一下示例和练习。\n\n\n2.  深度学习自然语言处理：Jupyter笔记本课程\n这是一个Jupyter笔记本并附随Jon Krohn的关于NLP深度学习的一系列精彩视频的回购协议。如果你有兴趣观看他的视频（它是通过O’Reilly的Safari平台提供的）请注册一个免费的10天试用版，。\nJon在这些笔记本和随附视频中的主要内容包括：\n1.预处理用于机器学习应用的自然语言数据；\n2.将自然语言转换为数字表示（使用word2vec)；\n3.通过训练自然语言的深层学习模型进行预测；\n4.在高级TensorFlow API Keras中应用先进的NLP方法；\n5.通过调整超参数来提高深度学习模型的性能。\n\n\n3.如何解决90%的NLP问题：一步一步的指导\n这是以笔记本的形式出现的另一套非常棒的教程，它遵循类似于上述Krohn的轨迹。Insight AI的Emmanuel Ameisen分解了完成哪些任务需要执行哪些步骤，阅读完本文后，您将知道如何：\n1.收集、准备和检查数据。\n2.从建立简单的模型开始，并在必要时过渡到深度学习。\n3.解释并理解你的模型，确保您实际上获取的是信息而不是噪音。\n\n\n4.Keras LSTM教程-如何轻松构建强大的深度学习语言模型\n本教程比之前的资源重点更多，因为它涵盖了在Keras中实施用于语言建模的LSTM。它通过附有解释、代码和视觉效果对此进行了详细的介绍说明。在本教程中，我将专注于在Keras中创建LSTM网络，简要回顾或概述LSTM的工作原理。在Keras LSTM教程中，我们将利用称为PTB语料库的大型文本数据集来实现序列到序列的文本预测模型。\n\n\n5.使用组合LSTM-CNN模型的Twitter情感分析\n这是一篇较短的教程（它是一个文章的概述），其中有使用LSTM/CNN的组合方法的代码来分析情感。该项目颠覆了体系结构，并报告了不同的结果。\n我们的CNN-LSTM模型的准确度比CNN模型高3%，但比LSTM模型差3.2%。同时，我们的LSTM-CNN模型比CNN模型的性能好8.5%，比LSTM模型好2.7%。\n关于该项目的结果的可靠性，我暂时无法保证。但是，其创新的情感分析方式与混合在不同的神经网络体系结构中搭配使用，使我将其纳入该列表中，希望能对读者有所启发。\n相关：\nl免费资源，用于深入学习自然语言处理入门。\nl自然语言处理键术语，解释。\nl处理文本数据科学任务的框架。\n本文由阿里云云栖社区组织翻译。\n文章原标题《5 Fantastic Practical Natural Language Processing Resources》\n作者：Matthew Mayo\n译者：乌拉乌拉，审校：袁虎。\n文章为简译，更为详细的内容，请查看原文文章","data":"2018年03月01日 23:25:07"}
{"_id":{"$oid":"5d36b8266734bd8e681d6294"},"title":"自然语言处理（NLP）领域学术会议","author":"木尧大兄弟","content":"转载于：https://blog.csdn.net/nuoline/article/details/8610774和https://blog.csdn.net/sinat_29694963/article/details/80591123\n1. 国际学术组织、学术会议与学术论文\n自然语言处理（natural language processing，NLP）在很大程度上与计算语言学（computational linguistics，CL）重合。与其他计算机学科类似，NLP/CL有一个属于自己的最权威的国际专业学会，叫做The Association for Computational Linguistics（ACL，URL：http://aclweb.org/），这个协会主办了NLP/CL领域最权威的国际会议，即ACL年会，ACL学会还会在北美和欧洲召开分年会，分别称为NAACL和EACL。除此之外，ACL学会下设多个特殊兴趣小组（special interest groups，SIGs），聚集了NLP/CL不同子领域的学者，性质类似一个大学校园的兴趣社团。其中比较有名的诸如SIGDAT（Linguistic data and corpus-based approaches to NLP）、SIGNLL（Natural Language Learning）等。这些SIGs也会召开一些国际学术会议，其中比较有名的就是SIGDAT组织的EMNLP（Conference on Empirical Methods on Natural Language Processing）和SIGNLL组织的CoNLL（Conference on Natural Language Learning）。此外还有一个International Committee on Computational Linguistics的老牌NLP/CL学术组织，它每两年组织一个称为International Conference on Computational Linguistics (COLING)的国际会议，也是NLP/CL的重要学术会议。NLP/CL的主要学术论文就分布在这些会议上。\n作为NLP/CL领域的学者最大的幸福在于，ACL学会网站建立了称作ACL Anthology的页面（URL：http://aclweb.org/anthology-new/），支持该领域绝大部分国际学术会议论文的免费下载，甚至包含了其他组织主办的学术会议，例如COLING、IJCNLP等，并支持基于Google的全文检索功能，可谓一站在手，NLP论文我有。由于这个论文集合非常庞大，并且可以开放获取，很多学者也基于它开展研究，提供了更丰富的检索支持，具体入口可以参考ACL Anthology页面上方搜索框右侧的不同检索按钮。\n与大部分计算机学科类似，由于技术发展迅速，NLP/CL领域更重视发表学术会议论文，原因是发表周期短，并可以通过会议进行交流。当然NLP/CL也有自己的旗舰学术期刊，发表过很多经典学术论文，那就是Computational Linguistics（URL：http://www.mitpressjournals.org/loi/coli）。该期刊每期只有几篇文章，平均质量高于会议论文，时间允许的话值得及时追踪。此外，ACL学会为了提高学术影响力，也刚刚创办了Transactions of ACL（TACL，URL：http://www.transacl.org/），值得关注。值得一提的是这两份期刊也都是开放获取的。此外也有一些与NLP/CL有关的期刊，如ACM Transactions on Speech and Language Processing，ACM Transactions on Asian Language Information Processing，Journal of Quantitative Linguistics等等。\n根据Google Scholar Metrics 2013年对NLP/CL学术期刊和会议的评价，ACL、EMNLP、NAACL、COLING、LREC、Computational Linguistics位于前5位，基本反映了本领域学者的关注程度。\nNLP/CL作为交叉学科，其相关领域也值得关注。主要包括以下几个方面：（1）信息检索和数据挖掘领域。相关学术会议主要由美国计算机学会（ACM）主办，包括SIGIR、WWW、WSDM等；（2）人工智能领域。相关学术会议主要包括AAAI和IJCAI等，相关学术期刊主要包括Artificial Intelligence和Journal of AI Research；（3）机器学习领域，相关学术会议主要包括ICML，NIPS，AISTATS，UAI等，相关学术期刊主要包括Journal of Machine Learning Research（JMLR）和Machine Learning（ML）等。例如最近兴起的knowledge graph研究论文，就有相当一部分发表在人工智能和信息检索领域的会议和期刊上。实际上国内计算机学会（CCF）制定了“中国计算机学会推荐国际学术会议和期刊目录”（http://www.ccf.org.cn/sites/ccf/aboutpm.jsp?contentId=2567814757463），通过这个列表，可以迅速了解每个领域的主要期刊与学术会议。\n最后，值得一提的是，美国Hal Daumé III维护了一个natural language processing的博客（http://nlpers.blogspot.com/），经常评论最新学术动态，值得关注。我经常看他关于ACL、NAACL等学术会议的参会感想和对论文的点评，很有启发。另外，ACL学会维护了一个Wiki页面（http://aclweb.org/aclwiki/），包含了大量NLP/CL的相关信息，如著名研究机构、历届会议录用率，等等，都是居家必备之良品，值得深挖。\n**\n2. 国内学术组织、学术会议与学术论文\n**\n与国际上相似，国内也有一个与NLP/CL相关的学会，叫做中国中文信息学会（URL：http://www.cipsc.org.cn/）。通过学会的理事名单（http://www.cipsc.org.cn/lingdao.php）基本可以了解国内从事NLP/CL的主要单位和学者。学会每年组织很多学术会议，例如全国计算语言学学术会议（CCL）、全国青年计算语言学研讨会（YCCL）、全国信息检索学术会议（CCIR）、全国机器翻译研讨会（CWMT），等等，是国内NLP/CL学者进行学术交流的重要平台。尤其值得一提的是，全国青年计算语言学研讨会是专门面向国内NLP/CL研究生的学术会议，从组织到审稿都由该领域研究生担任，非常有特色，也是NLP/CL同学们学术交流、快速成长的好去处。值得一提的是，2010年在北京召开的COLING以及2015年即将在北京召开的ACL，学会都是主要承办者，这也一定程度上反映了学会在国内NLP/CL领域的重要地位。此外，计算机学会中文信息技术专委会组织的自然语言处理与中文计算会议（NLP\u0026CC）也是最近崛起的重要学术会议。中文信息学会主编了一份历史悠久的《中文信息学报》，是国内该领域的重要学术期刊，发表过很多篇重量级论文。此外，国内著名的《计算机学报》、《软件学报》等期刊上也经常有NLP/CL论文发表，值得关注。\n过去几年，在水木社区BBS上开设的AI、NLP版面曾经是国内NLP/CL领域在线交流讨论的重要平台。这几年随着社会媒体的发展，越来越多学者转战新浪微博，有浓厚的交流氛围。如何找到这些学者呢，一个简单的方法就是在新浪微博搜索的“找人”功能中检索“自然语言处理”、 “计算语言学”、“信息检索”、“机器学习”等字样，马上就能跟过去只在论文中看到名字的老师同学们近距离交流了。还有一种办法，清华大学梁斌开发的“微博寻人”系统（http://xunren.thuir.org/）可以检索每个领域的有影响力人士，因此也可以用来寻找NLP/CL领域的重要学者。值得一提的是，很多在国外任教的老师和求学的同学也活跃在新浪微博上，例如王威廉（http://weibo.com/u/1657470871）、李沐（http://weibo.com/mli65）等，经常爆料业内新闻，值得关注。还有，国内NLP/CL的著名博客是52nlp（http://www.52nlp.cn/），影响力比较大。总之，学术研究既需要苦练内功，也需要与人交流。所谓言者无意、听者有心，也许其他人的一句话就能点醒你苦思良久的问题。无疑，博客微博等提供了很好的交流平台，当然也注意不要沉迷哦。\n**\n3. 如何快速了解某个领域研究进展\n**\n最后简单说一下快速了解某领域研究进展的经验。你会发现，搜索引擎是查阅文献的重要工具，尤其是谷歌提供的Google Scholar，由于其庞大的索引量，将是我们披荆斩棘的利器。\n当需要了解某个领域，如果能找到一篇该领域的最新研究综述，就省劲多了。最方便的方法还是在Google Scholar中搜索“领域名称 + survey / review / tutorial / 综述”来查找。也有一些出版社专门出版各领域的综述文章，例如NOW Publisher出版的Foundations and Trends系列，Morgan \u0026 Claypool Publisher出版的Synthesis Lectures on Human Language Technologies系列等。它们发表了很多热门方向的综述，如文档摘要、情感分析和意见挖掘、学习排序、语言模型等。\n如果方向太新还没有相关综述，一般还可以查找该方向发表的最新论文，阅读它们的“相关工作”章节，顺着列出的参考文献，就基本能够了解相关研究脉络了。当然，还有很多其他办法，例如去videolectures.net上看著名学者在各大学术会议或暑期学校上做的tutorial报告，去直接咨询这个领域的研究者，等等。\n附录：\nACL会议（Annual Meeting of the Association for Computational Linguistics）是自然语言处理与计算语言学领域最高级别的学术会议，由计算语言学协会主办，每年一届。主要涉及以下方面：\n对话(Dialogue)\n篇章(Discourse)\n评测( Eval)\n信息抽取( IE)\n信息检索( IR)\n语言生成(LanguageGen)\n语言资源(LanguageRes)\n机器翻译(MT)\n多模态(Multimodal)\n音韵学/ 形态学( Phon/ Morph)\n自动问答(QA)\n语义(Semantics)\n情感(Sentiment)\n语音(Speech)\n统计机器学习(Stat ML)\n文摘(Summarisation)\n句法(Syntax)\n自然语言处理及计算语言学常见缩略语\nACL = Association for Computational Linguistics(计算语言学协会)\nAFNLP = Asian Federation of Natural Language Processing(亚洲自然语言处理联盟)\nAI = Artificial Intelligence(人工智能)\nALPAC = Automated Language Processing Advisory Committee(语言自动处理咨询委员会)\nASR = Automatic Speech Recognition(自动语音识别)\nCAT = Computer Assisted/Aided Translation（计算机辅助翻译）\nCBC = Clustering by Committee\nCCG = Combinatory Categorial Grammar（组合范畴语法）\nCICLing = International Conference on Intelligent text processing and Computational Linguistics（国际智能文本处理与计算语言学大会）\nCL = Computational Linguistics（计算语言学）\nCOBUILD = Collins Birmingham University International Language Database（柯林斯伯明翰大学国际语言数据库）\nCOLING = International Conference on Computational Linguistics（国际计算语言学大会）\nCL = Computational Linguistics（计算语言学）\nCOBUILD = Collins Birmingham University International Language Database（柯林斯伯明翰大学国际语言数据库）\nCOLING = International Conference on Computational Linguistics（国际计算语言学大会）\nCRF = Conditional Random Fields（条件随机场）\nDRS = Discourse Representation Structure（篇章表述结构）\nDRT = Discourse Representation Theory（篇章表述理论）\nEACL = European chapter of the Association for Computational Linguistics\nEBMT = Example-based machine translation（基于实例的机器翻译）\nEM = Expectation Maximization（期望最大化）\nFAHQMT = Fully Automated High-Quality Machine Translation（全自动高质量机器翻译）\nFOL = First Order Logic（一阶逻辑）\nHAMT = Human Assisted/Aided Machine Translation（人工辅助机器翻译）\nHLT = Human Language Technologies（人类语言技术）\nHMM = Hidden Markov Model（隐马尔科夫模型）\nHPSG = Head-Driven Phrase Structure Grammar（中心语驱动短语结构语法）\nIE = Information Extraction（信息抽取）\nIR = Information Retrieval（信息检索）\nIST = Information Society Technologies（信息社会技术）\nKR = Knowledge Representation（知识表示）\nLFG = Lexical Functional Grammar（词汇功能语法）\nLSA = Latent Semantic Analysis（潜在语义分析）; Linguistics Society of America（美国语言学学会）\nLSI = Latent Semantic Indexing（潜在语义索引）\nMAHT = Machine Assised/Aided Human Translation（计算机辅助人工翻译）\nME = Maximum Entropy（最大熵）\nMI = Mutual Information（互信息）\nML = Machine Learning（机器学习）\nMRD = Machine-Readable Dictionary（机读词典）\nMT = Mechanical Translation/Machine Translation （机器翻译）\nNAACL = North American chapter of the Association for Computational Linguistics\nNE = Named Entity（命名实体）\nNEALT = Northern European Association for Language Technology\nNER = Named Entity Recognition（命名实体识别）\nNLG = Natural Language Generation（自然语言生成）\nNLP = Natural Language Processing（自然语言处理）\nNLU = Natural Language Understanding（自然语言理解）\nNML = National Museum of Language\nPLSA = Probabilistic Latent Semantic Analysis（概率潜在语义分析）\nPMI = Pointwise Mutual Information（点间互信息）\nPOS = Part of Speech（词性）\nRTE = Recognising Textual Entailment\nSLT = Spoken Language Translation（口语翻译）\nSVM = Support Vector Machine（支持向量机）\nTAG = Tree-Adjoining Grammar（树邻接语法）\nTINLAP = Theoretical Issues in Natural Language Processing\nTLA = Three-letter acronym（三字母缩略语）\nTMI = Theoretical and Methodological Issues (in Machine Translation)\nTREC = The Text REtrieval Conference（文本检索会议）\nVSM = Vector Space Model（向量空间模型）\nWSD = Word Sense Disambiguation（词义消歧）","data":"2019年03月30日 20:27:36"}
{"_id":{"$oid":"5d36b8326734bd8e681d629a"},"title":"自然语言处理NLP知识结构","author":"喜欢打酱油的老鸟","content":"自然语言处理NLP知识结构\n文|秦陇纪，数据简化DataSimp\n自然语言处理(计算机语言学、自然语言理解)涉及：字处理，词处理，语句处理，篇章处理词处理分词、词性标注、实体识别、词义消歧语句处理句法分析(SyntacticAnalysis)、语义分析(SenmanticAnalysis)等。其中，重点有：\n1.句法语义分析：分词，词性标记，命名实体识别。\n2.信息抽取\n3.文本挖掘：文本聚类，情感分析。基于统计。\n4.机器翻译：基于规则，基于统计，基于神经网络。\n5.信息检索\n6.问答系统\n7.对话系统建议…本文总结的自然语言处理历史、模型、知识体系结构内容，涉及NLP的语言理论、算法和工程实践各方面，内容繁杂。参考黄志洪老师自然语言处理课程、宗成庆老师《统计自然语言处理》，郑捷2017年电子工业出版社出版的图书《NLP汉语自然语言处理原理与实践》，以及国外著名NLP书籍的英文资料、汉译版资料。\n一、NLP知识结构概述\n1)自然语言处理：利用计算机为工具，对书面实行或者口头形式进行各种各样的处理和加工的技术，是研究人与人交际中以及人与计算机交际中的演员问题的一门学科，是人工智能的主要内容。\n2)自然语言处理是研究语言能力和语言应用的模型，建立计算机(算法)框架来实现这样的语言模型，并完善、评测、最终用于设计各种实用系统。\n3)研究问题(主要)：\n信息检索\n机器翻译\n文档分类\n问答系统\n信息过滤\n自动文摘\n信息抽取\n文本挖掘\n舆情分析\n机器写作\n语音识别\n研究模式：自然语言场景问题，数学算法，算法如何应用到解决这些问题，预料训练，相关实际应用\n自然语言的困难：\n场景的困难：语言的多样性、多变性、歧义性\n学习的困难：艰难的数学模型(hmm,crf,EM,深度学习等)\n语料的困难：什么的语料？语料的作用？如何获取语料？\n二、NLP知识十大结构\n1形式语言与自动机\n语言：按照一定规律构成的句子或者字符串的有限或者无限的集合。\n描述语言的三种途径：\n穷举法\n文法(产生式系统)描述\n自动机\n自然语言不是人为设计而是自然进化的，形式语言比如：运算符号、化学分子式、编程语言\n形式语言理论朱啊哟研究的是内部结构模式这类语言的纯粹的语法领域，从语言学而来，作为一种理解自然语言的句法规律，在计算机科学中，形式语言通常作为定义编程和语法结构的基础\n形式语言与自动机基础知识：\n集合论\n图论\n自动机的应用：\n1，单词自动查错纠正\n2，词性消歧(什么是词性？什么的词性标注？为什么需要标注？如何标注？)\n形式语言的缺陷：\n1、对于像汉语，英语这样的大型自然语言系统，难以构造精确的文法\n2、不符合人类学习语言的习惯\n3、有些句子语法正确，但在语义上却不可能，形式语言无法排出这些句子\n4、解决方向：基于大量语料，采用统计学手段建立模型\n2 语言模型\n1)语言模型(重要)：通过语料计算某个句子出现的概率(概率表示)，常用的有2-元模型，3-元模型\n2)语言模型应用：\n语音识别歧义消除例如，给定拼音串：ta shi yan yan jiu saun fa de\n可能的汉字串：踏实烟酒算法的他是研究酸法的他是研究算法的，显然，最后一句才符合。\n3)语言模型的启示：\n1、开启自然语言处理的统计方法\n2、统计方法的一般步骤：\n收集大量语料\n对语料进行统计分析，得出知识\n针对场景建立算法模型\n解释和应用结果\n4)语言模型性能评价，包括评价目标，评价的难点，常用指标(交叉熵，困惑度)\n5)数据平滑：\n数据平滑的概念，为什么需要平滑\n平滑的方法，加一法，加法平滑法，古德-图灵法，J-M法，Katz平滑法等\n6)语言模型的缺陷：\n语料来自不同的领域，而语言模型对文本类型、主题等十分敏感\nn与相邻的n-1个词相关，假设不是很成立。\n3概率图模型\n生成模型与判别模型，贝叶斯网络，马尔科夫链与隐马尔科夫模型(HMM)\n1)概率图模型概述(什么的概率图模型，参考清华大学教材《概率图模型》)\n2)马尔科夫过程(定义，理解)\n3)隐马尔科夫过程(定义，理解)\nHMM的三个基本问题(定义，解法，应用)\n注：第一个问题，涉及最大似然估计法，第二个问题涉及EM算法，第三个问题涉及维特比算法，内容很多，要重点理解，(参考书李航《统计学习方法》，网上博客，笔者github)\n4 马尔科夫网，最大熵模型，条件随机场(CRF)\n1)HMM的三个基本问题的参数估计与计算\n2)什么是熵\n3)EM算法(应用十分广泛，好好理解)\n4)HMM的应用\n5)层次化马尔科夫模型与马尔科夫网络\n提出原因，HMM存在两个问题\n6)最大熵马尔科夫模型\n优点：与HMM相比，允许使用特征刻画观察序列，训练高效\n缺点：存在标记偏置问题\n7)条件随机场及其应用(概念，模型过程，与HMM关系)\n参数估计方法(GIS算法，改进IIS算法)\nCRF基本问题：特征选取(特征模板)、概率计算、参数训练、解码(维特比)\n应用场景：\n词性标注类问题(现在一般用RNN+CRF)\n中文分词(发展过程，经典算法，了解开源工具jieba分词)\n中文人名，地名识别\n8)CRF++\n5 命名实体识别，词性标注，内容挖掘、语义分析与篇章分析(大量用到前面的算法)\n1)命名实体识别问题\n相关概率，定义\n相关任务类型\n方法(基于规程-\u003e基于大规模语料库)\n2)未登录词的解决方法(搜索引擎，基于语料)\n3)CRF解决命名实体识别(NER)流程总结：\n训练阶段：确定特征模板，不同场景(人名，地名等)所使用的特征模板不同，对现有语料进行分词，在分词结果基础上进行词性标注(可能手工)，NER对应的标注问题是基于词的，然后训练CRF模型，得到对应权值参数值\n识别过程：将待识别文档分词，然后送入CRF模型进行识别计算(维特比算法)，得到标注序列，然后根据标注划分出命名实体\n4)词性标注(理解含义，意义)及其一致性检查方法(位置属性向量，词性标注序列向量，聚类或者分类算法)\n6句法分析\n1)句法分析理解以及意义\n1、句法结构分析\n完全句法分析\n浅层分析\n2、依存关系分析\n2)句法分析方法\n1、基于规则的句法结构分析\n2、基于统计的语法结构分析\n7 文本分类，情感分析\n1)文本分类，文本排重\n文本分类：在预定义的分类体系下，根据文本的特征，将给定的文本与一个或者多个类别相关联\n典型应用：垃圾邮件判定，网页自动分类\n2)文本表示，特征选取与权重计算，词向量\n文本特征选择常用方法：\n1、基于本文频率的特征提取法\n2、信息增量法\n3、X2(卡方)统计量\n4、互信息法\n3)分类器设计\nSVM，贝叶斯，决策树等\n4)分类器性能评测\n1、召回率\n2、正确率\n3、F1值\n5)主题模型(LDA)与PLSA\nLDA模型十分强大，基于贝叶斯改进了PLSA，可以提取出本章的主题词和关键词，建模过程复杂，难以理解。\n6)情感分析\n借助计算机帮助用户快速获取，整理和分析相关评论信息，对带有感情色彩的主观文本进行分析，处理和归纳例如，评论自动分析，水军识别。\n某种意义上看，情感分析也是一种特殊的分类问题\n7)应用案例\n8信息检索，搜索引擎及其原理\n1)信息检索起源于图书馆资料查询检索，引入计算机技术后，从单纯的文本查询扩展到包含图片，音视频等多媒体信息检索，检索对象由数据库扩展到互联网。\n1、点对点检索\n2、精确匹配模型与相关匹配模型\n3、检索系统关键技术：标引，相关度计算\n2)常见模型：布尔模型，向量空间模型，概率模型\n3)常用技术：倒排索引，隐语义分析(LDA等)\n4)评测指标\n9 自动文摘与信息抽取，机器翻译，问答系统\n1)统计机器翻译的的思路，过程，难点，以及解决\n2)问答系统\n基本组成：问题分析，信息检索，答案抽取\n类型：基于问题-答案，基于自由文本\n典型的解决思路\n3)自动文摘的意义，常用方法\n4)信息抽取模型(LDA等)\n10深度学习在自然语言中的应用\n1)单词表示，比如词向量的训练(wordvoc)\n2)自动写文本\n写新闻等\n3)机器翻译\n4)基于CNN、RNN的文本分类\n5)深度学习与CRF结合用于词性标注\n三，中文NLP知识目录\n选自郑捷2017年电子工业出版社出版的图书《NLP汉语自然语言处理原理与实践》。\n第1章 中文语言的机器处理 1\n1.1 历史回顾 2\n1.1.1 从科幻到现实 2\n1.1.2 早期的探索 3\n1.1.3 规则派还是统计派 3\n1.1.4 从机器学习到认知计算 5\n1.2 现代自然语言系统简介 6\n1.2.1 NLP流程与开源框架 6\n1.2.2 哈工大NLP平台及其演示环境 9\n1.2.3 StanfordNLP团队及其演示环境 11\n1.2.4 NLTK开发环境 13\n1.3 整合中文分词模块 16\n1.3.1 安装Ltp Python组件 17\n1.3.2 使用Ltp 3.3进行中文分词 18\n1.3.3 使用结巴分词模块 20\n1.4 整合词性标注模块 22\n1.4.1 Ltp 3.3词性标注 23\n1.4.2 安装StanfordNLP并编写Python接口类 24\n1.4.3 执行Stanford词性标注 28\n1.5 整合命名实体识别模块 29\n1.5.1 Ltp 3.3命名实体识别 29\n1.5.2 Stanford命名实体识别 30\n1.6 整合句法解析模块 32\n1.6.1 Ltp 3.3句法依存树 33\n1.6.2 StanfordParser类 35\n1.6.3 Stanford短语结构树 36\n1.6.4 Stanford依存句法树 37\n1.7 整合语义角色标注模块 38\n1.8 结语 40\n第2章 汉语语言学研究回顾 42\n2.1 文字符号的起源 42\n2.1.1 从记事谈起 43\n2.1.2 古文字的形成 47\n2.2 六书及其他 48\n2.2.1 象形 48\n2.2.2 指事 50\n2.2.3 会意 51\n2.2.4 形声 53\n2.2.5 转注 54\n2.2.6 假借 55\n2.3 字形的流变 56\n2.3.1 笔与墨的形成与变革 56\n2.3.2 隶变的方式 58\n2.3.3 汉字的符号化与结构 61\n2.4 汉语的发展 67\n2.4.1 完整语义的基本形式——句子 68\n2.4.2 语言的初始形态与文言文 71\n2.4.3 白话文与复音词 73\n2.4.4 白话文与句法研究 78\n2.5 三个平面中的语义研究 80\n2.5.1 词汇与本体论 81\n2.5.2 格语法及其框架 84\n2.6 结语 86\n第3章 词汇与分词技术 88\n3.1 中文分词 89\n3.1.1 什么是词与分词规范 90\n3.1.2 两种分词标准 93\n3.1.3 歧义、机械分词、语言模型 94\n3.1.4 词汇的构成与未登录词 97\n3.2 系统总体流程与词典结构 98\n3.2.1 概述 98\n3.2.2 中文分词流程 99\n3.2.3 分词词典结构 103\n3.2.4 命名实体的词典结构 105\n3.2.5 词典的存储结构 108\n3.3 算法部分源码解析 111\n3.3.1 系统配置 112\n3.3.2 Main方法与例句 113\n3.3.3 句子切分 113\n3.3.4 分词流程 117\n3.3.5 一元词网 118\n3.3.6 二元词图 125\n3.3.7 NShort算法原理 130\n3.3.8 后处理规则集 136\n3.3.9 命名实体识别 137\n3.3.10 细分阶段与最短路径 140\n3.4 结语 142\n第4章 NLP中的概率图模型 143\n4.1 概率论回顾 143\n4.1.1 多元概率论的几个基本概念 144\n4.1.2 贝叶斯与朴素贝叶斯算法 146\n4.1.3 文本分类 148\n4.1.4 文本分类的实现 151\n4.2 信息熵 154\n4.2.1 信息量与信息熵 154\n4.2.2 互信息、联合熵、条件熵 156\n4.2.3 交叉熵和KL散度 158\n4.2.4 信息熵的NLP的意义 159\n4.3 NLP与概率图模型 160\n4.3.1 概率图模型的几个基本问题 161\n4.3.2 产生式模型和判别式模型 162\n4.3.3 统计语言模型与NLP算法设计 164\n4.3.4 极大似然估计 167\n4.4 隐马尔科夫模型简介 169\n4.4.1 马尔科夫链 169\n4.4.2 隐马尔科夫模型 170\n4.4.3 HMMs的一个实例 171\n4.4.4 Viterbi算法的实现 176\n4.5 最大熵模型 179\n4.5.1 从词性标注谈起 179\n4.5.2 特征和约束 181\n4.5.3 最大熵原理 183\n4.5.4 公式推导 185\n4.5.5 对偶问题的极大似然估计 186\n4.5.6 GIS实现 188\n4.6 条件随机场模型 193\n4.6.1 随机场 193\n4.6.2 无向图的团(Clique)与因子分解 194\n4.6.3 线性链条件随机场 195\n4.6.4 CRF的概率计算 198\n4.6.5 CRF的参数学习 199\n4.6.6 CRF预测标签 200\n4.7 结语 201\n第5章 词性、语块与命名实体识别 202\n5.1 汉语词性标注 203\n5.1.1 汉语的词性 203\n5.1.2 宾州树库的词性标注规范 205\n5.1.3stanfordNLP标注词性 210\n5.1.4 训练模型文件 213\n5.2 语义组块标注 219\n5.2.1 语义组块的种类 220\n5.2.2 细说NP 221\n5.2.3 细说VP 223\n5.2.4 其他语义块 227\n5.2.5 语义块的抽取 229\n5.2.6 CRF的使用 232\n5.3 命名实体识别 240\n5.3.1 命名实体 241\n5.3.2 分词架构与专名词典 243\n5.3.3 算法的策略——词典与统计相结合 245\n5.3.4 算法的策略——层叠式架构 252\n5.4 结语 259\n第6章 句法理论与自动分析 260\n6.1 转换生成语法 261\n6.1.1 乔姆斯基的语言观 261\n6.1.2 短语结构文法 263\n6.1.3 汉语句类 269\n6.1.4 谓词论元与空范畴 274\n6.1.5 轻动词分析理论 279\n6.1.6 NLTK操作句法树 280\n6.2 依存句法理论 283\n6.2.1 配价理论 283\n6.2.2 配价词典 285\n6.2.3 依存理论概述 287\n6.2.4 Ltp依存分析介绍 290\n6.2.5 Stanford依存转换、解析 293\n6.3 PCFG短语结构句法分析 298\n6.3.1 PCFG短语结构 298\n6.3.2 内向算法和外向算法 301\n6.3.3 Viterbi算法 303\n6.3.4 参数估计 304\n6.3.5 Stanford的PCFG算法训练 305\n6.4 结语 310\n第7章 建设语言资源库 311\n7.1 语料库概述 311\n7.1.1 语料库的简史 312\n7.1.2 语言资源库的分类 314\n7.1.3 语料库的设计实例：国家语委语料库 315\n7.1.4 语料库的层次加工 321\n7.2 语法语料库 323\n7.2.1 中文分词语料库 323\n7.2.2 中文分词的测评 326\n7.2.3 宾州大学CTB简介 327\n7.3 语义知识库 333\n7.3.1 知识库与HowNet简介 333\n7.3.2 发掘义原 334\n7.3.3 语义角色 336\n7.3.4 分类原则与事件分类 344\n7.3.5 实体分类 347\n7.3.6 属性与分类 352\n7.3.7 相似度计算与实例 353\n7.4 语义网与百科知识库 360\n7.4.1 语义网理论介绍 360\n7.4.2 维基百科知识库 364\n7.4.3 DBpedia抽取原理 365\n7.5 结语 368\n第8章 语义与认知 370\n8.1 回顾现代语义学 371\n8.1.1 语义三角论 371\n8.1.2 语义场论 373\n8.1.3 基于逻辑的语义学 376\n8.2 认知语言学概述 377\n8.2.1 象似性原理 379\n8.2.2 顺序象似性 380\n8.2.3 距离象似性 380\n8.2.4 重叠象似性 381\n8.3 意象图式的构成 383\n8.3.1 主观性与焦点 383\n8.3.2 范畴化：概念的认知 385\n8.3.3 主体与背景 390\n8.3.4 意象图式 392\n8.3.5 社交中的图式 396\n8.3.6 完形：压缩与省略 398\n8.4 隐喻与转喻 401\n8.4.1 隐喻的结构 402\n8.4.2 隐喻的认知本质 403\n8.4.3 隐喻计算的系统架构 405\n8.4.4 隐喻计算的实现 408\n8.5 构式语法 412\n8.5.1 构式的概念 413\n8.5.2 句法与构式 415\n8.5.3 构式知识库 417\n8.6 结语 420\n第9章 NLP中的深度学习 422\n9.1 神经网络回顾 422\n9.1.1 神经网络框架 423\n9.1.2 梯度下降法推导 425\n9.1.3 梯度下降法的实现 427\n9.1.4 BP神经网络介绍和推导 430\n9.2 Word2Vec简介 433\n9.2.1 词向量及其表达 434\n9.2.2 Word2Vec的算法原理 436\n9.2.3 训练词向量 439\n9.2.4 大规模上下位关系的自动识别 443\n9.3 NLP与RNN 448\n9.3.1Simple-RNN 449\n9.3.2 LSTM原理 454\n9.3.3 LSTM的Python实现 460\n9.4 深度学习框架与应用 467\n9.4.1 Keras框架介绍 467\n9.4.2 Keras序列标注 471\n9.4.3 依存句法的算法原理 478\n9.4.4 Stanford依存解析的训练过程 483\n9.5 结语 488\n第10章 语义计算的架构 490\n10.1 句子的语义和语法预处理 490\n10.1.1 长句切分和融合 491\n10.1.2 共指消解 496\n10.2 语义角色 502\n10.2.1 谓词论元与语义角色 502\n10.2.2PropBank简介 505\n10.2.3 CPB中的特殊句式 506\n10.2.4 名词性谓词的语义角色 509\n10.2.5PropBank展开 512\n10.3 句子的语义解析 517\n10.3.1 语义依存 517\n10.3.2 完整架构 524\n10.3.3 实体关系抽取 527\n10.4 结语 531 [29]\nhttps://blog.csdn.net/yH0VLDe8VG8ep9VGe/article/details/83747195","data":"2018年11月08日 15:18:08"}
{"_id":{"$oid":"5d36b9046734bd8e681d62cb"},"title":"中文自然语言处理的完整流程","author":"metal1","content":"第一步：获取语料\n语料，即语言材料。语料是语言学研究的内容。语料是构成语料库的基本单元。所以，人们简单地用文本作为替代，并把文本中的上下文关系作为现实世界中语言的上下文关系的替代品。我们把一个文本集合称为语料库（Corpus），当有几个这样的文本集合的时候，我们称之为语料库集合(Corpora)。（定义来源：百度百科）按语料来源，我们将语料分为以下两种：\n1.已有语料\n很多业务部门、公司等组织随着业务发展都会积累有大量的纸质或者电子文本资料。那么，对于这些资料，在允许的条件下我们稍加整合，把纸质的文本全部电子化就可以作为我们的语料库。\n2.网上下载、抓取语料\n如果现在个人手里没有数据怎么办呢？这个时候，我们可以选择获取国内外标准开放数据集，比如国内的中文汉语有搜狗语料、人民日报语料。国外的因为大都是英文或者外文，这里暂时用不到。也可以选择通过爬虫自己去抓取一些数据，然后来进行后续内容。\n第二步：语料预处理\n这里重点介绍一下语料的预处理，在一个完整的中文自然语言处理工程应用中，语料预处理大概会占到整个50%-70%的工作量，所以开发人员大部分时间就在进行语料预处理。下面通过数据洗清、分词、词性标注、去停用词四个大的方面来完成语料的预处理工作。\n1.语料清洗\n数据清洗，顾名思义就是在语料中找到我们感兴趣的东西，把不感兴趣的、视为噪音的内容清洗删除，包括对于原始文本提取标题、摘要、正文等信息，对于爬取的网页内容，去除广告、标签、HTML、JS 等代码和注释等。常见的数据清洗方式有：人工去重、对齐、删除和标注等，或者规则提取内容、正则表达式匹配、根据词性和命名实体提取、编写脚本或者代码批处理等。\n2.分词\n中文语料数据为一批短文本或者长文本，比如：句子，文章摘要，段落或者整篇文章组成的一个集合。一般句子、段落之间的字、词语是连续的，有一定含义。而进行文本挖掘分析时，我们希望文本处理的最小单位粒度是词或者词语，所以这个时候就需要分词来将文本全部进行分词。\n常见的分词算法有：基于字符串匹配的分词方法、基于理解的分词方法、基于统计的分词方法和基于规则的分词方法，每种方法下面对应许多具体的算法。\n当前中文分词算法的主要难点有歧义识别和新词识别，比如：“羽毛球拍卖完了”，这个可以切分成“羽毛 球拍 卖 完 了”，也可切分成“羽毛球 拍卖 完 了”，如果不依赖上下文其他的句子，恐怕很难知道如何去理解。\n3.词性标注\n词性标注，就是给每个词或者词语打词类标签，如形容词、动词、名词等。这样做可以让文本在后面的处理中融入更多有用的语言信息。词性标注是一个经典的序列标注问题，不过对于有些中文自然语言处理来说，词性标注不是非必需的。比如，常见的文本分类就不用关心词性问题，但是类似情感分析、知识推理却是需要的，下图是常见的中文词性整理。\n常见的词性标注方法可以分为基于规则和基于统计的方法。其中基于统计的方法，如基于最大熵的词性标注、基于统计最大概率输出词性和基于 HMM 的词性标注。\n4.去停用词\n停用词一般指对文本特征没有任何贡献作用的字词，比如标点符号、语气、人称等一些词。所以在一般性的文本处理中，分词之后，接下来一步就是去停用词。但是对于中文来说，去停用词操作不是一成不变的，停用词词典是根据具体场景来决定的，比如在情感分析中，语气词、感叹号是应该保留的，因为他们对表示语气程度、感情色彩有一定的贡献和意义。\n第三步：特征工程\n做完语料预处理之后，接下来需要考虑如何把分词之后的字和词语表示成计算机能够计算的类型。显然，如果要计算我们至少需要把中文分词的字符串转换成数字，确切的说应该是数学中的向量。有两种常用的表示模型分别是词袋模型和词向量。\n词袋模型（Bag of Word, BOW)，即不考虑词语原本在句子中的顺序，直接将每一个词语或者符号统一放置在一个集合（如 list），然后按照计数的方式对出现的次数进行统计。统计词频这只是最基本的方式，TF-IDF 是词袋模型的一个经典用法。\n词向量是将字、词语转换成向量矩阵的计算模型。目前为止最常用的词表示方法是 One-hot，这种方法把每个词表示为一个很长的向量。这个向量的维度是词表大小，其中绝大多数元素为 0，只有一个维度的值为 1，这个维度就代表了当前的词。还有 Google 团队的 Word2Vec，其主要包含两个模型：跳字模型（Skip-Gram）和连续词袋模型（Continuous Bag of Words，简称 CBOW），以及两种高效训练的方法：负采样（Negative Sampling）和层序 Softmax（Hierarchical Softmax）。值得一提的是，Word2Vec 词向量可以较好地表达不同词之间的相似和类比关系。除此之外，还有一些词向量的表示方式，如 Doc2Vec、WordRank 和 FastText 等。\n第四步：特征选择\n同数据挖掘一样，在文本挖掘相关问题中，特征工程也是必不可少的。在一个实际问题中，构造好的特征向量，是要选择合适的、表达能力强的特征。文本特征一般都是词语，具有语义信息，使用特征选择能够找出一个特征子集，其仍然可以保留语义信息；但通过特征提取找到的特征子空间，将会丢失部分语义信息。所以特征选择是一个很有挑战的过程，更多的依赖于经验和专业知识，并且有很多现成的算法来进行特征的选择。目前，常见的特征选择方法主要有 DF、 MI、 IG、 CHI、WLLR、WFO 六种。\n第五步：模型训练\n在特征向量选择好之后，接下来要做的事情当然就是训练模型，对于不同的应用需求，我们使用不同的模型，传统的有监督和无监督等机器学习模型， 如 KNN、SVM、Naive Bayes、决策树、GBDT、K-means 等模型；深度学习模型比如 CNN、RNN、LSTM、 Seq2Seq、FastText、TextCNN 等。这些模型在后续的分类、聚类、神经序列、情感分析等示例中都会用到，这里不再赘述。下面是在模型训练时需要注意的几个点。\n1.注意过拟合、欠拟合问题，不断提高模型的泛化能力。\n过拟合：模型学习能力太强，以至于把噪声数据的特征也学习到了，导致模型泛化能力下降，在训练集上表现很好，但是在测试集上表现很差。\n常见的解决方法有：\n增大数据的训练量；\n增加正则化项，如 L1 正则和 L2 正则；\n特征选取不合理，人工筛选特征和使用特征选择算法；\n采用 Dropout 方法等。\n欠拟合：就是模型不能够很好地拟合数据，表现在模型过于简单。\n常见的解决方法有：\n添加其他特征项；\n增加模型复杂度，比如神经网络加更多的层、线性模型通过添加多项式使模型泛化能力更强；\n减少正则化参数，正则化的目的是用来防止过拟合的，但是现在模型出现了欠拟合，则需要减少正则化参数。\n2.对于神经网络，注意梯度消失和梯度爆炸问题。","data":"2018年12月30日 20:12:43"}
{"_id":{"$oid":"5d36b9276734bd8e681d62d6"},"title":"《从零开始学习自然语言处理（NLP）》-倒排索引（1）","author":"l7H9JA4","content":"作者：刘才权\n\n编辑：陈人和\n\n\n\n前  言\n在这个日新月异的信息时代，海量数据的积累，计算能力的不断提升，机器学习尤其是深度学习的蓬勃发展，使得人工智能技术在不同领域焕发出蓬勃的活力。自己经历了嵌入式开发，移动互联网开发，目前从事自然语言处理算法开发工作。从工程软件开发到自然语言处理算法开发，希望通过这个系列的文章，能够由浅入深，通俗易懂的介绍自然语言处理的领域知识，分享自己的成长，同大家一起进步。\n章节目录\n\n\n问题描述\n问题简化\n关键词匹配\n倒排索引\n搜索引擎框架\n小结\n\n\n\n01\n问题描述\n倒排索引是搜索引擎的基础算法，在本文中我们以一个简单的例子来详细介绍倒排索引的思想和实现。\n假设用户有个搜索query：“林俊杰2019演唱会行程”。百度的搜索结果如下：\n如果要求你来设计一个搜索引擎，来解决这个问题，你会如何着手呢？\n\n\n02\n问题简化\n现在我们把这个问题具体化。我们除了有要查询的query：“林俊杰2019演唱会行程”。还有被查询的网页数据库。这里我们做个简化，假设我们的网页数据库内容只有如下4条：\n网页1：\n2019年，JJ林俊杰全球演唱会在北京首场演出，行程如下xxxxxxx；\n网页2：\n林俊杰，吴亦凡终于同框合影 ，惹粉丝们尖叫连连，xxxxx；\n网页3：\n蔡依林2019世界演唱会行程全曝光，xxxxx；\n网页4:\n告别2018，迎接崭新的2019，xxxxxx；\n\n简单来说，就是从网页1~4中选取最理想的查询结果。你会怎么做呢？\n\n\n03\n关键词匹配\n\n最容易想到的方法就是关键词匹配了，简单的来说，就是网页中包含查询的关键词越多，网页和查询query的相关度也就越大。\n在做关键词查询前，一般文本会先进行预处理。这里的预处理主要包括去停用词和分词。\n去停用词\n去除和查询不相关的内容，比如本例子中的标点符号。在其他场景中，除了标点符号也会去除一些特别的字或词。\n\n分词\n分词主要目的是将句子切长短语或关键字，这样才利于查询匹配。比如“林俊杰2019演唱会行程”可以分词成\n林俊杰/2019/演唱会/行程。\n当然网页也需要这样进行分词：\n网页1：\n2019/年/JJ/林俊杰/全球/演唱会/在/北京/首场/演出/行程/如下/xxxxxx\n网页2：\n林俊杰/吴亦凡/终于/同框/合影/惹/粉丝们/尖叫/连连/xxxxx\n网页3：\n蔡依林/2019/世界/演唱会/行程/全曝光/xxxx\n\n网页4：\n告别/2018/迎接/崭新/的/2019/xxxxxx；\n\n\n分词是一项专门的技术，在实际工程中可以至今借助工具来完成，比如jieba分词。\n分词处理后，我们用查询query中的关键词在网页数据库中进行关键词匹配，并统计匹配数目：\n\n\n网页序号\n匹配关键词\n匹配个数\n网页1\n2019，林俊杰，演唱会，行程\n4\n网页2\n林俊杰\n1\n网页3\n2019，演唱会，行程\n3\n网页4\n2019\n1\n从“匹配个数”中很容易确定，网页1就是和查询query最匹配的网页。\n\n\n04\n倒排索引\n讲到这里大家可能会疑问，这和倒排索引有什么关系？实际上，如果仔细考虑上面的关键词查询过程，会发现这种方法有个很大的效率问题：我们的例子中只有4个待查询的网页，而实际的互联网世界的网页数目是非常巨大的。假设互联网世界的网页数据为N，那么使用关键词查询的时间复杂度就是O(N)，然，这样的时间复杂度还是太大了，而倒排索引就很好的优化了这个问题。\n从倒排索引这个名字很容易联想出它的实现，关键就是“倒排”的“索引”。在前面的讲解中，我们的索引(key)是网页，内容(value)是关键字。倒排索引就是反过来：内容关键字作为索引(key)，所在网页作为内容(value)。前面的表格就可以改写成，\n\n关键词\n包含关键词的网页\n林俊杰\n网页1，网页2\n2019\n网页1，网页3，网页4\n演唱会\n网页1，网页3,\n行程\n网页1，网页3\n通过上面的表格，很明显网页1是包含最多关键词的网页，也是和查询query相关度最高的网页。采用倒排索引的方法，搜索的时间复杂度得到了明显的降低。\n\n\n\n\n05\n搜索引擎框架\n有了倒排索引的知识，我们就可以搭建简单的搜索引擎了，\n具体步骤包括：\n网页抓取\n主要是借助网络爬虫，来抓取网络世界的所有网页，并进行存储。网络爬虫是一项专门的技术，目前工程上也有很多现成的开源工具。\n倒排索引生成\n将抓取后的网页经过预处理后，整理生成倒排索引。\n用户在线查询\n借助倒排索引，搜索引擎能够满足用户的实时在线查询。前两个步骤是不用考虑实时性的，可以离线进行，而用户的在线查询则需要保证实时性。\n\n\n\n\n06\n小结\n\n本文通过一个搜索查询的例子，引出关键词查询的方案，及遇到的问题。进而介绍了倒排索引的原理，和搜索引擎的整体框架。现代搜索引擎是一个非常庞大和复杂的系统工程，这里的例子只是为了方便大家理解做了特别的简化。文中提到的分词和网络爬虫也是专门的文本处理技术，在后续的文章后，会根据需要专门展开。\n\n\n\n\n\n\n\n\nEND\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n往期回顾之作者刘才权\n【1】《机器学习》笔记-神经网络（5）\n【2】《从零开始学习自然语言处理(NLP)》 -基础准备(0)\n\n【3】《机器学习》笔记-降维与度量学习（10）\n【4】《机器学习》笔记-聚类（9）\n【5】《机器学习》笔记-集成学习（8）\n【6】《机器学习》笔记-贝叶斯分类器（7）\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n机器学习算法工程师\n一个用心的公众号\n长按，识别，加关注\n\n进群，学习，得帮助\n你的关注，我们的热度，\n我们一定给你学习最大的帮助\n\n\n\n\n你点的每个赞，我都认真当成了喜欢","data":"2019年02月26日 17:19:08"}
{"_id":{"$oid":"5d36b9476734bd8e681d62e6"},"title":"自然语言处理入门资料推荐","author":"AI深入浅出","content":"最近几个月小编遨游在税务行业的智能问答调研和开发中，里面涉及到了很多的自然语言处理NLP的功能点。虽然接触NLP也有近两年的时间了，现在真正要应用到问答中，避免不了还是需要再重新熟识并深入研究理解。\n\n\n下面是与NLP相关的一些书籍推荐、课件推荐和开源工具推荐。\n\n\n主要是记录下入门的资料，由于资料的存储位置没有做规整，所以本文没有附带资源下载链接。如果有同学需要其中的资源，可以在公众号上给我留言，回头我把资源链接反馈给您。\n\n\n\n\n部分开源工具和语料资源\n\n1、NLTK官方提供的语料库资源列表\n2、OpenNLP上的开源自然语言处理工具列表\n3、斯坦福大学自然语言处理组维护的“统计自然语言处理及基于语料库的计算语言学资源列表”\n4、LDC上免费的中文信息处理资源\n\n\n\n\n课件\n1、哈工大刘挺老师的“统计自然语言处理”课件；\n2、哈工大刘秉权老师的“自然语言处理”课件；\n3、中科院计算所刘群老师的“计算语言学讲义“课件；\n4、中科院自动化所宗成庆老师的“自然语言理解”课件；\n5、北大常宝宝老师的“计算语言学”课件；\n6、北大詹卫东老师的“中文信息处理基础”的课件及相关代码；\n7、MIT大牛Michael Collins的“Machine Learning Approaches for Natural Language Processing(面向自然语言处理的机器学习方法)”课件；\n8、Michael Collins的“Machine Learning （机器学习）”课件；\n9、SMT牛人Philipp Koehn “Advanced Natural Language Processing（高级自然语言处理）”课件；\n10、Philipp Koehn “Empirical Methods in Natural Language Processing”课件；\n11、Philipp Koehn“Machine Translation（机器翻译）”课件。\n\n\n书籍\n1、《自然语言处理综论》英文版第二版\n2、《统计自然语言处理基础》英文版\n3、《用Python进行自然语言处理》，NLTK配套书\n4、《Learning Python第三版》，Python入门经典书籍，详细而不厌其烦\n5、《自然语言处理中的模式识别》\n6、《EM算法及其扩展》\n7、《统计学习基础》\n8、《自然语言理解》英文版（似乎只有前9章）\n9、《Fundamentals of Speech Recognition》，质量不太好，不过第6章关于HMM的部分比较详细，作者之一便是Lawrence Rabiner；\n10、概率统计经典入门书：\n《概率论及其应用》（英文版，威廉*费勒著） 第一卷　　第二卷　　DjVuLibre阅读器（阅读前两卷书需要）\n11、一本利用Perl和Prolog进行自然语言处理的介绍书籍：《An Introduction to Language Processing with Perl and Prolog》\n12、国外机器学习书籍之：\n1) “Programming Collective Intelligence“，中文译名《集体智慧编程》，机器学习\u0026数据挖掘领域”近年出的入门好书，培养兴趣是最重要的一环，一上来看大部头很容易被吓走的”\n2) “Machine Learning“,机器学习领域无可争议的经典书籍，下载完毕将后缀改为pdf即可。\n豆瓣评论 by 王宁）：老书，牛人。现在看来内容并不算深，很多章节有点到为止的感觉，但是很适合新手（当然，不能”新”到连算法和概率都不知道）入门。比如决策树部分就很精彩，并且这几年没有特别大的进展，所以并不过时。另外，这本书算是对97年前数十年机器学习工作的大综述，参考文献列表极有价值。国内有翻译和影印版，不知道绝版否。\n3) “Introduction to Machine Learning”\n\n\n13、国外数据挖掘书籍之：\n1) “Data.Mining.Concepts.and.Techniques.2nd“，数据挖掘经典书籍。华裔科学家写的书，相当深入浅出。\n2) Data Mining:Practical Machine Learning Tools and Techniques\n3) Beautiful Data: The Stories Behind Elegant Data Solutions（ Toby Segaran, Jeff Hammerbacher）\n\n\n14、国外模式识别书籍之：\n1）“Pattern Recognition”\n2）“Pattern Recongnition Technologies and Applications”\n3）“An Introduction to Pattern Recognition”\n4）“Introduction to Statistical Pattern Recognition”\n5）“Statistical Pattern Recognition 2nd Edition”\n6）“Supervised and Unsupervised Pattern Recognition”\n7）“Support Vector Machines for Pattern Classification”\n\n\n15、国外人工智能书籍之：\n1）Artificial Intelligence: A Modern Approach (2nd Edition) 人工智能领域无争议的经典。\n2）“Paradigms of Artificial Intelligence Programming: Case Studies in Common LISP”\n\n\n16、其他相关书籍：\n1）Programming the Semantic Web，Toby Segaran , Colin Evans, Jamie Taylor\n2）Learning.Python第四版，英文\n\n\n任何梦想家都不足以成事，因为所有的成功者都是实干家。\n——《浪潮之巅》\n欢迎转发到朋友圈或分享给好友","data":"2017年11月26日 00:00:00"}
{"_id":{"$oid":"5d36b9526734bd8e681d62ec"},"title":"自然语言处理（五）","author":"dayday学习","content":"自然语言处理（五）\n传统机器学习\n1. 朴素贝叶斯的原理\n1.1 朴素贝叶斯相关的统计学知识\n1.2基本定义\n2. 利用朴素贝叶斯模型进行文本分类\n2.1模型原理与训练\n3. SVM的原理\n3.1快速理解SVM原理\n4. 利用SVM模型进行文本分类\n5. pLSA、共轭先验分布；LDA主题模型原理\n6. 使用LDA生成主题特征，在之前特征的基础上加入主题特征进行文本分类\n传统机器学习\n1. 朴素贝叶斯的原理\n1.1 朴素贝叶斯相关的统计学知识\n贝叶斯学派很古老，但是从诞生到一百年前一直不是主流。主流是频率学派。频率学派的权威皮尔逊和费歇尔都对贝叶斯学派不屑一顾，但是贝叶斯学派硬是凭借在现代特定领域的出色应用表现为自己赢得了半壁江山。\n贝叶斯学派的思想可以概括为先验概率+数据=后验概率。也就是说我们在实际问题中需要得到的后验概率，可以通过先验概率和数据一起综合得到。数据大家好理解，被频率学派攻击的是先验概率，一般来说先验概率就是我们对于数据所在领域的历史经验，但是这个经验常常难以量化或者模型化，于是贝叶斯学派大胆的假设先验分布的模型，比如正态分布，beta分布等。这个假设一般没有特定的依据，因此一直被频率学派认为很荒谬。虽然难以从严密的数学逻辑里推出贝叶斯学派的逻辑，但是在很多实际应用中，贝叶斯理论很好用，比如垃圾邮件分类，文本分类。\n我们先看看条件独立公式，如果X和Y相互独立，则有：\nP(X,Y)=P(X)P(Y)\nP(X,Y)=P(X)P(Y)\n我们接着看看条件概率公式：\nP(Y|X)=P(X,Y)/P(X)\nP(Y|X)=P(X,Y)/P(X)\nP(X|Y)=P(X,Y)/P(Y)\nP(X|Y)=P(X,Y)/P(Y)\n或者说:\nP(Y|X)=P(X|Y)P(Y)/P(X)\nP(Y|X)=P(X|Y)P(Y)/P(X)\n接着看看全概率公式\nP(X)=∑kP(X|Y=Yk)P(Yk)其中∑kP(Yk)=1\nP(X)=∑kP(X|Y=Yk)P(Yk)其中∑kP(Yk)=1\n从上面的公式很容易得出贝叶斯公式：\nP(Yk|X)=P(X|Yk)P(Yk)∑kP(X|Y=Yk)P(Yk)\n\n基于朴素贝叶斯公式，比较出后验概率的最大值来进行分类，后验概率的计算是由先验概率与类条件概率的乘积得出，先验概率和类条件概率要通过训练数据集得出，即为朴素贝叶斯分类模型，将其保存为中间结果，测试文档进行分类时调用这个中间结果得出后验概率。\n1.2基本定义\n朴素贝叶斯分类是一种十分简单的分类算法，叫它朴素贝叶斯分类是因为这种方法的思想真的很朴素，朴素贝叶斯的思想基础是这样的：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。\n朴素贝叶斯分类的正式定义如下：\n1、设 为一个待分类项，而每个a为x的一个特征属性。\n2、有类别集合。\n3、计算。\n\n4、如果 ，则。\n那么现在的关键就是如何计算第3步中的各个条件概率。我们可以这么做：\n1、找到一个已知分类的待分类项集合，这个集合叫做训练样本集。\n2、统计得到在各类别下各个特征属性的条件概率估计。即\n\n。\n3、如果各个特征属性是条件独立的，则根据贝叶斯定理有如下推导：\n\n因为分母对于所有类别为常数，因为我们只要将分子最大化皆可。又因为各特征属性是条件独立的，所以有：\n\n2. 利用朴素贝叶斯模型进行文本分类\n2.1模型原理与训练\n朴素贝叶斯分类器是一种有监督学习，常见有两种模型，多项式模型(multinomial model)即为词频型和伯努利模型(Bernoulli model)即文档型，还有一种高斯模型。\n前二者的计算粒度不一样，多项式模型以单词为粒度，伯努利模型以文件为粒度，因此二者的先验概率和类条件概率的计算方法都不同。计算后验概率时，对于一个文档d，多项式模型中，只有在d中出现过的单词，才会参与后验概率计算，伯努利模型中，没有在d中出现，但是在全局单词表中出现的单词，也会参与计算，不过是作为“反方”参与的。\n这里暂不考虑特征抽取、为避免消除测试文档时类条件概率中有为0现象而做的取对数等问题。\n# -*- coding: UTF-8 -*- from sklearn.naive_bayes import MultinomialNB import matplotlib.pyplot as plt import os import random import jieba \"\"\" 函数说明:中文文本处理 Parameters: folder_path - 文本存放的路径 test_size - 测试集占比，默认占所有数据集的百分之20 Returns: all_words_list - 按词频降序排序的训练集列表 train_data_list - 训练集列表 test_data_list - 测试集列表 train_class_list - 训练集标签列表 test_class_list - 测试集标签列表 Author: Jack Cui Blog: http://blog.csdn.net/c406495762 Modify: 2017-08-22 \"\"\" def TextProcessing(folder_path, test_size = 0.2): folder_list = os.listdir(folder_path) #查看folder_path下的文件 data_list = [] #数据集数据 class_list = [] #数据集类别 #遍历每个子文件夹 for folder in folder_list: new_folder_path = os.path.join(folder_path, folder) #根据子文件夹，生成新的路径 files = os.listdir(new_folder_path) #存放子文件夹下的txt文件的列表 j = 1 #遍历每个txt文件 for file in files: if j \u003e 100: #每类txt样本数最多100个 break with open(os.path.join(new_folder_path, file), 'r', encoding = 'utf-8') as f: #打开txt文件 raw = f.read() word_cut = jieba.cut(raw, cut_all = False) #精简模式，返回一个可迭代的generator word_list = list(word_cut) #generator转换为list data_list.append(word_list) #添加数据集数据 class_list.append(folder) #添加数据集类别 j += 1 data_class_list = list(zip(data_list, class_list)) #zip压缩合并，将数据与标签对应压缩 random.shuffle(data_class_list) #将data_class_list乱序 index = int(len(data_class_list) * test_size) + 1 #训练集和测试集切分的索引值 train_list = data_class_list[index:] #训练集 test_list = data_class_list[:index] #测试集 train_data_list, train_class_list = zip(*train_list) #训练集解压缩 test_data_list, test_class_list = zip(*test_list) #测试集解压缩 all_words_dict = {} #统计训练集词频 for word_list in train_data_list: for word in word_list: if word in all_words_dict.keys(): all_words_dict[word] += 1 else: all_words_dict[word] = 1 #根据键的值倒序排序 all_words_tuple_list = sorted(all_words_dict.items(), key = lambda f:f[1], reverse = True) all_words_list, all_words_nums = zip(*all_words_tuple_list) #解压缩 all_words_list = list(all_words_list) #转换成列表 return all_words_list, train_data_list, test_data_list, train_class_list, test_class_list \"\"\" 函数说明:读取文件里的内容，并去重 Parameters: words_file - 文件路径 Returns: words_set - 读取的内容的set集合 Author: Jack Cui Blog: http://blog.csdn.net/c406495762 Modify: 2017-08-22 \"\"\" def MakeWordsSet(words_file): words_set = set() #创建set集合 with open(words_file, 'r', encoding = 'utf-8') as f: #打开文件 for line in f.readlines(): #一行一行读取 word = line.strip() #去回车 if len(word) \u003e 0: #有文本，则添加到words_set中 words_set.add(word) return words_set #返回处理结果 \"\"\" 函数说明:根据feature_words将文本向量化 Parameters: train_data_list - 训练集 test_data_list - 测试集 feature_words - 特征集 Returns: train_feature_list - 训练集向量化列表 test_feature_list - 测试集向量化列表 Author: Jack Cui Blog: http://blog.csdn.net/c406495762 Modify: 2017-08-22 \"\"\" def TextFeatures(train_data_list, test_data_list, feature_words): def text_features(text, feature_words): #出现在特征集中，则置1 text_words = set(text) features = [1 if word in text_words else 0 for word in feature_words] return features train_feature_list = [text_features(text, feature_words) for text in train_data_list] test_feature_list = [text_features(text, feature_words) for text in test_data_list] return train_feature_list, test_feature_list #返回结果 \"\"\" 函数说明:文本特征选取 Parameters: all_words_list - 训练集所有文本列表 deleteN - 删除词频最高的deleteN个词 stopwords_set - 指定的结束语 Returns: feature_words - 特征集 Author: Jack Cui Blog: http://blog.csdn.net/c406495762 Modify: 2017-08-22 \"\"\" def words_dict(all_words_list, deleteN, stopwords_set = set()): feature_words = [] #特征列表 n = 1 for t in range(deleteN, len(all_words_list), 1): if n \u003e 1000: #feature_words的维度为1000 break #如果这个词不是数字，并且不是指定的结束语，并且单词长度大于1小于5，那么这个词就可以作为特征词 if not all_words_list[t].isdigit() and all_words_list[t] not in stopwords_set and 1 \u003c len(all_words_list[t]) \u003c 5: feature_words.append(all_words_list[t]) n += 1 return feature_words \"\"\" 函数说明:新闻分类器 Parameters: train_feature_list - 训练集向量化的特征文本 test_feature_list - 测试集向量化的特征文本 train_class_list - 训练集分类标签 test_class_list - 测试集分类标签 Returns: test_accuracy - 分类器精度 Author: Jack Cui Blog: http://blog.csdn.net/c406495762 Modify: 2017-08-22 \"\"\" def TextClassifier(train_feature_list, test_feature_list, train_class_list, test_class_list): classifier = MultinomialNB().fit(train_feature_list, train_class_list) test_accuracy = classifier.score(test_feature_list, test_class_list) return test_accuracy if __name__ == '__main__': #文本预处理 folder_path = './SogouC/Sample' #训练集存放地址 all_words_list, train_data_list, test_data_list, train_class_list, test_class_list = TextProcessing(folder_path, test_size=0.2) # 生成stopwords_set stopwords_file = './stopwords_cn.txt' stopwords_set = MakeWordsSet(stopwords_file) test_accuracy_list = [] deleteNs = range(0, 1000, 20) #0 20 40 60 ... 980 for deleteN in deleteNs: feature_words = words_dict(all_words_list, deleteN, stopwords_set) train_feature_list, test_feature_list = TextFeatures(train_data_list, test_data_list, feature_words) test_accuracy = TextClassifier(train_feature_list, test_feature_list, train_class_list, test_class_list) test_accuracy_list.append(test_accuracy) # ave = lambda c: sum(c) / len(c) # print(ave(test_accuracy_list)) plt.figure() plt.plot(deleteNs, test_accuracy_list) plt.title('Relationship of deleteNs and test_accuracy') plt.xlabel('deleteNs') plt.ylabel('test_accuracy') plt.show()\n3. SVM的原理\n3.1快速理解SVM原理\n很多讲解SVM的书籍都是从原理开始讲解，如果没有相关知识的铺垫，理解起来还是比较吃力的，以下的一个例子可以让我们对SVM快速建立一个认知。\n给定训练样本，支持向量机建立一个超平面作为决策曲面，使得正例和反例的隔离边界最大化。\n决策曲面的初步理解可以参考如下过程，\n1）如下图想象红色和蓝色的球为球台上的桌球，我们首先目的是找到一条曲线将蓝色和红色的球分开，于是我们得到一条黑色的曲线。\n\n2） 为了使黑色的曲线离任意的蓝球和红球距离（也就是我们后面要提到的margin）最大化，我们需要找到一条最优的曲线。如下图，\n\n3） 想象一下如果这些球不是在球桌上，而是被抛向了空中，我们仍然需要将红色球和蓝色球分开，这时就需要一个曲面，而且我们需要这个曲面仍然满足跟所有任意红球和蓝球的间距的最大化。需要找到的这个曲面，就是我们后面详细了解的最优超平面。\n\n4) 离这个曲面最近的红色球和蓝色球就是Support Vector。\n4. 利用SVM模型进行文本分类\n具体 参考https://blog.csdn.net/Kaiyuan_sjtu/article/details/80064145\n5. pLSA、共轭先验分布；LDA主题模型原理\npLSA：\n参考：https://www.cnblogs.com/Determined22/p/7237111.html\nLDA主题模型原理:\n参考：http://www.cnblogs.com/pinard/p/6831308.html\n6. 使用LDA生成主题特征，在之前特征的基础上加入主题特征进行文本分类\n参考：https://blog.csdn.net/Kaiyuan_sjtu/article/details/83572927\n参考：\n1.https://blog.csdn.net/u013710265/article/details/72780520\n2.https://blog.csdn.net/u013710265/article/details/72780520\n3.https://blog.csdn.net/yyy430/article/details/88346920","data":"2019年03月09日 21:49:40"}
{"_id":{"$oid":"5d36b95d6734bd8e681d62f2"},"title":"自然语言处理（五 文本相似度）","author":"zchenack","content":"简单共有词判断模型\nTFIDF向量表示\nTFIDFWord2vec\nLMSentence Embedding表示\n简单共有词判断模型\n假设现在有文本A和B，\nNum(A∩B)\nNum(A\\cap B) 表示A和B中相同词的数量，\nNum(A∪B)\nNum(A\\cup B)表示A和B中所有词的数量。那么定义A和B的相似程度为：\n\nSimilarity(A,B)=Num(A∩B)Num(A∪B)\n\\begin{equation} Similarity(A,B) = \\frac{Num(A\\cap B)}{Num(A\\cup B)} \\end{equation}\nTFIDF向量表示\n上述共有词方式，只利用了词语的信息，却忽略了词频信息，引入TFIDF将词语向量化，既考虑了Term Frequency词频，又考虑了词语在整个文档中的分布情况。文本A和文本B可以分别表示为：\n\nA=[a1,a2,...,aN]B=[b1,b2,...,bN]\n\\begin{equation} A = [a_1,a_2,...,a_N] \\\\ B = [b_1,b_2,...,b_N] \\end{equation}\n其中N表示词语的总数（或词典大小），\nai=TF(i)∗IDF(i)\na_i=TF(i)*IDF(i) ，\nTF(i)\nTF(i)表示词语i在文档（\nai\na_i表示为A文档）中出现的频率，\nIDF(i)\nIDF(i)表示在所有文档中出现的频率。同理可以得到\nbi\nb_i。得到A,B 文档的TFIDF向量表示后，可以根据相似度函数\nf(a,b)\nf(a,b)来计算A和B文档的相似度。\nf(a,b)\nf(a,b)可以选用一阶范数\n|a−b|\n|a-b|，也可以选用余弦相似度\ncosine(a,b)\ncosine(a,b)来表示。\nTFIDF+Word2vec\nTFIDF未给出词语与词语之间的关系，认为每个词语都是相互独立的个体，但有些词语是同义词，有些词语是反义词。需要表征词语之间意思相距程度。此处选用word2vec，利用额外的大预料为每个词语训练一个word2vec向量表示。该向量可以表示矩阵为\nMword2vec\nM_{word2vec}，维度为N*K，其中N表示词典大小，K表示向量维度：\n\nMword2vec=[mij]N∗K\n\\begin{equation} M_{word2vec}=[m_{ij}]_{N*K} \\end{equation}\n由上一段知道，TFIDF是一个M*N的向量，其中M表示文档的总数，N表示词典的大小。因此可以使用向量M表示文档（A或B）,如下所示：\n\nM=MTFIDF∗Mword2vec\n\\begin{equation} M=M_{TFIDF}*M_{word2vec} \\end{equation}\nM是一个M*K的矩阵，即每个文档可以表示为M的一个行向量（K维）。再使用该向量用于计算文本之间的相似度。\nLM+Sentence Embedding表示\n\n使用Deep Learning（LSTM）的方法对一个大语料训练一个Language Model，然后使用BiRNN模型训练得到句子的表达\n[ff;fb]\n[f_f;f_b],\nff\nf_f表示前向RNN的的表达，\nfb\nf_b表示反向RNN的表达。模型的输出，得到句子表达，然后再利用余弦相似度进行文本相似度比较。","data":"2017年12月16日 12:40:36"}
{"_id":{"$oid":"5d36b9686734bd8e681d62f8"},"title":"[NLP自然语言处理]谷歌BERT模型深度解析","author":"刺客五六柒","content":"BERT模型代码已经发布，可以在我的github: NLP-BERT--Python3.6-pytorch 中下载，请记得start哦\n目录\n一、前言\n二、如何理解BERT模型\n三、BERT模型解析\n论文的核心：详解BERT模型架构\n关键创新：预训练任务\n实验结果\n四、BERT模型的影响\n对BERT模型的观点\n参考文献\n一、前言\n最近谷歌搞了个大新闻，公司AI团队新发布的BERT模型，在机器阅读理解顶级水平测试SQuAD1.1中表现出惊人的成绩：全部两个衡量指标上全面超越人类，并且还在11种不同NLP测试中创出最佳成绩，包括将GLUE基准推至80.4％（绝对改进7.6％），MultiNLI准确度达到86.7% （绝对改进率5.6％）等。可以预见的是，BERT将为NLP带来里程碑式的改变，也是NLP领域近期最重要的进展。\n谷歌团队的Thang Luong直接定义：BERT模型开启了NLP的新时代！\n从现在的大趋势来看，使用某种模型预训练一个语言模型看起来是一种比较靠谱的方法。从之前AI2的 ELMo，到 OpenAI的fine-tune transformer，再到Google的这个BERT，全都是对预训练的语言模型的应用。\nBERT这个模型与其它两个不同的是\n它在训练双向语言模型时以减小的概率把少量的词替成了Mask或者另一个随机的词。我个人感觉这个目的在于使模型被迫增加对上下文的记忆。至于这个概率，我猜是Jacob拍脑袋随便设的。\n增加了一个预测下一句的loss。这个看起来就比较新奇了。\nBERT模型具有以下两个特点：\n第一，是这个模型非常的深，12层，并不宽(wide），中间层只有1024，而之前的Transformer模型中间层有2048。这似乎又印证了计算机图像处理的一个观点——深而窄 比 浅而宽 的模型更好。\n第二，MLM（Masked Language Model），同时利用左侧和右侧的词语，这个在ELMo上已经出现了，绝对不是原创。其次，对于Mask（遮挡）在语言模型上的应用，已经被Ziang Xie提出了（我很有幸的也参与到了这篇论文中）：[1703.02573] Data Noising as Smoothing in Neural Network Language Models。这也是篇巨星云集的论文：Sida Wang，Jiwei Li（香侬科技的创始人兼CEO兼史上发文最多的NLP学者），Andrew Ng，Dan Jurafsky都是Coauthor。但很可惜的是他们没有关注到这篇论文。用这篇论文的方法去做Masking，相信BRET的能力说不定还会有提升。\n二、如何理解BERT模型\n[1] BERT 要解决什么问题？\n通常情况 transformer 模型有很多参数需要训练。譬如 BERT BASE 模型: L=12, H=768, A=12, 需要训练的模型参数总数是 12 * 768 * 12 = 110M。这么多参数需要训练，自然需要海量的训练语料。如果全部用人力标注的办法，来制作训练数据，人力成本太大。\n受《A Neural Probabilistic Language Model》论文的启发，BERT 也用 unsupervised 的办法，来训练 transformer 模型。神经概率语言模型这篇论文，主要讲了两件事儿，1. 能否用数值向量（word vector）来表达自然语言词汇的语义？2. 如何给每个词汇，找到恰当的数值向量？\n这篇论文写得非常精彩，深入浅出，要言不烦，而且面面俱到。经典论文，值得反复咀嚼。很多同行朋友都熟悉这篇论文，内容不重复说了。常用的中文汉字有 3500 个，这些字组合成词汇，中文词汇数量高达 50 万个。假如词向量的维度是 512，那么语言模型的参数数量，至少是 512 * 50万 = 256M\n模型参数数量这么大，必然需要海量的训练语料。从哪里收集这些海量的训练语料？《A Neural Probabilistic Language Model》这篇论文说，每一篇文章，天生是训练语料。难道不需要人工标注吗？回答，不需要。\n我们经常说，“说话不要颠三倒四，要通顺，要连贯”，意思是上下文的词汇，应该具有语义的连贯性。基于自然语言的连贯性，语言模型根据前文的词，预测下一个将出现的词。如果语言模型的参数正确，如果每个词的词向量设置正确，那么语言模型的预测，就应该比较准确。天下文章，数不胜数，所以训练数据，取之不尽用之不竭。\n深度学习四大要素，1. 训练数据、2. 模型、3. 算力、4. 应用。训练数据有了，接下去的问题是模型。\n[2] BERT 的五个关键词 Pre-training、Deep、Bidirectional、Transformer、Language Understanding 分别是什么意思？\n《A Neural Probabilistic Language Model》这篇论文讲的 Language Model，严格讲是语言生成模型（Language Generative Model），预测语句中下一个将会出现的词汇。语言生成模型能不能直接移用到其它 NLP 问题上去？\n譬如，淘宝上有很多用户评论，能否把每一条用户转换成评分？-2、-1、0、1、2，其中 -2 是极差，+2 是极好。假如有这样一条用户评语，“买了一件鹿晗同款衬衫，没想到，穿在自己身上，不像小鲜肉，倒像是厨师”，请问这条评语，等同于 -2，还是其它？\n语言生成模型，能不能很好地解决上述问题？进一步问，有没有 “通用的” 语言模型，能够理解语言的语义，适用于各种 NLP 问题？BERT 这篇论文的题目很直白，《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》，一眼看去，就能猜得到这篇文章会讲哪些内容。\n这个题目有五个关键词，分别是 Pre-training、Deep、Bidirectional、Transformers、和 Language Understanding。其中 pre-training 的意思是，作者认为，确实存在通用的语言模型，先用文章预训练通用模型，然后再根据具体应用，用 supervised 训练数据，精加工（fine tuning）模型，使之适用于具体应用。为了区别于针对语言生成的 Language Model，作者给通用的语言模型，取了一个名字，叫语言表征模型 Language Representation Model。\n能实现语言表征目标的模型，可能会有很多种，具体用哪一种呢？作者提议，用 Deep Bidirectional Transformers 模型。假如给一个句子 “能实现语言表征[mask]的模型”，遮盖住其中“目标”一词。从前往后预测[mask]，也就是用“能/实现/语言/表征”，来预测[mask]；或者，从后往前预测[mask]，也就是用“模型/的”，来预测[mask]，称之为单向预测 unidirectional。单向预测，不能完整地理解整个语句的语义。于是研究者们尝试双向预测。把从前往后，与从后往前的两个预测，拼接在一起 [mask1/mask2]，这就是双向预测 bi-directional。细节参阅《Neural Machine Translation by Jointly Learning to Align and Translate》。\nBERT 的作者认为，bi-directional 仍然不能完整地理解整个语句的语义，更好的办法是用上下文全向来预测[mask]，也就是用 “能/实现/语言/表征/../的/模型”，来预测[mask]。BERT 作者把上下文全向的预测方法，称之为 deep bi-directional。如何来实现上下文全向预测呢？BERT 的作者建议使用 Transformer 模型。这个模型由《Attention Is All You Need》一文发明。\n这个模型的核心是聚焦机制，对于一个语句，可以同时启用多个聚焦点，而不必局限于从前往后的，或者从后往前的，序列串行处理。不仅要正确地选择模型的结构，而且还要正确地训练模型的参数，这样才能保障模型能够准确地理解语句的语义。BERT 用了两个步骤，试图去正确地训练模型的参数。第一个步骤是把一篇文章中，15% 的词汇遮盖，让模型根据上下文全向地预测被遮盖的词。假如有 1 万篇文章，每篇文章平均有 100 个词汇，随机遮盖 15% 的词汇，模型的任务是正确地预测这 15 万个被遮盖的词汇。通过全向预测被遮盖住的词汇，来初步训练 Transformer 模型的参数。然后，用第二个步骤继续训练模型的参数。譬如从上述 1 万篇文章中，挑选 20 万对语句，总共 40 万条语句。挑选语句对的时候，其中 2*10 万对语句，是连续的两条上下文语句，另外 2*10 万对语句，不是连续的语句。然后让 Transformer 模型来识别这 20 万对语句，哪些是连续的，哪些不连续。\n这两步训练合在一起，称为预训练 pre-training。训练结束后的 Transformer 模型，包括它的参数，是作者期待的通用的语言表征模型。\n三、BERT模型解析\n首先来看下谷歌AI团队做的这篇论文。\nBERT的新语言表示模型，它代表Transformer的双向编码器表示。与最近的其他语言表示模型不同，BERT旨在通过联合调节所有层中的上下文来预先训练深度双向表示。因此，预训练的BERT表示可以通过一个额外的输出层进行微调，适用于广泛任务的最先进模型的构建，比如问答任务和语言推理，无需针对具体任务做大幅架构修改。\n论文作者认为现有的技术严重制约了预训练表示的能力。其主要局限在于标准语言模型是单向的，这使得在模型的预训练中可以使用的架构类型很有限。\n在论文中，作者通过提出BERT：即Transformer的双向编码表示来改进基于架构微调的方法。\nBERT 提出一种新的预训练目标：遮蔽语言模型（masked language model，MLM），来克服上文提到的单向性局限。MLM 的灵感来自 Cloze 任务（Taylor, 1953）。MLM 随机遮蔽模型输入中的一些 token，目标在于仅基于遮蔽词的语境来预测其原始词汇 id。\n与从左到右的语言模型预训练不同，MLM 目标允许表征融合左右两侧的语境，从而预训练一个深度双向 Transformer。除了遮蔽语言模型之外，本文作者还引入了一个“下一句预测”（next sentence prediction）任务，可以和MLM共同预训练文本对的表示。\n论文的主要贡献在于：\n证明了双向预训练对语言表示的重要性。与之前使用的单向语言模型进行预训练不同，BERT使用遮蔽语言模型来实现预训练的深度双向表示。\n论文表明，预先训练的表示免去了许多工程任务需要针对特定任务修改体系架构的需求。 BERT是第一个基于微调的表示模型，它在大量的句子级和token级任务上实现了最先进的性能，强于许多面向特定任务体系架构的系统。\nBERT刷新了11项NLP任务的性能记录。本文还报告了 BERT 的模型简化研究（ablation study），表明模型的双向性是一项重要的新成果。相关代码和预先训练的模型将会公布在goo.gl/language/bert上。\nBERT目前已经刷新的11项自然语言处理任务的最新记录包括：将GLUE基准推至80.4％（绝对改进7.6％），MultiNLI准确度达到86.7% （绝对改进率5.6％），将SQuAD v1.1问答测试F1得分纪录刷新为93.2分（绝对提升1.5分），超过人类表现2.0分。\n论文的核心：详解BERT模型架构\n本节介绍BERT模型架构和具体实现，并介绍预训练任务，这是这篇论文的核心创新。\n模型架构\nBERT的模型架构是基于Vaswani et al. (2017) 中描述的原始实现multi-layer bidirectional Transformer编码器，并在tensor2tensor库中发布。由于Transformer的使用最近变得无处不在，论文中的实现与原始实现完全相同，因此这里将省略对模型结构的详细描述。\n在这项工作中，论文将层数（即Transformer blocks）表示为L，将隐藏大小表示为H，将self-attention heads的数量表示为A。在所有情况下，将feed-forward/filter 的大小设置为 4H，即H = 768时为3072，H = 1024时为4096。论文主要报告了两种模型大小的结果：\n: L=12, H=768, A=12, Total Parameters=110M\n: L=24, H=1024, A=16, Total Parameters=340M\n为了进行比较，论文选择了  ，它与OpenAI GPT具有相同的模型大小。然而，重要的是，BERT Transformer 使用双向self-attention，而GPT Transformer 使用受限制的self-attention，其中每个token只能处理其左侧的上下文。研究团队注意到，在文献中，双向 Transformer 通常被称为“Transformer encoder”，而左侧上下文被称为“Transformer decoder”，因为它可以用于文本生成。BERT，OpenAI GPT和ELMo之间的比较如图1所示。\n图1：预训练模型架构的差异。BERT使用双向Transformer。OpenAI GPT使用从左到右的Transformer。ELMo使用经过独立训练的从左到右和从右到左LSTM的串联来生成下游任务的特征。三个模型中，只有BERT表示在所有层中共同依赖于左右上下文。\n输入表示（input representation）\n论文的输入表示（input representation）能够在一个token序列中明确地表示单个文本句子或一对文本句子（例如， [Question, Answer]）。对于给定token，其输入表示通过对相应的token、segment和position embeddings进行求和来构造。图2是输入表示的直观表示：\n图2：BERT输入表示。输入嵌入是token embeddings, segmentation embeddings 和position embeddings 的总和。\n具体如下：\n使用WordPiece嵌入（Wu et al., 2016）和30,000个token的词汇表。用##表示分词。\n使用学习的positional embeddings，支持的序列长度最多为512个token。\n每个序列的第一个token始终是特殊分类嵌入（[CLS]）。对应于该token的最终隐藏状态（即，Transformer的输出）被用作分类任务的聚合序列表示。对于非分类任务，将忽略此向量。\n句子对被打包成一个序列。以两种方式区分句子。首先，用特殊标记（[SEP]）将它们分开。其次，添加一个learned sentence A嵌入到第一个句子的每个token中，一个sentence B嵌入到第二个句子的每个token中。\n对于单个句子输入，只使用 sentence A嵌入。\n关键创新：预训练任务\n与Peters et al. (2018) 和 Radford et al. (2018)不同，论文不使用传统的从左到右或从右到左的语言模型来预训练BERT。相反，使用两个新的无监督预测任务对BERT进行预训练。\n任务1: Masked LM\n从直觉上看，研究团队有理由相信，深度双向模型比left-to-right 模型或left-to-right and right-to-left模型的浅层连接更强大。遗憾的是，标准条件语言模型只能从左到右或从右到左进行训练，因为双向条件作用将允许每个单词在多层上下文中间接地“see itself”。\n为了训练一个深度双向表示（deep bidirectional representation），研究团队采用了一种简单的方法，即随机屏蔽（masking）部分输入token，然后只预测那些被屏蔽的token。论文将这个过程称为“masked LM”(MLM)，尽管在文献中它经常被称为Cloze任务(Taylor, 1953)。\n在这个例子中，与masked token对应的最终隐藏向量被输入到词汇表上的输出softmax中，就像在标准LM中一样。在团队所有实验中，随机地屏蔽了每个序列中15%的WordPiece token。与去噪的自动编码器（Vincent et al.， 2008）相反，只预测masked words而不是重建整个输入。\n虽然这确实能让团队获得双向预训练模型，但这种方法有两个缺点。首先，预训练和finetuning之间不匹配，因为在finetuning期间从未看到[MASK]token。为了解决这个问题，团队并不总是用实际的[MASK]token替换被“masked”的词汇。相反，训练数据生成器随机选择15％的token。例如在这个句子“my dog is hairy”中，它选择的token是“hairy”。然后，执行以下过程：\n数据生成器将执行以下操作，而不是始终用[MASK]替换所选单词：\n80％的时间：用[MASK]标记替换单词，例如，my dog is hairy → my dog is [MASK]\n10％的时间：用一个随机的单词替换该单词，例如，my dog is hairy → my dog is apple\n10％的时间：保持单词不变，例如，my dog is hairy → my dog is hairy. 这样做的目的是将表示偏向于实际观察到的单词。\nTransformer encoder不知道它将被要求预测哪些单词或哪些单词已被随机单词替换，因此它被迫保持每个输入token的分布式上下文表示。此外，因为随机替换只发生在所有token的1.5％（即15％的10％），这似乎不会损害模型的语言理解能力。\n使用MLM的第二个缺点是每个batch只预测了15％的token，这表明模型可能需要更多的预训练步骤才能收敛。团队证明MLM的收敛速度略慢于 left-to-right的模型（预测每个token），但MLM模型在实验上获得的提升远远超过增加的训练成本。\n任务2：下一句预测\n许多重要的下游任务，如问答（QA）和自然语言推理（NLI）都是基于理解两个句子之间的关系，这并没有通过语言建模直接获得。\n在为了训练一个理解句子的模型关系，预先训练一个二进制化的下一句测任务，这一任务可以从任何单语语料库中生成。具体地说，当选择句子A和B作为预训练样本时，B有50％的可能是A的下一个句子，也有50％的可能是来自语料库的随机句子。例如：\nInput = [CLS] the man went to [MASK] store [SEP]\nhe bought a gallon [MASK] milk [SEP]\nLabel = IsNext\nInput = [CLS] the man [MASK] to the store [SEP]\npenguin [MASK] are flight ##less birds [SEP]\nLabel = NotNext\n团队完全随机地选择了NotNext语句，最终的预训练模型在此任务上实现了97％-98％的准确率。\n实验结果\n如前文所述，BERT在11项NLP任务中刷新了性能表现记录！在这一节中，团队直观呈现BERT在这些任务的实验结果，具体的实验设置和比较请阅读原论文.\n图3：我们的面向特定任务的模型是将BERT与一个额外的输出层结合而形成的，因此需要从头开始学习最小数量的参数。在这些任务中，（a）和（b）是序列级任务，而（c）和（d）是token级任务。在图中，E表示输入嵌入，Ti表示tokeni的上下文表示，[CLS]是用于分类输出的特殊符号，[SEP]是用于分隔非连续token序列的特殊符号。\n图4：GLUE测试结果，由GLUE评估服务器给出。每个任务下方的数字表示训练样例的数量。“平均”一栏中的数据与GLUE官方评分稍有不同，因为我们排除了有问题的WNLI集。BERT 和OpenAI GPT的结果是单模型、单任务下的数据。所有结果来自https://gluebenchmark.com/leaderboard和https://blog.openai.com/language-unsupervised/\n图5：SQuAD 结果。BERT 集成是使用不同预训练检查点和fine-tuning seed的 7x 系统。\n图6：CoNLL-2003 命名实体识别结果。超参数由开发集选择，得出的开发和测试分数是使用这些超参数进行五次随机重启的平均值。\n四、BERT模型的影响\nBERT是一个语言表征模型（language representation model），通过超大数据、巨大模型、和极大的计算开销训练而成，在11个自然语言处理的任务中取得了最优（state-of-the-art, SOTA）结果。或许你已经猜到了此模型出自何方，没错，它产自谷歌。估计不少人会调侃这种规模的实验已经基本让一般的实验室和研究员望尘莫及了，但它确实给我们提供了很多宝贵的经验：\n深度学习就是表征学习 （Deep learning is representation learning）：\"We show that pre-trained representations eliminate the needs of many heavily engineered task-specific architectures\". 在11项BERT刷出新境界的任务中，大多只在预训练表征（pre-trained representation）微调（fine-tuning）的基础上加一个线性层作为输出（linear output layer）。在序列标注的任务里（e.g. NER），甚至连序列输出的依赖关系都先不管（i.e. non-autoregressive and no CRF），照样秒杀之前的SOTA，可见其表征学习能力之强大。\n规模很重要（Scale matters）：\"One of our core claims is that the deep bidirectionality of BERT, which is enabled by masked LM pre-training, is the single most important improvement of BERT compared to previous work\". 这种遮挡（mask）在语言模型上的应用对很多人来说已经不新鲜了，但确是BERT的作者在如此超大规模的数据+模型+算力的基础上验证了其强大的表征学习能力。这样的模型，甚至可以延伸到很多其他的模型，可能之前都被不同的实验室提出和试验过，只是由于规模的局限没能充分挖掘这些模型的潜力，而遗憾地让它们被淹没在了滚滚的paper洪流之中。\n预训练价值很大（Pre-training is important）：\"We believe that this is the first work to demonstrate that scaling to extreme model sizes also leads to large improvements on very small-scale tasks, provided that the model has been sufficiently pre-trained\". 预训练已经被广泛应用在各个领域了（e.g. ImageNet for CV, Word2Vec in NLP），多是通过大模型大数据，这样的大模型给小规模任务能带来的提升有几何，作者也给出了自己的答案。BERT模型的预训练是用Transformer做的，但我想换做LSTM或者GRU的话应该不会有太大性能上的差别，当然训练计算时的并行能力就另当别论了。\n对BERT模型的观点\n\n0. high-performance的原因其实还是归结于两点，除了模型的改进，更重要的是用了超大的数据集（BooksCorpus 800M + English Wikipedia 2.5G单词）和超大的算力（对应于超大模型）在相关的任务上做预训练，实现了在目标任务上表现的单调增长\n1. 这个模型的双向和Elmo不一样，大部分人对他这个双向在novelty上的contribution 的大小有误解，我觉得这个细节可能是他比Elmo显著提升的原因。Elmo是拼一个左到右和一个右到左，他这个是训练中直接开一个窗口，用了个有顺序的cbow。\n2. 可复现性差：有钱才能为所欲为（Reddit对跑一次BERT的价格讨论）\nFor TPU pods: 4 TPUs * ~$2/h (preemptible) * 24 h/day * 4 days = $768 (base model) 16 TPUs = ~$3k (large model) For TPU: 16 tpus * $8/hr * 24 h/day * 4 days = 12k 64 tpus * $8/hr * 24 h/day * 4 days = 50k\n最后他问到：For GPU:\"BERT-Large is 24-layer, 1024-hidden and was trained for 40 epochs over a 3.3 billion word corpus. So maybe 1 year to train on 8 P100s? \" ，然后这个就很interesting了。\n参考文献\n1. 知乎：如何评价谷歌最新的BERT模型\n2. 华尔街见闻：NLP历史突破\n3. OPENAI-Improving Language Understanding with Unsupervised Learning\n4. https://gluebenchmark.com/leaderboard","data":"2018年10月15日 17:49:18"}
{"_id":{"$oid":"5d36b9806734bd8e681d6303"},"title":"未来数据领域的珠穆朗玛峰之中文自然语言处理","author":"Soyoger","content":"人工智能或许是人类最美好的梦想之一。追溯到公元前仰望星空的古希腊人，当亚里士多德为了解释人类大脑的运行规律而提出了联想主义心理学的时候，他恐怕不会想到，两千多年后的今天，人们正在利用联想主义心理学衍化而来的人工神经网络，构建的超级人工智能成为最能接近梦想的圣境，并一次又一次地挑战人类大脑认知的极限。\n在以大数据、云计算为背景的技术框架支撑下，互联网发展极为迅速，过去一个技术或者行业热点从诞生到消亡需要几年乃至更长的时间，但是最近几年，其生命周期在不断缩短，大多数的热点从产生到消亡只需要1-2年，有些仅仅是半年甚至几个月的时间。互联网行业越来越凸显出快鱼吃慢鱼的特点。从技术本身也有体现，比如2012-2014年是移动互联网的热潮，安卓和ios APP开发工程师当时非常流行。随后，2015大数据、云计算之年，2016年后大数据时代，2017年被称为人工智能元年，2018年炒得最火的是区块链和币圈。在互联网以这种迅雷不及掩耳之势的发展速度下，作为初学者就很容易被各种技术概念迷惑，找不到自己想要的突破口和深入的领域，即便是计算机从业者有时候也分不清到底如何定位自己未来的技术方向。\n下面，我们先从中国互联网的发展历程说起。\n从1994诞生（加入国际互联网）到现在才短短的24年，就在这24年里，我们经历了4次非同凡响、一次比一次更彻底的发展大高潮。\n第一次互联网大浪潮（1994年—2000年），以四大门户和搜索为代表，能做网站的工程师就可以被称为技术牛人；第二次互联网大浪潮（2001年—2008年），从搜索到PC端社交化网络的发展，我们的社交形态发生了根本的变化，从线下交流正转变为线上交流，大量的数据开始生成；第三次互联网大浪潮（2009年—2014年）PC端互联网到移动互联网，此时各种APP如雨后春笋般的冒出来，尽管后来有很多APP都死了，但是移动互联网几乎颠覆了整个中国老百姓个人生活和商业形态，改变着我们每一个人的生活、消费、社交、出行方式等。\n那第四次是什么呢？没错，第四次互联网大浪潮（2015—至今），是在前3次发展基础上，以大数据、云计算为背景发展起来的人工智能技术革命，分布式计算让大数据处理提速，而昔日陨落的巨星深度学习此刻再次被唤醒，并很快在图像和语音方面取得重大突破，但在自然语言方面却显得有些暗淡，突破并不是很大。尽管有很多人都去从事计算机视觉、语音等方面的工作，但随着AI的继续发展，在NLP方向显得越来越重要。\n接着，我们总结一下数据领域成就和挑战\n有一个不可否认的事实，当前从事互联网的人们已经制造出了海量的数据，未来还将继续持续，其中包括结构化数据、半结构化和非结构化数据。笔者发现，对于结构化数据而言，在大数据、云计算技术“上下齐心”的大力整合下，其技术基本趋向成熟和稳定，比如关系型数据库以及基于Hadoop的HDFS分布式文件系统、Hive数据仓库和非关系型数据库Hbase，以及Elasticsearch集群等数据存储的关系数据库或者NoSql，可以用来管理和存储数据；基于MapReduce、Spark和Storm、Flink等大数据处理框架可以分别处理离线和实时数据等。而半结构化、非结构化的数据，除了以ELK为代表的日志处理流程，过去在其它限定领域基于规则和知识库也取得了一定的成果，因其自身的复杂性，未来更多领域应用都具有很大的困难和挑战。\n最后，我们看看国内外人工智能领域的工业现状\n今年5月19日有幸在北京国家会议中心参加了2018全球人工智能技术大会（GAITC）。在大会上，从中国科学院院士姚期智提出人工智能的新思维开始，其重点讲述了人工神经网络为代表的深度学习以及量子计算机将是未来发展的新思维；紧接着中国工程院院士李德毅分享了路测的学问-无人驾驶的后图灵测试，提出未来无人驾驶挑战应该是让无人驾驶具有司机的认知、思维和情感，而不是当前以GPS定位和动力学方面解决无人驾驶的问题；接下来微软全球资深副总裁王永东向我们展示的微软小冰，大家一起见证了微软小冰在社交互动、唱歌、作诗、节目主持和情感方面不凡的表现，而本人也真实测试了一下，小冰现在的表现已经非常优秀了，可以作诗、唱歌、聊天、节目主持等。然而要达到一个成年自然人的水平，在某些方面还不能完全表现出人的特性。下面这幅图是微软小冰的个人介绍，有兴趣可以在微信公众号关注小冰，进行体验。\n\n\n人工智能产业的快速发展，资本市场大量资金涌入，促使中国人工智能领域投融资热度快速升温。充分表明资本市场对于人工智能发展前景的认可。《2018年人工智能行业创新企业Top100》发布，据榜单显示：进入2018年人工智能行业创新企业前十名的企业分别是：百度、阿里云、美图秀秀、华大基因、科大讯飞、微鲸科技、华云数据、爱驰亿维、青云、七牛云。作为人工智能的一个重要组成部分，自然语言处理（NLP）的研究对象是计算机和人类语言的交互，其任务是理解人类语言并将其转换为机器语言。在目前的商业场中，NLP技术用于分析源自邮件、音频、文件、网页、论坛、社交媒体中的大量半结构化和非结构化数据，市场前景巨大。\n为什么说未来数据领域的珠穆朗玛峰是中文自然语言处理？\n正是基于上面对中国互联网发展的总结，对当前数据领域所面临的挑战以及资本市场对人工智能的认可分析，未来数据领域的重点是自然语言处理技术及其在智能问答、情感分析、语义理解、知识图谱等应用方面的突破。对于我们国内中文来说，如何更好的把前面所说的应用在中文处理上，显得更为重要和急迫，所以笔者认为未来数据领域的珠穆朗玛峰是中文自然语言处理 。\n作为初学者，我们目前面又临这样的尴尬，网上大部分自然语言处理内容都是英文为基础，大多数人先是学好了英语的处理，回头来再处理中文，却发现有很大的不同，这样不仅让中文自然语言处理学习者走了弯路，也浪费了大量时间和精力。中文的处理比英文复杂的多，网上中文相关资料少之又少，国内纯中文自然语言处理书籍只有理论方面的，却在实战方面比较空缺，这让中文自然语言处理的研究开发工作感到举步维艰，很难下笔。","data":"2018年06月14日 12:49:03"}
{"_id":{"$oid":"5d36b99c6734bd8e681d630e"},"title":"自然语言处理(NLP)之Word Embedding","author":"l_r_h000","content":"最近做完UNIT一个小项目后，结合同时期看KBQA的文章，对NLP/NLU方向产生了比较大的兴趣，想深入学习一下，结合一篇综述Recent Trends in Deep Learning Based Natural Language Processing（参考文献[5]为其阅读笔记）的阐述顺序，把相关的知识补一补，本文即第一部分Word Embedding。\n主要参考文献：\n[1] word2vec 中的数学原理详解\n[2] Word Embedding与Word2Vec\n[3] 自然语言处理中的N-Gram模型详解\n[4] 有谁可以解释下word embedding?——知乎\n[5] 2017-基于DL的NLP研究近况\n目录\n一、Word Embedding概述\n二、Word2vec之前\n2.1 one-hot\n2.2 n-gram\n2.3 co-occurrence matrix\n2.4 NLM\n三、Word2vec\n3.1 CBOW\n3.1.1 基于Hierarchical Softmax\n3.1.2 基于Negative Sampling\n3.2 Skip-gram\n3.2.1 基于Hierarchical Softmax\n3.2.2 基于Negative Sampling\n一、Word Embedding概述\n简单来说，词嵌入（Word Embedding）或者分布式向量（Distributional Vectors）是将自然语言表示的单词转换为计算机能够理解的向量或矩阵形式的技术。由于要考虑多种因素比如词的语义（同义词近义词）、语料中词之间的关系（上下文）和向量的维度（处理复杂度）等等，我们希望近义词或者表示同类事物的单词之间的距离可以理想地近，只有拿到很理想的单词表示形式，我们才更容易地去做翻译、问答、信息抽取等进一步的工作。\n在Word Embedding之前，常用的方法有one-hot、n-gram、co-occurrence matrix，但是他们都有各自的缺点，下面会说明。2003年，Bengio提出了NLM，是为Word Embedding的想法的雏形，而在2013年，Mikolov对其进行了优化，即Word2vec，包含了两种类型，Continuous Bag-of-Words Model 和 skip-gram model。\nWord Embedding是基于分布式假设(distributional hypothesis)：\n总的来说，word embedding就是一个词的低维向量表示（一般用的维度可以是几十到几千）。有了一个词的向量之后，各种基于向量的计算就可以实施，如用向量之间的相似度来度量词之间的语义相关性。其基于的分布式假设就是出现在相同上下文(context)下的词意思应该相近。所有学习word embedding的方法都是在用数学的方法建模词和context之间的关系。\n\n作者：李明磊9527\n链接：https://www.zhihu.com/question/32275069/answer/197721342\n来源：知乎\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n但是Word Embedding也有其局限性， 比如：\n难以对词组做分布式表达\n受限于上下文window的尺寸，有些词（例如好或坏）的上下文可能没什么不同甚至完全一样，这对情感分析任务的影响非常大\n此外，Word Embedding对于应用场景的依赖很强，所以针对特殊的应用场景可能需要重新训练，这样就会很消耗时间和资源，为此Bengio提出了基于负采样（negative sampling）的模型。\n下面本文会将对Word2vec之前的常用方法和Word2vec的两种模型做比较详细的记录和理解。\n二、Word2vec之前\n2.1 one-hot\none-hot是最简单的一种处理方式。通俗地去讲，把语料中的词汇去重取出，按照一定的顺序（字典序、出现顺序等）排列为词汇表，则每一个单词都可以表示为一个长度为N的向量，N为词汇表长度，即单词总数。该向量中，除了该词所在的分量为1，其余均置为0。\n例如，有语料库如下：\nJohn likes to watch movies. Mary likes movies too.\nJohn also likes to watch football games.\n假设我们的词汇表排序结果如下：\n{\"John\": 1, \"likes\": 2, \"to\": 3, \"watch\": 4, \"movies\": 5, \"also\":6, \"football\": 7, \"games\": 8, \"Mary\": 9, \"too\": 10}\n那么则有如下word的向量表示：\nJohn: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nlikes: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n……\n用这样的方式可以利用向量相加进一步表示句子和文本了，但是one-hot有很大的局限性：\n语义的相似性，“woman”、“madam”、“lady”从语义上将可能是相近的，one-hot无法表示\n英语单词中的复数时态，我们不会在排序是就把同一单词的不同形态区别开来，继而再进行向量表示\n单词之间的位置关系，很多时候句内之间多个单词（比如术语）会同时出现多次，one-hot无法表示\n词向量长度很大，一方面2的原因，另一方面本身大规模语料所含的词数很多，处理会很棘手\n2.2 n-gram\nn-gram可以表示单词间的位置关系所反映的语义关联，在说明n-gram之前，我们从最初的句子概率进行推导。\n假设一个句子S为n个单词有序排列，记为：\n我们将其简记为 ，则这个句子的概率为：\n对于单个概率意思为该单词在前面单词给定的情况下出现的概率，我们利用贝叶斯公式可以得到：\n其中最后一项为在语料中出现的频数。但是长句子或者经过去标点处理后的文本可能很长，而且太靠前的词对于词的预测影响不是很大，于是我们利用马尔可夫假设，取该词出现的概率仅依赖于该词前面的n-1个词，这就是n-gram模型的思想。\n所以上面的公式变为：\n在这里，我们不对n的确定做算法复杂度上的讨论，详细请参考文献[1]，一般来说，n取3比较合适。此外对于一些概率为0的情况所出现的稀疏数据，采用平滑化处理，此类算法很多，以后有时间再具体展开学习。\n所以n-gram的主要工作在于确定n之后，对语料中的各种吃词串进行频数统计和平滑化处理，对于所需要的句子概率，只要将之前语料中相关概率取出计算就可以了。\n当然实际情况是对做最优化处理，参数确定后以后的概率就可以通过函数确定了，这就需要构造函数，后面的NLM就是做这个工作。\nn-gram模型会将前文的语义关联纳入考虑，从而形成联合分布概率表达，但是尽管去前n-1个单词，语料大的情况下计算量还是很大，在模拟广义情境时严重受到了“维度灾难（curse of dimensionality）”。\n2.3 co-occurrence matrix\n共现矩阵也是考虑语料中词之间的关系来表示：\n一个非常重要的思想是，我们认为某个词的意思跟它临近的单词是紧密相关的。这是我们可以设定一个窗口（大小一般是5~10），如下窗口大小是2，那么在这个窗口内，与rests 共同出现的单词就有life、he、in、peace。然后我们就利用这种共现关系来生成词向量。\n例如，现在我们的语料库包括下面三份文档资料：\nI like deep learning.\nI like NLP.\nI enjoy flying.\n作为示例，我们设定的窗口大小为1，也就是只看某个单词周围紧邻着的那个单词。此时，将得到一个对称矩阵——共现矩阵。因为在我们的语料库中，I 和 like做为邻居同时出现在窗口中的次数是2，所以下表中I 和like相交的位置其值就是2。这样我们也实现了将word变成向量的设想，在共现矩阵每一行（或每一列）都是对应单词的一个向量表示。\n虽然Cocurrence matrix一定程度上解决了单词间相对位置也应予以重视这个问题。但是它仍然面对维度灾难。也即是说一个word的向量表示长度太长了。这时，很自然地会想到SVD或者PCA等一些常用的降维方法。当然，这也会带来其他的一些问题。\n窗口大小的选择跟n-gram中确定n也是一样的，窗口放大则矩阵的维度也会增加，所以本质上还是带有很大的计算量，而且SVD算法运算量也很大，若文本集非常多，则不具有可操作性。\n2.4 NLM\n神经语言模型（Neural Language Model）是Word Embeddings的基本思想，在很多其他文献中也有神经概率语言模型（Neural Probabilistic Language Model，NPLM）或者神经网络语言模型（Neural Network Language Model，NNLM），都是指一个东西。\nNLM的输入是词向量，根据参考文献[1]，词向量和模型参数（最终的语言模型）可以通过神经网络训练一同得到。相比于n-gram通过联合概率考虑词之间的位置关系，NLM则是利用词向量进一步表示词语之间的相似性，比如近义词在相似的上下文里可以替代，或者同类事物的词可以在语料中频数不同的情况下获得相近的概率。结合参考文献[1]，举一个简单例子：\n在一个语料C中，S1=“A dog is sitting in the room.”共出现了10000次，S2=\"A cat is sitting in the room\"出现了1次，按照n-gram的模型，当我们输入“A _____ is sitting in the room”来预测下划线上应该填入的词时，dog的概率会远大于cat，这是针对于语料C得到的概率。但是我们希望相似含义的词在目标向量空间中的距离比不相关词的距离更近，比如v(man)-v(woman)约等于v(gentleman)-v(madam)，用这样生成的词向量或者已经训练好的模型在去做翻译、问答等后续工作时，就会很有效果，而NLM利用词向量表示就能达到这样的效果。\n注：在参考文献[1]中，作者举的例子是从句子概率角度，我自己的理解稍有不同，将原例放在下面：\nNLM的神经网络训练样本同n-gram的取法，取语料中任一词w的前n-1个词作为Context(w)，则（Context(w)，w）就是一个训练样本了。这里的每一个词都被表示为一个长度为L的词向量，然后将Context(w)的n-1个词向量首位连接拼成（n-1）L的长向量。下面为NLM图解：\n【注】此图向量和矩阵的维度与参考文献中相反了\n我们得到的输出结果为长度为词汇总数的向量，如果想要第i个分量去表示当上下为context(w)时下一个词为词典中第i个词的概率，还需要softmax归一化，然后我们最初想要的结果便是：\n注意：这只是取一个词w后输出的向量y，我们需要的就是通过训练集所有的词都做一遍这个过程来优化得到理想的W，q和U，b。\n那么样本中最初的词向量如何获得呢？在参考文献[1]中有这样两段话：\n目前我还没有彻底搞懂神经网络中具体的机制，所以暂时标记一下，初步推测是初始化一个矩阵或者可以粗暴地用one-hot（不过这样输入层的L=D，计算量大了很多），然后随着训练的过程，词向量也是不断更新的，详细还要参考最优化理论。\n下面要说的Word2vec便是在NLM基础上的优化。\n三、Word2vec\n目前学习了解到的Word2vec有基于Hierarchical Softmax和基于Negative Sampling两种方式，参考文献[1]是从两种方式分别讲解了CBOW和Skip-gram的数学构建思路和过程，由于这两个模型是相反的过程，即CBOW是在给定上下文基础上预测中心词，Skip-gram在有中心词后预测上下文，我个人是把两个模型按照两种不同的计算方法做了梳理，当然数学推导还是一样的，只不过我自己看起来更舒服。在此再次感谢@peghoty大牛的详解。\n3.1 CBOW\n基于前面的介绍，CBOW的思想是取目标词w的上下文（前后相邻词）而不是仅之前的词作为预测前提，类似于共现矩阵的窗口，不同于NLM的是，Context(w)的向量不再是前后连接，而是求和，我们记为，此外还将NLM的隐藏层去掉了。当然最大的区别还是在输出层，基于Hierarchical Softmax的CBOW输出层为一颗霍夫曼树，叶子节点为语料中的词汇，构建依据便是各词的出现频数；基于Negative Sampling则是用随机负采样代替霍夫曼树的构建。\n3.1.1 基于Hierarchical Softmax\n霍夫曼树的构建在这里就不展开说了，比较简单的算法。沿用文献[1]的表示，基于Hierarchical Softmax的CBOW所要构建的霍夫曼树所需参数如下：\n：从根结点到w对应结点的路径\n：路径上包含结点个数\n：到w路径上的的结点\n：结点编码，根结点不编码\n：非叶子结点（包括根结点）对应的向量\n霍夫曼树构建按照频数大小有左右两种，其实都是自己约定的，在这里就不麻烦了，构建后左结点编码为0，为正类，右结点为1，为负类。\n根据逻辑回归，一个结点被分为正类的概率为\n的一些性质，后面用的到：\n所以之前我们要构造的目标函数就可以写为以下形式：\n这个公式跟之前看的概率图模型有点像，不过现在有点记不清了，后面我再梳理一下，看看能不能串起来。其中\n整体表达式\n这是一个单词，我们把对连乘做对数似然函数，然后将语料中所有单词都求和，则目标函数如下：\n明确参数有和，我们取其中子式来做关于两个参数的梯度：\n因为和是对称的，所以的为：\n所以两者就可以更新了:\n至此，我们完成了对参数的优化。\n参考文献[1]提出了这样一个问题：\n我的理解是可以的，可能我对最优化方法的学习还不够全面，从公式拆解上好像更能说的过去，但是对于收敛速度的影响可能会很大，取平均可能优化得到较好的结果较慢。\n3.1.2 基于Negative Sampling\n对于大规模语料，构建霍夫曼树的工作量是巨大的，而且叶子节点为N的霍夫曼数需要新添(N-1)个结点，而随着树的深度增加，参数计算的量也会增加很多很多，得到的词向量也会不够好，为此，Mikolov作出了优化，将构建霍夫曼树改为随机负采样方法。\n对于给定的上下文Context(w)去预测w，如果从语料中就是存在（Context(w),w），那么w就是正样本，其他词就是负样本。\n我们设负样本集为，词的标签：\n即正样本标签为1，负样本标签为0，等同于霍夫曼结点的左右编码，只不过与其取值相反，这样后面的公式也就很好理解了：\n同样,我们对两个参数求导：\n然后更新参数，公式形式是一样的，不再写了。\n可见，对于单词w，基于Hierarchical Softmax将其频数用来构建霍夫曼树，正负样本标签取自结点左右编码；而基于Negative Sampling将其频数作为随机采样线段的子长度，正负样本标签取自从语料中随机取出的词是否为目标词，构造复杂度小于前者。\n3.2 Skip-gram\n由于Skip-gram是CBOW的相反操作，输入输出稍有不同，在这里仅贴出关键公式，不再具体说明。\n3.2.1 基于Hierarchical Softmax\n以上均来自参考文献[1]，变量表示稍有不同。\n3.2.2 基于Negative Sampling\n这个作者分析较多，还没有完全看懂，后面再补","data":"2018年08月03日 17:26:16"}
{"_id":{"$oid":"5d36ba1a6734bd8e681d633b"},"title":"自然语言处理(NLP)入门指南资料","author":"打怪的蚂蚁","content":"作者：Melanie Tosik\n翻译：闵黎\n校对：丁楠雅\nMelanie Tosik目前就职于旅游搜索公司WayBlazer，她的工作内容是通过自然语言请求来生产个性化旅游推荐路线。回顾她的学习历程，她为期望入门自然语言处理的初学者列出了一份学习资源清单。\ndisplaCy网站上的可视化依赖解析树\nhttps://demos.explosion.ai/displacy/?text=Great%2C%20this%20is%20just%20what%20I%20needed!\u0026model=en\u0026cpu=1\u0026cph=0\n记得我曾经读到过这样一段话，如果你觉得有必要回答两次同样的问题，那就把答案发到博客上，这可能是一个好主意。根据这一原则，也为了节省回答问题的时间，我在这里给出该问题的标准问法：“我的背景是研究**科学，我对学习NLP很有兴趣。应该从哪说起呢？”\n在您一头扎进去阅读本文之前，请注意，下面列表只是提供了非常通用的入门清单（有可能不完整）。 为了帮助读者更好地阅读，我在括号内添加了简短的描述并对难度做了估计。最好具备基本的编程技能（例如Python）。\n\n\n在线课程\n\n\n•  Dan Jurafsky 和 Chris Manning：自然语言处理[非常棒的视频介绍系列]\nhttps://www.youtube.com/watch?v=nfoudtpBV68\u0026list=PL6397E4B26D00A269\n•  斯坦福CS224d：自然语言处理的深度学习[更高级的机器学习算法、深度学习和NLP的神经网络架构]\nhttp://cs224d.stanford.edu/syllabus.html\n•  Coursera：自然语言处理简介[由密西根大学提供的NLP课程]\nhttps://www.coursera.org/learn/natural-language-processing\n\n\n图书馆和开放资源\n\n\n•  spaCy（网站，博客）[Python; 新兴的开放源码库并自带炫酷的用法示例、API文档和演示应用程序]\n网站网址：https://spacy.io/\n博客网址：https://explosion.ai/blog/\n演示应用网址: https://spacy.io/docs/usage/showcase\n•  自然语言工具包（NLTK）（网站，图书）[Python; NLP实用编程介绍，主要用于教学目的]\n网站网址：http://www.nltk.org\n图书网址: http://www.nltk.org/book/\n•  斯坦福CoreNLP（网站）[由Java开发的高质量的自然语言分析工具包]\n网站网址: https://stanfordnlp.github.io/CoreNLP/\n\n\n活跃的博客\n\n\n•  自然语言处理博客（HalDaumé）\n博客网址：https://nlpers.blogspot.com/\n•  Google研究博客\n博客网址：https://research.googleblog.com/\n•  语言日志博客（Mark Liberman）\n\n博客网址：http://languagelog.ldc.upenn.edu/nll/\n\n\n书籍\n\n\n•  言语和语言处理（Daniel Jurafsky和James H. Martin）[经典的NLP教科书，涵盖了所有NLP的基础知识，第3版即将出版]\nhttps://web.stanford.edu/~jurafsky/slp3/\n• 统计自然语言处理的基础（Chris Manning和HinrichSchütze）[更高级的统计NLP方法]\nhttps://nlp.stanford.edu/fsnlp/\n•  信息检索简介（Chris Manning，Prabhakar Raghavan和HinrichSchütze）[关于排名/搜索的优秀参考书]\nhttps://nlp.stanford.edu/IR-book/\n•  自然语言处理中的神经网络方法（Yoav Goldberg）[深入介绍NLP的NN方法，和相对应的入门书籍]\nhttps://www.amazon.com/Network-Methods-Natural-Language-Processing/dp/1627052984\n入门书籍： http://u.cs.biu.ac.il/~yogo/nnlp.pdf\n\n\n其它杂项\n\n\n•  如何在TensorFlow中构建word2vec模型[学习指南]\nhttps://www.tensorflow.org/versions/master/tutorials/word2vec/index.html\n•  NLP深度学习的资源[按主题分类的关于深度学习的顶尖资源的概述]\nhttps://github.com/andrewt3000/dl4nlp\n•  最后一句话：计算语言学和深度学习——论自然语言处理的重要性。（Chris Manning）[文章]\nhttp://mitp.nautil.us/article/170/last-words-computational-linguistics-and-deep-learning\n•  对分布式表征的自然语言的理解（Kyunghyun Cho）[关于NLU的ML / NN方法的独立讲义]\nhttps://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf\n•  带泪水的贝叶斯推论（Kevin Knight）[教程工作簿]\nhttp://www.isi.edu/natural-language/people/bayes-with-tears.pdf\n•  国际计算语言学协会（ACL）[期刊选集]\nhttp://aclanthology.info/\n•  果壳问答网站(Quora)：我是如何学习自然语言处理的？\nhttps://www.quora.com/How-do-I-learn-Natural-Language-Processing\n\n\nDIY项目和数据集\n\n\n\n\n资料来源：http://gunshowcomic.com/\n•  Nicolas Iderhoff已经创建了一份公开的、详尽的NLP数据集的列表。除了这些，这里还有一些项目，可以推荐给那些想要亲自动手实践的NLP新手们：\n数据集：https://github.com/niderhoff/nlp-datasets\n•  基于隐马尔可夫模型（HMM）实现词性标注（POS tagging）.\nhttps://en.wikipedia.org/wiki/Part-of-speech_tagging\nhttps://en.wikipedia.org/wiki/Hidden_Markov_model\n•  使用CYK算法执行上下文无关的语法解析\nhttps://en.wikipedia.org/wiki/CYK_algorithm\nhttps://en.wikipedia.org/wiki/Context-free_grammar\n•  在文本集合中，计算给定两个单词之间的语义相似度，例如点互信息（PMI，Pointwise Mutual Information）\nhttps://en.wikipedia.org/wiki/Semantic_similarity\nhttps://en.wikipedia.org/wiki/Pointwise_mutual_information\n•  使用朴素贝叶斯分类器来过滤垃圾邮件\nhttps://en.wikipedia.org/wiki/Naive_Bayes_classifier\nhttps://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering\n•  根据单词之间的编辑距离执行拼写检查\nhttps://en.wikipedia.org/wiki/Spell_checker\nhttps://en.wikipedia.org/wiki/Edit_distance\n•  实现一个马尔科夫链文本生成器\nhttps://en.wikipedia.org/wiki/Markov_chain\n•  使用LDA实现主题模型\nhttps://en.wikipedia.org/wiki/Topic_model\nhttps://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\n•  使用word2vec从大型文本语料库，例如维基百科，生成单词嵌入。\nhttps://code.google.com/archive/p/word2vec/\nhttps://en.wikipedia.org/wiki/Wikipedia:Database_download\n\n\nNLP在社交媒体上\n\n\n•  Twitter：#nlproc，NLPers上的文章列表（由Jason Baldrige提供）\nhttps://twitter.com/hashtag/nlproc\nhttps://twitter.com/jasonbaldridge/lists/nlpers\n•  Reddit 社交新闻站点：/r/LanguageTechnology\nhttps://www.reddit.com/r/LanguageTechnology\n•  Medium发布平台：Nlp\n\nhttps://medium.com/tag/nlp\n原文链接：\nhttps://medium.com/towards-data-science/how-to-get-started-in-nlp-6a62aa4eaeff","data":"2017年08月14日 10:33:22"}
{"_id":{"$oid":"5d36ba396734bd8e681d634b"},"title":"语音识别 自然语言处理","author":"QFR璠璠瑜","content":"参考：《中文信息处理发展报告2016》\n什么是语音识别？\n语音识别(Automatic Speech Recognition,ASR)：利用计算机实现从语音到文字自动转换的任务。\n语音识别的技术有哪些？\n语音识别技术 = 早期基于信号处理和模式识别 + 机器学习 + 深度学习 + 数值分析+ 高性能计算 + 自然语言处理\n语音识别技术的发展可以说是有一定的历史背景，上世纪80年代，语音识别研究的重点已经开始逐渐转向大词汇量、非特定人连续语音识别。到了90年代以后，语音识别并没有什么重大突破，直到大数据与深度神经网络时代的到来，语音识别技术才取得了突飞猛进的进展。\n语音识别的相关领域有哪些？\n语音识别关联领域 = 自然语言理解 + 自然语言生成 + 语音合成\n语音识别的社会价值在哪里？\n语音信号是典型的局部稳态时间序列，而日常所见的大量信号都属于这种局部稳态时间序列信号，如视频，雷达信号，金融资产价格，经济数据等。这些信号的共同特点是在抽象的时间序列中包含大量不同层次的信息，可以用相似的模型进行分析。\n历史上，语音信号的研究成果在若干领域起到启发作用，如语音信号处理中的隐马尔科夫模型在金融分析，机械控制等领域都得到广泛的应用。近年来，深度神经网络在语音识别领域的巨大成功直接促进了各种深度学习模型在自然语言处理，图形图像处理，知识推理等众多领域的发展应用，取得了一个有一个令人惊叹的成果。\n怎么构建语音识别系统？\n语音识别系统构建总体包括两个部分：训练和识别。\n训练通常来讲都是离线完成的，将海量的未知语音通过话筒变成信号之后加在识别系统的输入端，经过处理后再根据语音特点建立模型，对输入的信号进行分析，并提取信号中的特征，在此基础上建立语音识别所需的模板。\n识别则通常是在线完成的，对用户实时语音进行自动识别。这个过程又基本可以分为“前端”和“后端”两个模块。前端主要的作用就是进行端点检测、降噪、特征提取等。后端的主要作用是利用训练好的“声音模型”和“语音模型”对用户的语音特征向量进行统计模式识别，得到其中包含的文字信息。\n语音识别技术中的关键问题是什么？\n语音特征抽取\n语音识别的一个主要困难在于语音信号的复杂性和多变性。一段看似简单的语音信号， 其中包含了说话人、发音内容、信道特征、口音方言等大量信息。不仅如此，这些底层信息互相组合在一起，又表达了如情绪变化、语法语义、暗示内涵等丰富的高层信息。如此众多 的信息中，仅有少量是和语音识别相关的，这些信息被淹没在大量其它信息中，因此充满了变动性。语音特征抽取即是在原始语音信号中提取出与语音识别最相关的信息，滤除其它无关信息。\n语音特征抽取的原则是：尽量保留对发音内容的区分性，同时提高对其它信息变量的鲁棒性。历史上研究者通过各种物理学、生理学、心理学等模型构造出各种精巧的语音特征抽 取方法，近年来的研究倾向于通过数据驱动学习适合某一应用场景的语音特征。\n模型构建\n语音识别中的建模包括声学建模和语言建模。声学建模是对声音信号（语音特征）的特性进行抽象化。自上世纪 70 年代中期以来，声学模型基本上以统计模型为主，特别是隐马尔科夫模型/高斯混合模型(HMM/GMM)结构。最近几年，深度神经网络(DNN)和各种异构神经 网络成为声学模型的主流结构。\n声学模型需要解决如下几个基本问题： 如何描述语音信号的短时平稳性；\n 如何描述语音信号在某一平稳瞬态的静态特性，即特征分布规律；\n 如何应用语法语义等高层信息；\n 如何对模型进行优化，即模型训练。\n同时，在实际应用中，还需要解决众多应用问题，例如：\n 如何从一个领域快速自适应到另一个领域；\n 如何对噪音、信道等非语音内容进行补偿；\n 如何利用少量数据建模；\n 如何提高对语音内容的区分性；\n 如何利用半标注或无标注数据，等等。 语言建模是对语言中的词语搭配关系进行归纳，抽象成概率模型。这一模型在解码过程中对解码空间形成约束，不仅减小计算量，而且可以提高解码精度。传统语言模型多基于 N元文法 (n-gram)，近年来基于递归神经网络（RNN）的语言模型发展很快，在某些识别任务 中取得了比 n-gram 模型更好的结果。\n语言模型要解决的主要问题是如何对低频词进行平滑。不论是 n-gram 模型还是 RNN 模型，低频词很难积累足够的统计量，因而无法得到较好的概率估计。平滑方法借用高频词或相似词的统计量，提高对低频词概率估计的准确性。除此之外，语言建模研究还包括：\n 如何对字母、字、词、短语、主题等多层次语言单元进行多层次建模\n 如何对应用领域进行快速自适应；\n 如何提高训练效率，特别是对神经网络模型来说，提高效率尤为重要；\n 如何有效利用大量噪声数据，等等。\n解码\n解码是利用语音模型和语言模型中积累的知识，对语音信号序列进行推理，从而得到相应语音内容的过程。早期的解码器一般为动态解码，即在开始解码前，将各种知识源以独立模块形式加载到内存中，动态构造解码图。现代语音识别系统多采用静态解码，即将各种知 识源统一表达成有限状态转移机（FST），并将各层次的 FST 嵌套组合在一起，形成解码图。 解码时，一般采用 Viterbi 算法在解码图中进行路径搜索。为加快搜索速度，一般对搜索路 径进行剪枝，保留最有希望的路径，即束搜索（beam search）。\n对解码器的研究包括但不限于如下内容：\n 如何加快解码速度，特别是在应用神经网络语言模型进行一遍解码时；\n 如何实现静态解码图的动态更新，如加入新词；\n 如何利用高层语义信息；\n 如何估计解码结果的信任度；\n 如何实现多语言和混合语言解码；\n 如何对多个解码器的解码结果进行融合。","data":"2018年07月18日 18:59:30"}
{"_id":{"$oid":"5d36ba466734bd8e681d6353"},"title":"《统计自然语言处理》知识结构总结","author":"miner_zhu","content":"一、自然语言处理概述\n1）自然语言处理：利用计算机为工具，对书面实行或者口头形式进行各种各样的处理和加工的技术，是研究人与人交际中以及人与计算机交际中的演员问题的一门学科，是人工智能的主要内容。\n2）自然语言处理是研究语言能力和语言应用的模型，建立计算机（算法）框架来实现这样的语言模型，并完善、评测、最终用于设计各种实用系统。\n3）研究问题（主要）：\n信息检索\n机器翻译\n文档分类\n问答系统\n信息过滤\n自动文摘\n信息抽取\n文本挖掘\n舆情分析\n机器写作\n语音识别\n研究模式：\n自然语言场景问题，数学算法，算法如何应用到解决这些问题，预料训练，相关实际应用\n自然语言的困难：\n场景的困难：语言的多样性、多变性、歧义性\n学习的困难：艰难的数学模型（hmm,crf,EM,深度学习等）\n语料的困难：什么的语料？语料的作用？如何获取语料？\n二、形式语言与自动机\n语言：按照一定规律构成的句子或者字符串的有限或者无限的集合。\n描述语言的三种途径：\n穷举法\n文法（产生式系统）描述\n自动机\n自然语言不是人为设计而是自然进化的，形式语言比如：运算符号、化学分子式、编程语言\n形式语言理论朱啊哟研究的是内部结构模式这类语言的纯粹的语法领域，从语言学而来，作为一种理解自然语言的句法规律，在计算机科学中，形式语言通常作为定义编程和语法结构的基础\n形式语言与自动机基础知识：\n集合论\n图论\n自动机的应用：\n1，单词自动查错纠正\n2，词性消歧（什么是词性？什么的词性标注？为什么需要标注？如何标注？）\n形式语言的缺陷：\n1、对于像汉语，英语这样的大型自然语言系统，难以构造精确的文法\n2、不符合人类学习语言的习惯\n3、有些句子语法正确，但在语义上却不可能，形式语言无法排出这些句子\n4、解决方向：基于大量语料，采用统计学手段建立模型\n三、语言模型\n1）语言模型（重要）：通过语料计算某个句子出现的概率（概率表示），常用的有2-元模型，3-元模型\n2）语言模型应用：\n语音识别歧义消除例如，给定拼音串：ta shi yan yan jiu saun fa de\n可能的汉字串：踏实烟酒算法的   他是研究酸法的      他是研究算法的，显然，最后一句才符合。\n3）语言模型的启示：\n1、开启自然语言处理的统计方法\n2、统计方法的一般步骤：\n收集大量语料\n对语料进行统计分析，得出知识\n针对场景建立算法模型\n解释和应用结果\n4） 语言模型性能评价，包括评价目标，评价的难点，常用指标（交叉熵，困惑度）\n5）数据平滑：\n数据平滑的概念，为什么需要平滑\n平滑的方法，加一法，加法平滑法，古德-图灵法，J-M法，Katz平滑法等\n6）语言模型的缺陷：\n语料来自不同的领域，而语言模型对文本类型、主题等十分敏感\nn与相邻的n-1个词相关，假设不是很成立。\n\n四、概率图模型，生成模型与判别模型，贝叶斯网络，马尔科夫链与隐马尔科夫模型（HMM）\n1）概率图模型概述（什么的概率图模型，参考清华大学教材《概率图模型》）\n2）马尔科夫过程（定义，理解）\n3）隐马尔科夫过程（定义，理解）\nHMM的三个基本问题（定义，解法，应用）\n注：第一个问题，涉及最大似然估计法，第二个问题涉及EM算法，第三个问题涉及维特比算法，内容很多，要重点理解，（参考书李航《统计学习方法》，网上博客，笔者github）\n五、马尔科夫网，最大熵模型，条件随机场（CRF）\n1)HMM的三个基本问题的参数估计与计算\n2）什么是熵\n3）EM算法（应用十分广泛，好好理解）\n4）HMM的应用\n5）层次化马尔科夫模型与马尔科夫网络\n提出原因，HMM存在两个问题\n6）最大熵马尔科夫模型\n优点：与HMM相比，允许使用特征刻画观察序列，训练高效\n缺点： 存在标记偏置问题\n7）条件随机场及其应用(概念，模型过程，与HMM关系)\n参数估计方法（GIS算法，改进IIS算法）\nCRF基本问题：特征选取（特征模板）、概率计算、参数训练、解码（维特比）\n应用场景：\n词性标注类问题（现在一般用RNN+CRF）\n中文分词（发展过程，经典算法，了解开源工具jieba分词）\n中文人名，地名识别\n8）  CRF++\n六、命名实体 识别，词性标注，内容挖掘、语义分析与篇章分析（大量用到前面的算法）\n1）命名实体识别问题\n相关概率，定义\n相关任务类型\n方法（基于规程-\u003e基于大规模语料库）\n2）未登录词的解决方法(搜索引擎，基于语料)\n3）CRF解决命名实体识别（NER）流程总结：\n训练阶段：确定特征模板，不同场景（人名，地名等）所使用的特征模板不同，对现有语料进行分词，在分词结                      果基础上进行词性标注（可能手工），NER对应的标注问题是基于词的，然后训练CRF模型，得到对应权值参数值\n识别过程：将待识别文档分词，然后送入CRF模型进行识别计算（维特比算法），得到标注序列，然后根据标                            注划分出命名实体\n4）词性标注（理解含义，意义）及其一致性检查方法（位置属性向量，词性标注序列向量，聚类或者分类算法）\n\n七、句法分析\n1）句法分析理解以及意义\n1、句法结构分析\n完全句法分析\n浅层分析（这里有很多方法。。。）\n2、 依存关系分析\n2）句法分析方法\n1、基于规则的句法结构分析\n2、基于统计的语法结构分析\n八、文本分类，情感分析\n1）文本分类，文本排重\n文本分类：在预定义的分类体系下，根据文本的特征，将给定的文本与一个或者多个类别相关联\n典型应用：垃圾邮件判定，网页自动分类\n2）文本表示，特征选取与权重计算，词向量\n文本特征选择常用方法：\n1、基于本文频率的特征提取法\n2、信息增量法\n3、X2（卡方）统计量\n4、互信息法\n3）分类器设计\nSVM，贝叶斯，决策树等\n4）分类器性能评测\n1、召回率\n2、正确率\n3、F1值\n5）主题模型（LDA）与PLSA\nLDA模型十分强大，基于贝叶斯改进了PLSA，可以提取出本章的主题词和关键词，建模过程复杂，难以理解。\n6）情感分析\n借助计算机帮助用户快速获取，整理和分析相关评论信息，对带有感情色彩的主观文本进行分析，处理和归纳例如，评论自动分析，水军识别。\n某种意义上看，情感分析也是一种特殊的分类问题\n\n7）应用案例\n\n九、信息检索，搜索引擎及其原理\n1）信息检索起源于图书馆资料查询检索，引入计算机技术后，从单纯的文本查询扩展到包含图片，音视频等多媒体信息检索，检索对象由数据库扩展到互联网。\n1、点对点检索\n2、精确匹配模型与相关匹配模型\n3、检索系统关键技术：标引，相关度计算\n2）常见模型：布尔模型，向量空间模型，概率模型\n3）常用技术：倒排索引，隐语义分析（LDA等）\n4）评测指标\n十、自动文摘与信息抽取，机器翻译，问答系统\n1）统计机器翻译的的思路，过程，难点，以及解决\n2）问答系统\n基本组成：问题分析，信息检索，答案抽取\n类型：基于问题-答案， 基于自由文本\n典型的解决思路\n3）自动文摘的意义，常用方法\n4）信息抽取模型（LDA等）\n\n十一、深度学习在自然语言中的应用\n1）单词表示，比如词向量的训练（wordvoc）\n2）自动写文本\n写新闻等\n3）机器翻译\n4）基于CNN、RNN的文本分类\n5）深度学习与CRF结合用于词性标注\n...............\n转自：https://blog.csdn.net/meihao5/article/details/79592667","data":"2018年09月19日 16:12:56"}
{"_id":{"$oid":"5d36ba6d6734bd8e681d6365"},"title":"自己在实习僧上总结的一些自然语言处理方向的职位要求","author":"weixin_38104825","content":"陌陌科技\n职位描述：\n岗位职责：\n1、参与陌陌平台文本spam识别的开发，参与优化文本分类、聚类，文本相似性，语言模型，情感分析，用户行为分析等工作，持续改进和升级现有产品；\n2、跟进文本挖掘、NLP和机器学习领域的前沿技术，将前沿技术应用于实际业务。\n\n\n岗位要求：\n1、在以下至少一个领域有一定了解：\n(1)统计机器学习相关方法，如深度神经网络、概率图模型，最优化方法等；\n(2)语义理解技术，如知识图谱、语义解析、知识挖掘等；\n2、良好的分析问题与发现问题的能力，善于归纳技术方案的特性，并找出其不足与改进方法；\n3、有一定编程能力，熟悉Hadoop、Spark等分布式计算框架者更佳；\n4、具有良好的沟通能力，和良好的团队合作精神；\n5、应聘实习生职位的要求能每周实习3天以上，实习半年以上。\n===================================================\n京东集团\n职位描述：\n职位：自然语言处理算法工程师\n主要研究方向为：自然语言处理、文本分析、或相关机器学习方向。非常欢迎从事深度学习、机器学习方向研究且有兴趣在文本处理方向做落地实践的同学。也欢迎投递简历。\n基本要求：\n1. 有自然语言处理、文本分析或文本理解等相关项目经验；\n2. 熟练掌握一门脚本语言，如python或者perl；\n3. 对中文分词、词性标注、命名实体识别的某一研究领域有较深的研究；\n4. 对文本分类、语义理解、文本摘要等技术方向有一定的了解和研究；\n5. 对于AI技术、新产品、新技术有关注和热情。\n有以下经验之一者优先考虑   ：\n1. 能够编写高质量的线上服务代码；\n2. NLP之外的其它机器学习方向的相关项目经验；\n3. 对基于DNN的NLP前沿方法有较深入了解；\n4. 对于对话系统、聊天机器人的原理有较深入了解。\n=======================================================\n爱奇艺\n职位介绍：\n1   feed流标签分类\n2   关键词提取，命名实体提取，文本纠错等\n3   对话系统\n职位要求：\n1    计算机，电子，数学，物理等相关专业，硕/博研究生均可\n2    熟悉Linux基本操作，熟悉C++和python，对数据结构和算法设计有较为深刻的理解\n3    有自然语言处理相关知识，包括但不限于分词，词性标注，命名实体识别等\n4    熟悉基本的机器学习算法和深度学习（如RNN,CNN等）算法\n5    熟悉一种深度学习框架，如tensorFlow，caffe\n6    优秀的分析问题和解决问题的能力，对解决具有挑战性问题充满激情\n7    每周至少4个工作日，6个月以上\n8    对2019年毕业的同学，提供转正机会\n=================================================================\n点智互动\n职位描述：\n1、负责NLP基础算法，包括分词，词性标注，命名实体识别，新词发现，句法和语义分析等算法优化；\n2、负责实现智能问答，从海量对话数据中挖掘数据，构建行业知识图谱，回答常见问题，提升用户体验；\n3、负责优化对话系统，研发具有学习能力的智能机器；\n4、负责数据 分类/聚类，情感分析和质量识别工作。\n任职要求：\n1、211/985计算机、模式识别、数学相关专业本科及以上学历；\n2、 有数据挖掘、策略和算法研发工作实践经验优先；\n3、有文本挖掘相关经验，有较丰富Python ,linux环境开发经验；\n4、有海量数据挖掘、知识图谱构建、深度学习研发实践经验优先；\n5、有搜索引擎、风控项目，推荐系统，行为分析系统，用户画像等研发经验优先。\n======================================================================\n最右APP\n工作职责:\n-研究数据挖掘或机器学习领域的前沿技术,并用于实际问题的解决和优化\n-大规模机器学习算法研究及并行化实现,为各种大规模机器学习应用研发核心技术\n-通过对数据的敏锐洞察,深入挖掘产品潜在价值和需求,进而提供更有价值的产品和服务,通过技术创新推动产品成长\n职责要求:\n-热爱互联网，对技术研究和应用抱有浓厚的兴趣，有强烈的上进心和求知欲，善于学习和运用新知识\n-具有以下一个或多个领域的理论背景和实践经验：机器学习/数据挖掘/计算机视觉/信息检索/推荐系统\n-至少精通一门编程语言，熟悉网络编程、多线程、分布式编程技术，对数据结构和算法设计有较为深刻的理解\n-良好的逻辑思维能力，对数据敏感，能够发现关键数据、抓住核心问题\n-较强的沟通能力和逻辑表达能力，具备良好的团队合作精神和主动沟通意识\n具有以下条件者优先：\n-熟悉文本分类、聚类、计算机视觉有相关项目经验\n-熟悉海量数据处理、最优化算法、分布式计算或高性能并行计算，有相关项目经验\n-有应用tensorflow等深度学习框架解决过实际应用问题的经验\n============================================\n字节跳动\n【职位描述】\n1. 文本分类：包括训练语料收集，清理，标注，特征选择，特征值优化，类别体系修改，训练算法改进等；\n2. 话题聚类：分析新闻语料，学习出语料中涵盖的相关话题，以及在线预测出文章中包含的话题；\n3. 分词、词性预测、命名实体识别：结合业界最新进展和语料改进和开发相关算法。\n【职位要求】\n1. 对职位描述中的一项或多项工作感兴趣且熟悉，有具体相关经验者优先；\n2. 具备强悍的编码能力，熟悉Linux开发环境，熟悉Python/C++/Java/Scala语言；\n3. 优秀的分析问题和解决问题的能力，对解决具有挑战性问题充满激情；\n实习时间≥4天/周   ≥3个月\n【需提交的材料】\n个人简历+做过的项目代码（岗位对编程能力有要求，希望通过代码进行二次筛选）\n===================================================================================\n搜狐\n工作内容：\n结合产品需求，应用自然语言处理技术进行文本分析、分类、关键词提取、相关性计算等任务\n职位要求：\n1. 自然语言处理等相关专业\n2. 熟练掌握c++或者java，熟悉python编程\n3. 熟悉linux和shell环境\n4. 有积极向上的工作热情、有独当一面的工作能力、思维清晰、表达准确\n\n5. 有大数据和短视频推荐系统方面的相关经验加分\n===================================================\n网易\n工作职责：\n1、理解业务、产品、运营的工作需求，将业务环节提取、抽象为数学问题，设计、实现基于数据挖掘的解决方案，进行数据特征工程和机器学习模型的选取和调优，对接产品、开发、数据人员实现模型的测试和落地；\n2. 分析和研究数据与实际业务的关联关系，针对具体业务需求场景，设计用户价值、用户行为预测、用户分类、用户画像、用户生命周期、用户流失、交易盈利、风险控制等模型并搭建职能投资顾问系统。\n任职资格：\n1.硕士或博士，应届生或工作经验1-3年；\n2.计算机、统计、电子、数学、数理统计等相关专业，机器学习/数据挖掘/信息检索/自然语言处理/统计分析相关背景；\n3.掌握常用的分类、回归、聚类、预测、关联规则、序列模式等挖掘算法，了解数据挖掘前沿技术；\n4.熟练使用一种或多种数据挖掘工具，使用python/shell/scala等脚本语言；\n5.具有很强的学习和研究能力，英语熟练，能够熟练阅读英文技术资料，积极创新、乐于协作、善于沟通。\n\n===========================================================================\n创新工场\n工作职责：\n1、对中文文本进行基础NLP处理，包括但不限于：关键词提取，命名实体识别，分词，词性标注，文本分析，新词发现，词义消歧等。对海量文本进行深度分析，情感分析，文本分类，文本生成等\n2、调研和设计策略算法，参与NLP深度学习相关算法的研究与开发\n4.对 对话系统，智能问答，知识图谱等有了解者加分\n5.学习NLP领域的先进技术并开展相关研发工作\n职位要求：\n1、计算机科学、自然语言处理、数学等相关学科的在读学士，硕士或博士。具备良好的数学功底，包括线性代数、数理统计、数值优化、优化理论等方面；\n\n2、熟悉NLP和机器学习，深度学习，增强学习，迁移学习等理论基础。熟悉版本控制工具；\n3、能快速阅读并理解顶会论文并能够参与复现和改进\n4、熟悉c/c++,java,python,R及GPU开发、调试、性能优化者优先，有深度学习经验，使用过Caffe,TensorFlow,Theano等工具的优先，熟悉Hadoop/Hive/HBase/Spark/Storm等系统的优先，参与过NLP领域顶会论文发表，如ACL, AAAI, 者优先。\n5、在文本分类、文本聚类、新词发现、深度学习等领域有丰富经验优先；\n6、责任心强，积极主动，有良好的沟通能力和团队合作能力\n===================================================\n注意这些都是实习生的要求啊！！！！！\n最后自己默默地总结一下：\n自然语言应用的领域方面：\n\n文本分类、相似性    4\n语言模型\n情感分析    3\n\n文本分析    5\n\n关键词提取    4\n\n文本纠错\n\n对话系统、智能问答    4\n\n分词、词性标注    3\n\n知识图谱    3\n技能要求：\nhadoop/spark    2\n脚本语言python    7\n\nlinux    5\nC++ java    4\n\nRNN CNN\n\ntensorflow caffe    2","data":"2018年07月09日 21:00:07"}
{"_id":{"$oid":"5d36ba916734bd8e681d636d"},"title":"自然语言处理 - 要代替 RNN、LSTM 的 Transformer","author":"GoWeiXH","content":"自然语言处理 - 要代替 RNN、LSTM 的 Transformer\nTransformer 结构\n计算过程\nSeq2Seq 模型，通常来讲里面是由 RNN、GRU、LSTM 的 cell 来组建的，但最近 Google 推出了一个新的架构 Transformer. 这个模型解决了 Seq2Seq 模型依赖之前结果无法并行的问题，而且最终的效果也是非常棒。\n原文：图解 Transformer\n已经这么详细的翻译了，我这里为自己总结一下关键点。\nTransformer 结构\nSeq2Seq 模型是这个样子的：\nTransformer 的宏观结构也是这样的，不同的是内部的微观结构。它里面又多个 Encoder 或 多个 Decoder 组成，而每个 Coder 内部又拥有不同的结构：\n\n\n计算过程\n最关键的是对每个词向量，每次在 Self -Attention 时初始化（后续反向传播更新）三个矩阵 query/key/value，利用 query * key 计算得到 score 并通过 softmax 计算得到 概率权重，乘以 value，得到激活词向量。\n每个词向量分别进入独立的 FFNN 计算。\n当 Encoder 计算完毕后，再转化为两个矩阵 key/value 代入到 Decoder 中 Encoder-Decoder Attention 的计算中。","data":"2019年05月28日 16:22:32"}
{"_id":{"$oid":"5d36bab16734bd8e681d637a"},"title":"自然语言处理基本概念","author":"喜欢打酱油的老鸟","content":"本文为 http://blog.sina.com.cn/s/blog_1334cae810102wovb.html 笔记\n自然语言处理常用术语\n文本主要分为三种文本，自由文本、结构化文本、半结构化文本，自然语言处理一般是对自由文本进行的处理。常见的基本操作如下：\n分词\n通常我们处理的自由文本分为中文、英文等。词为文本最基本的单位，分词是进行自然语言处理中最基本的步骤。分词算法分为词典方法和统计方法。其中，基于词典和人工规则的方法是按照一定的策略将待分析词与词典中的词条进行匹配（正向匹配、逆向匹配、最大匹配）。统计方法是基本字符串在语料库中出现的统计频率，典型的算法有HMM\\CRF等。其中CRF相比HMM有更弱的上下文无相关性假设，理论上效果更好一点。\n英文以空格为分割符，因此不需要进行分词的操作（这是片面的，对于一些特殊情况，依旧需要分词的操作 ，例如 it's等，另外对于英文中复合词的情况，也需要进行一定的识别，因此在进行关键词识别的时候会运营到分词的一些技术）。中文的分词工具有很多，近年来常用的是jieba 和stanford corenlp等。\n词性标注\n在进行词性标注时，需先定义出词性的类别：名词、动词、形容词、连词、副词、标点符号等。词性标注是语音识别、句法分析、信息抽取技术的基础技术之一，词性标注是标注问题，可以采用最大熵、HMM或CRF等具体算法进行模型的训练。自动问答系统中，为了提高用户问题匹配后端知识库的召回率，对一些关键词进行了过滤，包括连词、副词对于全文检索系统，理论上可以通过对用户输入的查询条件进行词性过滤，但由于全文检索是基于词袋模型的机械匹配，并且采用IDF作为特征值之一，因此词性标注的效果不大。\n句法分析\n句法分析的目的是确定句子的句法结构，主谓宾、动宾、定中、动补等。在问答系统和信息检索领域有重要的作用。\n命名实体识别\n命名实体识别是定位句子中出现的人名、地名、机构名、专有名词等。命名实体属于标注问题，因此可以采用HMM\\CRF等进行模型的训练。基于统计的命名实体识别需要基于分词、词性标注等技术。命名实体定义了五大类类型：设施（FAC）\\地理政治实体（GPE）\\位置（LOC）\\人物（PER）。在实际应用中，可以根据自己的业务需求，定义实体类别，并进行模型训练。\n实体关系抽取\n实体关系抽取是自动识别非结构化文档中两个实体之间的关联关系，属于信息抽取领域的基础知识之一。近年来，搜索领域流行的知识图谱技术是构建实体关系。实体关系抽取有多种方式，包括规则匹配、有监督学习、无监督学习。其中有监督学习需要预先定义实体关系类别，并通常将问题建模为分类问题。有监督学习需要预先人工标注语料库。\n\n---------------------\n作者：Virginia5\n来源：CSDN\n原文：https://blog.csdn.net/Virginia5/article/details/68060563\n版权声明：本文为博主原创文章，转载请附上博文链接！","data":"2018年11月30日 13:46:17"}
{"_id":{"$oid":"5d36babe6734bd8e681d6382"},"title":"最好的入门自然语言处理（NLP）的资源清单","author":"野心家-Andy","content":"最好的入门自然语言处理（NLP）的资源清单\nMelanie Tosik目前就职于旅游搜索公司WayBlazer，她的工作内容是通过自然语言请求来生产个性化旅游推荐路线。回顾她的学习历程，她为期望入门自然语言处理的初学者列出了一份学习资源清单。\n目录:\n·  在线课程\n·  图书馆和开放资源\n·  活跃的博客\n·  书籍\n·  数据集\n·  NLP之社交媒体\n·  其它\ndisplaCy网站上的可视化依赖解析树\ndisplaCy\n记得我曾经读到过这样一段话，如果你觉得有必要回答两次同样的问题，那就把答案发到博客上，这可能是一个好主意。根据这一原则，也为了节省回答问题的时间，我在这里给出该问题的标准问法：“我的背景是研究**科学，我对学习NLP很有兴趣。应该从哪说起呢？”\n在您一头扎进去阅读本文之前，请注意，下面列表只是提供了非常通用的入门清单（有可能不完整）。 为了帮助读者更好地阅读，我在括号内添加了简短的描述并对难度做了估计。最好具备基本的编程技能（例如Python）。\n在线课程\n1.Dan Jurafsky 和 Chris Manning：自然语言处理[非常棒的视频介绍系列] （YouTube）\nDan Jurafsky/Chris Manning 自然语言处理\n2.斯坦福CS224d：自然语言处理的深度学习[更高级的机器学习算法、深度学习和NLP的神经网络架构]\n斯坦福CS224d\n3.Coursera：自然语言处理简介[由密西根大学提供的NLP课程]\nCoursera 自然语言处理简介\n图书馆和开放资源\n1.spaCy（网站，博客）[Python; 新兴的开放源码库并自带炫酷的用法示例、API文档和演示应用程序]\nhttps://spacy.io/\nhttps://explosion.ai/blog/\nhttps://spacy.io/docs/usage/showcase\n2.自然语言工具包（NLTK）（网站，图书）[Python; NLP实用编程介绍，主要用于教学目的]\nhttp://www.nltk.org\nhttp://www.nltk.org/book/\n3.斯坦福CoreNLP（网站）[由Java开发的高质量的自然语言分析工具包]\nhttps://stanfordnlp.github.io/CoreNLP/\n活跃的博客\n1.自然语言处理博客（HalDaumé）\nhttps://nlpers.blogspot.com/\n2.Google研究博客\nhttps://research.googleblog.com/\n3.语言日志博客（Mark Liberman）\nhttp://languagelog.ldc.upenn.edu/nll/\n书籍\n1.言语和语言处理（Daniel Jurafsky和James H. Martin）[经典的NLP教科书，涵盖了所有NLP的基础知识，第3版即将出版]\nhttps://web.stanford.edu/~jurafsky/slp3/\n2.统计自然语言处理的基础（Chris Manning和HinrichSchütze）[更高级的统计NLP方法]\nhttps://nlp.stanford.edu/fsnlp/\n3.信息检索简介（Chris Manning，Prabhakar Raghavan和HinrichSchütze）[关于排名/搜索的优秀参考书]\nhttps://nlp.stanford.edu/IR-book/\n4.自然语言处理中的神经网络方法（Yoav Goldberg）[深入介绍NLP的NN方法，和相对应的入门书籍]\nhttps://www.amazon.com/Network-Methods-Natural-Language-Processing/dp/1627052984\nhttp://u.cs.biu.ac.il/~yogo/nnlp.pdf\n数据集\n1.Nicolas Iderhoff已经创建了一份公开的、详尽的NLP数据集的列表。除了这些，这里还有一些项目，可以推荐给那些想要亲自动手实践的NLP新手们：\nhttps://github.com/niderhoff/nlp-datasets\n2.基于隐马尔可夫模型（HMM）实现词性标注（POS tagging）.\nhttps://en.wikipedia.org/wiki/Part-of-speech_tagging\nhttps://en.wikipedia.org/wiki/Hidden_Markov_model\n3.使用CYK算法执行上下文无关的语法解析\nhttps://en.wikipedia.org/wiki/CYK_algorithm\nhttps://en.wikipedia.org/wiki/Context-free_grammar\n4.在文本集合中，计算给定两个单词之间的语义相似度，例如点互信息（PMI，Pointwise Mutual Information）\nhttps://en.wikipedia.org/wiki/Semantic_similarity\nhttps://en.wikipedia.org/wiki/Pointwise_mutual_information\n5.使用朴素贝叶斯分类器来过滤垃圾邮件\nhttps://en.wikipedia.org/wiki/Naive_Bayes_classifier\nhttps://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering\n6.根据单词之间的编辑距离执行拼写检查\nhttps://en.wikipedia.org/wiki/Spell_checker\nhttps://en.wikipedia.org/wiki/Edit_distance\n7.实现一个马尔科夫链文本生成器\nhttps://en.wikipedia.org/wiki/Markov_chain\n8.使用LDA实现主题模型\nhttps://en.wikipedia.org/wiki/Topic_model\nhttps://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\n9.使用word2vec从大型文本语料库，例如维基百科，生成单词嵌入。\nhttps://code.google.com/archive/p/word2vec/\nhttps://en.wikipedia.org/wiki/Wikipedia:Database_download\nNLP之社交媒体\n1.Twitter：#nlproc，NLPers上的文章列表（由Jason Baldrige提供）\nhttps://twitter.com/hashtag/nlproc\nhttps://twitter.com/jasonbaldridge/lists/nlpers\n2.Reddit 社交新闻站点：/r/LanguageTechnology\nhttps://www.reddit.com/r/LanguageTechnology\n3.Medium发布平台：Nlp\nhttps://medium.com/tag/nlp\n其它\n1.如何在TensorFlow中构建word2vec模型[学习指南]\nhttps://www.tensorflow.org/versions/master/tutorials/word2vec/index.html\n2.NLP深度学习的资源[按主题分类的关于深度学习的顶尖资源的概述]\nhttps://github.com/andrewt3000/dl4nlp\n3.最后一句话：计算语言学和深度学习——论自然语言处理的重要性。（Chris Manning）[文章]\nhttp://mitp.nautil.us/article/170/last-words-computational-linguistics-and-deep-learning\n4.对分布式表征的自然语言的理解（Kyunghyun Cho）[关于NLU的ML / NN方法的独立讲义]\nhttps://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf\n5.带泪水的贝叶斯推论（Kevin Knight）[教程工作簿]\nhttp://www.isi.edu/natural-language/people/bayes-with-tears.pdf\n6.国际计算语言学协会（ACL）[期刊选集]\nhttp://aclanthology.info/\n7.果壳问答网站(Quora)：我是如何学习自然语言处理的？\nhttps://www.quora.com/How-do-I-learn-Natural-Language-Processing","data":"2018年08月10日 11:31:37"}
{"_id":{"$oid":"5d36c04a6734bd8e681d647f"},"title":"自然语言处理（NLP）——词向量","author":"JustSleep","content":"一、Word Embedding概述\n简单来说，词嵌入（Word Embedding）或者分布式向量（Distributional Vectors）是将自然语言表示的单词转换为计算机能够理解的向量或矩阵形式的技术。由于要考虑多种因素比如词的语义（同义词近义词）、语料中词之间的关系（上下文）和向量的维度（处理复杂度）等等，我们希望近义词或者表示同类事物的单词之间的距离可以理想地近，只有拿到很理想的单词表示形式，我们才更容易地去做翻译、问答、信息抽取等进一步的工作。\n在Word Embedding之前，常用的方法有one-hot、n-gram、但是他们都有各自的缺点，下面会说明。之后，Bengio提出了NLM，是为Word Embedding的想法的雏形，再后来，Mikolov对其进行了优化，即Word2vec，包含了两种类型，Continuous Bag-of-Words Model 和 skip-gram model。\n二、Word2vec之前\n2.1 one-hot\none-hot是最简单的一种处理方式。通俗地去讲，把语料中的词汇去重取出，按照一定的顺序（字典序、出现顺序等）排列为词汇表，则每一个单词都可以表示为一个长度为N的向量，N为词汇表长度，即单词总数。该向量中，除了该词所在的分量为1，其余均置为0。\n2.2 n-gram\nn-gram可以表示单词间的位置关系所反映的语义关联，在说明n-gram之前，我们从最初的句子概率进行推导。\n假设一个句子S为n个单词有序排列，记为：\n我们将其简记为 ，则这个句子的概率为：\n对于单个概率意思为该单词在前面单词给定的情况下出现的概率，我们利用贝叶斯公式可以得到：\n其中最后一项为在语料中出现的频数。但是长句子或者经过去标点处理后的文本可能很长，而且太靠前的词对于词的预测影响不是很大，于是我们利用马尔可夫假设，取该词出现的概率仅依赖于该词前面的n-1个词，这就是n-gram模型的思想。\n所以上面的公式变为：\n在这里，我们不对n的确定做算法复杂度上的讨论，详细请参考文献[1]，一般来说，n取3比较合适。此外对于一些概率为0的情况所出现的稀疏数据，采用平滑化处理，此类算法很多，以后有时间再具体展开学习。\n2.4 神经语言模型（NLM）\n神经语言模型（Neural Language Model）是Word Embeddings的基本思想，\nNLM的输入是词向量，词向量和模型参数（最终的语言模型）可以通过神经网络训练一同得到。相比于n-gram通过联合概率考虑词之间的位置关系，NLM则是利用词向量进一步表示词语之间的相似性，比如近义词在相似的上下文里可以替代，或者同类事物的词可以在语料中频数不同的情况下获得相近的概率。结合参考文献[1]，举一个简单例子：\n在一个语料C中，S1=“A dog is sitting in the room.”共出现了10000次，S2=\"A cat is sitting in the room\"出现了1次，按照n-gram的模型，当我们输入“A _____ is sitting in the room”来预测下划线上应该填入的词时，dog的概率会远大于cat，这是针对于语料C得到的概率。但是我们希望相似含义的词在目标向量空间中的距离比不相关词的距离更近，比如v(man)-v(woman)约等于v(gentleman)-v(madam)，用这样生成的词向量或者已经训练好的模型在去做翻译、问答等后续工作时，就会很有效果，而NLM利用词向量表示就能达到这样的效果。\nNLM的神经网络训练样本同n-gram的取法，取语料中任一词w的前n-1个词作为Context(w)，则（Context(w)，w）就是一个训练样本了。这里的每一个词都被表示为一个长度为L的词向量，然后将Context(w)的n-1个词向量首位连接拼成（n-1）L的长向量。下面为NLM图解：\n\n包括四层：输入层、投影层、隐藏层、输出层\n注意：这只是取一个词w后输出的向量y，我们需要的就是通过训练集所有的词都做一遍这个过程来优化得到理想的W，q和U，b。\n上图中所有参数W、q、U、b，以及词向量都是通过训练得到的。\n三、Word2vec\n目前学习了解到的Word2vec有基于Hierarchical Softmax和基于Negative Sampling两种方式，由于这两个模型是相反的过程，即CBOW是在给定上下文基础上预测中心词，Skip-gram在有中心词后预测上下文。两个模型都包含三层：输入层、投影层、输出层。\n3.1 CBOW\n不同于NLM的是，Context(w)的向量不再是前后连接，而是求和，我们记为\n3.1.1 基于Hierarchical Softmax\n基于Hierarchical Softmax的CBOW所要构建的霍夫曼树所需参数如下：\n：从根结点到w对应结点的路径\n：路径上包含结点个数\n：到w路径上的的结点\n：结点编码，根结点不编码\n：非叶子结点（包括根结点）对应的向量\n霍夫曼树构建按照频数大小有左右两种，其实都是自己约定的，在这里就不麻烦了，构建后左结点编码为0，为正类，右结点为1，为负类。\n根据逻辑回归，一个结点被分为正类的概率为\n\n\n所以之前我们要构造的目标函数就可以写为以下形式：\n对以上式子进行最优化得到Θ和词向量V，我们发现词向量在这里是累加的，我们省略求各个词的V。\n3.1.2基于Negative Sampling\n对于大规模语料，构建霍夫曼树的工作量是巨大的，而且叶子节点为N的霍夫曼数需要新添(N-1)个结点，而随着树的深度增加，参数计算的量也会增加很多很多，得到的词向量也会不够好，为此，Mikolov作出了优化，将构建霍夫曼树改为随机负采样方法。\n对于给定的上下文Context(w)去预测w，如果从语料中就是存在（Context(w),w），那么w就是正样本，其他词就是负样本。\n我们设负样本集为，词的标签：\n训练目标为增大正样本的概率，减小负样本的概率。可见，对于单词w，基于Hierarchical Softmax将其频数用来构建霍夫曼树，正负样本标签取自结点左右编码；而基于Negative Sampling将其频数作为随机采样线段的子长度，正负样本标签取自从语料中随机取出的词是否为目标词，构造复杂度小于前者。\n3.2 Skip-gram\n由于Skip-gram是CBOW的相反操作，输入输出稍有不同，推导大同小异。\n..................","data":"2019年03月04日 21:29:59"}
{"_id":{"$oid":"5d36c1656734bd8e681d64ba"},"title":"自然语言处理NLP国内研究方向机构导师","author":"喜欢打酱油的老鸟","content":"自然语言处理NLP国内研究方向机构导师\n文|中文信息协会《中文信息处理发展报告2016》，数据简化DataSimp\n文字语言VS数字信息\n数字、文字和自然语言一样，都是信息的载体，他们之间原本有着天然的联系。语言和数学的产生都是为了交流，从文字、数字和语言的发展历史，可以了解到语言、文字和数字有着内在的联系。自然语言处理NLP主要涉及三种文本，自由文本、结构化文本、半结构化文本。\n自然语言理解Natural Language Understanding(NLU)，实现人机间自然语言通信，意味着要使计算机既能理解自然语言文本的意义，也能以自然语言文本表达给定的意图、思想等。自然语言生成NLG，是人工或机器生成语言。\n斯坦福自然语言处理NLP工具资料收集、斯坦福分词、Stanford中文实体识别，最早做自然语言处理的网址https://nlp.stanford.edu/software/segmenter.shtml。\n哈尔滨工业大学智能技术与自然语言处理研究室(IntelligentTechnology \u0026 Natural Language Processing Lab, ITNLPLab)是国内较早从事自然语言处理和语言智能技术的研究室。\n除了新兴的文本数据简化领域：秦陇纪(数据简化技术中心筹)，自然语言处理NaturalLanguage Processing领域主要包括基础研究和应用研究。\n基础研究\n词法与句法分析：李正华、陈文亮、张民(苏州大学)\n语义分析：周国栋、李军辉(苏州大学)\n篇章分析：王厚峰、李素建(北京大学)\n语言认知模型：王少楠，宗成庆(中科院自动化研究所)\n语言表示与深度学习：黄萱菁、邱锡鹏(复旦大学)\n知识图谱与计算：李涓子、候磊(清华大学)\n应用研究\n文本分类与聚类：涂存超，刘知远(清华大学)\n信息抽取：孙乐、韩先培(中国科学院软件研究所)\n情感分析：黄民烈(清华大学)\n自动文摘：万小军、姚金戈(北京大学)\n信息检索：刘奕群、马少平(清华大学)\n信息推荐与过滤：王斌(中科院信工所)，鲁骁(国家计算机网络应急中心)\n自动问答：赵军、刘康，何世柱(中科院自动化研究所)\n机器翻译：张家俊、宗成庆(中科院自动化研究所)\n社会媒体处理：刘挺、丁效(哈尔滨工业大学)\n语音技术：说话人识别——郑方(清华大学)，王仁宇(江苏师范大学)\n语音合成——陶建华(中科院自动化研究所)\n语音识别——王东(清华大学)\n文字识别：刘成林(中科院自动化研究所)\n多模态信息处理：陈晓鸥(北京大学)\n医疗健康信息处理：陈清财、汤步洲(哈尔滨工业大学)\n少数民族语言信息处理：吾守尔•斯拉木(新疆大学)\n— 完 —\nhttps://blog.csdn.net/yH0VLDe8VG8ep9VGe/article/details/83747195","data":"2018年11月08日 15:20:17"}
{"_id":{"$oid":"5d36c17a6734bd8e681d64c4"},"title":"GitHub项目：自然语言处理项目的相关干货整理","author":"CopperDong","content":"自然语言处理（NLP）是计算机科学，人工智能，语言学关注计算机和人类（自然）语言之间的相互作用的领域。本文作者为自然语言处理NLP初学者整理了一份庞大的自然语言处理项目领域的概览，包括了很多人工智能应用程序。选取的参考文献与资料都侧重于最新的深度学习研究成果。这些自然语言处理项目资源能为想要深入钻研一个自然语言处理NLP任务的人们提供一个良好的开端。\n自然语言处理项目的相关干货整理：\n指代消解\nhttps://github.com/Kyubyong/nlp_tasks#coreference-resolution\n论文自动评分\n论文：Automatic Text Scoring Using Neural Networks（使用神经网络的自动文本评分）：https://arxiv.org/abs/1606.04289\n论文：A Neural Approach to Automated Essay Scoring（一种自动将论文评分的神经学方法）：http://www.aclweb.org/old_anthology/D/D16/D16-1193.pdf\n挑战：Kaggle:The Hewlett Foundation: Automated Essay Scoring（Kaggle：The Hewlett Foundation:论文自动评分系统）：https://www.kaggle.com/c/asap-aes\n项目：Enhanced AI Scoring Engine（增强的人工智能得分引擎）：https://github.com/edx/ease\n自动语音识别\n维基百科： 语言识别：https://en.wikipedia.org/wiki/Speech_recognition\n论文：DeepSpeech 2: End-to-End Speech Recognition in English and Mandarin（深度语音2:用英语和普通话进行端对端语音识别）：https://arxiv.org/abs/1512.02595\n论文：WaveNet:A Generative Model for Raw Audio（WaveNet:原始音频的生成模型）：https://arxiv.org/abs/1609.03499\n项目：A TensorFlow implementation of Baidu’s Deep Speech architecture（百度深度语音架构的一个TensorFlow实现：https://github.com/mozilla/DeepSpeech\n项目：Speech-to-Text-WaveNet: End-to-end sentence level English speech recognition using DeepMind’s WaveNet（Speech-to-Text-WaveNet: 使用DeepMind的WaveNet，对端到端句子的英语水平语音识别）：https://github.com/buriburisuri/speech-to-text-wavenet\n挑战：The 5th CHiME Speech Separation and Recognition Challenge（第五届CHiME语音的分离和识别挑战）：http://spandh.dcs.shef.ac.uk/chime_challenge/\n资料：The 5thCHiME Speech Separation and Recognition Challenge（第五届CHiME语音的分离和识别挑战）：http://spandh.dcs.shef.ac.uk/chime_challenge/download.html\n资料：CSTRVCTK Corpus ：http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html\n资料：LibriSpeech ASR corpus：http://www.openslr.org/12/\n资料：Switchboard-1 Telephone Speech Corpus：https://catalog.ldc.upenn.edu/ldc97s62\n资料：TED-LIUM Corpus：http://www-lium.univ-lemans.fr/en/content/ted-lium-corpus\n自动摘要\n维基百科：自动摘要：https://en.wikipedia.org/wiki/Automatic_summarization\n书籍：Automatic Text Summarization（自动本文摘要）：https://www.amazon.com/Automatic-Text-Summarization-Juan-Manuel-Torres-Moreno/dp/1848216688/ref=sr_1_1?s=books\u0026ie=UTF8\u0026qid=1507782304\u0026sr=1-1\u0026keywords=Automatic+Text+Summarization\n论文：Text Summarization Using Neural Networks（使用神经网络进行文本摘要）：http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.823.8025\u0026rep=rep1\u0026type=pdf\n论文：Ranking with Recursive Neural Networks and Its Application to Multi-DocumentSummarization（使用递归神经网络及其应用程序对多文档摘要进行排序）：https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/viewFile/9414/9520\n资料：Text Analytics Conferences（文本分析会议）：https://tac.nist.gov/data/index.html\n资料：Document Understanding Conferences（文书理解会议）：http://www-nlpir.nist.gov/projects/duc/data.html\n共指消解\n信息：共指消解：https://nlp.stanford.edu/projects/coref.shtml\n论文：Deep Reinforcement Learning for Mention-Ranking Coreference Models（对Mention-Ranking的共指模型进行深度强化学习：https://arxiv.org/abs/1609.08667\n论文：Improving Coreference Resolution by Learning Entity-Level Distributed Representations（通过学习实体级分布式表示来改善相关的解决方案）：https://arxiv.org/abs/1606.01323\n挑战：CoNLL 2012 Shared Task: Modeling Multilingual Unrestricted Coreference in OntoNotes（CoNLL 2012共享任务:在OntoNotes中对多语言的不受限制的共指进行建模）：http://conll.cemantix.org/2012/task-description.html\n挑战：CoNLL 2011 Shared Task: Modeling Unrestricted Coreference in OntoNotes（CoNLL 2011共享任务:在OntoNotes中对多语言的不受限制的共指进行建模）：http://conll.cemantix.org/2011/task-description.html\n语法错误校正\n论文：Neural Network Translation Models for Grammatical Error Correction（语法错误校正的神经网络翻译模型）：https://arxiv.org/abs/1606.00189\n挑战：CoNLL 2013 Shared Task: Grammatical Error Correction（CoNLL 2013共享任务:语法错误校正）：http://www.comp.nus.edu.sg/~nlp/conll13st.html\n挑战：CoNLL 2014Shared Task: Grammatical Error Correction（CoNLL 2014共享任务:语法错误校正）：http://www.comp.nus.edu.sg/~nlp/conll14st.html\n资料：NUSNon-commercial research/trial corpus license：http://www.comp.nus.edu.sg/~nlp/conll14st/nucle_license.pdf\n资料：Lang-8 Learner Corpora：http://cl.naist.jp/nldata/lang-8/\n资料：Cornell Movie–Dialogs Corpus：http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html\n项目：Deep Text Corrector（深度文本校正器）：https://github.com/atpaino/deep-text-corrector\n产品：deep grammar：http://deepgrammar.com/\n字素转换到音素\n论文：Grapheme-to-Phoneme Models for （Almost） Any Language（适合(几乎)任何语言的字素到音素的模型）：https://pdfs.semanticscholar.org/b9c8/fef9b6f16b92c6859f6106524fdb053e9577.pdf\n论文：Polyglot Neural Language Models: A Case Study in Cross-Lingual Phonetic Representation Learning（多语言神经语言模型:跨语语音表达学习的案例研究）：https://arxiv.org/pdf/1605.03832.pdf\n论文：Multi task Sequence-to-Sequence Models for Grapheme-to-Phoneme Conversion（多任务序列到序列的字素到音素转换的模型）：https://pdfs.semanticscholar.org/26d0/09959fa2b2e18cddb5783493738a1c1ede2f.pdf\n项目：Sequence-to-Sequence G2P toolkit（序列到序列G2P工具包）：https://github.com/cmusphinx/g2p-seq2seq\n资料：Multilingual Pronunciation Data（多语种发音数据）：https://drive.google.com/drive/folders/0B7R_gATfZJ2aWkpSWHpXUklWUmM\n语种识别\n维基百科： 语种识别：https://en.wikipedia.org/wiki/Language_identification\n论文：AUTOMATIC LANGUAGE IDENTIFICATION USING DEEP NEURAL NETWORKS（使用深度神经网络的自动语言识别）：https://repositorio.uam.es/bitstream/handle/10486/666848/automatic_lopez-moreno_ICASSP_2014_ps.pdf?sequence=1\n挑战： 2015 Language Recognition Evaluation（2015语言识别评估）：https://www.nist.gov/itl/iad/mig/2015-language-recognition-evaluation\n语言建模\n维基百科：语言模型：https://en.wikipedia.org/wiki/Language_model\n工具包： KenLM Language Model Toolkit（KenLM语言模型工具包）：http://kheafield.com/code/kenlm/\n论文：Distributed Representations of Words and Phrases and their Compositionality（词汇和短语的分布表示及其组合性）：http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n论文：Character-Aware Neural Language Models（Character-Aware神经语言模型）：https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/viewFile/12489/12017\n资料： Penn Treebank ：https://github.com/townie/PTB-dataset-from-Tomas-Mikolov-s-webpage/tree/master/data\n词形还原\n维基百科：词形还原：https://en.wikipedia.org/wiki/Lemmatisation\n工具包：WordNet Lemmatizer：http://www.nltk.org/api/nltk.stem.html#nltk.stem.wordnet.WordNetLemmatizer.lemmatize\n资料：Treebank-3：https://catalog.ldc.upenn.edu/ldc99t42\n唇语辨别\n维基百科：唇读法：https://en.wikipedia.org/wiki/Lip_reading\n论文：Lip Reading Sentences in the Wild （在野外读懂唇语）：https://arxiv.org/abs/1611.05358\n论文：3D Convolutional Neural Networks for Cross Audio-Visual Matching Recognition（交叉视听匹配识别的3D卷积神经网络）：https://arxiv.org/abs/1706.05739\n项目： Lip Reading – Cross Audio-Visual Recognition using 3D Convolutional Neural Networks（唇读法—使用3D卷积神经网络的交叉视听识别：https://github.com/astorfi/lip-reading-deeplearning\n资料： The GRID audiovisual sentence corpus：http://spandh.dcs.shef.ac.uk/gridcorpus/\n机器翻译\n论文：Neural Machine Translation by Jointly Learning to Align and Translate（通过共同学习来调整和翻译神经机器翻译）：https://arxiv.org/abs/1409.0473\n论文：Neural Machine Translation in Linear Tim（在线性时间中的神经机器翻译）：https://arxiv.org/abs/1610.10099\n挑战： ACL2014 NINTH WORKSHOP ON STATISTICAL MACHINE TRANSLATION（ACL2014第九届统计机器翻译研讨会）：http://www.statmt.org/wmt14/translation-task.html#download\n资料：OpenSubtitles2016:http://opus.lingfil.uu.se/OpenSubtitles2016.php\n资料： WIT3:Web Inventory of Transcribed and Translated Talks:https://wit3.fbk.eu/\n资料： The QCRI Educational Domain （QED） Corpus：http://alt.qcri.org/resources/qedcorpus/\n命名实体识别\n维基百科：命名实体识别：https://en.wikipedia.org/wiki/Named-entity_recognition\n论文：Neural Architectures for Named Entity Recognition（命名实体识别的神经结构）：https://arxiv.org/abs/1603.01360\n项目： OSU Twitter NLP Tool：https://github.com/aritter/twitter_nlp\n挑战： Named Entity Recognition in Twitter（在推特上被命名的实体识别）：https://noisy-text.github.io/2016/ner-shared-task.html\n资料：CoNLL-2002 NER corpus：https://github.com/teropa/nlp/tree/master/resources/corpora/conll2002\n资料：CoNLL-2003 NER corpus：https://github.com/synalp/NER/tree/master/corpus/CoNLL-2003\n释义检测\n论文：Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection（动态池和展开递归自动编码器的释义检测）：http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.650.7199\u0026rep=rep1\u0026type=pdf\n项目：Paralex: Paraphrase-Driven Learning for Open Question Answering（Paralex：释义驱动学习的开放问答）：http://knowitall.cs.washington.edu/paralex/\n资料：Microsoft Research Paraphrase Corpus：https://www.microsoft.com/en-us/download/details.aspx?id=52398\n资料：Microsoft Research Video Description Corpus ：https://www.microsoft.com/en-us/download/details.aspx?id=52422\u0026from=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fdownloads%2F38cf15fd-b8df-477e-a4e4-a4680caa75af%2F\n资料： Pascal Dataset：http://nlp.cs.illinois.edu/HockenmaierGroup/pascal-sentences/index.html\n资料：Flicker Dataset：http://nlp.cs.illinois.edu/HockenmaierGroup/8k-pictures.html\n资料： TheSICK data set：http://clic.cimec.unitn.it/composes/sick.html\n资料： PPDB:The Paraphrase Database：http://www.cis.upenn.edu/~ccb/ppdb/\n资料：WikiAnswers Paraphrase Corpus：http://knowitall.cs.washington.edu/paralex/wikianswers-paraphrases-1.0.tar.gz\n语法分析\n维基百科：语法分析：https://en.wikipedia.org/wiki/Parsing\n工具包：The Stanford Parser: A statistical parser：https://nlp.stanford.edu/software/lex-parser.shtml\n工具包： spaCyparser：https://spacy.io/docs/usage/dependency-parse\n论文：A fastand accurate dependency parser using neural networks（快速而准确地使用神经网络的依赖解析器）：http://www.aclweb.org/anthology/D14-1082\n挑战：CoNLL2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies（CoNLL2017共享任务:从原始文本到通用依赖项的多语言解析）：http://universaldependencies.org/conll17/\n挑战：CoNLL2016 Shared Task: Multilingual Shallow Discourse Parsing（CoNLL2016共享任务:多语言的浅会话解析）：http://www.cs.brandeis.edu/~clp/conll16st/\n词性标记\n维基百科：词性标记：https://en.wikipedia.org/wiki/Part-of-speech_tagging\n论文：Unsupervised Part-Of-Speech Tagging with Anchor Hidden Markov Models（有Anchor Hidden Markov模型的非监督性的词性标记）：https://transacl.org/ojs/index.php/tacl/article/viewFile/837/192\n资料：Treebank-3：https://catalog.ldc.upenn.edu/ldc99t42\n工具包：nltk.tag package：http://www.nltk.org/api/nltk.tag.html\n拼音与中文转换\n论文：Neural Network Language Model for Chinese Pinyin Input Method Engine（中文拼音输入法引擎的神经网络语言模型）：http://aclweb.org/anthology/Y15-1052\n项目：Neural Chinese Transliterator：https://github.com/Kyubyong/neural_chinese_transliterator\n问答系统\n维基百科：问答系统：https://en.wikipedia.org/wiki/Question_answering\n论文：Ask Me Anything: Dynamic Memory Networks for Natural Language Processing（自然语言处理的动态内存网络）：http://www.thespermwhale.com/jaseweston/ram/papers/paper_21.pdf\n论文：Dynamic Memory Networks for Visual and Textual Question Answering（用于视觉和文本的问答系统的动态记忆网络）：http://proceedings.mlr.press/v48/xiong16.pdf\n挑战：TREC Question Answering Task（TREC问答系统任务）：http://trec.nist.gov/data/qamain.html\n挑战：SemEval-2017 Task 3: Community Question Answering:http://alt.qcri.org/semeval2017/task3/\n资料：MSMARCO: Microsoft MAchine Reading COmprehension Dataset(MSMARCO:微软机器阅读理解数据集）http://www.msmarco.org/\n资料：Maluuba NewsQA：https://github.com/Maluuba/newsqa\n资料：SQuAD:100,000+ Questions for Machine Comprehension of Text（SQuAD:100,000+个文本的机器理解的问题）：https://rajpurkar.github.io/SQuAD-explorer/\n资料：Graph Questions: A Characteristic-rich Question Answering Dataset（图形问题:一个特征丰富的问题回答数据集）：https://github.com/ysu1989/GraphQuestions\n资料： Story Cloze Test and ROC Stories Corpora：http://cs.rochester.edu/nlp/rocstories/\n资料：Microsoft Research WikiQA Corpus：https://www.microsoft.com/en-us/download/details.aspx?id=52419\u0026from=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fdownloads%2F4495da01-db8c-4041-a7f6-7984a4f6a905%2Fdefault.aspx\n资料：DeepMind Q\u0026A Dataset：http://cs.nyu.edu/~kcho/DMQA/\n资料： QASent：http://cs.stanford.edu/people/mengqiu/data/qg-emnlp07-data.tgz\n关系提取\n维基百科：关系提取：https://en.wikipedia.org/wiki/Relationship_extraction\n论文：A deep learning approach for relationship extraction from interaction context in social manufacturing paradigm（一种从社会生产范例的互动情境中提取关系深度学习的方法）：http://www.sciencedirect.com/science/article/pii/S0950705116001210\n语义角色标记\n维基百科：语义角色标记：https://en.wikipedia.org/wiki/Semantic_role_labeling\n书籍：Semantic Role Labeling（语义角色标记）：https://www.amazon.com/Semantic-Labeling-Synthesis-Lectures-Technologies/dp/1598298313/ref=sr_1_1?s=books\u0026ie=UTF8\u0026qid=1507776173\u0026sr=1-1\u0026keywords=Semantic+Role+Labeling\n论文：End-to-end Learning of Semantic Role Labeling Using Recurrent Neural Networks（使用循环神经网络对语义角色标签进行端到端学习）：http://www.aclweb.org/anthology/P/P15/P15-1109.pdf\n论文：Neural Semantic Role Labeling with Dependency Path Embeddings（有着依赖路径嵌入的神经语义角色标记）:https://arxiv.org/abs/1605.07515\n挑战：CoNLL-2005 Shared Task: Semantic Role Labeling（CoNLL-2005共享任务:语义角色标记）：http://www.cs.upc.edu/~srlconll/st05/st05.html\n挑战：CoNLL-2004 Shared Task: Semantic Role Labeling（CoNLL-2004共享任务:语义角色标记）：http://www.cs.upc.edu/~srlconll/st04/st04.html\n工具包：Illinois Semantic Role Labeler（SRL）：http://cogcomp.org/page/software_view/SRL\n资料：CoNLL-2005 Shared Task: Semantic Role Labeling（CoNLL-2005共享任务:语义角色标记）：http://www.cs.upc.edu/~srlconll/soft.html\n语句边界消歧\n维基百科：语句边界消歧：https://en.wikipedia.org/wiki/Sentence_boundary_disambiguation\n论文：A Quantitative and Qualitative Evaluation of Sentence Boundary Detection for theClinical Domain（对临床领域的语句边界检测进行定量和定性的评估）：https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5001746/\n工具包： NLTK Tokenizers：http://www.nltk.org/_modules/nltk/tokenize.html\n资料： The British National Corpus：http://www.natcorp.ox.ac.uk/\n资料：Switchboard-1 Telephone Speech Corpus：https://catalog.ldc.upenn.edu/ldc97s62\n情绪分析\n维基百科：情绪分析：https://en.wikipedia.org/wiki/Sentiment_analysis\n信息：Awesome Sentiment Analysis（了不起的情绪分析）：https://github.com/xiamx/awesome-sentiment-analysis\n挑战：Kaggle: UMICH SI650 – Sentiment Classification（Kaggle: UMICH SI650 – 情绪分类）：https://www.kaggle.com/c/si650winter11#description\n挑战：SemEval-2017 Task 4: Sentiment Analysis in Twitter（SemEval-2017任务4:推特上的情绪分析）：http://alt.qcri.org/semeval2017/task4/\n项目：SenticNet：http://sentic.net/about/\n资料：Multi-Domain Sentiment Dataset（version2.0）：http://www.cs.jhu.edu/~mdredze/datasets/sentiment/\n资料：Stanford Sentiment Treebank：https://nlp.stanford.edu/sentiment/code.html\n资料：Twitter Sentiment Corpus：http://www.sananalytics.com/lab/twitter-sentiment/\n资料：Twitter Sentiment Analysis Training Corpus：http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/\n源分离\n维基百科：源分离：https://en.wikipedia.org/wiki/Source_separation\n论文：From Blind to Guided Audio Source Separation（从盲目到有指导性的音频源分离）：https://hal-univ-rennes1.archives-ouvertes.fr/hal-00922378/document\n论文：Joint Optimization of Masks and Deep Recurrent Neural Networks for Monaural Source Separation （对单声道分离的掩膜和深层循环神经网络的联合优化）：https://arxiv.org/abs/1502.04149\n挑战：Signal Separation Evaluation Campaign（信号分离评估活动）：https://sisec.inria.fr/\n挑战： CHiME Speech Separation and Recognition Challenge(CHiME语音分离和识别的挑战)：http://spandh.dcs.shef.ac.uk/chime_challenge/\n说话者识别\n维基百科：说话者识别：https://en.wikipedia.org/wiki/Speaker_recognition\n论文：A NOVEL SCHEME FOR SPEAKER RECOGNITION USING A PHONETICALLY-AWARE DEEP NEURAL NETWORK（一种使用语音识别的深度神经网络的新方案）：https://pdfs.semanticscholar.org/204a/ff8e21791c0a4113a3f75d0e6424a003c321.pdf\n论文：DEEP NEURAL NETWORKS FOR SMALL FOOTPRINT TEXT-DEPENDENT SPEAKER VERIFICATION（深度神经网络，用于小范围的文本依赖的说话者验证）：https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41939.pdf\n挑战： NIST Speaker Recognition Evaluation（NIST说话者识别评价）：https://www.nist.gov/itl/iad/mig/speaker-recognition\n语音分段\n维基百科：语音分段：https://en.wikipedia.org/wiki/Speech_segmentation\n论文：Word Segmentation by 8-Month-Olds: When Speech Cues Count More Than Statistics（8个月大婴儿的单词分段:当语音提示比统计数字更重要时）：http://www.utm.toronto.edu/infant-child-centre/sites/files/infant-child-centre/public/shared/elizabeth-johnson/Johnson_Jusczyk.pdf\n论文：Unsupervised Word Segmentation and Lexicon Discovery Using Acoustic Word Embeddings（不受监督的单词分割和使用声学词嵌入的词汇发现）：https://arxiv.org/abs/1603.02845\n资料：CALLHOME Spanish Speech：https://catalog.ldc.upenn.edu/ldc96s35\n语音合成\n维基百科：语音合成：https://en.wikipedia.org/wiki/Speech_synthesis\n论文：WaveNet:A Generative Model for Raw Audio（WaveNet:原始音频的生成模型）：https://arxiv.org/abs/1609.03499\n论文：Tacotron:Towards End-to-End Speech Synthesis（Tacotron:对端到端的语音合成）：https://arxiv.org/abs/1703.10135\n资料： The World English Bible：https://github.com/Kyubyong/tacotron\n资料： LJ Speech Dataset：https://github.com/keithito/tacotron\n资料： Lessac Data：http://www.cstr.ed.ac.uk/projects/blizzard/2011/lessac_blizzard2011/\n挑战：Blizzard Challenge 2017：https://synsig.org/index.php/Blizzard_Challenge_2017\n项目： The Festvox project：http://www.festvox.org/index.html\n工具包：Merlin: The Neural Network （NN） based Speech Synthesis System（Merlin：基于神经网络的语音合成系统）：https://github.com/CSTR-Edinburgh/merlin\n语音增强\n维基百科：语音增强：https://en.wikipedia.org/wiki/Speech_enhancement\n书籍： Speech enhancement: theory and practice（语音增强：理论与实践）：https://www.amazon.com/Speech-Enhancement-Theory-Practice-Second/dp/1466504218/ref=sr_1_1?ie=UTF8\u0026qid=1507874199\u0026sr=8-1\u0026keywords=Speech+enhancement%3A+theory+and+practice\n论文 An Experimental Study on Speech Enhancement Based on Deep Neural Network（一项基于深度神经网络的语音增强实验）：http://staff.ustc.edu.cn/~jundu/Speech%20signal%20processing/publications/SPL2014_Xu.pdf\n论文： A Regression Approach to Speech Enhancement Based on Deep Neural Networks（一种基于深度神经网络的语音增强的回归方法）：https://www.researchgate.net/profile/Yong_Xu63/publication/272436458_A_Regression_Approach_to_Speech_Enhancement_Based_on_Deep_Neural_Networks/links/57fdfdda08aeaf819a5bdd97.pdf\n论文：Speech Enhancement Based on Deep Denoising Autoencoder（基于深度降噪自编码的语音增强）：https://www.researchgate.net/profile/Yu_Tsao/publication/283600839_Speech_enhancement_based_on_deep_denoising_Auto-Encoder/links/577b486108ae213761c9c7f8/Speech-enhancement-based-on-deep-denoising-Auto-Encoder.pdf\n词干提取\n维基百科：词干提取：https://en.wikipedia.org/wiki/Stemming\n论文： A BACKPROPAGATION NEURAL NETWORK TO IMPROVE ARABIC STEMMING（一个反向传播的神经网络，用来改善阿拉伯语的词干提取）：http://www.jatit.org/volumes/Vol82No3/7Vol82No3.pdf\n工具包： NLTK Stemmers：http://www.nltk.org/howto/stem.html\n术语提取\n维基百科：术语提取：https://en.wikipedia.org/wiki/Terminology_extraction\n论文： Neural Attention Models for Sequence Classification: Analysis and Application to KeyTerm Extraction and Dialogue Act Detection（序列分类的神经提示模型:分析和应用于关键词提取和对话法检测）：https://arxiv.org/pdf/1604.00077.pdf\n文本简化\n维基百科：文本简化：https://en.wikipedia.org/wiki/Text_simplification\n论文：Aligning Sentences from Standard Wikipedia to Simple Wikipedia（调整句子，从标准的维基百科到简单的维基百科）：https://ssli.ee.washington.edu/~hannaneh/papers/simplification.pdf\n论文：Problems in Current Text Simplification Research: New Data Can Help（当前文本简化研究中的问题:可提供帮助的新数据）：https://pdfs.semanticscholar.org/2b8d/a013966c0c5e020ebc842d49d8ed166c8783.pdf\n资料：Newsela Data：https://newsela.com/data/\n文本蕴涵\n维基百科：文本蕴含：https://en.wikipedia.org/wiki/Textual_entailment\n项目：Textual Entailment with TensorFlow（文本蕴含与TensorFlow）：https://github.com/Steven-Hewitt/Entailment-with-Tensorflow\n竞赛：SemEval-2013 Task 7: The Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge（SemEval-2013任务7:联合学生反应分析和第8届认知文本蕴含挑战）：https://www.cs.york.ac.uk/semeval-2013/task7.html\n音译\n维基百科：音译：https://en.wikipedia.org/wiki/Transliteration\n论文：A Deep Learning Approach to Machine Transliteration（一个机器音译的深度学习方法）：https://pdfs.semanticscholar.org/54f1/23122b8dd1f1d3067cf348cfea1276914377.pdf\n项目：Neural Japanese Transliteration—can you do better than SwiftKey™ Keyboard?（神经日语音译：你能比SwiftKey键盘做得更好吗?）：https://github.com/Kyubyong/neural_japanese_transliterator\n词嵌入\n维基百科：词嵌入：https://en.wikipedia.org/wiki/Word_embedding\n工具包：Gensim: word2vec：https://radimrehurek.com/gensim/models/word2vec.html\n工具包：fastText：https://github.com/facebookresearch/fastText\n工具包：GloVe:Global Vectors for Word Representation：https://nlp.stanford.edu/projects/glove/\n信息：Where to get a pretrained model？（哪里能够获得一个预先训练的模型？）：https://github.com/3Top/word2vec-api\n项目：Pre-trained word vectors of 30+ languages（30多种语言的预先训练的词向量）：https://github.com/Kyubyong/wordvectors\n项目：Polyglot: Distributed word representations for multilingual NLP（Polyglot:多语言NLP的分布式词汇表征）：https://sites.google.com/site/rmyeid/projects/polyglot\n词汇预测\n信息：What is Word Prediction?(什么是词汇预测？）：http://www2.edc.org/ncip/library/wp/what_is.htm\n论文： The prediction of character based on recurrent neural network language model（基于循环神经网络语言模型的字符预测）：http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7960065\n论文： An Embedded Deep Learning based Word Prediction（一个基于深度学习的词汇预测）：https://arxiv.org/abs/1707.01662\n论文：Evaluating Word Prediction: Framing Keystroke Savings（评估单词预测:框击键保存）：http://aclweb.org/anthology/P08-2066\n资料：An Embedded Deep Learning based Word Prediction（一个基于深度学习的词汇预测）：https://github.com/Meinwerk/WordPrediction/master.zip\n项目： Word Prediction using Convolutional Neural Networks—can you do better than iPhone™ Keyboard?（使用卷积神经网络的词汇预测——你能比iPhone键盘做得更好吗?）：https://github.com/Kyubyong/word_prediction\n词分割\n论文： Neural Word Segmentation Learning for Chinese（中文的神经词分割学习）：https://arxiv.org/abs/1606.04300\n项目：Convolutional neural network for Chinese word segmentation（中文的词分割的卷积神经网络）：https://github.com/chqiwang/convseg\n工具包：Stanford Word Segmenter：https://nlp.stanford.edu/software/segmenter.html\n工具包： NLTK Tokenizers：http://www.nltk.org/_modules/nltk/tokenize.html\n词义消歧\n维基百科：词义消歧：https://en.wikipedia.org/wiki/Word-sense_disambiguation\n论文：Train-O-Matic: Large-Scale Supervised Word Sense Disambiguation in Multiple Languages without Manual Training Data（Train-O-Matic:在没有人工训练数据的情况下，在多种语言中大规模的监督词义消歧）：http://www.aclweb.org/anthology/D17-1008\n资料：Train-O-Matic Data：http://trainomatic.org/data/train-o-matic-data.zip\n资料：BabelNet：http://babelnet.org/\n原项目地址：https://github.com/Kyubyong/nlp_tasks#speech-segmentation","data":"2018年06月01日 16:35:14"}
{"_id":{"$oid":"5d36c19e6734bd8e681d64ce"},"title":"自然语言处理总概括","author":"Dulpee","content":"自然语言处理\n自然语言处理是什么\n自然语言处理(Natural Language Process)就是利用计算机来处理人类语言的学科，属于计算机与语言学的交叉学科。\n自然语言处理有哪些技术\n大致包括如下技术:\n1.分词(Word Segmentation或Word Breaker，WB)\n在英文文本当中每个词之间都有间隔好分，但在中文文本当中一句话之间每个词是没有间隔的，所以需要对一个句子当中每个字进行切分，句子的基本语义单元就变成了词，这就是分词任务。\n2.句法分析（Parsing）\n句法分析指的是将句子中每个部分的组块(也就是每个词、字的归属类)标注出来。\n组块分析:标出句子的短语块,如“This is a dog(NP)” 超级标签分析:给每个句子加上超级标签，超级标签是一个树形结构图\n成分句法分析:分析句子成分，给出一颗由终结符和非终结符构成的成分句法树\n依存句法分析:分析句中词的依存关系，给出一颗由词语依存关系构成的依存句法树。\n3.信息抽取（Information Extraction，IE）：命名实体识别和关系抽取（Named Entity Recognition \u0026 Relation Extraction，NER):我们从一段文本中抽取关键信息即从无结构的文本中抽取结构化的信息，\n4.词性标注（Part Of Speech Tagging，POS）:对词语的词性进行标注\n5.指代消解（Coreference Resolution）:消除一些对文本处理没有意义的指代名词，减轻程序对语言的处理。\n6.词义消歧（Word Sense Disambiguation，WSD）:一个词他可能会有歧义，该任务是用来消除歧义的。\n7.机器翻译（Machine Translation，MT）:要实现文本的自动翻译\n8.自动文摘(Automatic Summarization):摘要是一大段文字，我们需要将里面的梗提取出来然后缩短方便阅读或方便提取信息。\n9.问答系统（Question Answering）:你提出一个问题机器给予你准确的答案\n10.OCR:也属于视觉模块内容，将图片当中的文字通过机器识别图像翻译成文本形式\n11.信息检索(Information Retrieval，IR):用户进行信息查询和获取的主要方式，是查找信息的方法和手段。\n自然语言处理核心问题是什么\n文本分类\n关键词提取\n情感分析\n语义消歧\n主题模型\n机器翻译\n问题问答\n汉语分词\n垂直领域的对话机器人\n自然语言处理有哪些应用方向\n搜索引擎\n文本主题/标签分类\n文本创作与生成\n机器翻译\n情感分析\n舆情监控\n语音识别系统\n对话机器人\n自然语言处理的难点是什么\n歧义问题:很多话的意思说的模棱两可，具有歧义\n知识问题:知识稀疏或者词汇稀疏，词汇稀疏导致了搭配稀疏，然后导致了语义稀疏，它有一个递进关系。一个比较出名的定律叫齐夫定律（Zipf Law），这个定律是说在自然语言语料当中，一个单词出现的频率和它在频率表当中的排名基本成一个反比关系。\n离散符号计算问题:我们看到的文本其实都是一些符号，对计算机来说，它看的其实也是一些离散的符号，但我们知道计算机其实最擅长的是数值型的运算，而不是符号的推理，并且符号之间的逻辑推理会非常复杂。\n语义本质的问题:到底什么是语义？什么是语义？语言里面到底是什么东西？符号背后真正的语义怎么来表示？语言学家他走的路子就是我构建好多形式化的、结构化的图之类的，这种结构去做语义或者是一些符号推导系统，认为它可以接近语义本质。但是，这些其实走得越远离计算机就越远，因为它越符号，语义的可解释性就会很差。拿数字来表示语义，我们也不知道这个数字到底它是什么东西。所以目前为止现在研究领域对这个问题解决得比较差。\n自然语言处理学习路线\n熟悉基本知识、基本操作\n如文本操作、正则、掌握一些基本文本处理框架英文有NLTK、spaCy，中文有中科院计算所NLPIR、哈工大LTP、清华大学THULAC、Hanlp分词器、Python jieba工具库\n知道什么是语言模型、利用语言模型来完成一些项目\n文本表示:将文本中的字符串转化为计算机当中的向量\n文本分类:分类模型传统的一个解决方法就是标带标注的语料，再特征提取，然后训分类器进行分类。这个分类器就会用比如说逻辑回归、贝叶斯、支持向量机、决策树等等。\n主题模型:使用无监督学习的方式对文本中的隐含语义进行聚类的统计模型\nseq2seq模型:通过深度神经网络将一个序列作为映射为另外一个输出的序列。\n文本生成:GAN文本生成，也叫机器人写作。","data":"2019年02月14日 20:40:03"}
{"_id":{"$oid":"5d36c1c96734bd8e681d64dd"},"title":"自然语言处理——8.2 功能合一文法(Function Unification Grammar, FUG)","author":"weixin_33743661","content":"提出起因\nChomsky 短语结构语法生成能力太强，产生许多不符合语法或有歧义的句子；\n标记十分简单，分析能力有限，难以反映自然语言的复杂特性。\nFUG 对短语结构语法的改进\n采用复杂特征集来描述词、句法规则、语义信息，以及句子的结构功能。\n试图以单一形式的结构模式来描述特征组合、功能分配、词条和组成成分的顺序，以达到对句子的完全功能描述。\n采用合一运算对复杂特征集进行运算。\n复杂特征集\n1. 复杂特征集功能描述的定义\n设为一个功能描述 (Functional Description)，当且仅当可以表示为：\n\n其中，表示特征名，表示特征值，且满足以下两个条件：\n(1) 特征名为原子，特征值为原子或另一个功能描述；\n(2) ，读作：复杂特征集中，特征的值等于 。\n2. 可以用复杂特征集描述词汇\n在词典中单词的特征可以包括词类、形态、句法和语义等多方面的信息，如：\n3. 可以用复杂特征集描述规则\n4. 可以用复杂特征集描述句子\n句子：\nWe helped her.\n5. 复杂特征集的特点\n(1) 允许利用多个特征描述同一个语言单位；\n(2) 从结构上看，复杂特征集是一种嵌套结构，可以有效地表示复杂词组或句子结构；\n(3) 特征名的定义及其相互关系具有明显的层次性，而所有自然语言的结构都是层次性的，复杂特征集的这一特点显然对语言的层次分析有益；\n(4)复杂特征集便于运算，两个复杂特征集通过合一运算可以产生另一个复杂特征集，这与句法分析中词组和句子的产生是一致的。\n合一运算\n1. 复杂特征集相容的定义\n若均为复杂特征集, 则是相容的, 当且仅当：\n(1) 如果，且都是原子，那么是相容的，当且仅当；\n(2) 如果 均为复杂特征集，是相容的，当且仅当 相容。\n2. 合一运算的递归定义\n(1) 在都是原子的情况下，如果，则, 否则；\n(2) 如果均为复杂特征集，则\n(a) 若，但 的值未经定义，则属于;\n(b) 若，但 的值未经定义，则属于;\n(c) 若，但，且与 相容(不相抵触)，则 属于，否则,。\n合一运算的作用\n(1) 合并原有的特征信息，构造新的特征结构；\n(2) 检查特征的相容性和规则执行的前提条件是否满足，如果参与合一的特征相冲突，就立即宣布合一失败。","data":"2018年10月09日 10:04:00"}
{"_id":{"$oid":"5d36c24b6734bd8e681d6504"},"title":"用Python进行自然语言处理 - 语言处理与Python","author":"Tonia_桐妹","content":"最近在看《Analyzing Text with the Natural Language Toolkit》的中文翻译版本，觉得蛮有意思的，就把学习过程中的遇到的问题和一些代码的运行结果记录下来。小白一只，如有错误，请您指正，谢谢！\n想要这本书资源的可以在评论区留下您的邮箱。\n下面进入正题（之前我已经装好了Python3.6版本）：\n第1章    语言处理与Python\n1.1 语言计算：文本和单词\nNLTK入门\n由于pip版本太老，先右键管理员身份打开cmd，根据提示输入 python -m pip install --upgrade pip 语句进行pip的更新。\n更新完毕，输入 pip install nltk 语句进行NLTK的安装。\n安装完毕，启动Python解释器。在Python提示符后输入以下命令：\n\u003e\u003e\u003eimport nltk\n\u003e\u003e\u003enltk.download()\n跳出以下界面：\n选中“book”这一行，点击“Download”。完成后，出现如下界面：\n关闭窗口。此时数据已经被下载到电脑上啦，你可以使用Python解释器去加载一些要用的文本。\n首先输入 from nltk.book import * ，即从NLTK的book模块加载all。\n若想找到这些文本，只需在\u003e\u003e\u003e后输入它们的名字即可。如：\n搜索文本\ntext1.concordance(\"monstrous\") 即搜索《白鲸记》中的词monstrous:\ntext2.concordance(\"affection\") 即搜索《理智与情感》中的词affection:\n通过上述的词语索引，我们可以看到其上下文。如text1中的monstrous，我们可以看到 the_____pictures (见下图红框)：\n可以通过 文本名.similar(\"关键词”) 语句来查看还有哪些词出现在相似的上下文中。\ncommon_contexts函数可以研究两个及以上的词的共同的上下文，如monstrous 和 very。\n判断词在文本中的位置\n从文本开头算起在它前面有多少个词，位置信息可以用离散图表示。图中，每一个竖线代表一个单词，每一行代表整个文本。\nPS：为了画图，我们需要安装Python的NumPy的Matplotlib包。\n如果没有安装，就会出现如下错误：\n通过Ctrl+Z 退出Python 解释器，然后通过 pip install matplotlib 语句进行下载安装。\n安装之后，进入Python解释器，记得导入nltk book模块的所有文本。然后输入如下语句（见黄框）：\n然后弹出一张图：\n关闭图片窗口后，输入另一条语句：\n弹出如下图片：\ngenerate函数\n不再适用，暂时没有替代的函数。如果有，希望哪位大佬可以告知。\n计数词汇\n使用len函数获取长度。（以文本中出现的词和标点符号为单位算出文本从头到尾的长度。）\n使用set函数获得词汇表。\n使用sorted(set(文本名称))得到一个词汇项的排序表\n再用len来获得这个数值。\n对文本词汇丰富度进行测量（需确保Python使用的是浮点除法）：\n计数一个词在文本中出现的次数，计算一个特定的词在文本中占据的百分比。\n定义函数\n用关键字def定义新函数：\n使用这些函数：\n1.8 练习\n1、尝试使用 Python 解释器作为一个计算器，输入表达式，如 12/(4+1)。\n2、26 个字母可以组成 26 的 10 次方或者 26**10 个 10 字母长的字符串。也就是 1411 67095653376L（结尾处的 L 只是表示这是 Python 长数字格式）。100 个字母长度的 字符串可能有多少个？\n3、Python 乘法运算可应用于链表。当你输入['Monty', 'Python'] * 20 或者 3 * sent1 会发生什么？\n4、复习 1.1 节关于语言计算的内容。在 text2 中有多少个词？有多少个不同的词？\n5、比较表格 1-1 中幽默和言情小说的词汇多样性得分，哪一个文体中词汇更丰富？\n言情小说的词汇更丰富。\n6、制作《理智与情感》中四个主角：Elinor，Marianne，Edward 和 Willoughby 的分布图。 在这部小说中关于男性和女性所扮演的不同角色，你能观察到什么？你能找出一对夫妻 吗？\n可以看出Elinor和Marianne出现的频率很高，且近乎贯穿全文，所以我认为两者是主角。\n个人原以为Elinor和Marianne是cp，但参考百度之后，发现她俩是姐妹关系，Elinor和Edward才是cp。（我猜是我忽视了性别，前两位是女，后两位是男。如果有大佬知道怎么从图上看出来的请告诉我）附上剧情简介。\n7、查找 text5 中的搭配\n8、思考下面的 Python 表达式：len(set(text4))。说明这个表达式的用途。描述在执行 此计算中涉及的两个步骤。\n这个表达式的用途是统计text4文本中有多少个不同的标识符。\n此计算涉及两个步骤，一是获得text4的词汇表，二是求取词汇表的长度\n9、复习 1.2 节关于链表和字符串的内容。\na. 定义一个字符串，并且将它分配给一个变量，如：my_string = 'My String'(在字符串中放一些更有趣的东西）。用两种方法输出这个变量的内容，一种是通过简 单地输入变量的名称，然后按回车；另一种是通过使用 print 语句。\nb. 尝试使用 my_string+ my_string 或者用它乘以一个数将字符串添加到它自身， 例如：my_string* 3。请注意，连接在一起的字符串之间没有空格。怎样能解决这个问题？\n10、使用的语法 my_sent=[\"My\", \"sent\"]，定义一个词链表变量 my_sent（用你自己的词或喜欢的话）。\na. 使用' '.join(my_sent)将其转换成一个字符串。\nb. 使用 split()在你指定的地方将字符串分割回链表。\n11、定义几个包含词链表的变量，例如：phrase1，phrase2 等。将它们连接在一起组成不同的组合（使用加法运算符），最终形成完整的句子。len(phrase1 + phrase2) 与 len(phrase1) + len(phrase2)之间的关系是什么？\nlen(phrase1 + phrase2) = len(phrase1) + len(phrase2)\n12、考虑下面两个具有相同值的表达式。哪一个在 NLP 中更常用？为什么？\na. \"Monty Python\"[6:12]\nb. [\"Monty\", \"Python\"][1]\nb在NLP中更常用，因为NLP的操作是基于词汇的。\n13、我们已经看到如何用词链表表示一个句子，其中每个词是一个字符序列。sent1[2][2] 代表什么意思？为什么？请用其他的索引值做实验。\nsent1[2][2]代表text1文本第一句的第三个词汇的第三个字符，即‘‘h’’\nsent5[3][4] 代表text5文本的第一句话的第四个词汇的第五个字符，即“l”\n14、 在变量 sent3 中保存的是 text3 的第一句话。在 sent3 中 the 的索引值是 1，因为 sent3[1]的值是“the”。sent3 中“the”的其它出现的索引值是多少？\n15、复习1.4节讨论的条件语句。在聊天语料库（text5）中查找所有以字母 b 开头的词。 按字母顺序显示出来。\n16、 在Python解释器提示符下输入表达式 range(10)。再尝试 range(10, 20), range (10, 20, 2)和 range(20, 10, -2)。在后续章节中我们将看到这个内置函数的多用用途。\n17、使用 text9.index()查找词 sunset 的索引值。你需要将这个词作为一个参数插入到圆括号之间。通过尝试和出错的过程中，找到完整的句子中包含这个词的切片。\n18、 使用链表加法、set 和 sorted 操作，计算句子 sent1...sent8 的词汇表。\n19、 下面两行之间的差异是什么？哪一个的值比较大？其他文本也是同样情况吗？\n\u003e\u003e\u003e sorted(set([w.lower() for w in text1]))\n\u003e\u003e\u003e sorted([w.lower() for w in set(text1)]\n后者的值较大，因为后者还包含一些大小写不同的单词。\n20、w.isupper()和 not w.islower()这两个测试之间的差异是什么？\n前者是测试w中所有字符是否都是大写字母\n后者是测试w中的所有字符是否不都是小写字母，即判断是否是大小写掺杂或者全是大写。\n21、写一个切片表达式提取 text2 中最后两个词。\n22、 找出聊天语料库（text5）中所有四个字母的词。使用频率分布函数（FreqDist）， 以频率从高到低显示这些词。\n23、复习 1.4 节中条件循环的讨论。使用 for 和 if 语句组合循环遍历《巨蟒和圣杯》（text6）的电影剧本中的词，输出所有的大写词，每行输出一个。\n24、写表达式找出 text6 中所有符合下列条件的词。结果应该是词链表的形式：['word 1', 'word2', ...]。\na. 以 ize 结尾\nb. 包含字母 z\nc. 包含字母序列 pt\nd. 除了首字母外是全部小写字母的词（即 titlecase）\n25、定义 sent 为词链表['she', 'sells', 'sea', 'shells', 'by', 'the', 'sea', 'shore']。 编写代码执行以下任务：\na. 输出所有 sh 开头的单词\nb. 输出所有长度超过 4 个字符的词\n26、下面的 Python 代码是做什么的？sum([len(w) for w in text1])，你可以用它来 算出一个文本的平均字长吗？\n该代码是用来求文本中所有的字长总和。\n27、定义一个名为 vocab_size(text)的函数，以文本作为唯一的参数，返回文本的词汇量。\n28、定义一个函数 percent(word, text)，计算一个给定的词在文本中出现的频率，结果以百分比表示。\n29、我们一直在使用集合存储词汇表。试试下面的 Python 表达式：set(sent3) \u003c set(text1)。\n实验在 set()中使用不同的参数。它是做什么用的？你能想到一个实际的应用吗？\nsent3的词汇集合属于text1的词汇集合。\nset(a)\u003cset(b)用来判断a词汇集合是否为b词汇集合的子集。","data":"2018年09月04日 12:46:47"}
{"_id":{"$oid":"5d36c2656734bd8e681d6511"},"title":"[转载]自然语言处理如何入门？ ——周明博士","author":"阶艺勿听","content":"作者：微软亚洲研究院\n链接：https://www.zhihu.com/question/19895141/answer/149475410\n来源：知乎\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n自然语言处理（简称NLP），是研究计算机处理人类语言的一门技术，包括：\n1.句法语义分析：对于给定的句子，进行分词、词性标记、命名实体识别和链接、句法分析、语义角色识别和多义词消歧。\n2.信息抽取：从给定文本中抽取重要的信息，比如，时间、地点、人物、事件、原因、结果、数字、日期、货币、专有名词等等。通俗说来，就是要了解谁在什么时候、什么原因、对谁、做了什么事、有什么结果。涉及到实体识别、时间抽取、因果关系抽取等关键技术。\n3.文本挖掘（或者文本数据挖掘）：包括文本聚类、分类、信息抽取、摘要、情感分析以及对挖掘的信息和知识的可视化、交互式的表达界面。目前主流的技术都是基于统计机器学习的。\n4.机器翻译：把输入的源语言文本通过自动翻译获得另外一种语言的文本。根据输入媒介不同，可以细分为文本翻译、语音翻译、手语翻译、图形翻译等。机器翻译从最早的基于规则的方法到二十年前的基于统计的方法，再到今天的基于神经网络（编码-解码）的方法，逐渐形成了一套比较严谨的方法体系。\n5.信息检索：对大规模的文档进行索引。可简单对文档中的词汇，赋之以不同的权重来建立索引，也可利用1，2，3的技术来建立更加深层的索引。在查询的时候，对输入的查询表达式比如一个检索词或者一个句子进行分析，然后在索引里面查找匹配的候选文档，再根据一个排序机制把候选文档排序，最后输出排序得分最高的文档。\n6.问答系统： 对一个自然语言表达的问题，由问答系统给出一个精准的答案。需要对自然语言查询语句进行某种程度的语义分析，包括实体链接、关系识别，形成逻辑表达式，然后到知识库中查找可能的候选答案并通过一个排序机制找出最佳的答案。\n7.对话系统：系统通过一系列的对话，跟用户进行聊天、回答、完成某一项任务。涉及到用户意图理解、通用聊天引擎、问答引擎、对话管理等技术。此外，为了体现上下文相关，要具备多轮对话能力。同时，为了体现个性化，要开发用户画像以及基于用户画像的个性化回复。\n随着深度学习在图像识别、语音识别领域的大放异彩，人们对深度学习在NLP的价值也寄予厚望。再加上AlphaGo的成功，人工智能的研究和应用变得炙手可热。自然语言处理作为人工智能领域的认知智能，成为目前大家关注的焦点。很多研究生都在进入自然语言领域，寄望未来在人工智能方向大展身手。但是，大家常常遇到一些问题。俗话说，万事开头难。如果第一件事情成功了，学生就能建立信心，找到窍门，今后越做越好。否则，也可能就灰心丧气，甚至离开这个领域。这里针对给出我个人的建议，希望我的这些粗浅观点能够引起大家更深层次的讨论。\n建议1：如何在NLP领域快速学会第一个技能？\n我的建议是：找到一个开源项目，比如机器翻译或者深度学习的项目。理解开源项目的任务，编译通过该项目发布的示范程序，得到与项目示范程序一致的结果。然后再深入理解开源项目示范程序的算法。自己编程实现一下这个示范程序的算法。再按照项目提供的标准测试集测试自己实现的程序。如果输出的结果与项目中出现的结果不一致，就要仔细查验自己的程序，反复修改，直到结果与示范程序基本一致。如果还是不行，就大胆给项目的作者写信请教。在此基础上，再看看自己能否进一步完善算法或者实现，取得比示范程序更好的结果。\n建议2：如何选择第一个好题目？\n工程型研究生，选题很多都是老师给定的。需要采取比较实用的方法，扎扎实实地动手实现。可能不需要多少理论创新，但是需要较强的实现能力和综合创新能力。而学术型研究生需要取得一流的研究成果，因此选题需要有一定的创新。我这里给出如下的几点建议。\n先找到自己喜欢的研究领域。你找到一本最近的ACL会议论文集, 从中找到一个你比较喜欢的领域。在选题的时候，多注意选择蓝海的领域。这是因为蓝海的领域，相对比较新，容易出成果。\n充分调研这个领域目前的发展状况。包括如下几个方面的调研：方法方面，是否有一套比较清晰的数学体系和机器学习体系；数据方面，有没有一个大家公认的标准训练集和测试集；研究团队，是否有著名团队和人士参加。如果以上几个方面的调研结论不是太清晰，作为初学者可能不要轻易进入。\n在确认进入一个领域之后，按照建议一所述，需要找到本领域的开源项目或者工具，仔细研究一遍现有的主要流派和方法，先入门。\n反复阅读本领域最新发表的文章，多阅读本领域牛人发表的文章。在深入了解已有工作的基础上，探讨还有没有一些地方可以推翻、改进、综合、迁移。注意做实验的时候，不要贪多，每次实验只需要验证一个想法。每次实验之后，必须要进行分析存在的错误，找出原因。\n对成功的实验，进一步探讨如何改进算法。注意实验数据必须是业界公认的数据。\n与已有的算法进行比较，体会能够得出比较一般性的结论。如果有，则去写一篇文章，否则，应该换一个新的选题。\n建议3：如何写出第一篇论文？\n接上一个问题，如果想法不错，且被实验所证明，就可开始写第一篇论文了。\n确定论文的题目。在定题目的时候，一般不要“…系统”、“…研究与实践”，要避免太长的题目，因为不好体现要点。题目要具体，有深度，突出算法。\n写论文摘要。要突出本文针对什么重要问题，提出了什么方法，跟已有工作相比，具有什么优势。实验结果表明，达到了什么水准，解决了什么问题。\n写引言。首先讲出本项工作的背景，这个问题的定义，它具有什么重要性。然后介绍对这个问题，现有的方法是什么，有什么优点。但是（注意但是）现有的方法仍然有很多缺陷或者挑战。比如（注意比如），有什么问题。本文针对这个问题，受什么方法（谁的工作）之启发，提出了什么新的方法并做了如下几个方面的研究。然后对每个方面分门别类加以叙述，最后说明实验的结论。再说本文有几条贡献，一般写三条足矣。然后说说文章的章节组织，以及本文的重点。有的时候东西太多，篇幅有限，只能介绍最重要的部分，不需要面面俱到。\n相关工作。对相关工作做一个梳理，按照流派划分，对主要的最多三个流派做一个简单介绍。介绍其原理，然后说明其局限性。\n然后可设立两个章节介绍自己的工作。第一个章节是算法描述。包括问题定义，数学符号，算法描述。文章的主要公式基本都在这里。有时候要给出简明的推导过程。如果借鉴了别人的理论和算法，要给出清晰的引文信息。在此基础上，由于一般是基于机器学习或者深度学习的方法，要介绍你的模型训练方法和解码方法。第二章就是实验环节。一般要给出实验的目的，要检验什么，实验的方法，数据从哪里来，多大规模。最好数据是用公开评测数据，便于别人重复你的工作。然后对每个实验给出所需的技术参数，并报告实验结果。同时为了与已有工作比较，需要引用已有工作的结果，必要的时候需要重现重要的工作并报告结果。用实验数据说话，说明你比人家的方法要好。要对实验结果好好分析你的工作与别人的工作的不同及各自利弊，并说明其原因。对于目前尚不太好的地方，要分析问题之所在，并将其列为未来的工作。\n结论。对本文的贡献再一次总结。既要从理论、方法上加以总结和提炼，也要说明在实验上的贡献和结论。所做的结论，要让读者感到信服，同时指出未来的研究方向。\n参考文献。给出所有重要相关工作的论文。记住，漏掉了一篇重要的参考文献（或者牛人的工作），基本上就没有被录取的希望了。\n写完第一稿，然后就是再改三遍。\n把文章交给同一个项目组的人士，请他们从算法新颖度、创新性和实验规模和结论方面，以挑剔的眼光，审核你的文章。自己针对薄弱环节，进一步改进，重点加强算法深度和工作创新性。\n然后请不同项目组的人士审阅。如果他们看不明白，说明文章的可读性不够。你需要修改篇章结构、进行文字润色，增加文章可读性。\n如投ACL等国际会议，最好再请英文专业或者母语人士提炼文字。","data":"2018年07月18日 11:08:48"}
{"_id":{"$oid":"5d36c2706734bd8e681d6515"},"title":"奋战聊天机器人（四）自然语言处理中的文本分类","author":"钟shi杰","content":"文本分类是机器学习在自然语言处理中的最常用也是最基础的应用，机器学习相关内容可以直接看我的有关scikit-learn相关教程，本节直接涉及nltk中的机器学习相关内容\n预备\n机器学习的过程是训练模型和使用模型的过程，训练就是基于已知数据做统计学习，使用就是用统计学习好的模型来计算未知的数据。\n机器学习分为有监督学习和无监督学习，文本分类也分为有监督的分类和无监督的分类。有监督就是训练的样本数据有了确定的判断，基于这些已有的判断来断定新的数据，无监督就是训练的样本数据没有什么判断，完全自发的生成结论。\n无论监督学习还是无监督学习，都是通过某种算法来实现，而这种算法可以有多重选择，贝叶斯就是其中一种。在多种算法中如何选择最适合的，这才是机器学习最难的事情，也是最高境界。\nnltk中的贝叶斯分类器\n贝叶斯是概率论的鼻祖，贝叶斯定理是关于随机事件的条件概率的一则定理，贝叶斯公式是：\nP(B|A)=P(A|B)P(B)/P(A)；即,已知P(A|B)，P(A)和P(B)可以计算出P(B|A)。\n贝叶斯分类器就是基于贝叶斯概率理论设计的分类器算法，nltk库中已经实现，具体用法如下：\n# encoding:utf-8 import nltk my_train_set = [ ({'feature1': u'a'}, '1'), ({'feature1': u'a'}, '2'), ({'feature1': u'a'}, '3'), ({'feature1': u'a'}, '3'), ({'feature1': u'b'}, '2'), ({'feature1': u'b'}, '2'), ({'feature1': u'b'}, '2'), ({'feature1': u'b'}, '2'), ({'feature1': u'b'}, '2'), ({'feature1': u'b'}, '2'), ] classifier = nltk.NaiveBayesClassifier.train(my_train_set) print(classifier.classify({'feature1': u'a'})) print(classifier.classify({'feature1': u'b'}))\n文档分类\n不管是什么分类，最重要的是要知道哪些特征最能反映这个分类的特点，也就是特征选取。\n文档分类使用的特征就最能代表这个分类的词\n因为对文档分类要经过训练和预测两个过程，而特征的提取是这两个过程都需要的，所以，习惯上我们会把特征提取单独抽象出来作为一个公共方法，比如：\nfrom nltk.corpus import movie_reviews all_words = nltk.FreeDist(w.lower() for w in movie_reviews.words()) word_features = all_words.keys()[:2000] def document_features(document): for word in word)features: features['contains(%s)' % word] = (word in document_words) return features\n这是一个简单的特征提取过程，前两行找到movie_reviews语料库中出现词频最高的2000个词作为特征，下面定义的函数就是特征提取函数，每个特征都是形如contains(*)的key，value就是True或False，表示这个词是否在文档中出现\n那么我们训练的过程就是：\nfeaturesets = [(document_features(d), c) for (d,c) in documents] classifier = nltk.NaiveBayesClassifier.train(featuresets)\n要预测一个新的文档时：\nclassifier.classify(document_features(d))\n通过\nclassifier.show_most_informative_features(5)\n可以找到最优信息量的特征，这对我们选取特征是非常有帮助的\n其他文本分类\n文本分类除了文档分类外还有许多其他类型的分类，比如：\n词性标注：属于一种文本分类，一般是基于上下文语境的文本分类\n句子分割：属于标点符号的分类任务，它的特征一般选取为单独句子标识符的合并链表、数据特征（下一个词是否大写、前一个词是什么、前一个词长度……）\n识别对话行为类型：对话行为类型是指问候、问题、回答、断言、说明等\n识别文字蕴含：即一个句子是否能得出另外一个句子的结论，这可以认为是真假标签的分类任务。这是一个有挑战的事情\n参考资料来源：http://www.shareditor.com/","data":"2017年09月01日 13:50:47"}
{"_id":{"$oid":"5d36c27b6734bd8e681d651d"},"title":"自然语言处理之语言模型（LM）","author":"Soyoger","content":"经过几天对nlp的理解，接下来我们说说语言模型，下面还是以PPT方式给出。\n一、统计语言模型\n1、什么是统计语言模型？\n一个语言模型通常构建为字符串s的概率分布p(s)，这里的p(s)实际上反映的是s作为一个句子出现的概率。\n这里的概率指的是组成字符串的这个组合，在训练语料中出现的似然，与句子是否合乎语法无关。假设训练语料来自于人类的语言，那么可以认为这个概率是的是一句话是否是人话的概率。\n2、怎么建立统计语言模型？\n对于一个由T个词按顺序构成的句子，p(s)实际上求解的是字符串的联合概率，利用贝叶斯公式，链式分解如下：\n\n\n从上面可以看到，一个统计语言模型可以表示成，给定前面的的词，求后面一个词出现的条件概率。\n我们在求p(s)时实际上就已经建立了一个模型，这里的p(*)就是模型的参数，如果这些参数已经求解得到，那么很容易就能够得到字符串s的概率。\n3、求解的问题\n假定字符串s为“i want to drink some water”，那么根据上面所建立的模型：\n\n\n问题归结为如何求解上面的每一个概率，比如，一种比较直观的方法就是分别计算出“I want to”和“I want to drink”在语料中出现的频数，然后再用除法：\n\n\n看起来好像很美好，实际上这里存在两个问题：\n（1）自由参数数目：\n假定字符串中字符全部来自与大小为V的词典，上述例子中我们需要计算所有的条件概率，对于所有的条件概率，这里的w都有V种取值，那么实际上这个模型的自由参数数目量级是V^6，6为字符串的长度。\n从上面可以看出，模型的自由参数是随着字符串长度的增加而指数级暴增的，这使我们几乎不可能正确的估计出这些参数。\n（2）数据稀疏性：\n从上面可以看到，每一个w都具有V种取值，这样构造出了非常多的词对，但实际中训练语料是不会出现这么多种组合的，那么依据最大似然估计，最终得到的概率实际是很可能是0。\n4、怎么解决？\n上面提出了传统统计语言模型的两个问题，后面分别介绍两种方法进行求解：N-gram语言模型，神经概率语言模型\n二、N-gram语言模型\n1、什么是N-gram语言模型？\n为了解决自由参数数目过多的问题，引入了马尔科夫假设：随意一个词出现的概率只与它前面出现的有限的n个词有关。基于上述假设的统计语言模型被称为N-gram语言模型。\n2、如何确定N的取值？\n通常情况下，n的取值不能够太大，否则自由参数过多的问题依旧存在：\n（1）当n=1时，即一个词的出现与它周围的词是独立，这种我们称为unigram，也就是一元语言模型，此时自由参数量级是词典大小V。\n（2）当n=2时，即一个词的出现仅与它前面的一个词有关时，这种我们称为bigram，叫二元语言模型，也叫一阶马尔科夫链，此时自由参数数量级是V^2。\n（3）当n=3时，即一个词的出现仅与它前面的两个词有关，称为trigram，叫三元语言模型，也叫二阶马尔科夫链，此时自由参数数量级是V^3。\n一般情况下只使用上述取值，因为从上面可以看出，自由参数的数量级是n取值的指数倍。\n从模型的效果来看，理论上n的取值越大，效果越好。但随着n取值的增加，效果提升的幅度是在下降的。同时还涉及到一个可靠性和可区别性的问题，参数越多，可区别性越好，但同时单个参数的实例变少从而降低了可靠性。\n3、建模与求解\nN-gram语言模型的求解跟传统统计语言模型一致，都是求解每一个条件概率的值，简单计算N元语法在语料中出现的频率，然后归一化。\n4、平滑化\n我们在传统统计语言模型提出了两个问题：自由参数数目和数据稀疏，上述N-gram只是解决了第一个问题，而平滑化就是为了解决第二个问题。\n假设有一个词组在训练语料中没有出现过，那么它的频次就为0，但实际上能不能认为它出现的概率为0呢？显然不可以，我们无法保证训练语料的完备性。那么，解决的方法是什么？如果我们默认每一个词组都出现1次呢，无论词组出现的频次是多少，都往上加1，这就能够解决概率为0的问题了。\n上述的方法就是加1平滑，也称为拉普拉斯平滑。平滑化还有许多方法，这里就不展开介绍了：\n（1）加法平滑\n（2）古德-图灵平滑\n（3）K平滑\n三、神经概率语言模型\n1、前置知识\n在N-gram语言模型中，计算条件概率的方法是简单的用词频做除法然后归一化。\n在机器学习的领域中，通用的做法是：对所考虑的问题建模后先为其构造一个目标函数，然后对这个目标函数进行优化，从而求得一组最优的参数，最后再利用这组参数对应的模型来进行预测。\n那么在上述的语言模型中，利用最大化对数似然，将目标函数设为：\n\n\nContext代表词w的上下文，对应N-gram就是词w的前N-1个词。之后对目标函数进行最大化，由上可见，概率实际上是w和的函数：\n\n\n其中θ为待定参数集，这样将计算所有的条件概率转化为了最优化目标函数，求解得到θ的过程。通过选取合适模型可以使得θ参数的个数远小于N-gram模型中参数的个数。\n2、什么是神经概率语言模型？\nBegio等人在2003年发表的A Neural Probabilistic Language Model，里面详解了这个方法。\n基本的思想其实与上述的前置知识有所联系，既然是神经概率语言模型，那么实现的时候自然有一个神经网络，结构图如下：\n\n\n\n它包括了四个层：输入层、投影层、隐藏层和输出层。\n2、计算流程\n（1）输入层\n这里就是词w的上下文，如果用N-gram的方法就是词w的前n-1个词了。每一个词都作为一个长度为V的one-hot向量传入神经网络中\n（2）投影层\n在投影层中，存在一个look-up表C，C被表示成一个V*m的自由参数矩阵，其中V是词典的大小，而m作为自定义的参数，一般是10^2的倍数。\n表C中每一行都作为一个词向量存在，这个词向量可以理解为每一个词的另一种分布式表示。每一个one-hot向量都经过表C的转化变成一个词向量。\nn-1个词向量首尾相接的拼起来，转化为(n-1)m的列向量输入到下一层。\n（3）隐藏层、输出层\n之后再对列向量进行计算，大致如下：\n\n\n其中tanh是激活函数，是为归一化的log概率，之后再用softmax进行归一化，就得到最终的概率输出了。\n在前置知识中我们提到了参数θ，那么在神经网络中，实际的参数如下：\n词向量：v(w)，w以及填充向量\n神经网络参数：W，p，U，q\n3、最后\n在传统统计语言模型中，我们提出两个问题：自由参数数目和数据稀疏。\n这里在实际上使用参数θ代替了自由参数指数级的求解，而数据稀疏问题，我们在最后使用softmax进行归一化，求解出来的概率是平滑的，所以也解决了这个问题。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n参考：（PPT来源小象学院史兴老师）","data":"2018年04月29日 17:06:02"}
{"_id":{"$oid":"5d36c2856734bd8e681d6521"},"title":"第01课：中文自然语言处理的完整流程","author":"凌洪涛","content":"第一步：获取语料\n语料，即语言材料。语料是语言学研究的内容。语料是构成语料库的基本单元。所以，人们简单地用文本作为替代，并把文本中的上下文关系作为现实世界中语言的上下文关系的替代品。我们把一个文本集合称为语料库（Corpus），当有几个这样的文本集合的时候，我们称之为语料库集合(Corpora)。（定义来源：百度百科）按语料来源，我们将语料分为以下两种：\n1.已有语料\n很多业务部门、公司等组织随着业务发展都会积累有大量的纸质或者电子文本资料。那么，对于这些资料，在允许的条件下我们稍加整合，把纸质的文本全部电子化就可以作为我们的语料库。\n2.网上下载、抓取语料\n如果现在个人手里没有数据怎么办呢？这个时候，我们可以选择获取国内外标准开放数据集，比如国内的中文汉语有搜狗语料、人民日报语料。国外的因为大都是英文或者外文，这里暂时用不到。也可以选择通过爬虫自己去抓取一些数据，然后来进行后续内容。\n第二步：语料预处理\n这里重点介绍一下语料的预处理，在一个完整的中文自然语言处理工程应用中，语料预处理大概会占到整个50%-70%的工作量，所以开发人员大部分时间就在进行语料预处理。下面通过数据洗清、分词、词性标注、去停用词四个大的方面来完成语料的预处理工作。\n1.语料清洗\n数据清洗，顾名思义就是在语料中找到我们感兴趣的东西，把不感兴趣的、视为噪音的内容清洗删除，包括对于原始文本提取标题、摘要、正文等信息，对于爬取的网页内容，去除广告、标签、HTML、JS 等代码和注释等。常见的数据清洗方式有：人工去重、对齐、删除和标注等，或者规则提取内容、正则表达式匹配、根据词性和命名实体提取、编写脚本或者代码批处理等。\n2.分词\n中文语料数据为一批短文本或者长文本，比如：句子，文章摘要，段落或者整篇文章组成的一个集合。一般句子、段落之间的字、词语是连续的，有一定含义。而进行文本挖掘分析时，我们希望文本处理的最小单位粒度是词或者词语，所以这个时候就需要分词来将文本全部进行分词。\n常见的分词算法有：基于字符串匹配的分词方法、基于理解的分词方法、基于统计的分词方法和基于规则的分词方法，每种方法下面对应许多具体的算法。\n当前中文分词算法的主要难点有歧义识别和新词识别，比如：“羽毛球拍卖完了”，这个可以切分成“羽毛 球拍 卖 完 了”，也可切分成“羽毛球 拍卖 完 了”，如果不依赖上下文其他的句子，恐怕很难知道如何去理解。\n3.词性标注\n词性标注，就是给每个词或者词语打词类标签，如形容词、动词、名词等。这样做可以让文本在后面的处理中融入更多有用的语言信息。词性标注是一个经典的序列标注问题，不过对于有些中文自然语言处理来说，词性标注不是非必需的。比如，常见的文本分类就不用关心词性问题，但是类似情感分析、知识推理却是需要的，下图是常见的中文词性整理。\n常见的词性标注方法可以分为基于规则和基于统计的方法。其中基于统计的方法，如基于最大熵的词性标注、基于统计最大概率输出词性和基于 HMM 的词性标注。\n4.去停用词\n停用词一般指对文本特征没有任何贡献作用的字词，比如标点符号、语气、人称等一些词。所以在一般性的文本处理中，分词之后，接下来一步就是去停用词。但是对于中文来说，去停用词操作不是一成不变的，停用词词典是根据具体场景来决定的，比如在情感分析中，语气词、感叹号是应该保留的，因为他们对表示语气程度、感情色彩有一定的贡献和意义。\n第三步：特征工程\n做完语料预处理之后，接下来需要考虑如何把分词之后的字和词语表示成计算机能够计算的类型。显然，如果要计算我们至少需要把中文分词的字符串转换成数字，确切的说应该是数学中的向量。有两种常用的表示模型分别是词袋模型和词向量。\n词袋模型（Bag of Word, BOW)，即不考虑词语原本在句子中的顺序，直接将每一个词语或者符号统一放置在一个集合（如 list），然后按照计数的方式对出现的次数进行统计。统计词频这只是最基本的方式，TF-IDF 是词袋模型的一个经典用法。\n词向量是将字、词语转换成向量矩阵的计算模型。目前为止最常用的词表示方法是 One-hot，这种方法把每个词表示为一个很长的向量。这个向量的维度是词表大小，其中绝大多数元素为 0，只有一个维度的值为 1，这个维度就代表了当前的词。还有 Google 团队的 Word2Vec，其主要包含两个模型：跳字模型（Skip-Gram）和连续词袋模型（Continuous Bag of Words，简称 CBOW），以及两种高效训练的方法：负采样（Negative Sampling）和层序 Softmax（Hierarchical Softmax）。值得一提的是，Word2Vec 词向量可以较好地表达不同词之间的相似和类比关系。除此之外，还有一些词向量的表示方式，如 Doc2Vec、WordRank 和 FastText 等。\n第四步：特征选择\n同数据挖掘一样，在文本挖掘相关问题中，特征工程也是必不可少的。在一个实际问题中，构造好的特征向量，是要选择合适的、表达能力强的特征。文本特征一般都是词语，具有语义信息，使用特征选择能够找出一个特征子集，其仍然可以保留语义信息；但通过特征提取找到的特征子空间，将会丢失部分语义信息。所以特征选择是一个很有挑战的过程，更多的依赖于经验和专业知识，并且有很多现成的算法来进行特征的选择。目前，常见的特征选择方法主要有 DF、 MI、 IG、 CHI、WLLR、WFO 六种。\n第五步：模型训练\n在特征向量选择好之后，接下来要做的事情当然就是训练模型，对于不同的应用需求，我们使用不同的模型，传统的有监督和无监督等机器学习模型， 如 KNN、SVM、Naive Bayes、决策树、GBDT、K-means 等模型；深度学习模型比如 CNN、RNN、LSTM、 Seq2Seq、FastText、TextCNN 等。这些模型在后续的分类、聚类、神经序列、情感分析等示例中都会用到，这里不再赘述。下面是在模型训练时需要注意的几个点。\n1.注意过拟合、欠拟合问题，不断提高模型的泛化能力。\n过拟合：模型学习能力太强，以至于把噪声数据的特征也学习到了，导致模型泛化能力下降，在训练集上表现很好，但是在测试集上表现很差。\n常见的解决方法有：\n增大数据的训练量；\n增加正则化项，如 L1 正则和 L2 正则；\n特征选取不合理，人工筛选特征和使用特征选择算法；\n采用 Dropout 方法等。\n欠拟合：就是模型不能够很好地拟合数据，表现在模型过于简单。\n常见的解决方法有：\n添加其他特征项；\n增加模型复杂度，比如神经网络加更多的层、线性模型通过添加多项式使模型泛化能力更强；\n减少正则化参数，正则化的目的是用来防止过拟合的，但是现在模型出现了欠拟合，则需要减少正则化参数。\n2.对于神经网络，注意梯度消失和梯度爆炸问题。","data":"2018年06月26日 13:44:07"}
{"_id":{"$oid":"5d36c2fd6734bd8e681d6541"},"title":"bat 2018自然语言处理校园招聘的要求","author":"最小森林","content":"寻找了多家国内主要IT公司有关NLP的2018校园招聘，于大家分享。另外查漏补缺，看看自己缺乏那些方面的经验和技术。\n下面直接放结论，没时间的可以只看总结：\n总结：\n主要在以下几个方面有要求，打勾的次数反应了热度：\n1.自然语言处理相关的具体操作：分词、语义、句意、对话、机器翻译、自动问答等√√√√√\n2.经典的机器学习算法、竞赛经历√√√\n3.多线程、网络编程、分布式编程√\n4.hadoop、spark√√√√\n5.SQL、NoSQL√\n6.linux√√\n前面3个更面向纯NLP，而后面的知识偏向数据分析，其实这两个岗位相辅相成，很多技能都是共通的。\n\n\n\n\n2018 阿里\n算法工程师-自然语言处理 Software engineer -Natural Language Processing\n岗位描述Job Description\n阿里巴巴广阔的商业生态需要丰富且深入的的自然语言处理技术，涵盖底层文本知识库建设、词法分析、句法分析、语义分析、文档分析、深度文本表示、文本生成、机器翻译、智能对话等。阿里巴巴的自然语言处理技术正在推进平台化、服务化策略，不断追求技术的深度以及技术与业务的适当解耦。本岗位需要招聘自然语言处理专业的优秀本科、硕士、博士毕业生一起来夯实基础、赋能商业，实现技术与商业的完美结合。期待追求卓越、自我驱动、聪明、乐观、自省、皮实的优秀人士加入阿里巴巴，共同开创人工智能的商业新格局。\n具体职责包括但不限于：\n1、紧跟业界最新自然语言处理技术动态，深入研发自然语言处理相关的知识库、词法、句法、语义、文档、深度学习、机器翻译、智能对话等技术，包括模块的实际开发以及对接自然语言处理平台的接入；\n2、理解自然语言处理技术应用的相关的业务场景及需求，在自然语言处理技术内核的基础上考虑业务场景的特殊性进而适当适配业务需求；\n3、在核心技术研发之外，也会适当参与到具体的NLP相关业务中，例如搜索Query分析、智能对话的语义解析及意图理解、商品评价的语义理解、内容搜索推荐的结构化分析、商品搜索推荐的标签体系、社会化问答的文本分析、智能客服的场景定制等；\n岗位要求Qualifications\n1、本科及以上学历，硕士博士优先，计算机、数学、信息管理等相关专业；\n2、具备极佳的工程实现能力，精通C/C++、Java、Python、Perl等至少一门语言；\n3、精通自然语言处理领域的1到2项底层技术，有实际成果并发表在自然语言处理国际顶级会议、期刊者优先，有在相关的自然语言处理竞赛中获得优异成绩者优先；\n4、熟悉深度学习以及常见机器学习算法的原理与算法，能熟练运用聚类、分类、回归、排序等模型解决有挑战性的问题，有大数据处理的实战经验；\n5、有强烈求知欲，对人工智能领域相关技术有热情；\n6、具有良好的数学基础，良好的英语阅读能力；\n7、有团队意识，与他人合作良好，最好具有团队协作的经验。\n工作地点Location\n成都市(Chengdu),上海市(Shanghai),杭州市(Hangzhou),北京市(Beijing),广州市(Guangzhou)\n参加面试的城市或地区Interview City or Region\n远程(Remote Interviews)\n\n\n\n\n\n\n2018 腾讯\n\n\n岗位描述\n腾讯拥有上亿量级的产品数据、极其丰富的产品场景、超大规模的计算资源、全谱领域的深厚技术积累、追求极致的创新氛围、适宜年轻人的企业文化，可为您提供充分的专业发挥空间，让您有可能做出影响整个互联网行业发展的优秀成果。\n\n\n该岗位主要职责包括但不限于：\n1、 负责词法分析、自动对话、语义挖掘和语言逻辑等相关研究工作；\n2、 负责自然语言处理的算法研发，包括但不限于语义分析、意图识别、人机对话、机器翻译、知识图谱、命名实体识别等；\n3、 负责NLP前沿问题的研究，结合未来实际应用场景，提供技术解决方案。\n岗位要求\n1、 计算机、应用数学、模式识别、人工智能、自控、统计学、运筹学、生物信息、物理学/量子计算、神经科学、社会学/心理学等专业，本科及以上，博士优先；\n2、 熟悉至少一种编程语言，包括但不限于java、C/C++、C#、python等；\n3、 在学术会议ACL、EMNLP、NAACL、COLING、IJCAI、AAAI等发表过文章，有深度学习学术或工程项目经验优先；\n4、 熟悉自然语言处理领域的1到2项底层技术，有实际成果并发表在自然语言处理国际顶级会议、期刊者优先，有在相关的自然语言处理竞赛中获得优异成绩者优先。\n工作地点\n深圳 北京 上海 广州 成都\n招聘城市\n哈尔滨 杭州 北京 南京 远程面试\n\n\n\n\n\n\n\n\n2018 百度\n\n\n北京-机器学习/数据挖掘/自然语言处理工程师\n所属部门: 百度\n工作地点: 北京市\n招聘人数: 210\n公       司: 百度\n职位类别: 技术\n发布时间: 2017-07-28\n工作职责:\n-研究数据挖掘或统计学习领域的前沿技术,并用于实际问题的解决和优化\n-大规模机器学习算法研究及并行化实现,为各种大规模机器学习应用研发核心技术\n-通过对数据的敏锐洞察,深入挖掘产品潜在价值和需求,进而提供更有价值的产品和服务,通过技术创新推动产品成长\n职责要求:\n-热爱互联网，对技术研究和应用抱有浓厚的兴趣，有强烈的上进心和求知欲，善于学习和运用新知识\n-具有以下一个或多个领域的理论背景和实践经验：机器学习/数据挖掘/深度学习/信息检索/自然语言处理/机制设计/博弈论\n-至少精通一门编程语言，熟悉网络编程、多线程、分布式编程技术，对数据结构和算法设计有较为深刻的理解\n-良好的逻辑思维能力，对数据敏感，能够发现关键数据、抓住核心问题\n-较强的沟通能力和逻辑表达能力，具备良好的团队合作精神和主动沟通意识\n具有以下条件者优先：\n-熟悉文本分类、聚类、机器翻译，有相关项目经验\n-熟悉海量数据处理、最优化算法、分布式计算或高性能并行计算，有相关项目经验\n\n\n\n\n\n\n\n\n\n\n2018 网易\nNLP算法研发工程师（网易杭州）\n岗位描述\n1、负责NLP技术在自动问答、人机对话、语义理解等方向上的应用研究；\n2、负责NLP相关核心技术研发及前沿算法跟踪，根据产品需求完成技术转化，推动业务发展。\n岗位要求\n我们希望你是：\n1、正直诚信、有责任感、有激情；\n2、模式识别/人工智能/数学/计算机相关专业，硕士以上学历；\n3、熟悉基于统计和句法/语法分析的自然处理方法，包括分词、词性标注、命名实体识别、依存句法分析、文本分类、文本检索、Deep Learning在NLP领域中的应用等等；\n4、具有较强编程能力（熟悉C++/Java），熟练使用至少一种脚本语言（python/shell等），熟悉hadoop、spark框架者尤佳；\n5、在自动问答、人机对话、口语理解、知识库管理等领域有实际的开发经验者优先；\n6、学习能力强，能独立分析并解决问题。\n\n\n\n\n\n\n2018 科大讯飞\n研究员（自然语言处理方向）\n工作地点： 合肥市,北京市...\n工作经验：\n学　　历：\n工作类型： 全职\n招聘人数：若干\n发布时间：2017-08-04\n职位描述\n您可以：\n1、负责语言理解、人机对话、意图识别、知识图谱、命名体识别等相关算法的研究和开发\n2、负责机器翻译相关算法的研究和开发\n任职要求\n我们需要您具备以下条件：\n1、重点院校硕士及以上学历，计算机、信号处理、自动化、应用数学等相关专业，具备一定的数理统计、模式识别、自然语言处理等理论知识\n2、英语六级以上，具备中英文学术论文的调研能力，有从事研究型项目的经历\n3、具备较好的C、C++或python等热门脚本语言编程能力，有一定的代码开发经历\n\n\n如果将优先考虑：\n1、熟悉RNN、CNN等深度学习算法及其常用工具如Caffe、Theano、TensorFlow等\n2、熟悉深度学习算法在自然语言理解中的应用\n3、有自然语言理解相关方向较丰富的实际系统研究和开发经验者\n4、在ACL、COLING、IJCAI、AAAI、ICLR、NIPS等会议上发表过文章，有深度学习学术或工程项目经验者\n\n\n2018 科大讯飞\n大数据分析工程师\n工作地点： 合肥市\n工作经验：\n学　　历：本科及以上\n工作类型： 全职\n招聘人数：若干\n发布时间：2017-08-04\n职位描述\n1、通过对数据的敏锐洞察，深入挖掘产品和服务的潜在价值和需求，进而提供更有价值的产品和服务，通过技术创新推动产品成长\n2、通过统计分析和数据挖掘算法解决实际问题，主要聚焦于城市动态产生的海量数据，进行城市交通分析、规划分析、商业地产分析等\n任职要求\n1、热爱技术，对技术研究和应用抱有浓厚的兴趣，善于学习和运用新知识\n2、具备商业分析报告撰写能力和良好的逻辑思维能力，对数据敏感，能够发现关键数据、抓住核心问题\n3、较强的沟通能力和逻辑表达能力，具备良好的团队合作精神和主动沟通意识\n4、具有以下一个或多个领域的理论背景和实践经验：统计、数学、信息技术、计算机等\n5、至少精通一门编程语言（C/C++，Java，Python，Scala等），了解Hadoop/Spark分布式编程技术\n6、至少掌握一种数据分析工具（R，SAS，SPSS，Matlab等），熟练运用SQL\n7、具有数据挖掘、海量数据处理等相关项目经验者优先\n\n\n\n\n\n\n\n\n2018 美团点评\n【2018届】机器学习／数据挖掘算法工程师-北京\n工作地点:北京\n职位类型:技术研发\n招聘类别:应届毕业生\n发布时间:2017-08-10 15:12:00\n岗位职责:\n在这里，你将通过机器学习、数据挖掘、深度/增强学习前沿技术对海量O2O数据进行洞察和预测，提高线下服务效率，优化线上用户体验，人和服务的高效连接潜力无限，帮助亿万用户吃得更好过得更好；在这里，你将从海量浏览和交易数据中不断抽象模式建立模型，一手保障商户在线营销可靠安全，另一手为消费者呈现有效评价提供优质服务，用技术提升消费质量；在这里，你可以发挥你的算法天赋，在海量数据的平台上实践各种机器学习和挖掘算法，搜索、推荐、广告、调度、无人配送、风控、金融、ERP和智能化交互，为你提供最广阔的施展天地。\n工作要求:\n1. 良好的数据结构和算法基础，具有较强的程序开发和分布式系统实现能力；2. 熟练掌握数据库设计原理，对NoSQL和分布式计算有理解和实践；3. 对概率论、机器学习和自然语言处理有一定的理论基础，在深度学习/增强学习/最优化等方向有理解或实践；4. 对数据敏感，思维逻辑清晰，对业务问题充满好奇，相信大数据背后的力量。\n\n\n\n\n2018 美团点评\n【2018届】自然语言处理开发工程师-上海\n工作地点:上海\n职位类型:技术研发\n招聘类别:应届毕业生\n发布时间:2017-08-10 15:06:40\n岗位职责:\n在这里，你将有机会深入研究自然语言处理领域的特定技术；在这里，你将用深度语义理解，让我们更懂用户；在这里，你将用深度语义计算，让我们精准匹配用户需求；在这里，你将用智能问答技术，让我们为客户实时解决各种问题。\n工作要求:\n1. 熟练掌握自然语言处理领域的基础理论和方法，并有丰富的相关方向的研究经验；2. 在一个或多个领域有深入研究：分词、文本分类、语义分析、语义表示、语义匹配、组块分析、主题模型、篇章分析等；3. 熟练掌握C/C++Python/Perl/Shell等编程语言及数据结构基础算法；4. 优秀的分析问题和解决问题的能力，对解决具有挑战性问题充满激情。\n\n\n\n\n\n\n2017携程\n你敢吗？\n携程作为中国在线旅游的领军企业，是一家崇尚数据驱动文化的公司。BI团队做为Ctrip的数据和数据应用中心，不断实践数据驱动的文化，不断利用数据和数据模型来解决Ctrip十三个业务线中的各种业务问题。您将有机会和旅游领域的业务、流程、研发、基数数据、资深机器学习专家等领域的专家合作。\n我们的数据科学家团队的理念是，理解问题、解决问题、驱动问题。您将会分析产品、操作、流量、用户甚至财务绩效相关的数据，从而能够更好的驱动业务发展的机会。您将有足够的支持把你的发现变成业务成果，一起分享Ctrip成功的业务结果。\n岗位介绍\n1、与产品沟通并准确理解需求；\n2、评估需求的可行性、设计算法并成功植入系统。\n携程旅行2018届春季实习生招聘已经启动！\n欢迎关注官方微信公众号：ctriptech_campus\n官方校招QQ群：314810731、545235287、529598646、278735052、541803697（添加任意一个即可）\n我们寻找这样的你\n1、2018届毕业生，全日制硕博。计算机或相关专业；\n2、熟悉算法相关理论，算法原理和机器学习基本理论，具备扎实良好的数学基础；\n3、熟悉中文分词、文本分类/聚类、语言模型、语义分析、情感分析、信息检索、问答系统设计等至少其中两项NLP相关算法；\n4、精通Java/C++/Python/Perl任意一种语言，熟练掌握SQL/Hive；\n5、良好的数据分析能力，能够从数据中发现规律；\n6、熟悉Linux环境/Shell命令；\n7、熟悉分布式计算Hadoop/hive/Spark/Map-Reduce等相关技术；\n8、具备深度学习相关项目经验者优先；\n9、至少能保证暑期7、8月份在公司实习。（通过实习考核即签三方）\n\n\n\n\n\n\n2018 小米\n\n\n职位名称： 数据挖掘工程师 工\n作地点： 北京\n职位类别： 研发工程师 招聘渠道： 校园招聘\n招聘地点： 北京\n\n\n工作职责：\n方向一：负责公司级数据产品与平台的设计与管理，对公司各部分业务数据进行数据采集、抽取、整合、提取和数据可视化等工作;\n方向二：负责小米数据管理平台的设计与研发，基于亿级用户的大数据，建立小米用户画像和用户标签体系，建设数据管理平台提供商业智能分析;\n*方向三：负责大数据的分析与挖掘，针对海量信息建模，挖掘潜在商业价值，预测用户行为，为产品决策及优化提供数据支持与建议;\n方向四：负责个性化推荐服务系统的运营与设计，将海量内容精准送达至海量的用户。\n\n\n工作要求：\n1、熟悉数据挖掘、机器学习, 自然语言处理等相关技术者优先考虑;\n2、拥有Hadoop、Spark等分布式环境开发经验者优先考虑;\n3、熟悉Linux，java、 Python (或Shell脚本 );\n4、优秀的分析和解决问题的能力，对大数据挖掘充满激情;\n\n\n\n\n2018今日头条\n算法工程师\n岗位描述：\n1、利用机器学习技术，改进头条的推荐、广告系统，优化数亿用户的阅读体验；2、分析基础数据，挖掘用户兴趣、文章价值，增强推荐、广告系统的预测能力；3、分析用户商业意图，挖掘流量潜在商业价值，提升流量变现；4、研究计算机视觉算法，给用户提供更多更酷炫的功能。\n岗位要求：\n1、2018年应届毕业生，本科及以上学历，计算机、机器学习和模式识别相关专业；2、热爱计算机科学和互联网技术，对人工智能类产品有浓厚兴趣；3、具备强悍的编码能力，熟悉 linux 开发环境，熟悉 C++ 和 Python 语言优先；4、有扎实的数据结构和算法功底，熟悉机器学习、自然语言处理、数据挖掘、分布式计算、计算机视觉中一项或多项；5、对推荐系统、计算广告、搜索引擎、图像和视频处理相关技术有经验者优先；6、优秀的分析问题和解决问题的能力，对解决具有挑战性问题充满激情。、、\n\n\n\n\n无码科技自然语言处理工程师\n于 无码科技 in 杭州\n无码科技致力构建值得用户信赖的搜索引擎。\n我们要找自然语言处理工程师。期待找到长期的合作伙伴，创始团队成员。\n职位描述：\n利用自然语言处理和机器学习算法对海量文本数据进行挖掘分析，包括但不限于文本聚类、语义理解、信息抽取、知识图谱、对话生成等。\n尝试新的机器学习算法、计算框架，提升机器学习系统效率。\n职位要求：\n两年以上自然语言处理相关的研发经验。\n具备较强的编码能力，扎实的数据结构和算法功底。\n熟悉机器学习的基本算法与概念，如：逻辑回归、神经网络、决策树等。\n熟悉自然语言处理常见算法与模型（如 LDA、Word2Vec、CNN/RNN 等）。\n较好的英文技术文献阅读能力。\n加分项：\n发表过高水平学术会议论文。\n熟悉 Apache Hadoop/Spark/Storm 等至少一种分布式系统。\n有过在医疗数据上应用机器学习 / 自然语言处理 (NLP) 的经历。\n工作地点：杭州市滨江区。\n\n\n\n\n\n\n总结：\n主要在以下几个方面有要求，打勾的次数反应了热度：\n1.自然语言处理相关的具体操作：分词、语义、句意、对话、机器翻译、自动问答等√√√√√\n2.经典的机器学习算法、竞赛经历√√√\n3.多线程、网络编程、分布式编程√\n4.hadoop、spark√√√√\n5.SQL、NoSQL√\n6.linux√√","data":"2017年08月11日 21:50:33"}
{"_id":{"$oid":"5d36c3276734bd8e681d654b"},"title":"自然语言处理能够把全网内容组织到什么程度？","author":"hejishan","content":"marginwidth=\"0\" marginheight=\"0\" src=\"http://www.zealware.com/csdnblog01.html\" frameborder=\"0\" width=\"728\" scrolling=\"no\" height=\"90\"\u003e\n自然语言处理能够把全网内容组织到什么程度？\nZhengyun 发表于创业+社区 2007-03-27 23:23:40\n我的要求是不需要任何推动力，用户不需要做任何输入或搜索，社区内就已经围绕着细粒度的话题展开了。\n\n\n结果我们做到的自然语言处理后的主题收敛性很强，哈哈。\n\n\n随手举个例子：\n\n\n推荐：《 转载：如此令人恶心的三亚今年春节我们在三亚的惊魂遭遇 》\n\n作者: [倾城] 2007-03-25 16:20:04 (XXX自动计算\n)\n\n相关博主论点： 《三亚制订旅游整治方案 欲让99%游客满意》 《市长向游客道歉是网络媒体和草根的胜利》 《三亚是否真的如此令人恶心？》 《又一中国高官道歉事件》 《《如此让人恶心的三亚》一文作者,其实三亚政府应该追究你的责任!》 《谁要保护游客的安全？？》 《市长向游客道歉显示新媒体和草根的力量》 阅读这个话题讨论\n…\n\n\n\n例子二：\n\n\n推荐：《 娱乐场所实名制管理不止一石三鸟 》\n\n作者: [诗情碧霄] 2007-03-25 17:23:57 (XXX自动计算\n)\n\n相关博主论点： 《时评 实名制不是万金油》 《欢场实名制，是个好东西！？》 《娱乐场所实行实名制，还有谁再去消费？》 《小姐不是小姐，翠花也不叫翠花》 阅读这个话题讨论\n…\n\n\n\n这两个例子是社会·民生自动分类的。\n\n\n\n我们再来看明星·演艺分类的例子：\n\n\n推荐：《 外国人才艺大赛出意外 变脸失误选手下跪 》\n\n作者: 王伟的BLOG 2007-03-25 01:34:14 (XXX自动计算\n)\n\n相关博主论点： 《黑人小伙表演变脸失误 泪流满面下跪》 《《全球博客文摘精典周刊-和谐世界》：老外参加央视节目变脸失败 下跪痛哭道歉》 《我，为此感动和鼓掌不断！！！----------而我们的优秀的传统文化、礼仪和精神又上哪里去了？》 《瞬间的感动!》 《这样的道歉，真是精品！》 阅读这个话题讨论…\n\n\n\n\n这些都是机器自己没有第一推动力情况下自行计算的结果。\n\n\n社会上有一个热点，我们就自动计算出来了，并灌输到社区里，所以能整合全网内容。\n\n\n百度贴吧毕竟还是有人输入了搜索关键词，从而形成第一推动力的。\n\n\n我们不需要。\n\n\n就像我经常说的一句话：“百度、Google是通过用户搜索输入的关键词来判断中文世界的热点。 而我们通过分析全网写作的文章来寻找热点的。 通过这种主题自动发现技术，可以很容易知道最近中文世界人们在讨论什么、在关注什么。\n\n\n”\n下面举几个长一点的例子：\n推荐博文：《 港选特首：一场只动眼不动手的选举（转自BBC中文网） 》\n作者:  TheTwoDogs 2007-03-25 15:58:47 (XXXX自动计算)\n相关博主论点： 《举选》   《为什么内地人对香港特首选举漠不关心》   《新华网:曾荫权当选香港第三任行政长官》   《我期待的晚年生活》   《香港特首选举结束了......》   《曾荫权的高票当选显示一国二制强大的生命力》   《更多要闻曾荫权高票连任香港行政长官》   《民主需要秩序。一个合理平稳的选举和权力交接是政治民主的保证》   《分析:曾-荫-权和梁-家-杰以后的路（转自BBC中文网）》   《为防泄密 港-特-首-选-举拆闭路电视（转自BBC中文网）》   《年轻没有失败》   《换届》   《曾荫权在香港第三任行政长官选举中以高票胜出》   《香港特别行政区第三任行政长官选举揭晓，现任行政长官曾荫权获得649票，以超过八成的得票率胜出。》   《曾荫权当选香港特区第三届行政长官》   《曾荫权当选新特首》   《曾荫权》        阅读这个话题讨论…\n推荐博文：《 重庆钉子户给政府出难题 》\n作者:  [ 蔡律http://cailv.bokee.com/] 2007-03-25 09:45:08 (XX自动计算)\n相关博主论点： 《根除野蛮拆迁的契机已经降临》   《随笔》   《搬迁最后期限已过　钉子户仍钉在孤岛上》   《长平：最牛钉子户的举动真牛》   《2007-3-24一种拆迁，两种命运：“钉子户”是谁眼里的“钉子”？var stattitle=\"一种拆迁，两种命运：“钉子户”是谁眼里的“钉子”？\"; 今天，重庆将成为众媒体、记者、网友和百姓聚焦之地！ 07全国两会刚结束，在笔者地脑海里一直有一组强烈的声音在回荡：权力过于集中造成》   《贪婪无耻的“最牛”钉子户》   《国旗，你不应该成为做秀的工具》   《“钉子”何以成“钉”？》   《不接受法庭判决挂起国旗和标语 重庆钉子户给政府出难题》      阅读这个话题讨论…\n\n\n\nTrackback: http://tb.blog.csdn.net/TrackBack.aspx?PostId=1543390","data":"2008年01月03日 20:57:00"}
{"_id":{"$oid":"5d36c3356734bd8e681d6553"},"title":"自然语言处理之情感分析与观点挖掘","author":"Paulzhao6518","content":"观点、情感以及与之相关的许多概念，如评价、评估、态度、感情、情绪和心情，与我们主观的感觉和感受密切相关。这些是人类心理活动的核心要素，也是影响人们日常行为的关键因素。情感分析也称为观点挖掘，是一个旨在利用可计算的方法从自然语言文本中提取观点和情感信息的研究课题。\n一.情感分析\n其伴随着网络社会媒体（如评论、论坛、博客和微博）的兴起而快速发展。\n情感分析研究的目标是从文本中分析出人们对于实体及其属性所表达的观点、情感、评价、态度和情绪。这些实体可以是各种产品、服务、机构、个人、事件、问题或主题等。包含很多相关研究任务，例如情感分析、观点挖掘、观点分析、观点信息抽取、情感挖掘、主观性分析、倾向性分析、情绪分析以及评论挖掘。\n从自然语言处理的角度看，情感分析的人物就是识别人们谈论的主题以及针对主题所表达出来的观点倾向。因此，它常被看成一个语义分析任务的子问题。\n情感分析研究可划分三个级别：篇章级、句子级和属性级。\n情感词典：承载情感信息最重要的基本单元是情感词，也称为观点词。但是仅仅依靠它们对构建精准的情感分析系统远远不够。\n辩论和评论分析：\n意图挖掘：意图就是一个人或者一群人试图遵循的行动步骤。尽管意图与情感是两个不同的概念，但是它们有许多相关点。第一，在一个含有意图倾向的句子中，作者通常会表达对于某一事物或实体的情感或情绪。第二，当一个人非常想得到某一东西的时候，他通常会对这个东西表达褒义的情感。第三，有些观点是通过描述意图的方式表达出来的。\n垃圾观点检测与评论质量：社会媒体的一个关键特点就是允许每个人在任何时间、任何地点以匿名的方式自由地表达自己的想法和观点。而不必害怕自己的真正身份被泄露。也不必担心这些言论会让自己招致麻烦。尽管这些观点和想法对于很多应用来说十分有价值，但是这种匿名的方式是有代价的。这种代价就是使得那些存有不良目的或隐藏企图的人可以通过发表虚假评论的方式欺骗情感分析系统，对某种产品、服务、机构和个人进行蓄意的夸奖或贬低，而不必暴露其真正的目的。这种发表虚假评论的个体被称为垃圾观点发布者，这种行为被称为垃圾观点发布。\n二.什么是情感分析\n情感分析主要研究那些表达或暗示褒义或贬义情感的观点信息。这里，观点是一个广义的概念，包括了情感、评估、评价、态度，以及其他相关信息，包括观点持有者和观点评价对象。\n观点、情感与目标：一个观点有两个重要组成部分：一个是观点评价的对象或目标g；另一个是针对该目标所表达的情感s。（g，s）中的g可以是一个实体，也可以是所评价实体的某个属性或一个侧面；s是一个正面（褒义）、负面（贬义）或中立的情感倾向或打分。正面（褒义）、负面（贬义）、中立则称为情感或观点倾向（极性）。\n可以把观点定义为一个四元组：（g, s, h, t）,h是观点持有者、t是时间\n情感对象：观点所评价的实体、实体的一部分或实体的一个属性。\n观点中的情感：情感是观点中所蕴含的感受、态度、评价或情绪。通常情感由一个三元组表示：（y, o, i）。其中，y是情感类型，分理性和感性；o是情感的倾向，正面、负面或中立；i是情感的强度。情感评分\n简化的观点定义：上述观点的定义，虽然简练，但很难应用于实际操作，从文本中识别出实体不同层次上的组件和属性是很困难的任务。其实大多数应用并不需要如此复杂的分析。因此，我们可以简化之前对于观点评价对象的定义，其层次结构只有2层，同时我们使用属性或方面这个词来指代目标实体的组件和参数。在这颗简化的树中，根节点依然是实体本身，第二层（叶子层）的节点是该实体的不同属性。\n重新定义观点的概念：五元组（e, a, s, h, t）\n其中，e是观点评价的目标实体，a是实体e中一个观点评价的实体属性，s是对实体e的a属性的观点中所包含的情感，h是观点持有者，t是观点发布时间；\n基于此定义的情感分析常称为基于属性的情感分析。\n情感分析的目标：给定一个包含观点信息的文档d，找出d中所有的观点五元组。对于更高级的分析需求，还要找出每个观点五元组中情感的原因和限定条件。\n情感分析的关键任务：实体消解或者实体聚类\n观点的不同类型：常规型观点和比较型观点，\n三.文档级情感分类:\n任务的目标是将一篇给定观点的文档（如产品评论）根据所持观点为正面或负面进行分类。定义是给定针对一个实体的观点文档d，判断观点持有者对实体的整体的观点倾向性s。大多数现有的技术都是基于监督学习的，也有一些无监督学习的方法。现有大多数技术都是特征工程加机器学习算法在实际中的直接应用。但目前还没有工作对于这些既有方法的有效性和准确性进行全面的、独立的评测和比较。\n3.1、基于监督的情感分类：本节提到两类分类方法：（1）使用一个标准的有监督机器学习算法进行情感分类；（2）使用一个专为情感分类设计的分类方法。\n基于机器学习算法的情感分类：情感分类的关键还是抽取有效的特征。一些特征样例：词和词频：带有词频信息的单独的词袋及与其相关的n-gram\n词性：研究表明，形容词是观点和情感的主要承载词。\nhttp://www.cis.upenn.edu~treebank/home.html\n情感词和情感短语：大多情感词都是形容词或副词\n观点的规划：文本结构或语音成分可以表示或隐含情感和观点\n情感转置词：有的表达可以反转文本中的情感倾向\n句法依存关系：\n3.2、基于无监督的情感分类：使用句法模板和网页检索的情感分类、使用情感词典的情感分类。\n四.句子级主客观和情感分类：\n文档级别的情感分类对实际应用来说还是太粗糙。句子级其目标是识别每个观点文档中的句子中所包含的情感倾向，判断每个句子包含的正面、负面还是中性的情感。这离实际应用的情感分类系统的需求更进一步，即提取针对每个评论对象的观点信息。因为句子太短从而包含的信息也少得多，因此，句子级别的情感分类要更加困难。大多数文档级别的情感分类论文都忽略中性类，主要是做准确的三类分类太难了。但是，对于句子级别的情感分类，中性类就不可以忽略了。\n句子级别分类有个潜在的假设是：一个句子只表达了一个观点，即只包含一个中情感。\n句子级情感分类：\n处理条件句：\n处理讽刺句：\n跨语言主客观分类和情感分类：\n在情感分类中使用语篇信息：\n句子级情绪分类：","data":"2018年09月19日 18:58:41"}
{"_id":{"$oid":"5d36c3d26734bd8e681d6573"},"title":"自然语言处理之文本标注问题","author":"lovive","content":"文本标注 (tagging) 是一个监督学习问题，可以认为标注问题是分类问题的一个推广，标注问题又是更复杂的结构预测 (structure prediction) 问题的简单形式，标注问题的输入是一个观测序列，输出是一个标记序列护着状态序列，标注问题的目标在于学习一个模型，使它能够对观测序列给出标记序列作为预测，注意的是可能的标记个数是有限的，但其组合所成的标记序列的个数是依序列长度呈指数级增长的。\n\n\n\n\n标注问题氛围学习和标注两个过程(如上图所示)，首先给定一个训练数据集：\n\n\n\n在这里xi为输入观测序列 (一维向量)，yi为相应的输出观测序列 (一维向量)，每个输入观测序列向量的长度为n，对不同样本具有不一样的值，学习系统基于训练数据集构建一个模型，表示为条件概率分布：\n\n\n\n这里的每个xi(i=1,2,...,n)取值为所有可能的观测，每个Yi (i = 1,2..., n)取值为所有可能的标记，一般n远小于N，标注系统按照学习得到的条件概率分布模型，对新输入观测序列找到相应的输出标记序列。具体的对每一个观测序列，找到上式中概率最大的标记序列。\n\n\n\n评价标注模型的指标与评价分类模型的指标一样，常用的有标注准确率，精确率和召回率。\n\n\n标注问题常用的统计学方法有：详解隐马尔可夫模型(HMM)和自然语言模型之条件随机场理论(CRF)，这两个模型，之前的文章有介绍过。\n\n\n标注问题在信息抽取，自然语言处理等领域被广泛应用，是这些领域的基本问题。例如，自然语言处理中的词性标注就是一个典型的标注问题：给定一个由单词组成的句子，对这个句子中的每一个单词进行词性标注，即对一个单词序列预测其对应的词性标记序列。\n\n\n举一个信息抽取的例子，从英文文章中抽取基本名词短语，为此，要对文章进行标注。英文单词是一个观测，英文句子是一个观测序列，标记表示名词短语的\"开始\"、\"结束\"或“其它”。标记序列表示英文句子中基本名词短语的所在位置。信息抽取时，将标记“开始”到标记“结束”的单词作为名词短语。\n\n\n标注模型的评价指标\n标注问题常用的评价指标是精确率 (precision )，召回率 (recall) 和F1值，它和分类问题的评价指标相同，为了简便，这里使用分类来进行说，通常标注模型在测试数据集上的预测和或正确或不正确，4中情况出现的总数分别记作：\nTP：将正确类预测为正类数\nFP：将正类预测为负类数\nFP：将负类预测为正类数\nTN：将负类预测为负类数\n那么精确率定义为：P = TP / (TP + FP)\n召回率定义为: R = TP / (TP + FN)\nF1值是根据精确率和召回率来进行计算的表达式为:\n2/ F1 = 1/ P + 1/ R\n即：F1 = 2TP /( 2TP + FP + FN)\n一般精确率和召回率都高时，F1值也会很高。\n\n\n参考学习资料：\n[1] 统计学习方法： 李航\n\n\n文章来源于微信公众号：言处理技术，更多内容请访问该公众号。\n\n\n欢迎关注公众号学习","data":"2017年12月05日 14:45:07"}
{"_id":{"$oid":"5d36c4146734bd8e681d658d"},"title":"Python自然语言处理工具小结","author":"banlucainiao","content":"1 Python 的几个自然语言处理工具\nNLTK:NLTK 在用 Python 处理自然语言的工具中处于领先的地位。它提供了 WordNet 这种方便处理词汇资源的借口，还有分类、分词、除茎、标注、语法分析、语义推理等类库。\nPattern:Pattern 的自然语言处理工具有词性标注工具(Part-Of-Speech Tagger)，N元搜索(n-gram search)，情感分析(sentiment analysis)，WordNet。支持机器学习的向量空间模型，聚类，向量机。\nTextBlob:TextBlob 是一个处理文本数据的 Python 库。提供了一些简单的api解决一些自然语言处理的任务，例如词性标注、名词短语抽取、情感分析、分类、翻译等等。\nGensim:Gensim 提供了对大型语料库的主题建模、文件索引、相似度检索的功能。它可以处理大于RAM内存的数据。作者说它是“实现无干预从纯文本语义建模的最强大、最高效、最无障碍的软件。\nPyNLPI:它的全称是：Python自然语言处理库（Python Natural Language Processing Library，音发作: pineapple） 这是一个各种自然语言处理任务的集合，PyNLPI可以用来处理N元搜索，计算频率表和分布，建立语言模型。他还可以处理向优先队列这种更加复杂的数据结构，或者像 Beam 搜索这种更加复杂的算法。\nspaCy:这是一个商业的开源软件。结合Python和Cython，它的自然语言处理能力达到了工业强度。是速度最快，领域内最先进的自然语言处理工具。\nPolyglot:Polyglot 支持对海量文本和多语言的处理。它支持对165种语言的分词，对196中语言的辨识，40种语言的专有名词识别，16种语言的词性标注，136种语言的情感分析，137种语言的嵌入，135种语言的形态分析，以及69中语言的翻译。\nMontyLingua:MontyLingua 是一个自由的、训练有素的、端到端的英文处理工具。输入原始英文文本到 MontyLingua ，就会得到这段文本的语义解释。适合用来进行信息检索和提取，问题处理，回答问题等任务。从英文文本中，它能提取出主动宾元组，形容词、名词和动词短语，人名、地名、事件，日期和时间，等语义信息。\nBLLIP Parser:BLLIP Parser（也叫做Charniak-Johnson parser）是一个集成了产生成分分析和最大熵排序的统计自然语言工具。包括 命令行 和 python接口 。\nQuepy:Quepy是一个Python框架，提供将自然语言转换成为数据库查询语言。可以轻松地实现不同类型的自然语言和数据库查询语言的转化。所以，通过Quepy，仅仅修改几行代码，就可以实现你自己的自然语言查询数据库系统。GitHub:https://github.com/machinalis/quepy\nHanNLP：HanLP是由一系列模型与算法组成的Java工具包，目标是普及自然语言处理在生产环境中的应用。不仅仅是分词，而是提供词法分析、句法分析、语义理解等完备的功能。HanLP具备功能完善、性能高效、架构清晰、语料时新、可自定义的特点。文档使用操作说明：Python调用自然语言处理包HanLP 和 菜鸟如何调用HanNLP\n2 OpenNLP：进行中文命名实体识别\nOpenNLP是Apach下的Java自然语言处理API，功能齐全。如下给大家介绍一下使用OpenNLP进行中文语料命名实体识别的过程。\n首先是预处理工作，分词去听用词等等的就不啰嗦了，其实将分词的结果中间加上空格隔开就可以了，OpenNLP可以将这样形式的的语料照处理英文的方式处理，有些关于字符处理的注意点在后面会提到。\n其次我们要准备各个命名实体类别所对应的词库，词库被存在文本文档中，文档名即是命名实体类别的TypeName，下面两个function分别是载入某类命名实体词库中的词和载入命名实体的类别。\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n/**\n* 载入词库中的命名实体\n*\n* @param nameListFile\n* @return\n* @throws Exception\n*/\npublic static List\u003cString\u003e loadNameWords(File nameListFile)\nthrows Exception {\nList\u003cString\u003e nameWords = new ArrayList\u003cString\u003e();\nif (!nameListFile.exists() || nameListFile.isDirectory()) {\nSystem.err.println(\"不存在那个文件\");\nreturn null;\n}\nBufferedReader br = new BufferedReader(new FileReader(nameListFile));\nString line = null;\nwhile ((line = br.readLine()) != null) {\nnameWords.add(line);\n}\nbr.close();\nreturn nameWords;\n}\n/**\n* 获取命名实体类型\n*\n* @param nameListFile\n* @return\n*/\npublic static String getNameType(File nameListFile) {\nString nameType = nameListFile.getName();\nreturn nameType.substring(0, nameType.lastIndexOf(\".\"));\n}\n因为OpenNLP要求的训练语料是这样子的：\n1\nXXXXXX\u003cSTART:Person\u003e????\u003cEND\u003eXXXXXXXXX\u003cSTART:Action\u003e????\u003cEND\u003eXXXXXXX\n被标注的命名实体被放在\u003cSTART\u003e\u003cEND\u003e范围中，并标出了实体的类别。接下来是对命名实体识别模型的训练，先上代码：\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.io.StringReader;\nimport java.util.Collections;\nimport opennlp.tools.namefind.NameFinderME;\nimport opennlp.tools.namefind.NameSample;\nimport opennlp.tools.namefind.NameSampleDataStream;\nimport opennlp.tools.namefind.TokenNameFinderModel;\nimport opennlp.tools.util.ObjectStream;\nimport opennlp.tools.util.PlainTextByLineStream;\nimport opennlp.tools.util.featuregen.AggregatedFeatureGenerator;\nimport opennlp.tools.util.featuregen.PreviousMapFeatureGenerator;\nimport opennlp.tools.util.featuregen.TokenClassFeatureGenerator;\nimport opennlp.tools.util.featuregen.TokenFeatureGenerator;\nimport opennlp.tools.util.featuregen.WindowFeatureGenerator;\n/**\n* 中文命名实体识别模型训练组件\n*\n* @author ddlovehy\n*\n*/\npublic class NamedEntityMultiFindTrainer {\n// 默认参数\nprivate int iterations = 80;\nprivate int cutoff = 5;\nprivate String langCode = \"general\";\nprivate String type = \"default\";\n// 待设定的参数\nprivate String nameWordsPath; // 命名实体词库路径\nprivate String dataPath; // 训练集已分词语料路径\nprivate String modelPath; // 模型存储路径\npublic NamedEntityMultiFindTrainer() {\nsuper();\n// TODO Auto-generated constructor stub\n}\npublic NamedEntityMultiFindTrainer(String nameWordsPath, String dataPath,\nString modelPath) {\nsuper();\nthis.nameWordsPath = nameWordsPath;\nthis.dataPath = dataPath;\nthis.modelPath = modelPath;\n}\npublic NamedEntityMultiFindTrainer(int iterations, int cutoff,\nString langCode, String type, String nameWordsPath,\nString dataPath, String modelPath) {\nsuper();\nthis.iterations = iterations;\nthis.cutoff = cutoff;\nthis.langCode = langCode;\nthis.type = type;\nthis.nameWordsPath = nameWordsPath;\nthis.dataPath = dataPath;\nthis.modelPath = modelPath;\n}\n/**\n* 生成定制特征\n*\n* @return\n*/\npublic AggregatedFeatureGenerator prodFeatureGenerators() {\nAggregatedFeatureGenerator featureGenerators = new AggregatedFeatureGenerator(\nnew WindowFeatureGenerator(new TokenFeatureGenerator(), 2, 2),\nnew WindowFeatureGenerator(new TokenClassFeatureGenerator(), 2,\n2), new PreviousMapFeatureGenerator());\nreturn featureGenerators;\n}\n/**\n* 将模型写入磁盘\n*\n* @param model\n* @throws Exception\n*/\npublic void writeModelIntoDisk(TokenNameFinderModel model) throws Exception {\nFile outModelFile = new File(this.getModelPath());\nFileOutputStream outModelStream = new FileOutputStream(outModelFile);\nmodel.serialize(outModelStream);\n}\n/**\n* 读出标注的训练语料\n*\n* @return\n* @throws Exception\n*/\npublic String getTrainCorpusDataStr() throws Exception {\n// TODO 考虑入持久化判断直接载入标注数据的情况 以及增量式训练\nString trainDataStr = null;\ntrainDataStr = NameEntityTextFactory.prodNameFindTrainText(\nthis.getNameWordsPath(), this.getDataPath(), null);\nreturn trainDataStr;\n}\n/**\n* 训练模型\n*\n* @param trainDataStr\n*            已标注的训练数据整体字符串\n* @return\n* @throws Exception\n*/\npublic TokenNameFinderModel trainNameEntitySamples(String trainDataStr)\nthrows Exception {\nObjectStream\u003cNameSample\u003e nameEntitySample = new NameSampleDataStream(\nnew PlainTextByLineStream(new StringReader(trainDataStr)));\nSystem.out.println(\"**************************************\");\nSystem.out.println(trainDataStr);\nTokenNameFinderModel nameFinderModel = NameFinderME.train(\nthis.getLangCode(), this.getType(), nameEntitySample,\nthis.prodFeatureGenerators(),\nCollections.\u003cString, Object\u003e emptyMap(), this.getIterations(),\nthis.getCutoff());\nreturn nameFinderModel;\n}\n/**\n* 训练组件总调用方法\n*\n* @return\n*/\npublic boolean execNameFindTrainer() {\ntry {\nString trainDataStr = this.getTrainCorpusDataStr();\nTokenNameFinderModel nameFinderModel = this\n.trainNameEntitySamples(trainDataStr);\n// System.out.println(nameFinderModel);\nthis.writeModelIntoDisk(nameFinderModel);\nreturn true;\n} catch (Exception e) {\n// TODO Auto-generated catch block\ne.printStackTrace();\nreturn false;\n}\n}\n｝\n注：\n参数：iterations是训练算法迭代的次数，太少了起不到训练的效果，太大了会造成过拟合，所以各位可以自己试试效果；\ncutoff：语言模型扫描窗口的大小，一般设成5就可以了，当然越大效果越好，时间可能会受不了；\nlangCode：语种代码和type实体类别，因为没有专门针对中文的代码，设成“普通”的即可，实体的类别因为我们想训练成能识别多种实体的模型，于是设置为“默认”。\n说明：\nprodFeatureGenerators()方法用于生成个人订制的特征生成器，其意义在于选择什么样的n-gram语义模型，代码当中显示的是选择窗口大小为5，待测命名实体词前后各扫描两个词的范围计算特征（加上自己就是5个），或许有更深更准确的意义，请大家指正；\ntrainNameEntitySamples()方法，训练模型的核心，首先是将如上标注的训练语料字符串传入生成字符流，再通过NameFinderME的train()方法传入上面设定的各个参数，订制特征生成器等等，关于源实体映射对，就按默认传入空Map就好了。\n源代码开源在：https://github.com/Ailab403/ailab-mltk4j，test包里面对应有完整的调用demo，以及file文件夹里面的测试语料和已经训练好的模型。\n3 StanfordNLP：\nStanford NLP Group是斯坦福大学自然语言处理的团队，开发了多个NLP工具。其开发的工具包括以下内容：\nStanford CoreNLP : 采用Java编写的面向英文的处理工具，下载网址为：。主要功能包括分词、词性标注、命名实体识别、语法分析等。\nStanford Word Segmenter : 采用CRF（条件随机场）算法进行分词，也是基于Java开发的，同时可以支持中文和Arabic，官方要求Java版本1.6以上，推荐内存至少1G。\n简单的示例程序：\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n//设置分词器属性。\nProperties props = new Properties();\n//字典文件地址，可以用绝对路径，如d:/data\nprops.setProperty(\"sighanCorporaDict\", \"data\");\n//字典压缩包地址，可以用绝对路径\nprops.setProperty(\"serDictionary\",\"data/dict-chris6.ser.gz\");\n//输入文字的编码；\nprops.setProperty(\"inputEncoding\", \"UTF-8\");\nprops.setProperty(\"sighanPostProcessing\", \"true\");\n//初始化分词器，\nCRFClassifier classifier = new CRFClassifier(props);\n//从持久化文件中加载分词器设置；\nclassifier.loadClassifierNoExceptions(\"data/ctb.gz\", props);\n// flags must be re-set after data is loaded\nclassifier.flags.setProperties(props);\n//分词\nList words = classifier.segmentString(\"语句内容\");\nStanford POS Tagger : 采用Java编写的面向英文、中文、法语、阿拉伯语、德语的命名实体识别工具。\nStanford Named Entity Recognizer ： 采用条件随机场模型的命名实体工具。\nStanford Parser ： 进行语法分析的工具，支持英文、中文、阿拉伯文和法语。\nStanford Classifier : 采用Java编写的分类器。\n最后附上关于中文分词器性能比较的一篇文章:http://www.cnblogs.com/wgp13x/p/3748764.html\n实现中文命名实体识别\n1、分词介绍\n斯坦福大学的分词器，该系统需要JDK 1.8+，从上面链接中下载stanford-segmenter-2014-10-26，解压之后，如下图所示\n进入data目录，其中有两个gz压缩文件，分别是ctb.gz和pku.gz，其中CTB：宾州大学的中国树库训练资料 ，PKU：中国北京大学提供的训练资料。当然了，你也可以自己训练，一个训练的例子可以在这里面看到http://nlp.stanford.edu/software/trainSegmenter-20080521.tar.gz\n2、NER介绍\n斯坦福NER是采用Java实现，可以识别出（PERSON，ORGANIZATION，LOCATION），使用本软件发表的研究成果需引用下述论文：\n下载地址在：http://nlp.stanford.edu/~manning/papers/gibbscrf3.pdf\n在NER页面可以下载到两个压缩文件，分别是stanford-ner-2014-10-26和stanford-ner-2012-11-11-chinese\n将两个文件解压可看到\n默认NER可以用来处理英文，如果需要处理中文要另外处理。\n3、分词和NER使用\n在Eclipse中新建一个Java Project，将data目录拷贝到项目根路径下，再把stanford-ner-2012-11-11-chinese解压的内容全部拷贝到classifiers文件夹下，将stanford-segmenter-3.5.0加入到classpath之中，将classifiers文件夹拷贝到项目根目录，将stanford-ner-3.5.0.jar和stanford-ner.jar加入到classpath中。最后，去http://nlp.stanford.edu/software/corenlp.shtml下载stanford-corenlp-full-2014-10-31，将解压之后的stanford-corenlp-3.5.0也加入到classpath之中。最后的Eclipse中结构如下：\nChinese NER：这段说明，很清晰，需要将中文分词的结果作为NER的输入，然后才能识别出NER来。\n同时便于测试，本Demo使用junit-4.10.jar，下面开始上代码\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\nimport edu.stanford.nlp.ie.AbstractSequenceClassifier;\nimport edu.stanford.nlp.ie.crf.CRFClassifier;\nimport edu.stanford.nlp.ling.CoreLabel;\n/**\n*\n* \u003cp\u003e\n* ClassName ExtractDemo\n* \u003c/p\u003e\n* \u003cp\u003e\n* Description 加载NER模块\n*\n*/\npublic class ExtractDemo {\nprivate static AbstractSequenceClassifier\u003cCoreLabel\u003e ner;\npublic ExtractDemo() {\nInitNer();\n}\npublic void InitNer() {\nString serializedClassifier = \"classifiers/chinese.misc.distsim.crf.ser.gz\"; // chinese.misc.distsim.crf.ser.gz\nif (ner == null) {\nner = CRFClassifier.getClassifierNoExceptions(serializedClassifier);\n}\n}\npublic String doNer(String sent) {\nreturn ner.classifyWithInlineXML(sent);\n}\npublic static void main(String args[]) {\nString str = \"我 去 吃饭 ， 告诉 李强 一声 。\";\nExtractDemo extractDemo = new ExtractDemo();\nSystem.out.println(extractDemo.doNer(str));\nSystem.out.println(\"Complete!\");\n}\n}\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\nimport java.io.File;\nimport java.io.IOException;\nimport java.util.Properties;\nimport org.apache.commons.io.FileUtils;\nimport edu.stanford.nlp.ie.crf.CRFClassifier;\nimport edu.stanford.nlp.ling.CoreLabel;\n/**\n*\n* \u003cp\u003e\n* Description 使用Stanford CoreNLP进行中文分词\n* \u003c/p\u003e\n*\n*/\npublic class ZH_SegDemo {\npublic static CRFClassifier\u003cCoreLabel\u003e segmenter;\nstatic {\n// 设置一些初始化参数\nProperties props = new Properties();\nprops.setProperty(\"sighanCorporaDict\", \"data\");\nprops.setProperty(\"serDictionary\", \"data/dict-chris6.ser.gz\");\nprops.setProperty(\"inputEncoding\", \"UTF-8\");\nprops.setProperty(\"sighanPostProcessing\", \"true\");\nsegmenter = new CRFClassifier\u003cCoreLabel\u003e(props);\nsegmenter.loadClassifierNoExceptions(\"data/ctb.gz\", props);\nsegmenter.flags.setProperties(props);\n}\npublic static String doSegment(String sent) {\nString[] strs = (String[]) segmenter.segmentString(sent).toArray();\nStringBuffer buf = new StringBuffer();\nfor (String s : strs) {\nbuf.append(s + \" \");\n}\nSystem.out.println(\"segmented res: \" + buf.toString());\nreturn buf.toString();\n}\npublic static void main(String[] args) {\ntry {\nString readFileToString = FileUtils.readFileToString(new File(\"澳门141人食物中毒与进食“问题生蚝”有关.txt\"));\nString doSegment = doSegment(readFileToString);\nSystem.out.println(doSegment);\nExtractDemo extractDemo = new ExtractDemo();\nSystem.out.println(extractDemo.doNer(doSegment));\nSystem.out.println(\"Complete!\");\n} catch (IOException e) {\ne.printStackTrace();\n}\n}\n}\n\n注意一定是JDK 1.8+的环境，最后输出结果如下：\n4 IKAnalyzer\nIK Analyzer是一个开源的，基于Java语言开发的轻量级的中文分词工具包。IK支持细粒度和智能分词两种切分模式，支持英文字母、数字、中文词汇等分词处理，兼容韩文、日文字符。可以支持用户自定义的词典，通过配置IKAnalyzer.cfg.xml文件来实现，可以配置自定义的扩展词典和停用词典。词典需要采用UTF-8无BOM格式编码，并且每个词语占一行。配置文件如下所示：\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\u003cproperties\u003e\n\u003ccomment\u003eIK Analyzer 扩展配置\u003c/comment\u003e\n\u003c!--用户可以在这里配置自己的扩展字典--\u003e\n\u003centry key=\"ext_dict\"\u003eext.dic;\u003c/entry\u003e\n\u003c!--用户可以在这里配置自己的扩展停止词字典--\u003e\n\u003centry key=\"ext_stopwords\"\u003estopword.dic;chinese_stopword.dic\u003c/entry\u003e\n\u003c/properties\u003e\n只需要把IKAnalyzer2012_u6.jar部署于项目的lib中，同时将IKAnalyzer.cfg.xml文件以及词典文件置于src中，即可通过API的方式开发调用。IK简单、易于扩展，分词结果较好并且采用Java编写，因为我平时的项目以Java居多，所以是我平时处理分词的首选工具。示例代码：\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n/**\n* IK分词功能实现\n* @return\n*/\npublic String spiltWords(String srcString){\nStringBuffer wordsBuffer = new StringBuffer(\"\");\ntry{\nIKSegmenter ik=new IKSegmenter(new StringReader(srcString), true);\nLexeme lex=null;\nwhile((lex=ik.next())!=null){\n//              System.out.print(lex.getLexemeText()+\" \");\nwordsBuffer.append(lex.getLexemeText()).append(\" \");\n}\n}catch(Exception e){\nlogger.error(e.getMessage());\n}\nreturn wordsBuffer.toString();\n}\n5 中科院ICTCLAS\nICTCLAS是由中科院计算所历经数年开发的分词工具，采用C++编写。最新版本命名为ICTCLAS2013，又名为NLPIR汉语分词系统。主要功能包括中文分词、词性标注、命名实体识别、用户词典功能，同时支持GBK编码、UTF8编码、BIG5编码，新增微博分词、新词发现与关键词提取。可以可视化界面操作和API方式调用。\n6 FudanNLP\nFudanNLP主要是为中文自然语言处理而开发的工具包，也包含为实现这些任务的机器学习算法和数据集。FudanNLP及其包含数据集使用LGPL3.0许可证。主要功能包括：\n信息检索：文本分类，新闻聚类。\n中文处理：中文分词，词性标注，实体名识别，关键词抽取，依存句法分析，时间短语识别。\n结构化学习：在线学习，层次分类，聚类，精确推理。\n工具采用Java编写，提供了API的访问调用方式。下载安装包后解压后，内容如下图所示：\n\n\n在使用时将fudannlp.jar以及lib中的jar部署于项目中的lib里面。models文件夹中存放的模型文件，主要用于分词、词性标注和命名实体识别以及分词所需的词典；文件夹example中主要是使用的示例代码，可以帮助快速入门和使用；java-docs是API帮助文档；src中存放着源码；PDF文档中有着比较详细的介绍和自然语言处理基础知识的讲解。初始运行程序时初始化时间有点长，并且加载模型时占用内存较大。在进行语法分析时感觉分析的结果不是很准确。\n\n\n转载自：http://www.cnblogs.com/baiboy/p/nltk2.html","data":"2017年11月28日 10:28:52"}
{"_id":{"$oid":"5d36c4326734bd8e681d6597"},"title":"自然语言处理的数学原理（一）","author":"光影流年925","content":"一个基本的搜索引擎的工作，基本上可以分成以下三个部分：\n利用网络爬虫下载网页，分析网页关键词，制成索引备用；\n理解用户输入，确定检索关键词；\n根据关键词和网页索引，按照相关性排序列出搜索结果。\n第一个部分主要涉及网络爬虫技术、图论、自然语言处理等技术；\n第二个部分主要涉及自然语言处理；\n第三个部分同样涉及自然语言处理。\n自然语言，即是人类用来交流的语言。\n由此可见，自然语言处理（NLP, Natural Language Processing）是现代搜索引擎很重要的内容，其终极目的是将自然语言转化为计算机容易处理的形式。\n从分词的角度来看文法分析与统计模型\n分词是 NLP 需要解决的基础问题，分词算法的好坏直接影响 NLP 的结果。\n这里我们先从一个简单的例子说起，逐步探讨合理的分词算法。\n从一个简单的句子说起\n现在有一个句子，比如：\n我去电脑城买了一台电脑。\n如果要让计算机对这个句子做分词处理，进而理解这个句子，你会有怎样的思路呢？\n大多数人首先会思考一下自己是怎么理解这个句子的。对于中国人来说，这样一个简单的句子，可能不需要什么特殊的思维过程。句子的文字形式和句子背后的含义可以在瞬间反映出来。稍有汉语文法知识的读者，可能会想：\n句子可以分成几个部分\n我 - 主语\n去电脑城买了一台电脑 - 谓语\n去电脑城 - 状语 买了 - 谓语动词 一台电脑 - 动词宾语（名词短语）\n。 - 句子结束的标识\n分别理解每个部分的意思\n将意思拼合起来，变成完整的句意\n它先通过文法分析，将句子拆分成一个二维的语法树，然后再理解各个部分的含义，最后做拼接。\n这样的方案（或者说是算法）是基于文法规则的，清晰明了，也易于实现（在计算机里就是几个循环判断）。对于程序员来说，这样的算法也特别亲切。因为程序员使用的高级编程语言（比如 C++）的语法规则和这样的方案非常相似。\n由于这样的算法直观、易于实现，所以人们相信在有了愈加全面的文法概括和愈加强大的计算能力时，人们就能彻底解决自然语言处理的问题了。\n文法分析的困境\n然而，如果你仔细观察文法分析的过程就会发现，这么一个简单的句子被分成了一个这样复杂的二维树状结构，耗费了六条注释。用计算机来处理这样一个过程当然不难，但是要处理现实生活中遇到的真实句子，往往就不那么容易了：\n由于理解（understanding）自然语言，需要关于外在世界的广泛知识以及运用操作这些知识的能力，自然语言认知，同时也被视为一个人工智能完备（AI-complete）的问题。\n这个句子依然可以用上述方法来处理：\n先分成主谓部分 再仔细拆分谓语部分\n比如：\n自然语言认知 - 主语 - 偏正短语 自然语言 - 名词作定语修饰 认知 - 名词 由于理解（understanding）自然语言，需要关于外在世界的广泛知识以及运用操作这些知识的能力 … 同时也被视为一个人工智能完备（AI-complete）的问题 - 谓语 由于理解（understanding）自然语言，需要关于外在世界的广泛知识以及运用操作这些知识的能力 - 原因状语 … … 同时也被视为 - 谓语动词短语 同时 - 状语 也被视为 - 谓语动词 一个人工智能完备（AI-complete）的问题 - 动词宾语 一个 - 定语 人工智能完备的 - 定语 问题 - 名词 。 - 句子结束的标志\n这个句子的语法分析树我没有写完，因为实在太复杂了。显而易见，单纯基于文法分析的分析器是很难处理生活中的真实句子的。\n那么问题出在哪里？我认为至少有两个问题。\n文法规则数量巨大，上万条语法规则才只能覆盖约 20% 的真实句子；且有些为了处理特殊情况的语法规则和其他规则相互矛盾。 自然语言与程序设计语言不同，自然语言中词汇的具体含义与上下文相关，而程序设计语言则没有这样的歧义性。\n从算法复杂度的角度来说，单纯基于文法分析的分析器，用于分析自然语言，其复杂度比分析程序设计语言要高出四个量级。从直观的印象来说，上述句子在一台现代计算机上用文法分析的方式处理，也需要至少一分钟的时间。这种低效是无法接受的。\n查字典分词法\n在之前的文法分析方法里，分词依赖于文法分析的结果。程序要先输出语法树，然后才能得到分词结果。而这样的方法已经被证明是低效的。\n这样的低效来源于复杂的文法分析过程。为了提高效率，人们很自然地想到：是否有办法绕开文法分析，直接尝试分词呢？对于中文分词，北京航空航天大学的梁南元教授提出了查字典分词法。做法相当简单，比如对于下列句子：\n山东大学数学学院是中国最好的数学基础教育基地之一。\n我们让计算机从左到右扫描整个句子，每扫到一个字，就往字典里查询，遇到字典里有的词就标注出来。于是整个句子就被分割成了这样：\n山东|大学|数学|学院|是|中国|最好的|数学|基础|教育|基地|之一。\n看起来结果不错。不过细心的读者很快就会发现：山东大学和基础教育都是完整的词，在它们之间不应该再做划分。会出现这种情况也不意外，我们要求计算机从左到右扫描。当计算机遇到「山东」二字的时候，就认为这是一个词了，自然不会再去寻找下一个字去寻求匹配。同理基础教育。\n梁教授提出了一个方案，即总是搜寻尽可能长的分词。这在计算机科学领域叫做「贪婪」。运用贪婪的办法，上述句子的分词就会变成：\n山东大学|数学|学院|是|中国|最好的|数学|基础教育|基地|之一。\n看起来就没什么问题了。\n不过，汉语博大精深，这种办法也不能一劳永逸。比如：\n大学生活区\n正确的分词应该是：\n大学|生活区\n但是按照贪婪的办法，会被分词成：\n大学生|活|区\n这就不对了。\n又比如：\n发展中国家\n正确的分词应该是：\n发展中|国家\n而不是：\n发展|中国|家\n可见，查字典的办法虽然效率很高，但是时有出错，并不牢靠。\n查字典的办法遇到的困境来自于自然语言的歧义性。人类在阅读自然语言时，会结合上下文判断有多个意向的词汇在文中的具体含义，但是计算机却没有这个能力。实际上，中国传统文学里说的「句读」，其目的就是通过分词断句来消除歧义。那么，怎么让计算机具备这样的能力呢？\n千呼万唤始出来的统计模型\n行文至此，数学终于要第一次展现其威力和美丽。\n我们之前提到，对一个句子做分词，其正确与否和词汇的二义性紧密相关。由于计算机无力综合上下文判断词汇含义，解决二义性，所以查字典的办法陷入了困境。\n数学中有所谓的「反证法」。在这里我们不讲反证法，但是要讲讲反证法的思想。反证法的核心思想就是「正难则反」：正面突破很困难，那就不走大路，开个后门照样进城。在这里，既然计算机没有能力综合上下文解决词汇的二义性，那么我们就不依赖计算机智能去解决，转而借助人工的力量解决。当然，我说的不是找一个工人实时干预程序的运行，帮助程序作出正确的判断；而是说，让计算机经过大量的文本训练，吸取人类的「分词经验」。而这个方法，就是统计模型。\n假定一个句子 SS 可以有几种分词的方案，比如有以下三种：\nA1,A2,A3,…,Aj (1)\nB1,B2,B3,…,Bk (2)\nC1,C2,C3,…,Cl (3)\n其中，A1, A2, B1, B2, C1, C2 等都是汉语的词汇。这样一来，如果 (1)(1) 是最好的分词，那么 (1)(1) 出现的概率应该最大。也就是说，分词方案 (1)(1) 应该满足 (4)(4)。\nP(A1,A2,A3,…,Aj)\u003eP(B1,B2,B3,…,Bk)\n(4)\nP(A1,A2,A3,…,Aj)\u003eP(C1,C2,C3,…,Cl)\n答案就是这么简单。\n当然，如何处理 (4)(4) 需要一点统计知识和技巧；得到这些分词方案也需要依靠动态规划算法（不然计算量太大）；还有诸如分词颗粒大小之类的细节问题需要处理。这些内容我们放在后续的小节里讨论，在这里，读者只需要知道这种利用统计的方法处理分词效果好、效率高就可以了。\n小结\n对于分词来说，统计模型的方法效率比文法分析的方法高，同时效果也要好。这里效率的提升是十分显著的。\n此外，我们发现一个优秀算法背后的数学模型是十分简洁优美的。统计模型只需要一个概率不等式组就可以描述，而文法分析模型几乎无法构建一个可读的数学模型。我们在设计算法的时候，要尽可能追求简洁优美的数学模型，从简单粗暴做起，逐步完善完美。正如牛顿爵士所言「真理在形式上总是简单的，而不是复杂含混的」。\n最后，文法分析方法是十分容易想到的，十分自然的处理方法，然而这种「自然」也使人误入歧途。这提醒我们，不可固执，不可迷信经验。","data":"2018年01月10日 09:07:20"}
{"_id":{"$oid":"5d36c4436734bd8e681d659f"},"title":"【总结】自然语言处理（NLP）算法：概述与分类","author":"黄辣鸡","content":"摘要：NLP概述。主要参考自然语言处理（NLP）知识结构总结和知乎上的一些问答。\n目录\nNLP界神级人物\nNLP知识结构\n1.概述\n2.形式语言与自动机\n3.语言模型\n4.概率图模型，生成模型与判别模型，贝叶斯网络，马尔科夫链与隐马尔科夫模型（HMM）\n5.马尔科夫网，最大熵模型，条件随机场（CRF）\n6.命名实体 识别，词性标注，内容挖掘、语义分析与篇章分析（大量用到前面的算法）\n7.句法分析\n8.文本分类，情感分析\n9.信息检索，搜索引擎及其原理\n10.自动文摘与信息抽取，机器翻译，问答系统\n11.深度学习在自然语言中的应用\nNLP用于征信\n参考资料\nNLP界神级人物\nMichael Colins（CU）， Jason Eisner（JHU），David Yarowsky (JHU)，Chris Manning，Dekang Lin（奇点机智）\nMichael Colins，英国人，哥伦比亚大学教授。研究领域包括parse re-ranking,tree kernels,semi-supervised learning,machine translation 和 exponentiated gradient algorithms。collins/eisner对NLP结构学习领域贡献极大，研究parsing并一战成名。\nhttp://www.cs.columbia.edu/~mcollins/\n非常喜欢 Michael Collins, 认为他写的paper看得最舒服最爽，犹如沐浴于樱花之中。Jason Eisner确实是厉害，不过看他paper实在太难看懂，写的语言非常抽象，我等屌丝实在难以深入理解。 经过Collins大侠的一些paper才能对Eisner的paper妙语进行理解。https://www.zhihu.com/question/32318281\nJason Eisner (JHU),约翰霍普金斯大学。\nhttp://www.cs.jhu.edu/~jason/\nDavid Yarowsky (JHU)，yarowsky早年研究词义消歧，是著名的yarowsky algorithm的作者，后来做了很多跨语言学习的开创性工作。\nhttp://www.cs.jhu.edu/~yarowsky/\nStanford NLP掌门Chris Manning，以《统计自然语言处理基础》一书以及Stanford NLP (toolkit) 而闻名。Dan Jurafsky，著有《语音与语言处理》一书，具有深厚的语言学背景。稍微提一下Manning的学生Richard Socher，近几年声名鹊起，在dl4nlp领域风头一时无两，属年轻一代翘楚。\nDekang Lin，林德康老师，前Google高级管理科学家（senior staff research scientist），在加入Google之前是加拿大Alberta大学计算机教授，发表过逾90篇论文、被引用超过12000次，目前做了一家NLP相关的创业公司奇点机智。\nNLP知识结构\n1.概述\n1）自然语言处理：利用计算机为工具，对书面实行或者口头形式进行各种各样的处理和加工的技术，是研究人与人交际中以及人与计算机交际中的演员问题的一门学科，是人工智能的主要内容。\n2）自然语言处理是研究语言能力和语言应用的模型，建立计算机（算法）框架来实现这样的语言模型，并完善、评测、最终用于设计各种实用系统。\n3）研究问题（主要）\n信息检索\n机器翻译\n文档分类\n问答系统\n信息过滤\n自动文摘\n信息抽取\n文本挖掘\n舆情分析\n机器写作\n语音识别\n4）困难所在\n场景的困难：语言的多样性、多变性、歧义性 学习的困难：艰难的数学模型（hmm,crf,EM,深度学习等） 语料的困难：什么的语料？语料的作用？如何获取语料？\n2.形式语言与自动机\n1）语言：按照一定规律构成的句子或者字符串的有限或者无限的集合\n2）描述语言的三种途径：\n穷举法\n文法（产生式系统）描述\n自动机\n3）自然语言不是人为设计而是自然进化的，形式语言比如：运算符号、化学分子式、编程语言，主要研究内部结构模式这类语言的纯粹的语法领域，从语言学而来，作为一种理解自然语言的句法规律，在计算机科学中，形式语言通常作为定义编程和语法结构的基础。\n4）形式语言与自动机基础知识：\n集合论\n图论\n5）自动机的应用：\n单词自动查错纠正\n词性消歧（什么是词性？什么的词性标注？为什么需要标注？如何标注？）\n6）形式语言的缺陷：\n对于像汉语，英语这样的大型自然语言系统，难以构造精确的文法\n不符合人类学习语言的习惯\n有些句子语法正确，但在语义上却不可能，形式语言无法排出这些句子\n解决方向：基于大量语料，采用统计学手段建立模型\n3.语言模型\n1）语言模型（重要）：通过语料计算某个句子出现的概率（概率表示），常用的有2-元模型，3-元模型\n2）语言模型应用：\n语音识别歧义消除例如，给定拼音串：ta shi yan yan jiu saun fa de\n可能的汉字串：踏实烟酒算法的 他是研究酸法的 他是研究算法的，显然，最后一句才符合。\n3）语言模型的启示：\n开启自然语言处理的统计方法\n统计方法的一般步骤：\n收集大量语料\n对语料进行统计分析，得出知识\n针对场景建立算法模型\n解释和应用结果\n4）语言模型性能评价，包括评价目标，评价的难点，常用指标（交叉熵，困惑度）\n5）数据平滑：\n数据平滑的概念，为什么需要平滑\n平滑的方法，加一法，加法平滑法，古德-图灵法，J-M法，Katz平滑法等\n6）语言模型的缺陷\n语料来自不同的领域，而语言模型对文本类型、主题等十分敏感\nn与相邻的n-1个词相关，假设不是很成立。\n4.概率图模型，生成模型与判别模型，贝叶斯网络，马尔科夫链与隐马尔科夫模型（HMM）\n1）概率图模型概述（什么的概率图模型，参考清华大学教材《概率图模型》）\n2）马尔科夫过程（定义，理解）\n3）隐马尔科夫过程（定义，理解）\nHMM的三个基本问题（定义，解法，应用）\n注：第一个问题，涉及最大似然估计法，第二个问题涉及EM算法，第三个问题涉及维特比算法，内容很多，要重点理解\n5.马尔科夫网，最大熵模型，条件随机场（CRF）\n1)HMM的三个基本问题的参数估计与计算\n2）什么是熵\n3）EM算法（应用十分广泛，好好理解）\n4）HMM的应用\n5）层次化马尔科夫模型与马尔科夫网络\n提出原因，HMM存在两个问题\n6）最大熵马尔科夫模型\n优点：与HMM相比，允许使用特征刻画观察序列，训练高效\n缺点： 存在标记偏置问题\n7）条件随机场及其应用(概念，模型过程，与HMM关系)\n参数估计方法（GIS算法，改进IIS算法）\nCRF基本问题：特征选取（特征模板）、概率计算、参数训练、解码（维特比）\n应用场景：\n+ 词性标注类问题（现在一般用RNN+CRF）\n+ 中文分词（发展过程，经典算法，了解开源工具jieba分词）\n+ 中文人名，地名识别\n8） CRF++\n6.命名实体 识别，词性标注，内容挖掘、语义分析与篇章分析（大量用到前面的算法）\n1）命名实体识别问题\n2）未登录词的解决方法(搜索引擎，基于语料)\n3）CRF解决命名实体识别（NER）流程总结：\n训练阶段：确定特征模板，不同场景（人名，地名等）所使用的特征模板不同，对现有语料进行分词，在分词结果基础上进行词性标注（可能手工），NER对应的标注问题是基于词的，然后训练CRF模型，得到对应权值参数值。\n识别过程：将待识别文档分词，然后送入CRF模型进行识别计算（维特比算法），得到标注序列，然后根据标 注划分出命名实体\n4）词性标注（理解含义，意义）及其一致性检查方法（位置属性向量，词性标注序列向量，聚类或者分类算法）\n7.句法分析\n1）句法分析理解以及意义\n句法结构分析\n\n完全句法分析\n浅层分析（这里有很多方法。。。）\n依存关系分析\n2）句法分析方法\n1.基于规则的句法结构分析\n2.基于统计的语法结构分析\n8.文本分类，情感分析\n1）文本分类，文本排重\n文本分类：在预定义的分类体系下，根据文本的特征，将给定的文本与一个或者多个类别相关联\n典型应用：垃圾邮件判定，网页自动分类\n2）文本表示，特征选取与权重计算，词向量\n文本特征选择常用方法：\n1、基于本文频率的特征提取法\n2、信息增量法\n3、X2（卡方）统计量\n4、互信息法\n3）分类器设计\nSVM，贝叶斯，决策树等\n4）分类器性能评测\n1、召回率\n2、正确率\n3、F1值\n5）主题模型（LDA）与PLSA\nLDA模型十分强大，基于贝叶斯改进了PLSA，可以提取出本章的主题词和关键词，建模过程复杂，难以理解。\n6）情感分析\n借助计算机帮助用户快速获取，整理和分析相关评论信息，对带有感情色彩的主观文本进行分析，处理和归纳例如，评论自动分析，水军识别。\n某种意义上看，情感分析也是一种特殊的分类问题\n9.信息检索，搜索引擎及其原理\n1）信息检索起源于图书馆资料查询检索，引入计算机技术后，从单纯的文本查询扩展到包含图片，音视频等多媒体信息检索，检索对象由数据库扩展到互联网\n1. 点对点检索\n2.精确匹配模型与相关匹配模型\n3.检索系统关键技术：标引，相关度计算\n2）常见模型：布尔模型，向量空间模型，概率模型\n3）常用技术：倒排索引，隐语义分析（LDA等）\n4）评测指标\n10.自动文摘与信息抽取，机器翻译，问答系统\n1）统计机器翻译的的思路，过程，难点，以及解决\n2）问答系统\n基本组成：问题分析，信息检索，答案抽取\n类型：基于问题-答案， 基于自由文本\n典型的解决思路\n3）自动文摘的意义，常用方法\n4）信息抽取模型（LDA等）\n11.深度学习在自然语言中的应用\n1）单词表示，比如词向量的训练（wordvoc）\n2）自动写文本 写新闻等\n3）机器翻译\n4）基于CNN、RNN的文本分类\n5）深度学习与CRF结合用于词性标注\nNLP用于征信\n近几年，国内P2P和现金贷的大量涌现，说明了个人小额信贷的市场需求巨大。在过去，针对该类小贷用户，一般单纯地依靠地推人员挨家挨户进行实地征信。如今，基于大数据和人工智能技术，可以实现智能征信和审批，极大地提高工作效率。通过多渠道获取用户多维度的数据，如通话记录、短信信息、购买历史、以及社交网络上的相关留存信息等；然后，从信息中提取各种特征建立模型，对用户进行多维度画像；最后，根据模型评分，对用户的个人信用进行评估。同样，对于市场上中小微企业融资难的问题，也可以通过大数据征信得以解决。\n信用评分模型案例：\n业务目标：建立信用评估系统，当把信用卡用户的信息导入到该系统时，系统会自动输出这批用户的违约风险及信用得分，为信用卡用户的管理提供决策支持。\n数据挖掘目标：建立信用卡用户的信用评估模型，该模型以用户的信息指标为输入，以违约为目标，建立预测模型，该模型可以根据输入指标的值，计算预测值（违约）。\n建立信用评分模型： 分类预测算法并不局限于神经网络算法，只要是适用于目标变量为字符型的分类预测算法都可以（如决策树、支持向量机、贝叶斯网络、KNN、Logistic回归等）。\nhttp://bbs.pinggu.org/thread-3823928-1-1.html\n参考资料\n黄志洪老师的自然语言处理课程\n参考书：宗成庆老师的《统计自然语言处理》\n自然语言处理（NLP）知识结构总结","data":"2018年07月29日 23:51:35"}
{"_id":{"$oid":"5d36c44f6734bd8e681d65a3"},"title":"自然语言处理常见应用领域及研究内容","author":"zzwwsun","content":"自然语言处理研究的内容包括但不限于如下分支领域：文本分类、信息抽取、自动摘要、智能问答、话题推荐、机器翻译、主题词识别、知识库构建、深度文本表示、命名实体识别、文本生成、文本分析（词法、句法、语法）、语音识别与合成等。下面给出一些分支领域的详细介绍：\n文本分类\n文本分类用计算机设备对文本集(或其他实体或物件)按照一定的分类体系或标准进行自动分类标记。\n定义\n基于分类体系的自动分类\n基于资讯过滤和用户兴趣(Profiles)的自动分类\n所谓分类体系就是针对词的统计来分类\n关键字分类，现在的全文检索\n词的正确切分不易分辨（白痴造句法）\n学习人类对文本分类的知识和策略\n从人对文本和类别之间相关性判断来学习文件用字和标记类别之间的关联\n过程\n文本分类一般包括了文本的表达、 分类器的选择与训练、 分类结果的评价与反馈等过程，其中文本的表达又可细分为文本预处理、索引和统计、特征抽取等步骤。文本分类系统的总体功能模块为：\n（1）预处理：将原始语料格式化为同一格式，便于后续的统一处理；\n（2）索引：将文档分解为基本处理单元，同时降低后续处理的开销；\n（3）统计：词频统计，项（单词、概念）与分类的相关概率；\n（4）特征抽取：从文档中抽取出反映文档主题的特征；\n（5）分类器：分类器的训练；\n（6）评价：分类器的测试结果分析。\n方法\n※ 词匹配法\n※ 知识工程\n※ 统计学习\n※ 分类算法\n现如今，统计学习方法已经成为了文本分类领域绝对的主流。主要的原因在于其中的很多技术拥有坚实的理论基础（相比之下，知识工程方法中专家的主观因素居多），存在明确的评价标准，以及实际表现良好。统计分类算法将样本数据成功转化为向量表示之后，计算机才算开始真正意义上的“学习”过程。常用的分类算法为：\n决策树，Rocchio，朴素贝叶斯，神经网络，支持向量机，线性最小平方拟合，kNN，遗传算法，最大熵，Generalized Instance Set等。\n基于资讯过滤和用户兴趣(Profiles)的自动分类\n所谓分类体系就是针对词的统计来分类\n关键字分类，现在的全文检索\n词的正确切分不易分辨（白痴造句法）\n学习人类对文本分类的知识和策略\n从人对文本和类别之间相关性判断来学习文件用字和标记类别之间的关联\n信息抽取\n信息抽取（Information Extraction: IE）是把文本里包含的信息进行结构化处理，变成表格一样的组织形式。输入信息抽取系统的是原始文本，输出的是固定格式的信息点。信息点从各种各样的文档中被抽取出来，然后以统一的形式集成在一起。这就是信息抽取的主要任务。信息以统一的形式集成在一起的好处是方便检查和比较。信息抽取技术并不试图全面理解整篇文档，只是对文档中包含相关信息的部分进行分析。至于哪些信息是相关的，那将由系统设计时定下的领域范围而定。\n简介\n信息抽取技术对于从大量的文档中抽取需要的特定事实来说是非常有用的。互联网上就存在着这么一个文档库。在网上，同一主题的信息通常分散存放在不同网站上，表现的形式也各不相同。若能将这些信息收集在一起，用结构化形式储存，那将是有益的。\n由于网上的信息载体主要是文本，所以，信息抽取技术对于那些把因特网当成是知识来源的人来说是至关重要的。信息抽取系统可以看作是把信息从不同文档中转换成数据库记录的系统。因此，成功的信息抽取系统将把互联网变成巨大的数据库！\n挑战\n信息抽取技术是近十年来发展起来的新领域，遇到许多新的挑战。\n信息抽取原来的目标是从自然语言文档中找到特定的信息，是自然语言处理领域特别有用的一个子领域。所开发的信息抽取系统既能处理含有表格信息的结构化文本，又能处理自由式文本（如新闻报道）。IE系统中的关键组成部分是一系列的抽取规则或模式，其作用是确定需要抽取的信息。网上文本信息的大量增加导致这方面的研究得到高度重视。\n纯文本抽出通用程序库\nDMCTextFilter V4.2是HYFsoft推出的纯文本抽出通用程序库，DMCTextFilter可以从各种各样的文档格式的数据中或从插入的OLE对象中，完全除掉特殊控制信息，快速抽出纯文本数据信息。便于用户实现对多种文档数据资源信息进行统一管理，编辑，检索和浏览。\nDMCTextFilter采用了先进的多语言、多平台、多线程的设计理念，支持多国语言（英语，中文简体，中文繁体，日本语，韩国语），多种操作系统（Windows，Solaris，Linux，IBM AIX，Macintosh，HP-UNIX），多种文字集合代码（GBK，GB18030，Big5，ISO-8859-1，KS X 1001，Shift_JIS，WINDOWS31J，EUC-JP，ISO-10646-UCS-2，ISO-10646-UCS-4，UTF-16，UTF-8等）。提供了多种形式的API功能接口（文件格式识别函数，文本抽出函数，文件属性抽出函数，页抽出函数，设定User Password的PDF文件的文本抽出函数等），便于用户方便使用。用户可以十分便利的将本产品组装到自己的应用程序中，进行二次开发。通过调用本产品的提供的API功能接口，实现从多种文档格式的数据中快速抽出纯文本数据。\n文件格式自动识别功能\n本产品通过解析文件内部的信息，自动识别生成文件的应用程序名和其版本号，不依赖于文件的扩展名，能够正确识别文件格式和相应的版本信息。可以识别的文件格式如下：支持Microsoft Office、RTF、PDF、Visio、OutlookEML和MSG、Lotus1-2-3、HTML、AutoCAD DXF和DWG、IGES、PageMaker、ClarisWorks、AppleWorks、XML、WordPerfect、Mac Write、Works、CorelPresentations、QuarkXpress、DocuWorks、WPS、压缩文件的LZH/ZIP/RAR以及一太郎、OASYS等文件格式\n文本抽出功能\n即使系统中没有安装作成文件的应用程序，可以从指定的文件或插入到文件中的OLE中抽出文本数据。\n文件属性抽出功能\n从指定的文件中，抽出文件属性信息。\n页抽出功能\n从文件中，抽出指定页中文本数据。\n对加密的PDF文件文本抽出功能\n从设有打开文档口令密码的PDF文件中抽出文本数据。\n流(Stream)抽出功能\n从指定的文件、或是嵌入到文件中的OLE对象中向流里抽取文本数据。\n支持的语言种类\n本产品支持以下语言：英语，中文简体，中文繁体，日本语，韩国语\n支持的字符集合的种类\n抽出文本时，可以指定以下的字符集合作为文本文件的字符集(也可指定任意特殊字符集，但需要另行定制开发)：GBK，GB18030，Big5，ISO-8859-1，KS X 1001，Shift_JIS，WINDOWS31J，EUC-JP，ISO-10646-UCS-2，ISO-10646-UCS-4，UTF-16，UTF-8等。\n自动文本摘要\n自动文本摘要是利用计算机自动地从原始文献中提取文摘，文摘是全面准确地反映某一文献中心内容地简单连贯的短文。常用方法是自动摘要将文本作为句子的线性序列，将句子视为词的线性序列。\n类型\n技术应用类型：\n·自动提取给定文章的摘要信息\n·自动计算文章中词的权重\n·自动计算文章中句子的权重\n提取\n·单篇文章的摘要自动提取\n·大规模文档的摘要自动提取\n·基于分类的摘要自动提取\n智能问答系统\n智能问答系统以一问一答形式，精确的定位网站用户所需要的提问知识，通过与网站用户进行交互，为网站用户提供个性化的信息服务。\n介绍\n智能问答系统是将积累的无序语料信息，进行有序和科学的整理，并建立基于知识的分类模型；这些分类模型可以指导新增加的语料咨询和服务信息，节约人力资源，提高信息处理的自动性，降低网站运行成本。基于对网站多年积累的关于政府和企业的基本情况常见问题及其解答，整理为规范的问答库形式，以支撑各种形式问题的智能问答。方便了用户，提高了办事效率，提升了企业形象。\n应用场景\n相关问答推送\n当网站用户提出问题时，系统不仅将问题答案推送出来，而且会将与这个问题相关的知识也都推送出来供用户查询，这样就做到了一次提问全面掌握所有信息。\n提问智能提示\n用户在提问的过程中, 系统将已经输入的内容自动分析给予优化的补全或相关提示。\n焦点问题自动排行\n对在一定的时间内，用户对知识提问的热度，系统自动聚焦，并按照访问频度将热点知识集中在系统页面上显示；具体类别的知识也按照访问频度排序，在页面知识类别栏目中显示。\n热点词聚焦\n系统对用户提交的业务关键词进行统计，并按照访问的频度进行聚焦，将与关键词相关的业务列表自动链接，形成业务热点关键词。\n在线客服问答\n模拟在线客服人员，以网站智能客服形式完成客服作用。\n引导式交互客服服务\n将常见问题整理成若干流程诊断型的知识，通过引导交互式地服务，尽量从Web端解决客户常见问题。\n客服座席协助\n完成专家坐席功能，在普通坐席人员无法回答问题时提供标准化的知识协助，帮助普通客服人员快速，准确回答。\n转人工客服\n用户可以直接在智能咨询服务系统中连接人工客服人员，向客服人员进行在线咨询。\n话题推荐\n话题推荐只是推荐系统的一个小小的应用分支，下面主要通过介绍推荐系统来了解话题推荐的大致内容。\n背景\n现在社会的信息过载，为了更好的对过载的信息进行有效的过滤。\n推荐系统\n推荐系统是利用电子商务网站向客户提供商品信息和建议，帮助用户决定应该购买什么产品，模拟销售人员帮助客户完成购买过程。个性化推荐是根据用户的兴趣特点和购买行为，向用户推荐用户感兴趣的信息和商品。现在的应用领域更为广泛，比如今日头条的新闻推荐，购物平台的商品推荐，直播平台的主播推荐，知乎上的话题推荐等等。\n定义\n推荐系统有3个重要的模块：用户建模模块、推荐对象建模模块、推荐算法模块。推荐系统把用户模型中兴趣需求信息和推荐对象模型中的特征信息匹配，同时使用相应的推荐算法进行计算筛选，找到用户可能感兴趣的推荐对象，然后推荐给用户。\n常用推荐方法\n基于内容推荐\n基于内容的推荐（Content-based Recommendation）是信息过滤技术的延续与发展，它是建立在项目的内容信息上作出推荐的，而不需要依据用户对项目的评价意见，更多地需要用机器学习的方法从关于内容的特征描述的事例中得到用户的兴趣资料。在基于内容的推荐系统中，项目或对象是通过相关的特征的属性来定义，系统基于用户评价对象的特征，学习用户的兴趣，考察用户资料与待预测项目的相匹配程度。用户的资料模型取决于所用学习方法，常用的有决策树、神经网络和基于向量的表示方法等。基于内容的用户资料是需要有用户的历史数据，用户资料模型可能随着用户的偏好改变而发生变化。\n基于内容推荐方法的优点是：\n1）不需要其它用户的数据，没有冷开始问题和稀疏问题。\n2）能为具有特殊兴趣爱好的用户进行推荐。\n3）能推荐新的或不是很流行的项目，没有新项目问题。\n4）通过列出推荐项目的内容特征，可以解释为什么推荐那些项目。\n5）已有比较好的技术，如关于分类学习方面的技术已相当成熟。\n缺点是要求内容能容易抽取成有意义的特征，要求特征内容有良好的结构性，并且用户的口味必须能够用内容特征形式来表达，不能显式地得到其它用户的判断情况。\n基于用户的系统过滤推荐过程\n协同过滤推荐（Collaborative Filtering Recommendation）技术是推荐系统中应用最早和最为成功的技术之一。它一般采用最近邻技术，利用用户的历史喜好信息计算用户之间的距离，然后 利用目标用户的最近邻居用户对商品评价的加权评价值来预测目标用户对特定商品的喜好程度，系统从而根据这一喜好程度来对目标用户进行推荐。协同过滤最大优 点是对推荐对象没有特殊的要求，能处理非结构化的复杂对象，如音乐、电影。\n协同过滤是基于这样的假设：为一用户找到他真正感兴趣的内容的好方法是首先找到与此用户有相似兴趣的其他用户，然后将他们感兴趣的内容推荐给此用户。其基本思想非常易于理解，在日常生活中，我们往往会利用好朋友的推荐来进行一些选择。协同过滤正是把这一思想运用到电子商务推荐系统中来，基于其他用户对某一内容的评价来向目标用户进行推荐。\n基于协同过滤的推荐系统可以说是从用户的角度来进行相应推荐的，而且是自动的即用户获得的推荐是系统从购买模式或浏览行为等隐式获得的，不需要用户努力地找到适合自己兴趣的推荐信息，如填写一些调查表格等。\n和基于内容的过滤方法相比，协同过滤具有如下的优点：\n1）能够过滤难以进行机器自动内容分析的信息，如艺术品，音乐等。\n2）共享其他人的经验，避免了内容分析的不完全和不精确，并且能够基于一些复杂的，难以表述的概念（如信息质量、个人品味）进行过滤。\n3）有推荐新信息的能力。可以发现内容上完全不相似的信息，用户对推荐信息的内容事先是预料不到的。这也是协同过滤和基于内容的过滤一个较大的差别，基于内容的过滤推荐很多都是用户本来就熟悉的内容，而协同过滤可以发现用户潜在的但自己尚未发现的兴趣偏好。\n4）能够有效的使用其他相似用户的反馈信息，较少用户的反馈量，加快个性化学习的速度。\n虽然协同过滤作为一种典型的推荐技术有其相当的应用，但协同过滤仍有许多的问题需要解决。最典型的问题有稀疏问题（Sparsity）和可扩展问题（Scalability）。\n基于关联规则推荐\n基于关联规则的推荐（Association Rule-based Recommendation）是以关联规则为基础，把已购商品作为规则头，规则体为推荐对象。关联规则挖掘可以发现不同商品在销售过程中的相关性，在零售业中已经得到了成功的应用。管理规则就是在一个交易数据库中统计购买了商品集X的交易中有多大比例的交易同时购买了商品集Y，其直观的意义就是用户在购买某些商品的时候有多大倾向去购买另外一些商品。比如购买牛奶的同时很多人会同时购买面包。\n算法的第一步关联规则的发现最为关键且最耗时，是算法的瓶颈，但可以离线进行。其次，商品名称的同义性问题也是关联规则的一个难点。\n基于效用推荐\n基于效用的推荐（Utility-based Recommendation）是建立在对用户使用项目的效用情况上计算的，其核心问题是怎么样为每一个用户去创建一个效用函数，因此，用户资料模型很大程度上是由系统所采用的效用函数决定的。基于效用推荐的好处是它能把非产品的属性，如提供商的可靠性（Vendor Reliability）和产品的可得性（Product Availability）等考虑到效用计算中。\n基于知识推荐\n基于知识的推荐（Knowledge-based Recommendation）在某种程度是可以看成是一种推理（Inference）技术，它不是建立在用户需要和偏好基础上推荐的。基于知识的方法因它们所用的功能知识不同而有明显区别。效用知识（Functional Knowledge）是一种关于一个项目如何满足某一特定用户的知识，因此能解释需要和推荐的关系，所以用户资料可以是任何能支持推理的知识结构，它可以是用户已经规范化的查询，也可以是一个更详细的用户需要的表示。\n组合推荐\n由于各种推荐方法都有优缺点，所以在实际中，组合推（HybridRecommendation）经常被采用。研究和应用最多的是内容推荐和协同过滤推荐的组合。最简单的做法就是分别用基于内容的方法和协同过滤推荐方法去产生一个推荐预测结果，然后用某方法组合其结果。尽管从理论上有很多种推荐组合方法，但在某一具体问题中并不见得都有效，组合推荐一个最重要原则就是通过组合后要能避免或弥补各自推荐技术的弱点。\n在组合方式上，有研究人员提出了七种组合思路：\n1）加权（Weight）：加权多种推荐技术结果。\n2）变换（Switch）：根据问题背景和实际情况或要求决定变换采用不同的推荐技术。\n3）混合（Mixed）：同时采用多种推荐技术给出多种推荐结果为用户提供参考。\n4）特征组合（Feature combination）：组合来自不同推荐数据源的特征被另一种推荐算法所采用。\n5）层叠（Cascade）：先用一种推荐技术产生一种粗糙的推荐结果，第二种推荐技术在此推荐结果的基础上进一步作出更精确的推荐。\n6）特征扩充（Feature augmentation）：一种技术产生附加的特征信息嵌入到另一种推荐技术的特征输入中。\n7）元级别（Meta-level）：用一种推荐方法产生的模型作为另一种推荐方法的输入。\n体系结构\n服务器端推荐系统\n推荐系统的体系结构研究的重要问题就是用户信息收集和用户描述文件放在什么地方，服务器还是客户机上，或者是处于二者之间的代理服务器上。\n最初的推荐系统都是基于服务器端的推荐系统。在这类推荐系统中，推荐系统与Web服务器一般共享一台硬件设备。在逻辑上，推荐系统要的用户信息收集和建模都依赖于Web服务器。\n由此可知，基于服务器端的推荐系统存在的问题主要包括：\n（1）个性化信息的收集完全由Web服务器来完成，受到了Web服务器功能的限制。\n（2）增加了Web服务器的系统开销。\n（3）对用户的隐私有极大威胁。无论是推荐系统的管理者还是入侵推荐系统的人员都能方便地获取存放在服务器上的用户数据。由于用户的个人数据是有很高价值的，接触到用户数据的部分人会出卖用户数据或把用户数据用于非法用途。\n客户端推荐系统\n基于客户端推荐系统：典型的客户端个性化服务系统有斯坦福大学的LIRA、麻省理工学院的Letizia、加州大学的Syskill\u0026Webert、卡内基·梅隆大学的PersonalWeb-Watcher等。\n基于客户端的推荐系统有如下优点：\n（1）由于用户的信息就在本地收集和处理，因而不但能够获取丰富准确的用户信息以构建高质量的用户模型。\n（2）少量甚至没有用户数据存放在服务器上，Web服务器不能访问和控制用户的数据，能比较好地保护用户的隐私。\n（3）用户更愿意向推荐系统提供个人信息，从而提高推荐系统的推荐性能。因为基于客户端的推荐系统中的用户数据存储在用户本地客户机上，用户对数据能够进行自行控制。\n基于客户端的推荐系统有一定缺点：\n（1）用户描述文件的形成、推荐策略的应用都依赖于所有用户数据分析的基础上进行的，而基于客户端的推荐系统较难获取其他用户的数据，用户描述文件较难得到，协同推荐策略实施也较难，所以推荐系统要重新设计，尤其是推荐策略必须进行修改。\n（2）个性化推荐处理过程中用户的数据资料还需要部分的传给服务器，存在隐私泄漏的危险，需要开发安全传输平台进行数据传输。\n知名团队\n明尼苏达大学GroupLens（John Riedl, Joseph A.Konstan）\n密西根大学（Paul Resnick）\n卡内基梅隆大学（JaimeCallan）\n微软研究院（Ryen W.White）\n纽约大学（Alexander Tuzhilin）\n百分点科技团队（Baifendian）\n机器翻译\n机器翻译又称为自动翻译，是利用计算机将一种自然语言(源语言)转换为另一种自然语言(目标语言)的过程。它是计算机语言学的一个分支，是人工智能的终极目标之一，具有重要的科学研究价值。\n基础\n机器翻译技术的发展一直与计算机技术、信息论、语言学等学科的发展紧密相随。从早期的词典匹配，到词典结合语言学专家知识的规则翻译，再到基于语料库的统计机器翻译，随着计算机计算能力的提升和多语言信息的爆发式增长，机器翻译技术逐渐走出象牙塔，开始为普通用户提供实时便捷的翻译服务。\n类别\n·基于规则的机译系统\n·基于统计\n·基于人工神经网络\n·在线机译\n主题词识别\n主题词识别应该是主题词提取的研究内容，通过对各类文本经过主题模型算法得到文本的主题词，对文本主题词进行分析和识别来实现对基于内容的文本匹配等功能。主要还是基于文本处理的基本方法：TF-IDF的文本表示，向量的文本表示方法，文本分词技术，主题模型等来提取主题词。\n如有理解上的错误请指正，如以后有新的理解会有所改进。\n知识库构建\n知识库是用于指示管理的一种特殊的数据库，以便于有关领域只是的采集、整理以及提取。知识库中的知识源于领域专家它是求解问题所需领域知识的集合，包括基本事实、规则和其它有关信息。\n现有知识库的构建常以本体论作为基础知识。本体论基础知识详见本体论研究综述论文的总结。\n特点\n1）知识库中的知识根据它们的应用领域特征、背景特征（获取时的背景信息）、使用特征、属性特征等而被构成便于利用的、有结构的组织形式。知识片一般是模块化的。\n2）知识库的知识是有层次的。最低层是“事实知识”，中间层是用来控制“事实”的知识（通常用规则、过程等表示）；最高层次是“策略”，它以中间层知识为控制对象。策略也常常被认为是规则的规则。因此知识库的基本结构是层次结构，是由其知识本身的特性所确定的。在知识库中，知识片间通常都存在相互依赖关系。规则是最典型、最常用的一种知识片。\n3）知识库中可有一种不只属于某一层次（或者说在任一层次都存在）的特殊形式的知识——可信度（或称信任度，置信测度等）。对某一问题，有关事实、规则和策略都可标以可信度。这样，就形成了增广知识库。在数据库中不存在不确定性度量。因为在数据库的处理中一切都属于“确定型”的。\n4）知识库中还可存在一个通常被称作典型方法库的特殊部分。如果对于某些问题的解决途径是肯定和必然的，就可以把其作为一部分相当肯定的问题解决途径直接存储在典型方法库中。这种宏观的存储将构成知识库的另一部分。在使用这部分时，机器推理将只限于选用典型方法库中的某一层体部分。\n功能\n1.知识库使信息和知识有序化，是知识库对组织的首要贡献。\n2.知识库加快知识和信息的流动，有利于知识共享与交流。\n3.知识库还有利于实现组织的协作与沟通。\n4.知识库可以帮助企业实现对客户知识的有效管理。\n缺陷\n不完整性\n⒈悬挂条件\n如果该规则的任意前提条件都不出现在数据库中，也不出现在所有规则的结论部分，则该规则永远不会被激活。\n2.无用结论\n如果一个规则结论部分的谓词没有在知识库中任何规则的前提条件中出现，该谓词称为无用条件。\n3.孤立规则\n如果一个规则前提部分的谓词都是悬挂条件，并且其结论部分的谓词都是无用结论，则称该规则为孤立的。\n不一致性\n⒈冗余规则\n⒉包含规则\n⒊循环规则\n⒋冲突规则\n文本表示\n要使得计算机能高效的处理真实文本，就必须找到一种理想的形式化表示方法，这种表示一方面能真实的反映文档内容(主题、领域或结构等)，另一方面也要有对不同文档的区分能力。\n随着对文本处理的要求越来越高，在对文本表示的的形式上的研究也在不断的进步和发展，下面给出一些常用的基于文本内容的表示方法：（以后补充基于行为的表示）\n传统的方法\n（1）one hot encoding\n入门级文本表示方法，应用词袋模型（BOW）+ TF-IDF技术，优点是简单粗暴配合LR效果也不赖，缺点也明显，维度太高且有词义鸿沟问题，不适合大语料。\n（2）主题模型系列\n1）LSA/LSI：将文档为行，词为列，表示成“文档-词”大矩阵，利用SVD（奇异值分解）矩阵分解的实现技术，训练得到词和文档的特征向量，有点儿像推荐里的隐语义模型，模型忽略了语序，更注重主题相关，适合长文本，实际使用效果还不错。\n2）LDA：比较适合长文本表示，不太适合短文本表示。LDA属于一种文档主题生成模型，引用其他博客的话：“LDA认为一篇文档的每个词都是通过“以一定概率选择了某个主题，并从这个主题中以一定概率选择某个词语”这样一个过程得到的，文档到主题服从多项式分布，主题到词语服从多项式分布”。\n深度表示（NN：神经网络）\n（1）word2vec + TF-IDF加权平均\n虽然word2vec非DNN系列，但其训练词向量效率和效果均表现不俗。首先通过word2vec训练词向量，再通过简单的词加权/关键tag加权/tf-idf加权平均得到文档向量表示。在加权之前做停用词剔除、词聚类等预处理是个不错的选择。（PS：该方法对短文本效果还可以，长文本就不咋地了。kaggle101中的一个word2vec题目的tutorial里作者如是说：他试了一下简单加权和各种加权，不管如何处理，效果还不如01，归其原因作者认为加权的方式丢失了最重要的句子结构信息和词相关信息（也可以说是词序信息），而doc2vec的方法则保存了这种信息。）\n（2）doc2vec\n提到word2vec就不得的不提doc2vec，二者亲兄弟，doc2vec在word2vec基础上增加了个段落向量，能直接训练处（段落）文档向量，在实际使用中，貌似效果一般，特别是长文本NLP相关任务。（PS：gensim有现成的API）。参考：2014 ICML《Distributed Representations of Sentences and Documents》\n（3）WMD\nICML2015的论文《From Word Embeddings To Document Distances, Kusner, Washington University》新提出一种计算doc相似度的方式，大致思路是将词之间的\n余弦距离作为ground distance，词频作为权重，在权重的约束条件下，求WMD的线性规划最优解。\n（4）glove\n最近学术界兴起了glove的方法，核心思想就是挖掘词语共现信息的内在含义，融合基于全局统计的方法（如LSI/LSA等）和基于局部预测方法（如word2vec等）于一体，貌似效果不错，在词聚类任务上的效果超越了word2vec。PS：《GloVe: Global Vectors forWord Representation》\n命名实体识别\n命名实体识别（Named Entity Recognition，简称NER），又称作“专名识别”，是指识别文本中具有特定意义的实体，主要包括人名、地名、机构名、专有名词等。\n作用\n命名实体识别是信息提取、问答系统、句法分析、机器翻译、面向Semantic Web的元数据标注等应用领域的重要基础工具，在自然语言处理技术走向实用化的过程中占有重要地位。一般来说，命名实体识别的任务就是识别出待处理文本中三大类（实体类、时间类和数字类）、七小类（人名、机构名、地名、时间、日期、货币和百分比）命名实体。\n过程\n通常包括两部分：（1）实体边界识别；（2） 确定实体类别（人名、地名、机构名或其他）。英语中的命名实体具有比较明显的形式标志（即实体中的每个词的第一个字母要大写），所以实体边界识别相对容易，任务的重点是确定实体的类别。和英语相比，汉语命名实体识别任务更加复杂，而且相对于实体类别标注子任务，实体边界的识别更加困难。\n难点\n（1）汉语文本没有类似英文文本中空格之类的显式标示词的边界标示符，命名实体识别的第一步就是确定词的边界，即分词；\n（2）汉语分词和命名实体识别互相影响；\n（3）除了英语中定义的实体，外国人名译名和地名译名是存在于汉语中的两类特殊实体类型；\n（4）现代汉语文本，尤其是网络汉语文本，常出现中英文交替使用，这时汉语命名实体识别的任务还包括识别其中的英文命名实体；\n（5）不同的命名实体具有不同的内部特征，不可能用一个统一的模型来刻画所有的实体内部特征。\n文本分析\n介绍\n文本分析是指对文本的表示及其特征项的选取；文本分析是文本挖掘、信息检索的一个基本问题，它把从文本中抽取出的特征词进行量化来表示文本信息。文本（text），与讯息（message）的意义大致相同，指的是有一定的符号或符码组成的信息结构体，这种结构体可采用不同的表现形态，如语言的、文字的、影像的等等。文本是由特定的人制作的，文本的语义不可避免地会反映人的特定立场、观点、价值和利益。因此，由文本内容分析，可以推断文本提供者的意图和目的。\n特征\n将它们从一个无结构的原始文本转化为结构化的计算机可以识别处理的信息，即对文本进行科学的抽象，建立它的数学模型，用以描述和代替文本。使计算机能够通过对这种模型的计算和操作来实现对文本的识别。由于文本是非结构化的数据,要想从大量的文本中挖掘有用的信息就必须首先将文本转化为可处理的结构化形式。目前人们通常采用向量空间模型来描述文本向量,但是如果直接用分词算法和词频统计方法得到的特征项来表示文本向量中的各个维,那么这个向量的维度将是非常的大。这种未经处理的文本矢量不仅给后续工作带来巨大的计算开销,使整个处理过程的效率非常低下,而且会损害分类、聚类算法的精确性,从而使所得到的结果很难令人满意。因此,必须对文本向量做进一步净化处理,在保证原文含义的基础上,找出对文本特征类别最具代表性的文本特征。为了解决这个问题,最有效的办法就是通过特征选择来降维。\n目前有关文本表示的研究主要集中于文本表示模型的选择和特征词选择算法的选取上。用于表示文本的基本单位通常称为文本的特征或特征项。特征项必须具备一定的特性:\n1)特征项要能够确实标识文本内容;\n2)特征项具有将目标文本与其他文本相区分的能力;\n3)特征项的个数不能太多;\n4)特征项分离要比较容易实现。\n在中文文本中可以采用字、词或短语作为表示文本的特征项。相比较而言，词比字具有更强的表达能力，而词和短语相比，词的切分难度比短语的切分难度小得多。因此，目前大多数中文文本分类系统都采用词作为特征项，称作特征词。这些特征词作为文档的中间表示形式，用来实现文档与文档、文档与用户目标之间的相似度计算 。如果把所有的词都作为特征项，那么特征向量的维数将过于巨大，从而导致计算量太大，在这样的情况下，要完成文本分类几乎是不可能的。特征抽取的主要功能是在不损伤文本核心信息的情况下尽量减少要处理的单词数，以此来降低向量空间维数，从而简化计算，提高文本处理的速度和效率。文本特征选择对文本内容的过滤和分类、聚类处理、自动摘要以及用户兴趣模式发现、知识发现等有关方面的研究都有非常重要的影响。通常根据某个特征评估函数计算各个特征的评分值，然后按评分值对这些特征进行排序，选取若干个评分值最高的作为特征词，这就是特征选择(Feature Selection)。\n特征选取的方式常见的有4种：\n(1)用映射或变换的方法把原始特征变换为较少的新特征。\n(2)从原始特征中挑选出一些最具代表性的特征。\n(3)根据专家的知识挑选最有影响的特征。\n(4)用数学的方法进行选取，找出最具分类信息的特征，这种方法是一种比较精确的方法，人为因素的干扰较少，尤其适合于文本自动分类挖掘系统的应用。\n以后会补充一篇结合《自然语言处理综述》这本书，从词法、句法、语法、语用方面来介绍文本分析。\n句法分析\n句法分析：(Parsing)就是指对句子中的词语语法功能进行分析，比如“我来晚了”，这里“我”是主语，“来”是谓语，“晚了”是补语。\n应用\n句法分析现在主要的应用在于中文信息处理中，如机器翻译等。它是语块分析（chunking）思想的一个直接实现，语块分析通过识别出高层次的结构单元来简化句子的描述。从不同的句子中找到语块规律的一条途径是学习一种语法，这种语法能够解释我们所找到的分块结构。这属于语法归纳的范畴。\n迄今为止，在句法分析领域中存在很多争议，也许你会发现恰巧有人提出了与你正在努力研究的语法归纳程序偶然产生的相似的句法结构，而且这些也可能已经被当成了句法结构模型的证据。但是，这些找到的结构依赖于学习程序中隐含的归纳偏置。这也指明了另外一个方向，我们需要事先知道模型能够找到什么样的结构，同时应该首先确定我们对句子进行句法分析的目的。这里有各种可能的目的：使用句法结构作为语义解释的第一步；识别短语语块，为信息检索系统的索引服务；构建一个概率句法分析器作为一个优于n元语法的语言模型。这些问题的共同目标是构建这样的一个系统：对于任意的句子都能够主产生证明有用的结构，也就是要构建一个句法分析器。\n句法分析的三种不同的途径可以利用概率：\n1、利用概率来确定句子：一种可能的做法是将句法分析器看成是一个词语网络上的语言模型，用来确定什么样的词序列经过网络的时候会获得最大概率。\n2、利用概率来加速语法分析： 第二个目标是利用概率对句法分析器的搜索空间进行排序或剪枝。这使得句法分析器能够在不影响结果质量的情况下尽快找到最优的分析途径。\n3、利用概率选择句法分析结果： 句法分析器可以从输入句子的众多分析结果中选择可能性最大的。\n语音识别技术\n语音识别技术，也被称为自动语音识别Automatic Speech Recognition，(ASR)，其目标是将人类的语音中的词汇内容转换为计算机可读的输入，例如按键、二进制编码或者字符序列。\n简介\n语音识别技术的应用包括语音拨号、语音导航、室内设备控制、语音文档检索、简单的听写数据录入等。语音识别技术与其他自然语言处理技术如机器翻译及语音合成技术相结合，可以构建出更加复杂的应用，例如语音到语音的翻译。\n语音识别技术所涉及的领域包括：信号处理、模式识别、概率论和信息论、发声机理和听觉机理、人工智能等等。\n语音识别技术和语音合成技术是比较大的两个研究分支，涉及的内容比较多、比较广，在此不做过多的介绍。如有可能，以后补充。","data":"2017年11月17日 21:08:18"}
{"_id":{"$oid":"5d36c52e6734bd8e681d65de"},"title":"《从零开始学习自然语言处理(NLP)》-基础准备(0)","author":"weixin_34392843","content":"写在最前面\n在这个日新月异的信息时代，海量数据的积累，计算能力的不断提升，机器学习尤其是深度学习的蓬勃发展，使得人工智能技术在不同领域焕发出蓬勃的活力。自己经历了嵌入式开发，移动互联网开发，目前从事自然语言处理算法开发工作。从工程软件开发到自然语言处理算法开发，希望通过这个系列的文章，能够由浅入深，通俗易懂的介绍自然语言处理的领域知识，分享自己的成长，同大家一起进步。\n信心与兴趣\n很多同学提到算法可能就会打退堂鼓，尤其是一直从事纯工程的软件开发。工作中连经典的数据结构都很少使用，更不用提五花八门的机器学习和深度学习算法。尤其各个大厂的算法专家、数据科学家都是背景爆表，动不动就是国外名校的Phd，至少也是国内清北、C9。实际上，针对普通的算法开发岗，从学习梯度上来讲，算法和工程的差异并不大。当然也并不需要非要名校硕博。\n以前经常和同事开玩笑说，好歹大家都是985的本硕，但手上这工作，找个高中生也能妥妥搞定啊。虽然是玩笑，线上的产品当然也不是像Demo那么简单，但真实的工作真的没有那么明显的条条框框限制。自己也是面试官，百度这样的大厂对普通的开发要求也只是大专。针对普通的算法开发，自己觉得本科基本足够了，当然更没有像211/985这样的限制。\n一般来讲，从事某个领域的工作，从底层基础到业务实现一般包括如下的几个层次（以互联网移动开发和算法开发为例）：\n方向\n通用基础\n专业基础\n领域基础\n业务方向\n移动开发\n操作系统、计算机网络、编译原理、数据结构、编程语言、设计模式\nAndroid开发、iOS开发\n驱动开发、Framework开发、应用开发\n电商、社交、智能硬件\n算法开发\n编程语言(python)、数学基础(线性代数、概率/统计、微积分)\n机器学习，深度学习\n语音、自然语言理解、计算机视觉、推荐、计算广告、风控\n搜索，智能客服，广告推荐，互联网金融\n从技能栈的对比来看，算法开发对数学要求要高些，这又会让很多同学看着头痛。实际上对数学的恐惧主要原因在于，对大多数人来说，数学主要是用来应付作业和考试的，而很少在真实的工程场景中使用它。\n自己的本科专业是电子信息方向（觉得空闲时间多，顺手拿了个计算机科学与技术的双学士），有一些专业课程也蛮让人头痛的，比如《通信原理》、《信号与系统》、《数字信号处理》、《微机原理》、《数字电路》等，五花八门的抽象概念和算法公式。当时参加全国大学生电子设计竞赛，选入的学校的电子设计校队，参加系统的学习和培训（有半年的时间参加集中培训，不用像其他同学一样日常上课）：\n当大家还在纠结《通信系统》里的调制解调原理时，我们已经在做单边带调制收音机了；\n当大家还在纠结《信号与系统》的傅里叶变换时，我们已经在做基于快速傅里叶变换的频谱仪了；\n当大家还在纠结《数字信号处理》里的滤波器时，我们已经在做50Hz及其谐波过滤的工频陷波器了；\n当大家还在纠结《微机原理》的X86汇编指令时，我们已经用上工业级的Msp430和C8051F系列控制器了；\n当大家还在纠结《数字电路》的逻辑控制和抱怨VHDL难学时，我们已经在用Verilog玩Altera的FPGA了。\n本来是非常枯燥有难以对付的专业课，但有了实际的使用场景和工程实践，反而变得非常有趣，不但专业课的教材就在手边经常翻阅，还会专门找相应的Paper，看看有没有更好更新的方案。自己对这些专业课不但没有反感，反而觉得超级有用，知识就是力量，在这一刻特别贴切。\n我们回过头来说数学，也是同样的体验。算法的开发工作为数学和算法提供了实践的土壤，理论有了实践这块沃土，也就不再那么枯燥和晦涩。像很多数学大神一样徒手推公式确实是件很难的事情，但基于基础的数学知识来解决工程问题，这并没有想象中的那么难。而且，学习本来就是一个往复的过程，先有一个大概，尝试用已有的知识解决问题，当问题解决不掉时，再反过来学习自己欠缺的知识。\n说了信心再说兴趣，很多同学会觉得目前机器学习和深度学习大火，是不是就该放弃手上的工程岗位，全力以赴的加入到算法的大军中。自己觉得要不加入这个方向还是看个人兴趣吧，现在算法岗位炙手可热，但三五年之后就不好说了。典型的就是Android/iOS移动开发，10年左右如日中天，市场蓝海，人才紧俏，公司抢人的盛况应该跟现在差不多。收益的决定因素是市场，但个人的成长从长期来看，还要看自己的兴趣。依照T型能力理论来说，深度方面，前端、后台、架构、嵌入式、客户端都OK；广度方面，机器学习和深度学习不论从是否要从事相关的领域开发，花些时间了解和学习一下，总的来说也还是不错的。\n好了，下面我们就具体看看从事自然语言处理需要准备的基础知识吧。\n编程语言\n编程语言的要求是很低的，弄掌握Python就基本OK。我的自己经验是：\n1 在网上找一篇Python入门的帖子，搭建环境，运行简单的例子（半天）\n2 找一本基础书籍，系统的熟悉下语言的基本特性和完整框架（1~2天）\nps：我之前有C/C++和Java的语音基础\n3 开始正常使用Python进行开发，遇到问题，求助搜索引擎（2个月以上）\n4 觉得自己对语言就基本的掌握后，可以根据选择进行进阶学习了\n系统的基础学习可以参考：《Python基础教程》\n晋级学习可以参考：《流畅的Python》\n经典数据结构与算法\n经典的数据结构和算法，主要指数组、链表、队列、堆栈、树、图这样的经典数据结构，以及各种排序/查找、深度搜索、广度搜索、最小路径、Hash等算法。对一般的算法开发，这部分不是必须项。如果时间有限，可以跳过这部分。但从长远来看，无论是普通的工程开发还是算法开发，经典的数据结构和算法还是必要了。\n落实到具体的学习上，基础的《数据结构与算法》应该随便一本教科书都OK。在实际操作方面，可以抽空刷一刷LeetCode，书的话可以考虑《进军硅谷，程序员面试解密》，内容基本都是LeetCode的原题，可以先刷题再看书。如果LeetCode上能持续刷上200+的题目，以后面对经典的数据结构和算法问题，应该也就没有恐惧的感觉了，反而遇到新问题还会饶有兴趣的去研究研究解决方案。\n数学基础\n对于算法开发，本科学习的《微积分》、《线性代数》、《概率/统计》基本上就够用了。有些同学会提到《概率图模型》和《凸优化》等。我觉得对于入门来说，前面的三门课基本够用，后面如果真的觉得需要深入，再看后面的内容也不迟。\n如果觉得当时大学的课程成绩还不错，那最快的方法就是把大学的教材拿出来重新再扫一般即可。如果觉得时间充裕，想再系统的学习一遍，在线视频是个不错的选择，\nMIT：《单变量微积分》\nMIT：《线性代数》\n可汗学院：《概率》\n可汗学院：《统计学》\n学习的过程不要太苛求全面理解，没必要像数学大牛一样能把所有的公式都手动的推导一遍。能理解基本的概念和原理，关键是有系统的知识框架，后面遇到细节问题，可以回过头来再看。\n机器学习\n这部分目前资料应该是铺天盖地的多，当然还有各式各样的培训班。自己觉得从入门角度，只要把吴恩达在Coursera上的机器学习课程完整的学习完，并完成作业拿到证书，机器学习这部分就算及格了。\nCoursera：《MachineLearning》\n一定要完成作业！一定要完成作业！一定要完成作业！重要的事情说三遍。\nCoursera不翻墙速度好像有些慢，网易云课堂也有视频，但不能提交作业作业。\n至于书的话，可以看看周志华老师的“西瓜书”：《机器学习》\n深度学习\n看完前面机器学习的内容你会发现两个事情：\n机器学习的各种算法好像都是些非常经典的算法，基本都是2000年之前提出的\n现在大家都在搞深度学习了，这些老掉牙的算法越来越没人用了\n真实情况也大抵如此，如果直接跳过机器学习直接学习深度学习可以吗？当然可以啊，而且，直接学习深度学习还会觉得深度学习的入门门槛更低。\n这估计会让很多同学觉得反常识。但反过来你问问自己，如果没有学习过汇编，上来就用C语言搞嵌入式应用开发可以吗？没有学习过C++，上来就用Java搞Android开发可以吗？当然可以啊。从入门角度没问题，但从长期来看还是需要的，想深入一个领域，知识的完备是必要的。比如嵌入开发用汇编写过两级中断向量表，做Android开发也开发过JNI。\n深度学习也是一样，从最快入门的角度来看，可以跳过机器学习，直接进入深度学习，但从长期看机器学习还是必要的。而且，看完了吴恩达的《机器学习》课程，再看他的深度学习也更流畅。\nCoursera：《DeepLearning》\n一定要完成作业！一定要完成作业！一定要完成作业！重要的事情说三遍。\n补充下，吴恩达在Coursera上的课程，《MachineLearning》是免费观看的，在线作业是付费的。《DeepLearning》无论是视频还是在线作业都是付费的。网易云课堂有免费的视频课程，但不能提交在线作业。其实，Coursera上的课程也挺便宜的，做完作业还有证书，还是挺不错的。\n至于书的话，可以看看“花书”《深度学习》\n自然语言处理\n好了终于到了这篇文章的核心部分了，因为是专业基础，所有市面上的系统学习资料也就没前面那么多了。但仔细找找的话也不少（都需要翻墙，手动捂脸）：\n经典自然语言处理（斯坦福）：《Natural Language Processing course by Dan Jurafsky and Christopher Manning》\n经典+深度学习（National Research University Higher School of Economics）：《Natural Language Processing》\n深度学习自然语言处理（斯坦福）：《Natural language processing with DeepLearning》\n深度学习自然语言处理（CMU）：《CMU Neural Nets for NLP》\n除了第一个之外，后面的视频都比较新。相对来说，书的资料就相对滞后了，如果有兴趣可以翻翻经典的宗成庆的《统计自然语言处理》，主要是针对经典的自然语言处理方法，虽然方法是经典的，但领域问题的描述还是很全面的。\n小节\n总的来说，算法开发和普通的工程开发，从学习梯度上来讲差别不大，也不需要高大上的背景和学历。确定兴趣，看好方向，就坚持一步一步积累，理论结合实践，相信很快你也能像我一样，跨过算法开发的门槛，体会用算法解决实际工程问题的乐趣。","data":"2019年01月12日 23:02:00"}
{"_id":{"$oid":"5d36c5506734bd8e681d65eb"},"title":"自然语言处理扫盲（一）","author":"QFR璠璠瑜","content":"大部分内容摘抄自知乎相关问题\n作者：微软亚洲研究院\n作者：陈见耸\n作者：刘知远\n背景知识\n自然语言处理是一门交叉的学科\n概率论：需要了解概率、条件概率、贝叶斯法则；二项分布、期望、方差；最大似然估计、梯度下降等等\n统计学：建模、数据稀疏问题、回退方法等\n机器学习：分类、感知器、支持向量机\n语言学：构词、词类、句法、语义；语料库和知识库等等\n建议1：如何在NLP领域快速学会第一个技能？\n我的建议是：找到一个开源项目，比如机器翻译或者深度学习的项目。理解开源项目的任务，编译通过该项目发布的示范程序，得到与项目示范程序一致的结果。然后再深入理解开源项目示范程序的算法。自己编程实现一下这个示范程序的算法。再按照项目提供的标准测试集测试自己实现的程序。如果输出的结果与项目中出现的结果不一致，就要仔细查验自己的程序，反复修改，直到结果与示范程序基本一致。如果还是不行，就大胆给项目的作者写信请教。在此基础上，再看看自己能否进一步完善算法或者实现，取得比示范程序更好的结果。\n项目不要太大，以小型的算法模块为佳，这样便于独立实现。像文本领域的文本分类、分词等项目就是比较合适的项目。 运行程序得到项目所声称的结果。然后看懂程序，这期间一般需要阅读程序实现所参考的文献。最后，自己尝试独立实现该算法，得到与示例程序相同的结果。再进一步的，可以调试参数，了解各参数对效果的影响，看是否能得到性能更好的参数组合。\n这一阶段主要是学习快速上手一个项目，从而对自然语言处理的项目有比较感性的认识——大体了解自然语言处理算法的原理、实现流程等。\n当我们对自然语言处理项目有了一定的认识之后，接下来就要深入进去。任何自然语言处理应用都包含算法和所要解决的问题两方面，要想深入进去就需要从这两方面进行着手。\n建议2：如何选择第一个好题目？\n工程型研究生，选题很多都是老师给定的。需要采取比较实用的方法，扎扎实实地动手实现。可能不需要多少理论创新，但是需要较强的实现能力和综合创新能力。而学术型研究生需要取得一流的研究成果，因此选题需要有一定的创新。我这里给出如下的几点建议。\n先找到自己喜欢的研究领域。你找到一本最近的ACL会议论文集, 从中找到一个你比较喜欢的领域。在选题的时候，多注意选择蓝海的领域。这是因为蓝海的领域，相对比较新，容易出成果。\n充分调研这个领域目前的发展状况。包括如下几个方面的调研：方法方面，是否有一套比较清晰的数学体系和机器学习体系；数据方面，有没有一个大家公认的标准训练集和测试集；研究团队，是否有著名团队和人士参加。如果以上几个方面的调研结论不是太清晰，作为初学者可能不要轻易进入。\n在确认进入一个领域之后，按照建议一所述，需要找到本领域的开源项目或者工具，仔细研究一遍现有的主要流派和方法，先入门。\n反复阅读本领域最新发表的文章，多阅读本领域牛人发表的文章。在深入了解已有工作的基础上，探讨还有没有一些地方可以推翻、改进、综合、迁移。注意做实验的时候，不要贪多，每次实验只需要验证一个想法。每次实验之后，必须要进行分析存在的错误，找出原因。\n对成功的实验，进一步探讨如何改进算法。注意实验数据必须是业界公认的数据。\n与已有的算法进行比较，体会能够得出比较一般性的结论。如果有，则去写一篇文章，否则，应该换一个新的选题。\n建议3：如何写出第一篇论文？\n接上一个问题，如果想法不错，且被实验所证明，就可开始写第一篇论文了。\n确定论文的题目。在定题目的时候，一般不要“…系统”、“…研究与实践”，要避免太长的题目，因为不好体现要点。题目要具体，有深度，突出算法。\n写论文摘要。要突出本文针对什么重要问题，提出了什么方法，跟已有工作相比，具有什么优势。实验结果表明，达到了什么水准，解决了什么问题。\n写引言。首先讲出本项工作的背景，这个问题的定义，它具有什么重要性。然后介绍对这个问题，现有的方法是什么，有什么优点。但是（注意但是）现有的方法仍然有很多缺陷或者挑战。比如（注意比如），有什么问题。本文针对这个问题，受什么方法（谁的工作）之启发，提出了什么新的方法并做了如下几个方面的研究。然后对每个方面分门别类加以叙述，最后说明实验的结论。再说本文有几条贡献，一般写三条足矣。然后说说文章的章节组织，以及本文的重点。有的时候东西太多，篇幅有限，只能介绍最重要的部分，不需要面面俱到。\n相关工作。对相关工作做一个梳理，按照流派划分，对主要的最多三个流派做一个简单介绍。介绍其原理，然后说明其局限性。\n然后可设立两个章节介绍自己的工作。第一个章节是算法描述。包括问题定义，数学符号，算法描述。文章的主要公式基本都在这里。有时候要给出简明的推导过程。如果借鉴了别人的理论和算法，要给出清晰的引文信息。在此基础上，由于一般是基于机器学习或者深度学习的方法，要介绍你的模型训练方法和解码方法。第二章就是实验环节。一般要给出实验的目的，要检验什么，实验的方法，数据从哪里来，多大规模。最好数据是用公开评测数据，便于别人重复你的工作。然后对每个实验给出所需的技术参数，并报告实验结果。同时为了与已有工作比较，需要引用已有工作的结果，必要的时候需要重现重要的工作并报告结果。用实验数据说话，说明你比人家的方法要好。要对实验结果好好分析你的工作与别人的工作的不同及各自利弊，并说明其原因。对于目前尚不太好的地方，要分析问题之所在，并将其列为未来的工作。\n结论。对本文的贡献再一次总结。既要从理论、方法上加以总结和提炼，也要说明在实验上的贡献和结论。所做的结论，要让读者感到信服，同时指出未来的研究方向。\n参考文献。给出所有重要相关工作的论文。记住，漏掉了一篇重要的参考文献（或者牛人的工作），基本上就没有被录取的希望了。\n写完第一稿，然后就是再改三遍。\n把文章交给同一个项目组的人士，请他们从算法新颖度、创新性和实验规模和结论方面，以挑剔的眼光，审核你的文章。自己针对薄弱环节，进一步改进，重点加强算法深度和工作创新性。\n然后请不同项目组的人士审阅。如果他们看不明白，说明文章的可读性不够。你需要修改篇章结构、进行文字润色，增加文章可读性。\n如投ACL等国际会议，最好再请英文专业或者母语人士提炼文字。\n建议4：对问题进行深入认识\n对问题的深入认识通常来源于两个方面，一是阅读当前领域的文献，尤其是综述性的文献，理解当前领域所面临的主要问题、已有的解决方案有哪些、有待解决的问题有哪些。这里值得一提的是，博士生论文的相关文献介绍部分通常会对本问题做比较详细的介绍，也是比较好的综述类材料。\n除了从文献中获取对问题的认识外，另一种对问题进行深入认识的直观方法就是对算法得出的结果进行bad case分析，总结提炼出一些共性的问题。对bad case进行分析还有一个好处，可以帮助我们了解哪些问题是主要问题，哪些问题是次要问题，从而可以帮助我们建立问题优先级。如果有具体任务的真实数据，一定要在真实数据上进行测试。这是因为，即使是相同的算法，在不同的数据集上，所得到的结果也可能相差很大。\n建议5：对算法进行深入理解\n除了具体的问题分析，对算法的理解是学习人工智能必须要过的关。经过这么多年的发展，机器学习、模式识别的算法已经多如牛毛。幸运的是，这方面已经有不少好的书籍可供参考。这里推荐华为李航的蓝宝书《统计学习方法》和周志华的西瓜书《机器学习》，这两本都是国内顶级的机器学习专家撰写的书籍，思路清晰，行文流畅，样例丰富。\n如果觉得教科书稍感乏味，那我推荐吴军的《数学之美》，这是一本入门级的科普读物，作者以生动有趣的方式，深入浅出的讲解了很多人工智能领域的算法，相信你一定会有兴趣。\n国外的书籍《Pattern Recognition and Machine Learning》主要从概率的角度解释机器学习的各种算法，也是不可多得的入门教材。如果要了解最新的深度学习的相关算法，可以阅读被誉为深度学习三架马车之一Bengio所著的《Deep Learning》。 在学习教材时，对于应用工程师来说，重要的是理解算法的原理，从而掌握什么数据情况下适合什么样的数据，以及参数的意义是什么。\n建议6：深入到领域前沿\n自然语言处理领域一直处在快速的发展变化当中，不管是综述类文章还是书籍，都不能反映当前领域的最新进展。如果要进一步的了解领域前沿，那就需要关注国际顶级会议上的最新论文了。下面是各个领域的一些顶级会议。这里值得一提的是，和其他人工智能领域类似，自然语言处理领域最主要的学术交流方式就会议论文，这和其他领域比如数学、化学、物理等传统领域都不太一样，这些领域通常都以期刊论文作为最主要的交流方式。 但是期刊论文审稿周期太长，好的期刊，通常都要两三年的时间才能发表，这完全满足不了日新月异的人工智能领域的发展需求，因此，大家都会倾向于在审稿周期更短的会议上尽快发表自己的论文。\n这里列举了国际和国内文本领域的一些会议，以及官网，大家可以自行查看。\n国际上的文本领域会议：\nACL：http://acl2017.org/ 加拿大温哥华 7.30-8.4\nEMNLP：http://emnlp2017.net/ 丹麦哥本哈根 9.7-9.11\nCOLING：没找到2017年的\n国内会议：\nCCKS http://www.ccks2017.com/index.php/att/ 成都 8月26-8月29\nSMP http://www.cips-smp.org/smp2017/ 北京 9.14-9.17\nCCL http://www.cips-cl.org:8080/CCL2017/home.html 南京 10.13-10.15\nNLPCC http://tcci.ccf.org.cn/conference/2017/ 大连 11.8-11.12\nNCMMSC http://www.ncmmsc2017.org/index.html 连云港 11.11 － 11.13\n像paperweekly，机器学习研究会，深度学习大讲堂等微信公众号，也经常会探讨一些自然语言处理的最新论文，是不错的中文资料。\n建议7：当然，工欲善其事，必先利其器。我们要做好自然语言处理的项目，还需要熟练掌握至少一门工具。当前，深度学习相关的工具已经比较多了，比如：tensorflow、mxnet、caffe、theano、cntk等。这里向大家推荐tensorflow，自从google推出之后，tensorflow几乎成为最流行的深度学习工具。究其原因，除了google的大力宣传之外，tensorflow秉承了google开源项目的一贯风格，社区力量比较活跃，目前github上有相当多数量的以tensorflow为工具的项目，这对于开发者来说是相当大的资源。\n以上就是对于没有自然语言处理项目经验的人来说，如何学习自然语言处理的一些经验，希望对大家能有所帮助。\n其中文献部分：\n1. 国际学术组织、学术会议与学术论文\n自然语言处理（natural language processing，NLP）在很大程度上与计算语言学（computational linguistics，CL）重合。与其他计算机学科类似，NLP/CL有一个属于自己的最权威的国际专业学会，叫做The Association for Computational Linguistics（ACL，URL：ACL Home Page），这个协会主办了NLP/CL领域最权威的国际会议，即ACL年会，ACL学会还会在北美和欧洲召开分年会，分别称为NAACL和EACL。除此之外，ACL学会下设多个特殊兴趣小组（special interest groups，SIGs），聚集了NLP/CL不同子领域的学者，性质类似一个大学校园的兴趣社团。其中比较有名的诸如SIGDAT（Linguistic data and corpus-based approaches to NLP）、SIGNLL（Natural Language Learning）等。这些SIGs也会召开一些国际学术会议，其中比较有名的就是SIGDAT组织的EMNLP（Conference on Empirical Methods on Natural Language Processing）和SIGNLL组织的CoNLL（Conference on Natural Language Learning）。此外还有一个International Committee on Computational Linguistics的老牌NLP/CL学术组织，它每两年组织一个称为International Conference on Computational Linguistics (COLING)的国际会议，也是NLP/CL的重要学术会议。NLP/CL的主要学术论文就分布在这些会议上。\n作为NLP/CL领域的学者最大的幸福在于，ACL学会网站建立了称作ACL Anthology的页面（URL：ACL Anthology），支持该领域绝大部分国际学术会议论文的免费下载，甚至包含了其他组织主办的学术会议，例如COLING、IJCNLP等，并支持基于Google的全文检索功能，可谓一站在手，NLP论文我有。由于这个论文集合非常庞大，并且可以开放获取，很多学者也基于它开展研究，提供了更丰富的检索支持，具体入口可以参考ACL Anthology页面上方搜索框右侧的不同检索按钮。\n与大部分计算机学科类似，由于技术发展迅速，NLP/CL领域更重视发表学术会议论文，原因是发表周期短，并可以通过会议进行交流。当然NLP/CL也有自己的旗舰学术期刊，发表过很多经典学术论文，那就是Computational Linguistics（URL：MIT Press Journals）。该期刊每期只有几篇文章，平均质量高于会议论文，时间允许的话值得及时追踪。此外，ACL学会为了提高学术影响力，也刚刚创办了Transactions of ACL（TACL，URL：Transactions of the Association for Computational Linguistics (ISSN: 2307-387X)），值得关注。值得一提的是这两份期刊也都是开放获取的。此外也有一些与NLP/CL有关的期刊，如ACM Transactions on Speech and Language Processing，ACM Transactions on Asian Language Information Processing，Journal of Quantitative Linguistics等等。\n根据Google Scholar Metrics 2013年对NLP/CL学术期刊和会议的评价，ACL、EMNLP、NAACL、COLING、LREC、Computational Linguistics位于前5位，基本反映了本领域学者的关注程度。\nNLP/CL作为交叉学科，其相关领域也值得关注。主要包括以下几个方面：（1）信息检索和数据挖掘领域。相关学术会议主要由美国计算机学会（ACM）主办，包括SIGIR、WWW、WSDM等；（2）人工智能领域。相关学术会议主要包括AAAI和IJCAI等，相关学术期刊主要包括Artificial Intelligence和Journal of AI Research；（3）机器学习领域，相关学术会议主要包括ICML，NIPS，AISTATS，UAI等，相关学术期刊主要包括Journal of Machine Learning Research（JMLR）和Machine Learning（ML）等。例如最近兴起的knowledge graph研究论文，就有相当一部分发表在人工智能和信息检索领域的会议和期刊上。实际上国内计算机学会（CCF）制定了“中国计算机学会推荐国际学术会议和期刊目录”（CCF推荐排名），通过这个列表，可以迅速了解每个领域的主要期刊与学术会议。\n最后，值得一提的是，美国Hal Daumé III维护了一个natural language processing的博客（natural language processing blog），经常评论最新学术动态，值得关注。我经常看他关于ACL、NAACL等学术会议的参会感想和对论文的点评，很有启发。另外，ACL学会维护了一个Wiki页面（ACL Wiki），包含了大量NLP/CL的相关信息，如著名研究机构、历届会议录用率，等等，都是居家必备之良品，值得深挖。\n2. 国内学术组织、学术会议与学术论文\n与国际上相似，国内也有一个与NLP/CL相关的学会，叫做中国中文信息学会（URL：中国中文信息学会）。通过学会的理事名单（中国中文信息学会）基本可以了解国内从事NLP/CL的主要单位和学者。学会每年组织很多学术会议，例如全国计算语言学学术会议（CCL）、全国青年计算语言学研讨会（YCCL）、全国信息检索学术会议（CCIR）、全国机器翻译研讨会（CWMT），等等，是国内NLP/CL学者进行学术交流的重要平台。尤其值得一提的是，全国青年计算语言学研讨会是专门面向国内NLP/CL研究生的学术会议，从组织到审稿都由该领域研究生担任，非常有特色，也是NLP/CL同学们学术交流、快速成长的好去处。值得一提的是，2010年在北京召开的COLING以及2015年即将在北京召开的ACL，学会都是主要承办者，这也一定程度上反映了学会在国内NLP/CL领域的重要地位。此外，计算机学会中文信息技术专委会组织的自然语言处理与中文计算会议（NLP\u0026CC）也是最近崛起的重要学术会议。中文信息学会主编了一份历史悠久的《中文信息学报》，是国内该领域的重要学术期刊，发表过很多篇重量级论文。此外，国内著名的《计算机学报》、《软件学报》等期刊上也经常有NLP/CL论文发表，值得关注。\n过去几年，在水木社区BBS上开设的AI、NLP版面曾经是国内NLP/CL领域在线交流讨论的重要平台。这几年随着社会媒体的发展，越来越多学者转战新浪微博，有浓厚的交流氛围。如何找到这些学者呢，一个简单的方法就是在新浪微博搜索的“找人”功能中检索“自然语言处理”、 “计算语言学”、“信息检索”、“机器学习”等字样，马上就能跟过去只在论文中看到名字的老师同学们近距离交流了。还有一种办法，清华大学梁斌开发的“微博寻人”系统（清华大学信息检索组）可以检索每个领域的有影响力人士，因此也可以用来寻找NLP/CL领域的重要学者。值得一提的是，很多在国外任教的老师和求学的同学也活跃在新浪微博上，例如王威廉（Sina Visitor System）、李沐（Sina Visitor System）等，经常爆料业内新闻，值得关注。还有，国内NLP/CL的著名博客是52nlp（我爱自然语言处理），影响力比较大。总之，学术研究既需要苦练内功，也需要与人交流。所谓言者无意、听者有心，也许其他人的一句话就能点醒你苦思良久的问题。无疑，博客微博等提供了很好的交流平台，当然也注意不要沉迷哦。\n3. 如何快速了解某个领域研究进展\n最后简单说一下快速了解某领域研究进展的经验。你会发现，搜索引擎是查阅文献的重要工具，尤其是谷歌提供的Google Scholar，由于其庞大的索引量，将是我们披荆斩棘的利器。\n当需要了解某个领域，如果能找到一篇该领域的最新研究综述，就省劲多了。最方便的方法还是在Google Scholar中搜索“领域名称 + survey / review / tutorial / 综述”来查找。也有一些出版社专门出版各领域的综述文章，例如NOW Publisher出版的Foundations and Trends系列，Morgan \u0026 Claypool Publisher出版的Synthesis Lectures on Human Language Technologies系列等。它们发表了很多热门方向的综述，如文档摘要、情感分析和意见挖掘、学习排序、语言模型等。\n如果方向太新还没有相关综述，一般还可以查找该方向发表的最新论文，阅读它们的“相关工作”章节，顺着列出的参考文献，就基本能够了解相关研究脉络了。当然，还有很多其他办法，例如去http://videolectures.net上看著名学者在各大学术会议或暑期学校上做的tutorial报告，去直接咨询这个领域的研究者，等等。","data":"2019年03月04日 15:01:01"}
{"_id":{"$oid":"5d36c55c6734bd8e681d65f1"},"title":"自然语言处理（NLP）-统计句法分析（CKY算法用于PCFG下的句法分析）","author":"bensonrachel","content":"1.先解释何为CFG及PCFG：\n一个栗子：\n2.CKY算法（或称CYK算法）\n“在计算机科学领域，CYK算法（也称为Cocke–Younger–Kasami算法）是一种用来对 上下文无关文法（CFG，Context Free Grammar）进行语法分析（parsing）的算法。该算法最早由John Cocke, Daniel Younger and Tadao Kasami分别独立提出，其中John Cocke还是1987年度的图灵奖得主。CYK算法是基于动态规划思想设计的一种自底向上语法分析算法。”\nCYK算法可以在O(n3)的时间内得出结果。\nCKY算法:\nCYK处理的CFG必须是CNF形式的。所以算法首先要把非CNF形式的CFG转化到（弱等价）CNF形式。CYK是一种自底向上的算法。\n乔姆斯基范式：\n乔姆斯基范式：CNF\n或者，ABC都为非终结符，为终结符。\n那个这个CFG就是采用CNF形式的，可见CNF语法都是二分叉的。任何语法都可以转化成一个弱等价的CNF形式，具体方法如下：（之后会有拓展版的，不只二元了，还有空的与一元的。）\n方法：\nCKY算法用于PCFG下的句法分析\n实现句子fish people fish tanks的句法树分析，实现最可能的统计句法树。\n基于概率的上下文无关语法（PCFG） 是一个五元组, 其定义为(T,  N，S，R，P). 可以看到, 这基本上与 CFG 类似, 只是多出来一个元素 p, 表示在语料中规则出现的概率. 使用p 可以定义一棵语法树出现的概率为树中所有规则出现概率之积. 这样, 当一个句子在可选的范围内有多棵可能的语法树时, 我们选择先验概率大的那棵树, 这样能最大程度避免解析错误。其中，\nN代表非终结符集合\nT代表终结符集合\nS代表初始非终结符\nR代表产生规则集\nP 代表每个产生规则的统计概率\n栗子：\n拓展版：加入了一元。\nCKY：\n动态规划：\n具体算法（类似填表的方法）：\n贴一个：\n维基百科的CYK算法用于CFG。\nhttps://en.wikipedia.org/wiki/CYK_algorithm#/media/File:CYK_algorithm_animation_showing_every_step_of_a_sentence_parsing.gif\n第一部分：\n下载stanford-parser-full-2018-10-17.zip\n解压：打开eclipse创建一个项目,导入在build path中引入stanford-parser-3.9.2-models.jar，stanford-parser.jar， slf4j-api.jar等相关库.\n调参：\n主要代码：\n结果：\n句法树：\nGUI界面：\n相关教程连接：\nhttp://www.cnblogs.com/Denise-hzf/p/6612574.html\n第二部分：\nPython3.5，pycharm.\n动态规划PCFG+CKY程序:\n链接：\nhttp://f.dataguru.cn/thread-693052-1-1.html\nPCFG 的训练\n对于 PCFG 中的 CFG 部分, 一般是由领域相关的专家给出的, 例如英语专家规定英语的 CFG. 而PCFG 中的 p 是从语料中统计而来. 运用最大似然估计, 可以有: P(X -\u003e Y) = count(X-\u003eY)/count(X)\n注意到, 规则中包括终端词与非终端词两种元素. 在一个适当规模的语料中, 我们可以认为所有的非终端词都会出现, 但是认为所有的终端词都会出现却是不现实的(想一下我们常听到的那个美国农民日常使用的英语单词只有数千个, 而所有的英语单词却有数万个的情况). 当语料中没有出现, 而在我们的测试样本中却出现了少见的单词时, PCFG 会对所有的语法树都给出概率为0的估计, 这对 PCFG 的模型是一个致命的问题.通常的补救措施是, 对语料中所有单词出现次数进行统计, 然后将出现频率少于 t 的所有单词都换成同一个 symbol. 在进行测试时, 先查找测试句子中的所有单词是否在句子中出现, 若没有出现, 则使用 symbol 代替. 通过这种方法, 可以避免 PCFG 模型给出概率为0 的估计, 同时也不会损失太多的信息.","data":"2018年12月02日 19:59:13"}
{"_id":{"$oid":"5d36c56b6734bd8e681d65f7"},"title":"自然语言处理（NLP）-1 从爬虫开始","author":"weixin_34055787","content":"1.爬python官网，解析页面html信息，python3使用urllib库\nimport urllib.request\n1. request type\nreq = urllib.request.Request(\"http://python.org/\")\nresp1 = urllib.request.urlopen(req)\nprint(\"*********req1*************\")\nprint(resp1.read())\nprint(\"**********************\")\n2. request type\nresp2 = urllib.request.urlopen(\"http://python.org/\")\nprint(\"*********req2*************\")\nprint(resp2.read())\nprint(\"**********************\")\n#########################################\nurl path get response\n#########################################\ndef getPageHtml(url):\nresponse = urllib.request.urlopen(url)\nreturn response.read()\nsplit page html data\ndef splitPageHtml(resouceData):\ndataArr = [data for data in resouceData.split()]\nprint(\"dataArr len is :\" + len(dataArr))\nprint(\"dataArr is start:\")\nprint(dataArr[0:100])\nprint(\"dataArr is end:\")\nresource = getPageHtml(\"http://python.org\")\nprint(splitPageHtml(resource))","data":"2018年08月13日 00:56:00"}
{"_id":{"$oid":"5d36c5806734bd8e681d65fd"},"title":"自然语言处理--趋势篇","author":"OceanEyes.GZY","content":"转载；原文地址：https://gitbook.cn/gitchat/geekbook/5b988b4eca9910654c0823f5/topic/5b993d66ca9910654c084853\n第五章 趋势篇\n随着深度学习时代的来临，神经网络成为一种强大的机器学习工具，自然语言处理取得了许多突破性发展，情绪分析、自动问答、机器翻译等领域都飞速发展。\n下图分别是 AMiner 计算出的自然语言处理近期热点和全球热点。通过对1994-2017年间自然语言处理领域论文的挖掘，总结出二十多年来，自然语言处理的领域关键词主要集中在计算机语言、神经网络、情感分析、机器翻译、词义消歧、信息提取、知识库和文本分析等领域。旨在基于历史的科研成果数据的基础上，对自然语言处理热度甚至发展趋势进行研究。图中，每个彩色分支表示一个关键词领域，其宽度表示该关键词的研究热度，各关键词在每一年份（纵轴）的位置是按照这一时间点上所有关键词的热度高低进行排序。\n图 14 自然语言处理近期热点图\n图 15 自然语言处理全球热点图\n图14显示，情绪分析、词义消歧、知识库和计算机语言学将是最近的热点发展趋势，图15显示词义消歧、词义理解、计算机语言学、信息检索和信息提取将是自然语言处理全球热点。\n我们同时在微博@ArnetMiner 中发起了关于自然语言处理未来发展趋势的投票，得到了如下结果。\n文本理解与推理：浅层分析到深度理解 135（28.1%）\n对话机器人：实用化、场景化 83 （17.3%）\nNLP 行业： 与专业领域结合 74（15.4%）\n学习模式： 先验语言知识与深度学习结合 45（9.4%）\n文本情感分析：事实性文本到情感性文本 43（9%）\n语言知识： 人工构建到自动构建 25（5.2%）\n信息检索： 跨语言、多媒体 23（4.8%）\n文本生成： 规范文本到自由文本 15（3.1%）\nNLP 平台化：封闭到开放 13（2.7%）\n对抗训练思想的应用 9（1.9%）\n共有465人次参与了投票，文本理解与推理由浅层分析到深度理解有135人次支持，占比28.1%，对话机器人实用化、场景化，NLP 行业与专业领域结合，学习模式由先验语言知识与深度学习结合以及文本情感分析由传统媒体到社交媒体依次排列，分别占比17.3%、15.4%、9.4%和9%。我们依据排列由高到低选取其中几项展开介绍。\n文本理解与推理：浅层分析向深度理解迈进\nGoogle 等公司已经推出了以阅读理解作为深入探索自然语言理解的平台。文本理解和推理是自然语言处理的重要部分，现在的机器软件已经可以根据文本的语境上下文分辨代词等指示词，这是文本理解与推理从浅层分析向深度理解迈进的重要一步。\n对话机器人：实用化、场景化\n从最初2012年到2014年的语音助手，到2014年起逐渐出现的聊天机器人微软小冰、百度小度，再到2016年哈工 SCIR-笨笨，对话机器人越来越智能。最初的语音助手可以听得到但是听不懂，之后的对话机器人可以听得懂但是实用性却不强，现在对话机器人更多的是和场景结合，即做特定场景时有用的人机对话。\nNLP+行业：与专业领域深度结合\n银行、电器、医药、司法、教育等领域对自然语言处理的需求都非常多。自然语言处理与各行各业的结合越来越紧密，专业化的服务趋势逐渐增强。刘挺教授预测，自然语言处理首先会在信息准备充分，并且服务方式本身就是知识和信息的领域产生突破，例如医疗、金融、教育和司法领域。\n学习模式：先验语言知识与深度学习结合\n自然语言处理中学习模式有一个较为明显的变化。在浅层到深层的学习模式中，浅层学习是分步骤的，深度学习的方法贯穿在浅层分析的每个步骤中，由各个步骤连接而成。而直接的深度学习则是直接的端到端，人为贡献的知识在深度学习中所占的比重大幅度减小。但如何将深度学习应用于自然语言处理需要进行更多的研究和探索，针对不同任务的不同字词表示，将先验知识和深度学习相结合是未来的一个发展趋势。\n文本情感分析：事实性文本到情感文本\n之前的研究主要是新闻领域的事实性文本，现在情感文本分析更受重视，并且在商业和政府舆情上可以得到很好地应用。如2017年新浪微舆情和哈工大推出“情绪地图”，网民可以登录新浪舆情官方网站查询任何关键词的“情绪地图”，这是语义情绪分析在舆情分析产业的首次正式应用。","data":"2019年02月27日 01:49:49"}
{"_id":{"$oid":"5d36c5976734bd8e681d6607"},"title":"自然语言处理的8大工具","author":"suxue_java","content":"小伙伴们注意了！\n小编在这里给大家送上关注福利：\n转发本文，关注+私信小编“资料”即可领取小编精心准备的资料一份！\n英语文本几乎无处不在。\n如果我们的系统能够理解并自动生成它，那将是最好的。\n然而，理解自然语言是一项复杂的任务。\n它是如此复杂，以至于许多研究人员花了一生的时间来做它。\n现在，已经发布了很多工具来完成自然语言处理工作。\n以下是我收集的8个工具。\n我还验证了它们都被某些应用程序至少使用一次，因此它们都是可运行的。\n有些来自工业公司，有些来自研究机构。\n它提供了解析、自动查找主题等功能。\nOpenNLP: a Java package to do text tokenization, part-of-speech tagging, chunking, etc. (tutorial)\nStanford Parser: a Java implementation of probabilistic natural language parsers, both highly optimized PCFG* and lexicalized dependency parsers, and a lexicalized PCFG parser\nScalaNLP: Natural Language Processing and machine learning.\nSnowball: a stemmer, support C and Java.\nMALLET: a Java-based package for statistical natural language processing, document classification, clustering, topic modeling, information extraction, and other machine learning applications to text.\nJGibbLDA: LDA in Java\nApache Lucene Core: a Java library for stop-words removal and stemming\nStanford Topic Modelling Toolbox: CVB0 algorithm, etc.\n最后，想学习Java的小伙伴们！\n转发！\n转发后关注+私信回复：“资料”就可以拿到一份我为大家准备的Java学习资料！\n小编V❤： suxueJava","data":"2019年02月27日 09:40:35"}
{"_id":{"$oid":"5d36c5a36734bd8e681d660d"},"title":"自然语言处理中的文本处理和特征工程","author":"梅逊雪","content":"机器之心报道\n文本处理\n现有数据中，文本是最非结构化的形式，里面有各种各样的噪声；如果没有预处理，文本数据都不能分析。清理和标准化文本的整个过程叫做文本预处理（textpreprocessing），其作用是使文本数据没有噪声并且可以分析。\n主要包括三个步骤：\n移除噪声词汇规范化对象标准化\n下图展示了文本预处理流程的结构。\n移除噪声\n任何与数据上下文和最终输出无关的文本都可被判作噪声。\n例如，语言停止词（stopword，语言中常用的词汇：系动词is，am，定冠词the，介词of，in）、URL或链接、社交媒体实体（提及、标签）、标点符号和特定行业词汇。这一步移除了文本中所有类型的噪声。\n移除噪声通用的做法是准备一个噪声实体的词典，在文本对象上逐个token（或逐词）迭代，消除在噪声词典中出现的标签。\n以下是实现这一步的Python代码：\n```\n#Samplecodetoremovenoisywordsfromatext\nnoise_list=[\"is\",\"a\",\"this\",\"...\"]\ndef_remove_noise(input_text):\nwords=input_text.split()\nnoise_free_words=[wordforwordinwordsifwordnotinnoise_list]\nnoise_free_text=\"\".join(noise_free_words)\nreturnnoise_free_text\n_remove_noise(\"thisisasampletext\")\n\u003e\u003e\u003e\"sampletext\"\n另外一种方法是使用正则表达式，尽管其只能解决特定模式的噪声。我们在之前的文章中详细介绍了正则表达式：https://www.analyticsvidhya.com/blog/2015/06/regular-expression-python/\n以下是从输入文本中移除正则表达式的Python代码：\n#Samplecodetoremovearegexpattern\nimportre\ndef_remove_regex(input_text,regex_pattern):\nurls=re.finditer(regex_pattern,input_text)\nforiinurls:\ninput_text=re.sub(i.group().strip(),'',input_text)\nreturninput_text\nregex_pattern=\"#[A-Za-z0-9\\w]*\"\n_remove_regex(\"removethis#hashtagfromanalyticsvidhya\",regex_pattern)\n\u003e\u003e\u003e\"removethisfromanalyticsvidhya\"\n词汇规范化\n另外一种文本形式的噪声是由一个词汇所产生的多种表示形式。\n例如，“play”，“player”,“played”，“plays”和“playing”，这些词汇都是由“play”变化而来的。虽然它们意义不一，但根据上下文都是相似的。词汇规范化这一步把一个词的不同展现形式转化为了他们规范化的形式（也叫做引理（lemma））。规范化是文本上的特征工程起中枢作用的一步，因为它把高维特征（N个不同的特征）转化为了对任何机器学习模型都很理想的低维空间（1个特征）。\n最常见的词汇规范化是：\n词干提取：词干提取是词汇后缀（“ing”，“ly”，“es”，“s”等）去除过程的一个基本规则。词形还原：词形还原与词干提取相反，是有组织地逐步获取词汇根形式的步骤，它使用到了词汇（词汇字典序）和形态分析（词的结构和语法关系）。\n下面是实现词形还原和词干提取的代码，使用了一个流行的Python库NLTK：\nfromnltk.stem.wordnetimportWordNetLemmatizer\nlem=WordNetLemmatizer()\nfromnltk.stem.porterimportPorterStemmer\nstem=PorterStemmer()\nword=\"multiplying\"\nlem.lemmatize(word,\"v\")\n\u003e\u003e\"multiply\"\nstem.stem(word)\n\u003e\u003e\"multipli\"\n对象标准化\n文本数据经常包含不在任何标准词典里出现的词汇或短语。搜索引擎和模型都识别不了这些。\n比如，首字母缩略词、词汇附加标签和通俗俚语。通过正则表达式和人工准备的数据词典，这种类型的噪声可以被修复。以下代码使用了词典查找方法来替代文本中的社交俚语。\nlookup_dict={'rt':'Retweet','dm':'directmessage',\"awsm\":\"awesome\",\"luv\":\"love\",\"...\"}\ndef_lookup_words(input_text):\nnew_words=[]\nforwordinwords:\nifword.lower()inlookup_dict:\nword=lookup_dict[word.lower()]\nnew_words.append(word)new_text=\"\".join(new_words)\nreturnnew_text\n_lookup_words(\"RTthisisaretweetedtweetbyShivamBansal\")\n\u003e\u003e\"RetweetthisisaretweetedtweetbyShivamBansal\"\n除了目前为止讨论过的三个步骤，其他类型的文本预处理有编码-解码噪声，语法检查器和拼写改正等。我之前的一篇文章给出了预处理及其方法的细节。\n文本到特征（文本数据上的特征工程）\n为了分析已经预处理过的数据，需要将数据转化成特征（feature）。取决于用途，文本特征可通过句法分析、实体/N元模型/基于词汇的特征、统计特征和词汇嵌入等方法来构建。下面来详细理解这些技巧。\n句法分析\n句法分析涉及到对句中词的语法分析和位置与词汇的关系的分析。依存语法（DependencyGrammar）和词性标注（PartofSpeechtags）是重要的文本句法属性。\n依赖树（DependencyTrees)——由一些词汇共同组成的句子。句中词与词之间的联系是由基本的依存语法决定的。从属关系语法是一类解决（已标签）两个词汇项（字词）间二元不对称关系的句法文本分析。每一种关系都可用三元组（关系、支配成分、从属成分）来表示。例如：考虑下面这个句子：“BillsonportsandimmigrationweresubmittedbySenatorBrownback,RepublicanofKansas.”词汇间的关系可由如下所示的树的形式观察得到\n观察树的形状可得：“submitted”是该句的根词（rootword），由两颗子树所连接（主语和宾语子树）。每一颗子树本身又是一颗依存关系树（dependencytree），其中的关系比如有-（“Bills”\u003c-\u003e“ports”“proposition”关系），(“ports”\u003c-\u003e“immigration”“conjugation”关系)\n这种类型的树，当从上至下迭代分析时可以得到语法关系三元组。对于很多自然语言处理问题，比如实体性情感分析，执行者（actor）与实体识别和文本分类等，语法关系三元组都可以用作特征。Pythonwrapper的StanfordCoreNLP（http://stanfordnlp.github.io/CoreNLP/来自斯坦福自然语言处理组，只允许商业许可证）和NTLK从属关系语法可以用来生成依赖树。\n词性标注（PoS/Partofspeechtagging）——除了语法关系外，句中每个词都与词性（名词、动词、形容词、副词等等）联系起来。词性标注标签决定了句中该词的用法和作用。这里有宾夕法尼亚大学定义的所有可能的词性标签表。以下代码使用了NTLK包对输入文本执行词性标签注释。（NTLK提供了不同的实现方式，默认是感知器标签）\nfromnltkimportword_tokenize,pos_tag\ntext=\"IamlearningNaturalLanguageProcessingonAnalyticsVidhya\"\ntokens=word_tokenize(text)\nprintpos_tag(tokens)\n\u003e\u003e\u003e[('I','PRP'),('am','VBP'),('learning','VBG'),('Natural','NNP'),('Language','NNP'),\n('Processing','NNP'),('on','IN'),('Analytics','NNP'),\n词性标注被用在许多重要的自然语言处理目的上：\nA.词义消歧：一些词汇根据用法有很多种意思。例如，下面的两个句子：\nI.“PleasebookmyflightforDelhi”II.“Iamgoingtoreadthisbookintheflight”\n“Book”在不同的上下文中出现，然而这两种情况的词性标签却不一样。在第一句中，“book”被用作动词，而在第二句中，它被用作名词。（Lesk算法也可被用于相同的目的）\nB.提高基于词汇的特征：当词汇作为特征时，一个学习模型可以学习到不同的词汇上下文，然而特征与词性连接起来，上下文就被保留了，因此得到了很强的特征。例如：\n句-“bookmyflight,Iwillreadthisbook”标签–(“book”,2),(“my”,1),(“flight”,1),(“I”,1),(“will”,1),(“read”,1),(“this”,1)带有POS的标签–(“book_VB”,1),(“my_PRP$”,1),(“flight_NN”,1),(“I_PRP”,1),(“will_MD”,1),(“read_VB”,1),(“this_DT”,1),(“book_NN”,1)\nC.规范化和词形归并（Lemmatizatio）：词性标签是将词转化为其基本形式（引理）的基础\nD.高效移除停止词：词性标签在移除停止词方面也非常有用。\n例如，有一些标签总是定义低频/较低重要性的词汇。\n例如：(IN–“within”,“upon”,“except”),(CD–“one”,”two”,“hundred”),(MD–“may”,“must”等)\n实体提取（实体作为特征）\n实体（entity）被定义为句中最重要的部分——名词短语、动词短语或两者都有。实体检测算法通常是由基于规则的解析、词典查询、词性标签和依存分析组合起来的模型。实体检测的适用性很广泛，在自动聊天机器人、内容分析器和消费者见解中都有应用。\n主题建模和命名实体识别是自然语言处理领域中两种关键的实体检测方法。\nA.命名实体识别（NER/NamedEntityRecognition）\n从文本中检测命名实体比如人名、位置、公司名称等的过程叫做命名实体识别（NER）。例如：\n句-SergeyBrin,themanagerofGoogleInc.iswalkinginthestreetsofNewYork.命名实体-(“人”:“SergeyBrin”),(“公司名”:“GoogleInc.”),(“位置”:“NewYork”)典型NER模型包含三个模块：\n名词短语识别：使用从属关系分析和词性分析将所有名词性短语从文本中提取出来。短语分类：将提取出的名词短语分类到各自的目录（位置，名称等）中。谷歌地图API提供了通往消除歧义位置的很好路径。然后，dbpedia，维基百科的开源数据库可以用来识别人名或公司名。除了这个，我们能通过结合不同来源的信息精确的查找表和词典。实体消歧：有些时候实体可能会误分类，因此在结果层上建一层交叉验证层非常有用。知识图谱就可以用来使用。目前流行的知识图谱有：谷歌知识图谱、IBMWatson和维基百科。\nB.主题建模\n主题建模是自动识别文本集中主题的过程，它以无监督的方式从语料库中的词汇里提取隐藏的模式。主题（topic）被定义为“文本集中共同出现术语的重复模式”。一个好的主题模型能对“健康”、“医生”、“病人”、“医院”建模为“健康保健”，“农场”、“作物”、“小麦”建模为“耕作”。\n隐含狄利克雷分布（LDA）是最流行的主题建模技术，以下是在Python环境下使用LDA技术实现主题建模的代码。若想查看更详细的细节，请参看：https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/\ndoc1=\"Sugarisbadtoconsume.Mysisterlikestohavesugar,butnotmyfather.\"\ndoc2=\"Myfatherspendsalotoftimedrivingmysisteraroundtodancepractice.\"\ndoc3=\"Doctorssuggestthatdrivingmaycauseincreasedstressandbloodpressure.\"\ndoc_complete=[doc1,doc2,doc3]\ndoc_clean=[doc.split()fordocindoc_complete]\nimportgensimfromgensim\nimportcorpora\n#Creatingthetermdictionaryofourcorpus,whereeveryuniquetermisassignedanindex.\ndictionary=corpora.Dictionary(doc_clean)\n#Convertinglistofdocuments(corpus)intoDocumentTermMatrixusingdictionarypreparedabove.\ndoc_term_matrix=[dictionary.doc2bow(doc)fordocindoc_clean]\n#CreatingtheobjectforLDAmodelusinggensimlibrary\nLda=gensim.models.ldamodel.LdaModel\n#RunningandTrainingLDAmodelonthedocumenttermmatrix\nldamodel=Lda(doc_term_matrix,num_topics=3,id2word=dictionary,passes=50)\n#Results\nprint(ldamodel.print_topics())\nC.N-Grams特征\nN-Grams是指N个词汇的结合体。N-Grams（N\u003e1）作为特征与词汇（Unigrams）作为特征相比，通常会更加富含信息。同时，bigrams（N=2）被认为是最重要的特征。以下代码生成了文本的bigrams。\ndefgenerate_ngrams(text,n):\nwords=text.split()\noutput=[]\nforiinrange(len(words)-n+1):\noutput.append(words[i:i+n])\nreturnoutput\n\u003e\u003e\u003egenerate_ngrams('thisisasampletext',2)\n#[['this','is'],['is','a'],['a','sample'],,['sample','text']]\n统计特征\n文本数据使用该节所讲的几种技术可直接量化成数字。\nA.术语频率-逆文献频率（TF–IDF）\nTF-IDF是经常被使用在信息检索问题上的权重模型。TF-IDF在不考虑文献中词的具体位置情况下，基于文献中出现的词汇将文本文献转化成向量模型。例如，假设有一个N个文本文献的数据集，在任何一个文献“D”中，TF和IDF会被定义为-术语频率（TF）-术语“t”的术语频率被定义为“t”在文献“D”中的数量。\n逆文献频率（IDF）-术语的逆文献频率被定义为文本集中可用文献的数量与包含术语“t”的文献的数量的比例的对数。\nTF-IDF公式给出了文本集中术语的相对重要性，以下为TF-IDF公式和使用Python的scikit学习包将文本转换为tf-idf向量。\nfromsklearn.feature_extraction.textimportTfidfVectorizer\nobj=TfidfVectorizer()\ncorpus=['Thisissampledocument.','anotherrandomdocument.','thirdsampledocumenttext']\nX=obj.fit_transform(corpus)\nprintX\n\u003e\u003e\u003e\n(0,1)0.345205016865\n(0,4)...0.444514311537\n(2,1)0.345205016865\n(2,4)0.444514311537\n模型创建了一个词典并给每一个词汇赋了一个索引。输出的每一行包含了一个元组（i,j）和在第i篇文献索引j处词汇的tf-idf值。\nB.数量/密度/可读性特征\n基于数量或密度的特征同样也能被用于模型和分析中。这些特征可能看起来比较繁琐但是对学习模型有非常大的影响。一些特征有：词数、句数、标点符号数和特定行业词汇的数量。其他类型的测量还包括可读性测量（比如音节数量、smogindex和易读性指数）。参考Textstat库创建这样的特征：https://github.com/shivam5992/textstat\n词嵌入（文本向量）\n词嵌入是将词表示为向量的方法，在尽量保存文本相似性的基础上将高维的词特征向量映射为低维特征向量。词嵌入广泛用于深度学习领域，例如卷积神经网络和循环神经网络。Word2Vec和GloVe是目前非常流行的两种做词嵌入的开源工具包，都是将文本转化为对应的向量。\nWord2Vec：https://code.google.com/archive/p/word2vec/GloVe：http://nlp.stanford.edu/projects/glove/\nWord2Vec是由预处理模块和两个浅层神经网络（CBOW/ContinuousBagofWords和Skip-gram）组成，这些模型广泛用于自然语言处理问题。Word2Vec首先从训练语料库中组织词汇，然后将词汇做词嵌入，得到对应的文本向量。下面的代码是利用gensim包实现词嵌入表示。\nfromgensim.modelsimportWord2Vec\nsentences=[['data','science'],['vidhya','science','data','analytics'],['machine','learning'],['deep','learning']]\n#trainthemodelonyourcorpus\nmodel=Word2Vec(sentences,min_count=1)\nprintmodel.similarity('data','science')\n\u003e\u003e\u003e0.11222489293\nprintmodel['learning']\n\u003e\u003e\u003earray([0.004593560.00303564-0.004676220.00209638,...])\n这些向量作为机器学习的特征向量，然后利用余弦相似性、单词聚类、文本分类等方法来衡量文本的相似性。","data":"2017年10月12日 10:51:09"}
{"_id":{"$oid":"5d36c5b86734bd8e681d6613"},"title":"NLP汉语自然语言处理原理与实践 1 中文语言的机器处理","author":"CopperDong","content":"安装NLTK\n1.3 整合中文分词模块\n按照使用的算法不同，下面介绍两大类中文分词模块\n基于条件随机场（CRF）的中文分词算法的开源系统\n基于张华平的NShort的中文分词算法的开源系统\n安装Ltp Python组件\nhttps://github.com/HIT-SCIR/ltp\n下载源代码：wget  https://github.com/HIT-SCIR/ltp/archive/v3.4.0.tar.gz\n\n下载语言模型：http://ospm9rsnd.bkt.clouddn.com/model/ltp_data_v3.4.0.zip   http://ospm9rsnd.bkt.clouddn.com/model/ltp_data_v3.3.0.zip\n源代码和语言模型包括：中文分词、词性标注、未登录词识别、依存句法、语义角色标注几个模块\n将项目与Python整合\npip install pyltp\n部署语言模型库：解压\n使用Ltp进行中文分词\n（1）\n# -*- coding: utf-8 -*- import sys import os from pyltp import Segmentor reload(sys) sys.setdefaultencoding('utf-8') model_path = \"ltp3.4/cws.model\" segmentor = Segmentor() segmentor.load(model_path) words = segmentor.segment(\"在包含问题的所有解的解空间树中，按照深度优先搜索的策略，从根节点出发深度探索解空间树。\") print \" | \".join(words)\n在 | 包含 | 问题 | 的 | 所有 | 解 | 的 | 解 | 空间 | 树 | 中 | ， | 按照 | 深度 | 优先 | 搜索 | 的 | 策略 | ， | 从 | 根节点 | 出发 | 深度 | 探索 | 解 | 空间 | 树 | 。\n（2）分词结果的后处理\n上述分词粒度过细，为了获得更精确的结果可以将错分的结果合并为专有名词。这就是分词结果的后处理过程，即一般外部用户词典的构成原理。\npostdict = {\"解 | 空间\":\"解空间\", \"深度 | 优先\":\"深度优先\"} seg_sent = \" | \".join(words) for key in postdict: seg_sent = seg_sent.replace(key, postdict[key]) print seg_sent\n在 | 包含 | 问题 | 的 | 所有 | 解 | 的 | 解空间 | 树 | 中 | ， | 按照 | 深度优先 | 搜索 | 的 | 策略 | ， | 从 | 根节点 | 出发 | 深度 | 探索 | 解空间 | 树 | 。\n（3）现在加入用户词典，词典中登录一些新词，如解空间\nuser_dict = \"ltp3.4/fulluserdict.txt\" #外部专有名词词典 segmentor1 = Segmentor() segmentor1.load_with_lexicon(model_path, user_dict) #加载专有名词词典 sent = \"在包含问题的所有解的解空间树中，按照深度优先搜索的策略，从根节点出发深度探索解空间树。\" words = segmentor.segment(sent) print \" | \".join(words)\n使用结巴分词模块\n张华平的NShort的中文分词算法是目前大规模中文分词的主流算法。在商用领域，大多数搜索引擎公司都使用该算法作为主要的分词算法。具有算法原理简单、容易理解、便于训练、大规模分词的效率高、模型支持增量扩展、模型占用资源低等优势。\n这里使用的结巴分词器是该算法的Python实现，结巴分词的算法核心就是Nshort中文分词算法。\nhttps://github.com/fxsjy/jieba，结巴分词模块可支持如下三种分词方式：\n精确模式，试图将句子最精确地切开，适合文本分析（类似Ltp的分词方式）\n全模式：把句子中所有可以成词的词语都扫描出来，速度非常块，但是不能解决歧义\n搜索引擎模式，在精确模式的基础上对长词再次切分，提高召回率，适合用于搜索引擎分词\n支持繁体分词\n支持基于概率的用户词典\n（1）安装\npip install jieba\n（2）使用结巴分词\n# -*- coding: utf-8 -*- import sys import os import jieba reload(sys) sys.setdefaultencoding('utf-8') sent = \"在包含问题的所有解的解空间树中，按照深度优先搜索的策略，从根节点出发深度探索解空间树。\" wordlist = jieba.cut(sent, cut_all=True) #全模式 print \" | \".join(wordlist) wordlist = jieba.cut(sent) #精确模式 print \" | \".join(wordlist) wordlist = jieba.cut_for_search(sent) #搜索引擎模式 print \" | \".join(wordlist)\n在 | 包含 | 问题 | 的 | 所有 | 解 | 的 | 解空 | 空间 | 树 | 中 |  |  | 按照 | 深度 | 优先 | 搜索 | 的 | 策略 |  |  | 从 | 根 | 节点 | 点出 | 出发 | 深度 | 探索 | 索解 | 解空 | 空间 | 树 |  |\n在 | 包含 | 问题 | 的 | 所有 | 解 | 的 | 解 | 空间 | 树中 | ， | 按照 | 深度 | 优先 | 搜索 | 的 | 策略 | ， | 从根 | 节点 | 出发 | 深度 | 探索 | 解 | 空间 | 树 | 。\n在 | 包含 | 问题 | 的 | 所有 | 解 | 的 | 解 | 空间 | 树中 | ， | 按照 | 深度 | 优先 | 搜索 | 的 | 策略 | ， | 从根 | 节点 | 出发 | 深度 | 探索 | 解 | 空间 | 树 | 。\n（3）使用用户词典\n○ → cat userdict.txt\n解空间 5 n\n解空间树 5 n\n根结点 5 n\n深度优先 5 n\n\njieba.load_userdict(\"userdict.txt\") wordlist = jieba.cut(sent, cut_all=True) #全模式 print \" | \".join(wordlist)\n在 | 包含 | 问题 | 的 | 所有 | 解 | 的 | 解空 | 解空间 | 解空间树 | 空间 | 树 | 中 |  |  | 按照 | 深度 | 深度优先 | 优先 | 搜索 | 的 | 策略 |  |  | 从 | 根 | 节点 | 点出 | 出发 | 深度 | 探索 | 索解 | 解空 | 解空间 | 解空间树 | 空间 | 树 |  |\n\n1.4 整合词性标注模块\n词性标注（Part of speech tagging 或者 POS Tagging），有称为词类标注，是指判断出在一个句子中每个词所扮演的语法角色。例如，表示人、事物、地点或抽象概念的名称就是名词；表示动作或状态变化的词为动词；用来描写或修饰名词性成分或表示概念的性质、状态、特征或属性的词称为形容词，等等。\n中文词性标注中影响词性标注精度的因素主要是要正确判断文本中那些常用词的词性。\n一般而言，中文的词性标注算法比较统一，大多数使用HMM或最大熵算法，如结巴的词性标注。为了获得更高的精度，也有使用CRF算法的，如Ltp中的词性标注。\n在一般的工程应用中，语料的中文分词和词性标注通常同时完成。\n目前流行的中文词性标签有两个类：北大词性标注集和宾州词性标注集，它们各有千秋\nLtp3.3 词性标注\n词性标注模块的文件名为pos.model\n# -*- coding: utf-8 -*- import sys import os from pyltp import * reload(sys) sys.setdefaultencoding('utf-8') #已分好词 sent = \"在 包含 问题 的 所有 解 的 解空间树 中 ， 按照 深度优先 搜索 的 策略 ， 从 根节点 出发 深度 探索 解空间树 。\" words = sent.split(\" \") postagger = Postagger() #实例化词性标注类 postagger.load('ltp3.4/pos.model') postags = postagger.postag(words) for word,postag in zip(words,postags): print word+\"/\"+postag,\n在/p 包含/v 问题/n 的/u 所有/b 解/v 的/u 解空间树/n 中/nd ，/wp 按照/p 深度优先/d 搜索/v 的/u 策略/n ，/wp 从/p 根节点/n 出发/v 深度/n 探索/v 解空间树/n 。/wp\n\n安装StanfordNLP并编写Python接口类\nhttps://stanfordnlp.github.io/CoreNLP/\nhttp://nlp.stanford.edu/software/stanford-corenlp-full-2017-06-09.zip只携带了英文的语言模型包，中文部分的语言模型需要单独下载，\nhttp://nlp.stanford.edu/software/stanford-chinese-corenlp-2017-06-09-models.jar\nmkdir stanford-corenlp  #解压到此目录\n其中stanford-corenlp.jar为主执行文件\n将stanford-chinese-corenlp-2017-06-09-models.jar中的中文模型全部解压到models目录中。其中pos-tagger目录下放置了词性标注的中文模型。\njar xvf ../../stanford-chinese-corenlp-2017-06-09-models.jar\n\nhttps://nlp.stanford.edu/software/tagger.shtml\n\nwget https://nlp.stanford.edu/software/stanford-postagger-full-2017-06-09.zip\n\n执行命令的参考脚本\n○ → cat stanford-postagger.sh\n\njava -mx300m -cp 'stanford-postagger.jar:' edu.stanford.nlp.tagger.maxent.MaxentTagger -model $1 -textFile $2\n\n执行./stanford-postagger.sh models/english-left3words-distsim.tagger sample-input.txt\n○ → cat ../postest.txt\n在 包含 问题 的 所有 解 的 解空间树 中 ， 按照 深度优先 搜索 的 策略 ， 从 根节点 出发 深度 探索 解空间树 。\n\n○ → ./stanford-postagger.sh models/chinese-distsim.tagger ../postest.txt\nLoading default properties from tagger models/chinese-distsim.tagger\nLoading POS tagger from models/chinese-distsim.tagger ... done [1.5 sec].\n在#P 包含#VV 问题#NN 的#DEC 所有#DT 解#VV 的#DEC 解空间树#NN 中#LC ，#PU 按照#P 深度优先#NN 搜索#NN 的#DEC 策略#NN ，#PU 从#P 根节点#NN 出发#VV 深度#JJ 探索#NN 解空间树#VV 。#PU\nTagged 23 words at 338.24 words per second.\n进入stanford-corenlp\n○ → java -mx5g -cp \"./*\" edu.stanford.nlp.tagger.maxent.MaxentTagger -model models/edu/stanford/nlp/models/pos-tagger/chinese-distsim/chinese-distsim.tagger -textFile ../postest.txt\n在#P 包含#VV 问题#NN 的#DEC 所有#DT 解#VV 的#DEC 解空间树#NN 中#LC ，#PU 按照#P 深度优先#NN 搜索#NN 的#DEC 策略#NN ，#PU 从#P 根节点#NN 出发#VV 深度#JJ 探索#NN 解空间树#VV 。#PU\n（1）新建stanford.py\n# -*- coding: utf-8 -*- import sys import os reload(sys) sys.setdefaultencoding('utf-8') # CoreNLP 3.6 jar包和中文模型包 # ejml-0.23.jar javax.json.jar jollyday.jar joda-time.jar jollyday.jar protobuf.jar slf4j.api.jar # slf4j-simple.jar stanford-corenlp-3.6.0.jar xom.jar class StanfordCoreNLP(): #所有StanfordNLP的父类 def __init__(self, jarpath): self.root = jarpath self.tempsrcpath = \"tempsrc\" #输入临时文件路径 self.jarlist = [\"ejml-0.23.jar\", \"javax.json.jar\", \"jollyday.jar\", \"joda-time.jar\", \"protobuf.jar\", \"slf4j-api.jar\", \"slf4j-simple.jar\", \"stanford-corenlp-3.8.0.jar\", \"xom.jar\"] self.jarpath = \"\" self.buildjars() def buildjars(self): #根据root路径构建所有的jar包路径 #self.jarpath += self.root + \"/*\" for jar in self.jarlist: self.jarpath += self.root + jar + \":\" def savefile(self,path,sent): #创建临时文件存储路径 fp = open(path, \"wb\") fp.write(sent) fp.close() def delfile(self, path): os.remove(path) #词性标注子类 class StanfordPOSTagger(StanfordCoreNLP): def __init__(self, jarpath, modelpath): StanfordCoreNLP.__init__(self, jarpath) self.modelpath = modelpath self.classfier = \"edu.stanford.nlp.tagger.maxent.MaxentTagger\" self.delimiter = \"/\" self.__buildcmd() def __buildcmd(self): self.cmdline = 'java -mx1g -cp \"'+self.jarpath+'\" ' + self.classfier+' -model \"'+self.modelpath+'\" -tagSeparator ' + self.delimiter print self.cmdline def tag(self, sent): self.savefile(self.tempsrcpath, sent) tagtxt = os.popen(self.cmdline+\" -textFile \"+self.tempsrcpath, 'r').read() self.delfile(self.tempsrcpath) return tagtxt def tagfile(self,inputpath, outpath): os.system(self.cmdline+' -textFile '+inputpath+' \u003e '+outpath)\nStanfordPostTagger.py\n# -*- coding: utf-8 -*- import sys import os reload(sys) sys.setdefaultencoding('utf-8') from stanford import StanfordPOSTagger root = \"stanford-corenlp/\" modelpath = root+\"models/edu/stanford/nlp/models/pos-tagger/chinese-distsim/chinese-distsim.tagger\" st = StanfordPOSTagger(root, modelpath) seg_sent = \"在 包含 问题 的 所有 解 的 解空间树 中 ， 按照 深度优先 搜索 的 策略 ， 从 根节点 出发 深度 探索 解空间树 。\" taglist = st.tag(seg_sent) print taglist\n在/P 包含/VV 问题/NN 的/DEC 所有/DT 解/VV 的/DEC 解空间树/NN 中/LC ，/PU 按照/P 深度优先/NN 搜索/NN 的/DEC 策略/NN ，/PU 从/P 根节点/NN 出发/VV 深度/JJ 探索/NN 解空间树/VV 。/PU\n\n1.5 整合命名实体识别模块\n本书将命名实体识别划分在语义范畴的原因是，命名实体识别不仅需要标注词的语法信息（名词），更重要的是要指示词的语义信息（人名还是组织机构名等）。这里所需要识别的命名实体一般不是指已知名词（词典中的登录词），而是指新词（或称未登录词）。\n更具体的命名实体识别任务还要识别出文本中三大类（实体类、时间类和数字类）、七小类（人名、机构名、地名、时间、日期、货币和百分比）命名实体。\nLtp命名实体识别\n命名实体识别模块的文件名为ner.model\n# -*- coding: utf-8 -*- import sys import os from pyltp import * reload(sys) sys.setdefaultencoding('utf-8') sent = \"欧洲 东部 的 罗马尼亚 ， 首都 是 布加勒斯特 ， 也 是 一 座 世界性 的 城市 。\" words = sent.split(\" \") postagger = Postagger() postagger.load(\"ltp3.4/pos.model\") # 导入词性标注模块 postags = postagger.postag(words) recognizer = NamedEntityRecognizer() recognizer.load(\"ltp3.4/ner.model\") # 导入命名实体识别模块 netags = recognizer.recognize(words, postags) for word,postag,netag in zip(words,postags,netags): print word+\"/\"+postag+\"/\"+netag,\n欧洲/ns/S-Ns 东部/nd/O 的/u/O 罗马尼亚/ns/S-Ns ，/wp/O 首都/n/O 是/v/O 布加勒斯特/ns/S-Ns ，/wp/O 也/d/O 是/v/O 一/m/O 座/q/O 世界性/n/O 的/u/O 城市/n/O 。/wp/O\n第一段是词”欧洲“，第二段是词性”ns“，第三段”S-Ns“就是识别的专名，”O“表示非专名，”S-Ns“表示地名。\nStanford命名实体识别\n如果仅用NER，可从http://nlp.stanford.edu/software/CRF-NER.shtml下载\n#命名实体类 class StanfordNERTagger(StanfordCoreNLP): def __init__(self,modelpath,jarpath): StanfordCoreNLP.__init__(self,jarpath) self.modelpath = modelpath self.classfier = \"edu.stanford.nlp.ie.crf.CRFClassifier\" self.__buildcmd() def __buildcmd(self): self.cmdline = 'java -mx1g -cp \"'+self.jarpath+'\" '+self.classfier+' -loadClassifier \"'+self.modelpath+'\"' print self.cmdline #标注句子 def tag(self, sent): self.savefile(self.tempsrcpath,sent) tagtxt = os.popen(self.cmdline+' -textFile '+self.tempsrcpath,'r').read() self.delfile(self.tempsrcpath) return tagtxt #标注文件 def tagfile(self,sent,outpath): self.savefile(self.tempsrcpath,sent) os.system(self.cmdline+' -textFile '+self.tempsrcpath+' \u003e '+outpath) self.delfile(self.tempsrcpath)\n# -*- coding: utf-8 -*- import sys import os from stanford import StanfordNERTagger reload(sys) sys.setdefaultencoding('utf-8') root = \"stanford-corenlp/\" modelpath = root+'models/edu/stanford/nlp/models/ner/chinese.misc.distsim.crf.ser.gz' st = StanfordNERTagger(modelpath,root) seg_sent = \"欧洲 东部 的 罗马尼亚 ， 首都 是 布加勒斯特 ， 也 是 一 座 世界性 的 城市 。\" taglist = st.tagfile(seg_sent, \"ner_test.txt\") print taglist\n欧洲/LOCATION 东部/O 的/O 罗马尼亚/GPE ，/O 首都/O 是/O 布加勒斯特/GPE ，/O 也/O 是/O 一/O 座/O 世界性/O 的/O 城市/O 。/O\n\n1.6 整合句法解析模块\n目前句法分析有两种不同的理论：一种是短语结构语法；另一种是依存语法。句法分析的开源系统也很多，但迄今为此，这些解析技术都还不够理想，仍旧很难找到高精度处理中文的句法解析系统。\n其中，比较突出的是Ltp中文句法分析系统，使用依存句法理论\n还有最著名的句法解析器是Stanford句法解析器。截至2015年，Stanford的句法树包含了如下三大主要解析器。\nPCFG概率解析器。是一个高度优化的词汇化PCFG依存解析器。该解析器使用A*算法，是一个随机上下无关文法解析器。除英语之外，该解析器还包含一个中文版本，使用滨州中文树库训练。解析器的输出格式包含依存关系输出和短语结构树输出。\nShift-Reduce解析器。为了提高PCFG概率解析器的性能，Stanford提供了一个基于移进-归约算法的高性能解析器。其性能远高于任何PCFG解析器，而且精度上比其他任何版本（包括RNN）的解析器都更准确。\n神经网络依存解析器。神经网络依存解析器是深度学习算法在句法解析中的一个重要应用。它通过中心词和修饰词之间的依存关系来构建出句子的句法树。有关此方面的研究是目前NLP的研究重点。\n\nLtp句法依存树\n句法解析模块的文件名为parser.model\n# -*- coding: utf-8 -*- import sys import os import nltk from nltk.tree import Tree #导入nltk tree结构 from nltk.grammar import DependencyGrammar #导入依存句法包 from nltk.parse import * from pyltp import * # 导入ltp应用包 import re reload(sys) sys.setdefaultencoding('utf-8') words = \"罗马尼亚 的 首都 是 布加勒斯特 。\".split(\" \") #例句 print words postagger = Postagger() #词性标注 postagger.load(\"ltp3.4/pos.model\") postags = postagger.postag(words) print len(postags) parser = Parser() #句法解析 parser.load(\"ltp3.4/parser.model\") arcs = parser.parse(words, postags) arclen = len(arcs) print arclen conll = \"\" for i in xrange(arclen): #构建Conll标准的数据结构 if arcs[i].head == 0: arcs[i].relation = \"ROOT\" conll += \"\\t\"+words[i]+\"(\"+postags[i]+\")\"+\"\\t\"+postags[i]+\"\\t\"+str(arcs[i].head)+\"\\t\"+arcs[i].relation+\"\\n\" print conll conlltree = DependencyGraph(conll) #转换为依存句法图 tree = conlltree.tree() # 构建树结构 tree.draw()\n罗马尼亚(ns) ns 3 ATT 的(u) u 1 RAD 首都(n) n 4 SBV 是(v) v 0 ROOT 布加勒斯特(ns) ns 4 VOB 。(wp) wp 4 WP\n依存关系\nStanford Parser类\n如果仅使用中文句法解析模块，可从http://nlp.stanford.edu/software/lex-parser.shtml下载\n\n#句法解析 class StanfordParser(StanfordCoreNLP): def __init__(self,modelpath,jarpath,opttype): StanfordCoreNLP.__init__(self,jarpath) self.modelpath = modelpath # 模型文件路径 self.classfier = \"edu.stanford.nlp.parser.lexparser.LexicalizedParser\" self.opttype = opttype self.__buildcmd() def __buildcmd(self): self.cmdline = 'java -mx500m -cp \"' + self.jarpath + '\" ' + self.classfier + ' -outputFormat \"' + self.opttype + '\" ' + self.modelpath + ' ' print self.cmdline #句法解析 def parse(self, sent): self.savefile(self.tempsrcpath, sent) tagtxt = os.popen(self.cmdline + self.tempsrcpath, \"r\").read() self.delfile(self.tempsrcpath) return tagtxt def tagfile(self, sent, outpath): self.savefile(self, tempsrcpath, sent) os.system(self.cmdline + self.tempsrcpath + ' \u003e ' + outpath) self.delfile(self.tempsrcpath)\nStanford短语结构树\n# -*- coding: utf-8 -*- import sys import os import nltk from nltk.tree import Tree #导入nltk tree结构 from stanford import * reload(sys) sys.setdefaultencoding('utf-8') # 配置环境变量 #os.environ['JAVA_HOME'] = root = \"stanford-corenlp/\" modelpath = root + \"models/edu/stanford/nlp/models/lexparser/chinesePCFG.ser.gz\" opttype = 'penn' #滨州树库格式 parser = StanfordParser(modelpath, root, opttype) result = parser.parse(\"罗马尼亚 的 首都 是 布加勒斯特 。\") print result tree = Tree.fromstring(result) tree.draw()\n(ROOT\n(IP\n(NP\n(DNP\n(NP (NR 罗马尼亚))\n(DEG 的))\n(NP (NN 首都)))\n(VP (VC 是)\n(NP (NR 布加勒斯特)))\n(PU 。)))\n\n\nstanford依存句法树\n# -*- coding: utf-8 -*- import sys import os import nltk from nltk.tree import Tree #导入nltk tree结构 from stanford import * reload(sys) sys.setdefaultencoding('utf-8') # 配置环境变量 #os.environ['JAVA_HOME'] = root = \"stanford-corenlp/\" modelpath = root + \"models/edu/stanford/nlp/models/lexparser/chinesePCFG.ser.gz\" opttype = 'typedDependencies' # parser = StanfordParser(modelpath, root, opttype) result = parser.parse(\"罗马尼亚 的 首都 是 布加勒斯特 。\") print result\nnmod:assmod(首都-3, 罗马尼亚-1)\ncase(罗马尼亚-1, 的-2)\nnsubj(布加勒斯特-5, 首都-3)\ncop(布加勒斯特-5, 是-4)\nroot(ROOT-0, 布加勒斯特-5)\npunct(布加勒斯特-5, 。-6)\n\n1.7 整合语义角色标注模块\n语义角色标注（SRL）来源于20世纪60年代美国语言学家菲尔墨提出的格语法理论。该理论是在句子语义理解上的一个重要突破。基于此理论，语义角色标注就发展起来了，并成为句子语义分析的一种重要方式。它采用”谓词-论元角色“的结构形式，标注句子成分相对于给定谓语动词的语义角色，每个语义角色被赋予一定的语义。\n美国宾州大学已经开发出一个具有使用价值的表示语义命题库，称为PropBank。\n语义角色标注系统已经处于NLP系统的末端，其精度和效率都受到前面几个模块的影响，所以，当前系统的精度都不高，在中文领域还没有投入商业应用的成功案例，本节介绍的是Ltp中文语义角色标注系统\n# -*- coding: utf-8 -*- import sys import os reload(sys) sys.setdefaultencoding('utf-8') from pyltp import * MODELDIR = \"ltp3.4/\" sentence = \"欧洲东部的罗马尼亚，首都是布加勒斯特，也是一座世界性的城市。\" segmentor = Segmentor() segmentor.load(os.path.join(MODELDIR, \"cws.model\")) words = segmentor.segment(sentence) wordlist = list(words) #从生成器变为列表元素 postagger = Postagger() postagger.load(os.path.join(MODELDIR, \"pos.model\")) postags = postagger.postag(words) parser = Parser() parser.load(os.path.join(MODELDIR, \"parser.model\")) arcs = parser.parse(words, postags) recognizer = NamedEntityRecognizer() recognizer.load(os.path.join(MODELDIR, \"ner.model\")) netags = recognizer.recognize(words, postags) #语义角色标注 labeller = SementicRoleLabeller() labeller.load(os.path.join(MODELDIR, \"srl/\")) roles = labeller.label(words, postags, netags, arcs) #输出标注结果 for role in roles: print 'rel:', wordlist[role.index] #谓词 for arg in role.arguments: if arg.range.start != arg.range.end: print arg.name, ' '.join(wordlist[arg.range.start:arg.range.end]) else: print arg.name,wordlist[arg.range.start]\nrel: 是\nA0 欧洲 东部 的 罗马尼亚\nA0 首都\nA1 布加勒斯特\nrel: 是\nADV 也\nA1 一 座 世界性 的\n\n\nrel标签表示谓词，A0指动作的实施，A1指动作的受事","data":"2017年12月12日 12:26:50"}
