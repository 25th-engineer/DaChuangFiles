大数据概述 : 1.数据量(Volume)
			 2.基于高度分析的新价值(Value)
			 3.多样性，复杂性(Variety)
			 4.速度(Velocity)
			 
reliable : 可靠 
scalable : 可扩展
distributed ：分布的

HDFS : Hadoop实现的一个分布式文件系统
	   源自于Google的GFS论文
	   
HDFS架构
	1 Master(NameNode/NN) 带 N个Slaves(DataNode/DN)
	HDFS/YARN/HBase
	一个文件会被拆分成多个Block
	例如:blocksize: 128M
		 130M ==> 2个Block ： 128M 和 2M

NN :
	1.负责客户端请求的响应、
	2.负责元数据(文件的名称，副本系数，Block存放的DN)的管理
	
DN ：
	1.存储用户的文件对应的数据块(Block)
	2.要定期向NN发送心跳信息，汇报本身及其所有的block信息，健康状况

repication factor:副本因子


资源调度框架YARN
	1.架构: 1RM(ResourceManager) + N NM(NodeManager)
	2.RM的职责 :一个集群active状态的RM只有一个，负责整个集群的资源管理和调度
		1) 处理客户端的请求(启动/杀死)
		2) 启动/监控ApplicationMaster(一个作业对应一个AM)
		3) 监控NM
		4) 系统的资源分配和调度
	3.NM的职责 :整个集群中有N个，负责单个节点的资源管理和使用以及task的运行情况
		1) 定期向RM汇报本节点的资源使用请求和各个Container的运行状态
		2) 接收并处理RM的Container启停的各种命令
		3) 单个节点的资源管理和任务管理
	4.ApplicationMaster : 每个应用/作业对应一个，负责应用程序的管理
		1) 数据切分
		2) 为应用程序向RM申请资源(container),并分配给内部任务
		3) 与NM通信以启停task，task是运行在container中的
		4) task的监控和容错
	5.Container : 对任务运行情况的描述 : cpu,memory,环境变量
	6.执行流程 :
		1) 用户向YARN提交作业
		2) RM为该作业分配第一个container(AM)
		3) RM会与对应的NM通信，要求NM在这个container上启动应用程序的AM
		4) AM首先向RM注册，然后AM将为各个任务申请资源，并监控运行情况
		5) AM采用轮训的方式通过RPC协议向RM申请和领取资源
		6) AM申请到资源以后，便和相应的NM通信，要求NM启动任务
		7) NM启动我们作业对应的task


大数据数据仓库Hive
	1.Hive是什么
		--由Facebook开源，最初用于解决海量结构化的日志数据统计问题
		--构建在Hadoop之上的数据仓库
		--Hive定义了一种类SQL查询语言 : HQL(类似SQL，但不完全相同)
		--通常用于进行离线数据处理
		--底层支持多种不同的执行引擎有:MapRedice,Tez,Spark
					Hive on MapRedice
					Hive on Tez
					Hive on Spark
		--支持多种不同的压缩格式，存储格式以及自定义函数
					压缩:GZIP,LZO,Snappy,BZIP2....
					存储:TextFile,SequenceFile,RCFile,ORC,Parquet
					UDF：自定义函数
	2.为什么要使用Hive
		--简单，容易上手(提供了类似于SQL的查询语言HQL)
		--为超大数据集设计的计算/存储扩展能力(MR计算，HDFS存储)
		--统一的元数据管理(可与Presto/Impala/SparkSQL等共享数据)
		
	3.基本使用:
		--创建表
			CREATE TABLE table_name
				[(col_name data_type [COMMENT col_comment])]
			create table hive_wordcount(context string)
		--使用Hive完成wordcount统计
			1)加载数据到hive表
				LOAD DATA LOCAL INPATH 'filepath' INTO TABLE tablename
			
				load data local inpath '/home/hadoop/data/hello.txt' into table into table hive_wordcount
			2)相关使用:
				select word,count(1) from hive_wordcount lateral view explode(split(context,'\t')) wc as word group by word
				lateral view explode():是把每行记录按照指定分隔符进行拆解
			3)hive ql提交执行以后会生成mr作业并在yarn上运行