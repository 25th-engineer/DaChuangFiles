## 20. 偏差和方差：两大误差来源

假设你的训练、开发和测试集都服从同一分布，那么你应该总是试图去获取更多的训练数据，因为这能提高你的系统性能，对吗？

尽管，有大量可供获取的数据并没有坏处，但不幸的是，它并不是总能像你期望的那样给你带来帮助，有时候只顾着获取更多的数据只会是浪费时间。那么该如何决定什么时候添加数据什么时候不添加数据呢？

在机器学习中，有两个主要的误差(Error)来源：偏差(Bias)和方差(Variance)。了解这两个指标能够省时省力地帮助我们作出是否添加数据或者其它能够提高系统性能的策略决定。

假设你希望构建一个只有5%误差的猫咪识别器。目前，你的训练集误差有15%，而开发集的误差有16%。这种情况下，往训练集塞更多的数据用处不大。此时的你应该关注其他变化。事实上，在你的训练集上增加更多的样本数据只会让你的算法在训练集上表现的越来越好而已。（我们会在后边的章节解释原因！）

如果当前你的算法在训练集上的误差是15%（准确率85%），而你的目标是降低误差到5%（准确率95%），因此首要的问题就是提高算法在训练集上的性能表现。算法在开发集和测试集的性能往往要低于训练集，也就是说，如果你的算法在你已有的数据（可以理解为训练集上）上的准确率都只有85%的话，那么想要算法在未见过的数据（可以理解为在开发集上）中达到95%的准确率无异于是天方夜谭。

假设你的算法在开发集上的误差是16%（准确率84%），通常将16%的误差分为两个部分：

- 首先，算法在训练集上的误差，在此例中是15%。我们非正式地把它看作是算法的偏差(Bias);
- 其次，算法在开发集上的表现和训练集上的表现的差值（开发集误差-训练集误差），在此例中，开发集和训练集的准确度差值为1%。我们非正式地把它看作是算法的方差(Variance) [1]。

> [1]：在统计学领域，对于方差和偏差有更正式的定义。但大致上可以将偏差定义为当你有一个很大的训练集的时候算法在训练集上的误差；将方差定义为算法在测试集表现和训练集表现的差值。当你的误差指标是均方误差时，你可以写出指定这两个量的公式，并证明“总体误差=偏差+方差”。但为了简化叙述如何通过对偏差和方差的分析来解决机器学习问题，这里给出非正式定义的偏差和方差就足够了。

学习算法的一些优化措施能够解决误差的第一部分——偏差——提高算法在训练集上的表现。有一些优化措施能够解决误差的第二部分——方差——帮助算法顺利地从训练集推广到开发/测试集上  [2]。为了选择出最能提升算法性能表现的优化方向，深刻理解错误的两个组成部分的优先级顺序会有奇效。

> [2]：还有一些方法可以通过对系统架构进行重大改动来同时减少方差和偏差，但这往往操作难度很大。

培养关于偏差和方差的良好直觉能够帮助你为算法选择有效的优化措施。
