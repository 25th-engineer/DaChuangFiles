## 46. 强化学习的例子

![](https://raw.githubusercontent.com/AlbertHG/Machine-Learning-Yearning-Chinese-ver/master/md_images/19.png)

假设你正在使用机器学习来教直升机进行复杂的飞行操作。这是一个电脑控制器直升机在引擎关闭时执行着陆的延时照片。

这是一个被称为“自动旋转(autorotation)”策略。即使直升机的引擎意外失灵，它也能允许直升机降落。人类飞行员将练习这个策略作为他们训练的一部分。你的目标是使用一种学习算法让直升机通过一个轨迹 T 来安全着陆。

为了应用强化学习，你必须设置一个“奖励函数” $R(.)$ 它给出一个分数来衡量每个可能的轨迹 T 有多好。 例如，如果轨迹 T 导致直升机坠毁，那么回报可能是 $R(T) = -1000$ ——这可真是一个巨大的负面报酬啊。一个能够让飞机安全着陆的轨迹 T 可能会产生一个正值 $R(T)$，具体值则取决于着陆的顺利程度。 通常通过手动选择奖励函数 $R(.)$ 来量化不同轨迹 T 的理想程度。它必须权衡着陆的颠簸如何，直升机是否落在正确的位置，乘客下落的程度等等多方面因素。 设计出一个好的奖励函数并不是一件容易的事。

给定奖励函数 $R(T)$，增强学习算法的任务是控制直升机，使其达到 $max_TR(T)$。然而，强化学习算法在实现这一最大化的过程中，可能会产生很多近似值，这样就无法逼近这个最大值了。

假设你选择了一些奖励函数 $R(.)$ 并运行了你的学习算法。然而，算法的表现似乎比人类飞行员糟糕得多——着陆时的颠簸更大，看起来也比人类飞行员控制的飞机更加危险。那么，你应该如何来判断故障到底是因为强化学习算法在试图找到一条轨迹 T 来实现 $max_TR(T)$ 呢？还是由于奖励函数试图在飞机降落颠簸度和飞机降落准确度之间的权衡导致的？

为了应用优化验证试验，设 $T_{human}$ 为人类飞行员所控制飞机的轨迹，设 $T_{out}$ 为算法控制飞机所实现的轨迹。根据我们上面的描述，$T_{human}$ 是 $T_{out}$ 的最优轨迹。因此关键的测试被等价为不等式是否满足：$R(T_{human}) >R(T_{out})$？

情形1：如果这个不等式成立，那就意味着，从奖励函数 $R(.)$ 中确实能得出 $R(T_{human})$ 优于 $R(T_{out})$ 的结论。也就是说我们的强化学习算法找到的是一个不是很优秀的 $T_{out}$，这表明，我们的强化学习算法还需改进。

情形2：假设不等式是 $R(T_{human}) \leq R(T_{out})$，这意味着即使我们知道 $T_{human}$ 是非常优秀路线，但是奖励函数 $R(.)$ 却给它分配了一个很差劲的分数。这表明，你应该努力改进 $R(.)$ 以更好地获得与良好着陆情况相对应的权衡。

许多机器学习应用程序都有使用近似搜索算法优化近似评分函数 $Score_X(.)$ 的“模式”。有时，没有指定的输入 $x$，则可以将评分函数简化为 $Score_(.)$。在我们上面的例子中，评分函数是奖励函数 $Score(T)= R(T)$，优化算法则具体为是一个试图让直升机执行良好轨迹 T 的强化学习算法。

与前面的示例不同的是，我们一直都是在与人类级别的性能 $T_{human}$ 进行比较，而不是“最佳”输出。我们假设人类级别的性能 $T_{human}$ 很好，即使不是“最佳”。一般来说，只要你有 $y^{* }$ （在本例中是 $T_{human}$ ）即使它不是“最优”输出，它也是你当前学习算法性能的优秀输出，那么优化验证测试依然能指导你确定优化算法和评分函数的改进方向。