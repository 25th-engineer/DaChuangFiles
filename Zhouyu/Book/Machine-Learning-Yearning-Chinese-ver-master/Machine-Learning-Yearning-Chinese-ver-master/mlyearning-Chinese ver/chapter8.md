## 8. 为团队进行算法优化建立单一数字评估指标

分类准确率是单一数字评估指标(Single-Number Evaluation Metric​​)的其中一种：你在开发集（测试集）上运行你的分类器，然后得到一个样本分类准确率的一个数字，通过这个指标，如果分类器A的准确率是97%，而分类器B的准确率是90%，则我们判定分类器A更好。

相比之下，查准率(Precision)和查全率(Recall) [2]就不属于单一数字评估指标：它给出了两个数字用来评估分类器。使用混合数字评估指标(Multiple-Number Evaluation Metrics)使得比较算法的差异变得更加困难。假设你的算法表现如下表：

> [2].对于猫分类器这个案例。查准率的定义是：分类器将开发（测试）集标记为猫的样本中，有多少真的是猫，$\frac{预测为正类的正类数量}{预测为正类的数量} * 100%$；查全率就是在开发（测试）集中，分类器正确识别出的真是猫的图片数量占所有的真是猫的图片数量的百分比,$\frac{预测为正类的正类数量}{正类数量} * 100%$。通常需要在高查准率与高查全率之间进行权衡。

分类器|查准率|查全率
---|---|---
A|95%|90%
B|98%|85%

上边的举例中，任意一个分类器都没有明显表现的比另一个好，所以这类混合数字评估指标无法立刻引导你判断出哪一个分类器更好。

在开发过程中，你的团队会进行大量关于算法架构、模型参数、特征选择方面的尝试。使用单一数字评估指标，比如准确率，可以允许你依照该指标下的性能表现对所有模型进行排序，从而快速的决断出哪一个模型效果更好。

如果你希望综合考虑查准率、查全率的性能度量，我建议使用一种标准的方法将它们组合成一个单一数字性能指标。例如，可以将查准率和查全率的均值作为评价指标。或者你可以计算“F1度量”(F1 score)，这是一种基于查准率和查全率的调和平均，比单纯的求两者的均值效果要好 [3]。

> [3].如果你想更多的了解关于“F1度量”的信息，请参阅：[https://en.wikipedia.org/wiki/F1_score](https://en.wikipedia.org/wiki/F1_score),其计算公式：$F1 = \frac{2}{\frac{1}{P}+\frac{1}{R} } = \frac{2PR}{P+R}$

分类器|查准率|查全率|F1度量
---|---|---|---
A|95%|90%|92.4%
B|98%|85%|91.0%

当你要在大量分类器中做出选择的时候，单一数字评估指标能帮助你更快的做出决策。它为算法表现提供了一个明确的优先级顺序，从而给出一个清晰的优化方向。

作为最后一个例子，假设你已经得到了你的猫分类器在四个主要市场（美国、印度、中国和其他）的分类准确率。这相当于给出了四个指标，通过将这四个地区的分类准确率进行平均或者加权平均，最终将得到一个单一数字指标。取均值或者加权平均是将多个指标合并为一个的最常用的办法之一。