## 23. 解决方差和偏差

这是解决偏差和方差最简单的公式：

- 如果存在高可避免偏差问题(High Avoidable Bias)，增大你模型的规模（比如，为模型设计更多的层数和神经元）；
- 如果存在高方差问题(High Variance)，则为训练集添加更多的数据。

如果你能够增加神经网络的规模并且无限制地增加训练数据，那么可以很好地解决许多学习算法出现的问题。

实际上，盲目增加模型的规模最终会导致你遇到计算成本的问题，因为训练非常大的模型是很慢的。而你也可能会耗尽获取更多训练数据的能力。（即使在网上，关于猫咪的图片也是有限的）

不同的模型架构——例如，不同的神经网络架构——会对你的问题产生不同的大小的偏差/方差量。近期的很多深度学习的研究开发出了很多有创新性的模型架构。所以如果你正在使用神经网络，那么学术文献将是一个很好的灵感来源。同时在 Github 上也有很多开源代码。但是需要提醒的一点就是：尝试新的模型架构产生的后果将会比单纯的增大原有模型规模和增加数据量更加难以预料。

增大模型的尺寸通常会减少偏差，但同时也会有使方差增大和过拟合的风险出现。然而，这种过拟合的问题一般只在你不使用正则化(Regularization)的时候才存在，如果在你的架构中已经包含了精心设计的正则化方法，那么就可以放心的增大模型的规模而不用担心过拟合的问题。

假设你正在使用诸如L2正则化、Dropout等深度学习技术去正则化参数来让算法在开发集上有好的表现，在此基础上如果增加模型的规模，对算法来说，他的性能通常会得到改善，至少不会变差。计算成本增加是避免使用更大规模的模型的唯一原因。