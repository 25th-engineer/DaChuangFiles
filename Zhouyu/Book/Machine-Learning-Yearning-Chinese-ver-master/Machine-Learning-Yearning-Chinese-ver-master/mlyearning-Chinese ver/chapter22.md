## 22. 比较最优误差

在我们的猫咪识别器的例子中，理想的误差，即最优分类器的误差应该接近0%。一个人类几乎总是能认出照片里边的猫来，所以，我们也希望机器能达到这个水平。

有些问题难度则更大，比如，假设你正在建立一个语言识别系统，并发现在音频片段中有14%都是背景噪声，或者是无法理解的内容，即使是人类来也无法识别出这部分信息。在这种情况下，意味着即使是“最佳的”语音识别系统也可能存在14%的误差。

假设在这个语音识别问题上，你的算法的表现如下：

- 训练集误差 = 15%
- 开发集误差 = 30%

从上边可以看出，算法在训练集上的性能表现已经接近了14%的最优误差，因此，就偏差或者训练集性能方面而言，已经没有太大的改进空间了。但是由于该算法对于开发集的拟合并不好，因此对于方差而言却存在着更大的改进空间。

这个例子类似上一节中的第三个例子（训练集误差15%，开发集误差30%）。如果最优误差是~0%，则15%的训练集误差就有很大改进空间，这表明执行减少偏差的改进措施是将会很有效果。但是，如果最优误差为14%，则相同的训练集表现告诉我们在分类器的偏差方面能改进的空间真的很小。

对于这种最优误差远远大于0%的问题，这里有关于算法误差更详细的分类。继续我们上边提到的语音识别的例子，30%的总开发集误差可细分为以下几类（类似的分类可以同时应用于对测试集误差的分析过程中）：

- 最优误差(Optimal Error Rate)：14%。假设我们定义：即使是世界上最优秀的语音系统也存在14%的误差。我们可以将这部分误差归类为学习算法偏差中“不可避免”的部分；
- 可避免偏差(Avoidable Bias)：1%。这被定义为是在训练误差和最优误差之间的差值（训练误差-最优误差） [3]；
- 方差(Variance)：15%。这被定为是开发集误差和训练集误差之间的差值（开发集误差-训练集误差）。为了和我们之前的一些定义统一起来，偏差和可避免偏差有下列联系 [4]；
    - 偏差 = 最优误差 + 可避免偏差

> [3]：如果这个差值是负数，那么你的训练集的表现比最优分类器都还要好，意味着你的算法在训练集过拟合了，并且该算法已经“Over-Memorized”了训练集。你应该专注于减少方差，而不是继续减少偏差。

> [4]：选择这样定义是为了更好地传达关于如何改进学习算法的思想。这些定义与统计学家对这些概念的定义并不一样。从技术上来说，本文所定义的“偏差”应该被称为“我们归因于偏差的错误”(Error we attribute to bias)，“可避免偏差”应该被称为“我们归因于学习算法偏差超过了最优误差的错误”(error we attribute to the learning algorithm's bias that is over the optimal error rate)。

“可避免偏差”反映了算法在训练集上的表现与“最优分类器”比还差多少。

方差的概念和前面的一样，没有变化。因为从理论上来说，我们总是可以通过大规模训练集的训练来将方差减少到零，只要使用足够大的训练集就可以完全避免方差，因此所有的方差都是“可避免的”。

再考虑一个例子，这里我们设定最优误差是14%：

- 训练集误差 = 15%
- 开发集误差 = 16%

在前一节的内容中，我们将有上述表现的分类器称为“高偏差”分类器。而现在我们可以认为该分类器的可避免偏差只有1%，也就是说，这个算法已经很优秀，几乎没有改进的空间了，因为它在开发集上的表现仅比最优分类器差了2%而已。

从这些例子中我们可以看出，知道最优误差有助于指导我们进行后续的步骤。在统计学中，“最优误差”也被称为“贝叶斯误差”(​Bayes Error Rate)或者“贝叶斯率”

那我们怎么知道最优误差是多少呢？对于一些人类擅长的任务来说，例如识别图片或者音频片段转录等，你可以要求人类来去对训练集数据进行识别，也就是测量出人类对于这个训练集的准确率来。这将给出最优误差的估计（用人类水平误差替代最优误差）。但是如果你正在解决一个对于人类而言都很困难的任务（比如，预测要推荐什么电影或者是向用户展示何种广告），最有误差是很难用这种方法来估计的。

在“比较人类表现水平”（第33节-第35节）这一部分内容中，我将会更加详细的讨论比较学习算法表现和人类水平(Human-Level)表现的这一过程。

在本章最后几节内容中，你将学习到如何通过查看训练和开发集误差来估计可避免/不可避免的偏差和方差。下一节我们将会讨论如何通过深入的分析理解来考虑优先使用的技术，即是减少偏少的技术还是减少方差的技术。根据项目当前的问题是“高（可避免）偏差”还是“高方差”，来应用完全不同的技术手段。请继续往下读！