## 28. 诊断偏差和方差：学习曲线

我们已经见过一些方法，并使用它们来估计有多少误差被归因到可避免偏差或者是归因到方差上。比如，我们通过估计最优误差和计算学习算法的训练集和开发集误差做到了这一点。现在，让我们来讨论一个信息量更大的方法：绘制学习曲线(Learning Curves)。

学习曲线以你的训练样本的数量为横轴，误差为纵轴。要绘制它，你可以使用不同的训练集大小来重复运行算法，得出在不同训练集大小下的开发集误差。比如，你总共有1000 个样本，则可以让算法分别在100、200、300、……、1000大小的样本尺寸下进行训练，然后绘制出在训练集大小不断增大下的开发集误差的变化趋势，如下图：

![](https://raw.githubusercontent.com/AlbertHG/Machine-Learning-Yearning-Chinese-ver/master/md_images/11.png)

随着训练集大小的增加，开发集误差在下降。

“期望误差率(Desired Error Rate)”是我们希望学习算法能够最终达到的那个表现水平，例如：

- 如果我们希望达到“人类表现水平”，那么“期望误差率”指的就是人类误差率；
- 如果我们的学习算法为特定的产品提供服务（例如为猫APP提供猫咪识别器），我们可能会需要一种直觉，来发觉需要达到怎么的表现水平的算法才能给用户提供一个绝佳的体验。
- 如果你长期从事在一个关键的应用程序上，那么你可能具备这个直觉，能够预计出在下季度/下一年中能够取得多大的进展。

所以，将期望的表现水平添加到你的学习曲线中：

![](https://raw.githubusercontent.com/AlbertHG/Machine-Learning-Yearning-Chinese-ver/master/md_images/12.png)

在图中，可以管观察红色的开发误差曲线，从而猜测出可以通过添加更多的数据来算法达到期望的性能水平。在上图展示的例子中，通过加倍训练集尺寸来促使学习算法朝着期望的性能改进看起来是合理的。

但是，假设开发误差曲线已经被“压平了(Plateaued)”，下图，你就能够立刻反应过来，继续添加数据已经不能够帮助你的算法往目标逼近了：

![](https://raw.githubusercontent.com/AlbertHG/Machine-Learning-Yearning-Chinese-ver/master/md_images/13.png)

通过观察学习曲线，你可能就能避免在花费了数月时间收集多达两倍的训练数据后，才意识到这对优化性能并没有帮助。

这个过程的一个缺点是：如果你只关注开发误差曲线，在有大量数据的情况下你很难推断和预测红色曲线的走势。所以，现在有另外一根额外的曲线可帮助你评估添加更多数据对性能的影响：训练误差曲线。
