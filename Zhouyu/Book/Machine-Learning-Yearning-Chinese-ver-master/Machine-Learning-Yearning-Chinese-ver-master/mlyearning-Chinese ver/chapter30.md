## 30. 解读学习曲线: 高偏差

假设你的开发误差曲线看起来是这样的：

![](https://raw.githubusercontent.com/AlbertHG/Machine-Learning-Yearning-Chinese-ver/master/md_images/15.png)

我们之前说过，如果你的开发误差曲线平稳，也就是说，无法通过添加数据来让算法达到期望的性能。

但是我们很难切确的知道红色的开发误差曲线的外推（[外推法](https://baike.baidu.com/item/%E5%A4%96%E6%8E%A8%E6%B3%95)）是怎样的。如果开发集很小，则更不确定，因为曲线可能是有噪声的。

假设我们将训练误差曲线添加到该图中并得到以下结果：

![](https://raw.githubusercontent.com/AlbertHG/Machine-Learning-Yearning-Chinese-ver/master/md_images/16.png)

现在，你可以绝对确定，添加更多的数据本身是无效的。为什么？记住我们下面提到两个结论：

- 随着我们添加越来越多的训练数据，训练误差只会越来越大。因此，蓝色的训练误差曲线只能够保持当前水平或者更糟糕（曲线趋势往上），所以随着训练集数据的增多，曲线会往远离期望性能曲线（绿线）的方向发展。
- 红色的开发误差曲线通常高于蓝色训练误差曲线，意味着，当训练误差曲线都高于期望性能曲线（绿线）并有继续远离的趋势的时候，几乎不可能通过添加数据的方式来让红色的开发误差曲线下降到期望的性能水平。

在同一个图表上同时检查开发误差曲线和训练误差曲线。使我们能够更加自信的去推断开发误差曲线。

为了更好的阐述，假设我们把对最优误差的估计作为期望性能表现，那么上图就是一个教科书式的关于一个具有高可避免偏差的学习曲线的例子：在最大的训练集规模下（大概理解为对应于我们所有的训练数据），训练误差和期望误差之间存在很大的差距（进步空间），说明该案例遇到了高可避免偏差问题。此外，训练误差曲线和开发误差曲线的差距很小，意味着方差不大。

以前，我们只在该图的最右点来测量训练集和开发集的误差，这个点与我们算法使用了全部可用训练数据相对应。而绘制完整的学习曲线可以让我们更加全面的了解在不同训练集大小下的算法性能表现。