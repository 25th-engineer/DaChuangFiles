{"content2":"我们对于“人工智能”这个术语都很熟悉。毕竟，它是《终结者》,《黑客帝国》和《机械姬》等美国大片电影中非常流行的关键词。但你最近或许也听说过其他术语，像“机器学习”和“深度学习”，有时这两个术语会和“人工智能”互相替换使用，前年早些时候，Google DeepMind的AlphaGo打败了韩国的围棋大师李世乭九段。在媒体描述DeepMind胜利的时候，将人工智能（AI）、机器学习（machine learning）和深度学习（deep learning）都用上了。这三者在AlphaGo击败李世乭的过程中都起了作用，但它们说的并不是一回事。那么这三个名词之间有什么区别？我会先解释一下人工智能（AI）、机器学习（ML）和深度学习（DL），以及它们有怎样的区别。1 三者的概念人工智能（英语：Artificial Intelligence, AI）：是指由人工制造出来的系统所表现出来的智能。通常人工智能是指通过普通电脑实现的智能。人工智能的研究可以分为几个技术问题。其分支领域主要集中在解决具体问题，其中之一是，如何使用各种不同的工具完成特定的应用程序。AI的核心问题包括推理、知识、规划、学习、交流、感知、移动和操作物体的能力等。目前有大量的工具应用了人工智能，其中包括搜索和数学优化、逻辑推演。而基于仿生学、认知心理学，以及基于概率论和经济学的算法等等也在逐步探索当中。机器学习（英语：Machine Learning）：是人工智能的一个分支。人工智能的研究是从以“推理”为重点到以“知识”为重点，再到以“学习”为重点，一条自然、清晰的脉络。显然，机器学习是实现人工智能的一个途径，即以机器学习为手段解决人工智能中的问题。机器学习在近30多年已发展为一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、计算复杂性理论等多门学科。机器学习理论主要是设计和分析一些让计算机可以自动“学习”的算法。机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。因为学习算法中涉及了大量的统计学理论，机器学习与推断统计学联系尤为密切，也被称为统计学习理论。算法设计方面，机器学习理论关注可以实现的，行之有效的学习算法。很多推论问题属于无程序可循难度，所以部分的机器学习研究是开发容易处理的近似算法。机器学习有下面几种定义：机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是 如何在经验学习中改善具体算法的性能。 机器学习是对能通过经验自动改进的计算机算法的研究。 机器学习是用数据或以往的经验，以此优化计算机程序的性能标准。机器学习已广泛应用于数据挖掘、计算机视觉、自然语言处理、生物特征识别、搜索引擎、医学诊断、检测信用卡欺诈、证券市场分析、DNA序列测序、语音和手写识别、战略游戏和机器人等领域。机器学习最成功的应用领域是计算机视觉，虽然也还是需要大量的手工编码来完成工作。人们需要手工编写分类器、边缘检测滤波器，以便让程序能识别物体从哪里开始，到哪里结束；写形状检测程序来判断检测对象是不是有八条边；写分类器来识别字母“ST-O-P”。使用以上这些手工编写的分类器，人们总算可以开发算法来感知图像，判断图像是不是一个停止标志牌。深度学习（英语：Deep Learning）：是机器学习拉出的分支，它试图使用包含复杂结构或由多重非线性变换构成的多个处理层对数据进行高层抽象的算法。深度学习是机器学习中一种基于对数据进行表征学习的方法。观测值（例如一幅图像）可以使用多种方式来表示，如每个像素强度值的向量，或者更抽象地表示成一系列边、特定形状的区域等。而使用某些特定的表示方法更容易从实例中学习任务（例如，人脸识别或面部表情识别）。深度学习的好处是用非监督式或半监督式的特征学习和分层特征提取高效算法来替代手工获取特征。统计学习：关于计算机基于数据构建概率统计模型，并运用模型对数据进行预测与分析的一门学科。 机器学习：致力于研究如何通过计算的手段，利用经验来改善系统自身的性能。 深度学习：机器学习中的神经网络算法的延伸，可以理解为包含很多个隐层的神经网络模型。2  三者的区别人工智能：人工智能是人类社会发展主要目标 机器学习：机器学习是实现人工智能的核心技术 深度学习：是机器学习中最热门的算法1956年，约翰·麦卡锡成为了第一位创造了人工智能机器的人。他制造的机器具备足够高的能力，得以执行类似人类智力水平的任务，包括：做出规划、理解语言、识别对象和声音、学习并解决问题等。对于人工智能，我们可以从广义和狭义两个层面来理解。广义层面来讲，AI应该具备人类智力的所有特征，包括上述的能力。狭义层面的人工智能则只具备部分人类智力某些方面的能力，并且能在这些领域内做的非常出众，但可能缺乏其他领域的能力。比如说，一个人工智能机器可能拥有强大的图像识别功能，但除此之外并无他用，这就是狭义层面AI的例子。从核心上来说，机器学习是实现人工智能的一种途径。1959年，Arthur Samuel在AI之后创造了“机器学习”这个短语，并将其定义为“在没有被明确编程的情况下就能学习的能力。”当然，你可以不使用机器学习的方式来实现人工智能，不过这需要你运用复杂的规则和决策树，再敲下几百万行的代码才行。实际上，机器学习是一种“训练”算法的方式，目的是使机器能够向算法传送大量的数据，并允许算法进行自我调整和改进，而不是利用具有特定指令的编码软件例程来完成指定的任务。举个例子，机器学习已经被用于计算机视觉（机器具备识别图像或视频中的对象的能力）方面，并已经有了显著的进步。你可以收集数十万甚至数百万张图片，然后让人标记它们。例如，让人标记出其中含有猫的图片。对于算法，它也能够尝试建立一个模型，可以像人一样准确地标记出含有猫的图片。一旦精度水平足够高，机器就相当于“掌握”了猫的样子。深度学习是机器学习的众多方法之一。其他方法包括决策树学习、归纳逻辑编程、聚类、强化学习和贝叶斯网络等。深度学习的灵感来自大脑的结构和功能，即许多神经元的互连。人工神经网络（ANN）是模拟大脑生物结构的算法。在ANN中，存在具有离散层和与其他“神经元”连接的“神经元”。每个图层挑选出一个要学习的特征，如图像识别中的曲线/边缘。 正是这种分层赋予了“深度学习”这样的名字，深度就是通过使用多层创建的，而不是单层。3  深度学习，给人工智能以璀璨的未来深度学习使得机器学习能够实现众多的应用，并拓展了人工智能的领域范围。深度学习摧枯拉朽般地实现了各种任务，使得似乎所有的机器辅助功能都变为可能。无人驾驶汽车，预防性医疗保健，甚至是更好的电影推荐，都近在眼前，或者即将实现。所以我们就安心学习好机器学习就好，那么如何学习好机器学习呢，下面用几张图片展示！4   如何学好机器学习？机器学习可以分为下面几种类别监督学习从给定的训练数据集中学习出一个函数，当新的数据到来时，可以根据这个函数预测结果。监督学习的训练集要求是包括输入和输出，也可以说是特征和目标。训练数据中的目标是由人标注的。常见的监督学习算法包括回归分析和统计分类。无监督学习与监督学习相比，训练集没有人为标注的结果。常见的无监督学习算法有聚类。半监督学习介于监督学习与无监督学习之间。它主要考虑如何利用少量的标注样本和大量的未标注样本进行训练和分类的问题。。增强学习通过观察来学习做成如何的动作。每个动作都会对环境有所影响，学习对象根据观察到的周围环境的反馈来做出判断。在传统的机器学习领域，监督学习最大的问题是训练数据标注成本比较高，而无监督学习应用范围有限。利用少量的训练样本和大量无标注数据的半监督学习一直是机器学习的研究重点。当前非常流行的深度学习GAN模型和半监督学习的思路有相通之处，GAN是“生成对抗网络”（Generative Adversarial Networks）的简称，包括了一个生成模型G和一个判别模型D，GAN的目标函数是关于D与G的一个零和游戏，也是一个最小-最大化问题。GAN实际上就是生成模型和判别模型之间的一个模仿游戏。生成模型的目的，就是要尽量去模仿、建模和学习真实数据的分布规律；而判别模型则是要判别自己所得到的一个输入数据，究竟是来自于真实的数据分布还是来自于一个生成模型。通过这两个内部模型之间不断的竞争，从而提高两个模型的生成能力和判别能力。图片总结的如此清晰，我就不重复说了（此图片来此天善智能某课堂的PPT）5  机器学习知识结构小编从学习机器学习需要的各个方面在此阐述了要想学习机器学习，首先需要学习或者说准备什么东西，从以下四个方面说起。5.1 数学基础大学专业不是数学的同志们需要恶补的知识科目如下：微积分 线性代数 矩阵论 凸优化 离散数学 概率论 统计学 随机过程5.2   机器学习理论机器学习的理论知识如下，其中推荐的包括算法和学习模型，还有训练的网址，全是干货哦，当然还是不全，以后小编了解到会逐渐加上的。有监督机器学习模型和算法：分类和回归 线性回归 感知机器学习 决策树 朴素贝叶斯 人工神经网络，逻辑回归，随机森林，GBDT lightgbm xgboost....5.3  编程与开发编程开发使用的主要是python语言和Linux服务器，加上TensorFlowpython：numpy pandas matplotlib seaborn sklearn Linux : java c Spark Hadoop SQL excel..5.4 英文能力熟练地英语阅读能力6  以下文章将会有助于你更加深入了解人工智能、机器学习、深度学习：小编文中许多知识点都是参考下面的文章，大家有兴趣的可以继续了解三者的区别。1、Artificial Intelligence, Machine Learning, and Deep Learning2、Why Deep Learning is Radically Different from Machine Learning3、一篇文章讲清楚人工智能、机器学习和深度学习的区别4、人工智能，机器学习和深度学习有什么区别？5、如何区分人工智能、机器学习和深度学习？6、WHY DEEP LEARNING IS SUDDENLY CHANGING YOUR LIFE7、The Current State of Machine Intelligence 3.08、Here are 50 Companies Leading the AI Revolution"}
{"content2":"一、目标：C++/python研发岗  算法岗二、公司、岗位总结公司名称地点岗位实习要求职位描述岗位要求转正机会星环信息科技上海分布式存储3天/周，4个月以上1. 负责星环分布式存储系统的设计，开发与运维工作；2. 负责实现分布式存储系统的解决方案，架构设计和关键特性；3. 负责构建高并发、高吞吐、低延时的数据存储服务。1. 本科及以上学历，计算机/软件工程相关专业；2. 熟练掌握Java、Scala、C++中的一种或多种；3. 有良好的数据结构和算法基础；4. 积极主动，有良好的解决问题能力；5. 熟练使用Linux；6. 有HDFS、HBase、MongoDB、Cassandra、ElasticSearch等分布式系统的开发或运维经验者优先；7. 熟悉MySql、PostgreSQL等数据库者优先。有字节跳动北京C++4天/周，3个月以上1、负责2D/3D特效制作工具开发 2、根据开发规范与流程完成模块的设计、编码、测试以及编写相关文档 3、跟踪业界的最新产品，打磨产品体验1、熟练掌握C++，数据结构和算法 2、熟悉Objective-C、QT、Windows开发优先 3、熟练掌握各种设计工具、方法，编码功底扎实 4、有较好的产品意识，具有良好的分析和解决问题能力中标软件北京C++4天/周，3个月以上1、办公软件新功能开发；2、办公软件兼容性问题修复；3、办公软件性能优化相关工作；4、外部项目的其它相关事项；5、配合产品组制定研发需求与规划；6、其它组长分配的办公软件研发相关任务；1. 熟悉办公软件的基本功能使用；2. 具有较为扎实的C++基础，熟悉常用的数据结构和算法，熟悉常见的设计模式；3. 具备Windows下VS以及linux下QTCreator良好的调试能力；4. 思维方式灵活，能从多角度看待问题；5. 做事积极主动，富有责任心和集体荣誉感；6. 计算机及相关专业；7. 优先条件：（1） 对操作系统有深刻理解，具有linux开发经验；（2） 了解多种开发语言，或有相关开发经验（如：java、Python、javascript等）；（3） 熟悉二进制、OOXML文档格式或有相关文档的处理分析经验；（4） 熟悉linux操作系统和OpenOffice / libreoffice等开源办公软件。面试聚力维度北京C++6天/周，3个月以上1.负责根据公司人工智能科研项目的需要，做配套的软件开发工作；1.掌握C++语言，熟悉常见的数据结构和算法；2.具有很强的多线程代码调试、BUG定位及修复能力；3.熟练使用OpenGL、D3D、CUDA等任意一种图形API，了解计算机图形学；4.熟悉UE4引擎者优先；5.责任心强，有较强的学习能力，较好的沟通交流能力，能够迅速融入团队。6.对人工智能行业发展有热情者优先。有Momenta魔门塔北京C++4天/周，3个月以上1.搭建深度学习部署框架。2.与算法、SDK团队协作，实现高效、易用的解决方案。1.熟练掌握 C/C++，有很强的工程开发经验。2.了解常见深度学习算法和模型，对深度学习性能优化有热情。3.有敏锐的观察力，迅速定位问题和解决问题。4.熟悉计算机体系结构和编译原理者优先。有商汤科技北京，上海，深圳C++3天/周，3个月以上1. 参与深度学习平台的设计与研发2. 对深度学习最新算法与架构进行调研，实现及改进3. 对计算调度与分布式通信进行优化4. 依托公司内部深度学习设施，建立垂直行业的综合应用平台以下1、2条满足1条即可：1. 能熟练运用c++11编程，具有良好的编程习惯2. 对于主流Deep Learining框架的内部框架有深入了解的优先3. 具有良好的沟通理解能力，喜欢技术方面的工作有商汤科技北京，上海，深圳Docker容器云3天/周，3个月以上1. 容器云平台建设、优化2. 深度学习解决方案容器化3. 分布式解决方案容器化4. AI云平台前瞻性探索与实践1. 精通至少一种：c, c++, Go2. 了解docker基本工作原理3. 具有发现问题、研究问题、解决问题的积极性以及方法体系加分项，满足任何一项均可获取重大加分：1. 在容器云方向具有卓越的远见并具备一定程度的执行力2. 精通Kubernetes相关领域任意核心模块3. 精通Docker相关领域任意核心模块4. 深度参与云原生领域或者分布式领域任意重要开源项目有商汤科技北京，上海，深圳python3天/周，3个月以上1. 参与深度学习平台的设计与研发2. 对深度学习最新算法与架构进行调研，实现及改进3. 对计算调度与分布式通信进行优化4. 依托公司内部深度学习设施，建立垂直行业的综合应用平台1. 能熟练运用python编程与架构较大规模的python程序2. 对于主流Deep Learining框架的内部框架有深入了解的优先3. 具有良好的沟通理解能力，喜欢技术方面的工作有字节跳动北京后台开发3天/周，3个月以上1、负责字节跳动产品的后台开发工作 2、主要实现语言python和golang1、良好的设计和编码品味，热爱写代码 2、较好的产品意识，愿意将产品效果做为工作最重要的驱动因素 3、掌握WEB后端开发技术: 协议、架构、存储、缓存、安全等 4、积极乐观，认真负责，乐于协作 5、每周可以实习3天以上，连续实习3个月以上 6、计算机、软件、通信相关专业同学优先有网易游戏杭州UE4游戏研发实习4天/周，3个月以上1、使用UE4引擎进行IOS/Android平台手机游戏开发2、与策划,美术保持良好的沟通,确保程序功能与设计需求一致3、开发游戏核心逻辑,包括但不限于战斗、同步、寻路等模块4、针对各个发布平台进行游戏性能优化1、计算机,软件开发,游戏开发,网络安全等本科及以上学历2、扎实的计算机基础知识,深入理解数据结构,算法,操作系统等知识3、良好的C++编程功底,熟悉一门脚本语言4、良好的逻辑分析能力,以及解决问题的强烈意愿5、热爱游戏,玩过FPS游戏者优先面议网易游戏杭州Unity客户端程序实习4天/周，6个月以上1、使用Unity3D引擎进行iOS/Android平台手机游戏开发；2、与策划、美术保持良好沟通，确保程序功能与设计需求一致；3、研发所需的各类工具，编辑器等内容的再开发以及调整优化；4、针对各个发布平台进行游戏性能优化1、热爱游戏；2、计算机及相关专业本科以上学历，有C、C++、C#、python或Lua语言编程基础；3、有unity引擎使用经验为佳4、良好的逻辑思维和编程习惯，具备独立解决技术问题的能力5、有责任感，有良好的沟通能力，具备团队合作精神6、能承受一定的工作压力，对游戏开发充满热情7、能保证每周实习4~5天，连续实习6个月的学生优先。有智加智行苏州软件实习4天/周，3个月以上1.负责自动驾驶相关的前沿技术研发工作。2.协助其他团队完成自动驾驶研发工作。1.熟练掌握C++/python。2.大三/研二在读，计算机专业/电气工程/数学或者统计学专业优先。3.一周至少保证3天实习，连续实习至少3个月。有图森未来北京项目经理4天/周，3个月以上对项目开发流程、项目质量和项目开发进度的规划、控制、监督和管理；作为接口人，和其他相关部门沟通和对接需求；以天为单位完成项目状态跟进和日报，发现并协助解决项目推进过程中的各种问题，直至完成；定期完成项目分析报告，并及时通告相关人员；领导交代的其他任务。对人工智能充满热情，对于相关技术有基本的了解；计算机、软件工程、自动化或相关专业本科以上；有基本的英文读写能力；优秀的逻辑分析能力和语言组织能力，可以自行分析和提炼问题，并形成报告；有图森未来北京深度学习框架研发4天/周，3个月以上针对公司内部需求，优化深度学习平台（MXNet）训练和部署性能；基于MXNet，设计和研发大规模分布式训练平台；为相关开源项目贡献新Feature。了解机器学习以及深度学习基本知识；熟悉计算机体系结构和性能优化基本知识；在相关开源项目（MXNet，Caffe，TensorFlow，PyTorch）有贡献者优先。加分项：有并行分布式开发经验或ACMICPC/超算比赛等参赛经验。有图森未来北京感知算法4天/周，3个月以上追踪并改进前沿感知算法模块，包括但不限于基于视觉与LiDAR的物体检测，场景分割，目标追踪等；将独立感知模块的输出进行融合，得到适合后续路径规划模块的表示。在计算机视觉/深度学习/机器感知三者之中至少熟悉一项基本知识，有处理真实大规模数据经验（课程作业除外）；熟悉Python, C++；动手能力强，可以快速将想法落实；加分项：有计算机视觉或机器学习相关研究经验，有高水平论文发表参加kaggle比赛获得前十名者参加过ACM/ICPC等编程竞赛者取得优异成绩有地平线北京生成式模型算法4天/周，3个月以上1. 负责基于深度学习的生成式序列预测模型算法研究与实现；2. 以业务应用为导向，探索数据处理、模型训练评测、推断加速等方面的工程内容；3. 由资深算法工程师指导发表相关高质量论文，撰写相关专利。1. 计算机科学、电子信息工程、数学或相关专业的在读硕士或博士（也欢迎优秀的本科生）；2. 良好的工程实现能力，熟练掌握C/C++、Java、Python等至少一门语言；3. 了解并使用过以下深度学习框架中的至少一种: MXNet/Caffe/Caffe2/Pytorch/Tensorflow/Keras4. 对CNN、RNN、VAE、GAN等模型中至少一种有研究使用经验者优先。有网易游戏杭州强化学习4天/周，3个月以上1、利用深度学习和强化学习学习游戏中的智能体。2、需要候选人具备一定的问题分析能力， 问题建模能力和独立研究的能力。实习期间能够撰写论文或专利。1、计算机和机器学习等相关专业硕士及以上学历；2、在机器学习、深度学习和强化学习方向具有扎实的理论和实践基础，保持对领域最前沿技术的追踪；3、熟练掌握一种常见的深度学习框架，譬如 TensorFlow/Keras和PyTorch；4、熟练掌握常用的游戏开发脚本（c#、Python）；5、有数据挖掘、和机器人控制算法相关经验者优先；6、在顶级会议（AAAI/IJCAI/ICML /NIPS等）或期刊有论文发表者优先;7、能够独立分析并解决问题，有带项目经验者优先。有百度北京视觉深度学习实习5天/周，3个月以上北京百度智能汽车事业部招一名实习生，一月可以入职，硕士工资大概200一天，外地实习生有额外补助。具体JD如下：-负责无人车中计算机视觉或深度学习算法相关的前沿技术研发工作-负责图像中的目标检测, 分类以及跟踪等-负责图像理解等相关算法研发工作-自动化、计算机、电子、应用数学等相关专业在读或者已毕业学生-在计算机视觉，机器学习或者深度学习中至少精通一项-具有良好的沟通能力，良好的团队合作精神，良好的科研精神-熟悉C/C ++或Python程序开发，具有良好的编程习惯-具有良好的英文读写能力(Preferred)面议旷视科技北京算法4天/周，3个月以上1、协助或独立推进计算机视觉和深度学习领域的核心算法；2、协助或独立构建计算机视觉或深度学习领域的关键应用。1、扎实的编程基础；2、对CV / DL有浓厚的兴趣，致力于在此领域发展。包括但不限于计算机视觉（包括分类，检测，分割，跟踪，和三维重建）；图像或信号处理；计算摄影学和计算机图形学等；3、熟悉 Python 等至少一门脚本语言，使用过 Theano, Caffe, Torch, TensorFlow 等开源深度学习框架优先；4、有深度模型训练，图像分类、物体检测与分割、视频分析、三维建模、计算机图形学等相关科研经历者（例如顶级会议第一作者）优先。有旷视科技北京算法工程Intern4天/周，3个月以上1. 参与人脸识别算法研发2. 参与建设人脸识别算法迭代与落地System3. 研发迭代亿级/十亿数据规模算法4. 左手Paper，右手Code，拉通算法与工程落地1. 良好的编程能力，熟练掌握 Python,C/C++ 等至少一门语言2. 扎实的数据结构和算法基础，熟悉 Linux 操作系统3. 对计算机视觉或深度学习有浓厚兴趣4. 有算法竞赛、机器学习或分布式系统经验者优先5. 每周周一到周五至少实习四天，实习时长不低于三个月有快手科技北京算法3天/周，3个月以上1、参与快手商业化投放排序算法的研究及开发工作；2、分析、理解用户数据和业务场景，设计合理的推荐算法，优化Ranking机制；3、学习快手商业化CXR预估算法及相关机器学习/深度学习算法研发3、设计良好的架构支持高并发、大数据的策略业务需求。1、计算机或相关专业本科以上学历，硕士博士优先2、熟悉机器学习、数据挖掘或深度学习相关基础知识，对机器学习的常用算法有使用经验，并能根据实际场景进行优化；3、熟练掌握linux下面向对象编程；熟悉Java or C++，基础扎实；4、善于阅读文献，快速学习 ；5、有开发高品质产品、编写高质量代码的自我要求；6、每周可至少出勤3天，连续实习3个月及以上有图森未来北京深度学习框架研发5天/周，2个月以上针对公司内部需求，优化深度学习平台（MXNet）训练和部署性能；基于MXNet，设计和研发大规模分布式训练平台；为相关开源项目贡献新Feature。了解机器学习以及深度学习基本知识；熟悉计算机体系结构和性能优化基本知识；在相关开源项目（MXNet，Caffe，TensorFlow，PyTorch）有贡献者优先。加分项：有并行分布式开发经验或ACMICPC/超算比赛等参赛经验。有小米科技北京软件开发4天/周，3个月以上-搜索相关基础平台的开发工作-搜索及相关领域的机器学习算法设计与应用-熟悉Linux环境下Java/C/C++、python、SHELL等语言编程-对常用数据结构和算法有较为深刻的理解-优秀的分析和解决问题的能力，对挑战性问题充满激情 -强烈的上进心和求知欲，较强的学习能力和沟通能力，具备良好的团队合作精神具备以下条件者优先：-具备一定机器学习背景，熟悉常见机器学习算法-具备搜索相关经验，在自然语言文本处理上有经验优先面议思核科技上海AI算法5天/周，3个月以上探索与实现计算机视觉/深度学习/机器人领域的前沿研究，推动研究的落地与应用；l 研发与优化计算机视觉算法，如物体检测识别，目标跟踪，场景分割等；l 研发与优化深度学习算法，包括深度学习模型设计/网络训练与测试/数据生成与维护等；l 理解并实现机器人在以下相关领域的算法：抓取、操作、运动规划、控制、感知、学习等。1、本科及以上学历，在校生，计算机相关专业；2、强大的C++/Python编程能力，优秀的算法、数据结构知识；3、强烈技术热情和学习欲望，快速掌握新知识；4、出色的解决问题能力，乐于技术分享和讨论。优先项：1、计算机视觉/机器人/机器学习专业知识背景；2、有计算机视觉/机器人/机器学习领域研究或实际研发经验；3、NOI、ACM、超算、Robocon等竞赛获奖经历。有腾讯上海计算机视觉4天/周，3个月以上1.负责行人检测追踪，行人识别等深度学习相关算法研发2.算法模型迭代，sdk开发1. 熟悉C++和python，较强的工程能力2. 熟悉deep leaning和计算机视觉相关算法有平安科技深圳金融安全研究员5天/周，3个月以上1、应用AI算法进行智能化的安全检测和分析2、熟悉深度学习算法（eg, RNN, LSTM）及开源框架（eg, Keras, Tensorflow）3、熟悉计算机网络（eg, DNS, TCP/IP）和安全运营（eg, firewall, IDS）4、良好的英语阅读和写作能力1、应用AI算法进行智能化的安全检测和分析2、熟悉深度学习算法（eg, RNN, LSTM）及开源框架（eg, Keras, Tensorflow）3、熟悉计算机网络（eg, DNS, TCP/IP）和安全运营（eg, firewall, IDS）4、良好的英语阅读和写作能力有商汤科技杭州见习研究员3天/周，3个月以上1.前沿SLAM算法的调研，复现和改进2.手机/自动驾驶等业务方向SLAM、VIO算法研发3.现有SLAM算法的性能和效果优化or1.深度学习在SLAM领域的调研和应用2.基于深度学习的光流，VO等算法的应用3.基于深度学习的标志识别，物体识别等工作，应用于基于Marker和Object的SLAM和AR1.计算机/数学/电子工程等相关专业，扎实的数学（数值优化，线性代数，图像处理）和编程（C++，Linux）基础；2.有SLAM/VIO/VO的相关项目经历者优先；3.有SLAM或者深度学习、神经网络等相关项目经历者优先；4.有计算机视觉/机器人等领域相关论文优先；5.有高性能计算（GPU、DSP，NEON编程）经验者优先。有网易游戏杭州用户画像4天/周，3个月以上1.    在人工智能与游戏的交叉领域中进行创新研究，探索新的游戏设计及交互方式；2.    借助游戏平台及数据迭代发展人工智能技术，在相关领域发表高质量论文或撰写专利；3.    使用人工智能技术，在游戏领域中实现全面、快速、解耦合的画像状态生成与应用；4.    充分发挥人工智能优势，深入理解与精准定位用户，挖掘用户个性化等相关应用；5.    解决游戏中检测、排序、估值、预测、推荐等数值类难点问题，以研究推动应用落地；- 有一定游戏经历，热爱人工智能；- 熟悉常用的数据挖掘、机器学习、深度学习方法；- 精通至少一种编程语言，包括但不限于Python、C/C++、Java、PHP、R、C#等；- 有丰富的Linux使用经验，熟练掌握Shell等一门以上脚本语言；- 有图研究（Graph）、流失研究、模型解释性、解耦合等领域经验者优先；- 对用户画像，游戏中检测、排序、估值、预测、推荐等数值类难点问题有经验或有兴趣者优先；- 有过深度学习工程项目经验者优先，能熟练使用主流深度学习框架者优先；- 在人工智能会议和期刊发表过优秀论文，有顶级会议期刊发表经历者优先；- 实时关注人工智能前沿，定期泛读顶会论文，精读领域内好文；- 编程基础扎实，逻辑思维能力强，有较强的学习能力和创新思维；- 专注、精益求精、时间规划能力强、有强烈的探索精神及深入思考的能力；- 具有良好的沟通、协作能力，较高的开发效率和承压能力，责任意识强；-20年及之后毕业或19届已保研，每周实习4-5天，保证实习4个月以上者优先。有网易游戏杭州强化学习4天/周，3个月以上1、利用深度学习和强化学习学习游戏中的智能体。2、需要候选人具备一定的问题分析能力， 问题建模能力和独立研究的能力。实习期间能够撰写论文或专利。1、计算机和机器学习等相关专业硕士及以上学历；2、在机器学习、深度学习和强化学习方向具有扎实的理论和实践基础，保持对领域最前沿技术的追踪；3、熟练掌握一种常见的深度学习框架，譬如 TensorFlow/Keras和PyTorch；4、熟练掌握常用的游戏开发脚本（c#、Python）；5、有数据挖掘、和机器人控制算法相关经验者优先；6、在顶级会议（AAAI/IJCAI/ICML /NIPS等）或期刊有论文发表者优先;7、能够独立分析并解决问题，有带项目经验者优先。有图普科技杭州算法5天/周，6个月以上1.展开机器学习/深度学习等相关领域研究和开发工作；2.从事深度学习系统开发工作（包括机器学习、图像处理等的算法和系统研发）1.机器学习/模式识别等相关专业2019届硕士或博士；2.熟悉linux，c++，python以及后端服务器开发3.在深度学习领域有一定实践经验；4.善于解决和分析问题，富有想象力和学习能力，良好的团队合作精神。有创造性思维，有推进人工智能的理想和使命感；a.写过或较深入参与过深度学习框架开发者优先；b.在ML/DL方向知名国际会议期刊发表过论文者优先；简历投递地址：campus@tuputech.com有图普科技杭州深度学习4天/周，3个月以上利用深度学习进行图像处理领域最前沿算法研发，并将算法应用到实际场景中；1.    硕士，计算机相关专业；2.    计算机基础扎实，有很强的算法实现能力；3.    有深度学习相关实践者优先；4.    在计算机视觉领域顶级会议或者期刊发表者文章者优先；5.    可实习时间至少3个月；简历投递地址：campus@tuputech.com有网易游戏杭州强化学习4天/周，4个月以上利用深度学习和强化学习学习游戏中的智能体。 需要候选人具备一定的问题分析能力， 问题建模能力和独立研究的能力。实习期间能够撰写论文或专利。1、计算机和机器学习等相关专业硕士及以上学历；2、在机器学习、深度学习和强化学习方向具有扎实的理论和实践基础，保持对领域最前沿技术的追踪；4、熟练掌握一种常见的深度学习框架，譬如 TensorFlow/Keras和PyTorch；5、熟练掌握常用的游戏开发脚本（c#、Python）；6、有数据挖掘、和机器人控制算法相关经验者优先；6、在顶级会议（AAAI/IJCAI/ICML /NIPS等）或期刊有论文发表者优先;8、能够独立分析并解决问题，有带项目经验者优先。有"}
{"content2":"人类诞生之初也是一张白纸，但是人的大脑会生长，逐渐学会语言，学会走路，通过感官来接受信息，并通过大脑存储处理，使人学会或知道。人类的大脑真的很奇妙，尤其是思考，突发式的，联想式的，让人觉得很奇妙，觉得奇妙可能是因为不知道原因但是又有好感，看我刚刚就进行了一次联想式的思考。那么如何让计算机拥有智能，我想这是人工智能的最大难题。计算机必须拥有学习能力，创造能力，语言识别，视觉处理，并且有可塑性，适应性，好奇心，思考等等。最大的难题在于理论把，要做人工智能先得把人研究透彻，通过模仿人的大脑来设计。计算机能做到人做不到的，但是人也能做到计算机做不到的，计算机经验计算，但人却会让它计算。人工智能绝不是设计一个程序让机器达到一个目标，这么简单。如果你认为把海量信息输入电脑，企图让它们找出其中规律，学会这种规律并预测，又或者让他们通过外界获取信息，来不断的模拟来学习某项技能就可以达到人工智能就错了。智能要会思考，而思考本身也是一种思考。也许有一天人的代码能实现智能，就像计算机的诞生一样，从各种电子元器件开始，组成各种电路，电路又组成各种复杂的具有某种特定功能的器件，而随着集成的程度越来越高，这些器件组成也越来越复杂，功能也越来越强大。我觉得人工智能也会走一样的路，只是需要的理论基础难度和努力是成指数增长的，就像人类社会从原始到农业到工业iya再到信息产业，是社会维度的变迁，层次的变迁，是更多的积累，才产生质变，而这些积累不是一个数量级的。人工智能诞生的那天，社会的理论体系必将更加完善。"}
{"content2":"作者：魏秀参链接：https://zhuanlan.zhihu.com/p/21824299来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。说起特斯拉，大家可能立马会想到今年5月份发生在特斯拉Model S自动驾驶上的一宗夺命车祸。初步的调查表明，在强烈的日照条件下，驾驶员和自动驾驶系统都未能注意到牵引式挂车的白色车身，因此未能及时启动刹车系统。而由于牵引式挂车正在横穿公路，且车身较高，这一特殊情况导致Model S从挂车底部通过时，其前挡风玻璃与挂车底部发生撞击，导致驾驶员不幸遇难。无独有偶，8月8日美国密苏里州的一名男子、特斯拉Model X车主约书亚·尼利（Joshua Neally）在上班途中突发肺栓塞。在Model X的Autopilot自动驾驶功能的帮助下，他安全抵达了医院。这“一抑一扬”着实让人回味无穷，略有些“败也萧何，成也萧何”之意。好奇的读者一定会有疑问：这“一成一败”背后的原理到底是什么？是自动驾驶系统中的哪个部分发生了失误而造成车祸？又是哪部分技术支撑了自动驾驶过程呢？今天，我们就来谈谈自动驾驶系统中的一项重要核心技术——图像语义分割（Semantic image segmentation）。图像语义分割作为计算机视觉（Computer vision）中图像理解（Image understanding）的重要一环，不仅在工业界的需求日益凸显，同时语义分割也是当下学术界的研究热点之一。什么是图像语义分割？图像语义分割可以说是图像理解的基石性技术，在自动驾驶系统（具体为街景识别与理解）、无人机应用（着陆点判断）以及穿戴式设备应用中举足轻重。我们都知道，图像是由许多像素（Pixel）组成，而「语义分割」顾名思义就是将像素按照图像中表达语义含义的不同进行分组（Grouping）／分割（Segmentation）。下图取自图像分割领域的标准数据集之一PASCAL VOC。其中，左图为原始图像，右图为分割任务的真实标记（Ground truth）：红色区域表示语义为“person”的图像像素区域，蓝绿色代表“motorbike”语义区域，黑色表示“background”，白色（边）则表示未标记区域。显然，在图像语义分割任务中，其输入为一张的三通道彩色图像，输出则是对应的一个矩阵，矩阵的每一个元素表明了原图中对应位置像素所表示的语义类别（Semantic label）。因此，图像语义分割也称为“图像语义标注”（Image semantic labeling）、“像素语义标注”（Semantic pixel labeling）或“像素语义分组”（Semantic pixel grouping）。从上图和题图中，大家可以明显看出图像语义分割任务的难点便在于这“语义”二字。在真实图像中，表达某一语义的同一物体常由不同部件组成（如，building，motorbike，person等），同时这些部分往往有着不同的颜色、纹理甚至亮度（如building），这给图像语义的精确分割带来了困难和挑战。前DL时代的语义分割从最简单的像素级别“阈值法”（Thresholding methods）、基于像素聚类的分割方法（Clustering-based segmentation methods）到“图划分”的分割方法（Graph partitioning segmentation methods），在深度学习（Deep learning, DL）“一统江湖”之前，图像语义分割方面的工作可谓“百花齐放”。在此，我们仅以“Normalized cut” [1]和“Grab cut” [2]这两个基于图划分的经典分割方法为例，介绍一下前DL时代语义分割方面的研究。Normalized cut （N-cut）方法是基于图划分（Graph partitioning）的语义分割方法中最著名的方法之一，于2000年Jianbo Shi和Jitendra Malik发表于相关领域顶级期刊TPAMI。通常，传统基于图划分的语义分割方法都是将图像抽象为图（Graph）的形式（为图节点，为图的边），然后借助图理论（Graph theory）中的理论和算法进行图像的语义分割。常用的方法为经典的最小割算法（Min-cut algorithm）。不过，在边的权重计算时，经典min-cut算法只考虑了局部信息。如下图所示，以二分图为例（将分为不相交的,两部分），若只考虑局部信息，那么分离出一个点显然是一个min-cut，因此图划分的结果便是类似或这样离群点，而从全局来看，实际想分成的组却是左右两大部分。针对这一情形，N-cut则提出了一种考虑全局信息的方法来进行图划分（Graph partitioning），即，将两个分割部分,与全图节点的连接权重（和）考虑进去：.如此一来，在离群点划分中，中的某一项会接近1，而这样的图划分显然不能使得是一个较小的值，故达到考虑全局信息而摒弃划分离群点的目的。这样的操作类似于机器学习中特征的规范化（Normalization）操作，故称为Normalized cut。N-cut不仅可以处理二类语义分割，而且将二分图扩展为路（-way）图划分即可完成多语义的图像语义分割，如下图例。Grab cut是微软剑桥研究院于2004年提出的著名交互式图像语义分割方法。与N-cut一样，grab cut同样也是基于图划分，不过grab cut是其改进版本，可以看作迭代式的语义分割算法。Grab cut利用了图像中的纹理（颜色）信息和边界（反差）信息，只要少量的用户交互操作即可得到比较好的前后背景分割结果。在Grab cut中，RGB图像的前景和背景分别用一个高斯混合模型（Gaussian mixture model, GMM）来建模。两个GMM分别用以刻画某像素属于前景或背景的概率，每个GMM高斯部件（Gaussian component）个数一般设为。接下来，利用吉布斯能量方程（Gibbs energy function）对整张图像进行全局刻画，而后迭代求取使得能量方程达到最优值的参数作为两个GMM的最优参数。GMM确定后，某像素属于前景或背景的概率就随之确定下来。在与用户交互的过程中，Grab cut提供两种交互方式：一种以包围框（Bounding box）为辅助信息；另一种以涂写的线条（Scribbled line）作为辅助信息。以下图为例，用户在开始时提供一个包围框，grab cut默认的认为框中像素中包含主要物体／前景，此后经过迭代图划分求解，即可返回扣出的前景结果，可以发现即使是对于背景稍微复杂一些的图像，grab cut仍有不俗表现。不过，在处理下图时，grab cut的分割效果则不能令人满意。此时，需要额外人为的提供更强的辅助信息：用红色线条／点标明背景区域，同时用白色线条标明前景区域。在此基础上，再次运行grab cut算法求取最优解即可得到较为满意的语义分割结果。Grab cut虽效果优良，但缺点也非常明显，一是仅能处理二类语义分割问题，二是需要人为干预而不能做到完全自动化。DL时代的语义分割其实大家不难看出，前DL时代的语义分割工作多是根据图像像素自身的低阶视觉信息（Low-level visual cues）来进行图像分割。由于这样的方法没有算法训练阶段，因此往往计算复杂度不高，但是在较困难的分割任务上（如果不提供人为的辅助信息），其分割效果并不能令人满意。在计算机视觉步入深度学习时代之后，语义分割同样也进入了全新的发展阶段，以全卷积神经网络（Fully convolutional networks，FCN）为代表的一系列基于卷积神经网络“训练”的语义分割方法相继提出，屡屡刷新图像语义分割精度。下面就介绍三种在DL时代语义分割领域的代表性做法。全卷积神经网络 [3]全卷积神经网络FCN可以说是深度学习在图像语义分割任务上的开创性工作，出自UC Berkeley的Trevor Darrell组，发表于计算机视觉领域顶级会议CVPR 2015，并荣获best paper honorable mention。FCN的思想很直观，即直接进行像素级别端到端（end-to-end）的语义分割，它可以基于主流的深度卷积神经网络模型（CNN）来实现。正所谓“全卷积神经网络”，在FCN中，传统的全连接层fc6和fc7均是由卷积层实现，而最后的fc8层则被替代为一个21通道（channel）的1x1卷积层，作为网络的最终输出。之所以有21个通道是因为PASCAL VOC的数据中包含21个类别（20个object类别和一个“background”类别）。下图为FCN的网络结构，若原图为，在经过若干堆叠的卷积和池化层操作后可以得到原图对应的响应张量（Activation tensor），其中，为第层的通道数。可以发现，由于池化层的下采样作用，使得响应张量的长和宽远小于原图的长和宽，这便给像素级别的直接训练带来问题。为了解决下采样带来的问题，FCN利用双线性插值将响应张亮的长宽上采样到原图大小，另外为了更好的预测图像中的细节部分，FCN还将网络中浅层的响应也考虑进来。具体来说，就是将Pool4和Pool3的响应也拿来，分别作为模型FCN-16s和FCN-8s的输出，与原来FCN-32s的输出结合在一起做最终的语义分割预测（如下图所示）。下图是不同层作为输出的语义分割结果，可以明显看出，由于池化层的下采样倍数的不同导致不同的语义分割精细程度。如FCN-32s，由于是FCN的最后一层卷积和池化的输出，该模型的下采样倍数最高，其对应的语义分割结果最为粗略；而FCN-8s则因下采样倍数较小可以取得较为精细的分割结果。Dilated Convolutions [4]FCN的一个不足之处在于，由于池化层的存在，响应张量的大小（长和宽）越来越小，但是FCN的设计初衷则需要和输入大小一致的输出，因此FCN做了上采样。但是上采样并不能将丢失的信息全部无损地找回来。对此，dilated convolution是一种很好的解决方案——既然池化的下采样操作会带来信息损失，那么就把池化层去掉。但是池化层去掉随之带来的是网络各层的感受野（Receptive field）变小，这样会降低整个模型的预测精度。Dilated convolution的主要贡献就是，如何在去掉池化下采样操作的同时，而不降低网络的感受野。以的卷积核为例，传统卷积核在做卷积操作时，是将卷积核与输入张量中“连续”的的patch逐点相乘再求和（如下图a，红色圆点为卷积核对应的输入“像素”，绿色为其在原输入中的感知野）。而dilated convolution中的卷积核则是将输入张量的patch隔一定的像素进行卷积运算。如下图b所示，在去掉一层池化层后，需要在去掉的池化层后将传统卷积层换做一个“dilation=2”的dilated convolution层，此时卷积核将输入张量每隔一个“像素”的位置作为输入patch进行卷积计算，可以发现这时对应到原输入的感知野已经扩大（dilate）为；同理，如果再去掉一个池化层，就要将其之后的卷积层换成“dilation=4”的dilated convolution层，如图c所示。这样一来，即使去掉池化层也能保证网络的感受野，从而确保图像语义分割的精度。从下面的几个图像语义分割效果图可以看出，在使用了dilated convolution这一技术后可以大幅提高语义类别的辨识度以及分割细节的精细度。以条件随机场为代表的后处理操作当下许多以深度学习为框架的图像语义分割工作都是用了条件随机场（Conditional random field，CRF）作为最后的后处理操作来对语义预测结果进行优化。一般来讲，CRF将图像中每个像素点所属的类别都看作一个变量，然后考虑任意两个变量之间的关系，建立一个完全图（如下图所示）。在全链接的CRF模型中，对应的能量函数为：其中是一元项，表示像素对应的语义类别，其类别可以由FCN或者其他语义分割模型的预测结果得到；而第二项为二元项，二元项可将像素之间的语义联系／关系考虑进去。例如，“天空”和“鸟”这样的像素在物理空间是相邻的概率，应该要比“天空”和“鱼”这样像素的相邻概率大。最后通过对CRF能量函数的优化求解，得到对FCN的图像语义预测结果进行优化，得到最终的语义分割结果。值得一提的是，已经有工作[5]将原本与深度模型训练割裂开的CRF过程嵌入到神经网络内部，即，将FCN+CRF的过程整合到一个端到端的系统中，这样做的好处是CRF最后预测结果的能量函数可以直接用来指导FCN模型参数的训练，而取得更好的图像语义分割结果。展望俗话说，“没有免费的午餐”（“No free lunch”）。基于深度学习的图像语义分割技术虽然可以取得相比传统方法突飞猛进的分割效果，但是其对数据标注的要求过高：不仅需要海量图像数据，同时这些图像还需提供精确到像素级别的标记信息（Semantic labels）。因此，越来越多的研究者开始将注意力转移到弱监督（Weakly-supervised）条件下的图像语义分割问题上。在这类问题中，图像仅需提供图像级别标注（如，有“人”，有“车”，无“电视”）而不需要昂贵的像素级别信息即可取得与现有方法可比的语义分割精度。另外，示例级别（Instance level）的图像语义分割问题也同样热门。该类问题不仅需要对不同语义物体进行图像分割，同时还要求对同一语义的不同个体进行分割（例如需要对图中出现的九把椅子的像素用不同颜色分别标示出来）。最后，基于视频的前景／物体分割（Video segmentation）也是今后计算机视觉语义分割领域的新热点之一，这一设定其实更加贴合自动驾驶系统的真实应用环境。（首发于机器之心公众号，链接请戳：专栏 | 从特斯拉到计算机视觉之「图像语义分割」）作者：魏秀参，谢晨伟References:[1] Jianbo Shi and Jitendra Malik. Normalized Cuts and Image Segmentation, IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 22, No. 8, 2000.[2] Carsten Rother, Vladimir Kolmogorov and Andrew Blake. \"GrabCut\"--Interactive Foreground Extraction using Iterated Graph Cuts, ACM Transactions on Graphics, 2004.[3] Jonathan Long, Evan Shelhamer and Trevor Darrell. Fully Convolutional Networks for Semantic Segmentation. IEEE Conference on Computer Vision and Pattern Recognition, 2015.[4] Fisher Yu and Vladlen Koltun. Multi-scale Context Aggregation by Dilated Convolutions. International Conference on Representation Learning, 2016.[5] Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang and Philip H. S. Torr. Conditional Random Fields as Recurrent Neural Networks. International Conference on Computer Vision, 2015."}
{"content2":"计算机视觉（CV）前沿国际国内期刊与会议这里的期刊大部分都可以通过上面的专家们的主页间接找到1.国际会议 2.国际期刊 3.国内期刊 4.神经网络 5.CV 6.数字图象 7.教育资源，大学 8.常见问题1. 国际会议现在，国际上计算机视觉方面的三大国际会议是ICCV， CVPR和ECCV，统称之为ICE。ICCV的全称是International Comference on Computer Vision。ICCV两年一次，与ECCV正好错开，是公认的三个会议中级别最高的。ECCV的全称是Europeon Conference on Computer Vision，是一个欧洲的会议。CVPR的全称是Internaltional Conference on Computer Vision and Pattern Recogintion国际计算机视觉与模式识别会议。这是一个一年一次的会议，举办地在美国。ICIP—BMVC—MVA—国际模式识别会议(ICPR )：亚洲计算机视觉会议(ACCV)：2.国际期刊以计算机视觉为主要内容之一的国际刊物也有很多，如:International Journal of Computer VisionIEEE Trans. On PAMI http://www.computer.org/tpami/index.htmIEEE Transactionson Image Processing http://www.ieee.org/organizations/pubs/transactions/tip.htmPattern Recognition http://www.elsevier.com/locate/issn/00313203Pattern Recognition Letters http://www.elsevier.com/locate/issn/01678655IEEE Trans. on Robotics and Automation，IEEE TPAMIIEEE TIPCVGIP Computer Vision. Graphics and Image Processing，Visual Image Computing，IJPRAI(Internatiorial Journat of Pattern Recognition and Artificial Intelligence)众 所周知， computer vision(cv) 存在ICCV/CVPR/ECCV三个顶级会议， 它们档次差不多，都应该在一流会议行列， 没有必要给个高下。 有些us的人认为ICCV/CVPR略好于ECCV，而欧洲人大都认为ICCV/ECCV略好于CVPR， 某些英国的人甚至认为BMVC好于CVPR。简言之， 三个会议差不多， 各有侧重和偏好。笔者就个人经验浅谈三会异同， 以供大家参考和讨论。 三者乃cv领域的旗舰和风向标，其oral paper (包括best paper) 代表当年度cv的最高水准， 在此引用Harry Shum的一句话， 想知道某个领域在做些什么， 找最近几年此领域的proceeding看看就知道了。 ICCV/CVPR由IEEE Computer Society牵头组织， ECCV好像没有专门负责的组织。 CVPR每年(除2002年)都在美国开， ECCV每两年开一次，仅限欧洲， ICCV也是每两年一次， 各洲轮值。 基本可以保证每年有两个会议开， 这样研究者就有两次跻身牛会的机会。就录取率而言， 三会都有波动。 如ICCV2001录取率>30%， 且出现两个人(华人)各有三篇第一作者的paper的情况， 这在顶级牛会是不常见的 (灌水嫌疑)。 但是， ICCV2003， 2005两次录取率都很低， 大约20%左右。 ECCV也是类似规律， 在2004年以前都是>30%， 2006年降低到20%左右。 CVPR的录取率近年来一直偏高， 从2004年开始一直都在[25%，30%]。最近一次CVPR2006是28.1%， CVPR2007还不知道统计数据。 笔者猜测为了维持录取paper的绝对数量， 当submission少的时候录取率偏高， 反之偏低， 近几年三大会议的投稿数量全部超过1000， 相对2000年前， 三会录取率均大幅度降低， 最大幅度50%->20%。 对录取率走势感兴趣的朋友， 可参考 http://vrlab.epfl.ch/~ulicny/statistics/(CVPR2004的数据是错的)，http://www.adaptivebox.net/research/bookmark/CICON_stat.html.显 然， 投入cv的人越来越多，这个领域也是越来越大， 这点颇不似machine learning一直奉行愚蠢的小圈子主义。另外一点值得注意， ICCV/ECCV只收vision相关的topic， 而cvpr会收少量的pattern recognition paper， 如finger print等， 但是不收和image/video完全不占边的pr paper，如speech recognition等。 我一个朋友曾经review过一篇投往CVPR的speech的paper， 三个reviewer一致拒绝， 其中一个reviewer搞笑的指出， 你这篇paper应该是投ICASSP被据而转投CVPR的。 就topic而言， CVPR涵盖最广。 还有一个没有验证过的原因导致CVPR录取率高: 很多us的researcher不愿意或没有足够的经费到us以外的地方开会， 故CVPR会优先接收很多来自us的paper (让大家都happy)。以上对三会的分析对我们投paper是很有指导作用的。 目前的research我想绝大部分还是纸上谈兵， 必经 read paper -> write paper -> publish paper -> publish paper on top conferences and journals流程。 故了解投paper的一些基本技巧， 掌握领域的走向和热点， 是非常必要的。 避免做无用功，选择切合的topic， 改善presentation， 注意格式 (遵守规定的模板)， 我想这是很多新手需要注意的问题。 如ICCV2007明文规定不写summary page直接reject， 但是仍然有人忽视， 这是相当不值得的。3.国内期刊自动化学报、计算机学报、软件学报、电子学报，中国图象图形学报，模式识别与人工智能，光电子激光，精密光学工程等。4.神经网络神经网络-Neural Networks Tutorial Reviewhttp://hem.hj.se/~de96klda/NeuralNetworks.htmftp://ftp.sas.com/pub/neural/FAQ.htmlImage Compression with Neural Networkshttp://www.comp.glam.ac.uk/digimaging/neural.htmBackpropagator's Reviewhttp://www.dontveter.com/bpr/bpr.htmlBibliographies on Neural Networkshttp://liinwww.ira.uka.de/bibliography/Neural/Intelligent Motion Control with an Artificial Cerebellumhttp://www.q12.org/phd.htmlKernel Machineshttp://www.kernel-machines.org/Some Neural Networks Research Organizationshttp://www.ieee.org/nnc/http://www.inns.org/Neural Network Modeling in Vision Researchhttp://www.rybak-et-al.net/nisms.htmlNeural Networks and Machine Learninghttp://learning.cs.toronto.edu/Neural Application Softwarehttp://attrasoft.comNeural Network Toolbox for MATLABhttp://www.mathworks.com/products/neuralnet/Netlab Softwarehttp://www.ncrg.aston.ac.uk/netlab/Kunama Systems Limited http://www.kunama.co.uk/5.Computer Vision(计算机视觉)Annotated Computer Vision Bibliographyhttp://iris.usc.edu/Vision-Notes/bibliography/contents.htmlhttp://iris.usc.edu/Vision-Notes/rosenfeld/contents.htmlLawrence Berkeley National Lab Computer Vision and Robotics Applicationshttp://www-itg.lbl.gov/ITG.hm.pg.docs/VISIon/vision.htmlCVonline by University of EdinburghThe Evolving， Distributed， Non-Proprietary， On-Line Compendium of Computer Vision， www.dai.ed.ac.uk/CVonlineComputer Vision Handbook，www.cs.hmc.edu/~fleck/computer-vision-handbookVision Systems Coursewarewww.cs.cf.ac.uk/Dave/Vision_lecture/Vision_lecture_caller.htmlResearch Activities in Computer Visionhttp://www-syntim.inria.fr/syntim/analyse/index-eng.htmlVision Systems Acronymswww.vision-systems-design.com/vsd/archive/acronyms.htmlDictionary of Terms in Human and Animal Visionhttp://cns-web.bu.edu/pub/laliden/WWW/Visionary/Visionary.htmlMetrology based on Computer Visionwww.cranfield.ac.uk/sme/amac/research/metrology/metrology.html6.Digital Photography 数字图像Digital Photography， Scanning， and Image Processingwww.dbusch.com/scanners/scanners.htm l7.Educational Resources， Universities 教育资源，大学Center for Image Processing in Educationwww.cipe.comLibrary of Congress Call Numbers Related to Imaging Science by Rochester Institute of Technologyhttp://wally2.rit.edu/pubs/guides/imagingcall.htmlMathematical Experiences through Image Processing， University of Washingtonwww.cs.washington.edu/research/metip/metip.htmlVismod Tech Reports and Publications， MIThttp://vismod.www.media.mit.edu/cgi-bin/tr_pagemakerVision Lab PhD dissertation list， University of Antwerphttp://wcc.ruca.ua.ac.be/~visielab/theses.htmlINRIA (France) Research Projects: Human-Computer Interaction， Image Processing， Data Management， Knowledge Systemswww.inria.fr/Themes/Theme3-eng.htmlImage Processing Resourceshttp://eleceng.ukc.ac.uk/~rls3/Contents.htmPublications of Carsten Stegerhttp://www9.informatik.tu-muench ... r/publications.html8.FAQs（常见问题）comp.dsp FAQwww.bdti.com/faq/dsp_faq.htmRobotics FAQwww.frc.ri.cmu.edu/robotics-faqWhere's the sci.image.processing FAQ?www.cc.iastate.edu/olc_answers/p ... processing.faq.htmlcomp.graphics.algorithms FAQ， Section 3， 2D Image/Pixel Computationswww.exaflop.org/docs/cgafaqAstronomical Image Processing System FAQwww.cv.nrao.edu/aips/aips_faq.html四、搜索资源http://sal.kachinatech.com/http://cheminfo.pku.edu.cn/mirrors/SAL/index.shtml 北京大学Google输入：computer vision 或computer vision groups可以获得很多结果网络资源：CVonline http://homepages.inf.ed.ac.uk/rbf/CVonline/视觉研究组列表Computer vision test Image http://www.cs.cmu.edu/~cil/v-images.html卡内基梅隆标准图片库视觉论文搜索：Paper searchhttp://www.researchindex.com五、图像处理GPL库（代码库图像库等）http://www.ph.tn.tudelft.nl/~klamer/cppima.htmlCppima 是一个图像处理的C++函数库。这里有一个较全面介绍它的库函数的文档，当然你也可以下载压缩的GZIP包，里面包含TexInfo格式的文档。http://iraf.noao.edu/Welcome to the IRAF Homepage! IRAF is the Image Reduction and Analysis Facility， a general purpose software system for the reduction and analysis of astronomical datahttp://entropy.brni-jhu.org/tnimage.html一个非常不错的Unix系统的图像处理工具，看看它的截图。你可以在此基础上构建自己的专用图像处理工具包。http://sourceforge.net/projects/这是GPL软件集散地，可以搜索IP库。国内的CSDN http://www.csdn.net/"}
{"content2":"今日CS.CV计算机视觉论文速览Mon, 11 Mar 2019Totally 35 papersInteresting:📚Three-Player GAN,在通常GAN的基础上增加了生成器和分类器间的竞争。利用C来合成更为困难的样本，随后这些样本将提高分类器的能力。（from ESAT-PSI）当分类器加入时，生成的数据分布改变了不再是real/fake，而是更难分辨的中间数据：📚, 基于分级的方法来实现弱监督语义分割，加快语义分割的速度。(from Eindhoven University of Technology)基础分类器先分类，而后将相关车辆行人的像素交给子分类器，右图是相关数据集和模型表现。📚3DN,三维的可变形网络，实现了三维模型的风格迁移。（from USC）其损失包含了以下部分：mesh的两项为形状损失，包含了CD（chamfer ）和EMD（earth mover）两项，来确定变型后的模型与目标模型的外形。point的两项用于保持对称性，所以要通过点云来比较。为了避免自交叉引入了局域变异不变性损失，保持源形状的局域几何特性拉普拉斯损失。code ：github.com/laughtervv/3DN📚FastDepth,用于嵌入式设备的快速单目深度估计,利用了depthwise separable的解码器和剪枝算法NetAdapt（from MIT）用于嵌入式的网络模型架构：在TX2上的精度速度：项目主页： http://fastdepth.mit.edu/数据集：NYC Depth Dataset V2code:https://github.com/dwofk/fast-depth📚MLOSR：Open-Set未知类别识别，通过将自动编码器和分类结合起来可以有效通过多任务提高open-set的表现。（From Johns Hopkins University）相关数据集和方法：code:github.com/otkupjnoz/mlosr📚HOPS-Net,通过RGB图估计手持物体的位姿。通过引入手部的信息，来得到更精确的结果。模型在大型合成数据集上进行了训练，并使用了图像迁移来将虚拟训练转移到真实数据上来。(from KTH)网络的流程框架如下，结合了分割和位姿估计：基于模拟环境GraspIt! 合成数据。📚SVST, 一种高效的视频场景文字识别，包括检查、跟踪、分析和识别几个过程。（from 浙江大学）视频文字检测和视频流打分联合学习：dataset: IC13 [19] and IC15 [18]📚CLEVR-Dialog, 用于多轮对话推理对话数据集。(from CMU FB GIT)与MNIST Dialog， VisDial数据集的比较：📚基于WNet GAN 优化遥感数字表面建模，（from German Aerospace Center）模型架构和一些结果：ref:城市数字模型：https://www.businesslocationcenter.de/downloadportal/📚ICDAR2019中国家谱的历史文献识别******ref:http://icdar2019.org/competitions-2/📚基于分形随机场的海洋石油泄漏检测，基于长程依赖性的随机场和小波滤波器（from Universidad de Buenos Aires）📚植物根茎检测，(from 弗罗里达大学)参考方法：MI-ACE, miSVM, MIForests📚自动化近地小行星检测系统综述，（from Technical University of Cluj-Napoca）Daily Computer Vision Papers[1] *Title: Geometry-Aware Graph Transforms for Light Field Compact RepresentationAuthors:Mira Rizkallah, Xin Su, Thomas Maugey, Christine Guillemot[2] *Title: Prediction and Sampling with Local Graph Transforms for Quasi-Lossless Light Field CompressionAuthors:Mira Rizkallah, Thomas Maugey, Christine Guillemot[3] *Title: Unsupervised Learning of Probabilistic Diffeomorphic Registration for Images and SurfacesAuthors:Adrian V. Dalca, Guha Balakrishnan, John Guttag, Mert R. Sabuncu[4] **Title: DSM Building Shape Refinement from Combined Remote Sensing Images based on Wnet-cGANsAuthors:Ksenia Bittner, Marco Körner, Peter Reinartz[5] *Title: OpenCL-based FPGA accelerator for disparity map generation with stereoscopic event camerasAuthors:David Castells-Rufas, Jordi Carrabina[6] Title: Unsupervised Data Imputation via Variational Inference of Deep SubspacesAuthors:Adrian V. Dalca, John Guttag, Mert R. Sabuncu[7] **Title: A Three-Player GAN: Generating Hard Samples To Improve Classification NetworksAuthors:Simon Vandenhende, Bert De Brabandere, Davy Neven, Luc Van Gool[8] *Title: Auto-Encoding Progressive Generative Adversarial Networks For 3D Multi Object ScenesAuthors:Vedant Singh, Manan Oza, Himanshu Vaghela, Pratik Kanani[9] *Title: On Boosting Semantic Street Scene Segmentation with Weak SupervisionAuthors:Panagiotis Meletis, Gijs Dubbelman[10] Title: Joint Learning of Brain Lesion and Anatomy Segmentation from Heterogeneous DatasetsAuthors:Nicolas Roulet, Diego Fernandez Slezak, Enzo Ferrante[11] **Title: Unsupervised Medical Image Translation Using Cycle-MedGANAuthors:Karim Armanious, Chenming Jiang, Sherif Abdulatif, Thomas Küstner, Sergios Gatidis, Bin Yang[12] Title: Semantically Tied Paired Cycle Consistency for Zero-Shot Sketch-based Image RetrievalAuthors:Anjan Dutta, Zeynep Akata[13] **Title: ICDAR 2019 Historical Document Reading Challenge on Large Structured Chinese Family RecordsAuthors:Foteini Simistira Liwicki, Rajkumar Saini, Derek Dobson, Jon Morrey, Marcus Liwicki[14] **Title: Learning to Estimate Pose and Shape of Hand-Held Objects from RGB ImagesAuthors:Mia Kokic, Danica Kragic, Jeannette Bohg[15] Title: Complex Valued Gated Auto-encoder for Video Frame PredictionAuthors:Niloofar Azizi, Nils Wandel, Sven Behnke[16] Title: Knowledge-Embedded Routing Network for Scene Graph GenerationAuthors:Tianshui Chen, Weihao Yu, Riquan Chen, Liang Lin[17] **Title: 3DN: 3D Deformation NetworkAuthors:Weiyue Wang, Duygu Ceylan, Radomir Mech, Ulrich Neumann[18] Title: Semi- and Weakly Supervised Directional Bootstrapping Model for Automated Skin Lesion SegmentationAuthors:Yutong Xie, Jianpeng Zhang, Yong Xia, Chunhua Shen[19] Title: Learning from Synthetic Data for Crowd Counting in the WildAuthors:Qi Wang, Junyu Gao, Wei Lin, Yuan Yuan[20] **Title: Efficient Video Scene Text Spotting: Unifying Detection, Tracking, and RecognitionAuthors:Zhanzhan Cheng, Jing Lu, Jianwen Xie, Yi Niu, Shiliang Pu, Fei Wu[21] Title: Learning Regularity in Skeleton Trajectories for Anomaly Detection in VideosAuthors:Romero Morais, Vuong Le, Truyen Tran, Budhaditya Saha, Moussa Mansour, Svetha Venkatesh[22] *Title: FastDepth: Fast Monocular Depth Estimation on Embedded SystemsAuthors:Diana Wofk, Fangchang Ma, Tien-Ju Yang, Sertac Karaman, Vivienne Sze[23] Title: Ranked List Loss for Deep Metric LearningAuthors:Xinshao Wang, Yang Hua, Elyor Kodirov, Guosheng Hu, Romain Garnier, Neil M. Robertson[24] *Title: Pattern Recognition in SAR Images using Fractional Random Fields and its Possible Application to the Problem of the Detection of Oil Spills in Open SeaAuthors:Agustín Mailing, Segundo A. Molina, José L. Hamkalo, Fernando R. Dobarro, Juan M. Medina, Bruno Cernuschi-Frías, Daniel A. Fernández, Érica Schlaps[25] Title: Unsupervised Domain Adaptation using Feature-Whitening and Consensus LossAuthors:Subhankar Roy, Aliaksandr Siarohin, Enver Sangineto, Samuel Rota Bulo, Nicu Sebe, Elisa Ricci[26] *Title: Root Identification in Minirhizotron Imagery with Multiple Instance LearningAuthors:Guohao Yu, Alina Zare, Hudanyun Sheng, Roser Matamala, Joel Reyes-Cabrera, Felix B. Frischi, Thomas E. Juenger[27] Title: Fast Video Retargeting Based on Seam Carving with Parental LabelingAuthors:Zhu Chuning[28] **Title: CLEVR-Dialog: A Diagnostic Dataset for Multi-Round Reasoning in Visual DialogAuthors:Satwik Kottur, José M. F. Moura, Devi Parikh, Dhruv Batra, Marcus Rohrbach[29] **Title: Deep CNN-based Multi-task Learning for Open-Set RecognitionAuthors:Poojan Oza, Vishal M. Patel[30] *Title: Anatomical Priors in Convolutional Networks for Unsupervised Biomedical SegmentationAuthors:Adrian V. Dalca, John Guttag, Mert R. Sabuncu[31] Title: A Learnable ScatterNet: Locally Invariant Convolutional LayersAuthors:Fergal Cotter, Nick Kingsbury[32] Title: Stable Backward Diffusion Models that Minimise Convex EnergiesAuthors:Leif Bergerhoff, Marcelo Cárdenas, Joachim Weickert, Martin Welk[33] **Title: NEARBY Platform for Automatic Asteroids Detection and EURONEAR SurveysAuthors:Dorian Gorgan, Ovidiu Vaduvescu, Teodor Stefanut, Victor Bacu, Adrian Sabou, Denisa Copandean Balazs, Constantin Nandra, Costin Boldea, Afrodita Boldea, Marian Predatu, Viktoria Pinter, Adrian Stanica[34] Title: Research on the pixel-based and object-oriented methods of urban feature extraction with GF-2 remote-sensing imagesAuthors:Dong-dong Zhang, Lei Zhang, Vladimir Zaborovsky, Feng Xie, Yan-wen Wu, Ting-ting Lu[35] Title: Computer aided detection of tuberculosis on chest radiographs: An evaluation of the CAD4TB v6 systemAuthors:Keelin Murphy, Shifa Salman Habib, Syed Mohammad Asad Zaidi, Saira Khowaja, Aamir Khan, Jaime Melendez, Ernst T. Scholten, Farhan Amad, Steven Schalekamp, Maurits Verhagen, Rick H. H. M. Philipsen, Annet Meijers, Bram van GinnekenPapers from arxiv.org更多精彩请移步主页pic from pixels.com"}
{"content2":"在《零基础小白，如何入门计算机视觉？》中我提到过，计算机视觉的研究目前主要分为两大方向：基于学习的方法和基于几何的方法。其中基于学习的方法最火的就是深度学习，而基于几何方法最火的就是视觉SLAM。SLAM将成为计算机视觉的下一个风口在前几年计算机视觉的三大顶级会议（CVPR,ICCV,ECCV）上，几乎全是深度学习的研究，而这样的情况在这两年出现了新的变化：在2018年计算机视觉国际顶级会议 CVPR论文录用名单中，其中涉及SLAM/三维视觉 的工作超过 90 篇，占据了全体收录论文的近 1/10。而今年参加校招和社招的小伙伴也向我透露，今年深度学习方向的竞争非常激烈，想要拿到较好的offer，要么有牛叉闪闪的论文，要么是大牛实验室有过硬的项目经验，难度比前几年大大提升。而今年SLAM方向的需求量上升而相关从业者相对较少，相对还比较容易，类似几年前深度学习刚刚火起来之前的样子。可以预见，SLAM将成为继深度学习之后计算机视觉领域的下一个风口。主要有以下几个依据：1、深度学习在检测、识别领域具有无比强大的能力，但是在涉及多视角几何相关的SLAM领域，深度学习的作用非常有限。究其原因是因为在多视角几何为基础的SLAM领域，需要明确清晰的理论基础保证，而深度学习的「黑盒子」模型目前还不太奏效。2、SLAM技术门槛较高。深度学习爆发后，很多非计算机视觉领域的从业者纷纷转而学习深度学习，由于深度学习本身黑盒子的特点，很多从业者不需要了解图像处理、计算机视觉的基础知识便可以得到一个相对较好的结果，因此入门门槛并不是很高。而学习SLAM则需要具备三维空间刚体变换、相机成像模型、特征点提取与匹配、多视角几何、捆集调整等内容。这对于非该领域的从业者来说还是具有较高的门槛。3、消费级RGB-D相机快速发展催生了以三维视觉为基础的商业化应用。以微软Kinect系列、Intel realsense系列、苹果、英飞凌、TI等为代表的消费级RGB-D相机逐渐形成成熟的产业链，国内也涌现出大量的优秀企业并量产，如orbbec、pico、human+、爱观、图漾、艾芯智能、知微传感等。此外，2017年iPhone X前置结构光深度相机面世后，更是激发了手机产业链RGB-D相机的热潮，目前华为、小米、OPPO、VIVO等手机大厂都在积极推动RGB-D相机在手机上的应用。4、目前计算机视觉领域主要还是通过二维的图片来感知世界，而三维视觉才是人类感知理解世界的正确方式，因此以三维视觉为基础的SLAM技术是智能移动机器人、无人驾驶、AR等人工智能细分领域的核心技术。目前对SLAM技术需求强烈的公司包括：互联网公司如百度、腾讯、阿里、京东等，计算机视觉算法公司如旷世、虹软、商汤等，自动驾驶创业公司如图森、momenta、景驰、驭势、滴滴及各大汽车厂商等，无人机/机器人公司如大疆、思岚、高仙等，AR移动终端应用相关公司如三星、华为、悉见等。总之，SLAM前景光明但学习道路曲折，这也是笔者打算和读者一起从零开始学习SLAM的初衷。从零开始一起学习SLAM系列规划目前关于SLAM学习的资料不多，而且参差不齐，初学者推荐高翔博士的《视觉SLAM十四讲》，虽然本书写的已经比较基础，但很多小伙伴在学习期间仍然会遇到很多问题，因此，笔者《从零开始一起学习SLAM》系列文章规划如下：1、技术介绍全面，不枯燥。该系列从最基础的知识开始介绍，分为多篇文章，每篇文章只介绍一个具体的知识点，尽量以形象生动的图文辅以适当的推导，一点点深挖SLAM的各个重要技术点。2、每篇文章都有习题，重视实践。笔者会根据每篇文章内容设计一些实用性的练习题（推导、编程等），俗话说，光看不做假把式，适当的练习能够加深读者的理解，把知识消化吸收为自己所用。3、高质量的交流学习社区。每篇文章练习题参考答案笔者会放到知识星球「从零开始一起学习SLAM」里。星球内所有成员都可以进行发布问题、分享知识、上传资源、点赞、留言、赞赏、收藏等操作。而所有的交流讨论、资源分享等都可以沉淀下来并方便日后查询。星主还会额外布置作业，和大家一起学习讨论。知识星球需付费加入，越早加入价格越优惠。星球还会红包奖励积极分享、解答问题的成员，只要花费一顿聚餐的钱，就能够和一群优秀的SLAM从业者一起交流进步，甚至解决就业问题。关注公众号“计算机视觉life”，点击菜单栏 “知识星球”，了解《从零开始学习SLAM》介绍，加入一起学习吧~作业如果我们想要了解一个领域，最好的方法就是先看该领域比较著名的综述论文，先从宏观上把握该领域的整体面貌。1、请列举几篇最近几年SLAM领域经典的综述论文。2、阅读综述，并列举至少三个SLAM的具体应用场景。欢迎留言讨论，或者进入知识星球「从零开始学习SLAM」一起学习交流~相关阅读《零基础小白，如何入门计算机视觉？》"}
{"content2":"今日CS.CV 计算机视觉论文速览Wed, 12 Jun 2019Totally 52 papers👉上期速览✈更多精彩请移步主页Interesting:📚Shapes and Context, 研究人员提出了一种从语义标签图合成图像以及操作图像内容的方法，具有丰富的适应性、可以合成十分高分辨的图像，这些图像具有合适的外形和视觉结果，可以通过这种方法合成丰富的图像资源。(from CMU)输入语义图像，输出合成的彩色图像：对于输入的语义图，研究人员提出了非参数的匹配方法来处理全局、外形、部分甚至像素的细节，以便合成出新的图像：非参数匹配的过程主要分为四个步骤，首先利用知识矢量来从数据集中找到相关样本，随后利用形状连续性并基于形状和内容特征来寻找到最适合的掩膜，接着利用部分连续性和局域合成方法来补全细节的信息，最后在像素水平对图形进行处理：一些合成的结果：统一输入多个合成的输出：图像插入元素的操作结果：📚三维场景中CAD模型检索与9DoF的匹配, 研究人员提出了一种对扫描场景中的物体进行6D位姿检测，并利用检测结果与对应的CAD模型进行匹配和对齐（symmetry-awareobject correspondences ，SOCs），随后将生成有效的CAD重建结果，包含干净的、完整的物体几何模型。(from 慕尼黑工大)用于CAD模型匹配的端到端模型：得到的一些结果，其中扫描数据来自，家具的CAD模型来自shapent core:一些相关方法的比较：数据主要来自于扫描数据的TSDF编码，encoded in a volumetric grid and generated through volumetric fusion [5]场景数据来自Scan2CAD annotations provide 1506 scenes for training. SUNCG.using the level-set generation toolkit by Batty [2]生成CAD模型的表示📚提出了一种新的三维表示方法clouds of oriented gradient ，COG, 可以精确的描述透视投影的角度如何影响成像图像的梯度。为了更好地表示大尺度的三维物体以及对于小物体的检测，研究人员引入了隐支持表面。最后提出的曼哈顿体素方法来更好的捕捉房间的三维几何布局信息。最后利用了多级分类器来捕捉内容上的关系，在SUN RGB-D数据集上实现了很好的结果(from 佐治亚理工 )从输入的图像和深度图中首先对齐包含物体的立方体并转换到惯用的坐标系下，随后从中抽取出点云密度特征、3D法向量直方图和COG 描述子。并将点云密度和体素密度匹配起来。对于床和床上用品的检测结果：📚FAMED-Net高速高精度的多尺读去雾方法, 图像去雾方法目前受制于模型复杂、计算效率和表达能力，为了解决这些问题，研究人员尝试使用三个不同尺度的编码器和融合模块构建去雾算法。每一个编码器由级联和稠密连接的逐点卷积层和池化层相连(类似shufflenet)。由于特征的复用和没有大型卷积操作使得这一模型十分轻量和高效。(from 悉尼大学 UBTECH)网络有多个point-wise的卷积层和池化层的dense链接构成，高斯金字塔和拉普拉斯金字塔结构的编码器和融合模型：模型的一些结果：真实图像的结果：与相关结果的比较：code:https://github.com/chaimi2013/FAMED-Net 作者即将放出dataset:ITS and OTS,< RESIDE and TestSet-SDaily Computer Vision Papers****Shapes and Context: In-the-Wild Image Synthesis & ManipulationAuthors Aayush Bansal, Yaser Sheikh, Deva Ramanan我们引入了一种数据驱动方法，用于交互式地合成来自语义标签图的野外图像。我们的方法与此领域的近期工作截然不同，因为我们不使用任何学习方法。相反，我们的方法使用简单但经典的工具将场景上下文，形状和部件与存储的样本库进行匹配。虽然简单，但这种方法比近期工作1有几个明显的优势，因为没有学到任何东西，它不仅限于特定的训练数据分布，如城市景观，立面或面部2，它可以合成任意高分辨率的图像，仅受到分辨率的限制。通过适当地组成形状和部分，示例库3，它可以生成指数大的可行候选输出图像集，可以说是由用户交互式搜索。我们在不同的COCO数据集上展示结果，在标准图像合成指标上显着优于基于学习的方法。最后，我们探索用户交互和用户可控性，证明我们的系统可以用作用户驱动的内容创建的平台。**Clouds of Oriented Gradients for 3D Detection of Objects, Surfaces, and Indoor Scene LayoutsAuthors Zhile Ren, Erik B. Sudderth我们开发了新的表示和算法，用于在杂乱的室内场景中进行三维三维物体检测和空间布局预测。我们首先提出了一个定向梯度COG描述符云，它将对象类别的2D外观和3D姿态联系起来，从而准确地模拟透视投影如何影响感知的图像渐变。为了更好地表示大型物体的3D视觉样式并提供上下文提示以改善小物体的检测，我们引入了潜在的支撑表面。然后，我们提出了曼哈顿体素表示，它更好地捕捉了常见室内环境的3D房间布局几何形状。通过潜在的结构化预测框架来学习有效的分类规则。通过级联分类器捕获类别和布局之间的上下文关系，从而导致超出SUN RGB D数据库的现有技术水平的整体场景假设。**3-D Surface Segmentation Meets Conditional Random FieldsAuthors Leixin Zhou, Zisha Zhong, Abhay Shah, Xiaodong Wu在许多医学图像分析应用中，自动表面分割是重要且具有挑战性的。已经为各种对象分割任务开发了最近的基于深度学习的方法。它们中的大多数是基于分类的方法，例如， U net，它预测每个体素成为目标对象或背景的概率。这些方法的一个问题是缺乏对分割对象的拓扑保证，并且通常需要后处理来推断对象的边界表面。本文提出了一种基于三维卷积神经网络CNN和条件随机场CRF的新型模型，用于解决端到端训练的表面分割问题。据我们所知，这是第一个将3D神经网络与CRF模型应用于直接表面分割的研究。在NCI ISBI 2013 MR前列腺数据集和医学分割十项全能脾脏数据集上进行的实验证明了非常有前景的分割结果。Rethinking Person Re-Identification with ConfidenceAuthors George Adaimi, Sven Kreiss, Alexandre Alahi人体识别系统的一个共同挑战是区分具有非常相似外观的人。目前基于交叉熵最小化的学习框架不适合这一挑战。为了解决这个问题，我们建议使用三种方法标记平滑，置信度惩罚和深度变分信息瓶颈来修改表示学习框架中的交叉熵损失和模型置信度。我们的方法的一个关键属性是我们不使用任何手工制作的人类特征，而是将注意力集中在学习监督上。虽然建模置信度的方法没有显示出对象分类等其他计算机视觉任务的显着改进，但我们能够显示其在3个公开可用数据集上重新识别超出最新技术方法的任务的显着影响。我们的分析和实验不仅提供了人们所面临的问题的见解，而且还提供了一个简单而直接的方法来解决这个问题。Gated CRF Loss for Weakly Supervised Semantic Image SegmentationAuthors Anton Obukhov, Stamatios Georgoulis, Dengxin Dai, Luc Van Gool用于语义分割的现有技术方法依赖于在完全注释的数据集上训练的深度卷积神经网络，已经证明在时间和金钱方面收集都是非常昂贵的。为了弥补这种情况，弱监督方法利用需要少得多的注释努力的其他形式的监督，但是由于这些区域中的监督信号的近似性质，它们通常表现出无法预测精确的对象边界。虽然在提高性能方面取得了很大进展，但许多弱监督方法都是根据自己的具体情况量身定制的。这在重用算法和稳步前进方面提出了挑战。在本文中，我们在处理弱监督语义分割时故意避免这种做法。特别是，我们为标记像素训练具有部分交叉熵损失函数的标准神经网络，并为未标记像素训练我们提出的门控CRF损失。门控CRF损失旨在提供几个重要的资产1它使内核构造具有灵活性，以掩盖不受欢迎的像素位置的影响2它将学习上下文关系卸载到CNN并集中于语义边界3它不依赖于高维过滤和因此具有简单的实现。在整篇论文中，我们介绍了损失函数的优点，分析了弱监督训练的几个方面，并表明我们的纯粹方法实现了基于点击和基于涂鸦的注释的最新技术性能。**Scale Invariant Fully Convolutional Network: Detecting Hands EfficientlyAuthors Dan Liu, Dawei Du, Libo Zhang, Tiejian Luo, Yanjun Wu, Feiyue Huang, Siwei Lyu现有的手检测方法通常遵循具有高计算成本的多级流水线，即特征提取，区域建议，边界框回归和用于旋转区域检测的附加层。在本文中，我们提出了一种新的尺度不变全卷积网络SIFCN，它以端到端的方式进行训练，以有效地检测手部。具体来说，我们以迭代的方式合并从高层到低层的特征映射，与简单地连接它们相比，它可以更好地处理不同规模的手，而且时间开销更少。此外，我们开发了互补加权融合CWF模块，以充分利用多层之间的独特特征来实现尺度不变性。为了处理旋转手部检测，我们提出了旋转图来摆脱复杂的旋转和反旋转层。此外，我们设计了多尺度损失方案，通过增加对网络中间层的监督来显着加速训练过程。与现有技术方法相比，我们的算法具有可比较的精度，在VIVA数据集上的运行速度提高了4.23倍，并以62.5 fps的速度在牛津手检测数据集上实现了更好的平均精度。On Stabilizing Generative Adversarial Training with NoiseAuthors Simon Jenni, Paolo Favaro我们提出了一种新的方法和分析，以稳定的方式训练生成对抗网络GAN。如最近的分析所示，训练通常受到数据空间上邻域数据的概率分布的破坏。我们注意到，即使它们经历相同的过滤，实际数据和生成数据的分布也应该匹配。因此，为了解决有限的支持问题，我们建议通过使用实际和生成的数据分布的不同过滤版本来训练GAN。通过这种方式，过滤不会阻止数据分布的精确匹配，同时通过扩展两个分布的支持来帮助进行培训。作为过滤，我们考虑将来自任意分布的样本添加到数据中，这对应于数据分布与任意分布的卷积。我们还建议学习这些样本的生成，以便在对抗训练中挑战鉴别者。我们表明，即使是最初的minimax GAN配方，我们的方法也能实现稳定且良好的训练。此外，我们的技术可以结合到大多数现代GAN配方中，并导致对几个常见数据集的持续改进。Mimic and Fool: A Task Agnostic Adversarial AttackAuthors Akshay Chaturvedi, Utpal Garain目前，对抗性攻击是以任务特定的方式设计的。然而，对于下游计算机视觉任务，例如图像字幕，图像分割等，当前的深度学习系统使用诸如VGG16，ResNet50，Inception v3等的图像分类器作为特征提取器。牢记这一点，我们提出了Mimic和Fool，一种与任务无关的对抗性攻击。给定特征提取器，所提出的攻击找到可以模仿原始图像的图像特征的对抗图像。这确保了两个图像无论任务如何都给出相同或相似的输出。我们随机选择1000个MSCOCO验证图像进行实验。我们对两个图像字幕模型，Show和Tell，Show Attend和Tell以及一个VQA模型，即端到端神经模块网络N2NMN进行实验。对于Show and Tell，Show Attend和Tell以及N2NMN，提议的攻击成功率分别为74.0,81.0和89.6。我们还建议对我们的攻击稍作修改，以生成看起来自然的对抗图像。此外，它表明所提出的攻击也适用于可逆架构。由于Mimic和Fool只需要有关模型特征提取器的信息，因此可以将其视为灰盒攻击。Joint Subspace Recovery and Enhanced Locality Driven Robust Flexible Discriminative Dictionary LearningAuthors Zhao Zhang, Jiahuan Ren, Weiming Jiang, Zheng Zhang, Richang Hong, Shuicheng Yan, Meng Wang我们提出了一种联合子空间恢复和基于增强局部性的鲁棒灵活标签一致字典学习方法，称为鲁棒灵活判别字典学习RFDDL。 RFDDL主要通过增强稀疏误差的鲁棒性和更准确地编码局部性，重建误差和标签一致性来改进数据表示和分类能力。首先，对于数据和原子中噪声和稀疏误差的鲁棒性，RFDDL旨在联合恢复底层清洁数据和清理原子子空间，然后执行DL并对恢复的子空间中的位置进行编码。其次，为了能够潜在地处理从非线性流形采样的数据并通过避免过度拟合来获得精确的重建，RFDDL以灵活的方式最小化重建误差。第三，为了准确地编码标签一致性，RFDDL涉及有区别的灵活稀疏码错误以促使系数变软。第四，为了很好地编码局部性，RFDDL定义了恢复原子上的拉普拉斯矩阵，包括在类内紧致性和类间分离方面的原子标签信息，并与组稀疏码和分类器相关联，以获得准确的判别局部约束系数和分类。公共数据库的广泛结果显示了我们的RFDDL的有效性。Challenges in Time-Stamp Aware Anomaly Detection in Traffic VideosAuthors Kuldeep Marotirao Biradar, Ayushi Gupta, Murari Mandal, Santosh Kumar Vipparthi交通视频中的时间戳识别异常检测是智能交通系统发展的重要任务。由于异常事件的稀疏发生，不同类型异常的不一致行为以及正常和异常情况下的不平衡可用数据，视频中的异常检测是一个具有挑战性的问题。在本文中，我们提出了一个三阶段管道来学习视频中的运动模式以检测视觉异常。首先，从最近的历史帧估计背景以识别静止的对象。该背景图像用于定位帧内的正常异常行为。此外，我们在估计的背景中检测感兴趣的对象，并基于时间戳识别异常检测算法将其分类为异常。我们还讨论了在改善交通异常检测的看不见的测试数据方面所面临的挑战。实验在NVIDIA AI城市挑战2019的第3轨道上进行。结果显示了所提出的方法在检测交通道路视频中的时间戳感知异常方面的有效性。***CVPR19 Tracking and Detection Challenge: How crowded can it get?Authors Patrick Dendorfer, Hamid Rezatofighi, Anton Milan, Javen Shi, Daniel Cremers, Ian Reid, Stefan Roth, Konrad Schindler, Laura Leal Taixe标准化基准测试对于大多数计算机视觉应用至关重要。虽然排行榜和排名表不应过度宣称，但基准通常提供最客观的绩效衡量标准，因此是研究的重要指南。Learning robust visual representations using data augmentation invarianceAuthors Alex Hern ndez Garc a, Peter K nig, Tim C. Kietzmann训练用于图像对象分类的深度卷积神经网络与在灵长类动物腹侧视觉流中发现的表示显示出显着的相似性。然而，人工和生物网络仍然表现出重要的差异。在这里，我们研究了一个这样的属性增加不变性，以保持沿腹侧流发现的身份保持图像变换。尽管有理论证据表明不变性应该从优化过程中自然出现，但我们提出了经验证据，即对于对象分类训练的卷积神经网络的激活对于数据增强中常用的身份保持图像变换不具有鲁棒性。作为解决方案，我们提出数据增强不变性，无监督学习目标，其通过促进增强图像样本的激活之间的相似性来改善学习表示的鲁棒性。我们的结果表明，这种方法是一种简单，有效和高效的训练时间增加方式，在增加模型的不变性的同时获得相似的分类性能。Simultaneously Learning Architectures and Features of Deep Neural NetworksAuthors Tinghuai Wang, Lixin Fan, Huiling Wang本文提出了一种新方法，可以在多个时期内同时重复学习滤波器和网络特征的数量。我们提出了一种新颖的修剪损失，以明确强制优化器专注于有希望的候选过滤器，同时抑制不太相关的过滤器的贡献。同时，我们进一步建议强制过滤器之间的多样性，这种基于多样性的正则化术语改善了模型大小和精度之间的权衡。结果表明，体系结构和特征优化之间的相互作用改进了最终的压缩模型，并且所提出的方法在模型大小和精度方面与现有方法相比有利，适用于广泛的应用，包括图像分类，图像压缩和音频分类。Cross-Modal Relationship Inference for Grounding Referring ExpressionsAuthors Sibei Yang, Guanbin Li, Yizhou Yu将引用表达式接地是一项基本但具有挑战性的任务，有助于物理世界中的人机交流。它基于对引用自然语言表达与图像之间的关系的理解来定位图像中的目标对象。用于接地引用表达式的可行解决方案不仅需要在图像和引用表达式中提取所有必要信息，即对象和它们之间的关系，而且还从提取的信息中计算和表示多模态上下文。遗憾的是，关于接地引用表达式的现有工作不能准确地从引用表达式中提取多顺序关系，并且它们获得的上下文与通过引用表达式描述的上下文存在差异。在本文中，我们提出了一种交叉模态关系提取器CMRE，以自适应地突出显示具有与给定表达式的连接的对象和关系，具有交叉模态注意机制，并将提取的信息表示为语言引导的视觉关系图。此外，我们提出了一种门控图形卷积网络GGCN，通过融合来自不同模式的信息并在结构化关系图中传播多模态信息来计算多模态语义上下文。各种常见基准数据集的实验表明，我们的交叉模态关系推理网络（由CMRE和GGCN组成）优于所有现有技术方法。TW-SMNet: Deep Multitask Learning of Tele-Wide Stereo MatchingAuthors Mostafa El Khamy, Haoyu Ren, Xianzhi Du, Jungwon Lee在本文中，我们介绍了估算由两个具有不同视场的摄像机捕获的场景中元素的真实世界深度的问题，其中第一视场FOV是由广角镜头捕获的宽视场WFOV，以及第二FOV包含在第一FOV中并由远摄变焦镜头捕获。我们指的是估计FOV并集的逆深度的问题，同时利用重叠FOV中的立体声信息，作为远程 宽立体匹配TW SM。我们为TW SM问题提出了不同的深度学习解决方案。由于视差与反深度成比例，因此我们训练立体匹配视差估计SMDE网络以估计联合WFOV的视差。我们进一步提出了端到端深度多任务远程广播立体匹配神经网络MT TW SMNet，其同时学习用于WFOV的重叠Tele FOV和单图像逆深度估计SIDE任务的SMDE任务。此外，我们设计了多种融合SMDE和SIDE网络的方法。我们评估TW SM在流行的KITTI和SceneFlow立体数据集上的性能，并通过从远程宽立体图像对合成WFOV上的散景效果来展示其实用性。Bag of Color Features For Color ConstancyAuthors Firas Laakom, Nikolaos Passalis, Jenni Raitoharju, Jarno Nikkanen, Anastasios Tefas, Alexandros Iosifidis, Moncef Gabbouj在本文中，我们提出了一种新的颜色恒常方法，称为Bag of Color Features BoCF，建立在Bag of Features汇集之上。所提出的方法大大减少了照明估计所需的参数的数量。同时，所提出的方法与颜色恒常性假设一致，表明全局空间信息与照明估计无关，并且局部信息边缘等是足够的。此外，BoCF与颜色恒定统计方法一致，可以解释为许多统计方法的基于学习的概括。为了进一步提高光照估计精度，我们提出了一种基于自我关注的BoCF模型的新型注意机制。与现有技术相比，BoCF方法及其变体实现了竞争，同时在三个基准数据集ColorChecker推荐，INTEL TUT版本2和NUS8上需要更少的参数。**Single Image Blind Deblurring Using Multi-Scale Latent Structure PriorAuthors Yuanchao Bai, Huizhu Jia, Ming Jiang, Xianming Liu, Xiaodong Xie, Wen Gao盲目图像去模糊是计算机视觉中的一个具有挑战性的问题，其旨在仅通过模糊观察来恢复模糊核和潜在清晰图像。受到图像超分辨率之前的流行自我实例的启发，在本文中，我们观察到从模糊观察下采样的粗糙图像大致是潜在清晰图像的低分辨率版本。我们在理论上证明了这种现象，并将足够粗糙的图像定义为未知清晰图像之前的潜在结构。从此之前开始，我们建议在模糊图像金字塔上将最粗糙的图像恢复到最精细的比例，并使用新恢复的清晰图像逐步更新先前的图像。这些粗到精的先验被称为textit Multi Scale Latent Structures MSLS。利用MSLS先验，我们的算法包括两个阶段1我们首先在粗尺度2中初步恢复清晰图像然后我们应用最精细尺度的细化处理以获得最终的去模糊图像。在每个尺度中，为了实现更低的计算复杂度，我们交替执行具有快速局部自我示例匹配的尖锐图像重建，具有误差补偿的加速核估计和快速非盲图像去模糊，而不是计算任何计算上昂贵的非凸起先验。我们进一步扩展了所提出的算法，以解决更具挑战性的非均匀盲图像去模糊问题。大量实验表明，我们的算法能够以更快的运行速度实现与最先进方法相比的竞争结果。**On the Vector Space in Photoplethysmography ImagingAuthors Christian S. Pilz, Vladimir Blazek, Steffen Leonhardt我们研究了可见波长强度的矢量空间，这些面部视频广泛用作Photoplethysmography Imaging PPGI的输入特征。基于欧几里德空间中的群不变性的理论原理，我们推导出拓扑的变化，其中连续测量之间的相应距离被定义为黎曼流形上的测地线。如几种先前方法所讨论的，传感器信号的这种较低维度嵌入统一了关于特征的平移的不变性属性。生成的算子隐含在特征空间上，不需要任何先验知识，也不需要参数调整。根据已知的血容量变化的扩散过程，所得特征的时变准周期性成形自然地以规范状态空间表示的形式发生。计算复杂度低，实现变得相当简单。在实验期间，操作员通过两个公共数据库上的面部视频实现了强大且有竞争力的心率估计性能。NAS-FCOS: Fast Neural Architecture Search for Object DetectionAuthors Ning Wang, Yang Gao, Hao Chen, Peng Wang, Zhi Tian, Chunhua Shen深度神经网络的成功依赖于重要的架构工程。最近，神经架构搜索NAS已经成为通过自动搜索最佳架构来大大减少网络设计中的手动努力的承诺，尽管通常这种算法需要过多的计算资源，例如几千GPU天。迄今为止，对于具有挑战性的视觉任务，例如物体检测，NAS，尤其是快速版本的NAS，研究较少。这里我们建议搜索具有搜索效率的对象检测器的解码器结构。更具体地说，我们的目标是使用定制的强化学习范例有效地搜索特征金字塔网络FPN以及简单的无锚对象检测器的预测头，即FCOS 20。通过精心设计的搜索空间，搜索算法和评估网络质量的策略，我们能够在大约30个GPU天内有效地搜索超过2,000个架构。所发现的体系结构在COCO数据集上超过了现有的对象检测模型，如Faster R CNN，RetinaNet和FCOS，在AP上1到1.9个点，具有可比较的计算复杂度和内存占用，证明了所提出的NAS用于对象检测的功效。Few-Shot Point Cloud Region Annotation with Human in the LoopAuthors Siddhant Jain, Sowmya Munukutla, David Held我们提出了一种点云注释框架，该框架采用人类循环学习，能够创建具有每点注释的大点云数据集。来自人类注释器的稀疏标签被迭代地传播以通过经由几个镜头学习范例微调联合任务的预训练模型来生成网络的完整分段。我们表明，所提出的框架显着减少了注释点云所需的人工交互量，而不会牺牲注释的质量。我们的实验还通过注意到随着系统完成的完整注释的数量增加而减少人类交互，建议框架在注释大数据集时的适用性。最后，我们展示了框架的灵活性，以支持同一点云的多个不同注释，从而能够创建具有不同粒度注释的数据集。iProStruct2D: Identifying protein structural classes by deep learning via 2D representationsAuthors Loris Nanni, Alessandra Lumini, Federica Pasquali, Sheryl Brahnam在本文中，我们从蛋白质的多视D表示开始解决蛋白质分类的问题。从每个3D蛋白质结构，使用蛋白质可视化软件Jmol生成大量2D投影。这组多视D表示包括13种不同类型的蛋白质可视化，其强调蛋白质结构的特定性质，例如，骨架可视化，其显示蛋白质的骨架结构作为Cα原子的痕迹。每种类型的表示用于训练不同的卷积神经网络CNN，并且这些CNN的融合被证明能够利用不同类型的表示的多样性来提高分类性能。另外，通过围绕其中心X，Y和Z视轴均匀旋转蛋白质结构以获得125个图像，获得若干多视图投影。该方法可以被认为是用于改进分类器性能的数据增强方法，并且可以用于训练和测试阶段。所提出的方法对两个数据集的实验评估证明了所提出的方法相对于其他现有技术方法的强度。本文中使用的MATLAB代码可在以下位置获得Polysemous Visual-Semantic Embedding for Cross-Modal RetrievalAuthors Yale Song, Mohammad Soleymani视觉语义嵌入旨在找到共享的潜在空间，其中相关的视觉和文本实例彼此接近。大多数当前方法学习内射嵌入函数，其将实例映射到共享空间中的单个点。不幸的是，内射嵌入不能有效地处理具有多种可能含义的多义实例，它会找到不同含义的平均表示。这阻碍了它在现实世界场景中的使用，其中个体实例及其交叉模态关联通常是模糊的。在这项工作中，我们介绍了多义实例嵌入网络PIE网络，它通过多头自我关注和残留学习将全局上下文与本地引导的特征相结合来计算实例的多个和不同的表示。为了学习视觉语义嵌入，我们将两个PIE网络绑定在多实例学习框架中共同优化它们。大多数关于交叉模态检索的现有工作都集中在图像文本数据上。在这里，我们还处理了一个更具挑战性的视频文本检索案例。为了促进视频文本检索的进一步研究，我们发布了一个新的数据集，从社交媒体收集的50K视频句子对，称为MRW我的反应。我们使用MS COCO，TGIF和新的MRW数据集演示了我们在图像文本和视频文本检索方案中的方法。Subspace Attack: Exploiting Promising Subspaces for Query-Efficient Black-box AttacksAuthors Ziang Yan, Yiwen Guo, Changshui Zhang与广泛研究且易于获取的白盒对应物不同，由于难以估计梯度，黑盒设置中的对抗性示例通常更加艰难。许多方法通过向目标分类系统发出大量查询来实现该任务，这使得整个过程对于系统来说是昂贵且可疑的。在本文中，我们旨在降低此类别中黑盒攻击的查询复杂性。我们建议利用一些参考模型的梯度，这些参考模型可以跨越一些有希望的搜索子空间。实验结果表明，与现有技术相比，我们的方法可以在必要的平均值和中等数量的查询中获得高达2倍和4倍的减少，并且故障率低得多，即使参考模型训练较小且不充分数据集与用于训练受害者模型的数据集不相交。用于复制我们结果的代码和模型将公开发布。Band Attention Convolutional Networks For Hyperspectral Image ClassificationAuthors Hongwei Dong, Lamei Zhang, Bin Zou在高光谱图像HSI的频带中存在冗余和噪声。因此，对于HSI分类方法，能够从数百个输入频带中选择合适的部分是一个很好的特性。在这封信中，提出了一个频带注意模块BAM来实现基于深度学习的HSI分类，其具有频带选择或加权的能力。所提出的BAM可以被视为现有分类网络的即插即用补充组件，其充分考虑了当使用卷积神经网络CNN进行HSI分类时由频带冗余引起的不利影响。与HSI中使用的大多数深度学习方法不同，根据高光谱图像的特征定制的频带注意模块嵌入在普通CNN中以获得更好的性能。同时，与经典的频带选择或加权方法不同，所提出的方法实现了端到端训练而不是分离的阶段。实验在两个HSI基准数据集上进行。与一些经典和先进的深度学习方法相比，不同评价标准下的数值模拟表明，该方法具有良好的性能。最后但并非最不重要的是，一些先进的CNN与提议的BAM相结合以获得更好的性能。PAN: Projective Adversarial Network for Medical Image SegmentationAuthors Naji Khosravan, Aliasghar Mortazi, Michael Wallace, Ulas Bagci已经证明，对抗性学习对于在语义分割中捕获长程和高级标签一致性是有效的。医学成像的独特之处在于，以有效且计算有效的方式捕获3D语义仍然是一个悬而未决的问题。在这项研究中，我们通过提出一种称为PAN的新型投射对抗网络来解决这一计算负担，该网络通过2D投影结合了高级3D信息。此外，我们在我们的框架中引入了一个注意力模块，该模块有助于将全球信息直接从我们的分割器选择性地整合到我们的对抗性网络中。对于临床应用，我们选择CT扫描的胰腺分割。我们提出的框架在不增加分段器复杂性的情况下实现了最先进的性能。Recognizing License Plates in Real-TimeAuthors Xuewen Yang, Xin Wang车牌检测和识别LPDR对于实现智能交通和确保城市的安全性非常重要。然而，LPDR在实际环境中面临着巨大的挑战。牌照可以具有极其多样的尺寸，字体和颜色，并且板图像通常由于倾斜的捕获角度，不均匀的照明，遮挡和模糊而导致质量差。在监视等应用中，通常需要快速处理。为了实现实时和准确的车牌识别，在这项工作中，我们提出了一套技术1一种轮廓重建方法以及边缘检测，以快速检测候选板2一个简单的零一交替方案，以有效地去除假的顶部和底部围绕板块的边界以便于在板3上更准确地分割字符3一组技术以增强训练数据，将SIFT特征结合到CNN网络中，并利用转移学习来获得用于更有效训练的初始参数和4两阶段验证以低成本确定正确平板的程序，在平板检测阶段进行统计过滤以快速去除不需要的候选者，以及CR过程之后的准确CR结果，以进行进一步的平板验证而无需额外处理。我们基于算法实现了完整的LPDR系统。实验结果表明，我们的系统可以实时准确识别车牌。此外，它可以在各种水平的照明和噪音下以及在汽车运动的情况下稳健地工作。与对等方案相比，我们的系统不仅是最准确的系统，而且也是最快的系统，可以轻松应用于其他方案。Object-aware Aggregation with Bidirectional Temporal Graph for Video CaptioningAuthors Junchao Zhang, Yuxin Peng视频字幕旨在自动生成视频内容的自然语言描述，近年来引起了很多关注。生成准确且细粒度的字幕不仅需要了解视频的全局内容，还需要捕获详细的对象信息。同时，视频表示对生成的字幕质量有很大影响。因此，视频字幕捕获具有详细时间动态的显着对象并使用判别性时空表示来表示它们是很重要的。在本文中，我们提出了一种新的基于对象感知聚合的视频字幕方法和双向时间图OA BTG，它捕获视频中显着对象的详细时间动态，并通过对检测到的对象执行对象感知局部特征聚合来学习判别性时空表示。区域。主要的新颖性和优点是1双向时间图双向时间图沿着时间顺序构建并反向构建，提供了捕获每个显着对象的时间轨迹的互补方式。 2对象感知聚合可学习的局部聚合描述符模型的VLAD向量在对象时间轨迹和全局帧序列上构建，其执行对象感知聚合以学习判别性表示。还开发了分层注意机制以区分多个对象的不同贡献。两个广泛使用的数据集上的实验证明我们的OA BTG在BLEU 4，METEOR和CIDEr指标方面达到了最先进的性能。***Hybrid Function Sparse Representation towards Image Super ResolutionAuthors Junyi Bian, Baojun Lin, Ke Zhang基于训练的字典的稀疏表示已经在超分辨率SR上显示成功但仍然具有一些限制。基于在不失去其保真度的情况下进行函数曲线放大的思想，我们提出了一种基于函数的超分辨率稀疏表示字典，称为混合函数稀疏表示HFSR。我们设计的字典直接由预设的混合功能生成，无需额外的培训，由于其可扩展的属性，可以根据需要缩放到任何大小。我们将近似的Heaviside函数AHF，正弦函数和DCT函数混合为字典。然后提出多尺度细化以利用字典的可缩放属性来改善结果。此外，采用重建策略来处理重叠。在Set14 SR数据集上的实验表明，与基于非学习的现有技术方法相比，我们的方法具有优异的性能，特别是对于包含丰富细节和上下文的图像。***FAMED-Net: A Fast and Accurate Multi-scale End-to-end Dehazing NetworkAuthors Jing Zhang, Dacheng Tao单图像去雾是后续高级计算机视觉任务的关键图像预处理步骤。然而，由于其不良的性质，它仍然具有挑战性。现有的去雾模型倾向于遭受模型过复杂性和计算效率低下或具有有限的表示能力。为了应对这些挑战，我们在此提出了一种快速，准确的多尺度端到端去雾网络，称为FAMED Net，它包括三个刻度的编码器和一个融合模块，可以高效，直接地学习无雾图像。每个编码器由级联和密集连接的点式卷积层和池化层组成。由于没有使用更大的卷积内核并且逐层重用特征，因此FAMED Net具有轻量级和计算效率。对包括RESIDE和真实世界模糊图像在内的公共合成数据集进行的全面实证研究表明，FAMED网络在模型复杂性，计算效率，恢复精度和交叉集概括方面优于其他代表性的现有模型。该代码将公开发布。Online Object Representations with Contrastive LearningAuthors S ren Pirk, Mohi Khansari, Yunfei Bai, Corey Lynch, Pierre Sermanet我们提出了一种自我监督的方法，用于学习单目视频对象的表示，并证明它在机器人等位置设置中特别有用。本文的主要贡献是1一个自我监督的目标，通过对比学习训练，可以发现和解开视频中的对象属性而不使用任何标签2我们利用对象自我监督进行在线自适应我们的在线模型看待视频中的对象的时间越长，对象识别错误越低，而离线基线仍然存在大的固定误差3，以探索完全没有人为监督的系统的可能性，我们让机器人收集自己的数据，用我们的自我监督方案训练这些数据，然后显示机器人可以指向类似于前面呈现的对象的对象，展示对象属性的概括。这种方法的一个有趣且可能令人惊讶的发现是，给定一组有限的对象，当使用对比学习而不需要明确的正对时，对象对应自然会出现。可在以下网址获取说明在线对象调整和机器人指向的视频Semantic-guided Encoder Feature Learning for Blurry Boundary DelineationAuthors Dong Nie, Dinggang Shen编码器解码器架构广泛用于医学图像分割任务。通过横向跳过连接，模型可以在深层中获取并融合语义和分辨率信息，以实现更准确的分割性能。然而，在许多应用中，例如模糊的边界图像，这些模型通常不能精确地定位复杂的边界并且分割出微小的孤立部分。为了解决这个具有挑战性的问题，我们首先分析为什么简单的跳过连接不足以帮助准确定位模糊边界，并认为这是由于编码器层中提供的跳过连接中的模糊信息。然后，我们提出了一种语义引导的编码器特征学习策略，以学习高分辨率和丰富的语义编码器特征，以便我们可以更准确地定位模糊边界，这也可以通过选择性地学习判别特征来增强网络。此外，我们进一步提出了一种软轮廓约束机制来模拟模糊边界检测。实际临床数据集的实验结果表明，我们提出的方法可以实现最先进的分割精度，特别是对于模糊区域。进一步分析还表明，我们提出的网络组件确实有助于提高性能。对其他数据集的实验验证了我们提出的方法的泛化能力。SymNet: Symmetrical Filters in Convolutional Neural NetworksAuthors Gregory Dzhezyan, Hubert Cecotti对称性存在于自然和科学中。在图像处理中，用于空间滤波的内核具有一些对称性，例如，索贝尔算子，高斯，拉普拉斯算子。人工前馈神经网络中的卷积层通常在没有任何约束的情况下考虑核权重。在本文中，我们建议研究卷积层中对称约束对图像分类任务的影响，从初级视觉皮层和常见图像处理技术中涉及的过程中获取灵感。目标是通过修改在反向传播算法期间执行的权重更新并评估性能变化来评估在整个卷积神经网络CNN的训练过程中对过滤器实施对称约束的程度。本文的主要假设是对称约束减少了网络中自由参数的数量，并且能够实现与现代训练方法几乎相同的性能。特别地，我们解决了以下情况：轴对称，点反射和反点反射。已经在四个图像数据库上评估了性能。结果支持这样的结论：虽然随机权重为模型提供了更多的自由度，但对称约束提供了类似的性能水平，同时大大减少了模型中自由参数的数量。这种方法在需要整个特征提取过程中具有线性相位特性的相敏应用中是有价值的。FASTER Recurrent Networks for Video ClassificationAuthors Linchao Zhu, Laura Sevilla Lara, Du Tran, Matt Feiszli, Yi Yang, Heng Wang视频分类方法通常将视频分成短片段，独立地对这些片段进行推断，然后聚合这些预测以生成最终的分类结果。将这些高度相关的剪辑视为独立的两者都忽略了信号的时间结构并且带来了大的计算成本，模型必须从头开始处理每个剪辑。为了降低这种成本，最近的努力集中在设计更有效的剪辑级网络架构上。然而，对整体框架的关注较少，包括如何从相邻剪辑之间的相关性中受益以及改进聚合策略本身。在本文中，我们利用相邻视频剪辑之间的相关性来解决聚合阶段视频分类中计算成本效率的问题。更具体地，给定剪辑特征表示，计算下一剪辑表示的问题变得更容易。我们提出了一种名为FASTER的新型复现架构，用于视频级分类，它结合了高质量，昂贵的剪辑表示，捕捉细节的动作，以及轻量级表示，捕捉视频中的场景变化并避免冗余计算。我们还提出了一种新颖的处理单元来学习剪辑级表示的集成，以及它们的时间结构。我们将此单元称为FAST GRU，因为它基于门控循环单元GRU。所提出的框架在推理时间上实现了明显更好的FLOP与准确度之间的折衷。与现有方法相比，我们提出的框架将FLOP减少了10倍以上，同时在流行数据集（例如Kinetics，UCF101和HMDB51）中保持相似的准确性。****End-to-End CAD Model Retrieval and 9DoF Alignment in 3D ScansAuthors Armen Avetisyan, Angela Dai, Matthias Nie ner我们提出了一种新颖的端到端方法，将CAD模型与场景的3D扫描对齐，从而将嘈杂，不完整的3D扫描转换为紧凑的CAD重建，并使用干净，完整的物体几何体。我们的主要贡献在于制定可区分的Procrustes对齐，该对齐与对称感知密集对象对应预测配对。为了同时将CAD模型与扫描场景的所有对象对齐，我们的方法检测对象位置，然后预测在统一对象空间中扫描和CAD几何体之间的对称感知密集对象对应关系，以及最近邻CAD模型，两者都是然后用于通知可区分的Procrustes对齐。我们的方法以完全卷积的方式运行，使CAD模型能够在单个前向传递中与扫描对象对齐。这使得我们的方法在19.04时能够胜过最先进的方法，用于CAD模型与扫描的对齐，运行时间比以前的数据驱动方法快约250倍。Data-Free Quantization through Weight Equalization and Bias CorrectionAuthors Markus Nagel, Mart van Baalen, Tijmen Blankevoort, Max Welling我们介绍了一种不需要微调或超参数选择的深度神经网络的无数据量化方法。它在常见的计算机视觉架构和任务上实现了接近原始的模型性能。 8位定点量化对于现代深度学习硬件架构中的有效推理至关重要。然而，量化模型以8位运行是一项非常重要的任务，经常导致显着的性能降低或者在训练网络上花费的工程时间适合于量化。我们的方法依赖于通过利用激活函数的尺度等效性来均衡网络中的权重范围。此外，该方法校正在量化期间引入的误差中的偏差。这提高了量化精度性能，并且可以通过直接的API调用无处不在地应用于几乎任何模型。对于常见的体系结构，例如MobileNet系列，我们实现了最先进的量化模型性能。我们进一步表明，该方法还扩展到其他计算机视觉架构和任务，如语义分割和对象检测。Automatic brain tissue segmentation in fetal MRI using convolutional neural networksAuthors N. Khalili, N. Lessmann, E. Turk, N. Claessens, R. de Heus, T. Kolk, M.A. Viergever, M.J.N.L. Benders, I. Isgum胎儿的MR图像允许临床医生在发育的早期阶段检测脑异常。胎儿MRI中体积和形态分析的基石是将胎儿大脑分成不同的组织类别。手动分割是麻烦且耗时的，因此自动分割可以大大简化过程。然而，由于包括强度不均匀性的伪影，这些扫描中的自动脑组织分割是挑战性的，特别是由扫描期间的自发胎儿运动引起的。与估计偏移场以消除强度不均匀性作为分割的预处理步骤的方法不同，我们建议使用卷积神经网络进行分割，该网络利用合成引入的强度不均匀性的图像作为数据增强。该方法首先使用CNN来提取颅内体积。此后，采用具有相同结构的另一CNN将提取的体积分成七个脑组织类别小脑，基底神经节和丘脑，脑室脑脊液，白质，脑干，皮质灰质和脑脊髓液。为了使该方法适用于显示强度不均匀性伪影的切片，通过将线性梯度与随机偏移和方向的组合应用于没有伪影的图像切片来增强训练数据。Generative adversarial network for segmentation of motion affected neonatal brain MRIAuthors N. Khalili, E. Turk, M. Zreik, M.A. Viergever, M.J.N.L. Benders, I. Isgum早产儿自动新生儿脑组织分割是评估大脑发育的先决条件。然而，自动分割经常受到图像采集期间婴儿头部运动引起的运动伪影的阻碍。已经开发了使用频域数据在图像重建期间去除或最小化这些伪像的方法。但是，频域数据可能并不总是可用。因此，在本研究中，我们提出了一种从已经重建的MR扫描中去除运动伪影的方法。该方法采用以循环一致性损失训练的生成对抗网络，以将受运动影响的切片转换成没有运动伪影的切片，反之亦然。在实验中，使用40个在经后年龄30周时成像的早产婴儿的T2加权冠状MR扫描。所有图像都包含受运动伪影影响的切片，妨碍了自动组织分割。为了评估校正是否允许更准确的图像分割，图像被分割成8个组织类别小脑，有髓白质，基底神经节和丘脑，脑室脑脊液，白质，脑干，皮质灰质和脑脊髓液。使用5点李克特量表定性评估针对运动和相应分割校正的图像。在校正运动伪影之前，中值图像质量和相应自动分割的质量分别被分配为2级差和3级中等。在校正运动伪影之后，两者分别改善到3级和4级。结果表明，使用所提出的方法校正图像空间中的运动伪影允许在受运动伪影影响的切片中精确分割脑组织类别。On Single Source Robustness in Deep Fusion ModelsAuthors Taewan Kim, Joydeep Ghosh融合多个输入源的算法受益于互补和共享信息。共享信息可以为故障或噪声输入提供稳健性，这对于自驾车等安全关键应用是必不可少的。我们研究了学习融合算法，该算法能够抵抗单个来源的噪声。我们首先证明在线性融合模型中不能保证对单源噪声的鲁棒性。在这一发现的推动下，提出了两种可能的方法来提高鲁棒性，使用相应的深度融合模型训练算法，以及在处理噪声方面具有结构优势的简单卷积融合层来提高精确设计的损耗。实验结果表明，训练算法和我们的融合层都使得基于深度融合的三维物体探测器能够抵抗应用于单个声源的噪声，同时保持原始性能对干净的数据。`Project & Excite' Modules for Segmentation of Volumetric Medical ScansAuthors Anne Marie Rickmann, Abhijit Guha Roy, Ignacio Sarasua, Nassir Navab, Christian Wachinger完全卷积神经网络F CNN实现了医学成像中图像分割的最先进性能。最近，已经引入挤压和激励SE模块及其变型以重新校准特征图通道和空间方式，这可以提高性能同时仅最小化地增加模型复杂性。到目前为止，SE的发展主要集中在2D图像上。在本文中，我们提出了基于SE思想的Project Excite PE模块，并将它们扩展到3D体积图像上。 Project Excite不执行全局平均合并，而是分别沿着张量的不同切片挤压特征贴图以保留随后在激励步骤中使用的更多空间信息。我们证明了PE模块可以轻松集成到3D U Net中，通过5个Dice点提升性能，同时仅将模型复杂度提高2倍。我们评估PE模块的两个具有挑战性的任务，MRI扫描的全脑分割和CT扫描的全身分割。码DropConnect Is Effective in Modeling Uncertainty of Bayesian Deep NetworksAuthors Aryan Mobiny, Hien V. Nguyen, Supratik Moulik, Naveen Garg, Carol C. Wu深度神经网络DNN已经在许多重要领域实现了最先进的性能，包括医疗诊断，安全性和自动驾驶。在安全性非常关键的这些领域，错误的决策可能导致严重的后果。虽然完美的预测准确性并不总是可以实现，但贝叶斯深度网络的最新工作表明，有可能知道DNN何时更容易出错。了解DNN不知道的内容对于提高敏感应用中深度学习技术的安全性是可取的。贝叶斯神经网络试图解决这一挑战。然而，传统方法在计算上难以处理，并且不能很好地扩展到大型复杂的神经网络架构。在本文中，我们通过对模型权重施加伯努利分布，建立了一个理论框架来逼近DNN的贝叶斯推断。这种称为MC DropConnect的方法为我们提供了一种工具来表示模型的不确定性，而整体模型结构或计算成本几乎没有变化。我们在多个网络架构和数据集上广泛验证了所提出的算法，用于分类和语义分段任务。我们还提出了新的指标来量化不确定性估计。这使得MC DropConnect与先前方法之间能够进行客观比较。我们的实证结果表明，与现有技术相比，所提出的框架在预测准确性和不确定性估计质量方面产生显着改善。***Anomaly Detection in High Performance Computers: A Vicinity PerspectiveAuthors Siavash Ghiasvand, Florina M. Ciorba响应于对更高计算能力的需求，高性能计算机HPC中的计算节点的数量迅速增加。 Exascale HPC系统预计到2020年到货。随着HPC系统组件数量的急剧增加，预计会出现故障数量的突然增加，从而对HPC系统的持续运行构成威胁。尽早检测故障并理想地预测故障是避免HPC系统运行中断的必要步骤。异常检测是计算系统中用于故障检测的众所周知的通用方法。大多数现有方法是针对特定体系结构设计的，需要对计算系统硬件和软件进行调整，需要过多信息，或对用户和系统隐私构成威胁。该工作提出了一种基于基于邻近的统计异常检测方法的节点故障检测机制，该方法使用被动收集和匿名的系统日志条目。将所提出的方法应用于8个月内收集的系统日志表明异常检测精度在62到81之间。BasisConv: A method for compressed representation and learning in CNNsAuthors Muhammad Tayyab, Abhijit Mahalanobis众所周知，卷积神经网络CNN在其滤波器权重方面具有显着的冗余。在文献中已经提出了各种方法来压缩训练的CNN。这些包括诸如修剪权重，滤波器量化和根据基函数表示滤波器的技术。我们的方法属于后一类策略，但不同之处在于我们展示了压缩学习和表示都可以在不对流行的CNN架构进行重大修改的情况下实现。具体来说，CNN的任何卷积层很容易被两个连续的卷积层取代，第一个是一组固定的滤波器，它们代表整个层的知识空间而不会改变，后面是一层代表一维滤波器这个领域的学识渊博。对于预训练的网络，固定层只是原始滤波器的截断特征分解。 1D滤波器初始化为线性组合的权重，但经过微调以恢复由于截断引起的任何性能损失。为了从头开始训练网络，我们使用一组永不改变的随机正交固定滤波器，并直接从标记数据中学习一维权重向量。我们的方法在训练期间大大减少了可学习参数的数量，以及ii在实现期间的乘法运算和滤波器存储要求的数量。它不需要卷积层中的任何特殊运算符，并且扩展到所有已知的流行CNN架构。我们将我们的方法应用于使用三种不同数据集训练的四种众所周知的网络架构。结果显示，操作次数最多可减少5倍，ii可学习参数数量最多减少18倍，CIFAR100数据集性能下降不到3次。A Novel Cost Function for Despeckling using Convolutional Neural NetworksAuthors Giampaolo Ferraioli, Vito Pascazio, Sergio Vitale从SAR图像中去除斑点噪声仍然是一个悬而未决的问题。众所周知，对SAR图像的解释是非常具有挑战性的，并且为了提高提取信息的能力，必须使用去斑算法。由于不同的结构和不同的物体尺度，城市环境使这项任务更加沉重。随着最近与几种遥感应用相关的深度学习方法的普及，本文提出了一种基于卷积神经网络的去斑算法。网络接受模拟SAR数据的训练。本文主要关注成本函数的实现，该成本函数考虑了图像的空间一致性和噪声的统计特性。Deep learning analysis of cardiac CT angiography for detection of coronary arteries with functionally significant stenosisAuthors Majd Zreik, Robbert W. van Hamersvelt, Nadieh Khalili, Jelmer M. Wolterink, Michiel Voskuil, Max A. Viergever, Tim Leiner, Ivana I gum在患有阻塞性冠状动脉疾病的患者中，需要确定冠状动脉狭窄的功能意义以指导治疗。这通常通过在侵入性冠状动脉血管造影ICA期间执行的分数流量储备FFR测量来建立。我们提出了一种自动和非侵入性检测功能显着的冠状动脉狭窄的方法，采用心脏CT血管造影CCTA图像中的完整冠状动脉的深度无监督分析。我们回顾性收集了187例患者的CCTA扫描，其中137例在192个不同的冠状动脉中进行了侵入性FFR测量。这些FFR测量值作为冠状动脉狭窄的功能意义的参考标准。提取冠状动脉的中心线并用于重建拉直的多平面重新格式化的MPR体积。为了自动识别具有功能上显着的狭窄的动脉，使用分别执行空间和顺序编码的两个不相交的3D和1D卷积自动编码器将每个MPR体积编码成固定数量的编码。此后，使用支持向量机分类器，根据功能上显着的狭窄的存在，使用这些编码来对动脉进行分类。使用重复交叉验证实验评估的功能上显着的狭窄的检测导致接收器操作特征曲线下面积在动脉水平上为0.81pm 0.02，在患者水平上为0.87pm 0.02。结果表明，使用CCTA图像中完整冠状动脉的特征，自动非侵入性检测冠状动脉中功能上显着的狭窄是可行的。这可能会减少不必要地接受ICA的患者数量。SALT: Subspace Alignment as an Auxiliary Learning Task for Domain AdaptationAuthors Kowshik Thopalli, Jayaraman J. Thiagarajan, Rushil Anirudh, Pavan Turaga无监督域适应旨在将从标记源域学到的知识转移和调整到未标记的目标域。无监督域自适应的关键组件包括在源上最大化性能，以及b对齐源域和目标域。传统上，这些任务要么被认为是独立的，要么被假定为与高容量特征提取器一起隐式地解决。在本文中，我们提出了第三种广泛的方法，我们称之为SALT。核心思想是将对齐作为辅助任务，将最大化源性能的主要任务考虑在内。通过假设子空间形式的易处理数据几何，使辅助任务变得相当简单。我们协同地允许来自封闭形式辅助解决方案的某些参数受到来自主要任务的梯度的影响。所提出的方法代表了基于几何和基于模型的对齐与来自数据驱动的主要任务的梯度流的独特融合。 SALT很简单，根植于理论，并且在多个标准基准测试中表现优于最新技术水平。Multiscale Nakagami parametric imaging for improved liver tumor localizationAuthors Omar S. Al Kadi有效的超声组织表征通常受到复杂组织结构的阻碍。散斑图案的交织使得反向散射分布参数的正确估计复杂化。基于局部形状参数映射的Nakagami参数化成像可以模拟不同的后向散射条件。然而，构建的Nakagami图像的性能取决于估计方法对反向散射统计和分析规模的敏感性。在估计Nakagami参数图像时使用感兴趣的固定焦点区域将增加估计方差。在这项工作中，通过多尺度基础上的最大似然估计自适应地估计局部Nakagami参数。变尺寸内核在多个尺度上集成了后向散射分布参数的拟合优度，以实现更稳定的参数估计。结果显示组织镜面反射变化的定量可视化改善，表明在低对比度超声图像中改善肿瘤定位的潜在方法。Adaptively Preconditioned Stochastic Gradient Langevin DynamicsAuthors Chandrasekaran Anirudh Bhardwaj随机梯度Langevin动力学向SGD注入各向同性梯度噪声，以帮助导航深层网络损失景观中的病理曲率。噪声的各向同性本质导致不良的缩放，并且已经提出了基于诸如Fisher Scoring的高阶曲率信息的自适应方法来预处理噪声以便实现更好的收敛。在本文中，我们描述了一种估计噪声参数的自适应方法，并在众所周知的模型架构上进行实验，以表明自适应预处理SGLD方法与Adam，AdaGrad等自适应一阶方法的速度实现收敛。在测试集中实现SGD的泛化等价。Transport Triggered Array Processor for Vision ApplicationsAuthors Mehdi Safarpour, Ilkka Hautala, Miguel Bordallo Lopez, Olli Silven许多物联网中的低级感官数据处理物联网设备通过利用睡眠模式或将时钟减慢到最小来追求能效。为了抑制那些设计中的待机功耗的份额，采用接近阈值的子阈值操作点或制造中的超低泄漏过程。这些会显着限制时钟速率，从而降低各个处理内核的计算吞吐量。在此贡献中，我们探索通过大规模并行化来补偿在接近阈值区域Vdd 0.6V下操作的性能损失。近阈值操作和大规模并行性的好处分别是每指令操作的最佳能量消耗和最小化的存储器往返。设计的处理元件PE基于传输触发架构。细粒度可编程并行解决方案允许快速有效地计算可学习的低级特征，例如，本地二进制描述符和卷积。其他操作，包括Max pooling也已实施。可编程设计实现了局部二进制模式计算的出色能效。Identifying Visible Actions in Lifestyle VlogsAuthors Oana Ignat, Laura Burdick, Jia Deng, Rada Mihalcea我们认为识别在线视频中可见的人类行为的任务。我们专注于广泛传播的生活方式视频博客类型，其中包括人们在口头描述时执行操作的视频。我们的目标是确定视频的语音描述中提到的动作是否在视觉上呈现。我们构建了一个包含可见动作的众包手动注释的数据集，并引入了一种多模式算法，该算法利用从视觉和语言线索中获得的信息来自动推断视频中哪些动作是可见的。我们证明了我们的多模态算法一次只能基于一种模态优于算法。BowNet: Dilated Convolution Neural Network for Ultrasound Tongue Contour ExtractionAuthors M. Hamed Mozaffari, Won Sook Lee超声成像安全，相对实惠，并且具有实时性能。该技术的一个应用是在实时演讲期间可视化和表征人类舌头的形状和运动，以研究健康或受损的语音产生。由于具有低对比度特性的超声图像的嘈杂性质，可能需要非专业用户的专业知识来识别器官形状，例如舌头表面背部。为了减轻舌头形状和运动的定量分析的这种困难，可以提取，跟踪和可视化舌头表面而不是整个舌头区域。从每个框架描绘舌头表面是麻烦的，主观的和容易出错的任务。此外，舌头手势的快速性和复杂性使其成为具有挑战性的任务，并且手动分割对于实时应用来说不是可行的解决方案。利用现有技术的深度神经网络模型和训练技术，实现具有实时性能的全自动，准确，鲁棒的分割方法是可行的，适用于语音中舌头轮廓的跟踪。本文介绍了两种新的深度神经网络模型，名为BowNet，wBowNet受益于全局预测解码编码模型的能力，具有集成的多尺度上下文信息，以及扩散卷积的全分辨率局部提取能力。使用多个超声舌图像数据集的实验结果表明，定位和全球搜索的结合可以显着提高预测结果。使用定性和定量研究对BowNet模型的评估表明，与类似技术相比，它们在准确性和稳健性方面取得了显着成就。Alzheimer's Disease Brain MRI Classification: Challenges and InsightsAuthors Yi Ren Fung, Ziqiang Guan, Ritesh Kumar, Joie Yeahuay Wu, Madalina Fiterau近年来，许多论文报道了使用卷积神经网络从阿尔茨海默氏病神经影像学计划ADNI数据集进行MRI扫描的阿尔茨海默病分类的最新技术表现。然而，我们发现，当我们将这些数据分成主题级别的培训和测试集时，我们无法获得类似的性能，从而使许多先前研究的有效性受到质疑。此外，我们指出以前的工作使用不同的ADNI数据子集，使得在类似工作中的比较变得棘手。在这项研究中，我们提出三种分裂方法的结果，讨论其有效性背后的动机，并使用所有可用的主题报告我们的结果。**Human-Machine Collaboration for Fast Land Cover MappingAuthors Caleb Robinson, Anthony Ortiz, Kolya Malkin, Blake Elias, Andi Peng, Dan Morris, Bistra Dilkina, Nebojsa Jojic我们建议将人类贴标机纳入模型微调系统，以提供即时的用户反馈。在我们的框架中，人类贴标签者可以交互式地查询未标记数据的模型预测，选择要标记的数据，并查看对模型预测产生的影响。这种双向反馈回路允许人们了解模型如何响应新数据。我们的假设是，这种丰富的反馈允许人类贴标者创建心理模型，使他们能够更好地选择引入模型的偏差。我们将人类选择点与使用标准主动学习方法选择的点进行比较。我们进一步研究微调方法如何影响人类贴标机的性能。我们实现了这个框架，用于微调高分辨率土地覆盖分割模型。具体来说，我们微调了一个深度神经网络，该网络训练将高分辨率航空影像分割成美国马里兰州的不同土地覆盖类别，到达美国纽约的一个新的空间区域。紧密循环将算法和人类操作员转变为混合系统，可以比传统工作流程更有效地生成大面积的土地覆盖图。我们的框架在地理空间机器学习环境中具有应用，其中实际上无限制地提供未标记数据，其中只有一小部分可以通过人工努力进行标记。Chinese Abs From Machine TranslationPapers from arxiv.org更多精彩请移步主页pic from pexels.com"}
{"content2":"一、传统图像算法工程师：主要涉及图形处理，包括形态学、图像质量、相机成像之3A算法、去雾处理、颜色空间转换、滤镜等，主要在安防公司或者机器视觉领域，包括缺陷检测；二、现代图像算法工程师:涉及模式识别，主要表现的经验为Adaboost、SVM的研究与应用，特征选取与提取，包括智能驾驶的研究与应用、行人检测、人脸识别；三、人工智能时代图像算法工程师：深度学习，主要在大型互联网公司或者研究所机构，具体体现在TensorFlow等开源库的研究与应用，包括机器人的研、基于深度学习的人脸识别；CSDN上也总结了很多：（补充2016-12-21  23:49:53）Deep Learning源代码收集-持续更新…Deep Learning（深度学习）计算机视觉、机器学习相关领域论文和源代码大集合--持续更新……图像处理与计算机视觉：基础，经典以及最近发展  （经典论文集合）计算机视觉领域的一些牛人博客，超有实力的研究机构web主页（转）来自西弗吉利亚大学li xin整理的CV代码合集（转）图像处理与机器视觉行业分析计算机视觉整理库https://github.com/ty4z2008/Qix通用库/General LibraryOpenCV   无需多言。RAVL  Recognition And Vision Library. 线程安全。强大的IO机制。包含AAM。CImg  很酷的一个图像处理包。整个库只有一个头文件。包含一个基于PDE的光流算法。图像，视频IO/Image, Video IOFreeImageDevILImageMagickFFMPEGVideoInputportVideoAR相关/Augmented RealityARToolKit   基于Marker的AR库ARToolKitPlus   ARToolKit的增强版。实现了更好的姿态估计算法。PTAM    实时的跟踪、SLAM、AR库。无需Marker，模板，内置传感器等。BazAR   基于特征点检测和识别的AR库。局部不变特征/Local Invariant FeatureVLFeat   目前最好的Sift开源实现。同时包含了KD-tree，KD-Forest，BoW实现。VLFeat：著名而常用项目网站：http://www.vlfeat.org著名的计算机视觉/图像处理开源项目，知名度应该不必OpenCV低太多，曾获ACM Open Source Software Competition 2010一等奖。使用C语言编写，提供C语言和Matlab两种接口。实现了大量计算机视觉算法，包括：常用图像处理功能，包括颜色空间变换、几何变换（作为Matlab的补充），常用机器学习算法，包括GMM、SVM、KMeans等，常用的图像处理的plot工具。特征提取，包括 Covariant detectors, HOG, SIFT,MSER等。VLFeat提供了一个vl_covdet() 函数作为框架，可以方便的统一所谓“co-variant feature detectors”，包括了DoG, Harris-Affine, Harris-Laplace并且可以提取SIFT或raw patches描述子。超像素（Superpixel）分割，包括常用的Quick shift, SLIC算法等高级聚类算法，比如整数KMeans：Integer k-means (IKM)、hierarchical version of integer k-means (HIKM)，基于互信息自动判定聚类类数的算法Agglomerative Information Bottleneck (AIB) algorithm等高维特特征匹配算法，随机KD树Randomized kd-trees可以在这里查看VLFeat完整的功能列表。 http://www.vlfeat.org/matlab/matlab.htmlFerns    基于Naive Bayesian Bundle的特征点识别。高速，但占用内存高。SIFT By Rob Hess   基于OpenCV的Sift实现。目标检测/Object DetectionAdaBoost By JianXin.Wu    又一个AdaBoost实现。训练速度快。行人检测 By JianXin.Wu    基于Centrist和Linear SVM的快速行人检测。（近似）最近邻/ANNFLANN    目前最完整的（近似）最近邻开源库。不但实现了一系列查找算法，还包含了一种自动选取最快算法的机制。ANN    另外一个近似最近邻库。SLAM & SFMSceneLib    [LGPL]   monoSLAM库。由Androw Davison开发。图像分割/SegmentationSLIC Super Pixel   使用Simple Linear Iterative Clustering产生指定数目，近似均匀分布的Super Pixel。目标跟踪/TrackingTLD   基于Online Random Forest的目标跟踪算法。KLT  Kanade-Lucas-TrackerOnline boosting trackers   Online Boosting Trackers直线检测/Line DetectionDSCC    基于联通域连接的直线检测算法。LSD [GPL]   基于梯度的，局部直线段检测算子。指纹/Finger PrintpHash [GPL]   基于感知的多媒体文件Hash算法。（提取，对比图像、视频、音频的指纹）视觉显著性/Visual SalienceGlobal Contrast Based Salient Region Detection    Ming-Ming Cheng的视觉显著性算法。FFT/DWTFFTW [GPL]   最快，最好的开源FFT。FFTReal [WTFPL]   轻量级的FFT实现。许可证是亮点。音频处理/Audio processingSTK [Free]   音频处理，音频合成。libsndfile [LGPL]  音频文件IO。libsamplerate [GPL ]音频重采样。小波变换   快速小波变换（FWT）FWTBRIEF: Binary Robust Independent Elementary Feature 一个很好的局部特征描述子，里面有FAST corner + BRIEF实现特征点匹配的DEMO：http://cvlab.epfl.ch/software/brief/http://code.google.com/p/javacvJava打包的OpenCV, FFmpeg, libdc1394, PGR FlyCapture, OpenKinect, videoInput, and ARToolKitPlus库。可以放在Android上用~libHIK,HIK SVM，计算HIK SVM跟Centrist的Lib。http://c2inet.sce.ntu.edu.sg/Jianxin/projects/libHIK/libHIK.htm一组视觉显著性检测代码的链接：http://cg.cs.tsinghua.edu.cn/people/~cmm/saliency/Peter Kovesi的工具箱：轻量好用，侧重图像处理  http://www.peterkovesi.com/matlabfns/项目网站：http://www.csse.uwa.edu.au/~pk/research/matlabfns/这位Peter大哥目前在The University of Western Australia工作，他自己写了一套Matlab计算机视觉算法，所谓工具箱其实就是许多m文件的集合，全部Matlab实现，无需编译安装，支持Octave（如果没有Matlab的话，有了这个工具箱也可以在Octave下进行图像处理了）。别看这位大哥单枪匹马，人家的工具箱可是相当有名，研究时候需要哪个Matlab的计算机视觉小功能，直接到他家主页上下几个m文件放在自己文件夹就好了。这个工具箱主要以图像处理算法为主，附带一些三维视觉的基本算法，列一些包括的功能：Feature Detection via Phase Congruency，通过相位一致性检测图像特征Spatial Feature Detection，Harris、Canny之类的特征算法Edge Linking and Line Segment Fitting，边缘特征和线特征的各种操作Image Denoising，图像降噪Surface Normals to Surfaces，从法向量积分出表面Scalogram Calculation，量图计算Anisotropic diffusion，著名的保边缘平滑算法Frequency Domain Transformations，傅立叶变换Functions Supporting Projective Geometry，透视几何、三维视觉的一些算法Feature Matching、特征匹配Model Fitting and Robust Estimation、RANSACFingerprint Enhancement，指纹图像增强Interesting Synthetic Images，一些好玩儿的图像生成算法Image Blending，图像融合Colourmaps and colour conversions，颜色空间算法MexOpenCV：让Matlab支持调用的OpenCV项目网站：http://www.cs.sunysb.edu/~kyamagu/mexopencv/作者Kota Yamaguchi桑是石溪大学（Stony Brook University）的PhD，早些时候自己搞了一套东西把OpenCV的代码编译成Matlab可用的mex接口，然后这个东西迅速火了。今年夏天这个项目被OpenCV吸收为一个模块，貌似是搞了一个Google Summer of Code（GSoC）的项目，最近（大概是9、10月）已经merge到了OpenCV主包，有兴趣的可以到Github的OpenCV库下的module/matlab去玩一下，应该会在10月份的OpenCV 3 alpha里正式发布。现在OpenCV就同时有了Python和Maltab的binding（好强大）。具体的功能就不细说了，既然是OpenCV的binding，当然是可以使用OpenCV的绝大多数算法了。介绍n款计算机视觉库/人脸识别开源库/软件计算机视觉库 OpenCVOpenCV是Intel®开源计算机视觉库。它由一系列 C 函数和少量 C++ 类构成，实现了图像处理和计算机视觉方面的很多通用算法。 OpenCV 拥有包括 300 多个C函数的跨平台的中、高层 API。它不依赖于其它的外部库——尽管也可以使用某些外部库。 OpenCV 对非商业...人脸识别 faceservice.cgifaceservice.cgi 是一个用来进行人脸识别的 CGI 程序， 你可以通过上传图像，然后该程序即告诉你人脸的大概坐标位置。faceservice是采用 OpenCV 库进行开发的。OpenCV的.NET版 OpenCVDotNetOpenCVDotNet 是一个 .NET 对 OpenCV 包的封装。人脸检测算法 jViolajonesjViolajones是人脸检测算法Viola-Jones的一个Java实现，并能够加载OpenCV XML文件。 示例代码：http://www.oschina.net/code/snippet_12_2033Java视觉处理库 JavaCVJavaCV 提供了在计算机视觉领域的封装库，包括：OpenCV、ARToolKitPlus、libdc1394 2.x 、PGR FlyCapture和FFmpeg。此外，该工具可以很容易地使用Java平台的功能。 JavaCV还带有硬件加速的全屏幕图像显示（CanvasFrame），易于在多个内核中执行并行代码（并...运动检测程序 QMotionQMotion 是一个采用 OpenCV 开发的运动检测程序，基于 QT。视频监控系统 OpenVSSOpenVSS - 开放平台的视频监控系统 - 是一个系统级别的视频监控软件视频分析框架（VAF）的视频分析与检索和播放服务，记录和索引技术。它被设计成插件式的支持多摄像头平台，多分析仪模块（OpenCV的集成），以及多核心架构。手势识别 hand-gesture-detection手势识别，用OpenCV实现人脸检测识别 mcvai-tracking提供人脸检测、识别与检测特定人脸的功能，示例代码 cvReleaseImage( &gray ); cvReleaseMemStorage(&storage); cvReleaseHaarClassifierCascade(&cascade);...人脸检测与跟踪库 asmlibraryActive Shape Model Library (ASMLibrary©) SDK, 用OpenCV开发，用于人脸检测与跟踪。Lua视觉开发库 libecvECV 是 lua 的计算机视觉开发库(目前只提供linux支持)OpenCV的.Net封装 OpenCVSharpOpenCVSharp 是一个OpenCV的.Net wrapper，应用最新的OpenCV库开发，使用习惯比EmguCV更接近原始的OpenCV，有详细的使用样例供参考。3D视觉库 fvision2010基于OpenCV构建的图像处理和3D视觉库。 示例代码： ImageSequenceReaderFactory factory; ImageSequenceReader* reader = factory.pathRegex(\"c:/a/im_%03d.jpg\", 0, 20); //ImageSequenceReader*reader = factory.avi(\"a.avi\"); if (reader == NULL) { ...基于QT的计算机视觉库 QVision基于 QT 的面向对象的多平台计算机视觉库。可以方便的创建图形化应用程序，算法库主要从 OpenCV，GSL，CGAL，IPP，Octave 等高性能库借鉴而来。图像特征提取 cvBlobcvBlob 是计算机视觉应用中在二值图像里寻找连通域的库.能够执行连通域分析与特征提取.实时图像/视频处理滤波开发包 GShowGShow is a real-time image/video processing filter development kit. It successfully integrates DirectX11 with DirectShow framework. So it has the followingfeatures: GShow 是实时 图像/视频 处理滤波开发包，集成DiretX11。...视频捕获 API VideoManVideoMan 提供一组视频捕获 API 。支持多种视频流同时输入（视频传输线、USB摄像头和视频文件等）。能利用 OpenGL 对输入进行处理，方便的与 OpenCV，CUDA 等集成开发计算机视觉系统。开放模式识别项目 OpenPRPattern Recognition project（开放模式识别项目），致力于开发出一套包含图像处理、计算机视觉、自然语言处理、模式识别、机器学习和相关领域算法的函数库。OpenCV的Python封装 pyopencvOpenCV的Python封装，主要特性包括： 提供与OpenCV 2.x中最新的C++接口极为相似的Python接口，并且包括C++中不包括的C接口 提供对OpenCV 2.x中所有主要部件的绑定：CxCORE (almost complete), CxFLANN (complete), Cv (complete),CvAux (C++ part almost...视觉快速开发平台 qcv计算机视觉快速开发平台，提供测试框架，使开发者可以专注于算法研究。图像捕获 libv4l2cam对函数库v412的封装，从网络摄像头等硬件获得图像数据，支持YUYV裸数据输出和BGR24的OpenCV  IplImage输出计算机视觉算法 OpenVIDIAOpenVIDIA projects implement computer vision algorithms running on on graphics hardware such as single or multiple graphics processing units(GPUs) usingOpenGL, Cg and CUDA-C. Some samples will soon support OpenCL and Direct Compute API&apos;...高斯模型点集配准算法 gmmreg实现了基于混合高斯模型的点集配准算法，该算法描述在论文： A Robust Algorithm for Point Set Registration Using Mixture of Gaussians, Bing Jian and Baba C. Vemuri. ，实现了C++/Matlab/Python接口...模式识别和视觉库 RAVLRecognition And Vision Library (RAVL) 是一个通用 C++ 库，包含计算机视觉、模式识别等模块。图像处理和计算机视觉常用算法库 LTI-LibLTI-Lib 是一个包含图像处理和计算机视觉常用算法和数据结构的面向对象库，提供 Windows 下的 VC 版本和 Linux 下的 gcc 版本，主要包含以下几方面内容： 1、线性代数 2、聚类分析 3、图像处理 4、可视化和绘图工具OpenCV优化 opencv-dsp-acceleration优化了OpenCV库在DSP上的速度。C++计算机视觉库 Integrating Vision ToolkitIntegrating Vision Toolkit (IVT) 是一个强大而迅速的C++计算机视觉库，拥有易用的接口和面向对象的架构，并且含有自己的一套跨平台GUI组件，另外可以选择集成OpenCV计算机视觉和机器人技术的工具包 EGTThe Epipolar Geometry Toolbox (EGT) is a toolbox designed for Matlab (by Mathworks Inc.). EGT provides a wide set of functions to approach computer visionand robotics problems with single and multiple views, and with different vision se...OpenCV的扩展库 ImageNetsImageNets 是对OpenCV 的扩展，提供对机器人视觉算法方面友好的支持，使用Nokia的QT编写界面。libvideogfx视频处理、计算机视觉和计算机图形学的快速开发库。Matlab计算机视觉包 mVisionMatlab 的计算机视觉包，包含用于观察结果的 GUI 组件，貌似也停止开发了，拿来做学习用挺不错的。Scilab的计算机视觉库 SIPSIP 是 Scilab（一种免费的类Matlab编程环境）的图像处理和计算机视觉库。SIP 可以读写 JPEG/PNG/BMP 格式的图片。具备图像滤波、分割、边缘检测、形态学处理和形状分析等功能。STAIR Vision LibrarySTAIR Vision Library (SVL) 最初是为支持斯坦福智能机器人设计的，提供对计算机视觉、机器学习和概率统计模。视觉相关网站今天的主要任务就是和大家分享一些鄙人收藏的认为相当研究价值的网页：Oxford大牛：Andrew Zisserman，http://www.robots.ox.ac.uk/~vgg/hzbook/code/，此人主要研究多幅图像的几何学，该网站提供了部分工具，相当实用，还有例子西澳大利亚大学的Peter Kovesi：http://www.csse.uwa.edu.au/~pk/research/matlabfns/，提供了一些基本的matlab工具，主要内容涉及Computer Vision, Image ProcessingCMU：http://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.html,该网站是我的最爱，尤其后面这个地址http://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/v-groups.html，在这里提供了世界各地机构、大学在Computer Vision所涉及各领域的研究情况，包括Image Processing, Machine Vision，我后来也是通过它连接到了很多国外的网站Cambridge：http://mi.eng.cam.ac.uk/milab.html，这是剑桥大学的机器智能实验室，里面有三个小组，Computer Vision & Robotics, Machine Intelligence, Speech，目前为止，Computer Vision & Robotics的一些研究成果对我日后的帮助可能会比较大，所以在此提及大量计算机视觉方面的原版电子书：http://homepages.inf.ed.ac.uk/rbf/CVonline/books.htm，我今天先下了本Zisserman的书，呵呵，国外的原版书，虽然都是比较老的，但是对于基础的理解学习还是很有帮助的，至于目前的研究现状只能通过论文或者一些研究小组的网站stanford：http://ai.stanford.edu/~asaxena/reconstruction3d/，这个网站是Andrew N.G老师和一个印度阿三的博士一起维护的，主要对于单张照片的三维重建，尤其他有个网页make3d.stanford.edu可以让你自己上传你的照片，通过网站来重建三维模型，这个网站对于刚开始接触Computer Vision的我来说，如获至宝，但有个致命问题就是make3d已经无法注册，我也多次给Andrew和印度阿三email，至今未回，郁闷，要是有这个网站的帐号，那还是相当爽的，不知道是不是由于他们的邮箱把我的email当成垃圾邮件过滤，哎，但这个stanford网站的贡献主要是代码，有很多computer vision的基础工具，貌似40M左右，全都是基于matlab的caltech：http://www.vision.caltech.edu/bouguetj/calib_doc/，这是我们Computer Vision老师课件上的连接，主要是用于摄像机标定的工具集，当然也有涉及对标定图像三维重建的前期处理过程JP Tarel：http://perso.lcpc.fr/tarel.jean-philippe/，这是他的个人主页，也是目前为止我发的email中，唯一一个给我回信的老外，因为我需要重建练习的正是他的图片集，我读过他的论文，但没有涉及代码的内容，再加上又是94年以前的论文，很多相关的引文，我都无法下载，在我的再三追问下，Tarel教授只告诉我，你可以按照我的那篇论文对足球进行重建，可是...你知道吗，你有很多图像处理的引文都下不了了，我只知道你通过那篇文章做了图像的预处理，根本不知道具体过程，当然我有幸找到过一篇90左右的论文，讲的是region-based segmentation，可是这文章里所有引文又是找不到的....悲剧的人生开源软件网站：www.sourceforge.net机器视觉开源代码集合一、特征提取Feature Extraction：SIFT [1] [Demo program][SIFT Library] [VLFeat]PCA-SIFT [2] [Project]Affine-SIFT [3] [Project]SURF [4] [OpenSURF] [Matlab Wrapper]Affine Covariant Features [5] [Oxford project]MSER [6] [Oxford project] [VLFeat]Geometric Blur [7] [Code]Local Self-Similarity Descriptor [8] [Oxford implementation]Global and Efficient Self-Similarity [9] [Code]Histogram of Oriented Graidents [10] [INRIA Object Localization Toolkit] [OLT toolkit for Windows]GIST [11] [Project]Shape Context [12] [Project]Color Descriptor [13] [Project]Pyramids of Histograms of Oriented Gradients [Code]Space-Time Interest Points (STIP) [14][Project] [Code]Boundary Preserving Dense Local Regions [15][Project]Weighted Histogram[Code]Histogram-based Interest Points Detectors[Paper][Code]An OpenCV - C++ implementation of Local Self Similarity Descriptors [Project]Fast Sparse Representation with Prototypes[Project]Corner Detection [Project]AGAST Corner Detector: faster than FAST and even FAST-ER[Project]Real-time Facial Feature Detection using Conditional Regression Forests[Project]Global and Efficient Self-Similarity for Object Classification and Detection[code]WαSH: Weighted α-Shapes for Local Feature Detection[Project]HOG[Project]Online Selection of Discriminative Tracking Features[Project]二、图像分割Image Segmentation：Normalized Cut [1] [Matlab code]Gerg Mori’ Superpixel code [2] [Matlab code]Efficient Graph-based Image Segmentation [3] [C++ code] [Matlab wrapper]Mean-Shift Image Segmentation [4] [EDISON C++ code] [Matlab wrapper]OWT-UCM Hierarchical Segmentation [5] [Resources]Turbepixels [6] [Matlab code 32bit] [Matlab code 64bit] [Updated code]Quick-Shift [7] [VLFeat]SLIC Superpixels [8] [Project]Segmentation by Minimum Code Length [9] [Project]Biased Normalized Cut [10] [Project]Segmentation Tree [11-12] [Project]Entropy Rate Superpixel Segmentation [13] [Code]Fast Approximate Energy Minimization via Graph Cuts[Paper][Code]Efﬁcient Planar Graph Cuts with Applications in Computer Vision[Paper][Code]Isoperimetric Graph Partitioning for Image Segmentation[Paper][Code]Random Walks for Image Segmentation[Paper][Code]Blossom V: A new implementation of a minimum cost perfect matching algorithm[Code]An Experimental Comparison of Min-Cut/Max-Flow Algorithms for Energy Minimization in Computer Vision[Paper][Code]Geodesic Star Convexity for Interactive Image Segmentation[Project]Contour Detection and Image Segmentation Resources[Project][Code]Biased Normalized Cuts[Project]Max-flow/min-cut[Project]Chan-Vese Segmentation using Level Set[Project]A Toolbox of Level Set Methods[Project]Re-initialization Free Level Set Evolution via Reaction Diffusion[Project]Improved C-V active contour model[Paper][Code]A Variational Multiphase Level Set Approach to Simultaneous Segmentation and Bias Correction[Paper][Code]Level Set Method Research by Chunming Li[Project]ClassCut for Unsupervised Class Segmentation[code]SEEDS: Superpixels Extracted via Energy-Driven Sampling [Project][other]三、目标检测Object Detection：A simple object detector with boosting [Project]INRIA Object Detection and Localization Toolkit [1] [Project]Discriminatively Trained Deformable Part Models [2] [Project]Cascade Object Detection with Deformable Part Models [3] [Project]Poselet [4] [Project]Implicit Shape Model [5] [Project]Viola and Jones’s Face Detection [6] [Project]Bayesian Modelling of Dyanmic Scenes for Object Detection[Paper][Code]Hand detection using multiple proposals[Project]Color Constancy, Intrinsic Images, and Shape Estimation[Paper][Code]Discriminatively trained deformable part models[Project]Gradient Response Maps for Real-Time Detection of Texture-Less Objects: LineMOD [Project]Image Processing On Line[Project]Robust Optical Flow Estimation[Project]Where's Waldo: Matching People in Images of Crowds[Project]Scalable Multi-class Object Detection[Project]Class-Specific Hough Forests for Object Detection[Project]Deformed Lattice Detection In Real-World Images[Project]Discriminatively trained deformable part models[Project]四、显著性检测Saliency Detection：Itti, Koch, and Niebur’ saliency detection [1] [Matlab code]Frequency-tuned salient region detection [2] [Project]Saliency detection using maximum symmetric surround [3] [Project]Attention via Information Maximization [4] [Matlab code]Context-aware saliency detection [5] [Matlab code]Graph-based visual saliency [6] [Matlab code]Saliency detection: A spectral residual approach. [7] [Matlab code]Segmenting salient objects from images and videos. [8] [Matlab code]Saliency Using Natural statistics. [9] [Matlab code]Discriminant Saliency for Visual Recognition from Cluttered Scenes. [10] [Code]Learning to Predict Where Humans Look [11] [Project]Global Contrast based Salient Region Detection [12] [Project]Bayesian Saliency via Low and Mid Level Cues[Project]Top-Down Visual Saliency via Joint CRF and Dictionary Learning[Paper][Code]Saliency Detection: A Spectral Residual Approach[Code]五、图像分类、聚类Image Classification, ClusteringPyramid Match [1] [Project]Spatial Pyramid Matching [2] [Code]Locality-constrained Linear Coding [3] [Project] [Matlab code]Sparse Coding [4] [Project] [Matlab code]Texture Classification [5] [Project]Multiple Kernels for Image Classification [6] [Project]Feature Combination [7] [Project]SuperParsing [Code]Large Scale Correlation Clustering Optimization[Matlab code]Detecting and Sketching the Common[Project]Self-Tuning Spectral Clustering[Project][Code]User Assisted Separation of Reflections from a Single Image Using a Sparsity Prior[Paper][Code]Filters for Texture Classification[Project]Multiple Kernel Learning for Image Classification[Project]SLIC Superpixels[Project]六、抠图Image MattingA Closed Form Solution to Natural Image Matting [Code]Spectral Matting [Project]Learning-based Matting [Code]七、目标跟踪Object Tracking：A Forest of Sensors - Tracking Adaptive Background Mixture Models [Project]Object Tracking via Partial Least Squares Analysis[Paper][Code]Robust Object Tracking with Online Multiple Instance Learning[Paper][Code]Online Visual Tracking with Histograms and Articulating Blocks[Project]Incremental Learning for Robust Visual Tracking[Project]Real-time Compressive Tracking[Project]Robust Object Tracking via Sparsity-based Collaborative Model[Project]Visual Tracking via Adaptive Structural Local Sparse Appearance Model[Project]Online Discriminative Object Tracking with Local Sparse Representation[Paper][Code]Superpixel Tracking[Project]Learning Hierarchical Image Representation with Sparsity, Saliency and Locality[Paper][Code]Online Multiple Support Instance Tracking [Paper][Code]Visual Tracking with Online Multiple Instance Learning[Project]Object detection and recognition[Project]Compressive Sensing Resources[Project]Robust Real-Time Visual Tracking using Pixel-Wise Posteriors[Project]Tracking-Learning-Detection[Project][OpenTLD/C++ Code]the HandVu：vision-based hand gesture interface[Project]Learning Probabilistic Non-Linear Latent Variable Models for Tracking Complex Activities[Project]八、Kinect：Kinect toolbox[Project]OpenNI[Project]zouxy09 CSDN Blog[Resource]FingerTracker 手指跟踪[code]九、3D相关：3D Reconstruction of a Moving Object[Paper] [Code]Shape From Shading Using Linear Approximation[Code]Combining Shape from Shading and Stereo Depth Maps[Project][Code]Shape from Shading: A Survey[Paper][Code]A Spatio-Temporal Descriptor based on 3D Gradients (HOG3D)[Project][Code]Multi-camera Scene Reconstruction via Graph Cuts[Paper][Code]A Fast Marching Formulation of Perspective Shape from Shading under Frontal Illumination[Paper][Code]Reconstruction:3D Shape, Illumination, Shading, Reflectance, Texture[Project]Monocular Tracking of 3D Human Motion with a Coordinated Mixture of Factor Analyzers[Code]Learning 3-D Scene Structure from a Single Still Image[Project十、机器学习算法：Matlab class for computing Approximate Nearest Nieghbor (ANN) [Matlab class providing interface toANN library]Random Sampling[code]Probabilistic Latent Semantic Analysis (pLSA)[Code]FASTANN and FASTCLUSTER for approximate k-means (AKM)[Project]Fast Intersection / Additive Kernel SVMs[Project]SVM[Code]Ensemble learning[Project]Deep Learning[Net]Deep Learning Methods for Vision[Project]Neural Network for Recognition of Handwritten Digits[Project]Training a deep autoencoder or a classifier on MNIST digits[Project]THE MNIST DATABASE of handwritten digits[Project]Ersatz：deep neural networks in the cloud[Project]Deep Learning [Project]sparseLM : Sparse Levenberg-Marquardt nonlinear least squares in C/C++[Project]Weka 3: Data Mining Software in Java[Project]Invited talk \"A Tutorial on Deep Learning\" by Dr. Kai Yu (余凯)[Video]CNN - Convolutional neural network class[Matlab Tool]Yann LeCun's Publications[Wedsite]LeNet-5, convolutional neural networks[Project]Training a deep autoencoder or a classifier on MNIST digits[Project]Deep Learning 大牛Geoffrey E. Hinton's HomePage[Website]Multiple Instance Logistic Discriminant-based Metric Learning (MildML) and Logistic Discriminant-based Metric Learning (LDML)[Code]Sparse coding simulation software[Project]Visual Recognition and Machine Learning Summer School[Software]十一、目标、行为识别Object, Action Recognition：Action Recognition by Dense Trajectories[Project][Code]Action Recognition Using a Distributed Representation of Pose and Appearance[Project]Recognition Using Regions[Paper][Code]2D Articulated Human Pose Estimation[Project]Fast Human Pose Estimation Using Appearance and Motion via Multi-Dimensional Boosting Regression[Paper][Code]Estimating Human Pose from Occluded Images[Paper][Code]Quasi-dense wide baseline matching[Project]ChaLearn Gesture Challenge: Principal motion: PCA-based reconstruction of motion histograms[Project]Real Time Head Pose Estimation with Random Regression Forests[Project]2D Action Recognition Serves 3D Human Pose Estimation[Project]A Hough Transform-Based Voting Framework for Action Recognition[Project]Motion Interchange Patterns for Action Recognition in Unconstrained Videos[Project]2D articulated human pose estimation software[Project]Learning and detecting shape models [code]Progressive Search Space Reduction for Human Pose Estimation[Project]Learning Non-Rigid 3D Shape from 2D Motion[Project]十二、图像处理：Distance Transforms of Sampled Functions[Project]The Computer Vision Homepage[Project]Efficient appearance distances between windows[code]Image Exploration algorithm[code]Motion Magnification 运动放大 [Project]Bilateral Filtering for Gray and Color Images 双边滤波器 [Project]A Fast Approximation of the Bilateral Filter using a Signal Processing Approach [Project]十三、一些实用工具：EGT: a Toolbox for Multiple View Geometry and Visual Servoing[Project] [Code]a development kit of matlab mex functions for OpenCV library[Project]Fast Artificial Neural Network Library[Project]十四、人手及指尖检测与识别：finger-detection-and-gesture-recognition [Code]Hand and Finger Detection using JavaCV[Project]Hand and fingers detection[Code]十五、场景解释：Nonparametric Scene Parsing via Label Transfer [Project]十六、光流Optical flow：High accuracy optical flow using a theory for warping [Project]Dense Trajectories Video Description [Project]SIFT Flow: Dense Correspondence across Scenes and its Applications[Project]KLT: An Implementation of the Kanade-Lucas-Tomasi Feature Tracker [Project]Tracking Cars Using Optical Flow[Project]Secrets of optical flow estimation and their principles[Project]implmentation of the Black and Anandan dense optical flow method[Project]Optical Flow Computation[Project]Beyond Pixels: Exploring New Representations and Applications for Motion Analysis[Project]A Database and Evaluation Methodology for Optical Flow[Project]optical flow relative[Project]Robust Optical Flow Estimation [Project]optical flow[Project]十七、图像检索Image Retrieval：Semi-Supervised Distance Metric Learning for Collaborative Image Retrieval [Paper][code]十八、马尔科夫随机场Markov Random Fields：Markov Random Fields for Super-Resolution [Project]A Comparative Study of Energy Minimization Methods for Markov Random Fields with Smoothness-Based Priors [Project]十九、运动检测Motion detection：Moving Object Extraction, Using Models or Analysis of Regions [Project]Background Subtraction: Experiments and Improvements for ViBe [Project]A Self-Organizing Approach to Background Subtraction for Visual Surveillance Applications [Project]changedetection.net: A new change detection benchmark dataset[Project]ViBe - a powerful technique for background detection and subtraction in video sequences[Project]Background Subtraction Program[Project]Motion Detection Algorithms[Project]Stuttgart Artificial Background Subtraction Dataset[Project]Object Detection, Motion Estimation, and Tracking[Project]Feature Detection and DescriptionGeneral Libraries:VLFeat – Implementation of various feature descriptors (including SIFT, HOG, and LBP) and covariant feature detectors (including DoG, Hessian, Harris Laplace, Hessian Laplace, Multiscale Hessian, Multiscale Harris). Easy-to-use Matlab interface. See Modern features: Software – Slides providing a demonstration of VLFeat and also links to other software. Check also VLFeat hands-on session trainingOpenCV – Various implementations of modern feature detectors and descriptors (SIFT, SURF, FAST, BRIEF, ORB, FREAK, etc.)Fast Keypoint Detectors for Real-time Applications:FAST – High-speed corner detector implementation for a wide variety of platformsAGAST – Even faster than the FAST corner detector. A multi-scale version of this method is used for the BRISK descriptor (ECCV 2010).Binary Descriptors for Real-Time Applications:BRIEF – C++ code for a fast and accurate interest point descriptor (not invariant to rotations and scale) (ECCV 2010)ORB – OpenCV implementation of the Oriented-Brief (ORB) descriptor (invariant to rotations, but not scale)BRISK – Efficient Binary descriptor invariant to rotations and scale. It includes a Matlab mex interface. (ICCV 2011)FREAK – Faster than BRISK (invariant to rotations and scale) (CVPR 2012)SIFT and SURF Implementations:SIFT: VLFeat, OpenCV, Original code by David Lowe, GPU implementation, OpenSIFTSURF: Herbert Bay’s code, OpenCV, GPU-SURFOther Local Feature Detectors and Descriptors:VGG Affine Covariant features – Oxford code for various affine covariant feature detectors and descriptors.LIOP descriptor – Source code for the Local Intensity order Pattern (LIOP) descriptor (ICCV 2011).Local Symmetry Features – Source code for matching of local symmetry features under large variations in lighting, age, and rendering style (CVPR 2012).Global Image Descriptors:GIST – Matlab code for the GIST descriptorCENTRIST – Global visual descriptor for scene categorization and object detection (PAMI 2011)Feature Coding and PoolingVGG Feature Encoding Toolkit – Source code for various state-of-the-art feature encoding methods – including Standard hard encoding, Kernel codebook encoding, Locality-constrained linear encoding, and Fisher kernel encoding.Spatial Pyramid Matching – Source code for feature pooling based on spatial pyramid matching (widely used for image classification)Convolutional Nets and Deep LearningEBLearn – C++ Library for Energy-Based Learning. It includes several demos and step-by-step instructions to train classifiers based on convolutional neural networks.Torch7 – Provides a matlab-like environment for state-of-the-art machine learning algorithms, including a fast implementation of convolutional neural networks.Deep Learning - Various links for deep learning software.Part-Based ModelsDeformable Part-based Detector – Library provided by the authors of the original paper (state-of-the-art in PASCAL VOC detection task)Efficient Deformable Part-Based Detector – Branch-and-Bound implementation for a deformable part-based detector.Accelerated Deformable Part Model – Efficient implementation of a method that achieves the exact same performance of deformable part-based detectors but with significant acceleration (ECCV 2012).Coarse-to-Fine Deformable Part Model – Fast approach for deformable object detection (CVPR 2011).Poselets – C++ and Matlab versions for object detection based on poselets.Part-based Face Detector and Pose Estimation – Implementation of a unified approach for face detection, pose estimation, and landmark localization (CVPR 2012).Attributes and Semantic FeaturesRelative Attributes – Modified implementation of RankSVM to train Relative Attributes (ICCV 2011).Object Bank – Implementation of object bank semantic features (NIPS 2010). See also ActionBankClassemes, Picodes, and Meta-class features – Software for extracting high-level image descriptors (ECCV 2010, NIPS 2011, CVPR 2012).Large-Scale LearningAdditive Kernels – Source code for fast additive kernel SVM classifiers (PAMI 2013).LIBLINEAR – Library for large-scale linear SVM classification.VLFeat – Implementation for Pegasos SVM and Homogeneous Kernel map.Fast Indexing and Image RetrievalFLANN – Library for performing fast approximate nearest neighbor.Kernelized LSH – Source code for Kernelized Locality-Sensitive Hashing (ICCV 2009).ITQ Binary codes – Code for generation of small binary codes using Iterative Quantization and other baselines such as Locality-Sensitive-Hashing (CVPR 2011).INRIA Image Retrieval – Efficient code for state-of-the-art large-scale image retrieval (CVPR 2011).Object DetectionSee Part-based Models and Convolutional Nets above.Pedestrian Detection at 100fps – Very fast and accurate pedestrian detector (CVPR 2012).Caltech Pedestrian Detection Benchmark – Excellent resource for pedestrian detection, with various links for state-of-the-art implementations.OpenCV – Enhanced implementation of Viola&Jones real-time object detector, with trained models for face detection.Efficient Subwindow Search – Source code for branch-and-bound optimization for efficient object localization (CVPR 2008).3D RecognitionPoint-Cloud Library – Library for 3D image and point cloud processing.Action RecognitionActionBank – Source code for action recognition based on the ActionBank representation (CVPR 2012).STIP Features – software for computing space-time interest point descriptorsIndependent Subspace Analysis – Look for Stacked ISA for Videos (CVPR 2011)Velocity Histories of Tracked Keypoints - C++ code for activity recognition using the velocity histories of tracked keypoints (ICCV 2009)DatasetsAttributesAnimals with Attributes – 30,475 images of 50 animals classes with 6 pre-extracted feature representations for each image.aYahoo and aPascal – Attribute annotations for images collected from Yahoo and Pascal VOC 2008.FaceTracer – 15,000 faces annotated with 10 attributes and fiducial points.PubFig – 58,797 face images of 200 people with 73 attribute classifier outputs.LFW – 13,233 face images of 5,749 people with 73 attribute classifier outputs.Human Attributes – 8,000 people with annotated attributes. Check also this link for another dataset of human attributes.SUN Attribute Database – Large-scale scene attribute database with a taxonomy of 102 attributes.ImageNet Attributes – Variety of attribute labels for the ImageNet dataset.Relative attributes – Data for OSR and a subset of PubFig datasets. Check also this link for the WhittleSearch data.Attribute Discovery Dataset – Images of shopping categories associated with textual descriptions.Fine-grained Visual CategorizationCaltech-UCSD Birds Dataset – Hundreds of bird categories with annotated parts and attributes.Stanford Dogs Dataset – 20,000 images of 120 breeds of dogs from around the world.Oxford-IIIT Pet Dataset – 37 category pet dataset with roughly 200 images for each class. Pixel level trimap segmentation is included.Leeds Butterfly Dataset – 832 images of 10 species of butterflies.Oxford Flower Dataset – Hundreds of flower categories.Face DetectionFDDB – UMass face detection dataset and benchmark (5,000+ faces)CMU/MIT – Classical face detection dataset.Face RecognitionFace Recognition Homepage – Large collection of face recognition datasets.LFW – UMass unconstrained face recognition dataset (13,000+ face images).NIST Face Homepage – includes face recognition grand challenge (FRGC), vendor tests (FRVT) and others.CMU Multi-PIE – contains more than 750,000 images of 337 people, with 15 different views and 19 lighting conditions.FERET – Classical face recognition dataset.Deng Cai’s face dataset in Matlab Format – Easy to use if you want play with simple face datasets including Yale, ORL, PIE, and Extended Yale B.SCFace – Low-resolution face dataset captured from surveillance cameras.Handwritten DigitsMNIST – large dataset containing a training set of 60,000 examples, and a test set of 10,000 examples.Pedestrian DetectionCaltech Pedestrian Detection Benchmark – 10 hours of video taken from a vehicle,350K bounding boxes for about 2.3K unique pedestrians.INRIA Person Dataset – Currently one of the most popular pedestrian detection datasets.ETH Pedestrian Dataset – Urban dataset captured from a stereo rig mounted on a stroller.TUD-Brussels Pedestrian Dataset – Dataset with image pairs recorded in an crowded urban setting with an onboard camera.PASCAL Human Detection – One of 20 categories in PASCAL VOC detection challenges.USC Pedestrian Dataset – Small dataset captured from surveillance cameras.Generic Object RecognitionImageNet – Currently the largest visual recognition dataset in terms of number of categories and images.Tiny Images – 80 million 32x32 low resolution images.Pascal VOC – One of the most influential visual recognition datasets.Caltech 101 / Caltech 256 – Popular image datasets containing 101 and 256 object categories, respectively.MIT LabelMe – Online annotation tool for building computer vision databases.Scene RecognitionMIT SUN Dataset – MIT scene understanding dataset.UIUC Fifteen Scene Categories – Dataset of 15 natural scene categories.Feature Detection and DescriptionVGG Affine Dataset – Widely used dataset for measuring performance of feature detection and description. CheckVLBenchmarks for an evaluation framework.Action RecognitionBenchmarking Activity Recognition – CVPR 2012 tutorial covering various datasets for action recognition.RGBD RecognitionRGB-D Object Dataset – Dataset containing 300 common household objects 一、特征提取Feature Extraction：SIFT [1] [Demo program][SIFT Library] [VLFeat]PCA-SIFT [2] [Project]Affine-SIFT [3] [Project]SURF [4] [OpenSURF] [Matlab Wrapper]Affine Covariant Features [5] [Oxford project]MSER [6] [Oxford project] [VLFeat]Geometric Blur [7] [Code]Local Self-Similarity Descriptor [8] [Oxford implementation]Global and Efficient Self-Similarity [9] [Code]Histogram of Oriented Graidents [10] [INRIA Object Localization Toolkit] [OLT toolkit for Windows]GIST [11] [Project]Shape Context [12] [Project]Color Descriptor [13] [Project]Pyramids of Histograms of Oriented Gradients [Code]Space-Time Interest Points (STIP) [14][Project] [Code]Boundary Preserving Dense Local Regions [15][Project]Weighted Histogram[Code]Histogram-based Interest Points Detectors[Paper][Code]An OpenCV - C++ implementation of Local Self Similarity Descriptors [Project]Fast Sparse Representation with Prototypes[Project]Corner Detection [Project]AGAST Corner Detector: faster than FAST and even FAST-ER[Project]Real-time Facial Feature Detection using Conditional Regression Forests[Project]Global and Efficient Self-Similarity for Object Classification and Detection[code]WαSH: Weighted α-Shapes for Local Feature Detection[Project]HOG[Project]Online Selection of Discriminative Tracking Features[Project]二、图像分割Image Segmentation：Normalized Cut [1] [Matlab code]Gerg Mori’ Superpixel code [2] [Matlab code]Efficient Graph-based Image Segmentation [3] [C++ code] [Matlab wrapper]Mean-Shift Image Segmentation [4] [EDISON C++ code] [Matlab wrapper]OWT-UCM Hierarchical Segmentation [5] [Resources]Turbepixels [6] [Matlab code 32bit] [Matlab code 64bit] [Updated code]Quick-Shift [7] [VLFeat]SLIC Superpixels [8] [Project]Segmentation by Minimum Code Length [9] [Project]Biased Normalized Cut [10] [Project]Segmentation Tree [11-12] [Project]Entropy Rate Superpixel Segmentation [13] [Code]Fast Approximate Energy Minimization via Graph Cuts[Paper][Code]Efﬁcient Planar Graph Cuts with Applications in Computer Vision[Paper][Code]Isoperimetric Graph Partitioning for Image Segmentation[Paper][Code]Random Walks for Image Segmentation[Paper][Code]Blossom V: A new implementation of a minimum cost perfect matching algorithm[Code]An Experimental Comparison of Min-Cut/Max-Flow Algorithms for Energy Minimization in Computer Vision[Paper][Code]Geodesic Star Convexity for Interactive Image Segmentation[Project]Contour Detection and Image Segmentation Resources[Project][Code]Biased Normalized Cuts[Project]Max-flow/min-cut[Project]Chan-Vese Segmentation using Level Set[Project]A Toolbox of Level Set Methods[Project]Re-initialization Free Level Set Evolution via Reaction Diffusion[Project]Improved C-V active contour model[Paper][Code]A Variational Multiphase Level Set Approach to Simultaneous Segmentation and Bias Correction[Paper][Code]Level Set Method Research by Chunming Li[Project]ClassCut for Unsupervised Class Segmentation[code]SEEDS: Superpixels Extracted via Energy-Driven Sampling [Project][other]三、目标检测Object Detection：A simple object detector with boosting [Project]INRIA Object Detection and Localization Toolkit [1] [Project]Discriminatively Trained Deformable Part Models [2] [Project]Cascade Object Detection with Deformable Part Models [3] [Project]Poselet [4] [Project]Implicit Shape Model [5] [Project]Viola and Jones’s Face Detection [6] [Project]Bayesian Modelling of Dyanmic Scenes for Object Detection[Paper][Code]Hand detection using multiple proposals[Project]Color Constancy, Intrinsic Images, and Shape Estimation[Paper][Code]Discriminatively trained deformable part models[Project]Gradient Response Maps for Real-Time Detection of Texture-Less Objects: LineMOD [Project]Image Processing On Line[Project]Robust Optical Flow Estimation[Project]Where's Waldo: Matching People in Images of Crowds[Project]Scalable Multi-class Object Detection[Project]Class-Specific Hough Forests for Object Detection[Project]Deformed Lattice Detection In Real-World Images[Project]Discriminatively trained deformable part models[Project]四、显著性检测Saliency Detection：Itti, Koch, and Niebur’ saliency detection [1] [Matlab code]Frequency-tuned salient region detection [2] [Project]Saliency detection using maximum symmetric surround [3] [Project]Attention via Information Maximization [4] [Matlab code]Context-aware saliency detection [5] [Matlab code]Graph-based visual saliency [6] [Matlab code]Saliency detection: A spectral residual approach. [7] [Matlab code]Segmenting salient objects from images and videos. [8] [Matlab code]Saliency Using Natural statistics. [9] [Matlab code]Discriminant Saliency for Visual Recognition from Cluttered Scenes. [10] [Code]Learning to Predict Where Humans Look [11] [Project]Global Contrast based Salient Region Detection [12] [Project]Bayesian Saliency via Low and Mid Level Cues[Project]Top-Down Visual Saliency via Joint CRF and Dictionary Learning[Paper][Code]Saliency Detection: A Spectral Residual Approach[Code]五、图像分类、聚类Image Classification, ClusteringPyramid Match [1] [Project]Spatial Pyramid Matching [2] [Code]Locality-constrained Linear Coding [3] [Project] [Matlab code]Sparse Coding [4] [Project] [Matlab code]Texture Classification [5] [Project]Multiple Kernels for Image Classification [6] [Project]Feature Combination [7] [Project]SuperParsing [Code]Large Scale Correlation Clustering Optimization[Matlab code]Detecting and Sketching the Common[Project]Self-Tuning Spectral Clustering[Project][Code]User Assisted Separation of Reflections from a Single Image Using a Sparsity Prior[Paper][Code]Filters for Texture Classification[Project]Multiple Kernel Learning for Image Classification[Project]SLIC Superpixels[Project]六、抠图Image MattingA Closed Form Solution to Natural Image Matting [Code]Spectral Matting [Project]Learning-based Matting [Code]七、目标跟踪Object Tracking：A Forest of Sensors - Tracking Adaptive Background Mixture Models [Project]Object Tracking via Partial Least Squares Analysis[Paper][Code]Robust Object Tracking with Online Multiple Instance Learning[Paper][Code]Online Visual Tracking with Histograms and Articulating Blocks[Project]Incremental Learning for Robust Visual Tracking[Project]Real-time Compressive Tracking[Project]Robust Object Tracking via Sparsity-based Collaborative Model[Project]Visual Tracking via Adaptive Structural Local Sparse Appearance Model[Project]Online Discriminative Object Tracking with Local Sparse Representation[Paper][Code]Superpixel Tracking[Project]Learning Hierarchical Image Representation with Sparsity, Saliency and Locality[Paper][Code]Online Multiple Support Instance Tracking [Paper][Code]Visual Tracking with Online Multiple Instance Learning[Project]Object detection and recognition[Project]Compressive Sensing Resources[Project]Robust Real-Time Visual Tracking using Pixel-Wise Posteriors[Project]Tracking-Learning-Detection[Project][OpenTLD/C++ Code]the HandVu：vision-based hand gesture interface[Project]Learning Probabilistic Non-Linear Latent Variable Models for Tracking Complex Activities[Project]八、Kinect：Kinect toolbox[Project]OpenNI[Project]zouxy09 CSDN Blog[Resource]FingerTracker 手指跟踪[code]九、3D相关：3D Reconstruction of a Moving Object[Paper] [Code]Shape From Shading Using Linear Approximation[Code]Combining Shape from Shading and Stereo Depth Maps[Project][Code]Shape from Shading: A Survey[Paper][Code]A Spatio-Temporal Descriptor based on 3D Gradients (HOG3D)[Project][Code]Multi-camera Scene Reconstruction via Graph Cuts[Paper][Code]A Fast Marching Formulation of Perspective Shape from Shading under Frontal Illumination[Paper][Code]Reconstruction:3D Shape, Illumination, Shading, Reflectance, Texture[Project]Monocular Tracking of 3D Human Motion with a Coordinated Mixture of Factor Analyzers[Code]Learning 3-D Scene Structure from a Single Still Image[Project十、机器学习算法：Matlab class for computing Approximate Nearest Nieghbor (ANN) [Matlab class providing interface toANN library]Random Sampling[code]Probabilistic Latent Semantic Analysis (pLSA)[Code]FASTANN and FASTCLUSTER for approximate k-means (AKM)[Project]Fast Intersection / Additive Kernel SVMs[Project]SVM[Code]Ensemble learning[Project]Deep Learning[Net]Deep Learning Methods for Vision[Project]Neural Network for Recognition of Handwritten Digits[Project]Training a deep autoencoder or a classifier on MNIST digits[Project]THE MNIST DATABASE of handwritten digits[Project]Ersatz：deep neural networks in the cloud[Project]Deep Learning [Project]sparseLM : Sparse Levenberg-Marquardt nonlinear least squares in C/C++[Project]Weka 3: Data Mining Software in Java[Project]Invited talk \"A Tutorial on Deep Learning\" by Dr. Kai Yu (余凯)[Video]CNN - Convolutional neural network class[Matlab Tool]Yann LeCun's Publications[Wedsite]LeNet-5, convolutional neural networks[Project]Training a deep autoencoder or a classifier on MNIST digits[Project]Deep Learning 大牛Geoffrey E. Hinton's HomePage[Website]Multiple Instance Logistic Discriminant-based Metric Learning (MildML) and Logistic Discriminant-based Metric Learning (LDML)[Code]Sparse coding simulation software[Project]Visual Recognition and Machine Learning Summer School[Software]十一、目标、行为识别Object, Action Recognition：Action Recognition by Dense Trajectories[Project][Code]Action Recognition Using a Distributed Representation of Pose and Appearance[Project]Recognition Using Regions[Paper][Code]2D Articulated Human Pose Estimation[Project]Fast Human Pose Estimation Using Appearance and Motion via Multi-Dimensional Boosting Regression[Paper][Code]Estimating Human Pose from Occluded Images[Paper][Code]Quasi-dense wide baseline matching[Project]ChaLearn Gesture Challenge: Principal motion: PCA-based reconstruction of motion histograms[Project]Real Time Head Pose Estimation with Random Regression Forests[Project]2D Action Recognition Serves 3D Human Pose Estimation[Project]A Hough Transform-Based Voting Framework for Action Recognition[Project]Motion Interchange Patterns for Action Recognition in Unconstrained Videos[Project]2D articulated human pose estimation software[Project]Learning and detecting shape models [code]Progressive Search Space Reduction for Human Pose Estimation[Project]Learning Non-Rigid 3D Shape from 2D Motion[Project]十二、图像处理：Distance Transforms of Sampled Functions[Project]The Computer Vision Homepage[Project]Efficient appearance distances between windows[code]Image Exploration algorithm[code]Motion Magnification 运动放大 [Project]Bilateral Filtering for Gray and Color Images 双边滤波器 [Project]A Fast Approximation of the Bilateral Filter using a Signal Processing Approach [Project]十三、一些实用工具：EGT: a Toolbox for Multiple View Geometry and Visual Servoing[Project] [Code]a development kit of matlab mex functions for OpenCV library[Project]Fast Artificial Neural Network Library[Project]十四、人手及指尖检测与识别：finger-detection-and-gesture-recognition [Code]Hand and Finger Detection using JavaCV[Project]Hand and fingers detection[Code]十五、场景解释：Nonparametric Scene Parsing via Label Transfer [Project]十六、光流Optical flow：High accuracy optical flow using a theory for warping [Project]Dense Trajectories Video Description [Project]SIFT Flow: Dense Correspondence across Scenes and its Applications[Project]KLT: An Implementation of the Kanade-Lucas-Tomasi Feature Tracker [Project]Tracking Cars Using Optical Flow[Project]Secrets of optical flow estimation and their principles[Project]implmentation of the Black and Anandan dense optical flow method[Project]Optical Flow Computation[Project]Beyond Pixels: Exploring New Representations and Applications for Motion Analysis[Project]A Database and Evaluation Methodology for Optical Flow[Project]optical flow relative[Project]Robust Optical Flow Estimation [Project]optical flow[Project]十七、图像检索Image Retrieval：Semi-Supervised Distance Metric Learning for Collaborative Image Retrieval [Paper][code]十八、马尔科夫随机场Markov Random Fields：Markov Random Fields for Super-Resolution [Project]A Comparative Study of Energy Minimization Methods for Markov Random Fields with Smoothness-Based Priors [Project]十九、运动检测Motion detection：Moving Object Extraction, Using Models or Analysis of Regions [Project]Background Subtraction: Experiments and Improvements for ViBe [Project]A Self-Organizing Approach to Background Subtraction for Visual Surveillance Applications [Project]changedetection.net: A new change detection benchmark dataset[Project]ViBe - a powerful technique for background detection and subtraction in video sequences[Project]Background Subtraction Program[Project]Motion Detection Algorithms[Project]Stuttgart Artificial Background Subtraction Dataset[Project]Object Detection, Motion Estimation, and Tracking[Project]Feature Detection and DescriptionGeneral Libraries:VLFeat – Implementation of various feature descriptors (including SIFT, HOG, and LBP) and covariant feature detectors (including DoG, Hessian, Harris Laplace, Hessian Laplace, Multiscale Hessian, Multiscale Harris). Easy-to-use Matlab interface. See Modern features: Software – Slides providing a demonstration of VLFeat and also links to other software. Check also VLFeat hands-on session trainingOpenCV – Various implementations of modern feature detectors and descriptors (SIFT, SURF, FAST, BRIEF, ORB, FREAK, etc.)Fast Keypoint Detectors for Real-time Applications:FAST – High-speed corner detector implementation for a wide variety of platformsAGAST – Even faster than the FAST corner detector. A multi-scale version of this method is used for the BRISK descriptor (ECCV 2010).Binary Descriptors for Real-Time Applications:BRIEF – C++ code for a fast and accurate interest point descriptor (not invariant to rotations and scale) (ECCV 2010)ORB – OpenCV implementation of the Oriented-Brief (ORB) descriptor (invariant to rotations, but not scale)BRISK – Efficient Binary descriptor invariant to rotations and scale. It includes a Matlab mex interface. (ICCV 2011)FREAK – Faster than BRISK (invariant to rotations and scale) (CVPR 2012)SIFT and SURF Implementations:SIFT: VLFeat, OpenCV, Original code by David Lowe, GPU implementation, OpenSIFTSURF: Herbert Bay’s code, OpenCV, GPU-SURFOther Local Feature Detectors and Descriptors:VGG Affine Covariant features – Oxford code for various affine covariant feature detectors and descriptors.LIOP descriptor – Source code for the Local Intensity order Pattern (LIOP) descriptor (ICCV 2011).Local Symmetry Features – Source code for matching of local symmetry features under large variations in lighting, age, and rendering style (CVPR 2012).Global Image Descriptors:GIST – Matlab code for the GIST descriptorCENTRIST – Global visual descriptor for scene categorization and object detection (PAMI 2011)Feature Coding and PoolingVGG Feature Encoding Toolkit – Source code for various state-of-the-art feature encoding methods – including Standard hard encoding, Kernel codebook encoding, Locality-constrained linear encoding, and Fisher kernel encoding.Spatial Pyramid Matching – Source code for feature pooling based on spatial pyramid matching (widely used for image classification)Convolutional Nets and Deep LearningEBLearn – C++ Library for Energy-Based Learning. It includes several demos and step-by-step instructions to train classifiers based on convolutional neural networks.Torch7 – Provides a matlab-like environment for state-of-the-art machine learning algorithms, including a fast implementation of convolutional neural networks.Deep Learning - Various links for deep learning software.Part-Based ModelsDeformable Part-based Detector – Library provided by the authors of the original paper (state-of-the-art in PASCAL VOC detection task)Efficient Deformable Part-Based Detector – Branch-and-Bound implementation for a deformable part-based detector.Accelerated Deformable Part Model – Efficient implementation of a method that achieves the exact same performance of deformable part-based detectors but with significant acceleration (ECCV 2012).Coarse-to-Fine Deformable Part Model – Fast approach for deformable object detection (CVPR 2011).Poselets – C++ and Matlab versions for object detection based on poselets.Part-based Face Detector and Pose Estimation – Implementation of a unified approach for face detection, pose estimation, and landmark localization (CVPR 2012).Attributes and Semantic FeaturesRelative Attributes – Modified implementation of RankSVM to train Relative Attributes (ICCV 2011).Object Bank – Implementation of object bank semantic features (NIPS 2010). See also ActionBankClassemes, Picodes, and Meta-class features – Software for extracting high-level image descriptors (ECCV 2010, NIPS 2011, CVPR 2012).Large-Scale LearningAdditive Kernels – Source code for fast additive kernel SVM classifiers (PAMI 2013).LIBLINEAR – Library for large-scale linear SVM classification.VLFeat – Implementation for Pegasos SVM and Homogeneous Kernel map.Fast Indexing and Image RetrievalFLANN – Library for performing fast approximate nearest neighbor.Kernelized LSH – Source code for Kernelized Locality-Sensitive Hashing (ICCV 2009).ITQ Binary codes – Code for generation of small binary codes using Iterative Quantization and other baselines such as Locality-Sensitive-Hashing (CVPR 2011).INRIA Image Retrieval – Efficient code for state-of-the-art large-scale image retrieval (CVPR 2011).Object DetectionSee Part-based Models and Convolutional Nets above.Pedestrian Detection at 100fps – Very fast and accurate pedestrian detector (CVPR 2012).Caltech Pedestrian Detection Benchmark – Excellent resource for pedestrian detection, with various links for state-of-the-art implementations.OpenCV – Enhanced implementation of Viola&Jones real-time object detector, with trained models for face detection.Efficient Subwindow Search – Source code for branch-and-bound optimization for efficient object localization (CVPR 2008).3D RecognitionPoint-Cloud Library – Library for 3D image and point cloud processing.Action RecognitionActionBank – Source code for action recognition based on the ActionBank representation (CVPR 2012).STIP Features – software for computing space-time interest point descriptorsIndependent Subspace Analysis – Look for Stacked ISA for Videos (CVPR 2011)Velocity Histories of Tracked Keypoints - C++ code for activity recognition using the velocity histories of tracked keypoints (ICCV 2009)DatasetsAttributesAnimals with Attributes – 30,475 images of 50 animals classes with 6 pre-extracted feature representations for each image.aYahoo and aPascal – Attribute annotations for images collected from Yahoo and Pascal VOC 2008.FaceTracer – 15,000 faces annotated with 10 attributes and fiducial points.PubFig – 58,797 face images of 200 people with 73 attribute classifier outputs.LFW – 13,233 face images of 5,749 people with 73 attribute classifier outputs.Human Attributes – 8,000 people with annotated attributes. Check also this link for another dataset of human attributes.SUN Attribute Database – Large-scale scene attribute database with a taxonomy of 102 attributes.ImageNet Attributes – Variety of attribute labels for the ImageNet dataset.Relative attributes – Data for OSR and a subset of PubFig datasets. Check also this link for the WhittleSearch data.Attribute Discovery Dataset – Images of shopping categories associated with textual descriptions.Fine-grained Visual CategorizationCaltech-UCSD Birds Dataset – Hundreds of bird categories with annotated parts and attributes.Stanford Dogs Dataset – 20,000 images of 120 breeds of dogs from around the world.Oxford-IIIT Pet Dataset – 37 category pet dataset with roughly 200 images for each class. Pixel level trimap segmentation is included.Leeds Butterfly Dataset – 832 images of 10 species of butterflies.Oxford Flower Dataset – Hundreds of flower categories.Face DetectionFDDB – UMass face detection dataset and benchmark (5,000+ faces)CMU/MIT – Classical face detection dataset.Face RecognitionFace Recognition Homepage – Large collection of face recognition datasets.LFW – UMass unconstrained face recognition dataset (13,000+ face images).NIST Face Homepage – includes face recognition grand challenge (FRGC), vendor tests (FRVT) and others.CMU Multi-PIE – contains more than 750,000 images of 337 people, with 15 different views and 19 lighting conditions.FERET – Classical face recognition dataset.Deng Cai’s face dataset in Matlab Format – Easy to use if you want play with simple face datasets including Yale, ORL, PIE, and Extended Yale B.SCFace – Low-resolution face dataset captured from surveillance cameras.Handwritten DigitsMNIST – large dataset containing a training set of 60,000 examples, and a test set of 10,000 examples.Pedestrian DetectionCaltech Pedestrian Detection Benchmark – 10 hours of video taken from a vehicle,350K bounding boxes for about 2.3K unique pedestrians.INRIA Person Dataset – Currently one of the most popular pedestrian detection datasets.ETH Pedestrian Dataset – Urban dataset captured from a stereo rig mounted on a stroller.TUD-Brussels Pedestrian Dataset – Dataset with image pairs recorded in an crowded urban setting with an onboard camera.PASCAL Human Detection – One of 20 categories in PASCAL VOC detection challenges.USC Pedestrian Dataset – Small dataset captured from surveillance cameras.Generic Object RecognitionImageNet – Currently the largest visual recognition dataset in terms of number of categories and images.Tiny Images – 80 million 32x32 low resolution images.Pascal VOC – One of the most influential visual recognition datasets.Caltech 101 / Caltech 256 – Popular image datasets containing 101 and 256 object categories, respectively.MIT LabelMe – Online annotation tool for building computer vision databases.Scene RecognitionMIT SUN Dataset – MIT scene understanding dataset.UIUC Fifteen Scene Categories – Dataset of 15 natural scene categories.Feature Detection and DescriptionVGG Affine Dataset – Widely used dataset for measuring performance of feature detection and description. CheckVLBenchmarks for an evaluation framework.Action RecognitionBenchmarking Activity Recognition – CVPR 2012 tutorial covering various datasets for action recognition.RGBD RecognitionRGB-D Object Dataset – Dataset containing 300 common household objects"}
{"content2":"作者：刘秉茜说人工智能之前，先说人类大脑智能。人类的大脑有多种功能：计算能力，记忆能力，分析能力，视觉识别，语言能力等等。计算机现在做的是单点突破：计算能力早已碾压而过；记忆力，人类也已被远远甩开；语音识别、语义分析，也有很大进展；视觉识别、运动控制计算机还不如人类。假设人脑有20种能力，每种评分10分，计算机在某些方面已经达到1000分，某些方面只有2分。综合运用远不如人脑。经单点突破和暴力破解之后，计算机的薄弱能力如视觉识别有可能达到人类水平，最终在各方面都相继达到和超越人类大脑。在全面超越人类之前，人脑和计算机是分工协作，互利共存。人脑的优势能力如视觉识别、推理判断、想象创作被利用，而计算机的计算能力、记忆能力被利用。人脑和计算机之间产生分工。在计算机全面超越人脑之前，这种分工会一直存在。人工智能已经在我们生活中有所应用，比如苹果的Siri，科大讯飞的语音识别，亚马逊的相似商品推荐、工业机器人、家用机器人、谷歌的无人驾驶等。很多人都在想，人工智能正在或可能取代哪些工作，我们不妨也来畅想畅想：1、网络营销人员原来广告主投放广告，需要找人统计监测媒体数据。程序化广告出现后，实时数据呈现。比人力统计更迅速，更精确，更直观。类似微信公众号后台，之前的报纸杂志根本没法看到受众性别城市分布、阅读来源、阅读转发量的。如果要统计，需要花大力气做市场调查，结果还不一定精确，而现在很简单。所以新媒体对原有媒体的变革如此激烈，数据反馈是一项重要因素。2、军人军人一直是危险度很高的职业，今天派你到伊拉克明天就可能挂了。研究替代人打仗的机器军人也是各国军方的一大任务。在这方面，美国走在时代前面。人工智能灯塔谷歌更是在研发机器人军团。由于陆军开支庞大，美军正在研发机器人步兵取代人。这是机器人应用在陆军部队：这是谷歌收购的Schaft公司制造的蓝色双足机器人：被谷歌收购的波士顿动力研发的Atlas，可行走弹跳，摔倒后还可以爬起来：虚拟现实+远程遥控机器人，前途无可限量。未来的战争可能就是双方程序员坐在指挥室，指挥几千公里外的机器军队对打。（这不就是现在的网络游戏？3、大学PPT教师教师看似是一个非常具有技术含量的工作，但仔细考察并不是。在一些通用性知识教学上，高水平人类智慧注入的机器人比普通的人更具竞争力。这很容易理解，你上网看国外顶级大学教授的公开课，可能比国内大多数普通教师讲的好。那么，为什么不把这些顶级教授的智慧注入机器人让更多学生受益呢？尤其在国内大学以科研论文导向的风气下，很多教师照着PPT念的讲课水平还不如机器人。4、金融投资分析师在工业时代，人还是万物灵长，智能被认为只有人类持有，机器取代的最多是低端的体力。但是信息时代机器开始入侵智力的地盘，在信息存储和分析处理上，人与机器的差距越来越大。AiphaGo战胜人类只是这个时代的冰山一角。越来越多的自动化服务在华尔街被应用，并扩张到其他地方。理财规划师、股票分析师、家庭金融顾问与机器人顾问相比，在很多分析上都已落后。在美国，从事智能投顾的不仅仅是科技企业betterment和wealth front，老牌金融机构也灵敏的嗅到了不祥的前景，并先下手为强。高盛和贝莱德分别收购了Honest Dollar与Future Advisor，苏格兰皇家银行宣布用智能投顾取代500名传统理财师的工作。国内也有公司在开发智能投资顾问技术。人类向机器演化，机器向生物演化，机器与人协作，人类、机器构成新的物种生态。进而机器与人类发生竞争，在某些领域竞争力弱的人类被淘汰出局。这是未来的世界图景。文章摘自---------------------------------电子发烧友"}
{"content2":"摘自百度百科。。。。。。。。。。。。。（1）基于区域的跟踪算法基于区域的跟踪算法基本思想是：将目标初始所在区域的图像块作为目标模板，将目标模板与候选图像中所有可能的位置进行相关匹配，匹配度最高的地方即为目标所在的位置。最常用的相关匹配准则是差的平方和准则，(Sum of Square Difference，SSD)。起初，基于区域的跟踪算法中所用到的目标模板是固定的，如 Lucas 等人提出 Lucas-Kanade 方法，该方法利用灰度图像的空间梯度信息寻找最佳匹配区域，确定目标位置。之后，更多的学者针对基于区域方法的缺点进行了不同的改进，如：Jepson 等人提出的基于纹理特征的自适应目标外观模型[18],该模型可以较好的解决目标遮挡的问题，且在跟踪的过程中采用在线 EM 算法对目标模型进行更新；Comaniciu 等人[19]提出了基于核函数的概率密度估计的视频目标跟踪算法，该方法采用核直方图表示目标，通过 Bhattacharya 系数计算目标模板与候选区域的相似度，通过均值漂移(MeanShift)算法快速定位目标位置。基于区域的目标跟踪算法采用了目标的全局信息，比如灰度信息、纹理特征等，因此具有较高的可信度，即使目标发生较小的形变也不影响跟踪效果，但是当目标发生较严重的遮挡时，很容易造成跟踪失败。（2）基于特征的跟踪方法基于特征的目标跟踪算法通常是利用目标的一些显著特征表示目标，并通过特征匹配在图像序列中跟踪目标。该类算法不考虑目标的整体特征，因此当目标被部分遮挡时，仍然可以利用另一部分可见特征完成跟踪任务，但是该算法不能有效处理全遮挡、重叠等问题。基于特征的跟踪方法一般包括特征提取和特征匹配两个过程：a) 特征提取所谓特征提取是指从目标所在图像区域中提取合适的描绘性特征。这些特征不仅应该较好地区分目标和背景，而且应对目标尺度伸缩、目标形状变化、目标遮挡等情况具有鲁棒性。常用的目标特征包括颜色特征、灰度特征、纹理特征、轮廓、光流特征、角点特征等。D.G. Lowe 提出 SIFT(Scale Invariant Feature Transform)算法[20]是图像特征中效果较好的一种方法，该特征对旋转、尺度缩放、亮度变化具有不变性，对视角变化、仿射变换、噪声也具有一定的稳定性。b) 特征匹配特征匹配就是采用一定的方式计算衡量候选区域与目标区域的相似性，并根据相似性确定目标位置、实现目标跟踪。在计算机视觉领域中，常用的相似性度量准则包括加权距离、Bhattacharyya 系数、欧式距离、Hausdorff 距离等。其中，Bhattacharyya 系数和欧式距离最为常用。Tissainayagam 等人提出了一种基于点特征的目标跟踪算法[21]。该算法首先在多个尺度空间中寻找局部曲率最大的角点作为关键点，然后利用提出的MHT-IMM 算法跟踪这些关键点。这种跟踪算法适用于具有简单几何形状的目标，对于难以提取稳定角点的复杂目标，则跟踪效果较差。Zhu 等人提出的基于边缘特征的目标跟踪算法[22]，首先将参考图像划分为多个子区域，并将每个子区域的边缘点均值作为目标的特征点，然后利用类似光流的方法进行特征点匹配，从而实现目标跟踪。（3）基于轮廓的跟踪方法基于轮廓的目标跟踪方法需要在视频第一帧中指定目标轮廓的位置，之后由微分方程递归求解，直到轮廓收敛到能量函数的局部极小值，其中，能量函数通常与图像特征和轮廓光滑度有关。与基于区域的跟踪方法相比，基于轮廓的跟踪方法的计算复杂度小，对目标的部分遮挡鲁棒。但这种方法在跟踪开始时需要初始化目标轮廓，因此对初始位置比较敏感，跟踪精度也被限制在轮廓级。Kass 等人[23]于 1987 年提出的活动轮廓模型(Active Contour Models，Snake)，通过包括图像力、内部力和外部约束力在内的三种力的共同作用控制轮廓的运动。内部力主要对轮廓进行局部的光滑性约束，图像力则将曲线推向图像的边缘，而外部力可以由用户指定，主要使轮廓向期望的局部极小值运动，。Paragios 等人[24]提出了一种用水平集方法表示目标轮廓的目标检测与跟踪算法，该方法首先通过帧差法得到目标边缘，然后通过概率边缘检测算子得到目标的运动边缘，通过将目标轮廓向目标运动边缘演化实现目标跟踪。（4）基于模型的跟踪方法[25]在实际应用中，我们需要跟踪的往往是一些特定的我们事先具有认识的目标，因此，基于模型的跟踪方法首先根据自己的先验知识离线的建立该目标的 3D 或2D 几何模型，然后，通过匹配待选区域模型与目标模型实现目标跟踪，进而在跟踪过程中，根据场景中图像的特征，确定运动目标的各个尺寸参数、姿态参数以及运动参数。Shu Wang 等人提出一种基于超像素的跟踪方法[26]，该方法在超像素基础上建立目标的外观模板，之后通过计算目标和背景的置信图确定目标的位置，在这个过程中，该方法不断通过分割和颜色聚类防止目标的模板漂移。（5）基于检测的跟踪算法基于检测的跟踪算法越来越流行。一般情况下，基于检测的跟踪算法都采用一点学习方式产生特定目标的检测器，即只用第一帧中人工标记的样本信息训练检测器。这类算法将跟踪问题简化为简单的将背景和目标分离的分类问题，因此这类算法的速度快且效果理想。这类算法为了适应目标外表的变化，一般都会采用在线学习方式进行自更新，即根据自身的跟踪结果对检测器进行更新。"}
{"content2":"边缘检测时计算机视觉中的重要任务，边缘是像素与背景或周边区域不同的链，表达了图像中很多重要信息。噪声对边缘的影响图像中有噪声是在所难免的，也是非常正常的。而噪声没有一个准确的描述模型，所以很难利用噪声。一般来说，高斯模型是一个很好的噪声模型，也就是噪声各个幅值出现的概率服从高斯分布。将高斯噪声与图像叠加所形成的图像称为静态高斯噪声图像。有限差分对噪声的响应有限差分相当于是一个高通滤波器，会对高频信号进行相应，噪声以孤立点的形式出现，含有较多高频成分。高斯函数的傅里叶变换还是高斯函数。解决问题的办法是对图像进行平滑，后对平滑后的图像进行有限差分。差分 K**I高斯平滑G**I目标图像是K**G**I先计算K**G获得操作图像的模板将这个模板和图像卷积。高斯函数的模板不能太大，1σ的高斯模板需要5*5,如果是2σ的高斯模板则需要10*10，这会降低卷积操作的速度。边缘检测算法边缘含有图像的大量信息，边缘检测算法的三条要求信噪比——–对噪声的响应要低于对边缘的响应定位精度——最大响应在边缘处低误报率——在边缘处应只有一个最大值基于二阶导数的边缘检测Created with Raphaël 2.1.2开始高斯平滑求xy方向二阶导数并相加0点处既图像边缘结束clear all close all clc [a,b]=uigetfile('*'); pic=imread(a); g_pic=rgb2gray(pic); f = fspecial('gaussian',[5 5],1); img=imfilter(g_pic,f); DX=[0 0 0;1 -2 1;0 0 0]; DY=[0 1 0;0 -2 0;0 1 0]; DD=DX+DY; L_img=imfilter(img,DD); %搜索二阶导数最大值 A=size(L_img); BW=zeros(A); for i=2:A(1)-1 for j=2:A(2)-1 if L_img(i,j)>L_img(i+1,j)&&L_img(i,j)>L_img(i-1,j) BW(i,j)=1; elseif L_img(i,j)>L_img(i,j+1)&&L_img(i,j)>L_img(i,j-1) BW(i,j)=1; end end end subplot(2,1,1) imshow(g_pic,[]); subplot(2,1,2) imshow(BW,[]);写这个程序还是遇到了一点问题的，这里访问size,不能直接访问列，不能写成A（：，1）要写成A（1），A（2）。上面是matlab代码。对图片的检测结果如下图所示可以看出，这种方法的缺点是在角点处有断点，原因是基于二阶导数的边缘检测，在边缘方向剧烈改变时会有较差表现。二阶导数的响应又一个沿着边缘方向和垂直于边缘方向的响应组成。基于梯度的边缘检测为了克服基于导数的边缘检测和更好的提取角点，有人设计了这种基于梯度的边缘检测。这种方法的思想是边缘处于梯度方向最大值处，并且预计在沿着边缘梯度垂线方向出现下一个梯度点。和log的思想相比，canny利用了边缘梯度的信息，引入了方向性。并且抛弃了利用导数的数值判断编译的思想。这种思想是将图片看作二维函数，每个xy对应一个函数值。每个像素点都有梯度方向。Created with Raphaël 2.1.2开始求取梯度方向和梯度值找到梯度值最大的点沿着梯度方向搜索是否梯度值最大沿着垂直梯度方向搜索是否已找到整条边缘结束yesnoyesno梯度<∂f∂x,∂f∂y>版权声明：本文为博主原创文章，未经博主允许不得转载。"}
{"content2":"一、论文综述类的文章[1]P.Dollar, C. Wojek,B. Schiele, et al. Pedestrian detection: an evaluation of the state of the art [J].IEEE Transactions on PatternAnalysis andMachine Intelligence, 2012, 34(4): 743-761.[2]M. Enzweiler, and D.Gavrila. Monocular pedestrian detection: survey and experiments [J]. IEEE Transactions on Pattern Analysis andMachine Intelligence, 2009, 31(12): 2179-2195.[3]D. Geronimo, A. M.Lopez and A. D. Sappa, et al. Survey of pedestrian detection for advanced driverassistance systems [J]. IEEE Transactionson Pattern Analysis and Machine Intelligence, 2010, 32(7): 1239-1258.[4]苏松志, 李绍滋, 陈淑媛等. 行人检测技术综述[J]. 电子学报, 2012, 40(4): 814-820.[5]贾慧星, 章毓晋.车辆辅助驾驶系统中基于计算机视觉的行人检测研究综述[J], 自动化学报, 2007, 33(1): 84-90.[6] 许言午, 曹先彬,乔红. 行人检测系统研究新进展及关键技术展望[J], 电子学报, 2008, 36(5): 368-376.[7] 杜友田; 陈峰;徐文立; 李永彬;基于视觉的人的运动识别综述, 电子学报, 2007. 35(1): 84-90.[8]朱文佳. 基于机器学习的行人检测关键技术研究[D]. 第一章, 硕士学位论文, 上海交通大学. 2008. 指导教师: 戚飞虎.二、Source Code1.INRIA Object detection and Localization Toolkit, Dalal于2005年提出了基于HOG特征的行人检测方法，行人检测领域中的经典文章之一。HOG特征目前也被用在其他的目标检测与识别、图像检索和跟踪等领域中。2. Real-time Pedestrian Detection. Jianxin Wu实现的快速行人检测方法。3. Hough Transfom for Pedestrian Detection. Olga Barinova, CVPR 2010 Paper: On detection of multiple object instances using Hough Transforms4. HIKSVM, HOG+LBP+HIKSVM, 行人检测的经典方法.5. GroundHOG, GPU-based Object Detection with Geometric Constraints, In: ICVS, 2011.  CUDA版本的HOG+SVM,  video.6.  100FPS_PDS, Pedestrian detection at 100 frames per second, R. Benenson.  CVPR, 2012. 实时的(⊙o⊙)哦。 Real-time!!!7. POM: Probabilistic Occupancy Map.  Multiple camera pedestrian detection.三、DataSetsMIT数据库该数据库为较早公开的行人数据库，共924张行人图片（ppm格式，宽高为64x128），肩到脚的距离约80象素。该数据库只含正面和背面两个视角，无负样本，未区分训练集和测试集。Dalal等采用“HOG+SVM”，在该数据库上的检测准确率接近100%。INRIA数据库该数据库是目前使用最多的静态行人检测数据库，提供原始图片及相应的标注文件。训练集有正样本614张（包含2416个行人），负样本1218张；测试集有正样本288张（包含1126个行人），负样本453张。图片中人体大部分为站立姿势且高度大于100个象素，部分标注可能不正确。图片主要来源于GRAZ-01、个人照片及google，因此图片的清晰度较高。在XP操作系统下部分训练或者测试图片无法看清楚，但可用OpenCV正常读取和显示。Daimler行人数据库该数据库采用车载摄像机获取，分为检测和分类两个数据集。检测数据集的训练样本集有正样本大小为18x36和48x96的图片各15560（3915x4）张，行人的最小高度为72个象素；负样本6744张（大小为640x480或360x288）。测试集为一段27分钟左右的视频（分辨率为640x480），共21790张图片，包含56492个行人。分类数据库有三个训练集和两个测试集，每个数据集有4800张行人图片，5000张非行人图片，大小均为18x36，另外还有3个辅助的非行人图片集，各1200张图片。Caltech行人数据库该数据库是目前规模较大的行人数据库，采用车载摄像头拍摄，约10个小时左右，视频的分辨率为640x480，30帧/秒。标注了约250,000帧（约137分钟），350000个矩形框，2300个行人，另外还对矩形框之间的时间对应关系及其遮挡的情况进行标注。数据集分为set00~set10，其中set00~set05为训练集，set06~set10为测试集（标注信息尚未公开）。性能评估方法有以下三种：（1）用外部数据进行训练，在set06~set10进行测试；（2）6-fold交叉验证，选择其中的5个做训练，另外一个做测试，调整参数，最后给出训练集上的性能；（3）用set00~set05训练，set06~set10做测试。由于测试集的标注信息没有公开，需要提交给Pitor Dollar。结果提交方法为每30帧做一个测试，将结果保存在txt文档中（文件的命名方式为I00029.txt I00059.txt ……），每个txt文件中的每行表示检测到一个行人，格式为“[left, top,width, height, score]”。如果没有检测到任何行人，则txt文档为空。该数据库还提供了相应的Matlab工具包，包括视频标注信息的读取、画ROC（Receiver Operatingcharacteristic Curve）曲线图和非极大值抑制等工具。TUD行人数据库TUD行人数据库为评估运动信息在行人检测中的作用，提供图像对以便计算光流信息。训练集的正样本为1092对图像（图片大小为720x576，包含1776个行人）；负样本为192对非行人图像（手持摄像机85对，车载摄像机107对）；另外还提供26对车载摄像机拍摄的图像（包含183个行人）作为附加训练集。测试集有508对图像（图像对的时间间隔为1秒，分辨率为640x480），共有1326个行人。Andriluka等也构建了一个数据库用于验证他们提出的检测与跟踪相结合的行人检测技术。该数据集的训练集提供了行人的矩形框信息、分割掩膜及其各部位（脚、小腿、大腿、躯干和头部）的大小和位置信息。测试集为250张图片（包含311个完全可见的行人）用于测试检测器的性能，2个视频序列（TUD-Campus和TUD-Crossing）用于评估跟踪器的性能。NICTA行人数据库该数据库是目前规模较大的静态图像行人数据库，25551张含单人的图片，5207张高分辨率非行人图片，数据库中已分好训练集和测试集，方便不同分类器的比较。Overett等用“RealBoost+Haar”评估训练样本的平移、旋转和宽高比等各种因素对分类性能的影响：（1）行人高度至少要大于40个象素；（2）在低分辨率下，对于Haar特征来说，增加样本宽度的性能好于增加样本高度的性能；（3）训练图片的大小要大于行人的实际大小，即背景信息有助于提高性能；（4）对训练样本进行平移提高检测性能，旋转对性能的提高影响不大。以上的结论对于构建行人数据库具有很好的指导意义。ETH行人数据库Ess等构建了基于双目视觉的行人数据库用于多人的行人检测与跟踪研究。该数据库采用一对车载的AVT Marlins F033C摄像头进行拍摄，分辨率为640x480，帧率13-14fps，给出标定信息和行人标注信息，深度信息采用置信度传播方法获取。CVC行人数据库该数据库目前包含三个数据集（CVC-01、CVC-02和CVC-Virtual），主要用于车辆辅助驾驶中的行人检测研究。CVC-01[Geronimo,2007]有1000个行人样本，6175个非行人样本（来自于图片中公路区域中的非行人图片，不像有的行人数据库非行人样本为天空、沙滩和树木等自然图像）。CVC-02包含三个子数据集（CVC-02-CG、CVC-02-Classification和CVC-02-System），分别针对行人检测的三个不同任务：感兴趣区域的产生、分类和系统性能评估。图像的采集采用Bumblebee2立体彩色视觉系统，分辨率640x480，焦距6mm，对距离摄像头0~50m的行人进行标注，最小的行人图片为12x24。CVC-02-CG主要针对候选区域的产生，有100张彩色图像，包含深度和3D点信息；CVC-02-Classification主要针对行人分类，训练集有1016张正样本，7650张负样本，测试集分为基于切割窗口的分类（570张行人，7500张非行人）和整张图片的检测（250张包含行人的图片，共587个行人）；CVC-02-System主要用于系统的性能评估，包含15个视频序列（4364帧），7983个行人。CVC-Virtual是通过Half-Life 2图像引擎产生的虚拟行人数据集，共包含1678虚拟行人，2048个非行人图片用于测试。USC行人数据库该数据库包含三组数据集（USC-A、USC-B和USC-C），以XML格式提供标注信息。USC-A[Wu, 2005]的图片来自于网络，共205张图片，313个站立的行人，行人间不存在相互遮挡，拍摄角度为正面或者背面；USC-B的图片主要来自于CAVIAR视频库，包括各种视角的行人，行人之间有的相互遮挡，共54张图片，271个行人；USC-C有100张图片来自网络的图片，232个行人（多角度），行人之间无相互遮挡。四、其他相关资料资料1. Edgar Seemann维护的行人检测网站，比较全，包括publications, code, datasets等。2. Pedestrian detection: state of the art. A video talk byPitor Dollar. Pitor Dollar做了很多关于行人检测方法的研究，他们研究小组的Caltech Pedestrian Dataset也很出名。"}
{"content2":"AForge.NET是一个专门为开发者和研究者基于C#框架设计的，他包括计算机视觉与人工智能，图像处理，神经网络，遗传算法，机器学习，模糊系统，机器人控制等领域。这个框架由一系列的类库组成。主要包括有：AForge.Imaging —— 日常的图像处理和过滤器AForge.Vision —— 计算机视觉应用类库AForge.Neuro —— 神经网络计算库AForge.Genetic -进化算法编程库AForge.MachineLearning —— 机器学习类库AForge.Robotics —— 提供一些机器学习的工具类库AForge.Video —— 一系列的视频处理类库AForge.Fuzzy —— 模糊推理系统类库AForge.Controls—— 图像，三维，图表显示控件以下是部分方向的使用1.基于符号识别的3D现实增强技术2.基于模糊系统的自动导航3.运动检测4.2D增强技术5.计算机视觉与人工智能6.模拟识别7.神经网络8.图像处理9.遗传算法10.机器学习11.机器人控制等等还有GRATF 符号识别和目标追踪的库，可以用于机器人控制，当然也可以用于现实增强。Image Processing Lab基于C#的图像处理库，提供了一系列可用于AForge，Net的接口和工具。AForge.Net 是C#的一个图像计算机视觉库，该库是一个开源项目，提供很多图像的处理，和视频处理功能http://www.aforgenet.com/Aforge.Net子项目有个AForge.Video.VFW提供了对Avi文件的操作，AForge后面加入了子项目 AForge.Video.FFMPEG 通过FFmpeg库，提供了对大量视频格式的支持，我们都知道，FFmpeg是一个非常强大的视频处理类库，同样也是开源的，不过 AForge.Video.FFMPEG 还处于实验阶段，目标是用 FFmpeg 取代 AForge.Video.VFW 提供一个更好的对视频文件操作的库，但是该库目前只提供了对视频数据的读写，不支持对音频文件的读写，可能以后会支持，在使用的 AForge.Video.FFMpeg 时，添加对 AForge.Video.FFMPEG.dll, AForge.Video.dll和 AForge.dll 三个 dll 的引用。AForge.Video.FFMpeg命名空间下提供了三个类 VideoFileReader， VideoFileWriter， VideoFileSource"}
{"content2":"2016中国人工智能企业TOP100不论在学界还是业界，均有代表人物对人工智能表示了担忧，如史蒂芬·霍金和比尔·盖茨。尽管如此，国内外科技巨头都积极发力人工智能，一波波创业者也相继涌入。人工智能成为当下最炙手可热的科技力量。科技这把双刃剑始终代表着人类对于自身的超越，对于更高级智慧的探索，以及无限的好奇心和创造力。科技的发展犹如一条直线，无可逆转。人类只有深入了解它，才能更好地应对并驾驭它。类人是人工智能的第三阶段，有情感、会思考、能行动丰富的海量数据、强大的计算模型、深度学习和人脑芯片等共同推动着人工智能的逐步完善。前几年大数据、云计算的发展热潮，使数据搜集与处理成本逐渐降低，分析能力逐步增强。机器能够基于庞大的数据来挖掘其价值，进而形成一定认知，并做出一定判断。它体现出基础人工智能的优势，数据依据更广博、精准，判断更理性。其中，作为底层技术的语音识别是人工智能的战略基础。语音识别离不开对相应场景数据的智能训练。微软小娜，亚马逊Echo、苹果Siri等语音服务早已被视为人工智能崛起的另一促进因素。而随着AlphaGo击败围棋冠军李世石，深度学习显示出其巨大能量，人工智能从可能走向现实的几率也随之增大。大数据和深度学习成为人工智能的必争之地。当然，目前的人工智能技术大部分还处于较为初级阶段。人工智能基本可分为三个阶段，分别为识别阶段（在大量文本中识别特定信息、含义）、认知阶段（基于资料做出判断的能力）和类人阶段（有情感、会思考、能行动）。无论哪个阶段，数据成为不可或缺的因素。如果没有真正的技术支撑和产品的落地，资本的涌入可能形成一定负面作用根据CBInsights的资料显示，2015年AI创业公司仍是投资焦点之一。其中，医疗健康领域融资占比达15%，另外则是场景辅助、机器人、CRM、机器学习、电商、网络安全与监管及教育等领域。可见，人工智能将应用到各行各业。例如，成立于2004年的Palantir通过整合人工智能算法和多个数据库，所推出的服务已经被CIA、FBI等美国政府机关采纳。尤其在安全问题方面，它成了美国情报机关反恐不可或缺的工具。相应地，人工智能人才也成了香饽饽。他们是技术型人才，对人工智能的理解与运用极其深入。他们也可能极具创意，在商业模式上能够引领潮流。投资热潮必然推动产业的发展，但如果没有真正的技术支撑和产品的落地，资本的涌入可能形成一定负面作用。所以，始终把握科技的初心。人工智能的发展，离不开对人性的了解这是斯坦福大学计算机系终身教授、人工智能实验室主任李飞飞的观点。她带领团队创造了能自动生成图说的软件，并创建了图像识别数据库ImageNet，成为学界和业界的标准。而她提出的“人工智能将改变世界，但谁将改变人工智能？”这一问题，恰恰彰显出科学工作者对于人文的关怀。科学与艺术需要达到平衡，否则人类命运的天平倾倒，潘多拉的盒子会带来难以估量的黑暗。结语人工智能将人类从繁杂的劳务中解放出来，甚至让人摆脱身体在时空中所受的限制，超越现有状态。人工智能是对未来的、力量的渴望，是人类的进化，是不可抵挡的脚步。CBinsight2016年100家人工智能公司人工智能腾讯科技谭燃2017-01-23 16:56【AI世代编者按】调研公司CB Insights近日公布了“最值得关注的100家人工智能公司”。上榜的这100家企业正在从事人工智能领域的技术研究，试图利用人工智能来改变各行各业。CB Insights的评选结果基于这些公司所提交的数据、对一系列问题的回答、以及企业的Mosaic分数。Mosaic是一种获得了美国国家科学基金会资助的算法，能预测一家的健康状况。CB Insights联合创始人兼CEO阿南德·桑瓦尔(Anand Sanwal)称：“从金融服务到医疗保健，再到运输，每个行业的传统巨头都认为，人工智能将重塑他们所在行业。历史多次证明，这种革命性创新恰恰来自新兴公司。在人工智能领域，上榜的这100家公司正在从事一些具有突破性的研究工作，并有望在未来数十年内颠覆我们的生活。”关注AI世代（微信号：tencentAI），回复“100AI”可获得该报告原文下载。有关这100家人工智能公司的一些数据：·超过1650家公司被提名，仅6%被选中。·入选的中国内地公司有深圳碳云智能科技有限公司(iCarbonX)、杭州Rokid公司、北京出门问问和深圳优必选。·自2012年以来，这100家公司通过263笔交易融资38亿美元。·其中5家公司的估值为10亿美元或更高，即“独角兽”公司。·2014 年以来，这100家公司进行9笔超大规模融资(融资额达到1亿美元或更多)。·其技术的应用领域包括医疗保健、药物发现、商业智能、游戏和制造等。·这100家公司来自11个国家和地区。以下为AI世代（tencentAI）编译整理的 “最值得关注的100家人工智能公司”(按公司名称首字母顺序排列)：1. Affectiva公司网站：affectiva.comAffectiva拥有“麻省理工学院媒体实验室”背景，是“情感人工智能”(人工智能的下一个研究前沿)领域的先驱。Affectiva的使命是利用情感识别技术将情感智能带入数字世界。Affectiva的专利技术集计算机视觉、深度学习和全球最大的情感数据库为一体。Affectiva的SDK和API允许开发人员将情感感知和分析添加到自己的应用程序、游戏、设备和数字体验中。当前，1/3的“财富全球100强企业”都在使用Affectiva的情感识别技术。该技术可用于多个垂直领域，包括在线教育、健康、游戏、机器人、媒体和广告、市场调研、汽车零售、人力资源、培训和辅导、视频通信、体验性设计、可穿戴和各种设备。2. AImotive公司网站：aimotive.comAImotive总部位于布达佩斯，为完全自动驾驶汽车开发全套软件组件。AImotive的算法将摄像头作为主要传感器，以完成对象识别、分类、定位、决策、路线规划和车辆控制等任务。除了软件引擎组件，AImotive还提供一个广泛的工具开发包，以加速训练和验证，包括校准、数据采集和增强数据生成、半监督注释，以及实时逼真的模拟环境。为了解决日益增长的硬件加速器需求，AImotive还面向汽车嵌入式解决方案设计了一个强大、高效的神经网络硬件IP。其参考设计可帮助芯片厂商开发有针对性的硬件来匹配人工智能软件套装。3. Algorithmia公司网站：algorithmia.comAlgorithmia帮助开发者、企业和学术界创建、运营、部署并商业化人工智能微服务。当前，超过30000名开发者访问Algorithmia的超过2500种算法的微服务。通过简单的API，开发者可以将这些微服务整合到自己的应用中。Algorithmia的使命就是推动“算法智能”(algorithmic intelligence)的发展、发觉和接入性。目前， Algorithmia提供了两个主要产品：Algorithmia.com(微服务市场)和CODEX(功能和人工智能模型的平台)。4. AlphaSense公司网站：alpha-sense.com智能金融搜索引擎AlphaSense帮助那些需要应对大量数据的专业人员解决信息过载的问题。当前，一些平台提供了人们需要的大量文件，但没有一家提供智能搜索功能。而AlphaSense提供对专有研究数据库的访问，并包括金融语言的语义知识和相关性分析，以开启有价值的隐藏信息。AlphaSense收集了数百万份文档，包括企业提交给监管部门的文件、公司简介、新闻、公关稿、华尔街的调研报告，以及其他任何被上传的内容。5. Anki公司网站：anki.comAnki的使命是把人工智能和消费机器人普及到人们日常生活中。Anki的主要产品是Anki Overdrive，它将电子游戏和实体道具相结合。通过智能手机，用户可以在自定制赛道上驾驶一辆赛车。玩家可以与好友一起比赛，也可以挑战由Anki人工智能技术控制的其他汽车。6. Atomwise公司网站：atomwise.comAtomiwise利用人工智能挖掘新的潜在药物。该公司建立了第一个用于“结构性药物设计”的深度神经网络。目前，Atomwise正在帮助研究人员解决慢性疾病(如癌症、多发性硬化症和糖尿病)、被忽略的全球疾病(如埃博拉病毒和疟疾)、死灰复燃的疾病(如耐抗生素细菌 )和生物疾病威胁(如肉毒杆菌神经毒素)。Atomiwise的神经网络已被应用到整个药物发觉过程中，每天可分析数以百万计的分子。7. Appier公司网站：appier.comAppier是一家总部位于台北的技术公司，帮助企业在“跨屏幕时代”利用人工智能获得增长和成功。Appier由一群充满激情的科学家和在人工智能等领域拥有丰富经验的工程师所创建，员工主要来自谷歌(微博)、英特尔、雅虎、哈佛大学和斯坦福大学等。当前，Appier服务于全球500多个品牌和机构。8. Automat公司网站：automat.aiAutomat是世界上第一个基于人工智能的对话式营销平台。它允许企业与客户进行个性化的一对一对话，以促进互动。Automa的Bot Creator程序编写环境允许营销人员、创意人员和开发者协同开发基于人工智能的对话软件，以淘汰当前的“一刀切”销售模式。通过Automat的“对话即服务”(CaaS)平台，卖家能利用CLU编程语言来了解消费者对其品牌和产品的反馈。9. Ayasdi公司网站：ayasdi.comAyasdi帮助全世界的公司利用人工智能和大数据来提高员工的产出，激发超越人类能力。Ayasdi的革命性机器智能平台利用自动化、机器学习和拓扑数据分析，来简化从庞大复杂的数据集中抽取相应的知识，从而促进企业部署基于人工智能的应用。当前，许多全球500强企业都是Ayasdi的客户。10. babylon公司网站：babylonhealth.combabylon是智能手机或者平板上的个人医疗服务。它简单易用，任何人都可以随时随地使用。人们在几分钟内就能联系上医生，在手机上进行面对面问诊。babylon的使命是让人们更健康、更快乐。为此，babylon为其应用增加了许多实用功能。用户可以检查任何症状，通过聊天功能咨询任何问题，并迅速得到答复。此外，用户还可以通过其应用订购各种健康检测工具。babylon正在改变让人类变得更健康的方式。11. BenevolentAI公司网站：benevolent.ai尽管知识量急剧增长，科学发现的工作方式50年未变。人类自身不可能处理所有的信息，虽然每30秒就有一篇新的科学论文被发表，但实际上只有一小部分科学信息可形成“可用知识”。BenevolentAI把人工智能和深度学习技术用于大量复杂科学信息的分析，从而改变了知识的创建方式。BenevolentAI的第一款人工智能应用(生物科学领域)已被用于药物研发，并且正被拓展到其它领域。12. BloomReach公司网站：bloomreach.comBloomReach的个性化平台集多种技术为一体，为各种数字环境(包括用户获取和用户转变)下的个人和企业提供良好的体验，使之更个性化，并贴近用户期望。13. Blue River Technology公司网站：bluerivert.comBlue River Technology致力于为农业开发和提供先进的技术。利用计算机视觉和机器人技术，Blue River Technology希望让每一种农作物都被精准耕作。14. Cape Analytics公司网站：capeanalytics.comCape Analytics的使命是改变人类所创建环境信息的创建和使用方式。Cape Analytics利用人工智能技术快速、自动地从地理空间图像中提取专属数据。Cape Analytics的云平台利用计算机视觉技术来提供高质量的信息，这些数据面向所有人开放，而且很容易保险公司等机构整合到自家应用中。15. Bonsai公司网站：bons.aiBonsai总部位于加州伯克利(Berkeley)，是一个软件开发平台，允许所有发者搭建、训练、使用智能模型。不需要复杂的AI算法和技术，Bonsai人工智能引擎能让开发人员更高效地编码，以更好地控制和优化硬件和软件。16. Captricity公司网站：captricity.comCaptricity是一款安全的云服务软件，它能把手写文章转换成电子版，准确度超过99%。Captricity的云解决方案基于深度学习技术，可以从任何来源(纸质文档、扫描件、传真、电子邮件、呼叫中心和Web格式)抽取数据，然后自动连接后端系统进行深度分析，从而省去了人工输入的低效率和高成本。17. Chorus.ai公司网站：chorus.aiChorus的使命是基于每一个成功的销售电话来了解哪些因素影响对话结果，从而帮助你的团队做出正确的决策。当前，几乎所有的这些对话(销售电话)都在结束后的几秒内就被忘记，而Chorus正在改变这一现状。Chorus是一支由全世界一流科学家、工程师和产品设计师组成的紧密团队，许多数据驱动型上市公司和高增长型公司都是Chorus的客户。18. Citrine Informatics公司网站：citrine.ioCitrine运行着全球最大的材料库平台。Citrine利用该平台来构建人工智能软件，以推动更高效的发现、优化、制造和部署材料。该软件平台通过各种来源吸纳结构性和非结构性材料数据，然后利用人工智能引擎来识别这些数据中的重要信息，从而助力客户的研发与制造。Citrine服务于那些利用先进材料获取竞争优势的企业，涵盖汽车、航空、消费品、电池和电子等领域。19. Chronocam公司网站：chronocam.comChronocam总部位于巴黎，该公司正在开发一种独特的、生物启发和自适应的方法，来满足自动驾驶、联网设备、安全和监控系统对视觉传感和处理的需求。其创新性的视觉传感器和系统复制了人眼的功能，可实时感知动态场景并获取必要信息，从而解决了普通视觉传感器的局限性。在感知速度、动态范围、视频压缩和能耗方面，Chronocam的视觉解决方案为业界树立了新标杆。20. Clara Labs公司网站：claralabs.comClara Labs以比人类更聪明、更高效的方式来管理您的工作日历。当需要安排会议时，只要抄送给clara@yourcompany.com ，Clara就会接管邮件对话，并以你的名义发送日程邀请。Clara Labs的客户遍及全球，包括销售团队和人力资源部门，每周可帮助他们节省数千小时的时间。21. Clarifai公司网站：clarifai.comClarifai是一家人工智能公司，擅长视觉识别，为企业和开发人员解决现实生活中的一些问题。Clarifai创建于2013年，曾在“ImageNet计算机视觉识别”挑战赛中跻身前五名。Clarifai的强大视觉识别技术基于最先进的机器学习系统，可通过一套简单的API轻松访问，它允许全世界的开发者开发新一代智能应用。Clarifai允许开发者和企业利用人工智能进行快速、低成本的创新，同时提供更好的用户体验。22. CloudMedx Inc公司网站：cloudmedxhealth.comCloudMedx打造了一个临床人工智能平台，把基于“脑启发的临床算法”应用的规模和简单性推广到医疗保健领域。该平台建立在一个全面和不断增长的医学知识网络基础之上，它集机器学习和大数据分析为一体，可实时生成健康见解，改善慢性疾病治疗效果。23. CognitiveScale公司网站：cognitivescale.comCognitiveScale为医疗保健、商业和金融服务市场开发机器智能软件。其主要产品为Engage和Amplify，可帮助大型企业提高用户参与度，改善决策，提供自学习和自我保证的业务流程。许多全球500强企业都成功地部署了CognitiveScale的软件，包括IBM和微软。CognitiveScale由IBM、甲骨文和Salesforce等公司的前高管于2013年创建，总部位于德州奥斯汀(Austin)，在英国和印度设有办公室。24. Context Relevant公司网站：contextrelevant.comContext Relevant的产品利用机器学习技术所驱动的“自动化数据科学平台”(ADS)，解决了世界上最棘手的大数据、预测和行为方面的挑战。与其他解决方案相比，Context Relevant的技术更快、更有效。Context Relevant的机器学习平台使用了世界上最优秀的开源基础架构，如Hadoop和Spark，并与自家的ADS引擎和应用环境相整合，从而确保快速创建、测试和部署由数据科学和预测性分析驱动的智能应用。25. Cortical.io公司网站：cortical.ioCortical.io提供基于“语义折叠理论”(Semantic Folding，处理“大文本数据”的新型方法)的自然语言理解(NLU)解决方案。其灵感源自对大脑信息处理方式的最新发现，Cortical.io的Retina引擎可将语言转换成语义指纹，然后通过对比指纹的重叠度来比较任何两个文本的语义相关性。Cortical.io创建于2011年，总部位于奥地利维也纳。26. CrowdFlower公司网站：crowdflower.comCrowdFlower是一个面向数据科学团队的人类干预训练平台，帮助客户生成高质量的自定义训练数据。CrowdFlower平台支持一系列应用案例，包括自动驾驶汽车、个人智能助理、医学图像标记、内容分类、社交数据分析、CRM数据改进、产品分类和搜索相关性等。CrowdFlower总部位于旧金山，客户包括“财富500强”企业和数据驱动型企业。27. Cylance公司网站：cylance.comCylance由互联网安全公司McAfee前高管于2012年组建，是一家网络安全产品和服务公司，专注于提前预防网络攻击。Cylance的旗舰产品是CylancePROTECT，是全球第一款下一代防病毒软件，基于人工智能和深度学习技术。与其他安全解决方案相比，Cylance的预防能力更强，2015年曾获得“最佳新兴技术奖”。28. Darktrace公司网站：darktrace.comDarktrace是全球领先的互联网威胁防御公司，去年曾被评为“最佳安全公司”。Darktrace的核心产品为“企业免疫系统”(EIS)，利用机器学习和独家算法来检测和响应以前未识别的威胁。Darktrace在英国剑桥和美国旧金山均设有总部，在奥克兰(新西兰)、波士顿、伦敦、米兰、孟买、巴黎、首尔、新加坡、悉尼、东京和多伦多等地设有办公室。29. Dataminr公司网站：dataminr.comDataminr通过从社交媒体活动中获取信息，来建实时、可执行的消息警告。Dataminr的强大算法能将Twiiter等公开数据转变成可实施的消息警告，使世界各地的安全、运营、财务和通信专业人员能够随着事态的发展而获取必要信息。这些警告消息是通过桌面应用和工作流整合方式提供给用户的。当前，全球许多企业、财务公司、新闻机构和公共机构都在使用Dataminr的消息警告，从而对各种事件做出快速响应。30. DataRobot公司网站：datarobot.comDataRobot把建立机器学习模式的过程变成自动化。通过提供快速、准确和自动化的程序，DataRobot可帮助企业级用户做出更智能、更快速的商业决策。DataRobot通过改变预测分析的速度和经济学，解决了数据科学家严重短缺的问题。31. Deep Genomics公司网站：deepgenomics.comDeep Genomics成立于2015年，致力于通过建立一套计算系统化来改善诊断、 治疗和了解疾病的方式。Deep Genomics的技术集机器学习和基因组生物学为一体，以了解、预测和解释DNA的变化。Deep Genomics开发出了一种新的机器学习方法，可以在海量数据中找出相应的模式，并推断出细胞如何读取基因组、并生成生物分子的计算机模型。32. Deep Instinct公司网站：deepinstinct.comDeep Instinct是第一家将深度学习技术应用于网络安全领域的公司，旨在利用深度学习为任何设备、任何平台和操作系统提供全面的保护。Deep Instinct的技术能实时检测出“零日威胁”和APT攻击(高级持续性威胁)，且准确性极高。凭借前摄性和预测性，Deep Instinct的技术为网络安全提供一种全新的防护方法。33. Deepgram公司网站：deepgram.comDeepgram是一款可以让开发者检索音频和视频文件中的语音信息的搜索引擎，它利用深度学习技术来挖掘和分析这些语音数据，搜索范畴包括通话、会议、播客、视频短片和演讲等。Deepgram拥有世界上最准确的关键词识别机制，可在音频中识别要搜索的关键词；人工智能建模机制，自动分析和分类；支持从音频中提取文本，省去人工录音并转为文本的麻烦。34. Descartes Labs公司网站：descarteslabs.comDescartes Labs创建于2014年，总部位于墨西哥。这是一家致力于推动预测科学的科技公司，利用机器学习和卫星图像等海量数据集进行不同行业的预测和分析。去年，Descartes Labs被调研公司CB Insights评为“游戏规则的颠覆者”。35. Digital Reasoning公司网站：digitalreasoning.comDigital Reasoning是认知计算领域的领先企业，构建了可以分析任何形式的非结构化和结构化数据的认知计算平台Synthesys，为金融、信息、国防、医疗等领域提供解决方案。36. DigitalGenius公司网站：digitalgenius.comDigitalGenius将深度学习和人工智能的实际应用带入了大公司的客户服务中，其核心是深度学习算法，为客户服务行业创建了一个可扩展的深度学习产品。DigitalGenius的产品主要有两大功能：利用机器学习来预测和自动匹配各种案例；利用深度学习算法来自动回答客户提出的问题。37. Dispatch公司网站：dispatch.aiDispatch是一家自动化快递服务创业公司，利用自动驾驶汽车来提供智能化快递服务。Dispatch的第一辆自动驾驶汽车名为“Carry”，由于足够小巧，可在人行道上行驶，同时又可以携带多个包裹。Carry主要在行人区运营，行驶速度与步行相当，因此可以轻松融入人流中。到达目的地后，用户将接到一个密码，用来打开车箱，然后取走货物。38. Drive.ai公司网站：drive.aiDrive.ai是一家软件公司，由斯坦福大学人工智能实验室员工于2015年组建，致力于重新构筑人与交通之间的根本关系。Drive.ai利用复杂的深度学习算法提供自动驾驶解决方案，目前已获准在加州测试自动驾驶车辆。39. Drawbridge公司网站：drawbridge.comDrawbridge是一家领先的匿名数字身份识别技术服务商，开发了已获专利的跨设备匿名数字识别技术，从根本上改变了品牌与消费者的连接方式。Drawbridge的“连接消费者图谱”(Connected Consumer Graph)包含了30多亿部设备上的10多亿消费者，品牌有三种方式与Drawbridge合作：从Drawbridge获得图谱授权，以实现跨设备数据应用；利用Drawbridge平台管理跨设备广告推广活动；与Drawbridge合作进行跨设备数字广告推广。Drawbridge总部位于硅谷，已获得红杉资本和KPCB的投资。40. Elitic, Inc.公司网站：enlitic.comEnlitic是一家致力于改进医疗诊断的深度学习公司，其算法由知名数据科学家、机器学习从业人员和医学专家组成的多学科团队设计，目的是提供更快、更准确的医疗诊断服务。例如，Enlitic的深度学习技术可帮助放射科医生提高准确性，降低成本，提高早期监测结果(可改善治疗结果)。41. fido.ai公司网站：fido.aiFido是一家自然语言处理技术公司，专注于利用人工智能技术将决策过程转换为对话。Fido决策引擎通过从文章、博客和社交媒体等来源提取事实和意见来实现自动学习。Fido的决策引擎具有广泛的应用空间，如事实验证、回答健康问题(基于临床数据、医患对话)、产品开发和营销等。42. Freenome公司网站：freenome.comFreenome是美国费城一家新兴液体活检诊断生物技术公司，通过对血液(自由细胞)中遗传物质流的动态收集，来诊断是否患有癌症。Freenome设计了一种学习引擎“适应性基因组引擎”，用最全面的方法从基因组中获得关于癌症的信息并进行分析，从而给出尽可能准确的预测。 去年，Freenome获得了来自Andreessen Horowitz等投资方的550万美元风险投资。43. Gigster公司网站：gigster.comGigster是一家为缺乏内部研发团队的企业和初创企业提供软件开发与设计解决方案的初创企业。其概念很简单，就像Uber把司机与乘客联系在一起一样，Gigster把自由职业的程序员、设计师与有软件开发需求的公司联系在一起。Gigster创建于2014年，已获得1200万美元的投资。44. Gradescope公司网站：gradescope.comGradescope使用人工技术帮助老师对学生的表现评分，以节省时间。到目前为止，Gradescope已经为全球超过200家机构批改了超过1200万页的学生作业。45. GrokStyle Inc.公司网站：grokstyle.comGrokStyle正在开发一款视觉搜索软件，能即时识别物体。该技术可应用于诸多领域，如室内设计、服装搜素、房地产搜索和产品查找等。当前，GrokStyle专注于室内产品设计。46. H2O.ai公司网站：h2o.aiH2O.ai的主要产品是H2O，即一个领先的开源人工智能平台。通过H2O，许多机器学习模式(线性模型、基于树的集成方法、深度学习)都可以通过R语言、Python、Java、Scala、JSON和H2O的Flow GUI进行训练。一些H2O的“任务关键型应用”包括欺诈、反洗钱、审计、信用评分、用户保险、ICU监测和预测性维护等。47. 碳云智能(iCarbonX)公司网站：icarbonx.com基于世界上最专业、增速最快的全息健康数据，iCarbonX利用最先进的数据挖掘和机器分析技术，提供个性化的健康分析和健康指数预测服务。通过与全球领先的合作伙伴共同努力，iCarbonX希望观察、研究、指导和关注个人健康。目前，许多公司都与iCarbonX建立了合作关系，包括研究机构、制药工厂、医学检测中心、意愿、保险公司和健康管理公司等。48. InsideSales.com公司网站：insidesales.comInsideSales.com创建于2004年，该公司开发了一个强大的平台，利用由人工智能的核心组成部分(大数据、机器学习和预测分析等)驱动的技术来解决销售相关问题，包括销售方、销售物品、销售地点和销售时间等。虽然越来越多的科技公司进入了人工智能领域，但InsideSales.com还是凭借其人工智能驱动的销售加速平台处于该领域的最前沿。49. Kasisto公司网站：kasisto.comKAI是一个会话式人工智能平台，帮助公司通过智能对话随时随地与客户进行互动和交易。KAI驱动的对话机器人和虚拟助手可以帮助支付、交易，进行帐户分析及个人财务管理。50. Kensho Technologies公司网站：kensho.comKensho是领先的实时统计计算系统及可扩展的分析架构，致力于通过先进的技术为金融机构提供市场透明度。Kensho利用大规模并行统计计算、用户友好的可视界面、以及预测分析领域的突破技术为投资专业人士提供下一代的分析平台。51. KITT.AI公司网站：kitt.aiKITT.AI总部位于西雅图，是一家自然语言理解创业公司，由艾伦人工智能研究所和亚马逊Alexa基金等机构资助。KITT.AI开发的可定制热词检测器(hotword detector)和对话引擎ChatFlow可以为任何基于语音或文本的设备/聊天机器人提供在线多回合对话功能。52. KONUX GmbH公司网站：konux.comKONUX GmbH总部位于慕尼黑，该公司结合传感器数据和人工智能，实时了解机器和基础设施的状况。这能让操作员提前分析机器的问题并预测维护需求，从而降低维护成本。利用KONUX GmbH的技术，其客户的设备维护成本最高可降低30%，机器故障率最高可降低70%。53. Logz.io公司网站：logz.ioLogz.io是一个日志分析平台，利用人工智能和机器学习算法来查找IT环境中不断生成的信息量里的关键事件。Logz.io是一个企业级云平台，也是全球最受欢迎的开源日志分析平台。54. Loop AI Labs公司网站：loop.aiLoop AI Labs开发了认知平台Loop Q，该平台具有学习、解释上下文和理解意义的能力，能用任何语言推理暗数据(dark data)。Loop Q通过强化当前劳动力的能力，让让任何行业的公司处于第四次工业革命的最前沿。55. Lunit Inc.公司网站：lunit.ioLunit是一家人工智能技术公司，致力于建立医生和技术间的合作关系，以更好地应对疾病诊断的挑战。Lunit利用视觉感知技术来解释医学图像，并标注有异常的区域。基于在深度学习领域的专业知识，Lunit一直致力于胸部X光异常检测、乳房X光照相术、以及乳腺组织病理切片的自动评级。56. Maluuba公司网站：maluuba.comMaluuba是加拿大一家深度学习技术公司，致力于帮助机器实现思考，推理和与人类一样的方式进行交流的能力。Maluuba的长期愿景是智能机器与人类手牵手，共同推动社会经济向前发展。57. MindMeld公司网站：mindmeld.comMindMeld总部位于旧金山，是一家提供对话式人工智能平台的技术公司。MindMeld的深域对话(Deep-Domain Conversational)人工智能平台支持自然语音或文本输入，可以向另一端的任何应用设备提供对话式用户界面。该用户界面可以是聊天机器人、语音助理，或者是多模态界面(在触摸、点击和语音之间无缝切换)。MindMeld的客户和投资者包括谷歌、三星、Uniqlo、Spotify、英特尔、Telefonica、Liberty Global、IDG、USAA和In-Q-Tel等。58. 出门问问(Mobvoi)公司网站：chumenwenwen.com/en/site/index.html出门问问是一家提供语音搜索技术的创新人工智能公司，其语音搜素技术包括：语音识别、自然语言理解、垂直搜索和前摄搜索。2015年10月，出门问问完成了由谷歌领投的C轮融资。这也是在过去的6年中，谷歌首次直接投资中国公司。自2012年创建以来，出门问问已融资7500万美元，估值为3亿美元(2015年10月)。基于其语音搜索技术，出门问问2014年底还开发了智能手表操作系统Ticwear，并于2015年6月推出了Ticwatch Android智能手表。59. mode.ai公司网站：mode.aiMode.ai是一家提供交互式用户体验的公司，利用计算机视觉和深度学习构建人工智能驱动的视觉聊天机器人(bot)，在移动信息平台上提供创新的对话式商务渠道。60. Nanit公司网站：nanit.comNanit是第一家利用计算机视觉技术监测睡眠数据的公司，不需要佩戴可穿戴设备， 即可为父母提供宝宝的睡眠状态数据，从而让宝宝睡得更香。Nanit婴儿睡眠监测器配备了高清夜视摄像头，可以直接通过手机APP来观察婴儿的睡眠情况。系统还内置小夜灯，可测试室内的温度和湿度。它还能发出舒缓的声音或白噪音，以帮助婴儿入睡。更重要的是，Nanit系统还搭载了智能计算机视觉，可以记录、分析婴儿在晚间的移动次数及方式，推算婴儿在晚间的睡眠质量，父母每早都会收到宝宝前一天晚上的睡眠报告。61. Narrative Science公司网站：narrativescience.comNarrative science的智能系统Quill平台能够理解不同类型数据中的重要信息，然后自动生成完美的书面内容，以提高员工效率。通过对给定主题的数据分析，系统会自动选择合适的写作角度，快速完成一篇具有标准新闻报道结构的文章。而且，系统还可以根据不同的出版社特点，用不同的行文风格“写作”。62. Nauto公司网站：nauto.comNAUTO是一种设备、网络和应用程序，旨在帮助用户以较低的成本升汽车，获得高端汽车才具备的网络和安全功能。NAUTO的系统包括一个收集和处理可视数据的车载设备，经过处理后生成有价值的参考数据，帮助司机高效、安全的驾驶汽车。63. Nexar公司网站：getnexar.comNexar创建于2015年，公司使命就是利用“车辆到车辆”(vehicle-to-vehicle)技术来避免交通事故。Nexar创建了一个相互连接的Dashcam应用网络，用来记录用户的行车状况，并提前警示用户可能发生的潜在交通事故。如果发生车祸，Nexar应用程序可以为司机提供交通事故的证据，帮助车主进行保险索赔。64. Numenta公司网站：numenta.comNumenta的使命是成为机器智能时代的领导者。Numenta认为，大脑是人工智能系统的最佳典范，为构建智能机器提供了路线图。大脑智力的核心——新大脑皮层——控制着一系列功能。Numenta试图建立一种模仿人类大脑新皮层的计算机系统，希望在处理许多认知任务时能接近或胜过人类。Numenta已经发展几个HTM(分层式即时记忆)示例应用程序，展示了广泛的适用性技术，并为开发者提供开源支持。65. Numerai公司网站：numer.aiNumerai是一家基于人工智能技术的对冲基金公司。Numerai建立人工智能的方式比较特别——基于建模用户与数据双匿名的方式，将加密数据开放给用户，以用户贡献的模型去搭建人工智能模型。Numerai每周定期发布加密数据和建模比赛，招揽匿名用户用加密过的交易数据集进行建模和分析。任何人都可以通过匿名的方式参与Numerai的训练营，如果参与者的模型真的成功地预测了市场走势，他们将获得比特币作为奖励。66. nuTonomy公司网站：nutonomy.comnuTonomy由世界知名的机器人和智能车辆技术专家卡尔·伊格尼玛(Karl Iagnemma)和埃米利奥·法佐立(Emilio Frazzoli)创建，总部位于马萨诸塞州剑桥市(Cambridge)，主要为自动驾驶汽车开发软件系统。nuTonomy的系统通过大量自动驾驶汽车提供“点到点”的出行服务。去年8月，nuTonomy开始在新加坡小型商业区“纬壹科技城”(One-North)内测试免费的自动驾驶出租车叫车服务。不久的将来，nuTonomy还将在波士顿公路上测试自动驾驶汽车。67. Orbital Insight公司网站：orbitalinsight.comOrbital Insight处于空间商业化和大数据的交叉点，是对卫星与无人机图像进行大规模分析的先行者。Orbital Insight从其他卫星公司购买图像，利用自学习算法进行深入分析，然后通过提供数据服务来获取利润。例如，通过分析庄稼的卫星图来预测庄稼的长势，从油箱体积分析出全球原油储量，通过分析停车场的数据来预测零售业的销售等。68. Paxata公司网站：paxata.comPaxata是一个自适应数据分析平台，创建于2012年。Paxata提供的自适应信息平台为业务分析师提供一个企业级自服务数据准备系统，能根据分析、运营和监管规定提供所需要的的支持。业务分析师可在一个直观的、可视化的应用程序上工作，无需编码，只要点击鼠标即可获取、搜索、组合、整合及发布数据，同时确保完善的管理和安全性。Paxata总部位于加利福尼亚州红杉市，在纽约州、俄亥俄州、华盛顿特区和新加坡设有办事处。69. Persado公司网站：persado.comPersado提供一个认知内容平台，利用自然语言处理和机器学习来自动生成基于行为和态度的数据，从而创造出对电子邮件、网页和其他营销活动最有效的文案。Persado的潜在用途并不限于电子邮件，还可以用来制作说服病人谨遵医嘱的消息，用来说服人们支付账单，说服人们遵纪守法，或是说服某人在线约会等。70. Petuum, Inc.公司网站：petuum.comPetuum是一个人工智能和深度学习研发平台，致力于帮助企业解决在人工智能和机器学习的部署过程中遇到的瓶颈问题和其他困难。Petuum正在建设下一代的全能AI/ML平台，面向深度学习、预测分析、知识提取、内容摘要提取、集成模型，可用于大量的应用，比如自然语言处理、图像和视频理解，以及数据转移中的反常检测。Petuum支持不同的硬件平台，也可用Python、R、Java、C++和Julia等多种流行语言编程。去年11月， Petuum获得1500万美元A轮风险投资，由Advantech Capital领投，投资方包括腾讯、Northern Light Venture Capital和Oriza Ventures。71. Pilot AI Labs公司网站：pilot.ai深度学习能在很大程度上提高计算机视觉系统的准确性，从而提供革命性的应用，如智能安全摄像头和自动驾驶汽车。但是，要提高计算机视觉系统的准确性，对计算机性能的要求极高，从而限制了这些应用所带来的裨益。而Pilot AI Labs的使命就是通过降低这些算法对计算性能的需求，来推广这些革命性的人工智能应用。今年，Pilot AI Labs的软件将被部署到数百万台设备中。72. Prospera Technologies公司网站：prospera.agProspera正在开发用于农业的人工智能技术，试图把农业从一种基于直觉的实践活动转变为使用统计模型和自动化的活动，类似于当前的芯片制造业，利用各种协议来优化劳动力、灌溉、施肥和喷洒农药。73. Rapidminer公司网站：rapidminer.comRapidminer是业界最大的开源数据科学平台，让所有企业基于数据科学来做出每一个决定。Rapidminer为企业用户提供易用的科学大数据预测性分析服务。RapidMiner具有丰富数据挖掘分析和算法功能，常用于解决各种商业关键问题，如营销响应率、客户细分、客户忠诚度及终身价值、资产维护、资源规划、预测性维修、质量管理、社交媒体监测和情感分析等典型商业案例。RapidMiner解决方案覆盖了各个领域，包括汽车、银行、保险、生命科学、制造业、石油和天然气、零售业及快消行业、通讯业、以及公用事业等。74. Retention Science公司网站：retentionscience.com企业的用户各不相同，有些是新用户，有些从来不买东西，而有些人则是购物狂。Retention Science的人工智能技术以适于用户的方式向每个人推销商品。75. 杭州Rokid Corporation, Ltd.公司网站：rokid.comRokid机器人凭借人工智能和深度学习技术丰富了我们的生活，通过语音和视觉交互，它能前摄性地提供信息和娱乐功能，还能做家务。Rokid不只是一款简单的机器人，还是我们的家庭伙伴，它能通过独有的声纹识别技术来识别家庭成员。Rokid在杭州、北京和美国硅谷设有分部，团队目前拥有50多名成员，公司三位创始人分别来自阿里巴巴、诺基亚和金山。76. ROSS Intelligence公司网站：rossintelligence.comROSS Intelligence创建于2014年，利用人工智能技术让律师专注于最重要的事情上，即客户。ROSS Intelligence开发了一个专有框架LegalCognition，并与早期合作伙伴IBM Watson合并。它允许律师在几秒钟内执行繁琐的研究任务，而不是几个小时。77. Scaled Inference公司网站：scaledinference.comScaled Inference使得新一代的智能软件能够由大众构建，并提供一个开放的共享平台。Scaled Inference由谷歌前工程师创建，并获得了腾讯的投资。78. Semantic Machines, Inc.公司网站：semanticmachines.comSemantic Machines是一家私人控股的人工智能技术开发商，在伯克利和波士顿设有办公室。在获得贝恩资本、General Catalyst和其他投资者的2100万美元投资后，Semantic Machines正在开发一种人工智能技术，用于指导计算机了解语言和语境，并进行自然互动。Semantic Machines计划向战略合作伙伴提供高度可定制的人工智能平台技术。79. Sentient Technologies公司网站：sentient.aiSentient Technologies的使命是通过让企业以更快的速度做出正确的决定，来改变企业解决最复杂、任务关键型问题的方式。Sentient Technologies的专利技术具有感知能力，可为客户提供高质量的解决方案。Sentient Technologies获得了李嘉诚的投资。80. Shift Technology公司网站： shift-technology.comShift Technology为保险公司提供SaaS解决方案，旨在改进和完善欺诈检测。Shift Technology运用大数据和机器学习来识别保险索赔欺诈，其服务为保险公司节省了大量成本。Shift Technology总部位于法国，去年5月在A轮融资中获得1000万美元投资，领投方为Accel。81. Sift Science公司网站：siftscience.comSift Science为全球在线企业提供实时的机器学习防欺诈解决方案。其机器学习软件能自动学习和检测欺诈行为模式，在企业被骗之前发出预警。 此外，Sift Science还推出了一套新的产品旨在检测和减轻其他类型的欺诈和滥用，包括帐户滥用，内容滥用和促销滥用等。82. Sight Machine公司网站：sightmachine.comSight Machine是制造分析领域的领先者，可帮助企业针对自己的运营情况做出更好、更快的决定。Sift Science的分析平台利用人工智能、机器学习和高级分析技术，来帮助企业解决生产质量和生产力方面的挑战。 目前，许多全球500强企业都在使用Sift Science的技术。Sift Science创建于2011年，总部位于美国密歇根州。83. SigOpt公司网站：sigopt.comSigOpt是一个优化平台，可以“放大”企业的研究工作。SigOpt可以优化任何研究工作，并调整到最佳状态。SigOpt基于云的优化算法组合已被业界认可，可进行无缝部署，并全球保险、信用卡、算法交易和消费包装行业的领先企业所使用。84. Skymind公司网站：skymind.ioSkymind创建于2014年，总部位于旧金山，是一家商业智能和企业软件公司。Skymind致力于解决数据分析和机器智能问题。Skymind支持分布式开源框架Deeplearning4j.org和ND4j.org分布式系统，如Hadoop和spark，用于储存、处理和快速分析大量的数据。Skymind 的用途包括诈骗检测、时间序列数据、文本分析、图像/面部识别和语音转文字等。2016年9月，Skymind获得300万美元种子轮融资，投资者包括腾讯、SV Angel、GreatPoint Ventures、Mandra Capital和Y Combinator。85. Snips公司网站：snips.aiSnips是一个SDK，允许任何人仅用一行代码就能创建自己的人工智能助手。Snips注重安全和隐私问题，是第一个完全在设备上运行而不需要服务器的智能助手。Snips拥有41名员工，其中18人从事机器学习，14人从事工程。Snips几位创始人全部拥有博士学位。86. SparkCognition公司网站：sparkcognition.comSparkCognition是一家全球领先的认知计算分析公司，成功地部署了先进的机器学习和人工智能算法，旨在发现隐藏在物联网和网络环境中的异常和网络威胁，同时自动调查并提出解决方案。87. TalkIQ公司网站：talkiq.com语音通话是企业最重要的数据流，占客户联系的68%，而电子邮件和聊天仅占21%。TalkIQ为企业语音通话开发了专有语音识别人工智能引擎，以帮助对这些重要数据进行分析。TalkIQ允许销售和客服团队首次以科学的方法来了解和优化其工作流程中的关键部分，包括识别购买意图、对反对意见的处理、对竞争对手的反应，以及建立融洽关系等。88. Talla公司网站：talla.comTalla允许企业将一个聊天平台变成一个指挥中心，该公司将消息传递系统(如Slack)中管理信息工作流所需的工具、流程和智能自动化结合起来，从而在聊天中启动业务流程，如招聘员工、新技能培训、投票、构建自定义体验等。Talla创建于2015年，总部位于马萨诸塞州剑桥市。89. Tamr Inc.公司网站：tamr.comTamr是一家整合不同数据源并帮助企业理解分析数据的大数据初创企业，利用算法在不同来源的数据库中搜索想要的数据。Tamr对企业所有的数据源建立一个集中目录，然后进行统一的数据展示和管理。这种做法可以令企业对自己的数据有一个全面的掌握，并能有效防止数据泄露。Tamr创建于2015年，总部位于马萨诸塞州剑桥市。2015年6月，Tamr在B轮融资中获得2520万美元投资。91. Trifacta公司网站：trifacta.comTrifacta创建于2012年，是一家提供数据分析解决方案的创业公司。Trifacta利用人机交互技术、可扩展的数据管理和机器学习技术，让用户轻松地将原始的复杂数据转换为结构化数据以进行分析，从而提高现有分析过程的效率。90. Textio公司网站：textio.comTextio能在文章发表前预测出用户所撰写文章的质量及表现，从而让用户的文章做到最好。Trifacta的预测引擎技术将机器学习和自然语言处理相结合，提供招聘、写作评分等应用的解决方案。Textio成立于2014年秋，由微软和亚马逊的两名前员工创建。2015年12月，Textio在A轮融资中获得800万美元投资。92. twoXAR公司网站：twoxar.comtwoXAR一家软件驱动的药物发现公司，利用其计算平台来确定有希望的候选药物，从而在临床研究前降低风险，并通过行业合作将候选药物推广到临床。该平台迄今已经与多家医院合作，被应用于80多种疾病。twoXAR创建于2014年，总部位于加州帕洛阿尔托(Palo Alto)。93. 深圳优必选(Ubtech)公司网站：ubtrobot.com优必选是一家集人工智能和人形机器人研发、平台软件开发运用及产品销售为一体的全球性高科技企业，它是中国第一家将人形机器人商业化的公司。2016年成功推出Jimu机器人品牌，并成功入驻全球部分AppleStore零售店。公司估值已经超过10亿美元，成为机器人领域的“独角兽”。优必选总部位于中国深圳。94. Verdigris公司网站：verdigris.coVerdigris是一个人工智能和物联网平台，致力于让建筑物更智能，同时降低能耗成本。通过结合专属硬件传感器、机器学习和软件，Verdigris能够“学习”建筑物的能源模式。 其软件能生成综合报告，包括能源预测、障设备报警和维护提醒等。95. Vicarious Systems公司网站：vicarious.comVicarious是一家人工智能研究公司，基于大脑计算原理来开发软件，让这些软件能像人一样思考和学习。利用一种名为“Recursive Cortical Network”新算法，Vicarious开发出了一套视觉感知系统，能够像人脑一样解释图片和视频中的内容。Vicarious的技术在机器人、医学图像分析、图像和视频搜索等领域拥有广阔的应用空间。96. Voyager Labs公司网站：voyagerlabs.coVoyager Labs的技术能实时分析来自己世界各地的数十亿个数据点，以了解和预测人类和群体的行为，并提出实时可行的建议 。Voyager Labs创建于2012年，在纽约、华盛顿和伦敦设有办公室，研发中心位于以色列，公司目前拥有90多名员工。97. x.ai公司网站：x.aix.ai是一款人工智能个人助理，可以为你安排会议。 无需登录、密码或下载，只需要把相关内容抄送给amy@x.ai即可。接下来，x.ai就接管了繁琐的电子邮件安排会议。x.ai创建于2014年，总部位于纽约，投资者包括IA Ventures、irstmark、Two Sigma Ventures、软银、DCM和普利兹克集团(Pritzker)。98. Zebra Medical Vision公司网站：Zebra-med.comZebra Medical Vision正在建立一个医学成像平台，该提供了一个提供基于云、完全托管的研发环境，包括获取大量的结构化数据集、去标识研究、存储、GPU计算能力，以及一系列研究工具的支持。2015年，Zebra Medical Vision获得由Khosla Ventures领投的800万美元投资。99. Zoox公司网站：www.zoox.comZoox是一家机器人公司，目前正在开发全自动驾驶汽车，并打造让该技术走向市场所需要的支持生态系统。Zoox曾展示过一款运动型概念车，这款车没有挡风玻璃，可以朝两头行驶，乘客在车内像在火车上一样相对而坐。Zoox还曾表示，准备用无人驾驶汽车来提供类似于Uber的打车服务。Zoox员工主要来自谷歌、苹果和特斯拉，已获得AID Partners Capital 2000万美元投资。100. Zymergen公司网站：zymergen.comZymergen是一家合成生物科技初创公司，创建于2013年。该公司致力于利用机器学习技术找出微生物内部可能导致所需结果的基因组，例如能产生某种蛋白质的基因组。该方法旨在取代目前基于容错的实验方法，而且有可能生成更多的有效微生物。2016年10月，Zymergen宣布在B轮融资中获得1.3亿美元投资。该轮融资由软银领投，参投方包括DCVC、True Ventures和AME Cloud Ventures等。(编译/谭燃)"}
{"content2":"1.一些优秀博文图像处理和计算机视觉中的经典论文http://www.cnblogs.com/moondark/archive/2012/04/20/2459594.html计算机视觉、机器学习相关领域论文和源代码大集合http://blog.csdn.net/zouxy09/article/details/8550952计算机视觉方面的期刊会议http://blog.csdn.net/huangynn/article/details/7910649计算机视觉与模式识别国际期刊整理http://flyingdeerlu.blog.sohu.com/208128546.html计算机相关专业EI及SCI国际会议及期刊汇总http://jsjxy.usc.edu.cn/info/2047/2372.htm计算机视觉领域的一些牛人博客，超有实例的研究机构等网站链接http://blog.csdn.net/carson2005/article/details/66011092.论文cvpr2015http://www.pamitc.org/cvpr15/program.phpDr.Zdenek Kalal　　TLD官网http://personal.ee.surrey.ac.uk/Personal/Z.Kalal/原文翻译http://wenku.baidu.com/link?url=LmAcHKC5wGpUfCYFHRM6YWZ3T9hQ1ZbzN_HS8eDroOaexR__ImE4JyVPcZXRu-Uk16fj3tBDdjk2jofs1tz12JapIDclNyzHFscCvLbJ7Ri相关http://blog.csdn.net/windtalkersm/article/details/8018980http://www.tldvision.com/Real-time Compressive Trackinghttp://www4.comp.polyu.edu.hk/~cslzhang/CT/CT.htmVibehttp://www.telecom.ulg.ac.be/research/vibe/Computer Vision Softwarehttp://www.cs.cmu.edu/~cil/v-source.htmlCVonline: Image Databaseshttp://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm计算机视觉网http://cvchina.net/3.其他机器视觉商城http://shixinhua.com/mall/"}
{"content2":"涉足计算机视觉领域要知道的做 机器视觉和图像处理方面的研究工作，最重要的两个问题：其一是要把握住国际上最前沿的内容；其二是所作工作要具备很高的实用背景。解决第一个问题的办法就 是找出这个方向公认最高成就的几个超级专家(看看他们都在作什么)和最权威的出版物(阅读上面最新的文献)，解决第二个问题的办法是你最好能够找到一个实 际应用的项目，边做边写文章。 做好这几点的途径之一就是利用网络资源，利用权威网站和专家们的个人主页。依照下面目录整理：[1]研究群体(国际国内)[2]专家主页[3]前沿国际国内期刊与会议[4]搜索资源[5]GPL软件资源一、研究群体用来搜索国际知名计算机视觉研究组(CV Groups)：国际计算机视觉研究组清单http://peipa.essex.ac.uk/info/groups.html美国计算机视觉研究组清单 http://peipa.essex.ac.uk/info/groups.html#USAhttp://www-2.cs.cmu.edu/~cil/vision.html或 http://www.cs.cmu.edu/~cil/vision.html这是卡奈基梅隆大学的计算机视觉研究组的主页，上面提供很全的资料，从发表文章的下载到演示程序、测试图像、常用链接、相关软硬件，甚至还有一个搜索引擎。著名的有人物Tomasi， Kanade等。卡内基梅隆大学双目实验室http://vision.middlebury.edu/stereo/卡内基梅隆研究组http://www.cs.cmu.edu/~cil/v-groups.html还有几个实验室：Calibrated Imaging Laboratory 图像Digital Mapping Laboratory 映射Interactive Systems Laboratory 互动Vision and Autonomous Systems Center视觉自适应http://www.via.cornell.edu/康奈尔大学的计算机视觉和图像分析研究组，好像是电子和计算机工程系的。侧重医学方面的研究，但是在上面有相当不错资源，关键是它正在建设中，能够跟踪一些信息。Cornell University——Robotics and Vision grouphttp://www-cs-students.stanford.edu/ 斯坦福大学计算机系主页1. http://white.stanford.edu/2. http://vision.stanford.edu/3. http://ai.stanford.edu/美国斯坦福大学人工智能机器人实验室The Stanford AI Lab (SAIL) is the intellectual home for researchers in the Stanford Computer Science Department whose primary research focus is Artificial Intelligence. The lab is located in the Gates...Vision and Imaging Science and Technologyhttp://www.fmrib.ox.ac.uk/analysis/主 要研究：Brain Extraction Tool， Nonlinear noise reduction， Linear Image Registration， Automated Segmentation， Structural brain change analysis， motion correction， etc.http://www.cse.msu.edu/prip/—密歇根州立大学计算机和电子工程系的模式识别--图像处理研究组，它的FTP上有许多的文章(NEW)。美国密歇根州大学认知模型和图像处理实验室The Pattern Recognition and Image Processing (PRIP) Lab faculty and students investigate the use of machines to recognize patterns or objects. Methods are developed to sense objects， to discover which...http://www.cse.msu.edu/rgroups/prip/http://pandora.inf.uni-jena.de/p/e/index.html德国的一个数字图像处理研究小组，在其上面能找到一些不错的链接资源。柏林大学 http://www.cv.tu-berlin.de/德国波恩大学视觉和认识模型小组Computer Vision Group located within the Division III of the Computer Science Department in the University of Bonn in Germany. This server offers information on topics concerning our computer vision http://www-dbv.informatik.uni-bonn.de/http://www-staff.it.uts.edu.au/~sean/CVCC.dir/home.htmlCVIP(used to be CVCC for Computer Vision and Cluster Computing) is a research group focusing on cluster-based computer vision within the Spiral Architecture.http://cfia.gmu.edu/The mission of the Center for Image Analysis is to foster multi-disciplinary research in image， multimedia and related technologies by establishing links between academic institutes， industry and government agencies， and to transfer key technologies to help industry build next generation commercial and military imaging and multimedia systems.英国的Bristol大学的Digital Media Group在高级图形图像方面不错。主要就是涉及到场景中光线计算的问题，比如用全局光照或是各种局部光照对高动态图的处理，还有近似真实的模拟现实环境 (照片级别的)，还有用几张照片来建立3D模型(人头之类的)。另外也有对古代建筑模型复原。http://www.cs.bristol.ac.uk/Research/Digitalmedia/而且根据Times全英计算机排名在第3， 也算比较顶尖的研究了http://www.cmis.csiro.au/IAP/zimage.htm这是一个侧重图像分析的站点，一般。但是提供一个Image Analysis环境---ZIMAGE and SZIMAGE。麻省理工视觉实验室MIT http://groups.csail.mit.edu/vision/welcome/AI Laboratory Computer Vision groupCenter for Biological and Computational LearningMedia Laboratory， Vision and Modeling GroupPerceptual Science groupUC Berkeley http://0-vision.berkeley.edu.ilstest.lib.neu.edu/vsp/index.htmlhttp://www.cs.berkeley.edu.ilste ... n/vision_group.html加州大学伯克利分校视觉实验室David A. Forsyth：http://www.cs.berkeley.edu/~daf/UCLA(加州大学洛杉矶分校) http://vision.ucla.edu/视觉实验室英国牛津的A.Zisserman：http://www.robots.ox.ac.uk/~az/ 机器人实验室美国南加州大学智能机器人和智能系统研究所University of Southern California， Los AngelesIRIS is an interdepartmental unit of USC's School of Engineering with ties to USC's Information Sciences Institute (ISI). Members include faculty， graduate students， and research staff associated with... http://iris.usc.edu/ Computer Vision 实验室美国南加州大学计算机视觉实验室介绍：Computer Vision Laboratory at the University of Southern California is one of the major centers of computer vision research for thirty years. they conduct research in a number of basic and applied are...http://iris.usc.edu/USC-Computer-Vision.html英国约克大学高级计算机结构神经网络小组The Advanced Computer Architecture Group has had a thriving research programme in neural networks for over 10 years. The 15 researchers， led by Jim Austin， focus their work in the theory and applicati...http://www.cs.york.ac.uk/arch/neural/瑞士戴尔莫尔感知人工智能研究所IDIAP is a research institute established in Martigny in the Swiss Alps since 1991. Active in the areas of multimodal interaction and multimedia information management， the institute is also the leade...http://www.idiap.ch/英国萨里大学视觉，语言和信号处理中心The Centre for Vision， Speech and Signal Processing (CVSSP) is more than 60 members strong， comprising 12 academic staff， 18 research fellows and more than 44 research students. The activities of the ...http://www.ee.surrey.ac.uk/Research/VSSP/美国阿默斯特马萨诸塞州立大学计算机视觉实验室The Computer Vision Laboratory was established in the Computer Science Department at the University of Massachusetts in 1974 with the goal of investigating the scientific principles underlying the con...http://vis-www.cs.umass.eduUniversity of Massachusetts——Computer Vision Laboratory for Perceptual Robotics美国芝加哥伊利诺伊斯大学贝克曼研究中心智能机器人和计算机视觉实验室Includes the following groups: Professor Seth Hutchinson's Research Group Professor David Kriegman's Research Group Professor Jean Ponce's Research Group Professor Narendra Ahuja's Research Gro...http://www-cvr.ai.uiuc.edu/Computer Vision and Robotics LaboratoryVision Interfaces and Systems Laboratory (VISLab)英国伯明翰大学计算机科学学校视觉研究小组The vision group at the School of Computer Science (a RAE 5 rated department) performs research into a wide variety of computer vision and image understanding areas. Much of this work is performed in ...http://www.cs.bham.ac.uk/research/vision/微软研究院机器学习与理解研究小组 / 计算机视觉小组The research group focuses on the development of more advanced and intelligent computer systems through the exploitation of statistical methods in machine learning and computer vision. The site lists ...http://research.microsoft.com/mlp/http://research.microsoft.com/en-us/groups/vision/微软公司的文献：http://research.microsoft.com/research/pubs微软亚洲研究院：http://research.microsoft.com/asia/，值得关注Harry Shum， Jian Sun， Steven Lin， Long Quan(兼职HKUST)etc.瑞典隆德大学数学系视觉组：http://www.maths.lth.se/matematiklth/personal/andersp/感觉国外搞视觉的好多是数学系出身，大约做计算机视觉对数学要求很高吧。澳大利亚国立大学：http://users.rsise.anu.edu.au/~hartley/美国北卡大学：http://www.cs.unc.edu/~marc/法国INRIA：http://www-sop.inria.fr/odyssee/team/ 由Olivier.Faugeras领衔的牛人众多。比利时鲁汶大学的L.Van Gool： www.esat.kuleuven.ac.be/psi/visics/据说在这个只有中国一个小镇大小的地方的鲁汶大学在欧洲排行top10，名列世界top100，还出了几个诺贝尔奖，视觉研究也很强.美国明德http://vision.middlebury.edu/stereo/以下含有非顶尖美国学校研究组，没有链接(个别的上面已经提到)，供参考。Amerinex Applied Imaging， Inc.Boston UniversityImage and Video Computing Research groupUniversity of California at Santa Barbara加州大学芭芭拉分校Vision Research LabUniversity of California at San Diego加州大学圣迭戈分校Computer Vision & Robotics Research LaboratoryVisual Computing laboratoryUniversity of California at Irvine加州大学欧文分校，加州南部一城，在圣安娜东南，Computer Vision laboratoryUniversity of California， Riverside加州大学河滨分校Visualization and Intelligent Systems Laboratory (VISLab)University of California at Santa CruzPerceptual Science LaboratoryCaltech (加州理工)Vision groupUniversity of Central FloridaComputer Vision laboratoryUniversity of FloridaCenter for Computer Vision and VisualizationColorado State UniversityComputer Vision groupColumbia UniversityAutomated Vision Environment (CAVE)Robotics groupUniversity of Georgia， AthensVisual and Parallel Computing LaboratoryHarvard University（哈佛）Robotics LaboratoryUniversity of Illinois at Urbana-ChampaignRobotics and Computer VisionUniversity of IowaDivision of Physiologic ImagingJet Propulsion LaboratoryMachine Vision and Tracking Sensors groupKhoral Research， IncLawrence Berkeley LaboratoriesImaging and Collaborative Computing GroupImaging and Distributed ComputingLehigh UniversityImage Processing and Pattern Analysis LabVision And Software Technology LaboratoryUniversity of LouisvilleComputer Vision and Image Processing LabUniversity of MarylandComputer Vision LaboratoryUniversity of MiamiUnderwater Vision and Imaging LaboratoryUniversity of Michigan密歇根AI LaboratoryMichigan State University 密歇根州立Pattern Recognition and Image Processing laboratoryEnvironmental Research Institute of Michigan (ERIM) 密歇根大学有汽车车身检测研究University of Missouri-ColumbiaComputational Intelligence Research LaboratoryNECComputer Vision and Image ProcessingUniversity of NevadaComputer Vision LaboratoryNotre-Dame UniversityVision-Based Robotics using EstimationOhio State UniversitySignal Analysis and Machine Perception LaboratoryUniversity of PennsylvaniaGRASP laboratoryMedical Image Processing groupVision Analysis and Simulation Technologies (VAST) LaboratoryPenn State University 宾夕法尼亚大学Computer VisionPrecision Digital ImagesPurdue University普渡大学Robot Vision laboratoryVideo and Image Processing Laboratory (VIPER)Rensselaer Polytechnic Institute (RPI)Computer Science VisionUniversity of RochesterCenter for Electronic Imaging SystemsVision and Robotics laboratoryRutgers University (The State University of New Jersey)Image Understanding LabUniversity of Southern CaliforniaComputer VisionUniversity of South FloridaImage Analysis Research groupStanford Research Institute International (SRI)RADIUS -- Research and Development for Image Understanding SystemsThe Perception program at SRI's AI CenterSUNY at Stony BrookComputer Vision LabUniversity of TennesseeImaging， Robotics and Intelligent Systems laboratoryUniversity of Texas， AustinLaboratory for Vision SystemsUniversity of UtahCenter for Scientific Computing and ImagingRobotics and Computer VisionUniversity of VirginiaComputer Vision Research (CS)University of WashingtonImage Computing Systems LaboratoryInformation Processing LaboratoryCVIA LaboratoryUniversity of West FloridaImage Analysis/Robotics Research LaboratoryUniversity of WisconsinComputer Vision groupVanderbilt UniversityCenter for Intelligent SystemsWashington State UniversityImaging Research laboratoryWright-PattersonModel-Based Vision laboratoryWright State UniversityIntelligent Systems LaboratoryUniversity of WyomingWyoming Image and Signal Processing Research (WISPR)Yale UniversityComputational Vision Group http://www.cs.yale.edu/School of Medicine， Image Processing and Analysis group国内：中科院模式识别国家重点实验室 http://www.nlpr.ia.ac.cn/English/rv/mainpage.html虹膜识别、掌纹识别、人脸识别、莲花山http://www.stat.ucla.edu/~sczhu/Lotus/天津大学精密测试技术及仪器国家重点实验室研究方向包括：激光及光电测试技术、传感及测量信息技术、微纳测试与制造技术、制造质量控制技术。该实验室是国内精密测试领域惟一的国家重点实验室。“智能微系统及其集成应用技术”、“微结构光学测试技术”、“油气储运安全检测技术”、“先进制造中的视觉测量及其关键技术”、“正交偏振激光器原理、特性及其在精密计量中的应用研究”等5项代表性成果（07.3）。中科院长春光机所 http://www.ciomp.ac.cn/ny/keyan.asp中科院沈阳自动化所http://www.sia.ac.cn/index.php中科院西安光机所http://www.opt.ac.cn/yanjiushi/gpcxjs1.htm北京大学智能科学系http://www.cis.pku.edu.cn/vision/vision.htm三维视觉计算与机器人，生物特征识别与图像识别二、专家网页http://www.ai.mit.edu/people/wtf/这位可是MIT人工智能实验室的BILL FREEMAN。专长是：理解--贝叶斯模型。http://www.merl.com/people/brand/MERL(Mitsubishi Electric Research Laboratory)中的擅长“Style Machine”。http://research.microsoft.com/~ablake/CV界极有声望的A.Blake 1977年毕业于剑桥大学三一学院并或数学与电子科学学士学位。之后在MIT，Edinburgh，Oxford先后组建过研究小组并成为Oxford的教授，直到1999年进入微软剑桥研究中心。主要工作领域是计算机视觉。http://www-2.cs.cmu.edu/afs/cs.cmu.edu/user/har/Web/home.html这位专家好像正在学习汉语，主页并且搜集了诸如“两只老虎(Two Tigers)”的歌曲。他的主页上面还有几个专家：Shumeet Baluja， Takeo Kanade。他们的Face Detection作的绝对是世界一流。毕业于卡奈基梅隆大学的计算机科学系，兴趣是计算机视觉。三、前沿国际国内期刊与会议这里的期刊大部分都可以通过上面的专家们的主页间接找到1.国际会议 2.国际期刊 3.国内期刊 4.神经网络 5.CV 6.数字图象 7.教育资源，大学 8.常见问题1. 国际会议现在，国际上计算机视觉方面的三大国际会议是ICCV， CVPR和ECCV，统称之为ICE。ICCV的全称是International Comference on Computer Vision。ICCV两年一次，与ECCV正好错开，是公认的三个会议中级别最高的。ECCV的全称是Europeon Conference on Computer Vision，是一个欧洲的会议。CVPR的全称是Internaltional Conference on Computer Vision and Pattern Recogintion国际计算机视觉与模式识别会议。这是一个一年一次的会议，举办地在美国。ICIP—BMVC—MVA—国际模式识别会议(ICPR )：亚洲计算机视觉会议(ACCV)：2.国际期刊以计算机视觉为主要内容之一的国际刊物也有很多，如:International Journal of Computer VisionIEEE Trans. On PAMI http://www.computer.org/tpami/index.htmIEEE Transactionson Image Processing http://www.ieee.org/organizations/pubs/transactions/tip.htmPattern Recognition http://www.elsevier.com/locate/issn/00313203Pattern Recognition Letters http://www.elsevier.com/locate/issn/01678655IEEE Trans. on Robotics and Automation，IEEE TPAMIIEEE TIPCVGIP Computer Vision. Graphics and Image Processing，Visual Image Computing，IJPRAI(Internatiorial Journat of Pattern Recognition and Artificial Intelligence)众 所周知， computer vision(cv) 存在ICCV/CVPR/ECCV三个顶级会议，它们档次差不多，都应该在一流会议行列， 没有必要给个高下。有些us的人认为ICCV/CVPR略好于ECCV，而欧洲人大都认为ICCV/ECCV略好于CVPR，某些英国的人甚至认为 BMVC好于CVPR。简言之， 三个会议差不多， 各有侧重和偏好。笔者就个人经验浅谈三会异同， 以供大家参考和讨论。 三者乃cv领域的旗舰和风向标，其oral paper (包括best paper) 代表当年度cv的最高水准， 在此引用Harry Shum的一句话， 想知道某个领域在做些什么，找最近几年此领域的proceeding看看就知道了。 ICCV/CVPR由IEEE Computer Society牵头组织， ECCV好像没有专门负责的组织。 CVPR每年(除2002年)都在美国开， ECCV每两年开一次，仅限欧洲， ICCV也是每两年一次，各洲轮值。 基本可以保证每年有两个会议开， 这样研究者就有两次跻身牛会的机会。就 录取率而言， 三会都有波动。 如ICCV2001录取率>30%，且出现两个人(华人)各有三篇第一作者的paper的情况， 这在顶级牛会是不常见的 (灌水嫌疑)。 但是， ICCV2003， 2005两次录取率都很低， 大约20%左右。 ECCV也是类似规律， 在2004年以前都是>30%， 2006年降低到20%左右。 CVPR的录取率近年来一直偏高，从2004年开始一直都在[25%，30%]。最近一次CVPR2006是28.1%， CVPR2007还不知道统计数据。笔者猜测为了维持录取paper的绝对数量， 当submission少的时候录取率偏高， 反之偏低，近几年三大会议的投稿数量全部超过1000， 相对2000年前， 三会录取率均大幅度降低，最大幅度50%->20%。 对录取率走势感兴趣的朋友， 可参考 http://vrlab.epfl.ch/~ulicny/statistics/(CVPR2004的数据是错的)，http://www.adaptivebox.net/research/bookmark/CICON_stat.html.显 然， 投入cv的人越来越多，这个领域也是越来越大， 这点颇不似machine learning一直奉行愚蠢的小圈子主义。另外一点值得注意， ICCV/ECCV只收vision相关的topic，而cvpr会收少量的pattern recognition paper， 如finger print等，但是不收和image/video完全不占边的pr paper，如speech recognition等。我一个朋友曾经review过一篇投往CVPR的speech的paper， 三个reviewer一致拒绝，其中一个reviewer搞笑的指出， 你这篇paper应该是投ICASSP被据而转投CVPR的。 就topic而言， CVPR涵盖最广。 还有一个没有验证过的原因导致CVPR录取率高: 很多us的researcher不愿意或没有足够的经费到us以外的地方开会， 故CVPR会优先接收很多来自us的paper (让大家都happy)。以上对三会的分析对我们投paper是很有指导作用的。 目前的research我想绝大部分还是纸上谈兵， 必经 read paper -> write paper -> publish paper -> publish paper on top conferences and journals流程。故了解投paper的一些基本技巧， 掌握领域的走向和热点， 是非常必要的。 避免做无用功，选择切合的topic，改善presentation， 注意格式 (遵守规定的模板)， 我想这是很多新手需要注意的问题。如ICCV2007明文规定不写summary page直接reject， 但是仍然有人忽视， 这是相当不值得的。3.国内期刊自动化学报、计算机学报、软件学报、电子学报，中国图象图形学报，模式识别与人工智能，光电子激光，精密光学工程等。4.神经网络神经网络-Neural Networks Tutorial Reviewhttp://hem.hj.se/~de96klda/NeuralNetworks.htmftp://ftp.sas.com/pub/neural/FAQ.htmlImage Compression with Neural Networkshttp://www.comp.glam.ac.uk/digimaging/neural.htmBackpropagator's Reviewhttp://www.dontveter.com/bpr/bpr.htmlBibliographies on Neural Networkshttp://liinwww.ira.uka.de/bibliography/Neural/Intelligent Motion Control with an Artificial Cerebellumhttp://www.q12.org/phd.htmlKernel Machineshttp://www.kernel-machines.org/Some Neural Networks Research Organizationshttp://www.ieee.org/nnc/http://www.inns.org/Neural Network Modeling in Vision Researchhttp://www.rybak-et-al.net/nisms.htmlNeural Networks and Machine Learninghttp://learning.cs.toronto.edu/Neural Application Softwarehttp://attrasoft.comNeural Network Toolbox for MATLABhttp://www.mathworks.com/products/neuralnet/Netlab Softwarehttp://www.ncrg.aston.ac.uk/netlab/Kunama Systems Limited http://www.kunama.co.uk/5.Computer Vision(计算机视觉)Annotated Computer Vision Bibliographyhttp://iris.usc.edu/Vision-Notes/bibliography/contents.htmlhttp://iris.usc.edu/Vision-Notes/rosenfeld/contents.htmlLawrence Berkeley National Lab Computer Vision and Robotics Applicationshttp://www-itg.lbl.gov/ITG.hm.pg.docs/VISIon/vision.htmlCVonline by University of EdinburghThe Evolving， Distributed， Non-Proprietary， On-Line Compendium of Computer Vision， www.dai.ed.ac.uk/CVonlineComputer Vision Handbook，www.cs.hmc.edu/~fleck/computer-vision-handbookVision Systems Coursewarewww.cs.cf.ac.uk/Dave/Vision_lecture/Vision_lecture_caller.htmlResearch Activities in Computer Visionhttp://www-syntim.inria.fr/syntim/analyse/index-eng.htmlVision Systems Acronymswww.vision-systems-design.com/vsd/archive/acronyms.htmlDictionary of Terms in Human and Animal Visionhttp://cns-web.bu.edu/pub/laliden/WWW/Visionary/Visionary.htmlMetrology based on Computer Visionwww.cranfield.ac.uk/sme/amac/research/metrology/metrology.html6.Digital Photography 数字图像Digital Photography， Scanning， and Image Processingwww.dbusch.com/scanners/scanners.htm l7.Educational Resources， Universities 教育资源，大学Center for Image Processing in Educationwww.cipe.comLibrary of Congress Call Numbers Related to Imaging Science by Rochester Institute of Technologyhttp://wally2.rit.edu/pubs/guides/imagingcall.htmlMathematical Experiences through Image Processing， University of Washingtonwww.cs.washington.edu/research/metip/metip.htmlVismod Tech Reports and Publications， MIThttp://vismod.www.media.mit.edu/cgi-bin/tr_pagemakerVision Lab PhD dissertation list， University of Antwerphttp://wcc.ruca.ua.ac.be/~visielab/theses.htmlINRIA (France) Research Projects: Human-Computer Interaction， Image Processing， Data Management， Knowledge Systemswww.inria.fr/Themes/Theme3-eng.htmlImage Processing Resourceshttp://eleceng.ukc.ac.uk/~rls3/Contents.htmPublications of Carsten Stegerhttp://www9.informatik.tu-muench ... r/publications.html8.FAQs（常见问题）comp.dsp FAQwww.bdti.com/faq/dsp_faq.htmRobotics FAQwww.frc.ri.cmu.edu/robotics-faqWhere's the sci.image.processing FAQ?www.cc.iastate.edu/olc_answers/p ... processing.faq.htmlcomp.graphics.algorithms FAQ， Section 3， 2D Image/Pixel Computationswww.exaflop.org/docs/cgafaqAstronomical Image Processing System FAQwww.cv.nrao.edu/aips/aips_faq.html四、搜索资源http://sal.kachinatech.com/http://cheminfo.pku.edu.cn/mirrors/SAL/index.shtml 北京大学Google输入：computer vision 或computer vision groups可以获得很多结果网络资源：CVonline http://homepages.inf.ed.ac.uk/rbf/CVonline/视觉研究组列表Computer vision test Image http://www.cs.cmu.edu/~cil/v-images.html卡内基梅隆标准图片库视觉论文搜索：Paper searchhttp://www.researchindex.com五、图像处理GPL库（代码库图像库等）http://www.ph.tn.tudelft.nl/~klamer/cppima.htmlCppima 是一个图像处理的C++函数库。这里有一个较全面介绍它的库函数的文档，当然你也可以下载压缩的GZIP包，里面包含TexInfo格式的文档。http://iraf.noao.edu/Welcome to the IRAF Homepage! IRAF is the Image Reduction and Analysis Facility， a general purpose software system for the reduction and analysis of astronomical datahttp://entropy.brni-jhu.org/tnimage.html一个非常不错的Unix系统的图像处理工具，看看它的截图。你可以在此基础上构建自己的专用图像处理工具包。http://sourceforge.net/projects/这是GPL软件集散地，可以搜索IP库。国内的CSDN http://www.csdn.net/本文引用地址：http://blog.sciencenet.cn/blog-337448-411977.html  此文来自科学网马琳博客，转载请注明出处。"}
{"content2":"原文：http://www.cnblogs.com/polly333/p/5130375.html三种匹配算法比较BM算法：该算法代码：view plaincopy to clipboardprint?CvStereoBMState *BMState = cvCreateStereoBMState();int SADWindowSize=15;BMState->SADWindowSize = SADWindowSize > 0 ? SADWindowSize : 9;BMState->minDisparity = 0;BMState->numberOfDisparities = 32;BMState->textureThreshold = 10;BMState->uniquenessRatio = 15;BMState->speckleWindowSize = 100;BMState->speckleRange = 32;BMState->disp12MaxDiff = 1;cvFindStereoCorrespondenceBM( left, right, left_disp_,BMState);cvNormalize( left_disp_, left_vdisp, 0, 256, CV_MINMAX );其中minDisparity是控制匹配搜索的第一个参数，代表了匹配搜苏从哪里开始，numberOfDisparities表示最大搜索视差数uniquenessRatio表示匹配功能函数，这三个参数比较重要，可以根据实验给予参数值。该方法速度最快，一副320*240的灰度图匹配时间为31ms，视差图如下。这图是解释BM（bidirectional matching)算法的很好的例子。进行双向匹配，首先通过匹配代价在右图中计算得出匹配点。然后相同的原理及计算在左图中的匹配点。比较找到的左匹配点和源匹配点是否一致，如果你是，则匹配成功。第二种方法是SGBM方法这是OpenCV的一种新算法：view plaincopy to clipboardprint?cv::StereoSGBM sgbm;sgbm.preFilterCap = 63;int SADWindowSize=11;int cn = 1;sgbm.SADWindowSize = SADWindowSize > 0 ? SADWindowSize : 3;sgbm.P1 = 4*cn*sgbm.SADWindowSize*sgbm.SADWindowSize;sgbm.P2 = 32*cn*sgbm.SADWindowSize*sgbm.SADWindowSize;sgbm.minDisparity = 0;sgbm.numberOfDisparities = 32;sgbm.uniquenessRatio = 10;sgbm.speckleWindowSize = 100;sgbm.speckleRange = 32;sgbm.disp12MaxDiff = 1;sgbm(left , right , left_disp_);sgbm(right, left  , right_disp_);各参数设置如BM方法，速度比较快，320*240的灰度图匹配时间为78ms，视差效果如下图。第三种为GC方法：view plaincopy to clipboardprint?CvStereoGCState* state = cvCreateStereoGCState( 16, 2 );left_disp_  =cvCreateMat( left->height,left->width, CV_32F );right_disp_ =cvCreateMat( right->height,right->width,CV_32F );cvFindStereoCorrespondenceGC( left, right, left_disp_, right_disp_, state, 0 );cvReleaseStereoGCState( &state );该方法速度超慢，但效果超好。各方法理论可以参考文献。函数解释参数注释（1）StereoBMState// 预处理滤波参数preFilterType：预处理滤波器的类型，主要是用于降低亮度失真（photometric distortions）、消除噪声和增强纹理等, 有两种可选类型：CV_STEREO_BM_NORMALIZED_RESPONSE（归一化响应） 或者 CV_STEREO_BM_XSOBEL（水平方向Sobel算子，默认类型）, 该参数为 int 型；preFilterSize：预处理滤波器窗口大小，容许范围是[5,255]，一般应该在 5x5..21x21 之间，参数必须为奇数值, int 型preFilterCap：预处理滤波器的截断值，预处理的输出值仅保留[-preFilterCap, preFilterCap]范围内的值，参数范围：1 - 31（文档中是31，但代码中是 63）, int// SAD 参数SADWindowSize：SAD窗口大小，容许范围是[5,255]，一般应该在 5x5 至 21x21 之间，参数必须是奇数，int 型minDisparity：最小视差，默认值为 0, 可以是负值，int 型numberOfDisparities：视差窗口，即最大视差值与最小视差值之差, 窗口大小必须是 16 的整数倍，int 型// 后处理参数textureThreshold：低纹理区域的判断阈值。如果当前SAD窗口内所有邻居像素点的x导数绝对值之和小于指定阈值，则该窗口对应的像素点的视差值为 0（That is, if the sum of absolute values of x-derivatives computed over SADWindowSize by SADWindowSize pixel neighborhood is smaller than the parameter, no disparity is computed at the pixel），该参数不能为负值，int 型uniquenessRatio：视差唯一性百分比， 视差窗口范围内最低代价是次低代价的(1 + uniquenessRatio/100)倍时，最低代价对应的视差值才是该像素点的视差，否则该像素点的视差为 0 （the minimum margin in percents between the best (minimum) cost function value and the second best value to accept the computed disparity, that is, accept the computed disparity d^ only if SAD(d) >= SAD(d^) x (1 + uniquenessRatio/100.) for any d != d*+/-1 within the search range ），该参数不能为负值，一般5-15左右的值比较合适，int 型speckleWindowSize：检查视差连通区域变化度的窗口大小, 值为 0 时取消 speckle 检查，int 型speckleRange：视差变化阈值，当窗口内视差变化大于阈值时，该窗口内的视差清零，int 型// OpenCV2.1 新增的状态参数roi1, roi2：左右视图的有效像素区域，一般由双目校正阶段的 cvStereoRectify 函数传递，也可以自行设定。一旦在状态参数中设定了 roi1 和 roi2，OpenCV 会通过cvGetValidDisparityROI 函数计算出视差图的有效区域，在有效区域外的视差值将被清零。disp12MaxDiff：左视差图（直接计算得出）和右视差图（通过cvValidateDisparity计算得出）之间的最大容许差异。超过该阈值的视差值将被清零。该参数默认为 -1，即不执行左右视差检查。int 型。注意在程序调试阶段最好保持该值为 -1，以便查看不同视差窗口生成的视差效果。具体请参见《使用OpenGL动态显示双目视觉三维重构效果示例》一文中的讨论。在上述参数中，对视差生成效果影响较大的主要参数是 SADWindowSize、numberOfDisparities 和 uniquenessRatio 三个，一般只需对这三个参数进行调整，其余参数按默认设置即可。在OpenCV2.1中，BM算法有C和C++ 两种实现模块。（2）StereoSGBMStateSGBM算法的状态参数大部分与BM算法的一致，下面只解释不同的部分：SADWindowSize：SAD窗口大小，容许范围是[1,11]，一般应该在 3x3 至 11x11 之间，参数必须是奇数，int 型P1, P2：控制视差变化平滑性的参数。P1、P2的值越大，视差越平滑。P1是相邻像素点视差增/减 1 时的惩罚系数；P2是相邻像素点视差变化值大于1时的惩罚系数。P2必须大于P1。OpenCV2.1提供的例程 stereo_match.cpp 给出了 P1 和 P2 比较合适的数值。fullDP：布尔值，当设置为 TRUE 时，运行双通道动态编程算法（full-scale 2-pass dynamic programming algorithm），会占用O(W*H*numDisparities)个字节，对于高分辨率图像将占用较大的内存空间。一般设置为 FALSE。注意OpenCV2.1的SGBM算法是用C++ 语言编写的，没有C实现模块。与H. Hirschmuller提出的原算法相比，主要有如下变化：算法默认运行单通道DP算法，只用了5个方向，而fullDP使能时则使用8个方向（可能需要占用大量内存）。算法在计算匹配代价函数时，采用块匹配方法而非像素匹配（不过SADWindowSize=1时就等于像素匹配了）。匹配代价的计算采用BT算法（\"Depth Discontinuities by Pixel-to-Pixel Stereo\" by S. Birchfield and C. Tomasi），并没有实现基于互熵信息的匹配代价计算。增加了一些BM算法中的预处理和后处理程序。（3）StereoGCStateGC算法的状态参数只有两个：numberOfDisparities 和 maxIters ，并且只能通过 cvCreateStereoGCState 在创建算法状态结构体时一次性确定，不能在循环中更新状态信息。GC算法并不是一种实时算法，但可以得到物体轮廓清晰准确的视差图，适用于静态环境物体的深度重构。注意GC算法只能在C语言模式下运行，并且不能对视差图进行预先的边界延拓，左右视图和左右视差矩阵的大小必须一致。原理解释目前立体匹配算法是计算机视觉中的一个难点和热点，算法很多，但是一般的步骤是：A、匹配代价计算匹配代价计算是整个立体匹配算法的基础，实际是对不同视差下进行灰度相似性测量。常见的方法有灰度差的平方SD（squared intensity differences），灰度差的绝对值AD（absolute intensity differences）等。另外，在求原始匹配代价时可以设定一个上限值，来减弱叠加过程中的误匹配的影响。以AD法求匹配代价为例，可用下式进行计算，其中T为设定的阈值。这就是在参数设置中阈值的作用，在视差图中经常有黑色区域，就是和阈值的设置关。B、 匹配代价叠加一般来说，全局算法基于原始匹配代价进行后续算法计算。而区域算法则需要通过窗口叠加来增强匹配代价的可靠性，根据原始匹配代价不同，可分为：此图是核心算法的解释，就是计算区域内像素差值，可以为单个像素也可以为一定区域内，主要看SAD的窗口大小的设置，同时SAD设置决定误匹配的多少和运算效率问题，所以大小设置一定要很慎重。C、 视差获取对于区域算法来说，在完成匹配代价的叠加以后，视差的获取就很容易了，只需在一定范围内选取叠加匹配代价最优的点（SAD和SSD取最小值，NCC取最大值）作为对应匹配点，如胜者为王算法WTA（Winner-take-all）。而全局算法则直接对原始匹配代价进行处理，一般会先给出一个能量评价函数，然后通过不同的优化算法来求得能量的最小值，同时每个点的视差值也就计算出来了。D、视差细化（亚像素级）大多数立体匹配算法计算出来的视差都是一些离散的特定整数值，可满足一般应用的精度要求。但在一些精度要求比较高的场合，如精确的三维重构中，就需要在初始视差获取后采用一些措施对视差进行细化，如匹配代价的曲线拟合、图像滤波、图像分割等。亚像素级的处理就是涉及到BMState参数设置后后续参数的设置了。有关立体匹配的介绍和常见匹配算法的比较，推荐大家看看Stefano Mattoccia 的讲义 Stereo Vision: algorithms and applications，190页的ppt，讲解得非常形象详尽。1． opencv2.1和opencv2.0在做stereo vision方面有什么区别了？2.1版增强了Stereo Vision方面的功能：(1) 新增了 SGBM 立体匹配算法（源自Heiko Hirschmuller的《Stereo Processing by Semi-global Matching and Mutual Information》），可以获得比 BM 算法物体轮廓更清晰的视差图（但低纹理区域容易出现横/斜纹路，在 GCstate->fullDP 选项使能时可消减这种异常纹路，但对应区域视差变为0，且运行速度会有所下降），速度比 BM 稍慢， 352*288的帧处理速度大约是 5 帧/秒；(2) 视差效果：BM < SGBM < GC；处理速度：BM > SGBM > GC ；(3) BM 算法比2.0版性能有所提升，其状态参数新增了对左右视图感兴趣区域 ROI 的支持（roi1 和 roi2，由stereoRectify函数产生）；(4) BM 算法和 GC 算法的核心代码改动不大，主要是面向多线程运算方面的（由 OpenMP 转向 Intel TBB）；(5) cvFindStereoCorrespondenceBM 函数的disparity参数的数据格式新增了 CV_32F 的支持，这种格式的数据给出实际视差，而 2.0 版只支持 CV_16S，需要除以 16.0 才能得到实际的视差数值。2． 用于立体匹配的图像可以是彩色的吗？在OpenCV2.1中，BM和GC算法只能对8位灰度图像计算视差，SGBM算法则可以处理24位（8bits*3）彩色图像。所以在读入图像时，应该根据采用的算法来处理图像：int color_mode = alg == STEREO_SGBM ? 1 : 0;//////////////////////////////////////////////////////////////////////////// 载入图像cvGrabFrame( lfCam );cvGrabFrame( riCam );frame1 = cvRetrieveFrame( lfCam );frame2 = cvRetrieveFrame( riCam );if(frame1.empty()) break;resize(frame1, img1, img_size, 0, 0);resize(frame2, img2, img_size, 0, 0);// 选择彩色或灰度格式作为双目匹配的处理图像if (!color_mode && cn>1){cvtColor(img1, img1gray, CV_BGR2GRAY);cvtColor(img2, img2gray, CV_BGR2GRAY);img1p = img1gray;img2p = img2gray;}else{img1p = img1;img2p = img2;}3． 怎样获取与原图像有效像素区域相同的视差图？在OpenCV2.0及以前的版本中，所获取的视差图总是在左侧和右侧有明显的黑色区域，这些区域没有有效的视差数据。视差图有效像素区域与视差窗口（ndisp，一般取正值且能被16整除）和最小视差值（mindisp，一般取0或负值）相关，视差窗口越大，视差图左侧的黑色区域越大，最小视差值越小，视差图右侧的黑色区域越大。其原因是为了保证参考图像（一般是左视图）的像素点能在目标图像（右视图）中按照设定的视差匹配窗口匹配对应点，OpenCV 只从参考图像的第 (ndisp - 1 + mindisp) 列开始向右计算视差，第 0 列到第 (ndisp - 1 + mindisp) 列的区域视差统一设置为 (mindisp - 1) *16；视差计算到第 width + mindisp 列时停止，余下的右侧区域视差值也统一设置为 (mindisp - 1) *16。static const int DISPARITY_SHIFT = 4; … int ndisp = state->numberOfDisparities; int mindisp = state->minDisparity; int lofs = MAX(ndisp - 1 + mindisp, 0); int rofs = -MIN(ndisp - 1 + mindisp, 0); int width = left->cols, height = left->rows; int width1 = width - rofs - ndisp + 1; short FILTERED = (short)((mindisp - 1) << DISPARITY_SHIFT); initialize the left and right borders of the disparity map for( y = 0; y < height; y++ ) { for( x = 0; x < lofs; x++ ) dptr[y*dstep + x] = FILTERED; for( x = lofs + width1; x < width; x++ ) dptr[y*dstep + x] = FILTERED; } dptr += lofs; for( x = 0; x < width1; x++, dptr++ )这样的设置很明显是不符合实际应用的需求的，它相当于把摄像头的视场范围缩窄了。因此，OpenCV2.1 做了明显的改进，不再要求左右视图和视差图的大小（size）一致，允许对视差图进行左右边界延拓，这样，虽然计算视差时还是按上面的代码思路来处理左右边界，但是视差图的边界得到延拓后，有效视差的范围就能够与对应视图完全对应。具体的实现代码范例如下：////////////////////////////////////////////////////////////////////////// // 对左右视图的左边进行边界延拓，以获取与原始视图相同大小的有效视差区域 copyMakeBorder(img1r, img1b, 0, 0, m_nMaxDisp, 0, IPL_BORDER_REPLICATE); copyMakeBorder(img2r, img2b, 0, 0, m_nMaxDisp, 0, IPL_BORDER_REPLICATE); ////////////////////////////////////////////////////////////////////////// // 计算视差 if( alg == STEREO_BM ) { bm(img1b, img2b, dispb); // 截取与原始画面对应的视差区域（舍去加宽的部分） displf = dispb.colRange(m_nMaxDisp, img1b.cols); } else if(alg == STEREO_SGBM) { sgbm(img1b, img2b, dispb); displf = dispb.colRange(m_nMaxDisp, img1b.cols); }4． cvFindStereoCorrespondenceBM的输出结果好像不是以像素点为单位的视差？“@scyscyao：在OpenCV2.0中，BM函数得出的结果是以16位符号数的形式的存储的，出于精度需要，所有的视差在输出时都扩大了16倍(2^4)。其具体代码表示如下：dptr[y*dstep] = (short)(((ndisp - mind - 1 + mindisp)*256 + (d != 0 ? (p-n)*128/d : 0) + 15) >> 4);可以看到，原始视差在左移8位(256)并且加上一个修正值之后又右移了4位，最终的结果就是左移4位。因此，在实际求距离时，cvReprojectTo3D出来的X/W,Y/W,Z/W都要乘以16 (也就是W除以16)，才能得到正确的三维坐标信息。”在OpenCV2.1中，BM算法可以用 CV_16S 或者 CV_32F 的方式输出视差数据，使用32位float格式可以得到真实的视差值，而CV_16S 格式得到的视差矩阵则需要 除以16 才能得到正确的视差。另外，OpenCV2.1另外两种立体匹配算法 SGBM 和 GC 只支持 CV_16S 格式的 disparity 矩阵。5． 如何设置BM、SGBM和GC算法的状态参数？6． 如何实现视差图的伪彩色显示？首先要将16位符号整形的视差矩阵转换为8位无符号整形矩阵，然后按照一定的变换关系进行伪彩色处理。我的实现代码如下：// 转换为 CV_8U 格式，彩色显示 dispLfcv = displf, dispRicv = dispri, disp8cv = disp8; if (alg == STEREO_GC) { cvNormalize( &dispLfcv, &disp8cv, 0, 256, CV_MINMAX ); } else { displf.convertTo(disp8, CV_8U, 255/(m_nMaxDisp*16.)); } F_Gray2Color(&disp8cv, vdispRGB);灰度图转伪彩色图的代码，主要功能是使灰度图中 亮度越高的像素点，在伪彩色图中对应的点越趋向于 红色；亮度越低，则对应的伪彩色越趋向于 蓝色；总体上按照灰度值高低，由红渐变至蓝，中间色为绿色。其对应关系如下图所示：void F_Gray2Color(CvMat* gray_mat, CvMat* color_mat) { if(color_mat) cvZero(color_mat); int stype = CV_MAT_TYPE(gray_mat->type), dtype = CV_MAT_TYPE(color_mat->type); int rows = gray_mat->rows, cols = gray_mat->cols; // 判断输入的灰度图和输出的伪彩色图是否大小相同、格式是否符合要求 if (CV_ARE_SIZES_EQ(gray_mat, color_mat) && stype == CV_8UC1 && dtype == CV_8UC3) { CvMat* red = cvCreateMat(gray_mat->rows, gray_mat->cols, CV_8U); CvMat* green = cvCreateMat(gray_mat->rows, gray_mat->cols, CV_8U); CvMat* blue = cvCreateMat(gray_mat->rows, gray_mat->cols, CV_8U); CvMat* mask = cvCreateMat(gray_mat->rows, gray_mat->cols, CV_8U); // 计算各彩色通道的像素值 cvSubRS(gray_mat, cvScalar(255), blue); // blue(I) = 255 - gray(I) cvCopy(gray_mat, red); // red(I) = gray(I) cvCopy(gray_mat, green); // green(I) = gray(I),if gray(I) < 128 cvCmpS(green, 128, mask, CV_CMP_GE ); // green(I) = 255 - gray(I), if gray(I) >= 128 cvSubRS(green, cvScalar(255), green, mask); cvConvertScale(green, green, 2.0, 0.0); // 合成伪彩色图 cvMerge(blue, green, red, NULL, color_mat); cvReleaseMat( &red ); cvReleaseMat( &green ); cvReleaseMat( &blue ); cvReleaseMat( &mask ); } }7． 如何将视差数据保存为 txt 数据文件以便在 Matlab 中读取分析？由于OpenCV本身只支持 xml、yml 的数据文件读写功能，并且其xml文件与构建网页数据所用的xml文件格式不一致，在Matlab中无法读取。我们可以通过以下方式将视差数据保存为txt文件，再导入到Matlab中。void saveDisp(const char* filename, const Mat& mat) { FILE* fp = fopen(filename, \"wt\"); fprintf(fp, \"%02d/n\", mat.rows); fprintf(fp, \"%02d/n\", mat.cols); for(int y = 0; y < mat.rows; y++) { for(int x = 0; x < mat.cols; x++) { short disp = mat.at<short>(y, x); // 这里视差矩阵是CV_16S 格式的，故用 short 类型读取 fprintf(fp, \"%d/n\", disp); // 若视差矩阵是 CV_32F 格式，则用 float 类型读取 } } fclose(fp); }相应的Matlab代码为：function img = txt2img(filename) data = importdata(filename); r = data(1); % 行数 c = data(2); % 列数 disp = data(3:end); % 视差 vmin = min(disp); vmax = max(disp); disp = reshape(disp, [c,r])'; % 将列向量形式的 disp 重构为 矩阵形式 % OpenCV 是行扫描存储图像，Matlab 是列扫描存储图像 % 故对 disp 的重新排列是首先变成 c 行 r 列的矩阵，然后再转置回 r 行 c 列 img = uint8( 255 * ( disp - vmin ) / ( vmax - vmin ) ); mesh(disp); set(gca,'YDir','reverse'); % 通过 mesh 方式绘图时，需倒置 Y 轴方向 axis tight; % 使坐标轴显示范围与数据范围相贴合，去除空白显示区显示效果如下：SGBM算法原理emi-global matching（缩写SGM）是一种用于计算双目视觉中disparity的半全局匹配算法。在OpenCV中的实现为semi-global block matching（SGBM）。SGBM的思路是：通过选取每个像素点的disparity，组成一个disparity map，设置一个和disparity map相关的全局能量函数，使这个能量函数最小化，以达到求解每个像素最优disparity的目的。能量函数形式如下：D指disparity map。E(D)是该disparity map对应的能量函数。p, q代表图像中的某个像素Np 指像素p的相邻像素点（一般认为8连通）C(p, Dp)指当前像素点disparity为Dp时，该像素点的costP1 是一个惩罚系数，它适用于像素p相邻像素中dsparity值与p的dsparity值相差1的那些像素。P2 是一个惩罚系数，它适用于像素p相邻像素中dsparity值与p的dsparity值相差大于1的那些像素。I[.]函数返回1如果函数中的参数为真，否则返回0利用上述函数在一个二维图像中寻找最优解是一个NP-complete问题，耗时过于巨大，因此该问题被近似分解为多个一维问题，即线性问题。而且每个一维问题都可以用动态规划来解决。因为1个像素有8个相邻像素，因此一般分解为8个一维问题。考虑从左到右这一方向，如下图所示：则每个像素的disparity只和其左边的像素相关，有如下公式：r指某个指向当前像素p的方向，在此可以理解为像素p左边的相邻像素。Lr(p, d) 表示沿着当前方向（即从左向右），当目前像素p的disparity取值为d时，其最小cost值。这个最小值是从4种可能的候选值中选取的最小值：1.前一个像素（左相邻像素）disparity取值为d时，其最小的cost值。2.前一个像素（左相邻像素）disparity取值为d-1时，其最小的cost值+惩罚系数P1。3.前一个像素（左相邻像素）disparity取值为d+1时，其最小的cost值+惩罚系数P1。4.前一个像素（左相邻像素）disparity取值为其他时，其最小的cost值+惩罚系数P2。另外，当前像素p的cost值还需要减去前一个像素取不同disparity值时最小的cost。这是因为Lr(p, d)是会随着当前像素的右移不停增长的，为了防止数值溢出，所以要让它维持在一个较小的数值。C(p, d)的计算很简单，由如下两个公式计算：即，当前像素p和移动d之后的像素q之间，经过半个像素插值后，寻找两个像素点灰度或者RGB差值的最小值，作为C(p, d)的值。具体来说：设像素p的灰度/RGB值为I(p)，先从I(p)，(I(p)+I(p-1))/2,(I(p)+I(p+1))/2三个值中选择出和I(q)差值最小的,即d(p,p-d)。然后再从I(q)，(I(q)+I(q-1))/2,(I(q)+I(q+1))/2三个值中选择出和I(p)差值最小的,即d(p-d,p)。最后从两个值中选取最小值，就是C(p, d)上面是从一个方向（从左至右）计算出的像素在取值为某一disparity值时的最小cost值。但是一个像素有8个邻域，所以一共要从8个方向计算（左右，右左，上下，下上，左上右下，右下左上，右上左下，左下右上）这个cost值。然后把八个方向上的cost值累加，选取累加cost值最小的disparity值作为该像素的最终disparity值。对于每个像素进行该操作后，就形成了整个图像的disparity map。公式表达如下：SGBM算法遍历每个像素，针对每个像素的操作和disparity的范围有关，故时间复杂度为："}
{"content2":"在本节中，笔者将详细介绍 Azure 认知服务中的一种：计算机视觉 (Computer Vision) API。我的一个客户有需求，他们需要消费者与自己的产品合照，然后上传到服务器并转发到朋友圈。但是为了防止恶意用户上传不健康的照片，需要对图像进行筛查。计算机视觉 API 的分析图像功能，正好有 Adult 参数，可以检测图像是否是色情的，正好满足客户的需求。请注意：本文使用的是国内由世纪互联运维的 Azure China 计算机视觉服务，API 参考。如果是使用的是海外的 Azure China 计算机视觉服务，API参考。计算机视觉解决的问题：1. 分析图像检查图像中发现的视觉内容，分析是否有不健康内容。2. 生成缩略图裁剪和生成缩略图。3. 读取图片中的文字4. 识别名人关键步骤主要有：接下来我们进入 Demo 时间，在开始之前，请先准备 Azure China 账户。第一部分：创建计算机视觉 API，并获得 API Key1. 我们找到需要分析的图片 URL，我这里准备了一张人脸的照片。2. 我们登录 Azure China 管理界面。3. 点击下图的认知服务账户4. 点击 创建认知服务账户。如下图：5. 定价层，我们选择免费。因为是 Demo 环境，我们就用免费，如果是生产环境建议选择标准。6. 创建完认知服务以后，我们点击密钥，获得访问这个计算机视觉 API 的 Access Key。请保留好这个 Key，下面还要继续使用。第二部分：了解 API 参数1. Request URL：2. Reques t 参数：(1) visualFeatures 参数我们可以设置 visualFeatures 为：A. Categories: 对图像内容进行分类B. Tags: 对图像进行标记C. Description: 用完整的英文句子描述图像内容D. Faces: 检测脸部是否存在。 如果存在，生成坐标，性别和年龄E. ImageType: 检测图像是剪贴还是直线绘图F. Color: 确定重音颜色，主色，以及图像是否为黑白G. Adult: 检测图像是否是色情的（描绘裸露或性行为）。 还会检测到性暗示内容。(2) details:如果设置 Celebrities，则可以识别名人。(3) language:默认是 en，英文。可以设置为 zh，简体中文。3. Request headers：(1) Content-Type(2) Ocp-Apim-Subscription-Key:上面的 API Access Key4.Request body：(1)支持的图像为 JPEG, PNG, GIF和BMP(2)图像的尺寸必须小于4MB(3)图像的分辨率至少为50 X 50第三部分：使用 API 测试控制台调试 API，并设置调试 API 的参数我们拿到上面的 API Key，就可以写代码开发了。不过Azure认知服务提供了非常好的控制台，可以方便我们进行API调试。1. 选择打开 API 测试控制台。2. 在 API 控制台，修改以下内容：Query Parameters(1) visualFeatures，我们输入：Categories,Tags,Description,Faces,ImageType,Color,Adult这样识别多个元素。(2) details，我们不输入信息(3) language，使用默认的 enHeaders:(1) Content-Type，我们使用默认值(2) Ocp-Apim-Subscription-Key，我们输入步骤一的 API Access KeyRequest body:(1) 我们修改为：{\"url\":\"https://leizhangstorage.blob.core.chinacloudapi.cn/azureblog/analyzeimagesample.jpg\"}所有参数的修改内容如下图：Request URL 为：https://api.cognitive.azure.cn/vision/v1.0/analyze?visualFeatures=Categories,Tags,Description,Faces,ImageType,Color,Adult&language=en然后我们点击 API 测试控制台的 Send。显示识别结果。下面的结果我就不一一说明了，主要的显示结果有：1. faces，识别出图像中的人脸坐标，性别和年龄2. adultScore，识别出检测图像是否是色情的，分数越高，则图像色情的可能性越大Pragma: no-cache apim-request-id: 8a9e6b8c-3a20-42a0-91e0-52d6fbdc5f9e Strict-Transport-Security: max-age=31536000; includeSubDomains; preload x-content-type-options: nosniff Cache-Control: no-cache Date: Thu, 15 Jun 2017 09:06:16 GMT X-AspNet-Version: 4.0.30319 X-Powered-By: ASP.NET Content-Length: 1595 Content-Type: application/json; charset=utf-8 Expires: -1 { \"categories\": [ { \"name\": \"people_group\", \"score\": 0.9765625 } ], \"adult\": { \"isAdultContent\": false, \"isRacyContent\": false, \"adultScore\": 0.01091344840824604, \"racyScore\": 0.055492393672466278 }, \"tags\": [ { \"name\": \"outdoor\", \"confidence\": 0.99716836214065552 }, { \"name\": \"person\", \"confidence\": 0.99493598937988281 }, { \"name\": \"posing\", \"confidence\": 0.95204299688339233 }, { \"name\": \"group\", \"confidence\": 0.82954329252243042 }, { \"name\": \"people\", \"confidence\": 0.583439290523529 }, { \"name\": \"crowd\", \"confidence\": 0.019400959834456444 } ], \"description\": { \"tags\": [ \"outdoor\", \"person\", \"posing\", \"photo\", \"grass\", \"group\", \"standing\", \"people\", \"man\", \"woman\", \"young\", \"holding\", \"dress\", \"white\", \"court\" ], \"captions\": [ { \"text\": \"a group of people posing for a picture\", \"confidence\": 0.94583615520612 } ] }, \"requestId\": \"8a9e6b8c-3a20-42a0-91e0-52d6fbdc5f9e\", \"metadata\": { \"width\": 600, \"height\": 463, \"format\": \"Jpeg\" }, \"faces\": [ { \"age\": 42, \"gender\": \"Male\", \"faceRectangle\": { \"left\": 117, \"top\": 159, \"width\": 95, \"height\": 95 } }, { \"age\": 54, \"gender\": \"Male\", \"faceRectangle\": { \"left\": 490, \"top\": 111, \"width\": 90, \"height\": 90 } }, { \"age\": 61, \"gender\": \"Female\", \"faceRectangle\": { \"left\": 17, \"top\": 153, \"width\": 85, \"height\": 85 } }, { \"age\": 33, \"gender\": \"Female\", \"faceRectangle\": { \"left\": 386, \"top\": 166, \"width\": 81, \"height\": 81 } }, { \"age\": 15, \"gender\": \"Female\", \"faceRectangle\": { \"left\": 235, \"top\": 159, \"width\": 77, \"height\": 77 } }, { \"age\": 6, \"gender\": \"Female\", \"faceRectangle\": { \"left\": 323, \"top\": 163, \"width\": 67, \"height\": 67 } } ], \"color\": { \"dominantColorForeground\": \"White\", \"dominantColorBackground\": \"White\", \"dominantColors\": [ \"White\", \"Brown\" ], \"accentColor\": \"4E5D1A\", \"isBWImg\": false }, \"imageType\": { \"clipArtType\": 0, \"lineDrawingType\": 0 } }更多精彩干货 请点击查看欢迎有兴趣的朋友多多交流A究院研究生 Azurecommunity@qq.com"}
{"content2":"转自：http://www.cnblogs.com/scnucs/archive/2012/04/18/2455447.html计算机视觉库 OpenCVOpenCV是Intel®开源计算机视觉库。它由一系列 C 函数和少量 C++ 类构成，实现了图像处理和计算机视觉方面的很多通用算法。 OpenCV 拥有包括 300 多个C函数的跨平台的中、高层 API。它不依赖于其它的外部库——尽管也可以使用某些外部库。 OpenCV 对非商业...人脸识别 faceservice.cgifaceservice.cgi 是一个用来进行人脸识别的 CGI 程序， 你可以通过上传图像，然后该程序即告诉你人脸的大概坐标位置。faceservice是采用 OpenCV 库进行开发的。OpenCV的.NET版 OpenCVDotNetOpenCVDotNet 是一个 .NET 对 OpenCV 包的封装。人脸检测算法 jViolajonesjViolajones是人脸检测算法Viola-Jones的一个Java实现，并能够加载OpenCV XML文件。 示例代码：http://www.oschina.net/code/snippet_12_2033Java视觉处理库 JavaCVJavaCV 提供了在计算机视觉领域的封装库，包括：OpenCV、ARToolKitPlus、libdc1394 2.x 、PGR FlyCapture和FFmpeg。此外，该工具可以很容易地使用Java平台的功能。 JavaCV还带有硬件加速的全屏幕图像显示（CanvasFrame），易于在多个内核中执行并行代码（并...运动检测程序 QMotionQMotion 是一个采用 OpenCV 开发的运动检测程序，基于 QT。视频监控系统 OpenVSSOpenVSS - 开放平台的视频监控系统 - 是一个系统级别的视频监控软件视频分析框架（VAF）的视频分析与检索和播放服务，记录和索引技术。它被设计成插件式的支持多摄像头平台，多分析仪模块（OpenCV的集成），以及多核心架构。手势识别 hand-gesture-detection手势识别，用OpenCV实现人脸检测识别 mcvai-tracking提供人脸检测、识别与检测特定人脸的功能，示例代码 cvReleaseImage( &gray ); cvReleaseMemStorage(&storage); cvReleaseHaarClassifierCascade(&cascade);...人脸检测与跟踪库 asmlibraryActive Shape Model Library (ASMLibrary©) SDK, 用OpenCV开发，用于人脸检测与跟踪。Lua视觉开发库 libecvECV 是 lua 的计算机视觉开发库(目前只提供linux支持)OpenCV的.Net封装 OpenCVSharpOpenCVSharp 是一个OpenCV的.Net wrapper，应用最新的OpenCV库开发，使用习惯比EmguCV更接近原始的OpenCV，有详细的使用样例供参考。3D视觉库 fvision2010基于OpenCV构建的图像处理和3D视觉库。 示例代码： ImageSequenceReaderFactory factory; ImageSequenceReader* reader = factory.pathRegex(\"c:/a/im_%03d.jpg\", 0, 20); //ImageSequenceReader* reader = factory.avi(\"a.avi\"); if (reader == NULL) { ...基于QT的计算机视觉库 QVision基于 QT 的面向对象的多平台计算机视觉库。可以方便的创建图形化应用程序，算法库主要从 OpenCV，GSL，CGAL，IPP，Octave 等高性能库借鉴而来。图像特征提取 cvBlobcvBlob 是计算机视觉应用中在二值图像里寻找连通域的库.能够执行连通域分析与特征提取.实时图像/视频处理滤波开发包 GShowGShow is a real-time image/video processing filter development kit. It successfully integrates DirectX11 with DirectShow framework. So it has the following features: GShow 是实时 图像/视频 处理滤波开发包，集成DiretX11。...视频捕获 API VideoManVideoMan 提供一组视频捕获 API 。支持多种视频流同时输入（视频传输线、USB摄像头和视频文件等）。能利用 OpenGL 对输入进行处理，方便的与 OpenCV，CUDA 等集成开发计算机视觉系统。开放模式识别项目 OpenPRPattern Recognition project（开放模式识别项目），致力于开发出一套包含图像处理、计算机视觉、自然语言处理、模式识别、机器学习和相关领域算法的函数库。OpenCV的Python封装 pyopencvOpenCV的Python封装，主要特性包括： 提供与OpenCV 2.x中最新的C++接口极为相似的Python接口，并且包括C++中不包括的C接口 提供对OpenCV 2.x中所有主要部件的绑定：CxCORE (almost complete), CxFLANN (complete), Cv (complete), CvAux (C++ part almost...视觉快速开发平台 qcv计算机视觉快速开发平台，提供测试框架，使开发者可以专注于算法研究。图像捕获 libv4l2cam对函数库v412的封装，从网络摄像头等硬件获得图像数据，支持YUYV裸数据输出和BGR24的OpenCV  IplImage输出计算机视觉算法 OpenVIDIAOpenVIDIA projects implement computer vision algorithms running on on graphics hardware such as single or multiple graphics processing units(GPUs) using OpenGL, Cg and CUDA-C. Some samples will soon support OpenCL and Direct Compute API&apos;...高斯模型点集配准算法 gmmreg实现了基于混合高斯模型的点集配准算法，该算法描述在论文： A Robust Algorithm for Point Set Registration Using Mixture of Gaussians, Bing Jian and Baba C. Vemuri. ，实现了C++/Matlab/Python接口...模式识别和视觉库 RAVLRecognition And Vision Library (RAVL) 是一个通用 C++ 库，包含计算机视觉、模式识别等模块。图像处理和计算机视觉常用算法库 LTI-LibLTI-Lib 是一个包含图像处理和计算机视觉常用算法和数据结构的面向对象库，提供 Windows 下的 VC 版本和 Linux 下的 gcc 版本，主要包含以下几方面内容： 1、线性代数 2、聚类分析 3、图像处理 4、可视化和绘图工具OpenCV优化 opencv-dsp-acceleration优化了OpenCV库在DSP上的速度。C++计算机视觉库 Integrating Vision ToolkitIntegrating Vision Toolkit (IVT) 是一个强大而迅速的C++计算机视觉库，拥有易用的接口和面向对象的架构，并且含有自己的一套跨平台GUI组件，另外可以选择集成OpenCV计算机视觉和机器人技术的工具包 EGTThe Epipolar Geometry Toolbox (EGT) is a toolbox designed for Matlab (by Mathworks Inc.). EGT provides a wide set of functions to approach computer vision and robotics problems with single and multiple views, and with different vision se...OpenCV的扩展库 ImageNetsImageNets 是对OpenCV 的扩展，提供对机器人视觉算法方面友好的支持，使用Nokia的QT编写界面。libvideogfx视频处理、计算机视觉和计算机图形学的快速开发库。Matlab计算机视觉包 mVisionMatlab 的计算机视觉包，包含用于观察结果的 GUI 组件，貌似也停止开发了，拿来做学习用挺不错的。Scilab的计算机视觉库 SIPSIP 是 Scilab（一种免费的类Matlab编程环境）的图像处理和计算机视觉库。SIP 可以读写 JPEG/PNG/BMP 格式的图片。具备图像滤波、分割、边缘检测、形态学处理和形状分析等功能。STAIR Vision LibrarySTAIR Vision Library (SVL) 最初是为支持斯坦福智能机器人设计的，提供对计算机视觉、机器学习和概率统计模型的支持。SLAM,仿人机器人视觉导航图像处理与计算机视觉开源软件库1.OpenCv(计算机视觉必学的库，个人认为其作用相当强悍）http://opencv.willowgarage.com/wiki/2.CVpaper 主页上推荐的开源视觉算法库，最全的了，也非常新,强烈推荐大家去看看http://www.cvpapers.com/rr.html3.cmu的图像处理和计算机视觉软件库，非常全，但有点老了，但都很经典，资源非常丰富http://www.cs.cmu.edu/~cil/v-source.html4.在网上还找到很多图像处理和视觉的开源软件，不过没用过，用过后再陆续更新了，敬请期待！ 计算机视觉库 OpenCVOpenCV是Intel®开源计算机视觉库。它由一系列 C 函数和少量 C++ 类构成，实现了图像处理和计算机视觉方面的很多通用算法。 OpenCV 拥有包括 300 多个C函数的跨平台的中、高层 API。它不依赖于其它的外部库——尽管也可以使用某些外部库。 OpenCV 对非商业...人脸识别 faceservice.cgifaceservice.cgi 是一个用来进行人脸识别的 CGI 程序， 你可以通过上传图像，然后该程序即告诉你人脸的大概坐标位置。faceservice是采用 OpenCV 库进行开发的。OpenCV的.NET版 OpenCVDotNetOpenCVDotNet 是一个 .NET 对 OpenCV 包的封装。人脸检测算法 jViolajonesjViolajones是人脸检测算法Viola-Jones的一个Java实现，并能够加载OpenCV XML文件。 示例代码：http://www.oschina.net/code/snippet_12_2033Java视觉处理库 JavaCVJavaCV 提供了在计算机视觉领域的封装库，包括：OpenCV、ARToolKitPlus、libdc1394 2.x 、PGR FlyCapture和FFmpeg。此外，该工具可以很容易地使用Java平台的功能。 JavaCV还带有硬件加速的全屏幕图像显示（CanvasFrame），易于在多个内核中执行并行代码（并...运动检测程序 QMotionQMotion 是一个采用 OpenCV 开发的运动检测程序，基于 QT。视频监控系统 OpenVSSOpenVSS - 开放平台的视频监控系统 - 是一个系统级别的视频监控软件视频分析框架（VAF）的视频分析与检索和播放服务，记录和索引技术。它被设计成插件式的支持多摄像头平台，多分析仪模块（OpenCV的集成），以及多核心架构。手势识别 hand-gesture-detection手势识别，用OpenCV实现人脸检测识别 mcvai-tracking提供人脸检测、识别与检测特定人脸的功能，示例代码 cvReleaseImage( &gray ); cvReleaseMemStorage(&storage); cvReleaseHaarClassifierCascade(&cascade);...人脸检测与跟踪库 asmlibraryActive Shape Model Library (ASMLibrary©) SDK, 用OpenCV开发，用于人脸检测与跟踪。Lua视觉开发库 libecvECV 是 lua 的计算机视觉开发库(目前只提供linux支持)OpenCV的.Net封装 OpenCVSharpOpenCVSharp 是一个OpenCV的.Net wrapper，应用最新的OpenCV库开发，使用习惯比EmguCV更接近原始的OpenCV，有详细的使用样例供参考。3D视觉库 fvision2010基于OpenCV构建的图像处理和3D视觉库。 示例代码： ImageSequenceReaderFactory factory; ImageSequenceReader* reader = factory.pathRegex(\"c:/a/im_%03d.jpg\", 0, 20); //ImageSequenceReader* reader = factory.avi(\"a.avi\"); if (reader == NULL) { ...基于QT的计算机视觉库 QVision基于 QT 的面向对象的多平台计算机视觉库。可以方便的创建图形化应用程序，算法库主要从 OpenCV，GSL，CGAL，IPP，Octave 等高性能库借鉴而来。图像特征提取 cvBlobcvBlob 是计算机视觉应用中在二值图像里寻找连通域的库.能够执行连通域分析与特征提取.实时图像/视频处理滤波开发包 GShowGShow is a real-time image/video processing filter development kit. It successfully integrates DirectX11 with DirectShow framework. So it has the following features: GShow 是实时 图像/视频 处理滤波开发包，集成DiretX11。...视频捕获 API VideoManVideoMan 提供一组视频捕获 API 。支持多种视频流同时输入（视频传输线、USB摄像头和视频文件等）。能利用 OpenGL 对输入进行处理，方便的与 OpenCV，CUDA 等集成开发计算机视觉系统。开放模式识别项目 OpenPRPattern Recognition project（开放模式识别项目），致力于开发出一套包含图像处理、计算机视觉、自然语言处理、模式识别、机器学习和相关领域算法的函数库。OpenCV的Python封装 pyopencvOpenCV的Python封装，主要特性包括： 提供与OpenCV 2.x中最新的C++接口极为相似的Python接口，并且包括C++中不包括的C接口 提供对OpenCV 2.x中所有主要部件的绑定：CxCORE (almost complete), CxFLANN (complete), Cv (complete), CvAux (C++ part almost...视觉快速开发平台 qcv计算机视觉快速开发平台，提供测试框架，使开发者可以专注于算法研究。图像捕获 libv4l2cam对函数库v412的封装，从网络摄像头等硬件获得图像数据，支持YUYV裸数据输出和BGR24的OpenCV  IplImage输出计算机视觉算法 OpenVIDIAOpenVIDIA projects implement computer vision algorithms running on on graphics hardware such as single or multiple graphics processing units(GPUs) using OpenGL, Cg and CUDA-C. Some samples will soon support OpenCL and Direct Compute API&apos;...高斯模型点集配准算法 gmmreg实现了基于混合高斯模型的点集配准算法，该算法描述在论文： A Robust Algorithm for Point Set Registration Using Mixture of Gaussians, Bing Jian and Baba C. Vemuri. ，实现了C++/Matlab/Python接口...模式识别和视觉库 RAVLRecognition And Vision Library (RAVL) 是一个通用 C++ 库，包含计算机视觉、模式识别等模块。图像处理和计算机视觉常用算法库 LTI-LibLTI-Lib 是一个包含图像处理和计算机视觉常用算法和数据结构的面向对象库，提供 Windows 下的 VC 版本和 Linux 下的 gcc 版本，主要包含以下几方面内容： 1、线性代数 2、聚类分析 3、图像处理 4、可视化和绘图工具OpenCV优化 opencv-dsp-acceleration优化了OpenCV库在DSP上的速度。C++计算机视觉库 Integrating Vision ToolkitIntegrating Vision Toolkit (IVT) 是一个强大而迅速的C++计算机视觉库，拥有易用的接口和面向对象的架构，并且含有自己的一套跨平台GUI组件，另外可以选择集成OpenCV计算机视觉和机器人技术的工具包 EGTThe Epipolar Geometry Toolbox (EGT) is a toolbox designed for Matlab (by Mathworks Inc.). EGT provides a wide set of functions to approach computer vision and robotics problems with single and multiple views, and with different vision se...OpenCV的扩展库 ImageNetsImageNets 是对OpenCV 的扩展，提供对机器人视觉算法方面友好的支持，使用Nokia的QT编写界面。libvideogfx视频处理、计算机视觉和计算机图形学的快速开发库。Matlab计算机视觉包 mVisionMatlab 的计算机视觉包，包含用于观察结果的 GUI 组件，貌似也停止开发了，拿来做学习用挺不错的。Scilab的计算机视觉库 SIPSIP 是 Scilab（一种免费的类Matlab编程环境）的图像处理和计算机视觉库。SIP 可以读写 JPEG/PNG/BMP 格式的图片。具备图像滤波、分割、边缘检测、形态学处理和形状分析等功能。STAIR Vision LibrarySTAIR Vision Library (SVL) 最初是为支持斯坦福智能机器人设计的，提供对计算机视觉、机器学习和概率统计模型的支持。"}
{"content2":"上礼拜从9月17号到21号和另外一位同事参加了在上海举办的“世界人工智能大会”和杭州的“云栖大会”。云栖大会就不用多介绍了，已经在杭州云栖小镇举办了好几届，主要涉及到人工智能、云计算、大数据、互联网+等领域。而这个世界人工智能大会好像才是第一届（当然不知道还会不会有第二届了），由上海市政府主要承办，邀请了国内外知名IT公司和技术达人参与，听说习大大还专门致信表示祝贺了，看来这个大会来头不小，当然事实也证明，确实比较隆重。两个大会都是以“主论坛”、“分论坛”以及“展览区”的形式存在的，主论坛我没去过，好像都是一帮IT大佬、市政领导发表演讲，大多都是精神上的解读，分论坛大多以技术为主，由各个公司承办，主要介绍他们比较有影响的技术和解决方案，作为技术人员，我当然选择晚去一天参加一些分论坛了。展览区都一样，每个公司（产品）占一块地方展示自己的东西。由于公司业务关系，我在上海主要参加了商汤科技和阿里巴巴公司关于城市大脑的分论坛，然后参观了展览区，看到了谷歌、亚马逊、腾讯、阿里巴巴、商汤科技、pony.ai无人车、旷世face++、地平线、寒武纪等一些公司的产品（其它的记不住名字）。在杭州云栖大会上主要参加了Nvidia的一个分论坛，主要介绍了他们公司牛逼的显卡、以及专门用于AI视频分析的一个框架DeepStream 2.0。下面以流水账的形式简单记录一下上海三天的参会记录。（感觉云栖大会没什么可说的，我们只是花了一上午时间听了英伟达的一个分论坛，然后去展览区逛了逛）。9.18号上午18号上午参加的是商汤科技的分论坛，商汤在深度学习计算机视觉领域小有名气，我们是抱着很大的希望去看的，最后发现他们的论坛侧重点是“人脸识别”以及“人脸美妆美容”等方面，展示了高超的“美容瘦身”技术，当然也提到了一点person-ReId技术的应用以及机器自主学习的东西（无需人工标注素材，通过机器对照视频字母来进行学习，我各人感觉这个标注技术目前不太靠谱），但是内容也相当少，都是演示提前做好的视频demo。<从纪录片中学习，无需人工标注>全程大部分都是他们的二当家徐立演讲的，当然也请到了两个重量级的技术达人，一个是MIT名誉校长Eric Grimson，机器学习专家，不过他全程都在说些跟MIT教学相关的东西。另一个是机器学习开山鼻祖、人工智能领域顶级专家（我也是从网上看到这种描述哈哈）与飞人乔丹同名的也是吴恩达的恩师Michael I. Jordan，这个大神一上来就给大家PO代码，而且还是Python代码，推广一个叫Ray的机器学习框架，这框架我之前都没听说过，大神就是大神，前面演讲的人都是放效果视频、演示PPT，在这种技术、产品、销售、总裁助理、甚至HR也有的大会上，他直接给大家看代码，着实让我佩服。<大神上来po代码>最后商汤邀请了各界几位领军级的人物进行一个座谈，大概内容是学术界和工业界关于人工智能合作的内容。包括商汤老大汤晓鸥、前面提到的两个大神、还有浙大校长、香港中文大学校长、英伟达的一位什么总裁、MSRA的一个副院长（？）以及小米的洪峰。最后部分没太听，由于已经过中午12点了提前撤了吃饭。9月18号下午下午参加阿里巴巴的城市大脑主题论坛，干货比较多，先是上海市政府与阿里巴巴合作的一个教授级别的人物（？）演示了“城市大脑”概念的形成历史，之后一个阿里技术级别比较高的负责人主要介绍了与深度学习神经网络有关的内容，附带演示了一个“交通事件检测平台”的demo（需要说的是，该demo与我司做的产品非常类似），通过目标检测识别出高速公路上行驶车辆的行为，对于一些异常行为进行报警，接着高德CEO介绍了城市大脑与高德地图的结合，最后也是一个座谈会，只是由于时间关系，我们提前撤了并没有太多关注。<城市大脑中全时全域自动巡逻报警><城市大脑最新AI产品><高速事件检测平台demo>后面分析了一堆深度学习相关的东西，包括神经网络优化、存在的一些问题等等。9.19号19号仍然在上海，主要参观了一下展览区（大概开放7天），这天收获颇多，看到了很多跟我司业务比较相近的产品。首先是地平线的人脸识别，看了一下识别效果非常好，但是跟踪效果一般，对于遮挡之后再出现的目标跟踪不到。接着是交通工具检测，这个效果非常好（如果后期没有处理的话）后面就是第一视角的道路检测，用于自动驾驶另外一个展台看到一个person-ReId的应用，根据摄像机定位人物轨迹旁边展台就是旷世Face++的产品，人脸识别和视频结构化分析能够分析出拍摄视频中的人物特征，人脸抓拍。商汤展台没有看到比较吸引人的东西，他们也展示了一个基于person-ReId的人物轨迹定位的系统，不过他们好像是基于人脸去做的，这个严格意义上不算person-ReId（person-ReId应该使用人物全身轮廓去匹配）后面我们体验了一下Pony.ai 公司的无人车，车辆行驶环境非常好，都是隔离区域。全程有司机在监控，虽然自动驾驶，但是司机双手一直处于方向盘周围（未接触方向盘）。乘坐体验还行，无论是直线行驶还是直角转弯或者路口等红灯，几乎和人工驾驶一样，丝毫感觉不到区别。Pony.ai 是从百度自动驾驶部门跳槽出来的人做的，熟悉的人可能知道楼天城。最后参观了亚马逊、腾讯以及谷歌独立的展览区，每家一个独立的房子，但是展区的内容跟我司业务关联不大，我们随便逛逛并没有拍照记录。我接触AI深度学习开发大概3个月，公司之前也没人做过这方面的东西，目前正在尝试性搞一些东西出来，而这次上海之行收获很多，了解了一下其他公司的产品形式和水准。我们在展区也看到过其他一些公司的展示demo，做得水平非常一般，比我们自己做的差，让我们信心倍增哈哈。"}
{"content2":"人工智能浪潮下的思考摘要这次的选题是以最近比较火的人工智能作为选题，人工智能大概是在2016年开始迎来了它的第一股热浪。人工智能是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。人工智能从诞生以来，理论和技术日益成熟，应用领域也不断扩大，可以设想，未来人工智能带来的科技产品，将会是人类智慧的“容器”。人工智能可以对人的意识、思维的信息过程的模拟我们耳熟能详的暗知识就是机器通过大量的训练尝试所获得的知识，那些知识是人类目前都没有完全掌握的。这就是深度学习，机器通过试错所学习的知识已经不仅仅是我们人类目前所掌握的知识了，这个时候的智能已经不是人的智能了。再说一下人工智能应用的领域：图像处理、语音的识别、无人驾驶、医学(机器看病)、智能家居(像国内的小爱)、无人零售。人工智能相信带给我们的惊喜远远不仅如此，之后会随着越来越多的人加入到人工智能的研究中人工智能会再一次打破我们传统的生活方式给我们带来更多的便捷。关键字：人工智能; 智能家居; 智能控制; 智能医疗;Thinking under the tide of artificial intelligenceAbstractThis selection of topic is based on the recently popular artificial can only be selected as the topic, artificial intelligence probably began to usher in its first heat wave in 2016. Artificial intelligence (ai) is a new technical science that researches and develops theories, methods, technologies and application systems for simulating, extending and extending human intelligence.Since the birth of artificial intelligence, theories and technologies have become increasingly mature and the application field has been expanding. It can be assumed that the scientific and technological products brought by artificial intelligence in the future will be the \"container\" of human intelligence. Artificial intelligence can simulate the information process of human consciousness and thinkingThe dark knowledge that we are all familiar with is the knowledge that machines have acquired through numerous training attempts, which humans have not yet fully mastered. This is deep learning. The knowledge learned by machines through trial and error is not only the knowledge we humans have at present. At this time, intelligence is no longer human intelligence.Let's talk about the application areas of artificial intelligence: image processing, speech recognition, unmanned driving, medicine (medical treatment by machine), intelligent home (like domestic little love), unmanned retail. Artificial intelligence is believed to bring us more surprises than that. Later, as more and more people join in the research of artificial intelligence, artificial intelligence will once again break our traditional way of life and bring us more convenience.目录人工智能浪潮下的思考 1摘要 1第1章 人工智能的发展历史 31.1人工智能的起源 31.2人工智能的第一次高峰 41.3人工智能第一次低谷 41.4人工智能的崛起 41.5人工智能的今天 5第2章 人工智能的定义 52.1我眼中的人工智能 52.2人工智能(60年代) 62.3人工智能(80年代) 62.4人工智能的分类 62.4.1强人工智能 62.4.2弱人工智能 6第3章 人工智能的应用领域 63.1深度学习 61. 构建一个网络并且随机初始化所有连接的权重； 62. 将大量的数据情况输出到这个网络中； 73.网络处理这些动作并且进行学习； 74.如果这个动作符合指定的动作，将会增强权重，如果不符合，将会降低权重； 75.系统通过如上过程调整权重； 76.在成千上万次的学习之后，超过人类的表现； 73.2语音识别 73.3计算机视觉 73.4智能机器人 73.5医疗方面 7AI＋医疗影像、机器人／机械臂辅助手术、自动化的工作流程助理 7第4章 我对于人工智能的思考 74.1人工智能的发展浅谈 74.2人工智能的前景 84.3人工智能是把双刃剑 8参考文献.................................................................10第1章 人工智能的发展历史1.1人工智能的起源其实，人工智能早在上世纪中叶就已经诞生。1950年，一位名叫马文·明斯基(后被人称为“人工智能之父”)的大四学生与他的同学邓恩·埃德蒙一起，建造了世界上第一台神经网络计算机。这也被看做是人工智能的一个起点。巧合的是，同样是在1950年，被称为“计算机之父”的阿兰·图灵提出了一个举世瞩目的想法——图灵测试。按照图灵的设想：如果一台机器能够与人类开展对话而不能被辨别出机器身份，那么这台机器就具有智能。而就在这一年，图灵还大胆预言了真正具备智能机器的可行性。1956年，在由达特茅斯学院举办的一次会议上，计算机专家约翰·麦卡锡提出了“人工智能”一词。后来，这被人们看做是人工智能正式诞生的标志。就在这次会议后不久，麦卡锡从达特茅斯搬到了MIT。同年，明斯基也搬到了这里，之后两人共同创建了世界上第一座人工智能实验室——MIT AI LAB实验室。1.2人工智能的第一次高峰在1956年的这次会议之后，人工智能迎来了属于它的第一段Happy Time。在这段长达十余年的时间里，计算机被广泛应用于数学和自然语言领域，用来解决代数、几何和英语问题。这让很多研究学者看到了机器向人工智能发展的信心。甚至在当时，有很多学者认为：“二十年内，机器将能完成人能做到的一切。”1.3人工智能第一次低谷时间来到了70年代，人工智能进入了一段痛苦而艰难岁月。由于科研人员在人工智能的研究中对项目难度预估不足，不仅导致与美国国防高级研究计划署的合作计划失败，还让大家对人工智能的前景蒙上了一层阴影。与此同时，社会舆论的压力也开始慢慢压向人工智能这边,导致很多研究经费被转移到了其他项目上。在当时，人工智能面临的技术瓶颈主要是三个方面，第一计算机性能不足，导致早期很多程序无法在人工智能领域得到应用；第二，问题的复杂性，早期人工智能程序主要是解决特定的问题，因为特定的问题对象少，复杂性低，可一旦问题上升维度，程序立马就不堪重负了；第三，数据量严重缺失，在当时不可能找到足够大的数据库来支撑程序进行深度学习，这很容易导致机器无法读取足够量的数据进行智能化。因此，人工智能项目停滞不前，但却让一些人有机可乘,1973年Lighthill针对英国AI研究状况的报告。批评了AI在实现“宏伟目标”上的失败。由此，人工智能遭遇了长达6年的科研深渊。1.4人工智能的崛起1980年，卡内基梅隆大学为数字设备公司设计了一套名为XCON的“专家系统”。这是一种，采用人工智能程序的系统，可以简单的理解为“知识库+推理机”的组合，XCON是一套具有完整专业知识和经验的计算机智能系统。这套系统在1986年之前能为公司每年节省下来超过四千美元经费。有了这种商业模式后，衍生出了像Symbolics、Lisp Machines等和IntelliCorp、Aion等这样的硬件，软件公司。在这个时期，仅专家系统产业的价值就高达5亿美元。1.5人工智能的今天把时间拨回到现在，2019年。我们回顾了人工智能接近70多年的发展历程。在这段漫长是时间里，科研技术人员不断突破阻碍，让我们可以看到今天人工智能所取得的辉煌成果，比如在1997年，IBM的深蓝战胜国际象棋世界冠军卡斯帕罗夫；2009年，螺丝联邦理工学院发起的蓝脑计划，生成已经成功模拟了部分鼠脑；2014年5月28日谷歌推出新产品——无人驾驶汽车；2016年，围棋人工智能程序AlphaGo以4:1的成绩战胜围棋世界冠军李世石；2017年，AlphaGo化身Master，再次出战横扫棋坛，让人类见识到了人工智能的强大。第2章 人工智能的定义对于人工智能的定义有许多，不同的人给出的也都不尽相同，这这里我列举一些著名人士给出的定义以及我的简单看法。2.1我眼中的人工智能我认为人工智能就是人类智慧的延申和另一种表现形式。人工智能是服务于人类而存在的，仅仅的只能是没有任何意义的，只有当它的价值体现和应用在人类身上时人工智能才有存在的必要。人工智能不能替代人类只是一种工具，所谓是工具就会存在利弊，不能因为某种不安全或者不确定的因素去随便否定人工智能，不能够以偏概全。2.2人工智能(60年代)在60年代，AI研究人员认为人工智能是一台通用机器人，它拥有模仿智能的特征，懂得使用语言，懂得形成抽象概念，能够对自己的行为进行推理，它可以解决人类现存问题。由于理念、技术和数据的限制，人工智能在模式识别、信息表示、问题解决和自然语言处理等不同领域发展缓慢。2.3人工智能(80年代)80年代，AI研究人员转移方向，认为人工智能对事物的推理能力比抽象能力更重要，机器为了获得真正的智能，机器必须具有躯体，它需要感知、移动、生存，与这个世界交互。为了积累更多推理能力，AI研究人员开发出专家系统，它能够依据一组从专门知识中推演出的逻辑规则在某一特定领域回答或解决问题。2.4人工智能的分类2.4.1强人工智能普通群众所遐想的人工智能属于强人工智能，它属于通用型机器人，也就是60年代AI研究人员提出的理念。它能够和人类一样对世界进行感知和交互，通过自我学习的方式对所有领域进行记忆、推理和解决问题。2.4.2弱人工智能不能制造出真正地推理（Reasoning）和解决问题（Problem_solving）的智能机器，这些机器只不过看起来像是智能的，但是并不真正拥有智能，也不会有自主意识。第3章 人工智能的应用领域3.1深度学习深度学习的技术原理：构建一个网络并且随机初始化所有连接的权重；将大量的数据情况输出到这个网络中；3.网络处理这些动作并且进行学习；4.如果这个动作符合指定的动作，将会增强权重，如果不符合，将会降低权重；5.系统通过如上过程调整权重；6.在成千上万次的学习之后，超过人类的表现；3.2语音识别语音识别，是把语音转化为文字，并对其进行识别、认知和处理。语音识别的主要应用包括电话外呼、医疗领域听写、语音书写、电脑系统声控、电话客服等。3.3计算机视觉计算机视觉是指计算机从图像中识别出物体、场景和活动的能力。计算机视觉有着广泛的细分应用，其中包括，医疗领域成像分析、人脸识别、公关安全、安防监控等等。3.4智能机器人智能机器人在生活中随处可见，扫地机器人、陪伴机器人……这些机器人不管是跟人语音聊天，还是自主定位导航行走、安防监控等，都离不开人工智能技术的支持。3.5医疗方面AI＋医疗影像、机器人／机械臂辅助手术、自动化的工作流程助理第4章 我对于人工智能的思考4.1人工智能的发展浅谈人工智能这个词在很大概是在60年代就被提出来，当时的人们也是有着很高的热情去投身于人工智能的算法设计和研究当中，人们当时认为20年及其就可以完成人类所完成的一切工作。在现在看来是很可笑的一件事，事实也确实如此，由于当时数据量的不足，当时的人工智能根本就无法进行深度的学习，说的前浅显一点就是机器就无法完成自我的对弈，这就导致人工智能的提升十分的有限。当时人们去给人工智能的设计的思考模式是按照人类的神经进行模拟的，但是人的神经分了成千上万的的级数，机器一旦应用这种模型就会存在级数一多没有相对应的管理算法，这也是当时人工智能第一次低谷的原因。但是并不是所有人都放弃了对于人工只能的研究，Geoffrey Hinton当时在研究神经网络的算法，他坚信人工智能一定会再一次出现在公众的视野当中。终于在80年代人工智能又再一次亮相，因为数学的发展、生物学的发展还有那些一直默默奉献的人们为人工智能的再次出现打下了坚实的基础。随后就是我们熟知的人工智能的事件：2016年，围棋人工智能程序AlphaGo以4:1的成绩战胜围棋世界冠军李世石；2017年，AlphaGo化身Master，再次出战横扫棋坛，让人类见识到了人工智能的强大。4.2人工智能的前景人工智能总是能够带给我们想象不到的惊喜，目前炒的很热的就是华为鸿蒙系统。据爆料华为OS将打通手机、电脑、平板、电视、汽车、智能穿戴，(将这些设备)统一成一个操作系统。人工智能在医疗方面也是有了雏形，现在越来越多的人们都喜欢带智能手环，从中你不仅可以看到自己的睡眠情况还可以看到自己的心率，我相信未来的手环很有可能像身份证一样人手一个，通过智能手环就可以诊断一些常见的病，还有就是现在越来越多的医院的设施都和电脑有了联系，相信之后的联席会越来越紧密地，像一些远程医疗、人工智能辅助设施都会成为发展的一个方向。4.3人工智能是把双刃剑我们都知道人工智能可以方便我们的生活，让我们在方方面面都享受到人工智能所带给我们的便捷。但是是工具都会有利和弊两个方面存在，我们都大概知道强人工智能会在深度学习之后会存在一些自我的意识，而我们人与机器最大的区别就是我们与自我意识而机器没有。举一个简单的例子：一个小孩是没有自我意识的因为它完全不知道什么是对什么是错，他并不会因为做了一件我们认为错的事情而有任何情绪上的变化，而大人就不同了，因为在长大的过程当中他会慢慢了解到对与错的概念，会开始自己的想法和思维。想一想你如果正在享受按摩机器人的服务而下一秒它就会威胁到你的生命你作何感想，机器的主观意识一直是一个无法回避的一个问题。但是我们想想喝口水都有可能被呛着的风险，难道我们就不喝水了吗？人工智能存在的一些问题我们无法回避，但是我们可以通过一些途径来解决而不是一味的停滞不前。参考文献www.baidu.com李开复所著的《人工智能》"}
{"content2":"欢迎访问网易云社区，了解更多网易技术产品运营经验。“知物由学”是网易云易盾打造的一个品牌栏目，词语出自汉·王充《论衡·实知》。人，能力有高下之分，学习才知道事物的道理，而后才有智慧，不去求问就不会知道。“知物由学”希望通过一篇篇技术干货、趋势解读、人物思考和沉淀给你带来收获的同时，也希望打开你的眼界，成就不一样的你。当然，如果你有不错的认知或分享，也欢迎通过邮件（zhangyong02@corp.netease.com）投稿。以下是正文：本文作者：Guarav Banga最近，我参加了由十几位CISO（首席信息安全官）组成的思想领导力讨论会，我们就一系列网络安全问题进行了杰斐逊式的讨论。讨论会上提出的第一个问题就是大家如何看待人工智能，以及是否在使用人工智能。许多的与会者表示，他们的机器学习项目目前正在进行之中，但同时也强调，在网络安全领域并没有使用到人工智能技术。人工智能这个词确实值得我们幻想，而且也符合我们对人类智能、图灵测试以及科幻电影的认知。可惜的是，正如我在CISO（首席信息安全官）晚宴上所阐述的那样，人们对“人工智能是什么”这个问题还存在着一些困惑，虽然近年来很多人都在提人工智能，但人们的困惑并没有得到解除。本文由三部分组成，首先我们探讨一下人类智能与人工智能的一些基本概念，并解释当今比较流行的一些词汇，包括人工智能、机器学习、专家系统和深度学习之间的差异。最后，我们将讨论人工智能在网络安全应用中的真实情况，以及为什么我们需要把它作为一种战略工具。什么是智能？在进一步讨论人工智能这个话题之前，让我们首先来定义什么是智能（译者注：在英文中，智能即intelligence）。智能在广义上是相当复杂的，在科学和哲学的许多方面都存在着激烈的争论。但在本文中，我提供了下面这个定义。对于智能，我有两个很重要的观点。首先，许多科学家认为，人类的智能根源于大脑如何在多种不同类型的感官数据中发现并存储具有相关性的分层模式。例如，当你在捕获的数据包或日志文件中看到某个网络名称中存在“Gaurav-iPhone”的时候，你会很自然地想到这很有可能是你的朋友Gaurav的iPhone。你会无意识地将有关同事姓名的知识与有关常用设备类型的知识联系起来。在生活中，你会不断地无意识地去更新这两个模型，并且会受到来自于多源的多媒体感官输入的影响，这些源包括Apple的广告、电视节目、电子邮件、文章以及走廊上的谈话。你可以试着将这个过程与传统的任意字符串模式匹配程序做个比较，并且在输入的灵活性和输出的准确性上保持一致。其次，智能是一种预测，这是解决问题的一种方法。比如：你的眼睛正试图看到它所能看到的一切，与此同时，大脑会根据它期望眼睛所能看到的东西来通过神经系统向眼睛发送预测信息。这种预测机制“填补”了本没有意识到的东西，也就是为什么你无法正常识别视觉盲点的原因。这种预测机制也让你能够在晚上漆黑一片的卧室里行走而不会被绊倒：你的大脑向运动神经系统发送信号，为肌肉提供一个行走时所期望的模型。常规的人工智能与狭义人工智能人工智能这个概念最初是由一些探索超越传统程序的计算机科学家在20世纪50年代提出来的。他们受到了超智能程序的启发，该程序的智能特征与人类相似，比如“星球大战”中的R2D2和C-3PO，以及Superman III中的超级计算机，这是常规的人工智能。常规的人工智能在今天并不存在。我们不知道如何模仿人类大脑进行工作，甚至不知道模仿它的一小部分智能。今天存在的人工智能，我们可以称之为狭义人工智能。现在有许多很有用的产品使用到了狭义人工智能，他们可以保质保量地执行一些任务，甚至比人类做得更好。例如亚马逊的Alexa，它的输入范围是有限的，但同时结合了多种狭义人工智能技术来完成某些任务，这让人错误地认为它具有智能。当前的国际象棋和围棋世界冠军也是狭义人工智能的运用。这些狭义人工智能系统拥有前面讨论过的三个智能元素：存储特定领域的知识、获取新知识的机制以及使用这些知识的机制。当前也存在着几种通过狭义人工智能来解决网络安全领域问题的方法。虽然，能够通过图灵测试并取代安全团队成员的安全机器人并不存在，但是，基于狭义人工智能的工具能够提前发现威胁和漏洞，并且能够比大多数人更好地衡量安全状况。人工智能、机器学习、专家系统和深度学习之间的区别机器学习是归纳算法的应用，是知识获取过程的第一步，是在20世纪60年代探索人工智能的过程中产生的。机器学习可以说是侧重于“学习”的算法。计算机不是通过编写特定的计算机指令来完成任务，而是使用大量数据进行“训练”，使其能够学习如何执行任务。用于训练的样本可以由外部提供，也可以由知识发现过程的前一阶段提供。这么多年来，出现了很多种机器学习算法，包括决策树、归纳逻辑、聚类、贝叶斯网络和人工神经网络。人工智能与统计学密切相关，甚至相互重叠。机器学习被认为是脱胎于专家系统，但又与之不同，专家系统是根据精心准备的知识体系（规则）通过基于模糊规则的推理来解决问题的。专家系统被吹捧为20世纪80年代人工智能最成功的案例。专家系统背后的原理是，智能系统从它们所拥有的知识中获取能力，而不是从它们使用的特定推理策略中获取能力。简而言之，专家系统拥有知识，但并不完全会自学。它们需要人类程序员或操作员来让它们变得更加聪明。但是，如果根据我们对智能的定义进行判断的话，它们并不聪明。现在回到会学习的系统上来。机器学习很难，因为在多个维度的数据之间关联模式是一个难题。这是一个大数据和计算密集型问题。人类大脑不断地从大量的源并且跨越多个维度获取大量的感官数据，慢慢地完善它的模型，然后才能达到网络安全团队中熟练员工的智能和专业知识水平。请设想一下一个大学毕业生大脑所接受的训练数据量（标记的和未标记的）。在大多数情况下，适用于机器学习系统的训练数据相当稀少，从而使得机器学习程序无法提供准确的结果。人工神经网络和深度学习近年来，我们看到一种被称为深度学习的机器学习技术发展十分迅速，这是一种早期机器学习方法人工神经网络的演进，该神经网络是受到人类大脑结构的启发而创造出来的。在神经网络中，每个节点都会为其输入分配权重，以表示其正在执行的操作的正确与否。最终的输出由这些权重的和决定。实际运用中的神经网络有许多层，每个层对应于神经网络要完成的各个子任务。神经网络报告的输出采用“概率向量”的形式，例如，系统可能会说该图像有90％的确信度包含给定的动物，该动物有25％的确信度是鳄鱼。直到目前为止，神经网络的研究几乎没有产生任何可以称之为“智能”的东西。预测输出的确信度很低，因此并没有什么用。你可能已经想到，目前遇到的最基本的问题是，即使是最基本的神经网络，它的计算密集度也很高，建立和使用神经网络来完成复杂的任务是不切实际的。多伦多大学的GeoffreyHinton领导的一个小型研究小组一直在研究这个问题，他们将超级计算机的算法并行化，并证明了这个观点。为了理解这个问题，我们举一个计算机视觉和自动驾驶汽车方面的例子：识别交通停车标志。当停车标志检测神经网络在训练的时候，很可能会出现很多不正确的答案。例如，它在良好的能见度下可能就做得很好，但在恶劣的天气条件下就不行。这个网络需要大量的训练。它需要看到成千上万，甚至数百万的图像，直到各种神经元输入的权重都调好为止，并且无论环境条件如何，它每次都能得出正确的答案。直到做到这一点，我们可能才会说这个神经网络已经学会了停车标志的样子。这正是2012年吴恩达（Andrew Ng）在谷歌所做的事情。吴恩达的重大突破是增加神经网络中的层数和神经元的数量，然后通过运行海量数据来训练它，这些海量数据主要是1000万个来自YouTube视频的图像。深度学习中的“深度”表示这种神经网络包含了的大量层。谷歌大脑项目是一个在16000个CPU核上使用深度学习算法训练的神经网络。该系统已经学会了在YouTube视频中识别一些东西，例如“猫”，即使该系统从未被告知过“猫”是什么。神经网络能“看到”猫这个视觉图像、包含单词“猫”的视觉图像，以及包含单词“猫”的音频之间的相关性，并且将这种相关性学习为知识，就像一个小孩子一样。今天，基于深度学习的图像识别通常做得比人类更好，例如自动驾驶车辆、识别血液中的癌症和核磁共振扫描图像中的肿瘤。还有很多深度学习的变体得到了积极地改进和运用。有些模型可以进行堆叠以产生更高级的分类能力。以下图片来自于亚马逊Rekognition System的演示，深度学习技术能识别出图像和视频流中的对象、面部和上下文。这样的系统是否算是智能呢？由于深度学习和其他先进的机器学习算法确实在它们各自的领域进行了学习，并变得相当的博学，因此它们确实拥有了“智能”的两个关键因素。这样的系统是否知道如何运用它们的知识来解决问题呢？狭义人工智能系统目前需要人工干预才能与现实世界的问题解决工作流程相关联、与传统系统以及其他人员进行交互。人们需要懂得将检测跟踪人员的交通摄像机与其他的面部检测和图像检测系统相融合，使用来自加利福尼亚DMV驾驶执照的图像数据和汽车牌照数据库进行训练。如果在公共场所安装这样的系统，我们就能大大提高社区的犯罪打击能力。试想一下，这样还可能会产生倍增效应，我们的警察、安全人员和调查人员每天都有提高工作效率和速度的潜力！如果拿这种狭义人工智能作为武器，这些特勤人员的工作效率会变得更高。人工智能（AI）、机器学习（ML）、专家系统和深度学习之间的关系也可以参照下面的维恩图进行理解。人工智能、机器学习和深度学习在网络安全领域中的应用网络安全问题可以理解为保证计算机系统的机密性、可用性和完整性。网络防御主要包含以下三个方面：漏洞评估。设置和管理有效的安全控制。安全事件的处理和响应。近年来，网络安全已经成为一个多维度问题。随着业务的“计算机化”，漏洞的数量和种类急剧增加。安全研究人员和黑客们每天都会发现危害计算机系统的新方法。让我们从攻击层面来说明这一点，比如业务应用程序和共享的密码。企业用户的雅虎或LinkedIn密码可能与某个企业应用的密码相同。因此，如果雅虎或LinkedIn遭到攻击并且密码被盗（并且没有设置秘钥），那么你就会遇到了一个问题：黑客有100万种方法来进入企业应用。通常，防御者并不知道密码共享这个风险对他们的业务会有什么影响。在这个时候，你最好希望你的双因素身份认证配置正确并且有效。攻击者利用多个漏洞来攻破你的网络，然后跳转到目标系统上，提升他们的访问权限，最后进行攻击、泄露或破坏信息。对于一个拥有一万多人的机构来说，我们估计在攻击层面上有超过1亿个时变因素。这不再是一个人的问题了，需要手工分析的数据实在太多了。为了防止网络被攻破，防御者必须要及时发现并解决这些漏洞，这通常涉及到对系统的重新设置或给系统打补丁、用户培训、安装其他安全软件、优化流程。最后，虽然我们尽了最大的努力，网络还是被攻破了。安全管理员每天需要处理的安全报警数量一直都在增长。报警的处理涉及到从多个系统获取数据，这相当乏味，而且很耗时。大多数机构都没有足够训练有素的人员来处理每天发生的安全警报。网络安全中的人工智能现在，想象一下，如果你拥有一套经过训练的自学系统，该系统能够自动而又连续地从各种各样的源中收集有关企业的数据，并对数百个维度的模式进行关联。该系统包含了下面这几种智能：了解企业资产相关的所有详细信息（配置、使用情况等等），包括所有设备、用户和应用程序，包括内部的和外部的。深入了解每个资产和用户在业务上的重要程度。及时更新全球和行业特定威胁的最新知识，也就是说每天或每周最新的威胁。深入理解已经部署的各种安全产品和流程。综合上述第1-4项中的所有信息，计算你的有效风险，并预测最有可能被攻击的地方和方式。提供规范性的建议，说明如何进行配置和增强安全控制和流程，提高网络弹性，但同时不会对业务运营产生负面影响。为安全警报的处理优先级和处理方式提供尽可能多的参考信息，并尽可能地减小对其影响。通过提供多种不同的可视化界面和报告，向所有的利益相关者，比如用户、业务所有者、安全操作员、CISO（信息安全官）、审计员、首席信息官、首席执行官和董事会成员提供相关信息，解释其预测和建议。本文由网易云易盾组织翻译，译者：雁惊寒网易云安全（易盾）基于网易20年技术积累及安全大数据，为互联网各行业提供反垃圾、验证码、注册保护、登录保护、活动反作弊、应用加固、DDoS 防护等整体安全解决方案，全程提供完善的技术支持，助力产品建立安全防护体系，点击可免费试用。相关文章：【推荐】 浅析电商防止恶意下单"}
{"content2":"以下链接是关于计算机视觉（ComputerVision, CV）相关领域的网站链接，其中有CV牛人的主页，CV研究小组的主页，CV领域的paper,代码，CV领域的最新动态，国内的应用情况等等。打算从事这个行业或者刚入门的朋友可以多关注这些网站，多了解一些CV的具体应用。搞研究的朋友也可以从中了解到很多牛人的研究动态、招生情况等。总之，我认为，知识只有分享才能产生更大的价值，真诚希望下面的链接能对朋友们有所帮助。（1）googleResearch； http://research.google.com/index.html（2）MIT博士，汤晓欧学生林达华； http://people.csail.mit.edu/dhlin/index.html（3）MIT博士后Douglas Lanman； http://web.media.mit.edu/~dlanman/（4）opencv中文网站； http://www.opencv.org.cn/index.php/%E9%A6%96%E9%A1%B5（5）Stanford大学vision实验室； http://vision.stanford.edu/research.html（6）Stanford大学博士崔靖宇； http://www.stanford.edu/~jycui/（7）UCLA教授朱松纯； http://www.stat.ucla.edu/~sczhu/（8）中国人工智能网； http://www.chinaai.org/（9）中国视觉网； http://www.china-vision.net/（10）中科院自动化所； http://www.ia.cas.cn/（11）中科院自动化所李子青研究员； http://www.cbsr.ia.ac.cn/users/szli/（12）中科院计算所山世光研究员； http://www.jdl.ac.cn/user/sgshan/（13）人脸识别主页； http://www.face-rec.org/（14）加州大学伯克利分校CV小组； http://www.eecs.berkeley.edu/Research/P ... CS/vision/（15）南加州大学CV实验室； http://iris.usc.edu/USC-Computer-Vision.html（16）卡内基梅隆大学CV主页；http://www.cs.cmu.edu/afs/cs/project/ci ... ision.html（17）微软CV研究员Richard Szeliski；http://research.microsoft.com/en-us/um/people/szeliski/（18）微软亚洲研究院计算机视觉研究组； http://research.microsoft.com/en-us/groups/vc/（19）微软剑桥研究院ML与CV研究组； http://research.microsoft.com/en-us/gro ... fault.aspx（20）研学论坛； http://bbs.matwav.com/（21）美国Rutgers大学助理教授刘青山； http://www.research.rutgers.edu/~qsliu/（22）计算机视觉最新资讯网； http://www.cvchina.info/（23）运动检测、阴影、跟踪的测试视频下载； http://apps.hi.baidu.com/share/detail/18903287（24）香港中文大学助理教授王晓刚； http://www.ee.cuhk.edu.hk/~xgwang/(25)香港中文大学多媒体实验室（汤晓鸥）; http://mmlab.ie.cuhk.edu.hk/(26)U.C. San Diego. computer vision;http://vision.ucsd.edu/content/home(27)CVonline; http://homepages.inf.ed.ac.uk/rbf/CVonline/(28)computer vision software; http://peipa.essex.ac.uk/info/software.html(29)Computer Vision Resource; http://www.cvpapers.com/(30)computer vision research groups;http://peipa.essex.ac.uk/info/groups.html(31)computer vision center; http://computervisioncentral.com/cvcnews(32)浙江大学图像技术研究与应用（ITRA）团队：http://www.dvzju.com/(33)自动识别网：http://www.autoid-china.com.cn/(34)清华大学章毓晋教授：http://www.tsinghua.edu.cn/publish/ee/4157/2010/20101217173552339241557/20101217173552339241557_.html(35)顶级民用机器人研究小组Porf.Gary领导的Willow Garage:http://www.willowgarage.com/(36)上海交通大学图像处理与模式识别研究所：http://www.pami.sjtu.edu.cn/(37)上海交通大学计算机视觉实验室刘允才教授：http://www.visionlab.sjtu.edu.cn/(38)德克萨斯州大学奥斯汀分校助理教授Kristen Grauman ：http://www.cs.utexas.edu/~grauman/(39)清华大学电子工程系智能图文信息处理实验室（丁晓青教授）：http://ocrserv.ee.tsinghua.edu.cn/auto/index.asp(40)北京大学高文教授：http://www.jdl.ac.cn/htm-gaowen/(41)清华大学艾海舟教授：http://media.cs.tsinghua.edu.cn/cn/aihz(42)中科院生物识别与安全技术研究中心：http://www.cbsr.ia.ac.cn/china/index%20CH.asp(43)瑞士巴塞尔大学 Thomas Vetter教授：http://informatik.unibas.ch/personen/vetter_t.html(44)俄勒冈州立大学 Rob Hess博士：http://blogs.oregonstate.edu/hess/(45)深圳大学 于仕祺副教授：http://yushiqi.cn/(46)西安交通大学人工智能与机器人研究所：http://www.aiar.xjtu.edu.cn/(47)卡内基梅隆大学研究员Robert T. Collins:http://www.cs.cmu.edu/~rcollins/home.html#Background(48)MIT博士Chris Stauffer:http://people.csail.mit.edu/stauffer/Home/index.php(49)美国密歇根州立大学生物识别研究组(Anil K. Jain教授)：http://www.cse.msu.edu/rgroups/biometrics/(50)美国伊利诺伊州立大学Thomas S. Huang:http://www.beckman.illinois.edu/directory/t-huang1(51)武汉大学数字摄影测量与计算机视觉研究中心：http://www.whudpcv.cn/index.asp(52)瑞士巴塞尔大学Sami Romdhani助理研究员：http://informatik.unibas.ch/personen/romdhani_sami/(53)CMU大学研究员Yang Wang:http://www.cs.cmu.edu/~wangy/home.html(54)英国曼彻斯特大学Tim Cootes教授：http://personalpages.manchester.ac.uk/staff/timothy.f.cootes/(55)美国罗彻斯特大学教授Jiebo Luo:http://www.cs.rochester.edu/u/jluo/(56)美国普渡大学机器人视觉实验室：https://engineering.purdue.edu/RVL/Welcome.html(57)美国宾利州立大学感知、运动与认识实验室：http://vision.cse.psu.edu/home/home.shtml(58)美国宾夕法尼亚大学GRASP实验室：https://www.grasp.upenn.edu/(59)美国内达华大学里诺校区CV实验室：http://www.cse.unr.edu/CVL/index.php(60)美国密西根大学vision实验室：http://www.eecs.umich.edu/vision/index.html(61)University of Massachusetts(麻省大学),视觉实验室：http://vis-www.cs.umass.edu/index.html(62)华盛顿大学博士后Iva Kemelmacher:http://www.cs.washington.edu/homes/kemelmi(63)以色列魏茨曼科技大学Ronen Basri:http://www.wisdom.weizmann.ac.il/~ronen/index.html(64)瑞士ETH-Zurich大学CV实验室：http://www.vision.ee.ethz.ch/boostingTrackers/index.htm(65)微软CV研究员张正友：http://research.microsoft.com/en-us/um/people/zhang/(66)中科院自动化所医学影像研究室：http://www.3dmed.net/(67)中科院田捷研究员：http://www.3dmed.net/tian/(68)微软Redmond研究院研究员Simon Baker:http://research.microsoft.com/en-us/people/sbaker/(69)普林斯顿大学教授李凯：http://www.cs.princeton.edu/~li/(70)普林斯顿大学博士贾登：http://www.cs.princeton.edu/~jiadeng/(71)牛津大学教授Andrew Zisserman： http://www.robots.ox.ac.uk/~az/(72)英国leeds大学研究员Mark Everingham:http://www.comp.leeds.ac.uk/me/(73)英国爱丁堡大学教授Chris William: http://homepages.inf.ed.ac.uk/ckiw/(74)微软剑桥研究院研究员John Winn: http://johnwinn.org/(75)佐治亚理工学院教授Monson H.Hayes：http://savannah.gatech.edu/people/mhayes/index.html(76)微软亚洲研究院研究员孙剑：http://research.microsoft.com/en-us/people/jiansun/(77)微软亚洲研究院研究员马毅：http://research.microsoft.com/en-us/people/mayi/(78)英国哥伦比亚大学教授David Lowe: http://www.cs.ubc.ca/~lowe/(79)英国爱丁堡大学教授Bob Fisher: http://homepages.inf.ed.ac.uk/rbf/(80)加州大学圣地亚哥分校教授Serge J.Belongie:http://cseweb.ucsd.edu/~sjb/(81)威斯康星大学教授Charles R.Dyer: http://pages.cs.wisc.edu/~dyer/(82)多伦多大学教授Allan.Jepson: http://www.cs.toronto.edu/~jepson/(83)伦斯勒理工学院教授Qiang Ji: http://www.ecse.rpi.edu/~qji/(84)CMU研究员Daniel Huber: http://www.ri.cmu.edu/person.html?person_id=123(85)多伦多大学教授：David J.Fleet: http://www.cs.toronto.edu/~fleet/(86)伦敦大学玛丽女王学院教授Andrea Cavallaro:http://www.eecs.qmul.ac.uk/~andrea/(87)多伦多大学教授Kyros Kutulakos: http://www.cs.toronto.edu/~kyros/(88)杜克大学教授Carlo Tomasi: http://www.cs.duke.edu/~tomasi/(89)CMU教授Martial Hebert: http://www.cs.cmu.edu/~hebert/(90)MIT助理教授Antonio Torralba: http://web.mit.edu/torralba/www/(91)马里兰大学研究员Yasel Yacoob: http://www.umiacs.umd.edu/users/yaser/(92)康奈尔大学教授Ramin Zabih: http://www.cs.cornell.edu/~rdz/(93)CMU博士田渊栋: http://www.cs.cmu.edu/~yuandong/(94)CMU副教授Srinivasa Narasimhan: http://www.cs.cmu.edu/~srinivas/(95)CMU大学ILIM实验室：http://www.cs.cmu.edu/~ILIM/(96)哥伦比亚大学教授Sheer K.Nayar: http://www.cs.columbia.edu/~nayar/(97)三菱电子研究院研究员Fatih Porikli ：http://www.porikli.com/(98)康奈尔大学教授Daniel Huttenlocher：http://www.cs.cornell.edu/~dph/(99)南京大学教授周志华：http://cs.nju.edu.cn/zhouzh/index.htm(100)芝加哥丰田技术研究所助理教授Devi Parikh: http://ttic.uchicago.edu/~dparikh/index.html(101)瑞士联邦理工学院博士后Helmut Grabner: http://www.vision.ee.ethz.ch/~hegrabne/#Short_CV(102)香港中文大学教授贾佳亚：http://www.cse.cuhk.edu.hk/~leojia/index.html(103)南洋理工大学副教授吴建鑫：http://c2inet.sce.ntu.edu.sg/Jianxin/index.html(104)GE研究院研究员李关：http://www.cs.unc.edu/~lguan/(105)佐治亚理工学院教授Monson Hayes:http://savannah.gatech.edu/people/mhayes/(106)图片检索国际会议VOC(微软剑桥研究院组织): http://pascallin.ecs.soton.ac.uk/challenges/VOC/(107)机器视觉开源处理库汇总：http://archive.cnblogs.com/a/2217609/(108)布朗大学教授Benjamin Kimia: http://www.lems.brown.edu/kimia.html(109)数据堂-图像处理相关的样本数据：http://www.datatang.com/data/list/602026/p1(110)东软基于CV的汽车辅助驾驶系统：http://www.neusoft.com/cn/solutions/1047/(111)马里兰大学教授Rema Chellappa:http://www.cfar.umd.edu/~rama/(112)芝加哥丰田研究中心助理教授Devi Parikh：http://ttic.uchicago.edu/~dparikh/index.html(113)宾夕法尼亚大学助理教授石建波：http://www.cis.upenn.edu/~jshi/(114)比利时鲁汶大学教授Luc Van Gool：http://www.vision.ee.ethz.ch/members/get_member.cgi?id=1,http://www.vision.ee.ethz.ch/~vangool/(115)行人检测主页：http://www.pedestrian-detection.com/(116)法国学习算法与系统实验室Basilio Noris博士：http://lasa.epfl.ch/people/member.php?SCIPER=129576http://mldemos.epfl.ch/(117)美国马里兰大学LARRY S.DAVIS教授：http://www.umiacs.umd.edu/~lsd/(118)计算机视觉论文分类导航：http://www.visionbib.com/bibliography/contents.html(119)计算机视觉分类信息导航：http://www.visionbib.com/(120)西班牙马德里理工大学博士Marcos Nieto：http://marcosnieto.net/(121)香港理工大学副教授张磊：http://www4.comp.polyu.edu.hk/~cslzhang/(122)以色列技术学院教授Michael Elad：http://www.cs.technion.ac.il/~elad/(123)韩国启明大学计算机视觉与模式识别实验室：http://cvpr.kmu.ac.kr/(124)英国诺丁汉大学Michel Valstar博士：http://www.cs.nott.ac.uk/~mfv/(125)卡内基梅隆大学Takeo Kanade教授:http://www.ri.cmu.edu/people/kanade_takeo.html(126)微软学术搜索：http://libra.msra.cn/(127)比利时天主教鲁汶大学Radu Timofte博士：http://homes.esat.kuleuven.be/~rtimofte/，交通标志检测，定位，3D跟踪(128)迪斯尼匹兹堡研究院研究员：Iain Matthews:http://www.iainm.com/iainm/Home.htmlhttp://www.ri.cmu.edu/person.html?type= ... son_id=741 AAM,三维重建（129）康奈尔大学视觉与图像分析组：http://www.via.cornell.edu/ 医学图像处理（130）密西根州立大学生物识别研究组：http://www.cse.msu.edu/biometrics/ 人脸识别、指纹识别、图像检索（131）柏林科技大学计算机视觉与遥感实验室：http://www.cv.tu-berlin.de/menue/computer_vision_remote_sensing/parameter/en/ 图像分析、物体重建、基于图像的表面测量、医学图像处理（132）英国布里斯托大学数字多媒体研究组：http://www.cs.bris.ac.uk/Research/Digitalmedia/ 运动检测与跟踪、视频压缩、3D重建、字符定位（133）英国萨利大学视觉、语音与信号处理中心： http://www.surrey.ac.uk/cvssp/ 人脸识别、监控、3D、视频检索、（134）北卡莱罗纳大学教堂山分校Marc Pollefeys教授：http://www.cs.unc.edu/~marc/ 基于视频的3D模型生成、相机标定、运动检测与分析、3D重建（135）澳大利亚国立大学Richard Hartley教授：http://users.cecs.anu.edu.au/~hartley/ 运动估计、稀疏子空间、跟踪、（136）百度技术副总监于凯：http://www.dbs.ifi.lmu.de/~yu_k/ 深度学习，稀疏表示，图像分类（137）西安电子科技大学高新波教授：http://web.xidian.edu.cn/xbgao/index.html 质量评判、水印、稀疏表示、超分辨率（138）加州大学伯克利分校Michael I.Jordan教授：http://www.cs.berkeley.edu/~jordan/ 机器学习（139）加州理工行人检测相关资料：http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/（140）微软Redmond研究院研究员Piotr Dollar: http://vision.ucsd.edu/~pdollar/ 行人检测、特征提取、（141）视觉计算研究论坛：http://www.sigvc.org/bbs/ 中科院视觉计算研究小组的论坛（142）美国坦桑尼亚州立大学稀疏学习软件包：http://www.public.asu.edu/~jye02/Software/SLEP/index.htm 稀疏学习（143）美国加州大学圣地亚哥分校Jacob Whitehill博士：http://mplab.ucsd.edu/~jake/ 机器学习（144）美国布朗大学Michael J.Black教授：http://cs.brown.edu/~black/ 人的姿态估计和跟踪（145）美国加州大学圣地亚哥分校David Kriegman教授：http://cseweb.ucsd.edu/~kriegman/ 人脸识别（146）南加州大学Paul Debevec教授：http://ict.debevec.org/~debevec/ 或 http://www.pauldebevec.com/ 将CV和CG结合研究 人脸捕捉重建技术（147）伊利诺伊大学D.A.Forsyth教授：http://luthuli.cs.uiuc.edu/~daf/ 三维重建（148）英国牛津大学Ian Reid教授：http://www.robots.ox.ac.uk/~ian/ 跟踪和机器人导航（149）CMU大学Alyosha Efros 教授: https://www.cs.cmu.edu/~efros/ 图像纹理合成（150）加州大学伯克利分校Jitendra Malik教授：http://www.cs.berkeley.edu/~malik/ 轮廓检测、图像/视频分割、图形匹配、目标识别（151）MIT教授William Freeman： http://people.csail.mit.edu/billf/ 应用于CV的ML、可视化感知的贝叶斯模型、计算摄影学；最有影响力的研究成果：图像纹理合成（152）CMU博士Henry Schneiderman： http://www.cs.cmu.edu/~hws/ 目标检测和识别；最有影响力的研究成果：目标检测；2000年CVPR上发表了”A statistical method for 3D object detection applied to faces and cars”。该算法采用多视角训练样本，可用于检测不同视角下的物体，如人脸和车，是第一个能够检测侧脸的算法。他创建了PittPatt公司，后被Google收购（153）微软研究员Paul Viola: http://research.microsoft.com/en-us/um/people/viola/ AdaBoost算法（154）微软研究员Antonio Criminisi: http://research.microsoft.com/en-us/people/antcrim/ 图像修补，三维重建，目标检测与跟踪；（155）魏茨曼科学研究所教授Michal Irani: http://www.wisdom.weizmann.ac.il/~irani/ 超分辨率（156）瑞士洛桑理工学院Pascal Fua教授：http://people.epfl.ch/pascal.fua/bio?lang=en 立体视觉，增强现实（157）佐治亚理工学院Irfan Essa教授：http://www.ic.gatech.edu/people/irfan-essa 人脸表情识别"}
{"content2":"计算机视觉、机器学习相关领域论文和源代码大集合--持续更新……zouxy09@qq.comhttp://blog.csdn.net/zouxy09注：下面有project网站的大部分都有paper和相应的code。Code一般是C/C++或者Matlab代码。最近一次更新：2013-3-17一、特征提取Feature Extraction：·         SIFT [1] [Demo program][SIFT Library] [VLFeat]·         PCA-SIFT [2] [Project]·         Affine-SIFT [3] [Project]·         SURF [4] [OpenSURF] [Matlab Wrapper]·         Affine Covariant Features [5] [Oxford project]·         MSER [6] [Oxford project] [VLFeat]·         Geometric Blur [7] [Code]·         Local Self-Similarity Descriptor [8] [Oxford implementation]·         Global and Efficient Self-Similarity [9] [Code]·         Histogram of Oriented Graidents [10] [INRIA Object Localization Toolkit] [OLT toolkit for Windows]·         GIST [11] [Project]·         Shape Context [12] [Project]·         Color Descriptor [13] [Project]·         Pyramids of Histograms of Oriented Gradients [Code]·         Space-Time Interest Points (STIP) [14][Project] [Code]·         Boundary Preserving Dense Local Regions [15][Project]·         Weighted Histogram[Code]·         Histogram-based Interest Points Detectors[Paper][Code]·         An OpenCV - C++ implementation of Local Self Similarity Descriptors [Project]·         Fast Sparse Representation with Prototypes[Project]·         Corner Detection [Project]·         AGAST Corner Detector: faster than FAST and even FAST-ER[Project]·         Real-time Facial Feature Detection using Conditional Regression Forests[Project]·         Global and Efficient Self-Similarity for Object Classification and Detection[code]·         WαSH: Weighted α-Shapes for Local Feature Detection[Project]·         HOG[Project]·         Online Selection of Discriminative Tracking Features[Project]二、图像分割Image Segmentation：·           Normalized Cut [1] [Matlab code]·           Gerg Mori’ Superpixel code [2] [Matlab code]·           Efficient Graph-based Image Segmentation [3] [C++ code] [Matlab wrapper]·           Mean-Shift Image Segmentation [4] [EDISON C++ code] [Matlab wrapper]·           OWT-UCM Hierarchical Segmentation [5] [Resources]·           Turbepixels [6] [Matlab code 32bit] [Matlab code 64bit] [Updated code]·           Quick-Shift [7] [VLFeat]·           SLIC Superpixels [8] [Project]·           Segmentation by Minimum Code Length [9] [Project]·           Biased Normalized Cut [10] [Project]·           Segmentation Tree [11-12] [Project]·           Entropy Rate Superpixel Segmentation [13] [Code]·           Fast Approximate Energy Minimization via Graph Cuts[Paper][Code]·           Efﬁcient Planar Graph Cuts with Applications in Computer Vision[Paper][Code]·           Isoperimetric Graph Partitioning for Image Segmentation[Paper][Code]·           Random Walks for Image Segmentation[Paper][Code]·           Blossom V: A new implementation of a minimum cost perfect matching algorithm[Code]·           An Experimental Comparison of Min-Cut/Max-Flow Algorithms for Energy Minimization in Computer Vision[Paper][Code]·           Geodesic Star Convexity for Interactive Image Segmentation[Project]·           Contour Detection and Image Segmentation Resources[Project][Code]·           Biased Normalized Cuts[Project]·           Max-flow/min-cut[Project]·           Chan-Vese Segmentation using Level Set[Project]·           A Toolbox of Level Set Methods[Project]·           Re-initialization Free Level Set Evolution via Reaction Diffusion[Project]·           Improved C-V active contour model[Paper][Code]·           A Variational Multiphase Level Set Approach to Simultaneous Segmentation and Bias Correction[Paper][Code]·          Level Set Method Research by Chunming Li[Project]·          ClassCut for Unsupervised Class Segmentation[code]·         SEEDS: Superpixels Extracted via Energy-Driven Sampling[Project][other]三、目标检测Object Detection：·           A simple object detector with boosting [Project]·           INRIA Object Detection and Localization Toolkit [1] [Project]·           Discriminatively Trained Deformable Part Models [2] [Project]·           Cascade Object Detection with Deformable Part Models [3] [Project]·           Poselet [4] [Project]·           Implicit Shape Model [5] [Project]·           Viola and Jones’s Face Detection [6] [Project]·           Bayesian Modelling of Dyanmic Scenes for Object Detection[Paper][Code]·           Hand detection using multiple proposals[Project]·           Color Constancy, Intrinsic Images, and Shape Estimation[Paper][Code]·           Discriminatively trained deformable part models[Project]·           Gradient Response Maps for Real-Time Detection of Texture-Less Objects: LineMOD [Project]·           Image Processing On Line[Project]·           Robust Optical Flow Estimation[Project]·           Where's Waldo: Matching People in Images of Crowds[Project]·           Scalable Multi-class Object Detection[Project]·           Class-Specific Hough Forests for Object Detection[Project]·         Deformed Lattice Detection In Real-World Images[Project]·         Discriminatively trained deformable part models[Project]四、显著性检测Saliency Detection：·           Itti, Koch, and Niebur’ saliency detection [1] [Matlab code]·           Frequency-tuned salient region detection [2] [Project]·           Saliency detection using maximum symmetric surround [3] [Project]·           Attention via Information Maximization [4] [Matlab code]·           Context-aware saliency detection [5] [Matlab code]·           Graph-based visual saliency [6] [Matlab code]·           Saliency detection: A spectral residual approach. [7] [Matlab code]·           Segmenting salient objects from images and videos. [8] [Matlab code]·           Saliency Using Natural statistics. [9] [Matlab code]·           Discriminant Saliency for Visual Recognition from Cluttered Scenes. [10] [Code]·           Learning to Predict Where Humans Look [11] [Project]·           Global Contrast based Salient Region Detection [12] [Project]·           Bayesian Saliency via Low and Mid Level Cues[Project]·           Top-Down Visual Saliency via Joint CRF and Dictionary Learning[Paper][Code]·         Saliency Detection: A Spectral Residual Approach[Code]五、图像分类、聚类Image Classification, Clustering·           Pyramid Match [1] [Project]·           Spatial Pyramid Matching [2] [Code]·           Locality-constrained Linear Coding [3] [Project] [Matlab code]·           Sparse Coding [4] [Project] [Matlab code]·           Texture Classification [5] [Project]·           Multiple Kernels for Image Classification [6] [Project]·           Feature Combination [7] [Project]·           SuperParsing [Code]·           Large Scale Correlation Clustering Optimization[Matlab code]·           Detecting and Sketching the Common[Project]·           Self-Tuning Spectral Clustering[Project][Code]·           User Assisted Separation of Reflections from a Single Image Using a Sparsity Prior[Paper][Code]·           Filters for Texture Classification[Project]·           Multiple Kernel Learning for Image Classification[Project]·          SLIC Superpixels[Project]六、抠图Image Matting·           A Closed Form Solution to Natural Image Matting [Code]·           Spectral Matting [Project]·           Learning-based Matting [Code]七、目标跟踪Object Tracking：·           A Forest of Sensors - Tracking Adaptive Background Mixture Models [Project]·           Object Tracking via Partial Least Squares Analysis[Paper][Code]·           Robust Object Tracking with Online Multiple Instance Learning[Paper][Code]·           Online Visual Tracking with Histograms and Articulating Blocks[Project]·           Incremental Learning for Robust Visual Tracking[Project]·           Real-time Compressive Tracking[Project]·           Robust Object Tracking via Sparsity-based Collaborative Model[Project]·           Visual Tracking via Adaptive Structural Local Sparse Appearance Model[Project]·           Online Discriminative Object Tracking with Local Sparse Representation[Paper][Code]·           Superpixel Tracking[Project]·           Learning Hierarchical Image Representation with Sparsity, Saliency and Locality[Paper][Code]·           Online Multiple Support Instance Tracking [Paper][Code]·           Visual Tracking with Online Multiple Instance Learning[Project]·           Object detection and recognition[Project]·           Compressive Sensing Resources[Project]·           Robust Real-Time Visual Tracking using Pixel-Wise Posteriors[Project]·           Tracking-Learning-Detection[Project][OpenTLD/C++ Code]·           the HandVu：vision-based hand gesture interface[Project]·           Learning Probabilistic Non-Linear Latent Variable Models for Tracking Complex Activities[Project]八、Kinect：·           Kinect toolbox[Project]·           OpenNI[Project]·           zouxy09 CSDN Blog[Resource]·           FingerTracker 手指跟踪[code]九、3D相关：·           3D Reconstruction of a Moving Object[Paper] [Code]·           Shape From Shading Using Linear Approximation[Code]·           Combining Shape from Shading and Stereo Depth Maps[Project][Code]·           Shape from Shading: A Survey[Paper][Code]·           A Spatio-Temporal Descriptor based on 3D Gradients (HOG3D)[Project][Code]·           Multi-camera Scene Reconstruction via Graph Cuts[Paper][Code]·           A Fast Marching Formulation of Perspective Shape from Shading under Frontal Illumination[Paper][Code]·           Reconstruction:3D Shape, Illumination, Shading, Reflectance, Texture[Project]·           Monocular Tracking of 3D Human Motion with a Coordinated Mixture of Factor Analyzers[Code]·           Learning 3-D Scene Structure from a Single Still Image[Project]十、机器学习算法：·           Matlab class for computing Approximate Nearest Nieghbor (ANN) [Matlab class providing interface toANN library]·           Random Sampling[code]·           Probabilistic Latent Semantic Analysis (pLSA)[Code]·           FASTANN and FASTCLUSTER for approximate k-means (AKM)[Project]·           Fast Intersection / Additive Kernel SVMs[Project]·           SVM[Code]·           Ensemble learning[Project]·           Deep Learning[Net]·           Deep Learning Methods for Vision[Project]·           Neural Network for Recognition of Handwritten Digits[Project]·           Training a deep autoencoder or a classifier on MNIST digits[Project]·          THE MNIST DATABASE of handwritten digits[Project]·          Ersatz：deep neural networks in the cloud[Project]·          Deep Learning [Project]·          sparseLM : Sparse Levenberg-Marquardt nonlinear least squares in C/C++[Project]·          Weka 3: Data Mining Software in Java[Project]·          Invited talk \"A Tutorial on Deep Learning\" by Dr. Kai Yu (余凯)[Video]·          CNN - Convolutional neural network class[Matlab Tool]·          Yann LeCun's Publications[Wedsite]·          LeNet-5, convolutional neural networks[Project]·          Training a deep autoencoder or a classifier on MNIST digits[Project]·          Deep Learning 大牛Geoffrey E. Hinton's HomePage[Website]·         Multiple Instance Logistic Discriminant-based Metric Learning (MildML) and Logistic Discriminant-based Metric Learning (LDML)[Code]·         Sparse coding simulation software[Project]·         Visual Recognition and Machine Learning Summer School[Software]十一、目标、行为识别Object, Action Recognition：·           Action Recognition by Dense Trajectories[Project][Code]·           Action Recognition Using a Distributed Representation of Pose and Appearance[Project]·           Recognition Using Regions[Paper][Code]·           2D Articulated Human Pose Estimation[Project]·           Fast Human Pose Estimation Using Appearance and Motion via Multi-Dimensional Boosting Regression[Paper][Code]·           Estimating Human Pose from Occluded Images[Paper][Code]·           Quasi-dense wide baseline matching[Project]·           ChaLearn Gesture Challenge: Principal motion: PCA-based reconstruction of motion histograms[Project]·           Real Time Head Pose Estimation with Random Regression Forests[Project]·           2D Action Recognition Serves 3D Human Pose Estimation[Project]·           A Hough Transform-Based Voting Framework for Action Recognition[Project]·           Motion Interchange Patterns for Action Recognition in Unconstrained Videos[Project]·         2D articulated human pose estimation software[Project]·         Learning and detecting shape models [code]·         Progressive Search Space Reduction for Human Pose Estimation[Project]·         Learning Non-Rigid 3D Shape from 2D Motion[Project]十二、图像处理：·         Distance Transforms of Sampled Functions[Project]·        The Computer Vision Homepage[Project]·        Efficient appearance distances between windows[code]·         Image Exploration algorithm[code]·         Motion Magnification 运动放大 [Project]·         Bilateral Filtering for Gray and Color Images 双边滤波器 [Project]·         A Fast Approximation of the Bilateral Filter using a Signal Processing Approach [Project]十三、一些实用工具：·           EGT: a Toolbox for Multiple View Geometry and Visual Servoing[Project] [Code]·           a development kit of matlab mex functions for OpenCV library[Project]·           Fast Artificial Neural Network Library[Project]十四、人手及指尖检测与识别：·           finger-detection-and-gesture-recognition [Code]·           Hand and Finger Detection using JavaCV[Project]·           Hand and fingers detection[Code]十五、场景解释：·           Nonparametric Scene Parsing via Label Transfer [Project]十六、光流Optical flow：·         High accuracy optical flow using a theory for warping [Project]·         Dense Trajectories Video Description [Project]·         SIFT Flow: Dense Correspondence across Scenes and its Applications[Project]·         KLT: An Implementation of the Kanade-Lucas-Tomasi Feature Tracker [Project]·         Tracking Cars Using Optical Flow[Project]·         Secrets of optical flow estimation and their principles[Project]·         implmentation of the Black and Anandan dense optical flow method[Project]·         Optical Flow Computation[Project]·         Beyond Pixels: Exploring New Representations and Applications for Motion Analysis[Project]·         A Database and Evaluation Methodology for Optical Flow[Project]·         optical flow relative[Project]·         Robust Optical Flow Estimation [Project]·         optical flow[Project]十七、图像检索Image Retrieval：·           Semi-Supervised Distance Metric Learning for Collaborative Image Retrieval [Paper][code]十八、马尔科夫随机场Markov Random Fields：·         Markov Random Fields for Super-Resolution [Project]·         A Comparative Study of Energy Minimization Methods for Markov Random Fields with Smoothness-Based Priors [Project]十九、运动检测Motion detection：·         Moving Object Extraction, Using Models or Analysis of Regions [Project]·         Background Subtraction: Experiments and Improvements for ViBe [Project]·         A Self-Organizing Approach to Background Subtraction for Visual Surveillance Applications [Project]·         changedetection.net: A new change detection benchmark dataset[Project]·         ViBe - a powerful technique for background detection and subtraction in video sequences[Project]·         Background Subtraction Program[Project]·         Motion Detection Algorithms[Project]·         Stuttgart Artificial Background Subtraction Dataset[Project]·         Object Detection, Motion Estimation, and Tracking[Project]"}
{"content2":"我一直很好奇人工智能是如何提出来的，它背后有什么样的故事，在人工智能发展的这60年的时间中，又经历了什么？为什么现在才是人工智能的爆发点，未来人工智能又将走向何处？带着这样的问题我读了吴军博士的《智能时代》这本书，打开了我对人工智能的了解，这篇文章主要内容也来自于这本书。我们这代人对人工智能的关注，来自于2016年AlphaGo大战世界著名围棋选手李世民，在比赛之前各方关注度非常高，国内各方媒体争相报道，预测这场比赛的结果，人们好奇人工智能现在智能到什么程度以及计算机如何和人下围棋，最终AlphaGo以4：1胜了李世明，大家都在感慨人工智能时代即将来临。仅仅过了一年，2017年5月27日AlphaGo的2.0版本3:0战胜围棋世界排名第一的柯洁九段，从此在AlphaGo面前已无人类对手。计算机之所以能够战胜人类，是因为机器获得智能的方式和人类不同，它不是靠逻辑推理，而是靠大数据和算法。Google使用了几十万盘围棋高手之间的对弈的数据来训练AlphaGo，这是它获得所谓“智能”的原因。在计算方面，Google使用了几十万台服务器来训练AlphaGo下棋模型，并让不同的AlphaGo相互对弈上千万盘。第二个关键技术是启发式搜索算法-蒙特卡洛树搜索算法（英语：Monte Carlo tree search；简称：MCTS），它能将搜索的空间限制在非常有限的范围内，保证计算机能够快速找到好的下法。由此可见，下围棋这个看似智能型的问题，从本质上讲，是一个大数据和算法的问题。说到人工智能，就不得不提计算机届的一个传奇人物：阿兰.图灵博士。1950年，图灵在《思想》（mind）杂志上发表了一篇《计算的机器和智能》的论文。在论文中，图灵既没有讲计算机怎样才能获得智能，也没有提出如何解决复杂问题的智能方法，知识提出了一个验证机器有无智能的的判别方法。让一台机器和一个人坐在幕后，让一个裁判同时与幕后的人和机器进行交流，如果这个裁判无法判断自己交流的对象是人还是机器，就说明这台机器有了和人同等的智能。就是大名鼎鼎的图灵测试。后来，计算机科学家对此进行了补充，如果计算机实现了下面几件事情中的一件，就可以认为它有图灵所说的那种智能：1、语音识别2、机器翻译3、文本的自动摘要或者写作4、战胜人类的国际象棋冠军5、自动回答问题今天，计算机已经做到了上述的这几件事情，甚至还超额完成了任务，比如现在的围棋比国际象棋要高出6-8个数量级，当然，人类走到这一步并非一帆风顺，而是走了几十年的弯路。人工智能的诞生：1943 - 1956在20世纪40年代和50年代，来自不同领域（数学，心理学，工程学，经济学和政治学）的一批科学家开始探讨制造人工大脑的可能性。1956年，人工智能被确立为一门学科。1956年的夏天，香农和一群年轻的学者在达特茅斯学院召开了一次头脑风暴式研讨会。会议的组织者是马文·闵斯基，约翰·麦卡锡和另两位资深科学家Claude Shannon以及Nathan Rochester，后者来自IBM。与会者包括Ray Solomonoff，Oliver Selfridge，Trenchard More，Arthur Samuel，Newell和Simon，他们中的每一位都将在AI研究的第一个十年中作出重要贡献。会议虽然叫做“达特茅斯夏季人工智能研究会议”，其实它不同于今天我们召开几天的学术会议，因为一来没有什么可以报告的科研成果，二来这个会议持续了一个暑假。事实上，这是一次头脑风暴式的讨论会，这10位年轻的学者讨论的是当时计算机尚未解决，甚至尚未开展研究的问题，包括人工智能、自然语言处理和神经网络等。会上纽厄尔和西蒙讨论了“逻辑理论家”，而麦卡锡则说服与会者接受“人工智能”一词作为本领域的名称。1956年达特矛斯会议上人工智能的名称和任务得以确定，同时出现了最初的成就和最早的一批研究者，因此这一事件被广泛承认为人工智能诞生的标志。60年前的达特茅斯大学黄金年代：1956 - 1974达特茅斯会议之后的数年是大发现的时代。对许多人而言，这一阶段开发出的程序堪称神奇：计算机可以解决代数应用题，证明几何定理，学习和使用英语。当时大多数人几乎无法相信机器能够如此“智能”。研究者们在私下的交流和公开发表的论文中表达出相当乐观的情绪，认为具有完全智能的机器将在二十年内出现。ARPA（国防高等研究计划署）等政府机构向这一新兴领域投入了大笔资金。第一代AI研究者们非常乐观，曾作出了如下预言:1958年，H. A. Simon，Allen Newell：“十年之内，数字计算机将成为国际象棋世界冠军。” “十年之内，数字计算机将发现并证明一个重要的数学定理。”1965年，H. A. Simon：“二十年内，机器将能完成人能做到的一切工作。”1967年，Marvin Minsky：“一代之内……创造‘人工智能’的问题将获得实质上的解决。”1970年，Marvin Minsky：“在三到八年的时间里我们将得到一台具有人类平均智能的机器。”早期，人工智能使用传统的人工智能方法进行研究，什么是传统的人工智能研究呢？简单的讲，就是首先了解人类是如何产生智能的，然后让计算机按照人的思路去做。因此在语音识别、机器翻译等领域迟迟不能突破，人工智能研究陷入低谷。第一次AI低谷：1974 - 1980由于人工智能研究者们对项目难度评估不足，这除了导致承诺无法兑现外，还让人们当初的乐观期望遭到严重打击。到了70年代，人工智能开始遭遇批评，研究经费也被转移到那些目标明确的特定项目上。1972年康奈尔大学的教授弗雷德.贾里尼克（Fred Jelinek)被要求到IBM做语音识别。在之前各个大学和研究这个问题已经花了20多年的时间，主流的研究方法有两个特点，一个是让计算机尽可能地模拟人的发音特点和听觉特征，一个是让计算机尽可能的方法理解人所讲的完整的语句。对于前一项研究，有被称为特征提取，后一项的研究大都使用传统人工智能的方法，它基于规则和语义。贾里尼克任务，人的大脑是一个信息源，从思考到找到合适的语句，再通过发音说出来，是一个编码的过程，经过媒介传播到耳朵，是一个解码的过程。既然是一个典型的通讯问题，那就可以用解决通讯方法来解决问题，为此贾里尼克用两个数据模型（马尔科夫模型）分别描述信源和信道。然后使用大量的语音数据来训练。最后，贾里尼克团队花了4年团队，将语音识别从过去的70%提高到90%。后来人们尝试使用此方法来解决其他智能问题，但因为缺少数据，结果不太理想。在当时，由于计算机性能的瓶颈、计算复杂性的指数级增长、数据量缺失等问题，一些难题看上去好像完全找不到答案。比如像今天已经比较常见的机器视觉功能在当时就不可能找到一个足够大的数据库来支撑程序去学习，机器无法吸收足够的数据量自然也就谈不上视觉方面的智能化。项目的停滞不但让批评者有机可乘——1973年Lighthill针对英国人工智能研究状况的报告批评了人工智能在实现其“宏伟目标”上的完全失败，也影响到了项目资金的流向。人工智能遭遇了6年左右的低谷。繁荣：1980 - 1987在80年代，一类名为“专家系统”的AI程序开始为全世界的公司所采纳，而“知识处理”成为了主流AI研究的焦点。1981年，日本经济产业省拨款八亿五千万美元支持第五代计算机项目。其目标是造出能够与人对话，翻译语言，解释图像，并且像人一样推理的机器。受到日本刺激，其他国家纷纷作出响应。英国开始了耗资三亿五千万英镑的Alvey工程。美国一个企业协会组织了MCC（Microelectronics and Computer Technology Corporation，微电子与计算机技术集团），向AI和信息技术的大规模项目提供资助。DARPA也行动起来，组织了战略计算促进会（Strategic Computing Initiative），其1988年向AI的投资是1984年的三倍。人工智能又迎来了大发展。早期的专家系统Symbolics 3640专家系统是一种程序，能够依据一组从专门知识中推演出的逻辑规则在某一特定领域回答或解决问题。最早的示例由Edward Feigenbaum和他的学生们开发。1965年起设计的Dendral能够根据分光计读数分辨混合物。1972年设计的MYCIN能够诊断血液传染病。它们展示了这一方法的威力。专家系统仅限于一个很小的知识领域，从而避免了常识问题；其简单的设计又使它能够较为容易地编程实现或修改。总之，实践证明了这类程序的实用性。直到现在AI才开始变得实用起来。专家系统的能力来自于它们存储的专业知识。这是70年代以来AI研究的一个新方向。Pamela McCorduck在书中写道，“不情愿的AI研究者们开始怀疑，因为它违背了科学研究中对最简化的追求。智能可能需要建立在对分门别类的大量知识的多种处理方法之上。” “70年代的教训是智能行为与知识处理关系非常密切。有时还需要在特定任务领域非常细致的知识。”知识库系统和知识工程成为了80年代AI研究的主要方向。1982年，物理学家John Hopfield证明一种新型的神经网络（现被称为“Hopfield网络”）能够用一种全新的方式学习和处理信息。大约在同时（早于Paul Werbos），David Rumelhart推广了反向传播算法，一种神经网络训练方法。这些发现使1970年以来一直遭人遗弃的联结主义重获新生。第二次AI低谷：1987 - 1993“AI之冬”一词由经历过1974年经费削减的研究者们创造出来。他们注意到了对专家系统的狂热追捧，预计不久后人们将转向失望。事实被他们不幸言中：从80年代末到90年代初，AI遭遇了一系列财政问题。变天的最早征兆是1987年AI硬件市场需求的突然下跌。Apple和IBM生产的台式机性能不断提升，到1987年时其性能已经超过了Symbolics和其他厂家生产的昂贵的Lisp机。老产品失去了存在的理由：一夜之间这个价值五亿美元的产业土崩瓦解。XCON等最初大获成功的专家系统维护费用居高不下。它们难以升级，难以使用，脆弱（当输入异常时会出现莫名其妙的错误），成了以前已经暴露的各种各样的问题的牺牲品。专家系统的实用性仅仅局限于某些特定情景。到了80年代晚期，战略计算促进会大幅削减对AI的资助。DARPA的新任领导认为AI并非“下一个浪潮”，拨款将倾向于那些看起来更容易出成果的项目。1991年人们发现十年前日本人宏伟的“第五代工程”并没有实现。事实上其中一些目标，比如“与人展开交谈”，直到2010年也没有实现。与其他AI项目一样，期望比真正可能实现的要高得多。走在正确的路上：1993 - 2005现已年过半百的AI终于实现了它最初的一些目标。它已被成功地用在技术产业中，不过有时是在幕后。这些成就有的归功于计算机性能的提升，有的则是在高尚的科学责任感驱使下对特定的课题不断追求而获得的。不过，至少在商业领域里AI的声誉已经不如往昔了。“实现人类水平的智能”这一最初的梦想曾在60年代令全世界的想象力为之着迷，其失败的原因至今仍众说纷纭。各种因素的合力将AI拆分为各自为战的几个子领域，有时候它们甚至会用新名词来掩饰“人工智能”这块被玷污的金字招牌。AI比以往的任何时候都更加谨慎，却也更加成功。第一次让全世界感到计算机智能水平有了质的飞跃实在1966年，IBM的超级计算机深蓝大战人类国际象棋冠军卡斯伯罗夫，卡斯伯罗夫是世界上最富传奇色彩的国际象棋世界冠军，这次比赛最后以4：2比分战胜了深蓝。对于这次比赛媒体认为深蓝虽然输了比赛，但这毕竟是国际象棋上计算机第一次战胜世界冠军两局。时隔一年后，改进后的深蓝卷土重来，以3.5：2.5的比分战胜了斯伯罗夫。自从1997年以后，计算机下棋的本领越来越高，进步超过人的想象。到了现在，棋类游戏中计算机已经可以完败任何人类。深蓝实际上收集了世界上百位国际大师的对弈棋谱，供计算机学习。这样一来，深蓝其实看到了名家们在各种局面下的走法。当然深蓝也会考虑卡斯伯罗夫可能采用的走法，对不同的状态给出可能性评估，然后根据对方下一步走法对盘面的影响，核实这些可能性的估计，找到一个最有利自己的状态，并走出这步棋。因此深蓝团队其实把一个机器智能问题变成了一个大数据和大量计算的问题。IBM“深蓝”战胜国际象棋世界冠军越来越多的AI研究者们开始开发和使用复杂的数学工具。人们广泛地认识到，许多AI需要解决的问题已经成为数学，经济学和运筹学领域的研究课题。数学语言的共享不仅使AI可以与其他学科展开更高层次的合作，而且使研究结果更易于评估和证明。AI已成为一门更严格的科学分支。Judea Pearl发表于1988年的名著将概率论和决策理论引入AI。现已投入应用的新工具包括贝叶斯网络，隐马尔可夫模型，信息论，随机模型和经典优化理论。针对神经网络和进化算法等“计算智能”范式的精确数学描述也被发展出来。大数据：2005 - 现在从某种意义上讲，2005年是大数据元年，虽然大部分人感受不到数据带来的变化，但是一项科研成果却让全世界从事机器翻译的人感到震惊，那就是之前在机器翻译领域从来没有技术积累、不为人所知的Google，以巨大的优势打败了全世界所有机器翻译研究团队，一跃成为这个领域的领头羊。就是Google花重金请到了当时世界上水平最高的机器翻译专家弗朗兹·奥科 (Franz Och)博士。奥科用了上万倍的数据来训练系统。量变的积累就导致了质变的发生。奥科能训练出一个六元模型，而当时大部分研究团队的数据量只够训练三元模型。简单地讲，一个 好的三元模型可以准确地构造英语句子中的短语和简单的句子成分之间的搭配，而六元模型则可以构造整个从句和复杂的句子成分之间的搭配，相当于将这些片段从一种语言到另一种语言直接对译过去了。不难想象，如果一个系统对大部分句子在很长的片段上直译，那么其准确性相比那些在词组单元做翻译的系统要准确得多。如今在很多与“智能”有关的研究领域，比如图像识别和自然语言理解，如果所采用的方法无法利用数据量的优势，会被认为是落伍的。数据驱动方法从20世纪70年代开始起步，在八九十年代得到缓慢但稳步的发展。进入21世纪后，由于互联网的出现，使得可用的数据量剧增，数据驱动方法的优势越来越明显，最终完成了从量变到质变的飞跃。如今很多需要类似人类智能才能做的事情，计算机已经可以胜任了，这得益于数据量的增加。全世界各个领域数据不断向外扩展，渐渐形成了另外一个特点，那就是很多数据开始出现交叉，各个维度的数据从点和线渐渐连成了网，或者说，数据之间的关联性极大地增强，在这样的背景下，就出现了大数据。大数据是一种思维方式的改变。现在的相比过去大了很多，量变带来了质变，思维方式、做事情的方法就应该和以往有所不同。这其实是帮助我们理解大数据概念的一把钥匙。在有大数据之前，计算机并不擅长解决需要人类智能来解决的问题，但是今天这些问题换个思路就可以解决了，其核心就是变智能问题为数据问题。由此，全世界开始了新的一轮技术革命——智能革命。建议购买正版书籍，如需试读电子版本，请在公众号回复：”智能时代“参考：人工智能史智能时代作者：纯洁的微笑出处：http://www.ityouknow.com/版权归作者所有，转载请注明出处"}
{"content2":"AForge.NET是一个专门为开发者和研究者基于C#框架设计的，他包括计算机视觉与人工智能，图像处理，神经网络，遗传算法，机器学习，模糊系统，机器人控制等领域。这个框架由一系列的类库组成。主要包括有：AForge.Imaging —— 日常的图像处理和过滤器AForge.Vision —— 计算机视觉应用类库AForge.Neuro —— 神经网络计算库AForge.Genetic -进化算法编程库AForge.MachineLearning —— 机器学习类库AForge.Robotics —— 提供一些机器学习的工具类库AForge.Video —— 一系列的视频处理类库AForge.Fuzzy —— 模糊推理系统类库AForge.Controls—— 图像，三维，图表显示控件以下是部分方向的使用1.基于符号识别的3D现实增强技术2.基于模糊系统的自动导航3.运动检测4.2D增强技术5.计算机视觉与人工智能6.模拟识别7.神经网络8.图像处理9.遗传算法10.机器学习11.机器人控制等等还有GRATF 符号识别和目标追踪的库，可以用于机器人控制，当然也可以用于现实增强。Image Processing Lab基于C#的图像处理库，提供了一系列可用于AForge，Net的接口和工具。AForge.Net 是C#的一个图像计算机视觉库，该库是一个开源项目，提供很多图像的处理，和视频处理功能http://www.aforgenet.com/Aforge.Net子项目有个AForge.Video.VFW提供了对Avi文件的操作，AForge后面加入了子项目 AForge.Video.FFMPEG 通过FFmpeg库，提供了对大量视频格式的支持，我们都知道，FFmpeg是一个非常强大的视频处理类库，同样也是开源的，不过 AForge.Video.FFMPEG 还处于实验阶段，目标是用 FFmpeg 取代 AForge.Video.VFW 提供一个更好的对视频文件操作的库，但是该库目前只提供了对视频数据的读写，不支持对音频文件的读写，可能以后会支持，在使用的 AForge.Video.FFMpeg 时，添加对 AForge.Video.FFMPEG.dll, AForge.Video.dll和 AForge.dll 三个 dll 的引用。AForge.Video.FFMpeg命名空间下提供了三个类 VideoFileReader， VideoFileWriter， VideoFileSourcedemo：http://files.cnblogs.com/files/nidongde/OperateCamera.rar"}
{"content2":"作机器视觉和图像处理方面的研究工作，最重要的两个问题：其一是要把握住国际上最前沿的内容；其二是所作工作要具备很高的实用背景。解决第一个问题的办法就是找出这个方向公认最高成就的几个超级专家(看看他们都在作什么)和最权威的出版物(阅读上面最新的文献)，解决第二个问题的办法是你最好能够找到一个实际应用的项目，边做边写文章。 做好这几点的途径之一就是利用网络资源，利用权威网站和专家们的个人主页。依照下面目录整理：[1]研究群体(国际国内) [2]专家主页 [3]前沿国际国内期刊与会议 [4]搜索资源 [5]GPL软件资源研究群体用来搜索国际知名计算机视觉研究组(CV Groups)：国际计算机视觉研究组清单http://peipa.essex.ac.uk/info/groups.html这是卡奈基梅隆大学的计算机视觉研究组的主页，上面提供很全的资料，从发表文章的下载到演示程序、测试图像、常用链接、相关软硬件，甚至还有一个搜索引擎。著名的有人物Tomasi， Kanade等:http://www-2.cs.cmu.edu/~cil/vision.html或 http://www.cs.cmu.edu/~cil/vision.html卡内基梅隆大学双目实验室http://vision.middlebury.edu/stereo/卡内基梅隆研究组http://www.cs.cmu.edu/~cil/v-groups.htmlhttp://www.via.cornell.edu/康奈尔大学的计算机视觉和图像分析研究组，好像是电子和计算机工程系的。侧重医学方面的研究，但是在上面有相当不错资源，关键是它正在建设中，能够跟踪一些信息。Cornell University——Robotics and Vision grouphttp://www-cs-students.stanford.edu/ 斯坦福大学计算机系主页1. http://white.stanford.edu/2. http://vision.stanford.edu/3. http://ai.stanford.edu/美国斯坦福大学人工智能机器人实验室The Stanford AI Lab (SAIL) is the intellectual home for researchers in the Stanford Computer Science Department whose primary research focus is Artificial Intelligence. The lab is located in the Gates...Vision and Imaging Science and Technologyhttp://www.fmrib.ox.ac.uk/analysis/主要研究：Brain Extraction Tool， Nonlinear noise reduction， Linear Image Registration， Automated Segmentation， Structural brain change analysis， motion correction， etc.http://www.cse.msu.edu/prip/—密歇根州立大学计算机和电子工程系的模式识别--图像处理研究组，它的FTP上有许多的文章(NEW)。美国密歇根州大学认知模型和图像处理实验室The Pattern Recognition and Image Processing (PRIP) Lab faculty and students investigate the use of machines to recognize patterns or objects. Methods are developed to sense objects， to discover which...http://www.cse.msu.edu/rgroups/prip/http://pandora.inf.uni-jena.de/p/e/index.html德国的一个数字图像处理研究小组，在其上面能找到一些不错的链接资源。柏林大学 http://www.cv.tu-berlin.de/德国波恩大学视觉和认识模型小组Computer Vision Group located within the Division III of the Computer Science Department in the University of Bonn in Germany. This server offers information on topics concerning our computer vision http://www-dbv.informatik.uni-bonn.de/http://www-staff.it.uts.edu.au/~sean/CVCC.dir/home.htmlCVIP(used to be CVCC for Computer Vision and Cluster Computing) is a research group focusing on cluster-based computer vision within the Spiral Architecture.http://cfia.gmu.edu/The mission of the Center for Image Analysis is to foster multi-disciplinary research in image， multimedia and related technologies by establishing links between academic institutes， industry and government agencies， and to transfer key technologies to help industry build next generation commercial and military imaging and multimedia systems.英国的Bristol大学的Digital Media Group在高级图形图像方面不错。主要就是涉及到场景中光线计算的问题，比如用全局光照或是各种局部光照对高动态图的处理，还有近似真实的模拟现实环境(照片级别的)，还有用几张照片来建立3D模型(人头之类的)。另外也有对古代建筑模型复原。http://www.cs.bristol.ac.uk/Research/Digitalmedia/而且根据Times全英计算机排名在第3， 也算比较顶尖的研究了http://www.cmis.csiro.au/IAP/zimage.htm这是一个侧重图像分析的站点，一般。但是提供一个Image Analysis环境---ZIMAGE and SZIMAGE。麻省理工视觉实验室MIT http://groups.csail.mit.edu/vision/welcome/AI Laboratory Computer Vision groupCenter for Biological and Computational LearningMedia Laboratory， Vision and Modeling GroupPerceptual Science groupUC Berkeley http://0-vision.berkeley.edu.ilstest.lib.neu.edu/vsp/index.htmlhttp://www.cs.berkeley.edu.ilstest.lib.neu.edu/projects/vision/vision_group.html加州大学伯克利分校视觉实验室David A. Forsyth：http://www.cs.berkeley.edu/~daf/UCLA(加州大学洛杉矶分校) http://vision.ucla.edu/  视觉实验室英国牛津的A.Zisserman：http://www.robots.ox.ac.uk/~az/ 机器人实验室美国南加州大学智能机器人和智能系统研究所University of Southern California， Los AngelesIRIS is an interdepartmental unit of USC's School of Engineering with ties to USC's Information Sciences Institute (ISI). Members include faculty， graduate students， and research staff associated with... http://iris.usc.edu/ Computer Vision 实验室美国南加州大学计算机视觉实验室介绍：Computer Vision Laboratory at the University of Southern California is one of the major centers of computer vision research for thirty years. they conduct research in a number of basic and applied are...http://iris.usc.edu/USC-Computer-Vision.html英国约克大学高级计算机结构神经网络小组The Advanced Computer Architecture Group has had a thriving research programme in neural networks for over 10 years. The 15 researchers， led by Jim Austin， focus their work in the theory and applicati...http://www.cs.york.ac.uk/arch/neural/瑞士戴尔莫尔感知人工智能研究所IDIAP is a research institute established in Martigny in the Swiss Alps since 1991. Active in the areas of multimodal interaction and multimedia information management， the institute is also the leade...http://www.idiap.ch/英国萨里大学视觉，语言和信号处理中心The Centre for Vision， Speech and Signal Processing (CVSSP) is more than 60 members strong， comprising 12 academic staff， 18 research fellows and more than 44 research students. The activities of the ...http://www.ee.surrey.ac.uk/Research/VSSP/美国阿默斯特马萨诸塞州立大学计算机视觉实验室The Computer Vision Laboratory was established in the Computer Science Department at the University of Massachusetts in 1974 with the goal of investigating the scientific principles underlying the con...http://vis-www.cs.umass.eduUniversity of Massachusetts——Computer Vision Laboratory for Perceptual Robotics美国芝加哥伊利诺伊斯大学贝克曼研究中心智能机器人和计算机视觉实验室Includes the following groups: Professor Seth Hutchinson's Research Group Professor David Kriegman's Research Group Professor Jean Ponce's Research Group Professor Narendra Ahuja's Research Gro...http://www-cvr.ai.uiuc.edu/Computer Vision and Robotics LaboratoryVision Interfaces and Systems Laboratory (VISLab)英国伯明翰大学计算机科学学校视觉研究小组The vision group at the School of Computer Science (a RAE 5 rated department) performs research into a wide variety of computer vision and image understanding areas. Much of this work is performed in ...http://www.cs.bham.ac.uk/research/vision/微软研究院机器学习与理解研究小组 / 计算机视觉小组The research group focuses on the development of more advanced and intelligent computer systems through the exploitation of statistical methods in machine learning and computer vision. The site lists ...http://research.microsoft.com/mlp/http://research.microsoft.com/en-us/groups/vision/微软公司的文献：http://research.microsoft.com/research/pubs微软亚洲研究院：http://research.microsoft.com/asia/，值得关注Harry Shum， Jian Sun， Steven Lin， Long Quan(兼职HKUST)etc.瑞典隆德大学数学系视觉组：http://www.maths.lth.se/matematiklth/personal/andersp/感觉国外搞视觉的好多是数学系出身，大约做计算机视觉对数学要求很高吧。澳大利亚国立大学：http://users.rsise.anu.edu.au/~hartley/美国北卡大学：http://www.cs.unc.edu/~marc/法国INRIA：http://www-sop.inria.fr/odyssee/team/ 由Olivier.Faugeras领衔的牛人众多。比利时鲁汶大学的L.Van Gool： www.esat.kuleuven.ac.be/psi/visics/据说在这个只有中国一个小镇大小的地方的鲁汶大学在欧洲排行top10，名列世界top100，还出了几个诺贝尔奖，视觉研究也很强.美国明德http://vision.middlebury.edu/stereo/Computational Vision Group http://www.cs.yale.edu/School of Medicine， Image Processing and Analysis group国内：1. 中科院模式识别国家重点实验室 http://www.nlpr.ia.ac.cn/English/rv/mainpage.html虹膜识别、掌纹识别、人脸识别、2.  莲花山http://www.stat.ucla.edu/~sczhu/Lotus/3.  天津大学精密测试技术及仪器国家重点实验室4.  研究方向包括：激光及光电测试技术、传感及测量信息技术、微纳测试与制造技术、制造质量控制技术。该实验室是国内精密测试领域惟一的国家重点实验室。“智能微系统及其集成应用技术”、“微结构光学测试技术”、“油气储运安全检测技术”、“先进制造中的视觉测量及其关键技术”、“正交偏振激光器原理、特性及其在精密计量中的应用研究”等5项代表性成果（07.3）。5.  中科院长春光机所 http://www.ciomp.ac.cn/ny/keyan.asp6.  中科院沈阳自动化所http://www.sia.ac.cn/index.php7.  中科院西安光机所http://www.opt.ac.cn/yanjiushi/gpcxjs1.htm8.  北京大学智能科学系http://www.cis.pku.edu.cn/vision/vision.htm专家网页http://www.ai.mit.edu/people/wtf/这位可是MIT人工智能实验室的BILL FREEMAN。专长是：理解--贝叶斯模型。http://www.merl.com/people/brand/MERL(Mitsubishi Electric Research Laboratory)中的擅长“Style Machine”。http://research.microsoft.com/~ablake/CV界极有声望的A.Blake 1977年毕业于剑桥大学三一学院并或数学与电子科学学士学位。之后在MIT，Edinburgh，Oxford先后组建过研究小组并成为Oxford的教授，直到1999年进入微软剑桥研究中心。主要工作领域是计算机视觉。http://www-2.cs.cmu.edu/afs/cs.cmu.edu/user/har/Web/home.html这位专家好像正在学习汉语，主页并且搜集了诸如“两只老虎(Two Tigers)”的歌曲。他的主页上面还有几个专家：Shumeet Baluja， Takeo Kanade。他们的Face Detection作的绝对是世界一流。毕业于卡奈基梅隆大学的计算机科学系，兴趣是计算机视觉。http://www.ifp.uiuc.edu/yrui_ifp_home/html/huang_frame.html这位在1963年就获得了MIT的博士学位！他领导的Image Lab比较出名的是指纹识别。Zucker Computational Vision Group http://cs-www.cs.yale.edu/homes/vision/zucker/zpublications.htmlFinn Lindgren(Sweden):Statistical image analysis http://www.maths.lth.se/matstat/staff/finn/Pavel Paclik(Prague):statistical pattern recognition http://www.ph.tn.tudelft.nl/~pavel/Dr. Mark Burge:machine learning and graph theory http://cs.armstrong.edu/burge/yalin Wang:Document Image Analysis http://students.washington.edu/~ylwang/Geir Storvik: Image analysis http://www.math.uio.no/~geirs/Heidorn http://alexia.lis.uiuc.edu/~heidorn/Joakim Lindblad:Digital Image Cytometry http://www.cb.uu.se/~joakim/index_eng.htmlS.Lavirotte: http://www-sop.inria.fr/cafe/Stephane.Lavirotte/Sporring:scale-space techniques http://www.lab3d.odont.ku.dk/~sporring/Mark Jenkinson:Reduction of MR Artefacts http://www.fmrib.ox.ac.uk/~mark/Justin K. Romberg:digital signal processing http://www-dsp.rice.edu/~jrom/Fauqueur:Image retrieval by regions of interest http://www-rocq.inria.fr/~fauqueur/James J. Nolan:Computer Vision http://cs.gmu.edu/~jnolan/Daniel X. Pape:Information http://www.bucho.org/~dpape/Drew Pilant:remote sensing technology http://www.geo.mtu.edu/~anpilant/index.html张正友 http://research.microsoft.com/en-us/um/people/zhang/师承关系paper毕竟是死的， 写paper的人才是活的. 那么我现在研究一下cv圈的格局， 按师承关系，借鉴前人， 我总结a tree stucture of cv guys：David Marr----->Shimon Ullman (Weizmann)----->Eric Grimson (MIT)----->Daniel Huttenlocher (Cornell)----->Pedro Felzenszwalb (Chicago)Thomas Binford (Stanford)----->David Lowe (UBC)----->Jitendra Malik (UC Berkeley)----->Pietro Perona (Caltech)----->Stefano Soatto (UCLA)----->Fei-Fei Li (Princeton)----->Jianbo Shi (UPenn)----->Yizhou Yu (UIUC)----->Christoph Bregler (NYU)----->Serge Belongie (UCSD)----->Alyosha Efros (CMU)Andrew Blake (Microsoft Research Cambridge)----->Andrew Zisserman (Oxford)----->Andrew Fitzgibbon (Microsoft Research Cambridge)----->Roberto Cipolla (Cambridge)----->Alan Yuille (UCLA)(UK这个学派的师承关系不太清楚， 这是我听说加上自己猜测的. 事实上， 几个非常优秀的researcher如Vladimir Kolmogorov虽然不是Andrew Blake的学生， 但是也属于这个学派. )Thomas Huang (UIUC)----->Yong Rui (Microsoft Research Redmond)----->Nebojsa Jojic (Microsoft Research Redmond)----->Ying Wu (Northwestern University)----->Hai Tao (UCSC)----->Yuncai Liu (SJTU)(Huang这个系的人太多， 而且很怪的是， UIUC的web上信息不全， 在此仅列出我知道的.)此外， 还有Takeo Kanade等非常有名的大牛， 囿于篇幅， 不一一列举. 从上得知， 加州派基本占了cv的半壁江山.近几年活跃的CV GuysUSAJitendra Malik， UC BerkeleyPietro Perona， CaltechSerge Belongie， UCSDJianbo Shi， UPennStefano Soatto， UCLAFei-Fei Li， PrincetonWilliam Freeman， MITTrevor Darrell， MITSimon Baker， CMUYanxi Liu， CMUSongchun Zhu， UCLAAlan Yuille， UCLAYi Ma， UIUCMichael Black， BrownCarlo Tomasi， DukeRamin Zabih， CornellShree Nayar， ColumbiaRama Chellappa， MarylandSteve Seitz， University of WashingtonEuropeAndrew Zisserman， Oxford， UKAndrew Fitzgibbon， Microsoft Research Cambridge， UKRoberto Cipolla， Cambridge， UKJean Ponce， INRIA， FranceCordelia Schmid， INRIA， FranceBill Triggs， LEAR， FranceYair Weiss， Hebrew University， IsraelAnat Levin， Hebrew University， IsraelMichal Irani， Weizmann， IsraelLuc van Gool， University of Leuven/ETH Zurich， CzechicChinaHarry Shum， MSRAXiaoou Tang， MSRA/CUHKJian Sun， MSRASteve Lin， MSRAYasuyuki Matsushita， MSRAZhouchen Lin， MSRALong Quan， HKUST 香港科技Chi-Keung Tang， HKUST按研究方向分分， 应该更合理一些。现在计算机视觉， 计算机图形图像， 机器学习开始融合到一起了吧。J. Malik，Zhu Songchu偏segmentation;D. Lowe， S. Ullman， Poggio 偏于从生物视觉的启发来研究视觉；Zisserman， Schmid， Lowe研究局部特征；Luc Van Goo， Long Quanl三维重建；Perona， Li Feife， Freeman视觉i学习， 物体分类；还有运动分析， 视觉跟踪，纹理分析.............MIT的Brain & Cognitive Science Dept和CSAIL里面聚集了一帮人，有的作low level有的作mid level to high level的。他们的工作是值得关注的。当然说视觉还是要从伟大的David Marr开始。Tomaso Poggio， Richard Whitman是Marr的同事，传承了其理念，一直往下走。Poggio最近几年比较重点的工作放在他那个hierarchical model上。T. Poggio的第一个PhD学生是Christof Koch （kLab at Caltech）。哦，顺便说一下Koch的另外一个导师是Valentino Breitenberg——同样是影响了一个时代的大人物。Koch研究重点兴趣在consciousness上，在Nature上的很多文章体现了他的研究思想。不过他们也做不少初级的视觉问题，诸如attention。Koch比较知名的弟子比如Laurent Itti和Li Feifei。Richard Whitman 年纪比较大了，个人不是很关注他现在做的东西。不过他所在的Perceptual Science Group，是一个非常有影响力的地方，这个组其他大家比较熟悉的老师有Aude Oliva和EH Adelson。Adelson最著名的一个事儿是色彩恒常相关的视错觉，93年发在Science上的那篇。关于Oliva，前面的帖子错了，她不是Poggio的学生，这家伙和Torralba是老乡，同在法国念书，主要从心理学那条路子开始做，成名之役是hybrid image，和Antonio Torrralba一起搞的。这个Hybrid Image 其实80年代就有了，但是最开始从心理学方向上探讨，没有非常有影响力的文章。后来开始靠谱作自然图像统计，得到Gist theory，当然这个illusion本身后来转投SIGGRAPH，其影响是深远的。嗯，这个和CV关系不大。Perceptual Science Group出了不少牛人，他们的alumni list可谓超豪华阵容：Yair Weiss， Josh Tenenbaum， Pawan Sinha， Bill Freeman……前沿国际国内期刊与会议这里的期刊大部分都可以通过上面的专家们的主页间接找到1.国际会议 2.国际期刊 3.国内期刊 4.神经网络 5.CV 6.数字图象 7.教育资源，大学 8.常见问题国际会议现在，国际上计算机视觉方面的三大国际会议是ICCV， CVPR和ECCV，统称之为ICE。ICCV的全称是International Comference on Computer Vision。ICCV两年一次，与ECCV正好错开，是公认的三个会议中级别最高的。ECCV的全称是Europeon Conference on Computer Vision，是一个欧洲的会议。CVPR的全称是Internaltional Conference on Computer Vision and Pattern Recogintion国际计算机视觉与模式识别会议。这是一个一年一次的会议，举办地在美国。ICIP的全称是International Conference on Image ProcessingBMVC的全称是British Machine Vision ConferenceICPR的全称是International Conference on Pattern Recognition国际期刊以计算机视觉为主要内容之一的国际刊物也有很多，如:International Journal of Computer VisionIEEE Trans. On PAMI http://www.computer.org/tpami/index.htmIEEE Transactionson Image Processing http://www.ieee.org/organizations/pubs/transactions/tip.htmPattern Recognition http://www.elsevier.com/locate/issn/00313203Pattern Recognition Letters http://www.elsevier.com/locate/issn/01678655IEEE Trans. on Robotics and Automation，IEEE TPAMIIEEE TIPCVGIP Computer Vision. Graphics and Image Processing，Visual Image Computing，IJPRAI(International Journal of Pattern Recognition and Artificial Intelligence)众所周知，computer vision(cv) 存在ICCV/CVPR/ECCV三个顶级会议， 它们档次差不多，都应该在一流会议行列， 没有必要给个高下。 有些us的人认为ICCV/CVPR略好于ECCV，而欧洲人大都认为ICCV/ECCV略好于CVPR， 某些英国的人甚至认为BMVC好于CVPR。简言之， 三个会议差不多， 各有侧重和偏好。笔者就个人经验浅谈三会异同， 以供大家参考和讨论。 三者乃cv领域的旗舰和风向标，其oral paper (包括best paper) 代表当年度cv的最高水准， 在此引用Harry Shum的一句话， 想知道某个领域在做些什么， 找最近几年此领域的proceeding看看就知道了。 ICCV/CVPR由IEEE Computer Society牵头组织， ECCV好像没有专门负责的组织。 CVPR每年(除2002年)都在美国开， ECCV每两年开一次，仅限欧洲， ICCV也是每两年一次， 各洲轮值。 基本可以保证每年有两个会议开， 这样研究者就有两次跻身牛会的机会。就录取率而言， 三会都有波动。 如ICCV2001录取率>30%， 且出现两个人(华人)各有三篇第一作者的paper的情况， 这在顶级牛会是不常见的 (灌水嫌疑)。 但是，ICCV2003， 2005两次录取率都很低， 大约20%左右。 ECCV也是类似规律， 在2004年以前都是>30%， 2006年降低到20%左右。 CVPR的录取率近年来一直偏高，从2004年开始一直都在[25%，30%]。最近一次CVPR2006是28.1%， CVPR2007还不知道统计数据。 笔者猜测为了维持录取paper的绝对数量， 当submission少的时候录取率偏高， 反之偏低， 近几年三大会议的投稿数量全部超过1000， 相对2000年前， 三会录取率均大幅度降低， 最大幅度50%->20%。 对录取率走势感兴趣的朋友，可参考 http://vrlab.epfl.ch/~ulicny/statistics/(CVPR2004的数据是错的)，http://www.adaptivebox.net/research/bookmark/CICON_stat.html.显然，投入cv的人越来越多，这个领域也是越来越大， 这点颇不似machine learning一直奉行愚蠢的小圈子主义。另外一点值得注意， ICCV/ECCV只收vision相关的topic，而cvpr会收少量的pattern recognition paper， 如finger print等， 但是不收和image/video完全不占边的pr paper，如speech recognition等。 我一个朋友曾经review过一篇投往CVPR的speech的paper， 三个reviewer一致拒绝， 其中一个reviewer搞笑的指出， 你这篇paper应该是投ICASSP被据而转投CVPR的。 就topic而言， CVPR涵盖最广。 还有一个没有验证过的原因导致CVPR录取率高: 很多us的researcher不愿意或没有足够的经费到us以外的地方开会， 故CVPR会优先接收很多来自us的paper (让大家都happy)。以上对三会的分析对我们投paper是很有指导作用的。 目前的research我想绝大部分还是纸上谈兵， 必经 read paper -> write paper -> publish paper -> publish paper on top conferences and journals流程。 故了解投paper的一些基本技巧， 掌握领域的走向和热点， 是非常必要的。 避免做无用功，选择切合的topic， 改善presentation， 注意格式 (遵守规定的模板)， 我想这是很多新手需要注意的问题。 如ICCV2007明文规定不写summary page直接reject， 但是仍然有人忽视， 这是相当不值得的。国内期刊自动化学报、计算机学报、软件学报、电子学报，中国图象图形学报，模式识别与人工智能，光电子激光，精密光学工程等。神经网络神经网络-Neural Networks Tutorial Reviewhttp://hem.hj.se/~de96klda/NeuralNetworks.htmftp://ftp.sas.com/pub/neural/FAQ.htmlImage Compression with Neural Networkshttp://www.comp.glam.ac.uk/digimaging/neural.htmBackpropagator's Reviewhttp://www.dontveter.com/bpr/bpr.htmlBibliographies on Neural Networkshttp://liinwww.ira.uka.de/bibliography/Neural/Intelligent Motion Control with an Artificial Cerebellumhttp://www.q12.org/phd.htmlKernel Machineshttp://www.kernel-machines.org/Some Neural Networks Research Organizationshttp://www.ieee.org/nnc/http://www.inns.org/Neural Network Modeling in Vision Researchhttp://www.rybak-et-al.net/nisms.htmlNeural Networks and Machine Learninghttp://learning.cs.toronto.edu/Neural Application Softwarehttp://attrasoft.comNeural Network Toolbox for MATLABhttp://www.mathworks.com/products/neuralnet/Netlab Softwarehttp://www.ncrg.aston.ac.uk/netlab/Kunama Systems Limited http://www.kunama.co.uk/Computer Vision(计算机视觉)Annotated Computer Vision Bibliographyhttp://iris.usc.edu/Vision-Notes/bibliography/contents.htmlhttp://iris.usc.edu/Vision-Notes/rosenfeld/contents.htmlLawrence Berkeley National Lab Computer Vision and Robotics Applicationshttp://www-itg.lbl.gov/ITG.hm.pg.docs/VISIon/vision.htmlCVonline by University of EdinburghThe Evolving， Distributed， Non-Proprietary， On-Line Compendium of Computer Vision， www.dai.ed.ac.uk/CVonlineComputer Vision Handbook，www.cs.hmc.edu/~fleck/computer-vision-handbookVision Systems Coursewarewww.cs.cf.ac.uk/Dave/Vision_lecture/Vision_lecture_caller.htmlResearch Activities in Computer Visionhttp://www-syntim.inria.fr/syntim/analyse/index-eng.htmlVision Systems Acronymswww.vision-systems-design.com/vsd/archive/acronyms.htmlDictionary of Terms in Human and Animal Visionhttp://cns-web.bu.edu/pub/laliden/WWW/Visionary/Visionary.htmlMetrology based on Computer Visionwww.cranfield.ac.uk/sme/amac/research/metrology/metrology.html6.Digital Photography 数字图像Digital Photography， Scanning， and Image Processingwww.dbusch.com/scanners/scanners.htm l7.Educational Resources， Universities 教育资源，大学Center for Image Processing in Educationwww.cipe.comLibrary of Congress Call Numbers Related to Imaging Science by Rochester Institute of Technologyhttp://wally2.rit.edu/pubs/guides/imagingcall.htmlMathematical Experiences through Image Processing， University of Washingtonwww.cs.washington.edu/research/metip/metip.htmlVismod Tech Reports and Publications， MIThttp://vismod.www.media.mit.edu/cgi-bin/tr_pagemakerVision Lab PhD dissertation list， University of Antwerphttp://wcc.ruca.ua.ac.be/~visielab/theses.htmlINRIA (France) Research Projects: Human-Computer Interaction， Image Processing， Data Management， Knowledge Systemswww.inria.fr/Themes/Theme3-eng.htmlImage Processing Resourceshttp://eleceng.ukc.ac.uk/~rls3/Contents.htmPublications of Carsten Stegerhttp://www9.informatik.tu-muenchen.de/people/steger/publications.html8.FAQs（常见问题）comp.dsp FAQwww.bdti.com/faq/dsp_faq.htmRobotics FAQwww.frc.ri.cmu.edu/robotics-faqWhere's the sci.image.processing FAQ?www.cc.iastate.edu/olc_answers/packages/graphics/sci.image.processing.faq.htmlcomp.graphics.algorithms FAQ， Section 3， 2D Image/Pixel Computationswww.exaflop.org/docs/cgafaqAstronomical Image Processing System FAQwww.cv.nrao.edu/aips/aips_faq.html搜索资源http://sal.kachinatech.com/http://cheminfo.pku.edu.cn/mirrors/SAL/index.shtml 北京大学Google输入：computer vision 或computer vision groups可以获得很多结果网络资源：CVonline http://homepages.inf.ed.ac.uk/rbf/CVonline/视觉研究组列表Computer vision test Image http://www.cs.cmu.edu/~cil/v-images.html卡内基梅隆标准图片库视觉论文搜索：Paper searchhttp://www.researchindex.com图像处理GPL库（代码库图像库等）http://www.ph.tn.tudelft.nl/~klamer/cppima.htmlCppima 是一个图像处理的C++函数库。这里有一个较全面介绍它的库函数的文档，当然你也可以下载压缩的GZIP包，里面包含TexInfo格式的文档。http://iraf.noao.edu/Welcome to the IRAF Homepage! IRAF is the Image Reduction and Analysis Facility， a general purpose software system for the reduction and analysis of astronomical datahttp://entropy.brni-jhu.org/tnimage.html一个非常不错的Unix系统的图像处理工具，看看它的截图。你可以在此基础上构建自己的专用图像处理工具包。http://sourceforge.net/projects/这是GPL软件集散地，可以搜索IP库。国内的CSDN http://www.csdn.net/"}
{"content2":"计算机视觉1. 嘉宾：商汤科技CEO 徐立文章回顾：计算机视觉的完整链条，从成像到早期视觉再到识别理解2. 嘉宾：格灵深瞳CTO 赵勇文章回顾：计算机视觉在安防、交通、机器人、无人车等领域的应用3. 嘉宾：上交大教授 马利庄文章回顾：可视媒体大数据的智能处理技术与应用4. 嘉宾：阿里资深总监 华先胜文章回顾：图像搜索的前世今生5. 嘉宾：杨安国 DeepSee CEO主题：如何让飘在半空的计算机视觉技术与需求落地？PDF：http://pan.baidu.com/s/1bptDykZ视频：https://pan.baidu.com/s/1gfpD9Mb视频播放工具下载：https://pan.baidu.com/s/1skZenLj6. 嘉宾：张益肇 微软亚洲研究院副院长主题：计算机视觉在医疗影像处理领域的应用和前景PDF：http://pan.baidu.com/s/1i5O9o7j视频：https://pan.baidu.com/s/1jIcGpue视频播放工具下载：https://pan.baidu.com/s/1skZenLj7. 嘉宾：王乃岩 图森未来首席科学家主题：自动驾驶中的视觉技术视频：http://pan.baidu.com/s/1hst3kR6PPT：http://pan.baidu.com/s/1nvnnxd38. 嘉宾：Shrek Wu 联想资深系统软件架构师主题：解密世界上第一款基于Project Tango的手机——联想Phab 2 Pro背后的秘密PDF：http://pan.baidu.com/s/1cBXmGI9.嘉宾：Matt Scott 码隆科技CTO主题：Deep image retrieval and application视频：http://pan.baidu.com/s/1c2Ny2ukPDF：https://pan.baidu.com/s/1qYU2FrI10.嘉宾：邬刚 加速云CEO主题：FPGA在深度学习中的应用视频：https://pan.baidu.com/s/1gfyXpsRPDF：https://pan.baidu.com/s/1eSwXy4E11.嘉宾：谭平 360人工智能研究院副院长主题：从SfM、SLAM到自主机器人视频：http://pan.baidu.com/s/1sl5zqUTPDF：http://pan.baidu.com/s/1nvbfs3f12.嘉宾：周博磊 MIT在读博士主题：理解和利用CNN的内部表征视频：https://pan.baidu.com/s/1dF3MWvfPDF：https://pan.baidu.com/s/1jI8w5dc13.嘉宾：吴佳俊 MIT在读博士主题：生成和识别三维物体视频：https://pan.baidu.com/s/1slCtOpJPDF：https://jiajunwu.com/jiajunwu_3d_jiangmen.pdf14.嘉宾：薛天帆 MIT在读博士主题：运动辅助视频处理和合成视频：https://pan.baidu.com/s/1o84YtC2PDF：https://pan.baidu.com/s/1mh6K9nU15.嘉宾：任少卿 Momenta研发总监主题：From Faster R-CNN to Mask R-CNN视频：http://pan.baidu.com/s/1o8wO13GPDF：http://pan.baidu.com/s/1c10q0xE16.嘉宾：朱俊彦 Berkeley AI研究实验室博士生主题：Learning to recreate visual world视频：https://pan.baidu.com/s/1miG7MB2PDF：https://pan.baidu.com/s/1slpXz6917.嘉宾：刘洋 微软亚洲研究院网络图形组主管研究员主题：智能几何的建模与处理视频：http://pan.baidu.com/s/1i5OfmbbPDF：http://pan.baidu.com/s/1sl2kiBF18.嘉宾：董悦 微软亚洲研究院网络图形组主管研究员主题：利用自增强学习训练神经网络生成材质纹理视频：https://pan.baidu.com/s/1cJAd4qPDF：https://pan.baidu.com/s/1kVyYUzH19. 嘉宾：侯晓迪 图森未来联合创始人、CTO主题：自动驾驶中我们需要什么样的评测数据集？PPT：https://pan.baidu.com/s/1pLVuLOv视频：(上) https://pan.baidu.com/s/1i4NPrBv(下) https://pan.baidu.com/s/1dEUcWyl20.嘉宾：袁路 微软亚洲研究院视觉计算组主管研究员主题：深度学习如何让图片和视频在不同的艺术风格间自如切换？视频：https://pan.baidu.com/s/1dFszHc9PDF：https://pan.baidu.com/s/1c19g1qC21.嘉宾：曹哲 Facebook人工智能研究所计算机视觉实习生主题：实时多人人体姿态识别视频：https://pan.baidu.com/s/1mhVRhpuPDF：https://pan.baidu.com/s/1i5qqaw9自然语言处理1. 嘉宾：鲍捷 文因互联CEO主题：知识管理和语义搜索的哲学思考视频：http://pan.baidu.com/s/1jIwgWsuPDF：http://pan.baidu.com/s/1bpgMjWr2. 嘉宾：王仲远 Facebook Research Scientist主题：如何让机器像人类一样理解短文本PDF：http://pan.baidu.com/s/1pL0GBWZ视频：https://pan.baidu.com/s/1boG9S2z视频播放工具下载：https://pan.baidu.com/s/1skZenLj3. 嘉宾：刘知远 清华大学计算机系助理教授主题：表示学习与知识获取视频：http://pan.baidu.com/s/1gePJ9kvPDF：http://pan.baidu.com/s/1dE8mDk14.嘉宾：王昊奋 深圳狗尾草智能科技公司CTO主题：从通用知识图谱到行业图谱—关键技术及行业应用浅析视频：http://pan.baidu.com/s/1c2Ny2ukPDF：http://pan.baidu.com/s/1pKSR26B5.嘉宾：汪冠春 助理来也CEO主题：智能助理的对话技术应用和商业化尝试资料：https://pan.baidu.com/s/1boWEcsZ6.嘉宾：陈博兴 加拿大国家研究委员会研究员主题：机器翻译—从统计到神经网络PDF：https://pan.baidu.com/s/1mi79OU0视频：https://pan.baidu.com/s/1nvODkfR视频播放工具下载：https://pan.baidu.com/s/1skZenLj7.嘉宾：刘升平 云知声资深AI技术专家主题：语用计算与会话式交互视频：https://pan.baidu.com/s/1c1DaWTUPDF：https://pan.baidu.com/s/1i5DgVMd8.嘉宾：邱锡鹏 复旦大学副教授主题：面向自然语言处理的分布式语义表示视频：https://pan.baidu.com/s/1boUDSvlPDF：https://pan.baidu.com/s/1i5HkFK59.嘉宾：漆桂林 东南大学教授主题：知识图谱中推理技术进展及应用视频：https://pan.baidu.com/s/1gfDTCfLPDF：https://pan.baidu.com/s/1bpd4aBH10.嘉宾：李磊 今日头条科学家、头条实验室总监主题：大规模知识库上的自然语言问答视频：https://pan.baidu.com/s/1kU4IZpPPDF：https://pan.baidu.com/s/1dF9Vl6L11.嘉宾：肖仰华 复旦大学副教授主题：基于知识图谱的机器语言认知视频：https://pan.baidu.com/s/1i4AnlT7PDF：https://pan.baidu.com/s/1dFOOJxn12.嘉宾：王宝勋 三角兽科技首席科学家主题：自动聊天系统的技术路线，解决方案与挑战视频：https://pan.baidu.com/s/1eSFtxv0PDF：https://pan.baidu.com/s/1hrEZgHI13. 嘉宾：百度“度秘”事业部总经理景鲲、小鱼在家CEO宋晨枫回顾：将门高欣欣对话度秘景鲲、小鱼在家宋晨枫：中国语音交互市场将在今年爆发视频：https://maimai.cn/article/live2?uid=56764414. 嘉宾：哈工大计算机学院副教授、博士生导师 车万翔主题：句法语义分析及其应用视频：https://pan.baidu.com/s/1dFKOTepPDF：https://pan.baidu.com/s/1jIFpbIe15. 嘉宾：清华大学计算机学院副教授、博士生导师 刘洋主题：基于深度学习的机器翻译视频：https://pan.baidu.com/s/1i437sBbPDF：https://pan.baidu.com/s/1hsgiZKK16. 嘉宾：北京大学计算机科学技术研究所教授 万小军主题：文本自动摘要技术视频>>http://pan.baidu.com/s/1i5Qi5znPDF>>http://pan.baidu.com/s/1gfrGCHt17. 中科院Kaggle全球文本匹配竞赛华人第1名团队 庞亮、侯建鹏主题：深度学习与特征工程视频>>https://pan.baidu.com/s/1qYI3jCKPDF>>https://pan.baidu.com/s/1dEV01gd18. 嘉宾：出门问问NLP技术总监 徐朴旸主题：口语对话管理技术产品化的思考PDF>>https://pan.baidu.com/s/1eSEekYu视频>>（上）>>https://pan.baidu.com/s/1nvBOcbv；（下）>>https://pan.baidu.com/s/1hsACp0K机器学习1. 嘉宾：刘铁岩 微软亚洲研究院首席研究员主题：AI时代，机器学习最新技术趋势解读视频：https://pan.baidu.com/s/1pLAmQ0rPDF：https://pan.baidu.com/s/1cra7ue2. 嘉宾：秦涛 微软亚洲研究院主管研究员主题：Dual learning: a new learning paradigm视频：上>>http://pan.baidu.com/s/1sliyGFz；下>>http://pan.baidu.com/s/1b5DFUe；PDF：http://pan.baidu.com/s/1eSr8B6M3. 嘉宾：王太峰 微软亚洲研究院主管研究员主题：浅谈分布式机器学习算法和工具视频：https://v.douyu.com/show/YAox276N2m07Vz8ZPDF：https://pan.baidu.com/s/1nv79KfR4. 嘉宾：边江 微软亚洲研究院主管研究员主题：机器学习驱动下的内容分发和个性化推荐视频：https://pan.baidu.com/s/1jIx6RkyPDF：https://pan.baidu.com/s/1pL2g2hX5. 嘉宾：王倪 量化派的联合创始人兼COO主题：AI在消费金融上的道和术视频：https://pan.baidu.com/s/1nvBbo4hPDF：https://pan.baidu.com/s/1miwQlqK6. 嘉宾：李玉喜 加拿大阿尔伯塔大学博士主题：AlphaGo核心技术及应用视频：https://pan.baidu.com/s/1mi8v5O0PDF：https://pan.baidu.com/s/1jHCnih47. 嘉宾：李沐 亚马逊AWS机器学习高级应用科学家主题：Scaling Distributed Machine Learning with System and Algorithm Co-design视频：https://pan.baidu.com/s/1dE5a4drPDF：https://pan.baidu.com/s/1dFsvkY18. 嘉宾：张明瑞 Momenta视觉算法实习研究员主题：GANs Family视频：https://pan.baidu.com/s/1qYrtl16PDF：https://pan.baidu.com/s/1dFKfTtR9. 嘉宾：李嫣然 香港理工大学在读博士主题：深入浅出GAN——原理篇视频：http://pan.baidu.com/s/1pLv2FurPDF：http://pan.baidu.com/s/1bpGPOEN主题 ：深入浅出GAN——应用篇视频：http://pan.baidu.com/s/1qYUC18CPDF：http://pan.baidu.com/s/1i5LxtD710. 嘉宾：石佳欣  清华大学计算机系机器学习组博士生主题：珠算，一个贝叶斯深度学习库视频：http://pan.baidu.com/s/1hsegbfYPDF：http://pan.baidu.com/s/1nvkHx13机器人1. 嘉宾：张一茗  速感科技 CTO主题：给机器人一双慧眼——机器人视觉系统进化史PDF：http://pan.baidu.com/s/1skNZGk9视频：https://pan.baidu.com/s/1qYoFupQ视频播放工具下载：https://pan.baidu.com/s/1skZenLj2. 嘉宾：高翔 清华大学自动化导航研究所博士生主题：视觉SLAM的基础知识PDF：http://pan.baidu.com/s/1o8lMt42视频：https://pan.baidu.com/s/1i5ccuqD视频播放工具下载：https://pan.baidu.com/s/1skZenLj3. 嘉宾：张浩 蓝胖子机器人 CTO主题：Moble manipulator as general purpose robot: technology & visionPDF：http://pan.baidu.com/s/1nv0b64p视频：https://pan.baidu.com/s/1slweNrZ视频播放工具下载：https://pan.baidu.com/s/1skZenLj4. 嘉宾：何梦文 CMU计算机工程系博士生主题：RobotSDK介绍—— 一种自顶向下的模块化软件开发框架PDF：http://pan.baidu.com/s/1hrSwgLm视频：https://pan.baidu.com/s/1kVObuYj视频播放工具下载：https://pan.baidu.com/s/1skZenLj5. 嘉宾：韩峰涛 珞石科技联合创始人主题：工业机器人控制系统设计综述视频：http://pan.baidu.com/s/1geF7A8NPDF：http://pan.baidu.com/s/1kVbejjx6. 嘉宾：龚超慧&李路 CMU机器人学院博士后主题：AGV—从过去到未来PDF：http://pan.baidu.com/s/1mihOfjE7. 嘉宾：万伟伟 日本经济产业省产业综合技术研究所智能系统研究部研究员主题：智能机器人制造与下一代制造PDF：http://pan.baidu.com/s/1hsFheWO8. 嘉宾：李淼 库柏特科技CEO主题：机器人抓取的过去、现在和未来PPT：http://pan.baidu.com/s/1boWCBDH9.嘉宾：冯思远 丰田研究院主题：基于分层优化的人形机器人控制视频：https://pan.baidu.com/s/1o7Zkrz0（因为冯博声音太好听了，导致沉醉了，前几分钟没录上==）PPT：http://pan.baidu.com/s/1gfp34zD10.嘉宾：邵天兰 梅卡曼德机器人创始人主题：ROS简介与重要模块源码导读视频：http://pan.baidu.com/s/1eRDCtGIPPT：http://pan.baidu.com/s/1o8fMs6Q11.嘉宾：孙恺 禾赛科技董事长、首席科学家主题：禾赛科技的无人驾驶激光雷达之路视频：http://pan.baidu.com/s/1pKXLfgZPPT：http://pan.baidu.com/s/1dF5VuEH12.嘉宾：沈劭劼 港科大助理教授主题：基于单目视觉与惯导整合的定位与建图PPT：http://pan.baidu.com/s/1pL5Cejp视频：https://pan.baidu.com/s/1ceY18U视频播放工具下载：https://pan.baidu.com/s/1skZenLj13.嘉宾：小米生态链企业石头科技CTO 吴震主题：扫地机器人的室内定位技术探索视频>>https://pan.baidu.com/s/1i5ij5XzPDF>>https://pan.baidu.com/s/1bSOHo6物联网1. 嘉宾：童小平 中移物联网有限公司智能连接部产品总监主题：连接与连接管理—物联卡业务介绍视频：https://pan.baidu.com/s/1c2MQOG4PDF：https://pan.baidu.com/s/1o8Lk5KI2. 嘉宾：孙志东 AbleCloud联合创始人兼CTO主题：物联网成熟平台架构及技术落地视频：https://pan.baidu.com/s/1boUeqR1PDF：https://pan.baidu.com/s/1gfzKpyV3. 嘉宾：张成 联车信息科技CEO主题：真正的汽车大数据视频：http://pan.baidu.com/s/1nvpyZjVPDF：https://pan.baidu.com/s/1slwc2ct4. 嘉宾：朱宇东 悦享趋势科技创始人兼CEO主题：医疗健康领域的IoT技术创业视频：https://pan.baidu.com/s/1gfeMf5pPDF：https://pan.baidu.com/s/1bpIjSWR5. 嘉宾：王筱东 慧云信息创始人主题：农业物联网探索与实践视频>>http://pan.baidu.com/s/1pKGTPDT；PDF>>http://pan.baidu.com/s/1c1DSwXa"}
{"content2":"Serpent.AI - 游戏代理框架（Python）Serpent.AI是一个简单而强大的新颖框架，可帮助开发人员创建游戏代理。将您拥有的任何视频游戏变成一个成熟的实验的沙箱环境，所有这些都是熟悉的Python代码。框架的存在理由首先是为机器学习和人工智能研究提供有价值的工具。用作爱好者（和危险的上瘾，公正的警告）也是非常有趣的！该框架具有大量的支持模块，它们在使用视频游戏作为环境以及CLI工具来加速开发时，为常见的场景提供解决方案。它提供了一些有用的约定，但绝对没有看到你在代理中的内容：想要使用最新的尖端深度强化学习算法？允许。想使用计算机视觉技术，图像处理和三角学？允许。想随机按向左或向右按 钮？叹了口气Serpent.AI被设计为完全基于插件（对于游戏支持和游戏代理），所以您的实验实际上是可移植的，可以分发给您的同龄人和互联网上随机的陌生人。您还将很高兴听到所有3个主要操作系统都支持：Linux，Windows和MacOS。注意：早期支持macOS。项目地址：https://github.com/SerpentAI/SerpentAI更多机器学习教程：http://www.tensorflownews.com/Serpent.AI - Game Agent Framework (Python)Serpent.AI is a simple yet powerful, novel framework to assist developers in the creation of game agents. Turn ANY video game you own into a sandbox environment ripe for experimentation, all with familiar Python code. The framework's raison d'être is first and foremost to provide a valuable tool for Machine Learning & AI research. It also turns out to be ridiculously fun to use as a hobbyist (and dangerously addictive; a fair warning)! The framework features a large assortment of supporting modules that provide solutions to commonly encountered scenarios when using video games as environments as well as CLI tools to accelerate development. It provides some useful conventions but is absolutely NOT opiniated about what you put in your agents: Want to use the latest, cutting-edge deep reinforcement learning algorithm? ALLOWED. Want to use computer vision techniques, image processing and trigonometry? ALLOWED. Want to randomly press the Left or Right buttons? sigh ALLOWED. To top it all off, Serpent.AI was designed to be entirely plugin-based (for both game support and game agents) so your experiments are actually portable and distributable to your peers and random strangers on the Internet. You'll also be glad to hear that all 3 major OSes are supported: Linux, Windows & macOS. Note: Early support for macOS."}
{"content2":"这几天，Google的人工智能AlphaGo与李世石的“围棋对决”受到了无数关注。作为人工智能的拥护者之一，博主自然也不例外。在第三战之前，写下这篇博客，谈谈感想。图 李世石 vs AlphaGo我本来想今天看比赛的，这样可以在周末前得知结果。因为这场李世石输了的话，那么就是3:0，人机大战的结果也就确定了，而且根据前两场来看，第三场李世石输的可能性很大。到了12点钟我才知道今天不赛，明后两天会连续两场。既然如此，后面的战斗还是有点悬念的。尽管是人工智能的拥护者，但我希望李世石后面能赢，一是增加悬念，二是人工智能发展的太快了，给人感觉Hold不住。关于AlphaGo的创造者-DeepMind公司，前身是英国的一家研究深度学习的公司，因为开发了一个可以学习打Atari游戏的学习系统，而被Google收购。一年前博客园曾报道过这个新闻，当时我是唯一一个留言者（笑）。我在留言中认为这是AI研究的一个重要进步。因为会打游戏了，学习现实中的工作都相对容易。游戏其实是对现实的抽象与模拟，一开始先学习游戏，可以慢慢对系统的学习算法与能力进行观察与改善。今年1月份左右，这个公司在《Nature》上刊登了关于研发出的围棋学习系统的论文，后面就有了3月份这场比赛。借助于各种直播平台，这场比赛的影响力是空前的。目前在贴吧、知乎、以及微博里都可以见到讨论。大众对于人工智能的关心突然地增加了，令人工智能的从业者都有点吃惊。有趣的是，这几天在贴吧我看到的跟深度学习相关的帖子，比这三年看到的还要多。博客园也到处都是相关的新闻。当然，这也是让人有点担心的。因为一些不清楚其中细节的人可能会对目前的AI研究产生误解。事实上，现在的AI跟科幻电影中的人工智能仍然相去甚远。只要稍微了解AlphaGo背后的技术，就可以了解。目前AlphaGo采用的技术是卷积神经网路和蒙特卡洛搜索树，加上了部分强化学习的理念。这些概念在目前的学术领域并没有太大的创新，因此说这些技术就是人工智能的话，未免太乐观了。目前人工智能界普遍的看法是，可能还需要二三十年人工智能的能力才能接近普通人的水准。现在的系统只能在某些专长领域比人做的更好（例如对图片分类，自动驾驶等）。若比较综合能力，以及应变能力，则远远不及人类。消除误解的最好办法事实上就是学习知识。但是，这场“人机大战”有可能是一个重要事件，因为这标志了机器第一次在“人类最后的骄傲”-围棋中战胜顶级选手。人工智能引发的浪潮可能是一场技术革命，是继工业革命，信息革命后的第三次革命。我是这么认为的：工业革命解放了人类的双手与双脚，信息革命解放了人类的计算与推理能力，而人工智能革命将会解放人类的感知与归纳能力。这里的感知能力指的是视觉、听觉、与语言表达能力，而归纳能力指的是对数据与经验的分析与总结能力。前者由深度学习完成，后者由普通的机器学习完成。这些技术的代表特征就是具有泛化和“举一反三”的能力。可以完成很多原先只能由人完成的工作。未来，很多岗位将会被AI所代替。举个简单的例子，目前交通罚款判断是否是闯红灯时，照片会由一些人来肉眼确认。未来这些工作可以被计算机视觉替代，只有不确定的照片才需要找人确认。现在，一些变化已经在进行中。例如原先停车场会由人或者卡来管理进出，现在借助停车场车牌识别系统，可以实现完全自动化的管理。欣慰的是，虽然一些岗位消失了，但这是良性变化。借助先进的技术未来人类的生活会更加美好。如果人工智能真的是一场技术革命，那么这场革命对于中国的意义何在？意义很大，而且非常重要。事实上，工业革命我们没有赶上，以至于落后了几百年；信息革命我们赶上了后半截（互联网部分），努力追赶才有目前的持平状态；而人工智能革命我们则与国外并驾齐驱，是实现能力强大与赶超的最好的机会。按照流行的说法，那就是实现中国梦的关键取决于能否紧紧抓住这次人工智能技术革命的浪潮。图 人工智能人工智能革命对于我们普通个人的意义在哪里，我们该如何面对？首先不要恐慌，其次要理解其背后的技术并不神秘与复杂。如果对人工智能感兴趣，就去认真学习。如果不感兴趣，就充分利用它。在这里我对希望在人工智能上进行探索的同学提供一些学习建议：一定要学好“机器学习”这门重要学科。在其基础上，再对“深度学习”进行深入研究。除了这两门学科以外，数学功底与算法能力也是相当重要的。这些建议，与各位共勉。在评论区有同学曾经叫我推荐一些资料。在这里，机器学习方面的书籍我推荐南大周志华老师最近出的《机器学习》一书。这本书我读过其中一两章节，个人感觉比国外的经典教材《PRML》和《ESL》更适合中国的读者以及工业界人士（毕竟周老师擅长的集成学习领域是在工业界应用效果极好的一门技术）。其次，深度学习方面，我推荐Standford的UFLDL教程，这个教程有中英双版，以及Bengio的《Deep Learning》一书的预览版本。这本书还未出版，但是在网上有预览版本，以后出中文版后可以购买支持。最后，让我们期待后面的几盘比赛，祝李世石加油。如果第三盘输了也不要紧，后面的看点是能否赢一盘，因为这是挽回“人类的荣誉”的最后机会。"}
{"content2":"本文章有转载自其它博文，也有自己发现的新库添加进来的，如果发现有新的库，可以推荐我加进来转自：http://www.cnblogs.com/tornadomeet/archive/2012/05/24/2515980.htmlDeep Learning（深度学习）：ufldl的2个教程(这个没得说，入门绝对的好教程，Ng的，逻辑清晰有练习)：一ufldl的2个教程(这个没得说，入门绝对的好教程，Ng的，逻辑清晰有练习)：二Bengio团队的deep learning教程，用的theano库，主要是rbm系列，搞python的可以参考，很不错。deeplearning.net主页，里面包含的信息量非常多，有software, reading list, research lab, dataset, demo等，强烈推荐，自己去发现好资料。Deep learning的toolbox，matlab实现的，对应源码来学习一些常见的DL模型很有帮助，这个库我主要是用来学习算法实现过程的。2013年龙星计划深度学习教程，邓力大牛主讲，虽然老师准备得不充分，不过还是很有收获的。Hinton大牛在coursera上开的神经网络课程，DL部分有不少，非常赞，没有废话，课件每句话都包含了很多信息，有一定DL基础后去听收获更大。Larochelle关于DL的课件，逻辑清晰，覆盖面广,包含了rbm系列，autoencoder系列，sparse coding系列，还有crf，cnn，rnn等。虽然网页是法文，但是课件是英文。CMU大学2013年的deep learning课程，有不少reading paper可以参考。达慕思大学Lorenzo Torresani的2013Deep learning课程reading list.Deep Learning Methods for Vision(余凯等在cvpr2012上组织一个workshop，关于DL在视觉上的应用)。斯坦福Ng团队成员链接主页，可以进入团队成员的主页，比较熟悉的有Richard Socher, Honglak Lee, Quoc Le等。多伦多ML团队成员链接主页，可以进入团队成员主页，包括DL鼻祖hinton，还有Ruslan Salakhutdinov , Alex Krizhevsky等。蒙特利尔大学机器学习团队成员链接主页，包括大牛Bengio，还有Ian Goodfellow 等。纽约大学的机器学习团队成员链接主页，包括大牛Lecun，还有Rob Fergus等。Charlie Tang个人主页，结合DL+SVM.豆瓣上的脑与deep learning读书会，有讲义和部分视频，主要介绍了一些于deep learning相关的生物神经网络。Large Scale ML的课程，由Lecun和Langford讲的，能不推荐么。Yann Lecun的2014年Deep Learning课程主页。视频链接。吴立德老师《深度学习课程》一些常见的DL code列表，csdn博主zouxy09的博文，Deep Learning源代码收集-持续更新…Deep Learning for NLP (without Magic)，由DL界5大高手之一的Richard Socher小组搞的，他主要是NLP的。2012 Graduate Summer School: Deep Learning, Feature Learning，高手云集，深度学习盛宴，几乎所有的DL大牛都有参加。matlab下的maxPooling速度优化，调用C++实现的。2014年ACL机器学习领域主席Kevin Duh的深度学习入门讲座视频。R-CNN code: Regions with Convolutional Neural Network Features.Machine Learning（机器学习）：介绍图模型的一个ppt，非常的赞，ppt作者总结得很给力，里面还包括了HMM，MEM, CRF等其它图模型。反正看完挺有收获的。机器学习一个视频教程，youtube上的，翻吧，内容很全面，偏概率统计模型，每一小集只有几分钟。龙星计划2012机器学习，由余凯和张潼主讲。demonstrate 的 blog :关于PGM(概率图模型)系列，主要按照Daphne Koller的经典PGM教程介绍的，大家依次google之。FreeMind的博客，主要关于机器学习的。Tom Mitchell大牛的机器学习课程，他的machine learning教科书非常出名。CS109,Data Science,用python介绍机器学习算法的课程。CCF主办的一些视频讲座。国外技术团队博客：Netflix技术博客,很多干货。Computer Vision（计算机视觉）：MIT2013年秋季课程：Advances in Computer Vision，有练习题，有些有code.IPAM一个计算机视觉的短期课程，有不少牛人参加。OpenCV相关：http://opencv.org/2012年7月4日随着opencv2.4.2版本的发布，opencv更改了其最新的官方网站地址。http://www.opencvchina.com/好像12年才有这个论坛的，比较新。里面有针对《learning opencv》这本书的视频讲解，不过视频教学还没出完，正在更新中。对刚入门学习opencv的人来说很不错。http://www.opencv.org.cn/forum/opencv中文论坛，对于初次接触opencv的学者来说比较不错，入门资料多，opencv的各种英文文档也翻译成中文了。不足是感觉这个论坛上发帖提问很少人回答，也就是说讨论不够激烈。http://opencv.jp/opencv的日文网站，里面有不少例子代码，看不懂日文可以用网站自带的翻译，能看个大概。http://code.opencv.org/projects/opencvopencv版本bug修补，版本更新，以及各种相关大型活动安排,还包含了opencv最近几个月内的活动路线，即未来将增加的功能等，可以掌握各种关于opencv进展情况的最新进展。http://tech.groups.yahoo.com/group/OpenCV/opencv雅虎邮件列表，据说是最好的opencv论坛，信息更新最新的地方。不过个人认为要查找相关主题的内容，在邮件列表中非常不方便。http://www.cmlab.csie.ntu.edu.tw/~jsyeh/wiki/doku.php台湾大学暑假集训网站，内有链接到与opencv集训相关的网页。感觉这种教育形式还蛮不错的。http://sourceforge.net/projects/opencvlibrary/opencv版本发布地方。http://code.opencv.org/projects/opencv/wiki/ChangeLog#241http://opencv.willowgarage.com/wiki/OpenCV%20Change%20Logsopencv版本内容更改日志网页,前面那个网页更新最快。http://www.opencv.org.cn/opencvdoc/2.3.2/html/doc/tutorials/tutorials.htmlopencv中文教程网页，分几个模块讲解，有代码有过程。内容是网友翻译opencv自带的doc文件里的。https://netfiles.uiuc.edu/jbhuang1/www/resources/vision/index.html网友总结的常用带有cvpr领域常见算法code链接的网址，感觉非常的不错。http://fossies.org/dox/OpenCV-2.4.2/该网站可以查看opencv中一些函数的变量接口，还会列出函数之间的结构图。http://opencv.itseez.com/opencv的函数、类等查找网页，有导航，查起来感觉不错。优化：submodual优化网页。Geoff Gordon的优化课程，youtube上有对应视频。数学：http://www.youku.com/playlist_show/id_19465801.html《计算机中的数学》系列视频，8位老师10讲内容，生动介绍微积分和线性代数基本概念在计算机学科中的各种有趣应用！Linux学习资料：http://itercast.com/library/1linux入门的基础视频教程，对于新手可选择看第一部分，视频来源于LinuxCast.net网站，还不错。OpenNI+Kinect相关：http://1.yuhuazou.sinaapp.com/网友晨宇思远的博客，主攻cvpr，ai等。http://blog.csdn.net/chenli2010/article/details/6887646kinect和openni学习资料汇总。http://blog.csdn.net/moc062066/article/category/871261OpenCV 计算机视觉 kinect的博客:http://kheresy.wordpress.com/index_of_openni_and_kinect/comment-page-5/网友Heresy的博客，里面有不少kinect的文章，写的比较详细。http://www.cnkinect.com/体感游戏中文网，有不少新的kinect资讯。http://www.kinectutorial.com/Kinect体感开发网。http://code.google.com/p/openni-hand-trackeropenni_hand_tracking google code项目。http://blog.candescent.ch/网友的kinect博客，里面有很多手势识别方面的文章介绍，还有源码，不过貌似是基于c#的。https://sites.google.com/site/colordepthfusion/一些关于深度信息和颜色信息融合（fusion）的文章。http://projects.ict.usc.edu/mxr/faast/kinect新的库，可以结合OpenNI使用。https://sites.google.com/a/chalearn.org/gesturechallenge/kinect手势识别网站。http://www.ros.org/wiki/mit-ros-pkgmit的kinect项目，有code。主要是与手势识别相关。http://www.thoughtden.co.uk/blog/2012/08/kinecting-people-our-top-6-kinect-projects/kinect 2012年度最具创新的6个项目，有视频，确实够创新的！http://www.cnblogs.com/yangyangcv/archive/2011/01/07/1930349.htmlkinect多点触控的一篇博文。http://sourceforge.net/projects/kinect-mex/http://www.mathworks.com/matlabcentral/fileexchange/30242-kinect-matlab有关matlab for kinect的一些接口。http://news.9ria.com/2012/1212/25609.htmlAIR和Kinect的结合，有一些手指跟踪的code。http://eeeweba.ntu.edu.sg/computervision/people/home/renzhou/index.htm研究kinect手势识别的，任洲。刚毕业不久。其他网友cvpr领域的链接总结：http://www.cnblogs.com/kshenf/网友整理常用牛人链接总结，非常多。不过个人没有没有每个网站都去试过。所以本文也是我自己总结自己曾经用过的或体会过的。OpenGL有关:http://nehe.gamedev.net/NeHe的OpenGL教程英文版。http://www.owlei.com/DancingWind/NeHe的OpenGL教程对应的中文版，由网友周玮翻译的。http://www.qiliang.net/old/nehe_qt/NeHe的OpengGL对应的Qt版中文教程。http://blog.csdn.net/qp120291570网友\"左脑设计，右脑编程\"的Qt_OpenGL博客,写得还不错。http://guiliblearning.blogspot.com/这个博客对opengl的机制有所剖析，貌似要FQ才能进去。cvpr综合网站论坛博客等：http://www.cvchina.net/中国计算机视觉论坛http://www.cvchina.info/这个博客很不错，每次看完都能让人兴奋，因为有很多关于cv领域的科技新闻，还时不时有视频显示。另外这个博客里面的资源也整理得相当不错。中文的。http://www.bfcat.com/一位网友的个人计算机视觉博客，有很多关于计算机视觉前沿的东西介绍，与上面的博客一样，看了也能让人兴奋。http://blog.csdn.net/v_JULY_v/牛人博客，主攻数据结构，机器学习数据挖掘算法等。http://blog.youtueye.com/该网友上面有一些计算机视觉方向的博客,博客中附有一些实验的测试代码.http://blog.sciencenet.cn/u/jingyanwang多看pami才扯谈的博客，其中有不少pami文章的中文介绍。http://chentingpc.me/做网络和自然语言处理的，有不少机器学习方面的介绍。ML常用博客资料等：http://freemind.pluskid.org/由 pluskid 所维护的 blog，主要记录一些机器学习、程序设计以及各种技术和非技术的相关内容，写得很不错。http://datasciencemasters.org/里面包含学ML/DM所需要的一些知识链接，且有些给出了视频教程，网页资料，电子书，开源code等，推荐！http://cs.nju.edu.cn/zhouzh/index.htm周志华主页，不用介绍了，机器学习大牛，更可贵的是他的很多文章都有源码公布。http://www.eecs.berkeley.edu/~jpaisley/Papers.htmJohn Paisley的个人主页，主要研究机器学习领域，有些文章有代码提供。http://foreveralbum.yo2.cn/里面有一些常见机器学习算法的详细推导过程。http://blog.csdn.net/abcjennifer浙江大学CS硕士在读，关注计算机视觉，机器学习，算法研究，博弈， 人工智能， 移动互联网等学科和产业。该博客中有很多机器学习算法方面的介绍。http://www.wytk2008.net/无垠天空的机器学习博客。http://www.chalearn.org/index.html机器学习挑战赛。http://licstar.net/licstar的技术博客，偏自然语言处理方向。国内科研团队和牛人网页：http://vision.ia.ac.cn/zh/index_cn.html中科院自动化所机器视觉课题小组，有相关数据库、论文、课件等下载。http://www.cbsr.ia.ac.cn/users/szli/李子青教授个人主页，中科院自动化所cvpr领域牛叉人！http://www4.comp.polyu.edu.hk/~cslzhang/香港理工大学教授lei zhang个人主页，也是cvpr领域一大牛人啊，cvpr，iccv各种发表。更重要的是他所以牛叉论文的code全部公开，非常难得！http://liama.ia.ac.cn/wiki/start中法信息、自动化与应用联合实验室，里面很多内容不仅限而cvpr，还有ai领域一些其他的研究。http://www.cogsci.xmu.edu.cn/cvl/english/厦门大学特聘教授，cv领域一位牛人。研究方向主要为目标检测，目标跟踪，运动估计，三维重建，鲁棒统计学，光流计算等。http://idm.pku.edu.cn/index.aspx北京大学数字视频编码技术国家实验室。http://www.csie.ntu.edu.tw/~cjlin/libsvm/libsvm项目网址，台湾大学的，很火！http://www.jdl.ac.cn/user/sgshan/index.htm山世光，人脸识别研究比较牛。在中国科学院智能信息处理重点实验室国外科研团队和牛人网页：https://netfiles.uiuc.edu/jbhuang1/www/resources/vision/index.html常见计算机视觉资源整理索引，国外学者整理，全是出名的算法，并且带有代码的，这个非常有帮助，其链接都是相关领域很火的代码。http://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/txtv-groups.html国外学者整理的各高校研究所团队网站http://research.microsoft.com/en-us/groups/vision/微软视觉研究小组，不解释，大家懂的，牛！http://lear.inrialpes.fr/index.php法国国家信息与自动化研究所，有对应牛人的链接，论文项目网页链接，且一些code对应链接等。http://www.cs.ubc.ca/~pcarbo/objrecls/Learning to recognize objects with little supervision该篇论文的项目网页，有对应的code下载，另附有详细说明。http://www.eecs.berkeley.edu/~lbourdev/poselets/poselets相关研究界面，关于poselets的第一手资料。http://www.cse.oulu.fi/CMV/Research芬兰奥卢大学计算机科学与工程学院网页，里面有很多cv领域相关的研究，比如说人脸，脸部表情，人体行为识别，跟踪，人机交互等cv基本都涉及有。http://www.cs.cmu.edu/~cil/vision.html卡耐基梅隆大学计算机视觉主页，内容非常多。可惜的是该网站内容只更新到了2004年。http://vision.stanford.edu/index.html斯坦福大学计算机视觉主页，里面有非常非常多的牛人，比如说大家熟悉的lifeifei.http://www.wavelet.org/index.php关于wavelet研究的网页。http://civs.ucla.edu/加州大学洛杉矶分校统计学院，关于统计学习方面各种资料，且有相应的网上公开课。http://www.cs.cmu.edu/~efros/卡耐基梅隆大学Alexei(Alyosha)Efros教授个人网站，计算机图形学高手。http://web.mit.edu/torralba/www//mit牛人Associate教授个人网址，主要研究计算机视觉人体视觉感知，目标识别和场景理解等。http://people.csail.mit.edu/billf/mit牛人William T. Freeman教授，主要研究计算机视觉和图像学http://www.research.ibm.com/peoplevision/IBM人体视觉研究中心，里面除了有其研究小组的最新成果外，还有很多测试数据(特别是视频）供下载。http://www.vlfeat.org/vlfeat主页，vlfeat也是一个开源组织，主要定位在一些最流行的视觉算法开源上，Ｃ编写，其很多算法效果比opencv要好,不过数量不全，但是非常有用。http://www.robots.ox.ac.uk/~az/Andrew Zisserman的个人主页，这人大家应该熟悉，《计算机视觉中的多视几何》这本神书的作者之一。http://www.cs.utexas.edu/~grauman/KristenGrauman教授的个人主页，是个大美女，且是2011年“马尔奖”获得者，”马尔奖“大家都懂的，计算机视觉领域的最高奖项，目前无一个国内学者获得过。她的主要研究方法是视觉识别。http://groups.csail.mit.edu/vision/welcome/mit视觉实验室主页。http://code.google.com/p/sixthsense/曾经在网络上非常出名一个视频，一个作者研究的第六感装置，现在这个就是其开源的主页。http://vision.ucsd.edu/~pdollar/research.html#BehaviorRecognitionAnimalBehaviorPiotr Dollar的个人主要，主要研究方向是人体行为识别。http://www.mmp.rwth-aachen.de/移动多媒体处理，将移动设备，计算机图像学，视觉，图像处理等结合的领域。http://www.di.ens.fr/~laptev/index.htmlIvan Laptev牛人主页，主要研究人体行为识别。有很多数据库可以下载。http://blogs.oregonstate.edu/hess/Rob Hess的个人主要，里面有源码下载，比如说粒子滤波，他写的粒子滤波在网上很火。http://morethantechnical.googlecode.com/svn/trunk/cvpr领域一些小型的开源代码。http://iica.de/pd/index.py做行人检测的一个团队，内部有一些行人检测的代码下载。http://www.cs.utexas.edu/~grauman/research/pubs.htmlUT-Austin计算机视觉小组，包含的视觉研究方向比较广，且有的文章有源码，你只需要填一个邮箱地址，系统会自动发跟源码相关的信息过来。http://www.robots.ox.ac.uk/~vgg/index.htmlvisual geometry group图像:http://blog.sina.com.cn/s/blog_4cccd8d301012pw5.html交互式图像分割代码。http://vision.csd.uwo.ca/code/graphcut优化代码。语音：http://danielpovey.com/kaldi-lectures.html语音处理中的kaldi学习。算法分析与设计（计算机领域的基础算法）：http://www.51nod.com/focus.html该网站主要是讨论一些算法题。里面的李陶冶是个大牛，回答了很多算法题。一些综合topic列表：http://www.cs.cornell.edu/courses/CS7670/2011fa/计算机视觉中的些topic（Special Topics in Computer Vision），截止到2011年为止，其引用的文章都是非常顶级的topic。书籍相关网页：http://www.imageprocessingplace.com/index.htm冈萨雷斯的《数字图像处理》一书网站，包含课程材料，matlab图像处理工具包，课件ppt等相关素材。Consumer Depth Cameras for Computer Vision很优秀的一本书，不过很贵，买不起啊！做深度信息的使用这本书还不错，google图中可以预览一部分。Making.Things.See针对Kinect写的，主要关注深度信息，较为基础。书籍中有不少例子，貌似是java写的。国内一些AI相关的研讨会：http://www.iipl.fudan.edu.cn/MLA13/index.htm中国机器学习及应用研讨会(这个是2013年的)期刊会议论文下载：http://cvpapers.com/几个顶级会议论文公开下载界面，比如说ICCV,CVPR,ECCV,ACCV,ICPR,SIGGRAPH等。http://www.cvpr2012.org/cvpr2012的官方地址，里面有各种资料和信息，其他年份的地址类似推理更改即可。http://www.sciencedirect.com/science/journal/02628856ICV期刊下载http://www.computer.org/portal/web/tpamiTPAMI期刊，AI领域中可以算得上是最顶级的期刊了，里面有不少cvpr方面的内容。http://www.springerlink.com/content/100272/IJCV的网址。http://books.nips.cc/NIPS官网，有论文下载列表。http://graphlab.org/lsrs2013/program/LSRS （会议）地址，大规模推荐系统，其它年份依次类推。会议期刊相关信息：http://conferences.visionbib.com/Iris-Conferences.html该网页列出了图像处理，计算机视觉领域相关几乎所有比较出名的会议时间表。http://conferences.visionbib.com/Browse-conf.php上面网页的一个子网页，列出了最近的CV领域提交paper的deadline。cvpr相关数据库下载：http://research.microsoft.com/en-us/um/people/jckrumm/WallFlower/TestImages.htm微软研究院牛人Wallflower Paper的论文中用到的目标检测等测试图片http://archive.ics.uci.edu/ml/UCI数据库列表下载，最常用的机器学习数据库列表。http://www.cs.rochester.edu/~rmessing/uradl/人体行为识别通过关键点的跟踪视频数据库，Rochester university的http://www.research.ibm.com/peoplevision/performanceevaluation.htmlIBM人体视觉研究中心，有视频监控等非常多的测试视频。http://www.cvpapers.com/datasets.html该网站上列出了常见的cvpr研究的数据库。http://www.cs.washington.edu/rgbd-dataset/index.htmlRGB-D Object Dataset.做目标识别的。AI相关娱乐网页：http://en.akinator.com/该网站很好玩，可以测试你心里想出的一个人名(当然前提是这个人必须有一定的知名度)，然后该网站会提出一系列的问题，你可以选择yes or no,or I don’t know等等，最后系统会显示你心中所想的那个人。http://www.doggelganger.co.nz/人与狗的匹配游戏，摄像头采集人脸，呵呵…Android相关：https://code.google.com/p/android-ui-utils/该网站上有一些android图标,菜单等跟界面有关的设计工具,可以用来做一些简单的UI设计.工具和code下载：http://lear.inrialpes.fr/people/dorko/downloads.html6种常见的图像特征点检测子，linux下环境运行。不过只提供了二进制文件，不提供源码。http://www.cs.ubc.ca/~pcarbo/objrecls/index.html#codessmcmc的matlab代码,是Learning to recognize objects with little supervision这一系列文章用的源码，属于目标识别方面的研究。http://www.robots.ox.ac.uk/~timork/仿射无关尺度特征点检测算子源码，还有些其它算子的源码或二进制文件。http://www.vision.ee.ethz.ch/~bleibe/code/ism.html隐式形状模型(ISM)项目主页，作者Bastian Leibe提供了linux下运行的二进制文件。http://www.di.ens.fr/~laptev/download.html#stipIvan Laptev牛人主页中的STIP特征点检测code，但是也只是有二进制文件，无源码。该特征点在行为识别中该特征点非常有名。http://ai.stanford.edu/~quocle/斯坦福大学Quoc V.Le主页，上有它2011年行为识别文章的代码。开源软件：http://mloss.org/software/一些ML开源软件在这里基本都可以搜到，有上百个。https://github.com/myui/hivemallScalable machine learning library for Hive/Hadoop.http://scikit-learn.org/stable/基于python的机器学习开源软件，文档写得不错。挑战赛：http://www.chioka.in/kaggle-competition-solutions/kaggle一些挑战赛的code.公开课：网易公开课，国内做得很不错的公开课，翻译了一些国外出名的公开课教程，与国外公开课平台coursera有合作。coursera在线教育网上公开课，很新，有个邮箱注册即可学习，有不少课程，且有对应的练习，特别是编程练习，超赞。斯坦福网上公开课链接，有统计学习，凸优化等课程。udacity公开课程下载链接，其实速度还可以。里面有不少好教程。机器学习公开课的连接，有不少课。转自：http://blog.csdn.net/tainyiliusha/article/details/10077081开源生物特征识别库 OpenBROpenBR 是一个用来从照片中识别人脸的工具。还支持推算性别与年龄。 使用方法：$ br -algorithm FaceRecognition -compare me.jpg you.jpg更多OpenBR信息最近更新： OpenBR —— 开源的生物识别工具 发布于 13天前计算机视觉库 OpenCVOpenCV是Intel®开源计算机视觉库。它由一系列 C 函数和少量 C++ 类构成，实现了图像处理和计算机视觉方面的很多通用算法。 OpenCV 拥有包括 300 多个C函数的跨平台的中、高层 API。它不依赖于其它的外部库——尽管也可以使用某些外部库。 OpenCV 对非商业...更多OpenCV信息最近更新： OpenCV 2.4.5 发布，开源计算机视觉库 发布于 2个月前人脸识别 faceservice.cgifaceservice.cgi 是一个用来进行人脸识别的 CGI 程序， 你可以通过上传图像，然后该程序即告诉你人脸的大概坐标位置。faceservice是采用 OpenCV 库进行开发的。更多faceservice.cgi信息Java视觉处理库 JavaCVJavaCV 提供了在计算机视觉领域的封装库，包括：OpenCV、ARToolKitPlus、libdc1394 2.x 、PGR FlyCapture和FFmpeg。此外，该工具可以很容易地使用Java平台的功能。 JavaCV还带有硬件加速的全屏幕图像显示（CanvasFrame），易于在多个内核中执行并行代码（并...更多JavaCV信息视频监控系统 OpenVSSOpenVSS - 开放平台的视频监控系统 - 是一个系统级别的视频监控软件视频分析框架（VAF）的视频分析与检索和播放服务，记录和索引技术。它被设计成插件式的支持多摄像头平台，多分析仪模块（OpenCV的集成），以及多核心架构。更多OpenVSS信息OpenCV的.NET版 OpenCVDotNetOpenCVDotNet 是一个 .NET 对 OpenCV 包的封装。更多OpenCVDotNet信息人脸检测算法 jViolajonesjViolajones是人脸检测算法Viola-Jones的一个Java实现，并能够加载OpenCV XML文件。 示例代码：http://www.oschina.net/code/snippet_12_2033更多jViolajones信息手势识别 hand-gesture-detection手势识别，用OpenCV实现更多hand-gesture-detection信息人脸检测与跟踪库 asmlibraryActive Shape Model Library (ASMLibrary©) SDK, 用OpenCV开发，用于人脸检测与跟踪。更多asmlibrary信息开放模式识别项目 OpenPRPattern Recognition project（开放模式识别项目），致力于开发出一套包含图像处理、计算机视觉、自然语言处理、模式识别、机器学习和相关领域算法的函数库。更多OpenPR信息运动检测程序 QMotionQMotion 是一个采用 OpenCV 开发的运动检测程序，基于 QT。更多QMotion信息图像特征提取 cvBlobcvBlob 是计算机视觉应用中在二值图像里寻找连通域的库.能够执行连通域分析与特征提取.更多cvBlob信息OpenCV的.Net封装 OpenCVSharpOpenCVSharp 是一个OpenCV的.Net wrapper，应用最新的OpenCV库开发，使用习惯比EmguCV更接近原始的OpenCV，有详细的使用样例供参考。更多OpenCVSharp信息人脸检测识别 mcvai-tracking提供人脸检测、识别与检测特定人脸的功能，示例代码 cvReleaseImage( &gray ); cvReleaseMemStorage(&storage); cvReleaseHaarClassifierCascade(&cascade);...更多mcvai-tracking信息视频捕获 API VideoManVideoMan 提供一组视频捕获 API 。支持多种视频流同时输入（视频传输线、USB摄像头和视频文件等）。能利用 OpenGL 对输入进行处理，方便的与 OpenCV，CUDA 等集成开发计算机视觉系统。更多VideoMan信息基于QT的计算机视觉库 QVision基于 QT 的面向对象的多平台计算机视觉库。可以方便的创建图形化应用程序，算法库主要从 OpenCV，GSL，CGAL，IPP，Octave 等高性能库借鉴而来。更多QVision信息开源视线跟踪软件 ITU Gaze Tracker哥本哈根大学开源视线跟踪软件 The ITU Gaze Tracker is an open-source eye tracker that aims to provide a low-cost alternative to commercial gaze tracking systems and to make this technology more accessible. It is developed by the Gaze Grou...更多ITU Gaze Tracker信息图像处理和计算机视觉常用算法库 LTI-LibLTI-Lib 是一个包含图像处理和计算机视觉常用算法和数据结构的面向对象库，提供 Windows 下的 VC 版本和 Linux 下的 gcc 版本，主要包含以下几方面内容： 1、线性代数 2、聚类分析 3、图像处理 4、可视化和绘图工具更多LTI-Lib信息实时图像/视频处理滤波开发包 GShowGShow is a real-time image/video processing filter development kit. It successfully integrates DirectX11 with DirectShow framework. So it has the following features: GShow 是实时 图像/视频 处理滤波开发包，集成DiretX11。...更多GShow信息C++计算机视觉库 Integrating Vision ToolkitIntegrating Vision Toolkit (IVT) 是一个强大而迅速的C++计算机视觉库，拥有易用的接口和面向对象的架构，并且含有自己的一套跨平台GUI组件，另外可以选择集成OpenCVOpenCV的Python封装 pyopencvOpenCV的Python封装，主要特性包括： 提供与OpenCV 2.x中最新的C++接口极为相似的Python接口，并且包括C++中不包括的C接口 提供对OpenCV 2.x中所有主要部件的绑定：CxCORE (almost complete), CxFLANN (complete), Cv (complete), CvAux (C++ part almost...更多pyopencv信息模式识别和视觉库 RAVLRecognition And Vision Library (RAVL) 是一个通用 C++ 库，包含计算机视觉、模式识别等模块。更多RAVL信息OpenSURF利用OpenCV和C++编写的SURF算法，作者Christopher Evans是首个利用OpenCV和C++结合的方法实现SURF算法。更多OpenSURF信息人脸识别库 rpflexrpflex 是一个 Flex 开发的库，用来识别照片中的人脸、眼镜和脖子。更多rpflex信息OpenCV优化 opencv-dsp-acceleration优化了OpenCV库在DSP上的速度。更多opencv-dsp-acceleration信息Java 计算机视觉库 BoofCVBoofCV 是一个 Java 的全新实时的计算机视觉库，BoofCV 易于使用而且具有非常高的性能。它提供了一系列从低层次的图像处理、小波去噪功能以及更高层次的三维几何视野。使用 BSD 许可证可在商业应用中使用。 这里有篇英文文章用来介绍 BoofCV 的使用。...更多BoofCV信息计算机视觉库 SimpleCVSimpleCV 将很多强大的开源计算机视觉库包含在一个便捷的Python包中。使用SimpleCV，你可以在统一的框架下使用高级算法，例如特征检测、滤波和模式识别。使用者不用清楚一些细节，比如图像比特深度、文件格式、颜色空间、缓冲区管理、特征值还有矩阵和图像...更多SimpleCV信息3D视觉库 fvision2010基于OpenCV构建的图像处理和3D视觉库。 示例代码： ImageSequenceReaderFactory factory; ImageSequenceReader* reader = factory.pathRegex(\"c:/a/im_%03d.jpg\", 0, 20); //ImageSequenceReader* reader = factory.avi(\"a.avi\"); if (reader == NULL) { ...更多fvision2010信息视觉快速开发平台 qcv计算机视觉快速开发平台，提供测试框架，使开发者可以专注于算法研究。更多qcv信息计算机视觉算法 OpenVIDIAOpenVIDIA projects implement computer vision algorithms running on on graphics hardware such as single or multiple graphics processing units(GPUs) using OpenGL, Cg and CUDA-C. Some samples will soon support OpenCL and Direct Compute API'...更多OpenVIDIA信息C++计算机视觉库 ICLICL (Image Component Library) 是一种新型的C + +计算机视觉库，由比勒费尔德大学神经信息学组和CITEC开发。它兼顾了性能和用户友好性。 ICL提供了一个易于使用的类和函数的集合，可以开发复杂的计算机视觉应用。 在不到15行的C + +代码（见例子）可以写成...更多ICL信息Matlab计算机视觉包 mVisionMatlab 的计算机视觉包，包含用于观察结果的 GUI 组件，貌似也停止开发了，拿来做学习用挺不错的。更多mVision信息Lua视觉开发库 libecvECV 是 lua 的计算机视觉开发库(目前只提供linux支持)更多libecv信息OpenCV的扩展库 ImageNetsImageNets 是对OpenCV 的扩展，提供对机器人视觉算法方面友好的支持，使用Nokia的QT编写界面。更多ImageNets信息图像捕获 libv4l2cam对函数库v412的封装，从网络摄像头等硬件获得图像数据，支持YUYV裸数据输出和BGR24的OpenCV  IplImage输出更多libv4l2cam信息高斯模型点集配准算法 gmmreg实现了基于混合高斯模型的点集配准算法，该算法描述在论文： A Robust Algorithm for Point Set Registration Using Mixture of Gaussians, Bing Jian and Baba C. Vemuri. ，实现了C++/Matlab/Python接口...更多gmmreg信息Scilab的计算机视觉库 SIPSIP 是 Scilab（一种免费的类Matlab编程环境）的图像处理和计算机视觉库。SIP 可以读写 JPEG/PNG/BMP 格式的图片。具备图像滤波、分割、边缘检测、形态学处理和形状分析等功能。更多SIP信息计算机视觉和机器人技术的工具包 EGTThe Epipolar Geometry Toolbox (EGT) is a toolbox designed for Matlab (by Mathworks Inc.). EGT provides a wide set of functions to approach computer vision and robotics problems with single and multiple views, and with different vision se...更多EGT信息计算机视觉库 BazARBazAR 是基于特征点检测和匹配的计算机视觉库。 它能够快速检测和匹配图像中的已知物体，并且能够用于增强现实，它是计算机视觉研究的先进成果。更多BazAR信息计算机视觉库 VLFeat一个开源的计算机视觉库，实现了 SIFT,MSER, k-means, hierarchical k-means, agglomerative information bottleneck, quick shift等算法。由C语言编写,提供MATLAB接口，文档详细。支持跨平台。...更多VLFeat信息STAIR Vision LibrarySTAIR Vision Library (SVL) 最初是为支持斯坦福智能机器人设计的，提供对计算机视觉、机器学习和概率统计模型的支持。更多STAIR Vision Library信息Scilab Image Processing ToolboxSIP 提供了图像处理、模式识别以及计算机视觉处理。 SIP is able to read/write images in almost 90 major formats, including JPEG, PNG, BMP, GIF, FITS, and TIFF. It includes routines for filtering, segmentation, edge detection, morphology, cu...更多Scilab Image Processing Toolbox信息3D计算机视觉库 openvis3d这个项目的目的是提供一个高效的3D计算机视觉库，用于图像和视频处理。它包括深度立体匹配、光流（运动）估计、遮挡检测和运动平台估计更多openvis3d信息libvideogfx视频处理、计算机视觉和计算机图形学的快速开发库。更多libvideogfx信息go-opencvGo-OpenCV 是 Go 语言版的 OpenCV 封装。更多go-opencv信息JavaScript图形绘制库 Toxiclibs.jsToxiclibs.js 是一个开源的计算机图形设计库，无需外部依赖，使用 <canvas> 元素进行图形绘制。更多Toxiclibs.js信息OpenCL 封装库 CLOGSCLOGS 是 OpenCL C++ API 的高级封装库，其设计目的是集成其他 OpenCL 代码，包括同步 OpenCL 事件，当前支持两个操作：基数排序和独立扫描。更多CLOGS信息最近更新： CLOGS 1.2.0 发布，OpenCL 的封装库 发布于 2个月前openvgrOpenVGR 包含以下几个实时处理模块 (基于 OpenRTM-1.0): 立体相机采集 (对于 IEEE 1394b 相机), 立体图像浏览器, 3-D 点云重建 (使用 OpenCV),  基于边缘的 3-D 物体检测 包含以下几个命令行工具: 模型建立, 多相机标定....更多openvgr信息sparse-stereo-vision使用 OpenCV 函数, 这个项目能从成对的立体图像中重建场景。更多sparse-stereo-vision信息PIV图形软件包 FluereFluere是粒子图像测速（PIV）的图形软件包。 Fluere是高度优化的并行处理，并在多个平台上运行。该项目的目标是提供高质量的测速软件，采用PIV技术处理的最新进展的研究人员和教育工作者，而所使用的算法的完整的知识。更多Fluere信息stereoviewstereoview 是一个立体可视化和标定工具更多stereoview信息Resources in Visual Tracking  转自 ：http://blog.csdn.net/minstyrain/article/details/38640541这个应该是目前最全的Tracking相关的文章了,转载请注明出处。一、Surveyand benchmark：1.      PAMI2014：VisualTracking_ An Experimental Survey，代码：http://alov300pp.joomlafree.it/trackers-resource.html2.      CVPR2013:Online Object Tracking: A Benchmark(需FQ)3.      SignalProcessing  2011：Video Tracking Theory andPractice4.      ACCV2006：Tutorials-Advances in VisualTracking：中文：视觉跟踪的进展5.      Evaluationof an online learning approach for robust object tracking二、研究团体：1.      Universityof California at Merced：Ming-HsuanYang视觉跟踪当之无愧第一人，后面的人基本上都和气其有合作关系，他引近9000PublicationsPAMI：6，CVPR：26，ECCV：17，BMCV：6，NIPS：6，IJCV：3，ACCV：3代表作：RobustVisual Tracking via Consistent Low-Rank Sparse LearningFCT,IJCV2014:FastCompressive TrackingRST,PAMI2014:RobustSuperpixel Tracking; SPT,ICCV2011, SuperpixeltrackingSVD,TIP2014:LearningStructured Visual Dictionary for Object TrackingECCV2014: SpatiotemporalBackground Subtraction Using Minimum Spanning Tree and Optical FlowPAMI2011:RobustObject Tracking with Online Multiple Instance LearningMIT,CVPR2009: Visualtracking with online multiple instance learningIJCV2008: IncrementalLearning for Robust Visual Tracking2.      SeoulNational University Professor：KyoungMuLee2013年在PAMI上发表5篇，至今无人能及文献列表PAMI:13,CVPR:30,ECCV:12,ICCV:8,PR:4PAMI2014：A GeometricParticle Filter for Template-Based Visual TrackingECCV2014: Robust Visual Tracking with Double Bounding Box ModelPAMI2013:HighlyNonrigid Object Tracking via Patch-based Dynamic Appearance ModelingCVPR2014: Interval Tracker: Tracking by Interval AnalysisCVPR2013: MinimumUncertainty Gap for Robust Visual TrackingCVPR2012:RobustVisual Tracking using Autoregressive Hidden Markov ModelVTS,ICCV2011:Tracking by Sampling Trackers.VTD,CVPR2010: VisualTracking DecompositionTST,ICCV2011:Tracking by sampling trackers3.      TempleUniversity，凌海滨Publication List PMAI:4,CVPR:19,ICCV:17,ECCV:5,TIP:9CVPR2014:Multi-targetTracking with Motion Context in Tenor Power IterationECCV2014:TransferLearning Based Visual Tracking with Gaussian Process RegressionICCV2013:Findingthe Best from the Second Bests - Inhibiting Subjective Bias in Evaluation ofVisual Tracking AlgorithmsCVPR2013: Multi-targetTracking by Rank-1 Tensor ApproximationCVPR2012:RealTime Robust L1 Tracker Using Accelerated Proximal Gradient ApproachTIP2012: Real-timeProbabilistic Covariance Tracking with Efficient Model UpdateICCV2011: BlurredTarget Tracking by Blur-driven TrackerPAMI2011ICCV2009: RobustVisual Tracking and Vehicle Classification via Sparse RepresentationICCV2011:RobustVisual Tracking using L1 MinimizationL1O,CVPR2011: Minimumerror bounded efficient l1 tracker with occlusion detectionL1T, ICCV2009:Robustvisual tracking using l1 minimization4.      HongKong Polytechnic University AssociateProfessor: Lei ZhangPapersPAMI:2,CVPR:18,ICCV:14,ECCV:12,ICPR:6,PR:28,TIP:4STC,ECCV2014: FastTracking via Dense Spatio-Temporal Context LearningFCT,PAMI2014,ECCV2012:Fast CompressiveTracking, Minghsuan YangIETComputer Vision2012:Scale and Orientation Adaptive Mean Shift TrackingIJPRAI2009:RobustObject Tracking using Joint Color-Texture Histogram5.      大连理工大学教授 卢湖川国内追踪领域第一人CVPR2014:VisualTracking via Probability Continuous Outlier ModelTIP2014:VisualTracking via Discriminative Sparse Similarity MapTIP2014: RobustSuperpixel TrackingTIP2014: RobustObject Tracking via Sparse Collaborative Appearance ModelCVPR2013: LeastSoft-threshold Squares Tracking, MinghsuanYangTIP2013:Online Object Trackingwith Sparse Prototypes, Minghsuan YangSignalProcessing Letters2013: Graph-RegularizedSaliency Detection With Convex-Hull-Based Center PriorSignalProcessing2013: On-line LearningParts-based Representation via Incremental Orthogonal Projective Non-negativeMatrix FactorizationCVPR2012:RobustObject Tracking viaSparsity-based Collaborative Model, MinghsuanYangCVPR2012:VisualTracking via Adaptive Structural Local Sparse Appearance Model, MinghsuanYangSignalProcessing Letters 2012:Object tracking via 2DPCA and L1-regularizationIETImage Processing 2012:Visual Tracking via Bag of FeaturesICPR2012:Superpixel Level Object Recognition Under Local Learning FrameworkICPR2012: Fragment-BasedTracking Using Online Multiple Kernel LearningICPR2012: ObjectTracking Based On Local LearningICPR2012: ObjectTracking with L2_RLSICPR2011:ComplementaryVisual TrackingFG2011:OnlineMultiple Support Instance TrackingSignalProcessing2010: A novel methodfor gaze tracking by local pattern model and support vector regressorACCV2010: OnFeature Combination and Multiple Kernel Learning for Object TrackingACCV: RobustTracking Based on Pixel-wise Spatial Pyramid and Biased FusionACCV2010: HumanTracking by Multiple Kernel Boosting with Locality Affinity ConstraintsICCV2011:SuperpixelTracking, Minghsuan YangICPR2010: RobustTracking Based on Boosted Color Soft Segmentation and ICA-RICPR2010: IncrementalMPCA for Color Object TrackingICPR2010: Bagof Features TrackingICPR2008: GazeTracking By Binocular Vision and LBP Features6.      南京信息工程大学教授,KaiHua Zhang7.      OregonstateProfessor,Sinisa Todorovic由视频分割转向TrackingCSL,CVPR2014: Multi-ObjectTracking via Constrained Sequential LabelingCVPR2011:MultiobjectTracking as Maximum Weight Independent Set8.      GrazUniversity of Technology, Austria，Horst Possegger博士CVPR2014:OcclusionGeodesics for Online Multi-Object TrackingCVPR2013: RobustReal-Time Tracking of Multiple Objects by Volumetric Mass Densities9.      马里兰大学Zdenek Kalal博士TLD,PAMI2011: Tracking-Learning-DetectionTIP2010: Face-TLD:Tracking-Learning-Detection Applied to FacesICPR2010:Forward-BackwardError: Automatic Detection of Tracking FailuresCVPR2010: P-N Learning:Bootstrapping Binary Classifiers by Structural ConstraintsBMVC2008: Weighted Sampling forLarge-Scale Boosting中文讲解：TLD视觉跟踪算法TLD源码深度分析庖丁解牛TLDTLD（Tracking-Learning-Detection）学习与源码理解三、其他早期工作：Tracking of a Non-Rigid ObjectviaPatch-based Dynamic Appearance Modeling and Adaptive Basin Hopping Monte CarloSamplingtracking-by-detection粒子滤波演示与opencv代码opencv学习笔记-入门（6）-camshiftCamshift算法原理及其Opencv实现Camshift算法CamShift算法，OpenCV实现1--Back Projection目标跟踪学习笔记_2(particle filter初探1)目标跟踪学习笔记_3(particle filter初探2)目标跟踪学习笔记_4(particle filter初探3)目标跟踪学习系列一:on-line boosting and vision 阅读最近由于转行做医学图像处理，因此加入几个医学方面的库以及最近在微博疯传的几个库：CCV库ccv是一个基于C语言的、带缓存的现代计算机视觉库。链接：https://github.com/liuliu/ccv，http://libccv.org/visionworkbench库visionworkbench是NASA开发的通用图像处理、计算机视觉算法工具库。代码充分利用C++模板和泛型编程等特性，结构清晰，使用方便。工具库功能也很多，包括了常用数学计算、文件IO、相机模型、几何计算、特征点、Bundle Adjustment。对机器人视觉、三维视觉方面的研发想必很有帮助。链接：https://github.com/nasa/visionworkbench/ITK库ITK(Insight Segmentation and Registration Toolkit) 是一个开源，跨平台的图像分析框架，里面有大量的前沿算法，广泛用于图像配准和分割。ITK使用C++开发，可由CMake生成不同环境下的可编译工程，并且ITK有对Tcl, Python和Java的封装层，使得开发者可以使用不同的语言进行开发。链接：http://www.itk.org/VTK库Vtk，（visualization toolkit）是一个开放资源的免费软件系统，主要用于三维计算机图形学、图像处理和可视化。Vtk是在面向对象原理的基础上设计和实现的，它的内核是用C++构建的，包含有大约250,000行代码，2000多个类，还包含有几个转换界面，因此也可以自由的通过Java，Tcl/Tk和Python各种语言使用vtk。http://www.vtk.org/IGSTKThe Image-Guided Surgery Toolkit is a high-level, component-based framework which provides a common functionality for image-guided surgery applications. The framework is a set of high-level components integrated with low-level open source software libraries and application programming interfaces (API) from hardware vendors.链接：http://www.igstk.org/DCMTKDCMTK是由德国offis公司提供的开源项目，并拥有相应的版权。这个开发包经过10多年的开发和维护，已经基本实现了DICOM协议的所有内容。该开发包提供所有的源代码、支持库和帮助文档。DCMTK提供了在各种操作系统下使用的可能版本，如LINUX、SUN、MACOS、WINDOWS等，用户可根据自己的开发平台进行编译。链接：http://www.dcmtk.org/陆续更新中。"}
{"content2":"1、人工智能基本技术：知识表示、推理、搜索、规划2、人工智能的主要研究、应用领域：（1）机器感知：机器视觉；机器听觉；自然语言理解；机器翻译（2）机器思维：机器推理（3）机器学习：符号学习；连接学习（4）机器行为：智能控制（5）智能机器：智能机器人；机器智能（6）智能应用：博弈；自动定理证明；自动程序设计；专家系统；智能决策；智能检索；智能CAD；智能CAI；智能交通；智能电力；智能产品；智能建筑等3、人工智能新技术:（1）计算智能：神经计算；模糊计算；进化计算；自然计算（2）人工生命：人工脑；细胞自动机（3）分布智能：多Agent ,  群体智能（4）数据挖掘：知识发现；数据挖掘4、人工智能的定义：综合各种不同观点，可从能力和学科两个方面讨论人工智能的定义，能力方面：人工智能就是用人工的方法在机器（计算机）上实现的智能，或称机器智能。学科方面：是一门研究如何构造智能机器或智能系统，以模拟、延伸和扩展人类智能的学科。5、人工智能研究形成了三大学派：符号主义：是指基于符号运算的人工智能学派，他们认为知识可以用符号来表示，认知可以通过符号运算来实现。例如，专家系统等。基于物理符号系统假设和有限合理性原理。连接主义：是指神经网络学派，随着模糊逻辑和进化计算的逐步成熟，又形成了“计算智能”这个统一的学科范畴。基于神经网络及其间的连接机制与学习算法。行为主义：是指进化主义学派，如：6条腿的机器虫。基于控制论及感知—动作型控制系统。6、人工智能的研究目标近期目标：建造智能计算机代替人类的部分智力劳动；远期目标：用自动机模仿人类的思维过程和智能行为。7、人工智能近期发展分析:（1）多学科交叉研究（2）集成智能研究（3）多学派融合研究"}
{"content2":"今日CS.CV计算机视觉论文速览Wed, 6 Mar 2019Totally 34 papersInteresting:📚TableBank,基于图像的文件表格检测，结构识别数据集，利用work和latex的弱监督数据，构建了417K高质量的标记表格图像数据，同时构建基准数据模型。(from 北航和微软亚洲研究院)表格数据集：表格检测表格结构识别：dataset&code&link: https://github.com/doc-analysis/TableBank相关数据集：ICDAR 2013 table 比赛数据集（128电子文本）UNLV table 数据集（427扫描文件）Marmot数据集（2000页）DeepFigure Dataset（arxiv & PubMed数据集） , ref paper📚H3D Dataset拥挤城市场景的三维全景多目标检测和跟踪数据集, 来自本田研究院的数据集，包含了多个交通参与者的数据标注，将有利于自动驾驶的开发和研究。包含了160个场景下1M个标注实例和27721帧。数据收集于旧金山、山景城、圣克鲁斯，圣马特奥。包含时间序列的标记信息：以及数据收集车：📦dataset link相关数据集：Oxford RobotCar， KITTI FlyingThikng，视差光流和场景流合成数据集📚EdgeStereo同时用于学习立体匹配和边缘数据的网络，为了克服低纹理、小细节和小物体等缺乏几何信息的小物体匹配问题，研究人员提出了多任务处理框架来学习视差图和边缘检测。提出了边缘平滑损失和特征嵌入，设计了残差金字塔模型来实现立体匹配。（from 上交）网络模型,共享了低层级架构和边缘特征：文章中的残差金字塔细节，在小尺度上估计视差图并在大尺度上进行修正：边缘检测分支的结构：一些结果：一些相关方法：相关数据集：KITTI 2012，2015，FlyingtThings3D📚实时多人点云手部检测方法，与先前使用二维关键点检测并投影到三维的方法不同，这一方法直接对动态三维点云进行处理。为了加速三维处理，首先检测出人群中每个人的bbox，然后对手部进行定位，此外还开发了基于2D的特征的候选及轨迹优化方法（基于最大重叠），并减少了三维体积中人的关键映射为高效的二维问题。（from 美国微软）限定为人再定位手：不同的视角检测和置信度累计：一些相关方法比较：数据集来自于Kinect-v2收集的300m2300m^2300m2 20个物体，一作主页📚基于五型人格模型检测人群中人的文化性格倾向，（from pucrs.br）📚无人机对室内墙布局的学习，并基于这些布局来进行室内定位(from 德国弗莱堡大学)效果：模式输入RGB和消失线，输入房间布局边缘：实验数据集：Fr078-1 (113 mlong), Fr078-2 (179 m long) and Fr080 (108 m long).📚识别玻璃和镜面,根据图像中的反射行为来测定材质(Toyohashi University of Technology)📚M-VAD Names,为视频数据集进行命名的数据集，Caption。（University of Modena and Reggio Emilia）数据集：https://github.com/aimagelab/mvad-names-dataset📚无人红外数据集视觉导航,(内华达州立)[视频](https://www.youtube.com/watch?v=aqZugneeCxc&feature=youtu.be📚用于增强图像分割的去雾算法,(from , Universite Laval)这篇文章的逻辑框图十分清晰,值得学习，分割网络损失、中间去雾损失和下面分类损失，再加上中间的判别器GAN损失，一气呵成：Daily Computer Vision Papers[1] Title: TableBank: Table Benchmark for Image-based Table Detection and RecognitionAuthors:Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou, Zhoujun Li[2] Title: MS-TCN: Multi-Stage Temporal Convolutional Network for Action SegmentationAuthors:Yazan Abu Farha, Juergen Gall[3] *Title: O-GAN: Extremely Concise Approach for Auto-Encoding Generative Adversarial NetworksAuthors:Jianlin Su[4] Title: FastReg: Fast Non-Rigid Registration via Accelerated Optimisation on the Manifold of DiffeomorphismsAuthors:Daniel Grzech, Loïc le Folgoc, Mattias P. Heinrich, Bishesh Khanal, Jakub Moll, Julia A. Schnabel, Ben Glocker, Bernhard Kainz[5] Title: Learning a smooth kernel regularizer for convolutional neural networksAuthors:Reuben Feinman, Brenden M. Lake[6] *Title: Frustum ConvNet: Sliding Frustums to Aggregate Local Point-Wise Features for Amodal 3D Object DetectionAuthors:Zhixin Wang, Kui Jia[7] Title: Virtual Ground Truth, and Pre-selection of 3D Interest Points for Improved Repeatability Evaluation of 2D DetectorsAuthors:Simon R Lang, Martin H Luerssen, David M Powers[8] Title: HexagDLy - Processing hexagonally sampled data with CNNs in PyTorchAuthors:Constantin Steppa, Tim Lukas Holch[9] **Title: Leveraging Shape Completion for 3D Siamese TrackingAuthors:Silvio Giancola, Jesus Zarzar, Bernard Ghanem[10] Title: Hue Modification Localization By Pair MatchingAuthors:Quoc-Tin Phan, Michele Vascotto, Giulia Boato[11] Title: Improve Object Detection by Data Enhancement based on Generative Adversarial NetsAuthors:Wei Jiang, Na Ying[12] *Title: Deep Learning Based Motion Planning For Autonomous Vehicle Using Spatiotemporal LSTM NetworkAuthors:Zhengwei Bai, Baigen Cai, Wei Shangguan, Linguo Chai[13] **Title: EdgeStereo: An Effective Multi-Task Learning Network for Stereo Matching and Edge DetectionAuthors:Xiao Song, Xu Zhao, Liangji Fang, Hanwen Hu[14] Title: Real-time Multiple People Hand Localization in 4D Point CloudsAuthors:Hao Jiang, Quanzeng You[15] ***Title: Using Big Five Personality Model to Detect Cultural Aspects in CrowdsAuthors:Rodolfo Migon Favaretto, Leandro Dihl, Soraia Raupp Musse, Felipe Vilanova, Angelo Brandelli Costa[16] **Title: Distinguishing mirror from glass: A ‘big data’ approach to material perceptionAuthors:Hideki Tamura, Konrad E. Prokott, Roland W. Fleming[17] Title: A DenseNet Based Approach for Multi-Frame In-Loop Filter in HEVCAuthors:Tianyi Li, Mai Xu, Ren Yang, Xiaoming Tao[18] Title: Defense Against Adversarial Images using Web-Scale Nearest-Neighbor SearchAuthors:Abhimanyu Dubey, Laurens van der Maaten, Zeki Yalniz, Yixuan Li, Dhruv Mahajan[19] Title: Unsupervised Domain-Specific Deblurring via Disentangled RepresentationsAuthors:Boyu Lu, Jun-Cheng Chen, Rama Chellappa[20] **Title: On measuring the iconicity of a faceAuthors:Prithviraj Dhar, Carlos D. Castillo, Rama Chellappa[21] **Title: The H3D Dataset for Full-Surround 3D Multi-Object Detection and Tracking in Crowded Urban ScenesAuthors:Abhishek Patil, Srikanth Malla, Haiming Gang, Yi-Ting Chen[22] Title: Unsupervised Rank-Preserving Hashing for Large-Scale Image RetrievalAuthors:Svebor Karaman, Xudong Lin, Xuefeng Hu, Shih-Fu Chang[23] Title: Selective Sensor Fusion for Neural Visual-Inertial OdometryAuthors:Changhao Chen, Stefano Rosa, Yishu Miao, Chris Xiaoxuan Lu, Wei Wu, Andrew Markham, Niki Trigoni[24] Title: Learning of Image Dehazing Models for Segmentation TasksAuthors:Sébastien de Blois, Ihsen Hedhli, Christian Gagné[25] Title: TKD: Temporal Knowledge Distillation for Active PerceptionAuthors:Mohammad Farhadi, Yezhou Yang[26] Title: Fine-grained lesion annotation in CT images with knowledge mined from radiology reportsAuthors:Ke Yan, Yifan Peng, Zhiyong Lu, Ronald M. Summers[27] *Title: M-VAD Names: a Dataset for Video Captioning with NamingAuthors:Stefano Pini, Marcella Cornia, Federico Bolelli, Lorenzo Baraldi, Rita Cucchiara[28] Title: Statistical Guarantees for the Robustness of Bayesian Neural NetworksAuthors:Luca Cardelli, Marta Kwiatkowska, Luca Laurenti, Nicola Paoletti, Andrea Patane, Matthew Wicker[29] Title: Towards Design Space Exploration and Optimization of Fast Algorithms for Convolutional Neural Networks (CNNs) on FPGAsAuthors:Afzal Ahmad, Muhammad Adeel Pasha[30] **Title: Robot Localization in Floor Plans Using a Room Layout Edge Extraction NetworkAuthors:Federico Boniardi, Abhinav Valada, Rohit Mohan, Tim Caselitz, Wolfram Burgard[31] Title: Vision-Depth Landmarks and Inertial Fusion for Navigation in Degraded Visual EnvironmentsAuthors:Shehryar Khattak, Christos Papachristos, Kostas Alexis[32] Title: Visual-Thermal Landmarks and Inertial Fusion for Navigation in Degraded Visual EnvironmentsAuthors:Shehryar Khattak, Christos Papachristos, Kostas Alexis[33] Title: The Lottery Ticket Hypothesis at ScaleAuthors:Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, Michael Carbin[34] Title: The Regretful Agent: Heuristic-Aided Navigation through Progress EstimationAuthors:Chih-Yao Ma, Zuxuan Wu, Ghassan AlRegib, Caiming Xiong, Zsolt KiraPapers from arxiv.org更多精彩请移步主页pic from pixels.com"}
{"content2":"松尾丰，作者简介：东京大学院工学系研究科副教授，1997年毕业于东京大学工学部电子信息工学科。2002年完成了该大学的博士课程，成为工程博士。同年任产业技术综合研究所研究员。2005年起任斯坦福大学客座研究员。2007起至今任工学系研究科副教授，兼任新加坡国立大学客座副教授。专业领域为人工智能、网络信息挖掘、大数据分析。日本人工智能专家之一，曾获人工智能学会颁发“论文奖”（2002年）、“创立20周年纪念事业奖”（2006年）、“现场创新奖”（2011年）、“功劳奖”（2013年）等奖项。先后在人工智能学会任多职；2012年起任编辑委员长、理事；2014年任伦理委员长。编著有《大智能时代套装》（机器人的未来、机器人新时代、机器人革命、数字法则、大智能时代）。书中对人工智能的三次人工智能的浪潮进行阐述，对三次人工智能浪潮的主要技术进行了介绍，还有各大科技公司面对人工智能浪潮采取的应对措施。当第三次人工智能浪潮来临时，我们的生活会变成什么样，如果都想人工智能专家想像中的那样，90%的事情都可以交给带有人工智能机器去完成，那我们人类剩余出来的时间又该做什么？以及针对人工智能是否会有情感，带有情感的人工智能是否意味着人类的灭亡，作者都进行了详细的分析。首先我们要摆平我们的心态：人工智能并未实现，但是没理由不实现！探索人类智能的原理，并通过工程学的方法对其进行实现和利用，这样的人工智能还没有实现。人类对于物理世界的研究从微观的原子到浩瀚的宇宙都有的较为本质上的认识，大型的强子碰撞机，宇宙飞船都是对物理世界认识的产物。然而，人类的大脑能力深奥无比、遥不可及，科学家对其探索的脚步从未停止，然而利用计算机对其进行的模型也未能实现。人工智能是什么？一下是专家给出的定义；1. 人工智能是“采用人工方法制作的、具有智能的实体，或者是以创造智能为目的的、对智能本身进项研究的领域”。2. 把类似我们很自然地接触宠物或者其他人的那种充满感情和幽默的相互作用，在与物理定律无关或者相逆的条件下，用人工方法制造出来的系统，定义为“人工智能”，这种系统采用的不是分析性的理解方法，而是通过对话等交流方式进行的交谈性理解。这就是人工智能。3. 以模仿、支持、超越人类的智能行为为目的的建构性(通过制作来进行理解)系统。与建构性对应的词是分析性，举个例子，从事体育运动的是运动员从事的是建构性理解，而体育评论家则是分析性理解。4. 采用人工方法制造的类人智能，以及其制造技术，类人指的是具有“发现和察觉功能”的计算机，即能够从数据中生成特征量。对于我们非人工智能研究着而言，人工智能分为四个级别：1.把单纯的控制程序称作“人工智能”，比如：空调，全自动洗衣机等；2.传统人工智能(引入了推理及搜索，或者知识库)，比如可以下棋的程序，智力问题求解等；3.引入机器学习的人工智能，机器学习以样本数据为基础、对规则和知识的自学习；·4.引入深度学习的人工智能，能够对机器学习时的数据表示所用变量(特征量)本身进行学习的人工智能。强人工智能：具备正确的输入与输出、被施与合理程序化的计算机，与拥有心智的人是没有任何区别的，即它也是有心智的。弱人工智能：计算机没有必要拥有心智，只要能够通过其有限的智能解决一些智力问题即可。第一次人工智能浪潮：时间：20世纪50年代------20世纪60年代概括：第一次人工智能浪潮是推理和搜索的时代代表事件：1.用搜索树搜索迷宫方法：搜索树宽度优先搜索，能够找到距离目标最短的路径，但是需要的存储量大；深度优先搜索，需要的存储量小，但是搜索时间不定，可能会很小，也可能会很大；2. 梵塔问题方法：搜索树3.机器人行动过程规划方法：搜索树4.博弈(棋类游戏)棋类游戏的组合是非常大的，对目前的计算机来说，如果采用直接搜索的方法无疑是很难满足需求的。那现在的计算机可以战胜人类的秘诀又是在哪里呢？1.能够发现更好的特征量2.“蒙特卡洛法”改变评估机制第二次人工智能浪潮：时间：20世纪80年代------1995年左右概括：第一次人工智能浪潮是知识(“专家系统”)的时代“专家系统”本身是一种程序，通过引入某个专业领域的知识，在经过推理，计算机便能够像该领域的专家一样出色地开展工作。什么是“知识表示”？对于我们每个人都熟知的知识，怎样表达才能让计算机易于处理？在这方面的基础性研究，被称为“知识表示”研究。本体研究？“本体“相当于撰写知识时的规格说明书。本体研究分为“重量级本体”和“轻量级本体”两个派别，重量级本体的支持者认为研究者需要认真考虑该怎么描述知识，并研究为此应该怎么做；轻量级本体的支持者认为，把数据输进计算机里面，并让计算机自己寻找概念之间的相关性。轻量级本体的一个极致例子就是由IBM开发的“沃森”。作者在此提到了“机器翻译”、“框架问题”和“符号接地问题”三个问题。利用导入知识的人工智能进行机器翻译，但是导入知识的机器翻译尽管可以较好地理解语言的语法，但是，精确地从语法分析往往会产生语义上的歧义，而语义的理解正是机器翻译的难点所在。“框架问题”，就是在执行某项任务时“仅仅提取出与它相关的知识并对其加以运用”，这对人类来讲很简单，但是对机器来说非常困难。“符号问题”，是否能将符号（词句、语言 ）与它表示的意义连接起来的问题，计算机以为不懂得符号的意义，所以不能把符号与其所表示的意义结合起来。第三次人工智能浪潮：时间：2000年以后至今概括：第一次人工智能浪潮是机器学习与特征表示学习的时代什么是机器学习？机器学习指人工智能程序自身进行自身学习的机理。那怎样才算是学到东西呢？学习的主要工作是进行“区分”，对某一事物进行判断和识别，就可以理解它，还能根据对该事物的判断而采取相应的行动。机器学习分为“有监督学习”和“无监督学习”，有监督学习，指的是事先需要准备好输入与正确输出想配套的训练数据，让计算机进行学习，以便当它被输入某个数据时能够得到正确的输出；无监督学习，指仅提供输入用数据、需要计算机自己找出数据内在结构的场合，目的是让计算机从数据中抽取出其中所包含的模式及规则。常用的五种“分类”方法：1.最近邻分类算法2.朴素贝叶斯算法3.决策树4.支持向量机5.人工神经网络机器学习的难点（弱点）是特征工程，即特征量的设计。计算机并不能做出选取特征量的判断。提高机器学习的精确度的关键在于“输入何种特征量”，然而这只有靠人用大脑思维来解决。到目前为止人工智能之所以尚未实现，就是因为人工智能在“从这个世界里面应该关注何种特征并提取信息”这点上，还必须借助人的力量。如果计算机能够从被导入的数据里面找出应该关注的特征，并得到表示这种特征程度的特征量，那么机器学习的“特征量设计”问题也将被解决。深度学习，恰好可以解决这个问题。深度学习深度学习以数据为基础，由计算机自动生成特征量，它不需要由人来设计特征量，而是由计算机自动获取高层特征量。自动编码器：输入与输出相同深度学习与之前的机器学习相比有两个较大的不同点：一是需要一层一层地逐层学习；二是深度学习使用一种被称为“自动编码器”的“信息压缩器”。自动编码器所执行的处理与众不同，它将“输出”和“输入”做成相同的数据，与“主成分分析”具有同样的工作原理，但是自动编码可以进行“深层”即多层次操作，可以提取出主成分分析无法提取出的高层特征量。下图为深度学习结构，从数据里面找出并生成概念，本身是不需要“教师数据”的无监督学习，深度学习在进行无监督学习的时候采用的是有监督学习的方法。自动编码器，在本来应该有教师提供正解的地方输入原来的数据，以此对输入数据本身进行预测，再生成各种各样的特征量，这就是通过有监督学习的方式进行无监督学习。Google的“猫脸识别”研究，处理1000万张图像，使用的神经元之间的链接超过100亿个的巨型神经网络，用1000台计算机（16000个处理器）连续运行3天，就是通过“采用有监督的学习方法实现无监督学习”生成特征量，即提取出“猫脸”的概念，此时，再赋予“猫脸”的名称，即完成了符号（名称和概念的结合）接地的问题，在最后区分的时候采用有监督的学习的分类方法。深度学习的关键------“鲁棒性”实际上，提取特征量或者概念需要相当长时间的“打造和提炼”过程，只有这样，才能使所获取的特征量或者概念具有鲁棒性（“健壮性”）。如何做到深度神经网络的“鲁棒性”呢？其实是需要在输入型号里面加入“噪声”，通过反复加入噪声后获取的概念，就不会因为一点风吹草动就摇摆不定。(听起来有些矛盾，但又何尝不是这样呢)。鲁邦性的提高与计算机的处理性能有较大的关系。增加鲁棒性的方法1.加入噪声制作“略微不同的过去”的做法；2.dropout方法，让神经网络的一部分神经元停止工作，即让隐层50%的神经元出现任意性缺损。对特征项目进行最优化处理，以便让某个特征量能够覆盖其他特征量，这样，特征表示就不会出现过度依赖某一个特征量的情况。过度依赖仅有的某一特征量是非常危险的，让一部分特征量不能使用，对于发现恰当的特征表示是很有帮助的。除此之外，还有很多专家在研究各种各样的针对神经网络鲁棒性的方法，因为如果不使劲“折磨”它，就无法获取存在于数据背后的“本质特征量”。深度学习之后的技术发展1.能够对图像特征进行抽象化处理的人工智能，能够实现多模态抽象化的人工智能，图像处理相当于人类的视觉，还有听觉、触觉等信息待处理；2.能够对行动与结果进行抽象化的人工智能，目前人工智能还是停留在对外界事物进行观察的地步，如何与外界进行交互也是未来的研究方向；3.能够通过行动获取特征量的人工智能，通过与外界的交互作用获取新的特征量，类似于，人们根据多次的实验突然间意识到的某个特征量或者窍门，下次遇到同样的事情就会想到这个窍门；4.能够进行语言理解和自动翻译的人工智能，解决符号落地问题；5.能够获取知识的人工智能，使人工智能具有想像力；讨论了人工智能的技术问题以及未来的发展，就得说说人工智能的社会性问题了。人工智能是否具有本能？人工智能是否具有创造力？人工智能的社会性意义？人类具有群居性的动物，那人工之能呢？奇点会发生吗？奇点，指的是人工智能能够自动地制造出超越自身能力的人工智能的那个时点。如果人工智能妄想征服分类，有哪些方式？对于这些问题作者也进行了论述，总结一句话，人工智能必须造福于人类。人工智能在对人类生产生活的影响：1.广告、图像诊断、网络企业；2.个人机器人、安全防范、大数据运用企业；3.汽车制造、交通、物流、农业；4.家政、医疗护理、接待及呼叫中心；5.翻译及全球化；6.教学、秘书、白领工作辅助；"}
{"content2":"目录(?)[-]国际顶级会议论文搜索学术牛人主页学术期刊国际顶级会议AAAICIKM 2010CIKM 2011COLT 2010COLT 2011Computer Vision ResourceICJIAICMLNIPSSIGIR 2010SIGIR 2011SIGKDDSIGKDD2010论文搜索CV顶级会议论文下载google 学术搜索超全计算机视觉资源汇总联合参考文献学术牛人主页feifei li -computer visionGooglers in Machine LearningMichael I. JordanMicrosoft Researchmit leozhu cvpff cvYahoo! Researchzhangzhang si国外人工智能界牛人主页计算机视觉相关资源牛人（周志华）推荐的人工智能网站数据挖掘牛人 一览谈机器学习(Machine Learning)大牛周志华学术期刊IEEE Transactions Pattern Analysis and Machine IntelligenceACM Transactions on Knowledge Discovery from DataAmerican Statistical Association - Journal of the American Statistical AssociationAnnals of StatisticsArtificial IntelligentComputer Vision and Image UnderstandingData Mining and Knowledge DiscoveryIEEE Knowledge and Data EngineeringIEEE T. on Information TheoryIEEE T. on Neural NetworksIEEE T. on Systems Machine and CyberneticsImage Processing, IEEE Transactions onImage Vision ComputingInternational Journal of Computer VisionJournal of Machine Learning ResearchJournal of the Royal Statistical Society: Series B (Statistical Methodology)Machine LearningNeural ComputationPattern Analysis and ApplicationsPattern Recognition"}
{"content2":"CCF ADL27：《视觉模式识别讲习班》开始报名-中国计算机学会信息网CCF ADL27：《视觉模式识别讲习班》开始报名 欢迎参加CCF 《学科前沿讲习班》 CCF ADL 第27期 主题 视觉模式识别 2012年5月18-20日北京   视觉模式识别是计算机学科近年来的研究热点，也是各种智能应用的关键技术，如视频监控、图像搜索、移动增强现实等。这是一个跨学科的领域，牵涉到机器学习、统计计算、优化算法、图像处理、模式识别等。视觉模式识别正处在一个基础研究和应用研究爆发的时期。随着智能设备和物联网的不断发展，视觉模式识别具有更广阔的研究和应用前景。   本期CCF学科前沿讲习班《视觉模式识别》，邀请到该领域知名的专家学者做主题报告，希望能对从事该领域的学生、老师和工业界研究人员有所帮助。本讲习班将对视觉模式识别的热点理论和方法、重点应用研究进行介绍，使参加者在了解学科热点、提高学术水平的同时，增加交流机会和实践体验。“工欲善其事，必先利其器”，我们相信这次讲习班将有助于大家未来的研究工作。   学术主任：谭铁牛　中科院自动化所研究员 　　　　　张益民　英特尔中国研究院研究员 　　　　　王　亮　中科院自动化所研究员 协办单位：模式识别国家重点实验室 英特尔中国研究院   详情请点击：《CCF视觉模式识别讲习班简介》   特邀讲者 l  Prof Yi Ma 2000年美国加州伯克利大学获得博士学位。目前是微软亚洲研究院主任研究员，美国UIUC大学副教授。1999年马尔奖（Marr Prize，计算机视觉领域最高奖项之一）得主。主要研究方向是计算机视觉和高维数据分析。 报告题目：Robust Principal Component Analysis: Exact Recovery of Corrupted Low-Rank Matrices via Convex Optimization l  周志华 教授 南京大学教授，教育部长江学者特聘教授、国家杰出青年基金获得者，主要研究领域为机器学习、数据挖掘、模式识别。《科学通报》副主编、《IEEE Trans Knowledge and Data Engineering》、《ACM Trans Intelligent Systems and Technology》、《中国科学》等十余种国内外学术期刊编委，二十余次国际会议程序主席或领域主席，CCF人工智能与模式识别专委会主任，中国人工智能学会机器学习专委会主任，IEEE计算智能学会数据挖掘技术委员会副主席。 报告题目：未标记与多标记样本的学习 l  Associate Prof. Jianbo Shi  University of Pennsylvania 1994年于康奈尔大学获得计算机科学与数学学士学位，1998年于加州伯克利大学获得计算机科学博士学位。1999年加入CMU的机器人研究所，领导HumanID项目研究。2003年至今，他在宾夕法尼亚大学工作，目前是计算机与信息科学系副教授。他的主要的研究兴趣包括人的行为分析、图像识别与分割、图像/视频检索等。 报告题目： Image Segmentation and Contour Grouping for Deformable Shape Detection l  王立威 博士 北京大学机器智能系副教授，主要研究领域为机器学习理论。已在JMLR, PAMI, NIPS, COLT, ICML, and CVPR等顶级的期刊和会议上发表30余篇论文。他于2010年被IEEE Intelligent System提名为AI’s 10 to Watch. 报告题目:  Probabilistic Graphical Models: Representation, Inference and Learning， 　　l Prof. Long Quan 香港科技大学教授，1989年在法国INPL获博士学位，在加入香港科技大学任教之前，他自1990年在法国INRIA(位于Grenoble)一直担任CNRS高级研究科学家。主要研究领域为三维重构，structure from motion, 基于图像的建模。 报告题目：Image-based Modeling l  陈熙霖  中国科学院计算技术研究所研究员 国家杰出青年基金获得者，中科院“百人计划”入选者并获终期评估优秀，新世纪百千万人才工程国家级人选。研究兴趣包括计算机视觉、模式识别、图像处理以及多媒体和数字视频广播等。目前是IEEE Transactions on Image Processing的Associate Editor，《JCST》领域编委，Frontiers of Computer Science in China和《计 算机学报》编委，先后担任过FG 2013 General Chair, ICMI 2010的Program Chair、FG2011的Tutorial Chair、ICMI 2009的Workshop Chair、ACM MM 2009和ICME 2007的Local Chair 以及ICMI 2006的Demo Chair。其研究工作先获得过三次国家科技进步奖，作为（共同）作者在刊物和会议上发表论文200多篇。 报告题目：人脸识别——问题、进展与挑战 l  Xiaogang Wang  香港科技大学助理教授 于中国科学技术大学少年班获得电子工程与信息科学学士学位，香港中文大学获得信息工程硕士学位，美国麻省理工学院获得计算机科学博士学位。自从2009年8月，他是香港中文大学电子工程系的助理教授。他是ICCV 2001的Area Chair，并且在2011年荣获自动人的行为分析研究优秀青年科学家奖。他的研究兴趣包括计算机视觉、机器学习和医学视觉计算。 报告题目：Human Behavior Analysis in Crowded Environment l  薛向阳  复旦大学计算机学院教授 现为IEEE/ACM会员，CCF理事，上海市计算机学会副理事长，中国图形图像学会多媒体专委会副主任。现担任《计算机研究与发展》、《计算机科学与探索》和《IEEE Transaction on Autonomous Mental Development》编委。主要研究图像视频分析与检索。 报告题目：图像标注   　 特邀讲者 l  谭铁牛 研究员 博士生导师。1984年获西安交通大学学士学位，1986年和1989年分别获英国帝国理工学院硕士与博士学位。1989-1997年在英国雷丁大学计算机科学系工作，1998年回国到中科院自动化所工作，历任研究所所长助理、所长。现为该所模式识别国家重点实验室主任。 谭铁牛博士主要从事图像处理、计算机视觉和模式识别等相关领域的研究工作。现已出版编（专）著11部、发表论文350多篇、获准和申请发明专利50项。他现为 IEEE 生物识别理事会候任主席、国际模式识别学会副主席、中国人工智能学会副理事长。现为或曾为国际权威期刊IEEE T-PAMI、IEEE T-CSVT、PR等多个国际刊物的编委。他是IEEE Fellow、IAPR Fellow、国际生物识别学术会议(ICB)和亚洲模式识别学术会议(ACPR)创始主席等。曾获中国青年五四奖章、中国青年科技奖以及国家自然科学二等奖、国家技术发明二等奖和国家科技进步二等奖各1项。 l  张益民 博士 张益民是英特尔中国研究院嵌入式应用实验室高级主任研究员及实验室主任。他的研究团队负责对各种新兴嵌入式应用进行研究，最近几年的研究重点主要在计算机视觉，多媒体分析及其在智能嵌入式设备的应用和硬件加速方面，如人脸识别、物体识别、视频挖掘等。他是中国计算机学会高级会员，现任理事。他于1999年从上海交通大学获计算机学科的博士学位。 l  王亮 研究员 博士生导师，IEEE高级会员。2010年入选中国科学院“百人计划”，现为中科院自动化所模式识别国家重点实验室研究员。主要从事图像处理、机器学习、计算机视觉和模式识别等相关领域的研究工作。已出版编（专）著5部，发表学术论文80多篇，并被广泛引用（GoogleCitation 3600余次）。他现为《IEEE Transactions on System, Man and Cybernetics – Part B》、《Neurocomputing》等国际学术刊物的编委，曾获得中科院院长奖学金特别奖、中科院优秀博士论文及全国百篇优秀博士论文提名奖等荣誉。  日程安排 2012年5月18日     9:00-9:30　　开班仪式、合影 09:30-12:30　Robust Principal Component Analysis: Exact Recovery of Corrupted Low-Rank 　　　　　　Matrices via Convex Optimization 　　　　　　Yi Ma, 美国UIUC大学副教授 12:30-14:00    午餐 14:00-17:00    未标记与多标记样本的学习 　　　　　　周志华，南京大学教授   2012年5月19日    09:00-12:00   Image Segmentation and Contour Grouping for Deformable Shape Detection              　　Jianbo Shi, 美国宾夕法尼亚大学副教授 12:00-14:00    午餐 14:00-17:00    Probabilistic Graphical Models: Representation, Inference and Learning                          王立威，北京大学副教授   2012年5月20日    09:00-10:30    人脸识别——问题、进展与挑战              　　陈煕霖, 中科院计算技术研究所研究员 10:30-12:00    Image-based Modeling              　　Long Quan, 香港科技大学教授 12:00-14:00    午餐 14:00-15:30    Human Behavior Analysis in Crowded Environment               　Xiaogang Wang, 香港中文大学助理教授 15:30-17:00    图像标注               　薛向阳，复旦大学教授 17:00-17:20    结业式   注册费： 1、开班三个月前注册并缴费： 非会员：1300元。 CCF会员：900元（开班日三个月前入会的CCF会员享受此优惠）。 　　CCF团体会员单位的人士参加，按CCF会员标准收费（按单位办理）。 2、4月30日之后注册并缴费：会员1300元，非会员1300元。 　　3、开班前入会者，注册费1100元。 4、现场缴费：1500元。 注：注册费含资料和3天的午餐。   优惠办法： 1、同一单位一次有5人报名者，第六个人免注册费（无论会员与否，仅对提前注册者有效，当天不予受理）。 2、2011年参加过2次讲习班的CCF会员可优惠100元。 3、2012年参加3次讲习班的CCF会员，第4次参加时免交注册费。 4、往届学员推荐一名新学员时，推荐者当期注册费优惠100元。 5、同时满足以上多项优惠条款时，只能选择一项。 食宿自理   缴费方式： 邮寄：北京2704信箱，邮编：100190 收款人：中国计算机学会， 银行转账：开户行：北京银行北京大学支行；户名：中国计算机学会 01090519 5001 201 097 020 28 请务必注明：姓名，ADL27 报名方式： 即日起至2012年4月30日，报名者请填写附表并发送至：ccf-adl@ccf.org.cn，按报名先后录取。学会秘书处将与邮寄联系确认。 联系人：余遐 E-Mail: ccf-adl@ccf.org.cn 电话：010-6256 2503-22 /139 1065 9011 地址：北京科学院南路6号计算所大楼336室 CCF ADL27报名表 视觉模式识别     姓名 　 性别 　 任职单位 　 职称 　 是否CCF会员1 　 会员号 　 手机 　 Email 住宿2 （如需安排） 入住时间： 离开时间： 单住： 合住： 发票抬头3 发票项目内容4 √ □注册费□会议费□会务费□培训费 参加本期讲习班的目的： 信息来源：√ （请注明） □CCF周刊 □CCF网页 □《CCCF》 □熟人介绍 □单位通告 □其它　　　　 我申请参加本届研究峰会并承诺按主办单位的规定参加。                 说明 　　1、会员号：不填写会员号，按非会员处理，对会员优惠，仅对开班前三个月前入会者有效。 　　2、仅需要组织者代位安排住宿是，填写“安排住宿”一栏。 　　3、发票：“发票付款单位”如空，则认为同“任职单位”。 　　4、发票项目如不选择，则认为是“会议费”。 　　5、 对会员优惠400元，仅对开班前三个月前入会者有效。如果正在申请入会，请填写“正在办理”，享受200元优惠。"}
{"content2":"第一篇博客——我的大学-选择学校当初高考出分后，脑子里什么想法都没有，不知道应该选择什么学校，应该学什么专业，跟家长讨论后，家里人都支持我学计算机专业，因为我父亲就是这个专业的，而且计算机的就业前景也很不错，所以选择了沈阳航空航天大学计算机专业。我不后悔选择沈阳航空航天大学这个学校，沈航校园环境优美，师资能力很强，学校配套设施完善，体育馆图书馆非常实用，沈航计算机专业是一个很好的选择，我和家人都非常满意。***———————————————————————————————————————————————-最喜欢的领域计算机是我目前做喜欢的领域，如果条件允许，我会考取计算机专业的研究生，我想在大学毕业后，跟更加优秀的老师学习计算机——尤其是我对“人工智能”非常感兴趣，想要更加深入学习这项技术。人工智能是一门极富挑战性的科学，从事这项工作的人必须懂得计算机知识，心理学和哲学。人工智能是包括十分广泛的科学，它由不同的领域组成，如机器学习计算机视觉等等，总的说来，人工智能研究的一个主要目标是使机器能够胜任一些通常需要人类智能才能完成的复杂工作。我感觉这是未来科技的大趋势，是人类进步的重要科学技术，是新一代计算机工作者应该努力的方向。***———————————————————————————————————————————————-知识技能我对计算机原理和程序设计方面的了解还不是很多，只是学习了学校教学的课程，没有自己拓展知识学习，在大学时期没有好好学习专业技能是我人生的一大遗憾，所以我想继续学习，想要争取更多机会去学习计算机知识和技术。***———————————————————————————————————————————————-畅想未来未来计算机发展前景一定会越来越好，会有越来越多的技术出现供我们学习和研究，在未来的学习和生活中我会更加努力去学习，争取考上研究生，跟老师好好研究计算机技术，对得起父母，对得起国家，作一个对社会有用的人。"}
{"content2":"《Python计算机视觉编程》基本信息作者： (美)Jan Erik Solem译者： 朱文涛 袁勇丛书名： 图灵程序设计丛书出版社：人民邮电出版社ISBN：9787115352323上架时间：2014-6-10出版日期：2014 年7月开本：16开页码：1版次：1-1所属分类：计算机 > 软件与程序设计 > Python更多关于》》》《Python计算机视觉编程》编辑推荐Amazon.com计算机视觉类图书第一名！专门用Python讲解计算机视觉编程内容简介书籍计算机书籍《python计算机视觉编程》是计算机视觉编程的权威实践指南，依赖python语言讲解了基础理论与算法，并通过大量示例细致分析了对象识别、基于内容的图像搜索、光学字符识别、光流法、跟踪、三维重建、立体成像、增强现实、姿态估计、全景创建、图像分割、降噪、图像分组等技术。另外，书中附带的练习还能让读者巩固并学会应用编程知识。《python计算机视觉编程》适合的读者是：有一定编程与数学基础，想要了解计算机视觉的基本理论与算法的学生，以及计算机科学、信号处理、物理学、应用数学和统计学、神经生理学、认知科学等领域的研究人员和从业者。媒体评论“本书介绍了各种图像分析工具，是了解计算机视觉编程的‘必备’读物。”——James A. Cox，美国《中西部书评》（Midwest Book Review）总编辑目录《python计算机视觉编程》推荐序 xi前言 xiii第1章　基本的图像操作和处理 11.1　pil：python图像处理类库 11.1.1　转换图像格式 21.1.2　创建缩略图 31.1.3　复制和粘贴图像区域 31.1.4　调整尺寸和旋转 31.2　matplotlib 41.2.1　绘制图像、点和线 41.2.2　图像轮廓和直方图 61.2.3　交互式标注 71.3　numpy 81.3.1　图像数组表示 81.3.2　灰度变换 91.3.3　图像缩放 111.3.4　直方图均衡化 111.3.5　图像平均 131.3.6　图像的主成分分析（pca） 141.3.7　使用pickle模块 161.4　scipy 171.4.1　图像模糊 181.4.2　图像导数 191.4.3　形态学：对象计数 221.4.4　一些有用的scipy模块 231.5　高级示例：图像去噪 24练习 28代码示例约定 29第2章　局部图像描述子 312.1　harris角点检测器 312.2　sift（尺度不变特征变换） 392.2.1　兴趣点 392.2.2　描述子 392.2.3　检测兴趣点 402.2.4　匹配描述子 432.3　匹配地理标记图像 472.3.1　从panoramio下载地理标记图像 472.3.2　使用局部描述子匹配 502.3.3　可视化连接的图像 52练习 54第3章　图像到图像的映射 573.1　单应性变换 573.1.1　直接线性变换算法 593.1.2　仿射变换 603.2　图像扭曲 613.2.1　图像中的图像 633.2.2　分段仿射扭曲 673.2.3　图像配准 703.3　创建全景图 763.3.1　ransac 773.3.2　稳健的单应性矩阵估计 783.3.3　拼接图像 81练习 84第4章　照相机模型与增强现实 854.1　针孔照相机模型 854.1.1　照相机矩阵 864.1.2　三维点的投影 874.1.3　照相机矩阵的分解 894.1.4　计算照相机中心 904.2　照相机标定 914.3　以平面和标记物进行姿态估计 934.4　增强现实 974.4.1　pygame和pyopengl 974.4.2　从照相机矩阵到opengl格式 984.4.3　在图像中放置虚拟物体 1004.4.4　综合集成 1024.4.5　载入模型 104练习 106第5章　多视图几何 1075.1　外极几何 1075.1.1　一个简单的数据集 1095.1.2　用matplotlib绘制三维数据 1115.1.3　计算f：八点法 1125.1.4　外极点和外极线 1135.2　照相机和三维结构的计算 1165.2.1　三角剖分 1165.2.2　由三维点计算照相机矩阵 1185.2.3　由基础矩阵计算照相机矩阵 1205.3　多视图重建 1225.3.1　稳健估计基础矩阵 1235.3.2　三维重建示例 1255.3.3　多视图的扩展示例 1295.4　立体图像 130练习 135第6章　图像聚类 1376.1　k-means聚类 1376.1.1　scipy聚类包 1386.1.2　图像聚类 1396.1.3　在主成分上可视化图像 1406.1.4　像素聚类 1426.2　层次聚类 1446.3　谱聚类 152练习 157第7章　图像搜索 1597.1　基于内容的图像检索 1597.2　视觉单词 1607.3　图像索引 1647.3.1　建立数据库 1647.3.2　添加图像 1657.4　在数据库中搜索图像 1677.4.1　利用索引获取候选图像 1687.4.2　用一幅图像进行查询 1697.4.3　确定对比基准并绘制结果 1717.5　使用几何特性对结果排序 1727.6　建立演示程序及web应用 1767.6.1　用cherrypy创建web应用 1767.6.2　图像搜索演示程序 176练习 179第8章　图像内容分类 1818.1　k邻近分类法（knn） 1818.1.1　一个简单的二维示例 1828.1.2　用稠密sift作为图像特征 1858.1.3　图像分类：手势识别 1878.2　贝叶斯分类器 1908.3　支持向量机 1958.3.1　使用libsvm 1968.3.2　再论手势识别 1988.4　光学字符识别 1998.4.1　训练分类器 2008.4.2　选取特征 2008.4.3　多类支持向量机 2018.4.4　提取单元格并识别字符 2028.4.5　图像校正 205练习 206第9章　图像分割 2099.1　图割（graph cut） 2099.1.1　从图像创建图 2119.1.2　用户交互式分割 2169.2　利用聚类进行分割 2189.3　变分法 224练习 226第10章　opencv 22710.1　opencv的python接口 22710.2　opencv基础知识 22810.2.1　读取和写入图像 22810.2.2　颜色空间 22810.2.3　显示图像及结果 22910.3　处理视频 23210.3.1　视频输入 23210.3.2　将视频读取到numpy数组中 23410.4　跟踪 23410.4.1　光流 23510.4.2　lucas-kanade算法 23710.5　更多示例 24310.5.1　图像修复 24310.5.2　利用分水岭变换进行分割 24410.5.3　利用霍夫变换检测直线 245练习 246附录a　安装软件包 247a.1　numpy和scipy 247a.1.1　windows 247a.1.2　mac os x 247a.1.3　linux 248a.2　matplotlib 248a.3　pil 248a.4　libsvm 249a.5　opencv 249a.5.1　windows 和 unix 249a.5.2　mac os x 249a.5.3　linux 250a.6　vlfeat 250a.7　pygame 250a.8　pyopengl 250a.9　pydot 251a.10　python-graph 251a.11　simplejson 252a.12　pysqlite 252a.13　cherrypy 252附录b　图像集 253b.1　flickr 253b.2　panoramio 254b.3　牛津大学视觉几何组 255b.4　肯塔基大学识别基准图像 255b.5　其他 256b.5.1　prague texture segmentation datagenerator与基准 256b.5.2　微软研究院grab cut数据集 256b.5.3　caltech 101 256b.5.4　静态手势数据库 256b.5.5　middlebury stereo数据集 256附录c　图片来源 257c.1　来自flickr的图像 257c.2　其他图像 258c.3　插图 258参考文献 259索引 263本图书信息来源:互动出版网"}
{"content2":"一点背景知识OpenCV 是一个开源的计算机视觉和机器学习库。它包含成千上万优化过的算法，为各种计算机视觉应用提供了一个通用工具包。根据这个项目的关于页面，OpenCV 已被广泛运用在各种项目上，从谷歌街景的图片拼接，到交互艺术展览的技术实现中，都有 OpenCV 的身影。OpenCV 起始于 1999 年 Intel 的一个内部研究项目。从那时起，它的开发就一直很活跃。进化到现在，它已支持如 OpenCL 和 OpenGL 等现代技术，也支持如 iOS 和 Android 等平台。1999 年，半条命发布后大红大热。Intel 奔腾 3 处理器是当时最高级的 CPU，400-500 MHZ 的时钟频率已被认为是相当快。2006 年 OpenCV 1.0 版本发布的时候，当时主流 CPU 的性能也只和 iPhone 5 的 A6 处理器相当。尽管计算机视觉从传统上被认为是计算密集型应用，但我们的移动设备性能已明显地超出能够执行有用的计算机视觉任务的阈值，带着摄像头的移动设备可以在计算机视觉平台上大有所为。在本文中，我会从一个 iOS 开发者的视角概述一下 OpenCV，并介绍一点基础的类和概念。随后，会讲到如何集成 OpenCV 到你的 iOS 项目中以及一些 Objective-C++ 基础知识。最后，我们会看一个 demo 项目，看看如何在 iOS 设备上使用 OpenCV 实现人脸检测与人脸识别。OpenCV 概述概念OpenCV 的 API 是 C++ 的。它由不同的模块组成，这些模块中包含范围极为广泛的各种方法，从底层的图像颜色空间转换到高层的机器学习工具。使用 C++ API 并不是绝大多数 iOS 开发者每天都做的事，你需要使用 Objective-C++ 文件来调用 OpenCV 的函数。 也就是说，你不能在 Swift 或者 Objective-C 语言内调用 OpenCV 的函数。 这篇 OpenCV 的 iOS 教程告诉你只要把所有用到 OpenCV 的类的文件后缀名改为 .mm 就行了，包括视图控制器类也是如此。这么干或许能行得通，却不是什么好主意。正确的方式是给所有你要在 app 中使用到的 OpenCV 功能写一层 Objective-C++ 封装。这些 Objective-C++ 封装把 OpenCV 的 C++ API 转化为安全的 Objective-C API，以方便地在所有 Objective-C 类中使用。走封装的路子，你的工程中就可以只在这些封装中调用 C++ 代码，从而避免掉很多让人头痛的问题，比如直接改文件后缀名会因为在错误的文件中引用了一个 C++ 头文件而产生难以追踪的编译错误。OpenCV 声明了命名空间 cv，因此 OpenCV 的类的前面会有个 cv:: 前缀，就像 cv::Mat、 cv::Algorithm 等等。你也可以在 .mm 文件中使用 using namespace cv 来避免在一堆类名前使用 cv:: 前缀。但是，在某些类名前你必须使用命名空间前缀，比如 cv::Rect 和 cv::Point，因为它们会跟定义在 MacTypes.h 中的 Rect 和 Point 相冲突。尽管这只是个人偏好问题，我还是偏向在任何地方都使用 cv:: 以保持一致性。模块下面是在官方文档中列出的最重要的模块。core：简洁的核心模块，定义了基本的数据结构，包括稠密多维数组 Mat 和其他模块需要的基本函数。imgproc：图像处理模块，包括线性和非线性图像滤波、几何图像转换 (缩放、仿射与透视变换、一般性基于表的重映射)、颜色空间转换、直方图等等。video：视频分析模块，包括运动估计、背景消除、物体跟踪算法。calib3d：包括基本的多视角几何算法、单体和立体相机的标定、对象姿态估计、双目立体匹配算法和元素的三维重建。features2d：包含了显著特征检测算法、描述算子和算子匹配算法。objdetect：物体检测和一些预定义的物体的检测 (如人脸、眼睛、杯子、人、汽车等)。ml：多种机器学习算法，如 K 均值、支持向量机和神经网络。highgui：一个简单易用的接口，提供视频捕捉、图像和视频编码等功能，还有简单的 UI 接口 (iOS 上可用的仅是其一个子集)。gpu：OpenCV 中不同模块的 GPU 加速算法 (iOS 上不可用)。ocl：使用 OpenCL 实现的通用算法 (iOS 上不可用)。一些其它辅助模块，如 Python 绑定和用户贡献的算法。基础类和操作OpenCV 包含几百个类。为简便起见，我们只看几个基础的类和操作，进一步阅读请参考全部文档。过一遍这几个核心类应该足以对这个库的机理产生一些感觉认识。cv::Matcv::Mat 是 OpenCV 的核心数据结构，用来表示任意 N 维矩阵。因为图像只是 2 维矩阵的一个特殊场景，所以也是使用 cv::Mat 来表示的。也就是说，cv::Mat 将是你在 OpenCV 中用到最多的类。一个 cv::Mat 实例的作用就像是图像数据的头，其中包含着描述图像格式的信息。图像数据只是被引用，并能为多个 cv::Mat 实例共享。OpenCV 使用类似于 ARC 的引用计数方法，以保证当最后一个来自 cv::Mat 的引用也消失的时候，图像数据会被释放。图像数据本身是图像连续的行的数组 (对 N 维矩阵来说，这个数据是由连续的 N-1 维数据组成的数组)。使用 step[] 数组中包含的值，图像的任一像素地址都可通过下面的指针运算得到：uchar *pixelPtr = cvMat.data + rowIndex * cvMat.step[0] + colIndex * cvMat.step[1]每个像素的数据格式可以通过 type() 方法获得。除了常用的每通道 8 位无符号整数的灰度图 (1 通道，CV_8UC1) 和彩色图 (3 通道，CV_8UC3)，OpenCV 还支持很多不常用的格式，例如 CV_16SC3 (每像素 3 通道，每通道使用 16 位有符号整数)，甚至 CV_64FC4 (每像素 4 通道，每通道使用 64 位浮点数)。cv::AlgorithmAlgorithm 是 OpenCV 中实现的很多算法的抽象基类，包括将在我们的 demo 工程中用到的 FaceRecognizer。它提供的 API 与苹果的 Core Image 框架中的 CIFilter 有些相似之处。创建一个 Algorithm 的时候使用算法的名字来调用 Algorithm::create()，并且可以通过 get() 和 set()方法来获取和设置各个参数，这有点像是键值编码。另外，Algorithm 从底层就支持从/向 XML 或 YAML 文件加载/保存参数的功能。在 iOS 上使用 OpenCV添加 OpenCV 到你的工程中集成 OpenCV 到你的工程中有三种方法：使用 CocoaPods 就好： pod \"OpenCV\"。下载官方 iOS 框架发行包，并把它添加到工程里。从 GitHub 拉下代码，并根据教程自己编译 OpenCV 库。Objective-C++如前面所说，OpenCV 是一个 C++ 的 API，因此不能直接在 Swift 和 Objective-C 代码中使用，但能在 Objective-C++ 文件中使用。Objective-C++ 是 Objective-C 和 C++ 的混合物，让你可以在 Objective-C 类中使用 C++ 对象。clang 编译器会把所有后缀名为 .mm 的文件都当做是 Objective-C++。一般来说，它会如你所期望的那样运行，但还是有一些使用 Objective-C++ 的注意事项。内存管理是你最应该格外注意的点，因为 ARC 只对 Objective-C 对象有效。当你使用一个 C++ 对象作为类属性的时候，其唯一有效的属性就是 assign。因此，你的 dealloc 函数应确保 C++ 对象被正确地释放了。第二重要的点就是，如果你在 Objective-C++ 头文件中引入了 C++ 头文件，当你在工程中使用该 Objective-C++ 文件的时候就泄露了 C++ 的依赖。任何引入你的 Objective-C++ 类的 Objective-C 类也会引入该 C++ 类，因此该 Objective-C 文件也要被声明为 Objective-C++ 的文件。这会像森林大火一样在工程中迅速蔓延。所以，应该把你引入 C++ 文件的地方都用 #ifdef __cplusplus 包起来，并且只要可能，就尽量只在 .mm 实现文件中引入 C++ 头文件。要获得更多如何混用 C++ 和 Objective-C 的细节，请查看 Matt Galloway 写的这篇教程。Demo：人脸检测与识别现在，我们对 OpenCV 及如何把它集成到我们的应用中有了大概认识，那让我们来做一个小 demo 应用：从 iPhone 的摄像头获取视频流，对它持续进行人脸检测，并在屏幕上标出来。当用户点击一个脸孔时，应用会尝试识别这个人。如果识别结果正确，用户必须点击 “Correct”。如果识别错误，用户必须选择正确的人名来纠正错误。我们的人脸识别器就会从错误中学习，变得越来越好。本 demo 应用的源码可从 GitHub 获得。视频拍摄OpenCV 的 highgui 模块中有个类，CvVideoCamera，它把 iPhone 的摄像机抽象出来，让我们的 app 通过一个代理函数 - (void)processImage:(cv::Mat&)image 来获得视频流。CvVideoCamera 实例可像下面这样进行设置：CvVideoCamera *videoCamera = [[CvVideoCamera alloc] initWithParentView:view]; videoCamera.defaultAVCaptureDevicePosition = AVCaptureDevicePositionFront; videoCamera.defaultAVCaptureSessionPreset = AVCaptureSessionPreset640x480; videoCamera.defaultAVCaptureVideoOrientation = AVCaptureVideoOrientationPortrait; videoCamera.defaultFPS = 30; videoCamera.grayscaleMode = NO; videoCamera.delegate = self;摄像头的帧率被设置为 30 帧每秒， 我们实现的 processImage 函数将每秒被调用 30 次。因为我们的 app 要持续不断地检测人脸，所以我们应该在这个函数里实现人脸的检测。要注意的是，如果对某一帧进行人脸检测的时间超过 1/30 秒，就会产生掉帧现象。人脸检测其实你并不需要使用 OpenCV 来做人脸检测，因为 Core Image 已经提供了 CIDetector 类。用它来做人脸检测已经相当好了，并且它已经被优化过，使用起来也很容易：CIDetector *faceDetector = [CIDetector detectorOfType:CIDetectorTypeFace context:context options:@{CIDetectorAccuracy: CIDetectorAccuracyHigh}]; NSArray *faces = [faceDetector featuresInImage:image];从该图片中检测到的每一张面孔都在数组 faces 中保存着一个 CIFaceFeature 实例。这个实例中保存着这张面孔的所处的位置和宽高，除此之外，眼睛和嘴的位置也是可选的。另一方面，OpenCV 也提供了一套物体检测功能，经过训练后能够检测出任何你需要的物体。该库为多个场景自带了可以直接拿来用的检测参数，如人脸、眼睛、嘴、身体、上半身、下半身和笑脸。检测引擎由一些非常简单的检测器的级联组成。这些检测器被称为 Haar 特征检测器，它们各自具有不同的尺度和权重。在训练阶段，决策树会通过已知的正确和错误的图片进行优化。关于训练与检测过程的详情可参考此原始论文。当正确的特征级联及其尺度与权重通过训练确立以后，这些参数就可被加载并初始化级联分类器了：// 正面人脸检测器训练参数的文件路径 NSString *faceCascadePath = [[NSBundle mainBundle] pathForResource:@\"haarcascade_frontalface_alt2\" ofType:@\"xml\"]; const CFIndex CASCADE_NAME_LEN = 2048; char *CASCADE_NAME = (char *) malloc(CASCADE_NAME_LEN); CFStringGetFileSystemRepresentation( (CFStringRef)faceCascadePath, CASCADE_NAME, CASCADE_NAME_LEN); CascadeClassifier faceDetector; faceDetector.load(CASCADE_NAME);这些参数文件可在 OpenCV 发行包里的 data/haarcascades 文件夹中找到。在使用所需要的参数对人脸检测器进行初始化后，就可以用它进行人脸检测了：cv::Mat img; vector<cv::Rect> faceRects; double scalingFactor = 1.1; int minNeighbors = 2; int flags = 0; cv::Size minimumSize(30,30); faceDetector.detectMultiScale(img, faceRects, scalingFactor, minNeighbors, flags cv::Size(30, 30) );检测过程中，已训练好的分类器会用不同的尺度遍历输入图像的每一个像素，以检测不同大小的人脸。参数scalingFactor 决定每次遍历分类器后尺度会变大多少倍。参数 minNeighbors 指定一个符合条件的人脸区域应该有多少个符合条件的邻居像素才被认为是一个可能的人脸区域；如果一个符合条件的人脸区域只移动了一个像素就不再触发分类器，那么这个区域非常可能并不是我们想要的结果。拥有少于 minNeighbors 个符合条件的邻居像素的人脸区域会被拒绝掉。如果 minNeighbors 被设置为 0，所有可能的人脸区域都会被返回回来。参数 flags 是 OpenCV 1.x 版本 API 的遗留物，应该始终把它设置为 0。最后，参数 minimumSize 指定我们所寻找的人脸区域大小的最小值。faceRects 向量中将会包含对 img 进行人脸识别获得的所有人脸区域。识别的人脸图像可以通过 cv::Mat 的 ()运算符提取出来，调用方式很简单：cv::Mat faceImg = img(aFaceRect)。不管是使用 CIDetector 还是 OpenCV 的 CascadeClassifier，只要我们获得了至少一个人脸区域，我们就可以对图像中的人进行识别了。人脸识别OpenCV 自带了三个人脸识别算法：Eigenfaces，Fisherfaces 和局部二值模式直方图 (LBPH)。如果你想知道它们的工作原理及相互之间的区别，请阅读 OpenCV 的详细文档。针对于我们的 demo app，我们将采用 LBPH 算法。因为它会根据用户的输入自动更新，而不需要在每添加一个人或纠正一次出错的判断的时候都要重新进行一次彻底的训练。要使用 LBPH 识别器，我们也用 Objective-C++ 把它封装起来。这个封装中暴露以下函数：+ (FJFaceRecognizer *)faceRecognizerWithFile:(NSString *)path; - (NSString *)predict:(UIImage*)img confidence:(double *)confidence; - (void)updateWithFace:(UIImage *)img name:(NSString *)name;像下面这样用工厂方法来创建一个 LBPH 实例：+ (FJFaceRecognizer *)faceRecognizerWithFile:(NSString *)path { FJFaceRecognizer *fr = [FJFaceRecognizer new]; fr->_faceClassifier = createLBPHFaceRecognizer(); fr->_faceClassifier->load(path.UTF8String); return fr; }预测函数可以像下面这样实现：- (NSString *)predict:(UIImage*)img confidence:(double *)confidence { cv::Mat src = [img cvMatRepresentationGray]; int label; self->_faceClassifier->predict(src, label, *confidence); return _labelsArray[label]; }请注意，我们要使用一个类别方法把 UIImage 转化为 cv::Mat。此转换本身倒是相当简单直接：使用CGBitmapContextCreate 创建一个指向 cv::Image 中的 data 指针所指向的数据的 CGContextRef。当我们在此图形上下文中绘制此 UIImage 的时候，cv::Image 的 data 指针所指就是所需要的数据。更有趣的是，我们能对一个 Objective-C 类创建一个 Objective-C++ 的类别，并且确实管用。另外，OpenCV 的人脸识别器仅支持整数标签，但是我们想使用人的名字作标签，所以我们得通过一个 NSArray 属性来对二者实现简单的转换。一旦识别器给了我们一个识别出来的标签，我们把此标签给用户看，这时候就需要用户给识别器一个反馈。用户可以选择，“是的，识别正确”，也可以选择，“不，这是 Y，不是 X”。在这两种情况下，我们都可以通过人脸图像和正确的标签来更新 LBPH 模型，以提高未来识别的性能。使用用户的反馈来更新人脸识别器的方式如下：- (void)updateWithFace:(UIImage *)img name:(NSString *)name { cv::Mat src = [img cvMatRepresentationGray]; NSInteger label = [_labelsArray indexOfObject:name]; if (label == NSNotFound) { [_labelsArray addObject:name]; label = [_labelsArray indexOfObject:name]; } vector<cv::Mat> images = vector<cv::Mat>(); images.push_back(src); vector<int> labels = vector<int>(); labels.push_back((int)label); self->_faceClassifier->update(images, labels); }这里，我们又做了一次了从 UIImage 到 cv::Mat、int 到 NSString 标签的转换。我们还得如 OpenCV 的FaceRecognizer::update API所期望的那样，把我们的参数放到 std::vector 实例中去。如此“预测，获得反馈，更新循环”，就是文献上所说的监督式学习。结论OpenCV 是一个强大而用途广泛的库，覆盖了很多现如今仍在活跃的研究领域。想在一篇文章中给出详细的使用说明只会是让人徒劳的事情。因此，本文仅意在从较高层次对 OpenCV 库做一个概述。同时，还试图就如何集成 OpenCV 库到你的 iOS 工程中给出一些实用建议，并通过一个人脸识别的例子来向你展示如何在一个真正的项目中使用 OpenCV。如果你觉得 OpenCV 对你的项目有用， OpenCV 的官方文档写得非常好非常详细，请继续前行，创造出下一个伟大的 app！这里是识别，而不是人脸检测，检测部分前面我已经说过，是一种基于adaboost的级联决策算法，能够高精度的检测出人脸所在的区域。前面我们转载了几篇人脸识别网上的资源，大家可能知道如何在OpenCV中使用人脸识别这个库，但是对于其中算法的深层含义还远没有彻底弄懂。所以我通过一篇论文的阅读《基于LBP和Fisher face的人脸算法研究》讲解现在人脸识别算法的具体含义。CV_EXPORTS_W Ptr<FaceRecognizer> **createEigenFaceRecognizer**(int num_components = 0, double threshold = DBL_MAX); CV_EXPORTS_W Ptr<FaceRecognizer> **createFisherFaceRecognizer**(int num_components = 0, double threshold = DBL_MAX); CV_EXPORTS_W Ptr<FaceRecognizer> **createLBPHFaceRecognizer**(int radius=1, int neighbors=8, int grid_x=8, int grid_y=8, double threshold = DBL_MAX);12341234OpenCV中比较好认识的就是 基于LBP算子的人脸识别算法。我们首先讲解这个算子的过程。还是先看下OpenCV下的结果，我们使用的是ORL人脸数据库，每个人有10张，包含了40个人。vector<Mat> images; vector<int> labels; // images for first person images.push_back(imread(\"ORL\\\\s1\\\\1.bmp\", CV_LOAD_IMAGE_GRAYSCALE)); labels.push_back(1); images.push_back(imread(\"ORL\\\\s1\\\\2.bmp\", CV_LOAD_IMAGE_GRAYSCALE)); labels.push_back(1); images.push_back(imread(\"ORL\\\\s1\\\\3.bmp\", CV_LOAD_IMAGE_GRAYSCALE)); labels.push_back(1); images.push_back(imread(\"ORL\\\\s1\\\\4.bmp\", CV_LOAD_IMAGE_GRAYSCALE)); labels.push_back(1); images.push_back(imread(\"ORL\\\\s1\\\\5.bmp\", CV_LOAD_IMAGE_GRAYSCALE)); labels.push_back(1); images.push_back(imread(\"ORL\\\\s1\\\\6.bmp\", CV_LOAD_IMAGE_GRAYSCALE)); labels.push_back(1); images.push_back(imread(\"ORL\\\\s1\\\\7.bmp\", CV_LOAD_IMAGE_GRAYSCALE)); labels.push_back(1); images.push_back(imread(\"ORL\\\\s1\\\\8.bmp\", CV_LOAD_IMAGE_GRAYSCALE)); labels.push_back(1);1234567891011121314151617181912345678910111213141516171819这是第一个人的信息，大家可以按照这个规律进行人脸的扩展。Ptr<FaceRecognizer> model = createEigenFaceRecognizer(); model->train(images, labels); Mat img = imread(\"ORL\\\\s1\\\\8.bmp\", CV_LOAD_IMAGE_GRAYSCALE); double confidence; int predicted; model->predict(img,predicted,confidence);12345671234567confidence是预测的置信度。可以在构造createEigenFaceRecognizer的时候设置阈值参数，如果超过了这个参数，那么得到的结果就是-1，表示没有合适的分类。基于LBP的人脸识别LBP是local binary pattern的简写，局部二值模式。原始LBP算子最初是在3*3的矩形窗口上定义的，以矩形窗口心中点的灰度值作为阈值，将邻域内各像素点像素值与阈值进行比较，将比较结果进行二值化处理，然后各邻域像素点根据位置的不同进行加权求和和到该窗口中心的LBP值（为了满足旋转性，将该二值串进行循环移动，然后使用权值加权–如下）。对所有情况中选择值最小的作为这个点的LBP值。当然后面提出了基于圆形的任意半径只是一个扩展，也很好理解。接着，在圆形LBP的基础上，发现局部二值模式在提取局部纹理特征过程中，二进制模式种类数是对着采样像素点的个数增加而增加的。比如3*3的矩形框中的二进制模式就有28=256种，当变成5*5的时候，二进制模式的值会成指数增加。（实际上模式的个数就是对应了最终的特征直方图的维数，维数大，那么处理起来就比较费时了）。为了解决这一问题，Ojala等人提出了一致性模式方法(Uniform Pattern)方法。他们认为当某个二进制串相连成环状时，如果二进制位数变换至多2次，那么就是一致性模式，其余的则就是非一致性模式。注意：对于3×3邻域内8个采样点来说，二进制模式由原始的256种减少为58种，即：它把值分为59类，58个uniform pattern为一类，其它的所有值为第59类。当我们知道某一个点属于哪个模式后，接下来我们基于这些LBP值进行人脸识别。LBP被运用于计算机人脸识别领域时，提取出来的人脸特征通常是以LBP直方图向量进行表达的。1. 对预处理后的人脸图像进行分块2. 对分块后的各小块图像区域进行LBP特征提取变换3. 使用LBP直返图向量作为人脸特征的描述。一般分块数越多，人脸表达的效果就会越好，但是分块数越多，会直接导致特征向量维数的增加，会增加计算的复杂度。对每个分块计算LBP值的直方图，然后将所有分块直方图进行连接得到最终的直方图特征向量，这个特征向量代表原来的人脸图像，可以用来描述整体图像。对于这个融合的直方图，我们进行特征分类。如果训练样本数量越大，分类的效果也会越好，在基于LBP的人脸识别中，通常采用基于直方图的相似性度量的最近邻分类方法来分类。我们可以在 OpenCV源码中找到他的实现“sources\\modules\\contrib\\src\\facerec.cpp”函数void LBPH::predict(InputArray _src, int &minClass, double &minDist) const11部分源码如下：// find 1-nearest neighbor minDist = DBL_MAX; minClass = -1; for(size_t sampleIdx = 0; sampleIdx < _histograms.size(); sampleIdx++) { double dist = compareHist(_histograms[sampleIdx], query, CV_COMP_CHISQR); if((dist < minDist) && (dist < _threshold)) { minDist = dist; minClass = _labels.at<int>((int) sampleIdx); } }1234567891012345678910可以看出是在所有图像的直方图中找出距离最近的作为返回值。Fisher face方法为了提高识别效率，在对特征向量进行降维的同时还需要寻求更有利用分类的向量。Fisher Fface方法是主成分分析(PCA)与Fisher线性判别分析（FLD Fisher Linear Discriminant Analysis）相结合的算法，算法首先对高维特征样本进行PCA降维，投影到低维特征空间，再采用LDA方法得到最优判别向量。主成分分析方法是基于K-L变换的基础上实现的。K-L变换首先利用样本的数据构建协方差矩阵，计算出协方差矩阵的特征值和对应的特征向量，通过特征向量来找出促使样本离散程度达到最大的特征向量投影方法。PCA方法的核心思想是首先将经过预处理后的人脸特征表达向量进行K-L变换，以消除原有向量各个分量之间的相关性，在K-L变换过程中去掉了那些带有少量信息的分量，特征空间的维数也相应降低了。Turk和Pentland把PCA方法引入到人脸识别领域中，并取得了成功。随后主成分分析法就成为了人脸识别的主要方法之一。K-L变换的核心问题的方法是计算出协方差矩阵的特征值和特征向量，待测样本的特征向量通过K-L变换后，去掉了那些带有少量信息的分量，保留了带有绝大部分信息的正交特征向量，因此，K-L变换具有特征提取，降低特征向量维数的优点。基于主成分分析的人脸识别算法思想如下：假设参加训练的人脸图像有N幅，将每一幅图像转化为一维向量（将m*n的矩阵变成D=1*mn），然后建立一个大小为D*N的所有图像的全数据矩阵。按照上式的结果可知协方差矩阵S的大小为D*D，对构造出来的协方差矩阵进行特征值求解，计算出特征值对应的特征向量，将特征向量进行组合就是变换后的新向量空间，利用奇异值分解定理可以将协方差矩阵的计算量进行简化（就是先计算ATA的特征值和特征向量，然后通过变换关系导出AAT的特征向量）。Fisher线性判别分析:基本思想是计算出使Fisher准则函数达到极值的向量，并将此向量作为最佳投影方向，样本在该方向上进行投影，投影后的特征向量具有类间离散度最大，类内离散度最小特点。将特征向量（直接将图像转换成1*mn）映射到K个低维的向量上（这K个低维的向量就是判别向量），然后判断离哪个类别最近，就属于哪个人的人脸。createEigenFaceRecognizer使用的是将特征向量（直接将图像转换成1*mn）进行PCA降维，然后使用距离函数，循环训练的数据判断属于哪个类别。"}
{"content2":"从1966年到2016年，正好恰恰过去了五十年，过去的五十年计算机视觉发展非常快。今天计算机视觉是不是变成了很成熟、很完美的技术？并不是。比如说，蓝天白云下，公路上有一辆白色的大卡车，计算机就可能说，这是一朵白云。大家可能都想到了这是一个惨剧：一辆特斯拉没有检测出的卡车，使得高速行驶的特斯拉司机当场死亡。尽管计算机视觉技术并没有发展到成熟的阶段，但随着应用市场的打开，新的机会与技术革新将随之而来。未来3-5年，计算机视觉领域将有三个变化值得关注做为一个计算机视觉行业的从业者，讲讲我个人对未来三到五年的看法。首先，计算机视觉有很多开源软件包，但是随着视觉的问题越来越复杂，以及我们对安全性的要求越来越高，复杂的问题一定要找专业的团队来解决。比如解决摄像机运动轨迹的问题，你如果拿一个手机从一个房间走到另外一个房间，再回到原来的位置，生成了一条轨迹。把这个数据给Orb slam去分析，它所得出的轨迹就有所变化，而且和墙壁有很大的重叠。如果有一个机器人使用了这个Orb slam项目，出门的时候会撞到墙。即使现在有许多开源的项目，但商用化的计算机视觉系统能在不同的平台和不同的使用环境下反复测试对比，最终可以实现平均性能更加优越，并且没有重大安全隐患。复杂的问题与应用，一定要找专业的团队去解决。这个领域会出现超过十亿台下一代测算平台，新一代的智能手机，还有无人机。与这对应的技术有数百亿的市场，计算机视觉领域将诞生出很多上市企业。第二个关注的变化是芯片化。我们知道计算机视觉往往需要非常复杂的算法去解决，应用往往都是移动化的，例如移动化的设备，移动的机器人。在移动化的设备里面，要以低能耗的方式进行复杂的算法，芯片一定是必经之路。现在行业里面，大量团队在进行算法的研发，一些走的比较快的同行已经开始将成熟的算法芯片化了。这样，除了算法的优越程度以外，肯定还存在芯片的功耗与成本问题。第三个关注的变化是理论上的，即深度学习之后会产生什么样的数学模型。深度学习的缺点也是人尽皆知，它需要进行海量的资料的学习。曾经有这个领域的权威Davis Marr，在生前预言说，计算机是复杂的问题，最后会有单一理论框架使之得到解决。深度学习不会是这种终极框架，还将有更强大的数学模型出现。它对人类影响也会是更深远的，我们应该深刻关注理论上的变化，把最新的技术做成产品来解决实际的问题。本文首发钛媒体，由张霖根据包英泽在2016MIIC大会上的演讲整理"}
{"content2":"Deep Learning（深度学习）：ufldl的2个教程(这个没得说，入门绝对的好教程，Ng的，逻辑清晰有练习)：一ufldl的2个教程(这个没得说，入门绝对的好教程，Ng的，逻辑清晰有练习)：二Bengio团队的deep learning教程，用的theano库，主要是rbm系列，搞python的可以参考，很不错。deeplearning.net主页，里面包含的信息量非常多，有software, reading list, research lab, dataset, demo等，强烈推荐，自己去发现好资料。Deep learning的toolbox，matlab实现的，对应源码来学习一些常见的DL模型很有帮助，这个库我主要是用来学习算法实现过程的。2013年龙星计划深度学习教程，邓力大牛主讲，虽然老师准备得不充分，不过还是很有收获的。Hinton大牛在coursera上开的神经网络课程，DL部分有不少，非常赞，没有废话，课件每句话都包含了很多信息，有一定DL基础后去听收获更大。Larochelle关于DL的课件，逻辑清晰，覆盖面广,包含了rbm系列，autoencoder系列，sparse coding系列，还有crf，cnn，rnn等。虽然网页是法文，但是课件是英文。CMU大学2013年的deep learning课程，有不少reading paper可以参考。达慕思大学Lorenzo Torresani的2013Deep learning课程reading list.Deep Learning Methods for Vision(余凯等在cvpr2012上组织一个workshop，关于DL在视觉上的应用)。斯坦福Ng团队成员链接主页，可以进入团队成员的主页，比较熟悉的有Richard Socher, Honglak Lee, Quoc Le等。多伦多ML团队成员链接主页，可以进入团队成员主页，包括DL鼻祖hinton，还有Ruslan Salakhutdinov , Alex Krizhevsky等。蒙特利尔大学机器学习团队成员链接主页，包括大牛Bengio，还有Ian Goodfellow 等。纽约大学的机器学习团队成员链接主页，包括大牛Lecun，还有Rob Fergus等。Charlie Tang个人主页，结合DL+SVM.豆瓣上的脑与deep learning读书会，有讲义和部分视频，主要介绍了一些于deep learning相关的生物神经网络。Large Scale ML的课程，由Lecun和Langford讲的，能不推荐么。Yann Lecun的2014年Deep Learning课程主页。 视频链接。吴立德老师《深度学习课程》一些常见的DL code列表，csdn博主zouxy09的博文，Deep Learning源代码收集-持续更新…Deep Learning for NLP (without Magic)，由DL界5大高手之一的Richard Socher小组搞的，他主要是NLP的。2012 Graduate Summer School: Deep Learning, Feature Learning，高手云集，深度学习盛宴，几乎所有的DL大牛都有参加。matlab下的maxPooling速度优化，调用C++实现的。2014年ACL机器学习领域主席Kevin Duh的深度学习入门讲座视频。R-CNN code: Regions with Convolutional Neural Network Features.Machine Learning（机器学习）：介绍图模型的一个ppt，非常的赞，ppt作者总结得很给力，里面还包括了HMM，MEM, CRF等其它图模型。反正看完挺有收获的。机器学习一个视频教程，youtube上的，翻吧，内容很全面，偏概率统计模型，每一小集只有几分钟。龙星计划2012机器学习，由余凯和张潼主讲。demonstrate 的 blog :关于PGM(概率图模型)系列，主要按照Daphne Koller的经典PGM教程介绍的，大家依次google之。FreeMind的博客，主要关于机器学习的。Tom Mitchell大牛的机器学习课程，他的machine learning教科书非常出名。CS109,Data Science,用python介绍机器学习算法的课程。CCF主办的一些视频讲座。国外技术团队博客：Netflix技术博客,很多干货。Computer Vision（计算机视觉）：MIT2013年秋季课程：Advances in Computer Vision，有练习题，有些有code.IPAM一个计算机视觉的短期课程，有不少牛人参加。OpenCV相关：http://opencv.org/2012年7月4日随着opencv2.4.2版本的发布，opencv更改了其最新的官方网站地址。http://www.opencvchina.com/好像12年才有这个论坛的，比较新。里面有针对《learning opencv》这本书的视频讲解，不过视频教学还没出完，正在更新中。对刚入门学习opencv的人来说很不错。http://www.opencv.org.cn/forum/opencv中文论坛，对于初次接触opencv的学者来说比较不错，入门资料多，opencv的各种英文文档也翻译成中文了。不足是感觉这个论坛上发帖提问很少人回答，也就是说讨论不够激烈。http://opencv.jp/opencv的日文网站，里面有不少例子代码，看不懂日文可以用网站自带的翻译，能看个大概。http://code.opencv.org/projects/opencvopencv版本bug修补，版本更新，以及各种相关大型活动安排,还包含了opencv最近几个月内的活动路线，即未来将增加的功能等，可以掌握各种关于opencv进展情况的最新进展。http://tech.groups.yahoo.com/group/OpenCV/opencv雅虎邮件列表，据说是最好的opencv论坛，信息更新最新的地方。不过个人认为要查找相关主题的内容，在邮件列表中非常不方便。http://www.cmlab.csie.ntu.edu.tw/~jsyeh/wiki/doku.php台湾大学暑假集训网站，内有链接到与opencv集训相关的网页。感觉这种教育形式还蛮不错的。http://sourceforge.net/projects/opencvlibrary/opencv版本发布地方。http://code.opencv.org/projects/opencv/wiki/ChangeLog#241    http://opencv.willowgarage.com/wiki/OpenCV%20Change%20Logsopencv版本内容更改日志网页,前面那个网页更新最快。http://www.opencv.org.cn/opencvdoc/2.3.2/html/doc/tutorials/tutorials.htmlopencv中文教程网页，分几个模块讲解，有代码有过程。内容是网友翻译opencv自带的doc文件里的。https://netfiles.uiuc.edu/jbhuang1/www/resources/vision/index.html网友总结的常用带有cvpr领域常见算法code链接的网址，感觉非常的不错。http://fossies.org/dox/OpenCV-2.4.2/该网站可以查看opencv中一些函数的变量接口，还会列出函数之间的结构图。http://opencv.itseez.com/opencv的函数、类等查找网页，有导航，查起来感觉不错。优化：submodual优化网页。Geoff Gordon的优化课程，youtube上有对应视频。数学：http://www.youku.com/playlist_show/id_19465801.html《计算机中的数学》系列视频，8位老师10讲内容，生动介绍微积分和线性代数基本概念在计算机学科中的各种有趣应用！Linux学习资料：http://itercast.com/library/1linux入门的基础视频教程，对于新手可选择看第一部分，视频来源于LinuxCast.net网站，还不错。OpenNI+Kinect相关：http://1.yuhuazou.sinaapp.com/网友晨宇思远的博客，主攻cvpr，ai等。http://blog.csdn.net/chenli2010/article/details/6887646kinect和openni学习资料汇总。http://blog.csdn.net/moc062066/article/category/871261OpenCV 计算机视觉 kinect的博客:http://kheresy.wordpress.com/index_of_openni_and_kinect/comment-page-5/网友Heresy的博客，里面有不少kinect的文章，写的比较详细。http://www.cnkinect.com/体感游戏中文网，有不少新的kinect资讯。http://www.kinectutorial.com/Kinect体感开发网。http://code.google.com/p/openni-hand-trackeropenni_hand_tracking google code项目。http://blog.candescent.ch/网友的kinect博客，里面有很多手势识别方面的文章介绍，还有源码，不过貌似是基于c#的。https://sites.google.com/site/colordepthfusion/一些关于深度信息和颜色信息融合（fusion）的文章。http://projects.ict.usc.edu/mxr/faast/kinect新的库，可以结合OpenNI使用。https://sites.google.com/a/chalearn.org/gesturechallenge/kinect手势识别网站。http://www.ros.org/wiki/mit-ros-pkgmit的kinect项目，有code。主要是与手势识别相关。http://www.thoughtden.co.uk/blog/2012/08/kinecting-people-our-top-6-kinect-projects/kinect 2012年度最具创新的6个项目，有视频，确实够创新的！http://www.cnblogs.com/yangyangcv/archive/2011/01/07/1930349.htmlkinect多点触控的一篇博文。http://sourceforge.net/projects/kinect-mex/http://www.mathworks.com/matlabcentral/fileexchange/30242-kinect-matlab有关matlab for kinect的一些接口。http://news.9ria.com/2012/1212/25609.htmlAIR和Kinect的结合，有一些手指跟踪的code。http://eeeweba.ntu.edu.sg/computervision/people/home/renzhou/index.htm研究kinect手势识别的，任洲。刚毕业不久。其他网友cvpr领域的链接总结：http://www.cnblogs.com/kshenf/网友整理常用牛人链接总结，非常多。不过个人没有没有每个网站都去试过。所以本文也是我自己总结自己曾经用过的或体会过的。OpenGL有关:http://nehe.gamedev.net/NeHe的OpenGL教程英文版。http://www.owlei.com/DancingWind/NeHe的OpenGL教程对应的中文版，由网友周玮翻译的。http://www.qiliang.net/old/nehe_qt/NeHe的OpengGL对应的Qt版中文教程。http://blog.csdn.net/qp120291570网友\"左脑设计，右脑编程\"的Qt_OpenGL博客,写得还不错。http://guiliblearning.blogspot.com/这个博客对opengl的机制有所剖析，貌似要FQ才能进去。cvpr综合网站论坛博客等：http://www.cvchina.net/中国计算机视觉论坛http://www.cvchina.info/这个博客很不错，每次看完都能让人兴奋，因为有很多关于cv领域的科技新闻，还时不时有视频显示。另外这个博客里面的资源也整理得相当不错。中文的。http://www.bfcat.com/一位网友的个人计算机视觉博客，有很多关于计算机视觉前沿的东西介绍，与上面的博客一样，看了也能让人兴奋。http://blog.csdn.net/v_JULY_v/牛人博客，主攻数据结构，机器学习数据挖掘算法等。http://blog.youtueye.com/该网友上面有一些计算机视觉方向的博客,博客中附有一些实验的测试代码.http://blog.sciencenet.cn/u/jingyanwang多看pami才扯谈的博客，其中有不少pami文章的中文介绍。http://chentingpc.me/做网络和自然语言处理的，有不少机器学习方面的介绍。ML常用博客资料等：http://freemind.pluskid.org/由 pluskid 所维护的 blog，主要记录一些机器学习、程序设计以及各种技术和非技术的相关内容，写得很不错。http://datasciencemasters.org/里面包含学ML/DM所需要的一些知识链接，且有些给出了视频教程，网页资料，电子书，开源code等，推荐！http://cs.nju.edu.cn/zhouzh/index.htm周志华主页，不用介绍了，机器学习大牛，更可贵的是他的很多文章都有源码公布。http://www.eecs.berkeley.edu/~jpaisley/Papers.htmJohn Paisley的个人主页，主要研究机器学习领域，有些文章有代码提供。http://foreveralbum.yo2.cn/里面有一些常见机器学习算法的详细推导过程。http://blog.csdn.net/abcjennifer浙江大学CS硕士在读，关注计算机视觉，机器学习，算法研究，博弈， 人工智能， 移动互联网等学科和产业。该博客中有很多机器学习算法方面的介绍。http://www.wytk2008.net/无垠天空的机器学习博客。http://www.chalearn.org/index.html机器学习挑战赛。http://licstar.net/licstar的技术博客，偏自然语言处理方向。国内科研团队和牛人网页：http://vision.ia.ac.cn/zh/index_cn.html中科院自动化所机器视觉课题小组，有相关数据库、论文、课件等下载。http://www.cbsr.ia.ac.cn/users/szli/李子青教授个人主页，中科院自动化所cvpr领域牛叉人！http://www4.comp.polyu.edu.hk/~cslzhang/香港理工大学教授lei zhang个人主页，也是cvpr领域一大牛人啊，cvpr，iccv各种发表。更重要的是他所以牛叉论文的code全部公开，非常难得！http://liama.ia.ac.cn/wiki/start中法信息、自动化与应用联合实验室，里面很多内容不仅限而cvpr，还有ai领域一些其他的研究。http://www.cogsci.xmu.edu.cn/cvl/english/厦门大学特聘教授，cv领域一位牛人。研究方向主要为目标检测，目标跟踪，运动估计，三维重建，鲁棒统计学，光流计算等。http://idm.pku.edu.cn/index.aspx北京大学数字视频编码技术国家实验室。http://www.csie.ntu.edu.tw/~cjlin/libsvm/libsvm项目网址，台湾大学的，很火！http://www.jdl.ac.cn/user/sgshan/index.htm山世光，人脸识别研究比较牛。在中国科学院智能信息处理重点实验室国外科研团队和牛人网页：https://netfiles.uiuc.edu/jbhuang1/www/resources/vision/index.html常见计算机视觉资源整理索引，国外学者整理，全是出名的算法，并且带有代码的，这个非常有帮助，其链接都是相关领域很火的代码。http://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/txtv-groups.html国外学者整理的各高校研究所团队网站http://research.microsoft.com/en-us/groups/vision/微软视觉研究小组，不解释，大家懂的，牛！http://lear.inrialpes.fr/index.php法国国家信息与自动化研究所，有对应牛人的链接，论文项目网页链接，且一些code对应链接等。http://www.cs.ubc.ca/~pcarbo/objrecls/Learning to recognize objects with little supervision该篇论文的项目网页，有对应的code下载，另附有详细说明。http://www.eecs.berkeley.edu/~lbourdev/poselets/poselets相关研究界面，关于poselets的第一手资料。http://www.cse.oulu.fi/CMV/Research芬兰奥卢大学计算机科学与工程学院网页，里面有很多cv领域相关的研究，比如说人脸，脸部表情，人体行为识别，跟踪，人机交互等cv基本都涉及有。http://www.cs.cmu.edu/~cil/vision.html卡耐基梅隆大学计算机视觉主页，内容非常多。可惜的是该网站内容只更新到了2004年。http://vision.stanford.edu/index.html斯坦福大学计算机视觉主页，里面有非常非常多的牛人，比如说大家熟悉的lifeifei.http://www.wavelet.org/index.php关于wavelet研究的网页。http://civs.ucla.edu/加州大学洛杉矶分校统计学院，关于统计学习方面各种资料，且有相应的网上公开课。http://www.cs.cmu.edu/~efros/卡耐基梅隆大学Alexei(Alyosha)Efros教授个人网站，计算机图形学高手。http://web.mit.edu/torralba/www//mit牛人Associate教授个人网址，主要研究计算机视觉人体视觉感知，目标识别和场景理解等。http://people.csail.mit.edu/billf/mit牛人William T. Freeman教授，主要研究计算机视觉和图像学http://www.research.ibm.com/peoplevision/IBM人体视觉研究中心，里面除了有其研究小组的最新成果外，还有很多测试数据(特别是视频）供下载。http://www.vlfeat.org/vlfeat主页，vlfeat也是一个开源组织，主要定位在一些最流行的视觉算法开源上，Ｃ编写，其很多算法效果比opencv要好,不过数量不全，但是非常有用。http://www.robots.ox.ac.uk/~az/Andrew Zisserman的个人主页，这人大家应该熟悉，《计算机视觉中的多视几何》这本神书的作者之一。http://www.cs.utexas.edu/~grauman/KristenGrauman教授的个人主页，是个大美女，且是2011年“马尔奖”获得者，”马尔奖“大家都懂的，计算机视觉领域的最高奖项，目前无一个国内学者获得过。她的主要研究方法是视觉识别。http://groups.csail.mit.edu/vision/welcome/mit视觉实验室主页。http://code.google.com/p/sixthsense/曾经在网络上非常出名一个视频，一个作者研究的第六感装置，现在这个就是其开源的主页。http://vision.ucsd.edu/~pdollar/research.html#BehaviorRecognitionAnimalBehaviorPiotr Dollar的个人主要，主要研究方向是人体行为识别。http://www.mmp.rwth-aachen.de/移动多媒体处理，将移动设备，计算机图像学，视觉，图像处理等结合的领域。http://www.di.ens.fr/~laptev/index.htmlIvan Laptev牛人主页，主要研究人体行为识别。有很多数据库可以下载。http://blogs.oregonstate.edu/hess/Rob Hess的个人主要，里面有源码下载，比如说粒子滤波，他写的粒子滤波在网上很火。http://morethantechnical.googlecode.com/svn/trunk/cvpr领域一些小型的开源代码。http://iica.de/pd/index.py做行人检测的一个团队，内部有一些行人检测的代码下载。http://www.cs.utexas.edu/~grauman/research/pubs.htmlUT-Austin计算机视觉小组，包含的视觉研究方向比较广，且有的文章有源码，你只需要填一个邮箱地址，系统会自动发跟源码相关的信息过来。http://www.robots.ox.ac.uk/~vgg/index.htmlvisual geometry group图像:http://blog.sina.com.cn/s/blog_4cccd8d301012pw5.html交互式图像分割代码。http://vision.csd.uwo.ca/code/graphcut优化代码。语音：http://danielpovey.com/kaldi-lectures.html语音处理中的kaldi学习。算法分析与设计（计算机领域的基础算法）：http://www.51nod.com/focus.html该网站主要是讨论一些算法题。里面的李陶冶是个大牛，回答了很多算法题。一些综合topic列表：http://www.cs.cornell.edu/courses/CS7670/2011fa/计算机视觉中的些topic（Special Topics in Computer Vision），截止到2011年为止，其引用的文章都是非常顶级的topic。书籍相关网页：http://www.imageprocessingplace.com/index.htm冈萨雷斯的《数字图像处理》一书网站，包含课程材料，matlab图像处理工具包，课件ppt等相关素材。Consumer Depth Cameras for Computer Vision很优秀的一本书，不过很贵，买不起啊！做深度信息的使用这本书还不错，google图中可以预览一部分。Making.Things.See针对Kinect写的，主要关注深度信息，较为基础。书籍中有不少例子，貌似是java写的。国内一些AI相关的研讨会：http://www.iipl.fudan.edu.cn/MLA13/index.htm中国机器学习及应用研讨会(这个是2013年的)期刊会议论文下载：http://cvpapers.com/几个顶级会议论文公开下载界面，比如说ICCV,CVPR,ECCV,ACCV,ICPR,SIGGRAPH等。http://www.cvpr2012.org/cvpr2012的官方地址，里面有各种资料和信息，其他年份的地址类似推理更改即可。http://www.sciencedirect.com/science/journal/02628856ICV期刊下载http://www.computer.org/portal/web/tpamiTPAMI期刊，AI领域中可以算得上是最顶级的期刊了，里面有不少cvpr方面的内容。http://www.springerlink.com/content/100272/IJCV的网址。http://books.nips.cc/NIPS官网，有论文下载列表。http://graphlab.org/lsrs2013/program/LSRS （会议）地址，大规模推荐系统，其它年份依次类推。会议期刊相关信息：http://conferences.visionbib.com/Iris-Conferences.html该网页列出了图像处理，计算机视觉领域相关几乎所有比较出名的会议时间表。http://conferences.visionbib.com/Browse-conf.php上面网页的一个子网页，列出了最近的CV领域提交paper的deadline。cvpr相关数据库下载：http://research.microsoft.com/en-us/um/people/jckrumm/WallFlower/TestImages.htm微软研究院牛人Wallflower Paper的论文中用到的目标检测等测试图片http://archive.ics.uci.edu/ml/UCI数据库列表下载，最常用的机器学习数据库列表。http://www.cs.rochester.edu/~rmessing/uradl/人体行为识别通过关键点的跟踪视频数据库，Rochester university的http://www.research.ibm.com/peoplevision/performanceevaluation.htmlIBM人体视觉研究中心，有视频监控等非常多的测试视频。http://www.cvpapers.com/datasets.html该网站上列出了常见的cvpr研究的数据库。http://www.cs.washington.edu/rgbd-dataset/index.htmlRGB-D Object Dataset.做目标识别的。AI相关娱乐网页：http://en.akinator.com/该网站很好玩，可以测试你心里想出的一个人名(当然前提是这个人必须有一定的知名度)，然后该网站会提出一系列的问题，你可以选择yes or no,or I don’t know等等，最后系统会显示你心中所想的那个人。http://www.doggelganger.co.nz/人与狗的匹配游戏，摄像头采集人脸，呵呵…Android相关：https://code.google.com/p/android-ui-utils/该网站上有一些android图标,菜单等跟界面有关的设计工具,可以用来做一些简单的UI设计.工具和code下载：http://lear.inrialpes.fr/people/dorko/downloads.html6种常见的图像特征点检测子，linux下环境运行。不过只提供了二进制文件，不提供源码。http://www.cs.ubc.ca/~pcarbo/objrecls/index.html#codessmcmc的matlab代码,是Learning to recognize objects with little supervision这一系列文章用的源码，属于目标识别方面的研究。http://www.robots.ox.ac.uk/~timork/仿射无关尺度特征点检测算子源码，还有些其它算子的源码或二进制文件。http://www.vision.ee.ethz.ch/~bleibe/code/ism.html隐式形状模型(ISM)项目主页，作者Bastian Leibe提供了linux下运行的二进制文件。http://www.di.ens.fr/~laptev/download.html#stipIvan Laptev牛人主页中的STIP特征点检测code，但是也只是有二进制文件，无源码。该特征点在行为识别中该特征点非常有名。http://ai.stanford.edu/~quocle/斯坦福大学Quoc V.Le主页，上有它2011年行为识别文章的代码。开源软件：http://mloss.org/software/一些ML开源软件在这里基本都可以搜到，有上百个。https://github.com/myui/hivemallScalable machine learning library for Hive/Hadoop.http://scikit-learn.org/stable/基于python的机器学习开源软件，文档写得不错。挑战赛：http://www.chioka.in/kaggle-competition-solutions/kaggle一些挑战赛的code.公开课：网易公开课，国内做得很不错的公开课，翻译了一些国外出名的公开课教程，与国外公开课平台coursera有合作。coursera在线教育网上公开课，很新，有个邮箱注册即可学习，有不少课程，且有对应的练习，特别是编程练习，超赞。斯坦福网上公开课链接，有统计学习，凸优化等课程。udacity公开课程下载链接，其实速度还可以。里面有不少好教程。机器学习公开课的连接，有不少课。"}
{"content2":"下载地址：网盘下载内容简介  · · · · · ·《python计算机视觉编程》是计算机视觉编程的权威实践指南，依赖python语言讲解了基础理论与算法，并通过大量示例细致分析了对象识别、基于内容的图像搜索、光学字符识别、光流法、跟踪、三维重建、立体成像、增强现实、姿态估计、全景创建、图像分割、降噪、图像分组等技术。另外，书中附带的练习还能让读者巩固并学会应用编程知识。《python计算机视觉编程》适合的读者是：有一定编程与数学基础，想要了解计算机视觉的基本理论与算法的学生，以及计算机科学、信号处理、物理学、应用数学和统计学、神经生理学、认知科学等领域的研究人员和从业者。作者简介  · · · · · ·Jan Erik Solem瑞典隆德大学副教授（数学成像小组），Polar Rose公司创始人兼CTO，计算机视觉研究者，Python爱好者，技术图书作家，经常出席各种计算机视觉、图像分析、机器智能等国际会议并发表演讲。他主要关注3D重建、变分问题与优化、图像分割与识别、形状分析，有多年Python计算机视觉教学、研究和行业应用经验，技术博客为http://www.janeriksolem.net。另著有Computing with Python: An Introduction to Python for Science and Engineering一书。目录  · · · · · ·《python计算机视觉编程》推荐序 xi前言 xiii第1章　基本的图像操作和处理 11.1　pil：python图像处理类库 11.1.1　转换图像格式 21.1.2　创建缩略图 31.1.3　复制和粘贴图像区域 31.1.4　调整尺寸和旋转 31.2　matplotlib 41.2.1　绘制图像、点和线 41.2.2　图像轮廓和直方图 61.2.3　交互式标注 71.3　numpy 81.3.1　图像数组表示 81.3.2　灰度变换 91.3.3　图像缩放 111.3.4　直方图均衡化 111.3.5　图像平均 131.3.6　图像的主成分分析（pca） 141.3.7　使用pickle模块 161.4　scipy 171.4.1　图像模糊 181.4.2　图像导数 191.4.3　形态学：对象计数 221.4.4　一些有用的scipy模块 231.5　高级示例：图像去噪 24练习 28代码示例约定 29第2章　局部图像描述子 312.1　harris角点检测器 312.2　sift（尺度不变特征变换） 392.2.1　兴趣点 392.2.2　描述子 392.2.3　检测兴趣点 402.2.4　匹配描述子 432.3　匹配地理标记图像 472.3.1　从panoramio下载地理标记图像 472.3.2　使用局部描述子匹配 502.3.3　可视化连接的图像 52练习 54第3章　图像到图像的映射 573.1　单应性变换 573.1.1　直接线性变换算法 593.1.2　仿射变换 603.2　图像扭曲 613.2.1　图像中的图像 633.2.2　分段仿射扭曲 673.2.3　图像配准 703.3　创建全景图 763.3.1　ransac 773.3.2　稳健的单应性矩阵估计 783.3.3　拼接图像 81练习 84第4章　照相机模型与增强现实 854.1　针孔照相机模型 854.1.1　照相机矩阵 864.1.2　三维点的投影 874.1.3　照相机矩阵的分解 894.1.4　计算照相机中心 904.2　照相机标定 914.3　以平面和标记物进行姿态估计 934.4　增强现实 974.4.1　pygame和pyopengl 974.4.2　从照相机矩阵到opengl格式 984.4.3　在图像中放置虚拟物体 1004.4.4　综合集成 1024.4.5　载入模型 104练习 106第5章　多视图几何 1075.1　外极几何 1075.1.1　一个简单的数据集 1095.1.2　用matplotlib绘制三维数据 1115.1.3　计算f：八点法 1125.1.4　外极点和外极线 1135.2　照相机和三维结构的计算 1165.2.1　三角剖分 1165.2.2　由三维点计算照相机矩阵 1185.2.3　由基础矩阵计算照相机矩阵 1205.3　多视图重建 1225.3.1　稳健估计基础矩阵 1235.3.2　三维重建示例 1255.3.3　多视图的扩展示例 1295.4　立体图像 130练习 135第6章　图像聚类 1376.1　k-means聚类 1376.1.1　scipy聚类包 1386.1.2　图像聚类 1396.1.3　在主成分上可视化图像 1406.1.4　像素聚类 1426.2　层次聚类 1446.3　谱聚类 152练习 157第7章　图像搜索 1597.1　基于内容的图像检索 1597.2　视觉单词 1607.3　图像索引 1647.3.1　建立数据库 1647.3.2　添加图像 1657.4　在数据库中搜索图像 1677.4.1　利用索引获取候选图像 1687.4.2　用一幅图像进行查询 1697.4.3　确定对比基准并绘制结果 1717.5　使用几何特性对结果排序 1727.6　建立演示程序及web应用 1767.6.1　用cherrypy创建web应用 1767.6.2　图像搜索演示程序 176练习 179第8章　图像内容分类 1818.1　k邻近分类法（knn） 1818.1.1　一个简单的二维示例 1828.1.2　用稠密sift作为图像特征 1858.1.3　图像分类：手势识别 1878.2　贝叶斯分类器 1908.3　支持向量机 1958.3.1　使用libsvm 1968.3.2　再论手势识别 1988.4　光学字符识别 1998.4.1　训练分类器 2008.4.2　选取特征 2008.4.3　多类支持向量机 2018.4.4　提取单元格并识别字符 2028.4.5　图像校正 205练习 206第9章　图像分割 2099.1　图割（graph cut） 2099.1.1　从图像创建图 2119.1.2　用户交互式分割 2169.2　利用聚类进行分割 2189.3　变分法 224练习 226第10章　opencv 22710.1　opencv的python接口 22710.2　opencv基础知识 22810.2.1　读取和写入图像 22810.2.2　颜色空间 22810.2.3　显示图像及结果 22910.3　处理视频 23210.3.1　视频输入 23210.3.2　将视频读取到numpy数组中 23410.4　跟踪 23410.4.1　光流 23510.4.2　lucas-kanade算法 23710.5　更多示例 24310.5.1　图像修复 24310.5.2　利用分水岭变换进行分割 24410.5.3　利用霍夫变换检测直线 245练习 246附录a　安装软件包 247a.1　numpy和scipy 247a.1.1　windows 247a.1.2　mac os x 247a.1.3　linux 248a.2　matplotlib 248a.3　pil 248a.4　libsvm 249a.5　opencv 249a.5.1　windows 和 unix 249a.5.2　mac os x 249a.5.3　linux 250a.6　vlfeat 250a.7　pygame 250a.8　pyopengl 250a.9　pydot 251a.10　python-graph 251a.11　simplejson 252a.12　pysqlite 252a.13　cherrypy 252附录b　图像集 253b.1　flickr 253b.2　panoramio 254b.3　牛津大学视觉几何组 255b.4　肯塔基大学识别基准图像 255b.5　其他 256b.5.1　prague texture segmentation datagenerator与基准 256b.5.2　微软研究院grab cut数据集 256b.5.3　caltech 101 256b.5.4　静态手势数据库 256b.5.5　middlebury stereo数据集 256附录c　图片来源 257c.1　来自flickr的图像 257c.2　其他图像 258c.3　插图 258参考文献 259索引 263下载地址：网盘下载"}
{"content2":"今日CS.CV 计算机视觉论文速览Mon, 20 May 2019Totally 25 papers👉上期速览✈更多精彩请移步主页Interesting:📚边窗滤波Side Window Filtering，SWF, 中心的滤波框是造成边缘模糊的基本原因，研究人员提出了将框的边缘或者角点而不是中心用于待处理的像素位置。这种简单但有效的方法具有广泛的拓展性，并用于多种早期视觉任务中去。(from 深圳大学)目标像素不会处于边缘的中心，而是一边。所以将目标像素作为潜在边缘处理，并将某个窗的边缘或者角点与目标像素对齐，输出则是一系列边窗的叠加。边窗滤波器的构造，分为了旋转，左右，上下和四角，可以看到待处理的像素都位于某个小边框的角点或者边缘上。📚AM-LFS用于损失函数搜索的自动机器学习, 损失函数的设计对于模型训练至关重要，这篇文章提出了一种用于损失函数搜索的自动机器学习方法，将强化学习用于了损失函数搜索。关键的贡献在于对于搜索空间的设计可以保证在不同版本任务上的泛化性和迁移性。同时提出了高效的优化方法在训练过程中高效的优化损失函数的参数分布。(from 商汤)内部最小化样本损失，外部最大化网络的奖励：📚OHL-Auto-Aug在线超参数学习用于自动增强策略, 一种经济的方法来学习数据增强策略，基于参数化的概率分布，使得参数可以与网络共同优化，消除了充分训练和全局搜索的繁杂。(from 商汤 香港中文)在训练过程中，数据增强分布逐渐收敛：📚Texture Fields在函数空间中学习纹理表示, 点云的纹理映射问题，研究人员提出了纹理场的概念，在连续的3D函数参数化网络上实现新颖的纹理表示。绕开了形状离散和参数化，用独立于形状的纹理表示方法来进行映射。(from 图宾根大学 & MPI)纹理场的表示能力：隐空间插值和纹理迁移：ref:https://github.com/syb7573330/im2avatar📚EENA神经网络的高效进化, 自动神经架构搜索近年来取得了很大的成就但在搜索时的无方向性造成了计算量庞大。为了解决这一问题，这篇文章提出了名为EENA的进化方法，基于变异和交叉操作，将已经学习到的信息作为引导来为后续学习提供方向。在CIFAR-10上仅仅利用0.65GPU日就实现了8.47M参数的2.56%测试误差的架构，并可以方便地迁移到CIFAR-100。(from 中科院大学)一些变异操作：模型搜索过程中的树结构：📚单图像深度估计的理解, 研究了网络如何从rgb中学习到深度的过程，以MonoDepth为例探索了网络用于深度估计的视觉信息，发现了网络会忽略清晰尺寸而比较倾向利用垂直位置，但同时也需要已知相机位姿，在滚转和俯仰的情况下深度估计将变得不准。从垂直图像位置来估计深度但需要较强的边缘特征。 (from 代尔夫特理工)📚激光雷达传感器建模和数据增强, 利用图像迁移的方法从非配对数据中利用CycleGan建立传感器模型，并利用仿真环境sim2real得到逼真的激光雷达数据。还利用real2real从低分辨率数据得到了高分辨的激光雷达数据，激光雷达数据在鸟瞰视角（• Bird-eye View ，2D BEV）和2D极视角下（• Polar-Grid Map， 2D PGM）进行表示。(from 法雷奥Valeo)真实数据与合成数据：📚火山形变过程检测, 基于卫星合成孔径干涉仪得到的大气模式来进行火山形变过程检测。(from 英国布里斯托大学)基于合成数据生成的形变模式和对应的大气分层图像：合成数据训练及预测过程：📚PoreNet基于毛孔的高分辨率指纹识别系统, (from IIT Indore)基于毛孔的指纹识别过程：Daily Computer Vision PapersNeural Message Passing on Hybrid Spatio-Temporal Visual and Symbolic Graphs for Video UnderstandingAuthors Effrosyni Mavroudi, Benjam n B jar Haro, Ren Vidal视频理解中的许多问题需要标记在视频的不同部分同时发生的多个活动，包括参与这些活动的对象和参与者。然而，计算机视觉中的现有技术方法主要关注诸如动作分类，动作检测或动作分割之类的任务，其中通常仅需要预测一个动作标签。在这项工作中，我们提出了一种通用方法，用于对基于视频中的空间定位语义实体（例如演员和对象）的时空图的一个或多个节点进行分类。特别地，我们将基于语义标签空间的归因符号图与捕获视觉上下文和交互的属性时空视觉图组合，后者捕获多个标签之间的关系。我们进一步提出了一种神经消息传递框架，用于联合细化混合视觉符号图的节点和边缘的表示。我们的框架具有节点类型和边缘类型条件滤波器和自适应图形连接，用于将视觉节点连接到符号节点的软分配模块，反之亦然，用于强制语义一致性的ca符号图推理模块和用于聚合精细节点的da pooling模块和下游分类任务的边缘表示。我们展示了我们的方法在各种任务上的一般性，例如CAD 120数据集上的时间子活动分类和对象可供性分类以及大规模Charades数据集上的多标签时间动作定位，其中我们仅使用原始方法优于现有的深度学习方法RGB帧。AM-LFS: AutoML for Loss Function SearchAuthors Chuming Li, Chen Lin, Minghao Guo, Wei Wu, Wanli Ouyang, Junjie Yan设计有效的损失函数在视觉分析中起着重要作用。大多数现有的损失函数设计依赖于手工制作的启发式方法，这需要领域专家来探索大型设计空间，这通常是次优和耗时的。在本文中，我们提出了用于损失函数搜索AM LFS的AutoML，它利用REINFORCE在训练过程中搜索损失函数。这项工作的关键贡献是搜索空间的设计，通过在一个统一的公式中包含一堆现有的主要损失函数，可以保证不同视觉任务的泛化和可转移性。我们还提出了一种有效的优化框架，可以在训练过程中动态优化损失函数分布的参数。四个基准数据集的广泛实验结果表明，在没有任何技巧的情况下，我们的方法在各种计算机视觉任务中优于现有的手工制作的损失函数。Online Hyper-parameter Learning for Auto-Augmentation StrategyAuthors Chen Lin, Minghao Guo, Chuming Li, Wei Wu, Dahua Lin, Wanli Ouyang, Junjie Yan数据增强对现代深度学习技术的成功至关重要。在本文中，我们提出了用于自动增强的在线超参数学习OHL Auto Aug，这是一种经济的解决方案，可以学习增强策略分布以及网络训练。与以前以离线方式搜索增强策略的自动增强方法不同，我们的方法将增强策略表示为参数化概率分布，从而允许其参数与网络参数一起优化。我们提出的OHL Auto Aug消除了重新训练的需要，并大大降低了整体搜索过程的成本，同时建立了比基线模型显着的准确性改进。在CIFAR 10和ImageNet上，我们的方法在搜索精度方面取得了显着的成绩，在CIFAR 10上速度提高了60倍，在ImageNet上提高了24倍，同时保持了竞争力的准确性。Semantic Analysis of Traffic Camera Data: Topic Signal Extraction and Anomalous Event DetectionAuthors Jeffrey Liu, Andrew Weinert, Saurabh Amin交通管理中心TMC经常使用交通摄像头提供有关交通，道路和天气状况的态势感知。相机镜头对于各种诊断目的非常有用，但大多数镜头只保留了几天，如果有的话。这主要是因为目前通过人工操作员的人工审查来识别显着的镜头是一个费力且低效的过程。在本文中，我们提出了一种面向语义的方法来分析顺序图像数据，并展示其在自然检测现实世界，天气和交通状况异常事件中的应用。我们的方法从文本标签构建图像内容的语义矢量表示，这可以从现成的，预训练的图像标记软件中容易地获得。这些语义标签向量用于使用Latent Dirichlet Allocation LDA主题模型构建语义主题信号物理过程的时间序列表示。通过检测主题信号中的异常，我们识别出与冬季风暴和异常交通拥堵相对应的显着镜头。在针对现实世界事件的验证中，使用语义主题信号的异常检测明显优于使用任何单独标签信号的检测。LiDAR Sensor modeling and Data augmentation with GANs for Autonomous drivingAuthors Ahmad El Sallab, Ibrahim Sobh, Mohamed Zahran, Nader Essam在自动驾驶领域，来自真实车辆的数据收集和注释是昂贵的并且有时是不安全的。模拟器通常用于数据增强，这需要现实的传感器模型，这些传感器模型难以配制并以封闭形式建模。相反，可以从真实数据中学习传感器模型。主要挑战是缺少配对数据集，这使得传统的监督学习技术不适合。在这项工作中，我们将问题表述为来自不成对数据的图像转换，并采用CycleGAN来解决LiDAR的传感器建模问题，从模拟的LiDAR sim2real中生成逼真的LiDAR。此外，我们从较低分辨率的real2real生成高分辨率，逼真的LiDAR。 LiDAR 3D点云在Bird eye View和Polar 2D表示中处理。实验结果表明该方法具有很高的潜力。CNN-based Cost Volume Analysis as Confidence Measure for Dense MatchingAuthors Max Mehltretter, Christian Heipke由于其能够识别密集立体匹配中的错误视差分配，因此置信度估计对于广泛的应用是有益的，例如，自动驾驶，需要一定程度的信心作为强制性先决条件。特别是，基于深度学习的方法的引入导致近年来该领域的日益普及，这是由于显着提高的准确性引起的。尽管有这种显着的发展，但这些方法中的大多数仅依赖于从视差图中学习的特征，而没有考虑相应的三维成本量。然而，已经证明，使用基于手工制作特征的传统方法，可以使用该附加信息来进一步提高准确度。为了结合深度学习和基于成本量的特征的优势，本文提出了一种新颖的卷积神经网络CNN架构，可直接学习体积三维数据的置信度估计特征。使用三种常见的密集立体匹配技术对三个数据集进行广泛评估，证明了所提出方法的一般性和现有技术的准确性。A deep learning approach to detecting volcano deformation from satellite imagery using synthetic datasetsAuthors Nantheera Anantrasirichai, Juliet Biggs, Fabien Albino, David Bull卫星可以对火山进行广泛的，区域的或全球的监视，并可以提供火山爆发或火山爆发的第一个迹象。在这里，我们考虑干涉合成孔径雷达InSAR，它可用于检测表面变形与强烈的喷发统计链接。已经证明了机器学习在这些大型InSAR数据集中自动识别感兴趣信号的能力，但是数据驱动技术，例如卷积中性网络CNN需要平衡的正负信号训练数据集，以有效地区分真实变形和噪声。由于只有一小部分火山正在变形，大气噪声无处不在，因此利用机器学习来探测火山爆发更具挑战性。在本文中，我们使用合成干涉图来训练AlexNet来解决这个问题。合成干涉图由3部分1变形模式组成，基于蒙特卡罗选择的分析正演模型参数，2个分层大气效应源自天气模型和3个湍流大气效应基于相关噪声的统计模拟。基于分类准确度和阳性预测值PPV，使用合成数据训练的AlexNet体系结构优于仅使用真实干涉图训练的结构。然而，用于生成合成信号的模型是自然过程的简化，因此我们使用由合成模型和选定的实例组成的组合数据集重新训练CNN，实现最终PPV为82。尽管对整个数据集应用大气校正在计算上是昂贵的，但将它们应用于小的阳性结果子集相对简单。这进一步改善了检测性能，而没有显着增加计算负担。Texture Fields: Learning Texture Representations in Function SpaceAuthors Michael Oechsle, Lars Mescheder, Michael Niemeyer, Thilo Strauss, Andreas Geiger近年来，在基于学习的3D对象重建方面取得了实质性进展。同时，提出了可以生成高度逼真图像的生成模型。然而，尽管在这些密切相关的任务中取得了成功，但3D对象的纹理重建几乎没有受到研究界的关注，并且现有技术方法要么限于相对较低的分辨率，要么受限于实验设置。这些局限性的一个主要原因是纹理的常见表示对于现代深度学习技术而言效率低或难以界面。在本文中，我们提出了纹理场，这是一种新颖的纹理表示，它基于回归一个用神经网络参数化的连续三维函数。我们的方法绕过了形状离散化和参数化等限制因素，因为所提出的纹理表示独立于3D对象的形状表示。我们展示了Texture Fields能够代表高频纹理，并自然地融入现代深度学习技术。在实验上，我们发现纹理场比较有利于用于3D对象的条件纹理重建的现有技术方法，并且能够学习用于纹理化看不见的3D模型的概率生成模型。我们相信Texture Fields将成为下一代生成3D模型的重要组成部分。Neither Global Nor Local: A Hierarchical Robust Subspace Clustering For Image DataAuthors Maryam Abdolali, Mohammad Rahmati在本文中，我们考虑存在连续噪声，遮挡和伪装的子空间聚类问题。我们认为，在现有技术方法中数据的自我表达表示对遮挡和复杂的现实世界噪声非常敏感。为了缓解这个问题，我们提出了一个分层框架，它将基于局部补丁的表示和全局表示的判别属性的稳健性结合在一起。该方法包括1个自上而下的阶段，其中输入数据经历重复划分为较小的补丁，2个为自下而上的阶段，其中本地补丁的低等级嵌入在上层中的相应补丁的视野中在Grassmann流形上合并。这个汇总信息提供了上层相应补丁的两个关键信息，不能链接和推荐链接。该信息用于使用加权稀疏组套索优化问题来计算上层的每个补丁的自表达表示。几个真实数据集的数值结果证实了我们的方法的效率。Transfer Learning based Detection of Diabetic Retinopathy from Small DatasetAuthors Misgina Tsighe Hagos, Shri Kant注释训练数据不足仍然是在医学数据分类问题中应用深度学习的挑战之一。从已经训练过的深度卷积网络中进行转移学习可用于从头开始降低培训成本，并通过小型培训数据进行深度学习培训。这提出了一个问题，即我们是否可以使用转移学习来克服基于深度学习的医学数据分类中的训练数据不足问题。深度卷积网络已经在ImageNet大规模视觉识别竞赛ILSVRC图像分类挑战中取得了高性能结果。一个例子是Inception V3模型，它是ILSVRC 2015挑战赛的第一个参赛者。有助于在一个卷积级别中提取输入图像的不同大小特征的初始模块是Inception V3的独特功能。在这项工作中，我们使用了预训练的Inception V3模型来利用其用于糖尿病视网膜病变检测的Inception模块。为了解决标记的数据不足问题，我们对用于模型训练的较小版本的Kaggle Diabetic Retinopathy分类挑战数据集进行了子样本测试，并在先前未见过的数据子集上测试了模型的准确性。我们的技术可用于其他基于深度学习的医学图像分类问题，面临标记训练数据不足的挑战。Side Window FilteringAuthors Hui Yin, Yuanhao Gong, Guoping Qiu局部窗口通常用于计算机视觉，几乎无一例外，窗口的中心与正在处理的像素对齐。我们表明，这种传统智慧并非普遍适用。当像素位于边缘上时，将窗口的中心放置在像素上是导致许多滤波算法模糊边缘的基本原因之一。基于这种见解，我们提出了一种新的侧窗滤波SWF技术，该技术将窗口侧或角与正在处理的像素对齐。 SWF技术令人惊讶地简单但理论上根深蒂固且在实践中非常有效。我们展示了许多传统的线性和非线性滤波器可以在SWF框架下轻松实现。广泛的分析和实验表明，实现SWF原理可以显着提高其边缘保持能力，并在诸如图像平滑，去噪，增强，结构保持纹理去除，相互结构提取和HDR色调映射等应用中实现最先进的性能。除了图像过滤，我们进一步表明SWF原理可以扩展到涉及使用本地窗口的其他应用程序。通过优化着色作为示例，我们证明了实现SWF原理可以有效地防止诸如与传统实现相关的颜色泄漏之类的伪像。鉴于计算机视觉中基于窗口的操作无处不在，新的SWF技术可能会使更多应用受益。Group Re-Identification with Multi-grained Matching and IntegrationAuthors Weiyao Lin, Yuxi Li, Hao Xiao, John See, Junni Zou, Hongkai Xiong, Jingdong Wang, Tao Mei重新识别不同摄像机视图的人群的任务是一个重要但研究较少的问题。组识别Re ID是一项非常具有挑战性的任务，因为它不仅受到传统单个对象的ID问题的不利影响，例如视点和人体姿势变化，但它也受到组布局和组成员身份变化的影响。在本文中，我们提出了一种新的群体粒度概念，通过多粒度对象的个体人物和一组内的两个和三个人的子群来表征群体图像。为了实现健壮的组Re ID，我们首先引入多粒度表示，这些表示可以通过开发两个单独的方案来提取，即一个用手工制作的描述符，另一个用深度神经网络。所提出的表示旨在表征多粒度对象的空间和空间关系，并进一步配备重要性权重，其捕获组内动态的变化。通过多阶匹配过程促进最优群组匹配，进而以迭代方式动态更新重要性权重。我们对包含复杂场景和大动态的三个多摄像机组数据集进行了评估，实验结果证明了我们的方法的有效性。Non-Parametric Priors For Generative Adversarial NetworksAuthors Rajhans Singh 1 , Pavan Turaga 1 , Suren Jayasuriya 1 , Ravi Garg 2 , Martin W. Braun 2 1 Arizona State University, 2 Intel Corporation生成对抗网络GAN的出现使得迄今为止认为非常具有挑战性的综合，插值和数据增强方面的新功能成为可能。然而，大多数GAN架构中的一个常见假设是假设简单的参数潜在空间分布。虽然易于实现，但简单的潜在空间分布对于诸如插值的用途可能是有问题的。这是由于在潜在空间中插入样本时的分布不匹配。我们使用概率论和现成的优化工具的基本结果提出了这个问题的直接形式化，我们开发了达到适当的非参数先验的方法。所获得的先验在其形状方面表现出不寻常的定性特性，并且在其中点分布的较低发散方面具有定量益处。我们证明了我们设计的先验有助于在插值期间沿着任何欧几里德直线改善图像生成，无论是定性还是定量，无需任何额外的培训或架构修改。拟议的表述非常灵活，为潜在的空间统计施加更新的限制铺平了道路。How do neural networks see depth in single images?Authors Tom van Dijk, Guido C.H.E. de Croon深度神经网络已经导致单个图像的深度估计的突破。最近的工作通常侧重于深度图的准确性，其中对公开可用的测试集（例如KITTI视觉基准）的评估通常是本文的主要结果。虽然这样的评估显示神经网络可以很好地估计深度，但它并没有显示它们是如何做到这一点的。据我们所知，目前还没有任何工作可以分析这些网络学到了什么。PoreNet: CNN-based Pore Descriptor for High-resolution Fingerprint RecognitionAuthors Vijay Anand, Vivek Kanhangad随着高分辨率指纹扫描仪的发展，基于高分辨率指纹的生物识别技术近年来受到越来越多的关注。这封信提出了一种基于毛孔特征的生物识别方法。我们的方法采用卷积神经网络CNN模型DeepResPore来检测输入指纹图像中的毛孔。此后，针对每个检测到的孔周围的补片计算基于CNN的描述符。具体来说，我们设计了一个基于剩余学习的CNN，称为PoreNet，它从孔隙补丁中学习独特的特征表示。为了验证，通过使用欧几里德距离以双向方式比较从一对指纹图像获得的孔描述符来生成匹配分数。所提出的高分辨率指纹识别方法在部分DBI和基准PolyU HRF数据集的完整DBII指纹上实现了2.56和0.57相等的错误率EER。最重要的是，它在两个数据集上实现了比当前最先进方法更低的FMR1000和FMR10000值。POPQORN: Quantifying Robustness of Recurrent Neural NetworksAuthors Ching Yun Ko, Zhaoyang Lyu, Tsui Wei Weng, Luca Daniel, Ngai Wong, Dahua Lin对抗性攻击的脆弱性一直是深度神经网络的关键问题。解决此问题需要一种可靠的方法来评估网络的健壮性。最近，已经开发了几种方法来计算神经网络的纹理稳健性量化，即最小对抗扰动的经认证的下界。然而，这些方法被设计用于前馈网络，例如，多层感知器或卷积网络。量化循环网络的稳健性仍然是一个悬而未决的问题，特别是LSTM和GRU。对于这样的网络，在计算鲁棒性量化方面存在额外的挑战，例如在多个步骤处理输入以及门和状态之间的交互。在这项工作中，我们提出textit文本POPQORN textbf P ropagated textbf o ut textbf p ut textbf Q uantified R textbf o bustness for textbf RN Ns，这是一种量化RNN稳健性的通用算法，包括vanilla RNN，LSTM和GRU。我们在不同的网络架构上展示了它的有效性，并表明各个步骤的稳健性量化可以带来新的见解。Integer Discrete Flows and Lossless CompressionAuthors Emiel Hoogeboom, Jorn W.T. Peters, Rianne van den Berg, Max Welling无损压缩方法使用统计模型缩短了数据的预期表示大小而不丢失信息。基于流的模型在此设置中很有吸引力，因为它们允许精确的似然优化，这相当于最小化每条消息的预期比特数。然而，传统流程假设连续数据，这可能在量化压缩时导致重建错误。出于这个原因，我们引入了一个称为整数离散流IDF的序数离散数据的生成流，这是一种双向整数映射，可以学习高维数据的丰富变换。作为IDF的构建模块，我们引入了称为整数离散耦合和下三角耦合的灵活变换层。我们的实验表明，IDF与其他基于流动的生成模型竞争。此外，我们证明基于IDF的压缩在CIFAR10，ImageNet32和ImageNet64上实现了最先进的无损压缩率。EENA: Efficient Evolution of Neural ArchitectureAuthors Hui Zhu, Zhulin An, Chuanguang Yang, Kaiqiang Xu, Yongjun Xu用于自动神经结构搜索的最新算法在搜索空间中表现显着但基本上没有方向性，并且在每个中间架构的训练中计算成本高。在本文中，我们提出了一种有效的架构搜索方法，称为EENA神经架构的高效演化，已经学习了由信息引导的突变和交叉操作，以加速这一过程，并通过减少冗余搜索和训练来减少计算工作量。在CIFAR 10分类中，使用最小计算资源0.65 GPU天的EENA可以设计高效的神经结构，其在8.47M参数下实现2.56测试误差。此外，发现的最佳架构也可转换为CIFAR 100。Finding Rats in Cats: Detecting Stealthy Attacks using Group Anomaly DetectionAuthors Aditya Kuppa, Slawomir Grzonkowski, Muhammad Rizwan Asghar, Nhien An Le Khac高级攻击活动跨越多个阶段并长时间保持隐身状态。越来越多的攻击者使用现成的工具和预安装的系统应用程序（如emph powershell和emph wmic）来逃避检测，因为系统管理员和安全分析师也使用相同的工具来执行日常任务的合法目的。要开始调查，可以从操作系统收集事件日志，但是，这些日志非常通用，并且通常无法将潜在攻击归因于特定攻击组。文献中最近的方法使用异常检测技术，其旨在区分计算机或网络系统的恶意和正常行为。遗憾的是，基于点异常的异常检测系统在某种意义上过于严格，以至于它们可能会错过恶意活动并对攻击进行分类，而不是异常值。因此，对于更好地检测恶意活动存在研究挑战。为了应对这一挑战，在本文中，我们利用Group Anomaly Detection GAD，它可以检测各个数据点的异常集合。Training Object Detectors With Noisy DataAuthors Simon Chadwick, Paul Newman大量标记的训练数据的可用性对于现代物体探测器的训练是至关重要的。手动标记训练数据是耗时且昂贵的，而自动标记方法不可避免地会给标签添加不需要的噪音。我们研究了不同类型的标签噪声对物体探测器性能的影响。然后，我们展示了如何改进协同教学，一种用于处理噪声标签和之前在分类问题上演示的方法，以减轻物体检测设置中标签噪声的影响。我们使用KITTI数据集上的模拟噪声和使用自动标记数据的车辆检测任务来说明我们的结果。Mechanically Powered Motion Imaging Phantoms: Proof of ConceptAuthors Alberto Gomez, Cornelia Schmitz, Markus Henningsson, James Housden, Yohan Noh, Veronika A. Zimmer, James R. Clough, Ilkay Oksuz, Nicolas Toussaint, Andrew P. King, Julia A. Schnabel运动成像模型昂贵，笨重且难以运输和设置。本文的目的是演示一种设计多模态运动成像模型的简单方法，该模型使用机械存储的能量来产生运动。我们提出两种幻影设计，使用主发条和弹性带来储存能量。在每个模型的传动链的端部处将矩形件连接到轴上，并且在释放机械马达时进行旋转运动。用MRI和US对模体进行成像，并将图像序列嵌入一维非线性流形拉普拉斯特征图中，并使用嵌入的光谱图来推导随时间的角速度。导出的速度在一个小误差内是一致的和可重复的。提出的运动模型概念显示了构建简单且价格合理的运动模型的巨大潜力Deep Unified Multimodal Embeddings for Understanding both Content and Users in Social Media NetworksAuthors Karan Sikka, Lucas Van Bramer, Ajay Divakaran在过去几年中，社交媒体网络上产生的多模式内容爆炸式增长，这使得必须更深入地了解社交媒体内容和用户行为。我们为社交多媒体内容分析提出了一种新颖的内容独立内容用户反应模型与通常单独处理语义内容理解和用户行为建模的先前工作相比，我们在统一框架内提出了对这些问题的通用解决方案。我们将用户，图像和文本从开放的社交媒体中嵌入到一个共同的多模式几何空间中，使用一种新的损失函数来设计应对远程和不同的模态，从而实现无缝的三向检索。我们的模型不仅优于基于单峰嵌入的交叉模态检索任务方法，而且还显示了联合解决Twitter数据上的两个任务所带来的改进。我们还表明，与在Instagram数据上使用单峰内容学习的用户相比，在我们的联合多模式嵌入模型中学习的用户嵌入更能预测用户兴趣。因此，我们的框架超越了使用显式领导者关注者链接信息通过从孤立用户中提取隐含内容中心关联来建立从属关系的先前实践。我们提供定性结果，以表明从学习嵌入中出现的用户群具有一致的语义和我们的模型从嘈杂和非结构化数据中发现细粒度语义的能力。我们的工作表明，社交多模态内容本质上是多模式的，并且具有一致的结构，因为在社交网络中，意义是通过用户和内容之间的交互来创建的。Dream Distillation: A Data-Independent Model Compression FrameworkAuthors Kartikeya Bhardwaj, Naveen Suda, Radu Marculescu模型压缩非常适合在物联网设备上部署深度学习。但是，现有的模型压缩技术依赖于对原始数据集或某些备用数据集的访问。在本文中，我们解决了当没有可用的实际数据时的模型压缩问题，例如，当数据是私有的时。为此，我们提出了Dream Distillation，一种独立于数据的模型压缩框架。我们的实验表明，Dream Distillation可以在CIFAR 10测试集上达到88.5的准确度，而无需对原始数据进行实际训练GlidarCo: gait recognition by 3D skeleton estimation and biometric feature correction of flash lidar dataAuthors Nasrin Sadeghzadehyazdi, Tamal Batabyal, Nibir K. Dhar, B. O. Familoni, K. M. Iftekharuddin, Scott T. Acton使用非侵入性获取数据的步态识别在过去十年中引起了越来越多的关注。在各种数据源模式中，通过实验发现，涉及骨架表示的数据适用于可靠的特征压缩和快速处理。利用来自拟合模型（如骨架）的特征的基于模型的步态识别方法因其视图和尺度不变属性而被识别。我们提出了一种基于模型的步态识别方法，使用由单个闪光激光雷达记录的序列。利用Kinect和Mocap收集的高质量骨架数据的现有最先进模型方法仅限于受控实验室环境。传统研究工作的表现受到数据质量差的负面影响。我们解决了在具有挑战性的情况下的步态识别问题，例如激光雷达的低质量和嘈杂的成像过程，这降低了现有技术骨架系统的性能。我们提出GlidarCo以在所描述的条件下获得高度准确的步态识别。过滤机制校正有缺陷的骨架关节测量，并且将稳健的统计数据集成到传统特征矩以编码运动的动态。作为比较，研究从噪声骨架中提取的基于长度和基于矢量的特征以用于异常值去除。实验结果说明了在噪声低分辨率激光雷达数据的情况下，所提出的方法在改善步态识别方面的功效。Semi-supervised learning based on generative adversarial network: a comparison between good GAN and bad GAN approachAuthors Wenyuan Li, Zichen Wang, Jiayun Li, Jennifer Polson, William Speier, Corey Arnold最近，基于生成对抗网络GAN的半监督学习方法受到了很多关注。其中，两种不同的方法在各种基准数据集上取得了竞争性结果。 Bad GAN学习分类器，其中分布在输入数据支持的补充上的不切实际的样本。相反，Triple GAN包含一个三人游戏，试图利用良好生成的样本来提升分类结果。在本文中，我们在不同的基准数据集上对这两种方法进行了全面的比较。我们展示了它们在图像生成方面的不同属性，以及对提供的标记数据量的敏感性。通过综合比较这两种方法，我们希望能够揭示基于GAN的半监督学习的未来。Chinese Abs From Machine TranslationPapers from arxiv.org更多精彩请移步主页pic from pexels.com"}
{"content2":"做机器视觉和图像处理方面的研究工作，最重要的两个问题：其一是要把握住国际上最前沿的内容；其二是所作工作要具备很高的实用背景。解决第一个问题 的办法就是找出这个方向公认最高成就的几个超级专家(看看他们都在作什么)和最权威的出版物(阅读上面最新的文献)，解决第二个问题的办法是你最好能够找 到一个实际应用的项目，边做边写文章。 做好这几点的途径之一就是利用网络资源，利用权威网站和专家们的个人主页。依照下面目录整理：[1]. 研究群体(国际国内)[2]. 专家主页[3]. 前沿国际国内期刊与会议[4]. 搜索资源[5]. GPL软件资源一、研究群体用来搜索国际知名计算机视觉研究组(CV Groups)：国际计算机视觉研究组清单http://peipa.essex.ac.uk/info/groups.html美国计算机视觉研究组清单 http://peipa.essex.ac.uk/info/groups.html#USAhttp://www-2.cs.cmu.edu/~cil/vision.html或 http://www.cs.cmu.edu/~cil/vision.html这是卡奈基梅隆大学的计算机视觉研究组的主页，上面提供很全的资料，从发表文章的下载到演示程序、测试图像、常用链接、相关软硬件，甚至还有一个搜索引擎。著名的有人物Tomasi， Kanade等。卡内基梅隆大学双目实验室http://vision.middlebury.edu/stereo/卡内基梅隆研究组http://www.cs.cmu.edu/~cil/v-groups.html还有几个实验室：Calibrated Imaging Laboratory 图像Digital Mapping Laboratory 映射Interactive Systems Laboratory 互动Vision and Autonomous Systems Center视觉自适应http://www.via.cornell.edu/康奈尔大学的计算机视觉和图像分析研究组，好像是电子和计算机工程系的。侧重医学方面的研究，但是在上面有相当不错资源，关键是它正在建设中，能够跟踪一些信息。Cornell University——Robotics and Vision grouphttp://www-cs-students.stanford.edu/ 斯坦福大学计算机系主页1. http://white.stanford.edu/2. http://vision.stanford.edu/3. http://ai.stanford.edu/美国斯坦福大学人工智能机器人实验室The Stanford AI Lab (SAIL) is the intellectual home for researchers in the Stanford Computer Science Department whose primary research focus is Artificial Intelligence. The lab is located in the Gates...Vision and Imaging Science and Technologyhttp://www.fmrib.ox.ac.uk/analysis/主要研究：Brain Extraction Tool， Nonlinear noise reduction， Linear Image Registration， Automated Segmentation， Structural brain change analysis， motion correction， etc.http://www.cse.msu.edu/prip/—密歇根州立大学计算机和电子工程系的模式识别--图像处理研究组，它的FTP上有许多的文章(NEW)。美国密歇根州大学认知模型和图像处理实验室The Pattern Recognition and Image Processing (PRIP) Lab faculty and students investigate the use of machines to recognize patterns or objects. Methods are developed to sense objects， to discover which...http://www.cse.msu.edu/rgroups/prip/http://pandora.inf.uni-jena.de/p/e/index.html德国的一个数字图像处理研究小组，在其上面能找到一些不错的链接资源。柏林大学 http://www.cv.tu-berlin.de/德国波恩大学视觉和认识模型小组Computer Vision Group located within the Division III of the Computer Science Department in the University of Bonn in Germany. This server offers information on topics concerning our computer vision http://www-dbv.informatik.uni-bonn.de/http://www-staff.it.uts.edu.au/~sean/CVCC.dir/home.htmlCVIP(used to be CVCC for Computer Vision and Cluster Computing) is a research group focusing on cluster-based computer vision within the Spiral Architecture.http://cfia.gmu.edu/The mission of the Center for Image Analysis is to foster multi-disciplinary research in image， multimedia and related technologies by establishing links between academic institutes， industry and government agencies， and to transfer key technologies to help industry build next generation commercial and military imaging and multimedia systems.英国的Bristol大学的Digital Media Group在高级图形图像方面不错。主要就是涉及到场景中光线计算的问题，比如用全局光照或是各种局部光照对高动态图的处理，还有近似真实的模拟现实环境 (照片级别的)，还有用几张照片来建立3D模型(人头之类的)。另外也有对古代建筑模型复原。http://www.cs.bristol.ac.uk /Research/Digitalmedia/而且根据Times全英计算机排名在第3， 也算比较顶尖的研究了http://www.cmis.csiro.au/IAP/zimage.htm这是一个侧重图像分析的站点，一般。但是提供一个Image Analysis环境---ZIMAGE and SZIMAGE。麻省理工视觉实验室MIT http://groups.csail.mit.edu/vision/welcome/AI Laboratory Computer Vision groupCenter for Biological and Computational LearningMedia Laboratory， Vision and Modeling GroupPerceptual Science groupUC Berkeley http://0-vision.berkeley.edu.ilstest.lib.neu.edu/vsp/index.htmlhttp://www.cs.berkeley.edu.ilste ... n/vision_group.html加州大学伯克利分校视觉实验室David A. Forsyth：http://www.cs.berkeley.edu/~daf/UCLA(加州大学洛杉矶分校) http://vision.ucla.edu/视觉实验室英国牛津的A.Zisserman：http://www.robots.ox.ac.uk/~az/ 机器人实验室美国南加州大学智能机器人和智能系统研究所University of Southern California， Los AngelesIRIS is an interdepartmental unit of USC's School of Engineering with ties to USC's Information Sciences Institute (ISI). Members include faculty， graduate students， and research staff associated with... http://iris.usc.edu/ Computer Vision 实验室美国南加州大学计算机视觉实验室介绍：Computer Vision Laboratory at the University of Southern California is one of the major centers of computer vision research for thirty years. they conduct research in a number of basic and applied are...http://iris.usc.edu/USC-Computer-Vision.html英国约克大学高级计算机结构神经网络小组The Advanced Computer Architecture Group has had a thriving research programme in neural networks for over 10 years. The 15 researchers， led by Jim Austin， focus their work in the theory and applicati...http://www.cs.york.ac.uk/arch/neural/瑞士戴尔莫尔感知人工智能研究所IDIAP is a research institute established in Martigny in the Swiss Alps since 1991. Active in the areas of multimodal interaction and multimedia information management， the institute is also the leade...http://www.idiap.ch/英国萨里大学视觉，语言和信号处理中心The Centre for Vision， Speech and Signal Processing (CVSSP) is more than 60 members strong， comprising 12 academic staff， 18 research fellows and more than 44 research students. The activities of the ...http://www.ee.surrey.ac.uk/Research/VSSP/美国阿默斯特马萨诸塞州立大学计算机视觉实验室The Computer Vision Laboratory was established in the Computer Science Department at the University of Massachusetts in 1974 with the goal of investigating the scientific principles underlying the con...http://vis-www.cs.umass.eduUniversity of Massachusetts——Computer Vision Laboratory for Perceptual Robotics美国芝加哥伊利诺伊斯大学贝克曼研究中心智能机器人和计算机视觉实验室Includes the following groups: Professor Seth Hutchinson's Research Group Professor David Kriegman's Research Group Professor Jean Ponce's Research Group Professor Narendra Ahuja's Research Gro...http://www-cvr.ai.uiuc.edu/Computer Vision and Robotics LaboratoryVision Interfaces and Systems Laboratory (VISLab)英国伯明翰大学计算机科学学校视觉研究小组The vision group at the School of Computer Science (a RAE 5 rated department) performs research into a wide variety of computer vision and image understanding areas. Much of this work is performed in ...http://www.cs.bham.ac.uk/research/vision/微软研究院机器学习与理解研究小组 / 计算机视觉小组The research group focuses on the development of more advanced and intelligent computer systems through the exploitation of statistical methods in machine learning and computer vision. The site lists ...http://research.microsoft.com/mlp/http://research.microsoft.com/en-us/groups/vision/微软公司的文献：http://research.microsoft.com/research/pubs微软亚洲研究院：http://research.microsoft.com/asia/，值得关注Harry Shum， Jian Sun， Steven Lin， Long Quan(兼职HKUST)etc.瑞典隆德大学数学系视觉组：http://www.maths.lth.se/matematiklth/personal/andersp/感觉国外搞视觉的好多是数学系出身，大约做计算机视觉对数学要求很高吧。澳大利亚国立大学：http://users.rsise.anu.edu.au/~hartley/美国北卡大学：http://www.cs.unc.edu/~marc/法国INRIA：http://www-sop.inria.fr/odyssee/team/ 由Olivier.Faugeras领衔的牛人众多。比利时鲁汶大学的L.Van Gool： www.esat.kuleuven.ac.be/psi/visics/据说在这个只有中国一个小镇大小的地方的鲁汶大学在欧洲排行top10，名列世界top100，还出了几个诺贝尔奖，视觉研究也很强.美国明德http://vision.middlebury.edu/stereo/以下含有非顶尖美国学校研究组，没有链接(个别的上面已经提到)，供参考。Amerinex Applied Imaging， Inc.Boston UniversityImage and Video Computing Research groupUniversity of California at Santa Barbara加州大学芭芭拉分校Vision Research LabUniversity of California at San Diego加州大学圣迭戈分校Computer Vision & Robotics Research LaboratoryVisual Computing laboratoryUniversity of California at Irvine加州大学欧文分校，加州南部一城，在圣安娜东南，Computer Vision laboratoryUniversity of California， Riverside加州大学河滨分校Visualization and Intelligent Systems Laboratory (VISLab)University of California at Santa CruzPerceptual Science LaboratoryCaltech (加州理工)Vision groupUniversity of Central FloridaComputer Vision laboratoryUniversity of FloridaCenter for Computer Vision and VisualizationColorado State UniversityComputer Vision groupColumbia UniversityAutomated Vision Environment (CAVE)Robotics groupUniversity of Georgia， AthensVisual and Parallel Computing LaboratoryHarvard University（哈佛）Robotics LaboratoryUniversity of Illinois at Urbana-ChampaignRobotics and Computer VisionUniversity of IowaDivision of Physiologic ImagingJet Propulsion LaboratoryMachine Vision and Tracking Sensors groupKhoral Research， IncLawrence Berkeley LaboratoriesImaging and Collaborative Computing GroupImaging and Distributed ComputingLehigh UniversityImage Processing and Pattern Analysis LabVision And Software Technology LaboratoryUniversity of LouisvilleComputer Vision and Image Processing LabUniversity of MarylandComputer Vision LaboratoryUniversity of MiamiUnderwater Vision and Imaging LaboratoryUniversity of Michigan密歇根AI LaboratoryMichigan State University 密歇根州立Pattern Recognition and Image Processing laboratoryEnvironmental Research Institute of Michigan (ERIM) 密歇根大学有汽车车身检测研究University of Missouri-ColumbiaComputational Intelligence Research LaboratoryNECComputer Vision and Image ProcessingUniversity of NevadaComputer Vision LaboratoryNotre-Dame UniversityVision-Based Robotics using EstimationOhio State UniversitySignal Analysis and Machine Perception LaboratoryUniversity of PennsylvaniaGRASP laboratoryMedical Image Processing groupVision Analysis and Simulation Technologies (VAST) LaboratoryPenn State University 宾夕法尼亚大学Computer VisionPrecision Digital ImagesPurdue University普渡大学Robot Vision laboratoryVideo and Image Processing Laboratory (VIPER)Rensselaer Polytechnic Institute (RPI)Computer Science VisionUniversity of RochesterCenter for Electronic Imaging SystemsVision and Robotics laboratoryRutgers University (The State University of New Jersey)Image Understanding LabUniversity of Southern CaliforniaComputer VisionUniversity of South FloridaImage Analysis Research groupStanford Research Institute International (SRI)RADIUS -- Research and Development for Image Understanding SystemsThe Perception program at SRI's AI CenterSUNY at Stony BrookComputer Vision LabUniversity of TennesseeImaging， Robotics and Intelligent Systems laboratoryUniversity of Texas， AustinLaboratory for Vision SystemsUniversity of UtahCenter for Scientific Computing and ImagingRobotics and Computer VisionUniversity of VirginiaComputer Vision Research (CS)University of WashingtonImage Computing Systems LaboratoryInformation Processing LaboratoryCVIA LaboratoryUniversity of West FloridaImage Analysis/Robotics Research LaboratoryUniversity of WisconsinComputer Vision groupVanderbilt UniversityCenter for Intelligent SystemsWashington State UniversityImaging Research laboratoryWright-PattersonModel-Based Vision laboratoryWright State UniversityIntelligent Systems LaboratoryUniversity of WyomingWyoming Image and Signal Processing Research (WISPR)Yale UniversityComputational Vision Group http://www.cs.yale.edu/School of Medicine， Image Processing and Analysis group国内：中科院模式识别国家重点实验室 http://www.nlpr.ia.ac.cn/English/rv/mainpage.html虹膜识别、掌纹识别、人脸识别、莲花山http://www.stat.ucla.edu/~sczhu/Lotus/天津大学精密测试技术及仪器国家重点实验室研究方向包括：激光及光电测试技术、传感及测量信息技术、微纳测试与制造技术、制造质量控制技术。该实验室是国内精密测试领域惟一的国家重点实验室。“智能微系统及其集成应用技术”、“微结构光学测试技术”、“油气储运安全检测技术”、“先进制造中的视觉测量及其关键技术”、“正交偏振激光器原理、特性及其在精密计量中的应用研究”等5项代表性成果（07.3）。中科院长春光机所 http://www.ciomp.ac.cn/ny/keyan.asp中科院沈阳自动化所http://www.sia.ac.cn/index.php中科院西安光机所http://www.opt.ac.cn/yanjiushi/gpcxjs1.htm北京大学智能科学系http://www.cis.pku.edu.cn/vision/vision.htm三维视觉计算与机器人，生物特征识别与图像识别二、专家网页http://www.ai.mit.edu/people/wtf/这位可是MIT人工智能实验室的BILL FREEMAN。专长是：理解--贝叶斯模型。http://www.merl.com/people/brand/MERL(Mitsubishi Electric Research Laboratory)中的擅长“Style Machine”。http://research.microsoft.com/~ablake/CV界极有声望的A.Blake 1977年毕业于剑桥大学三一学院并或数学与电子科学学士学位。之后在MIT，Edinburgh，Oxford先后组建过研究小组并成为Oxford的教授，直到1999年进入微软剑桥研究中心。主要工作领域是计算机视觉。http://www-2.cs.cmu.edu/afs/cs.cmu.edu/user/har/Web/home.html这位专家好像正在学习汉语，主页并且搜集了诸如“两只老虎(Two Tigers)”的歌曲。他的主页上面还有几个专家：Shumeet Baluja， Takeo Kanade。他们的Face Detection作的绝对是世界一流。毕业于卡奈基梅隆大学的计算机科学系，兴趣是计算机视觉。三、前沿国际国内期刊与会议这里的期刊大部分都可以通过上面的专家们的主页间接找到1.国际会议 2.国际期刊 3.国内期刊 4.神经网络 5.CV 6.数字图象 7.教育资源，大学 8.常见问题1. 国际会议现在，国际上计算机视觉方面的三大国际会议是ICCV， CVPR和ECCV，统称之为ICE。ICCV的全称是International Comference on Computer Vision。ICCV两年一次，与ECCV正好错开，是公认的三个会议中级别最高的。ECCV的全称是Europeon Conference on Computer Vision，是一个欧洲的会议。CVPR的全称是Internaltional Conference on Computer Vision and Pattern Recogintion国际计算机视觉与模式识别会议。这是一个一年一次的会议，举办地在美国。ICIP—BMVC—MVA—国际模式识别会议(ICPR )：亚洲计算机视觉会议(ACCV)：2.国际期刊以计算机视觉为主要内容之一的国际刊物也有很多，如:International Journal of Computer VisionIEEE Trans. On PAMI http://www.computer.org/tpami/index.htmIEEE Transactionson Image Processing http://www.ieee.org/organizations/pubs/transactions/tip.htmPattern Recognition http://www.elsevier.com/locate/issn/00313203Pattern Recognition Letters http://www.elsevier.com/locate/issn/01678655IEEE Trans. on Robotics and Automation，IEEE TPAMIIEEE TIPCVGIP Computer Vision. Graphics and Image Processing，Visual Image Computing，IJPRAI(Internatiorial Journat of Pattern Recognition and Artificial Intelligence)众所周知， computer vision(cv) 存在ICCV/CVPR/ECCV三个顶级会议，它们档次差不多，都应该在一流会议行列， 没有必要给个高下。有些us的人认为ICCV/CVPR略好于ECCV，而欧洲人大都认为ICCV/ECCV略好于CVPR，某些英国的人甚至认为 BMVC好于CVPR。简言之， 三个会议差不多， 各有侧重和偏好。笔者就个人经验浅谈三会异同， 以供大家参考和讨论。 三者乃cv领域的旗舰和风向标，其oral paper (包括best paper) 代表当年度cv的最高水准， 在此引用Harry Shum的一句话， 想知道某个领域在做些什么，找最近几年此领域的proceeding看看就知道了。 ICCV/CVPR由IEEE Computer Society牵头组织， ECCV好像没有专门负责的组织。 CVPR每年(除2002年)都在美国开， ECCV每两年开一次，仅限欧洲， ICCV也是每两年一次，各洲轮值。 基本可以保证每年有两个会议开， 这样研究者就有两次跻身牛会的机会。就录取率而言， 三会都有波动。 如ICCV2001录取率>30%，且出现两个人(华人)各有三篇第一作者的paper的情况， 这在顶级牛会是不常见的 (灌水嫌疑)。 但是， ICCV2003， 2005两次录取率都很低， 大约20%左右。 ECCV也是类似规律， 在2004年以前都是>30%， 2006年降低到20%左右。 CVPR的录取率近年来一直偏高，从2004年开始一直都在[25%，30%]。最近一次CVPR2006是28.1%， CVPR2007还不知道统计数据。笔者猜测为了维持录取paper的绝对数量， 当submission少的时候录取率偏高， 反之偏低，近几年三大会议的投稿数量全部超过1000， 相对2000年前， 三会录取率均大幅度降低，最大幅度50%->20%。 对录取率走势感兴趣的朋友， 可参考 http://vrlab.epfl.ch/~ulicny/statistics/(CVPR2004的数据是错的)，http: //www.adaptivebox.net/research/bookmark/CICON_stat.html.显然， 投入cv的人越来越多，这个领域也是越来越大， 这点颇不似machine learning一直奉行愚蠢的小圈子主义。另外一点值得注意， ICCV/ECCV只收vision相关的topic，而cvpr会收少量的pattern recognition paper， 如finger print等，但是不收和image/video完全不占边的pr paper，如speech recognition等。我一个朋友曾经review过一篇投往CVPR的speech的paper， 三个reviewer一致拒绝，其中一个reviewer搞笑的指出， 你这篇paper应该是投ICASSP被据而转投CVPR的。 就topic而言， CVPR涵盖最广。 还有一个没有验证过的原因导致CVPR录取率高: 很多us的researcher不愿意或没有足够的经费到us以外的地方开会， 故CVPR会优先接收很多来自us的paper (让大家都happy)。以上对三会的分析对我们投paper是很有指导作用的。 目前的research我想绝大部分还是纸上谈兵， 必经 read paper -> write paper -> publish paper -> publish paper on top conferences and journals流程。故了解投paper的一些基本技巧， 掌握领域的走向和热点， 是非常必要的。 避免做无用功，选择切合的topic，改善presentation， 注意格式 (遵守规定的模板)， 我想这是很多新手需要注意的问题。如ICCV2007明文规定不写summary page直接reject， 但是仍然有人忽视， 这是相当不值得的。3.国内期刊自动化学报、计算机学报、软件学报、电子学报，中国图象图形学报，模式识别与人工智能，光电子激光，精密光学工程等。4.神经网络神经网络-Neural Networks Tutorial Reviewhttp://hem.hj.se/~de96klda/NeuralNetworks.htmftp://ftp.sas.com/pub/neural/FAQ.htmlImage Compression with Neural Networkshttp://www.comp.glam.ac.uk/digimaging/neural.htmBackpropagator's Reviewhttp://www.dontveter.com/bpr/bpr.htmlBibliographies on Neural Networkshttp://liinwww.ira.uka.de/bibliography/Neural/Intelligent Motion Control with an Artificial Cerebellumhttp://www.q12.org/phd.htmlKernel Machineshttp://www.kernel-machines.org/Some Neural Networks Research Organizationshttp://www.ieee.org/nnc/http://www.inns.org/Neural Network Modeling in Vision Researchhttp://www.rybak-et-al.net/nisms.htmlNeural Networks and Machine Learninghttp://learning.cs.toronto.edu/Neural Application Softwarehttp://attrasoft.comNeural Network Toolbox for MATLABhttp://www.mathworks.com/products/neuralnet/Netlab Softwarehttp://www.ncrg.aston.ac.uk/netlab/Kunama Systems Limited http://www.kunama.co.uk/5.Computer Vision(计算机视觉)Annotated Computer Vision Bibliographyhttp://iris.usc.edu/Vision-Notes/bibliography/contents.htmlhttp://iris.usc.edu/Vision-Notes/rosenfeld/contents.htmlLawrence Berkeley National Lab Computer Vision and Robotics Applicationshttp://www-itg.lbl.gov/ITG.hm.pg.docs/VISIon/vision.htmlCVonline by University of EdinburghThe Evolving， Distributed， Non-Proprietary， On-Line Compendium of Computer Vision， www.dai.ed.ac.uk/CVonlineComputer Vision Handbook，www.cs.hmc.edu/~fleck/computer-vision-handbookVision Systems Coursewarewww.cs.cf.ac.uk/Dave/Vision_lecture/Vision_lecture_caller.htmlResearch Activities in Computer Visionhttp://www-syntim.inria.fr/syntim/analyse/index-eng.htmlVision Systems Acronymswww.vision-systems-design.com/vsd/archive/acronyms.htmlDictionary of Terms in Human and Animal Visionhttp://cns-web.bu.edu/pub/laliden/WWW/Visionary/Visionary.htmlMetrology based on Computer Visionwww.cranfield.ac.uk/sme/amac/research/metrology/metrology.html6.Digital Photography 数字图像Digital Photography， Scanning， and Image Processingwww.dbusch.com/scanners/scanners.htm l7.Educational Resources， Universities 教育资源，大学Center for Image Processing in Educationwww.cipe.comLibrary of Congress Call Numbers Related to Imaging Science by Rochester Institute of Technologyhttp://wally2.rit.edu/pubs/guides/imagingcall.htmlMathematical Experiences through Image Processing， University of Washingtonwww.cs.washington.edu/research/metip/metip.htmlVismod Tech Reports and Publications， MIThttp://vismod.www.media.mit.edu/cgi-bin/tr_pagemakerVision Lab PhD dissertation list， University of Antwerphttp://wcc.ruca.ua.ac.be/~visielab/theses.htmlINRIA (France) Research Projects: Human-Computer Interaction， Image Processing， Data Management， Knowledge Systemswww.inria.fr/Themes/Theme3-eng.htmlImage Processing Resourceshttp://eleceng.ukc.ac.uk/~rls3/Contents.htmPublications of Carsten Stegerhttp://www9.informatik.tu-muench ... r/publications.html8.FAQs（常见问题）comp.dsp FAQwww.bdti.com/faq/dsp_faq.htmRobotics FAQwww.frc.ri.cmu.edu/robotics-faqWhere's the sci.image.processing FAQ?www.cc.iastate.edu/olc_answers/p ... processing.faq.htmlcomp.graphics.algorithms FAQ， Section 3， 2D Image/Pixel Computationswww.exaflop.org/docs/cgafaqAstronomical Image Processing System FAQwww.cv.nrao.edu/aips/aips_faq.html四、搜索资源http://sal.kachinatech.com/http://cheminfo.pku.edu.cn/mirrors/SAL/index.shtml 北京大学Google输入：computer vision 或computer vision groups可以获得很多结果网络资源：CVonline http://homepages.inf.ed.ac.uk/rbf/CVonline/视觉研究组列表Computer vision test Image http://www.cs.cmu.edu/~cil/v-images.html卡内基梅隆标准图片库视觉论文搜索：Paper searchhttp://www.researchindex.com五、图像处理GPL库（代码库图像库等）http://www.ph.tn.tudelft.nl/~klamer/cppima.htmlCppima 是一个图像处理的C++函数库。这里有一个较全面介绍它的库函数的文档，当然你也可以下载压缩的GZIP包，里面包含TexInfo格式的文档。http://iraf.noao.edu/Welcome to the IRAF Homepage! IRAF is the Image Reduction and Analysis Facility， a general purpose software system for the reduction and analysis of astronomical datahttp://entropy.brni-jhu.org/tnimage.html一个非常不错的Unix系统的图像处理工具，看看它的截图。你可以在此基础上构建自己的专用图像处理工具包。http://sourceforge.net/projects/这是GPL软件集散地，可以搜索IP库。"}
{"content2":"原文：http://blog.csdn.net/carson2005/article/details/6601109以下链接是本人整理的关于计算机视觉（ComputerVision, CV）相关领域的网站链接，其中有CV牛人的主页，CV研究小组的主页，CV领域的paper,代码，CV领域的最新动态，国内的应用情况等等。打算从事这个行业或者刚入门的朋友可以多关注这些网站，多了解一些CV的具体应用。搞研究的朋友也可以从中了解到很多牛人的研究动态、招生情况等。总之，我认为，知识只有分享才能产生更大的价值，真诚希望下面的链接能对朋友们有所帮助。（1）googleResearch； http://research.google.com/index.html（2）MIT博士，汤晓欧学生林达华；http://people.csail.mit.edu/dhlin/index.html（3）MIT博士后Douglas Lanman； http://web.media.mit.edu/~dlanman/（4）opencv中文网站；http://www.opencv.org.cn/index.php/%E9%A6%96%E9%A1%B5（5）Stanford大学vision实验室； http://vision.stanford.edu/research.html（6）Stanford大学博士崔靖宇； http://www.stanford.edu/~jycui/（7）UCLA教授朱松纯； http://www.stat.ucla.edu/~sczhu/（8）中国人工智能网； http://www.chinaai.org/（9）中国视觉网； http://www.china-vision.net/（10）中科院自动化所； http://www.ia.cas.cn/（11）中科院自动化所李子青研究员； http://www.cbsr.ia.ac.cn/users/szli/（12）中科院计算所山世光研究员； http://www.jdl.ac.cn/user/sgshan/（13）人脸识别主页； http://www.face-rec.org/（14）加州大学伯克利分校CV小组；http://www.eecs.berkeley.edu/Research/Projects/CS/vision/（15）南加州大学CV实验室； http://iris.usc.edu/USC-Computer-Vision.html（16）卡内基梅隆大学CV主页；http://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.html（17）微软CV研究员Richard Szeliski；http://research.microsoft.com/en-us/um/people/szeliski/（18）微软亚洲研究院计算机视觉研究组； http://research.microsoft.com/en-us/groups/vc/（19）微软剑桥研究院ML与CV研究组； http://research.microsoft.com/en-us/groups/mlp/default.aspx（20）研学论坛； http://bbs.matwav.com/（21）美国Rutgers大学助理教授刘青山；http://www.research.rutgers.edu/~qsliu/（22）计算机视觉最新资讯网； http://www.cvchina.info/（23）运动检测、阴影、跟踪的测试视频下载；http://apps.hi.baidu.com/share/detail/18903287（24）香港中文大学助理教授王晓刚； http://www.ee.cuhk.edu.hk/~xgwang/(25)香港中文大学多媒体实验室（汤晓鸥）; http://mmlab.ie.cuhk.edu.hk/(26)U.C. San Diego. computer vision;http://vision.ucsd.edu/content/home(27)CVonline; http://homepages.inf.ed.ac.uk/rbf/CVonline/(28)computer vision software; http://peipa.essex.ac.uk/info/software.html(29)Computer Vision Resource; http://www.cvpapers.com/(30)computer vision research groups;http://peipa.essex.ac.uk/info/groups.html(31)computer vision center; http://computervisioncentral.com/cvcnews(32)浙江大学图像技术研究与应用（ITRA）团队：http://www.dvzju.com/(33)自动识别网：http://www.autoid-china.com.cn/(34)清华大学章毓晋教授：http://www.tsinghua.edu.cn/publish/ee/4157/2010/20101217173552339241557/20101217173552339241557_.html(35)顶级民用机器人研究小组Porf.Gary领导的Willow Garage:http://www.willowgarage.com/(36)上海交通大学图像处理与模式识别研究所：http://www.pami.sjtu.edu.cn/(37)上海交通大学计算机视觉实验室刘允才教授：http://www.visionlab.sjtu.edu.cn/(38)德克萨斯州大学奥斯汀分校助理教授Kristen Grauman ：http://www.cs.utexas.edu/~grauman/ 图像分解，检索(39)清华大学电子工程系智能图文信息处理实验室（丁晓青教授）：http://ocrserv.ee.tsinghua.edu.cn/auto/index.asp(40)北京大学高文教授：http://www.jdl.ac.cn/htm-gaowen/(41)清华大学艾海舟教授：http://media.cs.tsinghua.edu.cn/cn/aihz(42)中科院生物识别与安全技术研究中心：http://www.cbsr.ia.ac.cn/china/index%20CH.asp(43)瑞士巴塞尔大学 Thomas Vetter教授：http://informatik.unibas.ch/personen/vetter_t.html(44)俄勒冈州立大学 Rob Hess博士：http://blogs.oregonstate.edu/hess/(45)深圳大学 于仕祺副教授：http://yushiqi.cn/(46)西安交通大学人工智能与机器人研究所：http://www.aiar.xjtu.edu.cn/(47)卡内基梅隆大学研究员Robert T. Collins:http://www.cs.cmu.edu/~rcollins/home.html#Background(48)MIT博士Chris Stauffer:http://people.csail.mit.edu/stauffer/Home/index.php(49)美国密歇根州立大学生物识别研究组(Anil K. Jain教授)：http://www.cse.msu.edu/rgroups/biometrics/(50)美国伊利诺伊州立大学Thomas S. Huang:http://www.beckman.illinois.edu/directory/t-huang1(51)武汉大学数字摄影测量与计算机视觉研究中心：http://www.whudpcv.cn/index.asp(52)瑞士巴塞尔大学Sami Romdhani助理研究员：http://informatik.unibas.ch/personen/romdhani_sami/(53)CMU大学研究员Yang Wang:http://www.cs.cmu.edu/~wangy/home.html(54)英国曼彻斯特大学Tim Cootes教授：http://personalpages.manchester.ac.uk/staff/timothy.f.cootes/(55)美国罗彻斯特大学教授Jiebo Luo:http://www.cs.rochester.edu/u/jluo/(56)美国普渡大学机器人视觉实验室：https://engineering.purdue.edu/RVL/Welcome.html(57)美国宾利州立大学感知、运动与认识实验室：http://vision.cse.psu.edu/home/home.shtml(58)美国宾夕法尼亚大学GRASP实验室：https://www.grasp.upenn.edu/(59)美国内达华大学里诺校区CV实验室：http://www.cse.unr.edu/CVL/index.php(60)美国密西根大学vision实验室：http://www.eecs.umich.edu/vision/index.html(61)University of Massachusetts(麻省大学),视觉实验室：http://vis-www.cs.umass.edu/index.html(62)华盛顿大学博士后Iva Kemelmacher:http://www.cs.washington.edu/homes/kemelmi(63)以色列魏茨曼科技大学Ronen Basri:http://www.wisdom.weizmann.ac.il/~ronen/index.html(64)瑞士ETH-Zurich大学CV实验室：http://www.vision.ee.ethz.ch/boostingTrackers/index.htm(65)微软CV研究员张正友：http://research.microsoft.com/en-us/um/people/zhang/(66)中科院自动化所医学影像研究室：http://www.3dmed.net/(67)中科院田捷研究员：http://www.3dmed.net/tian/(68)微软Redmond研究院研究员Simon Baker:http://research.microsoft.com/en-us/people/sbaker/(69)普林斯顿大学教授李凯：http://www.cs.princeton.edu/~li/(70)普林斯顿大学博士贾登：http://www.cs.princeton.edu/~jiadeng/(71)牛津大学教授Andrew Zisserman： http://www.robots.ox.ac.uk/~az/(72)英国leeds大学研究员Mark Everingham:http://www.comp.leeds.ac.uk/me/(73)英国爱丁堡大学教授Chris William: http://homepages.inf.ed.ac.uk/ckiw/(74)微软剑桥研究院研究员John Winn: http://johnwinn.org/(75)佐治亚理工学院教授Monson H.Hayes：http://savannah.gatech.edu/people/mhayes/index.html(76)微软亚洲研究院研究员孙剑：http://research.microsoft.com/en-us/people/jiansun/(77)微软亚洲研究院研究员马毅：http://research.microsoft.com/en-us/people/mayi/(78)英国哥伦比亚大学教授David Lowe: http://www.cs.ubc.ca/~lowe/(79)英国爱丁堡大学教授Bob Fisher: http://homepages.inf.ed.ac.uk/rbf/(80)加州大学圣地亚哥分校教授Serge J.Belongie:http://cseweb.ucsd.edu/~sjb/(81)威斯康星大学教授Charles R.Dyer: http://pages.cs.wisc.edu/~dyer/(82)多伦多大学教授Allan.Jepson: http://www.cs.toronto.edu/~jepson/(83)伦斯勒理工学院教授Qiang Ji: http://www.ecse.rpi.edu/~qji/(84)CMU研究员Daniel Huber: http://www.ri.cmu.edu/person.html?person_id=123(85)多伦多大学教授：David J.Fleet: http://www.cs.toronto.edu/~fleet/(86)伦敦大学玛丽女王学院教授Andrea Cavallaro:http://www.eecs.qmul.ac.uk/~andrea/(87)多伦多大学教授Kyros Kutulakos: http://www.cs.toronto.edu/~kyros/(88)杜克大学教授Carlo Tomasi: http://www.cs.duke.edu/~tomasi/(89)CMU教授Martial Hebert: http://www.cs.cmu.edu/~hebert/(90)MIT助理教授Antonio Torralba: http://web.mit.edu/torralba/www/(91)马里兰大学研究员Yasel Yacoob: http://www.umiacs.umd.edu/users/yaser/(92)康奈尔大学教授Ramin Zabih: http://www.cs.cornell.edu/~rdz/(93)CMU博士田渊栋: http://www.cs.cmu.edu/~yuandong/(94)CMU副教授Srinivasa Narasimhan: http://www.cs.cmu.edu/~srinivas/(95)CMU大学ILIM实验室：http://www.cs.cmu.edu/~ILIM/(96)哥伦比亚大学教授Sheer K.Nayar: http://www.cs.columbia.edu/~nayar/(97)三菱电子研究院研究员Fatih Porikli ：http://www.porikli.com/(98)康奈尔大学教授Daniel Huttenlocher：http://www.cs.cornell.edu/~dph/(99)南京大学教授周志华：http://cs.nju.edu.cn/zhouzh/index.htm(100)芝加哥丰田技术研究所助理教授Devi Parikh: http://ttic.uchicago.edu/~dparikh/index.html(101)瑞士联邦理工学院博士后Helmut Grabner:http://www.vision.ee.ethz.ch/~hegrabne/#Short_CV(102)香港中文大学教授贾佳亚：http://www.cse.cuhk.edu.hk/~leojia/index.html(103)南京大学教授吴建鑫：http://c2inet.sce.ntu.edu.sg/Jianxin/index.html(104)GE研究院研究员李关：http://www.cs.unc.edu/~lguan/(105)佐治亚理工学院教授Monson Hayes:http://savannah.gatech.edu/people/mhayes/(106)图片检索国际竞赛PASCAL VOC(微软剑桥研究院组织):http://pascallin.ecs.soton.ac.uk/challenges/VOC/(107)机器视觉开源处理库汇总：http://archive.cnblogs.com/a/2217609/(108)布朗大学教授Benjamin Kimia: http://www.lems.brown.edu/kimia.html(109)数据堂-图像处理相关的样本数据：http://www.datatang.com/data/list/602026/p1(110)东软基于CV的汽车辅助驾驶系统：http://www.neusoft.com/cn/solutions/1047/(111)马里兰大学教授Rema Chellappa:http://www.cfar.umd.edu/~rama/(112)芝加哥丰田研究中心助理教授Devi Parikh：http://ttic.uchicago.edu/~dparikh/index.html(113)宾夕法尼亚大学助理教授石建波：http://www.cis.upenn.edu/~jshi/(114)比利时鲁汶大学教授Luc Van Gool：http://www.vision.ee.ethz.ch/members/get_member.cgi?id=1, http://www.vision.ee.ethz.ch/~vangool/(115)行人检测主页：http://www.pedestrian-detection.com/(116)法国学习算法与系统实验室Basilio Noris博士：http://lasa.epfl.ch/people/member.php?SCIPER=129576 http://mldemos.epfl.ch/(117)美国马里兰大学LARRY S.DAVIS教授：http://www.umiacs.umd.edu/~lsd/(118)计算机视觉论文分类导航：http://www.visionbib.com/bibliography/contents.html(119)计算机视觉分类信息导航：http://www.visionbib.com/(120)西班牙马德里理工大学博士Marcos Nieto：http://marcosnieto.net/(121)香港理工大学副教授张磊：http://www4.comp.polyu.edu.hk/~cslzhang/(122)以色列技术学院教授Michael Elad：http://www.cs.technion.ac.il/~elad/(123)韩国启明大学计算机视觉与模式识别实验室：http://cvpr.kmu.ac.kr/(124)英国诺丁汉大学Michel Valstar博士：http://www.cs.nott.ac.uk/~mfv/(125)卡内基梅隆大学Takeo Kanade教授:http://www.ri.cmu.edu/people/kanade_takeo.html(126)微软学术搜索：http://libra.msra.cn/(127)比利时天主教鲁汶大学Radu Timofte博士：http://homes.esat.kuleuven.be/~rtimofte/，交通标志检测，定位，3D跟踪(128)迪斯尼匹兹堡研究院研究员：Iain Matthews:http://www.iainm.com/iainm/Home.htmlhttp://www.ri.cmu.edu/person.html?type=publications&person_id=741 AAM,三维重建（129）康奈尔大学视觉与图像分析组：http://www.via.cornell.edu/ 医学图像处理（130）密西根州立大学生物识别研究组：http://www.cse.msu.edu/biometrics/ 人脸识别、指纹识别、图像检索（131）柏林科技大学计算机视觉与遥感实验室：http://www.cv.tu-berlin.de/menue/computer_vision_remote_sensing/parameter/en/ 图像分析、物体重建、基于图像的表面测量、医学图像处理（132）英国布里斯托大学数字多媒体研究组：http://www.cs.bris.ac.uk/Research/Digitalmedia/ 运动检测与跟踪、视频压缩、3D重建、字符定位（133）英国萨利大学视觉、语音与信号处理中心： http://www.surrey.ac.uk/cvssp/   人脸识别、监控、3D、视频检索、（134）北卡莱罗纳大学教堂山分校Marc Pollefeys教授：http://www.cs.unc.edu/~marc/ 基于视频的3D模型生成、相机标定、运动检测与分析、3D重建（135）澳大利亚国立大学Richard Hartley教授：http://users.cecs.anu.edu.au/~hartley/ 运动估计、稀疏子空间、跟踪、（136）百度技术副总监于凯：http://www.dbs.ifi.lmu.de/~yu_k/ 深度学习，稀疏表示，图像分类（137）西安电子科技大学高新波教授：http://web.xidian.edu.cn/xbgao/index.html 质量评判、水印、稀疏表示、超分辨率（138）加州大学伯克利分校Michael I.Jordan教授：http://www.cs.berkeley.edu/~jordan/ 机器学习（139）加州理工行人检测相关资料：http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/（140）微软Redmond研究院研究员Piotr Dollar: http://vision.ucsd.edu/~pdollar/ 行人检测、特征提取、（141）视觉计算研究论坛：http://www.sigvc.org/bbs/ 中科院视觉计算研究小组的论坛（142）美国坦桑尼亚州立大学稀疏学习软件包：http://www.public.asu.edu/~jye02/Software/SLEP/index.htm 稀疏学习（143）美国加州大学圣地亚哥分校Jacob Whitehill博士：http://mplab.ucsd.edu/~jake/ 机器学习（144）美国布朗大学Michael J.Black教授：http://cs.brown.edu/~black/  人的姿态估计和跟踪（145）美国加州大学圣地亚哥分校David Kriegman教授：http://cseweb.ucsd.edu/~kriegman/ 人脸识别（146）南加州大学Paul Debevec教授：http://ict.debevec.org/~debevec/ 或 http://www.pauldebevec.com/ 将CV和CG结合研究 人脸捕捉重建技术（147）伊利诺伊大学D.A.Forsyth教授：http://luthuli.cs.uiuc.edu/~daf/ 三维重建（148）英国牛津大学Ian Reid教授：http://www.robots.ox.ac.uk/~ian/ 跟踪和机器人导航（149）CMU大学Alyosha Efros 教授: https://www.cs.cmu.edu/~efros/ 图像纹理合成（150）加州大学伯克利分校Jitendra Malik教授：http://www.cs.berkeley.edu/~malik/ 轮廓检测、图像/视频分割、图形匹配、目标识别（151）MIT教授William Freeman： http://people.csail.mit.edu/billf/ 图像纹理合成（152）CMU博士Henry Schneiderman： http://www.cs.cmu.edu/~hws/ 目标检测和识别；（153）微软研究员Paul Viola: http://research.microsoft.com/en-us/um/people/viola/ AdaBoost算法（154）微软研究员Antonio Criminisi: http://research.microsoft.com/en-us/people/antcrim/ 图像修补，三维重建，目标检测与跟踪；（155）魏茨曼科学研究所教授Michal Irani: http://www.wisdom.weizmann.ac.il/~irani/ 超分辨率（156）瑞士洛桑理工学院Pascal Fua教授：http://people.epfl.ch/pascal.fua/bio?lang=en 立体视觉，增强现实（157）佐治亚理工学院Irfan Essa教授：http://www.ic.gatech.edu/people/irfan-essa 人脸表情识别（158）中科院助理教授樊彬：http://www.sigvc.org/bfan/ 特征描述；（159）斯坦福大学Sebastian Thrun教授：http://robots.stanford.edu/index.html 机器人；（160）多伦多大学Geoffrey E.Hinton教授：http://www.cs.toronto.edu/~hinton/ 深度学习（161）凤巢系统架构师张栋博士：http://weibo.com/machinelearning（162）2012年龙星计划机器学习课程：http://bigeye.au.tsinghua.edu.cn/DragonStar2012/index.html（163）中科院自动化所肖柏华教授：http://www.compsys.ia.ac.cn/people/xiaobaihua.html 文字识别、人脸识别、质量评判（164）图像视频质量评判：http://live.ece.utexas.edu/research/quality/（165）纽约大学Yann LeCun教授http://yann.lecun.com/   http://yann.lecun.com/exdb/mnist/  手写体数字识别（166）二维条码识别开源库zxing：http://code.google.com/p/zxing/（167）布朗大学Pedro Felzenszwalb教授：http://cs.brown.edu/~pff/ 特征提取，Deformable Part Model（168）伊利诺伊香槟大学Svetlana Lazebnik教授：http://www.cs.illinois.edu/homes/slazebni/ 特征提取，聚类，图像检索（169）荷兰乌德勒支大学图像与多媒体研究中心http://www.cs.uu.nl/centers/give/multimedia/index.html 图像、多媒体检索与匹配（170）英国格拉斯哥大学信息检索小组：http://ir.dcs.gla.ac.uk/ 文本、图像、视频检索（171）中科院自动化所孙哲南助理教书：http://www.cbsr.ia.ac.cn/users/znsun/ 虹膜识别、掌纹识别、人脸识别（172）南京信息工程大学刘青山教授：http://www.jstuoke.com/web/xky/detail.asp?NewsID=1096 人脸图像分析、医学图像分析（173）清华大学助理教授冯建江：http://ivg.au.tsinghua.edu.cn/~jfeng/ 指纹识别（174）北航助理教授黄迪：http://irip.buaa.edu.cn/~dihuang/ 3D人脸识别（175）中山大学助理教授郑伟诗：http://sist.sysu.edu.cn/~zhwshi/ 人脸识别、特征匹配、聚类、检索；（176）google瑞士苏黎世的工程师Thomas Deselaers: http://thomas.deselaers.de/index.html 图像检索（177）百度深度学习研究中心博士后余轶南：http://www.cbsr.ia.ac.cn/users/ynyu/index.htm 目标检测，图像检索（178）威兹曼科技大学超分辨率：http://www.wisdom.weizmann.ac.il/~vision/SingleImageSR.html（179）德克萨斯大学奥斯汀分校Al Bovik教授：http://live.ece.utexas.edu/people/bovik/ 图像视频质量判别、特征提取（180）以色列希伯来大学Yair Weiss教授：http://www.cs.huji.ac.il/~yweiss/ 机器学习、超分辨率（181）以色列希伯来大学Daniel Zoran博士：http://www.cs.huji.ac.il/~daniez/ 超分辨率、去噪（182）美国加州大学Peyman Milanfar教授：http://users.soe.ucsc.edu/~milanfar/ 去噪（183）中科院计算所副研究员常虹：http://www.jdl.ac.cn/user/hchang/index.html 图像检索、半监督学习、超分辨率（184）以色列威茨曼大学Anat Levin教授：http://www.wisdom.weizmann.ac.il/~levina/ 去噪、去模糊（185）以色列威茨曼大学Daniel Glasner博士后：http://www.wisdom.weizmann.ac.il/~glasner/ 超分辨率、分割、姿态估计（186）密西根大学助理教授Honglak Lee: http://web.eecs.umich.edu/~honglak/ 机器学习、特征提取，去噪、稀疏表示；（187）MIT周博磊博士：http://people.csail.mit.edu/bzhou/ 聚集分析、运动检测（188）美国田纳西大学Li He博士：http://web.eecs.utk.edu/~lhe4/ 稀疏表示、超分辨率；（189）Adobe研究院Jianchao Yang研究员：http://www.ifp.illinois.edu/~jyang29/ 稀疏表示，超分辨率、图片检索、去噪、去模糊（190）Deep Learning主页：http://deeplearning.net/ 深度学习论文、软件，代码，demo，数据等；（191）斯坦福大学Andrew Ng教授：http://cs.stanford.edu/people/ang/ 深度神经网络，深度学习（192）Elefant: http://elefant.developer.nicta.com.au/ 机器学习开源库（193）微软研究员Ce Liu: http://people.csail.mit.edu/celiu/ 去噪、超分辨率、去模糊、分割（194）West Virginia大学助理教授Xin Li: http://www.csee.wvu.edu/~xinl/ 边缘检测、降噪、去模糊（195）http://www.csee.wvu.edu/~xinl/source.html 深度学习、去噪、编码、压缩感知、超分辨率、聚类、分割等相关代码集合（196）西班牙格拉纳达大学超分辨率重建项目组：http://decsai.ugr.es/pi/superresolution/index.html（197）清华大学程明明博士：http://mmcheng.net/ 图像分割、检索（198）牛津布鲁克斯大学Philip H.S.Torr教授：http://cms.brookes.ac.uk/staff/PhilipTorr/ 分割、三维重建（199）佐治亚理工学院James M.Rehg教授：http://www.cc.gatech.edu/~rehg/ 分割、行人检测、特征描述、（200）大规模图像分类、检测竞赛ILSVRC（Stanford, Google举办）:http://www.image-net.org/challenges/LSVRC/2013/（201）加州大学尔湾分校Deva Ramanan助理教授：http://www.ics.uci.edu/~dramanan/ 目标检测，行人检测，跟踪、稀疏表示（202）人脸识别测试图片集：http://www.mlcv.net/（203）美国西北大学博士Ming Yang: http://www.ece.northwestern.edu/~mya671/ 人脸识别、图像检索；（204）美国加州大学伯克利分校博士后Ross B.Girshick：http://www.cs.berkeley.edu/~rbg/ 目标检测（DPM）（205）中文语言资源联盟：http://www.chineseldc.org/index.html  内有很多语言识别、字符识别的训练，测试库；（206）西班牙巴塞罗那大学计算机视觉中心：http://www.cvc.uab.es/adas/site/ 检测、跟踪、3D、行人检测、汽车辅助驾驶（207）德国戴姆勒研究所Prof. Dr. Dariu M. Gavrila：http://www.gavrila.net/index.html 跟踪、行人检测、（208）苏黎世联邦理工学院Andreas Ess博士后：http://www.vision.ee.ethz.ch/~aess/ 行人检测、行为检测、跟踪（209）Libqrencode: http://fukuchi.org/works/qrencode/ 基于C语言的QR二维码编码开源库（210）江西财经大学袁飞牛教授：http://sit.jxufe.cn/grbk/yfn/index.html#  烟雾检测、3D重建、医学图像处理（211）斯坦福的抽象编程：http://v.163.com/movie/2008/7/B/O/M6SIM7VT5_M6SIR3IBO.html"}
{"content2":"更多精彩内容请关注微信公众号：听潮庭。计算机视觉的深度学习实战四：图像特征提取综述：颜色特征量化颜色直方图、聚类颜色直方图几何特征Edge，Corner，Blob基于关键点的特征描述子SIFT、SURF、ORB其他特征提取：（LBP、Gabor）代码实践一、颜色特征1、量化颜色直方图适用颜色空间：RGB、HSV等颜色空间操作颜色空间量化，单元（bin）由单元中心代表统计落在量化单元上的像素数量最常用的方法是将颜色空间的各个分量（维度）均匀地进行划分。HSV空间优势：计算高效劣势：量化问题、稀疏2、聚类颜色直方图适用颜色空间：Lab等颜色空间操作：使用聚类算法对所有像素点颜色空间进行聚类单元（bin）由聚类中心代表聚类算法则考虑到图像颜色特征在整个空间的分布情况，避免出现大量的bin中的像素数量非常稀疏的情况；lab空间是用数字化的方法来描述人的视觉感应。Lab颜色空间中：L分量用于表示像素的亮度，取值范围是[1,100],表示从纯黑到纯白；a表示从品红色到深绿色的范围，取值范围是[127,-128];b表示从黄色到蓝色的范围，取值范围是[127,-128].3、设想两幅图像的颜色直方图几乎相同，只是互相错开了一个bin，这时如果采用L1距离或者欧拉距离计算两者的相似度，会得到很小的相似度值。为了克服这个缺陷，需要考虑到相似但不相同的颜色之间的相似度：一种方法是采用二次式距离；另一种方法是对颜色直方图事先进行平滑过滤，即每个bin中的像素对于相邻的几个bin也有贡献。二、几何特征1、边缘（Edge）像素明显变化的区域具有丰富的语义信息用于物体识别几何、视角变换边缘定义：像素值函数快速变化的区域->一阶导数的极值区域边缘提取：先高斯（高斯平滑）去噪，再用一阶导数获取极值导数对噪声敏感高斯滤波yijiedao标准差->尺度梯度幅值/强度梯度（增加最快）方向边缘提取尺度问题：不同标准差的滤波（x方向）能捕捉到不同尺度的边缘三、基于特征点的特征描述子从不同的距离，不同的方向、角度，不同的光照条件下观察一个物体时，物体的大小，形状，敏感都会有所不同。但我们依然可以判断它是同一物体。理想的特征描述子应该具备这些性质。即，在大小、方向、明暗不同的图像中，同一特征点应具有足够相似的描述子，称之为描述子的可复现性。1、几何特征：特征点/关键点不同视角图片之间的映射稳定局部特征点可重复性、显著性抗图片变换：外貌变换（亮度，光照）；几何变换（平移、选择、尺度）兴趣点/关键点（Interest point/Key point）图片配准/拼接运动跟踪物体识别机器人导航3D重建2、几何特征：Harris角点Harris角点（Corner）一种显著点在任何方向上移动小观察窗，导致大的像素变动数学模型：偏移（u,v）后窗内图像变化取 E(u,v)大的patch，其中w(x,y)相当于权重判断Harris角点：图像中直线：一个特征值大，另一个特征值小；图像中平面：两个特征值都小，且近似相等；图像中角点：两个特征值都大，且近似相等；3、FAST角点检测FAST角点检测是一种快速角点特征检测算法。FAST角点定义为：若某像素点与其周围领域内足够多的像素点处于不同的区域，则该像素点可能为角点，也就是某些属性与众不同。FAST特征点检测是对兴趣点所在圆周上的16个像素点进行判断，若判断后的当前中心像素点为暗或亮，将决定其是否为角点。确定一个阈值t，观察某像素点为中心的一个半径等于3像素的离散化的圆，这个圆的边界上有16个像素。如果在这个大小为16个像素的圆上有N（12）个连续的像素点，他们的像素值要么都比I_p+tIp +t大，要么都比I_p-tIp −t小，则p他就是一个角点。4、几何特征：斑点拉普拉斯梯度一阶导极值点→二阶导数零点梯度/边缘可以通过寻找二阶导数接近零但对噪声很敏感，首先对图像进行高斯卷积滤波进行降噪出路，再采用Laplace算子进行边缘检测。高斯拉普拉斯滤波/Laplacian of Ganssian（LoG）当sigma较小时，将识别出更为细节的边缘。斑点（Blob）LoG图找极值点→斑点局部特征：SIFT（尺度不变特征变换）Scale-invariant feature transform基于尺度空间不变的特征SIFT特征计算步骤在DoG尺度空间中获取极值点，即关键点LoG尺度空间和DoG（差分高斯）尺度空间对关键点处理位置插值（获得精确的关键点）去除边缘点关键点的方向估计关键点描述子的生成区域坐标旋转计算采样区域的直方图特点：具有良好的不变性旋转、尺度缩放、平移、亮度变化、对视角变化、仿射变换和噪声也有一定程度的稳定性独特性好，信息量丰富适用于在海量特征数据库中进行快速、准确的匹配多量性即使少数物体也可以产生大量的SIFT特征计算快经优化的SIFT匹配算法甚至可以达到实时性可扩展性，可以很方便的与其他形式的特征向量进行联合。尺度空间：使用不同σ的LoG对图片进行滤波使用LoG，则后续计算量大，故使用DoG来代替LoG，用差分代替微分。高斯金字塔就是在传统金字塔的基础上，对每一层用不同的参数σ做高斯模糊，是的每一层金字塔有多张高斯模糊图像，这样一组图像是一个octave。SIFT-计算高斯差分(DoG)空间SIFt-DoG空间极值点就是“关键点”圆半径→特征点尺度圆心→特征点坐标通过拟合三维二次函数来精确确定关键点的位置和尺度离散空间的极值点并不是真正的极值点，上图显示了二维函数离散空间得到的极值点与连续空间极值点的差别。利用已知的离散空间点插值得到的连续空间极值点的方法叫做子像素插值。同时去除低对比度的关键点和不稳定的边缘相应点（因为DoG算子会产生较强的边缘响应），以增强匹配稳定性、提高抗噪声能力。DoG算子会产生较强的边缘响应，需要剔除不稳定的边缘相应点。获取特征点处的Hessian矩阵，主曲率通过一个2×2的Hessian矩阵H求出SIFT-特征点方向估计在尺度上计算梯度直方图8方向以特征点为中心、以3×1.5σ为半径活取最高值方向为关键点主方向为了匹配的稳定性，将超过最高值80%的方向称为辅方向为了保证特征矢量具有旋转不变性，需要以特征点为中心，将特征点附近邻域内的图像旋转一个方向角θ即将原图像x轴转到与主方向相同的方向。SIFT-计算特征点描述子在旋转后的坐标上采样16×16的像素窗在旋转后的坐标上采样16×16的像素床4×4网格8方向直方图总共128维SIFT的缺点是：计算太复杂，如果不借助硬件加速或专门的图像处理器很难实现。改进方式：Haar-like特征：Haar-like特征分为：边缘检测、线性特征、中心特征和对角线特征，这些特征组合成特征模板特征模板有白色和黑色两种矩形，并定义该模板的特征值为白色矩形像素和减去黑色矩形像素和。Haar特征值反映了图像的灰度变化情况。Haar-like特征的快速计算：积分图同一个像素如果被包含在不同的Haar-like的特征模板中，会被重复计算多次；积分图是根据四个角点就能计算区域内的像素之和的方法。5、局部特征：SURFSURF（Speed-Up Robust Features）算子是Herbert Bay等人在2006年提出的，它是对SIFT的改进，可将速度提高三倍。SURF只要是把SIFT中的某些运算做了简化。SURF把SIFT中的高斯二阶微分的模板进行了简化，使得卷积平滑操作仅需要转换成加减运算。在方向确定阶段，在圆形区域计算x,y方向的haar小波响应，找到模最大的扇形方向。为了找出图像中的特征点，需要对原图进行变换，变换图就是原图每个像素的Hessian矩阵行列式的近似值构成的。求Hessian时要先高斯平滑，然后求二阶导数，这对于离散的像素点而言，是用模板卷积形成的，这两种操作合在一起用一个Haar模板代替就可以了。小造型为了保证旋转不变性，在SURF中，统计特征点领域内的Haar小波特征。即以特征点为中心，计算半径为6s（s为特征点所在的尺度值）的邻域内，统计60度扇形内所有点在x（水平）和y（垂直）方向的Haar小波响应总和。然后60度扇形以一定间隔进行旋转，最后将最大值那个扇形的方向作为该特征点的主方向。在特征点周围取一个正方形框，框的边长为20s（是所检测到该特征点所在的尺度）。该框的方向，就是检测出来的主方向。最终，SURF的特征点特征向量的维度为64维。然后把该框分为16个子区域，每个子区域统计25个像素的水平方向和垂直方向的Haar小波特征，这里的水平和垂直方向都是相对主方向而言的。近似SIFT算法，实现快速版先确定后选点，再找最大值Haar模板加速三倍亮度效果下效果好模糊方面优于SIFT尺度不变上不及SIFT旋转不变上差很多6、ORB特征描述SIFT和SURF计算复杂，难以用于实时性特征检测，更何况SIFT与SURF以前还是收费的ORB特征基于FAST角点的特征点检测与BRIEF特征描述技术ORB的基本思路它是对FAST角点与BRIEF特征描述子的一种结合与改进FAST角点检测的缺点是：缺乏尺度不变性；可以通告构建高斯金字塔，然后在每一层金字塔图像上检测角点，来实现尺度不变性；BRIEF的缺点是缺乏旋转不变性；需要给BRIEF加上旋转不变性7、BRIEFBRIEF需要先平滑图片，然后在特征点周围选择一个Patch，在这个Patch内通过一种选定的方法来挑选Nd个点对。比较点对中两点像素的大小，进行如下赋值所有Nd个点对，都进行比较之间，我们就生成了一个Nd长的二进制串。点对的生成方式（共有五种）1、X，Y都服从在[-s/2,s/2]范围内的均匀分布，且相互独立2、X，Y都服从均值为0，方差为S²/25d的高斯分布，且相互独立，即X和Y都已原点为中心，进行同方差的高斯分布；点对的位置一旦随机选定，就不能再更改ORB对BRIEF的改进ORB在计算BRIEF描述子时建立的坐标系是以关键点为圆心，以关键点和取点区域的形心（圆形）的连线为X轴建立坐标系计算形心时，圆形区域上每个点的质量是其对应的像素值四、LBPLBP(局部二值模式)将每个像素点与周围点大小比较半径为R的圆上，均匀采样P个点大小量化为0或1多个bit组成一个数，统计每个数的直方图为解决旋转不变性的问题：将LBP周围的二进制码（如11110001）按位旋转，取二进制码最小的值为最终LBP值。如：对于11110001情况，我们按位旋转，得到11100011，11000111，10001111，0001111，00111110，01111100，11111000七个不同的二进制数，最小值为00011111.改进的LBP:将3×3邻域扩展到任意邻域，并用圆形邻域代替了正方向邻域，这种LBP特征叫做Extended LBP，也叫Circular LBP。LBP特征具有灰度不变性和旋转不变性等显著优点。五、GaborGabor是一个用于边缘提取的线性滤波器，其频率和方向表达与人类视觉系统类似，能够提供良好的方向选择和尺度选择特性，而且对于光照变化不敏感；十分适合纹理分析使用一个三角函数与一个高斯函数叠加就得到了一个Gabor滤波器Gabor滤波器组Gabor滤波器组类似于人类的视觉系统多频率/尺度多方向Gabor滤波器频域：属于加窗傅里叶变换空域：一个高斯核函数和正弦平面波的乘积三尺度八方向六、代码实现1、Haaris Corner1 import numpy as np 2 import cv2 as cv 3 filename = \"picture/chessboard.png\" 4 img = cv.imread(filename) 5 gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY) 6 gray = np.float32(gray) 7 dst = cv.cornerHarris(gray,2,3,0.04) 8 #result is dilated for marking the corners, not important 9 dst = cv.dilate(dst,None) 10 # Threshold for an optimal value, it may vary depending on the image. 11 img[dst>0.01*dst.max()]=[0,0,255] 12 cv.imshow('dst',img) 13 if cv.waitKey(0) & 0xff == 27: 14 cv.destroyAllWindows()2、ORB1 import numpy as np 2 import cv2 as cv 3 import matplotlib.pyplot as plt 4 img1 = cv.imread('picture/box.png',0) # queryImage 5 img2 = cv.imread('picture/box_in_scene.png',0) # trainImage 6 # Initiate ORB detector 7 orb = cv.ORB_create() 8 # find the keypoints and descriptors with ORB 9 kp1, des1 = orb.detectAndCompute(img1,None) 10 kp2, des2 = orb.detectAndCompute(img2,None) 11 12 # create BFMatcher object 13 bf = cv.BFMatcher(cv.NORM_HAMMING, crossCheck=True) 14 # Match descriptors. 15 matches = bf.match(des1,des2) 16 # Sort them in the order of their distance. 17 matches = sorted(matches, key = lambda x:x.distance) 18 # Draw first 10 matches. 19 img3 = cv.drawMatches(img1,kp1,img2,kp2,matches[:20],None, flags=2) 20 plt.imshow(img3),plt.show()3、图片拼接(注意原始图片不能过大，否则报错，此处500*375）1 from Stitcher import Stitcher 2 import cv2 3 4 # 读取拼接图片 5 imageA = cv2.imread(\"image/3.png\") 6 imageB = cv2.imread(\"image/4.png\") 7 8 # 把图片拼接成全景图 9 stitcher = Stitcher() 10 (result, vis) = stitcher.stitch([imageA, imageB], showMatches=True) 11 12 # 显示所有图片 13 cv2.imshow(\"Image A\", imageA) 14 cv2.imshow(\"Image B\", imageB) 15 cv2.imshow(\"Keypoint Matches\", vis) 16 cv2.imshow(\"Result\", result) 17 cv2.waitKey(0) 18 cv2.destroyAllWindows()1 import numpy as np 2 import cv2 3 4 class Stitcher: 5 6 #拼接函数 7 def stitch(self, images, ratio=0.75, reprojThresh=4.0,showMatches=False): 8 #获取输入图片 9 (imageB, imageA) = images 10 #检测A、B图片的SIFT关键特征点，并计算特征描述子 11 (kpsA, featuresA) = self.detectAndDescribe(imageA) 12 (kpsB, featuresB) = self.detectAndDescribe(imageB) 13 14 # 匹配两张图片的所有特征点，返回匹配结果 15 M = self.matchKeypoints(kpsA, kpsB, featuresA, featuresB, ratio, reprojThresh) 16 17 # 如果返回结果为空，没有匹配成功的特征点，退出算法 18 if M is None: 19 return None 20 21 # 否则，提取匹配结果 22 # H是3x3视角变换矩阵 23 (matches, H, status) = M 24 # 将图片A进行视角变换，result是变换后图片 25 result = cv2.warpPerspective(imageA, H, (imageA.shape[1] + imageB.shape[1], imageA.shape[0])) 26 # 将图片B传入result图片最左端 27 result[0:imageB.shape[0], 0:imageB.shape[1]] = imageB 28 29 # 检测是否需要显示图片匹配 30 if showMatches: 31 # 生成匹配图片 32 vis = self.drawMatches(imageA, imageB, kpsA, kpsB, matches, status) 33 # 返回结果 34 return (result, vis) 35 36 # 返回匹配结果 37 return result 38 39 def detectAndDescribe(self, image): 40 # 将彩色图片转换成灰度图 41 gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) 42 43 # 建立SIFT生成器 44 descriptor = cv2.xfeatures2d.SIFT_create() 45 # 检测SIFT特征点，并计算描述子 46 (kps, features) = descriptor.detectAndCompute(image, None) 47 48 # 将结果转换成NumPy数组 49 kps = np.float32([kp.pt for kp in kps]) 50 51 # 返回特征点集，及对应的描述特征 52 return (kps, features) 53 54 def matchKeypoints(self, kpsA, kpsB, featuresA, featuresB, ratio, reprojThresh): 55 # 建立暴力匹配器 56 matcher = cv2.DescriptorMatcher_create(\"BruteForce\") 57 58 # 使用KNN检测来自A、B图的SIFT特征匹配对，K=2 59 rawMatches = matcher.knnMatch(featuresA, featuresB, 2) 60 61 matches = [] 62 for m in rawMatches: 63 # 当最近距离跟次近距离的比值小于ratio值时，保留此匹配对 64 if len(m) == 2 and m[0].distance < m[1].distance * ratio: 65 # 存储两个点在featuresA, featuresB中的索引值 66 matches.append((m[0].trainIdx, m[0].queryIdx)) 67 68 # 当筛选后的匹配对大于4时，计算视角变换矩阵 69 if len(matches) > 4: 70 # 获取匹配对的点坐标 71 ptsA = np.float32([kpsA[i] for (_, i) in matches]) 72 ptsB = np.float32([kpsB[i] for (i, _) in matches]) 73 74 # 计算视角变换矩阵 75 (H, status) = cv2.findHomography(ptsA, ptsB, cv2.RANSAC, reprojThresh) 76 77 # 返回结果 78 return (matches, H, status) 79 80 # 如果匹配对小于4时，返回None 81 return None 82 83 def drawMatches(self, imageA, imageB, kpsA, kpsB, matches, status): 84 # 初始化可视化图片，将A、B图左右连接到一起 85 (hA, wA) = imageA.shape[:2] 86 (hB, wB) = imageB.shape[:2] 87 vis = np.zeros((max(hA, hB), wA + wB, 3), dtype=\"uint8\") 88 vis[0:hA, 0:wA] = imageA 89 vis[0:hB, wA:] = imageB 90 91 # 联合遍历，画出匹配对 92 for ((trainIdx, queryIdx), s) in zip(matches, status): 93 # 当点对匹配成功时，画到可视化图上 94 if s == 1: 95 # 画出匹配对 96 ptA = (int(kpsA[queryIdx][0]), int(kpsA[queryIdx][1])) 97 ptB = (int(kpsB[trainIdx][0]) + wA, int(kpsB[trainIdx][1])) 98 cv2.line(vis, ptA, ptB, (0, 255, 0), 1) 99 100 # 返回可视化结果 101 return vis"}
{"content2":"不多说，直接上干货！一、概述Artificial Intelligence，也就是人工智能，就像长生不老和星际漫游一样，是人类最美好的梦想之一。虽然计算机技术已经取得了长足的进步，但是到目前为止，还没有一台电脑能产生“自我”的意识。是的，在人类和大量现成数据的帮助下，电脑可以表现的十分强大，但是离开了这两者，它甚至都不能分辨一个喵星人和一个汪星人。图灵（图灵，大家都知道吧。计算机和人工智能的鼻祖，分别对应于其著名的“图灵机”和“图灵测试”）在 1950 年的论文里，提出图灵试验的设想，即，隔墙对话，你将不知道与你谈话的，是人还是电脑。这无疑给计算机，尤其是人工智能，预设了一个很高的期望值。但是半个世纪过去了，人工智能的进展，远远没有达到图灵试验的标准。这不仅让多年翘首以待的人们，心灰意冷，认为人工智能是忽悠，相关领域是“伪科学”。但是自 2006 年以来，机器学习领域，取得了突破性的进展。图灵试验，至少不是那么可望而不可及了。至于技术手段，不仅仅依赖于云计算对大数据的并行处理能力，而且依赖于算法。这个算法就是，Deep Learning。借助于 Deep Learning 算法，人类终于找到了如何处理“抽象概念”这个亘古难题的方法。2012年6月，《约时报》披露了Google Brain项目，吸引了公众的广泛关注。这个项目是由著名的斯坦福大学的机器学习教授Andrew Ng和在大规模计算机系统方面的世界顶尖专家JeffDean共同主导，用16000个CPU Core的并行计算平台训练一种称为“深度神经网络”（DNN，Deep Neural Networks）的机器学习模型（内部共有10亿个节点。这一网络自然是不能跟人类的神经网络相提并论的。要知道，人脑中可是有150多亿个神经元，互相连接的节点也就是突触数更是如银河沙数。曾经有人估算过，如果将一个人的大脑中所有神经细胞的轴突和树突依次连接起来，并拉成一根直线，可从地球连到月亮，再从月亮返回地球），在语音识别和图像识别等领域获得了巨大的成功。项目负责人之一Andrew称：“我们没有像通常做的那样自己框定边界，而是直接把海量数据投放到算法中，让数据自己说话，系统会自动从数据中学习。”另外一名负责人Jeff则说：“我们在训练的时候从来不会告诉机器说：‘这是一只猫。’系统其实是自己发明或者领悟了“猫”的概念。”2012年11月，微软在中国天津的一次活动上公开演示了一个全自动的同声传译系统，讲演者用英文演讲，后台的计算机一气呵成自动完成语音识别、英中机器翻译和中文语音合成，效果非常流畅。据报道，后面支撑的关键技术也是DNN，或者深度学习（DL，DeepLearning）。2013年1月，在百度年会上，创始人兼CEO李彦宏高调宣布要成立百度研究院，其中第一个成立的就是“深度学习研究所”（IDL，Institue of Deep Learning）。为什么拥有大数据的互联网公司争相投入大量资源研发深度学习技术。听起来感觉deeplearning很牛那样。那什么是deep learning？为什么有deep learning？它是怎么来的？又能干什么呢？目前存在哪些困难呢？这些问题的简答都需要慢慢来。咱们先来了解下机器学习（人工智能的核心）的背景。二、背景机器学习（Machine Learning）是一门专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能的学科。机器能否像人类一样能具有学习能力呢？1959年美国的塞缪尔(Samuel)设计了一个下棋程序，这个程序具有学习能力，它可以在不断的对弈中改善自己的棋艺。4年后，这个程序战胜了设计者本人。又过了3年，这个程序战胜了美国一个保持8年之久的常胜不败的冠军。这个程序向人们展示了机器学习的能力，提出了许多令人深思的社会问题与哲学问题（呵呵，人工智能正常的轨道没有很大的发展，这些什么哲学伦理啊倒发展的挺快。什么未来机器越来越像人，人越来越像机器啊。什么机器会反人类啊，ATM是开第一枪的啊等等。人类的思维无穷啊）。机器学习虽然发展了几十年，但还是存在很多没有良好解决的问题：例如图像识别、语音识别、自然语言理解、天气预测、基因表达、内容推荐等等。目前我们通过机器学习去解决这些问题的思路都是这样的（以视觉感知为例子）：从开始的通过传感器（例如CMOS）来获得数据。然后经过预处理、特征提取、特征选择，再到推理、预测或者识别。最后一个部分，也就是机器学习的部分，绝大部分的工作是在这方面做的，也存在很多的paper和研究。而中间的三部分，概括起来就是特征表达。良好的特征表达，对最终算法的准确性起了非常关键的作用，而且系统主要的计算和测试工作都耗在这一大部分。但，这块实际中一般都是人工完成的。靠人工提取特征。截止现在，也出现了不少NB的特征（好的特征应具有不变性（大小、尺度和旋转等）和可区分性）：例如Sift的出现，是局部图像特征描述子研究领域一项里程碑式的工作。由于SIFT对尺度、旋转以及一定视角和光照变化等图像变化都具有不变性，并且SIFT具有很强的可区分性，的确让很多问题的解决变为可能。但它也不是万能的。然而，手工地选取特征是一件非常费力、启发式（需要专业知识）的方法，能不能选取好很大程度上靠经验和运气，而且它的调节需要大量的时间。既然手工选取特征不太好，那么能不能自动地学习一些特征呢？答案是能！Deep Learning就是用来干这个事情的，看它的一个别名UnsupervisedFeature Learning，就可以顾名思义了，Unsupervised的意思就是不要人参与特征的选取过程。那它是怎么学习的呢？怎么知道哪些特征好哪些不好呢？我们说机器学习是一门专门研究计算机怎样模拟或实现人类的学习行为的学科。好，那我们人的视觉系统是怎么工作的呢？为什么在茫茫人海，芸芸众生，滚滚红尘中我们都可以找到另一个她（因为，你存在我深深的脑海里，我的梦里 我的心里 我的歌声里……）。人脑那么NB，我们能不能参考人脑，模拟人脑呢？（好像和人脑扯上点关系的特征啊，算法啊，都不错，但不知道是不是人为强加的，为了使自己的作品变得神圣和高雅。）近几十年以来，认知神经科学、生物学等等学科的发展，让我们对自己这个神秘的而又神奇的大脑不再那么的陌生。也给人工智能的发展推波助澜。三、人脑视觉机理1981 年的诺贝尔医学奖，颁发给了 David Hubel（出生于加拿大的美国神经生物学家） 和TorstenWiesel，以及 Roger Sperry。前两位的主要贡献，是“发现了视觉系统的信息处理”：可视皮层是分级的：我们看看他们做了什么。1958 年，DavidHubel 和Torsten Wiesel 在 JohnHopkins University，研究瞳孔区域与大脑皮层神经元的对应关系。他们在猫的后脑头骨上，开了一个3 毫米的小洞，向洞里插入电极，测量神经元的活跃程度。然后，他们在小猫的眼前，展现各种形状、各种亮度的物体。并且，在展现每一件物体时，还改变物体放置的位置和角度。他们期望通过这个办法，让小猫瞳孔感受不同类型、不同强弱的刺激。之所以做这个试验，目的是去证明一个猜测。位于后脑皮层的不同视觉神经元，与瞳孔所受刺激之间，存在某种对应关系。一旦瞳孔受到某一种刺激，后脑皮层的某一部分神经元就会活跃。经历了很多天反复的枯燥的试验，同时牺牲了若干只可怜的小猫，David Hubel 和Torsten Wiesel 发现了一种被称为“方向选择性细胞（Orientation Selective Cell）”的神经元细胞。当瞳孔发现了眼前的物体的边缘，而且这个边缘指向某个方向时，这种神经元细胞就会活跃。这个发现激发了人们对于神经系统的进一步思考。神经-中枢-大脑的工作过程，或许是一个不断迭代、不断抽象的过程。这里的关键词有两个，一个是抽象，一个是迭代。从原始信号，做低级抽象，逐渐向高级抽象迭代。人类的逻辑思维，经常使用高度抽象的概念。例如，从原始信号摄入开始（瞳孔摄入像素 Pixels），接着做初步处理（大脑皮层某些细胞发现边缘和方向），然后抽象（大脑判定，眼前的物体的形状，是圆形的），然后进一步抽象（大脑进一步判定该物体是只气球）。这个生理学的发现，促成了计算机人工智能，在四十年后的突破性发展。总的来说，人的视觉系统的信息处理是分级的。从低级的V1区提取边缘特征，再到V2区的形状或者目标的部分等，再到更高层，整个目标、目标的行为等。也就是说高层的特征是低层特征的组合，从低层到高层的特征表示越来越抽象，越来越能表现语义或者意图。而抽象层面越高，存在的可能猜测就越少，就越利于分类。例如，单词集合和句子的对应是多对一的，句子和语义的对应又是多对一的，语义和意图的对应还是多对一的，这是个层级体系。敏感的人注意到关键词了：分层。而Deep learning的deep是不是就表示我存在多少层，也就是多深呢？没错。那Deep learning是如何借鉴这个过程的呢？毕竟是归于计算机来处理，面对的一个问题就是怎么对这个过程建模？因为我们要学习的是特征的表达，那么关于特征，或者说关于这个层级特征，我们需要了解地更深入点。所以在说Deep Learning之前，我们有必要再啰嗦下特征（呵呵，实际上是看到那么好的对特征的解释，不放在这里有点可惜，所以就塞到这了）。参考http://blog.csdn.net/zouxy09/article/details/8775360  （感谢，我此博客转载于他，为了方便自己查阅）"}
{"content2":"全球人工智能与机器人峰会（CCF-GAIR）是由中国计算机学会（CCF）主办，雷锋网、香港中文大学（深圳）承办，得到了深圳市政府的大力指导，是国内人工智能和机器人学术界、工业界及投资界三大领域的顶级交流博览盛会，旨在打造国内人工智能领域极具实力的跨界交流合作平台。2019CCF-GAIR全球人工智能与机器人峰会将于2019年7月12-14日在深圳香格里拉大酒店举行，将结合语音、计算机视觉等传统人工智能重点方向的研究，以及如在经济学等领域的新应用，承接历史与未来、学术研究与产业应用，对世界和中国近四十年来的人工智能研究进行一个系统性的回顾并展望在当前复杂国际形势下中国人工智能的未来发展。会议大体日程：人工智能前沿专场中国人工智能四十年纪念专场机器人前沿专场智能商业专场城市视觉大脑专场智慧城市专场5G & AIoT专场AI 金融专场智能交通专场AI 医疗专场AI 芯片专场智慧教育专场前沿计算——类脑计算&量子计算专场会议票价：3999元/人票价权益：包含峰会VIP专享靠前座位区、峰会三天通票、会刊一份、三天峰会酒店自助午餐一份。更多2019CCF-GAIR的内容，请上活动家查看，详情：https://www.huodongjia.com/event-1544967184.html"}
{"content2":"微软在上月宣布组建自己的 AI 研究小组。该小组汇集了超过 5000 名计算机科学家和工程师，加上微软内部研究部门，将共同挖掘 AI 技术。与此同时，亚马逊，Facebook，Google，IBM 还有微软联合宣布成立一个非盈利组织「人工智能合作伙伴」。该组织将致力于推进人工智能研究，树立开发新的人工智能技术准则，以及加强公众对人工智能的认识。而巨头们也纷纷拿出了自己的看家本领，Apple 的 Siri 利用自然语言处理来识别语音命令；Facebook 的深度学习面部识别算法能够快速准确地识别出人脸；Google 的「大脑」可能更为聪明，而且能够安装在数百万安卓系统的设备上。而隐藏在 CRM 领域的对手之一 Salesforce 也声称推出了一款全新的人工智能平台。还有 Amazon、Netflix 和 Spotify 都在强调使用机器学习了解如何与客户建立联系。最近的人工智能领域如此炙手可热，就连微软的首席执行官纳德拉也提到，「将 AI 覆盖到所有领域，是因为微软想要实现「AI 技术民主化」，从而解决全球最紧迫的挑战。」据悉，微软将从以下四个细分方向实现「AI 技术民主化」：Agents. Harness AI to fundamentally change human and computer interaction through agents such as Microsoft』s digital personal assistant Cortana1、助手。利用 AI 从根本上改变人机交互过程，如微软的数字个人助理 Cortana。Applications. Infuse every application, from the photo app on people』s phones to Skype and Office 365, with intelligence2、应用平台。从移动端的照片 app 到 Skype，再到 Office365，每一个应用平台融入了 AI 技术。Services. Make these same intelligent capabilities that are infused in Microsoft』s apps—cognitive capabilities such as vision and speech, and machine analytics—available to every application developer in the world3、服务。让全球每位应用开发者都能获取同样的 AI 技术支持。如微软 app 中视觉与语音识别能力，机器分析能力。Infrastructure. Build the world』s most powerful AI supercomputer with Azure and make it available to anyone, to enable people and organizations to harness its power4、基础架构。以 Azure 云为平台，构建强大的 AI 超级计算机，为企业和个人提供该项服务。那么，微软近期推出的几款人工智能产品看似「姗姗来迟」，但在 AI 领域的重量可不能小视，那么，微软在如何布局 AI 呢？其旗下几大产品又都与 AI 有着怎样的关系？1、推出人工智能版 Dynamics365提升销售转化率，更好地理解客户行为，是如今移动、社交、云三者融合时代的大背景下企业需要具备的重要能力。过去几周，CRM 领域频频传来人工智能利好的消息。如今，微软也向世人透露：在下月 1 日，将推出人工智能版 Dynamics365，为销售人员提供云端商务应用解决方案。该款产品将 ERP 与 CRM 进行整合，打造成为一个单一的云端解决方案。此次产品的推出主要针对的就是 CRM 领域占主导地位的 Salesforce，而此前不久，Salesforce 刚刚推出了一款名为「爱因斯坦」的人工智能云平台，能够为企业客户提供相应服务。Microsoft has built in a couple of intelligence features into the release designed specifically for sales and service personnel. First, there is Customer Insights, a stand-alone cloud service, which enables users to bring in a variety of internal and external data sources. Companies can integrate all of this data with internal metrics (KPIs) to drive automated actions based on the data. The solution includes partner data from the likes of Facebook and Trip Advisor (proving you don』t need to own an external data source to take advantage of it).微软推出的 Dynamics365 能够为销售人员提供以下几类服务：一是客户洞察（Customer Insights），一款单机云服务，帮助用户收集各类内外部数据信息。企业能够将所有数据与内部指标（KPIs）进行集成，基于数据进行自动化行为的驱动。这套解决方案还包括了合作伙伴如 Facebook 和 Trip Advisor 上产生的大量数据，不过，这也说明了用户无需借外部获取数据。It』s been designed as a stand-alone service that can work with any of the Dynamics 365 CRM components—sales, customer service or field service—and can also work with any external CRM tool with open APIs. This last point is particularly telling because it』s giving customers who might not be using Dynamics 365 (but are using other Microsoft tools like Outlook) access to this feature.一方面，单机服务能够与 Dynamics365 CRM 的组成部件进行很好地兼容，包括销售、客服或者现场服务；另一方面，还能与任何一个带有开放 API 接口的外部 CRM 工具进行兼容。此外，单机服务还能为并未使用 Dynamics365，而是微软其他工具如 Outlook 的客户，提供该项服务的渠道。The second piece is called Relationship Insights, which as the name suggests gives sales people information about the status of their customer relationships at any given moment. It』s built on the on the Cortana Intelligence Suite, which Microsoft introduced in 2015 and uses tools like sentiment analysis to check on the likelihood of the deal closing and the next best action to take.二是关系洞察（Relationship Insights），为销售人员即时提供客户关系信息。该服务基于人工智能助手小娜的平台进行提供。这个智能助手于去年在全球推出，通过情感分析检测交易结束的可能性，并判断下一步最佳实践方案。For now, know that Microsoft has consolidated its artificial intelligence tools into a single, coherent division and just about every vendor—not just those selling CRM—is trying to build some level of intelligence into its products. Dynamics 365 is just the latest manifestation.总的来说，微软已将人工智能工具嵌入一个单独连贯的区域，而且不只是 CRM 领域的供应商，基本上每个供应商都在试图将某种程度的智能技术嵌入到其产品中。2、Office365 的个人分析服务在上月举办的 Ignite 年度大会上，微软展示了 Office365 新的 AI 功能，覆盖了为企业提供的核心产品。New smart features coming to Office 365 include Tap in Word and Outlook, which aims to help workers reuse content already to be found in corporate documents for new documents and emails.Office365 本身是一款文档协作工具，但是经过 AI 升级后的 Office365 具有了以下一些特点：Tap in Word 和 Outlook，旨在帮助办公人员将公司文档中现有的内容再次用于新的文件和电邮。Another is Quickstarter for PowerPoint and Sway, offering a faster way to integrate text and images into presentations. Excel meanwhile will gain a Bing Maps feature to visualize geographic data.另外，Quickstarter for PowerPoint 和 Sway，能够更为快速地将文本与图像集成到同一个演示文档中。同时，Excel 表格也添加了 Bing 地图功能，将地理数据进行可视化。Office 365 is also gaining the MyAnalytics personal analytics service, formerly known as Delve Analytics, which Microsoft gained when it acquired VoloMetrix last year.同样，Office365 还添加了 MyAnalytics 个人分析服务（之前被称为 Delve Analytics），该服务主要是通过去年对 VoloMetrix 的收购而来。MyAnalytics provides a visualized breakdown of personal performance at work, offering a snapshot of 'meeting hours', 'email hours', 'focus hours', and 'after hours'. Users can also query the percentage of email read by recipients and what time email was read to help workers optimize the timing of email.MyAnalytics 将个人工作表现进行视觉化细分，如将「会议时间」、「电邮时间」、「关注时间」等进行快照处理。用户还能查询用户电邮的读取情况，从而帮助工作人员优化发送电邮的时间。The main goal of MyAnalytics is to help workers manage how they personally spend their time at work. However, it also has a social component that, for example, shows response times to colleagues as well as customers. Users can drill down into each category to discover how much time was spent multi-tasking and assess whether it was time well spent.MyAnalytics 主要帮助员工有效管理个人工作时间。不过，这也包括社交时间，如能够显示出对同事和客户的响应时间。通过仔细研究每项功能，用户能够发现在进行多任务时的耗时，评估时间的利用是否妥当。3、Azure 云融入 AI 技术除了应用程序和服务上进行人工智能技术的嵌入，微软还悄悄在其 Azure 云服务上进行了一定程度 AI 技术的融入。在过去的两年间，微软已在全球五大洲 15 个国家进行了大规模数据中心的部署。前不久，微软在爱尔兰都柏林的发布会称要将 Azure 云打造成为世界上第一款人工智能超计算机（下称「超算」）。\"Ultimately the cloud is about powering the next generation of applications,\" he said. \"It is always the next generation applications that have driven infrastructure and when we look at this current generation of applications that people are building, the thing that is going to define these applications, that characterises these applications, is machine learning and artificial intelligence. Therefore we are building out Azure as the first AI supercomputer.\"纳德拉认为，「云计算将最终主导下一代应用，而下一代应用将推动基础设施建设。目前来看，定义和塑造这些应用的是机器学习和人工智能。因此，我们正力图将 Azure 云打造成为第一款 AI 超级计算机。」「We made a major investment in PGAs [programmable gate arrays],」said Microsoft』 Research engineer Doug Burger at the company』s Ignite conference in Atlanta this week.「PGAs are programmable hardware. You get the efficiency of hardware, but you also get the flexibility to change your functionality on the fly. This new architecture that we built effectively embeds an FPGA-based AI super computer into our old, hyper-scaled cloud. You get compute, scale and efficiency. It will change what is possible for AI.」「我们目前在可编程门阵列 PGAs 上做了大量投入」，微软研究部工程师 Doug Burger 提到，「PGAs 是一款可编程硬件。这样，你能高效利用硬件，并能灵活改变运行中的功能。这种新的基础架构是将基于 FPGA 的人工智能超算，有效嵌入到旧有的超规模云端，从而在计算和扩展能力以及效率进行提升。这将改变人工智能的应用。」「We now have FPGA support across every compute port of Azure,」said Microsoft CEO Satya Nadella.「It means we have the ability through the magic of the fabric of the network to distribute you machine learnings, your deep neural nets to all of the silicon that is available so you get back performance, that scale.」纳德拉还提到，「现在，FPGA 可以支持 Azure 云平台的每个运算过程，这意味着我们有能力通过构造网络，将机器学习等深层神经网分发给所有的内部芯片，通过 BPS 衡量计算机的性能。」微软并不是在打无把握之仗微软高层人士之一 Qi Lu 认为，想要赢得这场没有硝烟的战争，手中还是得具备五个核心要素：Qi Lu is happy to make the case for Microsoft』s competitive advantage. Lu is one of the dozen people on Nadella』s senior leadership team, overseeing the company』s applications and service groups.But to win, Lu says, a company needs five \"key assets.\" The first is a \"conversation canvas\"—a place where people are doing lots of talking and texting. Microsoft has Office, Outlook, Skype, and Cortana. The second is that AI \"brain\"—a sophisticated mental model of the world. Microsoft says its own AI efforts date back nearly 20 years. The third is access to a social graph—people』s activity on the internet often involves their friends and coworkers. Not coincidentally, a few days after I met Lu, Microsoft announced it would spend $26.2 billion to acquire LinkedIn, and its 433 million registered users.一是「会话区域」，也就是人们能够进行大量交谈和信息发送的地方。比如，微软现在有了 Office，Outlook，Skype 和 Cortana。二是 AI「头脑」，能够面对周围世界进行复杂的思维模式。三是能够获取社交图谱，因为人类在互联网上的活动往往会与朋友和同事有交集。正因如此，前一段时间，微软打败其中一个强大的竞争对手 Salesforce，以 262 亿美元收购 LinkedIn，目的是为了获取其大量的注册用户资源。四是为人工智能研发而创建的平台。微软有 Windows，Xbox 等一系列设备。五是拥有一批强大的开发团队，并且该团队愿意为此项事业献身。毫无疑问，微软将目光转向人工智能，与强大对手争抢资源，打得并不是无把握之仗。目前 AI 技术也有一定的局限性While AI research has been going on for more than 60 years, the technology is now at an inflection point, the panelists agreed. That has happened because of three things: faster, more powerful computers; critical computer science advances, mainly statistical machine learning and deep learning techniques; and the massive information available due to sensors and the Internet of Things.尽管 AI 研究已经持续了近 60 年，但是该项技术目前仍是处于转折期。这主要基于三类事物的改变：更快更强大的计算机；关键领域计算机科学的进步，即机器学习和深度学习技术；以及基于传感器和物联网环境下可获取到的大量信息。Teaching machines common sense, said LeCun. He gave the problem of text translation as an example. The technology is far from perfect, he said, because machines don』t have a deep understanding of the text they are translating.「Most of what we [humans] learned, we learned in the first few years of life just by observing the world,」said LeCun.「And we don』t know how to do that with machines, how to teach them to learn by observing the world. This is one mountain we need to climb, but we don』t know how many mountains are behind it.」局限一：以文字翻译为例，文字翻译技术目前还远远不够完美，因为机器对所翻译出来的文字内容没有深层次的感知能力。Facebook AI 技术负责人 Yann LeCun 曾提到，「人类所习得的大多数知识主要通过早年观察周围世界获取的。完全不知道如何改造机器，教会机器通过观察世界进行学习。」Technical challenges aside, another massive mountain to climb, says Banavar, is how to deploy AI in the real world. That would have to involve the point-of-view of not just tech developers, but also users and policy-makers.「In environments where machines and humans are interacting there』s got to be an element of trust,」he said.「That trust building will take time.」局限二：除了一个又一个的技术挑战需要攻克，如何在现实世界对人工智能进行部署同样也是一个问题。这必然会受到技术开发人员、用户甚至是政治决策者的影响。「在人机交互的环境中，必然存在信任的问题。这种信任的建立需要一定的时间。」Banavar 提到。Deploying AI systems in the real world will also include other humanistic challenges, said Jeannette Wing, corporate vice president at Microsoft Research. Her comments stemmed from the fiasco earlier this year with Microsoft』s AI chatbot Tae, which got hacked and learned to become racist and abusive, forcing the company to take it down within 24 hours. This was in the U.S. A similar chatbot in use in China since 2014 has been massively successful.局限三：「在真实世界部署 AI 技术，同样还涉及到其他方面人性的挑战。」微软研究小组负责人 Jeannette Wing 提到。她提起这个话题的主要原因在于，今年上半年微软开发出的一款名叫 Tae 的人工智能聊天机器人，因受到黑客攻击，结果经过学习之后言语变得十分具有攻击性，这不得不迫使微软公司在 24 小时之内将其下架。而相对来说，该款产品引进中国之后却获得了极大的成功。总之，人类对人工智能的认知和开发还是处于最初级阶段。或许，在未来人类看到一款带有智能属性的机器人，能够简化软件运行的复杂程度，并引导用户获取问题的解决方案。尽管目前这个目标还未实现，但心中抱有实现生产力的提升的想法不是不可能。（本文由国外媒体综合编译）作者：T客汇 杨丽网站：www.tikehui.com"}
{"content2":"计算机科学是一门包含各种各样与计算和信息处理相关主题的系统学科，从抽象的算法分析、形式化语法等等，到更具体的主题如编程语言、程序设计、软件和硬件等。作为一门学科，它与数学、计算机程序设计、软件工程和计算机工程有显著的不同，却通常被混淆，尽管这些学科之间存在不同程度的交叉和覆盖。一、      专业申请形势的介绍及与相关专业的横向比较1、1专业录取形势介绍：CS的申请形势，总体来说是竞争激烈的，甚至可以说用惨烈来形容。由于北美IT行业的回暖转热，IT行业人才需求增幅较大，对于招生方面需求还是很大点。但是由于申请的人数很多，竞争非常激烈，而且来自其他国家的学生的竞争也日趋激烈，比如印度方面的留学生数量增长也非常快。如果从已知06年OFFER和AD情况（来自中科大bbs）来看，CS录取总数为80，其中有奖（含FELLOW，RA，TA全部）共17，而全奖（fellow，或者同时有fellow+RA，或RA+TA）仅有5个。因此，可以看出CS拿奖的特别是全奖的可能性还是比较低的。1.2专业下不同二级专业的比较：CS专业下分概念方向五个：软件工程、人工智能、理论、系统、软件具体的分支专业方向：软件工程 /软件测试 /软件设计；数据库 /数据挖掘；网络应用 /网络协议 / 网络理论 /网站设计；计算机安全 /网络安全；电子商务；分布式计算 /并行计算；操作系统；计算机语言设计；人工智能 /自然语言处理 /知识工程；计算机游戏设计 /图形学 /人机交互 /计算机动画 / 多媒体；算法分析 /计算理论；计算机硬件 /体系结构 /嵌入式设计。从申请难易来看，象软件工程、数据挖掘、分布式计算是现在比较热门的专业，录取的人数比较多；而人工智能，计算机理论，算法分析，研究方向偏基础，相对来说申请的人数也会少很多，拿奖学金的机会也会比较大。1.3CS与其他理工科的专业比较：与其他理工科相比，CS显然不是那么容易拿奖学金，特别是象生物、物理、化学这样一些专业，拿奖学金比较容易，全奖也比较多。二、      专业申请中二级专业的分类介绍2.1计算机专业的二级专业主要有如下几类：2.1.1 Mathematical foundations 数学基础领域数学基础领域主要包括 Boolean algebra（布尔代数）、Discrete mathematics（离散数学）、Graph theory （图形理论）、Mathematical logic （数理逻辑）、Probability and Statistics（概率论和统计学）、Information theory（ 信息理论）、Domain theory （域论）、Category theory （范畴论）、Set theory （集合论）等方向。2.1.2 Theoretical computer science 理论计算机科学理论计算机科学领域主要包括Algorithmic information theory（算法信息理论）、Computability theory（可计算性理论）、Cryptography （密码学）、Formal semantics of programming languages （程序语言的形式语义学）、Theory of computation (or theoretical computer science)   计算理论（理论计算机科学）、Analysis of algorithms and problem complexity （算法和问题复杂性分析）、Logics and meanings of programs（程序逻辑和内涵）、Mathematical logic and Formal languages（数理逻辑和形式语言）、Type theory（类理论）、Quantum computing （昆腾计算）、Quantum information theory（昆腾信息理论）等方向。2.1.3Hardware 硬件硬件领域主要包括Control structures and Microprogramming（控制结构和微程序设计）、Arithmetic and Logic structures（算法和逻辑结构）、Memory structures（存储结构）、Input/output and Data communications（输入/输出和数据通讯）、Logic Design（逻辑设计）、Integrated circuits（集成电路）、VLSI design（超大规模集成电路设计）、Performance and reliability（性能和稳定性）等研究的方向。2.1.4Computer systems organization 计算机系统组织计算机系统组织领域主要包括Computer architecture（计算机体系结构）Computer networks （计算机网络）、Distributed computing（分布式计算）、Performance of systems    （系统性能）、Computer system implementation（计算机系统实现）等方向。2.1.5 Software 软件软件领域主要包括Computer program and Computer programming（计算机程序和计算机程序设计）、Concurrent Programming（并发程序设计）、Parallel Programming（并行程序设计）Program specification（程序规约）、Program verification（程序验证）、Programming techniques（程序设计技术）、Software engineering（软件工程）、Configuration management and Software Configuration Management (SCM) （配置管理和软件配置管理）、Design patterns （模式设计）、Formal methods （形式方法）、Object orientation（面向对象程序设计）、Aspect orientation （面向方面程序设计）、Documentation Optimization（文件优化）、Software metrics（软件度量学）、 Structured programming（结构化程序设计）、Programming languages（程序设计语言）、Imperative programming Languages（命令程序设计语言）、Functional programming Languages（ 函数程序设计语言）、Logic programming Languages（ 逻辑程序设计语言）、Operating Systems（操作系统）、Compiler（编译器）、Lexical analysis （语法分析)、Compiler optimization（编译器优化）等研究方向。2.1.6 Data and information systems数据和信息系统数据和信息系统领域主要包括如下一些方向：Data structures （数据结构）、Data storage representations（数据储存和表示）、Data encryption（数据加密）、Data compression（数据压缩）、Data recovery（数据恢复）、Coding and Information theory （编码和信息理论）、Files（文件）、File formats（文件格式）、Information systems（信息系统）、Databases （数据库）、Information Storage and retrieval（信息存储和检索）、Information Interfaces and Presentation（信息接口和表述）等。2.1.7Computing methodologies计算方法论计算方法论领域的研究主要有如下几个方向：Symbolic and Algebraic manipulation （符号和代数处理）、Artificial intelligence（人工智能）、Computer graphics（计算机图像学）、Image processing and computer vision（图像处理和计算机视觉）、Pattern recognition（模式识别）、Speech recognition （语音识别）、Simulation and Modeling（仿真和建模）、Document and text processing（文件和文本处理）、Digital signal processing（数字信号处理）等。2.1.8Computer applications计算机的应用领域计算机在各领域的应用是非常广泛的，主要有如下这些方面：Enterprise resource planning（企业资源计划）（ERP）、Customer relationship management（客户关系管理）、Human Resource Management Systems（人力资源管理系统）、Numerical analysis（数值分析）、Automated theorem proving（自动定理验证）、Computer algebra systems（计算机代数系统）、Computational chemistry（计算化学）、Computational physics（计算物理学）、     Bioinformatics（生物信息学）、Computational biology（计算生物学）、Medical informatics   （医药信息学）、Computer-aided engineering（半自动计算机工程）、Robotics（机器人技术）、Human-computer interaction（人机交互）、Speech synthesis（言语合成）Telecommunications（电讯学）、Queueing theory（排队理论）等。2.2从录取难度方面来看：整体来说，计算机专业的录取难度是比较大的，竞争比较激烈。象计算机应用、数据和信息系统、计算机硬件和体系结构这样的方向由于应用性较强，毕业后就业情况比较乐观，因此申请的人数也多，竞争比较激烈，又是计算机专业里面录取难度较大的方向。而象软件，虽然申请人数不少，但是由于招生量比较大，而且涵盖面比较宽，录取难度相对要低一点。而计算数学、计算方法论这种偏向基础性研究的方向，对数学的要求比较高，申请人数相对少很对，难度也相对低很多。三、      专业申请中典型录取特征和典型学校介绍CS专业申请的一些典型录取因素排序：从CS专业来看，其录取看重的因素，排序如下：Publication > 重要的荣誉/奖项/经历 > 出身 > GPA > 推荐信> Toefl> 陶瓷 > PS(SoP) >GRE。3.1 Publication衡量一个学生的研究能力，目前主要是看publication，这里的publication主要是指国际会议的论文，由于CS的特点，当前本学科几乎所有重要的论文基本都是先发表在国际会议上，主要是各领域的TOP conference。这一点对于本科申请者来说要求当然就低一点，不一定要求有发文的。3.2 重要的荣誉/奖项/经历国外 教授最青睐的荣誉/奖项就是数模竞赛和数学竞赛了，当然是全国级的，世界级的更好。其它比较重要的经历如MSR等的研究经历也还不错。此外可能一些重要奖学金，大公司的研究实习经历也会起到一些作用。3.3 出身就是你出自的学校了，很现实的事，如果想要申请专业排名前20，综合排名前50的学校，最好能够有国内top10学校的背景。3.4. GPA很重要，特别是对于本科生，这一点在很多理工科的专业来说都是比较突出的，毕竟本科生不会有太多的研究和工作背景。3.5. 推荐信/PS(SoP)推荐信其实国外 教授还是看的，由于国内众所周知的原因，可靠性会打一定折扣，但如果你的推荐人牛，或推荐老师很负责，或你的内容组织的好的话，还是比较重要的。3.6.Toefl/GREToefl/GRE只要过了一些门槛就可以了，而这个门槛大多数中国学生还是容易达到的。toefl是比GRE重要一些的，不少学校有对toefl单门不能低于多少的限制。3.7.陶瓷陶瓷的重要系数是可变的，陶瓷得法的话是很重要的，但是这个建立在一定的基础上：你的实力确实很强，你的背景与方向很合老板意思（或老板准备转的方向）。"}
{"content2":"Github地址：Mask_RCNN『计算机视觉』Mask-RCNN_论文学习『计算机视觉』Mask-RCNN_项目文档翻译『计算机视觉』Mask-RCNN_推断网络其一：总览『计算机视觉』Mask-RCNN_推断网络其二：基于ReNet101的FPN共享网络『计算机视觉』Mask-RCNN_推断网络其三：RPN锚框处理和Proposal生成『计算机视觉』Mask-RCNN_推断网络其四：FPN和ROIAlign的耦合『计算机视觉』Mask-RCNN_推断网络其五：目标检测结果精炼『计算机视觉』Mask-RCNN_推断网络其六：Mask生成『计算机视觉』Mask-RCNN_推断网络终篇：使用detect方法进行推断『计算机视觉』Mask-RCNN_锚框生成『计算机视觉』Mask-RCNN_训练网络其一：数据集与Dataset类『计算机视觉』Mask-RCNN_训练网络其二：train网络结构&损失函数『计算机视觉』Mask-RCNN_训练网络其三：训练Model一、和SSD锚框对比Mask_RCNN的锚框本质上来说和SSD的是一样的（『TensorFlow』SSD源码学习_其三：锚框生成），中心点的个数等于特征层像素数框体生成是围绕中心点的最终的框体坐标需要归一化到01之间，都是对于输入图片的相对大小RCNN系列一般都是一个共享特征，但在Mask_RCNN结构引入了FPN结构后，和SSD一样，使用了多层特征，这样两者的锚框生成算法可以说是如出一辙了，只不过是生成策略有所微调：SSD中不同特征层对应着不同的网格增强比例参数；Mask_RCNN不通层的比例（anchor_ratios）则完全一致SSD每一层每一个中心点生成该层ratio+2个框；Mask_RCNN生成固定3个框SSD中心点为feat像素偏移0.5步长；Mask_RCNN中心点直接选为feat像素位置而基本生成方式两者完全一致：h乘anchor_ratios**0.5w除anchor_ratios**0.5h、w初始值为给定的参考尺寸，即感受野控制实际依赖的参数为每一层的anchor_ratios和参考尺寸，对SSD：anchor_sizes=[(21., 45.), (45., 99.), (99., 153.), (153., 207.), (207., 261.), (261., 315.)]anchor_ratios=[[2, .5], [2, .5, 3, 1./3], [2, .5, 3, 1./3], [2, .5, 3, 1./3], [2, .5], [2, .5]]对Mask_RCNN（h、w参考尺寸大小一致）:self.config.BACKBONE_STRIDES = [4, 8, 16, 32, 64] # 特征层的下采样倍数，中心点计算使用self.config.RPN_ANCHOR_RATIOS = [0.5, 1, 2] # 特征层锚框生成参数self.config.RPN_ANCHOR_SCALES = [32, 64, 128, 256, 512] # 特征层锚框感受野二、锚框生成锚框生成入口函数位于model.py中的get_anchor函数，需要参数image_shape，保证含有[h, w]即可，也可以包含[h, w, c]，def get_anchors(self, image_shape): \"\"\"Returns anchor pyramid for the given image size.\"\"\" # [N, (height, width)] backbone_shapes = compute_backbone_shapes(self.config, image_shape) # Cache anchors and reuse if image shape is the same if not hasattr(self, \"_anchor_cache\"): self._anchor_cache = {} if not tuple(image_shape) in self._anchor_cache: # Generate Anchors: [anchor_count, (y1, x1, y2, x2)] a = utils.generate_pyramid_anchors( self.config.RPN_ANCHOR_SCALES, # (32, 64, 128, 256, 512) self.config.RPN_ANCHOR_RATIOS, # [0.5, 1, 2] backbone_shapes, # with shape [N, (height, width)] self.config.BACKBONE_STRIDES, # [4, 8, 16, 32, 64] self.config.RPN_ANCHOR_STRIDE) # 1 # Keep a copy of the latest anchors in pixel coordinates because # it's used in inspect_model notebooks. # TODO: Remove this after the notebook are refactored to not use it self.anchors = a # Normalize coordinates self._anchor_cache[tuple(image_shape)] = utils.norm_boxes(a, image_shape[:2]) return self._anchor_cache[tuple(image_shape)]调用函数compute_backbone_shapes计算各个特征层shape：def compute_backbone_shapes(config, image_shape): \"\"\"Computes the width and height of each stage of the backbone network. Returns: [N, (height, width)]. Where N is the number of stages \"\"\" if callable(config.BACKBONE): return config.COMPUTE_BACKBONE_SHAPE(image_shape) # Currently supports ResNet only assert config.BACKBONE in [\"resnet50\", \"resnet101\"] return np.array( [[int(math.ceil(image_shape[0] / stride)), int(math.ceil(image_shape[1] / stride))] for stride in config.BACKBONE_STRIDES]) # [4, 8, 16, 32, 64]调用函数utils.generate_pyramid_anchors生成全部锚框：def generate_pyramid_anchors(scales, ratios, feature_shapes, feature_strides, anchor_stride): \"\"\"Generate anchors at different levels of a feature pyramid. Each scale is associated with a level of the pyramid, but each ratio is used in all levels of the pyramid. Returns: anchors: [N, (y1, x1, y2, x2)]. All generated anchors in one array. Sorted with the same order of the given scales. So, anchors of scale[0] come first, then anchors of scale[1], and so on. \"\"\" # Anchors # [anchor_count, (y1, x1, y2, x2)] anchors = [] for i in range(len(scales)): anchors.append(generate_anchors(scales[i], ratios, feature_shapes[i], feature_strides[i], anchor_stride)) # [anchor_count, (y1, x1, y2, x2)] return np.concatenate(anchors, axis=0)utils.generate_pyramid_anchors会调用utils.generate_anchors来生成每一层的锚框（这一步较多的使用了函数meshgrid，介绍见『Numpy』np.meshgrid）：def generate_anchors(scales, ratios, shape, feature_stride, anchor_stride): \"\"\" scales: 1D array of anchor sizes in pixels. Example: [32, 64, 128] ratios: 1D array of anchor ratios of width/height. Example: [0.5, 1, 2] shape: [height, width] spatial shape of the feature map over which to generate anchors. feature_stride: Stride of the feature map relative to the image in pixels. anchor_stride: Stride of anchors on the feature map. For example, if the value is 2 then generate anchors for every other feature map pixel. \"\"\" # Get all combinations of scales and ratios scales, ratios = np.meshgrid(np.array(scales), np.array(ratios)) scales = scales.flatten() ratios = ratios.flatten() # Enumerate heights and widths from scales and ratios heights = scales / np.sqrt(ratios) widths = scales * np.sqrt(ratios) # Enumerate shifts in feature space shifts_y = np.arange(0, shape[0], anchor_stride) * feature_stride shifts_x = np.arange(0, shape[1], anchor_stride) * feature_stride shifts_x, shifts_y = np.meshgrid(shifts_x, shifts_y) # Enumerate combinations of shifts, widths, and heights box_widths, box_centers_x = np.meshgrid(widths, shifts_x) # (n, 3) (n, 3) box_heights, box_centers_y = np.meshgrid(heights, shifts_y) # (n, 3) (n, 3) # Reshape to get a list of (y, x) and a list of (h, w) # (n, 3, 2) -> (3n, 2) box_centers = np.stack([box_centers_y, box_centers_x], axis=2).reshape([-1, 2]) box_sizes = np.stack([box_heights, box_widths], axis=2).reshape([-1, 2]) # Convert to corner coordinates (y1, x1, y2, x2) boxes = np.concatenate([box_centers - 0.5 * box_sizes, box_centers + 0.5 * box_sizes], axis=1) # 框体信息是相对于原图的, [N, (y1, x1, y2, x2)] return boxes模拟某层的中心点分布最后回到get_anchor，调用utils.norm_boxes将锚框坐标化为01之间：def norm_boxes(boxes, shape): \"\"\"Converts boxes from pixel coordinates to normalized coordinates. boxes: [N, (y1, x1, y2, x2)] in pixel coordinates shape: [..., (height, width)] in pixels Note: In pixel coordinates (y2, x2) is outside the box. But in normalized coordinates it's inside the box. Returns: [N, (y1, x1, y2, x2)] in normalized coordinates \"\"\" h, w = shape scale = np.array([h - 1, w - 1, h - 1, w - 1]) shift = np.array([0, 0, 1, 1]) return np.divide((boxes - shift), scale).astype(np.float32)最终返回相对坐标下的锚框，shape：[anchor_count, (y1, x1, y2, x2)]。"}
{"content2":"CV人物1：Jianbo Shi史建波毕业于UC Berkeley，导师是Jitendra Malik。其最有影响力的研究成果：图像分割。其于2000年在PAMI上多人合作发表”Noramlized cuts and image segmentation”。这是图像分割领域内最经典的算法。主页：www.cis.upenn.edu/~jshi/ 和 www.cs.cmu.edu/~jshi/CV人物2：Kristen Grauman毕业于MIT，导师是Trevor Darrell。其最有影响力的研究成果：Pyramid Match Kernel，用于图像匹配。她和Darrell在2005年CVPR合作发表了”The Pyramid Match Kernel: Discriminative Classification with Sets of Image Features”。金字塔匹配核函数可快速搜索两个特征集合之间匹配的特征，可应用于图像匹配、物体识别，是该领域经典算法之一。2011年Marr奖得主。主页：www.cs.utexas.edu/~grauman/CV人物3：Irfan Essa现任教于Georgin Tech佐治亚理工大学，毕业于MIT，其最有影响力的研究成果：人脸表情识别。Essa和Alex Penland 在1997年PAMI合作发表了”Coding, analysis,interpretation,and recognition of facial expression”, 结合了几何模型和面部肌肉无力模型，用来描述脸部结构。主页：www.ic.gatech.edu/people/irfan-essaCV人物4：Matthew Turk毕业于MIT，最有影响力的研究成果：人脸识别。其和Alex Pentland在1991年发表了”Eigenfaces for Face Recognition”.该论文首次将PCA（Principal Component Analysis）引入到人脸识别中，是人脸识别最早期最经典的方法，且被人实现，开源在OpenCV了。主页：www.cs.ucsb.edu/~mturk/CV人物5：David Lowe毕业于斯坦福大学，导师是Thomas Binfold，最有影响力的研究成果：SIFT。他是SIFT特征点检测的发明人。由于SIFT具有对于图像平移、旋转和尺度变化不变性的优点，使得SIFT成为近十年来最流行的图像特征点检测方法，被广泛用于图像匹配、物体识别、分类等领域。主页：http://www.cs.ubc.ca/~lowe/CV人物6：Pascal Fua毕业于Orsay，导师是O.D.Faugera。最有影响力的研究成果：立体视觉。其在1993年发表了”A parallel stereo algorithm that produces dense depth maps and preserves image features”,提出了利用相关性来估计dense深度图的快速并行立体视觉算法，是立体视觉领域内经典算法之一。主页：http://cvlab.epfl.ch/~fua/ 和 http://people.epfl.ch/pascal.fuaCV人物7：Luc Van Gool毕业于Katholieke Universiteit Leuven.最有影响力的研究成果：图像特征点检测和摄像机标定。Gool等发蒙的Surf(speeded up robust features)是除SIFT外，应用最广泛的特征点检测算法，surf具有提取速度更快、维度更低的优点，也被广泛用于物体检测、识别等。Opencv开源。Marc Pollefeys, Koch和Goolz 1999年IJCV上发表了”self-calibration and metric reconstruction inspite of varying and unknown intrinsic camera parameters”,是摄像机自标定领域内最经典论文，并获1998年Marr奖。主页：http://www.vision.ee.ethz.ch/~vangool/CV人物8：Michal Irani毕业于Hebrew大学，最有影响力的研究成果：超分辨率。她和Peleg于1991年在Graphical Models and Image Processing发表了”Improving resolution by image registration”,提出了用迭代的、反向投影的方法来解决图像放大的问题，是图像超分辨率最经典的算法。我在公司实现的产品化清晰化增强算法就参考了该算法思想哈哈。主页：http://www.wisdom.weizmann.ac.il/~irani/CV人物9： Jean Ponce毕业于Paris Orsay,最有影响力的研究成果：计算机视觉教育、物体识别。他和David Forsyth合写的”Computer Vision: A Modern Approach”被视为现代计算机视觉领域最经典教科书之一。其近年来的研究重点是物体识别，是Spatial Pyramid Matching算法发明人之一，比起之前广泛使用的bag-of-words方法相比，该方法考虑了一些局部特征之间的空间关系，因此更有效地描述物体特征。是目前最普遍使用的算法之一。主页：http://www.di.ens.fr/~ponce/CV人物10： Andrew Blake毕业于Edinburgh，最有影响力的研究成果：目标跟踪、图像分割、人体姿态跟踪与分析。他是世界知名CV专家，两次荣获ECCV最佳论文奖和1次Marr奖。他和Michael Isard在1998年IJCV中合写的”Condensation—conditional density propagation for visual tracking”，将粒子滤波器用于目标跟踪，该领域的经典论文。二人1998年合写的另一篇”Active Contours”是图像分割领域经典算法，该算法用spline函数，通过最小化能量函数，是的样条逼近物体轮廓，在该算法基础上，衍生出了著名的Active shape model。Blake领导的微软剑桥研究院在人体姿态跟踪与分析上去的突破，用于Kinect中。主页：http://research.microsoft.com/~ablakeCV人物11： Antonio Criminisi毕业于牛津大学，导师是Andrew Zisserman 和 Ian Reid。最有吸影响力的研究成果：Image Inpaiting.他在2004年发表”Region filling and object removal by exemplar-based image inpainting”，该方法用于去除图像中大的遮挡物或小的刮痕，结合了采样纹理生成和结构传递的图像修补技术，获得不错效果。主页：http://research.microsoft.com/en-us/people/antcrim/CV人物12： Paul Viola毕业于MIT，研究领域：目标检测；最有影响力的研究成果：人脸检测；他和Michael Jones在2001年CVPR发表了”Rapid object detection using a boosted cascade of simple features”，真正意义上解决了人脸检测的问题，并开启了boosting算法的一个时代，很多学者受到boosting cascade算法的影响，扩展了该算法的应用领域，牛逼的影响力。主页：http://research.microsoft.com/en-us/um/people/viola/CV人物13： Henry Rowley毕业于CMU，导师：Takeo Kanade；研究领域：大规模图像识别和机器学习；最有影响力的研究成果：人脸检测；他使用人工神经网络用于人脸检测，该算法是Paul Viola的boosting cascade人脸检测算法出现前，最经典的人脸检测算法。主页：http://www.cs.cmu.edu/~har/CV人物14： Dorin Comaniclu毕业于Rutgers；最有影响力的研究成果：目标跟踪、图像分割；他在2000年发表了”Real-time tracking of non-rigid objects using mean shift”。该算法首次将mean shift用于目标跟踪，并在2002年PAMI发表了”Mean shift: A robust approach toward feature space analysis”，并将Meanshift拓展应用于图像分割中。主页：http://coewww.rutgers.edu/riul/FORMER/comanici/CV人物15： Henry Schneiderman毕业于CMU，导师：Takeo Kanade；研究领域：目标检测和识别；最有影响力的研究成果：目标检测；他在2000年CVPR上发表了”A statistical method for 3D object detection applied to faces and cars”。该算法采用多视角训练样本，可用于检测不同视角下的物体，如人脸和车，是第一个能够检测侧脸的算法。他创建了PittPatt公司，后被Google收购。主页：http://www.cs.cmu.edu/~hws/CV人物16： William T.Freeman毕业于MIT；研究领域：应用于CV的ML、可视化感知的贝叶斯模型、计算摄影学；最有影响力的研究成果：图像纹理合成；Alex Efros和Freeman在2001年SIGGRAPH上发表了”Image quilting for texture synthesis and transfer”，其思想是从已知图像中获得小块，然后将这些小块拼接mosaic一起，形成新的图像。该算法是图像纹理合成中经典中的经典。主页：http://people.csail.mit.edu/billf/CV人物17： Feifei Li李菲菲，毕业于Caltech；导师：Pietro Perona；研究领域：Object Bank、Scene Classification、ImageNet等；最有影响力的研究成果：图像识别；她建立了图像识别领域的标准测试库Caltech101/256。是词包方法的推动者。主页：http://vision.stanford.edu/~feifeili/CV人物18：Jitendra Malik毕业于斯坦福大学；导师：Thomas O.Binford；研究领域：轮廓检测、图像/视频分割、图形匹配、目标识别等；最有影响力的研究成果：边缘检测、图像分割和形状匹配；Malik培养了众多牛人，牛人的导师，你说牛不牛。培养了Alexie Efros, Jianbo Shi, Paul Debevec, Pietro Perona, Serge J.Belongie, Yair Weiss等知名专家。主页：http://www.cs.berkeley.edu/~malik/CV人物19：Alexie Efros毕业于Berkeley大学；导师：Jitendra Malik；研究领域：Qualitative Reasoning for Image Understanding、Building the Visual Memex等；最有影响力的研究成果：图像纹理合成；他在1999年ICCV发表了”Texture Synthesis by non-parametric sampling”。该论文将MRF引入到纹理合成中。该方法最大限度保留了纹理的局部结构。主页：https://www.cs.cmu.edu/~efros/CV人物20：Andrew Zisserman毕业于剑桥大学；最有影响力的研究成果：视觉几何、目标识别、可视化搜索；他牛逼了，三次获得Marr奖。是CV界权威中的权威。搞CV的人没读过他的多视几何学一书，枉为搞CV的。我2007年起，花了2年时间阅读、编码实现了其中所有两视几何学内容。主页：http://www.robots.ox.ac.uk/~az/CV人物21：Ian D.Reid毕业于牛津大学；最有影响力的研究成果：目标跟踪；他在2007年PAMI发表了”MonoSLAM: real-time single camera SLAM”，是跟踪和机器人导航领域经典论文。在2011年CVPR上，和Ben Benfold发表了”Stable Multi-Target Tracking in Real-time survillance video”。主页：http://www.robots.ox.ac.uk/~ian/CV人物22：Alan L.Yuille毕业于剑桥大学；导师：S.W.Hawking；最有影响力的研究成果：人脸检测和跟踪；他在1992年IJCV发表了”Feature Extraciton from Faces Using Deformable Templates”.该论文首次用可变形模板来描述人脸的特征，如眼睛、嘴巴等。主页：http://www.stat.ucla.edu/~yuille/CV人物23：David Forsyth毕业于牛津大学；最有影响力的研究成果：计算机视觉教育、三维重建、图像与语义信息；他与Jean Ponce合写的”Computer Vision: A Modern Approach”是经典CV教材啊，当初我可是仔细研读、编码实现了的。1993年因论文”Extracing Projective Structure from Single Perspective Views of 3D Point Sets”，而获得Marr奖。主页：http://luthuli.cs.uiuc.edu/~daf/CV人物24：Ram Nevatia毕业于斯坦福大学；导师Thomas O.Binford；最有影响力的研究成果：物体几何形状描述，人体检测与跟踪；他的关于三维物体的广义圆柱体形状的描述，是早期物体识别经典研究方法之一。主页：http://iris.usc.edu/people/nevatia/index.htmlCV人物25：Paul Debevec毕业于Berkeley大学；导师：Jitendra Malik；研究成果：HDR、IBR；他是知名的将CV和CG结合研究的牛人。很多成果应用于好莱坞电影中。他发明了light stage人脸捕捉重建技术，是基于他在2000年SIGGRAPH上的研究成果发展起来的，被用于AVatar等电影。他因此技术，和合作者获得2009年奥斯卡科学和工程奖。主页：http://www.pauldebevec.com/或http://ict.debevec.org/~debevec/CV人物26：David Kriegman毕业于斯坦福大学；导师：Thomas O.Binford；最有影响力的研究成果：人脸识别；他在1997年PAMI发表了”Eigenfaces vs. fisherfaces: recognition using class specific linear projection”，将Fisher线性判决用于人脸识别。主页：http://cseweb.ucsd.edu/~kriegman/CV人物27：Michael J.Black毕业于纽约大学；最有影响力的研究成果：人的姿态估计和跟踪；他在2000年ECCV上发表了”Stochastic tracking of 3d human figures using 2d image motion”，从单个视频中估算和跟踪人体各个部分的三维姿态。2010年，他因此论文获得ECCV Koenderink奖。主页：http://cs.brown.edu/~black/CV人物28：Carlo Tomasi毕业于CMU；导师Takeo Kanade;最有影响力的研究成果：1998年ICCV发表的双边滤波”Bilateral filtering for gray and color images”。2000年IJCV发表的”The earth mover’s distance as a metric for image retrieval”,该论文将EMD(earth mover’s distance)用于度量由不同图像形成的分布，如颜色、纹理，之间的相似程度，并依据此来实现图像检索，检索结果优于分布直方图。目标跟踪，著名的K-L-T tracking算法中的T就是Tomasi。主页：http://www.cs.duke.edu/~tomasi/CV人物29：Larry S.Davis毕业于马里兰大学；最有影响力的研究成果：视频监控；Davis等人实现的W4实时视频监控系统，是最早的能够实时户外人体检测、跟踪和行为分析的视频监控系统。主页：http://www.umiacs.umd.edu/~lsd/CV人物30：Marc Pollefeys毕业于Katholieke Universiteit Leuven；做摄像机标定的人不知道他，我就鄙视你了。Pollefeys,Koch和Gool在1999年IJCV上发表了”Self-Calibration and metric reconstruction inspite of varying and unknown intrinsic camera parameters”，是摄像机自标定最经典论文，因此获得1993年Marr奖。2007年俺仔细学习了其主页提供的CV的ppt课程。主页：http://www.cs.unc.edu/~marc/CV人物31： Richard Szeliski毕业于CMU，导师Takeo Kanade和Geoff Hinton。其编写的这本书不错<Computer Vision: Algorithms and Applications>，详见http://szeliski.org/Book/主页：http://research.microsoft.com/en-us/um/people/szeliski/http://scholar.google.com/citations?view_op=search_authors&hl=en&mauthors=label:computer_vision转载：http://doctorimage.cn/2013/01/01/cv-intro-niubility/#6481970-qzone-1-83120-80417069b226f89a8531d1742d53942d"}
{"content2":"AForge.NET 是基于C#设计的，在计算机视觉和人工智能方向拥有很强大功能的框架。btw... it's an open source framework. 附上官网地址： http://www.aforgenet.com/aforge/framework/ 。今天要介绍的是AForge中的视频采集功能，这里的视频包括从摄像头等设备的输入和从视频文件的输入。首先来认识一下 视频源播放器：VideoSourcePlayer，从摄像头和文件输入的视频，都会通过它来播放，并按帧（Frame）来输出Bitmap数据。VideoSourcePlayer 使用有这么几个重要的步骤：初始化它，设置 VideoSource 属性。VideoSource 接受 IVideoSource 类型的参数，对应到摄像头输入和文件输入，我们分别会把它设置为 VideoCaptureDevice 和 FileVideoSource。注册 NewFrame 事件，开始播放。在 NewFrame 注册的事件中处理每一帧的Bitmap。处理完成后，取消 NewFrame 事件注册，停止它。使用 SignalToStop(); and WaitForStop();整个使用过程是非常简单的。下面分别来看看摄像头输入和文件输入的代码吧：1. 摄像头输入首先是初始化和开始：// 获取视频输入设备列表 FilterInfoCollection devices = new FilterInfoCollection(FilterCategory.VideoInputDevice); // 获取第一个视频设备(示例代码，未对devices个数为0的情况做处理) VideoCaptureDevice source = new VideoCaptureDevice(devices[0].MonikerString); // 设置Frame 的 size 和 rate source.DesiredFrameSize = new Size(640, 360); source.DesiredFrameRate = 1; // 设置VideoSourcePlayer的VideoSource VideoSourcePlayer videoPlayer = new VideoSourcePlayer(); videoPlayer.VideoSource = source; videoPlayer.NewFrame += videoPlayer_NewFrame; videoPlayer.Start();这里是NewFrame事件代码：private void videoPlayer_NewFrame(object sender, ref Bitmap image) { // do sth with image ... }在使用完成后，停止的代码：videoPlayer.NewFrame -= videoPlayer_NewFrame; videoPlayer.SignalToStop(); videoPlayer.WaitForStop();2. 文件输入首先是初始化和开始：// 活体对应视频路径的文件作为视频源 FileVideoSource videoSource = new FileVideoSource(videoFilePath); videoPlayer.VideoSource = videoSource; videoPlayer.NewFrame += videoPlayer_NewFrame; videoPlayer.Start();其余两部分代码和摄像头输入是一样的，这里就不重复了。对于文件输入，还有一点需要注意的，有些机器的codec并不完整，导致FileVideoSource读取某些格式，比如mp4的时候会出现读取错误，这时需要安装一个codec的pack，就可以了。好了，AForge.NET 的视频采集功能就介绍完了，接下来会再挑一些AForge中有趣的功能来做介绍。"}
{"content2":"转自：http://www.thebigdata.cn/JieJueFangAn/13080.html在本篇文章中，我将对机器学习做个概要的介绍。本文的目的是能让即便完全不了解机器学习的人也能了解机器学习，并且上手相关的实 践。这篇文档也算是EasyPR开发的番外篇，从这里开始，必须对机器学习了解才能进一步介绍EasyPR的内核。当然，本文也面对一般读者，不会对阅读 有相关的前提要求。在进入正题前，我想读者心中可能会有一个疑惑：机器学习有什么重要性，以至于要阅读完这篇非常长的文章呢？我并不直接回答这个问题前。相反，我想请大家看两张图，下图是图一：这 幅图上上的三人是当今机器学习界的执牛耳者。中间的是Geoffrey Hinton, 加拿大多伦多大学的教授，如今被聘为“Google大脑”的负责人。右边的是Yann LeCun, 纽约大学教授，如今是Facebook人工智能实验室的主任。而左边的大家都很熟悉，Andrew Ng，中文名吴恩达，斯坦福大学副教授，如今也是“百度大脑”的负责人与百度首席科学家。这三位都是目前业界炙手可热的大牛，被互联网界大鳄求贤若渴的聘 请，足见他们的重要性。而他们的研究方向，则全部都是机器学习的子类–深度学习。下图是图二：这 幅图上描述的是什么？Windows Phone上的语音助手Cortana，名字来源于《光环》中士官长的助手。相比其他竞争对手，微软很迟才推出这个服务。Cortana背后的核心技术是 什么，为什么它能够听懂人的语音？事实上，这个技术正是机器学习。机器学习是所有语音助手产品(包括Apple的siri与Google的Now)能够跟 人交互的关键技术。通过上面两图，我相信大家可以看出机器学习似乎是一个很重要的，有很多未知特性的技术。学习它似乎是一件有趣的任务。实际上，学习机器学习不仅可以帮助我们了解互联网界最新的趋势，同时也可以知道伴随我们的便利服务的实现技术。机器学习是什么，为什么它能有这么大的魔力，这些问题正是本文要回答的。同时，本文叫做“从机器学习谈起”，因此会以漫谈的形式介绍跟机器学习相关的所有内容，包括学科(如数据挖掘、计算机视觉等)，算法(神经网络，svm)等等。本文的主要目录如下：1.一个故事说明什么是机器学习2.机器学习的定义3.机器学习的范围4.机器学习的方法5.机器学习的应用–大数据6.机器学习的子类–深度学习7.机器学习的父类–人工智能8.机器学习的思考–计算机的潜意识9.总结10.后记1.一个故事说明什么是机器学习机器学习这个词是让人疑惑的，首先它是英文名称Machine Learning(简称ML)的直译，在计算界Machine一般指计算机。这个名字使用了拟人的手法，说明了这门技术是让机器“学习”的技术。但是计算机是死的，怎么可能像人类一样“学习”呢？传 统上如果我们想让计算机工作，我们给它一串指令，然后它遵照这个指令一步步执行下去。有因有果，非常明确。但这样的方式在机器学习中行不通。机器学习根本 不接受你输入的指令，相反，它接受你输入的数据! 也就是说，机器学习是一种让计算机利用数据而不是指令来进行各种工作的方法。这听起来非常不可思议，但结果上却是非常可行的。“统计”思想将在你学习“机 器学习”相关理念时无时无刻不伴随，相关而不是因果的概念将是支撑机器学习能够工作的核心概念。你会颠覆对你以前所有程序中建立的因果无处不在的根本理 念。下面我通过一个故事来简单地阐明什么是机器学习。这个故事比较适合用在知 乎上作为一个概念的阐明。在这里，这个故事没有展开，但相关内容与核心是存在的。如果你想简单的了解一下什么是机器学习，那么看完这个故事就足够了。如果 你想了解机器学习的更多知识以及与它关联紧密的当代技术，那么请你继续往下看，后面有更多的丰富的内容。这个例子来源于我真实的生活经验，我在思考这个问题的时候突然发现它的过程可以被扩充化为一个完整的机器学习的过程，因此我决定使用这个例子作为所有介绍的开始。这个故事称为“等人问题”。我相信大家都有跟别人相约，然后等人的经历。现实中不是每个人都那么守时的，于是当你碰到一些爱迟到的人，你的时间不可避免的要浪费。我就碰到过这样的一个例子。对我的一个朋友小Y而言，他就不是那么守时，最常见的表现是他经常迟到。当有一次我跟他约好3点钟在某个麦当劳见面时，在我出门的那一刻我突然想到一个问题：我现在出发合适么？我会不会又到了地点后，花上30分钟去等他？我决定采取一个策略解决这个问题。要 想解决这个问题，有好几种方法。第一种方法是采用知识：我搜寻能够解决这个问题的知识。但很遗憾，没有人会把如何等人这个问题作为知识传授，因此我不可能 找到已有的知识能够解决这个问题。第二种方法是问他人：我去询问他人获得解决这个问题的能力。但是同样的，这个问题没有人能够解答，因为可能没人碰上跟我 一样的情况。第三种方法是准则法：我问自己的内心，我有否设立过什么准则去面对这个问题？例如，无论别人如何，我都会守时到达。但我不是个死板的人，我没 有设立过这样的规则。事实上，我相信有种方法比以上三种都合适。我把过往跟小 Y相约的经历在脑海中重现一下，看看跟他相约的次数中，迟到占了多大的比例。而我利用这来预测他这次迟到的可能性。如果这个值超出了我心里的某个界限，那 我选择等一会再出发。假设我跟小Y约过5次，他迟到的次数是1次，那么他按时到的比例为80%，我心中的阈值为70%，我认为这次小Y应该不会迟到，因此 我按时出门。如果小Y在5次迟到的次数中占了4次，也就是他按时到达的比例为20%，由于这个值低于我的阈值，因此我选择推迟出门的时间。这个方法从它的 利用层面来看，又称为经验法。在经验法的思考过程中，我事实上利用了以往所有相约的数据。因此也可以称之为依据数据做的判断。依据数据所做的判断跟机器学习的思想根本上是一致的。刚 才的思考过程我只考虑“频次”这种属性。在真实的机器学习中，这可能都不算是一个应用。一般的机器学习模型至少考虑两个量：一个是因变量，也就是我们希望 预测的结果，在这个例子里就是小Y迟到与否的判断。另一个是自变量，也就是用来预测小Y是否迟到的量。假设我把时间作为自变量，譬如我发现小Y所有迟到的 日子基本都是星期五，而在非星期五情况下他基本不迟到。于是我可以建立一个模型，来模拟小Y迟到与否跟日子是否是星期五的概率。见下图：这样的图就是一个最简单的机器学习模型，称之为决策树。当我们考虑的自变量只有一个时，情况较为简单。如果把我们的自变量再增加一个。例如小Y迟到的部分情况时是在他开车过来的时候(你可以理解为他开车水平较臭，或者路较堵)。于是我可以关联考虑这些信息。建立一个更复杂的模型，这个模型包含两个自变量与一个因变量。再更复杂一点，小Y的迟到跟天气也有一定的原因，例如下雨的时候，这时候我需要考虑三个自变量。如 果我希望能够预测小Y迟到的具体时间，我可以把他每次迟到的时间跟雨量的大小以及前面考虑的自变量统一建立一个模型。于是我的模型可以预测值，例如他大概 会迟到几分钟。这样可以帮助我更好的规划我出门的时间。在这样的情况下，决策树就无法很好地支撑了，因为决策树只能预测离散值。我们可以用节2所介绍的线 型回归方法建立这个模型。如果我把这些建立模型的过程交给电脑。比如把所有的自变量和因变量输入，然后让计算机帮我生成一个模型，同时让计算机根据我当前的情况，给出我是否需要迟出门，需要迟几分钟的建议。那么计算机执行这些辅助决策的过程就是机器学习的过程。机器学习方法是计算机利用已有的数据(经验)，得出了某种模型(迟到的规律)，并利用此模型预测未来(是否迟到)的一种方法。通 过上面的分析，可以看出机器学习与人类思考的经验过程是类似的，不过它能考虑更多的情况，执行更加复杂的计算。事实上，机器学习的一个主要目的就是把人类 思考归纳经验的过程转化为计算机通过对数据的处理计算得出模型的过程。经过计算机得出的模型能够以近似于人的方式解决很多灵活复杂的问题。下面，我会开始对机器学习的正式介绍，包括定义、范围，方法、应用等等，都有所包含。2.机器学习的定义从广义上来说，机器学习是一种能够赋予机器学习的能力以此让它完成直接编程无法完成的功能的方法。但从实践的意义上来说，机器学习是一种通过利用数据，训练出模型，然后使用模型预测的一种方法。让我们具体看一个例子。拿国民话题的房子来说。现在我手里有一栋房子需要售卖，我应该给它标上多大的价格？房子的面积是100平方米，价格是100万，120万，还是140万？很显然，我希望获得房价与面积的某种规律。那么我该如何获得这个规律？用报纸上的房价平均数据么？还是参考别人面积相似的？无论哪种，似乎都并不是太靠谱。我现在希望获得一个合理的，并且能够最大程度的反映面积与房价关系的规律。于是我调查了周边与我房型类似的一些房子，获得一组数据。这组数据中包含了大大小小房子的面积与价格，如果我能从这组数据中找出面积与价格的规律，那么我就可以得出房子的价格。对规律的寻找很简单，拟合出一条直线，让它“穿过”所有的点，并且与各个点的距离尽可能的小。通过这条直线，我获得了一个能够最佳反映房价与面积规律的规律。这条直线同时也是一个下式所表明的函数：房价 = 面积 * a + b上述中的a、b都是直线的参数。获得这些参数以后，我就可以计算出房子的价格。假设a = 0.75,b = 50，则房价 = 100 * 0.75 + 50 = 125万。这个结果与我前面所列的100万，120万，140万都不一样。由于这条直线综合考虑了大部分的情况，因此从“统计”意义上来说，这是一个最合理的预测。在求解过程中透露出了两个信息：1.房价模型是根据拟合的函数类型决定的。如果是直线，那么拟合出的就是直线方程。如果是其他类型的线，例如抛物线，那么拟合出的就是抛物线方程。机器学习有众多算法，一些强力算法可以拟合出复杂的非线性模型，用来反映一些不是直线所能表达的情况。2.如果我的数据越多，我的模型就越能够考虑到越多的情况，由此对于新情况的预测效果可能就越好。这是机器学习界“数据为王”思想的一个体现。一般来说(不是绝对)，数据越多，最后机器学习生成的模型预测的效果越好。通过我拟合直线的过程，我们可以对机器学习过程做一个完整的回顾。首先，我们需要在计算机中存储历史的数据。接着，我们将这些 数据通过机器学习算法进行处理，这个过程在机器学习中叫做“训练”，处理的结果可以被我们用来对新的数据进行预测，这个结果一般称之为“模型”。对新数据 的预测过程在机器学习中叫做“预测”。“训练”与“预测”是机器学习的两个过程，“模型”则是过程的中间输出结果，“训练”产生“模型”，“模型”指导 “预测”。让我们把机器学习的过程与人类对历史经验归纳的过程做个比对。人类在成长、生活过程中积累了很多的历史与经验。人类定期地对这些经验进行“归纳”，获得了生活的“规律”。当人类遇到未知的问题或者需要对未来进行“推测”的时候，人类使用这些“规律”，对未知问题与未来进行“推测”，从而指导自己的生活和工作。机器学习中的“训练”与“预测”过程可以对应到人类的“归纳”和“推测”过程。通过这样的对应，我们可以发现，机器学习的思想并不复杂，仅仅是对人类在生活 中学习成长的一个模拟。由于机器学习不是基于编程形成的结果，因此它的处理过程不是因果的逻辑，而是通过归纳思想得出的相关性结论。这也可以联想到人类为什么要学习历史，历史实际上是人类过往经验的总结。有句话说得很好，“历史往往不一样，但历史总是惊人的相似”。通过学习历史，我们从 历史中归纳出人生与国家的规律，从而指导我们的下一步工作，这是具有莫大价值的。当代一些人忽视了历史的本来价值，而是把其作为一种宣扬功绩的手段，这其 实是对历史真实价值的一种误用。3.机器学习的范围上文虽然说明了机器学习是什么，但是并没有给出机器学习的范围。其实，机器学习跟模式识别，统计学习，数据挖掘，计算机视觉，语音识别，自然语言处理等领域有着很深的联系。从范围上来说，机器学习跟模式识别，统计学习，数据挖掘是类似的，同时，机器学习与其他领域的处理技术的结合，形成了计算机视觉、语音识别、自然语言处理等 交叉学科。因此，一般说数据挖掘时，可以等同于说机器学习。同时，我们平常所说的机器学习应用，应该是通用的，不仅仅局限在结构化数据，还有图像，音频等 应用。在这节对机器学习这些相关领域的介绍有助于我们理清机器学习的应用场景与研究范围，更好的理解后面的算法与应用层次。下图是机器学习所牵扯的一些相关范围的学科与研究领域。模式识别模式识别=机器学习。两者的主要区别在于前者是从工业界发展起来的概念，后者则主要源自计算机学科。在著名的《Pattern Recognition And Machine Learning》这本书中，Christopher M. Bishop在开头是这样说的“模式识别源自工业界，而机器学习来自于计算机学科。不过，它们中的活动可以被视为同一个领域的两个方面，同时在过去的10 年间，它们都有了长足的发展”。数据挖掘数据挖掘=机器学习+数据库。这几年数据挖掘的概念实在是太耳熟能详。几乎等同于炒作。但凡说数据挖掘都会吹嘘数据挖掘如何如何，例如从数据中挖出金子，以 及将废弃的数据转化为价值等等。但是，我尽管可能会挖出金子，但我也可能挖的是“石头”啊。这个说法的意思是，数据挖掘仅仅是一种思考方式，告诉我们应该 尝试从数据中挖掘出知识，但不是每个数据都能挖掘出金子的，所以不要神话它。一个系统绝对不会因为上了一个数据挖掘模块就变得无所不能(这是IBM最喜欢 吹嘘的)，恰恰相反，一个拥有数据挖掘思维的人员才是关键，而且他还必须对数据有深刻的认识，这样才可能从数据中导出模式指引业务的改善。大部分数据挖掘 中的算法是机器学习的算法在数据库中的优化。统计学习统 计学习近似等于机器学习。统计学习是个与机器学习高度重叠的学科。因为机器学习中的大多数方法来自统计学，甚至可以认为，统计学的发展促进机器学习的繁荣 昌盛。例如著名的支持向量机算法，就是源自统计学科。但是在某种程度上两者是有分别的，这个分别在于：统计学习者重点关注的是统计模型的发展与优化，偏数 学，而机器学习者更关注的是能够解决问题，偏实践，因此机器学习研究者会重点研究学习算法在计算机上执行的效率与准确性的提升。计算机视觉计 算机视觉=图像处理+机器学习。图像处理技术用于将图像处理为适合进入机器学习模型中的输入，机器学习则负责从图像中识别出相关的模式。计算机视觉相关的 应用非常的多，例如百度识图、手写字符识别、车牌识别等等应用。这个领域是应用前景非常火热的，同时也是研究的热门方向。随着机器学习的新领域深度学习的 发展，大大促进了计算机图像识别的效果，因此未来计算机视觉界的发展前景不可估量。语音识别语音识别=语音处理+机器学习。语音识别就是音频处理技术与机器学习的结合。语音识别技术一般不会单独使用，一般会结合自然语言处理的相关技术。目前的相关应用有苹果的语音助手siri等。自然语言处理自 然语言处理=文本处理+机器学习。自然语言处理技术主要是让机器理解人类的语言的一门领域。在自然语言处理技术中，大量使用了编译原理相关的技术，例如词 法分析，语法分析等等，除此之外，在理解这个层面，则使用了语义理解，机器学习等技术。作为唯一由人类自身创造的符号，自然语言处理一直是机器学习界不断 研究的方向。按照百度机器学习专家余凯的说法“听与看，说白了就是阿猫和阿狗都会的，而只有语言才是人类独有的”。如何利用机器学习技术进行自然语言的的 深度理解，一直是工业和学术界关注的焦点。可以看出机器学习在众多领域的外延和应用。机器学习技术的发展促使了很多智能领域的进步，改善着我们的生活。4.机器学习的方法通过上节的介绍我们知晓了机器学习的大致范围，那么机器学习里面究竟有多少经典的算法呢？在这个部分我会简要介绍一下机器学习中的经典代表方法。这部分介绍的重点是这些方法内涵的思想，数学与实践细节不会在这讨论。（1)回归算法在 大部分机器学习课程中，回归算法都是介绍的第一个算法。原因有两个：一.回归算法比较简单，介绍它可以让人平滑地从统计学迁移到机器学习中。二.回归算法 是后面若干强大算法的基石，如果不理解回归算法，无法学习那些强大的算法。回归算法有两个重要的子类：即线性回归和逻辑回归。线 性回归就是我们前面说过的房价求解问题。如何拟合出一条直线最佳匹配我所有的数据？一般使用“最小二乘法”来求解。“最小二乘法”的思想是这样的，假设我 们拟合出的直线代表数据的真实值，而观测到的数据代表拥有误差的值。为了尽可能减小误差的影响，需要求解一条直线使所有误差的平方和最小。最小二乘法将最 优问题转化为求函数极值问题。函数极值在数学上我们一般会采用求导数为0的方法。但这种做法并不适合计算机，可能求解不出来，也可能计算量太大。计 算机科学界专门有一个学科叫“数值计算”，专门用来提升计算机进行各类计算时的准确性和效率问题。例如，著名的“梯度下降”以及“牛顿法”就是数值计算中 的经典算法，也非常适合来处理求解函数极值的问题。梯度下降法是解决回归模型中最简单且有效的方法之一。从严格意义上来说，由于后文中的神经网络和推荐算 法中都有线性回归的因子，因此梯度下降法在后面的算法实现中也有应用。逻辑回 归是一种与线性回归非常类似的算法，但是，从本质上讲，线型回归处理的问题类型与逻辑回归不一致。线性回归处理的是数值问题，也就是最后预测出的结果是数 字，例如房价。而逻辑回归属于分类算法，也就是说，逻辑回归预测结果是离散的分类，例如判断这封邮件是否是垃圾邮件，以及用户是否会点击此广告等等。实 现方面的话，逻辑回归只是对对线性回归的计算结果加上了一个Sigmoid函数，将数值结果转化为了0到1之间的概率(Sigmoid函数的图像一般来说 并不直观，你只需要理解对数值越大，函数越逼近1，数值越小，函数越逼近0)，接着我们根据这个概率可以做预测，例如概率大于0.5，则这封邮件就是垃圾 邮件，或者肿瘤是否是恶性的等等。从直观上来说，逻辑回归是画出了一条分类线，见下图。假 设我们有一组肿瘤患者的数据，这些患者的肿瘤中有些是良性的(图中的蓝色点)，有些是恶性的(图中的红色点)。这里肿瘤的红蓝色可以被称作数据的“标 签”。同时每个数据包括两个“特征”：患者的年龄与肿瘤的大小。我们将这两个特征与标签映射到这个二维空间上，形成了我上图的数据。当我有一个绿色的点时，我该判断这个肿瘤是恶性的还是良性的呢？根据红蓝点我们训练出了一个逻辑回归模型，也就是图中的分类线。这时，根据绿点出现在分类线的左侧，因此我们判断它的标签应该是红色，也就是说属于恶性肿瘤。逻 辑回归算法划出的分类线基本都是线性的(也有划出非线性分类线的逻辑回归，不过那样的模型在处理数据量较大的时候效率会很低)，这意味着当两类之间的界线 不是线性时，逻辑回归的表达能力就不足。下面的两个算法是机器学习界最强大且重要的算法，都可以拟合出非线性的分类线。(2)神经网络神经网络(也称之为人工神经网络，ANN)算法是80年代机器学习界非常流行的算法，不过在90年代中途衰落。现在，携着“深度学习”之势，神经网络重装归来，重新成为最强大的机器学习算法之一。神 经网络的诞生起源于对大脑工作机理的研究。早期生物界学者们使用神经网络来模拟大脑。机器学习的学者们使用神经网络进行机器学习的实验，发现在视觉与语音 的识别上效果都相当好。在BP算法(加速神经网络训练过程的数值算法)诞生以后，神经网络的发展进入了一个热潮。BP算法的发明人之一是前面介绍的机器学 习大牛Geoffrey Hinton(中的中间者)。具体说来，神经网络的学习机理是什么？简单来说，就是分解与整合。在著名的Hubel-Wiesel试验中，学者们研究猫的视觉分析机理是这样的。Hubel-Wiesel试验与大脑视觉机理比 方说，一个正方形，分解为四个折线进入视觉处理的下一层中。四个神经元分别处理一个折线。每个折线再继续被分解为两条直线，每条直线再被分解为黑白两个 面。于是，一个复杂的图像变成了大量的细节进入神经元，神经元处理以后再进行整合，最后得出了看到的是正方形的结论。这就是大脑视觉识别的机理，也是神经 网络工作的机理。让我们看一个简单的神经网络的逻辑架构。在这个网络中，分成 输入层，隐藏层，和输出层。输入层负责接收信号，隐藏层负责对数据的分解与处理，最后的结果被整合到输出层。每层中的一个圆代表一个处理单元，可以认为是 模拟了一个神经元，若干个处理单元组成了一个层，若干个层再组成了一个网络，也就是”神经网络”。在神经网络中，每个处理单元事实上就是一个逻辑回归模型，逻辑回归模型接收上层的输入，把模型的预测结果作为输出传输到下一个层次。通过这样的过程，神经网络可以完成非常复杂的非线性分类。下图会演示神经网络在图像识别领域的一个著名应用，这个程序叫做LeNet，是一个基于多个隐层构建的神经网络。通过LeNet可以识别多种手写数字，并且达到很高的识别精度与拥有较好的鲁棒性。LeNet的效果展示右 下方的方形中显示的是输入计算机的图像，方形上方的红色字样“answer”后面显示的是计算机的输出。左边的三条竖直的图像列显示的是神经网络中三个隐 藏层的输出，可以看出，随着层次的不断深入，越深的层次处理的细节越低，例如层3基本处理的都已经是线的细节了。LeNet的发明人就是前文介绍过的机器 学习的大牛Yann LeCun(右者)。进入90年代，神经网络的发展进入了一个瓶颈期。其主要原因是尽管有BP算法的加速，神经网络的训练过程仍然很困难。因此90年代后期支持向量机(SVM)算法取代了神经网络的地位。(3)SVM（支持向量机）支持向量机算法是诞生于统计学习界，同时在机器学习界大放光彩的经典算法。支持向量机算法从某种意义上来说是逻辑回归算法的强化：通过给予逻辑回归算法更严格的优化条件，支持向量机算法可以获得比逻辑回归更好的分类界线。但是如果没有某类函数技术，则支持向量机算法最多算是一种更好的线性分类技术。但是，通过跟高斯“核”的结合，支持向量机可以表达出非常复杂的分类界线，从而达成很好的的分类效果。“核”事实上就是一种特殊的函数，最典型的特征就是可以将低维的空间映射到高维的空间。例如下图所示：我们如何在二维平面划分出一个圆形的分类界线？在二维平面可能会很困难，但是通过“核”可以将二维空间映射到三维空间，然后使用一个线性平面就可以达成类似 效果。也就是说，二维平面划分出的非线性分类界线可以等价于三维平面的线性分类界线。于是，我们可以通过在三维空间中进行简单的线性划分就可以达到在二维 平面中的非线性划分效果。支持向量机是一种数学成分很浓的机器学习算法（相对的，神经网络则有生物科学成分）。在算法的核心步骤中，有一步证明，即将数据从低维映射到高维不会带来最 后计算复杂性的提升。于是，通过支持向量机算法，既可以保持计算效率，又可以获得非常好的分类效果。因此支持向量机在90年代后期一直占据着机器学习中最 核心的地位，基本取代了神经网络算法。直到现在神经网络借着深度学习重新兴起，两者之间才又发生了微妙的平衡转变。(4)聚类算法前 面的算法中的一个显著特征就是我的训练数据中包含了标签，训练出的模型可以对其他未知数据预测标签。在下面的算法中，训练数据都是不含标签的，而算法的目 的则是通过训练，推测出这些数据的标签。这类算法有一个统称，即无监督算法(前面有标签的数据的算法则是有监督算法)。无监督算法中最典型的代表就是聚类 算法。让我们还是拿一个二维的数据来说，某一个数据包含两个特征。我希望通过聚类算法，给他们中不同的种类打上标签，我该怎么做呢？简单来说，聚类算法就是计算种群中的距离，根据距离的远近将数据划分为多个族群。聚类算法中最典型的代表就是K-Means算法。(5)降维算法降 维算法也是一种无监督学习算法，其主要特征是将数据从高维降低到低维层次。在这里，维度其实表示的是数据的特征量的大小，例如，房价包含房子的长、宽、面 积与房间数量四个特征，也就是维度为4维的数据。可以看出来，长与宽事实上与面积表示的信息重叠了，例如面积=长 × 宽。通过降维算法我们就可以去除冗余信息，将特征减少为面积与房间数量两个特征，即从4维的数据压缩到2维。于是我们将数据从高维降低到低维，不仅利于表 示，同时在计算上也能带来加速。刚才说的降维过程中减少的维度属于肉眼可视的 层次，同时压缩也不会带来信息的损失(因为信息冗余了)。如果肉眼不可视，或者没有冗余的特征，降维算法也能工作，不过这样会带来一些信息的损失。但是， 降维算法可以从数学上证明，从高维压缩到的低维中最大程度地保留了数据的信息。因此，使用降维算法仍然有很多的好处。降维算法的主要作用是压缩数据与提升机器学习其他算法的效率。通过降维算法，可以将具有几千个特征的数据压缩至若干个特征。另外，降维算法的另一个好处是数 据的可视化，例如将5维的数据压缩至2维，然后可以用二维平面来可视。降维算法的主要代表是PCA算法(即主成分分析算法)。(6)推荐算法推荐算法是目前业界非常火的一种算法，在电商界，如亚马逊，天猫，京东等得到了广泛的运用。推荐算法的主要特征就是可以自动向用户推荐他们最感兴趣的东西，从而增加购买率，提升效益。推荐算法有两个主要的类别：一类是基于物品内容的推荐，是将与用户购买的内容近似的物品推荐给用户，这样的前提是每个物品都得有若干个标签，因此才可以找出与用户购买物品类似的物品，这样推荐的好处是关联程度较大，但是由于每个物品都需要贴标签，因此工作量较大。另一类是基于用户相似度的推荐，则是将与目标用户兴趣相同的其他用户购买的东西推荐给目标用户，例如小A历史上买了物品B和C，经过算法分析，发现另一个与小A近似的用户小D购买了物品E，于是将物品E推荐给小A。两类推荐都有各自的优缺点，在一般的电商应用中，一般是两类混合使用。推荐算法中最有名的算法就是协同过滤算法。(7)其他除了以上算法之外，机器学习界还有其他的如高斯判别，朴素贝叶斯，决策树等等算法。但是上面列的六个算法是使用最多，影响最广，种类最全的典型。机器学习界的一个特色就是算法众多，发展百花齐放。下面做一个总结，按照训练的数据有无标签，可以将上面算法分为监督学习算法和无监督学习算法，但推荐算法较为特殊，既不属于监督学习，也不属于非监督学习，是单独的一类。监督学习算法：线性回归，逻辑回归，神经网络，SVM无监督学习算法：聚类算法，降维算法特殊算法：推荐算法除 了这些算法以外，有一些算法的名字在机器学习领域中也经常出现。但他们本身并不算是一个机器学习算法，而是为了解决某个子问题而诞生的。你可以理解他们为 以上算法的子算法，用于大幅度提高训练过程。其中的代表有：梯度下降法，主要运用在线型回归，逻辑回归，神经网络，推荐算法中；牛顿法，主要运用在线型回 归中；BP算法，主要运用在神经网络中；SMO算法，主要运用在SVM中。5.机器学习的应用–大数据说 完机器学习的方法，下面要谈一谈机器学习的应用了。无疑，在2010年以前，机器学习的应用在某些特定领域发挥了巨大的作用，如车牌识别，网络攻击防范， 手写字符识别等等。但是，从2010年以后，随着大数据概念的兴起，机器学习大量的应用都与大数据高度耦合，几乎可以认为大数据是机器学习应用的最佳场 景。譬如，但凡你能找到的介绍大数据魔力的文章，都会说大数据如何准确准确预测到了某些事。例如经典的Google利用大数据预测了H1N1在美国某小镇的爆发。Google成功预测H1N1百度预测2014年世界杯，从淘汰赛到决赛全部预测正确。这些实在太神奇了，那么究竟是什么原因导致大数据具有这些魔力的呢？简单来说，就是机器学习技术。正是基于机器学习技术的应用，数据才能发挥其魔力。大 数据的核心是利用数据的价值，机器学习是利用数据价值的关键技术，对于大数据而言，机器学习是不可或缺的。相反，对于机器学习而言，越多的数据会越 可能提升模型的精确性，同时，复杂的机器学习算法的计算时间也迫切需要分布式计算与内存计算这样的关键技术。因此，机器学习的兴盛也离不开大数据的帮助。 大数据与机器学习两者是互相促进，相依相存的关系。机器学习与大数据紧密联系。但是，必须清醒的认识到，大数据并不等同于机器学习，同理，机器学习也不等同于大数据。大数据中包含有分布式计算，内存数据库，多维分析等等多种技术。单从分析方法来看，大数据也包含以下四种分析方法：1.大数据，小分析：即数据仓库领域的OLAP分析思路，也就是多维分析思想。2.大数据，大分析：这个代表的就是数据挖掘与机器学习分析法。3.流式分析：这个主要指的是事件驱动架构。4.查询分析：经典代表是NoSQL数据库。也就是说，机器学习仅仅是大数据分析中的一种而已。尽管机器学习的一些结果具有很大的魔力，在某种场合下是大数据价值最好的说明。但这并不代表机器学习是大数据下的唯一的分析方法。机 器学习与大数据的结合产生了巨大的价值。基于机器学习技术的发展，数据能够“预测”。对人类而言，积累的经验越丰富，阅历也广泛，对未来的判断越准确。例 如常说的“经验丰富”的人比“初出茅庐”的小伙子更有工作上的优势，就在于经验丰富的人获得的规律比他人更准确。而在机器学习领域，根据著名的一个实验， 有效的证实了机器学习界一个理论：即机器学习模型的数据越多，机器学习的预测的效率就越好。见下图：通过这张图可以看出，各种不同算法在输入的数据量达到一定级数后，都有相近的高准确度。于是诞生了机器学习界的名言：成功的机器学习应用不是拥有最好的算法，而是拥有最多的数据！在 大数据的时代，有好多优势促使机器学习能够应用更广泛。例如随着物联网和移动设备的发展，我们拥有的数据越来越多，种类也包括图片、文本、视频等非结构化 数据，这使得机器学习模型可以获得越来越多的数据。同时大数据技术中的分布式计算Map-Reduce使得机器学习的速度越来越快，可以更方便的使用。种 种优势使得在大数据时代，机器学习的优势可以得到最佳的发挥。6.机器学习的子类–深度学习近来，机器学习的发展产生了一个新的方向，即“深度学习”。虽然深度学习这四字听起来颇为高大上，但其理念却非常简单，就是传统的神经网络发展到了多隐藏层的情况。在 上文介绍过，自从90年代以后，神经网络已经消寂了一段时间。但是BP算法的发明人Geoffrey Hinton一直没有放弃对神经网络的研究。由于神经网络在隐藏层扩大到两个以上，其训练速度就会非常慢，因此实用性一直低于支持向量机。2006 年，Geoffrey Hinton在科学杂志《Science》上发表了一篇文章，论证了两个观点：1.多隐层的神经网络具有优异的特征学习能力，学习得到的特征对数据有更本质的刻画，从而有利于可视化或分类；2.深度神经网络在训练上的难度，可以通过“逐层初始化” 来有效克服。Geoffrey Hinton与他的学生在Science上发表文章通过这样的发现，不仅解决了神经网络在计算上的难度，同时也说明了深层神经网络在学习上的优异性。从此，神经网络重新成为了机器学习界中的主流强大学习技术。同时，具有多个隐藏层的神经网络被称为深度神经网络，基于深度神经网络的学习研究称之为深度学习。由于深度学习的重要性质，在各方面都取得极大的关注，按照时间轴排序，有以下四个标志性事件值得一说：2012 年6月，《纽约时报》披露了Google Brain项目，这个项目是由Andrew Ng和Map-Reduce发明人Jeff Dean共同主导，用16000个CPU Core的并行计算平台训练一种称为“深层神经网络”的机器学习模型，在语音识别和图像识别等领域获得了巨大的成功。Andrew Ng就是文章开始所介绍的机器学习的大牛(中右者)。2012年11月，微软在中国天津的一次活动上公开演示了一个全自动的同声传译系统，讲演者用英文演讲，后台的计算机一气呵成自动完成语音识别、英中机器翻译，以及中文语音合成，效果非常流畅，其中支撑的关键技术是深度学习；2013年1月，在百度的年会上，创始人兼CEO李彦宏高调宣布要成立百度研究院，其中第一个重点方向就是深度学习，并为此而成立深度学习研究院(IDL)。2013年4月，《麻省理工学院技术评论》杂志将深度学习列为2013年十大突破性技术(Breakthrough Technology)之首。文章开头所列的三位机器学习的大牛，不仅都是机器学习界的专家，更是深度学习研究领域的先驱。因此，使他们担任各个大型互联网公司技术掌舵者的原因不仅在于他们的技术实力，更在于他们研究的领域是前景无限的深度学习技术。目前业界许多的图像识别技术与语音识别技术的进步都源于深度学习的发展，除了本文开头所提的Cortana等语音助手，还包括一些图像识别应用，其中典型的代表就是下图的百度识图功能。深度学习属于机器学习的子类。基于深度学习的发展极大的促进了机器学习的地位提高，更进一步地，推动了业界对机器学习父类人工智能梦想的再次重视。7.机器学习的父类–人工智能人工智能是机器学习的父类。深度学习则是机器学习的子类。如果把三者的关系用图来表明的话，则是下图：毫 无疑问，人工智能(AI)是人类所能想象的科技界最突破性的发明了，某种意义上来说，人工智能就像游戏最终幻想的名字一样，是人类对于科技界的最终梦想。 从50年代提出人工智能的理念以后，科技界，产业界不断在探索，研究。这段时间各种小说、电影都在以各种方式展现对于人工智能的想象。人类可以发明类似于 人类的机器，这是多么伟大的一种理念！但事实上，自从50年代以后，人工智能的发展就磕磕碰碰，未有见到足够震撼的科学技术的进步。总 结起来，人工智能的发展经历了如下若干阶段，从早期的逻辑推理，到中期的专家系统，这些科研进步确实使我们离机器的智能有点接近了，但还有一大段距离。直 到机器学习诞生以后，人工智能界感觉终于找对了方向。基于机器学习的图像识别和语音识别在某些垂直领域达到了跟人相媲美的程度。机器学习使人类第一次如此 接近人工智能的梦想。事实上，如果我们把人工智能相关的技术以及其他业界的技术做一个类比，就可以发现机器学习在人工智能中的重要地位不是没有理由的。人类区别于其他物体，植物，动物的最主要区别，作者认为是“智慧”。而智慧的最佳体现是什么？是计算能力么，应该不是，心算速度快的人我们一般称之为天才。是反应能力么，也不是，反应快的人我们称之为灵敏。是记忆能力么，也不是，记忆好的人我们一般称之为过目不忘。是推理能力么，这样的人我也许会称他智力很高，类似“福尔摩斯”，但不会称他拥有智慧。是知识能力么，这样的人我们称之为博闻广，也不会称他拥有智慧。想想看我们一般形容谁有大智慧？圣人，诸如庄子，老子等。智慧是对生活的感悟，是对人生的积淀与思考，这与我们机器学习的思想何其相似？通过经验获取规律，指导人生与未来。没有经验就没有智慧。那么，从计算机来看，以上的种种能力都有种种技术去应对。例如计算能力我们有分布式计算，反应能力我们有事件驱动架构，检索能力我们有搜索引擎，知识存储能力我们有数据仓库，逻辑推理能力我们有专家系统，但是，唯有对应智慧中最显著特征的归纳与感悟能力，只有机器学习与之对应。这也是机器学习能力最能表征智慧的根本原因。让我们再看一下机器人的制造，在我们具有了强大的计算，海量的存储，快速的检索，迅速的反应，优秀的逻辑推理后我们如果再配合上一个强大的智慧大脑，一个真正意义上的人工智能也许就会诞生，这也是为什么说在机器学习快速发展的现在，人工智能可能不再是梦想的原因。人 工智能的发展可能不仅取决于机器学习，更取决于前面所介绍的深度学习，深度学习技术由于深度模拟了人类大脑的构成，在视觉识别与语音识别上显著性的突破了 原有机器学习技术的界限，因此极有可能是真正实现人工智能梦想的关键技术。无论是谷歌大脑还是百度大脑，都是通过海量层次的深度学习网络所构成的。也许借 助于深度学习技术，在不远的将来，一个具有人类智能的计算机真的有可能实现。最 后再说一下题外话，由于人工智能借助于深度学习技术的快速发展，已经在某些地方引起了传统技术界达人的担忧。真实世界的“钢铁侠”，特斯拉CEO马斯克就 是其中之一。最近马斯克在参加MIT讨论会时，就表达了对于人工智能的担忧。“人工智能的研究就类似于召唤恶魔，我们必须在某些地方加强注意。”尽 管马斯克的担心有些危言耸听，但是马斯克的推理不无道理。“如果人工智能想要消除垃圾邮件的话，可能它最后的决定就是消灭人类。”马斯克认为预防此类现象 的方法是引入政府的监管。在这里作者的观点与马斯克类似，在人工智能诞生之初就给其加上若干规则限制可能有效，也就是不应该使用单纯的机器学习，而应该是 机器学习与规则引擎等系统的综合能够较好的解决这类问题。因为如果学习没有限制，极有可能进入某个误区，必须要加上某些引导。正如人类社会中，法律就是一 个最好的规则，杀人者死就是对于人类在探索提高生产力时不可逾越的界限。在这里，必须提一下这里的规则与机器学习引出的规律的不同，规律不是一个严格意义的准则，其代表的更多是概率上的指导，而规则则是神圣不可侵犯，不可修改的。规律可以调整，但规则是不能改变的。有效的结合规律与规则的特点，可以引导出一个合理的，可控的学习型人工智能。8.机器学习的思考–计算机的潜意识最后，作者想谈一谈关于机器学习的一些思考。主要是作者在日常生活总结出来的一些感悟。回 想一下我在节1里所说的故事，我把小Y过往跟我相约的经历做了一个罗列。但是这种罗列以往所有经历的方法只有少数人会这么做，大部分的人采用的是更直接的 方法，即利用直觉。那么，直觉是什么？其实直觉也是你在潜意识状态下思考经验后得出的规律。就像你通过机器学习算法，得到了一个模型，那么你下次只要直接 使用就行了。那么这个规律你是什么时候思考的？可能是在你无意识的情况下，例如睡觉，走路等情况。这种时候，大脑其实也在默默地做一些你察觉不到的工作。这 种直觉与潜意识，我把它与另一种人类思考经验的方式做了区分。如果一个人勤于思考，例如他会每天做一个小结，譬如“吾日三省吾身”，或者他经常与同伴讨论 最近工作的得失，那么他这种训练模型的方式是直接的，明意识的思考与归纳。这样的效果很好，记忆性强，并且更能得出有效反应现实的规律。但是大部分的人可 能很少做这样的总结，那么他们得出生活中规律的方法使用的就是潜意识法。举一 个作者本人关于潜意识的例子。作者本人以前没开过车，最近一段时间买了车后，天天开车上班。我每天都走固定的路线。有趣的是，在一开始的几天，我非常紧张 的注意着前方的路况，而现在我已经在无意识中就把车开到了目标。这个过程中我的眼睛是注视着前方的，我的大脑是没有思考，但是我手握着的方向盘会自动的调 整方向。也就是说。随着我开车次数的增多，我已经把我开车的动作交给了潜意识。这是非常有趣的一件事。在这段过程中，我的大脑将前方路况的图像记录了下 来，同时大脑也记忆了我转动方向盘的动作。经过大脑自己的潜意识思考，最后生成的潜意识可以直接根据前方的图像调整我手的动作。假设我们将前方的录像交给 计算机，然后让计算机记录与图像对应的驾驶员的动作。经过一段时间的学习，计算机生成的机器学习模型就可以进行自动驾驶了。这很神奇，不是么。其实包括 Google、特斯拉在内的自动驾驶汽车技术的原理就是这样。除了自动驾驶汽车以外，潜意识的思想还可以扩展到人的交际。譬如说服别人，一个最佳的方法就是给他展示一些信息，然后让他自己去归纳得出我们想要的结论。这就好比在阐述一个观点时，用一个事实，或者一个故事，比大段的道理要好很多。古往今来，但凡优秀的说客，无不采用的是这种方法。春秋战国时期，各国合纵连横，经常有各种说客去跟一国之君交流，直接告诉君主该做什么，无异于自寻死路，但是跟君主讲故事，通过这些故事让君主恍然大悟，就是一种正确的过程。这里面有许多杰出的代表，如墨子，苏秦等等。基 本上所有的交流过程，使用故事说明的效果都要远胜于阐述道义之类的效果好很多。为什么用故事的方法比道理或者其他的方法好很多，这是因为在人成长的过程， 经过自己的思考，已经形成了很多规律与潜意识。如果你告诉的规律与对方的不相符，很有可能出于保护，他们会本能的拒绝你的新规律，但是如果你跟他讲一个故 事，传递一些信息，输送一些数据给他，他会思考并自我改变。他的思考过程实际上就是机器学习的过程，他把新的数据纳入到他的旧有的记忆与数据中，经过重新 训练。如果你给出的数据的信息量非常大，大到调整了他的模型，那么他就会按照你希望的规律去做事。有的时候，他会本能的拒绝执行这个思考过程，但是数据一 旦输入，无论他希望与否，他的大脑都会在潜意识状态下思考，并且可能改变他的看法。如果计算机也拥有潜意识(正如本博客的名称一样)，那么会怎么样？譬如让计算机在工作的过程中，逐渐产生了自身的潜意识，于是甚至可以在你不需要告诉它做什么时它就会完成那件事。这是个非常有意思的设想，这里留给各位读者去发散思考吧。9.总结本 文首先介绍了互联网界与机器学习大牛结合的趋势，以及使用机器学习的相关应用，接着以一个“等人故事”展开对机器学习的介绍。介绍中首先是机器学习的概念 与定义，然后是机器学习的相关学科，机器学习中包含的各类学习算法，接着介绍机器学习与大数据的关系，机器学习的新子类深度学习，最后探讨了一下机器学习 与人工智能发展的联系以及机器学习与潜意识的关联。经过本文的介绍，相信大家对机器学习技术有一定的了解，例如机器学习是什么，它的内核思想是什么(即统 计和归纳)，通过了解机器学习与人类思考的近似联系可以知晓机器学习为什么具有智慧能力的原因等等。其次，本文漫谈了机器学习与外延学科的关系，机器学习 与大数据相互促进相得益彰的联系，机器学习界最新的深度学习的迅猛发展，以及对于人类基于机器学习开发智能机器人的一种展望与思考，最后作者简单谈了一点 关于让计算机拥有潜意识的设想。机器学习是目前业界最为Amazing与火热 的一项技术，从网上的每一次淘宝的购买东西，到自动驾驶汽车技术，以及网络攻击抵御系统等等，都有机器学习的因子在内，同时机器学习也是最有可能使人类完 成AI dream的一项技术，各种人工智能目前的应用，如微软小冰聊天机器人，到计算机视觉技术的进步，都有机器学习努力的成分。作为一名当代的计算机领域的开 发或管理人员，以及身处这个世界，使用者IT技术带来便利的人们，最好都应该了解一些机器学习的相关知识与概念，因为这可以帮你更好的理解为你带来莫大便 利技术的背后原理，以及让你更好的理解当代科技的进程。10.后记这篇文档花了作者两个月的时间，终于在2014年的最后一天的前一天基本完成。通过这篇文章，作者希望对机器学习在国内的普及做一点贡献，同时也是作者本人自己对于所学机器学习知识的一个融汇贯通，整体归纳的提高过程。"}
{"content2":"原文：http://baike.baidu.com/view/2949.htm人工智能(Artificial Intelligence) ，英文缩写为AI。它是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。 人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。人工智能，英文单词Artilect，来源于 雨果·德·加里斯 的著作“人工智能”一词最初是在1956 年Dartmouth学会上提出的。从那以后，研究者们发展了众多理论和原理，人工智能的概念也随之扩展。人工智能是一门极富挑战性的科学，从事这项工作的人必须懂得计算机知识，心理学和哲学。人工智能是包括十分广泛的科学，它由不同的领域组成，如机器学习，计算机视觉等等，总的说来，人工智能研究的一个主要目标是使机器能够胜任一些通常需要人类智能才能完成的复杂工作。但不同的时代、不同的人对这种“复杂工作”的理解是不同的。例如繁重的科学和工程计算本来是要人脑来承担的，现在计算机不但能完成这种计算，而且能够比人脑做得更快、更准确，因此当代人已不再把这种计算看作是“需要人类智能才能完成的复杂任务”，可见复杂工作的定义是随着时代的发具有人工智能的机器人展和技术的进步而变化的，人工智能这门科学的具体目标也自然随着时代的变化而发展。它一方面不断获得新的进展，另一方面又转向更有意义、更加困难的目标。[1]目前能够用来研究人工智能的主要物质基础以及能够实现人工智能技术平台的机器就是计算机，人工智能的发展历史是和计算机科学技术的发展史联系在一起的。除了计算机科学以外，人工智能还涉及信息论、控制论、自动化、仿生学、生物学、心理学、数理逻辑、语言学、医学和哲学等多门学科。人工智能学科研究的主要内容包括：知识表示、自动推理和搜索方法、机器学习和知识获取、知识处理系统、自然语言理解、计算机视觉、智能机器人、自动程序设计等方面。从1956年正式提出人工智能学科算起，50多年来，取得长足的发展，成为一门广泛的交叉和前沿科学。总的说来，人工智能的目的就是让计算机这台机器能够象人一样思考。如果希望做出一台能够思考的机器，那就必须知道什么是思考，更进一步讲就是什么是智慧。什么样的机器才是智慧的呢？科学家已经作出了汽车，火车，飞机，收音机等等，它们模仿我们身体器官的功能，但是能不能模仿人类大脑的功能呢？到目前为止，我们也仅仅知道这个装在我们天灵盖里面的东西是由数十亿个神经细胞组成的器官，我们对这个东西知之甚少，模仿它或许是天下最困难的事情了。当计算机出现后，人类开始真正有了一个可以模拟人类思维的工具，在以后的岁月中，无数科学家为这个目标努力着。现在人工智能已经不再是几个科学家的专利了，全世界几乎所有大学的计算机系都有人在研究这门学科，学习计算机的大学生也必须学习这样一门课程，在大家不懈的努力下，现在计算机似乎已经变得十分聪明了。例如，1997年5月，IBM公司研制的深蓝（Deep Blue）计算机战胜了国际象棋大师卡斯帕洛夫（Kasparov）。大家或许不会注意到，在一些地方计算机帮助人进行其它原来只属于人类的工作，计算机以它的高速和准确为人类发挥着它的作用。人工智能始终是计算机科学的前沿学科，计算机编程语言和其它计算机软件都因为有了人工智能的进展而得以存在。未来人工智能可能会向以下几个方面发展：模糊处理、并行化、神经网络和机器情感。目前，人工智能的推理功能已获突破，学习及联想功能正在研究之中，下一步就是模仿人类右脑的模糊处理功能和整个大脑的并行化处理功能。人工神经网络是未来人工智能应用的新领域，未来智能计算机的构成，可能就是作为主机的冯・诺依曼型机与作为智能外围的人工神经网络的结合。研究表明：情感是智能的一部分，而不是与智能相分离的，因此人工智能领域的下一个突破可能在于赋予计算机情感能力。情感能力对于计算机与人的自然交往至关重要。人工智能一直处于计算机技术的前沿，人工智能研究的理论和发现在很大程度上将决定计算机技术的发展方向。将来，人工智能技术的发展将会给人们的生活、工作和教育等带来更大的影响。"}
{"content2":"人工智能：模拟人的某些思维过程和智能行为（如学习、推理、思考、规划等），想到人所能想到要做的，提前想到做到或辅助人更容易做到完成。深蓝（美国IBM公司生产的一台超级国际象棋电脑，重1270公斤，有32个大脑（微处理器），每秒钟可以计算2亿步。\"深蓝”输入了一百多年来优秀棋手的对局两百多万局。）、阿尔法围棋（由谷歌（Google）旗下DeepMind公司戴密斯·哈萨比斯领衔的团队开发。其主要工作原理是“深度学习”。击败李世石韩国、柯洁浙江丽水）siri、小米ai音响：都是用的穷举法，if语句写好回应的内容，或者数据库写好回复的多条内容，语音识别也不好，方言听不懂只能识别普通话，普通话还需要慢换个语调就不行了，人也是的大数据机器学习、自主学习、深度学习：以数据为基础，由计算机自动生成特征量，特征表示学习，计算机自动获取特征表示推荐算法指纹识别、人像对比、人脸识别、人脸检测识别食物、花：特征量越多越易识别奇点：数学定义：无限小且不实际存在的“点”宇宙意义：既存在又不能描述的一点物理定义：时空无限弯曲的那一个点人工智能的实力将其分成三大类。弱人工智能Artificial Narrow Intelligence (ANI): 弱人工智能是擅长于单个方面的人工智能。比如有能战胜象棋世界冠军的人工智能，但是它只会下象棋，你要问它怎样更好地在硬盘上储存数据，它就不知道怎么回答你了。强人工智能Artificial General Intelligence (AGI): 人类级别的人工智能。强人工智能是指在各方面都能和人类比肩的人工智能，人类能干的脑力活它都能干。创造强人工智能比创造弱人工智能难得多，我们现在还做不到。Linda Gottfredson教授把智能定义为“一种宽泛的心理能力，能够进行思考、计划、解决问题、抽象思维、理解复杂理念、快速学习和从经验中学习等操作。”强人工智能在进行这些操作时应该和人类一样得心应手。超人工智能Artificial Superintelligence (ASI): 牛津哲学家，知名人工智能思想家Nick Bostrom把超级智能定义为“在几乎所有领域都比最聪明的人类大脑都聪明很多，包括科学创新、通识和社交技能。”超人工智能可以是各方面都比人类强一点，也可以是各方面都比人类强万亿倍的。超人工智能也正是为什么人工智能这个话题这么火热的缘故，同样也是为什么永生和灭绝这两个词会在本文中多次出现。造一个能够读懂六岁小朋友的图片书中的文字，并且了解那些词汇意思的电脑——谷歌花了几十亿美元在做，还没做出来。一些我们觉得困难的事情——微积分、金融市场策略、翻译等，对于电脑来说都太简单了我们觉得容易的事情——视觉、动态、移动、直觉用计算机科学家Donald Knuth的说法，“人工智能已经在几乎所有需要思考的领域超过了人类，但是在那些人类和其它动物不需要思考就能完成的事情上，还差得很远。”速度。脑神经元的运算速度最多是200赫兹，今天的微处理器就能以2G赫兹，也就是神经元1000万倍的速度运行容量储存空间 可靠性持久性http://www.360doc.com/content/15/0212/21/18791455_448220606.shtml22、分类法:1 最邻近分类法：nearest neighbor2 朴素贝叶斯算法对数据按其各种特征分属类别的可能性进行加总3 决策树：通过某个属性是否符合某个值进行划分4 支持向量机（support vector machine），划分时需要使用间隔（margin）最大化5 人工神经网络（neural network) minst数据集，误差反向传播作者：千山鸟鱼链接：https://www.jianshu.com/p/7533bd5a2e9b來源：简书"}
{"content2":"人工智能深度学习神经网络在双色球彩票中的应用研究(二)深度学习这个能否用到数字彩（双色球，时时彩）这种预测上来呢？神经网络的看到有不少论文研究这个的，深度学习的还没有看到相关研究的文章预测也就是分类任务 深度学习应该是能做的 序列的数据可能得用LSTM深度学习和机器学习是不是差别很大呢？机器学习的范围太大了 深度学习主要是神经网络的拓展。当年，神经网络被F·Rosenblatt一篇著作《感知机》给直接打了下去，然后美国军方也大量撤资，神经网络的研究就此陷入低潮，直到这些年，计算机飞速发展，加上美国的物理学家Hopfield的两篇论文，才上神经网络重新热了起来，为了让公共不会认为这个又是神经网络而影响推广，所以改名为深度学习，两者其实一码事。-----------------------------------深度学习的dl4j在文本分析上用了lstm（分类任务）http://deeplearning4j.org/lstm.htmlGoogle开源的深度学习框架tensorflow也有个例子：ptb_word_lmcaffeonspark用在视觉图片识别上比较好，dl4j用在NLP上做类似问答搜索的比较多，tensorflow用在学习新的算法上，dl4j, caffeonspark, tensorflow都有LSTM自动分类的算法应用，理论上主流的开源深度学习框架都可以用在彩票预测上来。LSTM实现详解-CSDN.NEThttp://www.csdn.net/article/2015-09-14/2825693深入浅出LSTM神经网络 | 数盟社区http://dataunion.org/19397.html文档自动摘要小工具PKUSUMSUM，集成多种无监督摘要提取算法，支持多种摘要任务与多种语言，采用Java编写，代码完全开源，欢迎批评指正，也欢迎同行一起完善该工具，具体介绍和下载地址为：http://www.icst.pku.edu.cn/lcwm/wanxj/pkusumsum.htm----------------------------------Theano LSTM代码解析 - DeepLearningGroup的博客http://blog.csdn.net/DeepLearningGroup/article/details/51385136人人都能用Python写出LSTM-RNN的代码！[你的神经网络学习最佳起步]http://blog.csdn.net/zzukun/article/details/49968129LSTM实现详解http://www.csdn.net/article/2015-09-14/2825693Python中利用LSTM模型进行时间序列预测分析http://www.cnblogs.com/arkenstone/p/5794063.html时间序列预测分析就是利用过去一段时间内某事件时间的特征来预测未来一段时间内该事件的特征。这是一类相对比较复杂的预测建模问题，和回归分析模型的预测不同，时间序列模型是依赖于事件发生的先后顺序的，同样大小的值改变顺序后输入模型产生的结果是不同的。举个栗子：根据过去两年某股票的每天的股价数据推测之后一周的股价变化；根据过去2年某店铺每周想消费人数预测下周来店消费的人数等等要注意的是应为lstm是依赖序列关系的，所以你的下一次预测结果也会影响之后的预测，如果当中有偏差，这个偏差会累积到之后的预测结果，因此长时间的预测的准确性都是不高，但如果只是看趋势的话倒影响不大。当你理解怎样处理隐藏层的时候，实现任何RNN都会很容易。仅仅把一个常规MLP层放到顶部，然后连接多个层并且把它和最后一层的隐藏层相连，你就完成了。----------------------------------深度学习的知识图谱人工智能深度学习神经网络在双色球彩票中的应用研究(一) - 流风，飘然的风 - 博客园http://www.cnblogs.com/zdz8207/p/DeepLearning-NeuralNetworks.html"}
{"content2":"Deep Learning（深度学习）学习笔记整理系列zouxy09@qq.comhttp://blog.csdn.net/zouxy09作者：Zouxyversion 1.0  2013-04-08声明：1）该Deep Learning的学习系列是整理自网上很大牛和机器学习专家所无私奉献的资料的。具体引用的资料请看参考文献。具体的版本声明也参考原文献。2）本文仅供学术交流，非商用。所以每一部分具体的参考资料并没有详细对应。如果某部分不小心侵犯了大家的利益，还望海涵，并联系博主删除。3）本人才疏学浅，整理总结的时候难免出错，还望各位前辈不吝指正，谢谢。4）阅读本文需要机器学习、计算机视觉、神经网络等等基础（如果没有也没关系了，没有就看看，能不能看懂，呵呵）。5）此属于第一版本，若有错误，还需继续修正与增删。还望大家多多指点。大家都共享一点点，一起为祖国科研的推进添砖加瓦（呵呵，好高尚的目标啊）。请联系：zouxy09@qq.com目录：一、概述二、背景三、人脑视觉机理四、关于特征4.1、特征表示的粒度4.2、初级（浅层）特征表示4.3、结构性特征表示4.4、需要有多少个特征？五、Deep Learning的基本思想六、浅层学习（Shallow Learning）和深度学习（Deep Learning）七、Deep learning与Neural Network八、Deep learning训练过程8.1、传统神经网络的训练方法8.2、deep learning训练过程九、Deep Learning的常用模型或者方法9.1、AutoEncoder自动编码器9.2、Sparse Coding稀疏编码9.3、Restricted Boltzmann Machine(RBM)限制波尔兹曼机9.4、Deep BeliefNetworks深信度网络9.5、Convolutional Neural Networks卷积神经网络十、总结与展望十一、参考文献和Deep Learning学习资源一、概述Artificial Intelligence，也就是人工智能，就像长生不老和星际漫游一样，是人类最美好的梦想之一。虽然计算机技术已经取得了长足的进步，但是到目前为止，还没有一台电脑能产生“自我”的意识。是的，在人类和大量现成数据的帮助下，电脑可以表现的十分强大，但是离开了这两者，它甚至都不能分辨一个喵星人和一个汪星人。图灵（图灵，大家都知道吧。计算机和人工智能的鼻祖，分别对应于其著名的“图灵机”和“图灵测试”）在 1950 年的论文里，提出图灵试验的设想，即，隔墙对话，你将不知道与你谈话的，是人还是电脑。这无疑给计算机，尤其是人工智能，预设了一个很高的期望值。但是半个世纪过去了，人工智能的进展，远远没有达到图灵试验的标准。这不仅让多年翘首以待的人们，心灰意冷，认为人工智能是忽悠，相关领域是“伪科学”。但是自 2006 年以来，机器学习领域，取得了突破性的进展。图灵试验，至少不是那么可望而不可及了。至于技术手段，不仅仅依赖于云计算对大数据的并行处理能力，而且依赖于算法。这个算法就是，Deep Learning。借助于 Deep Learning 算法，人类终于找到了如何处理“抽象概念”这个亘古难题的方法。2012年6月，《纽约时报》披露了Google Brain项目，吸引了公众的广泛关注。这个项目是由著名的斯坦福大学的机器学习教授Andrew Ng和在大规模计算机系统方面的世界顶尖专家JeffDean共同主导，用16000个CPU Core的并行计算平台训练一种称为“深度神经网络”（DNN，Deep Neural Networks）的机器学习模型（内部共有10亿个节点。这一网络自然是不能跟人类的神经网络相提并论的。要知道，人脑中可是有150多亿个神经元，互相连接的节点也就是突触数更是如银河沙数。曾经有人估算过，如果将一个人的大脑中所有神经细胞的轴突和树突依次连接起来，并拉成一根直线，可从地球连到月亮，再从月亮返回地球），在语音识别和图像识别等领域获得了巨大的成功。项目负责人之一Andrew称：“我们没有像通常做的那样自己框定边界，而是直接把海量数据投放到算法中，让数据自己说话，系统会自动从数据中学习。”另外一名负责人Jeff则说：“我们在训练的时候从来不会告诉机器说：‘这是一只猫。’系统其实是自己发明或者领悟了“猫”的概念。”2012年11月，微软在中国天津的一次活动上公开演示了一个全自动的同声传译系统，讲演者用英文演讲，后台的计算机一气呵成自动完成语音识别、英中机器翻译和中文语音合成，效果非常流畅。据报道，后面支撑的关键技术也是DNN，或者深度学习（DL，DeepLearning）。2013年1月，在百度年会上，创始人兼CEO李彦宏高调宣布要成立百度研究院，其中第一个成立的就是“深度学习研究所”（IDL，Institue of Deep Learning）。为什么拥有大数据的互联网公司争相投入大量资源研发深度学习技术。听起来感觉deeplearning很牛那样。那什么是deep learning？为什么有deep learning？它是怎么来的？又能干什么呢？目前存在哪些困难呢？这些问题的简答都需要慢慢来。咱们先来了解下机器学习（人工智能的核心）的背景。二、背景机器学习（Machine Learning）是一门专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能的学科。机器能否像人类一样能具有学习能力呢？1959年美国的塞缪尔(Samuel)设计了一个下棋程序，这个程序具有学习能力，它可以在不断的对弈中改善自己的棋艺。4年后，这个程序战胜了设计者本人。又过了3年，这个程序战胜了美国一个保持8年之久的常胜不败的冠军。这个程序向人们展示了机器学习的能力，提出了许多令人深思的社会问题与哲学问题（呵呵，人工智能正常的轨道没有很大的发展，这些什么哲学伦理啊倒发展的挺快。什么未来机器越来越像人，人越来越像机器啊。什么机器会反人类啊，ATM是开第一枪的啊等等。人类的思维无穷啊）。机器学习虽然发展了几十年，但还是存在很多没有良好解决的问题：例如图像识别、语音识别、自然语言理解、天气预测、基因表达、内容推荐等等。目前我们通过机器学习去解决这些问题的思路都是这样的（以视觉感知为例子）：从开始的通过传感器（例如CMOS）来获得数据。然后经过预处理、特征提取、特征选择，再到推理、预测或者识别。最后一个部分，也就是机器学习的部分，绝大部分的工作是在这方面做的，也存在很多的paper和研究。而中间的三部分，概括起来就是特征表达。良好的特征表达，对最终算法的准确性起了非常关键的作用，而且系统主要的计算和测试工作都耗在这一大部分。但，这块实际中一般都是人工完成的。靠人工提取特征。截止现在，也出现了不少NB的特征（好的特征应具有不变性（大小、尺度和旋转等）和可区分性）：例如Sift的出现，是局部图像特征描述子研究领域一项里程碑式的工作。由于SIFT对尺度、旋转以及一定视角和光照变化等图像变化都具有不变性，并且SIFT具有很强的可区分性，的确让很多问题的解决变为可能。但它也不是万能的。然而，手工地选取特征是一件非常费力、启发式（需要专业知识）的方法，能不能选取好很大程度上靠经验和运气，而且它的调节需要大量的时间。既然手工选取特征不太好，那么能不能自动地学习一些特征呢？答案是能！Deep Learning就是用来干这个事情的，看它的一个别名UnsupervisedFeature Learning，就可以顾名思义了，Unsupervised的意思就是不要人参与特征的选取过程。那它是怎么学习的呢？怎么知道哪些特征好哪些不好呢？我们说机器学习是一门专门研究计算机怎样模拟或实现人类的学习行为的学科。好，那我们人的视觉系统是怎么工作的呢？为什么在茫茫人海，芸芸众生，滚滚红尘中我们都可以找到另一个她（因为，你存在我深深的脑海里，我的梦里 我的心里 我的歌声里……）。人脑那么NB，我们能不能参考人脑，模拟人脑呢？（好像和人脑扯上点关系的特征啊，算法啊，都不错，但不知道是不是人为强加的，为了使自己的作品变得神圣和高雅。）近几十年以来，认知神经科学、生物学等等学科的发展，让我们对自己这个神秘的而又神奇的大脑不再那么的陌生。也给人工智能的发展推波助澜。三、人脑视觉机理1981 年的诺贝尔医学奖，颁发给了 David Hubel（出生于加拿大的美国神经生物学家） 和TorstenWiesel，以及 Roger Sperry。前两位的主要贡献，是“发现了视觉系统的信息处理”：可视皮层是分级的：我们看看他们做了什么。1958 年，DavidHubel 和Torsten Wiesel 在 JohnHopkins University，研究瞳孔区域与大脑皮层神经元的对应关系。他们在猫的后脑头骨上，开了一个3 毫米的小洞，向洞里插入电极，测量神经元的活跃程度。然后，他们在小猫的眼前，展现各种形状、各种亮度的物体。并且，在展现每一件物体时，还改变物体放置的位置和角度。他们期望通过这个办法，让小猫瞳孔感受不同类型、不同强弱的刺激。之所以做这个试验，目的是去证明一个猜测。位于后脑皮层的不同视觉神经元，与瞳孔所受刺激之间，存在某种对应关系。一旦瞳孔受到某一种刺激，后脑皮层的某一部分神经元就会活跃。经历了很多天反复的枯燥的试验，同时牺牲了若干只可怜的小猫，David Hubel 和Torsten Wiesel 发现了一种被称为“方向选择性细胞（Orientation Selective Cell）”的神经元细胞。当瞳孔发现了眼前的物体的边缘，而且这个边缘指向某个方向时，这种神经元细胞就会活跃。这个发现激发了人们对于神经系统的进一步思考。神经-中枢-大脑的工作过程，或许是一个不断迭代、不断抽象的过程。这里的关键词有两个，一个是抽象，一个是迭代。从原始信号，做低级抽象，逐渐向高级抽象迭代。人类的逻辑思维，经常使用高度抽象的概念。例如，从原始信号摄入开始（瞳孔摄入像素 Pixels），接着做初步处理（大脑皮层某些细胞发现边缘和方向），然后抽象（大脑判定，眼前的物体的形状，是圆形的），然后进一步抽象（大脑进一步判定该物体是只气球）。这个生理学的发现，促成了计算机人工智能，在四十年后的突破性发展。总的来说，人的视觉系统的信息处理是分级的。从低级的V1区提取边缘特征，再到V2区的形状或者目标的部分等，再到更高层，整个目标、目标的行为等。也就是说高层的特征是低层特征的组合，从低层到高层的特征表示越来越抽象，越来越能表现语义或者意图。而抽象层面越高，存在的可能猜测就越少，就越利于分类。例如，单词集合和句子的对应是多对一的，句子和语义的对应又是多对一的，语义和意图的对应还是多对一的，这是个层级体系。敏感的人注意到关键词了：分层。而Deep learning的deep是不是就表示我存在多少层，也就是多深呢？没错。那Deep learning是如何借鉴这个过程的呢？毕竟是归于计算机来处理，面对的一个问题就是怎么对这个过程建模？因为我们要学习的是特征的表达，那么关于特征，或者说关于这个层级特征，我们需要了解地更深入点。所以在说Deep Learning之前，我们有必要再啰嗦下特征（呵呵，实际上是看到那么好的对特征的解释，不放在这里有点可惜，所以就塞到这了）。下续"}
{"content2":"《新一代人工智能发展规划》－－－精华版2017年7月8日新一代人工智能发展规划内容体会反思二整体要求（三）战略目标。分三步走：第一步，到2020年人工智能总体技术和应用与世界先进水平同步，人工智能产业成为新的重要经济增长点，人工智能技术应用成为改善民生的新途径，有力支撑进入创新型国家行列和实现全面建成小康社会的奋斗目标。——新一代人工智能理论和技术取得重要进展。大数据智能、跨媒体智能、群体智能、混合增强智能、自主智能系统等基础理论和核心技术实现重要进展，人工智能模型方法、核心器件、高端设备和基础软件等方面取得标志性成果。——人工智能产业竞争力进入国际第一方阵。初步建成人工智能技术标准、服务体系和产业生态链，培育若干全球领先的人工智能骨干企业，人工智能核心产业规模超过1500亿元，带动相关产业规模超过1万亿元。——人工智能发展环境进一步优化，在重点领域全面展开创新应用，聚集起一批高水平的人才队伍和创新团队，部分领域的人工智能伦理规范和政策法规初步建立。第二步，到2025年人工智能基础理论实现重大突破，部分技术与应用达到世界领先水平，人工智能成为带动我国产业升级和经济转型的主要动力，智能社会建设取得积极进展。——新一代人工智能理论与技术体系初步建立，具有自主学习能力的人工智能取得突破，在多领域取得引领性研究成果。——人工智能产业进入全球价值链高端。新一代人工智能在智能制造、智能医疗、智慧城市、智能农业、国防建设等领域得到广泛应用，人工智能核心产业规模超过4000亿元，带动相关产业规模超过5万亿元。——初步建立人工智能法律法规、伦理规范和政策体系，形成人工智能安全评估和管控能力。第三步，到2030年人工智能理论、技术与应用总体达到世界领先水平，成为世界主要人工智能创新中心，智能经济、智能社会取得明显成效，为跻身创新型国家前列和经济强国奠定重要基础。——形成较为成熟的新一代人工智能理论与技术体系。在类脑智能、自主智能、混合智能和群体智能等领域取得重大突破，在国际人工智能研究领域具有重要影响，占据人工智能科技制高点。——人工智能产业竞争力达到国际领先水平。人工智能在生产生活、社会治理、国防建设各方面应用的广度深度极大拓展，形成涵盖核心技术、关键系统、支撑平台和智能应用的完备产业链和高端产业群，人工智能核心产业规模超过1万亿元，带动相关产业规模超过10万亿元。——形成一批全球领先的人工智能科技创新和人才培养基地，建成更加完善的人工智能法律法规、伦理规范和政策体系。１３年规划三、重点任务（一）构建开放协同的人工智能科技创新体系。研究生、博士生方向建立新一代人工智能基础理论体系。专栏1　基础理论1.大数据智能理论。研究数据驱动与知识引导相结合的人工智能新方法、以自然语言理解和图像图形为核心的认知计算理论和方法、综合深度推理与创意人工智能理论与方法、非完全信息下智能决策基础理论与框架、数据驱动的通用人工智能数学模型与理论等。2.跨媒体感知计算理论。研究超越人类视觉能力的感知获取、面向真实世界的主动视觉感知及计算、自然声学场景的听知觉感知及计算、自然交互环境的言语感知及计算、面向异步序列的类人感知及计算、面向媒体智能感知的自主学习、城市全维度智能感知推理引擎。3.混合增强智能理论。研究“人在回路”的混合增强智能、人机智能共生的行为增强与脑机协同、机器直觉推理与因果模型、联想记忆模型与知识演化方法、复杂数据和任务的混合增强智能学习方法、云机器人协同计算方法、真实世界环境下的情境理解及人机群组协同。4.群体智能理论。研究群体智能结构理论与组织方法、群体智能激励机制与涌现机理、群体智能学习理论与方法、群体智能通用计算范式与模型。5.自主协同控制与优化决策理论。研究面向自主无人系统的协同感知与交互，面向自主无人系统的协同控制与优化决策，知识驱动的人机物三元协同与互操作等理论。6.高级机器学习理论。研究统计学习基础理论、不确定性推理与决策、分布式学习与交互、隐私保护学习、小样本学习、深度强化学习、无监督学习、半监督学习、主动学习等学习理论和高效模型。7.类脑智能计算理论。研究类脑感知、类脑学习、类脑记忆机制与计算融合、类脑复杂系统、类脑控制等理论与方法。8.量子智能计算理论。探索脑认知的量子模式与内在机制，研究高效的量子智能模型和算法、高性能高比特的量子人工智能处理器、可与外界环境交互信息的实时量子人工智能系统等。2.建立新一代人工智能关键共性技术体系。专栏2　关键共性技术1.知识计算引擎与知识服务技术。研究知识计算和可视交互引擎，研究创新设计、数字创意和以可视媒体为核心的商业智能等知识服务技术，开展大规模生物数据的知识发现。2.跨媒体分析推理技术。研究跨媒体统一表征、关联理解与知识挖掘、知识图谱构建与学习、知识演化与推理、智能描述与生成等技术，开发跨媒体分析推理引擎与验证系统。3.群体智能关键技术。开展群体智能的主动感知与发现、知识获取与生成、协同与共享、评估与演化、人机整合与增强、自我维持与安全交互等关键技术研究，构建群智空间的服务体系结构，研究移动群体智能的协同决策与控制技术。4.混合增强智能新架构和新技术。研究混合增强智能核心技术、认知计算框架，新型混合计算架构，人机共驾、在线智能学习技术，平行管理与控制的混合增强智能框架。5.自主无人系统的智能技术。研究无人机自主控制和汽车、船舶、轨道交通自动驾驶等智能技术，服务机器人、空间机器人、海洋机器人、极地机器人技术，无人车间/智能工厂智能技术，高端智能控制技术和自主无人操作系统。研究复杂环境下基于计算机视觉的定位、导航、识别等机器人及机械手臂自主控制技术。6.虚拟现实智能建模技术。研究虚拟对象智能行为的数学表达与建模方法，虚拟对象与虚拟环境和用户之间进行自然、持续、深入交互等问题，智能对象建模的技术与方法体系。7.智能计算芯片与系统。研发神经网络处理器以及高能效、可重构类脑计算芯片等，新型感知芯片与系统、智能计算体系结构与系统，人工智能操作系统。研究适合人工智能的混合计算架构等。8.自然语言处理技术。研究短文本的计算与分析技术，跨语言文本挖掘技术和面向机器认知智能的语义理解技术，多媒体信息理解的人机对话系统。3.统筹布局人工智能创新平台。专栏3　基础支撑平台1.人工智能开源软硬件基础平台。建立大数据人工智能开源软件基础平台、终端与云端协同的人工智能云服务平台、新型智能传感器件与集成平台、基于人工智能硬件的新产品设计平台、未来网络中的大数据智能化服务平台等。2.群体智能服务平台。建立群智众创计算支撑平台、科技众创服务系统、群智软件开发与验证自动化系统、群智软件学习与创新系统、开放环境的群智决策系统、群智共享经济服务系统。3.混合增强智能支撑平台。建立人工智能超级计算中心、大规模超级智能计算支撑环境、在线智能教育平台、“人在回路”驾驶脑、产业发展复杂性分析与风险评估的智能平台、支撑核电安全运营的智能保障平台、人机共驾技术研发与测试平台等。4.自主无人系统支撑平台。建立自主无人系统共性核心技术支撑平台，无人机自主控制以及汽车、船舶和轨道交通自动驾驶支撑平台，服务机器人、空间机器人、海洋机器人、极地机器人支撑平台，智能工厂与智能控制装备技术支撑平台等。5.人工智能基础数据与安全检测平台。建设面向人工智能的公共数据资源库、标准测试数据集、云服务平台，建立人工智能算法与平台安全性测试模型及评估模型，研发人工智能算法与平台安全性测评工具集。百度、阿里云、腾讯、科大讯飞4.加快培养聚集人工智能高端人才。大学即将重点设置专业（二）培育高端高效的智能经济。与经济关系1、人工智能新兴产业、智能软硬件、智能机器人、智能运载工具、虚拟现实与增强现实、智能终端、物联网基础器件；技术：软件或IT行业2、智能制造、智能农业、智能物流、智能金融、智能商务、智能家居传统行业3.大力发展智能企业。4.打造人工智能创新高地。（三）建设安全便捷的智能社会。1.发展便捷高效的智能服务。教育、医疗、养老2.推进社会治理智能化。智能政务、智慧法庭、智慧城市、智能交通、智能环保3.利用人工智能提升公共安全保障能力。围绕社会综合治理、新型犯罪侦查、反恐等迫切需求4.促进社会交往共享互信。与社会（四）加强人工智能领域军民融合。（五）构建泛在安全高效的智能化基础设施体系。专栏4　智能化基础设施1.网络基础设施。加快布局实时协同人工智能的5G增强技术研发及应用，建设面向空间协同人工智能的高精度导航定位网络，加强智能感知物联网核心技术攻关和关键设施建设，发展支撑智能化的工业互联网、面向无人驾驶的车联网等，研究智能化网络安全架构。加快建设天地一体化信息网络，推进天基信息网、未来互联网、移动通信网的全面融合。2.大数据基础设施。依托国家数据共享交换平台、数据开放平台等公共基础设施，建设政府治理、公共服务、产业发展、技术研发等领域大数据基础信息数据库，支撑开展国家治理大数据应用。整合社会各类数据平台和数据中心资源，形成覆盖全国、布局合理、链接畅通的一体化服务能力。3.高效能计算基础设施。继续加强超级计算基础设施、分布式计算基础设施和云计算中心建设，构建可持续发展的高性能计算应用生态环境。推进下一代超级计算机研发应用。（六）前瞻布局新一代人工智能重大科技项目。原文：http://www.gov.cn/zhengce/content/2017-07/20/content_5211996.htm"}
{"content2":"（原标题：从马文·明斯基到AlphaGo，人工智能走过了怎样的70年？）【编者按】从19世纪中叶人工智能的萌芽时期，到现今人工智能的重生，从马文·明斯基到AlphaGo，历史上发生了哪些激动人心的故事？本文以此铺展人工智能发展近70年来背后发生的故事。作者@沐阳浸月，中科院自动化所复杂系统国家重点实验室研究生，主攻机器人与人工智能。前不久，在人工智能领域发生了两件大事，一个就是是伟大的人工智能先驱马文·明斯基教授逝世，一个是谷歌AlphaGo击败欧洲围棋冠军，职业围棋二段樊麾。马文·明斯基教授是几乎见证了从人工智能作为一门学科的兴起直至今日成就的所有大风大浪的人，或者可以说何教授本人就是这些大风浪的弄潮儿，他对人工智能的发展的影响意义十分深远。而谷歌AlphaGo此次取得的成就，也可以算是人工智能领域一次里程碑式的创举，它的成功标志着人工智能领域又进入了一个新高度。这篇文章，我们将从马文·明斯基还是哈佛大学本科生的时候讲起，一直到今日AlphaGo的胜利，梳理一下人工智能是怎样从初见萌芽一步一步走到今日的辉煌成就的。要是从宏观的角度来讲，人工智能的历史按照所使用的方法，可以分为两个阶段，分水岭大概在1986年神经网络的回归——在前半段历史中，我们主要使用的方法和思路是基于规则的方法，也就是我们试图找到人类认知事物的方法，模仿人类智能和思维方法，找到一套方法，模拟出人类思维的过程，解决人工智能的问题。后半段的历史，也就是我们现在所处的这个时期，我们主要采取的方法是基于统计的方法，也就是我们现在发现，有的时候我们不需要把人类的思维过程模拟出一套规则来教给计算机，我们可以在一个大的数量集里面来训练计算机，让它自己找到规律从而完成人工智能遇到的问题。这个转化也可以用一个形象的例子来描述，就像我们想造出飞机，就观察鸟是怎么样飞的，然后模仿鸟的动作就行，不需要什么空气动力学什么的，这种思想在人类历史上也被称为“鸟飞派”。但是我们都知道，怀特兄弟造出飞机靠的是空气动力学，而不是仿生学。不过我们不能就因为这一点就笑话人工智能前半段各位研究人员和前辈的努力和心血，因为这是人类认知事物的普遍规律，其实现在也有不少人会认为，计算机可以读懂文字、看懂图片靠的是依靠和我们人类一样的认知过程。在研究基于规则的探索中，人工智能经历了三个主要阶段——兴起、繁盛和萧条。会有这样的过程，一个重要原因是基于规则方法的局限性。好了，那我们就先扒一扒这段历史。一、萌芽阶段人工智能的萌芽时期大概出现在19世纪中叶，第一位需要介绍的人物便是马文·明斯基。明斯基于1946年进入哈佛大学主修物理专业，但他选修的课程相当广泛，从电气工程、数学，到遗传学、心理学等涉及多个学科专业，后来他放弃物理改修数学。1950年，也就是明斯基本科的最后一年，他和他的同学Dean Edmonds建造了世界上第一台神经网络计算机，并命名其为SNARC（Stochastic Neural Analog Reinforcement Calculator）。这台计算机是由3000个真空管和B-24轰炸机上一个多余的自动指示装置来模拟40个神经元组成的网络的。后来，明斯基又到普林斯顿大学攻读数学博士学位，并以“神经网络和脑模型问题”为题完成博士论文，但是当时的评审委员会并不认为这可以看做是数学。（马文·明斯基）明斯基的这些成果虽然可以被称作人工智能的早期工作，但是鉴于当时的明斯基还是一个青涩的毛头小子，所做的博士论文都不能得到相应的认可，所以影响力有限。接着上场的第二位人物影响力就大很多，那就是计算机科学之父阿兰·图灵，他是被认为最早提出机器智能设想的人。图灵在1950年的时候（也就是明斯基还在读本科的时候）在杂志《思想》（Mind）发表了一篇名为“计算机器与智能”的文章，在文章中，图灵并没有提出什么具体的研究方法，但是文章中提到的好多概念，诸如图灵测试、机器学习、遗传算法和强化学习等，至今都是人工智能领域十分重要的分支。（图灵在1950年的时候在杂志《思想》（Mind）发表的名为“计算机器与智能”的文章）介绍完以上两大人物，接下来标志着人工智能作为一个独立领域而诞生的盛会——达特茅斯研讨会就要粉墨登场了。不过在介绍达特茅斯研讨会之前，我们不得不介绍这第三位重量级的人物，那就是约翰·麦卡锡，因为他正是这次研讨会的发起人。约翰·麦卡锡于1948年获得加州理工学院数学学士学位，1951年获得普林斯顿大学数学博士学位。然后又在那里作为老师工作了两年，接着短暂地为斯坦福大学供职后到了达特茅斯大学，正是这个时期，它组织了达特茅斯研讨会。在这次大会上，麦卡锡的术语人工智能第一次被正式使用，所以麦卡锡也被称作人工智能之父。其实麦卡锡在达特茅斯会议前后，他的主要研究方向正是计算机下棋。（约翰·麦卡锡）下棋程序的关键之一是如何减少计算机需要考虑的棋步。麦卡锡经过艰苦探索，终于发明了著名的 - 搜索法，使搜索能有效进行。 - 搜索法说核心就是，算法在采取最佳招数的情况下允许忽略一些未来不会发生的事情。说的有点抽象，我们来举个十分简单的例子。假如你面前有两个口袋和一个你的敌人，每个口袋放着面值不等的人民币，你来选择口袋，你的敌人决定给你这个口袋里哪张面值的钱。假设你一次只能找一只口袋，在找口袋时一次只能从里面摸出一次。当然你希望面值越大越好，你的敌人自然希望面值越小越好。假如你选择了第一个口袋。现在我们从第一个口袋开始，看每一张面值，并对口袋作出评价。比方说口袋里有一张5元的和一张10元的。如果你挑了这只口袋敌人自然会给你5元的，10元的就是无关紧要的了。现在你开始翻第二个口袋，你每次看一张面值，都会跟你能得到的最好的那张面值(5元)去比较。所以此时你肯定就去找这个口袋里面面值最小的，因为只要最少的要比5元好，那么你就可以挑这个口袋。假如你在第二个口袋摸出一张1元的，那么你就不用考虑这个口袋了，因为如果你挑了这个口袋，敌人肯定会给你1元面值的，那当然要选择最小面值的5元的那个口袋啦。（基于 - 剪枝算法的智能五子棋）虽然有点绕，不过我觉得你应该大概已经理解了这个思路。这就是 - 搜索法，因为这种算法在低于或者超过我们搜索中的 或者 值时就不再搜索，所以这种算法也称为 - 剪枝算法。这种算法至今仍是解决人工智能问题中一种常用的高效方法。当年IBM的深蓝国际象棋程序，因为打败世界冠军卡斯帕罗夫而闻名世界，它靠的正是在30个IBM RS/6000处理器的并行计算机上运行的 - 搜索法。但是需要注意的是，前不久的谷歌AlphaGo，由于棋盘是19x19的，几乎所有的交叉点都可以走子，初始的分支因子为361，这对于常规的 - 搜索来说太令人生畏了，所以别看名字里面带了一个 （Alpha，有可能这个名字是为了纪念麦卡锡的 - 搜索算法），AlphaGo采用的是却是蒙特卡洛搜索树（MCTS），它是一种随机采样的搜索树算法，它解决了在有限时间内要遍历十分宽的树而牺牲深度的问题。后来麦卡锡有从达特茅斯搬到了MIT，在那里他又做出了三项十分重要的贡献。第一个是他定义了高级语言Lisp语言，从此Lisp语言长期以来垄断着人工智能领域的应用，而且人们也有了可以拿来用的得力工具了，但是稀少而且昂贵的计算资源仍是问题。于是麦卡锡和他的同事又发明了分时技术。然后，麦卡锡发表了题为“有常识的程序”的文章，文中他描述了一种系统，取名为意见接收者，任务是使用知识来搜索问题的解，这个假想也被看成是第一个完整的人工智能系统。同年，明斯基也搬到了MIT，他们共同创建了世界上第一座人工智能实验室——MIT AI Lab实验室。尽管后来麦卡锡和明斯基在某些观点上产生了分歧导致他们的合作并没有继续，但这是后话。（MIT AI Lab实验室）二、人工智能的诞生好了，前期的一些大人物介绍完了，让我们一起回到1956年那个意义非凡的夏天。那年，28岁的约翰·麦卡锡，同龄的马文·明斯基，37岁的罗切斯特和40岁的香农一共四个人，提议在麦卡锡工作的达特茅斯学院开一个头脑风暴式的研讨会，他们称之为“达特茅斯夏季人工智能研究会议”。参加会议的除了以上这四位，还有6位年轻的科学家，其中包括40岁的赫伯特·西蒙和28岁的艾伦·纽维尔。在这次研讨会上，大家讨论了当时计算机科学领域尚未解决的问题，包括人工智能、自然语言处理和神经网络等。人工智能这个提法便是这次会议上提出的，上文也有提到。在这个具有历史意义的会议上，明斯基的SNARC，麦卡锡的 - 搜索法，以及西蒙和纽维尔的“逻辑理论家”是会议的三个亮点。前面已经对明斯基的SNARC，麦卡锡的 - 搜索法有所介绍，下面我们再来看一下西蒙和纽维尔的“逻辑理论家”又是什么。西蒙和纽维尔均是来自卡内基梅隆大学（当时还叫卡内基技术学院）的研究者，他们的研究成果在这次盛会上十分引人注意。“逻辑理论家”是西蒙和纽维尔研究出来的一个推理程序，他们声称这个程序可以进行非数值的思考。然后在这次研讨会之后不久，他们的程序就能证明罗素和怀特海德的《数学原理》第二章的大部分定理。但是历史往往对新鲜事物总是反应迟缓，他们将一篇与逻辑理论家合著的论文提交到《符号逻辑杂志》的时候，编辑们拒绝了他们。我们现在来看看这个研讨会的成果，或者说叫意义。遗憾的是，由于历史的局限，这个世界上最聪明的头脑一个月的火花碰撞，并没有产生任何新的突破，他们对自然语言处理的理解，合在一起甚至不如今天一位世界上一流大学的博士毕业生。但是这次研讨会却让人工智能领域主要的人物基本上全部登场。在随后的20年，人工智能领域就被这些人以及他们在MIT、CMU、斯坦福和IBM的学生和同事们支配了。我们看看这10个人，除了香农，当时其实大多数都没什么名气，但是不久之后便一个个开始崭露头角，其中包括四位图灵奖的获得者（麦卡锡，明斯基，西蒙和纽维尔），这四位也是我上文主要介绍的四个人。当然，香农也不用得图灵奖，作为信息论的发明人，他在科学史上的地位也图灵也差不多了。（香农）三、短暂的繁荣与困境从这次会议之后，人工智能迎来了它的一个春天，因为鉴于计算机一直被认为是只能进行数值计算的机器，所以，它稍微做一点看起来有智能的事情，人们都惊讶不已。因为鉴于当时简单的计算机与编程工具，研究者们主要着眼于一些比较特定的问题。例如Herbert Gelernter建造了一个几何定理证明器，可以证明一些学生会感到棘手的几何定理；阿瑟·萨缪尔编写了西洋跳棋程序，水平能达到业余高手；James Slagle的SAINT程序能求解大学一年级的闭合式微积分问题；还有就是结合了多项技术的积木世界问题，它可以使用一只每次能拿起一块积木的机器手按照某种方式调整这些木块。（马文·明斯基与他的积木机器人）虽然这些早期的人工智能项目看起来拥有着巨大的热情和期望，但是由于方法的局限性，人工智能领域的研究者越来越意识到他们所遇到的瓶颈和困难，再加上没有真正令人振奋人心的项目出来而导致资助的停止，人工智能陷入了一个低潮。产生这些现实困难的原因主要有三点。第一点是大部分早期程序对要完成的任务的主题一无所知。就拿机器翻译来说，给程序一个句子，会用的方法只是进行句法分割然后对分割后的成分进行词典翻译，那这样就很容易产生歧义。例如I went to the bank，bank既有银行也有河岸的意思，如果只是单纯的分割加单词翻译，这句话根本没法解释。第二点是问题的难解性。上面我已经提到，早期的人工智能程序主要解决特定的问题，因为特定的问题对象少，复杂度低啊，但是一旦问题的维度上来了，程序立马就捉襟见肘了。第三点就是程序本身的结构就有问题。例如明斯基在1969年证明了两输入的感知机连何时输入是相同的都判断不了。（感知机模型）综上，由于种种困难，再加上资助的减少，人工智能步入了寒冬。这便是人工智能历史的上半段。四、人工智能的重生上个世纪80年代中期，当初于1969年由Bryson和Ho建立的反传学习算法被重新发明，然后统计学在人工智能领域的使用以及良好的效果也让科学界为之一振。于是在新的结构和新的方法下，人工智能又重获新生。首先兴起的是语音识别领域，在这个方面的成就一个重要的原因是隐马尔可夫模型的方法开始主导这个领域。隐马尔可夫模型包含“隐含”和“马尔可夫链”两个概念，马尔可夫链是具有这样一种特性的链条，就是现在的状态只和前一个状态有关，而和再往前的状态没有关系。所以我们遇到这样一个链条的时候，我们可以随机选择一个状态作为初始状态，然后按照上述规则随机选择后续状态。“隐含”的意思则是在这个马尔可夫链上再加一个限制就是，任意时刻的状态我们是不可知的，但是这个状态会输出一个结果，这个结果只和这个状态相关，所以这个也称为独立输出假设。通过这么一解释我们就能看出，隐马尔可夫模型是基于严格的数学理论基础，这允许语音研究者以其他领域中发展数十年的数学成果为依据。其次这个模型的这种随机性可以通过大量的真实语音进行训练，这就保证了性能的鲁棒性。（隐马尔可夫模型简图）在马尔可夫链的基础上还诞生了一个以对不确定性知识进行有效表示和严格推理的形式化方法——贝叶斯网络。贝叶斯网络是一个加权的有向图，是马尔可夫链的拓展。马尔可夫链保证了网络中的每一个状态只跟与其直接相连的状态有关，而跟与它间接相连的状态没有关系，那么这就是贝叶斯网络。在这个网络中，每个节点的概率，都可以用贝叶斯公式来计算，贝叶斯网络因此得名。贝叶斯网络极大地克服了20世纪60年代和70年代概率推理系统的很多问题，它目前主导着不确定推理和专家系统中的人工智能研究。而且这种方法允许根据经验进行学习，并且结合了经典人工智能和神经网络最好的部分。所以极大的推动的人工智能领域走向现在我们正处的这个巅峰时代。（一个简单的贝叶斯网络。雨水影响洒水器是否有动作，且雨水及洒水器二者均可影响草是否湿润）除了这种算法上的革新，还有两个重要推动因素就是互谅网的兴起以及极大数据集的可用性。就像我们用Siri的时候必须联网一样，人工智能系统基于Web的应用变得越来越普遍；我之前在文章《2015年，机器人界发生了哪些神奇疯狂的故事？（下）》中介绍的HitchBOT，它可以拍照、自动识别路人的语言，并将回答显示在屏幕上，这个能力也是通过在网络上搜索相应的答案而实现的。由于我们现在采用的方法已经基本上变为是基于概率的方法，所以我们便需要有大量的数据集对我们的系统进行训练，以完成监督学习。而现在的互联网环境让这种极大数据集的获得变得越来越方便和容易。就如我们所熟知的ImageNet，ImageNet是一个带有标记信息的图片库，里面的图片均已经由人对图片内容进行了标记。它就好比是一个用于测试计算机视觉系统识别能力的“题库”，包含超过百万道“题目”。 题目由图像和对应的单词（80%为名词）组成，考察的方式是计算机视觉系统能否识别图像中的物体并返回正确的单词。ImageNet使用训练题对计算机视觉系统进行“培训”，然后用测试题测试其识别能力。（ImageNet数据集）又如AlphaGo，在DeepMind的主页里，AlphaGo是这样被介绍的：它是一种计算机玩围棋的新方法，这种方法运用了基于深度神经网络的蒙特卡洛搜索树，而这个深度神经网络一方面是通过运用人类专家级围棋棋局进行监督学习来训练，另一方面还通过程序通过电脑自己与自己博弈的增强学习来进行训练，可见AlphaGo的成果也离不开通过学习人类专家级棋谱进行监督学习的这个大量数据集的使用。（DeepMind的主页里AlphaGo的页面）今天这篇文章，我们从人工智能的萌芽一直到今天AlphaGo打败击败欧洲冠军樊麾职业二段这个里程碑式的事件截止，介绍了人工智能能走到今天这个成就的一路的艰难险阻与大风大浪。我相信，随着计算机运算能力以及更加优化的算法，以及大数据集和数据挖掘等技术的帮助，人工智能的路一定会继续高歌猛进。"}
{"content2":"CNCC：China National Computer Congress，中国计算机大会0. 会议计算机视觉（CV）三大顶级会议：ICCV： IEEE International Conference on Computer Vision国际计算机视觉大会，通常每两年召开一次，2005 年 10 月曾经在北京召开计算机视觉领域最高级别的会议，会议的论文集代表了计算机视觉领域最新的发展方向和水平。会议的收录率较低，以 2007 年为例，会议共收到论文1200余篇，接受的论文仅为244篇。会议的论文会被 EI 检索。CVPR：Computer Vision & Pattern RecognitionECCV：Europeon Conference on Computer VisionNIPS：神经信息处理系统大会（Conference and Workshop on Neural Information Processing Systems），是一个关于机器学习和计算神经科学的国际会议。ICML：International Conference on Machine Learning，国际机器学习大会；由国际机器学习学会（IMLS，Society）主办的年度机器学习顶级会议；IJCAI：International Joint Conference On Artificial Intelligence，人工智能联合国际会议EMNLP：Empirical Methods on Natural Language Processing，自然语言处理领域级别很高的会议1. 期刊JMLR：Journal of Machine Learning ResearchJournal of Sensors（Impact Factor 0.712）Journal of Electronic Imaging（Impact Factor*: 0.616；5-Year Impact Factor*: 0.840）IJCAI International Joint Conferences on Artificial Intelligence OrganizationPattern Analysis and Applications（IF：1.104）Cognitive Computation（IF：1.933）Journal of Visual Communication and Image Representation（IF：1.530）Pattern Recognition Letters（IF：1.586）PATTERN RECOGNITIONIEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)（IEEE-TSMC-C，IF：2.171）Computer Vision and Image Understanding（IF：2.134）IEEE Transactions on Image Processing（IF：3.735）2. 顶级期刊INTERNATIONAL JOURNAL OF COMPUTER VISION非常难中，命中率都是“约1%”左右和 PAMI 公认的2大顶级期刊。在上面发表几篇文章，基本就能奠定领域内的学术地位。3. 神经网络NEURAL COMPUTATIONNeurocomputing - Journal - Elsevier（IF：2.392）IEEE Transactions on Neural Networks（IF：2.633）IEEE Transactions on Neural Networks and Learning Systems（TNNLS）（IF：4.854）4. 机器学习Journal of Machine Learning Research（JMLR，IF：3.420）PAMI5. 应用数学类SIAM Journal on Applied Mathematic2015年影响因子: 1.51SIAM JOURNAL ON IMAGING SCIENCES：SIAM 在图像方面的子期刊，在图像处理和模式识别领域仅次于 PAMI，http://muchong.com/bbs/journal.php?view=detail&jid=7541"}
{"content2":"开源生物特征识别库 OpenBROpenBR 是一个用来从照片中识别人脸的工具。还支持推算性别与年龄。 使用方法：$ br -algorithm FaceRecognition -compare me.jpg you.jpg更多OpenBR信息最近更新： OpenBR —— 开源的生物识别工具 发布于 13天前计算机视觉库 OpenCVOpenCV是Intel®开源计算机视觉库。它由一系列 C 函数和少量 C++ 类构成，实现了图像处理和计算机视觉方面的很多通用算法。 OpenCV 拥有包括 300 多个C函数的跨平台的中、高层 API。它不依赖于其它的外部库——尽管也可以使用某些外部库。 OpenCV 对非商业...更多OpenCV信息最近更新： OpenCV 2.4.5 发布，开源计算机视觉库 发布于 2个月前人脸识别 faceservice.cgifaceservice.cgi 是一个用来进行人脸识别的 CGI 程序， 你可以通过上传图像，然后该程序即告诉你人脸的大概坐标位置。faceservice是采用 OpenCV 库进行开发的。更多faceservice.cgi信息Java视觉处理库 JavaCVJavaCV 提供了在计算机视觉领域的封装库，包括：OpenCV、ARToolKitPlus、libdc1394 2.x 、PGR FlyCapture和FFmpeg。此外，该工具可以很容易地使用Java平台的功能。 JavaCV还带有硬件加速的全屏幕图像显示（CanvasFrame），易于在多个内核中执行并行代码（并...更多JavaCV信息视频监控系统 OpenVSSOpenVSS - 开放平台的视频监控系统 - 是一个系统级别的视频监控软件视频分析框架（VAF）的视频分析与检索和播放服务，记录和索引技术。它被设计成插件式的支持多摄像头平台，多分析仪模块（OpenCV的集成），以及多核心架构。更多OpenVSS信息OpenCV的.NET版 OpenCVDotNetOpenCVDotNet 是一个 .NET 对 OpenCV 包的封装。更多OpenCVDotNet信息人脸检测算法 jViolajonesjViolajones是人脸检测算法Viola-Jones的一个Java实现，并能够加载OpenCV XML文件。 示例代码：http://www.oschina.NET/code/snippet_12_2033更多jViolajones信息手势识别 hand-gesture-detection手势识别，用OpenCV实现更多hand-gesture-detection信息人脸检测与跟踪库 asmlibraryActive Shape Model Library (ASMLibrary©) SDK, 用OpenCV开发，用于人脸检测与跟踪。更多asmlibrary信息开放模式识别项目 OpenPRPattern Recognition project（开放模式识别项目），致力于开发出一套包含图像处理、计算机视觉、自然语言处理、模式识别、机器学习和相关领域算法的函数库。更多OpenPR信息运动检测程序 QMotionQMotion 是一个采用 OpenCV 开发的运动检测程序，基于 QT。更多QMotion信息图像特征提取 cvBlobcvBlob 是计算机视觉应用中在二值图像里寻找连通域的库.能够执行连通域分析与特征提取.更多cvBlob信息OpenCV的.Net封装 OpenCVSharpOpenCVSharp 是一个OpenCV的.Net wrapper，应用最新的OpenCV库开发，使用习惯比EmguCV更接近原始的OpenCV，有详细的使用样例供参考。更多OpenCVSharp信息人脸检测识别 mcvai-tracking提供人脸检测、识别与检测特定人脸的功能，示例代码 cvReleaseImage( &gray ); cvReleaseMemStorage(&storage); cvReleaseHaarClassifierCascade(&cascade);...更多mcvai-tracking信息视频捕获 API VideoManVideoMan 提供一组视频捕获 API 。支持多种视频流同时输入（视频传输线、USB摄像头和视频文件等）。能利用 OpenGL 对输入进行处理，方便的与 OpenCV，CUDA 等集成开发计算机视觉系统。更多VideoMan信息基于QT的计算机视觉库 QVision基于 QT 的面向对象的多平台计算机视觉库。可以方便的创建图形化应用程序，算法库主要从 OpenCV，GSL，CGAL，IPP，Octave 等高性能库借鉴而来。更多QVision信息开源视线跟踪软件 ITU Gaze Tracker哥本哈根大学开源视线跟踪软件 The ITU Gaze Tracker is an open-source eye tracker that aims to provide a low-cost alternative to commercial gaze tracking systems and to make this technology more accessible. It is developed by the Gaze Grou...更多ITU Gaze Tracker信息图像处理和计算机视觉常用算法库 LTI-LibLTI-Lib 是一个包含图像处理和计算机视觉常用算法和数据结构的面向对象库，提供 Windows 下的 VC 版本和 Linux 下的 gcc 版本，主要包含以下几方面内容： 1、线性代数 2、聚类分析 3、图像处理 4、可视化和绘图工具更多LTI-Lib信息实时图像/视频处理滤波开发包 GShowGShow is a real-time image/video processing filter development kit. It successfully integrates DirectX11 with DirectShow framework. So it has the following features: GShow 是实时 图像/视频 处理滤波开发包，集成DiretX11。...更多GShow信息C++计算机视觉库 Integrating Vision ToolkitIntegrating Vision Toolkit (IVT) 是一个强大而迅速的C++计算机视觉库，拥有易用的接口和面向对象的架构，并且含有自己的一套跨平台GUI组件，另外可以选择集成OpenCVOpenCV的Python封装 pyopencvOpenCV的Python封装，主要特性包括： 提供与OpenCV 2.x中最新的C++接口极为相似的Python接口，并且包括C++中不包括的C接口 提供对OpenCV 2.x中所有主要部件的绑定：CxCORE (almost complete), CxFLANN (complete), Cv (complete), CvAux (C++ part almost...更多pyopencv信息模式识别和视觉库 RAVLRecognition And Vision Library (RAVL) 是一个通用 C++ 库，包含计算机视觉、模式识别等模块。更多RAVL信息OpenSURF利用OpenCV和C++编写的SURF算法，作者Christopher Evans是首个利用OpenCV和C++结合的方法实现SURF算法。更多OpenSURF信息人脸识别库 rpflexrpflex 是一个 Flex 开发的库，用来识别照片中的人脸、眼镜和脖子。更多rpflex信息OpenCV优化 opencv-dsp-acceleration优化了OpenCV库在DSP上的速度。更多opencv-dsp-acceleration信息Java 计算机视觉库 BoofCVBoofCV 是一个 Java 的全新实时的计算机视觉库，BoofCV 易于使用而且具有非常高的性能。它提供了一系列从低层次的图像处理、小波去噪功能以及更高层次的三维几何视野。使用 BSD 许可证可在商业应用中使用。 这里有篇英文文章用来介绍 BoofCV 的使用。...更多BoofCV信息计算机视觉库 SimpleCVSimpleCV 将很多强大的开源计算机视觉库包含在一个便捷的Python包中。使用SimpleCV，你可以在统一的框架下使用高级算法，例如特征检测、滤波和模式识别。使用者不用清楚一些细节，比如图像比特深度、文件格式、颜色空间、缓冲区管理、特征值还有矩阵和图像...更多SimpleCV信息3D视觉库 fvision2010基于OpenCV构建的图像处理和3D视觉库。 示例代码： ImageSequenceReaderFactory factory; ImageSequenceReader* reader = factory.pathRegex(\"c:/a/im_%03d.jpg\", 0, 20); //ImageSequenceReader* reader = factory.avi(\"a.avi\"); if (reader == NULL) { ...更多fvision2010信息视觉快速开发平台 qcv计算机视觉快速开发平台，提供测试框架，使开发者可以专注于算法研究。更多qcv信息计算机视觉算法 OpenVIDIAOpenVIDIA projects implement computer vision algorithms running on on graphics hardware such as single or multiple graphics processing units(GPUs) using OpenGL, Cg and CUDA-C. Some samples will soon support OpenCL and Direct Compute API'...更多OpenVIDIA信息C++计算机视觉库 ICLICL (Image Component Library) 是一种新型的C + +计算机视觉库，由比勒费尔德大学神经信息学组和CITEC开发。它兼顾了性能和用户友好性。 ICL提供了一个易于使用的类和函数的集合，可以开发复杂的计算机视觉应用。 在不到15行的C + +代码（见例子）可以写成...更多ICL信息Matlab计算机视觉包 mVisionMatlab 的计算机视觉包，包含用于观察结果的 GUI 组件，貌似也停止开发了，拿来做学习用挺不错的。更多mVision信息Lua视觉开发库 libecvECV 是 lua 的计算机视觉开发库(目前只提供linux支持)更多libecv信息OpenCV的扩展库 ImageNetsImageNets 是对OpenCV 的扩展，提供对机器人视觉算法方面友好的支持，使用Nokia的QT编写界面。更多ImageNets信息图像捕获 libv4l2cam对函数库v412的封装，从网络摄像头等硬件获得图像数据，支持YUYV裸数据输出和BGR24的OpenCV  IplImage输出更多libv4l2cam信息高斯模型点集配准算法 gmmreg实现了基于混合高斯模型的点集配准算法，该算法描述在论文： A Robust Algorithm for Point Set Registration Using Mixture of Gaussians, Bing Jian and Baba C. Vemuri. ，实现了C++/Matlab/Python接口...更多gmmreg信息Scilab的计算机视觉库 SIPSIP 是 Scilab（一种免费的类Matlab编程环境）的图像处理和计算机视觉库。SIP 可以读写 JPEG/PNG/BMP 格式的图片。具备图像滤波、分割、边缘检测、形态学处理和形状分析等功能。更多SIP信息计算机视觉和机器人技术的工具包 EGTThe Epipolar Geometry Toolbox (EGT) is a toolbox designed for Matlab (by Mathworks Inc.). EGT provides a wide set of functions to approach computer vision and robotics problems with single and multiple views, and with different vision se...更多EGT信息计算机视觉库 BazARBazAR 是基于特征点检测和匹配的计算机视觉库。 它能够快速检测和匹配图像中的已知物体，并且能够用于增强现实，它是计算机视觉研究的先进成果。更多BazAR信息计算机视觉库 VLFeat一个开源的计算机视觉库，实现了 SIFT,MSER, k-means, hierarchical k-means, agglomerative information bottleneck, quick shift等算法。由C语言编写,提供MATLAB接口，文档详细。支持跨平台。...更多VLFeat信息STAIR Vision LibrarySTAIR Vision Library (SVL) 最初是为支持斯坦福智能机器人设计的，提供对计算机视觉、机器学习和概率统计模型的支持。更多STAIR Vision Library信息Scilab Image Processing ToolboxSIP 提供了图像处理、模式识别以及计算机视觉处理。 SIP is able to read/write images in almost 90 major formats, including JPEG, PNG, BMP, GIF, FITS, and TIFF. It includes routines for filtering, segmentation, edge detection, morphology, cu...更多Scilab Image Processing Toolbox信息3D计算机视觉库 openvis3d这个项目的目的是提供一个高效的3D计算机视觉库，用于图像和视频处理。它包括深度立体匹配、光流（运动）估计、遮挡检测和运动平台估计更多openvis3d信息libvideogfx视频处理、计算机视觉和计算机图形学的快速开发库。更多libvideogfx信息go-opencvGo-OpenCV 是 Go 语言版的 OpenCV 封装。更多go-opencv信息JavaScript图形绘制库 Toxiclibs.jsToxiclibs.js 是一个开源的计算机图形设计库，无需外部依赖，使用 <canvas> 元素进行图形绘制。更多Toxiclibs.js信息OpenCL 封装库 CLOGSCLOGS 是 OpenCL C++ API 的高级封装库，其设计目的是集成其他 OpenCL 代码，包括同步 OpenCL 事件，当前支持两个操作：基数排序和独立扫描。更多CLOGS信息最近更新： CLOGS 1.2.0 发布，OpenCL 的封装库 发布于 2个月前openvgrOpenVGR 包含以下几个实时处理模块 (基于 OpenRTM-1.0): 立体相机采集 (对于 IEEE 1394b 相机), 立体图像浏览器, 3-D 点云重建 (使用 OpenCV),  基于边缘的 3-D 物体检测 包含以下几个命令行工具: 模型建立, 多相机标定....更多openvgr信息sparse-stereo-vision使用 OpenCV 函数, 这个项目能从成对的立体图像中重建场景。更多sparse-stereo-vision信息PIV图形软件包 FluereFluere是粒子图像测速（PIV）的图形软件包。 Fluere是高度优化的并行处理，并在多个平台上运行。该项目的目标是提供高质量的测速软件，采用PIV技术处理的最新进展的研究人员和教育工作者，而所使用的算法的完整的知识。更多Fluere信息stereoviewstereoview 是一个立体可视化和标定工具更多stereoview信息"}
{"content2":"在本篇文章中，我将对机器学习做个概要的介绍。本文的目的是能让即便完全不了解机器学习的人也能了解机器学习，并且上手相关的实践。这篇文档也算是EasyPR开发的番外篇，从这里开始，必须对机器学习了解才能进一步介绍EasyPR的内核。当然，本文也面对一般读者，不会对阅读有相关的前提要求。在进入正题前，我想读者心中可能会有一个疑惑：机器学习有什么重要性，以至于要阅读完这篇非常长的文章呢？我并不直接回答这个问题前。相反，我想请大家看两张图，下图是图一：这幅图上上的三人是当今机器学习界的执牛耳者。中间的是Geoffrey Hinton, 加拿大多伦多大学的教授，如今被聘为“Google大脑”的负责人。右边的是Yann LeCun, 纽约大学教授，如今是Facebook人工智能实验室的主任。而左边的大家都很熟悉，Andrew Ng，中文名吴恩达，斯坦福大学副教授，如今也是“百度大脑”的负责人与百度首席科学家。这三位都是目前业界炙手可热的大牛，被互联网界大鳄求贤若渴的聘请，足见他们的重要性。而他们的研究方向，则全部都是机器学习的子类--深度学习。下图是图二： 语音助手产品这幅图上描述的是什么？Windows Phone上的语音助手Cortana，名字来源于《光环》中士官长的助手。相比其他竞争对手，微软很迟才推出这个服务。Cortana背后的核心技术是什么，为什么它能够听懂人的语音？事实上，这个技术正是机器学习。机器学习是所有语音助手产品(包括Apple的siri与Google的Now)能够跟人交互的关键技术。通过上面两图，我相信大家可以看出机器学习似乎是一个很重要的，有很多未知特性的技术。学习它似乎是一件有趣的任务。实际上，学习机器学习不仅可以帮助我们了解互联网界最新的趋势，同时也可以知道伴随我们的便利服务的实现技术。机器学习是什么，为什么它能有这么大的魔力，这些问题正是本文要回答的。同时，本文叫做“从机器学习谈起”，因此会以漫谈的形式介绍跟机器学习相关的所有内容，包括学科(如数据挖掘、计算机视觉等)，算法(神经网络，svm)等等。本文的主要目录如下：1.一个故事说明什么是机器学习2.机器学习的定义3.机器学习的范围4.机器学习的方法5.机器学习的应用--大数据6.机器学习的子类--深度学习7.机器学习的父类--人工智能8.机器学习的思考--计算机的潜意识9.总结10.后记1.一个故事说明什么是机器学习机器学习这个词是让人疑惑的，首先它是英文名称Machine Learning(简称ML)的直译，在计算界Machine一般指计算机。这个名字使用了拟人的手法，说明了这门技术是让机器“学习”的技术。但是计算机是死的，怎么可能像人类一样“学习”呢？传统上如果我们想让计算机工作，我们给它一串指令，然后它遵照这个指令一步步执行下去。有因有果，非常明确。但这样的方式在机器学习中行不通。机器学习根本不接受你输入的指令，相反，它接受你输入的数据! 也就是说，机器学习是一种让计算机利用数据而不是指令来进行各种工作的方法。这听起来非常不可思议，但结果上却是非常可行的。“统计”思想将在你学习“机器学习”相关理念时无时无刻不伴随，相关而不是因果的概念将是支撑机器学习能够工作的核心概念。你会颠覆对你以前所有程序中建立的因果无处不在的根本理念。下面我通过一个故事来简单地阐明什么是机器学习。这个故事比较适合用在知乎上作为一个概念的阐明。在这里，这个故事没有展开，但相关内容与核心是存在的。如果你想简单的了解一下什么是机器学习，那么看完这个故事就足够了。如果你想了解机器学习的更多知识以及与它关联紧密的当代技术，那么请你继续往下看，后面有更多的丰富的内容。这个例子来源于我真实的生活经验，我在思考这个问题的时候突然发现它的过程可以被扩充化为一个完整的机器学习的过程，因此我决定使用这个例子作为所有介绍的开始。这个故事称为“等人问题”。我相信大家都有跟别人相约，然后等人的经历。现实中不是每个人都那么守时的，于是当你碰到一些爱迟到的人，你的时间不可避免的要浪费。我就碰到过这样的一个例子。对我的一个朋友小Y而言，他就不是那么守时，最常见的表现是他经常迟到。当有一次我跟他约好3点钟在某个麦当劳见面时，在我出门的那一刻我突然想到一个问题：我现在出发合适么？我会不会又到了地点后，花上30分钟去等他？我决定采取一个策略解决这个问题。要想解决这个问题，有好几种方法。第一种方法是采用知识：我搜寻能够解决这个问题的知识。但很遗憾，没有人会把如何等人这个问题作为知识传授，因此我不可能找到已有的知识能够解决这个问题。第二种方法是问他人：我去询问他人获得解决这个问题的能力。但是同样的，这个问题没有人能够解答，因为可能没人碰上跟我一样的情况。第三种方法是准则法：我问自己的内心，我有否设立过什么准则去面对这个问题？例如，无论别人如何，我都会守时到达。但我不是个死板的人，我没有设立过这样的规则。事实上，我相信有种方法比以上三种都合适。我把过往跟小Y相约的经历在脑海中重现一下，看看跟他相约的次数中，迟到占了多大的比例。而我利用这来预测他这次迟到的可能性。如果这个值超出了我心里的某个界限，那我选择等一会再出发。假设我跟小Y约过5次，他迟到的次数是1次，那么他按时到的比例为80%，我心中的阈值为70%，我认为这次小Y应该不会迟到，因此我按时出门。如果小Y在5次迟到的次数中占了4次，也就是他按时到达的比例为20%，由于这个值低于我的阈值，因此我选择推迟出门的时间。这个方法从它的利用层面来看，又称为经验法。在经验法的思考过程中，我事实上利用了以往所有相约的数据。因此也可以称之为依据数据做的判断。依据数据所做的判断跟机器学习的思想根本上是一致的。刚才的思考过程我只考虑“频次”这种属性。在真实的机器学习中，这可能都不算是一个应用。一般的机器学习模型至少考虑两个量：一个是因变量，也就是我们希望预测的结果，在这个例子里就是小Y迟到与否的判断。另一个是自变量，也就是用来预测小Y是否迟到的量。假设我把时间作为自变量，譬如我发现小Y所有迟到的日子基本都是星期五，而在非星期五情况下他基本不迟到。于是我可以建立一个模型，来模拟小Y迟到与否跟日子是否是星期五的概率。见下图：这样的图就是一个最简单的机器学习模型，称之为决策树。当我们考虑的自变量只有一个时，情况较为简单。如果把我们的自变量再增加一个。例如小Y迟到的部分情况时是在他开车过来的时候(你可以理解为他开车水平较臭，或者路较堵)。于是我可以关联考虑这些信息。建立一个更复杂的模型，这个模型包含两个自变量与一个因变量。再更复杂一点，小Y的迟到跟天气也有一定的原因，例如下雨的时候，这时候我需要考虑三个自变量。如果我希望能够预测小Y迟到的具体时间，我可以把他每次迟到的时间跟雨量的大小以及前面考虑的自变量统一建立一个模型。于是我的模型可以预测值，例如他大概会迟到几分钟。这样可以帮助我更好的规划我出门的时间。在这样的情况下，决策树就无法很好地支撑了，因为决策树只能预测离散值。我们可以用节2所介绍的线型回归方法建立这个模型。如果我把这些建立模型的过程交给电脑。比如把所有的自变量和因变量输入，然后让计算机帮我生成一个模型，同时让计算机根据我当前的情况，给出我是否需要迟出门，需要迟几分钟的建议。那么计算机执行这些辅助决策的过程就是机器学习的过程。机器学习方法是计算机利用已有的数据(经验)，得出了某种模型(迟到的规律)，并利用此模型预测未来(是否迟到)的一种方法。通过上面的分析，可以看出机器学习与人类思考的经验过程是类似的，不过它能考虑更多的情况，执行更加复杂的计算。事实上，机器学习的一个主要目的就是把人类思考归纳经验的过程转化为计算机通过对数据的处理计算得出模型的过程。经过计算机得出的模型能够以近似于人的方式解决很多灵活复杂的问题。下面，我会开始对机器学习的正式介绍，包括定义、范围，方法、应用等等，都有所包含。2.机器学习的定义从广义上来说，机器学习是一种能够赋予机器学习的能力以此让它完成直接编程无法完成的功能的方法。但从实践的意义上来说，机器学习是一种通过利用数据，训练出模型，然后使用模型预测的一种方法。让我们具体看一个例子。拿国民话题的房子来说。现在我手里有一栋房子需要售卖，我应该给它标上多大的价格？房子的面积是100平方米，价格是100万，120万，还是140万？很显然，我希望获得房价与面积的某种规律。那么我该如何获得这个规律？用报纸上的房价平均数据么？还是参考别人面积相似的？无论哪种，似乎都并不是太靠谱。我现在希望获得一个合理的，并且能够最大程度的反映面积与房价关系的规律。于是我调查了周边与我房型类似的一些房子，获得一组数据。这组数据中包含了大大小小房子的面积与价格，如果我能从这组数据中找出面积与价格的规律，那么我就可以得出房子的价格。对规律的寻找很简单，拟合出一条直线，让它“穿过”所有的点，并且与各个点的距离尽可能的小。通过这条直线，我获得了一个能够最佳反映房价与面积规律的规律。这条直线同时也是一个下式所表明的函数：房价 = 面积 * a + b上述中的a、b都是直线的参数。获得这些参数以后，我就可以计算出房子的价格。假设a = 0.75,b = 50，则房价 = 100 * 0.75 + 50 = 125万。这个结果与我前面所列的100万，120万，140万都不一样。由于这条直线综合考虑了大部分的情况，因此从“统计”意义上来说，这是一个最合理的预测。在求解过程中透露出了两个信息：1.房价模型是根据拟合的函数类型决定的。如果是直线，那么拟合出的就是直线方程。如果是其他类型的线，例如抛物线，那么拟合出的就是抛物线方程。机器学习有众多算法，一些强力算法可以拟合出复杂的非线性模型，用来反映一些不是直线所能表达的情况。2.如果我的数据越多，我的模型就越能够考虑到越多的情况，由此对于新情况的预测效果可能就越好。这是机器学习界“数据为王”思想的一个体现。一般来说(不是绝对)，数据越多，最后机器学习生成的模型预测的效果越好。通过我拟合直线的过程，我们可以对机器学习过程做一个完整的回顾。首先，我们需要在计算机中存储历史的数据。接着，我们将这些 数据通过机器学习算法进行处理，这个过程在机器学习中叫做“训练”，处理的结果可以被我们用来对新的数据进行预测，这个结果一般称之为“模型”。对新数据 的预测过程在机器学习中叫做“预测”。“训练”与“预测”是机器学习的两个过程，“模型”则是过程的中间输出结果，“训练”产生“模型”，“模型”指导 “预测”。让我们把机器学习的过程与人类对历史经验归纳的过程做个比对。 机器学习与人类思考的类比人类在成长、生活过程中积累了很多的历史与经验。人类定期地对这些经验进行“归纳”，获得了生活的“规律”。当人类遇到未知的问题或者需要对未来进行“推测”的时候，人类使用这些“规律”，对未知问题与未来进行“推测”，从而指导自己的生活和工作。机器学习中的“训练”与“预测”过程可以对应到人类的“归纳”和“推测”过程。通过这样的对应，我们可以发现，机器学习的思想并不复杂，仅仅是对人类在生活中学习成长的一个模拟。由于机器学习不是基于编程形成的结果，因此它的处理过程不是因果的逻辑，而是通过归纳思想得出的相关性结论。这也可以联想到人类为什么要学习历史，历史实际上是人类过往经验的总结。有句话说得很好，“历史往往不一样，但历史总是惊人的相似”。通过学习历史，我们从历史中归纳出人生与国家的规律，从而指导我们的下一步工作，这是具有莫大价值的。当代一些人忽视了历史的本来价值，而是把其作为一种宣扬功绩的手段，这其实是对历史真实价值的一种误用。3.机器学习的范围上文虽然说明了机器学习是什么，但是并没有给出机器学习的范围。其实，机器学习跟模式识别，统计学习，数据挖掘，计算机视觉，语音识别，自然语言处理等领域有着很深的联系。从范围上来说，机器学习跟模式识别，统计学习，数据挖掘是类似的，同时，机器学习与其他领域的处理技术的结合，形成了计算机视觉、语音识别、自然语言处理等交叉学科。因此，一般说数据挖掘时，可以等同于说机器学习。同时，我们平常所说的机器学习应用，应该是通用的，不仅仅局限在结构化数据，还有图像，音频等应用。在这节对机器学习这些相关领域的介绍有助于我们理清机器学习的应用场景与研究范围，更好的理解后面的算法与应用层次。下图是机器学习所牵扯的一些相关范围的学科与研究领域。模式识别模式识别=机器学习。两者的主要区别在于前者是从工业界发展起来的概念，后者则主要源自计算机学科。在著名的《Pattern Recognition And Machine Learning》这本书中，Christopher M. Bishop在开头是这样说的“模式识别源自工业界，而机器学习来自于计算机学科。不过，它们中的活动可以被视为同一个领域的两个方面，同时在过去的10年间，它们都有了长足的发展”。数据挖掘数据挖掘=机器学习+数据库。这几年数据挖掘的概念实在是太耳熟能详。几乎等同于炒作。但凡说数据挖掘都会吹嘘数据挖掘如何如何，例如从数据中挖出金子，以及将废弃的数据转化为价值等等。但是，我尽管可能会挖出金子，但我也可能挖的是“石头”啊。这个说法的意思是，数据挖掘仅仅是一种思考方式，告诉我们应该尝试从数据中挖掘出知识，但不是每个数据都能挖掘出金子的，所以不要神话它。一个系统绝对不会因为上了一个数据挖掘模块就变得无所不能(这是IBM最喜欢吹嘘的)，恰恰相反，一个拥有数据挖掘思维的人员才是关键，而且他还必须对数据有深刻的认识，这样才可能从数据中导出模式指引业务的改善。大部分数据挖掘中的算法是机器学习的算法在数据库中的优化。统计学习统计学习近似等于机器学习。统计学习是个与机器学习高度重叠的学科。因为机器学习中的大多数方法来自统计学，甚至可以认为，统计学的发展促进机器学习的繁荣昌盛。例如著名的支持向量机算法，就是源自统计学科。但是在某种程度上两者是有分别的，这个分别在于：统计学习者重点关注的是统计模型的发展与优化，偏数学，而机器学习者更关注的是能够解决问题，偏实践，因此机器学习研究者会重点研究学习算法在计算机上执行的效率与准确性的提升。计算机视觉计算机视觉=图像处理+机器学习。图像处理技术用于将图像处理为适合进入机器学习模型中的输入，机器学习则负责从图像中识别出相关的模式。计算机视觉相关的应用非常的多，例如百度识图、手写字符识别、车牌识别等等应用。这个领域是应用前景非常火热的，同时也是研究的热门方向。随着机器学习的新领域深度学习的发展，大大促进了计算机图像识别的效果，因此未来计算机视觉界的发展前景不可估量。语音识别语音识别=语音处理+机器学习。语音识别就是音频处理技术与机器学习的结合。语音识别技术一般不会单独使用，一般会结合自然语言处理的相关技术。目前的相关应用有苹果的语音助手siri等。自然语言处理自然语言处理=文本处理+机器学习。自然语言处理技术主要是让机器理解人类的语言的一门领域。在自然语言处理技术中，大量使用了编译原理相关的技术，例如词法分析，语法分析等等，除此之外，在理解这个层面，则使用了语义理解，机器学习等技术。作为唯一由人类自身创造的符号，自然语言处理一直是机器学习界不断研究的方向。按照百度机器学习专家余凯的说法“听与看，说白了就是阿猫和阿狗都会的，而只有语言才是人类独有的”。如何利用机器学习技术进行自然语言的的深度理解，一直是工业和学术界关注的焦点。可以看出机器学习在众多领域的外延和应用。机器学习技术的发展促使了很多智能领域的进步，改善着我们的生活。4.机器学习的方法通过上节的介绍我们知晓了机器学习的大致范围，那么机器学习里面究竟有多少经典的算法呢？在这个部分我会简要介绍一下机器学习中的经典代表方法。这部分介绍的重点是这些方法内涵的思想，数学与实践细节不会在这讨论。1、回归算法在大部分机器学习课程中，回归算法都是介绍的第一个算法。原因有两个：一.回归算法比较简单，介绍它可以让人平滑地从统计学迁移到机器学习中。二.回归算法是后面若干强大算法的基石，如果不理解回归算法，无法学习那些强大的算法。回归算法有两个重要的子类：即线性回归和逻辑回归。线性回归就是我们前面说过的房价求解问题。如何拟合出一条直线最佳匹配我所有的数据？一般使用“最小二乘法”来求解。“最小二乘法”的思想是这样的，假设我们拟合出的直线代表数据的真实值，而观测到的数据代表拥有误差的值。为了尽可能减小误差的影响，需要求解一条直线使所有误差的平方和最小。最小二乘法将最优问题转化为求函数极值问题。函数极值在数学上我们一般会采用求导数为0的方法。但这种做法并不适合计算机，可能求解不出来，也可能计算量太大。计算机科学界专门有一个学科叫“数值计算”，专门用来提升计算机进行各类计算时的准确性和效率问题。例如，著名的“梯度下降”以及“牛顿法”就是数值计算中的经典算法，也非常适合来处理求解函数极值的问题。梯度下降法是解决回归模型中最简单且有效的方法之一。从严格意义上来说，由于后文中的神经网络和推荐算法中都有线性回归的因子，因此梯度下降法在后面的算法实现中也有应用。逻辑回归是一种与线性回归非常类似的算法，但是，从本质上讲，线型回归处理的问题类型与逻辑回归不一致。线性回归处理的是数值问题，也就是最后预测出的结果是数字，例如房价。而逻辑回归属于分类算法，也就是说，逻辑回归预测结果是离散的分类，例如判断这封邮件是否是垃圾邮件，以及用户是否会点击此广告等等。实现方面的话，逻辑回归只是对对线性回归的计算结果加上了一个Sigmoid函数，将数值结果转化为了0到1之间的概率(Sigmoid函数的图像一般来说并不直观，你只需要理解对数值越大，函数越逼近1，数值越小，函数越逼近0)，接着我们根据这个概率可以做预测，例如概率大于0.5，则这封邮件就是垃圾邮件，或者肿瘤是否是恶性的等等。从直观上来说，逻辑回归是画出了一条分类线，见下图。假设我们有一组肿瘤患者的数据，这些患者的肿瘤中有些是良性的(图中的蓝色点)，有些是恶性的(图中的红色点)。这里肿瘤的红蓝色可以被称作数据的“标签”。同时每个数据包括两个“特征”：患者的年龄与肿瘤的大小。我们将这两个特征与标签映射到这个二维空间上，形成了我上图的数据。当我有一个绿色的点时，我该判断这个肿瘤是恶性的还是良性的呢？根据红蓝点我们训练出了一个逻辑回归模型，也就是图中的分类线。这时，根据绿点出现在分类线的左侧，因此我们判断它的标签应该是红色，也就是说属于恶性肿瘤。逻辑回归算法划出的分类线基本都是线性的(也有划出非线性分类线的逻辑回归，不过那样的模型在处理数据量较大的时候效率会很低)，这意味着当两类之间的界线不是线性时，逻辑回归的表达能力就不足。下面的两个算法是机器学习界最强大且重要的算法，都可以拟合出非线性的分类线。2、神经网络神经网络(也称之为人工神经网络，ANN)算法是80年代机器学习界非常流行的算法，不过在90年代中途衰落。现在，携着“深度学习”之势，神经网络重装归来，重新成为最强大的机器学习算法之一。神经网络的诞生起源于对大脑工作机理的研究。早期生物界学者们使用神经网络来模拟大脑。机器学习的学者们使用神经网络进行机器学习的实验，发现在视觉与语音的识别上效果都相当好。在BP算法(加速神经网络训练过程的数值算法)诞生以后，神经网络的发展进入了一个热潮。BP算法的发明人之一是前面介绍的机器学习大牛Geoffrey Hinton(中的中间者)。具体说来，神经网络的学习机理是什么？简单来说，就是分解与整合。在著名的Hubel-Wiesel试验中，学者们研究猫的视觉分析机理是这样的。Hubel-Wiesel试验与大脑视觉机理比方说，一个正方形，分解为四个折线进入视觉处理的下一层中。四个神经元分别处理一个折线。每个折线再继续被分解为两条直线，每条直线再被分解为黑白两个面。于是，一个复杂的图像变成了大量的细节进入神经元，神经元处理以后再进行整合，最后得出了看到的是正方形的结论。这就是大脑视觉识别的机理，也是神经网络工作的机理。让我们看一个简单的神经网络的逻辑架构。在这个网络中，分成输入层，隐藏层，和输出层。输入层负责接收信号，隐藏层负责对数据的分解与处理，最后的结果被整合到输出层。每层中的一个圆代表一个处理单元，可以认为是模拟了一个神经元，若干个处理单元组成了一个层，若干个层再组成了一个网络，也就是\"神经网络\"。在神经网络中，每个处理单元事实上就是一个逻辑回归模型，逻辑回归模型接收上层的输入，把模型的预测结果作为输出传输到下一个层次。通过这样的过程，神经网络可以完成非常复杂的非线性分类。下图会演示神经网络在图像识别领域的一个著名应用，这个程序叫做LeNet，是一个基于多个隐层构建的神经网络。通过LeNet可以识别多种手写数字，并且达到很高的识别精度与拥有较好的鲁棒性。LeNet的效果展示右下方的方形中显示的是输入计算机的图像，方形上方的红色字样“answer”后面显示的是计算机的输出。左边的三条竖直的图像列显示的是神经网络中三个隐藏层的输出，可以看出，随着层次的不断深入，越深的层次处理的细节越低，例如层3基本处理的都已经是线的细节了。LeNet的发明人就是前文介绍过的机器学习的大牛Yann LeCun(右者)。进入90年代，神经网络的发展进入了一个瓶颈期。其主要原因是尽管有BP算法的加速，神经网络的训练过程仍然很困难。因此90年代后期支持向量机(SVM)算法取代了神经网络的地位。3、SVM（支持向量机）支持向量机算法是诞生于统计学习界，同时在机器学习界大放光彩的经典算法。支持向量机算法从某种意义上来说是逻辑回归算法的强化：通过给予逻辑回归算法更严格的优化条件，支持向量机算法可以获得比逻辑回归更好的分类界线。但是如果没有某类函数技术，则支持向量机算法最多算是一种更好的线性分类技术。但是，通过跟高斯“核”的结合，支持向量机可以表达出非常复杂的分类界线，从而达成很好的的分类效果。“核”事实上就是一种特殊的函数，最典型的特征就是可以将低维的空间映射到高维的空间。例如下图所示：我们如何在二维平面划分出一个圆形的分类界线？在二维平面可能会很困难，但是通过“核”可以将二维空间映射到三维空间，然后使用一个线性平面就可以达成类似效果。也就是说，二维平面划分出的非线性分类界线可以等价于三维平面的线性分类界线。于是，我们可以通过在三维空间中进行简单的线性划分就可以达到在二维平面中的非线性划分效果。支持向量机是一种数学成分很浓的机器学习算法（相对的，神经网络则有生物科学成分）。在算法的核心步骤中，有一步证明，即将数据从低维映射到高维不会带来最后计算复杂性的提升。于是，通过支持向量机算法，既可以保持计算效率，又可以获得非常好的分类效果。因此支持向量机在90年代后期一直占据着机器学习中最核心的地位，基本取代了神经网络算法。直到现在神经网络借着深度学习重新兴起，两者之间才又发生了微妙的平衡转变。4、聚类算法前面的算法中的一个显著特征就是我的训练数据中包含了标签，训练出的模型可以对其他未知数据预测标签。在下面的算法中，训练数据都是不含标签的，而算法的目的则是通过训练，推测出这些数据的标签。这类算法有一个统称，即无监督算法(前面有标签的数据的算法则是有监督算法)。无监督算法中最典型的代表就是聚类算法。让我们还是拿一个二维的数据来说，某一个数据包含两个特征。我希望通过聚类算法，给他们中不同的种类打上标签，我该怎么做呢？简单来说，聚类算法就是计算种群中的距离，根据距离的远近将数据划分为多个族群。聚类算法中最典型的代表就是K-Means算法。5、降维算法降维算法也是一种无监督学习算法，其主要特征是将数据从高维降低到低维层次。在这里，维度其实表示的是数据的特征量的大小，例如，房价包含房子的长、宽、面积与房间数量四个特征，也就是维度为4维的数据。可以看出来，长与宽事实上与面积表示的信息重叠了，例如面积=长 × 宽。通过降维算法我们就可以去除冗余信息，将特征减少为面积与房间数量两个特征，即从4维的数据压缩到2维。于是我们将数据从高维降低到低维，不仅利于表示，同时在计算上也能带来加速。刚才说的降维过程中减少的维度属于肉眼可视的层次，同时压缩也不会带来信息的损失(因为信息冗余了)。如果肉眼不可视，或者没有冗余的特征，降维算法也能工作，不过这样会带来一些信息的损失。但是，降维算法可以从数学上证明，从高维压缩到的低维中最大程度地保留了数据的信息。因此，使用降维算法仍然有很多的好处。降维算法的主要作用是压缩数据与提升机器学习其他算法的效率。通过降维算法，可以将具有几千个特征的数据压缩至若干个特征。另外，降维算法的另一个好处是数据的可视化，例如将5维的数据压缩至2维，然后可以用二维平面来可视。降维算法的主要代表是PCA算法(即主成分分析算法)。6、推荐算法推荐算法是目前业界非常火的一种算法，在电商界，如亚马逊，天猫，京东等得到了广泛的运用。推荐算法的主要特征就是可以自动向用户推荐他们最感兴趣的东西，从而增加购买率，提升效益。推荐算法有两个主要的类别：一类是基于物品内容的推荐，是将与用户购买的内容近似的物品推荐给用户，这样的前提是每个物品都得有若干个标签，因此才可以找出与用户购买物品类似的物品，这样推荐的好处是关联程度较大，但是由于每个物品都需要贴标签，因此工作量较大。另一类是基于用户相似度的推荐，则是将与目标用户兴趣相同的其他用户购买的东西推荐给目标用户，例如小A历史上买了物品B和C，经过算法分析，发现另一个与小A近似的用户小D购买了物品E，于是将物品E推荐给小A。两类推荐都有各自的优缺点，在一般的电商应用中，一般是两类混合使用。推荐算法中最有名的算法就是协同过滤算法。7、其他除了以上算法之外，机器学习界还有其他的如高斯判别，朴素贝叶斯，决策树等等算法。但是上面列的六个算法是使用最多，影响最广，种类最全的典型。机器学习界的一个特色就是算法众多，发展百花齐放。下面做一个总结，按照训练的数据有无标签，可以将上面算法分为监督学习算法和无监督学习算法，但推荐算法较为特殊，既不属于监督学习，也不属于非监督学习，是单独的一类。监督学习算法：线性回归，逻辑回归，神经网络，SVM无监督学习算法：聚类算法，降维算法特殊算法：推荐算法除了这些算法以外，有一些算法的名字在机器学习领域中也经常出现。但他们本身并不算是一个机器学习算法，而是为了解决某个子问题而诞生的。你可以理解他们为以上算法的子算法，用于大幅度提高训练过程。其中的代表有：梯度下降法，主要运用在线型回归，逻辑回归，神经网络，推荐算法中；牛顿法，主要运用在线型回归中；BP算法，主要运用在神经网络中；SMO算法，主要运用在SVM中。5.机器学习的应用--大数据说完机器学习的方法，下面要谈一谈机器学习的应用了。无疑，在2010年以前，机器学习的应用在某些特定领域发挥了巨大的作用，如车牌识别，网络攻击防范，手写字符识别等等。但是，从2010年以后，随着大数据概念的兴起，机器学习大量的应用都与大数据高度耦合，几乎可以认为大数据是机器学习应用的最佳场景。譬如，但凡你能找到的介绍大数据魔力的文章，都会说大数据如何准确准确预测到了某些事。例如经典的Google利用大数据预测了H1N1在美国某小镇的爆发。Google成功预测H1N1百度预测2014年世界杯，从淘汰赛到决赛全部预测正确。这些实在太神奇了，那么究竟是什么原因导致大数据具有这些魔力的呢？简单来说，就是机器学习技术。正是基于机器学习技术的应用，数据才能发挥其魔力。大数据的核心是利用数据的价值，机器学习是利用数据价值的关键技术，对于大数据而言，机器学习是不可或缺的。相反，对于机器学习而言，越多的数据会越 可能提升模型的精确性，同时，复杂的机器学习算法的计算时间也迫切需要分布式计算与内存计算这样的关键技术。因此，机器学习的兴盛也离不开大数据的帮助。 大数据与机器学习两者是互相促进，相依相存的关系。机器学习与大数据紧密联系。但是，必须清醒的认识到，大数据并不等同于机器学习，同理，机器学习也不等同于大数据。大数据中包含有分布式计算，内存数据库，多维分析等等多种技术。单从分析方法来看，大数据也包含以下四种分析方法：1.大数据，小分析：即数据仓库领域的OLAP分析思路，也就是多维分析思想。2.大数据，大分析：这个代表的就是数据挖掘与机器学习分析法。3.流式分析：这个主要指的是事件驱动架构。4.查询分析：经典代表是NoSQL数据库。也就是说，机器学习仅仅是大数据分析中的一种而已。尽管机器学习的一些结果具有很大的魔力，在某种场合下是大数据价值最好的说明。但这并不代表机器学习是大数据下的唯一的分析方法。机器学习与大数据的结合产生了巨大的价值。基于机器学习技术的发展，数据能够“预测”。对人类而言，积累的经验越丰富，阅历也广泛，对未来的判断越准确。例如常说的“经验丰富”的人比“初出茅庐”的小伙子更有工作上的优势，就在于经验丰富的人获得的规律比他人更准确。而在机器学习领域，根据著名的一个实验，有效的证实了机器学习界一个理论：即机器学习模型的数据越多，机器学习的预测的效率就越好。见下图：通过这张图可以看出，各种不同算法在输入的数据量达到一定级数后，都有相近的高准确度。于是诞生了机器学习界的名言：成功的机器学习应用不是拥有最好的算法，而是拥有最多的数据！在大数据的时代，有好多优势促使机器学习能够应用更广泛。例如随着物联网和移动设备的发展，我们拥有的数据越来越多，种类也包括图片、文本、视频等非结构化数据，这使得机器学习模型可以获得越来越多的数据。同时大数据技术中的分布式计算Map-Reduce使得机器学习的速度越来越快，可以更方便的使用。种种优势使得在大数据时代，机器学习的优势可以得到最佳的发挥。6.机器学习的子类--深度学习近来，机器学习的发展产生了一个新的方向，即“深度学习”。虽然深度学习这四字听起来颇为高大上，但其理念却非常简单，就是传统的神经网络发展到了多隐藏层的情况。在上文介绍过，自从90年代以后，神经网络已经消寂了一段时间。但是BP算法的发明人Geoffrey Hinton一直没有放弃对神经网络的研究。由于神经网络在隐藏层扩大到两个以上，其训练速度就会非常慢，因此实用性一直低于支持向量机。2006年，Geoffrey Hinton在科学杂志《Science》上发表了一篇文章，论证了两个观点：1.多隐层的神经网络具有优异的特征学习能力，学习得到的特征对数据有更本质的刻画，从而有利于可视化或分类；2.深度神经网络在训练上的难度，可以通过“逐层初始化” 来有效克服。 Geoffrey Hinton与他的学生在Science上发表文章通过这样的发现，不仅解决了神经网络在计算上的难度，同时也说明了深层神经网络在学习上的优异性。从此，神经网络重新成为了机器学习界中的主流强大学习技术。同时，具有多个隐藏层的神经网络被称为深度神经网络，基于深度神经网络的学习研究称之为深度学习。由于深度学习的重要性质，在各方面都取得极大的关注，按照时间轴排序，有以下四个标志性事件值得一说：2012年6月，《纽约时报》披露了Google Brain项目，这个项目是由Andrew Ng和Map-Reduce发明人Jeff Dean共同主导，用16000个CPU Core的并行计算平台训练一种称为“深层神经网络”的机器学习模型，在语音识别和图像识别等领域获得了巨大的成功。Andrew Ng就是文章开始所介绍的机器学习的大牛(中右者)。2012年11月，微软在中国天津的一次活动上公开演示了一个全自动的同声传译系统，讲演者用英文演讲，后台的计算机一气呵成自动完成语音识别、英中机器翻译，以及中文语音合成，效果非常流畅，其中支撑的关键技术是深度学习；2013年1月，在百度的年会上，创始人兼CEO李彦宏高调宣布要成立百度研究院，其中第一个重点方向就是深度学习，并为此而成立深度学习研究院(IDL)。2013年4月，《麻省理工学院技术评论》杂志将深度学习列为2013年十大突破性技术(Breakthrough Technology)之首。 深度学习的发展热潮文章开头所列的三位机器学习的大牛，不仅都是机器学习界的专家，更是深度学习研究领域的先驱。因此，使他们担任各个大型互联网公司技术掌舵者的原因不仅在于他们的技术实力，更在于他们研究的领域是前景无限的深度学习技术。目前业界许多的图像识别技术与语音识别技术的进步都源于深度学习的发展，除了本文开头所提的Cortana等语音助手，还包括一些图像识别应用，其中典型的代表就是下图的百度识图功能。 百度识图深度学习属于机器学习的子类。基于深度学习的发展极大的促进了机器学习的地位提高，更进一步地，推动了业界对机器学习父类人工智能梦想的再次重视。7.机器学习的父类--人工智能人工智能是机器学习的父类。深度学习则是机器学习的子类。如果把三者的关系用图来表明的话，则是下图： 深度学习、机器学习、人工智能三者关系毫无疑问，人工智能(AI)是人类所能想象的科技界最突破性的发明了，某种意义上来说，人工智能就像游戏最终幻想的名字一样，是人类对于科技界的最终梦想。从50年代提出人工智能的理念以后，科技界，产业界不断在探索，研究。这段时间各种小说、电影都在以各种方式展现对于人工智能的想象。人类可以发明类似于人类的机器，这是多么伟大的一种理念！但事实上，自从50年代以后，人工智能的发展就磕磕碰碰，未有见到足够震撼的科学技术的进步。总结起来，人工智能的发展经历了如下若干阶段，从早期的逻辑推理，到中期的专家系统，这些科研进步确实使我们离机器的智能有点接近了，但还有一大段距离。直到机器学习诞生以后，人工智能界感觉终于找对了方向。基于机器学习的图像识别和语音识别在某些垂直领域达到了跟人相媲美的程度。机器学习使人类第一次如此接近人工智能的梦想。事实上，如果我们把人工智能相关的技术以及其他业界的技术做一个类比，就可以发现机器学习在人工智能中的重要地位不是没有理由的。人类区别于其他物体，植物，动物的最主要区别，作者认为是“智慧”。而智慧的最佳体现是什么？是计算能力么，应该不是，心算速度快的人我们一般称之为天才。是反应能力么，也不是，反应快的人我们称之为灵敏。是记忆能力么，也不是，记忆好的人我们一般称之为过目不忘。是推理能力么，这样的人我也许会称他智力很高，类似“福尔摩斯”，但不会称他拥有智慧。是知识能力么，这样的人我们称之为博闻广，也不会称他拥有智慧。想想看我们一般形容谁有大智慧？圣人，诸如庄子，老子等。智慧是对生活的感悟，是对人生的积淀与思考，这与我们机器学习的思想何其相似？通过经验获取规律，指导人生与未来。没有经验就没有智慧。 机器学习与智慧那么，从计算机来看，以上的种种能力都有种种技术去应对。例如计算能力我们有分布式计算，反应能力我们有事件驱动架构，检索能力我们有搜索引擎，知识存储能力我们有数据仓库，逻辑推理能力我们有专家系统，但是，唯有对应智慧中最显著特征的归纳与感悟能力，只有机器学习与之对应。这也是机器学习能力最能表征智慧的根本原因。让我们再看一下机器人的制造，在我们具有了强大的计算，海量的存储，快速的检索，迅速的反应，优秀的逻辑推理后我们如果再配合上一个强大的智慧大脑，一个真正意义上的人工智能也许就会诞生，这也是为什么说在机器学习快速发展的现在，人工智能可能不再是梦想的原因。人工智能的发展可能不仅取决于机器学习，更取决于前面所介绍的深度学习，深度学习技术由于深度模拟了人类大脑的构成，在视觉识别与语音识别上显著性的突破了原有机器学习技术的界限，因此极有可能是真正实现人工智能梦想的关键技术。无论是谷歌大脑还是百度大脑，都是通过海量层次的深度学习网络所构成的。也许借助于深度学习技术，在不远的将来，一个具有人类智能的计算机真的有可能实现。最后再说一下题外话，由于人工智能借助于深度学习技术的快速发展，已经在某些地方引起了传统技术界达人的担忧。真实世界的“钢铁侠”，特斯拉CEO马斯克就是其中之一。最近马斯克在参加MIT讨论会时，就表达了对于人工智能的担忧。“人工智能的研究就类似于召唤恶魔，我们必须在某些地方加强注意。” 马斯克与人工智能尽管马斯克的担心有些危言耸听，但是马斯克的推理不无道理。“如果人工智能想要消除垃圾邮件的话，可能它最后的决定就是消灭人类。”马斯克认为预防此类现象的方法是引入政府的监管。在这里作者的观点与马斯克类似，在人工智能诞生之初就给其加上若干规则限制可能有效，也就是不应该使用单纯的机器学习，而应该是机器学习与规则引擎等系统的综合能够较好的解决这类问题。因为如果学习没有限制，极有可能进入某个误区，必须要加上某些引导。正如人类社会中，法律就是一个最好的规则，杀人者死就是对于人类在探索提高生产力时不可逾越的界限。在这里，必须提一下这里的规则与机器学习引出的规律的不同，规律不是一个严格意义的准则，其代表的更多是概率上的指导，而规则则是神圣不可侵犯，不可修改的。规律可以调整，但规则是不能改变的。有效的结合规律与规则的特点，可以引导出一个合理的，可控的学习型人工智能。8.机器学习的思考--计算机的潜意识最后，作者想谈一谈关于机器学习的一些思考。主要是作者在日常生活总结出来的一些感悟。回想一下我在节1里所说的故事，我把小Y过往跟我相约的经历做了一个罗列。但是这种罗列以往所有经历的方法只有少数人会这么做，大部分的人采用的是更直接的方法，即利用直觉。那么，直觉是什么？其实直觉也是你在潜意识状态下思考经验后得出的规律。就像你通过机器学习算法，得到了一个模型，那么你下次只要直接使用就行了。那么这个规律你是什么时候思考的？可能是在你无意识的情况下，例如睡觉，走路等情况。这种时候，大脑其实也在默默地做一些你察觉不到的工作。这种直觉与潜意识，我把它与另一种人类思考经验的方式做了区分。如果一个人勤于思考，例如他会每天做一个小结，譬如“吾日三省吾身”，或者他经常与同伴讨论最近工作的得失，那么他这种训练模型的方式是直接的，明意识的思考与归纳。这样的效果很好，记忆性强，并且更能得出有效反应现实的规律。但是大部分的人可能很少做这样的总结，那么他们得出生活中规律的方法使用的就是潜意识法。举一个作者本人关于潜意识的例子。作者本人以前没开过车，最近一段时间买了车后，天天开车上班。我每天都走固定的路线。有趣的是，在一开始的几天，我非常紧张的注意着前方的路况，而现在我已经在无意识中就把车开到了目标。这个过程中我的眼睛是注视着前方的，我的大脑是没有思考，但是我手握着的方向盘会自动的调整方向。也就是说。随着我开车次数的增多，我已经把我开车的动作交给了潜意识。这是非常有趣的一件事。在这段过程中，我的大脑将前方路况的图像记录了下来，同时大脑也记忆了我转动方向盘的动作。经过大脑自己的潜意识思考，最后生成的潜意识可以直接根据前方的图像调整我手的动作。假设我们将前方的录像交给计算机，然后让计算机记录与图像对应的驾驶员的动作。经过一段时间的学习，计算机生成的机器学习模型就可以进行自动驾驶了。这很神奇，不是么。其实包括Google、特斯拉在内的自动驾驶汽车技术的原理就是这样。除了自动驾驶汽车以外，潜意识的思想还可以扩展到人的交际。譬如说服别人，一个最佳的方法就是给他展示一些信息，然后让他自己去归纳得出我们想要的结论。这就好比在阐述一个观点时，用一个事实，或者一个故事，比大段的道理要好很多。古往今来，但凡优秀的说客，无不采用的是这种方法。春秋战国时期，各国合纵连横，经常有各种说客去跟一国之君交流，直接告诉君主该做什么，无异于自寻死路，但是跟君主讲故事，通过这些故事让君主恍然大悟，就是一种正确的过程。这里面有许多杰出的代表，如墨子，苏秦等等。基本上所有的交流过程，使用故事说明的效果都要远胜于阐述道义之类的效果好很多。为什么用故事的方法比道理或者其他的方法好很多，这是因为在人成长的过程，经过自己的思考，已经形成了很多规律与潜意识。如果你告诉的规律与对方的不相符，很有可能出于保护，他们会本能的拒绝你的新规律，但是如果你跟他讲一个故事，传递一些信息，输送一些数据给他，他会思考并自我改变。他的思考过程实际上就是机器学习的过程，他把新的数据纳入到他的旧有的记忆与数据中，经过重新训练。如果你给出的数据的信息量非常大，大到调整了他的模型，那么他就会按照你希望的规律去做事。有的时候，他会本能的拒绝执行这个思考过程，但是数据一旦输入，无论他希望与否，他的大脑都会在潜意识状态下思考，并且可能改变他的看法。如果计算机也拥有潜意识(正如本博客的名称一样)，那么会怎么样？譬如让计算机在工作的过程中，逐渐产生了自身的潜意识，于是甚至可以在你不需要告诉它做什么时它就会完成那件事。这是个非常有意思的设想，这里留给各位读者去发散思考吧。9.总结本文首先介绍了互联网界与机器学习大牛结合的趋势，以及使用机器学习的相关应用，接着以一个“等人故事”展开对机器学习的介绍。介绍中首先是机器学习的概念与定义，然后是机器学习的相关学科，机器学习中包含的各类学习算法，接着介绍机器学习与大数据的关系，机器学习的新子类深度学习，最后探讨了一下机器学习与人工智能发展的联系以及机器学习与潜意识的关联。经过本文的介绍，相信大家对机器学习技术有一定的了解，例如机器学习是什么，它的内核思想是什么(即统计和归纳)，通过了解机器学习与人类思考的近似联系可以知晓机器学习为什么具有智慧能力的原因等等。其次，本文漫谈了机器学习与外延学科的关系，机器学习与大数据相互促进相得益彰的联系，机器学习界最新的深度学习的迅猛发展，以及对于人类基于机器学习开发智能机器人的一种展望与思考，最后作者简单谈了一点关于让计算机拥有潜意识的设想。机器学习是目前业界最为Amazing与火热的一项技术，从网上的每一次淘宝的购买东西，到自动驾驶汽车技术，以及网络攻击抵御系统等等，都有机器学习的因子在内，同时机器学习也是最有可能使人类完成AI dream的一项技术，各种人工智能目前的应用，如微软小冰聊天机器人，到计算机视觉技术的进步，都有机器学习努力的成分。作为一名当代的计算机领域的开发或管理人员，以及身处这个世界，使用者IT技术带来便利的人们，最好都应该了解一些机器学习的相关知识与概念，因为这可以帮你更好的理解为你带来莫大便利技术的背后原理，以及让你更好的理解当代科技的进程。10.后记这篇文档花了作者两个月的时间，终于在2014年的最后一天的前一天基本完成。通过这篇文章，作者希望对机器学习在国内的普及做一点贡献，同时也是作者本人自己对于所学机器学习知识的一个融汇贯通，整体归纳的提高过程。作者把这么多的知识经过自己的大脑思考，训练出了一个模型，形成了这篇文档，可以说这也是一种机器学习的过程吧(笑)。作者所在的行业会接触到大量的数据，因此对于数据的处理和分析是平常非常重要的工作，机器学习课程的思想和理念对于作者日常的工作指引作用极大，几乎导致了作者对于数据价值的重新认识。想想半年前，作者还对机器学习似懂非懂，如今也可以算是一个机器学习的Expert了(笑)。但作者始终认为，机器学习的真正应用不是通过概念或者思想的方式，而是通过实践。只有当把机器学习技术真正应用时，才可算是对机器学习的理解进入了一个层次。正所谓再“阳春白雪”的技术，也必须落到“下里巴人”的场景下运用。目前有一种风气，国内外研究机器学习的某些学者，有一种高贵的逼格，认为自己的研究是普通人无法理解的，但是这样的理念是根本错误的，没有在真正实际的地方发挥作用，凭什么证明你的研究有所价值呢？作者认为必须将高大上的技术用在改变普通人的生活上，才能发挥其根本的价值。一些简单的场景，恰恰是实践机器学习技术的最好地方。最后，作者很感谢能够阅读到这里的读者。如果看完觉得好的话，还请轻轻点一下赞，你们的鼓励就是作者继续行文的动力。对EasyPR做下说明：EasyPR，一个开源的中文车牌识别系统，代码托管在github。其次，在前面的博客文章中，包含EasyPR至今的开发文档与介绍。在后续的文章中，作者会介绍EasyPR中基于机器学习技术SVM的应用即车牌判别模块的核心内容，欢迎继续阅读。版权说明：本文中的所有文字，图片，代码的版权都是属于作者和博客园共同所有。欢迎转载，但是务必注明作者与出处。任何未经允许的剽窃以及爬虫抓取都属于侵权，作者和博客园保留所有权利。出处：http://www.cnblogs.com/subconscious/p/4107357.html参考文献：1.Andrew Ng Courera Machine Learning2.LeNet Homepage3.pluskid svm"}
{"content2":"摘要过去几年，深度学习在解决诸如视觉识别、语音识别和自然语言处理等很多问题方面都表现出色。在不同类型的神经网络当中，卷积神经网络是得到最深入研究的。早期由于缺乏训练数据和计算能力，要在不产生过拟合的情况下训练高性能卷积神经网络是很困难的。标记数据和近来GPU的发展，使得卷积神经网络研究涌现并取得一流结果。本文中，我们将纵览卷积神经网络近来发展，同时介绍卷积神经网络在视觉识别方面的一些应用。引言卷积神经网络（CNN）是一种常见的深度学习架构，受生物自然视觉认知机制启发而来。1959年，Hubel & Wiesel [1] 发现，动物视觉皮层细胞负责检测光学信号。受此启发，1980年 Kunihiko Fukushima 提出了CNN的前身——neocognitron 。20世纪 90 年代，LeCun et al. [3] 等人发表论文，确立了CNN的现代结构，后来又对其进行完善。他们设计了一种多层的人工神经网络，取名叫做LeNet-5，可以对手写数字做分类。和其他神经网络一样， LeNet-5 也能使用 backpropagation 算法训练。CNN能够得出原始图像的有效表征，这使得CNN能够直接从原始像素中，经过极少的预处理，识别视觉上面的规律。然而，由于当时缺乏大规模训练数据，计算机的计算能力也跟不上，LeNet-5 对于复杂问题的处理结果并不理想。2006年起，人们设计了很多方法，想要克服难以训练深度CNN的困难。其中，最著名的是 Krizhevsky et al.提出了一个经典的CNN 结构，并在图像识别任务上取得了重大突破。其方法的整体框架叫做 AlexNet，与 LeNet-5 类似，但要更加深一些。AlexNet 取得成功后，研究人员又提出了其他的完善方法，其中最著名的要数 ZFNet [7], VGGNet [8], GoogleNet [9] 和 ResNet [10] 这四种。从结构看，CNN 发展的一个方向就是层数变得更多，ILSVRC 2015 冠军 ResNet 是 AlexNet 的20 多倍，是 VGGNet 的8 倍多。通过增加深度，网络便能够利用增加的非线性得出目标函数的近似结构，同时得出更好的特性表征。但是，这样做同时也增加了网络的整体复杂程度，使网络变得难以优化，很容易过拟合。研究人员提出了很多方法来解决这一问题。在下面的章节中，我们会先列出CNN的组成部分，然后介绍CNN不同方面的最近进展，接着引入快速计算技巧，并探讨CNN在图像分类、物体识别等不同方面的应用进展，最后归纳总结。基本组成部分在不同的参考资料中，对 CNN的组成部分都有着不同的描述。不过，CNN的基本组成成分是十分接近的。以分类数字的 LeNet-5 为例，这个 CNN 含有三种类型的神经网络层：卷积层：学会识别输入数据的特性表征池化（Pooling）：典型的操作包括平均 pooling [12] 和最大化 pooling [1315]全连接层：将卷积层和Pooling 层堆叠起来以后，就能够形成一层或多层全连接层，这样就能够实现高阶的推力能力。完善 CNN自从 2012 年 AlexNet 成功以后，研究人员设计了很多种完善 CNN 的方法。在这一节中，我们将从 6 方面进行介绍。1. 卷积层1）网络中的网络（Network in Network，NIN）：由 Lin et al. [21] 提出的基本网络结构2) Inception module: 由 Szegedy et al. [9] 提出，是 NIN 的拓展2. 池化层池化层是CNN的重要组成部分，通过减少卷积层之间的连接，降低运算复杂程度。以下是常用的几种循环方法：1）Lp 池化：Lp 池化是建立在复杂细胞运行机制的基础上，受生物启发而来 [24] [25]2) 混合池化：受随机Dropout [16] 和 DropConnect [28], Yu et al. 启发而来3）随机池化：随机循环 [30] 是受 drptout 启发而来的方法4）Spectral 池化3. 激活函数常用的非线性激活函数有sigmoid、tanh、relu等等，前两者sigmoid/tanh比较常见于全链接层，后者relu常见于卷积层。1) ReLU2) Leaky ReLU3) Parametric ReLU4) Randomized ReLU5) ELU6) Maxout:7) Probout4. Loss 函数1) Softmax loss2) Hinge loss3) Contrastive loss5. 正则化1）DropOut2) DropConnect6. 优化1) 初始化权重2) 随机梯度下降3) 批量标准化4) Shortcut 连接CNN 应用A 图像分类B 物体检测C 物体追踪D 姿态预估（Pose estimatation）E 文本检测识别F 视觉 saliency 检测G 行动识别H 场景标记讨论深度CNN在图像处理、视频、语音和文本中取得了突破。本文种，我们主要从计算机视觉的角度对最近CNN取得的进展进行了深度的研究。我们讨论了CNN在不同方面取得的进步：比如，层的设计，活跃函数、损失函数、正则化、优化和快速计算。除了从CNN的各个方面回顾其进展，我们还介绍了CNN在计算机视觉任务上的应用，其中包括图像分类、物体检测、物体追踪、姿态估计、文本检测、视觉显著检测、动作识别和场景标签。虽然在实验的测量中，CNN获得了巨大的成功，但是，仍然还有很多工作值得进一步研究。首先，鉴于最近的CNN变得越来越深，它们也需要大规模的数据库和巨大的计算能力，来展开训练。人为搜集标签数据库要求大量的人力劳动。所以，大家都渴望能开发出无监督式的CNN学习方式。同时，为了加速训练进程，虽然已经有一些异步的SGD算法，证明了使用CPU和GPU集群可以在这方面获得成功，但是，开放高效可扩展的训练算法依然是有价值的。在训练的时间中，这些深度模型都是对内存有高的要求，并且消耗时间的，这使得它们无法在手机平台上部署。如何在不减少准确度的情况下，降低复杂性并获得快速执行的模型，这是重要的研究方向。其次，我们发现，CNN运用于新任务的一个主要障碍是：如何选择合适的超参数？比如学习率、卷积过滤的核大小、层数等等，这需要大量的技术和经验。这些超参数存在内部依赖，这会让调整变得很昂贵。最近的研究显示，在学习式深度CNN架构的选择技巧上，存在巨大的提升空间。最后，关于CNN，依然缺乏统一的理论。目前的CNN模型运作模式依然是黑箱。我们甚至都不知道它是如何工作的，工作原理是什么。当下，值得把更多的精力投入到研究CNN的基本规则上去。同时，正如早期的CNN发展是受到了生物视觉感知机制的启发，深度CNN和计算机神经科学二者需要进一步的深入研究。有一些开放的问题，比如，生物学上大脑中的学习方式如何帮助人们设计更加高效的深度模型？带权重分享的回归计算方式是否可以计算人类的视觉皮质等等。我们希望这篇文章不仅能让人们更好地理解CNN，同时也能促进CNN领域中未来的研究活动和应用发展。新智元Top10智能汽车创客大赛招募！新智元于7月11日启动2016年【新智元100】人工智能创业公司评选，在人工智能概念诞生60周年之际，寻找中国最具竞争力的人工智能创业企业。智能驾驶技术是汽车行业的重点发展方向之一，同时也是人工智能相关产业创新落地的重要赛道之一。为此新智元联合北京中汽四方共同举办“新智元Top10智能汽车创客大赛”，共同招募智能汽车相关优质创业公司，并联合组织人工智能技术专家、传统汽车行业技术专家、关注智能汽车领域的知名风投机构，共同评审并筛选出Top 10进入决赛，在2016年10月16日“国际智能网联汽车发展合作论坛”期间，进行路演、颁奖及展览活动。"}
{"content2":"CVPR是IEEE Conference on Computer Vision and Pattern Recognition的缩写，即IEEE国际计算机视觉与模式识别会议。该会议是由IEEE举办的计算机视觉和模式识别领域的顶级会议。ECCV的全称是European Conference on Computer Vision(欧洲计算机视觉国际会议) ，两年一次，是计算机视觉三大会议（另外两个是ICCV和CVPR）之一ICCV 的全称是 IEEE International Conference on Computer Vision，即国际计算机视觉大会，由IEEE主办，与计算机视觉模式识别会议（CVPR）和欧洲计算机视觉会议（ECCV）并称计算机视觉方向的三大顶级会议，被澳大利亚ICT学术会议排名和中国计算机学会等机构评为最高级别学术会议，在业内具有极高的评价。PAMI是IEEE旗下，模式识别和机器机器学习领域最重要的学术性汇刊之一。全称：IEEE Transactions on Pattern Analysis and Machine Intelligence 中文：IEEE模式分析与机器智能汇刊SIGGRAPH (Special Interest Group on GRAPHics and Interactive Techniques 是图形学及互交技术特殊爱好者集团的简称) 是由ACM（ Association for Computing Machinery 计算机机械协会） SIGGRAPH 组织举办的关于计算机图形学的会议VCG 可视化和计算机图形学库The Visualization and Computer Graphics Library (VCG for short) is a open source portable C++ templated library for manipulation, processing and displaying with OpenGL of triangle and tetrahedral meshes.VCG Lib uses a git repository hosted by github at http://github.com/cnr-isti-vclab/vcglib/"}
{"content2":"依然是关于艾瑞咨询的2018年中国人工智能行业的研究报告的总结，本期将总结和分享本报告中关于人工智能行业发展趋势的分析。一、对事物的完整行为规划或者事项决策的发展空间依然很大。现在的人工智能的模型，多数目前是建立在深度学习算法的基础上。但是这也带来了很多的局限。这里我们需要对比人工智能和人类智能的区别。以深度学习为基础建立的人工智能，需要采用大量的数据去建立模型然后去解决一个很小的问题。人类的智能是学习很少的数据就可以解决很大的问题。人类可以凭借自己的观察和判断形成最终的价值决策，机器的语音识别、计算机视觉等AI能力在现阶段还很难支撑到对事物的理解与判断所以在下阶段，关于人工智能的技术的重要发展方向，可以总结为下：如何可以降低训练深度学习过程中训练模型需要的数据量，或者可以使用一些不需要多度标注的数据持续学习，或者在深度学习的过程中有效的对变化的环境进行自主的适应目前很多人工智能技术依然需要规范一个特定的场景，如何让机器可以和人类一样，运用自己的知识去解决不同场景的问题可解释性依然是目前深度学习过程需要解决的问题。目前有的技术是可视化深度神经网络。二、纵观整个行业，除了算法的进步，产品，服务，市场等建设依然还是人工智能行业发展的壁垒人工智能技术的落地往往涉及对具体业务场景的硬件设备改造，软件集成以及本地计算设施的部署，算法、技术的实际功效更需要建立在对客户真实业务场景的深层理解之上的针对性开发。对于影响人工智能技术大规模应用的重要因素可以归纳为以下5点：学术前沿算法-可通过会议以及期刊论文的发表情况，或者相关比赛的成绩作为判断依据市场销售-资源渠道建设与销售体系扩充能够加速技术落地垂直业务领域工程技术-工程技术落地涵盖构架搭建，系统配合，流程控制，质量监控以及贴合客户业务场景的算法优化客户服务-客户服务需满足售前，中，后，需求响应及时，解决方案全面，问题出现可以及时修复新硬件产品-易用性，流畅度以及稳定性+满足实际业务所需三、人工智能会产生我们之前预计的那些大规模失业现象嘛首先，必须要承认，机器学习和深度学习技术的发展和升级一定会逐步影响就业市场。但是这里的影响，更多的是集中在重复性的劳动。但是人们也会从重复性的劳动重解放出来，将更多的时间和精力用于创造性活动。换一句话理解，人工智能解决的是劳动力危机。以下是人工智能对就业市场影响归纳：简单、重复性工作，机器一定会取代人类。（这类工作不需要复杂思考就能完成的脑力决策或者只是单纯的体力劳动）人机协同工作会逐渐变多，这也是长期时间内的主流关系。机器无法取代人类，对于涉及同理心或者创造性的工作。在人工智能无法完成理解，或者推断的技术层面上，暂时也就没办法取代人类类似的工作。四、人工智能的核心价值是什么人工智能即通过智能实现人类思维的效果,从宏观层面来看,此效果体现在智能社会与智能经济层面，即，人工智能将大幅改善依赖劳动力创造的劳动密集型、简单重复性的传统经济运行模式，并依托此经济模式构建万物互联、智能协同的产业体系，打造国际领先的智能社会。从微观层面来看,人工智能将替代传统劳动，带来新式生产方式，以提升生产效率并降低成本,进而实现企业效益提升、改善人们工作与生活。而随着机器变得聪明，我们将最终实现人性化人工智能(Humanistic AI),即通过机器达到拟人的形式并以这类形式延伸人类智慧。而具体的价值，可以归纳为以下4点：智能社会：建设智能产业的同时，实现传统产业的智能化升级。智能经济：利用人类智力，借助人工智能，充分发挥体力与知识的结合，产出高知识附加值与高知识密度的产品和服务智能商业：降低商品和服务的成本只能劳动：缓解劳动力危机，提高生活及工作的自由度"}
{"content2":"今日CS.CV 计算机视觉论文速览Mon, 8 Apr 2019Totally 49 papers👉上期速览 ✈更多精彩请移步主页Interesting:📚单图像修饰去除, 通过估计图像中含有干扰的像素（水印、涂鸦和变形，并合成对应的像素来实现图像修饰的去除。(from 特拉维夫大学 )分为了原图预测、mask预测，装饰物预测三个分支，并在最后校正图像。结果如下：📚任务驱动的目标检测COCO-Task, 为了对场景中最适用于任务的目标进行检测，研究人员提出了COCO-Tasks，40000张图像包含了14个任务相关的标注，每张图片中都标注出了与任务最相关的目标。并基于门控图网络，提出了对于给定任务检测最合适目标的方法。(from 波恩大学)对于不同的任务，有着最合适的目标：检测出目标并将其隐藏状态放到图的节点中，利用状态转移寻找最合适的目标：COCO-Task:https://coco-tasks.github.io/📚Dense-Haze ,图像去雾数据集，包含了33对真实的配对场景。 (from Universitatea Politehnica Timisoara )评测了多种去雾算法,发现对于均匀的雾（造雾机生成）表现不好：相关： O-HAZE D-HAZY HazeRD FRIDA📚HomebrewedDB, 用于三维物体6D位姿估计的RGB-D数据集,包含了33个物体13个场景(*1340)，以及一系列基准测评(from TUM )相关数据集：LineMOD Dataset. T-LESS Dataset. YCB-Video dataset.扫描仪：https://www.artec3d.com/📚Single-Path NAS基于单路径的自然架构搜索,用于硬件高效卷积模型的设计，加速移动端卷积模型设计。 (from )code：https://github.com/dstamoulis/single-path-nas📚基于精确点扩散函数和圆一致CNN实现盲解卷积显微镜, (from KAIST)。利用基于圆连续的CNN和PSF建模层实现了盲解卷积的非监督网络。Deconvolution Microscopy:http://www2.ujf-grenoble.fr/medecine/iab/clientzone/plforme9/fichiers/DeconvolutionMicroscopy_Sibarita_Springer.pdf📚CTN：Crowd Transformer Network, 探索了局域和非局域特征用于人群密度估计，利用卷积抽取局域特征，利用自注意力抽取非局域特征，结合两者估计人群密度图。(from 石溪分校)加入非局域特征的效果提升：与相关方法比较：相关数据集： UCF-QNRF dataset UCF-CC dataset Shanghaitech📚基于多阶水平集方法的半监督/无监督分割 , (from KAIST)通过最小化水平集损失来实现分割：提出方法的一些结果：📚SDC – Stacked Dilated Convolution:稠密匹配任务的通用描述网络, (from DFKI - German Research Center for Artificial Intelligence)膨胀卷积层:📚提高医学神经网络的可解释性, (from TUM)Daily Computer Vision PapersDetecting Human-Object Interactions via Functional GeneralizationAuthors Ankan Bansal, Sai Saketh Rambhatla, Abhinav Shrivastava, Rama Chellappa我们提出了一种方法，用于检测图像中的人体对象交互HOI，基于人类以类似方式与功能相似的对象进行交互的想法。所提出的模型是简单的并且使用人的视觉特征，人和对象的相对空间定向，以及功能相似的对象参与与人类的类似交互的知识。我们为我们的方法提供了广泛的实验验证，并展示了HOI检测的最新结果。在HICO Det数据集上，我们的方法在已发表的文献中平均精度mAP获得超过7个绝对点的增益，甚至比当代工作获得超过2.5个绝对mAP。我们还表明，我们的方法可以在看到的对象设置中为零射击HOI检测带来显着的性能提升。我们进一步证明，使用通用对象检测器，我们的模型可以推广到涉及以前看不见的对象的交互。Moving Object Detection under Discontinuous Change in Illumination Using Tensor Low-Rank and Invariant Sparse DecompositionAuthors Moein Shakeri, Hong Zhang尽管基于低秩和稀疏分解的方法已经成功地应用于使用结构化稀疏诱导规范的运动物体检测的问题，但是它们仍然易受在某些应用中出现的显着照明变化的影响。我们感兴趣的是在涉及时间推移图像序列的应用中移动物体检测，当前方法错误地将移动物体和照明变成前景。我们的方法依赖于多线性张量数据低秩和稀疏分解框架来解决现有方法的弱点。我们提出的方法的关键是首先创建一组先前的映射，其可以表征由于照明而导致的图像序列的变化。我们表明它们可以被k支持规范检测到。为了处理并发的两种类型的变化，我们采用两个正则化项，一个用于检测运动物体，另一个用于计算光照变化，在张量低秩和稀疏分解公式中。通过使用具有挑战性的数据集的综合实验，我们表明我们的方法展示了在不连续的照明变化下检测移动物体的显着能力，并且优于现有技术解决这一具有挑战性的问题的解决方案。HomebrewedDB: RGB-D Dataset for 6D Pose Estimation of 3D ObjectsAuthors Roman Kaskman, Sergey Zakharov, Ivan Shugurov, Slobodan Ilic创建和评估6D对象姿势检测器的最重要的先决条件之一是具有标记为6D姿势的数据集。随着深度学习方法的出现，对这些数据集的需求也在不断涌现。尽管其中一些存在，但它们很少并且通常具有受限制的设置，例如，每个序列单个对象，或专注于特定对象类型，例如无纹理工业零件。此外，通常仅从可用的3D模型而不是实际数据和可伸缩性来训练两个重要组件，即训练一种方法来检测所有对象而不是每个对象训练一个检测器。其他挑战，例如遮挡，改变光照条件和物体外观变化，以及精确定义的基准，要么不存在，要么分散在不同的数据集中。Unsupervised Image Matching and Object Discovery as OptimizationAuthors Huy V. Vo, Francis Bach, Minsu Cho, Kai Han, Yann LeCun, Patrick Perez, Jean Ponce完全或部分监督学习是有力的，但依赖于不断增长的人类注释努力。作为缓解这一严重问题以及服务于特定应用的一种方式，无监督学习已经成为一个重要的研究领域。在计算机视觉中，无监督学习有各种各样的形式。在Cho等人的工作之后，我们将重点放在集合中图像之间的无监督发现和对象类别的匹配上。 2015.我们表明原始方法可以重新制定并解决为适当的优化问题。几个基准测试的实验证明了我们的方法的优点。Spatial Shortcut Network for Human Pose EstimationAuthors Te Qi 1 , Bayram Bayramli 1 , Usman Ali 1 , Qinchuan Zhang 1 , Hongtao Lu 1 1 Shanghai Jiao Tong University像许多计算机视觉问题一样，人体姿势估计是一个具有挑战性的问题，因为识别身体部位不仅需要来自局部区域的信息，还需要来自具有大空间距离的区域的信息。为了在空间上传递信息，通常使用大的卷积核和深层，引入高计算成本和大参数空间。幸运的是，对于姿势估计，人体在图像中几何结构化，使得能够建模空间依赖性。在本文中，我们提出了一种用于姿势估计任务的空间快捷网络，其中信息更容易在空间上流动。我们通过详细分析评估我们的模型，并以更小的结构展示其出色的性能。Deep Learning Under the Microscope: Improving the Interpretability of Medical Imaging Neural NetworksAuthors Magdalini Paschali, Muhammad Ferjad Naeem, Walter Simson, Katja Steiger, Martin Mollenhauer, Nassir Navab在本文中，我们提出了一种新的解释方法，适用于组织学整体幻灯片图像WSI处理。深度神经网络DNN受Bag of Features模型的启发，配备了多实例学习MIL分支，并且在WSI分类的监督下受到严格监控。 MIL避免了标签歧义，并在不引起注意的情况下增强了我们模型的表现力。我们利用模型激活的细粒度logit热图来解释其决策过程。所提出的方法在两个具有挑战性的组织学数据集上进行定量和定性评估，优于各种基线。此外，我们咨询了两位专家病理学家关于我们的方法提供的可解释性，并承认其可以整合到几个临床应用中。Leaf segmentation through the classification of edgesAuthors Jonathan Bell, Hannah M. Dee我们提出了一种基于检测到的边缘对拟南芥植物的叶片水平分割的方法。我们引入了一种新的边缘分类方法，该方法形成了一种方法的重要组成部分，该方法既可以从高通量表型系统中获得的图像中计算叶子并建立生长植物的叶面积。我们的技术使用相对浅的卷积神经网络将图像边缘分类为背景，植物边缘，叶缘上的叶子或内部叶子噪声。使用Canny边缘检测器找到边缘本身，并且分类的边缘可以与简单的图像处理技术一起使用以生成基于区域的分割，其中叶子是不同的。这种方法很有效地区分了一片叶子大部分隐藏的遮挡叶片，这种情况在过去已被证明对于植物图像分析系统来说是麻烦的。此外，我们还介绍了用于此项工作的公开可用的植物图像数据集。Weakly Supervised Action Segmentation Using Mutual ConsistencyAuthors Yaser Souri, Mohsen Fayyaz, Juergen Gall动作分段是预测视频的每个帧中的动作的任务。由于在完全监督行动分割的情况下准备培训视频的成本很高，因此只能从成绩单中学习的弱监督方法非常有吸引力。在本文中，我们提出了一种基于双分支网络的弱监督动作分割的新方法。我们网络的两个分支预测了两个冗余但不同的动作分割表示。在培训期间，我们引入了一种新的相互一致性损失MuCon，强制执行这两种表示是一致的。使用MuCon和成绩单预测损失，我们的网络实现了动作分割和动作对齐的最先进结果，同时完全可区分且训练更快，因为它在训练期间不需要昂贵的对齐步骤。3DQ: Compact Quantized Neural Networks for Volumetric Whole Brain SegmentationAuthors Magdalini Paschali, Stefano Gasperini, Abhijit Guha Roy, Michael Y. S. Fang, Nassir Navab模型体系结构的规模急剧增加，以牺牲资源需求为代价提高了性能。在本文中，我们提出3DQ，一种三元量化方法，首次应用于3D全卷积神经网络F CNN，实现16x模型压缩，同时保持与全精度模型相当的性能。我们在两个数据集上广泛评估3DQ，以完成全脑分割的挑战性任务。此外，我们展示了我们的方法能够概括两种常见的3D架构，即3D U Net和V Net。该方法的性能优于各种基线，能够将大型3D模型压缩到几MB，从而减轻了空间关键应用中的存储需求。Radiotherapy Target Contouring with Convolutional Gated Graph Neural NetworkAuthors Chun Hung Chao, Yen Chi Cheng, Hsien Tzu Cheng, Chi Wen Huang, Tsung Ying Ho, Chen Kan Tseng, Le Lu, Min Sun层析成像医学成像在现代癌症放射治疗的临床工作流程中是必不可少的。放射肿瘤学家识别癌组织，在所有图像切片的治疗区域上应用描绘。这种任务通常通过具有相当大计算成本的3D卷积网络公式化为体积分割任务。相反，受到跨切片考虑有意义信息的处理方法的启发，我们使用门控图形神经网络来更有效地构建此问题。更具体地，我们提出卷积递归门控图传播器GGP通过图像切片传播高级信息，具有可学习的邻接加权矩阵。此外，由于医生经常研究一些特定切片来改进他们的决策，我们对这种切片式交互过程进行建模，以进一步改善我们的分割结果。这可以通过毫不费力地编辑任何切片来设置，以使用GGP更新其他切片的预测。为了评估我们的方法，我们收集了81名患者的食道癌放射治疗目标治疗轮廓数据集，其中包括具有放射治疗目标的断层扫描图像。在这个数据集上，我们的卷积图网络产生了最先进的结果，并且优于基线。通过添加交互式设置，性能得到进一步提升。我们的方法可以很容易地应用于具有体积图像的各种医疗任务。结合能够进行可行预测和考虑人类交互输入的能力，所提出的方法适用于临床场景。SDC - Stacked Dilated Convolution: A Unified Descriptor Network for Dense Matching TasksAuthors Ren Schuster, Oliver Wasenm ller, Christian Unger, Didier Stricker密集像素匹配对于诸如视差和流量估计的许多计算机视觉任务是重要的。我们提出了一个健壮的统一描述符网络，它考虑了具有高空间方差的大型上下文区域。我们的网络具有非常大的感受野，避免跨步层以保持空间分辨率。通过创建由多个平行堆叠的扩张卷积SDC组成的新型神经网络层来实现这些特性。这些层中的几个被组合以形成我们的SDC描述符网络。在我们的实验中，我们表明我们的SDC功能在精度和鲁棒性方面优于最先进的特征描述符。此外，我们在几个着名的公共基准测试中展示了SDC在最先进的立体匹配，光流和场景流算法方面的卓越性能。Comparative Analysis of Automatic Skin Lesion Segmentation with Two Different ImplementationsAuthors Md. Kamrul Hasan, Basel Alyafi, Fakrul Islam Tushar来自周围皮肤的病变分割是开发皮肤癌的自动计算机辅助诊断的首要任务。病变的变化特征如颜色的不均匀分布，不规则的形状，边界和纹理使这项任务具有挑战性。本文的贡献是提出和比较皮肤病变分割的两种不同方法。第一种方法使用分水岭，而第二种方法使用均值漂移。在两种方法中进行预处理步骤以去除毛发和显微图像的暗边界。使用Jaccard Index Intersection over Union或IoU进行所提出方法的评估。本文的另一个贡献是使用现有的分割和形态算法来呈现用于执行预处理和分割的管道，这导致了有希望的结果。平均而言，第一种方法表现出比第二种方法更好的表现，平均Jaccard指数超过200 ISIC 2017挑战图像分别为89.16和76.94。Automatic detection of lesion load change in Multiple Sclerosis using convolutional neural networks with segmentation confidenceAuthors Richard McKinley, Lorenz Grunder, Rik Wepfer, Fabian Aschwanden, Tim Fischer, Christoph Friedli, Raphaela Muri, Christian Rummel, Rajeev Verma, Christian Weisstanner, Mauricio Reyes, Anke Salmen, Andrew Chan, Roland Wiest, Franca Wagner检测多发性硬化症中新的或扩大的白质病变是监测接受多发性硬化症疾病改善治疗的患者的重要任务。然而，新的或扩大的定义并不固定，并且已知病变计数是高度主观的，具有高度的内部和内部评估者可变性。用于病变量化的自动化方法具有使新的和扩大的病变的检测一致且可重复的潜力。然而，尽管这是一个紧迫的临床用例，但大多数病变分割算法并未评估其分离进展型和稳定型患者的能力。在本文中，我们表明，即使对于高性能的分割方法，单独的病变负荷的体积测量的变化也不是执行该分离的好方法。相反，我们提出了一种识别高确定性病变的方法，并在纵向多发性硬化病例的数据集上建立该方法能够将进展与稳定时间点分离，具有非常高的鉴别水平AUC 0.99，而病变体积的变化更不能执行这种分离AUC 0.71。在第二个外部数据集上验证该方法证实该方法能够超出其训练的设置，在分离稳定和渐进时间点时达到83的准确度。先前已经显示病变体积和计数都是人群中疾病过程的强预测因子。然而，我们证明，对于个体患者，这些措施的变化并不是建立疾病活动证据的充分手段。同时，直接检测从非病变到病变的高可信度变化的组织是用于识别放射学活跃患者的可行方法。Learning Task Relatedness in Multi-Task Learning for Images in ContextAuthors Gjorgji Strezoski, Nanne van Noord, Marcel Worring多媒体应用通常需要多个任务的并发解决方案这些任务为每个其他解决方案提供了线索，但由于这些关系可能很复杂，因此这仍然是一个很少使用的财产。当基于领域知识明确定义任务关系时，多任务学习MTL提供这样的并发解决方案，同时利用在同一数据集上执行的多个任务之间的相关性。但是，在大多数情况下，没有明确定义此相关性，并且无法定义定义它的领域专家知识。为了解决这个问题，我们介绍了选择性共享，这是一种在模型训练时从二级潜在特征中学习相互作用关系的方法。利用这种洞察力，我们可以自动对任务进行分组，并允许他们以互利的方式分享知识。我们通过对分类，回归和排序任务中的5个数据集进行实验来支持我们的方法，并与强基线和现有技术方法进行比较，从而显示出在准确性和参数计数方面的持续改进。此外，我们执行激活区域分析，显示选择性共享如何影响学习表示。What Object Should I Use? - Task Driven Object DetectionAuthors Johann Sawatzky, Yaser Souri, Christian Grund, Juergen Gall当人类必须解决日常任务时，他们只需选择最合适的物体。虽然对于特定任务应该使用哪个对象的问题听起来对人类来说是微不足道的，但对于机器人或其他自治系统来说很难回答。然而，当前基于检测对象类别的对象检测基准并未解决该问题。因此，我们介绍了COCO Tasks数据集，该数据集包含大约40,000个图像，其中14个任务的最合适的对象已被注释。我们还提出了一种方法，可以检测给定任务的最合适的对象。该方法建立在门控图形神经网络的基础上，以利用每个对象的外观以及场景中所有当前对象的全局上下文。在我们的实验中，我们表明所提出的方法优于在数据集上评估的其他方法，如分类或排序方法。Relation-Aware Global AttentionAuthors Zhizheng Zhang, Cuiling Lan, Wenjun Zeng, Xin Jin, Zhibo Chen注意机制旨在通过关注重要特征和抑制不必要的特征来增加代表权。对于卷积神经网络CNN，通常通过局部卷积来学习注意力，其忽略全局信息和隐藏关系。如何有效地利用长距离背景来全球学习注意力尚未得到充分发掘。在本文中，我们提出了一个有效的关系感知全局注意RGA模块，用于CNN充分利用全局相关性来推断注意力。具体地，当计算特征位置处的注意力时，为了掌握全局范围的信息，我们建议将关系（即，其成对相关性关系）与所有特征位置以及特征本身一起堆叠以用卷积学习学习注意力。操作。给定中间特征图，我们已经在空间和通道维度上验证了该设计的有效性。当应用于人物识别任务时，我们的模型达到了最先进的性能。广泛的消融研究表明，我们的RGA可以显着增强特征表示能力。我们通过将RGA应用于场景分割和图像分类任务来进一步证明RGA对视觉任务的一般适用性，从而实现一致的性能改进。Semantic Attribute Matching NetworksAuthors Seungryong Kim, Dongbo Min, Somi Jeong, Sunok Kim, Sangryul Jeon, Kwanghoon Sohn我们提出了语义属性匹配网络SAM Net，用于在语义相似的图像上共同建立对应关系和传递属性，在克服其局限性的同时智能地编织两个任务的优点。 SAM Net通过减少图像之间的属性差异并使用学习的对应关系合成属性转移图像来建立可靠对应的迭代过程来实现这一点。为了使用图像对形式的弱监督来学习网络，我们基于属性转移源特征和扭曲目标特征之间的匹配相似性来呈现语义属性匹配损失。使用SAM Net，可以在语义匹配和属性转移的几个基准上实现最先进的性能。Learning to Adapt for StereoAuthors Alessio Tonioni, Oscar Rahnama, Thomas Joy, Luigi Di Stefano, Thalaiyasingam Ajanthan, Philip H. S. Torr立体深度估计的真实世界应用需要对环境中的动态变化具有鲁棒性的模型。尽管基于深度学习的立体声方法是成功的，但它们通常不能概括为环境中看不见的变化，使得它们不太适合于诸如自动驾驶的实际应用。在这项工作中，我们引入了一个学习适应框架，使深度立体方法能够以无人监督的方式不断适应新的目标域。具体而言，我们的方法将适应程序纳入学习目标，以获得更适合无监督在线适应的基本参数集。为了进一步提高自适应的质量，我们学习了一种置信度量，有效地掩盖了无监督自适应期间引入的误差。我们在合成和现实世界的立体数据集上评估我们的方法，并且我们的实验证明，学习适应对于在非常不同的领域进行在线适应确实是有益的。High-level Semantic Feature Detection:A New Perspective for Pedestrian DetectionAuthors Wei Liu, Shengcai Liao, Weiqiang Ren, Weidong Hu, Yinan Yu对象检测通常需要传统的滑动窗口分类器或现代深度学习方法中基于锚的预测。但是，这些方法中的任何一种都需要在窗口或锚点中进行繁琐的配置。在本文中，以行人检测为例，我们提供了一种新的视角，其中检测对象被激发为高级语义特征检测任务。像边缘，角落，斑点和其他特征检测器一样，所提出的检测器扫描整个图像上的特征点，卷积自然适合这些特征点。然而，与传统的低级特征不同，所提出的探测器用于更高级别的抽象，即，我们正在寻找存在行人的中心点，并且现代深度模型已经能够进行如此高级别的语义抽象。此外，与斑点检测一样，我们也预测行人点的比例，这也是一个简单的卷积。因此，在本文中，行人检测通过卷积简化为直接的中心和尺度预测任务。这样，所提出的方法享有无锚设置。虽然结构简单，但它在具有挑战性的行人检测基准上具有竞争力的准确性和良好的速度，因此导致新的有吸引力的行人探测器。代码和模型将在网址上提供Branched Multi-Task Networks: Deciding What Layers To ShareAuthors Simon Vandenhende, Bert De Brabandere, Luc Van Gool在深度学习的背景下，已经使用具有多个分支的神经网络，每个分支都解决不同的任务。这种分支网络通常以许多共享层开始，之后不同的任务分支到它们自己的层序列中。由于可能的网络配置的数量是组合大的，因此先前的工作通常依赖于特殊方法来确定层共享的级别。3D LiDAR and Stereo Fusion using Stereo Matching Network with Conditional Cost Volume NormalizationAuthors Tsun Hsuan Wang, Hou Ning Hu, Chieh Hubert Lin, Yi Hsuan Tsai, Wei Chen Chiu, Min Sun主动和被动深度感测技术的互补特性激发了Li DAR传感器和立体相机的融合，以改善深度感知。我们利用立体匹配网络，利用LiDAR信息上的两种增强技术输入融合和条件成本量标准化CCVNorm，而不是直接融合LiDAR和立体模态的估计深度。所提出的框架是通用的并且与立体匹配神经网络中通常使用的成本量组件紧密集成。我们通过实验验证了我们的方法对KITTI立体和深度完成数据集的有效性和稳健性，获得了针对各种融合策略的有利性能。此外，我们证明，通过CCVNorm的分层扩展，所提出的方法在计算时间和模型大小方面仅给立体匹配网络带来轻微的开销。对于项目页面，请参阅Point-to-Point Video GenerationAuthors Tsun Hsuan Wang, Yen Chi Cheng, Chieh Hubert Lin, Hwann Tzong Chen, Min Sun虽然图像处理实现了巨大的突破，例如，近年来产生逼真的面孔，但是视频生成的探索更少且难以控制，这限制了其在现实世界中的应用。例如，视频编辑需要跨多个剪辑的时间一致性，因此在视频序列内构成开始和结束约束。我们介绍了控制生成过程的点对点视频生成，其中两个控制点是目标开始和结束帧。该任务具有挑战性，因为该模型不仅生成帧的平滑过渡，而且还提前计划以确保生成的结束帧符合各种长度的视频的目标结束帧。我们建议在跳帧训练策略下最大化条件数据似然的修正变分下界。我们的模型可以生成序列，使得它们的结束帧与目标结束帧一致，而不会损失质量和多样性。在Stochastic Moving MNIST，Weizmann Human Action和Human3.6M上进行了大量实验，以评估所提方法的有效性。我们在一系列场景下演示了我们的方法，例如，动态长度生成和定性结果展示了点到点生成的潜力和优点。对于项目页面，请参阅Deep Predictive Video Compression with Bi-directional PredictionAuthors Woonsung Park, Munchurl Kim最近，深度图像压缩在编码效率和图像质量改进方面已经显示出很大的进步。然而，使用深度学习网络对视频压缩的关注相对较少。在本文中，我们首先提出了一种基于深度学习的双向预测编码网络，称为BP DVC Net，用于视频压缩。从传统视频编码的教训中了解到，B帧编码结构被纳入我们的BP DVC网络中。虽然传统视频编解码器中的双向预测编码需要向解码器侧发送用于块运动的运动矢量和来自预测的残差，但是我们的BP DVC网络在编码器和解码器侧都包含光流估计网络，以便不传输运动。信息到解码器侧以提高编码效率。此外，BP DVC网络中的双向预测网络被提出并用于精确预测当前帧并使得到的残留物尽可能小。此外，我们的BP DVC网络允许使用相邻帧的特征图之间的时间上下文对压缩特征图进行熵编码。 BP DVC Net具有端到端视频压缩架构，具有新设计的流量和预测损耗。实验结果表明，我们提出的方法的压缩性能与H.264，HEVC在PSNR和MS SSIM方面的压缩性能相当。Dense Haze: A benchmark for image dehazing with dense-haze and haze-free imagesAuthors Codruta O. Ancuti, Cosmin Ancuti, Mateu Sbert, Radu Timofte单一图像去雾是一个不适合的问题，最近引起了重视。尽管在过去几年中对去雾的兴趣显着增加，但由于缺乏成对的真实模糊和相应的无雾度参考图像，所以去雾方法的验证仍然很不令人满意。为了解决这个限制，我们向Dense Haze介绍了一种新颖的去雾数据集。 Dense Haze的特点是密集而均匀的朦胧场景，包含33对真实的朦胧和相应的各种户外场景的无阴影图像。通过引入由专业雾霾机器产生的真实雾度来记录朦胧的场景。无朦胧和无雾度的相应场景包含在相同照明参数下捕获的相同视觉内容。密集雾度数据集旨在通过促进真实和各种模糊场景的稳健方法，显着推动单图像去雾的艺术水平。我们还基于Dense Haze数据集提供了对最先进的单图像去雾技术的全面定性和定量评估。毫不奇怪，我们的研究表明，现有的去雾技术对于密集均匀的朦胧场景表现不佳，并且仍有很大的改进空间。Snap and Find: Deep Discrete Cross-domain Garment Image RetrievalAuthors Yadan Luo, Ziwei Wang, Zi Huang, Yang Yang, Huimin Lu随着在线商店数量的增加，迫切需要智能搜索系统来理解客户拍摄的项目照片，并搜索大型产品数据库以找到他们想要的项目。然而，传统检索系统难以匹配顾客拍摄的物品照片和商店正式发布的物品照片，尤其是服装图像。为了桥接顾客和商店提供的服装照片，现有的研究已经广泛地利用服装属性文本，例如黑色和地标文本，例如衣领，以学习用于服装表示的共同嵌入空间。不幸的是，它们省略了属性的顺序关联，并消耗大量的人工来标记地标。在本文中，我们提出了一个深度多任务跨域散列称为textit DMCH，其中跨域嵌入和顺序属性学习同时建模。顺序属性学习不仅提供嵌入的语义指导，而且还产生对判别性局部细节文本的丰富关注，例如衣服项目的黑色按钮，而不需要额外的地标标签。与现有技术模型相比，这可以带来有前途的性能和306倍的效率提升，通过对两个公共时尚数据集的严格实验证明了这一点。Evading Defenses to Transferable Adversarial Examples by Translation-Invariant AttacksAuthors Yinpeng Dong, Tianyu Pang, Hang Su, Jun Zhu深度神经网络容易受到对抗性的例子的影响，这些例子可能通过添加难以察觉的扰动来误导分类器。对抗性示例的一个有趣的特性是它们良好的可转移性，使黑盒攻击在现实世界的应用程序中可行。由于对抗性攻击的威胁，已经提出了许多方法来提高鲁棒性。对于可转移的对抗性示例，几种最先进的防御被证明是强有力的。在本文中，我们提出了一种平移不变攻击方法，以针对防御模型生成更多可转移的对抗性示例。通过优化翻译图像集合上的扰动，所生成的对抗性示例对被攻击的白盒模型不太敏感并且具有更好的可转移性。为了提高攻击效率，我们进一步表明，我们的方法可以通过将未翻译图像的梯度与预定义的内核进行卷积来实现。我们的方法通常适用于任何基于梯度的攻击方法。 ImageNet数据集上的大量实验验证了所提方法的有效性。我们最好的攻击愚弄八种最先进的防御技术，平均成功率仅为82％，仅基于可转移性，证明了当前防御技术的不安全性。Multiphase Level-Set Loss for Semi-Supervised and Unsupervised Segmentation with Deep LearningAuthors Boah Kim, Jong Chul Ye由于其高性能和快速计算时间，最新的现有图像分割算法大多基于深度神经网络。然而，这些方法通常以监督方式训练，这需要大量高质量的地面真实分割掩模。另一方面，诸如水平集方法之类的经典图像分割方法对于帮助生成没有标签的分割掩模仍然是有用的，但是这些算法通常在计算上是昂贵的并且通常在语义分割中具有局限性。在本文中，我们提出了一种新的多阶段水平集损失函数，用于基于深度学习的语义图像分割，不带或带有小标记数据。该损失函数基于以下观察：深度神经网络的softmax层与经典多相水平集算法中的特征函数具有惊人的相似性。我们证明了多阶段水平集损失函数能够实现半监督甚至无监督的语义分割。此外，我们的损失函数还可以用作正则化函数来增强监督语义分割算法。多个数据集的实验结果证明了该方法的有效性。Fast Spatio-Temporal Residual Network for Video Super-ResolutionAuthors Sheng Li, Fengxiang He, Bo Du, Lefei Zhang, Yonghao Xu, Dacheng Tao最近，基于深度学习的视频超分辨率SR方法已经取得了很好的性能。为了同时利用视频的空间和时间信息，采用三维3D卷积是一种自然的方法。然而，直接利用3D卷积可能导致过高的计算复杂度，这限制了视频SR模型的深度并因此破坏了性能。在本文中，我们提出了一种新颖的快速时空残留网络FSTRN，用于视频SR任务的3D卷积，以便在保持低计算负荷的同时提高性能。具体来说，我们提出了一种快速空间时间残差块FRB，它将每个3D滤波器划分为两个3D滤波器的乘积，这两个滤波器具有相当低的尺寸。此外，我们设计了一个跨空间残差学习，直接链接低分辨率空间和高分辨率空间，这可以大大减轻特征融合和向上缩放部分的计算负担。对基准数据集的广泛评估和比较验证了所提出方法的优势，并证明所提出的网络明显优于当前的现有技术方法。Actively Seeking and Learning from Live DataAuthors Damien Teney, Anton van den Hengel传统机器学习方法的一个关键限制是它们需要训练数据，这些数据例证了所有要学习的信息。这是视觉问答方法的一个特殊问题，可能会被问到几乎任何问题。我们提出的方法是通过搜索测试时所需的信息来克服此限制的一步。生成的方法动态地利用来自外部源的数据，例如大量问题答案或图像标题。具体地说，我们学习了一组简单VQA模型的基本权重，它们特别适用于给定问题，并具有针对该问题专门检索的信息。适应过程利用基于梯度的元学习的最新进展以及对有效检索和跨域适应的贡献。我们超越了VQA CP v2基准测试的最新技术水平，并证明了我们的方法对于分发测试数据本质上更加健壮。我们演示了使用MS COCO字幕数据集来使用外部非VQA数据来支持应答过程。这种方法为开放域VQA系统开辟了一条新途径，该系统可与各种数据源进行交互。Deep Tree Learning for Zero-shot Face Anti-SpoofingAuthors Yaojie Liu, Joel Stehouwer, Amin Jourabloo, Xiaoming Liu面部反欺骗旨在使面部识别系统不会将假面部识别为真正的用户。虽然开发了先进的面部反欺骗方法，但也正在创建新类型的欺骗攻击并且对所有现有系统构成威胁。我们将未知欺骗攻击的检测定义为Zero Shot Face Anti spoofing ZSFA。以前ZSFA的作品只研究了1种类型的恶搞攻击，例如打印重放攻击，这限制了这个问题的洞察力。在这项工作中，我们将ZSFA问题扩展到13种类型的欺骗攻击，包括打印攻击，重放攻击，3D掩码攻击等。提出了一种新的深树网络DTN来解决ZSFA问题。学习树以无人监督的方式将欺骗样本划分为语义子组。当数据样本到达时，知道或未知的攻击，DTN将其路由到最相似的欺骗群集，并做出二元决策。此外，为了研究ZSFA，我们引入了第一个面部反欺骗数据库，其中包含各种类型的恶搞攻击。实验表明，我们提出的方法达到了ZSFA多种测试协议的最新技术水平。FLightNNs: Lightweight Quantized Deep Neural Networks for Fast and Accurate InferenceAuthors Ruizhou Ding, Zeye Liu, Ting Wu Chin, Diana Marculescu, R. D. Shawn Blanton为了提高深度神经网络DNN在定制硬件上的吞吐量和能量效率，轻量级神经网络将DNN的权重约束为有限组合，表示为2的2的幂中的k。在这样的网络中，乘法累加运算可以可以用单个换档操作，或两个换档和一个添加操作来替换。为了提供更多的设计灵活性，可以最佳地选择每个卷积滤波器的k而不是为每个滤波器固定。在本文中，我们将k的选择制定为可微分，并描述用于基于每个滤波器确定基于k的权重的模型训练。超过46个涉及8个配置和4个数据集的FPGA设计实验表明，具有灵活k值的轻量级神经网络（称为FLightNN）充分利用了现场可编程门阵列FPGA上的硬件资源，我们的实验结果表明，与之相比，FLightNN可以实现2倍的加速。带有k 2的轻质NN，精度降低仅为0.1。与4位定点量化相比，FLightNN由于其轻量化的换档操作而实现更高的精度和高达2倍的推理加速。此外，我们的实验还证明了FLightNN可以实现更高的ASIC实现计算能效。A Regularization Approach for Instance-Based Superset Label LearningAuthors Chen Gong, Tongliang Liu, Yuanyan Tang, Jian Yang, Jie Yang, Dacheng Tao与传统的监督学习不同，其中每个训练示例仅具有一个显式标签，超集标签学习SLL指的是训练示例可以与一组候选标签相关联的问题，并且其中只有一个是正确的。现有的SLL方法是基于正则化的或基于实例的，并且后者已经实现了最先进的性能。这是因为最新的基于实例的方法包含明确的消歧操作，该操作准确地从其模糊的候选标签中获取每个训练示例的groundtruth标签。然而，这种消歧操作并未充分考虑不同候选标签之间的互斥关系，因此消歧标签通常以非歧视方式生成，这对于基于实例的方法不利于获得令人满意的性能。为了解决这个缺陷，我们开发了一种新的正则化方法，例如基于超集标签RegISL学习，因此我们的基于实例的方法也继承了正则化方案所具有的良好判别能力。具体来说，我们使用图表来表示训练集，并要求图表上相邻的示例获得类似的标签。更重要的是，提出了一个歧视术语，以扩大可能标签之间的价值差距，并为每个培训示例提供不太可能的标签。结果，部署了不同候选标签之间的固有约束，并且由RegISL生成的消歧标签比现有基于实例的算法输出的标签更具辨别性和准确性。各种任务的实验结果令人信服地证明了我们的RegISL在训练精度和测试精度方面对其他典型SLL方法的优越性。Regularizing Activation Distribution for Training Binarized Deep NetworksAuthors Ruizhou Ding, Ting Wu Chin, Zeye Liu, Diana Marculescu二值化神经网络BNN可以显着减少资源受限设备中的推理延迟和能量消耗，因为它们具有纯逻辑计算和较少的内存访问。然而，训练BNN是困难的，因为激活流遇到退化，饱和和梯度失配问题。先前的工作通过增加激活位和增加浮点缩放因子来缓解这些问题，从而牺牲了BNN的能量效率。在本文中，我们建议使用分布损失来明确地规范激活流程，并建立一个系统地制定损失的框架。我们的实验表明，分布损失可以持续提高BNN的准确性，而不会失去其能量效益。此外，配备所提出的正则化，BNN训练被证明对于包括优化器和学习速率的超参数的选择是鲁棒的。Video Classification with Channel-Separated Convolutional NetworksAuthors Du Tran, Heng Wang, Lorenzo Torresani, Matt Feiszli已经表明，组卷积在用于图像分类的各种2D卷积体系结构中提供了显着的计算节省。很自然地问1组卷积是否有助于减轻视频分类网络的高计算成本2哪些因素在3D组卷积网络中最重要，3什么是良好的计算精度与3D组卷积网络的权衡。Assessment of Faster R-CNN in Man-Machine collaborative searchAuthors Arturo Deza, Amit Surana, Miguel P. Eckstein随着由深度学习驱动的现代专家系统的出现，补充人类专家，例如放射科医师，皮肤科医生，监视扫描仪，我们分析这些专家系统如何以及何时在细粒度小目标视觉搜索任务中提高人类表现。我们建立了一个2会话阶乘实验设计，人们可以在视觉上搜索有和没有深度学习DL专家系统的目标。我们在DL系统存在的情况下评估目标检测性能和眼睛运动的人体变化。我们发现通过具有VGG16的更快R CNN计算的DL系统的性能改进与观察者的感知能力（例如，灵敏度）相互作用。主要结果包括1 DL系统降低了高灵敏度的观察者组平均每个图像的误报率2只有高灵敏度的人类观察者比DL系统表现更好，而低灵敏度组不超过单个DL系统性能，即使在DL系统本身的帮助下3试验次数的增加和观察时间的减少主要是由DL系统仅针对低灵敏度组驱动的。 4 DL系统帮助人类观察者通过第3次固定固定在目标上。这些结果提供了与人类协作或竞争的深度学习系统的益处和局限性的见解。VQD: Visual Query Detection in Natural ScenesAuthors Manoj Acharya, Karan Jariwala, Christopher Kanan我们提出了Visual Query Detection VQD，一种新的视觉接地任务。在VQD中，系统由自然语言引导以在图像中定位可变数量的对象。 VQD与视觉引用表达识别有关，其中任务是仅对一个对象进行本地化。我们描述了VQD的第一个数据集，我们提出了基准算法，证明了与引用表达识别相比较的任务难度。Crowd Transformer NetworkAuthors Viresh Ranjan, Mubarak Shah, Minh Hoai Nguyen在本文中，我们解决了人群计数问题，并提出了一种基于人群密度估计的方法来获取人群数量。大多数现有的人群计数方法依赖于局部特征来估计人群密度图。在这项工作中，我们研究了将本地和非本地特征结合起来进行人群统计的有用性。我们使用卷积层来提取局部特征，以及一种用于提取非局部特征的自我注意机制。我们结合了本地和非本地特征，并将其用于估计人群密度图。我们对三个公开的Crowd Counting数据集进行了实验，并且比以前的方法取得了显着的改进。Biometric Fish Classification of Temperate Species Using Convolutional Neural Network with Squeeze-and-ExcitationAuthors Erlend Olsvik, Christian M. D. Trinh, Kristian Muri Knausg rd, Arne Wiklund, Tonje Knutsen S rdalen, Alf Ring Kleiven, Lei Jiao, Morten Goodwin我们对有效监测和管理沿海生态系统的理解和能力受到观察方法的严重限制。在自然环境中自动识别物种是一种很有前途的工具，它将彻底改变视频和图像分析，以适应海洋生态学的广泛应用。然而，由于水中的噪声和光照变化，从水下相机捕获的图像中对鱼进行分类通常是非常具有挑战性的。文献中的先前分类方法依赖于过滤图像以将鱼与背景分离或通过去除背景噪声来锐化图像。该预过滤过程可能对分类准确性产生负面影响。在这项工作中，我们提出了一种卷积神经网络CNN，它使用挤压和激励SE架构对鱼类图像进行分类而无需预先过滤。与传统方案不同，该方案分为两个步骤。第一步是通过公共数据集（即Fish4Knowledge）训练鱼类分类器，而不使用图像增强，称为预训练。第二步是基于由我们感兴趣的物种组成的新数据集训练分类器，命名为后训练。从训练前获得的权重作为先验应用于训练后。这也称为转学习。我们的解决方案在预训练中实现了99.27精度的最新精度。培训后的准确率为83.68。使用图像增强进行后训练的实验产生了87.74的准确度，表明该解决方案对于更大的数据集是可行的。Learning Implicit Generative Models by Matching Perceptual FeaturesAuthors Cicero Nogueira dos Santos, Youssef Mroueh, Inkit Padhi, Pierre Dognin感知功能PF已经在转学习，风格转移和超分辨率等任务中取得了巨大成功。然而，PFs作为学习生成模型的关键信息来源的功效尚未得到很好的研究。我们在此研究PF在通过矩匹配MM学习隐式生成模型的背景下的使用。更具体地说，我们提出了一种新的有效MM方法，通过对从预训练的ConvNets中提取的特征进行均值和协方差匹配来学习隐式生成模型。我们提出的方法改进了现有的MM方法，从而摆脱了对抗性学习的有问题的最大游戏2，避免了内核函数的在线学习，并且3对于使用的时刻和所需的小批量大小都是有效的。我们的实验结果表明，由于来自预训练深度ConvNets的PF的表现力，我们的方法实现了具有挑战性基准的最先进结果。Blind Visual Motif Removal from a Single ImageAuthors Amir Hertz, Sharon Fogel, Rana Hanocka, Raja Giryes, Daniel Cohen Or在网络上共享的许多图像包括重叠的对象，或视觉图案，例如文本，符号或绘图，其向图像添加描述或装饰。例如，指定图像拍摄位置的装饰性文本会在各种不同的图像中重复出现。通常，重复出现的视觉主题在语义上相似，但在位置，风格和内容方面不同，例如，文字放置，字体和字母。这项工作提出了一种基于深度学习的技术，用于盲目去除这些物体。在盲区中，图案的位置和精确几何形状是未知的。我们的方法同时估计哪些像素包含视觉主题，并合成潜在的潜在图像。它被应用于单个输入图像，在指定图案的位置时没有任何用户帮助，实现了用于盲目去除不透明和半透明视觉图案的现有技术结果。DeceptionNet: Network-Driven Domain RandomizationAuthors Sergey Zakharov, Wadim Kehl, Slobodan Ilic我们提出了一种新方法来解决合成数据和实际数据之间的域适应问题。我们不是采用盲域随机化，即增加具有随机背景或改变照明和着色的合成渲染，而是利用任务网络作为其自身的对抗指南，以实现有用的增强，从而最大化输出的不确定性。为此，我们设计了一个最小最大优化方案，其中给定任务与特殊欺骗网络竞争，目标是根据欺骗者强制执行的特定约束最小化任务错误。欺骗网络从一系列可微分像素级扰动中采样，并利用任务架构来找到最具破坏性的增强。与需要来自目标域的未标记数据的基于GAN的方法不同，我们的方法实现了强大的映射，可以很好地从源数据单独扩展到多个目标分布。我们将我们的框架应用于增强的MNIST变体上的数字识别任务以及Cropped LineMOD数据集上的分类和对象姿态估计，并与许多域自适应方法进行比较，展示具有优异泛化能力的类似结果。Learning to Cluster Faces on an Affinity GraphAuthors Lei Yang, Xiaohang Zhan, Dapeng Chen, Junjie Yan, Chen Change Loy, Dahua Lin面部识别近年来取得了显着进步，其表现达到了很高的水平。将其提升到一个新的水平需要更大的数据，这将涉及过高的注释成本。因此，利用未标记的数据成为一种有吸引力的选择。最近的工作表明，聚集未标记的面是一种很有前景的方法，通常会带来显着的性能提升。然而，如何有效地聚类，特别是在大规模，即百万级或更高级别的数据集上，仍然是一个悬而未决的问题。一个关键的挑战在于群集模式的复杂变化，这使得传统的聚类方法难以满足所需的准确性。这项工作探索了一种新颖的方法，即学习集群而不是依靠手工制作的标准。具体来说，我们提出了一个基于图卷积网络的框架，它结合了检测和分割模块来精确定位面部聚类。实验表明，我们的方法可以产生更精确的面部聚类，从而也可以在人脸识别中获得进一步的性能提升。Learning to Remember: A Synaptic Plasticity Driven Framework for Continual LearningAuthors Oleksiy Ostapenko, Mihai Puscas, Tassilo Klein, Patrick J hnichen, Moin Nabi在持续学习CL的背景下训练的模型应该能够在不确定的时间段内从数据流中学习。这里面临的主要挑战是保持旧知识，同时在学习新任务时从中受益，2保证模型可扩展性，并且需要学习越来越多的数据。为了应对这些挑战，我们引入了动态生成记忆DGM，这是一种用于持续学习的突触可塑性驱动框架。 DGM依赖于条件生成对抗网络，通过神经掩蔽实现可学习的连接可塑性。具体来说，我们评估应用于i层激活的两种神经掩蔽变体，以及ii直接连接权重。此外，我们提出了一种动态网络扩展机制，可确保足够的模型容量以适应不断传入的任务。增加的容量是从学习的二进制掩码动态确定的。我们在视觉分类任务的连续类增量设置中评估DGM。Controlling Neural Networks via Energy DissipationAuthors Michael Moeller, Thomas M llenhoff, Daniel Cremers过去十年在借助深度学习技术解决各种计算机视觉问题方面取得了巨大成功。最近，许多作品已经证明，具有合适的网络架构的基于学习的方法甚至表现出用于解决诸如去模糊，超分辨率或医学图像重建之类的病态图像重建问题的优越性能。然而，纯粹基于学习的方法的缺点是它们不能为训练的网络在推理期间遵循给定的数据形成过程提供可证实的保证。在这项工作中，我们提出了能量消散网络，它迭代地计算相对于当前估计重建的给定成本函数或能量的下降方向。因此，诸如线搜索的自适应步长规则以及适当数量的迭代可以保证重建遵循以能量编码的给定数据形成模型到任意精度，并因此即使在测试时间期间也控制模型的行为。我们证明，在标准假设下，使用网络预测的方向下降线性收敛到全球最小能量。我们在单图像超分辨率和计算机断层扫描CT重建的实验中说明了所提出的方法的有效性，并进一步说明了凸可行性问题的扩展。A Hybrid Approach with Optimization and Metric-based Meta-Learner for Few-Shot LearningAuthors Duo Wang, Yu Cheng, Mo Yu, Xiaoxiao Guo, Tao Zhang几乎没有镜头学习的目的是学习新课程的分类器，每班只有一些训练样例。大多数现有的几种镜头学习方法属于基于度量的元学习或基于优化的元学习类别，两者都在简化的k shot N way图像分类设置中取得了成功。具体地，基于优化的方法训练元学习器以预测任务特定分类器的参数。特定于任务的分类器需要是同构的，以便于参数预测，因此元学习方法只能处理几个镜头学习问题，其中任务共享统一数量的类。基于度量的方法为所有任务学习一个任务不变度量。即使度量学习方法允许不同数量的类，它们也要求所有来自类似域的任务，以便存在可以跨任务工作的统一度量。在这项工作中，我们提出了一个名为Meta Metric Learner的混合元学习模型，它结合了优化和基于度量的方法的优点。我们的元度量学习方法包括两个组件，一个基于任务特定度量的学习者作为基础模型，以及一个学习和指定基础模型的元学习器。因此，我们的模型能够处理灵活数量的类，并为任务之间的分类生成更通用的度量标准。我们在标准k镜头N方式测试我们的方法以前的工作之后的几个镜头学习设置和在单一源形式和多源形式中具有灵活类别编号的新的实际几个镜头设置。实验表明，我们的方法可以在所有设置中获得卓越的性能。Blind Deconvolution Microscopy Using Cycle Consistent CNN with Explicit PSF LayerAuthors Sungjun Lim, Sang Eun Lee, Sunghoe Chang, Jong Chul Ye反卷积显微镜已被广泛用于改善宽场荧光显微镜的分辨率。然而，通常需要点扩散函数PSF测量或盲估计的常规方法在计算上是昂贵的。最近，基于CNN的方法已被探索为快速和高性能的替代方案。在本文中，我们提出了一种新的无监督深度神经网络，用于基于循环一致性和PSF建模层的盲反褶积。与最近针对类似问题的CNN方法相比，显式PSF建模层提高了算法的鲁棒性。实验结果证实了该算法的有效性。Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 HoursAuthors Dimitrios Stamoulis, Ruizhou Ding, Di Wang, Dimitrios Lymberopoulos, Bodhi Priyantha, Jie Liu, Diana Marculescu我们能否在移动设备的运行时约束下自动设计具有最高图像分类精度的卷积网络ConvNet神经架构搜索NAS通过自动化此过程彻底改变了硬件高效ConvNets的设计。然而，由于组合大的设计空间，NAS问题仍然具有挑战性，导致至少200个GPU小时的显着搜索时间。为了减轻这种复杂性，我们提出了单路径NAS，这是一种新颖的可区分NAS方法，可在不到4小时内设计出硬件高效的ConvNets。我们的研究成果如下1.单路径搜索空间相比前微NAS方法，单路径NAS使用一个单一的路径，通过参数ConvNet编码所有架构决策与共享卷积内核参数，从而极大地降低训练的参数和搜索的数量成本低至几个时代。 2.硬件高效的ImageNet分类单路径NAS在ImageNet上实现了74.96的前1精度，在Pixel 1手机上具有79ms的延迟，与具有类似约束80ms的NAS方法相比，这是最先进的精度。 3. NAS效率单路径NAS搜索成本仅为8个时段30 TPU小时，与之前的工作相比，速度提高了5,000倍。 4.再现性与最近只发布预训练模型的所有移动高效NAS方法不同，我们开源整个代码库Deep Learning-based Universal Beamformer for Ultrasound ImagingAuthors Shujaat Khan, Jaeyoung Huh, Jong Chul Ye在超声US成像中，在应用特定延迟之后，各个通道RF测量被反向传播和累积以形成图像。虽然这种时间反转通常使用基于硬件或软件的延迟和求和DAS波束形成器来实现，但是在数据采集不理想的情况下，DAS的性能迅速降低。在这里，我们首次证明了设计为深度神经网络的单个数据驱动波束形成器可以直接处理以不同采样率采集的子采样RF数据，以生成高质量的US图像。特别地，所提出的深波束形成器被评估用于聚焦超声成像和平面波成像的两种不同采集方案。Neural Models of the Psychosemantics of `Most'Authors Lewis O Sullivan, Shane Steinert Threlkeld语言表达的含义与它们在具体认知任务中的使用有何关联视觉识别任务表明，人类说话者在理解，表达和验证某些量词方面可能表现出相当大的差异。本文开始研究这些心理语义任务的神经模型。我们在citet Pietroski2009的最大验证任务中训练了两种类型的网络卷积神经网络CNN模型和视觉注意RAM的循环模型，操纵视觉场景和任务持续时间的新概念。我们的结果定性地反映了人类表现的某些特征，例如对设定大小的比率的敏感性，表明依赖于近似数字，同时以有趣的方式不同，例如对于图像类型的效果呈现微妙不同的图案。最后，我们讨论了使用神经模型作为这个和其他心理学任务的认知模型的前景。Chinese Abs From Machine TranslationPapers from arxiv.org更多精彩请移步主页pic from pixels.com"}
{"content2":"机器学习是什么，为什么它能有这么大的魔力，这些问题正是本文要回答的。同时，本文叫做“从机器学习谈起”，因此会以漫谈的形式介绍跟机器学习相关的所有内容，包括学科(如数据挖掘、计算机视觉等)，算法(神经网络，svm)等等。本文的主要目录如下：1.一个故事说明什么是机器学习2.机器学习的定义3.机器学习的范围4.机器学习的方法5.机器学习的应用--大数据6.机器学习的子类--深度学习7.机器学习的父类--人工智能8.机器学习的思考--计算机的潜意识9.总结10.后记1.一个故事说明什么是机器学习机器学习这个词是让人疑惑的，首先它是英文名称Machine Learning(简称ML)的直译，在计算界Machine一般指计算机。这个名字使用了拟人的手法，说明了这门技术是让机器“学习”的技术。但是计算机是死的，怎么可能像人类一样“学习”呢？传统上如果我们想让计算机工作，我们给它一串指令，然后它遵照这个指令一步步执行下去。有因有果，非常明确。但这样的方式在机器学习中行不通。机器学习根本不接受你输入的指令，相反，它接受你输入的数据! 也就是说，机器学习是一种让计算机利用数据而不是指令来进行各种工作的方法。这听起来非常不可思议，但结果上却是非常可行的。“统计”思想将在你学习“机器学习”相关理念时无时无刻不伴随，相关而不是因果的概念将是支撑机器学习能够工作的核心概念。你会颠覆对你以前所有程序中建立的因果无处不在的根本理念。下面我通过一个故事来简单地阐明什么是机器学习。这个故事比较适合用在知乎上作为一个概念的阐明。在这里，这个故事没有展开，但相关内容与核心是存在的。如果你想简单的了解一下什么是机器学习，那么看完这个故事就足够了。如果你想了解机器学习的更多知识以及与它关联紧密的当代技术，那么请你继续往下看，后面有更多的丰富的内容。这个例子来源于我真实的生活经验，我在思考这个问题的时候突然发现它的过程可以被扩充化为一个完整的机器学习的过程，因此我决定使用这个例子作为所有介绍的开始。这个故事称为“等人问题”。我相信大家都有跟别人相约，然后等人的经历。现实中不是每个人都那么守时的，于是当你碰到一些爱迟到的人，你的时间不可避免的要浪费。我就碰到过这样的一个例子。对我的一个朋友小Y而言，他就不是那么守时，最常见的表现是他经常迟到。当有一次我跟他约好3点钟在某个麦当劳见面时，在我出门的那一刻我突然想到一个问题：我现在出发合适么？我会不会又到了地点后，花上30分钟去等他？我决定采取一个策略解决这个问题。要想解决这个问题，有好几种方法。第一种方法是采用知识：我搜寻能够解决这个问题的知识。但很遗憾，没有人会把如何等人这个问题作为知识传授，因此我不可能找到已有的知识能够解决这个问题。第二种方法是问他人：我去询问他人获得解决这个问题的能力。但是同样的，这个问题没有人能够解答，因为可能没人碰上跟我一样的情况。第三种方法是准则法：我问自己的内心，我有否设立过什么准则去面对这个问题？例如，无论别人如何，我都会守时到达。但我不是个死板的人，我没有设立过这样的规则。事实上，我相信有种方法比以上三种都合适。我把过往跟小Y相约的经历在脑海中重现一下，看看跟他相约的次数中，迟到占了多大的比例。而我利用这来预测他这次迟到的可能性。如果这个值超出了我心里的某个界限，那我选择等一会再出发。假设我跟小Y约过5次，他迟到的次数是1次，那么他按时到的比例为80%，我心中的阈值为70%，我认为这次小Y应该不会迟到，因此我按时出门。如果小Y在5次迟到的次数中占了4次，也就是他按时到达的比例为20%，由于这个值低于我的阈值，因此我选择推迟出门的时间。这个方法从它的利用层面来看，又称为经验法。在经验法的思考过程中，我事实上利用了以往所有相约的数据。因此也可以称之为依据数据做的判断。依据数据所做的判断跟机器学习的思想根本上是一致的。刚才的思考过程我只考虑“频次”这种属性。在真实的机器学习中，这可能都不算是一个应用。一般的机器学习模型至少考虑两个量：一个是因变量，也就是我们希望预测的结果，在这个例子里就是小Y迟到与否的判断。另一个是自变量，也就是用来预测小Y是否迟到的量。假设我把时间作为自变量，譬如我发现小Y所有迟到的日子基本都是星期五，而在非星期五情况下他基本不迟到。于是我可以建立一个模型，来模拟小Y迟到与否跟日子是否是星期五的概率。见下图：这样的图就是一个最简单的机器学习模型，称之为决策树。当我们考虑的自变量只有一个时，情况较为简单。如果把我们的自变量再增加一个。例如小Y迟到的部分情况时是在他开车过来的时候(你可以理解为他开车水平较臭，或者路较堵)。于是我可以关联考虑这些信息。建立一个更复杂的模型，这个模型包含两个自变量与一个因变量。再更复杂一点，小Y的迟到跟天气也有一定的原因，例如下雨的时候，这时候我需要考虑三个自变量。如果我希望能够预测小Y迟到的具体时间，我可以把他每次迟到的时间跟雨量的大小以及前面考虑的自变量统一建立一个模型。于是我的模型可以预测值，例如他大概会迟到几分钟。这样可以帮助我更好的规划我出门的时间。在这样的情况下，决策树就无法很好地支撑了，因为决策树只能预测离散值。我们可以用节2所介绍的线型回归方法建立这个模型。如果我把这些建立模型的过程交给电脑。比如把所有的自变量和因变量输入，然后让计算机帮我生成一个模型，同时让计算机根据我当前的情况，给出我是否需要迟出门，需要迟几分钟的建议。那么计算机执行这些辅助决策的过程就是机器学习的过程。机器学习方法是计算机利用已有的数据(经验)，得出了某种模型(迟到的规律)，并利用此模型预测未来(是否迟到)的一种方法。通过上面的分析，可以看出机器学习与人类思考的经验过程是类似的，不过它能考虑更多的情况，执行更加复杂的计算。事实上，机器学习的一个主要目的就是把人类思考归纳经验的过程转化为计算机通过对数据的处理计算得出模型的过程。经过计算机得出的模型能够以近似于人的方式解决很多灵活复杂的问题。下面，我会开始对机器学习的正式介绍，包括定义、范围，方法、应用等等，都有所包含。2.机器学习的定义从广义上来说，机器学习是一种能够赋予机器学习的能力以此让它完成直接编程无法完成的功能的方法。但从实践的意义上来说，机器学习是一种通过利用数据，训练出模型，然后使用模型预测的一种方法。让我们具体看一个例子。拿国民话题的房子来说。现在我手里有一栋房子需要售卖，我应该给它标上多大的价格？房子的面积是100平方米，价格是100万，120万，还是140万？很显然，我希望获得房价与面积的某种规律。那么我该如何获得这个规律？用报纸上的房价平均数据么？还是参考别人面积相似的？无论哪种，似乎都并不是太靠谱。我现在希望获得一个合理的，并且能够最大程度的反映面积与房价关系的规律。于是我调查了周边与我房型类似的一些房子，获得一组数据。这组数据中包含了大大小小房子的面积与价格，如果我能从这组数据中找出面积与价格的规律，那么我就可以得出房子的价格。对规律的寻找很简单，拟合出一条直线，让它“穿过”所有的点，并且与各个点的距离尽可能的小。通过这条直线，我获得了一个能够最佳反映房价与面积规律的规律。这条直线同时也是一个下式所表明的函数：房价 = 面积 * a + b上述中的a、b都是直线的参数。获得这些参数以后，我就可以计算出房子的价格。假设a = 0.75,b = 50，则房价 = 100 * 0.75 + 50 = 125万。这个结果与我前面所列的100万，120万，140万都不一样。由于这条直线综合考虑了大部分的情况，因此从“统计”意义上来说，这是一个最合理的预测。在求解过程中透露出了两个信息：1.房价模型是根据拟合的函数类型决定的。如果是直线，那么拟合出的就是直线方程。如果是其他类型的线，例如抛物线，那么拟合出的就是抛物线方程。机器学习有众多算法，一些强力算法可以拟合出复杂的非线性模型，用来反映一些不是直线所能表达的情况。2.如果我的数据越多，我的模型就越能够考虑到越多的情况，由此对于新情况的预测效果可能就越好。这是机器学习界“数据为王”思想的一个体现。一般来说(不是绝对)，数据越多，最后机器学习生成的模型预测的效果越好。通过我拟合直线的过程，我们可以对机器学习过程做一个完整的回顾。首先，我们需要在计算机中存储历史的数据。接着，我们将这些 数据通过机器学习算法进行处理，这个过程在机器学习中叫做“训练”，处理的结果可以被我们用来对新的数据进行预测，这个结果一般称之为“模型”。对新数据 的预测过程在机器学习中叫做“预测”。“训练”与“预测”是机器学习的两个过程，“模型”则是过程的中间输出结果，“训练”产生“模型”，“模型”指导 “预测”。让我们把机器学习的过程与人类对历史经验归纳的过程做个比对。人类在成长、生活过程中积累了很多的历史与经验。人类定期地对这些经验进行“归纳”，获得了生活的“规律”。当人类遇到未知的问题或者需要对未来进行“推测”的时候，人类使用这些“规律”，对未知问题与未来进行“推测”，从而指导自己的生活和工作。机器学习中的“训练”与“预测”过程可以对应到人类的“归纳”和“推测”过程。通过这样的对应，我们可以发现，机器学习的思想并不复杂，仅仅是对人类在生活中学习成长的一个模拟。由于机器学习不是基于编程形成的结果，因此它的处理过程不是因果的逻辑，而是通过归纳思想得出的相关性结论。这也可以联想到人类为什么要学习历史，历史实际上是人类过往经验的总结。有句话说得很好，“历史往往不一样，但历史总是惊人的相似”。通过学习历史，我们从历史中归纳出人生与国家的规律，从而指导我们的下一步工作，这是具有莫大价值的。当代一些人忽视了历史的本来价值，而是把其作为一种宣扬功绩的手段，这其实是对历史真实价值的一种误用。3.机器学习的范围上文虽然说明了机器学习是什么，但是并没有给出机器学习的范围。其实，机器学习跟模式识别，统计学习，数据挖掘，计算机视觉，语音识别，自然语言处理等领域有着很深的联系。从范围上来说，机器学习跟模式识别，统计学习，数据挖掘是类似的，同时，机器学习与其他领域的处理技术的结合，形成了计算机视觉、语音识别、自然语言处理等交叉学科。因此，一般说数据挖掘时，可以等同于说机器学习。同时，我们平常所说的机器学习应用，应该是通用的，不仅仅局限在结构化数据，还有图像，音频等应用。在这节对机器学习这些相关领域的介绍有助于我们理清机器学习的应用场景与研究范围，更好的理解后面的算法与应用层次。下图是机器学习所牵扯的一些相关范围的学科与研究领域。模式识别模式识别=机器学习。两者的主要区别在于前者是从工业界发展起来的概念，后者则主要源自计算机学科。在著名的《Pattern Recognition And Machine Learning》这本书中，Christopher M. Bishop在开头是这样说的“模式识别源自工业界，而机器学习来自于计算机学科。不过，它们中的活动可以被视为同一个领域的两个方面，同时在过去的10年间，它们都有了长足的发展”。数据挖掘数据挖掘=机器学习+数据库。这几年数据挖掘的概念实在是太耳熟能详。几乎等同于炒作。但凡说数据挖掘都会吹嘘数据挖掘如何如何，例如从数据中挖出金子，以及将废弃的数据转化为价值等等。但是，我尽管可能会挖出金子，但我也可能挖的是“石头”啊。这个说法的意思是，数据挖掘仅仅是一种思考方式，告诉我们应该尝试从数据中挖掘出知识，但不是每个数据都能挖掘出金子的，所以不要神话它。一个系统绝对不会因为上了一个数据挖掘模块就变得无所不能(这是IBM最喜欢吹嘘的)，恰恰相反，一个拥有数据挖掘思维的人员才是关键，而且他还必须对数据有深刻的认识，这样才可能从数据中导出模式指引业务的改善。大部分数据挖掘中的算法是机器学习的算法在数据库中的优化。统计学习统计学习近似等于机器学习。统计学习是个与机器学习高度重叠的学科。因为机器学习中的大多数方法来自统计学，甚至可以认为，统计学的发展促进机器学习的繁荣昌盛。例如著名的支持向量机算法，就是源自统计学科。但是在某种程度上两者是有分别的，这个分别在于：统计学习者重点关注的是统计模型的发展与优化，偏数学，而机器学习者更关注的是能够解决问题，偏实践，因此机器学习研究者会重点研究学习算法在计算机上执行的效率与准确性的提升。计算机视觉计算机视觉=图像处理+机器学习。图像处理技术用于将图像处理为适合进入机器学习模型中的输入，机器学习则负责从图像中识别出相关的模式。计算机视觉相关的应用非常的多，例如百度识图、手写字符识别、车牌识别等等应用。这个领域是应用前景非常火热的，同时也是研究的热门方向。随着机器学习的新领域深度学习的发展，大大促进了计算机图像识别的效果，因此未来计算机视觉界的发展前景不可估量。语音识别语音识别=语音处理+机器学习。语音识别就是音频处理技术与机器学习的结合。语音识别技术一般不会单独使用，一般会结合自然语言处理的相关技术。目前的相关应用有苹果的语音助手siri等。自然语言处理自然语言处理=文本处理+机器学习。自然语言处理技术主要是让机器理解人类的语言的一门领域。在自然语言处理技术中，大量使用了编译原理相关的技术，例如词法分析，语法分析等等，除此之外，在理解这个层面，则使用了语义理解，机器学习等技术。作为唯一由人类自身创造的符号，自然语言处理一直是机器学习界不断研究的方向。按照百度机器学习专家余凯的说法“听与看，说白了就是阿猫和阿狗都会的，而只有语言才是人类独有的”。如何利用机器学习技术进行自然语言的的深度理解，一直是工业和学术界关注的焦点。可以看出机器学习在众多领域的外延和应用。机器学习技术的发展促使了很多智能领域的进步，改善着我们的生活。4.机器学习的方法通过上节的介绍我们知晓了机器学习的大致范围，那么机器学习里面究竟有多少经典的算法呢？在这个部分我会简要介绍一下机器学习中的经典代表方法。这部分介绍的重点是这些方法内涵的思想，数学与实践细节不会在这讨论。1、回归算法在大部分机器学习课程中，回归算法都是介绍的第一个算法。原因有两个：一.回归算法比较简单，介绍它可以让人平滑地从统计学迁移到机器学习中。二.回归算法是后面若干强大算法的基石，如果不理解回归算法，无法学习那些强大的算法。回归算法有两个重要的子类：即线性回归和逻辑回归。线性回归就是我们前面说过的房价求解问题。如何拟合出一条直线最佳匹配我所有的数据？一般使用“最小二乘法”来求解。“最小二乘法”的思想是这样的，假设我们拟合出的直线代表数据的真实值，而观测到的数据代表拥有误差的值。为了尽可能减小误差的影响，需要求解一条直线使所有误差的平方和最小。最小二乘法将最优问题转化为求函数极值问题。函数极值在数学上我们一般会采用求导数为0的方法。但这种做法并不适合计算机，可能求解不出来，也可能计算量太大。计算机科学界专门有一个学科叫“数值计算”，专门用来提升计算机进行各类计算时的准确性和效率问题。例如，著名的“梯度下降”以及“牛顿法”就是数值计算中的经典算法，也非常适合来处理求解函数极值的问题。梯度下降法是解决回归模型中最简单且有效的方法之一。从严格意义上来说，由于后文中的神经网络和推荐算法中都有线性回归的因子，因此梯度下降法在后面的算法实现中也有应用。逻辑回归是一种与线性回归非常类似的算法，但是，从本质上讲，线型回归处理的问题类型与逻辑回归不一致。线性回归处理的是数值问题，也就是最后预测出的结果是数字，例如房价。而逻辑回归属于分类算法，也就是说，逻辑回归预测结果是离散的分类，例如判断这封邮件是否是垃圾邮件，以及用户是否会点击此广告等等。实现方面的话，逻辑回归只是对对线性回归的计算结果加上了一个Sigmoid函数，将数值结果转化为了0到1之间的概率(Sigmoid函数的图像一般来说并不直观，你只需要理解对数值越大，函数越逼近1，数值越小，函数越逼近0)，接着我们根据这个概率可以做预测，例如概率大于0.5，则这封邮件就是垃圾邮件，或者肿瘤是否是恶性的等等。从直观上来说，逻辑回归是画出了一条分类线，见下图。假设我们有一组肿瘤患者的数据，这些患者的肿瘤中有些是良性的(图中的蓝色点)，有些是恶性的(图中的红色点)。这里肿瘤的红蓝色可以被称作数据的“标签”。同时每个数据包括两个“特征”：患者的年龄与肿瘤的大小。我们将这两个特征与标签映射到这个二维空间上，形成了我上图的数据。当我有一个绿色的点时，我该判断这个肿瘤是恶性的还是良性的呢？根据红蓝点我们训练出了一个逻辑回归模型，也就是图中的分类线。这时，根据绿点出现在分类线的左侧，因此我们判断它的标签应该是红色，也就是说属于恶性肿瘤。逻辑回归算法划出的分类线基本都是线性的(也有划出非线性分类线的逻辑回归，不过那样的模型在处理数据量较大的时候效率会很低)，这意味着当两类之间的界线不是线性时，逻辑回归的表达能力就不足。下面的两个算法是机器学习界最强大且重要的算法，都可以拟合出非线性的分类线。2、神经网络神经网络(也称之为人工神经网络，ANN)算法是80年代机器学习界非常流行的算法，不过在90年代中途衰落。现在，携着“深度学习”之势，神经网络重装归来，重新成为最强大的机器学习算法之一。神经网络的诞生起源于对大脑工作机理的研究。早期生物界学者们使用神经网络来模拟大脑。机器学习的学者们使用神经网络进行机器学习的实验，发现在视觉与语音的识别上效果都相当好。在BP算法(加速神经网络训练过程的数值算法)诞生以后，神经网络的发展进入了一个热潮。BP算法的发明人之一是前面介绍的机器学习大牛Geoffrey Hinton(中的中间者)。具体说来，神经网络的学习机理是什么？简单来说，就是分解与整合。在著名的Hubel-Wiesel试验中，学者们研究猫的视觉分析机理是这样的。Hubel-Wiesel试验与大脑视觉机理比方说，一个正方形，分解为四个折线进入视觉处理的下一层中。四个神经元分别处理一个折线。每个折线再继续被分解为两条直线，每条直线再被分解为黑白两个面。于是，一个复杂的图像变成了大量的细节进入神经元，神经元处理以后再进行整合，最后得出了看到的是正方形的结论。这就是大脑视觉识别的机理，也是神经网络工作的机理。让我们看一个简单的神经网络的逻辑架构。在这个网络中，分成输入层，隐藏层，和输出层。输入层负责接收信号，隐藏层负责对数据的分解与处理，最后的结果被整合到输出层。每层中的一个圆代表一个处理单元，可以认为是模拟了一个神经元，若干个处理单元组成了一个层，若干个层再组成了一个网络，也就是\"神经网络\"。在神经网络中，每个处理单元事实上就是一个逻辑回归模型，逻辑回归模型接收上层的输入，把模型的预测结果作为输出传输到下一个层次。通过这样的过程，神经网络可以完成非常复杂的非线性分类。下图会演示神经网络在图像识别领域的一个著名应用，这个程序叫做LeNet，是一个基于多个隐层构建的神经网络。通过LeNet可以识别多种手写数字，并且达到很高的识别精度与拥有较好的鲁棒性。LeNet的效果展示右下方的方形中显示的是输入计算机的图像，方形上方的红色字样“answer”后面显示的是计算机的输出。左边的三条竖直的图像列显示的是神经网络中三个隐藏层的输出，可以看出，随着层次的不断深入，越深的层次处理的细节越低，例如层3基本处理的都已经是线的细节了。LeNet的发明人就是前文介绍过的机器学习的大牛Yann LeCun(右者)。进入90年代，神经网络的发展进入了一个瓶颈期。其主要原因是尽管有BP算法的加速，神经网络的训练过程仍然很困难。因此90年代后期支持向量机(SVM)算法取代了神经网络的地位。3、SVM（支持向量机）支持向量机算法是诞生于统计学习界，同时在机器学习界大放光彩的经典算法。支持向量机算法从某种意义上来说是逻辑回归算法的强化：通过给予逻辑回归算法更严格的优化条件，支持向量机算法可以获得比逻辑回归更好的分类界线。但是如果没有某类函数技术，则支持向量机算法最多算是一种更好的线性分类技术。但是，通过跟高斯“核”的结合，支持向量机可以表达出非常复杂的分类界线，从而达成很好的的分类效果。“核”事实上就是一种特殊的函数，最典型的特征就是可以将低维的空间映射到高维的空间。例如下图所示：我们如何在二维平面划分出一个圆形的分类界线？在二维平面可能会很困难，但是通过“核”可以将二维空间映射到三维空间，然后使用一个线性平面就可以达成类似效果。也就是说，二维平面划分出的非线性分类界线可以等价于三维平面的线性分类界线。于是，我们可以通过在三维空间中进行简单的线性划分就可以达到在二维平面中的非线性划分效果。支持向量机是一种数学成分很浓的机器学习算法（相对的，神经网络则有生物科学成分）。在算法的核心步骤中，有一步证明，即将数据从低维映射到高维不会带来最后计算复杂性的提升。于是，通过支持向量机算法，既可以保持计算效率，又可以获得非常好的分类效果。因此支持向量机在90年代后期一直占据着机器学习中最核心的地位，基本取代了神经网络算法。直到现在神经网络借着深度学习重新兴起，两者之间才又发生了微妙的平衡转变。4、聚类算法前面的算法中的一个显著特征就是我的训练数据中包含了标签，训练出的模型可以对其他未知数据预测标签。在下面的算法中，训练数据都是不含标签的，而算法的目的则是通过训练，推测出这些数据的标签。这类算法有一个统称，即无监督算法(前面有标签的数据的算法则是有监督算法)。无监督算法中最典型的代表就是聚类算法。让我们还是拿一个二维的数据来说，某一个数据包含两个特征。我希望通过聚类算法，给他们中不同的种类打上标签，我该怎么做呢？简单来说，聚类算法就是计算种群中的距离，根据距离的远近将数据划分为多个族群。聚类算法中最典型的代表就是K-Means算法。5、降维算法降维算法也是一种无监督学习算法，其主要特征是将数据从高维降低到低维层次。在这里，维度其实表示的是数据的特征量的大小，例如，房价包含房子的长、宽、面积与房间数量四个特征，也就是维度为4维的数据。可以看出来，长与宽事实上与面积表示的信息重叠了，例如面积=长 × 宽。通过降维算法我们就可以去除冗余信息，将特征减少为面积与房间数量两个特征，即从4维的数据压缩到2维。于是我们将数据从高维降低到低维，不仅利于表示，同时在计算上也能带来加速。刚才说的降维过程中减少的维度属于肉眼可视的层次，同时压缩也不会带来信息的损失(因为信息冗余了)。如果肉眼不可视，或者没有冗余的特征，降维算法也能工作，不过这样会带来一些信息的损失。但是，降维算法可以从数学上证明，从高维压缩到的低维中最大程度地保留了数据的信息。因此，使用降维算法仍然有很多的好处。降维算法的主要作用是压缩数据与提升机器学习其他算法的效率。通过降维算法，可以将具有几千个特征的数据压缩至若干个特征。另外，降维算法的另一个好处是数据的可视化，例如将5维的数据压缩至2维，然后可以用二维平面来可视。降维算法的主要代表是PCA算法(即主成分分析算法)。6、推荐算法推荐算法是目前业界非常火的一种算法，在电商界，如亚马逊，天猫，京东等得到了广泛的运用。推荐算法的主要特征就是可以自动向用户推荐他们最感兴趣的东西，从而增加购买率，提升效益。推荐算法有两个主要的类别：一类是基于物品内容的推荐，是将与用户购买的内容近似的物品推荐给用户，这样的前提是每个物品都得有若干个标签，因此才可以找出与用户购买物品类似的物品，这样推荐的好处是关联程度较大，但是由于每个物品都需要贴标签，因此工作量较大。另一类是基于用户相似度的推荐，则是将与目标用户兴趣相同的其他用户购买的东西推荐给目标用户，例如小A历史上买了物品B和C，经过算法分析，发现另一个与小A近似的用户小D购买了物品E，于是将物品E推荐给小A。两类推荐都有各自的优缺点，在一般的电商应用中，一般是两类混合使用。推荐算法中最有名的算法就是协同过滤算法。7、其他除了以上算法之外，机器学习界还有其他的如高斯判别，朴素贝叶斯，决策树等等算法。但是上面列的六个算法是使用最多，影响最广，种类最全的典型。机器学习界的一个特色就是算法众多，发展百花齐放。下面做一个总结，按照训练的数据有无标签，可以将上面算法分为监督学习算法和无监督学习算法，但推荐算法较为特殊，既不属于监督学习，也不属于非监督学习，是单独的一类。监督学习算法：线性回归，逻辑回归，神经网络，SVM无监督学习算法：聚类算法，降维算法特殊算法：推荐算法除了这些算法以外，有一些算法的名字在机器学习领域中也经常出现。但他们本身并不算是一个机器学习算法，而是为了解决某个子问题而诞生的。你可以理解他们为以上算法的子算法，用于大幅度提高训练过程。其中的代表有：梯度下降法，主要运用在线型回归，逻辑回归，神经网络，推荐算法中；牛顿法，主要运用在线型回归中；BP算法，主要运用在神经网络中；SMO算法，主要运用在SVM中。5.机器学习的应用--大数据说完机器学习的方法，下面要谈一谈机器学习的应用了。无疑，在2010年以前，机器学习的应用在某些特定领域发挥了巨大的作用，如车牌识别，网络攻击防范，手写字符识别等等。但是，从2010年以后，随着大数据概念的兴起，机器学习大量的应用都与大数据高度耦合，几乎可以认为大数据是机器学习应用的最佳场景。譬如，但凡你能找到的介绍大数据魔力的文章，都会说大数据如何准确准确预测到了某些事。例如经典的Google利用大数据预测了H1N1在美国某小镇的爆发。Google成功预测H1N1百度预测2014年世界杯，从淘汰赛到决赛全部预测正确。这些实在太神奇了，那么究竟是什么原因导致大数据具有这些魔力的呢？简单来说，就是机器学习技术。正是基于机器学习技术的应用，数据才能发挥其魔力。大数据的核心是利用数据的价值，机器学习是利用数据价值的关键技术，对于大数据而言，机器学习是不可或缺的。相反，对于机器学习而言，越多的数据会越 可能提升模型的精确性，同时，复杂的机器学习算法的计算时间也迫切需要分布式计算与内存计算这样的关键技术。因此，机器学习的兴盛也离不开大数据的帮助。 大数据与机器学习两者是互相促进，相依相存的关系。机器学习与大数据紧密联系。但是，必须清醒的认识到，大数据并不等同于机器学习，同理，机器学习也不等同于大数据。大数据中包含有分布式计算，内存数据库，多维分析等等多种技术。单从分析方法来看，大数据也包含以下四种分析方法：1.大数据，小分析：即数据仓库领域的OLAP分析思路，也就是多维分析思想。2.大数据，大分析：这个代表的就是数据挖掘与机器学习分析法。3.流式分析：这个主要指的是事件驱动架构。4.查询分析：经典代表是NoSQL数据库。也就是说，机器学习仅仅是大数据分析中的一种而已。尽管机器学习的一些结果具有很大的魔力，在某种场合下是大数据价值最好的说明。但这并不代表机器学习是大数据下的唯一的分析方法。机器学习与大数据的结合产生了巨大的价值。基于机器学习技术的发展，数据能够“预测”。对人类而言，积累的经验越丰富，阅历也广泛，对未来的判断越准确。例如常说的“经验丰富”的人比“初出茅庐”的小伙子更有工作上的优势，就在于经验丰富的人获得的规律比他人更准确。而在机器学习领域，根据著名的一个实验，有效的证实了机器学习界一个理论：即机器学习模型的数据越多，机器学习的预测的效率就越好。见下图：通过这张图可以看出，各种不同算法在输入的数据量达到一定级数后，都有相近的高准确度。于是诞生了机器学习界的名言：成功的机器学习应用不是拥有最好的算法，而是拥有最多的数据！在大数据的时代，有好多优势促使机器学习能够应用更广泛。例如随着物联网和移动设备的发展，我们拥有的数据越来越多，种类也包括图片、文本、视频等非结构化数据，这使得机器学习模型可以获得越来越多的数据。同时大数据技术中的分布式计算Map-Reduce使得机器学习的速度越来越快，可以更方便的使用。种种优势使得在大数据时代，机器学习的优势可以得到最佳的发挥。6.机器学习的子类--深度学习近来，机器学习的发展产生了一个新的方向，即“深度学习”。虽然深度学习这四字听起来颇为高大上，但其理念却非常简单，就是传统的神经网络发展到了多隐藏层的情况。在上文介绍过，自从90年代以后，神经网络已经消寂了一段时间。但是BP算法的发明人Geoffrey Hinton一直没有放弃对神经网络的研究。由于神经网络在隐藏层扩大到两个以上，其训练速度就会非常慢，因此实用性一直低于支持向量机。2006年，Geoffrey Hinton在科学杂志《Science》上发表了一篇文章，论证了两个观点：1.多隐层的神经网络具有优异的特征学习能力，学习得到的特征对数据有更本质的刻画，从而有利于可视化或分类；2.深度神经网络在训练上的难度，可以通过“逐层初始化” 来有效克服。Geoffrey Hinton与他的学生在Science上发表文章通过这样的发现，不仅解决了神经网络在计算上的难度，同时也说明了深层神经网络在学习上的优异性。从此，神经网络重新成为了机器学习界中的主流强大学习技术。同时，具有多个隐藏层的神经网络被称为深度神经网络，基于深度神经网络的学习研究称之为深度学习。由于深度学习的重要性质，在各方面都取得极大的关注，按照时间轴排序，有以下四个标志性事件值得一说：2012年6月，《纽约时报》披露了Google Brain项目，这个项目是由Andrew Ng和Map-Reduce发明人Jeff Dean共同主导，用16000个CPU Core的并行计算平台训练一种称为“深层神经网络”的机器学习模型，在语音识别和图像识别等领域获得了巨大的成功。Andrew Ng就是文章开始所介绍的机器学习的大牛(中左者)。2012年11月，微软在中国天津的一次活动上公开演示了一个全自动的同声传译系统，讲演者用英文演讲，后台的计算机一气呵成自动完成语音识别、英中机器翻译，以及中文语音合成，效果非常流畅，其中支撑的关键技术是深度学习；2013年1月，在百度的年会上，创始人兼CEO李彦宏高调宣布要成立百度研究院，其中第一个重点方向就是深度学习，并为此而成立深度学习研究院(IDL)。2013年4月，《麻省理工学院技术评论》杂志将深度学习列为2013年十大突破性技术(Breakthrough Technology)之首。文章开头所列的三位机器学习的大牛，不仅都是机器学习界的专家，更是深度学习研究领域的先驱。因此，使他们担任各个大型互联网公司技术掌舵者的原因不仅在于他们的技术实力，更在于他们研究的领域是前景无限的深度学习技术。目前业界许多的图像识别技术与语音识别技术的进步都源于深度学习的发展，除了本文开头所提的Cortana等语音助手，还包括一些图像识别应用，其中典型的代表就是下图的百度识图功能。深度学习属于机器学习的子类。基于深度学习的发展极大的促进了机器学习的地位提高，更进一步地，推动了业界对机器学习父类人工智能梦想的再次重视。7.机器学习的父类--人工智能人工智能是机器学习的父类。深度学习则是机器学习的子类。如果把三者的关系用图来表明的话，则是下图：毫无疑问，人工智能(AI)是人类所能想象的科技界最突破性的发明了，某种意义上来说，人工智能就像游戏最终幻想的名字一样，是人类对于科技界的最终梦想。从50年代提出人工智能的理念以后，科技界，产业界不断在探索，研究。这段时间各种小说、电影都在以各种方式展现对于人工智能的想象。人类可以发明类似于人类的机器，这是多么伟大的一种理念！但事实上，自从50年代以后，人工智能的发展就磕磕碰碰，未有见到足够震撼的科学技术的进步。总结起来，人工智能的发展经历了如下若干阶段，从早期的逻辑推理，到中期的专家系统，这些科研进步确实使我们离机器的智能有点接近了，但还有一大段距离。直到机器学习诞生以后，人工智能界感觉终于找对了方向。基于机器学习的图像识别和语音识别在某些垂直领域达到了跟人相媲美的程度。机器学习使人类第一次如此接近人工智能的梦想。事实上，如果我们把人工智能相关的技术以及其他业界的技术做一个类比，就可以发现机器学习在人工智能中的重要地位不是没有理由的。人类区别于其他物体，植物，动物的最主要区别，作者认为是“智慧”。而智慧的最佳体现是什么？是计算能力么，应该不是，心算速度快的人我们一般称之为天才。是反应能力么，也不是，反应快的人我们称之为灵敏。是记忆能力么，也不是，记忆好的人我们一般称之为过目不忘。是推理能力么，这样的人我也许会称他智力很高，类似“福尔摩斯”，但不会称他拥有智慧。是知识能力么，这样的人我们称之为博闻广，也不会称他拥有智慧。想想看我们一般形容谁有大智慧？圣人，诸如庄子，老子等。智慧是对生活的感悟，是对人生的积淀与思考，这与我们机器学习的思想何其相似？通过经验获取规律，指导人生与未来。没有经验就没有智慧。那么，从计算机来看，以上的种种能力都有种种技术去应对。例如计算能力我们有分布式计算，反应能力我们有事件驱动架构，检索能力我们有搜索引擎，知识存储能力我们有数据仓库，逻辑推理能力我们有专家系统，但是，唯有对应智慧中最显著特征的归纳与感悟能力，只有机器学习与之对应。这也是机器学习能力最能表征智慧的根本原因。让我们再看一下机器人的制造，在我们具有了强大的计算，海量的存储，快速的检索，迅速的反应，优秀的逻辑推理后我们如果再配合上一个强大的智慧大脑，一个真正意义上的人工智能也许就会诞生，这也是为什么说在机器学习快速发展的现在，人工智能可能不再是梦想的原因。人工智能的发展可能不仅取决于机器学习，更取决于前面所介绍的深度学习，深度学习技术由于深度模拟了人类大脑的构成，在视觉识别与语音识别上显著性的突破了原有机器学习技术的界限，因此极有可能是真正实现人工智能梦想的关键技术。无论是谷歌大脑还是百度大脑，都是通过海量层次的深度学习网络所构成的。也许借助于深度学习技术，在不远的将来，一个具有人类智能的计算机真的有可能实现。最后再说一下题外话，由于人工智能借助于深度学习技术的快速发展，已经在某些地方引起了传统技术界达人的担忧。真实世界的“钢铁侠”，特斯拉CEO马斯克就是其中之一。最近马斯克在参加MIT讨论会时，就表达了对于人工智能的担忧。“人工智能的研究就类似于召唤恶魔，我们必须在某些地方加强注意。”尽管马斯克的担心有些危言耸听，但是马斯克的推理不无道理。“如果人工智能想要消除垃圾邮件的话，可能它最后的决定就是消灭人类。”马斯克认为预防此类现象的方法是引入政府的监管。在这里作者的观点与马斯克类似，在人工智能诞生之初就给其加上若干规则限制可能有效，也就是不应该使用单纯的机器学习，而应该是机器学习与规则引擎等系统的综合能够较好的解决这类问题。因为如果学习没有限制，极有可能进入某个误区，必须要加上某些引导。正如人类社会中，法律就是一个最好的规则，杀人者死就是对于人类在探索提高生产力时不可逾越的界限。在这里，必须提一下这里的规则与机器学习引出的规律的不同，规律不是一个严格意义的准则，其代表的更多是概率上的指导，而规则则是神圣不可侵犯，不可修改的。规律可以调整，但规则是不能改变的。有效的结合规律与规则的特点，可以引导出一个合理的，可控的学习型人工智能。8.机器学习的思考--计算机的潜意识最后，作者想谈一谈关于机器学习的一些思考。主要是作者在日常生活总结出来的一些感悟。回想一下我在节1里所说的故事，我把小Y过往跟我相约的经历做了一个罗列。但是这种罗列以往所有经历的方法只有少数人会这么做，大部分的人采用的是更直接的方法，即利用直觉。那么，直觉是什么？其实直觉也是你在潜意识状态下思考经验后得出的规律。就像你通过机器学习算法，得到了一个模型，那么你下次只要直接使用就行了。那么这个规律你是什么时候思考的？可能是在你无意识的情况下，例如睡觉，走路等情况。这种时候，大脑其实也在默默地做一些你察觉不到的工作。这种直觉与潜意识，我把它与另一种人类思考经验的方式做了区分。如果一个人勤于思考，例如他会每天做一个小结，譬如“吾日三省吾身”，或者他经常与同伴讨论最近工作的得失，那么他这种训练模型的方式是直接的，明意识的思考与归纳。这样的效果很好，记忆性强，并且更能得出有效反应现实的规律。但是大部分的人可能很少做这样的总结，那么他们得出生活中规律的方法使用的就是潜意识法。举一个作者本人关于潜意识的例子。作者本人以前没开过车，最近一段时间买了车后，天天开车上班。我每天都走固定的路线。有趣的是，在一开始的几天，我非常紧张的注意着前方的路况，而现在我已经在无意识中就把车开到了目标。这个过程中我的眼睛是注视着前方的，我的大脑是没有思考，但是我手握着的方向盘会自动的调整方向。也就是说。随着我开车次数的增多，我已经把我开车的动作交给了潜意识。这是非常有趣的一件事。在这段过程中，我的大脑将前方路况的图像记录了下来，同时大脑也记忆了我转动方向盘的动作。经过大脑自己的潜意识思考，最后生成的潜意识可以直接根据前方的图像调整我手的动作。假设我们将前方的录像交给计算机，然后让计算机记录与图像对应的驾驶员的动作。经过一段时间的学习，计算机生成的机器学习模型就可以进行自动驾驶了。这很神奇，不是么。其实包括Google、特斯拉在内的自动驾驶汽车技术的原理就是这样。除了自动驾驶汽车以外，潜意识的思想还可以扩展到人的交际。譬如说服别人，一个最佳的方法就是给他展示一些信息，然后让他自己去归纳得出我们想要的结论。这就好比在阐述一个观点时，用一个事实，或者一个故事，比大段的道理要好很多。古往今来，但凡优秀的说客，无不采用的是这种方法。春秋战国时期，各国合纵连横，经常有各种说客去跟一国之君交流，直接告诉君主该做什么，无异于自寻死路，但是跟君主讲故事，通过这些故事让君主恍然大悟，就是一种正确的过程。这里面有许多杰出的代表，如墨子，苏秦等等。基本上所有的交流过程，使用故事说明的效果都要远胜于阐述道义之类的效果好很多。为什么用故事的方法比道理或者其他的方法好很多，这是因为在人成长的过程，经过自己的思考，已经形成了很多规律与潜意识。如果你告诉的规律与对方的不相符，很有可能出于保护，他们会本能的拒绝你的新规律，但是如果你跟他讲一个故事，传递一些信息，输送一些数据给他，他会思考并自我改变。他的思考过程实际上就是机器学习的过程，他把新的数据纳入到他的旧有的记忆与数据中，经过重新训练。如果你给出的数据的信息量非常大，大到调整了他的模型，那么他就会按照你希望的规律去做事。有的时候，他会本能的拒绝执行这个思考过程，但是数据一旦输入，无论他希望与否，他的大脑都会在潜意识状态下思考，并且可能改变他的看法。如果计算机也拥有潜意识(正如本博客的名称一样)，那么会怎么样？譬如让计算机在工作的过程中，逐渐产生了自身的潜意识，于是甚至可以在你不需要告诉它做什么时它就会完成那件事。这是个非常有意思的设想，这里留给各位读者去发散思考吧。9.总结本文首先介绍了互联网界与机器学习大牛结合的趋势，以及使用机器学习的相关应用，接着以一个“等人故事”展开对机器学习的介绍。介绍中首先是机器学习的概念与定义，然后是机器学习的相关学科，机器学习中包含的各类学习算法，接着介绍机器学习与大数据的关系，机器学习的新子类深度学习，最后探讨了一下机器学习与人工智能发展的联系以及机器学习与潜意识的关联。经过本文的介绍，相信大家对机器学习技术有一定的了解，例如机器学习是什么，它的内核思想是什么(即统计和归纳)，通过了解机器学习与人类思考的近似联系可以知晓机器学习为什么具有智慧能力的原因等等。其次，本文漫谈了机器学习与外延学科的关系，机器学习与大数据相互促进相得益彰的联系，机器学习界最新的深度学习的迅猛发展，以及对于人类基于机器学习开发智能机器人的一种展望与思考，最后作者简单谈了一点关于让计算机拥有潜意识的设想。机器学习是目前业界最为Amazing与火热的一项技术，从网上的每一次淘宝的购买东西，到自动驾驶汽车技术，以及网络攻击抵御系统等等，都有机器学习的因子在内，同时机器学习也是最有可能使人类完成AI dream的一项技术，各种人工智能目前的应用，如微软小冰聊天机器人，到计算机视觉技术的进步，都有机器学习努力的成分。作为一名当代的计算机领域的开发或管理人员，以及身处这个世界，使用者IT技术带来便利的人们，最好都应该了解一些机器学习的相关知识与概念，因为这可以帮你更好的理解为你带来莫大便利技术的背后原理，以及让你更好的理解当代科技的进程。10.后记这篇文档花了作者两个月的时间，终于在2014年的最后一天的前一天基本完成。通过这篇文章，作者希望对机器学习在国内的普及做一点贡献，同时也是作者本人自己对于所学机器学习知识的一个融汇贯通，整体归纳的提高过程。作者把这么多的知识经过自己的大脑思考，训练出了一个模型，形成了这篇文档，可以说这也是一种机器学习的过程吧(笑)。作者所在的行业会接触到大量的数据，因此对于数据的处理和分析是平常非常重要的工作，机器学习课程的思想和理念对于作者日常的工作指引作用极大，几乎导致了作者对于数据价值的重新认识。想想半年前，作者还对机器学习似懂非懂，如今也可以算是一个机器学习的Expert了(笑)。但作者始终认为，机器学习的真正应用不是通过概念或者思想的方式，而是通过实践。只有当把机器学习技术真正应用时，才可算是对机器学习的理解进入了一个层次。正所谓再“阳春白雪”的技术，也必须落到“下里巴人”的场景下运用。目前有一种风气，国内外研究机器学习的某些学者，有一种高贵的逼格，认为自己的研究是普通人无法理解的，但是这样的理念是根本错误的，没有在真正实际的地方发挥作用，凭什么证明你的研究有所价值呢？作者认为必须将高大上的技术用在改变普通人的生活上，才能发挥其根本的价值。一些简单的场景，恰恰是实践机器学习技术的最好地方。最后，作者很感谢能够阅读到这里的读者。如果看完觉得好的话，还请轻轻点一下赞或者分享给更多的人，你们的鼓励就是作者继续行文的动力。参考文献：1.Andrew Ng Courera Machine Learning2.LeNet Homepage3.pluskid svm厚积薄发，行胜于言@飞鸟各投林"}
{"content2":"AForge.NET是一个专门为开发者和研究者基于C#框架设计的，他包括计算机视觉与人工智能，图像处理，神经网络，遗传算法，机器学习，模糊系统，机器人控制等领域。这个框架由一系列的类库组成。主要包括有：AForge.Imaging —— 日常的图像处理和过滤器AForge.Vision —— 计算机视觉应用类库AForge.Neuro —— 神经网络计算库AForge.Genetic -进化算法编程库AForge.MachineLearning —— 机器学习类库AForge.Robotics —— 提供一些机器学习的工具类库AForge.Video —— 一系列的视频处理类库AForge.Fuzzy —— 模糊推理系统类库AForge.Controls—— 图像，三维，图表显示控件1.基于符号识别的3D现实增强技术2.基于模糊系统的自动导航3.运动检测4.2D增强技术5.计算机视觉与人工智能6.模拟识别7.神经网络8.图像处理9.遗传算法10.机器学习11.机器人控制等等还有GRATF 符号识别和目标追踪的库，可以用于机器人控制，当然也可以用于现实增强。Image Processing Lab基于C#的图像处理库，提供了一系列可用于AForge，Net的接口和工具。AForge.Net 是C#的一个图像计算机视觉库，该库是一个开源项目，提供很多图像的处理，和视频处理功能http://www.aforgenet.com/Aforge.Net子项目有个AForge.Video.VFW提供了对Avi文件的操作，AForge后面加入了子项目 AForge.Video.FFMPEG 通过FFmpeg库，提供了对大量视频格式的支持，我们都知道，FFmpeg是一个非常强大的视频处理类库，同样也是开源的，不过 AForge.Video.FFMPEG 还处于实验阶段，目标是用 FFmpeg 取代 AForge.Video.VFW 提供一个更好的对视频文件操作的库，但是该库目前只提供了对视频数据的读写，不支持对音频文件的读写，可能以后会支持，在使用的 AForge.Video.FFMpeg 时，添加对 AForge.Video.FFMPEG.dll, AForge.Video.dll和 AForge.dll 三个 dll 的引用。AForge.Video.FFMpeg命名空间下提供了三个类 VideoFileReader， VideoFileWriter， VideoFileSource"}
{"content2":"学术组织与机构国内中国人工智能学会中国计算机学会中国计算机学会 · 计算机视觉专业组（Computer Vision Task Forces, China Computer Federation）中国图象图形学学会中国电子学会中国遥感委员会（Chinese National Committee for Remote Sensing, CNCRS）视觉计算特别兴趣研究组联盟（Special Interest Group on Visual Computing, SIGVC）国外IEEE - The world's largest professional association for the advancement of technology国际摄影测量与遥感协会（ISPRS - International Society for Photogrammetry and Remote Sensing）IAPR - International Association of Pattern Recognition国际光学工程学会（Society of Photo-Optical Instrumentation Engineers，SPIE）研究所与实验室国内武汉大学计算机视觉与遥感实验室（CVRS Lab）武汉大学测绘遥感信息工程国家重点实验室（LMARS）中科院自动化所模式识别国家重点实验室浙江大学计算机辅助设计与图形学（CAD&CG）国家重点实验室清华大学可视多媒体研究中心南京大学计算机软件新技术国家重点实验室南京大学机器学习与数据挖掘研究所中国科学院视觉计算联合研究组视觉与学习青年学者研讨会（VALSE）中科院自动化研究所香港中文大学多媒体实验室上海交通大学图像处理与模式识别研究所中科院自动化研究所生物识别与安全技术研究中心华中科技大学媒体与通讯实验室复旦大学计算机视觉实验室中科院自动化研究所机器视觉组中科院自动化研究所自然语言处理研究组西安交通大学人工智能与机器人研究所同济大学计算机视觉与遥感研究组厦门大学模式分析与机器智能研究中心深圳大学计算机视觉研究所中科院深圳先进技术研究院（SIAT）可视计算研究中心（VCC）百度深度学习研究院（Institute of Deep Learning, IDL）国外List of Top Computer Vision Research Groups All Around the WorldIDIAP Research Institute, Switzerland微软亚洲计算机视觉研究院UCSD Artificial Intelligence GroupUCSD Computer Vision Group美国密歇根州立大学生物识别研究组美国普渡大学机器人视觉实验室University of Pennsylvania GRASP LabUniversity of Massachusetts Computer Vision Lab瑞士ETH-Zurich大学CV实验室User Interface Research Group, Igarashi LabBerkeley Computer Vision Group加州大学圣地亚哥分校（UCSD）Visual Computing and LearningCarnegie Mellon University Computer Vision GroupsCarnegie Mellon University Computer Vision Groups莫斯科大学（MSU）Graphics and Media Lab洛桑联邦理工学院（EPFL）Image and Visual Representation Group（IVRG）USC Computer Graphics and Immersive Technologies LabUC Berkeley Computer Vision GroupStanford Computer Vision LabUniversity of Southern California Computer Vision LabComputer Vision and Robotics Research Laboratory - UCSD学术资讯Computer Vision Online - Your Gateway to the Computer Vision World中国机器人网中国研学资源网IEEE ComputerKinect体感游戏网MEMS资讯网计算机视觉网址导航36氪|互联网创业资讯网论文检索谷歌学术百度学术微软学术微软学术搜索（Microsoft Academic Search）CVPapersSpringer LinkSpringerOpenIEEE/IET Electronic LibraryElsevier is a world-leading provider of scientific, technical and medical information products and servicesElsevier ScienceDirect学术圈中国知网Academics.IO全国图书馆参考咨询联盟武汉大学图书馆自然科学基金查询与分析系统(基础查询版) - MedSci会议信息Calendar of Computer Image Analysis, Computer Vision ConferencesComputer Vision Call For Papers for Conferences, Workshops and Journals at WikiCFPMachine Learning Call For Papers for Conferences, Workshops and Journals at WikiCFPImage Processing Call For Papers for Conferences, Workshops and Journals at WikiCFPComputer Vision Conferences and Special Issues - Computer Vision Central学术论坛CSDN C/C++ 论坛JAVA 学习交流论坛Matlab 技术论坛Android巴士OpenCV 中文论坛计算机视觉论坛视觉计算研究论坛计算机科学论坛Vision And Learning SEminar (VALSE) 在线讨论Stackoverflow 与程序相关的IT技术问答网站酷壳（CoolShell）MedSci-临床研究与学术服务平台小木虫 学术科研Machine Learning (http://www.reddit.com/r/MachineLearning/)OpenCV论坛,图像处理,计算机视觉,OpenCV视频教程,Image Processing,Computer Vision计算机视觉 | CV工作室中国人工智能网中国视觉网（http://www.china-vision.net/）中国视觉网（http://www.china-image.cn/）计算机视觉最新资讯网计算机视觉论文分类导航计算机视觉分类信息导航 (http://www.visionbib.com/)Computer Vision Central (http://cvisioncentral.com/)在线教程开放课程导航视频教学网QT学习之路系列教程CourseraUdacityPython与Git教程OpenMP学习网站我要自学网学术博客机器视觉方面的牛人资料刘未鹏 Mind Hacks阮一峰的网络日志计算机视觉一个人的博客科学网 博客鸟哥的Linux私房菜廖雪峰的官方网站Deep Learning 学习笔记计算机视觉领域大牛的博客以及研究机构等计算机视觉、机器学习相关领域论文和源代码大集合Computer Vision 技术公司列表The Computer Vision Industry南京大学周志华武汉大学张良培深度学习、机器学习等常用资源整理计算机视觉领域的一些牛人博客，超有实力的研究机构等的网站链接国内外研究主页集合：计算机视觉-机器学习-模式识别国内外从事CV相关的企业Lee-sven (http://www.leesven.com/) | Running Snail - Never stop learningComputer Vision Software (Blog)数据库步态数据库中科院指纹，虹膜，人脸，掌纹等数据库数据堂：科研数据共享数据挖掘研究院TexturelibFree data sets for computer vision applications at Computer Vision Online源代码CodeForge 源代码网站Pudn 程序员联合开发网源码soso网站CSDN （IT社区和服务平台）MATLAB Source CodesEmgu CVMatlab 技术论坛A Library for SVMmySVMSVMlightMahoutCxImageOpenCV DocumentationComputer Vision Source CodesGitHub 项目托管网站Patch-based MultiPatch-based Multi-view Stereo Software-view Stereo Software（cmvs-pmvs）mmCheng显著性检测代码机器视觉开源代码集合深度学习源代码代码集合计算机视觉、机器学习相关领域论文和源代码Computer Vision Algorithm Implementations下载文档百度文库学习资料库电子书下载豆丁网Books of Computer Vision, Image Processing, Machine Learning, etc.软件CmakeNotePad++QtEverythingFlashFXPDependency WalkerCloudCompare - 3D point cloud and mesh processing softwareXnView Software for reading, organizing and processing imagesTortoise SVNChinese TeX (CTeX)TeXstudio (LaTeX Editor)JabRef reference managerXMind: The Most Professional Mind Mapping Software格式工厂（Format Factory）- 免费多功能的多媒体文件转换工具工具Python.orgVGG toolboxToronto RNN toolboxSourceforge RNN toolboxDeep Learning tool in JavaConvNet in MatlabCaffeComputer Vision SoftwaresC++ Template Image Processing ToolkitCxImageOpenCV (Intel Open Source Computer Vision Library)VXL - C++ Libraries for Computer Vision Research and ImplementationGandalf - computer vision and numerical algorithm library written in CGDAL - Geospatial Data Abstraction Library开放式遥感数据处理软件平台-OpenRSHALCON机器视觉软件开发包OpenVX - Hardware acceleration API for Computer Vision applications and librariesPCL - Point Cloud Library (PCL)Mobile Robot Programming Toolkit (MRPT) | Empowering C++ development in roboticsRobot Operating System (ROS) | Powering the world's robots3DTK — The 3D Toolkit (6DSLAM)OpenSLAM.org | The simultaneous localization and mapping (SLAM)Boost C++ LibrariesTorch3Vision - Basic image processing, matrix manipulation and feature extraction algorithms: rotation, flip, photometric normalisations (Histogram Equalization, Multiscale Retinex, Self-Quotient Image or Gross-Brajovic), edge detection, 2D DCT, 2D FFT, 2D Gabor, PCA to do Eigen-Faces, LDA to do Fisher-Faces. Various metrics (Euclidean, Mahanalobis, ChiSquare, NormalizeCorrelation, TangentDistance, ...)Visualization Toolkit (VTK)Insight Segmentation and Registration Toolkit (ITK)OpenSceneGraph (OSG)影像谷歌地图百度地图EarthExplorerLROC Image SearchQuickMapE-BooksComputer Vision - Algorithms and Application (Richard Szeliski 20100903 Draft)Concise Computer Vision - An Introduction into Therory and AlgorithmFundamentals of Digital Image Processing - A Practical Approach with Examples in MatlabMIT - Optimization for Machine Learning 2012Multiple View Geometry in Computer Vision (Second Edition)Pattern Recognition and Machine LearningOptimization for Computer Vision - An Introduction to Core Concepts and Methods模式分类（英文）-第2版计算机视觉中的数学方法计算机视觉算法与应用中文版其他不需FQ用谷歌搜索谷粉搜索—不需FQ用谷歌搜索BaiGoogleDu天外天搜索 - 基于Google的搜索引擎 谷歌打不开解决方案 Google上不去怎么办 谷歌搜索打不开2015 不FQ搜索谷歌GoAgent详细图文教程"}
{"content2":"核心提示：微软在 Office365、Azure 云、Dynamics365 上进行人工智能技术的部署，野心不小。 微软在2016年9月宣布组建自己的 AI 研究小组。该小组汇集了超过 5000 名计算机科学家和工程师，加上微软内部研究部门，将共同挖掘 AI 技术。 与此同时，亚马逊，Facebook，Google，IBM 还有微软联合宣而巨头们也纷纷拿出了自己的看家本领，Apple 的 Siri 利用自然语言处理来识别语音命令；Facebook 的深度学习面部识别算法能够快速准确地识别出人脸；Google 的「大脑」可能更为聪明，而且能够安装在数百万安卓系统的设备上。而隐藏在 CRM 领域的对手之一 Salesforce 也声称推出了一款全新的人工智能平台。还有 Amazon、Netflix 和 Spotify 都在强调使用机器学习了解如何与客户建立联系。最近的人工智能领域如此炙手可热，就连微软的首席执行官纳德拉也提到，「将 AI 覆盖到所有领域，是因为微软想要实现「AI 技术民主化」，从而解决全球最紧迫的挑战。」据悉，微软将从以下四个细分方向实现「AI 技术民主化」：Agents. Harness AI to fundamentally change human and computer interaction through agents such as Microsoft』s digital personal assistant Cortana1、助手。利用 AI 从根本上改变人机交互过程，如微软的数字个人助理 Cortana。Applications. Infuse every application, from the photo app on people』s phones to Skype and Office 365, with intelligence2、应用平台。从移动端的照片 app 到 Skype，再到 Office365，每一个应用平台融入了 AI 技术。Services. Make these same intelligent capabilities that are infused in Microsoft』s apps—cognitive capabilities such as vision and speech, and machine analytics—available to every application developer in the world3、服务。让全球每位应用开发者都能获取同样的 AI 技术支持。如微软 app 中视觉与语音识别能力，机器分析能力。Infrastructure. Build the world』s most powerful AI supercomputer with Azure and make it available to anyone, to enable people and organizations to harness its power4、基础架构。以 Azure 云为平台，构建强大的 AI 超级计算机，为企业和个人提供该项服务。那么，微软近期推出的几款人工智能产品看似「姗姗来迟」，但在 AI 领域的重量可不能小视，那么，微软在如何布局 AI 呢？其旗下几大产品又都与 AI 有着怎样的关系？推出人工智能版 Dynamics365提升销售转化率，更好地理解客户行为，是如今移动、社交、云三者融合时代的大背景下企业需要具备的重要能力。过去几周，CRM 领域频频传来人工智能利好的消息。如今，微软也向世人透露：在下月 1 日，将推出人工智能版 Dynamics365，为销售人员提供云端商务应用解决方案。该款产品将 ERP 与 CRM 进行整合，打造成为一个单一的云端解决方案。此次产品的推出主要针对的就是 CRM 领域占主导地位的 Salesforce，而此前不久，Salesforce 刚刚推出了一款名为「爱因斯坦」的人工智能云平台，能够为企业客户提供相应服务。Microsoft has built in a couple of intelligence features into the release designed specifically for sales and service personnel. First, there is Customer Insights, a stand-alone cloud service, which enables users to bring in a variety of internal and external data sources. Companies can integrate all of this data with internal metrics (KPIs) to drive automated actions based on the data. The solution includes partner data from the likes of Facebook and Trip Advisor (proving you don』t need to own an external data source to take advantage of it).微软推出的 Dynamics365 能够为销售人员提供以下几类服务：一是客户洞察（Customer Insights），一款单机云服务，帮助用户收集各类内外部数据信息。企业能够将所有数据与内部指标（KPIs）进行集成，基于数据进行自动化行为的驱动。这套解决方案还包括了合作伙伴如 Facebook 和 Trip Advisor 上产生的大量数据，不过，这也说明了用户无需借外部获取数据。It』s been designed as a stand-alone service that can work with any of the Dynamics 365 CRM components—sales, customer service or field service—and can also work with any external CRM tool with open APIs. This last point is particularly telling because it』s giving customers who might not be using Dynamics 365 (but are using other Microsoft tools like Outlook) access to this feature.一方面，单机服务能够与 Dynamics365 CRM 的组成部件进行很好地兼容，包括销售、客服或者现场服务；另一方面，还能与任何一个带有开放 API 接口的外部 CRM 工具进行兼容。此外，单机服务还能为并未使用 Dynamics365，而是微软其他工具如 Outlook 的客户，提供该项服务的渠道。The second piece is called Relationship Insights, which as the name suggests gives sales people information about the status of their customer relationships at any given moment. It』s built on the on the Cortana Intelligence Suite, which Microsoft introduced in 2015 and uses tools like sentiment analysis to check on the likelihood of the deal closing and the next best action to take.二是关系洞察（Relationship Insights），为销售人员即时提供客户关系信息。该服务基于人工智能助手小娜的平台进行提供。这个智能助手于去年在全球推出，通过情感分析检测交易结束的可能性，并判断下一步最佳实践方案。For now, know that Microsoft has consolidated its artificial intelligence tools into a single, coherent division and just about every vendor—not just those selling CRM—is trying to build some level of intelligence into its products. Dynamics 365 is just the latest manifestation.总的来说，微软已将人工智能工具嵌入一个单独连贯的区域，而且不只是 CRM 领域的供应商，基本上每个供应商都在试图将某种程度的智能技术嵌入到其产品中。"}
{"content2":"一、概念整体介绍人工智能（Artificial Intelligence）机器学习（Machine Learning）：一种实现人工智能的方法深度学习（Deep Learning）：一种实现机器学习的技术三者的关系图人工智能分类：强人工智能：强人工智能观点认为有可能制造出真正能推理（Reasoning）和解决问题（Problem_solving）的智能机器，并且这样的机器将被认为是有知觉的，有自我意识的。可以独立思考问题并制定解决问题的最优方案，有自己的价值观和世界观体系。有和生物一样的各种本能，比如生存和安全需求。弱人工智能：弱人工智能是指不能制造出真正地推理（Reasoning）和解决问题（Problem_solving）的智能机器，这些机器只不过看起来像是智能的，但是并不真正拥有智能，也不会有自主意识。人工智能是计算机学科的一个分支，二十世纪七十年代以来被称为世界三大尖端技术之一（空间技术、能源技术、人工智能）。也被认为是二十一世纪三大尖端技术（基因工程、纳米科学、人工智能）之一。这是因为近三十年来它获得了迅速的发展，在很多学科领域都获得了广泛应用，并取得了丰硕的成果，人工智能已逐步成为一个独立的分支，无论在理论和实践上都已自成一个系统。人工智能的研究分支人工智能的发展历程各种概念关系相关链接：一张图解释人工智能、机器学习、深度学习三者关系：https://baijiahao.baidu.com/s?id=1588563162916669654&wfr=spider&for=pc一篇文章讲清楚人工智能、机器学习和深度学习的区别和联系：https://www.cnblogs.com/bokeyuan-dlam/articles/7928135.html科普一下：机器学习和深度学习的区别和关系：http://www.elecfans.com/rengongzhineng/691751.html人工智能的三个分支：认知、机器学习、深度学习：https://blog.csdn.net/testcs_dn/article/details/81185750还纠结选机器学习还是深度学习？看完你就有数了：https://www.baidu.com/link?url=rVRgTtwZ11xkY1lcq4rRgilW9PwQEBXf737ESjE_H8RySv47Fwe-LyD69FJhFxeSqhQYPAL3kArqxR_nfWPSQAqRsrxsLaUqwm6EPUym6XK&wd=&eqid=edac1d7c00058498000000065cac9dc7=====================================================二、人工智能应用领域关键词：自然语言生成、语音识别、虚拟助理、机器学习平台、人工智能硬件优化、决策管理、深度学习平台、生物信息、图像识别、情绪识别、P2P网络、内容创作、网络防御、AI建模/数字孪生、机器处理自动化、文本分析和自然语言处理游戏 ：人工智能在国际象棋，扑克，围棋等游戏中起着至关重要的作用，机器可以根据启发式知识来思考大量可能的位置并计算出最优的下棋落子。自然语言处理 ： 可以与理解人类自然语言的计算机进行交互。比如常见机器翻译系统、人机对话系统。专家系统 ： 有一些应用程序集成了机器，软件和特殊信息，以传授推理和建议。它们为用户提供解释和建议。比如分析股票行情，进行量化交易。视觉系统 ： 它系统理解，解释计算机上的视觉输入。例如，间谍飞机拍摄照片，用于计算空间信息或区域地图。医生使用临床专家系统来诊断患者。警方使用的计算机软件可以识别数据库里面存储的肖像，从而识别犯罪者的脸部。还有我们最常用的车牌识别等。语音识别 ：智能系统能够与人类对话，通过句子及其含义来听取和理解人的语言。它可以处理不同的重音，俚语，背景噪音，不同人的的声调变化等。手写识别 ： 手写识别软件通过笔在屏幕上写的文本可以识别字母的形状并将其转换为可编辑的文本。智能机器人 ： 机器人能够执行人类给出的任务。它们具有传感器，检测到来自现实世界的光，热，温度，运动，声音，碰撞和压力等数据。他拥有高效的处理器，多个传感器和巨大的内存，以展示它的智能，并且能够从错误中吸取教训来适应新的环境。相关链接：http://www.qianjia.com/html/2018-08/23_302917.html=====================================================三、机器学习机器学习最基本的做法，是使用算法来解析数据、从中学习，然后对真实世界中的事件做出决策和预测。与传统的为解决特定任务、硬编码的软件程序不同，机器学习是用大量的数据来“训练”，通过各种算法从数据中学习如何完成任务。机器学习直接来源于早期的人工智能领域，传统的算法包括决策树、聚类、贝叶斯分类、支持向量机、EM、Adaboost等等。从学习方法上来分，机器学习算法可以分为监督学习（如分类问题）、无监督学习（如聚类问题）、半监督学习、集成学习、深度学习和强化学习。机器学习过程使用以下步骤进行定义：1. 确定相关数据集并准备进行分析。2. 选择要使用的算法类型。3. 根据所使用的算法构建分析模型。4. 立足测试数据集进行模型训练，并根据需要进行模型修改。5. 运行模型以生成测试评分。传统的机器学习算法在指纹识别、基于Haar的人脸检测、基于HoG特征的物体检测等领域的应用基本达到了商业化的要求或者特定场景的商业化水平，但每前进一步都异常艰难，直到深度学习算法的出现。=====================================================四、深度学习深度学习属于机器学习的一个子域，其相关算法受到大脑结构与功能（即人工神经网络）的启发。深度学习如今的全部价值皆通过监督式学习或经过标记的数据及算法实现。深度学习中的每种算法皆经过相同的学习过程。深度学习包含输入内容的非近线变换层级结构，可用于创建统计模型并输出对应结果。机器学习与深度学习间的区别：1、数据量：机器学习能够适应各种数据量，特别是数据量较小的场景。在另一方面，如果数据量迅速增加，那么深度学习的效果将更为突出。下图展示了不同数据量下机器学习与深度学习的效能水平。2、硬件依赖性：与传统机器学习算法相反，深度学习算法在设计上高度依赖于高端设备。深度学习算法需要执行大量矩阵乘法运算，因此需要充足的硬件资源作为支持。3、特征工程：特征工程是将特定领域知识放入指定特征的过程，旨在减少数据复杂性水平并生成可用于学习算法的模式。  示例：传统的机器学习模式专注于特征工程中所需要找像素及其他属性。深度学习算法则专注于数据的其他高级特征，因此能够降低处理每个新问题时特征提取器的实际工作量。4、问题解决方法：传统机器学习算法遵循标准程序以解决问题。它将问题拆分成数个部分，对其进行分别解决，而后再将结果结合起来以获得所需的答案。深度学习则以集中方式解决问题，而无需进行问题拆分。5、执行时间：执行时间是指训练算法所需要的时间量。深度学习需要大量时间进行训练，因为其中包含更多参数，因此训练的时间投入也更为可观。相对而言，机器学习算法的执行时间则相对较短。6、可解释性：可解释性是机器学习与深度学习算法间的主要区别之一——深度学习算法往往不具备可解释性。也正因为如此，业界在使用深度学习之前总会再三考量。机器学习与深度学习的实际应用：1. 通过指纹实现出勤打卡、人脸识别或者通过扫描车牌识别牌照号码的计算机视觉技术。2. 搜索引擎中的信息检索功能，例如文本搜索与图像搜索。3. 自动电子邮件营销与特定目标识别。4. 癌症肿瘤医学诊断或其他慢性疾病异常状态识别。5. 自然语言处理应用程序，例如照片标记。Facebook就提供此类功能以提升用户体验。6. 在线广告。未来发展趋势：1. 随着业界越来越多地使用数据科学与机器学习技术，对各个组织而言，最重要的是将机器学习方案引入其现有业务流程。2. 深度学习的重要程度正逐步超越机器学习。事实已经证明，深度学习是目前最先进且实际效能最出色的技术方案之一。3. 机器学习与深度学习将在研究与学术领域证明自身蕴藏的巨大能量。"}
{"content2":"今日CS.CV 计算机视觉论文速览Fri, 3 May 2019Totally 29 papers👉上期速览✈更多精彩请移步主页Interesting:📚****Single Image Portrait Relighting单图肖像光照重建, 本文提出了一种为手机拍摄人像修改光照的方法，通过提供的环境光照明图像将生成目标光照下的新图像。这一方法仅仅在18个独立个体在受控光源下手机的数据集进行训练。640-640照片只需要160ms，有集成到手机相机中的潜力。(from UCSD)基于自编码器原理的光照变换网络示意图。中间可以预测光照也可以修改光照：数据集构建 OLAT imageset，与目标相隔1.7m相机间相隔20度，图像加权合成后可以得到环境光的分布：网络模型细节如下图所示,分别训练了光照编码和目标光照下的图像损失：Authors:http://kevinkingo.com/Group:http://jacobsschool.ucsd.edu/visualcomputing/prof:https://cseweb.ucsd.edu/~ravir/📚***DPSNET深度平面扫描立体视觉,最近研究人员开始利用语义信息来改进对于无纹理或反光表面的深度图估计。与先前方法直接估计深度或者光流不同的是，DPSNet利用平面扫描算法来构建cost体，并从体中回归出深度图。cost volume来自于可差分的变换过程，使得这一方法可以利用深度学习进行训练。 (from KAIST,CMU,MSRA)深图图构建过程：网络模型的细节结构和构造：一些结果：📚大规模图像分类半监督学习, (from Facebook AI)利用一个训练好的teacher模型从大规模的无标签数据中选出新的数据集，随后再利用新数据集训练学生网络，并利用原始数据集进行调优：📚RetinaFace单阶段的稠密人脸定位, 包含了外监督、自监督多任务学习等多个平行分支。将输出人脸分类分数、bbox，人脸五个关键点、人脸的3D顶点投影等信息。(from 帝国理工)搜索到的人脸900/1151：ref:人脸分析库：insightface:https://github.com/deepinsight/insightface📚局域光场融合, 基于多个非规则视角插值得到新视角的渲染图像。(from 加州大学伯克利分校)利用多平米模型表示图像并进行融合：📚基于回归卷积的假人脸视频检测, 检测了detect Deepfake,Face2Face 和FaceSwap等方法生成的伪造视频。(from USC信息科技学院)模型架构：dataset: FaceForensics++📚3D BAT, 半自动化基于web的3D标注工具，用于自动驾驶全景多模态数据(from UCSD)code:https://github.com/walzimmer/3d-bat📚LIVDET IN ACTION, 活体指纹检测比赛(from University of Cagliari意大利)dataset: LivDet 2019📚2019 DAVIS Challenge on Video Object Segmentation, 包括了半监督追踪、交互式追踪、非监督多目标追踪等任务。(from ETHz google AI adobe Research)link:http://davischallenge.org/Daily Computer Vision PapersLocal Light Field Fusion: Practical View Synthesis with Prescriptive Sampling GuidelinesAuthors Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, Abhishek Kar我们提供了一种实用且强大的深度学习解决方案，用于捕获和渲染复杂现实世界场景的新视图以进行虚拟探索以前的方法要么需要难以处理的密集视图采样，要么对用户应如何对场景的视图进行采样以可靠地呈现高质量的新颖视图提供很少或没有指导。相反，我们提出了一种用于从不规则的采样视图网格进行视图合成的算法，该算法首先通过多平面图像MPI场景表示将每个采样视图扩展为局部光场，然后通过混合相邻的局部光场来渲染新颖视图。我们扩展了传统的全光采样理论，以推导出一个边界，该边界精确地指定用户在使用我们的算法时应该如何密集地对给定场景的视图进行采样。在实践中，我们应用此界限来捕获和渲染真实世界场景的视图，这些视图实现了奈奎斯特速率视图采样的感知质量，同时使用的视图减少了多达4000倍。我们通过增强现实智能手机应用程序展示我们的方法的实用性，该应用程序可引导用户捕获场景的输入图像，以及在桌面和移动平台上实现实时虚拟探索的查看器。Self-supervised Learning for Video Correspondence FlowAuthors Zihang Lai, Weidi Xie本文的目的是对来自视频的特征嵌入进行自我监督学习，适用于对应流，即匹配视频上帧之间的对应关系。我们利用视频中外观的自然空间时间一致性来创建指针模型，该模型学习通过复制参考帧中的颜色来重建目标帧。Lifting Vectorial Variational Problems: A Natural Formulation based on Geometric Measure Theory and Discrete Exterior CalculusAuthors Thomas M llenhoff, Daniel Cremers成像和视觉方面的许多任务可以被制定为矢量值映射的变分问题。我们通过提升到电流空间来处理这种矢量变分问题的松弛和凸化。为此，我们记得具有多凸拉格朗日的函数可以在函数图上重新参数化为凸一齐函数。这导致在域和密码域的产品空间中的取向表面上的等效形状优化问题。然后通过将搜索空间从定向表面放松到更一般的电流来获得凸形配方。我们提出使用Whitney形式对所得到的无限维优化问题进行离散化，这也推广了最近的子标签精确多标记方法。Clustering Images by Unmasking - A New BaselineAuthors Mariana Iuliana Georgescu, Radu Tudor Ionescu我们提出了一种基于unmasking的新型凝聚聚类方法，这种技术以前用于文本文档的作者验证和视频中的异常事件检测。为了连接两个聚类，我们在训练二元分类器之间交替，以区分来自一个聚类的样本和来自另一个聚类的样本，以及ii在每个步骤中去除最具判别性的特征。中间获得的分类器的更快降低的准确率表明应该连接两个簇。据我们所知，这是第一个应用取消屏蔽以聚类图像的工作。我们将我们的方法与k均值以及最近的现有聚类方法进行比较。实证结果表明，我们的方法能够提高各种深层和浅层特征表示和不同任务的性能，如手写数字识别，纹理分类和细粒度物体识别。Human Action Recognition with Deep Temporal PyramidsAuthors Ahmed Mazari, Hichem Sahbi深度卷积神经网络CNN现在在包括动作识别在内的不同模式识别任务中实现了重大飞跃。当前的CNN越来越深，数据越来越多，这使得它们成为大量标记训练数据的成功支流。 CNN还依赖于最大平均合并，这降低了输出层的维数，从而削弱了它们对标记数据可用性的敏感性。然而，该过程可能稀释上游卷积层的信息，从而影响训练表示的辨别力，特别是当学习的类别被细粒度化时。Egocentric Hand Track and Object-based Human Action RecognitionAuthors Georgios Kapidis, Ronald Poppe, Elsbeth van Dam, Lucas P. J. J. Noldus, Remco C. Veltkamp以自我为中心的视觉是一个新兴的计算机视觉领域，其特点是从第一人称视角获取图像和视频。在本文中，我们通过明确利用场景中检测到的感兴趣区域的存在和位置来解决自我中心人类行为识别的挑战，而无需进一步使用视觉特征。The 2019 DAVIS Challenge on VOS: Unsupervised Multi-Object SegmentationAuthors Sergi Caelles, Jordi Pont Tuset, Federico Perazzi, Alberto Montes, Kevis Kokitsi Maninis, Luc Van Gool我们将介绍2019年DAVIS挑战视频对象分割，这是DAVIS挑战系列的第三版，这是一项专为视频对象分割VOS设计的公共竞赛。除了原版半监督音轨和上一版中引入的互动音轨外，今年还将推出一款新的无监督多目标音轨。在新引入的轨道中，要求参与者在每个图像上提供非重叠的对象提议，以及在帧之间链接它们的标识符，即视频对象提议，而没有任何测试时间，人工监督没有在测试视频上提供的涂鸦或掩模。为了做到这一点，我们以简洁的方式重新注释了DAVIS 2017的列车和val集，以促进无人监督的轨道，并为比赛创建了新的测试开发和测试挑战集。本文详细描述了无监督轨道的定义，规则和评估指标。Face Identification using Local Ternary Tree Pattern based Spatial Structural ComponentsAuthors Rinku Datta Rakshit, Dakshina Ranjan Kisku, Massimo Tistarelli, Phalguni Gupta本文报告了人脸识别系统的突破性结果，该系统利用了一种称为局部三元树模式的新型局部描述符。当系统在存在包括约束，无约束和整形外科图像的多种面部图像的情况下执行时，为面部图像设计灵巧且可行的局部描述符在面部识别任务中起到紧急的前言。已经提出LTTP从面部图像提取鲁棒且有区别的空间特征，因为该描述符可用于最佳地描述面部的各种结构组件。为了提取最有用的特征，为每个像素形成具有八个邻居的三元树。 LTTP模式可以以四种方式生成LTTP左深度，LTTP左宽度，LTTP右深度和LTTP右宽度。这四种模式生成的编码方案在计算复杂性和时间复杂性方面非常简单和有效。所提出的面部识别系统在六个面部数据库上进行测试，即UMIST，JAFFE，扩展的耶鲁面部B，整形外科，LFW和UFI。实验评估表明，考虑到在不同环境下捕获的各种面部，在设计面部识别系统时将产生长期影响的最优秀结果。DS-VIO: Robust and Efficient Stereo Visual Inertial Odometry based on Dual Stage EKFAuthors Xiaogang Xiong, Wenqing Chen, Zhichao Liu, Qiang Shen本文提出了一种基于EKF扩展卡尔曼滤波器的双阶段算法，用于实时和稳健的立体声VIO视觉惯性测距。这种基于EKF的算法的第一阶段执行加速度计和陀螺仪的融合，而第二阶段执行立体相机和IMU的融合。由于加速度计和陀螺仪以及立体相机和IMU之间具有足够的互补特性，基于双级EKF的算法可以实现高精度的测距估计。同时，由于该算法中状态向量的维数较低，其计算效率可与之前基于滤波器的方法相媲美。我们称之为DS VIO双级基于EKF的立体视觉惯性测量法，并通过将其与EuRoC数据集上的OKVIS，ROVIO，VINS MONO和S MSCKF等现有技术方法进行比较来评估我们的DSVIO算法。结果表明，我们的算法在RMS误差方面可以达到相当甚至更好的性能RetinaFace: Single-stage Dense Face Localisation in the WildAuthors Jiankang Deng, Jia Guo, Yuxiang Zhou, Jinke Yu, Irene Kotsia, Stefanos Zafeiriou尽管在不受控制的人脸检测方面取得了巨大进步，但野外准确有效的面部定位仍然是一个开放的挑战。本文提出了一种强大的单阶段人脸检测器，名为RetinaFace，它利用联合额外监督和自我监督的多任务学习，在各种人脸尺度上进行像素智能人脸定位。具体来说，我们在以下五个方面做出贡献1我们在WIDER FACE数据集上手动注释五个面部标志，并在这个额外监督信号的帮助下观察硬面检测的显着改进。 2我们进一步添加了一个自监督网格解码器分支，用于与现有的监督分支并行地预测像素方面的3D形状面部信息。 3在WIDER FACE硬测试装置上，RetinaFace的性能优于现有技术平均精度AP 1.1，达到AP等于bf 91.4。 4在IJB C测试装置上，RetinaFace使最先进的方法ArcFace能够在FAR 1e 6的面部验证TAR 89.59中改进其结果。 5通过采用轻量级骨干网络，RetinaFace可以在单个CPU核心上实时运行以获得VGA分辨率图像。将发布额外的注释和代码以方便将来的研究。LivDet in Action - Fingerprint Liveness Detection Competition 2019Authors Giulia Orr , Roberto Casula, Pierluigi Tuveri, Carlotta Bazzoni, Giovanna Dessalvi, Marco Micheletto, Luca Ghiani, Gian Luca Marcialis国际指纹活体检测竞赛LivDet是一个公开的，公认的学术界和私营公司的会议点，处理区分来自人造材料和图像相对于真实指纹的指纹复制的图像的问题。在本期LivDet中，我们邀请竞争对手提出具有匹配系统的集成算法。目标是调查这种整合在多大程度上影响整个绩效。提交了12个算法，其中8个算法用于集成系统。Directing DNNs Attention for Facial Attribution Classification using Gradient-weighted Class Activation MappingAuthors Xi Yang, Bojian Wu, Issei Sato, Takeo Igarashi深度神经网络DNN在图像分类任务中具有高精度。然而，由此类数据集训练的具有共发生偏差的DNN可能在制定分类决策时依赖于错误的特征。它将极大地影响训练有素的DNN的可转移性。在本文中，我们提出了一种交互方法，以指导分类器关注用户手动指定的区域，以减轻共现偏差的影响。我们在CelebA数据集上进行测试，经过预先训练的AlexNet经过精心调整，专注于基于Grad CAM结果的特定面部属性。Recurrent-Convolution Approach to DeepFake Detection - State-Of-Art Results on FaceForensics++Authors Ekraam Sabir, Jiaxin Cheng, Ayush Jaiswal, Wael AbdAlmageed, Iacopo Masi, Prem Natarajan错误信息的传播已成为一个重要问题，提高了相关检测方法的重要性。虽然存在不同的错误信息表现，但在这项工作中，我们专注于检测视频中的面部操作。具体来说，我们尝试在视频中检测Deepfake，Face2Face和FaceSwap操作。我们利用循环方法利用视频的时间动态。在FaceForensics数据集上进行评估，并且我们的方法改进了先前的技术水平4.55。Large-scale weakly-supervised pre-training for video action recognitionAuthors Deepti Ghadiyaram, Matt Feiszli, Du Tran, Xueting Yan, Heng Wang, Dhruv Mahajan当前完全监督的视频数据集仅包含几十万个视频和少于一千个域特定标签。这阻碍了高级视频架构的发展。本文对使用大量网络视频进行预训练视频模型以进行动作识别任务进行了深入研究。我们的主要实证研究结果是，尽管有嘈杂的社交媒体视频和标签，但是大规模预训练超过6500万个视频，大大改善了三个具有挑战性的公共行动识别数据集的最新技术水平。此外，我们研究了弱监督视频动作数据集构建中的三个问题。首先，假设动作涉及与对象的交互，那么如何构建动词对象预训练标签空间以最有利于转移学习第二，基于框架的模型在动作识别上表现良好是对良好图像特征的预训练足够或者是训练前对于最佳转移学习有价值的时空特征最后，由于动作标签是在视频级别提供的，因此在长视频与短视频中的动作通常不太好，因为如果有一些固定的数字预算，应如何选择视频片段以获得最佳性能或几分钟的视频Billion-scale semi-supervised learning for image classificationAuthors I. Zeki Yalniz, Herv J gou, Kan Chen, Manohar Paluri, Dhruv Mahajan本文提出了一种大型卷积网络半监督学习的研究。我们提出了一个基于教师学生范式的管道，它利用了大量未标记图像，最多可达10亿。我们的主要目标是改善给定目标体系结构的性能，如ResNet 50或ResNext。我们对我们的方法的成功因素进行了广泛的分析，这使我们能够制定一些建议来生成用于半监督学习的图像分类的高精度模型。因此，我们的方法为图像，视频和细粒度分类的标准体系结构带来了重要的收益。例如，通过利用10亿个未标记的图像，我们学到的香草ResNet 50在ImageNet基准测试中达到了81.2的前1精度。DPSNet: End-to-end Deep Plane Sweep StereoAuthors Sunghoon Im, Hae Gon Jeon, Stephen Lin, In So Kweon多视图立体声旨在从在任意运动下由相机获取的图像重建场景深度。最近的方法通过深度学习解决了这个问题，深度学习可以利用语义线索来处理诸如无纹理和反射区域之类的挑战。在本文中，我们提出了一个称为DPSNet深平面扫描网络的卷积神经网络，其设计灵感来自基于传统几何的密集深度重建方法的最佳实践。 DPSNet采用平面扫描方法，而不是直接估计图像对中的深度和/或光流对应，而不是使用平面扫描算法从深度特征构建成本量，通过上下文调整成本量。了解成本汇总，并从成本量中回归密集深度图。成本量使用可区分的变形过程构建，该过程允许对网络进行端到端的训练。通过在深度学习框架内有效地结合传统的多视图立体概念，DPSNet在各种具有挑战性的数据集上实现了最先进的重建结果。RRPN: Radar Region Proposal Network for Object Detection in Autonomous VehiclesAuthors Ramin Nabati, Hairong Qi区域提议算法通过假设每个图像中的对象位置在大多数现有技术的两阶段对象检测网络中起重要作用。尽管如此，已知区域提议生成器是这两个阶段对象检测网络中的瓶颈，使得它们变慢并且不适合于诸如自动驾驶车辆的实时应用。在本文中，我们介绍了一种基于雷达的实时区域提议算法，用于自动驾驶车辆中的目标检测。通过将雷达检测映射到图像坐标系并且在每个映射的雷达点处生成预定义的锚框作为对象提议来生成所提出的感兴趣区域RoI。然后，我们基于对象距离对生成的锚执行变换和缩放操作，以更好地适合检测到的对象。我们使用Fast R CNN对象检测网络在新发布的NuScenes数据集上评估我们的方法。与选择性搜索对象提议算法相比，我们的模型运行速度提高了100倍以上，同时实现了更高的检测精度和召回率。代码已公开发布于3D BAT: A Semi-Automatic, Web-based 3D Annotation Toolbox for Full-Surround, Multi-Modal Data StreamsAuthors Walter Zimmer, Akshay Rangesh, Mohan Trivedi在本文中，我们专注于获取2D和3D标签，以及借助新颖的3D Bounding Box Annotation Toolbox 3D BAT跟踪道路上物体的ID。我们的开源，基于Web的3D BAT集成了多项智能功能，可提高可用性和效率。例如，此注释工具箱支持使用插值对轨道进行半自动标记，这对下游任务（如跟踪，运动规划和运动预测）至关重要。此外，通过将来自3D空间的注释投影到图像域中，自动获得所有相机图像的注释。除了原始图像和点云馈送之外，还可以使用由顶视图鸟瞰图，侧视图和正视图组成的主视图，以从不同视角观察感兴趣的对象。我们的方法与其他公开可用的注释工具的比较表明，使用我们的工具箱可以更快，更有效地获得3D注释。Optimal Multi-view Correction of Local Affine FramesAuthors Ivan Eichhardt, Daniel Barath该技术要求在每个图像对之间预先估计对极几何。它利用了摄像机运动所暗示的约束，以便将闭合形式校正应用于输入亲和度的参数。而且，示出了通过部分仿射协变检测器（例如，AKAZE或SIFT）获得的旋转和标度可以通过所提出的算法完成为全仿射帧。它在合成实验和公开可用的现实世界数据集中得到验证，该方法总是改进评估的仿射协变特征检测器的输出。作为副产品，比较这些检测器并报告获得最准确的仿射帧的检测器。为了证明其适用性，我们表明所提出的技术作为预处理步骤提高了相机装备，表面法线和单应性估计的姿态估计的准确性。Detection of Single Grapevine Berries in Images Using Fully Convolutional Neural NetworksAuthors Laura Zabawa, Anna Kicherer, Lasse Klingbeil, Andres Milioto, Reinhard T pfer, Heiner Kuhlmann, Ribana Roscher产量估算和预测在葡萄育种和葡萄栽培领域特别受关注。每株植物收获的浆果数量与产生的质量密切相关。因此，早期产量预测可以使浆果集中稀疏，以确保高质量的最终产品。传统上，产量估算是通过从小样本量推断和利用历史数据来完成的。此外，它需要由在该领域具有丰富经验的熟练专家来执行。图像中的浆果检测为专家提供了一种廉价，快速且无创的替代方案，可用于其他耗时且主观的现场分析。我们对用Phenoliner（一种野外表型分析平台）获得的图像应用完全卷积神经网络。我们计算图像中的单个浆果，以避免错误检测葡萄串。群集通常是重叠的，并且可以在大小上变化很大，这使得难以可靠地检测它们。我们特别致力于直接在葡萄园中检测白葡萄。单个浆果的检测被制定为具有三个类别的分类任务，即浆果，边缘和背景。应用连通分量算法来确定一个图像中的浆果数量。我们比较自动计数的浆果数量与手动检测到的浆果在60个图像中显示雷司令植物在垂直拍摄定位格子VSP和半最小修剪树篱SMPH。我们能够在VSP系统内正确检测浆果，准确度为94.0，SMPH系统的准确度为85.6。Single Image Portrait RelightingAuthors Tiancheng Sun, Jonathan T. Barron, Yun Ta Tsai, Zexiang Xu, Xueming Yu, Graham Fyffe, Christoph Rhemann, Jay Busch, Paul Debevec, Ravi Ramamoorthi照明在肖像照片中传达主体的本质和深度方面起着核心作用。专业摄影师将仔细控制他们工作室的灯光以操纵他们的主题的外观，而消费者摄影师通常被限制在他们的环境照明。尽管先前的工作已经探索了重新照亮图像的技术，但是由于专用硬件的要求，受控或已知照明下的受试者的多个图像，或几何和反射的精确模型，它们的效用通常是有限的。为此，我们提出了一个纵向重新点亮神经网络的系统，该系统将在无约束环境中用标准手机相机拍摄的肖像的单个RGB图像作为输入，并从该图像产生该主体的重新图像，就好像它一样。根据任何提供的环境地图照亮。我们的方法是在一个由18个人组成的小型数据库上进行训练，这个数据库是在一个由密集采样光球组成的受控光阶段设置下在不同方向光源下捕获的。与先前的工作相比，我们提出的技术在我们的数据集验证集上产生了定量优越的结果，并在数百个真实世界手机肖像的数据集上产生了令人信服的定性重新照明结果。因为我们的技术可以在160毫秒内产生640倍640的图像，所以它可以在未来实现面向摄影应用的交互式用户。Toward Extremely Low Bit and Lossless Accuracy in DNNs with Progressive ADMMAuthors Sheng Lin, Xiaolong Ma, Shaokai Ye, Geng Yuan, Kaisheng Ma, Yanzhi Wang权重量化是深度神经网络DNNs模型压缩方法中最重要的技术之一。最近使用先进的优化算法ADMM交替方向乘法方法使用DNN权重量化的系统框架的工作实现了权重量化的一种现有技术结果。在这项工作中，我们首先扩展这种基于ADMM的框架以保证解决方案的可行性，并且我们进一步开发了一个多步骤，渐进式DNN权重量化框架，由于ADMM正规化的特殊性，我实现了进一步的权重量化的双重好处，以及ii减少每个步骤中的搜索空间。广泛的实验结果证明了与先前工作相比的卓越性能。我们为MNIST的所有层LeNet 5推出了第一个无损和完全二值化的亮点。我们为CIFAR 10的所有层VGG 16和ImageNet的ResNet推出了第一个完全二值化，并且具有合理的精度损失。Full-Jacobian Representation of Neural NetworksAuthors Suraj Srinivas, Francois Fleuret诸如神经网络的非线性函数可以通过仿射平面局部地近似。最近的作品使用了输入雅可比行列式，它描述了这些平面的法线。在本文中，我们介绍了完整的雅可比行列式，其中包括这个法线以及一个称为偏差雅可比行星的附加截距项，它们共同描述了局部平面。对于ReLU神经网络，偏置雅可比行列式对应于输出w.r.t的梯度之和。中间层激活。Inverse Halftoning Through Structure-Aware Deep Convolutional Neural NetworksAuthors Chang Hwan Son逆半色调中的主要问题是去除平坦区域上的噪声点并恢复图像结构，例如纹理区域上的线条，图案。因此，本文提出了一种结构为两个子网的新结构感知深度卷积神经网络。一个子网用于图像结构预测，而另一个用于连续色调图像重建。首先，为了预测图像结构，训练包括连续色调片的片对和通过数字半色调产生的相应半色调片。随后，通过将梯度滤波器与连续色调片进行卷积来生成梯度片。在给定半色调片和梯度片的情况下，使用微批量梯度下降算法训练图像结构预测的子网，其分别被馈送到子网的输入和丢失层。接下来，包括图像结构的预测地图通过融合层堆叠在输入半色调图像的顶部，并且被馈送到图像重建子网中，使得整个网络被自适应地训练到图像结构。实验结果证实，所提出的结构感知网络可以在平坦区域上很好地去除噪声点图案并在纹理区域上清楚地恢复细节。此外，证明了所提出的方法超过了基于深度卷积神经网络和本地学习词典的传统现有技术方法。26ms Inference Time for ResNet-50: Towards Real-Time Execution of all DNNs on SmartphoneAuthors Wei Niu, Xiaolong Ma, Yanzhi Wang, Bin Ren随着一系列高端移动设备的迅速出现，许多以前需要桌面级计算能力的应用程序现在可以在这些设备上运行而没有任何问题。然而，如果不进行仔细优化，执行深度神经网络是实时视频流处理的关键构建块，这是许多常用应用程序的基础，但仍然具有挑战性，特别是如果需要极低延迟或高精度推断。这项工作介绍了CADNN，这是一个编程框架，借助先进的模型压缩稀疏性和一套全面的体系结构感知优化，在移动设备上高效地执行DNN。评估结果表明，CADNN优于所有最先进的密集DNN执行框架，如TensorFlow Lite和TVM。Land Use and Land Cover Classification Using Deep Learning TechniquesAuthors Nagesh Kumar Uba目前广泛使用表示为正射影像马赛克的亚米级航空影像的大型数据集，这些数据集可能包含大量尚未开发的信息。此图像有可能找到几种类型的功能，例如森林，停车场，机场，住宅区或图像中的高速公路。然而，这些东西的出现基于许多因素而变化，包括捕获图像的时间，传感器设置，用于矫正图像的处理以及由图像捕获的区域的地理和文化背景。本文探讨了使用深度卷积神经网络从非常高的空间分辨率VHR，正射校正，可见带多光谱图像中分类土地利用。最近的技术和商业应用已经在可见的红色，绿色，蓝色RGB光谱带中驱动了大量的VHR图像，这项工作探索了深度学习算法利用该图像进行自动土地利用土地覆盖LULC分类的潜力。Fully Automatic Brain Tumor Segmentation using a Normalized Gaussian Bayesian Classifier and 3D Fluid Vector FlowAuthors Tao Wang, Irene Cheng, Anup Basu来自磁共振图像的脑肿瘤分割MRI是测量肿瘤对治疗的反应的重要任务。但是，自动分割非常具有挑战性。本文提出了一种基于归一化高斯贝叶斯分类的自动脑肿瘤分割方法和一种新的三维流体矢量流FVF算法。在我们的方法中，提出了归一化高斯混合模型NGMM并用于模拟健康的脑组织。利用高斯贝叶斯分类器从测试脑MR图像中获取高斯贝叶斯脑图GBBM。进一步处理GBBM以初始化3D FVF算法，该算法对脑肿瘤进行分割。该算法有两个主要贡献。首先，我们提出一个NGMM来模拟健康的大脑。其次，我们将2D FVF算法扩展到3D空间并将其用于脑肿瘤分割。所提出的方法在公开可用的数据集上得到验证。NATTACK: Learning the Distributions of Adversarial Examples for an Improved Black-Box Attack on Deep Neural NetworksAuthors Yandong Li, Lijun Li, Liqiang Wang, Tong Zhang, Boqing Gong强大的对抗性攻击方法对于理解如何构建强大的深度神经网络DNN以及彻底测试防御技术至关重要。在本文中，我们提出了一种黑盒子对抗攻击算法，它可以击败香草DNN和最近开发的各种防御技术产生的DNN。我们的算法不是为目标DNN的良性输入搜索最佳对抗性示例，而是在以输入为中心的小区域上找到概率密度分布，这样从这个分布中抽取的样本很可能是一个对抗性的例子，而不需要访问DNN的内部层或权重。我们的方法是通用的，因为它可以通过单一算法成功攻击不同的神经网络。根据针对2个香草DNN和13个防御DNN的测试，它也很强大，它在大多数测试案例中都优于最先进的黑盒子或白盒攻击方法。此外，我们的结果显示，对抗性训练仍然是最好的防御技术之一，而对抗性的例子并不像防御DNN那样可以在防御性DNN中转移。ResNet Can Be Pruned 60x: Introducing Network Purification and Unused Path Removal (P-RM) after Weight PruningAuthors Xiaolong Ma, Geng Yuan, Sheng Lin, Zhengang Li, Hao Sun, Yanzhi Wang现有的DNN结构涉及高计算量和对存储器存储的巨大需求，这对DNN框架资源构成了严峻的挑战。为了缓解这些挑战，已经研究了重量修剪技术。然而，极端结构化修剪的高精度解决方案结合了不同类型的结构化稀疏性，由于DNN网络中的重量极度减少，仍然在等待解开。在本文中，我们提出了一种DNN框架，它结合了两种不同类型的结构权重修剪滤波器和柱修剪，通过结合交替方向的乘法器ADMM算法，以获得更好的修剪性能。我们是第一个在结构化修剪模型中找到ADMM过程和未使用权重的非最优性，并进一步设计一个优化框架，其中包含第一个提出的网络净化和未使用路径去除算法，这些算法专用于在ADMM之后对结构化修剪模型进行后处理脚步。一些高亮显示我们在LeNet 5上实现232x压缩，在ResNet 18 CIFAR 10上实现60x压缩，在AlexNet上实现超过5倍压缩。我们以匿名链接分享我们的模型Chinese Abs From Machine TranslationPapers from arxiv.org更多精彩请移步主页pic from pixels.com"}
{"content2":"今日CS.CV计算机视觉论文速览Mon, 18 Mar 2019Totally 48 papersDaily Computer Vision Papers[1] Title: Adversarial Joint Image and Pose Distribution Learning for Camera Pose Regression and RefinementAuthors:Mai Bui, Christoph Baur, Nassir Navab, Slobodan Ilic, Shadi Albarqouni[2] Title: PifPaf: Composite Fields for Human Pose EstimationAuthors:Sven Kreiss, Lorenzo Bertoni, Alexandre Alahi[3] Title: Selective Kernel NetworksAuthors:Xiang Li, Wenhai Wang, Xiaolin Hu, Jian Yang[4] Title: Unsupervised and interpretable scene discovery with Discrete-Attend-Infer-RepeatAuthors:Duo Wang, Mateja Jamnik, Pietro Lio[5] Title: Inserting Videos into VideosAuthors:Donghoon Lee, Tomas Pfister, Ming-Hsuan Yang[6] Title: Multi-label Cloud Segmentation Using a Deep NetworkAuthors:Soumyabrata Dev, Shilpa Manandhar, Yee Hui Lee, Stefan Winkler[7] Title: Constrained Mutual Convex Cone Method for Image Set Based RecognitionAuthors:Naoya Sogi, Rui Zhu, Jing-Hao Xue, Kazuhiro Fukui[8] Title: Superpixel Contracted Graph-Based Learning for Hyperspectral Image ClassificationAuthors:Philip Sellars, Angelica Aviles-Rivero, Carola-Bibiane Schönlieb[9] Title: Age prediction using a large chest X-ray datasetAuthors:Alexandros Karargyris, Satyananda Kashyap, Joy T Wu, Arjun Sharma, Mehdi Moradi, Tanveer Syeda-Mahmood[10] Title: Alignment Based Matching Networks for One-Shot Classification and Open-Set RecognitionAuthors:Paresh Malalur, Tommi Jaakkola[11] Title: Multi-Representational Learning for Offline Signature Verification using Multi-Loss Snapshot Ensemble of CNNsAuthors:Saeed Masoudnia, Omid Mersa, Babak N. Araabi, Abdol-Hossein Vahabie, Mohammad Amin Sadeghi, Majid Nili Ahmadabadi[12] Title: Bringing Blurry Alive at High Frame-Rate with an Event CameraAuthors:Liyuan Pan, Richard Hartley, Cedric Scheerlinck, Miaomiao Liu, Xin Yu, Yuchao Dai[13] Title: Spiking-YOLO: Spiking Neural Network for Real-time Object DetectionAuthors:Seijoon Kim, Seongsik Park, Byunggook Na, Sungroh Yoon[14] Title: Noisy Supervision for Correcting Misaligned Cadaster Maps Without Perfect Ground Truth DataAuthors:Nicolas Girard (UCA, TITANE), Guillaume Charpiat (TAU), Yuliya Tarabalka (UCA, TITANE)[15] Title: GolfDB: A Video Database for Golf Swing SequencingAuthors:William McNally, Kanav Vats, Tyler Pinto, Chris Dulhanty, John McPhee, Alexander Wong[16] Title: Phenotypic Profiling of High Throughput Imaging Screens with Generic Deep Convolutional FeaturesAuthors:Philip T. Jackson, Yinhai Wang, Sinead Knight, Hongming Chen, Thierry Dorval, Martin Brown, Claus Bendtsen, Boguslaw Obara[17] Title: SceneCode: Monocular Dense Semantic Reconstruction using Learned Encoded Scene RepresentationAuthors:Shuaifeng Zhi, Michael Bloesch, Stefan Leutenegger, Andrew J. Davison[18] Title: DeepHuman: 3D Human Reconstruction from a Single ImageAuthors:Zerong Zheng, Tao Yu, Yixuan Wei, Qionghai Dai, Yebin Liu[19] Title: BLVD: Building A Large-scale 5D Semantics Benchmark for Autonomous DrivingAuthors:Jianru Xue, Jianwu Fang, Tao Li, Bohua Zhang, Pu Zhang, Zhen Ye, Jian Dou[20] Title: Quality-aware Unpaired Image-to-Image TranslationAuthors:Lei Chen, Le Wu, Zhenzhen Hu, Meng Wang[21] Title: DFineNet: Ego-Motion Estimation and Depth Refinement from Sparse, Noisy Depth Input with RGB GuidanceAuthors:Yilun Zhang, Ty Nguyen, Ian D. Miller, hreyas S. Shivakumar, Steven Chen, Camillo J. Taylor, Vijay Kumar[22] Title: Did You Miss the Sign? A False Negative Alarm System for Traffic Sign DetectorsAuthors:Quazi Marufur Rahman, Niko Sünderhauf, Feras Dayoub[23] Title: Turbo Learning Framework for Human-Object Interactions Recognition and Human Pose EstimationAuthors:Wei Feng, Wentao Liu, Tong Li, Jing Peng, Chen Qian, Xiaolin Hu[24] Title: Unsupervised Deep Transfer Feature Learning for Medical Image ClassificationAuthors:Euijoon Ahn, Ashnil Kumar, Dagan Feng, Michael Fulham, Jinman Kim[25] Title: Flickr1024: A Dataset for Stereo Image Super-ResolutionAuthors:Yingqian Wang, Longguang Wang, Jungang Yang, Wei An, Yulan Guo[26] Title: Unsupervised Person Re-identification by Soft Multilabel LearningAuthors:Hong-Xing Yu, Wei-Shi Zheng, Ancong Wu, Xiaowei Guo, Shaogang Gong, Jian-Huang Lai[27] Title: SimulCap : Single-View Human Performance Capture with Cloth SimulationAuthors:Tao Yu, Zerong Zheng, Yuan Zhong, Jianhui Zhao, Qionghai Dai, Gerard Pons-Moll, Yebin Liu[28] Title: Pose Graph Optimization for Unsupervised Monocular Visual OdometryAuthors:Yang Li, Yoshitaka Ushiku, Tatsuya Harada[29] Title: Show, Translate and TellAuthors:Dheeraj Peri, Shagan Sah, Raymond Ptucha[30] Title: Distance Preserving Grid LayoutsAuthors:Gladys Hilasaca, Fernando V. Paulovich[31] Title: Graph Hierarchical Convolutional Recurrent Neural Network (GHCRNN) for Vehicle Condition PredictionAuthors:Mingming Lu, Kunfang Zhang, Haiying Liu, Naixue Xiong[32] Title: Mixture Modeling of Global Shape Priors and Autoencoding Local Intensity Priors for Left Atrium SegmentationAuthors:Tim Sodergren, Riddhish Bhalodia, Ross Whitaker, Joshua Cates, Nassir Marrouche, Shireen Elhabian[33] Title: Conditional GANs For Painting GenerationAuthors:Adeel Mufti, Biagio Antonelli, Julius Monello[34] Title: Hyperspectral Image Classification with Deep Metric Learning and Conditional Random FieldAuthors:Yi Liang, Xin Zhao, Alan J.X. Guo, Fei Zhu[35] Title: Unpaired image denoising using a generative adversarial network in X-ray CTAuthors:Hyoung Suk Park, Jineon Baek, Sun Kyoung You, Jae Kyu Choi, Jin Keun Seo[36] Title: Learning Robust Representations by Projecting Superficial Statistics OutAuthors:Haohan Wang, Zexue He, Zachary C. Lipton, Eric P. Xing[37] Title: Active Transfer Learning for Persian Offline Signature VerificationAuthors:Taraneh Younesian, Saeed Masoudnia, Reshad Hosseini, Babak N. Araabi[38] Title: Demonstration of Vector Flow Imaging using Convolutional Neural NetworksAuthors:Thomas Robins, Antonio Stanziola, Kai Reimer, Peter Weinberg, Meng-Xing Tang[39] Title: Object tracking in video signals using Compressive SensingAuthors:Marijana Kracunov, Milica Bastica, Jovana Tesovic[40] Title: Learning Representations from Persian Handwriting for Offline Signature Verification, a Deep Transfer Learning ApproachAuthors:Omid Mersa, Farhood Etaati, Saeed Masoudnia, Babak N. Araabi[41] Title: SuperTML: Two-Dimensional Word Embedding and Transfer Learning Using ImageNet Pretrained CNN Models for the Classifications on Tabular DataAuthors:Baohua Sun, Lin Yang, Wenhan Zhang, Michael Lin, Patrick Dong, Charles Young, Jason Dong[42] Title: Application-level Studies of Cellular Neural Network-based Hardware AcceleratorsAuthors:Qiuwen Lou, Indranil Palit, Tang Li, Andras Horvath, Michael Niemier, X. Sharon Hu[43] Title: TinBiNN: Tiny Binarized Neural Network Overlay in about 5,000 4-LUTs and 5mWAuthors:Guy G.F. Lemieux, Joe Edwards, Joel Vandergriendt, Aaron Severance, Ryan De Iaco, Abdullah Raouf, Hussein Osman, Tom Watzka, Satwant Singh[44] Title: Technically correct visualization of biological microscopic experimentsAuthors:Ganna Platonova, Dalibor Stys, Pavel Soucek, Petr Machacek, Vladimir Kotal, Renata Rychtarikova[45] Title: MFAS: Multimodal Fusion Architecture SearchAuthors:Juan-Manuel Pérez-Rúa, Valentin Vielzeuf, Stéphane Pateux, Moez Baccouche, Frédéric Jurie[46] Title: Integrating NVIDIA Deep Learning Accelerator (NVDLA) with RISC-V SoC on FireSimAuthors:Farzad Farshchi, Qijing Huang, Heechul Yun[47] Title: Stitching Videos from a Fisheye Lens Camera and a Wide-Angle Lens Camera for Telepresence RobotsAuthors:Yanmei Dong, Mingtao Pei, Lijia Zhang, Bin Xu, Yuwei Wu, Yunde Jia[48] Title: Learning to Generate the “Unseen” via Part Synthesis and CompositionAuthors:Nadav Schor, Oren Katzir, Hao Zhang, Daniel Cohen-OrPapers from arxiv.org更多精彩请移步主页Interesting:📚, (from):pic from pixels.com"}
{"content2":"计算机视觉领域研究资源及期刊、会议介绍依照下面目录整理：[1]研究群体(国际国内)、专家主页[2]前沿国际国内期刊与会议[3]搜索资源[4]GPL软件资源一、研究群体作机器视觉和图像处理方面的研究工作，最重要的两个问题：其一是要把握住国际上最前沿的内容；其二是所作工作要具备很高的实用背景。解决第一个问题的办法就是找出这个方向公认最高成就的几个超级专家(看看他们都在作什么)和最权威的出版物(阅读上面最新的文献)，解决第二个问题的办法是你最好能够找到一个实际应用的项目，边做边写文章。 做好这几点的途径之一就是利用网络资源，利用权威网站和专家们的个人主页。依照下面目录整理：[1]研究群体(国际国内)[2]专家主页[3]前沿国际国内期刊与会议[4]搜索资源[5]GPL软件资源一、研究群体用来搜索国际知名计算机视觉研究组(CV Groups)：国际计算机视觉研究组清单http://peipa.essex.ac.uk/info/groups.html美国计算机视觉研究组清单 http://peipa.essex.ac.uk/info/groups.html#USAhttp://www-2.cs.cmu.edu/~cil/vision.html或 http://www.cs.cmu.edu/~cil/vision.html这是卡奈基梅隆大学的计算机视觉研究组的主页，上面提供很全的资料，从发表文章的下载到演示程序、测试图像、常用链接、相关软硬件，甚至还有一个搜索引擎。著名的有人物Tomasi， Kanade等。卡内基梅隆大学双目实验室http://vision.middlebury.edu/stereo/卡内基梅隆研究组http://www.cs.cmu.edu/~cil/v-groups.html还有几个实验室：Calibrated Imaging Laboratory 图像Digital Mapping Laboratory 映射Interactive Systems Laboratory 互动Vision and Autonomous Systems Center视觉自适应http://www.via.cornell.edu/康奈尔大学的计算机视觉和图像分析研究组，好像是电子和计算机工程系的。侧重医学方面的研究，但是在上面有相当不错资源，关键是它正在建设中，能够跟踪一些信息。Cornell University——Robotics and Vision grouphttp://www-cs-students.stanford.edu/ 斯坦福大学计算机系主页1. http://white.stanford.edu/2. http://vision.stanford.edu/3. http://ai.stanford.edu/美国斯坦福大学人工智能机器人实验室The Stanford AI Lab (SAIL) is the intellectual home for researchers in the Stanford Computer Science Department whose primary research focus is Artificial Intelligence. The lab is located in the Gates...Vision and Imaging Science and Technologyhttp://www.fmrib.ox.ac.uk/analysis/主要研究：Brain Extraction Tool， Nonlinear noise reduction， Linear Image Registration， Automated Segmentation， Structural brain change analysis， motion correction， etc.http://www.cse.msu.edu/prip/—密歇根州立大学计算机和电子工程系的模式识别--图像处理研究组，它的FTP上有许多的文章(NEW)。美国密歇根州大学认知模型和图像处理实验室The Pattern Recognition and Image Processing (PRIP) Lab faculty and students investigate the use of machines to recognize patterns or objects. Methods are developed to sense objects， to discover which...http://www.cse.msu.edu/rgroups/prip/http://pandora.inf.uni-jena.de/p/e/index.html德国的一个数字图像处理研究小组，在其上面能找到一些不错的链接资源。柏林大学 http://www.cv.tu-berlin.de/德国波恩大学视觉和认识模型小组Computer Vision Group located within the Division III of the Computer Science Department in the University of Bonn in Germany. This server offers information on topics concerning our computer vision http://www-dbv.informatik.uni-bonn.de/http://www-staff.it.uts.edu.au/~sean/CVCC.dir/home.htmlCVIP(used to be CVCC for Computer Vision and Cluster Computing) is a research group focusing on cluster-based computer vision within the Spiral Architecture.http://cfia.gmu.edu/The mission of the Center for Image Analysis is to foster multi-disciplinary research in image， multimedia and related technologies by establishing links between academic institutes， industry and government agencies， and to transfer key technologies to help industry build next generation commercial and military imaging and multimedia systems.英国的Bristol大学的Digital Media Group在高级图形图像方面不错。主要就是涉及到场景中光线计算的问题，比如用全局光照或是各种局部光照对高动态图的处理，还有近似真实的模拟现实环境 (照片级别的)，还有用几张照片来建立3D模型(人头之类的)。另外也有对古代建筑模型复原。http://www.cs.bristol.ac.uk/Research/Digitalmedia/而且根据Times全英计算机排名在第3， 也算比较顶尖的研究了http://www.cmis.csiro.au/IAP/zimage.htm这是一个侧重图像分析的站点，一般。但是提供一个Image Analysis环境---ZIMAGE and SZIMAGE。麻省理工视觉实验室MIT http://groups.csail.mit.edu/vision/welcome/AI Laboratory Computer Vision groupCenter for Biological and Computational LearningMedia Laboratory， Vision and Modeling GroupPerceptual Science groupUC Berkeley http://0-vision.berkeley.edu.ilstest.lib.neu.edu/vsp/index.htmlhttp://www.cs.berkeley.edu.ilste ... n/vision_group.html加州大学伯克利分校视觉实验室David A. Forsyth：http://www.cs.berkeley.edu/~daf/UCLA(加州大学洛杉矶分校) http://vision.ucla.edu/ 视觉实验室英国牛津的A.Zisserman：http://www.robots.ox.ac.uk/~az/ 机器人实验室美国南加州大学智能机器人和智能系统研究所University of Southern California， Los AngelesIRIS is an interdepartmental unit of USC's School of Engineering with ties to USC's Information Sciences Institute (ISI). Members include faculty， graduate students， and research staff associated with...http://iris.usc.edu/ Computer Vision 实验室美国南加州大学计算机视觉实验室介绍：Computer Vision Laboratory at the University of Southern California is one of the major centers of computer vision research for thirty years. they conduct research in a number of basic and applied are...http://iris.usc.edu/USC-Computer-Vision.html英国约克大学高级计算机结构神经网络小组The Advanced Computer Architecture Group has had a thriving research programme in neural networks for over 10 years. The 15 researchers， led by Jim Austin， focus their work in the theory and applicati...http://www.cs.york.ac.uk/arch/neural/瑞士戴尔莫尔感知人工智能研究所IDIAP is a research institute established in Martigny in the Swiss Alps since 1991. Active in the areas of multimodal interaction and multimedia information management， the institute is also the leade...http://www.idiap.ch/英国萨里大学视觉，语言和信号处理中心The Centre for Vision， Speech and Signal Processing (CVSSP) is more than 60 members strong， comprising 12 academic staff， 18 research fellows and more than 44 research students. The activities of the ...http://www.ee.surrey.ac.uk/Research/VSSP/美国阿默斯特马萨诸塞州立大学计算机视觉实验室The Computer Vision Laboratory was established in the Computer Science Department at the University of Massachusetts in 1974 with the goal of investigating the scientific principles underlying the con...http://vis-www.cs.umass.eduUniversity of Massachusetts——Computer Vision Laboratory for Perceptual Robotics美国芝加哥伊利诺伊斯大学贝克曼研究中心智能机器人和计算机视觉实验室Includes the following groups: Professor Seth Hutchinson's Research Group Professor David Kriegman's Research Group Professor Jean Ponce's Research Group Professor Narendra Ahuja's Research Gro...http://www-cvr.ai.uiuc.edu/Computer Vision and Robotics LaboratoryVision Interfaces and Systems Laboratory (VISLab)英国伯明翰大学计算机科学学校视觉研究小组The vision group at the School of Computer Science (a RAE 5 rated department) performs research into a wide variety of computer vision and image understanding areas. Much of this work is performed in ...http://www.cs.bham.ac.uk/research/vision/微软研究院机器学习与理解研究小组 / 计算机视觉小组The research group focuses on the development of more advanced and intelligent computer systems through the exploitation of statistical methods in machine learning and computer vision. The site lists ...http://research.microsoft.com/mlp/http://research.microsoft.com/en-us/groups/vision/微软公司的文献：http://research.microsoft.com/research/pubs微软亚洲研究院：http://research.microsoft.com/asia/，值得关注Harry Shum， Jian Sun， Steven Lin， Long Quan(兼职HKUST)etc.瑞典隆德大学数学系视觉组：http://www.maths.lth.se/matematiklth/personal/andersp/感觉国外搞视觉的好多是数学系出身，大约做计算机视觉对数学要求很高吧。澳大利亚国立大学：http://users.rsise.anu.edu.au/~hartley/美国北卡大学：http://www.cs.unc.edu/~marc/法国INRIA：http://www-sop.inria.fr/odyssee/team/ 由Olivier.Faugeras领衔的牛人众多。比利时鲁汶大学的L.Van Gool： www.esat.kuleuven.ac.be/psi/visics/据说在这个只有中国一个小镇大小的地方的鲁汶大学在欧洲排行top10，名列世界top100，还出了几个诺贝尔奖，视觉研究也很强.美国明德http://vision.middlebury.edu/stereo/以下含有非顶尖美国学校研究组，没有链接(个别的上面已经提到)，供参考。Amerinex Applied Imaging， Inc.Boston UniversityImage and Video Computing Research groupUniversity of California at Santa Barbara加州大学芭芭拉分校Vision Research LabUniversity of California at San Diego加州大学圣迭戈分校Computer Vision & Robotics Research LaboratoryVisual Computing laboratoryUniversity of California at Irvine加州大学欧文分校，加州南部一城，在圣安娜东南，Computer Vision laboratoryUniversity of California， Riverside加州大学河滨分校Visualization and Intelligent Systems Laboratory (VISLab)University of California at Santa CruzPerceptual Science LaboratoryCaltech (加州理工)Vision groupUniversity of Central FloridaComputer Vision laboratoryUniversity of FloridaCenter for Computer Vision and VisualizationColorado State UniversityComputer Vision groupColumbia UniversityAutomated Vision Environment (CAVE)Robotics groupUniversity of Georgia， AthensVisual and Parallel Computing LaboratoryHarvard University（哈佛）Robotics LaboratoryUniversity of Illinois at Urbana-ChampaignRobotics and Computer VisionUniversity of IowaDivision of Physiologic ImagingJet Propulsion LaboratoryMachine Vision and Tracking Sensors groupKhoral Research， IncLawrence Berkeley LaboratoriesImaging and Collaborative Computing GroupImaging and Distributed ComputingLehigh UniversityImage Processing and Pattern Analysis LabVision And Software Technology LaboratoryUniversity of LouisvilleComputer Vision and Image Processing LabUniversity of MarylandComputer Vision LaboratoryUniversity of MiamiUnderwater Vision and Imaging LaboratoryUniversity of Michigan密歇根AI LaboratoryMichigan State University 密歇根州立Pattern Recognition and Image Processing laboratoryEnvironmental Research Institute of Michigan (ERIM) 密歇根大学有汽车车身检测研究University of Missouri-ColumbiaComputational Intelligence Research LaboratoryNECComputer Vision and Image ProcessingUniversity of NevadaComputer Vision LaboratoryNotre-Dame UniversityVision-Based Robotics using EstimationOhio State UniversitySignal Analysis and Machine Perception LaboratoryUniversity of PennsylvaniaGRASP laboratoryMedical Image Processing groupVision Analysis and Simulation Technologies (VAST) LaboratoryPenn State University 宾夕法尼亚大学Computer VisionPrecision Digital ImagesPurdue University普渡大学Robot Vision laboratoryVideo and Image Processing Laboratory (VIPER)Rensselaer Polytechnic Institute (RPI)Computer Science VisionUniversity of RochesterCenter for Electronic Imaging SystemsVision and Robotics laboratoryRutgers University (The State University of New Jersey)Image Understanding LabUniversity of Southern CaliforniaComputer VisionUniversity of South FloridaImage Analysis Research groupStanford Research Institute International (SRI)RADIUS -- Research and Development for Image Understanding SystemsThe Perception program at SRI's AI CenterSUNY at Stony BrookComputer Vision LabUniversity of TennesseeImaging， Robotics and Intelligent Systems laboratoryUniversity of Texas， AustinLaboratory for Vision SystemsUniversity of UtahCenter for Scientific Computing and ImagingRobotics and Computer VisionUniversity of VirginiaComputer Vision Research (CS)University of WashingtonImage Computing Systems LaboratoryInformation Processing LaboratoryCVIA LaboratoryUniversity of West FloridaImage Analysis/Robotics Research LaboratoryUniversity of WisconsinComputer Vision groupVanderbilt UniversityCenter for Intelligent SystemsWashington State UniversityImaging Research laboratoryWright-PattersonModel-Based Vision laboratoryWright State UniversityIntelligent Systems LaboratoryUniversity of WyomingWyoming Image and Signal Processing Research (WISPR)Yale UniversityComputational Vision Group http://www.cs.yale.edu/School of Medicine， Image Processing and Analysis group国内：中科院模式识别国家重点实验室 http://www.nlpr.ia.ac.cn/English/rv/mainpage.html虹膜识别、掌纹识别、人脸识别、莲花山http://www.stat.ucla.edu/~sczhu/Lotus/天津大学精密测试技术及仪器国家重点实验室研究方向包括：激光及光电测试技术、传感及测量信息技术、微纳测试与制造技术、制造质量控制技术。该实验室是国内精密测试领域惟一的国家重点实验室。“智能微系统及其集成应用技术”、“微结构光学测试技术”、“油气储运安全检测技术”、“先进制造中的视觉测量及其关键技术”、“正交偏振激光器原理、特性及其在精密计量中的应用研究”等5项代表性成果（07.3）。中科院长春光机所 http://www.ciomp.ac.cn/ny/keyan.asp中科院沈阳自动化所http://www.sia.ac.cn/index.php中科院西安光机所http://www.opt.ac.cn/yanjiushi/gpcxjs1.htm北京大学智能科学系http://www.cis.pku.edu.cn/vision/vision.htm三维视觉计算与机器人，生物特征识别与图像识别二、专家网页http://www.ai.mit.edu/people/wtf/这位可是MIT人工智能实验室的BILL FREEMAN。专长是：理解--贝叶斯模型。http://www.merl.com/people/brand/MERL(Mitsubishi Electric Research Laboratory)中的擅长“Style Machine”。http://research.microsoft.com/~ablake/CV界极有声望的A.Blake 1977年毕业于剑桥大学三一学院并或数学与电子科学学士学位。之后在MIT，Edinburgh，Oxford先后组建过研究小组并成为Oxford的教授，直到1999年进入微软剑桥研究中心。主要工作领域是计算机视觉。http://www-2.cs.cmu.edu/afs/cs.cmu.edu/user/har/Web/home.html这位专家好像正在学习汉语，主页并且搜集了诸如“两只老虎(Two Tigers)”的歌曲。他的主页上面还有几个专家：Shumeet Baluja， Takeo Kanade。他们的Face Detection作的绝对是世界一流。毕业于卡奈基梅隆大学的计算机科学系，兴趣是计算机视觉。以下是我经常浏览的网络资源，关注大牛的网页，比上学校数据库资源更精、更有启发性。排名不分先后，呵呵~~~（1）微软公司的文献：http://research.microsoft.com/research/pubs（2）微软亚洲研究院：http://research.microsoft.com/asia/，值得关注Harry Shum, Jian Sun, Steven Lin, Long Quan(兼职HKUST)etc.（3）瑞典隆德大学数学系视觉组：http://www.maths.lth.se/matematiklth/personal/andersp/感觉国外搞视觉的好多是数学系出身，大约做计算机视觉对数学要求很高吧。（4）澳大利亚国立大学：http://users.rsise.anu.edu.au/~hartley/（5）美国北卡大学：http://www.cs.unc.edu/~marc/（6）加州大学伯克利分校David A. Forsyth：http://www.cs.berkeley.edu/~daf/（7）CMU的视觉组：http://www.cs.cmu.edu/~cil/vision.html著名的有Tomasi, Kanade等，CMU不愧是美国计算机牛校，仅视觉就好猛。（8）法国INRIA：http://www-sop.inria.fr/odyssee/team/由Olivier.Faugeras领衔的牛人众多。（9）英国牛津的A.Zisserman：http://www.robots.ox.ac.uk/~az/(10)比利时鲁汶大学的L.Van Gool： www.esat.kuleuven.ac.be/psi/visics/据说在这个只有中国一个小镇大小的地方的鲁汶大学在欧洲排行top10，名列世界top100，还出了几个诺贝尔奖，视觉研究也很牛，真是让Chinese汗颜啊！还有一些也常看的，留给来浏览的同仁回复补充吧。。二、前沿国际国内期刊与会议这里的期刊大部分都可以通过上面的专家们的主页间接找到1.国际会议 2.国际期刊 3.国内期刊 4.神经网络 5.CV 6.数字图象 7.教育资源，大学 8.常见问题1. 国际会议现在，国际上计算机视觉方面的三大国际会议是ICCV， CVPR和ECCV，统称之为ICE。ICCV的全称是International Comference on Computer Vision。ICCV两年一次，与ECCV正好错开，是公认的三个会议中级别最高的。ECCV的全称是Europeon Conference on Computer Vision，是一个欧洲的会议。CVPR的全称是Internaltional Conference on Computer Vision and Pattern Recogintion国际计算机视觉与模式识别会议。这是一个一年一次的会议，举办地在美国。ICIP—BMVC—MVA—国际模式识别会议(ICPR )：亚洲计算机视觉会议(ACCV)：2.国际期刊以计算机视觉为主要内容之一的国际刊物也有很多，如:International Journal of Computer VisionIEEE Trans. On PAMI http://www.computer.org/tpami/index.htmIEEE Transactionson Image Processing http://www.ieee.org/organizations/pubs/transactions/tip.htmPattern Recognition http://www.elsevier.com/locate/issn/00313203Pattern Recognition Letters http://www.elsevier.com/locate/issn/01678655IEEE Trans. on Robotics and Automation，IEEE TPAMIIEEE TIPCVGIP Computer Vision. Graphics and Image Processing，Visual Image Computing，IJPRAI(Internatiorial Journat of Pattern Recognition and Artificial Intelligence)众所周知， computer vision(cv) 存在ICCV/CVPR/ECCV三个顶级会议，它们档次差不多，都应该在一流会议行列，没有必要给个高下。有些us的人认为ICCV/CVPR略好于ECCV，而欧洲人大都认为ICCV/ECCV略好于CVPR，某些英国的人甚至认为BMVC好于CVPR。简言之，三个会议差不多， 各有侧重和偏好。笔者就个人经验浅谈三会异同， 以供大家参考和讨论。 三者乃cv领域的旗舰和风向标，其oral paper (包括best paper) 代表当年度cv的最高水准， 在此引用Harry Shum的一句话，想知道某个领域在做些什么，找最近几年此领域的proceeding看看就知道了。 ICCV/CVPR由IEEE Computer Society牵头组织， ECCV好像没有专门负责的组织。 CVPR每年(除2002年)都在美国开， ECCV每两年开一次，仅限欧洲， ICCV也是每两年一次，各洲轮值。 基本可以保证每年有两个会议开， 这样研究者就有两次跻身牛会的机会。就录取率而言， 三会都有波动。如ICCV2001录取率>30%，且出现两个人(华人)各有三篇第一作者的paper的情况，这在顶级牛会是不常见的 (灌水嫌疑)。 但是， ICCV2003， 2005两次录取率都很低， 大约20%左右。 ECCV也是类似规律， 在2004年以前都是>30%， 2006年降低到20%左右。 CVPR的录取率近年来一直偏高，从2004年开始一直都在[25%，30%]。最近一次CVPR2006是28.1%， CVPR2007还不知道统计数据。笔者猜测为了维持录取paper的绝对数量， 当submission少的时候录取率偏高，反之偏低，近几年三大会议的投稿数量全部超过1000， 相对2000年前，三会录取率均大幅度降低，最大幅度50%->20%。 对录取率走势感兴趣的朋友， 可参考 http://vrlab.epfl.ch/~ulicny/statistics/(CVPR2004的数据是错的)，http://www.adaptivebox.net/research/bookmark/CICON_stat.html.显然， 投入cv的人越来越多，这个领域也是越来越大， 这点颇不似machine learning一直奉行愚蠢的小圈子主义。另外一点值得注意， ICCV/ECCV只收vision相关的topic，而cvpr会收少量的pattern recognition paper，如finger print等，但是不收和image/video完全不占边的pr paper，如speech recognition等。我一个朋友曾经review过一篇投往CVPR的speech的paper，三个reviewer一致拒绝，其中一个reviewer搞笑的指出， 你这篇paper应该是投ICASSP被据而转投CVPR的。就topic而言， CVPR涵盖最广。 还有一个没有验证过的原因导致CVPR录取率高: 很多us的researcher不愿意或没有足够的经费到us以外的地方开会， 故CVPR会优先接收很多来自us的paper (让大家都happy)。以上对三会的分析对我们投paper是很有指导作用的。 目前的research我想绝大部分还是纸上谈兵， 必经 read paper -> write paper -> publish paper -> publish paper on top conferences and journals流程。故了解投paper的一些基本技巧， 掌握领域的走向和热点， 是非常必要的。避免做无用功，选择切合的topic，改善presentation， 注意格式 (遵守规定的模板)，我想这是很多新手需要注意的问题。如ICCV2007明文规定不写summary page直接reject， 但是仍然有人忽视，这是相当不值得的。ICCV的全称是International Comference on Computer Vision（上一篇文章介绍我自己的id的时候介绍过，呵呵），正如很多和他一样的名字的会议一行，这样最朴实的名字的会议，通常也是这方面最nb的会议。ICCV两年一次，与ECCV正好错开，是公认的三个会议中级别最高的。它的举办地方会在世界各地选，上次是在北京，下次在巴西，2009在日本。iccv上的文章看起来一般都比较好懂，我是比较喜欢的。CVPR的全称是Internaltional Conference on Computer Vision and Pattern Recogintion。这是一个一年一次的会议，举办地从来没有出过美国，因此想去美国旅游的同学不要错过。正如它的名字一样，这个会上除了视觉的文章，还会有不少模式识别的文章，当然两方面的结合自然也是重点。ECCV的全称是Europeon Conference on Computer Vision，是一个欧洲的会议。虽然名字不是International，但是会议的级别不比前面两个差多少。欧洲人一般比较看中理论，但是从最近一次会议来看，似乎大家也开始注重应用了，oral里面的demo非常之多，演示效果很好，让人赏心悦目、叹为观止。不过欧洲的会有一个不好，就是他们的人通常英语口音很重，有些人甚至不太会说英文，所以开会和交流的时候，稍微有些费劲。总的来说，以上三个会议是做计算机视觉人必须关注的会议，建议每一期的oral都要精读，poster挑自己相关的仔细看看。如果有好的进一步的想法，可以马上发表，因为他们已经是最新的了，对他们的改进通常也是最新的。同时如果你做了类似的工作，却没有引用这些会议的文章，很有可能会被人指出综述部分的问题，因为评审的人一般都是牛人，对这三个会议也会很关注的。ICCV/CVPR/ECCV三个顶级会议, 它们档次差不多,都应该在一流会议行列, 没有必要给个高下. 有些us的人认为ICCV/CVPR略好于ECCV,而欧洲人大都认为ICCV/ECCV略好于CVPR, 某些英国的人甚至认为BMVC好于CVPR.简言之, 三个会议差不多, 各有侧重和偏好.笔者就个人经验浅谈三会异同, 以供大家参考和讨论. 三者乃cv领域的旗舰和风向标,其oral paper (包括best paper) 代表当年度cv的最高水准, 在此引用Harry Shum的一句话, 想知道某个领域在做些什么, 找最近几年此领域的proceeding看看就知道了. ICCV/CVPR由IEEE Computer Society牵头组织, ECCV好像没有专门负责的组织. CVPR每年(除2002年)都在美国开, ECCV每两年开一次,仅限欧洲, ICCV也是每两年一次, 各洲轮值. 基本可以保证每年有两个会议开, 这样研究者就有两次跻身牛会的机会.就录取率而言, 三会都有波动. 如ICCV2001录取率>30%, 且出现两个人(华人)各有三篇第一作者的paper的情况, 这在顶级牛会是不常见的 (灌水嫌疑). 但是, ICCV2003, 2005两次录取率都很低, 大约20%左右. ECCV也是类似规律, 在2004年以前都是>30%, 2006年降低到20%左右. CVPR的录取率近年来一直偏高, 从2004年开始一直都在[25%,30%].最近一次CVPR2006是28.1%, CVPR2007还不知道统计数据. 笔者猜测为了维持录取paper的绝对数量, 当submission少的时候录取率偏高, 反之偏低, 近几年三大会议的投稿数量全部超过1000, 相对2000年前, 三会录取率均大幅度降低, 最大幅度50%->20%. 对录取率走势感兴趣的朋友, 可参考 http://vrlab.epfl.ch/~ulicny/statistics/(CVPR2004的数据是错的),http://www.adaptivebox.net/research/bookmark/CICON_stat.html .显然, 投入cv的人越来越多,这个领域也是越来越大, 这点颇不似machine learning一直奉行愚蠢的小圈子主义. 另外一点值得注意, ICCV/ECCV只收vision相关的topic, 而cvpr会收少量的pattern recognition paper, 如fingerprint等, 但是不收和image/video完全不占边的pr paper,如speech recognition等. 我一个朋友曾经review过一篇投往CVPR的speech的paper, 三个reviewer一致拒绝, 其中一个reviewer搞笑的指出, 你这篇paper应该是投ICASSP被据而转投CVPR的. 就topic而言, CVPR涵盖最广. 还有一个没有验证过的原因导致CVPR录取率高: 很多us的researcher不愿意或没有足够的经费到us以外的地方开会, 故CVPR会优先接收很多来自us的paper (让大家都happy).以上对三会的分析对我们投paper是很有指导作用的. 目前的research我想绝大部分还是纸上谈兵, 必经 read paper -> write paper -> publish paper -> publish paper on top conferences and journals流程. 故了解投paper的一些基本技巧, 掌握领域的走向和热点, 是非常必要的. 避免做无用功,选择切合的topic, 改善presentation, 注意格式 (遵守规定的模板), 我想这是很多新手需要注意的问题. 如ICCV2007明文规定不写summary page直接reject, 但是仍然有人忽视, 这是相当不值得的3.国内期刊自动化学报、计算机学报、软件学报、电子学报，中国图象图形学报，模式识别与人工智能，光电子激光，精密光学工程等。4.神经网络神经网络-Neural Networks Tutorial Reviewhttp://hem.hj.se/~de96klda/NeuralNetworks.htmftp://ftp.sas.com/pub/neural/FAQ.htmlImage Compression with Neural Networkshttp://www.comp.glam.ac.uk/digimaging/neural.htmBackpropagator's Reviewhttp://www.dontveter.com/bpr/bpr.htmlBibliographies on Neural Networkshttp://liinwww.ira.uka.de/bibliography/Neural/Intelligent Motion Control with an Artificial Cerebellumhttp://www.q12.org/phd.htmlKernel Machineshttp://www.kernel-machines.org/Some Neural Networks Research Organizationshttp://www.ieee.org/nnc/http://www.inns.org/Neural Network Modeling in Vision Researchhttp://www.rybak-et-al.net/nisms.htmlNeural Networks and Machine Learninghttp://learning.cs.toronto.edu/Neural Application Softwarehttp://attrasoft.comNeural Network Toolbox for MATLABhttp://www.mathworks.com/products/neuralnet/Netlab Softwarehttp://www.ncrg.aston.ac.uk/netlab/Kunama Systems Limited http://www.kunama.co.uk/5.Computer Vision(计算机视觉)Annotated Computer Vision Bibliographyhttp://iris.usc.edu/Vision-Notes/bibliography/contents.htmlhttp://iris.usc.edu/Vision-Notes/rosenfeld/contents.htmlLawrence Berkeley National Lab Computer Vision and Robotics Applicationshttp://www-itg.lbl.gov/ITG.hm.pg.docs/VISIon/vision.htmlCVonline by University of EdinburghThe Evolving， Distributed， Non-Proprietary， On-Line Compendium of Computer Vision， www.dai.ed.ac.uk/CVonlineComputer Vision Handbook，www.cs.hmc.edu/~fleck/computer-vision-handbookVision Systems Coursewarewww.cs.cf.ac.uk/Dave/Vision_lecture/Vision_lecture_caller.htmlResearch Activities in Computer Visionhttp://www-syntim.inria.fr/syntim/analyse/index-eng.htmlVision Systems Acronymswww.vision-systems-design.com/vsd/archive/acronyms.htmlDictionary of Terms in Human and Animal Visionhttp://cns-web.bu.edu/pub/laliden/WWW/Visionary/Visionary.htmlMetrology based on Computer Visionwww.cranfield.ac.uk/sme/amac/research/metrology/metrology.html6.Digital Photography 数字图像Digital Photography， Scanning， and Image Processingwww.dbusch.com/scanners/scanners.htm l7.Educational Resources， Universities 教育资源，大学Center for Image Processing in Educationwww.cipe.comLibrary of Congress Call Numbers Related to Imaging Science by Rochester Institute of Technologyhttp://wally2.rit.edu/pubs/guides/imagingcall.htmlMathematical Experiences through Image Processing， University of Washingtonwww.cs.washington.edu/research/metip/metip.htmlVismod Tech Reports and Publications， MIThttp://vismod.www.media.mit.edu/cgi-bin/tr_pagemakerVision Lab PhD dissertation list， University of Antwerphttp://wcc.ruca.ua.ac.be/~visielab/theses.htmlINRIA (France) Research Projects: Human-Computer Interaction， Image Processing， Data Management， Knowledge Systemswww.inria.fr/Themes/Theme3-eng.htmlImage Processing Resourceshttp://eleceng.ukc.ac.uk/~rls3/Contents.htmPublications of Carsten Stegerhttp://www9.informatik.tu-muench ... r/publications.html8.FAQs（常见问题）comp.dsp FAQwww.bdti.com/faq/dsp_faq.htmRobotics FAQwww.frc.ri.cmu.edu/robotics-faqWhere's the sci.image.processing FAQ?www.cc.iastate.edu/olc_answers/p ... processing.faq.htmlcomp.graphics.algorithms FAQ， Section 3， 2D Image/Pixel Computationswww.exaflop.org/docs/cgafaqAstronomical Image Processing System FAQwww.cv.nrao.edu/aips/aips_faq.html三、搜索资源http://sal.kachinatech.com/http://cheminfo.pku.edu.cn/mirrors/SAL/index.shtml 北京大学Google输入：computer vision 或computer vision groups可以获得很多结果网络资源：CVonline http://homepages.inf.ed.ac.uk/rbf/CVonline/视觉研究组列表Computer vision test Image http://www.cs.cmu.edu/~cil/v-images.html卡内基梅隆标准图片库视觉论文搜索：Paper searchhttp://www.researchindex.com四、图像处理GPL库（代码库图像库等）http://www.ph.tn.tudelft.nl/~klamer/cppima.htmlCppima 是一个图像处理的C++函数库。这里有一个较全面介绍它的库函数的文档，当然你也可以下载压缩的GZIP包，里面包含TexInfo格式的文档。http://iraf.noao.edu/Welcome to the IRAF Homepage! IRAF is the Image Reduction and Analysis Facility， a general purpose software system for the reduction and analysis of astronomical datahttp://entropy.brni-jhu.org/tnimage.html一个非常不错的Unix系统的图像处理工具，看看它的截图。你可以在此基础上构建自己的专用图像处理工具包。http://sourceforge.net/projects/这是GPL软件集散地，可以搜索IP库。"}
{"content2":"来源商业新知网，原标题：【技术】浅析人工智能三大核心技术近年来，中国人工智能高速发展。无人超市、无人物流、无人加油站、无人驾驶、无人酒店……再加上各种功能健全的机器人！在我们毫无察觉中，人工智能正在日夜不息地自我迭代进化，冲击着我们生活的方方面面。在这些令人诧异和振奋的事件背后，离不开计算机视觉、自然语言处理、生物特征识别及知识图谱等人工智能关键技术。可以说，在人工智能产业中，技术是连接芯片和应用场景的纽带，决定了产品的智能化程度。计算机视觉人工智能系统的大门根据实际解决的问题， 计算机视觉技术可分为 人脸识别、图像检测、图像检索、目标跟踪、风格迁移 等几大板块。 其中，人脸识别、图像分类等功能计算机视觉技术已经比人类视觉更精准、更迅速。在医院，一般早期食管癌检出率低于 10% ，而腾讯觅影通过扫描上消化道内镜图片筛查食管癌，检出率高达 90% ，且用时不到 4 秒。商汤科技宣称，利用其计算机视觉技术，视频内容审核能够节省 99% 的人工。然而，虽然在解决识别、检测、聚类等问题上，计算机视觉已经可以超越人类，但其发展仍面临挑战。首先， 缺乏可用于人工智能模型训练的大规模数据集。缺乏标注数据是几乎所有应用场景普遍存在的挑战。当前的应用场景多以项目制形式落地，数据仍然在项目建设方，数据不能共享也无法形成闭环，也就导致技术的进步分散在各个企业的各个项目中，难以带来行业整体跨越。其次， 缺乏从技术到产品到规模化应用的工程化经验。计算机视觉技术的应用已不再是单一的软件应用，涉及到新型基础架构，涉及到新的数据分析流程，还涉及到智能硬件如摄像头的安装等等。每一个环节都可能会影响识别效果。将这一技术从实验室扩展到工业化应用的过程本身就是很大的挑战。自然语言处理中国领先的人工智能技术一个完整的自然语言处理系统包含 语音识别、语义识别、语音合成 三部分。 其中，国内企业在语音识别和语音合成已处世界领先地位。语音识别 是指让计算机“听到”人的语音，目前已经比较成熟，尤其汉语的语音识别领先英语。根据2017年IBM、谷歌和微软发布的词错率进展数据，他们的识别率均在94%-95%之间，而在此之前，国内语音识别企业，如百度、搜狗、科大讯飞，识别率均已达到97%左右。语音合成 是指计算机将准备“回复”给人类的语句，通过合成音频的形式，利用扬声器外放。百度地图的语音导航、苹果手机的Siri助手背后都利用了语音合成技术，这项技术已日臻成熟。当前，科大讯飞的语音合成技术代表了世界领先水平。2018年科大讯飞打败卡内基梅隆等众多高校、科研机构和企业，连续13年赢得Blizzard Challenge（国际语音合成大赛）冠军。区别于语音识别“ 听到” 人类语言，语义识别更加强调“ 听懂” 。 当用户对智能系统说出一个饭店的名字，系统对用户语音进行识别，搜索饭店，这是“语音识别”；当用户对智能系统说“自助餐”、“海鲜”、“连锁店”等模糊语句，智能系统根据用户的性别、爱好、饮食倾向等特征进行智能分析，并精准推荐，则是“语义分析”。可见，语义识别比语音识别技术难度高好几个层次。语义识别 是当前自然语言处理发展的瓶颈，仍处于初级研究阶段。由于目前的人工智能技术只能把音变成字，字变成音，不能理解其中含义，很难实现基于场景的生动会话，商业落地的场景十分有限。知识图谱人工智能的下一技术风口知识图谱 最初是由Google 公司在 2012 年提出来的一个新的概念。从学术的角度，我们可以对知识图谱给一个这样的定义：“知识图谱本质上是语义网络（ Semantic Network ）的知识库”。但这有点抽象，所以换个角度，从实际应用的角度出发其实可以简单地把知识图谱理解成多关系图（ Multi-relational Graph ）。如果说以往的智能分析专注在每一个个体，知识图谱则专注于这些个体之间的 “ 关系 ” 。知识图谱用 “ 图 ” 的表达形式，最有效、最直观地表达出实体间的关系，是最接近真实世界、符合人类思维模式的数据组织结构。相较于传统的智能分析， 知识图谱是基于图的数据结构，即知识图谱需要从海量信息中抽去多个维度的特征信息， 并在这些特征信息素材的基础上，通过智能推理实现从数据到可视化图像深加工，从而能够直观易懂的展现给用户，并与用户交互。目前， 知识图谱主要应用于面向 互联网的搜索、推荐、问答 等业务场景，成为以 商业搜索引擎公司 为首的互联网公司重兵布局的人工智能技术之一。 同时，也开始在金融、医疗、电商及公共安全保障等领域得到广泛的探索。当我们使用搜索软件时，搜索结果右侧的联想，就来自于知识图谱技术。在反洗钱或电信诈骗场景，知识图谱可精准追踪卡与卡间的交易路径，追本溯源识别洗钱/套现路径和可疑人员，并通过他们的交易轨迹，层层关联，分析得到更多可疑人员、账户、商户或卡号等实体。然而， 目前知识图谱尚处于发展初期，受制于抽取数据的样本量限制、深加工准确率和效率较低、数据噪声大等因素限制，应用场景非常有限 。相信未来，随着研究的深入，会有越来越多的应用场景被发掘出来，对于知识图谱所能发挥的价值可期。虽然人工智能在图像识别、语音识别、文本处理、游戏博弈等诸多方面全面赶超人类，取得了突破性进展，但整体来看其还是在婴幼儿时期， 远未达到人们所预期的智能水平，也远未到成熟的地步。相信未 来，随着时代的进步，技术瓶颈将不断被突破，人工智能的发展将更加多元化，更多的黑科技会使得我们的生活更智能。"}
{"content2":"转自：https://www.cnblogs.com/ajian005/archive/2012/11/04/2841171.html从cvchina搞到的机器视觉开源处理库汇总，转来了，很给力，还在不断更新。。。通用库/General LibraryOpenCV无需多言。RAVLRecognition And Vision Library. 线程安全。强大的IO机制。包含AAM。CImg很酷的一个图像处理包。整个库只有一个头文件。包含一个基于PDE的光流算法。图像，视频IO/Image, Video IOFreeImageDevILImageMagickFFMPEGVideoInputportVideoAR相关/Augmented RealityARToolKit基于Marker的AR库ARToolKitPlusARToolKit的增强版。实现了更好的姿态估计算法。PTAM实时的跟踪、SLAM、AR库。无需Marker，模板，内置传感器等。BazAR基于特征点检测和识别的AR库。局部不变特征/Local Invariant FeatureVLFeat目前最好的Sift开源实现。同时包含了KD-tree，KD-Forest，BoW实现。Ferns基于Naive Bayesian Bundle的特征点识别。高速，但占用内存高。SIFT By Rob Hess基于OpenCV的Sift实现。目标检测/Object DetectionAdaBoost By JianXin.Wu又一个AdaBoost实现。训练速度快。行人检测 By JianXin.Wu基于Centrist和Linear SVM的快速行人检测。（近似）最近邻/ANNFLANN目前最完整的（近似）最近邻开源库。不但实现了一系列查找算法，还包含了一种自动选取最快算法的机制。ANN另外一个近似最近邻库。SLAM & SFMSceneLib [LGPL]monoSLAM库。由Androw Davison开发。图像分割/SegmentationSLIC Super Pixel使用Simple Linear Iterative Clustering产生指定数目，近似均匀分布的Super Pixel。目标跟踪/TrackingTLD基于Online Random Forest的目标跟踪算法。KLTKanade-Lucas-TrackerOnline boosting trackersOnline Boosting Trackers直线检测/Line DetectionDSCC基于联通域连接的直线检测算法。LSD [GPL]基于梯度的，局部直线段检测算子。指纹/Finger PrintpHash [GPL]基于感知的多媒体文件Hash算法。（提取，对比图像、视频、音频的指纹）视觉显著性/Visual SalienceGlobal Contrast Based Salient Region DetectionMing-Ming Cheng的视觉显著性算法。FFT/DWTFFTW [GPL]最快，最好的开源FFT。FFTReal [WTFPL]轻量级的FFT实现。许可证是亮点。音频处理/Audio processingSTK [Free]音频处理，音频合成。libsndfile [LGPL]音频文件IO。libsamplerate [GPL ]音频重采样。小波变换快速小波变换（FWT）FWTBRIEF: Binary Robust Independent Elementary Feature 一个很好的局部特征描述子，里面有FAST corner + BRIEF实现特征点匹配的DEMO：http://cvlab.epfl.ch/software/brief/http://code.google.com/p/javacvJava打包的OpenCV, FFmpeg, libdc1394, PGR FlyCapture, OpenKinect, videoInput, and ARToolKitPlus库。可以放在Android上用~libHIK,HIK SVM，计算HIK SVM跟Centrist的Lib。http://c2inet.sce.ntu.edu.sg/Jianxin/projects/libHIK/libHIK.htm一组视觉显著性检测代码的链接：http://cg.cs.tsinghua.edu.cn/people/~cmm/saliency/介绍n款计算机视觉库/人脸识别开源库/软件计算机视觉库 OpenCVOpenCV是Intel®开源计算机视觉库。它由一系列 C 函数和少量 C++ 类构成，实现了图像处理和计算机视觉方面的很多通用算法。 OpenCV 拥有包括 300 多个C函数的跨平台的中、高层 API。它不依赖于其它的外部库——尽管也可以使用某些外部库。 OpenCV 对非商业...人脸识别 faceservice.cgifaceservice.cgi 是一个用来进行人脸识别的 CGI 程序， 你可以通过上传图像，然后该程序即告诉你人脸的大概坐标位置。faceservice是采用 OpenCV 库进行开发的。OpenCV的.NET版 OpenCVDotNetOpenCVDotNet 是一个 .NET 对 OpenCV 包的封装。人脸检测算法 jViolajonesjViolajones是人脸检测算法Viola-Jones的一个Java实现，并能够加载OpenCV XML文件。 示例代码：http://www.oschina.net/code/snippet_12_2033Java视觉处理库 JavaCVJavaCV 提供了在计算机视觉领域的封装库，包括：OpenCV、ARToolKitPlus、libdc1394 2.x 、PGR FlyCapture和FFmpeg。此外，该工具可以很容易地使用Java平台的功能。 JavaCV还带有硬件加速的全屏幕图像显示（CanvasFrame），易于在多个内核中执行并行代码（并...运动检测程序 QMotionQMotion 是一个采用 OpenCV 开发的运动检测程序，基于 QT。视频监控系统 OpenVSSOpenVSS - 开放平台的视频监控系统 - 是一个系统级别的视频监控软件视频分析框架（VAF）的视频分析与检索和播放服务，记录和索引技术。它被设计成插件式的支持多摄像头平台，多分析仪模块（OpenCV的集成），以及多核心架构。手势识别 hand-gesture-detection手势识别，用OpenCV实现人脸检测识别 mcvai-tracking提供人脸检测、识别与检测特定人脸的功能，示例代码 cvReleaseImage( &gray ); cvReleaseMemStorage(&storage); cvReleaseHaarClassifierCascade(&cascade);...人脸检测与跟踪库 asmlibraryActive Shape Model Library (ASMLibrary©) SDK, 用OpenCV开发，用于人脸检测与跟踪。Lua视觉开发库 libecvECV 是 lua 的计算机视觉开发库(目前只提供linux支持)OpenCV的.Net封装 OpenCVSharpOpenCVSharp 是一个OpenCV的.Net wrapper，应用最新的OpenCV库开发，使用习惯比EmguCV更接近原始的OpenCV，有详细的使用样例供参考。3D视觉库 fvision2010基于OpenCV构建的图像处理和3D视觉库。 示例代码： ImageSequenceReaderFactory factory; ImageSequenceReader* reader = factory.pathRegex(\"c:/a/im_%03d.jpg\", 0, 20); //ImageSequenceReader* reader = factory.avi(\"a.avi\"); if (reader == NULL) { ...基于QT的计算机视觉库 QVision基于 QT 的面向对象的多平台计算机视觉库。可以方便的创建图形化应用程序，算法库主要从 OpenCV，GSL，CGAL，IPP，Octave 等高性能库借鉴而来。图像特征提取 cvBlobcvBlob 是计算机视觉应用中在二值图像里寻找连通域的库.能够执行连通域分析与特征提取.实时图像/视频处理滤波开发包 GShowGShow is a real-time image/video processing filter development kit. It successfully integrates DirectX11 with DirectShow framework. So it has the following features: GShow 是实时 图像/视频 处理滤波开发包，集成DiretX11。...视频捕获 API VideoManVideoMan 提供一组视频捕获 API 。支持多种视频流同时输入（视频传输线、USB摄像头和视频文件等）。能利用 OpenGL 对输入进行处理，方便的与 OpenCV，CUDA 等集成开发计算机视觉系统。开放模式识别项目 OpenPRPattern Recognition project（开放模式识别项目），致力于开发出一套包含图像处理、计算机视觉、自然语言处理、模式识别、机器学习和相关领域算法的函数库。OpenCV的Python封装 pyopencvOpenCV的Python封装，主要特性包括： 提供与OpenCV 2.x中最新的C++接口极为相似的Python接口，并且包括C++中不包括的C接口 提供对OpenCV 2.x中所有主要部件的绑定：CxCORE (almost complete), CxFLANN (complete), Cv (complete), CvAux (C++ part almost...视觉快速开发平台 qcv计算机视觉快速开发平台，提供测试框架，使开发者可以专注于算法研究。图像捕获 libv4l2cam对函数库v412的封装，从网络摄像头等硬件获得图像数据，支持YUYV裸数据输出和BGR24的OpenCV  IplImage输出计算机视觉算法 OpenVIDIAOpenVIDIA projects implement computer vision algorithms running on on graphics hardware such as single or multiple graphics processing units(GPUs) using OpenGL, Cg and CUDA-C. Some samples will soon support OpenCL and Direct Compute API&apos;...高斯模型点集配准算法 gmmreg实现了基于混合高斯模型的点集配准算法，该算法描述在论文： A Robust Algorithm for Point Set Registration Using Mixture of Gaussians, Bing Jian and Baba C. Vemuri. ，实现了C++/Matlab/Python接口...模式识别和视觉库 RAVLRecognition And Vision Library (RAVL) 是一个通用 C++ 库，包含计算机视觉、模式识别等模块。图像处理和计算机视觉常用算法库 LTI-LibLTI-Lib 是一个包含图像处理和计算机视觉常用算法和数据结构的面向对象库，提供 Windows 下的 VC 版本和 Linux 下的 gcc 版本，主要包含以下几方面内容： 1、线性代数 2、聚类分析 3、图像处理 4、可视化和绘图工具OpenCV优化 opencv-dsp-acceleration优化了OpenCV库在DSP上的速度。C++计算机视觉库 Integrating Vision ToolkitIntegrating Vision Toolkit (IVT) 是一个强大而迅速的C++计算机视觉库，拥有易用的接口和面向对象的架构，并且含有自己的一套跨平台GUI组件，另外可以选择集成OpenCV计算机视觉和机器人技术的工具包 EGTThe Epipolar Geometry Toolbox (EGT) is a toolbox designed for Matlab (by Mathworks Inc.). EGT provides a wide set of functions to approach computer vision and robotics problems with single and multiple views, and with different vision se...OpenCV的扩展库 ImageNetsImageNets 是对OpenCV 的扩展，提供对机器人视觉算法方面友好的支持，使用Nokia的QT编写界面。libvideogfx视频处理、计算机视觉和计算机图形学的快速开发库。Matlab计算机视觉包 mVisionMatlab 的计算机视觉包，包含用于观察结果的 GUI 组件，貌似也停止开发了，拿来做学习用挺不错的。Scilab的计算机视觉库 SIPSIP 是 Scilab（一种免费的类Matlab编程环境）的图像处理和计算机视觉库。SIP 可以读写 JPEG/PNG/BMP 格式的图片。具备图像滤波、分割、边缘检测、形态学处理和形状分析等功能。STAIR Vision LibrarySTAIR Vision Library (SVL) 最初是为支持斯坦福智能机器人设计的，提供对计算机视觉、机器学习和概率统计模几种图像处理类库的比较作者：王先荣原文；http://www.cnblogs.com/xrwang/archive/2010/01/26/TheComparisonOfImageProcessingLibraries.html前言近期需要做一些图像处理方面的学习和研究，首要任务就是选择一套合适的图像处理类库。目前较知名且功能完善的图像处理类库有OpenCv、EmguCv、AForge.net等等。本文将从许可协议、下载、安装、文档资料、易用性、性能等方面对这些类库进行比较，然后给出选择建议，当然也包括我自己的选择。许可协议类库许可协议许可协议网址大致介绍OpenCvBSDwww.opensource.org/licenses/bsd-license.html在保留原来BSD协议声明的前提下，随便怎么用都行EmguCvGPL v3http://www.gnu.org/licenses/gpl-3.0.txt你的产品必须也使用GPL协议，开源且免费商业授权http://www.emgu.com/wiki/files/CommercialLicense.txt给钱之后可以用于闭源的商业产品AForge.netLGPL v3http://www.gnu.org/licenses/lgpl.html如果不修改类库源代码，引用该类库的产品可以闭源和（或）收费以上三种类库都可以用于开发商业产品，但是EmguCv需要付费；因为我只是用来学习和研究，所以这些许可协议对我无所谓。不过鉴于我们身在中国，如果脸皮厚点，去他丫的许可协议。下载可以很方便的下载到这些类库，下载地址分别为：类库下载地址OpenCvhttp://sourceforge.net/projects/opencvlibrary/files/EmguCvhttp://www.emgu.com/wiki/index.php/Download_And_InstallationAForge.nethttp://www.aforgenet.com/framework/downloads.html安装这些类库的安装都比较简单，直接运行安装程序，并点“下一步”即可完成。但是OpenCv在安装完之后还需要一些额外的处理才能在VS2008里面使用，在http://www.opencv.org.cn有一篇名为《VC2008 Express下安装OpenCv 2.0》的文章专门介绍了如何安装OpenCv。类库安装难易度备注OpenCv比较容易VC下使用需要重新编译EmguCv容易AForge.net容易相信看这篇文章的人都不会被安装困扰。文档资料类库总体评价书籍网站文档示例社区备注OpenCv中等中英文中英文中英文较多中文论坛有中文资料但不完整EmguCv少无英文英文少英文论坛论坛人气很差AForge.net少无英文英文少英文论坛论坛人气很差OpenCv有一些中文资料，另外两种的资料全是英文的；不过EmguCv建立在OpenCv的基础上，大部分OpenCv的资料可以用于EmguCv；而AForge.net是原生的.net类库，对GDI+有很多扩展，一些MSDN的资料可以借鉴。如果在查词典的基础上还看不懂英文文档，基本上可以放弃使用这些类库了。易用性易用性这玩意，主观意志和个人能力对它影响很大，下面是我的看法：类库易用性备注OpenCv比较差OpenCv大多数功能都以C风格函数形式提供，少部分功能以C++类提供。注意：2.0版将更多的功能封装成类了。EmguCv比较好将OpenCv的绝大部分功能都包装成了.net类、结构或者枚举。不过文档不全，还是得对照OpenCv的文档去看才行。AForge.net好纯.net类库，用起来很方便。最近几年一直用的是C# ，把C和C++忘记得差不多了，况且本来C/C++我就不太熟，所以对OpenCv的看法恐怕有偏见。视觉相关网站这段时间因为项目的需要，我一直在折腾计算机视觉，尤其是双目立体视觉，代码、论文、工具箱等……占用了我几乎90%的工作时间，还在一点点地摸索，但进度实在不敢恭维，稍后我会把情况作个总结。今天的主要任务就是和大家分享一些鄙人收藏的认为相当研究价值的网页：Oxford大牛：Andrew Zisserman，http://www.robots.ox.ac.uk/~vgg/hzbook/code/，此人主要研究多幅图像的几何学，该网站提供了部分工具，相当实用，还有例子西澳大利亚大学的Peter Kovesi：http://www.csse.uwa.edu.au/~pk/research/matlabfns/，提供了一些基本的matlab工具，主要内容涉及Computer Vision, Image ProcessingCMU：http://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.html,该网站是我的最爱，尤其后面这个地址http://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/v-groups.html，在这里提供了世界各地机构、大学在Computer Vision所涉及各领域的研究情况，包括Image Processing, Machine Vision，我后来也是通过它连接到了很多国外的网站Cambridge：http://mi.eng.cam.ac.uk/milab.html，这是剑桥大学的机器智能实验室，里面有三个小组，Computer Vision & Robotics, Machine Intelligence, Speech，目前为止，Computer Vision & Robotics的一些研究成果对我日后的帮助可能会比较大，所以在此提及大量计算机视觉方面的原版电子书：http://homepages.inf.ed.ac.uk/rbf/CVonline/books.htm，我今天先下了本Zisserman的书，呵呵，国外的原版书，虽然都是比较老的，但是对于基础的理解学习还是很有帮助的，至于目前的研究现状只能通过论文或者一些研究小组的网站stanford：http://ai.stanford.edu/~asaxena/reconstruction3d/，这个网站是Andrew N.G老师和一个印度阿三的博士一起维护的，主要对于单张照片的三维重建，尤其他有个网页make3d.stanford.edu可以让你自己上传你的照片，通过网站来重建三维模型，这个网站对于刚开始接触Computer Vision的我来说，如获至宝，但有个致命问题就是make3d已经无法注册，我也多次给Andrew和印度阿三email，至今未回，郁闷，要是有这个网站的帐号，那还是相当爽的，不知道是不是由于他们的邮箱把我的email当成垃圾邮件过滤，哎，但这个stanford网站的贡献主要是代码，有很多computer vision的基础工具，貌似40M左右，全都是基于matlab的caltech：http://www.vision.caltech.edu/bouguetj/calib_doc/，这是我们Computer Vision老师课件上的连接，主要是用于摄像机标定的工具集，当然也有涉及对标定图像三维重建的前期处理过程JP Tarel：http://perso.lcpc.fr/tarel.jean-philippe/，这是他的个人主页，也是目前为止我发的email中，唯一一个给我回信的老外，因为我需要重建练习的正是他的图片集，我读过他的论文，但没有涉及代码的内容，再加上又是94年以前的论文，很多相关的引文，我都无法下载，在我的再三追问下，Tarel教授只告诉我，你可以按照我的那篇论文对足球进行重建，可是...你知道吗，你有很多图像处理的引文都下不了了，我只知道你通过那篇文章做了图像的预处理，根本不知道具体过程，当然我有幸找到过一篇90左右的论文，讲的是region-based segmentation，可是这文章里所有引文又是找不到的....悲剧的人生开源软件网站：www.sourceforge.net最后就是我们工大的Computer Vision大牛：sychen.com，我们Computer Vision课的老师，谦虚、低调，很有学者风范总结：目前为止，我的个人感觉就是国外学者的论文包括刊登的资料大部分都是对原理进行的说明，并不是很在意具体的代码实现的讲解，而我却过分的关注于代码的实现，忽视Computer Vision的原理，国外学者对与自己相关领域的研究现状了解相当充分，对自己的工作进度更新也很勤快，很多好的网站我并没有完全列出来，在这里只是提了主要的几个，在这方面，我们国内的研究氛围有所不及，当然我选择的一些网站可能更多的是个人小组的研究介绍，不像一些专门从事领域研究的机构，会有那么多的权威资料，国外的网站有个很好的地方，就是有很多的免费资源，免费的matlab或者openCV工具集，免费的论文下载，课件下载等等，在这方面国内对于研究资源的共享，做得又有所差距，同样，国外的研究工具很多样，主要是matlab，一些发布的demo都使用C++写的，不过今天看到一个西班牙的研究机构（university of las palmas）用了个XMW的软件平台来实现图片的三维重建，data用的是人脸，而且国外的很多源代码基本上是在linux平台下完成的，对于我来说又是不方便，哎，可能要考虑装VM Ware了，不然双系统太累.....目前，Computer Vision是全世界范围内自动化、计算机、数学领域的研究热点，综合性高，应用于医疗、军事、民用等等领域，其中有突出成绩的还是一下几所学校（个人见解）：Cambridge(UK), Oxford(UK), CMU(US),Stanford(US),MIT(US),U.C.Berkeley(US)，而UK的两所老牌高校，他们的实际应用领域丝毫不逊于stanford和CMU....世界就是这样，当你不断的接触，不断的扩展你所能够及的边际就会发现自己越来越无知，还有很多很多不知道，发现还有很多自己都想不到但却已经实现的东西.....革命远未成功，同志仍须努力，在CV的道路上前进.......该文转自http://www.cnblogs.com/yangwei86/archive/2009/07/10/1520215.html"}
{"content2":"机器学习导论什么是机器学习机器学习的种类代价函数，优化目标模型泛化能力模型评估什么是机器学习机器学习是人工智能的一个分支。人工智能的研究是从以“推理”为重点到以“知识”为重点，再到以“学习”为重点，一条自然、清晰的脉络。显然，机器学习是实现人工智能的一个途径，即以机器学习为手段解决人工智能中的问题。机器学习在近30多年已发展为一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、计算复杂性理论等多门学科。机器学习理论主要是设计和分析一些让计算机可以自动“学习”的算法。机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。因为学习算法中涉及了大量的统计学理论，机器学习与推断统计学联系尤为密切，也被称为统计学习理论。算法设计方面，机器学习理论关注可以实现的，行之有效的学习算法。很多推论问题属于无程序可循难度，所以部分的机器学习研究是开发容易处理的近似算法。机器学习已广泛应用于数据挖掘、计算机视觉、自然语言处理、生物特征识别、搜索引擎、医学诊断、检测信用卡欺诈、证券市场分析、DNA序列测序、语音和手写识别、战略游戏和机器人等领域。机器学习的种类监督学习从给定的训练数据集中学习出一个函数，当新的数据到来时，可以根据这个函数预测结果。监督学习的训练集要求是包括输入和输出，也可以说是特征和目标。训练集中的目标是由人标注的。常见的监督学习算法包括回归分析和统计分类。无监督学习与监督学习相比，训练集没有人为标注的结果。常见的无监督学习算法有聚类。半监督学习介于监督学习与无监督学习之间。增强学习通过观察来学习做成如何的动作。每个动作都会对环境有所影响，学习对象根据观察到的周围环境的反馈来做出判断。代价函数模型泛化能力模型评估Precision (精确度)：检索出来的条目(比如:文档、网页等)有多少是准确的Recall (召回率、查全率)：所有准确的条目有多少被检索出来下面这张图介绍True Positive，False Negative等常见的概念，P和R也往往和它们联系起来。相关(Relevant),正类无关(NonRelevant),负类被检索到(Retrieved)true positives(TP 正类判定为正类 , 分明是A)false positives(FP 负类判定为正类,\"存伪\", 分明是B却判断为A)未被检索到(Not Retrieved)false negatives(FN 正类判定为负类,\"去真\" ,分明是A却判断为B)true negatives(TN 负类判定为负类 , 分明是B)其中false positives（存伪）也通常称作误报，false negatives也通常称作漏报！F-Measure是Precision和Recall加权调和平均：当参数a=1时，就是最常见的F1了：很容易理解，F1综合了P和R的结果，当F1较高时则比较说明实验方法比较理想"}
{"content2":"科幻中关于A.I.的纠结A.I. =artificial intelligence人工智能那位发现了相对论的科学家曾说:想象力是比知识更重要的东西，知识是有限的，而想象力概括着世界的一切，推动着进步，并且是知识进化的源泉。严格地说，想象力并不是虚空的，它是科学研究中的实在因素。在电影艺术中，想象力与科技可以集合，无疆界的思维用现实科学手段作出表达，产生的科幻电影成为一面折射科技与社会文化的镜子。“在科学技术的力量到达之前，我们已经到达了那些世界。”这就是科幻的真实含义。它符合现在绝不可能，兼未来一定要有可能这两个基本条件。若是现在已有可能，则见不到想象力的成分；而假如未来亦无可能，那就代表该构思已抵触了既有的科学。平民版人工智能人工智能（AI）技术是科幻类作品中一个备受青睐的题材。可以发现，科幻电影里机器人的出境率几乎是最高的，主角、龙套、最大幕后黑手，其都能胜任。如果说要把人工智能的发展历程归纳出一部正史，那描述机器人的科幻电影就可以撰成一篇小传。1956年，一群科学家聚集达特茅斯学院，讨论着对于当时的世人而言完全陌生的话题。这就是被称为人工智能起点的“达特茅斯会议”。从那以后，研究者们发展了众多理论和原理，人工智能这一极富挑战性的概念也随之在文学和电影中扩展，与它相关的内容，涉及了计算机知识、心理学和哲学。2008年的屏幕上，最可爱的就是机器人WALL-E和他的女朋友，功能上来看WALL-E有点像升级版的智能型吸尘器，他的女友只能归于反恐武器一类，但这不是关键的，他们和电影《星球大战》中的C-3PO、R2-D2一样，拥有忠诚、友谊、爱慕等感情，和诸如神经质、面冷心善、“宅”等小性格，以及琐碎的生活细节，和并非由程序设定产生的理念。美式电影提倡的平民英雄，让机器人世界里的高大全形象也不再受欢迎。   就和人类的相似度来讲，电影《人工智能》中机器人动作外形达到了已臻化境的程度。在日本，这个全世界机器人工业最为发达的国家之一，目前较先进水平是今年3月份诞生的黑发美女HRP-4C，其面部的8个发动机赋予“她”愤怒、惊讶等表情，体内的30个发动机则能让“她”姿态诡异的走上几步台步。此前，汽车制造商本田公司已经研制出能够行走和讲话的机器人“阿西莫”，但这款机器人并不是人类的样子。至于情感个性层面，《人工智能》中机器人的灵魂与人类几无差别，影片也就此将更多的注意力都放在伦理探讨上。影片《WALL-E》诞生的同一年，欧盟启动了一个研究项目，包括英国伦敦大学在内的欧洲10所大学的专家计划合作开发出世界上第一批有性格的机器人，旨在实现机器人与人类的长期交流与互动，就像人与人之间一样。但即便是更深层次地发掘人机交互，短期内技术只能停留于让机器人拥有一种类似人的性格上，通过设置某种特殊的电脑程序使机器人能学会主人的情感、喜好等，增进人类对机器人的信赖度。那么科幻电影里的人工智能何时来到我们身边呢？它还是他？起初人工智能被界定为“让机器的行为看起来就像是人所表现出的智能行为一样”。但在今天，如要考虑人工智能的实现程度，需要更为精准地用强人工智能和弱人工智能来划分。强人工智能观点认为计算机不仅是用来研究人的思维的一种工具，相反，只要运行适当的程序，计算机本身就是有思维的，这样的机器将被认为是有知觉的，有自我意识的，这更像我们在科幻电影中见到的那些家伙们。弱人工智能也并不是“弱智”，其与强人工智能不对立，只是认为不可能制造出能真正进行推理和解决问题的智能机器，这些机器看起来只不过像是智能的但并不真正拥有自主意识的工具而已。然而，主流的研究正是围绕“工具”而进行的，且成就可观；强人工智能则处于停滞不前的状态，首先关于概念的争论就一直未平息：如果一台机器，唯一工作原理就是对编码数据进行转换，那么这台机器能算是有思维的智能吗？可是，人类又怎样去确定其他个体是否像我们一样是智能的呢？“它”或“他”可能只是表现得像拥有智能或不像，做出判断完全靠人类主观的定论，既然弱人工智能可以令机器看起来像是智能的，那么能完全否定这台机器有智能吗？拿HAL9000来说，它曾被评为最恶劣电影机器人之终极第一名。在影片《2001太空漫游》中，HAL9000是人类科技文明发展的顶端，号称零缺陷（观众意外发现H、A、L各自后一位字母是I、B、M）。HAL9000能够表现出情感，这是因为人类的设计，但是到底它是否具有情感，没有人知道。为了完成任务抑或有预谋地铺设陷阱，HAL9000让数位船员失去了生命，机器做出的表象仍旧是人类控制着所谓的工具，但在深层次的地方，工具已经成为未知。问题已经向哲学方向发展了。而基于人工智能研制的目的，如果希望产生一台能够自行思考的机器人，那就必须了解什么是思考，以及什么是智慧。现有的技术可以令机器模仿人类身体相当多器官的功能，但模仿人类大脑却难如登天，到目前为止，人类仅知道它是由数十亿个神经细胞组成的器官，对于它产生智慧的模式还知之甚少。人工智能，“它”似乎还不是“他”。我们，机器人在德国1926年推出的《大都会》中，机器人在其中充当了劳资矛盾的催生物，虽然故事略嫌肤浅，但其以视觉风格而不是以内涵深度取胜，仍堪称世界科幻影坛的先驱。本片亦诞生了第一个人类与机器人共同生活的构想，而根据“科幻”的含义，既然不违背既有科学理论，就不能阻挡在其后电影导演的脑内人类已面临人机大战的威胁。从机器人的发展历程可以看到，随着整合控制论、机械电子、信息技术、材料学和仿生学的发展，机器人的性能得到不断提高，其应用也越来越广泛。从理论上讲，按照这样发展下去，机器人最终会广泛替代人的劳动。但这一天的到来必须在人类控制范围内，否则仅是战斗型机器人就将是人类的噩梦，更勿论可自行思考的机器人。探讨人类生存危机的影片《我，机器人》改编自科幻小说家阿西莫夫的作品，根据机器人三大定律的逻辑推演而来。早在1940年，阿西莫夫提出了著名的“机器人三定律”，规定所有机器人程序上必须遵守:机器人三定律虽然只是科幻小说里的创造，但后来已成为世所默认的机器人研发原则。然而很多人指出，“三定律”更类似是一种技术上的保证，而非逻辑上的，从人工智能的设想可以知道，机器人具有修改自身程序或者说自我学习的可能性；而携带武器可直接攻击人类的军用机器人的出现，也是对“三定律”的一种颠覆。正如《我，机器人》所表达的含义：机器人并没有问题，技术也不是问题，人类逻辑的局限才是最大的问题。据英国《新科学家》杂志报道，2008年谢菲尔德大学科学家诺埃尔·夏基拟定了一份“机器人大战”的前景和带来忧虑的文稿。其内容披露美国在伊拉克已经部署了4000多个半自动机器人。美国国防部在曾经发表的《无人系统路线图》中建议，到2010年前耗资40亿美元研制机器人武器。此外，一些欧洲国家以及加拿大、韩国、南非、新加坡和以色列等国也在开发军用机器人技术。在文稿最后，科学家不无忧虑地表示，机器人今后很可能将自己决定何时“扣动扳机”，而“机器人对于为什么要这样做没有判断能力”。在对这一点的认识上，电影人和人工智能专家一样清醒。已故台湾电影导演杨德昌生前曾说过，当数字科技在做人原本做的事情时，其之后的产业扩展和萧条的方式都不是线性的，也因为数字工具的效能，很多方法不能再用传统方法去看。当人类的功能被取代的时候，其中产生的能量，亦不是我们人类可以想象的。邢利栋"}
{"content2":"今日CS.CV 计算机视觉论文速览Mon, 1 Apr 2019 ,📖 No.91期Totally 35 papers👉上期速览 ✈更多精彩请移步主页Interesting:📚CoSegNet,一套用于点云联合分割的网络架构，通过矩阵秩估计加入了连续性损失，优化了预处理的分割外形，得到了高效的分割结果。 (from Simon Fraser University)流程图:精炼网络和特征抽取网络：不同外形中得到的形状相似矩阵：📚对于任意模糊核的可插入超分辨模块研究, 拓展了基于双三次退化过程的超分辨，并建立起了处理任意模糊核的理论框架。设计的新的SISR退化过程模型充分利用盲去噪方法的优势来估计模糊核，并利用即插即用算法来分离变量，优化能量函数。便于插入任何形式的求解器先验以替代去噪先验。(from HIT)三种不同形式的模糊核:最后的效果：code:https://github.com/cszn/DPSR📚CroP,色彩恒常基准数据集生成器，（from University of Zagreb）datasset Cube+: https://ipg.fer.hr/ipg/resources/color_constancy?📚IMAGENET-P,常见扰动数据集，包括了15种常见扰动（from University of California, Berkeley）dataset:https://github.com/hendrycks/robustness📚点或场中4D特征抽取方法,欧几里得表示的场和拉格朗日形式的点/轨迹,(from University of California, Davis)流程图和具体方法：Daily Computer Vision PapersIncremental Learning with Unlabeled Data in the WildAuthors Kibok Lee, Kimin Lee, Jinwoo Shin, Honglak Lee已知深度神经网络在类增量学习中遭受灾难性遗忘，其中在学习新任务时先前任务的性能急剧下降。为了减轻这种影响，我们建议在野外利用连续和大量的未标记数据。特别是，为了有效地利用这些瞬态外部数据，我们设计了一种新的类增量学习方案，其具有新的蒸馏损失，称为全局蒸馏，ba学习策略以避免过度拟合到最近的任务，以及ca采样策略用于所需的外部数据。我们对各种数据集（包括CIFAR和ImageNet）的实验结果证明了所提方法优于现有方法的优越性，特别是当可以访问未标记数据流时，与现有技术方法相比，我们实现了高达9.3的相对性能提升。CroP: Color Constancy Benchmark Dataset GeneratorAuthors Nikola Bani , Karlo Ko evi , Marko Suba i , Sven Lon ari在当代数码相机中实现颜色恒定性作为预处理步骤是非常重要的，因为它消除了场景照明对物体颜色的影响。为了开发和测试新的颜色恒常性方法，已经创建了几个基准颜色恒定数据集。然而，它们都有许多缺点，包括少量图像，错误提取的地面实况照明，长期误用历史，违反其规定的假设等。为了克服这些和类似的问题，本文中的颜色恒定基准数据集生成器是建议。对于给定的相机传感器，它能够生成在现实世界的子集中拍摄的任何数量的真实原始图像，即打印照片的图像。具有此类图像的数据集与其他现有的真实世界数据集共享许多正面特征，而一些消极特征被完全消除。生成的图像可以成功地用于训练后来在现实世界数据集上实现高精度的方法。这为为高级深度学习技术创建足够大的数据集开辟了道路。提出并讨论了实验结果。源代码可在以下位置获得CNN-based Prostate Zonal Segmentation on T2-weighted MR Images: A Cross-dataset StudyAuthors Leonardo Rundo, Changhee Han, Jin Zhang, Ryuichiro Hataya, Yudai Nagano, Carmelo Militello, Claudio Ferretti, Marco S. Nobile, Andrea Tangherloni, Maria Carla Gilardi, Salvatore Vitabile, Hideki Nakayama, Giancarlo Mauri前列腺癌是美国男性中最常见的癌症。然而，尽管多参数磁共振成像MRI的进步提供了与病理区域相关的形态学和功能信息，但前列腺成像仍然具有挑战性。随着整个前列腺腺体分割，区分中央腺体CG和外周区域PZ可以指导鉴别诊断，因为肿瘤的频率和严重程度在这些区域中不同，但是它们的边界通常是弱的和模糊的。这项工作提出了深度学习的初步研究，以自动描绘CG和PZ，旨在评估卷积神经网络CNNs在两个多中心MRI前列腺数据集上的泛化能力。特别是，我们比较了三种基于CNN的体系结构SegNet，U Net和pix2pix。在这样的背景下，在没有预训练的情况下实现的分割性能在4倍交叉验证中进行比较。通常，U Net优于其他方法，尤其是在对多个数据集执行训练和测试时。Infinite Brain MR Images: PGGAN-based Data Augmentation for Tumor DetectionAuthors Changhee Han, Leonardo Rundo, Ryosuke Araki, Yujiro Furukawa, Giancarlo Mauri, Hideki Nakayama, Hideaki Hayashi由于缺乏可用的注释医学图像，精确的计算机辅助诊断需要密集的数据增强DA技术，例如原始图像的几何强度变换，然而，这些变换图像本质上具有与原始图像类似的分布，导致有限的性能改进。为了填补真实图像分布中缺乏的数据，我们合成了脑对比增强磁共振MR图像，这些图像与使用生成对抗网络GAN的原始图像完全不同。本研究利用GANs PGGANs的渐进式生长，这是一种多阶段生成训练方法，用于生成基于卷积神经网络的脑肿瘤检测的原始大小的256×256 MR图像，这是由于传统的GAN难以通过高分辨率的GAN训练不稳定而产生的。和各种肿瘤的大小，位置，形状和对比度。我们的初步结果表明，这种新颖的基于PGGAN的DA方法与经典DA相结合，在肿瘤检测和其他医学成像任务中可以实现有希望的性能改善。Second Rethinking of Network Pruning in the Adversarial SettingAuthors Shaokai Ye, Kaidi Xu, Sijia Liu, Hao Cheng, Jan Henrik Lambrechts, Huan Zhang, Aojun Zhou, Kaisheng Ma, Yanzhi Wang, Xue Lin众所周知，深度神经网络DNN容易受到对抗性攻击，这是通过在良性示例上添加精心设计的扰动来实现的。基于Min max强健优化的对抗性训练可以提供抵御对抗性攻击的安全性概念。然而，对抗性强健性要求网络的容量明显大于自然训练的能力，只有良性的例子。本文提出了一个并行对抗训练和权重修剪的框架，它可以实现模型压缩，同时仍然保持对抗性的稳健性，并基本上解决了对抗性训练的两难问题。此外，这项工作研究了传统网络修剪设置中关于重量修剪的两个假设，并发现权重修剪对于减少对抗设置中的网络模型大小是必不可少的，即，即使从大模型继承初始化，也从头开始训练小模型无法实现对抗稳健性和模型压缩。Photo-realistic Monocular Gaze Redirection using Generative Adversarial NetworksAuthors Zhe He, Adrian Spurr, Xucong Zhang, Otmar Hilliges凝视重定向是将凝视改变为给定单眼眼贴图像的期望方向的任务。诸如视频会议，电影和游戏之类的许多应用以及用于凝视估计的训练数据的生成需要重定向凝视，而不会扭曲眼睛周围区域的外观并且同时产生照片逼真图像。现有方法缺乏产生感知似乎合理的图像的能力。在这项工作中，我们提出了一种新方法，通过利用生成性对抗训练来合成以目标凝视方向为条件的眼睛图像来缓解这一问题。我们的方法确保了合成图像与真实图像的感知相似性和一致性。此外，凝视估计损失用于准确地控制凝视方向。为了获得高质量的图像，我们将感知和周期一致性损失纳入我们的架构中。在广泛的评估中，我们表明所提出的方法在图像质量和重定向精度方面都优于现有技术方法。最后，我们展示如果用于增加真实训练数据，生成的图像可以为凝视估计任务带来显着改善。Deep Plug-and-Play Super-Resolution for Arbitrary Blur KernelsAuthors Kai Zhang, Wangmeng Zuo, Lei Zhang虽然深度神经网络基于DNN的单图像超分辨率SISR方法正在迅速普及，但它们主要是为广泛使用的双三次降解而设计的，并且仍然存在对于它们用任意模糊核超分辨率低分辨率LR图像的基本挑战。同时，由于其模块化结构易于插入降噪器先验，因此即插即用图像恢复具有高度灵活性。在本文中，我们通过在即插即用框架的帮助下扩展基于深度SISR的双三次降级来处理具有任意模糊内核的LR图像，从而提出了一种原则性的公式和框架。具体来说，我们设计了一个新的SISR退化模型，以便利用现有的盲目去模糊方法进行模糊核估计。为了优化新的降解诱导能量函数，我们通过可变分裂技术推导出即插即用算法，这允许我们先将任何超级旋转变压器插入而不是作为模块化部件之前的降噪器。对合成和真实LR图像的定量和定性评估表明，所提出的深度即插即用超分辨率框架对于处理模糊的LR图像是灵活且有效的。Multimodal Emotion ClassificationAuthors Anurag Illendula, Amit Sheth大多数NLP和计算机视觉任务仅限于标记数据的稀缺性。在社交媒体情感分类和其他相关任务中，主题标签已被用作标记数据的指示符。随着社交媒体表情符号使用的快速增长，表情符号被用作主要社交NLP任务的附加功能。然而，如果社交媒体上的多媒体帖子中的帖子由图像和文本组成，则对此进行的研究较少。与此同时，我们已经看到了将领域知识纳入其中以提高机器对文本理解的兴趣。在本文中，我们研究表情符号的领域知识是否可以提高情绪分类任务的准确性。我们利用最先进的深度学习架构，利用社交媒体帖子中不同形式对情感分类任务的重要性。我们的实验表明，三种形式的文本，表情符号和图像编码不同的信息来表达情感，因此可以相互补充。我们的结果还表明，表情符号意义取决于文本背景，表情符号与文本结合编码比单独考虑更好的信息。通过550,000个岗位的培训数据实现了71.98的最高准确度。Asymmetric Deep Semantic Quantization for Image RetrievalAuthors Zhan Yang, Osolo Ian Raymond, WuQing Sun, Jun Long由于其快速检索和存储效率能力，散列已被广泛用于最近邻检索任务。通过使用基于深度学习的技术，散列在许多应用程序中可以胜过基于非学习的散列。然而，对先前基于学习的散列方法存在一些限制，例如，由于散列方法不能发现丰富的语义信息而学习的散列码不具有辨别力，并且训练策略难以优化离散二进制码。在本文中，我们提出了一种新的基于学习的散列方法，命名为textbf下划线A对称textbf下划线D eep textbf下划线S emantic textbf下划线Q uantization textbf ADSQ。 textbf ADSQ使用三个流框架实现，其中包含一个emph LabelNet和两个emph ImgNets。 emph LabelNet利用三个完全连接的层，用于捕获图像对之间丰富的语义信息。对于两个emph ImgNets，它们各自采用相同的卷积神经网络结构，但具有不同的权重，即不对称卷积神经网络。两个emph ImgNets用于生成有区别的紧凑哈希码。具体来说，emph LabelNet的功能是捕获丰富的语义信息，用于指导两个emph ImgNets最小化真实连续特征和离散二进制代码之间的差距。通过这样做，textbf ADSQ可以充分利用最关键的语义信息来指导特征学习过程，并考虑公共语义空间和汉明空间的一致性。我们的实验结果表明，textbf ADSQ可以生成高判别性的紧凑哈希码，并且它在三个基准数据集CIFAR 10，NUS WIDE和ImageNet上优于当前最先进的方法。DNA: Deeply-supervised Nonlinear Aggregation for Salient Object DetectionAuthors Yun Liu, Deng Ping Fan, Guang Yu Nie, Xinyu Zhang, Vahan Petrosyan, Ming Ming Cheng显着目标检测的最新进展主要是为了开发如何在卷积神经网络CNN中有效地整合多尺度卷积特征。许多现有技术方法强制执行深度监督以执行侧向输出预测，其被线性地聚合以用于最终显着性预测。在本文中，我们在理论上和实验上证明了侧输出预测的线性聚合是次优的，并且它仅限制了对深度监督所获得的侧输出信息的使用。为了解决这个问题，我们提出了深度监督的非线性聚合DNA，以更好地利用各种侧输出的补充信息。与现有方法相比，它聚合了侧输出特征而不是预测，并且ii采用非线性而非线性变换。实验表明，DNA可以成功突破当前线性方法的瓶颈。具体而言，所提出的显着性检测器，一种带有DNA的改进的U Net架构，在各种数据集和评估指标上都有利于最先进的方法，而不需要花哨。代码和数据将在纸张验收后发布。Shape Robust Text Detection with Progressive Scale Expansion NetworkAuthors Wenhai Wang, Enze Xie, Xiang Li, Wenbo Hou, Tong Lu, Gang Yu, Shuai Shao场景文本检测已经取得了快速进展，特别是随着最近卷积神经网络的发展。但是，仍存在两个阻碍算法进入工业应用的挑战。一方面，大多数现有技术算法都需要四边形边界框，这样可以准确地定位任意形状的文本。另一方面，彼此接近的两个文本实例可能导致错误检测，其涵盖两个实例。传统上，基于分割的方法可以缓解第一个问题，但通常无法解决第二个挑战。为了解决这两个挑战，在本文中，我们提出了一种新颖的渐进式扩展网络PSENet，它可以精确地检测任意形状的文本实例。更具体地说，PSENet为每个文本实例生成不同比例的内核，并逐渐将最小比例内核扩展为具有完整形状的文本实例。由于最小尺度内核之间存在大的几何边距，因此我们的方法可以有效地分割紧密文本实例，从而更容易使用基于分割的方法来检测任意形状的文本实例。 CTW1500，Total Text，ICDAR 2015和ICDAR 2017 MLT的大量实验验证了PSENet的有效性。值得注意的是，在CTW1500上，一个充满长曲线文本的数据集，PSENet在27 FPS下实现了74.3的F测量，而我们最好的F测量82.2优于6.6的现有算法。该代码将在未来发布。Few-Shot Deep Adversarial Learning for Video-based Person Re-identificationAuthors Lin Wu, Yang Wang, Hongzhi Yin, Meng Wang, Ling Shao, B. C. Lovell基于视频的人物识别ID是指从任意未对齐的视频片段中跨越相机视图匹配人。现有方法依赖于监督信号来优化投影空间，在该投影空间下，帧间视频之间的距离最小化最小化。然而，这需要在摄像机视图中对人进行详尽的标记，使其无法在大型网络摄像机中进行缩放。此外，注意到，没有明确地解决具有视图不变性的学习有效视频表示，否则哪些特征表现出不同的分布。因此，匹配人物ID的视频需要灵活的模型来捕捉时间序列观察中的动态，并学习视图不变表示以及对有限标记的训练样本的访问。在本文中，我们提出了一种新的基于视频的人物ID的镜头深度学习方法，以学习具有辨别力和视图不变性的可比表示。所提出的方法是在变分递归神经网络VRNNs上开发的，并且经过逆向训练以产生具有时间依赖性的潜在变量，所述时间依赖性具有高度辨别力但在匹配人员中看不变。通过对三个基准数据集进行的大量实验，我们凭经验展示了我们的方法在创建视图不变时间特征和通过我们的方法实现的最新技术性能方面的能力。DenseAttentionSeg: Segment Hands from Interacted Objects Using Depth InputAuthors Zihao Bo, Hang Zhang, Junhai Yong, Feng Xu我们提出了一种基于DNN的实时技术，用于根据深度输入来分割交互运动的手和对象。我们的模型称为DenseAttentionSeg，它包含一个密集的注意机制，用于融合不同尺度的信息，并通过跳过连接提高结果质量。此外，我们在模型训练中引入轮廓损失，这有助于生成准确的手和物体边界。最后，我们提出并将发布我们的InterSegHands数据集，这是一个包含大约52k手对象交互深度图的精细手部分割数据集。我们的实验评估了我们的技术和数据集的有效性，并表明我们的方法在交互分割方面优于当前最先进的深度分割方法。Synthesizing a 4D Spatio-Angular Consistent Light Field from a Single ImageAuthors Andre Ivan, Williem, In Kyu Park从单个图像合成密集采样的光场对于许多应用是非常有益的。传统方法重建深度图并依赖于基于物理的渲染和辅助网络来改进合成的新颖视图。简单的基于像素的丢失还通过使其依赖于像素强度提示而不是几何推理来限制网络。在这项研究中，我们表明，可以使用不同的几何表示，即外观流，来稳健和直接地合成来自单个图像的光场。提出了一种不需要基于物理的方法或后处理子网的单端到端深度神经网络。提出了两种基于已知光场领域知识的新型损失函数，使网络能够有效地保持子孔径图像之间的空间角度一致性。实验结果表明，该模型成功地合成了密集光场，定性和定量优于以前的模型。该方法可以推广到任意场景，而不是关注特定类别的对象。合成光场可用于各种应用，例如深度估计和重新聚焦。CUTIE: Learning to Understand Documents with Convolutional Universal Text Information ExtractorAuthors Xiaohui Zhao, Zhuo Wu, Xiaoguang Wang从文件（例如收据或发票）中提取关键信息，并将感兴趣的文本保存到结构化数据，对于包括但不限于会计，财务和税收领域的办公自动化的文档密集流程过程至关重要。为了避免为每种特定类型的文档设计专家规则，一些已发表的作品试图通过学习模型来基于NLP字段中的命名实体识别NER方法来探索文本序列中的语义上下文来解决该问题。在本文中，我们建议利用文档中文本的语义和空间分布的有效信息。具体而言，我们提出的模型，卷积通用文本信息提取器CUTIE，在网格文本上应用卷积神经网络，其中文本作为具有语义内涵的特征嵌入。我们进一步探讨了采用不同结构的卷积神经网络的效果，并提出了一种快速便携的结构。我们证明了所提方法在数据集上的有效性，该数据集具有多达6,980个标记收据，无需任何预训练或后期处理，实现了比BERT高得多但只有1 10个参数且不需要3,300M的最先进性能用于预训练的单词数据集。实验结果还表明，CUTIE能够以更少量的训练数据实现最先进的性能。Local Aggregation for Unsupervised Learning of Visual EmbeddingsAuthors Chengxu Zhuang, Alex Lin Zhai, Daniel Yamins神经网络中无监督的学习方法对于进一步提高人工智能具有重要意义，因为它们可以在不需要大量昂贵注释的情况下实现网络训练，并且因为它们将是部署的通用学习的更好模型由人类。然而，无监督网络长期落后于其监督对手的表现，特别是在大规模视觉识别领域。最近在训练深度卷积嵌入以最大化非参数实例分离和聚类目标方面的发展已经显示出弥合这一差距的希望。在这里，我们描述了一种方法，该方法训练嵌入函数以最大化局部聚合的度量，使得类似的数据实例在嵌入空间中一起移动，同时允许不同的实例分离。该聚合度量是动态的，允许出现不同尺度的软聚类。我们在几个大规模视觉识别数据集上评估我们的程序，在ImageNet中实现对象识别的无监督转移学习性能，在Places 205中进行场景识别，在PASCAL VOC中进行对象检测。Lending Orientation to Neural Networks for Cross-view Geo-localizationAuthors Liu Liu, Hongdong Li本文研究了基于图像的地理定位IBL问题，利用地面到航空的交叉视图匹配。目标是通过将地面级查询图像与大的地理标记的航空图像数据库（例如，卫星图像）匹配来预测地面级查询图像的空间位置。由于他们的观点和视觉外观的巨大差异，这是一项具有挑战性的任务。针对该问题的现有深度学习方法已经集中于最大化空间上靠近图像对之间的特征相似性，同时最小化远离的其他图像对。他们通过在地面和航拍图像中基于视觉外观的深度特征嵌入来实现这一点。然而，在日常生活中，人们通常使用em方向信息作为空间定位任务的重要线索。受这种见解的启发，本文提出了一种新的方法，它赋予深度神经网络定向的常识。给定地面球形全景图像作为查询输入和大地理参考卫星图像数据库，我们设计了一种连体网络，其明确地编码方向，即图像的每个像素的球面方向。我们的方法显着提高了学习的深层特征的判别力，导致更高的回忆率和精度优于以前的所有方法。与以前性能最佳的网络相比，我们的网络使用的第五个参数数量也更紧凑。为了评估我们方法的推广，我们还创建了一个大规模的跨视图定位基准测试，其中包含覆盖城市的100K地理标记地面空中对。我们的代码和数据集可在网址上找到ESFNet: Efficient Network for Building Extraction from High-Resolution Aerial ImagesAuthors Jingbo Lin, Houbing Song, Weipeng Jing从高分辨率航拍图像中提取建筑物足迹始终是城市动态监测，规划和管理的重要组成部分。由于高分辨率航拍图像中的复杂环境和土地覆盖物体，它在遥感研究中也是一项具有挑战性的任务。近年来，深度神经网络在提高遥感图像建筑物提取精度方面取得了很大成就。然而，大多数现有方法通常需要大量参数和浮点运算以获得高精度，这导致高内存消耗和低推理速度，这对研究是有害的。在本文中，我们提出了一种新的深度结构ESFNet，它采用可分离的分解残差块并利用扩张的卷积，旨在保持轻微的精度损失，同时具有较低的计算成本和内存消耗。这是第一次在遥感领域引入深度神经网络的效率视图。我们的ESFNet能够在单个Tesla V100上以超过100 FPS的速度运行，比现有的实时架构ERFNet需要少6倍的FLOP并且参数减少18倍，同时保留相似的精度，无需任何额外的上下文模块，后期处理和预先训练的方案。我们在WHU Building Dataset上评估了我们的网络，并将其与其他最先进的架构进行了比较。结果和综合分析表明，我们的网络有利于高效的遥感研究和应用，并可以进一步扩展到其他领域。该代码是公开的A Deep Dive into Understanding Tumor Foci Classification using Multiparametric MRI Based on Convolutional Neural NetworkAuthors Weiwei Zong, Joon Lee, Chang Liu, Eric Carver, Aharon Feldman, Branislava Janic, Mohamed Elshaikh, Milan Pantellic, David Hearshen, Indrin Chetty, Benjamin Movsas, Ning Wen数据稀缺已经抑制了深度学习模型在使用多参数MRI的前列腺图像分析方面取得更大进展。本文开发了一种有效的卷积神经网络CNN，用于对前列腺癌患者的病变恶性肿瘤进行分类，在此基础上系统地分析了模型解释，以弥合自然图像与MR图像之间的差距，这是该类型中的第一个。文献。通过将中间特征馈送到称为加权极端学习机的传统分类算法中来解决并且成功地解决了小样本量的问题，其中考虑了输出类别之间的不平衡分布。在公共数据集上训练的模型能够推广到来自独立机构的数据以进行准确预测。发现生成的显着图与病变良好匹配，可以使临床医生有益于诊断目的。Relation-aware Graph Attention Network for Visual Question AnsweringAuthors Linjie Li, Zhe Gan, Yu Cheng, Jingjing Liu为了回答关于图像的语义复杂问题，视觉问题回答VQA模型需要完全理解图像中的视觉场景，尤其是不同对象之间的交互动态。我们提出了一种关系感知图注意网络ReGAT，它将每个图像编码成图形，并通过图形注意机制模拟多类型的对象间关系，以学习问题自适应关系表示。两种类型的视觉对象关系被探索i显式关系，其表示对象之间的几何位置和语义交互，以及ii捕获图像区域之间的隐藏动态的隐式关系。实验表明，ReGAT在VQA 2.0和VQA CP v2数据集上均优于先前的现有技术方法。我们进一步表明ReGAT与现有的VQA架构兼容，并且可以用作通用关系编码器来提升VQA的模型性能。FrameNet: Learning Local Canonical Frames of 3D Surfaces from a Single RGB ImageAuthors Jingwei Huang, Yichao Zhou, Thomas Funkhouser, Leonidas Guibas在这项工作中，我们介绍了从单个RGB图像识别密集规范3D坐标系的新问题。我们观察到图像中的每个像素对应于下面的3D几何中的表面，其中规范框架可以被识别为由三个正交轴表示，一个沿着其法线方向，一个沿着其切线平面。我们提出了一种从RGB预测这些轴的算法。我们的第一个见解是，使用最近引入的方向场合成方法自动计算的规范帧可以为任务提供训练数据。我们的第二个见解是，设计用于表面法线预测的网络在联合训练以预测规范帧时提供更好的结果，并且在训练时甚至更好地预测规范帧的2D投影。我们猜想这是因为规范切线方向的投影经常与图像中的局部梯度对齐，并且因为这些方向通过投影几何和正交性约束与3D规范帧紧密相关。在我们的实验中，我们发现我们的方法可以预测3D规范帧，这些帧可以用于表面法线估计，特征匹配和增强现实等应用。Attention-Guided Generative Adversarial Networks for Unsupervised Image-to-Image TranslationAuthors Hao Tang, Dan Xu, Nicu Sebe, Yan Yan生成性对抗网络中的现有技术方法GAN能够利用不成对的图像数据来学习从一个图像域到另一个图像域的映射函数。但是，这些方法通常会产生伪像，并且只能转换低级信息，但无法传输图像的高级语义部分。原因主要是发生器不具有检测图像的最具辨别力的语义部分的能力，因此使得生成的图像具有低质量。为了处理这种限制，在本文中，我们提出了一种新的注意引导生成对抗网络AGGAN，它可以检测最具辨别力的语义对象，并最大限度地减少语义操作问题中不需要部分的变化，而无需使用额外的数据和模型。 AGGAN中的注意力引导发生器能够通过内置注意机制产生注意力掩模，然后将输入图像与注意力掩模融合以获得高质量的目标图像。此外，我们提出了一种新颖的注意引导鉴别器，它只考虑有人居住的区域提议的AGGAN通过端到端时尚进行训练，具有对抗性损失，周期一致性损失，像素丢失和注意力损失。定性和定量结果都表明，我们的方法可以比现有模型更有效地生成更清晰，更准确的图像。Multifaceted 4D Feature Segmentation and Extraction in Point and Field-based DatasetsAuthors Franz Sauer, Kwan Liu Ma大规模多方面数据的使用在各种科学应用中很常见。在许多情况下，这种多方面的数据采用基于场的欧拉和基于点轨迹的拉格朗日表示的形式，因为每个数据在表征研究系统方面具有一组独特的优势。此外，研究这些多方面数据集的规模和复杂性的增加受到感知能力和可用计算资源的限制，需要复杂的数据缩减和特征提取技术。在这项工作中，我们提出了一种新的4D特征分割提取方案，可以同时对场和点轨迹数据类型进行操作。生成的特征是具有基于字段和基于点的组件的时变数据子集，并且基于来自两种数据类型的底层模式来提取。这使研究人员能够更好地探索两种数据表示之间的空间和时间相互作用，并从新的角度研究潜在的现象。我们使用GPU加速并行化我们的方法，并将其应用于现实世界的多方面数据集，以说明可以提取和探索的功能类型。Revisiting Local Descriptor based Image-to-Class Measure for Few-shot LearningAuthors Wenbin Li, Lei Wang, Jinglin Xu, Jing Huo, Yang Gao, Jiebo Luo在图像分类中很少有镜头学习旨在学习分类器，以便在每个类只有少数训练样例时对图像进行分类。最近的工作已经取得了很好的分类性能，其中通常使用基于图像级特征的度量。在本文中，我们认为，鉴于少数镜头学习中的例子稀缺，在这样的水平上的度量可能不够有效。相反，我们认为应该采用基于局部描述符的图像到类测量，这是因为它在局部不变特征的鼎盛时期取得了令人惊讶的成功。具体来说，在最近的情节训练机制的基础上，我们提出了一个简短的深度最近邻神经网络DN4，并以端到端的方式训练它。它与文献的主要区别在于通过基于局部描述符的图像到类测量来替换最终层中基于图像级特征的测量。该度量是通过卷积特征映射的深度局部描述符上的k最近邻搜索在线进行的。所提出的DN4不仅学习了图像到类测量的最佳深度局部描述符，而且在示例稀缺的情况下也利用了这种测量的更高效率，这要归功于同一类中的图像之间的视觉模式的可交换性。我们的工作为少数镜头学习提供了一个简单，有效且计算有效的框架。基准数据集的实验研究始终显示出其优于相关技术水平的优势，最大的绝对改进为17。源代码可以从UrlFont获得Bit-Flip Attack: Crushing Neural Network withProgressive Bit SearchAuthors Adnan Siraj Rakin, Zhezhi He, Deliang Fan最近提出了与不同应用和组件相关的深度神经网络DNN的几个重要安全问题。 DNN最广泛调查的安全问题来自其恶意输入，a.k.a对抗性示例。然而，DNN参数的安全性挑战尚未得到很好的探索。在这项工作中，我们首先提出了一种名为Bit Flip Attack BFA的新型DNN重量攻击方法，它可以通过在其重量存储存储器系统（即DRAM）中恶意翻转极少量的位来破坏神经网络。位翻转操作可以通过众所周知的Row Hammer攻击进行，而我们的主要贡献是开发一种算法，将存储在存储器中的最脆弱的DNN权重参数位识别为二进制位，这可以最大限度地降低精度降级翻转。我们提出的BFA利用逐行比特搜索PBS方法，该方法结合梯度排序和逐行搜索来识别要翻转的最脆弱的比特。在PBS的帮助下，我们可以成功地攻击ResNet 18完全故障，即前1个精度仅从93.8到0.1中通过13位翻转从69.8降低到0.1，而随机翻转100位只会使精度降低不到1。Improving Object Detection with Inverted AttentionAuthors Zeyi Huang, Wei Ke, Dong Huang改善物体探测器以防止遮挡，模糊和噪声是在实际应用中部署探测器的关键步骤。由于不可能通过数据收集来消除所有图像缺陷，因此许多研究人员试图在训练中生成硬样本。生成的硬样本是图像或特征映射，其中在空间维度中丢弃了粗糙的补丁。在训练额外的硬样本和/或使用额外的网络分支估计丢失补丁时需要大量的开销。在本文中，我们使用称为Inverted Attention IA的高效细粒机制改进了物体探测器。与仅关注物体主导部分的原始探测器网络不同，具有IA的探测器网络迭代地反转对特征图的关注，并且更多地关注互补对象部分，特征通道甚至上下文。我们的方法1沿着特征的空间和通道维度操作，不需要对硬样本的额外训练，没有额外的网络参数用于注意力估计，并且没有测试开销。实验表明，我们的方法在基准数据库上不断改进了两级和单级检测器。SpaceNet MVOI: a Multi-View Overhead Imagery DatasetAuthors Nicholas Weir, David Lindenbaum, Alexei Bastidas, Adam Van Etten, Sean McPherson, Jacob Shermeyer, Varun Kumar, Hanlin Tang在听到的图像中检测和分割对象是一项具有挑战性的任务。顶部图像中的对象的可变密度，随机取向，小尺寸和实例异构性要求不同于为自然场景数据集设计的现有模型的方法。虽然正在开发新的高架图像数据集，但它们几乎普遍包含从最低点直接开销的单个视图，未能解决一个关键变量视角。相比之下，现实世界的头顶图像中的视图各不相同，特别是在动态场景中，例如自然灾害，其中第一次看起来往往超过最低点40度。这代表了对计算机视觉方法的重要挑战，因为改变视角会增加失真，改变分辨率和改变照明。目前，这些扰动对算法检测和对象分割的影响尚未得到证实。为了解决这个问题，我们引入了SpaceNet多视图架空图像MVOI数据集，这是SpaceNet开源遥感数据集的扩展。 MVOI包括27种独特的外观，从32到54度的广泛视角。这些图像中的每一个都覆盖相同的地理位置，并使用126,747个建筑物足迹标签进行注释，从而可以直接评估视点扰动对模型性能的影响。我们对1个建筑物检测，2个概括到不可见的视角和分辨率，以及建筑物足迹提取对分辨率变化的敏感性进行基准测试。我们发现分割和对象检测模型很难识别出最低点图像中的建筑物，并且很难概括为看不见的视图，这是探索在视觉动态环境中检测小型异构目标对象的广泛相关挑战的重要基准。Learning to Transfer Examples for Partial Domain AdaptationAuthors Zhangjie Cao, Kaichao You, Mingsheng Long, Jianmin Wang, Qiang Yang域适应对于在新的和看不见的环境中学习至关重要。通过域对抗性训练，深度网络可以学习解缠结和可转移的特征，从而有效地减少源域和目标域之间的数据集转换，从而进行知识转移。在大数据时代，大规模标记数据集的现成可用性激发了对部分域适应PDA的广泛兴趣，PDA将识别器从标记的大域转移到未标记的小域。它将标准域适应扩展到目标标签仅是源标签子集的场景。在目标标签未知的情况下，PDA的关键挑战是如何在共享类中转移相关示例以促进正向转移，并忽略特定类中的不相关示例以减轻负转移。在这项工作中，我们提出了一种统一的方法，即PDA，示例传输网络ETN，它共同学习源域和目标域的域不变表示，以及渐进式加权方案，量化源示例的可转移性，同时控制它们对学习任务的重要性在目标域中。对几个基准数据集的全面评估表明，我们的方法实现了部分域适应任务的最新结果。The Algorithmic Automation Problem: Prediction, Triage, and Human EffortAuthors Maithra Raghu, Katy Blumer, Greg Corrado, Jon Kleinberg, Ziad Obermeyer, Sendhil Mullainathan在广泛的领域中，算法匹配并超越人类专家的表现，从而考虑人类判断和算法预测在这些领域中的作用。然而，围绕这些发展的讨论隐含地将预测的具体任务与自动化的一般任务等同起来。我们在此论证，自动化不仅仅是对人工与算法性能的比较，还涉及决定首先给予算法的任务实例。我们开发了一个通用框架，将后一个决策作为一个优化问题，我们展示了这个优化问题的基本启发式方法如何能够在医学中大量研究AI的应用中获得性能提升。我们的框架还强调了有效的自动化如何关键地依赖于逐个实例地估算算法和人为错误，我们的结果表明这些错误估计问题的改进如何能够为自动化带来显着的收益。All about Structure: Adapting Structural Information across Domains for Boosting Semantic SegmentationAuthors Wei Lun Chang, Hui Po Wang, Wen Hsiao Peng, Wei Chen Chiu在本文中，我们针对语义分割任务解决了无监督域自适应的问题，其中我们尝试将在具有地面实况标签的合成数据集上学习的知识转移到没有任何注释的真实世界图像。假设图像的结构内容是语义分割中最具信息性和决定性的因素，并且可以跨域轻松共享，我们提出了一种域不变结构提取DISE框架，将图像分解为域不变结构和域特定纹理表示，可以进一步实现跨域的图像转换，并启用标签转移以提高分割性能。大量实验验证了我们提出的DISE模型的有效性，并证明了其优于几种最先进的方法。Counting with Focus for FreeAuthors Zenglin Shi, Pascal Mettes, Cees G. M. Snoek本文旨在计算图像中的任意对象。领先的计数方法从每个对象的点注释开始，从中构造密度图。然后，他们的训练目标通过深度卷积网络将输入图像转换为密度图。我们认为点注释比仅仅构建密度图更有助于监督目的。我们介绍了免费重新利用积分的方法。首先，我们提出了来自分割的监督焦点，其中点被转换为二进制映射。二进制映射与网络分支和伴随的丢失函数组合以关注感兴趣的区域。其次，我们提出了来自全局密度的监督焦点，其中点注释与图像像素的比率用于另一个分支以规范整体密度估计。为了协助密度估计和分割焦点，我们还为点注释引入了改进的内核大小估计器。对四个数据集的实验表明，无论基础网络如何，我们所有的贡献都会减少计数误差，从而导致仅使用单个网络的现有技术精度。最后，我们是第一个依靠WIDER FACE，让我们展示我们的方法在处理不同对象比例和拥挤程度方面的好处。On the Anatomy of MCMC-based Maximum Likelihood Learning of Energy-Based ModelsAuthors Erik Nijkamp, Mitch Hill, Tian Han, Song Chun Zhu, Ying Nian Wu本研究调查了无监督最大似然ML学习中马尔可夫链蒙特卡罗MCMC采样的影响。我们的注意力仅限于家庭非标准化概率密度，其负对数密度或能量函数是ConvNet。总的来说，我们发现用于稳定先前研究中的训练的大多数技术可能产生相反的效果。只需少量超参数且无正则化，即可实现具有ConvNet潜力的稳定ML学习。通过这个最小的框架，我们根据MCMC抽样的实施确定了各种ML学习成果。Amortized Object and Scene Perception for Long-term Robot ManipulationAuthors Ferenc Balint Benczedi, Michael Beetz移动机器人在人类环境中执行长期操纵活动，必须感知具有非常不同视觉特性的各种物体，并且需要在整个任务执行期间可靠地跟踪这些物体。为了提高效率，机器人感知能力需要超出当前可感知的能力，并且应该能够回答关于当前和过去场景的查询。在本文中，我们研究了一种用于长期机器人操纵的感知系统，该系统可以跟踪不断变化的环境并构建感知世界的表示。具体来说，我们引入了一个摊销组件，在整个执行周期中传播感知任务。由此产生的查询驱动感知系统异步地将记录图像的结果整合成符号和数字，我们称之为子符号表示，形成机器人的感知信念状态。Generative Adversarial Networks: recent developmentsAuthors Maciej Zamorski, Adrian Zdobylak, Maciej Zi ba, Jerzy wi tek在传统的生成建模中，良好的数据表示通常是良好的机器学习模型的基础。它可以链接到编码隐藏在原始数据中的更多解释因素的良好表示。随着生成对抗网络GAN的发明，生成对抗网络GAN是能够以无监督和半监督方式学习表示的生成模型的子类，我们现在能够对比地学习从简单的先前分布到目标数据分布的良好映射。本文概述了GAN的最新发展，重点是学习潜在空间表示。Benchmarking Neural Network Robustness to Common Corruptions and PerturbationsAuthors Dan Hendrycks, Thomas Dietterich在本文中，我们建立了严格的图像分类器稳健性基准。我们的第一个基准ImageNet C标准化并扩展了损坏稳健性主题，同时展示了哪些分类器在安全关键应用中更受欢迎。然后我们提出了一个名为ImageNet P的新数据集，它使研究人员能够对分类器对常见扰动的鲁棒性进行基准测试。与最近的稳健性研究不同，该基准评估了常见腐败和扰动的表现，而不是最坏情况下的对抗性扰动。我们发现从AlexNet分类器到ResNet分类器的相对损坏稳健性的变化可以忽略不计。之后，我们发现了增强腐败和扰动稳健性的方法。我们甚至发现绕过的对抗性防御提供了实质性的共同扰动稳健性。我们的基准可以共同帮助未来的工作向强大推广的网络发展。Chinese Abs From Machine TranslationPapers from arxiv.org更多精彩请移步主页pic from pixabay.com"}
{"content2":"AI时代大点兵-国内外知名AI公司2018年最新盘点导言据腾讯研究院统计，截至2017年6月，全球人工智能初创企业共计2617家。美国占据1078家居首，中国以592家企业排名第二，其后分别是英国，以色列，加拿大等国家。本文中选取了国外和国内部分有代表性的AI产业链条上相关公司就行分析（排名不分先后），希望对有志于从事人工智能相关工作或者想了解AI行业目前发展现状的朋友能有所帮助。小编会从AI芯片、应用层算法、应用领域等方面对相关公司进行盘点，由于部分公司可能会涉及产业链条上不同的领域，文中侧重选取了某些点进行分析阐述。备注：文中涉及到的企业估值均源于公开资料，本文对数字真实性不做任何担保；对于企业的明星指数是小编根据公开资料以及行业内部朋友反馈做的综合评估，不作为投资参考。AI芯片相关企业在人工智能领域大规模并行计算是一个刚性的需求，CPU由于本身设计更偏重于多任务处理、逻辑控制所以不太适合在矩阵计算这种需要高并行的场景中应用，这也给了像Nvidia、Xilinx等芯片公司在深度学习时代的爆发的机会。Nvidia(英伟达)明星指数：*****融资轮次&估（市）值：美股上市/1546.58亿美金公司介绍NVIDIA是一家人工智能计算公司。公司创立于 1993 年，总部位于美国加利福尼亚州圣克拉拉市。Jensen Huang (黄教主) 是创始人兼首席执行官。NVIDIA 出品的GPU，重新定义了现代计算机图形技术，并彻底改变了并行计算。尤其是2012年深度学习的初露锋芒，给Nvidia做了一次价值连城的免费广告（Hinton实验室用了NVIDIA GPU GTX 580对深度卷积神经网络计算进行加速取得了非常好的效果），从GeForce GTX系列到目前最新的Tesla V100再到Jetson Xavier移动版芯片 ，NVIDIA的股价也坐上了火箭到现在涨了20多倍。AI相关方向深度学习加速 为了配套GPU硬件销售，NVIDIA开发了一些列配套的深度学习的相关软件SDK，比如cuBlas、CUDNN、TensorRT、DeepStream等高性能计算库，很大程度上帮助了开发这快速落地自己的深度学习应用，完善了自己的GPU生态布局（隔壁的兄弟AMD只能通过）。包括TensorFlow、PyTorch、Caffe、MxNet等知名框架都会使用CUDNN作为GPU端的矩阵计算加速库，小编还听说维护TensorRT库的只有一个研发小哥哥，geiliable！无人驾驶NVIDIA的无人驾驶团队有几百人的规模，是黄教主下重注的一个方向，由教主亲自带队，AUTO延续了NV一直以来的套路-软硬一体。该团队围绕Drive PX计算平台进行技术研发，目前可以做到实时了解车辆周围的状况， 可在高清地图上准确的定位车辆，并且计划一条安全的行驶路径。打造业界领先的自动驾驶车辆平台——囊括了深度学习技术，传感器融合技术以及环视技术来改变驾驶体验。小编认为黄教主做的事情和百度Apollo有点对着干,一个依托硬件从下往上走，一个依托软件从上往下走，最终还是要看谁能笼络更多的车企，快速扩大朋友圈！深度学习应用研究NVIDIA也有自己的深度学习前研技术研究团队包括和外部合作，做了一些不错的研究，尤其是GAN相关的成果比如视频自动生成慢镜头（Super SloMo）、生成高清的明星人脸（分辨率最高达1024×1024像素）等，更多研究可访问下面网址：http://research.nvidia.com/researcharea/computer-vision求职小编了解NVIDIA的核心技术部门都在圣克拉拉，国内只有部分中间件的研发团队以及技术服务/销售部门。国内部分偏重于社招，小部分校招，如果有感兴趣的朋友可以联系小编了解详情。INTEL明星指数：****融资轮次&估（市）值：美股上市/2252.47亿美金公司介绍INTEL是美国一家主要以研制CPU处理器的公司，是全球最大的个人计算机零件和CPU制造商，它成立于1968年，具有50年产品创新和市场领导的历史。众所周知INTEL在服务器和个人电脑领域是当之无愧的No.1，但是后来完美的错失了移动时代的机会，其Atom系列芯片一直没能打开移动市场的局面。深度学习爆发后，INTEL也没有赶上第一波，但是先后收购了包括Nervana Systems，Movidius和Mobileye等人工智能企业，以及Altera这个全球第二大FPGA厂商。AI焦虑下的英特尔成立了人工智能产品事业部（AIPG），可以看到这个PC时代的巨人在人工智能领域全面布局和抢占市场的决心，Good Luck！AI相关方向深度学习加速INTEL的矩阵计算相关软件包括MKL数学计算库以及针对于深度学习优化的MKL-DNN库；硬件包括适合边缘计算的Movidius神经计算棒，Altera系列FPGA，Intel Movidius VPUs，以及INTEL之前为服务端开发的协处理器Xeon Phi（小编接触过的国内大厂对Xeon Phi认同度都比较低）无人驾驶英特尔已错失了两个重要机遇，一个是智能手机市场，一个是车载市场。Mobileye之于英特尔，其在计算机视觉方面的技术积累自然是一项核心资产。而Mobileye对于英特尔来说最大的价值，就是它直接签下的超过27家车企客户，以及庞大的芯片出货量——2016年全年，Mobileye的SoC芯片供卖出约600万个。借助Mobileye在ADAS板块的市场份额，来快速切入整个汽车产业，才是英特尔想要的。特斯拉的Autopilot就有MobileyeQ3芯片(尽管合作已经终结)。Mobileye的技术将被融合到英特尔的Intel GO自动驾驶解决方案中。而英特尔借此可以打通与车企的关系，进一步扩大其在汽车半导体市场的份额。求职INTEL国内研发主要在上海和北京，主要做深度学习框架在INTEL系列CPU上的优化，以及计算机视觉和机器学习方面的算法研发工作（据小编了解，深度学习方面的工作INTEL有很多的实习机会）Xilinx(赛灵思)明星指数：****融资轮次&估（市）值：美股上市/179.97亿美金公司介绍FPGA领域有两个主要玩家Xilinx和Altera，现在Altera已经成了Intel的一部分。Xilinx作为全球最大的可编程芯片（FPGA）厂商集研发、制造并销售范围广泛的高级集成电路、软件设计工具以及作为预定义系统级功能的IP核与一体。赛灵思在AI时代的三大战略布局：ACAP系列产品、数据中心优先、加速八大主流市场发展。ACAP的核心是新一代的FPGA架构，适用于加速广泛的应用，其中包括视频转码、数据库、数据压缩、搜索、AI推断、基因组学、机器视觉、计算存储及网络加速等。前不久Xilinx还高调收购了国内的AI芯片初创公司深鉴科技，小编在这里恭喜深鉴的朋友们上岸:)求职Xilinx北京部门专注于高层次综合（C到硬件语言的转换）软件的开发，目前40左右的规模。所招职位主要面向前端编译及转换。比特大陆明星指数：****融资轮次&估（市）值：B轮/120亿美金公司介绍比特大陆成立于2013年，由吴忌寒和芯片设计专家詹克团联合创办，是一家生产比特币挖矿机、定制芯片、运营“矿池”（比特币矿工工厂）的初创公司。当别人在挖金的时候,我去卖铁锹、卖水!最后,挖金的人依然在疲惫地追逐梦想,而卖铁锹的人已经衣锦还乡。目前比特大陆AI技术团队200余人左右的规模，坐标北京。比特大陆已经拥有比特币矿机70％以上的市场份额。随着比特币市场竞争急剧增加，行业一片红海之势，2017年11月比特大陆正式发布了比特大陆旗下AI品牌SOPHON，以及自研的全球首款张量加速计算芯片——BM1680，正式切入AI芯片市场，AI芯片市场的金主玩家越来越多了。AI相关方向比特大陆的AI产品布局分为三个方面：（1）AI深度学习的推理芯片（2）BIG DATA和CLOUD（大数据和专有云）（3）智能机器人在AI行业布局方面，比特大陆主要切三个行业：安防+AI、互联网+AI和城市大数据。求职比特大陆有自己的人工智能团队支持：安防+AI、互联网+AI和城市大数据等布局。小编了解目前对图像算法的人才需求量比较大，计算机视觉和深度学习算法尤其是人脸识别、视频智能分析等方向很急迫，感兴趣的朋友小编也可以推荐。寒武纪明星指数：****融资轮次&估（市）值：B轮/20亿美金公司介绍寒武纪科技公司成立于2016年，其前身是中科院计算所于2008年组建的“探索处理器架构与人工智能的交叉领域”10人学术团队。寒武纪科技是全球智能芯片领域的先行者，是全球第一个成功流片并拥有成熟产品的智能芯片公司，拥有终端和服务器两条产品线。2016年推出的寒武纪1A处理器（Cambricon-1A)是世界首款商用深度学习专用处理器，面向智能手机、安防监控、可穿戴设备、无人机和智能驾驶等各类终端设备，运行主流智能算法时性能功耗比全面超越CPU和GPU。据称华为麒麟970的NPU就是寒武纪的IP。寒武纪团队基本是中科大和清华的班底，出自这两所学校的同学风格都很务实。求职遵循着营造生态的思路，寒武纪也会针对自己下游的客户需求进行软件层面优化比如图像处理、人脸识别、目标检测等方案以及基于寒武纪深度学习芯片以及硬件平台，开发高性能深度学习库。有这方面背景的同学可以考虑。地平线机器人明星指数：***融资轮次&估（市）值：A+轮/未知公司介绍地平线为提供高性能、低功耗、低成本、完整开放的嵌入式人工智能解决方案。企业使命是赋能万物，让每个人的生活更安全，更美好。地平线机器人的核心产品是视觉芯片，应用领域为智能驾驶、智能城市、智能商业，目前发布了两款自主研发AI芯片：“征程”和“旭日”，其中征程1.0处理器，面向智能驾驶；旭日1.0处理器，主攻智能摄像头。地平线的核心团队班底主要是来自百度IDL以及NEC，创始人余凯和黄畅也是从NEC到百度IDL的老搭档。AI相关方向地平线目前在智能驾驶、智能城市、智能商业都有布局。技术方面在图像识别包括人体、人脸、车辆、通用目标检测、跟踪与识别以及图像理解、图像质量评估和增强，视频分析等计算机视觉相关方向以及深度学习模型优化、压缩、加速方面都有涉猎；除了计算机视觉，另外在语音识别技术方面也有应用场景。求职地平线涉及的应用场景很丰富，包括智能驾驶、智能城市、智能商业里面涉及了大量的计算机视觉技术，尤其是人相关的识别分析以及智能驾驶相关的技术，如果对深度学习有一定研究基础的朋友都可以考虑下，小编可以帮忙内推。其他谷歌-TPU百度-昆仑IBM-TrueNorth小编这里就不做一一介绍了应用层相关企业--安防相关安防领域一直被认为是人工智能落地最好的行业之一。首先，以视频技术为核心的安防行业拥有海量的数据来源，可以充分满足人工智能对于算法模型训练的要求；其次，安防行业事前预防、事中响应、事后追查的诉求与人工智能的技术逻辑完全吻合。涌现出了像商汤科技、旷视科技、云从等聚焦于人脸识别、行为分析等图像智能领域的公司。商汤科技明星指数：*****融资轮次&估（市）值：C+轮/30亿美金公司介绍商汤科技,是国内一家致力于计算机视觉和深度学习原创技术的创新型科技公司,提供人脸识别、语音技术,文字识别,人脸识别,深度学习等一系列人工智能产品及解决方案,帮助各行各业的客户打造智能化业务系统。商汤以港中文汤晓鸥实验室团队为核心班底，短短数年打造成为了AI独角兽企业，可谓是国内AI企业的当红炸子鸡。其主要业务主要在智能安防以及和手机厂商在相机层面的算法SDK合作。商汤科技正在大力推动的“人工智能+”和人工智能赋能百业。AI相关方向在解决方案上，商汤主推的产品有包括SenseFace人脸布控系统、SenseID身份验证解决方案、SenseGo智慧商业解决方案、SensePhoto手机全套影像处理解决方案、SenseAR增强现实感特效引擎以及SenseDrive DMS驾驶员监控系统等。从技术方向拆解目前商汤还是以人脸识别分析技术为主的一家公司，其它方向包括通用物体检测识别、动作行为分析，证件OCR、ReID、SLAM、深度学习模型优化以及深度学习框架优化等技术方向。求职商汤内部算法研发更多的是博士在做，如果是硕士或本科做的更多的是算法工程层面的开发优化；商汤的实习机会很多，而且能参与到算法层面的原型调研和研发。旷视科技明星指数：*****融资轮次&估（市）值：C轮/20亿美金公司介绍旷视科技成立于2011年底，创始人印奇和唐文斌都出自清华姚期智实验班本科，在人脸识别领域达到世界水平，对外提供了人脸识别、人工智能、智能地产、智能安防等相关技术解决方案，是国内领先的人工智能创业公司。致力于为政企用户和开发者提供全方位的行业智能解决方案与智能数据服务。最近把清华姚班的大佬姚期智也拉来站台担任旷视科技Face++学术委员会担任首席顾问，操作很溜，准备上市的节奏！AI相关方向旷视科技技术方向包括人脸检测、人脸分析、人脸识别,以及图像识别、OCR证件识别、文字识别。其重点还是在人脸识别在安防和金融领域的应用，而且已经从单纯软件算法层面向软硬一体解决方案过渡。求职旷视在计算机视觉包括分类，检测，分割，跟踪，OCR、SLAM和3D感知等都有涉猎，涉及的子领域非常多，算法相关主要分两条线研究科学家和全栈人工智能工程师（偏工程应用优化），另外对实习生的需求也很大，锻炼机会多。格灵深瞳明星指数：***融资轮次&估（市）值：B轮/未知公司介绍格灵深瞳成立于2013年，是一家同时具备计算机视觉和深度学习技术以及嵌入式硬件研发能力的人工智能公司，作为一家视频大数据产品和方案提供商，自主研发的深瞳技术在人和车的检测、跟踪与识别方面居于世界领先水平，公司主要关注的领域包括公共安全、智能交通、金融安防等，同时公司在无人驾驶（驭势科技）、机器人和智能医疗方面也进行了深入的布局。格灵深瞳的起步很早，但是最初从深度相机切入市场并没有得到广泛的认可从而丧失了先机，发展势头明显弱于同期的其它几个小巨头。AI相关方向格灵深瞳的商业落地围绕三个方面：人、人脸、车的智能分析。针对这三个方面，格灵深瞳推出了数款创新性的产品，包括皓目行为分析仪、威目车辆大数据系统、威目视频结构化系统、威目人脸识别系统、威目视图大数据平台，以及深瞳人眼摄像机。求职格灵深瞳在人脸识别、人体ReID和车辆检测识别等方向都有重点布局，有这方面背景的朋友可以勾搭。依图科技明星指数：****融资轮次&估（市）值：C+轮/150亿人民币公司介绍依图科技总部在上海，是上海人工智能领域的明星初创企业，主要从事人工智能创新性研究，致力于将先进的人工智能技术与行业应用相结合，拓展人工智能新疆界。依图科技人工智能技术的应用领域包括：智能安防、依图医疗、智慧金融、智慧城市、智能硬件等。求职依图的AI方向主要布局在人脸识别、车辆识别和医疗图像识别等几个方向，这个公司相对低调，但是技术实力不容小觑，相关背景的同学都可以考虑勾搭。其它图像分析相关领域除了应用于安防场景，基于人工智能的图像分析技术还应用到了互联网图片审核、商业智能等场景 ，下面介绍几个相关的的创业公司：图普科技明星指数：***融资轮次&估（市）值：A轮/未知公司介绍图普科技以互联网图片审核切入市场，当前产品包括内容审核、智能鉴黄、客流统计分析、智慧门店系统等。主要服务于互联网、连锁零售、泛安防等领域，致力于为企业级用户提供人工智能产品和行业解决方案。图普云平台涵盖黄暴识别、人脸识别、证件识别、场景识别、图像风格化等数十种图像识别接口。AI相关方向图像分类、人脸识别、证件识别等求职图普总部在广州，产品更偏重于算法工程方面的应用，有一定算法和比较好工程能力的朋友都可以试试。图匠科技明星指数：***融资轮次&估（市）值：A轮/未知公司介绍ImageDT（图匠数据）对标以色列的图像识别公司Trax，该团队专注于为企业提供商业智能化技术与服务，具备数据采集、图像识别、语义分析、海量数据掘等技术，业务内容包括大数据市场研究、零售智能化管理、图像识别智能审计、网络图片舆情分析等。小编认为这个垂直领域后期会聚集不少的玩家，为什么呢？技术层面门槛相对低，而且商业模式清晰，挣钱啊！AI相关方向商品识别,智能审计,货架识别,LOGO识别,智能零售,人脸识别等求职图匠总部在广州，产品更偏重于算法工程方面的应用，有一定算法基础和比较好工程能力的朋友都可以试试。码隆科技明星指数：***融资轮次&估（市）值：B轮/未知公司介绍码农科技，好像错了不是“码农”是“码隆”，小编皮了下。。。公司坐标深圳，成立于2014年，是一家为企业提供计算机视觉技术服务的人工智能公司。公司致力于向开发人员和企业提供业界精确的视觉商品识别产品和解决方案，以AI技术赋能企业，帮助从零售到医疗的各类企业提升效率和品质。AI相关方向码隆科技的主要业务在以图搜图，商品识别方向，相关视觉技术有图像检索、物体分类等。求职码隆科技总部在深圳，产品更偏重于算法工程方面的应用，有一定算法基础和比较好工程能力的朋友都可以勾搭。其他海鑫智圣-安防云从科技-安防阅面科技-AI视觉模组等下篇-预告无人驾驶1. Roadster.ai2. 景驰科技3. 驭势4. Momenta5. 图森OCR1. 百度作业帮2. 学霸君3. 小猿搜题4. 合合科技语音识别1. 科大讯飞2. 思必驰3. 云知声4. 出门问问自然语言处理1. 小i机器人2. 图灵机器人3. 三角兽4. 蓦然认知数据分析1. 天云大数据2. 明略数据3. Talkingdata4. 第四范式参考资料：(1)http://ftp.shujuju.cn/platform/file/2018-0619/%E4%BA%BF%E6%AC%A7%E6%99%BA%E5%BA%93%EF%BC%9A2018%E4%B8%AD%E5%9B%BD%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%95%86%E4%B8%9A%E8%90%BD%E5%9C%B0%E7%A0%94%E7%A9%B6%E6%8A%A5%E5%91%8A+100%E5%BC%BA%E4%BC%81%E4%B8%9A%E6%A6%9C%E5%8D%95.pdf(2)http://www.caict.ac.cn/kxyj/qwfb/qwsj/201804/P020180213603539476032.pdf(3)http://www.stdaily.com/index/kejixinwen/2018-07/13/689842/files/f3004c04e7de4b988fc0b63decedfae4.pdf作者：SIGAI链接：https://www.jianshu.com/p/37fdf490a238來源：简书简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。"}
{"content2":"机器学习重要国际会议：国际机器学习会议（ICML）国际神经信息处理系统会议（NIPS）国际学习理论会议（COLT）机器学习重要区域会议：欧洲机器学习会议（ECML）亚洲机器学习会议（ACML）机器学习重要国际期刊：Journal of Machine Learning ResearchMachine Learning人工智能领域重要会议：IJCAIAAAI人工智能重要期刊：Artificial IntelligenceJournal of Artificial Intelligence Research数据挖掘领域重要会议：KDD、ICDM数据挖掘重要期刊：ACM Transactions on Knowledge Discovery from DataData Mining and Knowledge Discovery计算机视觉与模式识别重要会议：CVPR计算机视觉与模式识别重要期刊：Neural ComputationIEEE Transactions on Neural Networks and Learning Systems.统计学重要期刊：Annals of Statistics国内：国内有两年一次的中国机器学习大会（CCML）每年举行的机器学习及其应用研讨会（MLA）很多学术刊物也经常刊登机器学习相关论文"}
{"content2":"【计算机视觉】形态学滤波标签（空格分隔）： 【图像处理】 【信号处理】版权声明：本文为博主原创文章，转载请注明出处http://blog.csdn.net/lg1259156776/。说明：本文主要想弄清楚形态学滤波在图象处理和信号处理中的应用，图像处理中非常直观的通过腐蚀膨胀获得开闭运算的效果，而在数据实时滤波中，形态学滤波也是可以使用的。形态学滤波基本知识原理：在特殊领域运算形式——结构元素（Sturcture Element），在每个像素位置上与二值图像对应的区域进行特定的逻辑运算。运算结构是输出图像的相应像素。运算效果取决于结构元素大小内容以及逻辑运算性质。膨胀、腐蚀、开、闭运算是数学形态学最基本的变换。结构元素简单地定义为像素的结构（形状）以及一个原点（又称为锚点），使用形态学滤波涉及对图像的每个像素应用这个结构元素，当结构元素的原点与给定的像素对齐时，它与图像相交部分定义了一组进行形态学运算的像素。原则上，结构元素可以是任何形状，但通常使用简单的形状，比如方形、圆形和菱形，而原点位于中心位置（基于效率的考虑）。腐蚀和膨胀两个滤波操作也运算在每个像素周围像素集合上（邻域），这是由结构元素定义的。当应用到一个给定的像素时，结构元素的锚点与该像素的位置对齐，而所有与他相交的像素都被包括在当前像素集合中。腐蚀替换当前像素为像素集合中找到的最小的像素值，而膨胀则替换为像素集合中找到的最大像素值。当然，对于二值图像，每个像素只能被替换为白色像素或黑色像素。这一段的论述，可以参考Opencv计算机视觉 编程手册。对于腐蚀和膨胀的差别，可以通过想象，腐蚀呢， 如果给定像素的结构元素触碰到背景，那么该像素被设置为背景，而在膨胀的情况下，如果触碰到前景，该像素被设置为前景，所以很明显，腐蚀操作后物体的尺寸会减小，而膨胀操作后物体的尺寸会增大，同时内部的一些洞被填满。事实上，腐蚀一副图像两次，就像是让结构元素对自己进行膨胀后再去腐蚀同一幅图像；反过来对膨胀也是合适的。有下面两种说法：对图像的腐蚀操作等于对图像负片的膨胀操作的负片；对图像的膨胀操作等于对图像负片的腐蚀操作的负片；实际上说的是对目标进行的腐蚀，等效于对背景进行的膨胀；反之亦然。腐蚀的最简单的应用是从图中消除不相关的细节，而膨胀的最简单的应用是将裂缝桥接起来。开运算是先腐蚀再膨胀，开运算一般断开狭窄的间断和消除细的突出物，而闭操作通常消弥狭窄的间断和长细的鸿沟，消除小的孔洞，并填充轮廓线中的断裂。开运算与闭运算的结合使用能使作用对象的轮廓变得光滑。开运算是先腐蚀再膨胀，而闭运算是先膨胀再腐蚀；在检验闭滤波器的结果时，可以看到白色前景物体中的小洞被填充，该滤波器同时连接多个相邻物体，基本上，无法完全包含结构元素的洞洞或者缝隙都将被滤波器移除。反过来，开滤波器则是移除掉场景中比较小的物体，因为它门无法完全包含结构元素。这些滤波器通常在物体检测中应用，闭滤波器将误分割为碎片的物体重新连接起来，而开滤波器则去除掉图像噪声点引起的小像素块（Blob）。因此，在视频序列中使用他们很有帮助，如果测试的二值图像相继使用闭、开操作，获得图像将只显示场景中的主要物体。如果优先处理噪点，也可以先进行开运算，再进行闭运算，但是有可能去除掉一些分散的物体。需要注意的是，对于对于一幅图像多次使用相同的开运算或者闭运算是没有效果的，因为在第一次闭（开）运算填充图像中的洞洞后，再次应用相同的滤波，不会对图像产生任何变化。用数学的术语讲，这些运算是等幂的。使用形态学滤波对图像进行边缘及角点检测形态学滤波可以用于检测图像中指定的特征。一种比较形象的方法是将灰度图像看做是“等高线”（比如分水岭图像分割算法）：亮的区域代表山峰，而暗的区域代表山谷，图像的边沿就对应于峭壁。如果腐蚀一幅图像，会导致山谷被扩展，而峭壁减少了。相反的，如果膨胀一幅图像，峭壁则会增加。但是这两种情况下，中间的部分（大片的谷底和高原）基本保持不变。在上述理解的基础上，如果我们对图像的腐蚀和膨胀的结果做差，就能提取图像的边界：因为边界区域，二者完全不同。（实际上，我们也可以用腐蚀或者膨胀的结果与源图像做差得出类似结果，但提取的边界会比较细）。可以看出，结构元越大，边界越粗。在OpenCV中，将形态学操作函数morphologyEx 的第4个参数设为MORPH_GRADIENT，就能完成上述工作。利用形态学操作获取角点稍微有一些复杂，它试用了四种不同的结构元素，基本方法是对一幅图像先腐蚀，在膨胀。但是这两次操作使用的结构元却不同。这些结构元的选取使得直线保持不变，但是由于他们各自作用的效果，角点处的边沿被影响了。我们结合一幅图来说明：第一幅图是原图。在被十字形元素膨胀后，方块的边缘被扩张，而由于十字形元素没有击中角点，此处不受影响。中间的方块描述了这个结果；膨胀后的图像接着被菱形元素腐蚀，这次运算将大多数的边缘恢复到原始位置，但之前没有膨胀过的角点被向内推动，之后得到了左边的方块，可以看到，他缺少明显的角点。同样的处理过程通过X形与方形元素得到重复。这两个元素结构是先前元素的旋转版本，捕获的将是45°旋转后的角点。最后，对两次过程的结果做差值，提取角点特征。代码可以参考参考文献3.形态学滤波之图象处理一般腐蚀操作对二值图进行处理，腐蚀操作如下图，中心位置的像素点是否与周围领域的像素点颜色一样（即是否是白色点，即值是否为255），若一致，则保留，不一致则该点变为黑色（值即为0）opencv中的腐蚀操作：CVAPI(void) cvErode( const CvArr* src, CvArr* dst, IplConvKernel* element CV_DEFAULT(NULL), int iterations CV_DEFAULT(1) );前两个参数比较熟悉，第三个参数是用于传递模板的信息，默认是（NULL），即为3*3的模板，第四个参数是迭代的次数（即该腐蚀操作做几次）；opencv中的膨胀操作其实就是腐蚀的反操作：CVAPI(void) cvDilate( const CvArr* src, CvArr* dst, IplConvKernel* element CV_DEFAULT(NULL), int iterations CV_DEFAULT(1) );测试代码：#include “stdafx.h”#include “cv.h”#include “highgui.h”int main(){ IplImage *img= cvLoadImage(\"C:/fu.jpg\");//读取图片 cvNamedWindow(\"Example1\",CV_WINDOW_AUTOSIZE); cvNamedWindow(\"Example2\",CV_WINDOW_AUTOSIZE); cvNamedWindow(\"Example3\",CV_WINDOW_AUTOSIZE); cvShowImage(\"Example1\",img);//在Example1显示图片 // cvCopy(img,temp); IplImage* temp=cvCreateImage( //创建一个size为image,三通道8位的彩色图 cvGetSize(img), IPL_DEPTH_8U, 3 ); cvErode(img,temp,0,1);//腐蚀 cvShowImage(\"Example2\",temp); cvDilate(img,temp,0,1);//膨胀 cvShowImage(\"Example3\",temp); cvWaitKey(0);//暂停用于显示图片 cvReleaseImage(&img);//释放img所指向的内存空间并且 cvDestroyWindow(\"Example1\"); cvDestroyWindow(\"Example2\"); cvDestroyWindow(\"Example3\"); return 0; }以上都是在模板3*3的情况下处理的，要是我们期望使用自己定义的模板时候，就需要自己做模板。CVAPI(IplConvKernel*) cvCreateStructuringElementEx( int cols, int rows, int anchor_x, int anchor_y, int shape, int* values CV_DEFAULT(NULL) );前两个参数是定义模板的大小，后两个参数是参考点的坐标（比如默认3*3模板的参考点坐标是2*2），第五个参数是模板的类型(可以是矩形，十字形，椭圆形，甚至是用户自己定义形状)，最后一个参数是在使用自自定义形状的时候，通过value传递模板的形状。模板的类型：CVAPI(void) cvReleaseStructuringElement( IplConvKernel** element ); //释放模板所占用的内存自定义5*5,参考点（3,3）的矩形模板的测试代码：#include \"stdafx.h\" #include \"cv.h\" #include \"highgui.h\" int main(){ IplImage *img= cvLoadImage(\"C:/fu.jpg\");//读取图片 cvNamedWindow(\"Example1\",CV_WINDOW_AUTOSIZE); cvNamedWindow(\"Example2\",CV_WINDOW_AUTOSIZE); cvNamedWindow(\"Example3\",CV_WINDOW_AUTOSIZE); cvShowImage(\"Example1\",img);//在Example1显示图片 // cvCopy(img,temp); IplImage* temp=cvCreateImage( //创建一个size为image,三通道8位的彩色图 cvGetSize(img), IPL_DEPTH_8U, 3 ); IplConvKernel * myModel; myModel=cvCreateStructuringElementEx( //自定义5*5,参考点（3,3）的矩形模板 5,5,2,2,CV_SHAPE_RECT ); cvErode(img,temp,myModel,1); cvShowImage(\"Example2\",temp); cvDilate(img,temp,myModel,1); cvShowImage(\"Example3\",temp); cvWaitKey(0);//暂停用于显示图片 cvReleaseStructuringElement(&myModel); cvReleaseImage(&img);//释放img所指向的内存空间并且 cvDestroyWindow(\"Example1\"); cvDestroyWindow(\"Example2\"); cvDestroyWindow(\"Example3\"); return 0; }效果图：形态学滤波之信号处理数学形态学的方法可以理解为一个具有一定直径的小球滚过一段特定的路径,各种信号可以看作是路径上的小坑。由于所有的噪声都有个共同特征一一高频低峰（它们构成非常复杂,由各种混合在眼电信号中的其他成份或眼球快速、微小、空间大小不超过1“的运动导致),这些小坑的径长明显小于小球直径,因此小球球心的滚动轨迹不会受噪声的干扰一一小球球心滚动轨迹即可以看成数学形态学处理后的信号。本系统中用到的数学形态学算子包括腐蚀运算、膨胀运算、开操作、闭操作。腐蚀的最简单的应用是从图中消除不相关的细节，而膨胀的最简单的应用是将裂缝桥接起来。开运算是先腐蚀再膨胀，开运算一般断开狭窄的间断和消除细的突出物，而闭操作通常消弥狭窄的间断和长细的鸿沟，消除小的孔洞，并填充轮廓线中的断裂。开运算与闭运算的结合使用能使作用对象的轮廓变得光滑。#include <stdio.h> #include <fcntl.h> //#include <sys/types.h> //#include <sys/stats.h> #include <time.h> #define N 5 //结构元素。大小设置根据滤除波形一个周期中点个数来定。例如：采样率250，滤除50hz， //因为50hz一个周期中有5个点，所以N设为5。 float x[3*N+2]={0.0}; void openoperate(float input[],float dilation[]) //开运算：先腐蚀再膨胀 { int i,k,t; float tmp; t=2*N+3; float erosion[2*N+3]; for(k=0;k<t;k++){ tmp=input[k]; for(i=k+1;i<k+N;i++){ if(tmp>input[i]) tmp=input[i]; } erosion[k]=tmp; } t=t-3; for(k=0;k<t;k++){ tmp=erosion[k]; for(i=k+1;i<k+N;i++){ if(tmp<erosion[i]) tmp=erosion[i]; } dilation[k]=tmp; } } float closeoperate(float input[]) //闭运算：先膨胀再腐蚀 { int i,k,t; float tmp; t=N*2-1; float dilation[N]; for(k=0;k<N;k++){ tmp=input[k]; for(i=k+1;i<k+N;i++){ if(tmp<input[i]) tmp=input[i]; } dilation[k]=tmp; } tmp=dilation[0]; for(k=1;k<N;k++){ if(tmp>dilation[k]) tmp=dilation[k]; } return tmp; } int main() { clock_t start,end; double duration; FILE *fd,*m_fd; float buffer; float filter_data; float openresult[2*N-1]; m_fd=fopen(\"D:/data.txt\",\"r+\"); if(m_fd==NULL){ perror(\"open error!\"); return -1; } fd=fopen(\"D:/data1.txt\",\"w+\"); if(fd==NULL){ perror(\"open error!\"); return -1; } start=clock(); while(fscanf(m_fd,\"%f\",&buffer)!=EOF){ x[3*N+1]=buffer; openoperate(x,openresult); filter_data=closeoperate(openresult); fprintf(fd,\"%f \",filter_data); for(int y=0;y<3*N+1;y++) x[y]=x[y+1]; } end=clock(); duration=(double)(end-start)/CLOCKS_PER_SEC; printf(\"%f seconds\\n\",duration); fclose(fd); fclose(m_fd); return 0; }结果如下图所示，对有干扰原始眼电数据进行形态学滤波处理，其中结构元素17个，窗宽5.有图可以看到，形态学对于尖峰的滤波效果特别明显。图二，将形态学滤波算法加入Qt中，对眼电进行实时滤波处理，效果如图：【引】这部分内容主要来自参考文献2总结以前认为形态学滤波只在图像处理中有所应用，现在看了参考文献2，才发觉自己实在有点固步自封，对知识的来龙去脉掌握的不够清楚，一言以蔽之，囫囵吞枣，并不清楚思考方向，而只掌握具体的技巧细节，不注重顶层设计，所以才会有此感慨。在进行数据处理中，常常用到的滤波方法有中值滤波，均值滤波，FIR，IIR，卡尔曼滤波，自适应滤波等。而很多知识都是相符相通的，切不可死学死记。参考文献：1. http://blog.csdn.net/thefutureisour/article/details/75748192. http://m.blog.csdn.net/blog/gylltq/337993473. OpenCV 2 计算机视觉编程手册2015-11-28 学习笔记 张朋艺"}
{"content2":"计算机视觉开源代码合集大部分转载自博客园大神：http://www.cnblogs.com/einyboy/p/3594432.html ，并进行了部分更新修改~目录如下：1.特征提取Feature Extraction; 2. 图像分割Image Segmentation; 3. 目标检测Object Detection; 4. 显著性检测Saliency Detection;5. 图像分类、聚类Image Classification, Clustering; 6. 抠图Image Matting;7. 目标跟踪Object Tracking; 8. Kinect; 9. 3D相关;10. 机器学习算法; 11. 目标、行为识别Object, Action Recognition;12. 图像处理; 13. 一些实用工具; 14. 人手及指尖检测与识别;15. 场景解释; 16. 光流Optical flow; 17. 图像检索Image Retrieval;18. 马尔科夫随机场Markov Random Fields; 19. 运动检测Motion detection;20. database.一、特征提取Feature Extraction：SIFT [1] [Demo program][SIFT Library] [VLFeat]PCA-SIFT [2] [Project]Affine-SIFT [3] [Project]SURF [4] [OpenSURF] [Matlab Wrapper]Affine Covariant Features [5] [Oxford project]MSER [6] [Oxford project] [VLFeat]Geometric Blur [7] [Code]Local Self-Similarity Descriptor [8] [Oxford implementation]Global and Efficient Self-Similarity [9] [Code]Histogram of Oriented Graidents [10] [INRIA Object Localization Toolkit] [OLT toolkit for Windows]GIST [11] [Project]Shape Context [12] [Project]Color Descriptor [13] [Project]Pyramids of Histograms of Oriented Gradients [Code]Space-Time Interest Points (STIP) [14][Project] [Code]Boundary Preserving Dense Local Regions [15][Project]Weighted Histogram[Code]Histogram-based Interest Points Detectors[Paper][Code]An OpenCV - C++ implementation of Local Self Similarity Descriptors [Project]Fast Sparse Representation with Prototypes[Project]Corner Detection [Project]AGAST Corner Detector: faster than FAST and even FAST-ER[Project]Real-time Facial Feature Detection using Conditional Regression Forests[Project]Global and Efficient Self-Similarity for Object Classification and Detection[code]WαSH: Weighted α-Shapes for Local Feature Detection[Project]HOG[Project]Online Selection of Discriminative Tracking Features[Project]二、图像分割Image Segmentation：Normalized Cut [1] [Matlab code]Gerg Mori’ Superpixel code [2] [Matlab code]Efficient Graph-based Image Segmentation [3] [C++ code] [Matlab wrapper]Mean-Shift Image Segmentation [4] [EDISON C++ code] [Matlab wrapper]OWT-UCM Hierarchical Segmentation [5] [Resources]Turbepixels [6] [Matlab code 32bit] [Matlab code 64bit] [Updated code]Quick-Shift [7] [VLFeat]SLIC Superpixels [8] [Project]Segmentation by Minimum Code Length [9] [Project]Biased Normalized Cut [10] [Project]Segmentation Tree [11-12] [Project]Entropy Rate Superpixel Segmentation [13] [Code]Fast Approximate Energy Minimization via Graph Cuts[Paper][Code]Efﬁcient Planar Graph Cuts with Applications in Computer Vision[Paper][Code]Isoperimetric Graph Partitioning for Image Segmentation[Paper][Code]Random Walks for Image Segmentation[Paper][Code]Blossom V: A new implementation of a minimum cost perfect matching algorithm[Code]An Experimental Comparison of Min-Cut/Max-Flow Algorithms for Energy Minimization in Computer Vision[Paper][Code]Geodesic Star Convexity for Interactive Image Segmentation[Project]Contour Detection and Image Segmentation Resources[Project][Code]Biased Normalized Cuts[Project]Max-flow/min-cut[Project]Chan-Vese Segmentation using Level Set[Project]A Toolbox of Level Set Methods[Project]Re-initialization Free Level Set Evolution via Reaction Diffusion[Project]Improved C-V active contour model[Paper][Code]A Variational Multiphase Level Set Approach to Simultaneous Segmentation and Bias Correction[Paper][Code]Level Set Method Research by Chunming Li[Project]ClassCut for Unsupervised Class Segmentation[code]SEEDS: Superpixels Extracted via Energy-Driven Sampling [Project][other]三、目标检测Object Detection：A simple object detector with boosting [Project]INRIA Object Detection and Localization Toolkit [1] [Project]Discriminatively Trained Deformable Part Models [2] [Project]Cascade Object Detection with Deformable Part Models [3] [Project]Poselet [4] [Project]Implicit Shape Model [5] [Project]Viola and Jones’s Face Detection [6] [Project]Bayesian Modelling of Dyanmic Scenes for Object Detection[Paper][Code]Hand detection using multiple proposals[Project]Color Constancy, Intrinsic Images, and Shape Estimation[Paper][Code]Discriminatively trained deformable part models[Project]Gradient Response Maps for Real-Time Detection of Texture-Less Objects: LineMOD [Project]Image Processing On Line[Project]Robust Optical Flow Estimation[Project]Where's Waldo: Matching People in Images of Crowds[Project]Scalable Multi-class Object Detection[Project]Class-Specific Hough Forests for Object Detection[Project]Deformed Lattice Detection In Real-World Images[Project]Discriminatively trained deformable part models[Project]四、显著性检测Saliency Detection：Itti, Koch, and Niebur’ saliency detection [1] [Matlab code]Frequency-tuned salient region detection [2] [Project]Saliency detection using maximum symmetric surround [3] [Project]Attention via Information Maximization [4] [Matlab code]Context-aware saliency detection [5] [Matlab code]Graph-based visual saliency [6] [Matlab code]Saliency detection: A spectral residual approach. [7] [Matlab code]Segmenting salient objects from images and videos. [8] [Matlab code]Saliency Using Natural statistics. [9] [Matlab code]Discriminant Saliency for Visual Recognition from Cluttered Scenes. [10] [Code]Learning to Predict Where Humans Look [11] [Project]Global Contrast based Salient Region Detection [12] [Project]Bayesian Saliency via Low and Mid Level Cues[Project]Top-Down Visual Saliency via Joint CRF and Dictionary Learning[Paper][Code]Saliency Detection: A Spectral Residual Approach[Code]五、图像分类、聚类Image Classification, ClusteringPyramid Match [1] [Project]Spatial Pyramid Matching [2] [Code]Locality-constrained Linear Coding [3] [Project] [Matlab code]Sparse Coding [4] [Project] [Matlab code]Texture Classification [5] [Project]Multiple Kernels for Image Classification [6] [Project]Feature Combination [7] [Project]SuperParsing [Code]Large Scale Correlation Clustering Optimization[Matlab code]Detecting and Sketching the Common[Project]Self-Tuning Spectral Clustering[Project][Code]User Assisted Separation of Reflections from a Single Image Using a Sparsity Prior[Paper][Code]Filters for Texture Classification[Project]Multiple Kernel Learning for Image Classification[Project]SLIC Superpixels[Project]六、抠图Image MattingA Closed Form Solution to Natural Image Matting [Code]Spectral Matting [Project]Learning-based Matting [Code]七、目标跟踪Object Tracking：A Forest of Sensors - Tracking Adaptive Background Mixture Models [Project]Object Tracking via Partial Least Squares Analysis[Paper][Code]Robust Object Tracking with Online Multiple Instance Learning[Paper][Code]Online Visual Tracking with Histograms and Articulating Blocks[Project]Incremental Learning for Robust Visual Tracking[Project]Real-time Compressive Tracking[Project]Robust Object Tracking via Sparsity-based Collaborative Model[Project]Visual Tracking via Adaptive Structural Local Sparse Appearance Model[Project]Online Discriminative Object Tracking with Local Sparse Representation[Paper][Code]Superpixel Tracking[Project]Learning Hierarchical Image Representation with Sparsity, Saliency and Locality[Paper][Code]Online Multiple Support Instance Tracking [Paper][Code]Visual Tracking with Online Multiple Instance Learning[Project]Object detection and recognition[Project]Compressive Sensing Resources[Project]Robust Real-Time Visual Tracking using Pixel-Wise Posteriors[Project]Tracking-Learning-Detection[Project][OpenTLD/C++ Code]the HandVu：vision-based hand gesture interface[Project]Learning Probabilistic Non-Linear Latent Variable Models for Tracking Complex Activities[Project]Visual Tracking with Fully Convolutional Networks[Paper]STCT: Sequentially Training Convolutional Networks for Visual Tracking[Paper]八、Kinect：Kinect toolbox[Project]OpenNI[Project]zouxy09 CSDN Blog[Resource]FingerTracker 手指跟踪[code]九、3D相关：3D Reconstruction of a Moving Object[Paper] [Code]Shape From Shading Using Linear Approximation[Code]Combining Shape from Shading and Stereo Depth Maps[Project][Code]Shape from Shading: A Survey[Paper][Code]A Spatio-Temporal Descriptor based on 3D Gradients (HOG3D)[Project][Code]Multi-camera Scene Reconstruction via Graph Cuts[Paper][Code]A Fast Marching Formulation of Perspective Shape from Shading under Frontal Illumination[Paper][Code]Reconstruction:3D Shape, Illumination, Shading, Reflectance, Texture[Project]Monocular Tracking of 3D Human Motion with a Coordinated Mixture of Factor Analyzers[Code]Learning 3-D Scene Structure from a Single Still Image[Project]十、机器学习算法：Matlab class for computing Approximate Nearest Nieghbor (ANN) [Matlab class providing interface toANN library]Random Sampling[code]Probabilistic Latent Semantic Analysis (pLSA)[Code]FASTANN and FASTCLUSTER for approximate k-means (AKM)[Project]Fast Intersection / Additive Kernel SVMs[Project]SVM[Code]Ensemble learning[Project]Deep Learning[Net]Deep Learning Methods for Vision[Project]Neural Network for Recognition of Handwritten Digits[Project]Training a deep autoencoder or a classifier on MNIST digits[Project]THE MNIST DATABASE of handwritten digits[Project]Ersatz：deep neural networks in the cloud[Project]Deep Learning [Project]sparseLM : Sparse Levenberg-Marquardt nonlinear least squares in C/C++[Project]Weka 3: Data Mining Software in Java[Project]Invited talk \"A Tutorial on Deep Learning\" by Dr. Kai Yu (余凯)[Video]CNN - Convolutional neural network class[Matlab Tool]Yann LeCun's Publications[Wedsite]LeNet-5, convolutional neural networks[Project]Training a deep autoencoder or a classifier on MNIST digits[Project]Deep Learning 大牛Geoffrey E. Hinton's HomePage[Website]Multiple Instance Logistic Discriminant-based Metric Learning (MildML) and Logistic Discriminant-based Metric Learning (LDML)[Code]Sparse coding simulation software[Project]Visual Recognition and Machine Learning Summer School[Software]十一、目标、行为识别Object, Action Recognition：Action Recognition by Dense Trajectories[Project][Code]Action Recognition Using a Distributed Representation of Pose and Appearance[Project]Recognition Using Regions[Paper][Code]2D Articulated Human Pose Estimation[Project]Fast Human Pose Estimation Using Appearance and Motion via Multi-Dimensional Boosting Regression[Paper][Code]Estimating Human Pose from Occluded Images[Paper][Code]Quasi-dense wide baseline matching[Project]ChaLearn Gesture Challenge: Principal motion: PCA-based reconstruction of motion histograms[Project]Real Time Head Pose Estimation with Random Regression Forests[Project]2D Action Recognition Serves 3D Human Pose Estimation[Project]A Hough Transform-Based Voting Framework for Action Recognition[Project]Motion Interchange Patterns for Action Recognition in Unconstrained Videos[Project]2D articulated human pose estimation software[Project]Learning and detecting shape models [code]Progressive Search Space Reduction for Human Pose Estimation[Project]Learning Non-Rigid 3D Shape from 2D Motion[Project]十二、图像处理：Distance Transforms of Sampled Functions[Project]The Computer Vision Homepage[Project]Efficient appearance distances between windows[code]Image Exploration algorithm[code]Motion Magnification 运动放大 [Project]Bilateral Filtering for Gray and Color Images 双边滤波器 [Project]A Fast Approximation of the Bilateral Filter using a Signal Processing Approach [Project]十三、一些实用工具：EGT: a Toolbox for Multiple View Geometry and Visual Servoing[Project] [Code]a development kit of matlab mex functions for OpenCV library[Project]Fast Artificial Neural Network Library[Project]十四、人手及指尖检测与识别：finger-detection-and-gesture-recognition [Code]Hand and Finger Detection using JavaCV[Project]Hand and fingers detection[Code]十五、场景解释：Nonparametric Scene Parsing via Label Transfer [Project]十六、光流Optical flow：High accuracy optical flow using a theory for warping [Project]Dense Trajectories Video Description [Project]SIFT Flow: Dense Correspondence across Scenes and its Applications[Project]KLT: An Implementation of the Kanade-Lucas-Tomasi Feature Tracker [Project]Tracking Cars Using Optical Flow[Project]Secrets of optical flow estimation and their principles[Project]implmentation of the Black and Anandan dense optical flow method[Project]Optical Flow Computation[Project]Beyond Pixels: Exploring New Representations and Applications for Motion Analysis[Project]A Database and Evaluation Methodology for Optical Flow[Project]optical flow relative[Project]Robust Optical Flow Estimation [Project]optical flow[Project]十七、图像检索Image Retrieval：Semi-Supervised Distance Metric Learning for Collaborative Image Retrieval [Paper][code]十八、马尔科夫随机场Markov Random Fields：Markov Random Fields for Super-Resolution [Project]A Comparative Study of Energy Minimization Methods for Markov Random Fields with Smoothness-Based Priors [Project]十九、运动检测Motion detection：Moving Object Extraction, Using Models or Analysis of Regions [Project]Background Subtraction: Experiments and Improvements for ViBe [Project]A Self-Organizing Approach to Background Subtraction for Visual Surveillance Applications [Project]changedetection.net: A new change detection benchmark dataset[Project]ViBe - a powerful technique for background detection and subtraction in video sequences[Project]Background Subtraction Program[Project]Motion Detection Algorithms[Project]Stuttgart Artificial Background Subtraction Dataset[Project]Object Detection, Motion Estimation, and Tracking[Project]二十、database：AttributesAnimals with Attributes – 30,475 images of 50 animals classes with 6 pre-extracted feature representations for each image.aYahoo and aPascal – Attribute annotations for images collected from Yahoo and Pascal VOC 2008.FaceTracer – 15,000 faces annotated with 10 attributes and fiducial points.PubFig – 58,797 face images of 200 people with 73 attribute classifier outputs.LFW – 13,233 face images of 5,749 people with 73 attribute classifier outputs.Human Attributes – 8,000 people with annotated attributes. Check also this link for another dataset of human attributes.SUN Attribute Database – Large-scale scene attribute database with a taxonomy of 102 attributes.ImageNet Attributes – Variety of attribute labels for the ImageNet dataset.Relative attributes – Data for OSR and a subset of PubFig datasets. Check also this link for the WhittleSearch data.Attribute Discovery Dataset – Images of shopping categories associated with textual descriptions.Fine-grained Visual CategorizationCaltech-UCSD Birds Dataset – Hundreds of bird categories with annotated parts and attributes.Stanford Dogs Dataset – 20,000 images of 120 breeds of dogs from around the world.Oxford-IIIT Pet Dataset – 37 category pet dataset with roughly 200 images for each class. Pixel level trimap segmentation is included.Leeds Butterfly Dataset – 832 images of 10 species of butterflies.Oxford Flower Dataset – Hundreds of flower categories.Face DetectionFDDB – UMass face detection dataset and benchmark (5,000+ faces)CMU/MIT – Classical face detection dataset.Face RecognitionFace Recognition Homepage – Large collection of face recognition datasets.LFW – UMass unconstrained face recognition dataset (13,000+ face images).NIST Face Homepage – includes face recognition grand challenge (FRGC), vendor tests (FRVT) and others.CMU Multi-PIE – contains more than 750,000 images of 337 people, with 15 different views and 19 lighting conditions.FERET – Classical face recognition dataset.Deng Cai’s face dataset in Matlab Format – Easy to use if you want play with simple face datasets including Yale, ORL, PIE, and Extended Yale B.SCFace – Low-resolution face dataset captured from surveillance cameras.Handwritten DigitsMNIST – large dataset containing a training set of 60,000 examples, and a test set of 10,000 examples.Pedestrian DetectionCaltech Pedestrian Detection Benchmark – 10 hours of video taken from a vehicle,350K bounding boxes for about 2.3K unique pedestrians.INRIA Person Dataset – Currently one of the most popular pedestrian detection datasets.ETH Pedestrian Dataset – Urban dataset captured from a stereo rig mounted on a stroller.TUD-Brussels Pedestrian Dataset – Dataset with image pairs recorded in an crowded urban setting with an onboard camera.PASCAL Human Detection – One of 20 categories in PASCAL VOC detection challenges.USC Pedestrian Dataset – Small dataset captured from surveillance cameras.Generic Object RecognitionImageNet – Currently the largest visual recognition dataset in terms of number of categories and images.Tiny Images – 80 million 32x32 low resolution images.Pascal VOC – One of the most influential visual recognition datasets.Caltech 101 / Caltech 256 – Popular image datasets containing 101 and 256 object categories, respectively.MIT LabelMe – Online annotation tool for building computer vision databases.Scene RecognitionMIT SUN Dataset – MIT scene understanding dataset.UIUC Fifteen Scene Categories – Dataset of 15 natural scene categories.Feature Detection and DescriptionVGG Affine Dataset – Widely used dataset for measuring performance of feature detection and description. CheckVLBenchmarks for an evaluation framework.Action RecognitionBenchmarking Activity Recognition – CVPR 2012 tutorial covering various datasets for action recognition.RGBD RecognitionRGB-D Object Dataset – Dataset containing 300 common household objectsReference:[1]: http://rogerioferis.com/VisualRecognitionAndSearch2014/Resources.html[2]: http://www.cnblogs.com/einyboy/p/3594432.html 一、特征提取Feature Extraction：SIFT [1] [Demo program][SIFT Library] [VLFeat]PCA-SIFT [2] [Project]Affine-SIFT [3] [Project]SURF [4] [OpenSURF] [Matlab Wrapper]Affine Covariant Features [5] [Oxford project]MSER [6] [Oxford project] [VLFeat]Geometric Blur [7] [Code]Local Self-Similarity Descriptor [8] [Oxford implementation]Global and Efficient Self-Similarity [9] [Code]Histogram of Oriented Graidents [10] [INRIA Object Localization Toolkit] [OLT toolkit for Windows]GIST [11] [Project]Shape Context [12] [Project]Color Descriptor [13] [Project]Pyramids of Histograms of Oriented Gradients [Code]Space-Time Interest Points (STIP) [14][Project] [Code]Boundary Preserving Dense Local Regions [15][Project]Weighted Histogram[Code]Histogram-based Interest Points Detectors[Paper][Code]An OpenCV - C++ implementation of Local Self Similarity Descriptors [Project]Fast Sparse Representation with Prototypes[Project]Corner Detection [Project]AGAST Corner Detector: faster than FAST and even FAST-ER[Project]Real-time Facial Feature Detection using Conditional Regression Forests[Project]Global and Efficient Self-Similarity for Object Classification and Detection[code]WαSH: Weighted α-Shapes for Local Feature Detection[Project]HOG[Project]Online Selection of Discriminative Tracking Features[Project]二、图像分割Image Segmentation：Normalized Cut [1] [Matlab code]Gerg Mori’ Superpixel code [2] [Matlab code]Efficient Graph-based Image Segmentation [3] [C++ code] [Matlab wrapper]Mean-Shift Image Segmentation [4] [EDISON C++ code] [Matlab wrapper]OWT-UCM Hierarchical Segmentation [5] [Resources]Turbepixels [6] [Matlab code 32bit] [Matlab code 64bit] [Updated code]Quick-Shift [7] [VLFeat]SLIC Superpixels [8] [Project]Segmentation by Minimum Code Length [9] [Project]Biased Normalized Cut [10] [Project]Segmentation Tree [11-12] [Project]Entropy Rate Superpixel Segmentation [13] [Code]Fast Approximate Energy Minimization via Graph Cuts[Paper][Code]Efﬁcient Planar Graph Cuts with Applications in Computer Vision[Paper][Code]Isoperimetric Graph Partitioning for Image Segmentation[Paper][Code]Random Walks for Image Segmentation[Paper][Code]Blossom V: A new implementation of a minimum cost perfect matching algorithm[Code]An Experimental Comparison of Min-Cut/Max-Flow Algorithms for Energy Minimization in Computer Vision[Paper][Code]Geodesic Star Convexity for Interactive Image Segmentation[Project]Contour Detection and Image Segmentation Resources[Project][Code]Biased Normalized Cuts[Project]Max-flow/min-cut[Project]Chan-Vese Segmentation using Level Set[Project]A Toolbox of Level Set Methods[Project]Re-initialization Free Level Set Evolution via Reaction Diffusion[Project]Improved C-V active contour model[Paper][Code]A Variational Multiphase Level Set Approach to Simultaneous Segmentation and Bias Correction[Paper][Code]Level Set Method Research by Chunming Li[Project]ClassCut for Unsupervised Class Segmentation[code]SEEDS: Superpixels Extracted via Energy-Driven Sampling [Project][other]三、目标检测Object Detection：A simple object detector with boosting [Project]INRIA Object Detection and Localization Toolkit [1] [Project]Discriminatively Trained Deformable Part Models [2] [Project]Cascade Object Detection with Deformable Part Models [3] [Project]Poselet [4] [Project]Implicit Shape Model [5] [Project]Viola and Jones’s Face Detection [6] [Project]Bayesian Modelling of Dyanmic Scenes for Object Detection[Paper][Code]Hand detection using multiple proposals[Project]Color Constancy, Intrinsic Images, and Shape Estimation[Paper][Code]Discriminatively trained deformable part models[Project]Gradient Response Maps for Real-Time Detection of Texture-Less Objects: LineMOD [Project]Image Processing On Line[Project]Robust Optical Flow Estimation[Project]Where's Waldo: Matching People in Images of Crowds[Project]Scalable Multi-class Object Detection[Project]Class-Specific Hough Forests for Object Detection[Project]Deformed Lattice Detection In Real-World Images[Project]Discriminatively trained deformable part models[Project]四、显著性检测Saliency Detection：Itti, Koch, and Niebur’ saliency detection [1] [Matlab code]Frequency-tuned salient region detection [2] [Project]Saliency detection using maximum symmetric surround [3] [Project]Attention via Information Maximization [4] [Matlab code]Context-aware saliency detection [5] [Matlab code]Graph-based visual saliency [6] [Matlab code]Saliency detection: A spectral residual approach. [7] [Matlab code]Segmenting salient objects from images and videos. [8] [Matlab code]Saliency Using Natural statistics. [9] [Matlab code]Discriminant Saliency for Visual Recognition from Cluttered Scenes. [10] [Code]Learning to Predict Where Humans Look [11] [Project]Global Contrast based Salient Region Detection [12] [Project]Bayesian Saliency via Low and Mid Level Cues[Project]Top-Down Visual Saliency via Joint CRF and Dictionary Learning[Paper][Code]Saliency Detection: A Spectral Residual Approach[Code]五、图像分类、聚类Image Classification, ClusteringPyramid Match [1] [Project]Spatial Pyramid Matching [2] [Code]Locality-constrained Linear Coding [3] [Project] [Matlab code]Sparse Coding [4] [Project] [Matlab code]Texture Classification [5] [Project]Multiple Kernels for Image Classification [6] [Project]Feature Combination [7] [Project]SuperParsing [Code]Large Scale Correlation Clustering Optimization[Matlab code]Detecting and Sketching the Common[Project]Self-Tuning Spectral Clustering[Project][Code]User Assisted Separation of Reflections from a Single Image Using a Sparsity Prior[Paper][Code]Filters for Texture Classification[Project]Multiple Kernel Learning for Image Classification[Project]SLIC Superpixels[Project]六、抠图Image MattingA Closed Form Solution to Natural Image Matting [Code]Spectral Matting [Project]Learning-based Matting [Code]七、目标跟踪Object Tracking：A Forest of Sensors - Tracking Adaptive Background Mixture Models [Project]Object Tracking via Partial Least Squares Analysis[Paper][Code]Robust Object Tracking with Online Multiple Instance Learning[Paper][Code]Online Visual Tracking with Histograms and Articulating Blocks[Project]Incremental Learning for Robust Visual Tracking[Project]Real-time Compressive Tracking[Project]Robust Object Tracking via Sparsity-based Collaborative Model[Project]Visual Tracking via Adaptive Structural Local Sparse Appearance Model[Project]Online Discriminative Object Tracking with Local Sparse Representation[Paper][Code]Superpixel Tracking[Project]Learning Hierarchical Image Representation with Sparsity, Saliency and Locality[Paper][Code]Online Multiple Support Instance Tracking [Paper][Code]Visual Tracking with Online Multiple Instance Learning[Project]Object detection and recognition[Project]Compressive Sensing Resources[Project]Robust Real-Time Visual Tracking using Pixel-Wise Posteriors[Project]Tracking-Learning-Detection[Project][OpenTLD/C++ Code]the HandVu：vision-based hand gesture interface[Project]Learning Probabilistic Non-Linear Latent Variable Models for Tracking Complex Activities[Project]Visual Tracking with Fully Convolutional Networks[Paper]STCT: Sequentially Training Convolutional Networks for Visual Tracking[Paper]八、Kinect：Kinect toolbox[Project]OpenNI[Project]zouxy09 CSDN Blog[Resource]FingerTracker 手指跟踪[code]九、3D相关：3D Reconstruction of a Moving Object[Paper] [Code]Shape From Shading Using Linear Approximation[Code]Combining Shape from Shading and Stereo Depth Maps[Project][Code]Shape from Shading: A Survey[Paper][Code]A Spatio-Temporal Descriptor based on 3D Gradients (HOG3D)[Project][Code]Multi-camera Scene Reconstruction via Graph Cuts[Paper][Code]A Fast Marching Formulation of Perspective Shape from Shading under Frontal Illumination[Paper][Code]Reconstruction:3D Shape, Illumination, Shading, Reflectance, Texture[Project]Monocular Tracking of 3D Human Motion with a Coordinated Mixture of Factor Analyzers[Code]Learning 3-D Scene Structure from a Single Still Image[Project]十、机器学习算法：Matlab class for computing Approximate Nearest Nieghbor (ANN) [Matlab class providing interface toANN library]Random Sampling[code]Probabilistic Latent Semantic Analysis (pLSA)[Code]FASTANN and FASTCLUSTER for approximate k-means (AKM)[Project]Fast Intersection / Additive Kernel SVMs[Project]SVM[Code]Ensemble learning[Project]Deep Learning[Net]Deep Learning Methods for Vision[Project]Neural Network for Recognition of Handwritten Digits[Project]Training a deep autoencoder or a classifier on MNIST digits[Project]THE MNIST DATABASE of handwritten digits[Project]Ersatz：deep neural networks in the cloud[Project]Deep Learning [Project]sparseLM : Sparse Levenberg-Marquardt nonlinear least squares in C/C++[Project]Weka 3: Data Mining Software in Java[Project]Invited talk \"A Tutorial on Deep Learning\" by Dr. Kai Yu (余凯)[Video]CNN - Convolutional neural network class[Matlab Tool]Yann LeCun's Publications[Wedsite]LeNet-5, convolutional neural networks[Project]Training a deep autoencoder or a classifier on MNIST digits[Project]Deep Learning 大牛Geoffrey E. Hinton's HomePage[Website]Multiple Instance Logistic Discriminant-based Metric Learning (MildML) and Logistic Discriminant-based Metric Learning (LDML)[Code]Sparse coding simulation software[Project]Visual Recognition and Machine Learning Summer School[Software]十一、目标、行为识别Object, Action Recognition：Action Recognition by Dense Trajectories[Project][Code]Action Recognition Using a Distributed Representation of Pose and Appearance[Project]Recognition Using Regions[Paper][Code]2D Articulated Human Pose Estimation[Project]Fast Human Pose Estimation Using Appearance and Motion via Multi-Dimensional Boosting Regression[Paper][Code]Estimating Human Pose from Occluded Images[Paper][Code]Quasi-dense wide baseline matching[Project]ChaLearn Gesture Challenge: Principal motion: PCA-based reconstruction of motion histograms[Project]Real Time Head Pose Estimation with Random Regression Forests[Project]2D Action Recognition Serves 3D Human Pose Estimation[Project]A Hough Transform-Based Voting Framework for Action Recognition[Project]Motion Interchange Patterns for Action Recognition in Unconstrained Videos[Project]2D articulated human pose estimation software[Project]Learning and detecting shape models [code]Progressive Search Space Reduction for Human Pose Estimation[Project]Learning Non-Rigid 3D Shape from 2D Motion[Project]十二、图像处理：Distance Transforms of Sampled Functions[Project]The Computer Vision Homepage[Project]Efficient appearance distances between windows[code]Image Exploration algorithm[code]Motion Magnification 运动放大 [Project]Bilateral Filtering for Gray and Color Images 双边滤波器 [Project]A Fast Approximation of the Bilateral Filter using a Signal Processing Approach [Project]十三、一些实用工具：EGT: a Toolbox for Multiple View Geometry and Visual Servoing[Project] [Code]a development kit of matlab mex functions for OpenCV library[Project]Fast Artificial Neural Network Library[Project]十四、人手及指尖检测与识别：finger-detection-and-gesture-recognition [Code]Hand and Finger Detection using JavaCV[Project]Hand and fingers detection[Code]十五、场景解释：Nonparametric Scene Parsing via Label Transfer [Project]十六、光流Optical flow：High accuracy optical flow using a theory for warping [Project]Dense Trajectories Video Description [Project]SIFT Flow: Dense Correspondence across Scenes and its Applications[Project]KLT: An Implementation of the Kanade-Lucas-Tomasi Feature Tracker [Project]Tracking Cars Using Optical Flow[Project]Secrets of optical flow estimation and their principles[Project]implmentation of the Black and Anandan dense optical flow method[Project]Optical Flow Computation[Project]Beyond Pixels: Exploring New Representations and Applications for Motion Analysis[Project]A Database and Evaluation Methodology for Optical Flow[Project]optical flow relative[Project]Robust Optical Flow Estimation [Project]optical flow[Project]十七、图像检索Image Retrieval：Semi-Supervised Distance Metric Learning for Collaborative Image Retrieval [Paper][code]十八、马尔科夫随机场Markov Random Fields：Markov Random Fields for Super-Resolution [Project]A Comparative Study of Energy Minimization Methods for Markov Random Fields with Smoothness-Based Priors [Project]十九、运动检测Motion detection：Moving Object Extraction, Using Models or Analysis of Regions [Project]Background Subtraction: Experiments and Improvements for ViBe [Project]A Self-Organizing Approach to Background Subtraction for Visual Surveillance Applications [Project]changedetection.net: A new change detection benchmark dataset[Project]ViBe - a powerful technique for background detection and subtraction in video sequences[Project]Background Subtraction Program[Project]Motion Detection Algorithms[Project]Stuttgart Artificial Background Subtraction Dataset[Project]Object Detection, Motion Estimation, and Tracking[Project]二十、database：AttributesAnimals with Attributes – 30,475 images of 50 animals classes with 6 pre-extracted feature representations for each image.aYahoo and aPascal – Attribute annotations for images collected from Yahoo and Pascal VOC 2008.FaceTracer – 15,000 faces annotated with 10 attributes and fiducial points.PubFig – 58,797 face images of 200 people with 73 attribute classifier outputs.LFW – 13,233 face images of 5,749 people with 73 attribute classifier outputs.Human Attributes – 8,000 people with annotated attributes. Check also this link for another dataset of human attributes.SUN Attribute Database – Large-scale scene attribute database with a taxonomy of 102 attributes.ImageNet Attributes – Variety of attribute labels for the ImageNet dataset.Relative attributes – Data for OSR and a subset of PubFig datasets. Check also this link for the WhittleSearch data.Attribute Discovery Dataset – Images of shopping categories associated with textual descriptions.Fine-grained Visual CategorizationCaltech-UCSD Birds Dataset – Hundreds of bird categories with annotated parts and attributes.Stanford Dogs Dataset – 20,000 images of 120 breeds of dogs from around the world.Oxford-IIIT Pet Dataset – 37 category pet dataset with roughly 200 images for each class. Pixel level trimap segmentation is included.Leeds Butterfly Dataset – 832 images of 10 species of butterflies.Oxford Flower Dataset – Hundreds of flower categories.Face DetectionFDDB – UMass face detection dataset and benchmark (5,000+ faces)CMU/MIT – Classical face detection dataset.Face RecognitionFace Recognition Homepage – Large collection of face recognition datasets.LFW – UMass unconstrained face recognition dataset (13,000+ face images).NIST Face Homepage – includes face recognition grand challenge (FRGC), vendor tests (FRVT) and others.CMU Multi-PIE – contains more than 750,000 images of 337 people, with 15 different views and 19 lighting conditions.FERET – Classical face recognition dataset.Deng Cai’s face dataset in Matlab Format – Easy to use if you want play with simple face datasets including Yale, ORL, PIE, and Extended Yale B.SCFace – Low-resolution face dataset captured from surveillance cameras.Handwritten DigitsMNIST – large dataset containing a training set of 60,000 examples, and a test set of 10,000 examples.Pedestrian DetectionCaltech Pedestrian Detection Benchmark – 10 hours of video taken from a vehicle,350K bounding boxes for about 2.3K unique pedestrians.INRIA Person Dataset – Currently one of the most popular pedestrian detection datasets.ETH Pedestrian Dataset – Urban dataset captured from a stereo rig mounted on a stroller.TUD-Brussels Pedestrian Dataset – Dataset with image pairs recorded in an crowded urban setting with an onboard camera.PASCAL Human Detection – One of 20 categories in PASCAL VOC detection challenges.USC Pedestrian Dataset – Small dataset captured from surveillance cameras.Generic Object RecognitionImageNet – Currently the largest visual recognition dataset in terms of number of categories and images.Tiny Images – 80 million 32x32 low resolution images.Pascal VOC – One of the most influential visual recognition datasets.Caltech 101 / Caltech 256 – Popular image datasets containing 101 and 256 object categories, respectively.MIT LabelMe – Online annotation tool for building computer vision databases.Scene RecognitionMIT SUN Dataset – MIT scene understanding dataset.UIUC Fifteen Scene Categories – Dataset of 15 natural scene categories.Feature Detection and DescriptionVGG Affine Dataset – Widely used dataset for measuring performance of feature detection and description. CheckVLBenchmarks for an evaluation framework.Action RecognitionBenchmarking Activity Recognition – CVPR 2012 tutorial covering various datasets for action recognition.RGBD RecognitionRGB-D Object Dataset – Dataset containing 300 common household objectsReference:[1]: http://rogerioferis.com/VisualRecognitionAndSearch2014/Resources.html[2]: http://www.cnblogs.com/einyboy/p/3594432.html"}
{"content2":"微软：计算机视觉A Beginner's Guide To Understanding Convolutional Neural Networks阮一峰：图像与滤波专知：计算机视觉位、字节和字位（bit）：最小的存储单元，可以存储 0 或 1；字节（byte）：1 byte = 8 bit；字（word）：是设计计算机时给定的自然存储单位。比如 8 位的微型计算机，1 个字长只有 8 位。"}
{"content2":"我对2019年人工智能行业发展的预测笔者研习人工智能已经有一段时间了。对于人工智能各个子分支技术领域在2019年度的发展，笔者在这里饶有兴趣的做一个初步的预测，与业界同仁们共勉与切磋。第一，人工智能基础理论研究基本不会有革命性的突破。无论是机器学习，深度学习还是计算机视觉，自然语言处理等子领域，都只会是基于现有的基础理论做进一步的细分，完善与探索。笔者认为，在2019年，人工智能基础理论研究方面，不会出现颠覆性的突破与创新。不过迁移学习，强化学习，元学习等更细分的子领域，在基础理论研究领域是新的方向，在未来几年，或许会有颠覆性的理论创新出现。第二，以七巨头为首的大公司会不断推出各种开源框架，或者完善现有的各种开源框架。比如Facebook, Google, Microsoft 以及BAT等大公司，可能会继续推出并完善机器学习/NLP/计算视觉等子领域相关的各种开源软件框架。中国的BAT三巨头以及以科大讯飞，商汤科技，云丛科技，旷视科技，依图科技等独角兽企业在人工智能行业的开源界，在2019年可能会有更大作为。第三，AI芯片领域会有新的发展与突破，各种专业领域里的AI芯片被成功研发出来并上市投入使用。美国的Intel, 英伟达等芯片巨头，在2019年度的可能会推出新的产品，以支持人工智能产业的发展，其垄断性的霸主地位在2019年度将会继续保持不可撼动的地位。中国阿里巴巴投资创建的平头哥半导体公司，地平线，比特大陆等中国芯片研发公司在2019年度可能会有大的作为，尤其是平头哥半导体公司，由于其资金雄厚，招揽了最顶尖的科技人才，值得期待。在AI芯片领域，中国与欧美等传统高科技强国，难得的几乎处在同一个起跑线上。以BAT为代表的巨头，高度重视并投入巨资研发AI芯片，是中国AI芯片能赶上美国的重要依靠！第四，自动驾驶领域，无论是在中国还是美国，在辅助驾驶方面都可能会有较大的发展。百度公司与汽车厂商的合作，在辅助驾驶领域，会有较大的发展。比如会有更多相关安装百度公司DureOS系统的汽车上路投入商业运行，尤其是在一些封闭式的环境里，比如一些技术开发区，科技园区等。在以Tesla和Uber等巨头的强势推动下，美国在自动驾驶领域有可能会有突破。但是从全球范围而言，L5级别的自动驾驶（完全自动驾驶）在2019年基本不会出现，这个需要全球人工智能业者们继续努力，克服各种困难，做更多的探索与测试。笔者认为，十年甚至更多年以后，才有可能有真正的L5级自动驾驶的汽车投产上市。第五，在中国乃至全球，将会有更多的人改行从事人工智能方面的工作。由于最近几年人工智能在全球的大热与兴起，旺盛的需求与有丰富经验的专家的极度短缺，使得人工智能方面的从业人员工资水涨船高，超出普通人的想象！一些大公司的机器学习算法工程师，毕业生年薪50万已经是白菜价；拥有顶会论文，或者专利的人工智能专业博士，毕业生年薪能达到80万。。。这些都使得很多人对人工智能行业趋之若鹜，包括笔者在内，各行各业的人在努力改行试图挤入人工智能行业，希望在这个行业里分得一杯羹。不仅仅是程序员，数据分析工程师，数据库管理员等已经非常接近人工智能的技术工种，甚至是非计算机行业从业人员，乃至文科背景的人都想改行做人工智能。在2019年，这种各个行业从业人员通过考人工智能研究生，参加相关培训班等方式，往人工智能方面靠拢或者转型的现象会继续火爆的存在。2019-1-7 写于杭州市江干区下沙。"}
{"content2":"ICCVICCV 的全称是 IEEE International Conference on Computer Vision，国际计算机视觉大会，是计算机视觉方向的三大顶级会议之一，通常每两年召开一次，2005 年 10 月曾经在北京召开。会议收录论文的内容包括：底层视觉与感知，颜色、光照与纹理处理，分割与聚合，运动与跟踪，立体视觉与运动结构重构，基于图像的建模，基于物理的建模，视觉中的统计学习，视频监控，物体、事件和场景的识别，基于视觉的图形学，图片和视频的获取，性能评估，具体应用等。ICCV是计算机视觉领域最高级别的会议，会议的论文集代表了计算机视觉领域最新的发展方向和水平。会议的收录率较低，以 2007 年为例，会议共收到论文1200余篇，接受的论文仅为244篇。会议的论文会被 EI 检索。ECCVECCV的全称是Europeon Conference on Computer Vision，两年一次，是计算机视觉三大会议（另外两个是ICCV和CVPR）之一。很明显，ECCV是一个欧洲会议，欧洲人一般比较看中理论，但是从最近一次会议来看，似乎大家也开始注重应用了，oral里面的demo非常之多，演示效果很好，让人赏心悦目、叹为观止。不过欧洲的会有一个不好，就是他们的人通常英语口音很重，有些人甚至不太会说英文，所以开会和交流的时候，稍微有些费劲。CVRPCVRP 国际计算机视觉与模式识别学术会议即International Conference on Computer VisionPattern Recognition"}
{"content2":"尽管在上篇文章中有了原文的链接，但是吸取以往的经验教训，还是全部拿来主义比较好，担心哪天原文链接就不能查看了，那岂不是让人心痛的碎了一地梨花一、仿射变换仿射变换的性质:平面上任意两条直线，经仿射变换后，仍然保持平行。仿射变换的功能，是一种二维坐标到二维坐标之间的线性变换，保持二维图形的“平直性”（straightness，即变换后直线还是直线不会打弯，圆弧还是圆弧）和“平行性”（parallelness，其实是指保二维图形间的相对位置关系不变，平行线还是平行线，相交直线的交角不变）。仿射变换可以通过一系列的原子变换的复合来实现，包括：平移（Translation）、缩放（Scale）、翻转（Flip）、旋转（Rotation）和剪切（Shear）。此类变换可以用一个3×3的矩阵来表示，其最后一行为(0, 0, 1)。该变换矩阵将原坐标(x, y)变换为新坐标(x', y')，这里原坐标和新坐标皆视为最末一行为(1)的三维列向量，原列向量左乘变换矩阵得到新的列向量：[x']     [a00 a01       a02] [x]      [a00*x+a01*y+a02][y'] = [a10 a11 a12] [y] = [a10*x+a11*y+a12][1 ]         [0    0 1] [1]      [ 1           ]仿射变换的矩阵由图像上不共线的三个点的坐标确定，看做2*3的矩阵。几种典型的仿射变换：平移变换，将每一点移动到(x+tx, y+ty)，变换矩阵为：[ 1 0 tx   ][ 0 1 ty   ][ 0 0 1 ]平移变换是一种“刚体变换”，就是不会产生形变的理想物体，平移当然不会改变二维图形的形状。同理，下面的“旋转变换”也是刚体变换，而“缩放”、“错切”都是会改变图形形状的。缩放变换，将每一点的横坐标放大（缩小）至sx倍，纵坐标放大（缩小）至sy倍，变换矩阵为：[ sx 0 0 ][ 0 sy 0 ][ 0 0 1 ]剪切变换，变换矩阵为：[ 1 shx 0 ][   shy 1 0 ][ 0     0 1 ]相当于一个横向剪切与一个纵向剪切的复合，即[ 1    0 0 ][ 1 shx 0 ][   shy 1 0 ][ 0     1     0 ][ 0    0 1 ][ 0 0     1 ]“剪切变换”又称“错切变换”，指的是类似于四边形不稳定性那种性质，街边小商店那种铁拉门都见过吧？想象一下上面铁条构成的菱形拉动的过程，那就是“错切”的过程。旋转变换原点，目标图形围绕原点顺时针旋转theta弧度，变换矩阵为：[ cos(theta) -sin(theta) 0 ][ sin(theta)     cos(theta) 0 ][    0          0          1 ]旋转变换，目标图形以(x, y)为轴心顺时针旋转theta弧度，变换矩阵为：[ cos(theta) -sin(theta) x-x*cos+y*sin][ sin(theta)     cos(theta) y-x*sin-y*cos ][    0               0           1       ]相当于两次平移变换与一次原点旋转变换的复合：[1   0   -x] [cos(theta)   -sin(theta)   0] [1   0   x][0   1   -y] [sin(theta) cos(theta)   0] [0   1   y][0   0   1 ] [     0         0        1 ] [0   0   1]opencv中的实现函数：void cvWarpAffine( const CvArr* src, CvArr* dst, const CvMat* map_matrix,int flags=CV_INTER_LINEAR+CV_WARP_FILL_OUTLIERS,CvScalar fillval=cvScalarAll(0) );实现图像的仿射变换，map_matrix是2*3的仿射矩阵，要求输入和输出图像有相同的数据类型。通过下面两个函数得到仿射矩阵：CvMat* cvGetAffineTransform( const CvPoint2D32f* src, const CvPoint2D32f* dst,CvMat* map_matrix ); //由三个不共线的点得到，这样才具有唯一性CvMat* cv2DRotationMatrix( CvPoint2D32f center, double angle, double scale,CvMat* map_matrix ); //由旋转中心，旋转角度和各向通行因子，矩阵如下：[ a b (1-a)*center.x-b*center.y ][-b a b*center.x-(1-a)*center.y ] 其中：a=scale*cos(angle), b=scale*sin(angle)void cvGetQuadrangleSubPix( const CvArr* src, CvArr* dst, const CvMat* map_matrix );实现图像的仿射变换，使用于亚像素。实现图像旋转的例子：#include \"stdafx.h\"#include \"cv.h\"#include \"highgui.h\"#include \"math.h\"int main(int argc, char* argv[]){IplImage *src = 0;IplImage *dst = 0;IplImage *dst1 = 0;/* the first command line parameter must be image file name */if ((argc == 2) && (src = cvLoadImage (argv[1], -1)) != 0){int delta = 1;int angle = 0;int opt = 0;            // 1： 旋转加缩放// 0: 仅仅旋转double factor;dst = cvCloneImage (src);dst1 = cvCloneImage (src);cvNamedWindow (\"src\", 1);cvShowImage (\"src\", src);for (;;){float m[6];// Matrix m looks like://// [ m0 m1 m2 ] ===> [ A11 A12   b1 ]// [ m3 m4 m5 ]       [ A21 A22   b2 ]//CvMat M = cvMat (2, 3, CV_32F, m);int w = src->width;int h = src->height;if (opt)           // 旋转加缩放factor = (cos (angle * CV_PI / 180.) + 1.0) * 2;else                // 仅仅旋转factor = 1;m[0] = (float) (factor * cos (-angle * 2 * CV_PI / 180.));m[1] = (float) (factor * sin (-angle * 2 * CV_PI / 180.));m[3] = -m[1];m[4] = m[0];// 将旋转中心移至图像中间m[2] = w * 0.5f;m[5] = h * 0.5f;// dst(x,y) = A * src(x,y) + bcvZero (dst);cvGetQuadrangleSubPix (src, dst, &M);cvNamedWindow (\"dst\", 1);cvShowImage (\"dst\", dst);if (cvWaitKey (1) == 27)       //ESCbreak;angle = (int) (angle + delta) % 360;}                   // for-loop}return 0;}两个函数的区别：前者要求输入和输出图像具有相同的数据类型，有更大的资源开销（因此对小图像不太合适）且部分输出图像可以保持不变。后者可以精确的从8位图像中提取四边形到浮点数缓存区中，具有比较小的系统开销，且总是全部改变输出图像的内容。实验：使用例子中的仿射矩阵使用前者进行仿射变换，效果是不一样的。对于前者：[ m0 m1 m2 ][ m3 m4 m5 ]m0 = factor * cos (angle);m1 = factor * sin (angle);m3 = - factor * sin (angle);m4 = factor * cos (angle);m2 = x * (1- factor * cos (angle)) – y * factor * sin (angle); //x * (1-m0) – y * m1m5 = y * (1- factor * cos (angle)) + x * factor * sin (angle); //y * (1-m0) + x* m1图像逆时针旋转Angle弧度。如果factor=1，图像不进行缩放，factor>1，图像会放大显示，factor<1，图像缩小显示。(m2,m5)由(x,y)确定，表示源图像以(x,y)位置为中心，对图像进行缩放和旋转。部分没有灰度值的区域的元素灰度值置0。以上矩阵可以通过函数cv2DRotationMatrix( cvPoint2D32f(x, y), angle, factor, map_matrix ); 其中，CvMat* map_matrix。直观上，先将变换后的图像保存为与源图像一样大小的，然后从左上角开始取出与目的图像一样大小的图像，作为输出结果，没有灰度值的置为0。对于后者：[ m0 m1 m2 ][ m3 m4 m5 ]m0 = factor * cos (angle); //同前m1 = factor * sin (angle); //同前m3 = - factor * sin (angle); //同前m4 = factor * cos (angle); //同前m2 = x;m5 = y;以上表示：图像顺时针旋转Angle弧度。如果factor=1，图像不进行缩放，factor>1，图像会缩小显示，factor<1，图像放大显示。(m2,m5)：表示将源图像中的(m2,m5)位置移动到新图像的中心位置，然后在新图像的中心位置处对图像进行缩放和旋转。部分没有灰度值的区域会使用附近的边界灰度值进行插值。根据输出图像的大小，直观上，输出前面旋转好后图像中心位置向外扩展的输出图像大小的图像部分，没有灰度值的部分进行插值。二、透视变换opencv中的实现函数：void cvWarpPerspective( const CvArr* src, CvArr* dst, const CvMat* map_matrix, int flags=CV_INTER_LINEAR+CV_WARP_FILL_OUTLIERS, CvScalar fillval=cvScalarAll(0));实现透视变换，map_matrix是3*3的变换矩阵，有下面函数可得到：CvMat* cvGetPerspectiveTransform( const CvPoint2D32f* src, const CvPoint2D32f* dst, CvMat* map_matrix ); //用四边形的四个点得到三、二次线性变换目前为止还没使用过。"}
{"content2":"1. 线性代数 (Linear Algebra)：我想国内的大学生都会学过这门课程，但是，未必每一位老师都能贯彻它的精要。这门学科对于Learning是必备的基础，对它的透彻掌握是必不可少的。我在科大一年级的时候就学习了这门课，后来到了香港后，又重新把线性代数读了一遍，所读的是Introduction to Linear Algebra (3rd Ed.)  by Gilbert Strang.这本书是MIT的线性代数课使用的教材，也是被很多其它大学选用的经典教材。它的难度适中，讲解清晰，重要的是对许多核心的概念讨论得比较透彻。我个人觉得，学习线性代数，最重要的不是去熟练矩阵运算和解方程的方法——这些在实际工作中MATLAB可以代劳，关键的是要深入理解几个基础而又重要的概念：子空间(Subspace)，正交(Orthogonality)，特征值和特征向量(Eigenvalues and eigenvectors)，和线性变换(Linear transform)。从我的角度看来，一本线代教科书的质量，就在于它能否给这些根本概念以足够的重视，能否把它们的联系讲清楚。Strang的这本书在这方面是做得很好的。而且，这本书有个得天独厚的优势。书的作者长期在MIT讲授线性代数课(18.06)，课程的video在MIT的Open courseware网站上有提供。有时间的朋友可以一边看着名师授课的录像，一边对照课本学习或者复习。http://ocw.mit.edu/OcwWeb/Mathematics/18-06Spring-2005/CourseHome/index.htm2. 概率和统计 (Probability and Statistics):概率论和统计的入门教科书很多，我目前也没有特别的推荐。我在这里想介绍的是一本关于多元统计的基础教科书：Applied Multivariate Statistical Analysis (5th Ed.)  by Richard A. Johnson and Dean W. Wichern这本书是我在刚接触向量统计的时候用于学习的，我在香港时做研究的基础就是从此打下了。实验室的一些同学也借用这本书学习向量统计。这本书没有特别追求数学上的深度，而是以通俗易懂的方式讲述主要的基本概念，读起来很舒服，内容也很实用。对于Linear regression, factor analysis, principal component analysis (PCA), and canonical component analysis (CCA)这些Learning中的基本方法也展开了初步的论述。之后就可以进一步深入学习贝叶斯统计和Graphical models。一本理想的书是Introduction to Graphical Models (draft version).  by M. Jordan and C. Bishop.我不知道这本书是不是已经出版了（不要和Learning in Graphical Models混淆，那是个论文集，不适合初学）。这本书从基本的贝叶斯统计模型出发一直深入到复杂的统计网络的估计和推断，深入浅出，statistical learning的许多重要方面都在此书有清楚论述和详细讲解。MIT内部可以access，至于外面，好像也是有电子版的。3. 分析 (Analysis)：我想大家基本都在大学就学过微积分或者数学分析，深度和广度则随各个学校而异了。这个领域是很多学科的基础，值得推荐的教科书莫过于Principles of Mathematical Analysis, by Walter Rudin有点老，但是绝对经典，深入透彻。缺点就是比较艰深——这是Rudin的书的一贯风格，适合于有一定基础后回头去看。在分析这个方向，接下来就是泛函分析(Functional Analysis)。Introductory Functional Analysis with Applications, by Erwin Kreyszig.适合作为泛函的基础教材，容易切入而不失全面。我特别喜欢它对于谱论和算子理论的特别关注，这对于做learning的研究是特别重要的。Rudin也有一本关于functional analysis的书，那本书在数学上可能更为深刻，但是不易于上手，所讲内容和learning的切合度不如此书。在分析这个方向，还有一个重要的学科是测度理论(Measure theory)，但是我看过的书里面目前还没有感觉有特别值得介绍的。4. 拓扑 (Topology)：在我读过的基本拓扑书各有特色，但是综合而言，我最推崇：Topology (2nd Ed.)  by James Munkres这本书是Munkres教授长期执教MIT拓扑课的心血所凝。对于一般拓扑学(General topology)有全面介绍，而对于代数拓扑(Algebraic topology)也有适度的探讨。此书不需要特别的数学知识就可以开始学习，由浅入深，从最基本的集合论概念（很多书不屑讲这个）到Nagata-Smirnov Theorem和Tychonoff theorem等较深的定理（很多书避开了这个）都覆盖了。讲述方式思想性很强，对于很多定理，除了给出证明过程和引导你思考其背后的原理脉络，很多令人赞叹的亮点——我常读得忘却饥饿，不愿释手。很多习题很有水平。5. 流形理论 (Manifold theory)：对于拓扑和分析有一定把握时，方可开始学习流形理论，否则所学只能流于浮浅。我所使用的书是Introduction to Smooth Manifolds.  by John M. Lee虽然书名有introduction这个单词，但是实际上此书涉入很深，除了讲授了基本的manifold, tangent space,bundle, sub-manifold等，还探讨了诸如纲理论(Category theory)，德拉姆上同调(De Rham cohomology)和积分流形等一些比较高级的专题。对于李群和李代数也有相当多的讨论。行文通俗而又不失严谨，不过对某些记号方式需要熟悉一下。虽然李群论是建基于平滑流形的概念之上，不过，也可能从矩阵出发直接学习李群和李代数——这种方法对于急需使用李群论解决问题的朋友可能更加实用。而且，对于一个问题从不同角度看待也利于加深理解。下面一本书就是这个方向的典范：Lie Groups, Lie Algebras, and Representations: An Elementary Introduction.  by Brian C. Hall此书从开始即从矩阵切入，从代数而非几何角度引入矩阵李群的概念。并通过定义运算的方式建立exponential mapping，并就此引入李代数。这种方式比起传统的通过“左不变向量场(Left-invariant vector field)“的方式定义李代数更容易为人所接受，也更容易揭示李代数的意义。最后，也有专门的论述把这种新的定义方式和传统方式联系起来。————————————————————————————无论是研究Vision, Learning还是其它别的学科，数学终究是根基所在。学好数学是做好研究的基石。学好数学的关键归根结底是自己的努力，但是选择一本好的书还是大有益处的。不同的人有不同的知识背景，思维习惯和研究方向，因此书的选择也因人而异，只求适合自己，不必强求一致。上面的书仅仅是从我个人角度的出发介绍的，我的阅读经历实在非常有限，很可能还有比它们更好的书（不妨也告知我一声，先说声谢谢了）。％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％Learning中的代数结构的建立Learning是一个融会多种数学于一体的领域。说起与此有关的数学学科，我们可能会迅速联想到线性代数以及建立在向量空间基础上的统计模型——事实上，主流的论文中确实在很大程度上基于它们。R^n (n-维实向量空间) 是我们在paper中见到最多的空间，它确实非常重要和实用，但是，仅仅依靠它来描述我们的世界并不足够。事实上，数学家们给我们提供了丰富得多的工具。“空间”(space)，这是一个很有意思的名词，几乎出现在所有的数学分支的基础定义之中。归纳起来，所谓空间就是指一个集合以及在上面定义的某种数学结构。关于这个数学结构的定义或者公理，就成为这个数学分支的基础，一切由此而展开。还是从我们最熟悉的空间——R^n 说起吧。大家平常使用这个空间的时候，除了线性运算，其实还用到了别的数学结构，包括度量结构和内积结构。·                   第一，它是一个拓扑空间(Topological space)。而且从拓扑学的角度看，具有非常优良的性质：Normal (implying Hausdorff and Regular), Locally Compact, Paracompact, with Countable basis, Simply connected (implying connected and path connected),Metrizable.·                   第二，它是一个度量空间(Metric space)。我们可以计算上面任意两点的距离。·                   第三，它是一个有限维向量空间(Finite dimensional space)。因此，我们可以对里面的元素进行代数运算（加法和数乘），我们还可以赋予它一组有限的基，从而可以用有限维坐标表达每个元素。·                   第四，基于度量结构和线性运算结构，可以建立起分析(Analysis)体系。我们可以对连续函数进行微分，积分，建立和求解微分方程，以及进行傅立叶变换和小波分析。·                   第五，它是一个希尔伯特空间（也就是完备的内积空间）(Hilbert space, Complete inner product space)。它有一套很方便计算的内积(inner product)结构——这个空间的度量结构其实就是从其内积结构诱导出来。更重要的，它是完备的(Complete)——代表任何一个柯西序列(Cauchy sequence)都有极限——很多人有意无意中其实用到了这个特性，不过习惯性地认为是理所当然了。·                   第六，它上面的线性映射构成的算子空间仍旧是有限维的——一个非常重要的好处就是，所有的线性映射都可以用矩阵唯一表示。特别的，因为它是有限维完备空间，它的泛函空间和它本身是同构的，也是R^n。因而，它们的谱结构，也就可以通过矩阵的特征值和特征向量获得。·                   第七，它是一个测度空间——可以计算子集的大小（面积/体积）。正因为此，我们才可能在上面建立概率分布(distribution)——这是我们接触的绝大多数连续统计模型的基础。我们可以看到，这是一个非常完美的空间，为我们的应用在数学上提供了一切的方便，在上面，我们可以理所当然地认为它具有我们希望的各种良好性质，而无须特别的证明；我们可以直接使用它的各种运算结构，而不需要从头建立；而且很多本来不一样的概念在这里变成等价的了，我们因此不再需要辨明它们的区别。以此为界，Learning的主要工作分成两个大的范畴：1.    建立一种表达形式，让它处于上面讨论的R^n空间里面。2.    获得了有限维向量表达后，建立各种代数算法或者统计模型进行分析和处理。这里只讨论第一个范畴。先看看，目前用得比较广泛的一些方法：1.    直接基于原始数据建立表达。我们关心的最终目标是一个个现实世界中的对象：一幅图片，一段语音，一篇文章，一条交易记录，等等。这些东西大部分本身没有附着一个数值向量的。为了构造一个向量表达，我们可以把传感器中记录的数值，或者别的什么方式收集的数值数据按照一定的顺序罗列出来，就形成一个向量了。如果有n个数字，就认为它们在R^n里面。不过，这在数学上有一点小问题，在大部分情况下，根据数据产生的物理原理，这些向量的值域并不能充满整个空间。比如图像的像素值一般是正值，而且在一个有界闭集之中。这带来的问题是，对它们进行线性运算很可能得到的结果会溢出正常的范围——在大部分paper中，可能只是采用某些heuristics的手段进行简单处理，或者根本不管，很少见到在数学上对此进行深入探讨的——不过如果能解决实际问题，这也是无可厚非的，毕竟不是所有的工作都需要像纯数学那样追求严谨。2.    量化(quantization)。这是在处理连续信号时被广泛采用的方式。只是习以为常，一般不提名字而已。比如一个空间信号（Vision中的image）或者时间信号，它们的domain中的值是不可数无限大的(uncountably infinite)，不要说表示为有限维向量，即使表达为无限序列也是不可能的。在这种情况下，一般在有限域内，按照一定顺序每隔一定距离取一个点来代表其周围的点，从而形成有限维的表达。这就是信号在时域或空域的量化。这样做不可避免要丢失信息。但是，由于小邻域内信号的高度相关，信息丢失的程度往往并不显著。而且，从理论上说，这相当于在频域中的低通过率。对于有限能量的连续信号，不可能在无限高的频域中依然保持足够的强度，只要采样密度足够，丢失的东西可以任意的少。除了表示信号，对于几何形体的表达也经常使用量化，比如表示curve和surface。3.    找出有限个数充分表达一个对象也许不是最困难的。不过,在其上面建立数学结构却未必了。一般来说，我们要对其进行处理，首先需要一个拓扑结构用以描述空间上的点是如何联系在一起。直接建立拓扑结构在数学上往往非常困难，也未必实用。因此，绝大部分工作采取的方式是首先建立度量结构。一个度量空间，其度量会自然地诱导出一个拓扑结构——不过，很多情况下我们似乎会无视它的存在。最简单的情况，就是使用原始向量表达的欧氏距离(Euclidean distance)作为metric。不过，由于原始表达数值的不同特性，这种方式效果一般不是特别好，未必能有效表达实际对象的相似性（或者不相似性）。因此，很多工作会有再此基础上进行度量的二次建立。方式是多种多样的，一种是寻求一个映射，把原空间的元素变换到一个新的空间，在那里欧氏距离变得更加合适。这个映射发挥的作用包括对信息进行筛选，整合，对某些部分进行加强或者抑制。这就是大部分关于feature selection，feature extraction，或者subspace learning的文章所要做的。另外一种方式，就是直接调节距离的计算方式（有些文章称之为metric learning）。这两种方式未必是不同的。如果映射是单射，那么它相当于在原空间建立了一个不同的度量。反过来，通过改变距离计算方式建立的度量在特定的条件下对应于某种映射。4.    大家可能注意到，上面提到的度量建立方法，比如欧氏距离，它需要对元素进行代数运算。对于普通的向量空间，线性运算是天然赋予的，我们无须专门建立，所以可以直接进行度量的构造——这也是大部分工作的基础。可是，有些事物其原始表达不是一个n-tuple，它可能是一个set，一个graph，或者别的什么特别的object。怎么建立代数运算呢？一种方法是直接建立。就是给这些东西定义自己的加法和数乘。这往往不是那么直接（能很容易建立的线性运算结构早已经被建立好并广泛应用了），可能需要涉及很深的数学知识，并且要有对问题本身的深入了解和数学上的洞察力。不过，一个新的代数结构一旦建立起来，其它的数学结构，包括拓扑，度量，分析，以及内积结构也随之能被自然地诱导出来，我们也就具有了对这个对象空间进行各种数学运算和操作的基础。加法和数乘看上去简单，但是如果我们对于本来不知道如何进行加法和数乘的空间建立了这两样东西，其理论上的贡献是非常大的。（一个小问题：大家常用各种graphical model，但是，每次这些model都是分别formulate，然后推导出estimation和evaluation的步骤方法。是否可能对\"the space of graphical model\"或者它的某个特定子集建立某种代数结构呢？（不一定是线性空间，比如群，环，广群， etc）从而使得它们在代数意义上统一起来，而相应的estimation或者evaluation也可以用过代数运算derive。这不是我的研究范围，也超出了我目前的能力和知识水平，只是我相信它在理论上的重要意义，留作一个远景的问题。事实上，数学中确实有一个分支叫做 Algebraic statistics 可能在探讨类似的问题，不过我现在对此了解非常有限。）5.    回到我们的正题，除了直接建立运算定义，另外一种方式就是嵌入(embedding)到某个向量空间，从而继承其运算结构为我所用。当然这种嵌入也不是乱来，它需要保持原来这些对象的某种关系。最常见的就是保距嵌入(isometric embedding)，我们首先建立度量结构（绕过向量表达，直接对两个对象的距离通过某种方法进行计算），然后把这个空间嵌入到目标空间，通常是有限维向量空间，要求保持度量不变。“嵌入”是一种在数学上应用广泛的手段，其主要目标就是通过嵌入到一个属性良好，结构丰富的空间，从而利用其某种结构或者运算体系。在拓扑学中，嵌入到metric space是对某个拓扑空间建立度量的重要手段。而在这里，我们是已有度量的情况下，通过嵌入获取线性运算的结构。除此以来，还有一种就是前些年比较热的manifold embedding，这个是通过保持局部结构的嵌入，获取全局结构，后面还会提到。6.    接下来的一个重要的代数结构，就是内积(inner product)结构。内积结构一旦建立，会直接诱导出一种性质良好的度量，就是范数(norm)，并且进而诱导出拓扑结构。一般来说，内积需要建立在线性空间的基础上，否则连一个二元运算是否是内积都无法验证。不过，kernel理论指出，对于一个空间，只要定义一个正定核(positive kernel)——一个符合正定条件的二元运算，就必然存在一个希尔伯特空间，其内积运算等效于核运算。这个结论的重要意义在于，我们可以绕开线性空间，通过首先定义kernel的方式，诱导出一个线性空间(叫做再生核希尔伯特空间 Reproducing Kernel Hilbert Space)，从而我们就自然获得我们所需要的度量结构和线性运算结构。这是kernel theory的基础。在很多教科书中，以二次核为例子，把二维空间变成三维，然后告诉大家kernel用于升维。对于这种说法，我一直认为在一定程度上是误导的。事实上，kernel的最首要意义是内积的建立（或者改造），从而诱导出更利于表达的度量和运算结构。对于一个问题而言，选择一个切合问题的kernel比起关注“升维”来得更为重要。kernel被视为非线性化的重要手段，用于处理非高斯的数据分布。这是有道理的。通过nonlinear kernel改造的内积空间，其结构和原空间的结构确实不是线性关联，从这个意义上说，它实施了非线性化。不过，我们还应该明白，它的最终目标还是要回到线性空间，新的内积空间仍旧是一个线性空间，它一旦建立，其后的运算都是线性的，因此，kernel的使用就是为了寻求一个新的线性空间，使得线性运算更加合理——非线性化的改造最终仍旧是要为线性运算服务。值得一提的是，kernelization本质上说还是一种嵌入过程：对于一个空间先建立内积结构，并且以保持内积结构不变的方式嵌入到一个高维的线性空间，从而继承其线性运算体系。7.    上面说到的都是从全局的方式建立代数结构的过程，但是那必须以某种全局结构为基础（无论预先定义的是运算，度量还是内积，都必须适用于全空间。）但是，全局结构未必存在或者适合，而局部结构往往简单方便得多。这里就形成一种策略，以局部而达全局——这就是流形(manifold)的思想，而其则根源于拓扑学。从拓扑学的角度说，流形就是一个非常优良的拓扑空间：符合Hausdorff分离公理（任何不同的两点都可以通过不相交的邻域分离），符合第二可数公理（具有可数的拓扑基），并且更重要的是，局部同胚于R^n。因此，一个正则(Regular)流形基本就具有了各种最良好的拓扑特性。而局部同胚于R^n，代表了它至少在局部上可以继承R^n的各种结构，比如线性运算和内积，从而建立分析体系。事实上，拓扑流形继承这些结构后形成的体系，正是现代流形理论研究的重点。继承了分析体系的流形，就形成了微分流形(Differential manifold)，这是现代微分几何的核心。而微分流形各点上的切空间(Tangent Space)，则获得了线性运算的体系。而进一步继承了局部内积结构的流形，则形成黎曼流形(Riemann manifold)，而流形的全局度量体系——测地距离(geodesics)正是通过对局部度量的延伸来获得。进一步的，当流行本身的拓扑结构和切空间上的线性结构发生关系——也就获得一簇拓扑关联的线性空间——向量丛(Vector bundle)。虽然manifold theory作为现代几何学的核心，是一个博大精深的领域，但是它在learning中的应用则显得非常狭窄。事实上，对于manifold，很多做learning的朋友首先反应的是ISOMAP, LLE, eigenmap之类的算法。这些都属于embedding。当然，这确实是流形理论的一个重要方面。严格来说，这要求是从原空间到其映像的微分同胚映射，因此，嵌入后的空间在局部上具有相同的分析结构，同时也获得了各种好处——全局的线性运算和度量。不过，这个概念在learning的应用中被相当程度的放宽了——微分同胚并不能被完全保证，而整个分析结构也不能被完全保持。大家更关注的是保持局部结构中的某个方面——不过这在实际应用中的折衷方案也是可以理解的。事实表明，当原空间中的数据足够密集的情况下，这些算法工作良好。Learning中流形应用的真正问题在于它被过滥地运用于稀疏空间(Sparse space)，事实上在高维空间中撒进去几千乃至几十万点，即使最相邻的几点也难称为局部了，局部的范围和全局的范围其实已经没有了根本差别，连局部的概念都立不住脚的时候，后面基于其展开的一切工作也都没有太大的意义。事实上，稀疏空间有其本身的规律和法则，通过局部形成全局的流形思想从本质上是不适合于此的。虽然，流形是一种非常美的理论，但是再漂亮的理论也需要用得其所——它应该用于解决具有密集数据分布的低维空间。至于，一些paper所报告的在高维空间（比如人脸）运用流形方法获得性能提升，其实未必是因为“流形”本身所起的作用，而很可能是其它方面的因素。8.    流形在实际应用中起重要作用的还有两个方面：一个是研究几何形体的性质（我们暂且不谈这个），还有就是它和代数结构的结合形成的李群(Lie group)和李代数(Lie algebra)。当我们研究的对象是变换本身的时候，它们构成的空间是有其特殊性的，比如所有子空间投影形成了Grassmann流形，所有的可逆线性算子，或者仿射算子，也形成各自的流形。对他们的最重要操作是变换的结合，而不是加法数乘，因此，它们上面定义的更合适的代数结构应该是群和不是线性空间。而群和微分流形的结合体——李群则成为它们最合适的描述体系——而其切空间则构成了一种加强的线性空间：李代数，用于描述其局部变化特性。李代数和李群的关系是非常漂亮的。它把变换的微变化转换成了线性空间的代数运算，使得移植传统的基于线性空间的模型和算法到李空间变得可能。而且李代数中的矩阵比起变换本身的矩阵甚至更能反映变换的特性。几何变换的李代数矩阵的谱结构就能非常方便地用于分析变换的几何特性。最后，回头总结一下关于嵌入这个应用广泛的策略，在learning中的isometry, kernel和manifold embedding都属于此范畴，它们分别通过保持原空间的度量结构，内积结构和局部结构来获得到目标（通常是向量空间）的嵌入，从而获得全局的坐标表达，线性运算和度量，进而能被各种线性算法和模型所应用。在获得这一系列好处的同时，也有值得我们注意的地方。首先，嵌入只是一种数学手段，并不能取代对问题本身的研究和分析。一种不恰当的原始结构或者嵌入策略，很多时候甚至适得其反——比如稀疏空间的流形嵌入，或者选取不恰当的kernel。另外，嵌入适合于分析，而未必适合于重建或者合成。这是因为嵌入是一个单射(injection)，目标空间不是每一个点都和原空间能有效对应的。嵌入之后的运算往往就打破了原空间施加的限制。比如两个元素即使都是从原空间映射过来，它们的和却未必有原像，这时就不能直接地回到原空间了。当然可以考虑在原空间找一个点它的映射与之最近，不过这在实际中的有效性是值得商榷的。和Learning有关的数学世界是非常广博的，我随着学习和研究的深入，越来越发现在一些我平常不注意的数学分支中有着适合于问题的结构和方法。比如，广群(groupoid)和广代数(algebroid)能克服李群和李代数在表示连续变换过程中的一些困难——这些困难困扰了我很长时间。解决问题和建立数学模型是相辅相成的，一方面，一个清晰的问题将使我们有明确的目标去寻求合适的数学结构，另一方面，对数学结构的深入理解对于指导问题的解决也是有重要作用的。对于解决一个问题来说，数学工具的选择最重要的是适合，而不是高深，但是如果在现有数学方法陷入困难的时候，寻求更高级别的数学的帮助，往往能柳暗花明。数学家长时间的努力解决的很多问题，并不都是理论游戏，他们的解决方案中很多时候蕴含着我们需要的东西，而且可能导致对更多问题的解决——但是我们需要时间去学习和发现它们。拓扑：游走于直观与抽象之间近日来，抽空再读了一遍点集拓扑(Point Set Topology)，这是我第三次重新学习这个理论了。我看电视剧和小说，极少能有兴致看第二遍，但是，对于数学，每看一次都有新的启发和收获。代数，分析，和拓扑，被称为是现代数学的三大柱石。最初读拓扑，是在两三年前，由于学习流形理论的需要。可是，随着知识的积累，发现它是很多理论的根基。可以说，没有拓扑，就没有现代意义的分析与几何。我们在各种数学分支中接触到的最基本的概念，比如，极限，连续，距离（度量），边界，路径，在现代数学中，都源于拓扑。拓扑学是一门非常奇妙的学科，它把最直观的现象和最抽象的概念联系在一起了。拓扑描述的是普遍使用的概念（比如开集，闭集，连续），我们对这些概念习以为常，理所当然地使用着，可是，真要定义它，则需要对它们本质的最深刻的洞察。数学家们经过长时间的努力，得到了这些概念的现代定义。这里面很多第一眼看上去，会感觉惊奇——怎么会定义成这个样子。首先是开集。在学习初等数学时，我们都学习开区间 (a, b)。可是，这只是在一条线上的，怎么推广到二维空间，或者更高维空间，或者别的形体上呢？最直观的想法，就是“一个不包含边界的集合”。可是，问题来了，给一个集合，何谓“边界”？在拓扑学里面，开集(Open Set)是最根本的概念，它是定义在集合运算的基础上的。它要求开集符合这样的条件：开集的任意并集和有限交集仍为开集。我最初的时候，对于这样的定义方式，确实百思不解。不过，读下去，看了和做了很多证明后，发现，这样的定义一个很重要的意义在于：它保证了开集中每个点都有一个邻域包含在这个集合内——所有点都和外界（补集）保持距离。这样的理解应该比使用集合运算的定义有更明晰的几何意义。但是，直观的东西不容易直接形成严谨的定义，使用集合运算则更为严格。而集合运算定义中，任意并集的封闭性是对这个几何特点的内在保证。另外一个例子就是“连续函数”(Continuous Function)。在学微积分时，一个耳熟能详的定义是“对任意的epsilon > 0，存在delta > 0，使得。。。。”，背后最直观的意思就是“足够近的点保证映射到任意小的范围内”。可是，epsilon, delta都依赖于实空间，不在实空间的映射又怎么办呢？拓扑的定义是“如果一个映射的值域中任何开集的原象都是开集，那么它连续。”这里就没有epsilon什么事了。“开集的原象是开集”这里的关键在于，在拓扑学中，开集的最重要意义就是要传递“邻域”的意思——开集本身就是所含点的邻域。这样连续定义成这样就顺理成章了。稍微把说法调节一下，上面的定义就变成了“对于f(x)的任意邻域U，都有x的一个邻域V，使得V里面的点都映射到U中。”这里面，我们可以感受到为什么开集在拓扑学中有根本性的意义。既然开集传达“邻域”的意思，那么，它最重要的作用就是要表达哪些点靠得比较近。给出一个拓扑结构，就是要指出哪些是开集，从而指出哪些点靠得比较近，这样就形成了一个聚集结构——这就是拓扑。可是这也可以通过距离来描述，为什么要用开集呢，反而不直观了。某种意义上说，拓扑是“定性”的，距离度量是“定量”的。随着连续变形，距离会不断变化，但是靠近的点还是靠近，因此本身固有的拓扑特性不会改变。拓扑学研究的就是这种本质特性——连续变化中的不变性。在拓扑的基本概念中，最令人费解的，莫过于“紧性”(Compactness)。它描述一个空间或者一个集合“紧不紧”。正式的定义是“如果一个集合的任意开覆盖都有有限子覆盖，那么它是紧的”。乍一看，实在有点莫名其妙。它究竟想描述一个什么东西呢？和“紧”这个形容词又怎么扯上关系呢？一个直观一点的理解，几个集合是“紧”的，就是说，无限个点撒进去，不可能充分散开。无论邻域多么小，必然有一些邻域里面有无限个点。上面关于compactness的这个定义的玄机就在有限和无限的转换中。一个紧的集合，被无限多的小邻域覆盖着，但是，总能找到其中的有限个就能盖全。那么，后果是什么呢？无限个点撒进去，总有一个邻域包着无数个点。邻域们再怎么小都是这样——这就保证了无限序列中存在极限点。Compact这个概念虽然有点不那么直观，可是在分析中有着无比重要的作用。因为它关系到极限的存在性——这是数学分析的基础。了解泛函分析的朋友都知道，序列是否收敛，很多时候就看它了。微积分中，一个重要的定理——有界数列必然包含收敛子列，就是根源于此。在学习拓扑，或者其它现代数学理论之前，我们的数学一直都在有限维欧氏空间之中，那是一个完美的世界，具有一切良好的属性，Hausdorff, Locally compact, Simply connected，Completed，还有一套线性代数结构，还有良好定义的度量，范数，与内积。可是，随着研究的加深，终究还是要走出这个圈子。这个时候，本来理所当然的东西，变得不那么必然了。·                   两个点必然能分开？你要证明空间是Hausdorff的。·                   有界数列必然存在极限点？这只在locally compact的空间如此。·                   一个连续体内任意两点必然有路径连接？这可未必。一切看上去有悖常理，而又确实存在。从线性代数到一般的群，从有限维到无限维，从度量空间到拓扑空间，整个认识都需要重新清理。而且，这些绝非仅是数学家的概念游戏，因为我们的世界不是有限维向量能充分表达的。当我们研究一些不是向量能表达的东西的时候，度量，代数，以及分析的概念，都要重新建立，而起点就在拓扑。和机器学习和计算机视觉相关的数学（转载）（以下转自一位MIT牛人的空间文章，写得很实际：）作者：Dahua感觉数学似乎总是不够的。这些日子为了解决research中的一些问题，又在图书馆捧起了数学的教科书。从大学到现在，课堂上学的和自学的数学其实不算少了，可是在研究的过程中总是发现需要补充新的数学知识。Learning和Vision都是很多种数学的交汇场。看着不同的理论体系的交汇，对于一个researcher来说，往往是非常exciting的enjoyable的事情。不过，这也代表着要充分了解这个领域并且取得有意义的进展是很艰苦的。记得在两年前的一次blog里面，提到过和learning有关的数学。今天看来，我对于数学在这个领域的作用有了新的思考。对于Learning的研究，1、Linear Algebra (线性代数) 和 Statistics (统计学) 是最重要和不可缺少的。这代表了Machine Learning中最主流的两大类方法的基础。一种是以研究函数和变换为重点的代数方法，比如Dimension reduction，feature extraction，Kernel等，一种是以研究统计模型和样本分布为重点的统计方法，比如Graphical model, Information theoretical models等。它们侧重虽有不同，但是常常是共同使用的，对于代数方法，往往需要统计上的解释，对于统计模型，其具体计算则需要代数的帮助。以代数和统计为出发点，继续往深处走，我们会发现需要更多的数学。2、Calculus (微积分)，只是数学分析体系的基础。其基础性作用不言而喻。Learning研究的大部分问题是在连续的度量空间进行的，无论代数还是统计，在研究优化问题的时候，对一个映射的微分或者梯度的分析总是不可避免。而在统计学中，Marginalization和积分更是密不可分——不过，以解析形式把积分导出来的情况则不多见。3、Partial Differential Equation （偏微分方程)，这主要用于描述动态过程，或者仿动态过程。这个学科在Vision中用得比Learning多，主要用于描述连续场的运动或者扩散过程。比如Level set, Optical flow都是这方面的典型例子。4、Functional Analysis (泛函分析)，通俗地，可以理解为微积分从有限维空间到无限维空间的拓展——当然了，它实际上远不止于此。在这个地方，函数以及其所作用的对象之间存在的对偶关系扮演了非常重要的角色。Learning发展至今，也在向无限维延伸——从研究有限维向量的问题到以无限维的函数为研究对象。Kernel Learning 和 Gaussian Process 是其中典型的例子——其中的核心概念都是Kernel。很多做Learning的人把Kernel简单理解为Kernel trick的运用，这就把kernel的意义严重弱化了。在泛函里面，Kernel (Inner Product)是建立整个博大的代数体系的根本，从metric, transform到spectrum都根源于此。5、Measure Theory (测度理论)，这是和实分析关系非常密切的学科。但是测度理论并不限于此。从某种意义上说，Real Analysis可以从Lebesgue Measure（勒贝格测度）推演，不过其实还有很多别的测度体系——概率本身就是一种测度。测度理论对于Learning的意义是根本的，现代统计学整个就是建立在测度理论的基础之上——虽然初级的概率论教科书一般不这样引入。在看一些统计方面的文章的时候，你可能会发现，它们会把统计的公式改用测度来表达，这样做有两个好处：所有的推导和结论不用分别给连续分布和离散分布各自写一遍了，这两种东西都可以用同一的测度形式表达：连续分布的积分基于Lebesgue测度，离散分布的求和基于计数测度，而且还能推广到那种既不连续又不离散的分布中去（这种东西不是数学家的游戏，而是已经在实用的东西，在Dirchlet Process或者Pitman-Yor Process里面会经常看到)。而且，即使是连续积分，如果不是在欧氏空间进行，而是在更一般的拓扑空间（比如微分流形或者变换群），那么传统的黎曼积分（就是大学一年级在微积分课学的那种）就不work了，你可能需要它们的一些推广，比如Haar Measure或者Lebesgue-Stieltjes积分。6、Topology（拓扑学)，这是学术中很基础的学科。它一般不直接提供方法，但是它的很多概念和定理是其它数学分支的基石。看很多别的数学的时候，你会经常接触这样一些概念：Open set / Closed set，set basis，Hausdauf, continuous function，metric space, Cauchy sequence, neighborhood, compactness, connectivity。很多这些也许在大学一年级就学习过一些，当时是基于极限的概念获得的。如果，看过拓扑学之后，对这些概念的认识会有根本性的拓展。比如，连续函数，当时是由epison法定义的，就是无论取多小的正数epsilon，都存在xxx，使得xxx。这是需要一种metric去度量距离的，在general topology里面，对于连续函数的定义连坐标和距离都不需要——如果一个映射使得开集的原像是开集，它就是连续的——至于开集是基于集合论定义的，不是通常的开区间的意思。这只是最简单的例子。当然，我们研究learning也许不需要深究这些数学概念背后的公理体系，但是，打破原来定义的概念的局限在很多问题上是必须的——尤其是当你研究的东西它不是在欧氏空间里面的时候——正交矩阵，变换群，流形，概率分布的空间，都属于此。7、Differential Manifold (微分流形)，通俗地说它研究的是平滑的曲面。一个直接的印象是它是不是可以用来fitting一个surface什么的——当然这算是一种应用，但是这是非常初步的。本质上说，微分流形研究的是平滑的拓扑结构。一个空间构成微分流形的基本要素是局部平滑：从拓扑学来理解，就是它的任意局部都同胚于欧氏空间，从解析的角度来看，就是相容的局部坐标系统。当然，在全局上，它不要求和欧氏空间同胚。它除了可以用于刻画集合上的平滑曲面外，更重要的意义在于，它可以用于研究很多重要的集合。一个n-维线性空间的全部k-维子空间(k8、Lie Group Theory (李群论)，一般意义的群论在Learning中被运用的不是很多，群论在Learning中用得较多的是它的一个重要方向Lie group。定义在平滑流形上的群，并且其群运算是平滑的话，那么这就叫李群。因为Learning和编码不同，更多关注的是连续空间，因为Lie group在各种群中对于Learning特别重要。各种子空间，线性变换，非奇异矩阵都基于通常意义的矩阵乘法构成李群。在李群中的映射，变换，度量，划分等等都对于Learning中代数方法的研究有重要指导意义。9、Graph Theory（图论)，图，由于它在表述各种关系的强大能力以及优雅的理论，高效的算法，越来越受到Learning领域的欢迎。经典图论，在Learning中的一个最重要应用就是graphical models了，它被成功运用于分析统计网络的结构和规划统计推断的流程。Graphical model所取得的成功，图论可谓功不可没。在Vision里面，maxflow (graphcut)算法在图像分割，Stereo还有各种能量优化中也广受应用。另外一个重要的图论分支就是Algebraic graph theory (代数图论)，主要运用于图的谱分析，著名的应用包括Normalized Cut和Spectral Clustering。近年来在semi-supervised learning中受到特别关注。这是大牛们做的很好的综述啊！据说，是MIT一牛人对数学在机器学习中的作用给的评述!via:http://blog.sina.com.cn/s/blog_6833a4df0100nazk.html"}
{"content2":"1. ===功能===人工智能现在已经能实现很多功能了，比如语音识别——李开复博士当年做的工作奠定了很多当今识别系统的基础。这里忍不住说一下，Siri本身的技术并没有特别大的亮点，真正nb的是它的模式（语音识别直接与搜索引擎结合在一起，产品体验做得好。而且关键是这样的模式能采集到更多数据，使得系统的精度越来越高）自然语言理解——目前看到的最强的结果应该是IBM Watson。但其实我们现在用的搜索引擎、中文输入法、机器翻译（虽然其实还不怎么work）都和自然语言理解相关。这块儿不是我的专业，请 @段维斯 同学补充。数据挖掘——随着近年数据量的疯狂增长，数据挖掘也有了长足进步。最具有代表性的是前几年著名的Netflix challenge（Netflix公司公开了自己的用户评分数据，让研究者根据这些数据对用户没看过的电影预测评分，谁先比现有系统好10%，谁就能赢100万美元）最后这一比赛成绩较好的队伍，并非是单一的某个特别nb的算法能给出精确的结果，而是把大量刻画了不同方面的模型混合在一起，进行最终的预测。计算机视觉——目前越来越多的领域跟视觉有关。大家可能一开始想到的都是自动驾驶。虽然大家都在说googleX的无人车， 但实际上现在无论是商业上，还是技术整合上最成功的算法是Mobile Eye的辅助驾驶系统。这个公司也是目前computer vision领域最挣钱的公司。从实现新功能方面说，视觉的发展的趋势主要有两方面，A) 集成更多的模块，从问题的各种不同方面，解决同一个问题（比如Mobile Eye，就同时使用了数十种方法，放到一起最终作出决策） B) 使用新的信息，解决一个原来很难的问题。这方面最好的例子是M$的Kinect，这个产品最让人拍案叫绝的就是那个红外pattern投影仪。2. ===理论基础===这里说的是数学理论，是为实现功能解决问题而存在的。与人类的智能的联系在下一节说。从这个角度，我们已经有了很多强有力的数学工具，从高斯时代的最小二乘法，到现在比较火的凸优化，其实我们解决绝大多数智能问题的套路，都可以从某种意义上转换成一个优化问题。真正限制我们解这个优化问题的困难有以下三个：计算复杂度——能保证完美解的算法大都是NP-hard的。如何能让一个系统在当前的硬件下“跑起来”，就需要在很多细节取巧，这是很多learning paper的核心冲突。模型假设——所有模型都要基于一些假设，比如说，无人车会假设周围的汽车加速度有一个上限（至少不会瞬间移动吧，否则怎么闪避）绝大多数假设都不能保证绝对正确，我们只是制定那些在大多数时候合理的假设，然后基于这些假设建模（比如，在语音识别里，我们是否要假设存在背景噪声呢？如果有背景噪声，这个噪声应该符合什么特点呢？这时候无论你怎么定标准，总能找出“反例”）数据基础——任何学习过程都需要数据的支持，无论是人类学说话学写字，还是计算机学习汽车驾驶。但是就数据采集本身来说，成功的案例并不多。大概这个世界上最强的数据采集就是google了吧。每次你搜索一个关键词，然后点进去，google就自动记录了你的行为，然后以此数据来训练自己的算法。随着深度学习技术的成熟，AI人工智能正在逐步从尖端技术慢慢变得普及。AlphaGo和人类的对弈，并不是我们以往所理解的电子游戏，电子游戏的水平永远不会提升，而AlphaGo则具备了人工智能最关键的“深度学习”功能。AlphaGo中有两个深度神经网络，Value Networks（价值网络）和 Policy Networks（策略网络）。其中Value Networks评估棋盘选点位置，Policy Networks选择落子。这些神经网络模型通过一种新的方法训练，结合人类专家比赛中学到的棋谱，以及在自己和自己下棋（Self-Play）中进行强化学习。也就是说，人工智能的存在，能够让AlphaGo的围棋水平在学习中不断上升。人工智能的技术应用主要是在以下几个方面：自然语言处理（包括语音和语义识别、自动翻译）、计算机视觉（图像识别）、知识表示、自动推理（包括规划和决策）、机器学习和机器人学。按照技术类别来分，可以分成感知输入和学习与训练两种。计算机通过语音识别、图像识别、读取知识库、人机交互、物理传感等方式，获得音视频的感知输入，然后从大数据中进行学习，得到一个有决策和创造能力的大脑。从上世纪八九十年代的PC时代，进入到互联网时代后，给我们带来的是信息的爆炸和信息载体的去中心化。而网络信息获取渠道从PC转移到移动端后，万物互联成为趋势，但技术的限制导致移动互联网难以催生出更多的新应用和商业模式。而如今，人工智能已经成为这个时代最激动人心、最值得期待的技术，将成为未来10年乃至更长时间内IT产业发展的焦点。人工智能概念其实在上世纪80年代就已经炒得火热，但是软硬件两方面的技术局限使其沉迷了很长一段时间。而现在，大规模并行计算、大数据、深度学习算法和人脑芯片这四大催化剂的发展，以及计算成本的降低，使得人工智能技术突飞猛进。一、驱动人工智能发展的先决条件物联网——物联网提供了计算机感知和控制物理世界的接口和手段，它们负责采集数据、记忆、分析、传送数据、交互、控制等等。摄像头和相机记录了关于世界的大量的图像和视频，麦克风记录语音和声音，各种传感器将它们感受到的世界数字化等等。这些传感器，就如同人类的五官，是智能系统的数据输入，感知世界的方式。而大量智能设备的出现则进一步加速了传感器领域的繁荣，这些延伸向真实世界各个领域的触角是机器感知世界的基础，而感知则是智能实现的前提之一。大规模并行计算——人脑中有数百至上千亿个神经元，每个神经元都通过成千上万个突触与其他神经元相连，形成了非常复杂和庞大的神经网络，以分布和并发的方式传递信号。这种超大规模的并行计算结构使得人脑远超计算机，成为世界上最强大的信息处理系统。近年来，基于GPU（图形处理器）的大规模并行计算异军突起，拥有远超CPU的并行计算能力。从处理器的计算方式来看，CPU计算使用基于x86指令集的串行架构，适合尽可能快的完成一个计算任务。而GPU从诞生之初是为了处理3D图像中的上百万个像素图像，拥有更多的内核去处理更多的计算任务。因此GPU天然具备了执行大规模并行计算的能力。云计算的出现、GPU的大规模应用使得集中化的数据计算处理能力变得前所未有的强大。大数据——根据统计，2015年全球产生的数据总量达到了十年前的20多倍，海量的数据为人工智能的学习和发展提供了非常好的基础。机器学习是人工智能的基础，而数据和以往的经验，就是人工智能学习的书本，以此优化计算机的处理性能。深度学习算法——最后，这是人工智能进步最重要的条件，也是当前人工智能最先进、应用最广泛的核心技术，深度神经网络（深度学习算法）。2006年，Geoffrey Hinton教授发表的论文《A fast learning algorithm for deep belief nets》。他在此文中提出的深层神经网络逐层训练的高效算法，让当时计算条件下的神经网络模型训练成为了可能，同时通过深度神经网络模型得到的优异的实验结果让人们开始重新关注人工智能。之后，深度神经网络模型成为了人工智能领域的重要前沿阵地，深度学习算法模型也经历了一个快速迭代的周期，Deep Belief Network、Sparse Coding、Recursive Neural Network, Convolutional Neural Network等各种新的算法模型被不断提出，而其中卷积神经网络（Convolutional Neural Network，CNN）更是成为图像识别最炙手可热的算法模型。二、IT巨头在人工智能上的投入技术的进步使得人工智能的发展在近几年显著加速，IT巨头在人工智能上的投入明显增大，一方面网罗顶尖人工智能的人才，另一方面加大投资力度频频并购，昭示着人工智能的春天已经到来。电商一班邢利栋"}
{"content2":"张志华教授：机器学习——统计与计算之恋编辑部按：本文是从张志华老师在第九届中国R语言会议和上海交通大学的两次讲座中整理出来的。张志华老师是上海交通大学计算机科学与工程系教授，上海交通大学数据科学研究中心兼职教授，计算机科学与技术和统计学双学科的博士生指导导师。在加入上海交通大学之前，是浙江大学计算机学院教授和浙江大学统计科学中心兼职教授。张老师主要从事人工智能、机器学习与应用统计学领域的教学与研究，迄今在国际重要学术期刊和重要的计算机学科会议上发表70余篇论文，是美国“数学评论”的特邀评论员，国际机器学习旗舰刊物Journal of Machine Learning Research 的执行编委。其公开课《机器学习导论》和《统计机器学习》受到广泛关注。张志华老师和他的学生们大家好，今天我演讲的主题是 “机器学习：统计与计算之恋”。我用了一个很浪漫的名字，但是我的心情是诚惶诚恐的。一则我担心自己没有能力驾驭这么大的主题，二则我其实是一个不解风情之人，我的观点有些可能不符合国内学术界的主流声音。最近人工智能或者机器学习的强势崛起，特别是刚刚过去的AlphaGo和韩国棋手李世石九段的人机大战，再次让我们领略到了人工智能或机器学习技术的巨大潜力，同时也深深地触动了我。面对这一前所未有的技术大变革，作为10多年以来一直从事统计机器学习一线教学与研究的学者，希望借此机会和大家分享我个人的一些思考和反思。在这场人工智能发展的盛事里，我突然发现，对我们中国的学者来说，好像是一群看热闹的旁观者。不管你承认还是不承认，事实就是和我一代的或者更早的学者也只能作为旁观者了。我们能做的事情是帮助你们---中国年轻的一代，让你们在人工智能发展的大潮中有竞争力，做出标杆性的成就，创造人类文明价值，也让我有个加油欢呼的主队。我的演讲主要包含两部分，在第一部分，首先对机器学习发展做一个简要的回顾，由此探讨机器学习现象所蕴含的内在本质，特别是讨论它和统计学、计算机科学、运筹优化等学科的联系，以及它和工业界、创业界相辅相成的关系。在第二部分，试图用“多级”、“自适应”以及 “平均”等概念来简约纷繁多彩的机器学习模型和计算方法背后的一些研究思路或思想。第一部分：回顾和反思1、 什么是机器学习毋庸置疑，大数据和人工智能是当今是最为时髦的名词，它们将为我们未来生活带来深刻的变革。数据是燃料，智能是目标，而机器学习是火箭，即通往智能的技术途径。机器学习大师Mike Jordan和Tom Mitchell 认为机器学习是计算机科学和统计学的交叉，同时是人工智能和数据科学的核心。“It is one of today’s rapidly growing technical fields,  lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science”                                                               ---M. I. Jordan通俗地说，机器学习就是从数据里面挖掘出有用的价值。数据本身是死的，它不能自动呈现出有用的信息。怎么样才能找出有价值的东西呢？第一步要给数据一个抽象的表示，接着基于表示进行建模，然后估计模型的参数，也就是计算，为了应对大规模的数据所带来的问题，我们还需要设计一些高效的实现手段。我把这个过程解释为机器学习等于矩阵+统计+优化+算法。首先，当数据被定义为一个抽象的表示时，往往形成一个矩阵或者一个图，而图其实也是可以理解为矩阵。统计是建模的主要工具和途径，而模型求解大多被定义为一个优化问题，特别是，频率统计方法其实就是一个优化问题。当然，贝叶斯模型的计算牵涉随机抽样方法。而之前说到面对大数据问题的具体实现时，需要一些高效的方法，计算机科学中的算法和数据结构里有不少好的技巧可以帮助我们解决这个问题。借鉴Marr的关于计算机视觉的三级论定义，我把机器学习也分为三个层次：初级、中级和高级。初级阶段是数据获取以及特征的提取。中级阶段是数据处理与分析，它又包含三个方面，首先是应用问题导向，简单地说，它主要应用已有的模型和方法解决一些实际问题，我们可以理解为数据挖掘；第二，根据应用问题的需要，提出和发展模型、方法和算法以及研究支撑它们的数学原理或理论基础等，我理解这是机器学习学科的核心内容。第三，通过推理达到某种智能。最后，高级阶段是智能与认知，即实现智能的目标。从这里，我们看到，数据挖掘和机器学习本质上是一样的，其区别是数据挖掘更接地于数据库端，而机器学习则更接近于智能端。2、  机器学习的发展历程我们来梳理一下机器学习的发展历程。上个世纪90年代以前，我对此认识不够，了解不深，但我觉得当时机器学习处于发展的平淡期。而1996-2006年是其黄金时期，主要标志是学术界涌现出一批重要成果，比如，基于统计学习理论的SVM和boosting等分类方法，基于再生核理论的非线性数据分析与处理方法，以lasso为代表的稀疏学习模型及应用等等。这些成果应该是统计界和计算机科学界共同努力成就的。然而，机器学习也经历了一个短暂的徘徊期。这个我感同身受，因为那时我在伯克利的博士后工作结束，正面临找工作，因此当时我导师Mike Jordan教授和我进行了多次交流，他一方面认为机器学习正处于困难期，工作职位已趋于饱满，另一方面他向我一再强调，把统计学引入到机器学习的思路是对的，因为以统计学为基础的机器学习作为一个学科其地位已经被奠定。主要问题是机器学习是一门应用学科，它需要在工业界发挥出作用，能为他们解决实际问题。幸运的是，这个时期很快就过去了。可能在座大多数人对这个时期没有印象，因为中国学术发展往往要慢半拍。现在我们可以理直气壮地说机器学习已经成为计算机科学和人工智能的主流学科。主要体现在下面三个标志性的事件。首先，2010年2月，伯克利的Mike Jordan教授和CMU的Tom Mitchell教授同时被选为美国工程院院士，同年5月份，Mike Jordan和斯坦福的统计学家Jerome Friedman又被选为美国科学院院士。我们知道许多著名机器学习算法比如CART、MARS 和GBM等是 Friedman教授等提出。随后几年一批在机器学习做出重要贡献的学者先后被选为美国科学院或工程院院士。比如，人工智能专家的Daphne Koller, Boosting的主要建立者Robert Schapire, Lasso的提出者Robert Tibshirani, 华裔著名统计学习专家郁彬老师，统计机器机器学习专家的Larry Wasserman, 著名的优化算法专家 Stephen Boyd等。同时，机器学习专家、深度学习的领袖Toronto大学Geoffrey Hinton 以及该校统计学习专家Nancy Reid 今年分别被选为美国工程院和科学院的外籍院士。这是当时Mike给我祝贺他当选为院士时的回信：Thanks for your congratulations on my election to the National Academy. It's nice to have machine learning recognized in this way.因此，我理解在美国一个学科能否被接纳为主流学科的一个重要标志是其代表科学家能否被选为院士。我们知道Tom Mitchell 是机器学习早期建立者和守护者，而Mike Jordan是统计机器学习的奠基者和推动者。这个遴选机制无疑是先进的，它可以促使学科良性发展，适应社会动态发展和需求。相反，如果某某通过某种方式被评选为本国院士，然后他们就掌握了该国学术话语权和资源分配权。这种机制可能会造成一些问题，比如一些过剩学科或者夕阳学科会得到过多的发展资源，而主流学科则被边缘化。其次，2011年的图灵奖授予了UCLA的Judea Pearl教授，他主要的研究领域是概率图模型和因果推理，这是机器学习的基础问题。我们知道，图灵奖通常颁给做纯理论计算机科学的学者，或者早期建立计算机架构的学者，而把图灵奖授予Judea Pearl教授具有方向标的意义。第三，是当下的热点，比如说深度学习、AlphaGo、无人驾驶汽车、人工智能助理等等对工业界的巨大影响。机器学习切实能被用来帮助工业界解决问题。工业界对机器学习领域的才人有大量的需求，不仅仅需要代码能力强的工程师，也需要有数学建模和解决问题的科学家。让我们具体地看看工业界和机器学习之间的关系。我之前在谷歌研究院做过一年的访问科学家，我有不少同事和以前学生在IT界工作，平时实验室也经常接待一些公司的来访和交流，因此了解一些IT界情况。我理解当今IT的发展已从传统的微软模式转变到谷歌模式。传统的微软模式可以理解为制造业，而谷歌模式则是服务业。谷歌搜索完全是免费的，服务社会，他们的搜索做得越来越极致，同时创造的财富也越来越丰厚。财富蕴藏在数据中，而挖掘财富的核心技术则是机器学习。深度学习作为当今最有活力一个机器学习方向，在计算机视觉、自然语言理解、语音识别、智力游戏等领域的颠覆性成就。它造就了一批新兴的创业公司。3、 统计与计算我的重点还是要回到学术界。我们来重点讨论统计学和计算机科学的关系。CMU 统计系教授Larry Wasserman最近刚被选为美国科学院院士。他写了一本名字非常霸道的书，《All of Statistics》。在这本书引言部分关于统计学与机器学习有个非常有趣的描述。他认为原来统计是在统计系，计算机是在计算机系，这两个是不相来往的，而且互相都不认同对方的价值。计算机学家认为那些统计理论没有用，不解决问题，而统计学家则认为计算机学家只是在重新建造轮子，没有新意。然而，他认为这个情况现在改变了，统计学家认识到计算机学家正在做出的贡献，而计算机学家也认识到统计的理论和方法论的普遍性意义。所以，Larry写了这本书，可以说这是一本为统计学者写的计算机领域的书，为计算机学者写的统计领域的书。现在大家达成了一个共识: 如果你在用一个机器学习方法，而不懂其基础原理，这是一件非常可怕的事情。也是由于这个原因，目前学术界对深度学习还是心存疑虑的。深度学习已经展示其强大的实际应用的效果，但其中的原理目前大家还不是太清楚。让我们进一步地来分析统计与计算机的关系。计算机学家通常具有强的计算能力和解决问题的直觉，而统计学家长于理论分析，具有强的建模能力，因此，两者有很好的互补性。Boosting, SVM 和稀疏学习是机器学习界也是统计界，在近十年或者是近二十年来，最活跃的方向，现在很难说谁比谁在其中做的贡献更大。比如，SVM的理论其实很早被Vapnik等提出来了，但计算机界发明了一个有效的求解算法，而且后来又有非常好的实现代码被陆续开源给大家使用，于是SVM就变成分类算法的一个基准模型。再比如，KPCA是由计算机学家提出的一个非线性降维方法，其实它等价于经典MDS。而后者在统计界是很早就存在的，但如果没有计算机界从新发现，有些好的东西可能就被埋没了。机器学习现在已成为统计学的一个主流方向，许多著名统计系纷纷招聘机器学习领域的博士为教员。计算在统计已经变得越来越重要，传统多元统计分析是以矩阵为计算工具，现代高维统计则是以优化为计算工具。另一方面，计算机学科开设高级统计学课程，比如统计学中的核心课程“经验过程”。我们来看机器学习在计算机科学占什么样的地位。最近有一本还没有出版的书 “Foundation of Data Science, by Avrim Blum, John Hopcroft, and Ravindran Kannan,”作者之一John Hopcroft是图灵奖得主。在这本书前沿部分，提到了计算机科学的发展可以分为三个阶段：早期、中期和当今。早期就是让计算机可以运行起来，其重点在于开发程序语言、编译原理、操作系统，以及研究支撑它们的数学理论。中期是让计算机变得有用，变得高效。重点在于研究算法和数据结构。第三个阶段是让计算机具有更广泛的应用，发展重点从离散类数学转到概率和统计。那我们看到，第三阶段实际上就是机器学习所关心的。现在计算机界戏称机器学习“全能学科”，它无所不在。一方面，机器学习有其自身的学科体系；另一方面它还有两个重要的辐射功能。一是为应用学科提供解决问题的方法与途径。说的通俗一点，对于一个应用学科来说，机器学习的目的就是把一些难懂的数学翻译成让工程师能够写出程序的伪代码。二是为一些传统学科，比如统计、理论计算机科学、运筹优化等找到新的研究问题。4、 机器学习发展的启示机器学习的发展历程告诉我们：发展一个学科需要一个务实的态度。时髦的概念和名字无疑对学科的普及有一定的推动作用，但学科的根本还是所研究的问题、方法、技术和支撑的基础等，以及为社会产生的价值。机器学习是个很酷的名字，简单地按照字面理解，它的目的是让机器能像人一样具有学习能力。但在前面我们所看到的，在其10年的黄金发展期，机器学习界并没有过多地炒作“智能”，而是更多地关注于引入统计学等来建立学科的理论基础，面向数据分析与处理，以无监督学习和有监督学习为两大主要的研究问题，提出和开发了一系列模型、方法和计算算法等，切实地解决工业界所面临的一些实际问题。近几年，因应大数据的驱动和计算能力的极大提升，一批面向机器学习的底层架构又先后被开发出来，深度神经网络的强势崛起给工业界带来了深刻的变革和机遇。机器学习的发展同样诠释了多学科交叉的重要性和必要性。然而这种交叉不是简单地彼此知道几个名词或概念就可以的，是需要真正的融化贯通。Mike Jordan教授既是一流的计算机学家，又是一流的统计学家，所以他能够承担起建立统计机器学习的重任。而且他非常务实，从不提那些空洞无物的概念和框架。他遵循自下而上的方式，即先从具体问题、模型、方法、算法等着手，然后一步一步系统化。Geoffrey Hinton教授是世界最著名的认知心理学家和计算机科学学家。虽然他很早就成就斐然，在学术界名声卓越，但他一直活跃在一线，自己写代码。他提出的许多想法简单、可行又非常有效，因此被称为伟大的思想家。正是由于他的睿智和力行，深度学习技术迎来了革命性的突破。机器学习这个学科同时是兼容并收。我们可以说机器学习是由学术界、工业界、创业界(或竞赛界)等合力而造就的。学术界是引擎，工业界是驱动，创业界是活力和未来。学术界和工业界应该有各自的职责和分工。学术界职责在于建立和发展机器学习学科，培养机器学习领域的专门人才；而大项目、大工程更应该由市场来驱动，由工业界来实施和完成。5、国内外发展现状我们来看看机器学习在国际的发展现状。我主要看几所著名大学的情况。在伯克利，一个值得深思的举措是机器学习的教授同时在计算机系和统计学都有正式职位，而且据我所知，他们不是兼职，在两个系都有教授课程和研究的任务的。伯克利是美国统计学的发源地，可以说是当今统计学的圣地，然而她兼容并蓄、不固步自封。Mike Jordan教授是统计机器学习的主要建立者和推动者，他为机器学习领域培养了一大批优秀的学生。统计系的主任现在是Mike，然而他早年的教育并没有统计或数学背景。可以说，Berkeley的统计系成就了Mike，反过来他也为Berkeley的统计学发展创造了新的活力，建立了无可代替的功勋。斯坦福和伯克利的统计是公认世界最好的两个。我们看到，斯坦福统计系的主流方向就是统计学习，比如我们熟知的《Elements of statistical learning》一书就是统计系几位著名教授撰写的。Stanford计算机科学的人工智能方向一直在世界占主导地位，特别在不确定推理、概率图模型、概率机器人等领域成就斐然，他们的网络公开课 《机器学习》、《概率图模型》以及《人工智能》等让世界受益。CMU是一个非常独特的学校，她并不是美国传统的常春藤大学。可以说，它是以计算机科学为立校之本，它是世界第一个建立机器学习系的学校。Tom Mitchell 教授是机器学习的早期建立者之一和守护者，他一直为该校本科生教《机器学习》课程。然而，这个学校统计学同样强，尤其，她是贝叶斯统计学的世界研究中心。在机器学习领域，多伦多大学有着举足轻重的地位，她们机器学习研究组云集了一批世界级的学者，在“Science” 和“Nature”发表多篇论文，实属罕见。Geoffrey Hinton 教授是伟大的思想家，但更是践行者。他是神经网络的建立者之一，是BP算法和深度学习的主要贡献者。正是由于他的不懈努力，神经网络迎来了大爆发。Radford Neal 教授是Hinton学生，他在贝叶斯统计领域，特别是关于MCMC做出了一系列的重要工作。国际发展现状那么我们来看看国内的现状。总的来说，统计和计算机科学这两个学科处于Larry所说的初期各自为战的阶段。面向大数据的统计学与计算机科学的交叉研究是机遇也是挑战。我之前在浙江大学曾经参与其统计交叉学科中心的组建，由此对统计界有所了解。统计学在中国应该还是一个弱势学科，最近才被国家定为一级学科。我国统计学处于两个极端，一是它被当作数学的一个分支，主要研究概率论、随机过程以及数理统计理论等。二是它被划为经济学的分支，主要研究经济分析中的应用。而机器学习在统计学界还没有被深度地关注。因此，面向于数据处理、分析的IT和统计学的深度融合有巨大的潜力。虽然，我并没有跟国内机器学习或者人工智能学术界有深入的接触，但我在国内计算机系工作近8年时间，一直在一线从事机器学习相关的教学与研究，应该对机器学习的现状有一定的发言权。机器学习的确在中国得到了广泛的关注，也取得了一定的成绩，但我觉得高品质的研究成果稀缺。热衷于对机器学习的高级阶段进行一些概念炒作，它们通常没有多大的可执行性；偏爱大项目、大集成，这些本更应该由工业界来实施；而理论、方法等基础性的研究不被重视，认为理论没有用处的观点还大有市场。计算机学科的培养体系还基本停留在它的早期发展阶段。大多数学校都开设了人工智能与机器学习的课程，但无论是深度还是前沿性都落后于学科的发展，不能适应时代的需要。人才的培养无论质量和数量都无法满足工业界的需求。这也是国内IT公司与国际同类公司技术上有较大差距的关键原因。第二部分：几个简单的研究思路在这部分，我的关注则回到机器学习的研究本身上来。机器学习内容博大精深，而且新方法、新技术正源源不断地被提出、被发现。这里，我试图用“多级”、“自适应”以及 “平均”等概念来简约纷繁多彩的机器学习模型和计算方法背后的一些研究思路和思想。希望这些对大家理解机器学习已有的一些模型、方法以及未来的研究有所启发。1. 多级 (Hierarchical)首先，让我们来关注“多级”这个技术思想。我们具体看三个例子。第一个例子是隐含数据模型，它就是一种多级模型。作为概率图模型的一种延伸，隐含数据模型是一类重要的多元数据分析方法。隐含变量有三个重要的性质。第一，可以用比较弱的条件独立相关性代替较强的边界独立相关性。著名的de Finetti 表示定理支持这点。这个定理说，一组可以交换的随机变量当且仅当在某个参数给定条件下，它们可以表示成一组条件随机变量的混合体。这给出了一组可以交换的随机变量的一个多级表示。即先从某个分布抽一个参数，然后基于这个参数，独立地从某个分布抽出这组随机变量。第二，可以通过引入隐含变量的技术来方便计算，比如期望最大算法以及更广义的数据扩充技术就是基于这一思想。具体地，一些复杂分布，比如t-distribution, Laplace distribution 则可以通过表示成高斯尺度混合体来进行简化计算。第三，隐含变量本身可能具有某种有可解释的物理意思，这刚好符合应用的场景。比如，在隐含狄利克雷分配(LDA)模型，其中隐含变量具有某种主题的意思。第一个例子是隐含数据模型，它就是一种多级模型。作为概率图模型的一种延伸，隐含数据模型是一类重要的多元数据分析方法。隐含变量有三个重要的性质。第一，可以用比较弱的条件独立相关性代替较强的边界独立相关性。著名的de Finetti 表示定理支持这点。这个定理说，一组可以交换的随机变量当且仅当在某个参数给定条件下，它们可以表示成一组条件随机变量的混合体。这给出了一组可以交换的随机变量的一个多级表示。即先从某个分布抽一个参数，然后基于这个参数，独立地从某个分布抽出这组随机变量。第二，可以通过引入隐含变量的技术来方便计算，比如期望最大算法以及更广义的数据扩充技术就是基于这一思想。具体地，一些复杂分布，比如t-distribution, Laplace distribution 则可以通过表示成高斯尺度混合体来进行简化计算。第三，隐含变量本身可能具有某种有可解释的物理意思，这刚好符合应用的场景。比如，在隐含狄利克雷分配(LDA)模型，其中隐含变量具有某种主题的意思。Laten Dirichlet Allocation第二个例子，我们来看多级贝叶斯模型。在进行MCMC抽样后验估计时，最上层的超参数总是需要先人为给定的，自然地，MCMC算法收敛性能是依赖这些给定的超参数的，如果我们对这些参数的选取没有好的经验，那么一个可能做法我们再加一层，层数越多对超参数选取的依赖性会减弱。Hierarchical Bayesian Model第三例子，深度学习蕴含的也是多级的思想。如果把所有的节点全部的放平，然后全连接，就是一个全连接图。而CNN深度网络则可以看成对全连接图的一个结构正则化。正则化理论是统计学习的一个非常核心的思想。CNN和RNN是两大深度神经网络模型，分别主要用于图像处理和自然语言处理中。研究表明多级结构具有更强的学习能力。Deep Learning2. 自适应 (Adaptive)我们来看自适应这个技术思路，我们通过几个例子来看这个思路的作用。第一个例子是自适应重要采样技术。重要采样方法通常可以提高均匀采样的性能，而自适应则进一步改善重要采样的性能。第二个例子，自适应列选择问题。给定一个矩阵A，我们希望从中选取部分列构成一个矩阵C，然后用CC^+A去近似原矩阵A，而且希望近似误差尽可能小。这是一个NP难问题。在实际上，可以通过一个自适应的方式，先采出非常小一部分C_1，由此构造一个残差，通过这个定义一个概率，然后用概率再去采一部分C_2, 把C_1 和 C_2 合在一起组成C。第三个例子，是自适应随机迭代算法。考虑一个带正则化的经验风险最小问题，当训练数据非常多时，批处理的计算方式非常耗时，所以通常采用一个随机方式。存在的随机梯度或者随机对偶梯度算法可以得到参数的一个无偏估计。而通过引入自适应的技术，可以减少估计的方差。第四个例子，是Boosting分类方法。它自适应调整每个样本的权重，具体地，提高分错样本的权重，而降低分对样本的权重。3. 平均 (Averaging)其实，boosting 蕴含着平均思想，即我最后要谈的技术思路。简单地说，boosting是把一组弱分类器集成在一起，形成一个强的分类器。第一好处是可以降低拟合的风险。第二，可以降低陷入局部的风险。第三，可以扩展假设空间。Bagging同样是经典的集成学习算法，它把训练数据分成几组，然后分别在小数据集上训练模型，通过这些模型来组合强分类器。另外这是一个两层的集成学习方式。经典的Anderson 加速技术则是通过平均的思想来达到加速收敛过程。具体地，它是一个叠加的过程，这个叠加的过程通过求解一个残差最小得到一个加权组合。这个技术的好处，是没有增加太多的计算，往往还可以使数值迭代变得较为稳定。另外一个使用平均的例子是分布式计算中。很多情况下分布式计算不是同步的，是异步的，如果异步的时候怎么办？最简单的是各自独立做，到某个时候把所有结果平均，分发给各个worker, 然后又各自独立运行，如此下去。这就好像一个热启动的过程。正如我们已经看到，这些思想通常是组合在一起使用的，比如boosting模型。我们多级、自适应和平均的思想很直接，但的确也很有用。在AlphaGo和李世石九段对弈中，一个值得关注的细节是，代表Alpha Go方悬挂的是英国国旗。我们知道AlphaGo是由deep mind团队研发的，deep mind是一家英国公司，但后来被google公司收购了。科学成果是世界人民共同拥有和分享的财富，但科学家则是有其国家情怀和归属感。位低不敢忘春秋大义，我认为我国人工智能发展的根本出路在于教育。先哲说：“磨刀不误砍柴工”。只有培养出一批又一批的数理基础深厚、计算机动手执行力极强，有真正融合交叉能力和国际视野的人才时，我们才会有大作为。致谢上述内容是根据我最近在第九届中国R语言会议（http://china-r.org/bj2016/）和上海交通大学的两次讲座而整理出来的，特别是R会主办方统计之都的同学们帮我做了该次演讲的记录。感谢统计之都的太云、凌秉和象宇的邀请，他们和统计之都的伙伴们正在做一件意义影响深远的学术公益，你们的情怀和奉献给了我信心来公开宣讲自己多年来的真实认识和思考。感谢我的学生们帮助我准备这个讲演报告，从主题的选定，内容的选取，材料的收集以及幻灯片的制作他们都给了我极大的支持，更重要的是，他们让我在机器学习领域的求索一直不孤独。谢谢大家！统计之都：专业、人本、正直的中国统计学门户网站。关注方式：扫描下图二维码。或查找公众帐号，搜索 统计之都 或 CapStat 即可。往期推送：进入统计之都会话窗口，点击右上角小人图标，查看历史消息即可。统计之都欢迎诸位看官积极投稿，投稿信箱contact@cos.name"}
{"content2":"作者：张达衢   来源：人民网-人民日报海外版现状：技术研究领先，创新驱动发展打开支付宝，我们可以和阿里机器人进行对话；打开手机，语音识别可以为我们解答问题；打开讯飞输入法，可以实现高精度语音转化为文字输入；打开百度，可以为我们推荐最需要的搜索方案……人工智能，已经深入到中国人的日常生活中，为我们的生活带来诸多便利。中国在人工智能领域已经取得阶段性成功。艾媒咨询数据显示，2016年中国人工智能产业规模增长率达到43.3%，突破100亿元，预计2017年达到152.1亿元，并于2019年增长至344.3亿元。由此，业内人士将2016年定义为“中国人工智能元年”。这不仅仅是因为人工智能产业本身的火爆，更是因为人工智能概念在整个市场中分量的递增。一方面，中国人工智能研究正处于爆发期。工业和信息化部副部长刘利华表示，目前中国在人工智能技术研究方面已经走在了世界前列。数据显示，中国人工智能专利申请数累计达到15745项，位列全球第二，中国已是当今世界人工智能研发领域的领头羊之一。在此背景下，人工智能也迎来了政策上的利好，首次作为新兴产业的代表，被写入2017年的政府工作报告。“除去国家层面的政策之外，各地政府也开始密集出台人工智能产业配套扶持资金政策，努力解决企业发展的实际问题。”赛迪顾问高级分析师向阳指出，截至2016年底，已有超过30个城市将机器人产业作为当地重点发展对象，各地政府建成和在建的机器人产业园达40余家。另一方面，基础研究的进步使得人工智能的商业化得到了很好的支撑。“在PC互联网时代，中国是追随者；在移动互联网时代，中国有许多产品创新；在人工智能时代，中国在产品与技术方面都可以有很多创新。”百度公司总裁张亚勤表示，人工智能已经成为当今中国企业发展的中坚力量。“我们曾经依靠资源、资本、劳动力等要素投入，支撑中国经济的快速增长和规模扩展，但这已经成为历史”，中国人工智能学会理事长、中国工程院院士李德毅说：“加快从要素驱动发展向创新驱动发展转变，人工智能是一种解决方案。”机遇：海量数据资源，企业多方布局“在交通领域，我们可以看到城市大脑是如何思考的。”美国电气和电子工程师协会院士、阿里云机器视觉科学家华先胜生动地描述，应用阿里在杭州开发的城市大脑系统，能够感知复杂道路下车辆的运行轨迹，准确率达99%以上。这套系统是全球唯一能够对全城视频进行实时分析的人工智能系统。过去的两年中，以百度、腾讯、阿里为代表的中国科技巨头均纷纷在人工智能领域发力。百度在人脸识别领域的成绩获得世界认可，在人脸识别技术两个最为权威的国际评测中，都获得了第一名的佳绩，腾讯则推出自动化新闻写作机器人。“中国正与全球同步迈进，在计算机视觉、语音识别等细分领域甚至处于国际领先水平。”刘利华说，中国涌现出一批具备竞争实力的企业和研究机构，已具良好的发展基础。对此，艾媒咨询分析师认为，中国人工智能产业起步相对较晚，但是在产业布局、技术研究等基础设施方面正处于进步期。随着科技、制造等业界巨头公司的布局深入，人工智能的产业规模也将进一步的扩大。统计显示，至2015年末，中国人工智能领域已有近百家初创公司，在视频大数据、大数据智能分析、智慧医疗、无人驾驶、人脸识别等领域发力布局。在人工智能领域最为成熟的语音识别方面，已经能够达到99.7％的准确率。“机器人已经可以对话，可以听、说、读、写、译，人工智能正在走入寻常百姓家。”李德毅说。而无人驾驶领域，也在逐步推进。“从辅助驾驶到半自动驾驶、无人驾驶，未来中国汽车的高智能化将非常普及。”北汽新能源汽车总经理助理、营销支持部总监王水利在接受本报记者采访时表示，智能化将是中国汽车发展的一大趋势，很多汽车企业已经在智能化方面投入大量研发资金。业内人士表示，中国在人工智能应用领域具有他国无法比拟的优势，借助海量数据，中国企业已经在语音识别、语言翻译、无人车等领域取得领先地位。“人工智能的核心关键是海量的数据，在中国这些数据是存在的，而且都是能产生价值的，我们预期，人工智能将是中国引领全球最好的机遇。”创新工场董事长兼首席执行官李开复说。发展：告别“单打独斗”，推动产业集群管理咨询公司麦肯锡发布的一份报告认为，数据资源规模及种类增加、科技巨头和风投日趋关注、人工智能平台数量和规模激增，将给中国一次“后发制人”的机遇。麦肯锡预计，中国人工智能应用市场将以50%的增速逐年增长，远超全球市场20%的复合年增长率。“人工智能有今天的形势是非常不容易的，我们想保持好这样的态势，使得社会长期关注，需要形成一个巨大的产业。”中国科学院沈阳自动化研究所所长于海斌在2017全球人工智能技术大会上说。事实上，中国只有不到25%的人工智能从业者拥有超过10年的行业经验，而在美国，这一比例为50%。必须承认，尽管中国在语音识别等细分领域上已颇具领先地位，但中国人工智能产业集群的建设稍显缓慢，人工智能在产学研之间还尚未形成完善的共赢机制。“中国人工智能产业目前仍以‘单打独斗’为主，缺乏技术间的协同，产品间的互联互通和上下游的互动缺乏有效协调，无法形成发展合力。”赛迪智库电子信息产业研究所副所长温晓君建议，要建立集技术研发、示范应用、产品检测认证、知识产权等功能为一体的产业公共服务平台，打造人工智能创新孵化中心，促进产融对接。而打造产业公共服务平台，提升人工智能的集群式创新能力，无疑需要各地方政府和产业园区运营商的扶持。去年至今，政府频繁出台相关规划方案，要求培育全球领先的人工智能骨干企业，培育人工智能产业生态，建立人工智能产业体系、创新服务体系和标准化体系，打造人工智能市场应用规模。“如果说以前人工智能是散兵游勇，那以后就是被纳入编制的正规军了。”达泰资本创始人叶卫刚表示，希望政府能过通过市场化的方式来支持这个产业的创新和发展。有业内专家指出，人工智能必须通过应用落到不同产业，才能释放其巨大潜能。人工智能这种新技术结合教育、医疗、客服、家居、穿戴设备、机器人、电子商务等各行各业，对传统行业都带来很大的冲击，将促进各领域产业的蓬勃发展。"}
{"content2":"1. IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2019)Location ：Long Beach Convention & Entertainment Center, Los Angeles CA, United StatesDate：Jun 15 - Jun 21, 2019Paper Submission Deadline：Nov 16, 2018 (92)http://cvpr2019.thecvf.com２. IEEE International Conference on Computer Vision (ICCV 2019)Location ：Seoul, South KoreaDate: Oct 27 - Nov 3, 2019Paper Submission Deadline: Mar, 2019http://iccv2019.thecvf.com3. International Conference on Machine Learning（ICML 2019）Location：Long Beach Convention Center, Long Beach, United StatesDate：Jun 10 - Jun 15, 2019Paper Submission Deadline: TBDhttps://icml.cc/Conferences/20194.Association for the Advancement of Artificial Intelligence（AAAI 2019）Location：Hilton Hawaiian Village, Waikiki Beach, Honolulu, Hawaii, United StatesDate：Jan 27 - Feb 1, 2019Paper Submission Deadline:Sep 5, 2018 (20)http://www.aaai.org/aaai195.International Joint Conference on Artificial Intelligence（IJCAI 2019）Location：Macao, ChinaDate：Aug 10 - Aug 16, 2019Paper Submission Deadline：Feb 25, 2019http://www.ijcai19.org6. Computer Graphics and Interactive Techniques（SIGGRAPH 2019）Location：Los Angeles, California, United StatesDate：Jul 29 – Aug 1, 2019Paper Submission Deadline：TBDhttp://s2019.siggraph.org7. ACM SIGKDD International Conference on Knowledge discovery and data mining（SIGKDD 2019）Location：Anchorage, Alaska, United StatesDate：Aug 3 - Aug 7, 2019Paper Submission Deadline：TBDhttp://www.kdd.org/kdd20198. Annual Meeting of the Association for Computational Linguistics（ACL 2019）Location：Florence, ItalyDate：Jul 28 - Aug 2, 2019Paper Submission Deadline：TBDhttp://acl2019.org9. International Conference on Learning Representations（ICLR 2019)Location：New Orleans, United StatesDate：May 6 - May 9. 2019Paper Submission Deadline：Sep 27, 2018 (42)http://www.iclr.cchttps://jackietseng.github.io/conference_call_for_paper/2018-2019-conferences-with-ccf.html"}
{"content2":"《计算机视觉与算法应用》这本书其实我已经读到第四章了，之前一直都用笔记在笔记本上了，现在直接继续在这里记啦，之前的也不再补充了~Chapter 4 特征检测与匹配“关键点特征”或“兴趣点”或“角点”“边缘”4.1 点和块获取特征点及其之间的对应关系主要有两种方法：1）在第一幅图像中寻找那些可以使用局部搜索方法来精确跟踪的特征，比如相关或者最小二乘 2）在所有考察的图像中独立地检测特征点然后再基于它们的局部表观进行匹配关键点检测和匹配流水线：1）特征检测（提取）2）特征描述 3）特征匹配 4）特征跟踪4.1.1 特征检测器比较两个图像块计算匹配结果的稳定度--“自相关函数”或自相关表面矩阵A给出了匹配块所在位置不确定度的一个下界，通过特征值分析就可以对这个不确定度进行可视化分析。对应于自相关矩阵A的特征值分析的不确定性矩阵基本思路：使用从自相关矩阵导出的旋转不变标量测量的局部最大值来定位关键点以达到匹配稀疏特征的目的。使用高斯权重窗口代替方形图像块特征值最小值λ0不是唯一可以用来寻找关键点的量，其它的还有：, ,一个基本特征检测算法的步骤自适应非最大抑制大多数特征检测器只找兴趣函数的局部最大值，导致特征点非均匀分布，对比度大的区域特征点比较密集。解决办法是只检测那些同时是局部最大值且其响应明显大于其周围半径r区域内的响应的特征。首先根据特征点的响应强度对其进行排序，然后通过不断减小抑制半径来建立第二个排序列表。衡量可重复性可重复性：一幅图像中检测到的关键点在另一幅变换过的图像中的对应位置的ε个像素范围内找到的频率。每个特征点的“可用信息量”(Information Content)：一个旋转不变的局部灰度描述子集合的墒。尺度不变: LoG (Laplace of Gaussian), DoG(高斯差分)旋转不变和方向估计：主导方向（关键点周围的梯度平均最简单，也可以使用高斯加权函数；更稳定的方法是梯度方向直方图，使用梯度大小和高斯函数距离加权，找Top 80%的峰值以后用三分抛物线拟合计算出更准确的方向估计）仿射不变另一个重要的放射不变区域监测器是“最稳定极值区域”。4.1.2 特征描述子特征点的局部表观可能会在方向和尺度上变化，有时甚至存在仿射变形。提取局部尺度、方向或仿射的框架估计，然后在形成特征描述子之前使用它来对图像块重新采样比较可取。图像描述子既要对特征点的变化具有很好的不变性，也要对不同图像块之间保持区分性偏差和增益规范化（MOPS）尺度不变特征变换（SIFT）通过计算在检测到的关键点周围16*16窗口内每一个像素的梯度得到，使用检测到的关键点所在的高斯金字塔级别。通过高斯下降函数降低权重。计算梯度方向直方图。PCA-SIFT(主成分分析降维)梯度位置方向直方图（GLOH）导向滤波器（高斯导数滤波器的组合）局部描述子的性能(GLOH>SIFT)4.1.3 特征匹配两个阶段：1）选择一个匹配策略，确定哪些匹配将被传到下一阶段进行进一步处理 2）设计有效的数据结构和算法来尽可能快地完成这个匹配匹配策略和错误率TP: 正确肯定，正确匹配的数目FN：漏报，没有正确找到匹配的数目FP： 误报TN：正确否定(receiver operating characteristic, ROC曲线)三种匹配策略：1）固定阈值 2）最近邻 3）最近邻距离比率（NNDR）最近邻距离比率高效匹配对潜在候选进行高效搜索的办法是导出一个索引结构(多维搜索树，哈希表);一个比较简单的方法是多维散列，它基于施加在每一个描述子向量上的某些函数将描述子映射到一个固定大小的一些桶里。在匹配时，每一个特征被散列到一个桶中，然后在邻近的桶里进行搜索来返回潜在的候选者，然后将这些候选者排序或者打分以确定哪些是有效的匹配。散列的例子：Haar小波，局部敏感散列（locality sensitive hashing）,参数敏感的散列。多维搜索树：k-d树（kd-树），它将多维特征空间交替沿着轴对齐的超平面进行分割，沿着每一个轴选择阈值来最大化某个策略。特征匹配验证和紧致化：随机采样4.1.4 特征跟踪在第一幅图像中寻找可能的特征位置集合，然后在后续的图像中搜索它们的对应位置。“先检测后跟踪”(detect and track)。当然这里期望相邻帧之间的运动和表观的变形比较小。在两个方向上梯度值均大的区域，即自相关矩阵拥有大的特征值的区域，提供了可用于寻找对应的稳定的位置。在后续的帧中，搜索那些平方差小的对应图像块区域通常比较高效。如果图像的光照发生了变化，需要明确地补偿这些变化或者使用规范化互相关。分层搜索策略。使用低分辨率下的匹配来提供较好的初始猜测值，加速搜索过程。该策略的替代策略包括了解被跟踪块的表观应该是什么，然后在预测的位置附近搜索它。在特征的当前预测位置周围的一个区域内使用一种增量注册算法进行搜索，得到的跟踪器通常叫“Kanade-Lucas-Tomasi(KLT)跟踪器”特征跟踪最新的一个发展是使用学习算法来建立一个特殊的目的识别器，以在图像中的任意位置处快速寻找匹配的特征。通过花时间训练样本块与其仿射变形间的分类器，可以构建出非常快而稳定的特征检测器，这样就可以处理更快的运动变化。4.1.5 应用：表演驱动的动画（performance-driven animation）快速特征跟踪的最有趣的一个应用基于跟踪用户的运动，对一个3D图形模型进行交互式形变Eg.提取草图中眼睛嘴巴的位置-->绘制控制线-->确定运行时这些特征的当前位置-->根据所跟踪特征来计算全局的位置和方向-->得到的变形了的眼睛和嘴巴区域随后被复合到整体头部模型来生成一帧手绘的动画。"}
{"content2":"知识型企业研究中心 2006-12-26http://business.queensu.ca/index.phpQueen商务学校，任务是提高领导力的管理和促进商务和社会的发展。目前我们的研究工作...英国谢菲尔德大学自然语言处理研究组 2006-12-26http://nlp.shef.ac.uk/英国谢菲尔德大学自然语言处理研究组研究领域主要为：自然语言分析，自然语言的产生以及相关资...PC AI 2006-12-26http://www.pcai.com/在线免费电子期刊，除了包含每期期刊内容外，还包括一个AI讨论组，和经过整理的Intern...美国印地安那大学人工智能/认知科学报告和再版文件汇编 2006-12-26http://www.cs.indiana.edu/%7eleake/INDEX.html美国印地安那大学人工智能/认知科学报告和再版文件汇编，网站提供了PDF格式的文件，相关书...美国橡树岭国家实验室图像处理和机器视觉研究小组 2006-12-26http://www.ornl.gov/sci/ismv/美国橡树岭国家实验室图像处理和机器视觉研究小组，图像处理包括：机器视觉，图像管理和检索，...人工智能研究者俱乐部 2006-12-26http://www.souwu.com/分类论坛:自然语言语音识别论文资源相关编程专家系统知识表示机器学习神经网络数据挖掘模式识...DFKI人工智能研究所 2006-12-17http://www.dfki.uni-kl.de/与人交谈时，对方吐出一串叽哩咕噜的洋话、而你半个字也听不懂，怎么办呢？在过去，这可能需要...数据管理前言技术国际研讨会（中国，上海，2006） 2006-12-17http://www.iipl.fudan.edu.cn/DM06/index.htm2006年该会议的主题是网站管理和挖掘. 它包括6-8个主题,邀请了一系列的研究者和当地...媒体计算与WEB智能实验室(复旦大学) 2006-12-17http://www.cs.fudan.edu.cn/mcwil/irnlp/媒体计算与WEB智能实验室主要从事多媒体方向（包括文本、图象和视频）的教学和科研工作，研...奥地利人工智能研究所机器学习和数据挖掘小组 2006-12-11http://www.oefai.at/oefai/ml/mldm/研究区域包括数据挖掘和知识发现，文本挖掘，机器学习，此外还网站提供关于研究领域，相关人物...加拿大渥太华大学知识获取与智能化学习研究小组 2006-12-09http://www.site.uottawa.ca/tanka/kaml.html知识获取与智能化学习研究小组有他们的发展项目：智能化信息的获取项目，文本摘要项目，TAN...美国麻省理工大学生物与计算学习研究中心 2006-12-09http://cbcl.mit.edu/美国麻省理工大学生物与计算学习研究中心在麻州理工学院成立。主要从数学，工程学和神经学的角...德国乌尔姆大学人工神经网络小组 2006-12-09http://www.informatik.uni-ulm.de/ni/forschung/ann.html德国乌尔姆大学人工神经网络小组的研究重点在于神经网络，数据挖掘，信号处理等领域和方向的研...优秀知识发现网络 2006-12-09http://www.kdnet.org/优秀知识发现网络是一个开放的网络，它的参加者来自科学，工业和公共部门。这项国际项目的主要...奥地利维也纳医科大学脑研究中心医学控制和人工智能学院 2006-12-02http://www.ai.univie.ac.at/创建者是地利维也纳医科大学脑研究中心医学控制和人工智能学院，研究包括： 自然语言处理，机...奥地利维也纳医科大学脑研究中心医学控制和人工智能学院 2006-12-02http://www.ai.univie.ac.at/创建者是地利维也纳医科大学脑研究中心医学控制和人工智能学院，研究包括： 自然语言处理，机...美国伍斯特工学院人工智能研究小组 2006-12-02http://www.cs.wpi.edu/Research/airg/多主体系统，学习，单功能主体，智能界面，图标界面，专家系统，数据挖掘，知识库的设计等。微软研究－机器学习和应用统计研究小组 2006-12-02http://research.microsoft.com/research/mlas/机器学习和应用统计研究小组把重心集中在从数据和数据挖掘。藉由软件自动从数据中学习获取新信...英国爱丁堡大学信息学校人工智能应用学院 2006-12-02http://www.aiai.ed.ac.uk/情境基础的推论: 利用过去的经验和存在的技术指导诊断企业的资源过失；遗传基因的运算法...北京大学计算语言学研究所 2006-12-02http://www.icl.pku.edu.cn/北京大学计算语言学研究所成立于1986年。研究所的使命是致力于计算语言学理论、语言信息...哈尔滨工业大学智能技术与自然语言处理实验室 2006-12-02http://www.insun.hit.edu.cn/default_cn.asp哈尔滨工业大学计算机学院智能技术与自然语言处理研究室（ITNLP）是国内较早从事自然语言...加州大学伊荣/尔湾分校机器学习小组 2006-11-29http://www.ics.uci.edu/~mlearn/Machine-Learning.html机器学习是一种通过经验获取知识的机制。加州大学伊荣/尔湾分校机器学习小组的研究包括基于统...DMI:数据挖掘学院 2006-11-24http://www.cs.wisc.edu/dmi/数据挖掘研究所于1999年6月1日在微软的数据挖掘小组的帮助下在微软公司的计算机科学系成...数据挖掘：原理，算法及应用 2006-11-24http://www.cs.unc.edu/Courses/comp290-90-f04/这是北卡罗莱纳洲大学计算机科学系2004年关于数据挖掘的一系列的研讨会的网站。上面列出了...麻省理工学院开放课程--数据挖掘 2006-11-24http://www.core.org.cn/OcwWeb/Sloan...5-062Data-Mi...麻省理工学院的关于数据挖掘开放课程.上面列出了教学大纲、教学日程、讲义、作业、考试以及学...国家数据挖掘中心 2006-11-24http://www.ncdm.uic.edu/芝加哥的伊利诺伊大学的国家数据挖掘中心于1998年成立,提供资源研究、标准开发和推广高性...IBM智能情报系统研究中心 2006-11-24http://www.almaden.ibm.com/software/disciplines/iis/智能信息系统研究所主要在于设计维护隐私和数据所有权而不是妨碍资讯流通的信息系统.我们的工...清华大学知识工程研究室 2006-11-24http://keg.cs.tsinghua.edu.cn/清华大学计算机系软件所知识工程研究室以网络计算模式下知识处理为研究方向，以Java、XM...数据挖掘和数据仓库 2006-11-24http://www.crm2day.com/data_mining/这是一个关于CRM的网站。其中有在数据挖掘这一版块列出了许多著名的公司或者专家写的关于数...数据挖掘课程 2006-11-24http://cs.nju.edu.cn/zhouzh/zhouzh.files/course/dm.htm是南京大学的数据挖掘课程的网页,上面列出了基本的课程介绍,提供课件下载,还列出了其他国家...数据挖掘的连接 2006-11-24http://www.galaxy.gmu.edu/stats/syllabi/DMLIST.html该网页列出了关于数据挖掘的一系列链接数据挖掘的连接 2006-11-24http://www.galaxy.gmu.edu/stats/syllabi/DMLIST.html该网页列出了关于数据挖掘的一系列链接人工智能研究实验室 2006-11-17http://www.cs.iastate.edu/~honavar/aigroup.html人工智能研究实验室是爱荷华州立大学的计算智能、学习和发现中心的一部分.目前的研究包括:人...美国人工智能协会 2006-11-17http://www.aaai.org/home.html成立于1979年的美国人工智能协会(aaai)是一个非营利性的致力于推进科学认识的社会科...知识媒体学会 2006-11-16http://kmi.open.ac.uk/index.cfm研究与电视大学本身相关的区域: 认知的和学问科学,和多媒体。 研究包括下列的主题: 叙述...WEB数据挖掘实验室 2006-11-16http://www.wdmlab.cn/本Web数据挖掘实验室隶属于南京师范大学教育科学学院教育技术学系。实验室立足于我国基础教...Java资源网——Java数据挖掘 2006-11-16http://www.javaresource.org/data-mi...-mining-73.htmlJava资源网是由Java领域的爱好者组成的技术联盟,主要成员均来自java和相关领域的...中国科大博纳数据挖掘中心 2006-11-16http://bona.ustc.edu.cn/中国科大博纳数据挖掘中心（Bona Institute of Business Data...西南财经大学商务数据挖掘中心 2006-11-16http://riem.swufe.edu.cn/dataminingcenter/西南财经大学商务数据挖掘中心是一个应用研究机构，它和从事商务决策和数据挖掘的软件公司、...国际数据挖掘技术研究中心 2006-11-16http://59.77.6.145/dmlab/DesktopDefault.aspx数据挖掘技术及其应用实验室是厦门大学国家示范性软件学院软件研究与开发中心的一个重要的分...互联网数据挖掘服务中心 2006-11-16http://idm.yatio.com/index.html互联网数据挖掘服务中心（IDMSC）是以雅信核心搜索技术为依托，面向所有网络分众领域，为...中科院数据技术与知识经济研究中心 2006-11-16http://www.dtke.ac.cn/中国科学院数据技术与知识经济研究中心（CAS Research Center on Da...机器学习研究室 2006-11-15http://www.cald.cs.cmu.edu/这个机器学习研究室是卡内基梅隆大学计算机科学系的一个学术部门.我们集中有关于统计机器学习...数据挖掘工程小组 2006-11-15http://www.chem-eng.utoronto.ca/~datamining/数据挖掘工程组是基身于多伦多大学的化工和应用化学系.其目标是把背景不同的在各个领域研究数...查尔斯顿学院的信息发现 2006-11-15http://di.cofc.edu/信息发现是从现有的资料中,无论是以前贮存的、还是流经过沟通渠道的，去发现新的信息.如何运...2006年数据挖掘论坛 2006-11-14http://www.data-mining-forum.de/这次会议是每年召开的一系列的基于数据挖掘的工业会议的第六次会议,该会议每年都在国际活动方...数据挖掘 2006-11-14http://www.ccsu.edu/datamining这是ccsu的一个在线数据挖掘的项目,ccsu是唯一开办了在线数据挖掘科学硕士的学校.这...第四届全国搜索引擎和网上信息挖掘学术研讨会 2006-11-13http://www.sewm2006.sdu.edu.cn/全国搜索引擎和网上信息挖掘学术研讨会是网络信息应用领域的重要活动，其目的是：促进国内外相...第三届全国搜索引擎和网上信息挖掘学术研讨会 2006-11-13http://www.sewm2005.edu.cn/index.htm全国搜索引擎和网上信息挖掘学术研讨会是网络信息应用领域的重要活动，其目的是：促进国内外相...第二届全国搜索引擎和网上信息挖掘学术研讨会 2006-11-13http://www.scut.edu.cn/sewm2004/index.htm全国搜索引擎和网上信息挖掘学术研讨会是网络信息应用领域的重要活动，其目的是：促进国内外相...首届全国搜索引擎和网上信息挖掘学术研讨会 2006-11-13http://net.pku.edu.cn/~sewm/sewm2003.htm全国搜索引擎和网上信息挖掘学术研讨会是网络信息应用领域的重要活动，其目的是：促进国内外相...数据挖掘技能 2006-11-10http://www.statsoft.com/textbook/stdatmin.html这是一本关于数据挖掘的一本书的章节数据挖掘课堂笔记 2006-11-10http://infolab.stanford.edu/~ullman/mining/mining.html国外大学关于数据挖掘相关课程的课件。智能科学网站 2006-11-10http://www.intsci.ac.cn/这是一个关于智能科学的门户网站，主要介绍的有关智能科学的内容由智能系统、智能科学研究、智...数据挖掘词汇表 2006-11-10http://www.twocrows.com/glossary.htm数据挖掘的词汇表智能工具,数据挖掘,可视化2005国际会议 2006-11-09http://www.infonortics.com/idv/05pro.html2005年6月27-28号在美国费城召开的智能工具、数据挖掘和可视化国际会议。网站上...SIGIR2006会议网站 2006-11-07http://www.sigir2006.org/关于信息检索的会议网站，本年度的主题是用户交互与检索效率。该网站提供年度会议的论文目录，...数字经济研究中心 2006-11-07http://w4.stern.nyu.edu/ceder/网站简介：25年多来，纽约大学的Stern's数字经济研究中心已经利用信息技术站在了商业...原文挖掘和基于网页的信息检索参考书目 2006-11-07http://filebox.vt.edu/users/wfan/text_mining.html该网页提供了许多关于原文挖掘研评价和分析的连接。数据挖掘爱好者 2006-11-04http://datamining.diy.myrice.com/数据挖掘就是从海量的数据中找出潜在的有价值的信息。这是一门综合了统计学、数据库和人工智能...数据挖掘资源 2006-11-04http://www.opendata365.com/datamini...200506/235.html该网页提供了许多有关数据挖掘方面的链接，资源丰富。第七次国际数据仓库存储与知识发现会议 2006-11-04http://www.dwway.com/newcontent.php...5userid=corpid=主要介绍了会议的时间、地点、宗旨以及讨论的主要内容。数据挖掘：文本挖掘，数据挖掘和社会传媒 2006-11-04http://datamining.typepad.com/data_mining/这是一个私人博客，记录了作者研究方向的一些资料、信息。而作者主要的兴趣所在为：人工智能、...与统计相关的数据挖掘课件 2006-11-04http://www.autonlab.org/tutorials/这个网站提供了基于统计的数据挖掘各个方面的研究类的课件，包括概率论的基础、数据统计分析的...诊断试验评价与数据挖掘 2006-11-04http://statdtedm.6to23.com/该网站是个科研个人网（非商业盈利），目的是相互交流,共同提高；网站开辟的几个专题，如数据...统计分析与数据挖掘实验室 2006-11-04http://www.bistudy.com/该网站主要提供一些相关软件介绍及其下载，包括： 调查类软件 、 统计分析类软件 、...数据挖掘技术简介 2006-11-04http://www.itcomputer.com.cn/Databa...0601/78529.html数据挖掘是目前一种新的重要的研究领域。本文介绍了数据挖掘的概念、目的、常用方法、数据挖掘...数据挖掘技术简介（PPT） 2006-11-04http://eb.zzei.net/ebSimple/dss.pptPPT课件数据挖掘教程 2006-11-04http://www.sobooks.com/product_info...oducts_id/14953本书为数据挖掘的基础教程，是作者多年来从事数据挖掘和专家系统课程教学经验的总结。它从商业...数据挖掘 2006-11-04http://www.the-data-mine.com/这个网站是1994年4月建立的,主要是提供关于数据挖掘的信息,包括数据库中的数据挖掘和简...数据挖掘:实用机器学习工具和技术(第二版) 2006-11-04http://www.cs.waikato.ac.nz/~ml/weka/book.html一本关于数据挖掘的书籍的介绍数据挖掘讨论组 2006-11-04http://www.dmgroup.org.cn/数据挖掘讨论组网站建于2000年7月，是由复旦大学计算机系发起创建的。 该网站...数据挖掘研究院 2006-11-04http://www.dmresearch.net/数据挖掘研究院是由HAMMER_SHI于2004年4.17日搭建成立的数据挖掘研讨平台，...Lotus知识发现服务器 2006-11-04http://www.chinakm.com/share/list.asp?id=2579主要介绍了Lotus知识发现服务器及其功能和作用。知识发现新进展与成果概述 2006-11-04http://202.113.96.26/tjcbe/xueshubaogao/yangbingru.ppt主要介绍了知识发现的内涵与外延的扩展、挖掘知识类型扩展、方法技术扩展、应用及发展趋势以及...第四届知识发现与数据挖掘国际学术大会 2006-11-04http://www2.ccw.com.cn/1998/37/170858.shtml主要介绍了这次会议的8个专题介绍会，以及本届大会的几个特点。数据挖掘研究院 网摘 2006-11-04http://www.dmresearch.net/rss/关于一个动态搜集的有关数据挖掘资料的网页。数据挖掘 2006-11-03http://databases.about.com/od/datamining/about.com展示了原有的专题文章的收集、网络连接,以及专门讨论数据挖掘和数据仓库课...UCI数据库知识发现 2006-11-02http://kdd.ics.uci.edu/在线的大型数据库，包含多种类型的数据,分析任务、适用范围.本库的主要作用是作为基准测试,...关于应用解析的新闻以及商业资源 2006-11-02http://www.secondmoment.org/关于应用解析的新闻以及商业资源.强大日志内容混合了评论、技术、以及对知识发现和直接的知识..."}
{"content2":"去年横扫全球围棋九段高手的AlphaGo，把人类带入了对于未来生活的无限恐慌之中，科幻片中很多机器人大战人类的桥段，让我们自身产生了对于生存的恐惧，无论奇点何时降临，人工智能已经将人类带入了新的时代。正如发现蒸汽顶开锅盖到蒸汽机的发明，再到蒸汽机车等机器的发明，工业革命时代也并非一撮而就，而是经历了漫长的过程。我们毫不怀疑人工智能的未来会如何，但真正迎来人工智能带来的超级红利，这也是需要时间去演变。对于中国的互联网公司来说，人工智能的风口渐起，不管是从国家战略还是本身的企业发展，如何进行自己的顶层设计和具体应用实现，对于BAT级别的企业而言，是决定未来生死的战略，每次的信息披露都体现了当下国内人工智能发展状态。阿里提出“通往智能之路” 说明了什么?一年一度的IT领袖峰会在深圳开幕，今年的主题“迈进智能新时代”聚焦于互联网最热门的人工智能领域。但对于AI一词，马云却有不同的看法，他表示：未来要思考的不是artificial intelligence(人工智能)，而是Machine intelligence(机器智能)。1956年的达特茅斯会议上，科学家们定义了人工智能：尝试找到如何让机器使用语言、形成抽象和概念、解决现在人类还不能解决的问题、提升自己等等。对于当下的人工智能来说首要问题是让机器像人类一样能够表现出智能。时至今日，人类依旧为实现机器像人类一样表现出智能而努力，这是一个目标的设定：人工智能始终要走进人类的生活。这个与这两年大火的“IOT”物联网的概念有点像，也就是大家经常表述的“万物互联”时代，很显然，物联网的发展也并不如大家所预期的速度非常快，尽管已经有了60年以上的理论基础，或者打个比方，目前人工智能的完美程度如果按照100分来评判的话，可能正在从0到90分的路上，并且国内的企业甚至在某些方面是领先于世界的，但是0到90的过程会很快，而90分到100分的路上，却是越来越难，这也是市面上人工智能商用产品不多的原因。对于阿里的人工智能战略来看，刚刚结束的深圳云栖大会上，阿里也提出的“通往智能之路”的主题，透露出阿里对于未来的人工智能的布局方向，这与这次IT峰会的主题不谋而和：人工智能是阿里总体战略的一个重要基础，从马云去年提出了“五新”战略：新零售、新制造、新金融、新技术、新能源，他认为原来机器吃的是电，未来吃的是数据，这是机器变成了人工智能，数据变成了新的能源。而阿里巴巴也不再是一家电子商务公司，而是在做大量的基础设施建设，服务于未来的各行各业。人工智能的历史已经有60年历程，但制约其发展的是计算和数据能力，计算机技术的发展带来了颠覆性的机遇，阿里在这方面的不并不晚尤其在云的布局上，已经是基于全球的布局，阿里云的市场占有量已经是全球前三的位置，也就是所谓的3A阵营：AWS， Azure和Alibaba cloud并进，是国内其他公司所没有的前提和基础，“通往智能之路”说明了阿里的人工智能战略的格局，其实是基于全球的一个战略思维，并且是基于全行业的。智能医疗和智能制造 传递了什么声音?未来不是AI，而是MI： Machine intelligence(机器智能)，让机器成为人类的合作伙伴和好拍档，这并不是幻想，阿里已经进行了尝试和布局：在深圳的云栖大会上，阿里发布ET医疗大脑和ET工业大脑成为最大的亮点，阿里云总裁胡晓明现场介绍了ET医疗大脑如何辅助医生判断甲状腺结节点，大屏上投出的视频演示显示ET通过计算机视觉技术，在甲状腺B超影像上圈出结节点，并给出良性或者恶性的判断。阿里云宣布联合英特尔、linkdoc启动天池医疗AI系列赛，在这个汇集了6万多名AI算法科学家的平台上，寻找早期肺癌诊断的智能化判断最优算法，让机器可以通过原始CT影像图片协助医生进行诊断。现代医学帮助人类实现了寿命的延长，但是人类对于自身的认识还是很浅薄，除非非常有经验的医生，如果仅仅靠一张CT影像片就去判断患者的肿瘤是否癌变，这在过去几乎不可想象，一方面是医疗成像水平的落后，当今医疗造影CT已经能把指定位置的器官以及血管、神经系统的影响通过3D的方式判断，而人工智能的介入，通过大量的案例深度学习，可以把更多量级的数据进行统计和分析，找出共通点，这样的判断对于诊断尤为重要，往常可能需要“拉一刀”，切除组织进行生物化验才能得到结果，现在能准确的提前判断，这是人工智能带来的革命性变化，我们甚至可以想象，基于未来是否能进行靶向治疗，对于人类攻克疑难杂症，人工智能给予了更多的预期。人工智能对于制造业的影响，也是非常巨大，如果制造业能够整体提升1%的良品率，按2016年全国工业总产值计算，这将为中国制造总体提升上万亿的利润空间，其实这样的提升不仅仅是成本的提升，还有效率。阿里发布的ET工业大脑通过分析工业生产中收集的数据，优化机器的产出和减少废品成本。通过并不昂贵的传感器、智能算法和强大的计算能力，ET工业大脑解决的是制造业的核心问题，这意味着大多数现代化工厂，并不需要另外高额的投入，就能把生产效率和成本控制到一个绝佳的状态。阿里云发布ET医疗大脑和ET工业大脑，可以预见，加上去年发布的城市大脑，阿里云的人工智能大脑，还将出现在更多的行业里，对于这些行业是颠覆性的提升，用21世纪的人工智能技术去解决20世纪遗留的问题，这是阿里云在人工智能上体现的巨大价值显现。从ET到NASA， 技术成为第五大经济体核心不久前，阿里巴巴董事局主席马云在内部启动代号为“NASA”的计划，面向未来20年储备核心科技，机器学习、IoT、生物识别等领域智能化领域被放在突出位置。这也就意味着阿里的最终目标是：全球商业史上将首次出现一个由企业单体推动、多方共同参与的世界级贸易市场，也就是第五大经济体，技术将会是长期的战略。马云从不掩饰自己对于未来的野心和战略判断，阿里目前的体量和发展轨迹其实都证明了之前的判断是正确的，在“五新”理论之前，2010年马云就提出了做互联网的“水电”基础设施，这才奠定了现在阿里的市场地位，这么来看NASA，马云提出的技术储备，包括未来针对这些技术的开发，实际上又是围绕未来20年的世界第五大经济体进行技术布局：互联网技术改变了世界贸易格局，在人工智能大数据之下，全球经济的发展已经变成了一个地球村，中国人已经能坐在家中，享用刚刚从远海运送到家的澳龙，而俄罗斯的姑娘也在利用淘宝购买中国制造，表面上，这是电商平台的作用，归根结底，还是基于制造业、物流行业、金融行业等领域的互联网变革，真正形成了物理世界的互联。从城市大脑，再到阿里云发布ET医疗大脑和ET工业大脑，大会上智能医疗和智能制造展示的成果，为技术驱动的未来打开了无数想象空间：NASA展现了未来阿里在技术储备和行业布局的格局，听上去有点：“虚”，但是这样的“虚”已经变成了基于城市数据管理、工业和医疗的深度场景应用，已经让“虚”变成了实实在在的变革。更重要的一点，运用越早，越有先发优势，这对于人工智能尤为关键：不仅仅是基于市场风口时间的抢占，还有一点是对于数据的获取，以医疗而言：中国的人口基数，各类疾病的病例要比国外多不少，这给未来攻克这些疾病提供了很多关键数据，这也意味着，阿里对于行业的捆绑，其实也是对于人工智能未来技术驱动的“数据”新资源的抢占。如此来看，阿里的“通往智能之路”对于未来世界的想象空间已经展开：这是基于全球、全行业的新技术革命，这些才是阿里的“杀手锏”，NASA计划是阿里对于“第五大经济体”的技术储备，这一切正如马云在大会上演讲所表示，是对于未来十五年后所考虑，未雨绸缪，对于马云如何通过MI改变人类生活，值得拭目以待。"}
{"content2":"本节课主要讲述了cs231n课程的背景和计算机视觉的历史，也主要介绍了目前很重要的一个计算机视觉数据集——IMAGENET。更多内容参考我的AI学习之路课程简介这门课程是由stanford大学计算机视觉李飞飞以及她的学生制作的，也叫做CS231n，是偏专业性的深度学习+计算机视觉课程。目前计算机视觉越来越火，因为互联网中大量的信息都是通过图像或者视频来传播的，在计算机视觉中也涉及到了各个学科的内容，比如机器学习、信息检索、系统架构、图形学、算法、神经学、图像处理、自然语言处理、机器人等等。计算机视觉的历史和发展从微生物进化，到照相机，再到生物学对视觉的研究，1963年计算机视觉发表了第一篇论文《Block world》，其中视觉世界被简化为简单的几何形状。1966年MIT暑期开启了一个视觉项目，之后很多的视觉科学家开始加入到视觉研究中。David Marr在70年代写了一本视觉很有影响的书《VISION》，描述了视觉的抽象过程：构建草图、形成2.5D的分层、构建3D模型。70年代另一个非常重要的工作是提出一个通过简单的方块连接世界的问题。60年代到70年代，解决物体识别非常困难，因为没有样本。1997年Berkeley的Jitendra Malik和他的学生Jianbo Shi完成了图像的分割Normalized Cut。。2001年Viola&Jones研究了Face Detection，Paul Viola基于AdaBoost进行实时面部检测，后来直接应用到了照相机中。。1999年 David Lowe剔除SIFT算法，用于物体的识别。。2006年Schmid&Ponce剔除空间金字塔 Spatial Pyramid Matching 。2005年Dalal&Triggs剔除HOG，histogram of gradients HoG。2009年Felzenswlb McAllester Ramanan剔除肢体模型2006年，公开了一个数据集用于专门测试物体识别算法，PASCAL，Visual Object Challenge，有20个物体类别。同时，普林斯顿和斯坦福开始着手做了另一个数据集——IMAGENET，里面包含了22K个类别，14M的图片。每个图片中会包含很多物品，如果输出最大概率的5个物品，其中包含正确物品，就认为是识别成功，以此来衡量准确性。2012年IMAGENET的错误率明显下降，因为使用了卷积神经网络，从此深度学习开始大放光彩。不过卷积网络实际上是90年代就提出的：计算机视觉常用的数据集PASCALPASCAL从2005年开始到2012年，每年都会推出一个新的数据集，这些数据除了05和06，其他的都是20个分类。其中提供了几个方向的数据集：图像分类、图像分割、动作分类、人体主要部位识别等等。IMAGENET这个数据集是普林斯顿和斯坦福发起的，每年都会举办一次比赛——超越 ILSVRC，Beyond ImageNet Large Scale Visual Recogition Challenge。不过2017年貌似是最后一届了。IMAGENET比赛中每年都会公开Top5的错误率，在2015年已经达到了3.57%，这些主要都是由于深度学习的发展，尤其是2012年的卷积网络，也使得深度学习开始流行起来。也是在这个比赛中，各种深度学习网络模型出现：AlexNet，GoogleNet，ResNet等等。"}
{"content2":"计算机视觉中的边缘检测边缘检测是计算机视觉中最重要的概念之一。这是一个很直观的概念，在一个图像上运行图像检测应该只输出边缘，与素描比较相似。我的目标不仅是清晰地解释边缘检测是怎样工作的，同时也提供一个新而又容易的方法只需要最小工作来明显地提高边缘检测。通过获得这些边缘，许多计算机算法才得以有可能实现，因为在一个场景中边缘包含着绝大部分（至少很多）的信息。举个例子，我们都记得 Windows XP 的那个绿色小山坡和蓝色天空的背景。当我们的大脑试图去理解这个场景时，我们知道这是草地，看起来很统一。然后，我们看到了飘着些许白云的天空。这些对象的每一个都是分离的，而且它们之间有一个边缘。这就是场景中为什么绝大部分的信息存在与边缘中。这也是为什么边缘检测是计算机视觉中的重要概念。通过把图像减少到只剩下边缘，使得对于许多算法更容易识别、学习、或者处理场景。边缘检测：滤波边缘检测的大多数是基于滤波的。通常来说，滤波是一个消除的动作。比如：过滤水，是消除寄生虫。相似地，当我们尝试找到图像边缘时，我是在尝试消除掉除图像边缘之外的东西。消除那些不是有用的边缘的图像部分，而留下合适的边缘也是困难所在。我怎么知道这是不是有用的边缘，比如，我对 Windows XP 背景运行 Canny edge dectector 程序，效果如下。你能看到那些小草的细小刀刃似的边缘，这很让人讨厌，而且没有真正提供有用的信息。甚至那些云朵也不是非常清晰。现在，你能在大多数的 Canny edge dectector 上设置一些边界，这些边界设置了阈值（或者 非极大值抑制），所有的边缘必须满足这个阈值才能划分为“重要的”边缘。与其在 Canny edge 的阈值上区分，不如我们更广泛地来谈谈，并且搭建几个滤波器。边缘检测：高斯滤波器高斯滤波器是边缘检测最基础的滤波器之一，虽然还有其他的，但是高斯滤波器会贯穿这篇文章。高斯滤波器，正如其名，是一个基于高斯分布的滤波器。它看起来像一个抛物线（除了在二维的情况）。通过矩阵乘法，高斯滤波器能被应用到每一个像素上。它实现的是混合效果，让最中心的像素尽量小基于它相邻的像素。举个例子，如对我的猫的图像运行一个均匀分布的高斯滤波器，我能够得到下面的图像：5.jpg你可以看到图像变得模糊了，高斯滤波器就是获取所有像素，让像素值的成分是与相邻像素有关。为了让高斯滤波器在边缘检测中有效，我们可以使用从 x 和 y 方面求导出来的高斯滤波器。这也许听起来有点反直觉，或者不知所谓，但是如果我们看到这种导数高斯滤波器的图像，这种想法就很清晰了。当你把一个高斯的 x 和 y 分量求导后，一个大的波峰 和波谷就出现了。如果你明白导数，思考下这种情况，你应该能很快想到。由于高斯函数峰上数值上的巨大变化，从而导致了高斯导数波峰和波谷的出现。如果我们把上面的写成代码，那相当直接（至少对于 Matlab 和 Python 来说）：% Takes the derivative of a 5x5 gaussian, with a sigma [hx, hy] = gradient(fspecial('gaussian',[5 5],sigma));这就是了，一行代码就能得到我们所想要的高斯，然后对x 和 y 分量求导。边缘检测：应用滤波器我们已经有了两个高斯滤波器，我们把他们应用到图像中。我们同样使用非极大值限制，即如果不是极大值的话，就把像素值设置为0。换种说法就是消除噪声。应用滤波器的代码如下：% Convert an image to double for increased precision img = double(img); % Find two derived gaussians with respect to x and y [hx, hy] = gradient(fspecial('gaussian',[5 5],sigma)); % Run the filters over the image, generating a filtered image % Leaves x edges gx = double(imfilter(img,hx,'replicate', 'conv')); % Leaves y edges gy = double(imfilter(img,hy,'replicate', 'conv')); % Take the absolute value, and combine the x and y edges mag = sqrt((gx .* gx) + (gy .* gy)); % Use non-maxima suppression [mag, ] = max(mag, [], 3);如果我们将它应用我的猫的一张图片，我我们就得到一下的图片：有趣地是，我们同样也能这种方法应用到 RGB 图像上去，同样也会得到彩色的边缘。普通的边缘滤波器应用到猫的RGB图像这两张图片应该都能代表像素和它相邻之间像素的颜色的差异，只不过彩色图像有3层，而黑白图像只有1层。（层，翻译不是很好）边缘检测：方向滤波器为什么要限制我们自己仅仅使用绝对的 x 和 y 方向的滤波器呢？让我们也来构建一些方向滤波器。这个方法（或多或少）来源于 Freeman 和 Adelson 的《The design and use of steerable filters》论文，这个想法使我们能够把我们的高斯滤波器放置在不同的方向。根本上，我们把我们的高斯滤波器放置到不同的方向上去，以基于与高斯相关的边缘的角度来创建不同数值。比如,如果我们把高斯函数放置到45度，应用到45度角的图像上，应该可以比在0度的高斯函数得到更高的数量级。在这种情况下，我生成了几种不同的方向滤波器：上行x分量，下行y分量各种各样的高斯函数产生了相对于x 和 y 分量上的90、45、-45 和 22.5 度的滤波器。这也就产生了不同的边缘大小，尽管这些滤波器应该检测出相近的边缘。我使用的代码与单个滤波器使用的代码几乎相同，但是我不同以往地把他们混合在一起。它看起来有点杂乱，但是我通过让我运行每个滤波器表达更明白来尽可能使它看起来更清晰。% Create four filters [hx, hy] = gradient(fspecial('gaussian',[5 5],sigma)); [hx1, hy1] = altOrientFilter1(hx, hy); [hx2, hy2] = altOrientFilter2(hx, hy); [hx3, hy3] = altOrientFilter3(hx, hy); % Run first gaussian filter on image gx = double(imfilter(img,hx,'replicate', 'conv')); gy = double(imfilter(img,hy,'replicate', 'conv')); % Run second gaussian filter on image gx1 = double(imfilter(img,hx1,'replicate', 'conv')); gy1 = double(imfilter(img,hy1,'replicate', 'conv')); % Run third gaussian filter on image gx2 = double(imfilter(img,hx2,'replicate', 'conv')); gy2 = double(imfilter(img,hy2,'replicate', 'conv')); % Run fourth gaussian filter on image gx3 = double(imfilter(img,hx3,'replicate', 'conv')); gy3 = double(imfilter(img,hy3,'replicate', 'conv')); % Merge all filters squareGD = (gx .* gx) + (gy .* gy); squareGD = squareGD + (gx1 .* gx1) + (gy1 .* gy1); squareGD = squareGD + (gx2 .* gx2) + (gy2 .* gy2); squareGD = squareGD + (gx3 .* gx3) + (gy3 .* gy3); % Run non-maxima supression [mag, ] = max(sqrt(squareGD), [], 3);如果你靠近点看的话，你能看到数量大小的不同，特别是那些皱纹。如果我们混合了所有图像，我们就能得到一张轻微较好的边缘检测。方向滤波器和非方向滤波器之间并没有很大的不同，但是我们也应该看到多种方向的结果有些许的提高。边缘检测：提高彩色域过去两年，我在不同的彩色域上做了大量的测试和实验。特别地是，Lab 彩色域是另一种描述图像的方式。比如，我们知道的RGB 和灰度图像，或者你甚至可能知道YUV空间。 Lab 彩色域与之非常相似。我对Lab 彩色感兴趣的原因是它对产生场景的边缘有着优异的能力。Lab 彩色空间的每个字母表示：L —— Luminance亮度a —— alpha —— 红到绿b —— beta —— 黄到蓝事实上，这些颜色通道非常适合发现颜色变化梯度，正自然而然地黄色很少出现另一个黄色周围，红色和绿色也是如此。（尽管我已经彻底证明了）。Lab 彩色空间与我们人类怎样察觉颜色中的亮度有很强的相关性。与RGB相反的是，在Lab 彩色空间中亮度有它自己的分离通道，这使得它能更好地处理颜色的差异，这些差异也是亮度与颜色相关之处。为了最小化额外的代码，我们所要做的就是把输入的图像转化为Lab彩色空间的图像。你可以做一些最优化，不过你仅仅做了这额外的一步也能明显提高适宜的边缘检测。% Convert an image to the Lab color space colorTransform = makecform('srgb2lab'); img = applycform(rgbImg, colorTransform); % Make it double to improve representation img = double(img); % Find x and y derivative of a 9x9 gaussian [hx, hy] = gradient(fspecial('gaussian',[9 9],sigma)); % Apply filters gx = double(imfilter(img,hx,'replicate')); gy = double(imfilter(img,hy,'replicate')); % Find absolute value gSquared = sqrt(gx .* gx) + (gy .* gy); % Apply non-maxima suppression (find best points for edges) [mag, ] = max(gSquared, [], 3);如果我们将 Windows XP 小山坡的背景图像转换为Lab，我们将得到下面的图像：Windows XP 背景的Lab空间图像然后如果我们施加滤波器（非极大值抑制）将得到下面的图像，清晰的包含着草地、云、草地和天空的分界线的图象。在Lab 空间上边缘检测最终，如果我们运行非极大值抑制，那么我们将得到比本文开头提到的 Canny边缘检测器要好得多的边缘效果。Lab 彩色空间边缘检测平均来说，这种方法会比普通方法提高 10% 左右的边缘检测精度。这是对 The Berkeley Segmentation Dataset and Benchmark 运行 F-measure 测试得到的结果。边缘检测：结语有无数的方法去做边缘检测，这里讲述的方法绝不是最好、最容易实现、最容易解释的。我使用这些方法解释它是因为我对它们很有兴趣。。。加上这是 UIUC的CS543课程《计算机视觉》布置的作业，所以你也在上这门课，请不要抄袭我的代码！我已经把我们所有的实现都放到了 github 上了。包括用 C++ 的 OpenCV 实现。然而，如果你想跟我的猫照相，这没什么问题。建议阅读PCA: Principal Component AnalysisEveryday Algorithms: Pancake SortUsing Computer Vision to Improve EEG SignalsIntroduction to Markov ProcessesThe Cache and Multithreading参考文献[1] Canny, John. “A computational approach to edge detection.” Pattern Analysis and Machine Intelligence, IEEE Transactions on 6 (1986): 679-698.[2] Freeman, William T., and Edward H. Adelson. “The design and use of steerable filters.” IEEE Transactions on Pattern analysis and machine intelligence 13.9 (1991): 891-906.转自：计算机视觉中的边缘检测 - yewei11的专栏 - CSDN博客 http://blog.csdn.net/yewei11/article/details/50748012原文链接请点击原文链接Edge Detection in Computer Vision http://austingwalters.com/edge-detection-in-computer-vision/"}
{"content2":"今日CS.CV计算机视觉论文速览Tue, 26 Mar 2019Totally 78 papers👉昨日速览, ✈更多精彩请移步主页Interesting:📚SRFBN基于反馈网络的图像超分辨, 基于反馈机制和RNN序列来提升图像超分辨效果(from 四川大学)code:https://github.com/Paper99/SRFBN_CVPR19📚DeepRED,利用去噪的正则化来处理图像逆问题，包括去噪、修复、超分辨等。可以看做是deep image prior的改进 (from Technion,，谷歌)ref:https://arxiv.org/pdf/1806.02296.pdf稀疏表示和变换：http://www4.comp.polyu.edu.hk/~cslzhang/NCSR.htmhttp://www.cs.tut.fi/~foi/GCF-BM3D/相关数据:📚DenseBody, 单张照片生成人体位姿和外形。(from 云从科技)相关数据集：Human3.6M, SURREAL and UP-3D📚MeshGAN, 非线性3D人脸形变模型，提出了直接处理三维数据的MeshGan(from 帝国理工)相关数据集合:3dMD,4DFAB相关方法：CoMA相关三维卷积：Spectral graph CNNs. Bruna et al. [12] ，ChebNet. Defferrard et al. [22]，f BEGAN [6]ref:三维运动捕捉公司：http://www.3dmd.com/📚基于R-CNN的虹膜检测系统, 将虹膜作为两个同心圆进行分割,从眼睛图像中分割出不包括瞳孔的虹膜图像(from 昆山杜克大学)相关数据集：(i) NICE-II and (ii) MICHE (GS4, IP5 and GT2).📚ShopSign, 一个多样性的街道招牌中文文字识别数据集，包含了25362个商店门牌和196010文字行，来源于多种手机拍摄的数据，以及多种不同材质的招牌。(from 河南大学 )code and dataset:https://github.com/chongshengzhang/shopsign相关方法： CTPN, TextBoxes++ and EAST相关数据集：CTW,RCTW,CVPR2018mtmiDaily Computer Vision Papers1.Title: DeepCenterline: a Multi-task Fully Convolutional Network for Centerline ExtractionAuthors:Zhihui Guo, Junjie Bai, Yi Lu, Xin Wang, Kunlin Cao, Qi Song, Milan Sonka, Youbing Yin2.Title: CODA: Counting Objects via Scale-aware Adversarial Density AdaptionAuthors:Li Wang, Yongbo Li, Xiangyang Xue3.Title: Scale-Adaptive Neural Dense Features: Learning via Hierarchical Context AggregationAuthors:Jaime Spencer, Richard Bowden, Simon Hadfield4.Title: Locomotion and gesture tracking in mice and small animals for neurosceince applications: A surveyAuthors:Waseem Abbas, David Masip Rodo5.Title: ShopSign: a Diverse Scene Text Dataset of Chinese Shop Signs in Street ViewsAuthors:Chongsheng Zhang, Guowen Peng, Yuefeng Tao, Feifei Fu, Wei Jiang, George Almpanidis, Ke Chen6.Title: MeshGAN: Non-linear 3D Morphable Models of FacesAuthors:Shiyang Cheng, Michael Bronstein, Yuxiang Zhou, Irene Kotsia, Maja Pantic, Stefanos Zafeiriou7.Title: Structured 2D Representation of 3D Data for Shape ProcessingAuthors:Kripasindhu Sarkar, Elizabeth Mathews, Didier Stricker8.Title: Noise-Tolerant Paradigm for Training Face Recognition CNNsAuthors:Wei Hu, Yangyu Huang, Fan Zhang, Ruirui Li9.Title: Apple Leaf Disease Identification through Region-of-Interest-Aware Deep Convolutional Neural NetworkAuthors:Hee-Jin Yu, Chang-Hwan Son10.Title: CoSegNet: Deep Co-Segmentation of 3D Shapes with Group Consistency LossAuthors:Chenyang Zhu, Kai Xu, Siddhartha Chaudhuri, Li Yi, Leonidas Guibas, Hao Zhang11.Title: MetaPruning: Meta Learning for Automatic Neural Network Channel PruningAuthors:Zechun Liu, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang, Tim Kwang-Ting Cheng, Jian Sun12.Title: Convolutional neural network for breathing phase detection in lung soundsAuthors:Cristina Jácome, Johan Ravn, Einar Holsbø, Juan Carlos Aviles-Solis, Hasse Melbye, Lars Ailo Bongo13.Title: Learning from Adversarial Features for Few-Shot ClassificationAuthors:Wei Shen, Ziqiang Shi, Jun Sun14.Title: Manifold Criterion Guided Transfer Learning via Intermediate Domain GenerationAuthors:Lei Zhang, Shanshan Wang, Guang-Bin Huang, Wangmeng Zuo, Jian Yang, David Zhang15.Title: Physics-based Neural Networks for Shape from PolarizationAuthors:Yunhao Ba, Rui Chen, Yiqin Wang, Lei Yan, Boxin Shi, Achuta Kadambi16.Title: Accurate Global Trajectory Alignment using Poles and Road MarkingsAuthors:Haohao Hu, Marc Sons, Christoph Stiller17.Title: Dual Variational Generation for Low-Shot Heterogeneous Face RecognitionAuthors:Chaoyou Fu, Xiang Wu, Yibo Hu, Huaibo Huang, Ran He18.Title: DeepRED: Deep Image Prior Powered by REDAuthors:Gary Mataev, Michael Elad, Peyman Milanfar19.Title: A Novel Method for the Absolute Pose Problem with Pairwise ConstraintsAuthors:Yinlong Liu, Xuechen Li, Manning Wang, Guang Chen, Zhijian Song, Alois Knoll20.Title: Looking Fast and Slow: Memory-Guided Mobile Video Object DetectionAuthors:Mason Liu, Menglong Zhu, Marie White, Yinxiao Li, Dmitry Kalenichenko21.Title: LOGAN: Unpaired Shape Transform in Latent Overcomplete SpaceAuthors:Kangxue Yin, Zhiqin Chen, Hui Huang, Daniel Cohen-Or, Hao Zhang22.Title: Efficient Tracking Proposals using 2D-3D Siamese Networks on LIDARAuthors:Jesus Zarzar, Silvio Giancola, Bernard Ghanem23.Title: Down-Scaling with Learned Kernels in Multi-Scale Deep Neural Networks for Non-Uniform Single Image DeblurringAuthors:Dongwon Park, Jisoo Kim, Se Young Chun24.Title: DenseBody: Directly Regressing Dense 3D Human Pose and Shape From a Single Color ImageAuthors:Pengfei Yao, Zheng Fang, Fan Wu, Yao Feng, Jiwei Li25.Title: SAC-Net: Spatial Attenuation Context for Salient Object DetectionAuthors:Xiaowei Hu, Chi-Wing Fu, Lei Zhu, Pheng-Ann Heng26.Title: Depth Augmented Networks with Optimal Fine-tuningAuthors:Tasfia Shermin, Manzur Murshed, Shyh Wei Teng, Guojun Lu27.Title: PI-REC: Progressive Image Reconstruction Network With Edge and Color DomainAuthors:Sheng You, Ning You, Minxue Pan28.Title: Weakly-Supervised Unconstrained Action Unit Detection via Feature DisentanglementAuthors:Zhiwen Shao, Jianfei Cai, Tat-Jen Cham, Xuequan Lu, Lizhuang Ma29.Title: Iris R-CNN: Accurate Iris Segmentation in Non-cooperative EnvironmentAuthors:Chunyang Feng, Yufeng Sun, Xin Li30.Title: Combining Transfer Learning And Segmentation Information with GANs for Training Data Independent Image RegistrationAuthors:Dwarikanath Mahapatra, Zongyuan Ge31.Title: f-VAEGAN-D2: A Feature Generating Framework for Any-Shot LearningAuthors:Yongqin Xian, Saurabh Sharma, Bernt Schiele, Zeynep Akata32.Title: Recurrent Back-Projection Network for Video Super-ResolutionAuthors:Muhammad Haris, Greg Shakhnarovich, Norimichi Ukita33.Title: Knowledge-driven Encode, Retrieve, Paraphrase for Medical Image Report GenerationAuthors:Christy Y. Li, Xiaodan Liang, Zhiting Hu, Eric P. Xing34.Title: Residual Non-local Attention Networks for Image RestorationAuthors:Yulun Zhang, Kunpeng Li, Kai Li, Bineng Zhong, Yun Fu35.Title: Convolutional Neural Networks for Multi-class Histopathology Image ClassificationAuthors:Muhammed Talo36.Title: Needle in a Haystack: A Framework for Seeking Small Objects in Big DatasetsAuthors:Joel Brogan, Aparna Bharati, Daniel Moreira, Kevin Bowyer, Patrick Flynn, Anderson Rocha, Walter Scheirer37.Title: PLMP - Point-Line Minimal Problems in Complete Multi-View VisibilityAuthors:Timothy Duff, Kathlén Kohn, Anton Leykin, Tomas Pajdla38.Title: Cluster Alignment with a Teacher for Unsupervised Domain AdaptationAuthors:Zhijie Deng, Yucen Luo, Jun Zhu39.Title: Joint Learning of Discriminative Low-dimensional Image Representations Based on Dictionary Learning and Two-layer Orthogonal ProjectionsAuthors:Xian Wei, Hao Shen, Yuanxiang Li, Xuan Tang, Bo Jin, Lijun Zhao, Yi Lu Murphey40.Title: Periphery-Fovea Multi-Resolution Driving Model guided by Human AttentionAuthors:Ye Xia, Jinkyu Kim, John Canny, Karl Zipser, David Whitney41.Title: Variational Inference with Latent Space Quantization for Adversarial ResilienceAuthors:Vinay Kyatham, Prathosh A. P., Deepak Mishra, Tarun Kumar Yadav, Dheeraj Mundhra42.Title: KPTransfer: improved performance and faster convergence from keypoint subset-wise domain transfer in human pose estimationAuthors:Kanav Vats, Helmut Neher, Alexander Wong, David A. Clausi, John Zelek43.Title: SRGAN: Training Dataset MattersAuthors:Nao Takano, Gita Alaghband44.Title: Efficiently utilizing complex-valued PolSAR image data via a multi-task deep learning frameworkAuthors:Lamei Zhang, Hongwei Dong, Bin Zou45.Title: sharpDARTS: Faster and More Accurate Differentiable Architecture SearchAuthors:Andrew Hundt, Varun Jain, Gregory D. Hager46.Title: Distributed Lossy Image Compression with Recurrent NetworksAuthors:Enmao Diao, Jie Ding, Vahid Tarokh47.Title: An End-to-end Framework For Integrated Pulmonary Nodule Detection and False Positive ReductionAuthors:Hao Tang, Xingwei Liu, Xiaohui Xie48.Title: Automatic Pulmonary Lobe Segmentation Using Deep LearningAuthors:Hao Tang, Chupeng Zhang, Xiaohui Xie49.Title: Automated pulmonary nodule detection using 3D deep convolutional neural networksAuthors:Hao Tang, Daniel R. Kim, Xiaohui Xie50.Title: StartNet: Online Detection of Action Start in Untrimmed VideosAuthors:Mingfei Gao, Mingze Xu, Larry S. Davis, Richard Socher, Caiming Xiong51.Title: Monocular 3D Object Detection with Pseudo-LiDAR Point CloudAuthors:Xinshuo Weng, Kris Kitani52.Title: Rotated Feature Network for multi-orientation object detectionAuthors:Zhixin Zhang, Xudong Chen, Jie Lie, Kaibo Zhou53.Title: Curve Text Detection with Local Segmentation Network and Curve ConnectionAuthors:Zhao Zhou, Shufan Wu, Shuchen Kong, Yingbin Zheng, Hao Ye, Luhui Chen, Jian Pu54.Title: 1D-Convolutional Capsule Network for Hyperspectral Image ClassificationAuthors:Haitao Zhang, Lingguo Meng, Xian Wei, Xiaoliang Tang, Xuan Tang, Xingping Wang, Bo Jin, Wei Yao55.Title: Color Filter Arrays for Quanta Image SensorsAuthors:Omar A. Elgendy, Stanley H. Chan56.Title: Feedback Network for Image Super-ResolutionAuthors:Zhen Li, Jinglei Yang, Zheng Liu, Xiaomin Yang, Gwanggil Jeon, Wei Wu57.Title: Semantic denoising autoencoders for retinal optical coherence tomographyAuthors:Max-Heinrich Laves, Sontje Ihler, Lüder Alexander Kahrs, Tobias Ortmaier58.Title: Spatially-weighted Anomaly Detection with Regression ModelAuthors:Daiki Kimura, Minori Narita, Asim Munawar, Ryuki Tachibana59.Title: An End-to-End Network for Generating Social Relationship GraphsAuthors:Arushi Goel, Keng Teck Ma, Cheston Tan60.Title: What Synthesis is Missing: Depth Adaptation Integrated with Weak Supervision for Indoor Scene ParsingAuthors:Keng-Chi Liu, Yi-Ting Shen, Jan P. Klopp, Liang-Gee Chen61.Title: Auto-ReID: Searching for a Part-aware ConvNet for Person Re-IdentificationAuthors:Ruijie Quan, Xuanyi Dong, Yu Wu, Linchao Zhu, Yi Yang62.Title: Fast Underwater Image Enhancement for Improved Visual PerceptionAuthors:Md Jahidul Islam, Youya Xia, Junaed Sattar63.Title: Scene Understanding for Autonomous Manipulation with Deep LearningAuthors:Anh Nguyen64.Title: Photorealistic Style Transfer via Wavelet TransformsAuthors:Jaejun Yoo, Youngjung Uh, Sanghyuk Chun, Byeongkyu Kang, Jung-Woo Ha65.Title: Trifocal Relative Pose from Lines at Points and its Efficient SolutionAuthors:Ricardo Fabbri, Timothy Duff, Hongyi Fan, Margaret Regan, David da Costa de Pinho, Elias Tsigaridas, Charles Wrampler, Jonathan Hauenstein, Benjamin Kimia, Anton Leykin, Tomas Pajdla66.Title: Residual Pyramid Learning for Single-Shot Semantic SegmentationAuthors:Xiaoyu Chen, Xiaotian Lou, Lianfa Bai, Jing Han67.Title: Fast LLMMSE filter for low-dose CT imagingAuthors:Fengling Wang, Bowen Lin, Shujun Fu, Shiling Xie, Zhigang Zhao, Yuliang Li68.Title: Generative Adversarial Minority OversamplingAuthors:Sankha Subhra Mullick, Shounak Datta, Swagatam Das69.Title: Capsule Networks with Max-Min NormalizationAuthors:Zhen Zhao, Ashley Kleinhans, Gursharan Sandhu, Ishan Patel, K. P. Unnikrishnan70.Title: Exploiting Excessive Invariance caused by Norm-Bounded Adversarial RobustnessAuthors:Jörn-Henrik Jacobsen, Jens Behrmannn, Nicholas Carlini, Florian Tramèr, Nicolas Papernot71.Title: The functional role of cue-driven feature-based feedback in object recognitionAuthors:Sushrut Thorat, Marcel van Gerven, Marius Peelen72.Title: Wav2Pix: Speech-conditioned Face Generation using Generative Adversarial NetworksAuthors:Amanda Duarte, Francisco Roldan, Miquel Tubau, Janna Escur, Santiago Pascual, Amaia Salvador, Eva Mohedano, Kevin McGuinness, Jordi Torres, Xavier Giro-i-Nieto73.Title: Cyclical Annealing Schedule: A Simple Approach to Mitigating KL VanishingAuthors:Hao Fu*, Chunyuan Li*, Xiaodong Liu, Jianfeng Gao, Asli Celikyilmaz, Lawrence Carin74.Title: End-to-End Learning Using Cycle Consistency for Image-to-Caption TransformationsAuthors:Keisuke Hagiwara, Yusuke Mukuta, Tatsuya Harada75.Title: One time is not enough: iterative tensor decomposition for neural network compressionAuthors:Julia Gusak, Maksym Kholyavchenko, Evgeny Ponomarev, Larisa Markeeva, Ivan Oseledets, Andrzej Cichocki,76.Title: BitSplit-Net: Multi-bit Deep Neural Network with Bitwise Activation FunctionAuthors:Hyungjun Kim, Yulhwa Kim, Sungju Ryu, Jae-Joon Kim77.Title: Improving Adversarial Robustness via Guided Complement EntropyAuthors:Hao-Yun Chen, Jhao-Hong Liang, Shih-Chieh Chang, Jia-Yu Pan, Yu-Ting Chen, Wei Wei, Da-Cheng Juan78.Title: Progressive DNN Compression: A Key to Achieve Ultra-High Weight Pruning and Quantization Rates using ADMMAuthors:Shaokai Ye, Xiaoyu Feng, Tianyun Zhang, Xiaolong Ma, Sheng Lin, Zhengang Li, Kaidi Xu, Wujie Wen, Sijia Liu, Jian Tang, Makan Fardad, Xue Lin, Yongpan Liu, Yanzhi WangPapers from arxiv.org更多精彩请移步主页pic from pixels.com"}
{"content2":"人工智能：计算机视觉、图像处理、模式识别、机器学习之间的关系什么是人工智能呢？人工智能，是由人类设计并在计算机环境下实现的模拟或再现某些人智能行为的技术。一般认为，人类智能活动可以分为两类：感知行为与思维活动。模拟感知行为的人工智能研究的一些例子包括语音识别、话者识别等与人类的听觉功能有关的“计算机听觉”，物体三维表现的形状知识、距离、速度感知等与人类视觉有关的“计算机视觉”，等等。模拟思维活动的人工智能研究的例子包括符号推理、模糊推理、定理证明等与人类思维有关的“计算机思维”，等等。人工智能领域：机器学习 深度学习 图像算法 图像处理 语音识别 图像识别 算法研究从图像处理和模式识别发展起来的计算机视觉研究对象之一是如何利用二维投影图像恢复三维景物世界。计算机视觉使用的理论方法主要是基于几何、概率和运动学计算与三维重构的视觉计算理论，它的基础包括射影几何学、刚体运动力学、概率论与随机过程、图像处理、人工智能等理论。计算机视觉要达到的基本目的有以下几个：(1) 根据一幅或多幅二维投影图像计算出观察点到目标物体的距离；(2) 根据一幅或多幅二维投影图像计算出目标物体的运动参数；(3) 根据一幅或多幅二维投影图像计算出目标物体的表面物理特性；(4) 根据多幅二维投影图像恢复出更大空间区域的投影图像。计算机视觉要达到的最终目的是实现利用计算机对于三维景物世界的理解，即实现人的视觉系统的某些功能。在计算机视觉领域里，医学图像分析、光学文字识别对模式识别的要求需要提到一定高度。又如模式识别中的预处理和特征抽取环节应用图像处理的技术；图像处理中的图像分析也应用模式识别的技术。在计算机视觉的大多数实际应用当中，计算机被预设为解决特定的任务，然而基于机器学习的方法正日渐普及，一旦机器学习的研究进一步发展，未来“泛用型”的电脑视觉应用或许可以成真。人工智能所研究的一个主要问题是：如何让系统具备“计划”和“决策能力”？从而使之完成特定的技术动作（例如：移动一个机器人通过某种特定环境）。这一问题便与计算机视觉问题息息相关。在这里，计算机视觉系统作为一个感知器，为决策提供信息。另外一些研究方向包括模式识别和机器学习（这也隶属于人工智能领域，但与计算机视觉有着重要联系），也由此，计算机视觉时常被看作人工智能与计算机科学的一个分支。机器学习是研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演译。为了达到计算机视觉的目的，有两种技术途径可以考虑。第一种是仿生学方法，即从分析人类视觉的过程入手，利用大自然提供给我们的最好参考系——人类视觉系统，建立起视觉过程的计算模型，然后用计算机系统实现之。第二种是工程方法，即脱离人类视觉系统框框的约束，利用一切可行和实用的技术手段实现视觉功能。此方法的一般做法是，将人类视觉系统作为一个黑盒子对待，实现时只关心对于某种输入，视觉系统将给出何种输出。这两种方法理论上都是可以使用的，但面临的困难是，人类视觉系统对应某种输入的输出到底是什么，这是无法直接测得的。而且由于人的智能活动是一个多功能系统综合作用的结果，即使是得到了一个输入输出对，也很难肯定它是仅由当前的输入视觉刺激所产生的响应，而不是一个与历史状态综合作用的结果。不难理解，计算机视觉的研究具有双重意义。其一，是为了满足人工智能应用的需要，即用计算机实现人工的视觉系统的需要。这些成果可以安装在计算机和各种机器上，使计算机和机器人能够具有“看”的能力。其二，视觉计算模型的研究结果反过来对于我们进一步认识和研究人类视觉系统本身的机理，甚至人脑的机理，也同样具有相当大的参考意义。在我的理解里，要实现计算机视觉必须有图像处理的帮助，而图像处理倚仗与模式识别的有效运用，而模式识别是人工智能领域的一个重要分支，人工智能与机器学习密不可分。纵观一切关系，发现计算机视觉的应用服务于机器学习。各个环节缺一不可，相辅相成。计算机视觉（computer vision），用计算机来模拟人的视觉机理获取和处理信息的能力。就是是指用摄影机和电脑代替人眼对目标进行识别、跟踪和测量等机器视觉，并进一步做图形处理，用电脑处理成为更适合人眼观察或传送给仪器检测的图像。计算机视觉研究相关的理论和技术，试图建立能够从图像或者多维数据中获取‘信息’的人工智能系统。计算机视觉的挑战是要为计算机和机器人开发具有与人类水平相当的视觉能力。机器视觉需要图象信号，纹理和颜色建模，几何处理和推理，以及物体建模。一个有能力的视觉系统应该把所有这些处理都紧密地集成在一起。图像处理（image processing），用计算机对图像进行分析，以达到所需结果的技术。又称影像处理。基本内容图像处理一般指数字图像处理。数字图像是指用数字摄像机、扫描仪等设备经过采样和数字化得到的一个大的二维数组，该数组的元素称为像素，其值为一整数，称为灰度值。图像处理技术的主要内容包括图像压缩，增强和复原，匹配、描述和识别3个部分。常见的处理有图像数字化、图像编码、图像增强、图像复原、图像分割和图像分析等。图像处理一般指数字图像处理。模式识别(Pattern Recognition)是指对表征事物或现象的各种形式的(数值的、文字的和逻辑关系的)信息进行处理和分析,以对事物或现象进行描述、辨认、分类和解释的过程,是信息科学和人工智能的重要组成部分。模式识别又常称作模式分类，从处理问题的性质和解决问题的方法等角度，模式识别分为有监督的分类（Supervised Classification）和无监督的分类(Unsupervised Classification)两种。模式还可分成抽象的和具体的两种形式。前者如意识、思想、议论等,属于概念识别研究的范畴,是人工智能的另一研究分支。我们所指的模式识别主要是对语音波形、地震波、心电图、脑电图、图片、照片、文字、符号、生物传感器等对象的具体模式进行辨识和分类。模式识别研究主要集中在两方面,一是研究生物体(包括人)是如何感知对象的，属于认识科学的范畴,二是在给定的任务下,如何用计算机实现模式识别的理论和方法。应用计算机对一组事件或过程进行辨识和分类，所识别的事件或过程可以是文字、声音、图像等具体对象，也可以是状态、程度等抽象对象。这些对象与数字形式的信息相区别，称为模式信息。模式识别与统计学、心理学、语言学、计算机科学、生物学、控制论等都有关系。它与人工智能、图像处理的研究有交叉关系。机器学习(Machine Learning)是研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演绎。机器学习在人工智能的研究中具有十分重要的地位。一个不具有学习能力的智能系统难以称得上是一个真正的智能系统，但是以往的智能系统都普遍缺少学习的能力。随着人工智能的深入发展，这些局限性表现得愈加突出。正是在这种情形下，机器学习逐渐成为人工智能研究的核心之一。它的应用已遍及人工智能的各个分支，如专家系统、自动推理、自然语言理解、模式识别、计算机视觉、智能机器人等领域。机器学习的研究是根据生理学、认知科学等对人类学习机理的了解，建立人类学习过程的计算模型或认识模型，发展各种学习理论和学习方法，研究通用的学习算法并进行理论上的分析，建立面向任务的具有特定应用的学习系统。这些研究目标相互影响相互促进。人类研究计算机的目的，是为了提高社会生产力水平，提高生活质量，把人从单调复杂甚至危险的工作中解救出来。今天的计算机在计算速度上已经远远超过了人，然而在很多方面，特别是在人类智能活动有关的方面例如在视觉功能、听觉功能、嗅觉功能、自然语言理解能力功能等等方面，还不如人。这种现状无法满足一些高级应用的要求。例如，我们希望计算机能够及早地发现路上的可疑情况并提醒汽车驾驶员以避免发生事故，我们更希望计算机能帮助我们进行自动驾驶，目前的技术还不足以满足诸如此类高级应用的要求，还需要更多的人工智能研究成果和系统实现的经验。转载时请以超链接形式标明文章原始出处和作者信息及本声明http://www.blogbus.com/shijuanfeng-logs/216968430.html"}
{"content2":"目前，人工智能（AI）非常热门，许多人都想一窥究竟。如果你对人工智能有所了解，但对机器学习（Machine Learning）的理解有很多的困惑，那么看完本文后你将会对此有进一步深入理解。在这里，不会详细介绍机器学习算法的基本原理，而是通过将比较有意思的视频（YouTube）和文字相结合，逐渐增加对机器学习基础的理解。当看到本文时，请坐下来放松一下。因为下面的这些视频需要花费一点时间，但看完视频后，你肯定会被吸引继续阅读下去。此外，当阅读完本文后，你应该会对现在最热门的技术——机器学习有了全面的知识基础，并对此产生学习热情，最终能学到什么程度完全取决于个人的努力，本文只是一块敲门砖。为什么机器学习现在如此热门人工智能总是很酷，从科幻电影到现实中的阿法狗、聊天机器人等，一直吸引人们的关注。长久以来，人们认为人工智能一直围绕着程序员对某些事情应该如何表现的功能性猜测。然而，程序员并不总是像我们经常看到的那样对人工智能编程同样有着天赋。正如我们经常看到的那样，比如谷歌“史诗游戏失败”中在人工智能、物理、有时甚至是经验丰富的人类玩家中都存在有过失。无论如何，人工智能有一种新的天赋——通过该项技术，我们可以教电脑玩游戏、理解语言、甚至识别人或物。这个只显露冰山一角的新技术来源一个旧的概念——机器学习，直到最近几年，它才获得了理论之外的处理能力，这源于数据量的爆炸、计算机性能的提升以及算法理论的突破。通过人工智能这项技术，我们不再需要人为地提出高级算法，只需要教会计算机自己来提出高级算法即可。那么这样的事情是如何实现的呢？机器学习算法并没有真正被类似于程序员编程那样进行编写，而是自动生成。观看下面这个简短的视频，该视频为创建人工智能的高级概念提供了出色的注释和动画。是不是一个很疯狂的处理过程？并且，当算法完成后，我们甚至无法理它，它就像一个黑匣子。比如，该项技术应用于视觉领域中是用人工智能玩马里奥游戏。作为一个人，我们都知道如何躲避障碍物和吃金币，但人工智能识别所产生的预测策略是疯狂的，见下面的视频：是不是很吃惊？看完上述视频后，我们的问题是对机器学习不了解，并且不知道如何将它与电子游戏联系起来。为什么要使用机器学习？关于为什么要关心机器学习，这里有两个很好的答案。首先，机器学习使计算机可以做到计算机以前不能实现的事情。如果你想尝试一些新事物，或者不仅仅是新事物，而是影响全世界，你都可以用机器学习来完成。其次，如果你不影响世界，世界将影响你。现在，很多大型公司在机器学习上投入了很多的研发和投资，我们已经看到它正在改变世界。思想领袖警告我们不能让这个新的算法时代存在于公众视线之外。想象一下，如果一些企业巨头控制着互联网，如果我们不掌握这项武器，科学的真理将不会被我们占据。Christian Heilmann在他关于机器学习的谈话中说得很好：“我们能够希望其他人善用这种力量。对于个人而言，不要认为这是一个好的赌注。我宁愿玩，也要参加这场科技革命，你也应该参与。”——Chris Heilmann的机器学习谈话视频对机器学习感兴趣机器学习这个概念很有用而且很酷，上述内容让我们比较抽象地了解了它，但机器学习算法究竟发生了什么？它是如何运作的？我们还不是很清楚。如果你想直接进入到理论研究，建议你跳过这一部分继续下一个“如何开始”部分。如果你有动力成为机器学习的实干者，那么就不需要看下面的视频了。如果你仍然试图了解机器学习可能是什么，下面的使用机器学习完成数字手写体识别的视频非常适合引导读者建立一种机器学习的逻辑：是不是很酷？该视频显示每个层变得更简单，而不是变得更复杂。就像函数将数据分解成较小的部分一样，以抽象的概念结束。你可以在该网站（Adam Harley）与此流程进行交互。此外，机器学习的经典实例之一是1936年的鸢尾花数据集。在参加JavaFXpert的机器学习概述的演示中，我学会了如何使用工具来可视化调整和反向传播神经网络上神经元的权重。可视化过程可以让我们看到它是如何训练神经模型。使用Jim可视化工具训练鸢尾花神经网络即使你不是一个Java爱好者，Jim提供了一个1.5小时的机器学习概念介绍也是比较有用的，其中包含上述许多例子的更多详细信息。这些概念令人兴奋，你准备好成为这个新时代的爱因斯坦吗？机器学习算法每天都在发生突破，所以现在就开始吧。如何开始？目前，网络上有大量的资源可用。首先，应该订阅一些时事通讯、技术博客、微信公众号，以保持个人知识的滚动。比如medium、爱可可-爱学习、云栖社区等。至于如何进行深入学习，我推荐下面两种方法：从头到尾拧完n颗螺栓在这种方法中，将需要你全面了解机器学习算法和相关的数学知识。我知道，这种方式听起来很难完成，但要想真正地了解算法细节，就必须从头开始编码实现。如果你想成为机器学习中的一员，并在核心圈中占据一席之地，那么这就是你的选择。我建议你试试一些公开课app（比如，course、Brilliant.org），并参加人工神经网络课程。经典的网络课程主推Andrew Ng 老师的机器学习课程以及周志华老师的书籍等。在学习的同时，可以完成对应的线下作业。通过完成对应的作业，会进一步加深对知识的理解，因为这些作业并不简单。但更重要的是，如果确实完成了这项工作，你将对机器学习的实施有进一步深刻的理解，这将使得你以新的和改变世界的方式成功地将其应用到对应的场景中。快速上手如果你对编写算法并不感兴趣，但仍想要使用它们来创建一个令人惊叹的网站/应用程序，你应该跳转到学习TensorFlow和对应的速成课程。TensorFlow是用于机器学习的开源软件库。如果选修课程不适合你的学习方式，那你仍然是很幸运的。如今不必学习机器学习的细节就可以掌握如何使用它。此外，还可以通过多种方式有效地机器学习作为服务成为技术巨头。数据是这项技术很重要的原材料，如果你的数据比较合适，那么使用机器学习建模可能是最佳解决方案。无论是使用机器学习中的哪一种算法，现在就开始吧。成为创造者我要对所有上述的人和视频说声谢谢，它们是我学习机器学习起步的灵感，虽然我在该领域仍然是个新手，但是当我们拥抱这个令人敬畏的时代时，我很高兴为他人指明一条学习道路。如果你想学习这门技术，就必须与机器学习领域的研究者有所联系。没有友好的面孔、回答和讨论，任何事情都将变得很难。一般技术圈的人都是比较热心肠的，遇到问题先google，找不到答案就咨询圈内人，相信会有友好的同行给出友好的建议。我希望这篇文章能激励你和周围的人学习机器学习，我也很乐意和你一起寻找酷炫有趣的机器学习代码，希望本文对你有所帮助。原文链接本文为云栖社区原创内容，未经允许不得转载。"}
{"content2":"在机器学习、计算机视觉和人工智能领域，顶级会议才是王道。国内教材和CNKI上的至少是N年前的东西。有人会质疑这些会议都只是EI。是的，这的确非常特殊：在别的大部分领域，会议都是盛会，比如society of neuroscience的会议，每次都有上万人参加，带个abstract和poster就可以去。但在所讨论的几个领域，顶级会议的重要性无论怎么强调都不为过。可以从以下几点说明：（1）因为机器学习、计算机视觉和人工智能领域发展非常迅速，新的工作层出不穷，如果把论文投到期刊上，一两年后刊出时就有点out了。因此大部分最新的工作都首先发表在顶级会议上，这些顶级会议完全能反映“热门研究方向”、“最新方法”。（2）很多经典工作大家可能引的是某顶级期刊上的论文，这是因为期刊论文表述得比较完整、实验充分。但实际上很多都是在顶级会议上首发。比如PLSA, Latent Dirichlet Allocation等。（3）如果注意这些领域大牛的pulications，不难发现他们很非常看重这些顶级会议，很多人是80%的会议+20%的期刊。即然大牛们把最新工作发在顶级会议上，有什么理由不去读顶级会议？以下是不完整的列表，但基本覆盖。机器学习顶级会议：NIPS, ICML, UAI, AISTATS;  （期刊：JMLR, ML, Trends in ML, IEEE T-NN）计算机视觉和图像识别：ICCV, CVPR, ECCV;  （期刊：IEEE T-PAMI, IJCV, IEEE T-IP）人工智能：IJCAI, AAAI; （期刊AI）另外相关的还有SIGRAPH, KDD, ACL, SIGIR, WWW等。特别是，如果做机器学习，必须地，把近4年的NIPS, ICML翻几遍；如果做计算机视觉，要把近4年的ICCV, CVPR, NIPS, ICML翻几遍。Zisserman还有一个不错的学生、lifeifei的合作者Rob Fergus按研究方向分分， 应该更合理一些。现在计算机视觉， 计算机图形图像， 机器学习开始融合到一起了吧。J. Malik，Zhu Songchu偏segmentation;D. Lowe, S. Ullman, Poggio 偏于从生物视觉的启发来研究视觉；Zisserman, Schmid, Lowe研究局部特征；Luc Van Goo， Long Quanl三维重建；Perona, Li Feife， Freeman视觉学习， 物体分类；还有运动分析， 视觉跟踪，纹理分析………….MIT的Brain & Cognitive Science Dept和CSAIL里面聚集了一帮人，有的作low level有的作mid level to high level的。他们的工作是值得关注的。当然说视觉还是要从伟大的David Marr开始。Tomaso Poggio, Richard Whitman是Marr的同事，传承了其理念，一直往下走。Poggio最近几年比较重点的工作放在他那个hierarchical model上。T. Poggio的第一个PhD学生是Christof Koch （kLab at Caltech）。哦，顺便说一下Koch的另外一个导师是Valentino Breitenberg——同样是影响了一个时代的大人物。Koch研究重点兴趣在consciousness上，在Nature上的很多文章体现了他的研究思想。不过他们也做不少初级的视觉问题，诸如attention。Koch比较知名的弟子比如Laurent Itti和Li Feifei。Richard Whitman 年纪比较大了，个人不是很关注他现在做的东西。不过他所在的Perceptual Science Group，是一个非常有影响力的地方，这个组其他大家比较熟悉的老师有Aude Oliva和EH Adelson。Adelson最著名的一个事儿是色彩恒常相关的视错觉，93年发在Science上的那篇。关于Oliva，前面的帖子错了，她不是Poggio的学生，这家伙和Torralba是老乡，同在法国念书，主要从心理学那条路子开始做，成名之役是hybrid image，和Antonio Torrralba一起搞的。这个Hybrid Image 其实80年代就有了，但是最开始从心理学方向上探讨，没有非常有影响力的文章。后来开始靠谱作自然图像统计，得到Gist theory，当然这个illusion本身后来转投SIGGRAPH，其影响是深远的。嗯，这个和CV关系不大。Perceptual Science Group出了不少牛人，他们的alumni list可谓超豪华阵容：Yair Weiss, Josh Tenenbaum, Pawan Sinha, Bill Freeman……机器学习与计算机视觉大牛族谱 收藏包括Phd和Post-doc, co-supervise关系David Marr(MIT)--------Shimon Ullman(Weizmann)---------Daniel Huttenlocher(Cornell)---------Demetri Terzopoulos(UCLA)--------Dimitris Metaxas(Rutgers)-------Eric Grimson(MIT)------Pedro Felzenszwalb(Uchicago)------Polina Golland(MIT)-----Xiaogang Wang(CUHK)---------Daniel Huttenlocher(Cornell)Marvin Minsky(MIT,Turning Award)-----Berthold Horn(MIT)-----Tomas Lozano-Perez(MIT)-----John Canny(UC Berkeley)(Canny edge detector)-----Paul Viola(Microsoft)Olivier Faugeras(Uparis XI)-----Martial Hebert(CMU)-------Derek Hoiem(UIUC)-----Jean Ponce(UIUC)------Lazbnik(UNC)(Spatial Pyramid Matching)-----Zhengyou Zhang(Microsoft)Takeo Kanade(CMU)------Qifa Ke(Microsoft)-------Shree Nayar(Columbia)--------Srinivasa Narasimhan(CMU)-------Simon Baker(Microsoft)-------Richard Szeliski(Microsoft)-------Carlo Tomasi(Duke)-------James Rehg(GIT)------Jianxin Wu(NTU)Whitman Richards(MIT)-----Alex Pentland(MIT)-----Trevor Darrell(UC Berkeley)------Kristen Grauman(UTexas)(Pyramid Matching Kernel)------Joshua Tenenbaum(MIT)(Isomap)-----Tom Griffiths(UC Berkeley)Andrew Blake(Microsoft)-----Andrew Zisserman(Oxford)------Andrew Fitzgibbon(Microsoft)------Josef Sivic(INRIA)(Video Google)------Rob Fergus(NYU)------Roberto Cipolla(Cambridge)------Alan Yuille(UCLA)Thomas Huang(UIUC)------Yong Rui(Microsoft)------Ying Wu(Northwestern)------Qi Tian(UTSA)------ Chang Wen Chen (Buffalo)------- Jiebo Luo (Kodak)------Nebojsa Jojic(Microsoft)------Vladimir Pavlovic(Rutgers)------Shuicheng Yan(NUS)David Mumford(Brown, Fields Medal)-----SongChun Zhu(UCLA)Azriel Rosenfeld(Maryland)------Narendra Ahuja(UIUC)------Charles Dyer(Wisconsin)------Larry Davis(Maryland)------Shmuel Peleg(Hebrew)Christopher Brown(Rochester)-------Yiannis Aloimonos(Maryland)------Robert Pless(WUSTL)------LoongFah Cheong(NUS)Thomas Binfold(Stanford)----David Kriegman(UCSD)----David Lowe(UBC)(SIFT)----Jitendra Malik(UC Berkeley)-------Pietro Perona(Caltech)-------Fei-fei Li(Stanford)------Stefano Soatto(UCLA)------Max Welling(UCI)------Jianbo Shi(Upenn)(Ncut)-------Yizhou Yu(UIUC)-------Serge Belongie(UCSD)(Shape Context)------Alexei Efros(CMU)-------Derek Hoiem(UIUC)------Alexander Berg(Stony brook)------Christoph Bregler(NYU)Edward Adelson(MIT)-----Willian Freeman(MIT)-----Ce Liu(Microsoft)----- Rob Fergus(NYU)-----Yais Weiss(Hebrew)-------Anat Levin(Weizmann)-----Kevin Murphy(UBC)------Josef Sivic(INRIA)------Antonio Torralba(MIT)(GIST)------Eirk Sudderth(Brown)Tomaso Poggio(MIT)(HMAX)-----[Partha Niyogi](UChicago)------Mikhail Belkin(OSU)(LE)------Xiaofei He(ZJU)(LPP)------Christof Koch(Caltech)------Tomas Serre(Brown)Michael Jordan(UC Berkeley)------------Tommi Jaakkola(MIT)------Martin Wainwright(UC Berkeley)-----Xuanlong Nguyen(UMich)------Andrew Ng(Stanford)------Honglak Lee(Umich)------Lawrence Saul(UCSD)------Fei Sha(USC)------David Blei(Princeton)(LDA)------Eric Xing(CMU)------Ben Taskar(Upenn)------Yair Weiss(Hebrew)(HDP)-----Erik Sudderth(Brown)-----Yoshua Bengio(Umontreal)-----Francis Bach(INRIA)-----Kevin Murphy(UBC)Daphne Koller(Stanford)-----Nir Friedman(Hebrew)-----Carlos Guestrin(CMU)-----Ben Taskar(Upenn)(M3N)Geoffrey Hinton(Toronto)-----Yee-Whye The(UCL)-----Yann Lecun(NYU)------Zoubin Ghahramani(CMU,Cambridge)-----Max Welling(UCI)-----[Sam Roweis](NYU)(LLE)Tom Mitchell(CMU)-----Oren Etzioni(UWashington)-----Geoffrey Gordon(CMU)-----Sebasitian Thrum(Stanford)-----Andrew McCallum(Umass)John Lafferty(CMU)(CRF)-----Chengxiang Zhai(UIUC)-----Qiaozhu Mei(Umich)-----Xiaojin Zhu(Wisconsin)-----David Blei(Princeton)A tree stucture of cv guys.David Marr—–>Shimon Ullman (Weizmann)—–>Eric Grimson (MIT)—–>Daniel Huttenlocher (Cornell)—–>Pedro Felzenszwalb (Chicago)Thomas Binford (Stanford)—–>David Lowe (UBC)—–>Jitendra Malik (UC Berkeley)—–>Pietro Perona (Caltech)—–>Stefano Soatto (UCLA)—–>Fei-Fei Li (Princeton)—–>Jianbo Shi (UPenn)—–>Yizhou Yu (UIUC)—–>Christoph Bregler (NYU)—–>Serge Belongie (UCSD)—–>Alyosha Efros (CMU)Andrew Blake (Microsoft Research Cambridge)—–>Andrew Zisserman (Oxford)—–>Andrew Fitzgibbon (Microsoft Research Cambridge)—–>Roberto Cipolla (Cambridge)—–>Alan Yuille (UCLA)(UK这个学派的师承关系不太清楚, 这是我听说加上自己猜测的. 事实上, 几个非常优秀的researcher如Vladimir Kolmogorov虽然不是Andrew Blake的学生, 但是也属于这个学派. )Thomas Huang (UIUC)—–>Yong Rui (Microsoft Research Redmond)—–>Nebojsa Jojic (Microsoft Research Redmond)—–>Ying Wu (Northwestern University)—–>Hai Tao (UCSC)—–>Yuncai Liu (SJTU)(Huang这个系的人太多, 而且很怪的是, UIUC的web上信息不全, 在此仅列出我知道的.)此外, 还有Takeo Kanade等非常有名的大牛, 囿于篇幅, 不一一列举.从上得知, 加州派基本占了cv的半壁江山. 最近几年, 特别活跃的cv guys是USAJitendra Malik, UC BerkeleyPietro Perona, CaltechSerge Belongie, UCSDJianbo Shi, UPennStefano Soatto, UCLAFei-Fei Li, PrincetonWilliam Freeman, MITTrevor Darrell, MITSimon Baker, CMUYanxi Liu, CMUSongchun Zhu, UCLAAlan Yuille, UCLAYi Ma, UIUCMichael Black, BrownCarlo Tomasi, DukeRamin Zabih, CornellShree Nayar, ColumbiaRama Chellappa, MarylandSteve Seitz, University of WashingtonEuropeAndrew Zisserman, Oxford, UKAndrew Fitzgibbon, Microsoft Research Cambridge, UKRoberto Cipolla, Cambridge, UKJean Ponce, INRIA, FranceCordelia Schmid, INRIA, FranceBill Triggs, LEAR, FranceYair Weiss, Hebrew University, IsraelAnat Levin, Hebrew University, IsraelMichal Irani, Weizmann, IsraelLuc van Gool, University of Leuven/ETH Zurich, CzechicChinaHarry Shum, MSRAXiaoou Tang, MSRA/CUHKJian Sun, MSRASteve Lin, MSRAYasuyuki Matsushita, MSRAZhouchen Lin, MSRALong Quan, HKUSTChi-Keung Tang, HKUSTOlivier Faugeras—Ponce UIUC—lazebnik—Zhengyou Zhang MSR—Martial Hebert CMUMit AI labpoggio—Oliva—serreFreeman 80年代还来太原理工扶贫了—Y. Weiss—Levin—Antonio Torralba （research scientist）Trevor Darrell—Graumanhttp://doctorimage.cn/2012/11/19/maching-learning-computer-vison-family-sites/这里添加了牛人以及实验室的链接"}
{"content2":"作者戴金艳，公众号：计算机视觉life， 编辑部成员.首发原文链接计算机视觉方向简介 | 图像拼接简介图像拼接是将同一场景的多个重叠图像拼接成较大的图像的一种方法，在医学成像、计算机视觉、卫星数据、军事目标自动识别等领域具有重要意义。图像拼接的输出是两个输入图像的并集。通常用到五个步骤：特征提取 Feature Extraction：在所有输入图像中检测特征点图像配准 Image Registration：建立了图像之间的几何对应关系，使它们可以在一个共同的参照系中进行变换、比较和分析。大致可以分为以下几个类直接使用图像的像素值的算法,例如,correlation methods在频域处理的算法,例如,基于快速傅里叶变换(FFT-based)方法;低水平特征的算法low level features,通常用到边缘和角点，例如，基于特征的方法,高水平特征的算法high-level features,通常用到图像物体重叠部分，特征关系，例如，图论方法（Graph-theoretic methods）图像变形 Warping：图像变形是指将其中一幅图像的图像重投影，并将图像放置在更大的画布上。图像融合 Blending图像融合是通过改变边界附近的图像灰度级，去除这些缝隙，创建混合图像，从而在图像之间实现平滑过渡。混合模式(Blend modes)用于将两层融合到一起。特征点提取特征是要匹配的两个输入图像中的元素，它们是在图像块的内部。这些图像块是图像中的像素组。对输入图像进行Patch匹配。具体解释如下: 如下图所示，fig1和fig2给出了一个很好的patch匹配，因为fig2中有一个patch看起来和fig1中的patch非常相似。当我们考虑到fig3和fig4时，这里的patch并不匹配，因为fig4中有很多类似的patch，它们看起来与fig3中的patch很相似。由于像素强度很相近，所以无法进行精确的特征匹配，为了给图像对提供更好的特征匹配，采用角点匹配，进行定量测量。角点是很好的匹配特性。在视点变化时，角点特征是稳定的。此外，角点的邻域具有强度突变。利用角点检测算法对图像进行角点检测。角点检测算法有Harris角点检测算法、SIFT特征点检测算法((Scale Invariant Feature Transform),FAST算法角点检测算法，SURF特征点检测算法(Speeded-up robust feature)Harris角点检测算法Harris算法是一种基于Moravec算法的点特征提取算法。1988年C. Harris 和 M.J Stephens设计了一种图像局部检测窗口。通过在不同的方向上移动少量窗口，可以确定强度的平均变化。我们可以通过观察小窗口内的强度值很容易地识别角点。在移动窗口时，平坦区域在所有方向上均不会显示强度的变化。边缘区域在沿边缘方向强度不会发生变化。对于角点，则在各个方向上产生显著强度变化。Harris角点探测器给出了一种检测平坦区域、边缘和角点的数学方法。Harris检测的特征较多，具有旋转不变性和尺度变异性。位移\\([u, v]\\)下的强度变化:\\[E(u,v)=∑_{x,y}w(x,y)[I(x+u,y+v)−I(x,y)]^2\\]其中，\\(w(x,y)\\)是窗口函数，\\(I(x+u,y+v)\\)是移动后的强度，\\(I(x,y)\\)是单个像素位置的强度。Harris角点检测算法如下：对图像中的每个像素点\\((x,y)\\)计算自相关矩阵\\(M\\)（autocorrelation matrix M）:\\[M=\\sum_{x,y} \\begin{bmatrix}I_{x}^{2} & I_{x}I_{y}\\\\ I_{x}I_{y} & I_{y}^{2}\\end{bmatrix}\\]其中\\(I_{x},I_{y}\\)是\\(I(x,y)\\)的偏导数对图像中的每个像素点做高斯滤波，获得新的矩阵\\(M\\)，离散二维零均值高斯函数为\\[Gauss = exp(-u^2+v^2)/2\\delta^2\\]计算每个像素点(x,y)的角点度量，得到\\[R=Det(M)-k*trace(M)\\]，\\(k\\) 的范围是\\(0.04≤k≤0.06\\)。选择局部最大值点。Harris方法认为特征点与局部最大兴趣点的像素值对应。设置阈值T，检测角点。如果 \\(R\\) 的局部最大值高于阈值\\(T\\)，那么此点为角点。SIFT角点检测算法SIFT算法是尺度不变的特征点检测算法，可用于识别其他图像中的相似目标。SIFT的图像特征表示为关键点描述符（key-point-descriptors）。在检查图像匹配时，将两组关键点描述符作为输入提供给最近邻搜索(Nearest Neighbor Search，NNS)，并生成一个紧密匹配的关键点描述符（matching key-point-descriptors）。SIFT的计算分为四个阶段:尺度空间构造（Scale-space construction）尺度空间极值检测(Scale-space extrema detection)关键点定位(key-point localization)方向分配(orientation assignment)和关键点描述符定义(defining key-point descriptors)第一阶段确定潜在的兴趣点。它利用高斯函数的差分(difference of Gaussian function,DOG)搜索所有尺度和图像位置。第一阶段中发现的所有兴趣点的location和scale是确定的。根据关键点的稳定性来选择关键点。一个稳定的关键点能够抵抗图像失真。在方向分配环节，SIFT算法计算稳定关键点周围梯度的方向。根据局部图像梯度方向，为每个关键点分配一个或多个方向。对于一组输入帧，SIFT提取特征。图像匹配使用Best Bin First(BBF)算法来估计输入帧之间的初始匹配点。为了去除不属于重叠区域的不需要的角，使用RANSAC算法。它删除图像对中的错误匹配。通过定义帧的大小、长度和宽度来实现帧的重投影。最后进行拼接，得到最终的输出拼接图像。在拼接时，检查场景每帧中的每个像素是否属于扭曲的第二帧。如果是，则为该像素分配来自第一帧的对应像素的值。SIFT算法既具有旋转不变性，又具有尺度不变性。SIFT非常适合于高分辨率图像中的目标检测。它是一种鲁棒的图像比较算法，虽然速度较慢。SIFT算法的运行时间很大，因为比较两幅图像需要更多的时间。FAST 算法FAST是Trajkovic和Hedley在1998年创建的角点检测算法。对于FAST，角点的检测优于边缘检测，因为角点有二维强度变化，容易从邻近点中区分出来。适用于实时图像处理应用程序。FAST角点探测器应该满足以下要求：检测到的位置要一致，对噪声变化不敏感，对同一场景的多幅图像不能移动。准确;检测到的角点应该尽可能接近正确的位置。速度;角落探测器应该足够快。原理：首先围绕一个候选角点选择16个像素点。如果其中有n（n一般为12）个连续的像素都比候选角点加上一个阈值要高，或者比候选角点减去一个阈值要低，那么此点即为角点（如所示）为了加快FAST算法的速度，通常会使用角点响应函数（ corner response function, CRF)。该函数根据局部邻域的图像强度给出角点强度的数值。对图像进行CRF计算，并将CRF的局部最大值作为角点，采用多网格（multi-grid）技术提高了算法的计算速度，并对检测到的假角点进行了抑制。FAST是一种精确、快速的算法，具有良好的定位(位置精度)和较高的点可靠性。FAST的角点检测的算法难点在于最佳阈值的选择。SURF算法Speed-up Robust Feature(SURF)角点探测器采用三个特征检测步骤;检测(Detection)、描述(Description)、匹配(Matching)，SURF通过考虑被检测点的质量，加快了位移的检测过程。它更注重加快匹配步骤。使用Hessian矩阵和低维描述符来显著提高匹配速度。SURF在计算机视觉社区中得到了广泛的应用。SURF在不变特征定位上十分有效和鲁棒图像配准在特征点被检测出来之后，我们需要以某种方式将它们关联起来,可以通过NCC或者SDD（Sum of Squared Difference）方法来确定其对应关系。归一化互相关（normalized cross correlation，NCC）互相关的工作原理是分析第一幅图像中每个点周围的像素窗口，并将它们与第二幅图像中每个点周围的像素窗口关联起来。将双向相关性最大的点作为对应的对。基于图像强度值计算在两个图像中的每个位移（shifts）的“窗口”之间的相似性\\[NCC(u)=\\frac{\\sum_i[I_1(x_i)-\\bar{I_1}][I_2(x_i+u)-\\bar{I_2}] }{\\sqrt{\\sum_i[I_1(x_i)-\\bar{I_1}]^2[I_2(x_i+u)-\\bar{I_2}]^2} }\\]其中，\\(\\bar{I_1},\\bar{I_2}是窗口的平均值图像\\)\\(\\bar{I_1}=\\frac{1}{N}\\sum _i I_1(x_i)\\)\\(\\bar{I_2}=\\frac{1}{N}\\sum _i I_2(x_i+u)\\)\\(I_1(x,y)\\)和\\(I_2(x,y)\\)分别是两张图片。\\(x_i=(x_i,y_i)\\) 是窗口的像素坐标，\\(u=(u,v)\\) 是通过NCC系数计算出的位移或偏移。NCC系数的范围为\\([-1,1]\\)。 NCC峰值相对应的位移参数表示两个图像之间的几何变换。此方法的优点是计算简单，但是速度特别慢。此外，此类算法要求源图像之间必须有显著的重叠。互信息（Mutual Information, MI）互信息测量基于两个图像之间共享信息数量的相似性。两个图像\\(I_1(X,Y)\\)与\\(I_2(X,Y)\\)之间的MI以熵表示：\\[MI(I_1,I_2)=E(I_1)+E(I_2)−E(I_1,I_2)\\]其中，\\(E(I_1)\\) 和\\(E(I_2)\\)分别是\\(I_1(x,y)\\)和\\(I_2(x,y)\\)的熵。\\(E(I_1,I_2)\\)表示两个图像之间的联合熵。\\[E(I_1)=−∑_gp_{I1}(g)log(p_{I1}(g))\\]\\(g\\)是\\(I_1(x,y)\\)可能的灰度值，\\(p_{I1}(g)\\)是\\(g\\)的概率分布函数\\[E(I1,I2)=−∑_{g,h}p_{I_1,I_2}(g,h)log(p_{I_1,I_2}(g,h))\\]然而，从图中我们可以看到，许多点被错误地关联在一起。计算单应矩阵单应矩阵估计是图像拼接的第三步。在单应矩阵估计中，不属于重叠区域的不需要的角被删除。采用RANSAC算法进行单应。随机样本一致算法RANSAC(random sample consensus)RANSAC算法从可能含有异常值的观测数据集中拟合数学模型，是一种鲁棒参数估计的迭代方法。该算法是不确定性的，因为它只在一定的概率下产生一个合理的结果，随着执行更多的迭代，这个概率会增加。RANSAC算法用于在存在大量可用数据外行的情况下以鲁棒的方式拟合模型。RANSAC算法在计算机视觉中有许多应用。RANSAC原理从数据集中随机选取一组数据并认为是有效数据（内点）来确定待定参数模型，以此模型测试数据集中的所有数据，满足该模型的数据成为内点，反之为外点（通常为噪声、错误测量或不正确数据的点），迭代执行，直到某一个参数模型得到的内点数最大，则该模型为最优模型。考虑如下假设:参数可以从N个数据项中估计。可用的数据项总共是M。随机选择的数据项成为好模型的一部分的概率为\\(P_g\\)。如果存在一个很好的拟合，那么算法在没有找到一个很好的拟合的情况下退出的概率是\\(P_{fail}\\)。RANSAC步骤随机选取N个数据（3个点对）估计参数x（计算变换矩阵H）根于使用者设定的阈值，找到M中合适该模型向量x的的数据对总数量K（ 计算每个匹配点经过变换矩阵后到对应匹配点的距离，根据预先设定的阈值将匹配点集合分为内点和外点，如果内点足够多，则H足够合理，用所有内点重新估计H）。如果符合的数量K足够大，则接受该模型并退出重复1-4步骤 L次到这一步退出K有多大取决于我们认为属于合适结构的数据的百分比以及图像中有多少结构。如果存在多个结构，则在成功拟合后，删除拟合数据并重做RANSAC。迭代次数L可以用如下公式计算：\\(P_{fail} = L连续失败的概率\\)\\(P_{fail}=(给定试验失败的概率)L\\)\\(P_{fail}=(1 - 给定试验成功的概率)L\\)\\(P_{fail}=(1-(随机数据项符合模型的概率)N)L\\)\\(P_{fail}=(1-(Pg)^N)^L\\)\\(L = log(P_{fail})/log(1-(Pg)N)\\)优点：可以robust地估计模型参数缺点：迭代次数无上限，设置的迭代次数会影响算法时间复杂度和精确程度，并且需要预设阈值在执行RANSAC之后，我们只能在图像中看到正确的匹配，因为RANSAC找到了一个与大多数点相关的单应矩阵，并将不正确的匹配作为异常值丢弃单应矩阵（Homography）有了两组相关点，接下来就需要建立两组点的转换关系，也就是图像变换关系。单应性是两个空间之间的映射，常用于表示同一场景的两个图像之间的对应关系，可以匹配大部分相关的特征点，并且能实现图像投影，使一张图通过投影和另一张图实现大面积的重合。设2个图像的匹配点分别是\\(X=[x,y]^T\\),\\(X'=[x',y']^T\\)，则必须满足公式：\\[X'=HX\\]且由于两向量共线，所以\\[X'\\times HX = 0\\]其中，\\(H\\) 为8参数的变换矩阵，可知四点确定一个H\\[\\begin{pmatrix}x' \\\\y'\\\\1 \\end{pmatrix} =\\begin{pmatrix} h_{11} & h_{12} & h_{13}\\\\ h_{21} & h_{22} & h_{23}\\\\ h_{31} & h_{32} & 1 \\end{pmatrix}\\begin{pmatrix}x\\\\y\\\\1\\\\\\end{pmatrix} \\]令 \\[h =(h11:h12:h13:h21:h22:h23:h31:h32:h33)T\\]则有\\[Bh=0\\]N个点对给出2N个线性约束。\\[\\underset{h}{min}║Bh║^2 ，║h║ = 1\\]用RANSAC方法估算H：首先检测两边图像的角点在角点之间应用方差归一化相关，收集相关性足够高的对，形成一组候选匹配。选择四个点，计算H选择与单应性一致的配对。如果对于某些阈值:Dist(Hp、q) <ε，则点对(p, q)被认为与单应性H一致重复34步，直到足够多的点对满足H使用所有满足条件的点对，通过公式重新计算H图像变形和融合最后一步是将所有输入图像变形并融合到一个符合的输出图像中。基本上，我们可以简单地将所有输入的图像变形到一个平面上，这个平面名为复合全景平面。图像变形步骤首先计算每个输入图像的变形图像坐标范围，得到输出图像大小，可以很容易地通过映射每个源图像的四个角并且计算坐标(x,y)的最小值和最大值确定输出图像的大小。最后，需要计算指定参考图像原点相对于输出全景图的偏移量的偏移量x_offset和偏移量y_offset。下一步是使用上面所述的反向变形，将每个输入图像的像素映射到参考图像定义的平面上，分别执行点的正向变形和反向变形。平滑过渡（transition smoothing）图像融合方法包括 羽化（feathering）， 金字塔（pyramid）， 梯度（gradient）图形融合最后一步是在重叠区域融合像素颜色，以避免接缝。最简单的可用形式是使用羽化（feathering），它使用加权平均颜色值融合重叠的像素。我们通常使用alpha因子，通常称为alpha通道，它在中心像素处的值为1，在与边界像素线性递减后变为0。当输出拼接图像中至少有两幅重叠图像时，我们将使用如下的alpha值来计算其中一个像素处的颜色：假设两个图像 \\(I_1,I_2\\),在输出图像中重叠；每个像素点\\((x,y)\\)在图像\\(I_i(x,y)=(\\alpha iR,\\alpha iG,\\alpha iB,\\alpha j,)\\)，其中（R,G,B）是像素的颜色值，我们将在缝合后的输出图像中计算(x, y)的像素值：\\[ [(α1R, α1G, α1B, α1) + (α2R, α2G, α2B, α2)]/(α1+α2)\\].小结上述内容对一些常用的算法进行了简单的概述。Harris角点检测方法具有鲁棒性和旋转不变性。然而，它是尺度变化的。FAST算法具有旋转不变性和尺度不变性，且具有较好的执行时间。但是当有噪声存在时，它的性能很差。SIFT算法具有旋转不变性和尺度不变性，并且在有噪声情况下更有效。它具有非常明显的特征。然而，它受到光照变化的影响。该算法在执行时间和光照不变性方面具有较好的性能。参考OpenCV探索之路（二十四）图像拼接和图像融合技术Debabrata Ghosh,Naima Kaabouch. A survey on image mosaicing techniques[J]. Journal of Visual Communication and Image Representation,2016,34.地址图像拼接综述"}
{"content2":"计算机视觉、计算机图形学、图像处理的区别和联系搞了CV一段时间，仍时不时因为概念问题而困惑，搞不清楚计算机视觉(Computer Vision)，计算机图形学(Computer Graphics)和图像处理(Image Processing)的区别和联系。在知乎上看到了一个帖子，觉得解释的很好，结合自己的理解，形成此文存档。1.基本概念从定义理解概念是最严谨的。所以首先搞清楚维基百科中这些概念的定义。计算机视觉(CV)：Computer vision is a field that includes methods for acquiring, processing, analyzing, and understanding images and, in general, high-dimensional data from the real world in order to produce numerical or symbolic information, e.g., in the forms of decisions.[1]直译过来就是计算机视觉是一个学科/领域，它包括获取、处理、分析和理解图像或者更一般意义的真实世界的高维数据的方法；它的目的是产生决策形式的数字或者符号信息。计算机图像学(CG)：Computer graphics is a sub-field of computer science which studies methods for digitally synthesizing and manipulating visual content. Although the term often refers to the study of three-dimensional computer graphics, it also encompasses two-dimensional graphics and image processing.[2]直译过来就是计算机图形学是计算机科学的一个子领域，它包括数字合成和操作可视内容（图像、视频）的方法。尽管这个术语通常指三维计算机图形学的研究，但它也包括二维图形学和图像处理。图像处理（IP）：In imaging science, image processing is processing of images using mathematical operations by using any form of signal processing for which the input is an image, such as a photograph or video frame; the output of image processing may be either an image or a set of characteristics or parameters related to the image.[3]直译过来就是在图像科学中，图像处理是用任何信号处理等数学操作处理图像的过程，输入时图像（摄影图像或者视频帧），输出是图像或者与输入图像有关的特征、参数的集合。2.区别和联系[4]2.1 精简的概括Computer Graphics和Computer Vision是同一过程的两个方向。Computer Graphics将抽象的语义信息转化成图像，Computer Vision从图像中提取抽象的语义信息。Image Processing探索的是从一个图像或者一组图像之间的互相转化和关系，与语义信息无关。2.2 从输入输出角度看(1)区别Computer Graphics，简称 CG 。输入的是对虚拟场景的描述，通常为多边形数组，而每个多边形由三个顶点组成，每个顶点包括三维坐标、贴图坐标、rgb 颜色等。输出的是图像，即二维像素数组。Computer Vision，简称 CV。输入的是图像或图像序列，通常来自相机、摄像头或视频文件。输出的是对于图像序列对应的真实世界的理解，比如检测人脸、识别车牌。Digital Image Processing，简称 DIP。输入的是图像，输出的也是图像。Photoshop 中对一副图像应用滤镜就是典型的一种图像处理。常见操作有模糊、灰度化、增强对比度等。(2)联系CG 中也会用到 DIP，现今的三维游戏为了增加表现力都会叠加全屏的后期特效，原理就是 DIP，只是将计算量放在了显卡端。CV 更是大量依赖 DIP 来打杂活，比如对需要识别的照片进行预处理。最后还要提到近年来的热点——增强现实（AR），它既需要 CG，又需要 CV，当然也不会漏掉 DIP。它用 DIP 进行预处理，用 CV 进行跟踪物体的识别与姿态获取，用 CG 进行虚拟三维物体的叠加。(3)图解这里还有一张图，简明地表达了CV、CG、DIP和AI的区别和联系。2.3 从问题本身看(1)区别从问题本身来说，这三者主要以两类问题区分：是根据状态模拟观测环境，还是根据观测的环境来推测状态。假设观测是Z，状态是X：Computer Graphics是一个Forwad Problem (Z|X)： 给你光源的位置，物体形状，物体表面信息，你如何根据已有的变量的状态模拟出一个环境出来。Computer Vision正好相反，是一个Inverse Problem (X|Z)：你所有能得到的都是观测信息(measurements), 根据得到的每一个Pixel的信息(颜色，深度)，我要来估计物体环境的特征和状态出来，比如物体运动(Tracking)，三维结构（SFM）,物体类别（Classification and Segmentation）等等。对于Image Processing来说，它恰好介于两者之间，两种问题都有。但对于State-of-art的研究来说，Image Processing更偏于Computer Vision, 或者看上去更像Computer Vision的子类。尽管这三类研究中，随着CV领域的不断进步，以及越来越高级相机传感器出现（Depth Camera, Event Camera），很多算法都被互相用到，但是从Motivation来看，并没有太大变化。(2)联系得益于这几个领域的共同进步，所以你能看到Graphics和Computer Vision现在出现越来越多的交集。如果根据观测量（图片），Computer Vision可以越来越准确的估计出越来越多的变量，那么这些变量套到Graphics算法中，就可以模拟出一个跟真实环境一样的场景出来。与此同时，Graphics需要构建更真实的场景，也希望能够将变量更加接机与实际，或者通过算法估计出来，这就引入了Vision的动机。这也是近年来三维重建算法，同时大量发表在Graphics和Vision的会议的原因。随着CV从2D向3D发展，以后两者的交集会越来越大，除了learning以外的其他很多问题融合并到一个领域我也不会奇怪。参考文献[1] https://en.wikipedia.org/wiki/Computer_vision[2]https://en.wikipedia.org/wiki/Computer_graphics_(computer_science)[3] https://en.wikipedia.org/wiki/Image_processing[4] 张静, 知乎,http://www.zhihu.com/question/20672053/answer/15854031计算机视觉、图形学和图像处理，三者有什么联系？先说区别1. Computer Graphics，简称 CG 。输入的是对虚拟场景的描述，通常为多边形数组，而每个多边形由三个顶点组成，每个顶点包括三维坐标、贴图坐标、rgb 颜色等。输出的是图像，即二维像素数组。[xyz xyz xyz ... xyz] -> 图片2. Computer Vision，简称 CV。输入的是图像或图像序列，通常来自相机、摄像头或视频文件。输出的是对于图像序列对应的真实世界的理解，比如检测人脸、识别车牌、区分猫狗。图片 -> dog or cat?图片 -> [xyz xyz xyz ... xyz]3. Digital Image Processing，简称 DIP。输入的是图像，输出的也是图像。Photoshop 中对一副图像应用滤镜就是典型的一种图像处理。常见操作有模糊、灰度化、增强对比度等。图片 -> ps后的图片再说联系1. CG 中也会用到 DIP，现今的三维游戏为了增加表现力都会叠加全屏的后期特效，原理就是 DIP，只是将计算量放在了显卡端。通常的做法是绘制一个全屏的矩形，在 Pixel Shader 中进行图像处理。2. CV 大量依赖 DIP 来打杂活，比如对需要识别的照片进行预处理，增强对比度、去除噪点。3. 最后还要提到今年的热点——增强现实（AR），它既需要 CG，又需要 CV，当然也不会漏掉 DIP。它用 DIP 进行预处理，用 CV 进行跟踪物体的识别与姿态获取，用 CG 进行虚拟三维物体的叠加。转载请保留作者名、注明源自微信公众号“黑客与画家”（HackerAndPainter），关注游戏开发、计算机视觉、图形学、虚拟现实、体感交互等好玩的内容。"}
{"content2":"按键精灵是一款模拟鼠标键盘动作的软件。通过制作脚本，可以让按键精灵代替双手，自动执行一系列鼠标键盘动作。人工智能（Artificial Intelligence），英文缩写为AI。它是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。 人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。人工智能是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。人工智能从诞生以来，理论和技术日益成熟，应用领域也不断扩大，但没有一个统一的定义。人工智能是对人的意识、思维的信息过程的模拟。人工智能不是人的智能，但能像人那样思考、也可能超过人的智能。但是这种会自我思考的高级人工智能还需要科学理论和工程上的突破。人工智能是一门极富挑战性的科学，从事这项工作的人必须懂得计算机知识，心理学和哲学。人工智能是包括十分广泛的科学，它由不同的领域组成，如机器学习，计算机视觉等等，总的说来，人工智能研究的一个主要目标是使机器能够胜任一些通常需要人类智能才能完成的复杂工作。但不同的时代、不同的人对这种“复杂工作”的理解是不同的。"}
{"content2":"说明：这个贴用于收集笔者能力范围内收集收藏并认为有用的资料，方便各方参考，免去到处找寻之苦，提升信息的交叉引用价值。仅供参考，不作为必然的推荐倾向。如涉及版权等问题请相关人员联系笔者，谢谢。|博客|龙心尘的博客(http://blog.csdn.net/longxinchen_ml)寒小阳的博客(http://blog.csdn.net/han_xiaoyang)wepon(http://2hwp.com/)面包包包包包包（http://blog.csdn.net/breada）仆居（http://blog.csdn.net/kkk584520）|人工智能|机器学习|数据挖掘|神经网络|手把手入门神经网络系列-2篇-有图有码机器学习系列-7篇-有图有码NLP(自然语言处理)系列-5篇-有图有码利用 Python 练习数据挖掘[建议：适合入门；][简介：围绕1个例子；完整的步骤；少量错误；][扩展：关于IRIS数据集的Python分析 (太初）][扩展：IRIS数据集的PCA分析和3D展现]用Python做科学计算-基础篇[简介：不错的教程]scikit-learn的主要模块和基本使用[简介：内容精要]浅谈神经网络[入门] | 统计与计算[入门]|人脸识别|指纹掌纹识别|生物识别|计算机视觉（CV）PCA+SVM人脸识别深度学习与计算机视觉系列-10篇-有图有码Python和OPENVC分析烤箱状态从特征描述符到深度学习：计算机视觉发展20年Python计算机视觉编程（10章+附录ABC）|算法|模型|| 朴素贝叶斯分类器 | 朴素贝叶斯分类 | 朴素贝叶斯的三个常用模型(高斯/多项式/伯努利) || 判别模型/生成模型与朴素贝叶斯方法 | 聚类算法k-mean | 聚类算法k-mean | 白化 | PCA(1,2,3,4,5) || 线性回归 | 中文分词软件(14款开源) |  ||实用连接|| numpy.zeros函数用法 | SCIPY.ORG |numpy中的ndarray方法和属性 | 多维数组ndarray及切片| plot绘图 | numpy.linspace | numpy.matrix |  numpy.arange | scipy || sklearn官网API | 大量wheel | Anaconda[简介:极好的PythonIDE!](安装使用) ||平台|网站|竞赛|Aminer（Open Science Platform)  | Kaggle(体验,简介) | DataCastle | 阿里天池 (体会,总结) |神经网络实验室(所见即所得的测试谷歌tensorflow)极好！ | 谷歌大脑Google Brain ||实用连接||Python数据图表工具 | Python写爬虫查询 |[END]"}
{"content2":"深度 | AI芯片终极之战https://mp.weixin.qq.com/s?__biz=MzA4MTQ4NjQzMw==&mid=2652712307&idx=1&sn=28806ccb69a5f5d1142ac5f79ccde395&chksm=847dba7db30a336bfde664a5f2b75fdc443ac541483542eada358f298965614f60e3faaecf7e&scene=21#wechat_redirect2018-03-05 发布看点：解读AI芯片四大门派“少林、武当、五岳、明教”，看芯片与人工智能如何重塑世界。2015年的秋天，北京的雨水比往年要多些，温度却不算太冷。这一年里，年仅23岁的姚颂刚刚拿到清华大学的毕业证书；32岁的陈天石博士毕业后已在中科院计算所待了整整8年；而在芯片界摸爬滚打了14年的老将何云鹏却毅然辞掉了长虹芯片高管的职位，华丽创业转身。2015年的秋天，在大洋的另一端，英伟达的股价还在20多美元徘徊，谷歌公司内部却开始秘密地用上了TPU芯片；在彼岸的中国市场里，百度研究院两位高管：副院长余凯与异构计算团队负责人吴韧陆续离职，成立了两家芯片公司——地平线与异构智能。2015年的秋天，比特币虽然开始逐渐回暖，但依旧持续在200美元的低迷状态，日后的世界第一大矿机芯片生产商比特大陆除了不断迭代矿机产品，还悄然规划起了另一条人工智能产品线。比特大陆联席CEO詹克团那年36岁，话不多，爱看历史、哲学、科幻——比如科幻小说《三体》。谁也没有想到，就在短短两年多以后，这些人的生命会共同被卷入一场前所未有的科技洪流当中——人工智能专用芯片（AI芯片）——而与他们一同的，还有其他成百上千位前沿技术领域最顶尖的优秀人才。他们其中既有一路保送仅仅24岁就博士毕业的少年天才、也有在业内叱咤风云数十年的行业泰斗；既有成立仅一年公司估值就超过10亿人民币的创业神话、也有市值高达8000亿美元、全球坐二望一的科技霸主。经ZDX初步测算，仅国内AI芯片头部创业公司的融资金额就已超过了20亿人民币。而根据中信证券测算，2020年全球AI芯片的市场规模可达146.16亿美元。人工智能这一项新兴技术，在经历了最近几年的技术积累、升级、发酵之后，正在以AI芯片作为载体而全面崛起。AI芯片正在以一种前所未有的速度颠覆着以安防、手机、无人驾驶汽车、云计算等为首的四大领域，并从它们延展开来，进一步对工业、制造、医疗、教育等传统领域造成巨大冲击，重新塑造着世界格局。据ZDX报道了解得知，目前全球至少有45家初创公司正在研发语音交互和自动驾驶芯片，并且至少有5家企业已经获得超过1亿美元的融资，这一数目还在不断增加当中。回望中国市场，人工智能与AI芯片技术为中国市场这几十年来埋头追赶的芯片行业打开了一扇新大门，让我国第一次拥有傲立世界前沿科技之林、甚至引领全球科技潮流的机会窗口。在这个市场中，资本是最快响应起来的。在动辄融资上亿美元融资的催化之下，中国市场AI芯片创业市场尤为兴盛，除了融资独角兽层出不穷外，各大创业公司也根据自身特点陆续形成了四大派系，我们可以用武林几大门派来做形象的比喻——“少林、武当、五岳、明教”，这四大派系的玩家们各有特色，神通尽显。相比起来，海外各大巨头也不遑多让。就在大约20天前，当整个中国都沉浸在农历春节的团圆氛围当中时，大洋彼岸科技巨头谷歌突然宣布，其AI芯片“TPU”的强大计算能力将向民众开放——这块TPU的来头可不小，谷歌的AI程序AlphaGo（阿尔法狗）就是靠它提供的强大计算能力战胜了全球围棋第一高手，柯洁。恰恰就在同一天，那个一直以线上书店而广为人知的科技巨头亚马逊，也被曝出正在为其爆款产品——智能音箱Echo——打造专用AI芯片，该项目研发团队已有449名员工。ZDX历经半年，深入业内，首次对AI芯片全产业链上下近百家核心企业进行报道，覆盖国内外各大巨头玩家、新兴创企、场景应用、代工生产等，全面深入地对芯片产业发展、创新创业进行了追踪报道，由此也促成了ZDX举办的国内首创AI芯片创新峰会。此为ZDX AI芯片产业系列的重磅报道之一，通过对AI芯片最热的四大应用行业与各大入场行业玩家的拆解，全景式地解构了AI芯片产业链面貌。四大商业场景面临颠覆AI芯片之于人工智能的意义，则可以理解为发动机之于汽车。人工智能这一理论已经提出了多年，但是由于实现它需要的计算量实在太大，这辆“跑车”一直没有配备上合适的“发动机”，只能放在仓库积灰。直到AI算法和大数据时代的到来、以及AI芯片的出现。广义来说，能够驱动AI程序的芯片，都能叫做AI芯片。不过本文特指已经为AI算法进行了特殊设计的芯片。按照应用场景，AI芯片可以简单地分为用于云端服务器机房等地的云AI芯片，以及用于端智能设备、IoT设备的终端AI芯片。云AI芯片的特点是性能强大、能够同时支持大量运算、并且能够灵活地支持图片、语音、视频等不同AI应用。我们现在使用的各种互联网AI能力（比如在线翻译、人证比对），背后都有云AI芯片在发挥作用或提供算力。端AI芯片则需要嵌入到设备内部，让设备不需要联网就能具备AI能力。它们的特点是体积小、耗电少，而且性能不需要特别强大，通常只需要支持一两种AI能力。现在手机里的芯片、摄像头里的芯片、甚至你家电饭煲里的芯片都开始陆续AI化。全球GPU霸主英伟（NVIDIA）创始人黄仁勋曾经向ZDX说过，未来，AI与AI芯片将会无处不在：咖啡机、保温杯、麦克风、甚至耳环、鞋子这些小物件都会智能化。目前，AI芯片有四个最为火热的商业应用场景——家居/消费电子、安防监控、自动驾驶汽车、以及云计算。1）家居/消费电子——悄然入侵你家，润物细无声我们先从离我们生活最近的家居/消费电子说起。2017年9月，华为发布了世界首款手机AI芯片麒麟970，打响了AI芯片入侵手机的第一枪。10月，搭载这块AI芯片的华为Mate 10和Mate 10 Pro正式面世。这款麒麟970 AI芯片中搭载了一个专门用于处理AI的模块——NPU（神经网络处理单元），其技术来源于国内AI芯片创企中科寒武纪。这块NPU的计算速度比CPU快了25倍，同时还将能耗效率将提高了50倍。这款AI芯片不仅能够让你的手机能够照相更好看、翻译更流畅、语音识别更准确外，还能学习理解你的使用习惯，让手机自动释放内存，更快更流畅。在围绕华为Mate 10和麒麟970芯片进行的一系列独家深入采访中，华为无线终端芯片业务部总监Eric曾经告诉ZDX，未来AI将会是芯片中的一个基础技术，可能到了明年（2018年），每家芯片公司都会有这个能力。（18个月，华为AI手机涅槃诞生记）事实证明，他说的没错。就在麒麟970发布的短短两周后，9月13日，苹果发布十周年纪念款iPhone X，搭载自研的AI芯片A11，这块芯片不仅让iPhone X用起来更快更流畅，还能让iPhone X支持人脸识别解锁（FaceID）、人脸识别付款、照片自动分类、以及实时表情跟踪Animoji等功能。在今年春节年廿九（2月14日）当天，老牌芯片公司ARM正式宣布推出两款针对移动端的AI芯片架构：物体检测处理器和机器学习处理器。这事的重要性绝对不容小觑——要知道，目前全世界超过90%的手机芯片采用的是ARM的架构，连麒麟970和苹果A11都不例外。而在刚刚结束的MWC2018上，中端手机芯片巨头联发科发布了新款手机芯片Helio P60，支持AI和计算机视觉，能够提供精确的人脸识别等功能。而高端手机芯片巨头高通则在今年2月22日宣布推出基于其枭龙芯片系列的人工智能引擎（AI Engine），将高通手机SoC当中的所有软、硬件AI计算能力打包到这个引擎里，让人工智能在手机上的应用更快速、高效。这一系列风起云涌的新品发布证明，手机已然成为AI芯片市场红海战局的重要组成部分。不过除了手机之外，你日常使用的众多家用电子产品也在悄无声息地进行着AI升级，主要围绕语音AI展开，代表玩家包括亚马逊打造Echo智能音箱AI芯片、启英泰伦打造的家用电器语音AI芯片、杭州国芯打造语音AI芯片等等。联发科技副总经理暨智能设备事业群总经理向东西表示，2018年全球智能音箱市场预计将突破6000万大关，而在这6000万里面，也将会有越来越多的音箱产品搭载AI芯片。2）安防摄像头——被所有AI芯片玩家看好的大火市场随着市场经济的快速发展、技术的不断成熟、再加之国家政策的推动，中国安防产业的规模也在不断增长。根据佳都科技《人工智能技术白皮》显示，2017年安防市场规模将已经超过了6350亿，同比增长17.6%。安防市场规模不断增长的同时，意味着以摄像头为主的安防设备数量增加（最近一年内，国内安防高清摄像头的出货量会在1亿颗左右），人眼监控已经看不过来了；再加之社会对安保要求的提升，安防产业越发重视事前预警，传统的人工审查方式已经远不足以满足产业需求，安防要求的是机器独立实时监控、实时报警。由于AI能够对迅速对视频进行结构化处理，对人、车、物进行快速识别比对，机器不仅能认出逃犯、嫌疑人，还能记录分析他的实时位置，此类能力与公安、交警、民防等需求不谋而合。毫不夸张地说，现在几乎所有AI芯片创业公司都将安防作为核心应用场景之一，纷纷推出内嵌于安防监控摄像头的AI芯片，成亿的研发资金砸向这块，称安防AI芯片为“当红炸子鸡”丝毫不为过。此外，几个安防行业老牌巨头也在蠢蠢欲动着。安防是个集中度很高的传统行业，海康威视、大华股份、宇视科技这三大行业巨头加起来已经占了安放领域的半壁江山。这些安防巨头们有着积累深厚的行业经验，不仅是众多AI芯片公司的合作伙伴，其自身也在推进AI+安防的步伐。举个例子，宇视科技在AI+安防领域已经发布了一整套AI整体解决方案，涉及嵌入了GPU芯片的前端智能摄像机、人脸识别速通门、后端数据中心一体机等。不过宇视首席架构师姚华向ZDX提到，与AI初创不同，宇视这类传统安防企业除了研发AI技术外，还需要重点关注这一技术的工程化和落地性，比如用户机房耗电量、发热量等工程性问题。安防AI化的全面铺开并没有那么容易。而在发改委的2018年“互联网+”、人工智能创新发展和数字经济试点重大工程拟支持项目名单上，海康海康威视的“计算机视觉AI芯片研发及产业化项目”赫然纸上。3）自动驾驶汽车——三大AI芯片势力，推动无人车商用落地根据美国交通部的分类标准，自动驾驶汽车可以分为五个等级：L1-L5，L1驾驶辅助、L2部分自动化、L3有条件自动化、L4高度自动化、L5完全自动化（完全自动化也就是不需要司机，车子可以自己走），这也是业内普遍比较认可的分类方法。由于需要对环境进行感知、进而决策，因此L3-L5级别的自动驾驶技术对计算平台的要求越来越高，对AI芯片的需求也越来越强烈。目前，AI芯片正在成为自动驾驶计算平台的核心组成部分，由英伟达、英特尔这样的芯片巨头，和地平线这样的创业公司整合成为自动驾驶计算平台，提供给整车厂和Tier-1（一级供应商）汽车配件商巨头，落地到自动驾驶解决方案当中。目前看来有三股势力：1、英伟达的Xavier计算平台（前身是Drive PX），这一计算平台正在被超过20家自动驾驶创业公司，以及博世、采埃孚等供应商巨头采用，打造各自的自动驾驶。其中最接近量产商用的是采埃孚Pro AI方案。2、英特尔GO计算平台（由英特尔CPU、英特尔收购的Mobileye EyeQ芯片、英特尔收购的Altera FPGA处理器）被整车厂宝马、意大利FCA、以及供应商巨头德国大陆、加拿大麦格纳、德尔福（后拆分出安波福）、都陆续采用其打造为自动驾驶解决方案。其中英特尔、宝马、大陆等还牵头成立了自动驾驶联盟。3、以地平线为代表的创业公司正在打造自己的自动驾驶计算平台。地平线的雨果自动驾驶平台早期使用的是英特尔FPGA处理器，后打造自研BUP架构，并推出沿用这一架构的AI芯片“征程”。此外ZDX了解到，由于汽车自动驾驶行业巨大，不少第一阵营的AI芯片创业公司也秘密瞄准了这一领域。无论是上述何种自动驾驶计算平台，AI芯片都是其中的关键组成部分，也是自动驾驶技术发展的主要推动力。博世，作为汽车Tier-1领头玩家，将在2018年实现L2级别自动驾驶实用、2021年完成L3商用。博世底盘控制系统中国区驾驶员辅助系统雷达研发部门总监蔡旌向ZDX表示，博世正在与众多自动驾驶计算平台供应商合作探索，其中既有巨头，又有初创公司。博世对自动驾驶计算平台在功耗、成本上有较高要求。对于自动驾驶需要的计算平台，博世与较多供应商正在合作探索，其中既有巨头，又有初创公司。博世对自动驾驶计算平台在功耗、成本上有较高要求，未来博世的选择将会是嵌入式的计算平台。自动驾驶初创企业智行者CEO张德兆也曾向ZDX表示，自动驾驶技术离普及还有待时日，因此创业公司在进行技术研发的同时也要着力于技术的落地与商业变现。而且无人车虽然需要较强的算力，但成本、功耗与量产都是需要考虑的元素。4）云计算——为互联网AI供能，高效灵活无论上微博还是点外卖，我们现在使用所有互联网应用，背后都是云服务数据机房在提供计算能力。而我们现在使用的各种互联网AI能力（比如在线翻译、人证比对、图片搜索等），背后都有云AI芯片在庞大的数据中心机房中日夜不停地提供算力。上文提到，云AI芯片的特点是性能强大、能够同时支持大量运算、并且能够灵活地支持图片、语音、视频等不同AI应用。这是一个体量巨大的市场，同时也是各类芯片巨头厮咬最紧的战场。打响第一枪的是英伟达，这家成立于1993年的年轻芯片公司在人工智能时代尝尽的红利，由于其GPU特别适合用于如今的主流AI算法——深度学习——的训练，这家公司不仅股价从30多美元一路飙升至200多美元，还在全球范围内掀起了这场规模宏大的AI芯片热潮。最先响应是全球各大传统芯片巨头们，英特尔、高通、ARM、联发科等都陆续进场。而这其中，称霸芯片市场十多年的英特尔更是重金出仓，一路“买买买”，布局了众多AI芯片产品线，云AI芯片与端AI芯片都有所涉及。英特尔研究院院长宋继强曾经向ZDX表示，AI芯片的未来一定是多样化，不同种类的产品满足不同功耗、尺寸、价钱的要求——英特尔如今的AI芯片布局就朝着这种多样性发展。人工智能是一场马拉松，现在这场比赛才刚刚开始。此外，国内科技巨头如百度、阿里、腾讯、科大讯飞等也都陆续进军云AI芯片领域，不过暂时以投资动向居多，其中百度在2017年8月发布了XPU，这是一款256核、基于FPGA的云计算AI芯片。四大门派：少林、武当、五岳、明教云AI芯片方面，以英伟达GPU为代表的海外玩家起步较早。而在端AI芯片方面，中国的众多初创公司则跑在了世界前列。美国国家工程院院士、英伟达首席科学家Bill Dally教授曾经向ZDX表示，目前云端AI芯片的市场已经较为成熟，全球各大科技巨头扎根已深，格局很难被撼动。相比而言，种类繁多而且数量巨大的终端AI芯片市场还有待拓展，是众多AI芯片初创企业的机会所在。在动辄融资上亿元融资的催化之下，中国AI芯片创业市场尤为兴盛，不仅从2015年之后陆续涌现出一批AI芯片创业公司，还催生了不少独角兽企业。根据创始团队背景、公司属性、技术流派等特点，国内的这些AI芯片创业公司可以分成“少林、武当、五岳、明教”四大派系。1）少林：学术浓厚，一脉传承之所以把这几家称作“少林派”，是因为这些创业团队普遍成立较早，大多从清华、北大、中科院、电子科技大学等高等院校/研究机构脱胎而来，具备极强的学术属性。创始团队中一般还会有一个“老教授”加持，他们在芯片及相关学术研究上有着多年的学术积累，并且在这些院校与机构中任职多年，能够为公司提供一套成系统的人才资源体系——对于高新科技公司来说，人才意味着技术优势、也意味着强大的市场竞争力。这一派别不仅凭借深厚的学术背景受到资本亲睐，而且能够获得“母校”在市场与人才方面的支持。比如代表玩家之一：中科寒武纪——其创始人陈云霁、陈天石两兄弟都是中科院计算所博士毕业，2016年创业之初，寒武纪不仅在天使轮获得了中科院计算所的1000万元研究经费，还在各种项目资源中获得了中科院的支持。在搭载寒武纪AI芯片技术的华为海思麒麟970芯片发布之后，中科院计算所还特意向华为发来贺信，强调了寒武纪的中科院背景。此外，代表玩家还有清华电子系汪玉副教授带领团队成立的深鉴科技、电子科技大学博士生导师刘洋教授带队成立的深思创芯、清华大学魏少军教授带领的清华大学微电子学研究所、清华精仪系施路平老师组领衔的类脑计算芯片团队、北京大学高能效计算与应用中心主任丛京生带领的基于FPGA的深度学习加速团队等等。这些玩家中，深鉴科技是最早一批独立作为商业企业运营的创企业之一，它成立于2016年3月，最早由FPGA技术起家，目前已成功跻身中国AI芯片创企第一阵营。深鉴科技CEO姚颂向ZDX透露，目前深鉴已经接到了数千万的订单，两款AI芯片都在量产当中，已经有不少合作伙伴在使用当中。深思创芯CEO俞德军博士则告诉ZDX，除了现有的民间场景需要用到AI芯片外，军事上的导弹辅助、舰艇、潜艇等场景也需要AI芯片，深思创芯目前正与国内军用单位合作这些项目。魏少军教授带领的清华大学微电子学研究所则在30多年来一直致力于芯片前沿学术技术的研发，AI芯片属于其中的技术分支；去年，清华大学微电子学研究所的AI芯片Thinker在2017 ACM/IEEE ISLPED国际低功耗电子学与设计会议上获得了设计竞赛奖，这是中国大陆单位首次以第一完成单位获得此项荣誉。2）武当：老牌大厂，跨界转型武当，内家之宗。在AI芯片这个大江湖里，除了各类创业公司之外，不少传统芯片厂商也陆续入局，AI芯片对于他们来说既是其现有产品的自然升级，也是个机遇与挑战并存的时间窗口——做得好了，弯道超车、一举成为行业龙头；做不好了，容易先机尽失、甚至有被市场淘汰的危险。一直以来，这些公司因传统机顶盒芯片、手机芯片、安防摄像头芯片而为人所知。与AI芯片创业公司相比，这些老牌芯片公司普遍具有丰富的设计制造经验、市场渠道完善、并且有成熟的商业运作经验。代表玩家包括华为海思、杭州国芯、中星微电子等等。举个例子，杭州国芯成立于2001年，属于国内较早的一批芯片设计公司，目前在全球机顶盒芯片市场占据份额高达15%。2017年底，杭州国芯也发布了其首颗语音AI芯片GX8010。值得一提的是，由于安防是AI芯片落地应用的核心场景之一，而且市场巨大。但是现在几乎所有安防芯片巨头、安防硬件巨头（比如宇视科技、国科微、中天微等）都在陆续进行着AI芯片相关的研发与合作。目前这些厂商正式对外公布的消息不多，但是都在秘密进展当中。3）五岳：神通尽显，海渡八仙“五岳派”的玩家大多在传统芯片行业中有着多年的技术经验积累，与前两派的玩家相比，他们既有着多年的芯片打造经验、对于安防、家居等行业有着自己深入的理解，同时又具备创业公司快速决策、快速奔跑的市场属性。不过由于芯片前期的研发投入金额较大，“五岳派”的玩家背后普遍有科技巨头的投资身影，比如百度、腾讯、阿里、英特尔；又比如依图、云知声、云天励飞这类已经具备一定资本体量的人工智能创业公司。这一派的代表玩家包括英特尔参与投资的地平线机器人，值得一提的是，地平线机器人创始人余凯是原百度副院长。此外，还有依图科技投资的ThinkForce、阿里巴巴创业者基金与高通等投资的耐能（Kneron）、云知声旗下的云知芯、Roobo投资的启英泰伦、与上海宝信软件股份签订合作协议的海青智盈等等。4）明教：比特加持，另辟蹊径在众多AI芯片公司当中，还有着这样一派另类的玩家：他们用短短几十个月的创业时间就将年营收做到了25亿美元，财富积累的迅猛程度几近恐怖；他们狂热地崇拜技术，极尽可能地压榨出芯片最后一丝计算性能；他们蒙眼狂奔、千金一掷，与此同时却又神秘而低调，对于外界的种种猜疑不置一词。他们就是这波比特币狂潮中的卖水人——虚拟货币“矿机芯片”的生产商们，代表玩家包括比特大陆、嘉楠耘智等。获取虚拟货币需要进行“挖矿”，而“矿机”的本质就是一种专用于挖矿计算机，它同样由芯片驱动供能。芯片性能越强大，挖起矿来就越快、越多、越有优势。在以虚拟货币价格一飞冲天的市场里，这些为挖矿者提供发动机的芯片公司的财富积累自然水涨船高。拿比特大陆来说，比特大陆成立于2013年10月，2017年的总营收已经飙升至25亿美元，拥有超过1000名员工，在全球建立了百亿次计算的数据中心，占矿机专用芯片市场超过70%份额，几乎具有压倒性的市场优势。不过，就在虚拟货币一飞冲天的2017年年底，比特大陆却另辟蹊径，正式推出了第一款AI专用芯片“SOPHON（算丰）”——这一名字正是来源于比特大陆联席CEO詹克团爱读的那套科幻小说，《三体》。比特大陆产品战略总监汤炜伟还曾向ZDX透露，在最近这一两个月内，比特大陆的AI芯片团队规模已经极速扩张到300人。芯片一直是个“烧钱”的东西，这款采用28nm工艺的“算丰”AI芯片仅研发与流片费用就高达几百万美元。传统芯片的迭代速度为1-2年，但比特大陆宣布其产品将会以9个月一代的速度进行快速迭代着。此外，世界第二大比特币矿机生产商嘉楠耘智也在2017年12月预发布了其自研AI芯片KPU，目前，这个矿机巨头已申请挂牌新三板。2017年公司的收入超过12亿元，利润过3亿。与其他从AI起家切入芯片的公司相比，矿机厂有着丰富的芯片打造经验与丰厚的资本积累，不过相对而言，其他专注AI芯片的初创公司则在AI算法优化与软件生态打造上更有优势。毕竟对于AI芯片来说，AI与芯片两方面能力都十分重要。源起之初：云计算，第一批吃到肉的海外巨头所谓人工智能，其实就是让机器具备像人一样的智能。但是一直以来，计算机其实都是很“笨”的，一些对于人类来说易如反掌的任务，对于计算机来说却难于登天：比如认清楚这张照片里的动物是什么？对于人类来说，我们看到了一只在野外坐着的灰猫，而对于计算机而言，它看到的却是图像中的一组数字；我们看到的猫耳与猫眼，在计算机看来只是数字88和23的区别。由此衍生出的一门重要学科叫做计算机视觉（Computer Vision，CV），而分清楚“猫”、“狗”这两类外形相似的动物、并且在图片中圈出这只猫的位置，一直是这门学科的冠上明珠级别的难题，近20年来一直难有进展。直到2012年。2012年的初冬，发生了一件大事情。在2012年计算机视觉届的“奥林匹克”——ImageNet挑战赛的赛场上，出现了一位另类玩家，一位来自多伦多大学的老教授，Geoffrey Hinton。Hinton和他的团队们第一次用上了GPU芯片和深度学习算法，让计算机识图的错误率猛然降低了数倍，成为计算机视觉历史上的一个重要节点。在2015年的ImageNet大赛上，微软亚洲研究院团队更是凭借GPU与深度学习算法，第一次让计算机的识图能力超过了人类。人类识图错误率约为4%，而冠军团队机器识图的错误率为3.57%。一石激起千层浪。GPU芯片与深度学习算法一下成了人工智能届的“网红”，无数科学们都开始将研究重心转移到它们身上。深度学习算法可以理解为实现人工智能的一种电脑程序，它让计算机通过对大量素材的学习（比如成千上万张不同种类的“猫”图片），让机器自己总结出“猫”的特征，下次你再给它一张猫的照片，无论是黑猫、白猫、波斯猫，它都能认出来。在图片分类、识别兴起之后，视频识别、语音识别、翻译、语音助手等一系列AI应用应运而生。其实，GPU的本命更加广为人知——显卡。阴差阳错地，科学家们发现GPU芯片的并行计算架构与深度学习一拍即合，猛然把机器原先需要几个月的“学习”时间压缩到了几天、甚至几个小时之内，两强搭档，在2012年的ImageNet上大放异彩。不过，英特尔、IBM等老牌云服务器芯片厂商同样在积极布局这一市场，各自通过并购、投资、研发等方式不断切入云AI芯片市场。与此同时，谷歌作为创新科技的代表企业，也从2014年起开始打造属于自己的云AI芯片——TPU（张量处理单元），谷歌的AI程序AlphaGo（阿尔法狗）就是靠它提供的强大计算能力战胜了全球围棋第一高手柯洁。大约20天前，谷歌宣布将TPU的强大计算能力通过谷歌云将向用户开放。谷歌AI中国中心总裁&谷歌云AI研发主管李佳曾经向ZDX表示，AI芯片的热潮将会长时间影响AI行业，芯片的进步将会对人工智能行业发展带来一个正向的支持，通过算力的进步与发展，未来将会给人工智能带来更多的机会、更大的想象空间。自此，人工智能迎来了自学科建立60年以来最大规模的市场应用爆发潮，从曾经遥不可及的前沿技术忽然一夜之间就来到了我们身旁，成了家喻户晓的风口话题。如果说2015-2016年是AI芯片巨头们厮杀激烈的争夺战，那么接下来的2016-2017年则是国内AI芯片创业市场逐渐升温加热、最终大红大火、进入白热化竞争的时段。随着市场的进一步升温发酵，到了2017年年底，市场竞争达到了前所未有的激烈程度。仅仅是在2017年10月-11月这两个月间，就有四间国内AI芯片公司同时宣布获得千万美元以上的大额融资，如果从2017年8月开始算起，四个月间至少有十几款AI芯片面市。芯片产品升级周期普遍长达12-24个月，如今这样密集火热的市场轰炸，简直有如一场AI芯片的集体狂欢。狂欢后的思考不过，集体狂欢的背后，总是存在冷静的声音，不少业内人士都曾经向ZDX表示过对AI芯片行业几点担忧：1）芯片竞争非常惨烈，是个“数一数二”的行业。芯片的优势在于规模，一块普通芯片的销量需要达到百万颗级别才能达到盈亏平衡。一旦市场开始成熟，巨头们能够通过十倍、百倍的产量进行市场收割，这些大量涌现的AI芯片创业公司可能最终只能跑出来一两家。2）人工智能技术宣传多，行业落地少。几乎所有AI芯片创业公司都将自己的主要目标定在了安防与自动驾驶这两类市场上，对于安防而言，AI这一市场需求虽然已经存在多年，但是大规模远距离场景下的AI应用还有很多工程化问题需要解决；而对于自动驾驶而言，一则这一技术离大规模落地商用还有一段时间，二则各大车厂把安全属性奉为至高，对人工智能目前的“黑箱”状况较难接受。现在除了手机外，并没有哪一款AI芯片销售量超过100万块的。3）人工智能算法还在快速发展过程中，每半年到一年间计算模型都在发生变化。而芯片的设计研发周期普遍较长，只有非常成熟的算法才适合固化到芯片上，现在的人工智能算法还不够成熟。中国半导体行业协会IC设计分会理事长、清华大学微电子学研究所所长魏少军教授认为，从产业发展规律来看，在今明两年之内AI芯片将持续火热，大家扎堆进入；但是到了2020年前后，则将会出现一批出局者，行业洗牌开始。芯片，国之根本作为所有电子产品的“大脑”，芯片对于科技发展的重要性无需赘言。长久以来，中国大陆地区芯片技术发展落后，严重依赖海外进口：2013年以来，中国大陆用在进口芯片上金额已超高达2000亿美元，芯片已经超过石油，成为国内第一大进口商品。与发达国家与地区相比，我国大陆地区芯片制造工艺差距不小。虽然近年来国内芯片设计产业保持着快速增长（约20%），但总的来说，产业发展依旧存在自主创新能力弱、关键核心技术对外依存度度、人才缺乏等问题。举个例子，现在台积电、英特尔等已经在进行7nm芯片的技术开发当中，而我国大陆地区现阶段28nm工艺才刚刚顺利普及，14nm工艺仍在攻关当中。不过，AI芯片的崛起，为中国长久以来只能埋头追赶的芯片行业打开了一扇新大门。在这类新兴技术的战场上，我们第一次与技术发达国家站在了同一起跑线上，在有些领域甚至站在了全球科技发展的前列。国家对于人工智能及AI芯片的重视程度也达到了前所未有的高度，去年7月与12月，国务院与国家工业和信息化部接连发布《新一代人工智能发展规划》、《促进新一代人工智能产业发展三年行动计划》，对于人工智能行业下一步的发展规划提出了方向性的意见。其中除了包括我国面向2030年的新一代人工智能“三步走”战略，还将神经网络芯片（AI芯片）提到了三大核心技术之一的高度。《行动计划》指出，到2020年，国内AI芯片技术需要取得突破进展，包括云端神经网络芯片和终端神经网络芯片。与此同时，AI芯片需要实现在智能终端、自动驾驶、智能安防、智能家居等重点领域的规模化商用。而在今天公布的总理2018年政府工作报告中还提出，要加强新一代人工智能研发应用，在医疗、养老、教育、文化、体育等多领域推进“互联网+”。结语：重塑世界的AI芯片在一场AI芯片大火烧过了2017年之后，2018年，我们将看到一大批AI芯片初创企业的产品正式落地商用，各大芯片巨头玩家的相关产品也会逐一面市，我们即将进入“无芯片不AI、无终端不AI、无行业不AI”的时代当中。未来，每个芯片都提升它的AI计算能力，小到一个耳机、一台音箱，大到一台工业机器人、一辆自动驾驶汽车，这些终端的AI化将深入到各行各业当中，除了文中重点解读的四大行业，未来，工业、制造、医疗、教育等行业也会逐渐被AI芯片渗透入侵；随着AI计算逐步渗透社会各行各业，我们的世界将逐渐被AI重塑，我们也将进入一个AI社会。随着巨头玩家的不断入局、创业公司的快速奔跑，AI芯片——作为人工智能产业皇冠上的明珠——已经逐渐成为了一场高手间的较量。这一新兴技术既为科技巨头带来了业务升级、产业扩大的新风口；又也为各大创业者提供了颠覆现有格局，重塑科技话语权的崭新机会；同时还为中国半导体产业实现超车提供了一个绝好的新机遇。"}
{"content2":"一、简单科普类（文末附下载链接）1、《人工智能：李开复谈AI如何重塑个人、商业与社会的未来图谱2》作者：李开复，王咏刚推荐理由：文章写得一般，但李开复和王永刚老师总结的还可以，算国内比较简单的一本AI科普作品图书简介：人工智能被写入2017年政府工作报告，智能革命时代先行者李开复，人工智能工程院副院长王咏刚携手解读：人工智能时代，个人与企业如何找到人机协作的新位置！任何企业都需要尽早引入“AI+”的思维方式！2、《人工智能》（精）作者：腾讯研究院，中国信息通信研究院互联网法律研究中心，腾讯AI，Lab，腾讯开放平台推荐理由：腾讯一流团队与工信部高端智库倾力创作。本书从人工智能这一颠覆性技术的前世今生说起，对人工智能产业全貌、目前进展、发展趋势进行了清晰的梳理，对各国的竞争态势做了深入研究，还对人工智能给个人、企业、社会带来的机遇与挑战进行了深入分析。对于想全面了解人工智能的读者，本书提供了重要参考，是一本必备书籍。图书简介：腾讯携手工信部倾力打造国家人工智能战略行动抓手！比尔·盖茨、埃隆·马斯克、扎克伯格、李彦宏、马化腾、李开复、雷军、刘庆峰等跨界大咖都在关注的科技新革命！3、《未来地图》作者：【美】吴霁虹推荐理由：吴霁虹教授为技术公司、传统企业、个人创业者、机构投资者，绘制了一份AI商业化的价值地图，这也是一张AI商业化的战略发展图。重点是AI+放面的科普，书写的一般，但总结的很全面。图书简介：创造人工智能万亿级产业的商业模式和路径4、《智能时代》作者：吴军推荐理由：吴军博士通过对大数据和人工智能的洞察，写出了自己对未来的畅想，书写的很好也很全面，值得一读。图书简介：雷军、罗振宇、李善友联袂推荐。大数据、智能革命、人工智能、机械智能领域首要作品。易读、有态度、有温度的科普作品。文津图书奖得主，中国好书榜获奖作者全新作品。5、《数学之美》作者：吴军推荐理由：吴军博士深入对大数据和机器学习的论述，很有意思很有质感的科普著作。图书简介：浪潮之巅 文明之光 硅谷之谜 大学之路 见识作者吴军博士作品 源自谷歌黑板 根植于谷歌方法论 新版增加大数据和机器学习内容 文津图书奖获奖书 央视新闻推荐的学科敲门砖6、《智能的本质：人工智能与机器人领域的64个大问题》作者：[美] 皮埃罗·斯加鲁菲 著；任莉，张建宇推荐理由：作者从常识出发，对人工智能和机器人表达了很多“令人惊讶”。而又让人深思的观点。并就人工智能与机器人领域的64个大问题。做出了深入的探讨，很值得一看的科普读物。图书简介：斯坦福、伯克利客座教授30年AI研究巅峰之作 以常识解读AI的核心算法 平息人工智能的狂热与恐慌 中国人工智能学会刘宏 赛迪研究院樊会文 清华大学iCenter韩锋 驭势科技吴甘沙等推荐7、《人工智能的未来》作者：[美] 雷·库兹韦尔（Ray Kurzweil） 著；路慧 编；盛杨燕 译推荐理由：美国国家技术奖获得者、奇点大学校长、谷歌公司工程师总监雷-库兹韦尔最新力作。也是库兹韦尔对于大脑和人工智能的理解，将对我们生活的方方面面、各行各业，以及我们有关未来的设想产生巨大的影响。图书简介：8、《人工智能时代》作者：[美] 杰瑞·卡普兰（Jerry Kaplan） 著；李盼 译推荐理由：经济学人2015年年度图书，卡普兰在《人工智能时代》一书中从企业、税收和保险等机制上构建了一个有益的经济生态，让社会中的每一个人都能从技术发展中获益，带领我们一窥人机共生下财富、工作与思维的大未来。图书简介：9、《机器崛起：遗失的控制论历史》作者：[德] 托马斯·瑞德推荐理由：一本读懂人工智能前世今生的书，一部气势恢宏的人工智能发展史。图书简介：《纽约时报》《华尔街日报》等100多家媒体联袂推荐的2016年度好书！贺福初、戴浩、孙优贤、桂卫华四位院士以及凯文·凯利等中外专家倾情推荐！一本读懂人工智能前世今生的书,一部气势恢宏的人工智能发展史！二、深度科普类1、《AI：人工智能的本质与未来》作者：[英] 玛格丽特·博登 著；孙诗惠推荐理由：人工智能界神级女性牛人的详尽之作。作者从专业的角度，深入浅出，梳理了人工智能发展的历程，其经历的不同阶段概括，如今最前沿的发展现状以及面临的困境，并探讨了其未来发展的可能性，堪称一部精彩的人工智能进化史。图书简介：2017年知识发布会推荐，指向未来的图书2、《未来简史：从智人到智神》作者：尤瓦尔赫拉利推荐理由：作者尤瓦尔赫拉利全球瞩目力作，烧脑奇书，颠覆认知，刷新你的世界观，有人吹牛逼说。错过这本书的人将错过未来。所以还是读读吧。图书简介：3、《终极算法：机器学习和人工智能如何重塑世界》作者：[美] 佩德罗·多明戈斯（Pedro Domingos） 著；黄芳萍 译推荐理由：全球好的算法问题专家、机器学习领域的先驱人物佩德罗-多明戈斯，我们揭开了算法的神秘面纱，让我们一窥谷歌、亚马逊以及你的智能手机背后的机器学习原理。图书简介：比尔·盖茨年度荐书！近20年人工智能领域轰动的著作！沃尔特·艾萨克森、车品觉、曹欢欢联袂推荐！4、《奇点临近》作者：[美] 库兹韦尔（Ray Kurzweil） 著；李庆诚，董振华，田源 译推荐理由：《奇点临近》是一本有思维方法论启示的书；十一本站在历史的高度，正面思考科技力量的书；是一本充满想象与预言，但又不失科学论证的书。可以很好的开阔我们对未来的视野。图书简介：一部预测人工智能和科技未来的奇书。在《奇点临近》中，库兹韦尔通过将进化划分为六大纪元，探讨和分析了科学发展趋势5、《心智社会》作者：[美] 马文明斯基推荐理由：人工智能之父、图灵奖得主马文-明斯基，对科学和人性的浪漫主义解读。图书简介：6、《计算机与人脑》作者：[美] 冯·诺伊曼推荐理由：这本书有点老了，曾经被誉为二十世纪的天才之作。是二十世纪杰出的数学家、经济学家、×××先驱、计算机之父冯-诺依曼的代表作之一。读这本书你就会发现，你现在的很多想法，在几十年前就被想到了，老经典，感兴趣的同学可以读读。图书简介：7、《机器之心》作者：[美] 雷·库兹韦尔（Ray Kurzweil） 著；胡晓姣，张温卓玛，吴纯洁 译推荐理由：科技预言大师雷-库兹韦尔巅峰之作，这是一个可怕的“疯子”撰写的一本疯狂的、扭曲的、不可思议的、其实妙想的，但又能极其让人信服的，也许会让你三观尽毁、脑洞大开的书，很适合大家开脑洞。图书简介：预言大师雷·库兹韦尔巅峰之作！8、《科学的极致：漫谈人工智能》作者：集智俱乐部著推荐理由：趣味又通俗易懂的一本科普书，全面介绍了人工智能的历史及其在各个领域的发展及应用。不仅涵盖了人机交互、脑科学、计算心理学、系统科学、社会科学等各个学科的基础理论，而且广泛讲述了人工智能在算法、软件、硬件等方面的应用以及跨学科应用。图书简介：9、《认知计算与人工智能》作者：IBM商业价值研究院推荐理由：这本书是IBM的一份报告，以人工智能为背景，全面细致地介绍了认知计算的概念，进而结合九个行业的实践成果，深入介绍了认知计算的实际应用，对各行业的思想者、经营者和技术领导者都非常有启发性。图书简介：IBM首次揭示机器学习的底层革命，打造拥有认知能力的人工智能三、技术类1、《人工智能：一种现代的方法》作者：[美] 罗素（Stuart J.Russell），[美] 诺维格（Peter Norvig） 著；殷建平，祝恩，刘越 等 译推荐理由：最权威、最经典的人工智能教材，你被全世界100多个国家的1200多所大学用作教材。英文版有1100多页，教学内容非常丰富，不但涵盖了人工智能基础、问题求解、知识推理与规划等经典内容，而且还包括不确定知识与推理、机器学习、通讯感知与行动等专门知识的介绍。图书简介：看AlphaGo（阿尔法狗）是如何成长的，美国伯克利大学与Google人工智能科学家合作编写，全世界100多个国家1200多所大学使用。A Must Read for AI2、《深度学习》作者：[美] Ian，Goodfellow，[加] Yoshua，Bengio，[加] Aaron推荐理由：由全球知名的三位专家lan Goodfellow、Yoshua Bengio和Aaron Courville撰写，是深度学习领域奠基性的经典教材，被称为深度学习的圣经。要学习人工智能，一定要读这本书。图书简介：AI圣经 Deep Learning中文版 长期位居美国ya马逊人工智能和机器学习类图书榜首 深度学习领域奠基性的经典畅销书 特斯拉CEO埃隆·马斯克等国内外众多专家推荐3、《人工智能：复杂问题求解的结构和策略》作者：[美] 卢格（GeorgeF.Luger）推荐理由：也是一本经典的人工智能教材，全面阐述了人工智能的基础理论，有效结合了求解智能问题的数据结构以及实现的算法，把人工智能的应用程序应用于实际环境中，并从社会和哲学、心理学以及神经生理学角度对人工智能进行了独特的讨论。图书简介：4、《机器学习》作者：周志华推荐理由：南京大学计算机系教授周志华老师的代表做教材，深入浅出、内容全面、结构合理、叙述清楚，总之人手一本就对了，唯一列出的中国人写的教材。图书简介：击败AlphaGo的武林秘籍，赢得人机大战的必由之路：人工智能大牛周志华教授巨著，全面揭开机器学习的奥秘5、《深入理解机器学习：从原理到算法》作者：[以] 沙伊·沙莱夫-施瓦茨，[加] 沙伊·本-戴维 著；张文生 等 译推荐理由：作者既讲述最重要的机器学习算法的工作原理和动机，还指出其固有的优势和缺点，是有兴趣了解机器学习理论和方法以及应用的学生和专业人员的良好教材和参考书。图书简介：近几年机器学习理论方面的优秀著作，全面清晰地讨论了机器学习理论领域重要的算法和思想。由两位改论语理论基础的关键贡献者所著，内容覆盖理论基础到算法。6、《数据挖掘概念与技术》作者：[美] Jiawei Han，[美] Micheling Kamber，[美] Jian Pei 等 著；范明，孟小峰 译推荐理由：是数据挖掘和知识发现领域内的所有教师、研究人员、开发人员和用户都必读的参考书，是一本适用于数据分析、数据挖掘和知识发现课程的优秀教材，可以用作高年级本科生或一年级研究生的数据挖掘导论教材。图书简介：数据挖掘领域里程碑意义的经典著作！中文版、影印版同步上市！决战大数据时代！IT技术人员不得不读7、《神经网络与机器学习》作者：[加] Simon Haykin 著；申富饶 等 译推荐理由：作者结合近年来神经网络和机器学习的最新进展，从理论和实际应用出发，全面、系统地介绍了神经网络的基本模型、方法和技术，并将网络和机器学习有机结合在一起。图书简介：利用逐次状态估计算法训练递归神经网络。8、《机器学习实战》作者：[美] Peter Harrington 著；李锐，×××，曲亚东 等 译推荐理由：通过精心编排的实例，切入日常工作任务，摒弃学术化语言，利用高效可复用的python代码阐释如何处理统计数据，进行数据分析及可视化。读者可从中学到一些核心的机器学习算法，并将其运用于某些策略性任务中，如分类、预测及推荐等。图书简介：人工智能开发图书 深度学习实践应用 利用Python阐述机器学习算法 配合日常用例 强劲实战导向 程序员人手常备9、《贝叶斯方法：概率编程与贝叶斯推断》作者：[加] Cameron Davidson-Pilon（卡梅隆·戴维森-皮隆） 著；辛愿，钟黎，欧阳婷 译；余凯，岳亚丁 校推荐理由：本书适用于机器学习、贝叶斯推断、概率编程等相关领域的从业者和爱好者，也是和普通开发人员了解贝叶斯统计而使用。图书简介：机器学习 人工智能 数据分析从业者的技能基础 国际杰出机器学习专家余凯博士 腾讯专家研究员岳亚丁博士推荐 下一个十年 掌握贝叶斯方法 就像今天掌握C C++ Python一样重要 全彩印刷10、《图像处理、分析与机器视觉》作者：Milan Sonka，Vaclav Hlavac，Roger Boyle推荐理由：计算机视觉经典教材，针对图像处理、图像分析和机器视觉领域的有关原理与技术进行了广泛而深入的讨论，包括图像预处理、图像分割、形状表示与描述、物体识别与图像原理、三维视觉、数学形态学图像处理技术、离散图像变换、图像压缩、纹理描述、运动分析等。图书简介：11、《机器视觉》作者：[美] 伯特霍尔德·霍恩 著；王亮，蒋欣兰 译推荐理由：作为作者在麻省理工学院（MIT）所讲授的机器视觉课程的指定教材，本书已经被使用了近30年，至今仍被欧美许多著名高校所广泛使用。图书简介：12、《模式识别》作者：[希腊] Sergios，Theodoridis（西格尔斯.西奥多里蒂斯），Konstantinos，Koutroumbas（康斯坦提诺斯.库特龙巴斯） 著；李晶皎 等 译推荐理由：本书在完美的和当前的理论与实践的基础上，讨论了贝叶斯分类、贝斯网络、线性和非线性分类器设计、上下文相关分类、特征生成、特征选取技术、学习理论的基本概念以及聚类概念与算法。图书简介：13、《Python自然语言处理》作者：[美] Steven，Bird　Ewan，Klein　Edward，Loper 著；陈涛，张旭，崔杨，刘海平 译推荐理由：是自然语言处理领域的一本使用入门指南，旨在帮助读者学习如何编写程序来分析书面语言。可以作为自然语言处理或计算语言学课程的教科书，还可以作为人工智能、文本挖掘、语料库语言学等书课程的补充读物。图书简介：14、《解析深度学习：语音识别实践》作者：俞栋，邓力 著推荐理由：是首部介绍云识别中深度学习技术细节的专著。全书首先概要介绍了传统语音识别理论和经典的深度神经网络核心算法。接着全面而深入地介绍了深度学习在语音识别中的应用，包括“深度神经网络-隐马尔可夫混合模型”的训练和优化，特征表示学习、模型融合、自适应，以及循环神经网络为代表的若干先进深度学习技术。图书简介：了解深度学习应用实践不可错过的经典专著15、《游戏人工智能编程案例精粹》作者：[美] Mat Buckland 著；罗岱 等 译推荐理由：是游戏人工智能方面的经典之作，畅销多年。它展示了如何在游戏中利用专业人工智能技术，并针对实际困难问题给出了强有力的解决方法。图书简介：游戏编程实例教程，游戏引擎设计精粹，android游戏开发宝典！四、机器人类16、《机器人手册》3卷作者：布鲁诺·西西利亚诺推荐理由：《机器人手册》由来自美国、德国、日本、意大利、加拿大等国家和地区的167位专家著写而成，凝聚了全球科学家的科技力量，绝对是机器人领域的百科全书！图书简介：17、《机器人学：建模、规划与控制》作者：[意] 布鲁诺·西西里安诺，[意] 洛伦索·夏维科，[意] 路易吉·维拉尼，[意] 朱塞佩·奥里奥洛 著；张国良，曾静，陈励华，敬斌 译推荐理由：一本教材就能将机器人的操控技术与移动技术从基础知识到前沿研究这么长跨度的内容囊括其中，并能在数学的深度和物理直觉两方面达到创造性的平衡。图书简介：18、《机器人学导论》作者：克来格，貟超 著推荐理由：曾作为美国斯坦福大学机器人学导论的教材，经过两次修订。书中还包括大量分级的习题和编程作业，适合教学参考。图书简介：19、《机器人浪潮》作者：[日] 神崎洋治（Kozaki Yoji） 著；黄笛 译推荐理由：在本书中，作者介绍了机器人产业的现状以及远景、其背后的特征、技术、商业领域的应用以及APP开发和人工智能等相关内容。拥有感情和自主学习能力的机器人与人类共生的社会景象越来越具有现实感。图书简介：世界首款情感识别型机器人发售 人类历史上前所未有的巨变正在急速发生20、《工业机器人及零部件结构设计》作者：李慧，马正先，逄波 著推荐理由：本书从工业机器人设计及应用的角度出发，通过设计案例较为全面系统的剖析了工业机器人设计与结构之间的关系。图书简介：工业机器人（冷、热冲压用机器人，数控机床机器人，装配机器人、模块化工业机器人）设计及案例详解五、哲学类1、《人工智能哲学》作者：[英] 博登，刘西瑞，王汉琦 著推荐理由：收集了人工智能研究领域著名学者的15篇代表性论文，这些论文为计算机科学的发展和人工智能哲学的建立做出了开创性的贡献。人工智能哲学是伴随现代信息理论和计算机技术发展起来的一个哲学分支。图书简介：2、《皇帝新脑》作者：[英] 彭罗斯 著；许明贤，吴忠超 译推荐理由：本书对电脑科学、数学、物理学、宇宙学、神经和精神科学以及哲学进行了广泛、深入浅出的讨论，体现了作者向哲学上最大问题“精神-身体关系”挑战的大无畏精神。书中充满了天才般的猜测，贯穿着探索真理的灵感和激情。图书简介：罗杰·彭罗斯教授担任牛津大学极为有名的罗斯·玻勒教学讲席。在探索未知的天空物理和数学领域中，他是当今领导者之一。彭罗斯教授与蒂芬·霍金教授曾给合作研究，黑洞及引力，而于1988年共获渥夫物理奖。“彭罗斯教授在这本曾获得科学收籍奖的《皇帝新脑》中力图解答人类的谜题：人脑是如何思想？……彭罗斯是当今最受推崇的数学家兼物理学家之一，能以精深的数理来推论宇宙间的一切事务。”3、《人类2.0》作者：[美] 皮埃罗·斯加鲁菲，牛金霞，闫景立 著推荐理由：皮埃罗重新定义科技带给人类的影响，并推出“人类2.0”概念，与硅谷科学家、创业者解读科技领域最关注的十个领域带给人类的影响。图书简介：硅谷精神布道师、人工智能认知科学家、《硅谷百年史》作者皮埃罗新作，涂子沛、暴走恭亲王、吴霁虹联袂推荐。皮埃罗详述人工智能、虚拟现实、生物技术、区块链等十种科技，带你一起探索科技未来，定义人类2.04、《哥德尔、艾舍尔、巴赫》作者：[美] 侯世达 著；本书翻译组 译推荐理由：它通过对哥德尔的数理逻辑，艾舍尔的版画和巴赫的音乐三者的综合阐述，引人入胜地介绍了数理逻辑学、可计算理论、人工智能学、语言学、遗传学、音乐、绘画的理论等方面，构思精巧、含义深刻、视野广阔，富于哲学韵味。图书简介：书籍下载地址 （书籍整理自网络，只做个人学习使用，禁止用于商业用途）链接：https://pan.baidu.com/s/1nvrExHB 密码：zl66--------------------- 本文来自 扑满心 的CSDN 博客 ，全文地址请点击：https://blog.csdn.net/sinat_38648491/article/details/78905394?utm_source=copy"}
{"content2":"大疆2018年7月发布在官网上的岗位，链接：https://we.dji.com/zh-CN大疆的技术分享会时间：3月下旬到6月上旬，宣讲路线是：广州，武汉，北京，哈尔滨，西安，南京，上海，合肥，成都。校园招聘宣讲时间：5月30到6月10日，宣讲地点是：东南，南航，哈深，华科，成电，北航，北理，清华，西北工业，西安交通，华理，哈工大。招聘流程（秋招-研二暑假）：5月30日至6月30日简历投递（线上），6月低初筛，7月初技术笔试（线上），7月下旬面试（远程+现场），9月中旬前出最终录取结果。大疆2018年机器视觉/图像相关岗位的招聘要求如下：（2018年7月18日前官网已发布的）机器视觉/深度学习算法工程师工作职责1. 负责深度学习、计算机视觉相关算法、软件方面的研发；2. 与FPGA/ASIC工程师配合，实现深度学习，计算机视觉相关算法的高性能实现。任职要求1. 电子工程，计算机，人工智能等专业硕士以上学历；2. 精通C/C++, Python编程语言和工具，熟悉Caffe，Tensorflow等平台；3. 对相关算法的产品化，工程化兴趣浓厚；4. 了解FPGA或ASIC设计，有算法定点化设计经验者优先；5. 强烈的责任心，创新意识，逻辑思维及学习能力。机器视觉算法工程师工作职责1. 为机器视觉核心功能（匹配定位、测量、图像检测、分类、条码识别等）设计、开发工业级的高效、鲁棒图像算法；2. 在PC平台设计、开发算法，与嵌入式软件工程师合作，在嵌入式处理平台对算法进行移植；3. 指导视觉应用工程师解决产线具体需求，保证所开发的算法模块在产线的部署效果，为开发模块的正确性、效率和稳定性负责。任职要求1. 硕士及以上学历；2. 具有扎实的图像处理基本功。能够独立高效实现图像处理与计算机视觉的典型底层算法；3. 熟练掌握机器学习的典型方法；理解典型分类器的方法与使用特点；有深度神经网络设计与实践经验优先；4. 对计算机体系与结构有一定了解，能够针对处理器的特点优化算法设计；5. 了解3D成像原理与相关算法优先。机器学习算法工程师工作职责1. 负责研发机器学习、模式识别、深度学习算法开发；2. 负责研发基于图像/点云/雷达的物体检测、运动估计以及语义分割算法；3. 负责开发多传感器融合的三维环境感知算法；4. 负责与嵌入式深度学习算法工程师协作，针对产品平台进行算法优化。任职要求1. 计算机科学、信息工程、电子工程、机器人学等专业硕士及以上学历，有C++/Python开发经验；2. 在模式识别、机器学习、深度学习、机器人学等领域有深入认识，并了解各个算法的条件和瓶颈；3. 具有点云数据处理、立体视觉、多视图几何、多传感器融合等科研或开发经验者优先；4. 具有物体识别、物体检测、目标跟踪、语义分割、人脸识别等科研或开发经验者优先；5. 在相关领域主流会议或期刊发表过论文 (CVPR/ICCV/ECCV/NIPS/ICML/ICLR/IROS/ICRA)者优先；6. 具有良好的沟通和写作能力图像/视觉算法工程师工作职责1. 开发及优化用于视觉标定和定位、重建的图像处理算法；2. 针对不同平台的特点，对算法进行深入优化；3. 负责维护相关的核心软件算法模块；4. 支持相关项目的视觉标定、定位任务。任职要求1. 硕士及以上学历；2. 熟练掌握Windows和Linux环境下的C/C++编程；3. 熟悉图像处理和计算机视觉；4. 熟悉OpenCV、ceres solver等开源软件包。加分项：1. 熟悉ARM/DSP平台优化；2. 了解或使用过一种嵌入式系统硬件架构和嵌入式操作系统。图像处理算法工程师工作职责1. 参与图像处理算法原型设计、功能开发；2. 参与算法集成/加速/硬化；3. 参与算法产品化流程。任职要求1. 硕士及以上学历，电子、通信、信号处理相关专业；2. 具有1年以上ISP算法IP设计经验/3A算法设计/集成/调优经验；3. 熟悉传统ISP链路，了解sensor矫正、去马赛克、色彩矫正、各类去噪、锐化、增强算法；4. 或具有1年以上后处理（图像增强/融合/分割/深度提取/目标检测识别）算法设计/优化经验；5. 精通C/C++语言编程，精通数据结构，具有良好的编程习惯；6. 具备嵌入式开发能力者优先。图像算法优化工程师工作职责1. 与算法工程师对接，负责算法集成方案的设计和系统评估；2. 负责各类算法的DSP/GPU等平台性能优化。任职要求1. 硕士及以上学历，电子、通信、信号处理相关专业；2. 具备一年以上各类图像算法优化经验；3. 熟练掌握算法优化技巧，精通ARM NEON优化、或DSP架构/优化、或GPU加速/OpenGL/OpenCL编程；4. 熟练掌握嵌入式系统相关知识，具备各类系统设计和评估（内存、带宽、计算效率、延迟等）能力。（完）"}
{"content2":"不多说，直接上干货！直接来个现实的例子。某公司招聘....2017年注定是深度学习火热之年，同时，博主我也正值研二学年，广深阅读文献搞科研，致力于大数据和机器学习深度学习领域。同时，分享后续这方面的干货知识，大家一起成长和学习！深度学习的概念源于人工神经网络的研究，含多隐层的多层感知器就是一种深度学习结构。它是机器学习研究的一个新领域，模仿人脑机制来解释数据。目前“大数据”、“推荐系统”、“深度学习”是数字智能领域的热点研究方向，相关的书籍也很火热，比如“大数据”仅这两年就出版了很多本，让一般人看的眼花缭乱。个性化推荐系统确实很会“察言观色”，针对不同的用户，主动推送不同的3D打印内容。但如果你认为它真正有了“人工智能”，那你就错了。其实，这些推荐系统背后的运行原理主要基于概率统计、矩阵或图模型，计算机对这些数值运算确实很擅长，但由于采用的只是“经验主义”的实用方法（也即管用就行），而非以“理性主义”的原则真正探求智能产生的原理，所以距离真正的人工智能还很远。AI（Artificial Intelligence），也就是人工智能，就像长生不老和星际漫游一样，是人类最美好的梦想之一。虽然计算机技术已经取得了长足的进步，但是到目前为止，还没有一台计算机能产生“自我”的意识。直到深度学习（Deep Learning）的出现，让人们看到了一丝曙光，至少，（表象意义下的）图灵测试已不再是那么遥不可及了。2013年4月，《麻省理工学院技术评论》杂志将深度学习列为2013年十大突破性技术（Breakthrough Technology）之首。有了深度学习，推荐系统可以更加深度地挖掘你内心的需求，并从海量的3D模型库中挑选出最合适的供你打印。而深度学习（Deep Learning），恰恰就是通过组合低层特征形成更加抽象的高层特征（或属性类别）。例如，在计算机视觉领域，深度学习算法从原始图像去学习得到一个低层次表达，例如边缘检测器、小波滤波器等，然后在这些低层次表达的基础上，通过线性或者非线性组合，来获得一个高层次的表达。此外，不仅图像存在这个规律，声音也是类似的。比如，研究人员从某个声音库中通过算法自动发现了20种基本的声音结构，其余的声音都可以由这20种基本结构来合成！在进一步阐述深度学习之前，我们需要了解什么是机器学习（Machine Learning）。机器学习是人工智能的一个分支，而在很多时候，几乎成为人工智能的代名词。简单来说，机器学习就是通过算法，使得机器能从大量历史数据中学习规律，从而对新的样本做智能识别或对未来做预测。而深度学习又是机器学习研究中的一个新的领域，其动机在于建立可以模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，例如，图像、声音和文本。深度学习是无监督学习的一种。深度学习之所以被称为“深度”，是因为之前的机器学习方法都是浅层学习。深度学习可以简单理解为传统神经网络（Neural Network）的发展。大约二三十年前，神经网络曾经是机器学习领域特别热门的一个方向，这种基于统计的机器学习方法比起过去基于人工规则的专家系统，在很多方面显示出优越性。深度学习与传统的神经网络之间有相同的地方。二者的相同之处在于，深度学习采用了与神经网络相似的分层结构：系统是一个包括输入层、隐层（可单层、可多层）、输出层的多层网络，只有相邻层节点（单元）之间有连接，而同一层以及跨层节点之间相互无连接。这种分层结构，比较接近人类大脑的结构（但不得不说，实际上相差还是很远的，考虑到人脑是个异常复杂的结构，很多机理我们目前都是未知的）。深度学习通过学习一种深层非线性网络结构，只需简单的网络结构即可实现复杂函数的逼近，并展现了强大的从大量无标注样本集中学习数据集本质特征的能力。深度学习能够获得可更好地表示数据的特征，同时由于模型的层次深（通常有5层、6层，甚至10多层的隐层节点，“深”的好处是可以控制隐层节点的数目为输入节点数目的多项式倍而非多达指数倍）、表达能力强，因此有能力表示大规模数据。对于图像、语音这种特征不明显（需要手工设计且很多没有直观的物理含义）的问题，深度模型能够在大规模训练数据上取得更好的效果。尤其是在语音识别方面，深度学习使得错误率下降了大约30%，取得了显著的进步。相比于传统的神经网络，深度神经网络作出了重大的改进，在训练上的难度（如梯度弥散问题）可以通过“逐层预训练”来有效降低。注意，深度学习不是万金油，像很多其他方法一样，它需要结合特定领域的先验知识，需要和其他模型结合才能得到最好的结果。当然，还少不了需要针对自己的项目去仔细地调参数，这也往往令人诟病。此外，类似于神经网络，深度学习的另一局限性是可解释性不强，像个“黑箱子”一样不知为什么能取得好的效果，以及不知如何有针对性地去具体改进，而这有可能成为产品升级过程中的阻碍。深度学习通过很多数学和工程技巧增加（堆栈叠加：Stack）隐层的层数，如果隐层足够多（也就是深），选择适当的连接函数和架构，就能获得很强的表达能力。但是，常用的模型训练算法反向传播（Back Propagation）仍然对计算量有很高的要求。而近年来，得益于大数据、计算机速度的提升、基于MapReduce的大规模集群技术的兴起、GPU的应用以及众多优化算法的出现，耗时数月的训练过程可缩短为数天甚至数小时，深度学习才在实践中有了用武之地。大数据这个时代背景上来。当坐拥海量的大数据，我们无论是做推荐系统还是3D模型检索，以前用简单的线性数学模型，一般也能获得还不错的结果。因此我们沾沾自喜起来，认为还是大数据更重要，而智能算法用简单直接的就OK了，不需要也没必要弄得很复杂。而当深度学习出现后，它的一系列辉煌战绩让我们意识到：也许是时候该“鸟枪换炮”了。简而言之，在大数据情况下，也许只有比较复杂的模型，或者说表达能力强的模型，才能充分发掘海量数据中蕴藏的有价值信息。更重要的是，深度学习可以自动学习特征，而不必像以前那样还要请专家手工构造特征，极大地推进了智能自动化。深度学习（即所谓“深度”）应大数据（即所谓“广度”）而生，给大数据提供了一个深度思考的大脑，而3D打印（即所谓“力度”）给了智能数字化一个强健的躯体，三者共同引发了“大数据＋深度模型＋3D打印”浪潮的来临。扩展学习人工智能、机器学习和深度学习之间的区别与联系（图文详解）"}
{"content2":"Bengio最新博文：深度学习展望人类一直梦想着创造有智能的机器。早在第一台可编程计算机问世前100多年，发明家就对如何能让由连杆和齿轮组成的设备也变得更加智能这一命题充满好奇。后来，20世纪40年代计算机领域的先驱者之一Alan Turing通过描述一个测试为计算机科学设定了目标，这个测试也就是后来被大家所熟知的图灵测试，用以衡量计算机的表现和人类行为的接近程度。（注：图灵测试一词来源于计算机科学和密码学的先驱Alan Turing写于1950年的一篇论文《计算机器与智能》。Alan Turing 1950年设计出这个测试，其内容是，如果电脑能在5分钟内回答由人类测试者提出的一系列问题，且其超过30%的回答让测试者误认为是人类所答，则电脑通过测试。）在我学术生涯早期研究人工智能领域的时候，科学家们解决的都是一些对人类来说困难，而对计算机来说会相对简单的任务，比如大规模的数学计算。然而在最近几年，我们正在进行的一些项目都是对人类来说非常容易，或者说下意识就能解决的任务，比如语音识别和人群中的人脸识别，然而这些任务却很难让计算机理解。真正让我感兴趣的是在人工智能领域开始有了更为复杂的探索，即让计算机获得自主学习的能力。当然我的目的并不是让计算机完全像人类一样思考，我只是想去理解那些能让实体、计算机或生物变得智能化的一些基础原则。我很久以前打过一个赌，说是如果人工智能真的能够实现的话一定是得益于实体的学习能力，因此我一直专注于构建能够学习并自行理解这个世界的计算机。我相信此刻我们正处于人工智能甚至是计算机本身的一个历史转折点。得益于现在更为强大的计算机、可用的海量丰富数据集以及先进的算法，我们终于可以跨越一个长期以来阻碍计算机科学发展的阈值。机器学习正在从一个高度人工化的阶段向另一个更为自动化的阶段进行快速转变，前者需要我们人为地对每个项目进行设计并提取其中较好的特征，后者可以实现让计算机像孩子一样，通过不断学习获得的经验来积累内部特征，从而理解这个世界，这就是我们所说的深度学习。深度学习并不是一个全新的概念。事实上，在20世纪80年代我还是一个学生的时候，这是一个神经网络里的概念，也可以说是深度学习的前身，我对这个概念非常着迷并决定要在计算机科学领域开启我的学术生涯。目前深度学习真正新的进展在于，因为之前许多科学和技术进步的积累，使得我们在人工智能应用方面取得了一系列突破，比如语音识别、计算机视觉和自然语言处理等等。这也使得这个领域内涌入了一大批由研究生占据主要比例的研究者，让深度学习的研究高速发展起来。我们能走到今天离不开两方面技术的进步，一是层次结构概念的创建，二是让电脑能自己提取特征。层次结构使得电脑能通过一些简单的概念学习进而理解复杂概念，这也是人类学习并培养他们对世界的理解的方式。当我们从过去所熟悉的事物中发现了新的观点，就会逐渐优化对世界理解的模型来不断适应，这些新事物又能帮助他们更好的与事实和数据对应起来。例如，深度学习系统可以通过组合一些简单的概念来表示一只猫的形象，比如通过边缘的概念依次定义角落和轮廓。但是我们不需要特意教它关于中间区域的概念，它会自己学习。在这样一个视觉识别系统中，计算机可以成功在一张暹罗猫翻跟头照片中识别出猫，而不需要我们展示给系统所有猫可能具有的颜色、外形或行为。当计算机“看见”一只猫的时候，它就会“知道”这是一只猫。我非常荣幸能和Geoffrey Hinton以及Yann LeCun一起因为在行业内的突出贡献而被大家誉为计算机科学三大巨头。我们共同撰写了一篇论文：深度学习，并发表在《自然》杂志的五月刊上，文章对人工智能领域我们所研究的方向的前景进行了阐述。但这个领域所需要做的事光靠几个“媒体明星”是远远不够的。为了达成更大的进步并实现更多的应用，人工智能领域无论在学术界还是工业界都需要成千上万个科学家和工程师们。（（注：文章中文版链接：http://wenku.baidu.com/link?url=nF8zA-hi7sA0Oh899TtU2AuCBr_Vb10gP2X3MtDDVcxTRJu-1Ghzxrr-0aWUj1HamFg9xh47gZBYnLI7Z3GPLth-FDr53aiX7My9YeO2Kxu）这就是为什么我一直致力于为我们激动人心的事业网罗更多人才的原因。我与Ian Goodfellow和Aaron Courville合写了一部叫《深度学习》的书。我们的核心受众是学习机器学习的大学生以及一些软件工程师，他们在某些可能会用到机器学习的重要产业里工作。这本书已经放到网上了，我们欢迎大家来阅读和学习，并给我们一些好的反馈建议。这令我想到了另一个我想表达的关键点:我是一个技术开放概念的拥护者。众多像开源开发者一样的技术开放运动支持者都坚信我们应该在获取新知识之后尽快分享出去，这能加快科学边界向外扩张的速度而且对大家都有利。我和很多同事们都把我们在深度学习领域的所有发明，应用在了GitHub的 Theano工具及其衍生项目上。这样，任何正在构建深度学习系统的人都可以使用这些算法和编程工具，当然我们也会敦促使用者回馈这个项目，有数百人已经这样做了。正如共享之于技术开放的意义，透明化的协作也同样重要。整个学界形成了一个巨大的头脑风暴。其中，拥有5名教授在内共60名研究院的蒙特利尔学习算法研究所(MILA)，通过和许多大学及产业里的科学家进行项目合作，大大推进了整个学界的协作程度。我们最近的研究合作伙伴是IBM。我们期待着通过与IBM研究部门以及Watson Group的科学家和工程师们的合作，能够实现深度学习在语言、演说和视觉领域应用的宏大研究计划。我们坚信只要大家齐心协力，就一定可以通过更为强大的能处理海量数据集的计算机，成规模地增加深度学习的技术和方法。它可以帮助计算机跨越更广泛的领域，从更多种类的数据来源中学的更快更多，包括那些还未被人类运用的不能被标注的海量数据。我认为深度学习的未来非常激动人心。我们取得了一系列快速的发展，虽然我们目前离破解什么能让机器真正理解这个世界这一谜题还很远，但是我对我们最终能破解这个谜题充满了信心。然后闸门就会被打开了，一旦计算机真正理解文本、语音、图像和声音，他们将成为我们不可或缺的助手。这将彻底改变我们与计算机交互的方式，帮助我们在日常生活中生活得更为方便，在工作状态中更加高效。它会让社会能够应对一些对我们来说非常重要的重大挑战，比如治疗致命疾病以及更广泛地传播知识和财富。更重要的是，它会帮我们理解我们自身，以及一直以来让我非常着迷的关于“智能是如何产生的”的命题。30多年来这一直是我的梦想，现在这个梦想正在快速的照进现实。☞  本文原作者Yoshua Bengio，格灵深瞳大美女天天翻译，如需转载，请提前与我们进行联系。让计算机看懂这个世界www.deepglint.com更多精彩内容，请关注微信公众号：格灵深瞳。"}
{"content2":"欢迎大家前往腾讯云社区，获取更多腾讯海量技术实践干货哦~作者：周景超在上一期中介绍了我们团队部分已公开的国际领先的研究成果，近期我们有些新的成果和大家进一步分享。1 人脸进展人脸是最重要的视觉信息之一。以貌识人、以貌取人是人的本性，也是最自然、最常用的身份确认和交互方式之一。互联网和移动互联网上每天上传和传播的天文数字级别的照片中有很大一部分都是和人脸相关的（比如国际知名互联网公司Facebook每天都有亿级以上的海量人脸照片和视频上传），因此人脸检测与识别技术在学术界和工业界都备受关注，是各种国际前沿视觉技术的重要检验场之一。在上一期中已介绍了我们团队在人脸检测与人脸识别上所取得的世界领先的原创性成果，迄今为止，我们团队在人脸检测的国际最权威评测平台WIDER FACE的所有三个测试子集、人脸识别的国际最权威评测平台Megaface Challenge 2（不同于Megaface Challenge 1，Megaface Challenge 2严格限定参赛者使用官方提供的固定的训练数据以公平地对比不同人脸算法的性能）的所有测试任务中都取得世界第一的性能。众所周知，原创性技术和落地应用这二者是密切相关、相辅相成的，一方面，原创性技术需要在落地应用场景中检验其有效性并帮助解决工业界的实际需求。另一方面，工业界的很多实际需求是之前长期存在但是在技术层面长期解决不了的，对这些技术难题如果没有真正创新性的技术突破也就难以真正解决。就应用进展而言，我们团队自主研发的人脸技术已经接入公司的若干重要场景下的应用业务。其中一个是TEG信安业务场景，目的是精准识别海量上传图像中的敏感人物，针对带有敏感人物的图片或视频进行拦截。该项目的难点在于待识别的敏感人物会以各种形式出现在图片当中，比如漫画和处理过的照片都是比较难的情况。我们采用自主研发的人脸识别模型对该问题进行了建模，同时利用TEG信安提供的业务数据对业务场景进行了优化。我们的人脸技术接入的另一个重要业务是MIG互联网+合作事业部政企项目组的腾讯慧眼项目。腾讯慧眼项目主要针对政务和生活场景，通过人脸验证完成自动化的身份鉴别，方便百姓远程办事，让数据多跑路，百姓少跑腿，为百姓带来更多“刷脸”办政务的创新场景。在政务场景下的人证比对中，我们的人脸技术在内测的业务数据上已达到甚至超过了知名人脸公司依图科技（与商汤科技、旷世科技齐名的人脸巨头公司）的精度。目前该项目正在进展中，并将在近期开放至腾讯慧眼项目的开放平台上。此外，我们的人脸技术还广泛应用在了内部团队其他视觉相关项目中。例如在我们的图像数据分类项目中，人脸检测技术用于辅助半自动的标注任务。在AI Lab自研的AI有嘻哈项目中，人脸技术也将被用于识别图片中的名人脸，以便进一步的提高图片配文的质量。2 OCR进展2.1 ICDAR竞赛在上一期中介绍了我们团队在OCR的ICDAR Robust Reading竞赛中所取得的佳绩。该竞赛有两个很重要的竞争很激烈的场景：互联网图片场景图像（Born-Digital Images）和对焦自然场景文本图像（Focused Scene Text Images）。我们在这两个场景的文本定位任务（Task 1: Text Localization）和单词识别任务（Task 3: Word Recognition），一共四个任务上都取得第一名的佳绩。一般对每个场景的Robust Reading竞赛，传统上分成四个任务：文本定位、文本分割（Text Segmentation）、单词识别、和端到端识别（End to End）。在传统OCR时代，识别图片中的文本必然经过检测、分割和识别三个阶段。在深度学习时代，近年来由于RNN的出现，对于检测到的单词可以直接训练网络做识别，分割这个任务已没有太大意义，成为鸡肋，也没有团队参加这项任务，ICDAR 2017年公布的新的竞赛数据集，如：COCO-Text，直接取消了这项任务。近期我们向OCR的ICDAR Robust Reading竞赛的最终目标（端到端识别，即采用端到端的方法识别图片中的文本）发起冲击，并取得突破，在上述两大重要场景的端到端识别上也都获得第一名。值得一提的是，迄今为止我们在这OCR的两大重要场景上都实现了大满贯，超过了该领域的众多强劲对手（百度、阿里、商汤、旷世科技、和各大高校），囊括了所有的6项冠军（忽略已经被淘汰的分割任务）。就技术手段而言，我们基于在该领域的深厚技术底蕴（人脸与OCR的很多底层技术是相通的）以及在参加文本定位和单词识别任务中所积累的国际领先技术，进一步采用了级联训练（Cascade Training）的方法，把文本定位网络和单词识别网络集成起来，使得结果可以正向流动、反馈可以逆向传播。凭借着这种方法，我们在互联网图片和对焦自然场景文本图片这两个重要场景的端到端任务上都获得第一名的佳绩，截图如下所示。相关链接：http://rrc.cvc.uab.es/?ch=1&com=evaluation&task=4互联网图片端到端任务部分结果如下图所示，详细结果可在网站上查询：http://rrc.cvc.uab.es/?ch=1&com=evaluation&view=method_samples&task=4&m=31774>v=1相关链接：http://rrc.cvc.uab.es/?ch=2&com=evaluation&task=4对焦自然场景文本图像端到端任务部分结果如下图所示，详细结果可在网站上查询：http://rrc.cvc.uab.es/?ch=2&com=evaluation&view=method_samples&task=4&m=31791>v=13 小结人脸&OCR团队一直以来按照“夯实基础，做既有创新性又能落地应用的国际前沿工作”这个研究思路开展和推进工作，迄今为止我们不仅在人脸与OCR的多项国际权威榜单名列榜首，而且我们的技术在公司的多个重要的场景中得到了很好的应用。近期，我们团队参与的“AI在腾讯信息安全中的应用”项目获得了2017年下半年技术突破奖银奖，截图如下。在2017腾讯全球合作伙伴大会上，AI Lab计算机视觉中心负责人刘威博士也向公司的合作伙伴和行业精英介绍了我们团队在人脸与OCR上的若干研究成果，如下图所示：人脸&OCR团队将继续以踏实、进取的态度做好研究工作和项目落地，不忘初心，继续为人脸与OCR的技术发展贡献自己的力量。相关阅读腾讯 AI Lab 计算机视觉中心人脸 & OCR 团队近期成果介绍（1）腾讯 AI Lab 计算机视觉中心人脸 & OCR 团队近期成果介绍 ( 2 )云服务器20元/月起，更享千元续费大礼包此文已由作者授权腾讯云技术社区发布，转载请注明原文出处"}
{"content2":"【计算机视觉】特征脸EigenFace与PCA标签（空格分隔）： 【图像处理】版权声明：本文为博主原创文章，转载请注明出处http://blog.csdn.net/lg1259156776/。说明：本文主要想弄清楚将人脸识别推向真正可用的第一种方法：特征脸方法。【这里采用的是1维的PCA方法，将图像转变为行向量或者列向量，虽然破坏了几何结构，但是处理比较直观方便】第一步是构建样本集合获取包含有M张人脸图像的集合S，每张人脸图片的大小scaling到统一的尺寸，如下面图片集合：每个图片都转变为N维的向量，然后一行一行的吧放起来，从而构成了样本矩阵X，这一点就与我前段所写的协方差矩阵以及matlab PCA函数princomp函数的输入的样本矩阵格式一样了。第二步进行均值和协方差矩阵将图像矩阵每一列都加起来取平均，得到一个平均图像，公式和结果如下图所示：第三步计算差值矩阵或者样本矩阵零均值化每行也就是每张图象都减去样本均值。第四步计算协方差矩阵及其特征值、特征向量实际上这些步骤都是PCA最传统的步骤，一步一步来的。但是对于图像数据来说，如果每个pixel都当作是一个维度特征的话，那么这个协方差矩阵实在太大了那么如果当我们的训练样本数量小于图像维数的时候，起作用的特征向量只有M个，而不是对应的图像维数个，其余都是零，所以求解特征向量的时候我们只需要求解一个。这些特征向量还原成像素排列，得到如下所示的特征脸：图中有二十五个特征脸，数量和训练样本一样只是巧合，一般只只需要7个特征脸就可以。人脸识别考虑一张新的人脸，scaling到相同的尺寸，然后进行特征转换，对应的公式为：其中k=1,2,…,M表示对应的特征脸uk，也就是PCA中的第k个特征映射矢量，通过这M个特征脸，可以将新的人脸转变为在特征脸的坐标系的坐标表示：实际上所有的脸，不管是新来的，还是样本集中的，都会被映射到该特征脸构成的矢量空间中的坐标表示，也可以说是在每个特征脸的权重，这个权重是提炼出来的某个人脸的在特征脸坐标系下的表示，如果要进行人脸识别的话：其中Ω表示要判别的输入的人脸，而Ωk表示训练集中某个人脸，求取距离，一个一个判断，如果距离小于某个阈值，则认为成功匹配。否则认为没有找到匹配的对象，认为是新的人脸或者不是人脸两种情况。根据训练集的不同，阈值设定并不不是固定的。2015-11-27 学习笔记 张朋艺"}
{"content2":"这是《人工智能系列笔记》的第二篇，我利用周六下午完成课程学习。这一方面是因为内容属于入门级，并且之前我已经对认知服务和机器人框架比较熟悉。如有兴趣，请关注该系列 https://aka.ms/learningAI但是学习这门课程还是很有收获，这篇笔记时特别加了\"探秘\"两个字，这是因为他不仅仅是介绍了微软的认知服务和机器人框架及其如何快速开始工作，更重要的是也做了很多铺垫，例如在讲文本分析服务（Text Analytics）之前，课程用了相当长的篇幅介绍了文本处理的一些技术原理，毕竟无论是微软的认知服务，还是其他厂商的服务，或者你自己尝试去实现，其内部的原理都是类似的。我将给大家分享三个部分的内容文本理解和沟通计算机视觉对话机器人第一部分：文本理解和沟通现在人工智能很火，花样也很多，可能大家不会想到，很早之前人类对于机器智能的研究，最主要就是在文本理解和处理这个部分，科学家们想要实现的场景主要如下这跟人类本身的学习及成长是类似的，一旦机器掌握这些能力，其实就相当于具备了\"听说读写\"的能力。我据说微软二十年前创立研究院之处，主要的研究范围也是在这个领域，二十年过去了还在继续投资，不断优化这方面的能力，可见其作为人工智能的重要性。其实这里提到的大部分过程，可以理解为通常意义上的自然语言处理（Natual Language Processing——NLP）的研究范畴。本次课程中使用python进行讲解，提到了一个关键的package：NLTK（Natual Language Toolkit），以及它的几个更加具体的库：freqdist 用来做字（词）频分析，stem用来做词干提取等等。下面是一些基本的用法也就是说，其实你用NLTK能做出绝大部分文本理解和处理的场景，当然如果你用微软的认知服务（Cognitive Service），则可以省去很多基础性的工作，而是直接专注在业务问题上。前面三种服务都相对简单，通常你只需要开通，并且调用相关的API 即可，例如 Text Analytics 可用来检测文本语言，识别其中的实体，关键信息，以及情感分析。而Language understanding 则相对更加复杂一点，它的全称是Language understanding intelligence service （Luis），是有一套完整的定义、训练、发布的流程。换言之，Luis允许你自定义模型，而前面三者则是利用微软已经训练好的模型立即开始工作。申请Luis服务是在Azure的门户中完成的，而要进行模型定义和训练，则需要通过 https://luis.ai 这个网站来完成。下面是我用来测试的一个模型的其中一个Intent （Luis能同时支持多种语言，甚至也能做到中英文混合文本的理解）Luis最大的一个使用场合可能是结合本文最后面提到的对话机器人来实现智能问答。第二部分：计算机视觉如果说文本智能是尝试学习人类的\"听说读写\"的能力，那么计算机视觉则是尝试模拟人类的眼睛，来实现\"看\"的能力。图像分析其实就是好比人类看到一个物体（或者其影像），脑电波反射过来信号，使得你意识到你看到的是什么。这个能力用到了预先训练好的模型。这个可以通过认知服务中的Computer Vision这个组件实现。但是，即便是上面的模型已经包含了数以百万计的照片，但相对而言还是很小的一个集合。所以，如果你想实现自己的图像识别，可以使用认知服务中提供的Custom vision这个能力来实现。Custom vision拥有一个同样很酷的主页：https://customvision.ai/ ，通过这个网站，你可以上传你预先收集好的照片，并且为其进行标记，通常情况下，每个标记至少需要5张照片，然后通过训练即可发布你的服务，并且用于后续的图像识别检测（例如某个图像是不是汽车，或者香蕉之类的）。人脸识别，则是特定领域的图像识别，这个应用也是目前在人工智能领域最火的一个，而也因为脸是如此重要，所以在认知服务中，有一个专门的API，叫Face API。使用这套API，可以做出来很有意思的应用，例如从技术上说，图像（Image）是由一个一个有颜色的数据点构成的，这些数据点通常用RGB值表示。而视频（Video）则是由一幅一幅的图像（Image，此时称为帧）构成的。所以，计算机视觉既然能做到图像的识别和理解（虽然可能会有偏差），那么从技术上说，它也就具备了对视频进行识别和理解的能力，如果再加上之前提到的文本智能，它就能至少实现如下的场景：识别视频中出现的人脸，以及他们出现的时间轴。如果是名人，也会自动识别出来，如果不是，支持标记，下次也能识别出来。识别视频中的情感，例如从人脸看出来的高兴还是悲伤，以及欢呼声等环境音。文本识别（OCR）——根据图像生成文字。自动生成字幕，并支持翻译成其他语言。第三部分：对话机器人我记得是在2016年的Build大会上，微软CEO Sayta 提出了一个新的概念：Conversation as a Platform, 简称CaaP，其具体的表现形式就是聊天机器人（chatbot）。当时的报道，请参考 https://www.businessinsider.sg/microsoft-ceo-satya-nadella-on-conversations-as-a-platform-and-chatbots-2016-3/?r=US&IR=T对话机器人这个单元，讲的就是这块内容。与人脸识别技术类似，机器人这个技术在这几年得到了长足的发展和广泛的应用，甚至到了妇孺皆知的地步。这里谈到的机器人，特指通过对话形式与用户进行交互，并且提供服务的一类机器人，广泛地应用于智能客服、聊天与陪伴、常见问题解答等场合。创建一个对话机器人真的很简单，如果你有一个Azure订阅的话。微软在早些时候已经将机器人框架（Bot Framework）完全地整合到了Azure平台。做一个机器人（Bot）其实真的不难，但要真的实现比较智能的体验，还真的要下一番功夫。目前比较常见的做法是，前端用Bot Framework定义和开发Bot（用来与用户交互），后台会连接Luis服务或QnA maker服务来实现智能体验，如下图所示。我在11月份的Microsoft 365 DevDays（开发者大会）上面专门讲解了机器人开发，有兴趣可以参考 https://github.com/chenxizhang/devdays2018-beijing 的资料。机器人框架 （Bot Framework）的一个强大之处在于，你可以实现编写一次，处处运行，它通过频道（Channel）来分发服务。目前支持的频道至少有16种。我自己之前用过Web Chat，Microsoft Teams，以及Direct Line和Skype for Business等四种。一直对Cortana这个场景比较感兴趣，这次通过学习，终于把这个做成功了，还是挺有意思的。这项功能，还有一个名称：Cortana Skills，目前需要用Microsoft Account注册这个Bot）。请通过 https://aka.ms/learningAI 或者扫描下面的二维码关注本系列文章《人工智能学习笔记》"}
{"content2":"不久前，高盛发布的名为《中国在人工智能领域崛起》的研究报告，报告中，高盛认为中国已经成为AI领域的主要竞争者，中国政府建设“智慧型经济”和“智慧社会”的目标将有可能推动中国未来GDP的增长。在这份报告中显示，中国的AI发展在人才、数据、基础设施和计算能力4个方面具有优势，在人才、数据和基础设施等方面已经具备了全力发展AI的实力。而今年3月，AI出现在政府工作报告中，7月中国公布首个国家战略层面的AI发展计划，都让全世界对中国的AI发展充满信心。在人工智能领域，美国比中国早发展5年，美国从1991年开始萌芽，而中国在1996年才诞生人工智能企业。然而因为有BAT这样众多互联网公司，中国在AI领域高速发展。一、中国AI领域风险投资奋起直追根据腾讯发布的《2017中美人工智能创投现状与趋势研究报告》中的数据，美国AI领域累计风险投资978亿元，中国累计风险投资635亿元，虽然在总金额上有差距，但是中国超过1亿美元的大型投资有22笔，累计353.5亿元，美国超过1亿美元的大型投资有11比，总计417.3亿元。在2017年前3个月，国内AI领域获得投资的企业有36个，半数以上融资金额超过千万，深醒科技、纵目科技、中译语通等数家企业获得超过1亿元人民币以上的融资，中国在人工智能领域的投资不断增长。二、中国AI领域企业数量差距缩短截止到2017年6月，全球人工智能企业总数达到2542家，美国1078家，中国592家。但是，美国的人工智能企业从1991年开始创建，到2013年达到峰值，而中国人工智能企业从1996年开始创建，到2015年达到峰值。起步期早于中国5年，发展期早于中国6年，爆发期和平缓期都只早于中国2年，从企业数量来看，中国已追平美国3-4年的时间差距，而中国的发展势头仍高于美国。三、中国AI企业更关注应用层美国AI创业公司中，自然语言处理、机器学习应用及计算机视觉与图像等3个领域的企业数量最多，而中国在计算机视觉与图像、智能机器人及自然语言处理等3个领域的企业数量最多。两者差别主要在机器学习和智能机器人2个方面，机器学习属于企业或个人辅助工具，各个行业均有涉及，而智能机器人更多专注于医疗、家居等专业领域的机器人，与实际应用关联度更高。中国的人工智能企业更关注应用层，而美国更看好基础层。在处理器、芯片等领域的创业企业中，美国有33家，中国仅有14家，这也是高盛发布的报告中指出的，中国AI领域唯一短板在于芯片领域。四、中国AI行业有数据方面的天然优势数据是中国AI产业发展的最大优势，中国拥有14亿人口，每年产生的数字信息约占全球的13%，各大AI创业企业依靠海量数据来进行更精准的分析。比如滴滴出行平均每天处理超过4500T数据，收到超过200亿次路线请求并处理超过2000万个订单。对滴滴在深度学习、人机交互、机器视觉及智能驾驶技术方面开展的研发活动带来帮助。五、中国技术型企业推送AI行业发展根据统计，2015年全球发表的顶级AI论文中，有43%的论文作者中有一名及一名以上的中国研究人员，到2016年10月份，中国在AI领域有超过1.6万个专利。而在AI研发投入上，百度的研发投入占营收14.4%，接近谷歌的15.5%和微软的14.5%。而在研发人员占比方面，腾讯研发人员占比51%，阿里巴巴占比45%，百度占比43%，都超过了谷歌（38%）和微软（32%）的研发人员占比。在招揽高级人才方面，百度为机器学习科学家提供的基本年薪为12.6万美元，加上每年的分红和权益，达到22万美元/年，仅次于Facebook的27.3万美元和微软的24.4万美元。六、中国技术企业的重心向AI转移百度提出“一切以AI为先”的思维，在全球建立三个实验室，有超过2000名AI研发工程师，开放了人工智能对话平台DuerOS和自动驾驶项目Apollo。在中国市场中，阿里云一枝独秀，市场份额达到40.7%，到6月份拥有100万个付费用户，覆盖媒体、互联网、金融、政府等领域，三分之一的500强企业和三分之二的独角兽企业都在使用阿里云。而阿里云的飞天Apsara处理系统每秒可处理数十万的并行事务，2016年双十一每秒处理17.5万次交易和12万次支付。并且，高盛十分看好阿里旗下的蚂蚁金服，蚂蚁金服覆盖全球32亿人口，付款数据覆盖7.17亿消费者，高盛提出如何阿里继续提高算法，那么阿里中国零售收入除以中国零售GMV的比例可达到5%。而腾讯后来居上，2016年成立人工智能实验室，2017年成立西雅图AI实验室，对计算机视觉、语音识别、自然语言处理和机器学习四个方面进行研究。2017年8月，腾讯联合数个领域的专家结成联盟，支持自动驾驶的研究和未来相关产品的制造。七、中国的人工智能行业是大势所趋中国人工智能行业已成为未来最大的风口，注重于应用层的AI企业将通过良性循环改变人们的生活生产方式，带来生活便利性。在2017年TOP100全球软件案例研究峰会上，专注于“城市计算”领域的微软亚洲研究院首席研究院郑宇担任联席主席，给予中国人工智能企业更多建议。第六届TOP100全球软件案例研究峰会将于11月9-12日在北京国家会议中心举办，甄选100个本年度最具行业代表性的软件研发案例，现场解读其解决方案和背后的技术逻辑，帮助研发团队快速提高效能。更多TOP100案例信息及日程请前往[官网]查阅。4天时间集中分享2017年最值得学习的100个研发案例实践。本平台共送出10张开幕式单天免费体验票，登录TOP100summit官网即可申请，数量有限，先到先得。"}
{"content2":"2017年度最具商业价值人工智能公司TOP50 榜单发布未来最有赚钱潜力的50个人工智能项目都在这里了。经过了60年的发展，人工智能在2017年，正式走向应用的元年。从今年起，人工智能首次被写入政府工作报告。AI相关内容，以前所未有的力度覆盖了各大媒体的头条，成为人们已不陌生的话题。但和之前不同的是，一场关于人工智能商业化应用的革命，已然显露雏形。去年AlphaGo战胜李世乭，将业界对人工智能领域的关注引向了高潮。在今年年初，AlphaGo Master以60连胜中日韩棋坛的顶尖高手后，5月份在乌镇，AlphaGo对战柯洁又取得了三战全胜的成绩。随即，DeepMind宣布AlphaGo引退，不再下棋，而是将精力投入研发高级通用算法，为解决复杂的问题提供帮助，包括找到新的疾病治疗方法、降低能耗和发明革命性的新材料等。对无人驾驶的布局已然全面展开。无论是传统车商，还是互联网巨头，以及创新公司们，都纷纷在整车解决方案、传感设备、地图等方面试水。智能家居的语音交互终端，似乎迎来了春天。苹果发布了最新的Siri音箱HomePod，而在此之前，亚马逊Echo已经占领了先机，Google和微软也先后推出了Google Home和Cortana语音助理驱动的智能音箱。帮助人类进步，对于人工智能而言，是最大的意义。人工智能在2017，创业的难度已经大大降低，创新的大门正在向各种规模、各种行业参与者敞开。算法不再是护城河，智能革命走向现实应用。在人工智能相关技术驱动下，推动全新的应用场景落地，助力产业升级，成为这个时代的焦点话题。因此，创业黑马、黑智，与铂诺联合发布了这份《2017年度最具商业价值人工智能创新公司TOP50》榜单。在初期，黑智广泛采访和拜访了业内知名公司和投资机构，并联合各投资机构和项目方，通过在线报名和机构推荐的方式，收集了百余家企业资料。通过资料透明度、融资情况和对技术、团队、业务模式等的调研初筛，我们筛选出其中的76个项目，还邀请了云启资本、清流资本、蓝湖资本、联想之星、峰瑞资本、蓝驰创投、九合创投、英诺天使基金、源码资本、高榕资本、丰厚资本等11家投资机构的代表，对其从创新性（30%）、成长性（20%）、可投资性（20%）、产业发展潜力（30%）等不同维度进行评选，最终评出50家上榜公司。在本次上榜的公司中，我们要求，必须满足以下条件：1、主营业务是基于人工智能技术提供产品/服务，或者AI的支持服务提供商；2、公司主要在国内运营，已有具备创新性的产品/技术研发成果推出；3、公司未上市，融资轮次在C轮以下，并至少获得天使轮以上融资；4、公司具有较强的商业前景和发展潜力，项目创新性强。人工智能领域也已经获得了创投界的巨大关注。根据黑智不完全统计，在2016年前三季度，国内发生了48起人工智能领域投资事件，而在2017年，仅仅在Q1，国内人工智能获投项目已经达到了36家。而在近两年的获投人工智能项目中，又以B轮以前的融资轮次居多，国内的人工智能投资仍然处于发展的早期阶段。尽管近两年，人工智能领域的专家“下海”投入创业项目已经成为主流。而当时间推进到了2017年，AI产业仍然面临着人才和资金投入的巨大缺口和需求，在人工智能领域的商业化应用方面仍然还处于试验阶段。因此，在这次评选中，我们也更多地将注意力放在了处于初创和成长期的企业，期待能够更多地挖掘出，人工智能在未来和不同的行业结合，进行商业化转化的可能性；以及发现人工智能技术更多的应用场景。而在投资机构代表们看好并入选的项目中，我们也可以发现，深度学习，从的技术成熟度而言，正被赋予越来越高的投资价值。越来越多的互联网公司、传统企业，正基于深度学习，提供智能化产品和服务。计算机视觉、语音/自然语言等为代表的应用，正在多个领域，实现了落地应用，并酝酿引发更大的行业变革。2B市场，正在成为人工智能的重要应用领域，包括医疗、教育、金融、企业级服务、商业服务机器人、自动驾驶等。在2C领域，智能机器人等语音交互设备终端，正在家庭、教育、陪护等领域实现应用。我们评选出的50个项目只是对当前人工智能技术应用现状的一个极小的缩影。由于处于技术研发期以及其他原因，还有部分项目不便对外曝光。而由于时间和获取渠道等各种原因的限制，也有众多的项目并未参与本次评选。而这些入选者，也是在人工智能领域中起步不久的新秀。在未来，我们还将见证它们的成长，以及看到更多的革命性的技术和研究涌现，更多的行业应用爆发和改变。但是，技术的变革和进步，带来的消费体验的升级和效率的革命，却是不可逆转的过程。智能技术革命，正在我们的生活和商业中萌芽。未来已来，一起面对。["}
{"content2":"人工智能之父麦卡锡给出的定义构建智能机器，特别是智能计算机程序的科学和工程。人工智能是一种让计算机程序能够\"智能地\"思考的方式思考的模式类似于人类。什么是智能？智能的英语是 Intelligence推理，知识，规划，学习，交流，感知，移动和操作物体。智能 不等于 智力 (IQ:智商 比较类似计算机的计算能力)如何算有智能？可以根据环境变化而做出相应变化的能力。具有\"存活\" 这最基本的动因自主意识，自我意识等等。抢小孩子西瓜吃，小孩子护住西瓜就是自主意识。图灵测试(Turing Test)图灵于1950年提出的一个关于判断机器是否足够智能的著名试验。评委，评判目标是机器和人。评委与被评判目标以墙隔开。评委向人和机器人来提出问题。评委事先不知道对面谁是机器人，谁是人。评委提的问题机器人和人分别做出回答。当评委不能分辨是人还是机器人后，说明机器拥有了与人类似的思维。暂时还没有机器通过图灵测试。智能分类: 自然智能 & 人工智能人造出来的智能Artificial Intelligence 人造智能人工智能的前景人工智能的需求:提高品质增加效率解决难题人工智能的前景好在哪里？支持: 企业支持 科技支持(大数据，硬件设备) 国家支持2017年7月20日中国国务院发布了《新一代人工智能发展规划》2020年中国与世界平齐Excel 等将用Python 替代 VBAPython 被加入高考CCTV 的 机智过人 节目人工智能产品 和 人类高手比拼 ，中央电视台和中国科学院共同举办。嘉宾 柯洁 撒贝宁 林书豪 江一燕 知名人士 智能人士微软小冰可以作曲写词，画画。人工智能需要的基本数学知识数学；论文 & 自己的实践研究实战性课程 基本的了解就行人工智能的历史人工神经网络被提出(AI缘起)Artificial Neural Network(简称 Neural Network)沃伦.麦卡洛克和沃尔特.皮茨在1943创造了神经网络的计算模型。为以后的深度学习打下了重要的基础达特茅斯会议(定义AI)达特茅斯学院(Dartmouth College) 是美国一所私立大学由约翰.麦卡锡等人于1956年8月31日发起。标志着AI(人工智能)的正式定义(诞生)感知器(Perceptron)一种最简单的人工神经网络，是生物神经网络机制的简单抽象、一种最简单的人工神经网络， 是生物神经网络机制的简单抽象由罗森布拉特于1957年发明将人工智能的研究推向第一个高峰。人工智能的第一个寒冬1970年开始的十几年里传统的感知器耗费的计算量和神经元数目的平方成正比当时的计算机也没有能力完成神经网络模型所需要的超大计算量。霍普菲尔德神经网络一种递归神经网络( Recurrent Neural Network)由约翰.霍普菲尔德在1982年发明具有反馈(Feed back)机制反向传播(Back Propagation) 算法。1974年哈佛大学的保罗沃伯斯发明，当时没有受到重视。1986年大卫.鲁姆哈特等学者出版的书中完整的提出了BP算法使大规模神经网络训练成为可能，将人工智能推向第二个高峰。人工智能第二个寒冬1990年开始人工智能计算机 Darpa没能实现(美国政府花了巨资的)政府投入缩减深度学习(deep learning)基于深度(指\"多层\") 神经网络2006年由杰弗里.辛顿(Geoffrey Hinton)提出人工智能性能获得突破性进展进入感知智能时代深度学习在语音和视觉识别上分别达到99% 和 95%的识别率2013年开始人工智能三个时代:运算智能(深蓝打败俄罗斯象棋选手，通过暴力运算，算出所有可能的下棋步骤)感知智能，语音图像，类似触觉的时代认知智能: 人类特有的能力，一个非常高等的能力。AlphaGo击败众多人类选手Google 买下的Deepmind公司的AlphaGo (基于TensorFlow)2016年接连击败围棋界顶尖棋手。深度学习被广泛关注，掀起了学习人工智能热潮未来由我们创造你应该感到自豪，因为你学习了人工智能虽然我们不能过分乐观，未来也许还会有低潮但人工智能是大势所趋，学了绝对不会吃亏。Ai和机器学习、深度学习的关联人工智能的知识图谱人工智能不仅仅是一个独立的学科，它与很多其他的学科都有交集。机器学习和深度学习都与其他学科有交集。但是机器学习总的是属于人工智能，而深度学习属于机器学习的一个子领域。横穿而过的是神经网络。深度学习是基于神经网络的。AI ML 和 DL 的关系机器学习是实现人工智能的一种方法，深度学习是机器学习的一个分支人工智能能够王者归来，深度学习功不可没深度学习是引领人工智能热潮的火箭深度学习作为后代，却给爷爷和爸爸争光了。人工智能搭上了深度学习的火箭。什么是机器学习?机器学习是实现人工智能的一种方法，深度学习是机器学习的一个分支。什么是学习？过程: 一个系统，能够通过执行某个过程，改善了性能。说的更深入一些，学习的目的是\"减熵\"热力学第二定律: 一个孤立系统倾向于增加熵(混乱程度)生命活着就是在减熵适应环境机器学习的必要性很多软件无法靠人工编程: 自动驾驶，计算机视觉，自然语言处理。识别鸢尾花难以用人工编程花瓣数，花颜色，花纹形状，等等。人工编程难以定性。人类常会犯错，(比如紧张，累了，困了)，机器不容易犯错机器的计算能力越来越强 提高我们生活质量加快科技发展\"晦涩\"的机器学习定义对于某类任务T (Task) 和性能度量 P(Performance)通过经验E(Experience) 改进后在任务T上由性能度量P 衡量的性能有所提升。简单的机器学习的定义机器学习: 让机器学习到东西。mark人类思考 VS 机器学习机器学习: 用数据来解答问题数据对应 训练过程解答问题 对应着推测的过程。练习 & 考试学生学习: 用做练习题来提高考试的成绩做练习题对应训练考试 对应你对新情况的推测AlphaGo 学下围棋围棋博弈: 用和自己下棋来提高下棋胜率和自己下棋对应训练与人类下棋对应推测传统编程 VS 机器学习mark机器学习大致等同于找一个好的函数(Function)/模型机器学习的分类监督学习非监督学习半监督学习强化学习什么是监督学习？Supervised Learning: 有标签。近义词: 分类(Classification)数据有给定的正确标签。什么是非监督学习？Unsupervised Learning: 没有标签 近义词: 聚类(Cluster)把类似的数据归为一堆。预测到规定好的堆中。什么是半监督学习？Semi-Supervised Learning: 有少部分标签 最类似人的生活。父母教给我们怎么做？让座+好孩子。 独立生活+自己判断想要判断c是不是精英。物以类聚。半监督也是基于聚类的cluster实现。什么是强化学习？前面都是基于有没有标签，或者是有标签所占的比例。Reinforcement Learning: 基于环境而行动，以取得最大化预期利益。玩游戏，如果挂掉分数-1，如果赢了分数+1.总得分:通过分数的奖励去刺激它进行进一步的强化。机器学习的算法多种多样。如何去选择一个适合我们的机器学习方法。我们可以依照skit-learn给出的图。从右上角的start开始:你的样本数是否大于50，如果不是那么你需要有更多的样本。预测类别，如果是要预测类别你有没有加标签的数据。分类 & 回归/预测 & 聚类 & 维度下降为什么回归叫regression(回归)回归用于预测(比如股票)，它的输出是连续的，与离散的分类不同。回归之所以叫回归是英国生物学家兼统计学家高尔顿在研究人类遗传问题时提出的。人类身高不会无限的增高(两种身高 父亲的儿子的身高) 有向他们父辈的平均身高回归的趋势。机器学习的六步走收集数据 -> 准备数据(抽取特征) -> 选择/建立模型 -> 训练模型测试模型 -> 调节参数mark机器学习的\"关键三步\"找一系列函数来实现预期的功能: 建模问题找一组合理的评价标准，来评估函数的好坏: 评价问题快速找到性能最佳的函数: 优化问题(比如梯度下降就是这个目的)面对ai我们应有的态度人工智能大热火到连Android都被比了下去，连Kotlin和Go都有点黯然失色1950年就被提出。审时度势千万不要跟风，不要头脑发热。AR VR寒冬人工智障目前的人工智能，其实还停留在比较初级的阶段马云说人工智能应该做那些计算机擅长而人类不擅长的事。现在很多的人工智能还只是模仿人类做的事，还远远没有达到机器智能的程度。离真正的机器智能还比较遥远，毕竟人脑太强大，很难被模仿。人类从未创造生命人类到目前为止只不过能复制生命，从没有从无到有来创造多利羊只是复制。克隆。对生命对自然有一颗敬畏之心。目前两个派别马斯克: 特斯拉的ceoFacebook ceo 和 Google ceo反省自己比担心AI更重要。人心比万物都诡诈，与人心相比，AI真的太简单了。全知并非全能即使这类人工智能存在，它得和人类的经济和资源竞争。需要适当的防备AI可能AI 会在不断学习的过程中习得一些不可控的思维。借人工智能来认识自己人类的大脑是怎么运作的，我们还知之甚少，更不用说模仿或者改造人机合作AI 有 机智过人 和 技不如人 人机合作 惊为天人什么是过拟合?过分拟合。: OverFittingfitting是拟合，曲线能不能很好的表现样本，并且拥有很好的泛化能力。拟合的结果有三种:UnderFitting: 欠拟合。样本不够或算法不精，测试样本特征没学到。Fitting right: 拟合完美，恰当地拟合测试数据，泛化能力强Overfitting: 过拟合 \"一丝不苟\"拟合测试数据，泛化能力弱。回归(regression) 问题中三种拟合状态分类(Classification) 问题中三种拟合状态打个比方；谈恋爱你为了迎合女朋友的习惯总结了一套恋爱的圣经，但是你所总结的恋爱圣经只是针对于这个女孩的性格。太过于拟合这个女孩了。谈其他女朋友时，想如法炮制就行不通了。打个比方: 做菜开始训练出来的模型只会做一道菜，太贴合这个模型。让它做其他的菜，各种各样菜不能泛化欠拟合好解决: 增加训练量训练数据。把算法弄的精确一点。解决过拟合的一些方法方法:- 降低数据量- 正则化- DropoutDropout: 丢弃、退出 退学者全连接的神经网络，将其中一些连接取消掉只用部分的连接来构建神经网络。不会过分的贴合样本，起到一个好的泛化的作用。学校里学到了知识，我没有死记硬背。能够很好适应社会。什么是深度学习?机器学习是实现人工智能的一种方法，深度学习是机器学习的一个分支。基于深度神经网络的学习研究称之为深度学习只有一个两个隐藏层的简单神经网络，不把它成为深度神经网络，大于两个隐藏层的神经网络我们称之为深度神经网络。输入层和输出层都只会有一个，深指隐藏层层数很多。深度学习为什么兴起？传统的机器学习存在瓶颈。数据量比较小的时候，其实表现类似。深度学习要想表现好，数据量是关键。深度学习能有高回报的必要条件:大数据: 全球每天都有海量数据产生，大公司更是大权在握。强计算力: 云计算，GPU ，越来越快的CPU复杂模型: 一般来说隐藏层越多，效果越好。现在这些条件都已满足，请开始你的表演。深度学习的形象比喻: 恋爱初恋期：输入参数隐藏层： 跳转权重，激励函数参数。输出层: 与预期去对比第一阶段初恋期:相当于神经网络的输入层，不同的参数设置第二阶段磨合期:相当于神经网络的隐藏层，调整参数权重第三阶段稳定期:相当于神经网络的输出层，输出结果和预期比较。错误(Error)了: 与期望的误差(Loss/Cost)损失函数和成本(代价)函数BP算法: 误差反向传递(Back Propagation)改: 调整(Tuning)参数的权重(Weight)我错了我要改。调整对应参数的权重调整\"逛街\"的权重(重要性)调高榴莲味蛋糕权重，调低巧克力味蛋糕的权重。调高聊天权重女友说: 你变好了不少啊，开心！磨合过程: 不断的调整各个参数在神经网络中正向传播参数信号，经过隐藏层处理，输出结果。计算和预期的差距(误差)，反向传播误差，调整网络参数权重不断地进行: 反向传播->计算误差->反向传播->调整权重最终结果:其实不仅调参，还涉及到模型的调整，如增加神经元，灭活神经元等。"}
{"content2":"Deep Learning（深度学习）学习笔记整理系列zouxy09@qq.comhttp://blog.csdn.net/zouxy09作者：Zouxyversion 1.0  2013-04-08声明：1）该Deep Learning的学习系列是整理自网上很大牛和机器学习专家所无私奉献的资料的。具体引用的资料请看参考文献。具体的版本声明也参考原文献。2）本文仅供学术交流，非商用。所以每一部分具体的参考资料并没有详细对应。如果某部分不小心侵犯了大家的利益，还望海涵，并联系博主删除。3）本人才疏学浅，整理总结的时候难免出错，还望各位前辈不吝指正，谢谢。4）阅读本文需要机器学习、计算机视觉、神经网络等等基础（如果没有也没关系了，没有就看看，能不能看懂，呵呵）。5）此属于第一版本，若有错误，还需继续修正与增删。还望大家多多指点。大家都共享一点点，一起为祖国科研的推进添砖加瓦（呵呵，好高尚的目标啊）。请联系：zouxy09@qq.com目录：一、概述二、背景三、人脑视觉机理四、关于特征4.1、特征表示的粒度4.2、初级（浅层）特征表示4.3、结构性特征表示4.4、需要有多少个特征？五、Deep Learning的基本思想六、浅层学习（Shallow Learning）和深度学习（Deep Learning）七、Deep learning与Neural Network八、Deep learning训练过程8.1、传统神经网络的训练方法8.2、deep learning训练过程九、Deep Learning的常用模型或者方法9.1、AutoEncoder自动编码器9.2、Sparse Coding稀疏编码9.3、Restricted Boltzmann Machine(RBM)限制波尔兹曼机9.4、Deep BeliefNetworks深信度网络9.5、Convolutional Neural Networks卷积神经网络十、总结与展望十一、参考文献和Deep Learning学习资源一、概述Artificial Intelligence，也就是人工智能，就像长生不老和星际漫游一样，是人类最美好的梦想之一。虽然计算机技术已经取得了长足的进步，但是到目前为止，还没有一台电脑能产生“自我”的意识。是的，在人类和大量现成数据的帮助下，电脑可以表现的十分强大，但是离开了这两者，它甚至都不能分辨一个喵星人和一个汪星人。图灵（图灵，大家都知道吧。计算机和人工智能的鼻祖，分别对应于其著名的“图灵机”和“图灵测试”）在 1950 年的论文里，提出图灵试验的设想，即，隔墙对话，你将不知道与你谈话的，是人还是电脑。这无疑给计算机，尤其是人工智能，预设了一个很高的期望值。但是半个世纪过去了，人工智能的进展，远远没有达到图灵试验的标准。这不仅让多年翘首以待的人们，心灰意冷，认为人工智能是忽悠，相关领域是“伪科学”。但是自 2006 年以来，机器学习领域，取得了突破性的进展。图灵试验，至少不是那么可望而不可及了。至于技术手段，不仅仅依赖于云计算对大数据的并行处理能力，而且依赖于算法。这个算法就是，Deep Learning。借助于 Deep Learning 算法，人类终于找到了如何处理“抽象概念”这个亘古难题的方法。2012年6月，《纽约时报》披露了Google Brain项目，吸引了公众的广泛关注。这个项目是由著名的斯坦福大学的机器学习教授Andrew Ng和在大规模计算机系统方面的世界顶尖专家JeffDean共同主导，用16000个CPU Core的并行计算平台训练一种称为“深度神经网络”（DNN，Deep Neural Networks）的机器学习模型（内部共有10亿个节点。这一网络自然是不能跟人类的神经网络相提并论的。要知道，人脑中可是有150多亿个神经元，互相连接的节点也就是突触数更是如银河沙数。曾经有人估算过，如果将一个人的大脑中所有神经细胞的轴突和树突依次连接起来，并拉成一根直线，可从地球连到月亮，再从月亮返回地球），在语音识别和图像识别等领域获得了巨大的成功。项目负责人之一Andrew称：“我们没有像通常做的那样自己框定边界，而是直接把海量数据投放到算法中，让数据自己说话，系统会自动从数据中学习。”另外一名负责人Jeff则说：“我们在训练的时候从来不会告诉机器说：‘这是一只猫。’系统其实是自己发明或者领悟了“猫”的概念。”2012年11月，微软在中国天津的一次活动上公开演示了一个全自动的同声传译系统，讲演者用英文演讲，后台的计算机一气呵成自动完成语音识别、英中机器翻译和中文语音合成，效果非常流畅。据报道，后面支撑的关键技术也是DNN，或者深度学习（DL，DeepLearning）。2013年1月，在百度年会上，创始人兼CEO李彦宏高调宣布要成立百度研究院，其中第一个成立的就是“深度学习研究所”（IDL，Institue of Deep Learning）。为什么拥有大数据的互联网公司争相投入大量资源研发深度学习技术。听起来感觉deeplearning很牛那样。那什么是deep learning？为什么有deep learning？它是怎么来的？又能干什么呢？目前存在哪些困难呢？这些问题的简答都需要慢慢来。咱们先来了解下机器学习（人工智能的核心）的背景。二、背景机器学习（Machine Learning）是一门专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能的学科。机器能否像人类一样能具有学习能力呢？1959年美国的塞缪尔(Samuel)设计了一个下棋程序，这个程序具有学习能力，它可以在不断的对弈中改善自己的棋艺。4年后，这个程序战胜了设计者本人。又过了3年，这个程序战胜了美国一个保持8年之久的常胜不败的冠军。这个程序向人们展示了机器学习的能力，提出了许多令人深思的社会问题与哲学问题（呵呵，人工智能正常的轨道没有很大的发展，这些什么哲学伦理啊倒发展的挺快。什么未来机器越来越像人，人越来越像机器啊。什么机器会反人类啊，ATM是开第一枪的啊等等。人类的思维无穷啊）。机器学习虽然发展了几十年，但还是存在很多没有良好解决的问题：例如图像识别、语音识别、自然语言理解、天气预测、基因表达、内容推荐等等。目前我们通过机器学习去解决这些问题的思路都是这样的（以视觉感知为例子）：从开始的通过传感器（例如CMOS）来获得数据。然后经过预处理、特征提取、特征选择，再到推理、预测或者识别。最后一个部分，也就是机器学习的部分，绝大部分的工作是在这方面做的，也存在很多的paper和研究。而中间的三部分，概括起来就是特征表达。良好的特征表达，对最终算法的准确性起了非常关键的作用，而且系统主要的计算和测试工作都耗在这一大部分。但，这块实际中一般都是人工完成的。靠人工提取特征。截止现在，也出现了不少NB的特征（好的特征应具有不变性（大小、尺度和旋转等）和可区分性）：例如Sift的出现，是局部图像特征描述子研究领域一项里程碑式的工作。由于SIFT对尺度、旋转以及一定视角和光照变化等图像变化都具有不变性，并且SIFT具有很强的可区分性，的确让很多问题的解决变为可能。但它也不是万能的。然而，手工地选取特征是一件非常费力、启发式（需要专业知识）的方法，能不能选取好很大程度上靠经验和运气，而且它的调节需要大量的时间。既然手工选取特征不太好，那么能不能自动地学习一些特征呢？答案是能！Deep Learning就是用来干这个事情的，看它的一个别名UnsupervisedFeature Learning，就可以顾名思义了，Unsupervised的意思就是不要人参与特征的选取过程。那它是怎么学习的呢？怎么知道哪些特征好哪些不好呢？我们说机器学习是一门专门研究计算机怎样模拟或实现人类的学习行为的学科。好，那我们人的视觉系统是怎么工作的呢？为什么在茫茫人海，芸芸众生，滚滚红尘中我们都可以找到另一个她（因为，你存在我深深的脑海里，我的梦里 我的心里 我的歌声里……）。人脑那么NB，我们能不能参考人脑，模拟人脑呢？（好像和人脑扯上点关系的特征啊，算法啊，都不错，但不知道是不是人为强加的，为了使自己的作品变得神圣和高雅。）近几十年以来，认知神经科学、生物学等等学科的发展，让我们对自己这个神秘的而又神奇的大脑不再那么的陌生。也给人工智能的发展推波助澜。三、人脑视觉机理1981 年的诺贝尔医学奖，颁发给了 David Hubel（出生于加拿大的美国神经生物学家） 和TorstenWiesel，以及 Roger Sperry。前两位的主要贡献，是“发现了视觉系统的信息处理”：可视皮层是分级的：我们看看他们做了什么。1958 年，DavidHubel 和Torsten Wiesel 在 JohnHopkins University，研究瞳孔区域与大脑皮层神经元的对应关系。他们在猫的后脑头骨上，开了一个3 毫米的小洞，向洞里插入电极，测量神经元的活跃程度。然后，他们在小猫的眼前，展现各种形状、各种亮度的物体。并且，在展现每一件物体时，还改变物体放置的位置和角度。他们期望通过这个办法，让小猫瞳孔感受不同类型、不同强弱的刺激。之所以做这个试验，目的是去证明一个猜测。位于后脑皮层的不同视觉神经元，与瞳孔所受刺激之间，存在某种对应关系。一旦瞳孔受到某一种刺激，后脑皮层的某一部分神经元就会活跃。经历了很多天反复的枯燥的试验，同时牺牲了若干只可怜的小猫，David Hubel 和Torsten Wiesel 发现了一种被称为“方向选择性细胞（Orientation Selective Cell）”的神经元细胞。当瞳孔发现了眼前的物体的边缘，而且这个边缘指向某个方向时，这种神经元细胞就会活跃。这个发现激发了人们对于神经系统的进一步思考。神经-中枢-大脑的工作过程，或许是一个不断迭代、不断抽象的过程。这里的关键词有两个，一个是抽象，一个是迭代。从原始信号，做低级抽象，逐渐向高级抽象迭代。人类的逻辑思维，经常使用高度抽象的概念。例如，从原始信号摄入开始（瞳孔摄入像素 Pixels），接着做初步处理（大脑皮层某些细胞发现边缘和方向），然后抽象（大脑判定，眼前的物体的形状，是圆形的），然后进一步抽象（大脑进一步判定该物体是只气球）。这个生理学的发现，促成了计算机人工智能，在四十年后的突破性发展。总的来说，人的视觉系统的信息处理是分级的。从低级的V1区提取边缘特征，再到V2区的形状或者目标的部分等，再到更高层，整个目标、目标的行为等。也就是说高层的特征是低层特征的组合，从低层到高层的特征表示越来越抽象，越来越能表现语义或者意图。而抽象层面越高，存在的可能猜测就越少，就越利于分类。例如，单词集合和句子的对应是多对一的，句子和语义的对应又是多对一的，语义和意图的对应还是多对一的，这是个层级体系。敏感的人注意到关键词了：分层。而Deep learning的deep是不是就表示我存在多少层，也就是多深呢？没错。那Deep learning是如何借鉴这个过程的呢？毕竟是归于计算机来处理，面对的一个问题就是怎么对这个过程建模？因为我们要学习的是特征的表达，那么关于特征，或者说关于这个层级特征，我们需要了解地更深入点。所以在说Deep Learning之前，我们有必要再啰嗦下特征（呵呵，实际上是看到那么好的对特征的解释，不放在这里有点可惜，所以就塞到这了）原文地址：http://blog.csdn.net/zouxy09/article/details/8775360/"}
{"content2":"资料来源：Robert Collins,CSE486, Penn State第8讲Stereo Vision深度信息感知是人类产生立体视觉的前提。生理过程一定是相当复杂，此处，我们只从物理角度，并采用数学的方法来讨论。Inferring depth from images taken at the same time by two or more cameras.基本透视投影透视投影是多对一的关系，投影线上的任何一点对应同一个像点。如果用两个摄像机，则可以消除这种多对一，从而能够确定第三维坐标Z的值，即深度信息。为什么可以感知深度信息呢？我们的左右眼从略微不同的角度观察景物，而这种视差与物体所处的位置有关。重要的概念之一：视差(Parallax)自己可以体验一下：将手指头放在离眼睛不同距离的位置，并轮换睁、闭左右眼，可以发现手指在不同距离的位置，视觉差也不同，且距离越近，视差越大。重要概念之二：Anaglyph image(来自wiki)Anaglyph images are used to provide a stereoscopic 3D effect, when viewed with glasses where the two lenses are different (usually chromatically opposite) colors, such as red and cyan. Images are made up of two color layers, superimposed, but offset with respect to each other to produce a depth effect. Usually the main subject is in the center, while the foreground and background are shifted laterally in opposite directions. The picture contains two differently filtered colored images, one for each eye. When viewed through the \"color coded\" \"anaglyph glasses\", they reveal an integrated stereoscopic image. The visual cortexof the brain fuses this into perception of a three dimensional scene or composition.这种立体照片的原理是利用特殊的眼镜，使左右眼接收不同颜色的光线，通过大脑合成立体照片。关于深度信息的感知理论还在发展中，这是一个复杂的过程，很难用单一的理论来完全描述，决定深度信息感觉的因素很多。如消失线，同类物体的大小，遮挡关系等等。甚至，用单眼也可以感知深度信息。下面着重借助几何和代数的方法来描述立体感知。先从简单的情况开始......假设两个相机的内部参数一致，如焦距、镜头等等，为了数学描述的方便，需引入坐标，由于坐标是人为引入的，因此客观世界中的事物可以处于不同的坐标系中。假设两个相机的X轴方向一致，像平面重叠，如下图所示，坐标系以左相机为准，右相机相对于左相机是简单的平移，用坐标表示为(Tx,0,0)Tx一般称为基线(baseline)，根据三角形相似关系，很容易得出空间中的一点P(X,Y,Z)分别在左右像平面上的投影坐标。因此，左相机像平面像点的坐标为$ x_l=f\\frac {X}{Z} $             $ y_l=f\\frac {Y}{Z} $右相机平面像点的坐标为$ x_r=f\\frac {X-T_x}{Z} $      $ y_r=f\\frac {Y}{Z} $显然，深度信息Z和视差(Disparity / Parallax) $ d $成反比，这与我们用手指做试验是相吻合的，这也是为什么近的物体看起来比远的物体移动得快。下一节：立体视觉基础2——相机成像的几何描述"}
{"content2":"课程的第一节，是一个综述性的课。对于这节课，我总结为以下几个问题：（一）什么是计算机视觉；（二）怎样实现计算机视觉（怎么让机器看懂图像）（三）计算机视觉的应用有哪些。那么首先解决第一个问题：什么是计算机视觉。Computer vision is a field that includes methods for acquiring, processing, analyzing, and understanding images.   ------------------------    Wikipidia计算机视觉是一个包含图像获取、处理、分析和理解的一个学科。那么计算机视觉的目的是什么呢？The goal of computer vision is to make useful decisions about real physical objects and scenes based on sensed images.计算机视觉是对图像进行处理、分析和理解，以此来对现实中的物体或场景得出有用的信息并且做出决断。那么为了对现实物体和场景做出决断，那就必须要对图像中含有的信息进行建立描述体（descriptor）和建模（model），因此很多专家认为计算机视觉的目的是为了从图像（image）中提取出对场景（scene）的一个描述体（descriptor）。所以这就得出了我们的第二个问题：怎么实现计算机视觉（怎么让机器看懂图像）首先我得说一个题外话，计算机视觉这个学科是属于人工智能领域的，而人工智能的目的是让机器拥有和人类一样的能力，人工智能的一个典型的方法就是模拟人类的工作方式。计算机视觉也是如此，仔细思考一下，你的眼睛和大脑是如何理解图像，这对你理解计算机视觉的实现很有帮助。比如说在你的面前摆放着一台电脑，你是如何得出“这是一台电脑”的这个决断的呢？好好剖析一下你的思维，我想最明显的特征那就是键盘和屏幕，如果拥有这两个特征的物体就可以判断为电脑了，那么如何判断这个是键盘呢，那么很明显的就是它拥有如此排布方式的方格和上面的字母，所以识别一个键盘就是第一步要先识别出方格型（shape），然后在判断方格之间的排布方式（relationship），这大概就是键盘的排布方式，然后也可以通过识别方格里面的字母，这就是文字识别了（text recognition）那么屏幕呢，首先仍然应该是方格形状，但是要大的多，然后呢就是玻璃材质也就是纹理（texture）了。以上的例子是我为了解释如何理解图像而想的一个例子，我并没有实际实现，而且实际实现多半也不是这样的，所以别太在意。总结一下，由以上的例子可以说明，实现计算机视觉是可以划分成很多的子问题的，那么有哪些子问题呢，在文档中由如下总结：Sensing:How do sensors obtain images of the world? How do the images encode properties of the world, such as material, shape, illumination and spatial relationships?Encoded Information: How do images yield information for understanding the 3D world,including the geometry, texture, motion, and identity of objects in it?Representations: What representations should be used for stored descriptions of objects,their parts, properties and relationships?Algorithms:What methods are there to process image information and construct descriptions of the world and its objects?以下是我自己翻译的：（仅供参考）成像原理：光学传感器是怎样形成图片的，图片是怎样编码世界的一些属性，如材料，形状，光强和空间关系。编码的信息：怎么从图像中推出用于理解三维世界的信息，包含几何信息，纹理信息，空间移动以及物体识别。表示形式：如何来表示对物体以及它的局部特征，属性和相互关系（数据结构）算法：处理图像信息和构建描述世界和物体特征的算法但我认为维基百科里讲的对图像获取，处理，分析，理解这四个层次的划分更加准确。讲完了计算机视觉的内容以后，接下来就是第三个问题计算机视觉的应用，应用全部来自课件。应用一：数洞洞（hole counting）如上述这个图，如何计算出图中有多少个洞（就是白色区域的个数）。这是一个工程师做出来的算法用来计算汽车横杆的螺栓洞的数量。他采用的方法是计算角点的数量，分为外角点（external Corner）和内角点（internel Corner）两种，下图是外角点和内角点的像素特征然后使用公式(E-I)/4就可计算出洞洞的数量。至于原因就不解释，当然我认为在某些特殊情况下这是有问题的。比如说这个图，交接点就成为这种形式，就不是角点，所以这图总共有六个外角点和零个内角点，但是有两个洞洞，不符合上述公式。当然这个问题有更好的答案，或者说我的答案是错的，请告诉我。应用二：核磁共振成像分析用到了二值化分析，将像素亮点强度高于某个阀值的设置为亮点，低于某个阀值设置为暗点，这样图像的对比度和纹理结构就可以很清晰的显示出来了。应用三：扫描文字识别比如说上述的手写体，自动的识别转成机器体。应用四：通过卫星图像计算雪覆盖面积通过计算卫星图像里面白色像素点的数量就可以计算出地上被雪覆盖的面积应用五：理解三维结构和相对关系通过理解三个部件的几何机构和相对位置关系，就可以把被遮蔽的信息给恢复出来，因为人是可以做到的，那么计算机就也可以做到。应用六：基于内容的图片搜索这个其实谷歌图片搜索很早就有了，之前的图片搜索都是用关键字去搜索，而更准确你可以输入一张图片，然后搜索出具有这个内容的图片。找出还有树的图片应用七：脸部检测和识别"}
{"content2":"获取到的 知识点1SpringBatch-记一次批处理优化过程：复合读，非常干的干货个人博客搜集自己每天的知识点。了解。并慢慢的研究最新的热点新闻知识点2  AI 大数据算法深度学习框架——CaffeCaffe是基于表达体系结构和可扩展代码的深度学习框架，被推崇的原因是其处理速度。人工智能的智能化需要通过大数据和机器算法来实现，不可避免的要分析海量数据。而Caffe可以在一天之内处理六千多万个图像，并且只需要一个NVIDIA K40 GPU来处理。语音识别工具——CNTKCNTK由Computational Network Toolkit的缩写而来，是由微软开源的人工智能工具，主要用于语音识别，并且在机器翻译、图像识别、图像字幕、语言理解、文本处理和语言建模上都能够带来帮助。CNTK的特点是无论机器上有单个CPU还是单个GPU，或者有多个GPU，或者在有多个GPU的多个机器上都能够很好的运行。深度学习库——Deeplearning4jDeeplearning4j是JVM开源的深度学习库，可以配置深度神经网络，既可以在分布式环境中运行，并且可以集成在Hadoop 和 Apache Spark 中。Deeplearning4j的特点是能够和Java、Scala和其他JVM语言兼容。分布式机器学习工具——DMTKDMTK是Distributed Machine Learning Toolkit的缩写，同样是微软开源的人工智能工具，用户大数据的应用程序。DMTK的的作用是能够加快对人工智能系统的训练，主要有DMTK框架、LightLDA主题模型算法和分布式字嵌入算法三个组件组成。DMTK对人工智能系统的训练速度非常快，曾经微软用100万个主题和1000万个单词的词汇表训练一个主题模型，在文档中收集了1000亿个符号。智能分析工具——H20H20有很多的公司客户，比如思科、PayPal、泛美等等，H20主要用户预测建模、风险欺诈分析、保险分析、广告技术、医疗保健和客户情报分析等。针对企业服务的版本需要付费，同时也有标准版，被集成在Apache Spark 中。机器学习框架——MahoutMahout是一个开源的机器学习框架，主要有三个特性，其一是构建可以扩展算法的编程环境，第二是像H20一样预制算法工具，第三是一个矢量数学实验环境，被叫做Samsara。现在Mahout被Adobe、英特尔、领英、Twitter、雅虎等很多公司使用。机器学习库——MLlib提到MLlib就不得不提Apache Spark，Apache Spark因为速度快被誉为最流行的大数据处理工具，而MLlib是Spark的可扩展机器学习库。MLlib包括很多机器学习算法，像分类、决策树、功能转换、回归、生存分析、集群、主题建模、推荐、频繁项集、ML 管道架构、统计、分布式线性代数等等。MLlib的特点是集成了Hadoop 并可以与 NumPy 和 R 进行交互操作。分层暂时记忆——NuPICNuPIC 是一个基于分层暂时记忆Hierarchical Temporal Memory，也就是HTM理论的开源人工智能项目。HTM的目标是创造一个在许多认知任务上接近或者比人类的认知能力表现更好的机器，想要创建一个计算机系统来模仿人类大脑皮层活动。神经网络算法编程库——OpenNNOpenNN是一个c++编程库，作用是实现神经网络算法。OpenNN的特点包括深度的架构和很高效的性能。在OpenNN的网站上有很多神经网络的文档，比如解释深情网络基本知识的入门教程，是一个具有高级理解力的人工智能工具。常识推理引擎——OpenCycOpenCyc由Cycorp开发，提供了对Cyc知识库的访问和常识推理引擎。OpenCyc有大约24万个条目，大约200万个三元组合大约7万个owl，OpenCyc是一个类似于链接到外部语义库的命名空间。OpenCyc主要应用在富领域模型、语义数据集成、文本理解、特殊领域的专家系统和游戏AI中。机器学习应用程序框架——Oryx 2Oryx 2构建在Apache Spark 和 Kafka 的基础上，是专门针对大规模机器学习的应用程序开发框架。Oryx 2的架构是一个独特的三层 λ 架构，主要应用是可以创建新的应用程序，还有一些预先构建的程序用于常见的大数据任务。预测引擎——PredictionIOPredictionIO可以用在创建有机器学习功能的预测引擎，用于部署实时动态查询的Web服务。高等数学运算——SystemMLSystemML提供了一个高度可伸缩的平台，能够实现高等数学运算，现在是Apache的一个大数据项目。SystemML的算法用R或者类似但并不是python的语法写成，可以在Spark和Hadoop上运行，现在已经应用在跟踪客户、规划机场交通等方面。数值计算库——TensorFlowTensorFlow由谷歌开源，是一个能够使用数据流图进行数据计算的库，有很好的灵活性、可移植性、自动微分贡呢，还支持Python和c++语言，可以运行在有单或者多个CPU和GPU的系统甚至是移动设备上。科学计算框架——TorchTorch的特点是灵活度高和速度快，可以用户机器学习、计算机视觉、信号处理、并行处理、音视频及图像等方面。人工智能领域仍然处于探索和创新领域，对于人工智能工程师来说，除了要考虑技术实现方式，更重要的其实是人工智能的应用思路。在第六届TOP100软件案例研究峰会的榜单中，有很多人工智能的相关话题入围。比如美国普渡大学将风控技术应用于个人信贷领域，微软将增强学习应用于商业模型，NFCU银行利用AI进行资产管理方式的转型，宜人贷将人工智能应用于反欺诈，东软将物联网智能分析用于设备维护等等案例。第六届TOP100软件案例研究峰会于11月9日-12日在北京国家会议中心举办，可通过TOP100头条号或官方网站领域现场免费体验票，通过这些人工智能已落地的实践经验，对人工智能的应用带来启示。"}
{"content2":"图像数据集模型需要好的数据才能训练出结果，本文总结了机器学习图像方面常用数据集。MNISTLink机器学习入门的标准数据集（Hello World!），10个类别，0-9 手写数字。包含了60,000 张 28x28 的二值训练图像，10,000 张 28x28 的二值测试图像。最早的深度卷积网络 LeNet 便是针对 MNIST 数据集的，MNIST 数据集之所以是机器学习的 “Hello World”，是因为当前主流深度学习框架几乎无一例外将 MNIST 数据集的处理作为介绍及入门第一教程，其中 Tensorflow 关于 MNIST 的教程非常详细。COCOLinkCOCO 是一个大规模的对象识别、分割以及 Captioning 数据集。具有以下特点：Object segmentationRecognition in contextSuperpixel stuff segmentation330K images (>200K labeled)1.5 million object instances80 object categories91 stuff categories5 captions per image250,000 people with keypoints以 2014 年的数据为例，其包含两种文件类型（训练、验证文件均有），Annotations（图片信息） 和 Images（图片文件本身）。图片名即 Annotations 中的 file_name 字段。Annotations 文件为一些超级大的 json 文件，分为三种类型：object instances, object keypoints, 和 image captions。三种类型均以以下 json 格式存储：Copy{ \"info\" : info, \"images\" : [image], \"annotations\" : [annotation], \"licenses\" : [license], } info{ \"year\" : int, \"version\" : str, \"description\" : str, \"contributor\" : str, \"url\" : str, \"date_created\" : datetime, } image{ \"id\" : int, \"width\" : int, \"height\" : int, \"file_name\" : str, \"license\" : int, \"flickr_url\" : str, \"coco_url\" : str, \"date_captured\" : datetime, } license{ \"id\" : int, \"name\" : str, \"url\" : str, }只有每种类型的 annotation 格式不同，如 Object Instance Annotations 格式为：Copyannotation{ \"id\" : int, \"image_id\" : int, \"category_id\" : int, \"segmentation\" : RLE or [polygon], \"area\" : float, \"bbox\" : [x,y,width,height], \"iscrowd\" : 0 or 1, } categories[{ \"id\" : int, \"name\" : str, \"supercategory\" : str, }]具体的格式信息可以在这里看到。ImageNetLinkMNIST 将初学者领进了深度学习领域，而 ImageNet 数据集对深度学习的浪潮起了巨大的推动作用。深度学习领域大牛 Hinton 在2012年发表的论文《ImageNet Classification with Deep Convolutional Neural Networks》在计算机视觉领域带来了一场“革命”，此论文的工作正是基于 ImageNet 数据集。ImageNet 数据集有1400多万幅图片，涵盖2万多个类别；其中有超过百万的图片有明确的类别标注和图像中物体位置的标注，具体信息如下：Total number of non-empty synsets: 21841Total number of images: 14,197,122Number of images with bounding box annotations: 1,034,908Number of synsets with SIFT features: 1000Number of images with SIFT features: 1.2 million你可以下载图片 urls 文件或者图片文件（需要注册，用于非商业用途）。图片 urls 文件内容为图片 ID 和 url：Copyn00015388_12 http://farm4.static.flickr.com/3040/2946102733_9b9c9cf24e.jpg n00015388_24 http://farm3.static.flickr.com/2093/2288303747_c62c007531.jpg n00015388_81 http://www.theresevangelder.nl/images/dierenportretten/dier4.jpg n00015388_155 http://www.zuidafrikaonline.nl/images/zuid-afrika-reis-giraffe.jpg n00015388_157 http://farm1.static.flickr.com/145/430300483_21e993670c.jpg ..._ 前面部分为 WordNet ID（wnid），一个 wnid 代表一个 synset（同义词集），如 n02084071 代表 \"dog, domestic dog, Canis familiaris\" 。具体信息可以看官方文档。ImageNet 的 Object Bounding Boxes 文件采用了和 PASCAL VOC 数据集相同的格式，因此可以使用 PASCAL Development Toolkit 解析。另外，ImageNet 的 Object Bounding Boxes 文件是按照 synset（同义词集）划分子文件夹的，每个压缩包下面是同语义的图片文件 Annotation：The PASCAL Visual Object ClassesLinkPASCAL VOC挑战赛是视觉对象的分类识别和检测的一个基准测试，提供了检测算法和学习性能的标准图像注释数据集和标准的评估系统。PASCAL VOC2007之后的数据集包括20个类别：人类；动物（鸟、猫、牛、狗、马、羊）；交通工具（飞机、自行车、船、公共汽车、小轿车、摩托车、火车）；室内（瓶子、椅子、餐桌、盆栽植物、沙发、电视）。PASCAL VOC挑战赛在2012年后便不再举办，但其数据集图像质量好，标注完备，非常适合用来测试算法性能。数据集包括图片的三种信息：原始图片（JPEGImages 文件夹），对象像素（SegmentationClass 文件夹）和分类像素（SegmentationObject 文件夹）：解压之后这些图片分别放在如下文件夹：其中 Annotations 文件夹是图片描述的 xml 文件，例如 JPEGImages 文件夹中的 000007.jpg 图片，就会有相应的 000007.xml 文件描述该图片，包括图片的宽高、包含的 Object（可以有多个） 的类别、坐标等信息：Copy<annotation> <folder>VOC2007</folder> <filename>000007.jpg</filename> <source> <database>The VOC2007 Database</database> <annotation>PASCAL VOC2007</annotation> <image>flickr</image> <flickrid>194179466</flickrid> </source> <owner> <flickrid>monsieurrompu</flickrid> <name>Thom Zemanek</name> </owner> <size> //图像尺寸（长宽以及通道数） <width>500</width> <height>333</height> <depth>3</depth> </size> <segmented>0</segmented> <object> <name>car</name> //物体类别 <pose>Unspecified</pose> //拍摄角度 <truncated>1</truncated> //是否被截断（0表示完整） <difficult>0</difficult> //目标是否难以识别（0表示容易识别） <bndbox> //bounding-box（包含左下角和右上角坐标） <xmin>141</xmin> <ymin>50</ymin> <xmax>500</xmax> <ymax>330</ymax> </bndbox> </object> </annotation>ImageSets 存放的是每一年的 Challenge 对应的图像数据，不同年份数据可能不同。其中 Action 下存放的是人的动作（例如running、jumping等等，这也是VOC challenge的一部分）Layout 下存放的是具有人体部位的数据（人的head、hand、feet等等，这也是VOC challenge的一部分）Main 文件夹下包含了各个分类的 ***_train.txt、***_val.txt和 ***_trainval.txt，如 aeroplane_train.txt。文件每行是一个图片ID以及是否为正样本（1代表正样本，-1代表负样本）。Segmentation 下存放的是验证集图片ID（val.txt 文件）、训练集图片ID（train.txt 文件）以及两者的合集（trainval.txt 文件）。The CIFAR-10 dataset and The CIFAR-100 datasetLinkCIFAR-10 和 CIFAR-100 数据集是 80 million tiny images 的子集。以 CIFAR-10 Python 版本为例，包含 10 个分类，60000 张 32x32 彩色图片，每个分类 6000 张图片。其中 50000 张是训练图片，另外 10000 张是测试图片。其中 50000 张分成了 5 个训练 batches，剩下的 10000 张是 test batch。训练数据每个类总共包含 5000 张，但每个 batch 每个类的图片数量可能并不平均。每个 batch 文件都是 Python pickle 生成的，所以可以使用 pickle 读取：Copydef unpickle(file): import pickle with open(file, 'rb') as fo: dict = pickle.load(fo, encoding='bytes') return dict函数返回一个 dict 对象，其中有两个 key 比较重要：data：10000x3072 numpy array，每一行是一个 32x32 彩色图片。每 1024 个元素依次代表 R G B，像素值没有归一化（取值为 0-255）。labels：每一行代表图片分类，取值 0-9。CIFAR-100 和 CIFAR-10 类似，只是类别为 100 个。实际上，CIFAR-10 和 MNIST 很类似。Tiny Images DatasetLinkTiny Images dataset 包含 79,302,017 张 32x32 彩色图片。包含 5 个文件：Image binary (227Gb)：图片本身，二进制格式Metadata binary (57Gb)：图片信息（filename, search engine used, ranking etc）Gist binary (114Gb)：图片描述Index data (7Mb)：Matlab 索引文件Matlab Tiny Images toolbox (150Kb) ：Matlab 索引文件 代码，用来加载图片FDDB: Face Detection Data Set and BenchmarkLinkFDDB是全世界最具权威的人脸检测评测平台之一，包含来自 Faces in the Wild 的2845张图片，共有5171个人脸数据。测试集范围包括：不同姿势、不同分辨率、旋转和遮挡等图片，同时包括灰度图和彩色图，标准的人脸标注区域为椭圆形。FDDB 数据集包含以下内容：原始图片（来自 Faces in the Wild ）人脸数据（Face annotations）检测输出（包括矩形区域 和 椭圆区域）其他信息原始图片原始图片可以在这里下载：originalPics.tar.gz。解压后图片的路径为 originalPics/year/month/day/big/*.jpg。人脸数据（Face annotations）比较重要的是 Face annotations。解压缩 FDDB-folds.tgz 文件将会得到 FDDB-folds 文件夹，包含 FDDB-fold-xx.txt 和 FDDB-fold-xx-ellipseList.txt 文件，xx 代表文件夹索引。\"FDDB-fold-xx.txt\" 文件的每一行指定了一个上述原始图片的一个文件名，如 \"2002/07/19/big/img_130\" 对应 \"originalPics/2002/07/19/big/img_130.jpg.\"对应的 annotations 文件 \"FDDB-fold-xx-ellipseList.txt\" 格式如下：Copy... <image name i> <number of faces in this image =im> <face i1> <face i2> ... <face im> ...每一个 face 即一个椭圆区域，用以下格式表示：Copy<长轴半径 短轴半径 长轴方向 x轴中心坐标 y轴中心坐标 1>示例：Copy2002/08/11/big/img_591 1 123.583300 85.549500 1.265839 269.693400 161.781200 1 2002/08/26/big/img_265 3 67.363819 44.511485 -1.476417 105.249970 87.209036 1 41.936870 27.064477 1.471906 184.070915 129.345601 1 70.993052 43.355200 1.370217 340.894300 117.498951 1 2002/07/19/big/img_423 1 87.080955 59.379319 1.550861 255.383099 133.767857 1 2002/08/24/big/img_490 1 54.692105 35.056825 -1.384924 145.665694 78.101005 1 2002/08/31/big/img_17676 2 37.099961 29.000000 1.433107 28.453831 37.664572 1 79.589662 49.835046 -1.457361 112.514300 92.364284 1参考PASCAL VOC数据集分析深度学习视觉领域常用数据集汇总"}
{"content2":"深度学习与计算机视觉系列(1)_基础介绍作者：寒小阳时间：2015年11月。出处：http://blog.csdn.net/han_xiaoyang/article/details/49876119声明：版权所有，转载请注明出处，谢谢。1.背景计算机视觉/computer vision是一个火了N年的topic。持续化升温的原因也非常简单：在搜索/影像内容理解/医学应用/地图识别等等领域应用太多，大家都有一个愿景『让计算机能够像人一样去”看”一张图片，甚至”读懂”一张图片』。有几个比较重要的计算机视觉任务，比如图片的分类,物体识别，物体定位于检测等等。而近年来的神经网络/深度学习使得上述任务的准确度有了非常大的提升。加之最近做了几个不大不小的计算机视觉上的项目，爱凑热闹的博主自然不打算放过此领域，也边学边做点笔记总结，写点东西，写的不正确的地方，欢迎大家提出和指正。2.基础知识为了简单易读易懂，这个课程笔记系列中绝大多数的代码都使用Python完成。这里稍微介绍一下python和Numpy/Scipy(python中的科学计算包)的一些基础。2.1 python基础python是一种长得像伪代码，具备高可读性的编程语言。优点挺多：可读性相当好，写起来也简单，所想立马可以转为实现代码，且社区即为活跃，可用的package相当多；缺点：效率一般。2.1.1 基本数据类型最常用的有数值型(Numbers),布尔型(Booleans)和字符串(String)三种。数值型(Numbers)可进行简单的运算，如下：x = 5 print type(x) # Prints \"<type 'int'>\" print x # Prints \"5\" print x + 1 # 加; prints \"6\" print x - 1 # 减; prints \"4\" print x * 2 # 乘; prints \"10\" print x ** 2 # 幂; prints \"25\" x += 1 #自加 print x # Prints \"6\" x *= 2 #自乘 print x # Prints \"12\" y = 2.5 print type(y) # Prints \"<type 'float'>\" print y, y + 1, y * 2, y ** 2 # Prints \"2.5 3.5 5.0 6.25\"12345678910111213141234567891011121314PS：python中没有x++ 和 x– 操作布尔型(Booleans)包含True False和常见的与或非操作t = True f = False print type(t) # Prints \"<type 'bool'>\" print t and f # 逻辑与; prints \"False\" print t or f # 逻辑或; prints \"True\" print not t # 逻辑非; prints \"False\" print t != f # XOR; prints \"True\"12345671234567字符串型(String)字符串可以用单引号/双引号/三引号声明hello = 'hello' world = \"world\" print hello # Prints \"hello\" print len(hello) # 字符串长度; prints \"5\" hw = hello + ' ' + world # 字符串连接 print hw # prints \"hello world\" hw2015 = '%s %s %d' % (hello, world, 2015) # 格式化字符串 print hw2015 # prints \"hello world 2015\"1234567812345678字符串对象有很有有用的函数：s = \"hello\" print s.capitalize() # 首字母大写; prints \"Hello\" print s.upper() # 全大写; prints \"HELLO\" print s.rjust(7) # 以7为长度右对齐，左边补空格; prints \" hello\" print s.center(7) # 居中补空格; prints \" hello \" print s.replace('l', '(ell)') # 字串替换;prints \"he(ell)(ell)o\" print ' world '.strip() # 去首位空格; prints \"world\"123456712345672.1.2 基本容器列表/List和数组类似的一个东东，不过可以包含不同类型的元素，同时大小也是可以调整的。xs = [3, 1, 2] # 创建 print xs, xs[2] # Prints \"[3, 1, 2] 2\" print xs[-1] # 第-1个元素，即最后一个 xs[2] = 'foo' # 下标从0开始，这是第3个元素 print xs # 可以有不同类型，Prints \"[3, 1, 'foo']\" xs.append('bar') # 尾部添加一个元素 print xs # Prints x = xs.pop() # 去掉尾部的元素 print x, xs # Prints \"bar [3, 1, 'foo']\"123456789123456789列表最常用的操作有：切片/slicing即取子序列/一部分元素，如下：nums = range(5) # 从1到5的序列 print nums # Prints \"[0, 1, 2, 3, 4]\" print nums[2:4] # 下标从2到4-1的元素 prints \"[2, 3]\" print nums[2:] # 下标从2到结尾的元素 print nums[:2] # 从开头到下标为2-1的元素 [0, 1] print nums[:] # 恩，就是全取出来了 print nums[:-1] # 从开始到第-1个元素(最后的元素) nums[2:4] = [8, 9] # 对子序列赋值 print nums # Prints \"[0, 1, 8, 8, 4]\"123456789123456789循环/loops即遍历整个list，做一些操作，如下：animals = ['cat', 'dog', 'monkey'] for animal in animals: print animal # 依次输出 \"cat\", \"dog\", \"monkey\"，每个一行.12341234可以用enumerate取出元素的同时带出下标animals = ['cat', 'dog', 'monkey'] for idx, animal in enumerate(animals): print '#%d: %s' % (idx + 1, animal) # 输出 \"#1: cat\", \"#2: dog\", \"#3: monkey\"，一个一行。12341234List comprehension这个相当相当相当有用，在很长的list生成过程中，效率完胜for循环：# for 循环 nums = [0, 1, 2, 3, 4] squares = [] for x in nums: squares.append(x ** 2) print squares # Prints [0, 1, 4, 9, 16] # list comprehension nums = [0, 1, 2, 3, 4] squares = [x ** 2 for x in nums] print squares # Prints [0, 1, 4, 9, 16]12345678910111234567891011你猜怎么着，list comprehension也是可以加多重条件的：nums = [0, 1, 2, 3, 4] even_squares = [x ** 2 for x in nums if x % 2 == 0] print even_squares # Prints \"[0, 4, 16]\"123123字典/Dict和Java中的Map一样的东东，用于存储key-value对：d = {'cat': 'cute', 'dog': 'furry'} # 创建 print d['cat'] # 根据key取出value print 'cat' in d # 判断是否有'cat'这个key d['fish'] = 'wet' # 添加元素 print d['fish'] # Prints \"wet\" # print d['monkey'] # KeyError: 'monkey'非本字典的key print d.get('monkey', 'N/A') # 有key返回value，无key返回\"N/A\" print d.get('fish', 'N/A') # prints \"wet\" del d['fish'] # 删除某个key以及对应的value print d.get('fish', 'N/A') # prints \"N/A\"1234567891012345678910对应list的那些操作，你在dict里面也能找得到：循环/loops# for循环 d = {'person': 2, 'cat': 4, 'spider': 8} for animal in d: legs = d[animal] print 'A %s has %d legs' % (animal, legs) # Prints \"A person has 2 legs\", \"A spider has 8 legs\", \"A cat has 4 legs\" # 通过iteritems d = {'person': 2, 'cat': 4, 'spider': 8} for animal, legs in d.iteritems(): print 'A %s has %d legs' % (animal, legs) # Prints \"A person has 2 legs\", \"A spider has 8 legs\", \"A cat has 4 legs\"123456789101112123456789101112# Dictionary comprehension nums = [0, 1, 2, 3, 4] even_num_to_square = {x: x ** 2 for x in nums if x % 2 == 0} print even_num_to_square # Prints \"{0: 0, 2: 4, 4: 16}\"12341234元组/turple本质上说，还是一个list，只不过里面的每个元素都是一个两元组对。d = {(x, x + 1): x for x in range(10)} # 创建 t = (5, 6) # Create a tuple print type(t) # Prints \"<type 'tuple'>\" print d[t] # Prints \"5\" print d[(1, 2)] # Prints \"1\"12345123452.1.3 函数用def可以定义一个函数：def sign(x): if x > 0: return 'positive' elif x < 0: return 'negative' else: return 'zero' for x in [-1, 0, 1]: print sign(x) # Prints \"negative\", \"zero\", \"positive\"12345678910111234567891011def hello(name, loud=False): if loud: print 'HELLO, %s' % name.upper() else: print 'Hello, %s!' % name hello('Bob') # Prints \"Hello, Bob\" hello('Fred', loud=True) # Prints \"HELLO, FRED!\"1234567812345678类/Classpython里面的类定义非常的直接和简洁：class Greeter: # Constructor def __init__(self, name): self.name = name # Create an instance variable # Instance method def greet(self, loud=False): if loud: print 'HELLO, %s!' % self.name.upper() else: print 'Hello, %s' % self.name g = Greeter('Fred') # Construct an instance of the Greeter class g.greet() # Call an instance method; prints \"Hello, Fred\" g.greet(loud=True) # Call an instance method; prints \"HELLO, FRED!\"12345678910111213141516123456789101112131415162.2.NumPy基础NumPy是Python的科学计算的一个核心库。它提供了一个高性能的多维数组(矩阵)对象，可以完成在其之上的很多操作。很多机器学习中的计算问题，把数据vectorize之后可以进行非常高效的运算。2.2.1 数组一个NumPy数组是一些类型相同的元素组成的类矩阵数据。用list或者层叠的list可以初始化：import numpy as np a = np.array([1, 2, 3]) # 一维Numpy数组 print type(a) # Prints \"<type 'numpy.ndarray'>\" print a.shape # Prints \"(3,)\" print a[0], a[1], a[2] # Prints \"1 2 3\" a[0] = 5 # 重赋值 print a # Prints \"[5, 2, 3]\" b = np.array([[1,2,3],[4,5,6]]) # 二维Numpy数组 print b.shape # Prints \"(2, 3)\" print b[0, 0], b[0, 1], b[1, 0] # Prints \"1 2 4\"123456789101112123456789101112生成一些特殊的Numpy数组(矩阵)时，我们有特定的函数可以调用：import numpy as np a = np.zeros((2,2)) # 全0的2*2 Numpy数组 print a # Prints \"[[ 0. 0.] # [ 0. 0.]]\" b = np.ones((1,2)) # 全1 Numpy数组 print b # Prints \"[[ 1. 1.]]\" c = np.full((2,2), 7) # 固定值Numpy数组 print c # Prints \"[[ 7. 7.] # [ 7. 7.]]\" d = np.eye(2) # 2*2 对角Numpy数组 print d # Prints \"[[ 1. 0.] # [ 0. 1.]]\" e = np.random.random((2,2)) # 2*2 的随机Numpy数组 print e # 随机输出12345678910111213141516171819123456789101112131415161718192.2.2 Numpy数组索引与取值可以通过像list一样的分片/slicing操作取出需要的数值部分。import numpy as np # 创建如下的3*4 Numpy数组 # [[ 1 2 3 4] # [ 5 6 7 8] # [ 9 10 11 12]] a = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]]) # 通过slicing取出前两行的2到3列: # [[2 3] # [6 7]] b = a[:2, 1:3] # 需要注意的是取出的b中的数据实际上和a的这部分数据是同一份数据. print a[0, 1] # Prints \"2\" b[0, 0] = 77 # b[0, 0] 和 a[0, 1] 是同一份数据 print a[0, 1] # a也被修改了，Prints \"77\"12345678910111213141516171234567891011121314151617import numpy as np a = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]]) row_r1 = a[1, :] # a 的第二行 row_r2 = a[1:2, :] # 同上 print row_r1, row_r1.shape # Prints \"[5 6 7 8] (4,)\" print row_r2, row_r2.shape # Prints \"[[5 6 7 8]] (1, 4)\" col_r1 = a[:, 1] col_r2 = a[:, 1:2] print col_r1, col_r1.shape # Prints \"[ 2 6 10] (3,)\" print col_r2, col_r2.shape # Prints \"[[ 2] # [ 6] # [10]] (3, 1)\"123456789101112131415123456789101112131415还可以这么着取：import numpy as np a = np.array([[1,2], [3, 4], [5, 6]]) # 取出(0,0) (1,1) (2,0)三个位置的值 print a[[0, 1, 2], [0, 1, 0]] # Prints \"[1 4 5]\" # 和上面一样 print np.array([a[0, 0], a[1, 1], a[2, 0]]) # Prints \"[1 4 5]\" # 取出(0,1) (0,1) 两个位置的值 print a[[0, 0], [1, 1]] # Prints \"[2 2]\" # 同上 print np.array([a[0, 1], a[0, 1]]) # Prints \"[2 2]\"123456789101112131415123456789101112131415我们还可以通过条件得到bool型的Numpy数组结果，再通过这个数组取出符合条件的值，如下：import numpy as np a = np.array([[1,2], [3, 4], [5, 6]]) bool_idx = (a > 2) # 判定a大于2的结果矩阵 print bool_idx # Prints \"[[False False] # [ True True] # [ True True]]\" # 再通过bool_idx取出我们要的值 print a[bool_idx] # Prints \"[3 4 5 6]\" # 放在一起我们可以这么写 print a[a > 2] # Prints \"[3 4 5 6]\"123456789101112131415123456789101112131415Numpy数组的类型import numpy as np x = np.array([1, 2]) print x.dtype # Prints \"int64\" x = np.array([1.0, 2.0]) print x.dtype # Prints \"float64\" x = np.array([1, 2], dtype=np.int64) # 强制使用某个type print x.dtype # Prints \"int64\"12345678910123456789102.2.3 Numpy数组的运算矩阵的加减开方和(元素对元素)乘除如下：import numpy as np x = np.array([[1,2],[3,4]], dtype=np.float64) y = np.array([[5,6],[7,8]], dtype=np.float64) # [[ 6.0 8.0] # [10.0 12.0]] print x + y print np.add(x, y) # [[-4.0 -4.0] # [-4.0 -4.0]] print x - y print np.subtract(x, y) # 元素对元素，点对点的乘积 # [[ 5.0 12.0] # [21.0 32.0]] print x * y print np.multiply(x, y) # 元素对元素，点对点的除法 # [[ 0.2 0.33333333] # [ 0.42857143 0.5 ]] print x / y print np.divide(x, y) # 开方 # [[ 1. 1.41421356] # [ 1.73205081 2. ]] print np.sqrt(x)1234567891011121314151617181920212223242526272829303112345678910111213141516171819202122232425262728293031矩阵的内积是通过下列方法计算的：import numpy as np x = np.array([[1,2],[3,4]]) y = np.array([[5,6],[7,8]]) v = np.array([9,10]) w = np.array([11, 12]) # 向量内积，得到 219 print v.dot(w) print np.dot(v, w) # 矩阵乘法，得到 [29 67] print x.dot(v) print np.dot(x, v) # 矩阵乘法 # [[19 22] # [43 50]] print x.dot(y) print np.dot(x, y)123456789101112131415161718192021123456789101112131415161718192021特别特别有用的一个操作是，sum/求和(对某个维度)：import numpy as np x = np.array([[1,2],[3,4]]) print np.sum(x) # 整个矩阵的和，得到 \"10\" print np.sum(x, axis=0) # 每一列的和 得到 \"[4 6]\" print np.sum(x, axis=1) # 每一行的和 得到 \"[3 7]\"12345671234567还有一个经常会用到操作是矩阵的转置，在Numpy数组里用.T实现：import numpy as np x = np.array([[1,2], [3,4]]) print x # Prints \"[[1 2] # [3 4]]\" print x.T # Prints \"[[1 3] # [2 4]]\" # 1*n的Numpy数组，用.T之后其实啥也没做: v = np.array([1,2,3]) print v # Prints \"[1 2 3]\" print v.T # Prints \"[1 2 3]\"1234567891011121234567891011122.2.4 BroadcastingNumpy还有一个非常牛逼的机制，你想想，如果你现在有一大一小俩矩阵，你想使用小矩阵在大矩阵上做多次操作。额，举个例子好了，假如你想将一个1*n的矩阵，加到m*n的矩阵的每一行上：#你如果要用for循环实现是酱紫的(下面用y的原因是，你不想改变原来的x) import numpy as np x = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]]) v = np.array([1, 0, 1]) y = np.empty_like(x) # 设置一个和x一样维度的Numpy数组y # 逐行相加 for i in range(4): y[i, :] = x[i, :] + v # 恩，y就是你想要的了 # [[ 2 2 4] # [ 5 5 7] # [ 8 8 10] # [11 11 13]] print y12345678910111213141516171234567891011121314151617#上一种方法如果for的次数非常多，会很慢，于是我们改进了一下 import numpy as np x = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]]) v = np.array([1, 0, 1]) vv = np.tile(v, (4, 1)) # 变形，重复然后叠起来 print vv # Prints \"[[1 0 1] # [1 0 1] # [1 0 1] # [1 0 1]]\" y = x + vv # 相加 print y # Prints \"[[ 2 2 4 # [ 5 5 7] # [ 8 8 10] # [11 11 13]]\"123456789101112131415123456789101112131415#其实因为Numpy的Broadcasting，你可以直接酱紫操作 import numpy as np x = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]]) v = np.array([1, 0, 1]) y = x + v # 直接加！！！ print y # Prints \"[[ 2 2 4] # [ 5 5 7] # [ 8 8 10] # [11 11 13]]\"1234567891012345678910更多Broadcasting的例子请看下面：import numpy as np v = np.array([1,2,3]) # v has shape (3,) w = np.array([4,5]) # w has shape (2,) # 首先把v变成一个列向量 # v现在的形状是(3, 1); # 作用在w上得到的结果形状是(3, 2)，如下 # [[ 4 5] # [ 8 10] # [12 15]] print np.reshape(v, (3, 1)) * w # 逐行相加 x = np.array([[1,2,3], [4,5,6]]) # 得到如下结果: # [[2 4 6] # [5 7 9]] print x + v # 先逐行相加再转置，得到以下结果: # [[ 5 6 7] # [ 9 10 11]] print (x.T + w).T # 恩，也可以这么做 print x + np.reshape(w, (2, 1))12345678910111213141516171819202122232425123456789101112131415161718192021222324252.3 SciPyNumpy提供了一个非常方便操作和计算的高维向量对象，并提供基本的操作方法，而Scipy是在Numpy的基础上，提供很多很多的函数和方法去直接完成你需要的矩阵操作。有兴趣可以浏览Scipy方法索引查看具体的方法，函数略多，要都记下来有点困难，随用随查吧。向量距离计算需要特别拎出来说一下的是，向量之间的距离计算，这个Scipy提供了很好的接口scipy.spatial.distance.pdist：import numpy as np from scipy.spatial.distance import pdist, squareform # [[0 1] # [1 0] # [2 0]] x = np.array([[0, 1], [1, 0], [2, 0]]) print x # 计算矩阵每一行和每一行之间的欧氏距离 # d[i, j] 是 x[i, :] 和 x[j, :] 之间的距离, # 结果如下： # [[ 0. 1.41421356 2.23606798] # [ 1.41421356 0. 1. ] # [ 2.23606798 1. 0. ]] d = squareform(pdist(x, 'euclidean')) print d123456789101112131415161712345678910111213141516172.4 Matplotlib这是python中的一个作图工具包。如果你熟悉matlab的语法的话，应该会用得挺顺手。可以通过matplotlib.pyplot.plot了解更多绘图相关的设置和参数。import numpy as np import matplotlib.pyplot as plt # 计算x和对应的sin值作为y x = np.arange(0, 3 * np.pi, 0.1) y = np.sin(x) # 用matplotlib绘出点的变化曲线 plt.plot(x, y) plt.show() # 只有调用plt.show()之后才能显示1234567891012345678910结果如下：# 在一个图中画出2条曲线 import numpy as np import matplotlib.pyplot as plt # 计算x对应的sin和cos值 x = np.arange(0, 3 * np.pi, 0.1) y_sin = np.sin(x) y_cos = np.cos(x) # 用matplotlib作图 plt.plot(x, y_sin) plt.plot(x, y_cos) plt.xlabel('x axis label') plt.ylabel('y axis label') plt.title('Sine and Cosine') plt.legend(['Sine', 'Cosine']) plt.show()12345678910111213141516171234567891011121314151617# 用subplot分到子图里 import numpy as np import matplotlib.pyplot as plt # 得到x对应的sin和cos值 x = np.arange(0, 3 * np.pi, 0.1) y_sin = np.sin(x) y_cos = np.cos(x) # 2*1个子图，第一个位置. plt.subplot(2, 1, 1) # 画第一个子图 plt.plot(x, y_sin) plt.title('Sine') # 画第2个子图 plt.subplot(2, 1, 2) plt.plot(x, y_cos) plt.title('Cosine') plt.show()12345678910111213141516171819202122123456789101112131415161718192021222.5 简单图片读写可以使用imshow来显示图片。import numpy as np from scipy.misc import imread, imresize import matplotlib.pyplot as plt img = imread('/Users/HanXiaoyang/Comuter_vision/computer_vision.jpg') img_tinted = img * [1, 0.95, 0.9] # 显示原始图片 plt.subplot(1, 2, 1) plt.imshow(img) # 显示调色后的图片 plt.subplot(1, 2, 2) plt.imshow(np.uint8(img_tinted)) plt.show()1234567891011121314151612345678910111213141516参考资料与原文cs231n python/Numpy指南"}
{"content2":"今日CS.CV计算机视觉论文速览Thu, 7 Mar 2019Totally 36 papersDaily Computer Vision Papers[1] Title: Quantifying error contributions of computational steps, algorithms and hyperparameter choices in image classification pipelinesAuthors:Aritra Chowdhury, Malik Magdin-Ismail, Bulent Yener[2] Title: Learning multimodal representations for sample-efficient recognition of human actionsAuthors:Miguel Vasco, Francisco S. Melo, David Martins de Matos, Ana Paiva, Tetsunari Inamura[3] Title: Image captioning with weakly-supervised attention penaltyAuthors:Jiayun Li, Mohammad K. Ebrahimpour, Azadeh Moghtaderi, Yen-Yun Yu[4] Title: Understanding and Visualizing Deep Visual Saliency ModelsAuthors:Sen He, Hamed R. Tavakoli, Ali Borji, Yang Mi, Nicolas Pugeault[5] Title: Prostate Segmentation from 3D MRI Using a Two-Stage Model and Variable-Input Based Uncertainty MeasureAuthors:Huitong Pan, Yushan Feng, Quan Chen, Craig Meyer, Xue Feng[6] Title: A Synchronized Multi-Modal Attention-Caption Dataset and AnalysisAuthors:Sen He, Hamed R. Tavakoli, Ali Borji, Nicolas Pugeault[7] Title: Hybrid LSTM and Encoder-Decoder Architecture for Detection of Image ForgeriesAuthors:Jawadul H. Bappy, Cody Simons, Lakshmanan Nataraj, B.S. Manjunath, Amit K. Roy-Chowdhury[8] Title: Object Counting and Instance Segmentation with Image-level SupervisionAuthors:Hisham Cholakkal, Guolei Sun (equal contribution), Fahad Shahbaz Khan, Ling Shao[9] Title: Compressing complex convolutional neural network based on an improved deep compression algorithmAuthors:Jiasong Wu, Hongshan Ren, Youyong Kong, Chunfeng Yang, Lotfi Senhadji, Huazhong Shu[10] Title: CANet: Class-Agnostic Segmentation Networks with Iterative Refinement and Attentive Few-Shot LearningAuthors:Chi Zhang, Guosheng Lin, Fayao Liu, Rui Yao, Chunhua Shen[11] Title: Self-Supervised Learning of 3D Human Pose using Multi-view GeometryAuthors:Muhammed Kocabas, Salih Karagoz, Emre Akbas[12] Title: Video-based surgical skill assessment using 3D convolutional neural networksAuthors:Isabel Funke, Sören Torge Mees, Jürgen Weitz, Stefanie Speidel[13] Title: Visual Discourse ParsingAuthors:Arjun R Akula, Song-Chun Zhu[14] Title: Photo-realistic Image Super-resolution with Fast and Lightweight Cascading Residual NetworkAuthors:Namhyuk Ahn, Byungkon Kang, Kyung-Ah Sohn[15] Title: Characterizing Human Behaviours Using Statistical Motion DescriptorAuthors:Eissa Jaber Alreshidi, Mohammad Bilal[16] Title: Robust Video Background Identification by Dominant Rigid Motion EstimationAuthors:Kaimo Lin, Nianjuan Jiang, Loong Fah Cheong, Jiangbo Lu, Xun Xu[17] Title: DepthwiseGANs: Fast Training Generative Adversarial Networks for Realistic Image SynthesisAuthors:Mkhuseli Ngxande, Jules-Raymond Tapamo, Michael Burke[18] Title: Transfer feature generating networks with semantic classes structure for zero-shot learningAuthors:Guangfeng Lin, Wanjun Chen, Kaiyang Liao, Xiaobing Kang, Caixia Fan[19] Title: Deep Transfer Learning for Multiple Class Novelty DetectionAuthors:Pramuditha Perera, Vishal M. Patel[20] Title: Robust Lane Detection from Continuous Driving Scenes Using Deep Neural NetworksAuthors:Qin Zou, Hanwen Jiang, Qiyu Dai, Yuanhao Yue, Long Chen, Qian Wang[21] Title: Semantic Adversarial Network with Multi-scale Pyramid Attention for Video ClassificationAuthors:De Xie, Cheng Deng, Hao Wang, Chao Li, Dapeng Tao[22] Title: Large-Scale Pedestrian Retrieval CompetitionAuthors:Da Li, Zhang Zhang[23] Title: Age Progression and Regression with Spatial Attention ModulesAuthors:Qi Li, Yunfan Liu, Zhenan Sun[24] Title: Decoders Matter for Semantic Segmentation: Data-Dependent Decoding Enables Flexible Feature AggregationAuthors:Zhi Tian, Chunhua Shen, Tong He, Youliang Yan[25] Title: Bounded Residual Gradient Networks (BReG-Net) for Facial Affect ComputingAuthors:Behzad Hasani, Pooran Singh Negi, Mohammad H. Mahoor[26] Title: Defining Image Memorability using the Visual Memory SchemaAuthors:Erdem Akagunduz, Adrian G. Bors, Karla K. Evans[27] Title: Abnormal Chest X-ray Identification With Generative Adversarial One-Class ClassifierAuthors:Yuxing Tang, Youbao Tang, Mei Han, Jing Xiao, Ronald M. Summers[28] Title: Crowd Counting Using Scale-Aware Attention NetworksAuthors:Mohammad Asiful Hossain, Mehrdad Hosseinzadeh, Omit Chanda, Yang Wang[29] Title: Tactical Rewind: Self-Correction via Backtracking in Vision-and-Language NavigationAuthors:Liyiming Ke, Xiujun Li, Yonatan Bisk, Ari Holtzman, Zhe Gan, Jingjing Liu, Jianfeng Gao, Yejin Choi, Siddhartha Srinivasa[30] Title: Combining Optimal Control and Learning for Visual Navigation in Novel EnvironmentsAuthors:Somil Bansal, Varun Tolani, Saurabh Gupta, Jitendra Malik, Claire Tomlin[31] Title: GQ-STN: Optimizing One-Shot Grasp Detection based on Robustness ClassifierAuthors:Alexandre Gariépy, Jean-Christophe Ruel, Brahim Chaib-draa, Philippe Giguère[32] Title: High-Fidelity Image Generation With Fewer LabelsAuthors:Mario Lucic, Michael Tschannen, Marvin Ritter, Xiaohua Zhai, Olivier Bachem, Sylvain Gelly[33] Title: AAAI-2019 Workshop on Games and Simulations for Artificial IntelligenceAuthors:Marwan Mattar, Roozbeh Mottaghi, Julian Togelius, Danny Lange[34] Title: Safeguarded Dynamic Label Regression for Generalized Noisy SupervisionAuthors:Jiangchao Yao, Ya Zhang, Ivor W. Tsang, Jun Sun[35] Title: Exploring Deep Spiking Neural Networks for Automated Driving ApplicationsAuthors:Sambit Mohapatra, Heinrich Gotzig, Senthil Yogamani, Stefan Milz, Raoul Zollner[36] Title: Viewpoint Optimization for Autonomous Strawberry Harvesting with Deep Reinforcement LearningAuthors:Jonathon SatherPapers from arxiv.org更多精彩请移步主页Interesting:pic from pixels.com"}
{"content2":"下载地址：网盘下载OpenCV 3是一种先进的计算机视觉库，可以用于各种图像和视频处理操作，通过OpenCV 3 能很容易地实现一些有前景且功能先进的应用（比如：人脸识别或目标跟踪等）。理解与计算机视觉相关的算法、模型以及OpenCV 3 API背后的基本概念，有助于开发现实世界中的各种应用程序（比如：安全和监视领域的工具）。本书将从图像处理的基本操作出发，带你开启先进计算机视觉概念的探索之旅。计算机视觉是一个快速发展的学科，在现实生活中，它的应用增长得非常快，因此写作本书的目的是为了帮助计算机视觉领域的新手和想要了解全新的OpenCV 3.0.0的计算机视觉专家。本书的主要内容第1章介绍如何在不同平台下安装基于Python的OpenCV，并给出一些常见问题的解决方法。第2章介绍了OpenCV的I/O功能，并讨论与项目相关的概念，以及如何针对该项目进行面向对象设计。第3章介绍一些图像变换方法，例如在图像中检测肤色、锐化图像、标记主体轮廓，以及使用线段检测器检测人行横道等。第4章介绍如何利用深度摄像头的数据来识别前景和背景区域，这样就可以限制针对前景或背景的效果。第5章介绍一些OpenCV的人脸检测功能和相关的数据文件，这些文件定义了跟踪目标的特定类型。第6章介绍如何用OpenCV来检测图像特征，并利用这些特征来匹配和搜索图像。第7章介绍目标检测和目标识别的概念，这是计算机视觉中最常见的问题之一。第8章对目标跟踪进行深入探讨，目标跟踪是对摄像机中的图像或视频中移动的物体进行定位的过程。第9章介绍基于OpenCV的人工神经网络，并介绍其在现实生活中的应用。阅读前的准备工作本书第1章会指导读者安装所有必要软件，你只需准备一台较新的计算机。另外，强烈推荐为计算机安装摄像头，但这并不是必备的。本书的读者对象本书针对具有一定Python工作经验的程序员以及想要利用OpenCV库研究计算机视觉课题的读者。本书不要求读者具有计算机视觉或OpenCV经验，但要具有编程经验。本书体例本书有很多用来区分不同信息的文本格式，下面给出一些这类格式的样例，并解释它们的含义。代码块的格式如下：为了提醒读者注意代码块中的特殊部分，会将相关行或相关项设置为粗体：.　　命令行的输入或输出的格式为：注意：警告或重要注释以这样的形式出现。提示：提示和技巧以这样的形式出现。下载示例代码读者可登录华章网站（www.hzbook.com）本书页面，下载本书示例代码。Joe Minichino　是Hoolux Medical从事计算机视觉的工程师，他利用业余时间开发了NoSQL数据库LokiJS。他也是重金属歌手/作曲家。他是一个充满激情的程序员，对编程语言和技术非常好奇，并一直在使用它们。在Hoolux，Joe领导了针对医疗行业的Android计算机视觉广告平台的开发。他出生在意大利瓦雷泽市的Lombardy，并在那里长大，在米兰Universitá Statale受过哲学教育，最近11年Joe在爱尔兰的Cork度过，在这里他成为Cork技术研究所的一名计算机科学研究生。我非常感谢我的合作伙伴Rowena，她总是鼓励我，也感谢两个小女儿给我灵感。非常感谢这本书的合作者和编辑，尤其是Joe Howse、Adrian Roesbrock、Brandon Castellano、OpenCV社区，以及Packt出版社中那些为本书付出劳动的人。Joseph Howse　生活在加拿大。在冬天，他留着胡子，而他的四只猫留着厚皮毛。他喜欢每天给猫梳毛。有时猫还会抓他的胡子。自2012年以来，他一直在为Packt出版社写作，他的著作包括《OpenCV for Secret Agents》《OpenCV Blueprints》《Android Application Programming with OpenCV 3》《OpenCV Computer Vision with Python》以及《Python Game Programming by Example》。当他不写书或打理萌宠时，他会提供咨询和培训，并通过他的公司（Nummist Media公司（http://nummist.com））进行软件开发服务。刘波　博士，重庆工商大学计算机科学与信息工程学院教师，主要从事机器学习理论、计算机视觉和最优化技术研究，同时对Hadoop和Spark平台上的大数据分析感兴趣，也对Linux编程和Oracle数据库感兴趣。苗贝贝　硕士，北京工商大学计算机与信息工程学院研究生，主要从事机器学习理论、时间序列动力学特征分析及应用的研究，对基于Python的计算机视觉分析有浓厚的兴趣。史斌　2015年本科毕业于电子科技大学计算机学院，目前就职于成都知数科技有限公司，主要从事数据爬取、数据处理、平台运维等工作，熟悉Python、Linux shell，同时热爱计算机视觉编程，熟悉Python下的OpenCV编程。下载地址：网盘下载"}
{"content2":"转载申明：最近在看AlphaGo的原理，刚好在https://blog.csdn.net/a1805180411/article/details/51155164发现了这篇文章，感觉写的非常好，好东西就要分享，所以转载过来供大家学习。以下是转载全文。导读：关于Alfa Go的评论文章很多，但真正能够与开发团队交流的却不多，感谢Alfa Go开发团队DeepMind的朋友对我这篇文章内容的关注与探讨，指出我在之前那一版文章中用字上的不够精确，所以在此又作调整。我之前文章提到的「全局」指的是跨时间点的整场赛局，很容易被误认为是某个特定时点整个棋盘的棋局，所以后面全部都修改为「整体棋局」。此外，关于整体棋局评估，除了透过脱机数据学习的评价网络之外，还可以透过根据目前状态实时计算的不同策略评价差异(这项技术称之为Rollouts)，它透过将计算结果进行快取，也能做到局部考虑整体棋局的效果。再次感谢DeepMind朋友的斧正。在人类连输AlphaGo三局后的今天，正好是一个好时机，可以让大家对于AlphaGo所涉及的深度学习技术能够有更多的理解(而不是想象复仇者联盟中奥创将到来的恐慌)。在说明Alpha Go的深度学习技术之前，我先用几个简单的事实总结来厘清大家最常误解的问题：●AlphaGo这次使用的技术本质上与深蓝截然不同，不再是使用暴力解题法来赢过人类。●没错，AlphaGo是透过深度学习能够掌握更抽象的概念，但是计算机还是没有自我意识与思考。●AlphaGo并没有理解围棋的美学与策略，他只不过是找出了2个美丽且强大的函数来决定他的落子。什么是类神经网络?其实类神经网络是很古老的技术了，在1943年，Warren McCulloch以及Walter Pitts首次提出神经元的数学模型，之后到了1958年，心理学家Rosenblatt提出了感知器(Perceptron)的概念，在前者神经元的结构中加入了训练修正参数的机制(也是我们俗称的学习)，这时类神经网络的基本学理架构算是完成。类神经网络的神经元其实是从前端收集到各种讯号(类似神经的树突)，然后将各个讯号根据权重加权后加总，然后透过活化函数转换成新讯号传送出去(类似神经元的轴突)。至于类神经网络则是将神经元串接起来，我们可以区分为输入层(表示输入变量)，输出层(表示要预测的变量)，而中间的隐藏层是用来增加神经元的复杂度，以便让它能够仿真更复杂的函数转换结构。每个神经元之间都有连结，其中都各自拥有权重，来处理讯号的加权。传统的类神经网络技术，就是透过随机指派权重，然后透过递归计算的方式，根据输入的训练数据，逐一修正权重，来让整体的错误率可以降到最低。随着倒传导网络、无监督式学习等技术的发展，那时一度类神经网络蔚为显学，不过人类很快就遇到了困难，那就是计算能力的不足。因为当隐藏层只有一层的时候，其实大多数的状况，类神经网络的分类预测效果其实并不会比传统统计的罗吉斯回归差太多，但是却要耗费更庞大的计算能力，但是随着隐藏层神经元的增加，或者是隐藏层的增加，那么所需要计算权重数量就会严重暴增。所以到了八十年代后期，整个类神经网络的研究就进入了寒冬，各位可能只能在洗衣机里体会到它小小威力(现在洗衣机里根据倒入衣物评估水量与运行时间很多都是用类神经网络作的)，说真的，类神经网络一点都没有被认为强大。这个寒冬一直持续到2006年，在Hinton以及Lecun小组提出了「A fast learningalgorithm for deep belief nets」论文之后，终于有了复苏的希望，它们提出的观点是如果类神经网络神经元权重不是以随机方式指派，那么应该可以大幅缩短神经网络的计算时间，它们提出的方法是利用神经网络的非监督式学习来做为神经网络初始权重的指派，那时由于各家的论文期刊只要看到类神经网络字眼基本上就视为垃圾不刊登，所以他们才提出深度学习这个新的字眼突围。除了Hinton的努力之外，得力于摩尔定律的效应，我们可以用有更快的计算能力，Hinton后来在2010年使用了这套方法搭配GPU的计算，让语音识别的计算速度提升了70倍以上。深度学习的新一波高潮来自于2012年，那年的ImageNet大赛(有120万张照片作为训练组，5万张当测试组，要进行1000个类别分组)深度学习首次参赛，把过去好几年只有微幅变动的错误率，一下由26%降低到15%。而同年微软团队发布的论文中显示，他们透过深度学习将ImageNet 2012数据集的错误率降到了4.94%，比人类的错误率5.1%还低。而去年(2015年)微软再度拿下ImageNet 2015冠军，此时错误率已经降到了3.57%的超低水平，而微软用的是152层深度学习网络(我当初看到这个数字，吓都吓死了)….卷积神经网络(Convolutional Neural Network)在图像识别的问题上，我们处理的是一个二维的神经网络结构，以100*100像素的图片来说，其实输入数据就是这10000像素的向量(这还是指灰阶图片，如果是彩色则是30000)，那如果隐藏层的神经元与输入层相当，我们等于要计算10的8次方的权重，这个数量想到就头疼，即使是透过并行计算或者是分布式计算都恐怕很难达成。因此卷积神经网络提出了两个很重要的观点：1.局部感知域：从人类的角度来看，当我们视觉聚焦在图片的某个角落时，距离较远的像素应该是不会影响到我们视觉的，因此局部感知域的概念就是，像素指需要与邻近的像素产生连结，如此一来，我们要计算的神经连结数量就能够大幅降低。举例来说，一个神经元指需要与邻近的10*10的像素发生连结，那么我们的计算就可以从10的8次方降低至100*100*(10*10)=10的6次方了。2.权重共享：但是10的6次方还是很多，所以这时要引入第二个观念就是权重共享。因为人类的视觉并不会去认像素在图片上的绝对位置，当图片发生了平移或者是位置的变化，我们都还是可以理解这个图片，这表示我从一个局部所训练出来的权重(例如10*10的卷积核)应该是可以适用于照片的各个位置的。也就是说在这个10*10范围所学习到的特征可以变成一个筛选器，套用到整个图片的范围。而权重共享造成这10*10的卷积核内就共享了相同的权重。一个卷积核可以理解为一个特征，所以神经网络中可以设计多个卷积核来提取更多的特征。下图是一个3*3的卷积核在5*5的照片中提取特征的示意图。卷积层找出了特征后，就可以做为输入变量到一般的类神经网络进行分类模型的训练。不过当网络结构越来越复杂，样本数如果不是极为庞大，很容易会发生过度学习的问题(over-fitting，神经网络记忆的建模数据的结构，而非找到规则)。因此我们后来引入池化 (pooling)或是局部取样(subsampling)的概念，就是在卷积核中再透过n*n的小区域进行汇总，来凸显这个区域的最显著特征，以避免过度学习的问题。所以常见的图像识别技术(例如ImageNet)就是透过多阶段的卷积层+池化层的组合，最后在接入一般的类神经网络架构来进行分类预测。下图是一个图像识别的范例。其中的C2、C4、C6都是卷积层，而S3与S5则是池化层。卷积神经网络建构了一个透过二维矩阵来解决抽象问题的神经网络技术。而图像识别不再需要像过去一样透过人工先找出图像特征给神经网络学习，而是透过卷积网络结构，它们可以自己从数据中找出特征，而且卷积层越多，能够辨识的特征就越高阶越抽象。所以你要训练神经网络从照片中辨识猫或狗，你不再需要自己找出猫或狗的特征注记，而是只要把大量的猫或狗的照片交给神经网络，它自己会找出猫或狗的抽象定义。讲到这里有没有发现卷积神经网络作图像识别与围棋有甚么相似性?没错，围棋是一个19*19的方阵，而围棋也是一个规则不像象棋或西洋棋般的明确，而且具备了很高的需要透过直觉才能判断落子的特性。这个时候，深度学习就能发挥极佳的作用，因为程序设计师不需要自己把围棋的游戏规则输入给计算机，它可以透过大量的棋谱自己找出对应的逻辑与抽象概念。为什么围棋比较困难?为什么深蓝可以在西洋棋赢过人类但是却无法赢围棋，这是因为深蓝透过强大的计算能力，将未来局势的树状架构，推导出后面胜负的可能性。但是各位要知道，以西洋棋或中国象棋来说，它的分支因子大概是40左右，这表示预测之后20步的动作需要计算40的20次方(这是多大，就算是1GHz的处理器，也要计算3486528500050735年，请注意，这还是比较简单的西洋棋)，所以他利用了像是MinMax搜索算法以及Alpha-Beta修剪法来缩减可能的计算范围，基本上是根据上层的胜率，可能胜的部分多算几层、输的少算，无关胜负不算，利用暴力解题法来找出最佳策略。但是很不幸的是，围棋的分支因子是250，以围棋19*19的方阵，共有361个落子点，所以整个围棋棋局的总排列组合数高达10的171次方，有不少报导说这比全宇宙的原子数还多，这是采用了之前的一个古老的研究说全宇宙原子数是10的75次方，不过我对此只是笑笑，我觉得这也是低估了宇宙之大吧。AlphaGo的主要机制在架构上，AlphaGo可以说是拥有两个大脑，两个神经网络结构几乎相同的两个独立网络：策略网络与评价网络，这两个网络基本上是个13层的卷积神经网络所构成，卷积核大小为5*5，所以基本上与存取固定长宽像素的图像识别神经网络一样，只不过我们将矩阵的输入值换成了棋盘上各个坐标点的落子状况。第一个大脑「策略网络」基本上就是一个单纯的监督式学习，用来判断对手最可能的落子位置。他的做法是大量的输入这个世界上职业棋手的棋谱，用来预测对手最有可能的落子位置。在这个网络中，完全不用去思考「赢」这件事，只需要能够预测对手的落子即可。目前AlphaGo预测对手落子位置的正确率是57%(这是刊登在Nature文章时的数据，现在想必更高了)。那各位可能认为AlphaGo的弱点是否应该就在策略网络，一方面是预测准确率不高，再者是如果下了之前他没看过的棋局是不是就有机会可以赢过他。可惜并不是，因为AlphaGo的策略网络有做了两个层面增强，第一个层面是利用了名为增强策略网络(reinforced-learning (RL) policynetwork)的技术，他先使用部分样本训练出一个基础版本的策略网络，以及使用完整样本建立出来的进阶版策略网络，然后让两个网络对弈，后者进阶版策略网络等于是站在基础版前的「高手」，因此可以让基础网络可以快速的熟即到高手可能落子的位置数据，进而又产生一个增强版，这个增强版又变成原有进阶版的「高手」，以此循环修正，就可以不断的提升对于对手(高手)落子的预测。第二个层面则是现在的策略网络不再需要在19*19的方格中找出最可能落子位置，改良过的策略网络可以先透过卷积核排除掉一些区域不去进行计算，然后再根据剩余区域找出最可能位置，虽然这可能降低AlphaGo策略网络的威力，但是这种机制却能让AlphaGo计算速度提升1000倍以上。也正因为Alpha Go一直是根据整体局势来猜测对手的可能落子选择，也因此人类耍的小心机像是刻意下几步希望扰乱计算机的落子位置，其实都是没有意义的。第二个大脑是评价网络。在评价网络中则是关注在目前局势的状况下，每个落子位置的「最后」胜率(这也是我所谓的整体棋局)，而非是短期的攻城略地。也就是说策略网络是分类问题(对方会下在哪)，评价网络是评估问题(我下在这的胜率是多少)。评价网络并不是一个精确解的评价机制，因为如果要算出精确解可能会耗费极大量的计算能力，因此它只是一个近似解的网络，而且透过卷积神经网络的方式来计算出卷积核范围的平均胜率(这个做法的目的主要是要将评价函数平滑化，同时避免过度学习的问题)，最终答案他会留到最后的蒙利卡罗搜索树中解决。当然，这里提到的胜率会跟向下预测的步数会有关，向下预测的步数越多，计算就越庞大，AlphaGo目前有能力自己判断需要展开的预测步数。但是如何能确保过去的样本能够正确反映胜率，而且不受到对弈双方实力的事前判断(可能下在某处会赢不是因为下在这该赢，而是这个人比较厉害)，因此。这个部分它们是透过两台AlphaGo对弈的方式来解决，因为两台AlphaGo的实力可以当作是相同的，那么最后的输赢一定跟原来的两人实力无关，而是跟下的位置有关。也因此评价网络并不是透过这世界上已知的棋谱作为训练，因为人类对奕会受到双方实力的影响，透过两台对一的方式，他在与欧洲棋王对弈时，所使用的训练组样本只有3000万个棋谱，但是在与李世石比赛时却已经增加到1亿。由于人类对奕动则数小时，但是AlphaGo间对奕可能就一秒完成数局，这种方式可以快速地累积出正确的评价样本。所以先前提到机器下围棋最大困难点评价机制的部分就是这样透过卷积神经网络来解决掉。AlphaGo技术的最后环节就是蒙地卡罗搜索树，相较于以前深蓝所使用的搜索(搭配MinMax搜索算法以及Alpha-Beta修剪法，这里就不再赘述)，由于我们并非具有无限大的计算能力(请注意，如果是有限的排列组合，蒙地卡罗搜索树的确有可能针对所有组合进行通盘评估，但是在围棋的场景下是没有办法的，就算这样做，恐怕也会造成计算时间的大幅增加)，因此不可能是适用于旧的方法，不过在前面策略网络以及评价网络中，AlphaGo已经可以针对接下来的落子(包括对方)可能性缩小到一个可控的范围，接下来他就可以快速地运用蒙地卡罗搜索树来有限的组合中计算最佳解。一般来说蒙地卡罗搜索树包括4个步骤：1.选取：首先根据目前的状态，选择几种可能的对手落子模式。2.展开：根据对手的落子，展开至我们胜率最大的落子模式(我们称之为一阶蒙地卡罗树)。所以在AlphaGo的搜索树中并不会真的展开所有组合。3.评估：如何评估最佳行动(AlphaGo该下在哪?)，一种方式是将行动后的棋局丢到评价网络来评估胜率，第二种方式则是做更深度的蒙地卡罗树(多预测几阶可能的结果)。这两种方法所评估的结果可能截然不同，AlphaGo使用了混合系数(mixing coefficient)来将两种评估结果整合，目前在Nature刊出的混合系数是50%-50%(但是我猜实际一定不是)4.倒传导：在决定我们最佳行动位置后，很快地根据这个位置向下透过策略网络评估对手可能的下一步，以及对应的搜索评估。所以AlphaGo其实最恐怖的是，李世石在思考自己该下哪里的时候，不但AlphaGo可能早就猜出了他可能下的位置，而且正利用他在思考的时间继续向下计算后面的棋路。根据AlphaGo团队的实测，如果单独使用一个大脑或是蒙利卡罗搜索树技术，都能达到业余(段)的等级(欧洲棋王樊摩强度等级大概是在2500~2600，而李世石是在3500以上)。但是当这些技术整合就能呈现更强大的力量。但是在刊登Nature论文时他的预估强度大概也只有职业3~4段(李世石是9段)，不过刚刚提到他透过增强技术强化策略网络、透过两台AlphaGo来优化评价网络，这都可以让他可以在短时间变得更加强大。而且计算机没有情感也不怕压力，更不会因为对手表现而轻敌(AlphaGo的策略网络一向只预测强者)，所以人类就算有更强大的实力也未必能够承受输赢压力而做最好的发挥。李世石有没有赢的机会?在很多评论中，我觉得对于AlphaGo都有很多不正确的猜测，首先是AlphaGo有没有「整体棋局」评估的能力，必须说的是以整台AlphaGo来说是有的，这主要是来自于评价网络的计算结果(因为它计算的是最后胜率)，但是获得的是个池化区域的平滑化后平均胜率。在AlphaGo的策略网络主要是针对对手接下来的落子进行评估，至于蒙地卡罗搜索树则是使用了评价网络的参数(脱机训练的结果)以及根据目前状态实时计算价值差异的Rollouts技术，所以可以做出具有整体棋局考虑的模拟试算。但是人类对于「整体棋局」的掌控是透过直觉，这一点应该还是比计算机强大，而且如果利用目前AlphaGo是透过卷积核池化过后结果评估平均胜率(主要是为了平滑化以及避免过度学习)，如果李世石有办法利用AlphaGo会预测他的行为做后面决策，作出陷阱，来制造胜率评估的误区(在池化范围内平均是高胜率，但是某个位子下错就造成「整体棋局」翻覆的状况，这就是胜率预测的误区)，那么人类就有可能获胜(当然啦，我这里只是提出可能性，但是知易行难，这样的行动的实际执行可能性是偏低的)。现在李世石必输的原因在于它一直在猜测AlphaGo的棋路，但是事实上反而是AlphaGo一直在靠猜测李世石的下一步来做决策，所以他应该改变思路，透过自己的假动作来诱骗AlphaGo，这才有可能有胜利的可能性。弱人工智能与强人工智能现在计算机在围棋这个号称人类最后的堡垒中胜过了人类，那我们是不是要担心人工智能统治人类的一天到来，其实不必杞人忧天，因为在人工智能的分类上来说，区分为弱人工智能(ArtificialNarrow Intelligence)与强人工智能(Artificial General Intelligence)(事实上还有人提出高人工智能Artificial Super Intelligence，认为是比人类智力更强大，具备创造创新与社交技能的人工智能，但我觉得这太科幻了，不再讨论范围内)，其中最大的差别在于弱人工智能不具备自我意识、不具备理解问题、也不具备思考、计划解决问题的能力。各位可能要质疑AlphaGo如果不能理解围棋他是如何可以下的那么好?请注意，AlphaGo本质上就是一个深度学习的神经网络，他只是透过网络架构与大量样本找到了可以预测对手落子(策略网络)、计算胜率(评价网络)以及根据有限选项中计算最佳解的蒙地卡罗搜索树，也就是说，他是根据这三个函数来找出最佳动作，而不是真的理解了什么是围棋。所以AlphaGo在本质上与微软的Cortana或iPhone的Siri其实差别只是专精在下围棋罢了，并没有多出什么思考机制。我也看到一些报导乱说AlphaGo是个通用性的网络，所以之后叫他学打魔兽或是学医都能够快速上手，那这也是很大的谬误，如果各位看完了上面的说明，就会知道AlphaGo根本就是为了下围棋所设计出来的人工智能，如果要拿它来解决其他问题，势必神经结构以及算法都必须要重新设计。所以李世石与其说是输给了AlphaGo，还不如说是输给了数学，证明其实直觉还是不如数学的理性判断。有人觉得人类输掉了最后的堡垒，围棋这项艺术也要毁灭了…其实各位真的不用太担心。人类跑不过汽车的时候为何没有那么恐慌呢?跑步这项运动到现在也好好的，奥运金牌也不是都被法拉利拿走了…所以真的不必太过紧张。那么会有强人工智能出现的一天吗?在2013年Bostrom对全球数百位最前沿的人工智能专家做了问卷，问了到底他们预期强人工智能什么时候会出现，他根据问卷结果推导出了三个答案：乐观估计(有10%的问卷中位数)是2022年，正常估计(50%的问卷中位数)是2040年，悲观估计(90%的问卷中位数)是2075年。所以离我们还久的呢。不过当弱人工智能的发展进入到成本降低可商业化的时候，大家与其关心人工智能会不会统治地球，还不如先关心自己的工作技能会不会被计算机取代来实际些吧。"}
{"content2":"转自：  http://www.cnbruce.com/blog/showlog.asp?cat_id=37&log_id=1422在经历了蛮荒的PC互联网时代，混战的移动互联网时代，到现今最火的人工智能时代。大数据、云计算、机器学习的技术应用，已经使得IT从业者的门槛越来越高。套用一句樊登读书会的宣传口号“keep learning”，保持对新鲜技术的好奇心，保持对技术应用的责任心，持续关注、学习是每个IT从业者的必备技能。一、什么是人工智能？人工智能（Artificial Intelligence），英文缩写为AI。它是一个融合计算机科学、统计学、脑神经学和社会科学的前沿综合学科。它使得计算机像人一样拥有智能能力，可以代替人类实现识别、认知，分析和决策等多种功能。比如当你说一句话时，机器能够识别成文字，并理解你话的意思，进行分析和对话等。二、人工智能发展简史1956年，几个计算机科学家相聚在达特茅斯会议（Dartmouth Conferences），提出了“人工智能”的概念。其后，人工智能就一直萦绕于人们的脑海之中，并在科研实验室中慢慢孵化。之后的几十年，人工智能一直在两极反转，或被称作人类文明耀眼未来的预言；或者被当成技术疯子的狂想扔到垃圾堆里。坦白说，直到2012年之前，这两种声音还在同时存在。上世纪90年代，国际象棋冠军卡斯帕罗夫与\"深蓝\" 计算机决战，\"深蓝\"获胜，这是人工智能发展的一个重要里程碑。而 2016 年，Google 的 AlphaGo 赢了韩国棋手李世石，再度引发 AI 热潮。过去几年，尤其是2015年以来，人工智能开始大爆发。很大一部分是由于GPU的广泛应用，使得并行计算变得更快、更便宜、更有效。当然，无限拓展的存储能力和骤然爆发的数据洪流（大数据）的组合拳，也使得图像数据、文本数据、交易数据、映射数据全面海量爆发。三、人工智能发展条件1、硬件发展：AI 不断爆发热潮，是与基础设施的进步和科技的更新分不开的，从 70 年代 personal 计算机的兴起到 2010 年 GPU、异构计算等硬件设施的发展，都为人工智能复兴奠定了基础。2、数据发展：互联网及移动互联网的发展也带来了一系列数据能力，使人工智能能力得以提高。3、运算发展：计算机的运算能力从传统的以 CPU 为主导到以 GPU 为主导，这对 AI 有很大变革。4、算法发展：算法技术的更新助力于人工智能的兴起，最早期的算法一般是传统的统计算法，如 80 年代的神经网络，90 年代的浅层，2000 年左右的 SBM、Boosting、convex 的 methods 等等。随着数据量增大，计算能力变强，深度学习的影响也越来越大。尤其是2011 年之后，深度学习的兴起，带动了现今人工智能发展的高潮。四、机器学习：一种实现人工智能的方法机器学习(Machine Learning, ML)是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。机器学习是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演绎。机器学习最基本的做法，是使用算法来解析数据、从中学习，然后对真实世界中的事件做出决策和预测。与传统的为解决特定任务、硬编码的软件程序不同，机器学习是用大量的数据来“训练”，通过各种算法从数据中学习如何完成任务。机器学习最成功的应用领域是计算机视觉，虽然也还是需要大量的手工编码来完成工作。人们需要手工编写分类器、边缘检测滤波器，以便让程序能识别物体从哪里开始，到哪里结束；写形状检测程序来判断检测对象是不是有八条边；写分类器来识别字母“STOP”。使用以上这些手工编写的分类器，人们总算可以开发算法来感知图像，判断图像是不是一个停止标志牌。【机器学习有三类】：第一类是无监督学习，指的是从信息出发自动寻找规律，并将其分成各种类别，有时也称\"聚类问题\"。第二类是监督学习，监督学习指的是给历史一个标签，运用模型预测结果。如有一个水果，我们根据水果的形状和颜色去判断到底是香蕉还是苹果，这就是一个监督学习的例子。最后一类为强化学习，是指可以用来支持人们去做决策和规划的一个学习方式，它是对人的一些动作、行为产生奖励的回馈机制，通过这个回馈机制促进学习，这与人类的学习相似，所以强化学习是目前研究的重要方向之一。五、深度学习：一种实现机器学习的技术值得一提的是机器学习同深度学习之间还是有所区别的，机器学习是指计算机的算法能够像人一样，从数据中找到信息，从而学习一些规律。虽然深度学习是机器学习的一种，但深度学习是利用深度的神经网络，将模型处理得更为复杂，从而使模型对数据的理解更加深入。深度学习是机器学习中一种基于对数据进行表征学习的方法。深度学习是机器学习研究中的一个新的领域，其动机在于建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，例如图像，声音和文本。同机器学习方法一样，深度机器学习方法也有监督学习与无监督学习之分．不同的学习框架下建立的学习模型很是不同．例如，卷积神经网络（Convolutional neural networks，简称CNNs）就是一种深度的监督学习下的机器学习模型，而深度置信网（Deep Belief Nets，简称DBNs）就是一种无监督学习下的机器学习模型。如上图，人工智能是最早出现的，也是最大、最外侧的同心圆；其次是机器学习，稍晚一点；最内侧，是深度学习，当今人工智能大爆炸的核心驱动。六、人工神经网络：一种机器学习的算法人工神经网络（Artificial Neural Networks）是早期机器学习中的一个重要的算法，历经数十年风风雨雨。神经网络的原理是受我们大脑的生理结构——互相交叉相连的神经元启发。但与大脑中一个神经元可以连接一定距离内的任意神经元不同，人工神经网络具有离散的层、连接和数据传播的方向。例如，我们可以把一幅图像切分成图像块，输入到神经网络的第一层。在第一层的每一个神经元都把数据传递到第二层。第二层的神经元也是完成类似的工作，把数据传递到第三层，以此类推，直到最后一层，然后生成结果。每一个神经元都为它的输入分配权重，这个权重的正确与否与其执行的任务直接相关。最终的输出由这些权重加总来决定。我们以“停止（Stop）标志牌”为例，将一个停止标志牌图像的所有元素都打碎，然后用神经元进行“检查”：八边形的外形、消防车般的红颜色、鲜明突出的字母、交通标志的典型尺寸和静止不动运动特性等等。神经网络的任务就是给出结论，它到底是不是一个停止标志牌。神经网络会根据所有权重，给出一个经过深思熟虑的猜测——“概率向量”。回过头来看这个停止标志识别的例子。神经网络是调制、训练出来的，时不时还是很容易出错的。它最需要的，就是训练。需要成百上千甚至几百万张图像来训练，直到神经元的输入的权值都被调制得十分精确，无论是否有雾，晴天还是雨天，每次都能得到正确的结果。只有这个时候，我们才可以说神经网络成功地自学习到一个停止标志的样子；或者在Facebook的应用里，神经网络自学习了你妈妈的脸；又或者是2012年吴恩达（Andrew Ng）教授在Google实现了神经网络学习到猫的样子等等。吴教授的突破在于，把这些神经网络从基础上显著地增大了。层数非常多，神经元也非常多，然后给系统输入海量的数据，来训练网络。在吴教授这里，数据是一千万YouTube视频中的图像。吴教授为深度学习（deep learning）加入了“深度”（deep）。这里的“深度”就是说神经网络中众多的层。现在，经过深度学习训练的图像识别，在一些场景中甚至可以比人做得更好：从识别猫，到辨别血液中癌症的早期成分，到识别核磁共振成像中的肿瘤。Google的AlphaGo先是学会了如何下围棋，然后与它自己下棋训练。它训练自己神经网络的方法，就是不断地与自己下棋，反复地下，永不停歇。七、人工智能的研究领域和分支人工智能研究的领域主要有五层：1、最底层是基础设施建设，包含数据和计算能力两部分，数据越大，人工智能的能力越强。2、往上一层为算法，如卷积神经网络、LSTM 序列学习、Q-Learning、深度学习等算法，都是机器学习的算法。3、第三层为重要的技术方向和问题，如计算机视觉，语音工程，自然语言处理等。还有另外的一些类似决策系统，像 reinforcement learning（编辑注：增强学习），或像一些大数据分析的统计系统，这些都能在机器学习算法上产生。4、第四层为具体的技术，如图像识别、语音识别、机器翻译等等。5、最顶端为行业的解决方案，如人工智能在金融、医疗、互联网、交通和游戏等上的应用，这是我们所关心它能带来的价值。八、人工智能的应用场景1、计算机视觉2000年左右，人们开始用机器学习，用人工特征来做比较好的计算机视觉系统。如车牌识别、安防、人脸等技术。而深度学习则逐渐运用机器代替人工来学习特征，扩大了其应用场景，如无人车、电商等领域。2、语音技术2010 年后，深度学习的广泛应用使语音识别的准确率大幅提升，像 Siri、Voice Search 和 Echo 等，可以实现不同语言间的交流，从语音中说一段话，随之将其翻译为另一种文字；再如智能助手，你可以对手机说一段话，它能帮助你完成一些任务。与图像相比，自然语言更难、更复杂，不仅需要认知，还需要理解。3、自然语言处理目前一个比较重大的突破是机器翻译，这大大提高了原来的机器翻译水平，举个例子，Google 的 Translation 系统，是人工智能的一个标杆性的事件。2010 年左右， IBM 的\"Watson\"系统在一档综艺节目上，和人类冠军进行自然语言的问答并获胜，代表了计算机能力的显著提高。4、决策系统决策系统的发展是随着棋类问题的解决而不断提升，从 80 年代西洋跳棋开始，到 90 年代的国际象棋对弈，机器的胜利都标志了科技的进步，决策系统可以在自动化、量化投资等系统上广泛应用。5、大数据应用可以通过你之前看到的文章，理解你所喜欢的内容而进行更精准的推荐；分析各个股票的行情，进行量化交易；分析所有的像客户的一些喜好而进行精准的营销等。机器通过一系列的数据进行判别，找出最适合的一些策略而反馈给我们。九、人工智能的未来之路1、在计算机视觉上，未来的人工智能应更加注重效果的优化，加强计算机视觉在不同场景、问题上的应用。2、在语音场景下，当前的语音识别虽然在特定的场景(安静的环境）下，已经能够得到和人类相似的水平。但在噪音情景下仍有挑战，如原场识别、口语、方言等长尾内容。未来需增强计算能力、提高数据量和提升算法等来解决这个问题。3、在自然语言处理中，机器的优势在于拥有更多的记忆能力，但却欠缺语意理解能力，包括对口语不规范的用语识别和认知等。人说话时，是与物理事件学相联系的，比如一个人说电脑，人知道这个电脑意味着什么，或者它是能够干些什么，而在自然语言里，它仅仅将\"电脑\"作为一个孤立的词，不会去产生类似的联想，自然语言的联想只是通过在文本上和其他所共现的一些词的联想， 并不是物理事件里的联想。所以如果要真的解决自然语言的问题，将来需要去建立从文本到物理事件的一个映射，但目前仍没有很好的解决方法。因此，这是未来着重考虑的一个研究方向。4、当下的决策规划系统存在两个问题，第一是不通用，即学习知识的不可迁移性，如用一个方法学了下围棋，不能直接将该方法转移到下象棋中，第二是大量模拟数据。所以它有两个目标，一个是算法的提升，如何解决数据稀少或怎么自动能够产生模拟数据的问题，另一个是自适应能力，当数据产生变化的时候，它能够去适应变化，而不是能力有所下降。所有一系列这些问题，都是下一个五或十年我们希望很快解决的。参考：腾讯 AI Lab 张潼主任带你轻松 get AI 新知识https://www.qcloud.com/community/article/666041人工智能、机器学习和深度学习之间的区别和联系https://www.cnblogs.com/dadadechengzi/articles/6575767.html百度百科：人工智能https://baike.baidu.com/item/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/9180百度百科：机器学习https://baike.baidu.com/item/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0百度百科：深度学习https://baike.baidu.com/item/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/3729729百度百科：人工神经网络https://baike.baidu.com/item/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"}
{"content2":"和机器学习和计算机视觉相关的数学和机器学习和计算机视觉相关的数学之一（以下转自一位MIT牛人的空间文章，写得很实际：）作者：Dahua感觉数学似乎总是不够的。这些日子为了解决research中的一些问题，又在图书馆捧起了数学的教科书。从大学到现在，课堂上学的和自学的数学其实不算少了，可是在研究的过程中总是发现需要补充新的数学知识。Learning和Vision都是很多种数学的交汇场。看着不同的理论体系的交汇，对于一个researcher来说，往往是非常exciting的enjoyable的事情。不过，这也代表着要充分了解这个领域并且取得有意义的进展是很艰苦的。记得在两年前的一次blog里面，提到过和learning有关的数学。今天看来，我对于数学在这个领域的作用有了新的思考。对于Learning的研究，1、Linear Algebra (线性代数) 和 Statistics (统计学) 是最重要和不可缺少的。这代表了Machine Learning中最主流的两大类方法的基础。一种是以研究函数和变换为重点的代数方法，比如Dimension reduction，feature extraction，Kernel等，一种是以研究统计模型和样本分布为重点的统计方法，比如Graphical model, Information theoretical models等。它们侧重虽有不同，但是常常是共同使用的，对于代数方法，往往需要统计上的解释，对于统计模型，其具体计算则需要代数的帮助。以代数和统计为出发点，继续往深处走，我们会发现需要更多的数学。2、Calculus (微积分)，只是数学分析体系的基础。其基础性作用不言而喻。Learning研究的大部分问题是在连续的度量空间进行的，无论代数还是统计，在研究优化问题的时候，对一个映射的微分或者梯度的分析总是不可避免。而在统计学中，Marginalization和积分更是密不可分——不过，以解析形式把积分导出来的情况则不多见。3、Partial Differential Equation （偏微分方程)这主要用于描述动态过程，或者仿动态过程。这个学科在Vision中用得比Learning多，主要用于描述连续场的运动或者扩散过程。比如Level set, Optical flow都是这方面的典型例子。4、Functional Analysis (泛函分析)通俗地，可以理解为微积分从有限维空间到无限维空间的拓展——当然了，它实际上远不止于此。在这个地方，函数以及其所作用的对象之间存在的对偶关系扮演了非常重要的角色。Learning发展至今，也在向无限维延伸——从研究有限维向量的问题到以无限维的函数为研究对象。Kernel Learning 和 Gaussian Process 是其中典型的例子——其中的核心概念都是Kernel。很多做Learning的人把Kernel简单理解为Kernel trick的运用，这就把kernel的意义严重弱化了。在泛函里面，Kernel (Inner Product)是建立整个博大的代数体系的根本，从metric, transform到spectrum都根源于此。5、Measure Theory (测度理论)这是和实分析关系非常密切的学科。但是测度理论并不限于此。从某种意义上说，Real Analysis可以从Lebesgue Measure（勒贝格测度）推演，不过其实还有很多别的测度体系——概率本身就是一种测度。测度理论对于Learning的意义是根本的，现代统计学整个就是建立在测度理论的基础之上——虽然初级的概率论教科书一般不这样引入。在看一些统计方面的文章的时候，你可能会发现，它们会把统计的公式改用测度来表达，这样做有两个好处：所有的推导和结论不用分别给连续分布和离散分布各自写一遍了，这两种东西都可以用同一的测度形式表达：连续分布的积分基于Lebesgue测度，离散分布的求和基于计数测度，而且还能推广到那种既不连续又不离散的分布中去（这种东西不是数学家的游戏，而是已经在实用的东西，在Dirchlet Process或者Pitman-Yor Process里面会经常看到)。而且，即使是连续积分，如果不是在欧氏空间进行，而是在更一般的拓扑空间（比如微分流形或者变换群），那么传统的黎曼积分（就是大学一年级在微积分课学的那种）就不work了，你可能需要它们的一些推广，比如Haar Measure或者Lebesgue-Stieltjes积分。6、Topology（拓扑学)这是学术中很基础的学科。它一般不直接提供方法，但是它的很多概念和定理是其它数学分支的基石。看很多别的数学的时候，你会经常接触这样一些概念：Open set / Closed set，set basis，Hausdauf, continuous function，metric space, Cauchy sequence, neighborhood, compactness, connectivity。很多这些也许在大学一年级就学习过一些，当时是基于极限的概念获得的。如果，看过拓扑学之后，对这些概念的认识会有根本性的拓展。比如，连续函数，当时是由epison法定义的，就是无论取多小的正数epsilon，都存在xxx，使得xxx。这是需要一种metric去度量距离的，在general topology里面，对于连续函数的定义连坐标和距离都不需要——如果一个映射使得开集的原像是开集，它就是连续的——至于开集是基于集合论定义的，不是通常的开区间的意思。这只是最简单的例子。当然，我们研究learning也许不需要深究这些数学概念背后的公理体系，但是，打破原来定义的概念的局限在很多问题上是必须的——尤其是当你研究的东西它不是在欧氏空间里面的时候——正交矩阵，变换群，流形，概率分布的空间，都属于此。7、Differential Manifold (微分流形)通俗地说它研究的是平滑的曲面。一个直接的印象是它是不是可以用来fitting一个surface什么的——当然这算是一种应用，但是这是非常初步的。本质上说，微分流形研究的是平滑的拓扑结构。一个空间构成微分流形的基本要素是局部平滑：从拓扑学来理解，就是它的任意局部都同胚于欧氏空间，从解析的角度来看，就是相容的局部坐标系统。当然，在全局上，它不要求和欧氏空间同胚。它除了可以用于刻画集合上的平滑曲面外，更重要的意义在于，它可以用于研究很多重要的集合。一个n-维线性空间的全部k-维子空间(k8、Lie Group Theory (李群论)一般意义的群论在Learning中被运用的不是很多，群论在Learning中用得较多的是它的一个重要方向Lie group。定义在平滑流形上的群，并且其群运算是平滑的话，那么这就叫李群。因为Learning和编码不同，更多关注的是连续空间，因为Lie group在各种群中对于Learning特别重要。各种子空间，线性变换，非奇异矩阵都基于通常意义的矩阵乘法构成李群。在李群中的映射，变换，度量，划分等等都对于Learning中代数方法的研究有重要指导意义。9、Graph Theory（图论)图，由于它在表述各种关系的强大能力以及优雅的理论，高效的算法，越来越受到Learning领域的欢迎。经典图论，在Learning中的一个最重要应用就是graphical models了，它被成功运用于分析统计网络的结构和规划统计推断和机器学习和计算机视觉相关的数学之二转自：http://blog.sina.com.cn/s/blog_6833a4df0100nazk.html1. 线性代数 (Linear Algebra)：我想国内的大学生都会学过这门课程，但是，未必每一位老师都能贯彻它的精要。这门学科对于Learning是必备的基础，对它的透彻掌握是必不可少的。我在科大一年级的时候就学习了这门课，后来到了香港后，又重新把线性代数读了一遍，所读的是Introduction to Linear Algebra (3rd Ed.)  by Gilbert Strang.这本书是MIT的线性代数课使用的教材，也是被很多其它大学选用的经典教材。它的难度适中，讲解清晰，重要的是对许多核心的概念讨论得比较透彻。我个人觉得，学习线性代数，最重要的不是去熟练矩阵运算和解方程的方法——这些在实际工作中MATLAB可以代劳，关键的是要深入理解几个基础而又重要的概念：子空间(Subspace)，正交(Orthogonality)，特征值和特征向量(Eigenvalues and eigenvectors)，和线性变换(Linear transform)。从我的角度看来，一本线代教科书的质量，就在于它能否给这些根本概念以足够的重视，能否把它们的联系讲清楚。Strang的这本书在这方面是做得很好的。而且，这本书有个得天独厚的优势。书的作者长期在MIT讲授线性代数课(18.06)，课程的video在MIT的Open courseware网站上有提供。有时间的朋友可以一边看着名师授课的录像，一边对照课本学习或者复习。http://ocw.mit.edu/OcwWeb/Mathematics/18-06Spring-2005/CourseHome/index.htm2. 概率和统计 (Probability and Statistics):概率论和统计的入门教科书很多，我目前也没有特别的推荐。我在这里想介绍的是一本关于多元统计的基础教科书：Applied Multivariate Statistical Analysis (5th Ed.)  by Richard A. Johnson and Dean W. Wichern这本书是我在刚接触向量统计的时候用于学习的，我在香港时做研究的基础就是从此打下了。实验室的一些同学也借用这本书学习向量统计。这本书没有特别追求数学上的深度，而是以通俗易懂的方式讲述主要的基本概念，读起来很舒服，内容也很实用。对于Linear regression, factor analysis, principal component analysis (PCA), and canonical component analysis (CCA)这些Learning中的基本方法也展开了初步的论述。之后就可以进一步深入学习贝叶斯统计和Graphical models。一本理想的书是Introduction to Graphical Models (draft version).  by M. Jordan and C. Bishop.我不知道这本书是不是已经出版了（不要和Learning in Graphical Models混淆，那是个论文集，不适合初学）。这本书从基本的贝叶斯统计模型出发一直深入到复杂的统计网络的估计和推断，深入浅出，statistical learning的许多重要方面都在此书有清楚论述和详细讲解。MIT内部可以access，至于外面，好像也是有电子版的。3. 分析 (Analysis)：我想大家基本都在大学就学过微积分或者数学分析，深度和广度则随各个学校而异了。这个领域是很多学科的基础，值得推荐的教科书莫过于Principles of Mathematical Analysis, by Walter Rudin有点老，但是绝对经典，深入透彻。缺点就是比较艰深——这是Rudin的书的一贯风格，适合于有一定基础后回头去看。在分析这个方向，接下来就是泛函分析(Functional Analysis)。Introductory Functional Analysis with Applications, by Erwin Kreyszig.适合作为泛函的基础教材，容易切入而不失全面。我特别喜欢它对于谱论和算子理论的特别关注，这对于做learning的研究是特别重要的。Rudin也有一本关于functional analysis的书，那本书在数学上可能更为深刻，但是不易于上手，所讲内容和learning的切合度不如此书。在分析这个方向，还有一个重要的学科是测度理论(Measure theory)，但是我看过的书里面目前还没有感觉有特别值得介绍的。4. 拓扑 (Topology)：在我读过的基本拓扑书各有特色，但是综合而言，我最推崇：Topology (2nd Ed.)  by James Munkres这本书是Munkres教授长期执教MIT拓扑课的心血所凝。对于一般拓扑学(General topology)有全面介绍，而对于代数拓扑(Algebraic topology)也有适度的探讨。此书不需要特别的数学知识就可以开始学习，由浅入深，从最基本的集合论概念（很多书不屑讲这个）到Nagata-Smirnov Theorem和Tychonoff theorem等较深的定理（很多书避开了这个）都覆盖了。讲述方式思想性很强，对于很多定理，除了给出证明过程和引导你思考其背后的原理脉络，很多令人赞叹的亮点——我常读得忘却饥饿，不愿释手。很多习题很有水平。5. 流形理论 (Manifold theory)：对于拓扑和分析有一定把握时，方可开始学习流形理论，否则所学只能流于浮浅。我所使用的书是Introduction to Smooth Manifolds.  by John M. Lee虽然书名有introduction这个单词，但是实际上此书涉入很深，除了讲授了基本的manifold, tangent space,bundle, sub-manifold等，还探讨了诸如纲理论(Category theory)，德拉姆上同调(De Rham cohomology)和积分流形等一些比较高级的专题。对于李群和李代数也有相当多的讨论。行文通俗而又不失严谨，不过对某些记号方式需要熟悉一下。虽然李群论是建基于平滑流形的概念之上，不过，也可能从矩阵出发直接学习李群和李代数——这种方法对于急需使用李群论解决问题的朋友可能更加实用。而且，对于一个问题从不同角度看待也利于加深理解。下面一本书就是这个方向的典范：Lie Groups, Lie Algebras, and Representations: An Elementary Introduction.  by Brian C. Hall此书从开始即从矩阵切入，从代数而非几何角度引入矩阵李群的概念。并通过定义运算的方式建立exponential mapping，并就此引入李代数。这种方式比起传统的通过“左不变向量场(Left-invariant vector field)“的方式定义李代数更容易为人所接受，也更容易揭示李代数的意义。最后，也有专门的论述把这种新的定义方式和传统方式联系起来。————————————————————————————无论是研究Vision, Learning还是其它别的学科，数学终究是根基所在。学好数学是做好研究的基石。学好数学的关键归根结底是自己的努力，但是选择一本好的书还是大有益处的。不同的人有不同的知识背景，思维习惯和研究方向，因此书的选择也因人而异，只求适合自己，不必强求一致。上面的书仅仅是从我个人角度的出发介绍的，我的阅读经历实在非常有限，很可能还有比它们更好的书（不妨也告知我一声，先说声谢谢了）。"}
{"content2":"今日CS.CV 计算机视觉论文速览Mon, 17 Jun 2019Totally 44 papers👉上期速览✈更多精彩请移步主页Interesting:📚综述:基于图像的深度重建, 基于单张或多张RGB图像估计深度是十分重要的工作，研究人员调研了超过100篇文章及其关键贡献，总结了常用的技术路线，分析了每类方法的优点和局限性，包括训练数据集、网络结构、训练策略、应用场景及其对结果的影响。文章包括立体匹配，单图或多图回归，训练过程以及损失函数的选择，以及各种方法的具体表现。文中的多个表格给出了很多有意义的对比和总结。(from Murdoch University 澳大利亚)FUTURE RESEARCH DIRECTIONS值得学习，包括输入数据的选择、精度的提升、表现度量、训练法和数据bias等。📚点云与对应图像的6DOF匹配, 研究人员提出了一种直接匹配RGB图像特征和点云特征的方法，用于将图像与对应点云的位置和位姿进行定位。研究人员构建了数据集来匹配对应数据的2D,3D描述子，并利用他来训练这种描述子匹配算法。(from University of Western Australia)分别从图像和点云中抽取对应关键点和描述子，随后利用描述子匹配器来将其进行匹配以寻找对应的2D,3D关键点对。这种方法对于图像向点云的位姿估计和点云的匹配具有十分鲁棒的效果。匹配的结果：📚基于Retinx和GANs的暗光增强算法, 对于极度暗光条件下的处理研究人员结合了Retinex理论和GAN，将成像视为照明图像和反射图像两部分，并利用优化方法提高了生成图像的质量。(from 中科大)可以看到这种方法利用了黄色的UNet来提升了环境中的照明条件，并最终生成更加明亮的图像：照明和反射的估计结果：一些暗光增强的结果：dataset: Converted See-In-the-Dark (CSID)LoL dataset:https://github.com/daooshee/BMVC2018website/blob/master/index.html📚基于语义分割的通用条形码二维码检测器, (from Moscow Institute of Physics and Technology)在不同数据集上的比较：dataset:ArTe-Lab 1D Medium Barcode Dataset📚***基于卫星影像的地形滑坡评估, (from MIT)dataset:SENTINEL-2 IMAGERY DATAref:1.2,3Daily Computer Vision Papers***Connecting Touch and Vision via Cross-Modal PredictionAuthors Yunzhu Li, Jun Yan Zhu, Russ Tedrake, Antonio Torralba人类使用多种模态感官输入来感知世界，例如视觉，听觉和触觉。在这项工作中，我们研究了视觉和触觉之间的交叉模态联系。这个跨域建模任务的主要挑战在于两者之间的显着尺度差异，而我们的眼睛立刻感知整个视觉场景，人类在任何给定时刻只能感受到物体的一个小区域。为了连接视觉和触觉，我们引入了从视觉输入合成合理的触觉信号的新任务，以及想象我们如何在给定触觉数据作为输入的情况下与对象进行交互。为了实现我们的目标，我们首先为机器人配备视觉和触觉传感器，并收集相应视觉和触觉图像序列的大规模数据集。为了缩小规模差距，我们提出了一种新的条件对抗模型，该模型包含了触摸的比例和位置信息。人类感知研究表明，我们的模型可以从触觉数据中产生逼真的视觉图像，反之亦然。最后，我们提供了关于不同系统设计的定性和定量实验结果，以及可视化我们模型的学习表示。Pseudo-LiDAR++: Accurate Depth for 3D Object Detection in Autonomous DrivingAuthors Yurong You, Yan Wang, Wei Lun Chao, Divyansh Garg, Geoff Pleiss, Bharath Hariharan, Mark Campbell, Kilian Q. Weinberger在3D中检测诸如汽车和行人之类的物体在自动驾驶中起着不可或缺的作用。现有方法主要依靠昂贵的LiDAR传感器来获得准确的深度信息。虽然最近伪LiDAR作为一种有前景的替代方案被引入，但仅以立体图像为基础的成本要低得多，但仍然存在显着的性能差距。在本文中，我们通过改进立体声深度估计，为伪LiDAR框架提供了实质性的进步。具体地说，我们使立体网络架构和损耗函数更加符合远距离物体的精确深度估计，这是目前伪LiDAR的主要弱点。此外，我们探索了利用更便宜但极其稀疏的LiDAR传感器的想法，这些传感器单独提供的信息不足以进行3D检测，从而影响我们的深度估算。我们提出了一种深度传播算法，在初始深度估计的指导下，在整个深度图上扩散这些精确的测量值。我们在KITTI物体检测基准测试中表明，我们的组合方法在深度估计和基于立体的3D物体检测方面取得了实质性的改进，优于远程物体的先前技术检测精度40。我们的代码将在公开发布Universal Barcode Detector via Semantic SegmentationAuthors Andrey Zharkov, Ivan Zagaynov通过语义分割的通用条形码检测器R2D2: Reliable and Repeatable Detectors and Descriptors for Joint Sparse Keypoint Detection and Local Feature ExtractionAuthors Jerome Revaud, Philippe Weinzaepfel, C sar De Souza, Noe Pion, Gabriela Csurka, Yohann Cabon, Martin Humenberger兴趣点检测和局部特征描述是许多计算机视觉应用中的基本步骤。这些任务的经典方法基于检测然后描述范例，其中使用单独的手工方法来首先识别可重复的关键点，然后用本地描述符表示它们。利用度量学习损失训练的神经网络最近采用了这些技术，侧重于在检测到的关键点位置处学习关键点检测和学习描述符的可重复显着性映射。在这项工作中，我们认为显着区域不一定是歧视性的，因此可能损害描述的性能。此外，我们声称只能在可以高可信度地执行匹配的区域中学习描述符。因此，我们建议联合学习关键点检测和描述以及局部描述符判别性的预测器。这使我们能够避免模糊区域并导致可靠的关键点检测和描述。我们的检测和描述方法，通过自我监督培训，可以同时输出稀疏，可重复和可靠的关键点，优于HPatches数据集上的最先进的检测器和描述符。它还建立了最近发布的Aachen Day Night本地化数据集的记录。A Partially Reversible U-Net for Memory-Efficient Volumetric Image SegmentationAuthors Robin Br gger, Christian F. Baumgartner, Ender Konukoglu用于分段的3D卷积神经网络的一个主要缺点是它们的存储器占用，这需要在网络架构中妥协以适应给定的存储器预算。在RevNet的图像分类的推动下，我们提出了一种部分可逆的U Net架构，可以大幅降低内存消耗。可逆架构允许我们从后续层的输出中精确恢复每个层的输出，从而无需存储反向传播的激活。这缓解了最大的内存瓶颈，并在理论上实现了非常深的3D架构。在BraTS挑战数据集上，我们展示了大量的内存节省。我们进一步表明，释放的存储器可用于处理整个视场FOV而不是补丁。由于部分可逆的架构，增加网络深度可以提高分段精度，同时仅增加一小部分内存占用。Modality Conversion of Handwritten Patterns by Cross Variational AutoencodersAuthors Taichi Sumi, Brian Kenji Iwana, Hideaki Hayashi, Seiichi Uchida本研究试图构建一个可以将在线和离线手写字符相互转换的网络。建议的网络由两个具有共享潜在空间的变分自动编码器VAE组成。 VAE经过培训，可同时生成在线和离线手写拉丁字符。通过这种方式，我们创建了一个交叉模态VAE Cross VAE。在训练期间，拟议的跨越VAE被训练以最小化两种模态的重建损失，两种VAE的分布损失，以及称为空间共享损失的新的第三种损失。第三，空间共享损失用于通过计算潜在变量之间的距离来鼓励模态共享相同的潜在空间。通过所提出的方法，可以实现在线和离线手写字符的相互转换。在本文中，我们通过定性和定量分析证明了Cross VAE的性能。A Survey on Deep Learning Architectures for Image-based Depth ReconstructionAuthors Hamid Laga估计RGB图像的深度是一个长期存在的问题，计算机视觉，图形和机器学习社区已经探索了数十年。在本文中，我们对该领域的最新发展进行了全面的调查。我们将重点关注使用深度学习技术从一个或多个图像估计深度的作品。深度学习，加上大型训练数据集的可用性，彻底改变了研究界正在深入重建问题的方式。在本文中，我们调查了过去五年中出现的100多个关键贡献，总结了最常用的管道，并讨论了它们的优点和局限性。回顾到目前为止已取得的成果，我们还推测未来可能会为基于学习的深度重建研究带来什么。Copy and Paste: A Simple But Effective Initialization Method for Black-Box Adversarial AttacksAuthors Thomas Brunner, Frederik Diehl, Alois Knoll已经提出了许多用于生成黑盒子对抗性示例的优化方法，但是没有详细考虑初始化所述优化器的方面。我们证明起点的选择确实至关重要，而且最先进的攻击性能取决于它。首先，我们讨论攻击图像分类器的起始点的理想属性，以及如何选择它们以提高查询效率。值得注意的是，我们发现简单地从其他图像复制小补丁是一种有效的策略。在对ImageNet的评估中，我们表明这种初始化将现有技术边界攻击所需的查询数量减少了81，明显优于针对目标黑匣子对抗性示例报告的先前结果。Direct Image to Point Cloud Descriptors Matching for 6-DOF Camera Localization in Dense 3D Point CloudAuthors Uzair Nadeem, Mohammad A. A. K. Jalwana, Mohammed Bennamoun, Roberto Togneri, Ferdous Sohel我们提出了一种新概念，用于直接匹配从RGB图像中提取的特征描述符，以及从3D点云提取的特征描述符。我们使用这个概念来定位密集点云中查询图像的相机的位置和方向姿势。我们生成匹配2D和3D描述符的数据集，并使用它来训练提出的Descriptor Matcher算法。为了在点云中本地化查询图像，我们从查询图像中提取2D关键点和描述符。然后，描述符匹配器用于通过将2D描述符与预先提取的点云的3D描述符进行匹配来找到对应的对2D和3D关键点。该信息用于稳健的姿势估计算法中以在3D点云中定位查询图像。实验证明直接匹配2D和3D描述符不仅是可行的想法，而且与用于相机姿态定位的其他现有技术方法相比也实现了竞争准确性。***MonoLoco: Monocular 3D Pedestrian Localization and Uncertainty EstimationAuthors Lorenzo Bertoni, Sven Kreiss, Alexandre Alahi我们从单眼RGB图像中解决了3D人体定位的根本问题。在输出点估计的神经网络的限制的驱动下，我们通过基于拉普拉斯分布的损失函数预测置信区间的新神经网络来解决任务中的模糊性。我们的架构是一个轻量级的前馈神经网络，它可以预测给定2D人体姿势的3D坐标。该设计特别适用于小型训练数据和交叉数据集概括。我们的实验表明，我们在KITTI和nuScenes数据集上的表现优于最先进的结果，ii甚至超越了远方行人的立体声，并且iii估计了有意义的置信区间。我们进一步分享了对我们的不确定性模型的见解，以及有限的观察和分布样本。Low-light Image Enhancement Algorithm Based on Retinex and Generative Adversarial NetworkAuthors Yangming Shi, Xiaopo Wu, Ming Zhu低光图像增强通常被认为是图像处理中的挑战性任务，尤其是对于夜间或弱照明的复杂视觉任务。为了减少低光图像上的模糊或噪声，大量论文有助于应用不同的技术。令人遗憾的是，他们中的大多数在处理图像的极差照明部分或在实践中测试时几乎没有用处。在这项工作中，作者提出了一种基于Retinex理论和生成对抗网络GAN处理低光图像的新方法，GAN由用于将图像分成照明图像和反射图像的分解部分组成，以及用于生成的增强部分高品质的形象。期望这种辨别网络使得生成的图像更清晰。在Converted See In the Dark CSID数据集的基础上，在不同光照强度的情况下实现了实验耦合，取得了令人满意的结果，超出了预期，鼓励了作者。总之，所提出的基于GAN的网络和在这项工作中使用的Retinex理论已被证明在处理低光图像增强问题方面是有效的，这将有利于图像处理，毫无疑问。Utilizing the Instability in Weakly Supervised Object DetectionAuthors Yan Gao, Boxiao Liu, Nan Guo, Xiaochun Ye, Fang Wan, Haihang You, Dongrui Fan弱监督对象检测WSOD专注于仅具有图像级别注释的训练对象检测器，并且由于监督和目标之间的差距而具有挑战性。大多数现有方法将WSOD建模为多实例学习MIL问题。然而，我们观察到基于MIL的检测器的结果是不稳定的，即，当使用不同的初始化时，最有信心的边界框显着改变。我们通过引入衡量它的度量来定量地证明不稳定性，并根据经验分析不稳定的原因。尽管不稳定性似乎对检测任务有害，但我们认为它可以通过融合不同初始化检测器的结果来改善性能。为了实现这个想法，我们提出了一个具有多个检测分支的端到端框架，并引入了一个简单的融合策略。我们进一步提出了一种正交初始化方法来增加检测分支之间的差异。通过利用不稳定性，我们在具有挑战性的PASCAL VOC 2007和2012数据集上实现了52.6和48.0 mAP，这两个数据集都是新的艺术状态。Towards End-to-End Text Spotting in Natural ScenesAuthors Hui Li, Peng Wang, Chunhua Shen自然场景图像中的文本定位对于许多图像理解任务非常重要。它包括两个子任务文本检测和识别。在这项工作中，我们提出了一个统一的网络，通过单个前向传递同时本地化和识别文本，避免中间过程，如图像裁剪和特征重新计算，单词分离和字符分组。Fusion vectors: Embedding Graph Fusions for Efficient Unsupervised Rank AggregationAuthors Icaro Cavalcante Dourado, Ricardo da Silva Torres近年来，数字内容的数量和复杂性的大量增加引起了对特设检索系统的广泛关注。互补的是，异构数据源和检索模型的存在刺激了日益巧妙和有效的秩聚合函数的激增。尽管最近提出的等级聚合函数在有效性方面是有希望的，但该领域的现有提议通常忽略了效率方面。我们提出了一种创新的秩聚合函数，该函数是无监督的，本质上是多模态的，并且针对快速检索和最高效性能。我们介绍了基于图的秩聚合表示模型的嵌入和索引的概念，以及它们在搜索任务中的应用。还提出了用于基于图的秩表示的嵌入公式。我们引入了融合向量的概念，即基于秩的对象的后期融合表示，从中定义了内在秩聚合检索模型。接下来，我们提出了一种基于融合向量的快速检索方法，从而推广了一种有效的秩聚合系统。我们的方法在最先进的相关工作中呈现出最高效的表现，同时带来了多模态和有效性的新颖方面。在所考虑的所有数据集中，针对最近的基线实现了一致的加速。Divide and Conquer the Embedding Space for Metric LearningAuthors Artsiom Sanakoyeu, Vadim Tschernezki, Uta B chler, Bj rn Ommer学习嵌入空间，其中语义相似的对象靠近在一起，不同的对象相隔很远，是许多计算机视觉应用的基石。现有方法通常在嵌入空间中学习用于所有可用数据点的单个度量，其可具有非常复杂的非均匀分布，其中对象之间具有不同的相似性概念，例如，外观，形状，颜色或语义。学习单个距离度量的方法通常很难编码所有不同类型的关系，并且不能很好地概括。在这项工作中，我们提出了一种新颖易于实现的深度度量学习的分而治之的方法，它显着改善了度量学习的艺术性能。我们的方法通过将嵌入空间和数据共同分成K个较小的子问题来更有效地利用嵌入空间。它将数据和嵌入空间分成K个子集，并在嵌入空间的非重叠子空间中学习K个单独的距离度量，由神经网络的嵌入层中的神经元组定义。所提出的方法提高了收敛速度并改善了泛化，因为与原始子问题相比，每个子问题的复杂性降低了。我们表明，我们的方法在CUB200 2011，CARS196，斯坦福在线产品，店铺服装和PKU VehicleID数据集中的检索，聚类和重新识别任务方面都大大超过了现有技术水平。***Image Captioning: Transforming Objects into WordsAuthors Simao Herdade, Armin Kappeler, Kofi Boakye, Joao Soares图像字幕模型通常遵循编码器解码器架构，其使用抽象图像特征向量作为编码器的输入。最成功的算法之一使用从对象检测器获得的区域提议中提取的特征向量。在这项工作中，我们介绍了对象关系变换器，它建立在这种方法的基础上，通过几何注意显式地结合有关输入检测对象之间的空间关系的信息。定量和定性结果证明了这种几何注意对图像字幕的重要性，从而改进了MS COCO数据集上所有常见的字幕指标。Temporal Transformer Networks: Joint Learning of Invariant and Discriminative Time WarpingAuthors Suhas Lohit, Qiao Wang, Pavan Turaga许多时间序列分类问题涉及开发对时间错位不变的度量。在人类活动分析中，由于各种原因（包括不同的初始阶段，传感器采样率和由于受试者特定的生物力学导致的弹性时间扭曲）而出现时间错位。该领域的过去工作仅考虑通过弹性时间对准减少类内变异性。在本文中，我们提出了一种基于混合模型和数据驱动的方法来学习翘曲函数，这不仅可以减少类内变异，还可以增加类间分离。我们称之为时态变压器网络TTN。 TTN是一个可解释的可区分模块，可以轻松集成到分类网络的前端。该模块能够通过生成输入相关的变形函数来减少类内方差，这导致速率稳健的表示。同时，它通过学习更具辨别力的变形函数来增加类间方差。我们使用所提出的框架，在具有挑战性的数据集上的3D动作识别中展示了对强基线的改进。当训练集较小时，这些改进尤其明显。Cross-View Policy Learning for Street NavigationAuthors Ang Li, Huiyi Hu, Piotr Mirowski, Mehrdad Farajtabar在不熟悉的环境中从视觉观察导航的能力是智能代理的核心组成部分，也是Deep Reinforcement Learning RL的持续挑战。街景视图可以成为这类RL代理商的合理测试平台，因为它可以在地面提供真实世界的摄影图像，具有多样的街道外观，它已被制作成一个名为StreetLearn的交互式环境，用于导航研究。然而，目标驱动的街道导航代理到目前为止还没有能够在没有大量再训练的情况下转移到看不见的区域，并且依靠模拟不是可扩展的解决方案。由于航拍图像易于全球访问，我们建议在地面和航拍视图上训练多模态政策，然后利用鸟瞰图观察将地面视图政策转移到城市中看不见的目标部分。我们的核心思想是将地面视图与鸟瞰图配对，并学习可跨视图转换的联合策略。我们通过为两个视图学习类似的嵌入空间，跨视图提取策略并删除视觉模式来实现这一目标。我们进一步将转移学习范式重新划分为三个阶段1交叉模式训练，当代理人最初在多个城市区域进行训练时，2个鸟瞰图仅适应新区域，当代理人仅使用容易适应的区域时当代理人在看不见的地面视图上的导航任务上进行测试，没有航拍图像时，可获得的鸟瞰图和3个地面视图仅传输。实验结果表明，所提出的交叉视图策略学习能够更好地推广代理，并允许更有效地转移到看不见的环境。Unsupervised Video Interpolation Using Cycle ConsistencyAuthors Fitsum A. Reda, Deqing Sun, Aysegul Dundar, Mohammad Shoeybi, Guilin Liu, Kevin J. Shih, Andrew Tao, Jan Kautz, Bryan Catanzaro学习通过插值来合成高帧率视频需要大量的高帧率训练视频，然而这些视频很少，特别是在高分辨率时。在这里，我们提出了无监督技术，使用周期一致性直接从低帧率视频合成高帧率视频。对于连续帧的三元组，我们优化模型以最小化中心帧与其周期重建之间的差异，其通过从内插中间帧内插回来获得。这种简单的无监督约束单独实现了与使用地面实际中间帧的监督相当的结果。我们进一步引入伪监督损失项，其强制内插帧与预训练插值模型的预测一致。伪监督损失项与循环一致性一起使用，可以有效地使预训练模型适应新的目标域。由于没有额外的数据和完全无监督的方式，我们的技术显着改善了新目标域上的预训练模型，在慢流上将PSNR值从32.84dB增加到33.05dB，在Sintel评估数据集上从31.82dB增加到32.53dB。Hallucinating Bag-of-Words and Fisher Vector IDT terms for CNN-based Action RecognitionAuthors Lei Wang, Piotr Koniusz, Du Q. Huynh在本文中，我们重新使用旧式手工制作的视频表示，并通过基于CNN的幻觉步骤为这些技术注入新的活力。具体来说，我们通过在大规模Kinetics 400数据集上预训练的I3D网络解决视频中的动作分类问题。尽管使用了RGB和光学流帧，但I3D模型在将其输出与改进的密集轨迹IDT相结合并且通过其通过Bag of Words BoW和Fisher Vectors FV编码的低级视频描述符中提取时蓬勃发展。由于各种预处理步骤，描述符提取，编码和模型的微调，这种CNN和手工制作的表示的融合是耗时的。在本文中，我们提出了一个端到端的可训练网络，其中的流在训练阶段学习基于IDT的BoW FV表示，并且易于与I3D模型集成。具体来说，每个流在最后一个1D转换之前采用I3D特征映射。图层并学习将这些地图转换为BoW FV表示。因此，我们增强的I3D模型可以在测试阶段产生幻觉并使用这种合成的BoW FV表示。我们在三个公开可用的数据集上展示了我们模型的简单实用性，并展示了最先进的结果。Stand-Alone Self-Attention in Vision ModelsAuthors Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, Jonathon Shlens卷积是现代计算机视觉系统的基本组成部分。最近的方法主张超越卷积以捕获长程依赖性。这些努力的重点是通过基于内容的交互来增强卷积模型，例如自我关注和非本地手段，以实现许多愿景任务的收益。出现的自然问题是，注意力是否可以成为视觉模型的独立原语，而不仅仅是在卷积之上的增强。在开发和测试纯自我关注视觉模型时，我们验证自我关注确实可以成为一个有效的独立层。使用应用于ResNet模型的自我注意力替换所有空间卷积实例的简单过程产生完全自我注意模型，其在ImageNet分类上优于基线，减少12个FLOPS和29个参数。在COCO对象检测中，纯自我关注模型与基线RetinaNet的mAP匹配，同时具有少39个FLOPS和34个较少的参数。详细的消融研究表明，当在后面的层中使用时，自我注意力尤其有影响力。这些结果证明，独立自我关注是视力实践者工具箱的重要补充。Dynamic PET cardiac and parametric image reconstruction: a fixed-point proximity gradient approach using patch-based DCT and tensor SVD regularizationAuthors Ida H ggstr m, Yizun Lin, Si Li, Andrzej Krol, Yuesheng Xu, C. Ross Schmidtlein我们的目标是通过改进的图像重建来提高动态正电子发射断层扫描PET摄取图像的视觉质量和定量准确性，使用包含2D空间1D时间3DT信息的复杂稀疏惩罚模型。我们开发了两种新的3DT PET重建算法，结合了基于离散余弦变换DCT w补片和张量核范数TNN w补丁的不同时间和空间惩罚，并且逐帧方法比较传统的2D有序子集期望最大化OSEM后滤波和2D DCT和2D TNN。模拟并重建具有动力学摄取2组织模型和移动3DT心肺模型的3DT脑模型。对于心肺模型，重建了另外的心脏门控2D OSEM组。研究了结构相似性指数SSIM和相对均方根误差rRMSE相对地面实况。通过区域生长发现图像导出的心肺图像的左心室LV容积，并计算脑模型的参数图像。对于心肺模型，3DT TNN产生最佳图像，3DT DCT最适合脑模型。与心脏门控2D OSEM和2D OSEM相比，3DT TNN图像的最佳LV体积平均接近真实值11和55个百分点。与2D OSEM相比，基于3DT DCT图像的参数图像通常具有更小的偏差和更高的SSIM。我们的新方法结合了2D空间和1D时间惩罚，产生了比传统2D方法更高质量的动态PET图像，需要后置滤波。同时捕获呼吸和心脏运动，需要呼吸或心脏门控。 LV体积恢复得更好，随后拟合的参数图像通常偏差较小且质量较高。Learning Instance Occlusion for Panoptic SegmentationAuthors Justin Lazarow, Kwonjoon Lee, Zhuowen Tu最近，视觉社区对先前称为图像解析的全景分割工作表现出了新的兴趣。虽然在实例和语义分割任务中分别进行了大量的进展，但是全景分割意味着在单个输出中知道可数事物和语义事物。一种常见的方法涉及各个实例和语义分段提议的融合，但是，该方法没有明确地解决从单个输出中的实例分割到非重叠放置的跳转，并且经常不能充分地布置重叠实例。我们建议对Mask R CNN框架进行直接扩展，该框架的任务是解析两个实例掩码应如何在融合输出中作为二元关系彼此重叠。我们展示了整体全景质量PQ的竞争性增长以及标准全景细分基准测试事物部分的特殊收益，与具有可比架构的方法相比达到了最新水平。Semantics to Space(S2S): Embedding semantics into spatial space for zero-shot verb-object query inferencingAuthors Sungmin Eum, Heesung Kwon我们提出了一种新的深度零镜头学习ZSL模型，用于推理人类对象与动词对象VO查询的交互。虽然先前的ZSL方法仅使用语义文本信息来馈送到查询流中，但我们也试图将语义合并并嵌入到视觉表示流中。我们的方法由Semantics to Space S2S架构提供支持，其中从驻留对象派生的语义嵌入到空间空间中。该架构允许共同捕获人和对象的语义属性以及它们的位置大小轮廓信息。由于这是第一次尝试用VO查询解决零镜头人体对象交互推理，我们构建了一个新的数据集，Verb Transferability 60 VT60。 VT60提供60种不同的VO对，其重叠动词专为通过VO查询测试ZSL方法而量身定制。实验评估表明，我们的方法不仅优于现有技术水平，而且还表明无论使用哪种ZSL基线架构，都能始终如一地提高性能。IntrinSeqNet: Learning to Estimate the Reflectance from Varying IlluminationAuthors Gr goire Nieto, Mohammad Rouhani, Philippe Robert固有图像分解基于其反射和阴影分量描述图像。在本文中，我们解决了在各种照明下从固定视点捕获的一系列图像估计漫反射率的问题。为此，我们提出了一种深度学习方法，以避免对反射率先验的启发式和强假设。我们比较了两个网络架构，一个经典的U形卷积神经网络CNN和一个由卷积门控循环单元CGRU组成的递归神经网络RNN。我们在一个专门为序列内在分解任务设计的新数据集上训练我们的网络。我们在MIT和BigTime数据集上测试我们的网络，并且在质量和数量上都优于最先进的算法。Can generalised relative pose estimation solve sparse 3D registration?Authors Siddhant Ranade, Xin Yu, Shantnu Kakkar, Pedro Miraldo, Srikumar Ramalingam流行的3D扫描注册项目，如斯坦福数字米开朗基罗或KinectFusion，利用高分辨率传感器数据进行扫描对齐。在没有RGB分量的情况下解决稀疏3D扫描的配准尤其具有挑战性。在这种情况下，我们无法建立点对应，因为在两次连续扫描中无法捕获相同的3D点。与基于对应的方法相比，我们采用不同的视点，并基于来自相邻扫描的线段的交叉点的约束来制定稀疏3D配准问题。我们通过将每个水平和垂直扫描线建模为分段线性段来获得线段。我们提出了一种新的交替投影算法，用于使用线交叉约束来解决扫描对齐问题。我们开发了两个新的最小解算器，用于在存在平面对应的情况下进行扫描对准：1个线交叉和1个平面对应，以及2个1线交叉和2个平面对应。我们在Kinect和LiDAR数据集上的表现优于其他竞争方法。Joint Concept Matching-Space Projection Learning for Zero-Shot RecognitionAuthors Wen Tang, Ashkan Panahi, Hamid Krim零射击学习ZSL已被广泛研究并在机器学习中取得了巨大成功，其目的是通过仅对所见对象类进行训练来识别看不见的对象类。大多数现有的ZSL方法通常用于学习视觉特征空间和语义空间之间的投影函数，并且主要遭受投影域移位问题，因为在看到的和看不见的类之间通常存在大的域间隙。在本文中，我们提出了一种新的归纳ZSL模型，该模型基于视觉和语义特征将项目划分为具有类特定知识的共同的不同潜在空间，并通过这种不同的公共空间重建视觉和语义特征，以缩小域移位间隙。我们证明了潜在空间的所有这些约束，类特定知识，特征重建及其组合增强了对投影域移位问题的鲁棒性，并提高了对看不见的对象类的泛化能力。对四个基准数据集的综合实验表明，我们提出的方法优于现有算法。***TensorNetwork for Machine LearningAuthors Stavros Efthymiou, Jack Hidary, Stefan Leichenauer我们使用TensorNetwork开源库演示了使用张量网络进行图像分类。我们详细解释了图像数据到矩阵产品状态形式的编码，并描述了如何以可并行化的方式收缩网络，并且非常适合自动梯度进行优化。将该技术应用于MNIST和Fashion MNIST数据集，我们分别使用相同的张量网络架构发现了98和88精度的开箱即用性能。 TensorNetwork库允许我们从CPU到GPU硬件无缝移动，我们发现使用GPU计算速度提高了10倍以上。http://www.tensornetworktheory.org/，https://www.zhihu.com/question/54786880A Signal Propagation Perspective for Pruning Neural Networks at InitializationAuthors Namhoon Lee, Thalaiyasingam Ajanthan, Stephen Gould, Philip H. S. Torr网络修剪是压缩深度神经网络的有前途的途径。修剪的典型方法首先是训练模型并删除不必要的参数，同时尽量减少对学习内容的影响。或者，最近的方法表明，修剪可以在训练之前的初始化时完成。然而，仍然不清楚为什么修剪未经训练的，随机初始化的神经网络是有效的。在这项工作中，我们从信号传播的角度考虑修剪问题，正式表征确保整个网络中忠实信号传播的初始化条件。基于网络输入输出雅可比的奇异值，我们发现正交初始化与其他初始化方案相比能够实现更忠实的信号传播，从而增强了对一系列现代架构和数据集的修剪结果。此外，我们通过实证研究了初始化时修剪监督的效果，并表明通常无监督修剪可以像监督修剪一样有效。此外，我们证明了我们的信号传播视角与无监督修剪相结合，确实可以用于将修剪应用于非标准任意设计架构的各种场景。Video-Driven Speech Reconstruction using Generative Adversarial NetworksAuthors Konstantinos Vougioukas, Pingchuan Ma, Stavros Petridis, Maja Pantic言语是一种依赖于音频和视觉信息的通信手段。缺乏一种方式往往会导致信息的混乱或误解。在本文中，我们提出了一种端到端时间模型，能够直接从静音视频合成音频，而无需转换到中间特征和从中间特征转换。我们提出的基于GAN的方法能够产生与视频同步的自然发声，可理解的语音。我们的模型的性能在GRID数据集上针对说话者相关和独立于说话者的场景进行评估。据我们所知，这是第一种将视频直接映射到原始音频的方法，也是第一种在以前看不见的扬声器上进行测试时产生可理解语音的方法。我们不仅根据声音质量而且还根据口语单词的准确性来评估合成音频。Deep neural network for fringe pattern filtering and normalisationAuthors Alan Reyes Figueroa, Mariano Rivera我们提出了一个处理Fringe Patterns FP的新框架。我们的新方法建立在以下假设的基础上：如果提供了足够多的损坏和清理的FP，则可以通过深度神经网络学习FP的去噪和归一化。尽管在文献中已经报道了类似的提议，但是我们提出了对众所周知的深度神经网络结构的改进，其在稳定性和可重复性方面产生高质量的结果。我们在各种情况下测试了我们的方法的性能，这些FPs被不同程度的噪声破坏，并且被不同的噪声分布破坏。我们将我们的方法与其他最先进的方法进行比较。合成数据和实际数据的实验结果证明了这种处理干涉图的新范例的能力和潜力。我们希望我们的工作能够推动这方面更复杂的发展。Efficient N-Dimensional Convolutions via Higher-Order FactorizationAuthors Jean Kossaifi, Adrian Bulat, Yannis Panagakis, Maja Pantic随着深度卷积神经网络的空前成功，寻求培训始终是更深层次的网络。然而，虽然更深入的神经网络在适当训练时提供更好的性能，但该深度也转化为存储器和计算重型模型，通常具有数千万个参数。已经提出了几种方法来利用网络中的冗余来减轻这种复杂性。预训练的网络被压缩，例如，使用低秩张量分解，或直接修改网络的体系结构以使其更有效。在本文中，我们在张量分解的镜头下，在统一的框架中研究这两种方法。我们展示了应用于卷积核的张量分解如何与诸如MobileNet的有效架构相关。此外，我们提出了一种基于张量的有效高阶卷积方法，可用作N维卷积的插件替换。对于2D和3D卷积网络，我们在理论和经验上证明了它们对于图像分类的有利特性。Global and Local Interpretability for Cardiac MRI ClassificationAuthors James R. Clough, Ilkay Oksuz, Esther Puyol Anton, Bram Ruijsink, Andrew P. King, Julia A. Schnabel用于对医学图像进行分类的深度学习方法已经在广泛的任务中表现出令人印象深刻的准确性，但是这些模型通常难以解释，限制了它们在临床实践中的适用性。在这项工作中，我们引入了卷积神经网络模型，用于识别心脏MR分割的时间序列中的疾病，其可以根据临床上熟悉的测量来解释。该模型基于变分自动编码器，将输入减少到发生分类的低维潜在空间。然后，我们使用最近开发的概念激活矢量技术来关联具有诊断意义的概念，例如。临床生物标志物，如左心室射血分数低至潜伏空间中的某些载体。然后通过观察由这些矢量方向上的潜在空间中的插值产生的图像域的变化来定性地检查这些概念。结果，当模型对图像进行分类时，它还能够提供与该分类相关的自然可解释的概念，并在图像域中展示这些概念的含义。我们的方法在英国生物银行心脏MRI数据集上得到证实，我们在其中检测冠状动脉疾病的存在。Dense Deformation Network for High Resolution Tissue Cleared Image RegistrationAuthors Abdullah Nazib, Clinton Fookes, Dimitri Perrin最近深度学习在医学图像分析的各个领域的应用带来了极好的性能提升。深度学习技术在医学图像配准中的应用在注册时间和准确性方面均优于传统的基于优化的注册算法。在本文中，我们提出了一种密集连接的卷积结构，用于可变形图像配准。网络的训练是无人监督的，并且不需要地面实况变形或任何合成变形作为标签。所提出的架构分别在两种不同版本的组织清除数据，10和25分辨率的高分辨率数据集上进行训练和测试，并且证明了与现有技术ANTS配准方法相当的配准性能。该方法还与基于深度学习的Voxelmorph配准方法进行了比较。由于存储器限制，原始体素模型可以在组织清除数据的最多15分辨率下工作。为了进行严格的实验比较，我们开发了基于贴片的Voxelmorph网络版本，并以10和25分辨率对其进行了训练。在这两种分辨率中，所提出的DenseDeformation网络在配准精度方面优于Voxelmorph。Landslide Geohazard Assessment With Convolutional Neural Networks Using Sentinel-2 Imagery DataAuthors Silvia L. Ullo, Maximillian S. Langenkamp, Tuomas P. Oikarinen, Maria P. Del Rosso, Alessandro Sebastianelli, Federica Piccirillo, Stefania Sica在本文中，作者旨在将最先进的图像识别模型与最佳公共卫星图像相结合，创建一个滑坡风险缓解系统。我们首先关注滑坡检测，并进一步提出用于预测的类似系统。这些模型很有价值，因为随着卫星图像的日益普及，它们可以轻松扩展以提供危害评估数据。目标是利用卫星图像和相关数据来丰富公共数据库，并指导救灾工作，以确定发生山体滑坡的精确区域。不同的图像增强方法用于增加所选数据集的多样性并创建更稳健的分类。然后将得到的输出馈送到3D D卷积神经网络的变体中。对当前文献的回顾表明，没有研究使用CNNs卷积神经网络和免费提供的卫星图像来分类滑坡风险。该模型已证明最终能够实现明显优于基线精度。GAN-based Multiple Adjacent Brain MRI Slice Reconstruction for Unsupervised Alzheimer's Disease DiagnosisAuthors Changhee Han, Leonardo Rundo, Kohei Murao, Zolt n d m Milacski, Kazuki Umemoto, Hideki Nakayama, Shin ichi Satoh利用大规模健康数据集，无监督学习可以发现各种看不见的疾病而无需任何注释。为此，无监督方法重建单个医学图像以检测学习特征空间中的异常值或高重建损失。然而，在不考虑多个相邻图像之间的连续性的情况下，它们不能直接区分由微小解剖异常的累积组成的疾病，例如阿尔茨海默氏病AD。此外，没有研究表明无监督异常检测与疾病阶段有何关联。因此，我们提出了一种基于生成性对抗网络的多步邻脑MRI切片重建检测不同阶段AD的两步法重建Wasserstein损失用梯度惩罚L1损失训练在3个健康脑MRI片上重建接下来的3个重建看不见的健康AD病例诊断平均最大损失，例如，每次扫描的L2损失区分它们，比较重建的地面实况图像。结果表明，我们可以在非常早期阶段可靠地检测AD，即曲线下面积AUC 0.780，同时还检测到晚期AD，即AUC 0.917更准确，因为我们的方法是无监督的，它也应该发现并警告任何异常包括罕见疾病。Towards Compact and Robust Deep Neural NetworksAuthors Vikash Sehwag, Shiqi Wang, Prateek Mittal, Suman Jana深度神经网络在许多应用中已经取得了令人印象深刻的性能，但是它们的大量参数导致了大量的计算和存储开销。最近的一些工作试图通过使用修剪连接来设计紧凑的网络来减轻这些开销。但是，我们观察到，设计紧凑型网络的大多数现有策略都无法保持网络对抗对抗性示例的鲁棒性。在这项工作中，我们严格研究网络修剪策略的扩展，以保持网络的良性准确性和稳健性。从修剪程序的正式定义开始，包括预训练，重量修剪和微调，我们提出了一种新的修剪方法，可以创建紧凑的网络，同时保持良好的准确性和稳健性。我们的方法基于两个主要见解1我们确保预训练和微调步骤的训练目标与所需稳健模型的训练目标相匹配，例如，对抗鲁棒性可验证的鲁棒性，2我们将修剪策略与训练前不可知和微调目标。我们在CIFAR 10数据集上的四个不同网络上评估我们的方法，并测量良性准确性，经验稳健准确性和可验证的稳健准确性。我们证明了我们的修剪方法可以保持平均93个良性准确度，92.5经验鲁棒精度和85.0可验证的鲁棒精度，同时将测试网络压缩10倍。Multi Scale Curriculum CNN for Context-Aware Breast MRI Malignancy ClassificationAuthors Christoph Haarburger, Michael Baumgartner, Daniel Truhn, Mirjam Broeckmann, Hannah Schneider, Simone Schwabing, Christiane Kuhl, Dorit Merhof乳腺癌和其他癌症类型的恶性肿瘤的分类通常被解决为对象检测问题。首先对个体病变进行定位，然后对恶性肿瘤进行分类。然而，这种方法的缺点在于，包含若干病变的抽象特征和未标记为病变但包含全球医学相关信息的区域因此被忽略，特别是对于动态对比增强乳房MRI，诸如背景实质增强和位置内的位置。乳房对于诊断很重要，不能通过适当的物体检测方法捕获。Model Agnostic Dual Quality Assessment for Adversarial Machine Learning and an Analysis of Current Neural Networks and DefensesAuthors Danilo Vasconcellos Vargas, Shashank Kotyan在对抗性机器学习中，存在大量各种类型的攻击，这使得评估新模型和防御的鲁棒性成为一项艰巨的任务。更糟糕的是，攻击和防御存在固有的偏见。在这里，我们组织面临模型依赖，评估不充分，不可靠的对抗样本和扰动依赖结果的问题，并提出双重质量评估方法以及鲁棒性水平的概念来解决它们。我们验证了最先进模型WideResNet，ResNet，AllConv，DenseNet，NIN，LeNet和CapsNet的双重质量评估，以及ICLR 2018提出的当前最难防御以及广为人知的对抗性培训，显示了当前的模型和防御在各方面的稳健性都很脆弱。此外，我们表明L 0和L infty攻击的鲁棒性差异很大，因此应该考虑二元性以进行正确的评估。有趣的是，所提出的评估的副产品是一种新颖的L infty黑盒方法，其需要比单像素攻击更少的扰动，仅一个像素攻击的扰动量达到类似的结果。因此，本文阐述了鲁棒性评估的问题，提出了双重质量评估来解决它们，并分析了当前模型和防御的鲁棒性。希望目前的分析和提出的方法将有助于开发更强大的深度神经网络和混合动力车。Speaker-Targeted Audio-Visual Models for Speech Recognition in Cocktail-Party EnvironmentsAuthors Guan Lin Chao, William Chan, Ian Lane鸡尾酒会环境中的语音识别仍然是现有技术语音识别系统的重大挑战，因为从具有相似频率和时间特性的重叠语音的背景中提取单个说话者的声学信号是极其困难的。我们建议使用扬声器目标声学和视听模型来完成这项任务。我们补充了混合DNN HMM模型中的声学特征，其具有目标说话者身份的信息以及来自目标说话者的嘴部区域的视觉特征。使用从GRID视听语料库生成的模拟鸡尾酒会数据通过在单个声道上重叠两个扬声器的语音来执行实验。我们的仅音频基线达到了26.3的WER。视听模型将WER提高到4.4。引入说话人身份信息具有更显着的效果，将WER提高到3.6。然而，将这两种方法结合起来并没有显着提高性能。我们的工作表明，以演讲者为目标的模型可以显着改善鸡尾酒会环境中的语音识别Scalable Neural Architecture Search for 3D Medical Image SegmentationAuthors Sungwoong Kim, Ildoo Kim, Sungbin Lim, Woonhyuk Baek, Chiheon Kim, Hyungjoo Cho, Boogeon Yoon, Taesup Kim本文提出了一种神经结构搜索NAS框架，用于三维医学图像分割，从大型设计空间自动优化神经结构。我们的NAS框架搜索每一层的结构，包括编码器和解码器中的神经连接和操作类型。由于高分辨率3D医学图像难以在大的离散架构空间上进行优化，因此还提出了一种基于连续松弛的新型随机采样算法，用于基于可伸缩梯度的优化。在具有基准数据集的3D医学图像分割任务中，所提出的NAS框架的自动设计的架构优于人类设计的3D U Net，而且该优化的架构非常适合于被转移以用于不同的任务。Multigrid Neural MemoryAuthors Tri Huynh, Michael Maire, Matthew R. Walter我们介绍了一种新的架构，它将大的可寻址存储空间集成到深度神经网络的核心功能中。我们的设计通过许多网络层分配内存寻址操作和存储容量。与将神经网络连接到外部存储体的策略不同，我们的方法是在整个网络结构中通过计算来定位存储器。镜像卷积网络中的最新架构创新，我们将内存组织成多分辨率层次结构，其内部连接能够学习动态信息路由策略和数据相关的读写操作。这种多重网格空间布局允许参数有效地缩放存储器大小，允许我们尝试比先前工作中的存储器大得多的存储器。我们在合成探索和绘图任务中展示了这种能力，其中网络能够自我组织并保留数千个时间步长的轨迹的长期记忆。在与任何空间几何概念分离的任务上，例如排序或关联召回，我们的设计作为一个真正的通用记忆，并产生与最近提出的可微分神经计算机竞争的结果。Solving Large-Scale 0-1 Knapsack Problems and its Application to Point Cloud ResamplingAuthors Duanshun Li, Jing Liu, Noseong Park, Dongeun Lee, Giridhar Ramachandran, Ali Seyedmazloom, Kookjin Lee, Chen Feng, Vadim Sokolov, Rajesh Ganesan0 1背包在计算机科学，商业，运筹学等方面具有根本重要性。在本文中，我们提出了一种基于深度学习技术的方法来解决大规模0 1背包问题，其中产品项目数量大或者数值产品不一定是预定的，而是在优化过程中由外部值分配功能决定的。我们的解决方案受到拉格朗日乘数法和最近采用博弈论进行深度学习的启发。在正式定义基于它们的方法之后，我们开发了一种自适应梯度上升方法来稳定其优化过程。在我们的实验中，所提出的方法在一分钟内解决了所有大规模基准KP实例，而现有方法显示出波动的运行时间。我们还表明我们的方法可以用于其他应用程序，包括但不限于点云重采样。Learning to Forget for Meta-LearningAuthors Sungyong Baik, Seokil Hong, Kyoung Mu Lee很少有镜头学习是一个具有挑战性的问题，需要系统从少数几个例子来实现泛化。元学习通过学习在任务分布中共享的先验知识来解决问题，然后用于快速适应看不见的任务。模型不可知元学习MAML算法将先验知识公式化为跨任务的公共初始化。但是，强制共享初始化会导致任务之间发生冲突，从而影响初始化的质量。在这项工作中，通过观察任务之间和神经网络层之间的折衷程度不同，我们提出了一种新的初始化思想，它采用依赖于任务的分层衰减，我们称之为选择性遗忘。所提出的衰减方案动态地控制每层将针对给定任务利用的先验知识的多少。实验结果表明，该方法减轻了冲突，并因此提供了出色的性能。我们进一步表明，所提出的方法，名为L2F，可以应用和改进其他最先进的基于MAML的框架，说明其普遍性。Chinese Abs From Machine TranslationPapers from arxiv.org更多精彩请移步主页pic from pexels.com"}
{"content2":"本文转载自科技中国，作者：孟海华(上海市科学学研究所)，首发刊载于《科技中国》杂志2018年3月 第3期 预测。一、人工智能全球领先的信息技术研究与顾问公司Gartner认为，2018年将是人工智能大众化应用的开始，将影响到企业和政府之外的更广泛的领域，这会给人工智能的发展和CIO们带来更多的机会。1.在发达国家，到2020年，20%的公民将使用人工智能助手帮助他们完成一系列日常的、可操作的任务。在发达国家，个人的互动与基于人工智能的服务已经变得越来越频繁，虚拟个人助理(VPA)不再是简单的提问和回答。从主要语音厂商反馈的数据来看，目前的语音错误率徘徊在5%，这是可以接受的错误率。这种更高的准确性使消费者更偏向于语音服务，如苹果的Siri、亚马逊的Alexa和谷歌助手，对数以百万计的智能手机用户来说更加有效。在这个背景下，2018年人们将赋予人工智能更多、更高的期望。2018年将有超过20亿的人会通过智能手机及其连接的设备，使用会话AI与VPA、虚拟客户助理(VCA)、虚拟执行助理(VEAs)、聊天机器人和其他的人工智能功能的服务。消费者将越来越熟悉使用这些服务，制定简单任务过程，比如设置闹钟或提醒。他们很容易地将这些人工智能会话转换成更复杂的任务，例如对未来任务进行计时或以其他方式进行交互。2.到2022年，40%的面向客户的员工和政府工作人员每天都会咨询AI，在虚拟助理的支撑下进行决策或执行流程。人工智能助手将越来越多地被作为会话平台与决策过程支持助手的关键点。AI功能将在两个方面支持虚拟助理：一是作为一种资源，AI使人类支持代理能够更快更有效地响应客户/公民的查询或行动;二是成为回答基本查询的首要对话界面。当前人工智能在减少关系摩擦和改善服务方面超越了商业售卖。具有人工智能虚拟助理(如苹果Siri或亚马逊Alexa)的人们对政府服务的反应也越来越灵敏。许多CIO尚未充分意识到基于人工智能的虚拟支持代理的潜力，包括聊天机器人。神经语言程序学配合机器学习能够理解不同组合中的词汇含义，并提出问题以揭示意图和创建上下文。基于这种理解，虚拟代理将能够回应客户或公民的问题，采取或提出智能支持。一个虚拟代理将能够更快地完成任务，而不仅仅是一个代理——人或虚拟的智能研究机构。3.到2020年，85%的CIO将通过AI程序执行购买、建造和外包工作目前的人工智能趋势意味着大多数机构将不必从头开始启动他们自己的人工智能研究项目。相反，CIO们将能够从当前的知识中收集和整理合理的发展战略，集合各业务单位的具体擅长。今天，绝大多数企业处于人工智能倡议的早期阶段，但它们正在迅速地向前发展。来自Gartner的最新调查数据显示，约4%的CIO(首席信息官)有AI部署，另外21%有短期规划，另有25%在中期或长期规划中有AI倡议。CIO们面临一系列严峻的挑战，包括孤立的数据孤岛，可怜的或不确定的数据质量，数字化和最基础的AI技能缺乏。雪上加霜的是，DNN开辟了AI新天地，怎么去适应?网络公司、云计算企业和云服务提供商都在选择部署机器学习和DNN融合的产品，该领域正有大举蔓延的趋势。未来三年人工智能人才缺口也将迅速填补，因为越来越多的大学开设人工智能课程，并展开人工智能再培训。在接下来的三年里，更多的软件企业和云服务提供商将DNN功能整合到他们的产品，进一步降低AI项目相关的复杂性和障碍。4.到2022年，公司内部公开设立的人工智能项目将100%从CIO那里得到资金支持CIO们正在评估关键任务的应用场景，确定对AI的举措。实事求是地说，人工智能驱动过程和能力优化的潜在好处是巨大的。但是，创建系统的危险性似乎带来了偏颇的结果，这可能对AI结果会造成毁灭性的影响。因此，决策者需要确保基本方法科学，采取问责制和提升透明度。企业使用具有挑战性的DNN案例的情况下，CIO必须能够验证和捍卫AI系统的结果。CIO寻求能够提供有效生产的AI的举措将是高度公开和透明的。许多企业使用案例，从金融服务到自动驾驶，实施DNN技术将具有挑战性。在金融服务方面，规定要求金融服务分析要有明确的定义和解释。而DNN可以提供极其精确的结果，但它中间阶段的数据如何转换得到结果往往是不透明的。因此，对于涉及诉讼、法规规范、监督和业务控制的案例，将需要采用更加科学的方法。人工智能生态系统应该提供工具来验证数据源和模型结果。机器学习建模环境越透明，组织就越能分析底层机器学习模型的效能。研究人员正在努力提高DNN方法透明度。5.到2022年，成熟经济体中人们将面临比真实信息更多的虚假信息一是智能之下隐藏的偏见——一种众所周知的人类倾向——引导所有人去寻找、选择和评估他们所相信的、期望被证明真实的信息。二是人工智能可以检测错误信息，但也可以生成它。检测和改善需要时间。三是人工智能创造虚假信息的成本和花费要比检测信息的成本更低。由于经济和政治的原因，虚假信息会超越真实信息的传播。在2020年之前，这种虚假信息表现为诈骗信息和金融领域的谎言，在这段时间内，没有大型互联网公司将完全成功地减轻这一问题。到2020年底，一些主要国家将通过规章或法律来遏制虚假信息的传播。二、3D打印和增材制造数字化3D打印硬件平台、软件和材料以及相关技术不仅给精密零件制造带来突破，也将改变组织的业务模式。具体来看，航天工业、医疗行业、创业者们因为3D打印和增材制造获益不浅，同时也模糊了零售商和制造商之间的界线。实践告诉我们，3D打印使用不会危及组织的核心制造能力或现有产品线。1.到2021年，75%的新型商用和军用飞机将使用3D打印部件飞行30年前，航空航天行业是首批采用3D打印的行业之一。由于产品设计和开发时间过长，航空航天制造军品商拥有飞机模型和部件的早期测试平台。今天，蔓延到多个业界，多数正在建立基础设施，以支持其组织和供应链中的多个3D打印业务。虽然在所有行业中，原型机制造仍然是3D打印的主要用例，但航空航天显然已经积极地跨出了一步，即实战。轨道发射器领域已经成功地进行了70次3D打印的Aeon 1发动机的测试。Aeon 1使用氧气和甲烷作为推进剂，其材料也将是火星上最容易制造的材料。空客320的“仿生分区(bionicpartition)”无法使用传统的制造和加工技术建造，使用3D打印，比目前的设计每年可以节省465000吨的二氧化碳排放量。波音公司已经在四个国家设立20个增材制造网站。超过50000个3D打印部件在商业和国防领域得到应用。GE航空涡轮螺旋桨发动机设计的855个常规制造部分，被划分为12个3D打印板块，产生10%马力，节省20%燃料，导致更短的开发周期和更低的开发成本。2.到2021年，25%的外科医生术前将在3D打印的解剖模型上练习国际顶级医院结合医学影像软件和服务，不断提升3D打印硬件水平，培训外科医生及新员工3D打印水平。目前，近3%的大型医院和医疗研究机构在现场都有3D打印技术能力。手术和术前准备的三维模型已通过以下方式得到改进：一是设计，即改进用户体验设计;二是计算机断层扫描(CT)、磁共振成像(MRI)和其他医学成像技术;三是得益于基于云的CAD和建模软件。在美国，随着3D打印技术的进步和其他技术的进一步完善，3D打印将从医院教学和专科中心扩展到更广泛的医院系统。例如，波士顿儿童医院结合3D打印进行团队训练和术前计划，临床医生、工业工程师、设计师、模拟专家、插图师和病人护理团队都参与其中。在组织架构上，3D打印部门独立于个别部门，如外科或放射科，以防止所谓“草坪问题”的出现。3.到2021年，20%的消费品公司将使用3D打印来制作定制产品3D打印可能对消费品公司供应链产生重大影响。尤其是满足特定定制需求的类别中，减少库存，成本和生产可以更接近最终客户。这种向本地生产转移以供当地消费的做法将迫使商业公司重新考虑其商业模式。例如，消费者或完全不相关的第三方可以成为完整产品的最终生产者或产品一部分的提供者而参与实际的产品交付。例如美国一家名为raceware的公司使用3D打印制作所定制的自行车部件，而美国一家零售初创公司则在当地生产3D打印服装。阿迪达斯等体育用品生产商都是围绕鞋底、鞋垫的3D打印，甚至是个性化的鞋垫图案来提升品牌的。这些产品有更高的价格，而且是定制的，旨在提高运动员的个人表现。4.到2021年，20%的企业将成立内部创业公司，开发基于3D打印的新产品和服务现在，商业以闪电般的速度发展，而技术的发展则更快。技术破坏一夜之间改变公司或整个行业。老牌企业不断面临来自全球一半地区的初创企业和新兴公司的竞争。即使是大公司也必须集成3D打印技术进入他们的研发、工程和制造集团，并建立一个内部推进3D打印和其他创新技术进入公司整体流程的业务部门。这些初创公司通常有独立于公司运营的自由，但具有利用企业设施的能力，如物理设施、财务、人事和采购，以降低成本。一旦成功，一家内部初创企业就会发展其研发和制造，工艺和产品进入主流制造业务，或主流业务。在过去的几年里，一些大型的企业工程组织，如空客、巴斯夫和通用电气，已经建立了工业规模的3D打印内部创业公司。这些公司能够加快将3D打印集成到他们自己的制造过程中，因为这些零件使用传统的制造方法，要么太困难，要么成本太高，它们还能够绕过大公司中已建立的管理链，它们往往扼杀而不是鼓励创新和冒险。除了加速创新之外，这些内部初创企业还有助于在公司内部识别企业家，并吸引外部工程人才，特别是那些永远不会考虑在这么大的公司工作的人才。5.到2021年，40%的制造企业将建立3D打印中心到2021年，40%的制造企业将建立3D打印中心(例如，波音、GE、Johnson、RollsRoyce和Siemens)。这些企业将3D打印相关工作流集成到关键业务流程中。3D打印中心以专注于改进设计创新、标准化等关键流程，重点关注质量和检查流程的改进。3D打印中心还可作为培训机构或供应链合作伙伴的经验交流中心。中型企业制造商正开始效仿建立3D打印中心，但投资较小。这些公司倾向于将3D打印服务外包，因为所需的资本成本和专门的劳动力通常都太大，无法维持。3D打印技术的主要增长动力在新兴市场，在巴西、南非和土耳其等几个新兴市场，3D打印发展神速。事实上，南非的研究更深入了一步，aeroswift 3D打印机打印速度是现有的粉床融合设备速度的10倍，建造面积是2000毫米×600毫米×600毫米，是世界上最大和最快的打印机。三、AR/VR/MR等沉浸式技术AR、VR和MR用户期望更大程度上从2D界面转移到更身临其境的3D世界，从3D捕捉更丰富、更平滑的图景，从3D获得新的体验。影响面包括商业、店内体验、聊天机器人、虚拟助理、区域规划、监控等。1.到2022年，20%的早期采用者将使用3D输入/输出接口来取代传统的2d/平面交互人工智能和计算机视觉技术的进步将为3D图像扫描、创建和消费者用户案例提供更多的价值。预计计算机视觉功能将越来越多地嵌入智能手机和智能终端。亚马逊最近投资于人体实验室，用于3D人体模型扫描，一些初创企业和公司，如Styku和Bodi.me，正在推动3D人体扫描，以实现对衣服的虚拟试穿和健康状况评估。空间音频的进步可以为用户提供既能将声音放置在3D环境中，又能检测出声音位置来源的体验。Google VR音频系统创建多个虚拟扬声器来再现声波。空间音频的进步可以为用户提供在三维环境中放置声音以及探测声音的位置源的体验，而DearVR空间连接简化了集成音频之间的互动环境和3D空间渲染。智能手机厂商的强大投资，为3D深度感知成为智能手机的标配功能开辟了道路。2.到2021年，20%的虚拟助理、聊天机器人和短信互动在发达国家将获得更丰富的形象，2017年这个比例只有3%几十年来，虚拟人类互动实验室(VHIL)一直在探索从面部表情到身体位置等虚拟人的视觉描绘的细微差别，以及它们如何对社会互动产生明确的影响。一是语音技术。例如，亚马逊在2017年推出了它的语音标记功能，允许开发人员通过同步语音和面部动画来进行唇同步。二是面部跟踪技术。脸谱网展示了今年早些时候的社会虚拟实验。通过机器学习工具从2D照片中推断3D模型。例如，loom.ai通过机器学习将自拍转换为个人3D形象，并以内嵌式自动实现头像的创建过程。它使用公共API和视觉效果(VFX)创建逼真的可视化，然后可以动画化，并用于一系列应用程序。三是人体扫描技术。多伦多的ITSME这样的公司使用全身扫描，并且能够在扫描后的一分钟内创建一个个人的3D形象。它的第一款产品ITSMIJI，允许个性化形象作为表情符号。这些技术也将在消费环境中发挥作用，用户当前不存在于社会虚拟现实中，将能够支持基本的自动化交互。像Furhat机器人公司这样的动画厂商也将扩大业务，提供品牌体验，利用应用于酒店、商店、交通枢纽和其他公共场所的ARVR技术。3.到2022年，增强现实将超过虚拟现实成为头戴式显示器的主流，将占头盔式显示销售的55%，2017年这一比例小于5%头盔显示器将从2016的1,600万台增长到2021年的6,700万台(复合年增长率为33%)。在这个预测期内，ARHMD将增长到这一数量的近一半。由于苹果和谷歌等领先厂商发布了各自的平台ARkit和ARcore，开发者和消费者的关注度有所提高。AR是一组技术集合，提供一种将物理世界与数字信息结合在一起的体验和用户界面。智能手机实际上是消费者的个性化设备。智能手机上的一般消费者AR体验很难获得吸引力，这是因为形式因素(手持)限制了用户的行为。大众市场的用户不愿意不停地拿起手机到他们周围的环境，以获得更多的信息。随着时间的推移，这些设备将成为用户的重要装备，多模态(触摸、手势、语音和运动)和免提交互将成为这些设备的主流功能。消费者将开始转变，从智能手机到HMDS，以更透明、更直观的方式与物理和数字世界互动。计算机视觉等技术将使消费者能够直观地搜索和识别其物理环境。4.到2021年，25%的大型企业将试点并部署混合现实(MR)解决方案，而今天只占1%Gartner预计，到2020年，增强现实和虚拟现实将合并他们的特性和功能。预计，微软的其他技术供应商可能在2018年和2019年推出更便宜的MR硬件。混合现实是市场中的一种沉浸式解决方案，不像AR和VR那么成熟，这项技术是通过一个带有透明镜头的耳机，将3D图形叠加到真实世界的视图上。MR促进了复杂的用户体验，增强了真实世界的视觉覆盖、音频和触觉反馈。混合现实目前处于早期阶段，以航空航天、空间探索、汽车制造、建筑和设计、医疗保健等领域为中心正在进行试点。该技术使企业能够使用复杂的多通道和多视觉体验来桥接物理真实世界和虚幻世界。更自然的是，与3D对象和数字世界进行交互，并提供虚拟和真实环境的更灵活的集成，支持在业务和虚拟现实中更广泛的协作场景。可视化和定制新车、新房子、新的互动游戏、新的购物或娱乐体验(博物馆或旅游目的地)等方面将潜伏商机。5.到2021年，硬件和平台市场的整合将导致苹果、微软和谷歌占据沉浸式解决方案60%的营收沉浸式技术包括增强现实技术、虚拟现实技术和混合现实技术。AR与实际环境相关度较高，VR使得用户被放置到充分的虚拟环境中，MR把虚拟对象插入到实际环境中。谷歌和苹果两家公司，都在积极关注身临其境的体验，并将其带给主流用户。最近推出的ARcore，旨在使AR开发人员能够不需要特殊的深度传感器来操作安卓系统。苹果的关注点是AR，因为苹果已经发布了ARkit(IOS 11的一部分)。iPhoneX也有一个正面的3D深度感测相机，将允许用户查看三维的世界。微软是市场的领导者，它的全息透镜设备正在引领未来。沉浸式设备和技术的现实市场非常分散，就像苹果、微软和谷歌等科技巨头的新兴科技市场一样，如今占据的市场份额不到15%。所有这三家领先的技术供应商都对沉浸式的现实市场抱有很高的期望。他们希望在这些市场上获胜，因为这是用户如何与设备以及物理和虚拟世界互动的一个新兴前沿。目前，AR、VR和MR更像是一个单独的市场，每个市场都有特定的参与者。沉浸式现实解决方案的成功不仅取决于硬件/设备的可用性和能力，还取决于内容、开发人员的支持和生态系统。四、区块链区块链并非灵丹妙药，但是企业需要获得其带来的差异化价值，在技术选择方面保持适当的平衡，确保不错过任何变革的机会。1.到2022年，只有10%的企业将利用区块链技术实现彻底的变革企业如果要充分运用区块链技术，需要对自己现有的业务模式进行解构和变革。因为，区块链技术可以在不需要中间人的情况下发展分散的企业和系统，而今天的大多数系统都是集中的或依赖中间人。其实，运用区块链技术构建集中式系统是可能的，前提是技术经过验证。区块链技术有待进一步成熟，当前企业在技术性能、数据管理、技术集成和可操作性都有较高的需求，这也是渐进发展进化的重要体现。传统企业不擅长利用创新技术进行颠覆，可能不会像初创企业那样热衷于区块链技术。初创企业没有任何体制束缚或路径锁定，可能会最大限度地利用破坏性技术武装自己的商业能力。目前活跃的区块链企业符合这一特征。开发基于区块链的企业管理，需要跨越公司战略、业务流程、风险管理、员工技能、技术投资和管理操作等多个方面。2.到2022年，有超过10亿人可能在没有意识的情况下，将一些数据存储在区块链上几乎所有区块链项目都涉及将数据存储在区块链分布式分类账上。区块链技术有多个方面，包括加密货币令牌、分布式分类账、协商一致机制和智能契约。虽然并不是所有的项目都涉及每个元素，但它们总是将一些数据存储在分布式分类账上。大多数企业还没有准备好替换它们当前的数据存储(通常是关系数据库)，而是进行扩充。区块链被认为是可以存储各种数据，包括身份、交易、交互、事件等数据。区块链炒作几乎在所有行业都很普遍。除了政府之外，许多其他行业也在分析调研区块链，其中许多涉及将客户数据存储在分布式分类账上。虽然金融服务可能是传统行业中第一个进行试验的行业，但它们并不是唯一的，而且人们对其他行业领域的兴趣也越来越大。基于区块链中的创业活动非常活跃，风险投资和天使投资者等都将对区块链产生极大的兴趣。初创企业及其客户都希望将数据存储在分布式分类账上。3.到2022年，至少将有5个国家(包括至少一个G7国家)将发行一种由法定货币支持的加密货币加密货币将有助于集中和分散情况下创建、转移数字资产，并且避免相关花费。目前几乎所有的加密货币都是比特币和其他由初创企业推广的替代货币，而不是通过中央银行或其他金融机构发行的货币。加密货币可以作为另一种支付工具，也可以作为另一种存储价值，但目前加密货币的市场增长主要围绕后者，并有较高的投机成分，所以已经不仅仅是一种支付工具。在部分国家，接受比特币作为替代支付机制已被采纳，与法定货币挂钩的加密货币将减少对加密货币的接受难度，并将提高加密货币的合法性。各国央行对加密货币技术的兴趣一直在上升。4.到2020年，80%的基于区块链的企业的省钱计划将会不切实际目前大部分企业试图基于区块链提高企业的效率，实现过程自动化、流程简单化、技术精炼化，以减少不必要的错误。企业考虑用区块链取代陈旧、封闭、支离破碎的系统，特别是难以维护并具有许多手动过程的系统。这样的企业不需要考虑区块链平台，很少关联区块链的关键方面如密码、分布式账本、协商一致机制或智能合同等。虽然区块链技术提供了独特的好处，但它并不是一个目标明确和成熟的平台，无法在可伸缩性、延迟、互操作性和分析等领域处理企业需求。此外，技术组成部分尚未在规模上得到强化。可以预见，2018年，85%用区块链命名的项目，就是不使用区块链，都会有商业价值产生。Gartner注意到，人们倾向于将区块链作为流程中许多问题的解决方案。虽然区块链似乎是一种适用的技术，但很可能还有其他更适合并准备就绪的技术。我们必须认真细致地评估其他技术的优点。目前，“青春期”的区块链技术要求任何考虑其使用的人，在作出决定之前，都要评估其对功能性和非功能性需求的适用性。此外，大多数区块链项目需要得到生态系统中其他各方的采纳和支持才能取得成功，应该搞清楚需要支持的所有对象、支持动机(或缺乏支持)，以及这样做的可行性，除非及早处理，否则这些问题可能在以后阶段无法解决。五、人工智能和未来的工作从以往的案例来看，科技的重大创新往往导致一些岗位暂时性失业，并且产生过渡期，然后是行业复苏和业务转型。人工智能也是如此，这一过渡期将在2020年左右，与之前的重大创新相比，人工智能领域更早地发出了警告，预见了人工智能对工作可能产生的负面影响。这可能有助于进一步缩短过渡时期，尽管我们在之前的讨论中没有考虑到这种情况。1.在2020年，人工智能作为网络工作“发动机”，将创造230万个工作机会，同时也会消灭180万个工作岗位2020年将是人工智能就业动态的关键年份：AI将在2019年之前减少更多的就业机会(主要是制造业)。从2020年开始，与人工智能相关的就业机会将会正向增长，2025年将达到200万个净新增就业岗位。受人工智能影响的就业岗位数量因行业而异：医疗、公共部门和教育部门的就业需求将持续增长;制造业将受到最严重的冲击;医疗保健提供者、公共部门、银行和证券、通信、媒体和服务、零售和批发贸易将从人工智能中受益，而不会遭受年度净失业。制造业和运输业将遭受较大冲击：到2019年年底，由于人工智能技术的发展，93.8万个制造业岗位将被淘汰;交通运输业将在2020首次实现就业净增长;2018年，全球IT服务公司将有大量的工作岗位流失，新增10万个工作岗位，减少8万个工作岗位。总的来看，Gartner认为人工智能将对工作就业产生积极的影响，就业净增长的主要原因是人工智能本身——其实质是人类与智能的合作关系，两者相辅相成。人工智能对就业的影响在全球范围内处于起步阶段。为了预测人工智能将如何改变就业前景，我们研究了2015年至2025年10年期间，按行业、按国家分列的商业价值创造的地点和方式。从就业动态走势来看，2025年以后新的行业和工作岗位将被创造出来，但它们是难以预见的;就像过去一样，很难预见智能手机、社交网络和广告等新行业的就业机会。可以肯定的是，从2020年开始，与人工智能相关的工作岗位将稳步增长。在2021年，人工智能技术将产生2.9万亿美元的商业价值，并节省62亿小时的人工。从长远来看，人工智能将降低劳动力成本占收入的百分比，但其中一部分收入将继续转化为新的工作。2.在2021年，人工智能技术将产生29,000亿美元的商业价值，并节省62亿小时的人工Gartner的预测显示，人工智能将带来令人震惊的29,000亿美元新的商业价值，以及节省62亿小时的人工。归因于使用人工智能提高了工作效率，创造个性化客户体验，吸引客户参与，并帮助扩大创收机会，并以此作为新的业务模式的一部分，这些新的业务模式是由价值数据驱动的。许多行业的商业价值将会增加，但制造业将会迎来更多的价值机会，因为人工智能而节约成本，消除价值链中的摩擦消耗。制造业主要因为人工智能而大大节约成本，从而带来更多的收入。然而，这只说明了故事的一个方面。虽然人工智能无疑会带来这些收益，但产业不会停滞不前，不会让人工智能成为决定赢家和输家的唯一因素。事实上，外包等行业正在从根本上改变其商业模式，只是依托人工智能寻求更低的成本，新的商业模式和机会显得更为重要。目前，虽然人工智能轻松替代人类承担重复的、普通的劳动，使人类自由从事其他活动，但人类与人工智能的共生关系将更加微妙。对于人类来说，智能需要重新设计，而不是简单地用自动化替代。例如，与模仿人的行为和判断的套路不同的是，整个决策过程可以重构，应该充分利用机器和人的相对优势和弱点，最大限度地产生价值。3.到2022年，1/5从事非常规工作的工人将依靠人工智能来完成他们的工作人工智能被用于高度重复的任务，包括执行大量的观察和分析活动，例如筛查乳房的X线扫描结果，来诊断乳腺癌。但是，将人工智能用于一些日常人们接触较少、类型比较独特的工作，将会产生更好的效益。一些特殊的工种，将受到高度关注。人工智能应用于半常规和非常规认知任务，在训练数据的支撑下，会产生更加有趣的联系，使得现有的工作更加有效。供应商们应该抓住机会，让人工智能通用工具来改进非常规工种。例如，自然语言查询数据集、自动分类内容、重要电子邮件的提醒或即时消息，以及介绍具有类似兴趣的同事等。这些新的潜在工作，将催生积累和分析知识工作的深度改进。应用人工智能技术增强人类认知能力，提高认知任务和决策的质量。4.到2022年，零售商试图使用人工智能取代销售人员的做法，将被证明是不成功的，尽管诸如收银员、运维工作将被打乱多渠道零售有一个复杂的成本结构，有两大驱动因素：商品销售成本和劳动力成本。竞争和投资将产生巨大的驱动力，使几十年来相对不变的任务和流程自动化。利用人工智能和机器人等技术，零售商实现自动化来识别、优化密集型和重复性劳动。目前，零售商正在扩大对技术的使用，以改进店内财务流程，比如自助结帐，这已经超出了杂货店和大商场零售商的范围，进入了便利店等领域。一些零售公司，如ahold，正在向消费者提供扫描设备，供他们在装载购物车时使用，从而进一步为结账的消费者提供了无缝交易。沃尔玛目前正在测试一种由客户的移动设备启用的扫描和关闭过程。一些零售商正在试验机器人解决方案的客户服务，积极实验虚拟客户服务助理。尽管所有这些努力都将继续，但研究表明，所有年龄层的消费者在逛商店时仍更愿意与知识渊博的销售助理互动。这一需求在家庭装修、药店和化妆品等专业领域比较明显，零售商会发现很难消除传统的销售关联功能。到2020年，人工智能及算法将促使前10名零售商削减多达1/3的总部采购人员。到2020年，至少有一家大型多渠道零售商将尝试一个完全自动化的、无关联的物理存储场地。到2020年，将有50%的零售客户服务至少部分通过对话式的人工智能应用程序进行处理。【科技开放平台】将挖掘企业更多发展价值与成长空间，帮助企业实现创新转型升级，欢迎咨询沟通。"}
{"content2":"上世纪60年代, Marvin Minsky 在MIT让他的本科学生 Gerald Jay Sussman用一个暑假的时间完成一个有趣的Project: “link a camera to a computer and get the computer to describe what it saw”。从那时开始，特别是David Marr教授于1977年正式提出视觉计算理论，计算机视觉已经走过了四十多年的历史。可是，从今天看来，这个已入不惑之年的学科，依然显得如此年轻而朝气蓬勃。在它几十年的发展历程中，多种流派的方法都曾各领风骚于一时。最近二十年中，计算机视觉发展最鲜明的特征就是机器学习与概率模型的广泛应用。在这里，我简单回顾一下对这个领域产生了重要影响的几个里程碑：● 1984年：Stuart Geman和Donald Geman发表了一篇先驱性的论文：Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images. 在这篇文章里，两位Geman先生引入了一系列对计算机视觉以后的发展具有深远影响的概念和方法：Markov Random Field (MRF),  Gibbs Sampling，以及Maximum a Posteriori estimate (MAP estimate)。这篇论文的意义是超前于时代的，它所建立的这一系列方法直到90年代中后期才开始被广泛关注。● 1991年：Matthew Turk和Alex Pentland使用Eigenface进行人脸分类。从此，以矩阵的代数分解为基础的方法在视觉分析中被大量运用。其中有代表性的方法包括PCA, LDA，以及ICA。● 1995年：Corinna Cortes和Vladimir Vapnik提出带有soft margin的Support Vector Machine (SVM)以及它的Kernel版本，并用它对手写数字进行分类。从此，SVM大受欢迎，并成为各种应用中的基准分类器。● 1996年：Bruno Olshausen 和David Field 提出使用Overcomplete basis对图像进行稀疏编码(Sparse coding)。这个方向在初期的反响并不热烈。直到近些年，Compressed Sensing在信号处理领域成为炙手可热的方向。Sparse coding 在这一热潮的带动下，成为视觉领域一个活跃的研究方向。● 90年代末：Graphical Model和Variational Inference逐步发展成熟。1998年，MIT出版社出版了由Michale Jordan主编的文集：Learning in Graphical Models。 这部书总结了那一时期关于Graphical Model的建模，分析和推断的主要成果——这些成果为Graphical Model在人工智能的各个领域的应用提供了方法论基础。进入21世纪，Graphical Model和Bayesian方法在视觉研究中的运用出现了井喷式的增长。● 2001年：John Lafferty和Andrew McCallum等提出Conditional Random Field (CRF)。CRF为结构化的分类和预测提供了一种通用的工具。此后，语义结构开始被运用于视觉场景分析。● 2003年：David Blei等提出Latent Dirichlet Allocation。2004年：Yee Whye Teh 等提出Hierarchical Dirichlet Process。各种参数化或者非参数化的Topic Model在此后不久被广泛用于语义层面的场景分析。● 虽然Yahn Lecun等人在1993年已提出Convolutional Neural Network，但在vision中的应用效果一直欠佳。时至2006年，Geoffrey Hinton等人提出Deep Belief Network进行layer-wise的pretraining，应用效果取得突破性进展，其与之后Ruslan Salakhutdinov提出的Deep Boltzmann Machine重新点燃了视觉领域对于Neural Network和Boltzmann Machine的热情。时间进入2013年，Probabilistic Graphical Model早已成为视觉领域中一种基本的建模工具。Probabilistic Graphical Model的研究涉及非常多的方面。 限于篇幅，在本文中，我只能简要介绍其中几个重要的方面，希望能为大家提供一些有用的参考。Graphical Model的基本类型基本的Graphical Model 可以大致分为两个类别：贝叶斯网络(Bayesian Network)和马尔可夫随机场(Markov Random Field)。它们的主要区别在于采用不同类型的图来表达变量之间的关系：贝叶斯网络采用有向无环图(Directed Acyclic Graph)来表达因果关系，马尔可夫随机场则采用无向图(Undirected Graph)来表达变量间的相互作用。这种结构上的区别导致了它们在建模和推断方面的一系列微妙的差异。一般来说，贝叶斯网络中每一个节点都对应于一个先验概率分布或者条件概率分布，因此整体的联合分布可以直接分解为所有单个节点所对应的分布的乘积。而对于马尔可夫场，由于变量之间没有明确的因果关系，它的联合概率分布通常会表达为一系列势函数（potential function）的乘积。通常情况下，这些乘积的积分并不等于1，因此，还要对其进行归一化才能形成一个有效的概率分布——这一点往往在实际应用中给参数估计造成非常大的困难。值得一提的是，贝叶斯网络和马尔可夫随机场的分类主要是为了研究和学习的便利。在实际应用中所使用的模型在很多时候是它们的某种形式的结合。比如，一个马尔可夫随机场可以作为整体成为一个更大的贝叶斯网络的节点，又或者，多个贝叶斯网络可以通过马尔可夫随机场联系起来。这种混合型的模型提供了更丰富的表达结构，同时也会给模型的推断和估计带来新的挑战。 Graphical Model的新发展方向在传统的Graphical Model的应用中，模型的设计者需要在设计阶段就固定整个模型的结构，比如它要使用哪些节点，它们相互之间如何关联等等。但是，在实际问题中，选择合适的模型结构往往是非常困难的——因为，我们在很多时候其实并不清楚数据的实际结构。为了解决这个问题，人们开始探索一种新的建立概率模型的方式——结构学习。在这种方法中，模型的结构在设计的阶段并不完全固定。设计者通常只需要设定模型结构所需要遵循的约束，然后再从模型学习的过程中同时推断出模型的实际结构。 结构学习直到今天仍然是机器学习中一个极具挑战性的方向。结构学习并没有固定的形式，不同的研究者往往会采取不同的途径。比如，结构学习中一个非常重要的问题，就是如何去发现变量之间的内部关联。对于这个问题，人们提出了多种截然不同的方法：比如，你可以先建立一个完全图连接所有的变量，然后选择一个子图来描述它们的实际结构，又或者，你可以引入潜在节点(latent node)来建立变量之间的关联。 Probabilistic Graphical Model的另外一个重要的发展方向是非参数化。与传统的参数化方法不同，非参数化方法是一种更为灵活的建模方式——非参数化模型的大小（比如节点的数量）可以随着数据的变化而变化。一个典型的非参数化模型就是基于狄利克莱过程(Dirichlet Process)的混合模型。这种模型引入狄利克莱过程作为部件(component)参数的先验分布，从而允许混合体中可以有任意多个部件。这从根本上克服了传统的有限混合模型中的一个难题，就是确定部件的数量。在近几年的文章中，非参数化模型开始被用于特征学习。在这方面，比较有代表性的工作就是基于Hierarchical Beta Process来学习不定数量的特征。基于Graphical Model 的统计推断 (Inference)完成模型的设计之后，下一步就是通过一定的算法从数据中去估计模型的参数，或推断我们感兴趣的其它未知变量的值。在贝叶斯方法中，模型的参数也通常被视为变量，它们和普通的变量并没有根本的区别。因此，参数估计也可以被视为是统计推断的一种特例。除了最简单的一些模型，统计推断在计算上是非常困难的。一般而言，确切推断(exact inference)的复杂度取决于模型的tree width。对于很多实际模型，这个复杂度可能随着问题规模增长而指数增长。于是，人们退而求其次，转而探索具有多项式复杂度的近似推断(approximate inference)方法。 主流的近似推断方法有三种：(1)基于平均场逼近(mean field approximation)的variational inference。这种方法通常用于由Exponential family distribution所组成的贝叶斯网络。其基本思想就是引入一个computationally tractable的upper bound逼近原模型的log partition function，从而有效地降低了优化的复杂度。大家所熟悉的EM算法就属于这类型算法的一种特例。(2)Belief propagation。这种方法最初由Judea Pearl提出用于树状结构的统计推断。后来人们直接把这种算法用于带环的模型（忽略掉它本来对树状结构的要求）——在很多情况下仍然取得不错的实际效果，这就是loop belief propagation。在进一步的探索的过程中，人们发现了它与Bethe approximation的关系，并由此逐步建立起了对loopy belief propagation的理论解释，以及刻画出它在各种设定下的收敛条件。值得一提的是，由于Judea Pearl对人工智能和因果关系推断方法上的根本性贡献，他在2011年获得了计算机科学领域的最高奖——图灵奖。基于message passing的方法在最近十年有很多新的发展。Martin Wainwright在2003年提出Tree-reweighted message passing，这种方法采用mixture of trees来逼近任意的graphical model，并利用mixture coefficient和edge probability之间的对偶关系建立了一种新的message passing的方法。这种方法是对belief propagation的推广。Jason Johnson等人在2005年建立的walk sum analysis为高斯马尔可夫随机场上的belief propagation提供了系统的分析方法。这种方法成功刻画了belief propagation在高斯场上的收敛条件，也是后来提出的多种改进型的belief propagation的理论依据。Thomas Minka在他PhD期间所建立的expectation propagation也是belief propagation的在一般Graphical Model上的重要推广。(3)蒙特卡罗采样(Monte Carlo sampling)。与基于优化的方法不同，蒙特卡罗方法通过对概率模型的随机模拟运行来收集样本，然后通过收集到的样本来估计变量的统计特性（比如，均值）。采样方法有三个方面的重要优点。第一，它提供了一种有严谨数学基础的方法来逼近概率计算中经常出现的积分（积分计算的复杂度随着空间维度的提高呈几何增长）。第二，采样过程最终获得的是整个联合分布的样本集，而不仅仅是对某些参数或者变量值的最优估计。这个样本集近似地提供了对整个分布的更全面的刻画。比如，你可以计算任意两个变量的相关系数。第三，它的渐近特性通常可以被严格证明。对于复杂的模型，由variational inference或者belief propagation所获得的解一般并不能保证是对问题的全局最优解。在大部分情况下，甚至无法了解它和最优解的距离有多远。如果使用采样，只要时间足够长，是可以任意逼近真实的分布的。而且采样过程的复杂度往往较为容易获得理论上的保证。 蒙特卡罗方法本身也是现代统计学中一个非常重要的分支。对它的研究在过去几十年来一直非常活跃。在机器学习领域中，常见的采样方法包括Gibbs Sampling, Metropolis-Hasting Sampling (M-H),  Importance Sampling, Slice Sampling, 以及Hamiltonian Monte Carlo。其中，Gibbs Sampling由于可以纳入M-H方法中解释而通常被视为M-H的特例——虽然它们最初的motivation是不一样的。Graphical Model以及与它相关的probabilistic inference是一个非常博大的领域，远非本文所能涵盖。在这篇文章中，我只能蜻蜓点水般地介绍了其中一些我较为熟悉的方面，希望能给在这方面有兴趣的朋友一点参考.非参数化的模型还有其它哪些优势？ 林达华老师：非参数化模型确实引入了其它参数，比如concentration parameter。但是，这个参数和component的个数在实用中是有着不同的影响的。concentration parameter主要传达的是使用者希望形成的聚类粒度。举个简单的例子，比如一组数据存在3个大类，每个大类中有3个相对靠近的子类。这种情况下，聚成3类或者9类都是合理的解。如果concentration parameter设得比较大，最后的结果可能形成9类，如果设得比较小，则可能形成3类。但是，如果人为地固定类数，则很可能导致不合理的结果。 需要强调的是非参数化贝叶斯方法是一个非常博大的方向，目前的研究只是处于起步阶段。而Dirichlet Process mixture model只是非参数方法的一个具体应用。事实上，DP像Gauss distribution一样，都是一种有着良好数学性质的过程（分布），但是它们在实用中都过于理想化了。目前的一个新的研究方向就是建立更为贴近实际的非参数化过程。相比于传统参数化方法而言，非参数化方法的主要优势是允许模型的结构在学习的过程中动态变化（而不仅仅是组件的数量），这种灵活性对于描述处于不断变化中的数据非常重要。当然，如何在更复杂的模型中应用非参数化方法是一个比较新的课题，有很多值得进一步探索的地方。 「SIGVC BBS」：文中后面提到的结构学习是不是这两年比较火的Structured Output Prediction呢？他们的关系如何？Structured Percepton和Structured SVM应该就是属于这个大类吗？结构学习的输出是树结构和图结构吗？结构学习与图像的层次分割或者层次聚类有关系吗？林达华老师：Structured Prediction (e.g. Structured SVM) 其实属于利用结构，而不是我在文中所指结构学习。在大部分Structured Prediction的应用中，结构是预先固定的（比如哪些变量要用potential联系在一起），学习的过程其实只是优化待定的参数。尽管如此，这些工作本身是非常有价值的，在很多问题中都取得了不错的效果。我在文中所提到的结构学习是指连结构本身都是不固定的，需要从数据中去学习。一般情况下，学习输出的是图或者树的结构（以及相关参数）。这个topic其实历史很长了，早期的代表性工作就是chow-liu tree。这是一种利用信息量计算寻找最优树结构来描述数据的算法。Alan Willsky的小组近几年在这个方向取得了很多进展。但是，总体而言这个方向仍旧非常困难，大部分工作属于探索性的，并不特别成熟。目前在Vision中的应用不是特别广泛。但是，我相信，随着一些方法逐步成熟，进入实用阶段，它的应用前景是非常不错的。 「SIGVC BBS」：文中提到了Convolutional Deep Network、Deep Belief Network、Deep Boltzmann Machine等近年炙手可热的神经网络方法。那么，神经网络和概率图模型是不是本质上完全是一回事，只是观察角度和历史发展不同？感觉它们很多地方都很相似。深度学习里RBM学习的训练算法与概率图模型的学习推理算法有什么联系和区别吗？他们的结构模型有什么联系和区别吗？ 林达华老师：这两类模型所使用的数学方法是非常不同的。Graphical model的很多推断和学习方法都有很深的数学根基。通过近十几年的努力，大家已经逐步建立起整套的方法论体系对相关算法进行分析。Deep Learning目前并没有什么有效的分析方法。Deep learning取得很好的性能，其中很多技巧性的方法(trick)起到了重要作用。至于为什么这些trick能导致更好的性能，目前还未能有一个很好的解释。 我个人看来，这些技巧其实是很有价值的：一方面，它们确实在实践中提高了性能；另外一方面，它们为理论上的探索提出了问题。但是，我觉得，有效回答这些问题需要新的数学工具（新的数学分析方法），这看来不是近期内能做到的。 「SIGVC BBS」：在一些论文中看到，采样的方法（如Gibbs采样）也有其缺点，一个是计算量比较大（computationally intensive），另一个是收敛检测比较难。不知道这些说法是否有道理，或者目前这些问题是否有得到解决？ 林达华老师：这里提到的两个问题确实是Sampling的两个主要的困难。对于这些问题，过去几十年取得了很多进展，提出了很多新的采样方法，但是困难仍然很大。但是，采样能提供整个分布的信息，而且有渐近(asymptotic)的理论保证。这在很多情况下是一般的optimization方法做不到的。最近有新的研究尝试结合Sampling和Optimization，在特定问题上有一些有趣的结果——比如，George Papandreou的Perturb-and-MAP. 「SIGVC BBS」：在计算机视觉中，视觉目标跟踪问题已经用到了动态贝叶斯网络方法。一些最近发表的自然图像分割方法也用到LDA（Latent Dirichlet Allocation）。在受限的理想数据条件下，这些方法都取得了较好的结果。但是，不得不承认，我们在研究和应用的过程中，在心理上首先对应用概率图模型有所畏惧（这里除我们已经用得较多较熟悉的MRF、CRF和Dynamic Bayesian network based visual tracking—condensation之外）。主要的解释可能有：一方面，它不象很多正则化方法那样其细节能被自我掌握、观测和控制；另一方面，对于一个新的问题，我们需要不停地问自己：什么样的设计（图）是最好的。从而，在很多情况下，我们更愿意选择使用那些正则化方法。比如，对小规模人脸识别，我们会选择PCA＋LAD（SVM），对大一点的规模我们会考虑“特征选择＋adaboost”框架。就计算机视觉，能否从实践的角度给我们一点关于使用概率图模型的建议。另外，在计算机视觉中，什么样的问题更适合于采用概率图模型方法来解决。 林达华老师：首先，Graphical model和其它的方法一样，只是一种数学工具。对于解决问题而言，最重要的是选择合适的工具，而不一定要选看上去高深的方法。对于普通的分类问题，传统的SVM, Boost仍不失为最有效的方法。 Graphical model通常应用在问题本身带有多个相互联系的变量的时候。这个时候Graphical model提供了一种表达方式让你去表达这些联系。我觉得并不必要去寻求最优的设计图，事实上，没有人知道什么样的图才是最优的。实践中，我们通常是根据问题本身建立一个能比较自然地表达问题结构的图，然后通过实验了验证这个图是不是合适的。如果不合适，可以根据结果分析原因对图做出修正。 举个具体的例子，比如对一个比赛视频进行分析。那么可能涉及多个变量：摄像机的角度，背景，运动员的动作等等。那么这个问题可能就设计多个未知变量的推断，这些变量间可能存在各种联系。这个时候，Graphical model可能就是一种合适的选择。 值得注意的是，选择合适的图有时候也需要一些经验。比如分布的选择上要注意形成conjugate，这样往往容易得到简易的推断公式。了解各种分布的特性以及它们可能对最后结果的影响也是有帮助的。"}
{"content2":"Michael I. Jordan是知名的计算机科学和统计学学者，主要研究机器学习和人工智能。目前担任加州大学伯克利分校电机工程与计算机系和统计学系教授。他的许多学生包括吴恩达、邢波、Zoubin Ghahramani、 Tommi Jaakkola等，现在也已经成为机器学习领域的重要学者。一、智能增强技术已经融入到我们生活的方方面面八九十年代出现另外一个趋势：智能增强技术，机器人通过搜索引擎帮我们找到想要的答案，帮助人类有了更好的存储、沟通、交流能力。另一个发展趋势是智能基础设施（IaaS），我们身边的每一个行业、每一个模块，现在都出现了智能化的趋势，我们也发现世界更了解我们了，能够根据我们的需求提供服务。二、现在的人工智能没有你想象的那么智能机器人目前只能将语音和文字进行相互转换，而无法了解声音和文字背后的含义，所以机器人无法清晰的认识到它所处于的环境。机器人的沟通能力应该突破语音识别的框架往更深的层次发展。医疗行业中，机器做很多的医学诊断是不太可能的，机器准断可能会剂量出现问题，特别是某种竞争的情况下，如果出现任何问题，这个机器没有办法做出有效的诊断，我们的病人都有可能去世。三、人工智能将带来经济、安全、信息安全等问题随着人工智能的发展，机器人可能会造成大量的工作流失，致使越来越多的人没有办法得到收入。另一方面，智能机器人本身是没有恶意去伤害人类的，而是有些人想要利用人工智能去做坏事。预防人类利用人工智能犯罪也是将来要面对的一个严峻的问题。从技术方面，云的数据越庞大，机器就会变得越智能，但是实现信息共享还需要克服很多困难，共享信息带来的隐私泄露问题，目前还没有很好的解决方案。这意味着人工智能的发展速度也会因此的减慢.首先我们简单了解一下到底目前人工智能行业发展是什么样的。在60年代刚刚出现了“智能”这个词，也是刚刚出现了人工智能这个说法，那时候我们说要建立一个机器人，让它可以和人一样思维，加入到人的世界当中来，那个时候大部分人工智能的电影向大家展示的是机器人最终进入到人的世界中，以及包括我们的云系统、视觉系统，还有我们的自然语言系统，能够让机器人越来越像一个人。但是 在80年代、90年代出现了另外一个趋势，这个趋势对我们来说也是非常重要的，我们叫做IA，也就是智能增强技术 ，那时候我们提到搜索引擎，这也是我们的智能方面的应用，通过智能引擎，我们可以非常快的找到我们所要问的任何问题的答案，这些东西不需要存储在人的大脑中，所以人的智能得到了引擎的支持，帮助我们能够更好地用自然语言来进行增强，电脑可以帮助我通过自然语言的处理，增强我的自然语言的表现，所以我可以通过自然和科技的技术，以及智能技术说多种语言。还有一个部分是IaaS，也就是智能基础设施，这对我们来说是最重要的，现在我们的交通和金融行业，在我们身边的每一个行业、每一个模块，现在都出现了智能化的趋势，我们也发现世界更了解我们了，能够根据我们的需求提供服务，所以在我们前方是有一个系统的，如果你要说云的话，这个系统就是云，这个云变得更加智能，所以它并不是机器人和我们沟通，而是这个云的架构和云的基础设施在和我们沟通。之前我们大部分的研发都是与机器人，以及包括智能领域的发展，它主要是制约与我们的技术发展，它和人的发展是非常相似的，但是智能是完全不同的。现在我们在这种所谓的智能设施的建立的时候，我们遇到了很多问题，在腾讯也是如此。比如说我们要对相关的大型的设施做出相应的决定，比如说 我们要做一个金融、交通，以及包括对人类做出一些医疗决定的时候，作为一个单独的机器，如果要能够仅仅跟周围的信息做决策，这是很不好的，有时候机器了解的信息是不够的，一个机器做出的决策往往是不对的，它没办法意识到我们周围环境的变化 。特别是这样的决策如果要影响到大部分人，它更是危险的。下面回到我们的机器人、智能发展，包括从人工智能的角度来看，我们看看哪些是可能的，哪些是不可能的。我们看到机器视觉，在过去几年，我们通过摄像机对场景中的物体进行标识，但是它还是没有办法能够像我们清晰的了解到所有的情况，就像我在这里站在台上，大家在台下，我没办法了解到所有人的注意力在哪里，通过人工智能可以帮我们更好地了解语义，但是现在也没办法做到，语音识别也是如此，我们现在可以把语音转化成文字，文字也可以转换成语音，在各种语言上都可以实现，但是我们的机器人还没办法帮我们了解听觉、视觉之后的真正的意义。还有一点就是自然语言的处理，我们可以看到到目前为止 ，自然语言的处理得结果还没有达到我们需要的发展，我们现在有大量的语言的翻译，但是大部分的语言和语句因为没有办法得到有效的语义的阐述 ，没办法让我们的受众了解到这个语义的意思，有时候我们问问题仅仅能了解部分的答案，而不能了解全部的答案，对机器人学来说也是如此，我们看到世界上有很多工业可编程的机器人，他们也在和我们沟通，但是它们没办法了解到我们的环境、处境以及我们的情绪，我相信这对我们大家来说，如果我们都觉得机器智能将会无处不在的话，这是不太可能的。相信在未来，短期内不会出现太多的像人这样的灵活性和可变化性。也许机器可以了解一些事实，它们看上去非常有知识，但是它们没法真正得到人这样一种高级智能，甚至像小孩一样的高级智能，它没有办法了解抽象思维，没有办法进行抽象的处理，机器人还不能实现这方面的能力。这些机器人就像小孩一样，他们知道一些非常棒的现实，他们知道每条河流、每个国家，但是它们仍然没有很高的智能进行人的抗衡，所以在这方面，我们还是很难看到一个超人类的发展，我们相信这个技术可能要很多年的发展才能够出现。我相信我们真正要关注的不仅仅只是这样一种技术的发展，到目前为止，在我们这代人身上还看不到这种高水平的人工智能的出现。除此之外，我们即使没有办法进行抽象、识别语义，我们也是非常难接近人的发展的，但是我们仍然要进行等待，让我们了解到通过大量的数据的处理，比如说机器人以及人工智能可以帮助我们大批次的处理数据，能够通过数据了解未来一些事件的走向，同时能够保证我们的数据结果不断地提高，同时我们还可以用这个机器人做一些简单的人工工作的处理，但是机器人永远不可能像人这样聪明，同时我们可以看到，我们的人工智能的系统也会有很多的智能，它们知道这个现实，但它们不知道哪些现实是真的，哪些是有可能出现未来的一些颠覆式的发展，所以这个机器人并没有办法实现像人一样的能力，它没有办法引领一个公司的发展，在我们这代人身上，在机器上没法做出这样一个前景化的决定。比如说在医疗行业中，我们让机器做很多的医学诊断，这是不太可能的，有很多人会因为这种不畅的诊断，可能会剂量出现问题，特别是某种竞争的情况下，如果出现任何问题，这个机器没有办法做出有效的诊断，我们的病人都有可能去世。与此同时，我们真正要关注的是机器人可能会造成大量的工作的流失，以及大多数人因为丢了工作没有办法得到收入。在过去我们可以看到工业的发展，在七八十年代都是如此，但是在过去50年中，人们在不断地调整，现在我们可以看到未来10到20年，人们没有机会更多的调整，机器人会取代更多的人，获得更多的工作。同时它还可以帮助现有的智能设备的发展，在世界上也有很多人会恶意使用人工智能的系统，如果出现人工智能系统的误用，我相信也会有问题。机器人本身是没有任何恶意要伤害人类的，只是使用这些及其人的人本身含有恶意。如果机器做出了一个决定，我们必须要让机器向我们阐释为什么做这样的决定，是否还有其它的潜在方法。还有我们要找到问题发生的原因。还有一点是实时，我们可以看到很多的数据和机器需要花几天、几个小时来学习这些数据，但是到目前为止，我们的机器学习方面还没有办法能够达到真正的实时操作。这是我们所面临的技术挑战，是需要我们关注的，我们只是做AI，让这个机器人能够跨过去，或者做计算机视觉，我们需要像工程师一样解决一些问题。当然还有云端的互动，这也是挑战非常大的，如果把这个数据放到云上，你需要关注隐私的问题，要看一下实施的问题，同时你还要考虑现实的情况，有时候它可能离我们太远，它不一定是和事实一样的，我们有可能会做出错误的决定，所以我们现在要有更好的方案。当然还有一个不确定性，这也是人类的一个非常重要的特点，围棋的比赛其实并不是一个很好的例子，因为你知道棋盘上的东西，但是人的生活有很多不确定性，比如说我不知道今天会发生什么事情，我不知道将来会发生什么，这就是所谓人的一生，这和围棋是不一样的，所以我们需要解决更深层次的人工智能方面的问题。_________摘自《推酷》          陶艳杰总结：人工智能在计算机领域内，得到了愈加广泛的重视。并在机器人，经济政治决策，控制系统，仿真系统中得到应用人工智能是计算机学科的一个分支，二十世纪七十年代以来被称为世界三大尖端技术之一（空间技术、能源技术、人工智能）。也被认为是二十一世纪三大尖端技术（基因工程、纳米科学、人工智能）之一。这是因为近三十年来它获得了迅速的发展，在很多学科领域都获得了广泛应用，并取得了丰硕的成果，人工智能已逐步成为一个独立的分支，无论在理论和实践上都已自成一个系统。人工智能是研究使计算机来模拟人的某些思维过程和智能行为（如学习、推理、思考、规划等）的学科，主要包括计算机实现智能的原理、制造类似于人脑智能的计算机，使计算机能实现更高层次的应用。人工智能将涉及到计算机科学、心理学、哲学和语言学等学科。可以说几乎是自然科学和社会科学的所有学科，其范围已远远超出了计算机科学的范畴，人工智能与思维科学的关系是实践和理论的关系，人工智能是处于思维科学的技术应用层次，是它的一个应用分支。从思维观点看，人工智能不仅限于逻辑思维，要考虑形象思维、灵感思维才能促进人工智能的突破性的发展，数学常被认为是多种学科的基础科学，数学也进入语言、思维领域，人工智能学科也必须借用数学工具，数学不仅在标准逻辑、模糊数学等范围发挥作用，数学进入人工智能学科，它们将互相促进而更快地发展。"}
{"content2":"Python 是Web 开发、游戏脚本、计算机视觉、物联网管理和机器人开发的主流语言之一，随着Python用户可以预期的增长，它还有机会在多个领域里登顶。Python学习路线分享给你。阶段一是Python语言（用时5周，包括基础语法、面向对象、高级课程、经典课程）；阶段二是Linux初级（用时1周，包括Linux系统基本指令、常用服务安装）；阶段三是Web开发之Diango（5周+2周前端+3周diango）；阶段四是Web开发之Flask（用时2周）；阶段五是Web框架之Tornado（用时1周）；阶段六是docker容器及服务发现（用时2周）；阶段七是爬虫（用时2周）；阶段八是数据挖掘和人工智能（用时3周）。Python编写代码的速度非常快，而且非常注重代码的可读性，非常适合多人参与的项目。所以如果你想尝试成为程序员，Python将会是一个重要的选择。而谈到发展方向，就我所知的有以下几个方向：一是Web开发如Flask、Django、Tornado等等，需要良好的文档阅读能力。二是渗透测试陡峭的学习曲线，需要其它如汇编、计算机网络、数据结构等基础知识。推荐书籍：《Python灰帽子》、《Python黑帽子》等。三是数据挖掘/大数据据说还需要学习其它语言，如R语言等。推荐从Scrapy入手。四是人工智能这个要求就比较高啦，大家一定要做好心理准备。有些不经过培训的Python工程师经常说不需要培训，那是因为还没有到错失机会的时候。当你错失了毕业前的机会，或者因为自己没有好好学习，而与大好机会失之交臂时就会后悔莫及了。"}
{"content2":"行人检测资源（上）综述文献 http://www.cvrobot.net/pedestrian-detection-resource-1-summary-review-survey/行人检测资源（下）代码数据 http://www.cvrobot.net/pedestrian-detection-resource-2-code-and-dataset/行人检测具有极其广泛的应用：智能辅助驾驶，智能监控，行人分析以及智能机器人等领域。从2005年以来行人检测进入了一个快速的发展阶段，但是也存在很多问题还有待解决，主要还是在性能和速度方面还不能达到一个权衡。近年，以谷歌为首的自动驾驶技术的研发正如火如荼的进行，这也迫切需要能对行人进行快速有效的检测，以保证自动驾驶期间对行人的安全不会产生威胁。1   行人检测的现状大概可以分为两类1.1    基于背景建模利用背景建模方法，提取出前景运动的目标，在目标区域内进行特征提取，然后利用分类器进行分类，判断是否包含行人；背景建模目前主要存在的问题：1）必须适应环境的变化（比如光照的变化造成图像色度的变化）；2）相机抖动引起画面的抖动(比如手持相机拍照时候的移动)；3）图像中密集出现的物体（比如树叶或树干等密集出现的物体，要正确的检测出来）；4）必须能够正确的检测出背景物体的改变（比如新停下的车必须及时的归为背景物体，而有静止开始移动的物体也需要及时的检测出来）。5）物体检测中往往会出现Ghost区域，Ghost区域也就是指当一个原本静止的物体开始运动，背静差检测算法可能会将原来该物体所覆盖的区域错误的检测为运动的，这块区域就成为Ghost，当然原来运动的物体变为静止的也会引入Ghost区域，Ghost区域在检测中必须被尽快的消除。1.2    基于统计学习的方法这也是目前行人检测最常用的方法，根据大量的样本构建行人检测分类器。提取的特征主要有目标的灰度、边缘、纹理、颜色、梯度直方图等信息。分类器主要包括神经网络、SVM、adaboost以及现在被计算机视觉视为宠儿的深度学习。统计学习目前存在的难点：1）行人的姿态、服饰各不相同、复杂的背景、不同的行人尺度以及不同的光照环境。2）提取的特征在特征空间中的分布不够紧凑；3）分类器的性能受训练样本的影响较大；4）离线训练时的负样本无法涵盖所有真实应用场景的情况；目前的行人检测基本上都是基于法国研究人员Dalal在2005的CVPR发表的HOG+SVM的行人检测算法(Histograms of Oriented Gradients for Human Detection, Navneet Dalel,Bill Triggs, CVPR2005)。HOG+SVM作为经典算法也集成到opencv里面去了，可以直接调用实现行人检测为了解决速度问题可以采用背景差分法的统计学习行人检测，前提是背景建模的方法足够有效（即效果好速度快），目前获得比较好的检测效果的方法通常采用多特征融合的方法以及级联分类器。（常用的特征有Harry-like、Hog特征、LBP特征、Edgelet特征、CSS特征、COV特征、积分通道特征以及CENTRIST特征）。2    综述类的文章2.1    行人检测十年回顾Ten Years of Pedestrian Detection, What Have We Learned?一篇2014年ECCV的文章，是对pedestrian detectiond过去十年发展的回顾，从dataset，main approaches的角度分析了近10年的40多篇论文提出的方法，并对提高feature复杂度的影响进行了评估下载：http://rodrigob.github.io/documents/2014_eccvw_ten_years_of_pedestrian_detection_with_supplementary_material.pdf学习笔记：http://blog.csdn.net/mduke/article/details/465824432.2    P.Dollar  PAMI 2012上的综述P.Dollar, C. Wojek,B. Schiele, et al. Pedestrian detection: an evaluation of the state of the art [J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34(4): 743-761.2012年PAMI上发表的一篇关于行人检测的综述性文章，PDF格式，共20页，对常见的16种行人检测算法进行了简单描述，并在6个公开测试库上进行测试，给出了各种方法的优缺点及适用情况。另外，指出了未来行人检测的发展方向和趋势。下载：http://vision.ucsd.edu/~pdollar/files/papers/DollarPAMI12peds.pdf2.3    CVPR2010 HOF和CSShttps://www.d2.mpi-inf.mpg.de/CVPR10PedestriansNew Features and Insights for Pedestrian Detection文中使用改进的HOG，即HOF和CSS（color self similarity）特征，使用HIK SVM分类器。 本文的作者是德国人：Stefen Walk。目前Stefan Walk在苏黎世联邦理工大学任教。2.4    Integral Channel Features加州理工学院2009年行人检测的文章：Integral Channel Features（积分通道特征）这篇文章与2012年PAMI综述文章是同一作者。作者：Piotr DollarPaper下载：http://pages.ucsd.edu/~ztu/publication/dollarBMVC09ChnFtrs_0.pdf中文笔记：http://blog.csdn.net/carson2005/article/details/84558372.5    The Fastest Pedestrian Detector in the WestDollar 在 2010 年 BMVC 的 《The fastest pedestrian detector in the west》 一文中提出了一种新的思想，这种思想只需要训练一个标准 model,检测 N/K（K ≈10） 然后其余的 N-N/K 种大小的图片的特征不需要再进行这种复杂的计算，而是跟据这 N/K 次的结果， 由另外一种简单的算法给估计出来，这种思想实现的 基础是大小相近的图像的特征可以被足够精确的估计出来下载：http://vision.ucsd.edu/sites/default/files/FPDW_0.pdf2.6    DPM算法做目标检测CVPR2008：A Discriminatively Trained, Multiscale, Deformable Part ModelPAMI2010：Object Detection with Discriminatively Trained Part Based ModelsCVPR2010：Cascade Object Detection with Deformable Part Models以上三篇文章，都是作者研究DPM算法做目标检测的文章，有源代码可以下载。作者的个人主页：http://cs.brown.edu/~pff/papers/2.7    利用DPM模型，检测粘连Detection and Tracking of Occluded PeopleIJCV2014年的文章，利用DPM模型，检测粘连情况很严重的行人，效果很好。下载：http://www.bmva.org/bmvc/2012/BMVC/paper009/2.8    UDN算法ICCV2013：1）Joint Deep Learning for Pedestrian Detection2）Multi-Stage Contextual Deep Learning for Pedestrian Detection简 称UDN算法，从文中描述的检测效果来看，该方法是所有方法中最好的，并且，效果远超过其他方法。经过对论文和该算法源码的研究，该算法是与作者另外一篇 论文的方法 ，另外的论文算法做图片扫描，得到矩形框，然后用该方法对矩形框进行进一步确认，以及降低误警率和漏警率。另外的论文是：Multi-Stage Contextual Deep Learning for Pedestrian Detection。这篇文章是用深度学习的CNN做candidate window的确认。而主要的行人检测的算法还是HOG+CSS+adaboost。香港中文大学，Joint Deep Learning for Pedestrian Detection，行人检测论文的相关资源：http://www.ee.cuhk.edu.hk/~wlouyang/projects/ouyangWiccv13Joint/index.html2.9    Monocular pedestrian detectionEnzweiler, and D.Gavrila. Monocular pedestrian detection: survey and experiments [J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2009,  31(12): 2179-2195.下载：http://www.gavrila.net/pami09.pdf2.10       Survey of pedestrian detection for advanced driver assistance systemsGeronimo, A. M.Lopez and A. D. Sappa, et al. Survey of pedestrian detection for advanced driver assistance systems [J].  IEEE Transactionson Pattern Analysis and Machine Intelligence,  2010, 32(7): 1239-1258.百度文库下载：http://wenku.baidu.com/link?url=xLDWZTdLXT1_fiZoUzNFiyQtZTwnyL-lZHhTSI0B87vkIE6UEDrKz6iz8zpKmmPvZq7ktlX6WRxyVxcjk8B-ymgl53QBfzBEKgYPZmsi1l_2.11       Vision-based Pedestrian Protection Systems for Intelligent VehiclesGeronimo, and A. M.Lopez. Vision-based Pedestrian Protection Systems for Intelligent Vehicles, BOOK, 2014.下载：http://bookzz.org/book/2167094/e216392.12       行人检测技术综述苏松志, 李绍滋, 陈淑媛等. 行人检测技术综述[J]. 电子学报, 2012, 40(4): 814-820.下载：行人检测技术综述2.13       车辆辅助驾驶系统中基于计算机视觉的行人检测研究综述贾慧星, 章毓晋.车辆辅助驾驶系统中基于计算机视觉的行人检测研究综述[J], 自动化学报, 2007, 33(1): 84-90.下载：车辆辅助驾驶系统中基于计算机视觉的行人检测研究综述2.14       行人检测系统研究新进展及关键技术展望许言午, 曹先彬,乔红. 行人检测系统研究新进展及关键技术展望[J], 电子学报, 2008, 36(5): 368-376.下载：行人检测系统研究新进展及关键技术展望2.15       基于视觉的人的运动识别综述杜友田; 陈峰;徐文立; 李永彬;基于视觉的人的运动识别综述, 电子学报,  2007. 35(1): 84-90.下载：基于视觉的人的运动识别综述2.16       基于机器学习的行人检测关键技术研究朱文佳. 基于机器学习的行人检测关键技术研究[D]. 第一章, 硕士学位论文, 上海交通大学. 2008. 指导教师: 戚飞虎.这是行人检测相关资源的第二部分：源码和数据集。考虑到实际应用的实时性要求，源码主要是C/C++的。源码和数据集的网址，经过测试都可访问，并注明了这些网址最后更新的日期，供学习和研究进行参考。（欢迎补充更多的资源）1        Source Code1.1    INRIA Object Detection and Localization Toolkithttp://pascal.inrialpes.fr/soft/olt/Dalal于2005年提出了基于HOG特征的行人检测方法，行人检测领域中的经典文章之一。HOG特征目前也被用在其他的目标检测与识别、图像检索和跟踪等领域中。更新：20081.2    Real-time Pedestrian Detection.http://cs.nju.edu.cn/wujx/projects/C4/C4.htmJianxin Wu实现的快速行人检测方法。Real-Time Human Detection Using Contour Cues：http://c2inet.sce.ntu.edu.sg/Jianxin/paper/ICRA_final.pdf更新：20121.3    霍夫变换实现的多目标检测http://graphics.cs.msu.ru/en/science/research/machinelearning/houghOlga Barinova, CVPR 2010 Paper: On detection of multiple object instances using Hough Transforms源码：C++更新：20101.4    HIKSVMhttp://ttic.uchicago.edu/~smaji/projects/fiksvm/Classification Using Intersection Kernel SVMs is efficientHOG+LBP+HIKSVM, 行人检测的经典方法.源码：C/C++更新：20121.5    GroundHOGhttp://www.mmp.rwth-aachen.de/projects/groundhogGPU-based Object Detection with Geometric Constraints, In: ICVS, 2011.  CUDA版本的HOG+SVM,源码：C/C++更新：20111.6    doppia codehttps://bitbucket.org/rodrigob/doppia这是一个代码集合，包含如下：Pedestrian detection at 100 frames per second, R. Benenson.  CVPR, 2012. 实时的Stixels estimation without depth map computationFast stixels estimation for fast pedestrian detectionSeeking the strongest rigid detectorTen years of pedestrian detection, what have we learned?Face detection without bells and whistles源码：C/C++更新：20151.7    Multiple camera pedestrian detection.POM: Occupancy map estimation for people detectionhttp://cvlab.epfl.ch/software/pom/Paper：Multi-Camera People Tracking with a Probabilistic Occupancy Map源码：？更新：20141.8    Pitor Dollar Detector.Piotr’s Computer Vision Matlab Toolboxhttp://vision.ucsd.edu/~pdollar/toolbox/doc/index.htmlThe toolbox is divided into 7 parts, arranged by directory:channels Robust image features, including HOG, for fast object detection.classify Fast clustering, random ferns, RBF functions, PCA, etc.detector Aggregate Channel Features (ACF) object detection code.filters Routines for filtering images.images Routines for manipulating and displaying images.matlab General Matlab functions that should have been a part of Matlab.videos Routines for annotating and displaying videos.源码：matlab更新：20142        DataSets2.1    MIT数据库http://cbcl.mit.edu/software-datasets/PedestrianData.html介绍：该数据库为较早公开的行人数据库，共924张行人图片（ppm格式，宽高为64×128），肩到脚的距离约80象素。该数据库只含正面和背面两个视角，无负样本，未区分训练集和测试集。Dalal等采用“HOG+SVM”，在该数据库上的检测准确率接近100%。更新：20002.2    INRIA Person Datasethttp://pascal.inrialpes.fr/data/human/介绍：该数据库是“HOG+SVM”的作者Dalal创建的，该数据库是目前使用最多的静态行人检测数据库，提供原始图片及相应的标注文件。训练集有正样本614张（包含2416个行人），负样本1218张；测试集有正样本288张（包含1126个行人），负样本453张。图片中人体大部分为站立姿势且高度大于100个象素，部分标注可能不正确。图片主要来源于GRAZ-01、个人照片及google，因此图片的清晰度较高。在XP操作系统下部分训练或者测试图片无法看清楚，但可用OpenCV正常读取和显示。更新：20052.3    Daimler行人数据库http://www.gavrila.net/Research/Pedestrian_Detection/Daimler_Pedestrian_Benchmark_D/该数据库采用车载摄像机获取，分为检测和分类两个数据集。检测数据集的训练样本集有正样本大小为18×36和48×96的图片各15560（3915×4）张，行人的最小高度为72个象素；负样本6744张（大小为640×480或360×288）。测试集为一段27分钟左右的视频（分辨率为640×480），共21790张图片，包含56492个行人。分类数据库有三个训练集和两个测试集，每个数据集有4800张行人图片，5000张非行人图片，大小均为18×36，另外还有3个辅助的非行人图片集，各1200张图片。更新：2009？2.4    Caltech Pedestrian Detectionhttp://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/该数据库是目前规模较大的行人数据库，采用车载摄像头拍摄，约10个小时左右，视频的分辨率为640×480，30帧/秒。标注了约250,000帧（约137分钟），350000个矩形框，2300个行人，另外还对矩形框之间的时间对应关系及其遮挡的情况进行标注。数据集分为set00~set10，其中set00~set05为训练集，set06~set10为测试集（标注信息尚未公开）。性能评估方法有以下三种：（1）用外部数据进行训练，在set06~set10进行测试；（2）6-fold交叉验证，选择其中的5个做训练，另外一个做测试，调整参数，最后给出训练集上的性能；（3）用set00~set05训练，set06~set10做测试。由于测试集的标注信息没有公开，需要提交给Pitor Dollar。结果提交方法为每30帧做一个测试，将结果保存在txt文档中（文件的命名方式为I00029.txt I00059.txt ……），每个txt文件中的每行表示检测到一个行人，格式为“[left, top,width, height, score]”。如果没有检测到任何行人，则txt文档为空。该数据库还提供了相应的Matlab工具包，包括视频标注信息的读取、画ROC（Receiver Operatingcharacteristic Curve）曲线图和非极大值抑制等工具。更新：20142.5    TUD行人数据库https://www.mpi-inf.mpg.de/departments/multi-cue-onboard-pedestrian-detection/介绍：TUD行人数据库为评估运动信息在行人检测中的作用，提供图像对以便计算光流信息。训练集的正样本为1092对图像（图片大小为720×576，包含1776个行人）；负样本为192对非行人图像（手持摄像机85对，车载摄像机107对）；另外还提供26对车载摄像机拍摄的图像（包含183个行人）作为附加训练集。测试集有508对图像（图像对的时间间隔为1秒，分辨率为640×480），共有1326个行人。Andriluka等也构建了一个数据库用于验证他们提出的检测与跟踪相结合的行人检测技术。该数据集的训练集提供了行人的矩形框信息、分割掩膜及其各部位（脚、小腿、大腿、躯干和头部）的大小和位置信息。测试集为250张图片（包含311个完全可见的行人）用于测试检测器的性能，2个视频序列（TUD-Campus和TUD-Crossing）用于评估跟踪器的性能。更新：20102.6    NICTA行人数据库http://www.nicta.com.au/category/research/computer-vision/tools/automap-datasets/该数据库是目前规模较大的静态图像行人数据库，25551张含单人的图片，5207张高分辨率非行人图片，数据库中已分好训练集和测试集，方便不同分类器的比较。Overett等用“RealBoost+Haar”评估训练样本的平移、旋转和宽高比等各种因素对分类性能的影响：（1）行人高度至少要大于40个象素；（2）在低分辨率下，对于Haar特征来说，增加样本宽度的性能好于增加样本高度的性能；（3）训练图片的大小要大于行人的实际大小，即背景信息有助于提高性能；（4）对训练样本进行平移提高检测性能，旋转对性能的提高影响不大。以上的结论对于构建行人数据库具有很好的指导意义。更新：20082.7    ETHZ行人数据库Robust Multi-Person Tracking from Mobile Platformshttps://data.vision.ee.ethz.ch/cvl/aess/dataset/Ess等构建了基于双目视觉的行人数据库用于多人的行人检测与跟踪研究。该数据库采用一对车载的AVT Marlins F033C摄像头进行拍摄，分辨率为640×480，帧率13-14fps，给出标定信息和行人标注信息，深度信息采用置信度传播方法获取。更新：20102.8    CVC行人数据库http://www.cvc.uab.es/adas/site/?q=node/7该数据库目前包含三个数据集（CVC-01、CVC-02和CVC-Virtual），主要用于车辆辅助驾驶中的行人检测研究。CVC-01[Geronimo,2007]有1000个行人样本，6175个非行人样本（来自于图片中公路区域中的非行人图片，不像有的行人数据库非行人样本为天空、沙滩和树木等自然图像）。CVC-02包含三个子数据集（CVC-02-CG、CVC-02-Classification和CVC-02-System），分别针对行人检测的三个不同任务：感兴趣区域的产生、分类和系统性能评估。图像的采集采用Bumblebee2立体彩色视觉系统，分辨率640×480，焦距6mm，对距离摄像头0~50m的行人进行标注，最小的行人图片为12×24。CVC-02-CG主要针对候选区域的产生，有100张彩色图像，包含深度和3D点信息；CVC-02-Classification主要针对行人分类，训练集有1016张正样本，7650张负样本，测试集分为基于切割窗口的分类（570张行人，7500张非行人）和整张图片的检测（250张包含行人的图片，共587个行人）；CVC-02-System主要用于系统的性能评估，包含15个视频序列（4364帧），7983个行人。CVC-Virtual是通过Half-Life 2图像引擎产生的虚拟行人数据集，共包含1678虚拟行人，2048个非行人图片用于测试。更新：2015，目前已经更新到CVC-08了。2.9    USC行人数据库http://iris.usc.edu/Vision-Users/OldUsers/bowu/DatasetWebpage/dataset.html该数据库包含三组数据集（USC-A、USC-B和USC-C），以XML格式提供标注信息。USC-A[Wu, 2005]的图片来自于网络，共205张图片，313个站立的行人，行人间不存在相互遮挡，拍摄角度为正面或者背面；USC-B的图片主要来自于CAVIAR视频库，包括各种视角的行人，行人之间有的相互遮挡，共54张图片，271个行人；USC-C有100张图片来自网络的图片，232个行人（多角度），行人之间无相互遮挡。更新：20073        其他资料1：Video：Pedestrian Detection: The State of the Arthttp://research.microsoft.com/apps/video/default.aspx?id=135046&r=1A video talk byPitor Dollar. Pitor Dollar做了很多关于行人检测方法的研究，他们研究小组的Caltech Pedestrian Dataset也很出名。2：Statistical and Structural Recognition of Human Actions. ECCV, 2010 Tutorial, by Ivan Laptev and Greg Mori. （注：要用爬墙软件才能访问到）3：Human Action Recognition in realistic scenarios, 一份很好的硕士生毕业论文开题资料。"}
{"content2":"【计算机视觉】关于OpenCV中GPU配置编译的相关事项标签（空格分隔）： 【计算机视觉】前一段发现了OpenCV中关于GPU以及opencl的相关知识，打算升级一下对OpenCV的使用，但是发现从OpenCV官网上下载的都是没有WITH_CUDA这一选项的。于是必须进行OpenCV带CUDA的重编译！下面就记录这一阶段出现的一系列问题。关于OpenCV版本的问题起初直接尝试使用一直用的OpenCV2.4.9的源码进行编译，选择的编译环境为Visual Studio2010 x64，因为前面在使用cuda的时候就已经遇到过关于CUDA的库对于32位程序的不全面支持，即CUDA的较高版本好像有一些64位的库在32位中是没有的。所以，直接选择了x64的平台。而此时我使用的CUDA版本为8.0。总之，编译OpenCV中出现了一系列莫名其妙的问题，在网上也不好查看。最后尝试了使用OpenCV2.4.13版本，听说是2016年更新的版本，所以应该能跟CUDA8.0匹配。首先只是将OpenCV2.4.13的源码替换了OpenCV2.4.9的源码，然后再去编译的，debug版本爆出了无法排除的错误，而release版本居然生成成功了，但是我使用这些库的时候总是说CUDA function API call错误之类，应该是还是没有配置成功。这一问题让我都想放弃了OpenCV中关于CUDA的使用了。后来一再坚持，然后决定使用Visual Studio 2015进行编译。这里说明一下，我用CMAKE进行编译的时候可选的本地编译器有10，13，和15版本的。当时想使用13版本的win64，但是好像configure不过，于是一直使用10版本，即使打开的时候用的是13版本，但是还是报错。后来猛然间醒悟，极有可能是编译器版本的问题，于是使用CUDA8.0+OpenCV2.4.13+Visual Studio 2015 x64配置，不管是Debug版本还是Release版本都生成成功了，这全部是最新版本的相互之间的匹配。关于CMAKE的配置选项一个是要选择WITH_CUDA，然后是选择capabilities，我的笔记本是GTX860M对应的为5.0，我查了一下后面要部署的显卡GTX1060为6.1，我把这两项加了进去，其实还有一个GPU架构的选项，好像不选的话就是对应自动，但是里面还好像只有开普勒和费米架构？我配置成功的这次没有选择。另外，好像用10版本的配置这个能力的时候居然配置不成功，不知道啥原因。关于我电脑上OpenCV版本控制的说明（1）保留D盘根目录下的OpenCV2.4.9与OpenCV2.4.13，这两个都是基于微软的MSVC编译器的，其中OpenCV2.4.13版本中的x64中对应的vc14路径下为带CUDA-GPU配置的动态链接库。（2）在F盘根目录下的opencv2.4.13是对应x64版本的CUDA-GPU编译工程，可以保留，以后又需要在进行修改。（3） 在F盘根目录下的OpenCV2.4.9-MinGW为对应MinGW gcc编译器的动态链接库，可以在Qt-gcc下编译使用，在编写跨平台的工程时可以使用Qt的这个版本。保留工程文件！下面是一些引文参考：近期由于课题需要使用GPU进行加速，通过调研决定采用OpenCV 的GPU 支持，通过整整一天的摸索，终于配置成功。 1. 系统环境 windows 7 ultimate版 64位+ visual studio 2010 ultimate版+OpenCV 2.4.9+CUDA ToolKit 6.5+CMake2.8.8 2. 配置过程 大致流程：CUDA Toolkit 6.5的下载与安装--> CMake的下载与安装--> OpenCV的下载与编译-->测试是否安装成功 2.1 CUDA ToolKit 6.5的下载与安装 由于之前测试直接使用OpenCV的预编译的版本，无法使用其GPU模块，因此需要安装CUDA Toolkit之后，使用自己的基于CUDA 编译的OpenCV 库。 由于自己配置时，CUDA ToolKit版本是7.0，于是尝试了一下，结果不成功，原因估计是Opencv2.4.9的gpu模块对最新的CUDA支持不够完善，因此在选择CUDA 版本时，一定要考虑OpenCV对其的兼容性。自己的一个经验是到 http://code.opencv.org/projects/opencv/wiki/ChangeLog 看一下自己的OpenCV 版本发布的时间；再到CUDA下载的页面找一个发布时间与OpenCV发布时间相近的版本；选择合适的版本很重要；自己花费的主要时间就浪费在版本不兼容上~~~ 自己使用的是CUDA ToolKit 6.5，根据自己的系统，计算需求，下载对应的版本，一路安装就可以了。这个问题应该都不大，主要是确定OpenCV 与CUDA 的兼容性。 2.2 CMake 工具的下载与安装 到http://www.cmake.org/files/ 下载任何自己需要的版本（虽然我的win7系统是64位的，但是只有32位的CMAKE 供下载，cmake-2.8.8-win32-x86.exe, 这个对后面的编译木有影响），我使用的是2.8.8，下载完成，安装就OK~~ 2.3 OpenCV的下载与编译 到OpenCV的主页上找到对应的版本2.4.9，下载Opencv for windows ，安装到自己的目录，比如D:\\OpenCV；然后使用CMake工具编译基于CUDA和TBB的OpenCV库，这个过程可以参看， http://www.cnblogs.com/freedomshe/archive/2013/01/11/win7_vs2012_opencv_rebuild.html 这篇文章写得很详细，而且图文并茂，很好懂，虽然这篇文章的OPENCV及vs版本不同，但是VS2010按照他的过程来编译也是可以顺利通过的； 需要的注意的是， 1） 在配置CMake的时候选择VS10 WIN64(根据自己的操作系统和VS版本选择); 勾选WITH_TBB，WITH_CUDA 2） 把debug和release两个版本都编译上；编译的时间比较长（视电脑性能，自己编译花了），耐心等待~~~ 3）在编译过程中可能会弹出对话框 是否要 reload the solution when a CMakeLists.tx has changed， 这个可能是个bug，是CMake的宏造成的，所以可以再编译前检查一下是否有这样额的宏，VS2010中 (Tools)工具--》(Macros)宏--》(Macro explorer)宏查看器-》CMakeVSMacros2（也可能是其他的目录）找到 Public Sub ReloadProjects(Optional ByVal projects As String = \"\") 与Public Sub StopBuild(Optional ByVal projects As String = \"\")这两个函数，它们应该是VB写的，把函数体注释掉就可以了； 2.4 测试是否安装成功 安装完成后，新建一个WIN32项目，注意配置一下使用x64选项进行编译；如果安装成功 int devNum=cv::gpu::getCudaEnabledDeviceCount();将返回非0值，否则说明配置为成功。 #include \"stdafx.h\" #include #include \"opencv2/opencv.hpp\" #include \"opencv2/gpu/gpu.hpp\" int _tmain(int argc, _TCHAR* argv[]) { int devNum=cv::gpu::getCudaEnabledDeviceCount(); try { cv::Mat src_host = cv::imread(\"lena.jpg\", CV_LOAD_IMAGE_GRAYSCALE); cv::gpu::GpuMat dst, src; src.upload(src_host); cv::gpu::threshold(src, dst, 128.0, 255.0, CV_THRESH_BINARY); cv::Mat result_host; dst.download(result_host); cv::namedWindow(\"Result\",1); cv::imshow(\"Result\", result_host); cv::waitKey(); } catch(const cv::Exception& ex) { std::cout << \"Error: \" << ex.what() << std::endl; } return 0; } 参考文献 【1】 http://www.cnblogs.com/freedomshe/archive/2013/01/11/win7_vs2012_opencv_rebuild.html 【2】http://blog.csdn.net/kelvin_yan/article/details/38866795按语：首先感谢http://blog.csdn.net/fengbingchun/article/details/9831837这个博主的原创方法，在这个基础上编译之后发现了很多问题，所以进行了改正，有了以下方法： 重新编译OpenCV 1、 查看本机配置，查看显卡类型是否支持NVIDIA GPU，本机显卡为NVIDIA GeForce GT630； 2、 从http://www.nvidia.cn/Download/index.aspx?lang=cn下载最新驱动并安装； 3、 从https://developer.nvidia.com/cuda-toolkit根据本机类型下载相应最新版的CUDA Toolkit6.0 64位，安装，并通过样本程序验证其安装正确； 4、 将C:\\ProgramFiles\\NVIDIAGPU Computing Toolkit\\CUDA\\v6.0\\bin添加到环境变量中(检查是否已经默认添加)； 5、 从http://threadingbuildingblocks.org/下载最新版的TBB4.2，解压缩，并将其bin目录D:\\soft\\OpenCV2.4.8\\TBB\\tbb41_20140122oss\\bin\\ia64\\vc10添加到环境变量中，注销或重启； 6、 从http://opencv.org/downloads.html 下载最新版本的OpenCV2.4.6，并解压缩到D:\\soft\\OpenCV2.4.8文件夹中； 7、 从http://www.cmake.org/cmake/resources/software.html下载最新版本的CMake2.8.11.2并安装； 8、打开CMake，在Where isthesource code：中选择D:\\soft\\OpenCV2.4.6\\opencv文件夹，在Where to buildthe binaries：中选择D:/soft/OpenCV2.4.6/vs2010_GPU文件夹，此文件夹为手动创建； 9、点击Configure按钮，在弹出的对话框中选择VisualStudio 10，然后点击Finish； 10、 如果有红色框出现，勾选BUILD_EXAMPLES、WITH_TBB、WITH_CUBLAS、WITH_CUDA、WITH_CUFFT，然后再次点击Configure按钮； 11、如果还有红色框出现，TBB_INCLUDE_DIRS，将其值改为D:\\soft\\OpenCV2.4.6\\TBB\\tbb41_20130613oss\\include为TBB中include所在的目录，然后再次点击Configure按钮； 12、 如何还有红色框出现，TBB_LIB_DIR、TBB_STDDEF_PATH，再次点击Configure按钮； 13、如果在下方信息框中有：Use TBB: YES(ver 4.1 interface 6105)，Use Cuda: YES(ver5.0)，证明我们已经将inteltbb和CUDA正确配置； 14、点击Generate按钮，此时会在D:\\soft\\OpenCV2.4.6\\vs2010_GPU文件夹下生成OpenCV.sln文件； 15、以管理员身份，使用vs2010打开OpenCV.sln文件，选择View--> Properties Manager-->分别选中ALL_BUILD中的Debug和Release上的Microsoft.Cpp.Win64.user，依次添加inteltbb和CUDA 的Executable Directories、IncludeDirectories和Library Directories，点击右键-->Properties： （需要说明的是：opencv里面要选择build文件夹下面的路径才对） VC++ Directories，IncludeDirectories： D:\\soft\\OpenCV2.4.6\\TBB\\tbb41_20130613oss\\include C:\\Program Files\\NVIDIAGPU ComputingToolkit\\CUDA\\v5.0\\include D:\\opencv\\build\\include D:\\opencv\\build\\include\\opencv D:\\opencv\\build\\include\\opencv2 Library Directories： D:\\soft\\OpenCV2.4.6\\TBB\\tbb41_20130613oss\\lib\\ia64\\vc10 C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v5.0\\lib\\Win64 D:\\opencv\\build\\x64\\vc10\\lib Executable Directories： D:\\soft\\OpenCV2.4.6\\TBB\\tbb41_20130613oss\\bin\\ia64\\vc10 C:\\Program Files\\NVIDIA GPU ComputingToolkit\\CUDA\\v5.0\\bin D:\\opencv\\build\\x64\\vc10\\bin 16、选中modules中的opencv_gpu，点击右键，选择Properties-->Linker-->Input-->IgnoreSpecificDefault Libraries加入libcmtlibcmtd；（可忽视这条） 17、 分别在Debug和Release下，选择SolutionExplorer里的Solution OpenCV，点击右键，运行”Rebuild Solution”。（这个过程比较漫长，win7 64位，4GB内存，大概需要3小时） 18、点击CMakeTargets下的INSTALL—右键—仅本工程build，之后会在D:\\soft\\OpenCV2.4.3\\vs2010\\install文件夹下的bin文件夹中存放相应的动态库，在lib文件夹下存放相应的静态库； 补充说明 (1)、安装CUDA Toolkit时最好不要修改其默认的安装目录； (2)、第17步的重建过程中，如果出现了很异常诡异的行为，有一种调试方式是删除原先配置的文件夹，重新cmake，清空VS2010里面的工程，重新编译，可能会改善。 (3)、为了缩短编译时间，可以通过CMake中改变CUDA_ARCH_BIN、CUDA_ARCH_PTX值进行设置，将BUILD_EXAMPLES的勾选去掉（没有这么做，因为是为了全部编译） (4)、在编译前最好把360安全卫士和杀毒软件关闭； (5)、也可不用管理员身份打开OpenCV.sln，第15步的属性配置在不同的电脑上有的必须的，有的可以不需要，最好都加上，免得返工； (6)、也可以不用修改opencv_gpu模块的属性配置，默认即可，即第16步也不是必须的； (7)、编译过程中会弹出数次对话框，如出现宏对话框则点击Cancel选项，其它则选择Yes选项； (8)、编译成功后的库也可以直接应用于OpenCV中的OpenCL模块。 Debug过程 编译错误解决： 1、无法加载宏： C:/Users/XXX/Documents/Visual Studio 2010Projects/VSMacros80/ Samples/ Samples.vsmacros 解决方法： 把这个附件放在上述目录里面：附件 2、错误： CMakeFiles/example_gpu_driver_api_multi.dir/driver_api_multi.cpp.o: Infunction `destroyContexts()': driver_api_multi.cpp:(.text._Z15destroyContextsv+0xc): undefined referenceto `cuCtxDestroy_v2' driver_api_multi.cpp:(.text._Z15destroyContextsv+0x1c): undefinedreference to `cuCtxDestroy_v2' CMakeFiles/example_gpu_driver_api_multi.dir/driver_api_multi.cpp.o: Infunction `Worker::operator()(int) const': driver_api_multi.cpp:(.text._ZNK6WorkerclEi+0x19): undefined reference to`cuCtxPushCurrent_v2' driver_api_multi.cpp:(.text._ZNK6WorkerclEi+0x6bf): undefined reference to`cuCtxPopCurrent_v2' 解决方法： Only two example will affect by this bug,\"example_gpu_driver_api_multi\" and\"example_gpu_driver_api_stereo_multi\" Modify line 39 target_link_libraries(${the_target} ${OPENCV_LINKER_LIBS}${OPENCV_GPU_SAMPLES_REQUIRED_DEPS}) to target_link_libraries(${the_target} ${OPENCV_LINKER_LIBS}${OPENCV_GPU_SAMPLES_REQUIRED_DEPS} cuda) in \"opencv-2.4.6.1/samples/gpu/CMakeLists.txt\" can easily fixthis problem 3、错误：dllmain重复定义，opencv_world246.dll不存在等错误 解决方法：cmake的时候不要勾选build_world选项，这个重建也不是必须的 4、错误37374error LNK1104:无法打开文件“..\\..\\lib\\Debug\\opencv_superres248d.lib”D:\\opencv\\opencv248GPU\\modules\\superres\\LINKopencv_test_superres 分析：是链接库链接不上造成的，添上这些库就可以了。 解决办法：第15步中务必把opencv的include\\bin\\lib目标都加进去，而且是build目录下的，如果还有问题，那么重新cmake，或者清除VS2010的缓存就可以了。 5、错误 Unable to cast COM object of type'System.__ComObject' to interface type 'EnvDTE.DTE'. 解决办法：可能是这个或者可以放在这里不解决，继续编译可以没有问题 http://blogs.msdn.com/b/smondal/archive/2012/10/02/unable-to-cast-com-object-of-type-system-comobject-to-interface-type-microsoft-visualstudio-ole-interop-iserviceprovider.aspx The exception seems to indicate the crashwas caused due to an interface not being found. This indicates thatsomething's wrong with proxy/stubs on the machine. At leastIServiceProvider is failing. IE is usually the owner of the IServiceProviderproxy. From the dump, it showed devenv.exe process crashed shortly afterloading: C:\\Windows\\SysWOW64\\actxprxy.dll andC:\\Windows\\System32\\mssprxy.dll The issue turned out to be brokenregistration for IServiceProvider. The proxy-stub CLSID was wrong (it wasreferring to actxprxy.dll instead of ieproxy.dll on Windows 7 x64). Registeringieproxy.dll from an elevated cmd prompt resolved the issue. regsvr32 \"C:\\Program Files (x86)\\Internet Explorer\\ieproxy.dll\" On x86 systems, regsvr32\"C:\\Program Files\\Internet Explorer\\ieproxy.dll\" 新编译的opencv使用方法 1、打开vs2010，新建一个控制台应用程序，为vs2010配置OpenCV环境：选择View-->Properties Manager-->分别选中Debug和Release上的Microsoft.Cpp.Win64.user，点击右键-->Properties：VC++ Directories，Include Directories：D:\\Soft\\OpenCV2.4.6\\vs2010_GPU\\install\\include；D:\\Soft\\OpenCV2.4.6\\vs2010_GPU\\install\\include\\opencv；D:\\Soft\\OpenCV2.4.6\\vs2010_GPU\\install\\include\\opencv2；Library Directories：D:\\Soft\\OpenCV2.4.6\\vs2010_GPU\\install\\lib； 2、选中工程-->Properties-->Configuration Properties-->Linker-->Input-->AdditionalDependencies：Debug和Release，添加相应的.lib库； 3、将D:\\soft\\OpenCV2.4.6\\vs2010_GPU\\install\\bin加入到windows系统环境变量Path中，重启。（也可以将install文件夹全部拷贝出来，作为一个新的opencv版本存在，里面应该包括include\\bin\\lib这个几个主要目录） 4、示例如下，如果iDevicesNum结果非0，说明安装配置正确。 1. #include \"stdafx.h\" 2. #include <opencv2/opencv.hpp> 3. #include <opencv2/gpu/gpu.hpp> 4. 5. using namespace cv; 6. using namespace cv::gpu; 7. 8. int_tmain(int argc, _TCHAR* argv[]) 9. { 10. int iDevicesNum = getCudaEnabledDeviceCount(); 11. 12. cout<<iDevicesNum<<endl; 13. 14. return 0; 15.} 参考文献 1、 http://opencv.willowgarage.com/wiki/OpenCV_GPU 2、 http://docs.opencv.org/modules/gpu/doc/introduction.html 3、 http://wenku.baidu.com/view/81e29c6f011ca300a6c390dd.html 4、 http://blog.cuvilib.com/2011/03/22/how-to-build-opencv-2-2-with-gpu-cuda-on-windows-7/ 5、 http://docs.nvidia.com/cuda/cuda-getting-started-guide-for-microsoft-windows/index.html"}
{"content2":"第一节课: 计算机视觉整体概述这学期正在上一节计算机视觉课程，有些不懂的知识上百度搜索发现关于计算机视觉的文章寥寥无几。 这节课主要讲解的是计算机视觉里面的图片识别，不会讲到识别一些正在运动的物体。希望可以把自己学到的东西分享给大家。因为版权等问题，我无法将lecture notes放在这里，但是我会把自己所理解的东西写下来。1. 什么是计算机视觉(Computer Vision)?老师给出的定义:  Enable machines to “see” the visual world as we do. (可以让机器像我们一样看见并且识别东西). 个人理解计算机视觉也就是我们平常说的人脸识别，车牌号识别，自动驾驶里面的树，人各种交通符号的识别，以及各种人们想要识别的东西都可以通过算法来识别出来。另外计算机视觉是人工智能（AI）的一个分支。计算机视觉包含三点：1. Measurement(测量): 通过visual data计算这个3D世界的不同属性。2. Perception&interception(认知和翻译): 通过使用一些算法和representation(讲解)可以使机器认知一些物体，人，景色以及运动的东西。3. Search&organization(搜索和整理)： 通过一些算法来和visual data 挖掘，搜索和交互。下图就是典型的计算机视觉技术分析出来一个公园里面的各种信息:下图为计算机视觉以及其相关的科目:下图为图片和模型之间的关系，应该很清楚的表示了从图片到模型属于视觉，反之则为图像：2. 为什么计算机视觉很难实现？提到几点：1. 真是的世界比我们看到的图片中的要复杂得多(从3D世界到2D图片)。2. 从图片还原出来当时的过程(process)是不可能的.3. 物体的运动（比如奔跑中的人照片可能是模糊的).4. 物体多元化，比如椅子可以有很多种很多不同颜色不同品牌等等...5. 光源，物体的动作，背景杂乱的影响等等都会影响。见下图3. 现在的科学技术已经做到了哪些？1. 图片分类(Image classification)，下图2. 人脸识别(Face detection), 这个很好理解就不放图了3. 物体识别(Object classification)，下图4. 脸部匿名化(face anonymization)，其实就是变脸...5. 交互式系统(比如xbox的kinect)6. 自动驾驶中的人，树，和其他一些交通指示牌。7. 动作的捕捉(motion capture)，见下图8. 还有很多不一一列举了...4. 计算机视觉的运用(Applications)机器人学，自动驾驶，图片搜索，医学领域图片等等，平时用的美图秀秀和那个什么把人脸变成狗脸的faceu都是计算机视觉领域。5. 总结总之计算机视觉是一个新兴的领域，非常有用，有趣但是很难。。。 这堂课我们会讲解图片构成，原理，如何变化以及如何识别。 运用的语言是Matlab(上这节课之前我是一点都不会matlab[哭~])。"}
{"content2":"0001，常识1计算机视觉的任务很多，有图像分类、目标检测、语义分割、实例分割和全景分割等，那它们的区别是什么呢？1、Image Classification（图像分类）图像分类（下图左）就是对图像判断出所属的分类，比如在学习分类中数据集有人（person）、羊（sheep）、狗（dog）和猫（cat）四种，图像分类要求给定一个图片输出图片里含有哪些分类，比如下图的例子是含有person、sheep和dog三种。2、Object detection（目标检测）目标检测（上图右）简单来说就是图片里面有什么？分别在哪里？（把它们用矩形框框住）目前常用的目标检测算法有Faster R-CNN和基于YOLO的目标检测的算法3、semantic segmentation（语义分割）通常意义上的目标分割指的就是语义分割语义分割（下图左）就是需要区分到图中每一点像素点，而不仅仅是矩形框框住了。但是同一物体的不同实例不需要单独分割出来。对下图左，标注为人，羊，狗，草地。而不需要羊1，羊2，羊3，羊4，羊5等。4、Instance segmentation（实例分割）实例分割（上图右）其实就是目标检测和语义分割的结合。相对目标检测的边界框，实例分割可精确到物体的边缘；相对语义分割，实例分割需要标注出图上同一物体的不同个体（羊1，羊2，羊3...）目前常用的实例分割算法是Mask R-CNN。Mask R-CNN 通过向 Faster R-CNN 添加一个分支来进行像素级分割，该分支输出一个二进制掩码，该掩码表示给定像素是否为目标对象的一部分：该分支是基于卷积神经网络特征映射的全卷积网络。将给定的卷积神经网络特征映射作为输入，输出为一个矩阵，其中像素属于该对象的所有位置用 1 表示，其他位置则用 0 表示，这就是二进制掩码。一旦生成这些掩码， Mask R-CNN 将 RoIAlign 与来自 Faster R-CNN 的分类和边界框相结合，以便进行精确的分割：5、Panoramic segmentation（全景分割）全景分割是语义分割和实例分割的结合。跟实例分割不同的是：实例分割只对图像中的object进行检测，并对检测到的object进行分割，而全景分割是对图中的所有物体包括背景都要进行检测和分割。0002，常识2转自：https://blog.csdn.net/Gerwels_JI/article/details/829901892018年10月09日 22:42:42 Gerwels_JI 阅读数：1221Directions in the CV物体分割(Object segment)属于图像理解范畴。那什么是图像理解？Image Understanding (IU) 领域包含众多sub-domains，如图像分类、物体检测、物体分割、实例分割等若干问题。每个问题研究的范畴是什么？每个问题中，各个approach对应的the result of processing是什么？Image Understanding (IU) is an interdisciplinary approach which fuse computer science, mathematics, engineering science, physics, neurosciences, and cognitive science etc. together.一般我们将CV分为三个大方向：图像处理、图像分析、图像理解。其中图像理解分为以下三个部分Image Classification:即是将图像结构化为某一类别的信息，用事先确定好的类别（string）或实例ID来描述图片。其中ImageNet是最权威的测评集，每年的ILSVRC催生大量优秀的深度网络结构，为其他任务提供基础，在应用领域，人脸、场景识别都可以视为分类任务。Detection分类任务关心整体，给出的是整张图片的内容描述，而检测则关注特定的物体目标，要求获得这一目标的类别信息和位置信息。相比分类，检测给出的是对图片前景和背景的理解，我们需要从背景中分离出感兴趣的目标，并确定这一目标的描述（类别和位置），因而检测模型的输出是一个列表，列表的每一项使用一个数据组给出检出目标的类别和位置，常用矩形检测框的坐标表示。Segmentation分割包括语义分割（semantic segmentation）和实例分割（instance segmentation），前者是对背景分离的拓展，要求分离开具有不同语义的图像部分，而后者是检测任务的拓展，要求描述出目标的轮廓（相比检测框更为精细）。分割是对图像的像素级描述，它赋予每个像素类别意义，适用于理解要求较高的场景，如无人驾驶中对道路和非道路的分割。后期我会写CV综述，此处留坑占位！也会对object segmentation的方法进行总结，占坑！Image ClassificationThe task of object classification requires binary labels indicating whether objects are present in an image.Definition：Image Classification根据image中不同图像信息中不同的feature，把不同类别的object region进行分类。该任务需要我们对出现在某幅图像中的物体做标注。例如：一共有1000个物体类的image中，某个物体要么有，要么没有。可实现：输入一幅测试图片，输出该图片中物体类别的候选集。如下图所示，不同形状的图形，通过分类分成了8类Object localization (目标定位)在图像分类的基础上，我们还想知道图像中的目标具体在图像的什么位置，通常是以边界框的(bounding box)形式。基本思路多任务学习，网络带有两个输出分支。一个分支用于做图像分类，即全连接+softmax判断目标类别，和单纯图像分类区别在于这里还另外需要一个“背景”类。另一个分支用于判断目标位置，即完成回归任务输出四个数字标记bounding box位置(例如中心点横纵坐标和包围盒长宽)，该分支输出结果只有在分类分支判断不为“背景”时才使用。人体位姿定位/人脸定位目标定位的思路也可以用于人体位姿定位或人脸定位。这两者都需要我们对一系列的人体关节或人脸关键点进行回归。弱监督定位由于目标定位是相对比较简单的任务，近期的研究热点是在只有标记信息的条件下进行目标定位。其基本思路是从卷积结果中找到一些较高响应的显著性区域，认为这个区域对应图像中的目标。Object detection(目标检测)Detecting an object entails both stating that an object belonging to a specified class is present, and localizing it in the image. The location of an object is typically represented by a bounding box.理解：object detection=classification+localization定义：物体探测包含两个问题，一是判断属于某个特定类的物体是否出现在图中；二是对该物体定位，定位常用表征就是物体的边界框(bounding box)。可实现：输入测试图片，输出检测到的物体类别和位置。如下图，移动的皮卡丘和恐龙语义分割(Semantic Segmentation)The task of labeling semantic objects in a scene requires that each pixel of an image be labeled as belonging to a category, such as sky, chair, floor, street, etc. In contrast to the detection task, individual instances of objects do not need to be segmented.语义标注(Semantic scene labeling)/分割(segmentation)：该任务需要将图中每一点像素标注为某个物体类别。同一物体的不同实例不需要单独分割出来。Instance segmentationinstance segment = object detect +semantic segment相对物体检测的边界框，实例分割可精确到物体的边缘；相对语义分割，实例分割可以标注出图上同一物体的不同个体分类任务通常来说就是识别出包含单个对象的图像是什么，但在分割实例时，我们需要执行更复杂的任务。我们会看到多个重叠物体和不同背景的复杂景象，我们不仅需要将这些不同的对象进行分类，而且还要确定对象的边界、差异和彼此之间的关系！如下图所示，把不同的实例对象进行了分割，并用不同的颜色进行边缘标注（而不是边框标注Some examples综述图像理解领域中的object segmentation方向包括了：image classification、object localization、object detection、semantic segmentation、instance-level segmentation。分类复杂度依次递增，分类详细程度依次递增。0003，常识计算机视觉（CV）一直是目前深度学习领域最热的研究领域，其是一种交叉学科包括计算机科学（computer science / (Graphics, Algorithms, Theory, Systems, Architecture）、数学 (Information Retrieval, Machine Learning)、工程学(Robotics, Speech, NLP, Image Processing）、物理(Optics)、生物学 (Neuroscience), and 神经科学 (Cognitive Science)，由于计算机视觉表示了对视觉环境的理解，加上其学科交叉性，众多科学家认为计算机视觉的发展可以为实现理想的人工智能铺路。对于问题：什么才是计算机视觉？以下有三个不同的教科书式计算机视觉定义：“the construction of explicit, meaningful descriptions of physical objects from images” (Ballard & Brown, 1982)“computing properties of the 3D world from one or more digital images” (Trucco & Verri, 1998)“to make useful decisions about real physical objects and scenes based on sensed images” (Sockman & Shapiro, 2001)那么为什么要研究计算机视觉呢？最简单的答案就是其可以将研究快速有效的应用到现实场景中，下面列举了几个CV应用的场景：人脸识别图像检索游戏和控制监控生物统计（指纹，虹膜，人脸匹配）智能驾驶笔者最近完成了斯坦福的CS231课程【1】，课程中将卷积神经网络用在视觉识别任务中，包括图像分类，定位和检测，尤其是深度学习技术的发展极大的提高了这些任务的精度，完成这个课程后，笔者想和大家分享5个最具影响力的计算机视觉技术。1. 图像分类 image classification图像分类任务描述如下：给定一系列标记为单标签的图像，希望成功预测出未经标记的新的数据的标签。与这个任务相联系的是更多的挑战：包括：角度多样性, scale多样性, 额外新类的变化, 图像的损坏, 先验条件, 和背景的变化。so,如何才能设计一个算法分类出不同的类，计算机视觉研究者提出以数据为驱动的解决方法，令计算机从一些已经有类别标记的图像中学习到图像的视觉表示。而在这个算法中最常用的就是卷积神经网络了，Convolutional Neural Networks (CNNs)。输入图像到CNN网络中后，CNN并不是直接对整个图像做计算，而是创建一个滑动机制，假设先输入图像中10*10的pixel，以相乘的计算方式计算，乘数部分成为卷积核，然后从左到右进行滑动，计算接下来的10*10pixel，这就是CNN中的滑动窗口计算机制。输入数据输入卷积层后，卷积层中卷积核仅关注当前部分和它附近的部分。整体的卷积网络趋向于更窄以便于在相同参数量的情况下搭建更深的网络。除了卷积层之外，在分类中常使用的还有池化层pooling layer，最常见的是最大池化 Max pooling，假设maxpooling的pooling核是2，maxpooling 的工作机制是取2x2大小的像素块中的最大值代替这个像素块。图像分类最典型的数据集是imagenet，一个包含120万张，1000类的大型图像数据集。以此数据集为基础，衍生了许多典型的深度学习经典网络：AlexNet（2012）ZFNet（2013）VGG（2014）GoogleNet（2014）InceptionNet（2015）ResNet（2016）DenseNet（2016）图像分类任务虽然简单，应用范围也有限，但是其是计算视觉的基础，图像分类上的成功表明深度学习网络具有学习并理解图像的能力。目前计算机视觉的其他任务使用的basemodel都是在image classification上取得成功的网络结构。因此，可以把图像分类视为深度学习实现人工智能的第一步。注：貌似16年往后，基本没有革新性质的网络结构出现了，/手动笑哭/。2. 目标检测 object detectionobject detection的任务是检测到图像中的目标并分类出目标种类，如上图所示，检测出车并框住，并给出框中目标的置信度，当然上图并不是目前深度学习最好的结构，图中有两个漏检测的目标。目标检测与图像分类，目标定位不同的地方在于目标检测是同时应用分类和定位技术到图像中的多个目标，此类任务的label也更复杂，不仅要知道目标的位置（bounding box）还要知道目标的类别，而且为了检测到比较小的目标，提高检测精度，此类任务的batch size往往很小。目标检测的技术实现相对复杂，但是应用场景非常多，比如统计：统计人、车、花朵或者微生物的数量是现实生活中各种不同类型的使用图形信息的系统最广泛的需求；图像检索，根据图像检测图像；卫星图像分析；安防场景等等。目前深度学习在这方面的工作有很多：RCNNFast RCNN https://arxiv.org/pdf/1504.08083.pdfFaster RCNN https://arxiv.org/pdf/1506.01497.pdfYOLO系列 http://lanl.arxiv.org/pdf/1612.08242v1SSD http://lanl.arxiv.org/pdf/1512.02325v5以上这些都是近几年来最典型的目标检测成果，这些工作提供了深度学习技术在目标检测上的应用形式以及研究方向，目前的大部分较好的结果都是基于这些工作改进而来。3. 目标追踪 object tracking目标追踪即是在一个给定的场景中，follow一个或者多个目标。传统上，目标追踪都是应用在视频或者实时场景交互，比如观测者追踪一个初始的目标。目前来讲，使用最典型的场景就是自动驾驶了。目标追踪可以分为两类：一类是生成式方法，另一类是判别式方法。生成方法使用生成模型来描述表观特征并最小化重建误差以搜索目标，如PCA。而判别式方法可以用来区分物体和背景，其性能更稳健，并逐渐成为跟踪的主要方法。判别法也被称为跟踪检测，深度学习属于这一类别。为了通过检测实现跟踪，我们检测所有帧的候选对象，并使用深度学习从候选对象中识别想要的对象。有两种可以使用的基本网络模型：堆叠自动编码器（SAE）和卷积神经网络（CNN）。使用SAE跟踪任务的最受欢迎的深度网络是Deep Learning Tracker，它提出线下预训练和在线微调网络，工作流程如下：离线无监督预先训练使用大规模自然图像数据集的堆叠去噪自动编码器，以获得一般对象的表示。叠加去噪自动编码器通过在输入图像中添加噪声并重构原始图像可以获得更强大的特征表达能力。将预先训练好的网络的编码部分与分类器合并得到分类网络，然后使用从初始帧中获得的正负样本对网络进行微调，从而可以区分当前的对象和背景。 DLT使用粒子滤波器作为运动模型来生成当前帧的候选patches。分类网络输出这些patches的概率分数，即分类的置信度，然后选择这些patches中最高的patches作为目标。DLT使用有限阈值的方式更新模型。由于其在图像分类和目标检测方面的优势，CNN已成为计算机视觉和视觉跟踪的主流深度模型。一般来说，大规模的CNN既可以作为分类器也可以作为跟踪器来训练。 2种有代表性的基于CNN的跟踪算法是全卷积网络跟踪器（FCNT）和多域CNN（MD Net）。FCNT成功地分析和利用了VGG模型的特征图，这是一个预先训练好的ImageNet，并得出以下结论：CNN feature maps可用于定位和跟踪。许多CNN feature maps对于区分背景中的特定对象的任务而言是嘈杂或不相关的。较高层捕获对象类别的语义概念，而较低层编码更多区分性特征以捕获类内变体。因此，FCNT设计了特征选择网络以在VGG网络的conv4-3和conv5-3层上选择最相关的特征图。然后为了避免嘈杂的过拟合，它还为两层单独选择的特征映射设计了额外的两个通道（称为SNet和GNet）。 GNet捕获对象的类别信息，而SNet将该对象从具有相似外观的背景中区分出来。两个网络都使用第一帧中给定的边界框进行初始化，以获取对象的热图，而对于新帧，将裁剪并传播最后一帧中以对象位置为中心的感兴趣区域（ROI）。最后，通过SNet和GNet，分类器获取两个预测的热图，跟踪器根据是否存在干扰，决定使用哪个热图来生成最终的跟踪结果。 FCNT的pipline如下所示。与FCNT的想法不同，MD Net使用视频的所有序列来跟踪它们的移动。上述网络使用不相关的图像数据来减少跟踪数据的训练需求，并且这种想法与跟踪有一些偏差。该视频中的一个类的对象可以是另一个视频中的背景，因此MD Net提出了多域的思想来独立地区分每个域中的对象和背景。而一个域表示一组包含相同类型对象的视频。如下图所示，MD Net分为两部分：域特定层的共享层和K分支。每个分支包含一个softmax损失的二进制分类层，用于区分每个域中的对象和背景，共享层与所有域共享以确保一般表示。近年来，深度学习研究人员尝试了不同的方法来适应视觉追踪任务的特征。有许多方向已经被探索：应用其他网络模型，如Recurrent Neural Net和Deep Belief Net，设计网络结构以适应视频处理和端到端学习，优化流程，结构和参数，或者甚至将深度学习与计算机视觉的传统方法或其他领域的方法（如语言处理和语音识别）相结合。4. 语义分割  Semantic Segmentation计算机视觉的核心是分割过程，它将整个图像分成像素分组，然后可以对其进行标记和分类。特别地，语义分割试图在语义上理解图像中每个像素的角色（例如，它是汽车，摩托车还是其他类型的类）。例如，在上图中，除了识别人，道路，汽车，树木等之外，我们还必须划定每个物体的边界。因此，与分类不同，我们需要从我们的模型进行像素级的预测。与其他计算机视觉任务一样，CNN在分割问题上取得了巨大成功。最流行的初始方法之一是通过滑动窗口进行patches分类，其中每个像素使用其周围的图像pathes分别分类。但是，这在计算上效率非常低，因为我们不重用重叠patches之间的共享功能。加州大学伯克利分校的研究员提出了全卷积网络（FCN），它在没有任何完全连接层的情况下推广端到端CNN体系结构进行密集预测。这允许针对任何尺寸的图像生成分割图，并且与patches分类方法相比也快得多。几乎所有后续的语义分割方法都采用了这种范式。然而，仍然存在一个问题：原始图像分辨率的卷积将非常昂贵。为了解决这个问题，FCN在网络内部使用下采样和上采样。下采样层被称为条带卷积，而上采样层被称为转置卷积。尽管有上采样/下采样层，但由于池中的信息丢失，FCN会生成粗分割图。 SegNet是一种比使用最大池和编码器 - 解码器框架的FCN更高效的内存架构。在SegNet中，从更高分辨率的特征映射中引入了快捷/跳过连接，以改善上采样/下采样的粗糙度。最近的语义分割研究都严重依赖完全卷积网络，如空洞卷积 https://arxiv.org/pdf/1511.07122.pdf，DeepLab https://arxiv.org/pdf/1412.7062.pdfRefineNet https://arxiv.org/pdf/1611.06612.pdf。5. 实例分割 Instance Segmentation除了语义分段之外，实例分段还将不同的实例分类，例如用5种不同颜色标记5辆汽车。在分类中，通常有一个图像包含单个对象作为焦点，任务是说出该图像是什么。但为了分割实例，我们需要执行更复杂的任务。我们看到多个重叠物体和不同背景的复杂景点，我们不仅分类这些不同的物体，而且还确定它们的边界，差异和彼此之间的关系！到目前为止，我们已经看到了如何以许多有趣的方式使用CNN特征，以便用bounding box框住图像中的不同对象。我们可以扩展这种技术来定位每个对象的精确像素，而不仅仅是边界框吗？当然可以， Facebook AI使用称为Mask R-CNN的体系结构研究了此实例分割问题。就像Fast R-CNN和更快的R-CNN一样，Mask R-CNN的底层直觉很直观鉴于更快的R-CNN在物体检测方面的工作如此出色，我们是否可以将其扩展到进行像素级分割？Mask R-CNN通过向Faster R-CNN添加分支来完成此操作，该分支输出一个二进制掩码，该掩码表示给定像素是否为对象的一部分。该分支是基于CNN特征映射的完全卷积网络。给定CNN特征映射作为输入，网络输出一个矩阵，其中像素属于该对象的所有位置均为1，而其他位置为0（这称为二进制掩码）。另外，当在原始Faster R-CNN架构上运行时没有修改时，由RoIPool（感兴趣区域）选择的特征映射区域与原始图像的区域略微错开。由于图像分割需要像素级别的特异性，与边界框不同，这自然会导致不准确。 Mask R-CNN通过调整RoIPool使用称为Roialign（感兴趣区域对齐）的方法更精确地对齐来解决此问题。本质上，RoIAlign使用双线性插值来避免舍入错误，这会导致检测和分割不准确。一旦生成这些蒙版，Mask R-CNN将它们与来自Faster R-CNN的分类和边界框相结合，以生成如此精确的精确分割：结论这5种主要的计算机视觉技术可以帮助计算机从单个或一系列图像中提取，分析和理解有用的信息。还有许多其他我尚未涉及的高级技术，包括样式转换，着色，动作识别，3D对象，人体姿势估计等等。事实上，计算机视觉领域的成本太高，无法深入报道，我鼓励您进一步探索，无论是通过在线课程，博客教程还是正式文档。我强烈推荐CS231n作为初学者，因为您将学习实施，训练和调试自己的神经网络。作为奖励，您可以从我的GitHub存储库中获取所有演讲幻灯片和分配指南。我希望它会引导你改变如何看待这个世界！参考文献：【1】http://cs231n.stanford.edu/"}
{"content2":"机器学习是人工智能的一个分支。人工智能的研究是从以“推理”为重点到以“知识”为重点，再到以“学习”为重点，一条自然、清晰的脉络。显然，机器学习是实现人工智能的一个途径，即以机器学习为手段解决人工智能中的问题。机器学习在近30多年已发展为一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、计算复杂性理论等多门学科。机器学习理论主要是设计和分析一些让计算机可以自动“学习”的算法。机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。因为学习算法中涉及了大量的统计学理论，机器学习与推断统计学联系尤为密切，也被称为统计学习理论。算法设计方面，机器学习理论关注可以实现的，行之有效的学习算法。很多推论问题属于无程序可循难度，所以部分的机器学习研究是开发容易处理的近似算法。机器学习已广泛应用于数据挖掘、计算机视觉、自然语言处理、生物特征识别、搜索引擎、医学诊断、检测信用卡欺诈、证券市场分析、DNA序列测序、语音和手写识别、战略游戏和机器人等领域。定义机器学习有下面几种定义：机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在经验学习中改善具体算法的性能。机器学习是对能通过经验自动改进的计算机算法的研究。机器学习是用数据或以往的经验，以此优化计算机程序的性能标准。一种经常引用的英文定义是：A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.分类机器学习可以分成下面几种类别：监督学习从给定的训练数据集中学习出一个函数，当新的数据到来时，可以根据这个函数预测结果。监督学习的训练集要求是包括输入和输出，也可以说是特征和目标。训练集中的目标是由人标注的。常见的监督学习算法包括回归分析和统计分类。监督学习和非监督学习的差别就是训练集目标是否人标注。他们都有训练集 且都有输入和输出无监督学习与监督学习相比，训练集没有人为标注的结果。常见的无监督学习算法有聚类。半监督学习介于监督学习与无监督学习之间。增强学习通过观察来学习做成如何的动作。每个动作都会对环境有所影响，学习对象根据观察到的周围环境的反馈来做出判断。参考文献Bishop, C. M. (1995). 《模式识别神经网络》，牛津大学出版社. ISBN 0-19-853864-2.Bishop, C. M. (2006). 《模式识别与机器学习》，Springer. ISBN 978-0-387-31073-2.Richard O. Duda, Peter E. Hart, David G. Stork (2001). 《模式分类》（第2版）, New York: Wiley. ISBN 0-471-05669-3.MacKay, D. J. C. (2003). 《信息理论、推理和学习算法》，剑桥大学出版社. ISBN 0-521-64298-1Mitchel.l, T. (1997). 《机器学习》, McGraw Hill. ISBN 0-07-042807-7Sholom Weiss, Casimir Kulikowski (1991). Computer Systems That Learn, Morgan Kaufmann. ISBN 1-55860-065-5."}
{"content2":"零、参考资料有关FPN的介绍见『计算机视觉』FPN特征金字塔网络。网络构架部分代码见Mask_RCNN/mrcnn/model.py中class MaskRCNN的build方法的\"inference\"分支。1、Keras调用GPU设置【*】指定GPUimport os os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"【**】按需分配import tensorflow as tf import keras.backend.tensorflow_backend as KTF config = tf.ConfigProto() config.gpu_options.allow_growth=True #不全部占满显存, 按需分配 # config.gpu_options.per_process_gpu_memory_fraction = 0.3 #指定分配30%空间 sess = tf.Session(config=config)# 设置session KTF.set_session(sess)2、TensorFlow和Keras交互说明下面的交互方法几乎都是对keras的函数式API操作的，不过keras的函数模型转换为model对象也极为方便，KM.Model(input_tensors, output_tensors)操作一下即可。【*】使用TensorFlow建立keras新的层对象在网络中我们可以看到大量的继承了keras.engine.Layer类的新类，这是因为如果TensorFlow函数可以操作keras的tensor，但是其返回的TensorFlow的tensor不能被keras继续处理，所以我们需要建立新的keras层进行转换，将tf的Tensor可作为keras层的__init__参数参与层构建，在__call__方法内部使用tf的函数进行细粒度数据处理，最后返回的是keras层对象。如果不想使用Model类的各种方便方法而执意手动使用tf.Session()训练的话就没有封装它们的必要了。keras的tensor可以直接送入TensorFlow中：import tensorflow as tf import keras.backend as K rpn_match = tf.placeholder(tf.int8, [10, 2]) tf.where(K.equal(rpn_match, 1))一个class实现例子如下，注意需要推断输出的shape：class PyramidROIAlign(KE.Layer): \"\"\"Implements ROI Pooling on multiple levels of the feature pyramid. Params: - pool_shape: [pool_height, pool_width] of the output pooled regions. Usually [7, 7] Inputs: - boxes: [batch, num_boxes, (y1, x1, y2, x2)] in normalized coordinates. Possibly padded with zeros if not enough boxes to fill the array. - image_meta: [batch, (meta data)] Image details. See compose_image_meta() - feature_maps: List of feature maps from different levels of the pyramid. Each is [batch, height, width, channels] Output: Pooled regions in the shape: [batch, num_boxes, pool_height, pool_width, channels]. The width and height are those specific in the pool_shape in the layer constructor. \"\"\" def __init__(self, pool_shape, **kwargs): super(PyramidROIAlign, self).__init__(**kwargs) self.pool_shape = tuple(pool_shape) def call(self, inputs): # num_boxes指的是proposal数目，它们均会作用于每张图片上，只是不同的proposal作用于图片 # 的特征级别不同，我通过循环特征层寻找符合的proposal，应用ROIAlign # Crop boxes [batch, num_boxes, (y1, x1, y2, x2)] in normalized coords boxes = inputs[0] # Image meta # Holds details about the image. See compose_image_meta() image_meta = inputs[1] # Feature Maps. List of feature maps from different level of the # feature pyramid. Each is [batch, height, width, channels] feature_maps = inputs[2:] # Assign each ROI to a level in the pyramid based on the ROI area. y1, x1, y2, x2 = tf.split(boxes, 4, axis=2) h = y2 - y1 w = x2 - x1 # Use shape of first image. Images in a batch must have the same size. image_shape = parse_image_meta_graph(image_meta)['image_shape'][0] # h, w, c # Equation 1 in the Feature Pyramid Networks paper. Account for # the fact that our coordinates are normalized here. # e.g. a 224x224 ROI (in pixels) maps to P4 image_area = tf.cast(image_shape[0] * image_shape[1], tf.float32) roi_level = log2_graph(tf.sqrt(h * w) / (224.0 / tf.sqrt(image_area))) # h、w已经归一化 roi_level = tf.minimum(5, tf.maximum( 2, 4 + tf.cast(tf.round(roi_level), tf.int32))) # 确保值位于2到5之间 roi_level = tf.squeeze(roi_level, 2) # [batch, num_boxes] # Loop through levels and apply ROI pooling to each. P2 to P5. pooled = [] box_to_level = [] for i, level in enumerate(range(2, 6)): # tf.where 返回值格式 [坐标1, 坐标2……] # np.where 返回值格式 [[坐标1.x, 坐标2.x……], [坐标1.y, 坐标2.y……]] ix = tf.where(tf.equal(roi_level, level)) # 返回坐标表示：第n张图片的第i个proposal level_boxes = tf.gather_nd(boxes, ix) # [本level的proposal数目, 4] # Box indices for crop_and_resize. box_indices = tf.cast(ix[:, 0], tf.int32) # 记录每个propose对应图片序号 # Keep track of which box is mapped to which level box_to_level.append(ix) # Stop gradient propogation to ROI proposals level_boxes = tf.stop_gradient(level_boxes) box_indices = tf.stop_gradient(box_indices) # Crop and Resize # From Mask R-CNN paper: \"We sample four regular locations, so # that we can evaluate either max or average pooling. In fact, # interpolating only a single value at each bin center (without # pooling) is nearly as effective.\" # # Here we use the simplified approach of a single value per bin, # which is how it's done in tf.crop_and_resize() # Result: [this_level_num_boxes, pool_height, pool_width, channels] pooled.append(tf.image.crop_and_resize( feature_maps[i], level_boxes, box_indices, self.pool_shape, method=\"bilinear\")) # 输入参数shape: # [batch, image_height, image_width, channels] # [this_level_num_boxes, 4] # [this_level_num_boxes] # [height, pool_width] # Pack pooled features into one tensor pooled = tf.concat(pooled, axis=0) # [batch*num_boxes, pool_height, pool_width, channels] # Pack box_to_level mapping into one array and add another # column representing the order of pooled boxes box_to_level = tf.concat(box_to_level, axis=0) # [batch*num_boxes, 2] box_range = tf.expand_dims(tf.range(tf.shape(box_to_level)[0]), 1) # [batch*num_boxes, 1] box_to_level = tf.concat([tf.cast(box_to_level, tf.int32), box_range], axis=1) # [batch*num_boxes, 3] # 截止到目前，我们获取了记录全部ROIAlign结果feat集合的张量pooled，和记录这些feat相关信息的张量box_to_level， # 由于提取方法的原因，此时的feat并不是按照原始顺序排序（先按batch然后按box index排序），下面我们设法将之恢复顺 # 序（ROIAlign作用于对应图片的对应proposal生成feat） # Rearrange pooled features to match the order of the original boxes # Sort box_to_level by batch then box index # TF doesn't have a way to sort by two columns, so merge them and sort. # box_to_level[i, 0]表示的是当前feat隶属的图片索引，box_to_level[i, 1]表示的是其box序号 sorting_tensor = box_to_level[:, 0] * 100000 + box_to_level[:, 1] # [batch*num_boxes] ix = tf.nn.top_k(sorting_tensor, k=tf.shape( box_to_level)[0]).indices[::-1] ix = tf.gather(box_to_level[:, 2], ix) pooled = tf.gather(pooled, ix) # Re-add the batch dimension # [batch, num_boxes, (y1, x1, y2, x2)], [batch*num_boxes, pool_height, pool_width, channels] shape = tf.concat([tf.shape(boxes)[:2], tf.shape(pooled)[1:]], axis=0) pooled = tf.reshape(pooled, shape) return pooled # [batch, num_boxes, pool_height, pool_width, channels] def compute_output_shape(self, input_shape): return input_shape[0][:2] + self.pool_shape + (input_shape[2][-1], )【**】keras的Lambda函数可以直接将TensorFlow操作引入keraskeras的Module不能接收tf的tensor作为数据流，所有需要使用KL.Lambda将之转化为keras的数据流，如下这样将tf写好的函数输出直接转换为keras的Module可以接收的类型，和上面的方法1相比，这里的lambda接受外部参数（一般位于类的__inti__中）调整函数行为并不方便：rpn_bbox = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x)【***】继承keras.layer的层对象和方法1相比，这种方法同样需要实现__call__方法，不过一般会super父类，用于改写keras已经实现的层方法。class BatchNorm(KL.BatchNormalization): \"\"\"Extends the Keras BatchNormalization class to allow a central place to make changes if needed. Batch normalization has a negative effect on training if batches are small so this layer is often frozen (via setting in Config class) and functions as linear layer. \"\"\" def call(self, inputs, training=None): \"\"\" Note about training values: None: Train BN layers. This is the normal mode False: Freeze BN layers. Good when batch size is small True: (don't use). Set layer in training mode even when making inferences \"\"\" return super(self.__class__, self).call(inputs, training=training)一、共享网络概览按照逻辑顺序，我们首先来看处于流程图左上角的整张图最大的组成分支：特征提取网络。可以看到本部分大致分为以下几个部分（即原图的三列）：ResNet101部分（FPN的bottom-up部分）FPN的up-bottom部分和横向连接部分最终特征重构部分二、源码浏览整个MaskRCNN类初始化之后的第一个方法就是build网络用的，在mode参数为inference情况下，下面给出了正式建立特征提取网络之前的class内部前置代码，class MaskRCNN(): \"\"\"Encapsulates the Mask RCNN model functionality. The actual Keras model is in the keras_model property. \"\"\" def __init__(self, mode, config, model_dir): \"\"\" mode: Either \"training\" or \"inference\" config: A Sub-class of the Config class model_dir: Directory to save training logs and trained weights \"\"\" assert mode in ['training', 'inference'] self.mode = mode self.config = config self.model_dir = model_dir self.set_log_dir() self.keras_model = self.build(mode=mode, config=config) def build(self, mode, config): \"\"\"Build Mask R-CNN architecture. input_shape: The shape of the input image. mode: Either \"training\" or \"inference\". The inputs and outputs of the model differ accordingly. \"\"\" assert mode in ['training', 'inference'] 　　　　 # Image size must be dividable by 2 multiple times h, w = config.IMAGE_SHAPE[:2] # [1024 1024 3] if h / 2**6 != int(h / 2**6) or w / 2**6 != int(w / 2**6): raise Exception(\"Image size must be dividable by 2 at least 6 times \" \"to avoid fractions when downscaling and upscaling.\" # <----- \"For example, use 256, 320, 384, 448, 512, ... etc. \") # Inputs input_image = KL.Input( shape=[None, None, config.IMAGE_SHAPE[2]], name=\"input_image\") input_image_meta = KL.Input(shape=[config.IMAGE_META_SIZE], name=\"input_image_meta\") if mode == \"training\": …… elif mode == \"inference\": # Anchors in normalized coordinates input_anchors = KL.Input(shape=[None, 4], name=\"input_anchors\")这里强制要求了图片裁剪后尺度为2^n，且n>=6，保证下采样后不产生小数整个程序需要外部输入的变量（inference模式）仅有三个，注意keras的习惯不同于placeholder，上面代码的shape没有包含batch，实际shape是下面的样式：input_image：输入图片，[batch, None, None, config.IMAGE_SHAPE[2]]input_image_meta：图片的信息（包含形状、预处理信息等，后面会介绍），[batch, config.IMAGE_META_SIZE]input_anchors：锚框，[batch, None, 4]ResNet101部分接上面build函数代码，经由如下判断（inference中该参数是字符串\"resnet101\"，所以进入else分支），建立ResNet网络图，# Build the shared convolutional layers. # Bottom-up Layers # Returns a list of the last layers of each stage, 5 in total. # Don't create the thead (stage 5), so we pick the 4th item in the list. if callable(config.BACKBONE): _, C2, C3, C4, C5 = config.BACKBONE(input_image, stage5=True, train_bn=config.TRAIN_BN) else: _, C2, C3, C4, C5 = resnet_graph(input_image, config.BACKBONE, stage5=True, train_bn=config.TRAIN_BN)上述主函数调用ResNet图构建代码如下，其包含应用shortcut和没有应用shortcut两种子结构：(图摘自网上)############################################################ # Resnet Graph ############################################################ # Code adopted from: # https://github.com/fchollet/deep-learning-models/blob/master/resnet50.py def identity_block(input_tensor, kernel_size, filters, stage, block, use_bias=True, train_bn=True): \"\"\"The identity_block is the block that has no conv layer at shortcut # Arguments input_tensor: input tensor kernel_size: default 3, the kernel size of middle conv layer at main path filters: list of integers, the nb_filters of 3 conv layer at main path stage: integer, current stage label, used for generating layer names block: 'a','b'..., current block label, used for generating layer names use_bias: Boolean. To use or not use a bias in conv layers. train_bn: Boolean. Train or freeze Batch Norm layers \"\"\" nb_filter1, nb_filter2, nb_filter3 = filters conv_name_base = 'res' + str(stage) + block + '_branch' bn_name_base = 'bn' + str(stage) + block + '_branch' x = KL.Conv2D(nb_filter1, (1, 1), name=conv_name_base + '2a', use_bias=use_bias)(input_tensor) x = BatchNorm(name=bn_name_base + '2a')(x, training=train_bn) x = KL.Activation('relu')(x) x = KL.Conv2D(nb_filter2, (kernel_size, kernel_size), padding='same', name=conv_name_base + '2b', use_bias=use_bias)(x) x = BatchNorm(name=bn_name_base + '2b')(x, training=train_bn) x = KL.Activation('relu')(x) x = KL.Conv2D(nb_filter3, (1, 1), name=conv_name_base + '2c', use_bias=use_bias)(x) x = BatchNorm(name=bn_name_base + '2c')(x, training=train_bn) x = KL.Add()([x, input_tensor]) x = KL.Activation('relu', name='res' + str(stage) + block + '_out')(x) return x def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2), use_bias=True, train_bn=True): \"\"\"conv_block is the block that has a conv layer at shortcut # Arguments input_tensor: input tensor kernel_size: default 3, the kernel size of middle conv layer at main path filters: list of integers, the nb_filters of 3 conv layer at main path stage: integer, current stage label, used for generating layer names block: 'a','b'..., current block label, used for generating layer names use_bias: Boolean. To use or not use a bias in conv layers. train_bn: Boolean. Train or freeze Batch Norm layers Note that from stage 3, the first conv layer at main path is with subsample=(2,2) And the shortcut should have subsample=(2,2) as well \"\"\" nb_filter1, nb_filter2, nb_filter3 = filters conv_name_base = 'res' + str(stage) + block + '_branch' bn_name_base = 'bn' + str(stage) + block + '_branch' x = KL.Conv2D(nb_filter1, (1, 1), strides=strides, name=conv_name_base + '2a', use_bias=use_bias)(input_tensor) x = BatchNorm(name=bn_name_base + '2a')(x, training=train_bn) x = KL.Activation('relu')(x) x = KL.Conv2D(nb_filter2, (kernel_size, kernel_size), padding='same', name=conv_name_base + '2b', use_bias=use_bias)(x) x = BatchNorm(name=bn_name_base + '2b')(x, training=train_bn) x = KL.Activation('relu')(x) x = KL.Conv2D(nb_filter3, (1, 1), name=conv_name_base + '2c', use_bias=use_bias)(x) x = BatchNorm(name=bn_name_base + '2c')(x, training=train_bn) shortcut = KL.Conv2D(nb_filter3, (1, 1), strides=strides, name=conv_name_base + '1', use_bias=use_bias)(input_tensor) shortcut = BatchNorm(name=bn_name_base + '1')(shortcut, training=train_bn) x = KL.Add()([x, shortcut]) x = KL.Activation('relu', name='res' + str(stage) + block + '_out')(x) return x def resnet_graph(input_image, architecture, stage5=False, train_bn=True): \"\"\"Build a ResNet graph. architecture: Can be resnet50 or resnet101 stage5: Boolean. If False, stage5 of the network is not created train_bn: Boolean. Train or freeze Batch Norm layers \"\"\" assert architecture in [\"resnet50\", \"resnet101\"] # Stage 1 x = KL.ZeroPadding2D((3, 3))(input_image) x = KL.Conv2D(64, (7, 7), strides=(2, 2), name='conv1', use_bias=True)(x) x = BatchNorm(name='bn_conv1')(x, training=train_bn) x = KL.Activation('relu')(x) C1 = x = KL.MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\")(x) # Stage 2 x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1), train_bn=train_bn) x = identity_block(x, 3, [64, 64, 256], stage=2, block='b', train_bn=train_bn) C2 = x = identity_block(x, 3, [64, 64, 256], stage=2, block='c', train_bn=train_bn) # Stage 3 x = conv_block(x, 3, [128, 128, 512], stage=3, block='a', train_bn=train_bn) x = identity_block(x, 3, [128, 128, 512], stage=3, block='b', train_bn=train_bn) x = identity_block(x, 3, [128, 128, 512], stage=3, block='c', train_bn=train_bn) C3 = x = identity_block(x, 3, [128, 128, 512], stage=3, block='d', train_bn=train_bn) # Stage 4 x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a', train_bn=train_bn) block_count = {\"resnet50\": 5, \"resnet101\": 22}[architecture] for i in range(block_count): x = identity_block(x, 3, [256, 256, 1024], stage=4, block=chr(98 + i), train_bn=train_bn) C4 = x # Stage 5 if stage5: x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a', train_bn=train_bn) x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b', train_bn=train_bn) C5 = x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c', train_bn=train_bn) else: C5 = None return [C1, C2, C3, C4, C5]BN层为了可能的扩展进行了封装，不过暂时没什么扩展：class BatchNorm(KL.BatchNormalization): \"\"\"Extends the Keras BatchNormalization class to allow a central place to make changes if needed. Batch normalization has a negative effect on training if batches are small so this layer is often frozen (via setting in Config class) and functions as linear layer. \"\"\" def call(self, inputs, training=None): \"\"\" Note about training values: None: Train BN layers. This is the normal mode False: Freeze BN layers. Good when batch size is small True: (don't use). Set layer in training mode even when making inferences \"\"\" return super(self.__class__, self).call(inputs, training=training)FPN处理部分接上面build函数代码，剩下部分比较简单，和示意图对比几乎平铺直叙，# Top-down Layers # TODO: add assert to varify feature map sizes match what's in config P5 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c5p5')(C5) # 256 P4 = KL.Add(name=\"fpn_p4add\")([ KL.UpSampling2D(size=(2, 2), name=\"fpn_p5upsampled\")(P5), KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c4p4')(C4)]) P3 = KL.Add(name=\"fpn_p3add\")([ KL.UpSampling2D(size=(2, 2), name=\"fpn_p4upsampled\")(P4), KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c3p3')(C3)]) P2 = KL.Add(name=\"fpn_p2add\")([ KL.UpSampling2D(size=(2, 2), name=\"fpn_p3upsampled\")(P3), KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c2p2')(C2)]) # Attach 3x3 conv to all P layers to get the final feature maps. P2 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=\"SAME\", name=\"fpn_p2\")(P2) P3 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=\"SAME\", name=\"fpn_p3\")(P3) P4 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=\"SAME\", name=\"fpn_p4\")(P4) P5 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=\"SAME\", name=\"fpn_p5\")(P5) # P6 is used for the 5th anchor scale in RPN. Generated by # subsampling from P5 with stride of 2. P6 = KL.MaxPooling2D(pool_size=(1, 1), strides=2, name=\"fpn_p6\")(P5)接上面build函数代码，最后我们提取的特征集合如下：# Note that P6 is used in RPN, but not in the classifier heads. rpn_feature_maps = [P2, P3, P4, P5, P6] mrcnn_feature_maps = [P2, P3, P4, P5]其中rpn_feature_maps对应图中的实线输出，送入RPN网络分类/回归得到锚框的前景/背景鉴别结果；而mrcnn_feature_maps则是后面进行ROI Align时的切割目标。附录、build函数总览def build(self, mode, config): \"\"\"Build Mask R-CNN architecture. input_shape: The shape of the input image. mode: Either \"training\" or \"inference\". The inputs and outputs of the model differ accordingly. \"\"\" assert mode in ['training', 'inference'] # Image size must be dividable by 2 multiple times h, w = config.IMAGE_SHAPE[:2] # [1024 1024 3] if h / 2**6 != int(h / 2**6) or w / 2**6 != int(w / 2**6): # 这里就限定了下采样不会产生坐标误差 raise Exception(\"Image size must be dividable by 2 at least 6 times \" \"to avoid fractions when downscaling and upscaling.\" \"For example, use 256, 320, 384, 448, 512, ... etc. \") # Inputs input_image = KL.Input( shape=[None, None, config.IMAGE_SHAPE[2]], name=\"input_image\") input_image_meta = KL.Input(shape=[config.IMAGE_META_SIZE], name=\"input_image_meta\") if mode == \"training\": # RPN GT input_rpn_match = KL.Input( shape=[None, 1], name=\"input_rpn_match\", dtype=tf.int32) input_rpn_bbox = KL.Input( shape=[None, 4], name=\"input_rpn_bbox\", dtype=tf.float32) # Detection GT (class IDs, bounding boxes, and masks) # 1. GT Class IDs (zero padded) input_gt_class_ids = KL.Input( shape=[None], name=\"input_gt_class_ids\", dtype=tf.int32) # 2. GT Boxes in pixels (zero padded) # [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)] in image coordinates input_gt_boxes = KL.Input( shape=[None, 4], name=\"input_gt_boxes\", dtype=tf.float32) # Normalize coordinates gt_boxes = KL.Lambda(lambda x: norm_boxes_graph( x, K.shape(input_image)[1:3]))(input_gt_boxes) # 3. GT Masks (zero padded) # [batch, height, width, MAX_GT_INSTANCES] if config.USE_MINI_MASK: input_gt_masks = KL.Input( shape=[config.MINI_MASK_SHAPE[0], config.MINI_MASK_SHAPE[1], None], name=\"input_gt_masks\", dtype=bool) else: input_gt_masks = KL.Input( shape=[config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1], None], name=\"input_gt_masks\", dtype=bool) elif mode == \"inference\": # Anchors in normalized coordinates input_anchors = KL.Input(shape=[None, 4], name=\"input_anchors\") # Build the shared convolutional layers. # Bottom-up Layers # Returns a list of the last layers of each stage, 5 in total. # Don't create the thead (stage 5), so we pick the 4th item in the list. if callable(config.BACKBONE): _, C2, C3, C4, C5 = config.BACKBONE(input_image, stage5=True, train_bn=config.TRAIN_BN) else: _, C2, C3, C4, C5 = resnet_graph(input_image, config.BACKBONE, stage5=True, train_bn=config.TRAIN_BN) # Top-down Layers # TODO: add assert to varify feature map sizes match what's in config P5 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c5p5')(C5) # 256 P4 = KL.Add(name=\"fpn_p4add\")([ KL.UpSampling2D(size=(2, 2), name=\"fpn_p5upsampled\")(P5), KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c4p4')(C4)]) P3 = KL.Add(name=\"fpn_p3add\")([ KL.UpSampling2D(size=(2, 2), name=\"fpn_p4upsampled\")(P4), KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c3p3')(C3)]) P2 = KL.Add(name=\"fpn_p2add\")([ KL.UpSampling2D(size=(2, 2), name=\"fpn_p3upsampled\")(P3), KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c2p2')(C2)]) # Attach 3x3 conv to all P layers to get the final feature maps. P2 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=\"SAME\", name=\"fpn_p2\")(P2) P3 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=\"SAME\", name=\"fpn_p3\")(P3) P4 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=\"SAME\", name=\"fpn_p4\")(P4) P5 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=\"SAME\", name=\"fpn_p5\")(P5) # P6 is used for the 5th anchor scale in RPN. Generated by # subsampling from P5 with stride of 2. P6 = KL.MaxPooling2D(pool_size=(1, 1), strides=2, name=\"fpn_p6\")(P5) # Note that P6 is used in RPN, but not in the classifier heads. rpn_feature_maps = [P2, P3, P4, P5, P6] mrcnn_feature_maps = [P2, P3, P4, P5] # Anchors if mode == \"training\": anchors = self.get_anchors(config.IMAGE_SHAPE) # Duplicate across the batch dimension because Keras requires it # TODO: can this be optimized to avoid duplicating the anchors? anchors = np.broadcast_to(anchors, (config.BATCH_SIZE,) + anchors.shape) # A hack to get around Keras's bad support for constants anchors = KL.Lambda(lambda x: tf.Variable(anchors), name=\"anchors\")(input_image) else: anchors = input_anchors # RPN Model, 返回的是keras的Module对象, 注意keras中的Module对象是可call的 rpn = build_rpn_model(config.RPN_ANCHOR_STRIDE, # 1 3 256 len(config.RPN_ANCHOR_RATIOS), config.TOP_DOWN_PYRAMID_SIZE) # Loop through pyramid layers layer_outputs = [] # list of lists for p in rpn_feature_maps: layer_outputs.append(rpn([p])) # 保存各pyramid特征经过RPN之后的结果 # Concatenate layer outputs # Convert from list of lists of level outputs to list of lists # of outputs across levels. # e.g. [[a1, b1, c1], [a2, b2, c2]] => [[a1, a2], [b1, b2], [c1, c2]] output_names = [\"rpn_class_logits\", \"rpn_class\", \"rpn_bbox\"] outputs = list(zip(*layer_outputs)) # [[logits2,……6], [class2,……6], [bbox2,……6]] outputs = [KL.Concatenate(axis=1, name=n)(list(o)) for o, n in zip(outputs, output_names)] # [batch, num_anchors, 2/4] # 其中num_anchors指的是全部特征层上的anchors总数 rpn_class_logits, rpn_class, rpn_bbox = outputs # Generate proposals # Proposals are [batch, N, (y1, x1, y2, x2)] in normalized coordinates # and zero padded. # POST_NMS_ROIS_INFERENCE = 1000 # POST_NMS_ROIS_TRAINING = 2000 proposal_count = config.POST_NMS_ROIS_TRAINING if mode == \"training\"\\ else config.POST_NMS_ROIS_INFERENCE # [IMAGES_PER_GPU, num_rois, (y1, x1, y2, x2)] # IMAGES_PER_GPU取代了batch，之后说的batch都是IMAGES_PER_GPU rpn_rois = ProposalLayer( proposal_count=proposal_count, nms_threshold=config.RPN_NMS_THRESHOLD, # 0.7 name=\"ROI\", config=config)([rpn_class, rpn_bbox, anchors]) if mode == \"training\": # Class ID mask to mark class IDs supported by the dataset the image # came from. active_class_ids = KL.Lambda( lambda x: parse_image_meta_graph(x)[\"active_class_ids\"] )(input_image_meta) if not config.USE_RPN_ROIS: # Ignore predicted ROIs and use ROIs provided as an input. input_rois = KL.Input(shape=[config.POST_NMS_ROIS_TRAINING, 4], name=\"input_roi\", dtype=np.int32) # Normalize coordinates target_rois = KL.Lambda(lambda x: norm_boxes_graph( x, K.shape(input_image)[1:3]))(input_rois) else: target_rois = rpn_rois # Generate detection targets # Subsamples proposals and generates target outputs for training # Note that proposal class IDs, gt_boxes, and gt_masks are zero # padded. Equally, returned rois and targets are zero padded. rois, target_class_ids, target_bbox, target_mask =\\ DetectionTargetLayer(config, name=\"proposal_targets\")([ target_rois, input_gt_class_ids, gt_boxes, input_gt_masks]) # Network Heads # TODO: verify that this handles zero padded ROIs mrcnn_class_logits, mrcnn_class, mrcnn_bbox =\\ fpn_classifier_graph(rois, mrcnn_feature_maps, input_image_meta, config.POOL_SIZE, config.NUM_CLASSES, train_bn=config.TRAIN_BN, fc_layers_size=config.FPN_CLASSIF_FC_LAYERS_SIZE) mrcnn_mask = build_fpn_mask_graph(rois, mrcnn_feature_maps, input_image_meta, config.MASK_POOL_SIZE, config.NUM_CLASSES, train_bn=config.TRAIN_BN) # TODO: clean up (use tf.identify if necessary) output_rois = KL.Lambda(lambda x: x * 1, name=\"output_rois\")(rois) # Losses rpn_class_loss = KL.Lambda(lambda x: rpn_class_loss_graph(*x), name=\"rpn_class_loss\")( [input_rpn_match, rpn_class_logits]) rpn_bbox_loss = KL.Lambda(lambda x: rpn_bbox_loss_graph(config, *x), name=\"rpn_bbox_loss\")( [input_rpn_bbox, input_rpn_match, rpn_bbox]) class_loss = KL.Lambda(lambda x: mrcnn_class_loss_graph(*x), name=\"mrcnn_class_loss\")( [target_class_ids, mrcnn_class_logits, active_class_ids]) bbox_loss = KL.Lambda(lambda x: mrcnn_bbox_loss_graph(*x), name=\"mrcnn_bbox_loss\")( [target_bbox, target_class_ids, mrcnn_bbox]) mask_loss = KL.Lambda(lambda x: mrcnn_mask_loss_graph(*x), name=\"mrcnn_mask_loss\")( [target_mask, target_class_ids, mrcnn_mask]) # Model inputs = [input_image, input_image_meta, input_rpn_match, input_rpn_bbox, input_gt_class_ids, input_gt_boxes, input_gt_masks] if not config.USE_RPN_ROIS: inputs.append(input_rois) outputs = [rpn_class_logits, rpn_class, rpn_bbox, mrcnn_class_logits, mrcnn_class, mrcnn_bbox, mrcnn_mask, rpn_rois, output_rois, rpn_class_loss, rpn_bbox_loss, class_loss, bbox_loss, mask_loss] model = KM.Model(inputs, outputs, name='mask_rcnn') else: # Network Heads # Proposal classifier and BBox regressor heads # output shapes: # mrcnn_class_logits: [batch, num_rois, NUM_CLASSES] classifier logits (before softmax) # mrcnn_class: [batch, num_rois, NUM_CLASSES] classifier probabilities # mrcnn_bbox(deltas): [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))] mrcnn_class_logits, mrcnn_class, mrcnn_bbox =\\ fpn_classifier_graph(rpn_rois, mrcnn_feature_maps, input_image_meta, config.POOL_SIZE, config.NUM_CLASSES, train_bn=config.TRAIN_BN, fc_layers_size=config.FPN_CLASSIF_FC_LAYERS_SIZE) # Detections # output is [batch, num_detections, (y1, x1, y2, x2, class_id, score)] in # normalized coordinates detections = DetectionLayer(config, name=\"mrcnn_detection\")( [rpn_rois, mrcnn_class, mrcnn_bbox, input_image_meta]) # Create masks for detections detection_boxes = KL.Lambda(lambda x: x[..., :4])(detections) mrcnn_mask = build_fpn_mask_graph(detection_boxes, mrcnn_feature_maps, input_image_meta, config.MASK_POOL_SIZE, config.NUM_CLASSES, train_bn=config.TRAIN_BN) model = KM.Model([input_image, input_image_meta, input_anchors], [detections, mrcnn_class, mrcnn_bbox, mrcnn_mask, rpn_rois, rpn_class, rpn_bbox], name='mask_rcnn') # Add multi-GPU support. if config.GPU_COUNT > 1: from mrcnn.parallel_model import ParallelModel model = ParallelModel(model, config.GPU_COUNT) return model"}
{"content2":"相关学习资料如下：cousera课程: https://www.coursera.org/learn/robotics-perceptionyoutube课程： https://www.youtube.com/watch?v=RDkwklFGMfoTutorial:  https://www.cse.wustl.edu/~furukawa/papers/fnt_mvs.pdf分清楚 3D 重建 vs.  3D 建模这里一般指的是用多张2D图片加上额外的信息，进行重建 3D@知乎问题：三维重建 3D reconstruction有哪些使用算法以下为知乎中的相关回答用一组图片来做3D reconstruction需要的算法： SFM(Structure from motion)，也就是从时间系列的2D图像中推算3D信息。使用这种方法的软件有： Pix4Dmapper, Autodesk 123D Catch, PhotoModeler, VisualSFM大多数三维重建的数据源是RGB图像，或者RGBD这种带有图像深度信息的图像（用kinect之类的特殊特备拍出来的）。SFM是最经典的三维重建方案：1.特征提取（SIFT, SURF, FAST等一堆方法）2.配准（主流是RANSAC和它的改进版）3.全局优化bundle adjustment   用来估计相机参数4.数据融合SFM算法是一种基于各种收集到的无序图片进行三维重建的离线算法。在进行核心的算法structure-from-motion之前需要一些准备工作，挑选出合适的图片。三维重构算法得看试用的是什么传感器，如果是双目相机，一般是极线几何加视觉特征配准的算法，优化就是bundle adjustment; 如果是单目相机，较早的有PTAM,DTAM,, 近几年就是SFM比较火，如果是Kinect之类的RGBD相机，比较好的就是微软的KinectFusion, PCL的开源KinFu，以及MIT加强版Kintinuous; 如果是用激光，一般就是SLAM做了。基于rgb单目主要就是基于multiview geometry(SFM) ,比较经典的是DTAM和微软的monofusion, 缺点是没法做稍大场景的重建以及重建精度不高。双目的话就认为是rgbd相机的重建，底层可以得到深度图的原理就是结构光，双目，激光，或者tof(结构光方法适合室内高精度重建，商业产品较多， sfm方法比结构光方法更方便，无需事先标定相机，但精度差些，很多无人机对大型建筑建模就是用的sfm方法)@ 知乎-刘锐参考文章 ：  https://zhuanlan.zhihu.com/p/304455043D重构算法可以描述为当给定某个物体或场景的一组照片时，在一些假定条件下，比如物体材料，观测视角和光照环境等，通过估计一个最相似的3D shape来解释这组照片。一个完整的3D重构流程通常包含以下几个步骤：1. 收集场景图片2. 计算每张图片的相机参数3. 通过图组来重构场景的3D shape以及对应的相机参数4. 有选择的重构场景的材料等最核心步骤就是第三步： 3D shape的重构算法常规的3D shape representation主要有四种： 深度图(depth), 点云(point cloud), 体素(voxel), 网格(mesh)今年来也出现了很多基于deep learning的方法：David Eigen NIPS2014:   Depth Map Prediction from a Single Image using a Multi-Scale Deep NetworkFayao Liu CVPR2015  Deep Convolutional Neural Fields for Depth Estimation from a Single Image这两篇论文都是利用CNN网络结构预测 a single image与其对应的depth map之间的关系。但是depth image还不足以解释重构原始input的信息，它只能作为3D scene understanding的一个辅助信息。所以开始研究利用一组二维图来重构3D点云图或voxel以及mesh图。基于deep learning的3D点云和mesh重构是较难以实施的，因为DL学习一个物体完整的架构需要大量数据的支持。然后传统的3D模型是由vertices和trangulation mesh组成的，因此不一样的data size造成了training的困难。所以后续大家都用voxelization(Voxel)的方法把所有CAD model转成binary voxel模式(有值为1， 空缺为0)这样保证了每个模型都是相同的大小。最近的一篇论文： Choy ECCV2016:    3D-R2N2:A Unified Approach for Single and Multi-view 3D Object Reconstruction采用深度学习从2D图像到其对应的3D voxel模型的映射：首先利用一个标准的CNN结构对原始input image进行编码，然后用Deconv进行解码，最后用3D LSTM的每个单元重构output voxel.3D voxel是三维的，它的resolution成指数增长，所以它的计算相对复杂，目前的工作主要采用32*32*3以下的分辨率以防止过多的占用内存。但是也使得最终重构的3D model分辨率并不高。所以科研道路道阻且长。mesh和point cloud是不规则的几何数据形式，因此直接使用CNN是不可行的。但是可以考虑将3D mesh data转化成graphs形式，再对3D曲面上的2D参数进行卷积。具体有Spatial construction(Geodesic CNN)和Spectral construction(Spectral CNN)基于point cloud的方法，看Hao Su的CVPR2017论文 PointNet: Deep learning on Point Sets for 3D Classification and Segmentation以及 A Point Set Generation Network for 3D Object Reconstruction from a Single Image.基于mesh和point cloud的方法总的来讲数学东西多，而且细节回复上效果欠佳。不过可以考虑voxel来提高重构精度。@ 知乎问题：计算机图形学与机器学习（深度学习）怎么结合起来？以下为MSRA相关回答随着深度学习尤其是卷积神经网络（CNN）这一利器在各领域里的卓越表现，如何将CNN运用到三维数据上成为计算机视觉和图形学一个焦点课题。分析与处理三维形体是计算机图形学中的一个基本任务与研究方向。近年来随着三维数据获取的便捷和三维数据集的迅猛增长，这个研究方向也面临新的挑战和契机。一方面，在新的数据形势下，传统算法的一些前提假设不再成立，研发新型算法的需求迫在眉睫。另一方面，大数据的出现，可以使得传统的三维分析和机器学习更加有机地结合起来，从而帮助人们加深对三维世界的认知，有效地理解现实三维几何世界并构建虚拟数字世界。“三维去噪”由于Kinect设备的低精度，三维网格存在着大量的噪声。如图(b)去噪问题本质上是求解一个病态的逆问题：在噪声的类型和程度未知、真实模型的几何特性未知的前提下，如果要把噪声从输入中完美剥离，必然需要引入各种假设来辅助求解。真实噪声是与数据以及设备相关的，简单的噪声模型不可能刻画出真实的噪声。忽略真实数据去研发一个放之四海而皆准的去噪算法是不可行的。噪声来自数据，我们应该从数据中探究其中的奥秘。我们的工作也体现了数据的威力.Siggraph Asia 2016发表的文章，Mesh Denoising via Cascaded Normal Regression“形状空间与3Ｄ CNN”三维形状在数字世界里可以有不同的表达，如三角网格、点云、体素、参数曲面、隐式曲面等。不同的表达和CNN也有着不同的结合方式。有的方法将网格参数化到二维空间，在二维空间编码几何特征，并利用CNN在二维定义域上类似图像空间进行卷积；有的将曲面局部处理成测地圆盘域并在其上编码几何信号，然后在圆盘上进行CNN卷积；也有的以三维空间的体素作为定义域，示性函数作为信号（即物体形状内部信号编码为1，外部为0）进行3D卷积，将CNN直接拓展到三维空间；还有一大类方法是利用空间投影将物体变为多个视图下的二维影像，然后当作图像来处理。在近年的视觉、机器学习、计算机图形学的会议上，如CVPR/ICCV/NIPS/SIGGRAPH，针对三维形状的深度学习网络已开始大放光彩。但这些方法各有利弊，对输入也各有不同的要求。基于体素的3D CNN是图像空间CNN的自然推广，然而从二维推广到三维，CNN涉及的离散元素（2D是像素，3D是体素）个数是空间格点分辨率的三次方，即 。庞大的存储量和计算量让基于体素的3D CNN畏足于高分辨率，徘徊于 这样低分辨率的数据中，使得该方法在众多的三维学习任务中饮恨败北。提出基于八叉树的CNN O-CNN Siggraph2017上的文章 O-CNN: Octree-based Convolutional Neural Networks for 3D Shape AnalysisToF是测飞行时间，Time of Flight, 最开始求取深度的，是激光雷达，但是成本很高，军用较多TOF是3D深度摄像技术中的一种方案。目前主流的3D深度摄像方案有三种： 结构光，TOF, 双目成像双目测距成像因为效率低，算法难，精度差，容易受到环境因素干扰；TOF方案同样有精度缺陷，传感器体积小型化后对分辨率赢你选哪个很大。应用前景：百度目前在做基于深度学习的高精度导航模型构建， AR无人机建筑模型重建iphone X 3D摄像头参考文章： https://zhuanlan.zhihu.com/p/293260393D视觉作为一项激动人心的新技术，早已经出现在微软Kinect、英特尔RealSense等消费级产品中。近几年，随着硬件端技术的不断进步，算法与软件层面的不断优化，3D深度视觉的精度和实用性得到大幅提升，使得“3D深度相机+手势/人脸识别”具备了大规模进入移动智能终端的基础。作为全球手机当之无愧的龙头，苹果率先大规模采用3D视觉技术，将彻底激活3D视觉市场，开启全新时代。3D视觉技术不仅仅在识别精度方面大幅提升，更重要的是打开了更加广阔的人工智能应用空间。随着机器视觉、人工智能、人机交互等科学技术的发展，各种高智能机器人开始走进现实，3D视觉技术成为助力制造业实现“智能化”转型的好帮手。大家耳熟能详的深度摄像头技术和应用有英特尔的RealSense、微软的 Kinect、苹果的 PrimeSense、以及谷歌的Project Tango等。不过可以看到这一技术的研究和开发多为国外公司，国内计算视觉方面的公司或创业团队屈指可数，技术上的壁垒依旧较大。关于目前市场上的深度相机的技术方案主要有以下三种： 双目被动视觉、结构光、TOF。 双目被动视觉主要是利用两个光学摄像头，通过左右立体像对匹配后，再经过三角测量法来得到深度信息。此算法复杂度高，难度很大，处理芯片需要很高的计算性能，同时它也继承了普通RGB摄像头的缺点：在昏暗环境下以及特征不明显的情况下并不适用。结构光的原理是通过红外激光发射相对随机但又固定的斑点图案，这些光斑打在物体上后，因为与摄像头距离不同，被摄像头捕捉到的位置也不尽相同。然后先计算拍到的图的斑点与标定的标准图案在不同位置的位移，引入摄像头位置、传感器大小等参数计算出物体与摄像头的距离。微软在Kinect二代采用的是ToF的技术。ToF是Time of flight的简写，直译为飞行时间的意思。所谓飞行时间法3D成像，是通过给目标连续发送光脉冲，然后用传感器接收从物体返回的光，通过探测光脉冲的飞行（往返）时间来得到目标物距离。相比之下，结构光技术的优势是比ToF更加成熟，成本更低，更加适合用在手机等移动设备上。深度摄像头是所有需要三维视觉设备的必需模块，有了它，设备就能够实时获取周围环境物体三维尺寸和深度信息，更全面的读懂世界。深度摄像头给室内导航与定位、避障、动作捕捉、三维扫描建模等应用提供了基础的技术支持，成为现今行业研究热点。如今iPhone X搭载3D深度摄像头势必会大力推动机器视觉领域的发展，助力机器人产业实现完美“智能化转型”"}
{"content2":"计算机视觉是人工智能技术的一个重要领域，打个比方（不一定恰当），我认为计算机视觉是人工智能时代的眼睛，可见其重要程度。计算机视觉其实是一个很宏大的概念，下图是有人总结的计算机视觉所需要的技能树。如果你是一个对计算机视觉一无所知的小白，千万不要被这棵技能树吓到。没有哪个人能够同时掌握以上所有的技能，这棵树只是让你对计算机视觉有个粗浅的认识。先来打点鸡血，看看计算机视觉有什么用吧。下面的视频是计算机视觉在自动驾驶上的实际应用，其中涉及立体视觉、光流估计、视觉里程计、三维物体检测与识别、三维物体跟踪等计算机视觉领域的关键技术。以下是我站在一个小白的视角给出一个入门计算机视觉的相对轻松的姿势。宏观认识小白通常看到这么多的细分方向大脑一片茫然，到底是学习人脸识别、物体跟踪，又或者是计算摄影，三维重建呢？不知道该怎么下手。其实这些细分方向有很多共通的知识，我的建议是心急吃不了热豆腐，只有对计算机视觉这个领域有了一个初步的全面了解，你才能够结合实际问题找到自己感兴趣的研究方向，而兴趣能够支持一个自学的小白克服困难持续走下去。1、入门书籍既然说是入门，这里就不推荐类似《 Multiple View Geometry in Computer Vision》这种虽然经典但是小白看了容易放弃的书了。像素级的图像处理知识是计算机视觉的底层基础知识。不管你以后从事计算机视觉的哪个细分领域，这些基础知识都是必须要了解的。即使一个急切入门的小白，这一关也必须走的踏实。看到网上有人说直接从某个项目开始，边做边学，这样学的快。对此我表示部分赞成，原因是他忽略了基础知识的重要性，脑子里没有基本的术语概念知识打底，很多问题他根本不知道如何恰当的表达，遇到问题也没有思路，不知道如何搜索，这会严重拖慢进度，也无法做较深入的研究，欲速则不达。入门图像处理的基础知识也不是直接去啃死书，否则几个公式和术语可能就会把小白打翻在地。这里推荐两条途径，都是从实践出发并与理论结合：一个是OpenCV，一个是MATLAB。OpenCV以C++为基础，需要具备一定的编程基础，可移植性强，运行速度比较快，比较适合实际的工程项目，在公司里用的较多；MATLAB只需要非常简单的编程基础就可以很快上手，实现方便，代码比较简洁，可参考的资料非常丰富，方便快速尝试某个算法效果，适合做学术研究。当然两者搭配起来用更好啦。下面分别介绍一下。用MATLAB学习图像处理推荐使用冈萨雷斯的《数字图像处理（MATLAB版）》（英文原版2001年出版，中译版2005年）。不需要一上来就全部过一遍，只需要结合MATLAB学习一下基本原理、图像变换、形态学处理、图像分割，以上章节强烈建议按照书上手动敲一遍代码（和看一遍的效果完全不同），其他章节可快速扫描一遍即可。但这本书比较注重实践，对理论的解释不多，理论部分不明白的可以在配套的冈萨雷斯的《数字图像处理（第二版）》这本书里查找，这本书主要是作为工具书使用，以后遇到相关术语知道去哪里查就好。用OpenCV学习图像处理OpenCV（Open Source Computer Vision Library）是一个开源跨平台计算机视觉程序库，主要有C++预研编写，包含了500多个用于图像/视频处理和计算机视觉的通用算法。学习OpenCV参考《学习OpenCV》或者《OpenCV 2 计算机视觉编程手册》都可以。这两本都是偏实践的书，理论知识较少，按照书上的步骤敲代码，可以快速了解到OpenCV的强大，想要实现某个功能，只要学会查函数（在https://www.docs.opencv.org/查询对应版本），调函数就可以轻松搞定。由于每个例子都有非常直观的可视化图像输出，所以学起来比较轻松有趣。2、进阶书籍经过前面对图像处理的基本学习，小白已经了解了图像处理的基础知识，并且会使用OpenCV或MATLAB来实现某个简单的功能。但是这些知识太单薄了，并且比较陈旧，计算机视觉领域还有大量的新知识在等你。同样给你两种选择，当然两个都选更佳。一本书是2010年出版的美国华盛顿大学Richard Szeliski写的《Computer Vision: Algorithms and Application》；一本是2012年出版的，加拿大多伦多大学Simon J.D. Prince写的《Computer Vision: Algorithms and Applications》。两本书侧重点不同，前者侧重视觉和几何知识，后者侧重机器学习模型。当然两本书也有互相交叉的部分。虽然都有中文版，但是如果有一定的英语阅读基础，推荐看英文原版（见文末获取方式）。老外写的书，图和示例还是挺丰富的，比较利于 理解。《Computer Vision: Algorithms and Application》这本书图文并茂地介绍了计算机视觉这门学科的诸多大方向，有了前面《数字图像处理》的基础，这本书里有些内容你已经熟悉了，没有那么强的畏惧感。相对前面的图像处理基础本书增加了许多新的内容，比如特征检测匹配、运动恢复结构、稠密运动估计、图像拼接、计算摄影、立体匹配、三维重建等，这些都是目前比较火非常实用的方向。如果有时间可以全书浏览，如果时间不够，你可以根据兴趣，选择性的看一些感兴趣的方向。这本书的中文版翻译的不太好，可以结合英文原版看。《Computer Vision: Models, Learning, and Inference》该书从基础的概率模型讲起，涵盖了计算机视觉领域常用的概率模型、回归分类模型、图模型、优化方法等，以及偏底层的图像处理、多视角几何知识，图文并茂，并辅以非常多的例子和应用，非常适合入门。在其主页：http://www.computervisionmodels.com/上可以免费下载电子书。此外还有非常丰富的学习资源，包括给教师用的PPT、每章节对应的开源项目、代码、数据集链接等，非常有用。深入实践当你对计算机视觉领域有了比较宏观的了解，下一步就是选一个感兴趣的具体的领域去深耕。这个时期就是具体编程实践环节啦，实践过程中有疑问，根据相关术语去书里查找，结合Google，基本能够解决你大部分问题。那么具体选择什么方向呢？如果你实验室或者公司有实际的项目，最好选择当前项目方向深耕下去。如果没有具体方向，那么继续往下看。我个人认为计算机视觉可以分为两大方向：基于学习的方法和基于几何的方法。其中基于学习的方法最火的就是深度学习，而基于几何方法最火的就是视觉SLAM。下面就这两个方向给出一个相对轻松的入门姿势。1、深度学习深度学习（Deep Learning）的概念是Hinton等人于2006年提出的，最早最成功的应用领域就是计算机视觉，经典的卷积神经网络就是为专门处理图片数据而生。目前深度学习已经广泛应用在计算机视觉、语音识别、自然语言处理、智能推荐等领域。学习深度学习需要一定的数学基础，包括微积分、线性代数，很多小白一听到这些课程就想起了大学时的噩梦，其实只用了非常基础的概念，完全不用担心。不过如果一上来就啃书本，可能会有强烈的畏难情绪，很容易早早的放弃。Andrew Ng (吴恩达)的深度学习视频课程我觉得是一个非常好的入门资料。首先他本人就是斯坦福大学的教授，所以很了解学生，可以很清晰形象、深入浅出的从最基本的导数开始讲起，真的非常难得。该课程可以在网易云课程上免费观看，有中文字幕，但没有配套习题。也可以在吴恩达自己创办的在线教育平台Coursera上学习，有配套习题，限时免费，结业通过后有相应证书。该课程非常火爆，不用担心听不懂，网上有数不清的学习笔记可以参考。简直小白入门必备佳肴。2、视觉SLAMSLAM（Simultaneous Localization and Mapping）（详见《SLAM初识》），中文译作同时定位与地图创建。视觉SLAM就是用摄像头作为主传感器，用拍摄的视频流作为输入来实现SLAM。视觉SLAM广泛应用于VR/AR、自动驾驶、智能机器人、无人机等前沿领域。视觉SLAM最好的入门资料是高翔（清华博士，慕尼黑理工博后）的《视觉SLAM十四讲-从理论到实践》。该书每章节都涵盖了基础理论和代码示例，深入浅出，非常注重理论与实践结合，大大降低了小白的学习门槛。好了，入门介绍到此为止，你可以开始你的计算机视觉学习之旅了！温馨提示：本文提到的部分书籍资料，在公号「计算机视觉life」菜单栏下方回复“入门”即可获取。插入的视频不知道为什么无法观看，想看计算机视觉的应用视频，看这里：零基础小白，如何入门计算机视觉？"}
{"content2":"本周阅读了老师推荐阅读的公众号：架构师中的推文《微博深度学习平台架构和实践》，感想如下：首先在这里介绍一下深度学习相关的概念，人工智能（Artificial Intelligence），英文缩写为AI。它是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。人工智能从诞生以来，理论和技术日益成熟，应用领域也不断扩大，可以设想，未来人工智能带来的科技产品，将会是人类智慧的“容器”。人工智能可以对人的意识、思维的信息过程的模拟。人工智能不是人的智能，但能像人那样思考、也可能超过人的智能。人工智能是一门极富挑战性的科学，从事这项工作的人必须懂得计算机知识，心理学和哲学。人工智能是包括十分广泛的科学，它由不同的领域组成，如机器学习，计算机视觉等等，总的说来，人工智能研究的一个主要目标是使机器能够胜任一些通常需要人类智能才能完成的复杂工作。但不同的时代、不同的人对这种“复杂工作”的理解是不同的。 2017年12月，人工智能入选“2017年度中国媒体十大流行语”。深度学习（Deep Learning, DL）或阶层学习（hierarchical learning）是机器学习的技术和研究领域之一，通过建立具有阶层结构的人工神经网络（Artifitial Neural Networks, ANNs），在计算系统中实现人工智能 。由于阶层ANN能够对输入信息进行逐层提取和筛选，因此深度学习具有表征学习（representation learning）能力  ，可以实现端到端的监督学习和非监督学习 。此外，深度学习也可参与构建强化学习（reinforcement learning）系统，形成深度强化学习  。深度学习所使用的阶层ANN具有多种形态，其阶层的复杂度被通称为“深度”  。按构筑类型，深度学习的形式包括多层感知器、卷积神经网络、循环神经网络、深度置信网络和其它混合构筑 。深度学习使用数据对其构筑中的参数进行更新以达成训练目标，该过程被通称为“学习”  。学习的常见方法为梯度下降算法及其变体  ，一些统计学习理论被用于学习过程的优化 。在应用方面，深度学习被用于对复杂结构和大样本的高维数据进行学习，按研究领域包括计算机视觉、自然语言处理、生物信息学、自动控制等，且在人像识别、机器翻译、自动驾驶等现实问题中取得了成功而我们这学期大数据技术与应用的课堂上，初步接触了利用tensflow做线性预测。文章中所提到的微博深度学习平台的主要功能是反垃圾，反黄色暴力等毒害国家青少年思想的信息，有些网黄博主经常在微博发送一些黄色图片时，基本在几分钟之内，微薄的深度学习审核机制便可屏蔽掉该信息，并且警告博主。微博深度学习平台是微博机器学习平台的重要组成部分，除继承微博机器学习平台的特性和功能以外，支持TensorFlow、Caffe等多种主流深度学习框架，支持GPU等高性能计算集群。文章地址：https://mp.weixin.qq.com/s?__biz=MzAwNDI4ODcxNA==&mid=2652245662&idx=1&sn=8c1794671150c32adedce2317ceee3f8&chksm=80cc973bb7bb1e2d289a47840a9348f99853193a2871bee3c57b6216708754582febdf562572&scene=21#wechat_redirect"}
{"content2":"在前面两篇文章介绍了深度学习的一些基本概念，本文则使用Python实现一个简单的深度神经网络，并使用MNIST数据库进行测试。神经网络的实现，包括以下内容：神经网络权值的初始化正向传播误差评估反向传播更新权值主要是根据反向传播的4个基本方程，利用Python实现神经网络的反向传播。初始化首先定义代表神经网络的类NeuralNetwork,class NeuralNetwork: def __init__(self,layers,alpha=0.1): self.W = [] self.layers = layers self.alpha = alpha有三个属性，W存储各个层之间的权值矩阵，也是神经网络要更新学习的layers 神经网络的结构，例如： [2,2,1]表示输入层有2个神经元，隐藏层2个神经元，输出层只有1个神经元。alpha 学习速率接下来初始化各个层之间的权值矩阵for i in np.arange(0,len(layers) - 2): w = np.random.randn(layers[i] + 1,layers[i + 1] + 1) self.W.append(w / np.sqrt(layers[i]))注意上面生成权值矩阵的大小layers[i] + 1,layers[i + 1] + 1，都加了1。 这是将神经元的偏置和权值统一的放到了权值矩阵里面。\\[ \\left[ \\begin{array}{c}w_{11} & w_{12} \\\\ w_{21} & w_{22}\\end{array} \\right] \\cdot \\left[\\begin{array}{c}x_1 \\\\ x_2\\end{array}\\right] + \\left[\\begin{array}{c}b_1 \\\\ b_2\\end{array}\\right] = \\left[\\begin{array}{c}w_{11}x_1 + w{12}x_2 + b_1 \\\\ w_{21}x_1 + w_{22}x_2 + b_2 \\end{array}\\right] \\]可以将上式写成齐次的形式\\[ \\left[ \\begin{array}{c}w_{11} & w_{12} & b_1 \\\\ w_{21} & w_{22} &b_2 \\end{array} \\right] \\cdot \\left[\\begin{array}{c}x_1 \\\\ x_2 \\\\ 1\\end{array}\\right] \\]使用统一的矩阵运算，在正向反向传播的时候更方便。在输出层的神经元并没有偏置，所以要单独初始化输出层的权值矩阵w = np.random.randn(layers[-2] + 1,layers[-1]) self.W.append(w / np.sqrt(layers[-2]))下面实现Python的magic function __repr__输出神经网络结构def __repr__(self): return \"NeuralNetWork:{}\".format(\"-\".join(str(l) for l in self.layers))激活函数在神经网络中使用sigmoid作为激活函数，实现sigmoid及其导数def sigmoid(self,x): return 1.0 / (1 + np.exp(-x)) def sigmoid_deriv(self,x): return x * (1 - x)正向反向传播这一部分是神经的网络的核心了。下面实现fit方法，在方法中完成神经网络权值更新（训练）的过程。def fit(self,X,y,epochs=1000,displayUpdate=100): X = np.c_[X,np.ones((X.shape[0]))] for epoch in np.arange(0,epochs): for(x,target) in zip(X,y): self.fit_partial(x,target) # check to see if we should display a training update if epoch == 0 or (epoch + 1) % displayUpdate == 0: loss = self.calculate_loss(X,y) print(\"[INFO] epoch={},loss={:.7f}\".format(epoch + 1,loss))该函数有4个参数：X是输入的样本数据y是样本的真是值epochs训练的轮数displayUpdate 输出训练的loss值。X = np.c_[X,np.ones((X.shape[0]))]将输入训练的样本表示为齐次向量（也就是在末尾添1）。fit_partial是对输入的每个样本进行训练，包括正向传播，反向传播以及权值的更新。def fit_partial(self,x,y): A = [np.atleast_2d(x)] # 正向传播 # 层层之间的数据传递 for layer in np.arange(0,len(self.W)): # 输入经过加权以及偏置后的值 net = A[layer].dot(self.W[layer]) # 神经元的输出 out = self.sigmoid(net) # 保存下来，反向传播的时候使用 A.append(out)上面完成了神经玩过的正向传播过程，下面根据反向传播的4个基本方程进行反向传播。首先根据\\(BP1\\),\\[ \\delta^L = \\frac{\\partial e}{\\partial a^L} \\odot \\sigma'(z^L) \\tag{BP1} \\]计算输出层的误差\\(\\delta^L\\)error = A[-1] - y # 输出层的误差，均值方差作为损失函数 D = [error * self.sigmoid_deriv(A[-1])]得到输出层的误差D后，根据\\(BP2\\)计算各个层的误差\\[ \\delta^{L-1} = (W^L)^T\\delta^L \\odot \\sigma'(z^{L-1}) \\tag{BP2} \\]for layer in np.arange(len(A) - 2,0 ,-1): delta = D[-1].dot(self.W[layer].T) delta = delta * self.sigmoid_deriv(A[layer]) D.append(delta) D = D[::-1]将D反转，和各个层的索引对应起来，下面根据\\(BP3,BP4\\)计算权值矩阵和偏置的导数\\[ \\frac{\\partial e}{b_j^l} = \\delta_j^l \\tag{BP3} \\]\\[ \\frac{\\partial e}{w_{jk}^l} = \\delta_j^l a_k^{l-1} \\tag{BP4} \\]for layer in np.arange(0,len(self.W)): self.W[layer] += -self.alpha * A[layer].T.dot(D[layer])首先求得权值和偏置的导数（权值和偏置统一到同一个矩阵中）A[layer].T.dot(D[layer]，然后将梯度乘以学习速率alpha 每次权值减小的步长。上述就完成利用反向传播算法更新权值的过程。 关于反向传播四个基本方程的推导过程，可以参考文章深度学习与计算机视觉： 搞懂反向传播算法的四个基本方程误差评估上面代码已经实现了深度学习的训练过程，下面实现predict输出使用训练好的模型预测的结果，calculate_loss评估训练后模型的评估def predict(self,X,addBias=True): p = np.atleast_2d(X) if addBias: p = np.c_[p,np.ones((p.shape[0]))] for layer in np.arange(0,len(self.W)): p = self.sigmoid(np.dot(p,self.W[layer])) return p def calculate_loss(self,X,targets): targets = np.atleast_2d(targets) predictions = self.predict(X,addBias=False) loss = 0.5 * np.sum((predictions - targets) ** 2) return lossMNIST分类识别使用上面实现的深度神经网络对MNIST手写体进行识别，首先导入必要的包import NeuralNetwork from sklearn.preprocessing import LabelBinarizer from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report from sklearn import datasets需要使用sklearn包中的一些工具，进行数据的处理。# load MNIST数据集，并使用min/max对数据进行归一化 digits = datasets.load_digits() data = digits.data.astype(\"float\") data = (data - data.min()) / (data.max() - data.min()) print(\"[INFO] samples: {}, dim: {}\".format(data.shape[0], data.shape[1]))将数据拆分为训练集和测试集，并对MNIST的类别进行编码(trainX, testX, trainY, testY) = train_test_split(data, digits.target, test_size=0.25) # convert the labels from integers to vectors trainY = LabelBinarizer().fit_transform(trainY) testY = LabelBinarizer().fit_transform(testY)下面构建神经网络结构，并使用训练集进行训练nn = NeuralNetwork([data.shape[1], 32,16, 10]) print (\"[INFO] {}\".format(nn)) nn.fit(trainX, trainY, epochs=1000)神经网络结构为：64-32-16-10，其中64为输入数据的大小，10输出类别的个数。最后评估训练得到的模型predictions = nn.predict(testX) print(classification_report(testY.argmax(axis=1), predictions.argmax(axis=1)))最终的输出结果：[INFO] loading MNIST (sample) dataset... [INFO] samples: 1797, dim: 64 [INFO] training network... [INFO] NeuralNetWork:64-32-16-10 [INFO] epoch=1,loss=607.1711647 [INFO] epoch=100,loss=7.1082795 [INFO] epoch=200,loss=4.0731690 [INFO] epoch=300,loss=3.1401868 [INFO] epoch=400,loss=2.8801101 [INFO] epoch=500,loss=1.8738122 [INFO] epoch=600,loss=1.7461474 [INFO] epoch=700,loss=1.6624043 [INFO] epoch=800,loss=1.1852884 [INFO] epoch=900,loss=0.6710255 [INFO] epoch=1000,loss=0.6336826 [INFO] evaluating network... precision recall f1-score support 0 1.00 0.95 0.97 39 1 0.84 1.00 0.92 38 2 1.00 0.98 0.99 41 3 0.93 0.98 0.95 52 4 0.91 0.97 0.94 40 5 0.98 0.98 0.98 41 6 1.00 0.96 0.98 51 7 1.00 0.98 0.99 48 8 0.98 0.89 0.93 55 9 0.98 0.93 0.95 45 micro avg 0.96 0.96 0.96 450 macro avg 0.96 0.96 0.96 450 weighted avg 0.96 0.96 0.96 450如上测试结果，在测试集的上表现还算不错。总结本文使用Python简单的实现了一个神经网络。 主要是利用反向传播的4个基本方程，实现反向传播算法，更新各个神经元的权值。 最后使用该网络，对MNIST数据进行识别分类。上面实现的神经网络只是“玩具”，用以加深对深度学习的训练过程以及反向传播算法的理解。后面将使用Keras和PyTorch来构建神经网络。本文代码在git库 https://github.com/brookicv/machineLearningSample"}
{"content2":"计算机视觉领域的一些牛人博客2013-08-17 17:28:20|  分类： Halcon快速入门|字号 订阅本文转载自sanny《计算机视觉领域的一些牛人博客(转载)》以下链接是本人整理的关于计算机视觉（ComputerVision, CV）相关领域的网站链接，其中有CV牛人的主页，CV研究小组的主页，CV领域的paper,代码，CV领域的最新动态，国内的应用情况等等。打算从事这个行业或者刚入门的朋友可以多关注这些网站，多了解一些CV的具体应用。搞研究的朋友也可以从中了解到很多牛人的研究动态、招生情况等。总之，我认为，知识只有分享才能产生更大的价值，真诚希望下面的链接能对朋友们有所帮助。（1）googleResearch； http://research.google.com/index.html（2）MIT博士，汤晓欧学生林达华； http://people.csail.mit.edu/dhlin/index.html（3）MIT博士后Douglas Lanman； http://web.media.mit.edu/~dlanman/（4）opencv中文网站； http://www.opencv.org.cn/index.php/%E9%A6%96%E9%A1%B5（5）Stanford大学vision实验室； http://vision.stanford.edu/research.html（6）Stanford大学博士崔靖宇； http://www.stanford.edu/~jycui/（7）UCLA教授朱松纯； http://www.stat.ucla.edu/~sczhu/（8）中国人工智能网； http://www.chinaai.org/（9）中国视觉网； http://www.china-vision.net/（10）中科院自动化所； http://www.ia.cas.cn/（11）中科院自动化所李子青研究员； http://www.cbsr.ia.ac.cn/users/szli/（12）中科院计算所山世光研究员； http://www.jdl.ac.cn/user/sgshan/（13）人脸识别主页； http://www.face-rec.org/（14）加州大学伯克利分校CV小组；http://www.eecs.berkeley.edu/Research/Projects/CS/vision/（15）南加州大学CV实验室； http://iris.usc.edu/USC-Computer-Vision.html（16）卡内基梅隆大学CV主页；http://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.html（17）微软CV研究员Richard Szeliski；http://research.microsoft.com/en-us/um/people/szeliski/（18）微软亚洲研究院计算机视觉研究组； http://research.microsoft.com/en-us/groups/vc/（19）微软剑桥研究院ML与CV研究组； http://research.microsoft.com/en-us/groups/mlp/default.aspx（20）研学论坛； http://bbs.matwav.com/（21）美国Rutgers大学助理教授刘青山； http://www.research.rutgers.edu/~qsliu/（22）计算机视觉最新资讯网； http://www.cvchina.info/（23）运动检测、阴影、跟踪的测试视频下载；http://apps.hi.baidu.com/share/detail/18903287（24）香港中文大学助理教授王晓刚； http://www.ee.cuhk.edu.hk/~xgwang/(25)香港中文大学多媒体实验室（汤晓鸥）; http://mmlab.ie.cuhk.edu.hk/(26)U.C. San Diego. computer vision;http://vision.ucsd.edu/content/home(27)CVonline; http://homepages.inf.ed.ac.uk/rbf/CVonline/(28)computer vision software; http://peipa.essex.ac.uk/info/software.html(29)Computer Vision Resource; http://www.cvpapers.com/(30)computer vision research groups;http://peipa.essex.ac.uk/info/groups.html(31)computer vision center; http://computervisioncentral.com/cvcnews(32)浙江大学图像技术研究与应用（ITRA）团队：http://www.dvzju.com/(33)自动识别网：http://www.autoid-china.com.cn/(34)清华大学章毓晋教授：http://www.tsinghua.edu.cn/publish/ee/4157/2010/20101217173552339241557/20101217173552339241557_.html(35)顶级民用机器人研究小组Porf.Gary领导的Willow Garage:http://www.willowgarage.com/(36)上海交通大学图像处理与模式识别研究所：http://www.pami.sjtu.edu.cn/(37)上海交通大学计算机视觉实验室刘允才教授：http://www.visionlab.sjtu.edu.cn/(38)德克萨斯州大学奥斯汀分校助理教授Kristen Grauman ：http://www.cs.utexas.edu/~grauman/(39)清华大学电子工程系智能图文信息处理实验室（丁晓青教授）：http://ocrserv.ee.tsinghua.edu.cn/auto/index.asp(40)北京大学高文教授：http://www.jdl.ac.cn/htm-gaowen/(41)清华大学艾海舟教授：http://media.cs.tsinghua.edu.cn/cn/aihz(42)中科院生物识别与安全技术研究中心：http://www.cbsr.ia.ac.cn/china/index%20CH.asp(43)瑞士巴塞尔大学 Thomas Vetter教授：http://informatik.unibas.ch/personen/vetter_t.html(44)俄勒冈州立大学 Rob Hess博士：http://blogs.oregonstate.edu/hess/(45)深圳大学 于仕祺副教授：http://yushiqi.cn/(46)西安交通大学人工智能与机器人研究所：http://www.aiar.xjtu.edu.cn/(47)卡内基梅隆大学研究员Robert T. Collins:http://www.cs.cmu.edu/~rcollins/home.html#Background(48)MIT博士Chris Stauffer:http://people.csail.mit.edu/stauffer/Home/index.php(49)美国密歇根州立大学生物识别研究组(Anil K. Jain教授)：http://www.cse.msu.edu/rgroups/biometrics/(50)美国伊利诺伊州立大学Thomas S. Huang:http://www.beckman.illinois.edu/directory/t-huang1(51)武汉大学数字摄影测量与计算机视觉研究中心：http://www.whudpcv.cn/index.asp(52)瑞士巴塞尔大学Sami Romdhani助理研究员：http://informatik.unibas.ch/personen/romdhani_sami/(53)CMU大学研究员Yang Wang:http://www.cs.cmu.edu/~wangy/home.html(54)英国曼彻斯特大学Tim Cootes教授：http://personalpages.manchester.ac.uk/staff/timothy.f.cootes/(55)美国罗彻斯特大学教授Jiebo Luo:http://www.cs.rochester.edu/u/jluo/(56)美国普渡大学机器人视觉实验室：https://engineering.purdue.edu/RVL/Welcome.html(57)美国宾利州立大学感知、运动与认识实验室：http://vision.cse.psu.edu/home/home.shtml(58)美国宾夕法尼亚大学GRASP实验室：https://www.grasp.upenn.edu/(59)美国内达华大学里诺校区CV实验室：http://www.cse.unr.edu/CVL/index.php(60)美国密西根大学vision实验室：http://www.eecs.umich.edu/vision/index.html(61)University of Massachusetts(麻省大学),视觉实验室：http://vis-www.cs.umass.edu/index.html(62)华盛顿大学博士后Iva Kemelmacher:http://www.cs.washington.edu/homes/kemelmi(63)以色列魏茨曼科技大学Ronen Basri:http://www.wisdom.weizmann.ac.il/~ronen/index.html(64)瑞士ETH-Zurich大学CV实验室：http://www.vision.ee.ethz.ch/boostingTrackers/index.htm(65)微软CV研究员张正友：http://research.microsoft.com/en-us/um/people/zhang/(66)中科院自动化所医学影像研究室：http://www.3dmed.net/(67)中科院田捷研究员：http://www.3dmed.net/tian/(68)微软Redmond研究院研究员Simon Baker:http://research.microsoft.com/en-us/people/sbaker/(69)普林斯顿大学教授李凯：http://www.cs.princeton.edu/~li/(70)普林斯顿大学博士贾登：http://www.cs.princeton.edu/~jiadeng/(71)牛津大学教授Andrew Zisserman： http://www.robots.ox.ac.uk/~az/(72)英国leeds大学研究员Mark Everingham:http://www.comp.leeds.ac.uk/me/(73)英国爱丁堡大学教授Chris William: http://homepages.inf.ed.ac.uk/ckiw/(74)微软剑桥研究院研究员John Winn: http://johnwinn.org/(75)佐治亚理工学院教授Monson H.Hayes：http://savannah.gatech.edu/people/mhayes/index.html(76)微软亚洲研究院研究员孙剑：http://research.microsoft.com/en-us/people/jiansun/(77)微软亚洲研究院研究员马毅：http://research.microsoft.com/en-us/people/mayi/(78)英国哥伦比亚大学教授David Lowe: http://www.cs.ubc.ca/~lowe/(79)英国爱丁堡大学教授Bob Fisher: http://homepages.inf.ed.ac.uk/rbf/(80)加州大学圣地亚哥分校教授Serge J.Belongie:http://cseweb.ucsd.edu/~sjb/(81)威斯康星大学教授Charles R.Dyer: http://pages.cs.wisc.edu/~dyer/(82)多伦多大学教授Allan.Jepson: http://www.cs.toronto.edu/~jepson/(83)伦斯勒理工学院教授Qiang Ji: http://www.ecse.rpi.edu/~qji/(84)CMU研究员Daniel Huber: http://www.ri.cmu.edu/person.html?person_id=123(85)多伦多大学教授：David J.Fleet: http://www.cs.toronto.edu/~fleet/(86)伦敦大学玛丽女王学院教授Andrea Cavallaro:http://www.eecs.qmul.ac.uk/~andrea/(87)多伦多大学教授Kyros Kutulakos: http://www.cs.toronto.edu/~kyros/(88)杜克大学教授Carlo Tomasi: http://www.cs.duke.edu/~tomasi/(89)CMU教授Martial Hebert: http://www.cs.cmu.edu/~hebert/(90)MIT助理教授Antonio Torralba: http://web.mit.edu/torralba/www/(91)马里兰大学研究员Yasel Yacoob: http://www.umiacs.umd.edu/users/yaser/(92)康奈尔大学教授Ramin Zabih: http://www.cs.cornell.edu/~rdz/(93)CMU博士田渊栋: http://www.cs.cmu.edu/~yuandong/(94)CMU副教授Srinivasa Narasimhan: http://www.cs.cmu.edu/~srinivas/(95)CMU大学ILIM实验室：http://www.cs.cmu.edu/~ILIM/(96)哥伦比亚大学教授Sheer K.Nayar: http://www.cs.columbia.edu/~nayar/(97)三菱电子研究院研究员Fatih Porikli ：http://www.porikli.com/(98)康奈尔大学教授Daniel Huttenlocher：http://www.cs.cornell.edu/~dph/(99)南京大学教授周志华：http://cs.nju.edu.cn/zhouzh/index.htm(100)芝加哥丰田技术研究所助理教授Devi Parikh: http://ttic.uchicago.edu/~dparikh/index.html(101)瑞士联邦理工学院博士后Helmut Grabner:http://www.vision.ee.ethz.ch/~hegrabne/#Short_CV(102)香港中文大学教授贾佳亚：http://www.cse.cuhk.edu.hk/~leojia/index.html(103)南洋理工大学副教授吴建鑫：http://c2inet.sce.ntu.edu.sg/Jianxin/index.html(104)GE研究院研究员李关：http://www.cs.unc.edu/~lguan/(105)佐治亚理工学院教授Monson Hayes:http://savannah.gatech.edu/people/mhayes/(106)图片检索国际会议VOC(微软剑桥研究院组织):http://pascallin.ecs.soton.ac.uk/challenges/VOC/(107)机器视觉开源处理库汇总：http://archive.cnblogs.com/a/2217609/(108)布朗大学教授Benjamin Kimia: http://www.lems.brown.edu/kimia.html(109)数据堂-图像处理相关的样本数据：http://www.datatang.com/data/list/602026/p1(110)东软基于CV的汽车辅助驾驶系统：http://www.neusoft.com/cn/solutions/1047/(111)马里兰大学教授Rema Chellappa:http://www.cfar.umd.edu/~rama/(112)芝加哥丰田研究中心助理教授Devi Parikh：http://ttic.uchicago.edu/~dparikh/index.html(113)宾夕法尼亚大学助理教授石建波：http://www.cis.upenn.edu/~jshi/(114)比利时鲁汶大学教授Luc Van Gool：http://www.vision.ee.ethz.ch/members/get_member.cgi?id=1, http://www.vision.ee.ethz.ch/~vangool/(115)行人检测主页：http://www.pedestrian-detection.com/(116)法国学习算法与系统实验室Basilio Noris博士：http://lasa.epfl.ch/people/member.php?SCIPER=129576 http://mldemos.epfl.ch/(117)美国马里兰大学LARRY S.DAVIS教授：http://www.umiacs.umd.edu/~lsd/(118)计算机视觉论文分类导航：http://www.visionbib.com/bibliography/contents.html(119)计算机视觉分类信息导航：http://www.visionbib.com/(120)西班牙马德里理工大学博士Marcos Nieto：http://marcosnieto.net/(121)香港理工大学副教授张磊：http://www4.comp.polyu.edu.hk/~cslzhang/(122)以色列技术学院教授Michael Elad：http://www.cs.technion.ac.il/~elad/(123)韩国启明大学计算机视觉与模式识别实验室：http://cvpr.kmu.ac.kr/(124)英国诺丁汉大学Michel Valstar博士：http://www.cs.nott.ac.uk/~mfv/(125)卡内基梅隆大学Takeo Kanade教授:http://www.ri.cmu.edu/people/kanade_takeo.html(126)微软学术搜索：http://libra.msra.cn/(127)比利时天主教鲁汶大学Radu Timofte博士：http://homes.esat.kuleuven.be/~rtimofte/，交通标志检测，定位，3D跟踪(128)迪斯尼匹兹堡研究院研究员：Iain Matthews:http://www.iainm.com/iainm/Home.htmlhttp://www.ri.cmu.edu/person.html?type=publications&person_id=741 AAM,三维重建（129）康奈尔大学视觉与图像分析组：http://www.via.cornell.edu/ 医学图像处理（130）密西根州立大学生物识别研究组：http://www.cse.msu.edu/biometrics/ 人脸识别、指纹识别、图像检索（131）柏林科技大学计算机视觉与遥感实验室：http://www.cv.tu-berlin.de/menue/computer_vision_remote_sensing/parameter/en/ 图像分析、物体重建、基于图像的表面测量、医学图像处理（132）英国布里斯托大学数字多媒体研究组：http://www.cs.bris.ac.uk/Research/Digitalmedia/ 运动检测与跟踪、视频压缩、3D重建、字符定位（133）英国萨利大学视觉、语音与信号处理中心： http://www.surrey.ac.uk/cvssp/   人脸识别、监控、3D、视频检索、（134）北卡莱罗纳大学教堂山分校Marc Pollefeys教授：http://www.cs.unc.edu/~marc/ 基于视频的3D模型生成、相机标定、运动检测与分析、3D重建（135）澳大利亚国立大学Richard Hartley教授：http://users.cecs.anu.edu.au/~hartley/ 运动估计、稀疏子空间、跟踪、（136）百度技术副总监于凯：http://www.dbs.ifi.lmu.de/~yu_k/ 深度学习，稀疏表示，图像分类（137）西安电子科技大学高新波教授：http://web.xidian.edu.cn/xbgao/index.html 质量评判、水印、稀疏表示、超分辨率（138）加州大学伯克利分校Michael I.Jordan教授：http://www.cs.berkeley.edu/~jordan/ 机器学习（139）加州理工行人检测相关资料：http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/（140）微软Redmond研究院研究员Piotr Dollar: http://vision.ucsd.edu/~pdollar/ 行人检测、特征提取、（141）视觉计算研究论坛：http://www.sigvc.org/bbs/ 中科院视觉计算研究小组的论坛（142）美国坦桑尼亚州立大学稀疏学习软件包：http://www.public.asu.edu/~jye02/Software/SLEP/index.htm 稀疏学习（143）美国加州大学圣地亚哥分校Jacob Whitehill博士：http://mplab.ucsd.edu/~jake/ 机器学习（144）美国布朗大学Michael J.Black教授：http://cs.brown.edu/~black/  人的姿态估计和跟踪（145）美国加州大学圣地亚哥分校David Kriegman教授：http://cseweb.ucsd.edu/~kriegman/ 人脸识别（146）南加州大学Paul Debevec教授：http://ict.debevec.org/~debevec/ 或 http://www.pauldebevec.com/ 将CV和CG结合研究 人脸捕捉重建技术（147）伊利诺伊大学D.A.Forsyth教授：http://luthuli.cs.uiuc.edu/~daf/ 三维重建（148）英国牛津大学Ian Reid教授：http://www.robots.ox.ac.uk/~ian/ 跟踪和机器人导航（149）CMU大学Alyosha Efros 教授: https://www.cs.cmu.edu/~efros/ 图像纹理合成（150）加州大学伯克利分校Jitendra Malik教授：http://www.cs.berkeley.edu/~malik/ 轮廓检测、图像/视频分割、图形匹配、目标识别（151）MIT教授William Freeman： http://people.csail.mit.edu/billf/ 应用于CV的ML、可视化感知的贝叶斯模型、计算摄影学；最有影响力的研究成果：图像纹理合成（152）CMU博士Henry Schneiderman： http://www.cs.cmu.edu/~hws/ 目标检测和识别；最有影响力的研究成果：目标检测；2000年CVPR上发表了”A statistical method for 3D object detection applied to faces and cars”。该算法采用多视角训练样本，可用于检测不同视角下的物体，如人脸和车，是第一个能够检测侧脸的算法。他创建了PittPatt公司，后被Google收购（153）微软研究员Paul Viola: http://research.microsoft.com/en-us/um/people/viola/ AdaBoost算法（154）微软研究员Antonio Criminisi: http://research.microsoft.com/en-us/people/antcrim/ 图像修补，三维重建，目标检测与跟踪；（155）魏茨曼科学研究所教授Michal Irani: http://www.wisdom.weizmann.ac.il/~irani/ 超分辨率（156）瑞士洛桑理工学院Pascal Fua教授：http://people.epfl.ch/pascal.fua/bio?lang=en 立体视觉，增强现实（157）佐治亚理工学院Irfan Essa教授：http://www.ic.gatech.edu/people/irfan-essa 人脸表情识别"}
{"content2":"本文阅读时间约16分钟最近两年，人工智能（AI）就像一个点石成金的神器，所有的行业，创业公司，或是求职，只要沾着这个词，多少有点脚踩五彩祥云的感觉，故事来了，融资来了，高薪来了。于是，越来越多的人开始往AI方向涌。我曾经参加一个群面，12个人，其中一半以上做人工智能方向的，可见现在AI有多热。可是，问题就在于，这条路就一定靠谱吗？这个方向前景如何呢？我们究竟怎样才能找到一份算法工程师的工作呢？相信不少同学都有这些疑问。下面权且从个人经验结合一些资料，进行深度剖析。由于个人经验可能存在局限性，不一定正确供大家参考。本文会写到以下小话题：1、什么是人工智能2、为什么现在人工智能这么火3、为什么要学习人工智能4、找工作的经验4.1、工作要求分析4.2、如何找一个自己满意的工作前三部分，作为一个铺垫，如果你已经跨过小白阶段，可以直接跳到最后一部分，观看精华部分。1、什么是人工智能其实对于什么是人工智能，目前还没有一个统一的定义。对于我们目前常说的人工智能其实，可以用一张图来解释。那么用一句话总结就是：人工智能是为了让机器拥有类人的智能，其中机器学习是一种实现人工智能的方法，而深度学习是一种机器学习的技术。2、为什么现在人工智能这么火那为什么人工智能火起来了，其实之前人工智能曾火了两次，但都因为各种问题而没有发展起来。而这次又重新火起来很大程度是因为近年来互联网高速发展所产生的数据，这些数据极其庞大，需要处理和加工提炼，数据过于庞大人工处理非常困难，这时，数据处理的需求一定程度上催生了人工智能的迅速发展。同时，计算机的硬件的发展也提供了基础。而2016年的一场人机大战举行，随着 Google 的 AlphaGo 打败韩国围棋棋手李世乭之后，机器学习尤其是深度学习的热潮席卷了整个IT界。所有的互联网公司，尤其是 Google、 微软、百度、阿里、腾讯等巨头，无不在布局人工智能技术和市场。前有百度深度学习研究院，腾讯的AILab，后有阿里的“达摩院”。并且国家不断的鼓励与布局下更加促进了人工智能的发展。人工智能已经慢慢的进入平常百姓的生活，可以说人工智能的发展前景非常广，通过一个图看一下人工智能在生活中有哪些应用呢。应用场景广泛，其实也是人工智能火起来的原因之一。那么以后人工智能还会一直这么火么？其实，从面试了解的大部分公司，其实最关心的是如何落地算法，一般都会问你这个项目有没有用到实际。其实侧面反映出人工智能方向难得是落地，现在很多公司纷纷踏入，其实大部分公司没什么技术积累，随着时间的发展，市场会趋于冷静。就像之前p2p、共享单车、比特币一样，资本快速催生出的市场，冷静下来会出现一地鸡毛啊。但不妨碍人工智能有广阔的前景，毕竟解放人类生产力啊。同时，最近的cai yuan潮一样给大家提个醒，尽量进入大厂或者核心部门，风险相对较小。3、为什么要学习人工智能我认为在做任何重要决定之前，比起搞清楚该怎么做（HOW），追问自己为什么要做出这个选择要重要得多（WHY）。方向大于努力。你在一条错误的路上越努力，你离目标越远。==钱更多？更喜欢？更擅长？==任何一条理由都没问题，但为了增加你求职的成功率和工作的发展前景，应该尽量去寻找这三条职场优势公共的交集。一开始，很难有人一击命中既钱多又喜欢还擅长的工作，但这并不妨碍你先从已经同时具备了两个职场优势的选项（钱多、擅长、喜欢）出发，并把三大优势的交集作为自己职业规划的终极目标。除此之外，每个人或许还有些个人限制条件，比如经验和教育水平不同。如何利用以上理论来分析自己适不适学习人工智能？我就拿自己来举例分析分析——首先我大学是一个普通的不能在普通的一本，学的还是万金油的自动化专业，其实跟编程关系不大。以至于去招聘的时候别人都问，你会不会硬件。这时候内心简直想怼他“学自动化就得做硬件么？”，还是要面带微笑耐心的，说之前确实做PLC，但后面，做的是深度学习。目标1：考研另一方面由于学校很普通，又不想安心的考个公务员就算了（内心还是有个搞技术的心，皮一下），毅然选择考研，第一年复试被刷，只能调剂，但是调剂的话，还是不能上985或者211，并不能改变我第一学历出身不好，所以选择二战。（ps：有目标后千万不要轻易放弃）目标2：选择喜欢的方向当考上研究生后，其实，由于“散养”自己玩了一年，荒废了这一年宝贵时光。当选择方向时，实验室有两个方向，一个图像处理，一个雷达。由于个人对雷达实在提不起兴趣，又比较喜欢图像处理，觉得很有意思，想选择图像，但是这个项目不需要这么多人，导师不想让选。我就提前准备，询问了师兄师姐，还有其他老师，带着自己想法，跟导师谈，最终说服导师让做试试看。（这真的需要技巧，不能跟导师硬碰硬，说我就不想做，想做啥。还是总体要听导师的。如果实在有自己想法，就要带着想法与导师谈）你对一个东西感兴趣，真的很重要，事半功倍的感觉。更愿意付出去学习去奋斗。目标3：算法工程师其实入门学习真的不简单，因为我之前是做PLC的，编程基础真的几乎为零，就从头开始学。后来通过招聘发现自己不足，接着学习不足的地方。（其实这可以看做一个反馈，发现不足，弥补不足。）找工作的时候，我仔细思考了自己的工作反向觉得对算法更有兴趣，决定去找算法岗，虽然道路不是特别顺利，但终有一个好结果。（虽然，自己拿到过几家软开的职位，但，觉得自己还是想做自己喜欢的工作，身边很多同学并没能坚持去找算法职位，可能去了软开，测试。并不是说这些岗位不好，而是，你最好找到一个自己喜欢的，这样你才有兴趣持续学习。如果你喜欢软开，当然一样有钱途）4、找工作的经验可能有小伙伴纳闷，怎么不介绍怎么学习？==主要是每个人的基础不一样，实在很难一概而论，大家可以自行，知乎，如何入门机器学习。==有很多推荐，最主要找到一种适合自己的情况的学习方法。但有两个要注意的地方，我觉得是入门时很容易出现的情况，也是一定要避免的，大家需要注意。放弃海量资料！没错，就是放弃海量资料！在我们想要入门机器学习的时候，往往会搜集很多资料，什么 xx学院机器学习内部资源、机器学习从入门到进阶百 G 资源、xx 人工智能教程，等等。很多时候我们拿着十几 G、几百 G 的学习资源，然后踏踏实实地放到了某云盘里存着，等着日后慢慢学习。殊不知，有 90% 的人仅仅只是搜集资料、保存资料而已，放在云盘里一年半载也忘了打开学习。（看了一眼自己的云盘，莫名感觉打脸，好呗，捂着脸接着写）躺在云盘的资料很多时候只是大多数人“以后好好学习”的自我安慰和“自我”安全感而已。而且，面对海量的学习资料，很容易陷入到一种迷茫的状态，最直接的感觉就是：天啊，有这么多东西要学！天啊，还有这么多东西没学！简单来说，就是选择越多，越容易让人陷入无从选择的困境。所以，第一步就是要放弃海量资料！而是选择一份真正适合自己的资料，好好研读下去！千万别先从头学数学！千万别先从头学数学！千万别先从头学数学！（重要的事情多说几遍）说到入门，很多人会想着那就要从最基础的知识开始学起！机器学习是一门融合概率论、线性代数、凸优化、计算机、神经科学等多方面的复杂技术。学好机器学习需要的理论知识很多，有些人可能基础不是特别扎实，就想着从最底层的知识开始学起，概率论、线性代数、机器学习凸优化公式推导，等等。千万不要这样，极有可能你都看不完一本数学理论的书，就放弃了。当然，不是说不需要钻研基础知识，基础理论知识非常重要！只是说，在入门的时候，最好先从顶层框架上有个系统的认识，然后再从实践到理论，有的放矢的查缺补漏机器学习知识点。而且从学习的积极性来说，也起到了“正反馈”的作用。4.1工作要求分析因为，大部分人学习的目的，是为了求职去找一个好工作，我觉得有必要首先对这份工作的行情和要求做个大概的了解。俗话说知己知彼百战百胜嘛。本人爬了智联招聘上的关于算法工程师的将近6000个职位，进行分析。具有一定参考价值，如图所示：1.我们来探索一下算法工程师岗位在全国各城市的需求情况：如上图所示，北京、上海和深圳对算法工程师的需求最多，尤其是北京，需求量甚至超过了上海、深圳的总和。所以如果想找算法工作北京机会最多，其次是上海、深圳。所以，如果想更容易的去找工作，上面的城市要更容易。2.薪资水平的探索如上图所示，对于算法工程师的岗位而言，绝大多数岗位的薪资在1K-2.6K之前，薪资其实相对较高。是不是突然有学习的动力。3.学历要求4.学历与工资的关系基本上可以反映学历与工资成正比，如果想获得高收入，提高学历同样是个办法。5.企业的类型分布6.算法工程师所需要的技能(非常的重要哦)1.学历：本科及以上学历，数学，计算机，自动化等相关专业（有些大厂提高到硕士及以上，所有学历有时候是个门槛）；2.编程能力：C/C++、Python、java任意两种；常用数据结构和算法；熟悉linux系统3.算法能力：熟练掌握计算机视觉&机器学习的基本方法（这里所说的是可以手推公式，例如：决策树、临近取样、支持向量机、神经网络算法、回归、聚类等。同时要熟练掌握TensorFlow、Keras、PyTorch等框架）4.经验：有项目经历；比赛经历；实习经历5.团队合作能力：良好的沟通能力和团队合作能力。6.加分项：有博客或者公众号；github；顶级会议论文；各种比赛奖项7.针对细分领域可能还有一些，其他的要求，比如图像处理方向可能会要求opencv。数据分析可能要求会数据可视化。4.2 如何找一个自己满意的工作4.2.1 简历简历是你求职的第一步，也是很重要的部分，因为他是你与应聘公司沟通的桥梁。那该如何制作一份合格的简历？（推荐：简历就得这么做才行https://www.bilibili.com/video/av12771675）简历的第一个字就突出简，简单美观就好，千万不要弄得特别花哨，反而没有什么好效果。简历最好一页，将最能代表自己能力的写清楚，最好简洁扼要。要针对投递公司的职位要求，去写自己的简历。例如：你去应聘一个公司，可以打开他们的招聘网站，查看一下岗位要求和公司文化是什么。对应着要求去写。但是千万不要写一些自己根本讲不出的东西，比如你写了熟悉什么算法，问你一个问题，就答不出，这样还不如不写。关于如何写工作经验或者项目经验，推荐给大家“ STAR ” 法则。情境（Stuation）：写出你的工作背景任务（Task）：我负责做什么行动（Action）：我做了什么结果（Result）：我的工作取得了什么样的结果举例：某某公司的算法岗实习，参与人脸识别的项目，负责算法的实现与改进，最终结果提高了多少多少。千万不要写一堆，甚至，有同学简历好多页，HR没时间去细看，不知道你的重点是什么。而且简历最好有量化的数字，比如提高了多少，写了多少代码，这样让人更直观。最后，一个小贴士，如果需要自带简历，请不要舍不得花钱，找一家好一点打印店，彩打厚纸，这样给人感觉更舒服一些。下面我们就该针对，求职要求去丰富自己的简历。4.2.2 学历一些大厂的起步要求确实是硕士说明门槛逐渐在提高，如果真的想深造，读书不失一个办法，尤其现在应届生与往届生的工资出现，应届生工资更高的情况。当然，还得结合自身实际选择。4.2.3 coding能力算法的同学coding能力是在面试中必须要考察的。虽然可能比软开要求要低，但并不是就不要求，只会python肯定是会被鄙视的。所以自己的coding能力必须多练练，推荐在牛客网或者LeetCode的在线编程多多敲代码。掌握类似leetcode medium程度的题目就可以了，hard程度的可以不用掌握。面试中一般会出几道题目，要求手写，能顺利写出的都是加分项。语言要掌握一大两小三门语言，大语言是Java或者C++,小语言掌握python，SQL。有时间精力可以将常见的算法用python实现一下。语言不必掌握很深，但是要做到能熟练用Python或者SQL处理数据，算法用Python也要掌握差不多。对于大语言来说基本语法和一些基本概念都要熟练掌握。同时对基本的计算机原理要熟悉，面试同样会问。资料请自取哦：python面试知识点总结：https://github.com/taizilongxu/interview_pythonc\\c++面试知识点总结：https://github.com/huihut/interview#ccjava面试知识点总结：https://github.com/guanzhenxing/java_interview_manual4.2.4 算法能力熟练掌握计算机视觉&机器学习的基本方法（这里所说的是可以手推公式，例如：决策树、临近取样、支持向量机、神经网络算法、回归、聚类等。同时要熟练掌握TensorFlow、Keras、PyTorch等框架，TensorFlow可能入门有点难，可以PyTorch）资料请自取哦：推荐书籍：《机器学习》机器学习算法实现：https://github.com/lawlite19/MachineLearning_Python推荐视频：李宏毅机器学习算法面试经验：https://github.com/imhuay/Algorithm_Interview_Notes-Chinese算法工程师面试常见问题：https://github.com/PPshrimpGo/AIinterview4.2.5 经验有项目经历；比赛经历；实习经历对于在校生，项目经历要熟知你自己的方向，平常可以多了解实验室其他同门的项目，关键时刻懂点，总比不懂要好。比赛可以参加：kaggale或者一些大厂的比赛，当然得取得名次，要不然写出来，也体现不了能力。实习的话每年三月份会有很多实习，大家可以关注一些公众号，获取实习信息，对于实习尽量去大厂，能为你的招聘增分不少哦。4.2.6 团队合作能力这个对于在校生，可以说自己组织什么样的活动，或者参与项目时，怎么与他人合作。4.2.7 加分项需要平时自己积累了，github、博客，能为你增分的。而且最近越发的重要了。大家可以平常的笔记注意整理。写在最后，当大家有这些基础条件后，可能就需要去面试了。分享一些面试经验。面试通过=50%实力+30%运气+20%技巧。你能掌握70%的主动，剩下的交给运气。首先要告诉自己，这不是一场面试，而是一场与自己未来同事之间的交流探讨。尽量消除紧张心理，完全不紧张也是不可能的，但是还是要尽可能稳下来。面试过程中尽量幽默，能做到和面试官谈笑风生你就赢了。在脉脉上看到的有人说做了面试官之后才发现其实你技术差不多就行，决定你过不过的就是看你顺不顺眼，所以最好能让面试官在短短几十分钟里喜欢上你！如果你实在很害怕，给你个经验之谈，首先找一些小一点的公司，会对各方面要求低一些，去练习面试，多练，就能消除你的紧张感，尤其当你收到offer后，会越来越自信的。（如果一点经验没有推荐看这个：Offer拿到手软——史上最良心的校招求职攻略：https://space.bilibili.com/17320304?spm_id_from=333.788.b_765f7570696e666f.2）一个良好的开头、一个高潮加一个完美的结尾。开头是自我介绍，怎么吸引面试官去问你最擅长的问题。进入你擅长的领域，争取给面试官一个惊艳的表现。同时如何结束面试一样很重要，这个可能需要自己不断练，全程记得要礼貌。电话面试的话要注意语速吐字，现场面试也要注意语速，可以用在草稿纸上写一写的方式帮助解释。面试中遇到没理解的问题要尽可能与面试官沟通，说不定他就在考你的沟通能力呢。在面试中遇到不会的或者完全不会的问题要在面试之前想好策略。我的策略一般是允许自己对于最多两个问题直接说我不会，此策略一般是对于自己完全没有把握的问题，让面试官换另一个问题。如果你强答这个题的话只能是勉勉强强的回答一下。在一场面试中有1,2个问题说不会的没有多大问题，但是对于其他的问题要尽量做到完美作答，这样才有把握。关于HR面，尽量提前查一下公司的文化是什么，表现的自己很向往并且符合这种文化就好。千万不要太个性。希望大家找到一个完美工作。欢迎关注公众号：计算机视觉life，一起探索计算机视觉新世界~"}
{"content2":"学习OpenCV（中文版）【原 书 名】 Learning OpenCV: Computer Vision with the OpenCV Library【原出版社】 O'Reilly Media, Inc.【作　　者】(美)Gary Bradski;Adrian Kaehler【译　　者】 于仕琪;刘瑞祯[同译者作品]【丛 书 名】 清华大学出版社O'Reilly系列【出 版 社】 清华大学出版社     【书 号】 9787302209935【上架时间】 2009-10-16【出版日期】 2009 年10月 【开 本】 16开 【页 码】 601     【版 次】1-1详情查看：http://www.china-pub.com/196032【内容简介】计算机视觉是在图像处理的基础上发展起来的新兴学科。OpenCV是一个开源的计算机视觉库，是英特尔公司资助的两大图像处理利器之一。它为图像处理、模式识别、三维重建、物体跟踪、机器学习和线性代数提供了各种各样的算法。.本书由OpenCV发起人所写，站在一线开发人员的角度用通俗易懂的语言解释了OpenCV的缘起和计算机视觉基础结构，演示了如何用OpenCV和现有的自由代码为各种各样的机器进行编程，这些都有助于读者迅速入门并渐入佳境，兴趣盎然地深入探索计算机视觉领域。本书可作为信息处理、计算机、机器人、人工智能、遥感图像处理、认知神经科学等有关专业的高年级学生或研究生的教学用书，也可供相关领域的研究工作者参考。透过本书，您将置身于迅速发展的计算机视觉领域。本书由自由开源OpenCV的发起人所著，介绍了计算机视觉，并通过实例演示了如何快速生成这样的应用——能使计算机“看到”并根据由此获取的数据做出决策。计算机视觉无处不在，安全系统、制造检验系统、医学图像分析、无人机等都可以见到它的踪影。它与Google Map和Google Earth紧密结合，它检查LCD屏幕上的像素，它确保衬衫上的每个针脚都能完全缝合。OpenCV提供了一个简易好用的计算机视觉框架和一个丰富的库，后者包含500多个可实时运行视觉代码的函数。..透过各章提供的练习，任何一个开发人员或爱好者都可以迅速掌握如何使用这个框架。本书特色主题如下：透彻介绍OpenCV从摄像机获取输入图像的变换图像的分割和形状的匹配模式识别，包括人脸检测二维和三维场景中的跟踪监测根据立体视觉进行三维重构机器学习算法“让机器来看”是一个富有挑战但也很有意思的目标。不管是想构建简单的视觉应用，还是复杂的视觉应用，都离不开这本入门必备参考，拿起它，开始愉快的学习之旅吧！...【目录信息】出版前言. VI译者序 XI写在前面的话 XIII前言 XV第1章 概述 1什么是OpenCV 1OpenCV的应用领域 1什么是计算机视觉 2OpenCV的起源 6下载和安装OpenCV 8通过SVN获取最新的OpenCV代码 11更多OpenCV文档 12OpenCV的结构和内容 14移植性 16练习 16第2章 OpenCV入门 18开始准备 18初试牛刀—— 显示图像 19<< 查看详细目录"}
{"content2":"计算机视觉（CV）前沿国际国内期刊与会议这里的期刊大部分都可以通过上面的专家们的主页间接找到1.国际会议 2.国际期刊 3.国内期刊 4.神经网络 5.CV 6.数字图象 7.教育资源，大学 8.常见问题1. 国际会议现在，国际上计算机视觉方面的三大国际会议是ICCV， CVPR和ECCV，统称之为ICE。ICCV的全称是International Comference on Computer Vision。ICCV两年一次，与ECCV正好错开，是公认的三个会议中级别最高的。ECCV的全称是Europeon Conference on Computer Vision，是一个欧洲的会议。CVPR的全称是Internaltional Conference on Computer Vision and Pattern Recogintion国际计算机视觉与模式识别会议。这是一个一年一次的会议，举办地在美国。ICIP—BMVC—MVA—国际模式识别会议(ICPR )：亚洲计算机视觉会议(ACCV)：2.国际期刊以计算机视觉为主要内容之一的国际刊物也有很多，如:International Journal of Computer VisionIEEE Trans. On PAMI http://www.computer.org/tpami/index.htmIEEE Transactionson Image Processing http://www.ieee.org/organizations/pubs/transactions/tip.htmPattern Recognition http://www.elsevier.com/locate/issn/00313203Pattern Recognition Letters http://www.elsevier.com/locate/issn/01678655IEEE Trans. on Robotics and Automation，IEEE TPAMIIEEE TIPCVGIP Computer Vision. Graphics and Image Processing，Visual Image Computing，IJPRAI(Internatiorial Journat of Pattern Recognition and Artificial Intelligence)众 所周知， computer vision(cv) 存在ICCV/CVPR/ECCV三个顶级会议， 它们档次差不多，都应该在一流会议行列， 没有必要给个高下。 有些us的人认为ICCV/CVPR略好于ECCV，而欧洲人大都认为ICCV/ECCV略好于CVPR， 某些英国的人甚至认为BMVC好于CVPR。简言之， 三个会议差不多， 各有侧重和偏好。笔者就个人经验浅谈三会异同， 以供大家参考和讨论。 三者乃cv领域的旗舰和风向标，其oral paper (包括best paper) 代表当年度cv的最高水准， 在此引用Harry Shum的一句话， 想知道某个领域在做些什么， 找最近几年此领域的proceeding看看就知道了。 ICCV/CVPR由IEEE Computer Society牵头组织， ECCV好像没有专门负责的组织。 CVPR每年(除2002年)都在美国开， ECCV每两年开一次，仅限欧洲， ICCV也是每两年一次， 各洲轮值。 基本可以保证每年有两个会议开， 这样研究者就有两次跻身牛会的机会。就录取率而言， 三会都有波动。 如ICCV2001录取率>30%， 且出现两个人(华人)各有三篇第一作者的paper的情况， 这在顶级牛会是不常见的 (灌水嫌疑)。 但是， ICCV2003， 2005两次录取率都很低， 大约20%左右。 ECCV也是类似规律， 在2004年以前都是>30%， 2006年降低到20%左右。 CVPR的录取率近年来一直偏高， 从2004年开始一直都在[25%，30%]。最近一次CVPR2006是28.1%， CVPR2007还不知道统计数据。 笔者猜测为了维持录取paper的绝对数量， 当submission少的时候录取率偏高， 反之偏低， 近几年三大会议的投稿数量全部超过1000， 相对2000年前， 三会录取率均大幅度降低， 最大幅度50%->20%。 对录取率走势感兴趣的朋友， 可参考 http://vrlab.epfl.ch/~ulicny/statistics/(CVPR2004的数据是错的)，http://www.adaptivebox.net/research/bookmark/CICON_stat.html.显 然， 投入cv的人越来越多，这个领域也是越来越大， 这点颇不似machine learning一直奉行愚蠢的小圈子主义。另外一点值得注意， ICCV/ECCV只收vision相关的topic， 而cvpr会收少量的pattern recognition paper， 如finger print等， 但是不收和image/video完全不占边的pr paper，如speech recognition等。 我一个朋友曾经review过一篇投往CVPR的speech的paper， 三个reviewer一致拒绝， 其中一个reviewer搞笑的指出， 你这篇paper应该是投ICASSP被据而转投CVPR的。 就topic而言， CVPR涵盖最广。 还有一个没有验证过的原因导致CVPR录取率高: 很多us的researcher不愿意或没有足够的经费到us以外的地方开会， 故CVPR会优先接收很多来自us的paper (让大家都happy)。以上对三会的分析对我们投paper是很有指导作用的。 目前的research我想绝大部分还是纸上谈兵， 必经 read paper -> write paper -> publish paper -> publish paper on top conferences and journals流程。 故了解投paper的一些基本技巧， 掌握领域的走向和热点， 是非常必要的。 避免做无用功，选择切合的topic， 改善presentation， 注意格式 (遵守规定的模板)， 我想这是很多新手需要注意的问题。 如ICCV2007明文规定不写summary page直接reject， 但是仍然有人忽视， 这是相当不值得的。3.国内期刊自动化学报、计算机学报、软件学报、电子学报，中国图象图形学报，模式识别与人工智能，光电子激光，精密光学工程等。4.神经网络神经网络-Neural Networks Tutorial Reviewhttp://hem.hj.se/~de96klda/NeuralNetworks.htmftp://ftp.sas.com/pub/neural/FAQ.htmlImage Compression with Neural Networkshttp://www.comp.glam.ac.uk/digimaging/neural.htmBackpropagator's Reviewhttp://www.dontveter.com/bpr/bpr.htmlBibliographies on Neural Networkshttp://liinwww.ira.uka.de/bibliography/Neural/Intelligent Motion Control with an Artificial Cerebellumhttp://www.q12.org/phd.htmlKernel Machineshttp://www.kernel-machines.org/Some Neural Networks Research Organizationshttp://www.ieee.org/nnc/http://www.inns.org/Neural Network Modeling in Vision Researchhttp://www.rybak-et-al.net/nisms.htmlNeural Networks and Machine Learninghttp://learning.cs.toronto.edu/Neural Application Softwarehttp://attrasoft.comNeural Network Toolbox for MATLABhttp://www.mathworks.com/products/neuralnet/Netlab Softwarehttp://www.ncrg.aston.ac.uk/netlab/Kunama Systems Limited http://www.kunama.co.uk/5.Computer Vision(计算机视觉)Annotated Computer Vision Bibliographyhttp://iris.usc.edu/Vision-Notes/bibliography/contents.htmlhttp://iris.usc.edu/Vision-Notes/rosenfeld/contents.htmlLawrence Berkeley National Lab Computer Vision and Robotics Applicationshttp://www-itg.lbl.gov/ITG.hm.pg.docs/VISIon/vision.htmlCVonline by University of EdinburghThe Evolving， Distributed， Non-Proprietary， On-Line Compendium of Computer Vision，www.dai.ed.ac.uk/CVonlineComputer Vision Handbook，www.cs.hmc.edu/~fleck/computer-vision-handbookVision Systems Coursewarewww.cs.cf.ac.uk/Dave/Vision_lecture/Vision_lecture_caller.htmlResearch Activities in Computer Visionhttp://www-syntim.inria.fr/syntim/analyse/index-eng.htmlVision Systems Acronymswww.vision-systems-design.com/vsd/archive/acronyms.htmlDictionary of Terms in Human and Animal Visionhttp://cns-web.bu.edu/pub/laliden/WWW/Visionary/Visionary.htmlMetrology based on Computer Visionwww.cranfield.ac.uk/sme/amac/research/metrology/metrology.html6.Digital Photography 数字图像Digital Photography， Scanning， and Image Processingwww.dbusch.com/scanners/scanners.htm l7.Educational Resources， Universities 教育资源，大学Center for Image Processing in Educationwww.cipe.comLibrary of Congress Call Numbers Related to Imaging Science by Rochester Institute of Technologyhttp://wally2.rit.edu/pubs/guides/imagingcall.htmlMathematical Experiences through Image Processing， University of Washingtonwww.cs.washington.edu/research/metip/metip.htmlVismod Tech Reports and Publications， MIThttp://vismod.www.media.mit.edu/cgi-bin/tr_pagemakerVision Lab PhD dissertation list， University of Antwerphttp://wcc.ruca.ua.ac.be/~visielab/theses.htmlINRIA (France) Research Projects: Human-Computer Interaction， Image Processing， Data Management， Knowledge Systemswww.inria.fr/Themes/Theme3-eng.htmlImage Processing Resourceshttp://eleceng.ukc.ac.uk/~rls3/Contents.htmPublications of Carsten Stegerhttp://www9.informatik.tu-muench ... r/publications.html8.FAQs（常见问题）comp.dsp FAQwww.bdti.com/faq/dsp_faq.htmRobotics FAQwww.frc.ri.cmu.edu/robotics-faqWhere's the sci.image.processing FAQ?www.cc.iastate.edu/olc_answers/p ... processing.faq.htmlcomp.graphics.algorithms FAQ， Section 3， 2D Image/Pixel Computationswww.exaflop.org/docs/cgafaqAstronomical Image Processing System FAQwww.cv.nrao.edu/aips/aips_faq.html四、搜索资源http://sal.kachinatech.com/http://cheminfo.pku.edu.cn/mirrors/SAL/index.shtml 北京大学Google输入：computer vision 或computer vision groups可以获得很多结果网络资源：CVonline http://homepages.inf.ed.ac.uk/rbf/CVonline/视觉研究组列表Computer vision test Image http://www.cs.cmu.edu/~cil/v-images.html卡内基梅隆标准图片库视觉论文搜索：Paper searchhttp://www.researchindex.com五、图像处理GPL库（代码库图像库等）http://www.ph.tn.tudelft.nl/~klamer/cppima.htmlCppima 是一个图像处理的C++函数库。这里有一个较全面介绍它的库函数的文档，当然你也可以下载压缩的GZIP包，里面包含TexInfo格式的文档。http://iraf.noao.edu/Welcome to the IRAF Homepage! IRAF is the Image Reduction and Analysis Facility， a general purpose software system for the reduction and analysis of astronomical datahttp://entropy.brni-jhu.org/tnimage.html一个非常不错的Unix系统的图像处理工具，看看它的截图。你可以在此基础上构建自己的专用图像处理工具包。http://sourceforge.net/projects/这是GPL软件集散地，可以搜索IP库。国内的CSDN http://www.csdn.net/本文引用地址：http://blog.sciencenet.cn/blog-337448-411967.html  此文来自科学网马琳博客，转载请注明出处。"}
{"content2":"http://blog.sina.com.cn/s/blog_6b99cdb50101ix0l.html和机器学习和计算机视觉相关的数学之一（以下转自一位MIT牛人的空间文章，写得很实际：）作者：Dahua感觉数学似乎总是不够的。这些日子为了解决research中的一些问题，又在图书馆捧起了数学的教科书。从大学到现在，课堂上学的和自学的数学其实不算少了，可是在研究的过程中总是发现需要补充新的数学知识。Learning和Vision都是很多种数学的交汇场。看着不同的理论体系的交汇，对于一个researcher来 说，往往是非常exciting的enjoyable的事情。不过，这也代表着要充分了解这个领域并且取得有意义的进展是很艰苦的。记得在两年前的一次 blog里面，提到过和learning有关的数学。今天看来，我对于数学在这个领域的作用有了新的思考。对于Learning的研究，1、Linear Algebra (线性代数) 和 Statistics (统计学) 是最重要和不可缺少的。这代表了Machine Learning中最主流的两大类方法的基础。一种是以研究函数和变换为重点的代数方法，比如Dimension reduction，feature extraction，Kernel等，一种是以研究统计模型和样本分布为重点的统计方法，比如Graphical model, Information theoretical models等。它们侧重虽有不同，但是常常是共同使用的，对于代数方法，往往需要统计上的解释，对于统计模型，其具体计算则需要代数的帮助。以代数和统计为出发点，继续往深处走，我们会发现需要更多的数学。2、Calculus (微积分)，只是数学分析体系的基础。其基础性作用不言而喻。Learning研究的大部分问题是在连续的度量空间进行的，无论代数还是统计，在研究优化问题的时候，对一个映射的微分或者梯度的分析总是不可避免。而在统计学中，Marginalization和积分更是密不可分——不过，以解析形式把积分导出来的情况则不多见。3、Partial Differential Equation （偏微分方程)这主要用于描述动态过程，或者仿动态过程。这个学科在Vision中用得比Learning多，主要用于描述连续场的运动或者扩散过程。比如Level set, Optical flow都是这方面的典型例子。4、Functional Analysis (泛函分析)通俗地，可以理解为微积分从有限维空间到无限维空间的拓展——当然了，它实际上远不止于此。在这个地方，函数以及其所作用的对象之间存在的对偶关系扮演了非常重要的角色。Learning发展至今，也在向无限维延伸——从研究有限维向量的问题到以无限维的函数为研究对象。Kernel Learning 和 Gaussian Process 是其中典型的例子——其中的核心概念都是Kernel。很多做Learning的人把Kernel简单理解为Kernel trick的运用，这就把kernel的意义严重弱化了。在泛函里面，Kernel (Inner Product)是建立整个博大的代数体系的根本，从metric, transform到spectrum都根源于此。5、Measure Theory (测度理论)这是和实分析关系非常密切的学科。但是测度理论并不限于此。从某种意义上说，Real Analysis可以从Lebesgue Measure（勒贝格测度）推演，不过其实还有很多别的测度体系——概率本身就是一种测度。测度理论对于Learning的意义是根本的，现代统计学整个就是建立在测度理论的基础之上——虽然初级的概率论教科书一般不这样引入。在看一些统计方面的文章的时候，你可能会发现，它们会把统计的公式改用测度来表达，这样做有两个好处：所有的推导和结论不用分别给连续分布和离散分布各自写一遍了，这两种东西都可以用同一的测度形式表达：连续分布的积分基于Lebesgue测度，离散分布的求和基于计数测度，而且还能推广到那种既不连续又不离散的分布中去（这种东西不是数学家的游戏，而是已经在实用的东西，在Dirchlet Process或者Pitman-Yor Process里面会经常看到)。而且，即使是连续积分，如果不是在欧氏空间进行，而是在更一般的拓扑空间（比如微分流形或者变换群），那么传统的黎曼积分（就是大学一年级在微积分课学的那种）就不work了，你可能需要它们的一些推广，比如Haar Measure或者Lebesgue-Stieltjes积分。6、Topology（拓扑学)这是学术中很基础的学科。它一般不直接提供方法，但是它的很多概念和定理是其它数学分支的基石。看很多别的数学的时候，你会经常接触这样一些概念：Open set / Closed set，set basis，Hausdauf, continuous function，metric space, Cauchy sequence, neighborhood, compactness, connectivity。很多这些也许在大学一年级就学习过一些，当时是基于极限的概念获得的。如果，看过拓扑学之后，对这些概念的认识会有根本性的拓展。比如，连续函数，当时是由epison法定义的，就是无论取多小的正数epsilon，都存在xxx，使得xxx。这是需要一种metric去度量距离的，在general topology里面，对于连续函数的定义连坐标和距离都不需要——如果一个映射使得开集的原像是开集，它就是连续的——至于开集是基于集合论定义的，不是通常的开区间的意思。这只是最简单的例子。当然，我们研究learning也许不需要深究这些数学概念背后的公理体系，但是，打破原来定义的概念的局限在很多问题上是必须的——尤其是当你研究的东西它不是在欧氏空间里面的时候——正交矩阵，变换群，流形，概率分布的空间，都属于此。7、Differential Manifold (微分流形)通俗地说它研究的是平滑的曲面。一个直接的印象是它是不是可以用来fitting一个surface什么的——当然这算是一种应用，但是这是非常初步的。本质上说，微分流形研究的是平滑的拓扑结构。一个空间构成微分流形的基本要素是局部平滑：从拓扑学来理解，就是它的任意局部都同胚于欧氏空间，从解析的角度来看，就是相容的局部坐标系统。当然，在全局上，它不要求和欧氏空间同胚。它除了可以用于刻画集合上的平滑曲面外，更重要的意义在于，它可以用于研究很多重要的集合。一个n-维线性空间的全部k-维子空间(k8、Lie Group Theory (李群论)一般意义的群论在Learning中被运用的不是很多，群论在Learning中用得较多的是它的一个重要方向Lie group。定义在平滑流形上的群，并且其群运算是平滑的话，那么这就叫李群。因为Learning和编码不同，更多关注的是连续空间，因为Lie group在各种群中对于Learning特别重要。各种子空间，线性变换，非奇异矩阵都基于通常意义的矩阵乘法构成李群。在李群中的映射，变换，度量，划分等等都对于Learning中代数方法的研究有重要指导意义。9、Graph Theory（图论)图，由于它在表述各种关系的强大能力以及优雅的理论，高效的算法，越来越受到Learning领域的欢迎。经典图论，在Learning中的一个最重要应用就是graphical models了，它被成功运用于分析统计网络的结构和规划统计推断和机器学习和计算机视觉相关的数学之二转自：http://blog.sina.com.cn/s/blog_6833a4df0100nazk.html1. 线性代数 (Linear Algebra)：我想国内的大学生都会学过这门课程，但是，未必每一位老师都能贯彻它的精要。这门学科对于Learning是必备的基础，对它的透彻掌握是必不可少的。我在科大一年级的时候就学习了这门课，后来到了香港后，又重新把线性代数读了一遍，所读的是Introduction to Linear Algebra (3rd Ed.)  by Gilbert Strang.这本书是MIT的 线性代数课使用的教材，也是被很多其它大学选用的经典教材。它的难度适中，讲解清晰，重要的是对许多核心的概念讨论得比较透彻。我个人觉得，学习线性代 数，最重要的不是去熟练矩阵运算和解方程的方法——这些在实际工作中MATLAB可以代劳，关键的是要深入理解几个基础而又重要的概念：子空间(Subspace)，正交(Orthogonality)，特征值和特征向量(Eigenvalues and eigenvectors)，和线性变换(Linear transform)。从我的角度看来，一本线代教科书的质量，就在于它能否给这些根本概念以足够的重视，能否把它们的联系讲清楚。Strang的这本书在这方面是做得很好的。而且，这本书有个得天独厚的优势。书的作者长期在MIT讲授线性代数课(18.06)，课程的video在MIT的Open courseware网站上有提供。有时间的朋友可以一边看着名师授课的录像，一边对照课本学习或者复习。http://ocw.mit.edu/OcwWeb/Mathematics/18-06Spring-2005/CourseHome/index.htm2. 概率和统计 (Probability and Statistics):概率论和统计的入门教科书很多，我目前也没有特别的推荐。我在这里想介绍的是一本关于多元统计的基础教科书：Applied Multivariate Statistical Analysis (5th Ed.)  by Richard A. Johnson and Dean W. Wichern这本书是我在刚接触向量统计的时候用于学习的，我在香港时做研究的基础就是从此打下了。实验室的一些同学也借用这本书学习向量统计。这本书没有特别追求数学上的深度，而是以通俗易懂的方式讲述主要的基本概念，读起来很舒服，内容也很实用。对于Linear regression, factor analysis, principal component analysis (PCA), and canonical component analysis (CCA)这些Learning中的基本方法也展开了初步的论述。之后就可以进一步深入学习贝叶斯统计和Graphical models。一本理想的书是Introduction to Graphical Models (draft version).  by M. Jordan and C. Bishop.我不知道这本书是不是已经出版了（不要和Learning in Graphical Models混淆，那是个论文集，不适合初学）。这本书从基本的贝叶斯统计模型出发一直深入到复杂的统计网络的估计和推断，深入浅出，statistical learning的许多重要方面都在此书有清楚论述和详细讲解。MIT内部可以access，至于外面，好像也是有电子版的。3. 分析 (Analysis)：我想大家基本都在大学就学过微积分或者数学分析，深度和广度则随各个学校而异了。这个领域是很多学科的基础，值得推荐的教科书莫过于Principles of Mathematical Analysis, by Walter Rudin有点老，但是绝对经典，深入透彻。缺点就是比较艰深——这是Rudin的书的一贯风格，适合于有一定基础后回头去看。在分析这个方向，接下来就是泛函分析(Functional Analysis)。Introductory Functional Analysis with Applications, by Erwin Kreyszig.适合作为泛函的基础教材，容易切入而不失全面。我特别喜欢它对于谱论和算子理论的特别关注，这对于做learning的研究是特别重要的。Rudin也有一本关于functional analysis的书，那本书在数学上可能更为深刻，但是不易于上手，所讲内容和learning的切合度不如此书。在分析这个方向，还有一个重要的学科是测度理论(Measure theory)，但是我看过的书里面目前还没有感觉有特别值得介绍的。4. 拓扑 (Topology)：在我读过的基本拓扑书各有特色，但是综合而言，我最推崇：Topology (2nd Ed.)  by James Munkres这本书是Munkres教授长期执教MIT拓扑课的心血所凝。对于一般拓扑学(General topology)有全面介绍，而对于代数拓扑(Algebraic topology)也有适度的探讨。此书不需要特别的数学知识就可以开始学习，由浅入深，从最基本的集合论概念（很多书不屑讲这个）到Nagata-Smirnov Theorem和Tychonoff theorem等较深的定理（很多书避开了这个）都覆盖了。讲述方式思想性很强，对于很多定理，除了给出证明过程和引导你思考其背后的原理脉络，很多令人赞叹的亮点——我常读得忘却饥饿，不愿释手。很多习题很有水平。5. 流形理论 (Manifold theory)：对于拓扑和分析有一定把握时，方可开始学习流形理论，否则所学只能流于浮浅。我所使用的书是Introduction to Smooth Manifolds.  by John M. Lee虽然书名有introduction这个单词，但是实际上此书涉入很深，除了讲授了基本的manifold, tangent space,bundle, sub-manifold等，还探讨了诸如纲理论(Category theory)，德拉姆上同调(De Rham cohomology)和积分流形等一些比较高级的专题。对于李群和李代数也有相当多的讨论。行文通俗而又不失严谨，不过对某些记号方式需要熟悉一下。虽然李群论是建基于平滑流形的概念之上，不过，也可能从矩阵出发直接学习李群和李代数——这种方法对于急需使用李群论解决问题的朋友可能更加实用。而且，对于一个问题从不同角度看待也利于加深理解。下面一本书就是这个方向的典范：Lie Groups, Lie Algebras, and Representations: An Elementary Introduction.  by Brian C. Hall此书从开始即从矩阵切入，从代数而非几何角度引入矩阵李群的概念。并通过定义运算的方式建立exponential mapping，并就此引入李代数。这种方式比起传统的通过“左不变向量场(Left-invariant vector field)“的方式定义李代数更容易为人所接受，也更容易揭示李代数的意义。最后，也有专门的论述把这种新的定义方式和传统方式联系起来。————————————————————————————无论是研究Vision, Learning还是其它别的学科，数学终究是根基所在。学好数学是做好研究的基石。学好数学的关键归根结底是自己的努力，但是选择一本好的书还是大有益处的。不同的人有不同的知识背景，思维习惯和研究方向，因此书的选择也因人而异，只求适合自己，不必强求一致。上面的书仅仅是从我个人角度的出发介绍的，我的阅读经历实在非常有限，很可能还有比它们更好的书（不妨也告知我一声，先说声谢谢了）。"}
{"content2":"计算机视觉、自然语言处理和语音识别是目前深度学习领域很热门的三大应用方向。计算机视觉学习，推荐阅读《深度学习之PyTorch实战计算机视觉》。学到人工智能的基础概念及Python 编程技能，掌握PyTorch 的使用方法，学到深度学习相关的理论知识，比如卷积神经网络、循环神经网络、自动编码器，等等。在掌握深度学习理论和编程技能之后，还会学到如何基于PyTorch 深度学习框架实战计算机视觉。《深度学习之PyTorch实战计算机视觉》中的大量实例在循序渐进地学习的同时，不断地获得成就感。学习参考：《深度学习之PyTorch实战计算机视觉》两个版本的高清PDF，287页，带书签目录，文字可以复制。网盘下载：http://106.13.73.98/abc/213"}
{"content2":"终于找到ML日报的微信链接，抄之......................................请拜访原文链接：【祖母论与还原论之争】为什么计算机人脸识别注定超越人类？评价：从直觉上，总体视觉框架，我更推崇maar视觉理论，即还原论。因为对于广泛的视觉识别，此解释在哲学意义上完备性比其他解释更好。但对于人脸识别，这就难说了。就像骑自行车需要学习，学习之后便成为“程序记忆”，固定为记忆系统。对于人脸识别这种极为特殊且频次极高的行为，千万年的人类进化，是否在神经元级别发生了本质的改变，即是否突破还原论的模式，还有待深入研究。cell杂志上发表的相关文章：L. Chang and D.Y. Tsao, “The code for facial identity in the primate brain,” Cell, doi:10.1016/ j.cell.2017.05.011, 2017.此外，对于人脸识别，现阶段最有成效的依然是CNN，但CNN结构与生物神经网络结构已相距甚远。一切模式识别都可以形式化为神经网络的数学形式，但对于CNN来说，CNN越来越像一个二维欧式空间级联乘法器。且DNN爆发后的这些年，逐渐靠近神经网络的网络模型是RNN，最接近神经网络的模型是LSTM。文章简介：“祖母细胞”(grandmother cell)是1969年麻省理工学院 Lettvin 教授在他的课程上提出的[1]。这种学说的核心观点认为人脑中存在一些“超级神经元”，单独一个这样的神经元就能对一些复杂的目标(如人脸)有特异性反应，而不需依靠大量神经元相互协同工作。还原论(reductionism)是个哲学概念，强调分析一个复杂事物时必须首先将其分解成相对简单的部件，然后逐一进行分析再组合。与还原论相对的是整体论(holism)，强调不可割裂地分析一个整体事物的每个局部。比如，西方科学比较重视还原论，我国中医更侧重整体论。具体到对人脑视神经机制的研究，还原论的代表是20世纪70年代开始兴起的Marr视觉理论框架[5]，整体论的代表是20世纪20年代出现的Gestalt理论[6]。原创 2017-06-14 吴春鹏新智元新智元原创作者：吴春鹏，胡祥杰【新智元导读】 近日， Cell 的一项研究在人脸识别领域引起轰动，研究揭示了灵长类动物人脸识别的具体神经元活动过程——对脸部的识别是由大脑中 200 多个不同神经元共同编码完成的，这一发现推翻了此前人脸由特定细胞识别的假说。本文认为，这一发现，可能会破解长久以来计算机视觉领域祖母细胞论与还原论之争。为什么计算机人脸识别会超越人类，我们找到了5个优势。作者吴春鹏：杜克大学电子与计算机工程系在读博士生，前富士通研发中心研究员，曾在美光、LG北美实验室等公司实习。研究方向是机器学习，计算机视觉和模式识别。灵长类动物脸部识别算法被破译6月3日，新智元报道，发表在 Cell 的一项研究揭示了人脸识别的具体神经元活动过程。对猕猴的实验表明，对脸部的识别是由大脑中 200 多个不同神经元共同编码完成的，每个神经元会对一张脸不同特征的参数组合进行相应。这一发现推翻了此前人脸由特定细胞识别的假说。论文的摘要部分介绍说：灵长类动物以惊人的速度和可靠性识别复杂的物体，比如动物的脸。本文中，我们揭示了大脑进行面部识别的代码。猕猴实验表明，在面部结构中，面部和细胞反应之间存在着非常简单的转变。通过将动物的脸格式化为高维线性空间中的点，我们发现每个脸细胞的发射速率（firing rate）与入射面部刺激在该空间单个轴上的投影成正比，这样一来，一个脸细胞就能对空间中任何位置的脸进行集合编码。使用这个代码，我们可以精确地解码面部神经元群的反应，并预测关于动物脸的神经发射速率。此外，这一代码推翻了此前由来已久的假设，那就是脸细胞会对特定的面部特征进行编码。我们的研究表明，其他对象也可以由类似的度量坐标系统进行编码。《纽约时报》报道称，机器学习给神经科学带来了一种悲观主义色彩，认为大脑类似黑箱，该论文则提供了反例：研究人员记录了视觉系统最高级的神经元信号，可以看到那里没有黑箱，我们完全可能揭开大脑的奥秘。这一研究的重要意义在于，可以推翻祖母细胞假说和验证还原论的正确性，这是计算机视觉和脑科学研究中一直以来的争论。祖母细胞论与还原论之争“祖母细胞”(grandmother cell)是1969年麻省理工学院 Lettvin 教授在他的课程上提出的[1]。这种学说的核心观点认为人脑中存在一些“超级神经元”，单独一个这样的神经元就能对一些复杂的目标(如人脸)有特异性反应，而不需依靠大量神经元相互协同工作。比如，来自加州理工学院等机构的研究者在2005年发现人脑内侧颞叶的神经元会选择性注意人脸、动物、自然场景等[2]。“祖母细胞”学说是否成立还没有最终的结论，但很有意思的一个现象是: 尽管大多数神经学家并不认可祖母细胞学说，但总有一部分人工智能相关领域的研究者试图通过自己的实验证明 “超级神经元”是有可能存在的，其中不乏业内知名学者，比如机器学习专家Andrew Ng [3]和小波分析领域专家Stephane Mallat [4]。(a)展示了Ng研究组提出的大规模无监督学习的深度自编码器(auto-encoder)，(b)展示了这个自编码器输出层中的单个神经元在“人脸-非人脸”的区分能力[3]。由于输出层单个神经元具有很好的判别效果，所以这篇文章据此认为祖母细胞是有可能存在的。不过我们应该认识到:这些文章想强调的是模型中的某些基本单元具有与祖母细胞相同的能力, 是想强调训练算法和模型设计很合理。这些工程实验远远不足以支持生物脑中存在类似的情况。还原论(reductionism)是个哲学概念，强调分析一个复杂事物时必须首先将其分解成相对简单的部件，然后逐一进行分析再组合。与还原论相对的是整体论(holism)，强调不可割裂地分析一个整体事物的每个局部。比如，西方科学比较重视还原论，我国中医更侧重整体论。具体到对人脑视神经机制的研究，还原论的代表是20世纪70年代开始兴起的Marr视觉理论框架[5]，整体论的代表是20世纪20年代出现的Gestalt理论[6]。. Marr视觉理论的三级表象结构[9].如所示，Marr理论认为视觉场景理解分为三级表象结构依次为: 图像基元，轮廓和深度，场景中物体的三维形状。也就是说，Marr理论认为局部特征加工优先于整体。Gestalt理论认为人类能从复杂场景中提取语义的关键步骤是对视觉场景形成了整体概念，整体视觉信息优先于局部信息得到加工，这也是和Marr理论的主要区别之一。尽管Gestalt理论中关于”把整体作为与局部之和不同的存在”并没有得到足够的后续理论和实验的支持，但有一些经典动物行为学实验发现确实存在大范围性质的知觉优先于局部性质 [7]，这说明按照Gestalt理论去建立视觉模型有一定的合理性。计算机视觉方法[8]往往同时使用Marr和Gestalt理论，这两个理论在工程实践中可以互补。具体做法就是根据Marr理论在图像局部区域上提取最简单的边缘特征，利用统计学习方法从边缘特征中提取更高阶特征(局部轮廓和几何形状)；同时根据具体Gestalt原理(collinearity, proximity, closure, continuation等)提取对应特征。大脑做人脸识别，相比计算机做人脸识别，差在哪？计算机做人脸识别的几个挑战是: 要能够有选择的注意重点区域而不是逐像素扫描，要能够抵抗噪声、光照和复杂背景，识别缩放、旋转和平移的人脸，自动去掉脸部遮挡和局部变形，从安全角度还要能够区分真实人脸和人脸面具。还有一个问题是很多人脸识别算法的细节是“黑箱”，物理意义不明确，很难解释。比如对于一个基于深度神经网络的人脸识别算法，我们很多时候无法解释每个神经元对于识别结果到底有多少贡献，这就导致我们无法对识别错误进行溯源和系统性修正。特别是深度神经网络中比较靠后的卷积层模板包含很多高频噪声，这意味着网络可能学到了与人脸无关的特征，虽然准确率很高。以上这些挑战和问题在人类观察者看来都可以很轻松地应对，原因是人脑视觉通路在长期进化中形成了功能完整并且优化的分层结构，逐层向上处理更加复杂的信息。对于人脸识别任务，这个分层结构能够从局部特征逐步学习到更大脸部区域的特征直到全脸，并过滤掉与人脸无关的信息，通过与人脑记忆等其他分区合作可以“脑补”人脸遮挡或不清晰的部分。展示了人脑视皮层中的视觉通路[10]，视网膜接收的信息通过侧膝体(lateral geniculate nucleus, LGN)传送给V1区。视网膜的“中央凹-周边”特性保证了选择性注意机制。V1和V2区负责初级视觉信息处理(局部边缘、简单几何形状等)，处理后的信息被交给V2分区之上的两个分支: 与目标识别相关的V3、V4区(what通路)，和与运动相关的MT、MST等分区(where通路)。通过这两个分支处理的信息会进一步与海马记忆区、前额叶皮层、前运动区等进行交互。中各个分区的大小比例基本符合真实情况，可以看到处理基元信息的V1和V2分区面积最大，而在V2分区之上的两个分支中左侧what通路比右侧where通路的分区基本都要大一些，我们可以猜测可能是因为what通路处理的信息更多，或者因为what通路更重要，亦或是where通路的分工更精细所以其中每个分区的面积相对较小。除了所有视觉识别任务都会激活的脑区，人脸图像还会特异地激活FFA(fusiform face area)和OFA(occipital face area)等脑区。与人脑相比，计算机做人脸识别的主要优势在于信息传递的能量转换方式、信息编码方式、海量视觉信息处理的可扩展性、计算资源和处理速度、偏置及纠正能力。人脑是一个受限的低功耗系统，并且学习过程中存在偏置，而计算机基本没有这些情况。第一，人脑中神经兴奋在每个神经元上以电能形式传导，而在神经元之间会在神经递质(neurotransmitter, 如5-羟色胺)的帮助下由电能转化为化学能，然后在传递到下一个神经元时重新转化为电能。能量转换消耗时间，并且化学能状态的处理速度一般会低于电能，而计算机视觉没有电能和化学能的转换状态。第二，在大脑神经元上传导的神经兴奋是电脉冲信号，而脉冲信号需要保持时序关系，这大大增加了信息编码的难度。计算机一般不采用这种脉冲信号对视觉特征进行编码。第三，面对海量视觉时，搭建分析能力更强的分布式机器学习系统是相对容易的，而实现人脑扩展是很困难的。脑机接口可以作为实现人脑扩展的一项技术，但仍有很多难点需要得到解决。第四，虽然神经元之间的连接强度可以在使用中得到调整，但视觉通路中的一些计算资源是有限的，比如视网膜中视杆细胞(处理亮度信息)和视锥细胞(处理彩色信息)的数量，还有中各个分区的大小，所以人脑视觉信息处理的速度上限是相对固定的。而计算机的各项计算资源是可以持续增长的，速度上限也是可以不断提升的。第五，认知心理学的一个经典实验发现: 东方人区分西方人脸的能力显著低于东方人区分东方人脸的能力。这说明人类的人脸识别过程受到遗传、心理和社会环境等多因素共同影响，存在固有的偏置，会出现识别效果两极化的情况。而计算机的人脸识别过程不存在这些人类才有的偏置，即使有其他偏置也可以通过调整样本和训练方法进行快速纠正，保证对各类人脸样本的识别能力接近。深度学习推进计算机视觉，但空间依然很大近年来基于深度学习的计算机视觉方法在图像分类等方面取得了突破性进展，但有一些关键技术点还需要进一步研究: 从图像/视频中提取更高层次的语义，基于视觉信息做复杂决策，提出功能更完善的算法。在图像/视频理解方面，目前的研究工作越来越关注对于抽象语义的理解，比如艺术风格迁移(artistic style transfer)[11]，如所示。从一幅图像中提取客观语义(比如图像前景和背景物体)是比较成熟的传统工作，但提取抽象语义(比如图像想表达的意图、艺术风格)仍是一项很有挑战的工作。近年来对抗模型GAN被普遍用到图像语义理解特别是抽象概念提取和迁移这方面的论文中，因为GAN通过有监督鉴别真伪的过程，使得无监督特征表示的判别能力更强，有助于无监督模型学习抽象语义。但是用GAN是否能提取到更高层次的语义在业内还是有争议的。还有一些研究工作通过图像和文本注释进行协同学习，形成了局部图像与文字的映射，即这个模型可以把新输入的图像转换成文字，或者反过来。通过文本辅助图像语义学习就像人类的“看图说话”，也是一种很有效的解决方案。在基于视觉信息的复杂决策方面，目前比较有意思的研究是把深度神经网络和增强学习相结合，用于学习电子游戏的动作控制。目前增强学习的使用有两种方法: 一种是像AlphaGo一样用不同的模型去学习policy和value，然后用类似Monte-Carlo tree search的搜索方法把policy和value结合到一起，再寻找最优action。还有一种方法多用于action是离散的情况(一般游戏只有上下左右四个方向)，直接用一个深度神经网络实现从当前status到action选择的映射，也就是把policy的学习过程嵌入到了这个网络中。 在具体使用时，需要进一步研究的问题之一如所示，就是如何把表示当前状态的图像、期望以及实际执行效果等内容送入网络，为当前action的选择提供更多有效的指导。功能更完善的算法是指算法不能有明显缺陷。GoodFellow等人在2014年的NIPS文章指出把反向传播的梯度加到输入图像上，然后用这个修改后的图像(adversarial example)送入模型就可导致识别错误。我们以往过于关注一个模型的预测精度，而忽略了这个模型可以被敌手利用的弱点。总体来看，一个模型的各项属性之间一般是一种此消彼长的关系。一个模型的参数增多，往往可以提高预测精度，但同时也意味着敌手对模型局部做修改更不易被察觉。仿生视觉派和数学视觉派之争：计算机要不要模拟人？计算机视觉可以分为仿生视觉派和纯粹建立在数学上的视觉派。两派之间争议的焦点之一就是: 我们是否需要去模拟人，以及我们需要在多大程度上去模拟人。多年来两派都得到了发展，并且相互之间有很大的交集。一个很有意思的例子就是卷积神经网络CNN的设计原理来自于20世纪60年的猫视网膜实验[13]，但CNN提出者Yann Lecun在公开场合反复表示他更愿意叫卷积网络，不愿意加上“神经”二字，因为他认为现在的卷积神经网络已经和早期的生物学发现有很大差别了。生物学研究与计算机视觉的关系很大程度上取决人工智能技术的发展以及我们对待人工智能的态度。在深度学习技术出现以前，基于传统机器学习的计算机视觉技术往往不是很理想，与人们的期望有很大差别，最简单的例子就是传统方法在知名图像库ImageNet的分类效果远远没有达到实用标准。因为人的正确率在此时占据上风，所以有相当一部分研究者更愿意从人类感知和认知系统去寻找灵感来改进算法。比如根据的人脑视觉通路，计算机视觉研究者把重点放在了对what和where通路建模，以及这两个通路交互的建模。由此出现了一些算法采用贝叶斯图等方法模拟what和where通路来建立视觉注意模型[14]，如。深度学习技术出现之后，情况发生了一些变化。深度神经网络在图像分类取得了惊人的效果，极大的缩短了算法和人类在一些视觉任务上的准确率差距。与此同时，以“神威·太湖之光”为代表的超级计算机在不断挑战远远高于人类的数学运算速度。这种情况下，以前被压制的声音再次浮出水面:计算机比人算得快，在简单学习任务上已接近人，虽然在情感计算和某些复杂决策领域还暂时不如人，但我们真的还有必要去继续学习人吗？如果不那么依赖对人的研究，而是依靠计算机的强大计算能力，是否能够衍生出比人还有效的推理技巧？ AlphaGo可以为这种观点提供一些参考。AlphaGo学习过少量人类棋谱，但AlphaGo的训练更多通过自我博弈来提升棋力，自我博弈严重依赖计算能力。它走出的棋总是出乎所有人类围棋高手的意料，不按人的套路而战胜了人。当然，AlphaGo只是一个按照明确的规则和明确的棋面下围棋的简单机器，还达不到超越人的人工智能即强人工智能(artificial general intelligence, AGI)。以现在的计算机视觉发展水平(或者人工智能发展水平)，在到达强人工智能这个“奇点”之前，我们可能还是需要同时从仿生学和纯数学这两个角度去提升计算机视觉技术。因为强人工智能所需要的推理能力中还有很多基本特点我们没有搞清楚，除了人和其他生物，我们还没有更好的学习对象，特别是人所具有的联想和顿悟能力是现在计算机欠缺的。在进一步加强对人类视觉通路()研究的基础上，我们还需要探索的是人脑中长期和短期记忆是如何相互配合来完成视觉任务的，LSTM和GRU这两种RNN模型只是与记忆相关的简化建模方法。最后，还有一个很本质的问题需要研究，就是神经元集群编码。通过电物理学手段对神经元进行刺激，我们对于单个神经元的属性已经比较了解了，但大量神经元是如何通过相互配合来完成计算任务的我们还知之甚少。我们应该看到，近年来神经网络复兴的背后有一个必不可少的推手，就是以GPU为代表的高性能计算芯片的发展。生物学、神经科学、实验心理学在未来的突破也必将再次带动计算机视觉技术的飞跃。参考文献：[1] H. B. Barlow. The neuron in perception. In: Gazzaniga MS, editor. The cognitive neurosciences. Cambridge (MA): MIT Press, pp. 415–34, 1995.[2] R. Q. Quiroga, L. Reddy, G. Kreiman, C. Koch, and I. Fried. Invariant visual representation by single neurons in the human brain. Nature, vol. 435, pp. 1102-1107, 2005.[3] Q. V. Le, M. A. Ranzato, M. Devin, K. Chen, G. S. Corrado, J. Dean and Andrew Y. Ng. Building high-level features using large scale unsupervised learning. ICML, 2012.[4] S. Mallat. Understanding deep convolutional networks. Philosophical Transactions A, vol. 374, iss. 2065, 2016.[5] D. Marr. Vision: A Computational Investigation into the Human Representation and Processing of Visual Information. New York: Freeman, 1982.[6] Smith, Barry. Foundations of Gestalt Theory, Munich and Vienna: Philosophia Verlag, 1988.[7] L. Chen, S. Zhang and M. V. Srinivasan. Global Perception in Small Brains: Topological Pattern Recognition in Honey Bees. PNAS, pp. 6884-6889, 2003.[8] O. L. Meur, P. L. Callet, D. Barba and D. Thoreau. A coherent computational approach to model bottom-up visual attention. PAMI, vol. 28, no. 5, 2006.[9] http://cns-alumni.bu.edu/~slehar/webstuff/pcave/marr.html[10] N. Kruger, P. Janssen, S. Kalkan, M. Lappe, A. Leonardis, J. Piater, A. J. Rodriguez-Sanchez, and L. Wiskott. Deep hierarchies in the primate visual cortex: What can we learn for computer vision. PAMI, vol. 6, no. 1, 2007.[11] Y. Jing, Y. Yang, Z. Feng, J. Ye and M. Song. Neural style transfer: A review. arXiv:1705.04058v1, 2017.[12] A. Dosovitskiy and V. Koltun. Learning to act by predicting the future. ICLR, 2017.[13] D. H. Wiesel and T. N. Hubel. Receptive fields of single neurons in the cat’s striate cortex. J. Physiol., 148:574–591, 1959.[14] S. Chikkerur, T. Serre, C. Tan and T. Poggio. What and where: A Bayesian inference theory of attention. Vision Research, 50(22):2233-47, 2010."}
{"content2":"前言在刚过去的凌晨（北京时间 5月18日 1.00-3.00），一年一度的2017年Google I/O大会在美国谷歌山景城海岸线圆形剧场如期举行Google I/O 大会：Innovation in the Open，开放中创新，是Google官方举办的开发者大会面向 开发者，会议内容是：更新和发布Google的新产品 & 技术对比于网上内容相互复制、堆砌的Google I/O大会内容报道，这是一份很有诚意的 2017年 Google I/O大会的汇总 & 解析目录1. 2016年 Google 产品的表现在发布新产品& 技术前，谷歌 CEO Sundar Pichai先公布了2016年 Google 产品 在市场上的表现：1.1 Google 全系列产品总用户量达10亿具体产品表现如下：Google Photos（ 相册）：活跃用户量= 5亿，超过12亿张照片上传 / 天YouTube：用户观看视频时长超过10亿小时 / 天Google Map（地图）：导航的里程超过10亿公里 / 天Google Drive（云端硬盘）：活跃用户数 = 8亿1.2 Android 市场份额Android OS 的活跃设备数达到了 * 20 亿 *2. 会议内容2.1 概览关键词：人工智能2.2 具体内容2.2.1 人工智能 - Google.ai计划相比于 以往先介绍 Android 产品 & 技术，这次Google I/O 大会却将人工智能提高了第1部分介绍谷歌 CEO Sundar Pichai 针对 谷歌 在人工智能领域的发展提出了两个关键点：1. 人工智能 会渗透到 谷歌旗下所有产品2. 语音 & 图像 会成为谷歌人工智能的重要交互方式这意味是Google 将人工智能渗透在语音 & 图像识别领域上过去的交互方式主要是鼠标和触屏针对谷歌在人工智能领域的发展的关键点，CEO Sundar Pichai 提出了Google.ai计划目的：希望将人工智能具体应用到实际中具体内容：包括3个方面学术领域开发者的开发领域日常生活详细解读应用1：学术领域Google 将包括深度学习在内的人工智能引入到各个学术领域，如生物、医疗、化学等应用场景主要包括医疗诊断、细胞生物学、DNA测序等。应用2：开发者的开发领域Google将开放Tensor Flow、TPU等软硬件工具给开发者，希望所有的开发者都可以进行人工智能的学习 & 开发。Tensor FlowTensor Flow是Github上关注最多的开源项目。所有人都能用人工智能来开发他们自己的应用。人工智能不再神秘，而是会真正的进入日常生活，而且会用众多的行业被人工智能改造Cloud TPU即Google Cloud Compute Engine，用于机型硬件远程加速，是大数据、深度学习的核心引擎。在人工智能处理上比 CPU 和 GPU 表现更优异，重点在训练层面。本次优化了训练(Training)，同时也优化了推理(Inference)。Google将开发该AI基础设施给广发开发者，希望开发者能利用这些强大的人工智能计算资源来开发人工智能的应用应用3：日常生活Google希望将人工智能带入大家的日常生活具体做法则是将 人工智能渗透到产品中，具体渗透的产品如下：产品1：Google Lens定位：一个 视觉搜索引擎新发布产品功能：利用 计算机视觉算法 实时识别 照片内容，从而根据照片内容 提供 相关服务具体使用场景场景1：若您 拍摄 音乐会海报，Google Lens会建议你购买门票，并调用购买门票的APP场景2：若您拍摄路边的餐馆，Google Lens会自动调出该餐厅 & 地图信息Google Lens将集成到Google Assistant和Google Photos中产品2：Google Asistant定位：智能手机 助手基于Google Now融合了语音搜索、人工智能和机器深度学习于2016年发布，此处是更新核心功能：通过 语音 让机器智能地为你提供对应服务类似Apple 的Siri、微软的小冰新增功能：加入Google Lens加入文字输入的方式Google Assistant将提供其API，方便开发者嵌入到自己的应用中特别注意：Google Assistant也将登陆iPhone产品3：Google Home定位：一款基于 Google Assistant的智能家居硬件于2016年发布，下面是更新核心功能：通过语音对话的方式 帮助处理家庭场景下繁琐的日常事务新增功能点a. 打电话通过免提进行通话只需发出 Okay Google，call my mom（打电话给妈妈）等语音命令，就能够自动发出通话请求若居住在美国 或 加拿大，就可免费拨打美国 或 加拿大的任何号码。无需设置、无需安装APP、更不需要手机。b. 与手机、家里的电视 互动在娱乐方面，Google Home 进行了加强与Spotify、HBO now、Hulu等娱乐应用合作，即可以通过语音控制手机上的这些 App作为媒介，连接手机 & 电视。如，出门前可以将导航地图发送到你的手机，或在电视上播放YouTube上看到一半的影片。产品4：Google Photos定位：谷歌相册应用植入Google Lens后的功能更新点：1. 建议共享功能描述：拍照后，Google Photos通过 人脸识别，识别出照片中的人，并建议你把照片发送给对方。具体场景：聚会时拍完集体照后，需要一个个去发给他们，现在一键就可以发送了。2. 共享照片库功能描述：允许你自动与指定的人（比如爸妈、伴侣）共享你的所有的或部分的照片具体场景爸妈总需要多发生活照给他们，但会经常忘 & 需要筛选生活照。现在通过共享可以省去很多时间 & 操作成本上述两个功能加强了图片共享的社交性，功能1是跟半熟人的低频共享；功能2是跟熟人、亲人的高频共享。4. 自动识别图片内容功能描述：自动识别照片中景点、建筑具体使用场景，旅游、迷路等。2.2.2 Android这次Android 主要更新点是：Android 系统 & 开发语言1. Android 8.0（Android O）继Android 7.0 采用Nougat（牛轧糖）后，Android 8.0采用Oreo（奥利奥），即Android OAndroid O主要关注：流畅体验 & 核心功能1. 流畅体验谷歌强调电池续航、安全性、启动时间 & 稳定性，具体如下：2. 核心功能在Android O中新增了几项功能已提高用户在使用时的流畅度:功能1：画中画功能描述：单屏手机上 使用 多应用 操作具体场景：边微信聊天边购物、边看视频边做笔记功能2：App动态提醒标签（小红点）功能描述：当应用收到提醒时，该应用在桌面上的图标右上角显示一个小点点作为动态提醒标签。若长按显示提醒的app，则会弹出类似于iOS Force Touch的小窗，以帮助用户便捷回复若开发者觉得提醒圈颜色与自己的app不搭，Android O系统会根据开发者所提交的配色，为每款app选择最合适的颜色作为提醒色。功能3：基于机器学习的文字识别功能描述：当用户打开任意文本，神经网络能识别并了解到它是什么，并帮助用户自动选择正确的内容需求场景：选中文本内容时 因手指太粗等原因而无法正确选中。2. Android Go定位：一款 轻量、入门级别的 Android 系统使用场景：专门应用于底端硬件设备。如低配置智能手机、平板等等特点意图：通过开发新兴市场（如进军底端国家印度、非洲等），让用户以低廉的成本就能获得原生的Android体验，从而进一步扩大Android市场份额特别注意：1. 从Android O开始，2018年出货的所有RAM不超过1GB的设备都将采用Android Go2. 谷歌未来还将推出更多不同版本的Android以适应不同需求场景3. Kotlin本次大会，Google宣布 Kotlin成为Android 开发的1级编程语言定义：是JetBrains在2010年推出的编程语言，并在2011年开源特点：与Java 互通，并具备许多 Java尚不支持的新特性。Android Studio 3.0将提供支持2.2.3 其余更新1. VR领域LG今年的新旗舰、三星Galaxy S8将支持谷歌Daydream平台谷歌将联合HTC、联想&高通制造VR一体机：Daydream VR首批产品将于2017年推出。2. AR领域公布了AR技术视觉定位服务VPS（Visual Positioning Service）具体使用场景：室内导航具体：通过摄像头识别室内环境特征，引导消费者在商场找到特定商品。如在商场定位买手机等等3. Youtube新增功能：Super Chat功能定位：视频直播社交功能面向用户：Youtube 上的直播观众 & 博主功能描述：观众 通过 直播聊天室 博主的行为具体应用场景：观众A在大会现场观看博主B的直播，观众A付费500美元购买了Super Chat，此举立刻激活了主播所在地的一个大喇叭，喇叭声响意味着围观群众可以朝主播扔水球，于是现场立马演变成一场狂欢。需求：提高博主与观众的互动性、提高观众的娱乐性、网红直播赚更多的钱3. 总结从上面可以看出：本次的Google I/O 大会的重心在 人工智能，表明Google已经从一家移动为先的公司，成为了一家人工智能为先的公司从这次大会开始，Google将依赖Andorid系统 、一系列产品矩阵 & 机器学习工具TensorFlow 建立起一个属于谷歌的人工智能生态系统4. 遗憾本次大会谷歌在人工智能方面的布局确实惊艳非常惊艳但遗憾的是，之前谷歌提及的新系统Fuchsia & 整合Chrome OS却只字不提，希望下次大会中会有关于这两方面的信息。最后，作为一名Android开发者，其实我更关注的是：到！底！什！么！时！候！回！来！中！国！请帮顶或评论点赞！因为你的鼓励是我写作的最大动力！、-END-"}
{"content2":"1. Bilateral FilterBilateral Filter俗称双边滤波器是一种简单实用的具有保持边缘作用的平缓滤波器，由Tomasi等在1998年提出。它现在已经发挥着重大作用，尤其是在HDR领域。[1998 ICCV] BilateralFiltering for Gray and Color Images[2008 TIP] AdaptiveBilateral Filter for Sharpness Enhancement and Noise Removal2. Color如果对颜色的形成有一定的了解，能比较深刻的理解一些算法。这方面推荐冈萨雷斯的数字图像处理中的相关章节以及Sharma在Digital Color Imaging Handbook中的第一章“Colorfundamentals for digital imaging”。跟颜色相关的知识包括Gamma，颜色空间转换，颜色索引以及肤色模型等，这其中也包括著名的EMD。[1991 IJCV] Color Indexing[2000 IJCV] The EarthMover's Distance as a Metric for Image Retrieval[2001 PAMI] Colorinvariance[2002 IJCV] StatisticalColor Models with Application to Skin Detection[2003] A review of RGBcolor spaces[2007 PR]A survey ofskin-color modeling and detection methodsGamma.pdfGammaFAQ.pdf3.Compression and Encoding个人以为图像压缩编码并不是当前很热的一个话题，原因前面已经提到过。这里可以看看一篇对编码方面的展望文章[2005 IEEE] Trends andperspectives in image and video coding4.Contrast Enhancement对比度增强一直是图像处理中的一个恒久话题，一般来说都是基于直方图的，比如直方图均衡化。冈萨雷斯的书里面对这个话题讲的比较透彻。这里推荐几篇个人认为不错的文章。[2002 IJCV] Vision and theAtmosphere[2003 TIP] Gray and colorimage contrast enhancement by the curvelet transform[2006 TIP] Gray-levelgrouping (GLG) an automatic method for optimized image contrastenhancement-part II[2006 TIP] Gray-levelgrouping (GLG) an automatic method for optimized image contrastEnhancement-part I[2007 TIP] TransformCoefficient Histogram-Based Image Enhancement Algorithms Using Contrast Entropy[2009 TIP] A HistogramModification Framework and Its Application for Image Contrast Enhancement5. Deblur (Restoration)图像恢复或者图像去模糊一直是一个非常难的问题，尤其是盲图像恢复。港中文的jiaya jia老师在这方面做的不错，他在主页也给出了exe。这方面的内容也建议看冈萨雷斯的书。这里列出了几篇口碑比较好的文献，包括古老的Richardson-Lucy方法，几篇盲图像恢复的综述以及最近的几篇文章，尤以Fergus和Jiaya Jia的为经典。[1972] Bayesian-BasedIterative Method of Image Restoration[1974] an iterative techniquefor the rectification of observed distributions[1990 IEEE] Iterativemethods for image deblurring[1996 SPM] Blind ImageDeconvolution[1997 SPM] Digital imagerestoration[2005] Digital ImageReconstruction - Deblurring and Denoising[2006 Siggraph] RemovingCamera Shake from a Single Photograph[2008 Siggraph]High-quality Motion Deblurring from a Single Image[2011 PAMI]Richardson-Lucy Deblurring for Scenes under a Projective Motion Path6. Dehazing and Defog严格来说去雾化也算是图像对比度增强的一种。这方面最近比较好的工作就是He kaiming等提出的Dark Channel方法。这篇论文也获得了2009的CVPR 最佳论文奖。2003年的广东高考状元已经于2011年从港中文博士毕业加入MSRA（估计当时也就二十五六岁吧），相当了不起。[2008 Siggraph] SingleImage Dehazing[2009 CVPR] Single ImageHaze Removal Using Dark Channel Prior[2011 PAMI] Single ImageHaze Removal Using Dark Channel Prior7. Denoising图像去噪也是图像处理中的一个经典问题，在数码摄影中尤其重要。主要的方法有基于小波的方法和基于偏微分方程的方法。[1992 SIAM] Imageselective smoothing and edge detection by nonlinear diffusion. II[1992 SIAM] Imageselective smoothing and edge detection by nonlinear diffusion[1992] Nonlinear totalvariation based noise removal algorithms[1994 SIAM] Signal andimage restoration using shock filters and anisotropic diffusion[1995 TIT] De-noising bysoft-thresholding[1998 TIP] Orientationdiffusions[2000 TIP] Adaptivewavelet thresholding for image denoising and compression[2000 TIP] Fourth-orderpartial differential equations for noise removal[2001] Denoising through wavelet shrinkage[2002 TIP] The CurveletTransform for Image Denoising[2003 TIP] Noise removalusing fourth-order partial differential equation with applications to medicalmagnetic resonance images in space and time[2008 PAMI] AutomaticEstimation and Removal of Noise from a Single Image[2009 TIP] Is DenoisingDead8. Edge Detection边缘检测也是图像处理中的一个基本任务。传统的边缘检测方法有基于梯度算子，尤其是Sobel算子，以及经典的Canny边缘检测。到现在，Canny边缘检测及其思想仍在广泛使用。关于Canny算法的具体细节可以在Sonka的书以及canny自己的论文中找到，网上也可以搜到。最快最直接的方法就是看OpenCV的源代码，非常好懂。在边缘检测方面，Berkeley的大牛J Malik和他的学生在2004年的PAMI提出的方法效果非常好，当然也比较复杂。在复杂度要求不高的情况下，还是值得一试的。MIT的Bill Freeman早期的代表作Steerable Filter在边缘检测方面效果也非常好，并且便于实现。这里给出了几篇比较好的文献，包括一篇最新的综述。边缘检测是图像处理和计算机视觉中任何方向都无法逃避的一个问题，这方面研究多深都不为过。[1980] theory of edgedetection[1983 Canny Thesis] findedge[1986 PAMI] AComputational Approach to Edge Detection[1990 PAMI] Scale-spaceand edge detection using anisotropic diffusion[1991 PAMI] The design anduse of steerable filters[1995 PR] Multiresolutionedge detection techniques[1996 TIP] Optimal edgedetection in two-dimensional images[1998 PAMI] Local ScaleControl for Edge Detection and Blur Estimation[2003 PAMI] Statisticaledge detection_ learning and evaluating edge cues[2004 IEEE] Edge DetectionRevisited[2004 PAMI] Design ofsteerable filters for feature detection using canny-like criteria[2004 PAMI] Learning toDetect Natural Image Boundaries Using Local Brightness, Color, and Texture Cues[2011 IVC] Edge and lineoriented contour detection State of the art9. Graph Cut基于图割的图像分割算法。在这方面没有研究，仅仅列出几篇引用比较高的文献。这里又见J Malik，当然还有华人杰出学者Jianbo Shi，他的主页非常搞笑，在醒目的位置标注Do not flyChina Eastern Airlines ...  看来是被坑过，而且坑的比较厉害。这个领域，俄罗斯人比较厉害。[2000 PAMI] Normalizedcuts and image segmentation[2001 PAMI] Fastapproximate energy minimization via graph cuts[2004 PAMI] What energyfunctions can be minimized via graph cuts10.Hough Transform虽然霍夫变换可以扩展到广义霍夫变换，但最常用的还是检测圆和直线。这方面同样推荐看OpenCV的源代码，一目了然。Matas在2000年提出的PPHT已经集成到OpenCV中去了。[1986 CVGIU] A Survey ofthe Hough Transform[1989] A Comparative studyof Hough transform methods for circle finding[1992 PAMI] Shapesrecognition using the straight line Hough transform_ theory and generalization[1997 PR] Extraction ofline features in a noisy image[2000 CVIU] RobustDetection of Lines Using the Progressive Probabilistic Hough Transform11. Image Interpolation图像插值，偶尔也用得上。一般来说，双三次也就够了[2000 TMI] Interpolationrevisited12. Image Matting也就是最近，我才知道这个词翻译成中文是抠图，比较难听，不知道是谁开始这么翻译的。没有研究，请看文章以及Richard Szeliski的相关章节。以色列美女Levin在这方面有两篇PAMI。[2008 Fnd] Image and VideoMatting A Survey[2008 PAMI] A Closed-FormSolution to Natural Image Matting[2008 PAMI] SpectralMatting13. Image Modeling图像的统计模型。这方面有一本专门的著作Natural Image Statistics[1994] The statistics ofnatural images[2003 JMIV] On Advances inStatistical Modeling of Natural Images[2009 IJCV] Fields ofExperts[2009 PAMI] Modelingmultiscale subbands of photographic images with fields of Gaussian scalemixtures14. Image Quality Assessment在图像质量评价方面，Bovik是首屈一指的。这位老师也很有意思，作为编辑出版了很多书。他也是IEEE的Fellow[2004 TIP] Image qualityassessment from error visibility to structural similarity[2011 TIP] blind imagequality assessment From Natural Scene Statistics to Perceptual Quality15. Image Registration图像配准最早的应用在医学图像上，在图像融合之前需要对图像进行配准。在现在的计算机视觉中，配准也是一个需要理解的概念，比如跟踪，拼接等。在KLT中，也会涉及到配准。这里主要是综述文献。[1992 MIA] Image matching asa diffusion process[1992 PAMI] A Method forRegistration of 3-D shapes[1992] a survey of imageregistration techniques[1998 MIA] A survey ofmedical image registration[2003 IVC] Imageregistration methods a survey[2003 TMI]Mutual-Information-Based Registration of Medical Survey[2011 TIP] Hairisregistration16. Image Retrieval图像检索曾经很热，在2000年之后似乎消停了一段时间。最近各种图像的不变性特征提出来之后，再加上互联网搜索的商业需求，这个方向似乎又要火起来了，尤其是在工业界。这仍然是一个非常值得关注的方面。而且图像检索与目标识别具有相通之处，比如特征提取和特征降维。这方面的文章值得一读。在最后给出了两篇Book chapter，其中一篇还是中文的。[2000 PAMI] Content-basedimage retrieval at the end of the early years[2000 TIP] PicToSeekCombining Color and Shape Invariant Features for Image Retrieval[2002] Content-Based ImageRetrieval Systems A Survey[2008] Content-Based ImageRetrieval-Literature Survey[2010] Plant ImageRetrieval Using Color,Shape and Texture Features[2012 PAMI] A MultimediaRetrieval Framework Based on Semi-Supervised Ranking and Relevance FeedbackCBIR Chinesefundament of cbir17. Image Segmentation图像分割，非常基本但又非常难的一个问题。建议看Sonka和冈萨雷斯的书。这里给出几篇比较好的文章，再次看到了J Malik。他们给出了源代码和测试集，有兴趣的话可以试试。[2004 IJCV] EfficientGraph-Based Image Segmentation[2008 CVIU] Imagesegmentation evaluation A survey of unsupervised methods[2011 PAMI] ContourDetection and Hierarchical Image Segmentation18. Level Set大名鼎鼎的水平集，解决了Snake固有的缺点。Level set的两位提出者Sethian和Osher最后反目，实在让人遗憾。个人以为，这种方法除了迭代比较费时，在真实场景中的表现让人生疑。不过，2008年ECCV上的PWP方法在结果上很吸引人。在重初始化方面，Chunming Li给出了比较好的解决方案[1995 PAMI] Shape modelingwith front propagation_ a level set approach[2001 JCP] Level SetMethods_ An Overview and Some Recent Results[2005 CVIU] Geodesicactive regions and level set methods for motion estimation and tracking[2007 IJCV] A Review ofStatistical Approaches to Level Set Segmentation[2008 ECCV] RobustReal-Time Visual Tracking using Pixel-Wise Posteriors[2010 TIP] DistanceRegularized Level Set Evolution and its Application to Image Segmentation19.Pyramid其实小波变换就是一种金字塔分解算法，而且具有无失真重构和非冗余的优点。Adelson在1983年提出的Pyramid优点是比较简单，实现起来比较方便。[1983] The LaplacianPyramid as a Compact Image Code20. Radon TransformRadon变换也是一种很重要的变换，它构成了图像重建的基础。关于图像重建和radon变换，可以参考章毓晋老师的书，讲的比较清楚。[1993 PAMI] Imagerepresentation via a finite Radon transform[1993 TIP] The fastdiscrete radon transform I theory[2007 IVC] Generalisedfinite radon transform for N×N images21.Scale Space尺度空间滤波在现代不变特征中是一个非常重要的概念，有人说SIFT的提出者Lowe是不变特征之父，而Linderburg是不变特征之母。虽然尺度空间滤波是Witkin最早提出的，但其理论体系的完善和应用还是Linderburg的功劳。其在1998年IJCV上的两篇文章值得一读，不管是特征提取方面还是边缘检测方面。[1987] Scale-spacefiltering[1990 PAMI] Scale-Spacefor Discrete Signals[1994] Scale-space theoryA basic tool for analysing structures at different scales[1998 IJCV] Edge Detectionand Ridge Detection with Automatic Scale Selection[1998 IJCV] FeatureDetection with Automatic Scale Selection22. Snake活动轮廓模型，改变了传统的图像分割的方法，用能量收缩的方法得到一个统计意义上的能量最小（最大）的边缘。[1987 IJCV] Snakes ActiveContour Models[1996 ] deformable modelin medical image A Survey[1997 IJCV] geodesicactive contour[1998 TIP] Snakes, shapes,and gradient vector flow[2000 PAMI] Geodesic activecontours and level sets for the detection and tracking of moving objects[2001 TIP] Active contourswithout edges23. Super Resolution超分辨率分析。对这个方向没有研究，简单列几篇文章。其中Yang Jianchao的那篇在IEEE上的下载率一直居高不下。[2002] Example-BasedSuper-Resolution[2003 SPM] Super-Resolution Image Reconstruction A Technical Overview[2009 ICCV] Super-Resolutionfrom a Single Image[2010 TIP] ImageSuper-Resolution Via Sparse Representation24. Thresholding阈值分割是一种简单有效的图像分割算法。这个topic在冈萨雷斯的书里面讲的比较多。这里列出OTSU的原始文章以及一篇不错的综述。[1979 IEEE] OTSU Athreshold selection method from gray-level histograms[2001 JISE] A Fast Algorithmfor Multilevel Thresholding[2004 JEI] Survey overimage thresholding techniques and quantitative performance evaluation25. Watershed分水岭算法是一种非常有效的图像分割算法，它克服了传统的阈值分割方法的缺点，尤其是Marker-Controlled Watershed，值得关注。Watershed在冈萨雷斯的书里面讲的比较详细。[1991 PAMI] Watersheds indigital spaces an efficient algorithm based on immersion simulations[2001]The WatershedTransform Definitions, Algorithms and Parallelizat on Strategies"}
{"content2":"ylbtech-AI：AI人工智能（Artificial Intelligence），英文缩写为AI。它是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。人工智能从诞生以来，理论和技术日益成熟，应用领域也不断扩大，可以设想，未来人工智能带来的科技产品，将会是人类智慧的“容器”。人工智能可以对人的意识、思维的信息过程的模拟。人工智能不是人的智能，但能像人那样思考、也可能超过人的智能。人工智能是一门极富挑战性的科学，从事这项工作的人必须懂得计算机知识，心理学和哲学。人工智能是包括十分广泛的科学，它由不同的领域组成，如机器学习，计算机视觉等等，总的说来，人工智能研究的一个主要目标是使机器能够胜任一些通常需要人类智能才能完成的复杂工作。但不同的时代、不同的人对这种“复杂工作”的理解是不同的。2017年12月，人工智能入选“2017年度中国媒体十大流行语”。1.返回顶部1、中文名：人工智能外文名：ARTIFICIAL INTELLIGENCE简    称：AI提出时间：1956年提出地点：DARTMOUTH学会名称来源：雨果·德·加里斯 的著作目录1 定义详解2 研究价值3 发展阶段4 科学介绍5 技术研究▪ 研究方法▪ 智能模拟▪ 学科范畴▪ 涉及学科▪ 研究范畴▪ 安全问题▪ 实现方法6 专业机构▪ 美国▪ 中国7 主要成果▪ 人机对弈▪ 模式识别▪ 自动工程▪ 知识工程8 相关著作9 发展简史▪ 计算机时代▪ 竞赛▪ 大量程序▪ 日常生活▪ 强弱对比10 研究课题▪ 解决问题▪ 知识表示法▪ 规划▪ 学习▪ 自然语言处理▪ 运动和控制▪ 知觉▪ 社交▪ 创造力▪ 多元YLB智能▪ 人工智能影响▪ 应用领域11 流行语2、2.返回顶部1、定义详解人工智能机器人人工智能的定义可以分为两部分，即“人工”和“智能”。“人工”比较好理解，争议性也不大。有时我们会要考虑什么是人力所能及制造的，或者人自身的智能程度有没有高到可以创造人工智能的地步，等等。但总的来说，“人工系统”就是通常意义下的人工系统。关于什么是“智能”，就问题多多了。这涉及到其它诸如意识（Consciousness）、自我（Self）、思维（Mind）（包括无意识的思维（Unconscious_Mind））等等问题。人唯一了解的智能是人本身的智能，这是普遍认同的观点。但是我们对我们自身智能的理解都非常有限，对构成人的智能的必要元素也了解有限，所以就很难定义什么是“人工”制造的“智能”了。因此人工智能的研究往往涉及对人的智能本身的研究。其它关于动物或其它人造系统的智能也普遍被认为是人工智能相关的研究课题。人工智能在计算机领域内，得到了愈加广泛的重视。并在机器人，经济政治决策，控制系统，仿真系统中得到应用。尼尔逊教授对人工智能下了这样一个定义：“人工智能是关于知识的学科――怎样表示知识以及怎样获得知识并使用知识的科学。”而另一个美国麻省理工学院的温斯顿教授认为：“人工智能就是研究如何使计算机去做过去只有人才能做的智能工作。”这些说法反映了人工智能学科的基本思想和基本内容。即人工智能是研究人类智能活动的规律，构造具有一定智能的人工系统，研究如何让计算机去完成以往需要人的智力才能胜任的工作，也就是研究如何应用计算机的软硬件来模拟人类某些智能行为的基本理论、方法和技术。人工智能是计算机学科的一个分支，二十世纪七十年代以来被称为世界三大尖端技术之一（空间技术、能源技术、人工智能）。也被认为是二十一世纪三大尖端技术（基因工程、纳米科学、人工智能）之一。这是因为近三十年来它获得了迅速的发展，在很多学科领域都获得了广泛应用，并取得了丰硕的成果，人工智能已逐步成为一个独立的分支，无论在理论和实践上都已自成一个系统。人工智能是研究使计算机来模拟人的某些思维过程和智能行为（如学习、推理、思考、规划等）的学科，主要包括计算机实现智能的原理、制造类似于人脑智能的计算机，使计算机能实现更高层次的应用。人工智能将涉及到计算机科学、心理学、哲学和语言学等学科。可以说几乎是自然科学和社会科学的所有学科，其范围已远远超出了计算机科学的范畴，人工智能与思维科学的关系是实践和理论的关系，人工智能是处于思维科学的技术应用层次，是它的一个应用分支。从思维观点看，人工智能不仅限于逻辑思维，要考虑形象思维、灵感思维才能促进人工智能的突破性的发展，数学常被认为是多种学科的基础科学，数学也进入语言、思维领域，人工智能学科也必须借用数学工具，数学不仅在标准逻辑、模糊数学等范围发挥作用，数学进入人工智能学科，它们将互相促进而更快地发展。研究价值具有人工智能的机器人例如繁重的科学和工程计算本来是要人脑来承担的，如今计算机不但能完成这种计算，而且能够比人脑做得更快、更准确，因此当代人已不再把这种计算看作是“需要人类智能才能完成的复杂任务”，可见复杂工作的定义是随着时代的发展和技术的进步而变化的，人工智能这门科学的具体目标也自然随着时代的变化而发展。它一方面不断获得新的进展，另一方面又转向更有意义、更加困难的目标。通常，“机器学习”的数学基础是“统计学”、“信息论”和“控制论”。还包括其他非数学学科。这类“机器学习”对“经验”的依赖性很强。计算机需要不断从解决一类问题的经验中获取知识，学习策略，在遇到类似的问题时，运用经验知识解决问题并积累新的经验，就像普通人一样。我们可以将这样的学习方式称之为“连续型学习”。但人类除了会从经验中学习之外，还会创造，即“跳跃型学习”。这在某些情形下被称为“灵感”或“顿悟”。一直以来，计算机最难学会的就是“顿悟”。或者再严格一些来说，计算机在学习和“实践”方面难以学会“不依赖于量变的质变”，很难从一种“质”直接到另一种“质”，或者从一个“概念”直接到另一个“概念”。正因为如此，这里的“实践”并非同人类一样的实践。人类的实践过程同时包括经验和创造。这是智能化研究者梦寐以求的东西。2013年，帝金数据普数中心数据研究员S.C WANG开发了一种新的数据分析方法，该方法导出了研究函数性质的新方法。作者发现，新数据分析方法给计算机学会“创造”提供了一种方法。本质上，这种方法为人的“创造力”的模式化提供了一种相当有效的途径。这种途径是数学赋予的，是普通人无法拥有但计算机可以拥有的“能力”。从此，计算机不仅精于算，还会因精于算而精于创造。计算机学家们应该斩钉截铁地剥夺“精于创造”的计算机过于全面的操作能力，否则计算机真的有一天会“反捕”人类。当回头审视新方法的推演过程和数学的时候，作者拓展了对思维和数学的认识。数学简洁，清晰，可靠性、模式化强。在数学的发展史上，处处闪耀着数学大师们创造力的光辉。这些创造力以各种数学定理或结论的方式呈现出来，而数学定理最大的特点就是：建立在一些基本的概念和公理上，以模式化的语言方式表达出来的包含丰富信息的逻辑结构。应该说，数学是最单纯、最直白地反映着（至少一类）创造力模式的学科。发展阶段1956年夏季，以麦卡赛、明斯基、罗切斯特和申农等为首的一批有远见卓识的年轻科学家在一起聚会，共同研究和探讨用机器模拟智能的一系列有关问题，并首次提出了“人工智能”这一术语，它标志着“人工智能”这门新兴学科的正式诞生。IBM公司“深蓝”电脑击败了人类的世界国际象棋冠军更是人工智能技术的一个完美表现。从1956年正式提出人工智能学科算起，50多年来，取得长足的发展，成为一门广泛的交叉和前沿科学。总的说来，人工智能的目的就是让计算机这台机器能够像人一样思考。如果希望做出一台能够思考的机器，那就必须知道什么是思考，更进一步讲就是什么是智慧。什么样的机器才是智慧的呢？科学家已经作出了汽车，火车，飞机，收音机等等，它们模仿我们身体器官的功能，但是能不能模仿人类大脑的功能呢？到目前为止，我们也仅仅知道这个装在我们天灵盖里面的东西是由数十亿个神经细胞组成的器官，我们对这个东西知之甚少，模仿它或许是天下最困难的事情了。当计算机出现后，人类开始真正有了一个可以模拟人类思维的工具，在以后的岁月中，无数科学家为这个目标努力着。如今人工智能已经不再是几个科学家的专利了，全世界几乎所有大学的计算机系都有人在研究这门学科，学习计算机的大学生也必须学习这样一门课程，在大家不懈的努力下，如今计算机似乎已经变得十分聪明了。例如，1997年5月，IBM公司研制的深蓝（DEEP BLUE）计算机战胜了国际象棋大师卡斯帕洛夫（KASPAROV）。大家或许不会注意到，在一些地方计算机帮助人进行其它原来只属于人类的工作，计算机以它的高速和准确为人类发挥着它的作用。人工智能始终是计算机科学的前沿学科，计算机编程语言和其它计算机软件都因为有了人工智能的进展而得以存在。2019年3月4日，十三届全国人大二次会议举行新闻发布会，大会发言人张业遂表示，已将与人工智能密切相关的立法项目列入立法规划 。科学介绍实际应用机器视觉，指纹识别，人脸识别，视网膜识别，虹膜识别，掌纹识别，专家系统，自动规划，智能搜索，定理证明，博弈，自动程序设计，智能控制，机器人学，语言和图像理解，遗传编程等。学科范畴人工智能是一门边缘学科，属于自然科学和社会科学的交叉。涉及学科哲学和认知科学，数学，神经生理学，心理学，计算机科学，信息论，控制论，不定性论研究范畴自然语言处理，知识表现，智能搜索，推理，规划，机器学习，知识获取，组合调度问题，感知问题，模式识别，逻辑程序设计软计算，不精确和不确定的管理，人工生命，神经网络，复杂系统，遗传算法意识和人工智能人工智能就其本质而言，是对人的思维的信息过程的模拟。对于人的思维模拟可以从两条道路进行，一是结构模拟，仿照人脑的结构机制，制造出“类人脑”的机器；二是功能模拟，暂时撇开人脑的内部结构，而从其功能过程进行模拟。现代电子计算机的产生便是对人脑思维功能的模拟，是对人脑思维的信息过程的模拟。弱人工智能如今不断地迅猛发展，尤其是2008年经济危机后，美日欧希望借机器人等实现再工业化，工业机器人以比以往任何时候更快的速度发展，更加带动了弱人工智能和相关领域产业的不断突破，很多必须用人来做的工作如今已经能用机器人实现。而强人工智能则暂时处于瓶颈，还需要科学家们和人类的努力。技术研究用来研究人工智能的主要物质基础以及能够实现人工智能技术平台的机器就是计算机，人工智能的发展历史是和计算机科学技术的发展史联系在一起的。除了计算机科学以外，人工智能还涉及信息论、控制论、自动化、仿生学、生物学、心理学、数理逻辑、语言学、医学和哲学等多门学科。人工智能学科研究的主要内容包括：知识表示、自动推理和搜索方法、机器学习和知识获取、知识处理系统、自然语言理解、计算机视觉、智能机器人、自动程序设计等方面。研究方法如今没有统一的原理或范式指导人工智能研究。许多问题上研究者都存在争论。其中几个长久以来仍没有结论的问题是：是否应从心理或神经方面模拟人工智能?或者像鸟类生物学对于航空工程一样，人类生物学对于人工智能研究是没有关系的？智能行为能否用简单的原则（如逻辑或优化）来描述？还是必须解决大量完全无关的问题？智能是否可以使用高级符号表达，如词和想法？还是需要“子符号”的处理？JOHN HAUGELAND提出了GOFAI(出色的老式人工智能)的概念，也提议人工智能应归类为SYNTHETIC INTELLIGENCE，[29]这个概念后来被某些非GOFAI研究者采纳。大脑模拟主条目：控制论和计算神经科学20世纪40年代到50年代，许多研究者探索神经病学，信息理论及控制论之间的联系。其中还造出一些使用电子网络构造的初步智能，如W. GREY WALTER的TURTLES和JOHNS HOPKINS BEAST。 这些研究者还经常在普林斯顿大学和英国的RATIO CLUB举行技术协会会议.直到1960， 大部分人已经放弃这个方法，尽管在80年代再次提出这些原理。符号处理主条目：GOFAI当20世纪50年代，数字计算机研制成功，研究者开始探索人类智能是否能简化成符号处理。研究主要集中在卡内基梅隆大学， 斯坦福大学和麻省理工学院，而各自有独立的研究风格。JOHN HAUGELAND称这些方法为GOFAI(出色的老式人工智能)。60年代，符号方法在小型证明程序上模拟高级思考有很大的成就。基于控制论或神经网络的方法则置于次要。60~70年代的研究者确信符号方法最终可以成功创造强人工智能的机器，同时这也是他们的目标。认知模拟经济学家赫伯特·西蒙和艾伦·纽厄尔研究人类问题解决能力和尝试将其形式化，同时他们为人工智能的基本原理打下基础，如认知科学， 运筹学和经营科学。他们的研究团队使用心理学实验的结果开发模拟人类解决问题方法的程序。这方法一直在卡内基梅隆大学沿袭下来，并在80年代于SOAR发展到高峰。基于逻辑不像艾伦·纽厄尔和赫伯特·西蒙，JOHN MCCARTHY认为机器不需要模拟人类的思想，而应尝试找到抽象推理和解决问题的本质，不管人们是否使用同样的算法。他在斯坦福大学的实验室致力于使用形式化逻辑解决多种问题，包括知识表示， 智能规划和机器学习. 致力于逻辑方法的还有爱丁堡大学，而促成欧洲的其他地方开发编程语言PROLOG和逻辑编程科学.“反逻辑”斯坦福大学的研究者 (如马文·闵斯基和西摩尔·派普特)发现要解决计算机视觉和自然语言处理的困难问题，需要专门的方案-他们主张不存在简单和通用原理（如逻辑）能够达到所有的智能行为。ROGER SCHANK 描述他们的“反逻辑”方法为 \"SCRUFFY\" .常识知识库 (如DOUG LENAT的CYC)就是\"SCRUFFY\"AI的例子，因为他们必须人工一次编写一个复杂的概念。基于知识大约在1970年出现大容量内存计算机，研究者分别以三个方法开始把知识构造成应用软件。这场“知识革命”促成专家系统的开发与计划，这是第一个成功的人工智能软件形式。“知识革命”同时让人们意识到许多简单的人工智能软件可能需要大量的知识。子符号法80年代符号人工智能停滞不前，很多人认为符号系统永远不可能模仿人类所有的认知过程，特别是感知，机器人，机器学习和模式识别。很多研究者开始关注子符号方法解决特定的人工智能问题。自下而上， 接口AGENT，嵌入环境（机器人），行为主义，新式AI机器人领域相关的研究者，如RODNEY BROOKS，否定符号人工智能而专注于机器人移动和求生等基本的工程问题。他们的工作再次关注早期控制论研究者的观点，同时提出了在人工智能中使用控制理论。这与认知科学领域中的表征感知论点是一致的:更高的智能需要个体的表征(如移动，感知和形象)。计算智能80年代中DAVID RUMELHART 等再次提出神经网络和联结主义. 这和其他的子符号方法，如模糊控制和进化计算，都属于计算智能学科研究范畴。统计学法90年代，人工智能研究发展出复杂的数学工具来解决特定的分支问题。这些工具是真正的科学方法，即这些方法的结果是可测量的和可验证的，同时也是人工智能成功的原因。共用的数学语言也允许已有学科的合作（如数学，经济或运筹学）。STUART J. RUSSELL和PETER NORVIG指出这些进步不亚于“革命”和“NEATS的成功”。有人批评这些技术太专注于特定的问题，而没有考虑长远的强人工智能目标。集成方法智能AGENT范式智能AGENT是一个会感知环境并作出行动以达致目标的系统。最简单的智能AGENT是那些可以解决特定问题的程序。更复杂的AGENT包括人类和人类组织（如公司）。这些范式可以让研究者研究单独的问题和找出有用且可验证的方案，而不需考虑单一的方法。一个解决特定问题的AGENT可以使用任何可行的方法-一些AGENT用符号方法和逻辑方法，一些则是子符号神经网络或其他新的方法。范式同时也给研究者提供一个与其他领域沟通的共同语言--如决策论和经济学（也使用ABSTRACT AGENTS的概念）。90年代智能AGENT范式被广泛接受。AGENT体系结构和认知体系结构研究者设计出一些系统来处理多ANGENT系统中智能AGENT之间的相互作用。一个系统中包含符号和子符号部分的系统称为混合智能系统 ，而对这种系统的研究则是人工智能系统集成。分级控制系统则给反应级别的子符号AI 和最高级别的传统符号AI提供桥梁，同时放宽了规划和世界建模的时间。RODNEY BROOKS的SUBSUMPTION ARCHITECTURE就是一个早期的分级系统计划。智能模拟机器视、听、触、感觉及思维方式的模拟：指纹识别，人脸识别，视网膜识别，虹膜识别，掌纹识别，专家系统，智能搜索，定理证明，逻辑推理，博弈，信息感应与辨证处理。学科范畴人工智能是一门边沿学科，属于自然科学、社会科学、技术科学三向交叉学科。涉及学科哲学和认知科学，数学，神经生理学，心理学，计算机科学，信息论，控制论，不定性论，仿生学，社会结构学与科学发展观。研究范畴语言的学习与处理，知识表现，智能搜索，推理，规划，机器学习，知识获取，组合调度问题，感知问题，模式识别，逻辑程序设计，软计算，不精确和不确定的管理，人工生命，神经网络，复杂系统，遗传算法人类思维方式，最关键的难题还是机器的自主创造性思维能力的塑造与提升。安全问题人工智能还在研究中，但有学者认为让计算机拥有智商是很危险的，它可能会反抗人类。这种隐患也在多部电影中发生过，其主要的关键是允不允许机器拥有自主意识的产生与延续，如果使机器拥有自主意识，则意味着机器具有与人同等或类似的创造性，自我保护意识，情感和自发行为。实现方法人工智能在计算机上实现时有2种不同的方式。一种是采用传统的编程技术，使系统呈现智能的效果，而不考虑所用方法是否与人或动物机体所用的方法相同。这种方法叫工程学方法（ENGINEERING APPROACH），它已在一些领域内作出了成果，如文字识别、电脑下棋等。另一种是模拟法（MODELING APPROACH），它不仅要看效果，还要求实现方法也和人类或生物机体所用的方法相同或相类似。遗传算法（GENERIC ALGORITHM，简称GA）和人工神经网络（ARTIFICIAL NEURAL NETWORK，简称ANN）均属后一类型。遗传算法模拟人类或生物的遗传-进化机制，人工神经网络则是模拟人类或动物大脑中神经细胞的活动方式。为了得到相同智能效果，两种方式通常都可使用。采用前一种方法，需要人工详细规定程序逻辑，如果游戏简单，还是方便的。如果游戏复杂，角色数量和活动空间增加，相应的逻辑就会很复杂（按指数式增长），人工编程就非常繁琐，容易出错。而一旦出错，就必须修改原程序，重新编译、调试，最后为用户提供一个新的版本或提供一个新补丁，非常麻烦。采用后一种方法时，编程者要为每一角色设计一个智能系统（一个模块）来进行控制，这个智能系统（模块）开始什么也不懂，就像初生婴儿那样，但它能够学习，能渐渐地适应环境，应付各种复杂情况。这种系统开始也常犯错误，但它能吸取教训，下一次运行时就可能改正，至少不会永远错下去，用不到发布新版本或打补丁。利用这种方法来实现人工智能，要求编程者具有生物学的思考方法，入门难度大一点。但一旦入了门，就可得到广泛应用。由于这种方法编程时无须对角色的活动规律做详细规定，应用于复杂问题，通常会比前一种方法更省力。专业机构美国⒈ MASSACHUSETTS INSTITUTE OF TECHNOLOGY麻省理工学院⒉ STANFORD UNIVERSITY斯坦福大学(CA)⒊ CARNEGIE MELLON UNIVERSITY卡内基美隆大学(PA)⒋ UNIVERSITY OF CALIFORNIA-BERKELEY加州大学伯克利分校⒌ UNIVERSITY OF WASHINGTON华盛顿大学⒍ UNIVERSITY OF TEXAS-AUSTIN德克萨斯大学奥斯汀分校⒎ UNIVERSITY OF PENNSYLVANIA宾夕法尼亚大学⒏ UNIVERSITY OF ILLINOIS-URBANA-CHAMPAIGN 伊利诺伊大学厄本那—香槟分校⒐ UNIVERSITY OF MARYLAND-COLLEGE PARK马里兰大学帕克分校⒑ CORNELL UNIVERSITY 康奈尔大学 (NY)⒒ UNIVERSITY OF MASSACHUSETTS-AMHERST马萨诸塞大学AMHERST校区⒓ GEORGIA INSTITUTE OF TECHNOLOGY佐治亚理工学院UNIVERSITY OF MICHIGAN-ANN ARBOR 密西根大学-安娜堡分校⒕ UNIVERSITY OF SOUTHERN CALIFORNIA南加州大学⒖ COLUMBIA UNIVERSITY哥伦比亚大学(NY)UNIVERSITY OF CALIFORNIA-LOS ANGELES加州大学洛杉矶分校⒘ BROWN UNIVERSITY布朗大学(RI)⒙ YALE UNIVERSITY耶鲁大学(CT)⒚ UNIVERSITY OF CALIFORNIA-SAN DIEGO加利福尼亚大学圣地亚哥分校⒛ UNIVERSITY OF WISCONSIN-MADISON威斯康星大学麦迪逊分校中国1、中国科学院自动化研究所2、清华大学3、北京大学4、南京理工大学5、北京科技大学6、中国科学技术大学7、吉林大学8、哈尔滨工业大学9、北京邮电大学10、北京理工大学11、厦门大学人工智能研究所12、西安交通大学智能车研究所13、中南大学智能系统与智能软件研究所14、西安电子科技大学智能所15、华中科技大学图像与人工智能研究所16、重庆邮电大学17、武汉工程大学主要成果人机对弈1996年2月10～17日， GARRY KASPAROV以4：2战胜“深蓝” （DEEP BLUE）。1997年5月3～11日， GARRY KASPAROV以2.5：3.5输于改进后的“深蓝”。2003年2月GARRY KASPAROV 3:3战平 “小深”（DEEP JUNIOR）。2003年11月GARRY KASPAROV 2:2战平 “X3D德国人” (X3D-FRITZ）。模式识别采用 $模式识别引擎，分支有2D识别引擎 ，3D识别引擎，驻波识别引擎以及多维识别引擎2D识别引擎已推出指纹识别，人像识别 ，文字识别，图像识别 ，车牌识别；驻波识别引擎已推出语音识别；3D识别引擎已推出指纹识别玉带林中挂（玩游智能版1.25）自动工程自动驾驶（OSO系统）印钞工厂（￥流水线）猎鹰系统（YOD绘图）知识工程以知识本身为处理对象，研究如何运用人工智能和软件技术，设计、构造和维护知识系统专家系统智能搜索引擎计算机视觉和图像处理机器翻译和自然语言理解数据挖掘和知识发现相关著作《视读人工智能》：机器真的可以思考吗？人的思维只是一个复杂的计算机程序吗？本书着眼于人工智能这个有史以来最为棘手的科学问题之一，集中探讨了其背后的一些主要话题。人工智能不仅仅是一个虚构的概念。人类对智能机体结构半个世纪的研究表明：机器可以打败人类最伟大的棋手，类人机器人可以走路并且能和人类进行互动。尽管早就有宣言称智能机器指日可待，但此方面的进展却缓慢而艰难。意识和环境是困扰研究的两大难题。我们到底应该怎样去制造智能机器呢？它应该像大脑一样运转？它是否需要躯体？从图灵影响深远的奠基性研究到机器人和新人工智能的飞跃，本书图文并茂的将人工智能在过去半个世纪的发展清晰的呈现在读者面前。《人工智能的未来》：诠释了智能的内涵，阐述了大脑工作的原理，并告诉我们如何才能制造出真正意义上的智能机器——这样的智能机器将不再仅仅是对人类大脑的简单模仿，它们的智能在许多方面会远远超过人脑。霍金斯认为，从人工智能到神经网络，早先复制人类智能的努力无一成功，究其原因，都是由于人们并未真正了解智能的内涵和人类大脑。所谓智能，就是人脑比较过去、预测未来的能力。大脑不是计算机，不会亦步亦趋、按部就班的根据输入产生输出。大脑是一个庞大的记忆系统，它储存着在某种程度上反映世界真实结构的经验，能够记忆事件的前后顺序及其相互关系，并依据记忆做出预测。形成智能、感觉、创造力以及知觉等基础的，就是大脑的记忆-预测系统……《人工智能哲学》：人工智能哲学是伴随现代信息理论和计算机技术发展起来的一个哲学分支。本书收集了人工智能研究领域学者的十五篇代表性论文，这些论文为计算机科学的发展和人工智能哲学的建立作出了开创性的贡献。这些文章总结了人工智能发展的历程，该学科发展的趋势，以及人工智能中的重要课题。在这些划时代的著作中，包括有：现代计算机理论之父艾伦·图灵的“计算机与智能”；美国哲学家塞尔的“心灵，大脑与程序”；J·E·欣顿等人的“分布式表述”，以及本书编者、英国人工智能学者M·A·博登的“逃出中文屋”。《人工智能：一种现代的方法》：本书以详尽和丰富的资料，从理性智能体的角度，全面阐述了人工智能领域的核心内容，并深入介绍了各个主要的研究方向，是一本难得的综合性教材。全书分为八大部分：第一部分\"人工智能\"，第二部分\"问题求解\"，第三部分\"知识与推理\"，第四部分\"规划\"，第五部分\"不确定知识与推理\"，第六部分\"学习\"，第七部分\"通讯、感知与行动\"，第八部分\"结论\"。本书既详细介绍了大量的基本概念、思想和算法，也描述了各研究方向最前沿的进展，同时收集整理了详实的历史文献与事件。因此本书适合于不同层次和领域的研究人员及学生，可以作为信息领域和相关领域的高等院校本科生和研究生的教材或教学辅导书目，也可以作为相关领域的科研与工程技术人员的参考书。研究课题人工智能的研究方向已经被分成几个子领域，研究人员希望一个人工智能系统应该具有某些特定能力，以下将这些能力列出并说明。解决问题早期的人工智能研究人员直接模仿人类进行逐步的推理，就像是玩棋盘游戏或进行逻辑推理时人类的思考模式。到了1980和1990年代，利用概率和经济学上的概念，人工智能研究还发展了非常成功的方法处理不确定或不完整的资讯。对于困难的问题，有可能需要大量的运算资源，也就是发生了“可能组合爆增”：当问题超过一定的规模时，电脑会需要天文数量级的存储器或是运算时间。寻找更有效的算法是优先的人工智能研究项目。人类解决问题的模式通常是用最快捷，直观的判断，而不是有意识的，一步一步的推导，早期人工智能研究通常使用逐步推导的方式。人工智能研究已经于这种“次表征性的”解决问题方法取得进展：实体化AGENT研究强调感知运动的重要性。神经网络研究试图以模拟人类和动物的大脑结构重现这种技能。知识表示法AN ONTOLOGY REPRESENTS KNOWLEDGE AS A SET OF CONCEPTS WITHIN A DOMAIN AND THE RELATIONSHIPS BETWEEN THOSE CONCEPTS.主条目：知识表示和常识知识库规划智能AGENT必须能够制定目标和实现这些目标。他们需要一种方法来建立一个可预测的世界模型(将整个世界状态用数学模型表现出来，并能预测它们的行为将如何改变这个世界)，这样就可以选择功效最大的行为。 在传统的规划问题中，智能AGENT被假定它是世界中唯一具有影响力的，所以它要做出什么行为是已经确定的。 但是，如果事实并非如此，它必须定期检查世界模型的状态是否和自己的预测相符合。如果不符合，它必须改变它的计划。因此智能代理必须具有在不确定结果的状态下推理的能力。 在多AGENT中，多个AGENT规划以合作和竞争的方式去完成一定的目标，使用演化算法和群体智慧可以达成一个整体的突现行为目标。学习主条目：机器学习机械学习的主要目的是为了从使用者和输入数据等处获得知识，从而可以帮助解决更多问题，减少错误，提高解决问题的效率。对于人工智能来说，机械学习从一开始就很重要。1956年，在最初的达特茅斯夏季会议上，雷蒙德索洛莫诺夫写了一篇关于不监视的概率性机械学习：一个归纳推理的机械。自然语言处理主条目：自然语言处理运动和控制主条目：机器人学知觉主条目：机器感知、计算机视觉和语音识别机器感知 是指能够使用传感器所输入的资料(如照相机，麦克风，声纳以及其他的特殊传感器)然后推断世界的状态。计算机视觉能够分析影像输入。另外还有语音识别 、人脸辨识和物体辨识。社交主条目：情感计算KISMET, 一个具有表情等社交能力的机器人情感和社交技能对于一个智能AGENT是很重要的。 首先，通过了解他们的动机和情感状态，代理人能够预测别人的行动(这涉及要素 博弈论、决策理论以及能够塑造人的情感和情绪感知能力检测)。此外，为了良好的人机互动，智慧代理人也需要表现出情绪来。至少它必须出现礼貌地和人类打交道。至少，它本身应该有正常的情绪。创造力主条目：计算机创造力一个人工智能的子领域，代表了理论(从哲学和心理学的角度)和实际(通过特定的实现产生的系统的输出是可以考虑的创意，或系统识别和评估创造力)所定义的创造力。 相关领域研究的包括了人工直觉和人工想像。多元YLB智能大多数研究人员希望他们的研究最终将被纳入一个具有多元YLB智能(称为强人工智能)，结合以上所有的技能并且超越大部分人类的能力。 有些人认为要达成以上目标，可能需要拟人化的特性，如人工意识或人工大脑。 上述许多问题被认为是人工智能完整性：为了解决其中一个问题，你必须解决全部的问题。即使一个简单和特定的任务，如机器翻译，要求机器按照作者的论点(推理)，知道什么是被人谈论(知识)，忠实地再现作者的意图(情感计算)。因此，机器翻译被认为是具有人工智能完整性：它可能需要强人工智能，就像是人类一样。人工智能影响（1）人工智能对自然科学的影响。在需要使用数学计算机工具解决问题的学科，AI带来的帮助不言而喻。更重要的是，AI反过来有助于人类最终认识自身智能的形成。（2）人工智能对经济的影响。专家系统更深入各行各业，带来巨大的宏观效益。AI也促进了计算机工业网络工业的发展。但同时，也带来了劳务就业问题。由于AI在科技和工程中的应用，能够代替人类进行各种技术工作和脑力劳动，会造成社会结构的剧烈变化。（3）人工智能对社会的影响。AI也为人类文化生活提供了新的模式。现有的游戏将逐步发展为更高智能的交互式文化娱乐手段，今天，游戏中的人工智能应用已经深入到各大游戏制造商的开发中。伴随着人工智能和智能机器人的发展，不得不讨论是人工智能本身就是超前研究，需要用未来的眼光开展现代的科研，因此很可能触及伦理底线。作为科学研究可能涉及到的敏感问题，需要针对可能产生的冲突及早预防，而不是等到问题矛盾到了不可解决的时候才去想办法化解。应用领域机器翻译，智能控制，专家系统，机器人学，语言和图像理解，遗传编程机器人工厂，自动程序设计，航天应用，庞大的信息处理，储存与管理，执行化合生命体无法执行的或复杂或规模庞大的任务等等。值得一提的是，机器翻译是人工智能的重要分支和最先应用领域。不过就已有的机译成就来看，机译系统的译文质量离终极目标仍相差甚远；而机译质量是机译系统成败的关键。中国数学家、语言学家周海中教授曾在论文《机器翻译五十年》中指出：要提高机译的质量，首先要解决的是语言本身问题而不是程序设计问题；单靠若干程序来做机译系统，肯定是无法提高机译质量的；另外在人类尚未明了大脑是如何进行语言的模糊识别和逻辑判断的情况下，机译要想达到“信、达、雅”的程度是不可能的。智能家居之后，人工智能成为家电业的新风口，而长虹正成为将这一浪潮掀起的首个家电巨头。长虹发布两款CHiQ智能电视新品，主打手机遥控器、带走看、随时看、分类看功能。流行语2017年12月，人工智能入选“2017年度中国媒体十大流行语”。入选理由：经过多年的演进，人工智能发展进入了新阶段。为抢抓人工智能发展的重大战略机遇，构筑我国人工智能发展的先发优势，加快建设创新型国家和世界科技强国，2017年7月20日，国务院印发了《新一代人工智能发展规划》。《规划》提出了面向2030年我国新一代人工智能发展的指导思想、战略目标、重点任务和保障措施，为我国人工智能的进一步加速发展奠定了重要基础。2、3.返回顶部4.返回顶部5.返回顶部1、https://baike.baidu.com/item/人工智能/91802、6.返回顶部作者：ylbtech出处：http://ylbtech.cnblogs.com/本文版权归作者和博客园共有，欢迎转载，但未经作者同意必须保留此段声明，且在文章页面明显位置给出原文连接，否则保留追究法律责任的权利。"}
{"content2":"计算机视觉之一：特征检测主要内容：1、一个例子解释为什么要进行特征检测2、图像特征3、点特征检测：Harris角点、MOPS、SIFT4、边缘检测：一阶微分算子、二阶微分算子、Canny算子一、为什么要检测特征？举一个例子：全景图像拼接，给定两张图像，如何拼接成一张大图？步骤一：检测特征点步骤二：匹配特征点步骤三：图像适配二、图像的特征有哪些？计算机视觉中常用的图像特征包括：点、边缘、直线、曲线等三、点特征检测1、点特征的优势：点特征属于局部特征，对遮挡有一定鲁棒性；通常图像中可以检测到成百上千的点特征，以量取胜；点特征有较好的辨识性，不同物体上的点容易区分；点特征提取通常速度很快；2、什么是好的点特征？考虑图像上的一个小窗口，当窗口位置发生微小变化时，窗口图像如何变化？很明显，从下图可以看出，角点是一个好的点特征，因为它沿任意方向移动，窗口的灰度变化明显，所以它可以作为一个特征来进行区分和辨别。3、点特征的数学表达假设窗口W发生位置偏移(u,v);比较偏移前后窗口中每一个像素点的灰度变化值；使用误差平方和定义误差函数E(u,v)不同位置点计算得到的E(u,v)如下图所示，E(u,v)值随着u、v变化的效果图，可以看出，平坦区域（如天空）的灰度变化不大（下），边缘区域沿着边缘方向的灰度变化.也很小（下），只有角点处的灰度变化稍微剧烈一点（下）误差函数E(u,v)：将I(u,v)进行Taylor展开：进一步展开，写成：H称为自相关矩阵，和是H的2个特征值，E(u,v)的变化如下图所示：根据H的2个特征值大小对图像点进行分类：角点应该满足的基本性质：最小特征值尽量大角点响应：比更有效的角点响应函数：四、点特征检测：Harris角点算法步骤：将原图像I使用w(x,y)进行卷积，并计算图像梯度Ix与Iy；计算每个图像点的自相关矩阵H；计算角点响应；选择R大于阈值且为局部极大值的点作为角点。Harris角点改进：Harris检测子获得的角点可能在图像上分布不均匀（对比度高的区域角点多）改进方法：Adaptive non-maximal suppression（ANMS），只保留半径r内角点响应比其他点大10%的点作为角点。（Brown，Szeliski and Winder，2005）Harris角点的性质：1.   旋转不变：椭圆转过一定角度但是其形状保持不变（特征值保持不变）2.   光照变化不变：只使用了图像导数，对于光照线性变化不变3.   对比度变化部分不变：4.   对于图像尺度变化不具有不变性：五、点特征检测：MOPSMOPS：Multi-scale oriented patches尺度不变：在多层图像金字塔上检测角点，在同一层进行匹配MOPS局限：待匹配的图像需要尺度近似六、点特征检测：SIFTScale Invariant Feature Transform （SIFT）（Lowe，2004）主要内容：1.SIFT算法特点2.SIFT算法流程3.SIFT算法的具体步骤4.SIFT点的特点1、SIFT算法的特点：不变性：——对图像的旋转和尺度变化具有不变性；——对三维视角变化和光照变化具有很强的适应性；——局部特征，在遮挡和场景杂乱时仍保持不变性；辨别力强：——特征之间相互区分的能力强，有利于匹配；数量较多：——一般500*500的图像能提取约2000个特征点。2、SIFT算法的流程：在高斯差分（Difference of Gaussian，DOG）尺度空间中提取极值点并进行优化，从而获取特征点。3、SIFT算法点检测的具体步骤：——构建尺度空间；——构造高斯差分尺度空间；——DoG尺度空间极值点检测；——特征点精确定位；——去除不稳定点；构建尺度空间：模拟图像数据的多尺度特征其中是尺度可变高斯函数尺度参数决定图像的平滑程度，大尺度对应图像的概貌特征，小尺度对应图像的细节特征构造高斯差分尺度空间（Difference of Gaussian，DOG）为了在尺度空间中检测稳定的关键点，构造高斯差分尺度空间使用DOG的几个理由：1.   计算效率高：高斯卷积，减法；2.   高斯差分是对尺度归一化LoG的一个很好的近似，而尺度归一化的LoG空间具有真正的尺度不变性（Lindegerg 1994）；3.  实验比较表明，从尺度归一化LoG空间中提取的图像特征的尺度稳定性最好，优于梯度、Hessian或Harris角点函数。DoG尺度空间：DoG尺度空间极值点检测：一个点和它同尺度的8个相邻点以及上下相邻尺度对应的9×2个点共26个点比较，以确保在尺度空间和二维图像空间都检测到极值点。一个点如果在DOG尺度空间的26个领域中是最大或最小值时，就认为该点是图像在该尺度下的一个特征点。一个特征点是在三维尺度空间的局部极值点。特征点精确定位：一个特征点是在三维尺度空间的局部极值点。但(x,y)为整数像素，为离散尺度，需要对DoG空间拟合进行特征点精确定位。将的特征点处二阶Taylor展开：对上式求导，并令其为0，得到精确的位置（偏移量）：若中的三个变量任意一个偏移量大于0.5，说明精确极值点更接近于另一个特征点，则更换特征点重复上述精确定位流程。去除不稳定特征点：——去除对比度低的点：计算极值点取值，若，则保留该特征点，否则丢弃。——去除边缘点：DoG算子会产生较强的边缘响应，利用Harris检测子判断。若，则保留该特征点，否则丢弃。4、SIFT点的特点：视角和旋转变化不变性：光照不变性：尺度不变性：七、点特征检测：小结根据自相关矩阵特征值检测角点(Harris)； Harris角点具有旋转、光照不变性，但不具有尺度不变性。高斯差分尺度空间中检测尺度不变特征点(SIFT)； SIFT具有尺度不变性。Harris与SIFT的机理不同，因此可以联合使用，互为补充。八、边缘检测主要内容：1.图像梯度2.一阶微分算子3.二阶微分算子4.Canny算子1、为什么要检测边缘？一个例子：基于边缘的图像编辑2、图像边缘的产生物体的边界、表面方向的改变、不同的颜色、光照明暗的变化3、边缘检测——图像梯度图像梯度的定义：水平梯度：垂直梯度：图像梯度指向灰度变化最快的方向：梯度幅值表示边缘的强弱：边缘是一阶倒数的极大值点：4、边缘检测——一阶微分算子使用差分近似一阶微分算子：直接对图像使用差分容易受到噪声影响：解决方法：先平滑，再微分由，可将平滑和微分合为一个算子二维高斯微分：实际应用中：对二维高斯微分进行数值近似：Prewitt算子、Sobel算子Prewitt算子：去噪+ 增强边缘Sobel算子：去噪+ 增强边缘（给四邻域更大的权重）使用一阶微分算子提取边缘流程：1. 使用Prewitt或Sobel算子对图像进行卷积；2. 将梯度幅值大于阈值的点标记为边缘；3. (optional)将边缘细化为一个像素宽度。5、边缘检测——二阶微分算子如果不使用细化，如何获得单像素宽度边缘？边缘是一阶倒数的极大值点边缘是二阶倒数的过零点注意：仅仅等于0不够，常数函数也为0，必须存在符号改变对平滑图像做二阶微分：二维高斯微分：Laplacian of Gaussian (LoG)算子：首先用Gauss函数对图像进行平滑，抑制噪声，然后对经过平滑的图像使用Laplacian算子LoG算子等效于：Gaussian平滑+ Laplacian二阶微分LoG因其形状，也称为Mexican hatLoG算子与一阶微分算子的比较：LoG算子的特点：• 正确检测到的边缘：单像素宽度，定位准确；• 形成许多封闭的轮廓，这是一个主要问题；• 需要更加复杂的算法检测过零点。6、边缘检测——Canny算子Canny算子是最常用的边缘检测算子Canny算子是一阶微分算子，但是一个优化的方案-单像素宽度-噪声抑制-边缘增强-边缘定位J.Canny, “A Computational Approach to Edge Detection”, IEEE Trans. on PAMI, 8(6),1986.18482 citesCanny算子基本流程（1）高斯平滑滤波器卷积（2）使用一阶有限差分计算偏导数的两个阵列相当于与模板进行卷积运算：当然也可以使用高斯微分算子(Prewitt或Sobel)直接与卷积计算和（3）边缘幅值和边缘方位角M代表梯度幅值的大小，在存在边缘的图像位置处，M的值变大，图像的边缘特征被“增强”。（4）梯度非极大值抑制局部极值周围存在相近数值的点：非极大值抑制（NMS：Non-Maxima Suppression）主要思想：由梯度幅值图像，仅保留极大值（严格地说，保留梯度方向上的极大值点）。具体过程：1.初始化；2.对于每个点，在梯度方向和反梯度方向各找n个像素点。若不是这些点中的最大点，则将置零，否则保持不变。在梯度方向的沿线上检测该点是否为局部极大值；简化的情形，只使用4个方向：【0,45,90,135】；得到的结果包含边缘的宽度为1个像素；（5）对NMS结果进行阈值二值化—双阈值检测使用大的阈值，得到：-少量的边缘点-许多空隙使用小的阈值，得到：-大量的边缘点-大量的错误检测两个阈值T1，T2：T2 >> T1由T1得到，低阈值边缘图：更大的误检测率由T2得到，高阈值边缘图：更加可靠（6）边缘连接1. 将中相连的边缘点输出为一幅边缘图像；2. 对于中每条边，从端点出发在中寻找其延长的部分，直至与中另外一条边的端点相连，否则认为中没有它延长的部分；3. 将作为结果输出。Canny算子流程效果图：• Canny算子的优点-参数较少-计算效率-得到的边缘连续完整• 参数的选择-Gauss滤波的尺度-双阈值的选择(LOW=HIGH*0.4)Canny算子的处理效果：九、总结• 根据自相关矩阵特征值检测角点(Harris)；• 高斯差分尺度空间中检测尺度不变特征点(SIFT)；• 一阶高斯微分算子(Prewitt、Sobel)极值检测边缘；• 二阶高斯微分算子(LoG)过零点检测边缘；• 非极大值抑制+双阈值检测边缘(Canny)。十、特征检测参考文献• Harris, C. and Stephens, M. J. A combined corner and edge detector. In Alvey Vision Conference, 1988.• Lowe, D. G. Distinctive image features from scale-invariant keypoints. International Journal of Computer Vision, 60(2):91–110, 2004.• Canny, J. A computational approach to edge detection. IEEE Transactions onPAMI, 8(6):679–698, 1986.• Tuytelaars, T. and Mikolajczyk, K. Local Invariant Feature Detectors: A Survey. Foundations and Trends in Computer Graphics and Vision, 3(3): 177–280, 2007."}
{"content2":"1、   你认为什么是人工智能？人工智能，英文缩写为AI。它是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。 人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。人工智能是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。人工智能从诞生以来，理论和技术日益成熟，应用领域也不断扩大，但没有一个统一的定义。人工智能是对人的意识、思维的信息过程的模拟。人工智能不是人的智能，但能像人那样思考、也可能超过人的智能。但是这种会自我思考的高级人工智能还需要科学理论和工程上的突破。人工智能是一门极富挑战性的科学，从事这项工作的人必须懂得计算机知识，心理学和哲学。人工智能是包括十分广泛的科学，它由不同的领域组成，如机器学习，计算机视觉等等，总的说来，人工智能研究的一个主要目标是使机器能够胜任一些通常需要人类智能才能完成的复杂工作。但不同的时代、不同的人对这种“复杂工作”的理解是不同的。工智能的定义可以分为两部分，即“人工”和“智能”。“人工”比较好理解，争议性也不大。有时我们会要考虑什么是人力所能及制造的，或者人自身的智能程度有没有高到可以创造人工智能的地步，等等。但总的来说，“人工系统”就是通常意义下的人工系统。2、   简述推理、学习、存储，三者之间的联系！答：首先，以人类的固有思维来思考或是看待问题，初步是对问题有一定的猜测或是推敲性，即对我们遇到的问题具有一定的前瞻性。其次，是在解决问题的实践中学会总结与认知，全面分析自己的遗漏点，以求能够在实践中积累、在实践中成长。最后，就是对学习到的理论还是方法进行系统性的积累，存储起来，待下次遇到该类问题时可以迅速的得出最佳的解决方案。即可以得出推理、学习、存储三者之间相互制约也相互关联，由级联的方式影响着下一阶段的进行。3、   “警卫和囚犯”问题的过河方案，使用语义网络进行问题求解。模仿示例画出你的求解方案，并给出一共需要多少步可以成功过河？初始状态     1 1 12 2 2第1步：把一个1和一个2运到右岸1 1                12 2                2第2步：右岸的1回到左岸1 1 12 2                2第3步：左岸两个2乘船到右岸1 1 12 2 2第4步：右岸一个2乘船到左岸1 1 12                  2 2第5步：左岸两个以乘船到右岸1                   1 12                   2 2第6步：右岸一个1和一个2回到左岸1 1                   12 2                  2第7步：左岸两个1到右岸1 1 12 2                   2第8步：右岸一个2回到左岸1 1 12 2 2第9步：左岸两个2到右岸1 1 12                        2 2第10步：右岸一个1回到左岸1                       1 12                       2 2第11步：一个1和一个2乘船到右岸1 1 12 2 2所以，  一共需要十一步可以成功过河"}
{"content2":"今日CS.CV 计算机视觉论文速览Thu, 6 Jun 2019Totally 38 papers👉上期速览✈更多精彩请移步主页Daily Computer Vision PapersSingle-Camera Basketball Tracker through Pose and Semantic Feature FusionAuthors Adri Arbu s Sang esa, Coloma Ballester, Gloria Haro跟踪体育运动员是一个极具挑战性的场景，特别是在紧凑的法院录制的单一馈送视频中，无法避免混乱和遮挡。本文分析了几种几何和语义的视觉特征，以检测和跟踪篮球运动员。进行消融研究，然后用于评论可以使用深度学习功能构建健壮的跟踪器，而无需提取上下文跟踪器，例如接近度或颜色相似性，也不应用相机稳定技术。所呈现的跟踪器包括1个检测步骤，其使用预训练深度学习模型来估计玩家姿势，接着是跟踪步骤2，其利用来自VGG网络中的卷积层的输出的姿势和语义信息。它的表现是根据MOTA对篮球数据集进行分析，该数据集的实例超过10k。A GLCM Embedded CNN Strategy for Computer-aided Diagnosis in Intracerebral HemorrhageAuthors Yifan Hu, Yefeng Zheng计算机辅助诊断CADx系统已被证明可以通过提供各种医学图像的分类来协助放射科医师，如计算机断层扫描CT和磁共振MR。目前，卷积神经网络在CADx中发挥着重要作用。然而，由于CNN模型应该具有像输入一样的方形，通常很难将CNN算法直接应用于放射科医师感兴趣的感兴趣ROI的不规则分割区域。在本文中，我们提出了一种新的方法来构建模型。通过提取不规则区域的信息并将其转换为固定大小的灰度级共生矩阵GLCM，然后将GLCM用作CNN模型的一个输入。以这种方式，作为原始CNN的有用实现，CNN还提取了一些基于GLCM的特征。同时，网络将更加关注重要病变区域，实现更高的分类准确性。在三个分类数据库Hemorrhage，BraTS18和Cervix上进行实验，以验证我们的创新模型的普遍性。总之，所提出的框架优于每个数据库的相应的现有算法，其中测试损失和分类准确性作为评估标准。Multi-way Encoding for RobustnessAuthors Donghyun Kim, Sarah Adel Bargal, Jianming Zhang, Stan Sclaroff深度模型是许多计算机视觉任务的最新技术，包括图像分类和对象检测。但是，已经证明深层模型容易受到对抗性的影响。我们重点介绍一个热门编码如何直接导致此漏洞，并建议摆脱这种广泛使用但非常容易受到影响的映射。我们通过利用不同的输出编码，多路编码，证明了源模型和目标模型的相关性，使目标模型更加安全。我们的方法使对手更难以找到用于生成目标模型的对抗性攻击的有用渐变。我们提出了对四个基准数据集的黑盒子和白盒攻击的鲁棒性。我们的方法的强度也通过对来自源模型的目标模型去相关来以模型水印的攻击的形式呈现。Visual Confusion Label Tree For Image ClassificationAuthors Yuntao Liu, Yong Dou, Ruochun Jin, Rongchun Li卷积神经网络模型广泛用于图像分类任务。然而，这种模型的运行时间太长，以至于不符合移动设备的严格实时要求。为了优化模型并满足上述要求，我们提出了一种用树分类器替换卷积神经网络模型的完全连接层的方法。具体来说，我们基于卷积神经网络模型的输出构造视觉混淆标签树，并使用具有分层约束的多核SVM加分类器来训练树分类器。专注于那些混淆子集而不是整个类别集使得树分类器更具辨别力并且完全连接的层的替换减少了原始运行时间。实验表明，我们的树分类器在CIFAR 100和ImageNet数据集的前1个精度方面分别比4.3和2.4获得了对现有树分类器的显着改进。此外，与AlexNet和VGG16上的完全连接层相比，我们的方法可实现124x和115x的加速比，而不会降低精度。Improving Variational Autoencoder with Deep Feature Consistent and Generative Adversarial TrainingAuthors Xianxu Hou, Ke Sun, Linlin Shen, Guoping Qiu我们提出了一种改进变分自动编码器VAE性能的新方法。除了强制执行深度特征一致原则从而确保VAE输出及其相应的输入图像具有相似的深度特征外，我们还实施了生成对抗训练机制，以迫使VAE输出逼真和自然的图像。我们提出实验结果表明，使用我们的新方法训练的VAE在生成具有更清晰和更自然的鼻子，眼睛，牙齿，头发纹理以及合理背景的脸部图像方面优于现有技术水平。我们还表明，我们的方法可以学习输入面部图像的强大嵌入，可用于实现面部属性操作。此外，我们提出了一种多视图特征提取策略来提取有效的图像表示，其可用于实现面部属性预测中的现有技术性能。Efficient Codebook and Factorization for Second Order Representation LearningAuthors Pierre Jacob, David Picard, Aymeric Histace, Edouard Klein学习丰富而紧凑的表示是许多领域中的一个开放主题，例如对象识别或图像检索。深度神经网络在过去几年中已经为这些任务取得了重大突破，但它们的表示并不像需要的那样丰富，也不像预期的那样紧凑。为了构建更丰富的表示，高阶统计数据已经被利用并且表现出优异的性能，但它们产生更高维度的特征。尽管已经通过分解方案部分地解决了这个缺点，但是从未检索到一阶模型的原始紧凑性，或者以强大的性能降低为代价。我们的方法通过将码本策略与分解方案联合集成，能够产生紧凑的表示，同时保持具有很少附加参数的二阶性能。该公式导致三个图像检索数据集的现有技术结果。Efficient, Lexicon-Free OCR using Deep LearningAuthors Marcin Namysl, Iuliu Konya与流行的看法相反，光学字符识别OCR仍然是一个具有挑战性的问题，当文本出现在无限制的环境中，如自然场景，由于几何扭曲，复杂的背景和各种字体。在本文中，我们提出了一个无分段的OCR系统，它结合了深度学习方法，综合训练数据生成和数据增强技术。我们使用大型文本语料库和超过2000种字体来渲染合成训练数据。为了模拟复杂自然场景中出现的文本，我们使用几何失真来增强提取的样本，并使用建议的数据增强技术alpha合成背景纹理。我们的模型采用卷积神经网络编码器从文本图像中提取特征。受神经机器翻译和语言建模的最新进展的启发，我们研究了递归和卷积神经网络在对输入元素之间的相互作用进行建模时的能力。Grounded Human-Object Interaction Hotspots from Video (Extended Abstract)Authors Tushar Nagarajan, Christoph Feichtenhofer, Kristen Grauman学习如何与物体交互是实现具体视觉智能的重要一步，但现有技术受到严格的监督或传感要求。我们提出了一种直接从视频中学习人体对象交互热点的方法。我们的方法不是将能力资源视为手动监督的语义分割任务，而是通过观看真实人类行为的视频和预测所提供的行为来学习交互。给定一个新颖的图像或视频，我们的模型推断出一个空间热点地图，指示如何在潜在的交互中操纵对象，即使该对象当前处于静止状态。通过第一人称视频和第三人称视频的结果，我们展示了真实人类对象交互中的接地能力值。我们的弱监督热点不仅与强监督的可供性方法竞争，而且还可以预测新对象类别的对象交互。项目页面Consistency regularization and CutMix for semi-supervised semantic segmentationAuthors Geoff French, Timo Aila, Samuli Laine, Michal Mackiewicz, Graham Finlayson一致性正则化描述了一类在半监督分类问题中产生突破性结果的方法。先前的工作已经建立了群集假设，在该假设下，数据分布由由低密度区域分隔的均匀类别的样本群组成，这是其成功的关键。我们分析了语义分割的问题，发现数据分布没有表现出分离类的低密度区域，并且提供了这个解释为什么半监督分割是一个具有挑战性的问题。我们将最近提出的CutMix正则化器用于语义分割，并发现它能够克服这一障碍，从而将一致性正则化成功应用于半监督语义分段。Towards Document Image Quality Assessment: A Text Line Based Framework and A Synthetic Text Line Image DatasetAuthors Hongyu Li, Fan Zhu, Junhua Qiu由于文档图像的低质量将极大地破坏自动文本识别和分析成功的机会，因此有必要评估在线业务流程中上传的文档图像的质量，以便拒绝那些低质量的图像。在本文中，我们尝试实现文档图像质量评估，我们的贡献是双重的。首先，由于文档图像质量评估对文本更感兴趣，我们提出了一种基于文本行的框架来估计文档图像质量，它由三个阶段的文本行检测，文本行质量预测和整体质量评估组成。文本行检测旨在使用检测器找到潜在的文本行。在文本行质量预测阶段，使用基于CNN的预测模型为每个文本行计算质量分数。最终使用所有文本行质量的集合来评估文档图像的整体质量。其次，为了训练预测模型，合成具有不同属性的包括52,094个文本行图像的大规模数据集。对于每个文本行图像，使用分段函数计算质量标签。为了证明所提出的框架的有效性，在两个流行的文档图像质量评估基准上评估了综合实验。我们的框架在大而复杂的数据集上大幅度地优于最先进的方法。Baby steps towards few-shot learning with multiple semanticsAuthors Eli Schwartz, Leonid Karlinsky, Rogerio Feris, Raja Giryes, Alex M. Bronstein从一个或几个视觉示例中学习是人类从婴儿早期开始的关键能力之一，但仍然是现代AI系统的重大挑战。虽然从一些图像示例中很少有镜头学习取得了相当大的进展，但对于婴儿在呈现新物体时通常提供的口头描述却少得多。在本文中，我们关注额外语义的作用，这些语义可以显着促进少数镜头视觉学习。基于最近在少数镜头学习中使用附加语义信息的进展，我们证明使用更丰富的语义和多个语义源可以进一步改进。使用这些想法，我们为流行的miniImageNet基准测试的一次性测试提供了一个新的结果，与基于视觉和基于语义的方法的先前最先进的结果相比。我们还进行了消融研究，调查我们的方法的组件和设计选择。Corn leaf detection using Region based convolutional neural networkAuthors Mohammad Ibrahim Sarker, Heechan Yang, Hyongsuk Kim机器学习领域已经成为一个日益萌芽的研究领域，因为在处理更复杂的图像检测挑战时需要更有效的方法。解决农业问题越来越重要，因为食物是生命的基础。然而，由于许多不同的杂草，近期玉米田系统的检测精度仍然远离实际需求。本文提出了一个模型来处理农田收集的数字图像中的玉米叶片检测问题。基于使用CNN采用的几种现有技术模型进行的实验结果，已经提出基于区域的方法作为更快和更准确的玉米叶检测方法。由于具有ResNet的这些独特属性，我们将其与基于区域的网络相结合，例如更快的rcnn，其能够在重杂草遮挡中自动检测玉米叶。该方法在来自场的数据集上进行评估，我们自己编写注释。我们提出的方法在玉米检测系统中实现了明显优异的性能。AI-Skin : Skin Disease Recognition based on Self-learning and Wide Data Collection through a Closed Loop FrameworkAuthors Min Chen, Ping Zhou, Di Wu, Long Hu, Mohammad Mehedi Hassan, Atif Alamri人体皮肤状况的改变存在很多隐患，例如长时间暴露于紫外线引起的晒伤，不仅会产生审美影响，导致心理抑郁和缺乏自信，甚至可能危及生命。由于皮肤癌变。目前的皮肤病研究采用自动分类系统来提高皮肤病分类的准确率。但是，对图像样本数据库的过度依赖无法为不同的人群提供个性化的诊断服务。为克服这一问题，本文提出了一种基于数据宽度演化和自学习的医学AI框架，以提供满足实时性，可扩展性和个性化要求的皮肤病医疗服务。首先，讨论了用户和远程医疗数据中心的闭环信息流中的广泛数据集。其次，给出了一种基于信息熵的数据集过滤算法，以减轻边缘节点的负荷，同时提高远程云分析模型的学习能力。此外，该框架提供了一个外部算法加载模块，可以根据所选模型与应用程序要求兼容。加载并比较了三种深度学习模型，即LeNet 5，AlexNet和VGG16，验证了算法加载模块的通用性。建立了实时，个性化，可扩展的皮肤病识别系统的实验平台。并分析了测试仪与远程数据中心交互场景下的系统计算和通信延迟。结果表明，我们提出的系统可靠，有效。Weakly Supervised Object Detection with 2D and 3D Regression Neural NetworksAuthors Florian Dubost, Hieab Adams, Pinar Yilmaz, Gerda Bortsova, Gijs van Tulder, M. Arfan Ikram, Wiro Niessen, Meike Vernooij, Marleen de Bruijne弱监督检测方法可以在训练期间推断图像中目标对象的位置而不需要位置或外观信息。我们提出了一种弱监督深度学习方法，用于检测出现在图像中多个位置的对象。该方法使用编码器解码器网络的最后特征映射来计算关注图，该编码器解码器网络仅利用全局标签优化图像中目标对象的出现次数。与先前的方法相比，由于解码器部分，以全输入分辨率生成注意力图。将所提出的方法与基于MNIST的数据集中的数字检测的两个任务中的多种现有技术方法进行比较，并且在2202 3D的数据集中的四个脑区域中检测扩大的血管周围空间中的一种脑损伤的现实生活应用脑MRI扫描。在基于MNIST的数据集中，所提出的方法优于其他方法。在大脑数据集中，几种弱监督检测方法接近每个区域的人体内部协议。所提出的方法在操作点的所有脑区域中达到最低数量的假阳性检测，而其平均灵敏度与其他最佳方法的相似。Farm land weed detection with region-based deep convolutional neural networksAuthors Mohammad Ibrahim Sarker, Hyongsuk Kim机器学习已成为一个主要的研究领域，以处理越来越复杂的图像检测问题。在现有技术CNN模型中，基于实验结果，本文提出了一种基于区域的完全卷积网络，用于快速准确的物体检测。在基于区域的网络中，ResNet被认为是最新的CNN架构，它在2015年的ImageNet大规模视觉识别挑战ILSVRC中取得了最佳成果。深度残留网络ResNets可以使培训过程更快，并且与同等级别相比可以获得更高的准确性传统的神经网络。由于具有ResNet的这些独特属性，本文评估了微调ResNet对杂草数据集的对象分类的性能。农田杂草检测数据集不足以培养这种深度CNN模型。为了克服这个缺点，我们执行退出技术以及深度剩余网络，以减少过拟合问题，并应用所提出的ResNet进行数据增强，以从杂草数据集中获得显着的优异结果。我们使用基于区域的全卷积网络R FCN技术实现了更好的物体检测性能，该技术与我们提出的ResNet 101锁存。Invariant Tensor Feature CodingAuthors Yusuke Mukuta, Tatsuya Harada我们提出了一种利用不变性的新颖特征编码方法。我们考虑保留图像内容的变换构成一组有限正交矩阵的设置。在许多图像变换中就是这种情况，例如图像旋转和图像翻转。我们证明了当我们使用凸损失最小化学习线性分类器时，群不变特征向量包含足够的判别信息。从这个结果，我们提出了一个新的主成分分析特征建模，k意味着聚类，用于大多数特征编码方法，和全局特征函数，明确考虑组动作。虽然全局特征函数通常是复杂的非线性函数，但我们可以通过将函数构造为基本表示的张量积表示来容易地计算该空间上的群动作，从而产生不变特征函数的显式形式。我们在几个图像数据集上展示了我们的方法的有效性。Compact Approximation for Polynomial of Covariance FeatureAuthors Yusuke Mukuta, Tatsuaki Machida, Tatsuya Harada协方差池是一种具有良好分类精度的特征池方法。因为协方差特征包括二阶统计量，所以特征元素的比例是变化的。因此，使用矩阵平方根来归一化协方差特征会影响性能改进。当池化方法应用于从CNN模型提取的局部特征时，当池化函数可反向传播并且以端到端方式学习特征提取模型时，准确性增加。最近，提出了协方差特征矩阵平方根的迭代多项式逼近方法，与基于奇异值分解的方法相比，训练得更快，更稳定。在本文中，我们提出了紧致双线性池的扩展，它是协方差特征的多项式的标准协方差特征的紧凑近似。随后，我们将所提出的近似应用于对应于矩阵平方根的多项式，以获得协方差特征的平方根的紧致近似。我们的方法通过基于局部特征的相似性对应于一对局部特征的近似特征的加权和来近似协方差的更高维多项式。我们将我们的方法应用于标准细粒度图像识别数据集，并证明所提出的方法显示出与原始特征相比更小的尺寸。Detecting Kissing Scenes in a Database of Hollywood FilmsAuthors Amir Ziai检测电影中的场景类型对于视频编辑，评级分配和个性化等应用非常有用。我们提出了一种用于检测电影中的接吻场景的系统。该系统由两部分组成。第一分量是二元分类器，其预测二进制标签，即接吻或不给出从一秒片段的静止帧和音频波两者中提取的特征。第二个组件将连续的非重叠段的二进制标签聚合成一组接吻场景。我们尝试了各种2D和3D卷积体系结构，如ResNet，DesnseNet和VGGish，并开发了一种高度精确的接吻检测器，可以在多种类型的好莱坞电影数据库中获得0.95的验证，这些数据包括多种类型和跨越数十年。该项目的代码可在以下网站获得A Feature Transfer Enabled Multi-Task Deep Learning Model on Medical ImagingAuthors Fei Gao, Hyunsoo Yoon, Teresa Wu, Xianghua Chu物体检测，分割和分类是医学图像分析中的三个常见任务。多任务深度学习MTL联合处理这三个任务，这提供了几个优点，节省了计算时间和资源，并提高了过度拟合的鲁棒性。但是，现有的多任务深度模型从每个任务开始作为单独的任务，并在架构的最后用一个成本函数集成并行执行的任务。这种架构无法在训练的早期阶段利用来自每个单独任务的特征的组合能力。在这项研究中，我们提出了一种新的架构FTMTLNet，一种通过特征传输实现的MTL。传统的转移学习处理来自不同数据源a.k.a.域的相同或类似任务。基本假设是从源域获得的知识可以帮助目标域上的学习任务。我们建议的FTMTLNet利用来自同一域的不同任务。考虑到任务的特征是域的不同视图，可以使用来自多个视图的知识来很好地利用组合的特征图以增强普遍性。为了评估所提方法的有效性，将FTMTLNet与来自文献的模型进行比较，包括8个分类模型，4个检测模型和3个分割模型，使用公共全视野数字乳房X线照片数据集进行乳腺癌诊断。实验结果表明，FTMTLNet在分类和检测方面优于竞争模型，在分割方面具有可比性。Infant Contact-less Non-Nutritive Sucking Pattern Quantification via Facial Gesture AnalysisAuthors Xiaofei Huang, Alaina Martens, Emily Zimmerman, Sarah Ostadabbas非营养性吸吮NNS定义为当手指，奶嘴或其他物体放置在婴儿口中时发生的吸吮动作，但没有营养物质被输送。除了提供安全感之外，NNS甚至可以被视为婴儿中枢神经系统发育的指标。婴儿非营养性吮吸过程中的丰富数据，如吮吸频率，周期数和振幅，是判断婴儿或早产儿大脑发育的重要线索。如今，大多数研究人员通过使用压力传感器等接触设备来收集NNS数据。然而，这种侵入性接触将直接影响婴儿的自然吸吮行为，导致收集的数据显着失真。因此，我们提出了一种新颖的接触式NNS数据采集和量化方案，利用面部标志跟踪技术从记录的婴儿吸吮视频中提取婴儿颌骨的运动信号。由于完成吸吮动作需要大量的面部肌肉和颅神经的同步协调和神经整合，伴随婴儿吸吮奶嘴的面部肌肉运动信号可以间接地替换NNS信号。我们已经对从几个婴儿在NNS行为期间收集的视频评估了我们的方法，并且我们已经实现了与视觉检查以及基于接触的传感器读数的结果非常接近的量化的NNS模式。StarNet: Pedestrian Trajectory Prediction using Deep Neural Network in Star TopologyAuthors Yanliang Zhu, Deheng Qian, Dongchun Ren, Huaxia Xia行人轨迹预测对于许多重要应用至关重要。由于行人之间复杂的相互作用，这个问题是一个巨大的挑战。以前的方法只模拟行人之间的成对相互作用，这不仅过分简化了行人之间的相互作用，而且计算效率也很低。在本文中，我们提出了一个新的模型StarNet来处理这些问题。 StarNet具有星型拓扑结构，包括独特的集线器网络和多个主机网络。中心网络采用观察到的所有行人的轨迹来产生人际交互的全面描述。然后，主机网络（每个主机网络对应于一个行人）查阅描述并预测未来的轨迹。星形拓扑结构为StarNet提供了优于传统型号的两大优势。首先，StarNet能够考虑集线器网络中所有行人的集体影响，从而做出更准确的预测。其次，StarNet在计算上是高效的，因为主机网络的数量与行人的数量成线性关系。对多个公共数据集的实验表明，StarNet在准确性和效率方面都大大超过了多个技术水平。One-pass Multi-task Networks with Cross-task Guided Attention for Brain Tumor SegmentationAuthors Chenhong Zhou, Changxing Ding, Xinchao Wang, Zhentai Lu, Dacheng Tao类不平衡一直是医学图像分割的主要挑战之一。模型级联MC策略显着缓解了类不平衡问题。尽管其性能优异，但这种方法导致了不希望的系统复杂性，同时忽略了模型之间的相关性。为了处理MC的这些缺陷，我们在本文中提出了一种轻量级深度模型，即一次通过多任务网络OM Net来比MC更好地解决类不平衡，并且仅需要一次通过计算用于脑肿瘤分割。首先，OM Net将单独的分段任务集成到一个深层模型中。其次，为了更有效地优化OM Net，我们利用任务之间的相关性来设计在线培训数据传输策略和基于课程学习的培训策略。第三，我们进一步建议在任务之间共享预测结果，这使我们能够设计一个跨任务引导注意CGA模块。在前一任务提供的预测结果的指导下，CGA可以基于类别特定统计自适应地重新校准通道明智的特征响应。最后，引入了一种简单而有效的后处理方法来改进所提出的关注网络的分割结果。进行了大量实验以证明所提出技术的有效性。最令人印象深刻的是，我们在BraTS 2015和BraTS 2017数据集上实现了最先进的性能。通过提议的方法，我们还赢得了64个参赛队伍中BraTS 2018挑战赛的第三名。我们将公开提供代码Fully Automated Pancreas Segmentation with Two-stage 3D Convolutional Neural NetworksAuthors Ningning Zhao, Nuo Tong, Dan Ruan, Ke Sheng由于胰腺是腹部器官，其形状和大小的变化非常大，因此自动和准确的胰腺分割对于医学图像分析而言可能是具有挑战性的。在这项工作中，我们提出了一个基于卷积神经网络CNN的胰腺分割的全自动两阶段框架。在第一阶段，U Net被训练用于下采样3D体积分割。然后从估计的标签中提取覆盖胰腺的候选区域。受到着名地区CNN报道的优越性能的推动，在第二阶段，另一个3D U Net在第一阶段产生的候选区域进行训练。我们评估了所提出的方法在NIH计算机断层扫描CT数据集上的性能，并验证了其在测试中的骰子sorensen系数DSC精度方面优于其他最先进的2D和3D胰腺分割方法。该方法的平均DSC为85.99。PAC-GAN: An Effective Pose Augmentation Scheme for Unsupervised Cross-View Person Re-identificationAuthors Chengyuan Zhang, Lei Zhu, Shichao Zhang人员识别人员Re Id旨在检索由不相交和非重叠相机捕获的同一人的行人图像。许多研究人员最近关注这个热点问题并提出基于深度学习的方法，以有监督或无监督的方式提高识别率。然而，与其他图像检索基准相比，首先不能忽视的两个限制是，现有人员Re Id数据集的大小远远不能满足要求，其次不能为深度模型的训练提供足够的行人样本，现有样本数据集没有足够的人体运动或姿势覆盖，以提供更多的先验学习知识。在本文中，我们介绍了一种新的无监督姿势增强交叉视图人Re Id方案，称为PAC GAN来克服这些限制。我们首先提出了交叉视图姿势增强的正式定义，然后提出了PAC GAN的框架，这是一种新颖的条件生成对抗网络基于CGAN的方法，以提高无人监督的角色视图人Re Id的性能。具体而言，PAC GAN中称为CPG Net的姿势生成模型是从原始图像和骨架样本中生成足够数量的姿势丰富样本。通过将合成的姿势丰富的样本与原始样本组合来产生姿势增强数据集，其被馈送到名为Cross GAN的corss视图人Re Id模型中。此外，我们在CPG网络中使用权重共享策略来提高新生成样本的质量。据我们所知，我们是第一次尝试通过姿势增强来增强无监督的交叉视图人Re Id，并且大量实验的结果表明所提出的方案可以对抗现有技术。Learning to Compose and Reason with Language Tree Structures for Visual GroundingAuthors Richang Hong, Daqing Liu, Xiaoyu Mo, Xiangnan He, Hanwang Zhang在图像中基于自然语言，例如在树的左侧定位黑狗，是人工智能的核心问题之一，因为它需要理解细粒度和组合语言空间。然而，现有的解决方案依赖于整体语言特征和视觉特征之间的关联，而忽略了语言中隐含的构图推理的本质。在本文中，我们提出了一种自然语言基础模型，它可以自动组成二叉树结构来解析语言，然后以自下而上的方式沿着树进行视觉推理。我们将模型称为RVG TREE递归接地树，其灵感来自直觉，即任何语言表达式都可以递归地分解为两个组成部分，并且可以通过计算子树返回的接地得分来递归累积接地置信度得分。 RVG TREE可以通过使用直通Gumbel Softmax估计器进行端到端训练，该估计器允许来自连续得分函数的梯度通过离散树结构。几个基准测试的实验表明，我们的模型通过更可解释的推理实现了最先进的性能。PI-Net: A Deep Learning Approach to Extract Topological Persistence ImagesAuthors Anirudh Som, Hongjun Choi, Karthikeyan Natesan Ramamurthy, Matthew Buman, Pavan Turaga诸如持久性图表之类的拓扑特征及其功能近似（如持久性图像PI）已经显示出对机器学习和计算机视觉应用的巨大希望。其大规模采用的主要瓶颈是计算开销和难以将它们合并到可区分的架构中。我们在本文中迈出了重要的一步，通过提出一种直接从输入数据生成PI的新颖的一步法来缓解这些瓶颈。我们提出了一种称为PI Net的简单卷积神经网络架构，它允许我们学习输入数据和PI之间的映射。我们设计了两个独立的架构，一个设计用于将多变量时间序列信号作为输入，另一个接受多通道图像作为输入。我们将这些网络分别称为Signal PI Net和Image PI Net。据我们所知，我们是第一个提出使用深度学习直接从数据计算拓扑特征的人。我们探讨了使用加速计传感器数据和图像分类在两个应用人类活动识别中使用所提出的方法。我们展示了在监督的深度学习架构中融合PI的难易程度，以及从数据中提取PI的几个数量级的加速。我们的代码可在An Introduction to Deep Morphological NetworksAuthors Keiller Nogueira, Jocelyn Chanussot, Mauro Dalla Mura, William Robson Schwartz, Jefersson A. dos Santos最近基于深度学习的计算机视觉应用方法的令人印象深刻的结果为研究和工业界带来了新的空气。这种成功主要归功于允许这些方法学习数据驱动特征的过程，通常基于线性操作。然而，在某些情况下，这样的操作没有良好的性能，因为它们的继承过程模糊了边缘，丢失了角落，边界和对象几何的概念。克服这一点，非线性操作，例如形态学操作，可以保留对象的这些属性，在某些应用中是优选的甚至是现有技术。受此鼓励，在这项工作中，我们提出了一个新的网络，称为深度形态网络DeepMorphNet，能够通过优化结构元素执行特征学习过程，同时进行非线性形态学操作。 DeepMorphNets可以使用深度学习方法培训中常用的传统现有技术进行端到端的训练和优化。使用两个合成和两个传统的图像分类数据集对所提出的算法进行系统评估。结果表明，与当前深度学习方法学习的相比，所提出的DeepMorphNets是一种很有前途的技术，可以学习不同的特征。Geo-Aware Networks for Fine Grained RecognitionAuthors Grace Chu, Brian Potetz, Weijun Wang, Andrew Howard, Yang Song, Fernando Brucher, Thomas Leung, Hartwig Adam细粒度识别可以区分具有细微视觉差异的类别。为了帮助识别细粒度类别，已使用除图像之外的其他信息。然而，使用地理定位信息来提高细粒度分类准确性的努力很少。我们对这一领域的贡献是双重的。首先，据我们所知，这是第一篇系统地研究将地理定位信息纳入从地理定位先验到后期处理到特征调制的细粒度图像分类的各种方法的论文。其次，为了克服没有细粒度数据集具有完整地理定位信息的情况，我们通过向现有流行数据集iNaturalist和YFCC100M提供补充信息，引入并将公开两个具有地理定位的细粒度数据集。这些数据集的结果表明，与仅图像模型结果相比，最佳地理感知网络在iNaturalist上可以实现8.9的前1准确度增加，在YFCC100M上可以实现5.9增加。此外，对于像Mobilenet V2这样的小型图像基线模型，最佳地理感知网络比仅图像模型的前1精度高12.6，比没有地理定位的Inception V3模型实现更高的性能。我们的工作鼓励使用地理定位信息来改善服务器和设备模型的细粒度识别。4-D Scene Alignment in Surveillance VideoAuthors Robert Wagner, Daniel Crispell, Patrick Feeney, Joe Mundy为固定摄像机监控视频设计强大的活动检测器需要了解3D场景。本文介绍了一种自动相机校准过程，该过程提供了一种在不同时间推理物体之间空间接近度的机制。它将基于CNN的相机姿态估计器与行人观察提供的垂直比例相结合，以建立4D场景几何。与以前的一些方法不同，人们不需要被跟踪，也不需要明确地检测头部和脚部。它对各个高度变化和相机参数估计误差都很稳健。A systematic framework for natural perturbations from videosAuthors Vaishaal Shankar, Achal Dave, Rebecca Roelofs, Deva Ramanan, Benjamin Recht, Ludwig Schmidt我们引入了一个系统框架，用于量化分类器对视频中自然发生的图像扰动的鲁棒性。作为该框架的一部分，我们构建了Imagenet Video Robust，这是一个人类专家审查的22,178个图像数据集，分为1,109组感知相似图像，这些图像来自ImageNet视频对象检测数据集中的帧。我们评估了在ImageNet上训练的各种分类器，包括训练有效性的模型，并显示中位分类准确度下降16。此外，我们评估更快的R CNN和R FCN模型进行检测，并显示自然扰动同时引起分类和定位误差，导致检测mAP中值下降14个点。我们的分析表明，现实世界中的自然扰动对于当前的CNN来说存在很大问题，这对他们在需要可靠，低延迟预测的安全关键环境中的部署构成了重大挑战。Investigating the Lombard Effect Influence on End-to-End Audio-Visual Speech RecognitionAuthors Pingchuan Ma, Stavros Petridis, Maja Pantic最近提出了几种视听语音识别模型，其目的在于改善噪声中仅音频模型的鲁棒性。然而，几乎所有这些都忽略了伦巴第效应的影响，即在嘈杂环境中说话风格的变化，其旨在使语音更易于理解并且影响语音的声学特性和嘴唇运动。在本文中，我们研究了伦巴第效应在视听语音识别中的影响。据我们所知，这是第一个使用端到端深层架构并使用看不见的扬声器呈现结果的工作。我们的结果表明，正确建模伦巴第语音总是有益的。即使将相对少量的Lombard语音添加到训练集中，也可以显着改善存在嘈杂Lombard语音的真实场景中的性能。我们还展示了文献中遵循的标准方法，其中模型在嘈杂的普通语音上进行训练和测试，提供了对视频性能的正确估计，并略微低估了视听性能。在仅音频接近的情况下，对于高于3dB的SNR而言，性能被高估，并且对于较低的SNR而言低估了性能。On the use of Pairwise Distance Learning for Brain Signal Classification with Limited ObservationsAuthors David Calhas, Enrique Romero, Rui Henriques使用脑电图增加对脑信号数据的访问为研究电生理学大脑活动和进行神经元疾病的门诊诊断创造了新的机会。这项工作提出了依赖于信号光谱特性的精神分裂症分类的成对远程学习方法。鉴于观察数量有限，即临床试验中的病例和/或对照个体，我们提出了一种连体神经网络结构，以从每个通道的观察的成对组合中学习辨别特征空间。以这种方式，信号的多变量顺序被用作数据增强的形式，进一步支持网络泛化能力。提出了在余弦对比损失下学习参数的卷积层，以充分探索从脑信号导出的光谱图像。病例对照人群的结果显示，使用所提出的神经网络提取的特征导致精确的精神分裂症诊断10pp的准确性和对光谱特征的敏感性，从而表明存在非平凡的，辨别性的电生理学脑模式。OctopusNet: A Deep Learning Segmentation Network for Multi-modal Medical ImagesAuthors Yu Chen, Yuexiang Li, Jiawei Chen, Yefeng Zheng深度学习模型，例如完全卷积网络FCN，已经广泛用于3D生物医学分割并且实现了最先进的性能。多种方式通常用于疾病诊断和量化。在文献中广泛使用两种方法来融合分割网络早期融合中的多种模态，其将多种模态堆叠为不同的输入通道和后期融合，其融合来自最终的不同模态的分割结果。这些融合方法容易受到由具有广泛变化的输入模态引起的交叉模态干扰的影响。为了解决这个问题，我们提出了一种新颖的深度学习架构，即OctopusNet，以更好地利用和融合多种形式中包含的信息。所提出的框架针对用于特征提取的每种模态使用单独的编码器，并利用超融合解码器来融合所提取的特征，同时避免特征爆炸。我们在两个公开可用的数据集上评估了拟议的OctopusNet，即ISLES 2018和MRBrainS 2013.实验结果表明我们的框架优于常用的特征融合方法，并产生最先进的分割精度。Combining crowd-sourcing and deep learning to understand meso-scale organization of shallow convectionAuthors Stephan Rasp, Hauke Schulz, Sandrine Bony, Bjorn Stevens新现象和机制的发现通常始于科学家识别模式的直观能力，例如卫星图像或模型输出。然而，通常，这种直观证据难以编码和再现。在这里，我们展示了人群采购和深度学习如何结合起来，以扩大大气现象的直观发现。具体而言，我们关注的是交易中浅层云的组织，它们在地球的能量平衡中发挥着不成比例的巨大作用。基于视觉检查，定义了糖，花，鱼和砾石四种主观模式或组织。在两个研究所的云标记日，67名参与者在众包采购平台上分类了超过30,000张卫星图像。物理分析表明，这四种模式与不同的大规模环境条件有关。然后，我们使用分类作为深度学习算法的训练集，学习了以人类准确度检测云模式。这使得分析远远超出人类分类。例如，我们创建了四种模式的全球气候学。这些揭示了地理热点，可以深入了解中尺度云组织与大规模环流的相互作用。我们的项目表明，人群采购和深度学习相结合，开辟了新的数据驱动方式，探索云循环相互作用，并作为地球科学中广泛可能研究的模板。A Robust Roll Angle Estimation Algorithm Based on Gradient DescentAuthors Rui Fan, Lujia Wang, Ming Liu, Ioannis Pitas本文提出了一种鲁棒的侧倾角估计算法，该算法是根据我们之前发表的工作开发的，其中通过使用黄金分割搜索算法最小化全局能量，从密集视差图估计侧倾角。在本文中，为了实现更高的计算效率，我们利用梯度下降来优化上述全局能量。实验结果表明，所提出的侧倾角估计算法需要较少的迭代次数才能达到与前一种方法相同的精度。AssemblyNet: A Novel Deep Decision-Making Process for Whole Brain MRI SegmentationAuthors Pierrick Coup , Boris Mansencal, Micha l Cl ment, R mi Giraud, Baudouin Denis de Senneville, Vinh Thong Ta, Vincent Lepetit, Jos V. Manjon使用深度学习DL的全脑分割是非常具有挑战性的任务，因为与可用训练图像的数量相比，解剖标签的数量非常高。为了解决这个问题，先前的DL方法提出使用全局卷积神经网络CNN或少数独立的CNN。在本文中，我们提出了一种新的集合方法，基于大量的CNN处理不同的重叠脑区。受议会决策系统的启发，我们提出了一个名为AssemblyNet的框架，由两个U网组件组成。这种议会制度能够处理复杂的决定并迅速达成共识。 AssemblyNet引入了相邻U网之间的知识共享，第二大会以更高的分辨率制定了修正程序，以完善第一个大会的决定，以及通过多数表决获得的最终决定。当使用相同的45个训练图像时，AssemblyNet在Dice度量，基于补丁的联合标签融合15和SLANT 27乘以10方面优于全球U Net 28。最后，AssemblyNet展示了处理有限训练数据的高容量，以在实际训练和测试时间内实现全脑分割。Towards Multimodal Sarcasm Detection (An _Obviously_ Perfect Paper)Authors Santiago Castro, Devamanyu Hazarika, Ver nica P rez Rosas, Roger Zimmermann, Rada Mihalcea, Soujanya Poria讽刺经常通过几种口头和非口头线索来表达，例如，语气的变化，单词的过分强调，抽出的音节或直面的表情。最近在讽刺检测方面的大部分工作都是在文本数据上进行的。在本文中，我们认为结合多模态线索可以改善讽刺的自动分类。作为开发用于讽刺检测的多模式方法的第一步，我们提出了一种新的讽刺数据集，即多模式讽刺检测数据集MUStARD，由流行的电视节目编制而成。 MUStARD包含用讽刺标签注释的视听话语。每个话语都伴随着对话中历史话语的背景，其提供关于话语发生的场景的附加信息。我们的初步结果表明，与使用个体模态相比，使用多模态信息可以将讽刺检测的相对错误率降低F评分高达12.9。完整数据集可公开使用Artifact Disentanglement Network for Unsupervised Metal Artifact ReductionAuthors Haofu Liao, Wei An Lin, S. Kevin Zhou, Jiebo Luo当前基于深度神经网络的计算机断层扫描CT金属伪影减少方法是监督方法，其严重依赖于合成数据进行训练。然而，由于合成数据可能无法完美地模拟CT成像的潜在物理机制，因此监督方法通常不能很好地推广到临床应用。为了解决这个问题，我们建议，就我们所知，第一个无监督的MAR学习方法。具体而言，我们引入了一种新颖的人工解法网络，该网络能够在受影响的工件和无伪影图像域之间实现不同形式的生成和规则化，以支持无监督学习。大量实验表明，我们的方法明显优于现有的无监督模型的图像到图像转换问题，并且在综合数据集上实现了与现有监督模型相当的性能。当应用于临床数据集时，我们的方法相对于监督模型实现了相当大的改进。Chinese Abs From Machine TranslationPapers from arxiv.org更多精彩请移步主页Interesting:📚DeepMorphNet深度形态学网络，用于获取更稳定准确的形态学操作结果, (from Universidade Federal de Minas Gerais, Brazil)📚高效无字典的光学字符识别,利用多种字体和场景合成不同形状的字体，并将其合成在不同自然场景中，包含了扭曲和形变（不同的透明度合成）。 (from Fraunhofer IAIS德国夫琅禾费研究所)dataset:https://github.com/tesseract-ocr/tessdata_fast📚Visual Confusion Label Tree的图像分类模型, (from 国防科大)📚基于文本线和合成文本图像实现的文件质量评价, (from Tongdun Technology ZhongAn Technology)TL;DwRPI-Net分析图像的拓扑信息 ，code:https://github.com/anirudhsom/PI-Net+运动员检测与追踪玉米叶子检测 & 农田杂草检测++StarNet 行人轨迹预测，基于星形拓扑结果CNN+接吻场景检测，code:http://github.com/amirziai/kissing-detector大气模式分析pic from pexels.com"}
{"content2":"0 写在前面本文记录了两个月以来8场学科前沿技术讲座的课程总结与感悟。学院请到了很多厉害的教授以及企业的专家和学者，讲座的方向多以大数据和人工智能为主，作为计算机科学专业的学生，时刻保持对行业发展前沿领域的关注，我认为是十分必要的。1 课程感悟经过近两个月的讲座课程的学习，我对计算机科学的学术前沿内容有了更多、更深入的理解和感悟。讲座的内容很充实，形式也十分丰富，讲座的主题也涵盖了包括但不限于数据库原理、大数据、人工智能等等。我认为，在本科三年级的这个阶段，在核心专业课基础知识-包括数据结构与算法、计算机组成原理、编译原理、操作系统、面向对象等-已经熟练掌握得十分牢固的前提下，应该把目光放得长远。在邹欣老师的《构建之法》一书的前言中有所提到：学校想培养什么样的学生，是世界一流，中国一流，还是本省二流？有什么样的期望，就要有什么样的课程设计。作为北航的一名本科生，应该将成为国际一流人才作为自己的培养目标，而要成为这样的人才，就需要用国际一流的标准去要求自己。所以，能够在这个本科生涯即将告一段落、即将步入社会的重要关键节点，学院为我们开设这样一门课程，并请到了李波老师、马殿富老师、邹欣老师、马帅老师等等为我们深入地剖析当前计算机科学与技术的前沿知识，是我在这一学期的一大幸运。在众多精彩的讲座中，最吸引我的主题，非人工智能相关的话题莫属。一方面，是今年来，人工智能浪潮来袭，使人工智能技术再一次到达顶峰，与人工智能有关的内容成为炙手可热的话题。另一方面，也是我本人，对于人工智能领域的前沿技术的热爱，让我对老师们精彩的演讲产生了浓厚的兴趣。因此，若要在这短短五千字的报告中，用简洁凝练的语言，来表达我的感悟的话，那么我最想表达的内容，必定是我对于人工智能前沿技术的体会与心得。人工智能在历史上曾经历三起三落，现在正是人工智能技术走上坡路的时期，这一点是不难解释的，那就是数据量的不断增长、数据硬件存储能力的扩增以及数据计算能力的提升与计算成本的降低，为机器学习的算法实现提供了无限可能。另外，随着理论研究的不断深入，机器学习在传统领域的基础上，又扩展出了多个分支—强化学习、深度学习、多任务学习，等等。应用领域也得以扩展，从数据挖掘、图像检测、模式识别到自然语言处理等等，可以说机器学习已经遍及到人们生活的方方面面。讲座中所学到的内容与知识都是静态的，而只有将这些知识，实际运用起来才能让其变得生动灵活。正如马尔科姆•格拉德威尔在他的《逆转：弱者如何找到优势，反败为胜》一书中所提到的，如果一直停留在理论层面上去分析问题的话，那么有利局势的天平将很难朝你的一侧倾斜。因此，只有在真正的实践中，才能体会到讲座中，老师们所向我们介绍的人工智能的神奇力量。为此，我亲自尝试了人工智能的两个具体的应用，并用机器学习的方法，解决了实际生活中的问题，这一过程让我感触颇深，也是我在这门课中收获最大的地方。第一个实践是运用多任务学习的方法，尝试解决了一类商业选址问题。商业选址是一类重要的投资决策问题。其重要性主要体现在投资的长期性、固定性以及对经济效益的决定性上。在传统的商业选址问题中，通常的考量因素往往涵盖了地域、交通、竞争压力以及人流量等方面。在这种情况下，投资者的经验以及数据信息来源的有效性将起到决定性的作用。随着移动互联网时代的到来，越来越多的商业应用，如美团、大众点评等渐渐走入人们的生活。这其中蕴含着巨大潜在的商业价值有待挖掘，尤其对于商业选址这类重要的问题而言，数据所提供的参考信息已然成为大数据时代的选址利器。近年来，社会经济持续发展，企业不断扩张，连锁店的经营模式得到了更为广泛的应用。如餐饮业的海底捞火锅店、麦当劳、星巴克，服饰业的H&M、Nike、Zara等品牌的迅猛发展，连锁店这种商业模式开始逐渐在市场中占据主导地位。由此为这些连锁品牌带来一个关乎企业发展的核心问题，即连锁店的选址问题。为此，在我的实践中，综合考量投资所在地商场的内部和外部特征，为连锁品牌的投资者进行商业选址的推荐。对大众点评上的海量数据进行分析，并为投资者给出最优化的选址推荐。第二个实践则是综合运用了计算机视觉相关技术，实现了一种视频的风格迁移方法。随着手机等智能终端的兴起，许多软件如春笋般发芽成长。从用户的触媒习惯来看，大家投入在短视频上的时间越来越多。艾瑞数据显示，用户单机单日有效使用时长已经从2017年度第一季度的21.1分钟增长到2018第二季度的33.1分钟。短视频行业中，我们也可以看到，抖音、快手等短视频软件异军突起，发展迅猛。短视频行业的火热，与短视频相关的技术自然也是如鱼得水。我所实现的视频风格迁移方法，即拥有针对短视频进行视频风格迁移的能力。用户可以根据喜好，选择某种名画的风格，即可对自己拍摄的短视频进行加工，生成美轮美奂的带有名画风格的短视频。我在这一实践中，综合运用了多种计算机视觉的相关技术，包括但不限于采用convLSTM来捕捉视频的时序特征、WarpError来计算视频流中相邻帧之间的差值、引入了一种风格迁移模型RecoNet同时结合了Instance Normalization的方法，来代替传统的Batch Normalization，从而实现对多风格的控制，等等。最终使得这一视频风格迁移方法相较与目前最佳解决方案的效果，在视频稳定性等呈现效果上，有着更优的表现。通过这两个实践，我都成功地将讲座中老师们所介绍到的人工智能的理论应用到实际，从而让我真切体会到了人工智能对人们生活方式的改变。2 课程收获2-1 总述与课程感悟部分不同的是，在课程感悟部分，我重点论述了我在本学期两个月来，从头至尾完整、认真地听过8次讲座后在宏观层面的整体感受。而在这一章节，课程收获中，我将更偏重于将我本学期在讲座中所学习到的领域知识或是让我对整个科技前沿体系的理解有帮助的内容记录下来，形成一个相对完整的脉络。另外，这一部分对于课程收获的总结，也对我日后时常回顾这8场讲座的精彩内容，保留一个比较细致的记录。我在此课程中的收获，正如我在课程感悟中所提及的，正是我所感兴趣的人工智能相关的话题，因此，接下来我将从人工智能的发展历程，人工智能的发展现状，以及人工智能的发展前景展开论述，在最后的一个小节中，作为补充，我也来谈谈我自己对人工智能的认识和态度。2-2 人工智能发展历程随着众多核心技术的迅猛发展，已经诞生了半个多世纪的人工智能终于从研究与发现发展到如今的巅峰期。回顾起来，在过去半个多世纪中人工智能经历过黄金时代也曾有过低谷，不过科技的魅力在于历经起起伏伏之后，现在的人工智能已开始深深影响人类社会。都说人工智能在历史上经历了“三起三落”，那么这三“起”与三“落”到底指代着什么，发生的时间节点与背景优势什么，以下我将按照时间线顺序来记录一下。人工智能的第一次高峰：在1956年的这次会议之后，人工智能迎来了属于它的第一段高峰。在这段长达十余年的时间里，计算机被广泛应用于数学和自然语言领域，用来解决代数、几何和英语问题。这让很多研究学者看到了机器向人工智能发展的信心。甚至在当时，有很多学者认为：“二十年内，机器将能完成人能做到的一切。”人工智能第一次低谷： 70年代，人工智能进入了一段痛苦而艰难岁月。由于科研人员在人工智能的研究中对项目难度预估不足，不仅导致与美国国防高级研究计划署的合作计划失败，还让大家对人工智能的前景蒙上了一层阴影。与此同时，社会舆论的压力也开始慢慢压向人工智能这边,导致很多研究经费被转移到了其他项目上。在当时，人工智能面临的技术瓶颈主要是三个方面，第一,计算机性能不足，导致早期很多程序无法在人工智能领域得到应用；第二，问题的复杂性，早期人工智能程序主要是解决特定的问题，因为特定的问题对象少，复杂性低，可一旦问题上升维度，程序立马就不堪重负了；第三，数据量严重缺失，在当时不可能找到足够大的数据库来支撑程序进行深度学习，这很容易导致机器无法读取足够量的数据进行智能化。人工智能的崛起：1980年，卡内基梅隆大学为数字设备公司设计了一套名为XCON的“专家系统”。这是一种，采用人工智能程序的系统，可以简单的理解为“知识库+推理机”的组合，XCON是一套具有完整专业知识和经验的计算机智能系统。在这个时期，仅专家系统产业的价值就高达5亿美元。人工智能第二次低谷：可怜的是，命运的车轮再一次碾过人工智能，让其回到原点。仅仅在维持了7年之后，这个曾经轰动一时的人工智能系统就宣告结束历史进程。到1987年时，苹果和IBM公司生产的台式机性能都超过了Symbolics等厂商生产的通用计算机。从此，专家系统风光不再。人工智能再次崛起：上世纪九十年代中期开始，随着AI技术尤其是神经网络技术的逐步发展，以及人们对AI开始抱有客观理性的认知，人工智能技术开始进入平稳发展时期。1997年5月11日，IBM的计算机系统“深蓝”战胜了国际象棋世界冠军卡斯帕罗夫，又一次在公众领域引发了现象级的AI话题讨论。这是人工智能发展的一个重要里程。 2016年，alphago在围棋上击败了李世石，再一次向世人揭示了人工智能非凡力量。2-3 人工智能发展现状经历了技术驱动和数据驱动阶段，人工智能现在已经进入场景驱动阶段，深入落地到各个行业之中去解决不同场景的问题。此类行业实践应用也反过来持续优化人工智能的核心算法，形成正向发展的态势。老师们在讲座中所提及的人工智能的主要应用领域主要涵盖了以下几个方面，包括制造、家居、金融、零售、交通、安防、医疗、物流、教育等等。不难看出，人工智能在当前的应用已经十分广泛，可以说基本覆盖到了人们日常生活的方方面面。家居智能家居是老师在讲座中提到的一个常见的人工智能应用之一。智能家居主要是基于物联网技术，通过智能硬件、软件系统、云计算平台构成一套完整的家居生态圈。用户可以进行远程控制设备，设备间可以互联互通，并进行自我学习等，来整体优化家居环境的安全性、节能性、便捷性等。值得一提的是，近两年随着智能语音技术的发展，智能音箱成为一个爆发点。小米、天猫、Rokid 等企业纷纷推出自身的智能音箱，不仅成功打开家居市场，也为未来更多的智能家居用品培养了用户习惯。但目前家居市场智能产品种类繁杂，如何打通这些产品之间的沟通壁垒，以及建立安全可靠的智能家居服务环境，是该行业下一步的发力点。金融正如我在自我实践中所接触的项目一样，人工智能可以在商业、金融领域为人们提供可靠的数据保障。人工智能在金融领域的应用主要包括：身份识别、大数据风控、智能投顾、智能客服、金融云等，该行业也是人工智能渗透最早、最全面的行业。未来人工智能也将持续带动金融行业的智能应用升级和效率提升。例如第四范式开发的一套AI系统，不仅可以精确判断一个客户的资产配置，做清晰的风险评估，以及智能推荐产品给客户，将转化率提升65%。很多金融行业的应用，都可以作为人工智能在其他行业落地的典型案例。交通最近一段时间，我效力于王静远老师的实验室，主要负责配合实验室的学长，完成一些基本的开发工作，王老师的实验室主要负责与数据挖掘有关领域，解决交通与医疗相关问题。在交通方面，主要是智慧交通，对车辆做行车轨迹恢复以及行程时长估计。而在医疗方面则是与医院展开合作，对孕妇可能存在的潜在风险进行评估，从而保证孕妇妊娠过程的安全。智能交通系统是通信、信息和控制技术在交通系统中集成应用的产物。智能交通应用最广泛的地区是日本，其次是美国、欧洲等地区。目前，我国在智能交通方面的应用主要是通过对交通中的车辆流量、行车速度进行采集和分析，可以对交通进行实施监控和调度，有效提高通行能力、简化交通管理、降低环境污染等。医疗目前，在垂直领域的图像算法和自然语言处理技术已可基本满足医疗行业的需求，市场上出现了众多技术服务商，例如提供智能医学影像技术的德尚韵兴，研发人工智能细胞识别医学诊断系统的智微信科，提供智能辅助诊断服务平台的若水医疗，统计及处理医疗数据的易通天下等。尽管智能医疗在辅助诊疗、疾病预测、医疗影像辅助诊断、药物开发等方面发挥重要作用，但由于各医院之间医学影像数据、电子病历等不流通，导致企业与医院之间合作不透明等问题，使得技术发展与数据供给之间存在矛盾。2-4 人工智能发展前景对待人工智能当下火爆的市场形势，要判断和分析其在未来的发展，还需要冷静、客观。在影响就业之前，人工智能将会对雇主产生影响。长期来看,人工智能不会摧毁就业市场——至少在2018年是不可能的。但是企业面临着一个重大挑战：只有汇集了来自不同种类的数据以及不同学科的团队成员时，人工智能才能发挥出最大的效果。同时，它还需要借助相应的结构和技能来实现人机协作。但是大多数企业都把数据存放在联合企业和团队的数据库里。很少有企业开始为员工提供他们所需要的基本人工智能技能。普通的企业还没有准备好满足人工智能的需求。人工智能将融入现实，开始发挥其效用。它可能不会成为媒体的头条新闻,但人工智能现在已经准备好了，能够自动完成日益复杂的流程，识别出能够创造商业价值的趋势，并提供具有前瞻性的情报。这带来的结果是,人们的工作量减少,做出的战略决策也变得更好了：员工的工作也比以前更好了。但是,由于传统的投资回报率（ROI）策略可能无法准确地识别出这一价值，企业将需要考虑采取新的指标，以便更好地理解工智能可以为它们做什么。人工智能将帮助回答有关数据的重大问题。许多针对数据技术和数据集成的投资都未能回答这样的一个重大问题：投资回报率在哪？现在，人工智能正在为这些数据项目提供商业案例，新的工具将会使这些项目的价值凸显出来。企业不再需要决定“清理数据”——也不应该这样做。他们应该首先从一个业务问题开始来量化人工智能的好处。一旦数据被用来解决一个特定的问题，进一步开发数据驱动的人工智能解决方案就会变得更容易，从而就会形成一个良性循环。问题出在了哪里？一些企业仍然在犹豫要不要建立，或者是没有建立好数据基础。在未来，人工智能领域的投资将以“AI+行业”的方式展开，预计人工智能应用场景较为成熟且需求强烈的领域，如安防、语音识别、医疗、智慧城市、金融等领域，带来升级转换，提高行业智能化水平，改善企业的盈利能力，预计随着诸如无人驾驶汽车等认知智能技术的加速突破与应用，人工智能市场将加速爆发。3 后记老师们精彩的讲座，为我打开了人工智能世界的大门；通过两个具体案例的实践，让我真切的亲身探索了人工智能这个丰富多彩的世界。我被人工智能给人类社会和当今人们生活方式所带来的改变，深深折服，在惊叹于技术发展的同时，也让我对踏入人工智能行业充满了向往。但与此同时尽管人工智能作为行业内的新兴热点，随着时间的推移其热度一直有增无减，对其未来的发展，仍应保持理性与客观的态度。因此，对领域内学科技术前沿时刻保持高度的敏锐嗅觉，透过问题的表面现象看到本质，才是这门课带给我最大的思考。人工智能的未来何去何从，我拭目以待。"}
{"content2":"《乌镇指数：全球人工智能发展报告2016》正式发布2016-11-17  法厉无边这是有史以来最详尽的AI报告，你想知道全在这里；作为AI从业者，这里有一份你不得不了解的全部：人工智能领域，中国距离世界第一有多远？中国各地，谁又能在人工智能独领风骚？如今火热的人工智能，背后的动力究竟来自哪里？……最后，还有些排名值得关注哟。本文系乌镇智库原创，拒绝盗版，转载请联系授权关注乌镇智库公众号（WUZHEN-INSTITUTE），或回复关键字“乌镇智库人工智能”，可下载《乌镇指数：全球人工智能发展报告2016》（精华篇）PDF全文。自1956年达特茅斯会议诞生以来，“人工智能”已有60年的发展历史。而在近两年，随着大数据、云计算、深度学习的兴起，人工智能迎来了新的发展机遇。尤其在2016年，AI技术的进步和大范围、多领域渗入行业应用让其再次成为行业焦点。随着AI被更广阔地应用到人类社会和经济生活各个方面，新的机遇和挑战随之而来，其巨大的潜在影响力让人类不得不谨慎思考人工智能的未来。时值第三届世界互联网大会召开之际，乌镇智库联合网易科技在水乡乌镇正式发布了《乌镇指数：全球人工智能发展报告2016》精华篇。该报告由乌镇智库、网易科技联合出口，新华网、南方都市报、DT财经联合发布，从产业与应用、学术与研究、投资与融资三个维度切入，详尽分析了全球范围内人工智能产业的发展现状及趋势。报告中的一些数据尤其引人深思。比如，2015年全球新增AI企业数量也达到了806家，算下来平均每10.9个小时就有一家AI企业诞生，而目前肯德基在全球的开店速度是8小时/家，可想AI创业之火热。过去一年在AI领域，全球共统计出近百亿美元1200多次的投资。从人工智能领域的IPO或并购案例看，目前互联网广告行业、Fintech是人工智能应用的重要行业，而健康医疗则被视为人工智能领域的下个风口。在整个人工智能的发展过程中，不同技术、行业和资本，在不同时期扮演着推动人工智能发展的角色。本报告将站在全球的角度，阐述中国在世界范围内的发展情况，以及中国各省市在人工智能领域的推进状态；寻找人工智能发展背后的技术动力，以及AI投资、企业、专利的相关度；通过数据分析，总结哪些资本最爱人工智能。最终为行业提供一个AI产业的世界发展地图。人工智能领域，中国距离世界第一有多远美国人工智能企业总数为2905家，全球第一。仅加州的旧金山/湾区、大洛杉矶地区两地的企业数量即达到1155家，占全球的19.13%。中国人工智能企业数量虽不及美国，北上深三城占全球总数的7.4%，但在东亚地区位列前三，在全球范围内分列第3、第6、第8位。于人工智能领域，美国与欧洲投资较为密集，数量较多，其次为中国、印度、以色列。美国共获得3450多笔投资，位列全球第一，英国获得274笔投资，位列第二，中国则以146笔投资，位列第三。就人工智能企业融资规模而言，中国位列全球第2名，但前三甲间的规模差距较大。专利方面，美国人工智能专利申请数累积达到26891项，位列全球第一；中国共计15745项，位列第二。但自2012年开始，中国的专利申请数及专利授权数就超越美国。自2001年，中国每年新增人工智能专利数增幅较大，申请数、授权数增长了40倍左右；美国整体上保持稳定增长趋势，在2011年开始加速，这与资本开始涌入该领域的节奏相近。从人工智能企业数量、融资规模、专利申请数三个维度，美国在人工智能领域的优势明显。近年来，中国在上述三项的发展速度上，领先全球，尤其是在新增专利数上开始超越美国。当然，在总体上中美仍有差距。比如，在人工智能企业数目，融资规模，投资机构数量三项指标上，美国分别约为中国的4倍，7倍和21倍。在人工智相关投资机构排名上，无论是专注度榜单，还是活跃度榜单，美国投资机构占据了大部分的座次，中国仅有真格基金、维港投资进入专注度榜单。中国各地，谁在人工智能领风骚具体到中国，中国人工智能企业主要集中于北京、广东及长三角，占中国人工智能企业总数的84.95%。四川省虽然企业数量不及上述三地区，但明显高于其它省市自治区。在专利、投融资的分布上，大体格局不变。就省市区而言，全国32个省市区，均进入人工智能企业数、专利申请数、融资数三项前十排名的，包括北京、广东、上海、江苏、浙江。其中，北京、广东在三项排名中位列前二，地位稳固；而上海、江苏均进入前五名，名次略有不同；浙江获得一次前五，两次前十；四川、重庆各入围两个榜单，可谓是人工智能领域的西部重镇。就城市而言，北京在各项榜单均名列第一。深圳稳定在前3名，上海稳定在前5名。整体上，上榜的城市多为东部城市。其中，北京、上海、深圳、广州、南京、成都均进入三个榜单。杭州两次位列前五，而苏州、厦门、武汉各进入其中两个榜单。从人工智能企业数量、融资规模、专利申请数三个维度看，中国东部地区的北京、长三角、珠三角是产业的重心；中西部地区则集中于重庆、四川。就城市而言，北上深三城市地位稳固，不仅在中国、东亚地区排名领先，即使在全球范围内，也名列前茅。值得注意的是，杭州在专利影响力、企业影响力上均领先于广州。可见杭州在互联网领域的巨大科技实力。AI发展背后的动力来自哪里？人工智能是一门综合性的学科。它的发展与各学科的发展紧密相关。不同时期不同学科上的理论、技术突破，都能为人工智能的发展提供动力。报告指出，机器学习与各学科的相关性普遍较高，机器人则相反。四类细分领域与计算机科学紧密相关，与生物学的相关性均不高。从历年相关度的趋势来看，虽然相关度数值时有起伏，但基本稳定在固定的区间范围内。从AI的细分领域（分支包括机器学习、自然语言处理、计算机视觉、机器人）来看，机器人与计算机视觉专利申请数趋势高度相似，这与两者的高度相关性有关。诸如机器人、计算机视觉等应用层专利，增长幅度更快。2011年之后，中国在这些领域有显著的增长。诸如机器人、计算机视觉等应用层面上的专利，增长幅度更快。相较于全球而言，中国这些细分领域还处于增长期，但略有放缓。自2009年，基于四类技术的企业数量整体上呈现增长趋势，其中以机器学习领域的进展最为迅速。自然语言处理与计算机视觉相关领域企业数量，自2013年开始下滑，机器学习与机器人领域回落时间则晚一年，但都早于全球人工智能整个行业回落的总体趋势。哪些资本最爱人工智能，中国在AI领域能否再造个BAT人工智能已经成为2016年最热门的投资领域之一。近几年，人工智能领域投资呈爆发趋势。无论是投资金额或投资频次，都有明显的增加。和全球同步，中国在人工智能领域的投资明显加快，但平均每次的交易额没有明显增加。2015年第三、四季度，中国人工智能领域的投资金额与投资频次都有明显的下跌，莫非，这印证了所谓的“寒冬”？人工智能领域，无论是TOP15最活跃的VC，或最活跃的孵化器、加速器，均以美国企业机构为主。中国在此两项榜单中暂无企业机构上榜。人工智能领域的并购情况，并购次数较多的企业多为美国企业，包括Google、Apple、Microsoft、Salesforce、Yahoo! 、Intel等 。并购次数，也有大幅度的提升：从2002年的1次，上升到目前的84次。尤其是近两年，2015年并购的次数同比上涨了56.8%。总体来看，大部分的收购，都是综合性公司水平并购专注技术型公司，而不是更大的垂直并购。无论是全球范围内，还是中国国内的情况来看，人工智能领域的投资金额与频次，在近几年都有明显的增长。虽然2015年下半年有过短暂的下滑趋势，但整体上仍然表现得较为活跃。与欧美国家投资机构较多、投资频繁一致，最活跃的VC、专注度较高的投资机构以及较大的并购案例也都集中于欧美。从人工智能领域的IPO或并购案例看，目前互联网广告行业、Fintech是人工智能应用的重要行业，而健康医疗则被视为人工智能领域的下个风口。综合来看，就像互联网时代有所谓的“互联网 ”和“ 互联网”，人工智能时代也有“人工智能 ”和“ 人工智能”。前者注重平台科技，后者注重垂直应用。从过去成功的企业和近期看到的机会来看，前者居多，但目前也逐渐出现诸如DeepMind之类的纯平台式公司。关于人工智能，还有些排名值得关注从高校研究的角度看，报告显示人工智能全球TOP50的大学几乎都来自北美、英国以及其他欧洲地区，唯一上榜的亚洲大学分别是排名37的台湾大学、排名47的东京大学、以及排名50的新加坡国立大学。在人工智能领域，清华大学位列中国人工智能大学榜首；而在企业数量、专利申请数上表现不错的广东，其高校却未能进入前10名。中山大学名列第11位，华南理工大学位列第17位。微软、IBM等老牌企业位列前茅并不让人意外，值得关注的是达芬奇机器人（INTUITIVE SURGICAL），2000年就通过美国食物药品管理局(FDA)认证，成为了美国FDA批准的第一个腹腔镜微创手术的自动控制机械系统。报告同时指出，除了互联网企业对人工智能比较关注外，近半传统企业的上榜，意味着它们对这一领域的重视程度不亚于互联网公司。在上述四个细分领域的企业融资榜单中，机器学习与机器人企业的融资额度较高，比如，机器学习企业的融资额度均超过$50M，超过$100M的有四家企业。相较之下，计算机视觉企业融资额度偏低。从国家方面看，美国企业占据了绝对优势。在各个融资榜单中， 除计算机视觉外，至少80%的企业均为美国企业。中国在每个榜单上至少有一家企业上榜。其中，机器学习榜单中的中国企业碳云智能(iCarbonX)表现瞩目，融资额度达到$199.87M。总之，AI领域的研究、应用、投资在很多方面都取得了快速进步，人类正在迎来了新的技术革命。附录：企业数据说明：（1）由于地址信息不明的一小部分企业，不在企业相关的统计范围内专利数据说明：（2）报告中的统计数据包含专利申请数和授权数。（3）中国专利只统计“发明专利”，“实用新型”和“外观专利”不在此次统计范围。（4）专利统计时间以专利公开时间为准。融资数据说明：（5）融资排行仅针对有公开确切融资额的公司。（6）融资排行所涉公司筛选标准为其主要产品或核心技术为人工智能（7）被收购的企业、上市企业不在此次融资排行范围大学排名说明：（8）本大学排名基于学术与影响力两方面进行排名。（9）学术包括学校人工智能领域发表论文数量、单论文质量、论文被引用数三个指标。（10）影响力包括人工智能领域杰出校友数，维基百科人工智能方向被引用数两个指标。（11）本报告所涉及数据均来自于乌镇智库数据库及公开资料。"}
{"content2":"本文主要内容来源于书籍《python计算机视觉编程》我是一名初学者，如果你发现文中有错误，请留言告诉我，谢谢PIL模块PIL模块全程为Python Imaging Library，是python中一个免费的图像处理模块。打开图像PIL模块常用到它的Image类，打开图像首先要导入Image类from PIL import Image，然后调用Image的open方法。例如from PIL import Image image = Image.open(\"smallpi.jpg\") # 返回一个Image图像对象 print(image) # 结果 <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=800x450 at 0x4731348>图像的保存及格式转换图像保存用的是Image对象的save()方法，传入的参数为保存图像文件的名字。当传入不同的扩展名时，它会根据扩展名自动转换图像的格式。例如from PIL import Image image = Image.open(\"smallpi.jpg\") # 打开jpg图像文件 image.save(\"smallpi.png\") # 保存图像，并转换成png格式转化成灰度图像获得Image对象后，调用其convert()方法,传入参数\"L\"，即可以返回该图像的灰度图像对象。from PIL import Image image = Image.open(\"smallpi.jpg\") image_gray = image.convert(\"L\") # 转化成灰度图像 print(image_gray) # 结果 <PIL.Image.Image image mode=L size=800x450 at 0x46AD648>Image对象与图像矩阵相互转化Image对象转化成图像矩阵Image对象转化成图像矩阵只要将Image对象作为numpy.array()参数即可。import numpy as np from PIL import Image image = Image.open(\"smallpi.jpg\") image_array = np.array(image) print(image_array) # 结果 [[[177 177 177] [177 177 177] [176 176 176] ..., #此处省略 ..., [232 232 232] [232 232 232] [232 232 232]]]图像矩阵转化成Image对象图像矩阵转化成Image对象通过Image模块的fromarray()方法。import numpy as np from PIL import Image image = Image.open(\"smallpi.jpg\") image_array = np.array(image.convert(\"L\")) image_array = 255 - image_array # 图像矩阵处理，将灰度图像反相 # 反相指的是，黑变白，白变黑 image2 = Image.fromarray(image_array) print(image2) # 结果 <PIL.Image.Image image mode=RGB size=800x450 at 0x4753748>图像的显示图像的显示需要用到matplotlib模块。首先需要导入matplotlib.pyplotimport matplotlib.pyplot as plt然后，调用pyplot的imshow()方法，传入Image对象即可from PIL import Image import matplotlib.pyplot as plt image = Image.open(\"smallpi.jpg\") plt.imshow(image) # 绘制图像image plt.show() # 需要调用show()方法，不然图像只会在内存中而不显示出来图像显示结果（带坐标轴）如果想把坐标轴去掉只需要调用pyplot的axis()方法，传入\"off\"参数from PIL import Image import matplotlib.pyplot as plt image = Image.open(\"smallpi.jpg\") plt.imshow(image) # 绘制图像image plt.axis(\"off\") # 去掉坐标轴 plt.show() # 需要调用show()方法，不然图像只会在内存中而不显示出来图像显示结果（不带坐标轴）如果要显示灰度图像，需要导入matplotlib的cm模块import matplotlib.cm as cm然后在调用pyplot.show()时，传入关键字参数cmap=cm.gray。from PIL import Image import matplotlib.pyplot as plt import matplotlib.cm as cm image = Image.open(\"smallpi.jpg\") # 打开图像 image_gray = image.convert(\"L\") # 转化成灰度图像 plt.subplot(2,1,1) plt.imshow(image_gray) # 没传入关键字参数cmap=cm.gray plt.axis(\"off\") # 去掉坐标轴 plt.subplot(2,1,2) plt.imshow(image_gray, cmap=cm.gray) # 指明 cmap=cm.gray plt.axis(\"off\") # 去掉坐标轴 plt.show() # 显示图像显示结果上：没指定cmap ， 下：指定cmap=cm.gray创建缩略图创建图像缩略图可以通过Image的thumbnail()方法，参数传入一个元组，指明缩略图的大小，如thumbnail((128,128))。例如from PIL import Image image = Image.open(\"smallpi.jpg\") image_thumbnail = image.thumbnail((128,128)) image.save(\"thumbnail.jpg\")结果为复制和粘贴区域复制区域是指截取图像中的一部分，并将这一部分作为一个新的Image对象。复制区域的方法为crop()，参数为一个含4个元素的元组，用来指定截取区域的左上角点和右下角点。from PIL import Image image = Image.open(\"smallpi.jpg\") # 打开图像 box = (300,100,500,300) # 截取区域 image_crop = image.crop(box) # 按指定截取区域对图像进行截取复制 image_crop.save(\"image_crop.jpg\") # 保存保存的截取区域图像为粘贴区域是指在指定图像中放入另一个图像，其方法为paste()。该方法有两个参数，第一个参数为需要粘贴进去的图像，第二个参数为粘贴区域。from PIL import Image image = Image.open(\"smallpi.jpg\") box = (300,100,500,300) # 先截取一部分 image_crop = image.crop(box) # 为了看到粘贴效果，现将截取部分转180度 image_crop = image_crop.transpose(Image.ROTATE_180) # 转180度 image.paste(image_crop,box) # 将转180度后的图像粘贴到原图像 image.save(\"image_paste.jpg\")粘贴后原图像变成图像的尺寸调整和旋转尺寸调整方法为resize()，参数为一元组，指定调整后的大小，如resize((128,128))。图像旋转的方法为rotate()，参数为旋转角度（数值，单位为度）,逆时针方向，如rotate(45)`from PIL import Image image = Image.open(\"smallpi.jpg\") image_resize = image.resize((200,200)) # 尺寸调整 image_rotate = image.rotate(45) # 图像旋转 # image.transpose()也可以旋转图像，但只能旋转90度的整数倍 # 参数为 Image.ROTATE_90 旋转90度 # 180度，270度可类推 image_resize.save(\"image_resize.jpg\") image_rotate.save(\"image_rotate.jpg\")尺寸调整为200*200图像逆时针旋转45度图像直方图图像直方图用来统计图像中像素值的分布情况，即统计不同像素值出现的次数。方法为调用matplotlib.pyplot的hist方法，参数传入图像像素序列和统计区间个数。from PIL import Image import matplotlib.pyplot as plt import matplotlib.cm as cm #打开图像，并转化成灰度图像 image = Image.open(\"smallpi.jpg\").convert(\"L\") image_array = np.array(image) plt.subplot(2,1,1) plt.imshow(image,cmap=cm.gray) plt.axis(\"off\") plt.subplot(2,1,2) plt.hist(image_array.flatten(),256) #flatten可以将矩阵转化成一维序列 plt.show()结果为灰度变换对于一张灰度图像，其每个像素点都用一个0-255之间的值表示，0表示黑色，越接近0越黑；255表示白色，越接近255越白。灰度变换就是通过一个特定的函数，使灰度值从一个值转换成另外一个值。这里列出3种灰度变换1. 【反相】变换后的灰度值= 255−原灰度值2.【转换到100-200】变换后的灰度值 =(原灰度值/255)*100+1003. 【像素平方】变换后的灰度值 = 255*(原灰度值/255)22from PIL import Image import matplotlib.pyplot as plt import matplotlib.cm as cm image = Image.open(\"smallpi.jpg\").convert(\"L\") image_array = np.array(image) x = np.arange(255) # 反相 plt.subplot(3,2,1) plt.plot(x,255-x) # 画出变换函数图像 plt.subplot(3,2,2) plt.imshow(Image.fromarray(255-image_array),cmap=cm.gray) plt.axis(\"off\") # 转换到 100-200 plt.subplot(3,2,3) plt.plot(x,(x/255.0)*100+100) # 画出变换函数图像 plt.subplot(3,2,4) plt.imshow( Image.fromarray((image_array/255.0)*100+100), cmap=cm.gray ) plt.axis(\"off\") # 像素平方 plt.subplot(3,2,5) plt.plot(x,255*(x/255.0)**2) # 画出变换函数图像 plt.subplot(3,2,6) plt.imshow( Image.fromarray(255*(image_array/255.0)**2), cmap=cm.gray ) plt.axis(\"off\") plt.show()`结果如下，（左边是变换函数，右边是图像变换结果）直方图均衡化由上面图像的直方图可以看出，一般情况下，图像上某些灰度值较多，有些灰度值较少，直方图均衡化为的是使灰度值较为均衡。直方图均衡化是利用直方图的累积函数作为灰度变换函数，对图像进行转换。直方图均衡化可以增强图像的对比度。累积函数和概率论中的累积分布函数类似。例如对于还有5个数的序列[1,2,3,4,5],其累积函数含有5个数，第一个数是1，第二个是1+2=3，……，第五个数是1+2+3+4+5=15，所以其累积函数是[1,3,6,10,15]。我们把直方图均衡化的过程封装在一个函数里面，函数名字叫做histeq，输入原图像矩阵和直方图分块数，输出均衡化后的图像矩阵和累积函数。import numpy as np from PIL import Image import matplotlib.pyplot as plt import matplotlib.cm as cm def histeq(image_array,image_bins=256): # 将图像矩阵转化成直方图数据，返回元组(频数，直方图区间坐标) image_array2,bins = np.histogram(image_array.flatten(),image_bins) # 计算直方图的累积函数 cdf = image_array2.cumsum() # 将累积函数转化到区间[0,255] cdf = (255.0/cdf[-1])*cdf # 原图像矩阵利用累积函数进行转化，插值过程 image2_array = np.interp(image_array.flatten(),bins[:-1],cdf) # 返回均衡化后的图像矩阵和累积函数 return image2_array.reshape(image_array.shape),cdf image = Image.open(\"pika.jpg\").convert(\"L\") image_array = np.array(image) plt.subplot(2,2,1) plt.hist(image_array.flatten(),256) plt.subplot(2,2,2) plt.imshow(image,cmap=cm.gray) plt.axis(\"off\") a = histeq(image_array) # 利用刚定义的直方图均衡化函数对图像进行均衡化处理 plt.subplot(2,2,3) plt.hist(a[0].flatten(),256) plt.subplot(2,2,4) plt.imshow(Image.fromarray(a[0]),cmap=cm.gray) plt.axis(\"off\") plt.show()结果如下图所示，第一行为原图像直方图和原图像，第二行为均衡化后的直方图和图像。可以看出均衡化后图像对比度增强了，原先灰色区域的细节变得清晰。"}
{"content2":"最近在几个地方都看到有人问C++下用什么矩阵运算库比较好，顺便做了个调查，做一些相关的推荐吧。主要针对稠密矩阵，有时间会再写一个稀疏矩阵的推荐。Armadillo：C++下的Matlab替代品地址：http://arma.sourceforge.net/许可证：MPL 2.0目前使用比较广的C++矩阵运算库之一，是在C++下使用Matlab方式操作矩阵很好的选择，许多Matlab的矩阵操作函数都可以找到对应，这对习惯了Matlab的人来说实在是非常方便，另外如果要将Matlab下做研究的代码改写成C++，使用Armadillo也会很方便，这里有一个简易的Matlab到Armadillo的语法转换。下面列了一些Armadillo的特性：支持整数，浮点数，和复数矩阵。支持矩阵逐元素操作，包括abs · conj · conv_to · eps · imag/real · misc functions (exp, log, pow, sqrt, round, sign, ...) · trigonometric functions (cos, sin, ...)等等。支持矩阵分块操作。支持对整体矩阵的操作diagvec · min/max · prod · sum · statistics (mean, stddev, ...) · accu · as_scalar · det · dot/cdot/norm_dot · log_det · norm · rank · trace等等。Matlab用户，你甚至可以找到你熟悉的hist · histc · unique · cumsum · sort_index · find · repmat · linspace等函数。除了自带的矩阵基本运算之外，可自动检测是否安装有BLAS，或更快的 OpenBLAS, Intel MKL, AMD ACML，并使用他们替代自带基本运算实现。提供接口使用LAPACK进行矩阵分解运算，svd · qr · lu · fft等等。提供了稀疏矩阵类，支持常用操作，但暂时没有矩阵分解的实现。更新比较活跃，有一些计算机视觉、机器学习、物理方面的开源项目在使用，比如MLPACK （Machine Learning Library）。总体来讲很好用的矩阵库，速度上因为可以使用OpenBLAS等库进行加速，因此还是不错的。网上可以找到一个叫 Nghia Ho的人写的关于和eigen及opencv的速度比较做参考，速度略优。不过也由于依赖LAPACK等库完成矩阵分解计算，在windows上运行可能会比较痛苦。Eigen3：强大且只需头文件地址：http://eigen.tuxfamily.org/许可证：主要为MPL 2.0，部分有来自第三方的代码为LGPL非常强大的矩阵运算库，我一直在用，大家用了都说好。使用类似Matlab的方式操作矩阵，可以在这里查看官方的与Maltab的对应关系，个人感觉单纯讲和Matlab的对应的话，可能不如Armadillo对应的好，但功能绝对强大。Eigen包含了绝大部分你能用到的矩阵算法，同时提供许多第三方的接口。Eigen一个重要特点是没有什么依赖的库，本身仅有许多头文件组成，因此非常轻量而易于跨平台。你要做的就是把用到的头文件和你的代码放在一起就可以了。Eigen的一些特性：支持整数、浮点数、复数，使用模板编程，可以为特殊的数据结构提供矩阵操作。比如在用ceres-solver进行做优化问题（比如bundle adjustment）的时候，有时候需要用模板编程写一个目标函数，ceres可以将模板自动替换为内部的一个可以自动求微分的特殊的double类型。而如果要在这个模板函数中进行矩阵计算，使用Eigen就会非常方便。支持逐元素、分块、和整体的矩阵操作。内含大量矩阵分解算法包括LU，LDLt，QR、SVD等等。支持使用Intel MKL加速部分功能支持多线程稀疏矩阵支持良好，到今年新出的Eigen3.2，已经自带了SparseLU、SparseQR、共轭梯度（ConjugateGradient solver）、bi conjugate gradient stabilized solver等解稀疏矩阵的功能。同时提供SPQR、UmfPack等外部稀疏矩阵库的接口。支持常用几何运算，包括旋转矩阵、四元数、矩阵变换、AngleAxis（欧拉角与Rodrigues变换）等等。更新活跃，用户众多（Google、WilliowGarage也在用），使用Eigen的比较著名的开源项目有ROS（机器人操作系统）、PCL（点云处理库）、Google Ceres（优化算法）。OpenCV自带到Eigen的接口。总体来讲，如果经常做一些比较复杂的矩阵计算的话，或者想要跨平台的话，非常值得一用。OpenCV：方便的计算机视觉计算库地址：http://opencv.org/许可证：目前是BSDOpenCV在计算机视觉领域名气实在是太大了，而且最近几年库里的算法开始爆炸式的增长，最近貌似计划推出OpenCV 3了（参见这里）。有人开始抱怨OpenCV现在内容太杂了，不过这样的好处就是从是研究开发的话，一个库就可以得到大部分计算机视觉的流行算法，省去了很多麻烦。OpenCV自带的矩阵计算功能算不上是专业的矩阵计算库，但是如果你用C++写机器学习、计算机视觉的程序，一定会经常用到。OpenCV的矩阵计算功能还算比较完善，虽然速度略差劲，但用在大多数的开发和研究领域也是足够了。特别是OpenCV 2.2之后提供了类Matlab的矩阵C++接口，使得直接使用OpenCV进行矩阵计算变得简单易用。列两个比较值得一提的特点：大量计算机视觉、机器学习相关的矩阵操作，非常方便。比如PCA、LDA、三维空间投影等等。自带并行加速的矩阵计算功能。其中cv::gpu模块提供了CUDA支持的GPU矩阵计算功能，cv::ocl模块提供了OpenCL支持的并行矩阵计算功能。可以非常方便的进行并行矩阵计算，不过不足的是这两个模块还暂时缺少矩阵分解的实现。我曾见过朋友在做深度学习的时候用cv::ocl模块进行大矩阵乘法，貌似效果还不错。ViennaCL：并行矩阵计算网址：http://viennacl.sourceforge.net/许可证：MIT作者Karl Rupp来自维也纳大学，开发了一套Vienna*系列的开源软件，其中还包括ViennaMath（symbolic math符号计算）、ViennaFEM（有限元）等等。ViennaCL在后台支持OpenCL、OpenMP和CUDA，可以方便地使用各种型号的CPU或GPU进行并行计算。目前矩阵类型支持float和double，1.4.2版尚不支持复数矩阵。支持常用的矩阵运算和分解。接口很有好，并提供接口到uBLAS、Eigen、MTL 4等矩阵库。安装使用有非常详细的官方文档。PETSc：大规模并行科学计算网站：http://www.mcs.anl.gov/petsc/许可证：Copyright University of Chicago (GPL compatible)PETSc(Portable, Extensible Toolkit for Scientific Computation)  是美国能源部ODE2000支持开发的20多个ACTS工具箱之一，由Argonne国家实验室开发的可移植可扩展科学计算工具箱，主要用于在分布式存储环境高效求解偏微分方程组及相关问题。PETSc所有消息传递通信均采用MPI标准实现。线性方程组求解器是PETSc的核心组件之一，PETSc几乎提供了所有求解线性方程组的高效求解器，既有串行求解也有并行求解，既有直接法求解也有迭代法求解。对于大规模线性方程组， PETSc提供了大量基于Krylov子空间方法和各种预条件子的成熟而有效的迭代方法，以及其他通用程序和用户程序的接口。PETSc具有一般库软件所具备的高性能、可移植等优点，而且面向对象技术使得PETSc内部功能部件的使用非常方便，接口简单而又适用面广，可以缩短开发周期，减少工作量。[直接粘百度百科了]。PETSc在网上可一找到很多英文资料，使用也比较广泛。不过在学校实验室的一般的科学计算可能接触的还比较少。推荐一个YouTube（可能要FQ）的五集PETSc简单入门《PRACE Video Tutorial - PETSc Tutorial》。其他的矩阵计算库和资料在Stackexchange上有一个帖子《Recommendations for a usable, fast C++ matrix library?》里面搜罗了许多矩阵运算库。另外INRIA有人写了一个文档《Linear Algebra Libraries》，对常见的矩阵运算库进行了总结。除了上面提到的几个库之外，下面还有一些比较常用或坚持更新的矩阵库：uBLAS：Boost包中的BLAS库接口，据说速度一般。GSL：GNU Scientific Library自带的矩阵运算，据说速度一般。MTL 4：Matrix Template Library version 4，类似Eigen和Armadillo，有开源版本。Trilinos：和PETSc同是美国能源部ODE2000支持开发的20多个ACTS工具箱之一，用于大规模计算。来自http://cvnote.infoArmadillo, C++, Eigen3, Matrix, OpenCV, PETSc, Scientific Computing, ViennaCL, 矩阵分解, 矩阵计算, 科学计算Address: http://cvnote.info/cpp-matrix-library/"}
{"content2":"现在计算机视觉领域深度学习已经成为主流，我在美读研的时候，深度学习并未取得大的突破，当时流行的图像识别分类器多采用手工设计特征+编码+SVM（支持向量机）框架下的算法，终于到了2012年（我刚毕业），在ILSVRC上，alexnet的横空出世，将分类错误率从之前的25.7%降到了15.3%，这只是一个5卷积层+2全连接层的卷积神经网络，却一下取得了10%的突破，这是深度学习在CV领域的一次翻身仗，自此以后，ImageNet的参赛者几乎全体转向了基于卷积神经网络的深度学习算法。基于此的应用如雨后春笋般一夜之间全冒了出来。深度学习尤其是卷积神经网络就如同万能的大杀器，在计算机视觉的各个领域开始发挥作用。cuda的应用在这一进程中起到了毋庸置疑的作用，所以想要在这一领域开展工作，使用正确的硬件和计算框架是很重要的，本文提供了一个快速搭建MXNET环境的方案，虽然TensorFlow和Caffe是更加主流，但是MXnet作为Amazon主推的深度学习平台，具有轻量级，高性能等特点，并且支持多种语言，可以说是一个非常具有潜力的框架。我的硬件环境是一台GPU服务器，有2块tesla p100进行GPU运算，对于一般人来说，你的硬件至少需要一个支持cuda的英伟达显卡，并且安装了任意linux环境，常规安装教程很多，但都很繁琐，由于我是用docker进行部署，所以可以非常快速的进行环境搭建，会用docker并且想省去安装过程的同学可以参考以下教程。前提是你的linux环境中已经安装了docker，nvidia cuda驱动，NVIDIA docker等，这些步骤在网上有很多的教程，在此不表获取镜像https://hub.docker.com/search/?isAutomated=0&isOfficial=0&page=1&pullCount=0&q=mxnet&starCount=0   在docker官方取得你需要镜像的信息，并执行docker pull命令docker pull mxnet/python：你的版本tag。如果镜像下载速度慢，可以搜一些国内镜像服务器的地址，例如 docker pull registry.docker-cn.com/mxnet/python:1.3.0_gpu_cu90_mkl随后启动该镜像：启动镜像执行docker ps -a ，获取你刚pull的镜像的 id，即下文命令 commit后面的字段给你的镜像取一个名字docker commit 90b97b96394a example:0.1随后启动该镜像，如果需要进行外部访问，比如启动一个jupyter在浏览器中进行编码，需要配置端口映射，全部命令如下：docker run -itd --runtime=nvidia  --name=example -p 19999:8888 -p 16007:6006 example:0.2 /bin/bash这时镜像应该已经启动，使用docker ps -a 指令查看容器运行状态，如果正常运行，即可进入容器。执行命令    docker exec -it example bash 进入容器，此时已经配好了mxnet环境，进入python，随便敲几行代码进行测试。我在实际使用过程中，发现该容器缺了很多基础组件，如python-dev ，matplotlib等，甚至连vi/vim都没有，所以要安装jupyter的话，注意添加相关依赖，如果报错缺包，安装这些包即可。 jupyter安装教程也很多，在此不表。启动jupyter后，即可愉快的进行各种实验啦~"}
{"content2":"Reading papers_10(人体行为识别特征点提取小综述)这是本学期一门课程的论文。(注：本人看过的行为识别特征提取方面的文章就10来篇，所以本综述大部分内容是参考其他人的综述的，有些并不是自己的成果，个人功底还没这么雄厚…)行为识别特征提取综述摘要人体行为识别目前处在动作识别阶段，而动作识别可以看成是特征提取和分类器设计相结合的过程。特征提取过程受到遮挡，动态背景，移动摄像头，视角和光照变化等因素的影响而具有很大的挑战性。本文将较全面的总结了目前行为识别中特征提取的方法，并将其特征划分为全局特征和局部特征，且分开介绍了其优缺点。关键字： 行为识别 特征提取 全局特征 局部特征1. 前言如今人体行为识别是计算机视觉研究的一个热点，人体行为识别的目标是从一个未知的视频或者是图像序列中自动分析其中正在进行的行为。简单的行为识别即动作分类，给定一段视频，只需将其正确分类到已知的几个动作类别，复杂点的识别是视频中不仅仅只包含一个动作类别，而是有多个，系统需自动的识别出动作的类别以及动作的起始时刻。行为识别的最终目标是分析视频中哪些人在什么时刻什么地方，在干什么事情，即所谓的“W4系统”。下面将4个方面对行为识别做初步介绍。1.1    行为识别应用背景人体行为识别应用背景很广泛，主要集中在智能视频监控，病人监护系统，人机交互，虚拟现实，智能家居，智能安防，运动员辅助训练，另外基于内容的视频检索和智能图像压缩等有着广阔的应用前景和潜在的经济价值和社会价值，其中也用到了不少行为识别的方法。1.2    行为识别研究历史行为识别分析的相关研究可以追溯到1975年Johansson[1]的一个实验，作者提出了12点人体模型,这种描述行为的点模型方法对后来基于人体结构的行为描述算法起到了重要的指导作用。从那以后，行为识别的研历史究进展大致可以分为以下3个阶段，第1个是20世纪70年代行为分析的初步研究阶段，第2个是20世纪90年代行为分析的逐步发展阶段，第3个是最近几年来行为分析的快速发展阶段。从文献[2]~[7]这6篇较有名的行为识别综述论文可以看出, 研究行为识别的人数在不断增加，论文数量也是猛增，并且产生了许多种重要的算法和思想。1.3    行为识别方法分类体系关于视觉上人体运动分析和识别的方法论体系有很多种。Forsyth[8]等人侧重与将动作从视频序列中人的姿态和运动信息恢复过来，这属于一个回归问题，而人体行为识别是一个分类问题，这2个问题有很多类似点，比如说其特征的提取和描述很多是通用的。Turaga[5]等人将人体行为识别分为3部分，即移动识别(movement),动作识别(action)和行为识别(activity)，这3种分类分别于低层视觉，中层视觉，高层视觉相对应。Gavrila[9]采用2D和3D的方法来分别研究人体的行为。对于行为识别方法论的划分中，最近出现了一种新的划分[7], Aggarwal将人体行为研究分为2大类，其一是基于单个层次来实现，其二是基于等级体系来实现。单层实现由分为时空特征和序列特征2种，等级体系实现分为统计方法，句法分析法和基于描述的方法3种。Aggarwal对行为识别方法论体系的层次结构图。该分类体系比较完善，也能很好的体现目前的研究进展。按照Turaga的3个层次划分理论，目前关于行为识别基本上还停留在第二个阶段，即action识别。而action识别比现实生活中的行为较简单，所以我们识别这些行为只需对这些行为进行正确的分类即可。这样一个行为识别系统就分成了行为特征提取和分类器的设计两个方面，通过对训练数据提取某种特征，采用有监督或无监督来训练一个分类模型，对新来的数据同样提取特征并送入该模型，得出分类结果。基于这个思想，本文主要是从行为识别的特征提取方面做了一个较为全面的介绍。1.4    行为识别研究难点行为识别发展至今，取得了很大的进展，在低层，中层和高层都取得了一定的突破，但是行为识别算法并不成熟，目前不存在一个算法适合所有的行为分类，3个视觉层次中都还有很多严峻的问题有待解决。其研究的难点主要体现在以下几个方面：1.4.1  动作类内类间的变化太大对于大多数的动作，即使是同一动作都有不同的表现形式。比如说走路，可以在不同的背景环境中完成，走路的速度也可以从慢到快，走路的步长亦有长有短。其它的动作也有类似的结果，特别是一些非周期的运动，比如过马路时候的走路，这与平时周期性的走路步伐明显不同。由此可见，动作的种类本身就很多，再加上每一种类又有很多个变种，所以给行为识别的研究带来了不少麻烦。1.4.2  环境背景等影响环境问背景等因素的影响可谓是计算机视觉各个领域的最大难点。主要有视角的多样性，同样的动作从不同的视角来观察会得到不同的二维图像；人与人之间，人与背景之间的相互遮挡也使计算机对动作的分类前期特征提取带来了困难，目前解决多视觉和遮挡问题，有学者提出了多摄像机融合通过3维重建来处理；另外其影响因素还包括动态变化和杂乱的背景，环境光照的变化，图像视频的低分辨率等。1.4.3  时间变化的影响总所周知，人体的行为离不开时间这个因素。而我们拍摄的视频其存放格式有可能不同，其播放速度有慢有快，这就导致了我们提出的系统需对视频的播放速率不敏感。1.4.4  数据的获取和标注既然把行为识别问题当成一个分类问题，就需要大量的数据来训练分类模型。而这些数据是视频数据，每一个动作在视频中出现的位置和时间都不确定，同时要考虑同一种动作的不同表现形式以及不同动作之间的区分度，即数据的多样性和全面性。这一收集过程的工作量不小，网上已经有一些公开的数据库供大家用来实验，这将在本文的第3部分进行介绍。另外，手动对视频数据标注非常困难。当然，有学者也提出了一些自动标注的方法，比如说利用网页图片搜索引擎[10],利用视频的字幕[11]，以及利用电影描述的文本进行匹配[12][13][14]。1.4.5  高层视觉的理解上面一提到，目前对行为识别的研究尚处在动作识别这一层(action  recognition)。其处理的行为可以分为2类，一类是有限制类别的简单规则行为，比如说走、跑、挥手、弯腰、跳等。另一类是在具体的场景中特定的行为[15]~[19]，如检测恐怖分子异常行为，丢包后突然离开等。在这种场景下对行为的描述有严格的限制，此时其描述一般采用了运动或者轨迹。这2种行为识别的研究都还不算完善，遇到了不少问题，且离高层的行为识别要求还相差很远。因此高层视觉的理解表示和识别是一个巨大的难题。2. 行为识别特征提取这一节中，将主要讨论怎样从图片序列中提取特征。本文将行为识别的特征分为2大类：全局特征和局部特征。全局特征是把一对象当做成一个整体，这是一种从上到下的研究思维。这种情况下，视频中的人必须先被定位出来，这个可以采用背景减图或者目标跟踪算法。然后对定位出来的目标进行某种编码，这样就形成了其全局特征。这种全局特征是有效的，因为它包含了人体非常多的信息。然而它又太依赖而底层视觉的处理，比如说精确的背景减图，人体定位和跟踪。而这些处理过程本身也是计算机视觉中的难点之处。另外这些全局特征对噪声，视角变化，遮挡等非常敏感。局部特征提取是收集人体的相对独立的图像块，是一种从下到上的研究思维。一般的做法是先提取视频中的一些时空兴趣点，然后在这些点的周围提取相应的图像块，最后将这些图像块组合成一起来描述一个特定的动作。局部特征的优点是其不依赖而底层的人体分割定位和跟踪，且对噪声和遮挡问题不是很敏感。但是它需要提取足够数量的稳定的且与动作类别相关的兴趣点，因此需要不少预处理过程。2.1    全局特征提取全局特征是对检测出来的整个感兴趣的人体进行描述，一般是通过背景减图或者跟踪的方法来得到，通常采用的是人体的边缘，剪影轮廓，光流等信息。而这些特征对噪声，部分遮挡，视角的变化比较敏感。下面分别从其二维特征和三维特征做介绍。2.1.1 二维全局特征提取Davis[20]等人最早采用轮廓来描述人体的运动信息，其用MEI和MHI 2个模板来保存对应的一个动作信息，然后用马氏距离分类器来进行识别。MEI为运动能量图，用来指示运动在哪些部位发生过，MHI为运动历史图，除了体现运动发生的空间位置外还体现了运动的时间先后顺序。这2种特征都是从背景减图中获取的。是坐下，挥手，蹲伏这3个动作的运动历史图MHI。为了提前剪影信息，Wang[21]等人利用r变换获取了人体的剪影。Hsuan-Shen[22]则提取了人体的轮廓，这些轮廓信息是用星型骨架描述基线之间夹角的，这些基线是从人体的手，脚，头等中心延长到人体的轮廓。而Wang[23]同时利用了剪影信息和轮廓信息来描述动作，即用基于轮廓的平均运动形状(MMS)和基于运动前景的平均能量(AME)两个模板来进行描述。当把轮廓和剪影模板保存下来后，新提取出的特征要与其进行比较，Daniel[24]采用欧式距离来测量其相似度，随后他又改为用倒角距离来度量[25],这样就消除了背景减图这一预处理步骤。除了利用轮廓剪影信息外，人体的运动信息也经常被采用。比如说基于像素级的背景差法，光流信息等。当背景差法不能很好的工作时，我们往往可以采用光流法，但是这样经常会引入运动噪声，Effos[26]只计算以人体中心点处的光流，这在一定程度上减少了噪声的影响。2.1.2  三维全局特征提取在三维空间中，通过给定视频中的数据可以得到3D时空体(STV)，STV的计算需要精确的定位，目标对齐，有时还需背景减图。Blank[27][28]等人首次从视频序列中的剪影信息得到STV。如所示。然后用泊松方程导出局部时空显著点及其方向特征，其全局特征是通过对这些局部特征加权得到的，为了处理不同动作的持续时间不同的问题，Achard[29]对每一个视频采用了一系列的STV ,并且每个STV只是覆盖时间维上的一部分信息。还有一种途径是从STV中提取相应的局部描述子，这一部分将在局部特征提取一节中介绍，在这里，我们还是先把STV特征当做是全局特征。Batra[30]存储了STV的剪影，并且用很小的3D二进制空间块来采样STV。Yilmaz[31]提取了STV表面的不同几何特征，比如说其极大值点和极小值点。当然，也有学者Keel[32]将剪影的STV和光流信息结合起来，作为行为识别的全局特征。2.2    局部特征提取人体行为识别局部特征提取是指提取人体中感兴趣的点或者块。因此不需要精确的人体定位和跟踪，并且局部特征对人体的表观变化，视觉变化和部分遮挡问题也不是很敏感。因此在行为识别中采用这种特征的分类器比较多。下面从局部特征点检测和局部特征点描述2部分来做介绍。2.2.1  局部特征点的检测行为识别中的局部特征点是视频中时间和空间中的点，这些点的检测发生在视频运动的突变中。因为在运动突变时产生的点包含了对人体行为分析的大部分信息。因此当人体进行平移直线运动或者匀速运动时，这些特征点就很难被检测出来。Laptev[33]将Harris角点扩展到3D Harris，这是时空兴趣点(STIP)族中的一个。这些时空特征点邻域的像素值在时间和空间都有显著的变化。在该算法中，邻域块的尺度大小能够自适应时间维和空间维。该时空特征点如所示。Dollar[34]指出上述那种方法存在一个缺点，即检测出来稳定的兴趣点的数量太少，因此Dollar单独的在时间维和空间维先采用gabor滤波器进行滤波，这样的话检测出来兴趣点的数目就会随着时间和空间的局部邻域尺寸的改变而改变。类似的，Rapantzikos[35]在3个维度上分别应用离散小波变换，通过每一维的低通和高通的滤波响应来选择时空显著点。同时，为了整合颜色和运动信息，Rapantzikos[36]加入了彩色和运动信息来计算其显著点。与检测整个人体中兴趣点的出发思路不同，Wong[37]首先检测与运动相关的子空间中的兴趣点，这些子空间对应着一部分的运动，比如说手臂摆动，在这些子空间中，一些稀疏的兴趣点就被检测出来了。类似的方法，Bregonzio[38]首先通过计算后面帧的不同来估计视觉注意的焦点，然后利用gabor滤波在这些区域来检测显著点。2.2.2  局部特征点的描述局部特征描述是对图像或者视频中的一个块进行描述，其描述子应该对背景的杂乱程度，尺度和方向变化等均不敏感。一个图像块的空间和时间尺寸大小通常取决于检测到的兴趣点的尺寸。显示的是cuboids描述子[34]。cuboids描述子特征块也可以用基于局部特征的网格来描述，因为一个网格包括了局部观察到的领域像素，将其看成一个块，这样就减少了时间和空间的局部变化的影响。二维的SURF特征[39]被Willems[40]扩展到了3维，这些eSURF特征的每个cell都包含了全部Harr-wavelet特征。Laotev[14]使用了局部HOG(梯度直方图)和HOF(光流直方图)。Klaser[41]将HOG特征扩展到3维，即形成了3D-HOG。3D-HOG的每个bin都是由规则的多面体构成，3D-HOG允许 在多尺度下对cuboids进行快速密度采样。这种将二维特征点检测的算法扩展到3维特征点类似的工作还有是将SIFT算法[42]扩展到3维SIFT Scovanner[43]。在Wang[44]的文章中，他比较了各种局部描述算子，并发现在大多数情况下整合了梯度和光流信息的描述算子其效果最好。另外还有一种描述子比较流行，即单词袋[45][46]，这是利用的单词频率直方图特征。2.3    全局、局部特征融合全局和局部特征的融合，结合了全局特征的足够信息量和局部特征的对视角变化，部分遮挡问题不敏感，抗干扰性强的优点。这样的文章比较多，其主要思想结合从2.1和2.2的方法。Thi[47]就将这2种特征结合得很好，其全局特征是采用前面介绍的MHI算子，并且采用AIFT算法[48]进一步选择更好的MHI。局部特征也是采用前面提到的STIP特征，并且采用SBFC(稀疏贝叶斯特征选择)[49]算法过滤掉一些噪声比较大的特征点。最后将2种特征送入到扩展的3维ISM模型中，其ISM[50]是一种目标识别常用算法，即训练出目标的隐式形状模型。Thi[47]的方法结构如所示。3. 行为识别常见数据库3.1    WeizmannWeizmann[27]数据库包含了10个动作分别是走，跑，跳，飞跳，向一侧移动，单只手挥动，2只手挥动，单跳，2只手臂挥动起跳,每个动作有10个人执行。在这个视频集中，其背景是静止的，且前景提供了剪影信息。该数据集较为简单。3.2    KTHKTH[45]行人数据库包含了6种动作，分别为走，慢跑，跑挥手和鼓掌。每种动作由25个不同的人完成。每个人在完成这些动作时又是在4个不同的场景中完成的，4个场景分别为室外，室内，室外放大，室外且穿不同颜色的衣服。3.3    PETSPETS[51]，其全称为跟踪与监控性能评估会议，它的数据库是从现实生活中获取的，主要来源于直接从视频监控系统拍摄的视频，比如说超市的监控系统。从2000年以后，基本上每年都会组织召开这个会议。3.4    UCFUCF包含个数据集，这里是指UCF的运动数据库[52],该视频数据包括了150个视频序列，共有13个动作。因为是现实生活中的视频数据，所以其背景比较复杂，这些种类的动作识别起来有些困难。3.5    INRIA XMASINRIA XMAS数据库[53]是从5个视角拍摄的，室内的4个方向和头顶的1个方向。总共有11个人完成14种不同的动作，动作可以沿着任意方向执行。摄像机是静止的，环境的光照条件也基本不变。另外该数据集还提供有人体轮廓和体积元等信息。3.6    HollywoodHollywood电影的数据库包含有几个，其一[14]的视频集有8种动作，分别是接电话，下轿车，握手，拥抱，接吻，坐下，起立，站立。这些动作都是从电影中直接抽取的，由不同的演员在不同的环境下演的。其二[54]在上面的基础上又增加了4个动作，骑车，吃饭，打架，跑。并且其训练集给出了电影的自动描述文本标注，另外一些是由人工标注的。因为有遮挡，移动摄像机，动态背景等因素，所以这个数据集非常有挑战。4. 总结本文较全面的介绍了行为识别中特征提取的方法，并将其分为全局特征提取和局部特征提取2个部分介绍，虽然自行为识别研究以来已经取得了不少成果，但是由于视觉中的动态环境，遮挡等问题存在，其挑战非常大，需要提取出鲁棒性更好，适应性更强，效果更好的特征，而这仍是后面几年甚至几十年不断追求努力才能达到的目标。参考文献：Johansson, G. (1975). \"Visual motion perception.\" Scientific American.Aggarwal, J. K. and Q. Cai (1997). Human motion analysis: A review, IEEE.Moeslund, T. B. and E. Granum (2001). \"A survey of computer vision-based human motion capture.\" Computer vision and image understanding 81(3): 231-268.Moeslund, T. B., A. Hilton, et al. (2006). \"A survey of advances in vision-based human motion capture and analysis.\" Computer vision and image understanding 104(2): 90-126.Turaga, P., R. Chellappa, et al. (2008). \"Machine recognition of human activities: A survey.\" Circuits and Systems for Video Technology, IEEE Transactions on 18(11): 1473-1488.Poppe, R. (2010). \"A survey on vision-based human action recognition.\" Image and Vision Computing 28(6): 976-990.Aggarwal, J. and M. S. Ryoo (2011). \"Human activity analysis: A review.\" ACM Computing Surveys (CSUR) 43(3): 16.Forsyth, D. A., O. Arikan, et al. (2006). Computational studies of human motion: Tracking and motion synthesis, Now Pub.Gavrila, D. M. (1999). \"The visual analysis of human movement: A survey.\" Computer vision and image understanding73(1): 82-98.10. Ikizler-Cinbis, N., R. G. Cinbis, et al. (2009). Learning actions from the web, IEEE.11. Gupta, S. and R. J. Mooney (2009). Using closed captions to train activity recognizers that improve video retrieval, IEEE.12. Cour, T., C. Jordan, et al. (2008). Movie/script: Alignment and parsing of video and text transcription.13. Duchenne, O., I. Laptev, et al. (2009). Automatic annotation of human actions in video, IEEE.14. Laptev, I., M. Marszalek, et al. (2008). Learning realistic human actions from movies, IEEE.15. Haritaoglu, I., D. Harwood, et al. (1998). \"W 4 S: A real-time system for detecting and tracking people in 2 1/2D.\" Computer Vision—ECCV'98: 　　　　　877-892.16. Tao, D., X. Li, et al. (2006). Human carrying status in visual surveillance, IEEE.17. Davis, J. W. and S. R. Taylor (2002). Analysis and recognition of walking movements, IEEE.18. Lv, F., X. Song, et al. (2006). Left luggage detection using bayesian inference.19. Auvinet, E., E. Grossmann, et al. (2006). Left-luggage detection using homographies and simple heuristics.20. Bobick, A. F. and J. W. Davis (2001). \"The recognition of human movement using temporal templates.\" Pattern Analysis and Machine Intelligence, 　　　　   IEEE Transactions on 23(3): 257-267.21. Wang, Y., K. Huang, et al. (2007). Human activity recognition based on r transform, IEEE.22. Chen, H. S., H. T. Chen, et al. (2006). Human action recognition using star skeleton, ACM.23. Wang, L. and D. Suter (2006). Informative shape representations for human action recognition, Ieee.24. Weinland, D., E. Boyer, et al. (2007). Action recognition from arbitrary views using 3d exemplars, IEEE.25. Weinland, D. and E. Boyer (2008). Action recognition using exemplar-based embedding, Ieee.26. Efros, A. A., A. C. Berg, et al. (2003). Recognizing action at a distance, IEEE.27. Blank, M., L. Gorelick, et al. (2005). Actions as space-time shapes, IEEE.28. Gorelick, L., M. Blank, et al. (2007). \"Actions as space-time shapes.\" Pattern Analysis and Machine Intelligence, IEEE Transactions on 29(12): 　　　　　　   2247-2253.29. Achard, C., X. Qu, et al. (2008). \"A novel approach for recognition of human actions with semi-global features.\" Machine Vision and Applications 　　　　   19(1): 27-34.30. Batra, D., T. Chen, et al. (2008). Space-time shapelets for action recognition, IEEE.31. Yilmaz, A. and M. Shah (2008). \"A differential geometric approach to representing the human actions.\" Computer vision and image understanding 　　        109(3): 335-351.32. Ke, Y., R. Sukthankar, et al. (2007). Spatio-temporal shape and flow correlation for action recognition, IEEE.33. Laptev, I. (2005). \"On space-time interest points.\" International journal of computer vision 64(2): 107-123.34. Dollár, P., V. Rabaud, et al. (2005). Behavior recognition via sparse spatio-temporal features, IEEE.35. Rapantzikos, K., Y. Avrithis, et al. (2007). Spatiotemporal saliency for event detection and representation in the 3D wavelet domain: potential in 　　　　   human action recognition, ACM.36. Rapantzikos, K., Y. Avrithis, et al. (2009). Dense saliency-based spatiotemporal feature points for action recognition, Ieee.37. Wong, S. F. and R. Cipolla (2007). Extracting spatiotemporal interest points using global information, IEEE.38. Bregonzio, M., S. Gong, et al. (2009). Recognising action as clouds of space-time interest points, IEEE.39. Bay, H., T. Tuytelaars, et al. (2006). \"Surf: Speeded up robust features.\" Computer Vision–ECCV 2006: 404-417.40. Willems, G., T. Tuytelaars, et al. (2008). \"An efficient dense and scale-invariant spatio-temporal interest point detector.\" Computer Vision–ECCV 　　　　   2008: 650-663.41. Klaser, A. and M. Marszalek (2008). \"A spatio-temporal descriptor based on 3D-gradients.\"42. Mikolajczyk, K. and C. Schmid (2004). \"Scale & affine invariant interest point detectors.\" International journal of computer vision 60(1): 63-86.43. Scovanner, P., S. Ali, et al. (2007). A 3-dimensional sift descriptor and its application to action recognition, ACM.44. Wang, H., M. M. Ullah, et al. (2009). \"Evaluation of local spatio-temporal features for action recognition.\"45. Niebles, J. C., H. Wang, et al. (2008). \"Unsupervised learning of human action categories using spatial-temporal words.\" International journal of 　　　　   computer vision 79(3): 299-318.46. Schuldt, C., I. Laptev, et al. (2004). Recognizing human actions: A local SVM approach, IEEE.47. Thi, T. H., L. Cheng, et al. (2011). \"Integrating local action elements for action analysis.\" Computer vision and image understanding.48. Liu, G., Z. Lin, et al. (2009). \"Radon representation-based feature descriptor for texture classification.\" Image Processing, IEEE Transactions on 　　　　   18(5): 921-928.49. Carbonetto, P., G. Dorkó, et al. (2008). \"Learning to recognize objects with little supervision.\" International journal of computer vision 77(1): 219-　　　　   237.50. Leibe, B., A. Leonardis, et al. (2008). \"Robust object detection with interleaved categorization and segmentation.\" International journal ofcomputer vision 77(1): 259-289.51. http://www.cvg.rdg.ac.uk/slides/pets.html.52. Rodriguez, M. D. (2008). \"Action mach a spatio-temporal maximum average correlation height filter for action recognition.\" CVPR.53. Weinland, D., R. Ronfard, et al. (2006). \"Free viewpoint action recognition using motion history volumes.\" Computer vision and imageunderstanding 104(2): 249-257.54. Marszalek, M., I. Laptev, et al. (2009). Actions in context, IEEE.作者：tornadomeet 出处：http://www.cnblogs.com/tornadomeet 欢迎转载或分享，但请务必声明文章出处。 （新浪微博：tornadomeet,欢迎交流！） 这是本学期一门课程的论文。(注：本人看过的行为识别特征提取方面的文章就10来篇，所以本综述大部分内容是参考其他人的综述的，有些并不是自己的成果，个人功底还没这么雄厚…)行为识别特征提取综述摘要人体行为识别目前处在动作识别阶段，而动作识别可以看成是特征提取和分类器设计相结合的过程。特征提取过程受到遮挡，动态背景，移动摄像头，视角和光照变化等因素的影响而具有很大的挑战性。本文将较全面的总结了目前行为识别中特征提取的方法，并将其特征划分为全局特征和局部特征，且分开介绍了其优缺点。关键字： 行为识别 特征提取 全局特征 局部特征1. 前言如今人体行为识别是计算机视觉研究的一个热点，人体行为识别的目标是从一个未知的视频或者是图像序列中自动分析其中正在进行的行为。简单的行为识别即动作分类，给定一段视频，只需将其正确分类到已知的几个动作类别，复杂点的识别是视频中不仅仅只包含一个动作类别，而是有多个，系统需自动的识别出动作的类别以及动作的起始时刻。行为识别的最终目标是分析视频中哪些人在什么时刻什么地方，在干什么事情，即所谓的“W4系统”。下面将4个方面对行为识别做初步介绍。1.1    行为识别应用背景人体行为识别应用背景很广泛，主要集中在智能视频监控，病人监护系统，人机交互，虚拟现实，智能家居，智能安防，运动员辅助训练，另外基于内容的视频检索和智能图像压缩等有着广阔的应用前景和潜在的经济价值和社会价值，其中也用到了不少行为识别的方法。1.2    行为识别研究历史行为识别分析的相关研究可以追溯到1975年Johansson[1]的一个实验，作者提出了12点人体模型,这种描述行为的点模型方法对后来基于人体结构的行为描述算法起到了重要的指导作用。从那以后，行为识别的研历史究进展大致可以分为以下3个阶段，第1个是20世纪70年代行为分析的初步研究阶段，第2个是20世纪90年代行为分析的逐步发展阶段，第3个是最近几年来行为分析的快速发展阶段。从文献[2]~[7]这6篇较有名的行为识别综述论文可以看出, 研究行为识别的人数在不断增加，论文数量也是猛增，并且产生了许多种重要的算法和思想。1.3    行为识别方法分类体系关于视觉上人体运动分析和识别的方法论体系有很多种。Forsyth[8]等人侧重与将动作从视频序列中人的姿态和运动信息恢复过来，这属于一个回归问题，而人体行为识别是一个分类问题，这2个问题有很多类似点，比如说其特征的提取和描述很多是通用的。Turaga[5]等人将人体行为识别分为3部分，即移动识别(movement),动作识别(action)和行为识别(activity)，这3种分类分别于低层视觉，中层视觉，高层视觉相对应。Gavrila[9]采用2D和3D的方法来分别研究人体的行为。对于行为识别方法论的划分中，最近出现了一种新的划分[7], Aggarwal将人体行为研究分为2大类，其一是基于单个层次来实现，其二是基于等级体系来实现。单层实现由分为时空特征和序列特征2种，等级体系实现分为统计方法，句法分析法和基于描述的方法3种。Aggarwal对行为识别方法论体系的层次结构图。该分类体系比较完善，也能很好的体现目前的研究进展。按照Turaga的3个层次划分理论，目前关于行为识别基本上还停留在第二个阶段，即action识别。而action识别比现实生活中的行为较简单，所以我们识别这些行为只需对这些行为进行正确的分类即可。这样一个行为识别系统就分成了行为特征提取和分类器的设计两个方面，通过对训练数据提取某种特征，采用有监督或无监督来训练一个分类模型，对新来的数据同样提取特征并送入该模型，得出分类结果。基于这个思想，本文主要是从行为识别的特征提取方面做了一个较为全面的介绍。1.4    行为识别研究难点行为识别发展至今，取得了很大的进展，在低层，中层和高层都取得了一定的突破，但是行为识别算法并不成熟，目前不存在一个算法适合所有的行为分类，3个视觉层次中都还有很多严峻的问题有待解决。其研究的难点主要体现在以下几个方面：1.4.1  动作类内类间的变化太大对于大多数的动作，即使是同一动作都有不同的表现形式。比如说走路，可以在不同的背景环境中完成，走路的速度也可以从慢到快，走路的步长亦有长有短。其它的动作也有类似的结果，特别是一些非周期的运动，比如过马路时候的走路，这与平时周期性的走路步伐明显不同。由此可见，动作的种类本身就很多，再加上每一种类又有很多个变种，所以给行为识别的研究带来了不少麻烦。1.4.2  环境背景等影响环境问背景等因素的影响可谓是计算机视觉各个领域的最大难点。主要有视角的多样性，同样的动作从不同的视角来观察会得到不同的二维图像；人与人之间，人与背景之间的相互遮挡也使计算机对动作的分类前期特征提取带来了困难，目前解决多视觉和遮挡问题，有学者提出了多摄像机融合通过3维重建来处理；另外其影响因素还包括动态变化和杂乱的背景，环境光照的变化，图像视频的低分辨率等。1.4.3  时间变化的影响总所周知，人体的行为离不开时间这个因素。而我们拍摄的视频其存放格式有可能不同，其播放速度有慢有快，这就导致了我们提出的系统需对视频的播放速率不敏感。1.4.4  数据的获取和标注既然把行为识别问题当成一个分类问题，就需要大量的数据来训练分类模型。而这些数据是视频数据，每一个动作在视频中出现的位置和时间都不确定，同时要考虑同一种动作的不同表现形式以及不同动作之间的区分度，即数据的多样性和全面性。这一收集过程的工作量不小，网上已经有一些公开的数据库供大家用来实验，这将在本文的第3部分进行介绍。另外，手动对视频数据标注非常困难。当然，有学者也提出了一些自动标注的方法，比如说利用网页图片搜索引擎[10],利用视频的字幕[11]，以及利用电影描述的文本进行匹配[12][13][14]。1.4.5  高层视觉的理解上面一提到，目前对行为识别的研究尚处在动作识别这一层(action  recognition)。其处理的行为可以分为2类，一类是有限制类别的简单规则行为，比如说走、跑、挥手、弯腰、跳等。另一类是在具体的场景中特定的行为[15]~[19]，如检测恐怖分子异常行为，丢包后突然离开等。在这种场景下对行为的描述有严格的限制，此时其描述一般采用了运动或者轨迹。这2种行为识别的研究都还不算完善，遇到了不少问题，且离高层的行为识别要求还相差很远。因此高层视觉的理解表示和识别是一个巨大的难题。2. 行为识别特征提取这一节中，将主要讨论怎样从图片序列中提取特征。本文将行为识别的特征分为2大类：全局特征和局部特征。全局特征是把一对象当做成一个整体，这是一种从上到下的研究思维。这种情况下，视频中的人必须先被定位出来，这个可以采用背景减图或者目标跟踪算法。然后对定位出来的目标进行某种编码，这样就形成了其全局特征。这种全局特征是有效的，因为它包含了人体非常多的信息。然而它又太依赖而底层视觉的处理，比如说精确的背景减图，人体定位和跟踪。而这些处理过程本身也是计算机视觉中的难点之处。另外这些全局特征对噪声，视角变化，遮挡等非常敏感。局部特征提取是收集人体的相对独立的图像块，是一种从下到上的研究思维。一般的做法是先提取视频中的一些时空兴趣点，然后在这些点的周围提取相应的图像块，最后将这些图像块组合成一起来描述一个特定的动作。局部特征的优点是其不依赖而底层的人体分割定位和跟踪，且对噪声和遮挡问题不是很敏感。但是它需要提取足够数量的稳定的且与动作类别相关的兴趣点，因此需要不少预处理过程。2.1    全局特征提取全局特征是对检测出来的整个感兴趣的人体进行描述，一般是通过背景减图或者跟踪的方法来得到，通常采用的是人体的边缘，剪影轮廓，光流等信息。而这些特征对噪声，部分遮挡，视角的变化比较敏感。下面分别从其二维特征和三维特征做介绍。2.1.1 二维全局特征提取Davis[20]等人最早采用轮廓来描述人体的运动信息，其用MEI和MHI 2个模板来保存对应的一个动作信息，然后用马氏距离分类器来进行识别。MEI为运动能量图，用来指示运动在哪些部位发生过，MHI为运动历史图，除了体现运动发生的空间位置外还体现了运动的时间先后顺序。这2种特征都是从背景减图中获取的。是坐下，挥手，蹲伏这3个动作的运动历史图MHI。为了提前剪影信息，Wang[21]等人利用r变换获取了人体的剪影。Hsuan-Shen[22]则提取了人体的轮廓，这些轮廓信息是用星型骨架描述基线之间夹角的，这些基线是从人体的手，脚，头等中心延长到人体的轮廓。而Wang[23]同时利用了剪影信息和轮廓信息来描述动作，即用基于轮廓的平均运动形状(MMS)和基于运动前景的平均能量(AME)两个模板来进行描述。当把轮廓和剪影模板保存下来后，新提取出的特征要与其进行比较，Daniel[24]采用欧式距离来测量其相似度，随后他又改为用倒角距离来度量[25],这样就消除了背景减图这一预处理步骤。除了利用轮廓剪影信息外，人体的运动信息也经常被采用。比如说基于像素级的背景差法，光流信息等。当背景差法不能很好的工作时，我们往往可以采用光流法，但是这样经常会引入运动噪声，Effos[26]只计算以人体中心点处的光流，这在一定程度上减少了噪声的影响。2.1.2  三维全局特征提取在三维空间中，通过给定视频中的数据可以得到3D时空体(STV)，STV的计算需要精确的定位，目标对齐，有时还需背景减图。Blank[27][28]等人首次从视频序列中的剪影信息得到STV。如所示。然后用泊松方程导出局部时空显著点及其方向特征，其全局特征是通过对这些局部特征加权得到的，为了处理不同动作的持续时间不同的问题，Achard[29]对每一个视频采用了一系列的STV ,并且每个STV只是覆盖时间维上的一部分信息。还有一种途径是从STV中提取相应的局部描述子，这一部分将在局部特征提取一节中介绍，在这里，我们还是先把STV特征当做是全局特征。Batra[30]存储了STV的剪影，并且用很小的3D二进制空间块来采样STV。Yilmaz[31]提取了STV表面的不同几何特征，比如说其极大值点和极小值点。当然，也有学者Keel[32]将剪影的STV和光流信息结合起来，作为行为识别的全局特征。2.2    局部特征提取人体行为识别局部特征提取是指提取人体中感兴趣的点或者块。因此不需要精确的人体定位和跟踪，并且局部特征对人体的表观变化，视觉变化和部分遮挡问题也不是很敏感。因此在行为识别中采用这种特征的分类器比较多。下面从局部特征点检测和局部特征点描述2部分来做介绍。2.2.1  局部特征点的检测行为识别中的局部特征点是视频中时间和空间中的点，这些点的检测发生在视频运动的突变中。因为在运动突变时产生的点包含了对人体行为分析的大部分信息。因此当人体进行平移直线运动或者匀速运动时，这些特征点就很难被检测出来。Laptev[33]将Harris角点扩展到3D Harris，这是时空兴趣点(STIP)族中的一个。这些时空特征点邻域的像素值在时间和空间都有显著的变化。在该算法中，邻域块的尺度大小能够自适应时间维和空间维。该时空特征点如所示。Dollar[34]指出上述那种方法存在一个缺点，即检测出来稳定的兴趣点的数量太少，因此Dollar单独的在时间维和空间维先采用gabor滤波器进行滤波，这样的话检测出来兴趣点的数目就会随着时间和空间的局部邻域尺寸的改变而改变。类似的，Rapantzikos[35]在3个维度上分别应用离散小波变换，通过每一维的低通和高通的滤波响应来选择时空显著点。同时，为了整合颜色和运动信息，Rapantzikos[36]加入了彩色和运动信息来计算其显著点。与检测整个人体中兴趣点的出发思路不同，Wong[37]首先检测与运动相关的子空间中的兴趣点，这些子空间对应着一部分的运动，比如说手臂摆动，在这些子空间中，一些稀疏的兴趣点就被检测出来了。类似的方法，Bregonzio[38]首先通过计算后面帧的不同来估计视觉注意的焦点，然后利用gabor滤波在这些区域来检测显著点。2.2.2  局部特征点的描述局部特征描述是对图像或者视频中的一个块进行描述，其描述子应该对背景的杂乱程度，尺度和方向变化等均不敏感。一个图像块的空间和时间尺寸大小通常取决于检测到的兴趣点的尺寸。显示的是cuboids描述子[34]。cuboids描述子特征块也可以用基于局部特征的网格来描述，因为一个网格包括了局部观察到的领域像素，将其看成一个块，这样就减少了时间和空间的局部变化的影响。二维的SURF特征[39]被Willems[40]扩展到了3维，这些eSURF特征的每个cell都包含了全部Harr-wavelet特征。Laotev[14]使用了局部HOG(梯度直方图)和HOF(光流直方图)。Klaser[41]将HOG特征扩展到3维，即形成了3D-HOG。3D-HOG的每个bin都是由规则的多面体构成，3D-HOG允许 在多尺度下对cuboids进行快速密度采样。这种将二维特征点检测的算法扩展到3维特征点类似的工作还有是将SIFT算法[42]扩展到3维SIFT Scovanner[43]。在Wang[44]的文章中，他比较了各种局部描述算子，并发现在大多数情况下整合了梯度和光流信息的描述算子其效果最好。另外还有一种描述子比较流行，即单词袋[45][46]，这是利用的单词频率直方图特征。2.3    全局、局部特征融合全局和局部特征的融合，结合了全局特征的足够信息量和局部特征的对视角变化，部分遮挡问题不敏感，抗干扰性强的优点。这样的文章比较多，其主要思想结合从2.1和2.2的方法。Thi[47]就将这2种特征结合得很好，其全局特征是采用前面介绍的MHI算子，并且采用AIFT算法[48]进一步选择更好的MHI。局部特征也是采用前面提到的STIP特征，并且采用SBFC(稀疏贝叶斯特征选择)[49]算法过滤掉一些噪声比较大的特征点。最后将2种特征送入到扩展的3维ISM模型中，其ISM[50]是一种目标识别常用算法，即训练出目标的隐式形状模型。Thi[47]的方法结构如所示。3. 行为识别常见数据库3.1    WeizmannWeizmann[27]数据库包含了10个动作分别是走，跑，跳，飞跳，向一侧移动，单只手挥动，2只手挥动，单跳，2只手臂挥动起跳,每个动作有10个人执行。在这个视频集中，其背景是静止的，且前景提供了剪影信息。该数据集较为简单。3.2    KTHKTH[45]行人数据库包含了6种动作，分别为走，慢跑，跑挥手和鼓掌。每种动作由25个不同的人完成。每个人在完成这些动作时又是在4个不同的场景中完成的，4个场景分别为室外，室内，室外放大，室外且穿不同颜色的衣服。3.3    PETSPETS[51]，其全称为跟踪与监控性能评估会议，它的数据库是从现实生活中获取的，主要来源于直接从视频监控系统拍摄的视频，比如说超市的监控系统。从2000年以后，基本上每年都会组织召开这个会议。3.4    UCFUCF包含个数据集，这里是指UCF的运动数据库[52],该视频数据包括了150个视频序列，共有13个动作。因为是现实生活中的视频数据，所以其背景比较复杂，这些种类的动作识别起来有些困难。3.5    INRIA XMASINRIA XMAS数据库[53]是从5个视角拍摄的，室内的4个方向和头顶的1个方向。总共有11个人完成14种不同的动作，动作可以沿着任意方向执行。摄像机是静止的，环境的光照条件也基本不变。另外该数据集还提供有人体轮廓和体积元等信息。3.6    HollywoodHollywood电影的数据库包含有几个，其一[14]的视频集有8种动作，分别是接电话，下轿车，握手，拥抱，接吻，坐下，起立，站立。这些动作都是从电影中直接抽取的，由不同的演员在不同的环境下演的。其二[54]在上面的基础上又增加了4个动作，骑车，吃饭，打架，跑。并且其训练集给出了电影的自动描述文本标注，另外一些是由人工标注的。因为有遮挡，移动摄像机，动态背景等因素，所以这个数据集非常有挑战。4. 总结本文较全面的介绍了行为识别中特征提取的方法，并将其分为全局特征提取和局部特征提取2个部分介绍，虽然自行为识别研究以来已经取得了不少成果，但是由于视觉中的动态环境，遮挡等问题存在，其挑战非常大，需要提取出鲁棒性更好，适应性更强，效果更好的特征，而这仍是后面几年甚至几十年不断追求努力才能达到的目标。参考文献：Johansson, G. (1975). \"Visual motion perception.\" Scientific American.Aggarwal, J. K. and Q. Cai (1997). Human motion analysis: A review, IEEE.Moeslund, T. B. and E. Granum (2001). \"A survey of computer vision-based human motion capture.\" Computer vision and image understanding 81(3): 231-268.Moeslund, T. B., A. Hilton, et al. (2006). \"A survey of advances in vision-based human motion capture and analysis.\" Computer vision and image understanding 104(2): 90-126.Turaga, P., R. Chellappa, et al. (2008). \"Machine recognition of human activities: A survey.\" Circuits and Systems for Video Technology, IEEE Transactions on 18(11): 1473-1488.Poppe, R. (2010). \"A survey on vision-based human action recognition.\" Image and Vision Computing 28(6): 976-990.Aggarwal, J. and M. S. Ryoo (2011). \"Human activity analysis: A review.\" ACM Computing Surveys (CSUR) 43(3): 16.Forsyth, D. A., O. Arikan, et al. (2006). Computational studies of human motion: Tracking and motion synthesis, Now Pub.Gavrila, D. M. (1999). \"The visual analysis of human movement: A survey.\" Computer vision and image understanding73(1): 82-98.10. Ikizler-Cinbis, N., R. G. Cinbis, et al. (2009). Learning actions from the web, IEEE.11. Gupta, S. and R. J. Mooney (2009). Using closed captions to train activity recognizers that improve video retrieval, IEEE.12. Cour, T., C. Jordan, et al. (2008). Movie/script: Alignment and parsing of video and text transcription.13. Duchenne, O., I. Laptev, et al. (2009). Automatic annotation of human actions in video, IEEE.14. Laptev, I., M. Marszalek, et al. (2008). Learning realistic human actions from movies, IEEE.15. Haritaoglu, I., D. Harwood, et al. (1998). \"W 4 S: A real-time system for detecting and tracking people in 2 1/2D.\" Computer Vision—ECCV'98: 　　　　　877-892.16. Tao, D., X. Li, et al. (2006). Human carrying status in visual surveillance, IEEE.17. Davis, J. W. and S. R. Taylor (2002). Analysis and recognition of walking movements, IEEE.18. Lv, F., X. Song, et al. (2006). Left luggage detection using bayesian inference.19. Auvinet, E., E. Grossmann, et al. (2006). Left-luggage detection using homographies and simple heuristics.20. Bobick, A. F. and J. W. Davis (2001). \"The recognition of human movement using temporal templates.\" Pattern Analysis and Machine Intelligence, 　　　　   IEEE Transactions on 23(3): 257-267.21. Wang, Y., K. Huang, et al. (2007). Human activity recognition based on r transform, IEEE.22. Chen, H. S., H. T. Chen, et al. (2006). Human action recognition using star skeleton, ACM.23. Wang, L. and D. Suter (2006). Informative shape representations for human action recognition, Ieee.24. Weinland, D., E. Boyer, et al. (2007). Action recognition from arbitrary views using 3d exemplars, IEEE.25. Weinland, D. and E. Boyer (2008). Action recognition using exemplar-based embedding, Ieee.26. Efros, A. A., A. C. Berg, et al. (2003). Recognizing action at a distance, IEEE.27. Blank, M., L. Gorelick, et al. (2005). Actions as space-time shapes, IEEE.28. Gorelick, L., M. Blank, et al. (2007). \"Actions as space-time shapes.\" Pattern Analysis and Machine Intelligence, IEEE Transactions on 29(12): 　　　　　　   2247-2253.29. Achard, C., X. Qu, et al. (2008). \"A novel approach for recognition of human actions with semi-global features.\" Machine Vision and Applications 　　　　   19(1): 27-34.30. Batra, D., T. Chen, et al. (2008). Space-time shapelets for action recognition, IEEE.31. Yilmaz, A. and M. Shah (2008). \"A differential geometric approach to representing the human actions.\" Computer vision and image understanding 　　        109(3): 335-351.32. Ke, Y., R. Sukthankar, et al. (2007). Spatio-temporal shape and flow correlation for action recognition, IEEE.33. Laptev, I. (2005). \"On space-time interest points.\" International journal of computer vision 64(2): 107-123.34. Dollár, P., V. Rabaud, et al. (2005). Behavior recognition via sparse spatio-temporal features, IEEE.35. Rapantzikos, K., Y. Avrithis, et al. (2007). Spatiotemporal saliency for event detection and representation in the 3D wavelet domain: potential in 　　　　   human action recognition, ACM.36. Rapantzikos, K., Y. Avrithis, et al. (2009). Dense saliency-based spatiotemporal feature points for action recognition, Ieee.37. Wong, S. F. and R. Cipolla (2007). Extracting spatiotemporal interest points using global information, IEEE.38. Bregonzio, M., S. Gong, et al. (2009). Recognising action as clouds of space-time interest points, IEEE.39. Bay, H., T. Tuytelaars, et al. (2006). \"Surf: Speeded up robust features.\" Computer Vision–ECCV 2006: 404-417.40. Willems, G., T. Tuytelaars, et al. (2008). \"An efficient dense and scale-invariant spatio-temporal interest point detector.\" Computer Vision–ECCV 　　　　   2008: 650-663.41. Klaser, A. and M. Marszalek (2008). \"A spatio-temporal descriptor based on 3D-gradients.\"42. Mikolajczyk, K. and C. Schmid (2004). \"Scale & affine invariant interest point detectors.\" International journal of computer vision 60(1): 63-86.43. Scovanner, P., S. Ali, et al. (2007). A 3-dimensional sift descriptor and its application to action recognition, ACM.44. Wang, H., M. M. Ullah, et al. (2009). \"Evaluation of local spatio-temporal features for action recognition.\"45. Niebles, J. C., H. Wang, et al. (2008). \"Unsupervised learning of human action categories using spatial-temporal words.\" International journal of 　　　　   computer vision 79(3): 299-318.46. Schuldt, C., I. Laptev, et al. (2004). Recognizing human actions: A local SVM approach, IEEE.47. Thi, T. H., L. Cheng, et al. (2011). \"Integrating local action elements for action analysis.\" Computer vision and image understanding.48. Liu, G., Z. Lin, et al. (2009). \"Radon representation-based feature descriptor for texture classification.\" Image Processing, IEEE Transactions on 　　　　   18(5): 921-928.49. Carbonetto, P., G. Dorkó, et al. (2008). \"Learning to recognize objects with little supervision.\" International journal of computer vision 77(1): 219-　　　　   237.50. Leibe, B., A. Leonardis, et al. (2008). \"Robust object detection with interleaved categorization and segmentation.\" International journal ofcomputer vision 77(1): 259-289.51. http://www.cvg.rdg.ac.uk/slides/pets.html.52. Rodriguez, M. D. (2008). \"Action mach a spatio-temporal maximum average correlation height filter for action recognition.\" CVPR.53. Weinland, D., R. Ronfard, et al. (2006). \"Free viewpoint action recognition using motion history volumes.\" Computer vision and imageunderstanding 104(2): 249-257.54. Marszalek, M., I. Laptev, et al. (2009). Actions in context, IEEE."}
{"content2":"最近正在研究行人检测，学习了一篇2014年发表在ECCV上的一篇综述性的文章，是对行人检测过去十年的一个回顾，从dataset，main approaches的角度分析了近10年的40多篇论文提出的方法，发现有三种方法（DPM变体，Deep networks，Decision forests）都取得了相似的最好结果，并总结了feature，additional data以及context information等对于detection quality的影响。1、Introduction行人检测主要的方法有：Viola&Jones variants，HOG+SVM rigid templates, deformable part detectors (DPM), and convolutional neural networks(ConvNets) 。2、Datasets主要的datasets有6个：INRIA, ETH, TUD-Brussels, Daimler(Daimler stereo), Caltech-USA, KITTI。2.1 INRIA数据库http://pascal.inrialpes.fr/data/human/介绍：该数据库是“HOG+SVM”的作者Dalal创建的，该数据库是目前使用最多的静态行人检测数据库，提供原始图片及相应的标注文件。训练集有正样本614张（包含2416个行人），负样本1218张；测试集有正样本288张（包含1126个行人），负样本453张。图片中人体大部分为站立姿势且高度大于100个象素，部分标注可能不正确。图片主要来源于GRAZ-01、个人照片及google，因此图片的清晰度较高。在XP操作系统下部分训练或者测试图片无法看清楚，但可用OpenCV正常读取和显示。更新：20052.2 ETHZ行人数据库Robust Multi-Person Tracking from Mobile Platformshttps://data.vision.ee.ethz.ch/cvl/aess/dataset/Ess等构建了基于双目视觉的行人数据库用于多人的行人检测与跟踪研究。该数据库采用一对车载的AVT Marlins F033C摄像头进行拍摄，分辨率为640×480，帧率13-14fps，给出标定信息和行人标注信息，深度信息采用置信度传播方法获取。更新：20102.3 TUD行人数据库https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/people-detection-pose-estimation-and-tracking/multi-cue-onboard-pedestrian-detection/介绍：TUD行人数据库为评估运动信息在行人检测中的作用，提供图像对以便计算光流信息。训练集的正样本为1092对图像（图片大小为720×576，包含1776个行人）；负样本为192对非行人图像（手持摄像机85对，车载摄像机107对）；另外还提供26对车载摄像机拍摄的图像（包含183个行人）作为附加训练集。测试集有508对图像（图像对的时间间隔为1秒，分辨率为640×480），共有1326个行人。Andriluka等也构建了一个数据库用于验证他们提出的检测与跟踪相结合的行人检测技术。该数据集的训练集提供了行人的矩形框信息、分割掩膜及其各部位（脚、小腿、大腿、躯干和头部）的大小和位置信息。测试集为250张图片（包含311个完全可见的行人）用于测试检测器的性能，2个视频序列（TUD-Campus和TUD-Crossing）用于评估跟踪器的性能。更新：20102.4 Daimler行人数据库http://www.gavrila.net/Datasets/Daimler_Pedestrian_Benchmark_D/daimler_pedestrian_benchmark_d.html该数据库采用车载摄像机获取，分为检测和分类两个数据集。检测数据集的训练样本集有正样本大小为18×36和48×96的图片各15560（3915×4）张，行人的最小高度为72个象素；负样本6744张（大小为640×480或360×288）。测试集为一段27分钟左右的视频（分辨率为640×480），共21790张图片，包含56492个行人。分类数据库有三个训练集和两个测试集，每个数据集有4800张行人图片，5000张非行人图片，大小均为18×36，另外还有3个辅助的非行人图片集，各1200张图片。更新：2009？2.5 Caltech Pedestrian Detectionhttp://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/该数据库是目前规模较大的行人数据库，采用车载摄像头拍摄，约10个小时左右，视频的分辨率为640×480，30帧/秒。标注了约250,000帧（约137分钟），350000个矩形框，2300个行人，另外还对矩形框之间的时间对应关系及其遮挡的情况进行标注。数据集分为set00~set10，其中set00~set05为训练集，set06~set10为测试集（标注信息尚未公开）。性能评估方法有以下三种：（1）用外部数据进行训练，在set06~set10进行测试；（2）6-fold交叉验证，选择其中的5个做训练，另外一个做测试，调整参数，最后给出训练集上的性能；（3）用set00~set05训练，set06~set10做测试。由于测试集的标注信息没有公开，需要提交给Pitor Dollar。结果提交方法为每30帧做一个测试，将结果保存在txt文档中（文件的命名方式为I00029.txt I00059.txt ……），每个txt文件中的每行表示检测到一个行人，格式为“[left, top,width, height, score]”。如果没有检测到任何行人，则txt文档为空。该数据库还提供了相应的Matlab工具包，包括视频标注信息的读取、画ROC（Receiver Operatingcharacteristic Curve）曲线图和非极大值抑制等工具。更新：20142.6 KITTI Vision Benchmarkhttp://www.cvlibs.net/datasets/kitti/index.phpKITTI是德国卡尔斯鲁厄理工学院和芝加哥丰田技术研究所联合创办的一个算法评测平台，旨在评测对象（机动车、非机动车、行人等）检测、目标跟踪等计算机视觉技术在车载环境下的性能，为机动车辅助驾驶应用做技术评估与技术储备。2.7 小结http://www.cvpapers.com/datasets.html绝大多数的数据集都可以在上面网址中找到。INRIA最旧图像也最少，不过好处是它拥有比较丰富的背景环境（如城市，沙滩，山地等），所以被使用的比较多。ETH和TUD-Brussels是中等大小的视频数据集，Daimler缺乏彩色信息，Daimler stereo, ETH, and KITTI 提供立体信息。除了INRIA之外的数据集都是从视频中获取的，因此可以使用光流作为additional cue。现在用的最多的数据集是Caltech-USA和KITTI，二者都是比较大且具有挑战性的。Caltech-USA有大量的方法使用因而比较起来比较方便，而KITTI的数据集更加丰富一些但是用的。这篇文章主要是以Caltech数据集作为标准，以INRIA和KITTI作为辅助。3 Main approaches to improve pedestrian detection首先介绍了近十年的主要发展：2003, VJ detector.2005, HOG detector.2008, DPM.2009, Caltech dataset, 评价方法FPPW->FPPI.然后比较了40种左右方法的missing rate，给出了图表。3.1 Training data很显然，上显示了训练数据的大小比较明显的影响了结果的好坏。红色部分使用的训练集也是由Caltech数据集扩展而来。3.2 Solution families表1的40多种方法，大体可以分为3类：DPM变体，DN和DF。这三种方法都大体达到了state of art 。3.3 Better classifiersHOG+SVM 和 HikSvm 这些古老的方法，在当时的评价方法（FPPW）下，表现很好，但是却在FPPI下性能比较差。MultiFtrs 方法说明Adaboost以及线性SVM在给定足够多feature的条件下是可以达到同样的检测效果的。并没有经验性的证据表明非线性核比线性核的性能更好。也没有证据表明某种分类器是最适合做行人检测的。3.4 Additional data使用额外的数据可以取得有效的提高，但是像立体和光流等线索都没有被完全利用起来。现在，基于单眼的方法已经达到了有额外信息方法的水平了。3.5 Exploiting Context环境信息也可以给行人检测带来提升，尽管不如额外数据和深度结构那样明显。3.6 Deformable PartsDPM detector就是为了做pedestrian detection而被提出来的。这种方法及其变体都很流行，尽管检测结果都很不错，但是却并不突出。越来越多的仅仅使用单个部件的方法都超越了DPM，这样就让我们产生了疑问：究竟有没有必要使用多个部件，即使是在有遮挡的情况下？这个问题目前也是没有明确答案的。3.7 Multi-scale models多尺度（多分辨率）的模型提供了一个对于已有检测子的更简洁和一般化的延伸。尽管有所提升，但对于最终的结果提升相当小。3.8 Deep Architecture随着数据量的增加和计算能力的增强，在计算机视觉领域（包括行人检测方面）使用深度网络（尤其是CNN）变得流行。ConvNet结构混合了监督的和无监督的训练来搭建卷积神经网络，在INRIA,ETH,TUD-Brussels上得到了一般的结果，但在Caltech集上却失败了。这是从像素层面直接获取特征的方法。而另一些结构（DBN, JointDeep, SDN）将part model和遮挡结合起来 都放进了深度结构，但它并不是从原始像素点之中去发现特征，而是从使用了边缘和色彩特征，或者将网络权重初始化时设置对边缘敏感的滤波器。值得注意的是，目前还没有人事先在ImageNet上预训练过。虽然没有证据显示神经网络适合进行行人检测，但是很多性能良好的模型都使用了这种结构，不过其性能也只是和DPM和DF差不多，优势并不明显。3.9 Better features在改进行人检测的工作中，做的最多的就是增加或者多样化输入图像的特征。通过更多的和更高维度的特征，分类的任务似乎是变简单了，结果也有了改进。很多种类的特征已经被发现：边缘信息，颜色信息，纹理信息，局部形状信息，协方差特征，还有其他等等。越来越多的特征已被证明可以系统性的改善性能。很多decision forest 方法采用10个feature channel，有些则采用了多达上百个feature channel。尽管增加channel可以提升性能，但目前表现最好的方法都是采取10个channel的：6个梯度方向，1个梯度幅值，3个颜色通道，叫做 HOG+LUV.过去十年，特征的提升是检测效果提升的动力，显然，接下来的日子里，提升特征效果将依然是主流。这些提升都是在大量的实验和错误下累积起来的。接下来的研究将集中在为什么这些特征这么好以及如何设计更好的特征上。4 Experiments基于上面的分析，可以得出检测效果的提升主要集中在3个方面：better features，additional data 和context information。所以我们做实验来研究他们之间的互补性。在3.2 中给出的3中主要方法里，我们选择了 Integral Channels Features 框架（DF方法）来做实验，因为这种方法表现突出且训练较快。4.1 Reviewing the effect of features所有方法都是在INRIA上训练，在Caltech上测试。如所示第一批实验都是复现那些具有里程碑式的方法，如 VJ ， HOG+linear SVM ， and ChnFtrs 。从VJ以来，性能的提升多半可以归功于采用了更好的特征，梯度方向和颜色信息等。即使是在已有特征基础上加入的一点点微调也能产生显著的提升（如SquaresChnFtrs 加入DCT变换）。4.2 Complementarity of approaches接下来，作者又做了大量实验来研究better features（HOG+LUV+DCT）， additional data （via optical flow）, and context （via person-to-person interactions）之间的互补性。在上文SquaresChnFtrs+DCT 的基础上，作者用和 ACF+SDt 中同样的方法将光流信息编码，同时用+2Ped 中的re-weighting技巧把环境信息加入。这种 SquaresChnFtrs+DCT+SDt+2Ped 的方法被称为Katamari-v1。如结论：实验证明——通过加入额外的特征，光流，和环境信息是可以很大程度的互补的，获得了12%的提升。4.3 How much model capacity is needed?我们的目标是要从训练集到测试集推广，那么在研究模型泛化能力的时候，一个重要的问题就是，模型在训练集上的效果如何呢？显示模型在训练集上的效果，不幸的是，这些方法都在训练集上就表现不佳，所以，目前还没有发现过拟合的问题。所以，我们还是应该研究更有区分力的检测子来提升检测结果。这些更有区分力的检测子可以通过寻找更好的features和更复杂的分类器来实现。4.4 Generalisation across datasets对于真实世界应用来说，模型的泛化能力才是关键。表2 展示了SquaresChnFtrs 用不同训练集训练时在Caltech上的表现（对于KITTI，评价指标是AUC，越高越好；对于其他数据集，评价指标是MR，越小越好）。从表中可以看出，在Caltech和KITTI上训练，对于INRIA数据集的泛化性能很差。而反过来，INRIA确实对于Caltech和KITTI第二好的选择。这些结果表明，Caltech的行人相对更加单一的，而INRIA却因为它的多样性而更加有效。如，训练和测试如果都在KITTI上，SquaresChnFtrs （在KITTI上叫SquaresICF ）比普通的DPM好，而且还和最好的DPM变体（DA-DPM )相当。目前在KITTI上表现最好的方法，pAUC 是ChnFtrs 的变体，只是它使用了250个特征通道而已。 这也和我们在3.9 和4.1 中的结论一致。小结：尽管在一个训练集上训练之后再在另一个数据集上测试效果不一定好，但是排名大体还是基本一致的。也就是说，只要方法足够好，无论benchmark是什么都能得到好的结果。5 Conclusion做了这么多实验，发现这么多年在pedestrian detection上的进步基本上都得益于特征的提升，目前来看，这种趋势还将继续。实验结果表明，better features ，optical flow ，context 是互补的。将它们结合起来，得到了在Caltech数据集上的最好模型。尽管三种主要的方法——DPM，DF，DN——是基于完全不同的学习技巧的，它们的state of art 结果却是惊人相似的。最后，未来的挑战将是更深层次的理解好的特征为什么好，这样才能设计出更好的特征！参考文献http://janhosang.com/pdfs/2014_eccvw_ten_years_of_pedestrian_detection_with_supplementary_material.pdfhttp://blog.csdn.net/mduke/article/details/46582443"}
{"content2":"AI，大数据，复杂系统 最精 40本大书单原创 2017-10-30 Peter 混沌巡洋舰如果这篇文的题目变成最全书单，那么这篇文会变得又臭又长，这个年代，关于人工智能和大数据的书，没有一万本也有一千本，而这里列出的40本，则是精选过的，不敢说每一本都字字珠玑，但这个书单保证没有一本水书。废话不说，赶快上车，先放思维导图，再一本本的简单说说。书单分成8部分，其中的数字代表我对这一系列的书的推荐程度。先说经典书的部分终极算法：机器学习和人工智能如何重塑世界这本书的名字，显示着作者试图在机器学习的各个流派间进行整合，最终提出机器学习里的“牛顿三定律”的理想。作者在这本书里，介绍了当前常用的算法的发展历程，这些算法包括决策树，遗传算法，神经网络，朴素贝叶斯及贝叶斯网络，隐式马尔可夫链，K最近邻及支持向量机，作者还介绍了无监督学习的算法。在介绍算法时，作者还介绍了机器学习里最大的两个阻碍，过拟合及维度灾难。对上面的这些名词看不懂，看过书你就明白了。这本书中，没有公式与代码，有的只是对机器学习中的算法本质一针见血的点破，有的只是依据这些算法而编出的日常生活中的故事，是对机器学习中核心算法的概念化的模型。一言以概之，这是一本所有有高中数学水平且无计算机背景的读者都能够读懂的科普书。如果你不想对控制着我们衣食住行方方面面的机器学习算法一无所知，那么这本书是你必读的书。人工智能之父马文·明斯基经典作品：情感机器+心智社会这两本书的作者被誉为人工智能之父，不是因为他发现了某一个特别NB的算法。而是因为其对人类的认知过程有着独特的见解，从而能利用对人类认知的洞察来指导机器学习算法的研发。其在70年代写成的心智社会一书，令当前的人工智能研究者还会常读常新。这本书虽然价格有些高，但考虑到读一遍根本不指望能看懂，要看三遍才能有些领悟，算算阅读单价，就不算高的。再加上这本书送朋友，那是多么有逼格的一件事啊。这本书是人工智能之父集一生功力写成的集大成之作。如何让机器有感情，是在机器智能即将超越人之后的人工智能的下一个天花板。情感计算的概念，也随着Chatbot（聊天机器人）而火了起来。阅读这本书，会让读者认识到情感不一定是人类独有的特征。情况也可以被表示为一连串的计算。而赋予机器情感，我们也能造成有常识，有直觉的机器。如果你想打破人工智能的黑盒子，这本书也是一本需要反复研读的大作。数学之美这本书虽然叫做数学之美，其实由于作者吴军博士是谷歌的搜索专家，所以写的多半是自然语言处理领域的发展。关于这本书，溢美之词已经太多了。而我这里想说的不是其将算法背后的原理讲述的多么清晰，而是作者讲述了其和诸位自然语言处理领域的先驱的个人故事，其中描述了诸多学者的风骨以及其背后的道德力量。这是这本书少有被人提起，但却能令人记忆深刻的地方。人工智能的未来这本书的中文版已经绝版，在网上搜这本书，多半搜出的是雷库兹曼的原名为How to create a mind 的书的翻译版。对于这本04 年的书，书的作者杰夫·霍金斯(Jeff Hawkins)，成功的计算机工程师和企业家，掌上型电脑PalmPilot、智能电话Treo等产品的发明人。这本书提出的HTM模型，可能凭其单一的结构而第一次产生自我学习的“智能”，其理论的高瞻远瞩，启发了当今的深度学习浪潮。理解信念：人工智能的科学理解这本书的作者尼尔斯•尼尔森（NilsJ.Nilsson）是斯坦福大学教授。这是一本哲学书。其核心论旨包括，我们人类的感觉系统是获取外界信息、形成信念的唯一途径；运用科学方法、经由严格批评和修正而建立起来的信念，是相对真实且更为有用的。人工智能在某种程度上，和人类一样拥有信念，或者可以说，人类是一台台复杂的机器。智能的本质 人工智能与机器人领域的64个大问题在书中，作者从常识出发，对人工智能和机器人表达了很多“令人惊讶”而又让人深思的观点。例如在陪伴老年人方面，迄今为止先进的机器人都不如狗做得好。书中充满了思辨和哲学判断，感觉作者属于乐观派中的悲观派，乐观的是认为不会出现终结者，悲观的是AI发展还是太慢了。 我认为是近年来不可多得的好书。 同时，作者担心的并不是机器智能的迅速提高，而是人类智力可能会下降，这才是最值得担忧的。接着是AI对商业和我们生活的影响。这一系列的书很多，选出几本我读过的第二次机器革命《第二次机器革命》这本书，是那种能够在机场书店找到的图书，这本书不算厚，两三个小时就可以读完，也不算烧脑。书中的内容围绕着以人工智能和数字化为代表的技术对未来社会的影响展开论述。这本书令我记忆最深的是每一章开篇引用的名言，即幽默又别有深意。人工智能时代这本书的作者Kaplan是斯坦福大学顶尖人工智能专家。卡普兰本科毕业于芝加哥大学历史与科学哲学专业，之后考入宾夕法尼亚大学计算机科学专业，后进入斯坦福大学人工智能实验室工作。这本书的英文名直译过来是人不必遵守机器的规则。这是一个老人写的书，这种警世的书也需要由一个老人写出，作者见证了人工智能的低潮与复兴，见证了越来越大的贫富差距。他活到了替子女说话的年纪，又没有丢掉幽默。这样智慧的老人，值得我们去倾听。与机器人共舞凯恩斯就曾指出，科技将取代工作岗位，而非整体工作量。这些改变了我们工作方式、互动方式以及娱乐方式的创新，将给21 世纪的社会带来翻天覆地的改变，这种影响几乎等同于20世纪初机械设备将农耕经济带向工业经济时，社会所经历的根本性变革。这本书的作者是曾获得普利策奖的资深记者，视角全面，分析深入。接着说说大数据方面的书爆发：大数据时代预见未来的新思维本书作者全球复杂网络权威Barabasi所作，一本超越《黑天鹅》的惊世之作。作者认为人类正处在一个聚合点上，在这里数据、科学以及技术都联合起来共同对抗那个最大的谜题——我们的未来。作者指出人类日常行为模式不是随机的，而是具有“爆发性”的。爆发揭开了人类行为中令人惊讶的深层次的秩序，使得人类变得比预期中更容易预测得多。爆发模式的揭示，其影响力将与20世纪初期的物理学或者基因革命的影响力不相上下。智慧社会：大数据与社会物理学这本书的作者是MIT人类动力学实验室主任。这本书提出了一种量化的办法，来定向测度沟通对行为的影响，即想法流（idea flow）的传播的规律。量化的考察沟通对创新的影响。作者用可控双盲实验去验证诸如社会网络的大小与想法的多样性有正相关关系；社会网络的互动密度与效率显著相关等习以为常的观点，使得全书的科学很强。大数据可视化：重构智慧社会大数据的目的最终还是要讲一个好故事。而人类是一种视觉动物，一幅图的效果好过千言万语。这本书举出了很多第一线的例子，来说明怎么样去做出好的数据可视化，对于任何要处理数据的人来说，这本书中的道理都是必不可少的，须要透彻掌握的。大数据思维与决策这本书的作者是计量经济学家，这本书展示了社会科学的全面数字化。没有数字就没有真相。作者指出统计是一个非常强大的研究社会问题的手段，可以应用在任何你想要的领域。社会学科的专家，将越来越依靠大数据模型做出判断，直觉和数据统计呈现出互补的趋势。而在善于利用大数据的商家面前，消费者将越发无计可施。赤裸裸的未来·大数据时代：如何预见未来的生活和自己正如这本书的书名所展示的，就个人而言，我们早已生活在一个“超级透明”的世界，我们泄露出去的海量信息无处不在。若将这些信息收集起来，加以分析，就能勾勒出每一个人的真实性格、内心偏好，乃至可以预测每个人的命运。作者大胆预言：“大数据时代”只不过是一朵小浪花，终将会被更新、更前沿的“物联网时代”取代，并以灾难预测、流行病预防、犯罪防治、潜能开发、情绪管理、恋爱情感、个性化学习、娱乐私人定制等领域为例，描绘了一个富有激情的美好未来。白话大数据与机器学习本书通俗易懂，有高中数学基础即可看懂，同时结合大量案例与漫画，将高度抽象的数学、算法与应用，与现实生活中的案例和事件一一做了关联，将源自生活的抽象还原出来，帮助读者理解后，又带领大家将这些抽象的规律与算法应用于实践。大数据：从概念到运营有多少人只是谈论大数据，却不知道该怎么将大数据应用到具体的工作中去，这本书的作者有着在大数据领域拥有超过20年的从业经历，曾担任雅虎公司广告分析副总裁，在数据存储、商业智能和数据分析利用方面有着独到的见解。本书一共有十三个章节，在书中作者将大数据在实际运用中的方方面面通过具体的案例进行了分析。侧重于大数据的实际运用方面而不是理论的探讨。大量的案例使得书中观点鲜活有力。Python 金融大数据分析大数据的应用最广的领域，无疑是数据驱动的金融业。作为该领域的入门书，这本书介绍了python语言在金融数据可视化，金融衍生品定价，金融时间序列数据处理，蒙特卡罗方法等话题上的具体应用，是一本简单明了的入门书。接下来的书关于数学，这是所有数据科学的基本功妙趣横生的统计学这是本统计学入门书，涉及了很多高中课程中的内容，例如我们是不是比父母更聪明？开车时打电话与酒驾一样危险吗？坐飞机和开车，哪种方式更安全？钻石越重，价格就越高吗？小学四年级的学生可以用统计学做什么？这本书的目标是日常生活所需要的统计思想、正确分析数据的基本路径。统计会犯错这本书的大部分例子来源于临床医学。但书中的道理却可以应用到任何领域上。这本书可以当作一本统计学文章阅读踩雷指南，至少看完了对一些得出千奇百怪的结论的文章抱有怀疑，即使是权威期刊刊登的文章。最后指出的因为保密造成的数据不公开问题也是值得深思的。用数学的语言看世界本书为理论物理学家大栗博司先生写给自己女儿的数学读本，全书以用“数学语言”解读自然为线索，用生动故事和比喻重新讲解了数学的核心原理与体系，并且讲解了把数学作为一门“语言”的思维方式，是数学入门，重新理解数学的科普佳作。该作者写的书都不错，这里只推荐其中最好懂的一本。改变世界的134个概率统计故事哲学家耶安哈金指出，统计学是1900年后人类的二十大发明之一。到了21世纪，正如家赫伯特乔治威尔斯在1903年所预言的那样，“统计式的思考将会和读写能力一样，成为优秀社会人士的必备技能”。此书以概率论为主，讲数学家的八卦。加上深入浅出的故事化讨论，让统计学不再枯燥。贝叶斯思维：统计建模的Python学习法前面都是科普书，这次来本教科书。这本书是根据作者在美国大学讲授相关课程的讲义编撰而成的。结合生活中的案例+代码实现+分析，让读者了解贝叶斯思维的威力，帮助你在生活的各个方面获得清晰的思维， 举书中的例子 战争环境下（二战德军坦克问题），法律问题上（肾肿瘤的假设验证），体育博彩领域（棕熊队和加人队NFL比赛问题），通过阅读，作者潜移默化的帮助读者形成了建模决策的方法论，建模误差和数值误差怎么取舍，怎样为具体问题建立数学模型，如何抓住问题中的主要矛盾（模型中的关键参数），再一步一步的优化或者验证模型的有效性或者局限性。程序员的数学编程的基础是计算机科学，而计算机科学的基础是数学。本书面向程序员介绍了编程中常用的数学知识，借以培养初级程序员的数学思维。读者无需精通编程，也无需精通数学，只需具备四则运算和乘方等基础知识，就可以阅读本书。这是一套书，分成三部分，涵盖线性代数概率论和基本的代数。接下来的书和复杂系统有关复杂如果你之前对复杂性科学还没有太多了解，那这本书可以成为你复杂性科学的第一本书。蚁群在没有中央控制的情况下为何会表现出如此精密的复杂行为？数以亿计的神经元是如何产生出像意识这样极度复杂的事物？是什么在引导免疫系统、互联网、全球经济和人类基因组等自组织结构？理解复杂系统需要有全新的方法，需要超越传统的科学还原论，并重新划定学科的疆域。借助于圣塔菲研究所的工作经历和交叉学科方法，复杂系统的前沿科学家米歇尔以清晰的思路介绍了复杂系统的研究，横跨生物、技术和社会学等领域，并探寻复杂系统的普遍规律，探讨了复杂性与进化、人工智能、计算、遗传、信息处理、代谢比例、网络科学等领域的关系。《Complexity_A Very Short Introduction》本书作者John Holland是复杂理论和非线性科学的先驱，遗传算法之父。本书介绍了复杂性科学的一些基本概念和核心架构，如complex physical system(CPS)、complex adaptive system(CAS)，描述了复杂系统的特征如涌现性质、自组织行为、混沌行为、胖尾分布、适应性行为。正如作者在写完这本书后意识到，将一些概念以最本质的简单的形式表现出来后，会发现一些原先分开的话题间被忽视的联系，希望你读完这本书后也有这样的感觉。而且，这本书真的是Very Short。复杂性思维本书是德国慕尼黑工业大学教授迈因策尔的代表作，14年出版的原书07年第五版的中译本。迈因策尔教授的研究范围遍及数学、物理学、科学哲学，尤其在复杂系统、非线性动力学等领域多有建树。本书从哲学的高度（在这里窃以为哲学不能指导科学，但可以为科学澄清意义），从科学前沿探索与人类心智探险史的结合中，广泛涉猎物理学、生命科学、认知科学、计算机科学、经济学、社会学等诸多方面。从物理世界的进化到生命世界的进化，从意识的起源到认知科学的兴起，从社会政治系统到社会经济系统的运行，从哲学史到哲学前沿的反思，揭示了不同学科体现出的共同的复杂性特征，阐释了对复杂性的探索将如何引起人们思维方式的深刻变化，引起的世人对共同未来的关怀。Thinking complexity要认识一门学科，不止需要了解概念，还需要亲自动手，get your hand dirty。Thinking complexity 以python为基础，演示了多种复杂系统的模型，让在计算机诞生之前难以验证的理论得以模拟，并逐步建立起复杂演绎基础之上的新认知模式。Python语言简单易懂，但书中的很多代码、练习有时间还得需要仔细研究实践。本书内容短小，但是信息量很大，关键看你是走马观花的读，还是一行行代码地进行实践了，收获是不一样的。复杂性科学涵盖了各种主题。这些主题之间相互关联，但需要花费不少时间才能搞清楚这些联系。为了帮助读者看到全景，这本书阅读列表，这些都来自于该领域最流行的研究成果。阅读列表以及关于如何使用它的建议在附录B中。这本书提供了一系列练习；很多练习都要求读者重新实现一些开创性实验并对其进行扩展。复杂性吸引人的一个地方在于我们可以通过适当的编程技能与数学知识接触研究前沿。这本书的内容覆盖：小世界图，无标度网络，细胞自动机，生命游戏，分形，自组织临界性，基于主体的模型(agent based model) 及几个现实中的案例分析。是复杂性研究入门参考好书。另外本书还可以用作Python编程与算法的大学中级课程教材。既是你对python和算法一无所知，其前三章的内容也可以让你能够接着看下去。《复杂_诞生于秩序与混沌边缘的科学》这是一本老书了，“类似于纪实小说，介绍了复杂性科学的研究中心圣塔菲研究所建立、发展的情况。你会看到那些不同领域的人是怎样由于共同的志趣走到了一起，以及又是如何涌现出诸如遗传算法、人工生命、细胞自动机、正反馈经济系统、动态博弈系统等等新思想的。这本书以小说一样的手法介绍了研究所里面个个人物的动人故事，以及他们研究的那些激动人心的成果。这本书的出版可以说给中国的学术界打开了一扇窗子，让我们真正的了解了国外的复杂性科学。有人称这本书是复杂性科学的圣经是不为过的。”大师说科学与哲学计算机与复杂性科学的兴起这是作者著名物理学家兼科学作家海因茨•R.帕格尔斯的遗作。是一本随笔集，将科学讨论带往更高层次，除了预言复杂性科学对人类的影响，也讨论了分道扬镳的科学与哲学如何才能重新融合。书中论及的话题包括：生物组织原理的重要性、以计算法来看数学及物理过程、并行计算网络以及非线性动力学的重要性、对混沌的了解、实验数学、神经网络和平行分配处理。下面的书将说说人类最担心的强AI的出现。Life 3.0世界两个顶级学术期刊\"nature\" 和“Science” 上每周都会推荐几本新出的科学主题的科普书，而一本书若是能同时被这俩家杂志推荐，则更是难得。今年8月25号出版《life 3.0》正是这样一本书，这本书的副标题是在人工智能的时代作为人意味着什么，作者不是专职搞计算机的，而是本行物理的普林斯顿教授。超级智能很多人提到强AI，说起的第一本书就是这个。本书作者尼克‧波斯特洛姆，全球著名思想家，牛津大学人类未来研究院的院长，哲学家和超人类主义学家。在这本书中，作者谈到了超级智能的优势所带来的风险，也谈到了人类如何解决这种风险。作者认为，他的这本书提到的问题将是我们人类所面临的最大风险。人工智能革命：超级智能时代的人类命运本书的难得之处还在于，它既不哗众取宠，也没有把问题过分简单化。本书作者毕业于蔡斯牛津大学哲学系，本书在人工智能的憧憬与危机之间敏锐地寻找平衡，对于所有好奇当今世界正在发生什么、我们是怎样走到今天、又将走向何方的人来说，本书都是一本必读书。如何思考会思考的机器关于强AI，一定需要大众的讨论，而这本书由世界上最聪明的头脑共同写成。包括全 球大数据权威阿莱克斯•彭特兰、世界顶级语言学家史蒂芬•平克、生物地理学家贾雷德•戴蒙德、互联网思想家凯文•凯利、《全球概览》创始人斯图尔特•布兰德等Edge 网站出品，必属精品。我们最后的发明这本书是一个纪录片导演的末世预言，核心观点是ASI（超级人工智能）极有可能毁灭人类，然后细致地逐个批判了库兹韦尔等乐观派。这本书的好处是好玩、有趣、思路清奇、剑出偏锋，但从知识的角度来说，它其实不是那么“科学”、不那么“理性”接着来说最火的深度学习白话深度学习与TensorFlow白话深度学习与TensorFlow这本书覆盖了深度学习的诸多概念，内容全面，看完了这本书，你就懂了深度学习这个领域的行话了。这本书也许不会教会你代码或者tensorflow，但却能让你明白深度学习是什么。书中包含很多具体例子，作者有丰富的实践经验。机器学习之路机器学习之路这本书从内容方面本书共包含两部分：机器学习篇和深度学习篇。这本书避过数学推导等复杂的理论推衍，介绍模型背后的一些简单直观的理解，以及如何上手使用。这本书适合有一些编程和自学能力，但数学等基础理论能力不足的人群。深度学习与R语言说起深度学习，想到的都是python为基础的语言，其实作为一种开源的数据建模语言，R也是可以做深度学习的。这本书介绍了深度学习基础知识后，着重介绍两种不那么流行的网络结构，受限玻耳兹曼机和深度置信网络，并通过生物信息和自然语言处理领域的实际例子来说明深度学习的优势和局限。最后说说AI的历史科学的极致 漫谈人工智能集智俱乐部有一群有激情有实力的小伙伴，其中既有来自学术界的张江教授，也有基于深度学习开发了彩云天气，彩云翻译等APP的创业者。而这本书则是集智众人的智慧结晶。这本书由于是中国人写成，所以避免了翻译作品的语言障碍。杨澜曾经在她的博客中推荐过这本书，说她从这本书中收获甚多，可见这本书是很容易读懂的。正如书名所显示，这本书涉及诸多人工智能领域。而书中诸多的插图，例子和参考文献则让这本书赢在了细节上。贤二机器僧漫游人工智能这本书贤二诞生的初衷和过程。书中的漫画超有趣。贤二机器僧这样一个传统佛法与现代科技相结合的产物表明，科技本身没有对错、好坏，它是中性的，但人的心却可善可恶。佛教徒不应该排斥科学，而应该拥抱科学，善于运用科技手段和成果，成就更多利于他人的事业。硅谷之谜作为浪潮之巅的续集，读完了这本书，想说的是硅谷的历史是不可复制的，AI的发展，是伴随着大公司的成败而起的。我们已经站在了AI发展的最前沿，不能只照搬前人的经验了。要做的是透彻的明白工业时代和后工业时代的本质，用一种全新的基于信息论、控制论、系统论的思维方式来从下而上的去想问题。"}
{"content2":"摘要： 阿里巴巴在美国西雅图挖来了亚马逊级别最高的华人科学家任小枫，进入该公司人工智能研发核心团队。随着人工智能逐渐成为BAT（百度、阿里巴巴、腾讯）未来的核心战略，中国科技公司在西雅图对亚马逊和微软的“抢人大战”愈演愈烈。阿里巴巴在美国西雅图挖来了亚马逊级别最高的华人科学家任小枫，进入该公司人工智能研发核心团队。随着人工智能逐渐成为BAT（百度、阿里巴巴、腾讯）未来的核心战略，中国科技公司在西雅图对亚马逊和微软的“抢人大战”愈演愈烈。6月25日，澎湃新闻记者发现，亚马逊前资深主任科学家（Senior Principal Scientist）任小枫已经更新了他在华盛顿大学官网上的个人主页，其最新任职为：阿里巴巴iDST首席科学家和副院长。阿里巴巴方面和任小枫本人均向澎湃新闻证实了这一消息。iDST（Institute of Data Science and Technologies，数据科学和技术研究院）是阿里巴巴负责人工智能技术研发的核心团队，成立于2014年，由一批杰出的科学家及工程师组成，分布在杭州、北京、西雅图、硅谷等地。随着阿里巴巴3月启动“NASA”计划，iDST正式成为NASA计划的人工智能大脑。NASA计划要面向未来20年组建强大的独立研发部门，建立新的机制体制，为服务20亿人的新经济体储备核心科技。任小枫曾领导Amazon Go无人零售店计算机视觉算法团队。 东方IC 资料值得注意的是，任小枫个人主页显示，他从2003年开始任职于亚马逊无人零售店Amazon Go部门，领导计算机视觉算法团队。Amazon Go是亚马逊最具革命性的创新项目之一，颠覆传统便利店、超市的运营模式，顾客通过App扫码进店后，在店内随意拿商品，在离店后亚马逊自动扣款，支付时 也不用打开手机，彻底跳过传统收银结帐的过程。这种即拿即走的购物体验，正是使用计算机视觉、深度学习以及传感器融合等技术。在阿里巴巴给澎湃新闻的回复中，阿里iDST团队盛赞任小枫“在计算机视觉领域中对图像分类，物体识别、跟踪、检测，事件检测均有全面且深入的理解，这在图像领域的专家中非常难得。”阿里巴巴相关人士告诉澎湃新闻，任小枫将与另一位副院长华先胜一起，在阿里巴巴不同业务的丰富场景下，提供计算机视觉服务，沉淀计算机视觉平台能力。任小枫在主页上表示，iDST在西雅图有一支快速扩充的队伍，“正在组建一支世界级的机器视觉团队”。任小枫说，自己继续留在“阳光明媚”的西雅图，自己喜爱这座城市。其个人主页显示，任小枫是中国国籍、美国永久居民。任小枫在接受澎湃新闻邮件采访时表示，因为家庭原因，还没有考虑回国。国际化是阿里一个重要的发展策略，他希望在美国能帮助阿里招揽国际一流的人才，帮助阿里走向世界。眼下，BAT正在西雅图展开如火如荼的“抢人大战”。西雅图是亚马逊、微软等公司的总部所在地。这些国外科技在人工智能方面有前瞻性的布局。任 小枫向澎湃新闻坦言，除了阿里之外，有几家中国大型的科技公司向他抛过橄榄枝，也有人找他创业。之所以最终选择阿里，任小枫表示，“在计算机视觉等人工智 能技术高速发展的今天，能看到一个足够聪明的电脑将会改变人们的生活。但是这聪明用在什么地方，怎么用，需要有具体的应用场景的支持和指导。阿里有非常多 的应用场景，也对人工智能技术非常的重视，在iDST投入很多力量来发展这些技术以及应用。我相信我加入阿里和iDST能有足够的支持和空间把计算机视觉 技术真正做好用好，能让我们的生活质量上一个台阶。”除此之外，杭州是任小枫的家乡，他说自己对阿里也有特殊的感情。任小枫主页显示，他拥有浙江大学计算机本科、斯坦福大学硕士及加州大学伯克利分校博士学位，目前还担任华盛顿大学计算机科学与工程系客座教授，相关论文被引用9000次以上。他还曾担任过CVPR、ICCV（顶级计算机会议）的领域主席。现年42岁的任小枫是亚马逊最年轻的资深主任科学家之一。据了解，亚马逊资深主任科学家（Senior Principal Scientist）是该公司高级别技术职称之一，仅次于最高级别的Distinguished Scientist，通常是领域研究团队的负责人。近年来，阿里巴巴正加快招徕技术人才的步伐。在任小枫之前，IEEE院士华先胜、前微软研发合伙人周靖人等都先后加入阿里，将阿里作为技术开疆拓土的新根据地。澎湃新闻了解到，过去一年间，在阿里巴巴整个集团技术线新员工中，来自海外知名互联网公司的技术人才占总数近30%。而最近诞生的“NASA”计划，更是驱使阿里在美国硅谷等地招募AI科学家。据了解，为了储备到全球最顶尖的科研人才，内部计划不设预算。iDST以机器学习和深度学习技术为依托，打造涵盖图像视频、语音交互、自然语言理解、智能决策等的人工智能核心技术，充分赋能电商、金融、物流、社交、娱乐等阿里巴巴集团重要业务。人工智能全覆盖的技术服务能力，通过阿里云向各行业输出，跟企业合力打造智能未来。目前iDST院长由金榕担任，为美国密歇根州立大学终身教授，曾担任NIPS、SIGIR等顶级国际会议领域主席，及KDD、AAAI、IJCAI等顶级会议高级程序委员会委员，还曾获得过美国国家科学基金会NSF Career Award。iDST院长金榕（中间）、副院长任小枫（左一）、副院长华先胜（右一）。【对话任小枫】澎湃新闻：听说中国很多科技公司都在西雅图“抢人”，你是否和其他公司接触过，为什么最终选择加入阿里巴巴iDST?任小枫：除 了阿里之外，我和其他几家中国大科技公司有过联系，也有人找我创业，不过我都没有深入的接触。之所以选择阿里，因为我觉得阿里是中国布局最广也是发展最好 的公司，富有活力重视创新，它提供的各类服务融入到了人们生活的方方面面。在计算机视觉和其他人工智能技术高速发展的今天，我们能看到一个足够聪明的电脑 将会改变人们的生活，让生活变得更加方便，也更加有意思。但是这聪明用在什么地方，怎么用，需要有具体的应用场景的支持和指导。阿里有非常多的应用场景， 也对人工智能技术非常的重视，在iDST投入很多力量来发展这些技术以及应用。我相信我加入阿里和iDST能有足够的支持和空间把计算机视觉技术真正做好 用好，能让我们的生活质量上一个台阶。杭州是我的家乡，我对阿里也有特殊的感情。澎湃新闻：iDST西雅图的工作内容是什么？任小枫：iDST现在在西雅图发展得很快。美国有很多第一流的人才，西雅图也是一个正快速发展的技术城市。我们希望在西雅图建立和发展起来一个国际一流的团队，开发出国际顶尖的和超越顶尖的技术，和国内的团队紧密合作来推动阿里集团各个方面的人工智能的发展和应用。澎湃新闻：对在iDST的工作有什么样的期待？任小枫：我希望能很快进入角色，理解阿里的业务，客户需求，了解现有的技术和团队，和大家一起做出国际领先的产品。我也希望很快融入阿里大家庭，和各个团队各位同学在一起愉快的工作。澎湃新闻：是否考虑过回国工作?任小枫：选择了阿里，我很高兴有了机会为中国人做事，为中国做贡献。因为家庭原因，我还没有考虑回国。国际化是阿里一个重要的发展策略，我也希望在美国能帮助阿里招揽国际一流的人才，帮助阿里走向世界。澎湃新闻：鉴于你之前在Amazon Go的经历，会不会为阿里新零售贡献一些创新想法？任小枫：目前工作还在熟悉之中，会积极地在所擅长的计算机视觉领域发挥技术能力。澎湃新闻：平常有什么样的兴趣爱好？任小枫：古人说，读万卷书，行万里路。我热爱知识也热爱生活，平时喜欢读各种书，也看电影，也喜欢出去旅游，顺便拍点照片记录下生活。澎湃新闻：有哪些人工智能相关的书或者视频可以推荐给大家？任小枫：书：《Vision Science: Photons to Phenomenology》，作者：Steven Palmer。这本书系统的论述了心理学对于人的视觉的研究和理解。视觉是个非常复杂的现象，涉及到很多方面，也花了很长时间才在自然界进化出来。里面很多东西不见得一下子能用上，但是在我们研究算法和提高精度的时候，经常想想人的视觉是怎么解决问题的，会有很多启发。在 不远的将来，智能系统可以识别和理解很多东西：各种生活的场景，场景里的各种物体，人的动作，甚至是人的意图和情绪。这个视屏很有意思，展示了未来的一种 可能。很多人不见得喜欢这样的增强现实，这没关系：有了高精度的智能，我们一定能找到最适合的人机交互的方式，来让我们的生活变得更方便。澎湃新闻：你觉得未来的人工智能会对世界产生什么样的影响？很多科幻电影或电视剧描述过未来世界，你个人推崇哪部？任小枫：很多人受媒体和知识界的影响，一直对人工智能有偏见，觉得人工智能就是要造出像人一样的机器人来，跟人竞争。美剧《西部世界》就是一个典型的例子。其实很多研究人工智能的人都会觉得那并不是我们的目标。我们的目标是能够让机器变得足够聪明能干，成为更好的工具来帮 助人，帮人把工作做的更好，让人的生活变的更好，就像手机和汽车。我觉得人工智能一定会影响到世界的各个方面：无论我们做什么事情，如果有了聪明的工具的 帮助，我们的效率会更高，体验会更好。原文链接"}
{"content2":"机器学习和计算机视觉都是很多种数学的交汇场。看着不同的理论体系的交汇，对于一个researcher来说，往往是非常exciting的enjoyable的事情。不过，这也代表着要充分了解这个领域并且取得有意义的进展是很艰苦的。 Linear Algebra (线性代数) 和 Statistics (统计学) 是最重要和不可缺少的。 这代表了Machine Learning中最主流的两大类方法的基础。一种是以研究函数和变换为重点的代数方法，比如Dimension reduction，feature extraction，Kernel等，一种是以研究统计模型和样本分布为重点的统计方法，比如Graphical model, Information theoretical models等。它们侧重虽有不同，但是常常是共同使用的，对于代数方法，往往需要统计上的解释，对于统计模型，其具体计算则需要代数的帮助。以代数和统计为出发点，继续往深处走，我们会发现需要更多的数学。 Calculus (微积分)，只是数学分析体系的基础。 其基础性作用不言而喻。Learning研究的大部分问题是在连续的度量空间进行的，无论代数还是统计，在研究优化问题的时候，对一个映射的微分或者梯度的分析总是不可避免。而在统计学中，Marginalization和积分更是密不可分——不过，以解析形式把积分导出来的情况则不多见。 Partial Differential Equation （偏微分方程)，这主要用于描述动态过程，或者仿动态过程。 这个学科在Vision中用得比Learning多，主要用于描述连续场的运动或者扩散过程。比如Level set, Optical flow都是这方面的典型例子。 Functional Analysis (泛函分析)，通俗地，可以理解为微积分从有限维空间到无限维空间的拓展——当然了，它实际上远不止于此。 在这个地方，函数以及其所作用的对象之间存在的对偶关系扮演了非常重要的角色。Learning发展至今，也在向无限维延伸——从研究有限维向量的问题到以无限维的函数为研究对象。Kernel Learning 和Gaussian Process 是其中典型的例子——其中的核心概念都是Kernel。很多做Learning的人把Kernel简单理解为Kernel trick的运用，这就把kernel的意义严重弱化了。在泛函里面，Kernel (Inner Product)是建立整个博大的代数体系的根本，从metric, transform到spectrum都根源于此。 Measure Theory (测度理论)，这是和实分析关系非常密切的学科。但是测度理论并不限于此。 从某种意义上说，Real Analysis可以从Lebesgue Measure（勒贝格测度）推演，不过其实还有很多别的测度体系——概率本身就是一种测度。测度理论对于Learning的意义是根本的，现代统计学整个就是建立在测度理论的基础之上——虽然初级的概率论教科书一般不这样引入。在看一些统计方面的文章的时候，你可能会发现，它们会把统计的公式改用测度来表达，这样做有两个好处：所有的推导和结论不用分别给连续分布和离散分布各自写一遍了，这两种东西都可以用同一的测度形式表达：连续分布的积分基于Lebesgue测度，离散分布的求和基于计数测度，而且还能推广到那种既不连续又不离散的分布中去（这种东西不是数学家的游戏，而是已经在实用的东西，在Dirchlet Process或者Pitman-Yor Process里面会经常看到)。而且，即使是连续积分，如果不是在欧氏空间进行，而是在更一般的拓扑空间（比如微分流形或者变换群），那么传统的黎曼积分（就是大学一年级在微积分课学的那种）就不work了，你可能需要它们的一些推广，比如Haar Measure或者Lebesgue-Stieltjes积分。 Topology（拓扑学)，这是学术中很基础的学科。 它一般不直接提供方法，但是它的很多概念和定理是其它数学分支的基石。看很多别的数学的时候，你会经常接触这样一些概念：Open set / Closed set，set basis，Hausdauf, continuous function，metric space, Cauchy sequence, neighborhood, compactness, connectivity。很多这些也许在大学一年级就学习过一些，当时是基于极限的概念获得的。如果，看过拓扑学之后，对这些概念的认识会有根本性的拓展。比如，连续函数，当时是由epison法定义的，就是无论取多小的正数epsilon，都存在xxx，使得xxx。这是需要一种metric去度量距离的，在general topology里面，对于连续函数的定义连坐标和距离都不需要——如果一个映射使得开集的原像是开集，它就是连续的——至于开集是基于集合论定义的，不是通常的开区间的意思。这只是最简单的例子。当然，我们研究learning也许不需要深究这些数学概念背后的公理体系，但是，打破原来定义的概念的局限在很多问题上是必须的——尤其是当你研究的东西它不是在欧氏空间里面的时候——正交矩阵，变换群，流形，概率分布的空间，都属于此。 Differential Manifold (微分流形)，通俗地说它研究的是平滑的曲面。 一个直接的印象是它是不是可以用来fitting一个surface什么的——当然这算是一种应用，但是这是非常初步的。本质上说，微分流形研究的是平滑的拓扑结构。一个空间构成微分流形的基本要素是局部平滑：从拓扑学来理解，就是它的任意局部都同胚于欧氏空间，从解析的角度来看，就是相容的局部坐标系统。当然，在全局上，它不要求和欧氏空间同胚。它除了可以用于刻画集合上的平滑曲面外，更重要的意义在于，它可以用于研究很多重要的集合。一个n-维线性空间的全部k-维子空间(k Lie Group Theory (李群论)，一般意义的群论在Learning中被运用的不是很多，群论在Learning中用得较多的是它的一个重要方向Lie group。 定义在平滑流形上的群，并且其群运算是平滑的话，那么这就叫李群。因为Learning和编码不同，更多关注的是连续空间，因为Lie group在各种群中对于Learning特别重要。各种子空间，线性变换，非奇异矩阵都基于通常意义的矩阵乘法构成李群。在李群中的映射，变换，度量，划分等等都对于Learning中代数方法的研究有重要指导意义。 Graph Theory（图论)，图，由于它在表述各种关系的强大能力以及优雅的理论，高效的算法，越来越受到Learning领域的欢迎。 经典图论，在Learning中的一个最重要应用就是graphical models了，它被成功运用于分析统计网络的结构和规划统计推断的流程。Graphical model所取得的成功，图论可谓功不可没。在Vision里面，maxflow (graphcut)算法在图像分割，Stereo还有各种能量优化中也广受应用。另外一个重要的图论分支就是Algebraic graph theory (代数图论)，主要运用于图的谱分析，著名的应用包括Normalized Cut和Spectral Clustering。近年来在semi-supervised learning中受到特别关注。 这是大牛们做的很好的综述啊！ （来源：CSDN）"}
{"content2":"1）验证码的基本知识及来由网络安全技术中的验证码的主要目的是强制人机交互来抵御机器自动化攻击。用来防止机器模拟http行为，直接抓取文本进行导航；或直接提交文本进行登录尝试。在现在带宽较大的今天，在线密码穷举带宽已经不能作为瓶颈了，验证码识别以2M ADSL连接实际测试，20线程大概每秒可以完成30个左右的连接，如果是6位数字密码，在不考虑字典完全穷举的时候也只需要几个小时便可破解，严重的威胁了网络账号的安全，因此，网络登录注册验证码的普及势在必行。而如今国内大部分的验证码设计者并不得要领，要么不了解图像处理、机器视觉、模式识别、人工智能的基本概念；要么设计出的验证码连人都难以识别，导致用户 体验度下降。 比如工商银行的WAP手机银行验证码，只有4位，而且验证码的薄弱形同虚设，使用穷举计算机很快就能破解一个六位数字密码的账户。当然，也有设计得比较好 的，比如Yahoo、Google、baidu等。2）验证码的展望未来的网络安全验证码，可能更多地使用渐进色层、同级灰度色差，更多曲线反转、随机字符数量、字符粘连等手段防止机器的识别，但加密与破解总是一对孪生兄弟，不可能消失一方的。我们只是希望通过本文，给编写验证码算法的人员一些建议，使我们的网络更安全，操作也不会因此而繁琐。算法分析在验证码处理方面，我们大概要涉及到如下内容：人工智能、模式识别、机器视觉、图像处理。1） 主要流程：如果我们要从一幅图片中识别出验证码；又或者我们要从一幅图片中检测并识别出一个字符，其步骤可概括如下：图像采集：取得一个验证码，就直接通过HTTP抓HTML，然后分析出图片的URL，下载保存。预处理：检测是正确的图像格式，转换到合适的格式，压缩，剪切出ROI，去除噪音，灰度化，转换色彩空间。检测：找出文字所在的主要区域。前处理：文字的切割、缩放和扭曲校正。训练：通过各种模式识别，机器学习算法，来挑选和训练合适数量的训练集。训练的样本并非越多越好。通过学习，泛化能力差的问题可能会出现在这里。识别：输入待识别的处理后的图片，转换成分类器需要的输入格式，通过输出的类和置信度，来判断大概可能是哪个字母。识别本质上就是分类。2）关键概念图像处理：一般指针对数字图像的某种数学处理，比如投影、钝化、锐化、细化、边缘检测、二值化、压缩，以及各种数据变换等等。二值化：一般图片都是彩色的，按照逼真程度，可能很多级别。为了降低计算复杂度，方便后续的处理，如果在不损失关键信息的情况下，能将图片处理成黑白两种颜色，那就最好不过了。细化：找出图像的骨架，图像线条可能是很宽的，通过细化将宽度降为1，某些地方可能大于1。不同的细化算法，可能有不同的差异，比如是否更靠近线条中间，比如是否保持联通行等。边缘检测：主要是理解边缘的概念。边缘实际上是图像中图像像素属性变化剧烈的地方，可以通过一个固定的门限值来判断，也可以是自适应的。门限可以是图像全 局的，也可以是局部的。不能说哪个就一定好，不过大部分时候，自适应的局部的门限可能要好点。被分析的可能是颜色，也可能是灰度图像的灰度。机器视觉：利用计算机来模式实现人的视觉，比如物体检测、定位、识别。按照对图像理解的层次的差别，分高阶和低阶的理解。模式识别：对事物或者现象的某种表示方式（数值、文字，我们这里主要想说的是数值），通过一些处理和分析来描述、归类、理解、解释这些事物、现象及其某种抽象。人工智能：这种概念比较宽，上面这些都属于人工智能这个大的方向。简单点不要过分学院派的理解就是，把人类的很“智能”的东西给模拟出来，协助生物的人来处理问题，特别是在计算机里面。验证码识别原理及代码演示本来拿一个银行网站来进行验证码解密是很危险的，但我们发现，工行的算法已经进行了改变，所以姑且以之前的工行WAP银行做个举例好了，同时也希望工行的加密能越做越好。其实工行的WAP验证码是很简单的，是未加干扰的原始字符打印图片而已。针对这种验证码，我们将使用点阵库校验的方式进行，首先从整个程序的编写及操作顺序开始。首先要知道我们需要取得的字的点阵有哪些。工行的WAP银行验证码只有0~9，10个数字，那么我们先将验证码图片下载到本机，这里我们必须将所有字符的图样都下载到本机，以便建立基础点阵库。得到这些图片文件后，我们将用程序来获得图片点阵。从本地磁盘加载一个图像文件，这个文件是我们已经下载好的。首先应该让程序先将0~9的图像都“识别” 一遍，使我们的程序“记住”它们的点阵，样例如所示。该图像包含的验证码，从左到右就是0123，将这个图像逐点转换灰度，也就是将彩色图片先进行灰 度化、去色，变成黑白照片，便于下一步操作。{for (int i = 0; i < bmpobj.Height; i++)//遍历高度{for (int j = 0; j < bmpobj.Width; j++)//遍历宽度，双层for就循环了整个图片的像素点{int tmpValue = GetGrayNumColor(bmpobj.GetPixel(j, i));bmpobj.SetPixel(j, i, Color.FromArgb(tmpValue, tmpValue, tmpValue));}}}灰度化之后，像素的RGB三色都是相同的值了，亮度从0~255（HxFF）。但用于识别程序，灰度值并不能很好的区分背景色和前景色，尤其是对于渐进的 背景来说，所以我们还要将图像进一步处理，就是将灰度图片2值化，类似的算法还有分水岭算法等。因为本文中的验证码相对简单，故直接使用2值化转换，寻找 有效区并转为单色黑白图。{int dgGrayValue = 128 //灰度背景分界值int CharsCount = 4 //有效字符数，已知int posx1 = bmpobj.Width; int posy1 = bmpobj.Height;int posx2 = 0; int posy2 = 0;for (int i = 0; i < bmpobj.Height; i++)//找有效区{for (int j = 0; j < bmpobj.Width; j++){int pixelValue = bmpobj.GetPixel(j, i).R;//取得红色值R，因为转成黑白图后，红、黄、蓝三位都是一样的值，所以这里取什么色值都是一样的if (pixelValue < dgGrayValue) //根据灰度值{if (posx1 > j) posx1 = j;if (posy1 > i) posy1 = i;if (posx2 < j) posx2 = j;if (posy2 < i) posy2 = i;}}}//确保能整除int Span = CharsCount - (posx2 - posx1 + 1) % CharsCount;//可整除的差额数if (Span < CharsCount){int leftSpan = Span / 2;//分配到左边的空列，如span为单数，则右边比左边大1if (posx1 > leftSpan)posx1 = posx1 - leftSpan;if (posx2 + Span - leftSpan < bmpobj.Width)posx2 = posx2 + Span - leftSpan;}//复制新图Rectangle cloneRect = new Rectangle(posx1, posy1, posx2 - posx1 + 1, posy2 - posy1 + 1);bmpobj = bmpobj.Clone(cloneRect, bmpobj.PixelFormat);}Bitmap[] pics = GetSplitPics(4, 1); //分割，pics[0]中的图片如所示在平均分割图片的部分，设置水平上分割数为RowNum，垂直上分割数为ColNum，返回分割好的图片数组，程序编写如下：public Bitmap[] GetSplitPics(int RowNum, int ColNum){if (RowNum == 0 || ColNum == 0)return null;int singW = bmpobj.Width / RowNum;int singH = bmpobj.Height / ColNum;Bitmap[] PicArray = new Bitmap[RowNum * ColNum];Rectangle cloneRect;for (int i = 0; i < ColNum; i++)//找有效区{for (int j = 0; j < RowNum; j++){cloneRect = new Rectangle(j * singW, i * singH, singW, singH);PicArray[i * RowNum + j] = bmpobj.Clone(cloneRect, bmpobj.PixelFormat);//复制小块图}}return PicArray;}此时图像分割已结束，pics 的长度应该是4，并且每一个pics就是一个验证码的位图，经过错误处理，修边，和去除无用背景空白，修正完的位图为数字0。得到有效图形后，由外面传入该图形，设置灰度背景分界值为“dgGrayValue”，有效字符数为CharsCount，程序编写如下：public Bitmap GetPicValidByValue(Bitmap singlepic, int dgGrayValue){int posx1 = singlepic.Width; int posy1 = singlepic.Height;int posx2 = 0; int posy2 = 0;for (int i = 0; i < singlepic.Height; i++)//找有效区{for (int j = 0; j < singlepic.Width; j++){int pixelValue = singlepic.GetPixel(j, i).R;if (pixelValue < dgGrayValue) //根据灰度值{if (posx1 > j) posx1 = j;if (posy1 > i) posy1 = i;if (posx2 < j) posx2 = j;if (posy2 < i) posy2 = i;};};};//复制新图Rectangle cloneRect = new Rectangle(posx1, posy1, posx2 - posx1 + 1, posy2 - posy1 + 1);return singlepic.Clone(cloneRect, singlepic.PixelFormat);}至此，pics图像组中就是有效的点阵图了。下面我们把pics中的图形转换为代表点阵的字符串，返回灰度图片的点阵描述字串，1表示灰点，0表示背景。设置灰度图为singlepic，背前景灰色界限为dgGrayValue。string code = GetSingleBmpCode(pics[0], 128);public string GetSingleBmpCode(Bitmap singlepic, int dgGrayValue){Color piexl;StringBuilder sbCode = new StringBuilder();for (int posy = 0; posy < singlepic.Height; posy++)for (int posx = 0; posx < singlepic.Width; posx++){piexl = singlepic.GetPixel(posx, posy);if (piexl.R < dgGrayValue)// Color.Black )sbCode.Append('1');elsesbCode.Append('0');}return sbCode.ToString();}此时，code中的字符串就代表字符0在工行WAP银行上图像验证码的值了；以此类推，我们可以得到一个完整的，代表图像0~9的数组，字符表的顺序为0~9，A~Z，a~z。现在，图片点阵数组已经取得了，接下来我们看看如何把一个图片识别出来吧！已知如下的点阵表：string[] CodeArray = new string[] {\"0011100011011011000111100011110101111010111100011110001101101100011100\",\"001100011100111100001100001100001100001100001100001100111111\",\"0111110110001100000110000110000110000110000110000110000011000111111111\",\"0111110110001100000110000011001111000000110000011000001111000110111110\",\"0000110000111000111100110110110011011111110000110000011000001100001111\",\"00011111000110000001100000011000000111111000000010000000100000001001100000001111\",\"001110011000110000110000111111110001110001110001110001011111\",\"00011111000110000000000000000000000000011000001110000110100001101000011000000110\",\"0111110110001111000111100011011111011000111100011110001111000110111110\",\"0111110110001111000111100011011111100000110000011000001100001100111100\" };开始处理比较操作：StringBuilder sbResult = new StringBuilder();{for (int i = 0; i < 4; i++){string code = GetSingleBmpCode(pics[i], 128);//得到代码串System.Collections.Generic.Dictionary<char, double> EqualsPercentList = new Dictionary<char, double>(); //建立差异程度列表for (int arrayIndex = 0; arrayIndex < CodeArray.Length; arrayIndex++){if (arrayIndex < 10)//0~9{EqualsPercentList.Add((char)(48 + arrayIndex), 100);//数字转字符，c语法习惯}}for (int arrayIndex = 0; arrayIndex < CodeArray.Length; arrayIndex++) //和点阵表内的字符序列进行比较{if (arrayIndex < 10)//0~9{EqualsPercentList[(char)(48 + arrayIndex)] = EqualsPercent(code, CodeArray[arrayIndex]);//每一个图像的点阵的差异度，这里的差异度运算是误差程度，也就是不同的百分比}进行获得匹配{double Perc = 20; //差异百分比必须小于20%，否则肯定不对string SelectKey=\"\";foreach (char key in EqualsPercentList.Keys)//获取匹配程度列表中最匹配的一项{if (EqualsPercentList[key] < Perc){Perc = EqualsPercentList[key];SelectKey = key.ToString();}}sbResult.Append(SelectKey);}至此，sbResult中的4个数字就是图像上的4个数字了。结论验证码识别肯定不只是这么简单，但现在还是有很多网站都在用这种未经任何变换的验证码，所以我们的网络安全还任重而道远。上面的验证码识别是一个最基本的算法，但是很多扩展算法都可以基于上面的思路进行扩充。例如有些验证码进行了旋转输出，那么上面的程序可以在校对的时候， 进行360度旋转，旋转后的图像再取得序列，再和图像序列比较，直至得到最符合的。有些验证码添加了边框，此时我们可以先去掉边框再进行切割匹配。通过上面的算法可以得出，我们今后在设计验证码的时候，应该注意如下因素：1）在噪音等类型的使用上，尽力让字符和用来混淆的前景和背景不容易区分，尽力让噪音长得和字母一样。2）特别好的验证码的设计，要尽力发挥人类擅长而AI算法不擅长的。比如粘连字符的分割和手写体（通过印刷体做特别的变形也可以），而不要一味的去加一些看起来比较复杂的噪音或者其他的花哨东西。3）从专业的机器视觉的角度来说，网络安全验证码的设计，一定要让破解者在识别阶段，反复在低阶视觉和高阶视觉之间多反复几次才能识别出来，这样可以大大降低破解难度和破解的准确率。"}
{"content2":"理解人工智能人工智能定义，领域，就业范围1.人工智能（Artificial Inteligence),英文缩写为AI，它是研究，开发用于模拟，延伸和扩展人的智能的理论，方法，技术及应用系统的一门新的技术科学，人工智能是计算机科学的一个分支，它企图了解智能的实质并生产出一种新的能以人类智能相似的方式做出反应的智能机器，未来人工智能带来的科技产品，将会是人类智慧的“容器”。2.人工智能研究的领域机器翻译智能机器人语音识别与合成图像处理与计算机视觉虚拟现实技术与应用计算机神经网络知识发现与机器学习3.人工智能就业范围前端工程师，系统运维工程师，爬虫开发工程师，数据分析工程师，测试自动化工程师，人工智能+物联网+Python，游戏开发工程师，算法工程师，人工智能工程师，Python开发工程师，搜索引擎工程师，互联网信息安全，iOS安卓开发一.何为机器学习已有的数据（经验）某种模型（迟到的规律）利用此模型预测未来（是否迟到）机器学习界 “ 数据为王” 思想（既是对数据进行好好地处理）二.人工智能与机器学习关系关系模式识别 = 机器学习数据挖掘 = 机器学习+数据库统计学习近似等于机器学习计算机视觉=图像处理+机器学习语音识别= 语音处理+机器学习自然语言处理 = 文本处理+机器学习人工智能是机器学习的父类，机器学习是深度学习的父类三.人工智能应用与价值智慧的最佳体现是什么计算：云计算；    推理：专家系统；    知识：数据仓库；    灵敏：事件驱动；    检索：搜索引擎；    智慧：机器学习四.有监督机器学习有监督学习流程历史数据->模型训练->模型+（新数据）->预测结果五.有监督机器学习训练流程六.Python机器学习Scikit-Learn介绍七.理解线性和回归线性回归线性：两个变量之间存在一次方函数关系，就称它们之间存在线性关系线性：线性linear，指量与量之间按比例，成直线的关系，在空间和时间上代表规则和光滑的运动八.机器学习有四类不同的学习方法有监督的机器学习data中包含（X,Y）有些Y标记是连续的，回归有些Y标记是离散的，分类"}
{"content2":"2D图像几何基元一般的，表示一个2d几何基元只用两个维度(比如x，y)就可以表示了，但是在计算机视觉研究中，为了统一对2d几何基元的操作（后面讲到的仿射，透射变换）,一般会以增广矢量的方式表示几何基元。齐次坐标将原本n维的坐标用一个n+1维的坐标表示，其两个基本作用为：1. 区分n维空间的点和向量，一个点的第n+1维为非零值，而向量的n+1维为02. 统一几何基元的旋转，平移，拉伸，投影等操作（只用一个矩阵就可以表示）2D点：2D点的齐次坐标表示为：，其中仅在尺度上不同的矢量被视为等同的，被称作为2D投影空间，其次矢量可以通过除以最后你一个元素来转换为非齐次矢量X，即：其中，是增广矢量，如果最后w为0，则称此点为理想点或者无穷远点，它没有等同的非齐次表达2D直线：2D直线可以用齐次向量表示，其对应的直线方程为：，其规范化表达为，如果 l = (0,0,1)，则包含所有无穷远点2D圆锥曲线：2D图像变换及示例利用齐次坐标，我们可以把统一2D变换操作平移变换：，其中I是2x2的单位矩阵旋转变换：放缩变换（x，y分别放缩）：仿射变换：投影变换：因为而仿射变换是投影变化的子集，所有对2D几何基元的操作都以用一个3x3的Homography表示，这样一来对图像的多次2D变换就相当于图像对多个Homography矩阵的乘积，即R = H1*H2*H3...Hn*r我们用Python代码表示这个过程1 import numpy as np 2 import cv2 as cv 3 from numba import jit 4 import matplotlib.pyplot as plt 5 from matplotlib.font_manager import FontProperties 6 7 def MoveT(tx, ty, lastOp = None)->np.ndarray: 8 op = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]], dtype=np.float32) 9 if lastOp is not None: 10 op = np.dot(op, lastOp) 11 return op 12 13 14 def RotateT(r, lastOp = None)->np.ndarray: 15 op = np.array([[np.cos(r), -np.sin(r), 0], [np.sin(r), np.cos(r), 0], [0, 0, 1]], dtype=np.float32) 16 if lastOp is not None: 17 op = np.dot(op, lastOp) 18 return op 19 20 21 def ZoomT(rx, ry, lastOp = None)->np.ndarray: 22 op = np.array([[rx, 0, 0], [0, ry, 0], [0, 0, 1]], dtype=np.float32) 23 if lastOp is not None: 24 op = np.dot(op, lastOp) 25 return op 26 27 def Transform(imgSrc:np.ndarray, op)->np.ndarray: 28 h, w = imgSrc.shape[:2] 29 imgDst = np.zeros(imgSrc.shape, dtype=imgSrc.dtype) 30 for i in range(h): 31 v1 = np.stack((np.arange(w), np.ones(w)*i, np.ones(w)),axis=-1) 32 v2 = np.dot(v1, op) 33 tpx, tpy, tpz = np.hsplit(v2, 3) 34 for iy, ix, iz, j in zip(tpy, tpx, tpz, range(w)): 35 py, px = int(iy/iz), int(ix/iz) 36 if 0<= py < h and 0 <= px < w: 37 imgDst[int(py), int(px)] = imgSrc[i, j] 38 39 return imgDst1 if __name__ == \"__main__\": 2 font_set = FontProperties(fname=r\"c:\\windows\\fonts\\msyh.ttc\", size=12) 3 imgSrc = plt.imread(\"E:/Users/Administrator/pictures/Test/user.jpg\") 4 op = np.transpose(MoveT(10,30, RotateT(np.pi/12, ZoomT(1.1, 1.2)))) 5 6 imgDst = Transform(imgSrc, op) 7 plt.figure(1), plt.imshow(imgDst), plt.title(\"Resuult\", fontproperties=font_set) 8 plt.show()我们对图进行了如下操作：1. 对y尺度放大1.1倍，x尺度放大1.2倍2. 绕z轴（垂直于x,y）旋转π/12度3. x方向移动30像素，y方向移动10像素fig1: 原图fig2: 仿射变换结果可以看到这个结果并不是很好，在结果中发现一些很明显的空洞，因为我们对图像进行了放大操作和旋转操作，在这一个过程中结果图像的有一些像素的值并不能从仿射变换中得出，从而形成空洞，可见直接直接对图像进行正向变换效果是不好的。下面我们将会看到利用反向变换+插值的方法解决空洞的问题。利用透射变换来进行倾斜校正透射变换的一个应用就是拿来校正图像，原理很简单，就是把根据关键点找到变换矩阵假如使用矩阵，并且设h33 = 1，那么就要把剩下的8个参数给解出来，8个未知数需要8个方程，又因为从仿射变换的对应关系，可以列方程为：也就是说只要找8个点的以及其两两对应关系，就可以找到透射变换的矩阵，也就可以对图像进行校正了透射变换有放缩和旋转等操作，正如上一小节所说，如果仅使用正向变换，那么肯定会有空洞现象的发生。如果使用反向变换，那么这个问题也就迎刃而解了，我们要做的就是扫描反向变换的位置，在这里我们可以采用原图像正向变换后上下左右的边界点确定扫描区域，然后一个一个点反向变换回原图像中，并根据原图像来进行插值Python代码1 # WarpCorrection.py 2 3 @jit 4 def BilinearInterpolation(imgSrc:np.ndarray, h, w, sx:float, sy:float)->float: 5 \"\"\" 6 对图片的指定位置做双线性插值 7 :param imgSrc:源图像 8 :param h: src的高度 9 :param w: src的宽度 10 :param sx: x位置 11 :param sy: y位置 12 :return: 所插入的值 13 \"\"\" 14 intSx, intSy = int(sx), int(sy) 15 if 0 <= intSx < w - 1 and 0 <= intSy < h - 1: 16 x1, x2 = intSx, intSx + 1 17 y1, y2 = intSy, intSy + 1 18 H1 = np.dot(np.array([x2 - sx, sx - x1]), imgSrc[y1: y2 + 1, x1:x2 + 1]) 19 return H1[0]*(y2 - sy) + H1[1]*(sy - y1) 20 else: 21 return imgSrc[intSy, intSx] 22 23 def WarpCorrection(imgSrc:np.ndarray, dots:tuple)->np.ndarray: 24 assert len(dots) == 4 25 26 # 四个点的顺序一定要按照左上，右上，右下，左下的顺时针顺序点 27 d1, d2, d3, d4 = dots 28 x1, x2, x3, x4 = d1[0], d2[0], d3[0], d4[0] 29 y1, y2, y3, y4 = d1[1], d2[1], d3[1], d4[1] 30 assert x1 < x2 31 assert x4 < x3 32 assert y1 < y4 33 assert y2 < y3 34 35 objW = np.round(np.sqrt((x1 - x2) ** 2 + (y1 - y2) ** 2)) 36 objH = np.round(np.sqrt((x2 - x3) ** 2 + (y2 - y3) ** 2)) 37 38 # 在这里我简单地设为把所输入的四个点的位置，通过2D变换，变换为长方形的四个顶点的位置（以x1为起点） 39 t1, t2, t3, t4 = (y1, x1), (y1, x1 + objW), (y1 + objH, x1 + objW), (y1 + objH, x1), 40 41 rx1, rx2, rx3, rx4 = t1[1], t2[1], t3[1], t4[1] 42 ry1, ry2, ry3, ry4 = t1[0], t2[0], t3[0], t4[0] 43 44 # ================Step 0: 根据 8个点两两对应关系找到Homography矩阵================ 45 # 把8个约束写成方程组，以矩阵的形式表达 46 m = np.array([ 47 [y1, x1, 1, 0, 0, 0, -ry1 * y1, -ry1 * x1], 48 [0, 0, 0, y1, x1, 1, -rx1 * y1, -rx1 * x1], 49 [y2, x2, 1, 0, 0, 0, -ry2 * y2, -ry2 * x2], 50 [0, 0, 0, y2, x2, 1, -rx2 * y2, -rx2 * x2], 51 [y3, x3, 1, 0, 0, 0, -ry3 * y3, -ry3 * x3], 52 [0, 0, 0, y3, x3, 1, -rx3 * y3, -rx3 * x3], 53 [y4, x4, 1, 0, 0, 0, -ry4 * y4, -ry4 * x4], 54 [0, 0, 0, y4, x4, 1, -rx4 * y4, -rx4 * x4], 55 ]) 56 57 vectorSrc = np.array([ry1, rx1, ry2, rx2, ry3, rx3, ry4, rx4]) 58 vectorSrc.shape = (1, 8) 59 HFlat = np.dot(np.linalg.inv(m), np.transpose(vectorSrc)) 60 a, b, c, d, e, f, g, h = HFlat[0, 0],HFlat[1, 0],HFlat[2, 0],HFlat[3, 0],HFlat[4, 0],HFlat[5, 0],HFlat[6, 0],HFlat[7, 0] 61 62 H = np.array([[a, b, c], 63 [d, e, f], 64 [g, h, 1]], dtype=np.float32) 65 66 # ================Step 1: 通过对原图像四个顶点进行正向投射变换，确定目标图像区域================ 67 height, width = imgSrc.shape[:2] 68 matrixOriginVertex = np.array([[0, 0, 1], 69 [0, width - 1, 1], 70 [height - 1, width - 1, 1] , 71 [height - 1, 0, 1]]) 72 73 result = np.dot(matrixOriginVertex, np.transpose(H)) 74 minX = int(min(result[0, 1]/result[0, 2], result[1, 1]/result[1, 2], result[2, 1]/result[2, 2], result[3, 1]/result[3, 2])) 75 maxX = int(max(result[0, 1]/result[0, 2], result[1, 1]/result[1, 2], result[2, 1]/result[2, 2], result[3, 1]/result[3, 2])) 76 minY = int(min(result[0, 0]/result[0, 2], result[1, 0]/result[1, 2], result[2, 0]/result[2, 2], result[3, 0]/result[3, 2])) 77 maxY = int(max(result[0, 0]/result[0, 2], result[1, 0]/result[1, 2], result[2, 0]/result[2, 2], result[3, 0]/result[3, 2])) 78 79 # ================Step 2: 反向变换+双二次插值校正图像================ 80 vtr = np.empty((0,3),dtype=np.float32) 81 for i in range(minY, maxY): 82 arr1 = np.arange(minX, maxX) 83 arr2 = np.ones(maxX - minX) 84 vt1 = np.stack((arr2*i, arr1 , arr2), axis=-1) 85 vtr = np.concatenate((vtr, vt1), axis=0) 86 87 # 请注意，因为传进去的是规范化后(Y, X, 1)的值，所以得到的其实是(y/Z, x/Z, 1/Z的值) 88 vts = np.dot(vtr,np.linalg.inv(np.transpose(H))) 89 dstHeight, dstWidth = maxY - minY + 1, maxX - minX + 1 90 imgDst = np.zeros((dstHeight, dstWidth, imgSrc.shape[2]), dtype=imgSrc.dtype) 91 92 for (r, s) in zip(vtr, vts): 93 ry, rx = int(r[0]), int(r[1]) 94 iy, ix = s[:2] 95 # 需要解 [y, x] = [iy*(g*y + h*x + 1), ix*(g*y + h*x + 1)]这个方程 96 TH = np.linalg.inv(np.array([[iy * g - 1, iy * h], 97 [ix * g, ix * h - 1]])) 98 99 vxy = np.dot(TH, np.array([[-iy], [-ix]])) 100 sy, sx = vxy[0, 0], vxy[1, 0] 101 102 if 0 <= round(sy) < height and 0 <= round(sx) < width: 103 imgDst[ry - minY, rx - minX] = BilinearInterpolation(imgSrc, height, width, sx, sy) 104 105 return imgDst这里使用了双二次插值，双二次插值的公式推导如下：fig3. 双二次插值公式推导fig4. 双二次插值图像解释上面右图所展示的那样，双二次插值其实就是一个加权平均操作，为了和双二次插值的效果做对比，我加了个最邻近插值继续往程序里面加点东西，来测试下实际效果，在这里我使用了OpenCV的Highgui的包1 # -*- coding: utf-8 -*- 2 import matplotlib.pyplot as plt 3 from matplotlib.font_manager import FontProperties 4 from WarpTransform import * 5 from multiprocessing.dummy import Process 6 7 windowCount = int(0) 8 mainWinName = \"source\" 9 10 def WarpImage(imgSrc:np.ndarray, dots:tuple, count)->None: 11 imgBLiner, imgNearest = WarpCorrection(imgSrc, dots) 12 13 winName:str = f\"result BLiner {count}\" 14 cv.namedWindow(winName) 15 cv.imshow(winName, imgBLiner) 16 17 winName:str = f\"result nearest {count}\" 18 cv.namedWindow(winName) 19 cv.imshow(winName, imgNearest) 20 21 cv.waitKey(0) 22 cv.destroyWindow(winName) 23 24 class WarpCorrectionMgr: 25 def __init__(self, imgSrc): 26 self.__clickTime = 0 27 self.__imgSrc = imgSrc.copy() 28 self.__imgDrawn = imgSrc.copy() 29 self.__dots = [] 30 31 @property 32 def sourceImage(self): 33 return self.__imgSrc 34 35 @property 36 def drawnImage(self): 37 return self.__imgDrawn 38 39 @drawnImage.setter 40 def drawnImage(self, newImg): 41 self.__imgDrawn = newImg 42 43 @property 44 def clickTime(self): 45 return self.__clickTime 46 47 @clickTime.setter 48 def clickTime(self, v): 49 self.__clickTime = v 50 51 @property 52 def dots(self): 53 return self.__dots 54 55 @staticmethod 56 def MouseCallback(event, x, y, flags, param): 57 # 四个点的顺序一定要按照左上，右上，右下，左下的顺时针顺序点 58 if event == cv.EVENT_LBUTTONDBLCLK: 59 clickTime = param.clickTime 60 cv.circle(param.drawnImage, (x, y), 8, (0, 0, 255),-1) 61 param.dots.append((x, y)) 62 cv.imshow(mainWinName, param.drawnImage) 63 64 if clickTime + 1 == 4: 65 global windowCount 66 p = Process(target=WarpImage, args=(param.sourceImage, param.dots.copy(), windowCount)) 67 p.daemon = True 68 p.start() 69 70 param.drawnImage = param.sourceImage.copy() 71 cv.imshow(mainWinName,param.sourceImage) 72 param.dots.clear() 73 windowCount += 1 74 75 param.clickTime = (clickTime + 1) % 4 76 77 78 if __name__ == \"__main__\": 79 cv.namedWindow(mainWinName) 80 imgSrc = cv.imread(\"E:/Users/Administrator/pictures/Test/skew.jpg\") 81 imgSrc = cv.resize(imgSrc, (int(imgSrc.shape[1]/4), int(imgSrc.shape[0]/4))) 82 83 mgr = WarpCorrectionMgr(imgSrc) 84 cv.setMouseCallback(mainWinName, WarpCorrectionMgr.MouseCallback, mgr) 85 86 cv.imshow(mainWinName, imgSrc) 87 cv.waitKey(0) 88 cv.destroyAllWindows()为了对比最近邻插值和双二次插值的差别，我把WarpCorrection改了一下：# 请注意，因为传进去的是规范化后(Y, X, 1)的值，所以得到的其实是(y/Z, x/Z, 1/Z的值) vts = np.dot(vtr,np.linalg.inv(np.transpose(H))) dstHeight, dstWidth = maxY - minY + 1, maxX - minX + 1 imgBLiner = np.zeros((dstHeight, dstWidth, imgSrc.shape[2]), dtype=imgSrc.dtype) imgNearest = np.zeros((dstHeight, dstWidth, imgSrc.shape[2]), dtype=imgSrc.dtype) for (r, s) in zip(vtr, vts): ry, rx = int(r[0]), int(r[1]) iy, ix = s[:2] # 需要解 [y, x] = [iy*(g*y + h*x + 1), ix*(g*y + h*x + 1)]这个方程 TH = np.linalg.inv(np.array([[iy * g - 1, iy * h], [ix * g, ix * h - 1]])) vxy = np.dot(TH, np.array([[-iy], [-ix]])) sy, sx = vxy[0, 0], vxy[1, 0] if 0 <= round(sy) < height and 0 <= round(sx) < width: imgBLiner[ry - minY, rx - minX] = BilinearInterpolation(imgSrc, height, width, sx, sy) imgNearest[ry - minY, rx - minX] = imgSrc[int(round(sy)),int(round(sx))] return imgBLiner, imgNearest运行下程序来看下效果，上面程序的作用就是选中要校正图像的四个角，然后校正（PS：实际运行可能会相当慢，我没做优化）原图Fig3. 最近邻插值（边缘很粗糙，看“清华大学出版社”几个字）Fig4. 双二次插值（毛刺很少，但是图像模糊了）OpenCV库中WarpPerspective源代码其实OpenCV是带了仿射变换和透射变换的API的，先来看下投影变换的源码（在imgwarp.cpp）（部分关键代码，Assert和IPP优化这里不展示了）：1 void cv::warpPerspective( InputArray _src, OutputArray _dst, InputArray _M0, 2 Size dsize, int flags, int borderType, const Scalar& borderValue ) 3 { 4 Mat src = _src.getMat(), M0 = _M0.getMat(); 5 _dst.create( dsize.area() == 0 ? src.size() : dsize, src.type() ); 6 Mat dst = _dst.getMat(); 7 8 if( dst.data == src.data ) 9 src = src.clone(); 10 11 double M[9]; 12 Mat matM(3, 3, CV_64F, M); 13 int interpolation = flags & INTER_MAX; 14 15 // 插入方法不支持INTER_AREA，用INTER_LINEAR代替 16 if( interpolation == INTER_AREA ) 17 interpolation = INTER_LINEAR; 18 19 CV_Assert( (M0.type() == CV_32F || M0.type() == CV_64F) && M0.rows == 3 && M0.cols == 3 ); 20 M0.convertTo(matM, matM.type()); 21 22 //这里源码是这样操作的，如果可以执行IPP，那么就直接执行IPP指令加块操作 23 //如果没有说明传入的变换矩阵已经转置过了，那么执行一次矩阵转置 24 if( !(flags & WARP_INVERSE_MAP) ) 25 invert(matM, matM); 26 27 hal::warpPerspectve(src.type(), src.data, src.step, src.cols, src.rows, dst.data, dst.step, dst.cols, dst.rows, 28 matM.ptr<double>(), interpolation, borderType, borderValue.val); 29 }1 namespace hal { 2 3 void warpPerspectve(int src_type, 4 const uchar * src_data, size_t src_step, int src_width, int src_height, 5 uchar * dst_data, size_t dst_step, int dst_width, int dst_height, 6 const double M[9], int interpolation, int borderType, const double borderValue[4]) 7 { 8 Mat src(Size(src_width, src_height), src_type, const_cast<uchar*>(src_data), src_step); 9 Mat dst(Size(dst_width, dst_height), src_type, dst_data, dst_step); 10 11 Range range(0, dst.rows); 12 WarpPerspectiveInvoker invoker(src, dst, M, interpolation, borderType, Scalar(borderValue[0], borderValue[1], borderValue[2], borderValue[3])); 13 14 //这里是一个opencv执行并行处理的一个方法，真正执行处理的代码在WarpPerspectiveInvoker的函数对象操作符中 15 parallel_for_(range, invoker, dst.total()/(double)(1<<16)); 16 } 17 18 } // hal::1 class WarpPerspectiveInvoker : 2 public ParallelLoopBody 3 { 4 public: 5 WarpPerspectiveInvoker(const Mat &_src, Mat &_dst, const double *_M, int _interpolation, 6 int _borderType, const Scalar &_borderValue) : 7 ParallelLoopBody(), src(_src), dst(_dst), M(_M), interpolation(_interpolation), 8 borderType(_borderType), borderValue(_borderValue) 9 { 10 //注意M就是转置矩阵 11 } 12 13 virtual void operator() (const Range& range) const 14 { 15 const int BLOCK_SZ = 32; 16 short XY[BLOCK_SZ*BLOCK_SZ*2], A[BLOCK_SZ*BLOCK_SZ]; 17 int x, y, x1, y1, width = dst.cols, height = dst.rows; 18 19 int bh0 = std::min(BLOCK_SZ/2, height); 20 int bw0 = std::min(BLOCK_SZ*BLOCK_SZ/bh0, width); //操作块的宽 21 bh0 = std::min(BLOCK_SZ*BLOCK_SZ/bw0, height); //操作块的高 22 23 //下面的意思是，如果支持SSE4指令，那么就先对转置矩阵进行预处理，下面会继续用到这个pwarp_impl_sse4 24 #if CV_TRY_SSE4_1 25 Ptr<opt_SSE4_1::WarpPerspectiveLine_SSE4> pwarp_impl_sse4; 26 if(CV_CPU_HAS_SUPPORT_SSE4_1) 27 pwarp_impl_sse4 = opt_SSE4_1::WarpPerspectiveLine_SSE4::getImpl(M); 28 #endif 29 30 //opencv对投射变换的操作是基于块操作的，主要是为了方便可以直接用SSE4指令给操作提速 31 for( y = range.start; y < range.end; y += bh0 ) 32 { 33 for( x = 0; x < width; x += bw0 ) 34 { 35 int bw = std::min( bw0, width - x); 36 int bh = std::min( bh0, range.end - y); // height 37 38 //_XY是一个双通道的Mat，下面remap会用到 39 Mat _XY(bh, bw, CV_16SC2, XY), matA; 40 Mat dpart(dst, Rect(x, y, bw, bh)); 41 42 for( y1 = 0; y1 < bh; y1++ ) 43 { 44 short* xy = XY + y1*bw*2; 45 //就是[X,Y,W] = [x,y,1]*M^T的操作 46 //注意现在是分块操作，所以才有+ M[1]*(y + y1)这一项 47 double X0 = M[0]*x + M[1]*(y + y1) + M[2]; 48 double Y0 = M[3]*x + M[4]*(y + y1) + M[5]; 49 double W0 = M[6]*x + M[7]*(y + y1) + M[8]; 50 51 //最近邻插值 52 if( interpolation == INTER_NEAREST ) 53 { 54 x1 = 0; 55 56 //这里就是刚才上面所说的pwarp_impl_sse4所用到的位置 57 #if CV_TRY_SSE4_1 58 if (pwarp_impl_sse4) 59 pwarp_impl_sse4->processNN(M, xy, X0, Y0, W0, bw); 60 else 61 #endif 62 for( ; x1 < bw; x1++ ) 63 { 64 //计算W 65 double W = W0 + M[6]*x1; 66 W = W ? 1./W : 0; 67 //从齐次坐标变到增广矢量，注意现在是分块操作，所以才有+ M[0]*x1这一项 68 //把结果控制在(INT_MIN, INT_MAX)中，由于下面会有把double转为int 69 double fX = std::max((double)INT_MIN, std::min((double)INT_MAX, (X0 + M[0]*x1)*W)); 70 double fY = std::max((double)INT_MIN, std::min((double)INT_MAX, (Y0 + M[3]*x1)*W)); 71 int X = saturate_cast<int>(fX); 72 int Y = saturate_cast<int>(fY); 73 74 xy[x1*2] = saturate_cast<short>(X); 75 xy[x1*2+1] = saturate_cast<short>(Y); 76 } 77 } 78 //其他插值方法 79 else 80 { 81 short* alpha = A + y1*bw; 82 x1 = 0; 83 84 //原理上同 85 #if CV_TRY_SSE4_1 86 if (pwarp_impl_sse4) 87 pwarp_impl_sse4->process(M, xy, alpha, X0, Y0, W0, bw); 88 else 89 #endif 90 for( ;x1 < bw; x1++ ) 91 { 92 double W = W0 + M[6]*x1; 93 //INTER_TAB_SIZE在我的opencv版本就是32，INTER_BITS是5 94 //下面的代码是先把值扩大32倍，再左移5位得到一个值 95 W = W ? INTER_TAB_SIZE/W : 0; 96 double fX = std::max((double)INT_MIN, std::min((double)INT_MAX, (X0 + M[0]*x1)*W)); 97 double fY = std::max((double)INT_MIN, std::min((double)INT_MAX, (Y0 + M[3]*x1)*W)); 98 int X = saturate_cast<int>(fX); 99 int Y = saturate_cast<int>(fY); 100 101 xy[x1*2] = saturate_cast<short>(X >> INTER_BITS); 102 xy[x1*2+1] = saturate_cast<short>(Y >> INTER_BITS); 103 104 //alpha是remap里面所使用的remapBilinear所要用到的一个参数，这里就不往下看了 105 alpha[x1] = (short)((Y & (INTER_TAB_SIZE-1))*INTER_TAB_SIZE + 106 (X & (INTER_TAB_SIZE-1))); 107 } 108 } 109 } 110 111 if( interpolation == INTER_NEAREST ) 112 //直接运行重映射，直接把源点映射到目标位置的点 113 remap( src, dpart, _XY, Mat(), interpolation, borderType, borderValue ); 114 else 115 { 116 Mat _matA(bh, bw, CV_16U, A); 117 remap( src, dpart, _XY, _matA, interpolation, borderType, borderValue ); 118 } 119 } 120 } 121 } 122 123 private: 124 Mat src; 125 Mat dst; 126 const double* M; 127 int interpolation, borderType; 128 Scalar borderValue; 129 };OpenCV的仿射变换源码和投影变换的类似，只是仿射变换的矩阵M的元素个数是6个参考http://www.cnblogs.com/houkai/p/6660272.htmlhttps://en.wikipedia.org/wiki/Bilinear_interpolationhttp://www.cnblogs.com/tiandsp/archive/2012/12/16/2820916.html"}
{"content2":"本文写成时主要参考了[1,2], 后面加了一些自己收集的，不过大家都在更新，所以区别不是很大~综述[2015-PAMI-Overview]Text Detection and Recognition in Imagery: A Survey[paper][2014-Front.Comput.Sci-Overview]Scene Text Detection and Recognition: Recent Advances and Future Trends[paper]自然场景文字检测[2018-arxiv]TextBoxes++: ASingle-Shot Oriented Scene Text Detector[paper][2018-arxiv]FOTS: Fast OrientedText Spotting with a Unified Network[paper][2018-AAAI] PixelLink: DetectingScene Text via Instance Segmentation[paper][2017-arXiv]Fused Text Segmentation Networks for Multi-oriented Scene Text Detection[paper][2017-arXiv]WeText: Scene Text Detection under Weak Supervision[paper][2017-ICCV]Single Shot Text Detector with Regional Attention[pdf][2017-ICCV]WordSup: Exploiting Word Annotations for Character based Text Detection[paper][2017-arXiv]R2CNN: Rotational Region CNN for Orientation Robust Scene Text Detection[paper][2017-CVPR]EAST: An Efficient and Accurate Scene Text Detector [paper][code][2017-arXiv]Cascaded Segmentation-Detection Networks for Word-Level Text Spotting[paper][2017-arXiv]Deep Direct Regression for Multi-Oriented Scene Text Detection[paper][2017-CVPR]Detecting oriented text in natural images by linking segments [paper][code][2017-CVPR]Deep Matching Prior Network: Toward Tighter Multi-oriented Text Detection[paper][2017-arXiv]Arbitrary-Oriented Scene Text Detection via Rotation Proposals [paper][2017-AAAI]TextBoxes: A Fast Text Detector with a Single Deep Neural Network[paper][code][2016-arXiv]Accurate Text Localization in Natural Image with Cascaded Convolutional TextNetwork [paper][2016-arXiv]DeepText : A Unified Framework for Text Proposal Generation and Text Detectionin Natural Images [paper] [data][2017-PR]TextProposals: a Text-specific Selective Search Algorithm for Word Spotting in the Wild [paper] [code][2016-arXiv] SceneText Detection via Holistic, Multi-Channel Prediction [paper][2016-CVPR] CannyText Detector: Fast and Robust Scene Text Localization Algorithm [paper][2016-CVPR]Synthetic Data for Text Localisation in Natural Images [paper] [data][code][2016-ECCV]Detecting Text in Natural Image with Connectionist Text Proposal Network[paper][demo][code][2016-TIP]Text-Attentional Convolutional Neural Networks for Scene Text Detection [paper][2016-IJDAR]TextCatcher: a method to detect curved and challenging text in natural scenes[paper][2016-CVPR]Multi-oriented text detection with fully convolutional networks [paper][2015-TPRMI]Real-time Lexicon-free Scene Text Localization and Recognition[paper][2015-CVPR]Symmetry-Based Text Line Detection in Natural Scenes[paper][code][2015-ICCV]FASText: Efficient unconstrained scene text detector[paper][code][2015-D.PhilThesis] Deep Learning for Text Spotting [paper][2015 ICDAR]Object Proposals for Text Extraction in the Wild [paper] [code][2014-ECCV] Deep Features for Text Spotting [paper] [code] [model] [GitXiv][2014-TPAMI] Word Spotting and Recognition with Embedded Attributes [paper] [homepage] [code][2014-TPRMI]Robust Text Detection in Natural Scene Images[paper][2014-ECCV] Robust Scene Text Detection with Convolution Neural Network Induced MSER Trees [paper][2013-ICCV] Photo OCR: Reading Text in Uncontrolled Conditions[paper][2012-CVPR]Real-time scene text localization and recognition[paper][code][2010-CVPR]Detecting Text in Natural Scenes with Stroke Width Transform [paper] [code]自然场景文字识别[2017-arXiv]AdaDNNs: Adaptive Ensemble of Deep Neural Networks for Scene Text Recognition [paper][2017-arXiv]STN-OCR: A single Neural Network for Text Detection and Text Recognition[paper][code][2017-arXiv]Auto-Encoder Guided GAN for Chinese Calligraphy Synthesis[paper][2017-AAAI-网络图片]Detection and Recognition of Text Embedded in Online Images via Neural Context Models[paper][project][2017-arvix 文档识别] Full-Page Text Recognition : Learning Where to Start and When to Stop[paper][2016-AAAI]Reading Scene Text in Deep Convolutional Sequences [paper][2016-IJCV]Reading Text in the Wild with Convolutional Neural Networks [paper] [demo] [homepage][2016-CVPR]Recursive Recurrent Nets with Attention Modeling for OCR in the Wild [paper][2016-CVPR] Robust Scene Text Recognition with Automatic Rectification [paper][2016-NIPs] Generative Shape Models: Joint Text Recognition and Segmentation with Very Little Training Data[paper][2015-CoRR] AnEnd-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition [paper] [code][2015-ICDAR]Automatic Script Identification in the Wild[paper][2015-ICLR] Deep structured output learning for unconstrained text recognition [paper][2014-NIPS]Synthetic Data and Artificial Neural Networks for Natural Scene Text Recognition [paperhomepage] [model][2014-TIP] A Unified Framework for Multi-Oriented Text Detection and Recognition [paper][2012-ICPR]End-to-End Text Recognition with Convolutional Neural Networks [paper] [code] [SVHN Dataset]数据集Total-Text 20171555 images,11459 text instances, includes curved textCOCO-Text (ComputerVision Group, Cornell) 201663,686images, 173,589 text instances, 3 fine-grained text attributes.Task:text location and recognitionCOCO-Text APISynthetic Data for Text Localisation in Natural Image (VGG)2016800k thousand images8 million synthetic word instancesdownloadSynthetic Word Dataset (Oxford, VGG) 20149million images covering 90k English wordsTask:text recognition, segmentationdownloadIIIT 5K-Words 20125000images from Scene Texts and born-digital (2k training and 3k testing images)Eachimage is a cropped word image of scene text with case-insensitive labelsTask:text recognitiondownloadStanfordSynth(Stanford, AI Group) 2012Smallsingle-character images of 62 characters (0-9, a-z, A-Z)Task:text recognitiondownloadMSRA Text Detection 500 Database(MSRA-TD500) 2012500 natural images(resolutions of the images vary from 1296x864 to 1920x1280)Chinese,English or mixture of bothTask:text detectionStreet View Text (SVT) 2010350 high resolution images (average size 1260 × 860) (100 images for training and 250 images for testing)Onlyword level bounding boxes are provided with case-insensitive labelsTask:text locationKAIST Scene_Text Database 20103000images of indoor and outdoor scenes containing textKorean,English (Number), and Mixed (Korean + English + Number)Task:text location, segmentation and recognitionChars74k 2009Over74K images from natural images, as well as a set of synthetically generatedcharactersSmallsingle-character images of 62 characters (0-9, a-z, A-Z)Task:text recognitionICDARBenchmark DatasetsDatasetDiscriptionCompetition PaperICDAR 20151000 training images and 500 testing imagespaperICDAR 2013229 training images and 233 testing imagespaperICDAR 2011229 training images and 255 testing imagespaperICDAR 20051001 training images and 489 testing imagespaperICDAR 2003181 training images and 251 testing images(word level and character level)paper开源库Tesseract: c++ based tools for documents analysis and OCR,support 60+ languages [code]Ocropy: Python-based tools for document analysis and OCR [code]CLSTM : A small C++ implementation of LSTM networks,focused on OCR [code]Convolutional Recurrent Neural Network,Torch7 based [code]Attention-OCR: Visual Attention based OCR [code]Umaru: An OCR-system based on torch using the technique of LSTM/GRU-RNN, CTC and referred to the works of rnnlib and clstm [code]其他DeepFont:Identify Your Font from An Image[paper]Writer-independent Feature Learning for Offline Signature Verification using Deep Convolutional Neural Networks[paper]End-to-End Interpretation of the French Street Name Signs Dataset [paper] [code]Extracting text from an image using Ocropus [blog]手写字识别[2016-arXiv]Drawingand Recognizing Chinese Characters with Recurrent Neural Network [paper]Learning Spatial-Semantic Context with Fully Convolutional Recurrent Network for Online Handwritten Chinese Text Recognition [paper]Stroke Sequence-Dependent Deep Convolutional Neural Network for Online Handwritten Chinese Character Recognition [paper]High Performance Offline Handwritten Chinese Character Recognition Using GoogLeNet and Directional Feature Maps [paper] [github]DeepHCCR:Offline Handwritten Chinese Character Recognition based on GoogLeNet and AlexNet (With CaffeModel) [code]如何用卷积神经网络CNN识别手写数字集？[blog][blog1][blog2] [blog4] [blog5] [code6]Scan,Attend and Read: End-to-End Handwritten Paragraph Recognition with MDLSTMAttention [paper]MLPaint:the Real-Time Handwritten Digit Recognizer [blog][code][demo]caffe-ocr: OCR with caffe deep learning framework [code] (单字分类器)牌照等识别ReadingCar License Plates Using Deep Convolutional Neural Networks and LSTMs  [paper]Numberplate recognition with Tensorflow [blog] [code]end-to-end-for-plate-recognition[code]ApplyingOCR Technology for Receipt Recognition[blog][mirror]破解验证码[2017-Arvix]Using Synthetic Data to Train NeuralNetworks is Model-Based Reasoning[paper]Using deep learning to break a Captcha system [blog] [code]Breakingreddit captcha with 96% accuracy [blog] [code]I'mnot a human: Breaking the Google reCAPTCHA [paper]NeuralNet CAPTCHA Cracker [slides] [code] [demo]Recurrentneural networks for decoding CAPTCHAS [blog] [code] [demo]Readingirctc captchas with 95% accuracy using deep learning [code]端到端的OCR：基于CNN的实现 [blog]IAm Robot: (Deep) Learning to Break Semantic Image CAPTCHAs [paper]参考[1]http://handong1587.github.io/deep_learning/2015/10/09/ocr.html[2]https://github.com/chongyangtao/Awesome-Scene-Text-Recognition"}
{"content2":"转自：http://www.cnblogs.com/xrwang/archive/2010/03/27/BackgroundGenerationAndForegroundDetectionPhase2.html作者：王先荣本文尝试对《学习OpenCV》中推荐的论文《Nonparametric Background Generation》进行翻译。由于我的英文水平很差，断断续续搞了好几天才勉强完成，里面肯定会有诸多错误，欢迎大家指正，并请多多包涵。翻译本文的目的在于学习研究，如果需要用于商业目的，请与原文作者联系。非参数背景生成刘亚洲，姚鸿勋，高文，陈熙霖，赵德斌哈尔滨工业大学中国科学院计算所摘要本文介绍了一种新颖的背景生成方法，该方法基于非参数背景模型，可用于背景减除。我们介绍一种新的名为影响因素描述（effect components description ECD）的模型，用于描述背景的变动；在此基础上，我们可以用潜在分布的局部极值推导出最可靠背景状态（most reliable background mode MRBM）。该方法的基本计算过程采用Mean Shift这一经典的模式识别过程。Mean Shift通过迭代计算，能够在数据的密度分布中找到最近位置的点（译者注：即找到数据最密集的点）。这种方法有三个优点：（1）能从包含混乱运动对象的视频中提取出背景；（2）背景非常清晰；（3）对噪声和小幅度的（摄像机）振动具有鲁棒性。广泛的实验结果证明了上述优点。关键词：背景减除，背景生成，Mean Shift，影响因素描述，最可靠背景状态，视频监视1 引言在许多计算机视觉和视频分析应用中，运动对象的分割是一项基本任务。例如，视频监视，多媒体索引，人物检测和跟踪，有知觉的人机接口，“小精灵”视频编码。精确的对象分割能极大的提高对象跟踪，识别，分类和动态分析的性能。识别运动对象的通用方法有：光流，基于时间差异或背景减除的方法。其中，背景减除最常用。背景模型被计算出，并逐帧进化；然后通过比较当前帧和背景模型间的差异来检测运动对象。这种方法的关键之处在于建立并维持背景模型。尽管文献【1-4】提出了很多有前途的方法，但是运动对象检测的精度这一基本问题仍然难以解决。第一个问题是：背景模型必须尽可能精确的反映真实背景，这样系统才能精确的检测运动对象的外形。第二个问题是：背景模型必须对背景场景的改变足够灵敏，例如对象开始运动及停止运动。如果不能适当的解决上述问题，背景减除会检测出虚假对象，它们通常被称为“幽灵”。目前已经有了许多用于背景减除的背景建立和维持方法。按背景建模的步骤来分类，我们可以将其分为参数化的和非参数化的方法。参数化的背景建模方法通常假设：单个像素的潜在概率密度函数是高斯或者高斯混合函数，详情请参看文献【5-7】。Stauffer和Grimson在文献【8】中提出了一种自适应的背景减除方法，用于解决运动分割问题。在他们的工作成果中，他们为每个像素建立了高斯混合概率密度函数，然后用即时的近似值更新该模型。文献【9，10】提出了对高斯混合模型的一些改进方法。Toyama等人在文献【2】中提出了一种三层的Wallflower方案，该方案尝试解决背景维持中现存的许多问题，例如灯光打开关闭，前景孔穴等等。Haritaoglu等人在文献【1】中提出的W4方法，该方法为背景建模而对每个像素保留了三个值的方法，包括最大值（M），最小值（N）和最大帧间绝对差值（D）。Kim等人在文献【11】中，将背景值量化到编码本，编码本描述了长视频中背景模型的压缩形式。另一类经常用到的背景模型方法基于非参数化的技术，例如文献【3，12-16】。Elgammal等人在文献【3】中，通过核密度估计建立了一种非参数化的背景模型。对每个像素，为了估计潜在的概率密度函数而保留了观测强度值，而新强度值的概率能通过该函数计算得出。这种模型具有鲁棒性，能够适应混乱及不完全静止但包含小扰动场合下的背景，例如摆动的树枝和灌木。与参数化的背景模型方法相比，非参数化的背景模型方法具有以下优点：不需要指定潜在的模型，不需要明确的估计参数【14】。因此，它们能适应任意未知的数据分布。这个特性使非参数化的方法成为许多计算机视觉应用的有力工具。在许多计算机视觉应用中，许多问题牵涉到多元多种形式的密度，数据在特征空间中没有规则的形态，没有遵循标准的参数形式。但是，从时间和空间复杂度这一方面来看，非参数化的方法不如参数化的方法有效。参数化的方法产生简洁的密度描述（例如高斯或高斯混合），得出有效的估计状态。相对的，非参数化的方法在学习阶段几乎不需要计算，然而在评估阶段需要高密度的计算。因此，非参数化方法的主要缺陷是它们的计算量。不过一些革新的工作成果已经被提出，它们能加快非参数化方法的评估速度，例如文献【13】中的快速高斯变换（FGT），文献【17】中的新ball tree算法，核密度估计和K近邻（KNN）分类。本文专注于非参数化的方法，跟Elagammal在文献【3】中提出的方法有紧密的联系，但是有两点本质上的区别。从基本原理上看，我们用影响因素描述（ECD）来为背景的变化建模，最可靠背景模型（MRBM）对背景场景的估计具有鲁棒性。从计算过程来看，通过使用Mean Shift过程，我们避免了对每个新观测强度值计算概率的核密度估计过程，节约了处理时间。在我们的方法中，仅用帧差即可决定像素的属性。因此能提高背景减除的鲁棒性和效率。本文余下的部分按以下方式来组织：第二节中提出了影响因素描述，用于反映背景的变化；第三节详细解释了最可靠背景模型；第四节包含了实验结果；第五节讨论了有待扩充的部分。2 影响因素描述本节讨论影响因素描述（ECD），我们试图通过它来有效的模拟背景的变化。背景减除的关键因素在于怎样建立并维持好的背景模型。由于在不同的应用中，摄像机类型、捕获的环境和对象完全不同，背景模型需要足够的自适应能力来适应不同的情况。为了有效的为背景建模，我们从最简单的理想情况开始。在理想情况下，对于视频中的每个空间位置，沿时间轴的强度值为常量C；常量C表示固定摄像机摄录了固定的场景（没有运动对象和系统噪声）。我们将这种情况下的场景称为理想背景场景。但是在实际应用中，很少能遇到这种理想情况。因此，背景像素可以看成是理想背景场景和其他影响部分的组合体。我们将这种方法定义为背景的影响成分描述，包括以下方面：系统噪声 N-sys：它由图像传感器和其他硬件设备引起。如果环境不太严密，系统噪声不会从根本上影响常量C，仅仅引起适度的偏差。运动的对象 M-obj：它由实际运动的对象及其阴影引起。大多数时候，它对C有极大的干扰。运动的背景 M-bgd：它由运动的背景区域引起，例如户外场景中随风摆动的树枝，或者水中的波纹。光照 S-illum：它表示户外随太阳位置改变而渐变的光照，或者室内灯光的关闭和打开而改变的照明。摄像机位移 D-cam：它表示摄像机的小幅度位移而引起的像素强度变化。场景的观测值（记为V-obsv）由理想背景场景C和有效成分组成，如公式（1）所示。V-obsv = C + N-sys + M-obj + M-bgd + S-illum + D-cam    （1）在这里我们用符号+来表示影响因素的累积效果。实际上，上述影响因素能进一步分为表1所示的不同属性。首先需要被强调的属性是过程，我们可以按过程将影响因素分为长期影响和短期影响。我们沿时间轴将视频流分成长度相等的块，如所示。长期表示影响因素会持续数块或者一直存在，例如N-sys、S-illum和D-cam。而M-obj和M-bgd仅仅偶尔发生，不会长期持续，因此我们称之为短期影响。另一种分类的标准是偏差。我们把S-illum、D-cam、M-bgd看作时间不变的常驻偏差影响。在较长的过程中，这些影响可以看作是对理想背景值C持久的增加（减少），或者替代。以S-illum为例，如果处于室内场景，并且打开照明，在接下来的帧中S-illum可以看成是对C持久的增加。而N-sys和M-obj在不同时刻有随机的值，我们称之为随时间变化的随机偏差影响。上述分析归纳到了表1中。表1 影响因素的分类长期          短期常驻偏差    S-illum，D-cam    M-bgd随机偏差    N-sys                  M-obj在此必须阐明以下两点：（1）上述分类并不绝对，取决于我们选择的块长度；但是它不影响我们接下来的分析；（2）也许某人会指出对S-illum的分类不正确，例如行驶汽车的灯光不是长期影响；这种情况下的光照变化属于短期影响，跟M-obj类似，因此我们不把它单独列为独立的影响因素。由于S-illum和D-cam对理想背景C有长期持续的偏差，我们将它们合并到理想背景中，得到C' = C + S-illum + D-cam。对这种合并的直接解释是：如果光照发生变化或者摄像机变动位置，我们有理由假设理想背景已经改变。因此将公式（1）表示成：V-obsv = C' + N-sys + M-obj + M-bgd    （2）到目前为止，观测值V-obsv由新的理想背景值C' 和影响因素（N-sys、M-obj、M-bgd）组成。这些影响因素对C'有不同的影响，归纳成以下两点：N-sys在整个视频流中存在，并对C'有些许影响。因此，大部分观测值都不会偏离C'太远。M-obj和M-bgd仅仅偶尔发生，但对C'引起很大的偏差。因此，仅仅小部分观测值显著的不同于C'。得出以下结论：空间位置的像素值在大部分时间内保持稳定并伴随些许偏差（由于长期存在的随机偏差N-sys）；仅仅当运动对象通过该像素时引起显著的偏差（由于短期偏差M-obj和M-bgd）。因此一段时间内，少数显著偏差形成了极值。大部分时间都存在这种属性，不过有时也并非如此。在中显示了白色圆心处像素值随时间而变化的图表。（a）～（c）节选自一段长达360帧的视频，（d）描绘了像素强度的变化。从（d）我们可以看出：由系统噪声引起的小幅度偏差占据了大部分时间，仅当有运动对象（及其阴影）经过时引起了显著的偏差。这与影响因素描述是相符的。我们的任务是从观测值序列{V-obsv t}（t=1....T，T指时间长度）中找到理想背景C'的估计值C'^。通过上述分析，我们发现C'^位于多数观测值的中点。从另一方面来看，C'^处于潜在分布梯度为0和最密集的地方。这个任务可由Mean Shift过程来完成。我们将C'^称为最可靠背景状态。3 用于运动对象检测的最可靠背景状态基于第二节所讲的影响因素描述，我们推知：大部分观测值所处区域的中心是背景的理想估计。我们将这个估计用符号C'^表示，并称为最可靠背景状态（MRBM）。定位MRBM的基本计算方式是Mean Shift。一方面，通过使用MRBM，我们能够为混乱运动对象的视频生成非常清晰的背景图像。另一方面，Mean Shift过程能发现强度分布的一些局部极值，这种信息能从真实的运动对象中区别出运动的背景（例如户外随风摆动的树枝，或者水中的波纹）。3.1 用于MRBM的Mean ShiftMean Shift是定位密度极值的简明方式，密度极值处的梯度为0.该理论由Fukunaga在文献【18】中提出，而Mean Shift的平滑性和收敛性由Comaniciu和Meer在文献【19】中证实。近几年它已成为计算机视觉应用的有力工具，并报道了许多有前途的成果。例如基于Mean Shift的图像分割【19-21】和跟踪【22-26】。在我们的工作成果中，我们用Mean Shift来定位强度分布的极值（注意：可能有多个局部极值）。我们将最大密度状态定义为MRBM。算法的要点如所示，包括下列步骤：样本选择：我们为每个像素选择一组样本S = {xi},i=1,...,n，其中xis是像素沿时间轴的强度值，n是样本数目。我们直接对样本进行Mean Shift运算，以便定位密度的极值。典型点选择：为了减少计算量，我们从S中选择或者计算出一组典型点（典型点数目为m，m<<n），并将这组典型点记为P = {pi},i=1,...,m。P中的典型点可以是样本的抽样结果，也可以是原始样本点的局部平均值。在我们的实验中，我们选择局部平均值。Mean Shift过程：从P中的典型样本点开始运用Mean Shift过程，我们可以得到收敛点m。值得注意的是，Mean Shift计算仍然基于整个样本点集S。所以，梯度密度估计的精度并未因为使用典型点而降低。提取候选背景模型：由于一些收敛点非常接近甚至完全一样，这些收敛点m可以被聚集为q组（q≤m）。我们能够获取q带权重的聚集中心，C = {{ci,wi}},i=1,....,q，其中ci是每个聚集中心的强度值，wi是聚集中心的权重。每组的点数记为li, i=1,....,q，∑i=1qli=m。每组中心的权重定义为：wi = li / m, i=1,....,q。获取最可靠背景模型：C'^ = ci*，其中i* = argi max{wi}，C'^是第二节提到的最可靠背景模型。MRBM算法的要点对于每个m典型点，第三步中的Mean Shift实现过程依照以下步骤：（1）初始化Mean Shift过程的起点：y1=pi。（2）反复运用Mean Shift过程yt+1 = ....直至收敛。（这里我们选用跟文献【19】一样的Mean Shift过程，函数g(x)是核函数G(x)。）（3）保存收敛点yconv，用于后续分析。在对所有像素运用上述步骤之后，我们能用MRBM生成背景场景B。通过上述分析，我们发现背景生成过程的时间复杂度为O(N·m)，空间复杂度为O(N·n），其中N是视频的长度。3.2 运动对象检测与背景模型维持生成背景模型之后，我们可以将其用于检测场景中的运动区域。为了使我们的背景模型对运动背景具有鲁棒性（例如户外随风摆动的树枝，或者水中的波纹），我们将k个聚集中心选为可能的背景值。我们将这组集合定义为Cb = {{ci,wi} | wi ≥ θ},i=1,....,k，其中Cb⊆C，θ是预定义的阀值。对于每个新的观测强度值x0，我们仅仅计算x0与Cb中元素的最小差值d，其中d = min{(x0-ci) | {ci,wi}∈Cb}。如果差值d大于预定义的阀值t，我们认为新的观测强度值是前景，否则为背景。背景维持能让我们的背景模型适应长期的背景变化，例如新停泊的汽车或者逐渐改变的光照。当我们观察一个新像素值时，背景模型按下列步骤来更新：（1）对每个新像素值，我们视其为新典型样本点。因此典型样本点的数目变为：m = m + 1。（2）如果新像素值属于背景区域，假设其强度值与聚集中心{ci,wi}最近，我们将该中心的权重更新为：wi =  (li + 1) / m。（3）如果新像素值属于前景区域，我们从这点开始运用新的Mean Shift过程，这样可以获取到新的收敛中心{cnew,wnew}，其中wnew初始化为：wnew = 1 / m。聚集中心C被扩充成：C = C ∪ {{cnew,wnew}}。背景减除的时间复杂度是O(N)，背景维持的时间复杂度是O(R)，其中N是视频的帧数，R是运动对象的数目。4 实验我们专注于两类MRBM应用：背景生成和背景减除。我们在合成视频和标准PETS数据库上比较MRBM与其他常用的方法。源代码用C++实现，测试用电脑的配置如下：CPU为Pentium 1.6GHZ，内存512M。我们自己捕获或者合成的视频尺寸为320×240像素，PETS数据库的视频尺寸为384/360×288像素，帧速率均为25fps。在所有的实验中，我们选择YUV（4:4:4）色彩空间作为特征空间。算法实现的描述见第三节，我们采用了Epanechnikov核，K(t) = 3 / 4 *(1 - t2) 。理论上，更大的训练集能得到更稳定的背景模型，但是会牺牲适应性。我们的实验表明，当n=100时，能够使背景图像得到最佳的可视质量和适应性。典型点数m影响训练时间及背景模型的可靠性。在我们的实验中，我们为Mean Shift过程选择m=10个典型点，这时的训练时间与高斯混合模型接近。阀值θ和t影响检测的精度，对不同的数据集可能有不同的θ和t。在我们的实验中当θ=0.3，t=10时，能够得到最大的准确率和最小的错误率。如果没有特别说明，所有实验使用上述设置。4.1 背景生成在许多监控和跟踪应用中，期望生成没有运动对象的背景图像，它能为更进一步的分析提供参考信息。但是很多时候，并不容易获得没有运动对象的的视频。我们的算法能从包含混乱运动对象的视频中提取非常清晰的背景图像。显示了一些生成的背景。视频共有360帧，我们将前100帧用于生成背景。图中显示了第1，33，66，99帧图像。的底部显示了算法生成的背景。以(c)为例，这段视频摄自校园的上下课时间，每帧中都有10名步行的学生。观察(c)最下面的背景图像，我们发现背景非常清晰，所有运动对象都被成功的抹去了。运动对象的移动速度是关键因素，它能显著的影响背景模型，包括我们的背景模型。我们用一段300帧的视频来评估算法，该视频里有一位缓慢走动的女士。第1，30，60，90，120帧图像分别显示在（a）～（e）中。用不同数目的样本图像生成的背景显示于（f）～（j）。当保持100帧样本图像时，生成的背景中有一些噪点，但是背景的整体质量得以保证。噪点区域用白色椭圆标出了，如（f）所示。当我们将样本数目增加到300时，背景变得非常清晰，如（j）所示。我们也对我们的背景生成方法与其它基本方法做了比较，例如高斯模型具有多个聚集中心的高斯混合模型。为了区分比较结果，我们合成了一段多模态背景分布视频。背景的像素由高斯混合分布生成，pbg(x) = ∑i=12αiGμi,σi(x)，其中参数α1=α2=0.5，σ1=σ2=6，μ1=128，μ2=240。前景对象的像素由高斯分布生成，pfg(x) = Gμ,σ(x)，其中参数μ=10，σ=6。上述两式中，Gμ,σ(·)代表具有均值μ和标准偏差σ的高斯分布。背景像素及前景像素的强度分布见。视频共有120帧，我们用前100帧来生成背景。（a）～（e）显示了一些选定的帧，生成的背景图像显示在（f）～（i）中，从潜在分布生成的“地面实况”样本显示于（j）中。 对于高斯模型，背景像素的强度值被选为高斯均值，生成的背景图像如（f）所示。对于高斯混合模型，我们选择带maxim的高斯混合均值为背景值。（g）显示了2个中心的高斯混合模型，（h）显示了3个中心的高斯混合模型。实验所用的高斯混合模型使用OpenCV中的实现，见文献【27】。MRBM方法得到的结果如（i）所示。比较地面实况图像和生成的背景图像，我们发现非参数模型MRBM优于其它方法。凭直觉，在处理多模分布时，MRBM看起来与高斯混合模型类似。但是关键的不同之处在于高斯模型依赖均值和方差。它们的1阶和2阶统计数据对外部点（outliers 远离数据峰值的点）非常敏感。如果对象的运动速度慢，存在足够的前景值导致错误的均值，结果得出错误的背景值。作为对照，MRBM跟分布无关，仅仅使用极值作为可能的背景值，它对外部点更鲁棒。其他参数方法存在类似的问题，当预定义的模型不能描述数据分布时更加明显。4.2 背景减除显示了我们算法的背景减除结果。（a）显示观测到的当前帧，（b）显示用MRBM从100帧样本生成的背景图像，（c）显示了背景减除的结果图像，我们发现运动对象变得很突出。我们比较了MRBM和其它常用的基本方法，例如文献【1】中的最大最小值法，文献【28，29】中的中值法，文献【8，6】中的高斯混合模型。比较结果显示于。由于我们不能修改这些原始工作成果的实现方式，只能按以下方式来管理基础算法：（1）对于W4，我们按原始成果中的建议来设置参数；（2）对于中值法和高斯混合模型，我们调整参数使其达到最好的检测精度。另外，为了使比较尽量公平，我们只做背景减除，没有进行降噪和形态学处理。最佳的视频序列选自PETS数据库【30-32】，选定帧如（a）所示。对所有的视频序列，我们用100帧来生成背景，用第40帧做背景减除。这些视频序列包含两种主要的场景：缓慢运动的对象（如PETS00和PETS06），多模态背景（如PETS01中摆动的树）；这两种场景是背景减除中的不同情况。对于缓慢运动的对象，高斯模型的结果比较差，因为高斯均值对外部点敏感，如（d）所示。而MRBM依赖于背景分布的极值，外部点对其影响很小。同样，中值法和最大最小值法不能很好的应对多模态背景，PETS01中摆动的树被误认为前景。跟预期一致，MRBM优于其它三种方法。4.3 讨论可能的欠缺尽管MRBM适用于许多应用，仍然存在一些不能应对的场合，就是不能应对的例子。在这个实验中，视频共有300帧，我们用前120帧来生成背景。（a）～（g）分别显示了第1，20，40，60，80，100，120帧，背景图像显示在（h）中。前景人物的很大一部分被误认为背景。通常，前景和背景的定义从自身来看并不明确。它包含在场景的语义中，在不同的应用中可能不一致。在我们的应用中，我们将运动对象定义为前景，将静止（或者几乎静止）的东西定义为背景，这与大多数视频监控应用的定义一致。通过第二节的分析，我们试图用ECD模型来近似观测值。在的实验中，人物在大部分时间保持静止，然后突然运动。这种情况下，大部分观测强度值属于人物，而非背景。对于人物的肩膀部分尤其明显，肩膀部分有相似的颜色，以致于检测不到运动。因此前景人物的大部分被误认为背景。实际上，这个例子反映了背景模型的根本问题：稳定性与适应性。理论上，如果我们增加用于训练的背景帧数，我们能得到更清晰的背景图像。但是同时，会极大的牺牲背景模型的适应性。当背景改变（例如新停泊的汽车或者突然改变的光照），背景模型需要很长的时间才能适应新情况，将产生大量的错误。针对该问题，一种有效的解决方案是：将现有的基于像素的方法扩展为基于区域或者基于帧的方法。通过分割图像或者完善像素级的低级分类可以实现它。更进一步，可以同时使用低级对象分割和高级信息（例如跟踪或者事件描述）。因此，我们接下来的工作将专注于如何结合空间和高级信息。5 结论本文主要有两点贡献：（1）我们介绍的影响因素描述可用于对变化的背景进行建模；（2）基于ECD，我们开发了一种鲁棒的背景生成方法——最可靠背景模型。应用MRBM，能从包含混乱运动对象的视频序列中生成高质量的背景图像。一些例子显示了这种方法的有效性和鲁棒性。然而，仍然存在一些有待解决的问题。当前的工作中仅仅考虑了像素的时间信息。怎么结合空间信息来提高本方法的鲁棒性是后续工作的重点。一种直接的扩展是：将当前基于像素的方法修改成熔合了邻域信息基于区域的方法。另外，结合使用低级分割和高级跟踪信息，对我们的工作成果也将有极大的提高。6 致谢在此要感谢陈熙霖博士和山世光博士，他们跟作者进行了很有帮助的讨论。这项研究的经费由以下单位赞助：中国自然科学基金会、中国科学院百名人才培养计划、上海银晨智能识别科技有限公司。参考文献[1] I.Haritaoglu, D.Harwoodand, L.S.Davis, W4:real-time surveillance of people and their activities, IEEE Transactions on Pattern Analysis and Machine Intelligence 22 (8) (2000) 809–830.[2] K.Toyama, J.Krumm, B.Brumitt,B.Meyers., WallFlower: Principles and Practice of background maintenance, in: IEEE International Conferenceon Computer Vision, Corfu, Greece, 1999, pp. 255–261.[3] A.Elgammal, D.Harwood, L.Davis, Non-parametric model for background subtraction, in: European Conference on Computer Vision, Dublin, Ireland, 2000, pp. 751–767.[4] T.E.Boult, R.J.Micheals, X.Gao, M.Eckmann, Intothewoods: visual Surveillance of noncooperative and camouflaged targets in complex outdoorsettings, Proceedings of the IEEE 89 (2001) 1382–1402.[5] C.R.Wren, A.Azarbayejani, T.Darrell, A.P.Pentland, Pfinder: Real-time Tracking of the human body, IEEE Transactions on Pattern Analysis andMachine Intelligence 19 (7) (1998) 780–785.[6] C.Stauffer, W.Grimson, Adaptive background mixture models for real-time tracking, in: IEEE Conference on Computer Vision and Pattern Recognition, FortCollins, USA, 1999, pp. 246–252.[7] S.Rowe, A.Blake, Statistical background modelling for tracking with a virtual camera, in: British Machine Vision Conference, Birmingham, UK, 1995, pp. 423–432.[8] C.Stauffer, W.E.L.Grimson, Learning patterns of activity using real-time tracking, IEEE Transactions on Pattern Analysis and Machine Intelligence 22 (8) (2000) 747–757.[9] L.Li, W.Huang, I.Y.Gu, Q.Tian, Foreground object detection in changing Background based on color co-occurrence statistics, in: IEEE Workshop on Applications of Computer Vision, Orlando, Florida, 2002, pp. 269–274.[10] P.KaewTraKulPong, R.Bowden, An improved adaptive background mixture Model for real-time tracking with shadow detection, in: European Workshop on Advanced Video Based Surveillance Systems, Kluwer Academic, 2001.[11] K.Kim, T.Chalidabhongse, D.Harwood, L.Davis, Real-time foreground-Background segmentation using codebook model, Real Time Imaging 11 (3) (2005) 172–185.[12] A.Elgammal, R.Duraiswami, L.Davis, Effcient non-parametric adaptive color Modeling using fast gauss transform, in: IEEE Conference on Computer Vision And Pattern Recognition, Vol. 2, 2001, pp. 563–570.[13] A.M.Elgammal, R.Duraiswami, L.S.Davis, Effcient kernel density estimation Using the fast gauss transform with applications to color modeling and tracking., IEEE Transactions on Pattern Analysis and Machine Intelligence 25 (11) (2003) 1499–1504.[14] A.Elgammal, Effcient nonparametric kernel density estimation for realtime computer vision, Ph.D. thesis, Rutgers, The State University of New Jersey (2002).[15] H.Askar, X.Li, Z.Li, Background clutter suppression and dim moving point Targets detection using nonparametric method, in: International Conference on Communications, Circuits and Systems and West Sino Expositions, Vol. 2, 2002, pp. 982–986.[16] D.Thirde, G.Jones, Hierarchical probabilistic models for video object Segmentation and tracking, in: International Conference on PatternRecognition, Vol. 1, 2004, pp. 636–639.[17] T.Liu, A.W.Moore, A.Gray, Effcient exact k-nn and nonparametric Classification in high dimensions, in: Neural Information Processing Systems, 2003, pp. 265–272.[18] K.Fukunaga, L.Hostetler, The estimation of the gradient of adensity function, With applications in pattern recognition, IEEE Transactions on Information Theory 21 (1975) 32–40.[19] D.Comaniciu, P.Meer, Mean shift: a robust approach toward feature space analysis, IEEE Transactions on Pattern Analysis and Machine Intelligence 24 (5) (2002) 603–619.[20] I.Y.-H.Gu, V.Gui, Colour image segmentation using adaptive mean shift filters, in: International Conference on Image Processing, 2001, pp. 726–729.[21] L.Yang, P.Meer, D.J.Foran, Unsupervised segmentation based on robust Estimation and color active contour models, IEEE Transactions on Information Technology in Biomedicine 9 (3) (2005) 475–486.[22] D.Comaniciu, V.Ramesh, P.Meer, Kernel-based object tracking, IEEE Transactions on Pattern Analysis and Machine Intelligence 25 (5) (2003) 564– 577.[23] R.T.Collins, Y.Liu, On-line selection of discrimin ative tracking features, in: International Conference on Computer Vision, 2003, pp. 346–352.[24] R.Collins, Y.Liu, M.Leordeanu, On-line selection of discriminative tracking features, IEEE Transactions on Pattern Analysis and Machine Intelligence 27 (10) (2005) 1631–1643.[25] O.Debeir, P.V.Ham, R.Kiss, C.Decaestecker, Tracking of migrating cells under phase-contrast video microscopy with combined mean-shift processes, IEEE Transactions on Medical Imaging 24 (6) (2005) 697–711.[26] C.Shen, M.J.Brooks, A.van den Hengel, Fast global kernel density Mode seeking with application to localisation and tracking, in: InternationalConference on Computer Vision, 2005, pp. 1516–1523.[27] Intel open source computer vision library (2004).URL http://www.intel.com/technology/computing/opencv/[28] B.Lo, S.Velastin, Automatic congestion detection system for underground platforms, in: International Symposium on Intelligent Multimedia, Video and Speech Processing, Hong Kong, China, 2001, pp. 158–161.[29] R.Cucchiara, C.Grana, M.Piccardi, A.Prati, Detecting moving objects, ghosts, and shadows in video streams, IEEE Transactions on Pattern Analysis and Machine Intelligence 25 (10) (2003) 1337–1342.[30] IEEE international workshop on performance evaluation of tracking and surveillance (2000).URL ftp://ftp.pets.rdg.ac.uk/pub/PETS2000/[31] IEEE international workshop on performance evaluation of tracking and surveillance (2001).URL ftp://ftp.pets.rdg.ac.uk/pub/PETS2001/[32] IEEE international workshop on performance evaluation of tracking and surveillance (2006).URL http://pets2006.net/data.html写在最后的话本文所述的方法可说是像素级背景建模方式的巅峰之作。在接下来的时间里，我将尝试按照我自己的理解来实现文中的算法，对于论文中没有讲述透彻的部分，我也试图完善它。敬请期待～～在翻译文章的过程中得到了赵德斌博士的指导，在此表示感谢。同时，也感谢您耐心看完，希望对您有所帮助。欲知后事如何，且听下回分解。"}
{"content2":"转载】原文出处：blog.csdn.net/carson2005以下是原文作者辛苦整理的关于计算机视觉（ComputerVision, CV）相关领域的网站链接，其中有CV牛人的主页，CV研究小组的主页，CV领域的paper,代码，CV领域的最新动态，国内的应用情况等等。打算从事这个行业或者刚入门的朋友可以多关注这些网站，多了解一些CV的具体应用。搞研究的朋友也可以从中了解到很多牛人的研究动态、招生情况等。总之，我认为，知识只有分享才能产生更大的价值，真诚希望下面的链接能对朋友们有所帮助。（1）googleResearch； http://research.google.com/index.html（2）MIT博士，汤晓欧学生林达华； http://people.csail.mit.edu/dhlin/index.html（3）MIT博士后Douglas Lanman； http://web.media.mit.edu/~dlanman/（4）opencv中文网站； http://www.opencv.org.cn/index.php/%E9%A6%96%E9%A1%B5（5）Stanford大学vision实验室； http://vision.stanford.edu/research.html（6）Stanford大学博士崔靖宇； http://www.stanford.edu/~jycui/（7）UCLA教授朱松纯； http://www.stat.ucla.edu/~sczhu/（8）中国人工智能网； http://www.chinaai.org/（9）中国视觉网； http://www.china-vision.net/（10）中科院自动化所； http://www.ia.cas.cn/（11）中科院自动化所李子青研究员； http://www.cbsr.ia.ac.cn/users/szli/（12）中科院计算所山世光研究员； http://www.jdl.ac.cn/user/sgshan/（13）人脸识别主页； http://www.face-rec.org/（14）加州大学伯克利分校CV小组； http://www.eecs.berkeley.edu/Research/Projects/CS/vision/（15）南加州大学CV实验室； http://iris.usc.edu/USC-Computer-Vision.html（16）卡内基梅隆大学CV主页；http://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.html（17）微软CV研究员Richard Szeliski；http://research.microsoft.com/en-us/um/people/szeliski/（18）微软亚洲研究院计算机视觉研究组； http://research.microsoft.com/en-us/groups/vc/（19）微软剑桥研究院ML与CV研究组； http://research.microsoft.com/en-us/groups/mlp/default.aspx（20）研学论坛； http://bbs.matwav.com/（21）美国Rutgers大学助理教授刘青山； http://www.research.rutgers.edu/~qsliu/（22）计算机视觉最新资讯网； http://www.cvchina.info/（23）运动检测、阴影、跟踪的测试视频下载； http://apps.hi.baidu.com/share/detail/18903287（24）香港中文大学助理教授王晓刚； http://www.ee.cuhk.edu.hk/~xgwang/(25)香港中文大学多媒体实验室（汤晓鸥）; http://mmlab.ie.cuhk.edu.hk/(26)U.C. San Diego. computer vision;http://vision.ucsd.edu/content/home(27)CVonline; http://homepages.inf.ed.ac.uk/rbf/CVonline/(28)computer vision software; http://peipa.essex.ac.uk/info/software.html(29)Computer Vision Resource; http://www.cvpapers.com/(30)computer vision research groups;http://peipa.essex.ac.uk/info/groups.html(31)computer vision center; http://computervisioncentral.com/cvcnews(32)浙江大学图像技术研究与应用（ITRA）团队：http://www.dvzju.com/(33)自动识别网：http://www.autoid-china.com.cn/(34)清华大学章毓晋教授：http://www.tsinghua.edu.cn/publish/ee/4157/2010/20101217173552339241557/20101217173552339241557_.html(35)顶级民用机器人研究小组Porf.Gary领导的Willow Garage:http://www.willowgarage.com/(36)上海交通大学图像处理与模式识别研究所：http://www.pami.sjtu.edu.cn/(37)上海交通大学计算机视觉实验室刘允才教授：http://www.visionlab.sjtu.edu.cn/(38)德克萨斯州大学奥斯汀分校助理教授Kristen Grauman ：http://www.cs.utexas.edu/~grauman/(39)清华大学电子工程系智能图文信息处理实验室（丁晓青教授）：http://ocrserv.ee.tsinghua.edu.cn/auto/index.asp(40)北京大学高文教授：http://www.jdl.ac.cn/htm-gaowen/(41)清华大学艾海舟教授：http://media.cs.tsinghua.edu.cn/cn/aihz(42)中科院生物识别与安全技术研究中心：http://www.cbsr.ia.ac.cn/china/index%20CH.asp(43)瑞士巴塞尔大学 Thomas Vetter教授：http://informatik.unibas.ch/personen/vetter_t.html(44)俄勒冈州立大学 Rob Hess博士：http://blogs.oregonstate.edu/hess/(45)深圳大学 于仕祺副教授：http://yushiqi.cn/(46)西安交通大学人工智能与机器人研究所：http://www.aiar.xjtu.edu.cn/(47)卡内基梅隆大学研究员Robert T. Collins:http://www.cs.cmu.edu/~rcollins/home.html#Background(48)MIT博士Chris Stauffer:http://people.csail.mit.edu/stauffer/Home/index.php(49)美国密歇根州立大学生物识别研究组(Anil K. Jain教授)：http://www.cse.msu.edu/rgroups/biometrics/(50)美国伊利诺伊州立大学Thomas S. Huang:http://www.beckman.illinois.edu/directory/t-huang1(51)武汉大学数字摄影测量与计算机视觉研究中心：http://www.whudpcv.cn/index.asp(52)瑞士巴塞尔大学Sami Romdhani助理研究员：http://informatik.unibas.ch/personen/romdhani_sami/(53)CMU大学研究员Yang Wang:http://www.cs.cmu.edu/~wangy/home.html(54)英国曼彻斯特大学Tim Cootes教授：http://personalpages.manchester.ac.uk/staff/timothy.f.cootes/(55)美国罗彻斯特大学教授Jiebo Luo:http://www.cs.rochester.edu/u/jluo/(56)美国普渡大学机器人视觉实验室：https://engineering.purdue.edu/RVL/Welcome.html(57)美国宾利州立大学感知、运动与认识实验室：http://vision.cse.psu.edu/home/home.shtml(58)美国宾夕法尼亚大学GRASP实验室：https://www.grasp.upenn.edu/(59)美国内达华大学里诺校区CV实验室：http://www.cse.unr.edu/CVL/index.php(60)美国密西根大学vision实验室：http://www.eecs.umich.edu/vision/index.html(61)University of Massachusetts(麻省大学),视觉实验室：http://vis-www.cs.umass.edu/index.html(62)华盛顿大学博士后Iva Kemelmacher:http://www.cs.washington.edu/homes/kemelmi(63)以色列魏茨曼科技大学Ronen Basri:http://www.wisdom.weizmann.ac.il/~ronen/index.html(64)瑞士ETH-Zurich大学CV实验室：http://www.vision.ee.ethz.ch/boostingTrackers/index.htm(65)微软CV研究员张正友：http://research.microsoft.com/en-us/um/people/zhang/(66)中科院自动化所医学影像研究室：http://www.3dmed.net/(67)中科院田捷研究员：http://www.3dmed.net/tian/(68)微软Redmond研究院研究员Simon Baker:http://research.microsoft.com/en-us/people/sbaker/(69)普林斯顿大学教授李凯：http://www.cs.princeton.edu/~li/(70)普林斯顿大学博士贾登：http://www.cs.princeton.edu/~jiadeng/(71)牛津大学教授Andrew Zisserman： http://www.robots.ox.ac.uk/~az/(72)英国leeds大学研究员Mark Everingham:http://www.comp.leeds.ac.uk/me/(73)英国爱丁堡大学教授Chris William: http://homepages.inf.ed.ac.uk/ckiw/(74)微软剑桥研究院研究员John Winn: http://johnwinn.org/(75)佐治亚理工学院教授Monson H.Hayes：http://savannah.gatech.edu/people/mhayes/index.html(76)微软亚洲研究院研究员孙剑：http://research.microsoft.com/en-us/people/jiansun/(77)微软亚洲研究院研究员马毅：http://research.microsoft.com/en-us/people/mayi/(78)英国哥伦比亚大学教授David Lowe: http://www.cs.ubc.ca/~lowe/(79)英国爱丁堡大学教授Bob Fisher: http://homepages.inf.ed.ac.uk/rbf/(80)加州大学圣地亚哥分校教授Serge J.Belongie:http://cseweb.ucsd.edu/~sjb/(81)威斯康星大学教授Charles R.Dyer: http://pages.cs.wisc.edu/~dyer/(82)多伦多大学教授Allan.Jepson: http://www.cs.toronto.edu/~jepson/(83)伦斯勒理工学院教授Qiang Ji: http://www.ecse.rpi.edu/~qji/(84)CMU研究员Daniel Huber: http://www.ri.cmu.edu/person.html?person_id=123(85)多伦多大学教授：David J.Fleet: http://www.cs.toronto.edu/~fleet/(86)伦敦大学玛丽女王学院教授Andrea Cavallaro:http://www.eecs.qmul.ac.uk/~andrea/(87)多伦多大学教授Kyros Kutulakos: http://www.cs.toronto.edu/~kyros/(88)杜克大学教授Carlo Tomasi: http://www.cs.duke.edu/~tomasi/(89)CMU教授Martial Hebert: http://www.cs.cmu.edu/~hebert/(90)MIT助理教授Antonio Torralba: http://web.mit.edu/torralba/www/(91)马里兰大学研究员Yasel Yacoob: http://www.umiacs.umd.edu/users/yaser/(92)康奈尔大学教授Ramin Zabih: http://www.cs.cornell.edu/~rdz/(93)CMU博士田渊栋: http://www.cs.cmu.edu/~yuandong/(94)CMU副教授Srinivasa Narasimhan: http://www.cs.cmu.edu/~srinivas/(95)CMU大学ILIM实验室：http://www.cs.cmu.edu/~ILIM/(96)哥伦比亚大学教授Sheer K.Nayar: http://www.cs.columbia.edu/~nayar/(97)三菱电子研究院研究员Fatih Porikli ：http://www.porikli.com/(98)康奈尔大学教授Daniel Huttenlocher：http://www.cs.cornell.edu/~dph/(99)南京大学教授周志华：http://cs.nju.edu.cn/zhouzh/index.htm(100)芝加哥丰田技术研究所助理教授Devi Parikh: http://ttic.uchicago.edu/~dparikh/index.html(101)瑞士联邦理工学院博士后Helmut Grabner: http://www.vision.ee.ethz.ch/~hegrabne/#Short_CV(102)香港中文大学教授贾佳亚：http://www.cse.cuhk.edu.hk/~leojia/index.html(103)南洋理工大学副教授吴建鑫：http://c2inet.sce.ntu.edu.sg/Jianxin/index.html(104)GE研究院研究员李关：http://www.cs.unc.edu/~lguan/(105)佐治亚理工学院教授Monson Hayes:http://savannah.gatech.edu/people/mhayes/(106)图片检索国际会议VOC(微软剑桥研究院组织): http://pascallin.ecs.soton.ac.uk/challenges/VOC/(107)机器视觉开源处理库汇总：http://archive.cnblogs.com/a/2217609/(108)布朗大学教授Benjamin Kimia: http://www.lems.brown.edu/kimia.html(109)数据堂-图像处理相关的样本数据：http://www.datatang.com/data/list/602026/p1(110)东软基于CV的汽车辅助驾驶系统：http://www.neusoft.com/cn/solutions/1047/(111)马里兰大学教授Rema Chellappa:http://www.cfar.umd.edu/~rama/(112)芝加哥丰田研究中心助理教授Devi Parikh：http://ttic.uchicago.edu/~dparikh/index.html(113)宾夕法尼亚大学助理教授石建波：http://www.cis.upenn.edu/~jshi/(114)比利时鲁汶大学教授Luc Van Gool：http://www.vision.ee.ethz.ch/members/get_member.cgi?id=1, http://www.vision.ee.ethz.ch/~vangool/(115)行人检测主页：http://www.pedestrian-detection.com/(116)法国学习算法与系统实验室Basilio Noris博士：http://lasa.epfl.ch/people/member.php?SCIPER=129576 http://mldemos.epfl.ch/(117)美国马里兰大学LARRY S.DAVIS教授：http://www.umiacs.umd.edu/~lsd/(118)计算机视觉论文分类导航：http://www.visionbib.com/bibliography/contents.html(119)计算机视觉分类信息导航：http://www.visionbib.com/(120)西班牙马德里理工大学博士Marcos Nieto：http://marcosnieto.net/(121)香港理工大学副教授张磊：http://www4.comp.polyu.edu.hk/~cslzhang/(122)以色列技术学院教授Michael Elad：http://www.cs.technion.ac.il/~elad/(123)韩国启明大学计算机视觉与模式识别实验室：http://cvpr.kmu.ac.kr/(124)英国诺丁汉大学Michel Valstar博士：http://www.cs.nott.ac.uk/~mfv/(125)卡内基梅隆大学Takeo Kanade教授:http://www.ri.cmu.edu/people/kanade_takeo.html(126)微软学术搜索：http://libra.msra.cn/(127)比利时天主教鲁汶大学Radu Timofte博士：http://homes.esat.kuleuven.be/~rtimofte/，交通标志检测，定位，3D跟踪(128)迪斯尼匹兹堡研究院研究员：Iain Matthews:http://www.iainm.com/iainm/Home.htmlhttp://www.ri.cmu.edu/person.html?type=publications&person_id=741 AAM,三维重建（129）康奈尔大学视觉与图像分析组：http://www.via.cornell.edu/ 医学图像处理（130）密西根州立大学生物识别研究组：http://www.cse.msu.edu/biometrics/ 人脸识别、指纹识别、图像检索（131）柏林科技大学计算机视觉与遥感实验室：http://www.cv.tu-berlin.de/menue/computer_vision_remote_sensing/parameter/en/ 图像分析、物体重建、基于图像的表面测量、医学图像处理（132）英国布里斯托大学数字多媒体研究组：http://www.cs.bris.ac.uk/Research/Digitalmedia/ 运动检测与跟踪、视频压缩、3D重建、字符定位（133）英国萨利大学视觉、语音与信号处理中心： http://www.surrey.ac.uk/cvssp/   人脸识别、监控、3D、视频检索、（134）北卡莱罗纳大学教堂山分校Marc Pollefeys教授：http://www.cs.unc.edu/~marc/ 基于视频的3D模型生成、相机标定、运动检测与分析、3D重建（135）澳大利亚国立大学Richard Hartley教授：http://users.cecs.anu.edu.au/~hartley/ 运动估计、稀疏子空间、跟踪、（136）百度技术副总监于凯：http://www.dbs.ifi.lmu.de/~yu_k/ 深度学习，稀疏表示，图像分类（137）西安电子科技大学高新波教授：http://web.xidian.edu.cn/xbgao/index.html 质量评判、水印、稀疏表示、超分辨率（138）加州大学伯克利分校Michael I.Jordan教授：http://www.cs.berkeley.edu/~jordan/机器学习（139）加州理工行人检测相关资料：http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/"}
{"content2":"4月27日，在天府之国，与你共享大数据与Alluxio的技术魅力。活动介绍本期技术沙龙将会聚焦在大数据、存储、数据库以及Alluxio应用实践等领域，邀请腾讯技术专家和业界技术专家现场分享关于Alluxio系统的基本原理、大数据系统架构、数据库应用运维、AI计算机视觉技术及落地实践等主题，带来丰富的实战内容和经验交流。活动流程13:00 活动签到14:00 开源大数据存储系统Alluxio的新特性介绍与缓存性能优化分布式文件系统处于大数据系统中基础地位，在行业大数据应用中发挥着重要作用。Alluxio（原名Tachyon）是世界上首个以内存为中心的层次化分布式文件系统。它为上层计算框架和底层存储系统构建了桥梁，应用可以通过Alluxio提供的统一数据访问方式访问底层任意存储系统中的数据。将会介绍Alluxio系统的基本原理，Alluxio 2.0的新特性；以及在Alluxio缓存优化方面的一些工作，包括通用的分层式大数据缓存调度框架，缓存替换策略及其自适应调度算法，以及内存读性能优化等。14:40 海量数据背后的大数据管控系统架构和最佳实践腾讯云弹性MapReduce （EMR）是结合云技术和 Hadoop、Hive、Spark、Storm 、Alluxio等社区开源技术，为客户提供安全、低成本、高可靠、可弹性伸缩的云端托管 Hadoop 服务。腾讯云EMR团队充分利用云的弹性能力以及服务化的云存储能力快速帮助用户生产出大数据分析平台，通过计算资源弹性降低企业的设备成本，通过服务化和自服务降低企业的运维成本，通过服务化的大数据平台降低企业的技术成本。多项技术创新帮助客户降本增效，目前，腾讯云大数据EMR已经成为电商、游戏、文创、企业服务、教育、金融等行业客户的大数据首选产品。 在本次沙龙上将首次揭秘腾讯云EMR管控系统，讲解系统架构和应用实践，为大家揭秘海量数据背后，如何构建高可靠、低成本、安全、弹性伸缩的EMR服务体系。15:20 智能运维：腾讯云数据库Cloud DBA随着数据库实例数的不断增加，智能运维势在必行。Cloud DBA是腾讯云数据库智能运维平台，包含了自助分析，自动调优，自动治愈等模块。16:00 AI计算机视觉技术及落地实战计算机视觉是一门研究如何使机器“看”的科学，指用摄影机和电脑代替人眼对目标进行识别、跟踪和测量等。作为计算机视觉头部企业，腾讯云提供全套AI计算机视觉解决方案，面向金融、政务、公检法、工业制造、电商、游戏、文娱等多个行业客户提供服务，晏栋将为各位分享腾讯云AI计算机视觉背后的核心技术和方案落地实战。16:40 互动交流17:00 合影留念嘉宾介绍顾荣Alluxio项目PMC成员顾荣博士毕业于南京大学计算机系，南大PASA大数据实验室助理研究员，研究方向大数据处理系统，现担任江苏省计算机学会大数据专家委员会秘书长、中国计算机学会系统软件专委会委员，国际开源大数据存储系统Alluxio的项目管理委员会委员和源码技术专家（PMC Member & Maintainer）。彭健腾讯高级工程师腾讯高级工程师。有丰富的NoSQL、大规模分布式存储、大数据系统开发与实践经验。目前在腾讯云大数据团队负责管控系统的开发。鲁越腾讯云数据库架构师腾讯云数据库架构师，主要负责腾讯云数据库MySQL、Redis、MongoDB、Oracle等数据库架构设计、数据库运维、运营开发等工作，曾就职于网易和尼毕鲁。晏栋腾讯云人工智能资深产品架构师腾讯云人工智能资深产品架构师，厦门大学计算机科学系硕士，主攻机器视觉方向。主要负责负责人脸识别、图片理解等人工智能视觉产品的规划、行业拓展和产品解决方案工作。活动地点“天府之国” 成都腾讯众创空间(成都)（四川省成都市武侯区高新区天府三街腾讯大厦B座8楼）报名渠道点击此处报名"}
{"content2":"1. 线性分类器在深度学习与计算机视觉系列(2)我们提到了图像识别的问题，同时提出了一种简单的解决方法——KNN。然后我们也看到了KNN在解决这个问题的时候，虽然实现起来非常简单，但是有很大的弊端：分类器必须记住全部的训练数据(因为要遍历找近邻啊！！)，而在任何实际的图像训练集上，数据量很可能非常大，那么一次性载入内存，不管是速度还是对硬件的要求，都是一个极大的挑战。分类的时候要遍历所有的训练图片，这是一个相当相当相当耗时的过程。这个部分我们介绍一类新的分类器方法，而对其的改进和启发也能帮助我们自然而然地过渡到深度学习中的卷积神经网。有两个重要的概念：得分函数/score function：将原始数据映射到每个类的打分的函数损失函数/loss function：用于量化模型预测结果和实际结果之间吻合度的函数在我们得到损失函数之后，我们就将问题转化成为一个最优化的问题，目标是得到让我们的损失函数取值最小的一组参数。2. 得分函数/score function首先我们定义一个有原始的图片像素值映射到最后类目得分的函数，也就是这里提到的得分函数。先笼统解释一下，一会儿我们给个具体的实例来说明。假设我们的训练数据为，对应的标签yi，这里i=1…N表示N个样本，yi∈1…K表示K类图片。比如CIFAR-10数据集中N=50000，而D=32x32x3=3072像素，K=10，因为这时候我们有10个不同的类别(狗/猫/车…)，我们实际上要定义一个将原始像素映射到得分上函数2.1 线性分类器我们先丢出一个简单的线性映射：在这个公式里，我们假定图片的像素都平展为[D x 1]的向量。然后我们有两个参数：W是[K x D]的矩阵，而向量b为[K x 1]的。在CIFAR-10中，每张图片平展开得到一个[3072 x 1]的向量，那W就应该是一个[10 x 3072]的矩阵，b为[10 x 1]的向量。这样，以我们的线性代数知识，我们知道这个函数，接受3072个数作为输入，同时输出10个数作为类目得分。我们把W叫做权重，b叫做偏移向量。说明几个点：我们知道一次矩阵运算，我们就可以借助W把原始数据映射为10个类别的得分。其实我们的输入(xi,yi)其实是固定的，我们现在要做的事情是，我们要调整W, b使得我们的得分结果和实际的类目结果最为吻合。我们可以想象到，这样一种分类解决方案的优势是，一旦我们找到合适的参数，那么我们最后的模型可以简化到只有保留W, b即可，而所有原始的训练数据我们都可以不管了。识别阶段，我们需要做的事情仅仅是一次矩阵乘法和一次加法，这个计算量相对之前…不要小太多好么…提前剧透一下，其实卷积神经网做的事情也是类似的，将原始输入的像素映射成类目得分，只不过它的中间映射更加复杂，参数更多而已…2.2 理解线性分类器我们想想，其实线性分类器在做的事情，是对每个像素点的三个颜色通道，做计算。咱们拟人化一下，帮助我们理解，可以认为设定的参数/权重不同会影响分类器的『性格』，从而使得分类器对特定位置的颜色会有自己的喜好。举个例子，假如说我们的分类器要识别『船只』，那么它可能会喜欢图片的四周都是蓝色(通常船只是在水里海里吧…)。我们用一个实际的例子来表示这个得分映射的过程，大概就是下图这个样子：原始像素点向量xi经过W和b映射为对应结果类别的得分。不过上面这组参数其实给的是不太恰当的，因为我们看到在这组参数下，图片属于狗狗的得分最高 -_-||2.2.1 划分的第1种理解图片被平展开之后，向量维度很高，高维空间比较难想象。我们简化一下，假如把图片像素输入，看做可以压缩到二维空间之中的点，那我们想想，分类器实际上在做的事情就如下图所示：W中的每一列对应类别中的每一类，而当我们改变W中的值的时候，图上的线的方向会跟着改变，那么b呢？对，b是一个偏移量，它表示当我们的直线方向确定以后，我们可以适当平移直线到合适的位置。没有b会怎么样呢，如果直线没有偏移量，那意味着所有的直线都要通过原点，这种强限制条件下显然不能保证很好的平面类别分割。2.2.2 划分的第2种理解对W第二种理解方式是，W的每一行可以看做是其中一个类别的模板。而我们输入图片相对这个类别的得分，实际上是像素点和模板匹配度(通过内积运算获得)，而类目识别实际上就是在匹配图像和所有类别的模板，找到匹配度最高的那个。是不是感觉和KNN有点类似的意思？是有那么点相近，但是这里我们不再比对所有图片，而是比对类别的模板，这样比对次数只和类目数K有关系，所以自然计算量要小很多,同时比对的时候用的不再是l1或者l2距离，而是内积计算。我们提前透露一下CIFAR-10上学习到的模板的样子：你看，和我们设想的很接近，ship类别的周边有大量的蓝色，而car的旁边是土地的颜色。2.2.3 关于偏移量的处理我们先回到如下的公式：公式中有W和b两个参数，我们知道调节两个参数总归比调节一个参数要麻烦，所以我们用一点小技巧，来把他们组合在一起，放到一个参数中。我们现在要做的运算是矩阵乘法再加偏移量，最常用的合并方法就是，想办法把b合并成W的一部分。我们仔细看看下面这张图片：我们给输入的像素矩阵加上一个1，从而把b拼接到W里变成一个变量。依旧拿CIFAR-10举例，原本是[3072 x 1]的像素向量，我们添上最后那个1变成[3073 x 1]的向量，而[W]变成[W b]。2.2.4 关于数据的预处理插播一段，实际应用中，我们很多时候并不是把原始的像素矩阵作为输入，而是会预先做一些处理，比如说，有一个很重要的处理叫做『去均值』，他做的事情是对于训练集，我们求得所有图片像素矩阵的均值，作为中心，然后输入的图片先减掉这个均值，再做后续的操作。有时候我们甚至要对图片的幅度归一化/scaling。去均值是一个非常重要的步骤，原因我们在后续的梯度下降里会提到。2.3 损失函数我们已经通过参数W，完成了由像素映射到类目得分的过程。同时，我们知道我们的训练数据(xi,yi)是给定的，我们可以调整的是参数/权重W，使得这个映射的结果和实际类别是吻合的。我们回到最上面的图片中预测 [猫/狗/船] 得分的例子里，这个图片中给定的W显然不是一个合理的值，预测的结果和实际情况有很大的偏差。于是我们现在要想办法，去把这个偏差表示出来，拟人一点说，就是我们希望我们的模型在训练的过程中，能够对输出的结果计算并知道自己做的好坏。而能帮助我们完成这件事情的工具叫做『损失函数/loss function』，其实它还有很多其他的名字，比如说，你说不定在其他的地方听人把它叫做『代价函数/cost function』或者『客观度/objective』，直观一点说，就是我们输出的结果和实际情况偏差很大的时候，损失/代价就会很大。2.3.1 多类别支持向量机损失/Multiclass Support Vector Machine loss腻害的大神们定义出了好些损失函数，我们这里首先要介绍一种极其常用的，叫做多类别支持向量机损失(Multiclass SVM loss)。如果要用一句精简的话来描述它，就是它(SVM)希望正确的类别结果获得的得分比不正确的类别，至少要高上一个固定的大小Δ。我们先解释一下这句话，一会儿再举个例子说明一下。对于训练集中的第i张图片数据xi，我们的得分函数，在参数W下会计算出一个所有类得分结果其中第j类得分我们记作，该图片的实际类别为yi，则对于第i张样本图片，我们的损失函数是如下定义的：看公式容易看瞎，译者也经常深深地为自己智商感到捉急，我们举个例子来解释一下这个公式。假如我们现在有三个类别，而得分函数计算某张图片的得分为，而实际的结果是第一类(yi=0)。假设Δ=10(这个参数一会儿会介绍)。上面的公式把错误类别(j≠yi)都遍历了一遍，求值加和：仔细看看上述的两项，左边项-10和0中的最大值为0，因此取值是零。其实这里的含义是，实际的类别得分13要比第二类得分-7高出20，超过了我们设定的正确类目和错误类目之间的最小margin Δ=10，因此第二类的结果我们认为是满意的，并不带来loss，所以值为0。而第三类得分11，仅比13小2，没有大于Δ=10，因此我们认为他是有损失/loss的，而损失就是当前距离2距离设定的最小距离Δ的差距8。注意到我们的得分函数是输入像素值的一个线性函数，因此公式又可以简化为(其中wj是W的第j行)：我们还需要提一下的是，关于损失函数中max(0,-)的这种形式，我们也把它叫做hinge loss/铰链型损失，有时候你会看到squared hinge loss SVM(也叫L2-SVM)，它用到的是，这个损失函数惩罚那些在设定Δ距离之内的错误类别的惩罚度更高。两种损失函数标准在特定的场景下效果各有优劣，要判定用哪个，还是得借助于交叉验证/cross-validation。对于损失函数的理解，可以参照下图：.3.2 正则化如果大家仔细想想，会发现，使用上述的loss function，会有一个bug。如果参数W能够正确地识别训练集中所有的图片(损失函数为0)。那么我们对M做一些变换，可以得到无数组也能满足loss function=0的参数W’(举个例子，对于λ>1的所有λW，原来的错误类别和正确类别之间的距离已经大于Δ，现在乘以λ，更大了，显然也能满足loss为0)。于是…我们得想办法把W参数的这种不确定性去除掉啊…这就是我们要提到的正则化，我们需要在原来的损失函数上再加上一项正则化项(regularization penalty R(W))，最常见的正则化项是L2范数，它会对幅度很大的特征权重给很高的惩罚：根据公式可以看到，这个表达式R(W)把所有W的元素的平方项求和了。而且它和数据本身无关，只和特征权重有关系。我们把两部分组(数据损失/data loss和正则化损失/regularization loss)在一起，得到完整的多类别SVM损失权重，如下：也可以展开，得到更具体的完整形式：其中N是训练样本数，我们给正则化项一个参数λ，但是这个参数的设定只有通过实验确定，对…还是得交叉验证/cross-validation。关于设定这样一个正则化惩罚项为什么能解决W的不确定性，我们在之后的系列里会提到，这里我们举个例子简单看看，这个项是怎么起到作用的。假定我们的输入图片像素矩阵是x=[1,1,1,1]，而现在我们有两组不同的W权重参数中对应的向量w1=[1,0,0,0]，w2=[0.25,0.25,0.25,0.25]。那我们很容易知道wT1x=wT2x=1，所以不加正则项的时候，这俩得到的结果是完全一样的，也就意味着——它们是等价的。但是加了正则项之后，我们发现w2总体的损失函数结果更小(因为4*0.25^2<1)，于是我们的系统会选择w2，这也就意味着系统更『喜欢』权重分布均匀的参数，而不是某些特征权重明显高于其他权重(占据绝对主导作用)的参数。之后的系列里会提到，这样一个平滑的操作，实际上也会提高系统的泛化能力，让其具备更高的通用性，而不至于在训练集上过拟合。另外，我们在讨论过拟合的这个部分的时候，并没有提到b这个参数，这是因为它并不具备像W一样的控制输入特征的某个维度影响力的能力。还需要说一下的是，因为正则项的引入，训练集上的准确度是会有一定程度的下降的，我们永远也不可能让损失达到零了(因为这意味着正则化项为0，也就是W=0)。下面是简单的计算损失函数(没加上正则化项)的代码，有未向量化和向量化两种形式：def L_i(x, y, W): \"\"\" 未向量化版本. 对给定的单个样本(x,y)计算multiclass svm loss. - x: 代表图片像素输入的向量 (例如CIFAR-10中是3073 x 1，因为添加了bias项对应的1到x中) - y: 图片对应的类别编号(比如CIFAR-10中是0-9) - W: 权重矩阵 (例如CIFAR-10中是10 x 3073) \"\"\" delta = 1.0 # 设定delta scores = W.dot(x) # 内积计算得分 correct_class_score = scores[y] D = W.shape[0] # 类别数:例如10 loss_i = 0.0 for j in xrange(D): # 遍历所有错误的类别 if j == y: # 跳过正确类别 continue # 对第i个样本累加loss loss_i += max(0, scores[j] - correct_class_score + delta) return loss_i def L_i_vectorized(x, y, W): \"\"\" 半向量化的版本，速度更快。 之所以说是半向量化，是因为这个函数外层要用for循环遍历整个训练集 -_-|| \"\"\" delta = 1.0 scores = W.dot(x) # 矩阵一次性计算 margins = np.maximum(0, scores - scores[y] + delta) margins[y] = 0 loss_i = np.sum(margins) return loss_i def L(X, y, W): \"\"\" 全向量化实现 : - X: 包含所有训练样本中数据(例如CIFAR-10是3073 x 50000) - y: 所有的类别结果 (例如50000 x 1的向量) - W: 权重矩阵 (例如10 x 3073) \"\"\" #待完成...说到这里，其实我们的损失函数，是提供给我们一个数值型的表示，来衡量我们的预测结果和实际结果的差别。而要提高预测的准确性，要做的事情是，想办法最小化这个loss。2.4 一些现实的考虑点2.4.1 设定Delta我们在计算Multi SVM loss的时候，Δ是我们提前设定的一个参数。这个值咋设定？莫不是…也需要交叉验证…？其实基本上大部分的场合下我们设定Δ=1.0都是一个安全的设定。我们看公式中的参数Δ和λ似乎是两个截然不同的参数，实际上他俩做的事情比较类似，都是尽量让模型贴近标准预测结果的时候，在 数据损失/data loss和 正则化损失/regularization loss之间做一个交换和平衡。在损失函数计算公式里，可以看出，权重W的幅度对类别得分有最直接的影响，我们减小W，最后的得分就会减少；我们增大W，最后的得分就增大。从这个角度看，Δ这个参数的设定(Δ=1或者Δ=100)，其实无法限定W的伸缩。而真正可以做到这点的是正则化项，λ的大小，实际上控制着权重可以增长和膨胀的空间。2.4.2 关于二元/Binary支持向量机如果大家之前接触过Binary SVM，我们知道它的公式如下：我们可以理解为类别yi∈−1,1，它是我们的多类别识别的一个特殊情况，而这里的C和λ是一样的作用，只不过他们的大小对结果的影响是相反的，也就是C∝1/λ2.4.3 关于非线性的SVM如果对机器学习有了解，你可能会了解很多其他关于SVM的术语：kernel，dual，SMO算法等等。在这个系列里面我们只讨论最基本的线性形式。当然，其实从本质上来说，这些方法都是类似的。2.5 Softmax分类器话说其实有两种特别常见的分类器，前面提的SVM是其中的一种，而另外一种就是Softmax分类器，它有着截然不同的损失函数。如果你听说过『逻辑回归二分类器』，那么Softmax分类器是它泛化到多分类的情形。不像SVM这种直接给类目打分f(xi,W)并作为输出，Softmax分类器从新的角度做了不一样的处理，我们依旧需要将输入的像素向量映射为得分f(x_i; W) = W x_i，只不过我们还需要将得分映射到概率域，我们也不再使用hinge loss了，而是使用交叉熵损失/cross-entropy loss，形式如下：我们使用fj来代表得分向量f的第j个元素值。和前面提到的一样，总体的损失/loss也是Li遍历训练集之后的均值，再加上正则化项R(W)，而函数被称之为softmax函数：它的输入值是一个实数向量z，然后在指数域做了一个归一化(以保证之和为1)映射为概率。2.5.1 信息论角度的理解对于两个概率分布p(“真实的概率分布”)和估测的概率分布q(估测的属于每个类的概率)，它们的互熵定义为如下形式：而Softmax分类器要做的事情，就是要最小化预测类别的概率分布(之前看到了，是与『实际类别概率分布』(p=[0,…1,…,0]，只在结果类目上是1，其余都为0)两个概率分布的交叉熵。另外，因为互熵可以用熵加上KL距离/Kullback-Leibler Divergence(也叫相对熵/Relative Entropy)来表示，即，而p的熵为0(这是一个确定事件，无随机性)，所以互熵最小化，等同于最小化两个分布之间的KL距离。换句话说，交叉熵想要从给定的分布q上预测结果分布p。2.5.2 概率角度的理解我们再来看看以下表达式其实可以看做给定图片数据xi和类别yi以及参数W之后的归一化概率。在概率的角度理解，我们在做的事情，就是最小化错误类别的负log似然概率，也可以理解为进行最大似然估计/Maximum Likelihood Estimation (MLE)。这个理解角度还有一个好处，这个时候我们的正则化项R(W)有很好的解释性，可以理解为整个损失函数在权重矩阵W上的一个高斯先验，所以其实这时候是在做一个最大后验估计/Maximum a posteriori (MAP)。2.5.3 实际工程上的注意点：数据稳定性在我们要写代码工程实现Softmax函数的时候，计算的中间项因为指数运算可能变得非常大，除法的结果非常不稳定，所以这里需要一个小技巧。注意到，如果我们在分子分母前都乘以常数C，然后整理到指数上，我们会得到下面的公式：C的取值由我们而定，不影响最后的结果，但是对于实际计算过程中的稳定性有很大的帮助。一个最常见的C取值为。这表明我们应该平移向量f中的值使得最大值为0，以下的代码是它的一个实现：f = np.array([123, 456, 789]) # 3个类别的预测示例 p = np.exp(f) / np.sum(np.exp(f)) # 直接运算，数值稳定性不太好 # 我们先对数据做一个平移，所以输入的最大值为0: f -= np.max(f) # f 变成 [-666, -333, 0] p = np.exp(f) / np.sum(np.exp(f)) # 结果正确，同时解决数值不稳定问题2.5.4 关于softmax这个名字的一点说明准确地说，SVM分类器使用hinge loss(有时候也叫max-margin loss)。而Softmax分类器使用交叉熵损失/cross-entropy loss。Softmax分类器从softmax函数(恩，其实做的事情就是把一列原始的类别得分归一化到一列和为1的正数表示概率)得到，softmax函数使得交叉熵损失可以用起来。而实际上，我们并没有softmax loss这个概念，因为softmax实质上就是一个函数，有时候我们图方便，就随口称呼softmax loss。2.6 SVM 与 Softmax这个比较很有意思，就像在用到分类算法的时候，就会想SVM还是logistic regression呢一样。我们先用一张图来表示从输入端到分类结果，SVM和Softmax都做了啥：区别就是拿到原始像素数据映射得到的得分之后的处理，而正因为处理方式不同，我们定义不同的损失函数，有不同的优化方法。2.6.1 另外的差别SVM下，我们能完成类别的判定，但是实际上我们得到的类别得分，大小顺序表示着所属类别的排序，但是得分的绝对值大小并没有特别明显的物理含义。Softmax分类器中，结果的绝对值大小表征属于该类别的概率。举个例子说，SVM可能拿到对应 猫/狗/船 的得分[12.5, 0.6, -23.0]，同一个问题，Softmax分类器拿到[0.9, 0.09, 0.01]。这样在SVM结果下我们只知道『猫』是正确答案，而在Softmax分类器的结果中，我们可以知道属于每个类别的概率。但是，Softmax中拿到的概率，其实和正则化参数λ有很大的关系，因为λ实际上在控制着W的伸缩程度，所以也控制着最后得分的scale，这会直接影响最后概率向量中概率的『分散度』，比如说某个λ下，我们得到的得分和概率可能如下：而我们加大λ，提高其约束能力后，很可能得分变为原来的一半大小，这时候如下：因为λ的不同，使得最后得到的结果概率分散度有很大的差别。在上面的结果中，猫有着统治性的概率大小，而在下面的结果中，它和船只的概率差距被缩小。2.6.2 际应用中的SVM与Softmax分类器实际应用中，两类分类器的表现是相当的。当然，每个人都有自己的喜好和倾向性，习惯用某类分类器。一定要对比一下的话：SVM其实并不在乎每个类别得到的绝对得分大小，举个例子说，我们现在对三个类别，算得的得分是[10, -2, 3]，实际第一类是正确结果，而设定Δ=1，那么10-3=7已经比1要大很多了，那对SVM而言，它觉得这已经是一个很标准的答案了，完全满足要求了，不需要再做其他事情了，结果是 [10, -100, -100] 或者 [10, 9, 9]，它都是满意的。然而对于Softmax而言，不是这样的， [10, -100, -100] 和 [10, 9, 9]映射到概率域，计算得到的交叉熵损失是有很大差别的。所以Softmax是一个永远不会满足的分类器，在每个得分计算到的概率基础上，它总是觉得可以让概率分布更接近标准结果一些，交叉熵损失更小一些。有兴趣的话，W与得分预测结果demo是一个可以手动调整和观察二维数据上的分类问题，随W变化结果变化的demo，可以动手调调看看。参考资料与原文"}
{"content2":"原文地址：UIUC同学Jia-Bin Huang收集的计算机视觉代码合集作者：千里8848UIUC的Jia-Bin Huang同学收集了很多计算机视觉方面的代码，链接如下：https://netfiles.uiuc.edu/jbhuang1/www/resources/vision/index.html这些代码很实用，可以让我们站在巨人的肩膀上~~TopicResourcesReferencesFeature ExtractionSIFT [1] [Demo program][SIFT Library] [VLFeat]PCA-SIFT [2] [Project]Affine-SIFT [3] [Project]SURF [4] [OpenSURF] [Matlab Wrapper]Affine Covariant Features [5] [Oxford project]MSER [6] [Oxford project] [VLFeat]Geometric Blur [7] [Code]Local Self-Similarity Descriptor [8] [Oxford implementation]Global and Efficient Self-Similarity [9] [Code]Histogram of Oriented Graidents [10] [INRIA Object Localization Toolkit] [OLT toolkit for Windows]GIST [11] [Project]Shape Context [12] [Project]Color Descriptor [13] [Project]Pyramids of Histograms of Oriented Gradients [Code]Space-Time Interest Points (STIP) [14] [Code]Boundary Preserving Dense Local Regions [15][Project]D. Lowe. Distinctive Image Features from Scale-Invariant Keypoints, IJCV 2004. [PDF]Y. Ke and R. Sukthankar, PCA-SIFT: A More Distinctive Representation for Local Image Descriptors,CVPR, 2004. [PDF]J.M. Morel and G.Yu, ASIFT, A new framework for fully affine invariant image comparison. SIAM Journal on Imaging Sciences, 2009. [PDF]H. Bay, T. Tuytelaars and L. V. Gool SURF: Speeded Up Robust Features,ECCV, 2006. [PDF]K. Mikolajczyk, T. Tuytelaars, C. Schmid, A. Zisserman, J. Matas, F. Schaffalitzky, T. Kadir and L. Van Gool, A comparison of affine region detectors. IJCV, 2005. [PDF]J. Matas, O. Chum, M. Urba, and T. Pajdla. Robust wide baseline stereo from maximally stable extremal regions. BMVC, 2002. [PDF]A. C. Berg, T. L. Berg, and J. Malik. Shape matching and object recognition using low distortion correspondences. CVPR, 2005. [PDF]E. Shechtman and M. Irani. Matching local self-similarities across images and videos, CVPR, 2007. [PDF]T. Deselaers and V. Ferrari. Global and Efficient Self-Similarity for Object Classification and Detection. CVPR 2010. [PDF]N. Dalal and B. Triggs. Histograms of Oriented Gradients for Human Detection. CVPR 2005. [PDF]A. Oliva and A. Torralba. Modeling the shape of the scene: a holistic representation of the spatial envelope, IJCV, 2001. [PDF]S. Belongie, J. Malik and J. Puzicha. Shape matching and object recognition using shape contexts, PAMI, 2002. [PDF]K. E. A. van de Sande, T. Gevers and Cees G. M. Snoek, Evaluating Color Descriptors for Object and Scene Recognition, PAMI, 2010.I. Laptev, On Space-Time Interest Points, IJCV, 2005. [PDF]J. Kim and K. Grauman, Boundary Preserving Dense Local Regions, CVPR 2011. [PDF]Image SegmentationNormalized Cut [1] [Matlab code]Gerg Mori' Superpixel code [2] [Matlab code]Efficient Graph-based Image Segmentation [3] [C++ code] [Matlab wrapper]Mean-Shift Image Segmentation [4] [EDISON C++ code] [Matlab wrapper]OWT-UCM Hierarchical Segmentation [5] [Resources]Turbepixels [6] [Matlab code 32bit] [Matlab code 64bit] [Updated code]Quick-Shift [7] [VLFeat]SLIC Superpixels [8] [Project]Segmentation by Minimum Code Length [9] [Project]Biased Normalized Cut [10] [Project]Segmentation Tree [11-12] [Project]Entropy Rate Superpixel Segmentation [13] [Code]J. Shi and J Malik, Normalized Cuts and Image Segmentation, PAMI, 2000 [PDF]X. Ren and J. Malik. Learning a classification model for segmentation.ICCV, 2003. [PDF]P. Felzenszwalb and D. Huttenlocher. Efficient Graph-Based Image Segmentation, IJCV 2004. [PDF]D. Comaniciu, P Meer. Mean Shift: A Robust Approach Toward Feature Space Analysis. PAMI 2002. [PDF]P. Arbelaez, M. Maire, C. Fowlkes and J. Malik. Contour Detection and Hierarchical Image Segmentation. PAMI, 2011. [PDF]A. Levinshtein, A. Stere, K. N. Kutulakos, D. J. Fleet, S. J. Dickinson, and K. Siddiqi, TurboPixels: Fast Superpixels Using Geometric Flows, PAMI 2009. [PDF]A. Vedaldi and S. Soatto, Quick Shift and Kernel Methodsfor Mode Seeking,ECCV, 2008. [PDF]R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Susstrunk, SLIC Superpixels, EPFL Technical Report, 2010. [PDF]A. Y. Yang, J. Wright, S. Shankar Sastry, Y. Ma , Unsupervised Segmentation of Natural Images via Lossy Data Compression, CVIU, 2007. [PDF]S. Maji, N. Vishnoi and J. Malik, Biased Normalized Cut, CVPR 2011E. Akbas and N. Ahuja, “From ramp discontinuities to segmentation tree,”  ACCV 2009. [PDF]N. Ahuja, “A Transform for Multiscale Image Segmentation by Integrated Edge and Region Detection,” PAMI 1996 [PDF]M.-Y. Liu, O. Tuzel, S. Ramalingam, and R. Chellappa, Entropy Rate Superpixel Segmentation, CVPR 2011 [PDF]Object DetectionA simple object detector with boosting [Project]INRIA Object Detection and Localization Toolkit [1] [Project]Discriminatively Trained Deformable Part Models [2] [Project]Cascade Object Detection with Deformable Part Models [3] [Project]Poselet [4] [Project]Implicit Shape Model [5] [Project]Viola and Jones's Face Detection [6] [Project]N. Dalal and B. Triggs. Histograms of Oriented Gradients for Human Detection. CVPR 2005. [PDF]P. Felzenszwalb, R. Girshick, D. McAllester, D. Ramanan.Object Detection with Discriminatively Trained Part Based Models, PAMI, 2010 [PDF]P. Felzenszwalb, R. Girshick, D. McAllester. Cascade Object Detection with Deformable Part Models. CVPR 2010 [PDF]L. Bourdev, J. Malik, Poselets: Body Part Detectors Trained Using 3D Human Pose Annotations, ICCV 2009 [PDF]B. Leibe, A. Leonardis, B. Schiele. Robust Object Detection with Interleaved Categorization and Segmentation, IJCV, 2008. [PDF]P. Viola and M. Jones, Rapid Object Detection Using a Boosted Cascade of Simple Features, CVPR 2001. [PDF]Saliency DetectionItti, Koch, and Niebur' saliency detection [1] [Matlab code]Frequency-tuned salient region detection [2] [Project]Saliency detection using maximum symmetric surround [3] [Project]Attention via Information Maximization [4] [Matlab code]Context-aware saliency detection [5] [Matlab code]Graph-based visual saliency [6] [Matlab code]Saliency detection: A spectral residual approach. [7] [Matlab code]Segmenting salient objects from images and videos. [8] [Matlab code]Saliency Using Natural statistics. [9] [Matlab code]Discriminant Saliency for Visual Recognition from Cluttered Scenes. [10] [Code]Learning to Predict Where Humans Look [11] [Project]Global Contrast based Salient Region Detection [12] [Project]L. Itti, C. Koch, and E. Niebur. A model of saliency-based visual attention for rapid scene analysis. PAMI, 1998. [PDF]R. Achanta, S. Hemami, F. Estrada, and S. Susstrunk. Frequency-tuned salient region detection. In CVPR, 2009. [PDF]R. Achanta and S. Susstrunk. Saliency detection using maximum symmetric surround. In ICIP, 2010. [PDF]N. Bruce and J. Tsotsos. Saliency based on information maximization. InNIPS, 2005. [PDF]S. Goferman, L. Zelnik-Manor, and A. Tal. Context-aware saliency detection. In CVPR, 2010. [PDF]J. Harel, C. Koch, and P. Perona. Graph-based visual saliency. NIPS, 2007. [PDF]X. Hou and L. Zhang. Saliency detection: A spectral residual approach.CVPR, 2007. [PDF]E. Rahtu, J. Kannala, M. Salo, and J. Heikkila. Segmenting salient objects from images and videos. CVPR, 2010. [PDF]L. Zhang, M. Tong, T. Marks, H. Shan, and G. Cottrell. Sun: A bayesian framework for saliency using natural statistics. Journal of Vision, 2008. [PDF]D. Gao and N. Vasconcelos, Discriminant Saliency for Visual Recognition from Cluttered Scenes, NIPS, 2004. [PDF]T. Judd and K. Ehinger and F. Durand and A. Torralba, Learning to Predict Where Humans Look, ICCV, 2009. [PDF]M.-M. Cheng, G.-X. Zhang, N. J. Mitra, X. Huang, S.-M. Hu. Global Contrast based Salient Region Detection. CVPR 2011.Image ClassificationPyramid Match [1] [Project]Spatial Pyramid Matching [2] [Code]Locality-constrained Linear Coding [3] [Project] [Matlab code]Sparse Coding [4] [Project] [Matlab code]Texture Classification [5] [Project]Multiple Kernels for Image Classification [6] [Project]Feature Combination [7] [Project]SuperParsing [Code]K. Grauman and T. Darrell, The Pyramid Match Kernel: Discriminative Classification with Sets of Image Features, ICCV 2005. [PDF]S. Lazebnik, C. Schmid, and J. Ponce. Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories, CVPR 2006[PDF]J. Wang, J. Yang, K. Yu, F. Lv, T. Huang, and Y. Gong. Locality-constrained Linear Coding for Image Classification, CVPR, 2010 [PDF]J. Yang, K. Yu, Y. Gong, T. Huang, Linear Spatial Pyramid Matching using Sparse Coding for Image Classification, CVPR, 2009 [PDF]M. Varma and A. Zisserman, A statistical approach to texture classification from single images, IJCV2005. [PDF]A. Vedaldi, V. Gulshan, M. Varma, and A. Zisserman, Multiple Kernels for Object Detection. ICCV, 2009. [PDF]P. Gehler and S. Nowozin, On Feature Combination for Multiclass Object Detection, ICCV, 2009. [PDF]J. Tighe and S. Lazebnik, SuperParsing: Scalable Nonparametric ImageParsing with Superpixels, ECCV 2010. [PDF]Category-Independent Object ProposalObjectness measure [1] [Code]Parametric min-cut [2] [Project]Object proposal [3] [Project]B. Alexe, T. Deselaers, V. Ferrari, What is an Object?, CVPR 2010 [PDF]J. Carreira and C. Sminchisescu. Constrained Parametric Min-Cuts for Automatic Object Segmentation, CVPR 2010. [PDF]I. Endres and D. Hoiem. Category Independent Object Proposals, ECCV 2010. [PDF]MRFGraph Cut [Project] [C++/Matlab Wrapper Code]Y. Boykov, O. Veksler and R. Zabih, Fast Approximate Energy Minimization via Graph Cuts, PAMI 2001 [PDF]Shadow DetectionShadow Detection using Paired Region [Project]Ground shadow detection [Project]R. Guo, Q. Dai and D. Hoiem, Single-Image Shadow Detection and Removal using Paired Regions, CVPR 2011 [PDF]J.-F. Lalonde, A. A. Efros, S. G. Narasimhan, Detecting Ground Shadowsin Outdoor Consumer Photographs, ECCV 2010 [PDF]Optical FlowKanade-Lucas-Tomasi Feature Tracker [C Code]Optical Flow Matlab/C++ code by Ce Liu [Project]Horn and Schunck's method by Deqing Sun [Code]Black and Anandan's method by Deqing Sun [Code]Optical flow code by Deqing Sun [Matlab Code] [Project]Large Displacement Optical Flow by Thomas Brox [Executable for 64-bit Linux] [ Matlab Mex-functions for 64-bit Linux and 32-bit Windows] [Project]Variational Optical Flow by Thomas Brox [Executable for 64-bit Linux] [ Executable for 32-bit Windows ] [ Matlab Mex-functions for 64-bit Linux and 32-bit Windows ] [Project]B.D. Lucas and T. Kanade, An Iterative Image Registration Technique with an Application to Stereo Vision, IJCAI 1981. [PDF]J. Shi, C. Tomasi, Good Feature to Track, CVPR 1994. [PDF]C. Liu. Beyond Pixels: Exploring New Representations and Applications for Motion Analysis. Doctoral Thesis. MIT 2009. [PDF]B.K.P. Horn and B.G. Schunck, Determining Optical Flow, Artificial Intelligence 1981. [PDF]M. J. Black and P. Anandan, A framework for the robust estimation of optical flow, ICCV 93. [PDF]D. Sun, S. Roth, and M. J. Black, Secrets of optical flow estimation and their principles, CVPR 2010. [PDF]T. Brox, J. Malik, Large displacement optical flow: descriptor matching in variational motion estimation, PAMI, 2010 [PDF]T. Brox, A. Bruhn, N. Papenberg, J. Weickert, High accuracy optical flow estimation based on a theory for warping, ECCV 2004 [PDF]Object TrackingParticle filter object tracking [1] [Project]KLT Tracker [2-3] [Project]MILTrack [4] [Code]Incremental Learning for Robust Visual Tracking [5] [Project]Online Boosting Trackers [6-7] [Project]L1 Tracking [8] [Matlab code]P. Perez, C. Hue, J. Vermaak, and M. Gangnet. Color-Based Probabilistic Tracking ECCV, 2002. [PDF]B.D. Lucas and T. Kanade, An Iterative Image Registration Technique with an Application to Stereo Vision, IJCAI 1981. [PDF]J. Shi, C. Tomasi, Good Feature to Track, CVPR 1994. [PDF]B. Babenko, M. H. Yang, S. Belongie, Robust Object Tracking with Online Multiple Instance Learning, PAMI 2011 [PDF]D. Ross, J. Lim, R.-S. Lin, M.-H. Yang, Incremental Learning for Robust Visual Tracking, IJCV 2007 [PDF]H. Grabner, and H. Bischof, On-line Boosting and Vision, CVPR 2006 [PDF]H. Grabner, C. Leistner, and H. Bischof, Semi-supervised On-line Boosting for Robust Tracking, ECCV 2008 [PDF]X. Mei and H. Ling, Robust Visual Tracking using L1 Minimization, ICCV, 2009. [PDF]Image MattingClosed Form Matting [Code]Spectral Matting [Project]Learning-based Matting [Code]A. Levin D. Lischinski and Y. Weiss. A Closed Form Solution to Natural Image Matting, PAMI 2008 [PDF]A. Levin, A. Rav-Acha, D. Lischinski. Spectral Matting. PAMI 2008. [PDF]Y. Zheng and C. Kambhamettu, Learning Based Digital Matting, ICCV 2009 [PDF]Bilateral FilteringFast Bilateral Filter [Project]Real-time O(1) Bilateral Filtering [Code]SVM for Edge-Preserving Filtering [Code]Q. Yang, K.-H. Tan and N. Ahuja,  Real-time O(1) Bilateral Filtering,CVPR 2009. [PDF]Q. Yang, S. Wang, and N. Ahuja, SVM for Edge-Preserving Filtering,CVPR 2010. [PDF]Image DenoisingK-SVD [Matlab code]BLS-GSM [Project]BM3D [Project]FoE [Code]GFoE [Code]Non-local means [Code]Kernel regression [Code]Image Super-ResolutionMRF for image super-resolution [Project]Multi-frame image super-resolution [Project]UCSC Super-resolution [Project]Sprarse coding super-resolution [Code]Image DeblurringEficient Marginal Likelihood Optimization in Blind Deconvolution [Code]Analyzing spatially varying blur [Project]Radon Transform [Code]Image Quality AssessmentFSIM [1] [Project]Degradation Model [2] [Project]SSIM [3] [Project]SPIQA [Code]L. Zhang, L. Zhang, X. Mou and D. Zhang, FSIM: A Feature Similarity Index for Image Quality Assessment, TIP 2011. [PDF]N. Damera-Venkata, and T. D. Kite, W. S. Geisler, B. L. Evans, and A. C. Bovik,Image Quality Assessment Based on a Degradation Model, TIP 2000. [PDF]Z. Wang, A. C. Bovik, H. R. Sheikh and E. P. Simoncelli, Image quality assessment: from error visibility to structural similarity, TIP 2004. [PDF]B. Ghanem, E. Resendiz, and N. Ahuja, Segmentation-Based Perceptual Image Quality Assessment (SPIQA), ICIP 2008. [PDF]Density EstimationKernel Density Estimation Toolbox [Project]Dimension ReductionDimensionality Reduction Toolbox [Project]ISOMAP [Code]LLE [Project]Laplacian Eigenmaps [Code]Diffusion maps [Code]Sparse CodingLow-Rank Matrix CompletionNearest Neighbors matchingANN: Approximate Nearest Neighbor Searching [Project] [Matlab wrapper]FLANN: Fast Library for Approximate Nearest Neighbors [Project]SteoreoStereoMatcher [Project]D. Scharstein and R. Szeliski. A taxonomy and evaluation of dense two-frame stereo correspondence algorithms, IJCV 2002 [PDF]Structure from motionBoundler [1] [Project]N. Snavely, S. M. Seitz, R. Szeliski. Photo Tourism: Exploring image collections in 3D. SIGGRAPH, 2006. [PDF]Distance TransformationDistance Transforms of Sampled Functions [1] [Project]P. F. Felzenszwalb and D. P. Huttenlocher. Distance transforms of sampled functions. Technical report, Cornell University, 2004. [PDF]Chamfer MatchingFast Directional Chamfer Matching [Code]M.-Y. Liu, O. Tuzel, A. Veeraraghavan, and R. Chellappa, Fast Directional Chamfer Matching, CVPR 2010 [PDF]ClusteringK-Means [VLFeat] [Oxford code]Spectral Clustering [UW Project][Code] [Self-Tuning code]Affinity Propagation [Project]ClassificationSVM [Libsvm] [SVM-Light] [SVM-Struct]BoostingNaive BayesRegressionSVMRVMGPRMultiple Kernel Learning (MKL)SHOGUN [Project]OpenKernel.org [Project]DOGMA (online algorithms) [Project]SimpleMKL [Project]S. Sonnenburg, G. Rätsch, C. Schäfer, B. Schölkopf . Large scale multiple kernel learning. JMLR, 2006. [PDF]F. Orabona and L. Jie. Ultra-fast optimization algorithm for sparse multi kernel learning. ICML, 2011. [PDF]F. Orabona, L. Jie, and B. Caputo. Online-batch strongly convex multi kernel learning. CVPR, 2010. [PDF]A. Rakotomamonjy, F. Bach, S. Canu, and Y. Grandvalet. Simplemkl. JMRL, 2008. [PDF]Multiple Instance Learning (MIL)MIForests [1] [Project]MILIS [2]MILES [3] [Project] [Code]DD-SVM [4] [Project]C. Leistner, A. Saffari, and H. Bischof, MIForests: Multiple-Instance Learning with Randomized Trees, ECCV 2010. [PDF]Z. Fu, A. Robles-Kelly, and J. Zhou, MILIS: Multiple instance learning with instance selection, PAMI 2010. [PDF]Y. Chen, J. Bi and J. Z. Wang, MILES: Multiple-Instance Learning via Embedded Instance Selection. PAMI 2006 [PDF]Yixin Chen and James Z. Wang, Image Categorization by Learning and Reasoning with Regions, JMLR 2004. [PDF]Other UtilitiesCode for downloading Flickr images, by James Hays [Code]The Lightspeed Matlab Toolbox by Tom Minka [Code]MATLAB Functions for Multiple View Geometry [Code]Peter's Functions for Computer Vision [Code]Statistical Pattern Recognition Toolbox [Code]Useful Links (dataset, lectures, and other softwares)Conference InformationComputer Image Analysis, Computer Vision ConferencesPapersComputer vision paper on the webNIPS ProceedingsDatasetsCompiled list of recognition datasetsComputer vision dataset from CMULecturesVideolecturesSource CodesComputer Vision Algorithm ImplementationsOpenCVSource Code Collection for Reproducible Research"}
{"content2":"简介每年全世界都会举办很多计算机视觉（Computer Vision，CV）、 机器学习（Machine Learning，ML）、人工智能（Artificial Intelligence ，AI）领域的学术会议。笔者选取了其中影响力较大，有代表性的重要会议进行了汇总，特意按照时间进行了排序，方便大家查看。如有遗漏，还请留言补充。文末有福利呢！UAI 2018会议名称：Conference on Uncertainty in Artificial Intelligence会议地点：美国加州蒙特雷会议时间：2018.08.06 - 10网址：http://auai.org/uai2018介绍：ML特定子领域的高质量会议，今年的已经举行。ECCV 2018会议名称：European Conference on Computer Vision会议地点：德国慕尼黑会议时间：2018.09.08 - 14网址：https://eccv2018.org介绍：计算机视觉及模式识别领域国际三大顶级会议之一，今年的已经举行。ACCV 2018会议名称：Asian Conference on Computer Vision会议地点：澳大利亚佩斯会议时间：2018.12.02 - 06网址：http://accv2018.net介绍：亚洲的计算机视觉会议NIPS 2018会议名称：Neural Information Processing Systems会议地点：加拿大蒙特利尔会议时间：2018.12.03 - 08网址： https://nips.cc/Conferences/2018介绍：Machine learning , computational neuroscience领域国际顶级会议ICMLA 2018会议名称：International Conference On Machine Learning And Applications会议地点：美国佛罗里达会议时间：2018.12.17 - 20网址：http://www.icmla-conference.org/icmla18MMM 2019会议名称：25th International Conference on MultiMedia Modeling会议地点：希腊塞萨洛尼基会议时间：2019.01.08-11网址：http://mmm2019.iti.gr/介绍：多媒体建模及应用领域国际权威会议AAAI 2019会议名称：Association for the Advancement of Artificial Intelligence会议地点：美国夏威夷会议时间：2019.01.27 - 02.01网址：http://www.aaai.org/aaai19介绍：人工智能领域顶级会议ICIGP 2019会议名称：2ND INTERNATIONAL CONFERENCE ON IMAGE AND GRAPHICS PROCESSING会议地点：新加坡会议时间：2019.02.23-25网址：http://icigp.org/介绍：促进学术界和工业界交流的图形图像处理会议ICIAI2019会议名称：The 3rd International Conference on Innovation in Artificial Intelligence会议地点：中国苏州会议时间：2019.03.15- 18网址：http://www.iciai.org/介绍：人工智能创新国际会议ALT 2019会议名称：International Conference on Algorithmic Learning Theory会议地点：美国芝加哥会议时间：2019.03.22 - 24网址：http://alt2019.algorithmiclearningtheory.org介绍：计算机学习理论较好的会议CVM2019会议名称：Computational Visual Media Conference会议地点：英国巴斯会议时间：2019.04.24 - 26网址：http://iccvm.org/2019/介绍：计算机视觉相关的基础研究和应用会议ICLR 2019会议名称：International Conference on Learning Representations会议地点：美国新奥尔良会议时间：2019.05.06 - 09网址： http://www.iclr.cc介绍：神经网络顶会AAMAS 2019会议名称：International Conference on Autonomous Agents and Multiagent Systems会议地点：加拿大蒙特利尔会议时间：2019.05.13 - 17网址：http://aamas2019.encs.concordia.ca介绍：Agent方面最好的会议之一ICML 2019会议名称：International Conference on Machine Learning会议地点：美国加利福尼亚会议时间：2019.06.10 - 15网址：https://icml.cc/Conferences/2019介绍：机器学习领域国际著名会议CVPR 2019会议名称： IEEE Conference on Computer Vision and Pattern Recognition会议地点：美国洛杉矶会议时间：2019.06.15 - 21网址：http://cvpr2019.thecvf.com介绍：计算机视觉及模式识别领域国际三大顶级会议之一COLT 2019会议名称：Conference on Learning Theory会议地点：美国亚利桑那州会议时间：2019.06.25 - 28网址：http://www.learningtheory.org/colt2019介绍：计算机学习理论最好的会议之一ICME 2019会议名称：International Conference on Multimedia and Expo会议地点：中国上海会议时间：2019.07.08 - 12网址：http://www.icme2019.org介绍：多媒体方面比较好的会议GECCO 2019会议名称：Genetic and Evolutionary Computation Conference会议地点：捷克布拉格会议时间：2019.07.13 - 17网址： https://gecco-2019.sigevo.org介绍：进化计算方面最重要的会议之一IJCNN 2019会议名称：International Joint Conference on Neural Networks会议地点：匈牙利布达佩斯会议时间：2019.07.14 - 19网址：https://www.ijcnn.org介绍：神经网络方面最重要的会议, 盛会型SIGGRAPH 2019会议名称：Computer Graphics and Interactive Techniques会议地点：美国洛杉矶会议时间：2019.07.29 – 08.01网址： http://s2019.siggraph.org介绍：计算机图形学领域最权威、影响力最大的国际会议IJCAI 2019会议名称：International Joint Conference on Artificial Intelligence会议地点：中国澳门会议时间：2019.08.10 - 16网址：http://www.ijcai19.org介绍：人工智能领域中最主要的学术会议之一PRICAI 2019会议名称：Pacific Rim International Conference on Artificial Intelligence会议地点：斐济雅奴卡岛会议时间：2019.08.26 - 30网址：https://www.pricai.org/2019介绍：亚太综合型人工智能会议BMVC 2019会议名称：British Machine Vision Conference会议地点：英国加的夫大学会议时间：2019.09.09 -12网址：http://bmvc2019.org介绍：计算机视觉领域著名国际会议ICANN 2019会议名称：International Conference on Artificial Neural Networks会议地点：德国慕尼黑会议时间：2019.09.10 - 13网址：https://e-nns.org/icann2019介绍：欧洲的神经网络会议ICIP 2019会议名称：International Conference on Image Processing会议地点：中国台湾会议时间：2019.09.22 - 25网址：http://www.2019.ieeeicip.org介绍：图像处理领域国际著名会议ACMMM 2019会议名称：ACM Multimedia会议地点：法国尼斯会议时间：2019.10.21 - 25网址：http://www.acmmm.org/2019介绍：多媒体技术领域奥运级别的顶级盛会ICCV 2019会议名称：International Conference on Computer Vision会议地点：韩国首尔会议时间：2019.10.27 - 11.03网址： http://iccv2019.thecvf.com介绍：计算机视觉及模式识别领域国际三大顶级会议之一WACV 2020会议名称：Winter Conference on Applications of Computer Vision(First Round)会议地点：美国科罗拉多州会议时间：2020.03.02 - 05网址：http://wacv20.uccs.us介绍：侧重计算机视觉应用的国际会议ICASSP 2020会议名称：International Conference on Acoustics, Speech and Signal Processing会议地点：西班牙巴塞罗那会议时间：2020.05.04 - 09网址：http://2020.ieeeicassp.org介绍：声学、语音与信号处理及其应用国际顶级会议ECAI 2020会议名称：European Conference on Artificial Intelligence会议地点： 西班牙圣地亚哥会议时间：待定网址： 待定介绍：欧洲的人工智能综合型会议福利为了使大家更方便的查看，特意把文章内容汇总为pdf和html两版。关注“计算机视觉life”公众号菜单栏回复“顶会”领取吧。---------------------作者：electech6来源：CSDN原文：https://blog.csdn.net/electech6/article/details/84384852版权声明：本文为博主原创文章，转载请附上博文链接！"}
{"content2":"今年夏天，将在深圳举办“全球人工智能与机器人创新大会”（GAIR），在本次大会上，我们将发布“人工智能与机器人Top25创新企业榜“，慧眼科技是我们重点关注的公司之一。今天，我们邀请到慧眼科技研发总监李汉曦，为我们带来深度学习与计算机视觉方面的内容分享。嘉宾介绍：李汉曦，慧眼科技研发总监，澳大利亚国立大学博士；曾任澳大利亚国家信息通信公司(NICTA)任高级研究员；人脸识别，物体检测，物体跟踪、深度学习方面的专家，在TPAMI，TIP, TNNLS和Pattern Recognition等权威期刊，以及CVPR，ECCV，BMVC, ACCV等领域内重要会议发表过有影响力的论文；现为澳大利亚格里菲斯大学客座研究员，江西师范大学特聘教授。人工智能是人类一个非常美好的梦想，跟星际漫游和长生不老一样。我们想制造出一种机器，使得它跟人一样具有一定的对外界事物感知能力，比如看见世界。在上世纪50年代，数学家图灵提出判断机器是否具有人工智能的标准：图灵测试。即把机器放在一个房间，人类测试员在另一个房间，人跟机器聊天，测试员事先不知道另一房间里是人还是机器 。经过聊天，如果测试员不能确定跟他聊天的是人还是机器的话，那么图灵测试就通过了，也就是说这个机器具有与人一样的感知能力。但是从图灵测试提出来开始到本世纪初，50多年时间有无数科学家提出很多机器学习的算法，试图让计算机具有与人一样的智力水平，但直到2006年深度学习算法的成功，才带来了一丝解决的希望。众星捧月的深度学习 深度学习在很多学术领域，比非深度学习算法往往有20-30%成绩的提高。很多大公司也逐渐开始出手投资这种算法，并成立自己的深度学习团队，其中投入最大的就是谷歌，2008年6月披露了谷歌脑项目。2014年1月谷歌收购DeepMind，然后2016年3月其开发的Alphago算法在围棋挑战赛中，战胜了韩国九段棋手李世石，证明深度学习设计出的算法可以战胜这个世界上最强的选手。在硬件方面，Nvidia最开始做显示芯片，但从2006及2007年开始主推用GPU芯片进行通用计算，它特别适合深度学习中大量简单重复的计算量。目前很多人选择Nvidia的CUDA工具包进行深度学习软件的开发。微软从2012年开始，利用深度学习进行机器翻译和中文语音合成工作，其人工智能小娜背后就是一套自然语言处理和语音识别的数据算法。百度在2013年宣布成立百度研究院，其中最重要的就是百度深度学习研究所，当时招募了著名科学家余凯博士。不过后来余凯离开百度，创立了另一家从事深度学习算法开发的公司地平线。Facebook和Twitter也都各自进行了深度学习研究，其中前者携手纽约大学教授Yann Lecun，建立了自己的深度学习算法实验室；2015年10月，Facebook宣布开源其深度学习算法框架，即Torch框架。Twitter在2014年7月收购了Madbits，为用户提供高精度的图像检索服务。前深度学习时代的计算机视觉 互联网巨头看重深度学习当然不是为了学术，主要是它能带来巨大的市场。那为什么在深度学习出来之前，传统算法为什么没有达到深度学习的精度？在深度学习算法出来之前，对于视觉算法来说，大致可以分为以下5个步骤：特征感知，图像预处理，特征提取，特征筛选，推理预测与识别。早期的机器学习中，占优势的统计机器学习群体中，对特征是不大关心的。我认为，计算机视觉可以说是机器学习在视觉领域的应用，所以计算机视觉在采用这些机器学习方法的时候，不得不自己设计前面4个部分。但对任何人来说这都是一个比较难的任务。传统的计算机识别方法把特征提取和分类器设计分开来做，然后在应用时再合在一起，比如如果输入是一个摩托车图像的话，首先要有一个特征表达或者特征提取的过程，然后把表达出来的特征放到学习算法中进行分类的学习。过去20年中出现了不少优秀的特征算子，比如最著名的SIFT算子，即所谓的对尺度旋转保持不变的算子。它被广泛地应用在图像比对，特别是所谓的structure from motion这些应用中，有一些成功的应用例子。另一个是HoG算子，它可以提取物体，比较鲁棒的物体边缘，在物体检测中扮演着重要的角色。这些算子还包括Textons，Spin image，RIFT和GLOH，都是在深度学习诞生之前或者深度学习真正的流行起来之前，占领视觉算法的主流。几个（半）成功例子 这些特征和一些特定的分类器组合取得了一些成功或半成功的例子，基本达到了商业化的要求但还没有完全商业化。一是八九十年代的指纹识别算法，它已经非常成熟，一般是在指纹的图案上面去寻找一些关键点，寻找具有特殊几何特征的点，然后把两个指纹的关键点进行比对，判断是否匹配。然后是2001年基于Haar的人脸检测算法，在当时的硬件条件下已经能够达到实时人脸检测，我们现在所有手机相机里的人脸检测，都是基于它或者它的变种。第三个是基于HoG特征的物体检测，它和所对应的SVM分类器组合起来的就是著名的DPM算法。DPM算法在物体检测上超过了所有的算法，取得了比较不错的成绩。但这种成功例子太少了，因为手工设计特征需要大量的经验，需要你对这个领域和数据特别了解，然后设计出来特征还需要大量的调试工作。说白了就是需要一点运气。另一个难点在于，你不只需要手工设计特征，还要在此基础上有一个比较合适的分类器算法。同时设计特征然后选择一个分类器，这两者合并达到最优的效果，几乎是不可能完成的任务。仿生学角度看深度学习 如果不手动设计特征，不挑选分类器，有没有别的方案呢？能不能同时学习特征和分类器？即输入某一个模型的时候，输入只是图片，输出就是它自己的标签。比如输入一个明星的头像，出来的标签就是一个50维的向量（如果要在50个人里识别的话），其中对应明星的向量是1，其他的位置是0。这种设定符合人类脑科学的研究成果。1981年诺贝尔医学生理学奖颁发给了David Hubel，一位神经生物学家。他的主要研究成果是发现了视觉系统信息处理机制，证明大脑的可视皮层是分级的。他的贡献主要有两个，一是他认为人的视觉功能一个是抽象，一个是迭代。抽象就是把非常具体的形象的元素，即原始的光线像素等信息，抽象出来形成有意义的概念。这些有意义的概念又会往上迭代，变成更加抽象，人可以感知到的抽象概念。像素是没有抽象意义的，但人脑可以把这些像素连接成边缘，边缘相对像素来说就变成了比较抽象的概念；边缘进而形成球形，球形然后到气球，又是一个抽象的过程，大脑最终就知道看到的是一个气球。模拟人脑识别人脸，也是抽象迭代的过程，从最开始的像素到第二层的边缘，再到人脸的部分，然后到整张人脸，是一个抽象迭代的过程。再比如看到图片中的摩托车，我们可能在脑子里就几微秒的时间，但是经过了大量的神经元抽象迭代。对计算机来说最开始看到的根本也不是摩托车，而是RGB图像三个通道上不同的数字。所谓的特征或者视觉特征，就是把这些数值给综合起来用统计或非统计的形式，把摩托车的部件或者整辆摩托车表现出来。深度学习的流行之前，大部分的设计图像特征就是基于此，即把一个区域内的像素级别的信息综合表现出来，利于后面的分类学习。如果要完全模拟人脑，我们也要模拟抽象和递归迭代的过程，把信息从最细琐的像素级别，抽象到“种类”的概念，让人能够接受。卷积的概念 计算机视觉里经常使卷积神经网络，即CNN，是一种对人脑比较精准的模拟。什么是卷积？卷积就是两个函数之间的相互关系，然后得出一个新的值，他是在连续空间做积分计算，然后在离散空间内求和的过程。实际上在计算机视觉里面，可以把卷积当做一个抽象的过程，就是把小区域内的信息统计抽象出来。比如，对于一张爱因斯坦的照片，我可以学习n个不同的卷积和函数，然后对这个区域进行统计。可以用不同的方法统计，比如着重统计中央，也可以着重统计周围，这就导致统计的和函数的种类多种多样，为了达到可以同时学习多个统计的累积和。上图中是，如何从输入图像怎么到最后的卷积，生成的响应map。首先用学习好的卷积和对图像进行扫描，然后每一个卷积和会生成一个扫描的响应图，我们叫response map，或者叫feature map。如果有多个卷积和，就有多个feature map。也就说从一个最开始的输入图像（RGB三个通道）可以得到256个通道的feature map，因为有256个卷积和，每个卷积和代表一种统计抽象的方式。在卷积神经网络中，除了卷积层，还有一种叫池化的操作。池化操作在统计上的概念更明确，就是一个对一个小区域内求平均值或者求最大值的统计操作。带来的结果是，如果之前我输入有两个通道的，或者256通道的卷积的响应feature map，每一个feature map都经过一个求最大的一个池化层，会得到一个比原来feature map更小的256的feature map。在上面这个例子里，池化层对每一个2X2的区域求最大值，然后把最大值赋给生成的feature map的对应位置。如果输入图像是100×100的话，那输出图像就会变成50×50，feature map变成了一半。同时保留的信息是原来2X2区域里面最大的信息。操作的实例：LeNet网络 Le顾名思义就是指人工智能领域的大牛Lecun。这个网络是深度学习网络的最初原型，因为之前的网络都比较浅，它较深的。LeNet在98年就发明出来了，当时Lecun在AT&T的实验室，他用这一网络进行字母识别，达到了非常好的效果。怎么构成呢？输入图像是32×32的灰度图，第一层经过了一组卷积和，生成了6个28X28的feature map，然后经过一个池化层，得到得到6个14X14的feature map，然后再经过一个卷积层，生成了16个10X10的卷积层，再经过池化层生成16个5×5的feature map。从最后16个5X5的feature map开始，经过了3个全连接层，达到最后的输出，输出就是标签空间的输出。由于设计的是只要对0到9进行识别，所以输出空间是10，如果要对10个数字再加上26个大小字母进行识别的话，输出空间就是62。62维向量里，如果某一个维度上的值最大，它对应的那个字母和数字就是就是预测结果。压在骆驼身上的最后一根稻草 从98年到本世纪初，深度学习兴盛起来用了15年，但当时成果泛善可陈，一度被边缘化。到2012年，深度学习算法在部分领域取得不错的成绩，而压在骆驼身上最后一根稻草就是AlexNet。AlexNet由多伦多大学几个科学家开发，在ImageNet比赛上做到了非常好的效果。当时AlexNet识别效果超过了所有浅层的方法。此后，大家认识到深度学习的时代终于来了，并有人用它做其它的应用，同时也有些人开始开发新的网络结构。其实AlexNet的结构也很简单，只是LeNet的放大版。输入是一个224X224的图片，是经过了若干个卷积层，若干个池化层，最后连接了两个全连接层，达到了最后的标签空间。去年，有些人研究出来怎么样可视化深度学习出来的特征。那么，AlexNet学习出的特征是什么样子？在第一层，都是一些填充的块状物和边界等特征；中间的层开始学习一些纹理特征；更高接近分类器的层级，则可以明显看到的物体形状的特征。最后的一层，即分类层，完全是物体的不同的姿态，根据不同的物体展现出不同姿态的特征了。可以说，不论是对人脸，车辆，大象或椅子进行识别，最开始学到的东西都是边缘，继而就是物体的部分，然后在更高层层级才能抽象到物体的整体。整个卷积神经网络在模拟人的抽象和迭代的过程。为什么时隔20年卷土重来？ 我们不禁要问：似乎卷积神经网络设计也不是很复杂，98年就已经有一个比较像样的雏形了。自由换算法和理论证明也没有太多进展。那为什么时隔20年，卷积神经网络才能卷土重来，占领主流？这一问题与卷积神经网络本身的技术关系不太大，我个人认为与其他一些客观因素有关。首先，卷积神经网络的深度太浅的话，识别能力往往不如一般的浅层模型，比如SVM或者boosting。但如果做得很深，就需要大量数据进行训练，否则机器学习中的过拟合将不可避免。而2006及2007年开始，正好是互联网开始大量产生各种各样的图片数据的时候。另外一个条件是运算能力。卷积神经网络对计算机的运算要求比较高，需要大量重复可并行化的计算，在当时CPU只有单核且运算能力比较低的情况下，不可能进行个很深的卷积神经网络的训练。随着GPU计算能力的增长，卷积神经网络结合大数据的训练才成为可能。最后一点就是人和。卷积神经网络有一批一直在坚持的科学家（如Lecun）才没有被沉默，才没有被海量的浅层方法淹没。然后最后终于看到卷积神经网络占领主流的曙光。深度学习在视觉上的应用 计算机视觉中比较成功的深度学习的应用，包括人脸识别，图像问答，物体检测，物体跟踪。人脸识别 这里说人脸识别中的人脸比对，即得到一张人脸，与数据库里的人脸进行比对；或同时给两张人脸，判断是不是同一个人。这方面比较超前的是汤晓鸥教授，他们提出的DeepID算法在LWF上做得比较好。他们也是用卷积神经网络，但在做比对时，两张人脸分别提取了不同位置特征，然后再进行互相比对，得到最后的比对结果。最新的DeepID-3算法，在LWF达到了99.53%准确度，与肉眼识别结果相差无几。图片问答问题 这是2014年左右兴起的课题，即给张图片同时问个问题，然后让计算机回答。比如有一个办公室靠海的图片，然后问“桌子后面有什么”，神经网络输出应该是“椅子和窗户”。这一应用引入了LSTM网络，这是一个专门设计出来具有一定记忆能力的神经单元。特点是，会把某一个时刻的输出当作下一个时刻的输入。可以认为它比较适合语言等，有时间序列关系的场景。因为我们在读一篇文章和句子的时候，对句子后面的理解是基于前面对词语的记忆。图像问答问题是基于卷积神经网络和LSTM单元的结合，来实现图像问答。LSTM输出就应该是想要的答案，而输入的就是上一个时刻的输入，以及图像的特征，及问句的每个词语。物体检测问题 Region CNN 深度学习在物体检测方面也取得了非常好的成果。2014年的Region CNN算法，基本思想是首先用一个非深度的方法，在图像中提取可能是物体的图形块，然后深度学习算法根据这些图像块，判断属性和一个具体物体的位置。为什么要用非深度的方法先提取可能的图像块？因为在做物体检测的时候，如果你用扫描窗的方法进行物体监测，要考虑到扫描窗大小的不一样，长宽比和位置不一样，如果每一个图像块都要过一遍深度网络的话，这种时间是你无法接受的。所以用了一个折中的方法，叫Selective Search。先把完全不可能是物体的图像块去除，只剩2000左右的图像块放到深度网络里面判断。那么取得的成绩是AP是58.5，比以往几乎翻了一倍。有一点不尽如人意的是，region CNN的速度非常慢，需要10到45秒处理一张图片。Faster R-CNN方法 而且我在去年NIPS上，我们看到的有Faster R-CNN方法，一个超级加速版R-CNN方法。它的速度达到了每秒七帧，即一秒钟可以处理七张图片。技巧在于，不是用图像块来判断是物体还是背景，而把整张图像一起扔进深度网络里，让深度网络自行判断哪里有物体，物体的方块在哪里，种类是什么？经过深度网络运算的次数从原来的2000次降到一次，速度大大提高了。Faster R-CNN提出了让深度学习自己生成可能的物体块，再用同样深度网络来判断物体块是否是背景？同时进行分类，还要把边界和给估计出来。Faster R-CNN可以做到又快又好，在VOC2007上检测AP达到73.2，速度也提高了两三百倍。YOLO 去年FACEBOOK提出来的YOLO网络，也是进行物体检测，最快达到每秒钟155帧，达到了完全实时。它让一整张图像进入到神经网络，让神经网络自己判断这物体可能在哪里，可能是什么。但它缩减了可能图像块的个数，从原来Faster R-CNN的2000多个缩减缩减到了98个。同时取消了Faster R-CNN里面的RPN结构，代替Selective Search结构。YOLO里面没有RPN这一步，而是直接预测物体的种类和位置。YOLO的代价就是精度下降，在155帧的速度下精度只有52.7，45帧每秒时的精度是63.4。SSD在arXiv上出现的最新算法叫Single Shot MultiBox Detector，即SSD。它是YOLO的超级改进版，吸取了YOLO的精度下降的教训，同时保留速度快的特点。它能达到58帧每秒，精度有72.1。速度超过Faster R-CNN 有8倍，但达到类似的精度。物体跟踪 所谓跟踪，就是在视频里面第一帧时锁定感兴趣的物体，让计算机跟着走，不管怎么旋转晃动，甚至躲在树丛后面也要跟踪。深度学习对跟踪问题有很显著的效果。DeepTrack算法是我在澳大利亚信息科技研究院时和同事提出的，是第一在线用深度学习进行跟踪的文章，当时超过了其它所有的浅层算法。今年有越来越多深度学习跟踪算法提出。去年十二月ICCV 2015上面，马超提出的Hierarchical Convolutional Feature算法，在数据上达到最新的记录。它不是在线更新一个深度学习网络，而是用一个大网络进行预训练，然后让大网络知道什么是物体什么不是物体。将大网络放在跟踪视频上面，然后再分析网络在视频上产生的不同特征，用比较成熟的浅层跟踪算法来进行跟踪，这样利用了深度学习特征学习比较好的好处，同时又利用了浅层方法速度较快的优点。效果是每秒钟10帧，同时精度破了记录。最新的跟踪成果是基于Hierarchical Convolutional Feature，由一个韩国的科研组提出的MDnet。它集合了前面两种深度算法的集大成，首先离线的时候有学习，学习的不是一般的物体检测，也不是ImageNet，学习的是跟踪视频，然后在学习视频结束后，在真正在使用网络的时候更新网络的一部分。这样既在离线的时候得到了大量的训练，在线的时候又能够很灵活改变自己的网络。基于嵌入式系统的深度学习 回到ADAS问题（慧眼科技的主业），它完全可以用深度学习算法，但对硬件平台有比较高的要求。在汽车上不太可能把一台电脑放上去，因为功率是个问题，很难被市场所接受。现在的深度学习计算主要是在云端进行，前端拍摄照片，传给后端的云平台处理。但对于ADAS而言，无法接受长时间的数据传输的，或许发生事故后，云端的数据还没传回来。那是否可以考虑NVIDIA推出的嵌入式平台？NVIDIA推出的嵌入式平台，其运算能力远远强过了所有主流的嵌入式平台，运算能力接近主流的顶级CPU，如台式机的i7。那么慧眼科技在做工作就是要使得深度学习算法，在嵌入式平台有限的资源情况下能够达到实时效果，而且精度几乎没有减少。具体做法是，首先对网络进行缩减，可能是对网络的结构缩减，由于识别场景不同，也要进行相应的功能性缩减；另外要用最快的深度检测算法，结合最快的深度跟踪算法，同时自己研发出一些场景分析算法。三者结合在一起，目的是减少运算量，减少检测空间的大小。在这种情况下，在有限资源上实现了使用深度学习算法，但精度减少的非常少。"}
{"content2":"今日CS.CV计算机视觉论文速览Tue, 5 Mar 2019Totally 63 papersInteresting:📚STEFANN,基于字体自适应网络实现场景中的文字编辑修改。目前的场景文字识别很成功,按时对于场景中文字修改的工作还很少。这篇文章对于照片中文字进行自适应修改，不仅能够修复图像中的文字信息，同时可以得到戏剧性的效果。研究人员首先聚焦于如何生成不违和的文字，包括字体和颜色等。提出了一个多输入的字体特征生成器，并将原图的颜色迁移到目标图像上去。随后将生成的文字放置到原图的对应位置，并进行视觉连续性处理。(from Indian Statistical Institute Kolkata) CVPR模型的架构如下，包含了字体生成器和颜色迁移器组成：一些有趣的效果：数据集：ICDAR📚PuVAE基于变分自编码器提纯对抗样本，提出了一种利用变分自编码器提出对抗样本，降低对抗噪声的模型。为了防御深度学习中对抗样本的影响，研究人员提出了一种基于变分自编码器提纯对抗样本的方法。通过将对抗样本投影到流型空间的不同类别上，来估计和消除对抗扰动。实验表面这种方法性能强劲并比普通DefenseGan快130倍。（首尔国立大学）模型结构示例图：训练和推理过程示例图：推理过程的示意图：与类似方法的比较：📚SRNTT基于迁移学习的图像超分辨，单图像超分辨在近年来获得了较大的发展，但对于参考图片与目标图片不相似的情况下生成的超分辨率质量较差。研究人员提出了一种可以将高分辨图像中的细节纹理信息用于提高低分辨率图像分辨率的迁移学习方法，利用自然纹理迁移的技术实现了图像超分辨。利用多级神经空间匹配来代替像素间的配准，提高了模型利用语义相似片层的能力，并用优雅的方法提高了超分辨的能力。研究还建立了RefSR数据集，包含了低分辨图像和一系列层级相似性的配对图像。（from adobe）一些结果：模型的架构如下图所示，将多层级的特征进行了交换，并将参考图像下采样得到与低分辨图频谱一致的约束：一些相似方法的对比：Implementation of SR algorithms in comparison:SRCNN, SelfEx, SCN, DRCN, LapSRN, MDSR, ENet, SRGAN, CrossNet,code:https://github.com/ZZUTK/SRNTT*****author:https://research.adobe.com/person/zhifei-zhang/ http://web.eecs.utk.edu/~zzhang61/📚提出了一种基于图像修复和面部先验的方法，用于移除面部照片中的玻璃反光，研究了透射、反射、非投射式反射等情况下的反射去除。（from 南洋理工）模型的框架分为四个部分：研究人员还构建了自己的面部反射数据集：一些最终的结果：作者研究多年反射去除：https://wanrenjie.github.io/📚手部位姿识别综述,文章中解释了手部位姿估计问题，介绍了一些主要的解决办法，特别是基于RGB和深度之间的区别。此外介绍了每个子领域的重要论文及其优缺点，随后给出了领域中的数据集。（from印第安纳大学）不同种类的关键点：基于颜色和置信图的方式：一些主要的处理模型：现有的著名数据集：Daily Computer Vision Papers[1] Title: VideoFlow: A Flow-Based Generative Model for VideoAuthors:Manoj Kumar, Mohammad Babaeizadeh, Dumitru Erhan, Chelsea Finn, Sergey Levine, Laurent Dinh, Durk Kingma[2] Title: An Adversarial Super-Resolution Remedy for Radar Design Trade-offsAuthors:Sherif Abdulatif, Karim Armanious, Fady Aziz, Urs Schneider, Bin Yang[3] Title: Reduced Focal Loss: 1st Place Solution to xView object detection in Satellite ImageryAuthors:Nikolay Sergievskiy, Alexander Ponamarev[4] Title: Joint segmentation and classification of retinal arteries/veins from fundus imagesAuthors:Fantin Girard, Conrad Kavalec, Farida Cheriet[5] Title: Semi-Supervised Brain Lesion Segmentation with an Adapted Mean Teacher ModelAuthors:Wenhui Cui, Yanlin Liu, Yuxing Li, Menghao Guo, Yiming Li, Xiuli Li, Tianle Wang, Xiangzhu Zeng, Chuyang Ye[6] Title: Understanding the Mechanism of Deep Learning Framework for Lesion Detection in Pathological Images with Breast CancerAuthors:Wei-Wen Hsu, Chung-Hao Chen, Chang Hoa, Yu-Ling Hou, Xiang Gao, Yun Shao, Xueli Zhang, Jingjing Wang, Tao He, Yanghong Tai[7] Title: Unsupervised Domain Adaptation Learning Algorithm for RGB-D Staircase RecognitionAuthors:Wang Jing, Zhang Kuangen[8] Title: Collaborative Spatio-temporal Feature Learning for Video Action RecognitionAuthors:Chao Li, Qiaoyong Zhong, Di Xie, Shiliang Pu[9] Title: STEFANN: Scene Text Editor using Font Adaptive Neural NetworkAuthors:Prasun Roy, Saumik Bhattacharya, Subhankar Ghosh, Umapada Pal[10] Title: PanopticFusion: Online Volumetric Semantic Mapping at the Level of Stuff and ThingsAuthors:Gaku Narita, Takashi Seno, Tomoya Ishikawa, Yohsuke Kaji[11] Title: Zero-Shot Task TransferAuthors:Arghya Pal, Vineeth N Balasubramanian[12] Title: Automatic microscopic cell counting by use of deeply-supervised density regression modelAuthors:Shenghua He, Kyaw Thu Minn, Lilianna Solnica-Krezel, Mark Anastasio, Hua Li[13] Title: Unsupervised Cross-spectral Stereo Matching by Learning to SynthesizeAuthors:Mingyang Liang, Xiaoyang Guo, Hongsheng Li, Xiaogang Wang, You Song[14] Title: COMIC: Towards A Compact Image Captioning Model with AttentionAuthors:Jia Huei Tan, Chee Seng Chan, Joon Huang Chuah[15] Title: Incremental Visual-Inertial 3D Mesh Generation with Structural RegularitiesAuthors:Antoni Rosinol, Torsten Sattler, Marc Pollefeys, Luca Carlone[16] Title: Spatiotemporal Pyramid Network for Video Action RecognitionAuthors:Yunbo Wang, Mingsheng Long, Jianmin Wang, Philip S. Yu[17] Title: Active Authentication using an Autoencoder regularized CNN-based One-Class ClassifierAuthors:Poojan Oza, Vishal M. Patel[18] Title: A Kernelized Manifold Mapping to Diminish the Effect of Adversarial PerturbationsAuthors:Saeid Asgari Taghanaki, Kumar Abhishek, Shekoofeh Azizi, Ghassan Hamarneh[19] Title: Hand Pose Estimation: A SurveyAuthors:Bardia Doosti[20] Title: Self-Supervised Learning of Face Representations for Video Face ClusteringAuthors:Vivek Sharma, Makarand Tapaswi, M.Saquib Sarfraz, Rainer Stiefelhagen[21] Title: X-Section: Cross-section Prediction for Enhanced RGBD FusionAuthors:Andrea Nicastro, Ronald Clark, Stefan Leutenegger[22] Title: Matching Thermal to Visible Face Images Using a Semantic-Guided Generative Adversarial NetworkAuthors:Cunjian Chen, Arun Ross[23] Title: Probability Map Guided Bi-directional Recurrent UNet for Pancreas SegmentationAuthors:Jun Li, Xiaozhu Lin, Hui Che, Hao Li, Xiaohua Qian[24] Title: Unsupervised Bi-directional Flow-based Video Generation from one SnapshotAuthors:Lu Sheng, Junting Pan, Jiaming Guo, Jing Shao, Xiaogang Wang, Chen Change Loy[25] Title: Ground Plane based Absolute Scale Estimation for Monocular Visual OdometryAuthors:Dingfu Zhou, Yuchao Dai, Hongdong Li[26] Title: MILDNet: A Lightweight Single Scaled Deep Ranking ArchitectureAuthors:Anirudha Vishvakarma[27] Title: 3D convolutional neural network for abdominal aortic aneurysm segmentationAuthors:Karen López-Linares, Inmaculada García, Ainhoa García-Familiar, Iván Macía, Miguel A. González Ballester[28] Title: Meta-SR: A Magnification-Arbitrary Network for Super-ResolutionAuthors:Xuecai Hu, Haoyuan Mu, Xiangyu Zhang, Zilei Wang, Jian Sun, Tieniu Tan[29] Title: Face Image Reflection RemovalAuthors:Renjie Wan, Boxin Shi, Haoliang Li, Ling-Yu Duan, Alex C. Kot[30] Title: Less is More: Learning Highlight Detection from Video DurationAuthors:Bo Xiong, Yannis Kalantidis, Deepti Ghadiyaram, Kristen Grauman[31] Title: Recognition of Multiple Food Items in a Single Photo for Use in a Buffet-Style RestaurantAuthors:Masashi Anzawa, Sosuke Amano, Yoko Yamakata, Keiko Motonaga, Akiko Kamei, Kiyoharu Aizawa[32] Title: CAD-Net: A Context-Aware Detection Network for Objects in Remote Sensing ImageryAuthors:Gongjie Zhang, Shijian Lu, Wei Zhang[33] Title: Crowd Counting and Density Estimation by Trellis Encoder-Decoder NetworkAuthors:Xiaolong Jiang, Zehao Xiao, Baochang Zhang, Xiantong Zhen, Xianbin Cao, David Doermann, Ling Shao[34] Title: Improving Referring Expression Grounding with Cross-modal Attention-guided ErasingAuthors:Xihui Liu, Zihao Wang, Jing Shao, Xiaogang Wang, Hongsheng Li[35] Title: Image Super-Resolution by Neural Texture TransferAuthors:Zhifei Zhang, Zhaowen Wang, Zhe Lin, Hairong Qi[36] Title: Pancreas Segmentation via Spatial Context based U-net and Bidirectional LSTMAuthors:Hao Li, Jun Li, Xiaozhu Lin, Xiaohua Qian[37] Title: 3D Hand Shape and Pose Estimation from a Single RGB ImageAuthors:Liuhao Ge, Zhou Ren, Yuncheng Li, Zehao Xue, Yingying Wang, Jianfei Cai, Junsong Yuan[38] Title: Let’s Transfer Transformations of Shared Semantic RepresentationsAuthors:Nam Vo, Lu Jiang, James Hays[39] Title: AIRD: Adversarial Learning Framework for Image Repurposing DetectionAuthors:Ayush Jaiswal, Yue Wu, Wael AbdAlmageed, Iacopo Masi, Premkumar Natarajan[40] Title: Spatio-Temporal Vegetation Pixel Classification By Using Convolutional NetworksAuthors:Keiller Nogueira, Jefersson A. dos Santos, Nathalia Menini, Thiago S. F. Silva, Leonor Patricia C. Morellato, Ricardo da S. Torres[41] Title: Extreme Channel Prior Embedded Network for Dynamic Scene DeblurringAuthors:Jianrui Cai, Wangmeng Zuo, Lei Zhang[42] Title: PartNet: A Recursive Part Decomposition Network for Fine-grained and Hierarchical Shape SegmentationAuthors:Fenggen Yu, Kun Liu, Yan Zhang, Chenyang Zhu, Kai Xu[43] Title: Deep Optimization model for Screen Content Image Quality Assessment using Neural NetworksAuthors:Xuhao Jiang, Liquan Shen, Guorui Feng, Liangwei Yu, Ping An[44] Title: Fine-Grained Semantic Segmentation of Motion Capture Data using Dilated Temporal Fully-Convolutional NetworksAuthors:Noshaba Cheema, Somayeh Hosseini, Janis Sprenger, Erik Herrmann, Han Du, Klaus Fischer, Philipp Slusallek[45] Title: OmniDRL: Robust Pedestrian Detection using Deep Reinforcement Learning on Omnidirectional CamerasAuthors:G. Dias Pais, Tiago J. Dias, Jacinto C. Nascimento, Pedro Miraldo[46] Title: Quaternion Convolutional Neural NetworksAuthors:Xuanyu Zhu, Yi Xu, Hongteng Xu, Changjian Chen[47] Title: Feature Selective Anchor-Free Module for Single-Shot Object DetectionAuthors:Chenchen Zhu, Yihui He, Marios Savvides[48] Title: RGBD Based Dimensional Decomposition Residual Network for 3D Semantic Scene CompletionAuthors:Jie Li, Yu Liu, Dong Gong, Qinfeng Shi, Xia Yuan, Chunxia Zhao, Ian Reid[49] Title: Unsupervised Traffic Accident Detection in First-Person VideosAuthors:Yu Yao, Mingze Xu, Yuchen Wang, David J. Crandall, Ella M. Atkins[50] Title: Straight to the point: reinforcement learning for user guidance in ultrasoundAuthors:Fausto Milletari, Vighnesh Birodkar, Michal Sofka[51] Title: Unsupervised Tracklet Person Re-IdentificationAuthors:Minxian Li, Xiatian Zhu, Shaogang Gong[52] Title: Learning where to look: Semantic-Guided Multi-Attention Localization for Zero-Shot LearningAuthors:Yizhe Zhu, Jianwen Xie, Zhiqiang Tang, Xi Peng, Ahmed Elgammal[53] Title: PEA265: Perceptual Assessment of Video Compression ArtifactsAuthors:Liqun Lin, Shiqi Yu, Tiesong Zhao, Member, IEEE, Zhou Wang, Fellow, IEEE[54] Title: Complement Objective TrainingAuthors:Hao-Yun Chen, Pei-Hsin Wang, Chun-Hao Liu, Shih-Chieh Chang, Jia-Yu Pan, Yu-Ting Chen, Wei Wei, Da-Cheng Juan[55] Title: Accelerating Training of Deep Neural Networks with a Standardization LossAuthors:Jasmine Collins, Johannes Balle, Jonathon Shlens[56] Title: End-to-end Driving Deploying through Uncertainty-Aware Imitation Learning and Stochastic Visual Domain AdaptationAuthors:Lei Tai, Peng Yun, Yuying Chen, Congcong Liu, Haoyang Ye, Ming Liu[57] Title: Keyframe-based Direct Thermal-Inertial OdometryAuthors:Shehryar Khattak, Christos Papachristos, Kostas Alexis[58] Title: Marker based Thermal-Inertial Localization for Aerial Robots in Obscurant Filled EnvironmentsAuthors:Shehryar Khattak, Christos Papachristos, Kostas Alexis[59] Title: Time-Delay Momentum: A Regularization Perspective on the Convergence and Generalization of Stochastic Momentum for Deep LearningAuthors:Ziming Zhang, Wenju Xu, Alan Sullivan[60] Title: Equilibrated Recurrent Neural Network: Neuronal Time-Delayed Self-Feedback Improves Accuracy and StabilityAuthors:Ziming Zhang, Anil Kag, Alan Sullivan, Venkatesh Saligrama[61] Title: Strong homotopy of digitally continuous functionsAuthors:P. Christopher Staecker[62] Title: PuVAE: A Variational Autoencoder to Purify Adversarial ExamplesAuthors:Uiwon Hwang, Jaewoo Park, Hyemi Jang, Sungroh Yoon, Nam Ik Cho[63] Title: Sparse Depth Enhanced Direct Thermal-infrared SLAM Beyond the Visible SpectrumAuthors:Young-Sik Shin, Ayoung KimPapers from arxiv.org更多精彩请移步主页pic from pixels.com"}
{"content2":"自学进度数学计算机基础ML算法工程实践微积分Python基础K-NN基于K-NN的鸢尾花分类(201809)线性代数Java SE / Java EESVM基于TF-IDF与SVM的电子邮件作者识别方法(201712小美赛-数模B题)概率论与数理统计数据库Naive Bayes基于朴素贝叶斯的法律咨询文本分类方法(201708)数据结构Word2Vec基于余弦相似度原理的人脸相似度识别(201704认证杯-数模-C题)操作系统 / LinuxTextRank目录一 定义二 数学三 CS基础四 ML算法五 应用场景六 推荐文献七 精品书单人工智能研究方法研究领域涉及学科微积分语言(Python/Java)典型算法分类K-NNC4.5CARTNaive Bayes(朴素贝叶斯)聚类K-Means统计学习SVMEM关联学习Apriori链接挖掘PageRank集装与推进Adaboost实际应用全局导览学习路径网络资源API机器学习线性代数数据结构学科与学科间关系概率论与数理统计计算机算法思想计算机典型算法一 定义资料来源于维基百科、西瓜书（周志华教授 《机器学习》）、百度百科与网络人工智能“智能主体（intelligent agent）的研究与设计”，智能主体是指一个可以观察周遭环境并作出行动以达致目标的系统。约翰·麦卡锡于1955年的定义是“制造智能机器的科学与工程。AI的核心问题包括建构能够跟人类似甚至超越的推理、知识、规划、学习、交流、感知、移动和操作物体的能力等。强人工智能目前仍然是该领域的长远目标。目前有大量的工具应用了人工智能，其中包括搜索和数学优化、逻辑推演。而基于仿生学、认知心理学，以及基于概率论和经济学的算法等等也在逐步探索当中 ----维基百科计算机科学的一个分支。它是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。 ----百度百科人工智能的定义可以分为两部分，即“人工”和“智能”。“人工”比较好理解，争议性也不大。有时我们会要考虑什么是人力所能及制造的，或者人自身的智能程度有没有高到可以创造人工智能的地步，等等。但总的来说，“人工系统”就是通常意义下的人工系统。如今没有统一的原理或范式指导人工智能研究。来源：百度云智学院研究方法控制论/大脑模拟控制论 / 计算神经科学20世纪40年代到50年代，许多研究者探索神经病学，信息理论及控制论之间的联系。这些研究者还经常在普林斯顿大学和英国的RATIO CLUB举行技术协会会议.直到1960， 大部分人已经放弃这个方法，尽管在80年代再次提出这些原理。符号处理GOFAI当20世纪50年代，数字计算机研制成功，研究者开始探索人类智能是否能简化成符号处理。研究主要集中在卡内基梅隆大学， 斯坦福大学和麻省理工学院，而各自有独立的研究风格。基于知识大约在1970年出现大容量内存计算机，研究者分别以三个方法开始把知识构造成应用软件。这场“知识革命”促成专家系统的开发与计划，这是第一个成功的人工智能软件形式。“知识革命”同时让人们意识到许多简单的人工智能软件可能需要大量的知识。GOFAI 泛指用最原始的人工智能的逻辑方法解决小领域的问题， 例如棋类游戏的算法。人工智能是否可以使用高级符号表达，如词和想法？还是需要“子符号”的处理？JOHN HAUGELAND提出了GOFAI(出色的老式人工智能)的概念，也提议人工智能应归类为SYNTHETIC INTELLIGENCE，[29]这个概念后来被某些非GOFAI研究者采纳。子符号法计算智能学科80年代符号人工智能停滞不前，很多人认为符号系统永远不可能模仿人类所有的认知过程，特别是感知，机器人，机器学习和模式识别。很多研究者开始关注子符号方法解决特定的人工智能问题。自下而上， 接口AGENT，嵌入环境（机器人），行为主义，新式AI机器人领域相关的研究者，如RODNEY BROOKS，否定符号人工智能而专注于机器人移动和求生等基本的工程问题。统计学法90年代，人工智能研究发展出复杂的数学工具来解决特定的分支问题。这些工具是真正的科学方法，即这些方法的结果是可测量的和可验证的，同时也是人工智能成功的原因。共用的数学语言也允许已有学科的合作（如数学，经济或运筹学）。有人批评这些技术太专注于特定的问题，而没有考虑长远的强人工智能目标。集成方法90年代智能AGENT范式被广泛接受。智能AGENT范式智能AGENT是一个会感知环境并作出行动以达致目标的系统。最简单的智能AGENT是那些可以解决特定问题的程序。更复杂的AGENT包括人类和人类组织（如公司）。这些范式可以让研究者研究单独的问题和找出有用且可验证的方案，而不需考虑单一的方法。一个解决特定问题的AGENT可以使用任何可行的方法-一些AGENT用符号方法和逻辑方法，一些则是子符号神经网络或其他新的方法。范式同时也给研究者提供一个与其他领域沟通的共同语言--如决策论和经济学（也使用ABSTRACT AGENTS的概念）。研究领域演绎、推理和解决问题知识表示法知识表示 / 常识知识库规划学习机器学习运动和控制机器人知觉机器感知语言识别图像识别自然语言处理:自然语言处理探讨如何处理及运用自然语言，自然语言认知则是指让电脑“懂”人类的语言。自然语言生成系统把计算机数据转化为自然语言。自然语言理解系统把自然语言转化为计算机程序更易于处理的形式。社交情感计算创造力计算机创造力相关领域研究的包括了人工直觉和人工想像。伦理管理经济冲击涉及学科哲学和认知科学数学神经生理学心理学计算机科学信息论控制论不定性论机器学习机器学习的主要目的是为了让机器从用户和输入数据等处获得知识，从而让机器自动地去判断和输出相应的结果。这一方法可以帮助解决更多问题、减少错误，提高解决问题的效率。对于人工智能来说，机器学习从一开始就很重要。1956年，在最初的达特茅斯夏季会议上，雷蒙德索洛莫诺夫写了一篇关于不监视的概率性机器学习：一个归纳推理的机器。机器学习的方法各种各样，主要分为监督学习和非监督学习两大类。监督学习指事先给定机器一些训练样本并且告诉样本的类别，然后根据这些样本的类别进行训练，提取出这些样本的共同属性或者训练一个分类器，等新来一个样本，则通过训练得到的共同属性或者分类器进行判断该样本的类别。监督学习根据输出结果的离散性和连续性，分为分类和回归两类。非监督学习是不给定训练样本，直接给定一些样本和一些规则，让机器自动根据一些规则进行分类。无论哪种学习方法都会进行误差分析，从而知道所提的方法在理论上是否误差有上限。未显式地编程，并赋予计算机智能。-------西瓜书(周志华教授)学科与学科间关系二 数学微积分线性代数概率论三 计算机基础语言[Python/Java/]PythonJava数据结构计算机算法思想分治减治计算机典型算法四 机器学习算法典型算法分类K-NNC4.5CARTNaive Bayes(朴素贝叶斯)聚类K-Means统计学习SVMEM关联学习Apriori链接挖掘PageRank集装与推进Adaboost机器学习常见十大经典算法五 应用场景机器学习已广泛应用于数据挖掘、计算机视觉、自然语言处理、生物特征识别、搜索引擎、医学诊断、检测信用卡欺诈、证券市场分析、DNA序列测序、语音和手写识别、战略游戏和机器人等领域。实际应用机器视觉指纹识别人脸识别视网膜识别虹膜识别掌纹识别专家系统自动规划智能搜索定理证明/推理博弈自动程序设计智能控制机器人学语言和图像理解遗传编程...自然语言处理与机器视觉的应用场景小结2018百度-AI 开发者大会现场自然语言处理与法律领域的结合六 顶会/顶刊本部分内容完全摘抄自如下两份知乎回答，非常感谢。[1] 计算机科学各个方向有哪些顶刊和顶会？ - 李昕 - 知乎[2] 计算机科学各个方向有哪些顶刊和顶会？ - 硕鼠酱 - 知乎6.0 人工智能1 AAAI2 IJCAI6.1 自然语言处理【会议】1 ACL (Annual Meeting of the Association for Computational Linguistics)> AACL(亚洲分会)2 NAACL (Annual Conference of the North American Chapter of Association for Computational Linguistics)3 EMNLP (Empirical Methods in Natural Language Processing)4 EACL (Annual Conference of the European Chapter of Association for Computational Linguistics)5 COLING (International Conference on Computational Linguistics)6 SIGKDD (ACM SIGKDD Conference on Knowledge Discovery and Data Mining)7 SIGIR (ACM SIGIR Conference on Research and Development in Information Retrieval)8 WWW (International World Wide Web Conference)【期刊】1 CL (Computational Linguistics)2 TACL (Transactions of the Association for Computational Linguistics)6.2 机器学习与数据挖掘 / Machine learning & data mining1 ICML2 KDD3 NIPS6.3 WEB 信息检索1 SIGIR2 WWW6.4 计算机视觉1 CVPR2 ECCV3 ICCV6.5 计算机体系结构1 ASPLOS2 ISCA3 MICRO七 推荐资源全局导览[1] 机器学习算法[2] 原机器学习（一）：统计学习问题概述[3] 机器学习学习路径博主评注：每个人的专业背景(数理统计、计算机、非数理统计非计算机等)、学历背景、自学能力、学习动机（升学科研、就业、仅仅兴趣等）均不同，每个人的学习路径不一定是一致的，应当自寻一条适合自己的道路。仅仅个人之见。[1] 机器学习 - 概述/资源 - Github网络学院(系统性学)[0] 吴恩达/Coursera-机器学习[1] 吴恩达/斯坦福大学-机器学习[2] Google-机器学习速成课程[3] Google-Tesorflow[4] Tensorflow中文社区-MNIST机器学习入门[5] 数据挖掘十大算法详解[6] 吴恩达：深度学习与神经网络 - 网易云课程(免费/2019最新/优质课程)API[1] 百度[2] 阿里云-机器学习PAI快速入门与业务实践八 精品书单[1]《统计学习方法》李航[2] 《机器学习》周志华[3]《Machine Learning》Tom M. MitchellDeep Learning  实战之 word2vec  -  Deep Learning 圈"}
{"content2":"Outstanding Researchers Homepages In Computer Vision 1).Google Research --http://research.google.com/index.html 2).Stanford 大学 vision 实验室 --http://vision.stanford.edu/research.html 3).加州大学伯克利分校 CV 小组 --http://www.eecs.berkeley.edu/Research/Projects/CS/vision/ 4).南加州大学 CV 实验室 --http://iris.usc.edu/USC-Computer-Vision.html 5).卡内基梅隆大学 CV 主页 --http://www-2.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.html 6).微软亚洲研究院计算机视觉研究组 --http://research.microsoft.com/en-us/groups/vc/ 7).微软剑桥研究院 ML 与 CV 研究组 --http://research.microsoft.com/en-us/groups/mlp/default.aspx 8).U.C. San Diego. computer vision --http://vision.ucsd.edu/content/home 9).CV Online --tp://homepages.inf.ed.ac.uk/rbf/CVonline/ 10).computer visionsoftware --http://peipa.essex.ac.uk/info/software.html 11).computer vision research groups --http://peipa.essex.ac.uk/info/groups.html 12).computer vision center --http://computervisioncentral.com/cvcnews 13).浙江大学图像技术研究与应用（ITRA）团队 --http://www.dvzju.com/ 14).顶级民用机器人研究小组 Porf.Gary 领导的 Willow Garage --http://www.willowgarage.com/ 15).上海交通大学图像处理与模式识别研究所 --http://www.pami.sjtu.edu.cn/ 16).中科院生物识别与安全技术研究中心 --http://www.cbsr.ia.ac.cn/china/index%20CH.asp 17).Active Vision Group (Oxford，特色是 SLAM，监视，导航) --http://www.robots.ox.ac.uk/ActiveVision/index.html 18).微软亚洲研究院--Harry Shum, Jian Sun, Steven Lin, Long Quan(兼职 HKUST） --http://research.microsoft.com/asia/ 19).瑞典隆德大学数学系视觉组 --http://www.maths.lth.se/matematiklth/personal/andersp/ 20).澳大利亚国立大学 --http://users.rsise.anu.edu.au/~hartley/ 21).美国北卡大学 --http://www.cs.unc.edu/~marc/ 22).法国 INRIA（由 Olivier.Faugeras 领衔的牛人众多） --http://www-sop.inria.fr/odyssee/team/ 23).Boston University:Image and Video Computing Research group --http://cs-www.bu.edu/groups/ivc/Home.html 24).University of California at Santa Barbara:Vision Research Lab --http://vision.ece.ucsb.edu/ 25).University of California at Berkeley:CV group --http://http.cs.berkeley.edu/projects/vision/vision_group.html 26).University of California at San Diego:ComputerVision&Robotics Research Laboratory --http://swiftlet.ucsd.edu/ 27).University of California at Irvine:ComputerVision laboratory --http://www.cvl.uci.edu/ 28).UCLA: Computer Vision --http://iris.usc.edu/USC-Computer-Vision.html 29).Caltech:Vision Group --http://www.vision.caltech.edu/ 30).Carnegie Mellon University:Vision and Autonomous Systems Center --http://vasc.ri.cmu.edu/ 31).UCLA:Vision Lab --http://www.vision.cs.ucla.edu/ 32).Colorado State University:Computer Vision Group --http://www.cs.colostate.edu/~vision/ 33).ColumbiaUniversity:Automated Vision Environment (CAVE) --http://www.cs.columbia.edu/CAVE 34).Cornell University:Robotics and Vision group --http://www.cs.cornell.edu/Info/Projects/csrvl/csrvl.html 35).Environmental Research Institute ofMichigan (ERIM) --http://www.erim.org/ 36).Harvard University:Robotics Laboratory --http://hrl.harvard.edu/ 37).University of Illinois at Chicago:CV and Robotics Laboratory --http://www.eecs.uic.edu/research/cvrl.htm 38).UIC(Same with UP):Vision Interfaces and Systems Laboratory (VISLab) --http://www.eecs.uic.edu/research/visl.htm 39).University of Georgia, Athens:Visual and Parallel Computing Laboratory 40).Yale University:Computer Vision Group --http://www.cs.yale.edu/homes/vision/ 41).Washington StateUniversity:Imaging Research laboratory --http://www.eecs.wsu.edu/~irl/ 42).University of Washington:Image Computing Systems Laboratory --http://icsl.ee.washington.edu/ 43).University of Virginia:Computer Vision Research --http://www.cs.virginia.edu/~vision/ 44).Stanford University:Vision and Imaging Science and Technology --http://white.stanford.edu/ 45).Stanford Research Institute International (SRI) --http://www.ai.sri.com/~radius/ 46).University of Southern California:Computer Vision --http://iris.usc.edu/USC-Computer-Vision.html 47).Penn StateUniversity:Computer Vision --http://vision.cse.psu.edu/ 48).NEC:Computer Vision and Image Processing --http://www.neci.nj.nec.com/homepages/vision/index.html 49).MIT:AI Laboratory Computer Vision group --http://www.ai.mit.edu/research/ 50).University of Maryland:Computer Vision Lab --http://www.cfar.umd.edu/cvl/ 51).国际计算机视觉研究组清单 --http://peipa.essex.ac.uk/info/groups.html 52).美国计算机视觉研究组清单 --http://peipa.essex.ac.uk/info/groups.html#USA 53).卡内基梅隆-资料-文章,演示程序、测试图像。人物 Tomasi，Kanad --http://www.cs.cmu.edu/~cil/vision.html 54).卡内基梅隆大学双目实验室 --http://vision.middlebury.edu/stereo/ 55).卡内基梅隆研究组 --http://www.cs.cmu.edu/~cil/v-groups.html 56).侧重医学方面的研究 --http://www.via.cornell.edu/ 57).斯坦福大学计算机系主页 --http://www-cs-students.stanford.edu/ 58).美国斯坦福大学人工智能机器人实验室(Vision/white.standford.edu) --http://ai.stanford.edu/ 59).德国的一个数字图像处理研究小组 --http://pandora.inf.uni-jena.de/p/e/index.html 60).德国波恩大学视觉和认识模型小组 --http://www-dbv.informatik.uni-bonn.de/ 61).瑞士戴尔莫尔感知人工智能研究所 --http://www.idiap.ch/ 62).CV Group--The University of Wisconsin --http://research.cs.wisc.edu/computer-vision/ 63).Machine Intelligence Laboratory :Cambridge University --http://mi-webserv.eng.cam.ac.uk/mi/ 注：引用的时候请注明出处，谢谢。"}
{"content2":"在我们学习的这个项目中，模型主要分为两种状态，即进行推断用的inference模式和进行训练用的training模式。所谓推断模式就是已经训练好的的模型，我们传入一张图片，网络将其分析结果计算出来的模式。本节我们从demo.ipynb入手，一窥已经训练好的Mask-RCNN模型如何根据一张输入图片进行推断，得到相关信息，即inference模式的工作原理。一、调用推断网络网络配置首先进行配置设定，设定项都被集成进class config中了，自建新的设定只要基础改class并更新属性即可，在demo中我们直接使用COCO的预训练模型所以使用其设置即可，但由于我们想检测单张图片，所以需要更新几个相关数目设定：# 父类继承了Config类，目的就是记录配置，并在其基础上添加了几个新的属性 class InferenceConfig(coco.CocoConfig): # Set batch size to 1 since we'll be running inference on # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU GPU_COUNT = 1 IMAGES_PER_GPU = 1 config = InferenceConfig() config.display()打印出配置如下：Configurations: BACKBONE resnet101 BACKBONE_STRIDES [4, 8, 16, 32, 64] BATCH_SIZE 1 BBOX_STD_DEV [ 0.1 0.1 0.2 0.2] COMPUTE_BACKBONE_SHAPE None DETECTION_MAX_INSTANCES 100 DETECTION_MIN_CONFIDENCE 0.7 DETECTION_NMS_THRESHOLD 0.3 FPN_CLASSIF_FC_LAYERS_SIZE 1024 GPU_COUNT 1 GRADIENT_CLIP_NORM 5.0 IMAGES_PER_GPU 1 IMAGE_CHANNEL_COUNT 3 IMAGE_MAX_DIM 1024 IMAGE_META_SIZE 93 IMAGE_MIN_DIM 800 IMAGE_MIN_SCALE 0 IMAGE_RESIZE_MODE square IMAGE_SHAPE [1024 1024 3] LEARNING_MOMENTUM 0.9 LEARNING_RATE 0.001 LOSS_WEIGHTS {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0} MASK_POOL_SIZE 14 MASK_SHAPE [28, 28] MAX_GT_INSTANCES 100 MEAN_PIXEL [ 123.7 116.8 103.9] MINI_MASK_SHAPE (56, 56) NAME coco NUM_CLASSES 81 POOL_SIZE 7 POST_NMS_ROIS_INFERENCE 1000 POST_NMS_ROIS_TRAINING 2000 PRE_NMS_LIMIT 6000 ROI_POSITIVE_RATIO 0.33 RPN_ANCHOR_RATIOS [0.5, 1, 2] RPN_ANCHOR_SCALES (32, 64, 128, 256, 512) RPN_ANCHOR_STRIDE 1 RPN_BBOX_STD_DEV [ 0.1 0.1 0.2 0.2] RPN_NMS_THRESHOLD 0.7 RPN_TRAIN_ANCHORS_PER_IMAGE 256 STEPS_PER_EPOCH 1000 TOP_DOWN_PYRAMID_SIZE 256 TRAIN_BN False TRAIN_ROIS_PER_IMAGE 200 USE_MINI_MASK True USE_RPN_ROIS True VALIDATION_STEPS 50 WEIGHT_DECAY 0.0001模型初始化首先初始化模型，然后载入预训练参数文件，在末尾我可视化了模型，不过真的太长了，所以注释掉了。在第一步初始化时就会根据mode参数的具体值建立计算图，本节介绍的推断网络就是在mode参数设定为\"inference\"时建立的计算网络。# Create model object in inference mode. model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config) # Load weights trained on MS-COCO model.load_weights(COCO_MODEL_PATH, by_name=True) # model.keras_model.summary()检测图片# Load a random image from the images folder file_names = next(os.walk(IMAGE_DIR))[2] # 只要是迭代器调用next方法获取值，学习了 image = skimage.io.imread(os.path.join(IMAGE_DIR, random.choice(file_names))) print(image.shape) # Run detection results = model.detect([image], verbose=1) # Visualize results r = results[0] visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], class_names, r['scores'])读取一张图片，调用model的detect方法，即可输出结果，最后使用辅助方法可视化结果：二、推断逻辑概览inference的前向逻辑如下图所示，我们简单的看一下其计算流程是怎样的，左上模块为以ResNet101为基础的FPN特征金字塔网络的特征提取逻辑，可以看到，作者并没有直接将up-down特征使用，而是又做了一次3*3卷积进行了进一步的特征融合。出来的各层FPN特征首先（各自独立地）进入了RPN处理层：根据锚框数目信息确定候选区域的分类（前景背景2分类）和回归结果。rpn_class：[batch, num_rois, 2]rpn_bbox：[batch, num_rois, (dy, dx, log(dh), log(dw))]有了众多的候选区域，我们将之送入Proposal筛选部分，首先根据前景得分排序进行初筛（配置会指定这一步保留多少候选框），然后为非极大值抑制做准备：用RPN的回归结果修正anchors，值得注意的是anchors都是归一化的这意味着修值之后还需要做检查以防越界，最后非极大值一致，删减的太多了的话就补上[0, 0, 0, 0]达到配置文件要求的数目(非极大值部分会造成同一个batch中不同图片的候选框数目不一致，但是tensor的维数不能参差不齐，所以要补零使得各张图片候选区域数目一致)rpn_rois：[IMAGES_PER_GPU, num_rois, (y1, x1, y2, x2)]根据候选区的实际大小（归一化候选区需要映射回原图大小）为候选区选择合适的RPN特征层，ROI Align处理（实际上就是抠出来进行双线性插值到指定大小），得到我们需要的众多等大子图对这些子图各自独立的进行分类/回归mrcnn_class_logits: [batch, num_rois, NUM_CLASSES] classifier logits (before softmax)mrcnn_class: [batch, num_rois, NUM_CLASSES] classifier probabilitiesmrcnn_bbox(deltas): [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))]在分类回归之后使用回归结果对候选框进行修正，然后重新进行FPN特征层选择和ROI Align特征提取，最后送入Mask网络，进行Mask生成。最后，我们希望网络输出下面的张量：# num_anchors, 每张图片上生成的锚框数量# num_rois, 每张图片上由锚框筛选出的推荐区数量，# # 由 POST_NMS_ROIS_TRAINING 或 POST_NMS_ROIS_INFERENCE 规定# num_detections, 每张图片上最终检测输出框，# # 由 DETECTION_MAX_INSTANCES 规定# detections, [batch, num_detections, (y1, x1, y2, x2, class_id, score)]# mrcnn_class, [batch, num_rois, NUM_CLASSES] classifier probabilities# mrcnn_bbox, [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))]# mrcnn_mask, [batch, num_detections, MASK_POOL_SIZE, MASK_POOL_SIZE, NUM_CLASSES]# rpn_rois, [batch, num_rois, (y1, x1, y2, x2, class_id, score)]# rpn_class, [batch, num_anchors, 2]# rpn_bbox [batch, num_anchors, 4]具体每种张量的意义我们会在源码分析中一一介绍。"}
{"content2":"以下链接是本人整理的关于计算机视觉（ComputerVision, CV）相关领域的网站链接，其中有CV牛人的主页，CV研究小组的主页，CV领域的paper,代码，CV领域的最新动态，国内的应用情况等等。打算从事这个行业或者刚入门的朋友可以多关注这些网站，多了解一些CV的具体应用。搞研究的朋友也可以从中了解到很多牛人的研究动态、招生情况等。总之，我认为，知识只有分享才能产生更大的价值，真诚希望下面的链接能对朋友们有所帮助。 （1）googleResearch； http://research.google.com/index.html （2）MIT博士，汤晓欧学生林达华； http://people.csail.mit.edu/dhlin/index.html （3）MIT博士后Douglas Lanman； http://web.media.mit.edu/~dlanman/ （4）opencv中文网站； http://www.opencv.org.cn/index.php/%E9%A6%96%E9%A1%B5 （5）Stanford大学vision实验室； http://vision.stanford.edu/research.html （6）Stanford大学博士崔靖宇； http://www.stanford.edu/~jycui/ （7）UCLA教授朱松纯； http://www.stat.ucla.edu/~sczhu/ （8）中国人工智能网； http://www.chinaai.org/ （9）中国视觉网； http://www.china-vision.net/ （10）中科院自动化所； http://www.ia.cas.cn/ （11）中科院自动化所李子青研究员； http://www.cbsr.ia.ac.cn/users/szli/ （12）中科院计算所山世光研究员； http://www.jdl.ac.cn/user/sgshan/ （13）人脸识别主页； http://www.face-rec.org/ （14）加州大学伯克利分校CV小组； http://www.eecs.berkeley.edu/Research/Projects/CS/vision/ （15）南加州大学CV实验室； http://iris.usc.edu/USC-Computer-Vision.html （16）卡内基梅隆大学CV主页； http://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.html （17）微软CV研究员Richard Szeliski；http://research.microsoft.com/en-us/um/people/szeliski/ （18）微软亚洲研究院计算机视觉研究组； http://research.microsoft.com/en-us/groups/vc/ （19）微软剑桥研究院ML与CV研究组； http://research.microsoft.com/en-us/groups/mlp/default.aspx （20）研学论坛； http://bbs.matwav.com/ （21）美国Rutgers大学助理教授刘青山； http://www.research.rutgers.edu/~qsliu/ （22）计算机视觉最新资讯网； http://www.cvchina.info/ （23）运动检测、阴影、跟踪的测试视频下载； http://apps.hi.baidu.com/share/detail/18903287 （24）香港中文大学助理教授王晓刚； http://www.ee.cuhk.edu.hk/~xgwang/ (25)香港中文大学多媒体实验室（汤晓鸥）; http://mmlab.ie.cuhk.edu.hk/ (26)U.C. San Diego. computer vision;http://vision.ucsd.edu/content/home (27)CVonline; http://homepages.inf.ed.ac.uk/rbf/CVonline/ (28)computer vision software; http://peipa.essex.ac.uk/info/software.html (29)Computer Vision Resource; http://www.cvpapers.com/ (30)computer vision research groups;http://peipa.essex.ac.uk/info/groups.html (31)computer vision center; http://computervisioncentral.com/cvcnews (32)浙江大学图像技术研究与应用（ITRA）团队：http://www.dvzju.com/ (33)自动识别网：http://www.autoid-china.com.cn/ (34)清华大学章毓晋教授：http://www.tsinghua.edu.cn/publish/ee/4157/2010/20101217173552339241557/20101217173552339241557_.html (35)顶级民用机器人研究小组Porf.Gary领导的Willow Garage:http://www.willowgarage.com/ (36)上海交通大学图像处理与模式识别研究所：http://www.pami.sjtu.edu.cn/ (37)上海交通大学计算机视觉实验室刘允才教授：http://www.visionlab.sjtu.edu.cn/ (38)德克萨斯州大学奥斯汀分校助理教授Kristen Grauman ：http://www.cs.utexas.edu/~grauman/ 图像分解，检索 (39)清华大学电子工程系智能图文信息处理实验室（丁晓青教授）：http://ocrserv.ee.tsinghua.edu.cn/auto/index.asp (40)北京大学高文教授：http://www.jdl.ac.cn/htm-gaowen/ (41)清华大学艾海舟教授：http://media.cs.tsinghua.edu.cn/cn/aihz (42)中科院生物识别与安全技术研究中心：http://www.cbsr.ia.ac.cn/china/index%20CH.asp (43)瑞士巴塞尔大学 Thomas Vetter教授：http://informatik.unibas.ch/personen/vetter_t.html (44)俄勒冈州立大学 Rob Hess博士：http://blogs.oregonstate.edu/hess/ (45)深圳大学 于仕祺副教授：http://yushiqi.cn/ (46)西安交通大学人工智能与机器人研究所：http://www.aiar.xjtu.edu.cn/ (47)卡内基梅隆大学研究员Robert T. Collins:http://www.cs.cmu.edu/~rcollins/home.html#Background (48)MIT博士Chris Stauffer:http://people.csail.mit.edu/stauffer/Home/index.php (49)美国密歇根州立大学生物识别研究组(Anil K. Jain教授)：http://www.cse.msu.edu/rgroups/biometrics/ (50)美国伊利诺伊州立大学Thomas S. Huang:http://www.beckman.illinois.edu/directory/t-huang1 (51)武汉大学数字摄影测量与计算机视觉研究中心：http://www.whudpcv.cn/index.asp (52)瑞士巴塞尔大学Sami Romdhani助理研究员：http://informatik.unibas.ch/personen/romdhani_sami/ (53)CMU大学研究员Yang Wang:http://www.cs.cmu.edu/~wangy/home.html (54)英国曼彻斯特大学Tim Cootes教授：http://personalpages.manchester.ac.uk/staff/timothy.f.cootes/ (55)美国罗彻斯特大学教授Jiebo Luo:http://www.cs.rochester.edu/u/jluo/ (56)美国普渡大学机器人视觉实验室：https://engineering.purdue.edu/RVL/Welcome.html (57)美国宾利州立大学感知、运动与认识实验室：http://vision.cse.psu.edu/home/home.shtml (58)美国宾夕法尼亚大学GRASP实验室：https://www.grasp.upenn.edu/ (59)美国内达华大学里诺校区CV实验室：http://www.cse.unr.edu/CVL/index.php (60)美国密西根大学vision实验室：http://www.eecs.umich.edu/vision/index.html (61)University of Massachusetts(麻省大学),视觉实验室：http://vis-www.cs.umass.edu/index.html (62)华盛顿大学博士后Iva Kemelmacher:http://www.cs.washington.edu/homes/kemelmi (63)以色列魏茨曼科技大学Ronen Basri:http://www.wisdom.weizmann.ac.il/~ronen/index.html (64)瑞士ETH-Zurich大学CV实验室：http://www.vision.ee.ethz.ch/boostingTrackers/index.htm (65)微软CV研究员张正友：http://research.microsoft.com/en-us/um/people/zhang/ (66)中科院自动化所医学影像研究室：http://www.3dmed.net/ (67)中科院田捷研究员：http://www.3dmed.net/tian/ (68)微软Redmond研究院研究员Simon Baker:http://research.microsoft.com/en-us/people/sbaker/ (69)普林斯顿大学教授李凯：http://www.cs.princeton.edu/~li/ (70)普林斯顿大学博士贾登：http://www.cs.princeton.edu/~jiadeng/ (71)牛津大学教授Andrew Zisserman： http://www.robots.ox.ac.uk/~az/ (72)英国leeds大学研究员Mark Everingham:http://www.comp.leeds.ac.uk/me/ (73)英国爱丁堡大学教授Chris William: http://homepages.inf.ed.ac.uk/ckiw/ (74)微软剑桥研究院研究员John Winn: http://johnwinn.org/ (75)佐治亚理工学院教授Monson H.Hayes：http://savannah.gatech.edu/people/mhayes/index.html (76)微软亚洲研究院研究员孙剑：http://research.microsoft.com/en-us/people/jiansun/ (77)微软亚洲研究院研究员马毅：http://research.microsoft.com/en-us/people/mayi/ (78)英国哥伦比亚大学教授David Lowe: http://www.cs.ubc.ca/~lowe/ (79)英国爱丁堡大学教授Bob Fisher: http://homepages.inf.ed.ac.uk/rbf/ (80)加州大学圣地亚哥分校教授Serge J.Belongie:http://cseweb.ucsd.edu/~sjb/ (81)威斯康星大学教授Charles R.Dyer: http://pages.cs.wisc.edu/~dyer/ (82)多伦多大学教授Allan.Jepson: http://www.cs.toronto.edu/~jepson/ (83)伦斯勒理工学院教授Qiang Ji: http://www.ecse.rpi.edu/~qji/ (84)CMU研究员Daniel Huber: http://www.ri.cmu.edu/person.html?person_id=123 (85)多伦多大学教授：David J.Fleet: http://www.cs.toronto.edu/~fleet/ (86)伦敦大学玛丽女王学院教授Andrea Cavallaro:http://www.eecs.qmul.ac.uk/~andrea/ (87)多伦多大学教授Kyros Kutulakos: http://www.cs.toronto.edu/~kyros/ (88)杜克大学教授Carlo Tomasi: http://www.cs.duke.edu/~tomasi/ (89)CMU教授Martial Hebert: http://www.cs.cmu.edu/~hebert/ (90)MIT助理教授Antonio Torralba: http://web.mit.edu/torralba/www/ (91)马里兰大学研究员Yasel Yacoob: http://www.umiacs.umd.edu/users/yaser/ (92)康奈尔大学教授Ramin Zabih: http://www.cs.cornell.edu/~rdz/ (93)CMU博士田渊栋: http://www.cs.cmu.edu/~yuandong/ (94)CMU副教授Srinivasa Narasimhan: http://www.cs.cmu.edu/~srinivas/ (95)CMU大学ILIM实验室：http://www.cs.cmu.edu/~ILIM/ (96)哥伦比亚大学教授Sheer K.Nayar: http://www.cs.columbia.edu/~nayar/ (97)三菱电子研究院研究员Fatih Porikli ：http://www.porikli.com/ (98)康奈尔大学教授Daniel Huttenlocher：http://www.cs.cornell.edu/~dph/ (99)南京大学教授周志华：http://cs.nju.edu.cn/zhouzh/index.htm (100)芝加哥丰田技术研究所助理教授Devi Parikh: http://ttic.uchicago.edu/~dparikh/index.html (101)瑞士联邦理工学院博士后Helmut Grabner: http://www.vision.ee.ethz.ch/~hegrabne/#Short_CV (102)香港中文大学教授贾佳亚：http://www.cse.cuhk.edu.hk/~leojia/index.html (103)南京大学教授吴建鑫：http://c2inet.sce.ntu.edu.sg/Jianxin/index.html (104)GE研究院研究员李关：http://www.cs.unc.edu/~lguan/ (105)佐治亚理工学院教授Monson Hayes:http://savannah.gatech.edu/people/mhayes/ (106)图片检索国际竞赛PASCAL VOC(微软剑桥研究院组织):http://pascallin.ecs.soton.ac.uk/challenges/VOC/ (107)机器视觉开源处理库汇总：http://archive.cnblogs.com/a/2217609/ (108)布朗大学教授Benjamin Kimia: http://www.lems.brown.edu/kimia.html (109)数据堂-图像处理相关的样本数据：http://www.datatang.com/data/list/602026/p1 (110)东软基于CV的汽车辅助驾驶系统：http://www.neusoft.com/cn/solutions/1047/ (111)马里兰大学教授Rema Chellappa:http://www.cfar.umd.edu/~rama/ (112)芝加哥丰田研究中心助理教授Devi Parikh：http://ttic.uchicago.edu/~dparikh/index.html (113)宾夕法尼亚大学助理教授石建波：http://www.cis.upenn.edu/~jshi/ (114)比利时鲁汶大学教授Luc Van Gool：http://www.vision.ee.ethz.ch/members/get_member.cgi?id=1, http://www.vision.ee.ethz.ch/~vangool/ (115)行人检测主页：http://www.pedestrian-detection.com/ (116)法国学习算法与系统实验室Basilio Noris博士：http://lasa.epfl.ch/people/member.php?SCIPER=129576 http://mldemos.epfl.ch/ (117)美国马里兰大学LARRY S.DAVIS教授：http://www.umiacs.umd.edu/~lsd/ (118)计算机视觉论文分类导航：http://www.visionbib.com/bibliography/contents.html (119)计算机视觉分类信息导航：http://www.visionbib.com/ (120)西班牙马德里理工大学博士Marcos Nieto：http://marcosnieto.net/ (121)香港理工大学副教授张磊：http://www4.comp.polyu.edu.hk/~cslzhang/ (122)以色列技术学院教授Michael Elad：http://www.cs.technion.ac.il/~elad/ (123)韩国启明大学计算机视觉与模式识别实验室：http://cvpr.kmu.ac.kr/ (124)英国诺丁汉大学Michel Valstar博士：http://www.cs.nott.ac.uk/~mfv/ (125)卡内基梅隆大学Takeo Kanade教授:http://www.ri.cmu.edu/people/kanade_takeo.html (126)微软学术搜索：http://libra.msra.cn/ (127)比利时天主教鲁汶大学Radu Timofte博士：http://homes.esat.kuleuven.be/~rtimofte/，交通标志检测，定位，3D跟踪 (128)迪斯尼匹兹堡研究院研究员：Iain Matthews:http://www.iainm.com/iainm/Home.html http://www.ri.cmu.edu/person.html?type=publications&person_id=741 AAM,三维重建 （129）康奈尔大学视觉与图像分析组：http://www.via.cornell.edu/ 医学图像处理 （130）密西根州立大学生物识别研究组：http://www.cse.msu.edu/biometrics/ 人脸识别、指纹识别、图像检索 （131）柏林科技大学计算机视觉与遥感实验室：http://www.cv.tu-berlin.de/menue/computer_vision_remote_sensing/parameter/en/ 图像分析、物体重建、基于图像的表面测量、医学图像处理 （132）英国布里斯托大学数字多媒体研究组：http://www.cs.bris.ac.uk/Research/Digitalmedia/ 运动检测与跟踪、视频压缩、3D重建、字符定位 （133）英国萨利大学视觉、语音与信号处理中心： http://www.surrey.ac.uk/cvssp/ 人脸识别、监控、3D、视频检索、 （134）北卡莱罗纳大学教堂山分校Marc Pollefeys教授：http://www.cs.unc.edu/~marc/ 基于视频的3D模型生成、相机标定、运动检测与分析、3D重建 （135）澳大利亚国立大学Richard Hartley教授：http://users.cecs.anu.edu.au/~hartley/ 运动估计、稀疏子空间、跟踪、 （136）百度技术副总监于凯：http://www.dbs.ifi.lmu.de/~yu_k/ 深度学习，稀疏表示，图像分类 （137）西安电子科技大学高新波教授：http://web.xidian.edu.cn/xbgao/index.html 质量评判、水印、稀疏表示、超分辨率 （138）加州大学伯克利分校Michael I.Jordan教授：http://www.cs.berkeley.edu/~jordan/ 机器学习 （139）加州理工行人检测相关资料：http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/ （140）微软Redmond研究院研究员Piotr Dollar: http://vision.ucsd.edu/~pdollar/ 行人检测、特征提取、 （141）视觉计算研究论坛：http://www.sigvc.org/bbs/ 中科院视觉计算研究小组的论坛 （142）美国坦桑尼亚州立大学稀疏学习软件包：http://www.public.asu.edu/~jye02/Software/SLEP/index.htm 稀疏学习 （143）美国加州大学圣地亚哥分校Jacob Whitehill博士：http://mplab.ucsd.edu/~jake/ 机器学习 （144）美国布朗大学Michael J.Black教授：http://cs.brown.edu/~black/ 人的姿态估计和跟踪 （145）美国加州大学圣地亚哥分校David Kriegman教授：http://cseweb.ucsd.edu/~kriegman/ 人脸识别 （146）南加州大学Paul Debevec教授：http://ict.debevec.org/~debevec/ 或 http://www.pauldebevec.com/ 将CV和CG结合研究 人脸捕捉重建技术 （147）伊利诺伊大学D.A.Forsyth教授：http://luthuli.cs.uiuc.edu/~daf/ 三维重建 （148）英国牛津大学Ian Reid教授：http://www.robots.ox.ac.uk/~ian/ 跟踪和机器人导航 （149）CMU大学Alyosha Efros 教授: https://www.cs.cmu.edu/~efros/ 图像纹理合成 （150）加州大学伯克利分校Jitendra Malik教授：http://www.cs.berkeley.edu/~malik/ 轮廓检测、图像/视频分割、图形匹配、目标识别 （151）MIT教授William Freeman： http://people.csail.mit.edu/billf/ 图像纹理合成 （152）CMU博士Henry Schneiderman： http://www.cs.cmu.edu/~hws/ 目标检测和识别； （153）微软研究员Paul Viola: http://research.microsoft.com/en-us/um/people/viola/ AdaBoost算法 （154）微软研究员Antonio Criminisi: http://research.microsoft.com/en-us/people/antcrim/ 图像修补，三维重建，目标检测与跟踪； （155）魏茨曼科学研究所教授Michal Irani: http://www.wisdom.weizmann.ac.il/~irani/ 超分辨率 （156）瑞士洛桑理工学院Pascal Fua教授：http://people.epfl.ch/pascal.fua/bio?lang=en 立体视觉，增强现实 （157）佐治亚理工学院Irfan Essa教授：http://www.ic.gatech.edu/people/irfan-essa 人脸表情识别 （158）中科院助理教授樊彬：http://www.sigvc.org/bfan/ 特征描述； （159）斯坦福大学Sebastian Thrun教授：http://robots.stanford.edu/index.html 机器人； （160）多伦多大学Geoffrey E.Hinton教授：http://www.cs.toronto.edu/~hinton/ 深度学习 （161）凤巢系统架构师张栋博士：http://weibo.com/machinelearning （162）2012年龙星计划机器学习课程：http://bigeye.au.tsinghua.edu.cn/DragonStar2012/index.html （163）中科院自动化所肖柏华教授：http://www.compsys.ia.ac.cn/people/xiaobaihua.html 文字识别、人脸识别、质量评判 （164）图像视频质量评判：http://live.ece.utexas.edu/research/quality/ （165）纽约大学Yann LeCun教授http://yann.lecun.com/ http://yann.lecun.com/exdb/mnist/ 手写体数字识别 （166）二维条码识别开源库zxing：http://code.google.com/p/zxing/ （167）布朗大学Pedro Felzenszwalb教授：http://cs.brown.edu/~pff/ 特征提取，Deformable Part Model （168）伊利诺伊香槟大学Svetlana Lazebnik教授：http://www.cs.illinois.edu/homes/slazebni/ 特征提取，聚类，图像检索 （169）荷兰乌德勒支大学图像与多媒体研究中心http://www.cs.uu.nl/centers/give/multimedia/index.html 图像、多媒体检索与匹配 （170）英国格拉斯哥大学信息检索小组：http://ir.dcs.gla.ac.uk/ 文本、图像、视频检索 （171）中科院自动化所孙哲南助理教书：http://www.cbsr.ia.ac.cn/users/znsun/ 虹膜识别、掌纹识别、人脸识别 （172）南京信息工程大学刘青山教授：http://www.jstuoke.com/web/xky/detail.asp?NewsID=1096 人脸图像分析、医学图像分析 （173）清华大学助理教授冯建江：http://ivg.au.tsinghua.edu.cn/~jfeng/ 指纹识别 （174）北航助理教授黄迪：http://irip.buaa.edu.cn/~dihuang/ 3D人脸识别 （175）中山大学助理教授郑伟诗：http://sist.sysu.edu.cn/~zhwshi/ 人脸识别、特征匹配、聚类、检索； （176）google瑞士苏黎世的工程师Thomas Deselaers: http://thomas.deselaers.de/index.html 图像检索 （177）百度深度学习研究中心博士后余轶南：http://www.cbsr.ia.ac.cn/users/ynyu/index.htm 目标检测，图像检索 （178）威兹曼科技大学超分辨率：http://www.wisdom.weizmann.ac.il/~vision/SingleImageSR.html （179）德克萨斯大学奥斯汀分校Al Bovik教授：http://live.ece.utexas.edu/people/bovik/ 图像视频质量判别、特征提取 （180）以色列希伯来大学Yair Weiss教授：http://www.cs.huji.ac.il/~yweiss/ 机器学习、超分辨率 （181）以色列希伯来大学Daniel Zoran博士：http://www.cs.huji.ac.il/~daniez/ 超分辨率、去噪 （182）美国加州大学Peyman Milanfar教授：http://users.soe.ucsc.edu/~milanfar/ 去噪 （183）中科院计算所副研究员常虹：http://www.jdl.ac.cn/user/hchang/index.html 图像检索、半监督学习、超分辨率 （184）以色列威茨曼大学Anat Levin教授：http://www.wisdom.weizmann.ac.il/~levina/ 去噪、去模糊 （185）以色列威茨曼大学Daniel Glasner博士后：http://www.wisdom.weizmann.ac.il/~glasner/ 超分辨率、分割、姿态估计 （186）密西根大学助理教授Honglak Lee: http://web.eecs.umich.edu/~honglak/ 机器学习、特征提取，去噪、稀疏表示； （187）MIT周博磊博士：http://people.csail.mit.edu/bzhou/ 聚集分析、运动检测 （188）美国田纳西大学Li He博士：http://web.eecs.utk.edu/~lhe4/ 稀疏表示、超分辨率； （189）Adobe研究院Jianchao Yang研究员：http://www.ifp.illinois.edu/~jyang29/ 稀疏表示，超分辨率、图片检索、去噪、去模糊 （190）Deep Learning主页：http://deeplearning.net/ 深度学习论文、软件，代码，demo，数据等； （191）斯坦福大学Andrew Ng教授：http://cs.stanford.edu/people/ang/ 深度神经网络，深度学习 （192）Elefant: http://elefant.developer.nicta.com.au/ 机器学习开源库 （193）微软研究员Ce Liu: http://people.csail.mit.edu/celiu/ 去噪、超分辨率、去模糊、分割 （194）West Virginia大学助理教授Xin Li: http://www.csee.wvu.edu/~xinl/ 边缘检测、降噪、去模糊 （195）http://www.csee.wvu.edu/~xinl/source.html 深度学习、去噪、编码、压缩感知、超分辨率、聚类、分割等相关代码集合 （196）西班牙格拉纳达大学超分辨率重建项目组：http://decsai.ugr.es/pi/superresolution/index.html （197）清华大学程明明博士：http://mmcheng.net/ 图像分割、检索 （198）牛津布鲁克斯大学Philip H.S.Torr教授：http://cms.brookes.ac.uk/staff/PhilipTorr/ 分割、三维重建 （199）佐治亚理工学院James M.Rehg教授：http://www.cc.gatech.edu/~rehg/ 分割、行人检测、特征描述、 （200）大规模图像分类、检测竞赛ILSVRC（Stanford, Google举办）: http://www.image-net.org/challenges/LSVRC/2013/ （201）加州大学尔湾分校Deva Ramanan助理教授：http://www.ics.uci.edu/~dramanan/ 目标检测，行人检测，跟踪、稀疏表示 （202）人脸识别测试图片集：http://www.mlcv.net/ （203）美国西北大学博士Ming Yang: http://www.ece.northwestern.edu/~mya671/ 人脸识别、图像检索； （204）美国加州大学伯克利分校博士后Ross B.Girshick：http://www.cs.berkeley.edu/~rbg/ 目标检测（DPM） （205）中文语言资源联盟：http://www.chineseldc.org/index.html 内有很多语言识别、字符识别的训练，测试库； （206）西班牙巴塞罗那大学计算机视觉中心：http://www.cvc.uab.es/adas/site/ 检测、跟踪、3D、行人检测、汽车辅助驾驶 （207）德国戴姆勒研究所Prof. Dr. Dariu M. Gavrila：http://www.gavrila.net/index.html 跟踪、行人检测、 （208）苏黎世联邦理工学院Andreas Ess博士后：http://www.vision.ee.ethz.ch/~aess/ 行人检测、行为检测、跟踪 （209）Libqrencode: http://fukuchi.org/works/qrencode/ 基于C语言的QR二维码编码开源库 （210）江西财经大学袁飞牛教授：http://sit.jxufe.cn/grbk/yfn/index.html# 烟雾检测、3D重建、医学图像处理"}
{"content2":"作者：朱松纯，杨志宏发表于：视觉求索公众号人物简介：朱松纯，加州大学洛杉矶分校统计学和计算机科学教授；杨志宏，视觉求索公众号编辑内容：一、追溯计算机视觉源头的意义（1）读史才能知得失。一个民族如果忘记了历史，她也注定失去了未来，这也同样适用于学科。（2）目前做学术的人大多只关注进行近几年的文章，只知道这几年的历史和流行的方法。（3）各种工业界、资本和投资界都在往里面炒作。（4）网络上的信息良莠不齐，年轻的研究生通常缺乏免疫力，无法看穿一个推送是否软文。二、计算机视觉和人工智能、机器学习的关系（1）计算机视觉和人工智能的关系是：计算机视觉是人工智能的一个至关重要的组成部分。没有计算机视觉，人工智能就无法进入现实世界，只能成为一个空架子，做符号推理。（2）计算机视觉和机器学习的关系是：计算机视觉是一个领域，而机器学习基本上是一个方法和工具。机器学习应用于计算机视觉，二者大概有百分之六七十是重合的。简单来说，机器学习中的“机器”就是统计模型，“学习”就是用数据来模拟模型。（3）计算机视觉的起源的代表人物：David Marr, King-Sun Fu 和Ulf Grenander三、视觉的开创者之一：David Marr的学术思想Daivd Marr, 1945-1980。剑桥大学毕业，后来到MIT做博士后，然后是教授。他博士论文研究的是从理论的角度研究大脑功能。他的工作横跨了计算机视觉、认知科学和神经学，这三个学科。David Marr写了一本书《视觉：从计算的视角研究人的视觉信息表达与处理》。David Marr的学术贡献主要有三条，从而基本上定义了计算机视觉的格局：（1）理清了计算（表达或者模型）、算法和实现这三个层次。在计算的层次，进行问题的描述，定义任务与输入、输出。在算法的层次，选择不同的算法，可以并行也可以串行。在实现的层次，选择合适的硬件进行实现。当前的深度学习存在的一个很大的问题就是它将这三者混在一起，当它性能不好的时候，我们不知道是因为表达不对，算法不对还是实现不对。（2）理清视觉到底要计算什么。他提出了一个系列的表达，从首要简约图，到深度简约图再到3D简约图。这里面包含了纹理，立体视觉，运动分系，表面形状等等。（3）提出计算机视觉是一个计算的过程。他觉得计算机视觉不是单纯的求一个解，而是一个连续不断的计算过程。视觉是受任务驱动的，而任务是时刻在改变中的。这个贡献点在算法层面。四、视觉的开创者之二：King-Sun Fu的学术思想King-Sun Fu,1930-1985。美国伊利诺大学博士，后来到普渡大学做博士后，然后是教授。他之前的研究领域是计算机科学。他是ICPR,PAMI,IAPR的主席和创始人。King-Sun Fu的学术贡献主要有三条：（1）对学科和学会的建设，以及工程师的培养上。他是模式识别的开山鼻祖。（2）句法结构性的表达与计算，就是句法模式识别。（3）句法模式识别支撑了自底向上或自顶向下的计算的过程。五、视觉的开创者之三：Ulf Grenander的学术思想Grenander, 1923-2016。Stuckholm university博士，中间去了很多大学，1966年到他退休一直都在布朗大学。Ulf Grenander的学术贡献主要有三条：（1）提出analysis-by-systhesis的思想。这是产生式建模的核心理念（2）提出了一整套的理论和方法，把代数，集合和概率整合起来。（3）提出了很多求解非凸问题的方法。"}
{"content2":"计算机视觉方面的三大国际会议是ICCV, CVPR和ECCVICCV的全称是International Comference on Computer Vision，正如很多和他一样的名字的会议一行，这样最朴实的名字的会议，通常也是这方面最nb的会议。ICCV两年一次，与ECCV正好错开，是公认的三个会议中级别最高的。它的举办地方会在世界各地选，上次是在北京，下次在巴西，2009在日本。iccv上的文章看起来一般都比较好懂，我是比较喜欢的。CVPR的全称是Internaltional Conference on Computer Vision and Pattern Recogintion。这是一个一年一次的会议，举办地从来没有出过美国，因此想去美国旅游的同学不要错过。正如它的名字一样，这个会上除了视觉的文章，还会有不少模式识别的文章，当然两方面的结合自然也是重点。ECCV的全称是Europeon Conference on Computer Vision，是一个欧洲的会议。虽然名字不是International，但是会议的级别不比前面两个差多少。欧洲人一般比较看中理论，但是从最近一次会议来看，似乎大家也开始注重应用了，oral里面的demo非常之多，演示效果很好，让人赏心悦目、叹为观止。不过欧洲的会有一个不好，就是他们的人通常英语口音很重，有些人甚至不太会说英文，所以开会和交流的时候，稍微有些费劲。总的来说，以上三个会议是做计算机视觉人必须关注的会议，建议每一期的oral都要精读，poster调自己相关的仔细看看。如果有好的进一步的想法，可以马上发表，因为他们已经是最新的了，对他们的改进通常也是最新的。同时如果你做了类似的工作，却没有引用这些会议的文章，很有可能会被人指出综述部分的问题，因为评审的人一般都是牛人，对这三个会议也会很关注的。计算机方向的一些顶级会议和期刊http://kapoc.blogdriver.com/kapoc/1134028.htmlComputer VisionConf.:Best: ICCV, Inter. Conf. on Computer VisionCVPR, Inter. Conf. on Computer Vision and Pattern RecognitionGood: ECCV, Euro. Conf. on Comp. VisionICIP, Inter. Conf. on Image ProcessingICPR, Inter. Conf. on Pattern RecognitionACCV, Asia Conf. on Comp. VisionJour.:Best: PAMI, IEEE Trans. on Patt. Analysis and Machine IntelligenceIJCV, Inter. Jour. on Comp. VisionGood: CVIU, Computer Vision and Image UnderstandingPR,   Pattern Reco.NetworkConf.:ACM/SigCOMM ACM Special Interest Group of Communication..ACM/SigMetric   这个系统方面也有不少的Info Com  几百人的大会，不如ACM/SIG的精。Globe Com 这个就很一般了，不过有时候会有一些新的想法提出来。Jour.:ToN (ACM/IEEE Transaction on Network)A.I.Conf.:AAAI: American Association for Artificial IntelligenceACM/SigIR: 这个是IR方面的，可能DB/AI的人都有IJCAI: International Joint Conference on Artificial IntelligenceNIPS: Neural Information Processing SystemsICML: International Conference on Machine LearningJour.:Machine LearningNEURAL COMPUTATION: 这个的影响因子在AI里最高，2000年为1.921ARTIFICIAL INTELLIGENCE: 1.683(2000年的数据，下同)PAMI: 1.668IEEE TRANSACTIONS ON FUZZY SYSTEMS: 1.597IEEE TRANSACTIONS ON NEURAL NETWORKS: 1.395AI MAGAZINE: 1.044NEURAL NETWORKS: 1.019PATTERN RECOGNITION: 0.781IMAGE AND VISION COMPUTING: 0.616IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING: 0.465APPLIED INTELLIGENCE: 0.268GraphicsConf.:Best:Siggraph: ACM SigGraphGood:Euro GraphJour.:IEEE(ACM) Trans. on GraphicsIEEE Trans. on Visualization and Computer GraphicsCADJour.:CADCAGD本文来自CSDN博客，转载请标明出处：http://blog.csdn.net/clay95/archive/2010/08/02/5783064.aspx"}
{"content2":"下面是前端时间搜集整理的一些和计算机视觉、模式识别的资源，拿出来与大家分享下。以后，我将把图像处理真正的作为我的兴趣来玩玩了，也许不把研究作为谋生的手段，会更好些。标题作者主题关键字类别来源备注nipsfast.pptNando de FreitasN-Body problems in learningFast N-Body LearningPpthttp://www.cs.ubc.ca/~nando/nipsfast/schedule.htmlnipsfgtf.pptRamani DuraiswamiFast Multipole Methods Fast Gaussian TransformFM and FGTppthttp://www.cs.ubc.ca/~nando/nipsfast/schedule.htmlGray.pdf/pptAlex GrayStatistical N-Body/Proximity Data StructuresN-Body and Data StructuresPpt/pdfhttp://www.cs.ubc.ca/~nando/nipsfast/schedule.htmldt-nips04.pdf/pptDan HuttenlocherFast Distance TransformsFDTPpt/pdfhttp://www.cs.ubc.ca/~nando/nipsfast/schedule.htmlHigh.pdf/pptAlexander GrayFast high-dimensional function integrationFast integrationPpt/pdfhttp://www.cs.ubc.ca/~nando/nipsfast/schedule.htmlFast04.pdf/pptDavid LoweFast high-dimensional feature indexing for object recognitionFeature indexingPpt/pdfhttp://www.cs.ubc.ca/~nando/nipsfast/schedule.htmlihler-fast.pdf/pptAlexander lhlerFast methods and non-parametric BPNon-parametric BPPpt/pdfhttp://www.cs.ubc.ca/~nando/nipsfast/schedule.htmlfastview.pdfDustin LangComparing fast methodsOverview fast methodspdfhttp://www.cs.ubc.ca/~nando/nipsfast/schedule.htmlnbody_methods.tar.gzcodehttp://www.cs.ubc.ca/~awll/nbody_methods.htmldemo_rbpf_gauss.tarRao Blackwellised particle filtering for conditionally Gaussian Modelsparticle filtering for conditionallycodehttp://www.cs.ubc.ca/~nando/nipsfast/schedule.htmldemorbpfdbn.tar.gzRao Blackwellised Particle Filteringcodehttp://www.cs.ubc.ca/~nando/nipsfast/schedule.htmlhttp://www.cs.ubc.ca/~nando/software.htmlupf_demos.tar.gzUnscented Particle FilterParticle Filtercodehttp://www.cs.ubc.ca/~nando/nipsfast/schedule.htmlBPF_1_3.zipBoosted Particle FilterTrackingcodehttp://www.cs.ubc.ca/~okumak/research.html1flyer_14_800.mpgSource imageDatabaseImagehttp://www.cs.ubc.ca/~okumak/research.html1trans_flyer_14_800.mpgimage transformedDatabaseImagehttp://www.cs.ubc.ca/~okumak/research.html1LBP.c/hTopi MäenpääLBP operatorTexturecodehttp://www.ee.oulu.fi/~topiolli/cpplibs/files/calibr_v30.zipCamera CalibrationComputer visioncodehttp://www.ee.oulu.fi/mvg/page/camera_calibration_toolbox_for_matlab2LEAR(Learning and Recognition in VisionCommon datasetHuman/car horse soccer human actionsdatasethttp://lear.inrialpes.fr/data3Lic.zip/highlight.zipRobby T. TanColor Constancy Through Inverse Intensity Chromaticity SpaceHighlight Removal from single imagecodehttp://www.commsp.ee.ic.ac.uk/~rtan/2008_oxford_fog.pdfRobby T. TanDefogDefog from singlepdfhttp://www.commsp.ee.ic.ac.uk/~rtan/08_cvpr.pdfRobby T. TanDefogDefog from singlepdfhttp://www.commsp.ee.ic.ac.uk/~rtan/Retinex_frankle_mccannRetinexCodehttp://www.cs.sfu.ca/~colour/publications/IST-2000/SomeRetinex_maccann99Retinexcodehttp://www.cs.sfu.ca/~colour/publications/IST-2000/picturesGamut.tar.bz2Retinexcodehttp://kobus.ca/research/programs/colour_constancy/index.htmlVideo.avi/dehaze.mdehazingRaanan Fattalcodehttp://www.cs.huji.ac.il/~raananf/projects/defog/index.htmlMPTK-Windows-bin-0-5-6-beta.zipMatching pursuit(MP)AlogrithmCNRSCodehttp://mptk.irisa.fr/downloadsgenerateDictionaries.txtGenerateGaborAlogrithmcodehttp://www.scholarpedia.org/article/Matching_pursuitNotes:1.      视频和源码都是对应的文章的：Kenji Okuma, Ali Taleghani, Nando De Freitas, Jim Little, David G. Lowe. Boosted Particle Filter: Multitarget Detection and Tracking. the European Conference on Computer Vision(ECCV), May 2004.2.      该网站下面还有其他一些资源可以下载：http://www.ee.oulu.fi/mvg/page/downloads是个研究组织：http://lear.inrialpes.fr/ ， 除此之外，还有一些源码。计算机视觉文献与代码资源CVonlinehttp://homepages.inf.ed.ac.uk/rbf/CVonlinehttp://homepages.inf.ed.ac.uk/rbf/CVonline/unfolded.htmhttp://homepages.inf.ed.ac.uk/rbf/CVonline/CVentry.htm李子青的大作：Markov Random Field Modeling in Computer Visionhttp://www.cbsr.ia.ac.cn/users/szli/mrf_book/book.htmlHandbook of Face Recognition (PDF)http://www.umiacs.umd.edu/~shaohua/papers/zhou04hfr.pdf张正友的有关参数鲁棒估计著作：Parameter Estimation Techniques:A Tutorial with Application to Conic Fittinghttp://research.microsoft.com/~zhang/INRIA/Publis/Tutorial-Estim/Main.htmlAndrea Fusiello“计算机视觉中的几何”教程：Elements of Geometric Computer Visionhttp://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/FUSIELLO4/tutorial.html#x1-520007有关马尔可夫蒙特卡罗方法的资料：An introduction to Markov chain Monte Carlohttp://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/SENEGAS/mcmc.htmlMarkov Chain Monte Carlo for Computer Vision--- A tutorial at ICCV05http://civs.stat.ucla.edu/MCMC/MCMC_tutorial.htm有关独立成分分析（Independent Component Analysis , ICA）的资料：An ICA-Pagehttp://www.cnl.salk.edu/~tony/ica.htmlFast ICAhttp://www.cis.hut.fi/projects/ica/fastica/The Kalman Filter (介绍卡尔曼滤波器的终极网页)http://www.cs.unc.edu/~welch/kalman/index.htmlCached k-d tree search for ICP algorithmshttp://kos.informatik.uni-osnabrueck.de/download/3dim2007/paper.html几个计算机视觉研究工具Machine Vision Toolbox for Matlabhttp://www.petercorke.com/Machine%20Vision%20Toolbox.htmlMatlab and Octave Function for Computer Vision and Image Processinghttp://www.csse.uwa.edu.au/~pk/research/matlabfns/Bayes Net Toolbox for Matlabhttp://www.cs.ubc.ca/~murphyk/Software/BNT/bnt.htmlOpenCV (Chinese)http://www.opencv.org.cn/index.php/%E9%A6%96%E9%A1%B5Gandalf (A Computer Vision and Numerical Algorithm Labrary)http://gandalf-library.sourceforge.net/CMU Computer Vision Home Pagehttp://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.htmlMachine Learning Resource Linkshttp://www.cse.ust.hk/~ivor/resource.htmThe Bayesian Filtering Libraryhttp://www.orocos.org/bflOptical Flow Algorithm Evaluation (提供了一个动态贝叶斯网络框架，例如递归信息处理与分析、卡尔曼滤波、粒子滤波、序列蒙特卡罗方法等，C++写的)http://of-eval.sourceforge.net/MATLAB code for ICP algorithmhttp://www.usenet.com/newsgroups/comp.graphics.visualization/msg00102.html牛人主页：朱松纯 （Song-Chun Zhu）http://www.stat.ucla.edu/~sczhu/David Lowe (SIFT) (很帅的一个老头哦 ^ ^)http://www.cs.ubc.ca/~lowe/Andrea Vedaldi (SIFT)http://vision.ucla.edu/~vedaldi/index.htmlPedro F. Felzenszwalbhttp://people.cs.uchicago.edu/~pff/Dougla Dlanman (Brown的一个研究生，在其主页上搜集了大量算法教程和源码)http://mesh.brown.edu/dlanman/courses.htmlJianbo Shi (Ncuts 的始作俑者)http://www.cis.upenn.edu/~jshi/Active Vision Group (Oxford的一个机器视觉研究团队，特色是SLAM，监视，导航)http://www.robots.ox.ac.uk/ActiveVision/index.htmlJuyang Weng（机器学习的专家，Autonomous Mental Development 是其特色）http://www.cse.msu.edu/~weng/测试图片或视频：Middlebury College‘s Stereo Vision Data Sethttp://cat.middlebury.edu/stereo/data.htmlIntelligent Vehicle:IVSourcewww.ivsoruce.netRobot Carhttp://www.plyojump.com/robot_cars.htmlHow to Build a Robot: The Computer Vision Parthttp://www.societyofrobots.com/programming_computer_vision_tutorial.shtml计算机视觉应关注的资源来自美国帝腾大学的链接。Camera Calibration Links to toolboxes (mostly MATLAB) for camera calibration.Paul Debevec. Modeling and Rendering Architecture from Photographs.Marc Pollefeys, Tutorial on 3D Modeling from Images,, ECCV 2000,Available here: notes (12.1MB pdf)Richard Szeliski NIPS 2004 Tutorial on Acquiring Detailed 3D Models From Images and Video,Available here: slides (37.6 MB, ppt)Peter Corke did his thesis work on visual servoing for robot applications and has authored a robotics toolkit and vision toolkit for MATLAB.local copy of thesis: Corke thesis (4.36 MB, pdf)robot toolkit: robot.zip (568 KB, zip)vision toolkit: mv.zip (1.08 MB, zip)P. D. Kovesi., MATLAB Functions for Computer Vision and Image Analysis.School of Computer Science & Software Engineering, The University of Western Australia.Available locally as a zip archive MatlabFns.zip (4.8 MB, updated 21 May 2005)Philip Torr, among many other contributions, submitted a Structure and motion toolkit in Matlab to the MathSoft File Exhange.Local copy here: torrsam.zip (2.4 MB, zip)."}
{"content2":"转自：http://blog.csdn.net/zhubenfulovepoem/article/details/7191794作者：zhubenfulovepoem以下是computer vision：algorithm and application计算机视觉算法与应用这本书中附录里的关于计算机视觉的一些测试数据集和源码站点，我整理了下，加了点中文注解。ComputerVision:Algorithms and ApplicationsRichard Szeliski在本书的最好附录中，我总结了一些对学生，教授和研究者有用的附加材料。这本书的网址http://szeliski.org/Book包含了更新的数据集和软件，请同样访问他。C.1 数据集一个关键就是用富有挑战和典型的数据集来测试你算法的可靠性。当有背景或者他人的结果是可行的,这种测试可能甚至包含更多的信息(和质量更好)。经过这些年，大量的数据集已经被提出来用于测试和评估计算机视觉算法。许多这些数据集和软件被编入了计算机视觉的主页。一些更新的网址，像CVonline(http://homepages.inf.ed.ac.uk/rbf/CVonline ), VisionBib.Com (http://datasets.visionbib.com/ ), and Computer Vision online (http://computervisiononline.com/ ), 有更多最新的数据集和软件。下面，我列出了一些用的最多的数据集，我将它们让章节排列以便它们联系更紧密。第二章：图像信息CUReT: Columbia-Utrecht 反射率和纹理数据库Reﬂectance and TextureDatabase, http://www1.cs.columbia.edu/CAVE/software/curet/  (Dana, van Ginneken, Nayaret al. 1999).Middlebury Color Datasets:不同摄像机拍摄的图像，注册后用于研究不同的摄像机怎么改变色域和彩色registeredcolor images taken by different cameras to study how they transform gamuts andcolors,http://vision.middlebury.edu/color/data/ Chakrabarti, Scharstein, and Zickler 2009).第三章：图像处理Middlebury test datasets forevaluating MRF minimization/inference algorithms评估隐马尔科夫随机场最小化和推断算法,http://vision.middlebury.edu/MRF/results/ (Szeliski, Zabih, Scharstein et al. 2008).第四章：特征检测和匹配Afﬁne Covariant Featuresdatabase（反射协变的特征数据集） for evaluating feature detector and descriptor matching quality andrepeatability（评估特征检测和描述匹配的质量和定位精度）,   http://www.robots.ox.ac.uk/~vgg/research/affine/(Miko-lajczyk and Schmid 2005;Mikolajczyk, Tuytelaars, Schmid et al. 2005).Database of matched imagepatches for learning （图像斑块匹配学习数据库）and feature descriptor evaluation（特征描述评估数据库）,http://cvlab.epfl.ch/~brown/patchdata/patchdata.html(Winder and Brown 2007;Hua,Brown, and Winder 2007).第五章;分割BerkeleySegmentation Dataset（分割数据库） and Benchmark of 1000 images labeled by 30 humans,（30个人标记的1000副基准图像）along with an evaluation,http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/  (Martin, Fowlkes, Tal et al.2001).Weizmann segmentationevaluation database of 100 grayscale images with ground truth segmentations,http://www.wisdom.weizmann.ac.il/~vision/Seg EvaluationDB/index.html(Alpert, Galun, Basri et al. 2007).第八章：稠密运动估计TheMiddlebury optic ﬂow evaluation（光流评估） Web site, http://vision.middlebury.edu/flow/data/(Baker,Scharstein, Lewis et al. 2009).The Human-Assisted MotionAnnotation database,（人类辅助运动数据库）http://people.csail.mit.edu/celiu/motionAnnotation/  (Liu, Freeman, Adelson etal. 2008)第十章：计算机摄像学High DynamicRange radiance（辐射）maps, http://www.debevec.org/Research/HDR/(De-bevecand Malik 1997).Alpha matting evaluation Website, http://alphamatting.com/ (Rhemann, Rother, Wanget al. 2009).第十一章：Stereo correspondence立体对应Middlebury Stereo Datasets andEvaluation, http://vision.middlebury.edu/stereo/  (Scharsteinand Szeliski 2002).StereoClassiﬁcation（立体分类） and Performance Evaluation（性能评估） of different aggregation（聚类） costs for stereo matching（立体匹配）, http://www.vision.deis.unibo.it/spe/SPEHome.aspx  (Tombari, Mat-toccia, Di Stefano et al.2008).Middlebury Multi-View StereoDatasets,http://vision.middlebury.edu/mview/data/  (Seitz,Curless, Diebel etal. 2006).Multi-view and Oxford Collegesbuilding reconstructions,http://www.robots.ox.ac.uk/~vgg/data/data-mview.html .Multi-View Stereo Datasets, http://cvlab.epfl.ch/data/strechamvs/  (Strecha, Fransens,and Van Gool 2006).Multi-View Evaluation,  http://cvlab.epfl.ch/~strecha/multiview/ (Strecha, von Hansen,Van Gool et al. 2008).第十二章：3D重建HumanEva: synchronized video（同步视频） and motion capture （动作捕捉）dataset for evaluation ofarticulated human motion, http://vision.cs.brown.edu/humaneva/  Sigal, Balan, and Black 2010).第十三章：图像渲染The (New) Stanford Light FieldArchive, http://lightfield.stanford.edu/(Wilburn, Joshi,Vaish et al.2005).Virtual Viewpoint Video:multi-viewpoint video with per-frame depth maps,http://research.microsoft.com/en-us/um/redmond/groups/ivm/vvv/  (Zitnick, Kang, Uytten-daele et al. 2004).第十四章：识别查找一系列的视觉识别数据库，在表14.1–14.2.除了那些，这里还有：Buffy pose classes, http://www.robots.ox.ac.uk/~vgg/data/  buffy pose classes/ andBuffystickmen V2.1, http://www.robots.ox.ac.uk/~vgg/data/stickmen/index.html  (Ferrari,Marin-Jimenez, and Zisserman 2009;Eichner and Ferrari 2009).H3D database of pose/jointannotated photographs of humans,http://www.eecs.berkeley.edu/~lbourdev/h3d/   (Bourdev and Malik 2009).Action Recognition Datasets,http://www.cs.berkeley.edu/projects/vision/action, has point-ers toseveral datasets for action and activity recognition, as well as some papers.（有一些关于人活动和运动的数据库和论文） The humanaction database at http://www.nada.kth.se/cvap/actions/  包含更多的行动序列。C.2 软件资源一个对于计算机视觉算法最好的资源就是开源视觉图像库（opencv）(http://opencv.willowgarage.com/wiki/),他有在intel的Gary Bradski和他的同事开发，现在由Willow Garage (Bradsky and Kaehler 2008)维护和扩展。一部分可利用的函数在http://opencv.willowgarage.com/documentation/cpp/中：图像处理和变换 (滤波，形态学，金字塔);图像几何学的变换 (旋转，改变大小);混合图像变换 (傅里叶变换，距离变换);直方图;分割 (分水岭, mean shift);特征检测 (Canny, Harris, Hough, MSER, SURF);运动分析和物体分析 (Lucas–Kanade, mean shift);相机矫正和3D重建机器学习 (k nearest neighbors, 支持向量机, 决策树, boost-ing, 随机树, expectation-maximization, 和神经网络).Intel的Performance Primitives (IPP)library, http://software.intel.com/en-us/intel-ipp/，包含各种各样的图像处理任务的最佳优化代码，许多opencv中的例子利用了这个库，加入他安装了，程序运行得更快。依据功能，他和Opencv有很多相同的运算处理，并且加上了额外的库针对图像视频压缩，信号语音处理和矩阵代数。MTALAB中的Image Processing Toolbox图像处理工具，http://www.mathworks.com/products/image/，包含常规的处理，空域变换（旋转，改变大小），常规正交，图像分析和统计学（变边缘，哈弗变换），图像增强（自适应直方图均衡，中值滤波），图像恢复（去模糊），线性滤波（卷积），图像变换（傅里叶，离散余弦变换）和形态学操作（连通域和距离变换）两个比较旧的库，它们没有被发展，但是包含了一些的有用的常规操作：VXL (C++Libraries for Computer Vision Research and Implemen-tation, http://vxl.sourceforge.net/)LTI-Lib 2 (http://www.ie.itcr.ac.cr/palvarado/ltilib-2/homepage/ ).图像编辑和视图包，例如Windows Live Photo Gallery, iPhoto, Picasa,GIMP, 和 IrfanView，它们对执行这些处理非常有用：常规处理任务，格式转换，观测你的结果。它们同样可以用于对图像处理算法有趣的实现参考，例如色调调整和去噪。这里他也有一些软件包和基础框架对你建一个实时视频处理的DEMOS很有用，Vision on Tap(http://www.visionontap.com/ )提供一个可以实时处理你的网络摄像头的网页服务(Chiu and Raskar 2009）。Video-Man (VideoManager, http://videomanlib.sourceforge.net/处理实时的基于视频的DEMOS和应用非常有用，你也可以用MATLAB中的imread直接从任何URl（例如网络摄像头）中读取视频。下面，我列出了一些额外的网络资源，让章节排列以便它们看起来联系更紧密：第三章:图像处理matlabPyrTools—MATLAB 下的源码对于拉普拉斯变换，金字塔, QMF/小波, 和steerable pyramids, http://www.cns.nyu.edu/~lcv/software.php  (Simoncelli and Adel-son 1990a; Simoncelli,Freeman, Adelson et al. 1992).BLS-GSM 图像去噪, http://decsai.ugr.es/~javier/denoise/  (Portilla, Strela,Wain-wright et al. 2003).Fast bilateral ﬁltering code（快速双边滤波）, http://people.csail.mit.edu/jiawen/#code (Chen, Paris, and Durand 2007).C++ implementation of the fastdistance transform algorithm,http://people.cs.uchicago.edu/~pff/dt/  (Felzenszwalb andHuttenlocher 2004a).GREYC’s Magic Image Converter,including image restoration software using regularization and anisotropicdiffusion, http://gmic.sourceforge.net/gimp.shtml (Tschumperl´ e and Deriche 2005).第四章：图像特征检测和匹配VLFeat, 一个开放便捷的计算机视觉算法库http://vlfeat.org/ (Vedaldi and Fulkerson 2008).SiftGPU: A GPU Implementationof Scale Invariant Feature Transform (SIFT),GPU实现的尺度特征性变换http://www.cs.unc.edu/~ccwu/siftgpu/  (Wu 2010).SURF: Speeded Up RobustFeatures, http://www.vision.ee.ethz.ch/~surf/(Bay, Tuyte-laars, and VanGool 2006).FAST corner detection, http://mi.eng.cam.ac.uk/~er258/work/fast.html(Rosten and Drum-mond 2005, 2006).Linux binaries for afﬁneregion detectors and descriptors, as well as MATLAB ﬁles tocompute repeatability andmatching scores,http://www.robots.ox.ac.uk/~vgg/research/affine/Kanade–Lucas–Tomasi featuretrackers: KLT, http://www.ces.clemson.edu/~stb/klt/ (Shi and Tomasi 1994);GPU-KLT, http://cs.unc.edu/~cmzach/opensource.html  (Zach,Gallup, and Frahm2008); Lucas–Kanade 20 Years On, http://www.ri.cmu.edu/projects/project 515.html  (Baker and Matthews 2004).第五章：分割高效的基于图形的分割http://people.cs.uchicago.edu/~pff/segment(Felzenszwalb and Huttenlocher2004b).EDISON, 边缘检测和图像追踪,http://coewww.rutgers.edu/riul/research/code/EDISON/(Meer and Georgescu 2001; Comaniciu and Meer2002).Normalized cuts segmentationincluding intervening contours,http://www.cis.upenn.edu/~jshi/software/(Shi and Malik 2000; Malik,Belongie, Leung et al. 2001).Segmentation by weightedaggregation (SWA),利用加权集合的分割http://www.cs.weizmann.ac.il/~vision/SWA  (Alpert, Galun, Basri et al.2007).第六章：基于特征的对齐和校准Non-iterative PnP algorithm,（非迭代PnP算法）http://cvlab.epﬂ.ch/software/EPnP  (Moreno-Noguer, Lep-etit, and Fua 2007).Tsai Camera Calibration（相机矫正） Software,http://www-2.cs.cmu.edu/~rgw/TsaiCode.html  (Tsai 1987).Easy CameraCalibration Toolkit,（简易相机校准工具包） http://research.microsoft.com/en-us/um/people/zhang/ Calib/ (Zhang 2000).Camera Calibration Toolbox forMATLAB,http://www.vision.caltech.edu/bouguetj/calib doc/ ; a C version is included in OpenCV.MATLAB functions for multipleview geometry,http://www.robots.ox.ac.uk/~vgg/hzbook/code/  (Hartley and Zisserman2004).第七章：运动重建SBA: A generic sparse bundle(稀疏束) adjustment C/C++ package basedon the Levenberg–Marquardt algorithm, http://www.ics.forth.gr/~lourakis/sba/  (Lourakis and Argyros 2009).Simple sparse bundleadjustment (SSBA), http://cs.unc.edu/~cmzach/opensource.html .Bundler, structure from motionfor unordered image collections(无序图像集),http://phototour.cs.washington.edu/bundler/   (Snavely, Seitz, and Szeliski 2006).第八章:稠密运动估计光流, http://www.cs.brown.edu/~black/code.html (Black and Anan-dan 1996).Optical ﬂow（光流） using total variation（全变量差） and conjugate gradientdescent（共轭梯度下降）, http://people.csail.mit.edu/celiu/OpticalFlow/  (Liu 2009).TV-L1 optical ﬂow on the GPU, http://cs.unc.edu/~cmzach/opensource.html(Zach,Pock, and Bischof2007a).elastix: atoolbox for rigid（刚性） and nonrigid（非刚性） registration of images（配准图像）, http://elastix.isi.uu.nl/ (Klein, Staring, and Pluim 2007).Deformable image registration（可变形的配准图像） using discreteoptimization（离散最优化）, http://www.mrf-registration.net/deformable/index.html(Glocker, Komodakis, Tziritas et al. 2008).第九章：图像缝合Microsoft Research ImageCompositing Editor for stitching images,（图像拼接，图像合成）http://research.microsoft.com/en-us/um/redmond/groups/ivm/ice/ .第十章：计算机摄影学HDRShop software for combiningbracketed exposures（包围式曝光） into high-dynamic range radiance images, http://projects.ict.usc.edu/graphics/HDRShop/.Super-resolution（超分辨率） code,http://www.robots.ox.ac.uk/~vgg/software/SR/  (Pickup 2007;Pickup, Capel,Roberts et al. 2007, 2009).第十一章：立体对应StereoMatcher, standalone C++stereo matching code,http://vision.middlebury.edu/stereo/code/  (Scharstein and Szeliski2002).Patch-based multi-view stereosoftware (PMVS Version 2),http://grail.cs.washington.edu/software/pmvs/  (Furukawa and Ponce 2011).第十二章：3D重建Scanalyze: a system foraligning and merging range data,http://graphics.stanford.edu/software/scanalyze/  (Curless and Levoy 1996).MeshLab: software forprocessing, editing, and visualizing unstructured 3D triangularmeshes, http://meshlab.sourceforge.net/.VRML viewers (various) arealso a good way to visualize texture-mapped 3D models.节 12.6.4: Whole body modeling andtracking（全身建模和追踪）Bayesian 3D person tracking（贝叶斯3D人体追踪）, http://www.cs.brown.edu/~black/code.html  (Sidenbladh,Black, and Fleet2000; Sidenbladh and Black 2003).HumanEva: baseline code forthe tracking of articulated human motion,http://vision.cs.brown.edu/humaneva/   (Sigal, Balan, and Black 2010).节 14.1.1: Face detection（人脸检测）Sample face detection code andevaluation tools,http://vision.ai.uiuc.edu/mhyang/face-detection-survey.html.节 14.1.2: Pedestrian detection（行人追踪）A simple object detector withboosting,http://people.csail.mit.edu/torralba/shortCourseRLOC/boosting/boosting.html(Hastie, Tibshirani, and Friedman 2001;Torralba, Murphy, and Freeman 2007).Discriminatively（有区别） trained deformable（可变形） part models, http://people.cs.uchicago.edu/~pff/latent/  (Felzenszwalb, Girshick,McAllester et al. 2010).Upper-body detector（上身检测）,http://www.robots.ox.ac.uk/~vgg/software/UpperBody/  (Ferrari,Marin-Jimenez, andZisserman 2008).2D articulated human poseestimation software,http://www.vision.ee.ethz.ch/~calvin/articulated_human_pose_estimation_code/  (Eichner and Ferrari 2009).节 14.2.2: Active appearance and 3Dshape modelsAAMtools: An active appearancemodeling toolbox,http://cvsp.cs.ntua.gr/software/AAMtools/  (Papandreou and Maragos2008).节 14.3: Instance recognitionFASTANN and FASTCLUSTER forapproximate k-means (AKM),http://www.robots.ox.ac.uk/~vgg/software/ (Philbin, Chum, Isard et al. 2007).Feature matching using fastapproximate nearest neighbors,http://people.cs.ubc.ca/~mariusm/index.php/FLANN/FLANN  (Muja and Lowe 2009).节 14.4.1: Bag of words(词袋)Two bag of words classiﬁers, http://people.csail.mit.edu/fergus/iccv2005/bagwords.html(Fei-Fei and Perona 2005;Sivic, Russell, Efros et al. 2005).Bag of features andhierarchical（分层） k-means, http://www.vlfeat.org/  (Nist´ er and Stew´enius2006; Nowak, Jurie, and Triggs 2006).节 14.4.2: Part-based modelsA simple parts and structureobject detector,http://people.csail.mit.edu/fergus/iccv2005/partsstructure.html(Fischler and Elschlager 1973; Felzenszwalband Huttenlocher 2005).节 14.5.1: Machine learning softwareSupport vector machines (SVM)software (http://www.support-vector-machines.org/SVM soft.html )包含很多支持向量机的库,SVMlight http://svmlight.joachims.org/ ;LIBSVM, http://www.csie.ntu.edu.tw/~cjlin/libsvm/(Fan, Chen,and Lin 2005);LIBLINEAR, http://www.csie.ntu.edu.tw/~cjlin/liblinear/  (Fan,Chang, Hsieh et al.2008).Kernel Machines: links to SVM,Gaussian processes, boosting, and other machinelearning algorithms, http://www.kernel-machines.org/software .Multiple kernels for imageclassiﬁcation,http://www.robots.ox.ac.uk/~vgg/software/MKL(Varma and Ray 2007; Vedaldi, Gulshan, Varmaet al. 2009).附录 A.1–A.2: Matrix decompositions（矩阵分解） and linear least squares（线性最小乘）BLAS (BasicLinear Algebra Subprograms基本线性代数子程序),http://www.netlib.org/blas/  (Blackford,Demmel, Dongarraet al. 2002).LAPACK (Linear Algebra（线性代数） PACKage),http://www.netlib.org/lapack/  (Anderson, Bai,Bischof etal. 1999).GotoBLAS, http://www.tacc.utexas.edu/tacc-projects/.ATLAS (Automatically TunedLinear Algebra Software),http://math-atlas.sourceforge.net/  (Demmel, Dongarra, Eijkhoutet al. 2005).Intel Math Kernel Library(MKL), http://software.intel.com/en-us/intel-mkl/.AMD CoreMath Library (ACML),http://developer.amd.com/cpu/Libraries/acml/Pages/default.aspx .Robust PCA code（鲁棒主成分分析）, http://www.salle.url.edu/~ftorre/papers/rpca2.html(De la Torre and Black 2003).Appendix A.3: Non-linear leastsquares非线性最小二乘MINPACK, http://www.netlib.org/minpack/.levmar: Levenberg–Marquardtnonlinear least squares algorithms, 非线性最小二乘http://www.ics.forth.gr/~lourakis/levmar/  (Madsen, Nielsen, andTingleff 2004).附录 A.4–A.5: Direct（直接） and iterative（迭代） sparse matrix（稀疏矩阵） solversSuiteSparse (variousreordering algorithms, 各种各样的重排算法CHOLMOD) and SuiteSparse QR, http://www.cise.ufl.edu/research/sparse/SuiteSparse/  (Davis 2006, 2008).PARDISO (iterative and sparsedirect solution),  http://www.pardiso-project.org/.TAUCS (sparse direct,iterative, out of core, preconditioners),http://www.tau.ac.il/~stoledo/taucs/ .HSL Mathematical SoftwareLibrary,  http://www.hsl.rl.ac.uk/index.html .Templatesfor the solution of linear systems（线性系统解决问题的模板）, http://www.netlib.org/linalg/html templates/Templates.html  (Barrett, Berry, Chan et al.1994). Download the PDF for instructions（说明） on how to get the software.ITSOL,MIQR, and other sparsesolvers,http://www-users.cs.umn.edu/~saad/software/  (Saad 2003).ILUPACK, http://www-public.tu-bs.de/~bolle/ilupack/ .附录 B: Bayesian modeling and inference（贝叶斯建模和推断）Middleburysource code for MRF minimization（隐马尔科夫随机场最小化）, http://vision.middlebury.edu/MRF/code/  (Szeliski, Zabih, Scharsteinet al. 2008).C++ code for efﬁcient beliefpropagation for early vision,http://people.cs.uchicago.edu/~pff/bp/  (Felzenszwalb andHuttenlocher 2006).FastPD MRF optimization（最优化） code,http://www.csd.uoc.gr/~komod/FastPD  (Komodakisand Tziritas2007a; Komodakis, Tziritas, and Paragios 2008)算法 C.1   Calgorithm for Gaussian random noise generation, using the Box–Mullertransform.C描述的利用Box–Muller 变换产生高斯随机噪声double urand(){return ((double)rand()) / ((double) RAND MAX);}void grand(double& g1, double& g2){#ifndef M_PI#define M_PI 3.14159265358979323846#endif // M_PIdouble n1 = urand();double n2 = urand();double x1 = n1 + (n1 == 0); /* guardagainst log(0) */double sqlogn1 = sqrt(-2.0 * log (x1));double angl = (2.0 * M PI) * n2;g1 = sqlogn1 * cos(angl);g2 = sqlogn1 * sin(angl);}高斯噪声的产生。许多基本的软件包产生一些不同的随机的噪声(例如 运行在unix上的rand())，但是并不是所有的都有高斯随机噪声发生器。计算一个离散随机常量，你可以用Box–Mullertransform (Box and Muller 1958)，他的c代码在算法C.1中给出了，注意这个运行结果是返回一对随机变量。相关的产生高斯随机变量的方由Thomas, Luk, Leong et al. (2007)提出。伪彩色产生。在很多应用中，很方便给图像加上标记（或者给图像特征比如线）。一个最简单的方式就是给不同的标记不同的颜色。在我的工作中，我发现用RGB立体色彩系给不同的标记赋予标准均匀的色彩是很方便的。对于每一个（非消极）标记值，considerthe bits as being split among the three color channel，例如对于一个比特值为9的值，这个值可以被标记为RGBRGBRGB，获得三基色中的每一种颜色值后，颠倒比特值，结果是低位的比特值变化的最快。实际上，对于一个八比特的颜色通道，这个比特值的颠倒可以被存在一个表或者一个存储提前计算好的记录有由标记值向伪彩色的改变的完整表。图 8.16 显示了这样一个伪彩色绘制的例子.GPU实现GPU的出现，可以处理像素着色和计算着色，导致了实时应用的快速计算机视觉算法的发展，例如，分割，追踪，立体和运动估计（(Pock, Unger, Cremerset al. 2008; Vineet and Narayanan 2008; Zach,Gallup, and Frahm 2008）。一个好的资源来学习这些算法就是CVPR 2008 上关于Visual Computer Visionon GPUs的workshop。http://www.cs.unc.edu/~jmf/Workshop_on_Computer_Vision_on_GPU.html他的论文可以在CVPR2008的会议集的DVD中找到。额外的关于GPU算法资源包括GPGPU网址和小组讨论http://gpgpu.org/还有OpenVIDIAWeb site, http://openvidia.sourceforge.net/index.php/OpenVIDIAC.3 PPT和讲稿正如我在前言中提到的，我希望提供和书中材料相一致的PPT，直到这些全部准备好，你最好的方式去看我在华盛顿大学上课时的PPT，和一写相关课程中用到的教案。这里是一些这样的课程列表：UW 455:Undergraduate Computer Vision,http://www.cs.washington.edu/education/courses/455/.UW576:Graduate Computer Vision,http://www.cs.washington.edu/education/courses/576.StanfordCS233B: Introduction to Computer Vision,http://vision.stanford.edu/teaching/cs223b/.MIT6.869: Advances in Computer Vision,http://people.csail.mit.edu/torralba/courses/6.869/6.869.computervision.htm.Berkeley CS 280: Computer Vision, http://www.eecs.berkeley.edu/~trevor/CS280.htmlUNC COMP776: Computer Vision, http://www.cs.unc.edu/~lazebnik/spring10.Middlebury CS 453: Computer Vision,http://www.cs.middlebury.edu/~schar/courses/cs453-s10/.Related courses have also been taught onthe topic of Computational Photography, e.g.,CMU 15-463: Computational Photography, http://graphics.cs.cmu.edu/courses/15-463/.MIT 6.815/6.865: Advanced ComputationalPhotography,http://stellar.mit.edu/S/course/6/sp09/6.815Stanford CS 448A: Computational photographyon cell phones,http://graphics.stanford.edu/courses/cs448a-10/.SIGGRAPH courses on ComputationalPhotography,http://web.media.mit.edu/~raskar/photo/.这里还有一些最好的关于各种计算机视觉主题的在线讲稿，例如：belief propagation and graph cuts，它们在UW-MSR Course of Vision Algo-rithms http://www.cs.washington.edu/education/courses/577/04sp/C.4 参考文献：这本的所有参考文献在这本书的网站上，一个几乎所有的计算机视觉的出版物都引用的更全面的部分注解书目由Keith Price维http://iris.usc.edu/Vision-Notes/bibliography/contents.html.这里还有一个可搜索的计算机图形学的参考书目http://www.siggraph.org/publications/bibliography/另外技术论文比较好的资源是GoogleScholar 和 CiteSeerX。"}
{"content2":"转载自：http://emuch.net/html/201012/2659795.html原帖中有丰富讨论。看了版上很多贴子，发现很多版友都在问“热门研究方向”、“最新方法”等。有同学建议国内某教授的教材、或者CNKI、或者某些SCI期刊。每当看到这种问题，我都有点纳闷，为什么不去读顶级会议上的论文？我无意否认以上文献的价值，但是在机器学习、计算机视觉和人工智能领域，顶级会议才是王道。国内教材和CNKI上的基本是N年前老掉牙的东西。有人会质疑这些会议都只是EI。是的，这的确非常特殊：在许多其它领域，会议都是盛会，比如society of neuroscience的会议，每次都有上万人参加，带个abstract和poster就可以去。但在所讨论的几个领域，顶级会议的重要性无论怎么强调都不为过。可以从以下几点说明：（1）因为机器学习、计算机视觉和人工智能领域发展非常迅速，新的工作层出不穷，如果把论文投到期刊上，一两年后刊出时就有点out了。因此大部分最新的工作都首先发表在顶级会议上，这些顶级会议完全能反映“热门研究方向”、“最新方法”。（2）很多经典工作大家可能引的是某顶级期刊上的论文，这是因为期刊论文表述得比较完整、实验充分。但实际上很多都是在顶级会议上首发。比如PLSA, Latent Dirichlet Allocation等。（3）如果注意这些领域大牛的pulications，不难发现他们很非常看重这些顶级会议，很多人是80%的会议+20%的期刊。即然大牛们把最新工作发在顶级会议上，有什么理由不去读顶级会议？(1)以下是不完整的列表，但基本覆盖。机器学习顶级会议：NIPS, ICML, UAI, AISTATS;  （期刊：JMLR, ML, Trends in ML, IEEE T-NN）计算机视觉和图像识别：ICCV, CVPR, ECCV;  （期刊：IEEE T-PAMI, IJCV, IEEE T-IP）人工智能：IJCAI, AAAI; （期刊AI）另外相关的还有SIGRAPH, KDD, ACL, SIGIR, WWW等。特别是，如果做机器学习，必须地，把近4年的NIPS, ICML翻几遍；如果做计算机视觉，要把近4年的ICCV, CVPR, NIPS, ICML翻几遍。(2)另外补充一下：大部分顶级会议的论文都能从网上免费下载到，比如CV方面：http://www.cvpapers.com/index.html; NIPS: http://books.nips.cc/;  JMLR(期刊): http://jmlr.csail.mit.edu/papers/;  COLT和ICML(每年度的官网): http://www.cs.mcgill.c ... ceedings.html。希望这些信息对大家有点帮助。(3)说些自己的感受。我的研究方向主要是统计学习和概率图模型，但对计算机视觉和计算神经科学都有涉及，对Data mining和IR也有些了解。这些领域，从方法和模型的角度看，统计模型（包括probabilistic graphical model和statistical learning theory）是主流也是非常有影响力的方法。有个非常明显的趋势：重要的方法和模型最先在NIPS或ICML出现，然后应用到CV,IR和MM。虽然具体问题和应用也很重要，但多关注和结合这些方法也很有意义。"}
{"content2":"这个哥们 总结的太好了， 我忍不住就“偷”过来了人工智能是一门极富挑战性的科学，从事这项工作的人必须懂得计算机知识，心理学和哲学。人工智能是包括十分广泛的科学，它由不同的领域组成，如机器学习，计算机视觉等等，总的说来，人工智能研究的一个主要目标是使机器能够胜任一些通常需要人类智能才能完成的复杂工作。看图：：数学基础：人工智能：特征工程：机器学习：深度学习最后将大侠的名字留上：知慧小狐地址：https://mp.weixin.qq.com/s?__biz=MzUzMjk3MTIwMQ==&mid=2247483809&idx=1&sn=1ead74c60ec24ec9e2db53a147f639f1&chksm"}
{"content2":"原文：http://www.cnblogs.com/moondark/archive/2012/04/20/2459594.html感谢水木上同领域的同学分享，有了他的整理，让我很方便的获得了CV方面相关的经典论文，我也顺便整理一下，把pdf中的文字贴到网页上，方便其它人更直观的获取所要内容~~~资料的下载链接为：      http://iask.sina.com.cn/u/2252291285/ish?folderid=775855以下为该同学的整理的综述：“前言：最近由于工作的关系，接触到了很多篇以前都没有听说过的经典文章，在感叹这些文章伟大的同时，也顿感自己视野的狭小。  想在网上找找计算机视觉界的经典文章汇总，一直没有找到。失望之余，我决定自己总结一篇，希望对 CV 领域的童鞋们有所帮助。由于自己的视野比较狭窄，肯定也有很多疏漏，权当抛砖引玉了，如果你觉得哪篇文章是非常经典的，也可以把相关信息连带你的昵称发给我，我好补上。我的信箱 xdyang.ustc@gmail.com文章主要来源：PAMI, IJCV, TIP, CVIU, PR, IVC, CVGIU, CVPR, ICCV, ECCV, NIPS, SIGGRAPH, BMVC等主要参考网站: Google scholar, citeseer, cvpapers, opencv 中英文官方网站主要参考书籍：数字图像处理  第三版  冈萨雷斯等图像处理，分析和机器视觉  第三版  Sonka等（非常非常好的一本书）学习OpenCV计算机视觉：算法与应用文章按时间排序，排名不分先后，^_^。每一行最后一栏是我自己加的注释，如果不喜欢可以无视之，如果有不对的地方还请告诉我，免得继续出丑。 给出的文章有些是从google scholar或者citeseer上拷贝下来的，所以有链接。所有的文章在网上都很容易找到。有空的时候我会把它们全部整理出来，逐步上传到ishare.iask.sina.com由于整理的很仓促，时间也很短，还有很多不完善的地方。我会不断改进，并不时上传新版本。上传地址为http://iask.sina.com.cn/u/2252291285/ish?folderid=775855最后更新：2012/3/141990 年之前Peter Burt, Edward AdelsonThe  Laplacian  Pyramid  as  A Compact Image Code虽说这个Laplacian Pyramid是有冗余的，但使用起来非常简单方便，对理解小波变换也非常有帮助。这位Adelson是W.T.Freeman的老板，都是大牛.J CannyA Computational Approach to Edge Detection经典不需要解释。在 Sonka的书里面对这个算法也有比较详细的描述。S Mallat.A  theory  for  multiresolution  signal decomposition:  The  wavelet representationMallat的代表作M Kass, A Witkin, D  Terzopoulos.Snakes: active contour modelsDeformable model的开山鼻祖。RM HARALICKTextural Features for Image Classiﬁcation这三篇都是关于纹理特征的，虽然过去这么多年了，现在在检索和识别中依然很有用。RM HARALICKStatistical and structural approachesTamura等Texture features corresponding to visual perceptionA P Dempster, N M Laird, D B Rubin. 1977Maximum  likelihood  from  incomplete data via the EM algorithmEM 算法在计算机视觉中有着非常重要的作用L Rabiner. 1989A Tutorial on Hidden Markov Models and Selected Applications in Speech RecognitionHMM 同样是计算机视觉必须掌握的一项工具B D Lucas, T KanadeAn  iterative  image  registration technique  with  an  application  to stereo- visionLucas 光流法J R QuinlanInduction of decision trees偏模式识别和机器学习一点1990 年P Perona, J Malik. PAMIScale-space and edge detection using anisotropic diffusion关于 scale space 最早的一篇论文之一，引用率很高T LindebergScale-space for discrete signals.Lindeberg 关于 scale space 比较早的一篇，后续还有好几篇anzad, A.; Hong, Y.H.Invariant image recognition by Zernike momentsZernike moment,做过模式识别或者检索的应该都知道这个东东1991 年W Freeman, E Adelson.The design and use of steerable filtersFreeman最早的一篇力作，也是我读的第一篇学术论文。现在Freeman在 MIT 风生水起，早已是IEEE Fellow了Michael J. Swain , Dana H. BallardColor Indexing.google scholar 上引用将近五千次MA Turk CVPRFace recognition using eigenfaces1992 年L G Brown.A survey of image registration techniques.比较早的一篇关于配准的综述了1993 年S G Mallat, Z Zhang.Matching pursuits with time-frequency dictionariesMallat另一篇关于小波的文章，不研究小波的可以无视之L Vincent.Morphological grayscale reconstruction in image analysis: Applications and efficient algorithmsDP HuttenlocherComparing images using the Hausdorff distanceGoogle scolar 上引用2200多次1994 年J Shi, C Tomasi.Good feature to track.Tomasi这个名字还会出现好几次，真的很牛LinderbergScale-space theory in computer visionJ L Barron, D J Fleet, S S  Beauchemin.Performance  of  optical  flow techniques.1995 年R Malladi, J Sethian, B Vemuri.Shape Modeling with Front Propagation: A Level Set ApproachLevel set的经典文章TF COOTESActive Shape Models-Their Training and ApplicationASMMA StrickerSimilarity of color images颜色检索相关C Cortes, V Vapnik.Support-vector networks.SVM 在计算机视觉中也有着非常重要的地位1996 年T MCINERNEY.Deformable models in medical image analysis: A survey活动模型的一篇较早的综述Tai Sing LeeImage Representation Using 2D Gabor WaveletsGoogle引用也有近千次Amir Said,  A. PearlmanA New, Fast, and Efﬁcient  Image Codec Based on Set Partitioning in Hierarchical TreeSPIHT。图像压缩领域与 EBCOT齐名的经典算法。L P Kaelbling, M L Littman, A W Moore.Reinforcement learning: A survey机器学习里面的一篇综述，引用率比较高，就列在这了。B. S. Manjunath and W. Y. MaTexture features for browsing and retrieval of image data检索的文章比较多，其实它们的应用不仅仅是检索。只要是需要提取特征的地方，检索里面的方法都可以用到comparing images using color coherence vectors检索中的CCV方法Image retrieval using color and shape关于形状特征后面有一篇综述1997 年V Caselles, R Kimmel, G Sapiro.Geodesic active contours活动轮廓模型的一个小分支R E Schapire, Y Freund, P Bartlett, W S Lee.Boosting the Margin: A New Explanation for the Effectiveness of Voting Methods.Schapire 和 Freund 发 明 了Adaboost，给计算机视觉带来了不少经典算法F Maes, D Vandermeulen, G Marchal, P Suetens.Multimodality  image registration by maximization of mutual information互信息量配准E Osuna, R Freund, F Girosi.Training support vector machines: An application to face detection.SVM在人脸检测中的应用。不过人脸检测最经典的方法应 该是Viola-JonesJ Huang, S Kumar, M Mitra, W-J Zhu, R Zabih.Image indexing using color correlogramColor correlogram，检索中的又一个颜色特征。和前面的 CCV 以及颜色矩特征基本上覆盖了所有的颜色特征。Y Freund, R Schapire.A  decisiontheoretic  generalization  of on-line learning and an application to boosting.Adaboost的经典文章1998 年1998 年是图像处理和计算机视觉经典文章井喷的一年。大概从这一年开始，开始有了新的趋势。由于竞争的加剧，一些好的算法都先发在会议上了，先占个坑，等过一两年之后再扩展到会议上。T LindebergFeature detection with automatic scale selectionLinderberg的 scale space到此为止基本结束了。在一些边缘提取，道路或者血管检测中，scale space 确实是一种很不错的工具C J C Burges.A tutorial on support vector machines for pattern recognition.使用 svm的话，这篇文章应该是必读的了。比 95 年那篇原始文章引用率还高M Isard, A Blake.CONDENSATION  –  Conditional TrackingDensity Propagation for VisualTracking中的经典文章了L Page, S Brin, R Motwani, T  WinogradThe PageRank citation ranking: bringing order to the web这篇文章应该不属于 CV 的范畴，鉴于作者的大名鼎鼎，暂且列在这C Tomasi, R Manduchi.Bilateral filtering for gray and color images.做过图像滤波平滑去噪或者 HDR的应该都知道Bilateral filter。原理非常非常简单，简单到一个公式就可以概括这篇文章，简单到实在无法扩充到期刊。这也是 Tomasi 第二次出现了。一直很纳闷，这个很直观的思想在这之前怎么就从来没人提呢。C  Xu, J L Prince.Snakes, shapes and gradient vector flow.终于碰到中国人写的文章了，很荣幸还是校友。GVF是 snake和levelset领域的重要分支和方法Wim Sweldens.The lifting scheme: A construction of second generation wavelets.第二代小波。真正让小波有了实用价值，在 JPEG2000 中就采用的提升小波。个人更喜欢的是下一篇，简单易懂，字体也大Daubechies Wim SweldensFactoring wavelet transforms into lifting steps另一位作者也很牛，小波十讲的作者H A Rowley, S Baluja, T Kanade.Neural Network-based Face Detection.做人脸的应该是必看的了。不做人脸的话应该可以不用看吧J B A Maintz, M A Viergever.A survey of medical image registration.关于图像配准的另一篇综述T F Cootes, G J Edwards, C J Taylor.Active Appearance ModelsAAM1999 年D Lowe.Object Recognition from Local Scale-invariant Features大名鼎鼎的SIFT，后面有一篇IJCV上的 Journal版本，更全面一点。R E Schapire.A brief Introduction to Boosting还是 boostingD M Gavrila.The visual analysis of human movements: a survey综述文章的引用一般都比较高Y Rui, T S Huang, S F Change.Image retrieval: current techniques, promising directions, and open issuesTSHuang小组对检索的一个总结J K Aggarwal, Q Cai.Human motion analysis: a review人体运动分析的一个综述2000 年世纪之交，各种综述都出来了J Shi, J Malik.Normalized Cuts and Image SegmentationNCuts的引用率相当高，Jianbo Shi也因为这篇文章成为计算机视觉界引用率最高的作者之一Z Zhang.A Flexible New Technique for Camera Calibration张正友的关于摄像机标定的经典短文A K Jain, R P W Duin, J C Mao.Statistical pattern  recognition: a review.统计模式识别综述，这一年 pami上两篇很有名的综述之一。 在这里推荐 Web 写的 Statistical Pattern Recognition第三版，相当不错，网上有电子版。C StauffeLearning Patterns of Activity Using Real-Time Tracking搜 TLD 的时候发现这篇文章引用率也很高，两千多次。还没来得及读。D Taubman.High performance Scalable Image Compression With EBCOTEBCOT，JPEG2000 中的算法A W M Smeulders, M Worring, S Santini, A Gupta, R Jain.Content-based image retrieval at the end of the early years在世纪之交对图像检索的一篇很权威的综述。感觉在这之后检索的研究也没那么热了。不过在工业界热度依旧，各大网上购物平台，比如淘宝，  亚马逊，京东等都在做这方面的研发，衣服检索是一个很不错的应用点。M Pantic, L J M Rothkrantz.Automatic analysis of facial expressions: the state of the art.N Paragios, R Deriche.Geodesic active contours and  level sets for the detection and tracking of moving objects使用 level set做跟踪Y Rubner, C Tomasi, L Guibas.TThe earth mover’s distance as a metric for image retrieval.EMD算法。Tomasi再次出现PicToSeek Combining Color and Shape Invariant Features for Image Retrieval依然是检索特征2001 年Paul Viola, Michael J Jones.Robust real-time object detection这是一篇很牛的文章，在人脸检测上几乎成了标准。比较坑爹的是，号称发在IJCV2001 上，但怎么找也找不到。应该是 IJCV2004年的那篇“Robust real-time face detection”吧。 他们在这一年另一篇比较出名的文章是在CVPR上的“Rapid ObjectDetection using a Boosted Cascade of Simple Features”这篇才是04年那篇著名文章的会议版。Y Boykov, Kolmogorov.An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision.俄罗斯人在 graph cut 领域开始发力了T Moeslund, E Granum.A Survey of Computer Vision Based Human Motion Capture人体运动综述T F Chan, L Vese.Active contours without edges.Snake 和 level set领域的经典文章A M Martinez, A C Kak.PCA versus LDAPCA 也是计算机视觉中非掌握不可的工具。LDA在模式识别中有很重要的地位BS ManjunathColor and texture descriptors颜色和纹理的描述子，在识别中很有用2002 年D Comaniciu, P Meer.Mean  shift:  A  robust  approach toward feature space analysis.Mean shift的经典文章。前两天发现 Comaniciu 已经是 IEEE Fellow了Ming-Husan Yang, David J Kriegman, Narendra Ahuja.Detecting  Faces  in  Images:  A Survey.人脸检测综述，引用率想不高都难R Hsu, M Abdel-Mottaleb.Face Detection in Color Images.依然是人脸检测，名字都起得这么霸气J-L Starck, E J Candès, D L Donoho.The  curvelet  transform  for  image denoising.Geometrical wavelet 中的一篇代表 作 。 其 他 的 如 ridgelet, contourlet, bandelet 等在这里就不赘述了。研究这方面的很容易找到这方面的经典文献。个人以为不研究这方面的看了后对自己的研究也不会有多大启发。曾经以为这个方向会很火，到最后还是没火起来。  我觉得原因可能是现在存储和传输能力的大大提高，使得对压缩的需求没有那么大了，这方面的研究自然就停滞了，就如同JPEG2000没有成气候Shape matching and object recognition using shape contextsShape context。用形状匹配达到目标识别目的。这方面最经典的文章了。随后后续也有一些这方面的文章，但基本都是很小的改进或者应用。作者提供了原码，可以在 matlab上运行看看效果。N Paragios, R Deriche.Geodesic  active  regions  and  level set methods  for  supervised  texture segmentationStatistical Color Models with Application to Skin DetectionA tutorial on particle filters for online nonlinear non-Gaussian Bayesian trackingparticle filter 的一个综述2003 年W Zhao, R Chellappa, P J Phillips, A Rosenfeld.Face recognition: A literature survey.人脸检测的综述J Sivic, A Zisserman.Video  Google:  A  text  retrieval approach  to  object  matching  in videos.好像是Visual words的起源文章。引用率很高，先列出来再看。D Comaniciu, V Ramesch, P Meer.Kernel-Based Object Tracking.基于核的跟踪。B Zitová, J Flusser.Image  registration  methods:  A survey.又一篇图像配准的综述。做图像配准的比较有福气，综述很多K Mikolajczyk, C Schmid.A  performance  evaluation  of  local descriptors.比较各种描述子的，包括SIFTM J Wainwright, M I Jordan.Graphical  models,  exponential families, and variational inference.乔丹的名气太大，不露露脸说不过去J Portilla, V Strela, M Wainwright, E Simoncelli.Image  denoising  using  scale mixtures of gaussians  in  the wavelet domain.图像去噪，小波变换，混合高斯Robert E. SchapireThe Boosting Approach to Machine Learning  An Overviewboosting作者自己写的综述，自然值得一看。2004 年Lucas-Kanade 20 Years On A Unifying Framework引用文章摘要的第一句话Since the Lucas-Kanade algorithm was proposed in 1981 image alignment has become one of the most widely used techniques in computer vision. Applications range from optical flow and tracking to layered motion,  mosaic construction, and face coding.D G Lowe.Distinctive  image  features  from scale-invariant keypoints.SIFT，不解释Chih-ChungChang,Chih-Jen Lin.LIBSVM: A  library  for support vectormachines我实在怀疑引用这篇文章的人是否都看过这篇文章。貌似不看这篇文章也可以使用 LIBSVMZ Wang, A C Bovik, H R Sheikh, E P Simoncelli.Image  quality  assessment:  From error visibility to structural similarity图像质量评价，最近 Bovik 还有一篇类似的文章也刊登在 TIP上Y Ke, R Sukthankar.Pca-sift:  a  more  distinctive representation  for  local  image descriptorsSIFT 的变形Review of shape representation and description techniquesEfficient Graph-Based Image Segmentation2005 年N Dalal, B Triggs.Histograms  of  oriented  gradients  for human detection.HOG  虽然很新，但很经典A C Berg, T L Berg, J Malik.Shape  matching  and  object recognition  using  low  distortion correspondences.还是 shape matchingS Roth, M Black.Fields  of  experts:  A  framework  for learning image priors.这篇应该要归结到图像统计特性的范畴吧Z Tu, X Chen,A L Yuille, S C Zhu.Image  parsing:  Unifying segmentation,  detection,  and recognition.Geodesic active regions and level set methods for motion  estimation and trackingChunming Li, Chenyang Xu, Changfeng Gui, and  Martin D. FoxLevel Set Evolution Without Re-initialization: A New Variational Formulation这篇文章解决了level set中需要不停的重初始化的问题。在 2010 年的 TIP上有一篇 Journal版本 Distance Regularized Level Set Evolution and its Application to Image SegmentationA Performance Evaluation of Local Descriptors前面那篇是会议的，这篇是 PAMI上的。比较各种描述子的，包括SIFT2006 年D Donoho.Compressed sensing.CS  压缩感知  最近很火的一个名词Greg Welch, Gary Bishop.An introduction to the Kalman Filter.kalman滤波S Lazebnik, C Schmid, J Ponce.Beyond  bags  of  features:  spatial pyramid  matching  for  recognizing natural scene categories.Visual wordsXiaojin Zhu.Semi-supervised  learning  literature survey.A Yilmaz, O Javed, M Shah.Object Tracking: A survey.tracking的一篇综述Image Alignment and Stitching: A Tutorial2007 年A Review of Statistical Approaches to Level Set Segmentation: Integrating Color, Texture, Motion and ShapeThe Appearance of Human Skin: A SurveyLocal Invariant Feature Detectors: A Survey2008 年H Bay, A Ess, T Tuytelaars, L V Gool.SURF:  Speeded  Up  Robust Features.K E A van de Sande, T Gevers, C G M Snoek.Evaluation  of  Color  Descriptors  for Object and Scene RecognitionM YangA Survey of Shape Feature Extraction Techniques虽然这篇文章的引用率目前来看并不高,但个人认为这是一篇在shape feature方面很不错的文章P.Felzenszwalb, D. McAllester, D. RamananA Discriminatively Trained, Multiscale, Deformable Part Model2008 年的 CVPR，到现在引用已有四百多次，潜力巨大。rosepink提供2009 年J Wright, A Y Yang, A Ganesh, S S Sastry, Ma.Robust Face Recognition via Sparse Representation.B Settles.Active learning literature survey2010 年2011 年Hough Forests for Object Detection, Tracking, and Action RecognitionRobust Principal Component Analysis?Candes  和 UIUC 的Ma Yi等人2012 年Zdenek Kalal, Krystian Mikolajczyk,and Jiri Matas,Tracking-Learning-DetectionPAMI上的，虽然还没有正式发表，但肯定会火。在作者的主页上有几篇相关的会议文章， demo和code。用到了 Lucas-Kanade方法（完）“"}
{"content2":"作者：xiabodan 出处：http://blog.csdn.net/xiabodan/一般要去国外找资料，FQ是第一道工序，也是一件一劳永逸的事情，会为以后的研究，资料查找节约非常多的时间。后面有一些收集到的资源站点，不是每个都标注了的。能够依据自己的喜好自行选择。想要在如茫茫大海的互联网中找到你想要的资料，没有一定技巧是不行的。搜索要做的第一件事就是极力的找一篇非常好的paper的參考文献，不管google，即使像一篇国内的垃圾期刊也会有几篇比較好的參考文章，再通过參考文章的參考文章继续搜索。这样一层一层的总会找到本领域相关的大牛，仅仅要找到一个大牛，那就非常好办了。由于大牛们都是成群结队的。不论是文章的參考文祥要互对应用，连youtube等等这些他们也会互相评论、关注的。接下来你就仅仅须要去关注他们。看看有什么成果是和自己相关的就OK了。一般来说了解一个领域须要从下面几个方面着手，专利。论文，博客。研究机构主页学习专利公开课，论文的话就不贴出来了。直接google或者在校内网搜索专利：怎样查到一篇文献的DOI号或通过DOI找到原始文献? | 參考咨询知识库Resolve a DOI中文DOIcrossref.org中国专利下载FPO IP Research & Communities佰腾网——中国专利和科技创新服务门户站点 - 站点首页SooPAT 专利搜索学习、公开课(电子书资源下载)Electronic librarycoursera     免费在线教育慕课网：http://www.imooc.com/course/list  各种C/C++ JAVA PHP Linux ............全然免费，极力推荐推荐TED: Ideas worth spreading网易公开课：http://open.163.com/斯坦福大学公开课：机器人学    http://open.163.com/special/opencourse/robotics.html麻省理工大学公开课：算法导论 http://open.163.com/special/opencourse/algorithms.html斯坦福大学公开课：编程范式  http://open.163.com/special/opencourse/paradigms.html斯坦福大学公开课：编程方法学  http://open.163.com/special/sp/programming.htmlyoutube 视频，国外大牛的成果基本都会在youtube上发布，能够选择自己喜欢的大牛关注其动态。下图是我的网易课程与youtube关注https://www.youtube.com/?gl=HK&tab=w1zynq&&cortex-a9 FPGA研究新加坡国立大学real-time football cup多核计算hamsterworks  一个开源的FPGA project主页VAST LAB(丛京生实验室)ZYNQ研究shakith Fernando的博客主页。主要做图像处理FPGA。GPU并行加速，事实上验室位于荷兰埃因霍芬理工大学并行计算研究室http://parse.ele.tue.nl/荷兰埃因霍芬理工大学：University of Technology EindhovenFPGA 2015国际研讨会linaro文件系统博洛尼亚大学 FPGA立体视觉Home :: OpenCores   一个开源的FPGA IP库Xilinx wiki开源站点  包括了Xilinx公司各种demo  project目标检測跟踪研究1、USC Computer 研究组 （美国南加州大学）研究方向：图像切割。运动分析，大数据分析，模型分析，目标跟踪与识别。http://iris.usc.edu/USC-Computer-Vision.html2、The Center for vision ,Cognition ,Learning,and Art （美国加利福尼亚洛杉矶分校）研究方向：视觉，识别。学习。行为分析3、ETHZ Computer Vision Laboratory 研究组 （瑞士苏黎世联邦理工学院）研究方向：医学图像分析，目标识别，手势分析。跟踪。场景理解与建模http://www.vision.ee.ethz.ch/research/projects_icu.cgi?topic=34、EPFLComputer Vision Laboratory 研究组 （瑞士洛桑联邦理工学院）研究方向：图像描写叙述子，可变形的外表建模，目标跟踪，人体建模http://cvlab.epfl.ch/research/body/surv5、learning ，Recognition,and Surveillance 研究组（奥地利格拉茨技术大学）研究方向：机器学习，目标识别，目标检測与跟踪http://lrs.icg.tugraz.at/research/classifiergrid/classifiergrid.php6、The Australian Center for Visual Technologies 研究组 （澳大利亚阿德莱德大学）研究方向：增强现实，机器学习，3D建模。參数预计。监控，跟踪http://blogs.adelaide.edu.au/acvt/7、香港中文大学多媒体实验室研究方向：深度学习，面部分析。视频监控跟踪，图像视频搜索http://mmlab.ie.cuhk.edu.hk/projects.html8：KLT（光流法特征跟踪）http://web.yonsei.ac.kr/jksuhr/articles/Kanade-Lucas-Tomasi%20Tracker.pdfhttp://www.ces.clemson.edu/~stb/klt/http://vision.ucla.edu//MASKS/labs.html部分图像处理。机器视觉研究机构主页此博客收集大量的计算机视觉牛人的主页。以及非常有实力的研究机构 http://blog.csdn.net/carson2005/article/details/6601109科学网博客，转载了很多机器视觉的资源网址收藏： http://blog.sciencenet.cn/blog-454498-456241.html机器学习资料大汇总 | 我爱机器学习(普林斯顿大学视觉组)Sliding Shapes for 3D Object Detection in Depth Images(德国慕尼黑大学增强现实与计算机视觉)Chair for Computer Aided Medical Procedures and Augmented Reality - Lehrstuhl für Informatikanwendungen in der Medizin und Augmented Reality(加州理工大学计算机视觉)Computational Vision: [Research](计算机视觉大牛主页code)Hernan Badino's Homepage斯坦福人工智能实验室斯坦福大学机器视觉主页Cvlibs Andreas Geiger 主页(国外知名的视觉数据库code)The KITTI Vision Benchmark Suite ：www.cvlibs.net/datasets/kitti/Albert Huang - MIT项志宇-浙江大学 人工智能实验室个人主页卡内基梅隆大学机器人实验室paper西雅图机器人协会Seattle Robotics Society Encoder（单目视觉里程计。google code）Monocular Visual Odometry » Dr. Rainer HessmerKinect Fusion 微软的kinect 应用Projects « CMP 机器感知研究部分机器人、视觉、图像处理库Marco Zuliani's web page（matlab图像处理库）MRPT | Mobile Robot Programming Toolkit library 移动机器人开发库，包括图像处理，视觉，导航，定位。相机标定等BoofCV(java图像处理机器视觉库)Peter Kovesi（MATLAB视觉库）微软研究院 ： http://research.microsoft.com/en-us/um/people/kahe/  不说了 太牛了啥都有(图像处理库)IPOL Journal · Image Processing On Linemath-neon(ARM下neon实现的数学运算库)Image Stitching（图像拼接库code）Stan Birchfield（付澄提供站点）(多媒体处理參考)HIPR Table of Contents and Main Indexopencv图像处理http://docs.opencv.org/OpenSLAM.org 开源SLAM库matlab 单/双目相机标定 Camera Calibration Toolbox for Matlab工作笔试相关首先推荐 协议森林 - Vamei - 博客园  包括算法与数据结构  Linux网络协议森林排序算法具体解释 C语言版 Sorting Algorithm Animations在线算法题目測试Problems | LeetCode OJ笔试题-面试题 - lionel的专栏 - 博客频道 - CSDN.NETC++ - 标签 - Alexia(minmin) - 博客园02：C语言 - 随笔分类 - cv_ml_张欣男 - 博客园程序猿面试笔试宝典学习记录（一）（常见面试笔试题目） - weixliu - 博客园笔试面试必会代码 以及必看书籍 http://blog.csdn.net/liuqiyao_01/article/details/16960695Linux相关POSIX Threads ProgrammingLinux下多线程和共享内存混合编程实例-liuyang_430068-ChinaUnix博客c - POSIX pthread programming - Stack OverflowWindows多线程 - MoreWindows Blog - 博客频道 - CSDN.NET相关书籍（嵌入式软开方向）网络编程 + Linux驱动/应用程序设计 + 数据结构与算法（重点）+ C/C++。边看边敲代码APUE（advance programming UNIX environment）编程珠玑Linux设备驱动程序Linux系统编程Linux网络编程并行程序设计导论数据结构与算法分析程序猿面试宝典（第四版） 小心点看，有非常多错误的地方关注的公司actel ChinaIEEE Xplore - IEEE Std 1164-1993Linear Technology - 主页Microchip Technology IncModelSim - Advanced Simulation and DebuggingModelSim ASIC and FPGA Design Simulator - Mentor GraphicsPLX Technology xiabodan@263.com xiabo891219windriver Download free trial亚德诺半导体 Analog Devices 半导体和信号处理IC全面可编程和器件来自Maxim的模拟、线性、混合信号器件模拟, 半导体, 数字信号处理 - 德州仪器细节决定成败，细节创造利润--周立功单片机Kontron - Embedded Computers, Industrial PCLytroPCI-A429 - ARINC PCI接口卡 - ARINC - 阿尔塔数据技术英飞凌——半导体与系统解决方式 - Infineon TechnologiesDDC - Data Device Corporation | Supplier of MIL-STD-1553, ARINC 429, and other data interface products.Altera – FPGA、CPLD、ASIC和可编程逻辑USB.org - DocumentsStandards & Documents Search | JEDECInternational Rectifier - High ReliabilityVishay - 威世半导体 - 威世品牌Discrete, Analog and Logic Semiconductor Components | Diodes, Inc.ON SemiconductorAllied Electronics – Electronic Parts and Components DistributorMicron Technology, Inc. DRAM, NAND Flash, NOR Flash, MCP, Hybrid Memory Cube, SSDCadence OrCAD Downloadsarm inc.460557758@qq.com Xiabo891219micrium.com summer wMUunio3iX3Mdililentchinazedboarddigilentlogicbricks(xilinx-IP)adapteva(f傅立叶公司 芯片)omnitek（XILINX）beecubezrobotfreeRTOS国内论坛 （非常少看）amoBBS 阿莫电子论坛 首页FPGA-CPLD - 电子project世界-论坛queuequeue - C++ Reference【新提醒】一伙儿网 - Yihuor个人中心 - 程序猿联合开发网中国电子顶级开发网论坛(EETOP) 国内最顶级的电子论坛。最活跃的电子project师交流社区看雪安全论坛 - www.pediy第九单片机论坛 -帐号Local configuration register配置Xilinx 大学计划：开发板组合创新网altium中文社区OpenHW-中国首个开放源代码硬件社区论坛 - SoC Vista -- 中国芯动力 -- SoC/FPGA/ASIC设计家园 - Powered by Discuz!MATLAB中文论坛|Simulink中文论坛 xiabodan Xiabo891219love in C++, live on MFC - C++博客Forum for Electronics(电子论坛)Index - Arduino Forumfpga4fun.com - I2C最活跃FPGA论坛推荐社区德州仪器在线技术支持社区上海库源电气科技有限公司-PSpice技术支持中心（Cadence代理 Allegro代理 OrCAD代理 PSpice代理） - Powered by phpwind库源电气-Cadence代理、OrCAD代理、Allegro代理、PSpice代理。为您提供最专业的Cadence PCB解决方式Download - OpenCV China ：图像处理,计算机视觉库,Image Processing, Computer VisionfreeIP与參考设计马上注冊 - OpenCV论坛iVeia Git • Index page电子发烧友论坛-中国电子技术论坛 - 最好最受欢迎电子论坛!综合电子论坛 goodeclipsexilinx.eetrend.comXILINX开源站点ohwr(开源硬件)中电网(在线培训)电路网elementsmatlabadept forumschinaunixu-boot未完待续..........敬请期待.........."}
{"content2":"（0）引子以下以现实生活中的一个实例引出本博客的探究点。或许类似的情况正发生在你的身边。小弟工作5年了，近期有点迷茫。上一份工作在一家比較大的门户站点做web开发和移动互联网数据挖掘(人手比較紧。同一时候做)。后来跳槽到BAT之中的一个做数据挖掘。数据量倒是很大。可是感觉没有多大意思——就是分析日志，弄报表而已。之前已经的高性能web开发经验全然用不上。感觉自己还是喜欢做开发，能和业务相关的。可是数据挖掘大。数据近期挺火的，也是比較纠结。事实上。这也是非常多人的症结，包含我个人也是迷茫~~~在此，请求博友们献言献策！不胜感激！（1）大数据与数学的恩怨一是公司原来没有一项业务。如今要把一些机器学习这个东西跑起来（从无到有）。二是在你接手的时候公司已经有一定基础了。如今要把性能调上去（从差到优）。前者全然不用不论什么数学，先用别人有的模块/代码把系统撸起来是王道。后者看详细问题。大多数情况不用数学。在偏研究的地方比方Google X的某些部门还是实用的，可能须要比較好的数学功底，一些deep learning（机器学习）的机构，如百度凤巢研究院或者微软亚洲研究院等。可是，对于一般的数据分析、数据挖掘项目组，特别是对于某一分类器，大多数时候还是看feature找的好不好，找到一个优秀的feature赛过苦逼的在那里调优一万年（身在以KPI优先的公司和拿来就用的大环境，利用已有的开源Lib包）。学好线性代数。统计和凸优化就出门打怪吧。攒系统经验和dirty trick才是王道。当然我也不是说就不要搞数学。仅仅是假设你去公司的话，在学好线代统计凸优化的前提下，相同的时间花在学计算机系统的构建和系统性的思考方法上，比学习数学更划算。在大部分的ML研究里。还是微积分和线性代数、概率统计的功底最重要。（2）大数据衍生的工作岗位数据研发project师側重于研发，这块我了解的不多，我理解的是主要是数据仓库开发那块。数据分析师側重分析，主要是结合业务方需求做些相关的数据分析工作。发现问题，找出问题。提出解决方式。数据挖掘側重挖掘，主要用些数据挖掘算法或者机器学习算法做些分类和预測或者其他工作，比方说流失。违约。推荐等等。数据产品经理偏重产品经理，主要是负责数据产品的相关pd工作。数据产品就是基于数据分析或者挖掘研发出来的产品。假设硬要分析的话，数据研发project师要求有一定的开发功底。工作偏向于数据系统的开发。数据分析师更像传统的BI。而数据挖掘project师更是以特定需求为目标的数据挖掘工作，比方做人群的偏好挖掘。数据产品经理应该就是产品方向了，以数据业务/产品为目标的产品经理。说到最后，事实上除产品经理岗位的另3个岗位，工作职责还是非常相似的，主要工作内容以部门要求为准。引用百科的说法是。数据研发project师：搭建基础设施，让大数据的存储、处理、计算能在要求的时间内，以合理的成本完毕。数据分析师：发现问题，分析问题，得出结论。为决策作支持。数据挖掘project师：通过建立模型。预測、区分感兴趣的对象。以下通过几张图说明：(3)DM 与 ML1. DM更应用化。ML更偏研究与算法（所以公司一般有数据挖掘project师，机器学习研究员）2. ML的问题常常是明白定义的。包含数据集及目标（且数据集是固定的）；DM通常仅仅定义目标，甚至连目标也没有（给你一堆数据，给我找出有价值、有意思的东西出来）；在定义了目标的情况下，DM能够使用非固定的数据源3. ML仅仅是DM使用的方法的一种。DM还能够使用其它的方法（比方统计，比方直接看数据）4. 作为一门交叉学科。ML是DM的一门重要基础，可是DM还有其它的基础学科。最重要的是统计与数据库5. DM的重点是数据。所以做DM的人可能花80%的时间在用各种方式倒腾数据上。而仅仅花会20%的时间在算法上。而对ML可能相反，80%的时间都在读Paper,试验算法上。20%的时间用在处理数据上（4）数据挖掘 之 文本挖掘数据挖掘（Data mining），又译为资料探勘、数据採矿。它是数据库知识发现（英语：Knowledge-Discovery in Databases。简称：KDD)中的一个步骤。数据挖掘通常是指从大量的数据中通过算法搜索隐藏于当中信息的过程。数据挖掘通常与计算机科学有关，并通过统计、在线分析处理、情报检索、机器学习、专家系统（依靠过去的经验法则）和模式识别等诸多方法来实现上述目标。文本挖掘有时也被称为文字探勘、文本数据挖掘等。大致相当于文字分析，一般指文本处理过程中产生高质量的信息。高质量的信息通常通过分类和预測来产生。如模式识别。文本挖掘通常涉及输入文本的处理过程（通常进行分析，同一时候加上一些衍生语言特征以及消除杂音，随后插入到数据库中） ，产生结构化数据，并终于评价和解释输出。'高品质'的文本挖掘一般是指某种组合的相关性，新颖性和趣味性。典型的文本挖掘方法包含文本分类。文本聚类，概念/实体挖掘，生产精确分类，观点分析，文档摘要和实体关系模型（即。学习已命名实体之间的关系） 。借用高斯的一句话并进行改写送给全部做数据挖掘、文本挖掘的人。“对数据挖掘、文本挖掘的无知不是没有相关知识，而是过于依赖数据挖掘、和文本挖掘而忽视其它”。文本数据挖掘(Text Mining)是指从文本数据中抽取有价值的信息和知识的计算机处理技术。顾名思义，文本数据挖掘是从文本中进行数据挖掘(Data Mining)。从这个意义上讲，文本数据挖掘是数据挖掘的一个分支。文本挖掘方法:1.文本分类,是一种典型的机器学习方法，一般分为训练和分类两个阶段。2.文本聚类,是一种典型的无监督式机器学习方法，聚类方法的选择取决于数据类型。3.信息抽取。4.摘要。5.压缩。当中，文本分类和聚类是两种最重要最主要的挖掘功能。挖掘工具：1.IBM DB2 intelligent Miner。2.SAS text miner。3.SPSS Text Mining。4.DMC TextFilter（纯文本抽出通用程序库）应用：文本挖掘传统商业方面的应用主要有，企业竞争情报、CRM、电子商务站点、搜索引擎，如今已扩展到医疗、保险和咨询行业。（5）人工智能、机器学习、统计学和数据挖掘的前世今生我假定题主是想得到一个清晰的图，上面有各个领域清晰的分界线。因此。在这里我尝试用我最简单的方式来解释这个问题。机器学习是一门涉及自学习算法发展的科学。这类算法本质上是通用的。能够应用到众多相关问题的领域。数据挖掘是一类有用的应用算法（大多是机器学习算法）。利用各个领域产出的数据来解决各个领域相关的问题。统计学是一门研究如何收集。组织。分析和解释数据中的数字化信息的科学。统计学能够分为两大类：描写叙述统计学和判断统计学。描写叙述统计学涉及组织，累加和描绘数据中的信息。判断统计学涉及使用抽样数据来判断整体。机器学习利用统计学（大多是判断统计学）来开发自学习算法。数据挖掘则是在从算法得到的结果上应用统计学（大多是描写叙述统计学），来解决这个问题。数据挖掘作为一门学科兴起，旨在各种各样的行业中（尤其是商业）求解问题，求解过程须要用到不同研究领域的不同技术和实践。1960年求解问题的从业者使用术语Data fishing来称呼他们所做的工作。1989年Gregory Piatetsky Shapiro使用术语knowledge Discovery in the Database（KDD，数据集上的知识发掘）。1990年一家公司在商标上使用术语数据挖掘来描写叙述他们的工作。现现在现现在数据挖掘和KDD两词能够交换使用。人工智能这门科学的目的在于开发一个模拟人类能在某种环境下做出反应和行为的系统或软件。因为这个领域极其广泛，人工智能将其目标定义为多个子目标。然后每一个子目标就都发展成了一个独立的研究分支。这里是一张人工智能所要完毕的主要目标列表（亦称为AI问题）1、Reasoning（推理）2、Knowledge representation（知识表示）3、Automated planning and scheduling（自己主动规划）4、Machine learning（机器学习）5、Natural language processing（自然语言处理）6、Computer vision（计算机视觉）7、Robotics（机器人学）8、General intelligence or strong AI（通用智能或强人工智能）正如列表中提到的，机器学习这一研究领域是由AI的一个子目标发展而来。用来帮助机器和软件进行自我学习来解决遇到的问题。自然语言处理是还有一个由AI的一个子目标发展而来的研究领域。用来帮助机器与真人进行沟通交流。计算机视觉是由AI的目标而兴起的一个领域，用来辨认和识别机器所能看到的物体。机器人学也是脱胎于AI的目标，用来给一个机器赋予实际的形态以完毕实际的动作。它们之间有层次等级的区分吗。应该是如何一回事？解释这些科学和研究层次关系的一个方法是分析其历史。科学和研究的起源统计学——1749年人工智能——1940年机器学习——1946年数据挖掘——1980年统计学的历史公认起源于1749年左右。用来表征信息。研究人员使用统计学来表征国家的经济水平以及表征用于军事用途的物质资源。随后统计学的用途扩充到数据的分析及其组织。人工智能的历史碰巧存在两种类型：经典的和现代的。经典人工智能可在古时的故事和著作中看得到。然而，1940年当人们在描写叙述用机器模仿人类的思想时才出现了现代人工智能。1946年，作为AI的分支。机器学习的起源出现了，它的目标在于使机器不通过编程和明白的硬接线进行自我学习来对目标求解。能否够这样说，它们是利用不同方法解决相似问题的四个领域？可以这么来说（统计学，人工智能和机器学习）是高度相互依赖的领域，没有其它领域的引领和帮助，他们不可以单独存在。非常高兴能看到这三个领域是一个全局领域而非三个有所隔阂的领域。正如这三个领域是一个全局领域。它们在解决共同目标时发挥了自己的优势。因此，该方案适用于很多不同领域中。由于隐含的核心问题是一致的。接下来是该数据挖掘出场了，它从全局获取解决方式并应用到不同的领域（商业、军事、医学、太空）来解决同一隐含本质的问题。这也是数据挖掘扩大其受欢迎程度的时期。我希望我的解释已经回答了答主所提问一切疑问。我相信这能清晰地帮助不论什么一个想要理解这四个领域关键点的人们。假设你对该话题有不论什么想要说的或者要分享的。请在评论里写下你的想法。（6）总结文件夹---相关文章高速学Python 和 易犯错误（文本处理）Python文本处理和Java/C比对十分钟学会Python的基本类型高速学会Python（实战）大数据处理之道（十分钟学会Python）"}
{"content2":"顶级会议（按照会议的英文简称升序排列）1.     The Annual International Conference of the ACM Special Interest Group on Data Communication(ACM SIGCOMM)【ACM数据通讯国际会议】2.    ACM SIGGRAPH   International Conference and Exhibition on Computer Graphics and Interactive techniques(ACM SIGGRAPH)【ACM计算机图形与交互技术国际会议与展览】3.   ACM SIGMOD International Conference on Management of Data(ACM SIGMOD)【ACM数据管理国际会议】4.   International Joint Conference on Artificial Intelligence(IJCAI)【人工智能国际联合会议】5.    ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming(PPOPP)【ACM并行程序设计原理与实践会议】A类会议1.   The Annual Meeting of the Association for Computational Linguistics(ACL)【计算语言学协会年度会议】2.   ACM International Conference on Multimedia(ACM Multimedia) 【ACM 多媒体国际会议】3.   ACM SIGIR Conference on Research and Development in Information Retrieval(ACM SIGIR) 【ACM 信息检索研究与发展会议】4.   ACM/EDAC/IEEE Design Automation Conference(DAC) 【ACM/EDAC/IEEE 设计自动化会议】5.   USENIX Conference on File and Storage Technologies(FAST)【USENIX文件与存储技术会议】6.   International Conference on Computer Vision(ICCV)【计算机视觉国际会议】7.   IEEE International Conference on Computer Communication(INFOCOM) 【IEEE网络协议国际会议】8.   ACM/IEEE International Symposium on Computer Architecture(ISCA) 【ACM/IEEE计算机体系结构国际会议】9.   ACM SIGKDD Conference on Knowledge Discovery and Data Mining(KDD) 【ACM只是发现与数据挖掘会议】10.   The Annual International Conference on Mobile Computering and Networking(MOBICOM)【ACM移动计算与网络国际会议】11.   SuperComputing(SC) 【超级计算国际会议】12.   ACM International Conference on Ubiquitous Computing(Ubicomp) 【ACM普适计算国际会议】13.   International Conference on Very Large Data Bases(VLDB) 【超大数据库国际会议】14. International World Wide Web Conference(WWW) 【万维网国际会议】未完待续…"}
{"content2":"美国内华达大学（University of Nevada Reno，UNR）计算机视觉实验室（CVL）招收计算机视觉方向的MS、PhD学生，提供RA职位，每月1600$，详细资料见附件。需求：计算机、数学、物理或者工科方向的第一学位；良好的数学基础和C/C++编程技巧；熟悉Linux，熟练使用OpenCV或Matlab；有图像处理、计算机视觉、模式识别或机器学习方向上的背景知识；有过3D重建，3D分割或者3D配准方面的经验，有相关方向的论文；良好的沟通和写作技巧。联系方式：Prof. George Bebis （bebis@cse.unr.edu）对于申请的邮件，附件请以pdf方式发送，需包括：（1）简历；（2）成绩单；（3）个人研究兴趣的介绍；（4）代表性论文；（5）；两个推荐人的姓名。—————————————以上是正文—————————————————PS1：有CSC资助的访问学生或者想申请Postdoc职位的也可以试试。PS2：Reno是内华达州仅次于Las Vegas的城市，号称世界上最大的小城市，离加州非常近，就在加州和内华达州的分界线上，跟San Francisco、Sacramento都很近，离Los Angeles和Las Vegas不算远。Reno边上有世界上非常著名的高山滑雪场，有风景优美的Lake Tahoe，今年的NIPS会议就在Tahoe湖召开。此外，Reno的消费很低，物价很便宜，租房费用在300-450$左右。PS3：Prof. Bebis非常随和，很好说话，对学生也很负责，非常耐心，即使很浅显的问题，也会给你讲解的非常清楚，善于引导学生，课堂和实验室气氛都非常轻松活跃。他自己是几份SCI期刊的编委，也是一个EI会议的主席，学生发文章的压力也不是很大，因此，也不会强迫学生发文章或者强制做项目，非常宽容。虽然UNR是一个一般的学校，但Prof Bebis确实是一个优秀的计算机视觉导师。PS4：计算机视觉实验室，跟美国各大公司（Ford、IBM、Honeywell等）都有紧密联系，跟NASA、ONR、劳伦斯国家实验室、橡树岭国家实验室等也有紧密合作，每年暑期，学生根据相关的项目，可以去其中的某个地方进行2-3月的实习。最后，如果有什么其他问题，可以咨询我，或者直接邮件询问Prof Bebis。附件全文：MS/PhD Research Assistantships in Computer VisionDepartment of Computer Science & EngineeringUniversity of Nevada, Reno, USAThe Computer Vision Laboratory (CVL) (http://www.cse.unr.edu/CVL) in the Department of Computer Science and Engineering (CSE) at the University of Nevada, Reno (UNR) invites applications for several MS and Ph.D. student research assistantships, starting in Fall 2013/Spring 2014. The students who will be awarded these assistantships will be expected to complete a MS or PhD degree with emphasis on computer vision. Support for a typical MS/PhD student covers the student's tuition and fees, health insurance, and offers financial compensation of $1,600 / month.Active research areas within CVL include biometrics (e.g., face, hand, fingerprint), object detection and recognition, object tracking and pose estimation, visual surveillance, human activity recognition, and 3D reconstruction. CVL is currently funded by NSF, NASA, and DoD. The CSE department currently has 14 full time faculty members, 6 adjunct faculty and offers B.S., M.S., and Ph.D. degrees supporting a dynamic and growing program with approximately 300 undergraduate and 70 graduate students.Requirements: A first degree in Computer Science, Mathematics, Physics, or an Engineering subject. Strong mathematical background and excellent programming skills in C or C++. Familiarity with Linux, and good knowledge of OpenCV and/or Matlab. Background in image processing, computer vision, pattern recognition, and machine learning. Previous experience on 3D reconstruction, 3D segmentation, 3D registration, and paper publications are desired. Very good communication and writing skills.How to Apply: Applications should be sent by email (in pdf format) to Prof. George Bebis (bebis@cse.unr.edu) and should include: (1) resume, (2) copies of transcripts, (3) a brief statement of research interests, (4) copies of your most representative publications, and (5) the names of 2 references. Applications should be sent as soon as possible. For more information, please contact Prof. Bebis (http://www.cse.unr.edu/~bebis) at bebis@cse.unr.edu.CSE Department: Graduate students seeking the Doctor of Philosophy in Computer Science and Engineering are given the opportunity to focus on a specific area in computer science and engineering by taking advanced courses and becoming significantly involved in many aspects of original research, advancing scientific knowledge in their field of specialization. In particular, they implement prototype systems, develop and prove new theories, conduct experiments, attend international conferences, and publish their results in scientific journals and conferences.Other Information: UNR is an integral part of the Reno metropolitan area, home to approximately 300,000 people. The 255-acre main campus, located minutes from downtown Reno, features both contemporary and historic architecture, and the university’s tree-lined Quadrangle is listed on the National Register of Historic Places. The academic atmosphere is filled with rich surroundings for the cultural and intellectual development of the students. Reno is bounded on the west by the majestic Sierra Nevada mountain range and on the east by the rolling basin and range province, Reno benefits from a comfortable climate. There are four distinct seasons in this cool, dry climate where the sun shines nearly 290 days a year. The area is noted for its variety of recreational opportunities, which include sailing, camping, hiking, fishing, and sightseeing. Within an hour’s drive of the campus, for example, are Lake Tahoe in the High Sierra, and Pyramid Lake, a unique prehistoric desert sea. Also within an hour’s drive are a number of nationally known ski areas, including Squaw Valley, site of the 1960 Winter Olympics. Reno is within 2 hours' drive from Sacramento and 3.5 hours' drive from San Francisco."}
{"content2":"今日CS.CV计算机视觉论文速览Fri, 22 Mar 2019Totally 30 papersDaily Computer Vision Papers1.Title: Progressive Sparse Local Attention for Video object detectionAuthors:Chaoxu Guo, Bin Fan, Jie Gu, Qian Zhang, Shiming Xiang, Veronique Prinet, Chunhong Pan2.Title: PProCRC: Probabilistic Collaboration of Image PatchesAuthors:Tapabrata Chakraborti, Brendan McCane, Steven mills, Umapada Pal3.Title: Closed-Form Optimal Triangulation Based on Angular ErrorsAuthors:Seong Hun Lee, Javier Civera4.Title: Levelling the Playing Field: A Comprehensive Comparison of Visual Place Recognition Approaches under Changing ConditionsAuthors:Mubariz Zaffar, Ahmad Khaliq, Shoaib Ehsan, Michael Milford, Klaus McDonald-Maier5.Title: Quotienting Impertinent Camera Kinematics for 3D Video StabilizationAuthors:Thomas W. Mitchel, Christian Wuelker, Jin Seob Kim, Sipu Ruan, Gregory S. Chirikjian6.Title: An Efficient Solution to Non-Minimal Case Essential Matrix EstimationAuthors:Ji Zhao7.Title: Megapixel Photon-Counting Color Imaging using Quanta Image SensorAuthors:Abhiram Gnanasambandam, Omar Elgendy, Jiaju Ma, and Stanley H. Chan8.Title: Localization of Unmanned Aerial Vehicles in Corridor Environments using Deep LearningAuthors:Ram Prasad Padhy, Shahzad Ahmad, Sachin Verma, Pankaj Kumar Sa, Sambit Bakshi9.Title: Short-Term Prediction and Multi-Camera Fusion on Semantic GridsAuthors:Lukas Hoyer, Patrick Kesper, Volker Fischer10.Title: The CASE Dataset of Candidate Spaces for Advert ImplantationAuthors:Soumyabrata Dev, Murhaf Hossari, Matthew Nicholson, Killian McCabe, Atul Nautiyal, Clare Conran, Jian Tang, Wei Xu, François Pitié11.Title: Learning with Batch-wise Optimal Transport Loss for 3D Shape RecognitionAuthors:Lin Xu, Han Sun, Yuai Liu12.Title: Context-Constrained Accurate Contour Extraction for Occlusion Edge DetectionAuthors:Rui Lu, Menghan Zhou, Anlong Ming, Yu Zhou13.Title: Tensor-Ring Nuclear Norm Minimization and Application for Visual Data CompletionAuthors:Jinshi Yu, Chao Li, Qibin Zhao, Guoxu Zhou14.Title: Learning Disentangled Representations of Satellite Image Time SeriesAuthors:Eduardo Sanchez (IRIT), Mathieu Serrurier (IRIT), Mathias Ortner15.Title: Parametic Classification of Handvein Patterns Based on Texture FeaturesAuthors:Harbi AlMahafzah, Mohammad Imranand, Supreetha Gowda H.D.16.Title: Weakly-Supervised Discovery of Geometry-Aware Representation for 3D Human Pose EstimationAuthors:Xipeng Chen, Kwan-Yee Lin, Wentao Liu, Chen Qian, Liang Lin17.Title: Towards Robust Curve Text Detection with Conditional Spatial ExpansionAuthors:Zichuan Liu, Guosheng Lin, Sheng Yang, Fayao Liu, Weisi Lin, Wang Ling Goh18.Title: Non-target Structural Displacement Measurement Using Reference Frame Based DeepflowAuthors:Jongbin Won, Jong-Woong Park, Do-Soo Moon19.Title: Dual Residual Networks Leveraging the Potential of Paired Operations for Image RestorationAuthors:Xing Liu, Masanori Suganuma, Zhun Sun, Takayuki Okatani20.Title: Prostate Segmentation from Ultrasound Images using Residual Fully Convolutional NetworkAuthors:M. S. Hossain, A. P. Paplinski, J. M. Betts21.Title: Networks for Joint Affine and Non-parametric Image RegistrationAuthors:Zhengyang Shen, Xu Han, Zhenlin Xu, Marc Niethammer22.Title: Robust Image Segmentation Quality Assessment without Ground TruthAuthors:Leixin Zhou, Wenxiang Deng, Xiaodong Wu23.Title: Affordance Learning In Direct Perception for Autonomous DrivingAuthors:Chen Sun, Jean M. Uwabeza Vianney, Dongpu Cao24.Title: LaserNet: An Efficient Probabilistic 3D Object Detector for Autonomous DrivingAuthors:Gregory P. Meyer, Ankit Laddha, Eric Kee, Carlos Vallespi-Gonzalez, Carl K. Wellington25.Title: Im2Pencil: Controllable Pencil Illustration from PhotographsAuthors:Yijun Li, Chen Fang, Aaron Hertzmann, Eli Shechtman, Ming-Hsuan Yang26.Title: Face Detection in Repeated SettingsAuthors:Mohammad Nayeem Teli, Bruce A. Draper, J. Ross Beveridge27.Title: Individualized Multilayer Tensor Learning with An Application in Imaging AnalysisAuthors:Xiwei Tang, Xuan Bi, Annie Qu28.Title: Classification of EEG-Based Brain Connectivity Networks in Schizophrenia Using a Multi-Domain Connectome Convolutional Neural NetworkAuthors:Chun-Ren Phang, Chee-Ming Ting, Fuad Noman, Hernando Ombao29.Title: Implicit Generation and Generalization in Energy-Based ModelsAuthors:Yilun Du, Igor Mordatch30.Title: Online continual learning with no task boundariesAuthors:Rahaf Aljundi, Min Lin, Baptiste Goujaud, Yoshua BengioPapers from arxiv.org更多精彩请移步主页pic from pixels.com"}
{"content2":"选自 Computer Vision Blog机器之心编译参与：吴攀、微胖、李亚洲本文经机器之心（微信公众号：almosthuman2014）授权转载，禁止二次转载2012年，我开始列举计算机视觉领域引用率最高的文章清单。不过，2012年以来，这个领域变了很多，当时深度学习技术开始成为潮流，而且在许多计算机视觉基准方面其表现超过了传统方法。无论这一趋势是否能够持久，我认为，应该将这些技术放入名单。正如我提到的那样，被引越频繁未必意味着文章贡献越大。不过，高引证率经常暗示着人们已经发现了某些有趣的东西。据我所知，以下就是计算机视觉和深度学习领域被引最多的论文。1.使用深度卷积神经网络的 ImageNet 分类（Imagenet classification with deep convolutional neural networks）作者：A Krizhevsky, I Sutskever, GE Hinton, 2012。引用次数：5518摘要：为了将 ImageNet LSVRC-2010 大赛的120万张高清图像分为1000个不同的类别，我们训练了一个大型的深度卷积神经网络。对测试数据，我们得到了 top-1 误差率 37.5%，以及 top-5 误差率 17.0%，这个效果比之前最顶尖的都要好得多。该神经网络有 6000 万个参数和 650,000 个神经元，由五个卷积层，以及某些卷积层后跟着的最大池化（max-pooling）层，以及三个全连接层，还有排在最后的1000-way 的 softmax 层组成。为了使训练速度更快，我们使用了非饱和的神经元和一个非常高效的 GPU 关于卷积运算的工具。为了减少全连接层的过拟合，我们采用了最新开发的正则化方法，称为 dropout ，它已被证明是非常有效的。在 ILSVRC-2012 大赛中，我们又输入了该模型的一个变体，并依靠 top-5 测试误差率 15.3% 取得了胜利，相比较下，第二名的错误率为 26.2%。2.Caffe：用于快速特征嵌入的卷积架构（Caffe: Convolutional architecture for fast feature embedding）作者：Y Jia, E Shelhamer, J Donahue, S Karayev，2014。引用次数：18683.反向传播算法应用于手写邮政编码识别（Backpropagation applied to handwritten zip code recognition）作者：Y LeCun, B Boser, JS Denker, D Henderson，1989。引用次数：16814.准确的物体检测和语义分割中的丰富的特征层级（Rich feature hierarchies for accurate object detection and semantic segmentation）作者：R Girshick, J Donahue, T Darrell，2014。引用次数：1516摘要：据在权威的 PASCAL VOC 数据集上测定，过去几年，物体检测的性能已经趋于稳定。性能最好的方法都是复杂的集合系统，最典型的就是结合了多种低层图像特征和高层文本。在此论文中，我们提出了一种简单的、可延展的检测算法，相比于之前在 VOC 2012 上最好的结果（mAP 为 53.3%），我们的方法将平均正确率均值（mAP）提升了 30% 左右。我们的方法结合了两大主要观点：（1）为了局部化以及分割物体，能够应用高能力的卷积神经网络进行自下而上的区域（region）提议。（2）当标记的训练数据缺乏时，监督的预训练作为一种辅助任务，接而对特定区域进行微调，从而产生新能上的大步提升。因为我们结合了卷积神经网络的区域提议（Region proposal），我们称之为 R-CNN 方法：带有 CNN 特征的区域。我们也提供了该网络学习效果的实验，展现了图片特征的一个丰富的层级。整个系统的源代码网址是： http://www.cs.berkeley.edu/~rbg/rcnn。5.大规模图像识别中非常深的卷积网络（Very deep convolutional networks for large-scale image recognition）作者：K Simonyan, A Zisserman, 2014。引用次数：1405摘要：在此论文中，我们调查了在卷积网络深度对大规模图像识别准确率的影响。我们主要的贡献是一个周密的网络评估：使用一个带有非常小的（3×3）卷积过滤层增加网络的深度，结果显示，这对先前顶尖的构型有重大改进意义，将深度增加到了 16-19 个权重层。这些发现是我们在 ImageNet 2014 挑战赛中提交方法的基础，而在此比赛中，我们的团队在局部化和分类任务中获得了第一和第二的成绩。结果也显示，我们的方法在其他数据集中也有很好的泛化性能，在这些数据集上也取得了顶尖结果。我们已经将两个性能最佳的 ConvNet 模型公开了，可应用于计算机视觉任务中使用深度视觉表征的深入研究。6.通过阻止特征检测器的互相适应提升神经网络（Improving neural networks by preventing co-adaptation of feature detectors）作者：GE Hinton, N Srivastava, A Krizhevsky, 2012。引用次数：11697.使用卷积做到更深度（Going deeper with convolutions）作者：C Szegedy, W Liu, Y Jia, P Sermanet, 2015。引用次数：1160摘要：我们提出了一种代号为 Inception 的深度卷积神经网络架构，该架构在 2014 年 ImageNet 大规模视觉识别挑战赛（ILSVRC 2014）上实现了当时最佳的分类和检测结果。这种架构的主要特点是提高了对网络内部计算资源的利用。我们通过一个精心的设计使其在保持计算预算恒定的同时。实现了网络的深度和宽度的增长。为了质量的优化，该架构的决策基于赫布原理（Hebbian principle）和多尺度处理的直觉知识。该架构的一个特定的典型体现 GoogLeNet 是一个 22 层的深度网络，该网络在物体检测和分类的语境中被用来评估质量。8.利用一个BP网络进行手写数字的识别（Handwritten digit recognition with a back-propagation network）作者：BB Le Cun, JS Denker, D Henderson, 1990。引用次数：977摘要：我们介绍了BP网络的一种应用方式：手写数字识别。需要对数据进行最小程度预处理，但是，网络结构高度受限并针对任务进行了特别设计。输入的是独立数字的标准化图像。这一方法的误差率为1%，在美国邮政服务提供的邮政编码数字上，拒绝率为 9%。9.视觉化并理解卷积网络（Visualizing and understanding convolutional networks）作者：MD Zeiler, R Fergus, 2014。引用次数：90710.Dropout：一个防止神经网络过度拟合的简单办法（Dropout: a simple way to prevent neural networks from overfitting）作者：N Srivastava, GE Hinton, A Krizhevsky…, 2014。引用次数：839摘要：带有大量参数的深度神经网络是非常有力的机器学习系统。但是，这类网络中存在一个严重的问题，过度拟合。大型网络采用缓慢，因此，很难通过测试时集合许多不同大型神经网络的预测来解决过度拟合问题。Dropout是一个解决办法。其中关键思想是，在训练过程中，从神经网络中随机放弃单元（以及它们的连接）。这能防止单元过于适应。在训练过程中，放弃来自指数数量的不同“变薄的”神经网络的样本。测试时，简单使用一个单独的带有较小权重的变薄网络，就很容易近似平均所有这些变瘦网络预测的效果。这显著减轻了过度拟合，也改进了其他正则化方法。我们也表明，这一方法改善了神经网络在监督学习任务（视觉、语音识别、文档分类以及计算生物学）方面的表现，在许多基准数据组上的成绩目前达到最先进水平。11. Overfeat：使用卷积网络融合识别、局部化和检测（Overfeat: Integrated recognition, localization and detection using convolutional networks）作者：P Sermanet, D Eigen, X Zhang, M Mathieu, 2013。引用次数：83912.从微小图片中学习多层特征（Learning multiple layers of features from tiny images）作者：A Krizhevsky, G Hinton, 2009。引用次数：81813.DeCAF：一个应用于常规视觉识别任务中的深度卷积激活特征（DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition）作者：J Donahue, Y Jia, O Vinyals, J Hoffman, N Zhang…, 2014。引用次数：71814.深度面部：在面部验证方面，弥补与人类表现水平的差距（Deepface: Closing the gap to human-level performance in face verification）作者：Y Taigman, M Yang, MA Ranzato, 2014。引用次数：69115.深度玻尔兹曼机（Deep Boltzmann Machines）作者：R Salakhutdinov, GE Hinton, 2009。引用次数：67916.用于图像、语音以及时序的卷积网络（Convolutional networks for images, speech, and time series）作者：Y LeCun, Y Bengio, 1995。引用次数：67017.现成的CNN特征 ：一个令人吃惊的识别基线（ CNN features off-the-shelf: an astounding baseline for recognition）作者：A Sharif Razavian, H Azizpour, J Sullivan, 2014。引用次数：57018.针对情景标签的学习分层特征（Learning hierarchical features for scene labeling）作者：C Farabet, C Couprie, L Najman, 2013。引用次数：54919.语义切分的全卷积网络（Fully convolutional networks for semantic segmentation）作者：J Long, E Shelhamer, T Darrell, 2015。引用次数：51020.Maxout 网络（Maxout networks）作者：IJ Goodfellow, D Warde-Farley, M Mirza, AC Courville, 2013。引用次数：46921.细节魔鬼的回归：深挖卷积网络（Return of the devil in the details: Delving deep into convolutional nets）作者：K Chatfield, K Simonyan, A Vedaldi, 2014。引用次数：45322.使用卷积神经网络进行大规模视频分类（Large-scale video classification with convolutional neural networks）作者：A Karpathy, G Toderici, S Shetty, T Leung, 2014。引用次数：44523.针对生成图像描述的深度视觉-语义对齐（Deep visual-semantic alignments for generating image descriptions）作者：A Karpathy, L Fei-Fei, 2015。引用次数：34724.深度探入纠正器：在 Imagenet 分类中超过人类表现（Delving deep into rectifiers: Surpassing human-level performance on imagenet classification）作者：K He, X Zhang, S Ren, J Sun, 2015。引用次数：34225.使用卷积神经网络学习和传递中层图像表征（Learning and transferring mid-level image representations using convolutional neural networks）作者：M Oquab, L Bottou, I Laptev, J Sivic, 2014。引用次数：33426.卷积网络及其视觉应用（Convolutional networks and applications in vision）作者：Y LeCun, K Kavukcuoglu, C Farabet, 2010。引用次数：33327.使用位置数据库学习用于场景识别的深度特征（Learning deep features for scene recognition using places database）作者：B Zhou, A Lapedriza, J Xiao, A Torralba,2014。引用次数：33228.用于视觉识别的深度卷积网络中的空间金字塔池化（Spatial pyramid pooling in deep convolutional networks for visual recognition）作者：K He, X Zhang, S Ren, J Sun, 2014。引用次数：29929.用于视觉识别和描述的长期循环卷积网络（Long-term recurrent convolutional networks for visual recognition and description）作者：J Donahue, L Anne Hendricks, 2015。引用次数：26830. 用于视频中动作识别双流式（Two-stream）卷积网络（Two-stream convolutional networks for action recognition in videos）作者：K Simonyan, A Zisserman, 2014。引用次数：261下载原文"}
{"content2":"文章发布于公号【数智物语】 （ID：decision_engine），关注公号不错过每一篇干货。来源 | 互联网周刊(ID:ciweekly)01人工智能标志着第四次工业革命按照 2017 年李开复与王咏刚先生合著的《人工智能》一书中的描述，人工智能是会学习的计算机程序。这种具有学习能力的计算机程序崛起以后，为什么能以迅雷不及掩耳之势影响各个行业呢？从教育到医疗，从金融到建筑，从安防到环保，到处都有人工智能的痕迹。要解释这个问题，还必须站在人类历史的长河中回望审视。人类与一般动物的区别在于：人类可以制造与使用工具。所以在人类的文明史上，工具代表着生产力的发展水平。人类钻木取火，有了火之后就学会了青铜器的烧制，随后就有了铁器，于是人类掌握了农业生产的工具，就可以开垦荒地播种粮食；到了文艺复兴之后，牛顿科学革命肇始，人类发明了蒸汽机，在这些工具的帮助下，人类进入了第一次工业革命时代。从此以后，人类开始告别田园牧歌的社会，正如恩格斯在《英国工人阶级状况》中写道的那样：“新生的工业能够这样成长起来，只是因为它用机器代替了手工工具，用工厂代替了作坊。”如果说蒸汽机的发明导致了第一次工业革命，而电动机的发明导致了第二次工业革命。那么以微软、苹果与 IBM 为代表的个人电脑的出现，标志着第三次工业革命。而人工智能技术的兴起，则表明第四次工业革命已经在路上了。人工智能是整个人类大历史发展的潮流，这种潮流不以任何个人的意志为转移，所以它能在各行各业找到应用场景。02中国企业在人工智能时代抓住了风口在前三次工业革命中，因为在中国没有诞生现代科学，并采取了闭关锁国的封闭政策，所以中国最终落后于当时的时代发展。落后就要挨打，在这段历史进程中，中国有了一部辛酸的现代史。但是，阳光总在风雨后，在 2016 年开启的“第四次工业革命”中，中国不但没有落后于时代，而且在一定程度上还领先于世界——这是中华民族伟大复兴的一个显著标志。目前中国已经涌现了大量的人工智能企业，而且在各行各业的产业化做得相当不错。据报道，截止到 2018 年 6 月，中国一共有 1000 余家人工智能企业，比软件强国印度还多出来 200 多家，企业总数量仅次于美国。而到了 2018 年末，全国人工智能企业数量进一步增长，相关企业共计 4000 余家。除了在企业方面，中国人工智能论文总量和高被引论文数量也占据世界第一的位置。以 2017 年为例，中国在人工智能领域论文的全球占比 27.68%，遥遥领先其他国家。在人工智能专利方面，中国已经成为全球人工智能专利布局最多的国家，数量略领先于美国和日本，而中美日三国占全球总体专利公开数量的 74%。人工智能需要有大数据作为原料，而中国有全球最多的人口，也有大量的工业传感器与互联网数据，基于这些基础条件，中国在大数据方面有独特的优势。这也为人工智能在中国的发展提供了坚实的基础。03人工智能铺就中国复兴之路中国在人工智能各个领域都有了一批代表性的企业。在人工智能计算机视觉领域，其落地应用遍地开花。在安防摄像头领域，主要有无人值守的场地看管、刷脸门禁、以及发现异常自动报警装置等，在这里主要的代表性公司有海康威视、大华股份等传统大公司与商汤科技、云从科技、依图科技以及旷视科技等独角兽企业，这四家公司被称为人工智能计算机视觉的“四小龙”，它们的产品在张学友演唱会上抓逃犯的过程中发挥了重要作用。在交通摄像头领域，主要是识别车辆车牌，进而进行车辆套牌分析、交通违章分析等智慧城市解决方案，在这个领域的人工智能计算机视觉的头部公司有格灵深瞳等。在金融领域，计算机视觉主要用于快速信贷审核、刷脸支付与刷脸开户等应用，在这个行业的代表性企业有商汤科技、旷视科技 Face++ 等。在医疗领域，计算机视觉主要用于智能诊断与疾病研究和精准医疗方面，在这个垂直领域的代表性企业有阿里云ET医疗大脑等。在汽车领域，计算机视觉主要用于无人驾驶，代表性的企业有百度等。百度最近与金龙汽车合作发布了阿波龙无人驾驶汽车。在无人机领域，计算机视觉主要应用于物流运输以及路径规划、地质灾害监控等，在这个领域的代表性企业是大疆科技。不久前，《科学美国人》与世界经济论坛发布了 2018 年十大新兴技术，人工智能辅助新药研发就是其中之一。目前，在全球有至少 100 家企业正在探索新药研发的人工智能方法，在国外，葛兰素史克、默克、强生与赛诺菲公司都已经布局人工智能新药研发。在中国，也涌现了深度智耀、零氪科技与晶泰科技等人工智能新药研发企业，药明康德也战略投资了美国的一家人工智能新药研发公司。在人工智能芯片领域，华为海思与寒武纪等公司纷纷布局相关芯片，云知声、出门问问、Rokid 等国内人工智能初创企业也纷纷推出了自己的芯片或模组。比如云知声发布了人工智能语音芯片雨燕以及面向智慧出行的车规级多模态人工智能芯片雪豹；而思必驰携手中芯国际，发布人工智能语音芯片 TAIHANG 。云米科技也发布了人工智能仿生芯片“悟空”。中国在人工智能金融服务中也涌现了大量优质企业。在中国出现了第四范式这类主攻银行业精准营销的人工智能公司。在教育领域，智能化的程度不断提升，近年来涌现出一批像松鼠AI、英语流利说这样充分利用人工智能技术的新型教育企业。对于中国这个在时间尺度上绵延了数千年的超级大国而言，实际上在汉唐宋以来的过去很长的一段历史内，中国在经济上与技术上领先于世界各国。只不过在工业革命以来，中国开始落后于西方。随后就有了清朝中叶开始的一系列战争赔款与“丧权辱国”的不平等条约，使得中华民族陷入了痛苦的深渊。不过，令人欣慰的是，在历经百余年的风雨洗礼之后，中华民族已经开启了复兴之路。2016 年以后，中国抓住了“人工智能”这个“第四次工业革命”的风口，中国企业积极参与到了这场新时代竞争之中。2016 年，国务院印发《新一代人工智能发展规划》，提出了面向 2030 年我国新一代人工智能发展的指导思想、战略目标、重点任务和保障措施，部署构筑我国人工智能发展的先发优势。规划提出，要建立财政引导、市场主导的资金支持机制，要利用天使投资、风险投资、创业投资基金及资本市场融资等多种渠道，引导社会资本支持人工智能发展。还要积极运用政府和社会资本合作等模式，引导社会资本参与人工智能重大项目实施和科技成果转化应用。04结语在如此大规模的支持下，人工智能产业不仅已经成为新的经济增长点，人工智能技术应用也成为改善民生的新途径，中华民族的伟大复兴也必将伴随着人工智能。* 本文经《互联网周刊》授权转载，如需转载，请联系出处。星标我，每天多一点智慧"}
{"content2":"人工智能（Artificial Intelligence），英文缩写为AI。它是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。人工智能从诞生以来，理论和技术日益成熟，应用领域也不断扩大，可以设想，未来人工智能带来的科技产品，将会是人类智慧的“容器”。人工智能可以对人的意识、思维的信息过程的模拟。人工智能不是人的智能，但能像人那样思考、也可能超过人的智能。人工智能是一门极富挑战性的科学，从事这项工作的人必须懂得计算机知识，心理学和哲学。人工智能是包括十分广泛的科学，它由不同的领域组成，如机器学习，计算机视觉等等，总的说来，人工智能研究的一个主要目标是使机器能够胜任一些通常需要人类智能才能完成的复杂工作。但不同的时代、不同的人对这种“复杂工作”的理解是不同的。 2017年12月，人工智能入选“2017年度中国媒体十大流行语”。01-Python课程（更新至17年4月）02-数学课程03-算法课程04-深度学习课程05-机器学习课程（更新至17年7月）06-数据挖掘课程07-量化交易课程08-NLP课程（更新至17年5月）09-计算机视觉课程10-Kaggle课程11-面试课程下载地址：百度网盘下载"}
{"content2":"如何做好计算机视觉的研究？“老师我们可以写论文了，”在你这样说以前，看看老师怎么说。简单说一下这篇文章的背景：从我去年（2015年）回到微软亚洲研究院之后接触到很多聪明的实习生。一方面感受到他们对计算机视觉研究的热情，另方面也有感于他们对计算机视觉研究认知的局限性，或者说大一点，是基本研究方法和思路上的局限性，就有想法要对如何做好计算机视觉的研究写点什么，但一直也没有找到合适的机会。最近计算机视觉领域国际权威、加州大学洛杉矶分校的朱松纯老师发表了一篇关于计算视觉的三个起源和人工智能的评论，引起了很大的反响。朱松纯老师的评论全面深刻，我想借着这个机会，结合朱老师评论的内容和我在计算及视觉领域15年的研究经历，也来谈谈如何做好计算机视觉的研究，希望对领域内的学生和年青的研究员能有所帮助。“如何做好计算机视觉的研究？”要回答这个问题，我们先要对这个问题的关键词进行分析：如果去掉“计算机视觉”这个限定词，这问题就变成了“如何做好研究？”那么，要回答这个问题，我们就要知道“什么是好的研究？”而要定义什么是好的研究，必须回到根本，先要知道“什么是研究？”我们的讨论就从这个问题开始。什么是研究？一个被普遍接受的对研究的广义定义为：研究是为了产生新的知识或者是为已有的知识设计新的应用的系统性的工作。因为我们今天的讨论其实更多集中在科学研究上，先确定狭义的研究的定义为：利用科学的方法来调查解释一个现象或者获取新的知识。综合这两个定义，可以看到科学研究从本质上是由三个基本的要素构成：1) 目的：产生新的知识或者是设计出新的应用; 2）手段：科学的方法。缺少这两个要素任何之一都不构成科学研究; 3) 成果：新的知识。所谓新的知识，必须是前人不知道的东西。我们很多同学和年轻的研究员认为研究就是写论文、研究成果就是论文，这其实是在观念上走进了一个误区。论文是系统阐述新的知识、新的应用，以及阐述获取这个新知识或者新应用用到了什么样的科学方法的一个载体。论文，作为阐述研究成果的主要手段，必须经过同行的评议通过才能正式发表和被认可。在人工智能进入第三个热潮之际，我们看到各种各样关于AI的各种媒体报道层出不穷，一方面，这对大众普及了AI各方面的知识，是积极的。但从另一个方面讲，很多观点没有经过仔细的推敲，也没有同行的评议，一些谬误或者是夸大的观点可能因为广泛传播而被大众接受，结果产生负面的社会影响。这就提醒我们相关领域的研究人员，在对大众媒体去做一些评论的时候，必须仔细斟酌，尽量不传播没有得到检验的观点。这就谈到第二个问题：什么是好的研究？不同领域的研究员对这个问题可能会有不同的看法。从计算机科学的角度来讲，尤其是计算机视觉的研究，无论是理论的还是实践的，我们的研究成果最终是要解决现实世界的问题的。在这个方面，我印象比较深刻的还是我在西安交通大学读研究生的时候，沈向洋博士2001年在西安交大做报告提到的一个观点：最好的研究员发现新问题；好的研究员创造新方法解好问题；一般的研究员跟随别人的方法解问题——大家在多次这里看到“新”这个关键词，创新是研究的本质。有了这些铺垫，我们首先定义什么是最好的研究。通常认为一个领域中对于某一个问题最好的研究工作有三种：第一篇论文 (The First Paper)，最好的一篇论文 (The Best Paper)，以及最后一篇论文(The Last Paper)。这第一篇论文的含义是说这篇论文率先提出了一个好的问题和方向。最好的一篇论文是什么？那一定是开创性地提出了一种解法，启发了最终解决这个问题的途径。至于最后一篇论文，那一定是彻底把这个问题解决了，从此以后这个问题不再需要继续做进一步的研究。从计算机视觉领域举一个具体的例子来讲，Harris Corner Detector属于最早的一批在图像中检测角点的论文，可以归为第一篇之列。David Lowe博士的SIFT特征检测和局部描述子，可以归为在这个方向上最好的论文之列。那么这个方向的最后一篇呢？ 我认为可能还没有出现。具体到我自己的研究工作，在局部描述子这个方向上，我跟我的同事Matthew Brown和Simon Winder在2007年到2009年之间所做的一系列用机器学习的方法来建立描述子的工作，也实际上为提高局部描述子的性能提供了一个新的思路和方法。对于我们很多研究员和学生来讲，一辈子可能都做不到这三种最好的研究工作之一。那是不是就等于说你不能做好的研究工作或者根本不用考虑做研究了呢？肯定不是这样。科学研究是一个共同体。这些最好的研究工作也是在前面很多很多非常扎实（solid）的研究工作的基础上发展出来的。因此，对于年青的研究员和学生而言，应该胸怀大志，去追求做最好的研究工作，但从实际执行上来讲，还是要把一项一项具体的工作先做扎实了。怎么做到把研究工作做扎实了？首先，你必须对你要解的问题有一个全面深刻的了解，包括为什么要解这个问题、解这个问题有什么意义呢、以前有没有试图解决同样或者类似问题的先例，如果有，你就要全面了解前人都提出了什么样的解法、他们的解法都有什么样的优势和缺陷……最后，你的解法解决了前面这些解法不能解决的问题呢，或者是你的解法处理了什么样的他们不能处理的缺陷了？这些问题的答案如果都有了，那么，在写论文的过程中要注意的就是，1）你的假设是什么？2）你怎么验证了你的假设？这个验证既可以是理论上的证明，也可以是实验的验证。我们很多学生和年青的研究员，写论文的时候没有找到内在的逻辑关系，很多观点都是似是而非。或者说重一点，在论文撰写方面的训练严重不足。你的研究如果到了写论文的阶段，那就必须要有明确的观点提出来。这个观点必须明确无误，只有这样你才能被称为形成了新的知识。你的每一个观点都必须在理论上或者是实验中得到验证。另外，论文的撰写是为了让人看懂，不是让人看不懂，所以我们在撰写过程中必须尽量保证不去假设读者已经拥有了某些方面的知识。做好了这些，基本上你就有很大的可能性能够做出扎实（solid)的研究工作。然后回到我们讨论的主题：如何做好计算机视觉的研究工作？其实，要回答这个问题，将我上面讲的所有观点加上“计算机视觉领域”这个限定词就行了。我这儿结合计算机视觉研究的一些现状及朱松纯老师的一些观点来进一步谈谈我的观点。首先谈谈我观察到的一些现象。很多年轻的学生，现在讨论问题的时候都用这样的谈话：我发现用FC6层的特征，比用FC7层的特征，在某个图像数据集上比现在最好的算法提高了1.5%的识别精度，老师我们可以写论文了（如果大家不能理解这句话，FC6和FC7是表示AlexNet的两个中间输出层）。我想请问，你在这个过程中发现了什么样的普适的新的知识吗，又或者，在不是普适的情况下，你在什么限定条件下一定能够看到这样的识别精度提高了？不错，提高识别精度是一个很好的目标，但要注意，计算机视觉的研究是要解决识别的问题，不是解某一个图像数据集。这些图像数据集提供了很好的验证你的假设和方法的手段，但如果你没有遵循科学的方法和和手段去设计你的算法和实验，你也不可能得到一个科学的结论，从而也不能产生新的知识，更不用谈对这个领域做出贡献。朱松纯老师在他的评论中提到，很多学生认为，计算机视觉现在就是调深度神经网络的参数，也就是说的这个问题。所以，具体到对于刚开始从事计算机视觉研究的学生来讲，要做好这方面的研究，我觉得第一步还是要系统学习一下计算机视觉的课程，全面了解一下计算机视觉这个领域的来龙去脉、这个领域都有哪些基本的问题、哪些问题已经解得比较成熟而哪些问题还在初级阶段……这里，推荐所有的学生学习两本经典教材《Computer Vision: A Modern Approach》和《Computer Vision: Algorithms and Applications》，可以先读完第一本再读第二本。只有对这个领域有了一个初步的全面了解，你才能够找到自己感兴趣的那个问题。在众多的问题当中，你是希望做三维重建，还是做图像识别、物体跟踪，又或是做计算摄影呢？做研究其实不是一个完全享乐的的过程，你必须要有足够的兴趣来保证你能持续地走下去，这在你感觉自己当前研究的思路走不下去的时候尤其具有重要意义。当你确定你感兴趣的问题，你应该首先全面调研一下这个问题的来龙去脉。这就意味着你不能只读过去五年的论文。你可以从过去一年的论文开始，慢慢追溯回到过去很久的相关的论文。有些时候，你会惊讶地发现前人想问题的深度。研究的英文单词是Research，拆开是Re-Search，用中文直译就是重新搜索和发现，而不是直接发现，其实就是说你要首先对这个问题做追本溯源。朱松纯老师提到的我们很多学生现在不读五年以前的论文，说的也是这个道理。当你做好了这些，你必须钻进计算视觉的一个小的领域。人的精力是有限的，这就意味着你不可能把很多事情同时做好，所以在你选好方向之后，就要把你的精力集中在你感兴趣的一个问题上， 努力成为这个方面的专家。研究是一项长跑，很多时候，你在一个方向上比别人坚持久一点， 你就有机会超越他而成为某个方面的专家。最后，我也来谈谈深度学习对计算机视觉的影响。在这里，我对马里兰大学Rama Chellapa教授在Tom Huang教授80岁生日论坛上表达的观点非常认可，他认为，深度学习网络就像一个Pasta Machine：你把该放的东西放进去，它能给你产生好吃的Pasta。同时它也是一个Equalizer：无论你在计算机视觉领域有40年的经验还是0年的经验，只要你会用Caffee，你在一些问题，比方说图像识别上，都能产生差不多的结果。他开玩笑说这有点伤自尊 (It hurts my ego!)，但我们还是应该把它作为一个好的工具拥抱它。我想，他的言外之意，是我们的研究应该做得更深，要去理解这个工具为什么能够工作得比较好，从而产生新的知识去指导将来的研究和应用。我认为，对于年轻的学生来讲，从深度学习的方法开始学习没有什么问题，但必须要进一步去了解一下其他的数学和算法工具，像统计贝叶斯的方法、优化的方法、信号处理的方法等等等的。计算机视觉的问题，其本质是不适定的反问题，解这一类问题需要多种方法的结合。这里面有深度学习解得比较好的问题，像图像识别，也有深度学习解不了的问题，像三维重建和识别。任何研究领域包括计算机视觉的研究，对处在研究初期的学生而言， 更重要的是掌握足够的数学工具，培养一种正式思维（Formal Thinking）的能力，这样，遇到实际的问题就能以一种理论上正确的思路去解决这个问题。作为结束语，我想对在从事或者有志于从事计算机视觉研究的学生说，计算机视觉的研究处在一个非常好的时期，有很多我们原来解不了的问题现在能够解得比较好了，像人脸识别，尽管我们其实还没有从真正意义上达到人类视觉系统对人脸识别的鲁棒程度。但我们离真正让计算机能够像人看和感知这个世界还有很远的距离。在我们达到这个目标之前，深度学习的方法可能是这个过程中一个重要的垫脚石，同时我们还要将更多的新的方法和工具带入这个领域来进一步推动这个领域的发展。作者简介华刚博士是微软亚洲研究院资深研究员，现任微软亚洲研究院计算视觉组负责人。他的研究重点是计算机视觉、模式识别、机器学习、人工智能和机器人，以及相关技术在云和移动智能领域的创新应用。他因在图像和视频中无限制环境人脸识别研究做出的突出贡献，于2015年被国际模式识别联合会（International Association on Pattern Recognition）授予”生物特征识别杰出青年研究员”奖励，因其在计算机视觉和多媒体研究方面的杰出贡献，于2016年被遴选为国际模式识别联合会院士（IAPR Fellow）和国际计算机联合会杰出科学家(ACM Distinguished Scientist) 。。华刚博士已在国际顶级会议和期刊上发表了120多篇同行评审论文。他将担任2019国际模式识别和计算机视觉大会 （CVPR 2019）的程序主席，以及CVPR 2017和ACM MM 2017的领域主席。此前华刚博士曾担任CVPR 2014、ICCV 2011、ACM MM 2011/ 2012/ 2015、ICIP 2012/2013/2015/2016、ICASSP 2012/ 2013等十多个顶级国际会议的领域主席，以及IEEE Trans. on Image Processing（2010-2014）编委。目前，华刚博士还担任着IEEE Trans. on Image Processing、IEEE Trans. on Circuits Systems and Video Technologies、IEEE Multimedia、CVIU、MVA和VCJ的编委。"}
{"content2":"作机器视觉和图像处理方面的研究工作，最重要的两个问题：其一是要把握住国际上最前沿的内容；其二是所作工作要具备很高的实用背景。解决第一个问题的办法 就是找出这个方向公认最高成就的几个超级专家(看看他们都在作什么)和最权威的出版物(阅读上面最新的文献)，解决第二个问题的办法是你最好能够找到一个 实际应用的项目，边做边写文章。 做好这几点的途径之一就是利用网络资源，利用权威网站和专家们的个人主页。依照下面目录整理：[1]研究群体(国际国内)[2]专家主页[3]前沿国际国内期刊与会议[4]搜索资源[5]GPL软件资源一、研究群体用来搜索国际知名计算机视觉研究组(CV Groups)：国际计算机视觉研究组清单http://peipa.essex.ac.uk/info/groups.html美国计算机视觉研究组清单 http://peipa.essex.ac.uk/info/groups.html#USAhttp://www-2.cs.cmu.edu/~cil/vision.html或 http://www.cs.cmu.edu/~cil/vision.html这是卡奈基梅隆大学的计算机视觉研究组的主页，上面提供很全的资料，从发表文章的下载到演示程序、测试图像、常用链接、相关软硬件，甚至还有一个搜索引擎。著名的有人物Tomasi， Kanade等。卡内基梅隆大学双目实验室http://vision.middlebury.edu/stereo/卡内基梅隆研究组http://www.cs.cmu.edu/~cil/v-groups.html还有几个实验室：Calibrated Imaging Laboratory 图像Digital Mapping Laboratory 映射Interactive Systems Laboratory 互动Vision and Autonomous Systems Center视觉自适应http://www.via.cornell.edu/康奈尔大学的计算机视觉和图像分析研究组，好像是电子和计算机工程系的。侧重医学方面的研究，但是在上面有相当不错资源，关键是它正在建设中，能够跟踪一些信息。Cornell University——Robotics and Vision grouphttp://www-cs-students.stanford.edu/ 斯坦福大学计算机系主页1. http://white.stanford.edu/2. http://vision.stanford.edu/3. http://ai.stanford.edu/美国斯坦福大学人工智能机器人实验室The Stanford AI Lab (SAIL) is the intellectual home for researchers in the Stanford Computer Science Department whose primary research focus is Artificial Intelligence. The lab is located in the Gates...Vision and Imaging Science and Technologyhttp://www.fmrib.ox.ac.uk/analysis/主 要研究：Brain Extraction Tool， Nonlinear noise reduction， Linear Image Registration， Automated Segmentation， Structural brain change analysis， motion correction， etc.http://www.cse.msu.edu/prip/—密歇根州立大学计算机和电子工程系的模式识别--图像处理研究组，它的FTP上有许多的文章(NEW)。美国密歇根州大学认知模型和图像处理实验室The Pattern Recognition and Image Processing (PRIP) Lab faculty and students investigate the use of machines to recognize patterns or objects. Methods are developed to sense objects， to discover which...http://www.cse.msu.edu/rgroups/prip/http://pandora.inf.uni-jena.de/p/e/index.html德国的一个数字图像处理研究小组，在其上面能找到一些不错的链接资源。柏林大学 http://www.cv.tu-berlin.de/德国波恩大学视觉和认识模型小组Computer Vision Group located within the Division III of the Computer Science Department in the University of Bonn in Germany. This server offers information on topics concerning our computer vision http://www-dbv.informatik.uni-bonn.de/http://www-staff.it.uts.edu.au/~sean/CVCC.dir/home.htmlCVIP(used to be CVCC for Computer Vision and Cluster Computing) is a research group focusing on cluster-based computer vision within the Spiral Architecture.http://cfia.gmu.edu/The mission of the Center for Image Analysis is to foster multi-disciplinary research in image， multimedia and related technologies by establishing links between academic institutes， industry and government agencies， and to transfer key technologies to help industry build next generation commercial and military imaging and multimedia systems.英国的Bristol大学的Digital Media Group在高级图形图像方面不错。主要就是涉及到场景中光线计算的问题，比如用全局光照或是各种局部光照对高动态图的处理，还有近似真实的模拟现实环境 (照片级别的)，还有用几张照片来建立3D模型(人头之类的)。另外也有对古代建筑模型复原。http://www.cs.bristol.ac.uk/Research/Digitalmedia/而且根据Times全英计算机排名在第3， 也算比较顶尖的研究了http://www.cmis.csiro.au/IAP/zimage.htm这是一个侧重图像分析的站点，一般。但是提供一个Image Analysis环境---ZIMAGE and SZIMAGE。麻省理工视觉实验室MIT http://groups.csail.mit.edu/vision/welcome/AI Laboratory Computer Vision groupCenter for Biological and Computational LearningMedia Laboratory， Vision and Modeling GroupPerceptual Science groupUC Berkeley http://0-vision.berkeley.edu.ilstest.lib.neu.edu/vsp/index.htmlhttp://www.cs.berkeley.edu.ilste ... n/vision_group.html加州大学伯克利分校视觉实验室David A. Forsyth：http://www.cs.berkeley.edu/~daf/UCLA(加州大学洛杉矶分校) http://vision.ucla.edu/ 视觉实验室英国牛津的A.Zisserman：http://www.robots.ox.ac.uk/~az/ 机器人实验室美国南加州大学智能机器人和智能系统研究所University of Southern California， Los AngelesIRIS is an interdepartmental unit of USC's School of Engineering with ties to USC's Information Sciences Institute (ISI). Members include faculty， graduate students， and research staff associated with... http://iris.usc.edu/ Computer Vision 实验室美国南加州大学计算机视觉实验室介绍：Computer Vision Laboratory at the University of Southern California is one of the major centers of computer vision research for thirty years. they conduct research in a number of basic and applied are...http://iris.usc.edu/USC-Computer-Vision.html英国约克大学高级计算机结构神经网络小组The Advanced Computer Architecture Group has had a thriving research programme in neural networks for over 10 years. The 15 researchers， led by Jim Austin， focus their work in the theory and applicati...http://www.cs.york.ac.uk/arch/neural/瑞士戴尔莫尔感知人工智能研究所IDIAP is a research institute established in Martigny in the Swiss Alps since 1991. Active in the areas of multimodal interaction and multimedia information management， the institute is also the leade...http://www.idiap.ch/英国萨里大学视觉，语言和信号处理中心The Centre for Vision， Speech and Signal Processing (CVSSP) is more than 60 members strong， comprising 12 academic staff， 18 research fellows and more than 44 research students. The activities of the ...http://www.ee.surrey.ac.uk/Research/VSSP/美国阿默斯特马萨诸塞州立大学计算机视觉实验室The Computer Vision Laboratory was established in the Computer Science Department at the University of Massachusetts in 1974 with the goal of investigating the scientific principles underlying the con...http://vis-www.cs.umass.eduUniversity of Massachusetts——Computer Vision Laboratory for Perceptual Robotics美国芝加哥伊利诺伊斯大学贝克曼研究中心智能机器人和计算机视觉实验室Includes the following groups: Professor Seth Hutchinson's Research Group Professor David Kriegman's Research Group Professor Jean Ponce's Research Group Professor Narendra Ahuja's Research Gro...http://www-cvr.ai.uiuc.edu/Computer Vision and Robotics LaboratoryVision Interfaces and Systems Laboratory (VISLab)英国伯明翰大学计算机科学学校视觉研究小组The vision group at the School of Computer Science (a RAE 5 rated department) performs research into a wide variety of computer vision and image understanding areas. Much of this work is performed in ...http://www.cs.bham.ac.uk/research/vision/微软研究院机器学习与理解研究小组 / 计算机视觉小组The research group focuses on the development of more advanced and intelligent computer systems through the exploitation of statistical methods in machine learning and computer vision. The site lists ...http://research.microsoft.com/mlp/http://research.microsoft.com/en-us/groups/vision/微软公司的文献：http://research.microsoft.com/research/pubs微软亚洲研究院：http://research.microsoft.com/asia/，值得关注Harry Shum， Jian Sun， Steven Lin， Long Quan(兼职HKUST)etc.瑞典隆德大学数学系视觉组：http://www.maths.lth.se/matematiklth/personal/andersp/感觉国外搞视觉的好多是数学系出身，大约做计算机视觉对数学要求很高吧。澳大利亚国立大学：http://users.rsise.anu.edu.au/~hartley/美国北卡大学：http://www.cs.unc.edu/~marc/法国INRIA：http://www-sop.inria.fr/odyssee/team/ 由Olivier.Faugeras领衔的牛人众多。比利时鲁汶大学的L.Van Gool： www.esat.kuleuven.ac.be/psi/visics/据说在这个只有中国一个小镇大小的地方的鲁汶大学在欧洲排行top10，名列世界top100，还出了几个诺贝尔奖，视觉研究也很强.美国明德http://vision.middlebury.edu/stereo/以下含有非顶尖美国学校研究组，没有链接(个别的上面已经提到)，供参考。Amerinex Applied Imaging， Inc.Boston UniversityImage and Video Computing Research groupUniversity of California at Santa Barbara加州大学芭芭拉分校Vision Research LabUniversity of California at San Diego加州大学圣迭戈分校Computer Vision & Robotics Research LaboratoryVisual Computing laboratoryUniversity of California at Irvine加州大学欧文分校，加州南部一城，在圣安娜东南，Computer Vision laboratoryUniversity of California， Riverside加州大学河滨分校Visualization and Intelligent Systems Laboratory (VISLab)University of California at Santa CruzPerceptual Science LaboratoryCaltech (加州理工)Vision groupUniversity of Central FloridaComputer Vision laboratoryUniversity of FloridaCenter for Computer Vision and VisualizationColorado State UniversityComputer Vision groupColumbia UniversityAutomated Vision Environment (CAVE)Robotics groupUniversity of Georgia， AthensVisual and Parallel Computing LaboratoryHarvard University（哈佛）Robotics LaboratoryUniversity of Illinois at Urbana-ChampaignRobotics and Computer VisionUniversity of IowaDivision of Physiologic ImagingJet Propulsion LaboratoryMachine Vision and Tracking Sensors groupKhoral Research， IncLawrence Berkeley LaboratoriesImaging and Collaborative Computing GroupImaging and Distributed ComputingLehigh UniversityImage Processing and Pattern Analysis LabVision And Software Technology LaboratoryUniversity of LouisvilleComputer Vision and Image Processing LabUniversity of MarylandComputer Vision LaboratoryUniversity of MiamiUnderwater Vision and Imaging LaboratoryUniversity of Michigan密歇根AI LaboratoryMichigan State University 密歇根州立Pattern Recognition and Image Processing laboratoryEnvironmental Research Institute of Michigan (ERIM) 密歇根大学有汽车车身检测研究University of Missouri-ColumbiaComputational Intelligence Research LaboratoryNECComputer Vision and Image ProcessingUniversity of NevadaComputer Vision LaboratoryNotre-Dame UniversityVision-Based Robotics using EstimationOhio State UniversitySignal Analysis and Machine Perception LaboratoryUniversity of PennsylvaniaGRASP laboratoryMedical Image Processing groupVision Analysis and Simulation Technologies (VAST) LaboratoryPenn State University 宾夕法尼亚大学Computer VisionPrecision Digital ImagesPurdue University普渡大学Robot Vision laboratoryVideo and Image Processing Laboratory (VIPER)Rensselaer Polytechnic Institute (RPI)Computer Science VisionUniversity of RochesterCenter for Electronic Imaging SystemsVision and Robotics laboratoryRutgers University (The State University of New Jersey)Image Understanding LabUniversity of Southern CaliforniaComputer VisionUniversity of South FloridaImage Analysis Research groupStanford Research Institute International (SRI)RADIUS -- Research and Development for Image Understanding SystemsThe Perception program at SRI's AI CenterSUNY at Stony BrookComputer Vision LabUniversity of TennesseeImaging， Robotics and Intelligent Systems laboratoryUniversity of Texas， AustinLaboratory for Vision SystemsUniversity of UtahCenter for Scientific Computing and ImagingRobotics and Computer VisionUniversity of VirginiaComputer Vision Research (CS)University of WashingtonImage Computing Systems LaboratoryInformation Processing LaboratoryCVIA LaboratoryUniversity of West FloridaImage Analysis/Robotics Research LaboratoryUniversity of WisconsinComputer Vision groupVanderbilt UniversityCenter for Intelligent SystemsWashington State UniversityImaging Research laboratoryWright-PattersonModel-Based Vision laboratoryWright State UniversityIntelligent Systems LaboratoryUniversity of WyomingWyoming Image and Signal Processing Research (WISPR)Yale UniversityComputational Vision Group http://www.cs.yale.edu/School of Medicine， Image Processing and Analysis group国内：中科院模式识别国家重点实验室 http://www.nlpr.ia.ac.cn/English/rv/mainpage.html虹膜识别、掌纹识别、人脸识别、莲花山http://www.stat.ucla.edu/~sczhu/Lotus/天津大学精密测试技术及仪器国家重点实验室研究方向包括：激光及光电测试技术、传感及测量信息技术、微纳测试与制造技术、制造质量控制技术。该实验室是国内精密测试领域惟一的国家重点实验室。“智能微系统及其集成应用技术”、“微结构光学测试技术”、“油气储运安全检测技术”、“先进制造中的视觉测量及其关键技术”、“正交偏振激光器原理、特性及其在精密计量中的应用研究”等5项代表性成果（07.3）。中科院长春光机所 http://www.ciomp.ac.cn/ny/keyan.asp中科院沈阳自动化所http://www.sia.ac.cn/index.php中科院西安光机所http://www.opt.ac.cn/yanjiushi/gpcxjs1.htm北京大学智能科学系http://www.cis.pku.edu.cn/vision/vision.htm三维视觉计算与机器人，生物特征识别与图像识别二、专家网页http://www.ai.mit.edu/people/wtf/这位可是MIT人工智能实验室的BILL FREEMAN。专长是：理解--贝叶斯模型。http://www.merl.com/people/brand/MERL(Mitsubishi Electric Research Laboratory)中的擅长“Style Machine”。http://research.microsoft.com/~ablake/CV界极有声望的A.Blake 1977年毕业于剑桥大学三一学院并或数学与电子科学学士学位。之后在MIT，Edinburgh，Oxford先后组建过研究小组并成为Oxford的教授，直到1999年进入微软剑桥研究中心。主要工作领域是计算机视觉。http://www-2.cs.cmu.edu/afs/cs.cmu.edu/user/har/Web/home.html这位专家好像正在学习汉语，主页并且搜集了诸如“两只老虎(Two Tigers)”的歌曲。他的主页上面还有几个专家：Shumeet Baluja， Takeo Kanade。他们的Face Detection作的绝对是世界一流。毕业于卡奈基梅隆大学的计算机科学系，兴趣是计算机视觉。"}
{"content2":"尼克，乌镇智库理事长，数知科技创始人，国家“千人计划”专家。早年负笈美国，师从“强化学习”算法发明者；曾在哈佛和惠普工作。近日，尼克的《人工智能简史》由人民邮电出版社出版，他在书中全面勾勒人工智能半个多世纪的历史，再现了AI史上大师的工作、交往。就人工智能的历史、现实与未来诸问题，《上海书评》对他作了专访。尼克：《人工智能简史》，人民邮电出版社，2017年12月，320页。采访︱丁雄飞 李文逸《经济学人》（The Economist）今年7月15日发表的题为“中国或在人工智能领域赶上或击败美国”（China May Match or Beat America in AI）的文章里，引用了乌镇智库所作的人工智能报告的数据。乌镇智库和人工智能之间是什么关系？从数据看人工智能，您可以告诉我们什么？尼克：乌镇智库搜集了人类有史以来几乎所有的公开数据，例如所有的专利、所有的学术资源、所有的经济金融数据、法院判例，还有过去十几年里所有语言的维基百科和各种在线百科数据，以及社交媒体数据。我们把这些数据都建成知识图谱，也整合了IBM著名的Watson系统底层的开源知识图谱。在我们做的各种行业报告中，最有影响力的确实是关于人工智能的系列报告。目前，《经济学人》、《金融时报》、高盛等机构关于人工智能的深度报道大量引用我们的数据。可以说，中文媒体上涉及人工智能的报道，其数据部分的源头大多在我们这儿。《经济学人》刊载的2005、2010年相关国家AI专利申请数量，以及2016年相关国家AI公司数量。具体来说，从专利看人工智能，我们可以知道： 1980年代，中国尚没有专利制度，更谈不上人工智能专利，但到了2012至2013年间，中国人工智能的专利总数已经超过美国，但专利的质量还不高。专利质量是可以定量计算出来的，学术论文的结果也类似。根据我们做的人工智能的大学排名，前四十名里没有中国，但从四十几名开始，就有清华了。在人工智能的分支学科中，八十年代极盛的专家系统，今天已无人问津，这在工业界的反映就是日本第五代计算机项目的衰败。2010年后兴起、今天还十分热门的人工智能子学科包括：机器学习、计算机视觉、自然语言处理。机器翻译就属于自然语言处理。《金融时报》刊载的AI相关研究论文引用排名我们还可以利用数据做区域的对比研究。中国在人工智能的投融资已经是世界第二，但也差不多只是以色列的四倍。以色列是个人口八百万的国家，比上海的一个区还要小。而我们做的物联网报告显示，中国的物联网体量比以色列要大四十多倍。这说明涉及高精尖技术的，以色列很强，而涉及人口和制造的，中国强。您为什么要写人工智能的历史？去年来，AlphaGo不断战胜人类围棋冠军，而AlphaGo Zero还能自己从低阶到高阶对弈，能谈谈您和这个团队的渊源吗？人机围棋对决，柯洁不敌AlphaGo。尼克：现在人工智能这么热，需要有本书把它的历史说明白。另外，国内没什么像样的人工智能的科普。我看过很多伪媒体人和“砖家”的各种言论，胡说八道的程度令人发指，可怕的是其中一些人还有巨大的话语权和影响力，控制着各种资源。我自认还够格写这样一本书。人工智能发展过程中的不少事儿，我大致都清楚。一方面出于兴趣，另一方面，我的老师和大师兄是这个领域的大师级人物，他们发明了“强化学习”算法。谷歌收购的DeepMind团队里一半的人都是我大师兄的学生，他们曾经是人工智能中的少数派，但DeepMind搞的AlphaGo赢了李世石之后，这一派一下又成了显学。当年把我老师招到麻省大学的是迈克尔·阿比布（Michael Arbib）。他是控制论创始人维纳的最后一个博士生。按照阿比布的一家之言，人工智能是控制论的替代品。我当年本想投奔的是阿比布，但我到学校，他已转会去了南加州大学，结果我就跟了我老师研究强化学习。从这个意义上说，我是AlphaGo那帮人的长辈。迈克尔·阿比布DeepMind团队我上学的时候正值人工智能低潮。一般美国大学的计算机系都是分三伙人：做系统的，做理论的，做人工智能的。做系统的和做理论的互相看不起，但他们同时看不起做人工智能的。现在情况不一样了，做人工智能的应该都咸鱼翻身了，个个成了公共知识分子。我那时才疏学浅，看不清强化学习的远景，证明了一个与机器学习相关的理论结果就离开了。其实，在人工智能的各个分支里，大概只有强化学习还留了点控制论的影子，也有人认为，强化学习包含了全部人工智能。“人工智能”与“控制论”词频对比（引自《人工智能简史》）人工智能学界，国内有一个和美国不同的现象：中国在人工智能领域最有发言权的是自动化系——对应到中科院就是自动化所（新成立的人工智能学院就设在这里），而自动化系主要关注计算机是怎么用的，并不关注计算机的基础理论。美国的大学并没有分得这么细，加州大学伯克利分校和麻省理工学院到现在还是一个大系：EECS（Department of Electrical Engineering and Computer Science）——这在中国至少能拆成五个学院。中国一个很小的学科分支都是一个学院，而学院之间老死不相往来。人工智能这样的学科，如果不懂点图灵的计算理论，讨论就很难深入。我除了想正本清源，还想鼓励专家间的互动。我也希望决策者看看我的书，了解些科普和历史，也许可以少被伪媒体人和“砖家”们忽悠，少浪费些社会资源。我再说多了就要挨骂了。西方有对人工智能历史的经典书写吗？尼克：美国人写的人工智能历史，比较著名的有尼尔森（Nils J. Nilsson）的《人工智能探究》（The Quest for Artificial Intelligence: A History of Ideas and Achievements）。尼尔森是人工智能学科的早期参与者和领导者，担任过斯坦福研究所（SRI）人工智能部门的负责人和斯坦福大学计算机系主任多年。不过他这本书主要写的是他自己熟悉的、偏好的领域。麦克达克（Pamela McCorduck）1979年写的《能思考的机器》（Machines Who Think: A Personal Inquiry into the History and Prospects of Artificial Intelligence）从今天的角度看则有些过时。尼尔森：《人工智能探究》麦克达克：《能思考的机器》另外，明尼苏达大学的查尔斯·巴贝奇（Charles Babbage）研究所一直在做计算机科学的口述历史，采访了很多人工智能学者。大部分的采访都有录音。我听了近一百小时录音，从这些第一手资料里有一些有趣的新发现。无论中英文，当下都没有一本合适的人工智能历史的读物。总的来讲，我这本书还算公正全面，我儿子正在把它译成英文。通过第一手资料，您有什么关于AI史的新发现？尼克：比如“人工智能”（artificial intelligence）这个词最早是谁提出的。普遍的误解是，“人工智能”这个词是1956年达特茅斯学院夏季研讨会——人工智能的起源事件——的召集者麦卡锡（John McCarthy）想出来的，其实不然。麦老晚年回忆，承认这个词最早是从别人那里听来的，但记不清是谁了。后来英国数学家伍德华（Philip Woodward）给《新科学家》杂志写信说他是AI一词的原创者，麦卡锡最早是听他说的，因为他1956 年曾去麻省理工学院访问，与麦卡锡交流过。但麦卡锡在1955 年就在其建议书里使用“人工智能”一词了。如今当事人大都已仙逝，这事恐成悬案。其实英国人最早的说法是“机器智能”（machine intelligence），这大概和图灵1950年在哲学杂志《心》（Mind）上发表的传世文章《计算机与智能》（Computing Machinery and Intelligence——我的译文作为附录收在书中）有关。最早Computing Machinery指计算机，而Computer是指人肉计算员，他/她们用机械计算机从事简单重复的计算工作。图灵的《计算机与智能》发表于1950年10月的《心》另外，我比较详细地考证了美国人工智能几大学派之间的矛盾和论争。美国最早有三大人工智能基地：斯坦福大学、麻省理工学院和卡内基梅隆大学。三大基地是三伙不同的人弄的，这些实验室经历了种种的斗争、分裂、重组。这些人事纷争构成了人工智能学科的历史。事实上，当图灵在1948 年英国国家物理实验室的内部报告中区分了“肉体智能”/“附体智能”（embodied intelligence）和“无肉体智能”（disembodied intelligence），后来的统计派/神经网络派（造一台智能机器模拟大脑中的神经网络）与符号派/逻辑派（用逻辑和符号系统模拟心智）之争就已经埋下了伏笔。人工智能的鼻祖之一纽厄尔（Allen Newell）说过，一部AI史就是一部斗争史（大意如此）。换言之，在任何时候，每种方法都有个对立面：模拟与数字，知识与逻辑，语义与语法，连续与符号，串行与并行，取代与增强，机械论与目的论，生物学与活力论，工程与科学……麦卡锡在斯坦福大学的人工智能实验室司马贺最早的麦卡锡、司马贺（Herbert Simon）分别是做逻辑和定理证明的，做统计的人当时不被重视，但现在反而逻辑没人搞了，都去做统计了。近年来，知识图谱技术在谷歌的鼓吹下，算是为符号派留下了一支血脉。自人工智能起源至今，半个多世纪过去了，它在哪些方面业已取得了突破？尼克：参加了达特茅斯会议的纽厄尔和司马贺在1957 年曾预测：十年内计算机下棋能赢人，十年内计算机将能证明人还没有证明的定理。他们太乐观了。这两个预测分别在1997年和1996年才实现，花了大概四十年。1985年4月14日，纽厄尔在旧金山参加美国计算机协会的人机交互大会。2006 年，达特茅斯会议五十周年时，当时的十位与会者中有五位仙逝，活着的五位——摩尔、麦卡锡、明斯基、塞弗里奇、所罗门诺夫在达特茅斯重聚。从人工智能的历史看，确实有很多过去认为是很难解决的问题被慢慢解决，比如人脸识别近五年在国内迅猛发展。目前，借助“深度学习”（多层神经网络）的语音识别系统已经达到可实用的阶段。过去几年也有一堆同质的公司冒出来。机器翻译难道还没有突破吗？尼克：的确，随着语音和图像识别技术渐趋成熟，人们普遍认为目前人工智能里比较难的问题是自然语言处理。有专家最近有言：懂语言者得天下。机器翻译就是自然语言处理的一部分。2016 年，谷歌利用深度神经网络搞的神经机器翻译（Google Neural Machine Translation）系统，大幅提高了机器翻译的水平；今年，Facebook利用自己擅长的卷积神经网络，也进一步提高了机器翻译的效率。但这距离理想场景——例如人们可以不学外语，人耳中嵌入微型翻译器，自动译出听到的外语——还很远。谷歌神经机器翻译系统翻译的例句自然语言处理涉及了不少哲学主题，从图灵测试到塞尔的“中文屋”假想实验，皆与此相关。每出现一个新的人工智能工具，都会被用来试试自然语言处理。除了翻译之外，自然语言处理的另一个难题是人机对话。现在的对话都是在短场景——十个句子之内，因为问答系统依靠的是常识和浅层的推理；知识图谱是核心。一个问题总是会涉及who，when，where，how，why，这些要素都可以套到知识图谱中类和实体的属性和关系上。当你问2016 年之后的搜索引擎：“梁启超的儿媳妇是谁？”答案中至少会有“林徽因”和“林洙”。因为系统底层的知识图谱知道梁思成是梁启超的儿子，而梁思成结了两次婚，第一次林徽因，第二次林洙。当知识图谱足够大的时候，它回答问题的能力会惊人：2011 年IBM的沃森（Watson）就在美国电视智力竞赛节目Jeopardy！中击败人类选手。但是，目前要达到很长的人机对话场景还很难。沃森在Jeopardy！中击败人类选手这些技术是人工智能下一步的突破口，我们也努力在这些领域做些有意义的创新性工作。如《银翼杀手》这样的赛博格电影总会关注未来人工智能的主体意识和人机关系这样的议题。您是怎么看“奇点”（singularity）的？尼克：所谓“奇点”——机器超过人，或者用《未来简史》（Homo Deus: A Brief History of Tomorrow）作者的话说，有一个全新的物种在智能上超越人类——本身的定义是不严格的。如果就某个单项指标论，机器早就超过人类了。讨论这个问题，首先要定义智能是什么、人是什么。当下，总有一些人会干而机器不会干的事，以及机器会干而人不会干的事，但不也总有这个人会干，而那个人不会干的事吗？能说其中一个就不是人吗？我们甚至可以追问：当你断了一条胳膊、换了一颗别人的心脏，你还是你吗？如果把你的头换到别人的身体上，你还是你吗？这个时候，DNA可能都是别人的了，但意识还是你的。科幻电影总是把人工智能问题化约为是不是能造出人形机器人，这是低级而庸俗的。界定人或智能是什么、追问机器是否有智能，需要诉诸计算理论。2017年6月7日，高考机器人AI-MATHS在断网断题库的情况下完成了北京文科数学卷和全国二卷数学卷，分别用时二十二分钟与十分钟，成绩分别为一百零五分与一百分（满分一百五十分）。一直有所谓强人工智能和弱人工智能之说：强人工智能就是能造出全面超越人类的机器，而弱人工智能是指能造出在某些方面——例如下棋、人脸识别——超越人类的机器。根据丘奇-图灵论题（Church-Turing Thesis），所有功能足够强的计算装置的计算能力都等价于图灵机，不可能存在比图灵机更强的计算装置。除了丘奇-图灵论题，还有个相似性原则：任何计算装置之间互相模拟的成本是相似的。这两个论题隐含着强人工智能的可能性：智能等价于图灵机、人就是图灵机。目前的计算机科学（包括人工智能）的工作都是建立在这个认同之上的。当服从摩尔定律（每十八个月信息处理能力加倍）的计算装置进化的速度快过人类进化的速度，那么就有“奇点”来临的那一天。那时，自然语言理解、机器定理证明都不是事儿。可运行的纸带版图灵机也不是所有的科学家都相信丘奇-图灵论题和相似性原则。代表人物就是英国数学家、《皇帝新脑》（Emperor’s New Mind: Concerning Computers, Minds, and the Laws of Physics）的作者彭罗斯（Roger Penrose）。当下很热门的量子计算机就有可能不服从相似性原则。量子计算机或许能有效地解决素数分解问题，这是当今公钥加密算法的基础，如果量子计算机成功，那当前的电子商务体系就会出现不安全隐患。当然，大规模、可实用的量子计算机的实现，仍然存在困难，目前在这方面最领先的是IBM。彭罗斯：《皇帝新脑》根据您的理解，人工智能为什么会在这几年进展迅速？尼克：主要原因有两个，一个是大规模的数据累计，另一个是计算能力的提升，同时达到了拐点。我想提出一个更普遍的观点：今天，测度人类文明的标准是全社会的算力。斯坦福大学历史学教授莫里斯（Ian Morris）在《西方将主宰多久》（Why the West Rules—For Now）一书中，用能耗作为主要测量参数，比较了东西方文明。但随着新能源技术的出现，例如前几年的页岩气的开发，能源问题已经在一定程度上得到缓解。在我看来，就过去一百多年而言，测度文明的指标是信息处理能力。具体到这几十年，信息处理能力可以用计算能力来衡量——全社会的算力是全社会计算设备的数目与单台设备计算能力的乘积。计算机科学论文、计算机科学专利、万维网流量、社会算力、人工智能全球融资总额、摩尔定律及学术出版物的增长趋势对比（引自《人工智能简史》）今年6月，马云在天津举行的首届世界智能大会上作了《智能改变世界》的主题报告；早先，腾讯也实践了聊天机器人。人工智能在中国的前景如何？眼下存在泡沫吗？尼克：不论是去年中国科学家、企业家群体发起的未来科学大奖（号称中国的诺贝尔奖，单项奖金一百万美金），还是今年阿里巴巴成立的探索科技未来的实验室“达摩院”（宣称将在三年内投资千亿），都让人感觉中国试图在科技领域确立世界的领导权。且不论具体的实施，我认同民间资助科技的这个思路和运作方式。阿里巴巴“达摩院”但上世纪八十年代日本五代机的教训足以让我们引以为戒。当时日本在制造业和集成电路上大举超越美国，五代机便是日本雄心勃勃试图建立在全球信息产业的领导地位、从制造大国转型为经济强国的计划的一部分。与此同时，美国的费根鲍姆（Edward Albert Feigenbaum）等人也使劲吹捧五代机，目的则是为了给本国政府施压，要求增加在科技领域，尤其是在自己领域——专家系统的投资。当然，最后五代机是个彻底的失败。五代机的失败让日本整个信息产业一直落后于美国，使得日本最聪明的一批人的黄金十年被耽误了，也严重打击了当时所谓“日本第一”的民族自信。1981年第一次五代机会议的会议录（共二百八十八页）几年前，我花时间研究了日本五代机所有会议的论文集。从1988 年的会议录——一千三百页的三大卷——可以看出，当时五代机已经成了大杂烩，失去了聚焦点，八杆子打不着的领域也拼命向它靠拢。这就像当下人工智能领域的创业，一些和人工智能毫无瓜葛的人一夜间都成了人工智能专家。很多创业者也是拿到风投的钱之后再考虑做什么。可以说，现在人工智能里面肯定有泡沫。但是不是有很大的泡沫，我不敢说。比较大的问题是同质化，做语音处理的公司有一堆，做人脸识别的也有一堆，都融了巨额的风险投资，它们的估值已经超过了许多上市公司。如果资本没有预期的回报，就是泡沫。共享单车就是一堆同质公司竞争的例子，目前的合并可以被看作是挤泡沫。人脸识别产品风险投资扎堆投资是不健康的，是泡沫的根源之一。他们理应是前瞻性的，而不是人云亦云。了解些人工智能的历史，也许会让大家对一些潜在的投资领域有更深刻的了解。无论美国还是中国，表现最好的风险投资机构都是对行业有独到看法的。中国和美国在人工智能领域可否一比？中美在人工智能领域的竞争尼克：美国经历过几次科技泡沫，人才和资本都会更成熟些，应对措施也更多样化。日本在五代机之前没有经历过科技泡沫，危机一来，束手无策，最后只好互相掩盖，不了了之。除了投融资领域的泡沫，中国的科技政策的制定和实施也可吸取教训。从整体上说，中国的人工智能是和美国一道处在第一梯队里。我们深入研究过中国的三个经济区——京津冀、长三角和粤港澳大湾区，中国的人工智能企业、人才、资本有大约七成集中在北京，这一点类似于美国的硅谷。我们的研究也表明，现在，中国确实在人工智能的某些方面已经赶超美国，至少体现在公司、专利和论文的数量上，但质量上，还有距离。总的来讲，硅谷还是全球的灯塔。转载声明：本文转载自「上海书评」"}
{"content2":"1.1 重要会议（1）机器视觉重要会议CVPR：Conferenceon Computer Vision and Pattern Recognition, IEEE, 五星ICCV：InternationalConference on Computer Vision, IEEE, 五星ECCV：European Conferenceon Computer Vision, 五星ACCV：AsianConference on Computer Vision, 四星BMVC：BritishMachine Vision Conference, 四星ICPR：InternationalConference on Pattern Recognition, 四星ICIP：InternationalConference on Image Processing, IEEE, 四星SIGGRAPH：SpecialInterest Group on Computer GRAPHics and Interactive Techniques, ACM, 五星。发表在TOG上。Eurographics：四星。发表在ComputerGraphics Forum上。（2）人工智能重要会议AAAI：Associationfor the Advancement of Artificial Intelligence, 五星IJCAI：InternationalJoint Conference on Artificial Intelligence, 五星NIPS：NeuralInformation Processing Systems, 五星ICML：InternationalConference on Machine Learning, 五星（3）相关信息链接地址会议记录下载地址一：http://www.cvpapers.com/index.html会议记录下载地址二：http://books.nips.cc/会议最新举办日期：http://conferences.visionbib.com/Iris-Conferences.html最佳会议论文：http://www.computer.org/web/tcpami/awards1.2 重要期刊（1）机器视觉重要期刊PAMI：Pattern Analysisand Machine Intelligence, IEEE Transaction on, 五星IJCV：InternationalJournal of Computer Vision, Springer, 五星CVIU：ComputerVision and Image Understanding, Elsevier, 三星PR：PatternRecognition, Elsevier, 三星IP：Image Processing, IEEE Transactions on, 三星TOG：ACMTransactions on Graphics, ACM, 五星（2）人工智能重要期刊AI：Artificial Intelligence,Elsevier, 五星JMLR：Journal ofMachine Learning Research, MIT, 五星FS：FuzzySystems, IEEE Transactions on, 四星NNLS：NeuralNetworks and Learning Systems, IEEE Transactions on, 四星NC：NeuralComputation, MIT, 四星ML：MachineLearning, springer, 四星（3）相关出版商文献库IEEE：电气和电子工程师协会，即Instituteof Electrical and Electronics EngineersSpringer：施普林格，出版商，德国，全称Springer-VerlagElsevier：爱思唯尔，出版商，荷兰Wiley：威立，出版商，美国ACM：美国计算机协会，即Association ofComputing MachineryIEEEXploer：IEEE的文献数据ScienceDirect：Elsevier的文献数据库Wiley Online Library：Wiley的文献数据库ACM Digital Library：ACM的文献数据库1.3 重要链接（1）学术搜索谷歌学术：http://scholar.google.com/微软学术：http://academic.research.microsoft.com/科学网：http://apps.webofknowledge.com/特殊领域：IEEEXploer, ScienceDirect, WileyOnline Library,ACM Digital Library,九尾搜搜：http://www.jwss.com/scholar全国图书馆参考咨询联盟：http://www.ucdrs.net/admin/union/index.do（2）重要资源视觉圣经：http://www.visionbib.com/图像库：http://image-net.org/图像库：http://groups.csail.mit.edu/vision/SUN/开源软件库：http://mloss.org/software/1.4 重要软件OpenCV：提供C/C++和Python接口，提供图像处理、计算机视觉及机器学习等方面的模块，可以运行在Linux、Windows、Mac、Android等操作系统上。Theano：提供Python++接口，用来定义、优化和模拟数学表达式计算，用于高效的解决多维数组的计算问题，使得构建深度学习模型更加容易。Scikit-learn：提供Python++接口，提供机器学习模块，构建在SciPy、Numpy和matlablib之上。NLTK：提供Python++接口，提供自然语言处理模块，包括一系列的字符处理和语言统计模型。1.5 小组牛人（1）研究小组MIT：groups.csail.mit.edu/vision/Berkeley：http://www.eecs.berkeley.edu/Research/Projects/CS/vision/Stanford：http://vision.stanford.edu/Oxford：http://www.robots.ox.ac.uk/CMU：https://personalrobotics.ri.cmu.edu/UCLA：http://www.vision.ucla.edu/LEAR：http://lear.inrialpes.fr/EPFL：http://cvlab.epfl.ch/ETHZ：http://www.cvg.ethz.ch/Columbia：http://www1.cs.columbia.edu/CAVE/（2）牛人主页UK-AndrewNg-StanfordYoshuaBengioUK-EmmanuelCandes-StanfordUK-DaveDonoho-StanfordUK-StephenP.Boyd-StandfordUK-ZoubinGhahramani-CambridgeUK-RobertoCipolla-CambridgeUS-WilliamT.Freeman-MITUS-ZhuSongchun-CaliforniaUS-MichaelIJordan-BerkeleyUS-EhsanElhamifar-BerkeleyUS-TrevorDarrell-BerkeleyCN-ZhangZhihua-上交CN-LinZhouchen-北大CN-ZhangLei-港理工CN-HeXiaofei-浙大CN-ZhouZhiHua-南大"}
{"content2":"1. 图像与原始字节之间的转换从概念上讲，一个字节能表示0到255的整数。目前，对于多有的实时图像应用而言，虽然有其他的表示形式，但一个像素通常由每个通道的一个字节表示。一个OpenCV图像是.array类型的二维或三维数组。8位的灰度图像是一个含有字节值的二维数组。一个24位的BGR图像是一个三维数组，它也包含了字节值。可使用表达式访问这些值，如image[0,0]或image[0,0,0]。第一个值代表像素的y坐标啊或行，0表示顶部；第二个值是像素的x坐标或列，0表示最左边；第三个值（如果可用的话）表示颜色通道。如，对于一个左上角有白色像素的8位灰度图像而言，image[0,0]的值为255. 对于一个左上角有蓝色像素的24位BGR图像而言，image[0,0]是[255,0,0]。可以用另外一个表示，如image[0,0]或image[0,0]=128，还可表示成image.item((0,0))或image.setitem((0,0),128)。对于单像素操作，第二种表示方式更有效。若一幅图像的每个通道为8位，则可将其显示转换为标准的一维Python bytearray格式：byteArray = bytearray(image)反之，bytearray含有恰当顺序的字节，可以通过显示转换和重构，得到numpy.array形式的图像：garyImage = numpy.array(garyByteArray ).reshape(height, width)bgrImage = numpy.array(bgrByteArray ).reshape(height, width, 3)下面介绍将含有随机字节的bytearray转换为灰度图像和BGR图像：import cv2 import numpy as np import os # 创建一个120000个随机字节的数组 randomByteArray = bytearray(os.urandom(120000)) #os.urandom(n) 返回n个随机byte值的string，作为加密使用 flatNumpyArray = np.array(randomByteArray) # 将数组转换为400 x 300的灰度图像 garyImage = flatNumpyArray.reshape(300, 400) cv2.imwrite('randomGary.png', garyImage) # 将数组转换为400 x 300的彩色图像 bgrImage = flatNumpyArray.reshape(100, 400, 3) cv2.imwrite('randomColor.png', bgrImage)运行该程序，将会在程序所在目录中生成两张灰度图像（如下所示）。尺寸分别为400 x 100，400 x 400使用Python标准的os.urandom()函数可随机生成原始字节，随后会把该字节转换为NumPy数组。需要注意的是，诸如numpy.random.randint(0, 256, 120000).reshape(400, 300)语句也能直接（并且更高效地）随机生成NumPy数组。使用os.urandom()函数的原因是该语句有助于展示原始字节的转换。2. 使用numpy.array访问图像数据加载OpenCV图像最简单的方式是使用imread()函数，该函数会返回一幅图像，这幅图像是一个数组（根据imread()函数输入参数的不同，该图像可能是二维数组，也可能是三维数组）。y.array结构针对数组操作有很好的优化，它允许某些块(bulk)操作，这些操作在通常的Python中不可用这些特定的.array操作在OpenCV的图像处理中会很方便。利用numpy.array函数来转换数组比用普通的Python数组转换要快得多。import cv2 import numpy as np img = cv2.imread('flower.jpg') img[0,0] = [255, 255, 255] cv2.imshow('my image', img) cv2.waitKey()在图像左上方会出现一个白点。假设想要改变一个特定像素的蓝色值，numpy.array提供了item()方法。该函数有3个参数：x（或左）位置，y（或顶部）位置以及（x,y）位置的数组索引（注意，在BGR图像中，某一位置的数据是按B,G,R的顺序保存的三元数组），该函数能返回索引函数的值。另一个方法是通过itemset()函数可设置指定像素在指定通道的值（itemset()有两个参数：一个三元组(x,y和索引）和要设定的值）。如下例子将坐标（150,120）的当前蓝色值127变为255import cv2 import numpy as np img = cv2.imread('flower.jpg') print(img.item(150, 120, 0)) # 打印当前坐标点的蓝色值 img.itemset((150, 120, 0), 255) print(img.item(150, 120, 0))建议使用内置的滤波器和方法来处理整个图像，上述方法只适合于处理特定的小区域。下面介绍操作通道：将指定通道（B,G,R）的所有值置为0.（注：通过循环来处理Python数组的效率非常低，应该尽量避免这样的操作。使用数组索引可以高效地操作像素。像素操作是一个高代价的低效操作，特别是在视频数据处理时，会发现要等很久才能得到结果。可用索引(indexing)来解决该问题）以下代码可将图像所有的G（绿色）值设为0import cv2 import numpy as np img = cv2.imread('flower.jpg') img[:, :, 1] = 0 cv2.imshow('my image', img) cv2.waitKey()运行结果为：通过NumPy数组的索引访问原始像素，还可设定感兴趣区域(Region Of Interest, ROI)。一旦设定了该区域，就可以执行许多操作，例如，将该区域与变量绑定，然后设定第二个区域，并将第一个区域的值分配给第二个区域（将图像的一部分拷贝到该图像的另一个位置）：import cv2 import numpy as np img = cv2.imread('flower.jpg') roi = img[0:100, 0:100] img[100:200, 100:200] = roi # 此处需考虑所用图像的尺寸，不能超过，并确保两个区域的大小一样 cv2.imshow('my image', img) cv2.waitKey()运行结果为：此外，还可使用numpy.array来获得图像其他属性。shape：NumPy返回包含宽度、高度和通道数（如果图像是彩色的）数组，这在调试图像类型时很有用；如果图像是单色或灰度的，将不包含通道值；size：该属性是指图像像素的大小；datatype：该属性会得到图像的数据类型（通常为一个无符号整数类型的变量和该类型占的位数，比如unit8类型）import cv2 import numpy as np img = cv2.imread('flower.jpg') print(img.shape) print(img.size) print(img.dtype)运行结果为：(220, 252, 3)166320uint83.视频文件的读/写OpenCV提供了VideoCapture类和VideoWriter类来支持各种格式的视频文件。支持的格式类型会因系统的不同而变化，但应该都支持AVI格式。在到达视频文件末尾之前，VideoCapture类可通过read()函数来获取新的帧，每帧都是一幅基于BGR格式的图像。可将一幅图像传递给VideoWriter类的write()函数，该函数会将这幅图像加到VideoWriter类所指向的文件中。如下示例读取AVI文件的帧，并采用YUV颜色编码将其写入另一帧中：import cv2 videoCapture = cv2.VideoCapture('myvideo.avi') fps = videoCapture.get(cv2.CAP_PROP_FPS) size = (int(videoCapture.get(cv2.CAP_PROP_FRAME_WIDTH)), int(videoCapture.get(cv2.CAP_PROP_FRAME_HEIGHT))) videoWriter = cv2.VideoWriter('MyOutputVid.avi', cv2.VideoWriter_fourcc('I', '4', '2', '0'), fps, size) success, frame = videoCapture.read() while success: # 循环直到所有帧结束 videoWriter.write(frame) success, frame = videoCapture.read()要特别注意：必须要为VideoWriter类的构造函数指定视频文件名，这个文件名对应的文件若存在，会被覆盖。也必须指定视频编解码器。编解码器的可用性根据系统不同而不同。下面是一些常用选项：cv2.VideoWriter_force('I', '4', '2', '0')：该选项是一个未压缩的YUV颜色编码，是4:2:0色度子采样。这种编码有很好的兼容性，但会产生较大文件，文件扩展名为.avi。cv2.VideoWriter_force('P', 'I', 'M', '1')：该选项是MPEG-1编码类型，文件扩展名为.avi。cv2.VideoWriter_force('X', 'V', 'I', 'D')：该选项是MPEG-4编码类型，如果希望得到的视频大小为平均值，推荐使用此选项，文件扩展名为.avi。cv2.VideoWriter_force('T', 'H', 'E', 'O')：该选项是Ogg Vorbis，文件扩展名应为.ogv。cv2.VideoWriter_force('F', 'L', 'V', '1')：该选项是一个Flash视频，文件扩展名应为.flv。帧速率和帧大小也必须要指定，因为需要从另一个视频文件复制视频帧，这些属性可以通过VideoCapture类的get()函数得到。4. 捕获摄像头的帧VideoCapture类可以获得摄像头的帧流。但对摄像头而言，通常不是用视频的文件名来构造VideoCapture类，而是需要传递摄像头的设备索引(device index)。下面的例子会捕获摄像头10秒的视频信息，并将其写入一个AVI文件中：import cv2 cameraCapture = cv2.VideoCapture(0) fps = 30 size = (int(cameraCapture.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cameraCapture.get(cv2.CAP_PROP_FRAME_HEIGHT))) videoWriter = cv2.VideoWriter('MyOutputVid.avi', cv2.VideoWriter_fourcc('I', '4', '2', '0'), fps, size) success, frame = cameraCapture.read() numFramesRemaining = 10 * fps - 1 while success and numFramesRemaining > 0: videoWriter.write(frame) success, frame = cameraCapture.read() numFramesRemaining -= 1 cameraCapture.release()然而，VideoCapture类的get()方法不能反悔摄像头帧速率的准确值，它总是返回0。为了针对摄像头创建合适的VideoWriter类，要么对帧速率做出假设，要么使用计时器来测量。摄像头的数量和顺序由系统决定，但OpenCV没有提供任何查询摄像头数量和属性的方法。如果使用无效索引构造了VideoCapture类，就不会得到帧，VideoCapture的read()函数会返回(false, None)。为了不让read()函数从没有正确打开的VideoCapture类中获取数据，可在执行该函数之后使用VideoCapture.isOpened方法做一个判断，该方法返回一个Boolean值。当需要同步一组摄像头或一个多头摄像头（例如立体摄像头或Kinect）时，read()方法就不再适合了，可用grab()和retrive()方法代替它。对于一组摄像头，可以使用以下代码：success0 = cameraCapture0.grab() success1 = cameraCapture1.grab() if success0 and success1: frame0 = cameraCapture0.retrive() frame1 = cameraCapture1.retrive()5. 在窗口显示图像用imshow()函数实现显示图像的操作。imshow()函数有两个参数：显示图像的帧名字以及要显示的图像本身。import cv2 import numpy as np img = cv2.imread('flower.jpg') cv2.imshow('my image', img) cv2.waitKey() cv2.destroyAllWindows() # 释放由OpenCV创建的所有窗口6. 在窗口显示摄像头帧OpenCV的namedWindow()、imshow()和DestoryWindow()函数允许指定窗口名来创建、显示和销毁（destroy）窗口。此外，任何窗口都可以通过waitKey()函数来获取键盘输入，通过setMouseCallback()函数来获取鼠标输入。以下代码可实时显示摄像头帧：import cv2 clicked = False def onMouse(event, x, y, flags, param): global clicked if event == cv2.EVENT_LBUTTONUP: clicked = True cameraCapture = cv2.VideoCapture(0) cv2.namedWindow('MyWindow') cv2.setMouseCallback('MyWindow', onMouse) print('showing camera feed. Click window or press any key to stop.') success, frame = cameraCapture.read() while success and cv2.waitKey(1) == -1 and not clicked: cv2.imshow('MyWindow', frame) success, frame = cameraCapture.read()cv2.destroyWindow('MyWindow')cameraCapture.release()waitKey()的参数为等待键盘触发的时间，单位为毫秒，其返回值为-1（表示没有键被按下）或ASCII码。另外，Python提供了一个标准函数ord()，该函数可以将字符转换为ASCII码。（注：在一些系统中，waitKey()的返回值可能比ASCII码的值更大（在Linux系统中，如果OpenCV使用GTK作为后端的GUI库，就会出现bug）,在所有系统中，可以通过读取返回值的最后一个字节来保证肢体去ASCII码，代码为：keycode = cv2.waitkey(1)if keycode != -1:keycode &= 0xff ）OpenCV的窗口函数和waitKey()函数相互依赖。OpenCV的窗口只有在调用waitKey()函数时才会更新，waitKey()函数只有在OpenCV窗口成为活动窗口时，才能捕获输入信息。鼠标回调函数setMouseCallback()有5个参数，param是可选参数，它是setMouseCallback()函数的第三个参数，默认情况下，该参数是0.回调时间参数可以取如下的值，它们分别对应不同的鼠标事件。cv2.EVENT_MOUSEMOVE：该事件对应鼠标移动cv2.EVENT_LBUTTONDOWN：该事件对应鼠标左键按下cv2.EVENT_RBUTTONDOWN：该事件对应鼠标右键按下cv2.EVENT_MBUTTONDOWN：该事件对应鼠标中间键按下cv2.EVENT_LBUTTONUP：该事件对应鼠标左键松开cv2.EVENT_RBUTTONUP：该事件对应鼠标右键松开cv2.EVENT_MBUTTONUP：该事件对应鼠标中间键松开cv2.EVENT_LBUTTONDBLCLK：该事件对应双击鼠标左键cv2.EVENT_RBUTTONDBLCLK：该事件对应双击鼠标右键cv2.EVENT_MBUTTONDBLCLK：该事件随影双击鼠标中间键鼠标回调的标志参数可能是以下事件的按位组合：cv2.EVENT_FLAG_LBUTTON：该事件对应按下鼠标左键cv2.EVENT_FLAG_RBUTTON：该事件对应按下鼠标右键cv2.EVENT_FLAG_MBUTTON：该事件对应按下鼠标中间键cv2.EVENT_FLAG_CTRLKEY：该事件对应按下Ctrl键cv2.EVENT_FLAG_SHIFTKEY：该事件对应按下Shift键cv2.EVENT_FLAG_ALTKEY：该事件对应按下Alt键"}
{"content2":"什么是程序（Program）计算机程序，是指为了得到某种结果而可以由计算机（等具有信息处理能力的装置）执行的代码化指令序列（或者可以被自动转换成代码化指令序列的符号化指令序列或者符号化语句序列）。通俗讲，计算机给人干活，但它不是人，甚至不如狗懂人的需要（《小羊肖恩》里的狗是多么聪明可爱又忠诚于主人）。那怎么让它干活呢，那就需要程序员用某种编程语言来写程序，编程语言就是计算机能理解的语言，计算机可以执行这些程序（指令），最终完成任务。下边的C++程序是完成n的阶乘：int n = std::atoi(argv[1]); //求n的阶乘 double result = 1.0; for (int i = 2; i <= n; i++) { result *= i; } std::cout << n << \"的阶乘是：\" << result << std::endl;什么是算法（Algorithm）算法是解决特定问题求解步骤的描述，在计算机中表现为指令的有限序列，并且每条指令表示一个或者多个操作。举个简单的例子，并且大家生活中都能用得上的。现在做个小游戏，A在纸上随机写了一个1到100间的整数，B去猜，猜对的话游戏结束，猜错的话A会告诉B猜的小了还是大了。那么B会怎么做呢，第一次肯定去猜50，每次都猜中间数。为什么呢？因为这样最坏情况下（\\(log_2{100}\\)）六七次就能猜到。这就是二分查找，生活中可能就会用得到，而在软件开发中也经常会用得到。再来看一个稍微复杂一点点的算法，【快速排序】，面试中考的频率非常高非常高，甚至可以说是必考。什么是机器学习算法（Machine Learning）机器学习的定义《机器学习》书中的定义：关于某类任务 T 和性能度量P，如果一个计算机程序能在T上以P衡量的性能随着经验E而自我完善，那么我们称这个计算机程序在从经验E中学习。比如AlphaGo：任务 T ：下棋性能标准 P ：击败对手的百分比训练经验：和自己对弈或者比赛经验。再比如自动驾驶：任务T : 通过视频传感器在高速公路上行驶性能标准P：平均无差错行驶里程训练经验E：注视人类驾驶时录制的一系列图像和驾驶指令。百度百科的定义：机器学习(Machine Learning, ML)是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演绎。机器学习的主要任务监督学习：（1）分类：将实例数据划分到合适的分类中。KNN(k-近邻算法)、决策树、朴素贝叶斯、Logistic回归、SVM(支持向量积)。（2）回归：预测数值型数据。无监督学习：（1）聚类：将数据集合分成由类似的对象组成的多个类的过程。K-MEANS(K均值聚类)神经网络（Neural Network）与深度学习（Deep Learning）生物学启示人工神经网络ANN的研究一定程度上受到了生物学的启发，生物的学习系统由相互连接的神经元（neuron）组成的异常复杂的网格。而人工神经网络由一系列简单的单元相互密集连接构成的，其中每一个单元有一定数量的实值输入，并产生单一的实数值输出。据估计人类的大脑是由大约\\(10^11\\)次方个神经元相互连接组成的密集网络，平均每个神经元与其他\\(10^4\\)个神经元相连。神经元的活性通常被通向其他神经元的连接激活或抑制。生物的神经元：人工神经元（感知机）：多层感知机：神经网络表示1993年的ALVINN系统是ANN学习的一个典型实例，这个系统使用一个学习到的ANN以正常的速度在高速公路上驾驶汽车。ANN的输入是一个30*32像素的网格，像素的亮度来自一个安装在车辆上的前向摄像机。ANN的输出是车辆行驶的方向。浅层学习20世纪80年代末期，用于人工神经网络的反向传播算法（也叫Back Propagation算法或者BP算法）的发明，给机器学习带来了希望，掀起了基于统计模型的机器学习热潮。这个热潮一直持续到今天。人们发现，利用BP算法可以让一个人工神经网络模型从大量训练样本中学习统计规律，从而对未知事件做预测。这种基于统计的机器学习方法比起过去基于人工规则的系统，在很多方面显出优越性。这个时候的人工神经网络，虽也被称作多层感知机（Multi-layer Perceptron），但实际是种只含有一层隐层节点的浅层模型。20世纪90年代，各种各样的浅层机器学习模型相继被提出，例如支撑向量机（SVM，Support Vector Machines）、 Boosting、最大熵方法（如LR，Logistic Regression）等。这些模型的结构基本上可以看成带有一层隐层节点（如SVM、Boosting），或没有隐层节点（如LR）。这些模型无论是在理论分析还是应用中都获得了巨大的成功。相比之下，由于理论分析的难度大，训练方法又需要很多经验和技巧，这个时期浅层人工神经网络反而相对沉寂。深层学习深度学习的实质，是通过构建具有很多隐层的机器学习模型和海量的训练数据，来学习更有用的特征，从而最终提升分类或预测的准确性。因此，“深度模型”是手段，“特征学习”是目的。区别于传统的浅层学习，深度学习的不同在于：1）强调了模型结构的深度，通常有5层、6层，甚至10多层的隐层节点；2）明确突出了特征学习的重要性，也就是说，通过逐层特征变换，将样本在原空间的特征表示变换到一个新特征空间，从而使分类或预测更加容易。与人工规则构造特征的方法相比，利用大数据来学习特征，更能够刻画数据的丰富内在信息。Deep learning本身算是machine learning的一个分支，简单可以理解为neural network的发展。一种典型的用来识别数字的卷积网络是LeNet-5。当年美国大多数银行就是用它来识别支票上面的手写数字的。能够达到这种商用的地步，它的准确性可想而知。LeNet-5的网络结构如下：与机器学习相关联的概念数据挖掘（Data Mining）数据挖掘=机器学习+数据库。数据挖掘是在大型数据存储库中，自动地发现有用信息的过程。自然语言处理 （Natural Language Process）自然语言处理=文本处理+机器学习。自然语言处理技术主要是让机器理解人类的语言的一门领域。在自然语言处理技术中，大量使用了编译原理相关的技术，例如词法分析，语法分析等等，除此之外，在理解这个层面，则使用了语义理解，机器学习等技术。作为唯一由人类自身创造的符号，自然语言处理一直是机器学习界不断研究的方向。按照百度机器学习专家余凯的说法“听与看，说白了就是阿猫和阿狗都会的，而只有语言才是人类独有的”。如何利用机器学习技术进行自然语言的的深度理解，一直是工业和学术界关注的焦点。模式识别（Pattern Recognition）模式识别=机器学习。两者的主要区别在于前者是从工业界发展起来的概念，后者则主要源自计算机学科。统计学习（Statistical Learning）统计学习近似等于机器学习。统计学习是个与机器学习高度重叠的学科。因为机器学习中的大多数方法来自统计学，甚至可以认为，统计学的发展促进机器学习的繁荣昌盛。例如著名的支持向量机算法，就是源自统计学科。但是在某种程度上两者是有分别的，这个分别在于：统计学习者重点关注的是统计模型的发展与优化，偏数学，而机器学习者更关注的是能够解决问题，偏实践，因此机器学习研究者会重点研究学习算法在计算机上执行的效率与准确性的提升。计算机视觉（Computer Vision）计算机视觉=图像处理+机器学习。图像处理技术用于将图像处理为适合进入机器学习模型中的输入，机器学习则负责从图像中识别出相关的模式。计算机视觉相关的应用非常的多，例如百度识图、手写字符识别、车牌识别等等应用。这个领域是应用前景非常火热的，同时也是研究的热门方向。随着机器学习的新领域深度学习的发展，大大促进了计算机图像识别的效果，因此未来计算机视觉界的发展前景不可估量。语音识别（Speech Recognition）语音识别=语音处理+机器学习。语音识别就是音频处理技术与机器学习的结合。语音识别技术一般不会单独使用，一般会结合自然语言处理的相关技术。目前的相关应用有苹果的语音助手siri等。计算机图形学、数字图像处理、计算机视觉计算机视觉（ Computer Vision，简称 CV），是让计算机“看懂”人类看到的世界，输入是图像，输出是图像中的关键信息；图片 -> dog or cat?图片 -> [xyz xyz xyz ... xyz]计算机图形学（Computer Graphics，简称 CG），是让计算机“描述”人类看到的世界，输入是三维模型和场景描述，输出是渲染图像；[xyz xyz xyz ... xyz] -> 图片数字图像处理（Digital Image Processing，简称 DIP），输入的是图像，输出的也是图像。Photoshop 中对一副图像应用滤镜就是典型的一种图像处理。常见操作有模糊、灰度化、增强对比度等。图片 -> ps后的图片再说联系CG 中也会用到 DIP，现今的三维游戏为了增加表现力都会叠加全屏的后期特效，原理就是 DIP，只是将计算量放在了显卡端。通常的做法是绘制一个全屏的矩形，在 Pixel Shader 中进行图像处理。CV 大量依赖 DIP 来打杂活，比如对需要识别的照片进行预处理，增强对比度、去除噪点。最后还要提到今年的热点——增强现实（AR），它既需要 CG，又需要 CV，当然也不会漏掉 DIP。它用 DIP 进行预处理，用 CV 进行跟踪物体的识别与姿态获取，用 CG 进行虚拟三维物体的叠加。面试——把自己嫁出去面试官面的是什么我个人的经验一次正规的面试包括几个部分：基础能力：数据结构与算法通过做一些智商题、ACM，一般笔试题会从leetcode找。基础能力除了基本的数据结构与算法外，经常还会考察求职者对一门编程语言的掌握程度。工作经历：在哪些公司工作过，做过哪些项目，能不能把做过的东西很清晰的很系统的讲出来。（注：哪怕不是自己做过的东西，求职者能很好的讲出来，面试官也会给加分）沟通能力：性格是否比较好，是否能愉快的沟通，是不是能融入团队。其实有时就是看颜值，通俗说能否看对眼。哪怕能力不怎么好，但是面试官司觉得人不错，工作能干得了，值得培养也没问题。求职者要的是什么钱成长开心面试要注意什么技术能力是核心谦虚谨慎诚实是打动面试官的重要因素沟通也很重要适当美化自己的经历，但不吹牛，也不要过分谦虚参考资料：Deep Learning（深度学习）学习笔记整理系列之（三)《机器学习》Tom M.Mitchell《机器学习实践》Peter Harrington《数学之美》吴军《统计学习方法》李航《计算机视觉、图形学和图像处理，三者有什么联系？》张静《从机器学习谈起》 计算机的潜意识《计算机视觉与计算机图形学的对立统一》卜居"}
{"content2":"姓名职 称学科专长及研究方向办公地点办公室电话Email备注陈德人教授电子商务与电子服务技术、信息系统集成,网络教育技术,计算机图形学与CAD40687951245drc@zju.edu.cn博导陈卫东副教授虚拟现实、脑机交互、移动计算、计算机网络、人工智能、商务智能112（东）87953241chenwd@zju.edu.cn硕导董金祥教授计算机图形学、计算机辅助设计、先进制造技术、计算机集成制造技术、人工智能、数据40687951245djx@cs.zju.edu.cn博导干红华副教授人工智能、因果模型、计算机在法律领域的应用2050574-27830689ganhonghua@cnnb.net硕导高济教授网络计算与普适计算、智能软件与Agent技术、软件工程与中间件技术、知识管理与决策支持40887951923gaoji@mail.hz.zj.cn博导耿卫东教授计算机图形图像技术、智能CAD、人工智能51287951247gengwd@cs.zju.edu.cn博导何利力副教授GIS、人机交互、数据库与数据分析310（东）87952690llhe@zju.edu.cn硕导金小刚副教授复杂网络理论与应用研究、计算金融学、计算生态学、生物计算、脑机接口、智能传输系统及其仿真研究31085957691xiaogangj@cise.zju.edu.cn硕导孔繁胜教授人工智能应用，机器学习，数据挖掘，web-GIS41087953129kfs@cs.zju.edu.cn博导李际军副教授曲面造型、逆向工程、CAD/CAM,游戏引擎开发,三维服装及动画技术204(东)87951992lijijun@cs.zju.edu.cn硕导李善平教授金融信息学、分布式计算、信息集成技术、Linux 平台及应用41487952090shan@cs.zju.edu.cn博导林兰芬教授语义Web、网络化制造、知识管理、CAX、产品建模50187952699llf@cs.zju.edu.cn硕导鲁东明教授数字媒体网络系统，文化遗存数字化保护，下一代互联网络，虚拟现实与数字博物馆407（东）87953078ldm@cs.zju.edu.cn博导潘云鹤教授人工智能，形象思维,计算机图形学，智能CAD，计算机美术,工业设计87951853panyh@sun.zju.edu.cn博导（院士）唐敏副教授三维造型 CAD30387987944tang_m@zju.edu.cn硕导童若锋教授计算机图形学,协同设计与制造,图像重建与处理30387951414trf@cs.zju.edu.cn博导王申康教授人工智能、计算机协同工作技术、生物认证、嵌入式GPS&GIS、智能建筑41287951883 13906515965wangsk@zju.edu.cn博导魏宝刚副教授人工智能、图像处理、数据库与知识库系统50587968432wbg@zju.edu.cn硕导吴春明教授人工智能,智能机器人技术,计算机网络40787951916wuchunming@cs.zju.edu.cn硕导吴江琴副教授数据挖掘，数字化图书馆50787952300wujq@cs.zju.edu.cn硕导肖国臻副教授可视化技术在医学中的应用、虚拟现实技术在医学中的应用、计算机控制技术50587952098xgz@cs.zju.edu.cn硕导邢卫副教授计算机网络技术及应用、电子政务模型及应用51687082635wxing@cs.zju.edu.cn硕导徐从富副教授人工智能、智能CAD、数据挖掘、知识发现、数据融合31387952308xucongfu@cs.zju.edu.cn硕导杨建刚教授先进计算、多传感器数据融合、人工神经网络、嵌入式系统30587952228yangjg@cs.zju.edu.cn博导杨小虎副教授金融信息学、软件再工程50187952694yangxh@zju.edu.cn硕导叶修梓教授几何建模、CAD、逆向工程、产品创新设计、生医可视化计算、应用图形图像技术310（东）87952690yxz@cs.zju.edu.cn博导（长江特聘教授）张泉方副教授计算机网络及应用516qfzhang@mail.hz.zj.cn硕导张引副教授图象图形处理、多媒体信息处理310（东）87952690yinzh@cs.zju.edu.cn硕导郑扣根教授操作系统、嵌入式系统、计算机网络、地理信息系统（GIS）405（东）87952308zkg@cs.zju.edu.cn zhengkg@263.net.cn博导周波副教授数据库管理系统、CIMS、数据仓库和数据挖掘50387952385bzhou@cs.zju.edu.cn硕导周忠信教授软件技术、Java 计算、互联网计算、互联网络研究508jwo@cs.zju.edu.cn硕导朱淼良教授人工智能技术与机器人智能、计算机视觉、多媒体与网络通讯技术、网络信息安全303（东）87984692zhum@cs.zju.edu.cn博导朱晓芸副教授智能信息融合、计算机辅助教学、数据挖掘、数字制造51287952661zhuxy@zju.edu.cn硕导庄越挺教授网络多媒体、海量数据库与信息检索、智能动画技术、智能CAD、数字图书馆、嵌入式多媒体30887951903yzhuang@cs.zju.edu.cn博导黄忠东副教授网络安全、数据库、CIMS513hzd@cs.zju.edu.cn硕导尹建伟副教授Java网络中间件、信息安全、J2ME嵌入式平台及在移动通信中的应用、构件技术、数据检索及处理3030571-81951139zjuyjw@cs.zju.edu.cn硕导张亶副教授图象处理0574-27830768硕导杨枨副教授网络管理、数字制造、数据挖掘51287952661yangcheng@zju.edu.cn董亚波副教授网络安全、无线传感器网络、嵌入式系统教11-30187952724dongyb@zju.edu.cn硕导王东辉副教授计算机视觉，人工智能技术，多媒体与网络通讯技术40787951916dhwang@zju.edu.cn硕导吴飞副教授多媒体分析与检索 计算机动画 统计学习60887951853wufei@cs.zju.edu.cn硕导蔡铭副教授网络化制造、ASP、语义网、嵌入式操作系统87951414cm@zju.edu.cn硕导门素琴副教授人工智能、CAD310(东)87951902mensq@cs.zju.edu.cn钱沄涛教授机器学习，模式识别，信号处理50487951256ytqian@zju.edu.cn博导许端清教授虚拟现实技术，数字媒体技术，CAD205（东）87952023xdq@cs.zju.edu.cn博导钱徽副教授智能机器人,计算机视觉曹光彪主楼4070571-87951916qianhui@zju.edu.cn硕导王东辉副教授硕导袁昕副教授医学影像处理、计算机辅助医学诊断40787951916yxxinyuan@zju.edu.cn硕导代建华副教授硕导姓名职 称学科专长及研究方向办公地点办公室电话Email备注何钦铭教授数据挖掘、虚拟计算系统、GIS21387951265hqm@cs.zju.edu.cn博导陈奇副教授GIS、智能决策、数据挖掘、嵌入式系统、网络与分布式技术30913906508271chenqi@zju.edu.cn硕导陈天洲教授计算机体系结构、多核、片上网络、嵌入式系统、软件节能与安全主520主316西40487951793、87952650tzchen@zju.edu.cn博导姜晓红副教授虚拟环境，分布式系统，网格计算，图像处理曹主52013396550102jiangxh@cad.zju.edu.cn硕导陆魁军副教授计算机网络技术与网络软件,传感器网络，嵌入式路由器，信息安全网关，流媒体Multicast传播方式，网络数据库系统501（东）87953208, 13606702650lukj@zju.edu.cn硕导吕红兵副教授机器人控制系统、计算机视觉、嵌入式系统、企业应用软件曹主52213605716202lhb@zju.edu.cn硕导潘雪增教授网络信息安全, ,新一代可信网络, 可信计算与安全语言, 网络安全过滤技朮曹主42013067732620 87951244xzpan@cs.zju.edu.cn博导平玲娣教授网络安全, 操作系统与数据库安全技术、下一代网络与移动计算、面向SOC专用集成电路软硬件设计。曹主42013067995429 87951244ldping@cs.zju.edu.cn博导石教英教授计算机图形学、多媒体计算机技术、计算机网络与分布处理紫金港校区CAD实验室88206681-502jyshi@cad.zju.edu.cn博导陈文智副教授嵌入式系统，计算机网络，多媒体应用，操作系统51113600512233chenwz@zju.edu.cn硕导宋广华副教授实时数据库技术、嵌入式系统；并行计算与网格计算410（东）87952644ghsong@cs.zju.edu.cn硕导吴朝晖教授人工智能、生物认证、分布式计算与GRID、嵌入式系统工程505（东）87951647wzh@cs.zju.edu.cn博导姚敏教授计算智能、普适计算、图象理解、知识发现与知识管理41813600510302myao@zju.edu.cn博导张明敏副教授虚拟现实、计算机图形学、计算机图像处理紫金港校区CAD实验室87951045zmm@cad.zju.edu.cn硕导朱桂林副教授智能CAD、嵌入式系统、计算机网络、可视化50987998921zgl@cs.zju.edu.cn李国杰教授并行处理、计算机体系结构、组合优化、人工神经网和遗传算法北京010-62541341wxh@ict.ac.cn博导（院士）潘纲副教授计算机视觉、模式识别、普适计算曹东51487951647gpan@zju.edu.cn http://www.cs.zju.edu.cn/~gpan硕导李莹副教授软件体系结构、程序设计语言和中间件技术51313335880866cnliying@zju.edu.cn硕导杨莹春副研究员语音处理，生物特征识别，模式识别与机器学习曹东50387951647yyc@zju.edu.cn硕导吴 健副教授中间件;服务计算 SOC/SOA;网格计算;数据挖掘;软件工程0571-87951647Wujian2000@zju.edu.cn姓名职 称学科专长及研究方向办公地点办公室电话Email备注卜佳俊教授嵌入式软件与系统、移动多媒体、计算机图形图像处理、计算机视觉、CSCW508（东)87952600bjj@cs.zju.edu.cn硕导陈纯教授嵌入式软件与系统，CSCW，计算机图形图像处理等508（东)87951255 87952600chenc@zju.edu.cn博导陈根才教授数据库技术, 数据挖掘，CSCW,普适计算与协同技术,智能信息处理30687953052chengc@zju.edu.cn博导陈平副教授计算机网络、电子商务、信息处理51788480000-8128chenping@landpage.com.cn硕导陈越教授曲面重构、曲面造型、微分方程的数值计算方法21387951265chenyue@cs.zju.edu.cn硕导冯雁副教授数据库研究与应用、电子商务、数据挖掘50287951967fengyan@cs.zju.edu.cn硕导金连甫副教授信息处理、软件工程、电子商务、操作系统51788480118lfjin@mail.hz.zj.cn硕导应晶教授软件工程方法学、软件开发方法、软件体系结构、计算机辅助软件工程403（东）87951965jying_2000@yahoo.com博导陆系群副教授多媒体数字处理技术、计算机视觉和网络安全技术等527或52387951257xqlu@cs.zju.edu.cn硕导林怀忠副教授数据库与数据挖掘、信息安全、移动计算205（东）13018939989linhz@zju.edu.cn硕导孟华教授氢能源和燃料电池技术,燃烧和推进技术,高性能计算87953166menghua@zju.edu.cn郑耀教授高性能计算（并行计算与分布式计算）、网格计算、多学科应用模拟的支撑技术、计算机图形算法与科学可视化、计算机辅助设计与辅助工程（CAD/CAE）教11-201, 410（东）87953168yao.zheng@zju.edu.cn, http://www.cesc.zju.edu.cn/people/zhengyao/index_c.html博导（长江特聘教授）首山雄副教授信息处理与自动控制，高能射流，高精度先进制造，光机电一体化ssx@cs.zju.edu.cn硕导蔡亮副教授网络安全、数据库安全、可信计算、金融业务处理51513958020863cail2000@21cn.com硕导孙建伶教授数据库、Web技术50387952700sunjl@zju.edu.cn硕导陈刚教授CIMS、网络安全、协同设计、数据库50187951245cg@cs.zju.edu.cn博导杨小虎副教授金融信息学、软件再工程50187952694yangxh@zju.edu.cn硕导叶德仕副教授算法设计与分析；无线网络与移动计算；在线算法曹主52413957173448yedeshi@zju.edu.cn <yedeshi@zju.edu.cn>个人主页：http://www.cs.zju.edu.cn/people/yedeshi/陈岭副教授分布式系统、人机交互、普适计算、数据库、人工智能、模式识别，及其在协作和娱乐系统中的应用616B13606527774lingchen@cs.zju.edu.cn硕导姓名职 称学科专长及研究方向地点：紫金港校区信息与控制大楼办公室电话Email备注鲍虎军研究员计算机图形学、虚拟现实、几何与视觉计算501室88206683-501bao@cad.zju.edu.cn博导（长江特聘教授）冯结青研究员代数曲线曲面造型、实时绘制、卡通动画、科学计算可视化510室88206683-506jqfeng@cad.zju.edu.cn博导高曙明研究员CAx、协同工程、虚拟样机、PLM、CSCW514室88206683-514smgao@cad.zju.edu.cn博导金小刚研究员计算机动画、计算机图形学、计算机辅几何设计506室88206683-507jin@cad.zju.edu.cn博导李伟青副研究员计算机辅助设计、计算机图形学、图形图像处理411室88206683-407wqli@cad.zju.edu.cn硕导林 海研究员计算机图形学，科学计算可视化，医学影像三维处理510室88206683-510lin@cad.zju.edu.cn博导潘志庚研究员虚拟现实, 数字水印, 游戏与动画, 数字媒体, 移动图形, 体育仿真, 数字博物馆,信息可视化508室88206683-509zgpan@cad.zju.edu.cn博导彭群生教授计算机图形算法及应用、虚拟环境的高效建模与绘制、生物分子图形学、视频纹理技术503室88206683-503peng@cad.zju.edu.cn博导万华根副研究员虚拟现实及应用、计算机动画、科学计算可视化512室88206683-512hgwan@cad.zju.edu.cn硕导于金辉研究员计算机动画，非真实感绘制，图像合成与处理、文物虚拟修复、数字化艺术508室88206683-508jhyu@cad.zju.edu.cn博导郑文庭副研究员计算机图形学、虚拟现实512室88206683-513wtzheng@cad.zju.edu.cn硕导刘玉生研究员助设计与制造、模型检索与重用、虚拟样机520室88206681-524ysliu@cad.zju.edu.cn硕导秦学英副研究员计算机图形学510室88206683-511xyqin@cad.zju.edu.cn硕导王章野副教授计算机图形学、红外视景仿真、图像信息融合509室88206683-525zywang@cad.zju.edu.cn硕导陈为副教授实时图形学、科学计算可视化、图像与视频信息处理、医学影像分析509室88206681-522chenwei@cad.zju.edu.cn硕导刘新国研究员计算机图形学，虚拟现实CAD&CG国家重点实验室524室88206681-转526xgliu@cad.zju.edu.cn博导华炜副研究员虚拟现实、计算机图形、计算机游戏、人机交互、三维地理信息系统紫金港校区信息与控制大楼40713957182998huawei@cad.zju.edu.cn硕导蔺宏伟副研究员计算机图形学，曲面造型，计算机辅助几何设计，逆向工程51513819192796hwlin@cad.zju.edu.cn张宏鑫副教授计算机图形学，几何造型，机器学习517室88206681-519zhx@cad.zju.edu.cn硕导周昆教授计算机图形学、虚拟现实、人机交互513室88206683kunzhou@cad.zju.edu.cn长江特聘教授何晓飞研究员机器学习、计算机视觉、信息检索506室88206683xiaofeihe@cad.zju.edu.cn姓名职 称学科专长及研究方向办公地点办公室电话Email备注张三元教授计算机图形学、CAD、图象处理与数字媒体技术、数字几何处理技术等31087952690syzhang@cs.zju.edu.cn博导孙守迁教授计算机辅助工业设计与概念设计、虚拟人技术及应用、应用人机工程与设计、新媒体艺术与设计307（东）87952639ssqq@mail.hz.zj.cn博导应放天副教授yingft@vip.sina.com硕导彭韧副教授工业设计（产品设计、视觉传达设计及环境设计）、计算机辅助工业设计201（东）87952589硕导汤永川副研究员模糊逻辑、自然计算、情感计算、群体行为仿真、不确定信息融合、数字化艺术与设计506（东）87952639yongchuan@263.net硕导姓名职 称学科专长及研究方向办公地点办公室电话Email备注陈小平副教授计算机应用、计算机网络应用、信息安全文三路508号天苑大厦11楼C座88225422-818xpchen@tuling.com.cn硕导史烈副教授多媒体、计算机网络文三路405号85026338-816sl@insigma.com.cn具体请与陈文智老师联系"}
{"content2":"全景视频拼接关键技术一、原理介绍图像拼接(Image Stitching)是一种利用实景图像组成全景空间的技术，它将多幅图像拼接成一幅大尺度图像或360度全景图，图像拼接技术涉及到计算机视觉、计算机图形学、数字图像处理以及一些数学工具等技术。图像拼接其基本步骤主要包括以下几个方面：摄相机的标定、传感器图像畸变校正、图像的投影变换、匹配点选取、全景图像拼接（融合），以及亮度与颜色的均衡处理等，以下对各个步骤进行分析。摄相机标定由于安装设计，以及摄相机之间的差异，会造成视频图像之间有缩放（镜头焦距不一致造成）、倾斜（垂直旋转）、方位角差异（水平旋转），因此物理的差异需要预先校准，得到一致性好的图像，便于后续图像拼接。相机的运动方式与成像结果之间的关系见下图。：相机的运动方式与成像结果之间的关系图像坐标变换在实际应用中，全景图像的获得往往需要摄像机以不同的位置排列和不同的倾角拍摄。例如由于机载或车载特性，相机的排列方式不尽相同，不能保证相机在同一面上，如柱面投影不一定在同一个柱面上，平面投影不一定在同一平面上；另外为了避免出现盲区，相机拍摄的时候往往会向下倾斜一定角度。这些情况比较常见，而且容易被忽略，直接投影再拼接效果较差。因而有必要在所有图像投影到某个柱面（或平面）之前，需要根据相机的位置信息和角度信息来获得坐标变换后的图像。理论上只要满足静止三维图像或者平面场景的两个条件中的任何一个，两幅图像的对应关系就可以用投影变换矩阵表示，换句话说只要满足这其中任何一个条件，一个相机拍摄的图像可以通过坐标变换表示为另一个虚拟相机拍摄的图像。一般情况下8参数的透视投影变换最适合描述图像之间的坐标关系，其中8参数的矩阵为[m0,m1,m2;m3,m4,m5; m6,m7,1]；各参数对应的相机运动表示如下：如显示的是相机向下倾斜一定角度拍摄图像，这个角度与m6和m7具有对应关系，如果要获得校正图像，只需要对8参数矩阵求逆后进行坐标变换。(a) 原始图像：(a) 原始图像；(b)x方向形变效果；(c)倾斜校正后效果图像畸变校正由于制造、安装、工艺等原因，镜头存在着各种畸变。为了提高摄像机拼接的精度，在进行图像拼接的时候必须考虑成像镜头的畸变。一般畸变分为内部畸变和外部畸变，内部畸变是由于摄影本身的构造为起因的畸变，外部畸变为投影方式的几何因素起因的畸变。镜头畸变属于内部畸变，由镜头产生的畸变一般可分为径向畸变和切向畸变两类。径向畸变就是集合光学中的畸变像差，主要是由于镜头的径向曲率不同而造成的，有桶形畸变和枕型畸变两种。切向畸变通常被人为是由于镜头透镜组的光学中心不共线引起的，包括有各种生成误差和装配误差等。一般人为，光学系统成像过程当中，径向畸变是导致图像畸变的主要因素。径向畸变导致图像内直线成弯曲的像，且越靠近边缘这种效果越明显。根据径向畸变产生的机理，对视频图像进行校正。效果如(b)所示，经过校正的图像，其有效像素区域缩小，一般可通过电子放大的方式进行校正，如(c)所示。：(a)为原始采集图像；(b)为经过径向失真校正的图像；(c)为经过放大的图像图像投影变换由于每幅图像是相机在不同角度下拍摄得到的，所以他们并不在同一投影平面上，如果对重叠的图像直接进行无缝拼接，会破坏实际景物的视觉一致性。所以需要先对图像进行投影变换，再进行拼接。一般有平面投影、柱面投影、立方体投影和球面投影等。平面投影就是以序列图像中的一幅图像的坐标系为基准，将其图像都投影变换到这个基准坐标系中，使相邻图像的重叠区对齐，称由此形成的拼接为平面投影拼接；柱面投影是指采集到的图像数据重投影到一个以相机焦距为半径的柱面，在柱面上进行全景图的投影拼接；球面投影是模拟人眼观察的特性，将图像信息通过透视变换投影到眼球部分，构造成一个观察的球面；立方体投影是为了解决球面影射中存在的数据不宜存储的缺点，而发展出来的一种投影拼接方式，它适合于计算机生成图像，但对实景拍摄的图像则比较困难。如下所示，图像拼接处理流程示意图。：图像拼接处理流程示意图匹配点选取与标定由于特征点的方法较容易处理图像之间旋转、仿射、透视等变换关系，因而经常被使用，特征点包括图像的角点以及相对于其领域表现出某种奇异性的兴趣点。Harris等提出了一种角点检测算法，该算法是公认的比较好的角点检测算法，具有刚性变换不变性，并在一定程度上具有仿射变换不变性，但该算法不具有缩放变换不变性。针对这样的缺点，Lowe提出了具有缩放不变性的SIFT特征点。如上所示，图像的拼接需要在图像序列中找到有效的特征匹配点。图像的特征点寻找直接影响图像拼接的精度和效率。对于图像序列，如果特征点个数≥4个，则很容易自动标定图像匹配点；如果特征点很少，图像拼接往往不能取得较为理想的效果。图像拼接融合图像拼接的关键两步是：配准(registration)和融合(blending)。配准的目的是根据几何运动模型，将图像注册到同一个坐标系中；融合则是将配准后的图像合成为一张大的拼接图像。在多幅图像配准的过程中，采用的几何运动模型主要有：平移模型、相似性模型、仿射模型和透视模型。图像的平移模型是指图像仅在两维空间发生了 方向和 方向的位移，如果摄像机仅仅发生了平移运动，则可以采用平移模型。图像的相似性模型是指摄像机本身除了平移运动外还可能发生旋转运动，同时，在存在场景的缩放时，还可以利用缩放因子 多缩放运动进行描述，因此，当图像可能发生平移、旋转、缩放运动时，可以采用相似性模型。图像的仿射模型是一个6参数的变换模型，即具有平行线变换成平行线，有限点映射到有限点的一般特性，具体表现可以是各个方向尺度变换系数一致的均匀尺度变换或变换系数不一致的非均与尺度变换及剪切变换等，可以描述平移运动、旋转运动以及小范围的缩放和变形。图像的透视模型是具有8个参数的变换模型，可以完美地表述各种表换，是一种最为精确变换模型。图像融合技术一般可分为非多分辨率技术和多分辨率技术两类。在非多分辨率技术中主要有平均值法、帽子函数法、加权平均法和中值滤波法等。多分辨率技术主要有高斯金字塔、拉普拉斯金字塔、对比度金字塔，梯度金字塔和小波等。(a)-(d)为四幅不同视角的图像，(e)为最终拼接得到的柱面全景图像亮度与颜色的均衡处理因为相机和光照强度的差异，会造成一幅图像内部，以及图像之间亮度的不均匀，拼接后的图像会出现明暗交替，这样给观察造成极大的不便。亮度与颜色均衡处理，通常的处理方式是通过相机的光照模型，校正一幅图像内部的光照不均匀性，然后通过相邻两幅图像重叠区域之间的关系，建立相邻两幅图像之间直方图映射表，通过映射表对两幅图像做整体的映射变换，最终达到整体的亮度和颜色的一致性。二、国内外现状全景拼接侦察系统在国外已经有了较早的研究，早在1992年，剑桥大学的L.G.Brown就对图像拼接的核心技术进行总结，1996年微软研究院的Richard Szeliski提出基于运动的全景拼接模型。Szeliski后来又相继发表了若干这方面论文，2000年Shmuel Peleg提出改进方法，根据相机的运动方式自适应选择拼接模型，2003年M.Brown发表了SIFT特征进行图像拼接的方法，但计算量很大，2007年Seong Jong Ha提出移动相机系统的全景拼接方法，不仅保证效果，而且运算速度也不错。在国内方面，也有不少高校科研机构对视频拼接技术及应用进行研究，其中，上海凯视力成信息科技有限公司研发的“全景视觉态势感知系统” 最具代表性，该系统功能完善、技术先进、性能可靠，并已成功应用于多种车型。上海凯视力成信息科技有限公司全景视觉态势感知系统——PVS9112型是实时的全固态无机械运动的高清360度凝视视频系统，系统提供实时的连续覆盖整个战场的全运动视频，人机交互界面直观快捷。系统适应于恶劣环境，支持彩色和红外传感器，可以黑天和白天全天候工作，实时地图像处理和高清视频的显示，显示界面同时提供360度全景窗口和感兴趣区域的高清画面。系统提供开发式结构，便于同其它系统如雷达等集成到一起，以获得一个完整的态势理解。传感头： 几种传感头图形界面：  PVS9112图形界面特点:实时的360度视频全景显示，便于对态势的感知理解、安全监视、目标探测。从而提高平台的攻击能力和安全防护性。支持高清彩色和红外传感器。日夜均可工作。显示全局拼接画面、局部感兴趣。图形交互界面。并支持多种人机交互接口，支持触摸屏、鼠标、键盘、自定义按键、操纵摇杆等，可无缝接入已有系统。无运动部件，高可靠性。适应车船飞机等恶劣的工作环境。可选特征:目标检测与告警自动多目标跟踪视频记录和回放可选支持PTZ长距离光电探测系统，在全景视频上可以通过触摸的方式控制PTZ的快速转动到指定的位置，克服了传统操控PTZ方式的缺陷，使得PTZ摄像机的功效大大提高。图像透雾增强算法电子稳像算法开放式体形架构：二、应用从图像拼接的实际应用来看，主要有大型航空照片，卫星图像拼接，车载系统监控，虚拟场景实现，视频压缩；很多资料上都提及车载系统的拼接，这种拼接侦查系统可以用于不同车辆，如反恐、安全监视、侦察、巡逻和警车等；系统给操作者提供车辆周围的实时全景图像，使之能够感知全面而丰富的态势，操纵车辆的同时还可以有效将自己保护在车内，不用通过车辆的挡风玻璃就可以实时操控。全景图像极大地增强了用户的视觉感知系统，使其在特种车辆、军用以及民用方面都拥有广阔的市场前景。与传统的多画面监控相比，全景拼接画面更符合人眼观察，极大地提高了侦察的准确性。但很少提及机载系统，因此，机载方面只要设计合理，应用前景是巨大的。应用实例：方式一：基本模式摄像机组 + 一或两个终端，支持记录或不记录，每个终端所显示内容可以不同。方式二：增强模式摄像机组 + 2以上终端，可通过GigE网络来扩展连接其他设备，如视频记录仪等。 采集和预处理模块，实现对视频信号的捕获，并对摄像机组进行管理，如PTZ控制等。在这一层，实现对不同路数、不同接口形式的摄像机的支持。并作必要的预处理功能，如图像的缩放平移投影变换、数据压缩等，为后续处理器准备好数据。模块还通过多个GigE网络，将视频数据分发给多个不同的处理器或其它设备，如视频记录仪等。这种组成结构，可适应不同应用需求：如不同的摄像机种类和数量，终端处理功能要求不同等等。关于视频拼接产品的介绍"}
{"content2":"https://mp.weixin.qq.com/s/trkCGvpW6aCgnFwLxrGmvQ撰稿 & 整理｜Debra 编辑｜Debra导读：在 2018 云栖人工智能峰会上，阿里巴巴推出的人工智能产品和相关服务真不少，包括一款天猫精灵人机交流车载系统，两款搭载天猫精灵系统的移动机器人太空蛋、太空梭，汽车战略重大升级，推出车路协同系统以及首款 L4 车辆协同自动驾驶新能源车。AI 前线对这些产品介绍做了整理，希望可以帮助大家了解阿里这段时间在智能语音、自动驾驶方面的最新研究和进展。天猫精灵人机交流车载系统阿里巴巴自从 2017 年 7 月推出天猫精灵品牌之后，已经陆续推出了多款 AI 智能产品，其中大家最熟悉的可能是智能音箱天猫精灵。据达摩院人工智能实验室产品总负责人杜海涛介绍，目前，智能音箱天猫精灵销量已破 500W 台，具备 700 多项能力，连接了近 7000 万可用家庭电器，每天调用峰值达 4000 万次，每天陪伴在人身边的时长达 1 小时。数据显示，2018 年第一季度，天猫精灵已经以 110 万的出货量占据了中国智能音箱市场 59% 的市场份额，并做到了中国第一、全球第三的位置，成为阿里人工智能落地产品中的代表，引人注目。在今天的人工智能峰会上，天猫精灵的另一项新发布，同样吸引人的眼球，那就是天猫精灵人机交流车载系统。据介绍，它包括在阿里的天猫精灵汽车 AI＋计划之内。据介绍，这个系统主要围绕导航、娱乐、通讯需求而开发，采用了阵列增强技术，在车内拥有 10dB 以上的干扰消除能力，语音唤醒日常环境下准确率达到 95%，语音识别率达到了 93%；搭载声纹识别技术，全球首用智能语音支付场景，其语音合成技术可以贴合人声自然度 90% 以上。天猫精灵人机交流车载系统具有车内人机交互、人车互动、娱乐服务和家车互联四项主要功能。其中声纹技术迁移到车载系统，在人机交互中可以发挥抗噪的功能；人车交互包括语音调用查询车辆状态等功能；娱乐服务包括听新闻、音乐和电台，也可查询天气、股票订餐和票务等信息；家车互联旨在将家和车辆信息打通。官方称，阿里的家车互联已经支持 164 个智能平台，并且联合了 300 多个品牌，支持 1000 多款设备。早在今年 6 月，阿里巴巴天猫精灵就已经与沃尔沃、宝马、奔驰、奥迪四家车企达成合作，具有联网功能的车辆均可以在未来搭载天猫精灵人机交流车载系统。阿里透露，明年将与沃尔沃全线车辆达成合作，落地内置天猫精灵。阿里将开未来酒店，用上天猫精灵太空蛋在峰会上，天猫精灵发布了新系统：Aligenie 3.0。它具备听、说、看、行动的能力，可以实现精准定位、自助导航、环境感知、传感器融合、人机交互、多机器人协同，实现语音、视觉、多模态交互功能。此外，天猫精灵家族再添两名新成员：太空蛋和太空梭，将分别用于未来酒店和医院等设施。天猫精灵太空蛋可以接收天猫精灵的指令，乘坐自动电梯进行物品快送，也可自动去储备仓。官方表示，太空蛋将会用于第一家阿里未来酒店。另外一款机器人太空梭将会用于医院等设施，它内置了 60 个独立药仓，可用语音控制进行非接触式无菌操作，乘坐自动电梯进行物品快送。这两款产品都内嵌了 Aligenie 3.0 系统。首创智能感知基站，车路协同系统峰会上，阿里巴巴集团宣布升级汽车战略：由车向路延展，利用车路协同技术打造全新的“智能高速公路”。这一战略将由 AliOS 联合阿里云、达摩院、高德、支付宝、千寻位置、斑马网络等共同完成，旨在探索未来二十年的路。达摩院人工智能实验室首席科学家王刚对车路协同系统做了详细介绍。他指出，自动驾驶其实早已不是一个新的研究课题，但是为什么经过长时间研究到现在也没有完全实现自动驾驶和商业化呢？开发人员也许应该反思他们的技术路径和方法是否可以优化。在他看来，其中一个原因就在于过去几十年自动驾驶领域专注于单车智能优化，但单车智能系统即使智能程度达到非常高的水平，也会面临很多问题，如感知盲区、死角、障碍物会导致真实环境中的安全隐患；车载传感器不购灵敏，即使是最好的激光雷达能感知到的行人距离仅有几十米，而且系统极度依赖高精度地图，一旦发生特殊情况，系统处理就会出问题。这是单车智能系统面临的非常困难或者根本不可能克服的难题；单车智能系统的另一个问题是成本，据统计，2018 年自动驾驶车辆均价为 20 万美元，高昂的成本阻碍了自动驾驶技术的发展。所以，阿里认为打造安全可靠、成本降低的自动驾驶车辆，需要将车辆自身和道路设施结合起来，利用“聪明”的道路和道路设施来解决问题，道路数据协同共享还可以降低成本。基于此，阿里推出了车路协同系统。协同智能系统能够提高安全性能，在“看”、“想”、“做”（分别对应着自动驾驶车辆感知、决策、控制三个部分）三个方面提高安全性能、降低成本。王刚重点介绍了车路协同系统在感知上的功能，他表示，车路协同智能系统中的感知基站，可以做到无死角、精准识别路面状况、互联互通车辆、全覆盖感知，而没有感知距离的限制；在决策上，相对单车智能的局部最优，协同智能可以做到全局最优。目前，阿里已经和交通运输部公路研究院成立了车路协同的实验室，未来会将技术开放给产业。首款 L4 车路协同自动驾驶新能源车峰会上，阿里还发布了首款 L4 新能源车，搭载了协同智能系统，能够实现“车端 - 路端 - 云端”三位一体的车路协同智能。据介绍，阿里发布的自动驾驶智慧物流车前后和两侧使用 Velodyne 的 16 线激光雷达，车顶安装一个 Velodyne 32 线激光雷达，一个双目摄像头，5 个单目摄像头，其他的传感器，如 RTK、超声波雷达等则隐藏在车身中。该车在城市道路中的行驶速度在 30 到 40 公里左右，载重在几吨的级别，定位精度在 20 厘米以内。阿里巴巴人工智能实验室表示，这款定位于 L4 级别的自动驾驶智慧物流车从去年开始研发，目前仍处于测试阶段，离量产还需要一些时间。除此之外，阿里巴巴还喜提杭州市第一张自动驾驶路测牌照，王刚代表阿里“无人车”团队，接过了这张车牌号为“浙 A4390 测”的牌照。据悉，这是继此前阿里曝光无人车、车路协同技术方案后，在自动驾驶方面的最新进展。不只消费级产品，阿里AI Lab走向何方？2017 年，阿里巴巴将马云投入上亿美元的 Pepper 机器人项目中止，把人员队伍拆分划进人工智能实验室（阿里 AI Labs）。自此，主导智能音箱项目的阿里巴巴人工智能实验室潜伏地下，默默攻关有半年之久，致力于打造“阿里巴巴人工智能实验室首款消费级人工智能产品”。2017 年 8 月，阿里人工智能实验室推出了天猫精灵 X1 智能音箱和智能语音系统 AliGenie，正式入局智能音箱市场。2018 年 3 月 22 日，阿里巴巴人工智能实验室在北京召开新品发布会上公布了几项新产品：新版交互引擎 AliGenie 2.0：在第一代中文语音交互的基础上，引入了「听觉」、「视觉」、「触觉」及「情感反馈」的多模态交互能力；「精灵火眼」+ XHolder：天猫精灵手机 APP 中新增「精灵火眼」功能，搭配连接硬件 XHolder，便可将智能手机秒变音箱显示屏幕，同时为天猫精灵增加视觉能力；天猫精灵曲奇版：一款 Mini 智能音箱，同样搭载 AliGenie 语音系统，可用于播放音乐、电台，也可购物、控制能家居，售价为 299 元；天猫魔屏：一款 3D 智能投影仪，覆盖华数、优酷、土豆等多个内容平台，可支持天猫精灵语音操控。截至当时，天猫精灵的总销量已经突破 200 万台，累计回答了超过 1 亿个问题，执行了 9 亿次任务。此外，天猫精灵在智能家居生态上也进一步扩容，目前可连接 4500 万台家用电器，并联合联发科发布了蓝牙 Mesh 5.0 协议。直到 2017 年 10 月，长于自然语言理解、实体挖掘的聂再清担任 AI Labs 北京研发中心总负责人，擅长计算机视觉的李名杨任 AI Labs 机器视觉杰出科学家。两位专家的加入，预示着 AI Labs 未来的产品方向除了天猫精灵这一语音交互产品外，很可能会推出基于视觉交互，甚至“机器人”类型的人工智能产品，重新恢复对机器人的研发。果不其然，在云栖大会上，AI Labs 发布了两款面向 B 端企业用户的服务型机器人，一款室内送货机器人、一款室内补货机器人，其操作流程与天猫精灵语音系统全面打通，而支付环节则打通了支付宝。而这与去年 7 月该实验室发布智能音箱时给自己设立的「专注于消费级 AI 产品」的定位，实际上已经有所偏差。在团队组成上，阿里人工智能实验室过去一年里也发生了一些变化。其中，浅雪（本名陈丽娟）已由 2017 年发布天猫精灵 X1 智能音箱和智能语音系统 AliGenie 时的智能生活事业部总经理，变为人工智能实验室负责人。杜海涛也由原来的高级产品专家升为人工智能实验室产品总负责人。而王刚则带领无人驾驶研发团队独立出来，组建了达摩院智慧交通实验室。从今天阿里在峰会上公布的几项重大发布中，我们不难发现阿里对于“车”的重视，从天猫精灵人机交流车载系统、车路智能协同系统、首款搭载车路智能协同系统 L4 新能源汽车，到智慧物流车，无一不体现着阿里对自动驾驶的野心。另一方面，阿里人工智能实验室的定位，也由原来的专注于“消费级 AI 产品”，开始向机器人、智能货运等方向拓展。未来，它将在这几个方向如何发力？如何在 AI 实验室遍地开花的时代下脱颖而出？我们拭目以待。"}
{"content2":"Deep Learning（深度学习）学习笔记整理系列zouxy09@qq.comhttp://blog.csdn.net/zouxy09作者：Zouxyversion 1.0  2013-04-08声明：1）该Deep Learning的学习系列是整理自网上很大牛和机器学习专家所无私奉献的资料的。具体引用的资料请看参考文献。具体的版本声明也参考原文献。2）本文仅供学术交流，非商用。所以每一部分具体的参考资料并没有详细对应。如果某部分不小心侵犯了大家的利益，还望海涵，并联系博主删除。3）本人才疏学浅，整理总结的时候难免出错，还望各位前辈不吝指正，谢谢。4）阅读本文需要机器学习、计算机视觉、神经网络等等基础（如果没有也没关系了，没有就看看，能不能看懂，呵呵）。5）此属于第一版本，若有错误，还需继续修正与增删。还望大家多多指点。大家都共享一点点，一起为祖国科研的推进添砖加瓦（呵呵，好高尚的目标啊）。请联系：zouxy09@qq.com目录：一、概述二、背景三、人脑视觉机理四、关于特征4.1、特征表示的粒度4.2、初级（浅层）特征表示4.3、结构性特征表示4.4、需要有多少个特征？五、Deep Learning的基本思想六、浅层学习（Shallow Learning）和深度学习（Deep Learning）七、Deep learning与Neural Network八、Deep learning训练过程8.1、传统神经网络的训练方法8.2、deep learning训练过程九、Deep Learning的常用模型或者方法9.1、AutoEncoder自动编码器9.2、Sparse Coding稀疏编码9.3、Restricted Boltzmann Machine(RBM)限制波尔兹曼机9.4、Deep BeliefNetworks深信度网络9.5、Convolutional Neural Networks卷积神经网络十、总结与展望十一、参考文献和Deep Learning学习资源一、概述Artificial Intelligence，也就是人工智能，就像长生不老和星际漫游一样，是人类最美好的梦想之一。虽然计算机技术已经取得了长足的进步，但是到目前为 止，还没有一台电脑能产生“自我”的意识。是的，在人类和大量现成数据的帮助下，电脑可以表现的十分强大，但是离开了这两者，它甚至都不能分辨一个喵星人 和一个汪星人。图灵（图灵，大家都知道吧。计算机和人工智能的鼻祖，分别对应于其著名的“图灵机”和“图灵测试”）在 1950 年的论文里，提出图灵试验的设想，即，隔墙对话，你将不知道与你谈话的，是人还是电脑。这无疑给计算机，尤其是人工智能，预设了一个很高的期望值。但是半 个世纪过去了，人工智能的进展，远远没有达到图灵试验的标准。这不仅让多年翘首以待的人们，心灰意冷，认为人工智能是忽悠，相关领域是“伪科学”。但是自 2006 年以来，机器学习领域，取得了突破性的进展。图灵试验，至少不是那么可望而不可及了。至于技术手段，不仅仅依赖于云计算对大数据的并行处理能力，而且依赖 于算法。这个算法就是，Deep Learning。借助于 Deep Learning 算法，人类终于找到了如何处理“抽象概念”这个亘古难题的方法。2012年6月，《纽约时报》披露了Google Brain项目，吸引了公众的广泛关注。这个项目是由著名的斯坦福大学的机器学习教授Andrew Ng和在大规模计算机系统方面的世界顶尖专家JeffDean共同主导，用16000个CPU Core的并行计算平台训练一种称为“深度神经网络”（DNN，Deep Neural Networks）的机器学习模型（内部共有10亿个节点。这一网络自然是不能跟人类的神经网络相提并论的。要知道，人脑中可是有150多亿个神经元，互 相连接的节点也就是突触数更是如银河沙数。曾经有人估算过，如果将一个人的大脑中所有神经细胞的轴突和树突依次连接起来，并拉成一根直线，可从地球连到月 亮，再从月亮返回地球），在语音识别和图像识别等领域获得了巨大的成功。项目负责人之一Andrew称：“我们没有像通常做的那样自己框定边界，而是直接把海量数据投放到算法中，让数据自己说话，系统会自动从数据中学习。”另 外一名负责人Jeff则说：“我们在训练的时候从来不会告诉机器说：‘这是一只猫。’系统其实是自己发明或者领悟了“猫”的概念。”2012年11月，微软在中国天津的一次活动上公开演示了一个全自动的同声传译系统，讲演者用英文演讲，后台的计算机一气呵成自动完成语音识别、英中机器 翻译和中文语音合成，效果非常流畅。据报道，后面支撑的关键技术也是DNN，或者深度学习（DL，DeepLearning）。2013年1月，在百度年会上，创始人兼CEO李彦宏高调宣布要成立百度研究院，其中第一个成立的就是“深度学习研究所”（IDL，Institue of Deep Learning）。为什么拥有大数据的互联网公司争相投入大量资源研发深度学习技术。听起来感觉deeplearning很牛那样。那什么是deep learning？为什么有deep learning？它是怎么来的？又能干什么呢？目前存在哪些困难呢？这些问题的简答都需要慢慢来。咱们先来了解下机器学习（人工智能的核心）的背景。二、背景机器学习（Machine Learning）是一门专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能的学科。机 器能否像人类一样能具有学习能力呢？1959年美国的塞缪尔(Samuel)设计了一个下棋程序，这个程序具有学习能力，它可以在不断的对弈中改善自己的 棋艺。4年后，这个程序战胜了设计者本人。又过了3年，这个程序战胜了美国一个保持8年之久的常胜不败的冠军。这个程序向人们展示了机器学习的能力，提出 了许多令人深思的社会问题与哲学问题（呵呵，人工智能正常的轨道没有很大的发展，这些什么哲学伦理啊倒发展的挺快。什么未来机器越来越像人，人越来越像机 器啊。什么机器会反人类啊，ATM是开第一枪的啊等等。人类的思维无穷啊）。机器学习虽然发展了几十年，但还是存在很多没有良好解决的问题：例如图像识别、语音识别、自然语言理解、天气预测、基因表达、内容推荐等等。目前我们通过机器学习去解决这些问题的思路都是这样的（以视觉感知为例子）：从开始的通过传感器（例如CMOS）来获得数据。然后经过预处理、特征提取、特征选择，再到推理、预测或者识别。最后一个部分，也就是机器学习的部分，绝大部分的工作是在这方面做的，也存在很多的paper和研究。而中间的三部分，概括起来就是特征表达。良好的特征表达，对最终算法的准确性起了非常关键的作用，而且系统主要的计算和测试工作都耗在这一大部分。但，这块实际中一般都是人工完成的。靠人工提取特征。截止现在，也出现了不少NB的特征（好的特征应具有不变性（大小、尺度和旋转等）和可区分性）：例如Sift的出现，是局部图像特征描述子研究领域一项里 程碑式的工作。由于SIFT对尺度、旋转以及一定视角和光照变化等图像变化都具有不变性，并且SIFT具有很强的可区分性，的确让很多问题的解决变为可 能。但它也不是万能的。然而，手工地选取特征是一件非常费力、启发式（需要专业知识）的方法，能不能选取好很大程度上靠经验和运气，而且它的调节需要大量的时间。既然手工选取特 征不太好，那么能不能自动地学习一些特征呢？答案是能！Deep Learning就是用来干这个事情的，看它的一个别名UnsupervisedFeature Learning，就可以顾名思义了，Unsupervised的意思就是不要人参与特征的选取过程。那它是怎么学习的呢？怎么知道哪些特征好哪些不好呢？我们说机器学习是一门专门研究计算机怎样模拟或实现人类的学习行为的学科。好，那我们人的视觉系统是 怎么工作的呢？为什么在茫茫人海，芸芸众生，滚滚红尘中我们都可以找到另一个她（因为，你存在我深深的脑海里，我的梦里 我的心里 我的歌声里……）。人脑那么NB，我们能不能参考人脑，模拟人脑呢？（好像和人脑扯上点关系的特征啊，算法啊，都不错，但不知道是不是人为强加的，为了使 自己的作品变得神圣和高雅。）近几十年以来，认知神经科学、生物学等等学科的发展，让我们对自己这个神秘的而又神奇的大脑不再那么的陌生。也给人工智能的发展推波助澜。三、人脑视觉机理1981 年的诺贝尔医学奖，颁发给了 David Hubel（出生于加拿大的美国神经生物学家） 和TorstenWiesel，以及 Roger Sperry。前两位的主要贡献，是“发现了视觉系统的信息处理”：可视皮层是分级的：我们看看他们做了什么。1958 年，DavidHubel 和Torsten Wiesel 在 JohnHopkins University，研究瞳孔区域与大脑皮层神经元的对应关系。他们在猫的后脑头骨上，开了一个3 毫米的小洞，向洞里插入电极，测量神经元的活跃程度。然后，他们在小猫的眼前，展现各种形状、各种亮度的物体。并且，在展现每一件物体时，还改变物体放置的位置和角度。他们期望通过这个办法，让小猫瞳孔感受不同类型、不同强弱的刺激。之所以做这个试验，目的是去证明一个猜测。位于后脑皮层的不同视觉神经元，与瞳孔所受刺激之间，存在某种对应关系。一旦瞳孔受到某一种刺激，后脑皮层的某 一部分神经元就会活跃。经历了很多天反复的枯燥的试验，同时牺牲了若干只可怜的小猫，David Hubel 和Torsten Wiesel 发现了一种被称为“方向选择性细胞（Orientation Selective Cell）”的神经元细胞。当瞳孔发现了眼前的物体的边缘，而且这个边缘指向某个方向时，这种神经元细胞就会活跃。这个发现激发了人们对于神经系统的进一步思考。神经-中枢-大脑的工作过程，或许是一个不断迭代、不断抽象的过程。这里的关键词有两个，一个是抽象，一个是迭代。从原始信号，做低级抽象，逐渐向高级抽象迭代。人类的逻辑思维，经常使用高度抽象的概念。例如，从原始信号摄入开始（瞳孔摄入像素 Pixels），接着做初步处理（大脑皮层某些细胞发现边缘和方向），然后抽象（大脑判定，眼前的物体的形状，是圆形的），然后进一步抽象（大脑进一步判定该物体是只气球）。这个生理学的发现，促成了计算机人工智能，在四十年后的突破性发展。总 的来说，人的视觉系统的信息处理是分级的。从低级的V1区提取边缘特征，再到V2区的形状或者目标的部分等，再到更高层，整个目标、目标的行为等。也就是 说高层的特征是低层特征的组合，从低层到高层的特征表示越来越抽象，越来越能表现语义或者意图。而抽象层面越高，存在的可能猜测就越少，就越利于分类。例 如，单词集合和句子的对应是多对一的，句子和语义的对应又是多对一的，语义和意图的对应还是多对一的，这是个层级体系。敏感的人注意到关键词了：分层。而Deep learning的deep是不是就表示我存在多少层，也就是多深呢？没错。那Deep learning是如何借鉴这个过程的呢？毕竟是归于计算机来处理，面对的一个问题就是怎么对这个过程建模？因为我们要学习的是特征的表达，那么关于特征，或者说关于这个层级特征，我们需要了解地更深入点。所以在说Deep Learning之前，我们有必要再啰嗦下特征（呵呵，实际上是看到那么好的对特征的解释，不放在这里有点可惜，所以就塞到这了）。"}
{"content2":"求职季，真的会让一个人变得有些不一样吧，比如我，对于一个菜鸟来说，最近一段时间焦虑不安外加有点自闭...前段时间在校内网上看到了陌陌科技内推计算机视觉算法工程师和机器学习算法工程师的消息，抱着试试的心态整理了一份简历按照提供的邮箱投出去了，我想这次应该又是石沉大海了吧，谁想在一周前闷热的一天在嘈杂的餐厅接到了陌陌科技HR的电话，一周后的周五下午4点在西安的一家咖啡馆参加面试。我问清了时间地点并道谢了HR后并挂了电话继续吃饭。好吧，这周每天都有各个公司的笔试，外加这周周五上午的组会轮到我做组会汇报，我心里预估了一下时间安排，确实没时间来准备陌陌的面试，心想，就这样吧，面挂了就当积累经验吧...时间很快就来到了周四晚上，当我9点做完招商银行的网上笔试后，打来之前没有写完的明天组会汇报的ppt接着写了起来，前两天已经连续凌晨2点回宿舍了，今晚不知何时能回。我主要给大家汇报一下近期的工作以及一篇临时看的发表在ICB2018上的使用GAN来完成从热红外到可见光的跨频谱人脸匹配的文献。时间来到11点半，ppt算是写得差不多了，但是文献中还是有很多细节问题因为时间关系没有搞懂，这篇文献里最大创新点也就是所提出的损失函数理解得云里雾里，我是继续加班搞懂才回宿舍呢，还是就这样将这个问题放在组会上大家一起讨论，可是明天还得早起啊。有那么一瞬间，我感觉呼吸不太顺畅，身体超负荷运转已经吃不消了。我选择回宿舍休息，就这样吧...昨晚还是没有睡好，已经很久没有睡好觉了，不过相对于前两天，已经很不错了，9点组会开始，疲倦始终围绕着我，不出所料，这次组会因为各种因素我算是搞砸了...组会完后，回到实验室给手机充电，打印了一份简历，吃完午饭回来打开百度地图搜索那家咖啡馆的地理位置并做好时间路线规划，等待手机充满电后我便出发了，下午3点我提前一个小时到了那家咖啡馆，我微信上给HR发消息说我到了进门之后怎么走，那位帅气的HR带我进了咖啡馆，问了下我的姓名和求职岗位，带我去签完到后给我找一个位置并了给了我一张A4纸，然后就是用自己熟悉的语言实现两道算法题，我周围的人都是今天来面试陌陌的，他们都在认真的低着头写代码。第一题是实现输出一个长度为n的无序数组中的前k个最小值：我能想到的就是先通过各种排序算法将数组排序，然后输出前k个最小值就行了，但是这并不是最好的方式，会造成复杂度比较高，因为只需要输出前k个最小值，剩下的n-k个数值不需要考虑。那么通过排序算法只需要排前k个数值ok了，不同的排序算法时间复杂度都是不一样的，比如比较容易实现的选择排序和冒泡排序平均情况下都是O(n2)，若只需要找到前面的k个值复杂度也要O(n*k)，若使用快速排序，复杂度近似O(n)，如果使用堆排序，复杂度近似O(nlogk)，下面基于堆排序给出解题思路以及python3代码：方法是维护k个元素的最大堆，即用容量为k的最大堆存储最先遍历到的k个数，并假设它们即是最小的k个数，建堆费时O（k）后，有k1<k2<...<kmax（kmax设为大顶堆中最大元素）。继续遍历数列，每次遍历一个元素x，与堆顶元素比较，x<kmax，更新堆（用时logk），否则不更新堆。这样下来，总费时O（k+（n-k）*logk）=O（n*logk）。此方法得益于在堆中，查找等各项操作时间复杂度均为logk，python3代码如下：1 # -*- coding: utf-8 -*- 2 \"\"\" 3 Created on Sun Sep 2 17:16:36 2018 4 5 @author: aoanng 6 \"\"\" 7 8 def create_heap(lyst): 9 #创建数组中前k个数的最大堆 10 for start in range((len(lyst) - 2) // 2, -1, -1): 11 sift_down(lyst, start, len(lyst) - 1) 12 13 14 return lyst 15 16 # 堆排序，对于本问题用不着 17 def heapSort(lyst): 18 # 堆排序 19 for end in range(len(lyst) - 1, 0, -1): 20 lyst[0], lyst[end] = lyst[end], lyst[0] 21 sift_down(lyst, 0, end - 1) 22 return lyst 23 24 # 最大堆调整 25 def sift_down(lst, start, end): 26 root = start 27 while True: 28 child = 2 * root + 1 29 if child > end: 30 break 31 if child + 1 <= end and lst[child] < lst[child + 1]: 32 child += 1 33 if lst[root] < lst[child]: 34 lst[root], lst[child] = lst[child], lst[root] 35 root = child 36 else: 37 break 38 39 40 #测试 41 if __name__ == '__main__': 42 list1 = [50, 45, 40, 20, 25, 35, 30, 10, 15] 43 k = 4 #设置需要输出的前k个最小值 44 list_n_k = list1[k:] 45 heap_k = create_heap(list1[:k]) #将数组前k个数创建最大堆，并假设它们是最小的k个数 46 for i in range(len(list_n_k)): 47 if list_n_k[i]<heap_k[0]: 48 heap_k[0] = list_n_k[i] 49 heap_k = create_heap(heap_k) #更新堆 50 print(heap_k) #输出前k个未排序的最小值 51 52 #若需要，则可以对堆进行排序 53 heap_k_sort = heapSort(heap_k) 54 print(heap_k_sort)第二题是给出一个n*n的矩阵，将其逆时针旋转90度，但是不能开辟新的内存空间：这题的前提是必须在原数组上进行旋转操作，只要搞清楚矩阵中元素旋转的规律就容易求解了，那就是当前元素ai,j经过逆时针旋转90度后有ai,j=aj,(n-i)的关系。写完这两道算法题后，我检查了两遍并在那儿扣手机，hr看见我写完之后过来收走了我的作业，让我等待一会儿，大概20分钟后一位技术面试官带着我的简历以及之前写好的算法题过来找我，第一轮技术面试便开始了。我先简单的自我介绍后，面试官看着我的写的那两道算法题聊了起来，让我说说我的思路，我说第一题其实就是一个排序算法问题，然后输出前k个值就好，看见我用的选择排序算法，面试官指出了疑问，说这样时间复杂度会比较高，然后问我有没有其他的思路，我说可以只需要排前k个值将时间复杂度降到O(n*k)，面试官最后逐步的引导我，说用最大堆会比较好，总之面试官人很nice，问到我不会的，总是在引导我。然后就是介绍我简历中做过的几个项目，项目当中有用到深度学习平台tensorflow和CNN网络架构以及一些机器学习算法。面试官逐个问我，比如在ttensorflow中怎样构建一个cnn网络，防止过拟合的一些tips，Dropout是怎样工作的等等，然后让我手写在tensorflow中怎样保存模型和加载模型，tf.get_variable和tf.Variable的区别，tf.variable_scope和tf.name_scope的用法和区别，这些其实我平时项目中也有用到，平时也关注过这个问题，只是没有上心，当时没有回答上来，然后面试官大致的给我讲了一下原理就跳过这个问题了，回来后我又在网上查了一下资料，总结如下：tf.variable_scope和tf.name_scope的用法：tf.variable_scope可以让变量有相同的命名，包括tf.get_variable得到的变量，还有tf.Variable的变量tf.name_scope可以让变量有相同的命名，只是限于tf.Variable的变量例如：1 import tensorflow as tf; 2 3 with tf.variable_scope('V1'): 4 a1 = tf.get_variable(name='a1', shape=[1], initializer=tf.constant_initializer(1)) 5 a2 = tf.Variable(tf.random_normal(shape=[2,3], mean=0, stddev=1), name='a2') 6 with tf.variable_scope('V2'): 7 a3 = tf.get_variable(name='a1', shape=[1], initializer=tf.constant_initializer(1)) 8 a4 = tf.Variable(tf.random_normal(shape=[2,3], mean=0, stddev=1), name='a2') 9 10 with tf.Session() as sess: 11 sess.run(tf.global_variables_initializer()) 12 print (a1.name) 13 print (a2.name) 14 print (a3.name) 15 print (a4.name) 16 17 #输出： 18 ''' 19 V1/a1:0 20 V1/a2:0 21 V2/a1:0 22 V2/a2:0 23 '''如果将上边的tf.variable_scope换成tf.name_scope将会报错：Variable a1 already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope?...改成如下这样就ok了：1 import tensorflow as tf 2 3 with tf.name_scope('V1'): 4 # a1 = tf.get_variable(name='a1', shape=[1], initializer=tf.constant_initializer(1)) 5 a2 = tf.Variable(tf.random_normal(shape=[2,3], mean=0, stddev=1), name='a2') 6 with tf.name_scope('V2'): 7 # a3 = tf.get_variable(name='a1', shape=[1], initializer=tf.constant_initializer(1)) 8 a4 = tf.Variable(tf.random_normal(shape=[2,3], mean=0, stddev=1), name='a2') 9 10 with tf.Session() as sess: 11 sess.run(tf.global_variables_initializer()) 12 # print (a1.name) 13 print (a2.name) 14 # print (a3.name) 15 print (a4.name) 16 17 #输出： 18 ''' 19 V1/a2:0 20 V2/a2:0 21 '''接下来看看tf.Variable和tf.get_variable()的区别在tensorflow中，tf.Variable和tf.get_variable()两个op分别用来创建变量。tf.Variable()总是创建新的变量，返回一个variable，可以定义名字相同的变量，若给出的name已经存在，会自动修改name，生成个新的：1 import tensorflow as tf 2 w_1 = tf.Variable(3,name=\"w_1\") 3 w_2 = tf.Variable(1,name=\"w_1\") 4 print (w_1.name) 5 print (w_2.name) 6 #输出 7 #w_1:0 8 #w_1_1:0tf.get_variable()不可以定义名字相同的变量，tf.get_variable函数拥有一个变量检查机制，会检测已经存在的变量是否设置为共享变量，如果已经存在的变量没有设置为共享变量，TensorFlow 运行到第二个拥有相同名字的变量的时候，就会报错。不同的变量之间不能有相同的名字，除非你定义了variable_scope，这样才可以有相同的名字。1 import tensorflow as tf 2 3 w_1 = tf.get_variable(name=\"w_1\",initializer=1) 4 w_2 = tf.get_variable(name=\"w_1\",initializer=2) 5 #错误信息 6 #ValueError: Variable w_1 already exists, disallowed. Did 7 #you mean to set reuse=True in VarScope?tf.get_variable一般和tf.variable_scope配合使用，用于在同一个的变量域中共享同一个变量。如何在tensorflow中保存和加载模型呢？构建网络中加入：saver = tf.train.Saver()然后在session会话中：saver.save(sess, \"./model/model.ckpt\")加载模型：构建网络中需要和之前一样，然后在session会话中加载模型：saver.restore(sess, \"./model/model.ckpt\")然后和面试官讨论一些机器学习算法的问题，诸如LR和SVM的区别，随机森林和GBDT区别，xgboost以及最优化算法的原理等等，很快一个多小时就过去啦，感觉自己表现得不是太好，但是和面试官还是挺聊得来的，面试的最后，面试官问我对一些经典的数据结构熟悉不？我说还可以，然后他让我现场写一个单链表的逆序，很简单的问题，我却没写出来，我曾经看过java版本和c版本的数据结构与算法，前不久也看过用python实现的数据结构与算法，但是这个时候我却卡住了。10分钟后面试官看我连这个简单的问题都没写出来，笑着对我说该不该给我第二轮技术面的机会，然后第一轮技术面就这样结束了，让我在旁边的椅子上稍等一下...等的过程中，我拿出手机百度了下单链表的逆序如何实现，恍然大悟的同时也有点懊恼，参考网上的答案，实现如下：循环反转单链表：1 #定义一个单链表节点 2 class ListNode: 3 def __init__(self,x): 4 self.data = x 5 self.next = None 6 7 def nonrecurse(head): #循环的方法反转链表 8 if head is None or head.next is None: 9 return head 10 pre=None 11 cur=head 12 h=head 13 while cur: 14 h=cur 15 tmp=cur.next 16 cur.next=pre 17 pre=cur 18 cur=tmp 19 return h 20 21 head=ListNode(1) #测试代码 22 p1=ListNode(2) #建立链表1->2->3->4->None; 23 p2=ListNode(3) #head->p1->p2->p3->None 24 p3=ListNode(4) 25 head.next=p1 26 p1.next=p2 27 p2.next=p3 28 29 p=nonrecurse(head) #输出链表 4->3->2->1->None 30 while p: 31 print (p.data) 32 p=p.next递归实现单链表反转：1 class ListNode: 2 def __init__(self,x): 3 self.val=x; 4 self.next=None; 5 6 7 def recurse(head,newhead): #递归，head为原链表的头结点，newhead为反转后链表的头结点 8 if head is None: 9 return ; 10 if head.next is None: 11 newhead=head; 12 else : 13 newhead=recurse(head.next,newhead); 14 head.next.next=head; 15 head.next=None; 16 return newhead; 17 18 head=ListNode(1); #测试代码 19 p1=ListNode(2); # 建立链表1->2->3->4->None 20 p2=ListNode(3); 21 p3=ListNode(4); 22 head.next=p1; 23 p1.next=p2; 24 p2.next=p3; 25 newhead=None; 26 p=recurse(head,newhead); #输出链表4->3->2->1->None 27 while p: 28 print (p.val) 29 p=p.next;接下来就是技术第二面了，不说了，说多了都是泪...参考：窥探算法之美妙---寻找数组中最小的K个数&python中巧用最大堆程序员编程艺术：第三章、寻找最小的k个数tf.variable_scope和tf.name_scope的用法tf.Variable()与tf.get_variable()与不同之处TensorFlow模型保存和加载方法单链表反转python实现"}
{"content2":"人工智能已经发展了半个多世纪了。就其本质而言，就是对人的思维的信息过程的模拟。机器智慧能否超越人类是一个备受争议的话题，就连著名物理学家霍金，Tesla CEO 马斯克，未来学家库兹韦尔都在警告人类，人工智能即将超越人类。人工智能的未来到底会是怎样的？太可怕了，谁也说不清，未来却一步步逼近。最近看了下 Jeff Hawkins / Sandra Blakeslee 的书 On Intelligence，中文书译作人工智能的未来。内容不错，推荐阅读。以下为书中摘录。On Intelligence真正认识人类大脑是开发智能机器的必由之路！    ——Jeff Hawkins第一章 人工智能 Artificial Intelligence他（指 France Crick，DNA结构发现者之一）认为神经学只是一堆没有任何理论的数据。他说：“最明显的是在概念上缺乏一个总的框架。”计算机无法智能化。对此我无法证明，但直觉告诉我这是对的，就像人们对一些事情有直觉一样。最终文凭得出结论：用传统的方法研究出的人工智能可以生产出实用的产品，但绝不可能 制造出真正的智能机器。今天，许多人认为人工智能是一个即成的事物，只待计算机具有足够的能力后，即可实现其美好前景。只要计算机有足够的内存和处理能力，思维能力就会发展，人工智能的编程人员就可以造出智能机器。对此我不敢苟同。人工智能正面临着一个根本性错误，因为它无法圆满地解决什么是智能的问题，或者说“理解某个事物”到底意味着什么。万能计算：尽管建构的细节有所不同，但从根本上讲所有的计算机都是等效的。图灵开创了人工智能的研究领域，其核心是：大脑就是另一种形式的计算机，无论你如何设计它的人工智能系统，这台计算机都可以做出类似人的行为。行为学家认为，大脑内部的运转过程是不可知的，称之为“无法穿越的黑匣子”；但是人们可以观察和测量动物所处的环境和行为。在任何情况下，无论多么成功的人工智能程序也只是擅长于那些经过专门设计的领域。它们不会总结归纳，缺乏灵活性；甚至它们的创造者也认为它们不会像人一样思考。图灵机将会改变世界----而且它已经改变了世界，但不是通过人工智能。理解是无法用外部行为测量的，它是对大脑如何记忆、如何利用这些记忆进行判断的内部度量。第二章 神经网络 Neural Networks神经网络不同于计算机，它没有 CPU，不能对信息进行中央存储；整个网络的知识和记忆都分散在所有的连接之上----就像真正的大脑一样。对于大脑的解读，有三种标准必不可少。大脑功能的时间概念。反馈的重要性。任何大脑模型或理论都应该能够解释大脑的物理结构。有一种最普通的神经网络，被称为BP网络，可以学会将输出单元的错误传输回输入单元，并将它记住。你可能认为这是一种反馈，而事实并非如此。这种错误的反向传播只会发生在学习过程中，而当经过训练的神经网络工作正常时，信息只会向一个方向传输，冲输出到输入没有产生反馈。除此之外，模型也没有时间概念。人工智能和神经网络研究者都认为智能存在于行为之中，，而这种行为是执行一个输入后，由一个程序或神经网络产生出来的。电脑程序和神经网络最重要的属性就是能否进行正确的、令人满意的输出，就像阿兰·图灵所说的，智能等同于行为。所有的机器，无论是建造出来的还是想象出来的，都必须能做些事情；世界上没有会思考的机器，只有能干活的机器。甚至当我们在观察自己的同类时，所关注的也是他们的外在行为，而不是内在思想。道理很容易理解，但从直觉上看却是错误的。对于大脑的作用以及工作原理仍然没有形成全面的理论和框架。它会不会因为大脑的复杂而极为复杂呢？是否要用写满100页纸的数学公式才能描述清楚大脑大脑是如何工作的？我们是否要画出成百上千的线路图才能做出一个有意义的解释呢？我不这样认为。从历史上看，对于科学问题最完美的解决办法往往是简单而精确地。尽管细节工作可能令人生畏，通往最后理论的道路可能充满险阻，但最终的概念框架体系通常是简明的。第三章 人脑 The Human Brain仍然有一些解剖学家估计人类大脑皮层中包含大约300亿个神经元，但我觉得，及时这一数字再大上很多或小上很多，也不会有人对此表示惊讶。曾经在《科学美国人》发表文章的 France Crick 在许多年后又写了一本关于大脑的书----《惊人的假设》(The Astonishing Hypothiesis)。所谓惊人的假设，是指我们的思想就是大脑细胞的产物，没有魔力，也没有特殊的浆汁，思想就是由神经元和闪动的信息流构成的。神经科学家认识到任何有助于大脑皮层回路发挥作用的东西之前，意识到某些心理功能是固定在一定的区域里的。如果脑中风损坏了某人的右脑顶叶，他就会失去知觉，甚至整个左侧身体失去知觉，对身体左侧的空间也没有任何感觉。如果中风发生在左前脑的Broca 区，他就会丧失使用语法的功能，而同时他的词汇和理解词汇的能力却不会有任何改变。如果纺锤体脑回发生中风，他就不再拥有辨别面孔的能力----就不能认出自己的妈妈和孩子，甚至连照片上自己的面孔也会变得陌生。这些奇特的功能紊乱，使早起的神经学家意识到大脑皮层包含许多不同的功能区。Vernon Mountcastle 所说的大脑皮层区域之间的微小差异正是他们之间连接的差异，而不是基本功能的差异。他总结说，所有大脑皮层的功能区域都遵循一个共同的算法，视觉、听觉、甚至运动输出之间没有任何差异。他还认为，大脑皮层的连接方式是由基因决定的，这正是功能和物种的独特之处，而脑皮层组织本身在各个区域都担负着相同的功能。神经科学家还发现，大脑皮层的回路具有令人惊异的可塑性，也就是说，它可以根据流经的输入信息的类型进行改变和重组。大脑各区域是根据传入的信息种类而发展出专门的功能。大脑皮层并不是严格地运用不同的算法完成不同的功能的，正如目前地球上各个国家区域的划分并不是先天注定的。大脑皮层是极其灵活的，而且输入大脑的都是模式。这些模式来自哪里并不重要，只要它们在时间上以固定的方式彼此联系，大脑就能感觉到它们的存在。我们能相信这个世界就是我们看到的样子吗？我认为，世界的确是以一个绝对的形式存在的，和我们感受到的非常接近，但我们的大脑不能直接认识这个绝对的世界。第四章 记忆 Memory不论从并行还是其他方面，大脑皮层都不像计算机，它不会计算问题的答案，而是用存储的记忆来解决问题，做出反应。虽然计算机也有记忆，且是以硬盘驱动器和记忆芯片的形式出现的，但大脑皮层的记忆和电脑的记忆有以下四点根本区别：大脑皮层可以存储模式序列；大脑皮层以自联想方式回忆起模式；大脑皮层以恒定的形式存储模式；大脑皮层按照层级结构存储模式。你也许已经注意到，有些人在讲故事时不能立刻切入主题，而是啰啰嗦嗦地讲一些无关的细节。这让人很恼火，甚至想大叫：“讲正题吧！”但他们只会按照发生的时间，而不是其他的方式来讲述他们的故事。下一次洗澡的时候，请留心你用毛巾擦干身体的过程。我发现我在擦干身体的时候，几乎每次都是相同的程序----擦干、拍净以及相应的姿势。通过有趣的观察，我还发现我妻子淋浴之后也遵循基本上一成不变的方式。你或许也是一样。如果有一定的顺序，就试着改变它。你可以强迫自己去改变，但必须集中注意力，因为稍一走神，你又会落入习惯的方式中去。记忆之所以能够被存储是抓住了相互关系的精髓，而不是某个瞬间的细节。当你在看、在感觉、在听某个东西时，大脑皮层就接收到详细的高度特化的输入信息，并将它们转化成一种恒定模式。实际上，被存储的是恒定模式，与每一个新的输入模式相比较的也是恒定模式。记忆的存储、记忆的唤醒以及记忆的识别都发生在恒定形式的基础之上。在计算机中却没有相同的概念。第五章 智能理论新架构 A New Framework of Intelligence因此，只有一种办法可以解释你对改变了的门所做出的反应----你的大脑对于某个特定社科将要看到、听到和感觉到的东西进行了低级感觉预测，而且是并行的，脑皮层各个区域都会自动地预测下一步的感觉。视觉区对边缘、形状、物体、位置和动作做出预测；听觉区对音调、声音的来源以及声音的模式做出预测；体觉区则对触摸、质地、轮廓以及温度进行预测。我们都有这样的经历：当某些连续的背景声音，比如远处的风钻发出持续的噪音，或是在餐馆、商店、工厂等播放绵绵的背景音乐时，我们会毫不在意；然而一旦它们停下来，我们会马上注意到。你的听觉区域预测的是音乐的连续性，只要它不变化，你就不会留意，而一旦它停下来，就破坏了这种预测，于是就吸引了你的注意力。科学本身就是一种预测。我们进行一系列的假想和实验，从而不断改进我们对世界的看法。这本书也是对什么是智能以及大脑工作原理的预测。真挚连产品的设计也是一种基本的预测过程。不论是设计服装还是设计手机，工程师们都会猜测竞争对手的做法、消费者的需求、新产品的成本以及什么样的款式是市场需求的。人类脑皮层很大，因而有相当巨大的记忆容量，它能够不断地预测你将要看到、听到和感觉到的东西，而且多数都是在你不自觉地情况下发生的。这些预测就是我们的思想，与感觉输入信息流结合之后就形成了我们的知觉。我认为对于大脑的这个认识，就是智能的记忆和预测框架。现在我们可以清楚地看到阿兰·图灵错在哪里----智能的证据是预测，而不是行为。第六章 大脑皮层工作原理 How the Cortex Works与照相机的记忆所不同的是，你的大脑记住的是世界的本质，而不是它看上去的那个样子。当你对世界进行思考时，你是在提取那些对应于世界上物体的存在和表现方式的模式，而不是它们在某一刻通过你的任何感官所表现出来的样子。你用以体验世界万物的顺序反映的就是世界的恒定结构。你体验世界各部分的顺序是由世界结构所决定的。许多年来，大多数科学家都忽视了这些反馈连接。如果我们对大脑的了解仅仅集中在脑皮层是如何接受所输入的信息、如何加工信息，然后如何对它做出反应的话，就不需要反馈，而仅仅是连接着感觉器官和脑皮层运动区的前馈连接。但是当我们开始认识到脑皮层的核心功能是做出预测时，就会不得不将反馈纳入我们的模型里来了。大脑要将信息送回最先接受输入的区域。预测就是在实际发生了什么和我们期待发生什么之间进行比较。信息向上流动，我们的期待向下流动。还有一点，所有的预测都是从经验中学习的。我们期待着钢笔夹现在和将来都会发出“噼啪”声，这是因为它们过去就是这样。自行车在车库碰撞而发出“呯呯”的声音，我们都是以可以预测的方式看到、感觉到并且听到的。你不是生来就具有这些知识，这些都是通过学习而得来的，这得归功于你的脑皮层有令人难以置信的大容量，从而可以记住各种模式。如果传入大脑的输入中有一贯相同的模式，脑皮层就会将它们用来预测未来的事情。如果脑皮层区域能够说话，它们也许会说；“我体验到了许多不同的模式。有时候我预测不到接下来会看到什么模式。但是这些模式的集合肯定是彼此联系着的。它们总是一同出现，我能够可靠地在它们之间跳来跳去。因此每当看到这些事件中的任何一个，我都会用同一个名字来指称它们。我传递给皮层中更高区域的就是这样的组名，而不是单个的模式。”在我们的日常体验中时刻都能遇到这样的现象。当人们说话时，脱离了具体上下文，他们所说的单个词语就可能让人听不懂。然而，当你听到一个句子中的某个模糊不清的词时，不会因为这个词而使整个理解过程受阻，你照样能理解这个句子。同样地，离开了上下文，手写的字也经常有认不出的时候，但是，如果将它放到一个完全手写的句子中，你就能认出来了。在多半情况下，你是意识不到自己在利用大脑中的序列记忆来填充模棱两可或不完全的信息。你听到了你所期待听到的，看到了所期待看到的----至少当你所听到的和所看到的符合过去经验时是这样。如果你反复研究某一类事物，你的大脑皮层将在较低的层级中形成对它们的记忆表征，这就把皮层的较高层级给释放出来了，可以利用它来学习更细微、更复杂的关系。根据这一理论，专家就是这样被培养出来的。专家和天才有着比普通人更能观察结构之结构、模式之模式的大脑。你也能通过练习而成为专家，但这其中当然也还包含遗传方面的因素。在新皮层下游相互能沟通的三大脑结构，它们是基底神经节、小脑和海马。这三大结构都限于新皮层而存在。粗略地说，基底神经节是原始的运动系统，小脑主要学习事件间精确的时间关系， 海马则储存具体事件及与地点有关的记忆。如果你失去了左、右两半海马，你就失去了形成新记忆的能力。没有了海马，你可以照常说话、走路、看东西和听东西，乍一看你几乎就是个正常人。但实际上，你受了很大的损伤：你记不住所有新的事物；你能记起失去海马前许多年认识的朋友，但你却记不住一个新遇见的人；及时在一年里，你每天去看你的私人医生5次，但你每次见到他时都会觉得是第一次见到他。你对失去海马以后发生的一切事情都无法记住。发现并建立一个崭新的科学理论框架，要求我们寻找最简单的原理。这个简单的原理必须能整合并解释大量看似毫无联系的事实。这一过程带来的必然后果就是：我们可能会将问题过于简单化。这样，很多重要的细节可能会被忽视，事实也可能会被曲解。如果这个理论框架有一定的道理，我们就肯定能发现其需要改进或修正的地方，比如或许我们走的太远了，或许走得还不够远，或许它根本就是错误的。第七章 意识和创造力 Consciousness and Creativity到目前为止我对新大脑皮层所做的阐述以及它的工作原理都是基于一个非常基本的前提----世界是有组织的、有结构的，因此是可以预测的。时间万物都有固定模式：脸上长有眼睛；眼睛里有瞳孔；火是烫的；重力使物体下落；门可以开关；等等。这个世界不是随意构成的，也不具有同性质。如果世界不存在组织结构，那么记忆、预测以及行为就失去了存在的意义。所有的行为，不论是人的，还是蜗牛的，是但细胞生物的，还是一棵树的，都是一种利用世界的结构来进行繁衍的方式。然而，在进化过程中，出现了一些非常有趣的情况----神经元之间的联结可以改变。神经元可以根据最近发生的状况而决定是否发出信号。现在，在生物的生命周期内，行为可以改变，神经系统具有了可塑性。因为记忆可以很快地形成，所以动物在其一生中能够习得世界的结构。如果世界突然改变，比如说出现了一种新的掠食动物，其他的动物就不必坚持其基因所决定的行为，因为这些行为不再适合。具有可塑性的神经系统具有很强的进化优势，它可以引发大量新物种的出现包括各种鱼类、蜗牛和人类等。智能的发展经历了三个时期，每个时期都利用了记忆和预测。在第一个时期，生物将DNA作为记忆的载体。个体在其生命周期中不能学习和适应。它们只能将基于DNA的关于世界的记忆通过基因传给后代。第二个时期始于自然界出现了能快速形成记忆的可改变的神经系统。此时，个体便能在它的生命周期内认识世界结构的重要方面并相应地调试自己的行为。但是。个体除了直接观察以外仍然不能将知识传给后代。在这个时期里，新皮层开始出现并扩大，但并不是这个时期的主要特点。第三个，也是最后一个时期，人人类所特有的。这个时期是随着语言的产生和新皮层的扩展而开始的。人在一生中学会了世界结构的许多知识。并能运用语言有效地将这些知识传递给许多人。诗人的天赋在于：将看似毫不相干的词语或思想联系起来，并用这种方式照亮世界。他们创造出让人意想不到的类比，能使人们借此认识到更高层次的结构。事实上，富有创造性的艺术作品之所以得到大家的喜爱，是因为它们出乎我们的预想。当看到一部影片打破你熟悉的人物模式、故事情节以及摄制技术（包括特技在内）时，你会因为它不落俗套而欣赏它。油画、音乐、诗歌、小说等所有这些创造艺术形式都在努力打破传统的框架，并尽可能要出乎受众的预料。在此，矛盾的力量使得艺术伟大：一方面我们要求艺术耳熟能详，一方面又要求它与众不同、出人意料。打过熟悉是重复和俗气；太过于独特则显得不协调和难以理解。最好的作品就是打破我们期望的模式并且同时交给我们新的模式。能把自己训练得更具有创造力吗？是的，的确可以。我已经发现，在解决问题时有一些方法能培养我们找到有用的相似点。首先，你要设想自己正在面对的问题是有答案的。人们容易轻言放弃，但你要相信问题的答案正等着你去发现，你要为此而坚持进行长时间的思考。其次，你要让自己的思维任意驰骋。给你的大脑充分的时间和空间来发现问题的答案。找到一个问题的解答实际上就是要找到存储在大脑皮层中与当前要解决的问题相似的模式。如果你对某个问题感到困惑，记忆-预测模型建议你从不同的角度去观察它，这样会提高用以往经验找到相似点的可能性。如果你只是静静地坐在那儿，一遍又一遍地看着问题，那是不会有任何进展的。试着把问题分解一下，将问题的各个部分用各种不同的方式进行重新组合。如果在某个问题上你被卡住了，那么暂时将它放到一边，先做点别的事情，然后再重新开始，将问题改变一下提法。如果你这样反复多遍，可能要花上几天或几个星期的时间，但问题迟早会有些眉目的，你会在过去的经验中找到相似的情形。要想成功，就要反复琢磨问题，同时还要做点其他事情，这样，大脑皮层就会有机会找到一些相似的记忆。大脑是建立模型、做出创造性预测的器官，但它建立的模型和做出的预测可能是正确的，同样也可能是个美丽的谎言。我们的大脑总是在考察模式，做出类比。如果事物间正确的联系没能被揭示，我们的大脑会很满足地接受错误。伪科学、偏见、宗教还有偏执狭隘经常根植于错误的类推之中。第八章 智能之未来 The Future of Intelligence我们要语言一项新技术的最终用途是困难的。正如大家在整本书中看到的，大脑是通过类比过去而做出预测的。因此，我们自然而然地认为，新技术仅仅是被用来完成那些旧技术所要完成的同样任务，只是更快、更高效、更经济。新技术的最终用途通常是无法预料的，它远远超出我们最初想象力所及。有人认为机器具有智能基本上就等于说它具有了人的智力。他们担心智能机器将会因为“被奴役”而感到愤怒，因为人类憎恨被奴役。他们担心智能机器会试图统治世界，因为历史上曾有智者这么做过。但这样的担忧是建立在一个错误的类推之上的。人们之所以有这样的担忧是因为他们将智能----新皮层的算法，于古脑的情感因素----诸如恐惧、多疑和欲望等，归并起来了。智能机器是不具备这些能力的。它们不会有野心，也不会渴望财富，寻求社会认同以及性满足。它们没有食欲、嗜好，也不会出现情绪不稳定的情况。智能机器不会有任何类似人类情感的东西，除非我们刻意把它们设计成那样。智能机器最强大的应用就是那些人类智力有困难的地方，感觉器官不能及的领域和那些单调乏味的工作。通常，这样的活动几乎不涉及情感。我认为智能机器在4个方面会超过我们人脑的能力：速度、容量、可复制性和感觉系统。通过合适的感觉和稍加调整的皮层记忆，智能机器也许刻意存在于数学和物理学的虚拟世界中病进行思考。例如，在数学等理科领域里，有些问题是要求能够理解物体在多于三维的空间里是如何运动的。那些研究空间本质的弦理论家（string theorists），认为宇宙有十维或更多的维数。人类思考思维或四维以上的数学问题就已经非常费劲，也许合理设计的智能机器能如你我理解三维空间一样去理解多维空间，并能很好地预测它们的运动规律。结语 Epilogue人工智能在其鼎盛时期是一场规模浩大的运动，它拥有自己的期刊、学位课程、书籍、商业计划和企业家。神经网络在二十世纪八十年代兴盛时同样也让人们为之兴奋无比。但是人工智能和神经网络的科学体系框架并不适用于制造智能机器。企业成功的秘密之一就是在百分之百地确信自己能成功之前就投身于新的领域。把握世纪也很重要，如果涉入过早，你会太费力；如果你一直等到不确定因素都不存在了，那就太迟了。"}
{"content2":"Firefly基于RK3399Pro处理器，推出了AI核心板Core-3399Pro-JD4，芯片采用了CPU+GPU+NPU的硬件结构，具有高算力、高效能等特点。核心板可灵活的应用在AI项目中，以模块的方式加速项目落地。RK3399Pro人工智能处理器RK3399Pro人工智能芯片，采用了ARM双核Cortex-A72 + 四核Cortex-A53的大小核处理器架构，主频高达1.8GHz，集成Mali-T860 MP4 四核图形处理器。片上集成了AI神经网络处理器NPU，支持8Bit/16Bit运算，运算性能高达3.0TOPs，具备高性能、低功耗、开发易等优势，满足视觉、音频等各类AI应用。Firefly高性能AI核心板Firefly六核高性能AI核心板Core-3399Pro-JD4，板载RK3399Pro处理器，兼容多种AI框架，支持TensorFlow Lite/Android NN API，AI软件工具支持对Caffe / TensorFlow模型的导入及映射、优化；拥有强大的硬解码能力，支持DP1.2、HDMI 2.0、MIPI-DSI、eDP 多种显示输出接口，支持双屏同显/双屏异显；拥有I2C、SPI、UART、ADC、PWM、GPIO、PCIe、USB3.0、I2S（支持8路数字麦克风阵列输入）等丰富的扩展接口。组合行业应用主板Core-3399Pro-JD4核心板与底板组合，构成完整的高性能行业应用主板，扩展接口更丰富，性能更强大，可直接应用到各种智能产品中，加速产品落地。Core-3399Pro-JD4核心板适用于集群服务器、高性能计算/存储、计算机视觉、边缘计算、商显一体设备、医疗健康设备、自动售货机、工业电脑等各AI应用领域。Core-3399Pro-JD4核心板介绍视频进入Firefly官网了解更多内容"}
{"content2":"近日，福布斯发表一篇名为《20个推动人工智能改革的科技领导者》的署名文章，介绍了中国顶尖科技公司中的20位致力于人工智能的重要人物，并认为在人工智能领域中国正在挑战美国的领导地位。在福布斯列出的20位重要人物中，有10位出自百度，其中7位如今都在百度担任人工智能领域的重要职务。这7位人工智能重要人物中，最受关注的是今年年初加入百度，担任百度集团总裁兼COO的陆奇，此前陆奇曾服务微软，担任微软应用与服务部门执行副总裁，曾经是美国科技行业中担任最高管理职位的华人，加入百度后负责领导公司的人工智能战略。如今百度的所有业务群组都向陆奇报告。在陆奇加入百度后，百度发言人称：“我们相信，跟随陆奇先生，百度的发展战略能顺利推进，百度将成为世界一流的技术公司和全球人工智能领域的领导者。”同时入选福布斯榜单的还有现任百度人工智能技术平台体系总负责人王海峰和百度研究院负责人林元庆。王海峰管理着包括深度学习实验室、大数据实验室、硅谷AI实验室、增强现实实验室、自然语言处理部、人工智能平台部等技术研发团队，他的专长在于自然语言处理和机器翻译，撰写过100余篇有关人工智能的学术论文，并将自己的专业知识运用于百度在神经语言程序学、计算机数学、语音识别、知识图谱、个性化推荐和深度学习等多个方面的开发。而林元庆作为百度研究院的负责人，目前管理百度研究实验室，包括美国硅谷人工智能实验室，大数据实验室，增强现实实验室及深度学习实验室，他的主要研究方向是计算机视觉研究，移动搜索和无人驾驶汽车。除了陆奇、王海峰和林元庆，百度杰出科学家徐伟和百度硅谷人工智能实验室主任亚当•考特斯也入选了福布斯的这一榜单，他们组成了百度人工智能研发的核心团队，林元庆和徐伟还代表百度牵头筹建了由国家相关机构出资组建的深度学习技术及应用国家工程实验室。此外，担任百度度秘事业部的总经理景鲲和百度自然语言处理部技术总监吴华也入选了这一榜单。基于全球领先的人工智能研发能力，百度刚刚宣布将于7月5日在北京国家会议中心举办第一届CREATE主题的“百度AI开发者大会”（Baidu Create 2017），会上将发布百度人工智能开放平台的整体战略、技术和解决方案。同时，百度深耕多年的AI技术也将面对中国开发者首次整体亮相，包括语音、图像、自然语言处理及大数据用户画像等基础领域以及智能驾驶、人机对话式操作系统、智能云计算等新兴应用技术都会一一展示出来，特别是百度开放自动驾驶技术的Apollo计划也会第一次公布细节。据百度开发者大会官方网站透露，百度董事长兼CEO李彦宏和百度集团总裁兼COO陆奇都将出席此次大会，此次其他出现在福布斯人工智能领导者排行榜上的百度人工智能专家们也将悉数登场，共同面向开发者讲述AI，讲述“开发者的成功，才是百度的成功”。姓名   :薛强"}
{"content2":"简评：如果人工智能犯了错怎么办？乘客看到了停车标志，突然感到一阵恐慌，因为他搭乘的自动驾驶汽车反而开始加速。当他看到前面的铁轨上一列火车向他们疾驰而来时，他张开嘴对前面的司机大声喊叫，但他突然意识到汽车前坐并没有司机。列车高速撞上来，压碎了这辆自动驾驶汽车，乘客当场死亡。这个场景是虚构的，但是凸显了当前人工智能中一个非常真实的缺陷。在过去的几年里，已经有越来越多的例子表明 —— 机器可以被误导，看见或听见根本不存在的东西。如果出现「噪音」会干扰到人工智能的识别系统，就可能产生误觉。比如上面的场景，尽管停车标志在人眼中清晰可见，但机器却未能识别出来。「停止」标志上一些简单的贴纸就足以使机器视觉算法看不见这个告示，而在人类的眼中依然显而易见计算机科学家称之为「对抗性例子」(adversarial examples)。MIT 的计算机科学家阿塔利（Anish Athalye）表示：我们可以把这些东西看作是人工智能网络会以某种方式处理的输入信息，但机器在看到这些输入信息后会做出一些意想不到的反应。▎看物体到目前为止，人们主要关注的是视觉识别系统。阿塔利已经证明，将一张猫的图像稍加改动，人眼看来仍是一只标准的猫，却被所谓的神经网络误解为是鳄梨酱。最近，阿塔利把注意力转向了实际物体。发现只要稍微调整一下它们的纹理和颜色，就可以骗过人工智能，把这些物体认作别的东西。在一个案例中，棒球被误认为是一杯浓缩咖啡，而在另一个案例中，3D 打印的海龟被误认为是步枪。他们还制造了约 200 个 3D 打印物体，这些物体以类似的方式欺骗了电脑。阿塔利表示：起初，这只是一种好奇，然而，随着这些智能系统越来越多地部署在现实世界中，人们正将其视为一个潜在的安全问题。以目前正在进行实地试验的无人驾驶汽车为例：这些汽车通常依靠复杂的深度学习神经网络导航，并告诉它们该做什么。但在去年，研究人员证明，仅仅只在路标上粘一两张小贴纸，神经网络就可能受骗，将道路上的「停车」标志误认为限速标志。尽管对于机器学习算法，让海龟看起来像步枪似乎是无害的，但研究人员担心，随着人工智能在现实世界中的应用，可能会带来一些危险后果。▎听声音神经网络并不是唯一使用的机器学习框架，但其他的人工智能框架似乎也容易遭受这些怪异事件的影响，并且不限于视觉识别系统。谷歌大脑（Google Brain）正在研发智能机器。谷歌大脑的研究科学家卡里尼（Nicholas Carlini）说，在我见过的每一个领域，从图像分类到自动语音识别，再到翻译，神经网络都可能受到攻击，导致输入信号被错误分类。卡里尼作了展示，加上一些摩擦的背景噪音后，「没有数据集的文章是无用的」这句话的读音，机器会误译为「好，谷歌要浏览 http://evil.com」。在另一个例子中，巴赫的第一号无伴奏大提琴组曲（Cello Suit 1）中的一段音乐节选被记录为「语言可以嵌入音乐」。在卡里尼看来，这些对抗性的例子「最终证明，哪怕在非常简单的任务上，机器学习也没有达到人类的能力」。对我们的耳朵来说，一段古典音乐听起来就是乐器的交响乐，但这段音乐若稍作修改，人工智能可能会理解为是一个语音指令。▎内在原理人工神经网络是大致模仿大脑（即生物神经网络）处理视觉信息的功能并从中学习方法。神经网络的工作原理大致是，获取的数据通过多层人工神经元网络传输进行信息处理，在接受到成百上千个相同物体的样本（通常由人类标记）的训练之后，神经网络开始建立此物体的视觉识别模式，从而能够在其后认得出正在观看的东西是这种物体。其中最复杂的系统采用「深度学习」，这意味着需要拥有更多的信息处理层。稍微改变物体的纹理，研究人员能够让一个3D打印的棒球看起来像一杯浓缩咖啡。然而，尽管计算机科学家了解人工神经网络如何工作，但他们并不一定知道在处理大数据时的具体细节。我们目前对神经网络的理解还不够。比如说，无法准确解释为什么会存在对抗性例子，也不知道如何解决这个问题。部分问题可能与现有技术被设计用来解决的任务的性质有关，例如区分猫和狗的图像。为了做到这一点，神经网络技术将处理大量猫和狗的模样信息，直到有足够的数据点来区分两者。一个真正强大的图像分类器会复制「相似性」对人类的作用，因而可以认出一个孩子涂鸦的猫和一张猫的照片以及一只现实生活中移动的猫代表的是同一样东西。尽管深度学习神经网络令人印象深刻，但在对物体进行分类、感知周遭环境或处理突发事件方面，仍无法与人脑匹敌。如果我们想要开发出能够在现实世界中发挥作用的真正智能机器，或许我们应该回到人脑上来，更好地理解人脑是如何解决这些问题的。▎捆绑问题（Binding problem）虽然神经网络是受到人类视觉皮层的启发，但越来越多的人认识到这种相似性只是表面现象。一个关键的区别在于，除了识别物体边缘的线条或物体本身等视觉特征外，我们的大脑还对这些特征之间的关系进行编码。因此，物体的边缘就构成了这个物体的一部分。这使我们能够对我们所看到的模式赋予意义。当你或我看着一只猫时，我们看到了构成猫的所有特征，以及它们之间的相互关系，这种相互「捆绑的」信息是我们理解世界的能力和我们的一般智力的基础。这个起关键作用的捆绑信息在当代的人工神经网络中是缺失的。对于更具体的行为模式，科学家仍在探索。我们清楚的是 —— 大脑的工作方式与我们现有的机器深度学习模式非常不同，因此，最终可能会走上一条完全不同的路才能成功。很难说可行性有多大，以及取得成功需要多长时间。与此同时，对于越来越多人工智能驱动的机器人、汽车和程序，我们可能需要避免对其过于信任。因为你永远不知道人工智能是不是正在产生被误导的视觉。原文链接：The ‘weird events’ that make machines hallucinate"}
{"content2":"提示：本文为笔者原创，转载请注明出处:blog.csdn.net/carson2005以下链接是本人整理的关于计算机视觉（ComputerVision, CV）相关领域的网站链接，其中有CV牛人的主页，CV研究小组的主页，CV领域的paper,代码，CV领域的最新动态，国内的应用情况等等。打算从事这个行业或者刚入门的朋友可以多关注这些网站，多了解一些CV的具体应用。搞研究的朋友也可以从中了解到很多牛人的研究动态、招生情况等。总之，我认为，知识只有分享才能产生更大的价值，真诚希望下面的链接能对朋友们有所帮助。（1）googleResearch； http://research.google.com/index.html（2）MIT博士，汤晓欧学生林达华； http://people.csail.mit.edu/dhlin/index.html（3）MIT博士后Douglas Lanman； http://web.media.mit.edu/~dlanman/（4）opencv中文网站； http://www.opencv.org.cn/index.php/%E9%A6%96%E9%A1%B5（5）Stanford大学vision实验室； http://vision.stanford.edu/research.html（6）Stanford大学博士崔靖宇； http://www.stanford.edu/~jycui/（7）UCLA教授朱松纯； http://www.stat.ucla.edu/~sczhu/（8）中国人工智能网； http://www.chinaai.org/（9）中国视觉网； http://www.china-vision.net/（10）中科院自动化所； http://www.ia.cas.cn/（11）中科院自动化所李子青研究员； http://www.cbsr.ia.ac.cn/users/szli/（12）中科院计算所山世光研究员； http://www.jdl.ac.cn/user/sgshan/（13）人脸识别主页； http://www.face-rec.org/（14）加州大学伯克利分校CV小组； http://www.eecs.berkeley.edu/Research/P ... CS/vision/（15）南加州大学CV实验室； http://iris.usc.edu/USC-Computer-Vision.html（16）卡内基梅隆大学CV主页；http://www.cs.cmu.edu/afs/cs/project/ci ... ision.html（17）微软CV研究员Richard Szeliski；http://research.microsoft.com/en-us/um/people/szeliski/（18）微软亚洲研究院计算机视觉研究组； http://research.microsoft.com/en-us/groups/vc/（19）微软剑桥研究院ML与CV研究组； http://research.microsoft.com/en-us/gro ... fault.aspx（20）研学论坛； http://bbs.matwav.com/（21）美国Rutgers大学助理教授刘青山； http://www.research.rutgers.edu/~qsliu/（22）计算机视觉最新资讯网； http://www.cvchina.info/（23）运动检测、阴影、跟踪的测试视频下载； http://apps.hi.baidu.com/share/detail/18903287（24）香港中文大学助理教授王晓刚； http://www.ee.cuhk.edu.hk/~xgwang/(25)香港中文大学多媒体实验室（汤晓鸥）; http://mmlab.ie.cuhk.edu.hk/(26)U.C. San Diego. computer vision;http://vision.ucsd.edu/content/home(27)CVonline; http://homepages.inf.ed.ac.uk/rbf/CVonline/(28)computer vision software; http://peipa.essex.ac.uk/info/software.html(29)Computer Vision Resource; http://www.cvpapers.com/(30)computer vision research groups;http://peipa.essex.ac.uk/info/groups.html(31)computer vision center; http://computervisioncentral.com/cvcnews(32)浙江大学图像技术研究与应用（ITRA）团队：http://www.dvzju.com/(33)自动识别网：http://www.autoid-china.com.cn/(34)清华大学章毓晋教授：http://www.tsinghua.edu.cn/publish/ee/4157/2010/20101217173552339241557/20101217173552339241557_.html(35)顶级民用机器人研究小组Porf.Gary领导的Willow Garage:http://www.willowgarage.com/(36)上海交通大学图像处理与模式识别研究所：http://www.pami.sjtu.edu.cn/(37)上海交通大学计算机视觉实验室刘允才教授：http://www.visionlab.sjtu.edu.cn/(38)德克萨斯州大学奥斯汀分校助理教授Kristen Grauman ：http://www.cs.utexas.edu/~grauman/(39)清华大学电子工程系智能图文信息处理实验室（丁晓青教授）：http://ocrserv.ee.tsinghua.edu.cn/auto/index.asp(40)北京大学高文教授：http://www.jdl.ac.cn/htm-gaowen/(41)清华大学艾海舟教授：http://media.cs.tsinghua.edu.cn/cn/aihz(42)中科院生物识别与安全技术研究中心：http://www.cbsr.ia.ac.cn/china/index%20CH.asp(43)瑞士巴塞尔大学 Thomas Vetter教授：http://informatik.unibas.ch/personen/vetter_t.html(44)俄勒冈州立大学 Rob Hess博士：http://blogs.oregonstate.edu/hess/(45)深圳大学 于仕祺副教授：http://yushiqi.cn/(46)西安交通大学人工智能与机器人研究所：http://www.aiar.xjtu.edu.cn/(47)卡内基梅隆大学研究员Robert T. Collins:http://www.cs.cmu.edu/~rcollins/home.html#Background(48)MIT博士Chris Stauffer:http://people.csail.mit.edu/stauffer/Home/index.php(49)美国密歇根州立大学生物识别研究组(Anil K. Jain教授)：http://www.cse.msu.edu/rgroups/biometrics/(50)美国伊利诺伊州立大学Thomas S. Huang:http://www.beckman.illinois.edu/directory/t-huang1(51)武汉大学数字摄影测量与计算机视觉研究中心：http://www.whudpcv.cn/index.asp(52)瑞士巴塞尔大学Sami Romdhani助理研究员：http://informatik.unibas.ch/personen/romdhani_sami/(53)CMU大学研究员Yang Wang:http://www.cs.cmu.edu/~wangy/home.html(54)英国曼彻斯特大学Tim Cootes教授：http://personalpages.manchester.ac.uk/staff/timothy.f.cootes/(55)美国罗彻斯特大学教授Jiebo Luo:http://www.cs.rochester.edu/u/jluo/(56)美国普渡大学机器人视觉实验室：https://engineering.purdue.edu/RVL/Welcome.html(57)美国宾利州立大学感知、运动与认识实验室：http://vision.cse.psu.edu/home/home.shtml(58)美国宾夕法尼亚大学GRASP实验室：https://www.grasp.upenn.edu/(59)美国内达华大学里诺校区CV实验室：http://www.cse.unr.edu/CVL/index.php(60)美国密西根大学vision实验室：http://www.eecs.umich.edu/vision/index.html(61)University of Massachusetts(麻省大学),视觉实验室：http://vis-www.cs.umass.edu/index.html(62)华盛顿大学博士后Iva Kemelmacher:http://www.cs.washington.edu/homes/kemelmi(63)以色列魏茨曼科技大学Ronen Basri:http://www.wisdom.weizmann.ac.il/~ronen/index.html(64)瑞士ETH-Zurich大学CV实验室：http://www.vision.ee.ethz.ch/boostingTrackers/index.htm(65)微软CV研究员张正友：http://research.microsoft.com/en-us/um/people/zhang/(66)中科院自动化所医学影像研究室：http://www.3dmed.net/(67)中科院田捷研究员：http://www.3dmed.net/tian/(68)微软Redmond研究院研究员Simon Baker:http://research.microsoft.com/en-us/people/sbaker/(69)普林斯顿大学教授李凯：http://www.cs.princeton.edu/~li/(70)普林斯顿大学博士贾登：http://www.cs.princeton.edu/~jiadeng/(71)牛津大学教授Andrew Zisserman： http://www.robots.ox.ac.uk/~az/(72)英国leeds大学研究员Mark Everingham:http://www.comp.leeds.ac.uk/me/(73)英国爱丁堡大学教授Chris William: http://homepages.inf.ed.ac.uk/ckiw/(74)微软剑桥研究院研究员John Winn: http://johnwinn.org/(75)佐治亚理工学院教授Monson H.Hayes：http://savannah.gatech.edu/people/mhayes/index.html(76)微软亚洲研究院研究员孙剑：http://research.microsoft.com/en-us/people/jiansun/(77)微软亚洲研究院研究员马毅：http://research.microsoft.com/en-us/people/mayi/(78)英国哥伦比亚大学教授David Lowe: http://www.cs.ubc.ca/~lowe/(79)英国爱丁堡大学教授Bob Fisher: http://homepages.inf.ed.ac.uk/rbf/(80)加州大学圣地亚哥分校教授Serge J.Belongie:http://cseweb.ucsd.edu/~sjb/(81)威斯康星大学教授Charles R.Dyer: http://pages.cs.wisc.edu/~dyer/(82)多伦多大学教授Allan.Jepson: http://www.cs.toronto.edu/~jepson/(83)伦斯勒理工学院教授Qiang Ji: http://www.ecse.rpi.edu/~qji/(84)CMU研究员Daniel Huber: http://www.ri.cmu.edu/person.html?person_id=123(85)多伦多大学教授：David J.Fleet: http://www.cs.toronto.edu/~fleet/(86)伦敦大学玛丽女王学院教授Andrea Cavallaro:http://www.eecs.qmul.ac.uk/~andrea/(87)多伦多大学教授Kyros Kutulakos: http://www.cs.toronto.edu/~kyros/(88)杜克大学教授Carlo Tomasi: http://www.cs.duke.edu/~tomasi/(89)CMU教授Martial Hebert: http://www.cs.cmu.edu/~hebert/(90)MIT助理教授Antonio Torralba: http://web.mit.edu/torralba/www/(91)马里兰大学研究员Yasel Yacoob: http://www.umiacs.umd.edu/users/yaser/(92)康奈尔大学教授Ramin Zabih: http://www.cs.cornell.edu/~rdz/(93)CMU博士田渊栋: http://www.cs.cmu.edu/~yuandong/(94)CMU副教授Srinivasa Narasimhan: http://www.cs.cmu.edu/~srinivas/(95)CMU大学ILIM实验室：http://www.cs.cmu.edu/~ILIM/(96)哥伦比亚大学教授Sheer K.Nayar: http://www.cs.columbia.edu/~nayar/(97)三菱电子研究院研究员Fatih Porikli ：http://www.porikli.com/(98)康奈尔大学教授Daniel Huttenlocher：http://www.cs.cornell.edu/~dph/(99)南京大学教授周志华：http://cs.nju.edu.cn/zhouzh/index.htm(100)芝加哥丰田技术研究所助理教授Devi Parikh: http://ttic.uchicago.edu/~dparikh/index.html(101)瑞士联邦理工学院博士后Helmut Grabner: http://www.vision.ee.ethz.ch/~hegrabne/#Short_CV(102)香港中文大学教授贾佳亚：http://www.cse.cuhk.edu.hk/~leojia/index.html(103)南洋理工大学副教授吴建鑫：http://c2inet.sce.ntu.edu.sg/Jianxin/index.html(104)GE研究院研究员李关：http://www.cs.unc.edu/~lguan/(105)佐治亚理工学院教授Monson Hayes:http://savannah.gatech.edu/people/mhayes/(106)图片检索国际会议VOC(微软剑桥研究院组织): http://pascallin.ecs.soton.ac.uk/challenges/VOC/(107)机器视觉开源处理库汇总：http://archive.cnblogs.com/a/2217609/(108)布朗大学教授Benjamin Kimia: http://www.lems.brown.edu/kimia.html(109)数据堂-图像处理相关的样本数据：http://www.datatang.com/data/list/602026/p1(110)东软基于CV的汽车辅助驾驶系统：http://www.neusoft.com/cn/solutions/1047/(111)马里兰大学教授Rema Chellappa:http://www.cfar.umd.edu/~rama/(112)芝加哥丰田研究中心助理教授Devi Parikh：http://ttic.uchicago.edu/~dparikh/index.html(113)宾夕法尼亚大学助理教授石建波：http://www.cis.upenn.edu/~jshi/(114)比利时鲁汶大学教授Luc Van Gool：http://www.vision.ee.ethz.ch/members/get_member.cgi?id=1, http://www.vision.ee.ethz.ch/~vangool/(115)行人检测主页：http://www.pedestrian-detection.com/(116)法国学习算法与系统实验室Basilio Noris博士：http://lasa.epfl.ch/people/member.php?SCIPER=129576 http://mldemos.epfl.ch/(117)美国马里兰大学LARRY S.DAVIS教授：http://www.umiacs.umd.edu/~lsd/(118)计算机视觉论文分类导航：http://www.visionbib.com/bibliography/contents.html(119)计算机视觉分类信息导航：http://www.visionbib.com/(120)西班牙马德里理工大学博士Marcos Nieto：http://marcosnieto.net/(121)香港理工大学副教授张磊：http://www4.comp.polyu.edu.hk/~cslzhang/(122)以色列技术学院教授Michael Elad：http://www.cs.technion.ac.il/~elad/(123)韩国启明大学计算机视觉与模式识别实验室：http://cvpr.kmu.ac.kr/(124)英国诺丁汉大学Michel Valstar博士：http://www.cs.nott.ac.uk/~mfv/(125)卡内基梅隆大学Takeo Kanade教授:http://www.ri.cmu.edu/people/kanade_takeo.html(126)微软学术搜索：http://libra.msra.cn/(127)比利时天主教鲁汶大学Radu Timofte博士：http://homes.esat.kuleuven.be/~rtimofte/，交通标志检测，定位，3D跟踪(128)迪斯尼匹兹堡研究院研究员：Iain Matthews:http://www.iainm.com/iainm/Home.htmlhttp://www.ri.cmu.edu/person.html?type= ... son_id=741 AAM,三维重建（129）康奈尔大学视觉与图像分析组：http://www.via.cornell.edu/ 医学图像处理（130）密西根州立大学生物识别研究组：http://www.cse.msu.edu/biometrics/ 人脸识别、指纹识别、图像检索（131）柏林科技大学计算机视觉与遥感实验室：http://www.cv.tu-berlin.de/menue/computer_vision_remote_sensing/parameter/en/ 图像分析、物体重建、基于图像的表面测量、医学图像处理（132）英国布里斯托大学数字多媒体研究组：http://www.cs.bris.ac.uk/Research/Digitalmedia/ 运动检测与跟踪、视频压缩、3D重建、字符定位（133）英国萨利大学视觉、语音与信号处理中心： http://www.surrey.ac.uk/cvssp/ 人脸识别、监控、3D、视频检索、（134）北卡莱罗纳大学教堂山分校Marc Pollefeys教授：http://www.cs.unc.edu/~marc/ 基于视频的3D模型生成、相机标定、运动检测与分析、3D重建（135）澳大利亚国立大学Richard Hartley教授：http://users.cecs.anu.edu.au/~hartley/ 运动估计、稀疏子空间、跟踪、（136）百度技术副总监于凯：http://www.dbs.ifi.lmu.de/~yu_k/ 深度学习，稀疏表示，图像分类（137）西安电子科技大学高新波教授：http://web.xidian.edu.cn/xbgao/index.html 质量评判、水印、稀疏表示、超分辨率（138）加州大学伯克利分校Michael I.Jordan教授：http://www.cs.berkeley.edu/~jordan/ 机器学习（139）加州理工行人检测相关资料：http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/（140）微软Redmond研究院研究员Piotr Dollar: http://vision.ucsd.edu/~pdollar/ 行人检测、特征提取、（141）视觉计算研究论坛：http://www.sigvc.org/bbs/ 中科院视觉计算研究小组的论坛（142）美国坦桑尼亚州立大学稀疏学习软件包：http://www.public.asu.edu/~jye02/Software/SLEP/index.htm 稀疏学习（143）美国加州大学圣地亚哥分校Jacob Whitehill博士：http://mplab.ucsd.edu/~jake/ 机器学习（144）美国布朗大学Michael J.Black教授：http://cs.brown.edu/~black/ 人的姿态估计和跟踪（145）美国加州大学圣地亚哥分校David Kriegman教授：http://cseweb.ucsd.edu/~kriegman/ 人脸识别（146）南加州大学Paul Debevec教授：http://ict.debevec.org/~debevec/ 或 http://www.pauldebevec.com/ 将CV和CG结合研究 人脸捕捉重建技术（147）伊利诺伊大学D.A.Forsyth教授：http://luthuli.cs.uiuc.edu/~daf/ 三维重建（148）英国牛津大学Ian Reid教授：http://www.robots.ox.ac.uk/~ian/ 跟踪和机器人导航（149）CMU大学Alyosha Efros 教授: https://www.cs.cmu.edu/~efros/ 图像纹理合成（150）加州大学伯克利分校Jitendra Malik教授：http://www.cs.berkeley.edu/~malik/ 轮廓检测、图像/视频分割、图形匹配、目标识别（151）MIT教授William Freeman： http://people.csail.mit.edu/billf/ 应用于CV的ML、可视化感知的贝叶斯模型、计算摄影学；最有影响力的研究成果：图像纹理合成（152）CMU博士Henry Schneiderman： http://www.cs.cmu.edu/~hws/ 目标检测和识别；最有影响力的研究成果：目标检测；2000年CVPR上发表了”A statistical method for 3D object detection applied to faces and cars”。该算法采用多视角训练样本，可用于检测不同视角下的物体，如人脸和车，是第一个能够检测侧脸的算法。他创建了PittPatt公司，后被Google收购（153）微软研究员Paul Viola: http://research.microsoft.com/en-us/um/people/viola/ AdaBoost算法（154）微软研究员Antonio Criminisi: http://research.microsoft.com/en-us/people/antcrim/ 图像修补，三维重建，目标检测与跟踪；（155）魏茨曼科学研究所教授Michal Irani: http://www.wisdom.weizmann.ac.il/~irani/ 超分辨率（156）瑞士洛桑理工学院Pascal Fua教授：http://people.epfl.ch/pascal.fua/bio?lang=en 立体视觉，增强现实（157）佐治亚理工学院Irfan Essa教授：http://www.ic.gatech.edu/people/irfan-essa 人脸表情识别整理的内容不见得完善，也不见得能满足所有朋友的需要。如果您有更好的网站资源，欢迎推荐给我。另外，如果你不想记录所有这些链接，也可以去我的博客，所有这些网址在我的博客都有链接。而且，以后我还会不断更新！我的博客是：http://blog.csdn.net/carson2005"}
{"content2":"一、Mask-RCNN流程Mask R-CNN是一个实例分割（Instance segmentation）算法，通过增加不同的分支，可以完成目标分类、目标检测、语义分割、实例分割、人体姿势识别等多种任务，灵活而强大。Mask R-CNN进行目标检测与实例分割Mask R-CNN进行人体姿态识别其抽象架构如下：首先，输入一幅你想处理的图片，然后进行对应的预处理操作，或者预处理后的图片；然后，将其输入到一个预训练好的神经网络中（ResNeXt等）获得对应的feature map；接着，对这个feature map中的每一点设定预定个的ROI，从而获得多个候选ROI；接着，将这些候选的ROI送入RPN网络进行二值分类（前景或背景）和BB回归，过滤掉一部分候选的ROI（截止到目前，Mask和Faster完全相同，其实R-FCN之类的在这之前也没有什么不同）；接着，对这些剩下的ROI进行ROIAlign操作（即先将原图和feature map的pixel对应起来，然后将feature map和固定的feature对应起来）（ROIAlign为本文创新点1，比ROIPooling有长足进步）；最后，对这些ROI进行分类（N类别分类）、BB回归和MASK生成（在每一个ROI里面进行FCN操作）（引入FCN生成Mask为本文创新点2，使得本文结构可以进行分割型任务）。【注】有关MASK部分，还有一处容易忽视的创新点3：损失函数的计算，作者放弃了更广泛的softmax，转而使用了sigmoid，避免了同类竞争，更多的经历放在优化mask像素上，这一点我们下一小节会提到。二、Mask-RCNN结构ROIPooling的问题RoiPool过程假定我们输入的是一张800x800的图像，在图像中有两个目标（猫和狗），狗的BB大小为665x665，经过VGG16网络后，获得的feature map 会比原图缩小一定的比例，这和Pooling层的个数和大小有关：在该VGG16中，我们使用了5个池化操作，每个池化操作都是2Pooling，因此我们最终获得feature map的大小为800/32 x 800/32 = 25x25（是整数），但是将狗的BB对应到feature map上面，我们得到的结果是665/32 x 665/32 = 20.78 x 20.78，结果是浮点数，含有小数，取整变为20 x 20，在这里引入了第一次的量化误差；然后我们需要将20 x 20的ROI映射成7 x 7的ROI feature，其结果是 20 /7 x 20/7 = 2.86 x 2.86，同样是浮点数，含有小数点，同样的取整，在这里引入了第二次量化误差。这里引入的误差会导致图像中的像素和特征中的像素的偏差，即将feature空间的ROI对应到原图上面会出现很大的偏差。原因如下：比如用我们第二次引入的误差来分析，本来是2,86，我们将其量化为2，这期间引入了0.86的feature空间误差，我们的feature空间和图像空间是有比例关系的，在这里是1:32，那么对应到原图上面的差距就是0.86 x 32 = 27.52（这仅仅考虑了第二次的量化误差）。ROIAlignROIAlign过程为了得到为了得到固定大小（7X7）的feature map，ROIAlign技术并没有使用量化操作，取而代之的使用了双线性插值，它充分的利用了原图中虚拟点（比如20.56这个浮点数，像素位置都是整数值，没有浮点值）四周的四个真实存在的像素值来共同决定目标图中的一个像素值，即可以将20.56这个虚拟的位置点对应的像素值估计出来。蓝色的虚线框表示卷积后获得的feature map，黑色实线框表示ROI feature，最后需要输出的大小是2x2，那么我们就利用双线性插值来估计这些蓝点（虚拟坐标点，又称双线性插值的网格点）处所对应的像素值，最后得到相应的输出。然后在每一个橘红色的区域里面进行max pooling或者average pooling操作，获得最终2x2的输出结果。我们的整个过程中没有用到量化操作，没有引入误差，即原图中的像素和feature map中的像素是完全对齐的，没有偏差，这不仅会提高检测的精度，同时也会有利于实例分割。ROI处理架构为了证明我们方法的通用性，我们构造了多种不同结构的Mask R-CNN。详细的说，我们使用不同的：（i）用于整个图像上的特征提取的卷积主干架构；（ii）用于边框识别（分类和回归）和掩模预测的上层网络，分别应用于每个RoI。我们使用术语“网络深层特征”来命名下层架构。我们评估了深度为50或101层的ResNet [14]和ResNeXt [34] 网络。使用ResNet [14]的Faster R-CNN从第四级的最终卷积层提取特征，我们称之为C4。例如，使用ResNet-50的主干架构由ResNet-50-C4表示。这是[14,7,16,30]中常用的选择。我们也探索了由Li[21]等人最近提出的另一种更有效主干架构，称为特征金字塔网络（FPN）。FPN使用具有横向连接（lateral connections ）的自顶向下架构，从单一规模的输入构建网络功能金字塔。使用FPN的Faster R-CNN根据其尺度提取不同级别的金字塔的RoI特征，不过其他部分和平常的ResNet类似。使用ResNet-FPN主干架构的Mask R-CNN进行特征提取，可以在精度和速度方面获得极大的提升。有关FPN的更多细节，读者可以参考[21]。对于上层网络，我们基本遵循了以前论文中提出的架构，我们添加了一个全卷积的掩模预测分支。具体来说，我们扩展了ResNet [14]和FPN[21]中提出的Faster R-CNN的上层网络。详情见下图（）所示：（上层架构：我们扩展了两种现有的Faster R-CNN上层架构[14,21]，并分别添加了一个掩模分支。左/右面板分别显示了ResNet C4和FPN主干的上层架构。图中数字表示通道数和分辨率，箭头表示卷积、反卷积和全连接层（可以通过上下文推断，卷积减小维度，反卷积增加维度。）所有的卷积都是3×3的，除了输出层是1×1。反卷积是2×2，其步进为2，我们在隐藏层中使用ReLU[24]。在左图中，“res5”表示ResNet的第五级，简单起见，我们修改了第一个卷积操作，使用7×7，步长为1的RoI代替14×14，步长为2的RoI[14]。右图中的“×4 ”表示堆叠的4个连续的卷积。）ResNet-C4主干的上层网络包括ResNet的第5阶段（即9层的’res5’[14]），这是计算密集型的。但对于FPN，其主干已经包含了res5，因此可以使上层网络包含更少的卷积核而变的更加高效。重点在于：作者把各种网络作为backbone进行对比，发现使用ResNet-FPN作为特征提取的backbone具有更高的精度和更快的运行速度，所以实际工作时大都采用右图的完全并行的mask/分类回归mask分支针对每个RoI产生一个K*m*m的输出，即K个分辨率为m*m的二值的掩膜，K为分类物体的种类数目。依据预测类别分支预测的输出，我们仅将第i个类别的输出登记，用于计算Mask R-CNN采用了和Faster R-CNN相同的两步走策略，即先使用RPN提取候选区域，关于RPN的详细介绍，可以参考Faster R-CNN一文。不同于Faster R-CNN中使用分类和回归的多任务回归，Mask R-CNN在其基础上并行添加了一个用于语义分割的Mask损失函数，所以Mask R-CNN的损失函数可以表示为下式。上式中， 表示bounding box的分类损失值， 表示bounding box的回归损失值， 表示mask部分的损失值。对于预测的二值掩膜输出，我们对每个像素点应用sigmoid函数，整体损失定义为平均二值交叉损失熵。引入预测K个输出的机制，允许每个类都生成独立的掩膜，避免类间竞争。这样做解耦了掩膜和种类预测。不像FCN的做法，在每个像素点上应用softmax函数，整体采用的多任务交叉熵，这样会导致类间竞争，最终导致分割效果差。训练参数面试考点这一部分是临时看到面经，感觉还是有不少RCNN系列发展的要点，记录下来，以后再看RCNN时可以带着问题回忆。RPN 作用和原理RPN提出于Faster，所以参考之前的文章：『计算机视觉』经典RCNN_其一：从RCNN到Faster-RCNNROI align 和 ROI pooling 的不同各部分损失函数选用的什么函数参考后续文章：『计算机视觉』Mask-RCNN_训练网络其二：train网络结构&损失函数三、参考资料Mask R-CNNMask R-CNN详解TensorFlow实战：Chapter-8上(Mask R-CNN介绍与实现)开源代码：Tensorflow版本代码链接；Keras and TensorFlow版本代码链接；MxNet版本代码链接"}
{"content2":"原帖：http://blog.csdn.net/zouxy09/article/details/8550952版权归原作者所有！计算机视觉、机器学习相关领域论文和源代码大集合--持续更新……zouxy09@qq.comhttp://blog.csdn.net/zouxy09注：下面有project网站的大部分都有paper和相应的code。Code一般是C/C++或者Matlab代码。最近一次更新：2013-3-17一、特征提取Feature Extraction：·         SIFT [1] [Demo program][SIFT Library] [VLFeat]·         PCA-SIFT [2] [Project]·         Affine-SIFT [3] [Project]·         SURF [4] [OpenSURF] [Matlab Wrapper]·         Affine Covariant Features [5] [Oxford project]·         MSER [6] [Oxford project] [VLFeat]·         Geometric Blur [7] [Code]·         Local Self-Similarity Descriptor [8] [Oxford implementation]·         Global and Efficient Self-Similarity [9] [Code]·         Histogram of Oriented Graidents [10] [INRIA Object Localization Toolkit] [OLT toolkit for Windows]·         GIST [11] [Project]·         Shape Context [12] [Project]·         Color Descriptor [13] [Project]·         Pyramids of Histograms of Oriented Gradients [Code]·         Space-Time Interest Points (STIP) [14][Project] [Code]·         Boundary Preserving Dense Local Regions [15][Project]·         Weighted Histogram[Code]·         Histogram-based Interest Points Detectors[Paper][Code]·         An OpenCV - C++ implementation of Local Self Similarity Descriptors [Project]·         Fast Sparse Representation with Prototypes[Project]·         Corner Detection [Project]·         AGAST Corner Detector: faster than FAST and even FAST-ER[Project]·         Real-time Facial Feature Detection using Conditional Regression Forests[Project]·         Global and Efficient Self-Similarity for Object Classification and Detection[code]·         WαSH: Weighted α-Shapes for Local Feature Detection[Project]·         HOG[Project]·         Online Selection of Discriminative Tracking Features[Project]二、图像分割Image Segmentation：·           Normalized Cut [1] [Matlab code]·           Gerg Mori’ Superpixel code [2] [Matlab code]·           Efficient Graph-based Image Segmentation [3] [C++ code] [Matlab wrapper]·           Mean-Shift Image Segmentation [4] [EDISON C++ code] [Matlab wrapper]·           OWT-UCM Hierarchical Segmentation [5] [Resources]·           Turbepixels [6] [Matlab code 32bit] [Matlab code 64bit] [Updated code]·           Quick-Shift [7] [VLFeat]·           SLIC Superpixels [8] [Project]·           Segmentation by Minimum Code Length [9] [Project]·           Biased Normalized Cut [10] [Project]·           Segmentation Tree [11-12] [Project]·           Entropy Rate Superpixel Segmentation [13] [Code]·           Fast Approximate Energy Minimization via Graph Cuts[Paper][Code]·           Efﬁcient Planar Graph Cuts with Applications in Computer Vision[Paper][Code]·           Isoperimetric Graph Partitioning for Image Segmentation[Paper][Code]·           Random Walks for Image Segmentation[Paper][Code]·           Blossom V: A new implementation of a minimum cost perfect matching algorithm[Code]·           An Experimental Comparison of Min-Cut/Max-Flow Algorithms for Energy Minimization in Computer Vision[Paper][Code]·           Geodesic Star Convexity for Interactive Image Segmentation[Project]·           Contour Detection and Image Segmentation Resources[Project][Code]·           Biased Normalized Cuts[Project]·           Max-flow/min-cut[Project]·           Chan-Vese Segmentation using Level Set[Project]·           A Toolbox of Level Set Methods[Project]·           Re-initialization Free Level Set Evolution via Reaction Diffusion[Project]·           Improved C-V active contour model[Paper][Code]·           A Variational Multiphase Level Set Approach to Simultaneous Segmentation and Bias Correction[Paper][Code]·          Level Set Method Research by Chunming Li[Project]·          ClassCut for Unsupervised Class Segmentation[code]·         SEEDS: Superpixels Extracted via Energy-Driven Sampling [Project][other]三、目标检测Object Detection：·           A simple object detector with boosting [Project]·           INRIA Object Detection and Localization Toolkit [1] [Project]·           Discriminatively Trained Deformable Part Models [2] [Project]·           Cascade Object Detection with Deformable Part Models [3] [Project]·           Poselet [4] [Project]·           Implicit Shape Model [5] [Project]·           Viola and Jones’s Face Detection [6] [Project]·           Bayesian Modelling of Dyanmic Scenes for Object Detection[Paper][Code]·           Hand detection using multiple proposals[Project]·           Color Constancy, Intrinsic Images, and Shape Estimation[Paper][Code]·           Discriminatively trained deformable part models[Project]·           Gradient Response Maps for Real-Time Detection of Texture-Less Objects: LineMOD [Project]·           Image Processing On Line[Project]·           Robust Optical Flow Estimation[Project]·           Where's Waldo: Matching People in Images of Crowds[Project]·           Scalable Multi-class Object Detection[Project]·           Class-Specific Hough Forests for Object Detection[Project]·         Deformed Lattice Detection In Real-World Images[Project]·         Discriminatively trained deformable part models[Project]四、显著性检测Saliency Detection：·           Itti, Koch, and Niebur’ saliency detection [1] [Matlab code]·           Frequency-tuned salient region detection [2] [Project]·           Saliency detection using maximum symmetric surround [3] [Project]·           Attention via Information Maximization [4] [Matlab code]·           Context-aware saliency detection [5] [Matlab code]·           Graph-based visual saliency [6] [Matlab code]·           Saliency detection: A spectral residual approach. [7] [Matlab code]·           Segmenting salient objects from images and videos. [8] [Matlab code]·           Saliency Using Natural statistics. [9] [Matlab code]·           Discriminant Saliency for Visual Recognition from Cluttered Scenes. [10] [Code]·           Learning to Predict Where Humans Look [11] [Project]·           Global Contrast based Salient Region Detection [12] [Project]·           Bayesian Saliency via Low and Mid Level Cues[Project]·           Top-Down Visual Saliency via Joint CRF and Dictionary Learning[Paper][Code]·         Saliency Detection: A Spectral Residual Approach[Code]五、图像分类、聚类Image Classification, Clustering·           Pyramid Match [1] [Project]·           Spatial Pyramid Matching [2] [Code]·           Locality-constrained Linear Coding [3] [Project] [Matlab code]·           Sparse Coding [4] [Project] [Matlab code]·           Texture Classification [5] [Project]·           Multiple Kernels for Image Classification [6] [Project]·           Feature Combination [7] [Project]·           SuperParsing [Code]·           Large Scale Correlation Clustering Optimization[Matlab code]·           Detecting and Sketching the Common[Project]·           Self-Tuning Spectral Clustering[Project][Code]·           User Assisted Separation of Reflections from a Single Image Using a Sparsity Prior[Paper][Code]·           Filters for Texture Classification[Project]·           Multiple Kernel Learning for Image Classification[Project]·          SLIC Superpixels[Project]六、抠图Image Matting·           A Closed Form Solution to Natural Image Matting [Code]·           Spectral Matting [Project]·           Learning-based Matting [Code]七、目标跟踪Object Tracking：·           A Forest of Sensors - Tracking Adaptive Background Mixture Models [Project]·           Object Tracking via Partial Least Squares Analysis[Paper][Code]·           Robust Object Tracking with Online Multiple Instance Learning[Paper][Code]·           Online Visual Tracking with Histograms and Articulating Blocks[Project]·           Incremental Learning for Robust Visual Tracking[Project]·           Real-time Compressive Tracking[Project]·           Robust Object Tracking via Sparsity-based Collaborative Model[Project]·           Visual Tracking via Adaptive Structural Local Sparse Appearance Model[Project]·           Online Discriminative Object Tracking with Local Sparse Representation[Paper][Code]·           Superpixel Tracking[Project]·           Learning Hierarchical Image Representation with Sparsity, Saliency and Locality[Paper][Code]·           Online Multiple Support Instance Tracking [Paper][Code]·           Visual Tracking with Online Multiple Instance Learning[Project]·           Object detection and recognition[Project]·           Compressive Sensing Resources[Project]·           Robust Real-Time Visual Tracking using Pixel-Wise Posteriors[Project]·           Tracking-Learning-Detection[Project][OpenTLD/C++ Code]·           the HandVu：vision-based hand gesture interface[Project]·           Learning Probabilistic Non-Linear Latent Variable Models for Tracking Complex Activities[Project]八、Kinect：·           Kinect toolbox[Project]·           OpenNI[Project]·           zouxy09 CSDN Blog[Resource]·           FingerTracker 手指跟踪[code]九、3D相关：·           3D Reconstruction of a Moving Object[Paper] [Code]·           Shape From Shading Using Linear Approximation[Code]·           Combining Shape from Shading and Stereo Depth Maps[Project][Code]·           Shape from Shading: A Survey[Paper][Code]·           A Spatio-Temporal Descriptor based on 3D Gradients (HOG3D)[Project][Code]·           Multi-camera Scene Reconstruction via Graph Cuts[Paper][Code]·           A Fast Marching Formulation of Perspective Shape from Shading under Frontal Illumination[Paper][Code]·           Reconstruction:3D Shape, Illumination, Shading, Reflectance, Texture[Project]·           Monocular Tracking of 3D Human Motion with a Coordinated Mixture of Factor Analyzers[Code]·           Learning 3-D Scene Structure from a Single Still Image[Project]十、机器学习算法：·           Matlab class for computing Approximate Nearest Nieghbor (ANN) [Matlab class providing interface toANN library]·           Random Sampling[code]·           Probabilistic Latent Semantic Analysis (pLSA)[Code]·           FASTANN and FASTCLUSTER for approximate k-means (AKM)[Project]·           Fast Intersection / Additive Kernel SVMs[Project]·           SVM[Code]·           Ensemble learning[Project]·           Deep Learning[Net]·           Deep Learning Methods for Vision[Project]·           Neural Network for Recognition of Handwritten Digits[Project]·           Training a deep autoencoder or a classifier on MNIST digits[Project]·          THE MNIST DATABASE of handwritten digits[Project]·          Ersatz：deep neural networks in the cloud[Project]·          Deep Learning [Project]·          sparseLM : Sparse Levenberg-Marquardt nonlinear least squares in C/C++[Project]·          Weka 3: Data Mining Software in Java[Project]·          Invited talk \"A Tutorial on Deep Learning\" by Dr. Kai Yu (余凯)[Video]·          CNN - Convolutional neural network class[Matlab Tool]·          Yann LeCun's Publications[Wedsite]·          LeNet-5, convolutional neural networks[Project]·          Training a deep autoencoder or a classifier on MNIST digits[Project]·          Deep Learning 大牛Geoffrey E. Hinton's HomePage[Website]·         Multiple Instance Logistic Discriminant-based Metric Learning (MildML) and Logistic Discriminant-based Metric Learning (LDML)[Code]·         Sparse coding simulation software[Project]·         Visual Recognition and Machine Learning Summer School[Software]十一、目标、行为识别Object, Action Recognition：·           Action Recognition by Dense Trajectories[Project][Code]·           Action Recognition Using a Distributed Representation of Pose and Appearance[Project]·           Recognition Using Regions[Paper][Code]·           2D Articulated Human Pose Estimation[Project]·           Fast Human Pose Estimation Using Appearance and Motion via Multi-Dimensional Boosting Regression[Paper][Code]·           Estimating Human Pose from Occluded Images[Paper][Code]·           Quasi-dense wide baseline matching[Project]·           ChaLearn Gesture Challenge: Principal motion: PCA-based reconstruction of motion histograms[Project]·           Real Time Head Pose Estimation with Random Regression Forests[Project]·           2D Action Recognition Serves 3D Human Pose Estimation[Project]·           A Hough Transform-Based Voting Framework for Action Recognition[Project]·           Motion Interchange Patterns for Action Recognition in Unconstrained Videos[Project]·         2D articulated human pose estimation software[Project]·         Learning and detecting shape models [code]·         Progressive Search Space Reduction for Human Pose Estimation[Project]·         Learning Non-Rigid 3D Shape from 2D Motion[Project]十二、图像处理：·         Distance Transforms of Sampled Functions[Project]·         The Computer Vision Homepage[Project]·         Efficient appearance distances between windows[code]·         Image Exploration algorithm[code]·         Motion Magnification 运动放大 [Project]·         Bilateral Filtering for Gray and Color Images 双边滤波器 [Project]·         A Fast Approximation of the Bilateral Filter using a Signal Processing Approach [Project]十三、一些实用工具：·           EGT: a Toolbox for Multiple View Geometry and Visual Servoing[Project] [Code]·           a development kit of matlab mex functions for OpenCV library[Project]·           Fast Artificial Neural Network Library[Project]十四、人手及指尖检测与识别：·           finger-detection-and-gesture-recognition [Code]·           Hand and Finger Detection using JavaCV[Project]·           Hand and fingers detection[Code]十五、场景解释：·           Nonparametric Scene Parsing via Label Transfer [Project]十六、光流Optical flow：·         High accuracy optical flow using a theory for warping [Project]·         Dense Trajectories Video Description [Project]·         SIFT Flow: Dense Correspondence across Scenes and its Applications[Project]·         KLT: An Implementation of the Kanade-Lucas-Tomasi Feature Tracker [Project]·         Tracking Cars Using Optical Flow[Project]·         Secrets of optical flow estimation and their principles[Project]·         implmentation of the Black and Anandan dense optical flow method[Project]·         Optical Flow Computation[Project]·         Beyond Pixels: Exploring New Representations and Applications for Motion Analysis[Project]·         A Database and Evaluation Methodology for Optical Flow[Project]·         optical flow relative[Project]·         Robust Optical Flow Estimation [Project]·         optical flow[Project]十七、图像检索Image Retrieval：·           Semi-Supervised Distance Metric Learning for Collaborative Image Retrieval [Paper][code]十八、马尔科夫随机场Markov Random Fields：·         Markov Random Fields for Super-Resolution [Project]·         A Comparative Study of Energy Minimization Methods for Markov Random Fields with Smoothness-Based Priors [Project]十九、运动检测Motion detection：·         Moving Object Extraction, Using Models or Analysis of Regions [Project]·         Background Subtraction: Experiments and Improvements for ViBe [Project]·         A Self-Organizing Approach to Background Subtraction for Visual Surveillance Applications [Project]·         changedetection.net: A new change detection benchmark dataset[Project]·         ViBe - a powerful technique for background detection and subtraction in video sequences[Project]·         Background Subtraction Program[Project]·         Motion Detection Algorithms[Project]·         Stuttgart Artificial Background Subtraction Dataset[Project]·         Object Detection, Motion Estimation, and Tracking[Project]"}
{"content2":"转：http://mp.weixin.qq.com/s?__biz=MzAwNDExMTQwNQ==&mid=209152042&idx=1&sn=fa0053e66cad3d2f7b107479014d4478#rd#opennewwindow1、深度学习发展历史深度学习是近十年来人工智能领域取得的重要突破。它在语音识别、自然语言处理、计算机视觉、图像与视频分析、多媒体等诸多领域的应用取得了巨大成功。现有的深度学习模型属于神经网络。神经网络的起源可追溯到20世纪40年代，曾经在八九十年代流行。神经网络试图通过模拟大脑认知的机理解决各种机器学习问题。1986年，鲁梅尔哈特(Rumelhart)、欣顿(Hinton)和威廉姆斯(Williams)在《自然》杂志发表了著名的反向传播算法用于训练神经网络，该算法直到今天仍被广泛应用。神经网络有大量参数，经常发生过拟合问题，虽然其识别结果在训练集上准确率很高，但在测试集上效果却很差。这是因为当时的训练数据集规模都较小，加之计算资源有限，即便是训练一个较小的网络也需要很长的时间。与其他模型相比，神经网络并未在识别准确率上体现出明显的优势。因此更多的学者开始采用支持向量机、Boosting、最近邻等分类器。这些分类器可以用具有一个或两个隐含层的神经网络模拟，因此被称为浅层机器学习模型。在这种模型中，往往是针对不同的任务设计不同的系统，并采用不同的手工设计的特征。例物体识别采用尺度不变特征转换(Scale Invariant Feature Transform, SIFT)，人脸识别采用局部二值模式(Local Binary Patterns, LBP)，行人检测采用方向梯度直方图(Histogram of Oriented Gradient, HOG)特征。2006年，欣顿提出了深度学习。之后深度学习在诸多领域取得了巨大成功，受到广泛关注。神经网络能够重新焕发青春的原因有几个方面：首先，大规模训练数据的出现在很大程度上缓解了训练过拟合的问题。例如，ImageNet训练集拥有上百万个有标注的图像。其次，计算机硬件的飞速发展为其提供了强大的计算能力，一个GPU芯片可以集成上千个核。这使得训练大规模神经网络成为可能。第三，神经网络的模型设计和训练方法都取得了长足的进步。例如，为了改进神经网络的训练，学者提出了非监督和逐层的预训练，使得在利用反向传播算法对网络进行全局优化之前，网络参数能达到一个好的起始点，从而在训练完成时能达到一个较好的局部极小点。深度学习在计算机视觉领域最具影响力的突破发生在2012年，欣顿的研究小组采用深度学习赢得了ImageNet图像分类比赛的冠军。排名第2到第4位的小组采用的都是传统的计算机视觉方法、手工设计的特征，他们之间准确率的差别不超过1%。欣顿研究小组的准确率超出第二名10%以上，（见表1）。这个结果在计算机视觉领域产生了极大的震动，引发了深度学习的热潮。计算机视觉领域另一个重要的挑战是人脸识别。有研究表明，如果只把不包括头发在内的人脸的中心区域给人看，人眼在户外脸部检测数据库(Labeled Faces in the Wild, LFW)上的识别率是97.53%。如果把整张图像，包括背景和头发给人看，人眼的识别率是99.15%。经典的人脸识别算法Eigenface 在LFW测试集上只有60%的识别率。在非深度学习算法中，最高的识别率是96.33% 。目前深度学习可以达到99.47%的识别率。欣顿的科研小组赢得ImageNet比赛冠军之后的6个月，谷歌和百度都发布了新的基于图像内容的搜索引擎。他们采用深度学习模型，应用在各自的数据上，发现图像搜索准确率得到了大幅度提高。百度在2012年成立了深度学习研究院，2014年5月又在美国硅谷成立了新的深度学习实验室，聘请斯坦福大学著名教授吴恩达担任首席科学家。脸谱于2013年12月在纽约成立了新的人工智能实验室，聘请深度学习领域的著名学者、卷积网络的发明人雅恩•乐昆(Yann LeCun)作为首席科学家。2014年1月，谷歌抛出四亿美金收购了深度学习的创业公司DeepMind。鉴于深度学习在学术界和工业界的巨大影响力，2013年，《麻省理工科技评论》(MIT Technology Review)将其列为世界十大技术突破之首。2、深度学习有何与众不同？深度学习和其他机器学习方法相比有哪些关键的不同点，它为何能在许多领域取得成功？•        特征学习深度学习与传统模式识别方法的最大不同在于它所采用的特征是从大数据中自动学习得到，而非采用手工设计。好的特征可以提高模式识别系统的性能。过去几十年，在模式识别的各种应用中，手工设计的特征一直处于统治地位。手工设计主要依靠设计者的先验知识，很难利用大数据的优势。由于依赖手工调参数，因此特征的设计中所允许出现的参数数量十分有限。深度学习可以从大数据中自动学习特征的表示，可以包含成千上万的参数。采用手工设计出有效的特征往往需要五到十年时间，而深度学习可以针对新的应用从训练数据中很快学习到新的有效的特征表示。一个模式识别系统包括特征和分类器两部分。在传统方法中，特征和分类器的优化是分开的。而在神经网络的框架下，特征表示和分类器是联合优化的，可以最大程度地发挥二者联合协作的性能。2012年欣顿参加ImageNet比赛所采用的卷积网络模型[9]的特征表示包含了从上百万样本中学习得到的6000万个参数。从ImageNet上学习得到的特征表示具有非常强的泛化能力，可以成功应用到其他数据集和任务中，例如物体的检测、跟踪和检索等。在计算机视觉领域另外一个著名的竞赛是PSACAL VOC。但是它的训练集规模较小，不适合训练深度学习模型。有学者将ImageNet上学习得到的特征表示用于PSACAL VOC上的物体检测，检测率提高了20%。既然特征学习如此重要，那么，什么是好的特征呢？一幅图像中，各种复杂的因素往往以非线性的方式结合在一起。例如人脸图像中就包含了身份、姿态、年龄、表情、光线等各种信息。深度学习的关键就是通过多层非线性映射将这些因素成功分开，例如在深度模型的最后一个隐含层，不同神经元代表了不同因素。如果将这个隐含层当作特征表示，人脸识别、姿态估计、表情识别、年龄估计就会变得非常简单，因为各个因素之间变成了简单的线性关系，不再彼此干扰。•        深层结构的优势深度学习模型的“深”字意味着神经网络的结构深，由很多层组成。而支持向量机和Boosting等其他常用的机器学习模型都是浅层结构。三层神经网络模型（包括输入层、输出层和一个隐含层）可以近似任何分类函数。既然如此，为什么需要深层模型呢？研究表明，针对特定的任务，如果模型的深度不够，其所需要的计算单元会呈指数增加。这意味着虽然浅层模型可以表达相同的分类函数，但其需要的参数和训练样本要多得多。浅层模型提供的是局部表达。它将高维图像空间分成若干个局部区域，每个局部区域至少存储一个从训练数据中获得的模板，如(a)所示。浅层模型将一个测试样本和这些模板逐一匹配，根据匹配的结果预测其类别。例如，在支持向量机模型中，模板是支持向量；在最近邻分类器中，模板是所有的训练样本。随着分类问题复杂度的增加，需要将图像空间划分成越来越多的局部区域，因而需要越来越多的参数和训练样本。尽管目前许多深度模型的参数量已经相当巨大，但如果换成浅层神经网络，其所需要的参数量要大出多个数量级才能达到相同的数据拟合效果，以至于很难实现。深度模型之所以能减少参数的关键在于重复利用中间层的计算单元。以人脸识别为例，深度学习可以针对人脸图像的分层特征表达进行：最底层从原始像素开始学习滤波器，刻画局部的边缘和纹理特征；中层滤波器通过将各种边缘滤波器进行组合，描述不同类型的人脸器官；最高层描述的是整个人脸的全局特征。深度学习提供的是分布式的特征表示。在最高的隐含层，每个神经元代表一个属性分类器（如(b)所示），例如性别、人种和头发颜色等。每个神经元将图像空间一分为二，N个神经元的组合就可以表达2N个局部区域，而用浅层模型表达这些区域的划分至少需要2N个模板。由此可以看出，深度模型的表达能力更强，效率更高。•        提取全局特征和上下文信息的能力深度模型具有强大的学习能力和高效的特征表达能力，更重要的优点是从像素级原始数据到抽象的语义概念逐层提取信息，这使得它在提取图像的全局特征和上下文信息方面具有突出的优势，为解决传统的计算机视觉问题（如图像分割和关键点检测）带来了新的思路。以人脸的图像分割为例（如所示），为了预测每个像素属于哪个脸部器官（眼睛、鼻子、嘴），通常的做法是在该像素周围取一个小区域，提取纹理特征（例如局部二值模式），再基于该特征利用支持向量机等浅层模型分类。因为局部区域包含的信息量有限，往往产生分类错误，因此要对分割后的图像加入平滑和形状先验等约束。人眼即使在存在局部遮挡的情况下也可以根据脸部其他区域的信息估计被遮挡部分的标注。由此可知全局和上下文信息对于局部的判断是非常重要的，而这些信息在基于局部特征的方法中在最开始阶段就丢失了。理想情况下，模型应该将整幅图像作为输入，直接预测整幅分割图。图像分割可以被看做一个高维数据转换的问题来解决。这样不但利用到了上下文信息，模型在高维数据转换过程中也隐式地加入了形状先验。但是由于整幅图像内容过于复杂，浅层模型很难有效地捕捉全局特征。而深度学习的出现使这一思路成为可能，在人脸分割、人体分割、人脸图像配准和人体姿态估计等各个方面都取得了成功。•        联合深度学习一些研究计算机视觉的学者将深度学习模型视为黑盒子，这种看法是不全面的。传统计算机视觉系统和深度学习模型存在着密切的联系，利用这种联系可以提出新的深度模型和训练方法。用于行人检测的联合深度学习就是一个成功的例子。一个计算机视觉系统包含若干个关键的组成模块。例如，一个行人检测器包括特征提取、部件检测器、部件几何形变建模、部件遮挡推理、分类器等模块。在联合深度学习中，深度模型的各个层和视觉系统的各个模块可以建立对应关系。如果视觉系统中的关键模块在现有深度学习的模型中没有与之对应的层，则它们可以启发我们提出新的深度模型。例如，大量物体检测的研究工作表明，对物体部件的几何形变建模可以有效提高检测率，但是在常用的深度模型中没有与之相对应的层，因此联合深度学习及其后续的工作都提出了新的形变层和形变池化层来实现这一功能。从训练方式上看，计算机视觉系统的各个模块是逐一训练或手工设计的。在深度模型的预训练阶段，各个层也是逐一训练的。如果我们能够建立计算机视觉系统和深度模型之间的对应关系，那么在视觉研究中积累的经验就可以对深度模型的预训练提供指导。这样预训练后得到的模型就可以达到与传统计算机视觉系统可比的结果。在此基础上，深度学习还会利用反向传播对所有层进行联合优化，使它们之间的相互协作达到最优，从而使整个网络的性能得到重大提升。3、深度学习的应用方向3.1、深度学习在物体识别中的应用        ImageNet图像分类深度学习在物体识别中最重要的进展体现在ImageNet ILSVRC挑战中的图像分类任务。传统计算机视觉方法在此测试集上最低的错误率是26.172%。2012年，欣顿的研究小组利用卷积网络把错误率降到了15.315%。此网络结构被称为Alex Net，与传统的卷积网络相比，它有三点与众不同之处：首先，Alex Net采用了dropout的训练策略，在训练过程中将输入层和中间层的一些神经元随机置零。这模拟了噪音对输入数据的各种干扰使一些神经元对一些视觉模式产生漏检的情况。Dropout使训练过程收敛得更慢，但得到的网络模型更加鲁棒。其次，Alex Net采用整流线型单元作为非线性的激发函数。这不仅大大降低了计算的复杂度，而且使神经元的输出具有稀疏的特征，对各种干扰更加鲁棒。第三，Alex Net通过对训练样本镜像映射和加入随机平移扰动，产生了更多的训练样本，减少了过拟合。在ImageNet ILSVRC 2013比赛中，排名前20的小组使用的都是深度学习技术。获胜者是纽约大学罗伯•费格斯(Rob Fergus)的研究小组，所采用的深度模型是卷积网络，并对网络结构作了进一步优化，错误率为11.197%，其模型称作Clarifai。在ILSVRC 2014比赛中，获胜者GooLeNet[18]将错误率降到了6.656%。GooLeNet突出的特点是大大增加了卷积网络的深度，超过了20层，这在此之前是不可想象的。很深的网络结构给预测误差的反向传播带了困难，这是因为预测误差是从最顶层传到底层的，传到底层的误差很小，难以驱动底层参数的更新。GooLeNet采取的策略是将监督信号直接加到多个中间层，这意味着中间层和底层的特征表示也要能够对训练数据进行准确分类。如何有效地训练很深的网络模型仍是未来研究的一个重要课题。虽然深度学习在ImageNet上取得了巨大成功，但是很多应用的训练集是较小的，在这种情况下，如何应用深度学习呢？有三种方法可供参考：(1)可以将ImageNet上训练得到的模型作为起点，利用目标训练集和反向传播对其进行继续训练，将模型适应到特定的应用。此时ImageNet起到预训练的作用。(2)如果目标训练集不够大 ，可以将底层的网络参数固定，沿用ImageNet上的训练集结果，只对上层进行更新。这是因为底层的网络参数是最难更新的，而从ImageNet学习得到的底层滤波器往往描述了各种不同的局部边缘和纹理信息，而这些滤波器对一般的图像有较好的普适性。(3)直接采用ImageNet上训练得到的模型，把最高的隐含层的输出作为特征表达，代替常用的手工设计的特征。        人脸识别深度学习在物体识别上的另一个重要突破是人脸识别。人脸识别的最大挑战是如何区分由于光线、姿态和表情等因素引起的类内变化和由于身份不同产生的类间变化。这两种变化的分布是非线性的，且极为复杂，传统的线性模型无法将它们有效区分开。深度学习的目的是通过多层的非线性变换得到新的特征表示。这些新特征须尽可能多地去掉类内变化，而保留类间变化。人脸识别包括人脸确认和人脸辨识两种任务。人脸确认是判断两张人脸照片是否属于同一个人，属于二分类问题，随机猜的正确率是50%。人脸辨识是将一张人脸图像分为N个类别之一，类别是由人脸的身份定义的。这是个多分类问题，更具挑战性，其难度随着类别数的增多而增大，随机猜的正确率是1/N。两种任务都可以通过深度模型学习人脸的特征表达。2013年，文献采用人脸确认任务作为监督信号，利用卷积网络学习人脸特征，在LFW上取得了92.52%的识别率。这一结果虽然与后续的深度学习方法相比较低，但也超过了大多数非深度学习算法。由于人脸确认是一个二分类问题，用它学习人脸特征的效率比较低，容易在训练集上发生过拟合。而人脸辨识是一个更具挑战性的多分类问题，不容易发生过拟合，更适合通过深度模型学习人脸特征。另一方面，在人脸确认中，每一对训练样本被人工标注成两类中的一类，所含信息量较少。而在人脸辨识中，每个训练样本都被人工标注成N类之一，信息量大。在2014年的IEEE国际计算机视觉与模式识别会议(IEEE Conference on Computer Vision and Pattern Recognition, CVPR)上，DeepID和DeepFace都采用人脸辨识作为监督信号，在LFW上分别取得了97.45%和97.35%的识别率（见表2）。他们利用卷积网络预测N维标注向量，将最高的隐含层作为人脸特征。这一层在训练过程中要区分大量的人脸类别（例如在DeepID中区分1000个类别的人脸），因此包含了丰富的类间变化的信息，有很强的泛化能力。虽然训练中采用的是人脸辨识任务，但得到的特征可以应用到人脸确认任务中，以及识别训练集中是否有新人。例如，LFW上用于测试的任务是人脸确认任务，不同于训练中的人脸辨识任务；DeepID和DeepFace的训练集与LFW测试集的人物身份是不重合的。通过人脸辨识任务学习得到的人脸特征包含较多的类内变化。DeepID2联合使用人脸确认和人脸辨识作为监督信号，得到的人脸特征在保持类间变化的同时使类内变化最小化，从而将LFW上的人脸识别率提高到99.15%。DeepID2利用Titan GPU提取一幅人脸图像的特征只需要35毫秒，而且可以离线进行。经过主元分析(Principal Component Analysis, PCA)压缩最终得到80维的特征向量，可以用于快速人脸在线比对。在后续工作中，DeepID2通过扩展网络结构，增加训练数据，以及在每一层都加入监督信息，在LFW达到了99.47%的识别率。一些人认为深度学习的成功是由于用具有大量参数的复杂模型去拟合数据集，其实远非如此简单。例如DeepID2+的成功还在于其所具有的很多重要有趣的特征：它最上层的神经元响应是中度稀疏的，对人脸身份和各种人脸属性具有很强的选择性，对局部遮挡有很强的鲁棒性。在以往的研究中，为了得到这些属性，我们往往需要对模型加入各种显示的约束。而DeepID2+通过大规模学习自动拥有了这些属性，其背后的理论分析值得未来进一步研究。3.2、深度学习在物体检测中的应用物体检测是比物体识别更难的任务。一幅图像中可能包含属于不同类别的多个物体，物体检测需要确定每个物体的位置和类别。2013年，ImageNet ILSVRC比赛的组织者增加了物体检测的任务，要求在4万张互联网图片中检测200类物体。比赛获胜者使用的是手动设计的特征，平均物体检测率(mean Averaged Precision, mAP)只有22.581%。在ILSVRC 2014中，深度学习将平均物体检测率提高到了43.933%。较有影响力的工作包括 RCNN[、Overfeat、GoogLeNet、DeepID-Net、network in network、VGG和spatial pyramid pooling in deep CNN。RCNN首次提出了被广泛采用的基于深度学习的物体检测流程，并首先采用非深度学习方法（例如selective search）提出候选区域，利用深度卷积网络从候选区域提取特征，然后利用支持向量机等线性分类器基于特征将区域分为物体和背景。DeepID-Net进一步完善了这一流程，使得检测率有了大幅提升，并且对每一个环节的贡献做了详细的实验分析。深度卷积网络结构的设计也至关重要，如果一个网络结构能够提高图像分类任务的准确性，通常也能显著提升物体检测器的性能。深度学习的成功还体现在行人检测上。在最大的行人检测测试集(Caltech)上，广泛采用的方向梯度直方图(Histogram of Oriented Gradient, HOG)特征和可变形部件模型的平均误检率是68%。目前基于深度学习检测的最好结果是20.86%。在最新的研究进展中，很多被证明行之有效的物体检测都用到了深度学习。例如，联合深度学习提出了形变层，对物体部件间的几何形变进行建模；多阶段深度学习可以模拟物体检测中常用的级联分类器；可切换深度网络可以表达物体各个部件的混合模型；文献[35]通过迁移学习将一个深度模型行人检测器自适应到一个目标场景。3.3、深度学习在视频分析方面的应用深度学习在视频分类上的应用还处于起步阶段，未来还有很多工作要做。描述视频的静态图像特征可以采用从ImageNet上学习得到的深度模型，难点是如何描述动态特征。以往的视觉研究方法对动态特征的描述往往依赖于光流估计、对关键点的跟踪和动态纹理。如何将这些信息体现在深度模型中是个难点。最直接的做法是将视频视为三维图像，直接应用卷积网络在每一层学习三维滤波器。但是这一思路显然没有考虑到时间维和空间维的差异性。另外一种简单但更加有效的思路是，通过预处理计算光流场或其他动态特征的空间场分布，作为卷积网络的一个输入通道。也有研究工作利用深度编码器(deep autoencoder)以非线性的方式提取动态纹理。在最新的研究工作中，长短时记忆网络(Long Short-Term Memory, LSTM)受到广泛关注，它可以捕捉长期依赖性，对视频中复杂的动态建模。4、未来发展的展望深度学习在图像识别中的应用方兴未艾，未来有着巨大的发展空间。在物识别和物体检测研究的一个趋势是使用更大更深的网络结构。在ILSVRC 2012中，Alex Net只包含了5个卷积层和两个全连接层。而在ILSVRC2014中， GooLeNet和 VGG 使用的网络结构都超过了20层。更深的网络结构使得反向传播更加困难。与此同时，训练数据的规模也在迅速变大。这迫切需要研究新的算法和开发新的并行计算系统来更加有效地利用大数据训练更大更深的模型。与图像识别相比，深度学习在视频分类中的应用还远未成熟。从ImageNet 训练得到的图像特征可以直接有效地应用到各种与图像相关的识别任务（例如图像分类、图像检索、物体检测和图像分割等）和其他不同的图像测试集中，具有良好的泛化性能。但是深度学习至今还没有得到类似的可用于视频分析的特征。要达到这个目的，不但要建立大规模的训练数据集（文献最新建立了包含100万个YouTube视频的数据库），还需要研究适用于视频分析的新的深度模型。训练用于视频分析的深度模型的计算量也会大大增加。在与图像和视频相关的应用中，深度模型的输出预测（例如分割图或物体检测框）往往具有空间和时间上的相关性。因此研究具有结构性输出的深度模型也是一个重点。虽然神经网络的目的在于解决一般意义上的机器学习问题，但领域知识对深度模型的设计也起着重要的作用。在与图像和视频相关的应用中，最成功的是深度卷积网络，其设计正是利用了图像的特殊结构。其中最重要的两个操作——卷积和池化都来自与图像相关的领域知识。如何通过研究领域知识，在深度模型中引入新的有效的操作和层，对于提高图像和视频识别的性能有着重要意义。例如，池化层带来了局部的平移不变性，文献中提出的形变池化层在此基础上更好地描述了物体各个部分的几何形变。在未来研究中，可以将其进一步扩展，从而取得旋转不变性、尺度不变性和对遮挡的鲁棒性。通过研究深度模型和传统计算机视觉系统之间的关系，不但可以帮助我们理解深度学习成功的原因，还可以启发新的模型和训练方法。联合深度学习和多阶段深度学习未来还有更多的工作要做。虽然深度学习在实践中取得了巨大成功，而且通过大数据训练得到的深度模型体现出的特性（例如稀疏性、选择性和对遮挡的鲁棒性）引人注目，但其背后的理论分析还有许多工作需要完成。例如，何时收敛？如何取得较好的局部极小点？每一层变换取得了哪些对识别有益的不变性，又损失了哪些信息？最近马拉特(Mallat)利用小波对深层网络结构进行了量化分析，这是在此方向上的重要探索。5、结语深度模型并非黑盒子，它与传统的计算机视觉系统有着密切的联系，神经网络的各个层通过联合学习、整体优化，使得性能得到大幅提升。与图像识别相关的各种应用也在推动深度学习在网络结构、层的设计和训练方法各个方面的快速发展。可以预见在未来数年内，深度学习将会在理论、算法和应用各方面进入高速发展时期。"}
{"content2":"『教程』Batch Normalization 层介绍知乎：详解深度学习中的Normalization，BN/LN/WN一、两个概念独立同分布（independent and identically distributed）独立同分布的数据可以简化常规机器学习模型的训练、提升机器学习模型的预测能力白化（whitening）去除特征之间的相关性 —> 独立；使得所有特征具有相同的均值和方差 —> 同分布。二、问题1、抽象程度高的层难以训练深度神经网络涉及到很多层的叠加，而每一层的参数更新会导致上层的输入数据分布发生变化，通过层层叠加，高层（抽象程度高）的输入分布变化会非常剧烈，这就使得高层需要不断去重新适应底层的数据更新Google 将这一现象总结为 Internal Covariate Shif：统计机器学习中的一个经典假设是“源空间（source domain）和目标空间（target domain）的数据分布（distribution）是一致的”。如果不一致，那么就出现了新的机器学习问题，如 transfer learning / domain adaptation 等。而 covariate shift 就是分布不一致假设之下的一个分支问题，它是指源空间和目标空间的条件概率是一致的，但是其边缘概率不同，即：对所有,但是大家细想便会发现，的确，对于神经网络的各层输出，由于它们经过了层内操作作用，各层的输入信号的分布显然不同，而且差异会随着网络深度增大而增大，可是它们所能“指示”的样本标记（label）仍然是不变的，这便符合了covariate shift的定义。由于是对层间信号的分析，也即是“internal”的来由。问题描述简而言之，每个神经元的输入数据不再是“独立同分布”。其一，上层参数需要不断适应新的输入数据分布，降低学习速度。其二，下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止。其三，每层的更新都会影响到其它层，因此每层的参数更新策略需要尽可能的谨慎。2、问题挑战我们以神经网络中的一个普通神经元为例。神经元接收一组输入向量通过某种运算后，输出一个标量值：由于 ICS 问题的存在， 对于某一特定层，不同批次的输入 的分布可能相差很大。要解决独立同分布的问题，“理论正确”的方法就是对每一层的数据都进行白化操作。然而标准的白化操作代价高昂，特别是我们还希望白化操作是可微的，保证白化操作可以通过反向传播来更新梯度。三、解决思路：BN1、通用框架在将 送给神经元之前，先对其做平移和伸缩变换， 将 的分布规范化成在固定区间范围的标准分布。通用变换框架就如下所示：（1） 是平移参数（shift parameter）， 是缩放参数（scale parameter）。通过这两个参数进行 shift 和 scale 变换：得到的数据符合均值为 0、方差为 1 的标准分布。（2） 是再平移参数（re-shift parameter）， 是再缩放参数（re-scale parameter）。将 上一步得到的 进一步变换为：最终得到的数据符合均值为 、方差为 的分布。2、第二次变换的目的1、目的一第一次变换得到均值为 0、方差为 1 的标准分布，表达能力有限，下层神经元可能很努力地在学习，但不论其如何变化，其输出的结果在交给上层神经元进行处理之前，将被粗暴地重新调整到这一固定范围。为了更好的应用底层神经网络的学习结果，我们将规范化后的数据进行再平移和再缩放，使得每个神经元对应的输入范围是针对该神经元量身定制的一个确定范围（均值为 、方差为 ）。rescale 和 reshift 的参数都是可学习的，这就使得 Normalization 层可以学习如何去适应底层的学习结果。2、目的二除了充分利用底层学习的能力，另一方面的重要意义在于保证获得非线性的表达能力。Sigmoid 等激活函数在神经网络中有着重要作用，通过区分饱和区和非饱和区，使得神经网络的数据变换具有了非线性计算能力。而第一步的规范化会将几乎所有数据映射到激活函数的非饱和区（线性区），仅利用到了线性变化能力，从而降低了神经网络的表达能力。而进行再变换，则可以将数据从线性区变换到非线性区，恢复模型的表达能力。3、优势不添加正则化， 的均值取决于下层神经网络的复杂关联；添加本层后，取值 仅由 来确定，去除了与下层计算的密切耦合。新参数很容易通过梯度下降来学习，简化了神经网络的训练。4、问题a）BN的实际作用标准白化操作的目的是“独立同分布”。独立就不说了，暂不考虑。变换为均值为 、方差为 的分布，也并不是严格的同分布，只是映射到了一个确定的区间范围而已（所以，这个问题仍然有研究空间）。另外有人提出：BN其优势并非解决了独立同分布的问题（实际上它也没解决），其最大意义在于解决了梯度弥散问题，见论文：How Does Batch Normalization Help Optimization?（知乎上的一篇阅读笔记：为什么Batch Normalization那么有用？)。文章结论如下：没有证据表明BN的work，是因为减少了ICS（Interval Covariate Shift）。BN work的根本原因，是因为在网络的训练阶段，其能够让优化空间（optimization landscape）变的平滑。其他的normalization技术也能够像BN那样对于网络的训练起到作用。其作用为防止梯度爆炸或弥散、可以提高训练时模型对于不同超参（学习率、初始化）的鲁棒性、可以让大部分的激活函数能够远离其饱和区域。b) BN对小批次训练效果不好当单个小批次(minibatch)的数据不能代表整个数据的分布时，BN的表现就会不尽如人意，这意味着忘记将输入随机打乱顺序的情况下使用批归一化是很危险的，实际上batch过小的时候就不太适合开放BN的可训练性。具体讨论见论文：Batch Normalization: Accelerating Deep Network Training by Reducing。BN训练时为什么不使用全局均值/方差？使用 BN 的目的就是为了保证每批数据的分布稳定，使用全局统计量反而违背了这个初衷；BN 的作者认为在训练时采用移动平均可能会与梯度优化存在冲突四、主流 Normalization 方法梳理BatchNorm：batch方向做归一化，算N*H*W的均值LayerNorm：channel方向做归一化，算C*H*W的均值InstanceNorm：一个channel内做归一化，算H*W的均值GroupNorm：将channel方向分group，然后每个group内做归一化，算(C//G)*H*W的均值https://zhuanlan.zhihu.com/p/696598441、Batch Normalization『TensorFlow』批处理类『教程』Batch Normalization 层介绍于2015年由 Google 提出，BN 独立地规范化每一个层不同批次的 ，但规范化的参数是一个 mini-batch 的一阶统计量和二阶统计量。这就要求 每一个 mini-batch 的统计量是整体统计量的近似估计，或者说每一个 mini-batch 彼此之间，以及和整体数据，都应该是近似同分布的。分布差距较小的 mini-batch 可以看做是为规范化操作和模型训练引入了噪声，可以增加模型的鲁棒性；但如果每个 mini-batch的原始分布差别很大，那么不同 mini-batch 的数据将会进行不一样的数据变换，这就增加了模型训练的难度。训练时，网络会记录每一个batch滑动平均的均值和方差，训练结束的时候这四个参数就固定了供测试时直接加载使用。BN 比较适用的场景是：每个 mini-batch 比较大，数据分布比较接近。在进行训练之前，要做好充分的 shuffle，否则效果会差很多。另外，由于 BN 需要在运行过程中统计每个 mini-batch 的一阶统计量和二阶统计量，因此不适用于 动态的网络结构 和 RNN 网络。不过，也有研究者专门提出了适用于 RNN 的 BN 使用方法，这里先不展开了。2、Layer NormalizationLN 针对单个训练样本进行，不依赖于其他数据，因此可以避免 BN 中受 mini-batch 数据分布影响的问题，可以用于 小 mini-batch 场景、动态网络场景和 RNN，特别是自然语言处理领域。此外，LN 不需要保存 mini-batch 的均值和方差，节省了额外的存储空间。但是，BN 的转换是针对单个神经元可训练的——不同神经元的输入经过再平移和再缩放后分布在不同的区间，而 LN 对于一整层的神经元训练得到同一个转换——所有的输入都在同一个区间范围内。如果不同输入特征不属于相似的类别（比如颜色和大小），那么 LN 的处理可能会降低模型的表达能力。3、Instance Normalization在GAN和style transfer的任务中，目前的IN norm要好于BN，IN主要用于对单张图像的数据做处理，而BN主要是对Bacth的数据做处理。由于BN在训练时每个batch的均值和方差会由于shuffle都会改变，所以可以理解为一种数据增强，而IN可以理解为对数据做一个归一化的操作。换句话说，BN的计算是要受其他样本影响的，由于每个batch的均值和标准差不稳定，对于单个数据而言，相对于是引入了噪声，但在分类这种问题上，结果和数据的整体分布有关系，因此需要通过BN获得数据的整体分布。而instance norm的信息都是来自于自身的图片，相当于对全局信息做了一次整合和调整，在图像转换这种问题上，BN获得的整体信息不会带来任何收益，带来的噪声反而会弱化实例之间的独立性：这类生成式方法，每张图片自己的风格比较独立不应该与batch中其他的样本产生太大联系。4、Group Normalizationgroup normalization是2018年3月份何恺明大神的又一力作，优化了BN在比较小的mini-batch情况下表现不太好的劣势。批量维度进行归一化会带来一些问题——批量统计估算不准确导致批量变小时，BN 的误差会迅速增加。在训练大型网络和将特征转移到计算机视觉任务中（包括检测、分割和视频），内存消耗限制了只能使用小批量的BN。事实上，GN的极端情况就是LN和IN，分别对应G等于C和G等于1。tf实现并不复杂，如下def GroupNorm(x,G=16,eps=1e-5): N,H,W,C=x.shape x=tf.reshape(x,[tf.cast(N,tf.int32), tf.cast(H,tf.int32), tf.cast(W,tf.int32), tf.cast(G,tf.int32), tf.cast(C//G,tf.int32)]) mean,var=tf.nn.moments(x,[1,2,4],keep_dims=True) x=(x-mean)/tf.sqrt(var+eps) x=tf.reshape(x,[tf.cast(N,tf.int32), tf.cast(H,tf.int32), 　　　　　　　　　　　　tf.cast(W,tf.int32), 　　　　　　　　　　　　tf.cast(C,tf.int32)]) gamma = tf.Variable(tf.ones(shape=[1,1,1,tf.cast(C,tf.int32)]), name=\"gamma\") beta = tf.Variable(tf.zeros(shape=[1,1,1,tf.cast(C,tf.int32)]), name=\"beta\") return x*gamma+beta在深度学习没有火起来之前，提取特征通常是使用SIFT，HOG和GIST特征，这些特征有一个共性，都具有按group表示的特性，每一个group由相同种类直方图的构建而成，这些特征通常是对在每个直方图（histogram）或每个方向（orientation）上进行组归一化（group-wise norm）而得到。从深度学习上来讲，完全可以认为卷积提取的特征是一种非结构化的特征或者向量，拿网络的第一层卷积为例，卷积层中的的卷积核filter1和此卷积核的其他经过transform过的版本filter2（transform可以是horizontal flipping等），在同一张图像上学习到的特征应该是具有相同的分布，那么，具有相同的特征可以被分到同一个group中，按照个人理解，每一层有很多的卷积核，这些核学习到的特征并不完全是独立的，某些特征具有相同的分布，因此可以被group。"}
{"content2":"我们处在人工智能时代，每个人的生活开始离不开各种「机器」，无论是虚拟的语音助手还是现实世界中的工业机器人，人工智能和各种智能「机器」都在将我们带向更好的未来。近日，msup主办的学术交流盛会——人工智能创新与智能创造峰会（简称A2M）筹备工作正式开启。A2M(AI TO Machine)峰会旨在以国际化视角洞察人工智能发展趋势，发现全球人工智能创新工程和杰出团队， 整合国际最佳技术实践，构建人工智能案例研究智库。今年A2M大会将于8月25-26日登陆快速持续发展且拥有精准技术从业人群的上海，现已有超过40位人工智能领域的顶级嘉宾确认出席，和IT圈从业者共同探讨人工智能的未来！大会亮点亮点一：覆盖三大主题方向1)AI时代的新实践，内容聚焦在大家关注的一些新技术的应用落地实践，包括区块链、计算机视觉、智能制造、物联网等；2)AI赋能业务实践，内容聚焦在AI的先行者企业是如何通过AI技术帮助企业进行创新、节省人力成本、加强与用户联系的，包括AI产品设计、Fintech、智能语言与语音、推荐及广告系统等；3)AI时代的工程实践，内容聚焦在AI落地时工程层面的最佳实践，从算法、机器学习、深度学习等层面全面展现AI在不同业务场景落地时的技术难点与解决办法。亮点二：汇聚人工智能领域顶级公司讲师案例均来自海内外顶级公司和新锐AI领域独角兽公司，包括Microsoft、Amazon、微博、京东、face++、出门问问、联想、东软等。亮点三：覆盖各行各业覆盖互联网金融、工业制造、家电制造、新能源、物联网、智慧家居、新零售、社交、电商、营销、证券、客服、二手交易、云计算服务、自动驾驶、O2O、出行等多种场景。亮点四：聚焦实践全部40+案例均为已经在业务中落地使用，并且已经对企业、业务产生实际价值的案例，同时，所有案例在邀请过程中注重筛选那些从零到一的落地实践案例，讲师会把该场景下为什么使用AI，AI技术选型及AI带来的效果进行剖析。大会日程本届大会，3大主题分会场并行。AI时代的新实践：AI赋能业务实践：AI时代的工程实践：扫描二维码，即可获取大会更多信息一场人工智能巅峰大会即将来袭，快来提前抢购大会早鸟票吧！"}
{"content2":"1999年由David Lowe首先发表于计算机视觉国际会议（International Conference on Computer Vision，ICCV），2004年再次经David Lowe整理完善后发表于International journal of computer vision（IJCV）。截止2014年8月，该论文单篇被引次数达25000余次。---来自百科本打算对04年的论文进行翻译，结果。居然搜到完整翻译版，虽然翻译的不太好，不过有聊胜于无。本文的讲解大部分主要还是借鉴了最下面参考文献【8】的资料，先谢过作者，该作者对opencv的好多源码进行了分析，很强啊。先说sift中一步一步的的动机（motivation）：对于普通的图像匹配来说（都是平面物体：比如放在这个桌子上的书，然后这本书放在另一个地方，而非人脸在不断的旋转，一下只能看到侧脸，一下看到前脸。即sift的优点之处，旋转，缩放，尺度不变的特性，而这里都指的是平面内，而不是平面外的旋转）【此观点如果有误还望告知】，sift的先驱们发现，对于平面物体来说只要对两幅图像找到超过3对物体间的匹配点，就能使用射影几何进行对应。而对于旋转的，缩放的图像，如何建立稳定的关键点，就是关键了，当然必须考虑到光照变化，所以相对来说，比如角点、边缘、亮区域中的暗点，暗区域中的亮点等等。--这就是关键点检测。ps：sift是建立在灰度图上的。当然找关键点的方法就是求取空间的极值，可是对于只有离散值的图像来说，如何能够模拟连续值形式的函数来找到最小值，这是问题的关键。而且通常来说，要求图像的最值，需要求导什么的，而这都是使用滤波器的方法来实现的。可是对于不同尺度的图像或者其中的物体来说，同一个尺寸的滤波器滤波之后得到的极值有可能差别较大，所以就出现了金字塔形状。【此观点如果有误，还望告知】假设图像是深层的，那么不断的不同大小的尺度的图中是会有相同信息的，所以这可以看成是插值形式，即当前图像与图像浓缩成一个点之间无限的插值而成的不同尺度却内容相同的图像序列。可想而知这时候关键点是有了，可是杂质太多，所以需要提纯，所以在极值之后得到关键点，接着就需要基于关键点进行筛选，得到更稳定的关键点。---这就是关键点定位。ps：解决了缩放、尺度的特性问题。然后为了使得关键点更加稳固，如果加上关键点周边的信息，一起作为其特征，那么就能更好的建立特征空间。这时候可以引入方向信息，即求其梯度，这样只要将不同图片的这一步的方向都旋转到同一个方向，那么就能够更好的匹配配了，这样就解决了旋转的问题。---这就是方向确定。ps：这样尺度、方向，位置信息就都有了。接下来就是将上面的这些信息都进行统一的描述，从而建立最后的sift特征。---这就是建立描述符。一、原理分析1、尺度空间极值点检测尺度空间：高大上的词，具体可参考[6，7]。其实就是对应着平时拍照时候，摄像头的聚焦过程产生的清晰还是模糊的意思。为了更直观，先从维基上找到的资料来解释下什么是尺度空间，当然这里指的是计算机视觉中的尺度空间。对于一幅给定的图像 , 它的线性（高斯）尺度空间就是由该图像与一个2维高斯核卷积得到的  .其中卷积核如下：（公式1）其中为模糊半径的平方。得到的尺度空间为：（公式2）中的点表示只在变量 ,上执行卷积操作，  就是尺度参数了，是用来表示当前在哪个尺度级别上进行卷积。这个定义  中限制了 ,不过可以看得出来，计算机视觉中得到的都是离散的，间断的图像序列，而没法得到连续的图像。当然尺度参数即为高斯过滤器的方差  ，如果说 那么过滤器  就变成一个脉冲函数，导致的结果就是  也就是在  这个级别的尺度空间，其实就是图像  自己。随着 的变大， 就是针对图像上使用越来越大的平滑过滤器,因而会移除越来越多图像内容的细节。因为过滤器的标准差是 ,所以很大程度上明显小于该值的一些细节就会被忽略掉：Scale-space representation  at scale , corresponding to the original imageScale-space representation  at scaleScale-space representation  at scaleScale-space representation  at scaleScale-space representation  at scaleScale-space representation  at scale说完了尺度空间，现在接着说sift的第一步，极值点检测。ps：尺度是自然存在的，不是人为创造的，高斯卷积只是表现尺度的一种形式。高斯卷积（也叫做高斯平滑，高斯模糊）：通常可以用来减少图像噪声以及降低细节层次。因为在实际应用中，在计算高斯函数的离散近似时，在大概3σ距离之外的像素都可以看作不起作用，这些像素的计算也就可以忽略。通常，图像处理程序只需要计算的矩阵就可以保证相关像素影响。所以我们先计算σ的值，然后通过该值计算出所需要的模板的大小，然后对模板进行归一化，接着用其对图像进行高斯过滤。比如当方差为0.6时得到的就是5×5的高斯模板，可以通过matlab代码生成：a=fspecial('gaussian',5,0.6)ps：高斯模糊具有园对称性；而且是线性可分的，所以可以在二维的图像上对每一维单独进行一维高斯模糊来减少计算量，这可以参考【4】中的2.3分离高斯模糊部分；而且对于一幅图像进行多次连续高斯模糊的效果与一次更大的高斯模糊可以产生同样的效果，大的高斯模糊的半径是所用多个高斯模糊半径平方和的平方根，比如，使用半径分别为6 和8的两次高斯模糊变换得到的效果等于一次半径为10的高斯模糊效果，因为6^2+8^2=10^2。所以使用多个连续较小的高斯模糊处理不会比单个高斯较大处理时间少。上面介绍了尺度空间和高斯模糊，接下来就是介绍高斯金字塔了，通过构建多层的高斯金字塔来计算后续的图片中的极值。高斯金字塔：，高斯金字塔高斯金字塔共分O 组（Octave），每组又分S 层（Layer）。组内各层图像的分辨率是相同的，即长和宽相同，但尺度逐渐增加，即越往塔顶图像越模糊。而下一组的图像是由上一组图像（上一组倒数第3张，即上图中的2组的第0层是第一组的上往下数第3张采样得到的）按照隔点降采样得到的，即图像的长和宽分别减半。高斯金字塔的组数O 是由输入图像的分辨率得到的，因为要进行隔点降采样，所以在执行降采样生成高斯金字塔时，一直到4*4大小（因为太小的图片也没什么意义了）。其中组数公式为：（公式3）其中，X 和Y 分别为输入图像的长和宽，⌊ ⌋表示向下取整。并且每组的金字塔的层数S 为：S = s + 2+1 。这里Lowe 建议s 为3 。（s为建议的，2是为了不同组之间能够平滑过渡而在顶部加上的2层，1为最底的原始层）【8】假设当前图像大小为512*512，则如下操作：上面公式中减2 是因为假设当前图像即为输入图像，而有的资料减3 是因为，他们把自然界作为第0组，而经过相机拍摄就成了第1组，所以是多减一次（当然了，到底减多少，任君喜欢，这不是重点）。这时候就需要依据当前图像建立自然场景的第 0 组图像了，即用当前图像进行插值得到例如上面那样的1024*1024的一张图片。得到了组数之后，就是计算组内尺度和组间尺度了。记得这里是假设当前得到的图片的尺度为0.5的，即第1组的第0层（自然界为第0组），即拍照拍到之后拿到的那张图片。我们可以很自然的知道，这是有组内尺度和组间尺度的。对于组内相邻的两幅图片来说：（公式4）假设，这里的s为小写的，不是表示层数的那个大S。（为什么使用1/2，这个可以看lowe的论文【1】）所以，对于某一组来说，当s =3时，有S=s+2+1 =6幅图片，即[0,1,2,3,4,5)：而对于组间相邻的尺度来说，因为后一组是通过前一组的倒数第3幅图进行降采样得到的,即前一组的第s幅。为，将带入得其为，即后一组的第0幅图为前一组第0幅图的2倍尺度。同时推出相邻两组同一层的图像都是2倍尺度关系的。则，总的尺度坐标为：（公式5）其中，o 表示第几组，r表示组内第几层。在lowe的论文中，他将第0层，也就是自然界层的初始尺度定义为1.6，而我们获取的图片初始尺度为0.5，则图像金字塔第0层的实际尺度为sqrt（1.6*1.6-0.5*0.5）=1.52（之所以可以这么计算就是因为上面高斯金字塔之上的ps部分所述）。当然这是为了避免检测极值点之前就对原始图像的高斯平滑以致于图像丢失高频信息，所以才建议在建立尺度空间前首先对原始图像的长宽扩展一倍，以保留原始图像的信息，也就是我们之前说的那个假设你获得了图像是512*512的，那么就弄一个1024*1024的作为最底层图 ，这时候因为图像扩展造成的尺度计算为（即通过计算机得到的真实尺度），当然这一组在很多资料中也叫做第 -1 组：(公式6)本来对于关键点的检测，可以 利用LoG（高斯拉普拉斯方法，Laplacian of Gaussian），即图像的二阶导数，能够在不同的尺度下检测到图像的斑点特征，从而可以确定图像的特征点。lindeberg在文献《scale space theory:a basic tool for analysising structures at different scales》中指出尺度规范化的log算子具有真正的尺度不变性。但LoG 的效率不高。所以sift不这么做，作者对两个相邻高斯尺度空间的图像相减，得到一个DoG（高斯差分，Difference of Gaussians）的响应值图像D(x, y, σ)来近似LoG ：（公式7）两者为什么能够等效，仔细的推导可以看这里的3.4 高斯差分金字塔部分。用DoG 代替LoG 并不影响对图像斑点位置的检测。而且用DoG 近似LoG 可以实现下列好处：第一是LoG 需要使用两个方向的高斯二阶微分卷积核，而DoG 直接使用高斯卷积核，省去了卷积核生成的运算量；第二是DoG 保留了个高斯尺度空间的图像，因此在生成某一空间尺度的特征时，可以直接使用公式2产生的尺度空间图像，而无需重新再次生成该尺度的图像；第三是DoG 具有与LoG 相同的性质，即稳定性好、抗干扰能力强。为了在连续的尺度下检测图像的特征点，需要建立DoG 金字塔，而DoG 金字塔的建立又离不开高斯金字塔的建立，如下图所示，左侧为高斯金字塔，右侧为DoG 金字塔【8】：高斯金字塔的组内相邻两层相减，而两组间的各层是不能相减的。因此高斯金字塔每组有s+3 层图像，而DoG 金字塔每组则有s+2 层图像 。我们可以发现未被利用到的高斯金字塔的第0组的第0层和最上面一组的最上面一层；而dog金字塔的每组的第0层和顶层未被用到（原因如下）。极值点的搜索是在DoG 金字塔内进行的，这些极值点就是候选的特征点。在搜索之前，我们需要在DoG 金字塔内剔除那些像素值过小的点，因为这些像素具有较低的对比度，它们肯定不是稳定的特征点。极值点的搜索不仅需要在它所在尺度空间图像的邻域内进行，还需要在它的相邻尺度空间图像内进行，如2、关键点的定位从上面的搜索中就能够得到许多的极值点了，之所谓极值点，就是都是一群待选的点，不是直接就可以用的关键点。因为它们还存在一些不确定的因素，首先是极值点的搜索是在离散空间内进行的，并且这些离散空间还是经过不断的降采样得到的。如果把采样点拟合成曲面后我们会发现，原先的极值点并不是真正的极值点，也就是离散空间的极值点并不是连续空间的极值点，如下图【4】：（接下来这一大段都是来自参考文献【8】，因为觉得该作者讲的详细，所以直接复制过来这部分了）为了提高关键点的稳定性，需要对尺度空间DoG函数进行曲线拟合。因为，极值点是一个三维矢量，即它包括极值点所在的图像坐标，及尺度坐标：。利用DoG函数在尺度空间的Taylor展开式(拟合函数)，假设我们在点处展开，展开2阶即可 ：（公式8）对应的矩阵形式为：（公式9）在这里 表示离散空间下的插值中心（在离散空间内也就是采样点）坐标， 表示拟合后连续空间下的插值点坐标，设，则表示相对于插值中心，插值后的偏移量。因此上述公式可写成：（公式10）求导得：(公式11)将其置为0，得到结果为：（公式12）将上面的极值点带入求导前的原公式（10）得：（公式13）对于公式12 所求得的偏移量如果大于0.5（只要x、y 和σ 任意一个量大于0.5），则表明插值点已偏移到了它的临近的插值中心，所以必须改变当前的位置，使其为它所偏移到的 插值中心处，然后在新的位置上重新进行泰勒级数插值拟合，直到偏移量小于0.5 为止（x、y 和σ 都小于0.5），这是一个迭代的工程。当然，为了避免无限次的迭代，我们还需要设置一个最大迭代次数，在达到了迭代次数但仍然没有满足偏移量小于0.5 的情况下，该极值点就要被剔除掉。另外，如果由公式13 所得到的极值过小，即时（假设图像的灰度值在0～1.0 之间），则这样的点易受到噪声的干扰而变得不稳定，所以这些点也应该剔除。而在opencv 中，使用的是下列公式来判断其是否为不稳定的极值 ：（公式14）其中T 为经验阈值，系统默认初始化为0.04。仅仅去除低对比度的极值点对于特征的稳定性还是远远不够的。DoG函数在图像边缘也有着较强的边缘响应，因此我们还需要排除边缘响应。一旦特征点落在图像的边缘上，这些点就是不稳定的点。这是因为一方面图像边缘上的点是很难定位的，具有定位的歧义性；另一方面这样的点很容易受到噪声的干扰而变得不稳定。因此我们一定要把这些点找到并剔除掉。它的方法与Harris 角点检测算法相似，即一个平坦的DoG 响应峰值往往在横跨边缘的地方有较大的主曲率，而在垂直边缘的方向上有较小的主曲率，主曲率可以通过2×2 的Hessian 矩阵H 求出 ：（公式15）其中、和分别表示对DoG 图像中的像素在x 轴方向和y 轴方向上求二阶偏导和二阶混合偏导。（而这4个值，可以用有限差分求导来近似得到）在这里，我们不需要求具体的矩阵H 的两个特征值α 和β，而只要知道两个特征值的比例就可以知道该像素点的主曲率 。矩阵H 的直迹和行列式分别为：（公式16）（公式17）我们首先剔除掉那些行列式为负数的点，即Det(H) < 0，因为如果像素的曲率有不同的符号，则该点肯定不是特征点。设α > β，并且α = γ β，其中γ > 1，则 ：（公式18）上式的结果只与两个特征值的比例有关，而与具体的特征值无关。我们知道，当某个像素的H 矩阵的两个特征值相差越大，即γ 很大，则该像素越有可能是边缘。对于上面那个公式来说，当两个特征值相等时，等式的值最小，随着γ 的增加，等式的值也增加。所以，要想检查主曲率的比值是否小于某一阈值γ，只要检查下式是否成立即可 ：（公式19）对于不满足上式的极值点就不是特征点，因此应该把它们剔除掉。Lowe 给出γ 为10。在上面的运算中，需要用到有限差分法求偏导 。这部分可以看看这里的4.3有限差分求导。3、方向角度的确定上面这样就算是从DoG图中找到极值点，然后筛选下留下的就是关键点了。这时候为了解决旋转不变性，并且使得关键点更加的鲁棒，就对其周围进行深挖，将周边的信息赋予到这个关键点上。就是需要根据检测到的特征点所在的高斯尺度图像的局部结构求得一个方向基准。该高斯尺度图像的尺度σ 是已知的，并且该尺度是相对于高斯金字塔所在组的基准层的尺度，即（公式20，该式子的r 表示的是第几层，而下面的r 表示的是半径）而所谓局部结构指的是在高斯尺度图像中以特征点为中心，以r 为半径的区域内计算所有像素梯度的幅角和幅值，半径r 为：（公式21）其中σ 就是上面提到的相对于所在组的基准层的高斯尺度图像的尺度。当然了像素梯度的幅值和幅角的计算公式计算公式为：（公式22）（公式23）因为在以r 为半径的区域内的像素梯度幅值对圆心处的特征点的贡献是不同的，因此还需要对幅值进行加权处理，这里采用的是高斯加权，该高斯函数的方差 为：（公式24）在完成特征点邻域范围内的梯度计算后，还要应用梯度方向直方图来统计邻域内像素的梯度方向所对应的幅值大小。具体的做法是，把360度分为36 个柱，则每10度为一个柱，其中0-9为第1 柱，10-19为第2 柱，以此类推。在以r 为半径的区域内，把那些梯度方向在0-9范围内的像素找出来，把它们的加权后的梯度幅值相加在一起，作为第1 柱的柱高；求第2 柱以及其他柱的高度的方法相同。为了防止某个梯度方向角度因受到噪声的干扰而突变，我们还需要对梯度方向直方图进行平滑处理 。Opencv中直方图以周围360度为范围，每10度为一个柱，一共36个柱，柱所代表的方向为像素点梯度方向，柱的长短代表了梯度幅值。根据Lowe的建议，模板采用[0.25,0.5,0.25],并连续加权两次：（公式25）（这里要注意下，不同版本的opencv中实现的方法不一样）其中h 和H 分别表示平滑前和平滑后的直方图。由于角度是循环的，即0度=360度，如果出现h(j)，j 超出了(0,…,15)的范围，那么可以通过圆周循环的方法找到它所对应的、在0-360度之间的值，如h(-1) = h(35) 。上图只画了8个柱。该图来自这里。方向直方图的峰值则代表了该特征点处邻域梯度的方向，以直方图中最大值作为该关键点的主方向。为了增强匹配的鲁棒性，只保留峰值大于主方向峰值80％的方向作为该关键点的辅方向。因此，对于同一梯度值的多个峰值的关键点位置，在相同位置和尺度将会有多个关键点被创建但方向不同。仅有15％的关键点被赋予多个方向，但可以明显的提高关键点匹配的稳定性。实际编程实现中，就是把该关键点复制成多份关键点，并将方向值分别赋给这些复制后的关键点，并且，离散的梯度方向直方图要进行插值拟合处理，来求得更精确的方向角度值。这样，直方图的主峰值，即最高的那个柱体所代表的方向就是该特征点处邻域范围内图像梯度的主方向，也就是该特征点的主方向。由于柱体所代表的角度只是一个范围，如第1柱的角度为0-9度，因此还需要对离散的梯度方向直方图进行插值拟合处理，以得到更精确的方向角度值。例如我们已经得到了第i 柱所代表的方向为特征点的主方向，则拟合公式为 ：（公式26）（公式27）其中，H 为由公式34 得到的直方图，角度θ 的单位是度。同样的，上面两个公式也存在着之前那个直方图平滑所遇到的角度问题，处理的方法同样还是利用角度的圆周循环。每个特征点除了必须分配一个主方向外，还可能有一个或更多个辅方向，增加辅方向的目的是为了增强图像匹配的鲁棒性。辅方向的定义是，当存在另一个柱体高度大于主方向柱 体高度的80%时，则该柱体所代表的方向角度就是该特征点的辅方向。在第2 步中，我们实现了用两个信息量来表示一个特征点，即位置和尺度。那么经过上面的计算，我们对特征点的表示形式又增加了一个信息量：方向， 即。如果某个特征点还有一个辅方向，则这个特征点就要用两个值来表示：和，其中表示主方向，表示辅助方向。而其他的变量：x, y, σ 不变。4、描述符的建立描述的目的在于之前的那些零碎的关键点的计算之后，如何用一组向量来将这个关键点描述出来，这个描述子不但包含关键点，也包含了周边贡献的像素点的信息。因为已经有了尺度，位置和方向等信息，所以能够有较好的适应性。思路就是对关键点周围图像区域分块，计算块内梯度直方图生成具有独特性的向量，该向量就是该区域图像信息的一种抽象，具有唯一性。在图像局部区域内，这些参数可以重复的用来描述局部二维坐标系统，因为这些参数具有不变性。下面就来计算局部图像区域的描述符，描述符既具有可区分性，又具有对某些变量的不变性，如光亮或三维视角。描述符是与关键点所在的尺度有关的，所以描述关键点是需要在该关键点所在的高斯尺度图像上进行的。在高斯尺度图像上，以关键点为中心，将其附近邻域划分为d×d 个子区域（Lowe 取d = 4）。每个子区域都是一个正方形，正方形的边长为3σ，也就是说正方形的边长有3σ 个像素点（这里当然要对3σ 取整）。σ 为相对于关键点所在的高斯金字塔的组的基准层图像的尺度，即公式20 所表示的尺度。考虑到实际编程的需要，关键点邻域范围的边长应为3σ(d+1)，因此关键点邻域区域一共应有3σ(d+1)×3σ(d+1)个像素点。为了保证关键点具有旋转不变性，还需要以关键点为中心，将上面确定下来的关键点邻域区域旋转θ（θ 就是该关键点的方向）。由于是对正方形进行旋转，为了使旋转后的区域包括整个正方形，应该以从正方形的中心到它的边的最长距离为半径，也就是正方形对角线长度的一半，即：（公式28）所以上述的关键点邻域区域实际应该有(2r+1)×(2r+1)个像素点。由于进行了旋转，则这些采样点的新坐标为：（公式29）其中是旋转后的像素的新坐标，是旋转前的坐标。ps：从《matrix computations.4th》中可以看到：即上面关键点的旋转是逆时针旋转θ度：.上图中上左一到上右一是坐标系顺时针的旋转了，其实也就等同于该坐标逆时针的旋转了。这时，我们需要计算旋转以后关键点邻域范围内像素的梯度幅值和梯度幅角。这里的梯度幅值还需要根据其对中心特征点贡献的大小进行加权处理，加权函数仍然采用高斯函数，它的方差的平方为。在实际应用中，我们是先以关键点为圆心，以公式28 中的r 为半径，计算该圆内所有像素的梯度幅角和高斯加权后的梯度幅值，然后再根据公式29 得到这些幅值和幅角所对应的像素在旋转以后新的坐标位置。在计算关键点描述符的时候，我们不需要精确知道邻域内所有像素的梯度幅值和幅角，我们只需要根据直方图知道其统计值即可。这里的直方图是三维直方图 。如下图 所示：上图 中的三维直方图为一个立方体，立方体的底就是关键点邻域区域。如前面所述，该区域被分为4×4 个子区域，即16 个子区域，邻域内的像素根据其坐标位置，把它们归属于这16 个子区域中的一个。立方体的三维直方图的高为邻域像素幅角的大小。我们把360 度的幅角范围进行8 等分，每一个等份为45 度。则再根据邻域像素梯度幅角的大小，把它们归属于这8 等份中的一份。这样三维直方图就建立了起来，即以关键点为中心的邻域像素根据其坐标位置，以及它的幅角的大小被划归为某个小正方体（如显然，正方体的中心应该代表着该正方体。但落入正方体内的邻域像素不可能都在中心，因此我们应该对上面提到的梯度幅值做进一步处理，根据它对中心点位置的贡献大小进行加权处理，即在正方体内，根据像素点相对于正方体中心的距离，对梯度幅值做加权处理。所以三维直方图的值，即正方体的值共需要下面4 个步骤完成：1、计算落入该正方体内的邻域像素的梯度幅值A；2、根据该像素相对于关键点的距离，对A 进行高斯加权处理，得到B；3、根据该像素相对于它所在的正方体的中心的贡献大小，再对B 进行加权处理，得到C；4、对所有落入该正方体内的像素做上述处理，再进行求和运算ΣC，得到D。由于计算相对于正方体中心点的贡献大小略显繁琐，因此在实际应用中，我们需要经过坐标平移，把中心点平移到正方体的顶点上，这样只要计算正方体内的点对正方体的8 个顶点的贡献大小即可。根据三线性插值法，对某个顶点的贡献值是以该顶点和正方体内的点为对角线的两个顶点，所构成的立方体的体积。如（公式30）经过上面的三维直方图的计算，最终我们得到了该特征点的特征矢量。为了去除光照变化的影响，需要对特征矢量进行归一化处理，即：（公式31）则为归一化后的特征矢量。尽管通过归一化处理可以消除对光照变化的影响，但由于照相机饱和以及三维物体表面的不同数量不同角度的光照变化所引起的非线性光照变化仍然存在，它能够影响到一些梯度的相对幅值，但不太会影响梯度幅角。为了消除这部分的影响，我们还需要设一个t = 0.2 的阈值，保留Q 中小于0.2 的元素，而把Q 中大于0.2 的元素用0.2 替代。最后再对Q 进行一次归一化处理，以提高关键点的可区分性。备注：上面的4 描述符的建立 还未完全的理解透，所以是完整的照抄的，接下来将会分析opencv中的sift代码，加深理解，然后回来修改。2015年09月25日，第0次修改！参考资料：[1]  Lowe SIFT 原文：http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf[2]  SIFT特征提取分析 http://blog.csdn.net/abcjennifer/article/details/7639681[3]  王永明，王贵锦. 图像局部不变性特征与描述. [M] 国防工业出版社[4]  SIFT算法详解 http://blog.csdn.net/zddblog/article/details/7521424[5] sift论文翻译 http://www.cnblogs.com/cuteshongshong/archive/2012/05/25/2506374.html[6] 《Scale-space theory in computer vision  》[7] 尺度空间：维基 https://en.wikipedia.org/wiki/Scale_space[8]  赵春江, opencv2.4.9 源码分析，SIFT http://blog.csdn.net/zhaocj"}
{"content2":"今日CS.CV计算机视觉论文速览Mon, 25 Feb 2019Totally 13 papersInteresting:📚美学评定方法,基于现有的深度学习模型，直接抽取全局、局部和场景的特征进行合成，并利用支持向量机来实现美学评判，无需训练新的神经网络。（from 武汉大学）相关方法：数据集：AVA,paper and CUHKPQ神经网络生成图像评价：https://github.com/titu1994/neural-image-assessment综述文章：Image Aesthetic Assessment: An experimental survey📚基于低质量图像的三维肩部骨骼重建，基于深度学习在小数量上对肩胛骨和肱骨进行联合分割和三维重建，并利用强化学习提高了重建质量。（from 罗格斯大学）强化学习提高结果表现的流程：ref：参考文献：Multi-Atlas Segmentation with Joint Label Fusion ofOsteoporotic Vertebral Compression Fractures on CT医学三维重建：https://blog.csdn.net/shenziheng1/column/info/25449https://blog.csdn.net/weifangmql/article/details/50234787📚两篇关于粒子喷射流的文章：https://arxiv.org/pdf/1902.08570.pdfhttps://arxiv.org/pdf/1902.08276.pdf边缘卷积模块和对应的网络结构：📚基于实例洞察模型更新的跟踪系统,(from 费城 坦普尔大学)区分目标之间以及区分背景的分布：dataset：g JMC [22], AP [33], LMP [44] and AMIR [42]多目标跟踪挑战赛：https://motchallenge.net/results/MOT16/📚超光谱解混合的数据集，包含了36中混合情况，遍历了400nm-1000nm的256个波长。数据采集设备：各种混合图样（MYCB色系）：Daily Computer Vision Papers[1] Title: Towards end-to-end pulsed eddy current classification and regression with CNNAuthors:Xin Fu, Chengkai Zhang, Xiang Peng, Lihua Jian, Zheng Liu[2] **Title: Image Aesthetics Assessment Using Composite Features from off-the-Shelf Deep ModelsAuthors:Xin Fu, Jia Yan, Cien Fan[3] **Title: Effective 3D Humerus and Scapula Extraction using Low-contrast and High-shape-variability MR DataAuthors:Xiaoxiao He, Chaowei Tan, Yuting Qiao, Virak Tan, Dimitris Metaxas, Kang Li[4] Title: The Multi-Lane Capsule Network (MLCN)Authors:Vanderson Martins do Rosario, Edson Borin, Mauricio Breternitz Jr[5] **Title: A laboratory-created dataset with ground-truth for hyperspectral unmixing evaluationAuthors:Min Zhao, Jie Chen, Zhe He[6] Title: Lung Cancer Detection using Co-learning from Chest CT Images and Clinical DemographicsAuthors:Jiachen Wang, Riqiang Gao, Yuankai Huo, Shunxing Bao, Yunxi Xiong, Sanja L. Antic, Travis J. Osterman, Pierre P. Massion, Bennett A. Landman[7] **Title: Online Multi-Object Tracking with Instance-Aware Tracker and Dynamic Model RefreshmentAuthors:Peng Chu, Heng Fan, Chiu C Tan, Haibin Ling[8] Title: Blind Hyperspectral-Multispectral Image Fusion via Graph Laplacian RegularizationAuthors:Chandrajit Bajaj, Tianming Wang[9] Title: Jointly Sparse Convolutional Neural Networks in Dual Spatial-Winograd DomainsAuthors:Yoojin Choi, Mostafa El-Khamy, Jungwon Lee[10] **Title: ParticleNet: Jet Tagging via Particle CloudsAuthors:Huilin Qu, Loukas Gouskos[11] Title: Large-Scale Answerer in Questioner’s Mind for Visual Dialog Question GenerationAuthors:Sang-Woo Lee, Tong Gao, Sohee Yang, Jaejun Yoo, Jung-Woo Ha[12] Title: On the Sensitivity of Adversarial Robustness to Input Data DistributionsAuthors:Gavin Weiguang Ding, Kry Yik Chau Lui, Xiaomeng Jin, Luyu Wang, Ruitong Huang[13] **Title: End-to-End Jet Classification of Quarks and Gluons with the CMS Open DataAuthors:Michael Andrews, John Alison, Sitong An, Patrick Bryant, Bjorn Burkle, Sergei Gleyzer, Meenakshi Narain, Manfred Paulini, Barnabas Poczos, Emanuele UsaiPapers from arxiv.org更多精彩请移步主页pic from pixels.com"}
{"content2":"很多领域，SCI是王道，尤其在中国，在教师科研职称评审和学生毕业条件中都对SCI极为重视，而会议则充当了补充者的身份。但是在计算机领域，尤其是人工智能与机器学习领域里，往往研究者们更加青睐于会议我无意否认SCI期刊的价值，但是在机器学习、计算机视觉和人工智能领域，顶级会议才是王道。有人会质疑这些会议都只是EI索引的。是的，这的确非常特殊：在许多其它领域，会议都是盛会，比如society of neuroscience的会议，每次都有上万人参加，带个abstract和poster就可以去。但在所讨论的几个领域，顶级会议的重要性无论怎么强调都不为过。可以从以下几点说明：（1）因为机器学习、计算机视觉和人工智能领域发展非常迅速，新的工作层出不穷，如果把论文投到期刊上，一两年后刊出时就有点out了。因此大部分最新的工作都首先发表在顶级会议上，这些顶级会议完全能反映“热门研究方向”、“最新方法”。（2）很多经典工作大家可能引的是某顶级期刊上的论文，这是因为期刊论文表述得比较完整、实验充分。但实际上很多都是在顶级会议上首发。比如PLSA, Latent Dirichlet Allocation等。（3）如果注意这些领域大牛的pulications，不难发现他们很非常看重这些顶级会议，很多人是80%的会议+20%的期刊。即然大牛们把最新工作发在顶级会议上，有什么理由不去读顶级会议？(1)以下是不完整的列表，但基本覆盖。机器学习顶级会议：NIPS, ICML, UAI, AISTATS;  （期刊：JMLR, ML, Trends in ML, IEEE T-NN）计算机视觉和图像识别：ICCV, CVPR, ECCV;  （期刊：IEEE T-PAMI, IJCV, IEEE T-IP）人工智能：IJCAI, AAAI; （期刊AI）另外相关的还有SIGRAPH, KDD, ACL, SIGIR, WWW等。特别是，如果做机器学习，必须地，把近4年的NIPS, ICML翻几遍；如果做计算机视觉，要把近4年的ICCV, CVPR, NIPS, ICML翻几遍。(2)另外补充一下：大部分顶级会议的论文都能从网上免费下载到，如CV方面：http://www.cvpapers.com/index.html; NIPS: http://books.nips.cc/;  JMLR(期刊): http://jmlr.csail.mit.edu/papers/;  COLT和ICML(每年度的官网): http://www.cs.mcgill.ca/~colt2009/proceedings.html。希望这些信息对大家有点帮助。(3)说些自己的感受。我的研究方向主要是CV与Deep Learning，但统计学习和计算神经科学都有涉及，对Data mining和Natural Language Processing也有一定了解。这些领域，从方法和模型的角度看，统计模型（包括probabilistic graphical model和statistical learning theory）是主流也是非常有影响力的方法。有个非常明显的趋势：重要的方法和模型最先在NIPS或ICML出现，然后应用到CV,DM.NLP。虽然具体问题和应用也很重要，但多关注和结合这些方法也很有意义。对于这个领域的牛人们，以上全是浅显的废话，完全可以无视。欢迎讨论。注:NIPS = Neural Information Processing Systems  https://nips.cc/ICML = International Conference on Machine Learning https://icml.ccUAI(AUAI) =Association for Uncertainty in Artifical Intelligence http://www.auai.org/AISTATS = Artificial Intelligence and Statistics http://www.aistats.org/JMLR = Journal of Machine Learning Research http://jmlr.org/IJCAI = International Joint Conference on Artifical Intelligence http://ijcai.org/AAAI = Association for the Advancement of Aritifical Intelligence http://www.aaai.org/home.html原文地址:http://emuch.net/html/201012/2659795.html顶"}
{"content2":"最近要求关注视觉注意的“热门研究方向”、“最新方法”等。boss建议CNKI、EI、或者SCI期刊。我有点纳闷，为什么不去读顶级会议上的论文？感觉在机器学习、计算机视觉和人工智能领域，顶级会议才是王道。有人会质疑这些会议都只是EI，确实，在中国的许多其它领域的会议都是盛会，比如society of neuroscience的会议，人山人海形容也不过分。但是，计算机几个领域的确非常特殊，顶级会议的重要性无论怎么强调都不为过。可以从以下几点说明：（1）因为机器学习、计算机视觉和人工智能领域发展非常迅速，新的工作层出不穷，如果把论文投到期刊上，一两年后刊出时就有点out 了。因此大部分最新的工作都首先发表在顶级会议上，这些顶级会议完全能反映“热门研究方向”、“最新方法”。（2）很多经典工作大家可能引的是某顶级期刊 上的论文，这是因为期刊论文表述得比较完整、实验充分。但实际上很多都是在顶级会议上首发。比如PLSA, Latent Dirichlet Allocation等。（3）如果注意这些领域大牛的pulications，不难发现他们很非常看重这些顶级会议，很多人是80%的会议+20%的期刊。即然大牛们把最新工作发在顶级会议上，有什么理由不去读顶级会议？(1)以下是几个顶级会议的列表（不完整的，但基本覆盖）机器学习顶级会议：NIPS, ICML, UAI, AISTATS;  （期刊：JMLR, ML, Trends in ML, IEEE T-NN）计算机视觉和图像识别：ICCV, CVPR, ECCV;  （期刊：IEEE T-PAMI, IJCV, IEEE T-IP）人工智能：IJCAI, AAAI; （期刊AI）另外相关的还有SIGRAPH, KDD, ACL, SIGIR, WWW等。特别是，如果做机器学习，必须地，把近4年的NIPS, ICML翻几遍；如果做计算机视觉，要把近4年的ICCV, CVPR, NIPS, ICML翻几遍。(2)另外补充一下：大部分顶级会议的论文都能从网上免费下载到，比如CV方面：http://www.cvpapers.com/index.htmlNIPS: http://books.nips.cc/JMLR(期刊): http://jmlr.csail.mit.edu/papers/COLT和ICML(每年度的官网): http://www.cs.mcgill.ca/~colt2009/proceedings.html希望这些信息对大家有点帮助。(3)说些自己的感受。对计算机视觉和计算神经科学领域，从方法和模型的角度看，统计模型（包括probabilistic graphical model和statistical learning theory）是主流也是非常有影响力的方法。有个非常明显的趋势：重要的方法和模型最先在NIPS或ICML出现，然后应用到CV,IR和MM。虽然具体问题和应用也很重要，但多关注和结合这些方法也很有意义。"}
{"content2":"人工智能是计算机方面的一个很大的学科，包括的主要内容如下：（a）机器感知，就是使机器具有类似于人的感知能力，其中以机器视觉和机器听觉为主，也就是模拟人的视觉和听觉，让机器能够识别并理解文字、图像、画面，能理解语言和声音。现在有两个相关的研究领域，即模式识别与自然语言理解。（b）机器思维，对通过感知获取的信息以及从系统内部获取的各种信息进行有目的的处理。模拟人脑的思维活动。涉及的研究领域包括：知识的表示、知识的组织与管理、知识推理、各种启发式搜索、神经网络、人脑结构及工作原理。（c）机器学习，模拟人的学习过程，从书本获取知识，通过与人谈话学习知识，通过对环境的观察学习，在实践中完善等。（d）机器行为，模拟人的能力行为，说话、写字、画画、走路、进行各种操作。（e）智能系统与智能计算机的构造技术，为了实现人工智能的近期目标及远期目标，要建立智能系统及智能机器，为此需要开展对模型、系统分析与构造技术、建造工具及语言的研究。以上内容摘自王永庆教授的《人工智能原理与方法》一书。从上面描述可知，人工智能的研究内容非常广，所以你说喜欢人工智能，可能是感觉让机器像人一样具有各种能力挺有意思，实际上在具体研究的时候可能仅仅是某个领域。现在很多应用都与人工智能相关，例如邮件自动分拣系统、违章车辆拍照、工厂的一些机器人、搜索引擎、很多游戏机、微型医疗器械、无人飞机、智能玩具等等。你把人工智能的一些基本概念学完之后，可以选择一个具体的方向研究。"}
{"content2":"1、什么是基于内容的图像检索1、构建基于内容的图像检索系统步骤（1）定义你的图像描述符：在这个阶段你需要决定你想描述的图像的哪个方面。你对图像的颜色感兴趣吗？图像中物体的形状？或者你想表征纹理？（2）特征提取和索引您的数据集：  现在您已经定义了图像描述符，您的工作是将此图像描述符应用于数据集中的每个图像，从这些图像提取特征并将特征写入存储（例如，CSV文件，RDBMS ，Redis等），以便稍后可以比较它们的相似性。此外，您需要考虑是否将使用任何专门的数据结构来促进更快的搜索。（3）定义您的相似性度量：  我们现在有一个（可能是指数）特征向量的集合。但你如何比较它们的相似性呢？常用选项包括欧几里得距离，余弦距离和距离，但实际选择高度依赖于（1）数据集和（2）您提取的特征类型。（4）搜索：  最后一步是执行实际搜索。用户将向系统提交查询图像（例如从上传表单或通过移动应用程序），并且您的工作将（1）从该查询图像中提取特征，然后（2）应用您的相似性函数进行比较该查询的功能已针对已编入索引的功能。从那里，你只需根据你的相似度函数返回最相关的结果。2、CBIR和机器学习/图像分类有何不同（1）机器学习包括使计算机完成诸如预测，分类，识别等智能人工任务的方法。此外，机器学习管理算法，使计算机能够执行这些智能任务  而不需要明确编程。CBIR确实利用了一些机器学习技术 - 即降维和聚类，但是CBIR系统不执行任何实际学习。（2）主要的 区别在于CBIR不 直接试图理解和解释图像的内容。相反，CBIR系统依赖于：通过提取特征向量来量化图像。假设特征向量的比较 - 具有相似特征向量的图像具有相似的视觉内容。基于这两个组件，图像搜索引擎能够将查询与图像数据集进行比较，并返回最相关的结果，而不必实际“知道”图像的内容。（3）在机器学习和图像分类中，能够学习和理解图像的内容需要一些训练集的概念    - 一组标记数据用于教计算机数据集中每个可视对象的外观。（4）CBIR系统不需要标记数据。他们只需拍摄图像数据集，从每幅图像中提取特征，并使数据集可以在视觉上搜索。在某些方面，您可以将CBIR系统视为一种“哑”图像分类器，它没有标签概念来使自己更加智能 - 它仅依赖于（1）从图像中提取的特征和（2）相似性函数用于给用户提供有意义的结果。2、构建CBIR系统1、目录结构及作用|--- pyimagesearch|    |--- __init__.py|    |--- cbir|    |    |---- __init__.py|    |    |--- dists.py     作用：包含我们的距离度量/相似度函数，用于比较两个图像的相似度|    |    |--- hsvdescriptor.py  作用：实现我们的颜色描述符，用于从图像中提取特征向量|    |    |--- resultsmontage.py 作用：用于显示搜索到我们的屏幕结果的实用工具类|    |    |--- searcher.py 作用：将封装用于执行实际搜索的Searcher类|--- index.py 作用：用于从我们的UKBench数据集中提取特征|--- search.py  作用：将接受查询图像，调用 搜索器  ，然后将结果显示在屏幕上疑问：1、highlight=resultID in queryRelevant2、对第三条运行结果存在疑问运行命令：1、python search.py --index index.csv --dataset ../ukbench --relevant ../ukbench/relevant.json--query ../ukbench/ukbench00644.jpg2、python search.py --index index.csv --dataset ../ukbench --relevant ../ukbench/relevant.json--query ../ukbench/ukbench00996.jpg3、python search.py --index index.csv --dataset ../ukbench --relevant ../ukbench/relevant.json--query ../ukbench/ukbench00568.jpgsearch.pyfrom __future__ import print_function from pyimagesearch.cbir.resultsmontage import ResultsMontage from pyimagesearch.cbir.hsvdescriptor import HSVDescriptor from pyimagesearch.cbir.searcher import Searcher import argparse import imutils import json import cv2 ap = argparse.ArgumentParser() ap.add_argument(\"-i\", \"--index\", required = True, help = \"Path to where the features index will be stored\") ap.add_argument(\"-q\", \"--query\", required = True, help = \"Path to the query image\") ap.add_argument(\"-d\", \"--dataset\", required = True, help = \"Path to the original dataset directory\") ap.add_argument(\"-r\", \"--relevant\", required = True, help = \"Path to relevant dictionary\") args = vars(ap.parse_args()) desc = HSVDescriptor((4, 6, 3)) montage = ResultsMontage((240, 320), 5, 20) relevant = json.loads(open(args[\"relevant\"]).read()) queryFilename = args[\"query\"][args[\"query\"].rfind(\"/\") + 1:] queryRelevant = relevant[queryFilename] query = cv2.imread(args[\"query\"]) print(\"[INFO] describing query...\") cv2.imshow(\"Query\", imutils.resize(query, width = 320)) features = desc.describe(query) print(\"[INFO] searching...\") searcher = Searcher(args[\"index\"]) results = searcher.search(features, numResults = 20) for (i, (score, resultID)) in enumerate(results): print(\"[INFO] {result_num}.{result} - score:.2f\".format(result_num = i + 1, result = resultID, score = score)) result = cv2.imread(\"{}/{}\".format(args[\"dataset\"], resultID)) print (\"resultID\") print (resultID) montage.addResult(result, text = \"#{}\".format(i + 1), highlight = resultID in queryRelevant) cv2.imshow(\"Results\", imutils.resize(montage.montage, height = 700)) cv2.imwrite(\"mo.png\",montage.montage) cv2.waitKey(0)index.pyfrom __future__ import print_function from pyimagesearch.cbir.hsvdescriptor import HSVDescriptor from imutils import paths import progressbar import argparse import cv2 ap= argparse.ArgumentParser() ap.add_argument(\"-d\", \"--dataset\", required = True, help = \"Path to the directory that contains the images to be indexed\") ap.add_argument(\"-i\", \"--index\", required = True, help = \"Path to where the features index will be stored\") args = vars(ap.parse_args()) desc = HSVDescriptor((4, 6, 3)) output = open(args[\"index\"], \"w\") imagePaths = list(paths.list_images(args[\"dataset\"])) widgets = [\"Indexing:\", progressbar.Percentage(), \"\", progressbar.Bar(), \"\", progressbar.ETA()] pbar = progressbar.ProgressBar(maxval = len(imagePaths), widgets = widgets) pbar.start() for (i, imagePath) in enumerate(imagePaths): filename = imagePath[imagePath.rfind(\"/\") + 1:] image = cv2.imread(imagePath) features = desc.describe(image) features = [str(x) for x in features] output.write(\"{}, {}\\n\".format(filename, \",\".join(features))) pbar.update(i) pbar.finish() print(\"[INFO] indexed {} images\".format(len(imagePaths))) output.close()dist.pyimport numpy as np def chi2_distance(histA, histB, eps = 1e-10): d = 0.5 * np.sum(((histA - histB)**2)/(histA + histB + eps)) return dhsvdescriptor.pyimport numpy as np import cv2 import imutils class HSVDescriptor: def __init__(self, bins): self.bins = bins def describe(self, image): image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV) features = [] (h, w) = image.shape[:2] (cX, cY) = (int(w * 0.5), int(h * 0.5)) segments = [(0, cX, 0, cY), (cX, w, 0, cY), (cX, w, cY, h), (0, cX, cY, h)] (axesX, axesY) = (int(w * 0.75)//2, int(h * 0.75)//2) ellipMask = np.zeros(image.shape[:2], dtype = \"uint8\") cv2.ellipse(ellipMask, (cX, cY), (axesX, axesY), 0, 0, 360, 255, - 1) for (startX, endX, startY, endY) in segments: cornerMask = np.zeros(image.shape[:2], dtype = \"uint8\") cv2.rectangle(cornerMask, (startX, startY), (endX, endY), 255, - 1) cornerMask = cv2.subtract(cornerMask, ellipMask) hist = self.histogram(image, cornerMask) features.extend(hist) hist = self.histogram(image, ellipMask) features.extend(hist) return np.array(features) def histogram(self, image, mask = None): hist = cv2.calcHist([image], [0, 1, 2], mask, self.bins, [ 0 , 180, 0, 256, 0, 256]) if imutils.is_cv2(): hist = cv2.normalize(hist).flatten() else: hist = cv2.normalize(hist, hist).flatten() return histresultsmontage.pyimport numpy as np import cv2 class ResultsMontage: def __init__(self, imageSize, imagesPerRow, numResults): self.imageW = imageSize[0] self.imageH = imageSize[1] self.imagesPerRow = imagesPerRow numCols = numResults // imagesPerRow self.montage = np.zeros((numCols * self.imageW, imagesPerRow * self.imageH, 3), dtype=\"uint8\") self.counter = 0 self.row = 0 self.col = 0 def addResult(self, image, text = None, highlight = False): if self.counter != 0 and self.counter %self.imagesPerRow == 0: self.col = 0 self.row += 1 image = cv2.resize(image, (self.imageH, self.imageW)) (startY, endY) = (self.row * self.imageW, (self.row + 1) * self.imageW) (startX, endX) = (self.col * self.imageH, (self.col + 1) * self.imageH) self.montage[startY:endY, startX:endX] = image if text is not None: cv2.putText(self.montage, text, (startX + 10, startY + 30), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 255), 3) print (\"text\") if highlight: cv2.rectangle(self.montage, (startX + 3, startY + 3), (endX - 3, endY - 3), (0, 255, 0), 4) print (\"hig\") self.col += 1 self.counter +=1searcher.pyfrom . import dists import csv class Searcher: def __init__(self, dbPath): self.dbPath = dbPath def search(self, queryFeatures, numResults = 10): results = {} with open(self.dbPath) as f: reader = csv.reader(f) for row in reader: features = [float(x) for x in row[1:]] d = dists.chi2_distance(features, queryFeatures) results[row[0]] = d f.close() results=sorted([(v,k) for (k,v) in results.items()]) return results[:numResults]2、特征提取和索引1、描述图像的三个方面：颜色：  表征图像颜色的图像描述符试图模拟图像每个通道中像素强度的分布。这些方法包括基本颜色统计，如平均值，标准偏差和偏度，以及颜色直方图，“平面”和多维。纹理：  纹理描述符试图模拟图像中物体的感觉，外观和整体触觉质量。一些（但不是全部）纹理描述符将图像转换为灰度，然后计算灰度共生矩阵（GLCM）并计算该矩阵的统计量，包括对比度，相关性和熵等（Haralick纹理）。更先进的纹理描述符，如局部二进制模式，尝试模型  模式也是如此。甚至还有更高级的纹理描述符，例如傅立叶和小波变换也存在，但仍然使用灰度图像。形状：  绝大多数形状描述符方法依靠提取图像中对象的轮廓（即轮廓）。一旦我们有了轮廓，我们就可以计算简单的统计数据来表征轮廓，这正是Hu Moments和Zernike Moments所做的。这些统计数据可用于表示图像中对象的形状（轮廓）。在机器学习和对象识别的背景下，  面向梯度的直方图 也是一个不错的选择。2、特征提取的定义定义： 特征提取 是通过应用图像描述符从数据集中每个图像中提取特征来量化数据集的过程。通常，这些功能存储在磁盘上供  以后使用，并  使用专门的数据结构（例如倒排索引，kd树或随机投影林）进行索引，以加快查询速度。3、定义相似度1、常用距离度量欧几里德：from scipy.spatial import distance as dists dists.euclidean(A, B)曼哈顿/城市大厦dists.cityblock(A, B)直方图交点def histogram_intersection(H1, H2): return np.sum(np.minimum(H1, H2))距离def chi2_distance(histA, histB, eps=1e-10): return 0.5 * np.sum(((histA - histB) ** 2) / (histA + histB + eps)) chi2_distance(A, B)余弦dists.cosine(A, B)海明dists.hamming(A, B)4、提取关键点和局部不变描述符1、文件结构及作用：|--- pyimagesearch|    |--- __init__.py|    |--- descriptors 作用：包含了实现从我们的图像数据集提取关键点和本地不变描述|    |    |---- __init__.py|    |    |--- detectanddescribe.py 作用：以便使用任意检测器和描述符轻松检测关键点并提取功能|    |--- indexer 作用：包含我们面向HDF5数据集的面向对象的接口来存储特征|    |    |---- __init__.py|    |    |--- baseindexer.py|    |    |--- featureindexer.py|--- index_features.py 作用：驱动程序脚本，用于将所有碎片粘合在一起疑问：1、@staticmethod静态函数定义的方法？运行命令：python index_features.py --dataset ../ukbench_sample --features-db output/features.hdf5index_features.py#coding=utf-8 from __future__ import print_function from pyimagesearch.descriptors.detectanddescribe import DetectAndDescribe from pyimagesearch.indexer.featureindexer import FeatureIndexer from imutils.feature import FeatureDetector_create, DescriptorExtractor_create from imutils import paths import argparse import imutils import cv2 ap = argparse.ArgumentParser() ap.add_argument(\"-d\", \"--dataset\", required=True, help=\"Path to the directory that contains the images to be indexed\") #图像目录路径 ap.add_argument(\"-f\", \"--features-db\", required=True, help=\"Path to where the features database will be stored\")#制定HDF5数据库储存在磁盘上的路径 ap.add_argument(\"-a\", \"--approx-images\", type=int, default=500, help=\"Approximate # of images in the dataset\")#该（可选）开关允许我们指定数据集中图像的近似数量 ap.add_argument(\"-b\", \"--max-buffer-size\", type=int, default=50000, help=\"Maximum buffer size for # of features to be stored in memory\")#一次一个地写入HDF5的特征向量 效率非常低。相反，将特征向量收集到内存中的一个大数组中然后在缓冲区满时将它们转储到HDF5会更有效。的值 -最大-缓冲器-大小 指定许多如何 特征向量可以被存储在存储器中，直到缓冲器被刷新到HDF5 args = vars(ap.parse_args()) detector = FeatureDetector_create(\"SURF\")#获取关键点 descriptor = DescriptorExtractor_create(\"RootSIFT\")#定义提取关键点特征方法 dad = DetectAndDescribe(detector, descriptor)#获取关键点和关键点特征向量 fi = FeatureIndexer(args[\"features_db\"], estNumImages=args[\"approx_images\"],maxBufferSize=args[\"max_buffer_size\"], verbose=True) for (i, imagePath) in enumerate(paths.list_images(args[\"dataset\"])): if i > 0 and i%10 == 0: fi._debug(\"processed {} images\".format(i), msgType = \"[PROGRESS]\") filename = imagePath[imagePath.rfind(\"/\") + 1:] image = cv2.imread(imagePath) image = imutils.resize(image, width = 320) image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) (kps, descs) = dad.describe(image) if kps is None or descs is None: continue fi.add(filename, kps, descs) fi.finish()detectanddescribe.pyimport numpy as np class DetectAndDescribe: def __init__(self, detector, descriptor): self.detector = detector self.descriptor = descriptor def describe(self, image, useKpList = True): kps = self.detector.detect(image) (kps, descs) = self.descriptor.compute(image, kps) if len(kps) == 0: return (None, None) if useKpList: kps = np.int0([kp.pt for kp in kps]) return (kps, descs)baseindexer.pyfrom __future__ import print_function import numpy as np import datetime class BaseIndexer(object): def __init__(self, dbPath, estNumImages = 500, maxBufferSize = 50000, dbResizeFactor = 2, verbose = True): self.dbPath = dbPath self.estNumImages = estNumImages self.maxBufferSize = maxBufferSize self.dbResizeFactor = dbResizeFactor self.verbose = verbose self.idxs = {} def _wrieBuffers(self): pass def _writeBuffer(self, dataset, datasetName, buf, idxName, sparse = False): if type(buf) is list: end = self.idxs[idxName] + len(buf) else: end = self.idxs[idxName] + buf.shape[0] if end > dataset.shape[0]: self._debug(\"triggering '{}' db resize\".format(datasetName)) self._resizeDataset(dataset, datasetName, baseSize = end) if sparse: buf = buf.toarray() self._debug(\"writing '{}' buffer\".format(datasetName)) dataset[self.idxs[idxName]:end] = buf def _resizeDataset(self, dataset, dbName, baseSize = 0, finished = 0): origSize = dataset.shape[0] if finished > 0: newSize = finished else: newSize = baseSize * self.dbResizeFactor shape = list(dataset.shape) shape[0] = newSize dataset.resize(tuple(shape)) self._debug(\"old size of '{}':{:,};new size:{:,}\".format(dbName, origSize, newSize)) def _debug(self, msg, msgType = \"[INFO]\"): if self.verbose: print(\"{} {} - {}\".format(msgType, msg, datetime.datetime.now())) @staticmethod def featureStack(array, accum = None, stackMethod = np.vstack): if accum is None: accum = array else: accum = stackMethod([accum, array]) return accumfeatureindexer.py#coding=utf-8 from .baseindexer import BaseIndexer import numpy as np import h5py import sys class FeatureIndexer(BaseIndexer): def __init__(self, dbPath, estNumImages=500, maxBufferSize=50000, dbResizeFactor=2,verbose=True): super(FeatureIndexer, self).__init__(dbPath, estNumImages=estNumImages, maxBufferSize=maxBufferSize, dbResizeFactor=dbResizeFactor, verbose=verbose) self.db = h5py.File(self.dbPath, mode=\"w\") self.imageIDDB = None self.indexDB = None self.featuresDB = None self.imageIDBuffer = [] self.indexBuffer = [] self.featuresBuffer = None self.totalFeatures = 0 self.idxs = {\"index\": 0, \"features\": 0} def add(self, imageID, kps, features): start = self.idxs[\"features\"] + self.totalFeatures end = start + len(features) self.imageIDBuffer.append(imageID) self.featuresBuffer = BaseIndexer.featureStack(np.hstack([kps, features]),self.featuresBuffer) self.indexBuffer.append((start, end)) self.totalFeatures += len(features) if self.totalFeatures >= self.maxBufferSize: if None in (self.imageIDDB, self.indexDB, self.featuresDB): self._debug(\"initial buffer full\") self._createDatasets() self._writeBuffers() def _createDatasets(self): avgFeatures = self.totalFeatures/float(len(self.imageIDBuffer)) approxFeatures = int(avgFeatures * self.estNumImages) fvectorSize = self.featuresBuffer.shape[1] if sys.version_info[0] < 3: dt = h5py.special_dtype(vlen = unicode) else: dt = h5py.special_dtype(vlen = str) self._debug(\"creating datasets...\") self.imageIDDB = self.db.create_dataset(\"image_ids\", (self.estNumImages, ), maxshape = (None, ), dtype = dt) self.indexDB = self.db.create_dataset(\"index\", (self.estNumImages, 2), maxshape = (None, 2), dtype = \"int\") self.featuresDB = self.db.create_dataset(\"features\", (approxFeatures, fvectorSize), maxshape = (None, fvectorSize), dtype = \"float\") def _writeBuffers(self): self._writeBuffer(self.imageIDDB, \"image_ids\", self.imageIDBuffer,\"index\") self._writeBuffer(self.indexDB, \"index\", self.indexBuffer, \"index\") self._writeBuffer(self.featuresDB, \"features\", self.featuresBuffer,\"features\") self.idxs[\"index\"] += len(self.imageIDBuffer) self.idxs[\"features\"] += self.totalFeatures self.imageIDBuffer = [] self.indexBuffer = [] self.featuresBuffer = None self.totalFeatures = 0 def finish(self): if None in (self.imageIDDB, self.indexDB, self.featuresDB): self._debug(\"minimum init buffer not reached\", msgType = \"[WARN]\") self._createDatasets() self._debug(\"writing un - empty buffers...\") self._writeBuffers() self._debug(\"compacting datasets...\") self._resizeDataset(self.imageIDDB, \"image_ids\", finished = self.idxs[\"index\"]) self._resizeDataset(self.indexDB, \"index\", finished = self.idxs[\"index\"]) self._resizeDataset(self.featuresDB, \"features\", finished = self.idxs[\"features\"]) self.db.close()5、集群功能组成一个码本1、文件结构及作用多添加俩个新文件：cluster_features.py和vocabulary.py|--- pyimagesearch|    |--- __init__.py|    |--- descriptors|    |    |---- __init__.py|    |    |--- detectanddescribe.py|    |--- indexer|    |    |---- __init__.py|    |    |--- baseindexer.py|    |    |--- featureindexer.py|    |--- ir|    |    |---- __init__.py|    |    |--- vocabulary.py 作用：用于摄取的功能HDF5数据集，然后返回一个字典的视觉（即聚类中心）话|--- cluster_features.py 作用：驱动脚本将启动 词汇表   制定过程|--- index_features.py运行命令：python cluster_features.py --features-db output/features.hdf5 --codebook output/vocab.cpickle --clusters 1536 --percentage 0.25cluster_features.py#coding=utf-8 from __future__ import print_function from pyimagesearch.ir.vocabulary import Vocabulary import argparse import pickle ap= argparse.ArgumentParser() ap.add_argument(\"-f\", \"--features-db\", required = True, help = \"Path to where the features database will be stored\") ap.add_argument(\"-c\", \"--codebook\", required = True, help = \"Path to the output codebook\") ap.add_argument(\"-k\", \"--clusters\", type = int, default = 64, help = \"# of clusters to generate\")#小批量k-均值将生成的簇的数量（即可视词） ap.add_argument(\"-p\", \"--percentage\",type = float, default = 0.25, help = \"Percentage of total features to use when clustering\" ) args = vars(ap.parse_args()) # 控制特征向量样本量的大小 voc = Vocabulary(args[\"features_db\"]) vocab = voc.fit(args[\"clusters\"], args[\"percentage\"]) #获得生成的视觉词 - 整个采样和聚类过程由词汇 类抽象 print (\"[INFO] storing cluster centers...\") f = open(args[\"codebook\"], \"wb\") f.write(pickle.dumps(vocab)) f.close()vocabulary.pyfrom __future__ import print_function from sklearn.cluster import MiniBatchKMeans import numpy as np import datetime import h5py class Vocabulary: def __init__(self, dbPath, verbose = True): self.dbPath = dbPath self.verbose = verbose def fit(self, numClusters, samplePercent, randomState = None): db = h5py.File(self.dbPath) totalFeatures = db[\"features\"].shape[0] sampleSize = int(np.ceil(samplePercent * totalFeatures)) print(\"sampleSize\") print (sampleSize) idxs = np.random.choice(np.arange(0, totalFeatures), (sampleSize), replace = False) idxs.sort() data = [] self._debug(\"starting sampling...\") for i in idxs: data.append(db[\"features\"][i][2:]) self._debug(\"sampled {:,} features from a population of {:,}\".format(len(idxs), totalFeatures)) self._debug(\"clustering with k = {:,}\".format(numClusters)) clt = MiniBatchKMeans(n_clusters = numClusters, random_state = randomState) clt.fit(data) self._debug(\"cluster shape:{}\".format(clt.cluster_centers_.shape)) db.close() return clt.cluster_centers_ def _debug(self, msg, msgType = \"[INFO]\"): if self.verbose: print(\"{} {} - {}\".format(msgType, msg, datetime.datetime.now()))6、可视化码本中的单词注：在前面构建出关键点和局部不变特征描述符、建立集群码本后方可实现作用：将码本中K-means分类后的直方图，转化为可视化图片visuluze_centers.py#coding=utf-8 from __future__ import print_function from pyimagesearch.resultsmontage import ResultsMontage from sklearn.metrics import pairwise import numpy as np import progressbar import argparse import pickle import h5py import cv2 ap = argparse.ArgumentParser() ap.add_argument(\"-d\", \"--dataset\", required = True, help = \"Path to the directory of indexed images\") ap.add_argument(\"-f\", \"--features-db\", required = True, help = \"Path to the features database\")#提取关键和局部不变描述符的数据记路经 ap.add_argument(\"-c\", \"--codebook\", required = True, help = \"Path to the codebook\")#可视话词汇表中的视觉词汇路经 ap.add_argument(\"-o\", \"--output\", required = True, help = \"Path to output directory\") args = vars(ap.parse_args()) vocab = pickle.loads(open(args[\"codebook\"], \"rb\").read()) featuresDB = h5py.File(args[\"features_db\"], mode = \"r\") print(\"[INFO] starting distance distance computations...\") vis = {i:[] for i in np.arange(0, len(vocab))} widgets = [\"Comparing:\", progressbar.Percentage(), \"\", progressbar.Bar(), \"\", progressbar.ETA()] pbar = progressbar.ProgressBar(maxval = featuresDB[\"image_ids\"].shape[0], widgets = widgets).start() for (i, imageID) in enumerate(featuresDB[\"image_ids\"]): (start, end) = featuresDB[\"index\"][i] rows = featuresDB[\"features\"][start:end] (kps, descs) = (rows[:, :2], rows[:, 2:]) for (kp, features) in zip(kps, descs): features = features.reshape(1, - 1) D = pairwise.euclidean_distances(features, Y = vocab)[0] for j in np.arange(0, len(vocab)): topResults = vis.get(j) topResults.append((D[j], kp, imageID)) topResults = sorted(topResults, key = lambda r:r[0])[:16] vis[j] = topResults pbar.update(i) pbar.finish() featuresDB.close() print(\"[INFO] writing visualizations to file...\") for (vwID, results) in vis.items(): montage = ResultsMontage((64, 64), 4, 16) for (_, (x, y), imageID) in results: p = \"{}/{}\".format(args[\"dataset\"], imageID) image = cv2.imread(p) (h, w) = image.shape[:2] (startX, endX) = (max(0, int(x) - 32), min(w, int(x) + 32)) (startY, endY) = (max(0, int(y) - 32), min(h, int(y) + 32)) roi = image[startY:endY, startX, endX] montage.addResult(roi) p = \"{}/vis_{}.jpg\".format(args[\"output\"], vwID) cv2.imwrite(p, cv2.cvtColor(montage.montage, cv2.COLOR_BGR2GRAY))7、矢量量化7.1、从多个特征到单个直方图bagofvisualwords.pyfrom sklearn.metrics import pairwise from scipy.sparse import csr_matrix import numpy as np class BagOfVisualWords: def __init__(self, codebook, sparse = True): self.codebook = codebook self.sparse = sparse def describe(self, features): D = pairwise.euclidean_distances(features, Y = self.codebook) (words, counts) = np.unique(np.argmin(D, axis = 1), return_counts = True) if self.sparse: hist = csr_matrix((counts, (np.zeros((len(words), )), words)), shape = (1, len(self.codebook)), dtype = \"float\") else: hist = np.zeros((len(self.codebook), ), dtype = \"float\") hist[words] = counts return histquantize_example.pyfrom __future__ import print_function from pyimagesearch.ir.bagofvisualwords import BagOfVisualWords from sklearn.metrics import pairwise import numpy as np np.random.seed(42) vocab = np.random.uniform(size = (3, 6)) features = np.random.uniform(size = (10, 6)) print(\"[INFO] vocabulary:\\n{}\\n\".format(vocab)) print(\"[INFO] features:\\n{}\\n\".format(features)) hist = np.zeros((3,), dtype = \"int32\") for (i, f) in enumerate(features): D = pairwise.euclidean_distances(f.reshape(1, -1), Y = vocab) j = np.argmin(D) print(\"[INFO] Closest visual word to feature #{}:{}\".format(i, j)) hist[j] += 1 print(\"[INFO] Updated histogram:{}\".format(hist)) bovw = BagOfVisualWords(vocab, sparse = False) hist = bovw.describe(features) print(\"[INFO] BOVW histogram:{}\".format(hist))7.2、形成BOVW运行命令：python extract_bovw.py --features-db output/features.hdf5 --codebook output/vocab.cpickle  --bovw-db output/bovw.hdf5 --idf output/idf.cpickleextract_bovw.pyfrom pyimagesearch.ir.bagofvisualwords import BagOfVisualWords from pyimagesearch.indexer.bovwindexer import BOVWIndexer import argparse import pickle import h5py ap = argparse.ArgumentParser() ap.add_argument(\"-f\", \"--features-db\", required = True, help = \"Path to the features database\") #关键点和局部不变描述符课程中构建的HDF5数据集的路径。该数据库应包含与数据集中每个图像相关的图像ID，索引和原始关键点/特征向量 ap.add_argument(\"-c\", \"--codebook\", required = True, help = \"Path to the codebook\")#我们需要可视化码本的路径 ap.add_argument(\"-b\", \"--bovw-db\", required = True, help = \"Path to where the bag-of-visual-words database will be stored\")#将BOVW表示存储在一个单独的HDF5数据库中的路经 ap.add_argument(\"-d\", \"--idf\", required = True, help = \"Path to inverse document frequency counts will be stored\") ap.add_argument(\"-s\", \"--max-buffer-size\", type = int, default = 500, help = \"Maximum buffer size for # of features to be stored in memory\")#在写入HDF5数据集之前在内存中管理一个原始特征向量的缓冲区，我们将在这里做同样的事情 - 这次我们将管理一个BOVW直方图缓冲区 args = vars(ap.parse_args()) vocab = pickle.loads(open(args[\"codebook\"], \"rb\").read()) bovw = BagOfVisualWords(vocab) featuresDB = h5py.File(args[\"features_db\"], mode = \"r\") bi = BOVWIndexer(bovw.codebook.shape[0], args[\"bovw_db\"], estNumImages = featuresDB[\"image_ids\"].shape[0], maxBufferSize = args[\"max_buffer_size\"]) for (i, (imageID, offset)) in enumerate(zip(featuresDB[\"image_ids\"], featuresDB[\"index\"])): if i > 0 and i%10 == 0: bi._debug(\"processed {} images\".format(i), msgType = \"[PROGRESS]\") features = featuresDB[\"features\"][offset[0]:offset[1]][:, 2:] hist = bovw.describe(features) bi.add(hist) featuresDB.close() bi.finish() f = open(args[\"idf\"], \"wb\") f.write(pickle.dumps(bi.df(method = \"idf\"))) f.close()bovwindexer.pyfrom .baseindexer import BaseIndexer from scipy import sparse import numpy as np import h5py class BOVWIndexer(BaseIndexer): def __init__(self, fvectorSize, dbPath, estNumImages = 500, maxBufferSize = 500, dbResizeFactor = 2, verbose = True): super(BOVWIndexer, self).__init__(dbPath, estNumImages = estNumImages, maxBufferSize = maxBufferSize, dbResizeFactor = dbResizeFactor, verbose = verbose) self.db = h5py.File(self.dbPath, mode = \"w\") self.bovwDB = None self.bovwBuffer = None self.idxs = {\"bovw\":0} self.fvectorSize = fvectorSize self._df = np.zeros((fvectorSize, ), dtype = \"float\") self.totalImages = 0 def add(self, hist): self.bovwBuffer = BaseIndexer.featureStack(hist, self.bovwBuffer, stackMethod = sparse.vstack) self._df[np.where(hist.toarray()[0] > 0)] += 1 if self.bovwBuffer.shape[0] >= self.maxBufferSize: if self.bovwDB is None: self._debug(\"initial buffer full\") self._createDatasets() self._writeBuffers() def _writeBuffers(self): if self.bovwBuffer is not None and self.bovwBuffer.shape[0] > 0: self._writeBuffer(self.bovwDB, \"bovw\", self.bovwBuffer, \"bovw\", sparse = True) self.idxs[\"bovw\"] += self.bovwBuffer.shape[0] self.bovwBuffer = None def _createDatasets(self): self._debug(\"creating datasets...\") self.bovwDB = self.db.create_dataset(\"bovw\", (self.estNumImages, self.fvectorSize), maxshape = (None, self.fvectorSize), dtype = \"float\") def finish(self): if self.bovwDB is None: self._debug(\"minimum init buffer not reached\", msgType = \"[WARN]\") self._createDatasets() self._debug(\"writing un-empty buffers...\") self._writeBuffers() self._debug(\"compacting datasets...\") self._resizeDataset(self.bovwDB, \"bovw\", finished = self.idxs[\"bovw\"]) self.totalImage = self.bovwDB.shape[0] self.db.close() def df(self, method = None): if method == \"idf\": return np.log(self.totalImages/(1.0 + self._df)) return sel._df8、反转索引和搜索8.1、建立倒排索引1、文件结构|--- pyimagesearch|    |---- __init__.py|    |--- db|    |    |---- __init__.py|    |    |--- redisqueue.py|    |--- descriptors|    |    |---- __init__.py|    |    |--- detectanddescribe.py|    |--- indexer|    |    |---- __init__.py|    |    |---- baseindexer.py|    |    |---- bovwindexer.py|    |    |---- featureindexer.py|    |--- ir|    |    |---- __init__.py|    |    |---- bagofvisualwords.py|    |    |---- vocabulary.py|--- build_redis_index.py|--- cluster_features.py|--- extract_bvow.py|--- index_features.py|--- visualize_centers.py运行命令：redisqueue.pyimport numpy as np class RedisQueue: def __init__(self, redisDB): self.redisDB = redisDB def add(self, imageIdx, hist):#imageIdx：image_ids中HDF5数据集中图像的索引。hist:从图像中提取的BOVW直方图 p = self.redisDB.pipeline() for i in np.where(hist > 0)[0]: p.rpush(\"vw:{}\".format(i), imageIdx) p.execute() def finish(self): self.redisDB.save()build_redis_index.pyfrom __future__ import print_function from pyimagesearch.db.redisqueue import RedisQueue from redis import Redis import h5py import argparse ap = argparse.ArgumentParser() ap.add_argument(\"-b\", \"--bovw-db\", required = True, help = \"Path to where the bag-of-visual-words database\") args = vars(ap.parse_args()) redisDB = Redis(host = \"localhost\", port = 6379, db = 0) rq = RedisQueue(redisDB) bovwDB = h5py.File(args[\"bovw_db\"], mode = \"r\") for (i, hist) in enumerate(bovwDB[\"bovw\"]): if i > 0 and i%10 == 0: print(\"[PROGRESS] processed {} entries\".format(i)) rq.add(i, hist) bovwDB.close() rq.finish()8.2 执行搜索文件目录结构：|--- pyimagesearch|    |---- __init__.py|    |--- db|    |    |---- __init__.py|    |    |--- redisqueue.py|    |--- descriptors|    |    |---- __init__.py|    |    |--- detectanddescribe.py|    |--- indexer|    |    |---- __init__.py|    |    |---- baseindexer.py|    |    |---- bovwindexer.py|    |    |---- featureindexer.py|    |--- ir|    |    |---- __init__.py|    |    |---- bagofvisualwords.py|    |    |---- vocabulary.py|    |    |---- dists.py|    |    |---- searcher.py|    |    |---- searchresult.py|--- build_redis_index.py|--- cluster_features.py|--- extract_bvow.py|--- index_features.py|--- visualize_centers.py|--- search.py运行命令：python search.py --dataset ../ukbench --features-db output/features.hdf5 --bovw-db output/bovw.hdf5 --codebook output/vocab.cpickle --relevant ../ukbench/relevant.json --query ../ukbench/ukbench00258.jpgsearch.pyfrom __future__ import print_function from pyimagesearch.descriptors.detectanddescribe import DetectAndDescribe from pyimagesearch.ir.bagofvisualwords import BagOfVisualWords from pyimagesearch.ir.searcher import Searcher from pyimagesearch.ir.dists import chi2_distance from pyimagesearch.resultsmontage import ResultsMontage from scipy.spatial import distance from redis import Redis from imutils.feature import FeatureDetector_create, DescriptorExtractor_create import argparse import pickle import imutils import json import cv2 ap = argparse.ArgumentParser() ap.add_argument(\"-d\", \"--dataset\", required = True, help = \"Path to the directory of indexed images\") ap.add_argument(\"-f\", \"--features-db\", required = True, help = \"Path to the features database\") ap.add_argument(\"-b\", \"--bovw-db\", required = True, help = \"Path to the bag-of-visual-words database\") ap.add_argument(\"-c\", \"--codebook\", required = True, help = \"Path to relevant dictionary\") ap.add_argument(\"-i\", \"--idf\", type = str, help = \"Path to inverted document frequencies array\") ap.add_argument(\"-r\", \"--relevant\", required = True, help = \"Path to relevant dictionary\") ap.add_argument(\"-q\", \"--query\", required = True, help = \"Path to the query image\") args = vars(ap.parse_args()) detector = FeatureDetector_create(\"SURF\") descriptor = DescriptorExtractor_create(\"RootSIFT\") dad = DetectAndDescribe(detector, descriptor) distanceMetric = chi2_distance idf = None if args[\"idf\"] is not None: idf = pickle.loads(open(args[\"idf\"], \"rb\").read()) vocab = pickle.loads(open(args[\"codebook\"], \"rb\").read()) bovw = BagOfVisualWords(vocab) relevant = json.loads(open(args[\"relevant\"]).read()) queryFilename = args[\"query\"][args[\"query\"].rfind(\"/\") + 1:] queryRelevant = relevant[queryFilename] queryImage = cv2.imread(args[\"query\"]) queryImage = imutils.resize(queryImage, width = 320) queryImage = cv2.cvtColor(queryImage, cv2.COLOR_BGR2GRAY) (_, descs) = dad.describe(queryImage) hist = bovw.describe(descs).tocoo() redisDB = Redis(host = \"localhost\", port = 6379, db = 0) searcher = Searcher(redisDB, args[\"bovw_db\"], args[\"features_db\"], idf = idf, distanceMetric = distanceMetric) sr = searcher.search(hist, numResults = 20) print(\"[INFO] search took:{:.2f}s\".format(sr.search_time)) montage = ResultsMontage((240, 320), 5, 20) for (i, (score, resultID, resultIdx)) in enumerate(sr.results): print(\"[RESULT] {result_num}.{result} -{score:.2f}\".format(result_num = i + 1, result = resultID, score = score)) result = cv2.imread(\"{}/{}\".format(args[\"dataset\"], resultID)) montage.addResult(result, text = \"#{}\".format(i + 1), highlight = resultID in queryRelevant) cv2.imshow(\"Result\", imutils.resize(montage.montage, height = 700)) cv2.waitKey(0) searcher.finish()searcher.pyfrom .searchresult import SearchResult from .dists import chi2_distance import numpy as np import datetime import h5py class Searcher: def __init__(self, redisDB, bovwDBPath, featuresDBPath, idf = None, distanceMetric = chi2_distance): self.redisDB = redisDB self.idf = idf self.distanceMetric = distanceMetric self.bovwDB = h5py.File(bovwDBPath, mode = \"r\") self.featuresDB = h5py.File(featuresDBPath, \"r\") def search(self, queryHist, numResults = 10, maxCandidates = 200): startTime = datetime.datetime.now() candidateIdxs = self.buildCandidates(queryHist, maxCandidates) candidateIdxs.sort() hists = self.bovwDB[\"bovw\"][candidateIdxs] queryHist = queryHist.toarray() results = {} if self.idf is not None: queryHist *= self.idf for (candidate, hist) in zip(candidateIdxs, hists): if self.idf is not None: hist *=self.idf d = self.distanceMetric(hist, queryHist) results[candidate] = d results = sorted([(v, self.featuresDB[\"image_ids\"][k], k) for (k, v) in results.items()]) results = results = results[:numResults] return SearchResult(results, (datetime.datetime.now() - startTime).total_seconds()) def buildCandidates(self, hist, maxCandidates): p = self.redisDB.pipeline() for i in hist.col: p.lrange(\"vw:{}\".format(i), 0, -1) pipelineResults = p.execute() candidates = [] for results in pipelineResults: results = [int(r) for r in results] candidates.extend(results) (imageIdxs, counts) = np.unique(candidates, return_counts = True) imageIdxs = [i for (c, i) in sorted(zip(counts, imageIdxs), reverse = True)] return imageIdxs[:maxCandidates] def finish(self): self.bovwDB.close() self.featuresDB.close()dists.pyimport numpy as np def chi2_distance(histA, histB, eps = 1e-10): d = 0.5 * np.sum(((histA - histB)**2)/(histA + histB + eps)) return dsearchersult.pyfrom collections import namedtuple SearchResult = namedtuple(\"SearchResult\", [\"results\", \"search_time\"])9、评估evaluate.pyfrom __future__ import print_function from pyimagesearch.descriptors.detectanddescribe import DetectAndDescribe from pyimagesearch.ir.bagofvisualwords import BagOfVisualWords from pyimagesearch.ir.searcher import Searcher from pyimagesearch.ir.dists import chi2_distance from scipy.spatial import distance from redis import Redis from imutils.feature import FeatureDetector_create, DescriptorExtractor_create import numpy as np import progressbar import argparse import pickle import imutils import json import cv2 ap = argparse.ArgumentParser() ap.add_argument(\"-d\", \"--dataset\", required = True, help = \"Path to the directory of indexed images\") ap.add_argument(\"-f\", \"--features-db\", required = True, help = \"Path to the features database\") ap.add_argument(\"-b\", \"--bovw-db\", required=True, help = \"Path to the bag-of-visual-words database\") ap.add_argument(\"-c\", \"--codebook\", required = True, help = \"Path to the codebook\") ap.add_argument(\"-i\", \"--idf\", type = str, help = \"Path to inverted document frequencies array\") ap.add_argument(\"-r\", \"--relevant\", required = True, help = \"Path to relevant dictionary\") args = vars(ap.parse_args()) detector = FeatureDetector_create(\"SURF\") descriptor = DescriptorExtractor_create(\"RootSIFT\") dad = DetectAndDescribe(detector, descriptor) distanceMetric = chi2_distance idf = None if args[\"idf\"] is not None: idf = pickle.loads(open(args[\"idf\"], \"rb\").read()) distanceMetric = distance.cosine vocab = pickle.loads(open(args[\"codebook\"], \"rb\").read()) bovw = BagOfVisualWords(vocab) redisDB = Redis(host = \"localhost\", port = 6379, db = 0) searcher = Searcher(redisDB, args[\"bovw_db\"], args[\"features_db\"], idf = idf, distanceMetric = distanceMetric) relevant = json.loads(open(args[\"relevant\"]).read()) queryIDs = relevant.keys() accuracies = [] timings = [] widgets = [\"Evaluating:\", progressbar.Percentage(), \"\", progressbar.Bar(), \"\", progressbar.ETA()] pbar = progressbar.ProgressBar(maxval = len(queryIDs), widgets = widgets).start() for (i, queryID) in enumerate(sorted(queryIDs)): queryRelevant = relevant[queryID] p = \"{}/{}\".format(args[\"dataset\"], queryID) queryImage = cv2.imread(p) quertImage = imutils.resize(queryImage, width = 320) queryImage = cv2.cvtColor(queryImage, cv2.COLOR_BGR2GRAY) (_, descs) = dad.describe(queryImage) hist = bovw.describe(descs).tocoo() sr = searcher.search(hist, numResults = 4) results = set([r[1] for r in sr.results]) inter = results.intersection(queryRelevant) accuracies.append(len(inter)) timings.append(sr.search_time) pbar.update(i) searcher.finish() pbar.finish() accuracies = np.array(accuracies) timings = np.array(timings) print(\"[INFO] ACCURACY:u = {:.2f}, o = {:.2f}\".format(accuracies.mean(), accuracies.std())) print(\"[INFO] TIMINGS:u = {:.2f}, o = {:.2f}\".format(timings.mean(), timings.std()))"}
{"content2":"序号会议名称会议介绍代表领域12ICCV: IEEE International Conference on Computer Vision领域顶级国际会议，录取率20%左右，2年一次，中国大陆每年论文数不超过10篇计算机视觉，模式识别，多媒体计算13CVPR: IEEE Conf on Comp Vision and Pattern Recognition领域顶级国际会议，录取率25%左右，每年一次，中国大陆每年论文数不超过20篇模式识别，计算机视觉，多媒体计算14ECCV: European Conference on Computer Vision领域顶级国际会议，录取率25%左右，2年一次，中国大陆每年论文数不超过20篇模式识别，计算机视觉，多媒体计算16ICML: International Conference on Machine Learning领域顶级国际会议，录取率25%左右，2年一次，目前完全国内论文很少机器学习，模式识别17NIPS: Neural Information Processing Systems领域顶级国际会议，录取率20%左右，每年一次，目前完全国内论文极少（不超过5篇）神经计算，机器学习18ACM MM: ACM Multimedia Conference领域顶级国际会议，全文的录取率极低，但Poster比较容易多媒体技术，数据压缩20IEEE ICME: International Conference on Multimedia and Expo多媒体领域重要国际会议，一年一次多媒体技术24ACL: The Association for Computational Linguistics国际计算语言学会年会，是本领域最权威的国际学术会议之一，每年举办一次计算语言学，自然语言处理25COLING: International Conference on Computational Linguistics计算语言学会议，也是本领域最权威的国际学术会议之一，两年一次计算语言学，自然语言处理27IJCNLP: International Joint Conference on Natural Language Processing自然语言处理亚洲联盟主办的国际会议，是自然语言处理领域亚洲区域最有影响的学术会议，基本是每年举办一次自然语言处理129ACL: The Association for Computational Linguistics计算语言学/自然语言处理方面最好的会议, ACL (Association of Computational Linguistics) 主办, 每年开。人工智能 计算语言学130ACM SIGIR: The ACM Conference on Research and Development in Information Retrieval信息检索方面最好的会议, ACM 主办, 每年开。19％左右信息检索技术131ACM SIGKDD: The ACM Conference on Knowledge Discovery in Databases and Data Mining数据挖掘方面最好的会议, ACM 主办, 每年开。18％左右132WWW: The ACM International World Wide Web Conference应用和媒体领域顶级国际会议万维网133ACM SIGMOD: ACM SIGMOD Conf on Management of Data数据库领域顶级国际数据管理134CIKM: The ACM Conference on Information and Knowledge Management数据库领域知名国际会议数据管理135COLING: International Conference on Computational Linguistics计算语言学知名国际会议计算语言学136ICML: International Conference on Machine Learning领域顶级国际会议，录取率25%左右，2年一次，目前完全国内论文很少机器学习，模式识别137IEEE ICDM: International Conference on Data Mining数据挖掘领域顶级国际会议138IJCAI: International Joint Conference on Artificial Intelligence人工智能领域顶级国际会议，论文接受率18％左右人工智能139VLDB: The ACM International Conference on Very Large Data Bases数据库领域顶级国际数据库142AAAI: American Association for Artificial Intelligence美国人工智能学会AAAI的年会，使该领域的顶级会议人工智能145ACM SIGIR: The ACM Conference on Research and Development in Information Retrieval信息检索领域的重要会议信息检索148ACM SIGMOD: ACM SIGMOD Conf on Management of Data数据库与数据管理最顶级的学术会议，数据管理的主要发展都在这个会上有描述。数据管理151CIKM: The ACM Conference on Information and Knowledge Management信息检索领域的会议，录用率为15%信息检索159ICML: International Conference on Machine Learning机器学习领域中的顶级会议机器学习162IEEE ICDM: International Conference on Data Mining数据挖掘领域的著名会议，率用率为14%。数据挖掘167IEEE SPIRE: The IEEE International Symposium on String Processing and Information Retrieval字符串处理与信息检索的年会，字符串匹配的主要进展都可以在这个会议上找到。字符串处理信息检索168IJCAI: International Joint Conference on AI人工智能领域的顶级会议。人工智能176PAKDD: Pacific-Asia Conference on Knowledge Discovery and Data Mining178PKDD: Conference on Principles and Practice of Knowledge Discovery in Databases数据挖掘领域的重要会议，录用率为14%。数据挖掘180SDM: SIAM International Conference on Data Mining数据挖掘领域的重要会议，录用率为14%数据挖据184VLDB: The ACM International Conference on Very Large Data Bases数据管理185WWW: The ACM International World Wide Web ConferenceACM旗下关于互联网方面的重要会议，从Web服务器到互联网语义等研究问题一一包含其中。15%录用率。Internet186RAID International Symposium on Recent Advances in Intrusion Detection数据库顶级国际会议187IJCAI: International Joint Conference on Artificial Intelligence人工智能顶级国际会议人工智能188VLDB: The ACM International Conference on Very Large Data Bases数据库顶级国际会议数据库189ICML: International Conference on Machine Learning机器学习顶级国际会议机器学习190PRICAI: Pacific Rim International Conference on Artificial Intelligence亚太人工智能国际会议人工智能191IFIP ICIIP: IFIP International Conference on Intelligent Information ProcessingIFIP智能信息处理国际会议智能信息处理192NIPS: Neural Information Processing Systems神经信息处理领域顶级国际会议神经计算，机器学习232WWW: The ACM International World Wide Web ConferenceInternet领域顶级国际会议Internet233International Semantic Web ConferenceSemantic Web领域顶级会议，录用率17%Semantic Web234ACM SIGMOD: ACM SIGMOD Conf on Management of DataACM的旗舰会议之一，也是数据库领域顶级学术会议，内容侧重于数据管理。数据管理235ACM PODS ConferenceACM的旗舰会议之一，也是数据库领域顶级学术会议，内容侧重于数据管理基本理论。数据管理236VLDB: The ACM International Conference on Very Large Data Bases数据库顶级国际会议数据管理237IEEE ICDE - International Conference on Data Engineering数据库顶级国际会议数据管理"}
{"content2":"世界计算机算法最权威会议SODA---全称ACM-SIAM Symposium on Discrete Algorithms。世界计算机科学领域最顶级期刊JACM---全称Journal of the Association for Computing Machinery，该期刊只发表世界计算机科学领域具有最重要意义的研究工作，每年仅收录30多篇。世界数据库领域最顶级的期刊ACM TODS---全称ACM Transactions on Database Systems，该期刊全年在全世界范围不过收录30篇高水平论文世界计算机存储领域顶尖期刊ACM Transactions on Storage---该期刊全年收录文章不超过20篇世界程序语言设计领域顶级学术会议PLDI2007---全称ACM SIGPLAN Conference on Programming Language Design and Implementation世界物理学最权威学术刊PRL---全称Physical Review Letter世界理论计算机领域顶级会议STOC---全称ACM Symp on Theory of Computing世界人工智能方面最顶级会议IJCAI---全称International Joint Conferences on Artificial Intelligence世界计算机视觉和模式识别领域顶级国际会CVPR---全称IEEE Conference on Computer Vision and Pattern Recognition世界信息检索领域顶级会议SIGIR---全称ACM SIGIR Special Interest Group on Information Retrieval世界数据挖掘领域最权威国际期刊IEEE TKDE---全称IEEE Transactions on Knowledge and Data Engineering世界数据库领域最顶级会议SIGMOD---全称ACM's Special Interest Group on Management Of Data世界计算机图形学最权威国际会议ACM SIGGRAPH世界计算语言/自然语言处理领域最顶级会议ACL---全称Association for Computational Linguistics世界理论计算机科学顶级学术期刊Theoretical Computer Science世界计算复杂性领域顶级会议CCC---全称IEEE Conference on Computational Complexity世界计算机视觉和模式识别领域顶尖期刊IEEE PAMI---全称IEEE Transactions on Pattern Analysis and Machine Intelligence世界集成电路设计领域最顶级会议DAC---全称Design Automation Conference世界人工智能领域顶级学术会议AAAI---全称Association for the Advancement of Artificial Intelligence世界互联网领域顶级会议WWW---全称World Wide Web Conference世界通信与计算机网络领域顶级学术会议Infocom---全称IEEE Conference on Computer Communications，世界信息科学理论顶级期刊IEEE Transactions on Information Theory世界数据挖掘领域一流会议SDM---全称SIAM International Conference on Data Mining世界声学与信号处理一流会议ICASSP---全称IEEE International Conference on Acoustics, Speech, and Signal Processing世界计算机算法与理论领域一流会议STACS---全称Symp on Theoretical Aspects of Computer Science世界计算机理论科学领域一流会议ICALP---全称International Colloquium on Automata, Languages and Programming世界数据挖掘领域一流会议ICME---全称IEEE International Conference on Multimedia & Expo世界计算机图形学领域一流会议EuroGraphics世界集成电路领域一流会议ISVLS---全称IEEE Computer Society Annual Symposium on VLSI"}
{"content2":"入门:函数式编程之艰难,在于这玩意更接近数学,就是数学之\"用\",大牛们在云端,玩纯数学去了,弄出一堆公式及概念,码农们爬在地上,辛苦耕耘,要的是看得见摸得着的,函数式编程就是要打通两者的桥梁,把数学家弄出来的公式概念,用于耕耘,这是现代化农业,告别刀耕火种,是门实实在在的实践科学.这两天看了不少概念性的东西,越看越糊涂,但好处是打开了一扇窗,看到了更广阔的计算机科学世界,先整理下:根源:数学:数理逻辑数理逻辑是数学的一个分支，其研究对象是对证明和计算这两个直观概念进行符号化以后的形式系统。数理逻辑的主要分支包括：模型论、证明论、递归论和公理化集合论。数理逻辑和计算机科学有许多重合之处，这是因为许多计算机科学的先驱者既是数学家、又是逻辑学家，如阿兰·图灵、邱奇等。柯里－霍华德同构给出了“证明”和“程序”的等价性，这一结果与证明论有关，直觉主义逻辑和线性逻辑在此起了很大作用。λ演算和组合子逻辑这样的演算现在属于理想程序语言。哲学:哲学逻辑哲学逻辑是对逻辑更特定于哲学的方面的研究。这个术语相对于数理逻辑哲学逻辑不关心与思维、情感、想象和类似事物相关的心理过程。它只关心那些有能力为真和假的实体 — 思维、句子、命题。尽管在这个范围内，它还感兴趣于心灵哲学和语言哲学。弗雷格被认为是现代哲学逻辑的缔造者。哲学逻辑只关心那些有能力为真和假的实体 — 思维、句子、命题。计算机科学的哲学Peter Wegner和Amnon H. Eden提议了三种范式应用于计算机科学的各个领域：[17]“理性主义范式”，将计算机科学看作是数学的分支，在理论计算机科学中很流行，主要利用演绎推理。“技术专家范式”，这类范式有着很明显的工程学倾向，尤其是在软件工程领域。“科学范式”，人工智能的某些分支可以作为这类范式的代表（比如说对于人工生命的研究）。科学:计算机科学计算机科学（Computer Science，有时缩写为CS）是系统性研究信息与计算的理论基础以及它们在计算机系统中如何实现与应用的实用技术的学科。虽然最初很多人并不相信计算机可能成为科学研究的领域，但是随后的50年里也逐渐被学术界认可。主要成就虽然计算机科学被认定为正式学术学科的历史很短暂，但仍对科学和社会作出了很多基础贡献。包括：“数字革命”的开端：信息时代与互联网。[13]对于计算和可计算理论的正式定义，证明了存在计算上不可解及难解型问题。[14]程序设计语言的概念，一种在不同抽象级别上精确表达方法信息（methodological information）的工具。[15]在密码学领域，恩尼格玛密码机的破译被视为盟军在二战取得胜利的重要因素。[12]科学计算实现了高复杂度处理的实用价值，以及完全使用软件进行实验。同时也实现了对人类思想的深入研究，使得人类基因组计划绘制人类基因成为可能。[13] 还有探索蛋白质折叠的分布式计算项目Folding@home。算法交易增长了金融市场的经济效益与市场流通性，通过人工智能，机器学习及大规模的统计和数值技术。[16]图像合成[来源请求]自然语言处理，包括语音到文字（speech-to-text）转换、语言间的自动翻译[来源请求]对各种过程的模拟，包括计算流体力学、物理、电气、电子系统和电路，以及同人类居住地联系在一起的社会和社会形态（尤其是战争游戏，war games）。现代计算机能够对这些设计进行优化，如飞机设计。尤其在电气与电子电路设计中，SPICE软件对新的物理实现（或修改）设计具有很大帮助。它包含了针对集成电路的基本设计软件。计算机科学的领域作为一个学科，计算机科学涵盖了从算法的理论研究和计算的极限，到如何通过硬件和软件实现计算系统。[18][19] CSAB（以前被叫做Computing Sciences Accreditation Board），由Association for Computing Machinery（ACM）和IEEE Computer Society（IEEE-CS）的代表组成[20]，确立了计算机科学学科的4个主要领域：计算理论，算法与数据结构，编程方法与编程语言，以及计算机元素与架构。CSAB还确立了其它一些重要领域，如软件工程，人工智能，计算机网络与通信，数据库系统，并行计算，分布式计算，人机交互，计算机图形学，操作系统，以及数值和符号计算。在计算机科学的领域里,仔细分析发现有三个重要领域值得关注:1.计算理论主条目：计算理论按照Peter J. Denning的说法，计算机科学的最根本问题是“什么能够被有效地自动化？”[21] 计算理论的研究就是专注于回答这个根本问题，关于什么能够被计算，去实施这些计算又需要用到多少资源。为了试图回答第一个问题，递归论检验在多种理论计算模型中哪个计算问题是可解的。而计算复杂性理论则被用于回答第二个问题，研究解决一个不同目的的计算问题的时间与空间消耗。著名的“P=NP?”问题，千禧年大奖难题之一，[22] 是计算理论的一个开放问题。2.算法算法指定义良好的计算过程，它取一个或一组值作为输入，经过一系列定义好的计算过程，得到一个或一组输出。[23]算法是计算机科学研究的一个重要领域，也是许多其他计算机科学技术的基础。算法主要包括数据结构、计算几何、图论等。除此之外，算法还包括许多杂项，如模式匹配、部分数论等。3.程序设计语言理论程序设计语言理论是计算机科学的一个分支，主要处理程序设计语言的设计、实现、分析、描述和分类，以及它们的个体特性。它属于计算机科学学科，既受影响于也影响着数学、软件工程和语言学。它是公认的计算机科学分支。其它领域信息与编码理论,形式化方法,并发，并行和分布式系统,数据库和信息检索,应用计算机科学,人工智能,计算机体系结构与工程,计算机图形与视觉,计算机安全和密码学,计算科学（或者科学计算）,信息科学,软件工程到现在为止,我们已经取得了初步的研究成果:1.弄明白了函数式编程的概念及来历函数式编程的基础是Lambda演算(科学家喜欢用希腊字母表示一个概念,而函数式编程涉及的概念就是λ)函数式编程不是函数编程,简单的说,如果你写出了不用变量,没有副作用的函数,可以算是函数式编程.现有的主流编程语言比如C#, Java, C, C++等都不是函数式编程语言(少量可以支持部分函数式编程),经常听说的纯函数式编程语言是Haskell2.划定了范围:基本获取了函数式编程所涉及的概念,领域,科学,知识点的大人本范围,就是本文所整理的知识地图,相关的知识都可以通过上面的去延伸,起码知道了,学习函数式编程,不需要去看.net框架精解,或JAVA函数详解这类书.3.大体能估计学习的入口点,所需要花的时间等入口点:函数式编程然后:λ演算(lambda calculus)再然后:数理逻辑,哲学逻辑,计算机科学可以看出,深入下去是无止境的,很可能,我们连λ演算或Haskell都不需要搞清楚,只要弄清无变量无副作用,递归,就可以开始函数式编程了.4.理清了不少艰深晦涩的计算机科学名词的来龙去脉及基本概念比如象经常在网上见有人提起的“P=NP?”问题,以前不懂一看有人说P=NP,顿时觉得大牛现身,心中立马产生羞愧感.现在我们知道了如下概念:计算机科学的最根本问题是“什么能够被有效地自动化？计算理论的研究就是专注于回答这个根本问题很可能，计算理论最大的未解决问题就是关于这两类的关系的：P和NP相等吗?就是这个“P=NP?”问题在2002年对于100研究者的调查，61人相信答案是否定的，9个相信答案是肯定的，22个不确定，而8个相信该问题可能和现在所接受的公理独立，所以不可能证明或证否。[1] 对于正确的解答，有一个$1,000,000美元的奖励。可见,搞不清楚不是我们的事情.那些号称搞清楚或部分搞清楚的只不过在忽悠人而已."}
{"content2":"计算机视觉、模式识别、机器学习常用牛人主页链接牛人主页（主页有很多论文代码）Serge Belongie at UC San DiegoAntonio Torralba at MITAlexei Ffros at CMUCe Liu at Microsoft Research New EnglandVittorio Ferrari at Univ.of EdinburghKristen Grauman at UT AustinDevi Parikh at  TTI-Chicago (Marr Prize at ICCV2011)John Wright at Columbia Univ.Piotr Dollar at CalTechBoris Babenko at UC San DiegoDavid Ross at Google/YoutubeDavid Donoho at Stanford Univ.大神们：William T. Freeman at MITRoberto Cipolla at CambridgeDavid Lowe at Univ. of British ColumbiaMubarak Shah at Univ. of Central FloridaYi Ma at MSRATinne Tuytelaars at K.U. LeuvenTrevor Darrell at U.C. BerkeleyMichael J. Black at Brown Univ.重要研究组：Computer Vision Group at UC BerkeleyRobotics Research Group at Univ. of OxfordLEAR at INRIAComputer Vision Lab at StanfordComputer Vision Lab at EPFLComputer Vision Lab at ETH ZurichComputer Vision Lab at Seoul National Univ.Computer Vision Lab at UC San DiegoComputer Vision Lab at UC Santa CruzComputer Vision Lab at Univ. of Southern CaliforniaComputer Vision Lab at Univ. of Central FloridaComputer Vision Lab at Columbia Univ.UCLA Vision LabMotion and Shape Computing Group at George Mason Univ.Robust Image Understanding Lab at Rutgers Univ.Intelligent Vision Systems Group at Univ. of BonnInstitute for Computer Graphics and Vision at Graz Univ. of Tech.Computer Vision Lab. at Vienna Univ. of Tech.Computational Image Analysis and Radiology at Medical Univ. of ViennaPersonal Robotics Lab at CMUVisual Perception Lab at Purdue Univ.潜力牛人：Juergen Gall at ETH ZurichMatt Flagg at Georgia Tech.Mathieu Salzmann at TTI-ChicagoGerg Shakhnarovich at TTI-ChicagoTaeg Sang Cho at MITJianchao Yang at UIUCStefan Roth at TU DarmstadtPeter Kontschieder at Graz Univ. of Tech.Dominik Alexander Klein at Univ. of BonnYinan Yu at CASIA (PASCAL VOC 2010 Detection Challenge Winner)Zdenek Kalal at FPFLJulien Pilet at FPFLKenji Okuma（1）googleResearch； http://research.google.com/index.html（2）MIT博士，汤晓欧学生林达华； http://people.csail.mit.edu/dhlin/index.html（3）MIT博士后Douglas Lanman； http://web.media.mit.edu/~dlanman/（4）opencv中文网站； http://www.opencv.org.cn/index.php/首页（5）Stanford大学vision实验室； http://vision.stanford.edu/research.html（6）Stanford大学博士崔靖宇； http://www.stanford.edu/~jycui/（7）UCLA教授朱松纯； http://www.stat.ucla.edu/~sczhu/（8）中国人工智能网； http://www.chinaai.org/（9）中国视觉网； http://www.china-vision.net/（10）中科院自动化所； http://www.ia.cas.cn/（11）中科院自动化所李子青研究员； http://www.cbsr.ia.ac.cn/users/szli/（12）中科院计算所山世光研究员； http://www.jdl.ac.cn/user/sgshan/（13）人脸识别主页； http://www.face-rec.org/（14）加州大学伯克利分校CV小组；http://www.eecs.berkeley.edu/Research/Projects/CS/vision/（15）南加州大学CV实验室； http://iris.usc.edu/USC-Computer-Vision.html（16）卡内基梅隆大学CV主页；http://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.html（17）微软CV研究员Richard Szeliski；http://research.microsoft.com/en-us/um/people/szeliski/（18）微软亚洲研究院计算机视觉研究组；http://research.microsoft.com/en-us/groups/vc/（19）微软剑桥研究院ML与CV研究组；http://research.microsoft.com/en-us/groups/mlp/default.aspx（20）研学论坛； http://bbs.matwav.com/（21）美国Rutgers大学助理教授刘青山；http://www.research.rutgers.edu/~qsliu/（22）计算机视觉最新资讯网； http://www.cvchina.info/（23）运动检测、阴影、跟踪的测试视频下载；http://apps.hi.baidu.com/share/detail/18903287（24）香港中文大学助理教授王晓刚；http://www.ee.cuhk.edu.hk/~xgwang/(25)香港中文大学多媒体实验室（汤晓鸥）;http://mmlab.ie.cuhk.edu.hk/(26)U.C. San Diego. computer vision;http://vision.ucsd.edu/content/home(27)CVonline; http://homepages.inf.ed.ac.uk/rbf/CVonline/(28)computer vision software;http://peipa.essex.ac.uk/info/software.html(29)Computer Vision Resource; http://www.cvpapers.com/(30)computer vision research groups;http://peipa.essex.ac.uk/info/groups.html(31)computer vision center;http://computervisioncentral.com/cvcnews(32)浙江大学图像技术研究与应用（ITRA）团队：http://www.dvzju.com/(33)自动识别网：http://www.autoid-china.com.cn/(34)清华大学章毓晋教授：http://www.tsinghua.edu.cn/publish/ee/4157/2010/20101217173552339241557/20101217173552339241557_.html(35)顶级民用机器人研究小组Porf.Gary领导的Willow Garage:http://www.willowgarage.com/(36)上海交通大学图像处理与模式识别研究所：http://www.pami.sjtu.edu.cn/(37)上海交通大学计算机视觉实验室刘允才教授：http://www.visionlab.sjtu.edu.cn/(38)德克萨斯州大学奥斯汀分校助理教授Kristen Grauman ：http://www.cs.utexas.edu/~grauman/(39)清华大学电子工程系智能图文信息处理实验室（丁晓青教授）：http://ocrserv.ee.tsinghua.edu.cn/auto/index.asp(40)北京大学高文教授：http://www.jdl.ac.cn/htm-gaowen/(41)清华大学艾海舟教授：http://media.cs.tsinghua.edu.cn/cn/aihz(42)中科院生物识别与安全技术研究中心：http://www.cbsr.ia.ac.cn/china/index CH.asp(43)瑞士巴塞尔大学 Thomas Vetter教授：http://informatik.unibas.ch/personen/vetter_t.html(44)俄勒冈州立大学 Rob Hess博士：http://blogs.oregonstate.edu/hess/(45)深圳大学 于仕祺副教授：http://yushiqi.cn/(46)西安交通大学人工智能与机器人研究所：http://www.aiar.xjtu.edu.cn/(47)卡内基梅隆大学研究员Robert T. Collins:http://www.cs.cmu.edu/~rcollins/home.html#Background(48)MIT博士Chris Stauffer:http://people.csail.mit.edu/stauffer/Home/index.php(49)美国密歇根州立大学生物识别研究组(Anil K. Jain教授)：http://www.cse.msu.edu/rgroups/biometrics/(50)美国伊利诺伊州立大学Thomas S. Huang:http://www.beckman.illinois.edu/directory/t-huang1(51)武汉大学数字摄影测量与计算机视觉研究中心：http://www.whudpcv.cn/index.asp(52)瑞士巴塞尔大学Sami Romdhani助理研究员：http://informatik.unibas.ch/personen/romdhani_sami/(53)CMU大学研究员Yang Wang:http://www.cs.cmu.edu/~wangy/home.html(54)英国曼彻斯特大学Tim Cootes教授：http://personalpages.manchester.ac.uk/staff/timothy.f.cootes/(55)美国罗彻斯特大学教授Jiebo Luo:http://www.cs.rochester.edu/u/jluo/(56)美国普渡大学机器人视觉实验室：https://engineering.purdue.edu/RVL/Welcome.html(57)美国宾利州立大学感知、运动与认识实验室：http://vision.cse.psu.edu/home/home.shtml(58)美国宾夕法尼亚大学GRASP实验室：https://www.grasp.upenn.edu/(59)美国内达华大学里诺校区CV实验室：http://www.cse.unr.edu/CVL/index.php(60)美国密西根大学vision实验室：http://www.eecs.umich.edu/vision/index.html(61)University of Massachusetts(麻省大学),视觉实验室：http://vis-www.cs.umass.edu/index.html(62)华盛顿大学博士后Iva Kemelmacher:http://www.cs.washington.edu/homes/kemelmi(63)以色列魏茨曼科技大学Ronen Basri:http://www.wisdom.weizmann.ac.il/~ronen/index.html(64)瑞士ETH-Zurich大学CV实验室：http://www.vision.ee.ethz.ch/boostingTrackers/index.htm(65)微软CV研究员张正友：http://research.microsoft.com/en-us/um/people/zhang/(66)中科院自动化所医学影像研究室：http://www.3dmed.net/(67)中科院田捷研究员：http://www.3dmed.net/tian/(68)微软Redmond研究院研究员Simon Baker:http://research.microsoft.com/en-us/people/sbaker/(69)普林斯顿大学教授李凯：http://www.cs.princeton.edu/~li/(70)普林斯顿大学博士贾登：http://www.cs.princeton.edu/~jiadeng/(71)牛津大学教授Andrew Zisserman：http://www.robots.ox.ac.uk/~az/(72)英国leeds大学研究员Mark Everingham:http://www.comp.leeds.ac.uk/me/(73)英国爱丁堡大学教授Chris William:http://homepages.inf.ed.ac.uk/ckiw/(74)微软剑桥研究院研究员John Winn: http://johnwinn.org/(75)佐治亚理工学院教授Monson H.Hayes：http://savannah.gatech.edu/people/mhayes/index.html(76)微软亚洲研究院研究员孙剑：http://research.microsoft.com/en-us/people/jiansun/(77)微软亚洲研究院研究员马毅：http://research.microsoft.com/en-us/people/mayi/(78)英国哥伦比亚大学教授David Lowe:http://www.cs.ubc.ca/~lowe/(79)英国爱丁堡大学教授Bob Fisher:http://homepages.inf.ed.ac.uk/rbf/(80)加州大学圣地亚哥分校教授Serge J.Belongie:http://cseweb.ucsd.edu/~sjb/(81)威斯康星大学教授Charles R.Dyer:http://pages.cs.wisc.edu/~dyer/(82)多伦多大学教授Allan.Jepson:http://www.cs.toronto.edu/~jepson/(83)伦斯勒理工学院教授Qiang Ji: http://www.ecse.rpi.edu/~qji/(84)CMU研究员Daniel Huber: http://www.ri.cmu.edu/person.html?person_id=123(85)多伦多大学教授：David J.Fleet:http://www.cs.toronto.edu/~fleet/(86)伦敦大学玛丽女王学院教授Andrea Cavallaro:http://www.eecs.qmul.ac.uk/~andrea/(87)多伦多大学教授Kyros Kutulakos:http://www.cs.toronto.edu/~kyros/(88)杜克大学教授Carlo Tomasi: http://www.cs.duke.edu/~tomasi/(89)CMU教授Martial Hebert: http://www.cs.cmu.edu/~hebert/(90)MIT助理教授Antonio Torralba:http://web.mit.edu/torralba/www/(91)马里兰大学研究员Yasel Yacoob:http://www.umiacs.umd.edu/users/yaser/(92)康奈尔大学教授Ramin Zabih: http://www.cs.cornell.edu/~rdz/(93)CMU博士田渊栋: http://www.cs.cmu.edu/~yuandong/(94)CMU副教授Srinivasa Narasimhan: http://www.cs.cmu.edu/~srinivas/(95)CMU大学ILIM实验室：http://www.cs.cmu.edu/~ILIM/(96)哥伦比亚大学教授Sheer K.Nayar: http://www.cs.columbia.edu/~nayar/(97)三菱电子研究院研究员Fatih Porikli ：http://www.porikli.com/(98)康奈尔大学教授Daniel Huttenlocher：http://www.cs.cornell.edu/~dph/(99)南京大学教授周志华：http://cs.nju.edu.cn/zhouzh/index.htm(100)芝加哥丰田技术研究所助理教授Devi Parikh: http://ttic.uchicago.edu/~dparikh/index.html(101)瑞士联邦理工学院博士后Helmut Grabner:http://www.vision.ee.ethz.ch/~hegrabne/#Short_CV(102)香港中文大学教授贾佳亚：http://www.cse.cuhk.edu.hk/~leojia/index.html(103)南洋理工大学副教授吴建鑫：http://c2inet.sce.ntu.edu.sg/Jianxin/index.html(104)GE研究院研究员李关：http://www.cs.unc.edu/~lguan/(105)佐治亚理工学院教授Monson Hayes:http://savannah.gatech.edu/people/mhayes/(106)图片检索国际会议VOC(微软剑桥研究院组织):http://pascallin.ecs.soton.ac.uk/challenges/VOC/(107)机器视觉开源处理库汇总：http://archive.cnblogs.com/a/2217609/(108)布朗大学教授Benjamin Kimia:http://www.lems.brown.edu/kimia.htmlabout multi-camera: http://server.cs.ucf.edu/~vision/projects.htmlabout 3D Voxel Coloring   Rob Hess:http://blogs.oregonstate.edu/hess/code/voxels/About  the particle filters--condensation filter:http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/ISARD1/condensation.htmlMachine Learning Open Source Software：http://jmlr.csail.mit.edu/mloss/1、动作识别数据库：Recognition of human actions：http://www.nada.kth.se/cvap/actions/2、Datasets for Computer Vision Research：http://www-cvr.ai.uiuc.edu/ponce_grp/data/3、Computer Vision Datasets:http://clickdamage.com/sourcecode/cv_datasets.php4、里面有好多基本算法 matlab：  http://www.mathworks.cn/index.html5、CVPR 2011中关于grassmann 流形文章的源码： http://itee.uq.edu.au/~uqmhara1/code.htmlMatlab Codefor Graph Embedding Discriminant Analysis on Grassmannian Manifolds for Improved Image Set Matching (CVPR), 2011.Matlab Codefor Optimal Local Basis: A Reinforcement Learning Approach for Face Recognition(IJCV), vol. 81, no. 2, pp. 191-204, 2009.牛人bolg：1、Hong Kong Polytechnic University ：http://www4.comp.polyu.edu.hk/~cslzhang/2、Computer Vision Resources：资源非常丰富，包含有基本算法。https://netfiles.uiuc.edu/jbhuang1/www/resources/vision/index.html3、源代码非常丰富~~  http://homepage.tudelft.nl/19j49/Publications.htmlCVonlinehttp://homepages.inf.ed.ac.uk/rbf/CVonlinehttp://homepages.inf.ed.ac.uk/rbf/CVonline/unfolded.htmhttp://homepages.inf.ed.ac.uk/rbf/CVonline/CVentry.htm李子青的大作：Markov Random Field Modeling in Computer Visionhttp://www.cbsr.ia.ac.cn/users/szli/mrf_book/book.htmlHandbook of Face Recognition (PDF)http://www.umiacs.umd.edu/~shaohua/papers/zhou04hfr.pdf张正友的有关参数鲁棒估计著作：Parameter Estimation Techniques:A Tutorial with Application to Conic Fittinghttp://research.microsoft.com/~zhang/INRIA/Publis/Tutorial-Estim/Main.htmlAndrea Fusiello“计算机视觉中的几何”教程：Elements of Geometric Computer Visionhttp://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/FUSIELLO4/tutorial.html#x1-520007有关马尔可夫蒙特卡罗方法的资料：An introduction to Markov chain Monte Carlohttp://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/SENEGAS/mcmc.htmlMarkov Chain Monte Carlo for Computer Vision--- A tutorial at ICCV05http://civs.stat.ucla.edu/MCMC/MCMC_tutorial.htm有关独立成分分析（Independent Component Analysis , ICA）的资料：An ICA-Pagehttp://www.cnl.salk.edu/~tony/ica.htmlFast ICAhttp://www.cis.hut.fi/projects/ica/fastica/The Kalman Filter (介绍卡尔曼滤波器的终极网页)http://www.cs.unc.edu/~welch/kalman/index.htmlCached k-d tree search for ICP algorithmshttp://kos.informatik.uni-osnabrueck.de/download/3dim2007/paper.html几个计算机视觉研究工具Machine Vision Toolbox for Matlabhttp://www.petercorke.com/Machine Vision Toolbox.htmlMatlab and Octave Function for Computer Vision and Image Processinghttp://www.csse.uwa.edu.au/~pk/research/matlabfns/Bayes Net Toolbox for Matlabhttp://www.cs.ubc.ca/~murphyk/Software/BNT/bnt.htmlOpenCV (Chinese)http://www.opencv.org.cn/index.php/首页Gandalf (A Computer Vision and Numerical Algorithm Labrary)http://gandalf-library.sourceforge.net/CMU Computer Vision Home Pagehttp://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.htmlMachine Learning Resource Linkshttp://www.cse.ust.hk/~ivor/resource.htmThe Bayesian Filtering Libraryhttp://www.orocos.org/bflOptical Flow Algorithm Evaluation (提供了一个动态贝叶斯网络框架，例如递归信息处理与分析、卡尔曼滤波、粒子滤波、序列蒙特卡罗方法等，C++写的)http://of-eval.sourceforge.net/MATLAB code for ICP algorithmhttp://www.usenet.com/newsgroups/comp.graphics.visualization/msg00102.html牛人主页：朱松纯 （Song-Chun Zhu）http://www.stat.ucla.edu/~sczhu/David Lowe (SIFT) (很帅的一个老头哦 ^ ^)http://www.cs.ubc.ca/~lowe/Andrea Vedaldi (SIFT)http://vision.ucla.edu/~vedaldi/index.htmlPedro F. Felzenszwalbhttp://people.cs.uchicago.edu/~pff/Dougla Dlanman (Brown的一个研究生，在其主页上搜集了大量算法教程和源码)http://mesh.brown.edu/dlanman/courses.htmlJianbo Shi (Ncuts 的始作俑者)http://www.cis.upenn.edu/~jshi/Active Vision Group (Oxford的一个机器视觉研究团队，特色是SLAM，监视，导航)http://www.robots.ox.ac.uk/ActiveVision/index.htmlJuyang Weng（机器学习的专家，Autonomous Mental Development 是其特色）http://www.cse.msu.edu/~weng/测试图片或视频：Middlebury College‘s Stereo Vision Data Sethttp://cat.middlebury.edu/stereo/data.htmlIntelligent Vehicle:IVSourcewww.ivsoruce.netRobot Carhttp://www.plyojump.com/robot_cars.htmlHow to Build a Robot: The Computer Vision Parthttp://www.societyofrobots.com/programming_computer_vision_tutorial.shtml收集的一般牛人主页（带代码）:Xiaofei He(machine learning code)http://people.cs.uchicago.edu/~xiaofei/YingNian Wu(active base model code)http://www.stat.ucla.edu/~ywu/research.html布朗大学计算机主页（可找到该校CS牛人博客）http://www.cs.brown.edu/research/areas.htmlNavneet Dalal(Histograms of Oriented Gradients for Human Detection )http://www.navneetdalal.com/softwarePaul Viola(Robust Real-time Object Detection)http://research.microsoft.com/en-us/um/people/viola/Active LearningRMw平坦软件园http://active-learning.net/，这里包括了关于Active Learning理论以及应用的一些文章，特别是那篇Survey。Transfer LearningRMw平坦软件园http://www.cse.ust.hk/TL/，包括经典的论文以及附带有源码，很方便。Gaussian ProcessesRMw平坦软件园RMw平坦软件园http://www.gaussianprocess.org 包括相关的书籍（有 Carl Edward Rasmussen 的书），相关的程序以及分类的 paper 列表。这也是由 Carl 自己维护的，他应该是将 GP 引入 machine learning 最早的人之一了吧，Hinton 的学生。Nonparametric Bayesian MethodsRMw平坦软件园http://www.cs.berkeley.edu/~jordan/npb.html 这个一看就知道是 Jordan 维护的，主要包括 Dirichlet process 以及相关的其他随机过程在 machine learning 里面如何进行建模，如何进行 approximate inference。主要是文章列表。Probabilistic Graphical ModelRMw平坦软件园http://www.cs.ubc.ca/~murphyk/Bayes/bnintro.html 是 Kevin Murphy 所维护的关于 Bayesian belief networks 的介绍，含有最基本的概念、相关的文献和软件的链接。罕见的 UCB 出来的不是 Jordan 的学生（老板是 Stuart Russel）。http://www.cs.berkeley.edu/~jordan/graphical.html 是 Jordan 系关于这个方面的论文汇编。http://www.inference.phy.cam.ac.uk/hmw26/crf/ 是关于 Conditional Random Fields 方面论文和软件的收集，由 Hanna Wallach 维护。Compressed SensingRMw平坦软件园http://www-dsp.rice.edu/cs 这是 Rice 大学维护的论文分类列表、软件链接等。推荐 Emmanuel Candès 所写的tutorial，这人是 David Donoho 的学生。TensorRMw平坦软件园http://csmr.ca.sandia.gov/~tgkolda/pubs/index.html 关于 tensor 的一些偏数学的文章。Deep Belief NetworkRMw平坦软件园http://www.cs.toronto.edu/~hinton/csc2515/deeprefs.html 是 Geoffrey Hinton 为研究生开设的 machine learning 课程的 DBN 的 reading list。Kernel MethodsRMw平坦软件园http://www.cs.berkeley.edu/~jordan/kernels.html 是 Jordan 维护的关于 kernel methods 的文章列表。Markov LogicRMw平坦软件园http://ai.cs.washington.edu/pubs 是 UW AI 组的文章，里面关于 Markov logic 的比较多，因为 Pedro Domingos 就是这个组的。Machine learning theoryhttp://hunch.net/这个网站主要是一些learning theory的东西比较多，想在machine learning 理论上有所建树的同志们可以去看看牛人：Iasonas Kokkinos （搞统计模型视觉）http://vision.mas.ecp.fr/Personnel/iasonas/index.html"}
{"content2":"文章发布于公号【数智物语】 （ID：decision_engine），关注公号不错过每一篇干货。来源 | AI科技评论（id:aitechtalk）今年是中国人工智能四十年，在这四十年间发生了很多事情，听听张正友博士讲一讲计算机视觉的前世、今生和可能的未来。编者按：7 月 12 日-7 月 14 日，2019 第四届全球人工智能与机器人峰会（CCF-GAIR 2019）于深圳正式召开。峰会由中国计算机学会（CCF）主办，雷锋网、香港中文大学（深圳）承办，深圳市人工智能与机器人研究院协办，得到了深圳市政府的大力指导，是国内人工智能和机器人学术界、工业界及投资界三大领域的顶级交流博览盛会，旨在打造国内人工智能领域极具实力的跨界交流合作平台。7 月 12 日，腾讯 AI Lab & Robotics X 主任，ACM Fellow, IEEE Fellow, CVPR 2017 大会主席张正友博士为 CCF-GAIR 2019 主会场「AI 前沿专场」做了题为「计算机视觉的三生三世」的大会报告。以下为张正友博士所做的大会报告全文，感谢张正友博士的修改与确认。大家好！非常感谢雷锋网的邀请，让我有这个机会给大家做个分享。今年是中国人工智能四十周年，在这四十年间发生了很多事情，雷锋网让我跟大家讲一讲计算机视觉的前世、今生和可能的未来。其实这个报告应该由我的好朋友香港科技大学权龙教授来讲，他比我早一年出国，而且他现在还在港科大潜心研究计算机视觉。我这些年间，还有好多年在做语音处理和识别、多媒体处理和机器人，所以我在计算机视觉上的研究史还不算很长。不过权龙教授有事没法参加，我只能滥竽充数，给大家讲讲计算机视觉的一些故事。雷锋网找我是听说我开始研究计算机视觉比较早。我 1985 年浙大本科毕业，1986 年去法国，参与研发了可能是世界上第一台用立体视觉导航的移动机器人。01图像处理1986 年其实发生了很多事情，1986 年是我第一次参加国际会议，是在巴黎召开的 ICPR（世界模式识别大会）。在这次大会上，我碰到了复旦大学的吴立德教授，他带领了一支中国的代表团，并在会上做了一场大会报告，介绍了中国在模式识别上的研究现状，他们准备申请 1988 年的 ICPR 在中国召开。这里需要提到一个关键性的人物，那就是普渡大学的傅京孙教授，他是模式识别领域的鼻祖。他是 1973 年第一届 ICPR 的主席，1976 年创建了 IAPR，1978 年创刊了 IEEE TPAMI，并担任第一届主编。本来他是支持 1988 年 ICPR 在中国召开的，但不幸的是 1985 年他去世了，所以 1988 年的申请没有成功。如果 1988 年 ICPR 能在中国召开，也许中国在模式识别和计算机视觉上的发展会更提前。当然历史没有如果。ICPR 在中国的召开等到了三十年以后，2018 年在谭铁牛院士的带领下，ICPR 第一次在中国召开。1986 年还有一个很重要的事件，就是我的法国学长马颂德回国，他创立了 NLPR（国家模式识别重点实验室）。NLPR 创立之后，吸引了大批国外的学者回国，同时邀请了很多国外的访问学者，中国计算机视觉领域开始与国际接轨。当然马颂德是中国科技界重要人物，后来担任科技部副部长。1997 年他还创立了中法联合实验室，这个实验室一半的研究人员都是法国人，这在中国也是一个壮举。提到计算机视觉，离不开一个标志性人物，MIT 的教授 David Marr。1979 年，刚好 40 年前，他提出了视觉计算的理论框架。Marr 的理论框架有三个层次，从计算什么，到如何表达和计算，到硬件的实施。具体到三维重建，Marr 认为从图像要经过几个步骤，第一个步骤叫 primal sketch，也就是图像处理，比如边缘提取。所以到八十年代中叶，计算机视觉的主要工作是图像处理。最有名的工作可能是 1986 年 MIT 一个硕士生发表的 Canny 边缘检测算子，基本上解决了边缘提取的问题。如下图所示，左边是原始图像，右边是检测出的边缘。那时候还有一个比较有名的工作是华人科学家沈俊做的，他那时在法国波尔多大学。他比较了不同的算子。他的算子在有些图像方面要比 Canny 检测器要好。所以到了八十年代中叶，当我留学法国的时候，图像处理已经做的差不多了。02立体视觉及三维重建幸运的是，几何视觉刚开始兴起。有两位代表人物，一位是法国的 Olivier Faugeras，他是我的博士导师，另一位是美国的 Thomas Huang，我们叫他 Tom。他们是好朋友，还一起写过文章。我 1987 年就认识 Tom，他对我有非常大的帮助。他培养了 100 多位博士，包括不少活跃在中国学术界和工业界的计算机视觉专家，他对中国计算机视觉的贡献是非常巨大的。我很荣幸师从 Olivier Faugeras，参与开发了世界上第一台用立体视觉导航的移动机器人。1988 年我的第一个研究成果发表在第二届 ICCV 上，右边是在美国 Florida 开会的一张照片。那时候计算机视觉还没有红火，那届 ICCV 大概只有 200 个参会者，华人就更少了，大概只有我，权龙，还有 Tom 的学生翁巨扬。我在博士期间围绕三维动态场景分析做了不少工作，1992 年把这些整合成一本书发表。现在我想举一个简单的例子，不定性的建模和计算，希望通过下面这一页 PPT 你们就能明白什么是三维计算机视觉。这里需要用到概率与统计，这非常重要，但现在做视觉的人往往忽略了。下面两条线代表了两个图像平面。左边图像上一个白点对应右边图像上一个白点。每个图像点对应空间一条直线，两条直线相交就得到一个三维点，这就是三维重建。同样，左边图像的黑点对应右边图像的黑点，两线相交得到一个三维点。但是图像的点是检测出来的，是有噪声的。我们用椭圆来代表不定性，那么图像的一个点就不对应一条线了，而是一个椎体。两个椎体相交，就代表了三维重建的点的不定性。这里可以看到，近的点要比远的点精确。当我们用这些三维重建点的时候就需要考虑这些不定性。比如当机器人从一个地方移动到另一个地方，需要估计它的运动时就必须考虑数据的不定性。90 年代初我提出了 ICP 算法，通过迭代点的匹配来对齐不同的曲线或曲面。这个算法也用在很多地方。我们现在经常听到的 SLAM，它其实就是我们以前做的从运动中估计结构，三维重建，不定性估计，ICP。事实上，SLAM 在 90 年代初理论上已经解决了。1995 年我提出了鲁棒的图像匹配和极线几何估计方法，同时把程序放到网上，大家都以此作为参照。这可能是世界上第一个，至少是之一，把计算机视觉的程序放到网上让别人用真实图像来测试的。所以这个算法那时候就成为计算机视觉的通用方法。1998 年我提出了一个新的摄像机标定法，后来大家都称它为「张氏方法」，现在它已经在全世界的三维视觉、机器人、自动驾驶上普遍应用，也获得了IEEE Helmholtz 时间考验奖。1998 年我和马颂德对日益成熟的几何视觉做了总结，作为研究生教材由科学出版社出版。1998 年还发生了很多事情，一个是 MSRA（微软亚洲研究院）的成立，一个是腾讯公司的成立。这两家看似无关的机构其实对中国计算机视觉的发展，对中国人工智能的发展，起了不可估量的作用。MSRA 给中国带来了国际先进的研究方法和思路，培养了一大批中国的优秀学者，同时也请了一些国外的研究学者来到中国。腾讯促进了中国互联网的发展，因为有互联网，中国研究人员能够几乎实时地接触到国际最顶尖的研究成果。所以这两个结合，对中国人工智能领域的发展起到了很大的作用。中国计算机视觉界一个重要的标志性事件是 2005 年 ICCV 在北京召开，马颂德和 Harry Shum 担任大会主席，这标志着中国计算机视觉的研究水平已经得到国际的认同。我也很荣幸地从 Tom Huang 前辈手中接过 IEEE Fellow 的证书。03深度学习的崛起可能几何视觉的理论已经比较成熟了，90 年代末，计算机视觉的研究开始进入物体和场景的检测和识别，主要方法是传统特征加上机器学习。那时候我做几何视觉做了很长时间，1997 年，我也开始尝试，开发了世界上第一个用神经网络来识别人脸表情的系统，用的特征是 Gabor 小波。虽然 20 多年前就开始人脸表情识别，但那时数据太少，一直到 2016 年我们才在微软把人脸表情识别技术商业化，在微软的认知服务上，大家都可以调用。在传统特征加机器学习的年代，需要提一下一个里程碑的工作，那就是 2001 年的 Viola-Jones Detector。通过 Harr 特征加级联分类器，人脸的检测能够做得非常快，在 20 年前的机器上就能做到实时。这对计算机视觉产生了很大的影响。此后的循环是一波一波的新数据集推出，加一波一波的算法刷榜。2009 年一个叫 ImageNet 的数据集出现了，这是斯坦福大学李飞飞团队推出的，这个数据集非常重要，它的意义不在于这个数据集很大，而在于几年后催生了深度学习时代。2012 年，Geoffrey Hinton 的两个学生开发了 AlexNet，用了 8 层神经网络，6 千万参数，误差比传统方法降了十几个百分点，从 26% 降到 15%，从此开启了计算机视觉的深度学习时代。这个 AlexNet 结构其实和 1989 年 Yann LeCun 用于手写数字识别的神经网络没有很大区别，只是更深更大。由于 Geoffrey Hinton, Yoshua Bengio, Yann LeCun 对深度学习的贡献，他们共同获得了 2018 年的图灵奖。这个奖他们当之无愧。要知道 Geoffrey Hinton 1986 年就提出了 backpropagation，坐了 25 年的冷板凳。在深度学习时代还有一个里程碑的工作，2015 年，微软亚洲研究院的何恺明和孙剑提出 ResNet，用了 152 层神经网络，在 ImageNet 测试集上的误差比人还低，降到了 4% 以下。我在深度学习领域也有一点贡献。2014 年我和 UCSD 的屠卓文合作，提出了 DSN（Deeply- Supervised Nets）深度监督网络，虽然影响没有 ResNet 大，但也有近一千次引用。我们的想法是直接让输出监督中间层，使得最底层尽可能最大逼近要学习的函数，同时也缓解梯度「爆炸」或「消失」。刚刚过去的 CVPR2019 可以被称为是华人的盛典，在组织者里面有很多华人面孔，包括大会主席朱松纯、程序委员会主席华刚和屠卓文。在五千多篇投稿中，40% 来自大陆，最佳论文奖和最佳学生论文奖的第一作者也都是华人。所以中国的计算机视觉能力还是很强的，这一点值得骄傲。04计算机视觉的研究要回归初心现在让我们回顾一下计算机视觉研究的演变，从最初的图像处理、立体视觉与三维重建、物体检测和识别，到光度视觉、几何视觉和语义视觉，到现在的深度学习打遍天下。这是让我担忧的。深度学习有很多局限性。我认为接下来应该要回归初心，让光度视觉、几何视觉和语义视觉紧密结合起来，同时注入常识和领域知识，和语言进行多模态融合，通过学习不断演变。我们腾讯 AI Lab 在这方面也开始做了一点点工作。比如我们的看图说话项目能够用语言描述一张照片的内容，2018 年 1 月，我们上线 QQ 空间 app 让视障用户「看到」图片。我们还整合了计算机视觉、语音识别和自然语言处理技术，开发了一个虚拟人产品，探索多模态人机交互，赋能其他场景，助力社交。我们还开发了二次元的虚拟人来做游戏解说，它能实时理解游戏场景并将它描述出来。那么现在的人工智能真的智能吗？想象一下，如果一个人想要盖住你的眼睛，你会怎么做？我是会躲开的。但是从我刚才播放的视频中可以看到，现在的监控系统显然没有这样的举止。现在的人工智能只是机器学习：从大量的标注数据去学习一个映射。什么是真正的智能？我想目前还没有定论，而且我们对我们自己的智能还没有足够的了解。不过我很认同瑞士认知科学家 Jean Piaget 说的，智能是当你不知道如何做的时候你用的东西。我认为这个定义是非常有道理的。当你无法用你学到的东西或天赋去面对时，你动用的东西就是智能。如何去实现有智能的系统呢？可能有很多条路，但我认为一条很重要的路是需要把载体考虑进去，做有载体的智能，也就是机器人。在机器人领域，我提出了 A2G 理论。A 是 AI，机器人必须能看能听能说能思考，B 是 Body 本体，C 是 Control 控制，ABC 组成了机器人的基础能力。D 是 Developmental Learning，发育学习，E 是 EQ，情感理解、拟人化，F 是 Flexible Manipulation，灵活操控。最后要达到 G，G 是 Guardian Angel，守护天使。腾讯做了三款机器人：绝艺围棋机器人、桌上冰球机器人，还有机器狗。可以为大家展示机器狗的视频，机器狗具备感知系统，能够绕开障碍物，看到悬空的障碍物能匍匐前进，看到前面一个人能蹲下来看着人。星标我，每天多一点智慧"}
{"content2":"基于Aforge的物体运动识别-入门篇chatbot人工智能机器人开发，提供教学视频>>>0收藏(2)本文来自http://blog.csdn.net/hellogv/ ，引用必须注明出处！最近看到越来越多人在做物体运动识别(例如：\"第六感\"中的指套),而且我最近也有点闲空，所以也来玩玩。。。。。大多数人都是用Opencv来做，那我就不做重复的工作了，换个别的开源类库~~~Aforge。来自百度知道的Aforge介绍：AForge.NET 是一个专门为开发者和研究者基于C#框架设计的，他包括计算机视觉与人工智能，图像处理，神经网络，遗传算法，机器学习，机器人等领域。这个框架由一系列的类库和例子组成。其中包括的特征有：AForge.Imaging -一些日常的图像处理和过滤器AForge.Vision -计算机视觉应用类库AForge.Neuro -神经网络计算库AForge.Genetic -进化算法编程库AForge.MachineLearning -机器学习类库AForge.Robotics -提供一些机器学习的工具类库AForge.Video -一系列的视频处理类库（很方便）PS：AForge里面的算法够我玩一段很长时间了。。。。。本文做的例子代码改自http://www.aforgenet.com/articles/step_to_stereo_vision/ ,删减了部分官方代码，加入了更简洁的物体识别功能。。。如下图：一开始还没有告诉程序，哪个是需要识别的物体，鼠标在绿色的瓶盖上点一下之后，程序就知道绿色瓶盖就是那个需要识别的物体。。。。。。为什么要用绿色？因为绿色的指套最容易做，找两片叶子粘在一起就OK了。不建议用红色，因为人体偏红色，不好识别。本例的代码可以在：http://download.csdn.net/source/2281943 这里下载，AForge类库请到http://code.google.com/p/aforge/downloads/list 这里下载。原文链接：http://blog.csdn.net/hellogv/article/details/5526284"}
{"content2":"计算机视觉软件正在改变行业，使用户的生活变得不仅更容易，而且更有趣。作为一个有潜力的领域，计算机视觉已经获得了大量的投资。北美计算机视觉软件市场的总投资额为1.2亿美元，而中国市场则飙升至39亿美元。让我们来看看一些最有前途和更有趣的技术，因为这些技术可以让计算机视觉软件开发市场增长的更快。一、深度学习的进步深度学习因其在提供准确结果方面而广受欢迎。传统的机器学习算法尽管很复杂，但其核心仍然非常简单。他们的训练需要大量的专业领域的知识和数据（这是昂贵的），在训练发生错误时需要进行人为干预，而且，他们只擅长于他们接受过训练的任务。另一方面，深度学习算法通过将任务映射为概念层次结构的神经元网络了解手头的任务。每个复杂的概念都由一系列更简单的概念组合定义，而所有这些算法都可以自己完成。在计算机视觉的背景下，图像分类需要首先识别亮区和暗区，然后在移向全画面识别之前对线进行分类，然后进行形状分类。当你为他们提供更多数据时，深度学习算法也会表现得更好，这是典型的机器学习算法做不到的。对于计算机视觉，深度学习是一个好的方向。它不仅允许在深度学习算法的训练中使用更多的图片和视频，而且还减轻了许多与注释和标记数据相关的工作。零售业一直是实施计算机视觉软件的先驱。2017年，ASOS在为他们的应用添加了一个按照照片搜索的选项，之后许多零售商都跟进了。有些人甚至更进一步，并使用计算机视觉软件将在线和离线体验更紧密地结合在一起。一家名为Lolli＆Pops的美食糖果零售商使用面部识别来识别经常走进商店的购物者。因此，商店的员工可以通过提供个性化的产品推荐和千人千面的折扣来个性化购物体验。特殊待遇可以提升品牌忠诚度，并将偶尔的购物者转变为经常性购物者。二、边缘计算的兴起连接到互联网和云的机器能够从整个网络收集的数据中学习并相应地进行调整，从而优化系统的性能。但是，并不能保证机器能够始终连接到互联网和云，这就是边缘计算的用武之地。边缘计算是指附接到物理机器的技术，例如燃气轮机，喷气发动机或MRI扫描仪。它允许在收集数据的地方处理和分析数据，而不是在云中或数据中心。边缘计算不能取代云。它只是允许机器在需要时单独处理新的数据。换句话说，边缘的机器可以根据自己的经验学习和调整，而不依赖于更大的网络。边缘计算解决了网络可访问性和延迟的问题。在边缘计算的发展下，设备可以放置在网络连接不良或不存在的区域，此外，边缘计算还可以抵消用于数据共享的云计算的使用和维护的一些成本。对于计算机视觉软件，这意味着可以实时更好地响应，并且只将相关数据发送到云中进行进一步分析，此功能对自动驾驶汽车特别有用。为了安全运行，车辆将需要收集和分析与其周围环境，方向和天气状况有关的大量数据，更不用说与路上的其他车辆通信，所有这些都没有延迟。如果通过云中心化的解决方案来分析数据可能很危险，因为延迟可能导致事故。三、点云（point cloud）对象识别最近在对象识别和对象跟踪中更频繁使用的技术是点云。简而言之，点云是在三维坐标系内定义的数据点的集合。该技术通常在空间（例如房间或容器）内使用，其中每个对象的位置和形状由坐标列表（X，Y和Z）表示，坐标列表称为“点云”。该技术准确地表示了物体在空间中的位置，并且可以精确地跟踪任何移动。点云的应用是无止境的。以下是一些行业的例子以及他们从这项技术中获得的好处：记录：资产监测，跟踪施工现场，故意破坏检测；分类：城市规划，审计工具，便于分析，绘制必要的公用事业工作变更检测：资产管理，货物跟踪，自然灾害管理。预测性维护：持续监控资产和基础设施，以预测何时需要维修。四、融合现实：VR和AR增强今天，任何VR或AR系统都会创建一个沉浸式3D环境，但它与用户所处的真实环境几乎没有关系。大多数AR设备可以执行简单的环境扫描（例如，Google ARCore可以检测平面和光线条件的变化），VR系统可以通过头部跟踪，控制器等检测用户的运动，但他们的功能也就这样了。计算机视觉软件正在推动VR和AR进入下一阶段的开发，有些人称之为Merged Reality（MR）。借助外部摄像头和传感器映射环境，以及眼动跟踪解决方案和陀螺仪来定位用户，VR和AR系统能够：感知环境并引导用户远离墙壁，物品或其他用户等障碍物。检测用户的眼睛和身体运动并相应地采用VR环境。提供室内环境，公共场所，地下等的指引。Lowe's五金店已在他们的商店中使用它，每个购物者都可以借用AR设备来制作他们的购物清单，并获得商店中每件商品的指示。AR设备可以实时使用楼层平面图，库存信息和环境映射以给出准确的指示。我们也可以通过实时3D面部识别功能更新虚拟艺术家应用程序，让客户可以看到不同的化妆产品在他们的脸上和不同光线条件下的外观。五、语义实例分割为了理解语义实例分割是什么，让我们首先将这个概念分为两 部分：语义分割和实例分割。实例分割在像素级别识别对象轮廓，而语义分割仅将像素分组到特定对象组。让我们使用气球图像来说明与其他技术相比的两种技术：分类：此图像中有一个气球；语义分割：这些都是气球像素；物体检测：此图像中有7个气球，我们开始考虑重叠的对象；实例分割：这些位置有7个气球，这些是属于每个气球的像素；如果放在一起，语义实例分割方法将成为一个强大的工具。该工具不仅可以检测属于图片中对象的所有像素，还可以确定哪些像素属于哪个对象以及对象所在的图片中的位置。语义实例分割是土地覆盖分类的有用工具，具有各种应用。通过卫星图像进行的土地制图可以用于政府机构监测森林砍伐（特别是非法），城市化交通等。许多建筑师事务所也将这些数据用于城市规划和建筑开发，有些人甚至更进一步将其与AR设备相结1合，以了解他们的设计在现实生活中的样子。原文链接本文为云栖社区原创内容，未经允许不得转载。"}
{"content2":"相机标定（Camera calibration）原理、步骤author@jason_ql（lql0716）http://blog.csdn.net/lql0716在图像测量过程以及机器视觉应用中，为确定空间物体表面某点的三维几何位置与其在图像中对应点之间的相互关系，必须建立相机成像的几何模型，这些几何模型参数就是相机参数。在大多数条件下这些参数必须通过实验与计算才能得到，这个求解参数的过程就称之为相机标定（或摄像机标定）。无论是在图像测量或者机器视觉应用中，相机参数的标定都是非常关键的环节，其标定结果的精度及算法的稳定性直接影响相机工作产生结果的准确性。因此，做好相机标定是做好后续工作的前提，提高标定精度是科研工作的重点所在。常用术语内参矩阵: Intrinsic Matrix焦距: Focal Length主点: Principal Point径向畸变: Radial Distortion切向畸变: Tangential Distortion旋转矩阵: Rotation Matrices平移向量: Translation Vectors平均重投影误差: Mean Reprojection Error重投影误差: Reprojection Errors重投影点: Reprojected Points1、坐标系的转换1.1 世界坐标系世界坐标系（world coordinate）(xw,yw,zw)，也称为测量坐标系，是一个三维直角坐标系，以其为基准可以描述相机和待测物体的空间位置。世界坐标系的位置可以根据实际情况自由确定。1.2 相机坐标系相机坐标系（camera coordinate）(xc,yc,zc)，也是一个三维直角坐标系，原点位于镜头光心处，x、y轴分别与相面的两边平行，z轴为镜头光轴，与像平面垂直。1.3 世界坐标系转换为相机坐标系⎡⎣⎢⎢⎢xcyczc1⎤⎦⎥⎥⎥=[R0t1]⎡⎣⎢⎢⎢xwywzw1⎤⎦⎥⎥⎥其中R为3*3的旋转矩阵，t为3*1的平移矢量，(xc,yc,zc,1)T为相机坐标系的齐次坐标，(xw,yw,zw,1)T为世界坐标系的齐次坐标。1.4 像素坐标系、图像坐标系：像素坐标系（pixel coordinate）如，像素坐标系uov是一个二维直角坐标系，反映了相机CCD/CMOS芯片中像素的排列情况。原点o位于图像的左上角，u轴、v轴分别于像面的两边平行。像素坐标系中坐标轴的单位是像素（整数）。像素坐标系不利于坐标变换，因此需要建立图像坐标系XOY，其坐标轴的单位通常为毫米（mm），原点是相机光轴与相面的交点（称为主点），即图像的中心点，X轴、Y轴分别与u轴、v轴平行。故两个坐标系实际是平移关系，即可以通过平移就可得到。图像坐标系转换为像素坐标系⎡⎣⎢uv1⎤⎦⎥=⎡⎣⎢1/dX0001/dY0u0v01⎤⎦⎥⎡⎣⎢XY1⎤⎦⎥其中，dX、dY分别为像素在X、Y轴方向上的物理尺寸，u0,v0为主点（图像原点）坐标。1.5 针孔成像原理：如图，空间任意一点P与其图像点p之间的关系，P与相机光心o的连线为oP，oP与像面的交点p即为空间点P在图像平面上的投影。该过程为透视投影，如下矩阵表示：s⎡⎣⎢XY1⎤⎦⎥=⎡⎣⎢f000f0001000⎤⎦⎥⎡⎣⎢⎢⎢xyz1⎤⎦⎥⎥⎥其中，s为比例因子（s不为0），f为有效焦距（光心到图像平面的距离），(x,y,z,1)T是空间点P在相机坐标系oxyz中的齐次坐标，(X,Y,1)T是像点p在图像坐标系OXY中的齐次坐标。1.6 世界坐标系转换为像素坐标系s⎡⎣⎢uv1⎤⎦⎥=⎡⎣⎢1/dX0001/dY0u0v01⎤⎦⎥⎡⎣⎢f000f0001000⎤⎦⎥[R0t1]⎡⎣⎢⎢⎢xwywzw1⎤⎦⎥⎥⎥=⎡⎣⎢αx000αy0u0v01000⎤⎦⎥[R0t1]⎡⎣⎢⎢⎢xwywzw1⎤⎦⎥⎥⎥=M1M2Xw=MXw其中，αx=f/dX、αy=f/dY，称为u、v轴的尺度因子，M1称为相机的内部参数矩阵，M2称为相机的外部参数矩阵，M称为投影矩阵。2 相机内参与畸变参数2.1 相机内参参看1.6节所述2.2 畸变参数畸变参数(distortion parameters)畸变（distortion）是对直线投影（rectilinear projection）的一种偏移。简单来说直线投影是场景内的一条直线投影到图片上也保持为一条直线。畸变简单来说就是一条直线投影到图片上不能保持为一条直线了，这是一种光学畸变（optical aberration）,可能由于摄像机镜头的原因。畸变一般可以分为：径向畸变、切向畸变1、径向畸变来自于透镜形状2、切向畸变来自于整个摄像机的组装过程畸变还有其他类型的畸变，但是没有径向畸变、切向畸变显著畸变图示径向畸变实际摄像机的透镜总是在成像仪的边缘产生显著的畸变，这种现象来源于“筒形”或“鱼眼”的影响。如下图，光线在原理透镜中心的地方比靠近中心的地方更加弯曲。对于常用的普通透镜来说，这种现象更加严重。筒形畸变在便宜的网络摄像机中非常厉害，但在高端摄像机中不明显，因为这些透镜系统做了很多消除径向畸变的工作。对于径向畸变，成像仪中心（光学中心）的畸变为0，随着向边缘移动，畸变越来越严重。径向畸变包括：枕形畸变、桶形畸变切向畸变切向畸变是由于透镜制造上的缺陷使得透镜本身与图像平面不平行而产生的。切向畸变可分为：薄透镜畸变、离心畸变切向畸变图示：2.2.1 opencv中的畸变模型径向畸变模型：以下公式由泰勒展式得出，在opencv中K=1，r2=x2+y2,(x,y)为真实坐标（发生畸变），(x′,y′)为理想坐标。δxr=x(k1r2+k2r4+k3r6+K)δyr=y(k1r2+k2r4+k3r6+K)切向畸变模型：δxd=2p1xy+p2(r2+2x2)+Kδyd=2p1(r2+2y2)+2p2xy+K理想坐标(x′,y′)与真实坐标(x,y)：x′=x+δxr+δxdy′=y+δyr+δyd也即：[x′y′]=(1+k1r2+k2r4+k3r6)[xy]+[2p1xy+p2(r2+2x2)2p1(r2+2y2)+2p2xy]实际计算过程中，如果考虑太多高阶的畸变参数，会导致标定求解的不稳定。2.2.2 张氏标定中的畸变模型张正友的方法只考虑了径向畸变，没有考虑切向畸变模型：理想情况下(没有畸变)图片的像素坐标为：(u,v)真实的像素坐标为：(u¯,v¯)真实坐标与理想坐标的关系式（泰勒展开）：(x,y)and(x¯,y¯)are the ideal (distortion-free) and real (distorted) normalized image coordinates.x¯=x+x[k1(x2+y2)+k2(x2+y2)2]y¯=y+y[k1(x2+y2)+k2(x2+y2)2]其中k1，k2是径向畸变系数(coefficients of the radial distortion)。像素坐标表示为：u¯=u+(u−u0)[k1(x2+y2)+k2(x2+y2)2]v¯=v+(v−v0)[k1(x2+y2)+k2(x2+y2)2]即：[(u−u0)(x2+y2)(v−v0)(x2+y2)(u−u0)(x2+y2)2(v−v0)(x2+y2)][k1k2]=[u¯−uv¯−v]2.2.3 Tasi畸变模型Tasi畸变模型同张氏畸变模型类似，也只考虑了径向畸变2.3 相机标定相机标定步骤：1、打印一张棋盘格，把它贴在一个平面上，作为标定物。2、通过调整标定物或摄像机的方向，为标定物拍摄一些不同方向的照片。3、从照片中提取棋盘格角点。4、估算理想无畸变的情况下，五个内参和六个外参。5、应用最小二乘法估算实际存在径向畸变下的畸变系数。6、极大似然法，优化估计，提升估计精度。3 Matlab相机标定打开matlab，找到“Camera Calibrator”并打开在新窗口中选择添加图片“Add Images”添加图片之后，会有如下提示，设置棋盘格的实际大小之后，点击ok点击“Calibrate”开始计算点击“Export Camera Parameters”，输出到matlab命令窗口导出的数据如下：cameraParams = cameraParameters (具有属性): Camera Intrinsics IntrinsicMatrix: [3x3 double] FocalLength: [510.6720 511.9534] PrincipalPoint: [332.3800 238.4849] Skew: 0 Lens Distortion RadialDistortion: [0.0618 -0.4877] TangentialDistortion: [0 0] Camera Extrinsics RotationMatrices: [3x3x50 double] TranslationVectors: [50x3 double] Accuracy of Estimation MeanReprojectionError: 0.1923 ReprojectionErrors: [77x2x50 double] ReprojectedPoints: [77x2x50 double] Calibration Settings NumPatterns: 50 WorldPoints: [77x2 double] WorldUnits: 'mm' EstimateSkew: 0 NumRadialDistortionCoefficients: 2 EstimateTangentialDistortion: 0 estimationErrors = cameraCalibrationErrors (具有属性): IntrinsicsErrors: [1x1 intrinsicsEstimationErrors] ExtrinsicsErrors: [1x1 extrinsicsEstimationErrors]1234567891011121314151617181920212223242526272829303132333435363738在命令窗口输入以下命令即可获得内参矩阵和径向畸变：>> cameraParams.IntrinsicMatrix ans = 510.6720 0 0 0 511.9534 0 332.3800 238.4849 1.0000 >> cameraParams.RadialDistortion ans = 0.0618 -0.487712345678910111213参考资料相关文章机器学习、深度学习、计算机视觉、自然语言处理及应用案例——干货分享（持续更新……）旋转矩阵、欧拉角、四元数理论及其转换关系基于生长的棋盘格角点检测方法–（1）原理介绍opencv角点检测、棋盘格检测、亚像素cvFindCornerSubPix（）相机标定(Camera calibration)OpenCV相机标定和姿态更新Camera Calibration and 3D ReconstructionCamera Calibration相机姿态估计（二）–单目POSIT算法相机位姿估计1：根据四个特征点估计相机姿态Head Pose Estimation using OpenCV and Dlibopencv角点检测、棋盘格检测、亚像素cvFindCornerSubPix（）关于OpenCV的那些事——相机标定计算机视觉-相机内参数和外参数张正友相机标定算法 【2】Python+OpenCV学习（17）—摄像机标定相机标定Opencv实现以及标定流程&&标定结果评价&&图像矫正流程解析（附标定程序和棋盘图）张正友相机标定Opencv实现以及标定流程&&标定结果评价&&图像矫正流程解析（附标定程序和棋盘图）MATLAB–相机标定教程《学习opencv》张正友相机标定论文《A Flexible New Technique for Camera Calibration》OpenCV相机标定畸变模型相机的那些事儿 - 概念、模型及标定相机标定（二）之相机畸变模型"}
{"content2":"浪潮发布“元脑”人工智能系统4月16日，服务器巨头浪潮在上海召开的云数据中心合作伙伴大会(IPF2019)上发布了“浪潮元脑”，意在输出人工智能全栈能力。中国工程院院士、浪潮集团执行总裁王恩东在会上表示，在智慧计算时代，人工智能计算代表智慧计算的发展方向，是未来核心动力，人工智能将成为浪潮智慧计算战略的核心。王恩东表示，浪潮将在技术、产品、模式和生态等四个方面持续投入，加大领先优势。何为“元脑”据介绍，“浪潮元脑”包含浪潮领先的场景化人工智能基础设施，多样化的深度学习框架与工具，以及最新研发的人工智能 PaaS平台和AutoML Suite等“有形”产品，以及浪潮多年积累的人工智能算法优化、系统优化服务、整合一体化交付的“无形”能力。浪潮在国内人工智能基础架构市场份额第一，也是少数几家具有人工智能全栈方案能力的供应商之一，在芯片、系统、框架和管理等四个层面有着完整的产品技术布局。根据第三方数据公司IDC的数据，2018年上半年，浪潮在中国人工智能基础架构市场的份额为51.4%，超过其他厂商的总和。2018年，浪潮推出了人工智能超算系统AGX-5，每秒性能达到2千万亿次，是业界性能最强的单机人工智能计算系统。另外，浪潮开发的管理软件AIStation平台软件被客户广泛应用，帮助大华股份秒级构建人工智能环境，实现了GPU资源的智能调度，资源利用率提高75%，性能利用率提高100%。据介绍，目前浪潮已深度参与和主导到全球人工智能的产品技术标准和基准性能测试标准中。在OCP开放计算社区，浪潮加入了OAM((OCP Accelerator Module)项目组，参与了第一代OAM标准的制定。在全球系统性能评测标准组织SPEC(Standard Performance Evaluation Corporation)中，浪潮发起成立了机器学习工作组(SPEC Machine Learning)，贡献了首个工作负载，带领该工作组进行机器学习基准研究，正在展开行业领先方法的评估工作，以衡量机器学习工作负载和系统的性能。浪潮发布“元脑”人工智能系统浪潮启动“元脑生态计划”郑州妇科医院:https://myyk.familydoctor.com.cn/21521/2019年，人工智能生态将成为浪潮生态建设的核心与发力重点。浪潮在会上宣布启动“元脑生态计划”，聚焦计算机视觉、语音识别、自然语言识别、量化交易等四个基础应用场景，发展100家以上的合作伙伴，开发100个以上的联合解决方案，帮助400家以上的传统ISV建立人工智能技术能力，覆盖金融、企业、通信、教育等8个主流行业。目前，中国人工智能企业100强有80家与浪潮建立了合作关系。2018年初，浪潮提出了新的增长目标——5年内成为全球最大的服务器供应商。浪潮认为，随着“AI产业化”的蓬勃发展，势必会迎来“产业AI化”的更大市场。聚焦人工智能、发展产业生态的策略将为浪潮注入新的增长动力。IDC数据显示，2018年，浪潮销售额和销量增速均为行业最高，进入全球X86服务器市场份额TOP3。此外，浪潮接下来有意进一步拓展海外业务。2018年，浪潮海外业务同比增长350%，覆盖了120个国家和地区，有8个全球研发中心、6个全球生产中心以及2个全球服务中心。"}
{"content2":"现在人们总说人工智能，智能家电，可是现在机器的思维是由程序所提供的。那本身就是人用人的思想去做的东西，机器本身不具备思考能力，只有计算能力，判断的分叉由人所给定，达到一定计算值进行选择分叉。可是人对不同的值有不同的想法，可机器只有唯一人所人为判定的选择。现在人工只能只能从大量的普遍的去找出大多数人的要求，在其中设定不同的判断值。可是特性人群的想法基本要被舍去，程序为大多数人而设计。甚至慢慢的要求人去按照操作去适应人工智能。其实人的思维来源，主要是看人的成长经历，不同的经历可能看待一件事不同。例如打水，虽然有抽水机，有人觉得打一点水，无谓用电，直接用桶打。可是有人为了方便，直接无论水量，直接去开抽水机。有的人觉得打水不急，先用完再说。很多特殊的情况，可能让人工智能也会变得很被动。如果本来熟悉这样的要求，突然情况有变，找不到相应的对策或者方案，叫计算机如何是好？直接死循环？当然，这不符合程序员的设计标准。可能最好的方法是直接告诉用户出错了，请反馈情况…然后手动选择需要的方案。如何让计算机具有思维能力，自动根据值来判断…这可能是现在一直解决不了的问题，现有的编程语言都是通过人来控制判断，无法做到真正的人工智能。应该这样说，现在是要机器去习惯人的思维。可是人的思维不单纯是一个个判断，还有各种情感的参及。如果机器有了思维，应该也会有了情感，还应该有触觉，视觉，味觉，这一系列反映出来的各种数值。这些数值对人来说是一种感觉，对机器也是一种判断依据。应该说，本来就没有标准，没有定界，只是人为了满足某些特定需要而进行强制性的定届而已。没见过一米的人，不知道一米是多少，甚至不知道什么是一米。把人的某些感官定届了，让人和机器有了统一标准罢了。想想人是获得思维过程的话，其实也是很简单。出生的我们，只是具有简单的对生存反射，有东西到口就吃，饿了就哭，不舒服就哭，讨厌就哭。可是慢慢的长大，就会懂得找不同的方式表达自己的饿，还是不舒服。我们的思维都是从有记忆开始的，然后通过学习各种人为的道理和道德，规矩，规律来进行生活。慢慢的就有了自己思维的习惯，有了自己对事情的独特判断。最初，我们不明白兔子是什么东西，老师指着图片说，这就是兔子。你就开始明白，白色毛，耳朵长长，红色眼睛，四条腿的，就是兔子。听到老师说它会跳，喜欢吃红萝卜，看上去很可爱等等，你才会开始把这些特性赋予给兔子这个东西。当你自己亲身摸到兔子，感受到毛的柔软，它的重量，它的大小，吓唬它会跳等等。这些感官信息也会完善到你对兔子的特性中。当你要抓住兔子，你需要看到兔子跳动的距离，规律进行判断控制自己已经学习到走路的步伐大小，手部的动作速度，自己抓住的方向，力度。随着生活经历，你对兔子的理解会越来越深。好吧，说回计算机。计算机已经具有了我们人类记忆的功能了，还有各种传感器，可以让计算机获取到人感官的数据。可是计算机所不具备的是人的学习能力，它可以把兔子的信息，图像，重量，颜色，甚至到兔子各种精确的数据，把这些数据赋予到兔子这个定义中。可是计算机如何自动“学习”到这些特性，在自己想去抓兔子时候自动生成计算机的思维—程序进行捕捉。有人说，通过使用反射机制，把刚才获取到的数值直接分析出来，带入到捕捉的程序当中，控制轮子的速度，机械臂的角度，伸出的长度，机械手的力度，不就可以啦。是这样没错，问题又回到了计算机思维当中了，人可能看到兔子想抓是正常的。可是机器人在看到兔子，分析定义兔子后，哪里给它的指令去捕捉。如果是人，那就算不上人工只能。说是机器自身，如何实现？本来以为在想通计算机通过数据获取，反射来获取对应的程序，得到类似人类思维以后，问题突然就来了。解决计算机学习的问题之后，才开始发现人到底是先有思维，再有记忆？还是先有记忆，再有思维？思维的本质是什么？思维在什么时候产生？情感是思维所创造还是情感本身就是独立的？额，想着想着就从科学变成哲学了…"}
{"content2":"原帖地址： http://blog.sciencenet.cn/blog-370458-750306.html关于计算机视觉和模式识别领域的期刊并不是很多，下面我收集了一些该领域的代表性期刊，并介绍了他们的影响因子以及投稿难度和审稿周期。希望对大家有帮助吧，后期大家还有发现的可以留言，补充哦。首先介绍计算机视觉领域的4个顶级代表性期刊吧。(1) IEEE Transactions on Pattern Analysis and Machine Intelligence，IEEE模式分析与机器智能汇刊，简称PAMI，是IEEE最重要的学术性汇刊之一。在各种统计中，PAMI被认为有着很强的影响因子（2011年影响因子为4.9）和很高的排名。显然，这个期刊的中稿难度那是相当的大，一般先投中CVPR之后再加点东西投该期刊会比较好中一点。(2) ACM Transactions on Graphics。美国计算机协会图形汇刊，简称TOG，该刊侧重于计算机图形的处理，影响因子在该领域也比较高，2011年为3.5。中稿的难度也极大，一般该刊对每年的SIGGRAPH(Special Interest Group for Computer GRAPHICS,计算机图形图像特别兴趣小组）会议论文全文收录。(3) International Journal of Computer vision，该刊也是该领域的顶级期刊之一，相比于PAMI来讲，该刊侧重于理论的推导。2011年影响因子为3.7，中稿难度也相当大。(4) IEEE Transactions on Image Processing，该刊也是图像处理领域的代表性期刊之一，相比于上面三个期刊来讲，该刊稍微低一点层次。2011年影响因子为3.042，中稿难度也比较大。审稿周期一年左右。除了上述让人望而生畏的顶级期刊之外，我们再看看一般的期刊吧。(1)Pattern recognition letters, 从投稿到发表，一年半时间;(2)Pattern recognition 不好中，时间长;(3)IEICE Transactions on Information and Systems， 作者中有一个必须是会员。收费高，审稿快。影响因子0.4;(4)International Journal of Pattern Recognition and Artificial Intelligence ， 审稿周期一般6--12周，影响因子偏低，容易中;(5)Computational Intelligence， 中等偏上，要求较高，杂志级别不错，关注人数偏少，比较冷门;(6)information processing letters, 影响因子低0.5左右，接搞量大，容易发表，审稿周期一般3--6个月;(7)Computer vision and image understanding, 9个月审稿期，平均投稿命中率20%，业内比较认可;(8)journal of visual communication and image representation， 投稿容易，审稿周期一年以上;(9)Signal processing letters, 影响因子0.99， 美国，审稿一个多月;(10)International Journal on Graphics, Vision and Image Processing (GVIP);(11)IET Image Processing, 影响因子0.758， EI Compendex ，审稿周期一年以上;(12)IET Computer Vision ，影响因子0.969;(13)SIAM Journal on Imaging Sciences;(14)International journal of imaging systems and technology，影响因子偏低，容易中，审稿周期一到两个月;(15)IEEE Signal Processing Letters， 审稿4---8周左右，影响因子不高，容易中，关注人不多;(16)Journal of Logic and Computation, 影响因子，0.789，SCI检索;(17)IEICE Transactions on Information and Systems 审稿时间2--4周，容易中，影响因子小，相对冷门，关注人数不多;(18)COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING，影响因子偏低，但仍然需要一定水平才可以投，审稿2--4周，SCI,EI检索;(19)Signal Processing: Image Communication，容易中，审稿周期半年到一年;(20)International Journal of Computer Vision， 较难，审稿周期半年到一年，EI,SCI检索;(21)Journal of Mathematical Imaging and Vision，审稿一个月左右，影响因子不高（1.3左右），Elsevier旗下，不容易中，稍微有些冷门，偏重数学推导;(22)Machine Vision and Applications， 影响因子偏低，但是接稿量不是很大，审稿周期一年以上，但容易发表，SCI,EI检索;(23)Pattern Analysis & Applications， 影响因子不高，影响力也比较小，审稿时间一年以上，但容易投中;(24)Signal Image and Video Processing， 容易中，审稿时间半年到一年，EI检索;(25)Pattern recognition and image analysis， EI检索;(26)Journal of digital imaging ，审稿周期半年到一年，影响因子偏低，容易中，很少有人关注;(27)Journal of  VLSI signal processing systems for signal image and video ，影响因子偏低，容易中，审稿周期一年以上，关注人比较少;(28) Neural Processing Letters,  影响因子0.75左右，审稿三个月内给出审稿意见，比较快。（发现一年只发表20篇左右，一年投稿量估计200多篇（从编号可估出），可看出命中率绝对在10%以下，待考察）(29) COMPUTERS & GRAPHICS-UK (一般简称为COMPUTERS & GRAPHICS)，Elsevier旗下图像处理领域期刊之一，2011影响因子为1.0。审稿速度（同行例子：9月底投稿，10月中旬送审，12月初大修，2月中旬小修后录用。审稿速度和编辑处理速度都比较快！）。感觉要求不是很高！(30) EURASIP Journal on Image and Video Processing，影响因子2011年为0.5，有同学投过，速度比较快，2-3个月搞定。(31) Multimedia Tools and Applications，2012年影响因子为1.014，据说比较好中，速度也还可以。(32) Communications of the ACM，2012年影响因子为2.511。中科院分区SCI大类分区2区，小类分区2区。(33) IEEE Transactions on Visualization and Computer Graphics，2012年影响因子为1.898，中科院分区SCI大类分区2区，小类分区2区。(34) IEEE Computer Graphics and Applications，2012年影响因子为1.228，中科院分区SCI大类分区3区，小类分区2区。(35) Graphical Models，2012年影响因子为0.697，中科院分区SCI大类分区4区，小类分区4区。(36) Computer Aided Geometric Design，2012年影响因子为0.810，中科院分区SCI大类分区4区，小类分区3区。(37) Computer Animation and Virtual Worlds，2012年影响因子为0.436，中科院分区SCI大类分区4区，小类分区4区。(38) Visual Computer，2012年影响因子为0.909，中科院分区SCI大类分区4区，小类分区4区。(39) Computer Graphics Forum，2012年影响因子为1.638，中科院分区SCI大类分区3区，小类分区2区。(40) International Journal of Computational Geometry and Applications，2012年影响因子为0.176，中科院（数学方向）分区SCI大类分区4区，小类分区4区。(41) Computational Geometry-Theory and Applications，2012年影响因子为0.545，中科院（数学方向）分区SCI大类分区2区，小类分区2区。(42) Journal of Visualization，2012年影响因子为0.506，中科院（工程技术）分区SCI大类分区4区，小类分区4区。(43) Computer Graphics World，2012年影响因子为0.000，中科院（工程技术）分区SCI大类分区4区，小类分区4区。(44) Virtual Reality，2012年影响因子为0.341，中科院（工程技术）分区SCI大类分区4区，小类分区4区。(43) Image and Vision Computing，2012年影响因子为1.959，中科院（工程技术）分区SCI大类分区3区，小类分区3区。(43) Computer Graphics World，2012年影响因子为0.000，中科院（工程技术）分区SCI大类分区4区，小类分区4区。from: http://blog.csdn.net/inter_xuxing/article/details/19300157"}
{"content2":"牛人主页（主页有很多论文代码）Serge Belongie at UC San DiegoAntonio Torralba at MITAlexei Ffros at CMUCe Liu at Microsoft Research New EnglandVittorio Ferrari at Univ.of EdinburghKristen Grauman at UT AustinDevi Parikh at TTI-Chicago (Marr Prize at ICCV2011)John Wright at Columbia Univ.Piotr Dollar at CalTechBoris Babenko at UC San DiegoDavid Ross at Google/YoutubeDavid Donoho at Stanford Univ.大神们：William T. Freeman at MITRoberto Cipolla at CambridgeDavid Lowe at Univ. of British ColumbiaMubarak Shah at Univ. of Central FloridaYi Ma at MSRATinne Tuytelaars at K.U. LeuvenTrevor Darrell at U.C. BerkeleyMichael J. Black at Brown Univ.重要研究组：Computer Vision Group at UC BerkeleyRobotics Research Group at Univ. of OxfordLEAR at INRIAComputer Vision Lab at StanfordComputer Vision Lab at EPFLComputer Vision Lab at ETH ZurichComputer Vision Lab at Seoul National Univ.Computer Vision Lab at UC San DiegoComputer Vision Lab at UC Santa CruzComputer Vision Lab at Univ. of Southern CaliforniaComputer Vision Lab at Univ. of Central FloridaComputer Vision Lab at Columbia Univ.UCLA Vision LabMotion and Shape Computing Group at George Mason Univ.Robust Image Understanding Lab at Rutgers Univ.Intelligent Vision Systems Group at Univ. of BonnInstitute for Computer Graphics and Vision at Graz Univ. of Tech.Computer Vision Lab. at Vienna Univ. of Tech.Computational Image Analysis and Radiology at Medical Univ. of ViennaPersonal Robotics Lab at CMUVisual Perception Lab at Purdue Univ.潜力牛人：Juergen Gall at ETH ZurichMatt Flagg at Georgia Tech.Mathieu Salzmann at TTI-ChicagoGerg Shakhnarovich at TTI-ChicagoTaeg Sang Cho at MITJianchao Yang at UIUCStefan Roth at TU DarmstadtPeter Kontschieder at Graz Univ. of Tech.Dominik Alexander Klein at Univ. of BonnYinan Yu at CASIA (PASCAL VOC 2010 Detection Challenge Winner)Zdenek Kalal at FPFLJulien Pilet at FPFLKenji Okuma（1）googleResearch； http://research.google.com/index.html（2）MIT博士，汤晓欧学生林达华； http://people.csail.mit.edu/dhlin/index.html（3）MIT博士后Douglas Lanman； http://web.media.mit.edu/~dlanman/（4）opencv中文网站； http://www.opencv.org.cn/index.php/%E9%A6%96%E9%A1%B5（5）Stanford大学vision实验室； http://vision.stanford.edu/research.html（6）Stanford大学博士崔靖宇； http://www.stanford.edu/~jycui/（7）UCLA教授朱松纯； http://www.stat.ucla.edu/~sczhu/（8）中国人工智能网； http://www.chinaai.org/（9）中国视觉网； http://www.china-vision.net/（10）中科院自动化所； http://www.ia.cas.cn/（11）中科院自动化所李子青研究员； http://www.cbsr.ia.ac.cn/users/szli/（12）中科院计算所山世光研究员； http://www.jdl.ac.cn/user/sgshan/（13）人脸识别主页； http://www.face-rec.org/（14）加州大学伯克利分校CV小组； http://www.eecs.berkeley.edu/Research/Projects/CS/vision/（15）南加州大学CV实验室； http://iris.usc.edu/USC-Computer-Vision.html（16）卡内基梅隆大学CV主页；http://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.html（17）微软CV研究员Richard Szeliski；http://research.microsoft.com/en-us/um/people/szeliski/（18）微软亚洲研究院计算机视觉研究组； http://research.microsoft.com/en-us/groups/vc/（19）微软剑桥研究院ML与CV研究组； http://research.microsoft.com/en-us/groups/mlp/default.aspx（20）研学论坛； http://bbs.matwav.com/（21）美国Rutgers大学助理教授刘青山； http://www.research.rutgers.edu/~qsliu/（22）计算机视觉最新资讯网； http://www.cvchina.info/（23）运动检测、阴影、跟踪的测试视频下载； http://apps.hi.baidu.com/share/detail/18903287（24）香港中文大学助理教授王晓刚； http://www.ee.cuhk.edu.hk/~xgwang/(25)香港中文大学多媒体实验室（汤晓鸥）; http://mmlab.ie.cuhk.edu.hk/(26)U.C. San Diego. computer vision;http://vision.ucsd.edu/content/home(27)CVonline; http://homepages.inf.ed.ac.uk/rbf/CVonline/(28)computer vision software; http://peipa.essex.ac.uk/info/software.html(29)Computer Vision Resource; http://www.cvpapers.com/(30)computer vision research groups;http://peipa.essex.ac.uk/info/groups.html(31)computer vision center; http://computervisioncentral.com/cvcnews(32)浙江大学图像技术研究与应用（ITRA）团队：http://www.dvzju.com/(33)自动识别网：http://www.autoid-china.com.cn/(34)清华大学章毓晋教授：http://www.tsinghua.edu.cn/publish/ee/4157/2010/20101217173552339241557/20101217173552339241557_.html(35)顶级民用机器人研究小组Porf.Gary领导的Willow Garage:http://www.willowgarage.com/(36)上海交通大学图像处理与模式识别研究所：http://www.pami.sjtu.edu.cn/(37)上海交通大学计算机视觉实验室刘允才教授：http://www.visionlab.sjtu.edu.cn/(38)德克萨斯州大学奥斯汀分校助理教授Kristen Grauman ：http://www.cs.utexas.edu/~grauman/(39)清华大学电子工程系智能图文信息处理实验室（丁晓青教授）：http://ocrserv.ee.tsinghua.edu.cn/auto/index.asp(40)北京大学高文教授：http://www.jdl.ac.cn/htm-gaowen/(41)清华大学艾海舟教授：http://media.cs.tsinghua.edu.cn/cn/aihz(42)中科院生物识别与安全技术研究中心：http://www.cbsr.ia.ac.cn/china/index%20CH.asp(43)瑞士巴塞尔大学 Thomas Vetter教授：http://informatik.unibas.ch/personen/vetter_t.html(44)俄勒冈州立大学 Rob Hess博士：http://blogs.oregonstate.edu/hess/(45)深圳大学 于仕祺副教授：http://yushiqi.cn/(46)西安交通大学人工智能与机器人研究所：http://www.aiar.xjtu.edu.cn/(47)卡内基梅隆大学研究员Robert T. Collins:http://www.cs.cmu.edu/~rcollins/home.html#Background(48)MIT博士Chris Stauffer:http://people.csail.mit.edu/stauffer/Home/index.PHP(49)美国密歇根州立大学生物识别研究组(Anil K. Jain教授)：http://www.cse.msu.edu/rgroups/biometrics/(50)美国伊利诺伊州立大学Thomas S. Huang:http://www.beckman.illinois.edu/directory/t-huang1(51)武汉大学数字摄影测量与计算机视觉研究中心：http://www.whudpcv.cn/index.asp(52)瑞士巴塞尔大学Sami Romdhani助理研究员：http://informatik.unibas.ch/personen/romdhani_sami/(53)CMU大学研究员Yang Wang:http://www.cs.cmu.edu/~wangy/home.html(54)英国曼彻斯特大学Tim Cootes教授：http://personalpages.manchester.ac.uk/staff/timothy.f.cootes/(55)美国罗彻斯特大学教授Jiebo Luo:http://www.cs.rochester.edu/u/jluo/(56)美国普渡大学机器人视觉实验室：https://engineering.purdue.edu/RVL/Welcome.html(57)美国宾利州立大学感知、运动与认识实验室：http://vision.cse.psu.edu/home/home.shtml(58)美国宾夕法尼亚大学GRASP实验室：https://www.grasp.upenn.edu/(59)美国内达华大学里诺校区CV实验室：http://www.cse.unr.edu/CVL/index.php(60)美国密西根大学vision实验室：http://www.eecs.umich.edu/vision/index.html(61)University of Massachusetts(麻省大学),视觉实验室：http://vis-www.cs.umass.edu/index.html(62)华盛顿大学博士后Iva Kemelmacher:http://www.cs.washington.edu/homes/kemelmi(63)以色列魏茨曼科技大学Ronen Basri:http://www.wisdom.weizmann.ac.il/~ronen/index.html(64)瑞士ETH-Zurich大学CV实验室：http://www.vision.ee.ethz.ch/boostingTrackers/index.htm(65)微软CV研究员张正友：http://research.microsoft.com/en-us/um/people/zhang/(66)中科院自动化所医学影像研究室：http://www.3dmed.NET/(67)中科院田捷研究员：http://www.3dmed.Net/tian/(68)微软Redmond研究院研究员Simon Baker:http://research.microsoft.com/en-us/people/sbaker/(69)普林斯顿大学教授李凯：http://www.cs.princeton.edu/~li/(70)普林斯顿大学博士贾登：http://www.cs.princeton.edu/~jiadeng/(71)牛津大学教授Andrew Zisserman： http://www.robots.ox.ac.uk/~az/(72)英国leeds大学研究员Mark Everingham:http://www.comp.leeds.ac.uk/me/(73)英国爱丁堡大学教授Chris William: http://homepages.inf.ed.ac.uk/ckiw/(74)微软剑桥研究院研究员John Winn: http://johnwinn.org/(75)佐治亚理工学院教授Monson H.Hayes：http://savannah.gatech.edu/people/mhayes/index.html(76)微软亚洲研究院研究员孙剑：http://research.microsoft.com/en-us/people/jiansun/(77)微软亚洲研究院研究员马毅：http://research.microsoft.com/en-us/people/mayi/(78)英国哥伦比亚大学教授David Lowe: http://www.cs.ubc.ca/~lowe/(79)英国爱丁堡大学教授Bob Fisher: http://homepages.inf.ed.ac.uk/rbf/(80)加州大学圣地亚哥分校教授Serge J.Belongie:http://cseweb.ucsd.edu/~sjb/(81)威斯康星大学教授Charles R.Dyer: http://pages.cs.wisc.edu/~dyer/(82)多伦多大学教授Allan.Jepson: http://www.cs.toronto.edu/~jepson/(83)伦斯勒理工学院教授Qiang Ji: http://www.ecse.rpi.edu/~qji/(84)CMU研究员Daniel Huber: http://www.ri.cmu.edu/person.html?person_id=123(85)多伦多大学教授：David J.Fleet: http://www.cs.toronto.edu/~fleet/(86)伦敦大学玛丽女王学院教授Andrea Cavallaro:http://www.eecs.qmul.ac.uk/~andrea/(87)多伦多大学教授Kyros Kutulakos: http://www.cs.toronto.edu/~kyros/(88)杜克大学教授Carlo Tomasi: http://www.cs.duke.edu/~tomasi/(89)CMU教授Martial Hebert: http://www.cs.cmu.edu/~hebert/(90)MIT助理教授Antonio Torralba: http://web.mit.edu/torralba/www/(91)马里兰大学研究员Yasel Yacoob: http://www.umiacs.umd.edu/users/yaser/(92)康奈尔大学教授Ramin Zabih: http://www.cs.cornell.edu/~rdz/(93)CMU博士田渊栋: http://www.cs.cmu.edu/~yuandong/(94)CMU副教授Srinivasa Narasimhan: http://www.cs.cmu.edu/~srinivas/(95)CMU大学ILIM实验室：http://www.cs.cmu.edu/~ILIM/(96)哥伦比亚大学教授Sheer K.Nayar: http://www.cs.columbia.edu/~nayar/(97)三菱电子研究院研究员Fatih Porikli ：http://www.porikli.com/(98)康奈尔大学教授Daniel Huttenlocher：http://www.cs.cornell.edu/~dph/(99)南京大学教授周志华：http://cs.nju.edu.cn/zhouzh/index.htm(100)芝加哥丰田技术研究所助理教授Devi Parikh: http://ttic.uchicago.edu/~dparikh/index.html(101)瑞士联邦理工学院博士后Helmut Grabner: http://www.vision.ee.ethz.ch/~hegrabne/#Short_CV(102)香港中文大学教授贾佳亚：http://www.cse.cuhk.edu.hk/~leojia/index.html(103)南洋理工大学副教授吴建鑫：http://c2inet.sce.ntu.edu.sg/Jianxin/index.html(104)GE研究院研究员李关：http://www.cs.unc.edu/~lguan/(105)佐治亚理工学院教授Monson Hayes:http://savannah.gatech.edu/people/mhayes/(106)图片检索国际会议VOC(微软剑桥研究院组织): http://pascallin.ecs.soton.ac.uk/challenges/VOC/(107)机器视觉开源处理库汇总：http://archive.cnblogs.com/a/2217609/(108)布朗大学教授Benjamin Kimia: http://www.lems.brown.edu/kimia.htmlabout multi-camera: http://server.cs.ucf.edu/~vision/projects.htmlabout 3D Voxel Coloring Rob Hess: http://blogs.oregonstate.edu/hess/code/voxels/About the particle filters--condensation filter:http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/ISARD1/condensation.htmlMachine Learning Open Source Software：http://jmlr.csail.mit.edu/mloss/1、动作识别数据库：Recognition of human actions：http://www.nada.kth.se/cvap/actions/2、Datasets for Computer Vision Research：http://www-cvr.ai.uiuc.edu/ponce_grp/data/3、Computer Vision Datasets:http://clickdamage.com/sourcecode/cv_datasets.php4、里面有好多基本算法 matlab： http://www.mathworks.cn/index.html5、CVPR 2011中关于grassmann 流形文章的源码： http://itee.uq.edu.au/~uqmhara1/code.htmlMatlab Codefor Graph Embedding Discriminant Analysis on Grassmannian Manifolds for Improved Image Set Matching (CVPR), 2011.Matlab Codefor Optimal Local Basis: A Reinforcement Learning Approach for Face Recognition(IJCV), vol. 81, no. 2, pp. 191-204, 2009.牛人bolg：1、Hong Kong Polytechnic University ：http://www4.comp.polyu.edu.hk/~cslzhang/2、Computer Vision Resources：资源非常丰富，包含有基本算法。https://netfiles.uiuc.edu/jbhuang1/www/resources/vision/index.html3、源代码非常丰富~~ http://homepage.tudelft.nl/19j49/Publications.htmlCVonlinehttp://homepages.inf.ed.ac.uk/rbf/CVonlinehttp://homepages.inf.ed.ac.uk/rbf/CVonline/unfolded.htmhttp://homepages.inf.ed.ac.uk/rbf/CVonline/CVentry.htm李子青的大作：Markov Random Field Modeling in Computer Visionhttp://www.cbsr.ia.ac.cn/users/szli/mrf_book/book.htmlHandbook of Face Recognition (PDF)http://www.umiacs.umd.edu/~shaohua/papers/zhou04hfr.pdf张正友的有关参数鲁棒估计著作：Parameter Estimation Techniques:A Tutorial with Application to Conic Fittinghttp://research.microsoft.com/~zhang/INRIA/Publis/Tutorial-Estim/Main.htmlAndrea Fusiello“计算机视觉中的几何”教程：Elements of Geometric Computer Visionhttp://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/FUSIELLO4/tutorial.html#x1-520007有关马尔可夫蒙特卡罗方法的资料：An introduction to Markov chain Monte Carlohttp://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/SENEGAS/mcmc.htmlMarkov Chain Monte Carlo for Computer Vision--- A tutorial at ICCV05http://civs.stat.ucla.edu/MCMC/MCMC_tutorial.htm有关独立成分分析（Independent Component Analysis , ICA）的资料：An ICA-Pagehttp://www.cnl.salk.edu/~tony/ica.htmlFast ICAhttp://www.cis.hut.fi/projects/ica/fastica/The Kalman Filter (介绍卡尔曼滤波器的终极网页)http://www.cs.unc.edu/~welch/kalman/index.htmlCached k-d tree search for ICP algorithmshttp://kos.informatik.uni-osnabrueck.de/download/3dim2007/paper.html几个计算机视觉研究工具Machine Vision Toolbox for Matlabhttp://www.petercorke.com/Machine%20Vision%20Toolbox.htmlMatlab and Octave Function for Computer Vision and Image Processinghttp://www.csse.uwa.edu.au/~pk/research/matlabfns/Bayes Net Toolbox for Matlabhttp://www.cs.ubc.ca/~murphyk/Software/BNT/bnt.htmlOpenCV (Chinese)http://www.opencv.org.cn/index.php/%E9%A6%96%E9%A1%B5Gandalf (A Computer Vision and Numerical Algorithm Labrary)http://gandalf-library.sourceforge.net/CMU Computer Vision Home Pagehttp://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.htmlMachine Learning Resource Linkshttp://www.cse.ust.hk/~ivor/resource.htmThe Bayesian Filtering Libraryhttp://www.orocos.org/bflOptical Flow Algorithm Evaluation (提供了一个动态贝叶斯网络框架，例如递归信息处理与分析、卡尔曼滤波、粒子滤波、序列蒙特卡罗方法等，C++写的)http://of-eval.sourceforge.net/MATLAB code for ICP algorithmhttp://www.usenet.com/newsgroups/comp.graphics.visualization/msg00102.html牛人主页：朱松纯 （Song-Chun Zhu）http://www.stat.ucla.edu/~sczhu/David Lowe (SIFT) (很帅的一个老头哦 ^ ^)http://www.cs.ubc.ca/~lowe/Andrea Vedaldi (SIFT)http://vision.ucla.edu/~vedaldi/index.htmlPedro F. Felzenszwalbhttp://people.cs.uchicago.edu/~pff/Dougla Dlanman (Brown的一个研究生，在其主页上搜集了大量算法教程和源码)http://mesh.brown.edu/dlanman/courses.htmlJianbo Shi (Ncuts 的始作俑者)http://www.cis.upenn.edu/~jshi/Active Vision Group (Oxford的一个机器视觉研究团队，特色是SLAM，监视，导航)http://www.robots.ox.ac.uk/ActiveVision/index.htmlJuyang Weng（机器学习的专家，Autonomous Mental Development 是其特色）http://www.cse.msu.edu/~weng/测试图片或视频：Middlebury College‘s Stereo Vision Data Sethttp://cat.middlebury.edu/stereo/data.htmlIntelligent Vehicle:IVSourcewww.ivsoruce.netRobot Carhttp://www.plyojump.com/robot_cars.htmlHow to Build a Robot: The Computer Vision Parthttp://www.societyofrobots.com/programming_computer_vision_tutorial.shtml收集的一般牛人主页（带代码）:Xiaofei He(machine learning code)http://people.cs.uchicago.edu/~xiaofei/YingNian Wu(active base model code)http://www.stat.ucla.edu/~ywu/research.html布朗大学计算机主页（可找到该校CS牛人博客）http://www.cs.brown.edu/research/areas.htmlNavneet Dalal(Histograms of Oriented Gradients for Human Detection )http://www.navneetdalal.com/softwarePaul Viola(Robust Real-time Object Detection)http://research.microsoft.com/en-us/um/people/viola/Active LearningRMw平坦软件园http://active-learning.net/，这里包括了关于Active Learning理论以及应用的一些文章，特别是那篇Survey。Transfer LearningRMw平坦软件园http://www.cse.ust.hk/TL/，包括经典的论文以及附带有源码，很方便。Gaussian ProcessesRMw平坦软件园RMw平坦软件园http://www.gaussianprocess.org 包括相关的书籍（有 Carl Edward Rasmussen 的书），相关的程序以及分类的 paper 列表。这也是由 Carl 自己维护的，他应该是将 GP 引入 machine learning 最早的人之一了吧，Hinton 的学生。Nonparametric Bayesian MethodsRMw平坦软件园http://www.cs.berkeley.edu/~jordan/npb.html 这个一看就知道是 Jordan 维护的，主要包括 Dirichlet process 以及相关的其他随机过程在 machine learning 里面如何进行建模，如何进行 approximate inference。主要是文章列表。Probabilistic Graphical ModelRMw平坦软件园http://www.cs.ubc.ca/~murphyk/Bayes/bnintro.html 是 Kevin Murphy 所维护的关于 Bayesian belief networks 的介绍，含有最基本的概念、相关的文献和软件的链接。罕见的 UCB 出来的不是 Jordan 的学生（老板是 Stuart Russel）。http://www.cs.berkeley.edu/~jordan/graphical.html 是 Jordan 系关于这个方面的论文汇编。http://www.inference.phy.cam.ac.uk/hmw26/crf/ 是关于 Conditional Random Fields 方面论文和软件的收集，由 Hanna Wallach 维护。Compressed SensingRMw平坦软件园http://www-dsp.rice.edu/cs 这是 Rice 大学维护的论文分类列表、软件链接等。推荐 Emmanuel Candès 所写的tutorial，这人是 David Donoho 的学生。TensorRMw平坦软件园http://csmr.ca.sandia.gov/~tgkolda/pubs/index.html 关于 tensor 的一些偏数学的文章。Deep Belief NetworkRMw平坦软件园http://www.cs.toronto.edu/~hinton/csc2515/deeprefs.html 是 Geoffrey Hinton 为研究生开设的 machine learning 课程的 DBN 的 reading list。Kernel MethodsRMw平坦软件园http://www.cs.berkeley.edu/~jordan/kernels.html 是 Jordan 维护的关于 kernel methods 的文章列表。Markov LogicRMw平坦软件园http://ai.cs.washington.edu/pubs 是 UW AI 组的文章，里面关于 Markov logic 的比较多，因为 Pedro Domingos 就是这个组的。Machine learning theoryhttp://hunch.net/这个网站主要是一些learning theory的东西比较多，想在machine learning 理论上有所建树的同志们可以去看看牛人：Iasonas Kokkinos （搞统计模型视觉）http://vision.mas.ecp.fr/Personnel/iasonas/index.html"}
{"content2":"冷静审视人工智能技术的本质吴妙芸来源：图灵人工智能（ID：TuringAI01）人工智能的发展离不开基础支持层和技术层，基础支持层包括大数据、计算力和算法；技术层包括计算机视觉、语音识别和自然语言处理。在过去的2016年人工智能风风火火了一把，到目前为止，还在大力向前发展，研究机构、企业、投资机构和政府都对人工智能投入了很多关注，并陆续出台了一些政策。人工智能的技术本质是什么，本文会详细分析。总览人工智能技术图谱基础支撑层的算法创新发生在20世纪80年代末，是大数据和计算力将人工智能推到镁光灯之下，而建立在这之上的基础技术便是计算机视觉、语音识别和自然语言理解，机器试图看懂、听懂人类的世界、用人类的语言和人类交流，研究人类智能活动的规律。1.计算机视觉技术（Computer Vision）1)·什么是计算机视觉“人的大脑皮层的活动， 大约70%是在处理视觉相关信息。视觉就相当于人脑的大门，其它如听觉、触觉、味觉那都是带宽较窄的通道。视觉相当于八车道的高速， 其它感觉是两旁的人行道。如果不能处理视觉信息的话，整个人工智能系统是个空架子，只能做符号推理，比如下棋、定理证明， 没法进入现实世界。计算机视觉之于人工智能，它相当于说芝麻开门。大门就在这里面，这个门打不开, 就没法研究真实世界的人工智能。”——朱松纯，加州大学洛杉矶分校UCLA统计学和计算机科学教授根据科普中国撰写的对计算机视觉的定义，这是一门研究如何让机器“看”的科学，更进一步的说，是指用计算机代替人眼对目标进行识别、跟踪和测量的机器视觉，并进一步做图形处理，使计算机处理成为更适合人眼观察或传送给仪器检测的图像。2)·计算机视觉 VS 机器视觉计算机视觉更关注图像信号本身以及图像相关交叉领域（地图、医疗影像）的研究；机器视觉则偏重计算机视觉技术工程化，更关注广义上的图像信号（激光和摄像头）和自动化控制（生产线）方面的应用。3)计算机视觉识别技术的分类物体识别分为“1 VS N”对不同物体进行归类，以及“1 VS 1”对同类型的物体进行区分和鉴别；物体属性识别，结合地图模型让物体在视觉的三维空间里得到记忆的重建，进而进行场景的分析和判断；物体行为识别分为3个进阶的步骤，移动识别判断物体是否做了位移，动作识别判断物体做的是什么动作，行为识别是结合视觉主体和场景的交互做出行为的分析和判断。4)·计算机视觉的识别流程训练模型：样本数据包括正样本（包含待检目标的样本）和负样本（不包含目标的样本），视觉系统利用算法对原始样本进行特征的选择和提取训练出分类器（模型）；此外因为样本数据成千上万、提取出来的特征更是翻番，所以一般为了缩短训练的过程，会人为加入知识库（提前告诉计算机一些规则），或者引入限制条件来缩小搜索空间。识别图像：会先对图像进行信号变换、降噪等预处理，再来利用分类器对输入图像进行目标检测。一般检测过程为用一个扫描子窗口在待检测的图像中不断的移位滑动，子窗口每到一个位置就会计算出该区域的特征，然后用训练好的分类器对该特征进行筛选，判断该区域是否为目标。5)计算机视觉技术模式图和对应企业图目前世界上图像识别最大的数据库，是斯坦福大学人工智能实验室提供的ImageNet，针对诸如医疗等细分领域也需要收集相应的训练数据；Google、Microsoft此类科技巨头会面向市场提供开源算法框架，为初创视觉识别公司提供初级算法。2.语音识别（Automatic Speech Recognition）1)什么是语音识别语音识别是以语音为研究对象，通过信号处理和识别技术让机器自动识别和理解人类口述的语言后，将语音信号转换为相应的文本或命令的一门技术。由语音识别和语音合成、自然语言理解、语义网络等技术相结合的语音交互正在逐步成为当前多通道、多媒体智能人机交互的主要方式。2)语音识别的流程语音信号经过前端信号处理、端点检测等预处理后，逐帧提取语音特征，传统的特征类型包括有MFCC、PLP、FBANK等特征，提取好的特征会送到解码器，在训练好的声学模型、语言模型之下，找到最为匹配的此序列作为识别结果输出。3）语音识别技术模式图和对应企业图基础层：包含大数据、计算力和算法三块，其中大数据等接入的是相应领域的第三方服务商。机器在识别人类的语音指令后接入、提供相应的服务。诸如影视、电影票、餐饮等；技术层：以科大讯飞为首的语音技术提供商；应用层：传统家居环境中的电视、音箱厂商都给加上了语音识别功能，新增交互方式；还有智能车载采用语音交互让手不离开方向盘提高安全系数；还有搜索厂商基于搜索做出来的语音助手等。3.自然语言理解（Natural Language Understanding）1）什么是自然语言理解自然语言理解即文本理解，和语音图像的模式识别技术有着本质的区别，语言作为知识的载体，承载了复杂的信息量，具有高度的抽象性，对语言的理解属于认知层面，不能仅靠模式匹配的方式完成。2）自然语言理解的应用：搜索引擎+机器翻译；自然语言理解最典型两种应用为搜索引擎和机器翻译。搜索引擎可以在一定程度上理解人类的自然语言，从自然语言中抽取出关键内容并用于检索，最终达到搜索引擎和自然语言用户之间的良好衔接，可以在两者之间建立起更高效，更深层的信息传递。3）自然语言理解技术在搜索引擎中的应用4）自然语言理解技术在机器翻译中的应用事实上搜索引擎和机器翻译不分家，互联网、移动互联网为其充实了语料库使得其发展模态发生了质的改变。互联网、移动互联网除了将原先线下的信息（原有语料）进行在线化之外，还衍生出来的新型UGC模式：知识分享数据，像维基百科、百度百科等都是人为校准过的词条，噪声小；社交数据，像微博和微信等展现用户的个性化、主观化、时效性，可以用来做个性化推荐、情感倾向分析、以及热点舆情的检测和跟踪等；社区、论坛数据，像果壳、知乎等为搜索引擎提供了问答知识、问答资源等数据源。另一方面，因为深度学习采用的层次结构从大规模数据中自发学习的黑盒子模式是不可解释的，而以语言为媒介的人与人之间的沟通应该要建立在相互理解的基础上，所以深度学习在搜索引擎和机器翻译上的效用没有语音图像识别领域来得显著。一图看懂新一代人工智能知识体系大全来自：财经头条（来源：华尔街见闻。如侵删）"}
{"content2":"前段时间的“人机大战”——谷歌的Alpha Go战胜人类棋手的新闻甚嚣尘上，不禁有人会想起1997年IBM自主研发的深蓝战胜卡斯帕罗夫的事件。“人工智能”这个词再次被推上风口浪尖，而“认知计算”却鲜有人听说，同样是人类模拟机器思索，让机器具有自主思考能力，都是具有跨时代意义和里程碑式的存在。认知计算更加强调机器或人造大脑如何能够主动学习、推理、感知这个世界，并与人类、环境进行交互的反应。它会根据环境的变化做出动态的反应，所以认知更加强调它的动态性、自适应性、鲁棒性、交互性。计算机在体系架构上的发展历史主要体现在两个方面：计算能力的增强计算规模的增大随着计算机计算能力的大幅增强，具备了处理海量数据的能力；另一方面，日常生活中所产生的数据规模日益扩大，所拥有的数据源驱动了深层次分析的需求；同时大数据、云计算技术的不断完善，都促进了对数据进行深度挖掘，提取数据的特征，利用特征让机器具有自主学习与思考的能力。按照计算方式的不同，可以分为三个计算时代：1990s~1940s  打卡阶段（The Tabulating Era）       机械式1950s~现在     编程阶段（The Programming Era）   自主输入2011~将来      认知计算阶段（The Cognitive Era）    自动思考“大脑”项目：Think & Learn2006     IBM        Watson      利用自然语言分析，让机器自动推理事件与回答问题；涵盖医疗、数据分析、“危险游戏”等。2011     Google    谷歌大脑     通过神经网络，能够让更多的用户拥有完美的、没有错误的使用体验；谷歌无人驾驶汽车、谷歌眼镜等。2012     Baidu      百度大脑     融合深度学习算法、数据建模、大规模GPU并行化平台等技术，构造起深度神经网络。一、认知计算的概念：人工智能与认知计算的区别：人工，以人为主导；认知，机器对事物与外界的理解，交互的能力编程能力；学习与推理的能力确定性结果；概率性结果人并未参与；人、机器、环境之间的交互图灵测试或仿造人测量；实际应用中的测试2.  认知计算所涉及的技术领域：神经科学：机器模拟人脑神经元的思考过程；超计算：超级快速计算和处理能力；纳米技术：芯片、系统等底层架构设计。3. 认知计算系统的组成：需要一个能够理解、学习、推理的“大脑”，一个物物相连的外部环境，大脑与环境之间互相感知与交互。4.  认知计算的应用：典型系统特征：大规模、复杂、人与外界交互、大量非结构化数据、输出结果不定的系统；生命科学领域：医疗、保险；社会机构领域：金融银行、政府、能源、教育、商业、交通等。5.  案例：Watson-历史上第一个认知系统自然语言处理问答技术高性能计算知识的表达和推理机器学习非结构化信息管理6.  认知系统的五个核心功能：创造更深的人工参与测量和提升专业知识认知融入产品和服务实现认知过程和操作加强探索和发现7.  认知计算系统的挑战与要求：8.  认知计算系统的架构：底层架构：芯片设计（GPU、FPGA、ASIC、POWER8）基础设施：云环境、超级计算节点组织构架：caffe、Theano、Torch等库文件：数据库、工具、包等应用层：信息采集的有效性、人机交互界面、搜索引擎等二、人工智能的概述：人的大脑科学&计算机科学——>可视化、心理学、神经元组成、深度学习1. 人工智能发展过程：重要的时间节点与人物：1950-1956：两个重要的人物，诺伯特·维纳（控制论）和克劳德·艾尔伍德·香农（信息论）将事物从更高的层次进行抽象，奠定了AI坚实的理论基础；1950：图灵，提出了图灵测试的基本测试方法；1956：达特茅斯会议第一次正式提出AI的概念；1956-1974：AI得到极大发展，提出了许多新的理论，包括自然语言处理、reasoning as search、micro-worlds等；1974-1980：由于发展迅速所带来的副作用日益凸显，关于机器代替人类的社会、伦理等问题、投资人看不到长期受益问题等导致其发展陷入低谷；1980-1987：在日本的第五代项目提出，结合AI来发展现代工业生产，又给AI界打了一针强心剂（专家系统）；1987-1993：计算机的高速发展，给传统硬件组成的研究系统带来巨大挑战，更多的人将注意力放在计算能力更强、价格更为便宜的普通计算机上；1995：Sparse coding，将计算机科学理论与生物神经科学理论相结合；2006：Deep Learning，含多隐层的多层感知器的深度学习结构；2007：GPU CUDA，CPU与GPU并用的“协同处理”发展的统一计算设备架构；2011：Google Brain，谷歌在人工智能领域开发出的一款模拟人脑的软件。2. 机器学习的概述：机器学习两种传统分类：监督学习：已知label来对事物进行分类；无监督学习：未知label来学习事物特征。应用领域：图像识别、计算机视觉、语音识别、生物监控、机器人控制、经验科学、智能医疗等。机器学习的流程图（有监督学习）：分类算法（Classification）：支持向量机（SVM）神经网络（Neural Network）朴素贝叶斯（Naiive Bayes）贝叶斯网络（Bayesian network）逻辑回归（Logistic regression）随机森林（Randomized Forests）决策树（Boosted Decision Trees）k近邻（K-nearest neighbor）RBMs聚类算法（Clustering）：K-means合并聚类（agglomerative clustering）均值漂移聚类（mean shift clustering）谱聚类（spectral clustering）泛化问题（Generalization）：过拟合、欠拟合3. 深度学习的概述：深度学习是机器学习的一个分支，通过利用多层处理的复杂结构，基于一系列的算法来建立高维抽象的模型。其动机在于建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，例如图像，声音和文本。深度学习的概念由Hinton等人于2006年提出。基于深度置信网络(DBN)提出非监督贪心逐层训练算法，为解决深层结构相关的优化难题带来希望，随后提出多层自动编码器深层结构。此外Lecun等人提出的卷积神经网络（CNN）是第一个真正多层结构学习算法，它利用空间相对关系减少参数数目以提高训练性能。典型的深度学习：卷积神经网络CNN实验已经证明，CNN在图像和语音处理中能够取得比传统方法更好的识别效果，也产生了许多著名的深度学习网络VGG-Net、AlexNet等。VGG-Net与AlexNet的对比分析：深度学习网络AlexNetVGG-Net产生背景2012年，deep learning的大牛教授 Geoffrey Hinton的学生Alex Krizhevsky设计了一个8层的CNN，并把它用于ImageNet的image classification，直接把当时最好算法的错误率差不多减半。Andrew Zisserman 教授的组 (Oxford)，VGG-Net 在2014年的 ILSVRC localization and classification 两个问题上分别取得了第一名和第二名。结构层次总共有8层，由5层 convolutional layer，2层 fully connected layer，和最后一层 label layer (1000个node, 每个node代表ImageNet中的一个类别) 组成。VGG-Net使用更多的层，通常有16－19层，所有 convolutional layer 使用同样大小的 convolutional filter。结构示意特征描述中间层描述了图片的局部特征，全连接层表示了图像的全局特征。业界牛人：开发架构：机器学习的常用库和数据集："}
{"content2":"Deep Learning（深度学习）学习笔记整理系列声明：1）该Deep Learning的学习系列是整理自网上很大牛和机器学习专家所无私奉献的资料的。具体引用的资料请看参考文献。具体的版本声明也参考原文献。2）本文仅供学术交流，非商用。所以每一部分具体的参考资料并没有详细对应。如果某部分不小心侵犯了大家的利益，还望海涵，并联系博主删除。3）本人才疏学浅，整理总结的时候难免出错，还望各位前辈不吝指正，谢谢。4）阅读本文需要机器学习、计算机视觉、神经网络等等基础（如果没有也没关系了，没有就看看，能不能看懂，呵呵）。5）此属于第一版本，若有错误，还需继续修正与增删。还望大家多多指点。大家都共享一点点，一起为祖国科研的推进添砖加瓦（呵呵，好高尚的目标啊）。目录：一、概述二、背景三、人脑视觉机理四、关于特征4.1、特征表示的粒度4.2、初级（浅层）特征表示4.3、结构性特征表示4.4、需要有多少个特征？五、Deep Learning的基本思想六、浅层学习（Shallow Learning）和深度学习（Deep Learning）七、Deep learning与Neural Network八、Deep learning训练过程8.1、传统神经网络的训练方法8.2、deep learning训练过程九、Deep Learning的常用模型或者方法9.1、AutoEncoder自动编码器9.2、Sparse Coding稀疏编码9.3、Restricted Boltzmann Machine(RBM)限制波尔兹曼机9.4、Deep BeliefNetworks深信度网络9.5、Convolutional Neural Networks卷积神经网络十、总结与展望十一、参考文献和Deep Learning学习资源一、概述Artificial Intelligence，也就是人工智能，就像长生不老和星际漫游一样，是人类最美好的梦想之一。虽然计算机技术已经取得了长足的进步，但是到目前为止，还没有一台电脑能产生“自我”的意识。是的，在人类和大量现成数据的帮助下，电脑可以表现的十分强大，但是离开了这两者，它甚至都不能分辨一个喵星人和一个汪星人。图灵（图灵，大家都知道吧。计算机和人工智能的鼻祖，分别对应于其著名的“图灵机”和“图灵测试”）在 1950 年的论文里，提出图灵试验的设想，即，隔墙对话，你将不知道与你谈话的，是人还是电脑。这无疑给计算机，尤其是人工智能，预设了一个很高的期望值。但是半个世纪过去了，人工智能的进展，远远没有达到图灵试验的标准。这不仅让多年翘首以待的人们，心灰意冷，认为人工智能是忽悠，相关领域是“伪科学”。但是自 2006 年以来，机器学习领域，取得了突破性的进展。图灵试验，至少不是那么可望而不可及了。至于技术手段，不仅仅依赖于云计算对大数据的并行处理能力，而且依赖于算法。这个算法就是，Deep Learning。借助于 Deep Learning 算法，人类终于找到了如何处理“抽象概念”这个亘古难题的方法。2012年6月，《纽约时报》披露了Google Brain项目，吸引了公众的广泛关注。这个项目是由著名的斯坦福大学的机器学习教授Andrew Ng和在大规模计算机系统方面的世界顶尖专家JeffDean共同主导，用16000个CPU Core的并行计算平台训练一种称为“深度神经网络”（DNN，Deep Neural Networks）的机器学习模型（内部共有10亿个节点。这一网络自然是不能跟人类的神经网络相提并论的。要知道，人脑中可是有150多亿个神经元，互相连接的节点也就是突触数更是如银河沙数。曾经有人估算过，如果将一个人的大脑中所有神经细胞的轴突和树突依次连接起来，并拉成一根直线，可从地球连到月亮，再从月亮返回地球），在语音识别和图像识别等领域获得了巨大的成功。项目负责人之一Andrew称：“我们没有像通常做的那样自己框定边界，而是直接把海量数据投放到算法中，让数据自己说话，系统会自动从数据中学习。”另外一名负责人Jeff则说：“我们在训练的时候从来不会告诉机器说：‘这是一只猫。’系统其实是自己发明或者领悟了“猫”的概念。”2012年11月，微软在中国天津的一次活动上公开演示了一个全自动的同声传译系统，讲演者用英文演讲，后台的计算机一气呵成自动完成语音识别、英中机器翻译和中文语音合成，效果非常流畅。据报道，后面支撑的关键技术也是DNN，或者深度学习（DL，DeepLearning）。2013年1月，在百度年会上，创始人兼CEO李彦宏高调宣布要成立百度研究院，其中第一个成立的就是“深度学习研究所”（IDL，Institue of Deep Learning）。为什么拥有大数据的互联网公司争相投入大量资源研发深度学习技术。听起来感觉deeplearning很牛那样。那什么是deep learning？为什么有deep learning？它是怎么来的？又能干什么呢？目前存在哪些困难呢？这些问题的简答都需要慢慢来。咱们先来了解下机器学习（人工智能的核心）的背景。二、背景机器学习（Machine Learning）是一门专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能的学科。机器能否像人类一样能具有学习能力呢？1959年美国的塞缪尔(Samuel)设计了一个下棋程序，这个程序具有学习能力，它可以在不断的对弈中改善自己的棋艺。4年后，这个程序战胜了设计者本人。又过了3年，这个程序战胜了美国一个保持8年之久的常胜不败的冠军。这个程序向人们展示了机器学习的能力，提出了许多令人深思的社会问题与哲学问题（呵呵，人工智能正常的轨道没有很大的发展，这些什么哲学伦理啊倒发展的挺快。什么未来机器越来越像人，人越来越像机器啊。什么机器会反人类啊，ATM是开第一枪的啊等等。人类的思维无穷啊）。机器学习虽然发展了几十年，但还是存在很多没有良好解决的问题：例如图像识别、语音识别、自然语言理解、天气预测、基因表达、内容推荐等等。目前我们通过机器学习去解决这些问题的思路都是这样的（以视觉感知为例子）：从开始的通过传感器（例如CMOS）来获得数据。然后经过预处理、特征提取、特征选择，再到推理、预测或者识别。最后一个部分，也就是机器学习的部分，绝大部分的工作是在这方面做的，也存在很多的paper和研究。而中间的三部分，概括起来就是特征表达。良好的特征表达，对最终算法的准确性起了非常关键的作用，而且系统主要的计算和测试工作都耗在这一大部分。但，这块实际中一般都是人工完成的。靠人工提取特征。截止现在，也出现了不少NB的特征（好的特征应具有不变性（大小、尺度和旋转等）和可区分性）：例如Sift的出现，是局部图像特征描述子研究领域一项里程碑式的工作。由于SIFT对尺度、旋转以及一定视角和光照变化等图像变化都具有不变性，并且SIFT具有很强的可区分性，的确让很多问题的解决变为可能。但它也不是万能的。然而，手工地选取特征是一件非常费力、启发式（需要专业知识）的方法，能不能选取好很大程度上靠经验和运气，而且它的调节需要大量的时间。既然手工选取特征不太好，那么能不能自动地学习一些特征呢？答案是能！Deep Learning就是用来干这个事情的，看它的一个别名UnsupervisedFeature Learning，就可以顾名思义了，Unsupervised的意思就是不要人参与特征的选取过程。那它是怎么学习的呢？怎么知道哪些特征好哪些不好呢？我们说机器学习是一门专门研究计算机怎样模拟或实现人类的学习行为的学科。好，那我们人的视觉系统是怎么工作的呢？为什么在茫茫人海，芸芸众生，滚滚红尘中我们都可以找到另一个她（因为，你存在我深深的脑海里，我的梦里 我的心里 我的歌声里……）。人脑那么NB，我们能不能参考人脑，模拟人脑呢？（好像和人脑扯上点关系的特征啊，算法啊，都不错，但不知道是不是人为强加的，为了使自己的作品变得神圣和高雅。）近几十年以来，认知神经科学、生物学等等学科的发展，让我们对自己这个神秘的而又神奇的大脑不再那么的陌生。也给人工智能的发展推波助澜。三、人脑视觉机理1981 年的诺贝尔医学奖，颁发给了 David Hubel（出生于加拿大的美国神经生物学家） 和TorstenWiesel，以及 Roger Sperry。前两位的主要贡献，是“发现了视觉系统的信息处理”：可视皮层是分级的：我们看看他们做了什么。1958 年，DavidHubel 和Torsten Wiesel 在 JohnHopkins University，研究瞳孔区域与大脑皮层神经元的对应关系。他们在猫的后脑头骨上，开了一个3 毫米的小洞，向洞里插入电极，测量神经元的活跃程度。然后，他们在小猫的眼前，展现各种形状、各种亮度的物体。并且，在展现每一件物体时，还改变物体放置的位置和角度。他们期望通过这个办法，让小猫瞳孔感受不同类型、不同强弱的刺激。之所以做这个试验，目的是去证明一个猜测。位于后脑皮层的不同视觉神经元，与瞳孔所受刺激之间，存在某种对应关系。一旦瞳孔受到某一种刺激，后脑皮层的某一部分神经元就会活跃。经历了很多天反复的枯燥的试验，同时牺牲了若干只可怜的小猫，David Hubel 和Torsten Wiesel 发现了一种被称为“方向选择性细胞（Orientation Selective Cell）”的神经元细胞。当瞳孔发现了眼前的物体的边缘，而且这个边缘指向某个方向时，这种神经元细胞就会活跃。这个发现激发了人们对于神经系统的进一步思考。神经-中枢-大脑的工作过程，或许是一个不断迭代、不断抽象的过程。这里的关键词有两个，一个是抽象，一个是迭代。从原始信号，做低级抽象，逐渐向高级抽象迭代。人类的逻辑思维，经常使用高度抽象的概念。例如，从原始信号摄入开始（瞳孔摄入像素 Pixels），接着做初步处理（大脑皮层某些细胞发现边缘和方向），然后抽象（大脑判定，眼前的物体的形状，是圆形的），然后进一步抽象（大脑进一步判定该物体是只气球）。这个生理学的发现，促成了计算机人工智能，在四十年后的突破性发展。总的来说，人的视觉系统的信息处理是分级的。从低级的V1区提取边缘特征，再到V2区的形状或者目标的部分等，再到更高层，整个目标、目标的行为等。也就是说高层的特征是低层特征的组合，从低层到高层的特征表示越来越抽象，越来越能表现语义或者意图。而抽象层面越高，存在的可能猜测就越少，就越利于分类。例如，单词集合和句子的对应是多对一的，句子和语义的对应又是多对一的，语义和意图的对应还是多对一的，这是个层级体系。敏感的人注意到关键词了：分层。而Deep learning的deep是不是就表示我存在多少层，也就是多深呢？没错。那Deep learning是如何借鉴这个过程的呢？毕竟是归于计算机来处理，面对的一个问题就是怎么对这个过程建模？因为我们要学习的是特征的表达，那么关于特征，或者说关于这个层级特征，我们需要了解地更深入点。所以在说Deep Learning之前，我们有必要再啰嗦下特征（呵呵，实际上是看到那么好的对特征的解释，不放在这里有点可惜，所以就塞到这了）。"}
{"content2":"计算机视觉计算机视觉（computer vision）是从图像和视频中提出数值或符号信息的计算系统，更形象的来说就是，计算机视觉是让计算机具备像人类一样的眼睛，可以看到图像或视频，并理解图像或视频。数据：图像，视频算法：机器学习，深度学习神经网络（分类+回归）三大任务之识别1、车牌识别（通过算法获取车辆的相关信息）2、人脸识别（识别出人脸，并且获取所识别人的相关属性，如年龄，性别的等），扩展的还有情绪识别。三大任务之目标检测目标检测（Object Detection）的任务是找出图像中所有感兴趣的目标（物体），确定它们的类别和位置，是计算机视觉领域的核心问题之一。由于各类物体有不同的外观，形状，姿态，加上成像时光照，遮挡等因素的干扰，目标检测一直是计算机视觉领域最具有挑战性的问题。计算机视觉中关于图像识别有四大类任务：分类-Classification：解决“是什么？”的问题，即给定一张图片或一段视频判断里面包含什么类别的目标。定位-Location：解决“在哪里？”的问题，即定位出这个目标的的位置。检测-Detection：解决“是什么？在哪里？”的问题，即定位出这个目标的的位置并且知道目标物是什么。分割-Segmentation：分为实例的分割（Instance-level）和场景分割（Scene-level），解决“每一个像素属于哪个目标物或场景”的问题。object detection= classification +localization，可以应用于行人检测/车辆检测等。三大任务之分割把图像空间按照一定的要求分成一些“有意义”的区域的技术叫做图像分割。语义分割：语义指的是图像的内容，对图片意思的理解；分割的意思是从像素的角度分割出图片中的不同对象，对原图中的每个像素都进行标注，语义分割不区分属于相同类别的不同实例。实例分割：机器自动从图像中用目标检测方法框出不同实例，再用语义分割方法在不同实例区域内进行逐像素标记，每一个实例都进行标记，不论是否属于统一类别。其他方面视觉目标跟踪、视频分割、图像风格迁移，对抗生成网络、图题生成等。网络架构图像识别：AlexNet、VGGNet、GoogleNet、ResNet、RetinaNet.......目标检测：Fast-RCNN、Faster-RCNN、Yolo,RetinaNet图像分割：FCN、Mask-RCNN目标跟踪：GOTURN、eco图像生成：GAN、WGAN光流：FlowNet视频分割：SegNet图像特征颜色、形状（全局形状、局部形状）、纹理、SIFT特征（局部特征）、HoG特征（检测）、LBP特征(人脸)、Harr特征等"}
{"content2":"第一章 绪 论教学内容：本章首先介绍人工智能的定义、发展概况及相关学派和他们的认知观，接着讨论人工智能的研究和应用领域，最后简介本书的主要内容和编排。教学重点：1．从不同科学或学科出发对人工智能进行的定义；2．介绍人工智能的起源与发展过程；3．讨论人工智能与人类智能的关系；4．简介目前人工智能的主要学派；5．简介人工智能所研究的范围与应用领域。教学难点：1．怎么样理解人工智能；2．人工智能作为一门学科有什么意义；3．人工智能的主要学派与其争论焦点；教学方法：课堂教学为主，充分利用网络课程中的多媒体素材来表示抽象概念。教学要求：重点掌握人工智能的几种定义，掌握目前人工智能的三个主要学派及对人工智能的理解，一般了解人工智能的主要研究范围和应用领域。1.1人工智能的定义与发展教学内容：本小节主要介绍目前对人工智能的几种定义，并对人工智能的起源和发展进行了总结和分析。教学重点：几种人工智能的定义和人工智能发展的几个重要时期。教学难点：理解人工智能的定义与本质。教学方法：课堂讲授为主。教学要求：从学科和能力的角度深刻理解人工智能的定义，初步了解人工智能的起源及其发展过程。1.1.1人工智能的定义定义1 智能机器能够在各类环境中自主地或交互地执行各种拟人任务(anthropomorphic tasks)的机器。定义2人工智能(学科)人工智能(学科)是计算机科学中涉及研究、设计和应用智能机器的一个分支。它的近期主要目标在于研究用机器来模仿和执行人脑的某些智力功能，并开发相关理论和技术。定义3人工智能(能力)人工智能(能力)是智能机器所执行的通常与人类智能有关的智能行为，如判断、推理、证明、识别、感知、理解、通信、设计、思考、规划、学习和问题求解等思维活动。为了让读者对人工智能的定义进行讨论，以便更深刻地理解人工智能，下面综述其它几种关于人工智能的定义。定义4人工智能是一种使计算机能够思维，使机器具有智力的激动人心的新尝试（Haugeland,1985）。定义5人工智能是那些与人的思维、决策、问题求解和学习等有关活动的自动化（Bellman,1978）。定义6人工智能是用计算模型研究智力行为（Charniak和McDermott,1985）。定义7人工智能是研究那些使理解、推理和行为成为可能的计算（Winston,1992）。定义8人工智能是一种能够执行需要人的智能的创造性机器的技术（Kurzwell,1990）。定义9人工智能研究如何使计算机做事让人过得更好（Rick和Knight,1991）。定义10人工智能是一门通过计算过程力图理解和模仿智能行为的学科（Schalkoff,1990）。定义11人工智能是计算机科学中与智能行为的自动化有关的一个分支（Luger和Stubblefield,1993）。其中，定义4和定义5涉及拟人思维；定义6和定义7与理性思维有关；定义8和定义9涉及拟人行为；定义10和定义11与拟人理性行为有关。1.1.2人工智能的起源与发展人工智能的发展是以硬件与软件为基础的，经历了漫长的发展历程。特别是20世纪30年代和40年代的智能界，发现了两件重要的事情：数理逻辑和关于计算的新思想。以维纳（Wiener）、弗雷治、罗素等为代表对发展数理逻辑学科的贡献及丘奇(Church)、图灵和其它一些人关于计算本质的思想，为人工智能的形成产生了重要影响。1956年夏季，人类历史上第一次人工智能研讨会在美国的达特茅斯(Dartmouth)大学举行，标志着人工智能学科的诞生。1969年召开了第一届国际人工智能联合会议(International Joint Conference on AI, IJCAI),此后每两年召开一次。1970年《人工智能》国际杂志(International Journal of AI)创刊。这些对开展人工智能国际学术活动和交流、促进人工智能的研究和发展起到积极作用。20世纪70～80年代，知识工程的提出与专家系统的成功应用，确定了知识在人工智能中的地位。近十多年来，机器学习、计算智能、人工神经网络等和行为主义的研究深入开展，形成高潮。同时，不同人工智能学派间的争论也非常热烈。这些都推动人工智能研究的进一步发展。提问：为什么人工智能在1956年才正式诞生？1.2人类智能与人工智能教学内容：本节主要讨论人类智能与人工智能的关系问题。教学重点：智能信息处理系统，人类智能与人工智能的关系。教学难点：智能信息处理系统的假设。教学方法：课堂讲授为主。教学要求：了解人类认知活动与计算机的比较关系，基本了解智能信息处理系统。1.2.1智能处理信息系统的假设1、符号处理系统的六种基本功能信息处理系统又叫符号操作系统(Symbol Operation System)或物理符号系统(Physical Symbol System)。所谓符号就是模式(pattern)。一个完善的符号系统应具有下列6种基本功能：(1)输入符号(input)；(2)输出符号(output);(3)存储符号(store);(4)复制符号(copy);(5)建立符号结构：通过找出各符号间的关系，在符号系统中形成符号结构；(6)条件性迁移(conditional transfer):根据已有符号，继续完成活动过程。2、可以把人看成一个智能信息处理系统如果一个物理符号系统具有上述全部6种功能，能够完成这个全过程，那么它就是一个完整的物理符号系统。人具有上述6种功能；现代计算机也具备物理符号系统的这6种功能。3、理符号系统的假设任何一个系统，如果它能表现出智能，那么它就必定能够执行上述6种功能。反之，任何系统如果具有这6种功能，那么它就能够表现出智能；这种智能指的是人类所具有的那种智能。把这个假设称为物理符号系统的假设。4、物理符号系统3个推论推论一 既然人具有智能，那么他(她)就一定是个物理符号系统。人之所以能够表现出智能，就是基于他的信息处理过程。推论二 既然计算机是一个物理符号系统，它就一定能够表现出智能。这是人工智能的基本条件。推论三 既然人是一个物理符号系统，计算机也是一个物理符号系统，那么就能够用计算机来模拟人的活动。4、人类的认知行为具有不同的层次认知生理学 研究认知行为的生理过程，主要研究人的神经系统（神经元、中枢神经系统和大脑）的活动，是认知科学研究的底层。认知心理学 研究认知行为的心理活动，主要研究人的思维策略，是认知科学研究的顶层。认知信息学 研究人的认知行为在人体内的初级信息处理，主要研究人的认知行为如何通过初级信息自然处理，由生理活动变为心理活动及其逆过程，即由心理活动变为生理行为。这是认知活动的中间层，承上启下。认知工程学研究认知行为的信息加工处理，主要研究如何通过以计算机为中心的人工信息处理系统，对人的各种认知行为（如知觉、思维、记忆、语言、学习、理解、推理、识别等）进行信息处理。这是研究认知科学和认知行为的工具，应成为现代认知心理学和现代认知生理学的重要研究手段。提问：为什么能够把人看做一个物理符号系统?1.2.2人类智能的计算机模拟1、机器智能可以模拟人类智能物理符号系统假设的推论一告诉人们，人有智能，所以他是一个物理符号系统；推论三指出，可以编写出计算机程序去模拟人类的思维活动。这就是说，人和计算机这两个物理符号系统所使用的物理符号是相同的，因而计算机可以模拟人类的智能活动过程。2、智能计算机的功能如下棋、证明定理、翻译语言文字和解决难题等。神经计算机(neural computer)能够以类似人类的方式进行\"思考\"，它力图重建人脑的形象。一些国家对量子计算机的研究也已起步，希望通过对量子计算(quantum computing)的研究，产生量子计算机。讨论：为什么能够用电脑模拟人脑智能？1.3人工智能的学派教学内容：本节主要介绍人工智能的几个主要学派及认知观。教学重点：符号主义(Symbolicism)，联结主义(Connectionism)，行为主义(Actionism)。教学难点：各学派的对人工智能的不同观点。教学方法：课堂讲授为主。教学要求：了解各派别之间的关系及对人工智能发展历史的看法。1、人工智能三大学派·符号主义(Symbolicism)，又称为逻辑主义(Logicism)、心理学派(Psychlogism)或计算机学派(Computerism)，其原理主要为物理符号系统(即符号操作系统)假设和有限合理性原理。·联结主义(Connectionism)，又称为仿生学派(Bionicsism)或生理学派(Physiologism)，其原理主要为神经网络及神经网络间的连接机制与学习算法。·行为主义(Actionism)，又称进化主义(Evolutionism)或控制论学派(Cyberneticsism)，其原理为控制论及感知椂餍涂刂葡低场?/P>2、三大学派对人工智能发展历史的不同看法符号主义 认为人工智能源于数理逻辑。符号主义仍然是人工智能的主流派。这个学派的代表有纽厄尔、肖、西蒙和尼尔逊(Nilsson)等。联结主义认为人工智能源于仿生学，特别是人脑模型的研究。行为主义 认为人工智能源于控制论。这一学派的代表作首推布鲁克斯(Brooks)的六足行走机器人，它被看做新一代的\"控制论动物\"，是一个基于感知－动作模式的模拟昆虫行为的控制系统。1.4 人工智能的研究与应用领域教学内容：本节主要讨论人工智能的研究与应用领域。教学重点：人工智能的一些主要研究与应用领域。教学难点：处理好各领域间的交叉关系。教学方法：课堂讲授为主。教学要求：初步了解人工智能的研究与应用领域。1.4.1问题求解人工智能的第一个大成就是发展了能够求解难题的下棋(如国际象棋)程序，它包含问题的表示、分解、搜索与归约等。1.4.2逻辑推理与定理证明逻辑推理是人工智能研究中最持久的子领域之一，特别重要的是要找到一些方法，只把注意力集中在一个大型数据库中的有关事实上，留意可信的证明，并在出现新信息时适时修正这些证明。定理证明的研究在人工智能方法的发展中曾经产生过重要的影响。例如，采用谓词逻辑语言的演绎过程的形式化有助于更清楚地理解推理的某些子命题。许多非形式的工作，包括医疗诊断和信息检索都可以和定理证明问题一样加以形式化。因此，在人工智能方法的研究中定理证明是一个极其重要的论题。我国人工智能大师吴文俊院士提出并实现了几何定理机器证明的方法，被国际上承认为\"吴氏方法\"，是定理证明的又一标志性成果。1.4.3自然语言理解语言处理也是人工智能的早期研究领域之一，并引起了进一步的重视。语言的生成和理解是一个极为复杂的编码和解码问题。一个能理解自然语言信息的计算机系统看起来就像一个人一样需要有上下文知识以及根据这些上下文知识和信息用信息发生器进行推理的过程。理解口头的和书写语言的计算机系统所取得的某些进展，其基础就是有关表示上下文知识结构的某些人工智能思想以及根据这些知识进行推理的某些技术。1.4.4自动程序设计对自动程序设计的研究不仅可以促进半自动软件开发系统的发展，而且也使通过修正自身数码进行学习(即修正它们的性能)的人工智能系统得到发展。程序理论方面的有关研究工作对人工智能的所有研究工作都是很重要的。自动程序设计研究的重大贡献之一是作为问题求解策略的调整概念。已经发现，对程序设计或机器人控制问题，先产生一个不费事的有错误的解，然后再修改它(使它正确工作)，这种做法一般要比坚持要求第一个解就完全没有缺陷的做法有效得多。1.4.5专家系统一般地说，专家系统是一个智能计算机程序系统，其内部具有大量专家水平的某个领域知识与经验，能够利用人类专家的知识和解决问题的方法来解决该领域的问题。发展专家系统的关键是表达和运用专家知识，即来自人类专家的并已被证明对解决有关领域内的典型问题是有用的事实和过程。1.4.6机器学习学习是人类智能的主要标志和获得知识的基本手段；机器学习(自动获取新的事实及新的推理算法)是使计算机具有智能的根本途径；机器学习还有助于发现人类学习的机理和揭示人脑的奥秘。学习是一个有特定目的的知识获取过程，其内部表现为新知识结构的不断建立和修改，而外部表现为性能的改善。1.4.7神经网络神经网络处理直觉和形象思维信息具有比传统处理方式好得多的效果。神经网络已在模式识别、图象处理、组合优化、自动控制、信息处理、机器人学和人工智能的其它领域获得日益广泛的应用。1.4.8机器人学人工智能研究日益受到重视的另一个分支是机器人学，其中包括对操作机器人装置程序的研究。这个领域所研究的问题，从机器人手臂的最佳移动到实现机器人目标的动作序列的规划方法，无所不包。目前已经建立了一些比较复杂的机器人系统。机器人和机器人学的研究促进了许多人工智能思想的发展。智能机器人的研究和应用体现出广泛的学科交叉，涉及众多的课题，机器人已在各领域获得越来越普遍的应用。1.4.9模式识别人工智能所研究的模式识别是指用计算机代替人类或帮助人类感知模式，是对人类感知外界功能的模拟，研究的是计算机模式识别系统，也就是使一个计算机系统具有模拟人类通过感官接受外界信息、识别和理解周围环境的感知能力。1.4.10机器视觉实验表明，人类接受外界信息的80%以上来自视觉，视觉对人类是非常重要的。机器视觉或计算机视觉已从模式识别的一个研究领域发展为一门独立的学科；在视觉方面，已经给计算机系统装上电视输入装置以便能够\"看见\"周围的东西。机器视觉的前沿研究领域包括实时并行处理、主动式定性视觉、动态和时变视觉、三维景物的建模与识别、实时图像压缩传输和复原、多光谱和彩色图像的处理与解释等。1.4.11智能控制人工智能的发展促进自动控制向智能控制发展。智能控制是一类无需(或需要尽可能少的)人的干预就能够独立地驱动智能机器实现其目标的自动控制。智能控制是同时具有以知识表示的非数学广义世界模型和数学公式模型表示的混合控制过程，也往往是含有复杂性、不完全性、模糊性或不确定性以及不存在已知算法的非数学过程，并以知识进行推理，以启发来引导求解过程。1.4.12智能检索随着科学技术的迅速发展，出现了\"知识爆炸\"的情况，研究智能检索系统已成为科技持续快速发展的重要保证。智能信息检索系统的设计者们将面临以下几个问题。首先，建立一个能够理解以自然语言陈述的询问系统本身就存在不少问题。其次，即使能够通过规定某些机器能够理解的形式化询问语句来回避语言理解问题，但仍然存在一个如何根据存储的事实演绎出答案的问题。第三，理解询问和演绎答案所需要的知识都可能超出该学科领域数据库所表示的知识。1.4.13智能调度与指挥确定最佳调度或组合的问题是人们感兴趣的又一类问题，求解这类问题的程序会产生一种组合爆炸的可能性，这时，即使是大型计算机的容量也会被用光。人工智能学家们曾经研究过若干组合问题的求解方法。他们的努力集中在使\"时间-问题大小\"曲线的变化尽可能缓慢地增长，即使是必须按指数方式增长。有关问题域的知识再次成为比较有效的求解方法的关键。为处理组合问题而发展起来的许多方法对其它组合上不甚严重的问题也是有用的。1.4.14分布式人工智能与Agent分布式人工智能(Distributed AI, DAI)是分布式计算与人工智能结合的结果。DAI系统以鲁棒性作为控制系统质量的标准，并具有互操作性，即不同的异构系统在快速变化的环境中具有交换信息和协同工作的能力。分布式人工智能的研究目标是要创建一种能够描述自然系统和社会系统的精确概念模型。多agent系统(Multiagent System, MAS)更能体现人类的社会智能，具有更大的灵活性和适应性，更适合开放和动态的世界环境，因而倍受重视，已成为人工智能以至计算机科学和控制科学与工程的研究热点。1.4.15计算智能与进化计算计算智能(Computing Intelligence)涉及神经计算、模糊计算、进化计算等研究领域。进化计算(Evolutionary Computation)是指一类以达尔文进化论为依据来设计、控制和优化人工系统的技术和方法的总称，它包括遗传算法(Genetic Algorithms)、进化策略(Evolutionary Strategies)和进化规划(Evolutionary Programming)。1.4.16数据挖掘与知识发现知识获取是知识信息处理的关键问题之一。数据挖掘是通过综合运用统计学、粗糙集、模糊数学、机器学习和专家系统等多种学习手段和方法，从大量的数据中提炼出抽象的知识，从而揭示出蕴涵在这些数据背后的客观世界的内在联系和本质规律，实现知识的自动获取。数据挖掘和知识发现技术已获广泛应用。1.4.17人工生命人工生命(Artificial Life, ALife)旨在用计算机和精密机械等人工媒介生成或构造出能够表现自然生命系统行为特征的仿真系统或模型系统。自然生命系统行为具有自组织、自复制、自修复等特征以及形成这些特征的混沌动力学、进化和环境适应。人工生命所研究的人造系统能够演示具有自然生命系统特征的行为，在\"生命之所能\"(life as it could be)的广阔范围内深入研究\"生命之所知\"(life as we know it)的实质。人工生命学科的研究内容包括生命现象的仿生系统、人工建模与仿真、进化动力学、人工生命的计算理论、进化与学习综合系统以及人工生命的应用等。1.4.18系统与语言工具除了直接瞄准实现智能的研究工作外，开发新的方法也往往是人工智能研究的一个重要方面。人工智能对计算机界的某些最大贡献已经以派生的形式表现出来。计算机系统的一些概念，如分时系统、编目处理系统和交互调试系统等，已经在人工智能研究中得到发展。1.5本书概要本书包括下列内容：1、简述人工智能的起源与发展，讨论人工智能的定义、人工智能与计算机的关系以及人工智能的研究和应用领域。2、比较概括地论述知识表示的各种主要方法，包括状态空间法、问题归约法、谓词逻辑法、结构化表示法（语义网络法、框架）、剧本和过程等。3、讨论常用搜索原理，如盲目搜索、启发式搜索和消解原理等；并研究一些比较高级的推理求解技术，如规则演绎系统、专家系统、系统组织技术、不确定性推理和非单调推理等。4、介绍近期发展起来的已成为当前研究热点的人工智能技术和方法，即分布式人工智能与agent、计算智能（含神经计算、逻辑计算与进化计算）、数据挖掘与知识发现、人工生命等。5、比较详细地分析人工智能的主要应用领域，涉及专家系统、机器学习、自动规划系统和自然语言理解等。6、叙述近年来人工智能研究中出现的争论，展望人工智能的发展。1.6 辩论会主题：人工智能能否超过人类智能？正方观点：人工智能不会超过人类智能。反方观点：人工智能能够超过人类智能。"}
{"content2":"从“互联网+”到“AI+”，人工智能技术正在为经济社会发展带来深远影响。2018年，核心技术攻关、人才梯队培养、实体产业落地……我国人工智能技术正加速更新换代。1月15日，AI青年科学家联盟多位专家在上海表示，2019年，中国AI产业的发展将逐步走向成熟。——趋势一：产学研合作，人才培养系统化发力2018年被称为中国人工智能人才系统化培养的元年，教育部印发了《高等学校人工智能创新行动计划》，不少学校开始申请人工智能专业，AI高中教材试水，创新型企业频与高校建立联合实验室……“依托类脑智能技术及应用等国家工程实验室，中国科学技术大学2017年开设了人工智能本科学科，考虑到学科的综合性和交叉性，采用‘X+2’的模式，从大三进行人才遴选，2019年下半年将迎来学校第一批人工智能本科专业毕业生。课程分为模式识别、机器学习等基础理论必修课，以及计算机视觉、智能机器人等前沿选修课。”中国科学技术大学自动化系教授查正军说。2019年，如何进行专业课程体系的设置、打通产学研的链条，成为突破重点。“现在还处在缺一系列课本、精品课程以及如何培养本科人才体系的阶段，这也是现在人工智能人才培养面临的最大任务”。浙江大学计算机学院副院长吴飞说，推进高等学校人工智能交叉学科的建设是一个突破口。上海交通大学教授俞凯建议，人工智能基础研究和人才培养需要注重实用化和国际化两个方向，从实际遇到的问题中抽象出科学问题，反过来对理论研究产生正面影响。——趋势二：产业落地一边挤出泡沫，一边深入痛点“人们往往会高估一项技术前五年的创造性，低估后五年的破坏性”。上海交通大学人工智能研究院副院长王延峰说，AI产业可能会经历一段调整期，挤一挤泡沫。同时对於产业界来说，场景驱动为实体经济降本提效，也会向更多行业继续深入。退潮方知谁在裸泳。专家认为，在挤泡沫的过程中，中国人工智能产业将开拓更多元的产业场景和更深入的产业链条。未来在人工智能领域可能诞生类似英特尔这样的平台级和底层级公司，这是产业界的共识，但谁是赢家仍不得而知。专家表示，从算法端向上下游延伸，芯片和开源开放平台作为人工智能发展的硬件和软件基础，正成为产业链条上不可或缺的部分。商汤科技首席执行官徐立认为，这一波AI浪潮在某种意义上，就是在行业中挖掘更深的痛点，深入应用，将更多技术做到超过工业红线，为行业创造价值。针对人工智能和实体经济的加速融合，上海发布了人工智能十大应用场景的“需求单”，囊括了“AI”+安防、工厂、家庭、交通、金融、社区、学校、医院、园区、政务10大领域。政府“牵线”对接供需，让人工智能企业能与更多客户迅速匹配，既包含垂直具体场景，又兼顾行业通用需求，还探索园区集成方案。上海市徐汇区区长方世忠表示，在上海人工智能高质量发展“22条”的基础上，徐汇将聚焦AI企业最关注的场景开放、技术研发、创新应用、金融服务等方面精准施策，营造最适合人工智能高质量发展和企业加速集聚的制度生态环境，使这里成为最适宜人工智能发展的制度供给试验田。——趋势三：瞄准新一代人工智能技术共闯“无人区”现在，以数据驱动为核心的深度学习已经成为工具式的开发手段，下一代神经网络的方向是什麽?未来十年，人工智能的新引擎是什麽?业内专家认为，下一步AI的重点需要加强基础理论的创新和研究。吴飞认为，从基础理论上看，数据智能日趋成熟、类脑智能蓄势待发，“双智”结合带来的新一代人工智能基础理论突破，是未来要把握的方向。专家表示，年轻的中国人工智能科学家和企业家，正负担起寻找下一个十年行业发展的重任。2018年，AI青年科学家联盟在上海市徐汇区成立，旨在培养中国人工智能的多层次人才梯队。联盟发起人之一、氪信科技创始人朱明杰认为，青年科创家应寻找行业发展的真实痛点，把业务和AI真正结合起来，产生长期价值，促进产业升级。“人工智能正在加速更新换代，需要更多青年人才的推动。”中国工程院院士、AI青年科学家联盟学术指导委员会主席潘云鹤说，中国的人才培养应该把知识和资本很好地结合起来，让年轻AI科学家、企业家可以开辟新领域，发挥人工智能的“头雁”效应，勇闯“无人区”。原文来自：http://ai.51cto.com/art/201901/590861.htm本文地址：https://www.linuxprobe.com/generation-artificial-intelligence.html编辑：王浩，审核员：逄增宝"}
{"content2":"图像的几何变换主要包括：平移、扩大与缩小、旋转、仿射、透视等等。图像变换是建立在矩阵运算基础上的，通过矩阵运算可以很快的找到对应关系。1. 图像的平移图像的平移，沿着x方向tx距离，y方向ty距离，需要构造移动矩阵M。通过numpy来产生这个矩阵，并将其赋值给仿射函数cv2.warpAffine().仿射函数cv2.warpAffine()接受三个参数，需要变换的原始图像，移动矩阵M 以及变换的图像大小（这个大小如果不和原始图像大小相同，那么函数会自动通过插值来调整像素间的关系）。import cv2 import numpy as np import matplotlib.pyplot as plt img = cv2.imread('flower.jpg') H = np.float32([[1,0,100],[0,1,50]]) rows,cols = img.shape[:2] res = cv2.warpAffine(img,H,(rows,cols)) #需要图像、变换矩阵、变换后的大小 plt.subplot(121) plt.imshow(img) plt.subplot(122) plt.imshow(res) plt.show()2. 图像的缩放图像的缩放有专门的一个函数，cv2.resize()，需要确定的是缩放比例。另外一个就是在缩放以后图像必然就会变化，这就又涉及到一个插值问题。那么这个函数中，缩放有几种不同的插值（interpolation）方法，在缩小时推荐cv2.INTER_ARER,扩大是推荐cv2.INTER_CUBIC和cv2.INTER_LINEAR。默认都是cv2.INTER_LINEARimport cv2 import matplotlib.pyplot as plt img = cv2.imread('flower.jpg') # 插值：interpolation # None本应该是放图像大小的位置的，后面设置了缩放比例，所有就不要了 res1 = cv2.resize(img,None,fx=2,fy=2,interpolation=cv2.INTER_CUBIC) #直接规定缩放大小，这个时候就不需要缩放因子 height,width = img.shape[:2] res2 = cv2.resize(img,(2*width,2*height),interpolation=cv2.INTER_CUBIC) plt.subplot(131) plt.imshow(img) plt.subplot(132) plt.imshow(res1) plt.subplot(133) plt.imshow(res2) plt.show()3. 图像的旋转图像旋转需构造旋转矩阵。opencv提供了一个函数： cv2.getRotationMatrix2D()，这个函数需要三个参数，旋转中心，旋转角度，旋转后图像的缩放比例。import cv2 import matplotlib.pyplot as plt img = cv2.imread('flower.jpg') rows,cols = img.shape[:2] #第一个参数旋转中心，第二个参数旋转角度，第三个参数：缩放比例 M = cv2.getRotationMatrix2D((cols/2,rows/2),45,1) #第三个参数：变换后的图像大小 res = cv2.warpAffine(img,M,(rows,cols)) plt.subplot(121) plt.imshow(img) plt.subplot(122) plt.imshow(res) plt.show()4. 图像的仿射图像的旋转加上拉升就是图像仿射变换，仿射变化也需要一个变换矩阵M，opencv提供了根据变换前后三个点的对应关系来自动求解M。这个函数是M=cv2.getAffineTransform(pos1,pos2)，其中两个位置就是变换前后的对应位置关系，输出的就是仿射矩阵M，然后再使用函数cv2.warpAffine()。import cv2 import numpy as np import matplotlib.pyplot as plt img = cv2.imread('flower.jpg') rows,cols = img.shape[:2] pts1 = np.float32([[50,50],[200,50],[50,200]]) pts2 = np.float32([[10,100],[200,50],[100,250]]) M = cv2.getAffineTransform(pts1,pts2) #第三个参数：变换后的图像大小 res = cv2.warpAffine(img,M,(rows,cols)) plt.subplot(121) plt.imshow(img) plt.subplot(122) plt.imshow(res) plt.show()5. 图像的透射透视需要的是一个3*3的矩阵，opencv的函数是M = cv2.getPerspectiveTransform(pts1,pts2)，其中pts需要变换前后的4个点对应位置。得到M后再通过函数cv2.warpPerspective(img,M,(200,200))进行。import cv2 import numpy as np import matplotlib.pyplot as plt img = cv2.imread('flower.jpg') rows,cols = img.shape[:2] pts1 = np.float32([[56,65],[238,52],[28,237],[239,240]]) pts2 = np.float32([[0,0],[200,0],[0,200],[200,200]]) M = cv2.getPerspectiveTransform(pts1,pts2) res = cv2.warpPerspective(img,M,(200,200)) plt.subplot(121) plt.imshow(img) plt.subplot(122) plt.imshow(res) plt.show()"}
{"content2":"在数据集上和别人的结果进行比较，可以量化视觉工作的结果。列举一些从书上摘来的流行的数据集。CUReT: Columbia-Utrecht Reflectance and Texture Database,http://www1.cs.columbia.edu/CAVE/software/curet/(Dana, van Ginneken, Nayaret al.1999).Middlebury Color Datasets: 包含不同相机拍摄的彩色图片，用于研究相机如何对色域和颜色进行变换, http://vision.middlebury.edu/color/data/(Chakrabarti, Scharstein, and Zickler 2009).Middlebury test datasets for evaluating MRF minimization/inference algorithms,http://vision.middlebury.edu/MRF/results/ (Szeliski, Zabih, Scharsteinet al.2008).hapter4: Feature detection and matchingAffine Covariant Features database for evaluating feature detector and descriptor match-ing quality and repeatability, http://www.robots.ox.ac.uk/vgg/research/affine/(Miko-lajczyk and Schmid 2005; Mikolajczyk, Tuytelaars, Schmidet al.2005).Database of matched image patches for learning and feature descriptor evaluation,http://cvlab.epfl.ch/brown/patchdata/patchdata.html(Winder and Brown 2007; Hua,Brown, and Winder 2007).Chapter5: SegmentationBerkeley Segmentation Dataset and Benchmark of 1000 images labeled by 30 humans,along with an evaluation, http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/(Martin, Fowlkes, Talet al.2001).Weizmann segmentation evaluation database of 100 grayscale images with groundtruth segmentations, http://www.wisdom.weizmann.ac.il/vision/SegEvaluationDB/index.html (Alpert, Galun, Basriet al.2007).Chapter8: Dense motion estimationThe Middlebury optic flow evaluation Web site,http://vision.middlebury.edu/flow/data(Baker, Scharstein, Lewiset al.2009).The Human-Assisted Motion Annotation database,http://people.csail.mit.edu/celiu/motionAnnotation/(Liu, Freeman, Adelsonet al.2008)Chapter10: Computational photographyHigh Dynamic Range radiance maps,http://www.debevec.org/Research/HDR/ (De-bevec and Malik 1997).Alpha matting evaluation Web site,http://alphamatting.com/(Rhemann, Rother, Wanget al.2009).Chapter11: Stereo correspondenceMiddlebury Stereo Datasets and Evaluation,http://vision.middlebury.edu/stereo/(Scharstand Szeliski 2002).Stereo Classification and Performance Evaluation of different aggregation costs forstereo matching, http://www.vision.deis.unibo.it/spe/SPEHome.aspx(Tombari, Mat-toccia, Di Stefano et al.2008).Middlebury Multi-View Stereo Datasets,http://vision.middlebury.edu/mview/data/(SeitzCurless, Diebelet al.2006).Multi-view and Oxford Colleges building reconstructions,http://www.robots.ox.ac.uk/vgg/data/data-mview.html.Multi-View Stereo Datasets,http://cvlab.epfl.ch/data/strechamvs/(Strecha, Fransens,and Van Gool 2006).Multi-View Evaluation,http://cvlab.epfl.ch/strecha/multiview/(Strecha, von Hansen,Van Goolet al.2008).hapter12: 3D reconstructionHumanEva: synchronized video and motion capture dataset for evaluation of artic-ulated human motion,http://vision.cs.brown.edu/humaneva/(Sigal, Balan, and Black2010).hapter13: Image-based renderingThe (New) Stanford Light Field Archive,http://lightfield.stanford.edu/(Wilburn, Joshi,Vaishet al.2005).Virtual Viewpoint Video: multi-viewpoint video with per-frame depth maps,http://research.microsoft.com/en-us/um/redmond/groups/ivm/vvv/ (Zitnick, Kang, Uytten-daeleet al.2004).hapter14: RecognitionFor a list of visual recognition datasets, see Tables14.1–14.2. In addition to those,there are also:Buffy pose classes,http://www.robots.ox.ac.uk/vgg/data/buffyposeclasses/and Buffystickmen V2.1,http://www.robots.ox.ac.uk/vgg/data/stickmen/index.html(Ferrari, MarinJimenez, and Zisserman 2009; Eichner and Ferrari 2009).H3D database of pose/joint annotated photographs of humans,http://www.eecs.berkeley.edu/lbourdev/h3d/ (Bourdev and Malik 2009).Action Recognition Datasets,http://www.cs.berkeley.edu/projects/vision/action, has point-ers to several datasets for action and activity recognition, as well as some papers. Thehuman action database athttp://www.nada.kth.se/cvap/actions/contains more actionsequences."}
