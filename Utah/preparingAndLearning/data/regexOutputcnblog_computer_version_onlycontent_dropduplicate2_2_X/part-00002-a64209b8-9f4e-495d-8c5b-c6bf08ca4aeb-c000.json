{"content2":"按照应用目的分类(物体识别、数据挖掘、恢复、分割)、按图像种类分类(普通图像、遥感图像)常用的图像处理算法：数字图像处理基础、遥感数字图像处理、机器视觉、计算机视觉图像处理程序：C++ OpenCV、Matlab与图像处理1. 数字图像处理-概述其实，造成“不可能图形”（三角形的三个角都是90°）的并不是图形本身，而是你对图形的三维知觉系统，这一系列在你知觉图形的立体心理模型时强制作用。在把二维平面图形知觉为你三维立体心理图形时，执行这一过程的机制会极大地影响你的视觉系统。正是在这一强制执行的机制的影响下，你的视觉系统对图形中的每一个点都赋予了深度。换句话说，一幅图像的某些二维结构元素和你三维知觉解释系统的某些结构元素相对应。二维直线被解释成三维直线。二维的平面被解释为三维的平面。在透视图像中，锐角和钝角都被解释为90°角。外面的线段被看作是外形轮廓的分界线。这一外形分界线在你定义整个心理图像的外形轮廓时起着及其重要的作用。这说明，在没有相反信息的影响下，你的视觉系统总是假定你从一个主要视角观看事物。三角形的每一个顶角都产生透视，三个90°的角，而且，每条边的距离变化不同。把三个顶角合成一个整体，就产生了一个空间不可能图形。相对性：环境对比的影响计算机视觉的发展历史：1950s：二维图像分析和识别，如光学字符识别，工件表面、显微图片和航空图片的分析和解释等。是模式识别的重要内容。1960s：MIT的Roberts通过计算机程序从数字图像中提取出诸如立方体、楔形体、棱柱体等多面体的三维结构，并对物体形状及物体的空间关系进行描述．这项研究开创了以理解三维场景为目的的三维计算机视觉的研究．Roberts对积木世界的创造性研究给人们以极大的启发，许多人相信，一旦由白色积木玩具组成的三维世界可以被理解，则可以推广到理解更复杂的三维场景．1970s：出现了一些视觉应用系统．-70年代中期，麻省理工学院(MIT)人工智能(AI)实验室正式开设“计算机视觉” ( Machine Vision) 课程，由B．K．P．Horn教授讲授．-David Marr教授于1973年应邀在MIT AI 实验室领导一个以博士生为主体的研究小组，1977年提出了不同于“积木世界”分析方法的计算视觉理论.1980s：Marr理论成为计算机视觉研究领域中的一个十分重要的理论框架．(ICCV, Marr奖)计算机视觉获得蓬勃发展，新概念、新方法、新理论、新应用不断涌现，比如，基于感知特征群的物体识别理论框架，主动视觉理论框架，视觉集成理论框架等．Marr模型80年代初，Marr首次从信息处理的角度综合了图像处理、心理物理学、神经生理学及临床神经病理学的研究成果，提出了第一个较完备的视觉系统框架。Marr将系统分为基元图、2.5维图（部分的、不完整的三维信息，缺少深度信息）和三维模型三个层次来表达视觉信息的处理过程，而每层的表达将适当的信息明朗化。基元图：使亮度变化（边沿）的信息明朗化，如角点、边缘、纹理、线条、边界等基本特征。2.5维图：使表面朝向的信息明朗化，如场景可见部分的深度、法线方向、轮廓等。三维模型：物体的形状与空间位置信息明朗化。优点：系统地阐述了用二维图像恢复三维物体的可能性和一般性方法。缺点：没有考虑视觉本身具有的反馈机制和不同层次的处理力度。计算机视觉(Computer Vision)与相关学科的关系- 图像处理 (Image Processing) 图像处理通常是把一幅图像变换成另外一幅图像，也就是说，图像处理系统的输入是图像，输出仍然是图像，信息恢复任务则留给人来完成。- 计算机图形学 (Computer Graphics) 通过几何基元，如线、圆和自由曲面等，来生成图像，属于图像综合，它在可视化（Visualization）和虚拟现实(Virtual Reality）中起着很重要的作用．计算机视觉正好是解决相反的问题，即从图像中估计几何基元和其它特征，属于图像分析．- 模式识别 (Pattern Recognition) 研究分类问题，确定符号、图画、物体等输入对象的类别．强调一类事物区别于其它事物所具有的共同特征。一般不关心三维世界的恢复问题。- 人工智能 (Artificial Intelligence) 涉及到智能系统的设计和智能计算的研究．在经过图像处理和图像特征提取过程后，接下来要用人工智能方法对场景特征进行表示，并分析和理解场景．- 媒体计算 (Multimedia Computing) 文字\\图形\\图像\\动画\\视频\\音频等各类感觉媒体的共性基础计算理论、计算方法，以及媒体系统实现技术。以实现下一代计算机能听、能看、会说、会学习为目标。- 认知科学与神经科学 (Cognitive science and Neuroscience) 将人类视觉作为主要的研究对象．计算机视觉中已有的许多方法与人类视觉极为相似．许多计算机视觉研究者对研究人类视觉计算模型比研究计算机视觉系统更感兴趣，希望计算机视觉更加自然化，更加接近生物视觉图信号处理层次-图像处理：图像采集、储存；图像重建；图像变换、增强、恢复、校正；图像(视频)压缩编码。-图像分析：边缘检测、图像分割；目标表达、描述；目标颜色、形状、纹理、空间和运动分析；目标检测、识别。-图像理解：图像配准、融合；3-D表示、建模、场景恢复；图像感知、解释、推理；基于内容的图像和视频检索。计算机视觉技术的应用-工业领域（生产装配、质量检验）-机器人（星球探测机器人）-遥感图像分析（植被分析）-医学图像分析（骨骼定位）-安全鉴别、监视与跟踪（门禁系统、视频监控）-国防系统（目标自动识别与目标跟踪）-图像与视频检索（基于内容的检索）-文物保护（数字博物馆）-其他（游戏、动画、体育、人机交互）-......Learning OpenCV：http://www.cnblogs.com/2008nmj/p/6133737.html"}
{"content2":"关于计算机视觉（随谈）zouxy09@qq.comhttp://blog.csdn.net/zouxy09之前看了这么一本说自然图像统计学的书，本来是想着要好好看，然后每天翻译几页的。但实习的时候太忙了。没有什么时间。所以仅仅把文件夹给翻译了，哈哈。这本书叫：Natural Image Statistics: A Probabilistic Approach to Early Computational Vision大家能够瞧瞧。只是。看到里面视觉概述的时候。自己也想扯扯点东西，按捺不住，就吐了不少文字。假设有什么不正确的地方。也还希望大家不吝指正。谢谢。一、什么叫视觉Vision？什么叫视觉？可能这个问题的第一反应就是：不就是看东西么。好吧，那么高大上的问题就被这么轻描淡写了。看东西不是目的。目的是看到后知道是个什么东西。然后基于这样的物体与事件的联系服务于高层意识理解。哈哈，瞬间档次又拉上去了。个人浅见，眼下大体视觉能够分为生物视觉和机器视觉。生物视觉就是有眼睛的动物能什么看东西的。比如喵星人、汪星人还有地球人啥的。每种动物的视觉感官不同。能感受的光波长也不同。我们假设能够知道它们是怎么工作了，就能够借鉴着做出具有相似强大功能的设备了。这当中涉及到的东西有非常多，它们的视觉流是怎样的？包括怎么接受光信号的输入，到怎样逐级的提取信息。再到高层的推理，还有高层究竟层的反馈等等。有的已经被解密了。有的还犹抱琵琶半遮面，也的神奇依然。遗憾的是，后者的比重更大。眼下来说，大家最感兴趣的还是人的视觉系统。它实在是太强大了，但却又太神奇，使得众多探秘者倾注心血也仅仅能窥知一二。只是也还是有令人欣慰的消息能够传出，然后对某些成果，也有种能用之中的一个二，受用一生的感觉。哈哈。人脑中大约70%的信息是来自于眼睛。大概20-30%的皮层区域是拿来做视觉处理的。生物视觉的最终目标还是想把人的视觉系统彻底告破于天下。嗯。理想是应该丰满点。那机器视觉又是啥呢？就是机器看东西嘛。由于内在的平台还是计算机，所以叫计算机视觉。Computer Vision。它属于人工智能里面非常核心的一部分。机器要像人一样，会看。会听，会说，会学习，会思考等等。当中。像人一样。视觉的信息的比重依然是最大的。所以计算机视觉的重要性毋庸置疑。计算机视觉研究的终极目标也是怎样让机器具有人一样的视觉！这听起来就挺难的是吗？嗯，之前人工智能的发展实在太缓慢了，近几年Deep Learning的出现、大数据和大机器的支撑才给人工智能的发展略微推进了一段距离，但还有非常远的路要走。非常远。好了，视觉是啥？视觉就是看东西，呃，不是非常专业。视觉是为了获取关于环境中物体和事件的信息。从物体发射或者反射出来的光中提取信息的过程。所以我们第一件须要考虑的事情是，这些信息在什么形式的时候才是有效的？物体发出或者反射的光会被收集和度量，当然了，这里没有特定任务的信息提取的处理。生物视觉系统和人工视觉系统都通过同一种方式来完毕这第一个步骤。也就是将这些光投影到一个二维的图像中。对于人眼和摄像头，虽然存在不少区别，但图像信息的处理基本是相同的。通过非常多的感光的细胞或者感光的原件接受光，然后将这些光的强度变成一个二维图像。然后图像的每一个点的光强度就保存了某种信息。一般来说，投影图像还具有时间和彩色的维度。但我们大部分关注静态和灰度的图像。这个图像能够表示为二维的标量函数I(x,y)，也就是给定每一个位置(x,y)。会得到一个相应的灰度值I(x,y)。虽然位置和灰度值都应该是连续的，但典型的情况是离散採样。也就是说x和y是整数，而灰度值在每一个点採样。在数字系统中，採样一般也是矩形。但实际上，生物系统中的空间採集採样并非矩形的。甚至是无规律的。视觉就是从这样的图像数据中提取信息的。物理环境的信息包括在这个图像中的，但非常遗憾，是隐含着的。视觉系统必须将这样的隐含的信息变换成明白的形式。比如识别环境中的物体。但这不是一件easy的事情。二、人类视觉系统的魔法视觉是个非常棘手的任务。虽然这对搞视觉的人来说没什么能够吃惊的。但对其它人来说，他们可能会认为非常吃惊，他似乎没有意识到自己与生俱来的双眼是如此的强大和不可复制。由于他差点儿毫不费力的无时无刻的既有效又高速运行这个任务。但实际上在你看我的文字的时候，你大脑的整个计算过程是非常复杂的，但可惜呀。我们一般仅仅在乎这个结果。大脑直接给我们它的计算结果，而没有告诉我们它完毕这个是多么的辛苦。默默无闻。以使得被忽略和遗忘。为了说明视觉的难度，我们看下图。事实上一个度量后的图像就是这样子的，每一个空间位置有个光的强度值。那请你告诉我，以下这幅图像是个啥？你能够破解吗？OK，我知道这个对大家非常难，那我们看看他究竟是啥。我们把每一个方块的数字换成同等幅度的灰度。瞬间清晰了。有木有，这是一张男人的脸！虽然眼睛接收到的就是上面的那些相似数字之类的东西，但我们的视觉系统把这些数字都变得有意义了，然后发现了真实世界的物体。到眼下为止。我们做了大量慘绝人寰的简化。实际上人的视网膜的光接收器是能够接收不同波长的光的。同一时候我们通常是双眼看世界，而不是仅仅用一个。另一个最大的区别是，我们是看动态的图像，而不是静态的。相机是一次成像，而人眼则是眼睛和大脑的组合。眼睛持续不断把图像信息传递给大脑。并且眼睛在不挺的转动，让高分辨率的感应区域扫过对象的各个细节。大脑把所获得的信息进行动态累加。就得到了我们所感觉到的图像了。你惊呼，差点被忽悠了。但这些区别并没有改变一个事实，就是光信息的确是通过上图那么的数据展示的。所以视觉系统的使命就是读懂这些数据。大部分凡人都会允许说。这些任务刚看起来会非常难，但经过一定的思考后，就会认为应该没那么难吧？图像灰度的边缘不是能够检測出来吗？仅仅要找到小数字和大数字相邻的地方就是边缘了呀。嗯，是的。边缘特征检測的方法眼下已经能够公式计算和实现了。那边缘得到了，将这些边缘片段连接到一起。那物体的轮廓或者说某种直观的物体表达形式是不是就出来了呢？嗯。你是聪明的，这样的直观的想法造就了眼下非常多计算机视觉的算法。非常遗憾的是，这些算法在一些合成图像或者一些在高度限制环境中的图像才有好的表现，但对于无约束的自然图像来说，他们的表现就非常一般。经过多年的研究，有一个结论：real-world images is extremely difficult！就算是找到物体轮廓这样一个任务也变得非常困难，由于一般物体的轮廓在物理世界中是不够清晰的。例如以下图：这不就是一个杯子吗？你看，你看，你就仅仅在乎结果！。！我们慢慢分析下嘛。图中放大了两个地方，一个是本来杯子有边缘的地方，在图像中却没有了边缘，由于背景和杯子非常像，边缘不清晰了。另一个是本来杯子没有边缘的地方，在图像中由于光照的关系，却变得有边缘了。这无中生有。有中化无，大自然的招数。怎么破！或许这还不能说服你。我们看看下图。我们教小孩子识图的时候，是不会拿个这么变态的图来吓他的。但不拿这个给你看。是说服不了你的。说服什么？就一点：视觉有多难，你的视觉系统有多牛掰！我们再看看以下的人脸：不要吃惊。实际上一个小孩在六岁之前，就已经认识一万到三万类的物体了。而这个物理世界。大约也就是三万种物体。你说。眼下的算法怎样才干够强大到囊括这三万种物体（包括能识别它们在全部环境下的全部形态）。那你会想，那人为什么能够呢？你最终问出这个问题了，累觉不爱。由于人有种能从少量样本学习的能力！一类物体，变化万千。但人仅仅要认识当中一两个。剩下的就不攻自破了。融会贯通，举一反三。乃人类与生俱来的才干！说这个好像又没什么实际意义，是吧。要怎么利用这个来指导眼下的视觉才是王道。但鄙人才疏学浅，往下就讲不了了。这也是生物视觉和机器视觉研究者穷尽一生想要共同探索的神奇世界。希望有生之年，能瞥见这漂亮的星空吧。我有时候会想。人从呱呱坠地開始，花了六年的时间。来建立自己强大的大脑，也就是硬件设施。在这里，我还是要对人脑的硬件设备惊叹一番。据了解（这三个字就表示了不知道对不正确，呵呵），人脑的CPU主频是320GHZ，眼下电脑由于半导体工艺的限制，能做到3GHZ左右每一个CPU核。所以咱们搞个300个CPU就能够了么？NO。NO。要运算，每一个CPU之间还得通信，訪问内存数据等，这些通信带宽的限制才是这样的大集群的最大瓶颈。但人脑呢？它的存储体您得按斤算，哈哈。据推測（哈哈。这算严谨么）。人脑存储大概10^15GB。这还不是牛的。最牛的来了，这些空间能够作为内存和缓存使用。不要问我这代表什么了！怀疑怀疑为什么自己老忘记前天老师上课讲的东西。我昨天还记得的啊，怎么就忘了呢，这海马体废了么！哈哈，可能是忘了保存到硬盘了！然后大脑一死机。就没有了！所以记得每天要记得好好吃饭。补充能量。再来膜拜下人眼。人眼是非常不可思议的精密系统。它能自己主动对焦，曝光自己主动补偿，自己主动运动模糊处理，眼下全部让凡人穷三代的单反都望尘莫及的！还包括免费赠送的夜视功能……最奇妙的是。实现这些功能。不须要升级windows和安装不论什么驱动。不须要耗电！每一个人出生的时候就被装配了全部这些功能。所以在这里也呼吁大家好好珍爱如此高档的设备吧，追剧不要太晚，打游戏不要太晚。上班看电脑多用眼药水。哈哈。扯远了。回来说说大脑的系统和软件吧。我们除了睡觉的时间。每时每刻都在接收输入。然后通过自己强大的硬件设施来处理输入。非常关键的一点是，大脑里面会慢慢的建立历史学习到的东西的知识关联库。随着知识的扩充与完好，大脑的网络越来越复杂。神经元个数，神经元连接的个数，千丝万缕却又理不清。但对于计算机呢？我们的硬件设施呢？最牛掰的就是超算了。我们也建立了一个非常大的网络，与人的大脑相当的网络，然后期待着几天、几个月的时间就能够让我们的计算机能达到人脑的水平，这会不会太苛刻了呢？当然了，这当中有几点不公平对照的区别。一是大脑的高层结构和运作机制实际上是不清晰的，所以眼下计算机所做的高层模拟是否与大脑高层相似，这个也没有答案。二是究竟是计算机快还是人脑快？做相同的事情。超算运作须要消耗一个城市一个月的电量，但人脑仅仅须要你吃个鸡腿补充下就好了。OK。what is your point?! I am Sorry!生物上的启示对计算机视觉的研究者是非常重要的。非常多视觉的处理过程和算法就是模拟生物视觉系统来进行设计的。并且也显示出了有效性。但到眼下为止。人类对自身大脑所知还是九牛一毛。更谈不上对视觉更有力的指导了。但实际上。视觉计算理论和生物界实验没有什么谁靠谁，谁欠谁的说法。这里面官方点算就是相辅相成：视觉计算理论能够指导生物实验性研究，实验性研究结果又能够指导计算理论的研究。好了，不说了。"}
{"content2":"CVonlinehttp://homepages.inf.ed.ac.uk/rbf/CVonlinehttp://homepages.inf.ed.ac.uk/rbf/CVonline/unfolded.htmhttp://homepages.inf.ed.ac.uk/rbf/CVonline/CVentry.htm李子青的大作：Markov Random Field Modeling in Computer Visionhttp://www.cbsr.ia.ac.cn/users/szli/mrf_book/book.htmlHandbook of Face Recognition (PDF)http://www.umiacs.umd.edu/~shaohua/papers/zhou04hfr.pdf张正友的有关参数鲁棒估计著作：Parameter Estimation Techniques:A Tutorial with Application to Conic Fittinghttp://research.microsoft.com/~zhang/INRIA/Publis/Tutorial-Estim/Main.htmlAndrea Fusiello“计算机视觉中的几何”教程：Elements of Geometric Computer Visionhttp://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/FUSIELLO4/tutorial.html#x1-520007有关马尔可夫蒙特卡罗方法的资料：An introduction to Markov chain Monte Carlohttp://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/SENEGAS/mcmc.htmlMarkov Chain Monte Carlo for Computer Vision--- A tutorial at ICCV05http://civs.stat.ucla.edu/MCMC/MCMC_tutorial.htm有关独立成分分析（Independent Component Analysis , ICA）的资料：An ICA-Pagehttp://www.cnl.salk.edu/~tony/ica.htmlFast ICAhttp://www.cis.hut.fi/projects/ica/fastica/The Kalman Filter (介绍卡尔曼滤波器的终极网页)http://www.cs.unc.edu/~welch/kalman/index.htmlCached k-d tree search for ICP algorithmshttp://kos.informatik.uni-osnabrueck.de/download/3dim2007/paper.html几个计算机视觉研究工具Machine Vision Toolbox for Matlabhttp://www.petercorke.com/Machine%20Vision%20Toolbox.htmlMatlab and Octave Function for Computer Vision and Image Processinghttp://www.csse.uwa.edu.au/~pk/research/matlabfns/Bayes Net Toolbox for Matlabhttp://www.cs.ubc.ca/~murphyk/Software/BNT/bnt.htmlOpenCV (Chinese)http://www.opencv.org.cn/index.php/%E9%A6%96%E9%A1%B5Gandalf (A Computer Vision and Numerical Algorithm Labrary)http://gandalf-library.sourceforge.net/CMU Computer Vision Home Pagehttp://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.htmlMachine Learning Resource Linkshttp://www.cse.ust.hk/~ivor/resource.htmThe Bayesian Filtering Libraryhttp://www.orocos.org/bflOptical Flow Algorithm Evaluation (提供了一个动态贝叶斯网络框架，例如递归信息处理与分析、卡尔曼滤波、粒子滤波、序列蒙特卡罗方法等，C++写的)http://of-eval.sourceforge.net/MATLAB code for ICP algorithmhttp://www.usenet.com/newsgroups/comp.graphics.visualization/msg00102.html牛人主页：朱松纯 （Song-Chun Zhu）http://www.stat.ucla.edu/~sczhu/David Lowe (SIFT) (很帅的一个老头哦 ^ ^)http://www.cs.ubc.ca/~lowe/Andrea Vedaldi (SIFT)http://vision.ucla.edu/~vedaldi/index.htmlPedro F. Felzenszwalbhttp://people.cs.uchicago.edu/~pff/Dougla Dlanman (Brown的一个研究生，在其主页上搜集了大量算法教程和源码)http://mesh.brown.edu/dlanman/courses.htmlJianbo Shi (Ncuts 的始作俑者)http://www.cis.upenn.edu/~jshi/Active Vision Group (Oxford的一个机器视觉研究团队，特色是SLAM，监视，导航)http://www.robots.ox.ac.uk/ActiveVision/index.htmlJuyang Weng（机器学习的专家，Autonomous Mental Development 是其特色）http://www.cse.msu.edu/~weng/测试图片或视频：Middlebury College‘s Stereo Vision Data Sethttp://cat.middlebury.edu/stereo/data.htmlIntelligent Vehicle:IVSourcewww.ivsoruce.netRobot Carhttp://www.plyojump.com/robot_cars.htmlHow to Build a Robot: The Computer Vision Parthttp://www.societyofrobots.com/programming_computer_vision_tutorial.shtmlposted @ 2011-03-31 11:36 Livid 阅读(112) 评论(0) 编辑（转）Computer Vision Open Source Algorithm ImplementationsComputer Vision Open Source Algorithm ImplementationsParticipate in Reproducible ResearchWARNING: this page is not and will never be exhaustive but only try to gather robust implementations of Computer Vision state of the art(back to computer vision resource)If you have additions or changes, send an e-mail (remove the \"nospam\").ChangelogRSS feed. If you have any issue please send an e-mail (remove the \"nospam\").This material is presented to ensure timely dissemination of computer vision algorithms. Copyright and all rights therein are retained by authors or by other copyright holders. All persons copying this information are expected to adhere to the terms and constraints invoked by each authors copyright.General Image ProcessingOpenCV(C/C++ code, BSD lic) Image manipulation, matrix manipulation, transformsTorch3Vision(C/C++ code, BSD lic) Basic image processing, matrix manipulation and feature extraction algorithms: rotation, flip, photometric normalisations (Histogram Equalization, Multiscale Retinex, Self-Quotient Image or Gross-Brajovic), edge detection, 2D DCT, 2D FFT, 2D Gabor, PCA to do Eigen-Faces, LDA to do Fisher-Faces. Various metrics (Euclidean, Mahanalobis, ChiSquare, NormalizeCorrelation, TangentDistance, ...)GradientShop(C/C++ code, GPL lic) GradientShop: A Gradient-Domain Optimization Framework for Image and Video FilteringImLab(C/C++ code, MIT lic) A Free Experimental System for Image Processing (loading, transforms, filters, histogram, morphology, ...)CIMG(C/C++ code, GPL and LGPL lic) CImg Library is an open source C++ toolkit for image processingGeneric Image Library (GIL) - boost integration(C/C++ code, MIT lic) Adobe open source C++ Generic Image Library (GIL)Image Acquisition, Decoding & encodingFFMPEG(C/C++ code, LGPL or GPL lic) Record, convert and stream audio and video (lot of codec)OpenCV(C/C++ code, BSD lic) PNG, JPEG,... images, avi video files, USB webcam,...Torch3Vision(C/C++ code, BSD lic) Video file decoding/encoding (ffmpeg integration), image capture from a frame grabber or from USB, Sony pan/tilt/zoom camera control using VISCA interfacelib VLC(C/C++ code, GPL lic) Used by VLC player: record, convert and stream audio and videoLive555(C/C++ code, LGPL lic) RTSP streamsImageMagick(C/C++ code, GPL lic) Loading & saving DPX, EXR, GIF, JPEG, JPEG-2000, PDF, PhotoCD, PNG, Postscript, SVG, TIFF, and moreDevIL(C/C++ code, LGPL lic) Loading & saving various image formatFreeImage(C/C++ code, GPL & FPL lic) PNG, BMP, JPEG, TIFF loadingSegmentationOpenCV(C/C++ code, BSD lic) Pyramid image segmentationBranch-and-Mincut(C/C++ code, Microsoft Research Lic) Branch-and-Mincut Algorithm for Image SegmentationEfficiently solving multi-label MRFs (Readme)(C/C++ code) Segmentation, object category labelling, stereoMachine LearningTorch(C/C++ code, BSD lic) Gradient machines ( multi-layered perceptrons, radial basis functions, mixtures of experts, convolutional networks and even time-delay neural networks), Support vector machines, Ensemble models (bagging, adaboost), Non-parametric models (K-nearest-neighbors, Parzen regression and Parzen density estimator), distributions (Kmeans, Gaussian mixture models, hidden Markov models, input-output hidden Markov models, and Bayes classifier), speech recognition toolsObject DetectionOpenCV(C/C++ code, BSD lic) Viola-jones face detection (Haar features)Torch3Vision(C/C++ code, BSD lic) MLP & cascade of Haar-like classifiers face detectionHough Forests(C/C++ code, Microsoft Research Lic) Class-Specific Hough Forests for Object DetectionEfficient Subwindow Object Detection(C/C++ code, Apache Lic) Christoph Lampert \"Efficient Subwindow\" algorithms for Object DetectionObject Category LabellingEfficiently solving multi-label MRFs (Readme)(C/C++ code) Segmentation, object category labelling, stereoOptical flowOpenCV(C/C++ code, BSD lic) Horn & Schunck algorithm, Lucas & Kanade algorithm, Lucas-Kanade optical flow in pyramids, block matchingGPU-KLT+FLOW(C/C++/OpenGL/Cg code, LGPL) Gain-Adaptive KLT Tracking and TV-L1 optical flow on the GPUFeatures Extraction & MatchingSIFT by R. Hess(C/C++ code, GPL lic) SIFT feature extraction & RANSAC matchingOpenSURF(C/C++ code) SURF feature extraction algorihtm (kind of fast SIFT)ASIFT (from IPOL)(C/C++ code, Ecole Polytechnique and ENS Cachan for commercial Lic) Affine SIFT (ASIFT)VLFeat (formely Sift++)(C/C++ code) SIFT, MSER, k-means, hierarchical k-means, agglomerative information bottleneck, and quick shiftSiftGPUA GPU Implementation of Scale Invariant Feature Transform (SIFT)Groupsac(C/C++ code, GPL lic) An enhance version of RANSAC that considers the correlation between data pointsNearest Neighbors matchingFLANN(C/C++ code, BSD lic) Approximate Nearest Neighbors (Fast Approximate Nearest Neighbors with Automatic Algorithm Configuration)ANN(C/C++ code, LGPL lic) Approximate Nearest Neighbor SearchingTrackingOpenCV(C/C++ code, BSD lic) Kalman, Condensation, CAMSHIFT, Mean shift, SnakesKLT: An Implementation of the Kanade-Lucas-Tomasi Feature Tracker(C/C++ code, public domain) Kanade-Lucas-Tomasi Feature TrackerGPU_KLT(C/C++/OpenGL/Cg code, ) A GPU-based Implementation of the Kanade-Lucas-Tomasi Feature TrackerGPU-KLT+FLOW(C/C++/OpenGL/Cg code, LGPL) Gain-Adaptive KLT Tracking and TV-L1 optical flow on the GPUSimultaneous localization and mappingReal-Time SLAM - SceneLib(C/C++ code, LGPL lic) Real-time vision-based SLAM with a single cameraPTAM(C/C++ code, Isis Innovation Limited lic) Parallel Tracking and Mapping for Small AR WorkspacesCamera Calibration & constraintOpenCV(C/C++ code, BSD lic) Chessboard calibration, calibration with rig or patternGeometric camera constraint - Minimal Problems in Computer VisionMinimal problems in computer vision arise when computing geometrical models from image data. They often lead to solving systems of algebraic equations.Camera Calibration Toolbox for Matlab(Matlab toolbox) Camera Calibration Toolbox for Matlab by Jean-Yves Bouguet (C implementation in OpenCV)Multi-View ReconstructionBundle Adjustment - SBA(C/C++ code, GPL lic) A Generic Sparse Bundle Adjustment Package Based on the Levenberg-Marquardt AlgorithmBundle Adjustment - SSBA(C/C++ code, LGPL lic) Simple Sparse Bundle Adjustment (SSBA)StereoEfficiently solving multi-label MRFs (Readme)(C/C++ code) Segmentation, object category labelling, stereoStructure from motionBundler(C/C++ code, GPL lic) A structure-from-motion system for unordered image collectionsPatch-based Multi-view Stereo Software (Windows version)(C/C++ code, GPL lic) A multi-view stereo software that takes a set of images and camera parameters, then reconstructs 3D structure of an object or a scene visible in the imageslibmv - work in progress(C/C++ code, MIT lic) A structure from motion library http://homepages.inf.ed.ac.uk/rbf/CVonlinehttp://homepages.inf.ed.ac.uk/rbf/CVonline/unfolded.htmhttp://homepages.inf.ed.ac.uk/rbf/CVonline/CVentry.htm李子青的大作：Markov Random Field Modeling in Computer Visionhttp://www.cbsr.ia.ac.cn/users/szli/mrf_book/book.htmlHandbook of Face Recognition (PDF)http://www.umiacs.umd.edu/~shaohua/papers/zhou04hfr.pdf张正友的有关参数鲁棒估计著作：Parameter Estimation Techniques:A Tutorial with Application to Conic Fittinghttp://research.microsoft.com/~zhang/INRIA/Publis/Tutorial-Estim/Main.htmlAndrea Fusiello“计算机视觉中的几何”教程：Elements of Geometric Computer Visionhttp://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/FUSIELLO4/tutorial.html#x1-520007有关马尔可夫蒙特卡罗方法的资料：An introduction to Markov chain Monte Carlohttp://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/SENEGAS/mcmc.htmlMarkov Chain Monte Carlo for Computer Vision--- A tutorial at ICCV05http://civs.stat.ucla.edu/MCMC/MCMC_tutorial.htm有关独立成分分析（Independent Component Analysis , ICA）的资料：An ICA-Pagehttp://www.cnl.salk.edu/~tony/ica.htmlFast ICAhttp://www.cis.hut.fi/projects/ica/fastica/The Kalman Filter (介绍卡尔曼滤波器的终极网页)http://www.cs.unc.edu/~welch/kalman/index.htmlCached k-d tree search for ICP algorithmshttp://kos.informatik.uni-osnabrueck.de/download/3dim2007/paper.html几个计算机视觉研究工具Machine Vision Toolbox for Matlabhttp://www.petercorke.com/Machine%20Vision%20Toolbox.htmlMatlab and Octave Function for Computer Vision and Image Processinghttp://www.csse.uwa.edu.au/~pk/research/matlabfns/Bayes Net Toolbox for Matlabhttp://www.cs.ubc.ca/~murphyk/Software/BNT/bnt.htmlOpenCV (Chinese)http://www.opencv.org.cn/index.php/%E9%A6%96%E9%A1%B5Gandalf (A Computer Vision and Numerical Algorithm Labrary)http://gandalf-library.sourceforge.net/CMU Computer Vision Home Pagehttp://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.htmlMachine Learning Resource Linkshttp://www.cse.ust.hk/~ivor/resource.htmThe Bayesian Filtering Libraryhttp://www.orocos.org/bflOptical Flow Algorithm Evaluation (提供了一个动态贝叶斯网络框架，例如递归信息处理与分析、卡尔曼滤波、粒子滤波、序列蒙特卡罗方法等，C++写的)http://of-eval.sourceforge.net/MATLAB code for ICP algorithmhttp://www.usenet.com/newsgroups/comp.graphics.visualization/msg00102.html牛人主页：朱松纯 （Song-Chun Zhu）http://www.stat.ucla.edu/~sczhu/David Lowe (SIFT) (很帅的一个老头哦 ^ ^)http://www.cs.ubc.ca/~lowe/Andrea Vedaldi (SIFT)http://vision.ucla.edu/~vedaldi/index.htmlPedro F. Felzenszwalbhttp://people.cs.uchicago.edu/~pff/Dougla Dlanman (Brown的一个研究生，在其主页上搜集了大量算法教程和源码)http://mesh.brown.edu/dlanman/courses.htmlJianbo Shi (Ncuts 的始作俑者)http://www.cis.upenn.edu/~jshi/Active Vision Group (Oxford的一个机器视觉研究团队，特色是SLAM，监视，导航)http://www.robots.ox.ac.uk/ActiveVision/index.htmlJuyang Weng（机器学习的专家，Autonomous Mental Development 是其特色）http://www.cse.msu.edu/~weng/测试图片或视频：Middlebury College‘s Stereo Vision Data Sethttp://cat.middlebury.edu/stereo/data.htmlIntelligent Vehicle:IVSourcewww.ivsoruce.netRobot Carhttp://www.plyojump.com/robot_cars.htmlHow to Build a Robot: The Computer Vision Parthttp://www.societyofrobots.com/programming_computer_vision_tutorial.shtml"}
{"content2":"OpenCV是什么？Open Source Computer Vision Library，开源的计算机视觉库。引起我学习兴趣四点：1.代码开源，更好的了解内部核心运行机制，就像浏览一张全裸美女图片，遮遮掩掩的有什么好看的。2.C编写，少部分C++，便于代码移植。3.视觉图形处理，人工智能核心全靠计算机如何理解图形。逻辑运算计算机秒杀人脑，但在图像分析方面计算机还是个菜鸟。所以视觉图形学发展潜力巨大。4.算法魅丽让数学更好玩。国内IT程序员的职业生涯大概35岁左右，为什么？技术瓶颈。优秀的程序员+较强的数学功底让你代码轻松写到60岁。应用领域：计算机科学和工程，数学和统计，医学研究，物理学，军事等人工智能领域。面向的对象：OpenCV深奥吗？拥有什么基础的人员可以接触呢？答案简单，只要你有兴趣有电脑就行。计算机软件学科的好处就是入门代价容易。让你做个小小的螺丝帽你还要有台车床，但一台二手的X86电脑就可以编写世界上最优秀的程序。OpenvCV框架：OpenvCV分为五大模块1.CXCORE模块:属于图形的最基本结构，函数和算法。2.CV模块：就是图像处理函数和视觉算法了。3.MLL模块：智能机器学习库，包含统计分类器和聚类器等。4.HighGUI：图像和视频输入/输出库。深入学习：预备知识：1.原生态：c/c++编程基础。2..net版：emgu cv封装了opencv的库。可以通过.net平台支持的语言开发。底层完全调用opencv库。3.android版4.IOS版深入学习OpenCV算法，需要具备高等数学基础，涵盖：线性代数，矩阵代数，最小二乘法，高斯分布，贝叶斯定律，函数求导等。亲们大学高等数学都学的咋样，估计都忘光了吧。我们就从高等数学符号温习下，这些曾经相识忘却了的符号回忆：（集合与逻辑）（线性代数）（数列，函数与极限）（初等函数）（微积分）（概率论与数理统计）（其它）OpenCV你的艺术魔法棒：未来智能家居将依托机器人的智能图像感知能力配合你的工作生活。OpenCV将架起视觉分析逻辑的桥梁。"}
{"content2":"1人工智能科普类：人工智能科普、人工智能哲学《智能的本质》斯坦福、伯克利客座教授30年AI研究巅峰之作《科学+遇见人工智能》李开复、张亚勤、张首晟等20余位科学家与投资人共同解读AI革命《人工智能时代》从人工智能的历史、现状、未来，工业机器人、商业机器人、家用机器人、机器翻译、机器学习等人工智能应用领域依次介绍了人工智能发展前景。《人工智能简史》 跟着图灵、冯•诺依曼、香农、西蒙、纽维尔、麦卡锡、明斯基等人工智能的先驱们重走人工智能之路，站在前人的肩膀上，看人工智能的三生三世，鉴以往才能知未来。2人工智能机器学习类：Python、机器学习、数据科学《Python机器学习实践指南》 结合了机器学习和Python 语言两个热门的领域，通过利用两种核心的机器学习算法来用Python 做数据分析。《Python机器学习——预测分析核心算法》从算法和Python语言实现的角度，认识机器学习。《机器学习实践应用》阿里机器学习专家力作，实战经验分享，基于阿里云机器学习平台，针对7个具体的业务场景，搭建了完整的解决方案。《NLTK基础教程——用NLTK和Python库构建机器学习应用》介绍如何通过NLTK库与一些Python库的结合从而实现复杂的NLP任务和机器学习应用。3人工智能深度学习类：深度学习、Tensorflow《深度学习》AI圣经，深度学习领域奠基性的经典畅销书 特斯拉CEO埃隆·马斯克等国内外众多专家推荐！《深度学习精要（基于R语言）》基于R语言实战,使用无监督学习建立自动化的预测和分类模型《TensorFlow技术解析与实战》包揽TensorFlow1.1的新特性 人脸识别 语音识别 图像和语音相结合等热点一应俱全《TensorFlow机器学习项目实战》第二代机器学习实战指南，提供深度学习神经网络等项目实战，有效改善项目速度和效率。4人工智能算法策略类：算法、推荐系统、编程等《神经网络算法与实现——基于Java语言》 完整地演示了使用Java开发神经网络的过程，既有非常基础的实例也有高级实例。《趣学算法》 50 多个实例循展示算法的设计、实现、复杂性分析及优化过程 培养算法思维 带您感受算法之美。《算法谜题》 Google、Facebook等一流IT公司算法面试必备，经典算法谜题合集。《Python算法教程》精通Python基础算法，畅销书Python基础教程作者力作。《编程之法：面试和算法心得》程序员面试宝典 笔试金典 CSDN访问量过千万的博客结构之法算法之道博主July著作。《趣题学算法》 一本有趣的、易学的、实用的，帮助读者快速入门应用的算法书。《Java遗传算法编程》 遗传算法设计 机器学习人工智能 来自Java专家的声音 用遗传算法解决类似旅行商的经典问题。《算法学习与应用从入门到精通》320个实例、753分钟视频、5个综合案例、74个技术解惑，一本书的容量，讲解了入门类、范例类和项目实战类三类图书的内容。5人工智能时间图像和视觉识别类：图像识别 、语音识别、自然语言处理、建模工程《OpenCV和Visual Studio图像识别应用开发》无人驾驶人脸识别基础技术 用OpenCV实现图像处理应用 计算机视觉编程实战手册。《人脸识别原理及算法——动态人脸识别系统研究》 介绍了动态场景下的人脸识别方法，该方法综合应用了人脸定位、人脸识别、视频处理等算法。《精通Python自然语言处理》用Python开发令人惊讶的NLP项目，自然语言处理任务，掌握利用Python设计和构建给予NLP的应用的实践。《Python自然语言处理》基于Python编程语言和NLTK，自然语言处理领域的一本实用入门指南。《贝叶斯方法：概率编程与贝叶斯推断》 机器学习 人工智能 数据分析从业者的技能基础 国际杰出机器学习专家余凯博士 腾讯专家研究员岳亚丁博士推荐。《贝叶斯思维：统计建模的Python学习法》Thin Stats和Think Python图书作者重磅出击，数据分析师、数据工程师、数据科学家案头常备。《概率编程实战》人工智能领域的先驱、美国加州大学伯克利分校教授Stuart Russell作序推荐！一本不可思议的Scala概率编程实战书籍！《自己动手写神经网络》机器学习与人工智能参考书，基于Java语言撰写。"}
{"content2":"一、模块概述上节的最后，我们进行了如下操作获取了有限的proposal，# [IMAGES_PER_GPU, num_rois, (y1, x1, y2, x2)] # IMAGES_PER_GPU取代了batch，之后说的batch都是IMAGES_PER_GPU rpn_rois = ProposalLayer( proposal_count=proposal_count, nms_threshold=config.RPN_NMS_THRESHOLD, # 0.7 name=\"ROI\", config=config)([rpn_class, rpn_bbox, anchors])总结一下：与 GT 的 IOU 大于0.7与某一个 GT 的 IOU 最大的那个 anchor进一步，我们需要按照RCNN的思路，使用proposal对共享特征进行ROI操作，在Mask-RCNN中这里有两个创新：ROI使用ROI Align取代了之前的ROI Pooling共享特征由之前的单层变换为了FPN得到的金字塔多层特征，即：mrcnn_feature_maps = [P2, P3, P4, P5]其中创新点2意味着我们不同的proposal对应去ROI的特征层并不相同，所以，我们需要：按照proposal的长宽，将不同的proposal对应给不同的特征层在对应特征层上进行ROI操作二、实现分析下面会用到高维切片函数，这里先行给出讲解链接：『TensorFlow』高级高维切片gather_nd接前文bulid函数代码，我们如下调入实现本节的功能，if mode == \"training\": …… else: # Network Heads # Proposal classifier and BBox regressor heads # output shapes: # mrcnn_class_logits: [batch, num_rois, NUM_CLASSES] classifier logits (before softmax) # mrcnn_class: [batch, num_rois, NUM_CLASSES] classifier probabilities # mrcnn_bbox(deltas): [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))] mrcnn_class_logits, mrcnn_class, mrcnn_bbox =\\ fpn_classifier_graph(rpn_rois, mrcnn_feature_maps, input_image_meta, config.POOL_SIZE, config.NUM_CLASSES, train_bn=config.TRAIN_BN, fc_layers_size=config.FPN_CLASSIF_FC_LAYERS_SIZE)FPN特征层分类函数纵览如下，############################################################ # Feature Pyramid Network Heads ############################################################ def fpn_classifier_graph(rois, feature_maps, image_meta, pool_size, num_classes, train_bn=True, fc_layers_size=1024): \"\"\"Builds the computation graph of the feature pyramid network classifier and regressor heads. rois: [batch, num_rois, (y1, x1, y2, x2)] Proposal boxes in normalized coordinates. feature_maps: List of feature maps from different layers of the pyramid, [P2, P3, P4, P5]. Each has a different resolution. image_meta: [batch, (meta data)] Image details. See compose_image_meta() pool_size: The width of the square feature map generated from ROI Pooling. num_classes: number of classes, which determines the depth of the results train_bn: Boolean. Train or freeze Batch Norm layers fc_layers_size: Size of the 2 FC layers Returns: logits: [batch, num_rois, NUM_CLASSES] classifier logits (before softmax) probs: [batch, num_rois, NUM_CLASSES] classifier probabilities bbox_deltas: [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))] Deltas to apply to proposal boxes \"\"\" # ROI Pooling # Shape: [batch, num_rois, POOL_SIZE, POOL_SIZE, channels] x = PyramidROIAlign([pool_size, pool_size], name=\"roi_align_classifier\")([rois, image_meta] + feature_maps) # Two 1024 FC layers (implemented with Conv2D for consistency) # TimeDistributed拆分了输入数据的第1维（从0开始），将完全一样的模型独立的应用于拆分后的输入数据，具体到下行， # 就是将num_rois个卷积应用到num_rois个维度为[batch, POOL_SIZE, POOL_SIZE, channels]的输入，结果合并 x = KL.TimeDistributed(KL.Conv2D(fc_layers_size, (pool_size, pool_size), padding=\"valid\"), name=\"mrcnn_class_conv1\")(x) # [batch, num_rois, 1, 1, 1024] x = KL.TimeDistributed(BatchNorm(), name='mrcnn_class_bn1')(x, training=train_bn) x = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.Conv2D(fc_layers_size, (1, 1)), name=\"mrcnn_class_conv2\")(x) x = KL.TimeDistributed(BatchNorm(), name='mrcnn_class_bn2')(x, training=train_bn) x = KL.Activation('relu')(x) shared = KL.Lambda(lambda x: K.squeeze(K.squeeze(x, 3), 2), name=\"pool_squeeze\")(x) # [batch, num_rois, 1024] # Classifier head mrcnn_class_logits = KL.TimeDistributed(KL.Dense(num_classes), name='mrcnn_class_logits')(shared) mrcnn_probs = KL.TimeDistributed(KL.Activation(\"softmax\"), name=\"mrcnn_class\")(mrcnn_class_logits) # BBox head # [batch, num_rois, NUM_CLASSES * (dy, dx, log(dh), log(dw))] x = KL.TimeDistributed(KL.Dense(num_classes * 4, activation='linear'), name='mrcnn_bbox_fc')(shared) # Reshape to [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))] s = K.int_shape(x) # 下行源码：K.reshape(inputs, (K.shape(inputs)[0],) + self.target_shape) mrcnn_bbox = KL.Reshape((s[1], num_classes, 4), name=\"mrcnn_bbox\")(x) return mrcnn_class_logits, mrcnn_probs, mrcnn_bbox下面我们来分析一下该函数。进入函数，首先调用了PyramidROI，# ROI Pooling # Shape: [batch, num_rois, POOL_SIZE, POOL_SIZE, channels] x = PyramidROIAlign([pool_size, pool_size], name=\"roi_align_classifier\")([rois, image_meta] + feature_maps)这个class基本实现了我们开篇所说的全部功能，即特征层分类并ROI。ROIAlign类首先我们依据『计算机视觉』FPN特征金字塔网络中第三节所讲，对proposal进行分类，注意的是我们使用于网络中的hw是归一化了的（以原图hw为单位长度），所以计算时需要还原（对于公式而言：w，h分别表示宽度和高度；k是分配RoI的level；是w,h=224,224时映射的level）。注意两个操作节点：level_boxes和box_indices，第一个记录了对应的level特征层中分配到的每个box的坐标，第二个则记录了每个box对应的图片在batch中的索引（一个记录了候选框索引对应的图片即下文中的两个大块，一个记录了候选框的索引对应其坐标即小黑框的坐标），两者结合可以索引到下面每个黑色小框的坐标信息。至于ROI Align本身，实际就是双线性插值，使用内置API实现即可。这里属于RPN网络和RCNN网络的分界线，level_boxes和box_indices本身属于RPN计算出来结果，但是两者作用于feature后的输出Tensor却是RCNN部分的输入，但是两部分的梯度不能相互流通的，所以需要tf.stop_gradient()截断梯度传播。############################################################ # ROIAlign Layer ############################################################ def log2_graph(x): \"\"\"Implementation of Log2. TF doesn't have a native implementation.\"\"\" return tf.log(x) / tf.log(2.0) class PyramidROIAlign(KE.Layer): \"\"\"Implements ROI Pooling on multiple levels of the feature pyramid. Params: - pool_shape: [pool_height, pool_width] of the output pooled regions. Usually [7, 7] Inputs: - boxes: [batch, num_boxes, (y1, x1, y2, x2)] in normalized coordinates. Possibly padded with zeros if not enough boxes to fill the array. - image_meta: [batch, (meta data)] Image details. See compose_image_meta() - feature_maps: List of feature maps from different levels of the pyramid. Each is [batch, height, width, channels] Output: Pooled regions in the shape: [batch, num_boxes, pool_height, pool_width, channels]. The width and height are those specific in the pool_shape in the layer constructor. \"\"\" def __init__(self, pool_shape, **kwargs): super(PyramidROIAlign, self).__init__(**kwargs) self.pool_shape = tuple(pool_shape) def call(self, inputs): # num_boxes指的是proposal数目，它们均会作用于每张图片上，只是不同的proposal作用于图片 # 的特征级别不同，我通过循环特征层寻找符合的proposal，应用ROIAlign # Crop boxes [batch, num_boxes, (y1, x1, y2, x2)] in normalized coords boxes = inputs[0] # Image meta # Holds details about the image. See compose_image_meta() image_meta = inputs[1] # Feature Maps. List of feature maps from different level of the # feature pyramid. Each is [batch, height, width, channels] feature_maps = inputs[2:] # Assign each ROI to a level in the pyramid based on the ROI area. y1, x1, y2, x2 = tf.split(boxes, 4, axis=2) h = y2 - y1 w = x2 - x1 # Use shape of first image. Images in a batch must have the same size. image_shape = parse_image_meta_graph(image_meta)['image_shape'][0] # h, w, c # Equation 1 in the Feature Pyramid Networks paper. Account for # the fact that our coordinates are normalized here. # e.g. a 224x224 ROI (in pixels) maps to P4 image_area = tf.cast(image_shape[0] * image_shape[1], tf.float32) roi_level = log2_graph(tf.sqrt(h * w) / (224.0 / tf.sqrt(image_area))) # h、w已经归一化 roi_level = tf.minimum(5, tf.maximum( 2, 4 + tf.cast(tf.round(roi_level), tf.int32))) # 确保值位于2到5之间 roi_level = tf.squeeze(roi_level, 2) # [batch, num_boxes] # Loop through levels and apply ROI pooling to each. P2 to P5. pooled = [] box_to_level = [] for i, level in enumerate(range(2, 6)): # tf.where 返回值格式 [坐标1, 坐标2……] # np.where 返回值格式 [[坐标1.x, 坐标2.x……], [坐标1.y, 坐标2.y……]] ix = tf.where(tf.equal(roi_level, level)) # 返回坐标表示：第n张图片的第i个proposal level_boxes = tf.gather_nd(boxes, ix) # [本level的proposal数目, 4] # Box indices for crop_and_resize. box_indices = tf.cast(ix[:, 0], tf.int32) # 记录每个propose对应图片序号 # Keep track of which box is mapped to which level box_to_level.append(ix) # Stop gradient propogation to ROI proposals level_boxes = tf.stop_gradient(level_boxes) box_indices = tf.stop_gradient(box_indices) # Crop and Resize # From Mask R-CNN paper: \"We sample four regular locations, so # that we can evaluate either max or average pooling. In fact, # interpolating only a single value at each bin center (without # pooling) is nearly as effective.\" # # Here we use the simplified approach of a single value per bin, # which is how it's done in tf.crop_and_resize() # Result: [this_level_num_boxes, pool_height, pool_width, channels] pooled.append(tf.image.crop_and_resize( feature_maps[i], level_boxes, box_indices, self.pool_shape, method=\"bilinear\")) # 输入参数shape: # [batch, image_height, image_width, channels] # [this_level_num_boxes, 4] # [this_level_num_boxes] # [height, pool_width] # Pack pooled features into one tensor pooled = tf.concat(pooled, axis=0) # [batch*num_boxes, pool_height, pool_width, channels] # Pack box_to_level mapping into one array and add another # column representing the order of pooled boxes box_to_level = tf.concat(box_to_level, axis=0) # [batch*num_boxes, 2] box_range = tf.expand_dims(tf.range(tf.shape(box_to_level)[0]), 1) # [batch*num_boxes, 1] box_to_level = tf.concat([tf.cast(box_to_level, tf.int32), box_range], axis=1) # [batch*num_boxes, 3] # 截止到目前，我们获取了记录全部ROIAlign结果feat集合的张量pooled，和记录这些feat相关信息的张量box_to_level， # 由于提取方法的原因，此时的feat并不是按照原始顺序排序（先按batch然后按box index排序），下面我们设法将之恢复顺 # 序（ROIAlign作用于对应图片的对应proposal生成feat） # Rearrange pooled features to match the order of the original boxes # Sort box_to_level by batch then box index # TF doesn't have a way to sort by two columns, so merge them and sort. # box_to_level[i, 0]表示的是当前feat隶属的图片索引，box_to_level[i, 1]表示的是其box序号 sorting_tensor = box_to_level[:, 0] * 100000 + box_to_level[:, 1] # [batch*num_boxes] ix = tf.nn.top_k(sorting_tensor, k=tf.shape( box_to_level)[0]).indices[::-1] ix = tf.gather(box_to_level[:, 2], ix) pooled = tf.gather(pooled, ix) # Re-add the batch dimension # [batch, num_boxes, (y1, x1, y2, x2)], [batch*num_boxes, pool_height, pool_width, channels] shape = tf.concat([tf.shape(boxes)[:2], tf.shape(pooled)[1:]], axis=0) pooled = tf.reshape(pooled, shape) return pooled # [batch, num_boxes, pool_height, pool_width, channels]初步分类/回归经过ROI之后，我们获取了众多shape一致的小feat，为了获取他们的分类回归信息，我们构建一系列并行的网络进行处理，# Two 1024 FC layers (implemented with Conv2D for consistency) # TimeDistributed拆分了输入数据的第1维（从0开始），将完全一样的模型独立的应用于拆分后的输入数据，具体到下行， # 就是将num_rois个卷积应用到num_rois个维度为[batch, POOL_SIZE, POOL_SIZE, channels]的输入，结果合并 x = KL.TimeDistributed(KL.Conv2D(fc_layers_size, (pool_size, pool_size), padding=\"valid\"), name=\"mrcnn_class_conv1\")(x) # [batch, num_rois, 1, 1, 1024] x = KL.TimeDistributed(BatchNorm(), name='mrcnn_class_bn1')(x, training=train_bn) x = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.Conv2D(fc_layers_size, (1, 1)), name=\"mrcnn_class_conv2\")(x) x = KL.TimeDistributed(BatchNorm(), name='mrcnn_class_bn2')(x, training=train_bn) x = KL.Activation('relu')(x) shared = KL.Lambda(lambda x: K.squeeze(K.squeeze(x, 3), 2), name=\"pool_squeeze\")(x) # [batch, num_rois, 1024] # Classifier head mrcnn_class_logits = KL.TimeDistributed(KL.Dense(num_classes), name='mrcnn_class_logits')(shared) mrcnn_probs = KL.TimeDistributed(KL.Activation(\"softmax\"), name=\"mrcnn_class\")(mrcnn_class_logits) # BBox head # [batch, num_rois, NUM_CLASSES * (dy, dx, log(dh), log(dw))] x = KL.TimeDistributed(KL.Dense(num_classes * 4, activation='linear'), name='mrcnn_bbox_fc')(shared) # Reshape to [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))] s = K.int_shape(x) # 下行源码：K.reshape(inputs, (K.shape(inputs)[0],) + self.target_shape) mrcnn_bbox = KL.Reshape((s[1], num_classes, 4), name=\"mrcnn_bbox\")(x) return mrcnn_class_logits, mrcnn_probs, mrcnn_bbox返回如下：mrcnn_class_logits:      [batch, num_rois, NUM_CLASSES]    classifier logits (before softmax)mrcnn_class:                [batch, num_rois, NUM_CLASSES]    classifier probabilitiesmrcnn_bbox(deltas):    [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))]KL.TimeDistributed实现建立一系列同样架构的的并行网络结构（dim0个），将[dim0, dim1, ……]中的每个[dim1, ……]作为输入，并行的计算输出。附、build函数总览def build(self, mode, config): \"\"\"Build Mask R-CNN architecture. input_shape: The shape of the input image. mode: Either \"training\" or \"inference\". The inputs and outputs of the model differ accordingly. \"\"\" assert mode in ['training', 'inference'] # Image size must be dividable by 2 multiple times h, w = config.IMAGE_SHAPE[:2] # [1024 1024 3] if h / 2**6 != int(h / 2**6) or w / 2**6 != int(w / 2**6): # 这里就限定了下采样不会产生坐标误差 raise Exception(\"Image size must be dividable by 2 at least 6 times \" \"to avoid fractions when downscaling and upscaling.\" \"For example, use 256, 320, 384, 448, 512, ... etc. \") # Inputs input_image = KL.Input( shape=[None, None, config.IMAGE_SHAPE[2]], name=\"input_image\") input_image_meta = KL.Input(shape=[config.IMAGE_META_SIZE], name=\"input_image_meta\") if mode == \"training\": # RPN GT input_rpn_match = KL.Input( shape=[None, 1], name=\"input_rpn_match\", dtype=tf.int32) input_rpn_bbox = KL.Input( shape=[None, 4], name=\"input_rpn_bbox\", dtype=tf.float32) # Detection GT (class IDs, bounding boxes, and masks) # 1. GT Class IDs (zero padded) input_gt_class_ids = KL.Input( shape=[None], name=\"input_gt_class_ids\", dtype=tf.int32) # 2. GT Boxes in pixels (zero padded) # [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)] in image coordinates input_gt_boxes = KL.Input( shape=[None, 4], name=\"input_gt_boxes\", dtype=tf.float32) # Normalize coordinates gt_boxes = KL.Lambda(lambda x: norm_boxes_graph( x, K.shape(input_image)[1:3]))(input_gt_boxes) # 3. GT Masks (zero padded) # [batch, height, width, MAX_GT_INSTANCES] if config.USE_MINI_MASK: input_gt_masks = KL.Input( shape=[config.MINI_MASK_SHAPE[0], config.MINI_MASK_SHAPE[1], None], name=\"input_gt_masks\", dtype=bool) else: input_gt_masks = KL.Input( shape=[config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1], None], name=\"input_gt_masks\", dtype=bool) elif mode == \"inference\": # Anchors in normalized coordinates input_anchors = KL.Input(shape=[None, 4], name=\"input_anchors\") # Build the shared convolutional layers. # Bottom-up Layers # Returns a list of the last layers of each stage, 5 in total. # Don't create the thead (stage 5), so we pick the 4th item in the list. if callable(config.BACKBONE): _, C2, C3, C4, C5 = config.BACKBONE(input_image, stage5=True, train_bn=config.TRAIN_BN) else: _, C2, C3, C4, C5 = resnet_graph(input_image, config.BACKBONE, stage5=True, train_bn=config.TRAIN_BN) # Top-down Layers # TODO: add assert to varify feature map sizes match what's in config P5 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c5p5')(C5) # 256 P4 = KL.Add(name=\"fpn_p4add\")([ KL.UpSampling2D(size=(2, 2), name=\"fpn_p5upsampled\")(P5), KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c4p4')(C4)]) P3 = KL.Add(name=\"fpn_p3add\")([ KL.UpSampling2D(size=(2, 2), name=\"fpn_p4upsampled\")(P4), KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c3p3')(C3)]) P2 = KL.Add(name=\"fpn_p2add\")([ KL.UpSampling2D(size=(2, 2), name=\"fpn_p3upsampled\")(P3), KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c2p2')(C2)]) # Attach 3x3 conv to all P layers to get the final feature maps. P2 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=\"SAME\", name=\"fpn_p2\")(P2) P3 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=\"SAME\", name=\"fpn_p3\")(P3) P4 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=\"SAME\", name=\"fpn_p4\")(P4) P5 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=\"SAME\", name=\"fpn_p5\")(P5) # P6 is used for the 5th anchor scale in RPN. Generated by # subsampling from P5 with stride of 2. P6 = KL.MaxPooling2D(pool_size=(1, 1), strides=2, name=\"fpn_p6\")(P5) # Note that P6 is used in RPN, but not in the classifier heads. rpn_feature_maps = [P2, P3, P4, P5, P6] mrcnn_feature_maps = [P2, P3, P4, P5] # Anchors if mode == \"training\": anchors = self.get_anchors(config.IMAGE_SHAPE) # Duplicate across the batch dimension because Keras requires it # TODO: can this be optimized to avoid duplicating the anchors? anchors = np.broadcast_to(anchors, (config.BATCH_SIZE,) + anchors.shape) # A hack to get around Keras's bad support for constants anchors = KL.Lambda(lambda x: tf.Variable(anchors), name=\"anchors\")(input_image) else: anchors = input_anchors # RPN Model, 返回的是keras的Module对象, 注意keras中的Module对象是可call的 rpn = build_rpn_model(config.RPN_ANCHOR_STRIDE, # 1 3 256 len(config.RPN_ANCHOR_RATIOS), config.TOP_DOWN_PYRAMID_SIZE) # Loop through pyramid layers layer_outputs = [] # list of lists for p in rpn_feature_maps: layer_outputs.append(rpn([p])) # 保存各pyramid特征经过RPN之后的结果 # Concatenate layer outputs # Convert from list of lists of level outputs to list of lists # of outputs across levels. # e.g. [[a1, b1, c1], [a2, b2, c2]] => [[a1, a2], [b1, b2], [c1, c2]] output_names = [\"rpn_class_logits\", \"rpn_class\", \"rpn_bbox\"] outputs = list(zip(*layer_outputs)) # [[logits2,……6], [class2,……6], [bbox2,……6]] outputs = [KL.Concatenate(axis=1, name=n)(list(o)) for o, n in zip(outputs, output_names)] # [batch, num_anchors, 2/4] # 其中num_anchors指的是全部特征层上的anchors总数 rpn_class_logits, rpn_class, rpn_bbox = outputs # Generate proposals # Proposals are [batch, N, (y1, x1, y2, x2)] in normalized coordinates # and zero padded. # POST_NMS_ROIS_INFERENCE = 1000 # POST_NMS_ROIS_TRAINING = 2000 proposal_count = config.POST_NMS_ROIS_TRAINING if mode == \"training\"\\ else config.POST_NMS_ROIS_INFERENCE # [IMAGES_PER_GPU, num_rois, (y1, x1, y2, x2)] # IMAGES_PER_GPU取代了batch，之后说的batch都是IMAGES_PER_GPU rpn_rois = ProposalLayer( proposal_count=proposal_count, nms_threshold=config.RPN_NMS_THRESHOLD, # 0.7 name=\"ROI\", config=config)([rpn_class, rpn_bbox, anchors]) if mode == \"training\": # Class ID mask to mark class IDs supported by the dataset the image # came from. active_class_ids = KL.Lambda( lambda x: parse_image_meta_graph(x)[\"active_class_ids\"] )(input_image_meta) if not config.USE_RPN_ROIS: # Ignore predicted ROIs and use ROIs provided as an input. input_rois = KL.Input(shape=[config.POST_NMS_ROIS_TRAINING, 4], name=\"input_roi\", dtype=np.int32) # Normalize coordinates target_rois = KL.Lambda(lambda x: norm_boxes_graph( x, K.shape(input_image)[1:3]))(input_rois) else: target_rois = rpn_rois # Generate detection targets # Subsamples proposals and generates target outputs for training # Note that proposal class IDs, gt_boxes, and gt_masks are zero # padded. Equally, returned rois and targets are zero padded. rois, target_class_ids, target_bbox, target_mask =\\ DetectionTargetLayer(config, name=\"proposal_targets\")([ target_rois, input_gt_class_ids, gt_boxes, input_gt_masks]) # Network Heads # TODO: verify that this handles zero padded ROIs mrcnn_class_logits, mrcnn_class, mrcnn_bbox =\\ fpn_classifier_graph(rois, mrcnn_feature_maps, input_image_meta, config.POOL_SIZE, config.NUM_CLASSES, train_bn=config.TRAIN_BN, fc_layers_size=config.FPN_CLASSIF_FC_LAYERS_SIZE) mrcnn_mask = build_fpn_mask_graph(rois, mrcnn_feature_maps, input_image_meta, config.MASK_POOL_SIZE, config.NUM_CLASSES, train_bn=config.TRAIN_BN) # TODO: clean up (use tf.identify if necessary) output_rois = KL.Lambda(lambda x: x * 1, name=\"output_rois\")(rois) # Losses rpn_class_loss = KL.Lambda(lambda x: rpn_class_loss_graph(*x), name=\"rpn_class_loss\")( [input_rpn_match, rpn_class_logits]) rpn_bbox_loss = KL.Lambda(lambda x: rpn_bbox_loss_graph(config, *x), name=\"rpn_bbox_loss\")( [input_rpn_bbox, input_rpn_match, rpn_bbox]) class_loss = KL.Lambda(lambda x: mrcnn_class_loss_graph(*x), name=\"mrcnn_class_loss\")( [target_class_ids, mrcnn_class_logits, active_class_ids]) bbox_loss = KL.Lambda(lambda x: mrcnn_bbox_loss_graph(*x), name=\"mrcnn_bbox_loss\")( [target_bbox, target_class_ids, mrcnn_bbox]) mask_loss = KL.Lambda(lambda x: mrcnn_mask_loss_graph(*x), name=\"mrcnn_mask_loss\")( [target_mask, target_class_ids, mrcnn_mask]) # Model inputs = [input_image, input_image_meta, input_rpn_match, input_rpn_bbox, input_gt_class_ids, input_gt_boxes, input_gt_masks] if not config.USE_RPN_ROIS: inputs.append(input_rois) outputs = [rpn_class_logits, rpn_class, rpn_bbox, mrcnn_class_logits, mrcnn_class, mrcnn_bbox, mrcnn_mask, rpn_rois, output_rois, rpn_class_loss, rpn_bbox_loss, class_loss, bbox_loss, mask_loss] model = KM.Model(inputs, outputs, name='mask_rcnn') else: # Network Heads # Proposal classifier and BBox regressor heads # output shapes: # mrcnn_class_logits: [batch, num_rois, NUM_CLASSES] classifier logits (before softmax) # mrcnn_class: [batch, num_rois, NUM_CLASSES] classifier probabilities # mrcnn_bbox(deltas): [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))] mrcnn_class_logits, mrcnn_class, mrcnn_bbox =\\ fpn_classifier_graph(rpn_rois, mrcnn_feature_maps, input_image_meta, config.POOL_SIZE, config.NUM_CLASSES, train_bn=config.TRAIN_BN, fc_layers_size=config.FPN_CLASSIF_FC_LAYERS_SIZE) # Detections # output is [batch, num_detections, (y1, x1, y2, x2, class_id, score)] in # normalized coordinates detections = DetectionLayer(config, name=\"mrcnn_detection\")( [rpn_rois, mrcnn_class, mrcnn_bbox, input_image_meta]) # Create masks for detections detection_boxes = KL.Lambda(lambda x: x[..., :4])(detections) mrcnn_mask = build_fpn_mask_graph(detection_boxes, mrcnn_feature_maps, input_image_meta, config.MASK_POOL_SIZE, config.NUM_CLASSES, train_bn=config.TRAIN_BN) model = KM.Model([input_image, input_image_meta, input_anchors], [detections, mrcnn_class, mrcnn_bbox, mrcnn_mask, rpn_rois, rpn_class, rpn_bbox], name='mask_rcnn') # Add multi-GPU support. if config.GPU_COUNT > 1: from mrcnn.parallel_model import ParallelModel model = ParallelModel(model, config.GPU_COUNT) return model"}
{"content2":"计算机视觉算法听起来似乎很高、大、上，以至于我们很难动手实现，然而事实真的如此吗？下面笔者将最近关于计算机视觉算法的一点认识分享出来，敬请大家批评指正。以文本分类问题为例，文本分类追求的目标就是将一篇特定的文本归到某个已有的类比当中（例如将有关NBA球星科比的报道，归到体育）。这个已有的类别可以是人工设定的，也可以是机器自动学习的。在初次接触到上述问题时，我感到大脑一篇空白，进而开始懊恼自己的无知，然后沮丧、抑郁等情绪接踵而至。但在调研了一些书籍、论文和代码之后，笔者发现问题似乎也没有原本认为的那么难，之所以这麽说是因为自己有了下面一些认识。细想起来，文本分类问题可以分解为以下几个字问题：如何收集文本、如何确定文本分类的模型、如何评价文本分类模型的好坏、如何将分类的结果很好的展示，分别对应的数据、模型、评价、显示。总结：每一个子问题都对应着一系列的函数实现，应该清楚的是，每个子问题之间是通过数据进行信息传递的，例如数据是为模型提供原材料的（数据应该加工为模型能够理解的形式）。"}
{"content2":"计算机视觉、模式识别方面稍微容易中的期刊分类： 杂谈(1)pattern recognition letters, 从投稿到发表，一年半时间(2)Pattern recognition 不好中，时间长(3)IEICE Transactions on Information and Systems， 作者中有一个必须是会员。收费高，审稿快。影响因子0.4(4)International Journal of Pattern Recognition and Artificial Intelligence ， 审稿周期一般6--12周，影响因子偏低，容易中。(5)Computational Intelligence， 中等偏上，要求较高，杂志级别不错，关注人数偏少，比较冷门(6)information processing letters, 影响因子低0.5左右，接搞量大，容易发表，审稿周期一般3--6个月。(7)Computer vision and image understanding, 9个月审稿期，平均投稿命中率20%，业内比较认可(8)journal of visual communication and image representation， 投稿容易，审稿周期一年以上(9)Signal processing letters, 影响因子0.99， 美国，审稿一个多月，(10)International Journal on Graphics, Vision and Image Processing (GVIP),(11)IET Image Processing, 影响因子0.758， EI Compendex ，审稿周期一年以上(12)IET Computer Vision ，影响因子0.969，(13)SIAM Journal on Imaging Sciences，(14)International Journal of Pattern Recognition and Artificial Intelligence (IJPRAI)，影响因子0.5， EI compendex, sci, 审稿时间超长，一两年(15)IEEE Signal Processing Letters， 审稿4---8周左右，影响因子不高，容易中，关注人不多(16)Journal of Logic and Computation, 影响因子，0.789，SCI检索(17)IEICE Transactions on Information and Systems 审稿时间2--4周，容易中，影响因子小，相对冷门，关注人数不多。(18)COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING，影响因子偏低，但仍然需要一定水平才可以投，审稿2--4周，SCI,EI检索(19)Signal Processing: Image Communication，容易中，审稿周期半年到一年(20)International Journal of Computer Vision， 较难，审稿周期半年到一年，EI,SCI检索(21)Journal of Mathematical Imaging and Vision，审稿半年到一年，影响因子不高，不容易中，稍微有些冷门。(22)Machine Vision and Applications， 影响因子偏低，但是接稿量不是很大，审稿周期一年以上，但容易发表，SCI,EI检索(23)Pattern Analysis & Applications， 影响因子不高，影响力也比较小，审稿时间一年以上，但容易投中。(24)Signal Image and Video Processing， 容易中，审稿时间半年到一年，EI检索(25)Pattern recognition and image analysis， EI检索，(26)Journal of digital imaging ，审稿周期半年到一年，影响因子偏低，容易中，很少有人关注(27)International journal of pattern recognition and artificial intelligence，影响因子偏低，容易中，关注人比较少。审稿周期半年到一年。(28)International journal of imaging systems and technology，影响因子偏低，容易中，审稿周期半年到一年。(29)journal of vlsi signal processing systems for signal image and video ，影响因子偏低，容易中，审稿周期一年以上，关注人比较少。目前比较好中的SCI期刊1.Pattern Recognition LettersISSN:0167-8655，1983年创刊，全年16期，Elsevier Science出版社，SCI、EI收录期刊，SCI2003年影响因子0.809。2.Pattern RecognitionISSN:0031-3203，1969年创刊，全年12期，Elsevier Science出版社，SCI、EI收录期刊，SCI影响因子1.611。3.Telematics and InformaticsISSN:0736-5853，1984年创刊，全年4期，ElsevierScience出版社，EI收录期刊，2003年EI收录21篇。刊载远距通信和信息科学在商业、工业、政府、教育等领域应用的研究论文与评论，涉及电子学、计算机图像处理、语言合成、声音识别、卫星电视、人工智能等方面的技术问题。4.Computational GeometryISSN: 0925-7721, 1991年创刊，全年9期，Elsevier Science 出版社出版，SCI、EI收录期刊，SCI 2002年影响因子1.000，2003年EI收录60篇。刊载计算几何学理论与应用，包括几何算法设计与分析，计算机图形学基本问题、模式识别、机器人学、图像处理、CAD-CAM、超大规模集成电路设计、地理信息系统中应用方面的论文和简报。5.Discrete and Computational GeometryISSN:0179-5376，1986年创刊，全年8期，Springer-Verlag出版社，SCI收录期刊，影响因子0.553。 数学与计算机科学国际期刊，覆盖了与几何学基础应用的广泛研究领域，刊载组合几何学、几何算法设计与分析的研究论文，涉及凸多胞形、极值几何问题、计算拓 扑学、数的几何学以及图论、数学规划、组合优化、图像处理、模式识别、结晶学、超大规模集成电路设计、机器人和计算机图学等。6.INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE影响因子：0.53BimonthlyISSN: 0218-0014WORLD SCIENTIFIC PUBL CO PTE LTD, 5 TOH TUCK LINK, SINGAPORE, SINGAPORE, 5962247.PATTERN ANALYSIS AND APPLICATIONS影响因子：0.78QuarterlyISSN: 1433-7541SPRINGER, 233 SPRING STREET, NEW YORK, USA, NY, 10013"}
{"content2":"计算机科学的论文最大特点在于：极度重视会议，而期刊则通常只用来做re-publication，也就是说很多期刊文章是会议论文的扩展版，而不是首发的工作。并且期刊的录用到发表中间的等待时间极长，有的甚至需要等上1-2年，因此即使投稿时是最新的工作，等发表的时候也不一定是最新了！（伤不起啊··╮(╯_╰)╭）也正因为如此，很多计算机期刊的影响因子都低的惊人，很多顶级刊物也只有1到2的印象因子！因此，要讨论计算机科学的publication，那么请忽视影响因子。注1：下面罗列的是在手上材料的基础上整理的，仅代表我自己的看法，必然有很多遗漏，有待补充与修改。注2：这些会议期刊都是最好的，对于初学者来说他们提供了大量的知识与内容，可以挑一些相关的文章通读与精读；对于Researcher来说，这些是证明自己的一种方式。人工智能和机器学习（AI & ML）[会议]NIPS: Neural Information Processing SystemsICML: International Conference on Machine Learning // NIPS和ICML对我来说很有难度，数学功底不够，但是确实可以提供很多很有价值的想法。IJCAI: International Joint Conference on Artificial IntelligenceAAAI: American Association for Artificial Intelligence //IJCAI和AAAI惯例是轮流开，页数比较短（6页），非常值得一试还有UAI[期刊]PAMINEURAL COMPUTATIONARTIFICIAL INTELLIGENCETKDE: IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERINGPR: PATTERN RECOGNITION......//还有很多，不一一列举，国内很重视期刊。信息检索和数据挖掘 （IR & DM）ACM SIGIR: ACM Special Interest Group on Information Retrieval //信息检索最顶级的会议，2011年第一次在中国召开，我也参加了该会议，这一届华人文章数量锐增。WWW: International World Wide Web Conference //顶级会议，内容和web相关，但是方法涉及机器学习、信息检索、数据挖掘等。我喜欢的会议KDD: ACM SIGKDD Conference on Knowledge Discovery and Data Mining //数据挖掘最顶级的会议//实际上这些会议的内容非常交叉，当然，任何一个会议都很难中，你需要good idea， good wrighting， good luck。大家一起加油吧！数据库 （Database）[会议]ACM SIGMODVLDB:International Conference on Very Large Data BasesICDE:International Conference on Data Engineering//这三个会议并称为数据库方向的三大顶级会议图像与视觉 （Graphic & Computer Vision)[会议]ICCV: International Conference on Computer VisionCVPR: International Conference on Computer Vision and Pattern Recognition还有ECCV, ICIP, ICPR[期刊]IJCV: International Journal on Computer VisionTIP: IEEE Transaction on Image Processing其他涉及操作系统OS，软件工程SE，安全Security的会议期刊我不是很熟，暂时不列了，可以补充。"}
{"content2":"VOC2007 与 VOC2012   此数据集可以用于图像分类,目标检测,图像分割!!!数据集下载镜像网站: http://pjreddie.com/projects/pascal-voc-dataset-mirror/VOC2012: Train/Validation Data(1.9GB),Test Data(1.8GB),主页: http://host.robots.ox.ac.uk:8080/pascal/VOC/voc2012/VOC2007: Train/Validation Data(439MB),Test Data(431MB),主页: http://host.robots.ox.ac.uk:8080/pascal/VOC/voc2007/MNIST手写体数据集(用作10类图像分类)包含了60,000张28x28的二值(手写数字的)训练图像,10,000张28x28的二值(手写数字的)测试图像.用作分类任务,可以分成0-9这10个类别!引用:Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. \"Gradient-based learning applied to document recognition.\" Proceedings of the IEEE, 86(11):2278-2324, November 1998.CIFAR-10(用作10类图像分类)此数据集包含了60,000张32x32的RGB图像,总共有10类图像,大约6000张图像/类,50,000张做训练,10,000张做测试!此数据集有三个版本的数据可供下载: Python版本(163MB), MATLAB版本(175MB), 二值版本(162MB)!CIFAR-100(用作100类图像分类)这个数据集和CIFAR-10相比,它具有100个类,大约600张/类,每类500张训练,500张测试.这100类又可以grouped成20个大类.此数据集也有三个版本的数据可供下载: Python版本(161MB), MATLAB版本(175MB), 二值版本(161MB)!引用: Learning Multiple Layers of Features from Tiny Images, Alex Krizhevsky, 2009CIFAR-10和CIFAR-100都是80 million tiny images dataset的子集!80 million tiny images dataset这个数据集包含了79,302,017张32x32的RGB图像,下载时包含了5个文件,网站上也提供了示例代码教你如何加载这些数据!1. Image binary (227GB)2. Metadata binary (57GB)3. Gist binary (114GB)4. Index data (7MB)5. Matlab Tiny Images toolbox (150kB)Caltech_101(用作101类图像分类)这个数据集包含了101类的图像,每类大约有40~800张图像,大部分是50张/类,在2003年由lifeifei收集,每张图像的大小大约是300x200.数据集下载: 101_ObjectCategories.tar.gz(131MB)Caltech_256(用作256类图像分类)此数据集和Caltech_101相似,包含了30,607张图像,数据集下载: 256_ObjectCategroies.tar(1.2GB)ImagenetIMAGENET Large Scale Visual Recognition Challenge(ILSVRC)从2010年开始,每年举办的ILSVRC图像分类和目标检测大赛,数据集下载: http://image-net.org/download-imagesDetectionPASCAL VOC 2009 datasetClassification/Detection Competitions, Segmentation Competition, Person Layout Taster Competition datasetsLabelMe datasetLabelMe is a web-based image annotation tool that allows researchers to label images and share the annotations with the rest of the community. If you use the database, we only ask that you contribute to it, from time to time, by using the labeling tool.BioID Face Detection Database1521 images with human faces, recorded under natural conditions, i.e. varying illumination and complex background. The eye positions have been set manually.CMU/VASC & PIE Face datasetYale Face datasetCaltechCars, Motorcycles, Airplanes, Faces, Leaves, BackgroundsCaltech 101Pictures of objects belonging to 101 categoriesCaltech 256Pictures of objects belonging to 256 categoriesDaimler Pedestrian Detection Benchmark15,560 pedestrian and non-pedestrian samples (image cut-outs) and 6744 additional full images not containing pedestrians for bootstrapping. The test set contains more than 21,790 images with 56,492 pedestrian labels (fully visible or partially occluded), captured from a vehicle in urban traffic.MIT Pedestrian datasetCVC Pedestrian DatasetsCVC Pedestrian DatasetsCBCL Pedestrian DatabaseMIT Face datasetCBCL Face DatabaseMIT Car datasetCBCL Car DatabaseMIT Street datasetCBCL Street DatabaseINRIA Person Data SetA large set of marked up images of standing or walking peopleINRIA car datasetA set of car and non-car images taken in a parking lot nearby INRIAINRIA horse datasetA set of horse and non-horse imagesH3D Dataset3D skeletons and segmented regions for 1000 people in imagesHRI RoadTraffic datasetA large-scale vehicle detection datasetBelgaLogos10000 images of natural scenes, with 37 different logos, and 2695 logos instances, annotated with a bounding box.FlickrBelgaLogos10000 images of natural scenes grabbed on Flickr, with 2695 logos instances cut and pasted from the BelgaLogos dataset.FlickrLogos-32The dataset FlickrLogos-32 contains photos depicting logos and is meant for the evaluation of multi-class logo detection/recognition as well as logo retrieval methods on real-world images. It consists of 8240 images downloaded from Flickr.TME Motorway Dataset30000+ frames with vehicle rear annotation and classification (car and trucks) on motorway/highway sequences. Annotation semi-automatically generated using laser-scanner data. Distance estimation and consistent target ID over time available.PHOS (Color Image Database for illumination invariant feature selection)Phos is a color image database of 15 scenes captured under different illumination conditions. More particularly, every scene of the database contains 15 different images: 9 images captured under various strengths of uniform illumination, and 6 images under different degrees of non-uniform illumination. The images contain objects of different shape, color and texture and can be used for illumination invariant feature detection and selection.CaliforniaND: An Annotated Dataset For Near-Duplicate Detection In Personal Photo CollectionsCalifornia-ND contains 701 photos taken directly from a real user's personal photo collection, including many challenging non-identical near-duplicate cases, without the use of artificial image transformations. The dataset is annotated by 10 different subjects, including the photographer, regarding near duplicates.USPTO Algorithm Challenge, Detecting Figures and Part Labels in PatentsContains drawing pages from US patents with manually labeled figure and part labels.Abnormal Objects DatasetContains 6 object categories similar to object categories in Pascal VOC that are suitable for studying the abnormalities stemming from objects.Human detection and tracking using RGB-D cameraCollected in a clothing store. Captured with Kinect (640*480, about 30fps)Multi-Task Facial Landmark (MTFL) datasetThis dataset contains 12,995 face images collected from the Internet. The images are annotated with (1) five facial landmarks, (2) attributes of gender, smiling, wearing glasses, and head pose.ClassificationPASCAL VOC 2009 datasetClassification/Detection Competitions, Segmentation Competition, Person Layout Taster Competition datasetsCaltechCars, Motorcycles, Airplanes, Faces, Leaves, BackgroundsCaltech 101Pictures of objects belonging to 101 categoriesCaltech 256Pictures of objects belonging to 256 categoriesETHZ Shape ClassesA dataset for testing object class detection algorithms. It contains 255 test images and features five diverse shape-based classes (apple logos, bottles, giraffes, mugs, and swans).Flower classification data sets17 Flower Category DatasetAnimals with attributesA dataset for Attribute Based Classification. It consists of 30475 images of 50 animals classes with six pre-extracted feature representations for each image.Stanford Dogs DatasetDataset of 20,580 images of 120 dog breeds with bounding-box annotation, for fine-grained image categorization.Video classification USAA datasetThe USAA dataset includes 8 different semantic class videos which are home videos of social occassions which feature activities of group of people. It contains around 100 videos for training and testing respectively. Each video is labeled by 69 attributes. The 69 attributes can be broken down into five broad classes: actions, objects, scenes, sounds, and camera movement.McGill Real-World Face Video DatabaseThis database contains 18000 video frames of 640x480 resolution from 60 video sequences, each of which recorded from a different subject (31 female and 29 male).RecognitionFace and Gesture Recognition Working Group FGnetFace and Gesture Recognition Working Group FGnetFeretFace and Gesture Recognition Working Group FGnetPUT face9971 images of 100 peopleLabeled Faces in the WildA database of face photographs designed for studying the problem of unconstrained face recognitionUrban scene recognitionTraffic Lights Recognition, Lara's public benchmarks.PubFig: Public Figures Face DatabaseThe PubFig database is a large, real-world face dataset consisting of 58,797 images of 200 people collected from the internet. Unlike most other existing face datasets, these images are taken in completely uncontrolled situations with non-cooperative subjects.YouTube FacesThe data set contains 3,425 videos of 1,595 different people. The shortest clip duration is 48 frames, the longest clip is 6,070 frames, and the average length of a video clip is 181.3 frames.MSRC-12: Kinect gesture data setThe Microsoft Research Cambridge-12 Kinect gesture data set consists of sequences of human movements, represented as body-part locations, and the associated gesture to be recognized by the system.QMUL underGround Re-IDentification (GRID) DatasetThis dataset contains 250 pedestrian image pairs + 775 additional images captured in a busy underground station for the research on person re-identification.Person identification in TV seriesFace tracks, features and shot boundaries from our latest CVPR 2013 paper. It is obtained from 6 episodes of Buffy the Vampire Slayer and 6 episodes of Big Bang Theory.ChokePoint DatasetChokePoint is a video dataset designed for experiments in person identification/verification under real-world surveillance conditions. The dataset consists of 25 subjects (19 male and 6 female) in portal 1 and 29 subjects (23 male and 6 female) in portal 2.Hieroglyph DatasetAncient Egyptian Hieroglyph Dataset.Rijksmuseum Challenge Dataset: Visual Recognition for Art DatasetOver 110,000 photographic reproductions of the artworks exhibited in the Rijksmuseum (Amsterdam, the Netherlands). Offers four automatic visual recognition challenges consisting of predicting the artist, type, material and creation year. Includes a set of baseline features, and offer a baseline based on state-of-the-art image features encoded with the Fisher vector.The OU-ISIR Gait Database, Treadmill DatasetTreadmill gait datasets composed of 34 subjects with 9 speed variations, 68 subjects with 68 subjects, and 185 subjects with various degrees of gait fluctuations.The OU-ISIR Gait Database, Large Population DatasetLarge population gait datasets composed of 4,016 subjects.Pedestrian Attribute Recognition At Far DistanceLarge-scale PEdesTrian Attribute (PETA) dataset, covering more than 60 attributes (e.g. gender, age range, hair style, casual/formal) on 19000 images.FaceScrub Face DatasetThe FaceScrub dataset is a real-world face dataset comprising 107,818 face images of 530 male and female celebrities detected in images retrieved from the Internet. The images are taken under real-world situations (uncontrolled conditions). Name and gender annotations of the faces are included.TrackingBIWI Walking Pedestrians datasetWalking pedestrians in busy scenarios from a bird eye view\"Central\" Pedestrian Crossing SequencesThree pedestrian crossing sequencesPedestrian Mobile Scene AnalysisThe set was recorded in Zurich, using a pair of cameras mounted on a mobile platform. It contains 12'298 annotated pedestrians in roughly 2'000 frames.Head trackingBMP image sequences.KIT AIS DatasetData sets for tracking vehicles and people in aerial image sequences.MIT Traffic Data SetMIT traffic data set is for research on activity analysis and crowded scenes. It includes a traffic video sequence of 90 minutes long. It is recorded by a stationary camera.Shinpuhkan 2014 dataset: Multi-Camera Pedestrian Dataset for Tracking People across Multiple CamerasThis dataset consists of more than 22,000 images of 24 people which are captured by 16 cameras installed in a shopping mall \"Shinpuh-kan\". All images are manually cropped and resized to 48x128 pixels, grouped into tracklets and added annotation.ATC shopping center dataasetThe tracking environment consists of multiple 3D range sensors, covering an area of about 900 m2, in the \"ATC\" shopping center in Osaka, Japan.Human detection and tracking using RGB-D cameraCollected in a clothing store. Captured with Kinect (640*480, about 30fps)Multiple Camera TrackingHallway Corridor - Multiple Camera Tracking: An indoor camera network dataset with 6 cameras (contains ground plane homography).Multiple Object Tracking BenchmarkA centralized benchmark for multi-object tracking.SegmentationImage Segmentation with A Bounding Box Prior datasetGround truth database of 50 images with: Data, Segmentation, Labelling - Lasso, Labelling - RectanglePASCAL VOC 2009 datasetClassification/Detection Competitions, Segmentation Competition, Person Layout Taster Competition datasetsMotion Segmentation and OBJCUT dataCows for object segmentation, Five video sequences for motion segmentationGeometric Context DatasetGeometric Context Dataset: pixel labels for seven geometric classes for 300 imagesCrowd Segmentation DatasetThis dataset contains videos of crowds and other high density moving objects. The videos are collected mainly from the BBC Motion Gallery and Getty Images website. The videos are shared only for the research purposes. Please consult the terms and conditions of use of these videos from the respective websites.CMU-Cornell iCoseg DatasetContains hand-labelled pixel annotations for 38 groups of images, each group containing a common foreground. Approximately 17 images per group, 643 images total.Segmentation evaluation database200 gray level images along with ground truth segmentationsThe Berkeley Segmentation Dataset and BenchmarkImage segmentation and boundary detection. Grayscale and color segmentations for 300 images, the images are divided into a training set of 200 images, and a test set of 100 images.Weizmann horses328 side-view color images of horses that were manually segmented. The images were randomly collected from the WWW.Saliency-based video segmentation with sequentially updated priors10 videos as inputs, and segmented image sequences as ground-truthDaimler Urban Segmentation DatasetThe dataset consists of video sequences recorded in urban traffic. The dataset consists of 5000 rectified stereo image pairs. 500 frames come with pixel-level semantic class annotations into 5 classes: ground, building, vehicle, pedestrian, sky. Dense disparity maps are provided as a reference.Foreground/BackgroundWallflower DatasetFor evaluating background modelling algorithmsForeground/Background Microsoft Cambridge DatasetForeground/Background segmentation and Stereo dataset from Microsoft CambridgeStuttgart Artificial Background Subtraction DatasetThe SABS (Stuttgart Artificial Background Subtraction) dataset is an artificial dataset for pixel-wise evaluation of background models.Saliency Detection (source)AIM120 Images / 20 Observers (Neil D. B. Bruce and John K. Tsotsos 2005).LeMeur27 Images / 40 Observers (O. Le Meur, P. Le Callet, D. Barba and D. Thoreau 2006).Kootstra100 Images / 31 Observers (Kootstra, G., Nederveen, A. and de Boer, B. 2008).DOVES101 Images / 29 Observers (van der Linde, I., Rajashekar, U., Bovik, A.C., Cormack, L.K. 2009).Ehinger912 Images / 14 Observers (Krista A. Ehinger, Barbara Hidalgo-Sotelo, Antonio Torralba and Aude Oliva 2009).NUSEF758 Images / 75 Observers (R. Subramanian, H. Katti, N. Sebe1, M. Kankanhalli and T-S. Chua 2010).JianLi235 Images / 19 Observers (Jian Li, Martin D. Levine, Xiangjing An and Hangen He 2011).Extended Complex Scene Saliency Dataset (ECSSD)ECSSD contains 1000 natural images with complex foreground or background. For each image, the ground truth mask of salient object(s) is provided.Video SurveillanceCAVIARFor the CAVIAR project a number of video clips were recorded acting out the different scenarios of interest. These include people walking alone, meeting with others, window shopping, entering and exitting shops, fighting and passing out and last, but not least, leaving a package in a public place.ViSORViSOR contains a large set of multimedia data and the corresponding annotations.CUHK Crowd Dataset474 video clips from 215 crowded scenes, with ground truth on group detection and video classes.?TImes Square Intersection (TISI) DatasetA busy outdoor dataset for research on visual surveillance.Educational Resource Centre (ERCe) DatasetAn indoor dataset collected from a university campus for physical event understanding of long video streams.Multiview3D Photography DatasetMultiview stereo data sets: a set of imagesMulti-view Visual Geometry group's data setDinosaur, Model House, Corridor, Aerial views, Valbonne Church, Raglan Castle, Kapel sequenceOxford reconstruction data set (building reconstruction)Oxford collegesMulti-View Stereo dataset (Vision Middlebury)Temple, DinoMulti-View Stereo for Community Photo CollectionsVenus de Milo, Duomo in Pisa, Notre Dame de ParisIS-3D DataDataset provided by Center for Machine PerceptionCVLab datasetCVLab dense multi-view stereo image database3D Objects on TurntableObjects viewed from 144 calibrated viewpoints under 3 different lighting conditionsObject Recognition in Probabilistic 3D ScenesImages from 19 sites collected from a helicopter flying around Providence, RI. USA. The imagery contains approximately a full circle around each site.Multiple cameras fall dataset24 scenarios recorded with 8 IP video cameras. The first 22 first scenarios contain a fall and confounding events, the last 2 ones contain only confounding events.CMP Extreme View Dataset15 wide baseline stereo image pairs with large viewpoint change, provided ground truth homographies.KTH Multiview Football Dataset IIThis dataset consists of 8000+ images of professional footballers during a match of the Allsvenskan league. It consists of two parts: one with ground truth pose in 2D and one with ground truth pose in both 2D and 3D.Disney Research light field datasetsThis dataset includes: camera calibration information, raw input images we have captured, radially undistorted, rectified, and cropped images, depth maps resulting from our reconstruction and propagation algorithm, depth maps computed at each available view by the reconstruction algorithm without the propagation applied.ActionUCF Sports Action DatasetThis dataset consists of a set of actions collected from various sports which are typically featured on broadcast television channels such as the BBC and ESPN. The video sequences were obtained from a wide range of stock footage websites including BBC Motion gallery, and GettyImages.UCF Aerial Action DatasetThis dataset features video sequences that were obtained using a R/C-controlled blimp equipped with an HD camera mounted on a gimbal.The collection represents a diverse pool of actions featured at different heights and aerial viewpoints. Multiple instances of each action were recorded at different flying altitudes which ranged from 400-450 feet and were performed by different actors.UCF YouTube Action DatasetIt contains 11 action categories collected from YouTube.Weizmann action recognitionWalk, Run, Jump, Gallop sideways, Bend, One-hand wave, Two-hands wave, Jump in place, Jumping Jack, Skip.UCF50UCF50 is an action recognition dataset with 50 action categories, consisting of realistic videos taken from YouTube.ASLANThe Action Similarity Labeling (ASLAN) Challenge.MSR Action Recognition DatasetsThe dataset was captured by a Kinect device. There are 12 dynamic American Sign Language (ASL) gestures, and 10 people. Each person performs each gesture 2-3 times.KTH Recognition of human actionsContains six types of human actions (walking, jogging, running, boxing, hand waving and hand clapping) performed several times by 25 subjects in four different scenarios: outdoors, outdoors with scale variation, outdoors with different clothes and indoors.Hollywood-2 Human Actions and Scenes datasetHollywood-2 datset contains 12 classes of human actions and 10 classes of scenes distributed over 3669 video clips and approximately 20.1 hours of video in total.Collective Activity DatasetThis dataset contains 5 different collective activities : crossing, walking, waiting, talking, and queueing and 44 short video sequences some of which were recorded by consumer hand-held digital camera with varying view point.Olympic Sports DatasetThe Olympic Sports Dataset contains YouTube videos of athletes practicing different sports.SDHA 2010Surveillance-type videosVIRAT Video DatasetThe dataset is designed to be realistic, natural and challenging for video surveillance domains in terms of its resolution, background clutter, diversity in scenes, and human activity/event categories than existing action recognition datasets.HMDB: A Large Video Database for Human Motion RecognitionCollected from various sources, mostly from movies, and a small proportion from public databases, YouTube and Google videos. The dataset contains 6849 clips divided into 51 action categories, each containing a minimum of 101 clips.Stanford 40 Actions DatasetDataset of 9,532 images of humans performing 40 different actions, annotated with bounding-boxes.50Salads datasetFully annotated dataset of RGB-D video data and data from accelerometers attached to kitchen objects capturing 25 people preparing two mixed salads each (4.5h of annotated data). Annotated activities correspond to steps in the recipe and include phase (pre-/ core-/ post) and the ingredient acted upon.Penn Sports ActionThe dataset contains 2326 video sequences of 15 different sport actions and human body joint annotations for all sequences.CVRR-HANDS 3DA Kinect dataset for hand detection in naturalistic driving settings as well as a challenging 19 dynamic hand gesture recognition dataset for human machine interfaces.TUM Kitchen Data SetObservations of several subjects setting a table in different ways. Contains videos, motion capture data, RFID tag readings,...TUM Breakfast Actions DatasetThis dataset comprises of 10 actions related to breakfast preparation, performed by 52 different individuals in 18 different kitchens.MPII Cooking Activities DatasetCooking Activities dataset.GTEA Gaze+ DatasetThis dataset consists of seven meal-preparation activities, each performed by 10 subjects. Subjects perform the activities based on the given cooking recipes.Human pose/ExpressionAFEW (Acted Facial Expressions In The Wild)/SFEW (Static Facial Expressions In The Wild)Dynamic temporal facial expressions data corpus consisting of close to real world environment extracted from movies.ETHZ CALVIN DatasetCALVIN research group datasetsHandNet (annotated depth images of articulating hands)This dataset includes 214971 annotated depth images of hands captured by a RealSense RGBD sensor of hand poses. Annotations: per pixel classes, 6D fingertip pose, heatmap. Images -> Train: 202198, Test: 10000, Validation: 2773. Recorded at GIP Lab, Technion.Image stitchingIPM Vision Group Image Stitching datasetsImages and parameters for registerationMedicalVIP Laparoscopic / Endoscopic DatasetCollection of endoscopic and laparoscopic (mono/stereo) videos and imagesMouse Embryo Tracking DatabaseDB Contains 100 examples with the uncompressed frames, up to the 10th frame after the appearance of the 8th cell; a text file with the trajectories of all the cells, from appearance to division; a movie file showing the trajectories of the cells.MiscZurich Buildings DatabaseZuBuD Image Database contains over 1005 images about Zurich city building.Color Name Data SetsMall datasetThe mall dataset was collected from a publicly accessible webcam for crowd counting and activity profiling research.QMUL Junction DatasetA busy traffic dataset for research on activity analysis and behaviour understanding.Miracl-VC1Miracl-VC1 is a lip-reading dataset including both depth and color images. Fifteen speakers positioned in the frustum of a MS Kinect sensor and utter ten times a set of ten words and ten phrases. VOC2007 与 VOC2012   此数据集可以用于图像分类,目标检测,图像分割!!!数据集下载镜像网站: http://pjreddie.com/projects/pascal-voc-dataset-mirror/VOC2012: Train/Validation Data(1.9GB),Test Data(1.8GB),主页: http://host.robots.ox.ac.uk:8080/pascal/VOC/voc2012/VOC2007: Train/Validation Data(439MB),Test Data(431MB),主页: http://host.robots.ox.ac.uk:8080/pascal/VOC/voc2007/MNIST手写体数据集(用作10类图像分类)包含了60,000张28x28的二值(手写数字的)训练图像,10,000张28x28的二值(手写数字的)测试图像.用作分类任务,可以分成0-9这10个类别!引用:Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. \"Gradient-based learning applied to document recognition.\" Proceedings of the IEEE, 86(11):2278-2324, November 1998.CIFAR-10(用作10类图像分类)此数据集包含了60,000张32x32的RGB图像,总共有10类图像,大约6000张图像/类,50,000张做训练,10,000张做测试!此数据集有三个版本的数据可供下载: Python版本(163MB), MATLAB版本(175MB), 二值版本(162MB)!CIFAR-100(用作100类图像分类)这个数据集和CIFAR-10相比,它具有100个类,大约600张/类,每类500张训练,500张测试.这100类又可以grouped成20个大类.此数据集也有三个版本的数据可供下载: Python版本(161MB), MATLAB版本(175MB), 二值版本(161MB)!引用: Learning Multiple Layers of Features from Tiny Images, Alex Krizhevsky, 2009CIFAR-10和CIFAR-100都是80 million tiny images dataset的子集!80 million tiny images dataset这个数据集包含了79,302,017张32x32的RGB图像,下载时包含了5个文件,网站上也提供了示例代码教你如何加载这些数据!1. Image binary (227GB)2. Metadata binary (57GB)3. Gist binary (114GB)4. Index data (7MB)5. Matlab Tiny Images toolbox (150kB)Caltech_101(用作101类图像分类)这个数据集包含了101类的图像,每类大约有40~800张图像,大部分是50张/类,在2003年由lifeifei收集,每张图像的大小大约是300x200.数据集下载: 101_ObjectCategories.tar.gz(131MB)Caltech_256(用作256类图像分类)此数据集和Caltech_101相似,包含了30,607张图像,数据集下载: 256_ObjectCategroies.tar(1.2GB)ImagenetIMAGENET Large Scale Visual Recognition Challenge(ILSVRC)从2010年开始,每年举办的ILSVRC图像分类和目标检测大赛,数据集下载: http://image-net.org/download-images"}
{"content2":"摘要： 计算机视觉是人工智能时代的一项重要科学，也是无人驾驶技术的核心。学习计算机视觉首先必须要知道计算机是如何识别图像的，本文主要简述了计算机如何‘看’图片，相对于视频文件来说，道理是一样的。更多深度文章，请关注：https://yq.aliyun.com/cloud计算机视觉是一门研究如何使机器‘看’的科学，我们都喜欢看美丽的图像，但是你有没有想过计算机是如何看这些图像的？接下来，我会详细介绍说明计算机如何处理图像的。看到上面的图像，一个正常的人可以很容易地知道，图像中有一只猫。但是，计算机可以真正看到猫吗？答案是否定的，计算机看到数字矩阵（0到255之间）。一般来说，我们可以将图像分类为灰度图像或彩色图像。首先，我们先讨论灰度图像然后再讨论彩色。上图是灰度图像，图像中每个像素都表示的是像素的亮度，下面我们就说一说这个亮度问题。了解更多关于像素。让我们先来看看上面图片中计算机是怎么看的。我已将上面的图像大小调整为18 * 18，以便于理解。与我们不同的是，计算机将图像看作2D矩阵。你可能听说有人说这幅画的大小是1800 * 700或1300 * 700，这个大小显示了一个图像的宽度和高度。换句话说，如果大小为1300 * 700，则水平方向为1300像素，垂直方向为700像素。这意味着总共有910000（1300 * 700）像素。如果图像的大小为700 * 500，那么矩阵的维数将为（700,500）。这里，矩阵中的每个元素（像素）表示该像素中的亮度强度。这里，0表示黑色，255表示白色，数字越小，越接近黑色（数字大小决定黑的程度）。彩色图像在灰度图像中，每个像素表示仅一种颜色的强度。换句话说，它有一个通道。而在彩色图像中，我们有3个通道RGB（红，绿，蓝）。标准数码相机都有3（RGB）通道。如上图所示，彩色图像由红色、绿色和蓝色三个通道组成。现在的问题是，计算机如何看待这个形象？同样，答案是他们看到矩阵。现在下一个问题应该是，我们要如何在矩阵中表示这个图像，因为它有3个通道，与我们只有一个通道的灰度图像不同。在这种情况下，我们利用3D矩阵来实现表示彩色图像。我们有一个通道的矩阵，但在这种情况下，我们将有三个矩阵堆叠在一起，这就是为什么它是3D。700 * 700彩色图像的尺寸将为（700,700,3）。假设第一个矩阵表示红色通道，则该矩阵的每个元素表示该像素中的红色强度，同样为绿色和蓝色。通常，彩色图像中的每个像素具有与其相关联的三个数字（0至255）。这些数字表示该特定像素中的红色、绿色和蓝色的强度。至于为什么是红绿蓝这三色，想必大家都知道色度学的最基本原理，即三基色原理。大多数颜色都可以通过三色按照不同的比例混合产生。结论计算机将图像看作矩阵。灰度图像具有一个通道（灰色），因此我们可以在2D矩阵中表示灰度图像，其中每个元素表示该特定像素中亮度的强度。记住，0表示黑色，255表示白色。灰度图像有一个通道，而彩色图像有三个通道RGB（红、绿、蓝）。我们可以在深度为3的3D矩阵中表示彩色图像。本文由阿里云云栖社区组织翻译。文章原标题《How do computers see an image ?》，作者：Savan Visalpara作者个人网站：https://savan77.github.io/，可以与作者交流。译者：袁虎，审阅：李烽 阿福文章为简译，更为详细的内容，请查看原文版权声明：本文内容由互联网用户自发贡献，本社区不拥有所有权，也不承担相关法律责任。如果您发现本社区中有涉嫌抄袭的内容，欢迎发送邮件至：yqgroup@service.aliyun.com 进行举报，并提供相关证据，一经查实，本社区将立刻删除涉嫌侵权内容。原文连接"}
{"content2":"人工智能综述:AI的发展概括：人工智能学科自从诞生之后, 技术理论不断发展, 应用领域不断延伸。应用领域主要包括智能机器人、图像处理、自然语言处理及语音识别等。人工智能的基础理论科学包括计算机科学、逻辑学、生物学、心理学及哲学等众多学科。从人工智能的发展历史、人工智能的技术核心以及人工智能的应用前景3个方面阐述人工智能的发展与应用。0 引言人工智能是集合了计算机科学、逻辑学、生物学、心理学和哲学等众多学科，在语音识别、图像处理、自然语言处理、自动定理证明及智能机器人等应用领域取得了显著成果。人工智能在社会发展中扮演着不可或缺的角色。人工智能在提升劳动效率、减低劳动成本、优化人力资源结构及创造新的工作岗位需求方面带来了革命性的成果。人工智能的出现为疲软的全球经济提供了新的动力，提升了全球GDP的增长速度。人工智能规模发展迅速，截至2018年，中国人工智能市场规模已达238.2亿元。人工智能的产生已经为人类创造出很大的经济效益，正在惠及生活的方方面面，无人驾驶、人工智能医疗及语音识别等，为人类的生活提供了便利。同时人工智能的出现，取代了很多传统岗位，同时也创造了很多新的岗位来消化社会劳动力。人工智能的出现极大地推动了社会发展，让社会发展步入新的时期。1 人工智能发展历史人工智能雏形的出现是在1955年，在一次“学习机器讨论会”上，著名的科学家艾伦·纽厄尔和奥利弗·塞弗里奇分别提出了下棋与计算机模式识别的研究。在次年的达特茅斯会议上，提出了“人工智能”一词，并讨论确定了人工智能最初的发展路线与发展目标。之后由阿瑟·塞缪尔提出了机器学习理论，根据这一理论编写完成了能够与人类进行对弈的西洋跳棋程序，并于1962年战胜了美国的西洋跳棋大师。20世纪70年代中叶符号学派走向低谷，以仿生学为基础的研究学派逐渐火热。神经网络由于BP算法的广泛应用获得了高速发展。在大环境下，专家系统的大量使用使工业界节约了大量成本，提升了产业效益。例如价值上亿的矿藏由PROSPECTOR专家系统成功地分析得出。在此之后，人们开始尝试研究具有通用性的人工智能程序，却遇到了严重的阻碍，陷入停滞。人工智能又一次步入低谷。1997年，“深蓝”的成功让人工智能的发展又提上日程。随着算力的增加，人工智能的瓶颈被打破，为基于大数据的深度学习与增强学习提供了发展的可能。GPU不断发展，与此同时定制化处理器的研制成功使算力不断提升，为人工智能的爆发提供了基础。在无人驾驶领域北京地平线信息技术有限公司，发布了一款嵌入式视觉芯片，主要针对无人驾驶汽车领域。阿里投资千亿成立“达摩院”，在机器学习等方面开展研究和进行产品开发。人工智能步入了快速发展期。如图 1 所示，人工智能自出现以来，经历了两次低谷三次浪潮，现在正处于人工智能的第三次浪潮，人工智能正在快速发展，为生产力的提升提供变革的动力。2 人工智能技术核心2.1 计算机视觉人们认识世界 91% 是通过视觉来实现。同样，计算机视觉的最终目标就是让计算机能够像人一样通过视觉来认识和了解世界，它主要是通过算法对图像进行识别分析，目前计算机视觉最广泛的应用是人脸识别和图像识别。2.1.1 图像分类传统图像分类的方法主要经过2个步骤: 特征提取和训练分类器。特征提取通用的方法主要包括2种，使用通用特征提取和使用自己设计的特征提取。例如在人脸识别中，使用HOG，LBP等通用特征进行检测。选定特征之后，使用传统的机器学习方法，例如adaboost等建模方法训练分类模型，然后选择效果最好的模型进行调参，最终生成人脸检测模型。自2015年之后，图像处理使用深度学习进行分类的方法得到了广泛应用。神经网络通过神经元构建成网络，通过激活函数使模型具有非线性拟合能力。只需要给模型设计好输入和输出，模型就能自动学习特征提取和训练分类器的过程。深度学习的使用让图像分类过程中最为费时费力的过程得以简化，提升了图像分类的效果和效率。VGG，ResNet(残差神经网络)，inception 这几种结构是工程中最常用的。工程上使用的模型必须要兼顾效率和效果，即在保证精度的同时也要保证速度。所以，在训练好模型之后会对模型进行微调和缩减。FRCNN，Mask－RCNN，YOLO是现在常用的网络模型，这几个模型拥有的共同点就是精度高、速度快。例如应用在人脸识别领域，这几个模型都可以实时检测并得出结果。2.1.2 目标追踪目标跟踪主要有 3 类算法，相关滤波算法、检测与跟踪相结合的算法和基于深度学习的算法。相关滤波( Correlation Filter，CF) 是当前研究的一个重点，最初它应用在信号领域，之后引入目标跟踪领域。它引入了快速傅里叶变换从而使得算法效率得到有效提升。MOSSE是目标检测算法中应用最早的算法。在研究过程中在实时性应用方面做出突破的算法是CSK。在CSK的基础上进行改进，产生了KCF算法。之后还提出了CN，DSST，SRDCF等算法。检测与跟踪相结合的算法，简单来说就是目标跟踪的判别式算法。算法的实现理念是先找出目标的位置，然后再对目标进行跟踪。深度学习的推广也影响到目标跟踪研究。基于深度学习的算法有分类和回归两类。R－CNN，Fast R－CNN，Faster R－CNN是基于分类的算法。3 种算法最大的不同在于检测窗口的选择，R－CNN 采用滑动窗口，Fast R－CNN 采用 Selective Search，Faster R－CNN采用RPN。2.1.3 语义分割计算机视觉就是将图片分割成像素，然后对像素进行处理。语义分割的意义是理解分割后像素的含义，例如图片中识别人、摩托、汽车及路灯等，它需要对密集的像素进行判别。卷积神经网络推动了语义分割算法的发展。语义分割中最基础的方法是通过滑动的窗口进行分类预测。2014 年，全卷积神经网络(Fully Convolutional Networks，FCN)的出现替代了网络全连接层。基于FCN 研究出 Encoder－Decoder 架构。Encoder是降低空间维度的操作，Decoder是恢复空间维度和细节信息的操作。之后空洞卷积( Dialated/Atrous) 取代了Pooling操作。空洞卷积的优点是它可以保持空间分辨率。除了之前的几种方法，还有一种叫条件随机场(Conditional Random Fields，CRFs)的方法来提升分割效果。2.2 机器学习机器学习的基本思想是通过计算机对数据的学习来提升自身性能的算法。机器学习中需要解决的最重要的4类问题是预测、聚类、分类和降维。机器学习按照学习方法分类可分为: 监督学习、无监督学习、半监督学习和强化学习。2.2.1 监督学习监督学习指的是用打好标签的数据训练预测新数据的类型或值。根据预测结果的不同可以分为2类: 分类和回归。监督学习的典型方法有SVM和线性判别。回归问题指预测出一个连续值的输出，例如可以通过房价数据的分析，根据样本的数据输入进行拟合，进而得到一条连续的曲线用来预测房价。分类问题指预测一个离散值的输出，例如根据一系列的特征判断当前照片是狗还是猫，输出值就是1或者0。2.2.2 无监督学习无监督学习是在数据没有标签的情况下做数据挖掘，无监督学习主要体现在聚类。简单来说是将数据根据不同的特征在没有标签的情况下进行分类。无监督学习的典型方法有k－聚类及主成分分析等k－聚类的一个重要前提是数据之间的区别可以用欧氏距离度量，如果不能度量的话需要先转换为可用欧式距离度量。主成分分析是一种统计方法。通过使用正交变换将存在相关性的变量，变为不存在相关性的变量，转换之后的变量叫做主成分。其基本思想就是将最初具有一定相关性的指标，替换为一组相互独立的综合指标。2.2.3 半监督学习半监督学习根据字面意思可以理解为监督学习和无监督学习的混合使用。事实上是学习过程中有标签数据和无标签数据相互混合使用。一般情况下无标签数据比有标签数据量要多得多。半监督学习的思想很理想化，但是在实际应用中不多。一般常见的半监督学习算法有自训练算法( Self－training) 、基于图的半监督算法( Graph－based Semi－supervisedLearning) 和半监督支持向量机(S3VM)。2.2.4 强化学习随着Alpha Go的火热，强化学习成为了当前最火热的研究领域之一，强化学习词热点居高不下。强化学习是通过与环境的交互获得奖励，并通过奖励的高低来判断动作的好坏进而训练模型的方法。强化学习中探索和开发的权重高低是一个难题: 为获得更好的奖励必须尽量选择能获得高奖励的动作，但是为了获得更好的奖励，也必须要挖掘未知的动作。强化学习的基础来源于行为心理学。在1991年Thorndike提出了效用法则，即在环境中让人或者动物感到舒服的动作，人或者动物会不断强化这一动作。反之，如果人或者动物感觉到不舒服的行为，人或者动物会减少这种动作。强化学习换言之是强化得到奖励的行为，弱化受到惩罚的行为。通过试错的机制训练模型，找到最佳的动作和行为获得最大的回报。它模仿了人或者动物学习的模式，并且不需要引导智能体向某个方向学习。智能体可以自主学习，不需要专业知识的引导和人力的帮助。基础的强化学习算法有使用表格学习的q_learning，sarsa以及使用神经网络学习的DQN，直接输出行为的 Policy Gradients及Actor Critic等。强化学习算法应用到游戏领域取得了不错的成果，在星际(图 2)和潮人篮球(图 3)的AI训练方面都取得了不错的成果。2.3 自然语义处理自然语言处理(NLP)是指计算机拥有识理解人类文本语言的能力，是计算机科学与人类语言学的交叉学科。自然语言是人与动物之间的最大区别，人类的思维建立在语言之上，所以自然语言处理也就代表了人工智能的最终目标。机器若想实现真正的智能自然语言处理是必不可少的一环。自然语言处理分为语法语义分析、信息抽取、文本挖掘、信息检索、机器翻译、问答系统和对话系统7个方向。句法语义分析，是对于给定的语言提取词进行词性和词义分析，然后分析句子的句法、语义角色和多词义选取。信息抽取，是指从给定的一段文字中抽取时间、地点和人物等主要信息，以及因果关系等句子关系。文本挖掘，对大量的文档提供自动索引，通过关键词或其他有用信息的输入自动检索出需要的文档信息。机器翻译，输入源文字并自动将源文字翻译为另一种语言，根据媒介的不同可以分为很多的细类，如文本翻译、图形翻译及手语翻译等。问答系统，是提出一个文字表达的问题，计算机可以给出准确的答案，过程中需要对问题进行语义分析，然后在资料库中寻出对应答案。对话系统，指计算机可以联系上下文和用户进行聊天及交流等任务，针对不同的用户采用不同的回复方式等功能。自然语言处理主要有 5 类技术，分别是分类、匹配、翻译、结构预测及序列决策过程。2.4 语音识别现在人类对机器的运用已经到了一个极高的状态，所以人们对于机器运用的便捷化也有了依赖。采用语言支配机器的方式是一种十分便捷的形式。语音识别技术是将人类的语音输入转换为一种机器可以理解的语言，或者转换为自然语言的一种过程。人类的声音信号经过话筒接收以后，转变成为电信号并作为语音识别系统的输入，然后系统对传入信号进行处理，再进行特征抽取，提取特征参数，从而提取出特征。将特征与原有数据库进行对比，最终输出识别出的语言结果。语音识别的难点主要集中在噪声处理、鲁棒性和语音模型上。在输入语音时总是可能出现各种各样的噪声，提高对噪声的处理是提高识别准确率的重要一环。鲁棒性，现有的语音识别系统对环境的依懒性偏高，不同的环境中识别的准确性可能会有较大差别。语音模型的优化也是面临的一个重大问题，语言的复杂性毋庸置疑，语言的语义、情绪及语速等都会影响到语音的真实意义，所以优化语音模型，优化语音模型的基础就是需要大量的数据。3 人工智能应用前景人工智能市场发展迅速，不断将科研成果应用到实践中。除了现在的基础科研，还将科研成果不断付诸实践，各种人工智能计算机不断产出。以上人工智能四大核心技术的应用前景十分广阔。3.1 计算机视觉应用在计算机视觉领域，中国融资过亿的企业就有 11 家。商汤科技是一家以计算机视觉技术为核心的企业，专注于人工智能视觉引擎，拥有自主研发的深度学习平台，不断产出计算机视觉技术，它涉及的行业有无人驾驶、平安城市及金融等高技术产业，不断将产业技术付诸实践，吸收融资后致力于商汤的自主技术商业化。国内眼擎科技公司发布的AI视觉成像芯片全球首发，它的出现提升了现有的视觉识别能力，即使在极其复杂的环境中依然可以拥有十分优秀的视觉能力。计算机视觉技术在安防领域的应用也十分广泛。通过视频内容自动识别车辆、人还有其他信息，为安防提供技术支持，并在追逃阶段可以自动汇报追踪相应的可疑车辆和人的运动轨迹，为公安机关抓捕提供可靠的信息。计算机视觉领域不断有企业涌现出旺盛的生命力，体现了人工智能这一技术方向的巨大潜力。3．2 机器学习应用机器学习与自动驾驶、金融及零售等行业紧密结合，不断提升行业的发展潜力。在自动驾驶领域运用机器学习的技术，不断提升自动驾驶的路测能力，通过强化学习的手段让无人汽车在环境中不断提升自己的能力，训练出的模型在基本路测环境中保持稳定。通过不断引入新的机器学习技术，让无人驾驶的商业化成为可展望的未来。零售行业运用机器学习的技术分析用户的喜好，进行定点推送，提供顾客更偏向购买的物品，提升零售的成功率。在金融领域人工智能的市场规模已经变得越来越大，通过机器学习的技术手段，预测风险和股市的走向。运用机器学习的手段进行金融风险管控，整合多源的资料，实时向人提供风险预警信息。利用大数据对相应的金融风险进行分析，实时提供相应金融资产的风险预警，节省投资理财的人力物力消耗，构建科学合理的风险管控体系，为金融业的发展添砖加瓦。3．3 自然语言处理应用自然语言处理应用领域也很广阔。在邮件领域，它被用来分析处理垃圾邮件，为用户提供良好的应用环境。通过语言识别对文档进行自动分类，节省了人力并为企业的自动化运转提供了技术支持; 在书籍分类中，可以根据书籍内容进行自动分类，为用户查找相应书籍提供便捷的寻找手段; 自动翻译的便捷功能，让语言不再成为知识交流的障碍，在线翻译软件可以即时翻译出绝大部分文本; 人工智能客服的出现也改变了用户体验，基本问题可以直接找机器客服解决。在金融领域的智能客服和智能投资顾问也运用了自然语言处理技术。智能投资顾问和智能客服采用语义识别技术，对咨询者的语义进行分析，并在资源库中找出最合适的回答方式和内容。智能投资顾问管理的资产在 2012 年还基本不存在，在2014年时技能已经达了140亿美元，到2019年初处于其管理下的资产已经到达了一个十分惊人的数字。3．4 语音识别应用语音识别应用的领域更加广泛，语音识别技术的普及让即时翻译不再困难。在微信中，通过语音识别技术可以不听取他人语音直接翻译为相应的文本，使微信交流功能在不方便听取语音的环境中不受影响。智能家居是一种以居住环境为平台的先进理念，通过人工智能的方式让与生活相关的家居统筹管理，使人的生活环境更加智能、舒适。智能家居中也应用了语音识别技术，通过解析人的语言命令，让家居进入相应的开关程序，并对人的命令作出回应，提升人的居住体验。4 结束语人工智能技术综合了多个学科领域，对人类的发展具有不可替代的作用。可以预见的是，人工智能必将成为下一次工业革命的核心。由此带来的变革不仅体现在技术上，对人类的心理、人文及伦理等方面都会造成冲击。当前90%的人力工作将来都有可能被人工智能取代，但是当人工智能取代传统岗位之后依然会衍生出新的岗位，不会引起大面积失业。人工智能时代已经降临，在教育层面应当响应时代号召，积极学习人工智能各项新技术; 在社会层面应当积极接受新的事物，不断前进并开拓出更多新的生活方式，不断与时俱进、更新思想大跨步迈进人工智能新时代。摘自：人工智能综述:AI的发展"}
{"content2":"在深度学习算法出来之前，对于视觉算法来说，大致可以分为以下5个步骤：特征感知，图像预处理，特征提取，特征筛选，推理预测与识别。早期的机器学习中，占优势的统计机器学习群体中，对特征是不大关心的。我认为，计算机视觉可以说是机器学习在视觉领域的应用，所以计算机视觉在采用这些机器学习方法的时候，不得不自己设计前面4个部分。但对任何人来说这都是一个比较难的任务。传统的计算机识别方法把特征提取和分类器设计分开来做，然后在应用时再合在一起，比如如果输入是一个摩托车图像的话，首先要有一个特征表达或者特征提取的过程，然后把表达出来的特征放到学习算法中进行分类的学习。过去20年中出现了不少优秀的特征算子，比如最著名的SIFT算子，即所谓的对尺度旋转保持不变的算子。它被广泛地应用在图像比对，特别是所谓的structure from motion这些应用中，有一些成功的应用例子。另一个是HoG算子，它可以提取物体，比较鲁棒的物体边缘，在物体检测中扮演着重要的角色。这些算子还包括Textons，Spin image，RIFT和GLOH，都是在深度学习诞生之前或者深度学习真正的流行起来之前，占领视觉算法的主流。这些特征和一些特定的分类器组合取得了一些成功或半成功的例子，基本达到了商业化的要求但还没有完全商业化。一是八九十年代的指纹识别算法，它已经非常成熟，一般是在指纹的图案上面去寻找一些关键点，寻找具有特殊几何特征的点，然后把两个指纹的关键点进行比对，判断是否匹配。然后是2001年基于Haar的人脸检测算法，在当时的硬件条件下已经能够达到实时人脸检测，我们现在所有手机相机里的人脸检测，都是基于它或者它的变种。第三个是基于HoG特征的物体检测，它和所对应的SVM分类器组合起来的就是著名的DPM算法。DPM算法在物体检测上超过了所有的算法，取得了比较不错的成绩。但这种成功例子太少了，因为手工设计特征需要大量的经验，需要你对这个领域和数据特别了解，然后设计出来特征还需要大量的调试工作。说白了就是需要一点运气。另一个难点在于，你不只需要手工设计特征，还要在此基础上有一个比较合适的分类器算法。同时设计特征然后选择一个分类器，这两者合并达到最优的效果，几乎是不可能完成的任务。参考文档：1 https://www.leiphone.com/news/201605/zZqsZiVpcBBPqcGG.html"}
{"content2":"今日CS.CV计算机视觉论文速览Mon, 4 Mar 2019Totally 39 papersInteresting:📚MaskScoring R-CNN 为实例分割任务中增加了预测质量的评分结果,文章提出了网络模块学习预测实例的质量，并为实例分割结果进行打分。这一模块通过实例特征和对应的预测掩膜联合起来得到掩膜区域，实验表明这一机制提高了实例分割的结果，并显著增强了多个的性能。（from 华中科技大学）实验显示MaskIoU与分数具有更强的相关性：网络中输出maskIoU的区域及其不同设计：代码：https://github.com/zjhuang22/maskscoring_rcnn.ref 公众号📚EvoNet基于多图像的超分辨网络，利用基于残差的EvoNet输出多张超分辨重建图像(2X)，并基于图像进行shift-and-fusion融合(2x)，最终利用Evo成像模型得到最终的超分辨结果。（from Poland and with Silesian University of Technology）一些结果：一些相关算法比较：高精度数据集DIV 2K📚视频中非标记目标的探索，面临着定位和多重物体的挑战。此外现实中的物体还具有明显的长尾效应。研究人员利用10+个小时的视频数据中抽取了360,000个目标，并基于双目多帧候选区域、通用目标追踪、非标记数据识别及其聚类来完成未知类别的探索。(from 亚琛工业大学)论文的流程如下：实际中的表现如下：下图可以看到数据具有明显的长尾效应：一些相关算法的表现：相关数据集：KATTI Raw, OXFORD ROBOTCAR DATASET作者的项目网站：https://www.vision.rwth-aachen.de/page/lsom📚提出了一种基于Wasserstein GAN的模型用于图像去雾,在无需多张图像或者先验的情况下，利用GAN在Wasserstein 损失的基础上学习干净图像的概率分布，并基于梯度惩罚实现Lipschitz约束。再结合L1和纹理损失后，最终得到了端到端的单图像去噪模型。（from Indian Institute of Technology Kharagpur）模型损失定义如下：自然条件下去雾结果：人工数据下去雾结果：相关方法和结果：😎 相关数据集O-Haze,D-Hazy, ref。emoji📚📚📚📚Daily Computer Vision Papers[1] Title: A Behavioral Approach to Visual Navigation with Graph Localization NetworksAuthors:Kevin Chen, Juan Pablo de Vicente, Gabriel Sepulveda, Fei Xia, Alvaro Soto, Marynel Vazquez, Silvio Savarese[2] ***Title: Deep Learning for Multiple-Image Super-ResolutionAuthors:Michal Kawulok, Pawel Benecki, Szymon Piechaczek, Krzysztof Hrynczenko, Daniel Kostrzewa, Jakub Nalepa[3] Title: Quantifying contribution and propagation of error from computational steps, algorithms and hyperparameter choices in image classification pipelinesAuthors:Aritra Chowdhury, Malik Magdon-Ismail, Bulent Yener[4] **Title: Single Image Haze Removal Using Conditional Wasserstein Generative Adversarial NetworksAuthors:Joshua Peter Ebenezer, Bijaylaxmi Das, Sudipta Mukhopadhyay[5] Title: Deep Neural Network and Data Augmentation Methodology for off-axis iris segmentation in wearable headsetsAuthors:Viktor Varkarakis, Shabab Bazrafkan, Peter Corcoran[6] Title: Automatic microscopic cell counting by use of unsupervised adversarial domain adaptation and supervised density regressionAuthors:Shenghua He, Kyaw Thu Minn, Lilianna Solnica-Krezel, Hua Li, Mark Anastasio[7] Title: Answer Them All! Toward Universal Visual Question Answering ModelsAuthors:Robik Shrestha, Kushal Kafle, Christopher Kanan[8] *Title: Large-Scale Object Mining for Object Discovery from Unlabeled VideoAuthors:Aljosa Osep, Paul Voigtlaender, Jonathon Luiten, Stefan Breuers, Bastian Leibe[9] Title: Transformation Consistent Self-ensembling Model for Semi-supervised Medical Image SegmentationAuthors:Xiaomeng Li, Lequan Yu, Hao Chen, Chi-Wing Fu, Pheng-Ann Heng[10] **Title: Octree guided CNN with Spherical Kernels for 3D Point CloudsAuthors:Huan Lei, Naveed Akhtar, Ajmal Mian[11] Title: Progress Regression RNN for Online Spatial-Temporal Action Localization in Unconstrained VideosAuthors:Bo Hu, Jianfei Cai, Tat-Jen Cham, Junsong Yuan[12] Title: Provably scale-covariant networks from oriented quasi quadrature measures in cascadeAuthors:Tony Lindeberg[13] ***Title: Adversarial Generation of Handwritten Text Images Conditioned on SequencesAuthors:Eloi Alonso, Bastien Moysset, Ronaldo Messina[14] Title: Frequency Domain Transformer Networks for Video PredictionAuthors:Hafez Farazi, Sven Behnke[15] Title: Object Recognition in Deep Convolutional Neural Networks is Fundamentally Different to That in HumansAuthors:Ben Lonnqvist, Alasdair D. F. Clarke, Ramakrishna Chakravarthi[16] **Title: Mask Scoring R-CNNAuthors:Zhaojin Huang, Lichao Huang, Yongchao Gong, Chang Huang, Xinggang Wang[17] **Title: Single Image Deblurring and Camera Motion Estimation with Depth MapAuthors:Liyuan Pan, Yuchao Dai, Miaomiao Liu[18] Title: Lung CT Imaging Sign Classification through Deep Learning on Small DataAuthors:Guocai He[19] *Title: Pyramid Feature Selective Network for Saliency detectionAuthors:Ting Zhao, Xiangqian Wu[20] *Title: Image-Based Geo-Localization Using Satellite ImageryAuthors:Sixing Hu, Gim Hee Lee[21] Title: Video Extrapolation with an Invertible Linear EmbeddingAuthors:Robert Pottorff, Jared Nielsen, David Wingate[22] Title: Local Geometric Indexing of High Resolution Data for Facial Reconstruction from Sparse MarkersAuthors:Matthew Cong, Lana Lan, Ronald Fedkiw[23] ***Title: A Sketch Based 3D Shape Retrieval Approach Based on Efficient Deep Point-to-Subspace Metric LearningAuthors:Yinjie Lei, Ziqin Zhou, Pingping Zhang, Yulan Guo, Zijun Ma, Lingqiao Liu[24] **Title: Self-supervised Learning for Single View Depth and Surface Normal EstimationAuthors:Huangying Zhan, Chamara Saroj Weerasekera, Ravi Garg, Ian Reid[25] Title: Video Summarization via Actionness RankingAuthors:Mohamed Elfeki, Ali Borji[26] *Title: GAN Based Image Deblurring Using Dark Channel PriorAuthors:Shuang Zhang, Ada Zhen, Robert L. Stevenson[27] Title: Appearance-based Gesture recognition in the compressed domainAuthors:Shaojie Xu, Anvesha Amaravati, Justin Romberg, Arijit Raychowdhury[28] Title: Broad Neural Network for Change Detection in Aerial ImagesAuthors:Shailesh Shrivastava, Alakh Aggarwal, Pratik Chattopadhyay[29] Title: On the Effectiveness of Low Frequency PerturbationsAuthors:Yash Sharma, Gavin Weiguang Ding, Marcus Brubaker[30] *Title: SPDA: Superpixel-based Data Augmentation for Biomedical Image SegmentationAuthors:Yizhe Zhang, Lin Yang, Hao Zheng, Peixian Liang, Colleen Mangold, Raquel G. Loreto, David P. Hughes, Danny Z. Chen[31] Title: A Deep DUAL-PATH Network for Improved Mammogram Image ProcessingAuthors:Heyi Li, Dongdong Chen, William H. Nailon, Mike E. Davies, Dave Laurenson[32] Title: Multi-Object Representation Learning with Iterative Variational InferenceAuthors:Klaus Greff, Raphaël Lopez Kaufmann, Rishab Kabra, Nick Watters, Chris Burgess, Daniel Zoran, Loic Matthey, Matthew Botvinick, Alexander Lerchner[33] *Title: Learning To Follow Directions in Street ViewAuthors:Karl Moritz Hermann, Mateusz Malinowski, Piotr Mirowski, Andras Banki-Horvath, Keith Anderson, Raia Hadsell[34] *Title: TamperNN: Efficient Tampering Detection of Deployed Neural NetsAuthors:Erwan Le Merrer, Gilles Trédan[35] Title: Optimal Projection Guided Transfer Hashing for Image RetrievalAuthors:Ji Liu, Lei Zhang[36] *Title: A Unified Neural Architecture for Instrumental Audio TasksAuthors:Steven Spratley, Daniel Beck, Trevor Cohn[37] Title: A detailed comparative study of open source deep learning frameworksAuthors:Ghadeer Al-Bdour, Raffi Al-Qurran, Mahmoud Al-Ayyoub, Ali Shatnawi[38] Title: Neuromodulated Goal-Driven Perception in Uncertain DomainsAuthors:Xinyun Zou, Soheil Kolouri, Praveen K. Pilly, Jeffrey L. Krichmar[39] *Title: Speeding up Deep Learning with Transient ServersAuthors:Shijian Li, Robert J. Walls, Lijie Xu, Tian GuoPapers from arxiv.org更多精彩请移步主页pic from pixels.com😢writing_hand:致歉：上周学校事情很忙😵，没有及时更新！感谢小伙伴们一直关注，会坚持update~~❤"}
{"content2":"\"与其停留在概念理论层面，不如动手去实现一个简单demo 。\"       ——鲁迅没有源码都是耍流氓github前言目前提供AI开发相关API接口的公司有很多，国外如微软、谷歌，国内的百度、腾讯等都有开放API接口。开发者只需要调用相关接口，几步就能开发出一个“智能APP”。通常情况AI接口有以下几类：计算机视觉图像分类、图像目标检测以及视频检测跟踪等等。这类API主要用于处理图像和视频，能够给图像打tag，并分析视频图片中的物体及其对应坐标轨迹等。语言包括自然语言处理，分析自然语言含义，评估情绪等，例如机器翻译等。语音将语言音频转换为文本，使用声音进行验证，或向应用添加说话人识别。知识通过映射复杂信息和数据来解决任务，例如智能建议和语义搜索。基于Web Service的智能API接口让我们不需要了解复杂的机器学习以及数学知识就能轻松开发出智能APP。但是，本文将介绍如何完全自己动手去实现一个智能API接口服务，由于涉及到的东西非常多，本文仅以我比较熟悉的“计算机视觉”为例，包含“图像分类（image classification）”和“目标检测（target detect）”，之后如果有机会，我会介绍“视频轨迹跟踪”相关的东西，大概就是图像处理的升级版。在开始正文之前，先解释几个名词。AI的概念近一两年尤其火热，“机器学习”以及“深度学习”的技术介绍到处都是，这里再简单介绍一下我对它们的理解：人工智能：又名AI，概念出现得特别早，上世纪五六十年代就有。人工智能大概可以分为两大类，一类“强人工智能”，你可以理解为完全具备跟人类一样的思维和意识的计算机程序；第二类“弱人工智能”，大概就是指计算机能够完成大部分相对较高级的行为，比如前面提到的理解图片含义，理解语言含义以及理解语音等等。我们日常提到的人工智能通常指第二类，常见的有计算机视觉、语音识别、机器翻译、推荐系统、搜索引擎甚至一些智能美图的APP，这些都可以说使用了人工智能技术，因为它们内部都使用了相关机器学习或者深度学习的算法。机器学习：这个概念也出现得很早，大概上世界八九十年代（？）。以前的概念中，计算机必须按照人编写的程序去执行任务，对于程序中没有的逻辑，计算机是不可能去做的。机器学习出现后，计算机具备人类“掌握经验”的能力，在通过大量学习/总结规律之后，计算机能够预测它之前并没有见过的事物。深度学习：深度学习的概念近几年才出现，你可以理解为它是机器学习的升级。之所以近几年突然流行，是因为一些传统机器学习算法（比如神经网络）要想取得非常好的性能，神经网络必须足够复杂，同时需要大量的学习数据，这时计算能力遇到了瓶颈。而近几年随着硬件性能普遍提升，再加上互联网时代爆炸式的数据存储，训练出足够复杂的模型已经不再是遥不可及。因此，可以将深度学习理解为更复杂的机器学习方式。好了，基本概念理清楚之后，开始进入正题了。这次我需要实现计算机视觉中的两大智能API接口：图片分类和目标检测。技术和开发环境下面是用到的技术和环境：1）Python 3.5.2 (PIL、numpy、opencv、matplotlib等一些常见的库)2）Tensorflow 1.8.0(GPU版本)3）Keras 2.2.0 (backend是tensorflow)4）Yolo v3(目标检测算法)5）Windows 10 + Navida GTX 1080 显卡（需要安装cuda 和 cudnn）6）VS Code 1.19.3关于以上技术的介绍以及初次使用时的安装步骤，我这里不再多说了，网上教程很多，提示一下，初次安装环境，会有很多坑。一定要使用gpu版本的tensorflow，如果仅仅是自己搞着练练手，熟悉熟悉流程，安装cpu版本也行。接口定义好了，技术环境介绍完了之后，再把接口确定下来：名称接口参数返回在线图片检测/detect/onlineMethod=POSTonline_image_url=url[string]{“image”:”result_url”,“results”:[{“box”:[left, top, right, bottom],“score”:score,“class”:class},{“box”:[left, top, right, bottom],“score”:score,“class”:class}...],“time”:create_time,“type”:”online”}本地图片检测/detect/localMethod=POSTlocal_image=file data[byte]multipart/form-data{“image”:”result_url”,“results”:[{“box”:[left, top, right, bottom],“score”:score,“class”:class},{“box”:[left, top, right, bottom],“score”:score,“class”:class}...],“time”:create_time,“type”:”local”}在线图片分类/classification/onlineMethod=POSTonline_image_url=url[string]还没完成本地图片分类/classification/localMethod=POSTlocal_image=file_data[byte]multipart/form_data还没完成写这篇博客的时候，图片分类的模型还没有训练好，所以暂时放一下，下次更新。以上四个接口分两类，一类是提交在线图片的url即可，二类是提交本地图片文件（表单上传）。两类都需要POST方式提交，返回结果是json格式，里面包含了处理之后的图片url（所有的结果已经绘制在上面了），还有处理的raw_data，客户端收到这些raw_data后可以自己用作其他地方。目标检测目标检测算法使用的是YOLO V3，这里是C语言实现的版本：http://pjreddie.com/darknet/ 。由于我比较熟悉Python，所以我用的是另外一个Python版本的实现（基于Keras），这里是Keras版本的实现：https://github.com/qqwweee/keras-yolo3。 如果想要训练更好的模型，需要自己准备数据集，源码中有一个我写的开源工具，专门用来标记这个框架所用的数据集（这个工具需要.net 4.0+）。训练数据集使用的是微软的COCO数据集（https://github.com/cocodataset/cocoapi），这个也是C语言版本的默认数据集，你可以直接从官网上下载训练好的模型使用。图片分类待更新...Web服务器由于是Web API，那么你首先必须得有一个自己的Web Server。因为这是一个demo程序，所以没必要使用类似Django 、Flask这样的框架，于是索性就自己写一个吧。功能很简单，提供静态文件访问、以及可以处理我的API接口就行，写完核心代码大约200行（包含API接口处理的逻辑）。整个Web程序用到的模块大概有：http.server、PIL、urllib、io、uuid、time、json、os以及cgi。可以看到并不复杂。整个Web Server的代码：处理逻辑从调用API接口到返回处理结果的流程相当简单，跟普通的HTTP请求一样，客户端发送HTTP请求，携带对象参数，Web Server在接收到数据后，开始调用计算模块，并将计算结果转换成json格式返回给客户端：图中橙色部分为关键部分，详细实现请参见源码中的vision模块。Demo效果Demo中写好了一个静态html页面，运行python server.py后，在浏览中访问：http://localhost:8080/web-app/index.html即可看见测试页面。左边为处理之后的图片，右边为返回的json结果。检测在线图片，在文本框中copy图片url，点击提交。上传本地图片，点击提交。与此同时，在控制台（或我自己的VS Code集成终端）中可以看到如下输出：最开始是检测花费的时间，接着就是检测到的目标物体以及对应的坐标、分数等等。后面是转换之后的json字符串，最后客户端根据json中的url加载处理之后的图片。视频目标跟踪这里稍微说一下跟视频有关的处理。对于视频来讲，它跟图片一样，由一张张图片组成，唯一的区别就是它具备时间的维度。我们不仅要检测每帧中的目标，还要判断前后帧之间各个目标之间的联系。然后利用目标物体的位移差来分析物体行为，对于路上车辆来讲，可以分析“异常停车”、“压线”、“逆行掉头”、“车速”、“流量统计”、“抛洒物”等数据。关于机器学习AI开发离不开机器学习（深度学习），而机器学习涉及到的知识相对来讲非常广泛，不仅仅要求开发者掌握好编程技能，还对数学知识有较高的要求。我认为作为普通程序员，如果要学习AI开发，请用一种Top Down的方式，抛开晦涩难懂的数学理论，先找个适合自己的机器学习框架（比如tensorflow或者基于它的keras），学会如何准备训练数据集（比如本文中如何去标记图片？），如何训练自己的模型，然后用训练得到的模型去解决一些小问题（比如本文中的图像目标检测）。等自己对机器学习有一种具体的认识之后，经过一段时间的摸索，会自然而然地引导我们去了解底层的数学原理，这个时候再去搞清楚这些原理是什么。个人认为，不要先上来就要搞懂什么是梯度下降优化法、什么是目标函数、什么是激活函数，什么是学习率...，这些概念确实需要掌握，但是不是你学习机器学习最开始的时候。另外学习机器学习，请使用Python。计划下一篇介绍基于图片识别的视频自动分类，比如自动鉴黄等软件。"}
{"content2":"人工智能的诞生及发展人工智能对于我们来说其实并不是什么新鲜产物，早在1956年的夏季美国的达特茅斯（Dartmouth）大学就产生了。其实算起来也就比福大创立早了那么2年而已（福州大学于1958年创立）。具体的发展历史自己百度，博客里面写这些东西去其实很无聊。https://baike.baidu.com/item/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/9180?fr=aladdin说起来在其诞生后的5年后（1960年代前期）出现了第一门处理人工智能的语言LISP，后来也越演越烈，大家也许没听过LISP但是，这门语言的变种在大型数据库里面用的还挺多。其也影响着现代程序语言的历史，像是后面的C语言，C++多少都有这个的影子。人工智能的定义人工智能就是用人工的方法在机器（计算机）上实现的智能；或者说人们使用机器模拟人类的智能。由于人工智能在机器上实现的，所以又称为机器智能。人工智能学科是计算机科学中研究、设计和应用智能机器的一个分支，研究内容包括：如何设计和构造智能机器（智能计算机）或智能系统，使它能模拟、延伸、扩展人类智能；如何在这种智能机器（计算机）上实现人类智能，使机器人具有类似于人的智能；如何应用于这种智能机器。人工智能的研究内容及应用领域专家系统机器翻译机器视觉问题求解机器学习模式识别自动定论证明自动程序设计自然语言理解机器人学智能信息检索智能控制智能决策支持系统数据挖掘网络上的信息过滤人工智能网络等内容均属于人工智能的研究范围人工智能研究的方法及途径第一种观点——符号主义（Symbolicism）：运用计算机儿科学的方法，进行人工智能的研究，通过研究逻辑演绎在计算机上的实现方法，实现人类智能在计算机上的模拟。又称为逻辑主义（Logicism）或计算机学派（Computerism）。第二种观点——联结主义（Connectionism）：运用仿生学的方法进行研究，通过研究人脑的工作模型了解人类智能的本质。联结主义学派认为人类智能的基本单元是神经元，认知过程是由神经元构成的网络的信息传递，这种传递是分布并行进行的。其原理主要为神经网络及神经网络间的连接机制与学习算法。神经网路为主的连接机制方法，它属于非符号处理范畴。它所研究的内容实际上就是神经网路。第三种观点——行为主义：运用进化论的思想进行人工智能的研究，通过对外界事物的动态感知与交互，使计算机智能模拟系统逐步进化，提高智能水平。行为主义又称进化主义，起源于控制论，提出智能取决于感知和行动，取决于对外界复杂环境的适应。技术路线：（1）专用路线：专门开发一些专用的智能计算机（如LISP机、PROLOG机）或专用的软件系统（如EMYCIN专家系统开发工具），或者专用于开发人工智能的计算机语言。（2）通用路线：用现有的一般的计算机硬件或软件系统能够有效的支持人工智能系统的开发，并且能够解决一般的人工智能问题。（举例：老黄【NVIDIA英伟达】的TITAN V）。通用路线认为，在人工智能应用系统及产品开发过程中应该将知识工程视为软件工程的一个峰值，充分利用知识工程的思想，将之融入整个系统产品开发的全过程中。（3）硬件路线：认为智能机器的开发主要有赖于各种智能引荐、智能工具及固化技术，没有这些技术，智能产品的开发室不可能的，因此人工智能的发展，还有赖于硬件技术的发展，诸如超大规模集成电路（现在已经到了这个地步了），人工神经网络的发展（Intel 的Movidivs了解一下？还要其他的AI芯片）（4）软件路线：软件路线认为智能机器的研制主要在于各种智能软件及工具的开发及运用，发展软件技术室人工智能发展的必由之路。因此，启发性程序设计、自动编程系统、知识工程以及其他各种智能算法就成了研究的主要对象。这里要说明的一点事目前各种面向对象的语言编程系统以及各种应用系统的开发工具都带有辅助人类编程的功能，或者说稍微带点智能编程的功能。（常见的梗：Visual Studio(Eclipse)你也是个成熟的IDE，是时候自己敲代码了。）以上内容来自于张仰森《人工智能教程 学习指导与习题解析》，很多时候大部头的书看不完的时候，可以买学习指导看，学习指导一般就是方法论的范畴，讲的都是实打实的东西。像是学校里面学的C语言也就是这样，但是这大多数都是浅尝辄止，更多的内容度都需要自己去挖掘。像是《人工智能教程》里面最后一章（数据挖掘与Agent技术）出去随随便便都是一个学期要上的课。"}
{"content2":"今天给大家介绍一下经典的开源机器学习软件：编程语言：搞实验个人认为当然matlab最灵活了（但是正版很贵），但是更为前途的是python（numpy+scipy+matplotlib)和C/C++，这样组合既可搞研究，也可搞商业开发，易用性不比matlab差，功能组合更为强大，个人认为，当然R和java也不错.1.机器学习开源软件网（收录了各种机器学习的各种编程语言学术与商业的开源软件）http://mloss.org2 偶尔找到的机器学习资源网：（也非常全，1和2基本收录了所有ML的经典开源软件了）http://www.dmoz.org/Computers/Artificial_Intelligence/Machine_Learning/Software/3 libsvm （支持向量机界最牛的，不用多说了，台湾大学的林教授的杰作）http://www.csie.ntu.edu.tw/~cjlin/libsvm/4 WEKA （基于java的机器学习算法最全面最易用的开源软件）http://www.cs.waikato.ac.nz/ml/weka/5 scikit (本人最喜欢的一个基于python的机器学习软件，代码写得非常好，而且官方的文档非常全，所有都有例子，算法也齐全，开发也活跃，强烈推荐给大家用）http://scikit-learn.org/stable/6 OpenCv(最牛的开源计算机视觉库了，前途无可限量，做图像处理与模式识别的一定要用，总不能整天抱着matlab做实验和工业界脱节吧，但是有一定难度)http://opencv.willowgarage.com/wiki/7 Orange (基于c++和python接口的机器学习软件，界面漂亮，调用方便,可以同时学习C＋＋和python，还有可视化的功能，）http://orange.biolab.si/8 Mallet (基于JAVA实现的机器学习库，主要用于自然语言处理方面，特色是马尔可夫模型和随机域做得好，可和WEKA互补）http://mallet.cs.umass.edu/9 NLTK(PYTHON的自然处理开源库，非常易用，也强大，还有几本orelly的经典教程）http://nltk.org/10 lucene(基于java的包括nutch,solr,hadoop,mahout等全套，是做信息检索和搜索引擎的同志们必学的开源软件了，学JAVA的必学）http://lucene.apache.org/当然还有很多很好的开源软件了，以后陆续添加，待续，困了。。。。。"}
{"content2":"原文地址：[ZZ]计算机视觉、机器学习相关领域论文和源代码大集合作者：计算机视觉与模式注：下面有project网站的大部分都有paper和相应的code。Code一般是C/C++或者Matlab代码。最近一次更新：2013-1-29一、特征提取Feature Extraction：SIFT [1] [Demo program][SIFT Library] [VLFeat]PCA-SIFT [2] [Project]Affine-SIFT [3] [Project]SURF [4] [OpenSURF] [Matlab Wrapper]Affine Covariant Features [5] [Oxford project]MSER [6] [Oxford project] [VLFeat]Geometric Blur [7] [Code]Local Self-Similarity Descriptor [8] [Oxford implementation]Global and Efficient Self-Similarity [9] [Code]Histogram of Oriented Graidents [10] [INRIA Object Localization Toolkit] [OLT toolkit for Windows]GIST [11] [Project]Shape Context [12] [Project]Color Descriptor [13] [Project]Pyramids of Histograms of Oriented Gradients [Code]Space-Time Interest Points (STIP) [14][Project] [Code]Boundary Preserving Dense Local Regions [15][Project]Weighted Histogram[Code]Histogram-based Interest Points Detectors[Paper][Code]An OpenCV - C++ implementation of Local Self Similarity Descriptors [Project]Fast Sparse Representation with Prototypes[Project]Corner Detection [Project]AGAST Corner Detector: faster than FAST and even FAST-ER[Project]二、图像分割Image Segmentation：Normalized Cut [1] [Matlab code]Gerg Mori’ Superpixel code [2] [Matlab code]Efficient Graph-based Image Segmentation [3] [C++ code] [Matlab wrapper]Mean-Shift Image Segmentation [4] [EDISON C++ code] [Matlab wrapper]OWT-UCM Hierarchical Segmentation [5] [Resources]Turbepixels [6] [Matlab code 32bit] [Matlab code 64bit] [Updated code]Quick-Shift [7] [VLFeat]SLIC Superpixels [8] [Project]Segmentation by Minimum Code Length [9] [Project]Biased Normalized Cut [10] [Project]Segmentation Tree [11-12] [Project]Entropy Rate Superpixel Segmentation [13] [Code]Fast Approximate Energy Minimization via Graph Cuts[Paper][Code]Efﬁcient Planar Graph Cuts with Applications in Computer Vision[Paper][Code]Isoperimetric Graph Partitioning for Image Segmentation[Paper][Code]Random Walks for Image Segmentation[Paper][Code]Blossom V: A new implementation of a minimum cost perfect matching algorithm[Code]An Experimental Comparison of Min-Cut/Max-Flow Algorithms for Energy Minimization in Computer Vision[Paper][Code]Geodesic Star Convexity for Interactive Image Segmentation[Project]Contour Detection and Image Segmentation Resources[Project][Code]Biased Normalized Cuts[Project]Max-flow/min-cut[Project]Chan-Vese Segmentation using Level Set[Project]A Toolbox of Level Set Methods[Project]Re-initialization Free Level Set Evolution via Reaction Diffusion[Project]Improved C-V active contour model[Paper][Code]A Variational Multiphase Level Set Approach to Simultaneous Segmentation and Bias Correction[Paper][Code]Level Set Method Research by Chunming Li[Project]三、目标检测Object Detection：A simple object detector with boosting [Project]INRIA Object Detection and Localization Toolkit [1] [Project]Discriminatively Trained Deformable Part Models [2] [Project]Cascade Object Detection with Deformable Part Models [3] [Project]Poselet [4] [Project]Implicit Shape Model [5] [Project]Viola and Jones’s Face Detection [6] [Project]Bayesian Modelling of Dyanmic Scenes for Object Detection[Paper][Code]Hand detection using multiple proposals[Project]Color Constancy, Intrinsic Images, and Shape Estimation[Paper][Code]Discriminatively trained deformable part models[Project]Gradient Response Maps for Real-Time Detection of Texture-Less Objects: LineMOD [Project]Image Processing On Line[Project]Robust Optical Flow Estimation[Project]Where's Waldo: Matching People in Images of Crowds[Project]四、显著性检测Saliency Detection：Itti, Koch, and Niebur’ saliency detection [1] [Matlab code]Frequency-tuned salient region detection [2] [Project]Saliency detection using maximum symmetric surround [3] [Project]Attention via Information Maximization [4] [Matlab code]Context-aware saliency detection [5] [Matlab code]Graph-based visual saliency [6] [Matlab code]Saliency detection: A spectral residual approach. [7] [Matlab code]Segmenting salient objects from images and videos. [8] [Matlab code]Saliency Using Natural statistics. [9] [Matlab code]Discriminant Saliency for Visual Recognition from Cluttered Scenes. [10] [Code]Learning to Predict Where Humans Look [11] [Project]Global Contrast based Salient Region Detection [12] [Project]Bayesian Saliency via Low and Mid Level Cues[Project]Top-Down Visual Saliency via Joint CRF and Dictionary Learning[Paper][Code]五、图像分类、聚类Image Classification, ClusteringPyramid Match [1] [Project]Spatial Pyramid Matching [2] [Code]Locality-constrained Linear Coding [3] [Project] [Matlab code]Sparse Coding [4] [Project] [Matlab code]Texture Classification [5] [Project]Multiple Kernels for Image Classification [6] [Project]Feature Combination [7] [Project]SuperParsing [Code]Large Scale Correlation Clustering Optimization[Matlab code]Detecting and Sketching the Common[Project]Self-Tuning Spectral Clustering[Project][Code]User Assisted Separation of Reflections from a Single Image Using a Sparsity Prior[Paper][Code]Filters for Texture Classification[Project]Multiple Kernel Learning for Image Classification[Project]SLIC Superpixels[Project]六、抠图Image MattingA Closed Form Solution to Natural Image Matting [Code]Spectral Matting [Project]Learning-based Matting [Code]七、目标跟踪Object Tracking：A Forest of Sensors - Tracking Adaptive Background Mixture Models [Project]Object Tracking via Partial Least Squares Analysis[Paper][Code]Robust Object Tracking with Online Multiple Instance Learning[Paper][Code]Online Visual Tracking with Histograms and Articulating Blocks[Project]Incremental Learning for Robust Visual Tracking[Project]Real-time Compressive Tracking[Project]Robust Object Tracking via Sparsity-based Collaborative Model[Project]Visual Tracking via Adaptive Structural Local Sparse Appearance Model[Project]Online Discriminative Object Tracking with Local Sparse Representation[Paper][Code]Superpixel Tracking[Project]Learning Hierarchical Image Representation with Sparsity, Saliency and Locality[Paper][Code]Online Multiple Support Instance Tracking [Paper][Code]Visual Tracking with Online Multiple Instance Learning[Project]Object detection and recognition[Project]Compressive Sensing Resources[Project]Robust Real-Time Visual Tracking using Pixel-Wise Posteriors[Project]Tracking-Learning-Detection[Project][OpenTLD/C++ Code]the HandVu：vision-based hand gesture interface[Project]八、Kinect：Kinect toolbox[Project]OpenNI[Project]zouxy09 CSDN Blog[Resource]九、3D相关：3D Reconstruction of a Moving Object[Paper] [Code]Shape From Shading Using Linear Approximation[Code]Combining Shape from Shading and Stereo Depth Maps[Project][Code]Shape from Shading: A Survey[Paper][Code]A Spatio-Temporal Descriptor based on 3D Gradients (HOG3D)[Project][Code]Multi-camera Scene Reconstruction via Graph Cuts[Paper][Code]A Fast Marching Formulation of Perspective Shape from Shading under Frontal Illumination[Paper][Code]Reconstruction:3D Shape, Illumination, Shading, Reflectance, Texture[Project]Monocular Tracking of 3D Human Motion with a Coordinated Mixture of Factor Analyzers[Code]Learning 3-D Scene Structure from a Single Still Image[Project]十、机器学习算法：Matlab class for computing Approximate Nearest Nieghbor (ANN) [Matlab class providing interface toANN library]Random Sampling[code]Probabilistic Latent Semantic Analysis (pLSA)[Code]FASTANN and FASTCLUSTER for approximate k-means (AKM)[Project]Fast Intersection / Additive Kernel SVMs[Project]SVM[Code]Ensemble learning[Project]Deep Learning[Net]Deep Learning Methods for Vision[Project]Neural Network for Recognition of Handwritten Digits[Project]Training a deep autoencoder or a classifier on MNIST digits[Project]THE MNIST DATABASE of handwritten digits[Project]Ersatz：deep neural networks in the cloud[Project]Deep Learning [Project]sparseLM : Sparse Levenberg-Marquardt nonlinear least squares in C/C++[Project]Weka 3: Data Mining Software in Java[Project]Invited talk \"A Tutorial on Deep Learning\" by Dr. Kai Yu (余凯)[Video]CNN - Convolutional neural network class[Matlab Tool]Yann LeCun's Publications[Wedsite]LeNet-5, convolutional neural networks[Project]Training a deep autoencoder or a classifier on MNIST digits[Project]Deep Learning 大牛Geoffrey E. Hinton's HomePage[Website]十一、目标、行为识别Object, Action Recognition：Action Recognition by Dense Trajectories[Project][Code]Action Recognition Using a Distributed Representation of Pose and Appearance[Project]Recognition Using Regions[Paper][Code]2D Articulated Human Pose Estimation[Project]Fast Human Pose Estimation Using Appearance and Motion via Multi-Dimensional Boosting Regression[Paper][Code]Estimating Human Pose from Occluded Images[Paper][Code]Quasi-dense wide baseline matching[Project]ChaLearn Gesture Challenge: Principal motion: PCA-based reconstruction of motion histograms[Prpject]十二、图像处理：Distance Transforms of Sampled Functions[Project]The Computer Vision Homepage[Project]十三、一些实用工具：EGT: a Toolbox for Multiple View Geometry and Visual Servoing[Project] [Code]a development kit of matlab mex functions for OpenCV library[Project]Fast Artificial Neural Network Library[Project]From:http://blog.csdn.net/zouxy09"}
{"content2":"一、掌握知识（一）计算机视觉之OpenCV图片读取与展示、图片写入、图片质量控制、像素操作几何变换、图片特效、图像美化、机器学习机器学习：视频分解图片、图片合成视频（二）计算机视觉之TensorFlow：手写数字识别常量变量、Numpy模块使用四则运算、matplotlib模块使用矩阵操作基础、神经网络（三）计算机视觉之TensorFlow：刷脸识别Harr+adaboost人脸识别Hog+svm小狮子识别二、附录（相关知识）（一）KNN算法（邻近算法）K最近邻(kNN，k-NearestNeighbor)分类算法是数据挖掘分类技术中最简单的方法之一。所谓K最近邻，就是k个最近的邻居的意思，说的是每个样本都可以用它最接近的k个邻居来代表。kNN算法的核心思想是如果一个样本在特征空间中的k个最相邻的样本中的大多数属于某一个类别，则该样本也属于这个类别，并具有这个类别上样本的特性。该方法在确定分类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。kNN方法在类别决策时，只与极少量的相邻样本有关。由于kNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，kNN方法较其他方法更为适合。算法流程1. 准备数据，对数据进行 预处理2. 选用合适的数据结构存储训练数据和测试元组3. 设定参数，如k4.维护一个大小为k的的按距离由大到小的 优先级队列，用于存储最近邻训练元组。随机从训练元组中选取k个元组作为初始的最近邻元组，分别计算测试元组到这k个元组的距离，将训练元组标号和距离存入优先级队列5. 遍历训练元组集，计算当前训练元组与测试元组的距离，将所得距离L 与优先级队列中的最大距离Lmax6. 进行比较。若L>=Lmax，则舍弃该元组，遍历下一个元组。若L < Lmax，删除优先级队列中最大距离的元组，将当前训练元组存入优先级队列。7. 遍历完毕，计算优先级队列中k 个元组的多数类，并将其作为测试元组的类别。8. 测试元组集测试完毕后计算误差率，继续设定不同的k值重新进行训练，最后取误差率最小的k 值。（二）CNN卷积神经网络结构卷积神经网络(Convolutional Neural Network, CNN)是深度学习技术中极具代表的网络结构之一，在图像处理领域取得了很大的成功，在国际标准的ImageNet数据集上，许多成功的模型都是基于CNN的。CNN相较于传统的图像处理算法的优点之一在于，避免了对图像复杂的前期预处理过程（提取人工特征等），可以直接输入原始图像。图像处理中，往往会将图像看成是一个或多个的二维向量，如之前博文中提到的MNIST手写体图片就可以看做是一个28 × 28的二维向量（黑白图片，只有一个颜色通道；如果是RGB表示的彩色图片则有三个颜色通道，可表示为三张二维向量）。传统的神经网络都是采用全连接的方式，即输入层到隐藏层的神经元都是全部连接的，这样做将导致参数量巨大，使得网络训练耗时甚至难以训练，而CNN则通过局部连接、权值共享等方法避免这一困难，有趣的是，这些方法都是受到现代生物神经网络相关研究的启发"}
{"content2":"http://blog.csdn.net/zouroot/article/details/53053740领军人物以及他们的主页（1）googleResearch； http://research.google.com/index.html（2）MIT博士，汤晓欧学生林达华； http://people.csail.mit.edu/dhlin/index.html（3）MIT博士后Douglas Lanman； http://web.media.mit.edu/~dlanman/（4）OpenCV中文网站； http://www.opencv.org.cn/index.php/%E9%A6%96%E9%A1%B5（5）Stanford大学vision实验室； http://vision.stanford.edu/research.html（6）Stanford大学博士崔靖宇； http://www.stanford.edu/~jycui/（7）UCLA教授朱松纯； http://www.stat.ucla.edu/~sczhu/（8）中国人工智能网； http://www.chinaai.org/（9）中国视觉网； http://www.china-vision.net/（10）中科院自动化所； http://www.ia.cas.cn/（11）中科院自动化所李子青研究员； http://www.cbsr.ia.ac.cn/users/szli/（12）中科院计算所山世光研究员； http://www.jdl.ac.cn/user/sgshan/（13）TAISAU动态人脸识别http://www.taisau.com/（14）加州大学伯克利分校CV小组； http://www.eecs.berkeley.edu/Research/P ... CS/vision/（15）南加州大学CV实验室； http://iris.usc.edu/USC-Computer-Vision.html（16）卡内基梅隆大学CV主页；http://www.cs.cmu.edu/afs/cs/project/ci ... ision.html（17）微软CV研究员Richard Szeliski；http://research.microsoft.com/en-us/um/people/szeliski/（18）微软亚洲研究院计算机视觉研究组； http://research.microsoft.com/en-us/groups/vc/（19）微软剑桥研究院ML与CV研究组； http://research.microsoft.com/en-us/gro ... fault.aspx（20）研学论坛； http://bbs.matwav.com/（21）美国Rutgers大学助理教授刘青山； http://www.research.rutgers.edu/~qsliu/（22）计算机视觉最新资讯网； http://www.cvchina.info/（23）深大计算机视觉研究所http://csse.szu.edu.cn/2012/index.php/article/view/aid/544.html（24）香港中文大学助理教授王晓刚； http://www.ee.cuhk.edu.hk/~xgwang/(25)香港中文大学多媒体实验室（汤晓鸥）; http://mmlab.ie.cuhk.edu.hk/(26)U.C. San Diego. computer vision;http://vision.ucsd.edu/content/home(27)CVonline; http://homepages.inf.ed.ac.uk/rbf/CVonline/(28)computer vision software; http://peipa.essex.ac.uk/info/software.html(29)Computer Vision Resource; http://www.cvpapers.com/(30)computer vision research groups;http://peipa.essex.ac.uk/info/groups.html(31)computer vision center; http://computervisioncentral.com/cvcnews(32)浙江大学图像技术研究与应用（ITRA）团队：http://www.dvzju.com/(33)自动识别网：http://www.autoid-china.com.cn/(34)清华大学章毓晋教授：http://www.tsinghua.edu.cn/publish/ee/4157/2010/20101217173552339241557/20101217173552339241557_.html(35)顶级民用机器人研究小组Porf.Gary领导的Willow Garage:http://www.willowgarage.com/(36)上海交通大学图像处理与模式识别研究所：http://www.pami.sjtu.edu.cn/(37)上海交通大学计算机视觉实验室刘允才教授：http://www.visionlab.sjtu.edu.cn/(38)德克萨斯州大学奥斯汀分校助理教授Kristen Grauman ：http://www.cs.utexas.edu/~grauman/(39)清华大学电子工程系智能图文信息处理实验室（丁晓青教授）：http://ocrserv.ee.tsinghua.edu.cn/auto/index.asp(40)北京大学高文教授：http://www.jdl.ac.cn/htm-gaowen/(41)清华大学艾海舟教授：http://media.cs.tsinghua.edu.cn/cn/aihz(42)中科院生物识别与安全技术研究中心：http://www.cbsr.ia.ac.cn/china/index%20CH.asp(43)瑞士巴塞尔大学 Thomas Vetter教授：http://informatik.unibas.ch/personen/vetter_t.html(44)俄勒冈州立大学 Rob Hess博士：http://blogs.oregonstate.edu/hess/(45)深圳大学 于仕祺副教授：http://yushiqi.cn/(46)西安交通大学人工智能与机器人研究所：http://www.aiar.xjtu.edu.cn/(47)卡内基梅隆大学研究员Robert T. Collins:http://www.cs.cmu.edu/~rcollins/home.html#Background(48)MIT博士Chris Stauffer:http://people.csail.mit.edu/stauffer/Home/index.php(49)美国密歇根州立大学生物识别研究组(Anil K. Jain教授)：http://www.cse.msu.edu/rgroups/biometrics/(50)美国伊利诺伊州立大学Thomas S. Huang:http://www.beckman.illinois.edu/directory/t-huang1(51)武汉大学数字摄影测量与计算机视觉研究中心：http://www.whudpcv.cn/index.asp(52)瑞士巴塞尔大学Sami Romdhani助理研究员：http://informatik.unibas.ch/personen/romdhani_sami/(53)CMU大学研究员Yang Wang:http://www.cs.cmu.edu/~wangy/home.html(54)英国曼彻斯特大学Tim Cootes教授：http://personalpages.manchester.ac.uk/staff/timothy.f.cootes/(55)美国罗彻斯特大学教授Jiebo Luo:http://www.cs.rochester.edu/u/jluo/(56)美国普渡大学机器人视觉实验室：https://engineering.purdue.edu/RVL/Welcome.html(57)美国宾利州立大学感知、运动与认识实验室：http://vision.cse.psu.edu/home/home.shtml(58)美国宾夕法尼亚大学GRASP实验室：https://www.grasp.upenn.edu/(59)美国内达华大学里诺校区CV实验室：http://www.cse.unr.edu/CVL/index.php(60)美国密西根大学vision实验室：http://www.eecs.umich.edu/vision/index.html(61)University of Massachusetts(麻省大学),视觉实验室：http://vis-www.cs.umass.edu/index.html(62)华盛顿大学博士后Iva Kemelmacher:http://www.cs.washington.edu/homes/kemelmi(63)以色列魏茨曼科技大学Ronen Basri:http://www.wisdom.weizmann.ac.il/~ronen/index.html(64)瑞士ETH-Zurich大学CV实验室：http://www.vision.ee.ethz.ch/boostingTrackers/index.htm(65)微软CV研究员张正友：http://research.microsoft.com/en-us/um/people/zhang/(66)中科院自动化所医学影像研究室：http://www.3dmed.net/(67)中科院田捷研究员：http://www.3dmed.net/tian/(68)微软Redmond研究院研究员Simon Baker:http://research.microsoft.com/en-us/people/sbaker/(69)普林斯顿大学教授李凯：http://www.cs.princeton.edu/~li/(70)普林斯顿大学博士贾登：http://www.cs.princeton.edu/~jiadeng/(71)牛津大学教授Andrew Zisserman： http://www.robots.ox.ac.uk/~az/(72)英国leeds大学研究员Mark Everingham:http://www.comp.leeds.ac.uk/me/(73)英国爱丁堡大学教授Chris William: http://homepages.inf.ed.ac.uk/ckiw/(74)微软剑桥研究院研究员John Winn: http://johnwinn.org/(75)佐治亚理工学院教授Monson H.Hayes：http://savannah.gatech.edu/people/mhayes/index.html(76)微软亚洲研究院研究员孙剑：http://research.microsoft.com/en-us/people/jiansun/(77)微软亚洲研究院研究员马毅：http://research.microsoft.com/en-us/people/mayi/(78)英国哥伦比亚大学教授David Lowe: http://www.cs.ubc.ca/~lowe/(79)英国爱丁堡大学教授Bob Fisher: http://homepages.inf.ed.ac.uk/rbf/(80)加州大学圣地亚哥分校教授Serge J.Belongie:http://cseweb.ucsd.edu/~sjb/(81)威斯康星大学教授Charles R.Dyer: http://pages.cs.wisc.edu/~dyer/(82)多伦多大学教授Allan.Jepson: http://www.cs.toronto.edu/~jepson/(83)伦斯勒理工学院教授Qiang Ji: http://www.ecse.rpi.edu/~qji/(84)CMU研究员Daniel Huber: http://www.ri.cmu.edu/person.html?person_id=123(85)多伦多大学教授：David J.Fleet: http://www.cs.toronto.edu/~fleet/(86)伦敦大学玛丽女王学院教授Andrea Cavallaro:http://www.eecs.qmul.ac.uk/~andrea/(87)多伦多大学教授Kyros Kutulakos: http://www.cs.toronto.edu/~kyros/(88)杜克大学教授Carlo Tomasi: http://www.cs.duke.edu/~tomasi/(89)CMU教授Martial Hebert: http://www.cs.cmu.edu/~hebert/(90)MIT助理教授Antonio Torralba: http://web.mit.edu/torralba/www/(91)马里兰大学研究员Yasel Yacoob: http://www.umiacs.umd.edu/users/yaser/(92)康奈尔大学教授Ramin Zabih: http://www.cs.cornell.edu/~rdz/(93)CMU博士田渊栋: http://www.cs.cmu.edu/~yuandong/(94)CMU副教授Srinivasa Narasimhan: http://www.cs.cmu.edu/~srinivas/(95)CMU大学ILIM实验室：http://www.cs.cmu.edu/~ILIM/(96)哥伦比亚大学教授Sheer K.Nayar: http://www.cs.columbia.edu/~nayar/(97)三菱电子研究院研究员Fatih Porikli ：http://www.porikli.com/(98)康奈尔大学教授Daniel Huttenlocher：http://www.cs.cornell.edu/~dph/(99)南京大学教授周志华：http://cs.nju.edu.cn/zhouzh/index.htm(100)芝加哥丰田技术研究所助理教授Devi Parikh: http://ttic.uchicago.edu/~dparikh/index.html(101)瑞士联邦理工学院博士后Helmut Grabner:http://www.vision.ee.ethz.ch/~hegrabne/#Short_CV(102)香港中文大学教授贾佳亚：http://www.cse.cuhk.edu.hk/~leojia/index.html(103)南洋理工大学副教授吴建鑫：http://c2inet.sce.ntu.edu.sg/Jianxin/index.html(104)GE研究院研究员李关：http://www.cs.unc.edu/~lguan/(105)佐治亚理工学院教授Monson Hayes:http://savannah.gatech.edu/people/mhayes/(106)图片检索国际会议VOC(微软剑桥研究院组织):http://pascallin.ecs.soton.ac.uk/challenges/VOC/(107)机器视觉开源处理库汇总：http://archive.cnblogs.com/a/2217609/(108)布朗大学教授Benjamin Kimia: http://www.lems.brown.edu/kimia.html(109)数据堂-图像处理相关的样本数据：http://www.datatang.com/data/list/602026/p1(110)东软基于CV的汽车辅助驾驶系统：http://www.neusoft.com/cn/solutions/1047/(111)马里兰大学教授Rema Chellappa:http://www.cfar.umd.edu/~rama/(112)芝加哥丰田研究中心助理教授Devi Parikh：http://ttic.uchicago.edu/~dparikh/index.html(113)宾夕法尼亚大学助理教授石建波：http://www.cis.upenn.edu/~jshi/(114)比利时鲁汶大学教授Luc Van Gool：http://www.vision.ee.ethz.ch/members/get_member.cgi?id=1,http://www.vision.ee.ethz.ch/~vangool/(115)行人检测主页：http://www.pedestrian-detection.com/(116)法国学习算法与系统实验室Basilio Noris博士：http://lasa.epfl.ch/people/member.php?SCIPER=129576http://mldemos.epfl.ch/(117)美国马里兰大学LARRY S.DAVIS教授：http://www.umiacs.umd.edu/~lsd/(118)计算机视觉论文分类导航：http://www.visionbib.com/bibliography/contents.html(119)计算机视觉分类信息导航：http://www.visionbib.com/(120)西班牙马德里理工大学博士Marcos Nieto：http://marcosnieto.net/(121)香港理工大学副教授张磊：http://www4.comp.polyu.edu.hk/~cslzhang/(122)以色列技术学院教授Michael Elad：http://www.cs.technion.ac.il/~elad/(123)韩国启明大学计算机视觉与模式识别实验室：http://cvpr.kmu.ac.kr/(124)英国诺丁汉大学Michel Valstar博士：http://www.cs.nott.ac.uk/~mfv/(125)卡内基梅隆大学Takeo Kanade教授:http://www.ri.cmu.edu/people/kanade_takeo.html(126)微软学术搜索：http://libra.msra.cn/(127)比利时天主教鲁汶大学Radu Timofte博士：http://homes.esat.kuleuven.be/~rtimofte/，交通标志检测，定位，3D跟踪(128)迪斯尼匹兹堡研究院研究员：Iain Matthews:http://www.iainm.com/iainm/Home.htmlhttp://www.ri.cmu.edu/person.html?type= ... son_id=741AAM,三维重建（129）康奈尔大学视觉与图像分析组：http://www.via.cornell.edu/医学图像处理（130）密西根州立大学生物识别研究组：http://www.cse.msu.edu/biometrics/人脸识别、指纹识别、图像检索（131）柏林科技大学计算机视觉与遥感实验室：http://www.cv.tu-berlin.de/menue/computer_vision_remote_sensing/parameter/en/图像分析、物体重建、基于图像的表面测量、医学图像处理（132）英国布里斯托大学数字多媒体研究组：http://www.cs.bris.ac.uk/Research/Digitalmedia/运动检测与跟踪、视频压缩、3D重建、字符定位（133）英国萨利大学视觉、语音与信号处理中心： http://www.surrey.ac.uk/cvssp/人脸识别、监控、3D、视频检索、（134）北卡莱罗纳大学教堂山分校Marc Pollefeys教授：http://www.cs.unc.edu/~marc/基于视频的3D模型生成、相机标定、运动检测与分析、3D重建（135）澳大利亚国立大学Richard Hartley教授：http://users.cecs.anu.edu.au/~hartley/运动估计、稀疏子空间、跟踪、（136）百度技术副总监于凯：http://www.dbs.ifi.lmu.de/~yu_k/深度学习，稀疏表示，图像分类（137）西安电子科技大学高新波教授：http://web.xidian.edu.cn/xbgao/index.html质量评判、水印、稀疏表示、超分辨率（138）加州大学伯克利分校Michael I.Jordan教授：http://www.cs.berkeley.edu/~jordan/机器学习（139）加州理工行人检测相关资料：http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/（140）微软Redmond研究院研究员Piotr Dollar: http://vision.ucsd.edu/~pdollar/行人检测、特征提取、（141）视觉计算研究论坛：http://www.sigvc.org/bbs/中科院视觉计算研究小组的论坛（142）美国坦桑尼亚州立大学稀疏学习软件包：http://www.public.asu.edu/~jye02/Software/SLEP/index.htm稀疏学习（143）美国加州大学圣地亚哥分校Jacob Whitehill博士：http://mplab.ucsd.edu/~jake/机器学习（144）美国布朗大学Michael J.Black教授：http://cs.brown.edu/~black/人的姿态估计和跟踪（145）美国加州大学圣地亚哥分校David Kriegman教授：http://cseweb.ucsd.edu/~kriegman/人脸识别（146）南加州大学Paul Debevec教授：http://ict.debevec.org/~debevec/或http://www.pauldebevec.com/将CV和CG结合研究 人脸捕捉重建技术（147）伊利诺伊大学D.A.Forsyth教授：http://luthuli.cs.uiuc.edu/~daf/三维重建（148）英国牛津大学Ian Reid教授：http://www.robots.ox.ac.uk/~ian/跟踪和机器人导航（149）CMU大学Alyosha Efros 教授: https://www.cs.cmu.edu/~efros/图像纹理合成（150）加州大学伯克利分校Jitendra Malik教授：http://www.cs.berkeley.edu/~malik/轮廓检测、图像/视频分割、图形匹配、目标识别（151）MIT教授William Freeman： http://people.csail.mit.edu/billf/应用于CV的ML、可视化感知的贝叶斯模型、计算摄影学；最有影响力的研究成果：图像纹理合成（152）CMU博士Henry Schneiderman： http://www.cs.cmu.edu/~hws/目标检测和识别；最有影响力的研究成果：目标检测；2000年CVPR上发表了”A statistical method for 3D object detection applied to faces and cars”。该算法采用多视角训练样本，可用于检测不同视角下的物体，如人脸和车，是第一个能够检测侧脸的算法。他创建了PittPatt公司，后被Google收购（153）微软研究员Paul Viola: http://research.microsoft.com/en-us/um/people/viola/AdaBoost算法（154）微软研究员Antonio Criminisi: http://research.microsoft.com/en-us/people/antcrim/图像修补，三维重建，目标检测与跟踪；（155）魏茨曼科学研究所教授Michal Irani: http://www.wisdom.weizmann.ac.il/~irani/超分辨率（156）瑞士洛桑理工学院Pascal Fua教授：http://people.epfl.ch/pascal.fua/bio?lang=en立体视觉，增强现实（157）佐治亚理工学院Irfan Essa教授：http://www.ic.gatech.edu/people/irfan-essa人脸表情识别（157）人脸识别主页； http://www.face-rec.org/(158)运动检测、阴影、跟踪的测试视频下载； http://apps.hi.baidu.com/share/detail/18903287"}
{"content2":"如何理解人工智能、机器学习和深度学习三者的关系今天我们就用最简单的方法——同心圆，可视化地展现出它们三者的关系和应用如上图，人工智能是最早出现的，也是最大、最外侧的同心圆;其次是机器学习，稍晚一点;最内侧，是深度学习，当今人工智能大爆炸的核心驱动。人工智能的一些较小的子集发展了起来。先是机器学习，然后是深度学习。深度学习又是机器学习的子集。而如今，深度学习造成了前所未有的巨大的影响。| 从概念的提出到走向繁荣1956年，几个计算机科学家相聚在达特茅斯会议(Dartmouth Conferences)，提出了“人工智能”的概念。其后，人工智能就一直萦绕于人们的脑海之中，并在科研实验室中慢慢孵化。之后的几十年，人工智能一直在两极反转，或被称作人类文明耀眼未来的预言;或者被当成技术疯子的狂想扔到垃圾堆里。坦白说，直到2012年之前，这两种声音还在同时存在。过去几年，尤其是2015年以来，人工智能开始大爆发。很大一部分是由于GPU的广泛应用，使得并行计算变得更快、更便宜、更有效。当然，无限拓展的存储能力和骤然爆发的数据洪流(大数据)的组合拳，也使得图像数据、文本数据、交易数据、映射数据全面海量爆发。让我们慢慢梳理一下计算机科学家们是如何将人工智能从最早的一点点苗头，发展到能够支撑那些每天被数亿用户使用的应用的。| 人工智能(Artificial Intelligence)——为机器赋予人的智能早在1956年夏天那次会议，人工智能的先驱们就梦想着用当时刚刚出现的计算机来构造复杂的、拥有与人类智慧同样本质特性的机器。这就是我们现在所说的“ 强人工智能 ”(General AI)。这个无所不能的机器，它有着我们所有的感知(甚至比人更多)，我们所有的理性，可以像我们一样思考。人们在电影里也总是看到这样的机器：友好的，像星球大战中的C-3PO;邪恶的，如终结者。强人工智能现在还只存在于电影和科幻小说中，原因不难理解，我们还没法实现它们，至少目前还不行。我们目前能实现的，一般被称为“ 弱人工智能 ”(Narrow AI)。 弱人工智能是能够与人一样，甚至比人更好地执行特定任务的技术。 例如，Pinterest上的图像分类;或者Facebook的人脸识别。这些是弱人工智能在实践中的例子。这些技术实现的是人类智能的一些具体的局部。但它们是如何实现的?这种智能是从何而来?这就带我们来到同心圆的里面一层，机器学习。| 机器学习—— 一种实现人工智能的方法机器学习最基本的做法，是使用算法来解析数据、从中学习，然后对真实世界中的事件做出决策和预测。与传统的为解决特定任务、硬编码的软件程序不同，机器学习是用大量的数据来“训练”，通过各种算法从数据中学习如何完成任务。机器学习直接来源于早期的人工智能领域。传统算法包括决策树学习、推导逻辑规划、聚类、强化学习和贝叶斯网络等等。众所周知，我们还没有实现强人工智能。早期机器学习方法甚至都无法实现弱人工智能。机器学习最成功的应用领域是计算机视觉，虽然也还是需要大量的手工编码来完成工作。人们需要手工编写分类器、边缘检测滤波器，以便让程序能识别物体从哪里开始，到哪里结束;写形状检测程序来判断检测对象是不是有八条边;写分类器来识别字母“ST-O-P”。使用以上这些手工编写的分类器，人们总算可以开发算法来感知图像，判断图像是不是一个停止标志牌。这个结果还算不错，但并不是那种能让人为之一振的成功。特别是遇到云雾天，标志牌变得不是那么清晰可见，又或者被树遮挡一部分，算法就难以成功了。这就是为什么前一段时间，计算机视觉的性能一直无法接近到人的能力。它太僵化，太容易受环境条件的干扰。随着时间的推进，学习算法的发展改变了一切。| 深度学习——一种实现机器学习的技术人工神经网络(Artificial Neural Networks)是早期机器学习中的一个重要的算法，历经数十年风风雨雨。神经网络的原理是受我们大脑的生理结构——互相交叉相连的神经元启发。但与大脑中一个神经元可以连接一定距离内的任意神经元不同， 人工神经网络具有离散的层、连接和数据传播的方向 。例如，我们可以把一幅图像切分成图像块，输入到神经网络的第一层。在第一层的每一个神经元都把数据传递到第二层。第二层的神经元也是完成类似的工作，把数据传递到第三层，以此类推，直到最后一层，然后生成结果。每一个神经元都为它的输入分配权重，这个权重的正确与否与其执行的任务直接相关。最终的输出由这些权重加总来决定。我们仍以停止(Stop)标志牌为例。将一个停止标志牌图像的所有元素都打碎，然后用神经元进行“检查”：八边形的外形、救火车般的红颜色、鲜明突出的字母、交通标志的典型尺寸和静止不动运动特性等等。神经网络的任务就是给出结论，它到底是不是一个停止标志牌。神经网络会根据所有权重，给出一个经过深思熟虑的猜测——“概率向量”。这个例子里，系统可能会给出这样的结果：86%可能是一个停止标志牌;7%的可能是一个限速标志牌;5%的可能是一个风筝挂在树上等等。然后网络结构告知神经网络，它的结论是否正确。即使是这个例子，也算是比较超前了。直到前不久，神经网络也还是为人工智能圈所淡忘。其实在人工智能出现的早期，神经网络就已经存在了，但神经网络对于“智能”的贡献微乎其微。主要问题是，即使是最基本的神经网络，也需要大量的运算。神经网络算法的运算需求难以得到满足。不过，还是有一些虔诚的研究团队，以多伦多大学的Geoffrey Hinton为代表，坚持研究，实现了以超算为目标的并行算法的运行与概念证明。但也直到GPU得到广泛应用，这些努力才见到成效。我们回过头来看这个停止标志识别的例子。 神经网络 是调制、训练出来的，时不时还是很容易出错的。它 最需要的，就是训练 。需要成百上千甚至几百万张图像来训练，直到神经元的输入的权值都被调制得十分精确，无论是否有雾，晴天还是雨天，每次都能得到正确的结果。只有这个时候，我们才可以说神经网络成功地自学习到一个停止标志的样子;或者在Facebook的应用里，神经网络自学习了你妈妈的脸;又或者是2012年吴恩达(Andrew Ng)教授在Google实现了神经网络学习到猫的样子等等。吴教授的突破在于，把这些神经网络从基础上显著地增大了。层数非常多，神经元也非常多，然后给系统输入海量的数据，来训练网络。在吴教授这里，数据是一千万YouTube视频中的图像。吴教授为深度学习(deep learning)加入了“深度”(deep)。这里的 “深度”就是说神经网络中众多的层 。现在，经过深度学习训练的图像识别，在一些场景中甚至可以比人做得更好：从识别猫，到辨别血液中癌症的早期成分，到识别核磁共振成像中的肿瘤。Google的AlphaGo先是学会了如何下围棋，然后与它自己下棋训练。它训练自己神经网络的方法，就是不断地与自己下棋，反复地下，永不停歇。| 深度学习，给人工智能以璀璨的未来深度学习使得机器学习能够实现众多的应用，并拓展了人工智能的领域范围。 深度学习摧枯拉朽般地实现了各种任务，使得似乎所有的机器辅助功能都变为可能。无人驾驶汽车，预防性医疗保健，甚至是更好的电影推荐，都近在眼前，或者即将实现。人工智能就在现在，就在明天。有了深度学习，人工智能甚至可以达到我们畅想的科幻小说一般。你的C-3PO我拿走了，你有你的终结者就好了。"}
{"content2":"人工智能岗位替代----编辑摘要：人 工 智 能 岗 位 替代编 辑 来 源：（百 度、中 国 新 闻  网）作 者 ：任 宏 冉目前人工智能已经为人类创造出了非常可观的经济效益，人工智能可以代替人类做大量人类不想做、不能做的工作，而且机器犯错误的概率比人低，并且能够持续工作，大大的提升工作效率，节约了大量的成本，未来的人工智能可能还会代替人类工作，代替人类做家务，帮助人类学习，甚至可以照顾老人和小孩，实时监护人类的健康，生病了直接给人来治疗，延长人类的寿命，让人类的生活变得越来越美好。科技的发展是一把双刃剑，汽车分发明颠覆了传统的马车行业，人工智能的发展同样也将颠覆许多行业。机器人代替了许多人类的工作将导致大量的人口失业，机器新的学习速度远远快于人类，阿尔法狗战胜李世石引起人们的恐慌，有人说不怕阿尔法狗战胜李世石，怕的是阿尔法够故意输掉一局，如果未来的某一天，机器人变成像电影《机械姬》中有意识的机器人，那么人类随时会变成机器人的奴隶，同时，人工智能面临着技术失控的危险，霍金曾发出警告，人类面临一个不确定的未来，先进的人工智能设备能够独立思考，并适应环境变化，它们未来或将成为导致人类灭亡的终结者！如果真的有一天，人工智能机器人变成了能独立思考，独立的做出准确的判断，一旦有一天人工智能反客为主，到时人工智能对于人类将会是毁灭性的灾难。甚至被人工智能消灭。地球将被人工智能统治。任何的科学技术的发展最大的威胁就是失去人类的控制，人工智能亦是如此，无论人工智能如何发展，都必须保证始终受人类控制，在不伤害人类的情况下服务于人类。这样人类才会更加容易的接受人工智能。人工智能改变了人们的生活，我们对人工智能应加以好的利用，同时要避免带来的弊端，人工智能与人类、与社会、与自然和谐相处，这样才能长远的发展。人工智能其实就是试图探索人脑认知的过程，再把它在计算机上加以实现。对于一般性的新闻写作来说，人工智能都可以解决，并不复杂。甚至即便要是想模仿一个艺术大师的名画，人工智能也可以做到很高的仿真度。黄民烈认为，从目前来看，人工智能在视觉和语音方面已经做得非常出色，但对于语言文字和对知识的理解推理上，人工智能还有很长的路要走。那么人工智能是否能替代网络编辑呢，下面我给大家介绍一下网络编辑的工作及重要点。从大数据角度看，我们媒体的未来在哪里？在沙龙上，中国传媒大学新媒体研究院副教授卢迪为大家勾勒出她认为的未来媒体图景。在卢迪看来，未来媒体除了要更智能，更加知道受众想看什么以外，更要求传统媒体要与新型媒体一体化发展。这种一体化发展不是说媒体被互联网融合，而是要让媒体先成为新媒体，才能在这基础上进行媒体融合。新媒体的核心要素就是数据，新媒体的发展规律则是会有效地使用数据，并知道如何发挥数据的价值。卢迪总结说，媒体的传播能力等于它的规模和实力，再加它的国际影响力。未来的媒体必然是大数据作为引擎的媒体，归根结底，数据是驱动未来媒体产生商业模式，驱动媒体产品创新和做大品牌最重要的根基。原创是网络编辑最核心的优势在沙龙的最后阶段，新浪新闻媒体产品总监王晓晨谈到，大数据时代，网络编辑是否会被取代这个话题。按照内容划分，网络编辑的主要工作分为：对热点发现和跟踪、制作专题、原创，以及推荐内容。在发现热点方面，机器的效率非常高，但是缺点是不知道如何聚焦。王晓晨认为，聚焦热点的时候就能看出编辑的重要性，他可以根据自己的经验和知识做决策。做专题也是机器的速度更快，但是需要编辑去做推荐。总体看来，原创目前是网络编辑最核心的优势，也是他们唯一可以固守的阵地，机器还无取代他们的可能性。未来，利用机器的好处是，可以拆解编辑工作流程的每一个环节，将最机械的部分替换掉，节省出人力做更有意义的事情。王晓晨觉得，网络编辑目前大可不必恐慌，现在重要的是要拥抱变化，不断探索媒体的发展趋势，重新定位自己。"}
{"content2":"转载至http://bbs.image-pro.com.cn/read.php?tid=1168做图像处理方面的研究工作，最重要的两个问题：其一是要把握住国际上最前沿的内容；其二是所作工作要具备很高的实用背景。解决第一个问题的办法就是找出这个方向公认最牛的几个超级大拿(看看他们都在作什么)和最权威的出版物(阅读上面最新的文献)，解决第二个问题的办法是你最好能够找到一个实际应用的项目，边做边写文章。做好这几点的途径之一就是充分利用网络资源，特别是权威网站和大拿们的个人主页。下面是我收集的一些资源，希望对大家有用。(这里我要感谢SMTH AI版的alamarik和Graphics版的faintt)导航栏：[1]研究群体[2]大拿主页[3]前沿期刊[4]GPL软件资源[5]搜索引擎一、研究群体http://www-2.cs.cmu.edu/~cil/vision.html这是卡奈基梅隆大学的计算机视觉研究组的主页，上面提供很全的资料，从发表文章的下载到演示程序、测试图像、常用链接、相关软硬件，甚至还有一个搜索引擎。http://www.cmis.csiro.au/IAP/zimage.htm这是一个侧重图像分析的站点，一般。但是提供一个Image Analysis环境---ZIMAGE and SZIMAGE。http://www.via.cornell.edu/康奈尔大学的计算机视觉和图像分析研究组，好像是电子和计算机工程系的。侧重医学方面的研究，但是在上面有相当不错资源，关键是它正在建设中，能够跟踪一些信息。http://www2.parc.com/istl/groups/did/didoverview.shtml有一个很有意思的项目：DID(文档图像解码)。http://www-cs-students.stanford.edu/斯坦福大学计算机系主页，自己找吧:(http://www.fmrib.ox.ac.uk/analysis/主要研究：Brain Extraction Tool,Nonlinear noise reduction,Linear Image Registration,Automated Segmentation,Structural brain change analysis,motion correction,etc.http://www.cse.msu.edu/prip/这是密歇根州立大学计算机和电子工程系的模式识别--图像处理研究组，它的FTP上有许多的文章(NEW)。http://pandora.inf.uni-jena.de/p/e/index.html德国的一个数字图像处理研究小组，在其上面能找到一些不错的链接资源。http://www-staff.it.uts.edu.au/~sean/CVCC.dir/home.htmlCVIP(used to be CVCC for Computer Vision and Cluster Computing) is a research group focusing on cluster-based computer vision within the Spiral Architecture.http://cfia.gmu.edu/The mission of the Center for Image Analysis is to foster multi-disciplinary research in image, multimedia and related technologies by establishing linksbetween academic institutes, industry and government agencies, and to transfer key technologies tohelp industry build nextgeneration commercial and military imaging and multimedia systems.http://peipa.essex.ac.uk/info/groups.html可以通过它来搜索全世界各地的知名的计算机视觉研究组(CV Groups)，极力推荐。二、图像处理GPL库http://www.ph.tn.tudelft.nl/~klamer/cppima.htmlCppima 是一个图像处理的C++函数库。这里有一个较全面介绍它的库函数的文档，当然你也可以下载压缩的GZIP包，里面包含TexInfo格式的文档。http://iraf.noao.edu/Welcome to the IRAF Homepage! IRAF is the Image Reduction and Analysis Facility, a general purpose softwaresystem for the reduction and analysis of astronomical data.http://entropy.brni-jhu.org/tnimage.html一个非常不错的Unix系统的图像处理工具，看看它的截图。你可以在此基础上构建自己的专用图像处理工具包。http://sourceforge.net/projects/这是GPL软件集散地，到这里找你想要得到的IP库吧。三、搜索资源当然这里基本的搜索引擎还是必须要依靠的，比如Google等，可以到我常用的链接看看。下面的链接可能会节省你一些时间：http://sal.kachinatech.com/http://cheminfo.pku.edu.cn/mirrors/SAL/index.shtml四、大拿网页http://www.ai.mit.edu/people/wtf/这位可是MIT人工智能实验室的BILL FREEMAN。大名鼎鼎！专长是：理解--贝叶斯模型。http://www.merl.com/people/brand/MERL(Mitsubishi Electric Research Laboratory)中的擅长“Style Machine”高手。http://research.microsoft.com/~ablake/CV界极有声望的A.Blake 1977年毕业于剑桥大学三一学院并或数学与电子科学学士学位。之后在MIT，Edinburgh，Oxford先后组建过研究小组并成为Oxford的教授，直到1999年进入微软剑桥研究中心。主要工作领域是计算机视觉。http://www-2.cs.cmu.edu/afs/cs.cmu.edu/user/har/Web/home.html这位牛人好像正在学习汉语，并且搜集了诸如“两只老虎(Two Tigers)”的歌曲，嘿嘿:)他的主页上面还有几个牛：Shumeet Baluja, Takeo Kanade。他们的Face Detection作的绝对是世界一流。他毕业于卡奈基梅隆大学的计算机科学系，兴趣是计算机视觉。http://www.ifp.uiuc.edu/yrui_ifp_home/html/huang_frame.html这位老牛在1963年就获得了MIT的博士学位！他领导的Image Lab比较出名的是指纹识别。--------------------------------------------------------------------------------下面这些是我搜集的牛群(大部分是如日中天的Ph.D们)，可以学习的是他们的Study Ways!Finn Lindgren(Sweden):Statistical image analysis http://www.maths.lth.se/matstat/staff/finn/Pavel Paclik(Prague):statistical pattern recognition http://www.ph.tn.tudelft.nl/~pavel/Dr. Mark Burge:machine learning and graph theory http://cs.armstrong.edu/burge/yalin Wang:Document Image Analysis http://students.washington.edu/~ylwang/Geir Storvik: Image analysis http://www.math.uio.no/~geirs/Heidorn http://alexia.lis.uiuc.edu/~heidorn/Joakim Lindblad:Digital Image Cytometry http://www.cb.uu.se/~joakim/index_eng.htmlS.Lavirotte: http://www-sop.inria.fr/cafe/Stephane.Lavirotte/Sporring:scale-space techniques http://www.lab3d.odont.ku.dk/~sporring/Mark Jenkinson:Reduction of MR Artefacts http://www.fmrib.ox.ac.uk/~mark/Justin K. Romberg:digital signal processing http://www-dsp.rice.edu/~jrom/Fauqueur:Image retrieval by regions of interest http://www-rocq.inria.fr/~fauqueur/James J. Nolan:Computer Vision http://cs.gmu.edu/~jnolan/Daniel X. Pape:Information http://www.bucho.org/~dpape/Drew Pilant:remote sensing technology http://www.geo.mtu.edu/~anpilant/index.html五、前沿期刊(TOP10)这里的期刊大部分都可以通过上面的大拿们的主页间接找到，在这列出主要是为了节省直接想找期刊投稿的兄弟的时间:)IEEE Trans. On PAMI http://www.computer.org/tpami/index.htmIEEE Transactionson Image Processing http://www.ieee.org/organizations/pubs/transactions/tip.htmPattern Recognition http://www.elsevier.com/locate/issn/00313203Pattern Recognition Letters http://www.elsevier.com/locate/issn/01678655"}
{"content2":"欢迎大家前往腾讯云+社区，获取更多腾讯海量技术实践干货哦~本文由云加社区技术沙龙 发表于云+社区专栏演讲嘉宾：叶聪，腾讯云AI和大数据中心高级研发工程师。在多年的云计算系统研发经历中，负责领导了多个百万级用户及产品的架构设计和开发。AI要走进千家万户，融入整个社会，而不仅仅是曲高和寡的模型。所以现在非常讲究AI场景化，使它成为与产业相关的部分。一、朋友圈爆款活动背后的技术今天我会从朋友圈的一些爆款的互动活动作为切入点，带大家过一遍一些计算及视觉技术与应用，或者把AI技术产业化。我们经常在朋友圈会看到一些比较有趣的互动的活动，比如像军装照、武士青年这类活动视觉比较流行的应用，也是目前探索出来的计算机视觉能够最快来到大家身边的方式。以过去的五四青年节的活动为例，这个有趣的活动就应用了人脸检测与人脸检索的技术相结合。上传一张照片最匹配民国时候有为青年的照片，做成这种页面供大家分享。这种模式最开始就是由腾讯开启，现已成为了整个行业应用的参考。除了这个活动，H5这类应用往往会出现用户访问爆发性的增长，为了应对这种突如其来的流量暴增，使用一整套云架构部署，而不是传统机房里面用几台服务器处理。比如像五四青年的活动，5万KPS的架构就有这样的承载能力。从个人角度想要做这样的应用，需要了解以下的基础知识。二、计算机视觉定义首先从计算机视觉的基本定义开始，学术角度来讲是研究如何让计算机从图像和视频中获取高级抽象的信息的一种方法。但我比较喜欢后面这种工程角度的解释，计算机视觉是可以使机器来模仿人类自动化。现在我们利用计算机视觉让整个流程更自动化，机器可以部分的代替人力去理解图片中的一些信息。计算机视觉还包含一些子分支，现在这个分支还会慢慢扩张，这个分类也在慢慢的扩大。主要包括物体识别、对象检测、语义分割、3D的重建，动作识别等等。底下有几个比较常见的视觉应用，第一个是人脸识别；第二个是无人驾驶；第三个是语义分割。语义分割在人工智能这个领域还是比较常见的。一般指词性的分割，或者词的分割。图像分割里面的语义分割一般指的是把图像里面的不同类型的对象进行标注和区分。右边这张图就是个例子。这张图上基本上把人、路、车都用了不同的颜色标注。提到机器视觉，首先就要知道机器是怎么去理解图片的？这里就要带入RGB-alpha 的一个算法。顾名思义，就是红绿蓝三色，然后，alpha 是什么？如果大家在早期的时候玩过一些电脑硬件，你可能会发现，最早期的显卡是 24 位彩色，后来出现一个叫 32 位真彩色，都是彩色的，有什么不同吗？因为在计算机领域，我们用 8 位的二进制去表示一种颜色，红绿蓝加在一起就是 24 位，基本上我们把所有颜色都表示出来了。三、曾经的图像处理——传统方法首先聊一下一些传统的图像处理的方法。上面这几个滤波器如果有学图像相关的同学都会经常用到，包括空间滤波器、小波滤波器。如果对图像进行分类，我们肯定要提取图像中的一些特征，然后根据这些图像提取出的特征用算法进行一些分类，所以是两步走。关于如何提取图像的特征，现在已经有很多比较成熟的方法。最容易想到的就是图像里面对象会有边缘。边缘就代表了图像的特点。除了边缘检测的方法还会有其他的特征，比如Haar特征。对象具有边缘、对角线、中心都可以利用这种黑白的对比图，标注出它灰度的变化，把刚才说的三种维度的信息组成一个集合，这样就可以提取图片的特征。还有一个比较新的特征提取方法，就是2012年的时候康奈尔提出来的，利用一个图片里面对象的局部对称性来提取特征。左边这个图有不同形状的对象，右边这个图就把它的对称性理解为重心提取出来，图上越白色的地方代表对称性越强。这种算法优点是整个图形提取出的特征是比较有对称性。这样就容易把这个东西从背景里面提出来。另外还有一些特征提法，比如SIFT，尺度不变特征。我先解释一下什么是尺度，我们看待一个东西，从远到近，越远它尺度越大，就说明这个东西相对我们越模糊，能观测到的特征点就少。随着靠近，看到的越多就能越发现一些特点。有些特点是不管远近都能辨识出来的，就认为是尺度不变的特征。还有一种跟灰度有关的方法叫HOG方向梯度直方图，它也是利用灰度的原理，把整个图形分成很多块，描述里面灰度变化最小的或者最大的。一般选用最小的方向。好处是它计算起来的成本比较靠谱的，比如这张图上，人穿黑色的衣服，所以在黑色衣服上它的灰度变化最小一定是垂直的。到了旁边白色背景可能就是水平的。利用它这个HOG的梯度图的方向，很容易把这个人从背景中间区分出来。聊了很多的Feature Design的方法，并不存在最优的。在图像分割和对象检测这个领域也有一些其他算法，例如非常有意思的分水岭算法，顾名思义是利用图像的灰度特性把图像整个灰度的曲线显示出来。假设往里面灌水，随着水越来越多，肯定会有两个相邻的山谷联通，在那里建一道坝，这些坝在图上来看就是紫色的曲线，这样就可以把对象里灰度不同的部分给分割出来。ASM，中文名叫可变模板匹配，或者叫主观形状模型。对象检测的常用算法主观形状模型以人脸为标准，提取人脸上68个点，对这些点进行变换，然后去匹配目标。刚才聊了一些传统方法，随着近几年硬件性能的提升以及大数据的广泛的应用，让本来已经沉寂许久的深度学习算法变成了可行。深度学习这个名字是比较抽象，其实可以理解为深度比较高的神经网络，多重神经网络。这样会比较贴切。简单介绍一下神经网络，左边和右边两张图就是神经网络。一般谈及神经网络的，是不说输入层的，所以左边是两层神经网络，右边是三层。四、图像处理的爆发——深度学习方法下面有两个深度学习的网络，所谓的深度学习实际就是深度神经网络，叫深度神经网络大家更容易理解。左边那个是一个两层的神经网络，这里要解释一下，我们一般说神经网络的层数是不算输入层的。输入层就是你输入信号的地方，将中间过程的结果进行汇总，若做分类它会把它进行归类的一层。这其中学过统计或者相关的同学都会用的逻辑回归很有意思传统的机器学习算法和深度学习算法很有关系。底下写了一行字（英文），这两者其实都是很特殊的单层神经网络，所以深度学习并不是遥不可及的，它的由来是很有逻辑性的，是从深度算法中间慢慢剥离出来的。说完简单的神经网络再来谈谈复杂的。在实际使用的时候，不一定是简单的一两层，很有可能是多层的。比如这个图上面至少有三层，实际上我们使用的神经网络是很复杂的。不是像上面这张图那么简单。除了刚才看到的哪些三角形或者金字塔形的神经网络，神经网络是有非常多种的，比如感知器，FF，RBF，使用的场景都不太一样。所以神经网络研究者往往会根据场景选不同的网络，对网络进行一些比较，有的时候也可以解决相似的问题。深度学习研究的方向跟传统学习的区别，可能在网络上会更加的铸磨，包括怎么调整中间多加一层，少加一层，在这个方向做一些测试。在图像识别领域应用比较广的神经网络是CNN，即卷积神经网络。这个图上是一个非常标准的，或者说非常初级的CNN的网络，包括比如像卷积层，卷积层一般是用来做图像的特征学习的。最后会有一个全链接层，把所有前一层的数据进行全链接。自动做分类。这个想法其实来源于传统的机器学习，最后会有一个层去做分类。在CNN之后非常聪明的科学家们就发现了很多可以更加优化的点，所以已经很少有人单纯用这个CNN做训练了。目前比较流行的做图像分割的还有很多基于 CNN 的新网络。比如在 CNN 基础上大家又加入了一个叫 Region Proposal Network 的东西，利用它们可以去优化传统 CNN 中间的一些数据的走向。Faster-RCNN 不是一蹴而就的，它是从RCNN中借鉴了 SPPNET 的一些特性，然后发明了 Fast-RCNN，又在 Fast-RCNN 的基础上进一步的优化变成了 Faster-RCNN。即便到了Faster R-CNN这个领域也远远没有走到尽头。我们一直做对象检测的时候都是采用，要尽量的缩小范围的思路。每一个上面提到的算法都能找到一两篇非常好的论文，大家可以了解一下这个算法是怎么演变过来的。我们的目的是用分类的方法一次性把一个图片里面的信息全部理解，就出现了右边的算法。接下来谈一下在有了一个模型算法以后，应如何做一些应用。讲几个腾讯云这边支持过的AI应用的例子。首先是五四青年的活动，它是解决图像匹配的一个活动。首先我们将民国时候的老照片作为训练数据，对它进行提取，和标注数据，对每个照片都进行标注，生成一个模型。大家在玩这个游戏的时候会传一张照片测试数据，等特征提取完，模型会返回一个它的分类，这个分数不是执行度，不可以完全参考。而是会选择一个分数最大的值返回到前端，生成一个页面，这就是整个流程。第二个应用也比较流行：人脸融合的一个应用。类似军装照这类，原理上跟刚才的PPT有点类似，首先会对图像的人脸部分进行关键点的定位，将特征提取出来。因为角度也是会影响很多的匹配效果，所以要对图片进行一定程度的旋转，把它与模版图进行统一化。下一步将上传图片的人脸部分根据特征值抠出来。跟我们的模版图进行融合。由于上传的照片各种光线角度不完全一样，还要对图片进行一些优化，将光影、曲线调得比较平缓，这样就可以呈现一个非常好的效果。这是我之前在斯坦福的时候做的一个项目，对图片的内容进行描述生成一个故事。我们做了一个模型是可以任意的帮助用户根据图片生成不同类型的故事。比如可以是浪漫小说，可以是科幻小说，不同的训练集可以根据情况调整大家想做一些自己项目的时候也可以像我们一样采用开源数据库。资源很多，不会成为大家研究的瓶颈。五、解析云端AI能力支撑聊过刚才的这些部分，背后是什么样的？如果我写了一个模型做了一个有趣的应用，是不是我就可以把它放到网上大家就可以玩了？其实不是这么简单。如果只是简单的让我们把它放到一个网站托管，为了支撑这么庞大的一个系统我们需要做些什么？应该采用云服务，尽量不在我们的物理集上部署。比如采用这个图里面提出的静态应用加速，虚拟机，对象储存，以及GPU云服务器加速我们出结果的速度。整个系统里面用到了跨地域的负载均衡，弹性降级。上面提到的应用一般都会有短时间达到特别高的访问峰值，持续一段时间会有很快的回落。负载均衡会把一些机器释放掉降低成本。这是一整套的商业化的体系的运作。再介绍一下腾讯云人工智能目前的提供的服务领域，包括各种人脸合成，身份证识别，智能监控，人脸轧机还有智能语音方面上的：关键词搜索，语音合成等等。同时还有一些机器学习的平台可以帮助大家快速的去实现一些模型。同时还有大数据，可以做海量数据的数据挖掘。还有其他一些底层的服务，包括CPU等等。相当于像物理机，但是完全可以交给物理云去托管。介绍一下人脸识别的例子。人脸识比较常见的几个应用，比如微信，人脸合成只需知道你的身份。可靠性高达99.5%。人脸检测又分静态与活体的检测，静态检测只是比对你的特征与库里面是否一致。但假设有人用一些视频或照片来攻击这个系统，可以采取包括以下几种模式的一整套活体检测的方法：一是让你读一段文字或者做一些动作，或者是屏幕用不同的光线去照射脸部，利用反射光的一些特点来对脸部做区分，3D建模，从而判断是视频还是真人。除此之外还可采用具有3D建模能力的摄像头，打一道光到人脸上，再将反射光收集起来。目前这些技术我们的这些技术都已比较成熟。刚才讲的是1比1的人脸合成，除此之外还有用于大型场所管理的1：N的。大家如果有机会，如果去深圳可以去腾讯的滨海大厦参观一下，整套系统都是用这个搭建的。另外我们提供针对场景的云智智能视频管理平台。同时提供设备管理以及视频监控、对外接口等一整套服务。再来说下文字OCR，后面冀博士会详细的介绍这部分。目前我们能使用的领域也非常广泛，包括名片识别，传单识别，快递单识别等等。到了技能进阶这个部分，很多人觉得做AI并不需要做算法，其实不然。首先从有想法到落地分非常多的环节。算法是非常重要的，只是相关性没有那么强。从AI算法的角度来讲，首先要打好数学基础，若想继续发展还需要积累一定的算法知识。同时锻炼自己对最新算法学术成果吸收能力，读论文实现算法，整套技能的提高。这就要求在对算法一定要有了解的同时，要对算法有比较强的封装能力。有了一个算法怎样去封装，让它根据场景达到最优的效果。除此之外还可以做AI产品开发和应用，但这就要求对目标的AI应用场景比较了解。用户什么样，他们对于AI产品的需求是怎么样的，怎样打磨产品让他们可以简单的使用复杂的这些技术。这一整套其实都需要很多的思考。六、技能进阶建议如果我们在 AI 这个方向上想有所进步的话，我们应该怎么做？右边有一个金字塔，并不是说AI算法是它们的最高级。只是想表达如果想要做成这件事，需要的整个团队的大小，首先要有AI算法的专家，同时要有更多工程实践的人，然后还要有更多的产品开发人员把它打磨成产品。这点上来讲像亚马逊的公司做得就非常好，每次都会寻找一个场景然后去营造场景做AI，而不是单纯从技术角度出发。算法研究方面我们要做什么？首先是要打好比较强的数学基础。因为机器学习中间大量的用到了比大学高等数学更复杂的数学知识，这些知识需要大家早做研究打好基础，这就需要读很多论文。同时还要锻炼自己对新的学术成果的理解和吸收能力，像刚才提到了一个神经网络图像的分类问题，实际上，短短的十年时间实现了那么多不同网络的进化，每一个新的网络提出了，甚至还没有发表，只是在预发表库里面，大家就要很快的去吸收理解它，想把它转化成可以运行的模型，这个是要反复锻炼的。第二块是工程实现方面，如果想从事这方面首先要加强自己逻辑算法封装的能力，尽量锻炼自己对模型的训练和优化能力，这块会需要大家把一个设计好的算法给落实到代码上，不断的去调整优化实现最好的结果，这个过程也是需要反复磨炼的。最后一个方向是产品应用，这个首先大家要有一定的开发能力，不管是移动开发还是 Web 开发，同时要提升自己 AI 产品场景的理解和应用。实际上很多AI产品跟传统的产品是有很大的理解上的区别，大家可能要更新自己的这种想法，多去看一些 AI 产品目前是怎么做的，有没有好的点子，多去试用体会。同时如果我们想把一个 AI 的模型变成百万级、千万级用户使用的流行产品，我们还需要有系统构建能力和优化能力。所以在这三个领域都可以思考。关于怎么去学习计算机视觉的一些知识，这里有一些资料。首先斯坦福大学的一些课程是非常好的，可以利用这个作为切入点，把关键的知识点搞懂。如果不喜欢看电脑的话也可以看其他的几本书。都是我觉得不错的不同的作者写的书。右边是一些比较常见的，每天都会用到。底层还有一些打包好的更高级的书。Q&AQ：在视觉这块的话前期讲过焦躁问题。如果是人为的加入噪音的话，处理的话是不是特别麻烦？它的效果和原对比有什么区别？A：降噪其实是一个很复杂的问题，就像你说的这种情况，如果噪音不是自然噪音，是手动添加噪音，你要找它的模式才能去比较有针对性的调整，我没有对这块有特别深的研究。我的感觉是你可以用一些深度学习比较发达，利用深度学习的模型去做。比如去马赛克这块，打造马赛克的一些照片，利用深度学习算法就能恢复到原照片，或者把一些第一分辨率的照片变成高分辨率的。Q：我想请问的问题是，PPT中关于多模态识别那一块，现在无论是无人驾驶还是多模态识别都是有（英文）融入到图片当中，我就想问一下这块有那些工作是可以做的？比如说一个图片里面一个男的和女的，男的拿着戒指，这是个求婚的场景。从图片中的内容里面它是体现不了特征信息的，就是关于这块的。A：由于这个图片它可能主要的内容不是那么明显，其实这难点是在于语义识别这块。你要能提取戒指这个关键词可能跟什么有关。这就变成自然语言识别这块的事了，应该有一个词库，可能去做搜索。所以这块我觉得难点不在于图像而在于后续。问答AI开发的语言要求是什么？相关阅读冀永楠：OCR的应用锦集及背后技术吴琛：智慧工地——履约考勤系统的应用实践江铖:乳腺癌识别By AI此文已由作者授权腾讯云+社区发布，原文链接：https://cloud.tencent.com/developer/article/1183692?fromSource=waitui搜索关注公众号「云加社区」，第一时间获取技术干货，关注后回复1024 送你一份技术课程大礼包！海量技术实践经验，尽在云加社区！"}
{"content2":"人工智能研究趋势直到本世纪初，人工智能的吸引点主要在于它所传递的承诺，但在过去的十五年里，大多这样的承诺已经得到兑现。人工智能技术已经充斥了我们的生活。当它们成为了社会的一股中心力量时，该领域正在从仅仅建立智能系统，转向了建立有人类意识的、值得信赖的智能系统。几个因素加速了人工智能革命。其中最重要的是机器学习的成熟，部分由云计算资源和广泛普及的、基于 Web 的数据收集所支持。机器学习已经被「深度学习（deep learning）」急剧地向前推进了，后者是一种利用被称作反向传播的方法所训练的适应性人工神经网络的一种形式。信息处理算法的这种性能飞跃一直伴随着用于基本操作的硬件技术的显著进步，比如感觉、感知和目标识别。数据驱动型产品的新平台和新市场，以及发现新产品和新市场的经济激励机制，也都促进了人工智能驱动型技术的问世。所有这些趋势都推动着下文中所描述的「热门」研究领域。这种编辑只是想要通过某个或另一个度量标准来反映目前比其他领域得到更大关注的领域。它们不一定比其他领域更重要或更有价值。事实上目前的一些「热门」领域在过去几年中并不怎么流行，而其他领域可能在未来会以类似的方式重新出现。大规模机器学习许多机器学习的基本问题（如监督和非监督学习）是很好理解的。目前努力的一个重点是将现有算法扩展到更庞大的数据集上。例如鉴于传统方法能够负担得起若干遍数据集的处理，现代方法是为单次处理所设计；某些情况只认同非线性方法（那些只关注一部分数据的方法）。深度学习成功训练卷积神经网络的能力非常有益于计算机视觉领域，比如目标识别、视频标签、行为识别和几个相关变体的应用。深度学习也在大举进军感知方面的其他领域，如音频、语音和自然语言处理。强化学习鉴于传统机器学习主要关注于模式挖掘，强化学习将重点转移到决策中，这种技术将有助于促进人工智能在现实世界中更深入地进入相关研究和实践领域。作为一种经验驱动型的序贯决策框架，强化学习已经存在了几十年，但是这个方法在实践中没有取得很大成功，主要是由于表征和缩放的问题。然而深度学习的出现为强化学习提供了「一贴强心剂」。由谷歌 DeepMind 开发的计算机程序 AlphaGo 在五次对抗比赛中击败了人类围棋冠军，它最近所取得的成功在很大程度上要归功于强化学习。AlphaGo 是通过使用一个人类专家数据库来初始化一个自动代理的方法被训练的，但随后提炼的方法是通过大量地自我对抗游戏以及应用强化学习。机器人至少在静态环境中，机器人导航在很大程度上被解决了。目前的努力是在考虑如何训练机器人以泛型的、预测性的方式与周围世界进行交互。互动环境中产生的一个自然要求是操纵，这是当下所感兴趣的另一个话题。深度学习革命只是刚开始影响机器人，这在很大程度上是因为要获得大的标记数据集还很困难，这些数据集已推动了其他基于学习的人工智能领域。免去了标记数据需求的强化学习可能会有助于弥合这一差距，但是它要求系统在没有错误地伤害自己或其他系统的情况下能够安全地探索出一个政策空间。在可信赖的机器感知方面的进步，包括计算机视觉、力和触觉感知，其中大部分将由机器学习驱动，它们将继续成为推进机器人能力的关键。计算机视觉计算机视觉是目前最突出的机器感知形式。它是受深度学习的兴起影响最大的人工智能子领域。直到几年前，支持向量机还是大多视觉分类任务所选择的方法。但是特别是在 GPU 中的大规模计算的汇合，使得更大数据集的可获得性（尤其是通过互联网）以及神经网络算法的改进导致了基准任务中能的显著提高（比如 ImageNet 中的分类器）。计算机首次能够比人类更好地执行一些（狭义定义的）视觉分类任务。目前的研究多是关注于为图像和视频自动添加字幕。自然语言处理自然语言处理是另一个通常与自动语音识别一同被当做非常活跃的机器感知领域。它很快成为一种拥有大数据集的主流语言商品。谷歌宣布目前其 20% 的手机查询都是通过语音进行的，并且最近的演示已经证明了实时翻译的可能性。现在研究正在转向发展精致而能干的系统，这些系统能够通过对话而不只是响应程式化的要求来与人互动。协同系统协同系统方面进行的是对模型和算法的研究，用以帮助开发能够与其他系统和人类协同工作的自主系统。该研究依赖于开发正式的协作模型，并学习让系统成为有效合作伙伴所需的能力。能够利用人类和机器的互补优势的应用正吸引到越来越多的兴趣——对人类来说可以帮助人工智能系统克服其局限性，对代理来说可以扩大人类的能力和活动。众包和人类计算在完成许多任务方面由于人类的能力是优于自动化方法的，因而在众包和人类计算方面，通过利用人类智力来解决那些计算机无法单独解决好的问题，该领域研究调查了增强计算机系统的方法，这项研究的提出仅仅是在大约 15 年前，现在它已经在人工智能领域确立了自己的存在。最有名的众包例子是维基百科，它是一个由网络公民维护和更新的知识库，并且在规模上和深度上远远超越了传统编译的信息源，比如百科全书和词典。众包专注于设计出创新的方式来利用人类智力。Citizen 科学平台激发志愿者去解决科学问题，而诸如亚马逊的 Mechanical Turk 等有偿众包平台，则提供对所需要的人类智力的自动访问。通过短时间内收集大量标记训练数据和/或人机交互数据，该领域的工作促进了人工智能的其它分支学科的进步，包括计算机视觉和自然语言处理。基于人类和机器的不同能力和成本，目前的研究成果探索出了它们之间理想的任务分离。算法博弈理论与 (基于) 计算机 (统计技术的) 社会选择包括激励结构、人工智能的经济和社会计算维度吸引到了新的关注。自 20 世纪 80 年代初以来，分布式人工智能和多代理（multi-agent）系统就已经被研究了，于 20 世纪 90 年代末开始有显著起色，并由互联网所加速。一个自然的要求是系统能够处理潜在的不恰当激励，包括自己所感兴趣的人类参加者或公司，以及自动化的、基于人工智能的、代表它们的代理。备受关注的主题包括计算机制设计（computational mechanism design）（一种激励设计的经济理论，它寻求激励兼容的系统，其中输入会被如实报告）、(基于) 计算机 (统计技术的) 社会选择（computational social choice）（一种有关如何为替代品排列顺序的理论）、激励对齐信息获取（incentive aligned information elicitation）（预测市场、评分规则、同行预测）和算法博弈理论（algorithmic game theory）（市场、网络游戏和室内游戏的平衡，比如poker——它在近几年通过抽象技术和无遗憾学习（no-regret learning）已经取得了显著的进步）。物联网（IoT）越来越多的研究机构致力于这样一个想法：一系列设备可以相互连接以收集和分享它们的感官信息。这些设备可以包括家电、汽车、建筑、相机和其他东西。虽然这就是一个技术和无线网络连接设备的问题，人工智能可以为了智能的、有用的目的去处理和使用所产生的大量数据。目前这些设备使用的是令人眼花缭乱的各种不兼容的通信协议。人工智能可以帮助克服这个「巴别塔」。神经形态计算传统计算机执行计算的冯诺依曼模型，它分离了输入/输出、指令处理和存储器模块。随着深度神经网络在一系列任务中的成功，制造商正在积极追求计算的替代模型——特别是那些受到生物神经网络所启发的——为了提高硬件的效率和计算系统的稳定性的模型。目前这种「神经形态的（neuromorphic）」计算机尚未清楚地显示出巨大成功，而是刚开始有望实现商业化。但可能它们在不久的将来会变成寻常事物（即使仅作为冯诺依曼所增加的兄弟姐妹们）。深度神经网络在应用景观中已经激起了异常波动。当这些网络可以在专门的神经形态硬件上被训练和被执行，而不是像今天这样在标准的冯诺依曼结构中被模拟时，一个更大的波动可能会到来。总体趋势以及人工智能研究的未来数据驱动型范式的巨大成功取代了传统的人工智能范式。诸如定理证明、基于逻辑的知识表征与推理，这些程序获得的关注度在降低，部分原因是与现实世界基础相连接的持续挑战。规划（Planning）在七十和八十年代是人工智能研究的一根支柱，也受到了后期较少的关注，部分原因是它强烈依赖于建模假设，难以在实际的应用中得到满足。基于模型的方法——比如视觉方面基于物理的方法和机器人技术中的传统控制与制图——已经有很大一部分让位于通过检测手边任务的动作结果来实现闭环的数据驱动型方法。即使最近非常受欢迎的贝叶斯推理和图形模式似乎也正在失宠，被数据和深度学习显著成果的洪流所淹没。在接下来的十五年中，会有更多关注集中在针对人类意识系统的开发上，这意味着它们是明确按照要与之互动的人类特点来进行建模与设计的。很多人的兴趣点在于试图找到新的、创造性的方法来开发互动和可扩展的方式来教机器人。此外在考虑社会和经济维度的人工智能时，物联网型的系统——设备和云——正变得越来越受欢迎。在未来的几年中，对人类安全的、新的感知/目标识别能力和机器人平台将会增加，以及数据驱动型产品数量与其市场规模将会变大。当从业者意识到纯粹的端到端深度学习方法的不可避免的局限性时，会重新出现一些人工智能的传统形式。我们不鼓励年轻的研究人员重新发明理论，而是在人工智能领域以及相关领域（比如控制理论、认知科学和心理学）的第一个五十年期间，保持对于该领域多方面显著进展的觉察。"}
{"content2":"作者：Andrew_____来源：CSDN原文：https://blog.csdn.net/Maximize1/article/details/82562090文章目录1 Baidu 计算机视觉算法研发工程师2 海康威视2.1 图像算法工程师（图像处理\\视频编解码\\3D视觉）研究院2.2 AI算法工程师（计算机视觉\\机器学习\\模式识别\\人脸识别\\自然语言处理\\语音识别\\音频处理）研究院2.3 图像算法工程师-产品研发中心2.4 图像算法工程师-微影传感3 华为3.1 计算机视觉算法工程师4 商汤4.1 计算机视觉研究员4.2 见习计算机视觉研究员4.3 计算机视觉算法工程师1 Baidu 计算机视觉算法研发工程师时间：2018.09.09职位：计算机视觉算法研发工程师工作地点: 深圳市招聘人数: 5工作职责:负责百度计算机视觉相关的技术(含无人车自定位、地图重建、图像点云物体识别分类算法）、系统、产品的研发工作，包括但不限于：移动图像技术应用、图像内容搜索、人脸检测识别、图像分类标注、OCR、增强现实、图像质量评价、图像处理、点云视觉定位、三维视觉重建、物体分类识别等；招聘要求：掌握计算机视觉和图像处理基本算法，并在如下一个或多个相关方向有较深入研究：移动图像技术应用、图像内容搜索、人脸检测识别、图像分类标注、OCR、增强现实、图像质量评价、图像处理等；了解机器学习基本算法，如分类、回归、聚类、概率模型等熟悉和掌握C/C++和脚本语言编程(如Shell, Python, Perl等）具有良好的沟通能力，和良好的团队合作精神有相关实践经验者优先2 海康威视时间：2018.09.102.1 图像算法工程师（图像处理\\视频编解码\\3D视觉）研究院任职资格：有较好数学基础，较强的图像处理算法设计与开发能力；掌握c/c++、python等编程语言，能编程实现相关算法；较强的论文检索，英文专业文献阅读能力；良好的沟通与协作能力。具备以下经验之一者优先：1、 了解深度学习，在图像处理领域进行过深度学习相关研究与实现；2、 具备较丰富的图像处理、计算成像、3D视觉、视频编解码研究经历；3、 具备较丰富的雷达信号处理研究经历。2.2 AI算法工程师（计算机视觉\\机器学习\\模式识别\\人脸识别\\自然语言处理\\语音识别\\音频处理）研究院工作职责:负责模式识别、计算机视觉、深度学习、人脸识别、自然语言处理、语音识别、音频处理相关算法的研究与实现。任职资格：具有较好的数学功底，对模式识别、机器学习有较深入的理解与认识；在下述任一领域有研究经历：计算机视觉、人脸识别、语音识别、自然语言理解、麦克阵列；熟练掌握C\\C++\\Python，具备扎实的编程基础；熟练使用Caffe、Tensorflow、PyTorch及其它开源框架者优先；有实际项目开发或参加竞赛经验者优先；在CVPR、ICCV、CSVT、TMM、TIP、PAMI等国际顶级会议或期刊上发表过文章者优先；2.3 图像算法工程师-产品研发中心工作职责:负责图像处理模块的算法设计，并在相关平台上实现。任职资格：精通C语言，掌握Matlab，熟悉OpenCV 等图像工具；熟悉图像处理或模式识别相关算法处理；对红外图像的非均匀性校正、细节增强、去噪、测温等相关算法有一定了解；具有较强的文献获取能力，英文文献检索和阅读能力；有红外图像算法处理经验者优先。2.4 图像算法工程师-微影传感工作职责:负责图像处理模块的算法设计，并在相关平台上实现。任职资格：精通C语言，掌握Matlab，熟悉OpenCV 等图像工具；熟悉图像处理或模式识别相关算法处理；对红外图像的非均匀性校正、细节增强、去噪、测温等相关算法有一定了解；具有较强的文献获取能力，英文文献检索和阅读能力；有红外图像算法处理经验者优先。3 华为3.1 计算机视觉算法工程师工作职责:深度学习算法的研发，特别是在计算机视觉领域的应用研究，以及模型加速、模型加密、模型量化等研发；研究基于 deep learning 的人的属性或人的附属物体类型及其属性的识别算法；计算机视觉算法研发与产品开发，包括但不限于：人脸识别、人脸属性、人体重识别识别；目标检测、目标分类、目标属性识别；图像分割、图像解说、图形对象识别、图像生成、图片审核、图像优化、图像聚类；目标检测、识别、跟踪；视频分割、视频语义提取；OCR、文字检测；语义分割、超分辨率；任职资格：熟练掌握C/C++、Python等语言，掌握一种以上深度学习框架，掌握Opencv等基础库的使用；掌握计算机视觉基础知识、深度学习、经典机器学习等。有实践经验者优先；具备一定的科研能力，能快速理解paper，具备算法创新能力者优先；具备对现实问题抽象出数学模型的逻辑分析能力，并能够求解数学问题；remark：华为的所有岗位入职时随机分配，不宜；华为的很多岗位偏通信，不宜；2012实验室、消费者 BU (business unit) 均有有视觉岗；2012实验室从事基础研究，部门利润低、工资外收入低；消费者 BU 市场发展空间不大；（phq）4 商汤4.1 计算机视觉研究员工作职责:负责计算机视觉（包括深度学习）算法的开发与性能提升，负责下述研究课题中的一项或多项，包括但不限于：检测、跟踪、分类、语义分割、深度估计、图像处理、视频分析、3D图形与视觉、强化学习、自然语言理解、机器人技术相关算法等；推动计算机视觉&机器学习算法在众多实际应用领域的性能优化和落地；提出和实现最前沿的算法，保持算法在工业界和学术界的领先。任职资格：熟练掌握计算机视觉的基本方法；较强的算法实现能力，熟练掌握 C/C++ 编程，熟悉 Shell/Python/Matlab 编程之一;算法基础扎实，掌握常见设计模式，具有良好代码风格和质量意识，能独立完成算法模块设计、开发和测试；满足以下一个或多个条件的，优先考虑：在国际顶尖会议或期刊（包括但不限于CVPR, ICCV, ECCV, NIPS, ICML, AAAI, TPAMI, IJCV等）上发表过论文；有较强的代码能力者优先，获得过ACM或其他商业代码竞赛的荣誉，如ACM区预赛金牌、NOI银牌以上、百度之星决赛等，或代码开源在GitHub上并有较大影响；有较强的比赛经验者或者在重要数据集的排行榜上排名靠前的优先，这些比赛包括但不限于ImageNet等相关竞赛、Kaggle等一些国内外商业比赛、亚太大学生/国际空中/Robomaster机器人大赛等；有较丰富的相关经验者优先，比如有一年以上在人工智能领域公司进行计算机视觉&机器学习方面实习的经验，或来自国内外计算机视觉/机器学习/计算机图形学/数据挖掘等领域内知名实验室。4.2 见习计算机视觉研究员工作职责:负责计算机视觉相关算法的开发与性能提升，涉及的问题包括但不限于：检测、跟踪、分类、语义分割、强化学习、3D视觉和图像处理等。推动计算机视觉算法和深度学习在智能视频，安防监控，手机，自动驾驶，OCR等众多实际应用领域的性能优化和落地。提出和实现最前沿的算法，保持算法在工业界和学术界的领先。任职资格：掌握机器学习（特别是深度学习）和计算机视觉的基本方法。优秀的分析问题和解决问题的能力，对解决具有挑战性的问题充满激情。较强的算法实现能力，熟练掌握 C/C++ 编程，熟悉 Shell/Python/Matlab 编程。有较强的代码能力优先，获得过ACM或其他商业代码竞赛的荣誉，如ACM区预赛金牌、NOI银牌以上、百度之星决赛等；或代码开源在github上并有较大影响。有较强的学术比赛经验或者在重要数据集的Leaderboard上排名靠前，比如ImageNet等学术数据集或者Kaggle等一些国内外商业比赛。4.3 计算机视觉算法工程师工作职责:负责基于深度学习的视觉算法SDK等算法产品的研发、优化和维护；配合视觉计算领域的顶级研究人员，参与图像视频算法的设计研发，负责底层算法的工程化、封装、优化、测试等；参与图像视觉应用软件和相关自动化工具的开发和维护。任职资格：熟练掌握C/C++，算法基础扎实，具有良好代码风格和质量意识；具有图像/视频处理相关背景知识和开发经验；满足以下条件者优先：有深度学习相关研发经验者优先；了解GPU并行计算，熟悉CUDA编程或OpenCL编程；熟悉 X86、ARM 的多线程和SIMD代码优化技术；具有Neon或OpenCL或OpenGL等移动端性能优化经验；学习能力强，具备优秀的分析和解决问题能力，具备良好的沟通能力和团队合作意识。---------------------作者：Andrew_____来源：CSDN原文：https://blog.csdn.net/Maximize1/article/details/82562090"}
{"content2":"Matlab计算机视觉/图像处理工具箱推荐转载http://cvnote.info/matlab-cv-ip-toolbox/计算机视觉/图像处理研究中经常要用到Matlab，虽然其自带了图像处理和计算机视觉的许多功能，但是术业有专攻，在进行深入的视觉算法研究的时候Matlab的自带功能难免会不够用。本文收集了一些比较优秀的Matlab计算机视觉工具箱，希望能对国内的研究者有所帮助。VLFeat：著名而常用项目网站：http://www.vlfeat.org许可证：BSD著名的计算机视觉/图像处理开源项目，知名度应该不必OpenCV低太多，曾获ACM Open Source Software Competition 2010一等奖。使用C语言编写，提供C语言和Matlab两种接口。实现了大量计算机视觉算法，包括：常用图像处理功能，包括颜色空间变换、几何变换（作为Matlab的补充），常用机器学习算法，包括GMM、SVM、KMeans等，常用的图像处理的plot工具。特征提取，包括 Covariant detectors, HOG, SIFT,MSER等。VLFeat提供了一个vl_covdet() 函数作为框架，可以方便的统一所谓“co-variant feature detectors”，包括了DoG, Harris-Affine, Harris-Laplace并且可以提取SIFT或raw patches描述子。超像素（Superpixel）分割，包括常用的Quick shift, SLIC算法等高级聚类算法，比如整数KMeans：Integer k-means (IKM)、hierarchical version of integer k-means (HIKM)，基于互信息自动判定聚类类数的算法Agglomerative Information Bottleneck (AIB) algorithm等高维特曾匹配算法，随机KD树Randomized kd-trees可以在这里查看VLFeat完整的功能列表。（欢迎访问计算机视觉研究笔记http://cvnote.info或者关注新浪@cvnote）MexOpenCV：让Matlab支持调用的OpenCV项目网站：http://www.cs.sunysb.edu/~kyamagu/mexopencv/作者Kota Yamaguchi桑是石溪大学（Stony Brook University）的PhD，早些时候自己搞了一套东西把OpenCV的代码编译成Matlab可用的mex接口，然后这个东西迅速火了。今年夏天这个项目被OpenCV吸收为一个模块，貌似是搞了一个Google Summer of Code（GSoC）的项目，最近（大概是9、10月）已经merge到了OpenCV主包，有兴趣的可以到Github的OpenCV库下的module/matlab去玩一下，应该会在10月份的OpenCV 3 alpha里正式发布。现在OpenCV就同时有了Python和Maltab的binding（好强大）。具体的功能就不细说了，既然是OpenCV的binding，当然是可以使用OpenCV的绝大多数算法了。比如这样：1234567%loadanimage(Matlab)I=imread('cameraman.tif');%computetheDFT(OpenCV)If=cv.dft(I,cv.DFT_COMPLEX_OUTPUT);Peter Kovesi的工具箱：轻量好用，侧重图像处理项目网站：http://www.csse.uwa.edu.au/~pk/research/matlabfns/这位Peter大哥目前在The University of Western Australia工作，他自己写了一套Matlab计算机视觉算法，所谓工具箱其实就是许多m文件的集合，全部Matlab实现，无需编译安装，支持Octave（如果没有Matlab的话，有了这个工具箱也可以在Octave下进行图像处理了）。别看这位大哥单枪匹马，人家的工具箱可是相当有名，研究时候需要哪个Matlab的计算机视觉小功能，直接到他家主页上下几个m文件放在自己文件夹就好了。这个工具箱主要以图像处理算法为主，附带一些三维视觉的基本算法，列一些包括的功能：Feature Detection via Phase Congruency，求达人解释这个是啥Spatial Feature Detection，Harris、Canny之类的特征算法Edge Linking and Line Segment Fitting，边缘特征和线特征的各种操作Image Denoising，图像降噪Surface Normals to Surfaces，从法向量积分出表面Scalogram Calculation，求达人解释这个是啥Anisotropic diffusion，著名的保边缘平滑算法Frequency Domain Transformations，傅立叶变换Functions Supporting Projective Geometry，透视几何、三维视觉的一些算法Feature Matching、特征匹配Model Fitting and Robust Estimation、RANSACFingerprint Enhancement，指纹图像增强Interesting Synthetic Images，一些好玩儿的图像生成算法Image Blending，图像融合Colourmaps and colour conversions，颜色空间算法可以在网站上看到全部功能的介绍和下载，非常推荐试一下，也可以学到不少算法。Machine Vision Toolbox：侧重机器视觉、三维视觉项目网站：http://www.petercorke.com/Machine_Vision_Toolbox.html许可证：LGPL以前没有用过这个工具箱，最近发现竟然非常强大，而且和我自己的工作还很相关。这个工具箱侧重机器视觉，作者是另一个Peter，Peter Corke在机器人界很有名，他在2011年写了一本书《Robotics, Vision & Control》介绍了机器视觉相关的颜色、相机模型、三维视觉、控制等研究，并配套这个工具箱。算法包括了大量常用的视觉和图像处理小函数，，这些就不提了，提几个别的工具箱一般没有的功能Bag of words的Matlab实现各种相机模型的实现，包括普通相机、鱼眼相机、Catadioptric相机模型等等。如果你做机器人视觉、挂在各种广角相机的话，这些模型实现会很有用自带简单的相机标定功能对极几何（Epipolar Geomtry）的相关算法函数Plucker坐标的实现，做广义相机模型（Generalized camera model）很有用DIPUM Toolbox：经典教材配套项目地址：http://www.imageprocessingplace.com/DIPUM_Toolbox_2/DIPUM_Toolbox_2.htm冈萨雷斯著名的图像处理教材《数字图像处理》的配套工具包，主要是书中图像处理算法的实现，名气自然是不必说了，网上可以免费下到加密后的p文件放在Matlab下面用，作为图像处理入门的上手玩具。MATLAB Functions for Multiple View Geometry：又一个经典教材配套项目网站：http://www.robots.ox.ac.uk/~vgg/hzbook/code/许可证：MIT又是一本大名鼎鼎的教材《计算机视觉中的多图几何》（Multiple View Geometry in Computer Vision），值得所有做三维视觉的研究者好好研究的书，国内很早就翻译了中文版。作者Zisserman提供了部分书中算法的Matlab实现，是深入理解书中理论的非常好的辅助材料。其他的工具箱DIPImage & DIPLib，提供Matlab和C接口的图像处理功能，比较早，现在估计很少有人用或者知道了吧？Matlab CVPR toolbox，计算机视觉和模式识别相关的Matlab功能，好像没什么人用。相关领域的工具箱，比如做机器学习的、做Markov随机场的等等，以后有机会写一下。特定功能的工具箱，比如相机标定工具箱，这个可推荐的还阵挺多，以后有机会写一下。这个链接里可以找到一些Matlab的开源工具箱。（欢迎访问计算机视觉研究笔记http://cvnote.info或者关注新浪@cvnote）computer vision toolbox, Matlab, MexOpenCV, OpenCV, VLFeatAddress: http://cvnote.info/matlab-cv-ip-toolbox/"}
{"content2":"一、RPN锚框信息生成上文的最后，我们生成了用于计算锚框信息的特征（源代码在inference模式中不进行锚框生成，而是外部生成好feed进网络，training模式下在向前传播时直接生成锚框，不过实际上没什么区别，锚框生成的讲解见『计算机视觉』Mask-RCNN_锚框生成）：rpn_feature_maps = [P2, P3, P4, P5, P6]接下来，我们基于上述特征首先生成锚框的信息，包含每个锚框的前景/背景得分信息及每个锚框的坐标修正信息。接前文主函数，我们初始化rpn model class的对象，并应用于各层特征：# Anchors if mode == \"training\": …… else: anchors = input_anchors # RPN Model, 返回的是keras的Module对象, 注意keras中的Module对象是可call的 rpn = build_rpn_model(config.RPN_ANCHOR_STRIDE, # 1 3 256 len(config.RPN_ANCHOR_RATIOS), config.TOP_DOWN_PYRAMID_SIZE) # Loop through pyramid layers layer_outputs = [] # list of lists for p in rpn_feature_maps: layer_outputs.append(rpn([p])) # 保存各pyramid特征经过RPN之后的结果具体的RPN模块调用函数栈如下，############################################################ # Region Proposal Network (RPN) ############################################################ def rpn_graph(feature_map, anchors_per_location, anchor_stride): \"\"\"Builds the computation graph of Region Proposal Network. feature_map: backbone features [batch, height, width, depth] anchors_per_location: number of anchors per pixel in the feature map anchor_stride: Controls the density of anchors. Typically 1 (anchors for every pixel in the feature map), or 2 (every other pixel). Returns: rpn_class_logits: [batch, H * W * anchors_per_location, 2] Anchor classifier logits (before softmax) rpn_probs: [batch, H * W * anchors_per_location, 2] Anchor classifier probabilities. rpn_bbox: [batch, H * W * anchors_per_location, (dy, dx, log(dh), log(dw))] Deltas to be applied to anchors. \"\"\" # TODO: check if stride of 2 causes alignment(校准,对齐) issues if the feature map # is not even. # Shared convolutional base of the RPN shared = KL.Conv2D(512, (3, 3), padding='same', activation='relu', strides=anchor_stride, name='rpn_conv_shared')(feature_map) # Anchor Score. [batch, height, width, anchors per location * 2]. x = KL.Conv2D(2 * anchors_per_location, (1, 1), padding='valid', activation='linear', name='rpn_class_raw')(shared) # Reshape to [batch, anchors, 2] rpn_class_logits = KL.Lambda( lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x) # Output tensors to a Model must be Keras tensors, 所以下面不行 # rpn_class_logits = tf.reshape(x, [tf.shape(x)[0], -1, 2]) # Softmax on last dimension of BG/FG. rpn_probs = KL.Activation( \"softmax\", name=\"rpn_class_xxx\")(rpn_class_logits) # Bounding box refinement. [batch, H, W, anchors per location * depth] # where depth is [x, y, log(w), log(h)] x = KL.Conv2D(anchors_per_location * 4, (1, 1), padding=\"valid\", activation='linear', name='rpn_bbox_pred')(shared) # Reshape to [batch, anchors, 4] rpn_bbox = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x) return [rpn_class_logits, rpn_probs, rpn_bbox] def build_rpn_model(anchor_stride, anchors_per_location, depth): \"\"\"Builds a Keras model of the Region Proposal Network. It wraps the RPN graph so it can be used multiple times with shared weights. anchors_per_location: number of anchors per pixel in the feature map anchor_stride: Controls the density of anchors. Typically 1 (anchors for every pixel in the feature map), or 2 (every other pixel). depth: Depth of the backbone feature map. Returns a Keras Model object. The model outputs, when called, are: rpn_class_logits: [batch, H * W * anchors_per_location, 2] Anchor classifier logits (before softmax) rpn_probs: [batch, H * W * anchors_per_location, 2] Anchor classifier probabilities. rpn_bbox: [batch, H * W * anchors_per_location, (dy, dx, log(dh), log(dw))] Deltas to be applied to anchors. \"\"\" input_feature_map = KL.Input(shape=[None, None, depth], name=\"input_rpn_feature_map\") # [rpn_class_logits, rpn_probs, rpn_bbox] input_feature_map 3 1 outputs = rpn_graph(input_feature_map, anchors_per_location, anchor_stride) return KM.Model([input_feature_map], outputs, name=\"rpn_model\")接前文主函数，我们将获取的list形式的各层锚框信息进行拼接重组：# Loop through pyramid layers layer_outputs = [] # list of lists for p in rpn_feature_maps: layer_outputs.append(rpn([p])) # 保存各pyramid特征经过RPN之后的结果 # Concatenate layer outputs # Convert from list of lists of level outputs to list of lists # of outputs across levels. # e.g. [[a1, b1, c1], [a2, b2, c2]] => [[a1, a2], [b1, b2], [c1, c2]] output_names = [\"rpn_class_logits\", \"rpn_class\", \"rpn_bbox\"] outputs = list(zip(*layer_outputs)) # [[logits2,……6], [class2,……6], [bbox2,……6]] outputs = [KL.Concatenate(axis=1, name=n)(list(o)) for o, n in zip(outputs, output_names)] # [batch, num_anchors, 2/4] # 其中num_anchors指的是全部特征层上的anchors总数 rpn_class_logits, rpn_class, rpn_bbox = outputs目的很简单，原来的返回值为[(logits2, class2, bbox2), (logits3, class3, bbox3), ……]，首先将之转换为[[logits2,……6], [class2,……6], [bbox2,……6]]，然后将每个小list中的tensor按照第一维度（即anchors维度）拼接，得到三个tensor，每个tensor表明batch中图片对应5个特征层的全部anchors的分类回归信息，即：[batch, anchors, 2分类结果 or (dy, dx, log(dh), log(dw))]。二、Proposal建议区生成上一步我们获取了全部锚框的信息，这里我们的目的是从中挑选指定个数的更可能包含obj的锚框作为建议区域，即我们希望获取在上一步的二分类中前景得分更高的框，同时，由于锚框生成算法的设计，其数量巨大且重叠严重，我们在得分高低的基础上，进一步的希望能够去重（非极大值抑制），这就是proposal生成的目的。接前文主函数，我们用下面的代码进入候选区生成过程，# Generate proposals # Proposals are [batch, N, (y1, x1, y2, x2)] in normalized coordinates # and zero padded. # POST_NMS_ROIS_INFERENCE = 1000 # POST_NMS_ROIS_TRAINING = 2000 proposal_count = config.POST_NMS_ROIS_TRAINING if mode == \"training\"\\ else config.POST_NMS_ROIS_INFERENCE # [IMAGES_PER_GPU, num_rois, (y1, x1, y2, x2)] # IMAGES_PER_GPU取代了batch，之后说的batch都是IMAGES_PER_GPU rpn_rois = ProposalLayer( proposal_count=proposal_count, nms_threshold=config.RPN_NMS_THRESHOLD, # 0.7 name=\"ROI\", config=config)([rpn_class, rpn_bbox, anchors])proposal_count是一个整数，用于指定生成proposal数目，不足时会生成坐标为[0,0,0,0]的空值进行补全。1、初始化ProposalLayer class下面我们来看看ProposalLayer的过程，在初始部分我们获取[rpn_class, rpn_bbox, anchors]三个张量作为参数，class ProposalLayer(KE.Layer): \"\"\"Receives anchor scores and selects a subset to pass as proposals to the second stage. Filtering is done based on anchor scores and non-max suppression to remove overlaps. It also applies bounding box refinement deltas to anchors. Inputs: rpn_probs: [batch, num_anchors, (bg prob, fg prob)] rpn_bbox: [batch, num_anchors, (dy, dx, log(dh), log(dw))] anchors: [batch, num_anchors, (y1, x1, y2, x2)] anchors in normalized coordinates Returns: Proposals in normalized coordinates [batch, rois, (y1, x1, y2, x2)] \"\"\" def __init__(self, proposal_count, nms_threshold, config=None, **kwargs): super(ProposalLayer, self).__init__(**kwargs) self.config = config self.proposal_count = proposal_count self.nms_threshold = nms_threshold def call(self, inputs): # [rpn_class, rpn_bbox, anchors] # Box Scores. Use the foreground class confidence. [batch, num_rois, 2]->[batch, num_rois] scores = inputs[0][:, :, 1] # Box deltas. 记录坐标修正信息：(dy, dx, log(dh), log(dw)). [batch, num_rois, 4] deltas = inputs[1] deltas = deltas * np.reshape(self.config.RPN_BBOX_STD_DEV, [1, 1, 4]) # [ 0.1 0.1 0.2 0.2] # Anchors. 记录坐标信息：(y1, x1, y2, x2). [batch, num_rois, 4] anchors = inputs[2]这里的变量scores = inputs[0][:, :, 1]，即我们只需要全部候选框的前景得分。2、top k锚框筛选然后我们获取前景得分最大的n个候选框，# Improve performance by trimming to top anchors by score # and doing the rest on the smaller subset. pre_nms_limit = tf.minimum(self.config.PRE_NMS_LIMIT, tf.shape(anchors)[1]) # 输入矩阵时输出每一行的top k. [batch, top_k] ix = tf.nn.top_k(scores, pre_nms_limit, sorted=True, name=\"top_anchors\").indices提取top k锚框，我们同时对三个输入进行了提取# batch_slice函数： # # 将batch特征拆分为单张 # # 然后提取指定的张数 # # 使用单张特征处理函数处理，并合并（此时返回的第一维不是输入时的batch，而是上步指定的张数） scores = utils.batch_slice([scores, ix], lambda x, y: tf.gather(x, y), self.config.IMAGES_PER_GPU) deltas = utils.batch_slice([deltas, ix], lambda x, y: tf.gather(x, y), self.config.IMAGES_PER_GPU) pre_nms_anchors = utils.batch_slice([anchors, ix], lambda a, x: tf.gather(a, x), self.config.IMAGES_PER_GPU, names=[\"pre_nms_anchors\"])附录.辅助函数batch_slice其中使用了一个后面也会大量使用的函数：batch_slice，我尝试使用tf的while_loop进行了改写。这个函数将只支持batch为1的函数进行了扩展（实际就是不能有batch维度的函数），tf.gather函数只能进行一维数组的切片，而scares为2维[batch, num_rois]，相对的ix也是二维[batch, top_k]，所以我们需要将两者切片应用函数后将结果拼接。【注】本函数位于util.py而非model.py# ## Batch Slicing # Some custom layers support a batch size of 1 only, and require a lot of work # to support batches greater than 1. This function slices an input tensor # across the batch dimension and feeds batches of size 1. Effectively, # an easy way to support batches > 1 quickly with little code modification. # In the long run, it's more efficient to modify the code to support large # batches and getting rid of this function. Consider this a temporary solution def batch_slice(inputs, graph_fn, batch_size, names=None): \"\"\"Splits inputs into slices and feeds each slice to a copy of the given computation graph and then combines the results. It allows you to run a graph on a batch of inputs even if the graph is written to support one instance only. inputs: list of tensors. All must have the same first dimension length graph_fn: A function that returns a TF tensor that's part of a graph. batch_size: number of slices to divide the data into. names: If provided, assigns names to the resulting tensors. \"\"\" if not isinstance(inputs, list): inputs = [inputs] outputs = [] for i in range(batch_size): inputs_slice = [x[i] for x in inputs] output_slice = graph_fn(*inputs_slice) if not isinstance(output_slice, (tuple, list)): output_slice = [output_slice] outputs.append(output_slice) # 使用tf.while_loop实现循环体代码如下： # import tensorflow as tf # i = 0 # outputs = [] # # def cond(index): # return index < batch_size # 返回bool值 # # def body(index): # index += 1 # inputs_slice = [x[i] for x in inputs] # output_slice = graph_fn(*inputs_slice) # if not isinstance(output_slice, (tuple, list)): # output_slice = [output_slice] # outputs.append(output_slice) # return index # 返回cond需要的判断参数进行下一次判断 # # tf.while_loop(cond, body, [i]) # Change outputs from a list of slices where each is # a list of outputs to a list of outputs and each has # a list of slices # 下面示意中假设每次graph_fn返回两个tensor # [[tensor11, tensor12], [tensor21, tensor22], ……] # ——> [(tensor11, tensor21, ……), (tensor12, tensor22, ……)] zip返回的是多个tuple outputs = list(zip(*outputs)) if names is None: names = [None] * len(outputs) # 一般来讲就是batch维度合并回去（上面的for循环实际是将batch拆分了） result = [tf.stack(o, axis=0, name=n) for o, n in zip(outputs, names)] if len(result) == 1: result = result[0] return result3、锚框坐标初调我们在RPN中获取了全部锚框的坐标回归结果，rpn_bbox：[batch, anchors, (dy, dx, log(dh), log(dw))]，2小节中我们将top k锚框的坐标信息以及top k的回归信息提取了出来，现在我们将之合并（使用RPN回归的结果取修正top k锚框的坐标），# Apply deltas to anchors to get refined anchors. # [IMAGES_PER_GPU, top_k, (y1, x1, y2, x2)] boxes = utils.batch_slice([pre_nms_anchors, deltas], lambda x, y: apply_box_deltas_graph(x, y), self.config.IMAGES_PER_GPU, names=[\"refined_anchors\"])函数如下，def apply_box_deltas_graph(boxes, deltas): \"\"\"Applies the given deltas to the given boxes. boxes: [N, (y1, x1, y2, x2)] boxes to update deltas: [N, (dy, dx, log(dh), log(dw))] refinements to apply \"\"\" # dy = (y_n - y_o)/h_o # dx = (x_n - x_o)/w_o # dh = h_n/h_o # dw = w_n/w_o # Convert to y, x, h, w height = boxes[:, 2] - boxes[:, 0] width = boxes[:, 3] - boxes[:, 1] center_y = boxes[:, 0] + 0.5 * height center_x = boxes[:, 1] + 0.5 * width # Apply deltas center_y += deltas[:, 0] * height center_x += deltas[:, 1] * width height *= tf.exp(deltas[:, 2]) width *= tf.exp(deltas[:, 3]) # Convert back to y1, x1, y2, x2 y1 = center_y - 0.5 * height x1 = center_x - 0.5 * width y2 = y1 + height x2 = x1 + width result = tf.stack([y1, x1, y2, x2], axis=1, name=\"apply_box_deltas_out\") return result自此我们在代码层面认识到了回归结果4个坐标值的真正含义：dy = (y_n - y_o)/h_odx = (x_n - x_o)/w_odh = h_n/h_o #dw = w_n/w_o注意，我们的锚框坐标实际上是位于一个归一化了的图上（SSD也是如此且有过介绍，见『TensorFlow』SSD源码学习_其三：锚框生成，即所有锚框位于一个长宽为1的虚拟画布上），上一步的修正进行之后不再能够保证这一点，所以我们需要切除锚框越界的的部分（即只保留锚框和[0,0,1,1]画布的交集）。# Clip to image boundaries. Since we're in normalized coordinates, # clip to 0..1 range. [IMAGES_PER_GPU, top_k, (y1, x1, y2, x2)] window = np.array([0, 0, 1, 1], dtype=np.float32) boxes = utils.batch_slice(boxes, # boxes来源自anchors, 修正deltas的影响 lambda x: clip_boxes_graph(x, window), self.config.IMAGES_PER_GPU, names=[\"refined_anchors_clipped\"])保留交集函数如下，def clip_boxes_graph(boxes, window): \"\"\" boxes: [N, (y1, x1, y2, x2)] window: [4] in the form y1, x1, y2, x2 \"\"\" # Split wy1, wx1, wy2, wx2 = tf.split(window, 4) y1, x1, y2, x2 = tf.split(boxes, 4, axis=1) # Clip y1 = tf.maximum(tf.minimum(y1, wy2), wy1) x1 = tf.maximum(tf.minimum(x1, wx2), wx1) y2 = tf.maximum(tf.minimum(y2, wy2), wy1) x2 = tf.maximum(tf.minimum(x2, wx2), wx1) clipped = tf.concat([y1, x1, y2, x2], axis=1, name=\"clipped_boxes\") clipped.set_shape((clipped.shape[0], 4)) return clipped4、非极大值抑制最后进行非极大值抑制，确保不会出现过于重复的推荐区域，# Filter out small boxes # According to Xinlei Chen's paper, this reduces detection accuracy # for small objects, so we're skipping it. # Non-max suppression def nms(boxes, scores): \"\"\" 非极大值抑制子函数 :param boxes: [top_k, (y1, x1, y2, x2)] :param scores: [top_k] :return: \"\"\" indices = tf.image.non_max_suppression( boxes, scores, self.proposal_count, # 参数三为最大返回数目 self.nms_threshold, name=\"rpn_non_max_suppression\") proposals = tf.gather(boxes, indices) # Pad if needed, 一旦返回数目不足, 填充(0,0,0,0)直到数目达标 padding = tf.maximum(self.proposal_count - tf.shape(proposals)[0], 0) # 在后面添加全0行 proposals = tf.pad(proposals, [(0, padding), (0, 0)]) return proposals proposals = utils.batch_slice([boxes, scores], nms, self.config.IMAGES_PER_GPU) return proposals # [IMAGES_PER_GPU, proposal_count, (y1, x1, y2, x2)]没错，TensorFlow以经封装好了：tf.image.non_max_suppression至此，我们获取了全部的推荐区域。"}
{"content2":"（1）、WallFlower dataset 【链接】:用于评价背景建模算法的好坏， Ground-truth foreground provided.（2）、Foreground/Background segmentation and Stereo dataset 【链接】:from Microsoft Cambridge.（3）、VISOR: Video Surveillance Online Repositiory 【链接】:大量的视频和路面实况.（4）、3D Photography Dataset 【链接】（5）、Multi-model, multi-camera meeting room dataset 【链接】（6）、Advanced Video and Signal based Surveillance 【链接】:各种用于跟踪和检测的数据集（7）、Caltech image collections 【链接】:用于目标物体检测，分割和分类（8）、INRIA Datasets 【链接】:车辆, 人, 马, 人类行为等（9）、CAVIAR surveillance Dataset 【链接】（10）、Videos for Head Tracking 【链接】（11）、Pedestrian dataset from MIT 【链接】（12）、Shadow detection datasets 【链接】（13）、Flash and non-Flash dataset 【链接】（14）、Experiments on skin region detection and tracking 【链接】:包括一个ground-truthed dataset（15）、MIT Face Dataset 【链接】（16）、MIT Car Datasets 【链接】（17）、MIT Street Scenes 【链接】:CBCL StreetScenes Challenge Framework是一个图像、注释、软件和性能检测的对象集[cars, pedestrians, bicycles, buildings, trees, skies, roads, sidewalks, and stores]（18）、LabelMe Dataset 【链接】:超过150,000已经标注的照片（19）、MuHAVi 【链接】:Multicamera Human Action Video Data，A large body of human action video data using 8 cameras. Includes manually annotated silhouette data. 用于测试人行为的数据集（20）、INRIA Xmas Motion Acquisition Sequences (IXMAS) 【链接】:Multiview dataset for view-invariant human action recognition.（21）、i-LIDS datasets 【链接】:UK Government benchmark datasets for automated surveillance.（22）、The Daimler Pedestrian Detection Benchmark 【链接】:contains 15,560 pedestrian and non-pedestrian samples (image cut-outs) and 6744 additional full images not containing pedestrians for bootstrapping. The test set contains more than 21,790 images with 56,492 pedestrian labels (fully visible or partially occluded), captured from a vehicle in urban traffic.（23）、Stereo Pedestrian Detection Evaluation Dataset 【链接】:a dataset for evaluating pedestrian detection using stereo camera images and video. 用于测试行人检测算法的数据集（24）、Colour video and Thermal infrared datasets 【链接】:Dataset of videos in colour and thermal infrared. Videos are aligned temporally and spatially. Ground-truth for object tracking is provided.（25）、北航数字媒体【链接】:面向室外视频监控的运动目标检测跟踪库（26）、google结果：http://www.cvpapers.com/datasets.htmlhttp://riemenschneider.hayko.at/vision/dataset/http://homepages.inf.ed.ac.uk/cgi/rbf/CVONLINE/entries.pl?TAG363http://www.computervisiononline.com/datasets转自：http://blog.csdn.net/lucky_greenegg/article/details/10241295"}
{"content2":"转自一位MIT牛人的空间文章：总感觉做计算机视觉和显著性检测这一块，数学的储备不够。从大学到现在，课堂上学的和自学的数学其实不算少了，可是在研究的过程中总是发现需要补充新的数学知识。Learning和Vision都是很多种数学的交汇场。看着不同的理论体系的交汇，对于一个researcher来说，往往是非常exciting的enjoyable的事情。不过，这也代表着要充分了解这个领域并且取得有意义的进展是很艰苦的。记得在两年前的一次blog里面，提到过和learning有关的数学。今天看来，我对于数学在这个领域的作用有了新的思考。对于Learning的研究：1、Linear Algebra (线性代数) 和 Statistics (统计学) 是最重要和不可缺少的。2、Calculus (微积分)，只是数学分析体系的基础。3、Partial Differential Equation （偏微分方程)4、Functional Analysis (泛函分析)5、Measure Theory (测度理论)6、Topology（拓扑学)7、Differential Manifold (微分流形)8、Lie Group Theory (李群论)9、Graph Theory（图论)书籍推荐：1. 线性代数 (Linear Algebra)：Introduction to Linear Algebra (3rd Ed.)  by Gilbert Strang.这本书是MIT的线性代数课使用的教材，也是被很多其它大学选用的经典教材。它的难度适中，讲解清晰，重要的是对许多核心的概念讨论得比较透彻。我个人觉得，学习线性代数，最重要的不是去熟练矩阵运算和解方程的方法——这些在实际工作中MATLAB可以代劳，关键的是要深入理解几个基础而又重要的概念：子空间(Subspace)，正交(Orthogonality)，特征值和特征向量(Eigenvalues and eigenvectors)，和线性变换(Linear transform)。从我的角度看来，一本线代教科书的质量，就在于它能否给这些根本概念以足够的重视，能否把它们的联系讲清楚。Strang的这本书在这方面是做得很好的。而且，这本书有个得天独厚的优势。书的作者长期在MIT讲授线性代数课(18.06)，课程的video在MIT的Open courseware网站上有提供。有时间的朋友可以一边看着名师授课的录像，一边对照课本学习或者复习。http://ocw.mit.edu/OcwWeb/Mathematics/18-06Spring-2005/CourseHome/index.htm2. 概率和统计 (Probability and Statistics):概率论和统计的入门教科书很多，我目前也没有特别的推荐。我在这里想介绍的是一本关于多元统计的基础教科书：Applied Multivariate Statistical Analysis (5th Ed.)  by Richard A. Johnson and Dean W. Wichern这本书是我在刚接触向量统计的时候用于学习的，我在香港时做研究的基础就是从此打下了。实验室的一些同学也借用这本书学习向量统计。这本书没有特别追求数学上的深度，而是以通俗易懂的方式讲述主要的基本概念，读起来很舒服，内容也很实用。对于Linear regression, factor analysis, principal component analysis (PCA), and canonical component analysis (CCA)这些Learning中的基本方法也展开了初步的论述。之后就可以进一步深入学习贝叶斯统计和Graphical models。一本理想的书是：Introduction to Graphical Models (draft version).  by M. Jordan and C. Bishop.我不知道这本书是不是已经出版了（不要和Learning in Graphical Models混淆，那是个论文集，不适合初学）。这本书从基本的贝叶斯统计模型出发一直深入到复杂的统计网络的估计和推断，深入浅出，statistical learning的许多重要方面都在此书有清楚论述和详细讲解。MIT内部可以access，至于外面，好像也是有电子版的。3. 分析 (Analysis)：我想大家基本都在大学就学过微积分或者数学分析，深度和广度则随各个学校而异了。这个领域是很多学科的基础，值得推荐的教科书莫过于Principles of Mathematical Analysis, by Walter Rudin有点老，但是绝对经典，深入透彻。缺点就是比较艰深——这是Rudin的书的一贯风格，适合于有一定基础后回头去看。在分析这个方向，接下来就是泛函分析(Functional Analysis)。Introductory Functional Analysis with Applications, by Erwin Kreyszig.适合作为泛函的基础教材，容易切入而不失全面。我特别喜欢它对于谱论和算子理论的特别关注，这对于做learning的研究是特别重要的。Rudin也有一本关于functional analysis的书，那本书在数学上可能更为深刻，但是不易于上手，所讲内容和learning的切合度不如此书。在分析这个方向，还有一个重要的学科是测度理论(Measure theory)，但是我看过的书里面目前还没有感觉有特别值得介绍的。4. 拓扑 (Topology)：在我读过的基本拓扑书各有特色，但是综合而言，我最推崇：Topology (2nd Ed.)  by James Munkres这本书是Munkres教授长期执教MIT拓扑课的心血所凝。对于一般拓扑学(General topology)有全面介绍，而对于代数拓扑(Algebraic topology)也有适度的探讨。此书不需要特别的数学知识就可以开始学习，由浅入深，从最基本的集合论概念（很多书不屑讲这个）到Nagata-Smirnov Theorem和Tychonoff theorem等较深的定理（很多书避开了这个）都覆盖了。讲述方式思想性很强，对于很多定理，除了给出证明过程和引导你思考其背后的原理脉络，很多令人赞叹的亮点——我常读得忘却饥饿，不愿释手。很多习题很有水平。5. 流形理论 (Manifold theory)：对于拓扑和分析有一定把握时，方可开始学习流形理论，否则所学只能流于浮浅。我所使用的书是Introduction to Smooth Manifolds.  by John M. Lee虽然书名有introduction这个单词，但是实际上此书涉入很深，除了讲授了基本的manifold, tangent space,bundle, sub-manifold等，还探讨了诸如纲理论(Category theory)，德拉姆上同调(De Rham cohomology)和积分流形等一些比较高级的专题。对于李群和李代数也有相当多的讨论。行文通俗而又不失严谨，不过对某些记号方式需要熟悉一下。虽然李群论是建基于平滑流形的概念之上，不过，也可能从矩阵出发直接学习李群和李代数——这种方法对于急需使用李群论解决问题的朋友可能更加实用。而且，对于一个问题从不同角度看待也利于加深理解。下面一本书就是这个方向的典范：Lie Groups, Lie Algebras, and Representations: An Elementary Introduction.  by Brian C. Hall此书从开始即从矩阵切入，从代数而非几何角度引入矩阵李群的概念。并通过定义运算的方式建立exponential mapping，并就此引入李代数。这种方式比起传统的通过“左不变向量场(Left-invariant vector field)“的方式定义李代数更容易为人所接受，也更容易揭示李代数的意义。最后，也有专门的论述把这种新的定义方式和传统方式联系起来。————————————————————————————无论是研究Vision, Learning还是其它别的学科，数学终究是根基所在。学好数学是做好研究的基石。学好数学的关键归根结底是自己的努力，但是选择一本好的书还是大有益处的。不同的人有不同的知识背景，思维习惯和研究方向，因此书的选择也因人而异，只求适合自己，不必强求一致。上面的书仅仅是从我个人角度的出发介绍的，我的阅读经历实在非常有限，很可能还有比它们更好的书。"}
{"content2":"看了版上很多贴子，发现很多版友都在问“热门研究方向”、“最新方法”等。有同学建议国内某教授的教材、或者CNKI、或者某些SCI期刊。每当看到这种问题，我都有点纳闷，为什么不去读顶级会议上的论文？我无意否认以上文献的价值，但是在机器学习、计算机视觉和人工智能领域，顶级会议才是王道。国内教材和CNKI上的基本是N年前老掉牙的东西。有人会质疑这些会议都只是EI。是的，这的确非常特殊：在许多其它领域，会议都是盛会，比如society of neuroscience的会议，每次都有上万人参加，带个abstract和poster就可以去。但在所讨论的几个领域，顶级会议的重要性无论怎么强调都不为过。可以从以下几点说明：（1）因为机器学习、计算机视觉和人工智能领域发展非常迅速，新的工作层出不穷，如果把论文投到期刊上，一两年后刊出时就有点out了。因此大部分最新的工作都首先发表在顶级会议上，这些顶级会议完全能反映“热门研究方向”、“最新方法”。（2）很多经典工作大家可能引的是某顶级期刊上的论文，这是因为期刊论文表述得比较完整、实验充分。但实际上很多都是在顶级会议上首发。比如PLSA, Latent Dirichlet Allocation等。（3）如果注意这些领域大牛的pulications，不难发现他们很非常看重这些顶级会议，很多人是80%的会议+20%的期刊。即然大牛们把最新工作发在顶级会议上，有什么理由不去读顶级会议？(1)以下是不完整的列表，但基本覆盖。机器学习顶级会议：NIPS, ICML, UAI, AISTATS;  （期刊：JMLR, ML, Trends in ML, IEEE T-NN）计算机视觉和图像识别：ICCV, CVPR, ECCV;  （期刊：IEEE T-PAMI, IJCV, IEEE T-IP）人工智能：IJCAI, AAAI; （期刊AI）另外相关的还有SIGRAPH, KDD, ACL, SIGIR, WWW等。特别是，如果做机器学习，必须地，把近4年的NIPS, ICML翻几遍；如果做计算机视觉，要把近4年的ICCV, CVPR, NIPS, ICML翻几遍。(2)另外补充一下：大部分顶级会议的论文都能从网上免费下载到，比如CV方面：http://www.cvpapers.com/index.html; NIPS: http://books.nips.cc/;  JMLR(期刊): http://jmlr.csail.mit.edu/papers/;  COLT和ICML(每年度的官网): http://www.cs.mcgill.ca/~colt2009/proceedings.html。希望这些信息对大家有点帮助。(3)说些自己的感受。我的研究方向主要是统计学习和概率图模型，但对计算机视觉和计算神经科学都有涉及，对Data mining和IR也有些了解。这些领域，从方法和模型的角度看，统计模型（包括probabilistic graphical model和statistical learning theory）是主流也是非常有影响力的方法。有个非常明显的趋势：重要的方法和模型最先在NIPS或ICML出现，然后应用到CV,IR和MM。虽然具体问题和应用也很重要，但多关注和结合这些方法也很有意义。对于这个领域的牛人们，以上全是浅显的废话，完全可以无视。欢迎讨论。注:NIPS = Neural Information Processing Systems  https://nips.cc/ICML = International Conference on Machine Learning https://icml.ccUAI(AUAI) =Association for Uncertainty in Artifical Intelligence http://www.auai.org/AISTATS = Artificial Intelligence and Statistics http://www.aistats.org/JMLR = Journal of Machine Learning Research http://jmlr.org/IJCAI = International Joint Conference on Artifical Intelligence http://ijcai.org/AAAI = Association for the Advancement of Aritifical Intelligence http://www.aaai.org/home.html原文地址:http://emuch.net/html/201012/2659795.html"}
{"content2":"周志华点评机器学习会议作者：南京大学周志华老师微博：http://weibo。com/zhouzh2012编者提示：这篇文章写于几年前，但现在看来各会议影响力基本一致，有参考价值。但排名不可能做到完全客观，烦请大家仔细阅读周老师文章后的寄语。说明:纯属个人看法，仅供参考。tier-1的列得较全，tier-2的不太全，tier-3的很不全。同分的按字母序排列。不很严谨地说，tier-1是可以令人羡慕的，tier-2是可以令人尊敬的，由于AI的相关会议非常多，所以能列进tier-3的也是不错的一.     tier-1:̶          IJCAI(1+):International Joint Conference on Artificial Intelligence。AI最好的综合性会议，1969年开始，每两年开一次，奇数年开。因为AI实在太大，所以虽然每届基本上能录100多篇（现在 已经到200多篇了），但分到每个领域就没几篇了，象machine learning、computer vision这么大的领域每次大概也就10篇左右，所以难度很大。不过从录用率上来看倒不太低，基本上20%左右，因为内行人都会掂掂分量，没希望的就别 浪费reviewer的时间了。最近中国大陆投往国际会议的文章象潮水一样，而且因为国内很少有能自己把关的研究组，所以很多会议都在complain说 中国的低质量文章严重妨碍了PC的工作效率。在这种情况下，估计这几年国际会议的录用率都会降下去。另外，以前的IJCAI是没有poster的，03年 开始，为了减少被误杀的好人，增加了2页纸的poster。值得一提的是，IJCAI是由貌似一个公司的”IJCAI Inc。”主办的(当然实际上并不是公司，实际上是个基金会)，每次会议上要发几个奖，其中最重要的两个是IJCAI Research Excellence Award和Computer & Thoughts Award，前者是终身成就奖，每次一个人，基本上是AI的最高奖(有趣的是，以AI为主业拿图灵奖的6位中，有2位还没得到这个奖)，后者是奖给35岁 以下的青年科学家，每次一个人。这两个奖的获奖演说是每次IJCAI的一个重头戏。另外，IJCAI的PC member相当于其他会议的areachair，权力很大，因为是由PC member去找reviewer来审，而不象一般会议的PC member其实就是reviewer。为了制约这种权力，IJCAI的审稿程序是每篇文章分配2位PC member，primary PC member去找3位reviewer，second PC member找一位。̶          AAAI(1):National Conference on Artificial Intelligence美国人工智能学会AAAI的年会。是一个很好的会议，但其档次不稳定，可以给到1+，也可以给到1-或者2+，总的来说我给 它”1″。这是因为它的开法完全受IJCAI制约:每年开，但如果这一年的IJCAI在北美举行，那么就停开。所以，偶数年里因为没有IJCAI，它就是 最好的AI综合性会议，但因为号召力毕竟比IJCAI要小一些，特别是欧洲人捧AAAI场的比IJCAI少得多(其实亚洲人也是)，所以比IJCAI还是 要稍弱一点，基本上在1和1+之间;在奇数年，如果IJCAI不在北美，AAAI自然就变成了比IJCAI低一级的会议(1-或2+)，例如2005年既 有IJCAI又有AAAI，两个会议就进行了协调，使得IJCAI的录用通知时间比AAAI的deadline早那么几天，这样IJCAI落选的文章可以 投往AAAI。在审稿时IJCAI的PCchair也在一直催，说大家一定要快，因为AAAI那边一直在担心IJCAI的录用通知出晚了AAAI就麻烦 了。̶          COLT(1):Annual Conference on Computational Learning Theory。这是计算学习理论最好的会议，ACM主办，每年举行。计算学习理论基本上可以看成理论计算机科学和机器学习的交叉，所以这个会被一些人看成 是理论计算机科学的会而不是AI的会。我一个朋友用一句话对它进行了精彩的刻画:“一小群数学家在开会”。因为COLT的领域比较小，所以每年会议基本上 都是那些人。这里顺便提一件有趣的事，因为最近国内搞的会议太多太滥，而且很多会议都是LNCS/LNAI出论文集，LNCS/LNAI基本上已经被搞臭 了，但很不幸的是，LNCS/LNAI中有一些很好的会议，例如COLT。̶          CVPR(1):IEEE International Conference on Computer Vision and Pattern Recognition. 计算机视觉和模式识别方面最好的会议之一，IEEE主办，每年举行。虽然题目上有计算机视觉，但个人认为它的模式识别味道更重一些。事实上它应该是模式识 别最好的会议，而在计算机视觉方面，还有ICCV与之相当。IEEE一直有个倾向，要把会办成”盛会”，历史上已经有些会被它从quality很好的会办 成”盛会”了。CVPR搞不好也要走这条路。这几年录的文章已经不少了。最近负责CVPR会议的TC的chair发信说，对这个community来说， 让好人被误杀比被坏人漏网更糟糕，所以我们是不是要减少好人被误杀的机会啊?所以我估计明年或者后年的CVPR就要扩招了。̶          ICCV(1):IEEE International Conference on Computer Vision. 介绍CVPR的时候说过了，计算机视觉方面最好的会之一。IEEE主办，每年举行。̶          ICML(1):International Conference on Machine Learning机器学习方面最好的会议之一。现在是IMLS主办，每年举行。参见关于NIPS的介绍。̶          NIPS(1):Annual Conference on Neural Information Processing Systems。神经计算方面最好的会议之一，NIPS主办，每年举行。值得注意的是，这个会每年的举办地都是一样的，以前是美国丹佛，现在是加拿大温哥 华;而且它是年底开会，会开完后第2年才出论文集，也就是说，NIPS’05的论文集是06年出。会议的名字“Advances in Neural Information Processing Systems”，所以，与ICML\\ECML这样的”标准的”机器学习会议不同，NIPS里有相当一部分神经科学的内容，和机器学习有一定的距离。但由 于会议的主体内容是机器学习，或者说与机器学习关系紧密，所以不少人把NIPS看成是机器学习方面最好的会议之一。这个会议基本上控制在Michael Jordan的徒子徒孙手中，所以对Jordan系的人来说，发NIPS并不是难事，一些未必很强的工作也能发上去，但对这个圈子之外的人来说，想发一篇 实在很难，因为留给”外人”的口子很小。所以对Jordan系以外的人来说，发NIPS的难度比ICML更大。换句话说，ICML比较开放，小圈子的影响 不象NIPS那么大，所以北美和欧洲人都认，而NIPS则有些人(特别是一些欧洲人，包括一些大家)坚决不投稿。这对会议本身当然并不是好事，但因为 Jordan系很强大，所以它似乎也不太care。最近IMLS(国际机器学习学会)改选理事，有资格提名的人包括近三年在ICML\\ECML\\COLT 发过文章的人，NIPS则被排除在外了。无论如何，这是一个非常好的会。̶          ACL(1-):Annual Meeting of the Association for Computational Linguistics. 计算语言学/自然语言处理方面最好的会议，ACL(Association of Computational Linguistics)主办，每年开。̶          KR(1-):International Conference on Principles of Knowledge Representation and Reasoning. 知识表示和推理方面最好的会议之一，实际上也是传统AI(即基于逻辑的AI)最好的会议之一。KR Inc。主办，现在是偶数年开。̶          SIGIR(1-):Annual International ACMSIGIR Conference on Research and Development in Information Retrieval信息检索方面最好的会议，ACM主办，每年开。这个会现在小圈子气越来越重。信息检索应该不算AI，不过因为这里面用到机器学习越来越多，最近几年甚至有点机器学习应用会议的味道了，所以把它也列进来。̶          SIGKDD(1-): ACM SIG KDD International Conference on Knowledge Discovery and DataMining。数据挖掘方面最好的会议，ACM主办，每年开。这个会议历史比较短，毕竟，与其他领域相比，数据挖掘还只是个小弟弟甚至小侄儿。在几年前还很难把它列在tier-1里面，一方面是名声远不及其他的top conference响亮，另一方面是相对容易被录用。但现在它被列在tier-1应该是毫无疑问的事情了。̶          UAI(1-): International Conference on Uncertainty in Artificial Intelligence。名字叫”人工智能中的不确定性”，涉及表示\\推理\\学习等很多方面，AUAI(Association of UAI)主办，每年开。*Impactfactor(AccordingtoCiteseer03):IJCAI：1。82(top4。09%)AAAI：1。49(top9。17%)COLT：1。49(top9。25%)ICCV：1。78(top4。75%)ICML：2。12(top1。88%)NIPS：1。06(top20。96%)ACL：1。44(top10。07%)KR：1。76(top4。99%)SIGIR：1。10(top19。08%)Average：1。56(top8。02%)二.             tier-2:̶          AAMAS(2+):International Joint Conferenceon Autonomous Agentsand Multiagent Systems. AAMAS(2+):agent方面最好的会议。但是现在agent已经是一个一般性的概念，几乎所有AI有关的会议上都有这方面的内容，所以AAMAS下降的趋势非常明显。̶          ECCV(2+):European Conference on Computer Vision. ECCV(2+):计算机视觉方面仅次于ICCV的会议，因为这个领域发展很快，有可能升级到1-去。̶          ECML(2+):European Conference on Machine Learning. ECML(2+):机器学习方面仅次于ICML的会议，欧洲人极力捧场，一些人认为它已经是1-了。我保守一点，仍然把它放在2+。因为机器学习发展很快，这个会议的reputation上升非常明显。̶          ICDM(2+):IEEE International Conference on Data Mining. ICDM(2+):数据挖掘方面仅次于SIGKDD的会议，目前和SDM相当。这个会只有5年历史，上升速度之快非常惊人。几年前ICDM还比不上PAKDD，现在已经拉开很大距离了。̶          SDM(2+):SIAM International Conference on Data Mining. SDM(2+):数据挖掘方面仅次于SIGKDD的会议，目前和ICDM相当。SIAM的底子很厚，但在CS里面的影响比ACM和IEEE还是要小，SDM眼看着要被ICDM超过了，但至少目前还是相当的。̶          ICAPS(2):International Conference on Automated Planning and Scheduling. ICAPS(2):人工智能规划方面最好的会议，是由以前的国际和欧洲规划会议合并来的。因为这个领域逐渐变冷清，影响比以前已经小了。̶          ICCBR(2):International Conference on Case-Based Reasoning. ICCBR(2):Case-BasedReasoning方面最好的会议。因为领域不太大，而且一直半冷不热，所以总是停留在2上。̶          COLLING(2):International Conference on Computational Linguistics. COLLING(2):计算语言学/自然语言处理方面仅次于ACL的会，但与ACL的差距比ICCV-ECCV和ICML-ECML大得多。̶          ECAI(2):European Conference on Artificial Intelligence. ECAI(2):欧洲的人工智能综合型会议，历史很久，但因为有IJCAI/AAAI压着，很难往上升。̶          ALT(2-):International Conference on Algorithmic Learning Theory. ALT(2-):有点象COLT的tier-2版，但因为搞计算学习理论的人没多少，做得好的数来数去就那么些group，基本上到COLT去了，所以ALT里面有不少并非计算学习理论的内容。̶          EMNLP(2-):Conference on Empirical Methods in Natural Language Processing. EMNLP(2-):计算语言学/自然语言处理方面一个不错的会。有些人认为与COLLING相当，但我觉得它还是要弱一点。̶          ILP(2-):International Conference on Inductive Logic Programming. ILP(2-):归纳逻辑程序设计方面最好的会议。但因为很多其他会议里都有ILP方面的内容，所以它只能保住2-的位置了。̶          PKDD(2-):European Conference on Principles and Practice of Knowledge Discovery in Databases. PKDD(2-):欧洲的数据挖掘会议，目前在数据挖掘会议里面排第4。欧洲人很想把它抬起来，所以这些年一直和ECML一起捆绑着开，希望能借ECML 把它带起来。但因为ICDM和SDM，这已经不太可能了。所以今年的PKDD和ECML虽然还是一起开，但已经独立审稿了(以前是可以同时投两个会，作者 可以声明优先被哪个会考虑，如果ECML中不了还可以被PKDD接受)。*Impactfactor(AccordingtoCiteseer03):ECCV：1。58(top7。20%)ECML：0。83(top30。63%)ICDM：0。35(top59。86%)ICCBR：0。72(top36。69%)ECAI：0。69(top38。49%)ALT：0。63(top42。91%)ILP：1。06(top20。80%)PKDD：0。50(top51。26%)Average：0。80(top32。02%)三.             tier-3:̶          ACCV(3+):Asian Conference on Computer Vision. ACCV(3+):亚洲的计算机视觉会议，在亚太级别的会议里算很好的了。̶          DS(3+):International Conference on Discovery Science. DS(3+):日本人发起的一个接近数据挖掘的会议。̶          ECIR(3+):European Conference on IR Research. ECIR(3+):欧洲的信息检索会议，前几年还只是英国的信息检索会议。̶          ICTAI(3+):IEEE International Conference on Tools with Artificial Intelligence. ICTAI(3+):IEEE最主要的人工智能会议，偏应用，是被IEEE办烂的一个典型。以前的quality还是不错的，但是办得越久声誉反倒越差了，糟糕的是似乎还在继续下滑，现在其实3+已经不太呆得住了。̶          PAKDD(3+):Pacific-Asia Conference on Knowledge Discovery and Data Mining. PAKDD(3+):亚太数据挖掘会议，目前在数据挖掘会议里排第5。̶          ICANN(3+):International Conference on Artificial Neural Networks. ICANN(3+):欧洲的神经网络会议，从quality来说是神经网络会议中最好的，但这个领域的人不重视会议，在该领域它的重要性不如IJCNN。̶          AJCAI(3):Australian Joint Conference on Artificial Intelligence. AJCAI(3):澳大利亚的综合型人工智能会议，在国家/地区级AI会议中算不错的了。̶          CAI(3):Canadian Conference on Artificial Intelligence. CAI(3):加拿大的综合型人工智能会议，在国家/地区级AI会议中算不错的了。̶          CEC(3):IEEE Congress on Evolutionary Computation. CEC(3):进化计算方面最重要的会议之一，盛会型。IJCNN/CEC/FUZZ-IEEE这三个会议是计算智能或者说软计算方面最重要的会议，它们经常一起开，这时就叫WCCI(WorldCongressonComputationalIntelligence)。但这个领域和CS其他分支不太一样，倒是和其他学科相似，只重视journal，不重视会议，所以录用率经常在85%左右，所录文章既有quality非常高的论文，也有入门新手的习作。̶          FUZZ-IEEE(3):IEEE International Conferenceon Fuzzy Systems. FUZZ-IEEE(3):模糊方面最重要的会议，盛会型，参见CEC的介绍。̶          GECCO(3):Genetic and Evolutionary Computation Conference. GECCO(3):进化计算方面最重要的会议之一，与CEC相当，盛会型。̶          ICASSP(3):International Conference on Acoustics，Speech，and Signal Processing. ICASSP(3):语音方面最重要的会议之一，这个领域的人也不很care会议。̶          ICIP(3):International Conference on Image Processing. ICIP(3):图像处理方面最著名的会议之一，盛会型。̶          ICPR(3):International Conference on Pattern Recognition. ICPR(3):模式识别方面最著名的会议之一，盛会型。̶          IEA/AIE(3):International Conference on Industrial and Engineering Applications of Artificial Intelligence and ExpertSystems. IEA/AIE(3):人工智能应用会议。一般的会议提名优秀论文的通常只有几篇文章，被提名就已经是很高的荣誉了，这个会很有趣，每次都搞1、20篇的优秀论文提名，专门搞几个session做被提名论文报告，倒是很热闹。̶          IJCNN(3):International Joint Conference on Neural Networks. IJCNN(3):神经网络方面最重要的会议，盛会型，参见CEC的介绍。̶          IJNLP(3):International Joint Conference on Natural Language Processing. IJNLP(3):计算语言学/自然语言处理方面比较著名的一个会议。̶          PRICAI(3):Pacific-Rim International Conference on Artificial Intelligence. PRICAI(3):亚太综合型人工智能会议，虽然历史不算短了，但因为比它好或者相当的综合型会议太多，所以很难上升。*Impactfactor(AccordingtoCiteseer03):ACCV：0。42(top55。61%)ICTAI：0。25(top69。86%)PAKDD：0。30(top65。60%)ICANN：0。27(top67。73%)AJCAI：0。16(top79。44%)CAI：0。26(top68。87%)ICIP：0。50(top50。20%)IEA/AIE：0。09(top87。79%)PRICAI：0。19(top76。33%)Average：0。27(top68。30%)列list只是为了帮助新人熟悉领域，给出的评分或等级都是个人意见，仅供参考。特别要说明的是:1。tier-1conference上的文章并不一定比tier-3的好，只能说前者的平均水准更高。2。研究工作的好坏不是以它发表在哪儿来决定的，发表在高档次的地方只是为了让工作更容易被同行注意到。tier-3会议上发表1篇被引用10次的文章可 能比在tier-1会议上发表10篇被引用0次的文章更有价值。所以，数top会议文章数并没有太大意义，重要的是同行的评价和认可程度。3。很多经典工作并不是发表在高档次的发表源上，有不少经典工作甚至是发表在很低档的发表源上。原因很多，就不细说了。4。会议毕竟是会议，由于审稿时间紧，错杀好人和漏过坏人的情况比比皆是，更何况还要考虑到有不少刚开始做研究的学生在代老板审稿。5。会议的reputation并不是一成不变的，新会议可能一开始没什么声誉，但过几年后就野鸡变凤凰，老会议可能原来声誉很好，但越来越往下滑。6。只有计算机科学才重视会议论文，其他学科并不把会议当回事。但在计算机科学中也有不太重视会议的分支。7、Politics无所不在。你老板是谁，你在哪个研究组，你在哪个单位，这些简单的因素都可能造成决定性的影响。换言之，不同环境的人发表的难度是不 一样的。了解到这一点后，你可能会对high-level发表源上来自low-level单位名不见经传作者的文章特别注意(例如如果<计算机学 报>上发表了平顶山铁道电子信息科技学院的作者的文章，我一定会仔细读)。8、评价体系有巨大的影响。不管是在哪儿谋生的学者，都需要在一定程度上去迎合评价体系，否则连生路都没有了，还谈什么做研究。以国内来说，由于评价体系 只重视journal，有一些工作做得很出色的学者甚至从来不投会议。另外，经费也有巨大的制约作用。国外很多好的研究组往往是重要会议都有文章。但国内 是不行的，档次低一些的会议还可以投了只交注册费不开会，档次高的会议不去做报告会有很大的负面影响，所以只能投很少的会议。这是在国内做CS研究最不利 的地方。我的一个猜想：人民币升值对国内CS研究会有不小的促进作用(当然，人民币升值对整个中国来说利大于弊还是弊大于利很难说)。"}
{"content2":"横看成岭侧成峰，计算视觉大不同。观看的角度不同，成像自然不同，这对计算机视觉来说，是个大麻烦。但计算机视觉应用如此广泛，却又有不得不研究的理由。指纹机大家都用过吧，这不过是冰山之一角。产品检测，机器人，医学成像等等，都有计算机视觉的身影。学习计算机视觉，OpenCV 是个不错的选择。下载安装到指定位置后，会发现有 3.5G 之多。初步看看，重复太多，32位，64位各有一套，而 vc10，vc11，vc12 再次重复。只需把要用到的 build -> x86 -> vc12 调试版提取出来即可。我是编译源代码，将生成的 bin -> *.dll， lib -> *.lib 和原来 OpenCV 安装目录 build -> include 提取出来，放入新建文件夹中，约 60MB。平时学习，使用这个即可。1.配置系统环境变量，将 bin 加入 Path 变量，注销一下使其有效。2.启动 VS2013，新建 Win32 Console 空项目。在项目 属性 -> VC++ 目录 -> 包含目录 添加 include，库目录 添加 lib。3.新建头文件 x01CV.h，内容如下：#pragma once #pragma comment(lib,\"opencv_calib3d249d.lib\") #pragma comment(lib,\"opencv_contrib249d.lib\") #pragma comment(lib,\"opencv_core249d.lib\") #pragma comment(lib,\"opencv_features2d249d.lib\") #pragma comment(lib,\"opencv_flann249d.lib\") #pragma comment(lib,\"opencv_gpu249d.lib\") #pragma comment(lib,\"opencv_highgui249d.lib\") #pragma comment(lib,\"opencv_imgproc249d.lib\") #pragma comment(lib,\"opencv_legacy249d.lib\") #pragma comment(lib,\"opencv_ml249d.lib\") #pragma comment(lib,\"opencv_nonfree249d.lib\") #pragma comment(lib,\"opencv_objdetect249d.lib\") #pragma comment(lib,\"opencv_ocl249d.lib\") #pragma comment(lib,\"opencv_photo249d.lib\") #pragma comment(lib,\"opencv_stitching249d.lib\") #pragma comment(lib,\"opencv_superres249d.lib\") #pragma comment(lib,\"opencv_ts249d.lib\") #pragma comment(lib,\"opencv_video249d.lib\") #pragma comment(lib,\"opencv_videostab249d.lib\") #include <opencv2/core/core.hpp> #include <opencv2/highgui/highgui.hpp> #include <opencv2/imgproc/imgproc.hpp> #include <opencv/cv.h>x01CV.h4.新建源文件 Main.cpp，内容如下：#include <iostream> #include \"x01CV.h\" using namespace cv; using namespace std; namespace { const string AppTitle = \"x01 Lab - OpenCV Demo\"; int g_filterValue = 3; Mat g_srcImage, g_destImage; void OnBoxFilter(int, void*) { boxFilter(g_srcImage, g_destImage, -1, Size(g_filterValue + 1, g_filterValue + 1)); imshow(AppTitle, g_destImage); } void OnBlur(int, void*) { blur(g_srcImage, g_destImage, Size(g_filterValue + 1, g_filterValue + 1), Point(-1, -1)); imshow(AppTitle, g_destImage); } void OnGaussianBlur(int, void*) { int v = g_filterValue * 2 + 1; GaussianBlur(g_srcImage, g_destImage, Size(v, v), 0, 0); imshow(AppTitle, g_destImage); } void OnMediaBlur(int, void*) { medianBlur(g_srcImage, g_destImage, g_filterValue * 2 + 1); imshow(AppTitle, g_destImage); } void OnBilateralFilter(int, void*) { int v = g_filterValue + 7; bilateralFilter(g_srcImage, g_destImage, v, v * 2, v / 2); imshow(AppTitle, g_destImage); } } int main() { g_srcImage = imread(\"1.jpg\"); // For erode and dilate demo /*Mat kernel = getStructuringElement(MORPH_RECT, Size(15, 15)); Mat dest; erode(g_srcImage, dest, kernel); namedWindow(AppTitle); imshow(AppTitle, dest);*/ g_destImage = g_srcImage.clone(); while (true) { char c = char(waitKey()); if (c == 'q') break; namedWindow(AppTitle); if (c == 'a' || c == 'A') createTrackbar(\"Kernel Value\", AppTitle, &g_filterValue, 40, OnBlur); else if (c == 'b' || c == 'B') createTrackbar(\"Kernel Value\", AppTitle, &g_filterValue, 40, OnBoxFilter); else if (c == 'c' || c == 'C') createTrackbar(\"Kernel Value\", AppTitle, &g_filterValue, 40, OnGaussianBlur); else if (c == 'd' || c == 'D') createTrackbar(\"Kernel Value\", AppTitle, &g_filterValue, 40, OnMediaBlur); else if (c == 'e' || c == 'E') createTrackbar(\"Kernel Value\", AppTitle, &g_filterValue, 40, OnBilateralFilter); imshow(AppTitle, g_destImage); } return 0; }Main.cpp5.F5 运行无误，按 a 或 b, c, d, e 等，可进行模糊处理。效果图如下："}
{"content2":"参考彭亮老师的视频教程：转载请注明出处及彭亮老师原创视频教程： http://pan.baidu.com/s/1kVNe5EJ1. 课程介绍2. 机器学习 （Machine Learning, ML)2.1 概念：多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。2.2 学科定位：人工智能(Artificial Intelligence, AI）的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演绎。2.3 定义：探究和开发一系列算法来如何使计算机不需要通过外部明显的指示，而可以自己通过数据来学习，建模，并且利用建好的模型和新的输入来进行预测的学科。Arthur Samuel (1959): 一门不需要通过外部程序指示而让计算机有能力自我学习的学科Langley（1996) ： “机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在经验学习中改善具体算法的性能”Tom Michell (1997):  “机器学习是对能通过经验自动改进的计算机算法的研究”2.4： 学习：针对经验E (experience) 和一系列的任务 T (tasks) 和一定表现的衡量 P，如果随之经验E的积累，针对定义好的任务T可以提高表现P，就说计算机具有学习能力例子： 下棋，语音识别，自动驾驶汽车等3. 机器学习的应用：语音识别自动驾驶语言翻译计算机视觉推荐系统无人机识别垃圾邮件4. Demo：人脸识别无人驾驶汽车电商推荐系统5. 置业市场需求：LinkedIn所有职业技能需求量第一：机器学习，数据挖掘和统计分析人才"}
{"content2":"转载至（https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650717606&idx=4&sn=b94b58d4fe75c1a1e42274720a269a99&scene=21#wechat_redirect），理解三者之间关系的最简便方法就是将它们视觉化为一组同心圆——首先是最大的部分人工智能——然后是后来兴旺的机器学习——最后是促使当下人工智能大爆发的深度学习——在最里层。从萧条到繁荣自从 1956 年几个计算机科学家在达特茅斯会议上聚集并开辟了人工智能这一领域，人工智能就进入了我们的想象，并在实验研究中进行着酝酿。在过去的几十年里，人工智能以及轮番被誉为人类文明取得最美好未来的关键，或者是作为一个头脑发烧的轻率概念被扔进了科技垃圾堆中。坦白说直到 2012 年，它就这样在二者之间交杂。过去的几年里，尤其从 2015 年开始，人工智能开始爆发了。这很大程度上与 GPU 的广泛应用有关，为了使并行处理更快、更便宜、更强大。这也与近乎无限的存储能力和各类数据洪流（所有的大数据运动）——图像、文本、交易、测绘数据，只要你说得出来——一道进行。让我们梳理一遍计算机科学家是如何从萧条——直到 2012 年——到繁荣，开发出每天由成千上百万的人使用的应用。人工智能——机器诠释的人类智能回到 1956 年夏天的那场会议，人工智能先驱们的梦想是借由新兴计算机构建具有人类智力特征的复杂机器。这就是所谓的「通用人工智能（General AI）」的概念——拥有人类的所有感觉（甚至可能更多）、所有理智，像人类一样思考的神奇机器。你已经在电影中无休止地看到过这些被我们当做朋友的机器，比如《星球大战》中的 C-3PO  以及成为人类敌人的机器——终结者。通用人工智能机器向来有充足的理由出现在电影和科幻小说中；我们不能阻止，至少现在还不行。我们能做什么？这就到了「狭义人工智能（Narrow AI）」的概念。指的是能够将特殊任务处理得同人类一样好，或者更好的技术。狭义人工智能的相关案例比如有 Pinterest 上的图像分类、Facebook 中的人脸识别。这些是狭义人工智能在实践中的例子。这些技术展示了人类智能的一些方面。但是如何做到的呢？那个智能来自哪里？所以接下来看第二个同心圆，机器学习。机器学习——实现人工智能的一种方式机器学习最基础的是运用算法来分析数据、从中学习、测定或预测现实世界某些事。所以不是手动编码带有特定指令设定的软件程序来完成某个特殊任务，而是使用大量的数据和算法来「训练」机器，赋予它学习如何执行任务的能力。机器学习直接源自早期那帮人工智能群体，演化多年的算法包括了决策树学习（decision tree learning）、归纳逻辑编程（inductive logic programming）。其他的也有聚类（clustering）、强化学习（reinforcement learning）和贝叶斯网络（Bayesian networks）等。我们知道，这些早期机器学习方法都没有实现通用人工智能的最终目标，甚至没有实现狭义人工智能的一小部分目标。事实证明，多年来机器学习的最佳应用领域之一是计算机视觉，尽管它仍然需要大量的手工编码来完成工作。人们会去写一些手写分类器，像是边缘检测过滤器（edge detection filters）使得程序可以识别对象的启止位置；形状检测（shape detection）以确定它是否有八条边；一个用来识别单词「S-T-O-P」的分类器。从这些手写分类器中他们开发出能够理解图像的算法，「学习」判定它是否是一个停止标志。这很好，但还不够好。特别是有雾天气标志不完全可见的情况下，或者被树遮住了一部分。计算机视觉和图像检测直到目前都不能与人类相媲美，是因为它太过脆弱，太容易出错了。是时间和正确的学习算法改变了这一切。深度学习——一种实现机器学习的技术源自最早进行机器学习那群人的另一种算法是人工神经网络（Artificial Neural Networks），它已有几十年的历史。神经网络的灵感来自于我们对大脑生物学的理解——所有神经元之间的相互连接。但是不像生物大脑中的任何神经元，可以在一定的物理距离内连接到任何其他神经元，这些人工神经网络的层、连接和数据传播方向是离散的。比如你可以把一个图像切成一堆碎片并输入到神经网络的第一层中。然后第一层的单个神经元们将数据传递给第二层。第二层神经元将数据传给第三层，如此一直传到最后一层并输出最终结果。每个神经元分配一个权重到它的输入——评估所执行的任务的准确或不准确。然后最终的输出由所有这些权重来确定。所以想想那个停止标志的例子。一个停止标志图像的特征被切碎并由神经元来「检查」——它的形状、它的消防红色彩、它的独特字母、它的交通标志尺寸以及和它的运动或由此带来的缺失。神经网络的任务是判定它是否为一个停止标志。这提出了一个「概率向量」，它真是一个基于权重的高度受训的猜测。在我们的例子中，系统可能有 86% 的把握认为图像是一个停止标志，7% 的把握认为这是一个限速标志，5% 的把握认为这是一只被卡在树上的风筝，等等——然后网络架构告诉神经网络结果的正确与否。甚至这个例子都有些超前了，因为直到现在，神经网络都被人工智能研究社区避开了。自从最早的人工智能起，他们一直在做这方面研究，而「智能」成果收效甚微。问题很简单，即最基本的神经网络属于计算密集型，这并不是一个实用的方法。不过，由多伦多大学的 Geoffrey Hinton 带领的异端研究小组一直在继续相关研究工作，最终在超级计算机上运行并行算法证明了这个概念，但这是直到 GPU 被部署之后才兑现的诺言。如果我们再回到停止标志的例子，当网络正在进行调整或者「训练」时，出现大量的错误答案，这个机会是非常好的。它需要的就是训练。它需要看到成千上万，甚至数以百万计的图像，直到神经元的输入权重被精确调整，从而几乎每一次都能得到正确答案——无论有雾没雾，晴天还是雨天。在这一点上，神经网络已经教会了自己停止标志看起来会是什么样的；或者在 Facebook 例子中就是识别妈妈的脸；或者吴恩达 2012 年在谷歌所做的猫的图片。吴恩达的突破在于从根本上使用这些神经网络 并将它们变得庞大，增加了层数和神经元的数量，然后通过系统运行大量的数据来训练它。吴恩达使用了 1000 万个 YouTube 视频的图像。他将「深度」运用在深度学习中，这就描述了这些神经网络的所有层。如今，在一些场景中通过深度学习训练机器识别图像，做得比人类好，从识别猫咪到确定血液中的癌症指标和磁共振成像扫描中的肿瘤指标。谷歌的 AlphaGo 学会了游戏，并被训练用于 Go 比赛。通过反复与自己对抗来调整自己的神经网络。感谢深度学习，让人工智能有一个光明的未来。深度学习 已经实现了许多机器学习方面的实际应用和人工智能领域的全面推广。深度学习解决了许多任务让各种机器助手看起来有可能实现。无人驾驶机车、更好的预防医疗，甚至是更好的电影推荐，如今都已实现或即将实现。人工智能在当下和未来。有了深度学习，人工智能甚至可以达到我们长期所想象的科幻小说中呈现的状态。我拿走你的 C-3PO，你可以留着终结者。"}
{"content2":"微软新一代体感游戏机Kinect的出现直接导致了我对这一领域的强烈兴趣。虽然在之前的神经网络学习过程中对计算机视觉的一些分支领域有所涉及（例如：人脸识别、签名识别，图像处理等），但并没有系统地了解过这一领域。我希望通过这一系列博文可以记录下这个学习的过程和自己的一些心得感悟。既然是入门，选择参考书就非常重要。这里我选择了英文和中文参考书各一本：《Computer Vision: Algorithms and Applications》(作者：Richard Szeliski）和《计算机视觉中的数学方法》（吴福朝,科学出版社,.2008）。此外，还有一些有用的链接：1.http://research.microsoft.com/en-us/um/people/szeliski/ 上述作者Richard Szeliski个人主页，这里可以下载到《Computer Vision: Algorithms and Applications》的pdf版本2.http://research.microsoft.com/en-us/default.aspx 微软研究院主页3.http://computervisioncentral.com/cvcnews Computer Vision Central 主页，提供关于计算机视觉的最新产品、研究动态和业界资讯。4.http://www.cvchina.info/ CVChina 主页，以译介的方式报道计算机视觉，增强现实，机器学习，图像处理，机器视觉等学术界，工业界的最新进展。计算机视觉领域顶级的国际会议：International Conference on Computer Vision (ICCV) (奇数年举办)IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (每年举办)IEEE International Conference on Image Processing (ICIP) (每年举办)European Conference on Computer Vision (ECCV) (偶数年举办)下一篇正式开始学习计算机视觉, to be continued..."}
{"content2":"【计算机视觉】全息投影技术标签（空格分隔）： 【图像处理】 【科普杂谈】说明：本文主要想弄清楚全息投影技术的一些科普知识，基于前天DIY了一个小型简易的projector，现在对其原理进行说明一下。简要说明全息投影技术也称虚拟成像技术是利用干涉和衍射原理记录并再现物体真实的三维图像的记录和再现的技术。实际上全息技术在我们的日常生活中已经被广泛地使用了，在产品展览、汽车服装发布会、舞台节目、互动、酒吧娱乐、场所互动投影等，都有全息投影的身影。伦敦Diesel T台秀在舞台的两边设置了全息投影膜，在模特走台的同时，在投影膜上会出现亦幻亦真的烟雾，或者让人汗毛直立的怪兽，这些都是设计师的灵感，让发布会的主题更容易为广大的观众所接受。全息投影技术空气投影和交互技术：在美国麻省一位叫ChadDyne的29岁理工研究生发明了一种空气投影和交互技术，这是显示技术上的一个里程碑，它可以在气流形成的墙上投影出具有交互功能的图像。此技术来源海市蜃楼的原理，将图像投射在水蒸气上，由于分子震动不均衡，可以形成层次和立体感很强的图像。激光束投射实体的3D影像：日本公司ScienceandTechnology发明了一种可以用激光束来投射实体的3D影像，这种技术是利用氮气和氧气在空气中散开时，混合成的气体变成灼热的浆状物质，并在空气中形成一个短暂的3D图像.这种方法主要是不断在空气中进行小型爆破来实现的。360度全息显示屏：南加利福尼亚大学创新科技研究院的研究人员宣布他们成功研制一种360度全息显示屏，这种技术是将图像投影在一种高速旋转的镜子上从而实现三维图像，可以说这些技术很多国家都在研制，毫不夸张的说这项技术它包含了未来，谁最先使用这项技术，谁就最先走入未来的先进技术行列。全息技术遇到的问题普通的摄像是二维平面采样，而全息投影成像则是多角度摄像，并且将这些照片叠加。为了实现立体“叠加”，需要利用光的干涉原理，用单一的光线（常用投影机）进行照射，使物体反射的光分裂（分光技术）成多束相干光，将这些相干光叠加就能实现全息立体影像。全息影像制作需要比普通摄像处理100倍以上的信息量，对拍摄以及处理和传输平台都提出了很高的要求。全息技术是利用干涉和衍射原理记录并再现物体真实的三维图像的记录和再现的技术。第一步是利用干涉原理记录物体光波信息，此即拍摄过程：被摄物体在激光辐照下形成漫射式的物光束；另一部分激光作为参考光束射到全息底片上，和物光束叠加产生干涉，把物体光波上各点的位相和振幅转换成在空间上变化的强度，从而利用干涉条纹间的反差和间隔将物体光波的全部信息记录下来。记录着干涉条纹的底片经过显影、定影等处理程序后，便成为一张全息图，或称全息照片；第二步是利用衍射原理再现物体光波信息，这是成象过程：全息图犹如一个复杂的光栅，在相干激光照射下，一张线性记录的正弦型全息图的衍射光波一般可给出两个象，即原始象（又称初始象）和共轭象。再现的图像立体感强，具有真实的视觉效应。全息图的每一部分都记录了物体上各点的光信息，所以原则上它的每一部分都能再现原物的整个图像，通过多次曝光还可以在同一张底片上记录多个不同的图像，而且能互不干扰地分别显示出来.目前全息投影膜技术只有德国和瑞典的少数厂家所掌控，造价昂贵。加之高清高亮的工程投影机价格也不便宜，整个项目的成本非常之高。并且全息投影的使用场地也有限，其只能位于市内，并且对环境的要求非常高。2015-11-29 整理 张朋艺"}
{"content2":"本期主要浅谈学习机器视觉中的一些基本概念，适合新手学习，同时也把学习过程中的一些心得体会分享出来，愿君共勉。计算机视觉：人类如何能够通过眼睛识别自然界中的一些物体，并在大脑中形成一幅画面，是人类视觉神经中枢经过长久以来地进化所形成的可以判断物体远近视觉器官，计算机视觉就是用各种成象系统代替视觉器官作为输入敏感手段，由计算机来代替大脑完成处理和解释。脑海中怎么形成三维信息：主要是由阴影、纹理、运动、模糊、遮挡、高光、轮廓、对焦等三维信息构成一种视觉效应。双目立体视觉：双目立体视觉融合两只眼睛获得的图像并观察它们之间的差别，使我们可以获得明显的深度感，建立特征间的对应关系，将同一空间物理点在不同图像中的映像点对应起来，这个差别，我们称作视差(Disparity)图像。（为什么不用单目视觉呢？也就说一个摄像头对于空间上某一个点来说在相机底片上所成的面是没有深度信息的，所以存在深度感知歧义）平行光轴立体视觉系统由相似三角形原理可知：深度 Z=T*f/(Xl-Xr);其中Z是深度，T是基线，f是焦距，d=Xl-Xr是视差。会聚光轴立体视觉系统：极线约束定理：空间上一点P在π平面上的投影P点与在π'平面上的P'必在平面OPO上，其中可称为对应点或匹配点。对应点必在极线上。简而言之，立体视觉主要有以下步骤：（1）图像摄取：主要通过双目摄像头(CCD)对空间图片的获取（2）系统标定：主要对相机的内部参数（光心、焦距、畸变系数）和外参（旋转矩阵、平移矩阵）的校正，使得左右两幅相机的图片能够平行且共面。（3）立体匹配：为每个左图像中的像素点f(x,y)搜索对应的右图像中的像素点。通过以上的学习可以初步地了解到计算机视觉的基本概念以及研究意义，下期会着重的对一些立体匹配方法进行介绍，因为立体匹配方法对于一幅图像的深度图有着至关重要的影响，所以好的立体匹配方法是计算机视觉处理的基础。"}
{"content2":"今日CS.CV计算机视觉论文速览Fri, 22 Feb 2019Totally 20 papersDaily Computer Vision Papers[1] **Title: Deep CNN-based Speech Balloon Detection and Segmentation for Comic BooksAuthors:David Dubray, Jochen Laubrock[2] Title: Boundary-weighted Domain Adaptive Neural Network for Prostate MR Image SegmentationAuthors:Qikui Zhu, Bo Du, Pingkun Yan[3] *Title: Cross-Sensor Periocular Biometrics: A Comparative Benchmark including Smartphone AuthenticationAuthors:Fernando Alonso-Fernandez, Kiran B. Raja, R. Raghavendra, Cristoph Busch, Josef Bigun, Ruben Vera-Rodriguez, Julian Fierrez[4] **Title: GSLAM: A General SLAM Framework and BenchmarkAuthors:Yong Zhao, Shibiao Xu, Shuhui Bu, Hongkai Jiang, Pengcheng Han[5] *Title: A Parallel Optical Image Security System with Cascaded Phase-only MasksAuthors:Shuming Jiao, Yang Gao, Ting Lei, Zhenwei Xie, Xiaocong Yuan[6] **Title: A Joint Deep Learning Approach for Automated Liver and Tumor SegmentationAuthors:Nadja Gruber, Stephan Antholzer, Werner Jaschke, Christian Kremser, Markus Haltmeier[7] Title: Deep Discriminative Representation Learning with Attention Map for Scene ClassificationAuthors:Jun Li, Daoyu Lin, Yang Wang, Guangluan Xu, Chibiao Ding[8] Title: ComplexFace: a Multi-Representation Approach for Image Classification with Small DatasetAuthors:Guiying Zhang, Yuxin Cui, Yong Zhao, Jianjun Hu[9] **Title: Long-Bone Fracture Detection using Artificial Neural Networks based on Contour Features of X-ray ImagesAuthors:Alice Yi Yang, Ling Cheng[10] **Title: Towards Real-time Eyeblink Detection in The Wild:Dataset,Theory and PracticesAuthors:Guilei Hu, Yang Xiao, Zhiguo Cao, Lubin Meng, Zhiwen Fang, Joey Tianyi Zhou[11] *Title: Evaluation of Algorithms for Multi-Modality Whole Heart Segmentation: An Open-Access Grand ChallengeAuthors:Xiahai Zhuang, Lei Li, Christian Payer, Darko Stern, Martin Urschler, Mattias P. Heinrich, Julien Oster, Chunliang Wang, Orjan Smedby, Cheng Bian, Xin Yang, Pheng-Ann Heng, Aliasghar Mortazi, Ulas Bagci, Guanyu Yang, Chenchen Sun, Gaetan Galisot, Jean-Yves Ramel, Thierry Brouard, Qianqian Tong, Weixin Si, Xiangyun Liao, Guodong Zeng, Zenglin Shi, Guoyan Zheng, Chengjia Wang, Tom MacGillivray, David Newby, Kawal Rhode, Sebastien Ourselin, Raad Mohiaddin, Jennifer Keegan, David Firmin, Guang Yang[12] Title: Atrial Scar Quantification via Multi-scale CNN in the Graph-cuts FrameworkAuthors:Lei Li, Fuping Wu, Guang Yang, Lingchao Xu, Tom Wong, Raad Mohiaddin, David Firmin, Jennifer Keegan, Xiahai Zhuang[13] Title: Improvement Multi-Stage Model for Human Pose EstimationAuthors:Zhihui Su, Ming Ye, Guohui Zhang, Lei Dai, Jianda Sheng[14] Title: Class-independent sequential full image segmentation, using a convolutional net that finds a segment within an attention region, given a pointer pixel within this segmentAuthors:Sagi Eppel[15] Title: Perceptual Quality-preserving Black-Box Attack against Deep Learning Image ClassifiersAuthors:Diego Gragnaniello, Francesco Marra, Giovanni Poggi, Luisa Verdoliva[16] *Title: Self-supervised Learning for Dense Depth Estimation in Monocular EndoscopyAuthors:Xingtong Liu, Ayushi Sinha, Masaru Ishii, Gregory D. Hager, Austin Reiter, Russell H. Taylor, Mathias Unberath[17] Title: Adversarial Augmentation for Enhancing Classification of Mammography ImagesAuthors:Lukas Jendele, Ondrej Skopek, Anton S. Becker, Ender Konukoglu[18] Title: Domain Partitioning NetworkAuthors:Botos Csaba, Adnane Boukhayma, Viveka Kulharia, András Horváth, Philip H. S. Torr[19] *Title: Cloud-Based Autonomous Indoor Navigation: A Case StudyAuthors:Uthman Baroudi, M. Alharbi, K. Alhouty, H. Baafeef, K. Alofi[20] Title: Probabilistic Neural-symbolic Models for Interpretable Visual Question AnsweringAuthors:Ramakrishna Vedantam, Karan Desai, Stefan Lee, Marcus Rohrbach, Dhruv Batra, Devi ParikhPapers from arxiv.org更多精彩请移步主页Interesting:📚pic from pixels.com"}
{"content2":"人工智能的概念早在60多年前就被提出，但又一度沉寂。随着谷歌人工智能程序AlphaGo（阿尔法狗）战胜围棋世界冠军李世石，再次为世人瞩目。然而，与无限风光一起相伴而来的，还有关于人工智能的种种争议！“在我的一生中，见证了社会深刻的变化。其中最深刻的，同时也是对人类影响与日俱增的变化，是人工智能的崛起。简单来说，我认为强大的人工智能的崛起，要么是人类历史上最好的事，要么是最糟的。”著名物理学家霍金生前反复告诫。在互联网和大数据风起云涌的今天，人工智能究竟会成为造福人类的天使，还是控制人类的魔鬼？面对类似疑虑，请听中国科学院院士、中国人工智能学会副理事长谭铁牛怎么说。崛起经过60多年的不断发展，人工智能迎来发展的春天，成为推动新一轮科技和产业革命的重要驱动力1956年，在达特茅斯学院暑期研讨班上，一位名叫约翰·麦卡锡的年轻人首次提出了人工智能的概念，那时研讨的主题是怎样用机器模拟人的智能。事实上，与人工智能相关的研究，在此之前早已开展。“人工智能的主要目标是模拟、延伸和扩展人类智能，探寻智能本质，研发具有类人智能的智能机器。比如，让机器或者计算机会听、会看、会说、会想、会决策，与人类一样。”谭铁牛解释。时光荏苒，白云苍狗。60多年风风雨雨，随着大数据、云计算、互联网、物联网等信息技术的发展，以深度神经网络为代表的人工智能技术成功跨越科学与应用之间的“技术鸿沟”，如愿迎来发展的春天：图像分类、语音识别、知识问答、人机对弈、无人驾驶……一系列具有广阔应用前景的人工智能技术相继突破从“不能用、不好用”到“可以用”的技术拐点，成为推动新一轮科技和产业革命的重要驱动力。医学行业便是其中之一。2017年，斯坦福大学在国际权威期刊《自然》上发表论文宣布，他们通过深度学习的方法，采用近13万张痣、皮疹和其他皮肤病变的图像训练机器识别其中的皮肤癌症状，在与21位皮肤科医生的诊断结果进行对比后发现，这个深度神经网络的诊断准确率达到91%，与医生不相上下。这样的惊喜比比皆是。通过深度神经网络的应用创新，国际计算机视觉竞赛ImageNet图像分类的Top5误差率从2012年的16%降到2017年的3%左右（已经低于人的错误率）；我国的Face＋＋（旷视科技）人脸识别技术的准确率在LFW国际公开测试中达到全球最高的99.5%（超过了人类肉眼识别的准确率97.52%），与此相关的刷脸支付被《麻省理工科技评论》评为2017年十大全球突破性技术。“人工智能的近期进展主要集中在专用智能领域，例如，微软语音识别系统5.1%的错误率比肩专业速记员。”谭铁牛说，从可应用性来看，人工智能大体可以分为专用人工智能和通用人工智能。面向特定领域的人工智能技术（即专用人工智能）由于任务单一、应用背景需求明确、领域知识积累深厚、建模计算简单可行，更容易实现单点突破，在局部智能水平的单项测试中可以超越人类智能。“专用人工智能取得突破性进展，主要源于深度学习、强化学习、对抗学习等统计机器学习理论的进步。深度学习，简单说，就是借鉴人的大脑在处理信息过程当中的层次化处理。”谭铁牛解释说。挑战琴棋书画样样精通的人工智能似乎已所向披靡，然而，专家表示，人工智能总体发展水平仍然处于起步阶段“艺术创作将是人类对抗人工智能的‘最后一座堡垒’！”曾几何时，一些专家对此深信不疑。然而，现实很快给予他们一击：2016年，谷歌开发的人工智能画家——“初创主义”在旧金山拍卖会上大放异彩，在它创作的29幅作品中，有的被卖出8000美元的高价。无独有偶。在法国巴黎，索尼计算机科学实验室“深沉巴赫”创作的合唱曲目，甚至被专业音乐家误认为是“真巴赫”的作品。而人工智能创作的小说《一台电脑写一篇小说的一天》，则通过了日本“星新一文学奖”初审第一轮。琴棋书画样样精通的人工智能似乎已所向披靡，无所不能。当人工智能崛起，人类会不会被取而代之？“有智能没智慧、有智商没情商、会计算不会‘算计’、有专才无通才。”谭铁牛历数人工智能诸多局限并一一阐释：智慧是高级智能，目前的人工智能无意识和悟性，缺乏综合决策能力；机器对于人的情感理解与交流还处于起步阶段；人工智能系统可谓有智无心，更无谋；会下围棋的“阿尔法狗”不会下象棋。换句话说，现在，人工智能总体发展水平仍然处于起步阶段。谭铁牛举例说，机器翻译如今已经做得相当不错，但这简单的3句话“他吃食堂”“他吃面条”“他吃大腕”，机器翻译不出；“那辆白车是黑车”“能穿多少穿多少”，这也无法翻译。再比如，看到校园里“欢迎新老师生前来就餐”的横幅，很多人一目了然，但人工智能却无法理解，这些不能之处正是当前人工智能遇到的瓶颈。“人工智能系统的能力维度可分为信息感知、机器学习、概念抽象和规划决策几方面。从知识规则到统计学习，第二波人工智能技术在信息感知和机器学习方面进展显著，但是在概念抽象和规划决策方面刚刚起步。”美国DARPA（国防高级研究计划署）如是判断。人工智能已经达到5岁小孩的水平、人工智能系统的智能水平即将超越人类水平、30年内机器人将统治世界……面对一些错误认识和炒作，谭铁牛有些无奈。“现有人工智能还有很大的局限性，有很多人认为通用人工智能很快就能实现，只要给机器人发指令就可以做任何事，这是对人工智能预期过高。”谭铁牛提醒说。趋势对比人类大脑，真正意义上完备的人工智能系统应该是一个通用的智能系统，而对它的研究与应用任重道远未来，人工智能应何去何从？谭铁牛认为，人类大脑是一个通用的智能系统，能举一反三、融会贯通，可处理视觉、听觉、判断、推理、学习、思考、规划、设计等各类问题，可谓“一脑万用”。因此，真正意义上完备的人工智能系统应该是一个通用的智能系统，而通用人工智能研究与应用刚刚起步，依然任重道远。他坦陈，当前人工智能处于从“不能用”到“可以用”的技术拐点，但是距离“很好用”，还有数据、能耗、泛化、可解释性、可靠性、安全性等诸多瓶颈，理论创新和产业应用发展空间巨大。总体而言，人工智能的发展趋势是理论更完备、技术更先进、产业更繁荣、应用更广泛、法规更健全。通用智能被认为是人工智能皇冠上的明珠，是全世界科技巨头竞争的焦点。如何实现从专用智能到通用智能的跨越式发展，既是下一代人工智能发展的必然趋势，也是研究与应用领域的挑战。在谭铁牛看来，人工智能和人类智能各有所长，因此需要取长补短，融合多种智能模式的智能技术将在未来具有广阔应用前景。“人+机器”的组合将是人工智能研究的主流方向，“人机共存”将是人类社会的新常态，而学科交叉将成为人工智能创新的源泉。至于它究竟会成为造福人类的天使，还是控制人类的魔鬼，这取决于人类自身。“高科技本身没有天使和魔鬼之分，人工智能亦是如此。对于人工智能这把双刃剑的使用取决于人类自身。我们应未雨绸缪，形成合力，确保人工智能的正面效应，确保人工智能造福于人类。"}
{"content2":"对用卷积神经网络进行目标检测方法的一种改进，通过提取多尺度的特征信息进行融合，进而提高目标检测的精度，特别是在小物体检测上的精度。FPN是ResNet或DenseNet等通用特征提取网络的附加组件，可以和经典网络组合提升原网络效果。一、问题背景网络的深度（对应到感受野）与总stride通常是一对矛盾的东西，常用的网络结构对应的总stride一般会比较大（如32），而图像中的小物体甚至会小于stride的大小，造成的结果就是小物体的检测性能急剧下降。传统解决这个问题的思路包括：（1）多尺度训练和测试，又称图像金字塔，如下图(a)所示。目前几乎所有在ImageNet和COCO检测任务上取得好成绩的方法都使用了图像金字塔方法。然而这样的方法由于很高的时间及计算量消耗，难以在实际中应用。（2）特征分层，即每层分别预测对应的scale分辨率的检测结果。如下图(c)所示。SSD检测框架采用了类似的思想。这样的方法问题在于直接强行让不同层学习同样的语义信息。而对于卷积神经网络而言，不同深度对应着不同层次的语义特征，浅层网络分辨率高，学的更多是细节特征，深层网络分辨率低，学的更多是语义特征。因而，目前多尺度的物体检测主要面临的挑战为：1. 如何学习具有强语义信息的多尺度特征表示？2. 如何设计通用的特征表示来解决物体检测中的多个子问题？如object proposal, box localization, instance segmentation.3. 如何高效计算多尺度的特征表示？二、特征金字塔网络（Feature Pyramid Networks）作者提出了FPN算法。做法很简单，如下图所示。把低分辨率、高语义信息的高层特征和高分辨率、低语义信息的低层特征进行自上而下的侧边连接，使得所有尺度下的特征都有丰富的语义信息。图中未注明的是融合之后的feat还需要进行一次3*3卷积作者的算法结构可以分为三个部分：自下而上的卷积神经网络（上图左），自上而下过程（上图右）和特征与特征之间的侧边连接。自下而上的部分其实就是卷积神经网络的前向过程。在前向过程中，特征图的大小在经过某些层后会改变，而在经过其他一些层的时候不会改变，作者将不改变特征图大小的层归为一个阶段，因此每次抽取的特征都是每个阶段的最后一个层的输出，这样就能构成特征金字塔。具体来说，对于ResNets，作者使用了每个阶段的最后一个残差结构的特征激活输出。将这些残差模块输出表示为{C2, C3, C4, C5}，对应于conv2，conv3，conv4和conv5的输出。自上而下的过程采用上采样进行。上采样几乎都是采用内插值方法，即在原有图像像素的基础上在像素点之间采用合适的插值算法插入新的元素，从而扩大原图像的大小。通过对特征图进行上采样，使得上采样后的特征图具有和下一层的特征图相同的大小。根本上来说，侧边之间的横向连接是将上采样的结果和自下而上生成的特征图进行融合。我们将卷积神经网络中生成的对应层的特征图进行1×1的卷积操作，将之与经过上采样的特征图融合，得到一个新的特征图，这个特征图融合了不同层的特征，具有更丰富的信息。 这里1×1的卷积操作目的是改变channels，要求和后一层的channels相同。在融合之后还会再采用3*3的卷积核对每个融合结果进行卷积，目的是消除上采样的混叠效应，如此就得到了一个新的特征图。这样一层一层地迭代下去，就可以得到多个新的特征图。假设生成的特征图结果是P2，P3，P4，P5，它们和原来自底向上的卷积结果C2，C3，C4，C5一一对应。金字塔结构中所有层级共享分类层（回归层）。三、fast rcnn中的特征金字塔Fast rcnn中的ROI Pooling层使用region proposal的结果和特征图作为输入。经过特征金字塔，我们得到了许多特征图，作者认为，不同层次的特征图上包含的物体大小也不同，因此，不同尺度的ROI，使用不同特征层作为ROI pooling层的输入。大尺度ROI就用后面一些的金字塔层，比如P5；小尺度ROI就用前面一点的特征层，比如P4。但是如何确定不同的roi对应的不同特征层呢？作者提出了一种方法：，224是ImageNet的标准输入，k0是基准值，设置为5，代表P5层的输出（原图大小就用P5层），w和h是ROI区域的长和宽，假设ROI是112 * 112的大小，那么k = k0-1 = 5-1 = 4，意味着该ROI应该使用P4的特征层。k值做取整处理。这意味着如果RoI的尺度变小（比如224的1/2），那么它应该被映射到一个精细的分辨率水平。与RPN一样，FPN每层feature map加入3*3的卷积及两个相邻的1*1卷积分别做分类和回归的预测。在RPN中，实验对比了FPN不同层feature map卷积参数共享与否，发现共享仍然能达到很好性能，说明特征金字塔使得不同层学到了相同层次的语义特征。用于RPN的FPN：用FPN替换单一尺度的FMap。它们对每个级都有一个单一尺度的anchor（不需要多级作为其FPN）。它们还表明，金字塔的所有层级都有相似的语义层级。Faster RCNN：他们以类似于图像金字塔输出的方式观察金字塔。因此，使用下面这个公式将RoI分配到特定level。其中w，h分别表示宽度和高度。k是分配RoI的level。是w,h=224,224时映射的level。四、其他问题Q1：不同深度的feature map为什么可以经过upsample后直接相加？答：作者解释说这个原因在于我们做了end-to-end的training，因为不同层的参数不是固定的，不同层同时给监督做end-to-end training，所以相加训练出来的东西能够更有效地融合浅层和深层的信息。Q2：为什么FPN相比去掉深层特征upsample(bottom-up pyramid)对于小物体检测提升明显？（RPN步骤AR从30.5到44.9，Fast RCNN步骤AP从24.9到33.9）答：作者在poster里给出了这个问题的答案对于小物体，一方面我们需要高分辨率的feature map更多关注小区域信息，另一方面，如图中的挎包一样，需要更全局的信息更准确判断挎包的存在及位置。Q3：如果不考虑时间情况下，image pyramid是否可能会比feature pyramid的性能更高？答：作者觉得经过精细调整训练是可能的，但是image pyramid（金字塔）主要的问题在于时间和空间占用太大，而feature pyramid可以在几乎不增加额外计算量情况下解决多尺度检测问题。五、代码层面看FPN本部分截取自知乎文章：从代码细节理解 FPN，作者使用Mask-RCNN的源码辅助理解FPN结构，项目地址见MRCNN，关于MRCNN，文章『计算机视觉』RCNN学习_其三：Mask-RCNN会介绍。1、 怎么做的上采样？高层特征怎么上采样和下一层的特征融合的，代码里面可以看到:P5 = KL.Conv2D(256, (1, 1), name='fpn_c5p5')(C5)C5是 resnet最顶层的输出，它会先通过一个1*1的卷积层，同时把通道数转为256，得到FPN 的最上面的一层 P5。KL.UpSampling2D(size=(2, 2),name=\"fpn_p5upsampled\")(P5)Keras 的 API 说明告诉我们：也就是说，这里的实现使用的是最简单的上采样，没有使用线性插值，没有使用反卷积，而是直接复制。2、 怎么做的横向连接？P4 = KL.Add(name=\"fpn_p4add\") ([KL.UpSampling2D(size=(2, 2), name=\"fpn_p5upsampled\")(P5), KL.Conv2D(256,(1, 1), name='fpn_c4p4')(C4)])这里可以很明显的看到，P4就是上采样之后的 P5加上1*1 卷积之后的 C4，这里的横向连接实际上就是像素加法，先把 P5和C4转换到一样的尺寸，再直接进行相加。注意这里对从 resnet抽取的特征图做的是 1*1 的卷积：1x1的卷积我认为有三个作用：使bottom-up对应层降维至256；缓冲作用，防止梯度直接影响bottom-up主干网络，更稳定；组合特征。3、 FPN自上而下的网络结构代码怎么实现？# 先从 resnet 抽取四个不同阶段的特征图 C2-C5。 _, C2, C3, C4, C5 = resnet_graph(input_image, config.BACKBONE,stage5=True, train_bn=config.TRAIN_BN) # Top-down Layers 构建自上而下的网络结构 # 从 C5开始处理，先卷积来转换特征图尺寸 P5 = KL.Conv2D(256, (1, 1), name='fpn_c5p5')(C5) # 上采样之后的P5和卷积之后的 C4像素相加得到 P4，后续的过程就类似了 P4 = KL.Add(name=\"fpn_p4add\")([ KL.UpSampling2D(size=(2, 2), name=\"fpn_p5upsampled\")(P5), KL.Conv2D(256, (1, 1),name='fpn_c4p4')(C4)]) P3 = KL.Add(name=\"fpn_p3add\")([ KL.UpSampling2D(size=(2, 2), name=\"fpn_p4upsampled\")(P4), KL.Conv2D(256, (1, 1), name='fpn_c3p3')(C3)]) P2 = KL.Add(name=\"fpn_p2add\")([ KL.UpSampling2D(size=(2, 2),name=\"fpn_p3upsampled\")(P3), KL.Conv2D(256, (1, 1), name='fpn_c2p2')(C2)]) # P2-P5最后又做了一次3*3的卷积，作用是消除上采样带来的混叠效应 # Attach 3x3 conv to all P layers to get the final feature maps. P2 = KL.Conv2D(256, (3, 3), padding=\"SAME\", name=\"fpn_p2\")(P2) P3 = KL.Conv2D(256, (3, 3), padding=\"SAME\",name=\"fpn_p3\")(P3) P4 = KL.Conv2D(256, (3, 3), padding=\"SAME\",name=\"fpn_p4\")(P4) P5 = KL.Conv2D(256, (3, 3), padding=\"SAME\",name=\"fpn_p5\")(P5) # P6 is used for the 5th anchor scale in RPN. Generated by # subsampling from P5 with stride of 2. P6 = KL.MaxPooling2D(pool_size=(1, 1), strides=2,name=\"fpn_p6\")(P5) # 注意 P6是用在 RPN 目标区域提取网络里面的，而不是用在 FPN 网络 # Note that P6 is used in RPN, but not in the classifier heads. rpn_feature_maps = [P2, P3, P4, P5, P6] # 最后得到了5个融合了不同层级特征的特征图列表；注意 P6是用在 RPN 目标区域提取网络里面的，而不是用在 FPN 网络；另外这里 P2-P5最后又做了一次3*3的卷积，作用是消除上采样带来的混叠效应。4、 如何确定某个 ROI 使用哪一层特征图进行 ROIpooling ?看代码：# Assign each ROI to a level in the pyramid based on the ROI area. # 这里的 boxes 是 ROI 的框，用来计算得到每个 ROI 框的面积 y1, x1, y2, x2 = tf.split(boxes, 4, axis=2) h = y2 - y1 w = x2 - x1 # Use shape of first image. Images in a batch must have the same size. # 这里得到原图的尺寸，计算原图的面积 image_shape = parse_image_meta_graph(image_meta)['image_shape'][0] # Equation 1 in the Feature Pyramid Networks paper. Account for # the fact that our coordinates are normalized here. # e.g. a 224x224 ROI (in pixels) maps to P4 # 原图面积 image_area = tf.cast(image_shape[0] * image_shape[1], tf.float32) # 分两步计算每个 ROI 框需要在哪个层的特征图中进行 pooling roi_level = log2_graph(tf.sqrt(h * w) / (224.0 / tf.sqrt(image_area))) roi_level = tf.minimum(5, tf.maximum( 2, 4 + tf.cast(tf.round(roi_level), tf.int32)))不同尺度的ROI，使用不同特征层作为ROI pooling层的输入，大尺度ROI就用后面一些的金字塔层，比如P5；小尺度ROI就用前面一点的特征层，比如P4。那怎么判断ROI改用那个层的输出呢？论文的 K 使用如下公式，代码做了一点更改，替换为roi_level：# 代码里面的计算替换为以下计算方式： roi_level = min(5, max(2, 4 + log2(sqrt(w * h) / ( 224 / sqrt(image_area)) ) ) )224是ImageNet的标准输入，k0是基准值，设置为5，代表P5层的输出（原图大小就用P5层），w和h是ROI区域的长和宽，image_area是输入图片的长乘以宽，即输入图片的面积，假设ROI是112 * 112的大小，那么k = k0-1 = 5-1 = 4，意味着该ROI应该使用P4的特征层。k值会做取整处理，防止结果不是整数。5、 上面得到的5个融合了不同层级的特征图怎么使用？可以看到，这里只使用2-5四个特征图:for i, level in enumerate(range(2, 6)): # 先找出需要在第 level 层计算ROI ix = tf.where(tf.equal(roi_level, level)) level_boxes = tf.gather_nd(boxes, ix) # Box indicies for crop_and_resize. box_indices = tf.cast(ix[:, 0], tf.int32) # Keep track of which box is mapped to which level box_to_level.append(ix) # Stop gradient propogation to ROI proposals level_boxes = tf.stop_gradient(level_boxes) box_indices = tf.stop_gradient(box_indices) # Crop and Resize # From Mask R-CNN paper: \"We sample four regular locations, so # that we can evaluate either max or average pooling. In fact, # interpolating only a single value at each bin center (without # pooling) is nearly as effective.\" # # Here we use the simplified approach of a single value per bin, # which is how it's done in tf.crop_and_resize() # Result: [batch * num_boxes, pool_height, pool_width, channels] # 使用 tf.image.crop_and_resize 进行 ROI pooling pooled.append(tf.image.crop_and_resize( feature_maps[i], level_boxes, box_indices, self.pool_shape, method=\"bilinear\"))对每个 box，都提取其中每一层特征图上该box对应的特征，然后组成一个大的特征列表pooled。6、 金字塔结构中所有层级共享分类层是怎么回事？先看代码：# ROI Pooling # Shape: [batch, num_boxes, pool_height, pool_width, channels] # 得到经过 ROI pooling 之后的特征列表 x = PyramidROIAlign([pool_size, pool_size], name=\"roi_align_classifier\")([rois, image_meta] + feature_maps) # 将上面得到的特征列表送入 2 个1024通道数的卷积层以及 2 个 rulu 激活层 # Two 1024 FC layers (implemented with Conv2D for consistency) x = KL.TimeDistributed(KL.Conv2D(1024, (pool_size, pool_size), padding=\"valid\"), name=\"mrcnn_class_conv1\")(x) x = KL.TimeDistributed(BatchNorm(), name='mrcnn_class_bn1')(x, training=train_bn) x = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.Conv2D(1024, (1, 1)), name=\"mrcnn_class_conv2\")(x) x = KL.TimeDistributed(BatchNorm(), name='mrcnn_class_bn2')(x, training=train_bn) x = KL.Activation('relu')(x) shared = KL.Lambda(lambda x: K.squeeze(K.squeeze(x, 3), 2), name=\"pool_squeeze\")(x) # 分类层 # Classifier head mrcnn_class_logits = KL.TimeDistributed(KL.Dense(num_classes), name='mrcnn_class_logits')(shared) mrcnn_probs = KL.TimeDistributed(KL.Activation(\"softmax\"), name=\"mrcnn_class\")(mrcnn_class_logits) # BBOX 的位置偏移回归层 # BBox head # [batch, boxes, num_classes * (dy, dx, log(dh), log(dw))] x = KL.TimeDistributed(KL.Dense(num_classes * 4, activation='linear'), name='mrcnn_bbox_fc')(shared) # Reshape to [batch, boxes, num_classes, (dy, dx, log(dh), log(dw))] s = K.int_shape(x) mrcnn_bbox = KL.Reshape((s[1], num_classes, 4), name=\"mrcnn_bbox\")(x)这里的PyramidROIAlign得到的 x就是上面一步得到的从每个层的特征图上提取出来的特征列表，这里对这个特征列表先接两个1024通道数的卷积层，再分别送入分类层和回归层得到最终的结果。也就是说，每个 ROI 都在P2-P5中的某一层得到了一个特征，然后送入同一个分类和回归网络得到最终结果。FPN中每一层的heads 参数都是共享的，作者认为共享参数的效果也不错就说明FPN中所有层的语义都相似。7、 它的思想是什么？把高层的特征传下来，补充低层的语义，这样就可以获得高分辨率、强语义的特征，有利于小目标的检测。8、 横向连接起什么作用？如果不进行特征的融合（也就是说去掉所有的1x1侧连接），虽然理论上分辨率没变，语义也增强了，但是AR下降了10%左右！作者认为这些特征上下采样太多次了，导致它们不适于定位。Bottom-up的特征包含了更精确的位置信息。六、资源资料Feature Pyramid Networks for Object Detection（CVPR 2017论文）知乎：特征金字塔网络FPN知乎：从代码细节理解 FPNFPN特征金字塔网络--论文解读详解何恺明团队4篇大作 | 从特征金字塔网络、Mask R-CNN到学习分割一切源码资料：官方：Caffe2https://github.com/facebookresearch/Detectron/tree/master/configs/12_2017_baselinesCaffehttps://github.com/unsky/FPNPyTorchhttps://github.com/kuangliu/pytorch-fpn (just the network)MXNethttps://github.com/unsky/FPN-mxnetTensorflowhttps://github.com/yangxue0827/FPN_Tensorflow"}
{"content2":"目录介绍API分类使用‘视觉’API完成的Demo点击直接看干货介绍从3月份Google家的阿尔法狗打败韩国围棋冠军选手李世石，到之后微软Build2016大会宣布的“智能机器人”战略。种种迹象表明未来慢慢会进入“人工智能”时代，人工智能不再像以前那样听起来高大上，普通的码农屌丝也能开发出具备人类智慧的APP。听起来是不是很叼？以前是这样的：You：吴博士，您研究的主要方向是撒？吴博士：人工智能。You：挖槽，好叼。你觉得未来机器人会不会控制人类呢？吴博士：......现在是这样的：You：博士来来来，看看我这个App（将‘高富帅’才会用的肾8递过去）。吴博士：嗯，很普通嘛。有什么特别的吗？You：用它自拍一下试试...吴博士：（拍完照）。窝草，它怎么认识我姓Wu？年龄显示也太大了吧，我才42啊！You：博士，这是一个具备'人类意识'的App额...博士你听，它从你脸色判断出你常年肾透支啊...吴博士：......借用微软认知服务官网上的一段话：“Give your Apps a human sideKnock down barriers between you and your ideas. Enable natural and contextual interaction with tools that augment users' experiences via the power of machine-based AI. Plug them in and bring your ideas to life.”翻译过来大概是：“让你的应用拥有人的智慧你的想法不再只是想法。基于机器学习的人工智能使自然的和基于语境的人机交互变为可能，为你的应用增强用户体验。现在你就可以在你的应用中接入这些智能，把你的想法变成现实。”微软已经公开了“认知服务”的一些API，用起来特别简单，NuGet上有现成的Package。主要分为5大类：API分类视觉类人脸识别情绪识别视频检测计算机视觉语音类声纹识别自定义智能识别语音识别语言类必应拼写检查文本分析网络级语言模型语言分析语言理解知识类学术搜索实体链接推荐知识搜索搜索类必应图片搜索必应新闻搜索必应网络搜索必应自动推荐必应视频搜索以上5大类中，每类API都可以免费试用，当然有次数限制。对于那种想体验一下的童鞋完全够用，如果想要大范围商用，花钱买也行。使用“视觉”类API完成的demo废话说太多，还是要上点干货。空余时间利用“视觉”类API做了一个Demo。具备以下功能：图片分析。描述图片、给图片贴Tags、判断图片是否涉黄、找出图片中的人脸等等；人脸分析。判断人脸年龄、性别（跟How-Old颜值相机类似）；人脸情绪分析。判断每张脸的表情，是否‘生气’、‘高兴’、‘惊讶’...可以说功能强大，呵呵，至少比how-old颜值相机功能大一点 :)源码下载（干活）：https://github.com/sherlockchou86/ComputerVision.UWP手机上也可以运行，图片有三种方式：URL网络图片、本地上传、以及相机拍照。都有对应的API。没有做成App上传到Store，感觉界面并不太友好，只适合开发人员看看而已。下面是截图：【1】人脸识别【2】情绪分析【3】图片描述（注意description中的内容，直接可以识别图中人物为川普）【4】判断图片是否涉黄（虽然图中adult content为false，但是racy content为true，说明露得不是很多）【5】给图片打标签（分类），可以看到，非常准确本文仅介绍了跟“视觉”有关的API使用方法，大概就相当于人类的“眼睛”。其他诸如“嘴巴说话”、“大脑分析语音”、“理解文本”以及“掌握知识”等一系列API暂未涉及到。本文仅是开端，大家可以试试其他：）开源有益，多谢点赞！"}
{"content2":"机器视觉技术是计算机学科的一个重要分支，自起步发展至今，机器视觉已经有20多年的历史，其功能以及应用范围随着工业自动化的发展逐渐完善和推广。20世纪50年代开始研究二维图像的统计模式识别。60年代Roberts开始进行三维机器视觉的研究。70年代中，MIT人工智能实验室正式开设“机器视觉”课程。80年代开始，开始了全球性的研究热潮，机器视觉获得了蓬勃发展，新概念、新理论不断涌现。现在，机器视觉仍然是一个非常活跃的研究领域，与之相关的学科涉及：图像处理、计算机图形学、模式识别、人工智能、人工神经元网络等。机器视觉在中国的发展历史：1990年以前，仅仅在大学和研究所中有一些研究图像处理和模式识别的实验室。20世纪90年代初，一些来自这些研究机构的工程师成立了他们自己的视觉公司，开发了第一代图像处理产品，例如基于ISA总线的灰度级图像采集卡，和一些简单的图像处理软件库，他们的产品在大学的实验室和一些工业场合得到了应用，人们能够做一些基本的图像处理和分析工作。1990-1998年为初级阶段。期间真正的机器视觉系统市场销售额微乎其微。主要的国际机器视觉厂商还没有进入中国市场。自从1998年，越来越多的电子和半导体工厂，包括香港和台湾投资的工厂，落户广东和上海。带有机器视觉的整套的生产线和高级设备被引入中国。1998-2002年定义为机器视觉概念引入期。在此阶段，许多著名视觉设备供应商，诸如：Matsushita, Omron，Cognex, DVT, CCS, Data Translation, Matrix, Coreco,开始接触中国市场寻求本地合作伙伴，但符合要求的本地合作伙伴寥若晨星。例如，北京和利时电机技术有限公司曾经被五家外国公司选做主要代理商或解决方案提供商。从2002年至今，我们称之为机器视觉发展期，中国机器视觉呈快速增长趋势。机器视觉发展现状当前，机器视觉的应用已经超越了其传统的检验领域，向着更深层、更为多样化的领域扩展。与此同时，机器视觉产品的采购量也在节节攀升。创科创建于2003年。成立以来,我们致力于图像算法的研究,成功地开发出国内先进的机器视觉软件包,并已经应用在不同的领域上,所推出的软件产品填补了国内相关行业的空白。经过多年的努力我们已发展为实力雄厚,技术精湛的机器视觉软件行家,开发的产品达到国际先进水平,并于2006年通过软件企业认证,是目前国内唯一认证的机器视觉软件开发商。"}
{"content2":"一、图像的传统特征介绍1.HOG方向梯度直方图（Histogram of Oriented Gradient, HOG）特征是一种在计算机视觉和图像处理中用来进行物体检测的特征描述子。它通过计算和统计图像局部区域的梯度方向直方图来构成特征。例如，区别圆形的大小或者不一样的形状，此时有用的是边缘信息的特征，而图像的梯度（x和y导数）的大小在边缘和拐角处（突然强度变化的区域）很大，因此可以作为区分的特征。具体提取参考：2.LBP局部二值模式（Local Binary Pattern, LBP）是一种用来描述图像局部纹理特征的算子，它具有旋转不变性和灰度不变性等显著的优点。3.HaarHaar特征分为三类：边缘特征、线性特征、中心特征和对角线特征，组合成特征模板。特征模板内有白色和黑色两种矩形，并定义该模板的特征值为白色矩形像素和减去黑色矩形像素和。Haar特征值反映了图像的灰度变化情况。注：参考链接：https://blog.csdn.net/q123456789098/article/details/52748918二、传统的图像特征提取与深度学习对比难点：传统的图像特征提取需要自主选择提取哪些特征，并需要相关领域知识，像角点、边缘、纹理，只有使用不同的特征才能更好地描述不同的类别，而在训练的时候，面对很多特征需要对大量参数进行微调。优点：相比深度学习需要大量数据及长时间的训练且应用场景有限，像铝表面、布匹的缺陷瑕疵检测，使用传统的特征可以短时间、高效的完成。深度学习：采用端到端的自动学习，提取出需要的特征，提供了最具描述性、显著性的特征。理解传统的计算机视觉实际上真的有助于你更好的使用深度学习。例如，计算机视觉中最常见的神经网络是卷积神经网络。但是什么是卷积？它实际上是一种广泛使用的图像处理技术（例如Sobel边缘检测）。了解卷积有助于了解神经网络的内在机制，在解决问题时，它可以帮助你设计和调整模型。参考https://blog.csdn.net/weixin_41036461/article/details/80587405三、深度学习常用的数据增强方法随机裁剪颜色抖动，具体有：随机调整饱和度、亮度、对比度，也可以叫做扭转颜色随机角度旋转加入高斯噪声图像翻转，镜像import os import math import random import numpy as np from PIL import Image, ImageEnhance from multiprocessing import Pool def random_crop(img, size, scale=[0.08, 1.0], ratio=[3. / 4., 4. / 3.]): aspect_ratio = math.sqrt(random.uniform(*ratio)) w = 1. * aspect_ratio h = 1. / aspect_ratio bound = min((float(img.size[0]) / img.size[1]) / (w**2), (float(img.size[1]) / img.size[0]) / (h**2)) scale_max = min(scale[1], bound) scale_min = min(scale[0], bound) target_area = img.size[0] * img.size[1] * random.uniform(scale_min, scale_max) target_size = math.sqrt(target_area) w = int(target_size * w) h = int(target_size * h) i = random.randint(0, img.size[0] - w) j = random.randint(0, img.size[1] - h) img = img.crop((i, j, i + w, j + h)) img = img.resize((size, size), Image.LANCZOS) return img def randomColor(image): \"\"\" 对图像进行颜色抖动 :param image: PIL的图像image :return: 有颜色色差的图像image \"\"\" random_factor = np.random.randint(0, 31) / 10. # 随机因子 color_image = ImageEnhance.Color(image).enhance(random_factor) # 调整图像的饱和度 random_factor = np.random.randint(10, 21) / 10. # 随机因子 brightness_image = ImageEnhance.Brightness( color_image).enhance(random_factor) # 调整图像的亮度 random_factor = np.random.randint(10, 21) / 10. # 随机因1子 contrast_image = ImageEnhance.Contrast( brightness_image).enhance(random_factor) # 调整图像对比度 random_factor = np.random.randint(0, 31) / 10. # 随机因子 return ImageEnhance.Sharpness(contrast_image).enhance(random_factor) def randomRotation(image, mode=Image.BICUBIC): \"\"\" 对图像进行随机任意角度(0~360度)旋转 :param mode 邻近插值,双线性插值,双三次B样条插值(default) :param image PIL的图像image :return: 旋转转之后的图像 \"\"\" random_angle = np.random.randint(1, 360) return image.rotate(random_angle, mode) def distort_color(img): def random_brightness(img, lower=0.5, upper=1.5): e = random.uniform(lower, upper) return ImageEnhance.Brightness(img).enhance(e) def random_contrast(img, lower=0.5, upper=1.5): e = random.uniform(lower, upper) return ImageEnhance.Contrast(img).enhance(e) def random_color(img, lower=0.5, upper=1.5): e = random.uniform(lower, upper) return ImageEnhance.Color(img).enhance(e) ops = [random_brightness, random_contrast, random_color] random.shuffle(ops) img = ops[0](img) img = ops[1](img) img = ops[2](img) return img def randomGaussian(image, mean=0.2, sigma=0.3): \"\"\" 对图像进行高斯噪声处理 :param image: :return: \"\"\" def gaussianNoisy(im, mean=0.2, sigma=0.3): \"\"\" 对图像做高斯噪音处理 :param im: 单通道图像 :param mean: 偏移量 :param sigma: 标准差 :return: \"\"\" for _i in range(len(im)): im[_i] += random.gauss(mean, sigma) return im # 将图像转化成数组 img = np.asarray(image) img.flags.writeable = True # 将数组改为读写模式 width, height = img.shape[:2] img_r = gaussianNoisy(img[:, :, 0].flatten(), mean, sigma) img_g = gaussianNoisy(img[:, :, 1].flatten(), mean, sigma) img_b = gaussianNoisy(img[:, :, 2].flatten(), mean, sigma) img[:, :, 0] = img_r.reshape([width, height]) img[:, :, 1] = img_g.reshape([width, height]) img[:, :, 2] = img_b.reshape([width, height]) return Image.fromarray(np.uint8(img)) def randomFlip(image): #图像翻转（类似于镜像，镜子中的自己） #FLIP_LEFT_RIGHT,左右翻转 #FLIP_TOP_BOTTOM,上下翻转 #ROTATE_90, ROTATE_180, or ROTATE_270.按照角度进行旋转，与randomRotate()功能类似 return image.transpose(Image.FLIP_LEFT_RIGHT)"}
{"content2":"2007年07月15日00:23:00随着苹果的iPhone和微软的surfacecomputer的发布，被采用的最新技术multitouch也日益引起业界的注意，与之相关的互动游戏、互动投影、互动多媒体等等技术，也逐步走入人们的视野。互动的本质在于信息反馈，从人获取信息的角度看，大部分的信息获取都是通过眼睛完成的，那么也可以断定，在以计算机为辅助工具的机器世界中，互动也几乎都跟摄像头密切相关，从而引出计算机视觉的一个巨大应用领域：互动世界。利用计算机视觉能做哪些互动应用呢，一个通用的计算机视觉互动系统应该包括下面设备和模块：—摄像头：相当于人的眼睛，可以是普通的可见光摄像头，也可以是红外光摄像头；用于获取当前视角下的视频图像；—智能图像识别模块：相当与人的大脑，用于对摄像头获取的视频图像进行分析，识别，以得到某种特定的信息；从现有的开发工具看，opencv是一个不错的起步平台；—应用程序接口模块：相当与人的神经系统，将识别模块获得的信息按照某种协议传输给应用程序；现在常用的接口模块还是跟鼠标消息相对接，当然最新的multitouch也使用另外的接口协议；—应用程序：相当于场景实现，即根据特定的动作信息，展示出不同的场景或者动作实现。现在看，互动应用有如下的方式：（1）互动游戏：利用摄像头，捕捉人体各个的肢体动作、表情动作，以这些动作信息代替鼠标和键盘等传统人机界面，从而创造出一种新的游戏模式。（2）互动展示：利用摄像头获取并识别人的手、手指、四肢或者其它物品在投影屏幕上的位置，实现投影屏幕上的触摸选择，打开界面、转换画面、信息查询等控制功能。（3）电子白板：用摄像头识别书写笔或者手指的动作，配合投影仪使用，以电子屏、电子笔或手指代替普通的白板、水性笔，实现手写输入，操控电脑的应用程序等功能；（4）multitouch：通过摄像头和投影仪，实现人机界面的多点信息输入，实现自鼠标发明以来的最大变革，可以想像，利用multitouch，以后所有的计算机将完全实现虚拟的键盘和鼠标。上面只是简单地写了几个所能想到的应用。实际上好好利用计算机视觉系统，可以在互动世界中，创造出很多意想不到的应用。摄像头与投影仪和显示屏的结合，仍然仅仅是技术，是工具，更加具有挑战性的是如何利用这些工具，创造出绚丽多彩的应用世界。这倒是应了一句话：人有多大胆，地有多大产，不怕做不到，就怕想不到！更多的参考http://www.dotouch.cn/"}
{"content2":"今日CS.CV计算机视觉论文速览Fri, 29 Mar 2019Totally 30 papersInteresting:📚TensorMask, (from Facebook AI Research)📚ThunderNet, 提出了一种基于ARM平台的实时目标检测系统，通过轻量级的前端和内容提升/空间注意模块提升了检测精度。 (from 国防科技大学 旷视)框架图及效率如下图所示：📚自由视角下的视频合成, 基于体积视觉外壳建立稀疏点云，并利用连接元件标记算法有效抽取三维BBOX，随后重建了ROI区域的稠密点云和多边形表面，并得到的新视角下的新图像。(from KDDI Research, Inc)📚Pyramid Mask Text Detector (PMTD), 基于金字塔掩膜的文字检测，将2D soft mask投影到3D空间，并引入了新的平面聚类算法来驱动基于物体三维外形文字box的优化。(from 商汤)流程框图：dataset:ICDAR 2017 MLT📚Many Task Learning (MaTL) , 利用了多任务路由层，将多种任务综合到统一模型中去。(from 阿姆斯特丹大学)实验结果：📚皮革表面的缺陷分割, (from 台湾逢甲大学)研究人员所用的系统和一些检测结果：📚spatially localized atlas network tiles (SLANT), 基于三维CT数据的三维脑部组织分割，基于多个三维空间网络来进行分割，每个网络负责处理一个空间部分的信息, 最后利用标签融合实现三维重构。将原先需要30小时的计算缩短到了十五分钟(from Vanderbilt University）code:https://github.com/MASILab/SLANTbrainSegDaily Computer Vision Papers1.Title: TensorMask: A Foundation for Dense Object SegmentationAuthors:Xinlei Chen, Ross Girshick, Kaiming He, Piotr Dollár2.Title: Fast video object segmentation with Spatio-Temporal GANsAuthors:Sergi Caelles, Albert Pumarola, Francesc Moreno-Noguer, Alberto Sanfeliu, Luc Van Gool3.Title: 3D Whole Brain Segmentation using Spatially Localized Atlas Network TilesAuthors:Yuankai Huo, Zhoubing Xu, Yunxi Xiong, Katherine Aboud, Prasanna Parvathaneni, Shunxing Bao, Camilo Bermudez, Susan M. Resnick, Laurie E. Cutting, Bennett A. Landman4.Title: Automatic Defect Segmentation on Leather with Deep LearningAuthors:Sze-Teng Liong, Y.S. Gan, Yen-Chang Huang, Chang-Ann Yuan, Hsiu-Chi Chang5.Title: Many Task Learning with Task RoutingAuthors:Gjorgji Strezoski, Nanne van Noord, Marcel Worring6.Title: Robust, fast and accurate: a 3-step method for automatic histological image registrationAuthors:Johannes Lotz, Nick Weiss, Stefan Heldmann7.Title: Depth from a polarisation + RGB stereo pairAuthors:Dizhong Zhu, William A.P. Smith8.Title: Road User Detection in VideosAuthors:Hughes Perreault, Guillaume-Alexandre Bilodeau, Nicolas Saunier, Pierre Gravel9.Title: Describing like humans: on diversity in image captioningAuthors:Qingzhong Wang, Antoni B. Chan10.Title: High Fidelity Face Manipulation with Extreme Pose and ExpressionAuthors:Chaoyou Fu, Yibo Hu, Xiang Wu, Guoli Wang, Qian Zhang, Ran He11.Title: AED-Net: An Abnormal Event Detection NetworkAuthors:Tian Wang, Zichen Miao, Yuxin Chen, Yi Zhou, Guangcun Shan, Hichem Snoussi12.Title: Smooth Adversarial ExamplesAuthors:Hanwei Zhang, Yannis Avrithis, Teddy Furon, Laurent Amsaleg13.Title: Feature Intertwiner for Object DetectionAuthors:Hongyang Li, Bo Dai, Shaoshuai Shi, Wanli Ouyang, Xiaogang Wang14.Title: Feature Fusion Encoder Decoder Network For Automatic Liver Lesion SegmentationAuthors:Xueying Chen, Rong Zhang, Pingkun Yan15.Title: FastFCN: Rethinking Dilated Convolution in the Backbone for Semantic SegmentationAuthors:Huikai Wu, Junge Zhang, Kaiqi Huang, Kongming Liang, Yizhou Yu16.Title: Pyramid Mask Text DetectorAuthors:Jingchao Liu, Xuebo Liu, Jie Sheng, Ding Liang, Xin Li, Qingjie Liu17.Title: A Fast Free-viewpoint Video Synthesis Algorithm for Sports ScenesAuthors:Jun Chen, Ryosuke Watanabe, Keisuke Nonaka, Tomoaki Konno, Hiroshi Sankoh, Sei Naito18.Title: BubbleNets: Learning to Select the Guidance Frame in Video Object Segmentation by Deep Sorting FramesAuthors:Brent A. Griffin, Jason J. Corso19.Title: ThunderNet: Towards Real-time Generic Object DetectionAuthors:Zheng Qin, Zeming Li, Zhaoning Zhang, Yiping Bao, Gang Yu, Yuxing Peng, Jian Sun20.Title: InfoMask: Masked Variational Latent Representation to Localize Chest DiseaseAuthors:Saeid Asgari Taghanaki, Mohammad Havaei, Tess Berthier, Francis Dutil, Lisa Di Jorio, Ghassan Hamarneh, Yoshua Bengio21.Title: Network Slimming by Slimmable Networks: Towards One-Shot Architecture Search for Channel NumbersAuthors:Jiahui Yu, Thomas Huang22.Title: Zero-shot Image Recognition Using Relational Matching, Adaptation and CalibrationAuthors:Debasmit Das, C. S. George Lee23.Title: Align2Ground: Weakly Supervised Phrase Grounding Guided by Image-Caption AlignmentAuthors:Samyak Datta, Karan Sikka, Anirban Roy, Karuna Ahuja, Devi Parikh, Ajay Divakaran24.Title: Laplace Landmark LocalizationAuthors:Joseph P Robinson, Yuncheng Li, Ning Zhang, Yun Fu, and Sergey Tulyakov25.Title: Improving MAE against CCE under Label NoiseAuthors:Xinshao Wang, Elyor Kodirov, Yang Hua, Neil M. Robertson26.Title: GANs-NQM: A Generative Adversarial Networks based No Reference Quality Assessment Metric for RGB-D Synthesized ViewsAuthors:Suiyi Ling, Jing Li, Junle Wang, Patrick Le Callet27.Title: Model Vulnerability to Distributional Shifts over Image Transformation SetsAuthors:Riccardo Volpi, Vittorio Murino28.Title: Outlier-Robust Spatial Perception: Hardness, General-Purpose Algorithms, and GuaranteesAuthors:Vasileios Tzoumas, Pasquale Antonante, Luca Carlone29.Title: What does AI see? Deep segmentation networks discover biomarkers for lung cancer survivalAuthors:Stephen Baek, Yusen He, Bryan G. Allen, John M. Buatti, Brian J. Smith, Kristin A. Plichta, Steven N. Seyedin, Maggie Gannon, Katherine R. Cabel, Yusung Kim, Xiaodong Wu30.Title: Scaling up the randomized gradient-free adversarial attack reveals overestimation of robustness using established attacksAuthors:Francesco Croce, Jonas Rauber, Matthias HeinPapers from arxiv.org更多精彩请移步主页pic from pixels.com"}
{"content2":"http://www.aforgenet.com/framework/downloads.htmlAForge.NET Framework-2.2.5简介AForge.NET是一个专门为开发者和研究者基于C#框架设计的，他包括计算机视觉与人工智能，图像处理，神经网络，遗传算法，机器学习，模糊系统，机器人控制等领域。折叠编辑本段主要架构这个框架由一系列的类库组成。主要包括有:AForge.Imaging -- 一些日常的图像处理和过滤器AForge.Vision -- 计算机视觉应用类库AForge.Neuro -- 神经网络计算库AForge.Genetic -进化算法编程库AForge.MachineLearning -- 机器学习类库AForge.Robotics -- 提供一些机器学习的工具类库AForge.Video -- 一系列的视频处理类库AForge.Fuzzy -- 模糊推理系统类库AForge.Controls-- 图像，三维，图表显示控件折叠编辑本段特点该框架架构合理，易于扩展，涉及多个较前沿的技术模块，可以为相关开发人员或科研人员的工作提供极大便利。该框架使用LGPLv3协议，2.0以前版本遵循GPLv3协议，如果对于协议有协商需要可以联系项目作者。折叠编辑本段开发工具Image Processing Prototyper该工具意在让图像算法和识别算法的测试更简单更快捷，让开发人员更专注于算法本身，而不是构建测试界面和用例。AForge，NET Debugging Visualizers虽然Image Processing Prototyper方便快捷，但是并不是所有类都可以受惠，AForge，NET Debugging Visualizers就是一个补充。它支持以下4类的调试:System.Drawing.ImageSystem.Drawing.Imaging.BitmapDataAForge.Imaging.UnmanagedImageAForge.Math.Histogram折叠编辑本段参考资料AFORGE .NET 的资料相当丰富，官方SVN自带例子若干，博客园，51CTO等技术网站均有大量相关文章。下面是一些示例展示。1.基于符号识别的3D现实增强技术AR技术2.基于模糊系统的自动导航fuzzy3.运动检测4.2D增强技术折叠编辑本段开源库折叠GRATF符号识别和目标追踪的库，可以用于机器人控制，当然也可以用于现实增强。折叠ImageProcessingLab基于C#的图像处理库，提供了一系列可用于AForge，Net的接口和工具。"}
{"content2":"2018工程院院士一、机械与运载工程学部(123人)陈学东  邓宗全  丁荣军  董春鹏  樊会涛  冯培德  冯煜芳  甘晓华  高金吉  关杰  郭东明  何琳  侯晓  黄庆学  黄先祥  蒋庄德  金东寒  李椿萱  李德群  李骏  李魁武  李培根  李钊  林忠钦  刘人怀  刘怡昕  刘永才  卢秉恒  路甬祥  马伟明  邱志明  孙聪  孙逢春  谭建荣  唐长红  田红旗(女)  王华明  王玉明  王振国  吴光辉  吴有生  夏长亮  杨德森  杨凤田  杨华勇  杨绍卿  尹泽勇  尤政  张军  张彦仲  钟志华  周济  周志成  朱能鸿  朱英富二、信息与电子工程学部(124人)序号姓名出生年2018年龄领域序号姓名出生年2018年龄领域1柴天佑194772控制，东北大学21桂卫华195068过程控制，武汉，长沙2陈纯195563计算机应用22何友信息融合、雷达3陈杰196553人工智能，同济，北京理工23姜会林光学机械4陈鲸194079通信与信息系统24李德仁测量遥感5陈良惠光电25李德毅194474控制、人工智能，北京6陈志杰空中交通管理系统技术，军队航空26李国杰194375计算机体系结构、并行算法、人工智能、计算机网络、信息技术，中科院北京7陈左宁(女)高性能计算机，系统软件27李天初1945光学，贵阳人，中国计量院8戴浩1945军队指挥信息系统28李同保光学9戴琼海196454控制，东北大学29廖湘科计算机系统软件与通用操作系统10邓中翰大规模集成电路，芯片30刘玠194375冶金控制系统，辽宁鞍山11丁文华195662传播技术系统，传媒大学，中央电视台31刘永坚航空电子对抗12段宝岩195563机械电子，西安电子科大32刘韵洁通信网络13樊邦奎无人侦查，信号处理33刘泽金激光14范滇元激光34陆军预警机信息系统15方滨兴网络安全35卢锡城高性能计算机16方家熊光传感36吕跃广光电17费爱国数据链，信息系统37倪光南计算机，联想集团18封锡盛194177水下机器人，沈阳38宁滨195959控制系统、轨道交通控制，北京交大19高文195662计算机智能算法与系统研究，在高效视频编解码算法与标准化、图像检索技术、视频分析技术、人脸识别技术、手语识别技术等方。北京大学39潘云鹤194672同济建筑，计算机图形、人工智能、CAD和工业设计，和李耘的智能设计相仿，浙大20龚惠兴光电40沈昌祥海军信息系统研制41孙家广194672制造系统信息化，计算机图形学、计算机辅助设计、软件系统建模、分析，国家信息化部，北京61张广军测试计量42孙优贤194078造纸、生物化工控制、在线优化、故障诊断、容错控制，浙大62张尧学计算机网络43谭久彬精密测量仪器63张钟华计量仪器仪表44王恩东计算机结构，浪潮计算机64赵沁平1948计算机软件与虚拟现实45王沙飞信号处理与信息安全65郑南宁1952人工智能、计算机视觉与模式识别46王天然194375智能机器人，机器人与工业自动化，沈阳66庄松林1940光电47韦钰(女)生物电子学，分子电子学48吴澄194078控制，复杂生产制造过程实时智能控制与优化49邬贺铨数字光纤通信50邬江兴通信、计算机网络51吴建平计算机网络52吴曼青雷达53吴伟仁遥感、测控、深空探测54吾守尔•斯拉木信息处理、网络安全55徐扬生195860机器人与智能控制，香港中文大学56许祖彦固态激光器57杨小牛信号处理58于全光纤通信、军队通信60余少华光纤通信、网络通信三、化工、冶金与材料工程学部(107人)钱锋九、工程管理学部(33人)曹耀峰  柴洪峰  陈晓红(女)  丁烈云  范国滨  胡文瑞  黄维和  金智新  凌文  刘德培  刘合  刘玠  刘人怀  卢春房  栾恩杰  邵安林  孙永福  王安  王基铭  王金南  王陇德  王玉普  向巧(女)  杨善林  赵晓哲  郑静晨  郑南宁  周建平2018科学院院士信息技术科学部（94人）包为民（1960，58岁，军队总装备，制导与控制）保铮  陈定昌  陈桂林  陈国良陈翰馥（1937，81岁，中科院，随机系统的能观性、不用初值的状态估计，最优随机奇异控制）陈俊亮  陈星弼  陈星旦  褚君浩戴汝为（1934左右，84岁，中科院，钱学森学生，自动控制、系统科学、思维科学、模式识别、人工智能）董韫美房建成（1965，53岁，北京航空航天，导航、制导与控制，姿态控制）干福熹  龚旗煌  顾瑛管晓宏(1955，63岁，四川泸州人，西安交大，电力系统，混沌系统，网络化系统)郭光灿郭雷（1961，57岁，中科院，随机自适应跟踪、极点配置与LQG控制）郝跃  何积丰  侯朝焕  侯洵怀进鹏（1962，56岁，山东人，北航，工业和信息化部，网络化软件技术与系统）黄宏嘉黄琳（1936，82岁，北大，统稳定性与控制理论，单输入系统极点配置定理，二次型最优控制）黄民强  黄如  黄维  简水生  姜杰  金亚秋  匡定波  雷啸霖李启虎（1939，79岁，中科院，水声信号处理和声纳）李树深  李未  李衍达  林惠民  刘国治  刘明  刘盛纲  刘颂豪  刘永坦  陆建华  陆汝钤  陆元九  吕建  毛军发梅宏（1963，55岁，贵州余庆人，北京理工，上海交大，计算机软件）彭堃墀  秦国刚  沈绪榜宋健（1931，67岁，工程院院长，最优控制系统理论）谭铁牛（1964，54岁，中科院自动化研究所，模式识别、图像处理和计算机视觉，人工智能学会）王家骐  王建宇   王立军  王启明王巍  王圩  王阳元  王永良  王育竹  王越  王占国  王之江吴朝晖（1966，52岁，浙大校长，复杂服务计算支撑平台，产生了重大经济与社会效益，脑机融合的混合智能）吴德馨吴宏鑫（1939，79岁，空间技术研究院，航天和工业领域的自适应控制和智能控制理论与应用）吴培亨  吴一戎  夏建白徐宗本（1955，63岁，西安交大副校长，智能信息处理、机器学习、数据建模）许宁生  薛永祺  杨德仁  杨芙清  杨学军  姚建铨  姚期智  尹浩张钹（1935，83岁，清华，控制理论，人工智能问题求解的商空间理论，解决不同粒度空间的问题描述、它们之间的相互转换以及复杂性分析）张景中张嗣瀛（1925，93岁，东北大学，控制理论，运动稳定性及最优控制）郑建华（1956，62岁，解放军保密委员会，复杂信息系统分析和相关基础理论，对该领域的序列论、函数论、算法设计与分析）郑耀宗  郑有炓  郑志明  周炳琨  周巢尘  周兴铭  周志鑫  朱中梁技术科学部（137人）蔡其巩   曹楚南   曹春晓   常青   陈维江   陈云敏   陈祖煜   成会明   程耿东程时杰（1954，64岁，华中科技大学，电力系统自适应控制，稳定性控制）丁汉  都有为   段文晖   范守善   方岱宁   高德利   高镇同   葛昌纯   顾秉林   顾诵芬   顾逸东   郭烈锦   郭万林   过增元   韩杰才   韩祯祥   何满潮   何雅玲胡海岩（1956，62岁，北京理工，振动控制系统的非线性动力学建模、稳定性与分岔分析）胡文瑞   胡聿贤   黄克智   姜中宏   金红光   金展鹏   赖远明   李济生   李述汤   李依依   李应红   林皋   刘宝镛   刘昌胜   刘广均   刘维民   刘竹生   柳百新   卢柯卢强（1936，82岁，清华，电力系统最优控制理论）路甬祥   雒建斌   闵桂荣   南策文   倪晋仁欧阳明高（1958，60岁，清华大学，湖北天门人，节能与新能源汽车动力系统，发动机电控高压柴油喷射）欧阳予   潘际銮   彭一刚   齐康   邱大洪   邱勇   任露泉   芮筱亭   申长雨   沈保根   沈志云   宋家树   宋振骐   孙家栋   孙钧唐   叔贤   陶文铨   滕锦光   田永君   汪耕   汪卫华   王补宣   王崇愚   王大中   王淀佐   王光谦   王立鼎   王希季   王锡凡   王曦   王自强   魏炳波   魏悦广   温诗铸   闻邦椿   吴承康   吴良镛   吴硕贤   伍小平   邢球痕   熊有伦   徐建中   徐性初   宣益民   薛其坤   闫楚良   严陆光   杨孟飞   杨叔子   杨伟   杨卫   杨槱   姚熹  叶恒强  叶培建   于起峰   余梦伦   俞大鹏   俞鸿儒   翟婉明   张楚汉   张清杰   张统一   张兴钤   张佑启   张泽赵   淳生   郑平   郑时龄   郑哲敏   钟万勰   周国治   周孝信   周远   朱荻   朱静   朱森元   朱位秋   祝世宁   庄逢辰   邹世昌   邹志刚©周东华和浙大孙优贤、褚健关系浙大培养的博士后中有四人（褚健、李平、陈思平、周东华）先后获“中国优秀博士后奖”"}
{"content2":"国内外研究主页集合：计算机视觉-机器学习-模式识别来自 http://cvnote.info/pages-collection-by-carson2005/国际大牛Adobe研究院 Jianchao Yang研究员 [进入主页]  (稀疏表示，超分辨率、图片检索、去噪、去模糊)CMU Srinivasa Narasimhan副教授 [进入主页]CMU Henry Schneiderman博士 [进入主页]  (目标检测和识别)CMU 田渊栋博士 [进入主页]CMU Alyosha Efros教授 [进入主页]  (图像纹理合成)CMU Yang Wang研究员 [进入主页]CMU Martial Hebert教授 [进入主页]CMU Daniel Huber研究员 [进入主页]CMU Robert T. Collins研究员 [进入主页]CMU Takeo Kanade教授 [进入主页]GE研究院 李关研究员 [进入主页]Google Thomas Deselaers工程师 [进入主页]  (图像检索)Leeds大学 Mark Everingham研究员 [进入主页]MIT 林达华博士 [进入主页]MIT 周博磊博士 [进入主页]  (聚集分析、运动检测)MIT William Freeman教授 [进入主页]  (图像纹理合成)MIT Douglas Lanman博士后 [进入主页]MIT Chris Stauffer博士 [进入主页]MIT Antonio Torralba助理教授 [进入主页]Rutgers大学 刘青山助理教授 [进入主页]Stanford大学 Andrew Ng教授 [进入主页]  (深度神经网络，深度学习)Stanford大学 Sebastian Thrun教授 [进入主页]  (机器人)Stanford大学 崔靖宇博士 [进入主页]UC.Berkeley Jitendra Malik教授 [进入主页]  (轮廓检测、图像/视频分割、图形匹配、目标识别)UC.Berkeley Michael I.Jordan教授 [进入主页]  (机器学习)UC.Irvine Deva Ramanan助理教授 [进入主页]  (目标检测，行人检测，跟踪、稀疏表示)UCLA 朱松纯教授 [进入主页]UCSC Peyman Milanfar教授 [进入主页]  (去噪)UCSD Jacob Whitehill博士 [进入主页]  (机器学习)UCSD David Kriegman教授 [进入主页]  (人脸识别)UCSD Serge J.Belongie教授 [进入主页]UIUC Svetlana Lazebnik教授 [进入主页]  (特征提取，聚类，图像检索)UIUC D.A.Forsyth教授 [进入主页]  (三维重建)West Virginia大学 Xin Li助理教授 [进入主页]  (边缘检测、降噪、去模糊)三菱电子研究院 Fatih Porikli 研究员 [进入主页]以色列威茨曼大学 Daniel Glasner博士后 [进入主页]  (超分辨率、分割、姿态估计)以色列威茨曼大学 Anat Levin教授 [进入主页]  (去噪、去模糊)以色列希伯来大学 Daniel Zoran博士 [进入主页]  (超分辨率、去噪)以色列希伯来大学 Yair Weiss教授 [进入主页]  (机器学习、超分辨率)以色列技术学院 Michael Elad教授 [进入主页]伦敦大学玛丽女王学院 Andrea Cavallaro教授 [进入主页]伦斯勒理工学院 Qiang Ji教授 [进入主页]佐治亚理工学院 James M.Rehg教授 [进入主页]  (分割、行人检测、特征描述)佐治亚理工学院 Monson H.Hayes教授 [进入主页]佐治亚理工学院 Irfan Essa教授 [进入主页]  (人脸表情识别)佐治亚理工学院 Monson Hayes教授 [进入主页]俄勒冈州立大学 Rob Hess博士 [进入主页]北卡莱罗纳大学教堂山分校 Marc Pollefeys教授 [进入主页]  (基于视频的3D模型生成、相机标定、运动检测与分析、3D重建)华盛顿大学 Iva Kemelmacher博士后 [进入主页]南加州大学 Paul Debevec教授 [进入主页]  (或 http://www.pauldebevec.com/ 将CV和CG结合研究 人脸捕捉重建技术)哥伦比亚大学 Sheer K.Nayar教授 [进入主页]多伦多大学 Allan.Jepson教授 [进入主页]多伦多大学 David J.Fleet教授 [进入主页]多伦多大学 Geoffrey E.Hinton教授 [进入主页]  (深度学习)多伦多大学 Kyros Kutulakos教授 [进入主页]威斯康星大学麦迪逊分校 Charles R.Dyer教授 [进入主页]宾夕法尼亚大学 石建波助理教授 [进入主页]密歇根州立大学 Anil K. Jain教授 [进入主页]密西根大学 Honglak Lee助理教授 [进入主页]  (机器学习、特征提取，去噪、稀疏表示)布朗大学 Michael J.Black教授 [进入主页]  (人的姿态估计和跟踪)布朗大学 Pedro Felzenszwalb教授 [进入主页]  (特征提取，Deformable Part Model)布朗大学 Benjamin Kimia教授 [进入主页]康奈尔大学 Ramin Zabih教授 [进入主页]康奈尔大学 Daniel Huttenlocher教授 [进入主页]微软研究院 Ce Liu研究员 [进入主页]  (去噪、超分辨率、去模糊、分割)微软研究院 Paul Viola研究员 [进入主页]  (AdaBoost算法)微软研究院 Antonio Criminisi研究员 [进入主页]  (图像修补，三维重建，目标检测与跟踪)微软研究院 Richard Szeliski研究员 [进入主页]微软研究院 张正友研究员 [进入主页]微软Redmond研究院 Piotr Dollar研究员 [进入主页]  (行人检测、特征提取)微软Redmond研究院 Simon Baker研究员 [进入主页]微软亚洲研究院 马毅研究员 [进入主页]微软亚洲研究院 孙剑研究员 [进入主页]微软剑桥研究院 John Winn研究员 [进入主页]德克萨斯州大学奥斯汀分校 Al Bovik教授 [进入主页]  (图像视频质量判别、特征提取)德克萨斯州大学奥斯汀分校 Kristen Grauman 助理教授 [进入主页]  (图像分解，检索)普林斯顿大学 李凯教授 [进入主页]普林斯顿大学 贾登博士 [进入主页]曼彻斯特大学 Tim Cootes教授 [进入主页]杜克大学 Carlo Tomasi教授 [进入主页]比利时天主教鲁汶大学 Radu Timofte博士 [进入主页]  (交通标志检测，定位，3D跟踪)比利时鲁汶大学 Luc Van Gool教授 [进入主页]  (http://www.vision.ee.ethz.ch/~vangool/)法国学习算法与系统实验室 Basilio Noris博士 [进入主页]  (http://mldemos.epfl.ch/)澳大利亚国立大学 Richard Hartley教授 [进入主页]  (运动估计、稀疏子空间、跟踪)爱丁堡大学 Chris William教授 [进入主页]爱丁堡大学 Bob Fisher教授 [进入主页]牛津大学 Andrew Zisserman教授 [进入主页]牛津大学 Ian Reid教授 [进入主页]  (跟踪和机器人导航)牛津布鲁克斯大学 Philip H.S.Torr教授 [进入主页]  (分割、三维重建)瑞士巴塞尔大学 Sami Romdhani助理研究员 [进入主页]瑞士巴塞尔大学 Thomas Vetter教授 [进入主页]瑞士洛桑理工学院 Pascal Fua教授 [进入主页]  (立体视觉，增强现实)瑞士联邦理工学院 Helmut Grabner博士后 [进入主页]田纳西大学 Li He博士 [进入主页]  (稀疏表示、超分辨率)纽约大学 Yann LeCun教授 [进入主页]  (http://yann.lecun.com/exdb/mnist/ 手写体数字识别)罗彻斯特大学 Jiebo Luo教授 [进入主页]芝加哥丰田技术研究所 Devi Parikh助理教授 [进入主页]英属哥伦比亚大学 David Lowe教授 [进入主页]诺丁汉大学 Michel Valstar博士 [进入主页]迪斯尼匹兹堡研究院 Iain Matthews研究员 [进入主页]  (http://www.ri.cmu.edu/person.html?type=publications&person_id=741 AAM,三维重建)香港中文大学 贾佳亚教授 [进入主页]香港中文大学 王晓刚助理教授 [进入主页]香港理工大学 张磊副教授 [进入主页]马德里理工大学 Marcos Nieto博士 [进入主页]马里兰大学 Rama Chellappa教授 [进入主页]马里兰大学 LARRY S.DAVIS教授 [进入主页]马里兰大学 Yasel Yacoob研究员 [进入主页]魏茨曼科学研究所 Michal Irani教授 [进入主页]  (超分辨率)国内大牛上海交通大学 刘允才教授 [进入主页]中山大学 郑伟诗助理教授 [进入主页]  (人脸识别、特征匹配、聚类、检索)中科院 樊彬助理教授 [进入主页]  (特征描述)中科院 田捷研究员 [进入主页]中科院自动化所 李子青研究员 [进入主页]中科院自动化所 肖柏华教授 [进入主页]  (文字识别、人脸识别、质量评判)中科院自动化所 刘成林研究员 [进入主页]中科院计算所 常虹副研究员 [进入主页]  (图像检索、半监督学习、超分辨率)中科院计算所 山世光研究员 [进入主页]北京大学 高文教授 [进入主页]北航助理 黄迪教授 [进入主页]  (3D人脸识别)南京信息工程大学 刘青山教授 [进入主页]  (人脸图像分析、医学图像分析)南京大学 吴建鑫教授 [进入主页]南京大学 周志华教授 [进入主页]深圳大学 于仕祺副教授 [进入主页]清华大学 程明明博士 [进入主页]  (图像分割、检索)清华大学 冯建江助理教授 [进入主页]  (指纹识别)清华大学 艾海舟教授 [进入主页]清华大学 章毓晋教授 [进入主页]百度 张栋博士 [进入主页]百度深度学习研究院 余轶南博士 [进入主页]  (目标检测，图像检索)百度深度学习研究院 余凯博士 [进入主页]  (深度学习，稀疏表示，图像分类)西安电子科技大学 高新波教授 [进入主页]  (质量评判、水印、稀疏表示、超分辨率)国际组织CMU ILIM实验室 [进入主页]CVonline [进入主页]Computer Vision Resource [进入主页]Deep Learning主页 [进入主页]  (深度学习论文、软件，代码，demo，数据等)Elefant [进入主页]  (机器学习开源库)Google Research [进入主页]Stanford大学vision实验室 [进入主页]UCSD Computer Vision [进入主页]香港中文大学多媒体实验室 [进入主页]Computer Vision Center [进入主页]Computer Vision Research Groups [进入主页]Computer Vision Software [进入主页]人脸识别主页 [进入主页]以色列魏茨曼科技大学Ronen Basri [进入主页]加州大学伯克利分校CV小组 [进入主页]加州理工行人检测相关资料 [进入主页]南加州大学CV实验室 [进入主页]卡内基梅隆大学CV主页 [进入主页]图像视频质量评判 [进入主页]图片检索国际竞赛PASCAL VOC(微软剑桥研究院组织) [进入主页]顶级民用机器人研究小组Porf.Gary领导的Willow Garage [进入主页]威兹曼科技大学超分辨率 [进入主页]密西根州立大学生物识别研究组 [进入主页]  (人脸识别、指纹识别、图像检索)康奈尔大学视觉与图像分析组 [进入主页]  (医学图像处理)微软亚洲研究院计算机视觉研究组 [进入主页]微软剑桥研究院ML与CV研究组 [进入主页]柏林科技大学计算机视觉与遥感实验室 [进入主页]  (图像分析、物体重建、基于图像的表面测量、医学图像处理)瑞士ETH-Zurich大学CV实验室 [进入主页]美国伊利诺伊州立大学Thomas S. Huang [进入主页]美国内达华大学里诺校区CV实验室 [进入主页]美国坦桑尼亚州立大学稀疏学习软件包 [进入主页]  (稀疏学习)美国宾利州立大学感知、运动与认识实验室 [进入主页]美国宾夕法尼亚大学GRASP实验室 [进入主页]美国密西根大学vision实验室 [进入主页]美国普渡大学机器人视觉实验室 [进入主页]英国布里斯托大学数字多媒体研究组 [进入主页]  (运动检测与跟踪、视频压缩、3D重建、字符定位)英国格拉斯哥大学信息检索小组 [进入主页]  (文本、图像、视频检索)英国萨利大学视觉、语音与信号处理中心 [进入主页]  (人脸识别、监控、3D、视频检索)荷兰乌德勒支大学图像与多媒体研究中心 [进入主页]  (图像、多媒体检索与匹配)行人检测主页 [进入主页]西班牙格拉纳达大学超分辨率重建项目组 [进入主页]计算机视觉分类信息导航 [进入主页]计算机视觉论文分类导航 [进入主页]运动检测、阴影、跟踪的测试视频下载 [进入主页]韩国启明大学计算机视觉与模式识别实验室 [进入主页]University of Massachusetts(麻省大学) 视觉实验室 [进入主页]大规模图像分类、检测竞赛ILSVRC（Stanford Google举办） [进入主页]国内组织OpenCV中文网站 [进入主页]上海交通大学图像处理与模式识别研究所 [进入主页]东软基于CV的汽车辅助驾驶系统 [进入主页]中国人工智能网 [进入主页]中国视觉网 [进入主页]中科院生物识别与安全技术研究中心 [进入主页]中科院自动化所 [进入主页]中科院自动化所医学影像研究室 [进入主页]中科院自动化所孙哲南助理教书 [进入主页]  (虹膜识别、掌纹识别、人脸识别)武汉大学数字摄影测量与计算机视觉研究中心 [进入主页]浙江大学图像技术研究与应用（ITRA）团队 [进入主页]百度技术副总监于凯 [进入主页]  (深度学习，稀疏表示，图像分类)西安交通大学人工智能与机器人研究所 [进入主页]清华大学电子工程系智能图文信息处理实验室 [进入主页]计算机视觉最新资讯网 [进入主页]视觉计算研究论坛 [进入主页]  (中科院视觉计算研究小组的论坛)数据堂-图像处理相关的样本数据 [进入主页]"}
{"content2":"课时1 计算机视觉历史回顾与介绍上CS231n：这一一门关于计算机视觉的课程，基于一种专用的模型架构，叫做神经网络（更细一点说，是卷积神经网络CNN）。计算机视觉是人工智能领域中发展最为迅猛的一个分支，是一个与很多领域紧密关联的学科。视觉信息，或者叫像素信息是最难被利用的信息，可以把它称之为“互联网中的暗物质”。我们现在面对的问题：非常大量的数据，以及这些“暗物质”的挑战。我们面对的问题，我们使用的建模（方式）也是跨学科的如果兴趣范围是关于计算机视觉的一些广泛的讨论，比如工具之类或者一些基础讨论关于3D视觉，或者是机器人视觉识别，可以学CS231a；CS231n针对的是更专的领域，在模型和应用范围都有针对性，模型方面，只讨论神经网络，应用方面，基本只针对于计算机视觉。卷积神经网络结构是为了解决识别问题的需要而出现的，之后，识别问题又促进了深度学习的进化，如此反复。基础视觉区的神经元是按一列一列的组织起来，每一列神经元只“喜欢”某一种特定的形状，某种简单的线条组合。"}
{"content2":"http://www.csdn.net/article/2015-03-24/2824301【编者按】本文来自CMU的博士，MIT的博士后，vision.ai的联合创始人Tomasz Malisiewicz的个人博客文章，阅读本文，你可以更好的理解计算机视觉是怎么一回事，同时对机器学习是如何随着时间缓慢发展的也有个直观的认识。以下为正文：本文我们来关注下三个非常相关的概念（深度学习、机器学习和模式识别），以及他们与2015年最热门的科技主题（机器人和人工智能）的联系。环绕四周，你会发现不缺乏一些初创的高科技公司招聘机器学习专家的岗位。而其中只有一小部分需要深度学习专家。我敢打赌，大多数初创公司都可以从最基本的数据分析中获益。那如何才能发现未来的数据科学家？你需要学习他们的思考方式。三个与“学习”高度相关的流行词汇模式识别（Pattern recognition）、机器学习（machine learning）和深度学习（deep learning）代表三种不同的思想流派。模式识别是最古老的（作为一个术语而言，可以说是很过时的）。机器学习是最基础的（当下初创公司和研究实验室的热点领域之一）。而深度学习是非常崭新和有影响力的前沿领域，我们甚至不会去思考后深度学习时代。我们可以看下图所示的谷歌趋势图。可以看到：1）机器学习就像是一个真正的冠军一样持续昂首而上；2）模式识别一开始主要是作为机器学习的代名词；3）模式识别正在慢慢没落和消亡；4）深度学习是个崭新的和快速攀升的领域。2004年至今三个概念的谷歌搜索指数（图来源于 谷歌趋势 ）1. 模式识别：智能程序的诞生模式识别是70年代和80年代非常流行的一个术语。它强调的是如何让一个计算机程序去做一些看起来很“智能”的事情，例如识别“3”这个数字。而且在融入了很多的智慧和直觉后，人们也的确构建了这样的一个程序。例如，区分“3”和“B”或者“3”和“8”。早在以前，大家也不会去关心你是怎么实现的，只要这个机器不是由人躲在盒子里面伪装的就好（）。不过，如果你的算法对图像应用了一些像滤波器、边缘检测和形态学处理等等高大上的技术后，模式识别社区肯定就会对它感兴趣。光学字符识别就是从这个社区诞生的。因此，把模式识别称为70年代，80年代和90年代初的“智能”信号处理是合适的。决策树、启发式和二次判别分析等全部诞生于这个时代。而且，在这个时代，模式识别也成为了计算机科学领域的小伙伴搞的东西，而不是电子工程。从这个时代诞生的模式识别领域最著名的书之一是由Duda & Hart执笔的“模式识别（Pattern Classification）”。对基础的研究者来说，仍然是一本不错的入门教材。不过对于里面的一些词汇就不要太纠结了，因为这本书已经有一定的年代了，词汇会有点过时。 一个字符“3”的图像被划分为16个子块。自定义规则、自定义决策，以及自定义“智能”程序在这个任务上，曾经都风靡一时（更多信息，可以查看这个 OCR 网页）小测试：计算机视觉领域最著名的会议叫CVPR，这个PR就是模式识别。你能猜出第一届CVPR会议是哪年召开的吗？2. 机器学习：从样本中学习的智能程序在90年代初，人们开始意识到一种可以更有效地构建模式识别算法的方法，那就是用数据（可以通过廉价劳动力采集获得）去替换专家（具有很多图像方面知识的人）。因此，我们搜集大量的人脸和非人脸图像，再选择一个算法，然后冲着咖啡、晒着太阳，等着计算机完成对这些图像的学习。这就是机器学习的思想。“机器学习”强调的是，在给计算机程序（或者机器）输入一些数据后，它必须做一些事情，那就是学习这些数据，而这个学习的步骤是明确的。相信我，就算计算机完成学习要耗上一天的时间，也会比你邀请你的研究伙伴来到你家然后专门手工得为这个任务设计一些分类规则要好。 典型的机器学习流程（图来源于 Natalia Konstantinova 博士的博客）。在21世纪中期，机器学习成为了计算机科学领域一个重要的研究课题，计算机科学家们开始将这些想法应用到更大范围的问题上，不再限于识别字符、识别猫和狗或者识别图像中的某个目标等等这些问题。研究人员开始将机器学习应用到机器人（强化学习，操控，行动规划，抓取）、基因数据的分析和金融市场的预测中。另外，机器学习与图论的联姻也成就了一个新的课题---图模型。每一个机器人专家都“无奈地”成为了机器学习专家，同时，机器学习也迅速成为了众人渴望的必备技能之一。然而，“机器学习”这个概念对底层算法只字未提。我们已经看到凸优化、核方法、支持向量机和Boosting算法等都有各自辉煌的时期。再加上一些人工设计的特征，那在机器学习领域，我们就有了很多的方法，很多不同的思想流派，然而，对于一个新人来说，对特征和算法的选择依然一头雾水，没有清晰的指导原则。但，值得庆幸的是，这一切即将改变……延伸阅读：要了解更多关于计算机视觉特征的知识，可以看看原作者之前的博客文章：“ 从特征描述子到深度学习：计算机视觉的20年 ”。3. 深度学习：一统江湖的架构快进到今天，我们看到的是一个夺人眼球的技术---深度学习。而在深度学习的模型中，受宠爱最多的就是被用在大规模图像识别任务中的卷积神经网络（Convolutional Neural Nets，CNN），简称ConvNets。 ConvNet框架（图来源于 Torch的教程 ）深度学习强调的是你使用的模型（例如深度卷积多层神经网络），模型中的参数通过从数据中学习获得。然而，深度学习也带来了一些其他需要考虑的问题。因为你面对的是一个高维的模型（即庞大的网络），所以你需要大量的数据（大数据）和强大的运算能力（图形处理器，GPU）才能优化这个模型。卷积被广泛用于深度学习（尤其是计算机视觉应用中），而且它的架构往往都是非浅层的。如果你要学习Deep Learning，那就得先复习下一些线性代数的基本知识，当然了，也得有编程基础。我强烈推荐Andrej Karpathy的博文：“ 神经网络的黑客指南 ”。另外，作为学习的开端，可以选择一个不用卷积操作的应用问题，然后自己实现基于CPU的反向传播算法。对于深度学习，还存在很多没有解决的问题。既没有完整的关于深度学习有效性的理论，也没有任何一本能超越机器学习实战经验的指南或者书。另外，深度学习不是万能的，它有足够的理由能日益流行，但始终无法接管整个世界。不过，只要你不断增加你的机器学习技能，你的饭碗无忧。但也不要对深度框架过于崇拜，不要害怕对这些框架进行裁剪和调整，以得到和你的学习算法能协同工作的软件框架。未来的Linux内核也许会在Caffe（一个非常流行的深度学习框架）上运行，然而，伟大的产品总是需要伟大的愿景、领域的专业知识、市场的开发，和最重要的：人类的创造力。其他相关术语1）大数据（Big-data）：大数据是个丰富的概念，例如包含大量数据的存储，数据中隐含信息的挖掘等。对企业经营来说，大数据往往可以给出一些决策的建议。对机器学习算法而言，它与大数据的结合在早几年已经出现。研究人员甚至任何一个日常开发人员都可以接触到云计算、GPU、DevOps和PaaS等等这些服务。2）人工智能（Artificial Intelligence）：人工智能应该是一个最老的术语了，同时也是最含糊的。它在过去50年里经历了几度兴衰。当你遇到一个说自己是做人工智能的人，你可以有两种选择：要么摆个嘲笑的表情，要么抽出一张纸，记录下他所说的一切。延伸阅读：原作者2011的博客：“ 计算机视觉当属人工智能 ”。结论关于机器学习的讨论在此停留（不要单纯的认为它是深度学习、机器学习或者模式识别中的一个，这三者只是强调的东西有所不同），然而，研究会继续，探索会继续。我们会继续构建更智能的软件，我们的算法也将继续学习，但我们只会开始探索那些能真正一统江湖的框架。如果你也对深度学习的实时视觉应用感兴趣，特别是那些适合机器人和家居智能化的应用，欢迎来我们的网站 vision.ai 交流。希望未来，我能说的再多一点……作者简介：Tomasz Malisiewicz，CMU的博士，MIT的博士后，vision.ai的联合创始人。关注计算机视觉，在这个领域也做了大量的工作。另外，他的博客也富含信息量和价值，感兴趣的可以浏览他个人主页和博客。原文链接： Deep Learning vs Machine Learning vs Pattern Recognition（译者/邹晓艺，CSDN 博客专家，关注机器学习、计算机视觉、人机交互和人工智能等领域  责编/钱曙光）"}
{"content2":"计算机视觉目标检测算法综述版权声明：转载请注明出处 https://blog.csdn.net/qq_16525279/article/details/81698684传统目标检测三步走：区域选择、特征提取、分类回归遇到的问题：1.区域选择的策略效果差、时间复杂度高2.手工提取的特征鲁棒性较差深度学习时代目标检测算法的发展：Two-Stage：R-CNN论文地址：Rich feature hierarchies for accurate object detection and semantic segmentation地位：是用卷积神经网络（CNN）做目标检测的第一篇，意义影响深远。核心思想：1.区域选择不再使用滑窗，而是采用启发式候选区域生成算法（Selective Search）2.特征提取也从手工变成利用CNN自动提取特征，增强了鲁棒性。流程步骤：1.使用Selective Search算法从待检测图像中提取2000个左右的区域候选框2.把所有侯选框缩放成固定大小（原文采用227×227）3.使用CNN（有5个卷积层和2个全连接层）提取候选区域图像的特征，得到固定长度的特征向量4.将特征向量输入到SVM分类器，判别输入类别；送入到全连接网络以回归的方式精修候选框优点：1.速度传统的区域选择使用滑窗，每滑一个窗口检测一次，相邻窗口信息重叠高，检测速度慢。R-CNN 使用一个启发式方法（Selective Search），先生成候选区域再检测，降低信息冗余程度，从而提高检测速度。2.特征提取传统的手工提取特征鲁棒性差，限于如颜色、纹理等低层次（Low level）的特征。不足：1.算力冗余先生成候选区域，再对区域进行卷积，这里有两个问题：其一是候选区域会有一定程度的重叠，对相同区域进行重复卷积；其二是每个区域进行新的卷积需要新的存储空间。2.图片缩放候选区域中的图像输入CNN（卷积层并不要求输入图像的尺寸固定，只有第一个全连接层需要确定输入维度，因为它和前一层之间的权重矩阵是固定大小的，其他的全连接层也不要求图像的尺寸固定）中需要固定尺寸（227 * 227），会造成物体形变，导致检测性能下降。3.训练测试不简洁候选区域生成、特征提取、分类、回归都是分开操作，中间数据还需要单独保存。SPP Net论文地址：Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition优点：1.将提取候选框特征向量的操作转移到卷积后的特征图上进行，将R-CNN中的多次卷积变为一次卷积，大大降低了计算量，不仅减少存储量而且加快了训练速度。2.在最后一个卷积层和第一个全连接层之间做一些处理，引入了Spatial Pyramid pooling层，对卷积特征图像进行空间金字塔采样获得固定长度的输出，可对特征层任意长宽比和尺度区域进行特征提取。Spatial Pyramid pooling具体做法：在得到卷积特征图之后，对卷积特征图进行三种尺度的切分：4*4，2*2，1*1，对于切分出来的每个小块进行max-pooling下采样，之后再将下采样的结果全排列成一个列向量，送入全连接层。Spatial Pyramid pooling操作示意图：例如每个候选区域在最后的512张卷积特征图中得到了512个该区域的卷积特征图，通过spp-net下采样后得到了一个512×（4×4+2×2+1×1）维的特征向量，这样就将大小不一的候选区的特征向量统一到了一个维度。总结：不仅减少了计算冗余，更重要的是打破了固定尺寸输入这一束缚。Fast R-CNN论文地址：Fast R-CNN结构创新：将原来的串行结构改成并行结构。网络创新：加入RoI pooling layer，它将不同大小候选框的卷积特征图统一采样成固定大小的特征。ROI池化层的做法和SPP层类似，但只使用一个尺度进行网格划分和池化。Fast R-CNN针对R-CNN和SPPNet在训练时是多阶段的和训练的过程中很耗费时间空间的问题进行改进。设计了多任务损失函数(multi-task loss)，将分类任务和边框回归统一到了一个框架之内。Faster R-CNN论文地址：Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks在Faster R-CNN之前，生成候选区域都是用的一系列启发式算法（Selective Search），基于Low Level特征生成区域。存在的问题：1.生成区域的靠谱程度随缘，而Two Stage算法正是依靠生成区域的靠谱程度——生成大量无效区域则会造成算力的浪费、少生成区域则会漏检；2.生成候选区域的算法（Selective Search）是在 CPU 上运行的，而训练在GPU上面，跨结构交互必定会有损效率。革新：提出Region Proposal Network（RPN）网络替代Selective Search算法，利用神经网络自己学习去生成候选区域。这种生成方法同时解决了上述的两个问题，神经网络可以学到更加高层、语义、抽象的特征，生成的候选区域的可靠程度大大提高。使得整个目标识别真正实现了端到端的计算，将所有的任务都统一在了深度学习的框架之下，所有计算都在GPU内进行，使得计算的速度和精度都有了大幅度提升。从上图看出RPN和RoI pooling共用前面的卷积神经网络——将RPN嵌入原有网络，原有网络和RPN一起预测，大大地减少了参数量和预测时间。Faster R-CNN在做下采样和RoI Pooling时都对特征图大小做了取整操作。整体思路：首先对整张图片进行卷积计算，得到卷积特征，然后利用RPN进行候选框选择，再返回卷积特征图取出候选框内的卷积特征利用ROI提取特征向量最终送入全连接层进行精确定位和分类，总之：RPN+Fast R-CNN=Faster R-CNN。RPN网络在RPN中引入了anchor的概念，feature map中每个滑窗位置都会生成 k 个anchor，然后判断anchor覆盖的图像是前景还是背景，同时回归Bounding box（Bbox）的精细位置，预测的Bbox更加精确。解释：为了提取候选框，作者使用了一个小的神经网络也即就是一个n×n的卷积核(文中采用了3×3的网络)，在经过一系列卷积计算的特征图上进行滑移，进行卷积计算。每一个滑窗计算之后得到一个低维向量（例如VGG net 最终有512张卷积特征图，每个滑窗进行卷积计算的时候可以得到512维的低维向量），得到的特征向量，送入两种层：一种是边框回归层进行定位，另一种是分类层判断该区域是前景还是背景。3*3滑窗对应的每个特征区域同时预测输入图像3种尺度（128,256,512），3种长宽比（1:1,1:2,2:1）的region proposal，这种映射的机制称为anchor。所以对于40*60图图，总共有约20000(40*60*9)个anchor，也就是预测20000个region proposal。总结：Faster R-CNN可以说是真正意义上的深度学习目标检测算法。Faster R-CNN将一直以来分离的region proposal和CNN分类融合到了一起，使用端到端的网络进行目标检测，无论在速度上还是精度上都得到了不错的提高。然而Faster R-CNN还是达不到实时的目标检测，预先获取region proposal，然后在对每个proposal分类计算量还是比较大。小结：一开始的串行到并行，从单一信息流到两条信息流。总的来说，从R-CNN, SPP NET, Fast R-CNN, Faster R-CNN一路走来，基于深度学习目标检测的流程变得越来越精简，精度越来越高，速度也越来越快。可以说基于region proposal的R-CNN系列目标检测方法是当前目标最主要的一个分支。One-Stage：尽管Faster R-CNN在计算速度方面已经取得了很大进展，但是仍然无法满足实时检测的要求，因此有人提出了基于回归的方法直接从图片中回归出目标物体的位置以及种类。具有代表性的两种方法是YOLO和SSD。YOLO论文地址：You Only Look Once: Unified, Real-Time Object Detection区别于R-CNN系列为代表的两步检测算法，YOLO舍去了候选框提取分支（Proposal阶段），直接将特征提取、候选框回归和分类在同一个无分支的卷积网络中完成，使得网络结构变得简单，检测速度较Faster R-CNN也有近10倍的提升。这使得深度学习目标检测算法在当时的计算能力下开始能够满足实时检测任务的需求。网络结构：首先将图片resize到固定尺寸（448 * 448），然后通过一套卷积神经网络，最后接上全连接直接输出结果，这就他们整个网络的基本结构。更具体地做法，是将输入图片划分成一个S*S的网格，每个网格负责检测网格里面的物体是啥，并输出Bbox Info和置信度。这里的置信度指的是该网格内含有什么物体和预测这个物体的准确定。更具体的是如下定义：从这个定义得知，当框中没有物体的时候，整个置信度都会变为 0 。这个想法其实就是一个简单的分而治之想法，将图片卷积后提取的特征图分为S*S块，然后利用优秀的分类模型对每一块进行分类，将每个网格处理完使用NMS（非极大值抑制）的算法去除重叠的框，最后得到我们的结果。YOLO模型：图片描述：(1) 给个一个输入图像，首先将图像划分成7*7的网格。(2) 对于每个网格，我们都预测2个边框（包括每个边框是目标的置信度以及每个边框区域在多个类别上的概率）。(3)根据上一步可以预测出7*7*2个目标窗口，然后根据阈值去除可能性比较低的目标窗口，最后非极大值抑制去除冗余窗口即可。可以看到整个过程非常简单，不需要中间的region proposal在找目标，直接回归便完成了位置和类别的判定。SSD论文地址：SSD: Single Shot MultiBox DetectorYOLO 这样做的确非常快，但是问题就在于这个框有点大，就会变得粗糙——小物体就容易从这个大网中漏出去，因此对小物体的检测效果不好。所以 SSD 就在 YOLO 的主意上添加了 Faster R-CNN 的 Anchor 概念，并融合不同卷积层的特征做出预测。上图是SSD的一个框架图，首先SSD获取目标位置和类别的方法跟YOLO一样，都是使用回归，但是YOLO预测某个位置使用的是全图的特征，SSD预测某个位置使用的是这个位置周围的特征（感觉更合理一些）。假如某一层特征图大小是8*8，那么就使用3*3的滑窗提取每个位置的特征，然后这个特征回归得到目标的坐标信息和类别信息(图c)。不同于Faster R-CNN，这个anchor是在多个feature map上，这样可以利用多层的特征并且自然的达到多尺度（不同层的feature map 3*3滑窗感受野不同）。特点：1.基于多尺度特征图像的检测：在多个尺度的卷积特征图上进行预测，以检测不同大小的目标，一定程度上提升了小目标物体的检测精度。2.借鉴了Faster R-CNN中的Anchor思想，在不同尺度的特征图上采样候选区域，一定程度上提升了检测的召回率以及小目标的检测效果。还有一个重大的进步是结合了不同尺寸大小 Feature Maps 所提取的特征，然后进行预测。这个尝试就大大地提高了识别的精度，且高分辨率（尺寸大）的 Feature Map 中含有更多小物体的信息，也是因为这个原因 SSD 能够较好的识别小物体。总结：和 YOLO 最大的区别是，SSD 没有接 FC 减少了大量的参数量、提高了速度。小结：SSD和YOLO采用了回归方法进行目标检测使得目标检测速度大大加快，SSD引入Faster R-CNN的anchor机制使得目标定位和分类精度都较YOLO有了大幅度提高。基于回归方法的目标检测基本达到了实时的要求，是目标检测的另一个主要思路。"}
{"content2":"为什么不去读顶级会议上的论文？适应于机器学习、计算机视觉和人工智能?看了版上很多贴子，发现很多版友都在问“热门研究方向”、“最新方法”等。有同学建议国内某教授的教材、或者CNKI、或者某些SCI期刊。每当看到这种问题，我都有点纳闷，为什么不去读顶级会议上的论文？我无意否认以上文献的价值，但是在机器学习、计算机视觉和人工智能领域，顶级会议才是王道。国内教材和CNKI上的基本是N年前老掉牙的东西。有人会质疑这些会议都只是EI。是的，这的确非常特殊：在许多其它领域，会议都是盛会，比如society of neuroscience的会议，每次都有上万人参加，带个abstract和poster就可以去。但在所讨论的几个领域，顶级会议的重要性无论怎么强调都不为过。可以从以下几点说明：（1）因为机器学习、计算机视觉和人工智能领域发展非常迅速，新的工作层出不穷，如果把论文投到期刊上，一两年后刊出时就有点out了。因此大部分最新的工作都首先发表在顶级会议上，这些顶级会议完全能反映“热门研究方向”、“最新方法”。（2）很多经典工作大家可能引的是某顶级期刊上的论文，这是因为期刊论文表述得比较完整、实验充分。但实际上很多都是在顶级会议上首发。比如PLSA, Latent Dirichlet Allocation等。（3）如果注意这些领域大牛的pulications，不难发现他们很非常看重这些顶级会议，很多人是80%的会议+20%的期刊。即然大牛们把最新工作发在顶级会议上，有什么理由不去读顶级会议？(1)以下是不完整的列表，但基本覆盖。机器学习顶级会议：NIPS, ICML, UAI, AISTATS; （期刊：JMLR, ML, Trends in ML, IEEE T-NN）计算机视觉和图像识别：ICCV, CVPR, ECCV; （期刊：IEEE T-PAMI, IJCV, IEEE T-IP）人工智能：IJCAI, AAAI; （期刊AI）另外相关的还有SIGRAPH, KDD, ACL, SIGIR, WWW等。特别是，如果做机器学习，必须地，把近4年的NIPS, ICML翻几遍；如果做计算机视觉，要把近4年的ICCV, CVPR, NIPS, ICML翻几遍。(2)另外补充一下：大部分顶级会议的论文都能从网上免费下载到，比如CV方面：http://www.cvpapers.com/index.html; NIPS: http://books.nips.cc/; JMLR(期刊): http://jmlr.csail.mit.edu/papers/; COLT和ICML(每年度的官网): http://www.cs.mcgill.ca/~colt2009/proceedings.html。希望这些信息对大家有点帮助。(3)说些自己的感受。我的研究方向主要是统计学习和概率图模型，但对计算机视觉和计算神经科学都有涉及，对Data mining和IR也有些了解。这些领域，从方法和模型的角度看，统计模型（包括probabilistic graphical model和statistical learning theory）是主流也是非常有影响力的方法。有个非常明显的趋势：重要的方法和模型最先在NIPS或ICML出现，然后应用到CV,IR和MM。虽然具体问题和应用也很重要，但多关注和结合这些方法也很有意义。对于这个领域的牛人们，以上全是浅显的废话，完全可以无视。欢迎讨论。"}
{"content2":"三者之间既有区别，又有联系，不确切的描述：计算机图形学≈画图计算机视觉≈看图数字图像处理≈看图前沐浴更衣焚香做好各种仪式，然后再看图计算机图形学（Computer Graphics）讲的是图形，也就是图形的构造方式，是一种从无到有的概念，从数据得到图像。是给定关于景象结构、表面反射特性、光源配置及相机模型的信息，生成图像。计算机视觉（Computer Vision）是给定图象，从图象提取信息，包括景象的三维结构，运动检测，识别物体等。数字图像处理（Digital Image Processing）是对已有的图像进行变换、分析、重构，得到的仍是图像。模式识别（PR）本质就是分类，根据常识或样本或二者结合进行分类，可以对图像进行分类，从图像得到数据。Computer Graphics和Computer Vision是同一过程的两个方向。Computer Graphics将抽象的语义信息转化成图像，Computer Vision从图像中提取抽象的语义信息。Image Processing探索的是从一个图像或者一组图像之间的互相转化和关系，与语义信息无关。总之，计算机图形学是计算机视觉的逆问题，两者从最初相互独立的平行发展到最近的融合是一大趋势。图像模式的分类是计算机视觉中的一个重要问题，模式识别中的许多方法可以应用于计算机视觉中。区别：Computer Graphics，简称 CG 。输入的是对虚拟场景的描述，通常为多边形数组，而每个多边形由三个顶点组成，每个顶点包括三维坐标、贴图坐标、rgb颜色等。输出的是图像，即二维像素数组。Computer Vision，简称 CV。输入的是图像或图像序列，通常来自相机或usb摄像头。输出的是对于图像序列对应的真实世界的理解，比如检测人脸、识别车牌。Digital Image Processing，简称 DIP。输入的是图像，输出的也是图像。Photoshop中对一副图像应用滤镜就是典型的一种图像处理。常见操作有模糊、灰度化、增强对比度等。联系：CG 中也会用到 DIP，现今的三维游戏为了增加表现力都会叠加全屏的后期特效，原理就是 DIP，只是将计算量放在了显卡端。CV 更是大量依赖 DIP 来打杂活，比如对需要识别的照片进行预处理。最后还要提到近年来的热点——增强现实（AR），它既需要 CG，又需要 CV，当然也不会漏掉 DIP。它用 DIP 进行预处理，用 CV 进行跟踪物体的识别与姿态获取，用 CG 进行虚拟三维物体的叠加。简单点说：1 计算机视觉，里面人工智能的东西更多一些，不仅仅是图像处理的知识，还涵盖了人工智能，机器学习等领域知识；2，计算机图形学，主要涉及图形成像及游戏类开发，如opengl等，还有就是视频渲染等；3，图像处理，这个主要针对图像图像的基本处理，如图像检索或则图像识别，压缩，复原等等操作。计算机图形学和数字图像处理是比较老的技术。计算机视觉要迟几十年才提出。计算机图形学和数字图像处理的区别在于图形和图像。图形是矢量的、纯数字式的。图像常常由来自现实世界的信号产生，有时也包括图形。而图像和图形都是数据的简单堆积，图像是像素的叠加，图形则是基本图元的叠加。计算机视觉要从图像中整理出一些信息或统计数据，也就是说要对计算机图像作进一步的分析。计算机图形学的研究成果可以用于产生数字图像处理所需要的素材，计算机视觉需要以数字图像处理作为基础。计算机视觉与数字图像处理的这种关系类似于物理学和数学的关系。另外，如果不是浙江大学的或者中科院计算所的，不建议做计算机图形学这一方向，难度太大（图形比图像虽然表面上只高一维，但实际上工作量大了好多倍；其次，图像，国内外差距目前已经很小，好发重要期刊；图形，除上面两个单位和微软外，国内外差距很大，不好发重要期刊）数字图像处理主要是对已有的图像，比如说可见光的图像、红外图像、雷达成像进行噪声滤除、边缘检测、图像恢复等处理，就像用ps 处理照片一样的。人脸识别啊、指纹识别啊、运动物体跟踪啊，都属于图像处理。去噪有各种滤波算法；其他的有各种时频变化算法，如傅里叶变化，小波变换等，有很多这方面的书籍。图形学主要研究如何生成图形的，像用autoCAD作图，就是图形学中算法的应用。各种动漫软件中图形算法的生成等。"}
{"content2":"空间技术、原子能技术以及人工智能合称为20世纪的三大科学技术。形象思维：迅速做出决策而不要求十分精确时需要的是形象思维，同理对一个问题进行假设猜想也是应用到了形象思维。逻辑思维：也就是抽象思维。要求进行雅阁的论证时，或者是对一个问题的假设进行论证。顿悟思维：省略~~what is past isprologue！信息论的创始人：香农自然界的四大奥妙：智能的发生、物质的本质、宇宙的起源、生命的本质英国哲学家“培根”提出了归纳，同时“知识就是力量”就是培根的警句。机器感知：就是是机器具有类似于人的感知能力，其中以机器视觉与机器听觉为主。对应于听和视觉听:就是对于自然语言的理解视:也即是对应于模式识别感知：是人类获取外部信息的基本途径。机器思维：机器思维是指对通过感知得来的外部信息及机器内部的各种工作信息进行有目的的处理机器学习：知识是智能的的基础，要使计算机有智能，就必须使它有知识。主要研究如何使计算机具有类似于人的学习能力，使它能通过学习自动地获取知识。涉及脑科学、神经心理学、计算机视觉、计算机听觉等都有密切联系，依赖于这些学科的共同发展。机器视觉：是模式识别的一个重要方面。计算机视觉通分为低层视觉和高层视觉两类。低层视觉主要执行预处理功能，如边缘检测、移动目标检测、纹理分析，以及立体造型、曲面色彩等。主要目的是使得看见的对象更突出。这时还不是理解阶段。高层视觉住傲视理解对象，需要掌握与对象相关的知识。机器视觉的前言课题包括：实时图像的并行处理、实时图像的压缩、传输与复原，三维景物的建模识别，动态和时变视觉等。机器视觉系统是指通过图像摄取装置将摄取的目标转换成图像信号，传送给专用的图像处理系统，根据像素分布和宽度、颜色等信息，转换成数字信号，图像系统对这些信号进行各种运算，抽取目标的额特征，进而根据判别的结果来控制现场的设备动作。主要研究目标是使计算机具有通过二维图像认知三维环境信息的能力，能够感知与处理三维环境中物体的形状位置、姿态运动等几何信息。"}
{"content2":"计算机视觉在工业方面的应用分类：            计算机视觉随想杂谈2011-09-21 14:17188人阅读评论(0)收藏举报videoprintingimagesystemclassificationvisualizationhttp://www.cs.ubc.ca/~lowe/vision.html本人是计算机视觉方向的研究生，偶然发现这个网页，列举了一些计算机视觉在工业上的应用，觉得比较受用，拿来分享。原网址见上面连接。Automobile driver assistanceIteris (Santa Ana, California). Lane departure and collision warning systems for trucks and cars. Used in over 100,000 vehicles (2009). Also creates traffic monitoring systems.MobilEye (Jerusalem, Israel). Vision systems that warn automobile drivers of danger, provide adaptive cruise control, and give driver assistance.Eye and Head TrackingMirametrix (Montreal, Quebec). Free-head eye-tracker using computer vision technologies.Smart Eye (Göteborg, Sweden). Systems to track eye and gaze position. Applications include detection of drowsiness or inattention in drivers.SMI (Berlin, Germany). Eye and gaze tracking systems.Film and Video: Sports analysisHawkeye (Winchester, UK). Uses multiple cameras to provide precise tracking in tennis, cricket, and other sports for refereeing and commentary.LiberoVision (Zurich, Switzerland). Creates photorealistic 3D visualization of sporting events for sports broadcasting and analysis.QuesTec (Deer Park, New York). Systems for tracking sports action to provide enhanced broadcasts.Red Bee Media (London, UK). Develops Piero system for sports analysis and augmentation.Amisco (Nice, France). Systems for tracking sports players and the ball in real time, using some human assistance.Sportvision (New York, NY). Vision systems to provide real-time graphics augmentaion for sports broadcasts.Film and Video2d3 (Oxford, UK). Systems for tracking objects in video or film and solving for 3D motion to allow for precise augmentation with 3D computer graphics.Image Metrics (Manchester, England). A markerless tracking system for the human face that can be used to map detailed motion and facial expressions to synthetic characters.Imagineer Systems (Guildford, UK). Computer vision software for the film and video industries.MirriAd (London, UK). Uses computer vision methods to track consistent regions in video and insert virtual advertising.Mova (San Francisco, California). Provides 3D tracking of points on the human face or other surfaces for character annimation. Uses invisible phosphorescent makeup to provide a random texture for stereo matching.Orad (Kfar Saba, Israel). Systems for creating virtual television sets, sports analysis, and other applications of real-time augmented reality.Ooyala (Mountain View, California). Video content management and delivery, including object identification and tracking.VideoSurf (San Mateo, California) Uses computer vision to search and classify online video.Games and Gesture RecognitionGestureTek (Toronto, Canada). Tracks human gestures for playing grames or interacting with computers.Microsoft Kinect (Redmond, Washington). Provides motion sensing and gesture recogntion for the Xbox gaming system. Creates 3D depth maps and registed images in real time using projected infrared light patterns and a single camera. Sold over 8 million units in the first 60 days, which makes it the fastest selling consumer electronics device (Wikipedia).Reactrix (Redwood City, California). Interactive advertising for projected displays that tracks human gestures.Sony EyeToy uses computer vision to track the hand and body motions of players to control the Sony Playstation. Sales were over 10 million units by 2008.(Wikipedia)General purpose vision systemsCognex (Natick, Massachusetts) is one of the largest machine vision companies (800 employees, 2010). Develops systems for inspection and localization tasks, people counting, and many other areas.(Hoover's)Evolution Robotics (Pasadena, California). Vision systems for object recognition and navigation. Applications include mobile robotics and grocery retail. Develops integrated vision and navigation solutions for household robots.Matrox Imaging (Dorval, Quebec, Canada). Software and hardware for machine vision applications.National Instruments (Austin, Texas). Vision software and systems used for many applications, including inspection, biomedical, and security.Neptec (Ottawa, Canada). Laser-based 3D vision systems for use on the space shuttles and other applications.Newton Research Labs (Renton, Washington). Vision systems for high-speed tracking and mobile robots.Point Grey Research (Vancouver, Canada). Real-time stereo vision systems, spherical vision systems, and imaging hardware.Sarnoff (Princeton, New Jersey). Vision systems for tracking, registration, navigation, biometrics, and other applications.Seeing Machines (Canberra, Australia). Systems for tracking head position and eye gaze direction.Soliton (Bangalore, India). Smart cameras for industrial inspection and other applications.SpikeNet (Toulouse, France). Trainable vision systems for performing recognition.Supercomputing Systems (Zurich, Switzerland). Developed the leanXcam, a low-cost intelligent camera using open source software.TYZX (Menlo Park, California). Produces real-time stereo vision systems that use a custom chip for fast stereo matching.ViSSee (Lugano, Switzerland). Developing a low-cost real-time sensor for measuring speed using an approach modeled on vision in the fruit fly.VISIONx (Pointe-Claire, Quebec, Canada). Vision systems for high accuracy measurement and other applications.Vitronic (Wiesbaden, Germany). Vision systems for inspection, manufacturing, logistics, traffic management, and other applications.Industrial automation and inspection: Automotive industryCogniTens (Ramat-Hasharon, Israel). Has developed a system for accurate scanning of 3D objects for the automotive and other industries. The system uses a 4-camera head with projection of textured illumination to enable accurate stereo matching.Perceptron (Plymouth, Michigan). Creates 3D laser scanning systems for automotive and other applications.Industrial automation and inspection: Electronics industryKLA-Tencor (San Jose, California). Systems for inspection and process control in semiconductor manufacturing.Orbotech (Yavne, Israel). Automated inspection systems for printed circuit boards and flat panel displays.(Hoover's)Industrial automation and inspection: Food and agricultureMontrose Technologies (Ottawa, Canada). Vision systems for the baked goods industry. Systems monitor bake color, shape, and size of bread, cookies, tortillas, etc.Ellips (Eindhoven, The Netherlands). Vision systems for inspecting and grading fruits and vegetables.Industrial automation and inspection: Printing and textilesAdvanced Vision Technology (Hod Hasharon, Israel). Systems to inspect output from high-speed printing presses.Elbit Vision Systems Ltd. (Yoqneam, Israel). Vision systems for textile inspection and other applications.Mnemonics (Mt. Laurel, New Jersey). Vision systems for print quality inspection and other applications.Xiris Automation (Burlington, Ontario, Canada). Inspection for the printing and packaging industries.Industrial automation and inspection: OtherAdept (Pleasanton, California). Industrial robots with vision for part placement and inspection.Avalon Vision Solutions (Lithia Springs, Georgia). Vision systems for the plastics industry.Basler (Ahrensburg, Germany). Inspection systems for optical media, sealants, displays, and other industries.Hermary Opto Electronics (Coquitlam, BC, Canada). Develops 3D scanners for sawmills and other applications.JLI vision (Soborg, Denmark). Vision systems for industrial inspection tasks, including food processing, glassware, medical devices, and the steel industry.LMI Technologies (Vancouver, Canada). Develops 3D vision systems using laser sensors for inspection of wood products, roads, automotive manufacturing, and other areas.MVTec (Munich, Germany). Vision systems for inspection and other applications.NeuroCheck GmbH (Remseck, Germany). Inspection systems for quality control.PPT Vision (Eden Prairie, Minnesota). Vision systems for automotive, pharmaceutical, electronics, and other industries.Seegrid (Pittsburgh, PA). Industrial mobile robots that use vision for mapping and navigation.SIGHTech (San Jose, California). Trainable computer vision systems for inspection and automation.Virtek Vision International (Waterloo, Ontario, Canada). Laser-based inspection and templating systems.Wintriss Engineering (San Diego, California). Vision systems for suface inspection and sports vision applications.Medical and biomedicalClaron Technology (Toronto, Canada). Uses real-time stereo vision to detect and track the pose of markers for surgical applications.Noesis (St. Laurent, Quebec, Canada). Software for biomedical and other scientific image analysis.Object Recogntion for Mobile DevicesKooaba (Zurich, Switzerland). Visual search for smartphones, photo management, and other applications.SnapTell (Palo Alto, California). Image recognition and product search for camera phones. Owned by Amazon A9.Panoramic PhotographyCloudburst Research (Vancouver, Canada). Develops AutoStitch Panorama, which provides fully automated image stitching for the iPhone platform (disclosure: the author of this list is a founder of the company).HumanEyes (Jerusalem, Israel). Develops software for creating 3D stereo views, including stereo mosaicing.Kolor (Challes les eaux, France). Develops the Autopano Pro software for automated panorama stitching of digital images. Also provides high-dynamic-range imaging by combining multiple exposures.People trackingBrickstream (Atlanta, GA). Tracking people within stores for sales, marketing, and security.Reveal (Auckland, New Zealand). Systems for counting and tracking pedestrians using overhead cameras.VideoMining (State College, PA). Tracking people in stores to improve marketing and service.Safety monitoringMG International (Boulogne, France). The Poseidon System monitors swimming pools to warn of accidents and drowning victims.Security and BiometricsAimetis (Waterloo, Ontario, Canada). Systems for intelligent video surveillance.Aurora (Northampton, UK). Systems for biometric face recognition.AuthenTec (Melbourne, Florida). Fingerprint recognition systems with a novel sensor.Briefcam (Jerusalem, Israel). Develops software for summarization of long surveillance video in a short summary video.Cernium (Reston, Virginia). Systems for behavior recognition in real-time video surveillance.Digital Persona (Redwood City, California). Fingerprint recognition systems.EVITECH (Paris, France). Smart video surveillance systems.Equinox (New York, NY). Security systems using novel sensors, such as registered visible and thermal infrared images and use of polarized lighting.Genetec (Montreal, Canada). Security systems for license plate recognition, surveillance, and access control.Honeywell (Morristown, New Jersey). Range of video surveillance systems and analysis software.Imagemetry (Prague, Czech Republic). Image processing and computer vision for image forensics.IntelliVision (San Jose, California). Automated monitoring systems, including face and object recognition.L-1 Identity Solutions (Stamford, Connecticut). Fingerprint, iris, and face recognition systems as well as other security applications.(Hoover's)ObjectVideo (Reston, Virginia). Automated video surveillance products for tracking, classification, and activity recognition.Viion Systems (Montreal, Canada). Detection and identification of computer users.Vitamin D (Menlo Park, California). Detection and monitoring of people in video streams.Three-dimensional modelingCreative Dimension Software (Guildford, UK). Creates 3D models from a set of images. Objects are imaged on a calibration mat.Eos Systems (Vancouver, Canada). PhotoModeler software allows creation of texture-mapped 3-D models from a small number of photographs. Also provides accurate photogrammetric 3D measurements from multiple images of a scene.Creaform (Quebec City, Canada). Develops 3D scanners for the human body and other applications.Traffic and road managementImage Sensing Systems (St. Paul, Minnesota). Created the Autoscope system that uses roadside video cameras for real-time traffic management. Over 100,000 cameras are in use.Yotta (Leamington Spa, UK). Imaging and scanning solutions for road network surveying.Web ApplicationsFace.com (Tel Aviv, Israel). Image retrieval based on face recognition.Incogna (Ottawa, Canada). Develops a system for image search on the web. Uses GPUs for increased performance.LTU Technologies (Paris, France). Image retrieval based on content.Photometria (San Diego, California). Virtual makeover website,TAAZ.com uses computer vision methods to allow users to try on makeup, hair styles, sunglasses, and jewelery"}
{"content2":"(1) 国际会议通常，国际上计算机视觉方面的三大国际会议是ICCV， CVPR和ECCV，统称之为ICE。ICCV，International Comference on Computer Vision，国际计算机视觉会议，是公认的三个会议中级别最高的，收录率一般在20%左右，由IEEE主办。【收录论文的内容：底层视觉与感知，颜色、光照与纹理处理，分割与聚合，运动与跟踪，立体视觉与运动结构重构，基于图像的建模，基于物理的建模，视觉中的统计学习，视频监控，物体、事件和场景的识别，基于视觉的图形学，图片和视频的获取，性能评估，具体应用等。】CVPR，Internaltional Conference on Computer Vision and Pattern Recogintion，国际计算机机器视觉和模式识别会议。一年一次，举办地在美国(除2002年)，录取率25%左右，由IEEE主办。ECCV，Europeon Conference on Computer Vision，欧洲计算机视觉会议，两年一次 ，欧洲，收录率一般在20%多，由springer和一些商业媒体承办。【每次会议在全球范围录用论文300篇左右，中国大陆的论文数量一般在10-20篇之间】ICIP——International Conference on Image Processing.BMVC——British Machine Vision Conference.IAPR MVA——IAPR Machine Vision Applications.国际模式识别会议(ICPR)——International Conference on Pattern Recognition.亚洲计算机视觉会议(ACCV)——Asian Conference on Computer Vision.SIGGRAPH——Special Interest Group for Computer GRAPHICS，计算机图形图像特别兴趣小组，更偏重图形方面。EUROGRAPHICS——European Association for computer graphics，与SIGGRAPH对应，只不过仅在欧洲范围内召开。IJCAI——International Joint Conference on Artificial Intelligence.ICSLP—— International Conference on Spoken Language Processing.ICASSP——International Conference on Acoustics, Speech and Signal Processing.ICML——International Conference on Machine Learning，在ML领域比较高级别的一个国际会议。ICME——International Consulting Management & Engineering，比较水。......(2) 国际期刊以计算机视觉为主要内容之一的国际刊物也有很多，如:International Journal of Computer Vision, IJCV http://link.springer.com/journal/11263.IEEE Trans. On PAMI http://www.computer.org/portal/web/tpami/home.IEEE Transactionson Image Processing(TIP)http://www.signalprocessingsociety.org/publications/periodicals/image-processing/.Pattern Recognition(PR) http://www.elsevier.com/locate/issn/00313203.Pattern Recognition Letters(PR Letters) http://www.journals.elsevier.com/pattern-recognition-letters/.IEEE Trans. on Robotics and Automation.CVGIP——Computer Vision, Graphics and Image Processing.Visual Image Computing.IJPRAI (Internatiorial Journat of Pattern Recognition and Artificial Intelligence).Multimedia http://www.computer.org/portal/web/computingnow/multimedia.(3) 国内会议略。(4) 国内期刊自动化学报、计算机学报、软件学报、电子学报，中国图象图形学报，模式识别与人工智能，光电子激光，光学 精密工程等。顶尖期刊：PAMI:IEEE Trans. on Patt. Analysis and Machine IntelligenceIJCV: International Journal on Computer VisionCVIU: Computer Vision and Image UnderstandingPR:Pattern RecognitionImage and Vision ComputingPattern Recognition Letter原文地址：https://blog.csdn.net/Julialove102123/article/details/70766754"}
{"content2":"孙剑博士分享的是《深度学习变革视觉计算》，分别从视觉智能、计算机摄影学和AI计算三个方面去介绍。他首先回顾了深度学习发展历史，深度学习发展到今天并不容易，过程中遇到了两个主要障碍：第一，深度神经网络能否很好地被训练。在深度学习获得成功之前曾被很多人怀疑，相比传统的机器学习理论，深度学习神经网络的参数要比数据大10倍甚至上百倍；第二，当时的训练过程非常不稳定，论文即使给出了神经网络训练方法，其他研究者也很难把结果复现出来。这些障碍直到2012年才开始慢慢被解除。人工智能可以分为感知和认知两大部分，语音、NLP和CV是AI的三大支柱。今天的分享主要分为三个方面，这也是深度学习引入计算机视觉后，对我们的研究带来的三大变革：第一，视觉智能是回答了机器如何理解一张照片或者视频，这方面的研究发生了哪些变化？第二，计算机摄影学研究如何从输入图像生成另一幅我们期望的图像，这个领域发生了哪些变化？第三，今天的AI计算发生了哪些变化？视觉智能计算机视觉研究的几个问题：分类、检测、分割及将前三者用于视频序列的识别工作。计算机视觉尤其是语义理解核心是如何在计算机中表示一张照片，以至于可以操作它、理解它，用它做各种各样的应用。2000年后最好的方法是Feature-base，从一张图中抽取很多局部的特征，编码成一个非常长的向量。2010年深度学习后，神经网络给我们带来了更强大的视觉表示方法。深度神经网络有两个特征：首先，它是对一张图片做映射，映射到一个高维空间的向量上；它由非常长的非线性变换组成，进来的信号进行多次非线性变换，直到人们得到想要的图像表示。第二，这个非线性变换中的所有参数都是根据监督信号全自动学习的，不需要人工设计。神经网络在前面一些层学到了类似边缘、角点或纹理等初级模式，在后面一些层学到越来越多的语义模式例如物体或物体部分。整体学到了分层结构的表示。从2012年AlexNet，一个8 Layers的神经网络，后来有VGG, 一个19 Layers的神经网络，到了2015年，我们提出了152 Layers的神经网络。随着网络层数的增加与数据的增多，我们第一次在ImageNet数据集上让机器超越了人类。esNet的核心思想是加入跳层连接，不要学习直接的映射而是学习残差映射，这样非常有利于训练或优化。ResNet出来后，同行给了各种各样的解释。这是我比较相信的解释：而非ResNet很容易表示0映射，即输入信号和输出很接近0；而ResNet很容易表示Identity映射，即输入信号和输出很接近，直观的理解是当一个网络非常深时，相邻的变化越来越小。这种参数化的形式更利于学习，以至于我们神经网络的优化更容易。ShuffleNet有V1和V2版本，核心是提出了一套设计原理：比如让卷积更平衡；尽量不要产生分支；降低整体结构的碎片化，避免逐元素操作。神经网络设计的最新研究方面，目前很热的趋势叫AutoML或者NAS。这是一个很好的网站（automl.org），大家可以在这里看最新的文章。NAS的问题核心是解一个嵌套的权重训练问题和网络结构搜索问题。以上是今天的第一部分，说的是视觉智能，我们从Feature的功能化定义，到走向模型的设计，再走到现在的模型搜索。计算摄影学二部分，我想分享以前做了很多年的研究方向——计算摄影学。除了计算智能，计算机视觉中还有一个问题是给输入一个图像，输出是另一个图像。从输入质量比较差的图像（比如模糊、有噪声、光照不好）恢复更好的图像，这就是计算摄影学，也是目前研究很活跃的方向。计算摄影学以前是怎么做的？这篇（上图）是我们2009年的Dehaze去雾，引入黑通道先验并结合雾的物理产生过程来恢复没有雾的图像，效果非常好，并获得了CVPR 2009最佳论文。如何从一张模糊图像和噪声图像恢复成清晰的图像，这里用了很多传统的反卷积方法。这是另一问题，被称为图像抠图：左边是输入，右边是输出，目的是把前景精细分离出来。今天的深度学习的方法是抛弃了以前的做法，不需要做任何显式的假设，通过全卷积的Encoder-Decoder输出想要的图像。举个例子，关于Image Matting问题，今天的方法是：通过一个多任务的网络，可以直接输出Matting的结果，非常细的毛发都能提取出来。Matting不光可以做图像合成，它还可以用单摄像头就拍出像单反一样的效果。AI-ISP可以得到非常好的降噪效果和高质量的图像。这个方法获得了今年CVPR图像降噪的冠军，同时我们将这个方法应用在OPPO今年最新的旗舰手机OPPO Reno 10倍变焦版的夜摄超画质拍摄技术上。AI计算左边传统的冯诺伊曼计算架构，服务了我们很多年。但随着数据的日益增大，出现了“冯诺伊曼瓶颈”，指内存和计算单元之间搬运数据的瓶颈。右边是今天神经网络做训练、推理的方法，它突破了这个瓶颈。因为神经网络计算非常简单，基本上只包含向量和矩阵之间的操作，可以避免很多判断和分支，用大规模并行的计算方式消除瓶颈。旷视和北京智源人工智能研究院共同推出的Objects365，第一阶段开源超过1000万的标注框，这是目前世界上最大的检测数据集，不光是数据大，可以真正学到更好的Feature，这是第二方面。"}
{"content2":"计算机视觉和模式识别领域的SCI期刊计算机视觉和模式识别领域的代表就是四大顶会了：ICCV、ECCV、CVPR、NIPS，还有就是难度最高的PAMI了，这些都让人望而生畏。那么除了这些耳熟能详的期刊和会议之外，还有哪些该领域的期刊呢？下面我收集了一些该领域的代表性期刊，并介绍了他们的影响因子以及投稿难度和审稿周期。希望对大家有帮助吧，后期大家还有发现的可以留言，补充哦。首先介绍计算机视觉领域的4个顶级代表性期刊吧。(1) IEEE Transactions on Pattern Analysis and Machine Intelligence，IEEE模式分析与机器智能汇刊，简称PAMI，是IEEE最重要的学术性汇刊之一。在各种统计中，PAMI被认为有着很强的影响因子（2011年影响因子为4.9）和很高的排名。显然，这个期刊的中稿难度那是相当的大，一般先投中CVPR之后再加点东西投该期刊会比较好中一点。(2) ACM Transactions on Graphics。美国计算机协会图形汇刊，简称TOG，该刊侧重于计算机图形的处理，影响因子在该领域也比较高，2011年为3.5。中稿的难度也极大，一般该刊对每年的SIGGRAPH(Special Interest Group for Computer GRAPHICS,计算机图形图像特别兴趣小组）会议论文全文收录。(3) International Journal of Computer vision，该刊也是该领域的顶级期刊之一，相比于PAMI来讲，该刊侧重于理论的推导。2011年影响因子为3.7，中稿难度也相当大。(4) IEEE Transactions on Image Processing，该刊也是图像处理领域的代表性期刊之一，相比于上面三个期刊来讲，该刊稍微低一点层次。2011年影响因子为3.042，中稿难度也比较大。审稿周期一年左右。除了上述让人望而生畏的顶级期刊之外，我们再看看一般的期刊吧。(1)Pattern recognition letters, 从投稿到发表，一年半时间;(2)Pattern recognition 不好中，时间长;(3)IEICE Transactions on Information and Systems， 作者中有一个必须是会员。收费高，审稿快。影响因子0.4;(4)International Journal of Pattern Recognition and Artificial Intelligence ， 审稿周期一般6–12周，影响因子偏低，容易中;(5)Computational Intelligence， 中等偏上，要求较高，杂志级别不错，关注人数偏少，比较冷门;(6)information processing letters, 影响因子低0.5左右，接搞量大，容易发表，审稿周期一般3–6个月;(7)Computer vision and image understanding, 9个月审稿期，平均投稿命中率20%，业内比较认可;(8)journal of visual communication and image representation， 投稿容易，审稿周期一年以上;(9)Signal processing letters, 影响因子0.99， 美国，审稿一个多月;(10)International Journal on Graphics, Vision and Image Processing (GVIP);(11)IET Image Processing, 影响因子0.758， EI Compendex ，审稿周期一年以上;(12)IET Computer Vision ，影响因子0.969;(13)SIAM Journal on Imaging Sciences;(14)International journal of imaging systems and technology，影响因子偏低，容易中，审稿周期一到两个月;(15)IEEE Signal Processing Letters， 审稿4—8周左右，影响因子不高，容易中，关注人不多;(16)Journal of Logic and Computation, 影响因子，0.789，SCI检索;(17)IEICE Transactions on Information and Systems 审稿时间2–4周，容易中，影响因子小，相对冷门，关注人数不多;(18)COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING，影响因子偏低，但仍然需要一定水平才可以投，审稿2–4周，SCI,EI检索;(19)Signal Processing: Image Communication，容易中，审稿周期半年到一年;(20)International Journal of Computer Vision， 较难，审稿周期半年到一年，EI,SCI检索;(21)Journal of Mathematical Imaging and Vision，审稿一个月左右，影响因子不高（1.3左右），Elsevier旗下，不容易中，稍微有些冷门，偏重数学推导;(22)Machine Vision and Applications， 影响因子偏低，但是接稿量不是很大，审稿周期一年以上，但容易发表，SCI,EI检索;(23)Pattern Analysis & Applications， 影响因子不高，影响力也比较小，审稿时间一年以上，但容易投中;(24)Signal Image and Video Processing， 容易中，审稿时间半年到一年，EI检索;(25)Pattern recognition and image analysis， EI检索;(26)Journal of digital imaging ，审稿周期半年到一年，影响因子偏低，容易中，很少有人关注;(27)Journal of  VLSI signal processing systems for signal image and video ，影响因子偏低，容易中，审稿周期一年以上，关注人比较少;(28) Neural Processing Letters,  影响因子0.75左右，审稿三个月内给出审稿意见，比较快。（发现一年只发表20篇左右，一年投稿量估计200多篇（从编号可估出），可看出命中率绝对在10%以下，待考察）(29) COMPUTERS & GRAPHICS-UK (一般简称为COMPUTERS & GRAPHICS)，Elsevier旗下图像处理领域期刊之一，2011影响因子为1.0。审稿速度（同行例子：9月底投稿，10月中旬送审，12月初大修，2月中旬小修后录用。审稿速度和编辑处理速度都比较快！）。感觉要求不是很高！(30) EURASIP Journal on Image and Video Processing，影响因子2011年为0.5，有同学投过，速度比较快，2-3个月搞定。(31) Multimedia Tools and Applications，2012年影响因子为1.014，据说比较好中，速度也还可以。(32) Communications of the ACM，2012年影响因子为2.511。中科院分区SCI大类分区2区，小类分区2区。(33) IEEE Transactions on Visualization and Computer Graphics，2012年影响因子为1.898，中科院分区SCI大类分区2区，小类分区2区。(34) IEEE Computer Graphics and Applications，2012年影响因子为1.228，中科院分区SCI大类分区3区，小类分区2区。(35) Graphical Models，2012年影响因子为0.697，中科院分区SCI大类分区4区，小类分区4区。(36) Computer Aided Geometric Design，2012年影响因子为0.810，中科院分区SCI大类分区4区，小类分区3区。(37) Computer Animation and Virtual Worlds，2012年影响因子为0.436，中科院分区SCI大类分区4区，小类分区4区。(38) Visual Computer，2012年影响因子为0.909，中科院分区SCI大类分区4区，小类分区4区。(39) Computer Graphics Forum，2012年影响因子为1.638，中科院分区SCI大类分区3区，小类分区2区。(40) International Journal of Computational Geometry and Applications，2012年影响因子为0.176，中科院（数学方向）分区SCI大类分区4区，小类分区4区。(41) Computational Geometry-Theory and Applications，2012年影响因子为0.545，中科院（数学方向）分区SCI大类分区2区，小类分区2区。(42) Journal of Visualization，2012年影响因子为0.506，中科院（工程技术）分区SCI大类分区4区，小类分区4区。(43) Computer Graphics World，2012年影响因子为0.000，中科院（工程技术）分区SCI大类分区4区，小类分区4区。(44) Virtual Reality，2012年影响因子为0.341，中科院（工程技术）分区SCI大类分区4区，小类分区4区。(43) Image and Vision Computing，2012年影响因子为1.959，中科院（工程技术）分区SCI大类分区3区，小类分区3区。(43) Computer Graphics World，2012年影响因子为0.000，中科院（工程技术）分区SCI大类分区4区，小类分区4区。"}
{"content2":"自从谷歌眼镜被推出以来，围绕人脸识别，出现了很多争议。我们相信，不管是不是通过智能眼镜，人脸识别将在人与人交往甚至人与物交互中开辟无数种可能性。为了帮助研究过程中探索人脸识别，我们列出以下人脸检测和识别API。希望有所帮助！Face Recognition - 拉姆达实验室斯蒂芬弄的。示例代码和图形演示点击http://api.lambdal.com/docs，我们的API提供了面部识别，面部检测，眼睛定位，鼻子定位，嘴巴定位，和性别分类。如果您有任何疑问，只需发一封邮件到s@lambdal.com。Face (Detection) -  计算机视觉面部识别和面部检测。这是一个完美的face.com替代品。目前，我们有一个免费的API进行人脸检测。Animetrics Face Recognition -  Animetrics的人脸识别API可用于图片中的人脸检测。面部特征或“地标”的信息被返回作为图象上的坐标。 Animetrics人脸识别也会在三维坐标轴上侦测并返回脸部位置信息。Skybiometry Face Detection and Recognition一个易于使用的人脸检测与识别的API。必须在您的SkyBiometry帐户中创建应用程序来使用它。（如果你还没有帐户，请先注册）。ImageVision Face Detection -  测试版发布更好的人脸检测服务！ImageVision是一个计算机视觉公司改进技术，确定在任意的（数字）的图像中的人脸的位置和大小。Face and scene recognition by Rekognition.com -  Face.com的替代品！快速，强大和可扩展的rekognition引擎可以做面部检测，采集，识别，场景理解！它可以自动训练使用Facebook上的图像和标签！FaceRect -  FaceRect是一个功能强大且免费的API进行人脸检测，能够发现网页中的或者上传文件中特定图片上的脸部（包括正面和侧面），并能够在一张图片中找到多张人脸，生成的 JSON输出每个脸部的边界框。Infatics Face Detection -简单的人脸检测API。OpenCV Face Recognizer  -基于OpenCV（开源计算机视觉库： http://opencv.org ）是一个开源的BSD授权的库，其中包括数百个计算机视觉算法。Libface -  Libface库旨在使人脸识别技术应用于开源社区。这是用C++编写的，托管在  Sourceforge上。这个库使用 OpenCV 2.0  ，目标是成为一个中间件，在人脸识别和检测时，开发人员不必包括任何OpenCV的代码。Automatic naming of characters in video 用来是标记电视或电影每个帧中出现的人名。CCV -现代计算机视觉库。OpenBR-开源的生物特征识别。Flandmark -开源实现面部地标探测器。计算机表情识别工具箱（CERT）-一个终端到终端的完全自动化的实时面部表情识别系统。Nviso 3D facial imaging technology-从面部表情分析人类的情感。比任何其他方法更直接和自动化。FaceReader-  FaceReader是世界上第一个能够自动分析面部表情的工具，使用户能够客观的评估一个人的情绪。Affdex -  -  通过摄像头，Affdex从面部表情读取的人们的情绪状态，比如喜好和兴趣，使营销人员更快，更准确地洞察到消费者对品牌和媒体的回应。Faceware -  分析仪从视频中抓取一个演员的面部表现，生成用于在Retargeter™ 的IMPD文件  。它通过将用户在前端的简单输入和在后端的复杂的计算机视觉算法组合起来实现这一功能。20.   Face detection in iOS-在iOS中的人脸检测。Face-Recognition-SDK-在您的应用程序添加面部识别。Oddcast face detection API-这个人脸跟踪API允许Flash开发人员利用以前只在高端视频游戏中使用的高级技术。BioID-世界上第一个基于摄像头的个人识别。Betaface API-人脸检测和识别。Discrete Area Filters Face Detector—可检测脸部15个点，4个部分，多张脸，或遮住的脸。Face detection using Support Vector Machine-该程序是克隆MATLAB中的“人脸检测系统”，可以代替神经网络算法的人脸检测，它是基于 SVG。fdlib - 是一个 C / C + +  和  MATLAB的人脸检测库，可检测图像中的正脸。Visage-一个人机接口，旨在用脸部取代传统的鼠标。用一个摄像头，将脸部面相的运动变成鼠标指针的运动。如左/右眼睛闪烁产生左/右鼠标点击事件。Face tracking Project（卡耐基梅隆大学）–结合可变形模板和颜色匹配来跟踪脸部。Real-time face detection program（实时人脸检测程序）-来自德国弗劳恩霍夫研究所IIS的演示。展示了用边缘定位匹配的面部跟踪和检测。Evaluation of Face Recognition Algorithms-来自科罗拉多州立大学研究人员开发的人脸识别算法，它提供了一套标准的众所周知的算法，并建立实验协议。Computer Vision Source Code-实用的图像处理代码集合。Acsys biometrics SDK（ACSYS生物识别SDK ）-允许第三方开发者用先进的面部生物识别技术来实现自己应用。Cognitec SDK-为世界各地的企业和政府客户开发领先的人脸识别技术和应用。KeyLemonFaceSDK-为主要的操作系统提供集成识别技术。FaceIT SDKFaceSDK- 人脸识别和基于面部的生物识别功能，易整合。Verilook SDK-–使用了VeriLook算法，该算法确保快速和可靠的面部识别。Beyond Reality Face SDK-在视频流中的一张简单图片上，计算面部位置和3D角度。这些信息可以被用来将三维对象放置到图像上，或通过头部运动控制一个应用程序。InSight SDK-通过测量面部肌肉的运动，对人脸进行完全自动化分析，并将这些面部肌肉运动转化为七个普遍的面部表情。Visage FaceDetect SDK-以C++软件开发工具包的形式，包含了很多在静止图像中发现脸和五官的有用技术。Microsoft Research Face SDK Beta- 集成了微软研究团队最新的面部识别技术。How To: Kinect for Windows SDK Face RecognitionBayometric FaceIt Face Recognition SDK-结合了传统的面部皮肤生物识别技术。FacePhi FaceRecognition SDK –包含了一组.NET和Silverlight库360Biometrics Face SDK-非常先进的脸部识别系统，用来将人脸图像嵌入或链接到已有的数据库中。Hunter TrueIDIMRSV-现实世界中的实时感知计算软件，用一个基本的摄像头，就可以测量25英尺外，多达25人的性别，年龄，关注点，目光等信息。它也有一个REST API（应用编程接口）Bob-一个信号处理和机器学习工具箱，最初是由IDIAP研究所的生物识别技术团队在瑞士开发的  。"}
{"content2":"清华大学计算机研究生课程表计算机系研究生课程介绍组合数学课程名称：组合数学课程编号：60240013课内学时： 48开课学期： 秋任课教师：黄连生【主要内容】 主要介绍组合数学的基本内容，包括基本记数方法、母函数与递推关系、容斥 原理与鸽巢原理、Burnside引理与Polya定理、区组设计与编码的初步概念、线性规划问题的单纯形算法。数据结构课程名称：数据结构课程编号：60240023课内学时： 48开课学期： 春秋任课教师：严蔚敏【主要内容】 线性表、树、图等各种基本类型数据结构的结构特性、存储表示及基本操作实现 的算法；查找表的各种表示方法；各种内排序算法的设计与分析；文件组织方法的简单介绍。软件工程技术和设计课程名称：软件工程技术和设计课程编号：60240033课内学时： 48开课学期： 春任课教师：周之英【主要内容】 1、软件开发技术发展史；2、软件工程技术方法的基本原则；3、软件过程改进； 4、需求工程；5、软件体系结构；6、面向对象设计方法；7、Design Pattern；8、 分布式系统对象模型：CORBA及DCOM/COM(OLE)等；9、实例分析（实时系统的设计）等。专家系统课程名称：专家系统课程编号：60240043课内学时： 48开课学期： 春任课教师：艾海舟【主要内容】 讲解专家系统的基本原理、构造方法、应用实例、开发工具和发展趋势，介绍人 工智能原理和知识工程的相关内容，包括产生式系统、搜索技术、知识表示、知识获取 、推理机、不确定推理方法等内容。人工智能课程名称：人工智能课程编号：60240052课内学时： 32开课学期： 秋任课教师：陈群秀【主要内容】 人工智能的定义、发展历史及研究的课题；人工智能的典型系统结构–产生式系统； 搜索技术（盲目搜索、启发式搜索、博奕树搜索）；谓词演算（知识表示）；人工智能语言程序设计。微型计算机系统接口技术课程名称：微型计算机系统接口技术课程编号：60240063课内学时： 48开课学期： 春任课教师：李 芬【主要内容】 本课程是全部用PC机控制的以硬件为主的软硬件结合的综合接口技术。通过使用EDA， 掌握先进的设计手段，结合磁盘接口技术、多媒体接口、通讯接口及虚拟现实接口技术的设计。 旨在使学生从硬件方面对计算机技术及应用有较深的了解和提高。计算机图形学基础课程名称：计算机图形学基础课程编号：60240073课内学时： 48开课学期： 春任课教师：胡事民，周登文【主要内容】 本课程主要讲授计算机图形学的基本概念、原理、算法和基本系统。主要内容包括： 计算机图形设备及系统、扫描转换、区域填充、裁剪、曲线曲面、实体造型、消隐、光照模型、 明暗效果、纹理、光线跟踪、反走样等。高等计算机系统结构课程名称：高等计算机系统结构课程编号：70240013课内学时： 48开课学期： 秋任课教师：郑纬民【主要内容】 本课程系统介绍了现代计算机系统结构的理论、技术、结构和工具环境。具体包括 程序划分、可扩展性原理、互连和通信、存储器模型、Cache一致性算法、并行计算机结构等。计算机网络体系结构课程名称：计算机网络体系结构课程编号：70240023课内学时： 48开课学期： 秋任课教师：史美林【主要内容】 本课程分两个阶段讲授。第一阶段主要讲授网络高层协议；第二阶段主要讲授网络 低层协议。结合协议的讲授，两个阶段中还会介绍一些当前流行的或先进的网络技术和组网方法。人工智能原理课程名称：人工智能原理课程编号：70240033课内学时： 48开课学期： 秋任课教师：朱小燕【主要内容】 本课程主要是面向计算机系的研究生针对几个主题展开讨论。主要教学内容有： 1、推理方法：归结推理、不确定推理方法；2、知识表示：知识表示观，知识表示方法 ；3、机器学习：机器学习的传统方法，神经网络方法；4、自然语言理解。计算机控制理论及应用课程名称：计算机控制理论及应用课程编号：70240043课内学时： 48开课学期： 秋任课教师：孙增圻，朱纪洪【主要内容】 计算机控制系统的常规设计方法；基于状态方程和传递函数模型的极点配置与 最优控制的设计方法；系统辩识和自适应控制；计算机控制系统仿真和性能计算； 采样周期选择和量化效应分析等。计算语言学课程名称：计算语言学课程编号：70240052课内学时： 32开课学期： 春任课教师：苑春法【主要内容】 计算语言学的研究对象是人类的自然语言，研究目标是使计算机能象人一样读、 写、听、说。通过建立自然语言的数学模型，用一定的数据结构和算法来表达语言信息， 进而使语言信息可以被计算。本课程讲授计算语言学的基本理论、研究方法和近年来的新进展。分布式数据库系统课程名称：分布式数据库系统课程编号：70240063课内学时： 48开课学期： 秋任课教师：周立柱【主要内容】 分布式数据库的设计；查询分解与数据定位；分布式查询处理的优化方法； 事务的语义模型与可串行化理论；分布式并发控制；分布式数据库管理系统的可靠性问题； 以及目前数据库研究的某些新进展。另外还要求设计、实现一个具备分布式查询处理功能的实验系统。智能控制课程名称：智能控制课程编号：70240073课内学时： 48开课学期： 春任课教师：孙增圻，张再兴【主要内容】 该课程系统介绍智能控制的理论和主要技术内容，主要包括：模糊逻辑控制、 神经网络控制、专家控制、学习控制、分层递阶智能控制及遗传算法等。计算机视觉课程名称：计算机视觉课程编号：70240083课内学时： 48开课学期： 秋任课教师：徐光祐【主要内容】 本课程研究根据图象信息识别和理解景物中物体的性质，类别及其空间关系的原理和方法。 重点研究根据图象获取关于物体表面性质和形状信息，以及根据图象线索进行聚类，视觉建模和 视觉推理的方法。将为开发计算机视觉的应用提供理论基础。数据安全课程名称：数据安全课程编号：70240093课内学时： 48开课学期： 秋任课教师：郭保安 【主要内容】 主要讲述信息系统中数据的加密，数字签名，用户的身份认证，秘密分存以及各种安全协 议等内容。以信息论、复杂性理论、数论和代数为基础，重点讲述RSA、DSA、DES、IDEA、MD5、 SHA等密码算法和现代密码分析技术，进而阐述现代密码算法的设计理论以及其应用等。知识工程课程名称：知识工程课程编号：70240103课内学时： 48开课学期： 春任课教师：王克宏【主要内容】 知识工程与知识处理技术的有关理论知识、网络计算模式与环境下知识处理问题的 研究与实现技术、知识的系统化管理与组织、知识处理技术、解结果的综合机制、知识 查询与处理语言、处理结果的可视化可听化可操化、知识处理系统的实现、机器学习与知识获取更新。VLSI设计基础课程名称：VLSI设计基础课程编号：70240113课内学时： 48开课学期： 秋任课教师：蔡懿慈【主要内容】 VLSI设计及IC CAD的发展及展望，VLSI工艺介绍，器件及电路设计基础，逻辑及 系统设计基础，版图设计基础，半定制及全定制设计方法，EDA的发展及系统介绍等等。语音信号数字处理课程名称：语音信号数字处理课程编号：70240123课内学时： 48开课学期： 春任课教师：吴文虎【主要内容】 包括语言产生的机理、人类发声的数学模型、语音的时域特征与线性预测分析、 语音的频谱、倒谱、矢量量化、隐马尔可夫模型、人工神经网络、语音合成技术与识别技术等。多媒体计算机技术课程名称：多媒体计算机技术课程编号：70240133课内学时： 48开课学期： 春任课教师：钟玉琢，蔡莲红【主要内容】 该课程从研究、开发和应用角度出发，综合讲述多媒体计算机的基本原理、 关键技术及其开发应用。主要内容包括：多媒体技术现状及其发展趋势、视频和音频获取技术、 多媒体数据压缩编码技术、多媒体计算机硬件及软件系统结构、多媒体数据库与基本内容检索等。计算机辅助几何设计技术课程名称：计算机辅助几何设计技术课程编号：70240143课内学时： 48开课学期： 春任课教师：秦开怀【主要内容】 主要介绍计算机辅助几何造型的理论和方法。包括：1、曲线和曲面的基本原理，2、多项式样条曲线曲面，3、自由曲线和曲面造型技术，4、几何模型表示技术，5、以实体造型为主的几何造型的理论和方法。软件复用及面向对象的软件工程环境课程名称：软件复用及面向对象的软件工程环境课程编号：70240153课内学时： 48开课学期： 春任课教师：蒋维杜【主要内容】1）软件工程回顾：软件工程发展的背景、主要成就及传统软件工程中的主要 问题；2）软件复用：软件复用和可复用软件，软件的模块化，软件模块的品质因素；3）面向对象软件的构造：面向对象方法论，面向对象技术，面向对象软件工程方法。超大规模集成电路布图理论与算法课程名称：超大规模集成电路布图理论与算法课程编号：70240162课内学时： 32开课学期： 春任课教师：洪先龙【主要内容】 本课程介绍了超大规模集成电路布图设计过程中的各种自动布图算法，包括布图规划、布局、线网布线、总体布线、详细布线、过点分配及通孔最少化等。为了适应深亚微米工艺下布图技术发展的需要，我们还介绍了性能驱动的布图算法。数字系统自动设计课程名称：数字系统自动设计课程编号：70240173课内学时： 48开课学期： 春任课教师：薛宏熙【主要内容】 本课程介绍开发EDA（Electronic Design Automation）工具的基本原理和方法， 包括：硬件描述语言、综合、模拟、故障诊断、测试、形式验证等。课程的实践环节包括习题和实验。计算几何课程名称：计算几何课程编号：70240183课内学时： 48开课学期： 秋任课教师：邓俊辉【主要内容】 计算几何学、组合几何学的主要问题：1、Introduction2、Arrangement and Configuration3、Semispace of Configuration4、Dissection of Point Set5、Convex Hull6、Visibility7、Intersection8、Point Location9、Voronoi Diagram10、Triangulation 等。算法与算法复杂性理论课程名称：算法与算法复杂性理论课程编号：70240193课内学时： 48开课学期： 春任课教师：黄连生【主要内容】主要介绍算法的时间复杂性和空间复杂性概念，DFS算法和WFS算法，分治策略 和优先策略，整数规划与动态规划，FFT算法，分类与查找，NP完全理论等内容。分布式多媒体系统与技术课程名称：分布式多媒体系统与技术课程编号：70240203课内学时： 48开课学期： 秋任课教师：刘斌，徐光祐，史元春【主要内容】 分布式多媒体技术与系统概述；分布式多媒体应用的性能要求及对网络的要求； 子网技术；通信协议；client-server计算；RAID盘阵列技术；支持分布式多媒体应用等。工程数据库设计与应用课程名称：工程数据库设计与应用课程编号：70240213课内学时： 48开课学期： 春任课教师：叶晓俊，赵致格【主要内容】 工程数据库的基本概念、组织结构、建模工具、版本管理等；工程数据库系统 的开发工具及开发方法；工程数据等在企业的应用技术；基于Web技术的工程数据库的设计与应用等内容。计算机支持的协同工作CSCW课程名称：计算机支持的协同工作CSCW课程编号：70240223课内学时： 48开课学期： 春 任课教师：史美林【主要内容】1、CSCW的基本概念、体系结构、协同控制机制；2、CSCW的基本技术、群组通 信支持、群件、多媒体会议系统、工作流系统、协同协作系统等；3、CSCW的应用支持及应用系统。微计算机系统设计课程名称：微计算机系统设计课程编号：70240233课内学时： 48开课学期： 春 任课教师：唐瑞春【主要内容】 介绍计算机系统设计领域最新的技术发展，讲授微机系统中有关CPU接口、 DRAM系统、PLD、总线、显示系统等的设计技术及相关的调试技术，通过具体的 作业和实验解决设计中的实际问题，并学习微机系统设计中的最新工具。计算机图形学课程名称：计算机图形学课程编号：70240243课内学时： 48开课学期： 春任课教师：胡事民【主要内容】 本课程主要讲授真实感图形的生成与处理算法。包括：绪论、局部光照模型、 整体光照模型、光线跟踪及其加速、辐射度算法、轴变形/FFD与计算机动画、自然 景物模拟、科学计算可视化、基于图象绘制与制造、小波方法与多分辨率网格造型等。计算机网络和计算机系统的性能评价课程名称：计算机网络和计算机系统的性能评价课程编号：70240253课内学时： 48开课学期： 春任课教师：林闯【主要内容】排队论， 随机Petri网和模拟技术。在排队论中讲述随机过程、马尔可夫过程、 排队网络模型、自相似数据传输模型等。在随机Petri网中讲述基本概念和性能分析 技术、随机网模型方法及分解与压缩技术。在模拟中讲述模拟模型技术、程序软件、结果分析。并行计算课程名称：并行计算课程编号：70240262课内学时： 32开课学期： 春任课教师：杨广文【主要内容】 讨论并行计算机的结构模型、并行算法、并行程序的设计原理与方法等。从计算 的角度，简单介绍当代主流计算机的结构及平行计算的性能评测方法；介绍常用的几 种并行计算模型；讨论并行算法的设计方法、设计技术及设计过程；讨论几个数值问题的并行算法。ATM交换技术与B-ISDN原理课程名称：ATM交换技术与B-ISDN原理课程编号：80240013课内学时： 48开课学期： 秋任课教师：刘斌【主要内容】 现代电信网络与计算机网的发展及传统技术；ATM的基本概念与B-ISDN的思想； ATM用户-网络接口；ATM交换技术；B-ISDN中的业务量控制；ATM与IP互连技术；ATM 组网与应用技术；宽带网络技术的新进展。计算机视觉专题课程名称：计算机视觉专题课程编号：80240023课内学时： 48开课学期： 春任课教师：林学訚，白雪生【主要内容】本课程以跟踪计算机视觉近期研究新方向为目的，着重选择近期提出的典型新 算法，联系其数学基础知识等，进行讲解和讨论。如射影几何及其在计算机视觉中的应用、优化算法在图像序列中的运用等。多媒体技术基础与应用课程名称：多媒体技术基础与应用课程编号：80240083课内学时： 48开课学期： 秋任课教师：林福宗【主要内容】 (1)采用\"教师－学生\"＋\"教师－网络－学生\"的教学模式。(2)内容包括：多媒 体的计算、多媒体的存储、多媒体网络应用(含应用特点、因特网、多目标广播和通 信系统)和多媒体网页编辑语言。(3)设计制作一个有中等难度的多媒体网页。科学计算可视化课程名称：科学计算可视化课程编号：80240103课内学时： 48开课学期： 春任课教师：唐泽圣，唐龙，柴建云【主要内容】 了解掌握将科学计算、工程计算的中间数据或结果数据转换为图象的基本理论、 方法和技术，并结合学生本人的专业完成一个大作业。"}
{"content2":"作为一名Android开发工程师，身边总有些同行很焦虑，看着人工智能越来越火，总是担心Android要不行了，所以，我们需要转行么？Android还能走多久？其实，无论是对于Android还是iOS开发者而言，我们更应该做的是稳固提升自己的技术，活到老学到老，时刻与不断发展的框架、标准和范式保持同步。同时，还要能活学活用，在工作中使用最合适的工具，以提高工作效率。随着机器学习在越来越多的应用程序中寻得了一席之地，越来越多的程序员加入AI领域，那么，入行AI领域需要哪些技能呢？人工智能到底有多火我相信大家之所以能来看这篇文章，也间接说明了人工智能这几年的火爆。在开发一个App越来越方便的今天，聊天可以用环信SDK，推送可以用Jpush，地图可以用百度和高德，集成社群公会系统可以用GangSDK……开发的门槛越来越低，多方位SDK的集成已经给我们省去了大量的时间人力成本，拔高和学习，我们需要从哪些方面入手？自从基于深度学习技术的算法2012年在ImageNet比赛中获得冠军以来，深度学习先是席卷了整个学术界，后又在工业界传播开来，一瞬间各大企业如果没有AI部门都不好意思对外宣传了。BAT中，百度宣布“All In AI”，阿里建立了达摩院及AI实验室，腾讯也在前不久会议上宣布“Ai In All”，并具有腾讯优图、AI Lab和微信AI实验室。2017年7月20日，国务院发布《新一代人工智能发展规划》，将人工智能上升为国家战略，为中国人工智能产业做出战略部署，对重点任务做出明确解析，抢抓重大机遇，构筑我国人工智能发展的先发优势。技术的发展往往遵循一个可预期的模式，先是萌芽，然后炒作，而后幻灭，接着才是技术成熟后的稳步爬升，最后到达应用高峰。研究分析机构Gartner每年都会推出这样一个分析新兴技术发展趋势的技术炒作周期报告。前段时间，Gartner发布了2017年的新兴技术炒作周期报告，报告聚焦了前端、后端与平台发展的三大趋势，提出了AI将无所不在（人工智能），体验将透明化和沉浸式（AR、VR），以及平台全面数字化（区块链）的观点，建议企业架构师和对技术创新有追求的人员应该积极探索和关注这三大趋势，从而了解掌握这三大趋势对自己公司和自己职业发展的未来影响。简单来说这三大趋势分别对应于括号中我备注的大家平时耳熟能详的词语。从曲线图可以看出，物联网、虚拟助手、深度学习、机器学习、无人车、纳米电子、认知计算以及区块链正处在炒作的高峰。实际上AR、VR属于计算机视觉，也可以归于人工智能范畴，因此总体上来说，未来人工智能将无处不在。Gartner把深度学习、强化学习、常规人工智能、无人车、认知计算、无人机、会话式用户接口、机器学习、智能微尘、智能机器人、智能工作环境等均列为AI技术范畴。在人机大战等吸引眼球的活动助推下，很多AI技术目前正处在炒作的高峰期。比如深度学习、机器学习、认知计算以及无人车等。对比2016年的炒作周期曲线可以发现，有些太过超前的概念仍然不愠不火，比如智能微尘。有些概念因为炒作过高已经迅速进入到了幻灭期，比如商用无人机去年还处在触发期，今年就已经接近幻灭期边缘了。相对而言，正处在炒作高峰的深度学习和机器学习技术有望在2到5年内达到技术成熟和模式成熟。除了人工智能这么火之外，对于软件工程师，尤其是移动端开发工程师，有一点我们更要关注，那就是移动端深度学习逐渐成为新的深度学习研究趋势。未来会有越来越多的基于深度学习的移动端应用出现，作为开发者的我们了解深度学习更有助于我们开发出优秀的应用，同时提升自身能力，积极抓住机会，应对未来各种变化。什么是机器学习（Machine Learning，ML）？深度学习的基础是机器学习，事实上深度学习只是机器学习的一个分支。因此我们要入门深度学习就要先了解一些机器学习的基础知识。机器学习最基本的做法，是使用算法来解析数据、从中学习，然后对真实世界中的事件做出决策和预测。与传统的为解决特定任务、硬编码的软件程序不同，机器学习是用大量的数据来“训练”，通过各种算法从数据中学习如何完成任务。有人曾举过一个例子，很形象生动，当你使用手机的语音识别进行唤醒时，有没有想过实现这一功能的全部内部流程呢？我们日常交互的大部分计算机程序，都可以使用最基本的命令来实现，但是基于机器学习的程序却没有那么简单，想象下如何写一个程序来回应唤醒词，例如“Okay，Google”，“Siri”，和“Alexa”。如果在一个只有你自己和代码编辑器的房间里，仅使用最基本的指令编写这个程序，你该怎么做？不妨思考一下……这个问题非常困难。你可能会想像下面的程序：ifinput_command=='Okey,Google':run_voice_assistant()但实际上，你能拿到的只有麦克风里采集到的原始语音信号，可能是每秒44,000个样本点。怎样才能识别出语音内容？或者简单点，判断这些信号中是否包含唤醒词。如果你被这个问题难住了，不用担心。这就是我们为什么需要机器学习。虽然我们不知道怎么告诉机器去把语音信号转成对应的字符串，但我们自己可以。换句话说，就算你不清楚怎么编写程序，好让机器识别出唤醒词“Alexa”，你自己完全能够 识别出“Alexa”这个词。由此，我们可以收集一个巨大的数据集（dataset），里面包含了大量语音信号，以及每个语音型号是否对应我们需要的唤醒词。使用机器学习的解决方式，我们并非直接设计一个系统去准确地辨别唤醒词，而是写一个灵活的程序，并带有大量的参数（parameters）。通过调整这些参数，我们能够改变程序的行为。我们将这样的程序称为模型。总体上看，我们的模型仅仅是一个机器，通过某种方式，将输入转换为输出。在上面的例子中，这个模型的输入是一段语音信号，它的输出则是一个回答{yes, no}，告诉我们这段语音信号是否包含了唤醒词。如果我们选择了正确的模型，必然有一组参数设定，每当它听见“Alexa”时，都能触发yes的回答；也会有另一组参数，针对“Apricot”触发yes。我们希望这个模型既可以辨别“Alexa”，也可以辨别“Apricot”，因为它们是类似的任务。这时候你大概能猜到了，如果我们随机地设定这些参数，模型可能无法辨别“Alexa”，“Apricot”，甚至任何英文单词。在而大多数的深度学习中，学习就是指在训练过程中更新模型的行为（通过调整参数）。换言之，我们需要用数据训练机器学习模型，其过程通常如下：1.初始化一个几乎什么也不能做的模型；2.抓一些有标注的数据集（例如音频段落及其是否为唤醒词的标注）；3.修改模型使得它在抓取的数据集上能够更准确执行任务；4.重复以上步骤2和3，直到模型看起来不错。什么是机器学习算法？从本质上讲，机器学习采用了可以从数据中学习和预测数据的算法。这些算法通常来自于统计学，从简单的回归算法到决策树等等。什么是机器学习模型？一般来说，它是指在训练机器学习算法后创建的模型构件。一旦有了一个经过训练的机器学习模型，你就可以用它来根据新的输入进行预测。机器学习的目的是正确训练机器学习算法来创建这样的模型。机器学习已广泛应用于数据挖掘、计算机视觉、自然语言处理、生物特征识别、搜索引擎、医学诊断、检测信用卡欺诈、证券市场分析、DNA序列测序、语音和手写识别、战略游戏和机器人等领域。虽然深度学习技术的发展，也促进了语音和文本领域的发展，但变化最显著的还是属于计算机视觉领域。而且由于作者是做计算机视觉的，因此这里也没法深入介绍语音和自然语言处理领域的过多细节，就简要介绍下计算机视觉领域的技术发展和相关的应用，后续的实验环节，大部分也会是基于深度学习的图像应用为主。"}
{"content2":"不多说，直接上干货！大家，都知道，在2016年，Google DeepMind的AlphaGo打败了韩国的围棋大师李世乭九段。在媒体描述DeepMind胜利的时候，将人工智能（AI）、机器学习（machine learning）和深度学习（deep learning）都用上了。这三者在AlphaGo击败李世乭的过程中都起了作用，但它们说的并不是一回事。今天我就用最简单的方法——同心圆，可视化地展现出它们三者的关系和应用。如上图，人工智能是最早出现的，也是最大、最外侧的同心圆；其次是机器学习，稍晚一点；最内侧，是深度学习，当今人工智能大爆炸的核心驱动。五十年代，人工智能曾一度被极为看好。之后，人工智能的一些较小的子集发展了起来。先是机器学习，然后是深度学习。深度学习又是机器学习的子集。深度学习造成了前所未有的巨大的影响。从概念的提出到走向繁荣1956年，几个计算机科学家相聚在达特茅斯会议（Dartmouth Conferences），提出了“人工智能”的概念。其后，人工智能就一直萦绕于人们的脑海之中，并在科研实验室中慢慢孵化。之后的几十年，人工智能一直在两极反转，或被称作人类文明耀眼未来的预言；或者被当成技术疯子的狂想扔到垃圾堆里。坦白说，直到2012年之前，这两种声音还在同时存在。过去几年，尤其是2015年以来，人工智能开始大爆发。很大一部分是由于GPU的广泛应用，使得并行计算变得更快、更便宜、更有效。当然，无限拓展的存储能力和骤然爆发的数据洪流（大数据）的组合拳，也使得图像数据、文本数据、交易数据、映射数据全面海量爆发。让我们慢慢梳理一下计算机科学家们是如何将人工智能从最早的一点点苗头，发展到能够支撑那些每天被数亿用户使用的应用的。人工智能（Artificial Intelligence）——为机器赋予人的智能早在1956年夏天那次会议，人工智能的先驱们就梦想着用当时刚刚出现的计算机来构造复杂的、拥有与人类智慧同样本质特性的机器。这就是我们现在所说的“强人工智能”（General AI）。这个无所不能的机器，它有着我们所有的感知（甚至比人更多），我们所有的理性，可以像我们一样思考。人们在电影里也总是看到这样的机器：友好的，像星球大战中的C-3PO；邪恶的，如终结者。强人工智能现在还只存在于电影和科幻小说中，原因不难理解，我们还没法实现它们，至少目前还不行。我们目前能实现的，一般被称为“弱人工智能”（Narrow AI）。弱人工智能是能够与人一样，甚至比人更好地执行特定任务的技术。例如，Pinterest上的图像分类；或者Facebook的人脸识别。这些是弱人工智能在实践中的例子。这些技术实现的是人类智能的一些具体的局部。但它们是如何实现的？这种智能是从何而来？这就带我们来到同心圆的里面一层，机器学习。机器学习—— 一种实现人工智能的方法机器学习最基本的做法，是使用算法来解析数据、从中学习，然后对真实世界中的事件做出决策和预测。与传统的为解决特定任务、硬编码的软件程序不同，机器学习是用大量的数据来“训练”，通过各种算法从数据中学习如何完成任务。机器学习直接来源于早期的人工智能领域。传统算法包括决策树学习、推导逻辑规划、聚类、分类、回归、强化学习和贝叶斯网络等等（当然还有很多）。众所周知，我们还没有实现强人工智能。早期机器学习方法甚至都无法实现弱人工智能。机器学习最成功的应用领域是计算机视觉，虽然也还是需要大量的手工编码来完成工作。人们需要手工编写分类器、边缘检测滤波器，以便让程序能识别物体从哪里开始，到哪里结束；写形状检测程序来判断检测对象是不是有八条边；写分类器来识别字母“ST-O-P”。使用以上这些手工编写的分类器，人们总算可以开发算法来感知图像，判断图像是不是一个停止标志牌。这个结果还算不错，但并不是那种能让人为之一振的成功。特别是遇到云雾天，标志牌变得不是那么清晰可见，又或者被树遮挡一部分，算法就难以成功了。这就是为什么前一段时间，计算机视觉的性能一直无法接近到人的能力。它太僵化，太容易受环境条件的干扰。随着时间的推进，学习算法的发展改变了一切。深度学习——一种实现机器学习的技术人工神经网络（Artificial Neural Networks）是早期机器学习中的一个重要的算法，历经数十年风风雨雨。神经网络的原理是受我们大脑的生理结构——互相交叉相连的神经元启发。但与大脑中一个神经元可以连接一定距离内的任意神经元不同，人工神经网络具有离散的层、连接和数据传播的方向。例如，我们可以把一幅图像切分成图像块，输入到神经网络的第一层。在第一层的每一个神经元都把数据传递到第二层。第二层的神经元也是完成类似的工作，把数据传递到第三层，以此类推，直到最后一层，然后生成结果。每一个神经元都为它的输入分配权重，这个权重的正确与否与其执行的任务直接相关。最终的输出由这些权重加总来决定。我们仍以停止（Stop）标志牌为例。将一个停止标志牌图像的所有元素都打碎，然后用神经元进行“检查”：八边形的外形、救火车般的红颜色、鲜明突出的字母、交通标志的典型尺寸和静止不动运动特性等等。神经网络的任务就是给出结论，它到底是不是一个停止标志牌。神经网络会根据所有权重，给出一个经过深思熟虑的猜测——“概率向量”。这个例子里，系统可能会给出这样的结果：86%可能是一个停止标志牌；7%的可能是一个限速标志牌；5%的可能是一个风筝挂在树上等等。然后网络结构告知神经网络，它的结论是否正确。即使是这个例子，也算是比较超前了。直到前不久，神经网络也还是为人工智能圈所淡忘。其实在人工智能出现的早期，神经网络就已经存在了，但神经网络对于“智能”的贡献微乎其微。主要问题是，即使是最基本的神经网络，也需要大量的运算。神经网络算法的运算需求难以得到满足。不过，还是有一些虔诚的研究团队，以多伦多大学的Geoffrey Hinton为代表，坚持研究，实现了以超算为目标的并行算法的运行与概念证明。但也直到GPU得到广泛应用，这些努力才见到成效。我们回过头来看这个停止标志识别的例子。神经网络是调制、训练出来的，时不时还是很容易出错的。它最需要的，就是训练。需要成百上千甚至几百万张图像来训练，直到神经元的输入的权值都被调制得十分精确，无论是否有雾，晴天还是雨天，每次都能得到正确的结果。只有这个时候，我们才可以说神经网络成功地自学习到一个停止标志的样子；或者在Facebook的应用里，神经网络自学习了你妈妈的脸；又或者是2012年吴恩达（Andrew Ng）教授在Google实现了神经网络学习到猫的样子等等。吴教授的突破在于，把这些神经网络从基础上显著地增大了。层数非常多，神经元也非常多，然后给系统输入海量的数据，来训练网络。在吴教授这里，数据是一千万YouTube视频中的图像。吴教授为深度学习（deep learning）加入了“深度”（deep）。这里的“深度”就是说神经网络中众多的层。现在，经过深度学习训练的图像识别，在一些场景中甚至可以比人做得更好：从识别猫，到辨别血液中癌症的早期成分，到识别核磁共振成像中的肿瘤。Google的AlphaGo先是学会了如何下围棋，然后与它自己下棋训练。它训练自己神经网络的方法，就是不断地与自己下棋，反复地下，永不停歇。深度学习，给人工智能以璀璨的未来深度学习使得机器学习能够实现众多的应用，并拓展了人工智能的领域范围。深度学习摧枯拉朽般地实现了各种任务，使得似乎所有的机器辅助功能都变为可能。无人驾驶汽车，预防性医疗保健，甚至是更好的电影推荐，都近在眼前，或者即将实现。人工智能就在现在，就在明天。有了深度学习，人工智能甚至可以达到我们畅想的科幻小说一般。你的C-3PO我拿走了，你有你的终结者就好了。参考https://www.leiphone.com/news/201609/gox8CoyqMrXMi4L4.html本文原载于微信公众号将门创业"}
{"content2":"人工智能从谷歌的阿尔法围棋（AlphaGo）赢了韩国围棋冠军李世石之后，开始火了。到了2017年5月，AlphaGo在赢了世界第一的柯洁之后，全世界都在讨论人工智能可能会给人类带来的影响。有人悲观地认为机器人会取代人类，也有人乐观地认为人类可以通过人工智能过上更幸福的生活。事实上人工智能已经在不断地改变人类的生活，比如我们经常使用的Cortana，或者是车牌人脸自动识别，乃至自动聊天客服机器人，都是人工智能的领域。人工智能当前的发展，主要分成下面几个子领域，包括视觉影像的自动处理，语音的自动处理，自然语言处理，知识管理等等。在每个领域都可以极大地提高处理能力，也让很多传统的职业消失。1， 视觉影像视觉是人类获取信息最大的渠道，也是人工智能最主要的研究方向，这些年包括人脸识别，车牌识别，场景识别，文字识别等等，都是这个领域中最重要的内容。从场景的角度来说，我们可以通过人脸识别来进行身份验证，通缉犯的抓捕，门禁系统管理甚至人的情感识别。这些信息的快速获得和处理，能够取代很多传统的工作，比如银行柜员，保安，一些警察的工作等等。在国内，大量的车牌自动识别系统已经部署在不同的停车场，加上移动支付的全面普及，停车场收费这个工作岗位也基本上要消失了。文字识别也是被广泛地应用在了翻译领域，以前我们旅游的时候要翻字典，现在可以直接用手机摄像头对着不明白的文字，自动翻译成想要的语言。当前计算机视觉处理的能力已经超过了普通人，除去计算机处理速度不谈，在分辨物体细微差别和存在干扰的图像中，计算机图像处理的错误率也已经低于5%，超过人眼和大脑的处理能力。2.语音当前在语音领域的最大应用就是速记和同声传译，另外还有说话人识别的功能。以前我们有专业的速记来记录现场的会议纪要，说话的内容等等。高级别的会议还需要专业的翻译人员提供现场的同声传译，不过人工智能如果发展到一定程度，会把这两部分的工作完全取代掉，毕竟人不是机器，需要休息，还会有出错的情况，而机器则完全不会出错。语音有一个另外的功能就是智能音响的唤醒词。Amazon，微软，谷歌以及国内的阿里，京东都先后推出了智能音箱，通过一个唤醒词，比如Hi Siri, Alex, Dingdong Dingdong来激活音响，并通过语音识别和人直接进行交互，比如询问天气，新闻，控制家电等等。这种弱人工智能的应用，会在最近的几年快速地进入千家万户。3.语言语言类的人工智能主要是解决机器对自然语言的理解，比如理解语言的观点，倾向性和主题等等。语言理解的应用主要集中在智能聊天机器人，自动摘要等。比如当前我们很多银行，电子商务购物网站的客服，其实都是由自动聊天机器人充当的。很多报社，新闻网站的新闻，其实也是通过自动摘要机器人直接写出来的。另外很多案件的案例查找，类似案例的判例查找，都是通过语言理解的机器人来自动实现。造成的后果就是很多初级的客服人员，初级的报社编辑甚至初级律师的失业。4. 知识人工智能在知识领域的进展是真正颠覆人类的部分，比如在一开头我们提到的AlphaGo，在下棋的时候下出了一些传统棋谱中完全没有出现的招式，最后赢得胜利。自动聊天机器人逐步发展出各种种族歧视，Facebook的聊天机器人还发展出一些人类不能理解的语言。凡此种种，都让悲观主义者觉得人工智能不可控。不过当前我们也不必杞人忧天，杯弓蛇影。人工智能的发展还远远达不到能够颠覆人类的地步，知识领域的人工智能也实现特定领域的分析。在医疗领域，人工智能可以通过分析大量的数据研究病人的MRI图像，对病人的情况进行诊断。在金融领域，人工智能的智能决策可以决定对一只股票是买入还是卖出，从而获得收益。在生物科学，医药领域，人工智能的智能学习算法能够帮助科研人员选择研究的方向。在社交领域，对各种社交媒体的文本和相关内容进行情感分析，能够了解一群人对某一些事情的倾向性。在这些领域中，人工智能已经实实在在地开始改变人类的发展方向。快速实现自己的人工智能讲了那么多，其实当前人工智能的能力还集中在弱人工智能的阶段，即解决简单的智能问题，计算机实现的智能还完全无法和人类相比。随着云计算的出现，大量的人工智能问题其实可以非常简单地通过云计算平台解决。比如我们刚才提到的视觉图像处理，在微软Microsoft Azure平台上，可以直接用分析图像的API获得每个人脸的位置，性别，表情等等信息。调用起来也非常简单，直接调用https://[location].api.cognitive.microsoft.com/vision/v1.0/analyze[?visualFeatures][&details][&language] 通过使用Http Post以及一些相应的参数，包括图像的分类，是否要识别人脸，是否识别成人或种族内容，以及识别名人。调用完这个API以后，如果成功，我们会获得200的返回，同时加上一段Json格式的字符串，里面描述了所有的相应内容，包括人脸的位置，颜色，是否为成人内容等等。具体的内容可以参考https://azure.microsoft.com/zh-cn/services/cognitive-services/computer-vision/再举个例子，如果我们想做翻译软件，以前是非常复杂的，需要处理各种数据。当有了云计算以后，我们就可以直接使用云计算提供的翻译API实现自己想要的内容。通过微软的翻译服务https://docs.microsofttranslator.com/text-translate.html#!/default/get_Translate 我们可以通过Get方法调用https://api.microsofttranslator.com/V2/Http.svc/Translate，同时提供源语言和目标语言获得专业的翻译结果。微软提供的人工智能API服务当前，微软Azure平台提供5大类30小类人工智能的API，包括影响，语音，语言，知识和搜索五大类。影响里的算法可以帮助自动审查内容，通过返回人脸、图像和情绪等智能见解构建更人性化的应用。包括计算机影像 API，人脸 API，内容审查器，情感 API，视频 API，自定义影像服务以及视频索引器。语音算法包括处理应用程序中的口述语言，例如翻译工具语音 API，说话人识别 API，必应语音 API和自定义语音服务。语言API包括了处理自然语言、评估观点和主题，并了解如何识别用户需求的功能，包括语言理解智能服务，文本分析 API，必应拼写检查 API，翻译工具文本 API，Web 语言模型 API和语言分析 API。而知识的人工智能领域则可以完成规划复杂的信息和数据，以解决智能推荐和语义搜索等任务，包括建议 API，学术知识 API，知识探索服务，QnA Maker API，实体链接智能服务 API和自定义决策服务，最后搜索则是调用了大量必应搜索的API，比如必应自动推荐 API，必应图像搜索 API，必应新闻搜索 API，必应视频搜索 API，必应 Web 搜索 API，必应自定义搜索，必应实体搜索 API。值得一提的是，微软的Azure云服务平台还有一个基于人工智能的云计算实验室项目，对于一些对领先技术有兴趣的读者，可以尝试这些不同的应用并进行反馈。主要有：• 布拉格项目 基于手势的控制• 约翰内斯堡项目 物流路线• 阿布达比项目 距离矩阵• Project Nanjing 等时线计算• 库斯科项目 与维基百科条目相关的活动• 伍伦贡项目 位置见解比如布拉格项目就可以通过Kinect的摄像头和不同手势来控制对象的变化和行为。我们可以用短短的几行代码，就可以自定义手势并且实现相应的功能，比如下面这段代码就可以实现识别手势旋转和让对象旋转的功能。我们可以访问https://azure.microsoft.com/zh-cn/services/cognitive-services/ 获得所有完整的的信息。结语我们并无法预期何时机器可以超过人类，不过机器的智能发展速度要远远快于人类进化的速度。随着奇点的到来，人类在很多智力上会大大落后于机器。云计算就是人工智能的发动机。作为开发者和技术人员，可以通过云的能力，把人工智能集成到自己的应用中去，帮助人类利用好人工智能的能力，让我们的生活越来越美好。"}
{"content2":"超像素分割技术发展情况梳理(Superpixel Segmentation）Sason@CSDN当前更新日期：2013.05.12.一. 基于图论的方法(Graph-based algorithms)：1. Normalized cuts, 2000.Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 22(8):888–905,  2000.T. Cour, F. Benezit, and J. Shi. Spectral segmentation with multiscale graph decomposition. In IEEE Computer Vision and Pattern Recognition (CVPR) 2005, 2005.2. Graph-based segmentation, 2004.Pedro Felzenszwalb and Daniel Huttenlocher. Efﬁcient graph-basedimage segmentation. International Journal of Computer Vision (IJCV),59(2):167–181, September 2004.3. Graph cuts method, 2008.Alastair Moore, Simon Prince, Jonathan Warrell, Umar Mohammed, andGraham Jones. Superpixel Lattices. IEEE Computer Vision and PatternRecognition (CVPR), 2008.4. GCa10 and GCb10, 2010.O. Veksler, Y. Boykov, and P. Mehrani. Superpixels and supervoxels in an energy optimization framework. In European Conference on Computer Vision (ECCV), 2010.5. Entropy Rate Superpixel Segmentation, 2011.Ming-Yu Liu, Tuzel, O., Ramalingam, S. , Chellappa, R., Entropy Rate Superpixel Segmentation, CVPR,2011.二. 基于梯度下降的方法(Gradient-ascent-based algorithms)：1. Watershed,1991.Luc Vincent and Pierre Soille. Watersheds in digital spaces: An efﬁcient algorithm based on immersion simulations. IEEE Transactions on Pattern Analalysis and Machine Intelligence, 13(6):583–598, 1991.2. Mean Shift, 2002.D. Comaniciu and P. Meer. Mean shift: a robust approach toward featurespace analysis. IEEE Transactions on Pattern Analysis and MachineIntelligence, 24(5):603–619, May 2002.3. Quick Shift, 2008A. Vedaldi and S. Soatto. Quick shift and kernel methods for mode seeking. In European Conference on Computer Vision (ECCV), 2008.4. Turbopixel, 2009.A. Levinshtein, A. Stere, K. Kutulakos, D. Fleet, S. Dickinson, and K. Siddiqi. Turbopixels: Fast superpixels using geometric ﬂows. IEEETransactions on Pattern Analysis and Machine Intelligence (PAMI),2009.自然图像抠图/视频抠像技术发展情况梳理（image matting, alpha matting, video matting）--计算机视觉专题1http://blog.csdn.net/anshan1984/article/details/8581225图像/视觉显著性检测技术发展情况梳理(Saliency Detection、Visual Attention）--计算机视觉专题2http://blog.csdn.net/anshan1984/article/details/8657176超像素分割技术发展情况梳理(Superpixel Segmentation）--计算机视觉专题3http://blog.csdn.net/anshan1984/article/details/8918167欢迎来到我的CSDN博客：http://blog.csdn.net/anshan1984/"}
{"content2":"看了版上很多贴子，发现很多版友都在问“热门研究方向”、“最新方法”等。有同学建议国内某教授的教材、或者CNKI、或者某些SCI期刊。每当看到这种问题，我都有点纳闷，为什么不去读顶级会议上的论文？我无意否认以上文献的价值，但是在机器学习、计算机视觉和人工智能领域，顶级会议才是王道。国内教材和CNKI上的基本是N年前老掉牙的东西。有人会质疑这些会议都只是EI。是的，这的确非常特殊：在许多其它领域，会议都是盛会，比如society of neuroscience的会议，每次都有上万人参加，带个abstract和poster就可以去。但在所讨论的几个领域，顶级会议的重要性无论怎么强调都不为过。可以从以下几点说明：（1）因为机器学习、计算机视觉和人工智能领域发展非常迅速，新的工作层出不穷，如果把论文投到期刊上，一两年后刊出时就有点out了。因此大部分最新的工作都首先发表在顶级会议上，这些顶级会议完全能反映“热门研究方向”、“最新方法”。（2）很多经典工作大家可能引的是某顶级期刊上的论文，这是因为期刊论文表述得比较完整、实验充分。但实际上很多都是在顶级会议上首发。比如PLSA, Latent Dirichlet Allocation等。（3）如果注意这些领域大牛的pulications，不难发现他们很非常看重这些顶级会议，很多人是80%的会议+20%的期刊。即然大牛们把最新工作发在顶级会议上，有什么理由不去读顶级会议？(1)以下是不完整的列表，但基本覆盖。机器学习顶级会议：NIPS, ICML, UAI, AISTATS;  （期刊：JMLR, ML, Trends in ML, IEEE T-NN）计算机视觉和图像识别：ICCV, CVPR, ECCV;  （期刊：IEEE T-PAMI, IJCV, IEEE T-IP）人工智能：IJCAI, AAAI; （期刊AI）另外相关的还有SIGRAPH, KDD, ACL, SIGIR, WWW等。特别是，如果做机器学习，必须地，把近4年的NIPS, ICML翻几遍；如果做计算机视觉，要把近4年的ICCV, CVPR, NIPS, ICML翻几遍。(2)另外补充一下：大部分顶级会议的论文都能从网上免费下载到，比如CV方面：http://www.cvpapers.com/index.html; NIPS: http://books.nips.cc/;  JMLR(期刊):http://jmlr.csail.mit.edu/papers/;  COLT和ICML(每年度的官网): http://www.cs.mcgill.ca/~colt2009/proceedings.html。希望这些信息对大家有点帮助。(3)说些自己的感受。我的研究方向主要是统计学习和概率图模型，但对计算机视觉和计算神经科学都有涉及，对Data mining和IR也有些了解。这些领域，从方法和模型的角度看，统计模型（包括probabilistic graphical model和statistical learning theory）是主流也是非常有影响力的方法。有个非常明显的趋势：重要的方法和模型最先在NIPS或ICML出现，然后应用到CV,IR和MM。虽然具体问题和应用也很重要，但多关注和结合这些方法也很有意义。对于这个领域的牛人们，以上全是浅显的废话，完全可以无视。欢迎讨论。"}
{"content2":"转：https://baijiahao.baidu.com/s?id=1588563162916669654&wfr=spider&for=pc对于很多初入学习人工智能的学习者来说，对人工智能、机器学习、深度学习的概念和区别还不是很了解，有可能你每天都能听到这个概念，也经常提这个概念，但是你真的懂它们之间的关系吗？那么接下来就给大家从概念和特点上进行阐述。先看下三者的关系。人工智能包括了机器学习和深度学习，机器学习包括了深度学习，他们是子类和父类的关系。下面这张图则更加细分。2、什么是人工智能人工智能（ArtificialIntelligence），英文缩写为AI。是计算机科学的一个分支。人工智能是对人的意识、思维的信息过程的模拟。人工智能不是人的智能，但能像人那样思考、也可能超过人的智能。数学常被认为是多种学科的基础科学，数学也进入语言、思维领域，人工智能学科也必须借用数学工具。人工智能实际应用：机器视觉，指纹识别，人脸识别，视网膜识别，虹膜识别，掌纹识别，专家系统，自动规划，智能搜索，定理证明，博弈，自动程序设计，智能控制，机器人学，语言和图像理解，遗传编程等。人工智能目前也分为：强人工智能(BOTTOM-UPAI)和弱人工智能(TOP-DOWNAI)，有兴趣大家可以自行查看下区别。3、什么是机器学习机器学习(MachineLearning,ML)，是人工智能的核心，属于人工智能的一个分支。机器学习理论主要是设计和分析一些让计算机可以自动“学习”的算法。机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。所以机器学习的核心就是数据，算法（模型），算力（计算机运算能力）。机器学习应用领域十分广泛，例如：数据挖掘、数据分类、计算机视觉、自然语言处理(NLP)、生物特征识别、搜索引擎、医学诊断、检测信用卡欺诈、证券市场分析、DNA序列测序、语音和手写识别、战略游戏和机器人运用等。机器学习就是设计一个算法模型来处理数据，输出我们想要的结果，我们可以针对算法模型进行不断的调优，形成更准确的数据处理能力。但这种学习不会让机器产生意识。机器学习的工作方式选择数据：将你的数据分成三组：训练数据、验证数据和测试数据。模型数据：使用训练数据来构建使用相关特征的模型。验证模型：使用你的验证数据接入你的模型。测试模型：使用你的测试数据检查被验证的模型的表现。使用模型：使用完全训练好的模型在新数据上做预测。调优模型：使用更多数据、不同的特征或调整过的参数来提升算法的性能表现。机器学习的分类基于学习策略的分类1、机械学习(Rotelearning)2、示教学习(Learningfrominstruction或Learningbybeingtold)3、演绎学习(Learningbydeduction)4、类比学习(Learningbyanalogy)5、基于解释的学习(Explanation-basedlearning,EBL)6、归纳学习(Learningfrominduction)基于所获取知识的表示形式分类1、代数表达式参数2、决策树3、形式文法4、产生式规则5、形式逻辑表达式6、图和网络7、框架和模式（schema）8、计算机程序和其它的过程编码9、神经网络10、多种表示形式的组合综合分类1、经验性归纳学习(empiricalinductivelearning)2、分析学习（analyticlearning）3、类比学习4、遗传算法（geneticalgorithm）5、联接学习6、增强学习（reinforcementlearning）学习形式分类1、监督学习(supervisedlearning)2、非监督学习(unsupervisedlearning)注：细分的话还有半监督学习和强化学习。当然，后面的深度学习也有监督学习、半监督学习和非监督学习的区分。4、机器学习之监督学习监督学习（SupervisedLearning）是指利用一组已知类别的样本调整分类器的参数，使其达到所要求性能的过程，也称为监督训练或有教师学习。也就是我们输入的数据是有标签的样本数据（有一个明确的标识或结果、分类）。例如我们输入了50000套房子的数据，这些数据都具有房价这个属性标签。监督学习就是人们常说的分类，通过已有的训练样本（即已知数据以及其对应的输出）去训练得到一个最优模型（这个模型属于某个函数的集合，最优则表示在某个评价准则下是最佳的）。再利用这个模型将所有的输入映射为相应的输出，对输出进行简单的判断从而实现分类的目的。就像我输入了一个人的信息，他是有性别属性的。我们输入我们的模型后，我们就明确的知道了输出的结果，也可以验证模型的对错。举个例子，我们从小并不知道什么是手机、电视、鸟、猪，那么这些东西就是输入数据，而家长会根据他的经验指点告诉我们哪些是手机、电视、鸟、猪。这就是通过模型判断分类。当我们掌握了这些数据分类模型，我们就可以对这些数据进行自己的判断和分类了。在监督式学习下，输入数据被称为“训练数据”，每组训练数据有一个明确的标识或结果，如对防垃圾邮件系统中“垃圾邮件”“非垃圾邮件”，对手写数字识别中的“1“，”2“，”3“，”4“等。在建立预测模型的时候，监督式学习建立一个学习过程，将预测结果与“训练数据”的实际结果进行比较，不断的调整预测模型，直到模型的预测结果达到一个预期的准确率。监督式学习的常见应用场景如分类问题和回归问题。常见监督式学习算法有决策树（ID3，C4.5算法等），朴素贝叶斯分类器，最小二乘法，逻辑回归（LogisticRegression），支持向量机（SVM），K最近邻算法（KNN，K-NearestNeighbor），线性回归（LR，LinearRegreesion），人工神经网络（ANN，ArtificialNeuralNetwork），集成学习以及反向传递神经网络（BackPropagationNeuralNetwork）等等。下图是几种监督式学习算法的比较：5、机器学习之非监督学习非监督学习（UnsupervisedLearing）是另一种研究的比较多的学习方法，它与监督学习的不同之处，在于我们事先没有任何训练样本，而需要直接对数据进行建模。是否有监督（Supervised），就看输入数据是否有标签（Label）。输入数据有标签（即数据有标识分类），则为有监督学习，没标签则为无监督学习（非监督学习）。在很多实际应用中，并没有大量的标识数据进行使用，并且标识数据需要大量的人工工作量，非常困难。我们就需要非监督学习根据数据的相似度，特征及相关联系进行模糊判断分类。6、机器学习之半监督学习半监督学习（Semi-supervisedLearning）是有标签数据的标签不是确定的，类似于：肯定不是某某某，很可能是某某某。是监督学习与无监督学习相结合的一种学习方法。半监督学习使用大量的未标记数据，以及同时使用标记数据，来进行模式识别工作。当使用半监督学习时，将会要求尽量少的人员来从事工作，同时，又能够带来比较高的准确性。在此学习方式下，输入数据部分被标识，部分没有被标识，这种学习模型可以用来进行预测，但是模型首先需要学习数据的内在结构以便合理的组织数据来进行预测。半监督学习有两个样本集，一个有标记，一个没有标记。分别记作Lable={(xi,yi)}，Unlabled={(xi)}，并且数量,L<<U。注：单独使用有标记样本,我们能够生成有监督分类算法单独使用无标记样本,我们能够生成无监督聚类算法两者都使用,我们希望在1中加入无标记样本,增强有监督分类的效果;同样的,我们希望在2中加入有标记样本,增强无监督聚类的效果一般而言,半监督学习侧重于在有监督的分类算法中加入无标记样本来实现半监督分类，也就是在1中加入无标记样本，增强分类效果。应用场景包括分类和回归，算法包括一些对常用监督式学习算法的延伸，这些算法首先试图对未标识数据进行建模，在此基础上再对标识的数据进行预测。如自训练算法(self-training)、多视角算法(Multi-View)、生成模型（EnerativeModels）、图论推理算法（GraphInference）或者拉普拉斯支持向量机（LaplacianSVM）等。"}
{"content2":"计算机视觉和机器学习领域 近两年部分综述文章，欢迎推荐其他的文章，不定期更新。【2015】[1].    E.Sariyanidi, H. Gunes, A. Cavallaro, Automatic Analysisof Facial Affect: A Survey of Registration, Representation, and Recognition,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 37, NO. 6,JUNE 2015PDF:  2015-1. Automatic Analysis of Facial Affect A Survey of Registration, Representa.pdf (1.51 MB)[2].    T. Li,H. Chang, M. Wang, B.B. Ni, R.C. Hong, S.C. Yan, CrowdedScene Analysis: A Survey, IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FORVIDEO TECHNOLOGY, VOL. 25, NO. 3, MARCH 2015PDF:  2015-2. Crowded Scene Analysis A Survey.pdf (5.42 MB)[3].    Z.Zhang, Y. Xu, J. Yang, X.L. Li, D. Zhang, A Survey of Sparse Representation: Algorithms and Applications, IEEE ACCESS, date ofpublication May 6, 2015PDF:  2015-3. A Survey of Sparse Representation.pdf (4.8 MB)[4].    J.Galbally, S. Marcel, J. Fierrez, Biometric AntispoofingMethods: A Survey in Face Recognition, IEEE ACCESS, date of publicationDecember 18, 2014PDF:  2015-4. Biometric Antispoofing Methods A Survey in Face Recognition.pdf (10.43 MB)[5].    B.Tian, B. T. Morris, M. Tang, Y.Q. Liu, Y. J. Yao, C. Guo, D.Y. Shen, S.H. Tang, Hierarchical and Networked Vehicle Surveillance in ITS:A Survey, IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL.16, NO. 2, APRIL 2015PDF:  2015-5. Hierarchical and Networked Vehicle Surveillance in ITS A Survey.pdf (1.12 MB)[6].    A. Betancourt,P. Morerio, C. S. Regazzoni, and M. Rauterberg, TheEvolution of First Person Vision Methods: A Survey, IEEE TRANSACTIONS ONCIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 25, NO. 5, MAY 2015PDF:  2015-6. The Evolution of First Person Vision Methods A Survey.pdf (2.69 MB)[7].    L.Shao, F. Zhu, and X.L. Li, Transfer Learning for VisualCategorization: A Survey, IEEE TRANSACTIONS ON NEURAL NETWORKS ANDLEARNING SYSTEMS, VOL. 26, NO. 5, MAY 2015PDF:  2015-7. Transfer Learning for Visual Categorization A Survey.pdf (3.11 MB)[8].    Freek Stulp, Olivier Sigaud, Many regression algorithms, one unified model: A review, Neural Networks, June 2015PDF: 2015-NN-Many regression algorithms, one unified model A review.pdf (1.1 MB)[9].      Qixiang Ye, David Doermann，Text Detection and Recognition in Imagery: A Survey， IEEE TPAMI, July 2015PDF:  2015-PAMI-Text Detection and Recognition in Imagery A survey.pdf (1.12 MB)[10].    Salient Object Detection: A Benchmark, Ali Borji, Ming-Ming Cheng, Huaizu Jiang, Jia Li, arXiv eprint, 2015.  [pdf] [Project page]【2014】[1].    S. Fu,H. B. He, Z.G. Hou, Learning Race from Face: A Survey, IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 36, NO.12, DECEMBER 2014PDF:  2014-1. Learning Race from Face A Survey.pdf (2.55 MB)[2].    H.L. Zhou,A. Mian, L. Wei, D. Creighton, M. Hossny, and S. Nahavandi, Recent Advances on Singlemodal and Multimodal FaceRecognition: A Survey, IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS, VOL.44, NO. 6, DECEMBER 2014PDF:  2014-2. Recent Advances on Singlemodal and Multimodal Face Recognition A Survey.pdf(669.19 KB)[3]       Salient Object Detection: A Survey, Ali Borji, Ming-Ming Cheng, Huaizu Jiang, Jia Li, arXiv eprint, 2014.  [pdf] [Project page] [Bib]【2013】[1].  O. D. Lara, M.A. Labrador, A Survey on Human Activity Recognition using WearableSensors,IEEE COMMUNICATIONS SURVEYS & TUTORIALS, VOL. 15, NO. 3,THIRD QUARTER 2013PDF:  2013-1. A Survey on Human Activity Recognition using Wearable Sensors.pdf (1.65 MB)[2].  A. Sotiras, C. Davatzikos, Nikos. Paragios, Deformable Medical Image Registration: A Survey, IEEETRANSACTIONS ON MEDICAL IMAGING, VOL. 32, NO. 7, JULY 2013PDF:  2013-2.Deformable Medical Image Registration A Survey.pdf (1.3 MB)[3].  A. Alrahayfeh, M. Faezipour, Eye Tracking and Head Movement Detection: A State-of-ArtSurvey, IEEE Journal of Translational Engineering in Health andMedicine, 2013PDF:  2013-3. Eye Tracking and Head Movement Detection A State-of-Art Survey.pdf (2.22 MB)[4].  P.V.K. Borges, N. Conci, and A. Cavallaro, Video-Based Human Behavior Understanding: A Survey, IEEETRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 23, NO. 11,NOVEMBER 2013PDF:  2013-4. Video-Based Human Behavior Understanding A Survey.pdf (15.27 MB)[5] Mao Ye, Qing Zhang, Liang Wang, Jiejie Zhu, Ruigang Yang, Juergen Gall. A Survey on Human Motion Analysis from Depth Data. Time-of-Flight and Depth Imaging.  2013.PDF: survey-motionanalysis_DRAFT.pdf  (14.47 MB)整理于：http://www.valseonline.org/thread-505-1-1.html"}
{"content2":"运动目标跟踪在军事制导，视觉导航，机器人，智能交通，公共安全等领域有着广泛的应用。例如，在车辆违章抓拍系统中，车辆的跟踪就是必不可少的。在入侵检测中，人、动物、车辆等大型运动目标的检测与跟踪也是整个系统运行的关键所在。所以，在计算机视觉领域目标跟踪是一个很重要的分支。运动目标检测是运动目标跟踪的前提；运动目标检测，依据目标与摄像机之间的关系可以分为静态背景下的运动检测与动态背景下的运动检测。一，静态背景下的运动检测：整个监控过程中只有目标在运动；主要包括以下几种方法。1，背景差分法；整个监控过程中，需要不停地维护一个“纯背景”。对于任意一帧监控画面而言，将其与纯背景进行差分，从而得到出现在当前画面中的运动目标。该方法对光照变化、天气、背景变化比较敏感。而且，需要不停进行地依靠学习来维护一个纯背景画面。此外，背景的维护和更新，阴影去除等对运动目标的检测至关重要。2，帧间差分法；通过相邻帧之间的差值计算，来获得运动目标的位置、形状等信息的方法。该方法对光照的适应能力很强，但由于运动目标像素上的相似性，从而不能完整地检测出运动目标。需要提醒的是，有研究人员将相邻帧间的差分进行改进，得到三帧差分方法。即，利用相邻三帧之间的差值计算，来进行运动目标的检测。该方法经很多研究人员和工程师的实际测试，证明了其在特定环境中优良的性能。3，光流法；在空间中，运动可以用运动场描述；而在一个图像平面上，物体的运动往往是通过图像序列中图像灰度分布的不同来体现, 从而使空间中的运动场转移到图像上就表示为光流场。光流场反映了图像上每一点灰度的变化趋势。它可看成是带有灰度的像素点在图像平面上运动而产生的瞬时速度场,也是一种对真实运动场的近似估计。在比较理想的情况下，它能够检测独立运动的对象而不需要预先知道场景的任何信息, 可以很精确地计算出运动物体的速度，并且可用于动态场景的情况。但是大多数光流方法的计算相当复杂, 对硬件要求比较高, 不适于实时处理, 而且对噪声比较敏感,抗噪性差。二，动态背景下的运动检测：监控过程中，目标和背景都在发生运动或变化；在运动目标检测的应用环境中，动态背景相比而言更加复杂。根据相机的运动形式，可以分为以下两种：1，相机支架固定；但相机可以随着云台的运动而发生旋转，倾斜等运动。另外，相机也可以根据远程计算机指令来控制镜头调焦，从而产生远景和近景缩放运动。2，相机置于移动设备之上（例如，车载相机）；对于以上两种相机运动形式的任意一种而言，在进行运动目标检测之前，都需要根据一定的方法进行全局运动估计与补偿。通常，可以利用块匹配法、特征点匹配法等进行运动量的估计。当然，也可以利用光流法建立光流场模型，利用光流方程求解图像像素点的运动速度。运动目标跟踪就是在一个连续视频序列中，在每一帧监控画面中找到感兴趣的运动目标（例如，车辆，行人，动物等）。跟踪可以大致分为以下几个步骤：（1）目标的有效描述；目标的跟踪过程跟目标检测一样，需要对其进行有效的描述，即，需要提取目标的特征，从而能够表达该目标；一般来说，我们可以通过图像的边缘、轮廓、形状、纹理、区域、直方图、矩特征、变换系数等来进行目标的特征描述；（2）相似性度量计算；常用的方法有：欧式距离、马氏距离、棋盘距离、加权距离、相似系数、相关系数等；（3）目标区域搜索匹配；如果对场景中出现的所有目标都进行特征提取、相似性计算，那么，系统运行所耗费的计算量是很大的。所以，我们通常采用一定的方式对运动目标可能出现的区域进行估计，从而减少冗余，加快目标跟踪的速度；常见的预测算法有：Kalman滤波、粒子滤波、均值漂移等；"}
{"content2":"2018年国际消费性电子展（CES）上，最明显的一个趋势是Amazon与Google的语音技术进驻战，如AmazonAlexa进驻到Acer笔电内，Google Assist进驻到KIA汽车内，其他如智能电视、智能喇叭，乃至传统数字录放机TiVo都成为抢占进驻的对象。Google Assistant语音识别进驻大战这是一波新的抢滩战，过去Google具有PC上网的搜寻入口优势，使Google赚取庞大的广告中介收益。但上网的形式在改变，包含走动时对手机「说」、客厅躺卧时对智慧喇叭「说」、或开车时对行车计算机「说」，都要得到搜寻响应，而且是语音响应，这就成了语音识别进驻大战的开端，而且比过去的搜索引擎更激烈。搜索引擎一次还可以显现三、五个结果选项，但语音响应只能逐一念，听完前三个大概就没耐性了，所以第一个响应的答案，将更为重要。为了让自家的语音识别、认知运算、人工智能等技术更普及，Amazon与Google也推出相关套件，并尽可能平价供应给有兴趣体验的开发人员，例如2016年6月Amazon即以树莓派计算机（RPi 2 Model B）为基础搭配其Alexa软件，就可模块出Amazon Echo的效果，并进行相关开发（详细信息可参考这里）。https://aws.amazon.com/cn/blogs/china/raspberry-alexa/Google智能语音，强势出击Voice Kit去年，Google也推出开发类似的套件，称为AIY，取自人工智能的AI（Artificial Intelligence），与自己动手做的DIY（Do ItYourself）两字合并而成，意指自己动手做的人工智能套件。Google率先推出语音版本的套件，即Google AIY Voice Kit，而后也推出视觉版本的套件Google AIY Vision Kit。Google AIY或许受到Amazon的启发，所以也是以树莓派计算机为基础，再搭上必要的硬件外围与零件，构成最基本需求的语音、视觉辨识开发装置，同时也可能受到IBM发起的TJBot项目（同样是以树莓派为基础的语音识别、人工智能应用装置）所影响，追加了纸板外壳与简单的互动接口，例如会三色发光的LED灯号与按钮，但并没有TJBot的摆动手臂（用伺服马达驱动）。Google推出第一套语音版时（称为V1），所搭配使用的树莓派计算机仍是一般最普及常见的Model B（树莓派3），价格比较高，约35美元。但之后再推出的语音版及视觉版则改采了较便宜的Pi Zero W（无线版的Pi Zero），价格约10美元。无论视觉版或语音版套件都包含了树莓派在里头，也包含了树莓派所需要的MicroSD记忆卡，另外若是视觉版还额外包含了树莓派用的摄影机模块（V2版），如此语音版套件的价格约49.99美元，视觉版则为89.99美元。视觉版 VS 语音版既然视觉版与语音版有价差，那么差在何处？事实上视觉版与语音版有一些相同的配件，如三色LED灯号、按钮、GPIO接脚线路等，两版本的主要差异在于：视觉版的介接板卡（称为VisionBonnet board，语音版则称为VoiceBonnet board）上多一颗图像处理人工智能芯片，也称为VPU（Vision Processing Unit视觉处理单元），即Intel Movidius MA2450，Movidius是Intel于2016年购并的业者。视觉版MA2450可以强化视觉运算，减轻树莓派计算机的视觉运算负荷，同时还能执行神经网络的运算工作，此是视觉版要多贵出40美元的主因。当然！视觉版还有提供摄影机与摄影机上盖护镜，但同时没有语音版的喇叭，取而代之的只有简单的蜂鸣器。语音版由于语音运算明显比视觉轻量，纯耗用树莓派计算机的运算力即可，不需要额外的加速运算芯片，因而成本与订价可以低于视觉版。Vision Kit的辨识模型简介值得注意的是，视觉版有附带三套以TensorFlow为基础的神经网络模型软件：第一套是盘中物辨识，是采开放源代码的神经网络模型项目MobileNets所建构成，可辨识上千个常见的生活用品；第二套可辨识出影像中的人脸，并透过脸部表情分析目前的心情愉悦度，打出愉悦分数；第三套则可辨识影像中何者为人、何者为猫、何者为狗。此外官网还有列出其他的辨识模型，例如：辨识碗盘内放的东西为何物？辨识自然界的景物等，多数模型也是基于MobileNets所构成，但也有基于SqueezeNet所构成的模型。视觉版中也附有编译工具软件，可以在工作站或云端上，对模型进行训练、再训练等编译。至于语音版，所附的软件主要是Google Assist的软件开发工具包（SDK）与示范用应用程序（DEMO Application）。其他配套也包含Google Play应用程序商店的卖架上可以找到Google AIY Projects的手机应用程序（App），可免费下载安装使用，并搭配AIY硬件套件一起开发运用。结束语Google在最后也出一些发想题，好激发大家的人工智能开发撰写动力，例如用视觉版套件开发出能辨识各种动植物的人工智能软件，辨识正在行驶的车子是否有偏离车道？辨识您的访客是否满意您家里的节庆装饰等，这些都期望能以视觉版套件的软硬件为基础，更快完成与实现。原文参考https://mp.weixin.qq.com/s?__biz=MzU0MTg0ODIzNA==&mid=2247484725&idx=1&sn=6d53e71d6a7f0b4b8aca0d2ab253cf6a&chksm=fb22e2f0cc556be61a07a93a7ce2221d4a866eaeb6ad124c06f7f5dd6051a138c77ab692d524#rd"}
{"content2":"今日CS.CV计算机视觉论文速览Wed, 20 Mar 2019Totally 66 papersInteresting:📚LiteFlowNet2, 基于数据可信度和正则化的轻量级的光流框架(from 香港中文)系统架构和S,M单元细节：与相关方法的比较：code:https://github.com/twhui/LiteFlowNet2📚合成搞怪图像的新方法, (from University of Bern)📚EV-IMO,用于实践相机的移动分割数据集 (from 马里兰)Depth and per-pixel pose inference on sparse event data:数据集:http://prg.cs.umd.edu/EV-IMO.html.📚SCALP, 利用等高线线性路径的超像素分割(from Univ. Bordeaux)contour作为先验：数据集：[The Berkeley Segmentation Dataset and Benchmark](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/)ref:http://blog.sina.com.cn/s/blog_6f126f5b010134a3.html http://blog.sina.com.cn/s/blog_50363a7901011dtd.html📚Atari-HEAD, 基于雅达利的人眼注意力数据集，包含了44 小时16个游戏共 2.97M个相关动作(from 德克萨斯奥斯丁大学)数据集：https://zenodo.org/record/2587121📚卡通人物虚拟穿衣系统, 基于输入的衣服数据，模型可以得到多种体型下的衣着试穿效果，并以250fps生成动态效果。(from Universidad Rey Juan Carlos)📚域自适应的人体皮肤检测, (from Universidade de Brasilia)作者主页：https://cic.unb.br/~teodecampos/📚SDPS-Net, 自标定的光度立体视觉网络，可以处理未知光照、形状和反射的立体重建问题。(from 港大 牛津 北大 鹏城实验室)网络分为标定部分和法向量估计部分：code:https://guanyingc.github.io/SDPS-Net📚PZnet, 提出了一种多核CPU上运行的PZNet，在Intel Xeon CPUs使用ZnnPhi库实现.，加速了三维卷积特别是医学领域的图像处理。(from 普林斯度 MIT )📚基于单个RGB生成新视角图像序列, (from Poznan University of Technology)生成的RGB和深度图：📚跨季节的语义分割数据集, (from Chalmers University of Technology)通过季节标注的数据训练分割结果更加鲁棒：code:https://github.com/maunzzz/cross-season-segmentation数据集：https://visuallocalization.net/📚添加了流型和感知损失的超分辨GAN, (from IIT Bombay)数据集:https://github.com/arjunvekariyagithub/camelyon16-grand-challenge医学图像比赛Camelyon：https://camelyon17.grand-challenge.org/生物医学图像分析比赛网站：https://camelyon17.grand-challenge.org/Daily Computer Vision Papers1.Title: Learning Correspondence from the Cycle-Consistency of TimeAuthors:Xiaolong Wang, Allan Jabri, Alexei A. Efros2.Title: Human Activity Recognition for Edge DevicesAuthors:Manjot Bilkhu, Hammababdullah Ayyubi3.Title: Visual Cue Integration for Small Target Motion Detection in Natural Cluttered BackgroundsAuthors:Hongxin Wang, Jigen Peng, Qinbing Fu, Huatian Wang, Shigang Yue4.Title: PZnet: Efficient 3D ConvNet Inference on Manycore CPUsAuthors:Sergiy Popovych, Davit Buniatyan, Aleksandar Zlateski, Kai Li, H. Sebastian Seung5.Title: EV-IMO: Motion Segmentation Dataset and Learning Pipeline for Event CamerasAuthors:Anton Mitrokhin, Chengxi Ye, Cornelia Fermuller, Yiannis Aloimonos, Tobi Delbruck6.Title: Understanding the Limitations of CNN-based Absolute Camera Pose RegressionAuthors:Torsten Sattler, Qunjie Zhou, Marc Pollefeys, Laura Leal-Taixe7.Title: Bilinear Representation for Language-based Image Editing Using Conditional Generative Adversarial NetworksAuthors:Xiaofeng Mao, Yuefeng Chen, Yuhong Li, Tao Xiong, Yuan He, Hui Xue8.Title: MUSEFood: Multi-sensor-based Food Volume Estimation on SmartphonesAuthors:Junyi Gao, Weihao Tan, Liantao Ma, Yasha Wang, Wen Tang9.Title: Crowd Counting with Decomposed UncertaintyAuthors:Min-hwan Oh, Peder A. Olsen, Karthikeyan Natesan Ramamurthy10.Title: A Lightweight Optical Flow CNN - Revisiting Data Fidelity and RegularizationAuthors:Tak-Wai Hui, Xiaoou Tang, Chen Change Loy11.Title: Evaluating Sequence-to-Sequence Models for Handwritten Text RecognitionAuthors:Johannes Michael, Roger Labahn, Tobias Grüning, Jochen Zöllner12.Title: Self-calibrating Deep Photometric Stereo NetworksAuthors:Guanying Chen, Kai Han, Boxin Shi, Yasuyuki Matsushita, Kwan-Yee K. Wong13.Title: IvaNet: Learning to jointly detect and segment objets with the help of Local Top-Down ModulesAuthors:Shihua Huang, Lu Wang14.Title: Bilateral Cyclic Constraint and Adaptive Regularization for Unsupervised Monocular Depth PredictionAuthors:Alex Wong, Byung-Woo Hong, Stefano Soatto15.Title: Appearance-Based Gaze Estimation Using Dilated-ConvolutionsAuthors:Zhaokang Chen, Bertram E. Shi16.Title: Semantic Image Synthesis with Spatially-Adaptive NormalizationAuthors:Taesung Park, Ming-Yu Liu, Ting-Chun Wang, Jun-Yan Zhu17.Title: Graph Convolutional Label Noise Cleaner: Train a Plug-and-play Action Classifier for Anomaly DetectionAuthors:Jia-Xing Zhong, Nannan Li, Weijie Kong, Shan Liu, Thomas H. Li, Ge Li18.Title: QATM: Quality-Aware Template Matching For Deep LearningAuthors:Jiaxin Cheng, Yue Wu, Wael Abd-Almageed, Premkumar Natarajan19.Title: Complex Scene Classification of PolSAR Imagery based on a Self-paced Learning ApproachAuthors:Wenshuai Chen, Shuiping Gou, Xinlin Wang, Licheng Jiao, Changzhe Jiao, Alina Zare20.Title: An End-to-End Joint Unsupervised Learning of Deep Model and Pseudo-Classes for Remote Sensing Scene RepresentationAuthors:Zhiqiang Gong, Ping Zhong, Weidong Hu, Fang Liu, Bingwei Hui21.Title: Multidimensional ground reaction forces and moments from wearable sensor accelerations via deep learningAuthors:William R. Johnson, Ajmal Mian, Mark A. Robinson, Jasper Verheul, David G. Lloyd, Jacqueline A. Alderson22.Title: AttoNets: Compact and Efficient Deep Neural Networks for the Edge via Human-Machine Collaborative DesignAuthors:Alexander Wong, Zhong Qiu Lin, Brendan Chwyl23.Title: Robust superpixels using color and contour features along linear pathAuthors:Rémi Giraud, Vinh-Thong Ta, Nicolas Papadakis24.Title: Learning-Based Animation of Clothing for Virtual Try-OnAuthors:Igor Santesteban, Miguel A. Otaduy, Dan Casas25.Title: Weighted Mean CurvatureAuthors:Yuanhao Gong, Orcun Goksel26.Title: Training recurrent neural networks robust to incomplete data: application to Alzheimer's disease progression modelingAuthors:Mostafa Mehdipour Ghazi, Mads Nielsen, Akshay Pai, M. Jorge Cardoso, Marc Modat, Sebastien Ourselin, Lauge Sørensen27.Title: SuperPatchMatch: an Algorithm for Robust Correspondences using Superpixel PatchesAuthors:Rémi Giraud, Vinh-Thong Ta, Aurélie Bugeau, Pierrick Coupé, Nicolas Papadakis28.Title: An Optimized PatchMatch for Multi-scale and Multi-feature Label FusionAuthors:Rémi Giraud, Vinh-Thong Ta, Nicolas Papadakis, José V. Manjón, D. Louis Collins, Pierrick Coupé, Alzheimer's Disease Neuroimaging Initiative29.Title: Evaluation Framework of Superpixel Methods with a Global Regularity MeasureAuthors:Rémi Giraud, Vinh-Thong Ta, Nicolas Papadakis30.Title: Proximal Splitting Networks for Image RestorationAuthors:Raied Aljadaany, Dipan K. Pal, Marios Savvides31.Title: SCALP: Superpixels with Contour Adherence using Linear PathAuthors:Rémi Giraud, Vinh-Thong Ta, Nicolas Papadakis32.Title: Robust Shape Regularity Criteria for Superpixel EvaluationAuthors:Rémi Giraud, Vinh-Thong Ta, Nicolas Papadakis33.Title: Inverse Path Tracing for Joint Material and Lighting EstimationAuthors:Dejan Azinović, Tzu-Mao Li, Anton Kaplanyan, Matthias Nießner34.Title: STNReID : Deep Convolutional Networks with Pairwise Spatial Transformer Networks for Partial Person Re-identificationAuthors:Hao Luo, Xing Fan, Chi Zhang, Wei Jiang35.Title: Bags of Tricks and A Strong Baseline for Deep Person Re-identificationAuthors:Hao Luo, Youzhi Gu, Xingyu Liao, Shenqi Lai, Wei Jiang36.Title: Spatiotemporal Filtering for Event-Based Action RecognitionAuthors:Rohan Ghosh, Anupam Gupta, Andrei Nakagawa, Alcimar Soares, Nitish Thakor37.Title: AdaGraph: Unifying Predictive and Continuous Domain Adaptation through GraphsAuthors:Massimiliano Mancini, Samuel Rota Bulò, Barbara Caputo, Elisa Ricci38.Title: Discriminating Original Region from Duplicated One in Copy-Move ForgeryAuthors:Saba Salehi, Ahmad Mahmoodi-Aznaveh39.Title: Reconstructing neuronal anatomy from whole-brain imagesAuthors:James Gornet, Kannan Umadevi Venkataraju, Arun Narasimhan, Nicholas Turner, Kisuk Lee, H. Sebastian Seung, Pavel Osten, Uygar Sümbül40.Title: GFD-SSD: Gated Fusion Double SSD for Multispectral Pedestrian DetectionAuthors:Yang Zheng, Izzat H. Izzat, Shahrzad Ziaee41.Title: Visual Query Answering by Entity-Attribute Graph Matching and ReasoningAuthors:Peixi Xiong, Huayi Zhan, Xin Wang, Baivab Sinha, Ying Wu42.Title: Domain adaptation for holistic skin detectionAuthors:Aloisio Dourado, Frederico Guth, Teofilo Emidio de Campos, Li Weigang43.Title: Hand range of motion evaluation for Rheumatoid Arthritis patientsAuthors:Luciano Walenty Xavier Cejnog, Roberto Marcondes Cesar Jr., Teofilo Emidio de Campos, Valeria Meirelles Carril Elui44.Title: Unsupervised Part-Based Disentangling of Object Shape and AppearanceAuthors:Dominik Lorenz, Leonard Bereska, Timo Milbich, Björn Ommer45.Title: Spatiotemporal Feature Learning for Event-Based VisionAuthors:Rohan Ghosh, Anupam Gupta, Siyi Tang, Alcimar Soares, Nitish Thakor46.Title: Real time backbone for semantic segmentationAuthors:Zhengeng Yang, Hongshan Yu, Qiang Fu, Wei Sun, Wenyan Jia, Mingui Sun, Zhi-Hong Mao47.Title: Robust Super-Resolution GAN, with Manifold-based and Perception LossAuthors:Uddeshya Upadhyay, Suyash P. Awate48.Title: A Cross-Season Correspondence Dataset for Robust Semantic SegmentationAuthors:Måns Larsson, Erik Stenborg, Lars Hammarstrand, Torsten Sattler, Mark Pollefeys, Fredrik Kahl49.Title: Ontology Based Global and Collective Motion Patterns for Event Classification in Basketball VideosAuthors:Lifang Wu, Zhou Yang, Jiaoyu He, Meng Jian, Yaowen Xu, Dezhong Xu, Chang Wen Chen50.Title: Fast Interactive Object Annotation with Curve-GCNAuthors:Huan Ling, Jun Gao, Amlan Kar, Wenzheng Chen, Sanja Fidler51.Title: Domain Generalization by Solving Jigsaw PuzzlesAuthors:Fabio Maria Carlucci, Antonio D'Innocente, Silvia Bucci, Barbara Caputo, Tatiana Tommasi52.Title: Learning Super-resolution 3D Segmentation of Plant Root MRI Images from Few ExamplesAuthors:Ali Oguz Uzman, Jannis Horn, Sven Behnke53.Title: Directional PointNet: 3D Environmental Classification for Wearable RoboticsAuthors:Kuangen Zhang, Jing Wang, Chenglong Fu54.Title: Visual recognition in the wild by sampling deep similarity functionsAuthors:Mikhail Usvyatsov, Konrad Schindler55.Title: Detecting GAN generated Fake Images using Co-occurrence MatricesAuthors:Lakshmanan Nataraj, Tajuddin Manhar Mohammed, B. S. Manjunath, Shivkumar Chandrasekaran, Arjuna Flenner, Jawadul H. Bappy, Amit K. Roy-Chowdhury56.Title: Generate What You Can't See - a View-dependent Image GenerationAuthors:Karol Piaskowski, Rafal Staszak, Dominik Belter57.Title: Calibration of Asynchronous Camera Networks for Object Reconstruction TasksAuthors:Amy Tabb, Henry Medeiros58.Title: Low Power Inference for On-Device Visual Recognition with a Quantization-Friendly SolutionAuthors:Chen Feng, Tao Sheng, Zhiyu Liang, Shaojie Zhuo, Xiaopeng Zhang, Liang Shen, Matthew Ardi, Alexander C. Berg, Yiran Chen, Bo Chen, Kent Gauen, Yung-Hsiang Lu59.Title: Live Reconstruction of Large-Scale Dynamic Outdoor WorldsAuthors:Ondrej Miksik, Vibhav Vineet60.Title: Patch Clustering for Representation of Histopathology ImagesAuthors:Wafa Chenni, Habib Herbi, Morteza Babaie, H.R.Tizhoosh61.Title: Deep Features for Tissue-Fold Detection in Histopathology ImagesAuthors:Morteza Babaie, H.R. Tizhoosh62.Title: Classification of dry age-related macular degeneration and diabetic macular edema from optical coherence tomography images using dictionary learningAuthors:Elahe Mousavi, Rahele Kafieh, Hossein Rabbani63.Title: Zero Shot Learning with the Isoperimetric LossAuthors:Shay Deutsch, Andrea Bertozzi, Stefano Soatto64.Title: Smart, Deep Copy-PasteAuthors:Tiziano Portenier, Qiyang Hu, Paolo Favaro, Matthias Zwicker65.Title: Atari-HEAD: Atari Human Eye-Tracking and Demonstration DatasetAuthors:Ruohan Zhang, Zhuode Liu, Lin Guan, Luxin Zhang, Mary M Hayhoe, Dana H Ballard66.Title: Multiparametric Deep Learning Tissue Signatures for a Radiological Biomarker of Breast Cancer: Preliminary ResultsAuthors:Vishwa S. Parekh, Katarzyna J. Macura, Susan Harvey, Ihab Kamel, Riham EI-Khouli, David A. Bluemke, Michael A. JacobsPapers from arxiv.org更多精彩请移步主页pic from pixels.com"}
{"content2":"人工智能范畴及深度学习主流框架，谷歌 TensorFlow，IBM Watson认知计算领域IntelligentBehavior介绍======================================大家现在对人工智能的期望太高了，2017是人工智能投资资本热的一年，但实际突破还是有限，估计过几年又会死掉一大批人工智能的创业公司，大家变得回归理性，调整到正常的认知水平上。这种革命性技术不可能有资本追求快速暴利那么快见效的，几年内都很难有重大突破. 2020年再来看估计能有理性后的下一轮突破性应用出来。======================================工业机器人，家用机器人这些只是人工智能的一个细分应用而已。图像识别，语音识别，推荐算法，NLP自然语言，广告算法，预测算法，数据挖掘，无人驾驶、医疗咨询机器人、聊天机器人，这些都属于人工智能的范畴。人工智能现在用到的基础算法是深度学习里面的神经网络算法，具体应用场景有不同的专业算法实际上很多细分领域的，差别还是很多的机器人的对运动控制算法，图像识别算法要求比较高像alphaGo，推荐算法，语音识别这些就主要靠深度学习算法和大数据训练了深度学习的开源框架现在主流的有：caffeonspark（微软）, tensorflow（google），Theano，dl4j, Torch，Kerascaffeonspark用在视觉图片识别上比较好，dl4j用在NLP上做类似问答搜索的比较多，tensorflow用在学习新的算法上比较好====================================谷歌 TensorFlow 1.0 发布，更快、更灵活、更方便开发北京时间2017年2月16日凌晨2点，Google 于在加利福尼亚州山景城举行了首届 TensorFlow Dev 峰会。在会上 Google 宣布正式发布 TensorFlow 1.0 版本，保证了 Google 的机器学习库的API稳定性。主要亮点如下：更快：TensorFlow 1.0 现在简直快到难以置信！ XLA为未来更多的性能改进奠定了基础，而现在 tensorflow.org 调整了模型以实现最大速度。很快我们将发布几个流行模型，以展示如何充分利用TensorFlow 1.0 - 包括针对Inception v3的8位 GPU的7.3x加速和针对64位 GPU的分布式Inception v3的58x加速！更灵活：TensorFlow 1.0引入了一个高级API，包含tf.layers，tf.metrics和tf.losses模块。同时还宣布增加了一个新的tf.keras模块，它与另一个流行的高级神经网络库Keras完全兼容。更便于开发：TensorFlow 1.0 保证了 Python API稳定性（查看细节），可以不破坏现有的代码便能获取新功能。现在支持window下的开发使用了。====================================IBM Watson - 最新问答 - 知乎https://www.zhihu.com/topic/19560027/newest关于IBM的Watson你必须要了解的几点|人工智能|ibm|watsonhttp://tech.sina.com.cn/it/2016-03-03/doc-ifxqafha0300264.shtml代表着IBM在认知计算领域最核心的技术Watson（沃森）的一战成名，是在2011年2月的美国问答节目《Jeopardy！》上。在这次节目中，Watson战胜了这一节目的两位冠军选手，这被和1996年同样来自IBM的“深蓝”战胜国际象棋大师卡斯帕罗夫相提并论，被认为是人工智能历史上的一个里程碑。IBM将认知系统的三项重要特质定义为理解、推理、学习。尽管IBM提出Watson和认知计算多年，但在科技领域对于认知计算和人工智能两者的关系依然很模糊。来自IBM的观点是，“火了”二十多年的人工智能概念从历史和研究角度来讲主要目的是为了让机器表现得更像人，我们称之为IntelligentBehavior。IBM也承认其认知计算从技术角度上来讲和AI有很多共性的地方，比如机器学习（MachineLearning）、深度学习（DeepLearning）等方面都很类似。现在，企业正面临着大数据带来巨大的挑战。传统计算方式会错过世界上80%的信息（非结构化数据），而认知技术能够支持组织去发现数据中隐藏的模式，挖掘出令人惊喜的新的商机，也能够加速发现新药、发现新的登月方式，甚至发现未知领域。作为全球首个认知系统，Watson是IBM认知计算系统的杰出代表。Watson具备用用自然语言进行深度问答、关系抽取、性格分析、清晰分析，权衡分析等28项能力，而且这些能力也都被转成了数字服务或API；到2016年底，Watson API的数量将达到50项。围绕IBM Watson这个基于云计算和开放标准的平台，一个认知生态系统正在被构建和分项给每一个人，目前：有36个国家、17个行业的企业都在使用认知技术超过7.7万名开发者已参与到Watson Developer Cloud平台来引导、测试和部署他们的商业构想超过350个生态系统合作伙伴及既有企业内部的创新团队，正在构建基于认知技术的应用、产品和服务，其中100家企业已将产品推向市场Watson API的月调用量高达13亿次，并仍在增长IBM Watson能够帮助企业将大数据分析、人工智能、认知体验、认知知识和计算机基础架构等认知技术融入数字应用、产品与运营。数据分析的增长是一项关键业务功能，需要借助新的工作和技能来满足需求。Watson扮演的角色是增强人们开展工作的能力，而并非取代。例如，位于墨西哥和新加坡的银行利用Watson增强员工的理财知识，为其提供以往无法承受的新服务。总之，随着科技的进步，某些工作可以被取代，但利用Watson真正的目的是补充人们的知识和技能，使其更加明智、更好地开展工作。现在来看，Watson当时的获胜的确是迈出的重要一步，但显然比较过去，现在所处的位置和发展步伐更加可圈可点，毕竟Watson的实际行业应用正在越来越多，从医疗到零售，再到制造、金融。从人工智能和认知计算对比的角度，现如今Watson在认知计算领域的成长已经超出人工智能的范畴。虽然认知计算包括人工智能的一些要素，但前者是一个更宽泛的概念。认知计算不是制造“为人们思考”的机器，而是与“增加人类智慧”有关，能够帮助人们更好地思考和做出更为全面的决定。人工智能的概念已经有二十多年了，人工智能从历史和研究角度来讲主要目的是为了让机器表现得“更像人”，我们称之为Intelligent Behavior，它只是认知计算的一个维度。IBM的认知计算从技术角度上来讲和人工智能是有很多共性的地方，比如机器学习(Machine Learning)、深度学习(Deep Learning)等方面都很类似。但是，认知计算目的并不是为了取代人，认知计算的时候除了要能够表现人和计算机的交互更加自然流畅之外，还会更多强调推理和学习，以及如何把这样的能力结合具体的商业应用、解决商业的问题。分析股票保险,帮助人类作出最佳决定回到更多人关心的金融服务类别。在刚过去的IBM Executive Summit 2016上，IBM展出了多款基于认知运算的商业方案。例如保险业可分析保单和保费,Watson除可检查各种理赔申请的可信性和应赔金额外,也可按申索次数和金额而自动计算续保时的保费,甚至利用物联网技术掌握投保人的生活和饮食习惯,为客户订制更适合的保险计划。又例如股票分析往往看股民情绪,并非只看企业的业绩。IBM Watson就可透过自然语言分析,收集该公司在网络上被报导和讨论的活跃程度,甚至能分析内文是属于正面的还是负面的,从而掌握股民情绪,协助持股的基金或企业及早作出部署。虽然现场指并未做到直接按分析结果自动买卖,但用作个人理财智能助理却绝对有其价值。==========================================Q：人工智能深度学习工作好找吗？A:现在职位很少不好找的，要求都非常高，一般要硕士毕业以上，博士以上的，要看得懂英文论文，对数学算法也有要求。目前找这方面的工作门槛很高，自己根据兴趣做些小应用可能还轻松点，但人工智能是未来的大趋势，提前学习了解是有好处的。=============================GPU 执行某些任务时的运算速度远快于 CPU，Google 云端平台官方博客宣布将从 2017 年初开始提供新的云端 GPU 服务。Google 称，复杂医学分析、金融计算、地震/地下探测、机器学习、视频渲染、转码和科学模拟等将受益于 GPU 提供的高并行计算能力。Google 云端提供的 GPU 包括了 AMD FirePro S9300 x2、NVIDIA Tesla P100 和 K80 GPU。云端 GPU 将基于分钟收费。Google 举例称，创业公司 MapD 的测试结果显示，对 12 亿纽约出租车数据集执行标准的分析查询，GPU 实例的速度 85 倍于 CPU 实例。"}
{"content2":"书签菜单126Feisky优酷网Google中国石油大学主页鲜果RSS阅读器校内网萃园站猪八戒威客网CNKI翻译助手新闻中心_腾讯网编程ARM开发者论坛周立功单片机Matlab中文论坛『Delphi园地』-源码,控件,文档,工具,免费下载,建设专业Delphi资源平台大富翁编程网站 delphibbs.com嵌入式在线 - 工程师的快乐社区编程论坛 -编程论坛ChinaUnix.net = 全球最大的Linux/Unix应用与开发者社区 = IT人的网上家园ChinaUnix.net是中国最大的Linux/Unix技术社区网站,我们还交流程序开发,数据库,存储备份,服务器技术,网络安全等技术,并提供IT人才招聘,软件下载,BLOG,IT培训等服务。华清远见嵌入式培训中心Delphi 编程天地 - Powered by Discuz!proteus仿真社区CSDN.NET - 中国最大的IT技术社区，为IT专业技术人员提供最全面的信息传播和服务平台csdnThe Computer Vision Homepage21IC中国电子网学术网站诺贝尔学术资源网 文献互助|学术资源|Ezproxy|科研交流|文献检索|VPN - powered by phpwind.net文献互助|学术资源|Ezproxy|科研交流|文献检索|VPN零点花园 文献代理,学术交流,统计年鉴,基金标书零点花园 文献代理, 学术交流, 统计年鉴, 基金标书, 学术论坛, 国内最著名的学术论坛之一 - Discuz! BoardThe Computer Vision HomepageHUNNISH的OPENCV专栏小木虫论坛 学术-科研-第一站深之JohnChen的专栏阿果资源网首页 - OpenCV China ：图像处理,计算机视觉库,Image Processing, Computer Vision自动化学报研学论坛百思论坛计算机视觉,机器视觉-中国视觉网.计算机视觉,机器视觉:光学定位,检测,影像测量,读码的专业技术媒体!机器视觉，图像处理，人工智能，机器人，模式识别—中国图像网由中国图象图形学学会所创办，是中国图像及视觉领域的专业网站。利用学会行业资源的优势，结合图像技术领域的特点，致力于为图像领域的科技人员提供一个高效的交流平台。网站包括机器视觉，图像处理，人工智能，机器人，模式识别五个部分。英语"}
{"content2":"刘艺博：首都师范大学，资源环境与旅游学院在读硕士研究生，2018年在IEEE J-STARS杂志发表论文一篇，主要研究方向：移动激光测量系统、三维激光点云处理、深度学习。内 容三维视觉是随着计算机视觉、计算机图形学、人工智能、摄影测量以及机器人等技术综合发展而来的新兴研究方向，三维视觉帮助计算机更全面地理解三维场景空间环境，主要有三维感知、位姿感知、三维建模、三维理解四个方面的研究内容。三维视觉广泛应用于智能无人系统、AR和VR、数字城市建模、体感娱乐设备等产品中，从多方面带给人们一种更智能的生活体验。其中，三维场景理解是三维视觉研究的关键内容，实现对物体的检测、识别、跟踪与检索，以及对物体或场景的分割和语义标记等任务。三维场景理解的方法也从设计点云特征提取算法发展到结合深度学习方法的点云级语义分割和三维目标检测。结合深度学习的三维视觉成为近两年研究的热点，从直接处理点云框架PointNet开始，有一系列相关的研究成果发布，在处理三维点云空间理解任务上不断取得突破，本次公开课以深度学习三维点云语义分割为主要研究对象，分析现有方法的处理思路，总结在运行过程中遇到的一些问题。https://mp.weixin.qq.com/s?__biz=MzI5MTM1MTQwMw==&mid=2247500720&idx=3&sn=e583ebd5af19503d76eae06a8e231c9f&chksm=ec137db4db64f4a27b135c70c40ea68af6c17b61f2db5bdab5ff3381add854aba81c0dc63abf&mpshare=1&scene=23&srcid=1220EFHTlgWcDR3DcFLSewve&client=tim&ADUIN=760511554&ADSESSION=1545649728&ADTAG=CLIENT.QQ.5591_.0&ADPUBNO=26867#rd"}
{"content2":"Written by joezou(邹镇洪), 2019/3/4项目内容这个作业属于课程人工智能实战 2019 - 北京航空航天大学这个作业的要求在第一次作业 介绍自己，提出课程项目建议 - 作业 - 人工智能实战 2019我在这个课程的目标是学会利用云部署机器学习模型并完成一个app这个作业在这些方面帮助我实现目标了解如何开发实用项目其他参考文献无作业正文：1. 描述你在这门课想要达到的具体目标。掌握在在云上部署机器学习模型的能力。部分地借助Azure或AWS完成一个实用app。提高团队合作coding的能力。2. 描述你学过什么计算机语言， 代码量大约是多少行。我们这次课程主要用 Python 语言， 请抓紧时间自学或巩固你的Python 知识。Python：代码20k+行，主要包括一个关于商品推荐的项目。C：代码约4k行，主要是数值分析作业。3. 如果你要和另外 4 - 5 名同学一起做一个小的 AI 项目，你有什么想法，请描述一下。请按照这个 NABCD 格式来写你的项目建议。我的构想项目是辅助视觉系统，但实现起来较为困难，暂认为以B中第2部分所述，先构建一个video to text的demo比较合适。以下是NABCD内容：1) N (Need 需求)对于视障人员而言，目前主要辅助工具为导盲犬、陪护人员等具备视力的生物，但无疑这些服务均需要一定成本和时间进行培养，且仍在许多情况下不适用，比如导盲犬无法于盲人进行充分的信息交流、陪护人员难以做到终生陪伴等。对于如夜晚等弱视环境而言，视觉辅助系统，如夜视镜，也有着重要的工程应用。于此同时，各种视觉辅助穿戴设备不断涌现，如谷歌眼镜等，说明这里有一定的市场，而在计算视觉和自然语言处理技术的加持下它会发展得更好。2) A (Approach 做法)计划分三部分达成，其中第2部分在3-5年达成内较为现实：提高微型计算机计算能力主要通过提高传感器精度和计算机实时图像处理能力达成，尤其是后者，需要解决当前CV和NLP技术在小规模机器（眼睛搭载单片机）上以低能耗高速计算的能力，以及高能小型电池的研发。但还有一个替代方案，如果日后5G通信和物联网技术发展充分，也可以借助街道摄像头获取街景数据，眼睛负责近距离图像，而数据全部上传云端，云端模型完成训练后传回本地，好处是可以通过云端的机器数量冗余来暴力实现高速实时处理，特别适用于限制范围的场景，如实景游戏。搭建video to text模型现有算法专注于文本理解、视频实体识别、视频分类等基本步骤，这些步骤的组合可以搭建一个粗糙的video to text模型，但该模型的复杂度过高，我们希望可以构建全新的end to end模型，并希望输出为“自然对画面的动态描述”而不是“机械地描述客观环境状态”，以模拟真人的对话效果。实景测试并逐步推向市场按科学实验->康复医疗->工地应用->市场娱乐的步骤逐步开发产品，从医疗入手是因为医疗领域的需求最硬且对产品功能多样化要求最低，最后进军娱乐是因为娱乐市场对产品体验要求最高，过早投入面临较高的冷场风险。3) B (Benefit 好处)无疑，当前没有任何一款设备可以帮助视障患者恢复视觉，当前视觉辅助产品也仅限于雷达和红外探测等初级感知智能，无法适用于复杂的生活实景，也无法于用户进行充分的信息交流，如浙大在2017年的视觉辅助系统。这款产品将为用户带来全新的体验，在较大程度上让盲人获取外部视觉信息。但无疑，在当先微机计算能力、电池技术、人工智能技术和移动通信水平下，5年内仍看不到实现的可能，且产品在早期必然价格不菲。目前有可能实现的仅有第2部分。但我相信这是一项重要的穿戴产品，随着计算机视觉技术和智能眼睛技术的发展终将出现，因此早期的技术累计是必要的。4) C (Competitors 竞争)产品竞争：现有的初级视觉辅助系统和AR设备（游戏领域）；未来的脑机智能设备（直接与脑电信号交流）当前技术竞争：各大CV公司的视频理解技术均为强大竞争，国内主要是字节跳动、BAT，自动驾驶厂商也构成部分竞争。5) D (Delivery 交付， Data 数据）交付：已在A中阐述，先从医疗合作入手。数据：未进行调研，目前国内仅有浙大一家开发了盲人辅助视觉系统，2017年获得600万美元融资，估计竞争较少是技术问题。4. 创建一个你的 github 账户， 学习github 的基本操作，并且关注 https://github.com/microsoft/ai-edu， 浏览相关的课程内容。（在你的博客文章列出账户链接即可）我的github5. 这是选修课，你主动选择了要学习 AI 实战技能，那老师就会让你练习基础和实战技能。请看 师生关系 这个文章，了解健身教练和健身学员之间的互相期待。"}
{"content2":"是在去年（2011）知道这个online class的，那时的域名是www.ml-class.org，只有machine learning的课程。记得去年的某天，我在某处（应该是网易公开课）看了一集机器学习的公开课。很感兴趣，于是找到了Andrew Ng教授的主页，最后发现了这么一个网上公开课。在寒假时，我把公开课里没有任何字幕的Lecture下了下来，硬着头皮看了几个章节，结果是一知半解。现在工作之余，又想好好地把这个课程学习一下，于是决定通过写学习笔记的方式来督促自己去深究。现在ml-class已经成为了coursera.org的一份子，各路大牛在这开设online class，涉及各类学科，都是些难得的资源。ML class的页面链接：https://class.coursera.org/ml-2012-002/class/index，现在的版本已经能够下载字幕了。1. Welcome | 欢迎在这一节中，主要介绍了机器学习技术的基本概况，它的魅力以及它的应用实例。虽然我们一直以来不知道机器学习为何物，但是在生活中却经常使用与这个技术相关的服务，如搜索服务、相片识别、垃圾邮件分类等等。由此可见机器学习的魅力之大。Andrew Ng说\"For me one of the reasons I'm excited is the AI dream of someday building machines as intelligent as you or me.\"这么一个AI dream，很多人会为之动心吧！机器学习在人工智能中起着重要的作用，它随着人工智能的发展而得以发展，可以认为它使得计算机获得了一种强大的全新的能力。应用机器学习技术的领域有：数据挖掘（Data Mining）、手写识别（Handwriting Recognition）、自然语言处理（Natural Language Processing, NLP）以及计算机视觉（Computer Vision）。2. What is machine learning | 机器学习是什么对于“什么是机器学习，什么不是”，虽然很难给出一个能够被普遍接受的定义，但是还是有些定义值得研究的。Arthur Samuel的定义是：Field of study that gives computers the ability to learn without being explicitly programmed. 其含义为，机器学习就是研究如何不通过明确地编写程序而实现给予计算机学习能力。Tom M. Mitchell的定义：A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E. 对于某类任务T和性能度量P，如果一个计算机程序在T上以P衡量的性能随着经验E而自我完善，那么我们称这个计算机程序在从经验E中学习。（翻译来源：《机器学习》，Tom M. Mitchell，曾华军等译，机械工业出版社）用上述第二个定义来定义手写识别学习问题：任务T：识别和分类图像中的手写文字性能度量P：分类的正确率训练经验E：已知分类的手写文字数据库将要介绍的机器学习算法主要可分为有监督学习（Supervised Learning）和无监督学习（Unsupervised Learning）。在有监督学习中，我们将用训练样本去训练计算机，相当于教它做某事，而在无监督学习中，我们放手让它自己去学习。除此之外，还有增强学习（Reinforcement Learning）和推荐系统（Recommender System）将会被介绍。这个课程还会介绍有关学习算法实际应用的建议。未完待续……"}
{"content2":"发表于2015-03-24 22:58| 11934次阅读| 来源个人博客| 26 条评论| 作者Tomasz Malisiewicz模式识别深度学习机器学习数据科学家摘要：本文我们来关注下三个非常相关的概念（深度学习、机器学习和模式识别），以及他们与2015年最热门的科技主题（机器人和人工智能）的联系，让你更好的理解计算机视觉，同时直观认识机器学习的缓慢发展过程。【编者按】本文来自CMU的博士，MIT的博士后，vision.ai的联合创始人Tomasz Malisiewicz的个人博客文章，阅读本文，你可以更好的理解计算机视觉是怎么一回事，同时对机器学习是如何随着时间缓慢发展的也有个直观的认识。以下为正文：本文我们来关注下三个非常相关的概念（深度学习、机器学习和模式识别），以及他们与2015年最热门的科技主题（机器人和人工智能）的联系。环绕四周，你会发现不缺乏一些初创的高科技公司招聘机器学习专家的岗位。而其中只有一小部分需要深度学习专家。我敢打赌，大多数初创公司都可以从最基本的数据分析中获益。那如何才能发现未来的数据科学家？你需要学习他们的思考方式。三个与“学习”高度相关的流行词汇模式识别（Pattern recognition）、机器学习（machine learning）和深度学习（deep learning）代表三种不同的思想流派。模式识别是最古老的（作为一个术语而言，可以说是很过时的）。机器学习是最基础的（当下初创公司和研究实验室的热点领域之一）。而深度学习是非常崭新和有影响力的前沿领域，我们甚至不会去思考后深度学习时代。我们可以看下图所示的谷歌趋势图。可以看到：1）机器学习就像是一个真正的冠军一样持续昂首而上；2）模式识别一开始主要是作为机器学习的代名词；3）模式识别正在慢慢没落和消亡；4）深度学习是个崭新的和快速攀升的领域。2004年至今三个概念的谷歌搜索指数（图来源于 谷歌趋势 ）1. 模式识别：智能程序的诞生模式识别是70年代和80年代非常流行的一个术语。它强调的是如何让一个计算机程序去做一些看起来很“智能”的事情，例如识别“3”这个数字。而且在融入了很多的智慧和直觉后，人们也的确构建了这样的一个程序。例如，区分“3”和“B”或者“3”和“8”。早在以前，大家也不会去关心你是怎么实现的，只要这个机器不是由人躲在盒子里面伪装的就好（）。不过，如果你的算法对图像应用了一些像滤波器、边缘检测和形态学处理等等高大上的技术后，模式识别社区肯定就会对它感兴趣。光学字符识别就是从这个社区诞生的。因此，把模式识别称为70年代，80年代和90年代初的“智能”信号处理是合适的。决策树、启发式和二次判别分析等全部诞生于这个时代。而且，在这个时代，模式识别也成为了计算机科学领域的小伙伴搞的东西，而不是电子工程。从这个时代诞生的模式识别领域最著名的书之一是由Duda & Hart执笔的“模式识别（Pattern Classification）”。对基础的研究者来说，仍然是一本不错的入门教材。不过对于里面的一些词汇就不要太纠结了，因为这本书已经有一定的年代了，词汇会有点过时。 一个字符“3”的图像被划分为16个子块。自定义规则、自定义决策，以及自定义“智能”程序在这个任务上，曾经都风靡一时（更多信息，可以查看这个 OCR 网页）小测试：计算机视觉领域最著名的会议叫CVPR，这个PR就是模式识别。你能猜出第一届CVPR会议是哪年召开的吗？2. 机器学习：从样本中学习的智能程序在90年代初，人们开始意识到一种可以更有效地构建模式识别算法的方法，那就是用数据（可以通过廉价劳动力采集获得）去替换专家（具有很多图像方面知识的人）。因此，我们搜集大量的人脸和非人脸图像，再选择一个算法，然后冲着咖啡、晒着太阳，等着计算机完成对这些图像的学习。这就是机器学习的思想。“机器学习”强调的是，在给计算机程序（或者机器）输入一些数据后，它必须做一些事情，那就是学习这些数据，而这个学习的步骤是明确的。相信我，就算计算机完成学习要耗上一天的时间，也会比你邀请你的研究伙伴来到你家然后专门手工得为这个任务设计一些分类规则要好。 典型的机器学习流程（图来源于 Natalia Konstantinova 博士的博客）。在21世纪中期，机器学习成为了计算机科学领域一个重要的研究课题，计算机科学家们开始将这些想法应用到更大范围的问题上，不再限于识别字符、识别猫和狗或者识别图像中的某个目标等等这些问题。研究人员开始将机器学习应用到机器人（强化学习，操控，行动规划，抓取）、基因数据的分析和金融市场的预测中。另外，机器学习与图论的联姻也成就了一个新的课题---图模型。每一个机器人专家都“无奈地”成为了机器学习专家，同时，机器学习也迅速成为了众人渴望的必备技能之一。然而，“机器学习”这个概念对底层算法只字未提。我们已经看到凸优化、核方法、支持向量机和Boosting算法等都有各自辉煌的时期。再加上一些人工设计的特征，那在机器学习领域，我们就有了很多的方法，很多不同的思想流派，然而，对于一个新人来说，对特征和算法的选择依然一头雾水，没有清晰的指导原则。但，值得庆幸的是，这一切即将改变……延伸阅读：要了解更多关于计算机视觉特征的知识，可以看看原作者之前的博客文章：“ 从特征描述子到深度学习：计算机视觉的20年 ”。3. 深度学习：一统江湖的架构快进到今天，我们看到的是一个夺人眼球的技术---深度学习。而在深度学习的模型中，受宠爱最多的就是被用在大规模图像识别任务中的卷积神经网络（Convolutional Neural Nets，CNN），简称ConvNets。 ConvNet框架（图来源于 Torch的教程 ）深度学习强调的是你使用的模型（例如深度卷积多层神经网络），模型中的参数通过从数据中学习获得。然而，深度学习也带来了一些其他需要考虑的问题。因为你面对的是一个高维的模型（即庞大的网络），所以你需要大量的数据（大数据）和强大的运算能力（图形处理器，GPU）才能优化这个模型。卷积被广泛用于深度学习（尤其是计算机视觉应用中），而且它的架构往往都是非浅层的。如果你要学习Deep Learning，那就得先复习下一些线性代数的基本知识，当然了，也得有编程基础。我强烈推荐Andrej Karpathy的博文：“ 神经网络的黑客指南 ”。另外，作为学习的开端，可以选择一个不用卷积操作的应用问题，然后自己实现基于CPU的反向传播算法。对于深度学习，还存在很多没有解决的问题。既没有完整的关于深度学习有效性的理论，也没有任何一本能超越机器学习实战经验的指南或者书。另外，深度学习不是万能的，它有足够的理由能日益流行，但始终无法接管整个世界。不过，只要你不断增加你的机器学习技能，你的饭碗无忧。但也不要对深度框架过于崇拜，不要害怕对这些框架进行裁剪和调整，以得到和你的学习算法能协同工作的软件框架。未来的Linux内核也许会在Caffe（一个非常流行的深度学习框架）上运行，然而，伟大的产品总是需要伟大的愿景、领域的专业知识、市场的开发，和最重要的：人类的创造力。其他相关术语1）大数据（Big-data）：大数据是个丰富的概念，例如包含大量数据的存储，数据中隐含信息的挖掘等。对企业经营来说，大数据往往可以给出一些决策的建议。对机器学习算法而言，它与大数据的结合在早几年已经出现。研究人员甚至任何一个日常开发人员都可以接触到云计算、GPU、DevOps和PaaS等等这些服务。2）人工智能（Artificial Intelligence）：人工智能应该是一个最老的术语了，同时也是最含糊的。它在过去50年里经历了几度兴衰。当你遇到一个说自己是做人工智能的人，你可以有两种选择：要么摆个嘲笑的表情，要么抽出一张纸，记录下他所说的一切。延伸阅读：原作者2011的博客：“ 计算机视觉当属人工智能 ”。结论关于机器学习的讨论在此停留（不要单纯的认为它是深度学习、机器学习或者模式识别中的一个，这三者只是强调的东西有所不同），然而，研究会继续，探索会继续。我们会继续构建更智能的软件，我们的算法也将继续学习，但我们只会开始探索那些能真正一统江湖的框架。如果你也对深度学习的实时视觉应用感兴趣，特别是那些适合机器人和家居智能化的应用，欢迎来我们的网站 vision.ai 交流。希望未来，我能说的再多一点……作者简介：Tomasz Malisiewicz，CMU的博士，MIT的博士后，vision.ai的联合创始人。关注计算机视觉，在这个领域也做了大量的工作。另外，他的博客也富含信息量和价值，感兴趣的可以浏览他个人主页和博客。原文链接： Deep Learning vs Machine Learning vs Pattern Recognition（译者/邹晓艺，CSDN 博客专家，关注机器学习、计算机视觉、人机交互和人工智能等领域  责编/钱曙光）"}
{"content2":"计算机视觉-机器学习近年部分综述计算机视觉和机器学习领域 近两年部分综述文章，欢迎推荐其他的文章，不定期更新。【2015】[1].    E.Sariyanidi, H. Gunes, A. Cavallaro, Automatic Analysisof Facial Affect: A Survey of Registration, Representation, and Recognition,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 37, NO. 6,JUNE 2015PDF:  2015-1. Automatic Analysis of Facial Affect A Survey of Registration, Representa.pdf (1.51 MB)[2].    T. Li,H. Chang, M. Wang, B.B. Ni, R.C. Hong, S.C. Yan, CrowdedScene Analysis: A Survey, IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FORVIDEO TECHNOLOGY, VOL. 25, NO. 3, MARCH 2015PDF:  2015-2. Crowded Scene Analysis A Survey.pdf (5.42 MB)[3].    Z.Zhang, Y. Xu, J. Yang, X.L. Li, D. Zhang, A Survey of Sparse Representation: Algorithms and Applications, IEEE ACCESS, date ofpublication May 6, 2015PDF:  2015-3. A Survey of Sparse Representation.pdf (4.8 MB)[4].    J.Galbally, S. Marcel, J. Fierrez, Biometric AntispoofingMethods: A Survey in Face Recognition, IEEE ACCESS, date of publicationDecember 18, 2014PDF:  2015-4. Biometric Antispoofing Methods A Survey in Face Recognition.pdf (10.43 MB)[5].    B.Tian, B. T. Morris, M. Tang, Y.Q. Liu, Y. J. Yao, C. Guo, D.Y. Shen, S.H. Tang, Hierarchical and Networked Vehicle Surveillance in ITS:A Survey, IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL.16, NO. 2, APRIL 2015PDF:  2015-5. Hierarchical and Networked Vehicle Surveillance in ITS A Survey.pdf (1.12 MB)[6].    A. Betancourt,P. Morerio, C. S. Regazzoni, and M. Rauterberg, TheEvolution of First Person Vision Methods: A Survey, IEEE TRANSACTIONS ONCIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 25, NO. 5, MAY 2015PDF:  2015-6. The Evolution of First Person Vision Methods A Survey.pdf (2.69 MB)[7].    L.Shao, F. Zhu, and X.L. Li, Transfer Learning for VisualCategorization: A Survey, IEEE TRANSACTIONS ON NEURAL NETWORKS ANDLEARNING SYSTEMS, VOL. 26, NO. 5, MAY 2015PDF:  2015-7. Transfer Learning for Visual Categorization A Survey.pdf (3.11 MB)[8].    Freek Stulp, Olivier Sigaud, Many regression algorithms, one unified model: A review, Neural Networks, June 2015PDF: 2015-NN-Many regression algorithms, one unified model A review.pdf (1.1 MB)[9].      Qixiang Ye, David Doermann，Text Detection and Recognition in Imagery: A Survey， IEEE TPAMI, July 2015PDF:  2015-PAMI-Text Detection and Recognition in Imagery A survey.pdf (1.12 MB)[10].    Salient Object Detection: A Benchmark, Ali Borji, Ming-Ming Cheng, Huaizu Jiang, Jia Li, arXiv eprint, 2015.  [pdf] [Project page]【2014】[1].    S. Fu,H. B. He, Z.G. Hou, Learning Race from Face: A Survey, IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 36, NO.12, DECEMBER 2014PDF:  2014-1. Learning Race from Face A Survey.pdf (2.55 MB)[2].    H.L. Zhou,A. Mian, L. Wei, D. Creighton, M. Hossny, and S. Nahavandi, Recent Advances on Singlemodal and Multimodal FaceRecognition: A Survey, IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS, VOL.44, NO. 6, DECEMBER 2014PDF:  2014-2. Recent Advances on Singlemodal and Multimodal Face Recognition A Survey.pdf(669.19 KB)[3]       Salient Object Detection: A Survey, Ali Borji, Ming-Ming Cheng, Huaizu Jiang, Jia Li, arXiv eprint, 2014.  [pdf] [Project page] [Bib]【2013】[1].  O. D. Lara, M.A. Labrador, A Survey on Human Activity Recognition using WearableSensors,IEEE COMMUNICATIONS SURVEYS & TUTORIALS, VOL. 15, NO. 3,THIRD QUARTER 2013PDF:  2013-1. A Survey on Human Activity Recognition using Wearable Sensors.pdf (1.65 MB)[2].  A. Sotiras, C. Davatzikos, Nikos. Paragios, Deformable Medical Image Registration: A Survey, IEEETRANSACTIONS ON MEDICAL IMAGING, VOL. 32, NO. 7, JULY 2013PDF:  2013-2.Deformable Medical Image Registration A Survey.pdf (1.3 MB)[3].  A. Alrahayfeh, M. Faezipour, Eye Tracking and Head Movement Detection: A State-of-ArtSurvey, IEEE Journal of Translational Engineering in Health andMedicine, 2013PDF:  2013-3. Eye Tracking and Head Movement Detection A State-of-Art Survey.pdf (2.22 MB)[4].  P.V.K. Borges, N. Conci, and A. Cavallaro, Video-Based Human Behavior Understanding: A Survey, IEEETRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 23, NO. 11,NOVEMBER 2013PDF:  2013-4. Video-Based Human Behavior Understanding A Survey.pdf (15.27 MB)[5] Mao Ye, Qing Zhang, Liang Wang, Jiejie Zhu, Ruigang Yang, Juergen Gall. A Survey on Human Motion Analysis from Depth Data. Time-of-Flight and Depth Imaging.  2013.PDF: survey-motionanalysis_DRAFT.pdf  (14.47 MB)整理于：http://www.valseonline.org/thread-505-1-1.html声明：如果转载了本文的版本，也请注明转载出处：http://www.cvrobot.net/computer-vision-and-machine-learning-summary-in-recent-years/如果您对该机器学习、图像视觉算法技术感兴趣，可以关注新浪微博：视觉机器人  或者加入 QQ群：101371386欢迎投稿或者推荐好的内容~~"}
{"content2":"《Windows Azure Platform 系列文章目录》微软Azure认知服务的计算机视觉API，还提供读取图片中的文字功能在海外的Windows Azure认知服务的读取图片功能，已经集成了用户界面，可以直接读取图片功能。具体的链接是：https://azure.microsoft.com/zh-cn/services/cognitive-services/computer-vision/请注意不要在生产环境中使用这个URL地址，因为服务器是在海外Azure数据中心。网络传输会有很大的延迟，谢谢！我们点击上图中的浏览按钮，上传一张图片。我这里有一个示例图片，地址是：https://leizhangstorage.blob.core.chinacloudapi.cn/azureblog/ocr.jpg我们上传成功后，就可以看到Azure计算机视觉识别结果。如下图：可以看到识别结果还是相当不错的。"}
{"content2":"原文地址：https://github.com/HarleysZhang/2019_algorithm_intern_information----------------------------------------------------------------------------------------------2019春招算法实习岗位表2019年春招近期实习岗位汇总，还有待完善，持续更新，也希望各位一起更新。 注意：大部分岗位都是面向2020年毕业的学生。公司招聘岗位工作地点截止时间投递方式内推方式是否投递信息来源文行知远感知算法实习生北京/广州未知内推邮箱：hzshuai@gmail.com简历命名格式：CVer推荐+姓名+电话号码+实习生无CVer公众号商汤科技深度学习/计算机视觉见习研究员等北京/上海/深圳/杭州未知内推邮箱：wwlwenli@163.com邮件主题：CVer推荐+商汤2019春招内推、商汤2020实习内推，简历附件命名：姓名+岗位名称+到岗时间+可见习时长无CVer公众号阿里达摩院计算机视觉算法实习生北京/杭州/深圳未知内推邮箱：zhoujing.zjh@alibaba-inc.com简历命名格式：CVer推荐+姓名+电话号码+方向(工程/算法)无CVer公众号京东AI Lab计算机视觉算法实习生北京朝阳区未知内推邮箱：wangxiaobo8@jd.com简历命名格式：CVer推荐+姓名+电话号码无CVer公众号字节跳动AI 算法岗北京2.25-5.31投递官网内推码：WWNUPDJ、NAATGNC，邮箱咨询简历进度：wangyang.wyang@bytedance.com无CVer公众号海康威视AI算法岗杭州/其他未知投递官网内推码：P4Y8AQ、587TA3、587LAC、5SYMAJ无CVer公众号图森未来感知算法实习生北京朝阳区未知内推邮箱：ostnie@foxmail.com简历命名：CVer推荐+姓名+电话号码+应聘岗位无CVer公众号平安科技计算机视觉实习生北京/深圳3.19网申内推码：PvqgIB无牛客讨论区抖音火山计算机视觉算法实习生北京/其他未知网申内推码：QURKAW2无牛客讨论区招银网络科技算法实习生深圳/杭州/成都/6.30网申内推码：9171XYCD、9211LYCD无牛客讨论区奇虎360算法实习生北京/其他3.31网申内推码：暂无无牛客讨论区爱奇艺C++开发工程师北京4月中旬网申内推码：zNG4KR无信息学院qq群蚂蚁金服算法实习生杭州/其他3-5月邮箱：xiaoxue.sxx@antfin.com邮件标题：【姓名】+【学校】+【应聘岗位】无信息学院qq群华为算法实习生成都/其他3月28网申无无信息学院qq群菜鸟网络算法实习生杭州/北京未知qq：11933043761193304376@qq.com无无oppo手机算法实习生北京未知网申内推码：15209235173无信息学院qq群创新工厂人工智能工程院机器人/自动化实习生北京未知投递邮箱：hr@chuangxin.com投递格式：姓名+岗位(实习生)+可到岗日期无我爱计算机视觉公众号旷世科技detection组算法实习生北京未知投递邮箱： zhangzhiqiang@megvii.com暂无内推无我爱计算机视觉公众号锐明技术计算机视觉算法实习生深圳/重庆未知网申暂无内推无我爱计算机视觉公众号作业帮计算机视觉方向实习生北京未知投递邮箱：wujunbin@zuoyebang.com暂无内推无我爱计算机视觉公众号爱莫科技算法实习生深圳未知投递邮箱：hr@mall-ai.com暂无内推无我爱计算机视觉阿里巴巴业务平台事业部算法-机器学习实习生杭州3月12-4月23网申暂无内推无知乎拼多多算法实习生上海/其他3月15-4月30网申暂无内推无朋友推荐小马智行计算机视觉实习生北京市海淀区未知内推邮箱：zhijie@pony.ai简历命名：姓名+岗位+CVer推荐+实习开始时间+可实习月数无CVer公众号上海媒智科技计算机视觉算法实习生上海未知内推邮箱：chenxi.huang@media-smart.cn简历格式：姓名+电话号码+全职/实习+CVer推荐无CVer公众号旷视科技算法研究员实习生北京市海淀区长期有效内推邮箱：ur@megvii.com简历命名：姓名+电话+岗位+CVer推荐(免笔试)无CVer公众号淘宝-智能制造事业部图形图像算法实习生杭州未知内推邮箱：yuanliang.syl@alibaba-inc.com简历命名格式：姓名+电话号码+应聘岗位+CVer推荐无CVer公众号360安全集团机器学习算法实习生北京未知网申内推码:nAJvUTY2(免简历筛选)无qq群图森未来感知算法实习生等北京未知内推邮箱:yifyang29@gmail.com简历命名格式：姓名+电话号码+应聘岗位无我爱计算机视觉公众号最右视频图像算法工程师实习北京未知内推邮箱:sunhaiyong2014@xiaochuankeji.cn简历命名格式：CVer推荐+姓名+电话+社招/校招/实习CVer公众号爱奇艺智能平台部-视频分析组视频理解算法实习生北京中关村未知内推邮箱:zhangyuntao@qiyi.com简历命名格式：CVer推荐+学校+研究方向+实习开始时间+毕业时间CVer公众号--------------------------------------------------------------------------------------------------------------------------------阿里算法工程师(计算机视觉方向)一面(1个小时10分钟)--->简历面自我介绍，差不多10分钟。简历项目和比赛介绍，中间有问一些项目和比赛细节，问了一些延伸和开放性问题：Adam和SGD优化器哪个更好，好在哪里，哪个使模型更加容易发散?FPN作用讲下yolov3的架构，和two-stage的mask-rcnn有什么区别代码测试，求n个数里面前k个最大的数。 我最开始说用快排，面试说还有其他方法吗，我一紧张说了个时间复杂度更大的方法，面试官提醒我可以考虑树排序，但是我没学过，回答不上来，最后面试官说你本科没学过数据结构，那就先算了。问了几个机器学习算法，KNN和SVM的细节。 这里答的不好，太久没用传统机器学习算法，很多东西都忘了，中间一个简单的几何中常见距离计算方式(欧式距离)，我忘了居然答余弦距离。问了我有什么想问的。一面总结：面试官人比较友好，自己项目细节一定要熟悉，简历上的东西最好清楚掌握，数据结构和常用算法一定要掌握，这是我的第一个面试经历，不管接下来的面试能否通过，都还是值得纪念和自省的。--------------------------------------------------------------------------------------------------------------------------------格灵深瞳算法实习生一面（29分钟）-->基础面/项目面/终面(4月28日晚更新，已挂)自我介绍，差不多３分钟自我介绍要简介些，我这里自我介绍有点太详细了钢筋检测项目介绍和目标检测框架细节大致介绍自己的工作和项目细节问了faster rcnn、Mask rcnn的细节，faster rcnn的rpn结构介绍下，rpn的loss是什么，master rcnn和faster rcnn有什么区别和改进retinanet的结构和创新点，讲一下ssd和retinanet的区别鲸鱼识别项目介绍和图像分类网络细节介绍大致介绍下鲸鱼识别项目resnet网络的创新，为什么能解决梯度消失问题，残差模块详细介绍下，为什么能解决网络层数加深带来的梯度消失和网络退化问题。你有什么想问的问了去了之后我能做什么什么时候能出面试结果面试官给我提出建议：加强论文阅读和基础原理细节掌握、加强原理的表述和表达能力面试总结１．格林深瞳实习生面试只有一面，所以项目和基础都在这一面都问了。这次面试官问的很多问题，给了我很多启发，自己项目虽然做的多，但是在很多理论和基础原理上细节功夫下的不够，论文看的不够多。２．其实自己也知道，自己在基础理论和原理方面掌握得不够深，但是由于缺乏时间，我还是没做到自己的目标，希望借这次面试反映出的自己理论缺失点，来提醒和激励自己一定要把基础理论和原理彻底掌握。３．经过阿里的面试，自己回去把更多的项目细节掌握了，这次格林深瞳面试之后一定要把基础理论和原理掌握，从图像分类网络:resnet等，到目标检测和图像分割网络：faster rcnn、mask rcnn、ssd、yolov3等彻底掌握基础原理和细节，多看相关论文和博客。"}
{"content2":"一、Mask生成概览上一节的末尾，我们已经获取了待检测图片的分类回归信息，我们将回归信息（即待检测目标的边框信息）单独提取出来，结合金字塔特征mrcnn_feature_maps，进行Mask生成工作（input_image_meta用于提取输入图片长宽，进行金字塔ROI处理，即PyramidROIAlign）。# Detections # output is [batch, num_detections, (y1, x1, y2, x2, class_id, score)] in # normalized coordinates detections = DetectionLayer(config, name=\"mrcnn_detection\")( [rpn_rois, mrcnn_class, mrcnn_bbox, input_image_meta]) # Create masks for detections detection_boxes = KL.Lambda(lambda x: x[..., :4])(detections) mrcnn_mask = build_fpn_mask_graph(detection_boxes, mrcnn_feature_maps, input_image_meta, config.MASK_POOL_SIZE, config.NUM_CLASSES, train_bn=config.TRAIN_BN)二、Mask生成函数我们在『计算机视觉』Mask-RCNN_推断网络其四：FPN和ROIAlign的耦合已经介绍过了PyramidROIAlign class的内容，def build_fpn_mask_graph(rois, feature_maps, image_meta, pool_size, num_classes, train_bn=True): \"\"\"Builds the computation graph of the mask head of Feature Pyramid Network. rois: [batch, num_rois, (y1, x1, y2, x2)] Proposal boxes in normalized coordinates. feature_maps: List of feature maps from different layers of the pyramid, [P2, P3, P4, P5]. Each has a different resolution. image_meta: [batch, (meta data)] Image details. See compose_image_meta() pool_size: The width of the square feature map generated from ROI Pooling. num_classes: number of classes, which determines the depth of the results train_bn: Boolean. Train or freeze Batch Norm layers Returns: Masks [batch, num_rois, MASK_POOL_SIZE, MASK_POOL_SIZE, NUM_CLASSES] \"\"\" # ROI Pooling # Shape: [batch, num_rois, MASK_POOL_SIZE, MASK_POOL_SIZE, channels] x = PyramidROIAlign([pool_size, pool_size], name=\"roi_align_mask\")([rois, image_meta] + feature_maps) # Conv layers x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding=\"same\"), name=\"mrcnn_mask_conv1\")(x) x = KL.TimeDistributed(BatchNorm(), name='mrcnn_mask_bn1')(x, training=train_bn) x = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding=\"same\"), name=\"mrcnn_mask_conv2\")(x) x = KL.TimeDistributed(BatchNorm(), name='mrcnn_mask_bn2')(x, training=train_bn) x = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding=\"same\"), name=\"mrcnn_mask_conv3\")(x) x = KL.TimeDistributed(BatchNorm(), name='mrcnn_mask_bn3')(x, training=train_bn) x = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding=\"same\"), name=\"mrcnn_mask_conv4\")(x) x = KL.TimeDistributed(BatchNorm(), name='mrcnn_mask_bn4')(x, training=train_bn) x = KL.Activation('relu')(x) x = KL.TimeDistributed(KL.Conv2DTranspose(256, (2, 2), strides=2, activation=\"relu\"), name=\"mrcnn_mask_deconv\")(x) x = KL.TimeDistributed(KL.Conv2D(num_classes, (1, 1), strides=1, activation=\"sigmoid\"), name=\"mrcnn_mask\")(x) return xPyramidROIAlign之后（这里会降采样一次），最终生成众多宽高等同输入ROI feat，但是是单通道的Mask输出（最后的激活函数很疯狂，relu接sigmoid，保证每个像素位置介于01之间），此时的Mask掩码输出大小为[2*MASK_POOL_SIZE, 2*MASK_POOL_SIZE]，对demo.pynb而言是28*28（源码注释给的数目是没有*2的，由于最后有一个stride为2的转置卷积，所以应该是疏忽，毕竟config的另外一个参量MASK_SHAPE值为28*28，虽然在推断网络没有使用到）。至此，推断网络的最后一个输出——对象级别原始Mask计算了出来。三、build函数返回然后，我们将整个build函数构建model所需要的输入tensor和输出tensor进行打包，创建keras模型，model = KM.Model([input_image, input_image_meta, input_anchors], [detections, mrcnn_class, mrcnn_bbox, mrcnn_mask, rpn_rois, rpn_class, rpn_bbox], name='mask_rcnn') # Add multi-GPU support. if config.GPU_COUNT > 1: from mrcnn.parallel_model import ParallelModel model = ParallelModel(model, config.GPU_COUNT) return model整理一下模型输出Tensor：# num_anchors, 每张图片上生成的锚框数量# num_rois, 每张图片上由锚框筛选出的推荐区数量，# # 由 POST_NMS_ROIS_TRAINING 或 POST_NMS_ROIS_INFERENCE 规定# num_detections, 每张图片上最终检测输出框，# # 由 DETECTION_MAX_INSTANCES 规定# detections, [batch, num_detections, (y1, x1, y2, x2, class_id, score)]# mrcnn_class, [batch, num_rois, NUM_CLASSES] classifier probabilities# mrcnn_bbox, [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))]# mrcnn_mask, [batch, num_detections, MASK_POOL_SIZE, MASK_POOL_SIZE, NUM_CLASSES]# rpn_rois, [batch, num_rois, (y1, x1, y2, x2, class_id, score)]# rpn_class, [batch, num_anchors, 2]# rpn_bbox [batch, num_anchors, 4]由于我们的GPU_COUNT为1，不是多卡训练，所以不需要GPU支持（多GPU模型后面应该会单开一节讲），直接将model返回即可。附、MaskRCNN class网络构建方法总览再次将build函数全貌贴出，分支选项按照mode='inference'选即为本系列推断网络的内容。############################################################ # MaskRCNN Class ############################################################ class MaskRCNN(): \"\"\"Encapsulates the Mask RCNN model functionality. The actual Keras model is in the keras_model property. \"\"\" def __init__(self, mode, config, model_dir): \"\"\" mode: Either \"training\" or \"inference\" config: A Sub-class of the Config class model_dir: Directory to save training logs and trained weights \"\"\" assert mode in ['training', 'inference'] self.mode = mode self.config = config self.model_dir = model_dir self.set_log_dir() self.keras_model = self.build(mode=mode, config=config) def build(self, mode, config): \"\"\"Build Mask R-CNN architecture. input_shape: The shape of the input image. mode: Either \"training\" or \"inference\". The inputs and outputs of the model differ accordingly. \"\"\" assert mode in ['training', 'inference'] # Image size must be dividable by 2 multiple times h, w = config.IMAGE_SHAPE[:2] # [1024 1024 3] if h / 2**6 != int(h / 2**6) or w / 2**6 != int(w / 2**6): # 这里就限定了下采样不会产生坐标误差 raise Exception(\"Image size must be dividable by 2 at least 6 times \" \"to avoid fractions when downscaling and upscaling.\" \"For example, use 256, 320, 384, 448, 512, ... etc. \") # Inputs input_image = KL.Input( shape=[None, None, config.IMAGE_SHAPE[2]], name=\"input_image\") input_image_meta = KL.Input(shape=[config.IMAGE_META_SIZE], name=\"input_image_meta\") if mode == \"training\": # RPN GT input_rpn_match = KL.Input( shape=[None, 1], name=\"input_rpn_match\", dtype=tf.int32) input_rpn_bbox = KL.Input( shape=[None, 4], name=\"input_rpn_bbox\", dtype=tf.float32) # Detection GT (class IDs, bounding boxes, and masks) # 1. GT Class IDs (zero padded) input_gt_class_ids = KL.Input( shape=[None], name=\"input_gt_class_ids\", dtype=tf.int32) # 2. GT Boxes in pixels (zero padded) # [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)] in image coordinates input_gt_boxes = KL.Input( shape=[None, 4], name=\"input_gt_boxes\", dtype=tf.float32) # Normalize coordinates gt_boxes = KL.Lambda(lambda x: norm_boxes_graph( x, K.shape(input_image)[1:3]))(input_gt_boxes) # 3. GT Masks (zero padded) # [batch, height, width, MAX_GT_INSTANCES] if config.USE_MINI_MASK: input_gt_masks = KL.Input( shape=[config.MINI_MASK_SHAPE[0], config.MINI_MASK_SHAPE[1], None], name=\"input_gt_masks\", dtype=bool) else: input_gt_masks = KL.Input( shape=[config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1], None], name=\"input_gt_masks\", dtype=bool) elif mode == \"inference\": # Anchors in normalized coordinates input_anchors = KL.Input(shape=[None, 4], name=\"input_anchors\") # Build the shared convolutional layers. # Bottom-up Layers # Returns a list of the last layers of each stage, 5 in total. # Don't create the thead (stage 5), so we pick the 4th item in the list. if callable(config.BACKBONE): _, C2, C3, C4, C5 = config.BACKBONE(input_image, stage5=True, train_bn=config.TRAIN_BN) else: _, C2, C3, C4, C5 = resnet_graph(input_image, config.BACKBONE, stage5=True, train_bn=config.TRAIN_BN) # Top-down Layers # TODO: add assert to varify feature map sizes match what's in config P5 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c5p5')(C5) # 256 P4 = KL.Add(name=\"fpn_p4add\")([ KL.UpSampling2D(size=(2, 2), name=\"fpn_p5upsampled\")(P5), KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c4p4')(C4)]) P3 = KL.Add(name=\"fpn_p3add\")([ KL.UpSampling2D(size=(2, 2), name=\"fpn_p4upsampled\")(P4), KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c3p3')(C3)]) P2 = KL.Add(name=\"fpn_p2add\")([ KL.UpSampling2D(size=(2, 2), name=\"fpn_p3upsampled\")(P3), KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c2p2')(C2)]) # Attach 3x3 conv to all P layers to get the final feature maps. P2 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=\"SAME\", name=\"fpn_p2\")(P2) P3 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=\"SAME\", name=\"fpn_p3\")(P3) P4 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=\"SAME\", name=\"fpn_p4\")(P4) P5 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=\"SAME\", name=\"fpn_p5\")(P5) # P6 is used for the 5th anchor scale in RPN. Generated by # subsampling from P5 with stride of 2. P6 = KL.MaxPooling2D(pool_size=(1, 1), strides=2, name=\"fpn_p6\")(P5) # Note that P6 is used in RPN, but not in the classifier heads. rpn_feature_maps = [P2, P3, P4, P5, P6] mrcnn_feature_maps = [P2, P3, P4, P5] # Anchors if mode == \"training\": anchors = self.get_anchors(config.IMAGE_SHAPE) # Duplicate across the batch dimension because Keras requires it # TODO: can this be optimized to avoid duplicating the anchors? anchors = np.broadcast_to(anchors, (config.BATCH_SIZE,) + anchors.shape) # A hack to get around Keras's bad support for constants anchors = KL.Lambda(lambda x: tf.Variable(anchors), name=\"anchors\")(input_image) else: anchors = input_anchors # RPN Model, 返回的是keras的Module对象, 注意keras中的Module对象是可call的 rpn = build_rpn_model(config.RPN_ANCHOR_STRIDE, # 1 3 256 len(config.RPN_ANCHOR_RATIOS), config.TOP_DOWN_PYRAMID_SIZE) # Loop through pyramid layers layer_outputs = [] # list of lists for p in rpn_feature_maps: layer_outputs.append(rpn([p])) # 保存各pyramid特征经过RPN之后的结果 # Concatenate layer outputs # Convert from list of lists of level outputs to list of lists # of outputs across levels. # e.g. [[a1, b1, c1], [a2, b2, c2]] => [[a1, a2], [b1, b2], [c1, c2]] output_names = [\"rpn_class_logits\", \"rpn_class\", \"rpn_bbox\"] outputs = list(zip(*layer_outputs)) # [[logits2,……6], [class2,……6], [bbox2,……6]] outputs = [KL.Concatenate(axis=1, name=n)(list(o)) for o, n in zip(outputs, output_names)] # [batch, num_anchors, 2/4] # 其中num_anchors指的是全部特征层上的anchors总数 rpn_class_logits, rpn_class, rpn_bbox = outputs # Generate proposals # Proposals are [batch, N, (y1, x1, y2, x2)] in normalized coordinates # and zero padded. # POST_NMS_ROIS_INFERENCE = 1000 # POST_NMS_ROIS_TRAINING = 2000 proposal_count = config.POST_NMS_ROIS_TRAINING if mode == \"training\"\\ else config.POST_NMS_ROIS_INFERENCE # [IMAGES_PER_GPU, num_rois, (y1, x1, y2, x2)] # IMAGES_PER_GPU取代了batch，之后说的batch都是IMAGES_PER_GPU rpn_rois = ProposalLayer( proposal_count=proposal_count, nms_threshold=config.RPN_NMS_THRESHOLD, # 0.7 name=\"ROI\", config=config)([rpn_class, rpn_bbox, anchors]) if mode == \"training\": # Class ID mask to mark class IDs supported by the dataset the image # came from. active_class_ids = KL.Lambda( lambda x: parse_image_meta_graph(x)[\"active_class_ids\"] )(input_image_meta) if not config.USE_RPN_ROIS: # Ignore predicted ROIs and use ROIs provided as an input. input_rois = KL.Input(shape=[config.POST_NMS_ROIS_TRAINING, 4], name=\"input_roi\", dtype=np.int32) # Normalize coordinates target_rois = KL.Lambda(lambda x: norm_boxes_graph( x, K.shape(input_image)[1:3]))(input_rois) else: target_rois = rpn_rois # Generate detection targets # Subsamples proposals and generates target outputs for training # Note that proposal class IDs, gt_boxes, and gt_masks are zero # padded. Equally, returned rois and targets are zero padded. rois, target_class_ids, target_bbox, target_mask =\\ DetectionTargetLayer(config, name=\"proposal_targets\")([ target_rois, input_gt_class_ids, gt_boxes, input_gt_masks]) # Network Heads # TODO: verify that this handles zero padded ROIs mrcnn_class_logits, mrcnn_class, mrcnn_bbox =\\ fpn_classifier_graph(rois, mrcnn_feature_maps, input_image_meta, config.POOL_SIZE, config.NUM_CLASSES, train_bn=config.TRAIN_BN, fc_layers_size=config.FPN_CLASSIF_FC_LAYERS_SIZE) mrcnn_mask = build_fpn_mask_graph(rois, mrcnn_feature_maps, input_image_meta, config.MASK_POOL_SIZE, config.NUM_CLASSES, train_bn=config.TRAIN_BN) # TODO: clean up (use tf.identify if necessary) output_rois = KL.Lambda(lambda x: x * 1, name=\"output_rois\")(rois) # Losses rpn_class_loss = KL.Lambda(lambda x: rpn_class_loss_graph(*x), name=\"rpn_class_loss\")( [input_rpn_match, rpn_class_logits]) rpn_bbox_loss = KL.Lambda(lambda x: rpn_bbox_loss_graph(config, *x), name=\"rpn_bbox_loss\")( [input_rpn_bbox, input_rpn_match, rpn_bbox]) class_loss = KL.Lambda(lambda x: mrcnn_class_loss_graph(*x), name=\"mrcnn_class_loss\")( [target_class_ids, mrcnn_class_logits, active_class_ids]) bbox_loss = KL.Lambda(lambda x: mrcnn_bbox_loss_graph(*x), name=\"mrcnn_bbox_loss\")( [target_bbox, target_class_ids, mrcnn_bbox]) mask_loss = KL.Lambda(lambda x: mrcnn_mask_loss_graph(*x), name=\"mrcnn_mask_loss\")( [target_mask, target_class_ids, mrcnn_mask]) # Model inputs = [input_image, input_image_meta, input_rpn_match, input_rpn_bbox, input_gt_class_ids, input_gt_boxes, input_gt_masks] if not config.USE_RPN_ROIS: inputs.append(input_rois) outputs = [rpn_class_logits, rpn_class, rpn_bbox, mrcnn_class_logits, mrcnn_class, mrcnn_bbox, mrcnn_mask, rpn_rois, output_rois, rpn_class_loss, rpn_bbox_loss, class_loss, bbox_loss, mask_loss] model = KM.Model(inputs, outputs, name='mask_rcnn') else: # Network Heads # Proposal classifier and BBox regressor heads # output shapes: # mrcnn_class_logits: [batch, num_rois, NUM_CLASSES] classifier logits (before softmax) # mrcnn_class: [batch, num_rois, NUM_CLASSES] classifier probabilities # mrcnn_bbox(deltas): [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))] mrcnn_class_logits, mrcnn_class, mrcnn_bbox =\\ fpn_classifier_graph(rpn_rois, mrcnn_feature_maps, input_image_meta, config.POOL_SIZE, # 7 config.NUM_CLASSES, train_bn=config.TRAIN_BN, fc_layers_size=config.FPN_CLASSIF_FC_LAYERS_SIZE) # Detections # output is [batch, num_detections, (y1, x1, y2, x2, class_id, score)] in # normalized coordinates detections = DetectionLayer(config, name=\"mrcnn_detection\")( [rpn_rois, mrcnn_class, mrcnn_bbox, input_image_meta]) # Create masks for detections detection_boxes = KL.Lambda(lambda x: x[..., :4])(detections) mrcnn_mask = build_fpn_mask_graph(detection_boxes, mrcnn_feature_maps, input_image_meta, config.MASK_POOL_SIZE, # 14 config.NUM_CLASSES, train_bn=config.TRAIN_BN) # num_anchors, 每张图片上生成的锚框数量 # num_rois, 每张图片上由锚框筛选出的推荐区数量， # # 由 POST_NMS_ROIS_TRAINING 或 POST_NMS_ROIS_INFERENCE 规定 # num_detections, 每张图片上最终检测输出框， # # 由 DETECTION_MAX_INSTANCES 规定 # detections, [batch, num_detections, (y1, x1, y2, x2, class_id, score)] # mrcnn_class, [batch, num_rois, NUM_CLASSES] classifier probabilities # mrcnn_bbox, [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))] # mrcnn_mask, [batch, num_detections, MASK_POOL_SIZE, MASK_POOL_SIZE, NUM_CLASSES] # rpn_rois, [batch, num_rois, (y1, x1, y2, x2, class_id, score)] # rpn_class, [batch, num_anchors, 2] # rpn_bbox [batch, num_anchors, 4] model = KM.Model([input_image, input_image_meta, input_anchors], [detections, mrcnn_class, mrcnn_bbox, mrcnn_mask, rpn_rois, rpn_class, rpn_bbox], name='mask_rcnn') # Add multi-GPU support. if config.GPU_COUNT > 1: from mrcnn.parallel_model import ParallelModel model = ParallelModel(model, config.GPU_COUNT) return model"}
{"content2":"计算机视觉会议CVPR: International Conference on Computer Vision and Pattern Recognition  (每年一次)ICCV: International Conference on Computer Vision  (奇数年)ECCV: European Conference on Computer Vision  (偶数年)三大顶会, 建议每一期的oral都要精读, poster挑自己相关的仔细看看!!!AAAI: AAAI Conference on Artificial IntelligenceICML: International Conference on Machine LearningNIPS: Annual Conference on Neural Information Processing SystemsACM MM: ACM International Conference on MultimediaIJCAI: International Joint Conference on Artificial IntelligenceACCV: Asian Conference on Computer VisionICPR: International Conference on Pattern RecognitionBMVC: British Machine Vision Conference计算机视觉期刊A类TPAMI: IEEE Trans on Pattern Analysis and Machine IntelligenceIJCV: International Journal of Computer VisionTIP: IEEE Transactions on Image ProcessingCVIU: Computer Vision and Image UnderstandingPattern Recognition"}
{"content2":"原文地址：http://tech.sina.com.cn/d/i/2015-12-12/doc-ifxmpnqi6368668.shtmlscience   Human-level concept learning through probabilistic program induction原文地址：http://science.sciencemag.org/content/350/6266/1332github：https://github.com/brendenlake/BPL本文经机器之心(微信公众号：almosthuman2014)授权转载，禁止二次转载作者：汪汪今天，《科学》杂志封面刊登了一篇重磅研究：人工智能终于能像人类一样学习，并通过了图灵测试。这个人工智能像你一样学习写字假设你从来没有见过菠萝。有一天，有人送了你一个菠萝。尽管你这辈子只见过这一个菠萝，但你只用一眼就看出了菠萝的特征。第二天，你去水果店，很快就能从一堆苹果、葡萄、柚子中认出菠萝来。你甚至还能在纸上画出菠萝的简笔画。这种‘仅从一个例子就形成概念’的能力对人来说很容易。然而，尽管人工智能近年来取得了长足的进步，但要让机器做到这一点，却难于上青天，因为目前的人工智能通常需要从大量的数据中进行学习，你得让它看成千上万张菠萝的图片才行。不过，这个事实或许从今天开始改变了。今天，一篇人工智能论文登上了《科学》杂志的封面，为人们带来了人工智能领域的一个重大突破：三名分别来自麻省理工学院、纽约大学和多伦多大学的研究者开发了一个‘只看一眼就会写字’的计算机系统。（论文Human-level concept learning through probabilistic program induction见文末，或回复12可下载pdf格式。）《科学》杂志封面只需向这个系统展示一个来自陌生文字系统的字符，它就能很快学到精髓，像人一样写出来，甚至还能写出其他类似的文字——更有甚者，它还通过了图灵测试。下面就是机器和人写出的字符。你猜哪些是机器写出来的？傻傻分不清了吧？机器的作品是1，2；2，1；1，1这三名研究者分别是纽约大学数据科学中心的Brenden Lake，多伦多大学计算机科学与统计学系的Ruslan Salakhutdinov和麻省理工学院大脑与认知科学系的Joshua Tenenbaum。他们创造的AI系统能够迅速学会写陌生的文字，从某种意义上说明它领悟到了字符的本质特征（也就是字符的整体结构），同时还能识别出非本质特征（也就是那些因书写造成的轻微变异）。三名研究者从左到右分别是：Ruslan Salakhutdinov， Brenden Lake和Joshua B。 Tenenbaum。图/Alain Decarie/The New York Times人类的概念具有极大的弹性，因此，尽管许多概念的边界十分模糊，但我们依然能进行明确的分类。这三位研究者声称，他们的系统就抓住了这种弹性。该系统能模仿人类的一个特殊天赋——从少量案例中学习新概念。它所根植的计算结构叫做概率程序（probabilistic program），还可能有助于对人类获得复杂概念的过程进行建模。Joshua B。 Tenenbaum是麻省理工学院大脑与认知科学系的教授，他说：‘目前的人工智能领域大都聚焦在对模式进行分类。但是，这种类型的智能所缺少的不是分类或识别的能力，而是思考。所以，尽管我们研究的只是手写字符，但依然大言不惭地使用“概念”这种词。因为我们能用字符来研究更加丰富和复杂的概念。我们能理解字符的来历和构件，也能理解如何用不同的方式来使用字符，并造出新的字符来。’通过‘图灵测试’这篇论文的第一作者Brenden Lake曾在Tenenbaum的团队中获得认知科学博士学位，如今他是纽约大学的博士后。根据Lake的介绍，他们在论文中分析了三个核心原则。这些原则都很通用，既可以用在字符上，也可以用在其他的概念上：组合性（compositionality）：表征是由更简单的基元构建而成。因果性（causality）：模型表征了字符生成的抽象因果结构。学会学习（learning to learn）：过去的概念知识有助于学习新的概念。研究者对这个AI系统进行了几项测试。测试1：研究者向它展示了它从未见过的书写系统（例如藏文）中的一个字符例子，并让它写出同样的字符。这里并不是让它复制出完全相同的字符，而是让它写出9个不同的变体，就像人每次手写的笔迹都不相同一样。例如，在看了一个藏文字符之后，算法挑选出该字符用不同的笔迹写出来的例子，识别出组成字符的笔画，并重画出来。测试2：研究者向系统展示了一个陌生书写系统中的几个不同的字符，并让它创造出一些相似的字符。测试3：研究者让它在一个假定的书写系统中创造出全新的字符。与此同时，人类被试也被要求做同样的事情。最后，研究者要求一组人类裁判（来自亚马逊土耳其机器人，Amazon Mechanical Turk）分辨出哪些字符是机器写的，哪些是人类写的。结果，裁判的正确率仅为52%，和随机的结果差不多。于是，机器通过了所谓的视觉图灵测试。红圈标出的是机器的作品传统的机器学习系统（比如手机上的语音识别算法）在某些分类任务上的表现很好，但它们首先需要大量的数据集来进行训练。相比之下，人类只需要少量的例子就能抓住某个概念的精髓。这种‘一次性学习’正是研究者希望他们的系统能模拟的能力。学会如何学习三位研究者采用的方法是‘贝叶斯程序学习’（BPL，Bayesian Program Learning），能让计算机系统对人类认知进行很好的模拟。传统的机器学习方法需要大量的数据来训练，而这种方法只需要一个粗略的模型，然后使用推理算法来分析案例，补充模型的细节。在这篇论文中，研究者的模型只规定了字符由笔画组成，笔画由抬高笔触来区分，而笔画又由更小的子笔画组成，子笔画用笔尖速度为零的点来区分。有了这个初始模型之后，研究者向AI展现了人类手写文字的方式，包括笔画顺序等，让系统学习连续的笔画和子笔画之间的统计关系，以及单个笔画所能容忍的变异程度。这个系统从未在它所分析的书写系统上进行过任何训练，它只是推理出了人类写字的一般规律。Tenenbaum说：‘每个星期，我们似乎都能读到机器在人脸识别、语音识别方面与人类旗鼓相当的新闻。但是，对我这种研究心智的科学家来说，机器学习和人类学习之间的鸿沟是巨大的。我们希望弥合这个鸿沟，这是我们的长期目标。’各方评价剑桥大学的信息工程教授Zoubin Ghahramani说：‘我认为这对人工智能、认知科学和机器学习是一个重大的贡献。深度学习目前已取得了重要的成功，这篇论文非常清醒地表明了深度学习的局限性，因为深度学习需要大量的数据，并且在这篇论文所描述的任务上表现很差。这篇论文也展现了实现类人机器学习的重要方法。’也有一些人对‘人工智能超越人脑’这种说法持谨慎态度。艾伦人工智能研究所的Oren Etzioni说：‘我对“超人的表现”这种说法非常谨慎。当然，这个算法确实超过一般人的表现，除了达斯汀·霍夫曼。’（指霍夫曼主演的《雨人》电影。）与深度学习优势互补多伦多大学和谷歌的人工智能先驱Geoffrey Hinton说这个研究‘令人印象非常深刻’。他说，这个模型能通过视觉图灵测试，这很重要，‘是一个不错的成就。’Hinton是深度学习的奠基者。深度学习近年来取得了举世瞩目的成就，被广泛应用在许多领域，例如语音翻译、图像识别等，还用在谷歌的图像搜索和Facebook的人脸识别上，获得了巨量的数据以供学习。然而，这篇新论文说‘贝叶斯程序学习’在某些方面比深度学习的表现更好。三位作者和Hinton都礼貌地表明，这两种方法在不同的任务上各领风骚，假如能彼此借鉴，一定能互相弥补。如果能建出一个混血系统，说不定能有更大的提升。在数据量巨大但较混乱的情况下，深度学习能发挥优势；而在数据量少而清晰的情况下，贝叶斯学习占领上风。Hinton说，这篇论文最令人兴奋的成果或许是能让那些宣称智能计算机系统的学习方式与人类完全不同的批评者闭嘴，因为他们的主要论据正是计算机不能从单个例子中形成概念。未来在未来，这种机器学习的技术能够完成很多任务，例如读懂手语、提升语音识别软件的性能等。运用这种方法，或许只用向计算机展示一张人脸照片，它就能从任何角度识别出这个人。它甚至有可能用来制定军事行动计划。当然，尽管这个成果很重要，但它对人工智能领域来说只是一个小小的起点，不代表未来的机器学习都必须采用这种方法。正如它颠覆了‘计算机如何理解概念’这个课题一样，在这个日新月异的领域中，极有可能下个月就出现一种新方法，将它甩在后面飞扬的尘土中。"}
{"content2":"智能先锋从左至右，依次：Yann LeCun、Geoffrey Hinton、Yoshua Bengio、Andrew NgGeoffrey Hinton：是人工智能领域的鼻祖，他发表了许多让神经网络得以应用的论文，激活了整个人工智能领域。他还培养了许多人工智能的大家。比如LeCun就是他的博士后。Yann LeCun：他改进了卷积神经网路算法，使卷积神经网络具有了工程应用价值，现在卷积神经网络依旧是计算机视觉领域最有效的模型之一。Yoshua Bengio：他推动了循环神经网路算法的发展，使循环神经网络得到工程应用，用循环神经网络解决了自然语言处理中的问题。Andrew Ng：他是人工智能和机器学习领域国际上最权威的学者之一。吴恩达也是在线教育平台Coursera的联合创始人。什么是机器学习机器学习是一种统计学方法，计算机利用已有数据得出某种模型，再利用此模型预测结果。机器学习三要素：数据、算法、算力 。什么是深度学习深层次神经网络，源于对生物脑神经元结构的研究。生物学中的神经元：下图左侧有许多支流汇总在一起，生物学中称这些支流叫做树突。树突具有接受刺激并将冲动传入细胞体的功能，是神经元的输入。这些树突汇总于细胞核又沿着一条轴突输出。轴突的主要功能是将神经冲动由胞体传至其他神经元，是神经元的输出。人脑便是由860亿个这样的神经元组成，所有的思维意识，都以它为基本单元，连接成网络实现的。计算机中的神经元模型：1943年，心理学家McCulloch和数学家Pitts参考了生物神经元的结构，发表了抽象的神经元模型MP。神经元模型是一个包含输入，输出与计算功能的模型。输入可以类比为神经元的树突，输出可以类比为神经元的轴突，计算可以类比为细胞核。人工智能 Vs 机器学习 Vs 深度学习人工智能，就是用机器模拟人的意识和思维。机器学习，则是实现人工智能的一种方法，是人工智能的子集。深度学习，就是深层次神经网络，是机器学习的一种实现方法，是机器学习的子集。神经网络的发展历史（三起两落）第一次兴起：1958年，人们把两层神经元首尾相接，组成单层神经网络，称做感知机。感知机成了首个可以学习的人工神经网络。引发了神经网络研究的第一次兴起。第一次寒冬：1969年，这个领域的权威学者 Minsky 用数学公式证明了只有单层神经网络的感知机无法对异或逻辑进行分类，Minsky 还指出要想解决异或可分问题，需要把单层神经网络扩展到两层或者以上。然而在那个年代计算机的运算能力，是无法支撑这种运算量的。只有一层计算单元的感知机，暴露出他的天然缺陷，使得神经网络研究进入了第一个寒冬。第二次兴起：1986年，Hinton等人提出了反向传播方法，有效解决了两层神经网络的算力问题。引发了神经网络研究的第二次兴起。第二次寒冬：1995年，支持向量机诞生。支持向量机可以免去神经网络需要调节参数的不足，还避免了神经网络中局部最优的问题。一举击败神经网络，成为当时人工智能领域的主流算法，使得神经网络进入了他的第二个冬季。第三次兴起：2006年，深层次神经网络出现，2012年，卷积神经网络在图像识别领域中的惊人表现，又引发了神经网络研究的再一次兴起。"}
{"content2":"今日CS.CV计算机视觉论文速览Thu, 28 Mar 2019Totally 32 papersDaily Computer Vision Papers1.Title: GAN-based Pose-aware Regulation for Video-based Person Re-identificationAuthors:Alessandro Borgia, Yang Hua, Elyor Kodirov, Neil M. Robertson2.Title: Privacy Protection in Street-View Panoramas using Depth and Multi-View ImageryAuthors:Ries Uittenbogaard, Clint Sebastian, Julien Vijverberg, Bas Boom, Dariu M. Gavrila, Peter H.N. de With3.Title: Accurate Monocular 3D Object Detection via Color-Embedded 3D Reconstruction for Autonomous DrivingAuthors:Xinzhu Ma, Zhihui Wang, Haojie Li, Wanli Ouyang, Pengbo Zhang4.Title: Social Behavioral Phenotyping of Drosophila with a2D-3D Hybrid CNN FrameworkAuthors:Ziping Jiang, Paul L. Chazot, M. Emre Celebi, Danny Crookes, Richard Jiang5.Title: Self-Supervised Learning via Conditional Motion PropagationAuthors:Xiaohang Zhan, Xingang Pan, Ziwei Liu, Dahua Lin, Chen Change Loy6.Title: Spatially-Adaptive Residual Networks for Efficient Image and Video DeblurringAuthors:Kuldeep Purohit, A. N. Rajagopalan7.Title: Diversity with Cooperation: Ensemble Methods for Few-Shot ClassificationAuthors:Nikita Dvornik (Thoth), Cordelia Schmid (Thoth), Julien Mairal (Thoth)8.Title: Speed Invariant Time Surface for Learning to Detect Corner Points with Event-Based CamerasAuthors:Jacques Manderscheid, Amos Sironi, Nicolas Bourdis, Davide Migliore, Vincent Lepetit9.Title: Rethinking the Evaluation of Video SummariesAuthors:Mayu Otani, Yuta Nakashima, Esa Rahtu, Janne Heikkilä10.Title: Dense Intrinsic Appearance Flow for Human Pose TransferAuthors:Yining Li, Chen Huang, Chen Change Loy11.Title: A novel framework for automatic detection of Autism: A study on Corpus Callosum and Intracranial Brain VolumeAuthors:Hamza Sharif, Rizwan Ahmed Khan12.Title: Linkage Based Face Clustering via Graph Convolution NetworkAuthors:Zhongdao Wang, Liang Zheng, Yali Li, Shengjin Wang13.Title: 3D Face Mask Presentation Attack Detection Based on Intrinsic Image AnalysisAuthors:Lei Li, Zhaoqiang Xia, Xiaoyue Jiang, Yupeng Ma, Fabio Roli, Xiaoyi Feng14.Title: Image search using multilingual texts: a cross-modal learning approach between image and text Maxime Portaz Qwant ResearchAuthors:Maxime Portaz, Hicham Randrianarivo (CEDRIC), Adrien Nivaggioli, Estelle Maudet, Christophe Servan (LIUM), Sylvain Peyronnet (ELM)15.Title: Deformable kernel networks for guided depth map upsamplingAuthors:Beomjun Kim, Jean Ponce, Bumsub Ham16.Title: Small Data Challenges in Big Data Era: A Survey of Recent Progress on Unsupervised and Semi-Supervised MethodsAuthors:Guo-Jun Qi, Jiebo Luo17.Title: Auto-Embedding Generative Adversarial Networks for High Resolution Image SynthesisAuthors:Yong Guo, Qi Chen, Jian Chen, Qingyao Wu, Qinfeng Shi, Mingkui Tan18.Title: W-Net: Reinforced U-Net for Density Map EstimationAuthors:Varun Kannadi Valloli, Kinal Mehta19.Title: Mimicking the In-Camera Color Pipeline for Camera-Aware Object CompositingAuthors:Jun Gao, Xiao Li, Liwei Wang, Sanja Fidler, Stephen Lin20.Title: Training Quantized Network with Auxiliary Gradient ModuleAuthors:Bohan Zhuang, Lingqiao Liu, Mingkui Tan, Chunhua Shen, Ian Reid21.Title: Deep Co-Training for Semi-Supervised Image SegmentationAuthors:Jizong Peng, Guillermo Estradab, Marco Pedersoli, Christian Desrosiers22.Title: BAE-NET: Branched Autoencoder for Shape Co-SegmentationAuthors:Zhiqin Chen, Kangxue Yin, Matthew Fisher, Siddhartha Chaudhuri, Hao Zhang23.Title: Colorectal cancer diagnosis from histology images: A comparative studyAuthors:Junaid Malik, Serkan Kiranyaz, Suchitra Kunhoth, Turker Ince, Somaya Al-Maadeed, Ridha Hamila, Moncef Gabbouj24.Title: Information Maximizing Visual Question GenerationAuthors:Ranjay Krishna, Michael Bernstein, Li Fei-Fei25.Title: Improved Generalization of Heading Direction Estimation for Aerial Filming Using Semi-supervised RegressionAuthors:Wenshan Wang, Aayush Ahuja, Yanfu Zhang, Rogerio Bonatti, Sebastian Scherer26.Title: Pix2Vex: Image-to-Geometry Reconstruction using a Smooth Differentiable RendererAuthors:Felix Petersen, Amit H. Bermano, Oliver Deussen, Daniel Cohen-Or27.Title: AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree SearchAuthors:Linnan Wang, Yiyang Zhao, Yuu Jinnai, Yuandong Tian, Rodrigo Fonseca28.Title: Text Processing Like Humans Do: Visually Attacking and Shielding NLP SystemsAuthors:Steffen Eger, Gözde Gül Şahin, Andreas Rücklé, Ji-Ung Lee, Claudia Schulz, Mohsen Mesgar, Krishnkant Swarnkar, Edwin Simpson, Iryna Gurevych29.Title: Graph Convolution for Multimodal Information Extraction from Visually Rich DocumentsAuthors:Xiaojing Liu, Feiyu Gao, Qiong Zhang, Huasha Zhao30.Title: TossingBot: Learning to Throw Arbitrary Objects with Residual PhysicsAuthors:Andy Zeng, Shuran Song, Johnny Lee, Alberto Rodriguez, Thomas Funkhouser31.Title: Neural-networks for geophysicists and their application to seismic data interpretationAuthors:Bas Peters, Eldad Haber, Justin Granek32.Title: SUSI: Supervised Self-Organizing Maps for Regression and Classification in PythonAuthors:Felix M. Riese, Sina KellerPapers from arxiv.org更多精彩请移步主页pic from pixels.com"}
{"content2":"相关书籍PDF资料下载见：https://pan.baidu.com/s/1sk0uHSHQKG1Jrw47TCZULw 提取码：zja9最全AI书籍资料整理推荐：《21个项目玩转深度学习：基于TensorFlow的实践详解》PDF+源代码《153分钟学会R》《AI·未来》《ggplot2：数据分析与图形艺术》《Hadoop权威指南(第2版)》《Hadoop权威指南（第四版）》中文PDF+英文PDF+源代码《Keras快速上手：基于Python的深度学习实战》《OpenCV3计算机视觉 Python语言实现(第二版)》《OpenCV官方教程中文版（For Python）》《Python3网络爬虫开发实战》中文PDF+源代码《Python编程从入门到实践》（高清中文版PDF+高清英文版PDF+源代码）《Python编程快速上手：让繁琐工作自动化》【高清中文版PDF+高清英文版PDF+源代码】《Python编程入门(第3版)》中文PDF+英文PDF《Python程序设计与算法基础教程》《Python核心编程(第三版)》（高清中文版PDF+高清英文版PDF+源代码）《Python机器学习》高清英文版PDF+中文版PDF+源代码及数据集《Python机器学习基础教程》高清中文版PDF+高清英文版PDF+源代码《Python机器学习及实践从零开始通往Kaggle竞赛之路》《Python机器学习经典实例》(高清中文版PDF+高清英文版PDF+源代码)《Python机器学习实践指南》(高清中文版PDF+高清英文版PDF+源代码)《Python机器学习—预测分析核心算法》高清中文版PDF+高清英文版PDF+源代码《Python基础教程（第3版）》（高清中文版PDF+高清英文版PDF+源代码）《Python金融大数据分析》中文版PDF+英文版PDF+源代码《Python金融实战》中文版PDF+英文版PDF+源代码《Python进行自然语言处理》《Python深度学习》高清中文版pdf+高清英文版pdf+源代码《Python神经网络编程》中文版PDF+英文版PDF+源代码《Python数据处理》（高清中文版PDF+高清英文版PDF+源代码）《Python数据分析基础》高清中文PDF+高清英文PDF+源代码《Python数据分析基础教程：NumPy学习指南(第2版)》高清中文PDF+英文PDF+源代码《Python数据科学手册》高清中文版PDF+高清英文版PDF+源代码《Python数据可视化编程实战》中文版PDF+英文版PDF+源代码《Python数据挖掘入门与实践》高清中文版+高清英文版+源代码《Python网络数据采集》高清中文版PDF+高清英文版PDF+源代码《Python学习手册(第4版)》高清中文PDF+高清英文PDF+源代码《Python游戏编程快速上手(第3版)》高清中文版PDF+高清英文版PDF+源代码《Python语言及其应用》高清中文版PDF+高清英文版PDF+源代码《R数据科学》高清中文版PDF+高清英文版PDF+源代码《R数据可视化手册》高清英文版PDF+中文版PDF+源代码《R语言编程艺术》中文版PDF+英文版PDF+源代码《R语言实战（第2版）》高清中文版PDF+高清英文版PDF+源代码《R语言与网站分析》《TensorFlow 官方文档中文版》《TensorFlow机器学习实战指南》中文版PDF+英文版PDF+源代码《TensorFlow机器学习项目实战》中文PDF+英文PDF+源代码《TensorFlow技术解析与实战》高清中文PDF+源代码《TensorFlow实战》中文版PDF+源代码《TensorFlow实战Google深度学习框架(第2版)》中文版PDF和源代码《白话大数据与机器学习》PDF+《图解机器学习》PDF《白话深度学习与TensorFlow》中文版PDF《贝叶斯方法概率编程与贝叶斯推断》中文版PDF+英文版PDF+源代码《贝叶斯思维：统计建模的Python学习法》高清中文版PDF+高清英文版PDF+源代码《从Excel到Python数据分析进阶指南》高清中文版PDF《大数据之路：阿里巴巴大数据实践》《大数据治理》《大数据治理与服务》《动手学深度学习》高清PDF《父与子的编程之旅python【第二版】》高清中文版PDF+高清英文版PDF+源代码《机器学习：实用案例解析》中文版PDF+英文版PDF+源代码《机器学习》《机器学习》《机器学习导论》《机器学习导论》《机器学习基础教程》中文PDF+英文PDF《机器学习实践应用》高清PDF+源代码《机器学习实战：基于Scikit-Learn和TensorFlow》高清中英文PDF+源代码《机器学习实战》(高清中文版PDF+高清英文版PDF+源代码)《机器学习系统设计》高清中文版+高清英文版+源代码《机器学习与数据科学(基于R的统计学习方法)》高清中文PDF+源代码《机器学习之路》《机器之心》《教孩子学编程Python语言版》中文版PDF+英文版PDF+源代码《解析卷积神经网络深度学习实践手册》《解析深度学习语音识别实践》《精通Python爬虫框架Scrapy》中文PDF+英文PDF+源代码《精通Python自然语言处理》高清中文版PDF+高清英文版PDF+源代码《矩阵分析与应用（第二版）张贤达》PDF《浪潮之巅》《利用Python进行数据分析(第二版)》高清中文版PDF+高清英文版PDF+源代码《零基础入门学习Python》电子书PDF+笔记+课后题及答案《零起点Python大数据与量化交易》中文PDF+源代码《流畅的Python》高清中文版PDF_mobi+高清英文版PDF_mobi+源代码大全套《面向机器智能的TensorFlow实践》中文版PDF+英文版PDF+源代码《模式识别与机器学习》《趣学Python编程》中文PDF+英文PDF+源代码《人工智能：国家人工智能战略行动抓手》《人工智能：智能系统指南》《人工智能》《人工智能基础》《人工智能时代的教育革命》《人工智能一种现代方法》《人类简史》《深度学习、优化与识别》PDF+《深度学习原理与TensorFlow实践》PDF《深度学习：一起玩转TensorLayer》《深度学习：原理与应用实践》中文版PDF《深度学习精要（基于R语言）》高清中文版PDF+高清英文版PDF+源代码《深度学习入门：基于Python的理论与实现》高清中文版PDF+源代码《深度学习-伊恩·古德费洛》【中文版和英文版】【高清完整版PDF】《深度学习与计算机视觉》《深度学习之PyTorch实战计算机视觉》PDF《深度学习之TensorFlow：入门、原理与进阶实战》PDF+源代码《深入浅出强化学习：原理入门》《深入浅出深度学习：原理剖析与python实践》PDF+源代码《神经网络与机器学习（第3版）》高清英文PDF+中文PDF《神经网络与深度学习（美）MichaelNielsen著》中文版PDF+英文版PDF+源代码《生物信息学机器学习方法》《失控》《时间简史》《时间序列分析及应用：R语言（原书第2版）》《实用机器学习(孙亮 著)》PDF+源代码《数据科学实战手册(R+Python)》中文PDF+英文PDF+源代码《数据科学中的R语言》中文PDF+源代码《数据挖掘实用机器学习技术（中文第二版）》《数据挖掘与R语言》《数理统计与数据分析 原书第3版》《统计手册：金融中的统计方法》《统计学习方法》《统计学习基础 数据挖掘、推理与预测》《图说D3数据可视化利器从入门到进阶》《未来简史》《吴恩达深度学习课程笔记》《一天搞懂深度学习》《智能革命》《智能时代》阶段一、人工智能基础 －　高等数学必知必会本阶段主要从数据分析、概率论和线性代数及矩阵和凸优化这四大块讲解基础，旨在训练大家逻辑能力，分析能力。拥有良好的数学基础，有利于大家在后续课程的学习中更好的理解机器学习和深度学习的相关算法内容。同时对于AI研究尤为重要，例如人工智能中的智能很大一部分依托“概率论”实现的。一、数据分析1）常数e2）导数3）梯度4）Taylor5）gini系数6）信息熵与组合数7）梯度下降8）牛顿法二、概率论1）微积分与逼近论2）极限、微分、积分基本概念3）利用逼近的思想理解微分，利用积分的方式理解概率4）概率论基础5）古典模型6）常见概率分布7）大数定理和中心极限定理8）协方差(矩阵)和相关系数9）最大似然估计和最大后验估计三、线性代数及矩阵1）线性空间及线性变换2）矩阵的基本概念3）状态转移矩阵4）特征向量5）矩阵的相关乘法6）矩阵的QR分解7）对称矩阵、正交矩阵、正定矩阵8）矩阵的SVD分解9）矩阵的求导10）矩阵映射/投影四、凸优化1）凸优化基本概念2）凸集3）凸函数4）凸优化问题标准形式5）凸优化之Lagerange对偶化6）凸优化之牛顿法、梯度下降法求解阶段二、人工智能提升 － Python高级应用随着AI时代的到来以及其日益蓬勃的发展，Python作为AI时代的头牌语言地位基本确定，机器学习是着实令人兴奋，但其复杂度及难度较大，通常会涉及组装工作流和管道、设置数据源及内部和云部署之间的分流而有了Python库后，可帮助加快数据管道，且Python库也在不断更新发布中，所以本阶段旨在为大家学习后续的机器学习减负。一、容器1）列表:list2）元组:tuple3）字典: dict4）数组: Array5）切片6）列表推导式7）浅拷贝和深拷贝二、函数1）lambda表达式2）递归函数及尾递归优化3）常用内置函数/高阶函数4）项目案例：约瑟夫环问题三、常用库1）时间库2）并发库3）科学计算库4）Matplotlib可视化绘图库5）锁和线程6）多线程编程阶段三、人工智能实用 － 机器学习篇机器学习利用算法去分析数据、学习数据，随后对现实世界情况作出判断和预测。因此，与预先编写好、只能按照特定逻辑去执行指令的软件不同，机器实际上是在用大量数据和算法去“自我训练”，从而学会如何完成一项任务。所以本阶段主要从机器学习概述、数据清洗和特征选择、回归算法、决策树、随机森林和提升算法、SVM、聚类算、EM算法、贝叶斯算法、隐马尔科夫模型、LDA主题模型等方面讲解一些机器学习的相关算法以及这些算法的优化过程，这些算法也就是监督算法或者无监督算法。一、机器学习1）机器学习概述二、监督学习1）逻辑回归2）softmax分类3）条件随机场4）支持向量机svm5）决策树6）随机森林7）GBDT8）集成学习三、非监督学习1）高斯混合模型2）聚类3）PCA4）密度估计5）LSI6）LDA7）双聚类四、数据处理与模型调优1）特征提取2）数据预处理3）数据降维4）模型参数调优5）模型持久化6）模型可视化阶段四、人工智能实用 － 数据挖掘篇本阶段主要通过音乐文件分类和金融反欺诈模型训练等项目，帮助大家对于上阶段的机器学习做更深入的巩固，为后续深度学习及数据挖掘提供项目支撑。项目一：百度音乐系统文件分类音乐推荐系统就是利用音乐网站上的音乐信息，向用户提供音乐信息或者建议，帮助用户决定应该听什么歌曲。而个人化推荐则是基于音乐信息及用户的兴趣特征、听歌历史行为，向用户推荐用户可能会感兴趣的音乐或者歌手。推荐算法主要分为以下几种：基于内容的推荐、协同过滤推荐、基于关联规则推荐、基于效用推荐、基于知识推荐等；推荐系统常用于各个互联网行业中，比如音乐、电商、旅游、金融等。项目二：千万级P2P金融系统反欺诈模型训练目前比较火的互联网金融领域，实质是小额信贷，小额信贷风险管理，本质上是事前对风险的主动把控，尽可能预测和防范可能出现的风险。本项目应用GBDT、Randomforest等机器学习算法做信贷反欺诈模型，通过数据挖掘技术，机器学习模型对用户进行模型化综合度量，确定一个合理的风险范围，使风险和盈利达到一个平衡的状态。阶段五、人工智能前沿 －　深度学习篇深度学习是实现机器学习的技术，同时深度学习也带来了机器学习的许多实际应用，拓展了AI的使用领域，本阶段主要从TensorFlow、BP神经网络、深度学习概述、CNN卷积神经网络、递归神经网、自动编码机，序列到序列网络、生成对抗网络，孪生网络，小样本学习技术等方面讲解深度学习相关算法以，掌握深度学习前沿技术，并根据不同项目选择不同的技术解决方案。针对公司样本不足，采用小样本技术和深度学习技术结合，是项目落地的解决方案。1）TensorFlow基本应用2）BP神经网络3）深度学习概述4）卷积神经网络(CNN)5）图像分类(vgg,resnet)6）目标检测(rcnn,fast-rcnn,faster-rcnn,ssd)7）递归神经网络(RNN)8）lstm,bi-lstm,多层LSTM9）无监督学习之AutoEncoder自动编码器10）Seq2Seq11）Seq2Seq with Attension12）生成对抗网络13）irgan14）finetune及迁移学习15）孪生网络16）小样本学习阶段六、人工智能进阶 － 自然语言处理篇自然语言处理（NLP）是计算机科学领域与人工智能领域中的一个重要方向。它已成为人工智能的核心领域。自然语言处理解决的是“让机器可以理解自然语言”这一到目前为止都还只是人类独有的特权，被誉为人工智能皇冠上的明珠，被广泛应用。本阶段从NLP的字、词和句子全方位多角度的学习NLP，作为NLP的基础核心技术，对NLP为核心的项目，如聊天机器人，合理用药系统，写诗机器人和知识图谱等提供底层技术。通过学习NLP和深度学习技术，掌握NLP具有代表性的前沿技术。1）词（分词，词性标注）代码实战2）词（深度学习之词向量，字向量）代码实战3）词（深度学习之实体识别和关系抽取）代码实战4）词（关键词提取，无用词过滤）代码实战5）句（句法分析，语义分析）代码实战6）句（自然语言理解,一阶逻辑）代码实战7）句（深度学习之文本相似度）代码实战阶段七、人工智能进阶 － 图像处理篇数字图像处理(Digital Image Processing)是通过计算机对图像进行去除噪声、增强、复原、分割、提取特征等处理的方法和技术。广泛的应用于农牧业、林业、环境、军事、工业和医学等方面，是人工智能和深度学习的重要研究方向。深度学习作为当前机器学习领域最热门的技术之一，已经在图像处理领域获得了应用，并且展现出巨大的前景。本阶段学习了数字图像的基本数据结构和处理技术，到前沿的深度学习处理方法。掌握前沿的ResNet,SSD,Faster RCNN等深度学习模型，对图像分类，目标检测和模式识别等图像处理主要领域达到先进水平。实际工作中很多项目都可以转化为本课程的所学的知识去解决，如行人检测，人脸识别和数字识别。一、图像基础图像读，写，保存，画图（线，圆，多边形，添加文字）二、图像操作及算数运算图像像素读取，算数运算，ROI区域提取三、图像颜色空间运算图像颜色空间相互转化四、图像几何变换平移，旋转，仿射变换，透视变换等五、图像形态学腐蚀，膨胀，开/闭运算等六、图像轮廓长宽，面积，周长，外接圆，方向，平均颜色，层次轮廓等七、图像统计学图像直方图八、图像滤波高斯滤波，均值滤波，双边滤波，拉普拉斯滤波等阶段八、人工智能终极实战 － 项目应用本阶段重点以项目为导向，通过公安系统人脸识别、图像识别以及图像检索、今日头条CTR广告点击量预估、序列分析系统、聊天机器人等多个项目的讲解，结合实际来进行AI的综合运用。阶段九、人工智能实战 － 企业项目实战课程一、基于Python数据分析与机器学习案例实战教程课程风格通俗易懂，基于真实数据集案例实战。主体课程分成三个大模块(1)python数据分析，(2)机器学习经典算法原理详解,(3)十大经典案例实战。通过python数据科学库numpy,pandas,matplot结合机器学习库scikit-learn完成一些列的机器学习案例。算法课程注重于原理推导与流程解释，结合实例通俗讲解复杂的机器学习算法，并以实战为主，所有课时都结合代码演示。算法与项目相结合，选择经典kaggle项目，从数据预处理开始一步步代码实战带大家快速入门机器学习。旨在帮助同学们快速上手如何使用python库来完整机器学习案例。选择经典案例基于真实数据集，从数据预处理开始到建立机器学习模型以及效果评估，完整的讲解如何使用python及其常用库进行数据的分析和模型的建立。对于每一个面对的挑战，分析解决问题思路以及如何构造合适的模型并且给出合适评估方法。在每一个案例中，同学们可以快速掌握如何使用pandas进行数据的预处理和分析，使用matplotlib进行可视化的展示以及基于scikit-learn库的机器学习模型的建立。1）Python数据分析与机器学习实战课程简介2）Python快速入门3）Python科学计算库Numpy4）Python数据分析处理库Pandas5）Python可视化库Matplotlib6）回归算法7）模型评估8）K近邻算法9）决策树与随机森林算法10）支持向量机11）贝叶斯算法12）神经网络13）Adaboost算法14）SVD与推荐15）聚类算法16）案例实战：使用Python库分析处理Kobe Bryan职业生涯数据17）案例实战：信用卡欺诈行为检测18）案例实战：泰坦尼克号获救预测19）案例实战：鸢尾花数据集分析20）案例实战：级联结构的机器学习模型21）案例实战：员工离职预测22）案例实战：使用神经网络进行手写字体识别23）案例实战：主成分分析24）案例实战：基于NLP的股价预测25）案例实战：借贷公司数据分析课程二、人工智能与深度学习实战课程风格通俗易懂，必备原理，形象解读，项目实战缺一不可！主体课程分成四个大模块(1)神经网络必备基础知识点，(2)深度学习模型，(3)深度学习框架Caffe与Tensorflow，(4)深度学习项目实战。 课程首先概述讲解深度学习应用与挑战，由计算机视觉中图像分类任务开始讲解深度学习的常规套路。对于复杂的神经网络，将其展开成多个小模块进行逐一攻破，再挑战整体神经网络架构。对于深度学习模型形象解读卷积神经网络原理，详解其中涉及的每一个参数，对卷积网络架构展开分析与评估，对于现阶段火爆的对抗生成网络以及强化学习给出形象解读，并配合项目实战实际演示效果。 基于框架实战，选择两款深度学习最火框架，Caffe与Tensorflow，首先讲解其基本使用方法，并结合案例演示如何应用框架构造神经网络模型并完成案例任务。 选择经典深度学习项目实战，使用深度学习框架从零开始完成人脸检测，验证码识别，人脸关键点定位，垃圾邮件分类，图像风格转换，AI自己玩游戏等。对于每一个项目实战，从数据预处理开始一步步构建网络模型并展开分析与评估。 课程提供所涉及的所有数据，代码以及PPT，方便大家快速动手进行项目实践！1）深度学习概述与挑战2）图像分类基本原理门3）深度学习必备基础知识点4）神经网络反向传播原理5）神经网络整体架构6）神经网络案例实战图像分类任务7）卷积神经网络基本原理8）卷积参数详解9）卷积神经网络案例实战10）经典网络架构分析11）分类与回归任务12）三代物体检测算法分析13）数据增强策略14）TransferLearning15）网络架构设计16） 深度学习框架Caffe网络结构配置17）Caffe18）深度学习项目实战人脸检测19）人脸正负样本数据源制作20）人脸检测网络架构配置习模型21）人脸检测代码实战22）人脸关键点定位项目实战23）人脸关键点定位网络模型24）人脸关键点定位构建级联网络25）人脸关键点定位测试效果与分析26）Tensorflow框架实战27）Tensorflow构建回归模型28）Tensorflow构建神经网络模型29）Tensorflow深度学习模型30）Tensorflow打造RNN网络模型31）Tensorflow项目实战验证识别32）项目实战图像风格转换33）QLearning算法原理34）DQN网络架构35）项目实战DQN网络让AI自己玩游戏36）项目实战对抗生成网络等"}
{"content2":"一、验证码的基本知识验证码的主要目的是强制人机交互来抵御机器自动化攻击的。大部分的验证码设计者并不得要领，不了解图像处理，机器视觉，模式识别，人工智能的基本概念。设计的比较好的验证码，比如Yahoo,Google,Microsoft等很难识别。而国内Tencent的中文验证码虽然难，但算不上好。某些专家认为，不存在一种用计算机表示的信息不能被计算机识别的情况，所以，对验证码来说，所有验证码都是可以识别出来的。二、验证码识别处理基本知识人工智能，模式识别，机器视觉，图像处理的基本知识1)主要流程：1.读取验证码图像：把各种验证码图像解码，得到平面坐标表示的数据，很多开源库可以用来做这一步。2.前处理：验证码识别的预处理主要是去除图像上的干扰，找出字符区域。对于连接在一起的字符，进行分割。对于复杂的验证码，分割这一步非常难处理。对于单个字符，OCR技术的识别率还是能达到比较高的。3.训练：对于模式识别，主要是对样本进行特征提取，然后保存下来。不是训练的样本越多越好。过学习，泛化能力差的问题可能在这里出现。这一步不是必须的，有些识别算法是不需要训练的。4.识别：对于模式识别来说，主要是验证码字符与训练的样本的特征匹配。2)关键概念：模式识别：对事物或者现象的某种表示方式（数值，文字，我们这里主要想说的是数值），通过一些处理和分析，来描述，归类，理解，解释这些事物，现象及其某种抽象。验证码识别的难易：1、字符间粘连越厉害的验证码越难识别2、干扰越没有规律的验证码越难识别；2、字形数目越少越容易识别（一个字符可能有多种字行）验证码识别的步骤：1、提取验证码图片样本，构造识别库；2、对要识别的验证码做预处理（去噪、去干扰、分割等）；3、识别（与构造的识别库做比较）；http://blog.csdn.net/yzm888提供验证码识别接口/DLL开发服务"}
{"content2":"1. 什么是NLP所谓NLP就是自然语言处理，即计算机识别人的自然沟通语言，将人的语言转换成表达含义相同的文字。因为NLP的目的是将人和计算机通过自然语言沟通成为可能，而人最方便的沟通是通过语音发声，计算机只能识别二进制串，或者将可以同等转化为二进制的文字。所以NLP的目的是将语言发声和同等含义的文字进行相互转换后并进行人机交互。技术发展现状：但人工智能在很多方面，如语言理解、视觉场景理解、决策分析等，仍然举步维艰。一个关键的问题就是，机器必须要掌握大量的知识，特别是常识知识才能实现真正类人的智能。这也说明当前随着大数据红利的消失殆尽，以深度学习为代表的感知智能水平日益接近其“天花板”，而以知识为中心的认知智能将是下一代人工智能技术的关键方向。弱人工智能，强人工智能，超人工智能：弱人工智能：这种弱人工智能应用的非常广泛，但是因为比较“弱”，所以很多人没有意识到它们就是人工智能。就好像现在手机当中的自动拦截骚扰电话、邮箱的自动过滤、还有在象棋方面打败人类的机器人。强人工智能：能够有自己的思考方式，能够进行推理然后制作计划，最后进行执行，并且拥有一定的学习能力，能够在实践当中不断进步。超人工智能：智慧程度比人类还要高，在大部分领域当中都超越人类的人工智能，目前这种研发这种人工智能的程度非常之大，但是未来也并不是没有可能的，毕竟现在人工智能领域的发展速度的确是非常快速的2. NLP的工作原理（1）NLP的工作步骤：首先，将语音转换为文字其次，对文字进行拆分再者，词性理解最后，语义理解（2）各步骤3. NLP的框架有哪些，及这些框架的组成，特点tensorflow的插件SyntaxNet4. NLP的具体应用有哪些5. 写一个NLP具体的例子，带程序流程图，实例代码7. 四大强人工智能开放平台自动驾驶，城市大脑，医疗影像，智能语音"}
{"content2":"近日，腾讯优图与《科学》（Science）杂志共同发布《Seeing is believing: R&D applications of computer vision》（眼见为实：计算机视觉的研发和应用）主题报告，通过全球计算机视觉领域的专家访谈，为大众带来当下计算机视觉技术发展的全面解读，也为即将到来的计算机视觉峰会拉开序幕。人工智能 (AI) 曾经只是一种存在于科幻领域的科技，而现在，研究实验室已经不断研发出了各种应用 AI 的日常产品。AI 技术的进步很大程度上得益于计算机视觉的发展。计算机视觉技术关注的是构建能够收集和处理视觉信息的软件。应用计算机视觉可以识别照片中的人物、读取 X 光片、进行工厂机器人系统的智能升级，但它的影响范围远不止于此。大多数人都对自己的视觉习以为常，殊不知要拿起叉子或接住球，我们的大脑要进行大量的运算。计算机变得足够快速、强大和小巧来实现计算机视觉的实际应用，不过是这几年的事情。最先进的计算机视觉技术要运用到深度学习，而深度学习是 AI 的一大领域，灵感来自于人脑。深度学习算法使用的人工神经网络（ANN），是指能够分析并相互传递信息的互相连接的节点层，与神经元的通信机制类似。如果我们向神经网络展示一张自拍照，一层神经元将会识别类似于面部轮廓的粗线条；另一层神经元会关注五官之间的区域，例如眼睛到嘴巴的距离；还有其他神经元会负责观察耳朵的形状。藉此，该算法可判断出这是不是一张人物照片，甚至看出这人是谁。「在大多数计算机视觉任务当中，神经网络都能轻而易举地生成最佳算法，」腾讯优图实验室杰出科学家贾佳亚说道。腾讯总部位于中国深圳，是互联网服务和产品、娱乐及人工智能的全球领导者。像人工神经网络一样，计算机视觉技术工程师也在试图模仿人类视觉系统的运作机制。但是机器比人更有优势的一处是，它不需要像人类那样依赖可见光，还能使用传感器更清楚地看到世界。「在人脸识别、图像分类等众多任务中，计算机视觉能比人类视觉完成的更优秀。但在其他需要推理的任务，计算机视觉还有很长的路要走。」贾佳亚表示，「人类能轻易明白物体彼此之间的关联，我们看到一张图就能编出一个故事。但计算机还远不能达到这种程度的理解能力和想象力。」随着计算机视觉技术的不断发展，它将会带来更多新的发现。计算机视觉和 AI 都处于各自发展的初始阶段，还有很多东西值得探索。计算机视觉技术的进步可能会推动 AI 的迅猛发展，把科幻小说的情节全部变成现实——比如无人车、机器人管家，甚至远距离太空旅行。在实践中学习：AI 的工作原理跟大多数 AI 系统类似，计算机视觉需要学习浩如烟海的数据。研究人员查阅数据并根据其特征仔细为数据添加标签，这些特征就是他们希望 AI 去理解的东西。就计算机视觉的任务而言，研究人员会收集成百上千的照片用于分析。加标签的数据会成为范例，据此训练 AI 进行分类或寻找规律。为了测试 AI 的学习效果，研究人员会展示新的、未加标签的图像，测试其是否能够正确分类。除了要在收集、标注和筹备庞大资料的工作中投入人力外，另一个重要障碍就是运行训练算法需要的大量计算能力。费用低廉的在线服务，让研究人员可以在云端训练算法，而无需为强大的计算机投入数千美元，不过，要得出训练结果仍需数小时甚至数日。对着镜头微笑：图像和视频识别人工神经网络领域最重要的进展之一出自 ImageNet。ImageNet 收集了 1400 万标签图像并于 2009 年发布。ImageNet 挑战赛要求参赛者设计一个能够跟人类一样对照片进行分类的算法，但一直没有出现获胜者。直到 2012 年，一个使用深度学习算法的参赛队伍取得了显著优于以往尝试的结果。今天，人们与计算机视觉产生交互的最常见的几种方式包括图像自动标记和拍照面部识别等，都是基于 ImageNet 获胜的技术。这些应用有助进行网上购物可视化搜索、自动标注社交媒体照片等特定任务。除了图像识别外，这项新科技也推动了照片编辑技术的发展。图像分割算法是计算机视觉的组成部分，可以帮助机器将一张图片分成不同的部分，例如识别背景和前景中的人物。用户可迅速编辑照片，达到专业修图的效果。视觉识别能力目前也应用于视频。计算机视觉算法可以查看摄像机的视频流，并且标记重要部分，这样人们就无需反复回看长达数小时的视频。了解视频中人物的情绪是一项研究人员正在开展的工作，不过有些机构已经率先采用了这类技术。例如坐落在中国东部浙江省的杭州第十一中学，正在尝试用摄像头追踪学生的课堂行为，这些摄像头被称为「慧眼」。让机器人拥有正常视力将 2D 图像转化成 3D 数据，带来了一系列不同的挑战以及众多的新机遇。给机器人赋予计算机视觉就是一个正在展现出前景的领域。机器人在 20 世纪 60 年代开始投入制造业使用，这些装置可以提升重物，执行重复性任务，并且可以一次进行数小时的精确测量，从而轻松地胜过人类。斯德哥尔摩 KTH 皇家理工学院的机器人学教授 Danica Kragic 说：「这一领域始终关注的是建造出那些可以完成人类无法完成任务的机器人。」Kragic 表示，因为人类有 40% 的大脑致力于处理视觉信息，如果要创造能够模仿并参与我们世界的机器，了解它们在多大程度上需要视觉信息是非常重要的。「我们人类在做任何事情时都会自然而然地使用视觉反馈，」Kragic 说。能够处理视觉信息的机器可以在工厂中完成更复杂的工作，甚至进入了我们的家庭。某些技能（例如，拾取会因压力而改变形状的柔软物品）对机器人来说仍然是遥不可及的。这是因为人类在观察时，获得的不仅仅是视觉信息；我们还会获得有关物体物理属性的线索，以及与之交互所需要的物理知识。机器需要能够收集这类信息，才能像人类一样毫不费力地穿行在物理世界中。「在五官感觉当中，视觉是最重要的，因为它赋予了人类理解这个复杂世界的能力。」贾佳亚说，「同样地，计算机视觉就是为了让计算机能够像人类一样观察环境并能跟环境互动。」赋予机器人能够更好地了解世界的传感器是该技术的下一个迭代，它可能让机器人完成在今天尚无法实现的任务。即将上路：自动驾驶汽车自动驾驶汽车是 AI 开发领域中获得资金最充裕、最受关注的领域之一，全面了解世界对于自动驾驶汽车 (AV) 也是至关重要的。除了摄像头，大多数无人驾驶汽车使用激光雷达、雷达、GPS 和感知算法进行导航。「我们使用的许多算法都来自计算机视觉，但现在它不仅仅是关于摄像头数据，」多伦多大学副教授兼优步多伦多高级技术集团负责人 Raquel Urtasun 介绍说。「我们想给汽车装上的，不仅仅是我们的眼睛。」像优步这样的公司希望到 2020 年前能实现自动驾驶汽车上路和载客。这些汽车只会在预先选定的路线上行驶，或需要有人坐在驾驶员座位上，以便在出现任何问题时能够手动接管。这项技术最终的目标是实现真正自主，使得乘坐者除了注意路况外，还可以做其他活动。Urtasun 表示，为了实现这一目标，需要在硬件和软件两方面都取得进步。在硬件方面，激光雷达可能花费数万美元，这使得大规模部署成本太高；在软件方面，工程师需要找到一种方法来使 AI 具备归纳、区分不同物体的能力。如果一个人类驾驶员在道路上看到一些出乎意料的东西（比如一条坠落的电源线），他们会知道应该绕过电线。而如果一辆自动驾驶汽车遇到训练中没有经历过的事情，它可能无法安全地做出反应。Urtasun 表示，虽然自动驾驶汽车现在尚未迎来发展的黄金期，但她对自己在改进传感器和训练算法上的努力能够有效应用仍然充满希望。Urtasun 进一步介绍，幸运的是，「这项技术能够解决许多其他问题。」改进的激光雷达可以使地图测绘和土地调查更加准确，甚至配备传感器的非自动驾驶汽车也可以帮助改善交通状况。特快专递：无人机汽车不是研究人员唯一希望能够自动驾驶的东西：无人驾驶飞机也正在接受自动飞行的训练。无人机研究与自动驾驶汽车研究面临着同样的难题。高质量的训练数据既困难又昂贵，不同的飞行方式意味着无人机需要接受不同的新场景训练，而且法规使得在某些领域难以进行测试。即使是曾经受过训练，飞行过程仍然会非常困难。「任何尝试过控制无人机的人都知道这不是件容易的事情，」比利时研究型大学天主教鲁汶大学的教授 Tinne Tuytelaars 说道。不过，与自动驾驶汽车不同，无人机犯错的成本更低。「如果一架无人机坠毁，」Tuytelaars 耸了耸肩，「也不是件什么大不了的事。」无人机已经可以投入到诸如协助救灾和管道检查等的应用。有朝一日它们将会可以进行送货并提供载客服务。像亚马逊和波音这样的公司已经在测试无人机，未来它们可能会像现在的邮递员那样投递包裹。在某些情况下，多架无人机可能出现在同一个空域内，并且可以比人类飞行员更好地实现彼此间飞行的协调。使它们自动飞行意味着可以降低成本，将技术带到全世界更多人和公司的手中。机器人医生除了交通工具，计算机视觉给医疗领域带来的变化是最显著的。AI 算法已经可以比放射科医生更好地从医学影像中识别出病症，例如骨折和肺炎。「大数据的爆发，尤其在医疗领域的爆发，意味着我们能获得更多的数据来进行研究。」西班牙奥维耶多大学计算机学系助理教授 Beatriz Remeseiro 表示，「我们正在利用数据去解决比以往更复杂的难题。」去年，谷歌宣布开发出新的图像识别算法，可用于检测糖尿病视网膜病变的迹象，这种病变如果不及时治疗会导致失明。这种算法能媲美人类专家，可以在患者视网膜的照片中发现小动脉瘤，这种动脉瘤是病变的早期迹象。2017 年，腾讯也发布了一款用于医学领域的 AI 产品——腾讯觅影，能够通过扫描上消化道内镜图片筛查食管癌，对早期食管癌的识别准确率高达 90%。目前，腾讯觅影已经应用于中国 100 多家医院，未来也将辅助诊断糖尿病视网膜病变、肺结节、宫颈癌及乳腺癌等。其他运用 AI 技术的工具也被用来更早地发现中风，为患者提供更好的生存机会。美国食品和药物管理局最近宣布将简化流程，以便帮助 AI 产品更快地获得批准。当然，这些工具并不会很快就替代医生，它们起到更多是顾问的作用，而非取代从业医师。计算机视觉可以提高工作效率，并使医生短缺地区的人们能得到更多医疗服务。这些创新技术也正在被用来最大限度地减少对人体的侵入性危害。例如，CT 扫描比 X 射线能获取更多信息，但会使患者暴露在更大的辐射中。AI 则可以对 X 光片进行分析后，给医生提供相当于 CT 扫描的信息。「医学影像是通过计算机视觉可以提供更多信息从而真正产生影响的领域，」康奈尔大学计算机科学系教授、谷歌研究所研究科学家 Ramin Zabih 表示。「医学史已经证明，如果医师可以获得更多的数据，这可能意味着能更好地帮助到患者。」远和近：边缘设备和航天器所有这些领域都令人印象深刻，计算机视觉的未来会更加光明。即将开始影响该行业的最大变化之一，就是边缘设备——在两个网络的边界控制数据流转的硬件。大多数 AI 处理需要在大型远程云服务器上完成，因为运行这些算法的计算密集程度很高。另一方面，人们制造了边缘设备，从而具有足够的处理能力能在本地完成工作。随着像 Nvidia 和 Facebook 这样的公司开始制造专门用于运行 AI 的芯片，边缘设备正在变得越来越普遍。这将可以实现更快、更安全的数据处理，并且能让用户通过自己的数据进行更多 AI 自定义训练，增加个人结果的定制程度。「它将推动更多的创新，」总部位于加利福尼亚的 Movidius 公司前首席执行官 Remi El-Ouazzane 表示，该公司为计算机视觉设计专用的低功耗处理器芯片。目前 Remi El-Ouazzane 也担任英特尔 AI 产品集团首席运营官，这是另一家生产半导体芯片和微处理器的加州科技公司。El-Ouazzane 表示，从智能家居设备和监控摄像头到自动驾驶汽车，数十亿台设备都可以运用 AI 技术并在边缘设备上工作。这还将创造能够找到失踪人员的技术（例如，通过扫描人群图像），或者可以在孩子睡觉前没有刷牙的时候能够提醒父母。「问题不是『能不能实现』，而是『什么时候实现』，」他说。在边缘设备的微世界之外，天文学家们也对计算机视觉特别感兴趣，他们从无尽太空中收集到大量数据集并进行研究。Kaggle 是一个用于预测建模和分析竞赛的在线平台，在 Kaggle 上就有一个比赛利用深度学习和计算机视觉技术让研究人员能够通过观察天文图像发现更多关于支配我们宇宙的暗物质的相关信息。此外，还有一个致力于通过 AI 促进探索太空的研究孵化器。前沿开发实验室 (FDL) 是美国航空航天局 (NASA) 与英特尔 AI、谷歌云、洛克希德和 IBM 等公司共同建立的合伙机构。FDL 将天文学家和计算机科学家带到了加利福尼亚州硅谷共同工作 8 周，解决诸如了解太阳耀斑、绘制月球地图和寻找小行星等问题。根据 FDL 创始人之一 James Parr 的说法，如果没有计算机视觉，计划就无法成功。事实上，位于美国加州帕萨迪纳的 NASA 喷气推进实验室 (JPL) 对于摄像技术的发明起到了至关重要的作用，该技术影响了如今的大部分计算机视觉软件。「计算机视觉与太空计划之间存在共生关系，」Parr 说。「但这个议题在太空行业的讨论度还不足够。」太空探索将同样受到影响，因为 AI 对于前往火星以及更远的地方至关重要。太空旅行者和地球指挥中心之间的通信滞后意味着系统必须要能够做出自主决定，而这些决策很多都是由视觉数据来推动的。「随着我们不断向外探索，我们需要机器人和自治系统为宇航员做好准备并提供协助、建造结构、定位并提取资源，」Parr 说。「这是发现和探索过程中激动人心的时刻。」探索计算机视觉将如何改变地球上的生活，同样令人激动。随着 AI 扩展到更多领域并发展出新功能，它可能会遇到新的技术难题。但是，当我们回顾 AI 的历史时，给计算机提供视觉的能力可能是最重要的一项进步。拥有视力的机器将带领我们走向更光明的未来。"}
{"content2":"本文为转载，出处：blog.csdn.net/carson2005最近研究 CV 方面的内容，找到一篇 Blog 介绍一些牛人或实验室在研究 CV，希望对有需要的人有所帮助。<!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 16.0px 'Helvetica Neue'} p.p2 {margin: 0.0px 0.0px 0.0px 0.0px; font: 16.0px 'Helvetica Neue'; min-height: 18.0px} p.p3 {margin: 0.0px 0.0px 0.0px 0.0px; font: 16.0px 'Helvetica Neue'; color: #dca10d} span.s1 {font: 16.0px '.PingFang SC'; color: #000000} span.s2 {color: #000000} span.s3 {font: 16.0px '.PingFang SC'} span.s4 {font: 16.0px 'Helvetica Neue'} span.s5 {color: #dca10d} -->Learning WebSite（1）googleResearch； http://research.google.com/index.html（2）MIT博士，汤晓欧学生林达华； http://people.csail.mit.edu/dhlin/index.html（3）MIT博士后Douglas Lanman； http://web.media.mit.edu/~dlanman/（4）opencv中文网站； http://www.opencv.org.cn/index.php/首页（5）Stanford大学vision实验室； http://vision.stanford.edu/research.html（6）Stanford大学博士崔靖宇； http://www.stanford.edu/~jycui/（7）UCLA教授朱松纯； http://www.stat.ucla.edu/~sczhu/（8）中国人工智能网； http://www.chinaai.org/（9）中国视觉网； http://www.china-vision.net/（10）中科院自动化所； http://www.ia.cas.cn/（11）中科院自动化所李子青研究员； http://www.cbsr.ia.ac.cn/users/szli/（12）中科院计算所山世光研究员； http://www.jdl.ac.cn/user/sgshan/（13）人脸识别主页； http://www.face-rec.org/（14）加州大学伯克利分校CV小组；http://www.eecs.berkeley.edu/Research/Projects/CS/vision/（15）南加州大学CV实验室； http://iris.usc.edu/USC-Computer-Vision.html（16）卡内基梅隆大学CV主页；http://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.html（17）微软CV研究员Richard Szeliski；http://research.microsoft.com/en-us/um/people/szeliski/（18）微软亚洲研究院计算机视觉研究组； http://research.microsoft.com/en-us/groups/vc/（19）微软剑桥研究院ML与CV研究组； http://research.microsoft.com/en-us/groups/mlp/default.aspx（20）研学论坛； http://bbs.matwav.com/（21）美国Rutgers大学助理教授刘青山； http://www.research.rutgers.edu/~qsliu/（22）计算机视觉最新资讯网； http://www.cvchina.info/（23）运动检测、阴影、跟踪的测试视频下载；http://apps.hi.baidu.com/share/detail/18903287（24）香港中文大学助理教授王晓刚； http://www.ee.cuhk.edu.hk/~xgwang/(25)香港中文大学多媒体实验室（汤晓鸥）; http://mmlab.ie.cuhk.edu.hk/(26)U.C. San Diego. computer vision;http://vision.ucsd.edu/content/home(27)CVonline; http://homepages.inf.ed.ac.uk/rbf/CVonline/(28)computer vision software; http://peipa.essex.ac.uk/info/software.html(29)Computer Vision Resource; http://www.cvpapers.com/(30)computer vision research groups;http://peipa.essex.ac.uk/info/groups.html(31)computer vision center; http://computervisioncentral.com/cvcnews(32)浙江大学图像技术研究与应用（ITRA）团队：http://www.dvzju.com/(33)自动识别网：http://www.autoid-china.com.cn/(34)清华大学章毓晋教授：http://www.tsinghua.edu.cn/publish/ee/4157/2010/20101217173552339241557/20101217173552339241557_.html(35)顶级民用机器人研究小组Porf.Gary领导的Willow Garage:http://www.willowgarage.com/(36)上海交通大学图像处理与模式识别研究所：http://www.pami.sjtu.edu.cn/(37)上海交通大学计算机视觉实验室刘允才教授：http://www.visionlab.sjtu.edu.cn/(38)德克萨斯州大学奥斯汀分校助理教授Kristen Grauman ：http://www.cs.utexas.edu/~grauman/ 图像分解，检索(39)清华大学电子工程系智能图文信息处理实验室（丁晓青教授）：http://ocrserv.ee.tsinghua.edu.cn/auto/index.asp(40)北京大学高文教授：http://www.jdl.ac.cn/htm-gaowen/(41)清华大学艾海舟教授：http://media.cs.tsinghua.edu.cn/cn/aihz(42)中科院生物识别与安全技术研究中心：http://www.cbsr.ia.ac.cn/china/index CH.asp(43)瑞士巴塞尔大学 Thomas Vetter教授：http://informatik.unibas.ch/personen/vetter_t.html(44)俄勒冈州立大学 Rob Hess博士：http://blogs.oregonstate.edu/hess/(45)深圳大学 于仕祺副教授：http://yushiqi.cn/(46)西安交通大学人工智能与机器人研究所：http://www.aiar.xjtu.edu.cn/(47)卡内基梅隆大学研究员Robert T. Collins:http://www.cs.cmu.edu/~rcollins/home.html#Background(48)MIT博士Chris Stauffer:http://people.csail.mit.edu/stauffer/Home/index.php(49)美国密歇根州立大学生物识别研究组(Anil K. Jain教授)：http://www.cse.msu.edu/rgroups/biometrics/(50)美国伊利诺伊州立大学Thomas S. Huang:http://www.beckman.illinois.edu/directory/t-huang1(51)武汉大学数字摄影测量与计算机视觉研究中心：http://www.whudpcv.cn/index.asp(52)瑞士巴塞尔大学Sami Romdhani助理研究员：http://informatik.unibas.ch/personen/romdhani_sami/(53)CMU大学研究员Yang Wang:http://www.cs.cmu.edu/~wangy/home.html(54)英国曼彻斯特大学Tim Cootes教授：http://personalpages.manchester.ac.uk/staff/timothy.f.cootes/(55)美国罗彻斯特大学教授Jiebo Luo:http://www.cs.rochester.edu/u/jluo/(56)美国普渡大学机器人视觉实验室：https://engineering.purdue.edu/RVL/Welcome.html(57)美国宾利州立大学感知、运动与认识实验室：http://vision.cse.psu.edu/home/home.shtml(58)美国宾夕法尼亚大学GRASP实验室：https://www.grasp.upenn.edu/(59)美国内达华大学里诺校区CV实验室：http://www.cse.unr.edu/CVL/index.php(60)美国密西根大学vision实验室：http://www.eecs.umich.edu/vision/index.html(61)University of Massachusetts(麻省大学),视觉实验室：http://vis-www.cs.umass.edu/index.html(62)华盛顿大学博士后Iva Kemelmacher:http://www.cs.washington.edu/homes/kemelmi(63)以色列魏茨曼科技大学Ronen Basri:http://www.wisdom.weizmann.ac.il/~ronen/index.html(64)瑞士ETH-Zurich大学CV实验室：http://www.vision.ee.ethz.ch/boostingTrackers/index.htm(65)微软CV研究员张正友：http://research.microsoft.com/en-us/um/people/zhang/(66)中科院自动化所医学影像研究室：http://www.3dmed.net/(67)中科院田捷研究员：http://www.3dmed.net/tian/(68)微软Redmond研究院研究员Simon Baker:http://research.microsoft.com/en-us/people/sbaker/(69)普林斯顿大学教授李凯：http://www.cs.princeton.edu/~li/(70)普林斯顿大学博士贾登：http://www.cs.princeton.edu/~jiadeng/(71)牛津大学教授Andrew Zisserman： http://www.robots.ox.ac.uk/~az/(72)英国leeds大学研究员Mark Everingham:http://www.comp.leeds.ac.uk/me/(73)英国爱丁堡大学教授Chris William: http://homepages.inf.ed.ac.uk/ckiw/(74)微软剑桥研究院研究员John Winn: http://johnwinn.org/(75)佐治亚理工学院教授Monson H.Hayes：http://savannah.gatech.edu/people/mhayes/index.html(76)微软亚洲研究院研究员孙剑：http://research.microsoft.com/en-us/people/jiansun/(77)微软亚洲研究院研究员马毅：http://research.microsoft.com/en-us/people/mayi/(78)英国哥伦比亚大学教授David Lowe: http://www.cs.ubc.ca/~lowe/(79)英国爱丁堡大学教授Bob Fisher: http://homepages.inf.ed.ac.uk/rbf/(80)加州大学圣地亚哥分校教授Serge J.Belongie:http://cseweb.ucsd.edu/~sjb/(81)威斯康星大学教授Charles R.Dyer: http://pages.cs.wisc.edu/~dyer/(82)多伦多大学教授Allan.Jepson: http://www.cs.toronto.edu/~jepson/(83)伦斯勒理工学院教授Qiang Ji: http://www.ecse.rpi.edu/~qji/(84)CMU研究员Daniel Huber: http://www.ri.cmu.edu/person.html?person_id=123(85)多伦多大学教授：David J.Fleet: http://www.cs.toronto.edu/~fleet/(86)伦敦大学玛丽女王学院教授Andrea Cavallaro:http://www.eecs.qmul.ac.uk/~andrea/(87)多伦多大学教授Kyros Kutulakos: http://www.cs.toronto.edu/~kyros/(88)杜克大学教授Carlo Tomasi: http://www.cs.duke.edu/~tomasi/(89)CMU教授Martial Hebert: http://www.cs.cmu.edu/~hebert/(90)MIT助理教授Antonio Torralba: http://web.mit.edu/torralba/www/(91)马里兰大学研究员Yasel Yacoob: http://www.umiacs.umd.edu/users/yaser/(92)康奈尔大学教授Ramin Zabih: http://www.cs.cornell.edu/~rdz/(93)CMU博士田渊栋: http://www.cs.cmu.edu/~yuandong/(94)CMU副教授Srinivasa Narasimhan: http://www.cs.cmu.edu/~srinivas/(95)CMU大学ILIM实验室：http://www.cs.cmu.edu/~ILIM/(96)哥伦比亚大学教授Sheer K.Nayar: http://www.cs.columbia.edu/~nayar/(97)三菱电子研究院研究员Fatih Porikli ：http://www.porikli.com/(98)康奈尔大学教授Daniel Huttenlocher：http://www.cs.cornell.edu/~dph/(99)南京大学教授周志华：http://cs.nju.edu.cn/zhouzh/index.htm(100)芝加哥丰田技术研究所助理教授Devi Parikh: http://ttic.uchicago.edu/~dparikh/index.html(101)瑞士联邦理工学院博士后Helmut Grabner:http://www.vision.ee.ethz.ch/~hegrabne/#Short_CV(102)香港中文大学教授贾佳亚：http://www.cse.cuhk.edu.hk/~leojia/index.html(103)南京大学教授吴建鑫：http://c2inet.sce.ntu.edu.sg/Jianxin/index.html(104)GE研究院研究员李关：http://www.cs.unc.edu/~lguan/(105)佐治亚理工学院教授Monson Hayes:http://savannah.gatech.edu/people/mhayes/(106)图片检索国际竞赛PASCAL VOC(微软剑桥研究院组织):http://pascallin.ecs.soton.ac.uk/challenges/VOC/(107)机器视觉开源处理库汇总：http://archive.cnblogs.com/a/2217609/(108)布朗大学教授Benjamin Kimia: http://www.lems.brown.edu/kimia.html(109)数据堂-图像处理相关的样本数据：http://www.datatang.com/data/list/602026/p1(110)东软基于CV的汽车辅助驾驶系统：http://www.neusoft.com/cn/solutions/1047/(111)马里兰大学教授Rema Chellappa:http://www.cfar.umd.edu/~rama/(112)芝加哥丰田研究中心助理教授Devi Parikh：http://ttic.uchicago.edu/~dparikh/index.html(113)宾夕法尼亚大学助理教授石建波：http://www.cis.upenn.edu/~jshi/(114)比利时鲁汶大学教授Luc Van Gool：http://www.vision.ee.ethz.ch/members/get_member.cgi?id=1, http://www.vision.ee.ethz.ch/~vangool/(115)行人检测主页：http://www.pedestrian-detection.com/(116)法国学习算法与系统实验室Basilio Noris博士：http://lasa.epfl.ch/people/member.php?SCIPER=129576 http://mldemos.epfl.ch/(117)美国马里兰大学LARRY S.DAVIS教授：http://www.umiacs.umd.edu/~lsd/(118)计算机视觉论文分类导航：http://www.visionbib.com/bibliography/contents.html(119)计算机视觉分类信息导航：http://www.visionbib.com/(120)西班牙马德里理工大学博士Marcos Nieto：http://marcosnieto.net/(121)香港理工大学副教授张磊：http://www4.comp.polyu.edu.hk/~cslzhang/(122)以色列技术学院教授Michael Elad：http://www.cs.technion.ac.il/~elad/(123)韩国启明大学计算机视觉与模式识别实验室：http://cvpr.kmu.ac.kr/(124)英国诺丁汉大学Michel Valstar博士：http://www.cs.nott.ac.uk/~mfv/(125)卡内基梅隆大学Takeo Kanade教授:http://www.ri.cmu.edu/people/kanade_takeo.html(126)微软学术搜索：http://libra.msra.cn/(127)比利时天主教鲁汶大学Radu Timofte博士：http://homes.esat.kuleuven.be/~rtimofte/，交通标志检测，定位，3D跟踪(128)迪斯尼匹兹堡研究院研究员：Iain Matthews:http://www.iainm.com/iainm/Home.htmlhttp://www.ri.cmu.edu/person.html?type=publications&person_id=741 AAM,三维重建（129）康奈尔大学视觉与图像分析组：http://www.via.cornell.edu/ 医学图像处理（130）密西根州立大学生物识别研究组：http://www.cse.msu.edu/biometrics/ 人脸识别、指纹识别、图像检索（131）柏林科技大学计算机视觉与遥感实验室：http://www.cv.tu-berlin.de/menue/computer_vision_remote_sensing/parameter/en/ 图像分析、物体重建、基于图像的表面测量、医学图像处理（132）英国布里斯托大学数字多媒体研究组：http://www.cs.bris.ac.uk/Research/Digitalmedia/ 运动检测与跟踪、视频压缩、3D重建、字符定位（133）英国萨利大学视觉、语音与信号处理中心： http://www.surrey.ac.uk/cvssp/   人脸识别、监控、3D、视频检索、（134）北卡莱罗纳大学教堂山分校Marc Pollefeys教授：http://www.cs.unc.edu/~marc/ 基于视频的3D模型生成、相机标定、运动检测与分析、3D重建（135）澳大利亚国立大学Richard Hartley教授：http://users.cecs.anu.edu.au/~hartley/ 运动估计、稀疏子空间、跟踪、（136）百度技术副总监于凯：http://www.dbs.ifi.lmu.de/~yu_k/ 深度学习，稀疏表示，图像分类（137）西安电子科技大学高新波教授：http://web.xidian.edu.cn/xbgao/index.html 质量评判、水印、稀疏表示、超分辨率（138）加州大学伯克利分校Michael I.Jordan教授：http://www.cs.berkeley.edu/~jordan/ 机器学习（139）加州理工行人检测相关资料：http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/（140）微软Redmond研究院研究员Piotr Dollar: http://vision.ucsd.edu/~pdollar/ 行人检测、特征提取、（141）视觉计算研究论坛：http://www.sigvc.org/bbs/ 中科院视觉计算研究小组的论坛（142）美国坦桑尼亚州立大学稀疏学习软件包：http://www.public.asu.edu/~jye02/Software/SLEP/index.htm 稀疏学习（143）美国加州大学圣地亚哥分校Jacob Whitehill博士：http://mplab.ucsd.edu/~jake/ 机器学习（144）美国布朗大学Michael J.Black教授：http://cs.brown.edu/~black/  人的姿态估计和跟踪（145）美国加州大学圣地亚哥分校David Kriegman教授：http://cseweb.ucsd.edu/~kriegman/ 人脸识别（146）南加州大学Paul Debevec教授：http://ict.debevec.org/~debevec/ 或 http://www.pauldebevec.com/ 将CV和CG结合研究 人脸捕捉重建技术（147）伊利诺伊大学D.A.Forsyth教授：http://luthuli.cs.uiuc.edu/~daf/ 三维重建（148）英国牛津大学Ian Reid教授：http://www.robots.ox.ac.uk/~ian/ 跟踪和机器人导航（149）CMU大学Alyosha Efros 教授: https://www.cs.cmu.edu/~efros/ 图像纹理合成（150）加州大学伯克利分校Jitendra Malik教授：http://www.cs.berkeley.edu/~malik/ 轮廓检测、图像/视频分割、图形匹配、目标识别（151）MIT教授William Freeman： http://people.csail.mit.edu/billf/ 图像纹理合成（152）CMU博士Henry Schneiderman： http://www.cs.cmu.edu/~hws/ 目标检测和识别；（153）微软研究员Paul Viola: http://research.microsoft.com/en-us/um/people/viola/ AdaBoost算法（154）微软研究员Antonio Criminisi: http://research.microsoft.com/en-us/people/antcrim/ 图像修补，三维重建，目标检测与跟踪；（155）魏茨曼科学研究所教授Michal Irani: http://www.wisdom.weizmann.ac.il/~irani/ 超分辨率（156）瑞士洛桑理工学院Pascal Fua教授：http://people.epfl.ch/pascal.fua/bio?lang=en 立体视觉，增强现实（157）佐治亚理工学院Irfan Essa教授：http://www.ic.gatech.edu/people/irfan-essa 人脸表情识别（158）中科院助理教授樊彬：http://www.sigvc.org/bfan/ 特征描述；（159）斯坦福大学Sebastian Thrun教授：http://robots.stanford.edu/index.html 机器人；（160）多伦多大学Geoffrey E.Hinton教授：http://www.cs.toronto.edu/~hinton/ 深度学习（161）凤巢系统架构师张栋博士：http://weibo.com/machinelearning（162）2012年龙星计划机器学习课程：http://bigeye.au.tsinghua.edu.cn/DragonStar2012/index.html（163）中科院自动化所肖柏华教授：http://www.compsys.ia.ac.cn/people/xiaobaihua.html 文字识别、人脸识别、质量评判（164）图像视频质量评判：http://live.ece.utexas.edu/research/quality/（165）纽约大学Yann LeCun教授http://yann.lecun.com/   http://yann.lecun.com/exdb/mnist/  手写体数字识别（166）二维条码识别开源库zxing：http://code.google.com/p/zxing/（167）布朗大学Pedro Felzenszwalb教授：http://cs.brown.edu/~pff/ 特征提取，Deformable Part Model（168）伊利诺伊香槟大学Svetlana Lazebnik教授：http://www.cs.illinois.edu/homes/slazebni/ 特征提取，聚类，图像检索（169）荷兰乌德勒支大学图像与多媒体研究中心http://www.cs.uu.nl/centers/give/multimedia/index.html 图像、多媒体检索与匹配（170）英国格拉斯哥大学信息检索小组：http://ir.dcs.gla.ac.uk/ 文本、图像、视频检索（171）中科院自动化所孙哲南助理教书：http://www.cbsr.ia.ac.cn/users/znsun/ 虹膜识别、掌纹识别、人脸识别（172）南京信息工程大学刘青山教授：http://www.jstuoke.com/web/xky/detail.asp?NewsID=1096 人脸图像分析、医学图像分析（173）清华大学助理教授冯建江：http://ivg.au.tsinghua.edu.cn/~jfeng/ 指纹识别（174）北航助理教授黄迪：http://irip.buaa.edu.cn/~dihuang/ 3D人脸识别（175）中山大学助理教授郑伟诗：http://sist.sysu.edu.cn/~zhwshi/ 人脸识别、特征匹配、聚类、检索；（176）google瑞士苏黎世的工程师Thomas Deselaers: http://thomas.deselaers.de/index.html 图像检索（177）百度深度学习研究中心博士后余轶南：http://www.cbsr.ia.ac.cn/users/ynyu/index.htm 目标检测，图像检索（178）威兹曼科技大学超分辨率：http://www.wisdom.weizmann.ac.il/~vision/SingleImageSR.html（179）德克萨斯大学奥斯汀分校Al Bovik教授：http://live.ece.utexas.edu/people/bovik/ 图像视频质量判别、特征提取（180）以色列希伯来大学Yair Weiss教授：http://www.cs.huji.ac.il/~yweiss/ 机器学习、超分辨率（181）以色列希伯来大学Daniel Zoran博士：http://www.cs.huji.ac.il/~daniez/ 超分辨率、去噪（182）美国加州大学Peyman Milanfar教授：http://users.soe.ucsc.edu/~milanfar/ 去噪（183）中科院计算所副研究员常虹：http://www.jdl.ac.cn/user/hchang/index.html 图像检索、半监督学习、超分辨率（184）以色列威茨曼大学Anat Levin教授：http://www.wisdom.weizmann.ac.il/~levina/ 去噪、去模糊（185）以色列威茨曼大学Daniel Glasner博士后：http://www.wisdom.weizmann.ac.il/~glasner/ 超分辨率、分割、姿态估计（186）密西根大学助理教授Honglak Lee: http://web.eecs.umich.edu/~honglak/ 机器学习、特征提取，去噪、稀疏表示；（187）MIT周博磊博士：http://people.csail.mit.edu/bzhou/ 聚集分析、运动检测（188）美国田纳西大学Li He博士：http://web.eecs.utk.edu/~lhe4/ 稀疏表示、超分辨率；（189）Adobe研究院Jianchao Yang研究员：http://www.ifp.illinois.edu/~jyang29/ 稀疏表示，超分辨率、图片检索、去噪、去模糊（190）Deep Learning主页：http://deeplearning.net/ 深度学习论文、软件，代码，demo，数据等；（191）斯坦福大学Andrew Ng教授：http://cs.stanford.edu/people/ang/ 深度神经网络，深度学习（192）Elefant: http://elefant.developer.nicta.com.au/ 机器学习开源库（193）微软研究员Ce Liu: http://people.csail.mit.edu/celiu/ 去噪、超分辨率、去模糊、分割（194）West Virginia大学助理教授Xin Li: http://www.csee.wvu.edu/~xinl/ 边缘检测、降噪、去模糊（195）http://www.csee.wvu.edu/~xinl/source.html 深度学习、去噪、编码、压缩感知、超分辨率、聚类、分割等相关代码集合（196）西班牙格拉纳达大学超分辨率重建项目组：http://decsai.ugr.es/pi/superresolution/index.html（197）清华大学程明明博士：http://mmcheng.net/ 图像分割、检索（198）牛津布鲁克斯大学Philip H.S.Torr教授：http://cms.brookes.ac.uk/staff/PhilipTorr/ 分割、三维重建（199）佐治亚理工学院James M.Rehg教授：http://www.cc.gatech.edu/~rehg/ 分割、行人检测、特征描述、（200）大规模图像分类、检测竞赛ILSVRC（Stanford, Google举办）:http://www.image-net.org/challenges/LSVRC/2013/（201）加州大学尔湾分校Deva Ramanan助理教授：http://www.ics.uci.edu/~dramanan/ 目标检测，行人检测，跟踪、稀疏表示（202）人脸识别测试图片集：http://www.mlcv.net/（203）美国西北大学博士Ming Yang:  http://www.ece.northwestern.edu/~mya671/ 人脸识别、图像检索；（204）美国加州大学伯克利分校博士后Ross B.Girshick：http://www.cs.berkeley.edu/~rbg/ 目标检测（DPM）（205）中文语言资源联盟：http://www.chineseldc.org/index.html  内有很多语言识别、字符识别的训练，测试库；（206）西班牙巴塞罗那大学计算机视觉中心：http://www.cvc.uab.es/adas/site/ 检测、跟踪、3D、行人检测、汽车辅助驾驶（207）德国戴姆勒研究所Prof. Dr. Dariu M. Gavrila：http://www.gavrila.net/index.html 跟踪、行人检测、（208）苏黎世联邦理工学院Andreas Ess博士后：http://www.vision.ee.ethz.ch/~aess/ 行人检测、行为检测、跟踪（209）Libqrencode: http://fukuchi.org/works/qrencode/ 基于C语言的QR二维码编码开源库（210）江西财经大学袁飞牛教授：http://sit.jxufe.cn/grbk/yfn/index.html#  烟雾检测、3D重建、医学图像处理（211）耶路撒冷大学Raanan Fattal教师：http://www.cs.huji.ac.il/~raananf/  图像增强、（212）耶路撒冷大学Dani Lischnski教授：http://www.cs.huji.ac.il/~danix/ 去模糊、纹理合成、图像增强（213）北京航空航天大学周付根教授：http://www.sa.buaa.edu.cn/html/yhxy/szll/jsfc/txcl/1521.html  医学图像处理（214）北京航空航天大学姜志国教授：http://www.sa.buaa.edu.cn/html/yhxy/xygk/zyld/1595.html 医学图像处理，遥感图像分割；<!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px 'Helvetica Neue'} p.p2 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px 'Helvetica Neue'; min-height: 14.0px} p.p3 {margin: 0.0px 0.0px 0.0px 0.0px; font: 12.0px 'Helvetica Neue'; color: #dca10d} span.s1 {font: 12.0px '.PingFang SC'; color: #000000} span.s2 {color: #000000} span.s3 {font: 12.0px '.PingFang SC'} span.s4 {font: 12.0px 'Helvetica Neue'} span.s5 {color: #dca10d} -->"}
{"content2":"人工智能实践：TensorFlow笔记-01-开篇概述从今天开始，从零开始学习TensorFlow，有相同兴趣的同志，可以互相学习笔记，本篇是开篇介绍 Tensorflow，已经人工智能领域的一些名词介绍人工智能实践：TensorFlow笔记-01-概述什么是人工智能?人工智能：机器模拟人的意识和思维艾伦·麦席森·图灵（1912/06--1954/06），美国数学家，逻辑学家，“计算机科学之父”，“人工智能之父”人工智能助理谷歌 Assistant，微软 Cortana，苹果Siri，亚马逊 Alexa，小米 小爱同学什么是机器学习？机器学习是人工智能的一种方法，是人工智能的子集机器学习是一种统计学方法，计算机利用已有数据，得出某种模型，再利用此模型预测结果先使用以往数据训练模，再用模型预测新数据的结果随着经验的增加，效果会变好机器学习三要素1.数据 2.算法 3.算力决策树模型机器学习最主要的应用：1.对连续数据的预测2.对离散数据的归类机器学习的应用领域计算机视觉，语音识别，自然语言处理什么是深度学习？深度学习是机器学习的子集深度学习是深层次神经网络，是机器学习的一种实现方法它试图使用包含复杂结构或者由多重非线性变换构成的多个处理层对数据进行高层抽象的算法李彦宏：简单的说，深度学习就是一个函数集，如此而已深度学习将特征提取和分类结合到一个框架中，用数据学习特征，是一种可以自动学习特征的方法深度学习是一种特征学习方法，把原始的数据通过非线性的复杂模型转换为更高层次、更抽象的表达人工智能，机器学习和深度学习的关系什么是 TensorFlow ？TensorFlow 是谷歌基于 DistBelief 进行研发的第二代人工智能学习系统Tensor（张量）意味着N维数组，Flow（流）意味着基于数据流图的计算，TensorFlow 为张量从流图的一端流动到另一端计算过程TensorFlow 是将复杂的数据结构传输至人工智能神经网中进行分析和处理过程的系统TensorFlow 可被用于语音识别或图像识别等多项机器学习和深度学习领域，对2011年开发的深度学习基础架构 DistBelief 进行了各方面的改进，它可在小到一部智能手机、大到数千台数据中心服务器的各种设备上运行TensorFlow 将完全开源，任何人都可以用我的 TensorFlow 笔记人工智能概述1.TensorFlow笔记-01-开篇概述2.TensorFlow笔记-02-Windows下搭建TensorFlow环境（win版非虚拟机）TensorFlow框架1.TensorFlow笔记-03-张量，计算图，会话2.TensorFlow笔记-04-神经网络的实现过程,前向传播3.TensorFlow笔记-05-反向传播,搭建神经网络的八股神经网络优化1.损失函数2.学习率3.滑动平均4.正则化5.神经网络搭建八股全连接网络基础1.MNIST数据2.模块化搭建神经八股3.手写数字识别准确率输出全连接网络实践1.输入手写数字图片输出识别结果2.制作数据集卷积网络基础1.卷积神经网络2.lenet5代码讲解卷积网络实践1.复现已有的卷积神经网络2.用vgg16实现图片识别本笔记不允许任何个人和组织转载"}
{"content2":"模式识别，计算机视觉领域，期刊(1)pattern recognition letters, 从投稿到发表，一年半时间(2)Pattern recognition 不好中，时间长(3)IEICE Transactions on Information and Systems， 作者中有一个必须是会员。收费高，审稿快。影响因子0.4(4)International Journal of Pattern Recognition and Artificial Intelligence ， 审稿周期一般6--12周，影响因子偏低，容易中。(5)Computational Intelligence， 中等偏上，要求较高，杂志级别不错，关注人数偏少，比较冷门(6)information processing letters, 影响因子低0.5左右，接搞量大，容易发表，审稿周期一般3--6个月。(7)Computer vision and image understanding, 9个月审稿期，平均投稿命中率20%，业内比较认可(8)journal of visual communication and image representation， 投稿容易，审稿周期一年以上(9)Signal processing letters, 影响因子0.99， 美国，审稿一个多月，(10)International Journal on Graphics, Vision and Image Processing (GVIP),(11)IET Image Processing, 影响因子0.758， EI Compendex ，审稿周期一年以上(12)IET Computer Vision ，影响因子0.969，(13)SIAM Journal on Imaging Sciences，(14)International Journal of Pattern Recognition and Artificial Intelligence (IJPRAI)，影响因子0.5， EI compendex, sci, 审稿时间超长，一两年(15)IEEE Signal Processing Letters， 审稿4---8周左右，影响因子不高，容易中，关注人不多(16)Journal of Logic and Computation, 影响因子，0.789，SCI检索(17)IEICE Transactions on Information and Systems 审稿时间2--4周，容易中，影响因子小，相对冷门，关注人数不多。(18)COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING，影响因子偏低，但仍然需要一定水平才可以投，审稿2--4周，SCI,EI检索(19)Signal Processing: Image Communication，容易中，审稿周期半年到一年(20)International Journal of Computer Vision， 较难，审稿周期半年到一年，EI,SCI检索(21)Journal of Mathematical Imaging and Vision，审稿半年到一年，影响因子不高，不容易中，稍微有些冷门。(22)Machine Vision and Applications，影响因子偏低，但是接稿量不是很大，审稿周期一年以上，但容易发表，SCI,EI检索(23)Pattern Analysis & Applications，影响因子不高，影响力也比较小，审稿时间一年以上，但容易投中。(24)Signal Image and Video Processing， 容易中，审稿时间半年到一年，EI检索(25)Pattern recognition and image analysis， EI检索，(26)Journal of digital imaging ，审稿周期半年到一年，影响因子偏低，容易中，很少有人关注(27)International journal of pattern recognition and artificial intelligence，影响因子偏低，容易中，关注人比较少。审稿周期半年到一年。(28)International journal of imaging systems and technology，影响因子偏低，容易中，审稿周期半年到一年。(29)journal of vlsi signal processing systems for signal image and video ，影响因子偏低，容易中，审稿周期一年以上，关注人比较少。"}
{"content2":"原文：http://www.thinkface.cn/thread-4354-1-1.htmlhttp://www.thinkface.cn/thread-4488-1-1.html﻿﻿人脸对齐任务即根据输入的人脸图像，自动定位出面部关键特征点，如眼睛、鼻尖、嘴角点、眉毛以及人脸各部件轮廓点等，如下图所示。这项技术的应用很广泛，比如自动人脸识别，表情识别以及人脸动画自动合成等。由于不同的姿态、表情、光照以及遮挡等因素的影响，准确地定位出各个关键特征点看似很困难。我们简单地分析一下这个问题，不难发现这个任务其实可以拆分出三个子问题：1. 如何对人脸表观图像（输入）建模2. 如何对人脸形状（输出）建模3.如何建立人脸表观图像（模型）与人脸形状（模型）的关联以往的研究工作也离不开这三个方面。人脸形状建模典型的方法有可变形模板（Deformable Template）、点分布模型（主动形状模型Active Shape Model）、图模型等。人脸表观建模又可分为全局表观建模和局部表观建模。全局表观建模简单的说就是考虑如何建模整张人脸的表观信息，典型的方法有主动表观模型Active Appearance Model（产生式模型）和Boosted Appearance Model（判别式模型）。对应的局部表观建模则是对局部区域的表观信息建模，包括颜色模型、投影模型、侧剖线模型等。近来，级联形状回归模型在特征点定位任务上取得了重大突破，该方法使用回归模型，直接学习从人脸表观到人脸形状（或者人脸形状模型的参数）的映射函数，进而建立从表观到形状的对应关系。此类方法不需要复杂的人脸形状和表观建模，简单高效，在可控场景（实验室条件下采集的人脸）和非可控场景（网络人脸图像等）均取得不错的定位效果。此外，基于深度学习的人脸对齐方法也取得令人瞩目的结果。深度学习结合形状回归框架可以进一步提升定位模型的精度，成为当前特征定位的主流方法之一。下面我将具体介绍级联形状回归和深度学习这两大类方法的研究进展。级联线性回归模型人脸对齐问题可以看作是学习一个回归函数F，以图象I作为输入，输出θ为特征点的位置（人脸形状）：θ = F（I）。简单的说，级联回归模型可以统一为以下框架：学习多个回归函数{f1 ,…, fn-1, fn}来逼近函数F：θ = F（I）=  fn (fn-1 (…f1(θ0, I) ,I) , I)θi= fi (θi-1, I),    i=1,…,n所谓的级联，即当前函数fi的输入依赖于上一级函数fi-1的输出θi-1，而每一个fi的学习目标都是逼近特征点的真实位置θ，θ0为初始形状。通常情况，fi不是直接回归真实位置θ，而回归当前形状θi-1与真实位置θ之间的差：Δθi = θ - θi-1。接下来我将详细介绍几个典型的形状回归方法，他们根本的不同点在于函数fi的设计不同以及输入特征不同。在加州理工学院从事博士后研究的Piotr Dollár于2010年首次提出级联形状回归模型CascadedPose Regression（CPR），来预测物体的形状，该工作发表在国际计算机视觉与模式识别会议CVPR上。如下图所示，如下图所示，给定初始形状θ0，通常为平均形状，根据初始形状θ0提取特征（两个像素点的差值）作为函数f1的输入。每个函数fi建模成Random Fern回归器，来预测当前形状θi-1与目标形状θ的差Δθi，并根据Δ?i预测结果更新当前形状得θ i = θi-1+Δ?i，作为下一级函数fi+1的输入。该方法在人脸、老鼠和鱼三个数据集上取得不错的实验结果，通用的算法框架亦可用于其他形状估计任务，比如人体姿态估计等。该方法的不足之处在于对初始化形状θ0比较敏感，使用不同的初始化做多次测试并融合多次预测结果可以一定程度上缓解初始化对于算法的影响，但并不能完全解决该问题，且多次测试会带来额外的运算开销。当目标物体被遮挡时，性能也会变差。与上一个工作来自同一课题组的Xavier P. Burgos-Artizzu，针对CPR方法的不足，进一步提出Robust Cascaded Pose Regression（RCPR）方法，并发表在2013年国际计算视觉会议ICCV上。为了解决遮挡问题，Piotr Dollár提出同时预测人脸形状和特征点是否被遮挡的状态，即fi的输出包含Δθi和每个特征点是否被遮挡的状态pi：{Δθi , pi }= fi(θi-1, I),    i=1,…,n当某些特征点被遮挡时，则不选取该特征点所在区域的特征作为输入，从而避免遮挡对定位的干扰。此外，作者提出智能重启技术来解决形状初始化敏感的问题：随机初始化一组形状，运行{f1 ,…,fn-1, fn}的前10%的函数，统计形状预测的方差，如果方差小于一定阈值，说明这组初始化不错，则跑完剩下的90%的级联函数，得到最终的预测结果；如果方差大于一定阈值，则说明初始化不理想，选择重新初始化一组形状。该策略想法直接，但效果很不错。另外一个很有趣的工作Supervised Descent Method（SDM），从另一个角度思考问题，即考虑如何使用监督梯度下降的方法来求解非线性最小二乘问题，并成功地应用在人脸对齐任务上。不难发现，该方法最终的算法框架也是一个级联回归模型。与CPR和RCPR不同的地方在于：fi建模成了线性回归模型；fi的输入为与人脸形状相关的SIFT特征。该特征的提取也很简单，即在当前人脸形状θi-1的每个特征点上提取一个128维的SIFT特征，并将所有SIFT特征串联到一起作为fi的输入。该方法在LFPW和LFW-A&C数据集上取得不错的定位结果。同时期的另一个工作DRMF则是使用支持向量回归SVR来建模回归函数fi，并使用形状相关的HOG特征（提取方式与形状相关的SIFT类似）作为fi输入，来级联预测人脸形状。与SDM最大的不同在于，DRMF对于人脸形状做了参数化的建模。fi的目标变为预测这些形状参数而不再是直接的人脸形状。这两个工作同时发表在CVPR 2013上。由于人脸形状参数化模型很难完美地刻画所有形状变化，SDM的实测效果要优于DRMF。微软亚洲研究院孙剑研究员的团队在CVPR 2014上提出更加高效的级联形状回归方法Regressing LocalBinary Features（LBF）。和SDM类似，fi也是建模成线性回归模型；不同的地方在于，SDM直接使用SIFT特征，LBF则基于随机森林回归模型在局部区域学习稀疏二值化特征。通过学习稀疏二值化特征，大大减少了运算开销，比CRP、RCPR、SDM、DRMF等方法具有更高的运行效率（LBF可以在手机上跑到300FPS），并且在IBUG公开评测集上取得优于SDM、RCPR的性能。级联形状回归模型成功的关键在于：1. 使用了形状相关特征，即函数fi的输入和当前的人脸形状θi-1紧密相关；2. 函数fi的目标也与当前的人脸形状θi-1相关，即fi的优化目标为当前形状θi-1与真实位置θ之间的差Δθi。此类方法在可控和非可控的场景下均取得良好的定位效果，且具有很好的实时性。深度模型以上介绍的级联形状回归方法每一个回归函数fi都是浅层模型（线性回归模型、Random Fern等）。深度网络模型，比如卷积神经网络（CNN）、深度自编码器（DAE）和受限玻尔兹曼机（RBM）在计算机视觉的诸多问题，如场景分类，目标跟踪，图像分割等任务中有着广泛的应用，当然也包括特征定位问题。具体的方法可以分为两大类：使用深度模型建模人脸形状和表观的变化和基于深度网络学习从人脸表观到形状的非线性映射函数。主动形状模型ASM和主动表观模型AAM使用主成分分析（PCA）来建模人脸形状的变化。由于姿态表情等因素的影响，线性PCA模型很难完美地刻画不同表情和姿态下的人脸形状变化。来自伦斯勒理工学院JiQiang教授的课题组在CVPR2013提出使用深度置信网络（DBN）来刻画不同表情下人脸形状的复杂非线性变化。此外，为了处理不同姿态的特征点定位问题，进一步使用3向RBM网络建模从正面到非正面的人脸形状变化。最终该方法在表情数据库CK+上取得比线性模型AAM更好的定位结果。该方法在同时具备多姿态多表情的数据库ISL上也取得较好的定位效果，但对同时出现极端姿态和夸张表情变化的情况还不够理想。下图是深度置信网络（DBN）：建模不同表情下的人脸形状变化的示意图。香港中文大学唐晓鸥教授的课题组在CVPR 2013上提出3级卷积神经网络DCNN来实现人脸对齐的方法。该方法也可以统一在级联形状回归模型的大框架下，和CPR、RCPR、SDM、LBF等方法不一样的是，DCNN使用深度模型-卷积神经网络，来实现fi。第一级f1使用人脸图像的三块不同区域（整张人脸，眼睛和鼻子区域，鼻子和嘴唇区域）作为输入，分别训练3个卷积神经网络来预测特征点的位置，网络结构包含4个卷积层，3个Pooling层和2个全连接层，并融合三个网络的预测来得到更加稳定的定位结果。后面两级f2, f3在每个特征点附近抽取特征，针对每个特征点单独训练一个卷积神经网络（2个卷积层，2个Pooling层和1个全连接层）来修正定位的结果。该方法在LFPW数据集上取得当时最好的定位结果。借此机会也介绍本人发表在欧洲视觉会议ECCV2014的一个工作：即提出一种由粗到精的自编码器网络（CFAN）来描述从人脸表观到人脸形状的复杂非线性映射过程。该方法级联了多个栈式自编码器网络fi，每一个fi刻画从人脸表观到人脸形状的部分非线性映射。具体来说，输入一个低分辨率的人脸图像I，第一层自编码器网络f1可以快速地估计大致的人脸形状，记作基于全局特征的栈式自编码网络。网络f1包含三个隐层，隐层节点数分别为1600,900,400。然后提高人脸图像的分辨率，并根据f1得到的初始人脸形状θ1，抽取联合局部特征，输入到下一层自编码器网络f2来同时优化、调整所有特征点的位置，记作基于局部特征的栈式自编码网络。该方法级联了3个局部栈式自编码网络{f2 , f3, f4}直到在训练集上收敛。每一个局部栈式自编码网络包含三个隐层，隐层节点数分别为1296,784,400。得益于深度模型强大的非线性刻画能力，该方法在XM2VTS，LFPW，HELEN数据集上取得比DRMF、SDM更好的结果。此外，CFAN可以实时地完成人脸人脸对齐（在I7的台式机上达到23毫秒/张），比DCNN（120毫秒/张）具有更快的处理速度。下图是CFAN：基于由粗到精自编码器网络的实时人脸对齐方法的示意图。以上基于级联形状回归和深度学习的方法对于大姿态（左右旋转-60°~+60°）、各种表情变化都能得到较好的定位结果，处理速度快，具备很好的产品应用前景。针对纯侧面（±90°）、部分遮挡以及人脸检测与特征定位联合估计等问题的解决仍是目前的研究热点。作者简介张杰，中科院计算技术研究所VIPL课题组博士生，专注于深度学习技术及其在人脸识别领域的应用。相关研究成果发表在计算机视觉国际顶级学术会议ICCV, CVPR和ECCV，并担任国际顶级期刊TIP和TNNLS审稿人。目前有很多的人脸对齐算法，比较传统的有ASM、AAM、CLM和一些列改进算法，而目前比较流行的有ESR、3D-ESR、SPR、LBF、SDM、CFSS等。ASM算法相对容易，其中STASM是目前正面脸当中比较好的算法，原作者和CLM比较过。但是STASM速度较慢，大概10frame/s左右。ASM对齐在精度上不如AAM，AAM由于使用全局纹理信息，因此精度较高，但是遇到光照和多姿态时，对初始化Shape要求很高，不然容易陷入局部优化。CLM 分别继承了ASM和AAM的一些特征，效果得到了提升。对局部器官特征的概率假设和优化算法的选择，是CLM各种算法的本质区别。CLM比较好的论文和文献有：automati feature localtion with constraint models  David Cristinacce, Tim CootesFace Alignment through Subspace ConstrainedMean-Shifts  这篇文章综述了ASM/CLMCLM开源代码较少，但是有很好的从算法到工程实现的代码。Constrained LocalModel (CLM) Implementation - Xiaoguang Yan很多学者刚接触到人脸对齐时，不知道它有什么用处，下面就列举出人脸对齐的应用领域：(1)人脸器官定位、器官跟踪。通过人脸对齐，我们能够定位到人脸的每个部件，提取相应的部件特征。(2)表情识别。通过人脸对齐后，我们能够利用对齐后的人脸形状分析人脸的表情状态。(3)人脸漫画/素描图像生成。通过人脸对齐后，我们能够进行人脸漫画和素描生成。如：魔漫相机(4)虚拟现实和增强现实。通过人脸对齐后，我们能够做出很多好玩的应用。如2D应用：3D应用：(5)人脸老化、年轻化、年龄推断。特征融合/图像增强。通过人脸对齐后，我们能够有效提取人脸特征，并分析人脸年龄、人脸老化等。(6)纹理过渡。如：长得很像某人的狗脸。换脸7)性别鉴别。通过人脸对齐，能够对人脸进行性别识别，男女之间的人脸形状有一定的差异性。8)3D卡通。通过人脸对齐能够进行3D卡通模拟。人脸对齐应用广泛，有着巨大的研究价值。学者们一定要好好研究哦！"}
{"content2":"什么是人工智能？人工智能是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学，是认知、决策、反馈的过程。人工智能技术的细分领域有哪些？人工智能技术应用的细分领域：深度学习、计算机视觉、智能机器人、虚拟个人助理、自然语言处理—语音识别、自然语言处理—通用、实时语音翻译、情境感知计算、手势控制、视觉内容自动识别、推荐引擎等。下面，我们就每个细分领域，从概述和技术原理角度稍微做一下展开，供大家拓展一下知识。1、深度学习深度学习作为人工智能领域的一个重要应用领域。说到深度学习，大家第一个想到的肯定是AlphaGo，通过一次又一次的学习、更新算法，最终在人机大战中打败围棋大师。对于一个智能系统来讲，深度学习的能力大小，决定着它在多大程度上能达到用户对它的期待。。深度学习的技术原理：1.构建一个网络并且随机初始化所有连接的权重； 2.将大量的数据情况输出到这个网络中； 3.网络处理这些动作并且进行学习； 4.如果这个动作符合指定的动作，将会增强权重，如果不符合，将会降低权重； 5.系统通过如上过程调整权重； 6.在成千上万次的学习之后，超过人类的表现；2、计算机视觉计算机视觉是指计算机从图像中识别出物体、场景和活动的能力。计算机视觉有着广泛的细分应用，其中包括，医疗领域成像分析、人脸识别、公关安全、安防监控等等。计算机视觉计算机视觉的技术原理：计算机视觉技术运用由图像处理操作及其他技术所组成的序列来将图像分析任务分解为便于管理的小块任务。3、语音识别语音识别，是把语音转化为文字，并对其进行识别、认知和处理。语音识别的主要应用包括电话外呼、医疗领域听写、语音书写、电脑系统声控、电话客服等。语音识别语音识别技术原理：1、 对声音进行处理，使用移动函数对声音进行分帧； 2、 声音被分帧后，变为很多波形，需要将波形做声学体征提取； 3、 声音特征提取之后，声音就变成了一个矩阵。然后通过音素组合成单词；4、虚拟个人助理苹果手机的Siri，以及小米手机上的小爱，都算是虚拟个人助理的应用。虚拟个人助理技术原理：（以小爱为例）1、用户对着小爱说话后，语音将立即被编码，并转换成一个压缩数字文件，该文件包含了用户语音的相关信息； 2、由于用户手机处于开机状态，语音信号将被转入用户所使用移动运营商的基站当中，然后再通过一系列固定电 线发送至用户的互联网服务供应商(ISP)，该ISP拥有云计算服务器； 3、该服务器中的内置系列模块，将通过技术手段来识别用户刚才说过的内容。5、自然语言处理自然语言处理（NLP），像计算机视觉技术一样，将各种有助于实现目标的多种技术进行了融合，实现人机间自然语言的通信。NLP自然语言处理技术原理：1、汉字编码词法分析； 2、句法分析； 3、语义分析； 4、文本生成； 5、语音识别；6、智能机器人智能机器人在生活中随处可见，扫地机器人、陪伴机器人……这些机器人不管是跟人语音聊天，还是自主定位导航行走、安防监控等，都离不开人工智能技术的支持。智能机器人技术原理：人工智能技术把机器视觉、自动规划等认知技术、各种传感器整合到机器人身上，使得机器人拥有判断、决策的能力，能在各种不同的环境中处理不同的任务。智能穿戴设备、智能家电、智能出行或者无人机设备其实都是类似的原理。7、引擎推荐淘宝、京东等商城，以及36氪等资讯网站，会根据你之前浏览过的商品、页面、搜索过的关键字推送给你一些相关的产品、或网站内容。这其实就是引擎推荐技术的一种表现。Google为什么会做免费搜索引擎，目的就是为了搜集大量的自然搜索数据，丰富他的大数据数据库，为后面的人工智能数据库做准备。引擎推荐技术原理：推荐引擎是基于用户的行为、属性（用户浏览行为产生的数据），通过算法分析和处理，主动发现用户当前或潜在需求，并主动推送信息给用户的浏览页面。未来人工智能应用领域的展望除了上面的应用之外，人工智能技术肯定会朝着越来越多的分支领域发展。医疗、教育、金融、衣食住行等等涉及人类生活的各个方面都会有所渗透。未来已来，顺应未来发展大势，让自己的企业具备应对未来发展环境的能力，这就是你最应该做的事情。希望大家能在读了本片文章后，能激发出更大的兴趣去了解、学习人工智能的知识，也许，下一步可以从《人工智能简史》开始。"}
{"content2":"浙江大学直接攻博研究生培养方案（计算机学院计算机应用技术专业）一、培养目标：本专业以培养能在国民经济建设、科学技术发展和社会进步中发挥积极作用的创新型、复合型高层次计算机科学与技术专门人才为目标。要求学生掌握坚实宽广 的基础理论和系统深入的专业知识，能了解计算机科学与技术的最新成果和发展方向，并能在计算机应用技术领域做出开创性的研究成果。二、学制：5年三、主要研究方向：01人工智能与机器学习；02 认知科学和形象思维模拟；03计算机辅助设计（CAD）与计算机图形学（CG）；04 现代集成制造技术；05 智能设计；06 智能感知；07 信息智能和决策支持；08 计算机视觉与智能机器人；09 网络多媒体技术；10 工程数据库；11 智能控制；12 计算机网络和信息通讯系统；13 计算机支持合作系统（CSCW）；14 信息安全；15 电子商务与电子政务；16 中间件；17 数字图书馆技术四、课程学习要求课程设置实行学分制。本专业直博生必须修满的最低学分数为38学分，其中公共学位课7学分，专业学位课16学分，选修课11学分，读书报告4学分。五、培养环节要求1、读书报告要求：要求每位直接攻博研究生在学期间做读书报告10次，其中至少公开在学科或学院的学术论坛做读书报告4次。完成累计10次计4学分。2、开题报告要求：论文开题报告之前要求有足够的文献阅读量，直博生应填写规定格式的开题报告，并在研究所（或本专业）公开、集中进行，由导师（组）和本专业其他教师共同审定。开题报告的时间，可根据直博生本人研究进展确定，但最迟应在入学后第三学年末进行。3、专业外语要求：文献阅读以外文文献为主，能以外文撰写学术论文，并参加相关学科的国外专家学术报告会。4、发表论文要求：直博生在学期间应公开发表高质量的论文数篇，才能申请博士学位论文答辩。博士研究生发表论文要求、博士学位论文答辩和学位授予工作按浙江大学研究生院相关规定执行。六、其他课　程　设　置类别课程编号课  程  名  称学分学时上课学期备  注公共课0510001博士生英语232秋或冬学位课0420001自然辩证法232秋或冬学位课0220002科学社会主义理论与实践124秋或冬学位课0410001当代科技革命与马克思主义232秋或冬学位课公共素质类课程至少选修1学分任意选修课专业学位课2111001计算机应用数学（上）232春2111002计算机应用数学（下）232夏2112001计算机科学与技术前沿232秋冬2122002高级操作系统232秋冬2122003高级计算机系统结构232秋冬2122001计算理论232秋冬2122023人工智能引论232秋可根据各自的研究方向任选4学分。2122021计算机视觉232春2122022高级数据库技术232冬2122019高级形式化方法232冬2122018高级计算机网络232春2122020计算机图形学464秋冬专业选修课2124025电子商务技术232秋2122006VLSI设计232秋冬2124027计算机动画与应用232春2124003计算机安全232冬2124017非真实感图形学232秋2124028普适计算232冬2124062三维CAD建模232秋2124057高端计算及其应用232秋0711026生物信息学专题232秋2124058高级编译技术232春2124012网格计算232秋专业选修课2124059多核计算232冬2124064机器学习导引232秋2124014高级软件工程232春2124072嵌入式系统设计原理232春2124060网络多媒体计算232春2124065人工智能前沿232冬或夏2124066科学计算可视化232夏2124067语音、语言处理与理解232冬2124061网络多媒体搜索引擎232夏2124063生物智能与算法232春2124068图像处理与建模232春2124069传感器网络及信息处理232夏2124070并行计算机结构和程序设计232夏2124071网络算法232夏2124073虚拟现实技术(1)232春2124074虚拟现实技术(2)232夏2114001计算机专业外语116任意限选浙江大学博士研究生培养方案（计算机学院计算机应用技术专业）一、培养目标：本专业以培养能在国民经济建设、科学技术发展和社会进步中发挥积极作用的创新型、复合型高层次计算机科学与技术专门人才为目标。要求学生掌握坚实宽广的基础理论和系统深入的专业知识，并能在计算机应用技术领域做出开创性的成果。二、学制：3.5年三、主要研究方向：01人工智能与机器学习；02 认知科学和形象思维模拟；03计算机辅助设计（CAD）与计算机图形学（CG）；04 现代集成制造技术；05 智能设计；06 智能感知；07 信息智能和决策支持；08 计算机视觉与智能机器人；09 网络多媒体技术；10 工程数据库；11 智能控制；12 计算机网络和信息通讯系统；13 计算机支持合作系统（CSCW）；14 信息安全；15 电子商务与电子政务；16 中间件；17 数字图书馆技术四、课程学习要求课程设置实行学分制。本专业博士研究生必须修满的最低学分数为14学分，其中公共学位课4学分，专业学位课6学分，选修课2学分，读书报告2学分。五、培养环节要求1、读书报告要求：要求每位博士研究生在学期间做读书报告6次，其中至少公开在学科或学院的学术论坛做读书报告1~2次。完成累计6次计2学分。2、开题报告要求：论文开题报告之前要求有足够的文献阅读量，博士研究生应填写规定格式的开题报告，并在研究所（或本专业）公开、集中进行，由导师（组）或本专业其他教师共同审定。开题报告的时间，可根据博士研究生本人研究进展确定，但最迟应在入学后第二学年末进行。3、专业外语要求：文献阅读以外文文献为主，能以外文撰写学术论文，并参加相关学科的国外专家学术报告会。4、发表论文要求：博士研究生在学期间应公开发表高质量的论文数篇，才能申请博士学位论文答辩。博士研究生发表论文要求、博士学位论文答辩和学位授予工作按浙江大学研究生院相关规定执行。六、其他课　程　设　置类别课程编号课　程　名　称学分学时上课学期备　注公共课0510001博士生英语232秋或冬学位课0410001当代科技革命与马克思主义232秋或冬学位课专业学位课2111001计算机应用数学（上）232春2111002计算机应用数学（下）232夏2112001计算机科学与技术前沿232秋冬专业选修课0000998选修相关学科课程232任意2114001计算机专业外语116任意限选浙江大学硕士研究生培养方案（计算机学院计算机应用技术专业）一、培养目标：本专业以培养在国民经济建设、科学技术发展和社会进步中，能发挥积极作用的创新型、复合型高层次计算机科学与技术专门人才为目标。要求学生掌握坚实的 基础理论和系统的专业知识并能了解计算机科学与技术的最新成果和发展方向，能独立从事本专业领域的理论、方法及应用研究。二、学制：2.5年三、主要研究方向：01人工智能与机器学习；02 认知科学和形象思维模拟；03计算机辅助设计（CAD）与计算机图形学（CG）；04 现代集成制造技术；05 智能设计；06 智能感知；07 信息智能和决策支持；08 计算机视觉与智能机器人；09 网络多媒体技术；10 工程数据库；11 智能控制；12 计算机网络和信息通讯系统；13 计算机支持合作系统（CSCW）；14 信息安全；15 电子商务与电子政务；16 中间件；17 数字图书馆技术四、课程学习要求硕士研究生在攻读学位期间，应修最低总学分26学分，其中公共学位课5学分，专业学位课10学分，选修课9学分，读书报告2学分。五、培养环节要求1、读书报告要求：要求每位硕士研究生在学期间做读书报告4次，其中至少公开在学科或学院的学术论坛做读书报告1次。完成累计4次计2学分。2、开题报告要求：硕士研究生应填写规定格式的开题报告，由导师（组）和本专业其他教师共同审定。开题报告的时间，应在入学后第二学年初进行。3、专业外语要求：文献阅读以外文文献为主，并参加相关学科的国外专家学术报告会。4、发表论文要求：学位论文答辩前至少有1篇与学位论文有关的被SCI、EI收录的学术论文或在研究生院规定的A类期刊上的学术论文发表（含录用）；或发明专利1项；或软件著作权1个。六、其他课　程　设　置类别课程编号课　程　名　称学分学时上课学期备　注公共课0520001硕士生英语232秋或冬学位课0420001自然辩证法232秋或冬学位课0220002科学社会主义理论与实践124秋或冬学位课公共素质类课程至少选修1学分任意选修课专业学位课2122002高级操作系统232秋冬2122003高级计算机系统结构232秋冬2122001计算理论232秋冬2122023人工智能引论232秋可根据各自的研究方向任选4学分。2122021计算机视觉232春2122022高级数据库技术232冬2122019高级形式化方法232冬2122018高级计算机网络232春2122020计算机图形学464秋冬专业选修课2124025电子商务技术232秋2122006VLSI设计232秋冬2124027计算机动画与应用232春2124003计算机安全232冬2124017非真实感图形学232秋2124028普适计算232冬2124062三维CAD建模232秋2124057高端计算及其应用232秋0711026生物信息学专题232秋2124058高级编译技术232春2124012网格计算232秋2124059多核计算232冬2124064机器学习导引232秋2124014高级软件工程232春2124072嵌入式系统设计原理232春专业选修课2124060网络多媒体计算232春2124065人工智能前沿232冬或夏2124066科学计算可视化232夏2124067语音、语言处理与理解232冬2124061网络多媒体搜索引擎232夏2124063生物智能与算法232春2124068图像处理与建模232春2124069传感器网络及信息处理232夏2124070并行计算机结构和程序设计232夏2124071网络算法232夏2124073虚拟现实技术(1)232春2124074虚拟现实技术(2)232夏"}
{"content2":"转自：https://yq.aliyun.com/news/294479摘要： 影响因子：2.http://mchelp.manuscriptcentral.IEEETransactionsonEvolutionaryComputation是由IEEE计算智能学会出版的双月同行评审科学期刊。【导读】撰写成功的研究论文不仅仅是传播您的知识。大多数期刊都会在提交之前规定适用于您的内容的详细创作指南。许多研究论文甚至因为没有遵循期刊的指导原则而被拒绝。为了帮助您快速开始研究论文格式，本博客文章列出了计算机科学领域顶级国际期刊的期刊格式和创作指南。作者｜Harry Ven编译｜专知整理｜Yingying，李大囧1.IEEE Transactions on Pattern Analysis and Machine IntelligencePAMI是由IEEE计算机学会出版的月度同行评审科学期刊。 它涵盖了计算机视觉和图像理解，模式分析和识别以及机器智能的研究。 机器学习，搜索技术，文档和手写分析，医学图像分析，视频和图像序列分析，基于内容的图像和视频检索，以及面部和手势识别等。影响因子： 9.455(2017)作者指南：http://mchelp.manuscriptcentral.com/gethelpnow/training/author/2.Artificial IntelligenceArtificial Intelligence是人工智能研究的科学期刊。 它成立于1970年，由Elsevier出版。影响因子： 3.333 (2015)作者教程：https://www.elsevier.com/journals/artificial-intelligence/0004-3702/guide-for-authors3.Communications of the ACMCommunications of the ACM是计算机械协会（ACM）的月刊。 重点是信息技术进步和相关管理问题的实际影响; ACM还出版了多种理论期刊。影响因子： 5.694 (2013)作者指南http://mchelp.manuscriptcentral.com/gethelpnow/training/author/4.ComputerComputer是IEEE计算机协会从业者导向的杂志，包含同行评审的文章，定期专栏和有关当前计算相关问题的访谈。影响因子： 3.301 (2015)作者指南：http://cacm.acm.org/system/assets/0000/6052/CACM_Author-Guidelines.pdf5.IEEE Transactions on ComputersIEEE Transactions on Computers是一份月度同行评审的科学期刊，涵盖计算机设计的各个方面。 它成立于1952年，由IEEE计算机协会出版。影响因子：2.916 (2017)作者指南：http://mchelp.manuscriptcentral.com/gethelpnow/training/author/6.IEEE Transactions on Evolutionar ComputationIEEE Transactions on Evolutionary Computation是由IEEE计算智能学会出版的双月同行评审科学期刊。 它涵盖了进化计算和相关领域，包括自然启发算法，基于人口的方法，以及选择和变异是不可或缺的优化，以及这些范例相结合的混合系统。影响因子： 8.124 (2017)作者指南：http://mchelp.manuscriptcentral.com/gethelpnow/training/author/7.IEEE Transactions on Fuzzy SystemsIEEE Transactions on Fuzzy Systems是由IEEE计算智能学会出版的双月同行评审科学期刊。 它涵盖了从硬件到软件的模糊系统的理论，设计或应用，包括重要的技术成果，探索性开发或基于模糊模型的现场系统的性能研究。影响因子：8.415 (2017)作者指南：http://mchelp.manuscriptcentral.com/gethelpnow/training/author/8.Journal of CryptologyJournal of Cryptology是密码学和密码学领域的科学期刊。 该期刊每季度由国际密码学研究协会出版。影响因子：1.021(2015)作者指南：http://www.springer.com/computer/theoretical+computer+science/journal/1459.IEEE Transactions on Information TheoryIEEE Transactions on Information Theory是由IEEE信息理论学会出版的月度同行评审科学期刊。 它涵盖了信息理论和通信数学。影响因子：2.679 (2016)作者指南：http://mchelp.manuscriptcentral.com/gethelpnow/training/author/10.IEEE Transactions on Neural Networks and Learning SystemsIEEE Transactions on Neural Networks and Learning Systems是由IEEE计算智能学会出版的月度同行评审科学期刊。 它涵盖了神经网络和相关学习系统的理论，设计和应用。影响因子： 7.982 (2017)作者指南：http://mchelp.manuscriptcentral.com/gethelpnow/training/author/11.Journal of the ACMJournal of the ACM是一本同行评审的科学期刊，涵盖计算机科学，尤其是理论方面。 它是计算机协会的官方期刊。影响因子：2.353(2011)作者指南：http://jacm.acm.org/acm-author-guidelines.cfm12.Journal of Artificial Intelligence ResearchJournal of Artificial Intelligence Research人工智能研究杂志是一本开放获取同行评审的科学期刊，涵盖人工智能各个领域的研究。 纸张卷由AAAI印刷机印刷。影响因子：2.284 (2016)作者指南：http://www.jair.org/submission_info.html13.Journal of Functional ProgrammingJournal of Functional Programming是一本同行评审的科学期刊，涵盖功能编程语言的设计，实现和应用，涵盖从数学理论到工业实践的范围。影响因子：1.357(2015)作者指南：http://mchelp.manuscriptcentral.com/gethelpnow/training/author/14.International Journal of Computer VisionIJCV是Springer出版的期刊。影响因子： 8.222(2016)作者指南：http://www.springer.com/computer/image+processing/journal/1126315.Journal of Machine Learning ResearchJournal of Machine Learning Research是一份同行评审的开放获取科学期刊，涵盖机器学习。影响因子： 14.03(2015)作者指南：http://www.jmlr.org/author-info.html#Links16.SIAM Journal on Computing (SICOMP)SIAM计算杂志（SICOMP）是一本专注于计算机科学的数学和形式方面的科学期刊。 它由工业和应用数学学会（SIAM）出版。影响因子： 5.694 (2013)作者指南：https://www.siam.org/journals/sicomp/authors.php原文链接：https://blog.typeset.io/top-16-international-computer-science-journals-a-template-guide-3b838a7fd902请关注专知公众号（扫一扫最下面专知二维码，或者点击上方蓝色专知）后台回复“PF” 就可以获取部分期刊Word和Latex模板下载链接~-END-专 · 知人工智能领域26个主题知识资料全集获取与加入专知人工智能服务群:欢迎微信扫一扫加入专知人工智能知识星球群，获取专业知识教程视频资料和与专家交流咨询！"}
{"content2":"回顾本质矩阵的定义本质矩阵的基本性质：结合成像的几何关系Longuet-Higgins equation注意大小写的区别哦，大小表示物点矢量，小与表示像点矢量。像平面上的一点可以看作：• (u,v) 2D film point(局限于像平面上来考虑)• (u,v,f) 3D point on film plane(相机坐标系中来考虑)• k(u,v,f) viewing ray into the scene(透过像点和原点射线上点的像，相机坐标系中来考虑)• k(X, Y, Z) ray through point P in the scene(在世界坐标系中来考虑)设$l$为像平面上的一直线：$au+bv+c=0$由点线结合关系可得：因此有这样就可以用几何的观点来解释上述方程：左像平面上的一点$p_l$乘以本质矩阵$E$，结果为一条直线，该直线就是$p_l$的极线，且过$p_l$在右像平面上的对应点$p_r$。这个结论十分喜人。同理有• Remember: epipoles belong to the epipolar lines• And they belong to all the epipolar lines关于本质矩阵的关系总结如下：本质矩阵采用的是相机的外部参数，也就是说采用相机坐标(The essential matrix uses CAMERA coordinates)，如果要分析数字图像，则要考虑坐标(u,v)，此时需要用到内部参数(To use image coordinates we must consider the INTRINSIC camera parameters)从像素级来考虑，有如下关系short version: The same equation works in pixel coordinates too!矩阵$F$称为基本矩阵:$ F=M_r^{-T}RSM_l^{-1} $• has rank 2• depends on the INTRINSIC and EXTRINSIC Parameters (f, etc ; R & T)Analogous to essential matrix. The fundamental matrix also tells how pixels (points) in each image are related to epipolar lines in the other image.例子：由$ F*e_l =0 $，并根据下图，where is the epipole? vector in the right nullspace of matrix $F$，即$F$的右零空间。当然$e_l$是非零向量，也就是说$ F*e_l =0 $有非零解，说明矩阵$F$不是满秩的，或者说它是奇异的，However, due to noise,F may not be singular.So instead, next best thing is eigenvector associated with smallest eigenvalue of F。>> [u,d] = eigs(F’ * F)u =-0.0013 0.2586 -0.96600.0029 -0.9660 -0.25861.0000 0.0032 -0.0005d = 1.0e8*-1.0000 0 00 -0.0000 00 0 -0.0000eigenvector associated with smallest eigenvalue>> uu = u(:,3)uu = ( -0.9660 -0.2586 -0.0005)>> uu / uu(3) : to get pixel coords(1861.02 498.21 1.0)where is the epipole?相反的问题是：如果已知点的对应关系，如何计算本质矩阵和基本矩阵呢：下一节：计算机视觉基础6——本质矩阵和基本矩阵的求解"}
{"content2":"在这个看脸的时代，颜值就是一切。怎样可以成为控制颜值的“黑魔法师”？相信，阅读以下这些经典的图像处理书籍能够助你一臂之力。赶紧紧随大圣众包威客平台的脚步吧！《数字图像处理基础》随着台式计算机的处理能力日益增强，各种图像拍摄的设备（例如平板电脑、手机摄像头、数码相机、扫描仪等）的普及，以及互联网的加持，使得数字图像处理变得与文字处理一样普及。本书就数字图像处理的各个基本主题，先给出有关问题的数学公式，然后根据数学公式给出实现有关问题的伪代码，最后在Java语言及ImageJ平台下完整实现。《数字图像处理基础》作为高等学校计算机及相关专业“数字图像处理”课程的教材，是非常适合的。2.《数字图像处理（第三版）》在数字图像处理领域，本书作为主要教材已有30多年。这一版本除保留了前两版的大部分内容外，作者更在13个方面对本书进行了修订，新增了400多幅图像、200多幅图表及80多道习题，融入了近年来数字图像处理领域的重要进展，因而本书特色鲜明且与时俱进。全书仍分为12章，即绪论、数字图像基础、灰度变换与空间滤波、频率域滤波、图像复原与重建、彩色图像处理、小波和多分辨率处理、图像压缩、形态学图像处理、图像分割、表示与描述、目标识别。3.《Image Processing and Analysis》《ImageProcessingandAnalysis》将现代数学与现代图像处理中最先进的方法联系起来，组织成一个连贯的逻辑结构。作者通过它们连接到傅里叶和光谱分析中的少数共同线程，揭示了传统图像处理的原理，从而整合了现代图像处理方法的多样性。可以说，这本书是全面而且综合的，它涵盖了当代图像分析和处理中的4个最强大的数学工具类，同时也探索了它们的内在连接和集成。4.《图像处理、分析与机器视觉（第3版）》《图像处理、分析与机器视觉（第3版）》是为计算机专业图像处理、图像分析和机器视觉课程编写的教材。此书针对这三方领域的有关原理与技术，展开了广泛而深入的讨论，包括图像预处理、图像分割、形状表示与描述、物体识别与图像理解、三维视觉、数学形态学图像处理技术、离散图像变换、图像压缩、纹理描述、运动分析等。本书力图将复杂的概念通过具体的示例以易于理解的算法来描述，并提供了大量包含图示和处理结果的插图，特别有助于读者的学习和理解。另外，它也覆盖了十分广泛的领域，包括人工智能、信号处理、人工神经网络、模式识别、机器学习、模糊数学等一系列相关学科，读者可以通过学习此书，学到很多具有普遍价值的知识和具体的应用方法。5.《数字图像处理与机器视觉》《数字图像处理与机器视觉》将理论知识、科学研究和工程实践有机结合起来，内容涉及数字图像处理和识别技术的方方面面，包括图像的点运算、几何变换、空域和频域滤波、小波变换、图像复原、彩色图像处理、形态学处理、图像分割、图像压缩以及图像特征提取等。本书也对机器视觉进行了前导性的探究，重点介绍了3种目前在工程技术领域非常流行的分类技术——人工神经网络（ANN）、支持向量机（SVM）和AdaBoost，并在配套给出的识别案例中直击光学字符识别（OCR）、人脸识别和性别分类等热点问题。《数字图像处理与机器视觉》结构紧凑，内容深入浅出，讲解图文并茂，适合于计算机、通信和自动化等相关专业的本科生、研究生，以及工作在图像处理和识别领域一线的广大工程技术人员阅读参考。6.《Mathematical Problems in Image Processing》本书更新的第二版提供了各种图像分析的应用程序，并展示了如何离散化这些精确的数学。《MathematicalProblemsinImageProcessing》展示了它对数学领域的贡献，并突出了未解决的理论问题。而对于计算机视觉社区，此书提出了一个清晰、自洽的涉及数学的图像处理问题。值得一提的是，第二版《Mathematical Problems in Image Processing》提供了对PDE框架所涵盖的图像处理应用程序进展的回顾，并更新了现有的材料，同时，本书还提供了用于以最小的努力创建模拟的编程工具，十分难得。DT时代，想成为朋友圈中人人膜拜的图像处理高手？从阅读开始吧！"}
{"content2":"起源人工智能的起源普遍认为是 1956 年的达特茅斯会议。 因为这次会议本身就是为了人工智能而召开的，而且参会的人后来也成了人工智能各个方向上的大牛。 参会的主要 6 个人：麦卡锡，会议的召集人，也是 LISP 的发明者克门尼，BASIC 发明人，做过爱因斯坦的数学助理，和麦卡锡一起研究出了 分时系统明斯基，提出神经网络香农，信息论创始人纽厄儿，图灵奖得主司马贺，图灵奖得主，诺贝尔奖得主，纽厄儿的学生，他们俩都是人工智能中的符号学派人工智能的概念虽然是达特茅斯会议提出的，但是在此之前已经有关于人工智能的研究，只不过那时不叫人工智能， 而是称为\"机器智能\"或是\"智能机器\"，对此，最广为人知的就是图灵的论文和他的图灵测试。发展人工智能提出的很早，但发展的并不顺，不像其他学科都逐渐走向统一，从诞生之初，人工智能的研究方向就不断分裂。 目前主流的人工智能方向有：符号学派这是传统的人工智能方向，专注于如何制造智能机器特别是智能的计算机程序和工程，但不局限于模拟生物的智能行为。 这一学派和图灵的研究一脉相承。符号学派利用知识和搜索来代替真实人脑的神经网络结构，擅长解决利用现有知识做比较复杂的推理，规划，逻辑运算和判断等问题。连接学派之所以成为连接学派，是因为这一学派认为高级的智能行为是从大量神经网络的连接中自发出现的。 神经网络就是这个学派提出的，但是神经网路发展也不顺利，后来借助统计学习理论，稍有起色，在某些方面形成了优势。连接学派可以很好的解决机器学习的问题，并自动获取知识。擅长解决模式识别，聚类，联想等非结构化的问题。行为学派行为学派和前 2 个学派不一样，没有把重点关注在高级智能上，而是关注比人类低级的多的昆虫上。 这个学派产生了极其紧密的极其昆虫，也有能够适应各种环境的机器狗，当然，从动物身上不仅仅学到这些， 还有遗传算法，这是对进化方面的研究。行为学派关注模拟身体的运作机制，而不是脑。擅长解决适应性，学习，快速行为反应等问题，也可以解决一定的识别，聚类，联想等问题。其他这 3 个学派大致是从软件，硬件，身体 3 个角度来模拟和理解智能的。 随着人工智能的发展，单独依靠某一个学派，发展都会遇到瓶颈，于是，很多人工智能研究者逐渐不再关注人工智能的理论， 而是从实际应用出发，能够解决实际问题的就是好方法，就这样，逐渐产生了很多新兴的学科：自动定理证明模式识别机器学习自然语言理解计算机视觉自动程序设计总结人工智能进入 21 世纪第二个十年，最引人注目的成就就是 深度学习 深度学习也是一种对大脑的模拟，它模仿了人类大脑的深层体系结构。 到了今天，充足的食物（大数据）和强劲的消化系统（GPU，云计算）成为了深度学习崛起的契机。除此之外，还有一种 被称为 人类计算 的方式，也叫人工人工智能就是利用互联网上大量的人群来完成需要机器完成的工作。 这种方式比较有意思，实际的案例有：卡内基梅隆大学的 路易斯-冯-安 reCAPTCHE: 利用验证码来完成英文古文献的数字化同样是冯-安 设计的 Verbosity 游戏，目的是构建知识库此外还有 Matchin，Duolingo现在，最火的人工智能方式是机器学习，但是人工智能远远不只是机器学习，我们在学习，讨论机器学习的同时， 也要站在更高的角度上看待人工智能，因为未来人工智能的统一在何方还是未知。"}
{"content2":"今日CS.CV计算机视觉论文速览Wed, 13 Mar 2019Totally 25 papersInteresting:📖自动医学图像分析，主要就x光乳腺癌检测，胸片CT肺结合检测，脑部颈部病变检测等方面展开研究，并阐述了如何生成数据、利用弱监督标签、结合boosting方法等。（欧文分校博士论文）1 Introduction2 Adversarial Deep Structured Nets for Mass Segmentation from Mammograms3 Deep Multi-instance Networks with Sparse Label Assignment for Whole Mammogram Classification4 DeepLung: Deep 3D Dual Path Nets for Automated Pulmonary Nodule Detection and Classification5 DeepEM: Deep 3D ConvNets With EM For Weakly Supervised Pulmonary Nodule Detection6 AnatomyNet: Deep Learning for Fast and Fully Automated Whole-volume Segmentation of Head and Neck Anatomy7 Conclusion软件著作：SOFTWAREAnatomyNethttps://github.com/wentaozhu/AnatomyNet-for-anatomical-segmentationDeep learning for fast and fully automated whole-volume segmentation of head and neckanatomy.DeepEMhttps://github.com/wentaozhu/DeepEM-for-Weakly-Supervised-DetectionDeep 3D ConvNets with EM for weakly supervised pulmonary nodule detection.DeepLungxivhttps://github.com/wentaozhu/DeepLungDeep 3d dual path nets for automated pulmonary nodule detection and classification.Adversarial DSNhttps://github.com/wentaozhu/adversarial-deep-structural-networksAdversarial Deep Structural Networks for Mammographic Mass Segmentation.Deep MILhttps://github.com/wentaozhu/deep-mil-for-whole-mammogram-classificationDeep multi-instance networks with sparse label assignment for whole mammogram classifi-cation.Regularized Deep LSTMhttp://www.escience.cn/system/file?fileId=87579Co-Occurrence feature learning for skeleton based action recognition using regularized deepLSTM networks.📖深度学习架构的理解相似性和差异性，主要集中于比较模型间和的相似性，研究发现核相似性达到99.9%的模型表现却不尽相同，而没用太大相似性的架构却有着相同的表现！20个网络的相关性分析，表格中的数据为模型在相同视觉智能和参数相关性上的差异：12种resnet的相关性分析，📖从RGB图像生成复杂的形态学网格,主要集中与复杂形态的重建，提出了基于骨架媒介的和此方法，利用骨架保持形态并减小计算复杂度，并逐阶段纠正体积和重建和网格的精调。（from 华南理工）骨架体K合成的流程以及mesh优化的结果：dataset：ShapeNet-Skeleton datasetrelate: 3D Shape net:http://3dshapenets.cs.princeton.edu/3D Shape Synthesis and Recognition📖GOGGLES,基于数据编程,多源数据的弱监督（data programming，label function）和相似性（affinity）编码实现自动数据生成。code: https://github.com/chu-data-lab/GOGGLES/数据集：http://www.vision.caltech.edu/visipedia/CUB-200-2011.html📖CaP，Cascaded Projection, 端到端的神经网络压缩和加速工具，基于数据驱动的方法实现，在保持高精度和高通量的前提下极大减小内存消耗。通过低秩投影的方法将输入输出连续的滤波器投影到统一的低维空间中来实现压缩，并通过最小化分类损失和特征间的差距来优化投影过程，并通过bp和sgd在几何约束下得到代理矩阵来进行优化，解决了精度、大小和速度的问题。（from rit Rochester Institute of Technology）压缩优化过程：📖非监督的视频显著性物体检测，主要利用了流补全的技术。首先利用光流检测备选区域得到光流边界，随后通过光流和补全流之间的差异得到残差流，并以此为线索得到运动显著性掩膜。只依赖于运动信息让这种方法具有灵活性和普适性。（from Inria, Centre Rennes）相关方法和指标：相关数据集和基准：https://davischallenge.org/davis2016/code.html📖快速深度图生成,提出了一种更高效的网络架构用于从双目视觉生成视差图。研究人员利用了半分辨率的输入，减小了网络计算量，并且使用了低维（视差）特征向量来实现了图像间的配准。(from Swarthmore College,华盛顿大学)code:https://projects.ayanc.org/fdscs/📖Parallel Medical Imaging,(from 中科院自动化所)数据合成方法值得注意：!!!注：自动生成ct图像📖自适应迁移学习综述，（from 重庆大学）几种自适应结构📖手部骨骼分割，（from 新加坡国立）dataset: 2017 Pediatric Bone Age Prediction Challenge [23]📖利用深度学习生成超像素，（from KU Leuven）相关：Simple Linear Iterative Clustering (SLIC)Daily Computer Vision Papers[1] Title: Dense Classification and Implanting for Few-Shot LearningAuthors:Yann Lifchitz, Yannis Avrithis, Sylvaine Picard, Andrei Bursuc[2] Title: Placental Flattening via Volumetric ParameterizationAuthors:S. Mazdak Abulnaga, Esra Abaci Turk, Mikhail Bessmeltsev, P. Ellen Grant, Justin Solomon, Polina Golland[3] Title: An End-to-End Network for Panoptic SegmentationAuthors:Huanyu Liu, Chao Peng, Changqian Yu, Jingbo Wang, Xu Liu, Gang Yu, Wei Jiang[4] **Title: Cascaded Projection: End-to-End Network Compression and AccelerationAuthors:Breton Minnehan, Andreas Savakis[5] Title: Discriminative Principal Component Analysis: A REVERSE THINKINGAuthors:Hanli Qiao[6] **Title: Fast Deep Stereo with 2D Convolutional Processing of Cost SignaturesAuthors:Kyle Yee, Ayan Chakrabarti[7] Title: Hierarchical Autoregressive Image Models with Auxiliary DecodersAuthors:Jeffrey De Fauw, Sander Dieleman, Karen Simonyan[8] Title: Parallel Medical Imaging: A New Data-Knowledge-Driven Evolutionary Framework for Medical Image AnalysisAuthors:Chao Gou, Tianyu Shen, Wenbo Zheng, Oliver Kwan, Fei-Yue Wang[9] Title: Unsupervised motion saliency map estimation based on optical flow inpaintingAuthors:L. Maczyta, P. Bouthemy, O. Le Meur[10] Title: Image Classification base on PCA of Multi-view Deep RepresentationAuthors:Yaoqi Sun, Liang Li, Liang Zheng, Ji Hu, Yatong Jiang, Chenggang Yan[11] Title: Semi-Supervised Self-Taught Deep Learning for Finger Bones SegmentationAuthors:Ziyuan Zhao, Xiaoman Zhang, Cen Chen, Wei Li, Songyou Peng, Jie Wang, Xulei Yang, Le Zhang, Zeng Zeng[12] Title: Paradox in Deep Neural Networks: Similar yet Different while Different yet SimilarAuthors:Arash Akbarinia, Karl R. Gegenfurtner[13] Title: Occlusion-guided compact template learning for ensemble deep network-based pose-invariant face recognitionAuthors:Yuhang Wu, Ioannis A. Kakadiaris[14] Title: Deep Learning for Automated Medical Image AnalysisAuthors:Wentao Zhu[15] Title: A Skeleton-bridged Deep Learning Approach for Generating Meshes of Complex Topologies from Single RGB ImagesAuthors:Jiapeng Tang, Xiaoguang Han, Junyi Pan, Kui Jia, Xin Tong[16] Title: Knowledge Adaptation for Efficient Semantic SegmentationAuthors:Tong He, Chunhua Shen, Zhi Tian, Dong Gong, Changming Sun, Youliang Yan[17] Title: Transfer Adaptation Learning: A Decade SurveyAuthors:Lei Zhang[18] Title: Fast Registration for cross-source point clouds by using weak regional affinity and pixel-wise refinementAuthors:Xiaoshui Huang, Lixin Fan, Qiang Wu, Jian Zhang, Chun Yuan[19] Title: Quality-Gated Convolutional LSTM for Enhancing compressed videoAuthors:Ren Yang, Xiaoyan Sun, Mai Xu, Wenjun Zeng[20] Title: Generating superpixels using deep image representationsAuthors:Thomas Verelst, Matthew Blaschko, Maxim Berman[21] Title: GOGGLES: Automatic Training Data Generation with Affinity CodingAuthors:Nilaksh Das, Sanya Chaba, Sakshi Gandhi, Duen Horng Chau, Xu Chu[22] Title: A total variation based regularizer promoting piecewise-Lipschitz reconstructionsAuthors:Martin Burger, Yury Korolev, Carola-Bibiane Schönlieb, Christiane Stollenwerk[23] Title: Generating Compact Geometric Track-Maps for Train Positioning ApplicationsAuthors:Hanno Winter, Stefan Luthardt, Volker Willert, Jürgen Adamy[24] Title: Theory III: Dynamics and Generalization in Deep NetworksAuthors:Andrzej Banburski, Qianli Liao, Brando Miranda, Lorenzo Rosasco, Bob Liang, Jack Hidary, Tomaso Poggio[25] Title: Progressive Generative Adversarial Binary Networks for Music GenerationAuthors:Manan Oza, Himanshu Vaghela, Kriti SrivastavaPapers from arxiv.org更多精彩请移步主页pic from pixels.com"}
{"content2":"课程简介:人工智能（Artificial Intelligence），英文缩写为AI。它是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。人工智能从诞生以来，理论和技术日益成熟，应用领域也不断扩大，可以设想，未来人工智能带来的科技产品，将会是人类智慧的“容器”。人工智能可以对人的意识、思维的信息过程的模拟。人工智能不是人的智能，但能像人那样思考、也可能超过人的智能。人工智能是一门极富挑战性的科学，从事这项工作的人必须懂得计算机知识，心理学和哲学。人工智能是包括十分广泛的科学，它由不同的领域组成，如机器学习，计算机视觉等等，总的说来，人工智能研究的一个主要目标是使机器能够胜任一些通常需要人类智能才能完成的复杂工作。但不同的时代、不同的人对这种“复杂工作”的理解是不同的。 2017年12月，人工智能入选“2017年%下载地址：百度网盘下载"}
{"content2":"人工智能，用计算机实现人类智能。机器通过大量训练数据训练，程序不断自我学习、修正训练模型。模型本质，一堆参数，描述业务特点。机器学习和深度学习(结合深度神经网络)。传统计算机器下棋，贪婪算法，Alpha-Beta修剪法配合Min-Max算法。AlphaGo，蒙特卡洛树搜索法(Monte Carlo tree search,MCTS)和深度卷积神经网络(deep convolutional neural network,DCNN)。估值网络(value network，盘面评估函数)，计算盘面分类。策略网络(policy network)，计算每个棋概率、胜率。训练模型过程，分类方法得到直接策略，直接策略对历史棋局资料库进行神经网络学习(深度卷积神经网络)得到习得策略，强化学习自我对局(蒙特卡洛树状搜寻法)得到改良策略，回归整体统计得到估值网络。谷歌《Nature》论文，《Mastering the game of Go with deep neural networks and tree search》。深度学习。前身 是人工神经网络(artificial neural network,ANN)，模仿人脑神经元传递、处理信息模式。输入层(input layer)输入训练数据，输出层(output layer)输出计算结果，中间隐藏层(hidden layer)向前传播数据。数据预处理，图片，图像居中、灰度调整、梯度锐化、去除噪声、倾斜度调整。输入神经网络第一层，第一层提取图像特征，有用向下传递，最后一层输出结果。前向传播(forword propagation)。分类概率向量，前5概率值。深度学习，利用已知数据学习模型，在未知数据做出预测。神经元特性，激活函数(activation function)，非线性函数，输入非线性变化，前向传播；成本函数(cost function),定量评估预测值和真实值差距，调整权重参数，减少损失，反向传播(backword propagation)。神经网络算法核心，计算、连接、评估、纠错、训练。深度学习增加中间隐藏层数和神经元数，网络变深变宽，大量数据训练。分类(classification)。输入训练数据特征(feature)、标记(label)，找出特征和标记映射关系(mapping)，标记纠正学习偏差，提高预测率。有标记学习为监督学习(supervised learning)。无监督学习(unsuperVised learning)，数据只有特征没有标记。训练不指定明确分类，数据聚群结构，相似类型聚集一起。没有标记数据分组合，聚类(clustering)；成功激励制度，强化学习(reinforcement learning,RL)。延迟奖赏与训练相关，激励函数获得状态行动映射，适合连续决策领域。半监督学习(semi-supervised learning)，训练数据部分有标记，部分没有，数据分布必然不完全随机，结合有标记数据局部特征，大量无标记数据整体分布，得到较好分类结果。有监督学习(分类、回归)-半监督学习(分类、回归)-半监督聚类(标记不确定)-无监督学习(聚类)。深度学习入门，算法知识、大量数据、计算机(最好GPU)。学习数学知识，训练过程涉及过程抽象数学函数，定义网络结构，定义线性非线性函数，设定优化目标，定义损失函数(loss function)，训练过程求解最优解次优解，基本概率统计、高等数学、线性代数，知道原理、过程，兴趣涉猎推导证明。经典机器学习理论、基本算法，支持向量机、逻辑回归、决策树、朴素贝叶斯分类器、随机森林、聚类算法、协同过滤、关联性分析、人工神经网络、BP算法、PCA、过拟合、正则化。编程工具(语言)，Python解释型、面向对象、动态数据类型高级程序设计语言，线性代数库、矩阵操作，Numpy、Pandas第三方库，机器学习库sklearn，SVM、逻辑回归，MATLAB，R，C++，Java，Go。经典论文，最新动态研究成果，手写数据字识别，LeNet，物体目标检测，MSCNN，博客、笔记、微信公众号、微博、新媒体资讯，新训练方法，新模型。自己动手训练神经网络，选择开源深度学习框架，主要考虑用的人多，方向主要集中视觉、语音，初学最好从计算机视觉入手，用各种网络模型训练手写数字(MNIST)、图像分类(CIFAR)数据集。学入兴趣工作领域，计算机视觉，自然语言处理，预测，图像分类、目标检测、视频目标检测，语音识别、语音合成、对话系统、机器翻译、文章摘要、情感分析，医学行业，医学影像识别，淘宝穿衣，衣服搭配，款式识别，保险、通信客服，对话机器人智能问答系统，智能家居，人机自然语言交互。工作问题，准确率、坏案例(bad case)、识别速度，可能瓶颈，结合具体行业领域业务创新，最新科研成果，调整模型，更改模型参数，贴近业务需求。传统基于规则，依赖知识。统计方法为核心机器学习，重要的是做特征工程(feature engineering)，调参，根据领域经验提取特征，文字等抽象领域，特征相对容易提取，语音一维时域信号、图像二维空域信号等领域，提取特征困难。深度学习，神经网络每层自动学习特征。TensorFlow深度学习开源工具。TensorFlow支持异构设备分布式计算(heterogeneous distributed computing)。异构，包含不同成分，异构网络、异构数据库。异构设备，CPU、GPU核心协同合作。分布式架构调度分配计算资源、容错。TensorFlow支持卷积神经网络(convolutional neural network,CNN)、循环神经网络(recurrent neural network,RNN)，长短期记忆网络(long short-term memory,LSTM,RNN特例)。《The Unreasonable Effectiveness of Recurrent Neural Networks》。Tensor库对CPU/GPU透明，不同设备运行由框架实现，用户指定什么设置做什么运算。完全独立代码库，脚本语言(Python)操作Tensor，实现所有深度学习内容，前向传播、反向传播、图形计算。共享训练模型，TensorFlow slim模块。没有编译过程，更大更复杂网络，可解释性，有效日志调试。研究人群。学者，深度学习理论研究，网络模型，修改参数方法和理论，产耱科研前沿，理论研究、模型实验，新技术新理论敏感。算法改进者，现有网络模型适配应用，达到更好立人日木，模型改进，新算法改进应用现有模型，为上层应用提供优良模型。工业研究者，掌握各种模型网络结构、算法实现，阅读优秀论文，复现成果，应用工业，主流人群。TensorFlow工业优势，基于服务端大数据服务(谷歌云平台、搜索)，面向终端用户移动端(Android)和嵌入式。模型压缩、8位低精度数据存储。TensorFlow特性。高度灵活性(deep flexibility)，数据流图(data flow graph)数值计算，只需要构建图，书写计算内部循环，自定义上层库。真正可移植性(true portability)，CPU、GPU、台式机、服务器、移动端、云端服务器、Docker容器。产研结合(connect research and production)，快速试验框架，新算法，训练模型。自动求微分(auto-differentiation)，只需要定义预测模型结构、目标函数，添加数据。多语言支持(language options)，Python、C++、Java接口，C++实现核心，Jupyter Notebook，特征映射(feature map)，自定义其他语言接口。优化性能(maximize performance)，线程、队列、分布式计算支持，TensorFlow数据流图不同计算元素分配不同设备，最大化利用硬件资源。应用公司。谷歌、京东、小米、Uber、eBay、Dropbox、Airbnb。2016.4，0.8版支持分布式、多GPU。2016.6，0.9版支持移动设备。2017.2，1.0版Java、Go实验API，专用编译器XLA、调试工具Debugger，tf.transform数据预处理，动态图计算TensorFlow Fold。机器学习赛事。ImageNet ILSVRC(ImageNet Large Scale Visual Recognition Challenge，大规模视觉识别挑战赛)，对象检测、图像识别算法。2010年开始，最大图像识别数据库，1500万张有标记高分辨率图像数据集，22000类别，比寒用1000类别各1000图像，120万训练图像，5万验证图像，15万测试图像。每年邀请知名IT公司测试图片分类系统。Top-1,预测输出概率最高类别错误率。Top-5,预测输出概率前五类别错误率。2016，CUImage目标检测第一，商汤科技、香港中文大学；CUvideo视频物体检测子项目第一，商汤科技、香港中文大学；SenseCUSceneParsing场景分析第一，商汤科技、香港中文大学；Trimps-Soushen目标定位第一，公安部三所NUIST视频物体探测两个子项目第一，南京信息工程大学；Hikvvision场景分类第一，海康威视；Kaggel，2010年成立，数据发掘、数据分析预测竞赛在线平台。公司出数据出钱，计算机科学家、数学家、数据科学家领取任务，提供解决方案。3万到25万美元奖励。天池大数据竞赛，阿里，穿衣搭配、微博互动预测、用户重复购买行为预测，赛题攻略。国内人工智能公司。腾讯优图、阿里云ET、百度无人驾驶，搜狗、云从科技、商汤科技、昆仑万维、格灵深瞳。陌上花科技，衣+(dress+)，图像识别、图像搜索、特体追踪检测是、图片自动化标记、图像视频智能分析、边看边买、人脸识别分析。旷视科技，Face++，人脸识别精度，美颜，支付。科大讯飞，语音识别、语音合成、语言云、分词、词性标注、命名实体识别、依存句法分析、语义角色标注。地平线，嵌入式。参考资料：《TensorFlow技术解析与实战》欢迎付费咨询(150元每小时)，我的微信：qingxingfengzi"}
{"content2":"《Windows Azure Platform 系列文章目录》在上一节内容中，笔者介绍了微软认知服务的概览。在本节中，笔者将详细介绍微软认知服务中的一种：计算机视觉 (Computer Vision) API我的一个客户有需求，他们需要消费者与自己的产品合照，然后上传到服务器并转发到朋友圈。但是为了防止恶意用户上传不健康的照片，需要对图像进行筛查。计算机视觉API的分析图像功能，正好有Adult参数，可以检测图像是否是色情的，正好满足客户的需求。请注意：-　　本文使用的是国内由世纪互联运维的Azure China计算机视觉服务，API参考：https://dev.cognitive.azure.cn/docs/services/56f91f2d778daf23d8ec6739/operations/56f91f2e778daf14a499e1fa-　　如果是使用的是海外的Azure China计算机视觉服务，API参考：https://westus.dev.cognitive.microsoft.com/docs/services/56f91f2d778daf23d8ec6739/operations/56f91f2e778daf14a499e1fa计算机视觉解决的问题：1.分析图像检查图像中发现的视觉内容，分析是否有不健康内容。2.生成缩略图裁剪和生成缩略图3.读取图片中的文字4.识别名人关键步骤主要有：1.创建计算机视觉API，并获得API Key2.了解API参数3.使用API测试控制台调试API，并设置调试API的参数接下来我们进入Demo时间，在开始之前，请先准备Azure China账户第一部分：创建计算机视觉API，并获得API Key1.我们找到需要分析的图片URL，我这里准备了一张人脸的照片：https://leizhangstorage.blob.core.chinacloudapi.cn/azureblog/analyzeimagesample.jpg2.我们登录Azure China管理界面：https://portal.azure.cn3.点击下图的认知服务账户4.点击 创建认知服务账户。如下图：5.定价层，我们选择免费。因为是Demo环境，我们就用免费，如果是生产环境建议选择标准。6.创建完认知服务以后，我们点击密钥，获得访问这个计算机视觉API的Access Key。请保留好这个Key，下面还要继续使用。第二部分：了解API参数1.Request URL: https://api.cognitive.azure.cn/vision/v1.0/analyze[?visualFeatures][&details][&language]2.Request参数(1)visualFeatures参数我们可以设置visualFeatures为：A.Categories:对图像内容进行分类B.Tags:对图像进行标记C.Description:用完整的英文句子描述图像内容D.Faces:检测脸部是否存在。 如果存在，生成坐标，性别和年龄E.ImageType:检测图像是剪贴还是直线绘图F.Color:确定重音颜色，主色，以及图像是否为黑白G.Adult:检测图像是否是色情的（描绘裸露或性行为）。 还会检测到性暗示内容。(2)details:如果设置Celebrities，则可以识别名人(3)language:默认是en，英文可以设置为zh，简体中文3.Request headers(1)Content-Type(2)Ocp-Apim-Subscription-Key:上面的API Access Key4.Request body(1)支持的图像为JPEG, PNG, GIF和BMP(2)图像的尺寸必须小于4MB(3)图像的分辨率至少为50 X 50第三部分：使用API测试控制台调试API，并设置调试API的参数我们拿到上面的API Key，就可以写代码开发了。不过Azure认知服务提供了非常好的控制台，可以方便我们进行API调试1.我们点击：https://dev.cognitive.azure.cn/docs/services/56f91f2d778daf23d8ec6739/operations/56f91f2e778daf14a499e1fa，选择打开API测试控制台2.在API控制台，修改以下内容：Query Parameters(1)visualFeatures，我们输入：Categories,Tags,Description,Faces,ImageType,Color,Adult这样识别多个元素。(2)details，我们不输入信息(3)language，使用默认的enHeaders:(1)Content-Type，我们使用默认值(2)Ocp-Apim-Subscription-Key，我们输入步骤一的API Access KeyRequest body:(1)我们修改为：{\"url\":\"https://leizhangstorage.blob.core.chinacloudapi.cn/azureblog/analyzeimagesample.jpg\"}所有参数的修改内容如下图：Request URL为：https://api.cognitive.azure.cn/vision/v1.0/analyze?visualFeatures=Categories,Tags,Description,Faces,ImageType,Color,Adult&language=en然后我们点击API测试控制台的Send。显示识别结果。下面的结果我就不一一说明了，主要的显示结果有：1.faces，识别出图像中的人脸坐标，性别和年龄2.adultScore，识别出检测图像是否是色情的，分数越高，则图像色情的可能性越大Pragma: no-cache apim-request-id: 8a9e6b8c-3a20-42a0-91e0-52d6fbdc5f9e Strict-Transport-Security: max-age=31536000; includeSubDomains; preload x-content-type-options: nosniff Cache-Control: no-cache Date: Thu, 15 Jun 2017 09:06:16 GMT X-AspNet-Version: 4.0.30319 X-Powered-By: ASP.NET Content-Length: 1595 Content-Type: application/json; charset=utf-8 Expires: -1 { \"categories\": [ { \"name\": \"people_group\", \"score\": 0.9765625 } ], \"adult\": { \"isAdultContent\": false, \"isRacyContent\": false, \"adultScore\": 0.01091344840824604, \"racyScore\": 0.055492393672466278 }, \"tags\": [ { \"name\": \"outdoor\", \"confidence\": 0.99716836214065552 }, { \"name\": \"person\", \"confidence\": 0.99493598937988281 }, { \"name\": \"posing\", \"confidence\": 0.95204299688339233 }, { \"name\": \"group\", \"confidence\": 0.82954329252243042 }, { \"name\": \"people\", \"confidence\": 0.583439290523529 }, { \"name\": \"crowd\", \"confidence\": 0.019400959834456444 } ], \"description\": { \"tags\": [ \"outdoor\", \"person\", \"posing\", \"photo\", \"grass\", \"group\", \"standing\", \"people\", \"man\", \"woman\", \"young\", \"holding\", \"dress\", \"white\", \"court\" ], \"captions\": [ { \"text\": \"a group of people posing for a picture\", \"confidence\": 0.94583615520612 } ] }, \"requestId\": \"8a9e6b8c-3a20-42a0-91e0-52d6fbdc5f9e\", \"metadata\": { \"width\": 600, \"height\": 463, \"format\": \"Jpeg\" }, \"faces\": [ { \"age\": 42, \"gender\": \"Male\", \"faceRectangle\": { \"left\": 117, \"top\": 159, \"width\": 95, \"height\": 95 } }, { \"age\": 54, \"gender\": \"Male\", \"faceRectangle\": { \"left\": 490, \"top\": 111, \"width\": 90, \"height\": 90 } }, { \"age\": 61, \"gender\": \"Female\", \"faceRectangle\": { \"left\": 17, \"top\": 153, \"width\": 85, \"height\": 85 } }, { \"age\": 33, \"gender\": \"Female\", \"faceRectangle\": { \"left\": 386, \"top\": 166, \"width\": 81, \"height\": 81 } }, { \"age\": 15, \"gender\": \"Female\", \"faceRectangle\": { \"left\": 235, \"top\": 159, \"width\": 77, \"height\": 77 } }, { \"age\": 6, \"gender\": \"Female\", \"faceRectangle\": { \"left\": 323, \"top\": 163, \"width\": 67, \"height\": 67 } } ], \"color\": { \"dominantColorForeground\": \"White\", \"dominantColorBackground\": \"White\", \"dominantColors\": [ \"White\", \"Brown\" ], \"accentColor\": \"4E5D1A\", \"isBWImg\": false }, \"imageType\": { \"clipArtType\": 0, \"lineDrawingType\": 0 } }"}
{"content2":"原文链接：https://mp.weixin.qq.com/s/nXFVTorYOm5LjRV5Cic2_w如果你不能用数据表示你所知，那么说明你对它所知不多；如果你对它所知不多，那么你就无法控制它；如果你无法控制它，那么就只能靠运气了。—— 陈希章不久前，我开始正儿八经地系统地学习人工智能，并且发起了一个结对学习的活动，目前已经有将近20位同学一起结对，详情请参考下面文章的说明—— 约你六个月时间一起学习实践人工智能 。目前仍接受报名，但我会对人数总量做一定的控制，并且各位在加入之前必须想清楚自己能否真的花时间坚持下去，一定时间没有学习进度的会被请出群。我之前承诺大家，会将在学习过程中的笔记分享出来。这是第一篇，也是我完成第一门课《Introduction to Artificial Intelligence》第一单元《Machine Learning》的一些心得。我写的笔记，只是学习过程中的一些记录，或者一些思考，很有可能会有很多地方有错误，欢迎大家指出，帮助我提高。这门课是人工智能入门，它的内容分为四个部分：机器学习概述，语音和通信，计算机视觉，对话平台。本节介绍了机器学习的基本概念，主要应用场景，并着重通过以Azure Machine Learning Studio为载体，讲解了导入数据、建立和训练、验证模型，发布成Web Service的全过程，通过完成本节学习，你可以对Machine Learning有些基本概念，而课后的练习是一个完整的范例，你可以了解如何通过分类算法建立模型来实现糖尿病的预测。（它分别使用了逻辑回归和决策树算法做比较，并最终选择了决策树作为最优解）。机器学习的定义，有兴趣可以参考维基百科的说明：https://zh.wikipedia.org/wiki/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0机器学习是人工智能的一个分支。人工智能的研究历史有着一条从以\"推理\"为重点，到以\"知识\"为重点，再到以\"学习\"为重点的自然、清晰的脉络。显然，机器学习是实现人工智能的一个途径，即以机器学习为手段解决人工智能中的问题。机器学习在近30多年已发展为一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、计算复杂性理论等多门学科。机器学习理论主要是设计和分析一些让计算机可以自动\"学习\"的算法。机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。因为学习算法中涉及了大量的统计学理论，机器学习与推断统计学联系尤为密切，也被称为统计学习理论。算法设计方面，机器学习理论关注可以实现的，行之有效的学习算法。很多推论问题属于无程序可循难度，所以部分的机器学习研究是开发容易处理的近似算法。看了这么一大段的介绍，其实还是会比较晕。其实最简单理解的话，机器学习最重要的研究目标就是从大量的数据中找出来一些规律，并且能利用该规律进行预测。经过这么多年的发展，解决绝大部分问题的算法都已经存在了，我们现在很多时候要做的是收集和准备数据（包括清洗和整理），然后根据业务领域经验建模，并且选择不同的算法去训练模型、验证模型，发现和逼近最好的预测模型。下图是目前Azure Machine Learning 中支持的算法列表。机器学习根据其原理分为监督学习无监督学习半监督学习监督学习与非监督学习的根本区别在于训练集数据是否需要人为地进行标记。典型的监督学习算法包括回归和分类，而典型的无监督学习算法是聚类。半监督学习是介于两者之间的。而关于回归（Regression）和分类（Classification），又有一些明显的区别，我倾向于采纳下面知乎网友的回答。那么，回到我们这堂课的命题：通过机器学习来预测某个病人是否为糖尿病（及其概率），这是一个定性问题，它的预测是离散的，而不是连续的，所以这是一个分类的任务。假设我们手工有15000个病例样本，分别记录了他们的血糖，血压，年龄等信息，以及他们是否确诊为糖尿病的数据。（这个Diabetic字段非常重要，而这其实也就是需要人工标记的关键信息）另外，还有一份数据是病人和医生的对照表，在本课程学习和作业中，虽然并不是必须的，但这符合真实场景的需要。对于人工智能和机器学习而言，相比较看起来很酷炫的建模、训练、验证，其实很重要的工作都是在收集数据，有好的数据才会有真正有价值的人工智能。而收集数据中最关键的一个工作是定义哪些数据需要收集，例如上述例子，为什么我们去分析糖尿病时需要选择这些数据，而不是其他的。这里面其实已经有一个建模的过程，而这部分是计算机科学无法实现的，它所依赖的是自然科学和专家经验。如果只是做这种预测，那么我要说，其实我在十几年前就已经能做出来了，那时候我记得\"人工智能（Artificial Intelligence）\"或\"机器学习（Machine Learning）\"并没有现在这么流行，人们更热衷于讨论\"商业智能（Business Intelligence）\"和\"数据挖掘（Data Mining）\"，我对SQL Server 的BI 和Data Mining还算有一些研究，所以做这种预测还是比较轻松的。今时不同往日，SQL Server仍然还有这些能力。但真正的大数据时代，我们可能还需要云端的解决方案。微软的Azure Machine Learning 解决方案就是其中之一，而Azure Machine Learning Studio会提供你需要的一切。下图是我创建好的Training experiment （用来做训练的实验）作为一个强大的Machine Learning的工具，它预设了上百个组件，并且可以随时对数据进行可视化分析。对于回归和分类算法，通常我们会在进行数据连接，规范化处理后，对数据集进行拆分，一部分（通常70%）用于训练模型，另一部分（通常30%）用于验证模型。如下图所示：在最后一步是验证模型，通常我们会选择多个算法比对其输出结果。如下图所知这种输出，Accuracy 越高，则表示准确性越高，可信度也就越高。模型训练好之后，接下来就是怎么用于预测了。你可以通过创建一个Predictive experiment来实现这个需求。请注意，它其实跟之前的Training experiment看起来很像，只不过你仔细看的话，会发现input处不一样，而且中间的一些组件，它是引用到了Training experiment中的。你可以一键将其发布为一个Web Service，以便支持客户端调用。下图是在Excel中通过一个Add-in进行预测分析的效果（支持批量对数据集进行预测）下图是我通过Python调用REST API进行预测的范例欢迎大家关注我的《人工智能学习笔记》，请关注本公众号，并扫描下面二维码收藏本系列文章。"}
{"content2":"第八章 目标跟踪1检测目标的移动基本的运动检测,示例代码如下:import cv2 import numpy as np # 捕获摄像头图像 camera = cv2.VideoCapture(0) # es = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (10, 10)) kernel = np.ones((5, 5), np.uint8) background = None while (True):     ret, frame = camera.read()     # 将第一帧设为图像的背景     if background is None:         # 转换颜色空间         background = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)         # 高斯模糊         background = cv2.GaussianBlur(background, (21, 21), 0)         continue     # 转换颜色空间并作模糊处理     gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)     gray_frame = cv2.GaussianBlur(gray_frame, (21, 21), 0)     # 取得差分图     diff = cv2.absdiff(background, gray_frame)     diff = cv2.threshold(diff, 25, 255, cv2.THRESH_BINARY)[1]     # 膨胀     diff = cv2.dilate(diff, es, iterations=2)     # 得到图像中目标的轮廓     image, cnts, hierarchy = cv2.findContours(diff.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)     for c in cnts:         if cv2.contourArea(c) < 1500:             continue         # 计算矩形边框         (x, y, w, h) = cv2.boundingRect(c)         # 绘制矩形         cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)     # 显示图像     cv2.imshow('contours', frame)     cv2.imshow('dif', diff)     if cv2.waitKey(int(1000 / 12)) & 0xFF == ord('q'):         break cv2.destroyAllWindows() camera.release()运行结果如下:2背景分割器 knn mog2和GMGOpencv3有三种背景分割器K-nearest(knn)Mixture of Gaussians(MOG2)Geometric multigid(GMC)backgroundSubtractor用于分割前景和背景示例代码如下:import cv2 import numpy as np cv2.ocl.setUseOpenCL(False) cap = cv2.VideoCapture(0) mog = cv2.createBackgroundSubtractorMOG2() while (True):     ret, frame = cap.read()     fgmask = mog.apply(frame)     cv2.imshow('frame', fgmask)     if cv2.waitKey(30) & 0xFF == ord('q'):         break cap.release() cv2.destroyAllWindows()运行结果如下:使用backgroundSubtractorKNN来实现运动检测示例代码如下:import cv2 cv2.ocl.setUseOpenCL(False) bs = cv2.createBackgroundSubtractorKNN(detectShadows=True) # 读取本地视频 camera = cv2.VideoCapture('../traffic.flv') while (True):     ret, frame = camera.read()     fgmask = bs.apply(frame.copy())     # 设置阈值     th = cv2.threshold(fgmask,  # 源图像                        244,  # 阈值                        255,  # 最大值                        cv2.THRESH_BINARY)[1]  # 阈值类型     # 膨胀     dilated = cv2.dilate(th,  # 源图像                          cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3)),  # 内核                          iterations=2)  # 腐蚀次数     # 查找图像中的目标轮廓     image, contours, hier = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)     for c in contours:         if cv2.contourArea(c) > 1600:             (x, y, w, h) = cv2.boundingRect(c)             cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 255, 0), 2)     cv2.imshow('mog', fgmask)  # 分割前景与背景     cv2.imshow('thresh', th)  #     cv2.imshow('detection', frame)  # 运动检测结果     if cv2.waitKey(30) & 0xFF == 27:         break camera.release() cv2.destroyAllWindows()运行结果如下:均值漂移meanShift示例代码如下:import cv2 import numpy as np # 取得摄像头图像 cap = cv2.VideoCapture(0) ret, frame = cap.read() # 设置跟踪窗体大小 r, h, c, w = 10, 200, 10, 200 track_window = (c, r, w, h) # 提取roi roi = frame[r:r + h, c:c + w] # 转换颜色空间 hsv_roi = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV) # 根据阈值构建掩码 mask = cv2.inRange(hsv_roi, np.array((100., 30., 32.)), np.array((180., 120., 255.))) # 计算roi图形的彩色直方图 roi_hist = cv2.calcHist([hsv_roi], [0], mask, [180], [0, 180]) cv2.normalize(roi_hist, roi_hist, 0, 255, cv2.NORM_MINMAX) # 指定停止条件 term_crit = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1) while (True):     ret, frame = cap.read()     if ret == True:         # 更换颜色空间         hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)         # histogram back projection calculation 直方图反向投影         dst = cv2.calcBackProject([hsv], [0], roi_hist, [0, 180], 1)         # 均值漂移         ret, track_window = cv2.meanShift(dst, track_window, term_crit)         # 绘制矩形显示图像         x, y, w, h = track_window         img2 = cv2.rectangle(frame, (x, y), (x + w, y + h), 255, 2)         cv2.imshow('img2', img2)         # esc退出         if cv2.waitKey(60) & 0xFF == 27:             break     else:         break cv2.destroyAllWindows() cap.release()运行结果如下:彩色直方图calHist函数函数原型:def calcHist(images, #源图像channels, #通道列表mask,#可选的掩码histSize, #每个维度下直方图数组的大小ranges,#每一个维度下直方图bin的上下界的数组hist=None,#输出直方图是一个[]维稠密度的数组accumulate=None)#累计标志Camshift示例代码如下:#!/usr/bin/env python # -*- coding: utf-8 -*- # @Time    : 2016/12/15 16:48 # @Author  : Retacn # @Site    : camshift实现物体跟踪 # @File    : camshift.py # @Software: PyCharm __author__ = \"retacn\" __copyright__ = \"property of mankind.\" __license__ = \"CN\" __version__ = \"0.0.1\" __maintainer__ = \"retacn\" __email__ = \"zhenhuayue@sina.com\" __status__ = \"Development\" import cv2 import numpy as np # 取得摄像头图像 cap = cv2.VideoCapture(0) ret, frame = cap.read() # 设置跟踪窗体大小 r, h, c, w = 300, 200, 400, 300 track_window = (c, r, w, h) # 提取roi roi = frame[r:r + h, c:c + w] # 转换颜色空间 hsv_roi = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV) # 根据阈值构建掩码 mask = cv2.inRange(hsv_roi, np.array((100., 30., 32.)), np.array((180., 120., 255.))) # 计算roi图形的彩色直方图 roi_hist = cv2.calcHist([hsv_roi], [0], mask, [180], [0, 180]) cv2.normalize(roi_hist, roi_hist, 0, 255, cv2.NORM_MINMAX) # 指定停止条件 term_crit = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1) while (True):     ret, frame = cap.read()     if ret == True:         # 更换颜色空间         hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)         # histogram back projection calculation 直方图反向投影         dst = cv2.calcBackProject([hsv], [0], roi_hist, [0, 180], 1)         # 均值漂移         ret, track_window = cv2.CamShift(dst, track_window, term_crit)         # 绘制矩形显示图像         pts = cv2.boxPoints(ret)         pts = np.int0(pts)         img2 = cv2.polylines(frame, [pts], True, 255, 2)         cv2.imshow('img2', img2)         # esc退出         if cv2.waitKey(60) & 0xFF == 27:             break     else:         break cv2.destroyAllWindows() cap.release()运行结果如下:4 卡尔曼滤波器函数原型为：def KalmanFilter(dynamParams=None,#状态的维度measureParams=None, #测量的维度controlParams=None,#控制的维度type=None)#矩阵的类型示例代码如下：import cv2 import numpy as np # 创建空帧 frame = np.zeros((800, 800, 3), np.uint8) # 测量坐标 last_measurement = current_measurement = np.array((2, 1), np.float32) # 鼠标运动预测 last_prediction = current_predication = np.zeros((2, 1), np.float32) def mousemove(event, x, y, s, p):     # 设置全局变量     global frame, measurements, current_measurement, last_measurement, current_predication, last_prediction     last_prediction = current_predication     last_measurement = current_measurement     current_measurement = np.array([[np.float32(x)], [np.float32(y)]])     kalman.correct(current_measurement)     current_predication = kalman.predict()     # 实际移动起始点     lmx, lmy = last_measurement[0], last_measurement[1]     cmx, cmy = current_measurement[0], current_measurement[1]     # 预测线起止点     lpx, lpy = last_prediction[0], last_prediction[1]     cpx, cpy = current_predication[0], current_predication[1]     # 绘制连线     cv2.line(frame, (lmx, lmy), (cmx, cmy), (0, 100, 0))  # 绿色     cv2.line(frame, (lpx, lpy), (cpx, cpy), (0, 0, 200))  # 红色 # 创建窗体 cv2.namedWindow('mouse_detection') # 注册鼠标事件的回调函数 cv2.setMouseCallback('mouse_detection', mousemove) # 卡尔曼滤波器 kalman = cv2.KalmanFilter(4, 2) kalman.measurementMatrix = np.array([[1, 0, 0, 0], [0, 1, 0, 0]], np.float32) kalman.transitionMatrix = np.array([[1, 0, 1, 0], [0, 1, 0, 1], [0, 0, 1, 0], [0, 0, 0, 1]], np.float32) kalman.processNoiseCov = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]], np.float32) * 0.03 while (True):     cv2.imshow('mouse_detection', frame)     if cv2.waitKey(30) & 0xFF == 27:         break cv2.destroyAllWindows()运行结果如下：一个基于行人跟踪的例子示例代码如下:import cv2 import numpy as np import os.path as path import argparse font = cv2.FONT_HERSHEY_SIMPLEX parser = argparse.ArgumentParser() parser.add_argument(\"-a\", \"--algorithm\",                     help=\"m (or nothing) for meanShift and c for camshift\") args = vars(parser.parse_args()) # 计算矩阵中心(行人位置) def center(points):     x = (points[0][0] + points[1][0] + points[2][0] + points[3][0]) / 4     y = (points[0][1] + points[1][1] + points[2][1] + points[3][1]) / 4     # print(np.array([np.float32(x), np.float32(y)], np.float32))     # [ 588.   257.5]     return np.array([np.float32(x), np.float32(y)], np.float32) # 行人 class Pedestrian():     def __init__(self, id, frame, track_window):         self.id = int(id)  # 行人id         x, y, w, h = track_window  # 跟踪窗体         self.track_window = track_window         # 更换颜色空间         self.roi = cv2.cvtColor(frame[y:y + h, x:x + w], cv2.COLOR_BGR2HSV)         # 计算roi图形的彩色直方图         roi_hist = cv2.calcHist([self.roi], [0], None, [16], [0, 180])         self.roi_hist = cv2.normalize(roi_hist, roi_hist, 0, 255, cv2.NORM_MINMAX)         # 设置卡尔曼滤波器         self.kalman = cv2.KalmanFilter(4, 2)         self.kalman.measurementMatrix = np.array([[1, 0, 0, 0], [0, 1, 0, 0]], np.float32)         self.kalman.transitionMatrix = np.array([[1, 0, 1, 0], [0, 1, 0, 1], [0, 0, 1, 0], [0, 0, 0, 1]], np.float32)         self.kalman.processNoiseCov = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]],                                                np.float32) * 0.03         # 测量坐标         self.measurement = np.array((2, 1), np.float32)         # 鼠标运动预测         self.predication = np.zeros((2, 1), np.float32)         # 指定停止条件         self.term_crit = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1)         self.center = None         self.update(frame)     def __del__(self):         print('Pedestrian %d destroyed' % self.id)     # 更新图像帧     def update(self, frame):         # 更换颜色空间         hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)         # histogram back projection calculation 直方图反向投影         back_project = cv2.calcBackProject([hsv], [0], self.roi_hist, [0, 180], 1)         # camshift         if args.get('algorithm') == 'c':             ret, self.track_window = cv2.CamShift(back_project, self.track_window, self.term_crit)             # 绘制跟踪框             pts = cv2.boxPoints(ret)             pts = np.int0(pts)             self.center = center(pts)             cv2.polylines(frame, [pts], True, 255, 1)         # 均值漂移         if not args.get('algorithm') or args.get('algorithm') == 'm':             ret, self.track_window = cv2.meanShift(back_project, self.track_window, self.term_crit)             # 绘制跟踪框             x, y, w, h = self.track_window             self.center = center([[x, y], [x + w, y], [x, y + h], [x + w, y + h]])             cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 255, 0), 2)         self.kalman.correct(self.center)         prediction = self.kalman.predict()         cv2.circle(frame, (int(prediction[0]), int(prediction[1])), 4, (0, 255, 0), -1)         # 计数器         cv2.putText(frame, 'ID: %d --> %s' % (self.id, self.center), (11, (self.id + 1) * 25 + 1), font, 0.6, (0, 0, 0),                     1, cv2.LINE_AA)         # 跟踪窗口坐标         cv2.putText(frame, 'ID: %d --> %s' % (self.id, self.center), (10, (self.id + 1) * 25), font, 0.6, (0, 255, 0),                     1, cv2.LINE_AA) def main():     # 加载视频     # camera = cv2.VideoCapture('../movie.mpg')     # camera = cv2.VideoCapture('../traffic.flv')     camera = cv2.VideoCapture('../768x576.avi')     # 初始化背景分割器     history = 20     bs = cv2.createBackgroundSubtractorKNN(detectShadows=True)     # 创建显示主窗口     cv2.namedWindow('surveillance')     pedestrians = {}  # 行人字典     firstFrame = True     frames = 0     fourcc = cv2.VideoWriter_fourcc(*'XVID')     out = cv2.VideoWriter('../output.avi', fourcc, 20.0, (640, 480))     while (True):         print('----------------------frmae %d----------------' % frames)         grabbed, frane = camera.read()         if (grabbed is False):             print(\"failed to grab frame\")             break         ret, frame = camera.read()         fgmask = bs.apply(frame)         if frames < history:             frames += 1             continue         # 设置阈值         th = cv2.threshold(fgmask.copy(), 127, 255, cv2.THRESH_BINARY)[1]         # 腐蚀         th = cv2.erode(th, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3)), iterations=2)         # 膨胀         dilated = cv2.dilate(th, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (8, 3)), iterations=2)         # 查找轮廓         image, contours, hier = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)         counter = 0         for c in contours:             if cv2.contourArea(c) > 500:                 # 边界数组                 (x, y, w, h) = cv2.boundingRect(c)                 # 绘制矩形                 cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 1)                 if firstFrame is True:                     pedestrians[counter] = Pedestrian(counter, frame, (x, y, w, h))                 counter += 1         # 更新帧内容         for i, p in pedestrians.items():             p.update(frame)         # false 只跟踪已有的行人         # firstFrame = True         firstFrame = False         frames += 1         # 显示         cv2.imshow('surveillance', frame)         out.write(frame)         if cv2.waitKey(120) & 0xFF == 27:  # esc退出             break     out.release()     camera.release() if __name__ == \"__main__\":     main()运行结果如下:"}
{"content2":"机器视觉（Machine Vision, MV） & 计算机视觉(Computer Vision, CV) 从学科分类上， 二者都被认为是人工智能（ Artificial Intelligence ）下属科目.机器视觉就是用机器代替人眼来做测量和判断。机器视觉系统是通过机器视觉产品（即图像摄取装置，分CMOS和CCD两种）将被摄取目标转换成图像信号，传送给专用的图像处理系统，得到被摄目标的形态信息，根据像素分布和亮度、颜色等信息，转变成数字化信号;图像系统对这些信号进行各种运算来抽取目标的特征，进而根据判别的结果来控制现场的设备动作。计算机视觉是指用计算机实现人的视觉功能――对客观世界的三维场景的感知、识别和理解。它是一门研究如何使机器“看”的科学，更进一步的说，就是是指用摄影机和电脑代替人眼对目标进行识别、跟踪和测量等机器视觉，并进一步做图形处理，使电脑处理成为更适合人眼观察或传送给仪器检测的图像。计算机视觉也可以看作是研究如何使人工系统从图像或多维数据中“感知”的科学。它的最终研究目标就是使计算机能象人那样通过视觉观察和理解世界，具有自主适应环境的能力。这里主要有两类方法：一类是仿生学的方法，参照人类视觉系统的结构原理，建立相应的处理模块完成类似的功能和工作；另一类是工程的方法，从分析人类视觉过程的功能着手，并不去刻意模拟人类视觉系统内部结构，而仅考虑系统的输入和输出，并采用任何现有的可行的手段实现系统功能。两者用到的技术类似，主要区别是运用场景、侧重方面不同，MV 偏工业车间应用，更多注重广义图像信号（激光，摄像头）与自动化控制（生产线）方面的应用。侧重对量的分析，如零件直径。CV偏软件算法， 更多关注人相关的应用，注重（2D, 3D）图像信号本身的研究以及和图像相关的交叉学科研究（医学图像分析，地图导航）,侧重对质的分析，如人脸识别、车牌识别。计算机视觉的应用场景相对复杂，要识别的物体类型也多，形状不规则，规律性不强。有些时候甚至很难用客观量作为识别的依据，比如识别年龄，性别。所以深度学习比较适合计算机视觉。而且光线，距离，角度等前提条件，往往是动态的，所以对于准确度要求，一般来说要低一些。机器视觉则刚好相反，场景相对简单固定，识别的类型少(在同一个应用中)，规则且有规律，但对准确度，处理速度要求都比较高。关于速度，一般机器视觉的分辨率远高于计算机视觉，而且往往要求实时，所以处理速度很关键，目前基本上不适合采用深度学习。商业方面，计算机视觉的应用面更广一些，毕竟很多业务是跟人相关，比如人脸识别，行为分析等，很多垂直领域都有计算机视觉潜在需求，相对来说，更适合创业；而机器视觉顾名思义，业务主要跟机器相关，而且对准确度甚至安全性要求很高，也就在资质品牌方面有较高的门槛，所以寡头垄断严重，一般来说，更适合上班而不是创业。链接：https://www.zhihu.com/question/23183532/answer/105619829链接：https://www.zhihu.com/question/23183532/answer/23896265来自为知笔记(Wiz)"}
{"content2":"CVonlinehttp://homepages.inf.ed.ac.uk/rbf/CVonlinehttp://homepages.inf.ed.ac.uk/rbf/CVonline/unfolded.htmhttp://homepages.inf.ed.ac.uk/rbf/CVonline/CVentry.htm李子青的大作：Markov Random Field Modeling in Computer Visionhttp://www.cbsr.ia.ac.cn/users/szli/mrf_book/book.htmlHandbook of Face Recognition (PDF)http://www.umiacs.umd.edu/~shaohua/papers/zhou04hfr.pdf张正友的有关参数鲁棒估计著作：Parameter Estimation Techniques:A Tutorial with Application to Conic Fittinghttp://research.microsoft.com/~zhang/INRIA/Publis/Tutorial-Estim/Main.htmlAndrea Fusiello“计算机视觉中的几何”教程：Elements of Geometric Computer Visionhttp://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/FUSIELLO4/tutorial.html#x1-520007有关马尔可夫蒙特卡罗方法的资料：An introduction to Markov chain Monte Carlohttp://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/SENEGAS/mcmc.htmlMarkov Chain Monte Carlo for Computer Vision--- A tutorial at ICCV05http://civs.stat.ucla.edu/MCMC/MCMC_tutorial.htm有关独立成分分析（Independent Component Analysis , ICA）的资料：An ICA-Pagehttp://www.cnl.salk.edu/~tony/ica.htmlFast ICAhttp://www.cis.hut.fi/projects/ica/fastica/The Kalman Filter (介绍卡尔曼滤波器的终极网页)http://www.cs.unc.edu/~welch/kalman/index.htmlCached k-d tree search for ICP algorithmshttp://kos.informatik.uni-osnabrueck.de/download/3dim2007/paper.html几个计算机视觉研究工具Machine Vision Toolbox for Matlabhttp://www.petercorke.com/Machine%20Vision%20Toolbox.htmlMatlab and Octave Function for Computer Vision and Image Processinghttp://www.csse.uwa.edu.au/~pk/research/matlabfns/Bayes Net Toolbox for Matlabhttp://www.cs.ubc.ca/~murphyk/Software/BNT/bnt.htmlOpenCV (Chinese)http://www.opencv.org.cn/index.php/%E9%A6%96%E9%A1%B5Gandalf (A Computer Vision and Numerical Algorithm Labrary)http://gandalf-library.sourceforge.net/CMU Computer Vision Home Pagehttp://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.htmlMachine Learning Resource Linkshttp://www.cse.ust.hk/~ivor/resource.htmThe Bayesian Filtering Libraryhttp://www.orocos.org/bflOptical Flow Algorithm Evaluation (提供了一个动态贝叶斯网络框架，例如递归信息处理与分析、卡尔曼滤波、粒子滤波、序列蒙特卡罗方法等，C++写的)http://of-eval.sourceforge.net/MATLAB code for ICP algorithmhttp://www.usenet.com/newsgroups/comp.graphics.visualization/msg00102.html牛人主页：朱松纯 （Song-Chun Zhu）http://www.stat.ucla.edu/~sczhu/David Lowe (SIFT) (很帅的一个老头哦 ^ ^)http://www.cs.ubc.ca/~lowe/Andrea Vedaldi (SIFT)http://vision.ucla.edu/~vedaldi/index.htmlPedro F. Felzenszwalbhttp://people.cs.uchicago.edu/~pff/Dougla Dlanman (Brown的一个研究生，在其主页上搜集了大量算法教程和源码)http://mesh.brown.edu/dlanman/courses.htmlJianbo Shi (Ncuts 的始作俑者)http://www.cis.upenn.edu/~jshi/Active Vision Group (Oxford的一个机器视觉研究团队，特色是SLAM，监视，导航)http://www.robots.ox.ac.uk/ActiveVision/index.htmlJuyang Weng（机器学习的专家，Autonomous Mental Development 是其特色）http://www.cse.msu.edu/~weng/测试图片或视频：Middlebury College‘s Stereo Vision Data Sethttp://cat.middlebury.edu/stereo/data.htmlIntelligent Vehicle:IVSourcewww.ivsoruce.netRobot Carhttp://www.plyojump.com/robot_cars.htmlHow to Build a Robot: The Computer Vision Parthttp://www.societyofrobots.com/programming_computer_vision_tutorial.shtml CVonlinehttp://homepages.inf.ed.ac.uk/rbf/CVonlinehttp://homepages.inf.ed.ac.uk/rbf/CVonline/unfolded.htmhttp://homepages.inf.ed.ac.uk/rbf/CVonline/CVentry.htm李子青的大作：Markov Random Field Modeling in Computer Visionhttp://www.cbsr.ia.ac.cn/users/szli/mrf_book/book.htmlHandbook of Face Recognition (PDF)http://www.umiacs.umd.edu/~shaohua/papers/zhou04hfr.pdf张正友的有关参数鲁棒估计著作：Parameter Estimation Techniques:A Tutorial with Application to Conic Fittinghttp://research.microsoft.com/~zhang/INRIA/Publis/Tutorial-Estim/Main.htmlAndrea Fusiello“计算机视觉中的几何”教程：Elements of Geometric Computer Visionhttp://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/FUSIELLO4/tutorial.html#x1-520007有关马尔可夫蒙特卡罗方法的资料：An introduction to Markov chain Monte Carlohttp://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/SENEGAS/mcmc.htmlMarkov Chain Monte Carlo for Computer Vision--- A tutorial at ICCV05http://civs.stat.ucla.edu/MCMC/MCMC_tutorial.htm有关独立成分分析（Independent Component Analysis , ICA）的资料：An ICA-Pagehttp://www.cnl.salk.edu/~tony/ica.htmlFast ICAhttp://www.cis.hut.fi/projects/ica/fastica/The Kalman Filter (介绍卡尔曼滤波器的终极网页)http://www.cs.unc.edu/~welch/kalman/index.htmlCached k-d tree search for ICP algorithmshttp://kos.informatik.uni-osnabrueck.de/download/3dim2007/paper.html几个计算机视觉研究工具Machine Vision Toolbox for Matlabhttp://www.petercorke.com/Machine%20Vision%20Toolbox.htmlMatlab and Octave Function for Computer Vision and Image Processinghttp://www.csse.uwa.edu.au/~pk/research/matlabfns/Bayes Net Toolbox for Matlabhttp://www.cs.ubc.ca/~murphyk/Software/BNT/bnt.htmlOpenCV (Chinese)http://www.opencv.org.cn/index.php/%E9%A6%96%E9%A1%B5Gandalf (A Computer Vision and Numerical Algorithm Labrary)http://gandalf-library.sourceforge.net/CMU Computer Vision Home Pagehttp://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.htmlMachine Learning Resource Linkshttp://www.cse.ust.hk/~ivor/resource.htmThe Bayesian Filtering Libraryhttp://www.orocos.org/bflOptical Flow Algorithm Evaluation (提供了一个动态贝叶斯网络框架，例如递归信息处理与分析、卡尔曼滤波、粒子滤波、序列蒙特卡罗方法等，C++写的)http://of-eval.sourceforge.net/MATLAB code for ICP algorithmhttp://www.usenet.com/newsgroups/comp.graphics.visualization/msg00102.html牛人主页：朱松纯 （Song-Chun Zhu）http://www.stat.ucla.edu/~sczhu/David Lowe (SIFT) (很帅的一个老头哦 ^ ^)http://www.cs.ubc.ca/~lowe/Andrea Vedaldi (SIFT)http://vision.ucla.edu/~vedaldi/index.htmlPedro F. Felzenszwalbhttp://people.cs.uchicago.edu/~pff/Dougla Dlanman (Brown的一个研究生，在其主页上搜集了大量算法教程和源码)http://mesh.brown.edu/dlanman/courses.htmlJianbo Shi (Ncuts 的始作俑者)http://www.cis.upenn.edu/~jshi/Active Vision Group (Oxford的一个机器视觉研究团队，特色是SLAM，监视，导航)http://www.robots.ox.ac.uk/ActiveVision/index.htmlJuyang Weng（机器学习的专家，Autonomous Mental Development 是其特色）http://www.cse.msu.edu/~weng/测试图片或视频：Middlebury College‘s Stereo Vision Data Sethttp://cat.middlebury.edu/stereo/data.htmlIntelligent Vehicle:IVSourcewww.ivsoruce.netRobot Carhttp://www.plyojump.com/robot_cars.htmlHow to Build a Robot: The Computer Vision Parthttp://www.societyofrobots.com/programming_computer_vision_tutorial.shtml"}
{"content2":"近年来，计算机视觉取得了很大进展。这些是我将在这里提到的主题内容：技术：人脸检测：Haar，HOG，MTCNN，Mobilenet面部识别：CNN，Facenet对象识别：alexnet，inceptionnet，resnet迁移学习：在一个新主题上用很少的资源重新训练大型神经网络图像分割：rcnnGAN计算机视觉硬件：选择什么，GPU很重要集成视觉的UI应用程序：ownphotos应用：个人照片组织自动驾驶汽车自主无人机解决验证码/ OCR过滤基于图片的网站/应用程序的图片自动标记应用的图片从视频中提取信息（电视节目、电影）视觉问题回答艺术关注的人：重要的深度学习创始人：Andrew ng，Yann lecun，Bengio yoshua，Hinton joffrey课程 ：deep learning@ courseramachine learning@ coursera相关领域：深度强化学习：使用cnn作为输入层，查看ppo和dqn与nlp的交互：lstm 2 cnn人脸检测面部检测是关于在面部周围放置盒子人脸检测是检测人脸的其中一项任务。有几种算法可以做到这一点。https://github.com/nodefluxio/face-detector-benchmark提供了这些方法的速度基准，并且有易于重用的实现代码。Haar 分类器Haar 特征它们是自2000年以来在opencv中出现的旧计算机视觉方法。它是一种机器学习模型，具有专门用于对象检测的功能。 Haar 分类器速度快但准确度低。请参阅https://docs.opencv.org/3.4.3/d7/d8b/tutorial_py_face_detection.html中有关如何使用它的更长解释和示例HOG：方向梯度直方图方向梯度直方图HOG是一种新的生成对象检测功能的方法：它自2005年开始使用。它基于计算图像像素的梯度，然后将这些特征馈送到机器学习算法中，例如SVM。它具有比haar分类器更好的精度。它的一个实现在dlib中。这是在face_recognition（https://github.com/ageitgey/face_recognition）库中。MTCNN一种使用CNN变化来检测图像的新方法，精度更高但速度稍慢。请参阅https://kpzhang93.github.io/MTCNN_face_detection_alignment/index.htmlMobileNet这是我这些天用于面部检测的最好和最快的方法，基于通用移动网络架构。请参阅https://arxiv.org/abs/1704.04861物体检测对许多物体进行物体检测可以使用与面部检测类似的方法来实现对象检测。这里有2篇文章介绍了实现它的最新方法。这些方法有时也提供了对象类（实现对象识别）：（https://towardsdatascience.com/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c r-fcnhttps://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e a comparison of r-cnn, fast r-cnn, faster r-cnn and yolo）卷积神经网络最近深度学习的进展使新架构取得了很大成功。使用许多卷积层的神经网络就是其中之一。卷积层利用图像的2D结构在神经网络的下一层中生成有用信息。有关什么是卷积的详细说明，请参阅https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1。卷积层物体识别对象识别是将对象分类为类别（如猫，狗，......）的一般问题基于卷积的深度神经网络已被用于在此任务上取得很好的效果。ILSVR会议一直在ImageNet上举办竞赛（http://www.image-net.org/许多图片的数据库，包括猫、狗等物品标签）更成功的神经网络现在已经使用越来越多的层。ResNet架构是迄今为止对对象进行分类的最佳选择。Resnet架构要正确地训练它，需要使用数百万张图像，即使使用数十个昂贵的GPU也仍然需要花费大量时间。这就是为什么每次都不需要在这些大数据集上进行重新训练的方法非常有用的原因。迁移学习和嵌入就是采用的这样的方法。有关resnet的预训练模型，请访问https://github.com/tensorflow/tensor2tensor#image-classification人脸识别面部识别就是要弄清楚谁是一张脸。历史方法解决该任务的历史方法是将特征工程应用于标准机器学习（例如svm）或应用深度学习方法进行对象识别。这些方法的问题是它们需要每个人的大量数据。实际上，数据并不总是可用的。Facenet谷歌研究人员在2015年推出了Facenet（ https://arxiv.org/abs/1503.03832）。它提出了一种识别面部的方法，但却不需要为每个人提供大量的面部样本。它的工作方式是拍摄大量面孔的图片数据集（例如http://vis-www.cs.umass.edu/lfw/）。然后采用现有的计算机视觉架构，例如初始（或resnet），然后用计算面部嵌入的层替换对象识别NN的最后一层。对于数据集中的每个人，（负样本、正样本、第二正样本）选择三个面（使用启发法）并将其馈送到神经网络，这产生了3个嵌入。在这3次嵌入中，计算三重态损失，这使得正样本与任何其他正样本之间的距离最小化，并且最大化位置样本与任何其他负样本之间的距离。三元组损失最终结果是每个面（即使在原始训练集中不存在的面）现在也可以表示为一个嵌入，它与其他人的面部嵌入有很大距离的嵌入（128数字的向量）。然后，这些嵌入可以与任何机器学习模型（甚至简单的诸如knn）一起使用来识别人。关于facenet和face embedding非常有趣的事情就是使用它你可以识别只有几张照片或者只有一张照片的人。这是它的一个tensorflow实现：https：//github.com/davidsandberg/facenet这是人脸识别管道背后的思想的一个很酷的应用，而不是识别熊脸：https：//hypraptive.github.io/2017/01/21/facenet-for-bears.html迁移学习在自定义数据集上快速重新构建精确的神经网络训练非常深的神经网络（如resnet）是非常耗费资源的，需要大量数据。计算机视觉是高度计算密集型的（对多个gpu进行数周的训练）并且需要大量数据。为了解决这个问题，我们已经讨论过为面部计算通用嵌入。另一种方法是采用现有网络并仅在其他数据集上重新训练其几个层。这是一个教程：codelab教程。它建议你重新训练一个初始模型，训练未知的花类。https://medium.com/@14prakash/transfer-learning-using-keras-d804b2e04ef8提供了在进行迁移学习时应该对哪一层进行再训练的良好指导。图像分割用于自动驾驶的图像分割近年来，图像分割成为了一项令人印象深刻的新任务。它包括识别图像的每个像素。此任务与对象检测有关。实现它的一种算法是mask r-cnn。GAN大规模的GAN由ian goodfellow引入的Generative Adversial Networks是一个神经网络架构，分为两部分：鉴别器和发生器。鉴别器检测图片是否是类，它通常在对象分类数据集上预先训练。生成器为给定的类生成图像在学习期间调整发生器的权重，以便产生鉴别器无法与该类的真实图像区分的图像。以下是最大的GAN（https://arxiv.org/abs/1809.11096）生成的图像示例请参阅https://github.com/eriklindernoren/Keras-GAN在keras中的GAN实现计算机视觉硬件要训 练大型模型，需要大量资源。实现这一目标有两种方法。首先是使用云服务，例如google cloud或aws。第二种方法是自己构建一台带有GPU的计算机。只需1000美元，就可以构建一台体面的机器来训练深度学习模型。视觉界面面对自己的照片仪表板Ownphotos是一个令人惊叹的用户界面，您可以导入照片并自动计算面部嵌入，进行物体识别和识别面部。它用 ：面部识别：face_recognition物体检测：densecap，places365应用视觉问题回答计算机视觉有很多应用：个人照片组织自动驾驶汽车自动无人机解决验证码/ OCR过滤基于图片的网站/应用程序的图片自动标记应用的图片从视频中提取信息（电视节目，电影）视觉问题解答：结合NLP和计算机视觉艺术：GAN结论正如我们在这里看到的，这里有许多新的有趣的方法和应用程序。我认为人工智能在一般情况下最有趣的是特别是在可以重复使用的学习算法中，能够将这些方法应用于越来越多的任务，而不需要太多的处理能力和数据：迁移学习：它可以使得重新利用预训练的大型神经网络成为可能嵌入（例如facenet）：可以识别许多类而无需对这些类中的任何类进行训练转自：https://www.toutiao.com/i6628041041964433934/?iid=52489949221&app=news_article&group_id=6628041041964433934&timestamp=1543476801"}
{"content2":"一、微软认知服务API1、年龄、性别检测2、物体分类、识别3、识别名人全新的名人识别模块可以识别20万来自全球各地涉及商界、政界、体育界以及娱乐界的名人。4、读取图片中的文字光学字符识别（OCR）可检测图片中的文字信息，并将提取出来的文字信息转化成为机器可读的字符串。通过分析图像来检测嵌入式文本，生成字符串并提供搜索功能。可以让直接拍摄文本的图片，无需手动转录文字，从而节省时间和精力。5、情绪识别+视频图片情绪识别可检测到的情感有愤怒、轻蔑、厌恶、恐惧、快乐、无表情、悲伤以及惊讶。这些情感具有特定的面部表情，被认为是可进行跨文化和普适交流的。6、人脸技术人脸识别、人脸关键点定位、人脸验证技术（核对两张脸是否属于同一个人。并给出置信度评分。 ）、人脸辨识、相似人脸搜索、人脸分组7、视频中的图像技术抖动的视频自动进行平滑和稳定处理、使用具有高精度的人脸位置检测和跟踪功能来分析视频，可在一个视频中最多检测 64 张人脸、测在静态背景的视频中发生运动的时间。此服务将分析检测到帧运动的输入视频并输出元数据，还可定义运动发生时所在的精确坐标、自动创建运动缩略图摘要，让人们快速预览你的视频..二、国内图像处理1、face++旷视（人脸识别）人脸识别、证件识别、图像识别2、格灵深瞳（图像识别）格灵深瞳是一家专注于计算机视觉以及人工智能的科技公司，投资人给这个团队的未来市场估值达几千亿。CTO赵勇是Google Glass团队核心成员之一。据说他们的成员都在来自海内外一级名校的学霸，要加入他们比进哈佛大学还要困难。2014年6月获得红杉资本的A轮投资。3、图普科技（图像识别）国内最大的图像识别云服务平台，每日处理数亿的图片及视频内容。创始人李明强是微信创始团队成员之一，曾带领团队打造出QQ邮箱。业界最专业的智能图片鉴黄师。2014年8月获A轮融资。4、Linkface 脸云科技（人脸识别）FDDB人脸检测公开测试世界第一，300-W Benchmark 准确率世界第一, LFW人脸识别准确率已达99.5%以上，即将推出中国银联和京东钱包的“人脸识别”。使用“高斯脸”算法首次击败了人眼识别率的陆超超也加入了Linkface。获得A轮融资，2015年底被SenseTime并购。5、腾讯优图（人脸识别）腾讯旗下专门研究机器学习、智能识别的团队，在另一项人脸识别测试LFW排名第一。相比与创业公司，腾讯优图有着得天独厚的大数据、研发投入、平台等多种优势。6、SenseTime 商汤科技（图像识别）在人脸识别测试LFW中准确率超过Facebook和Google。目前已开始对外提供精准的人脸识别技术，以及集成了人脸识别、危险品识别、行为检测、车辆检测等的安防监控系统。2014年11月获得IDG千万美元A轮融资。7、衣+（图像识别）衣+是计算机视觉搜索引擎，创造人和中意商品的连接。在ImageNet2015计算机视觉竞赛上获得五项世界第一 。团队成员来自于斯坦福、剑桥、IBM、Intel、阿里巴巴、百度等。创始人张默曾就职微软和IBM，任IBM开源联盟负责人、主机Linux中国区技术负责人，还做过平面模特。获1000万美元A轮融资。8、码隆科技（图像识别）主要产品StyleAI，一款通过服饰照片快速了解明星模特们或者其他用户如何搭配类似服饰的应用。团队成员来自微软、百度、腾讯等名企。入选微软创投加速器，已获得1200万元天使轮投资。9、依图科技依图：与您一起构建计算机视觉的未来大数据智慧平台10、tu Simple图森我们提供最佳的自动驾驶解决方案11、飞搜打造最好的在线人脸识别引擎，提供了人脸校验、人脸属性分析、目标场景识别、名人识别、色情识别、食品分析http://www.faceall.cn/12、faceFinger 脸指一直致力于为社会各界提供以人脸、指掌纹等生物特征识别技术为核心的安全防范解决方案。http://www.facefinger.cn:8000/dashboard/demos/index13、颜鉴人脸检测、人脸比对http://colorreco.com/faceCompare最精准的人脸识别、五官定位算法，不论是检出率、错检率、误检率、检测速度、支持旋转角度等标准均处于全球领先水平。.14、Netposa 东方网力科技股份有限公司以视频解码、智能分析、智能检索、云计算等技术为核心，为公安用户提供一整套贴合业务流程的视频侦查装备和管理平台。.15、网易易盾——广告图片识别http://dun.163.com/trial/image/ad.16、车伯乐拍照识车依靠车辆语义分割和机器识别，分析比对图片信息与数据库信息，从而识别出照片中的车辆信息。目前车伯乐的拍照识车准确率已经超过96%。.17、百度研究院http://apistore.baidu.com/.18、荟萃-荟集人力之萃http://huicui.me/?from=singlemessage&isappinstalled=0图片识别可智能识别图片内容、属性、分类、是否涉黄等，支持单图多图多种形式。语音转化可替您将文字转成语音、文字转成方言（真人语音），识别语音、歌曲等。视频识别可以为您完成视频内容收集，字幕识别，视频内容鉴定等内容。视频创作为您拍摄或收集某一主题的视频，以小视频形式上传。网页展示任务可自定义任意网页在用户端展示时间，如新品推广、广告观看等类型。自定义任务抢票？秒杀？联系上下文？只要你脑洞够大，任意H5网页类任务皆可接入。19.深图智服http://www.deepir.com/tech智能鉴别色情、反动、暴力、恐怖、血腥、违规等图片与视频。商标，人脸，检测数百种不同商标，电商以图搜图语义分割，衣物，室外20、Linkfluence社交舆情，营销报告https://linkfluence.com/zh-hans/...三、国外图像处理公司1、Enlitic 医疗诊断Enlitic于2014年8月在三藩市成立，采用深度学习和图像分析帮助医生做出诊断并标记出医学图像中的异常，从而让医疗更加精确和更有效率。例如，Enlitic可以分析X光、核磁共振成像或CT扫描得到的医学图像，然后找出数据中的趋势或单个图像中的异常情况。公司创始人Jeremy Howard（同时也是一名黑客和数据科学家）认为，数据科学是项非常性感的职业。但是，数据科学家现在做的大部分工作却是产品推荐或者广告投放等。他觉得这不够性感。为了给数据科学找到更好的应用，让深度学习做些更有意义的事情，他将注意力放在了医疗方面。他的基本思考是，创造一个类似星际迷航三录仪一样的系统（不过可能无法便携），搜集有关特定病人的数据——从医疗图片、实验室检测结果到医生的便笺——让深度学习分析这些数据，做出诊断并给出治疗建议。这并非让机器替代医生，而是为其提供让诊断更加便利的工具。公司还将与诊所、医院以及其他医疗单位合作，分析算法，进一步精炼公司的技术。2、Affectiva 面部表情分析Affectiva的技术采用先进的计算机视觉算法来捕捉和识别视觉刺激所激发的情感反应。Affectiva的旗舰产品Affdex简单易用；只要一个摄像头，任何地方都可以，也不需要安装软件。另外Affdex也很简洁低调，没有凌乱的布线或电极。3、Deepomatic 计算机视觉Deepomatic正在打造一个能够将媒体图片中任何想要的产品与电子商务网站中同样或类似的产品链接起来的按钮。内容发行者可以将他们的图像经过算法的扫描，检测和识别出其中让人心动的产品（如：时尚产品）。通过将自动理解产品属性和图案颜色的比较相结合，Deepomatic将这些图像和电子商务网站上同样或类似的产品链接在一起。Deepomatic便由此可以和内容所有者分享这个新增的收入来源。4、Clarifai 图像和视频识别2013年，Clarifai的第一个图像识别系统在识别图像中的物体的ImageNet比赛中进入了前五强。自那以后，Clarifai的深度学习系统不断进化，提高了识别的速度、词典的大小和内存的占用，同时应用范围也超出了图像识别，可以从各种形式的数据中提取知识。Clarifai的技术中枢是高性能深度学习API，在这之上Clarifai正在构建新一代智能应用。这让Clarifai能够通过以全新的创新的方式向所有人提供高技术解决方案应对日常问题。公司创始人Matthew Zeiler，纽约大学计算机科学PHD，曾和深度学习领域两个最牛人物一——Geoff Hinton和Yann LeCun一起工作过五年。他发现真正困难的地方是建立学习模式——处理所有视觉数据的关键算法——能够快速处理各种不同图片。Zeiler很清楚，「训练这些模型与其说是科学工作，不如说是种艺术」，「需要很多年的经验。」这正好是Clarifai的切入点。Zieler的想法是，只要用户将照片上传到Clarifai软件，这个软件就会分析出照片里的内容并提供与此类似的更多照片。与传统基于文本的图片搜索截然不同的是，这是真的以视觉为基础的搜索。可以免费获得其API使用：5、Descartes Labs 图像识别、卫星与农业基于深度学习和先进的遥感算法，Descartes Labs正在教导计算机如何看世界以及世界如何随时间改变。他们的第一个应用是使用大量卫星图像（包括可见光段和不可见光段），更好了解全球农作物生产。6、MetaMind 自然语言处理和图像识别MetaMind想让人人都能使用深度学习。该公司正在打造一个用于自然语言处理、图像理解和知识基础的分析的人工智能平台。该公司提供了用于医疗成像、食物识别和解决方案定制的产品。MetaMind希望提供包括自然语言处理在哪的更为广泛多样的工具。 深度学习有望帮助机器真正理解用户说的话，而这种技术的关键特点之一就是能够自我训练，这也是许多人相信它能有助于自然语言处理的根据所在。而这正是另一个MetaMind工具正在开发的领域，当你输入连个句子，这个工具能够告诉你两个句子的相似程度。这种技术能够被商家用来自动回复客户问题。Socher说，用户的询问方式多种多样，尽管绝大多数时候，意思差不都多。这个工具也能用来分析一些社交网络（比如推特）上用户对公司的评价。MetaMind目前从事深度学习咨询的业务，也会提供自己的深度学习服务和软件。借由运行MetaMind的数以百计装备成千上万图像处理器的学习机器，这一在线服务省去了客户建立自己系统的麻烦和成本。但是，如果客户想要运营自己的深度学习系统，MetaMind会为它提供软件和专业服务，如果确有必要的话。7、HyperVerge 计算机视觉与图像识别引擎HyperVerge使用深度学习算法处理云中消费者的图片和视频。HyperVerge开发的用于图像处理的已获专利的专有图像技术模型包括：面部检测、面部识别、场景识别、差照片检测、重复照片检测、照片分类、相册总结、面部美化和照片美化。8、Tractable 计算机视觉Tractable正在开发专有的机器学习算法，重点是用于计算机视觉的深度学习。该公司的重点是让未标记的数据和监督学习一起工作。应用领域包括保险索赔、工业检测、远程监控等.9、服装推荐Indico演示网址：https://indico.io/demos/clothing-matching推荐系统正在日益凸显它的价值。随着商品数目的增加，瞄准那些可能愿意购买某种产品的特定消费人群变得格外重要。在这一类应用的开发中，深度学习同样也能帮助到我们！我不是个时尚爱好者，但是我知道人们会“浪费”很多时间在选择穿什么样的衣服上。如果我们能拥有一个知道我们的喜好并且能够向我们推荐完美的穿着的智能机器人该有多好！幸运的是，在深度学习的帮助下，这成为了可能。Indico 的官方文章（https://indico.io/blog/fashion-matching-tutorial/）已经对它做了非常详细的描述。现在让我们弄清楚你要如何在你的终端上构建这个推荐系统。.10、使用深度学习（Algorithmia API）为照片着色六招教你用Python构建好玩的深度学习应用自动着色一直是计算机视觉社区中的热门话题。从一张黑白图片获得一张彩色照片似乎是件超现实的事。想象一个4岁的孩子拿着蜡笔全神贯注于涂鸦本的场景，我们是否能教会人工智能同样去做这件事情呢？.11、AWS的人脸表情识别https://aws.amazon.com/cn/rekognition/?nc2=h_a1检测图像中的对象、场景和面孔。您还可以搜索和比较面孔。借助 Rekognition 的 API，您可以快速为应用程序添加基于深度学习的复杂视觉搜索和图像分类功能。12、Hashley — 为你的照片自动生成诙谐的标签或评论http://hash.ai/13、Hotness.ai — 基于照片来分析你的性感程度http://hotness.ai/14、Bitesnap — 基于照片的食物识别 AI，可用于控制热量摄入https://getbitesnap.com/Entrupy — 自动识别衣服与配饰的真假https://www.entrupy.com/15、Fify — 帮你买衣服http://fify.ai/16、GoFind —用照片来为你在线搜衣服http://gofind.ai/17、Mode.ai — 帮你在线找衣服http://mode.ai/18、AI Weekly — 每周的人工智能与机器学习新闻、资源汇总http://aiweekly.co19、Approximately Correct —人工智能与机器学习博客http://approximatelycorrect.com/20、Axiomzen — AI 行业通讯半月刊http://ai.axiomzen.co/21、Concerning.ai — AI 评论https://concerning.ai22、Fast.ai —致力于普及深度学习http://www.fast.ai/23、Machinelearning.ai — 专注于机器学习、人工智能新闻http://machinelearning.ai24、Machine Learning Weekly — 一份手工的机器学习、深度学习周报http://mlweekly.com25、PRAI —人工智能、机器学习、深度学习的一个论坛https://pr.ai/index.php26、Rekognition：为社交图片应用提供面部和场景的识别和优化。Rekognition API 可以利用眼睛、嘴、鼻子和面部的特征实现情绪识别和性别检测，可以用来确定性别、年龄和情绪。链接：http://www.programmableweb.com/api/rekognition参考文献：1、机器之心：业界｜最值得关注的18家深度学习创业公司2、知乎，国内有哪些人工智能领域的创业团队或创业公司？版权声明：本文为博主原创文章，转载请注明来源“素质云博客”，谢谢合作！！微信公众号：素质云笔记 https://blog.csdn.net/sinat_26917383/article/details/54864341"}
{"content2":"Jetson TX2是NIVDIA瞄准人工智能在Jetson TK1和TX1推出后的升级TX2的GPU和CPU都进行了升级，内存增加到了8GB、存储增加到了32GB，支持Wifi和蓝牙，编解码支持H.265，体型同样小巧。据NVIDIA官方介绍，Jetson TX2提供两种运行模态：一种是MAX Q，能效比能达到最高，是上一代的TX1的2倍，功耗在7.5W以下;另一种是MAX P，性能可以做到最高，能效比同样可以做到前一代的2倍，功耗则在15W以下。资源配置丰富且强大1.模组配置Jetson TX2的特点是NVIDIA Pascal GPU有256个CUDA有能力的核心。CPU复杂部分由两个ARM v8 64位CPU集群组成，它们由高性能的相干互连结构连接。为提高单线程性能，优化了丹佛2(双核)CPU集群;第二个CPU集群是一个ARM Cortex-A57 QuadCore，它更适合于多线程应用程序。内存子系统包含一个128位的内存控制器，它提供高带宽LPDDR4支持。8 GB LPDDR4主存和32 GB eMMC闪存集成在模块上。从TX1 64位到128位的设计是一个主要的性能提升。该模块还支持硬件视频编码器和解码器，支持4K超高清视频以不同格式的60fps。这与混合Jetson TX1模块略有不同，后者使用了Tegra SoC上运行的专用硬件和软件来完成这些任务。还包括一个音频处理引擎，全硬件支持多通道音频。Jetson TX2支持Wi-Fi和蓝牙无线连接。Wi-fi比之前的Jetson TX1有了很大的改进。包括千兆以太网基础。重点：TX2共两块CPU，和一块有256个CUDA核心的GPU。CPU一块是双核的丹佛二，第二快是ARM CortexA57。8G的运行内存，32G flash存储器。 256 core NVIDIA Pascal GPU. ARMv8 (64-bit) Multi-Processor CPU Complex. Advanced HD Video Encoder. Advanced HD Video Decoder. Display Controller Subsystem. 128-bit Memory Controller. 8GB LPDDR4 and 32 GB eMMC memory 1.4Gpix/s Advanced image signal processing Audio Processing Engine.2. 对外接口有两个扩展头，一个40大头针，2.54毫米间隔的标头，与树莓Pi类似，还有一个30针，2.54毫米间隔，是扩展的GPIO。Jetson TX2还包括一个5MP摄像机在摄像机扩展头，和一个显示扩展头用于添加额外的显示面板。Jetson TX2添加了一个CAN总线控制器到模块。CAN是一种网络格式，经常用于汽车和其他车辆。CAN总线信号可以直接用于GPIO扩展头。英伟达设计了两种模式。Max-Q是能源效率模式的名称，它可以在性能和性能曲线的弯曲之前，将帕克SoC的工作效率调高，达到7.5W。这种模式的结果是，TX2在最大性能模式下与TX1有相似的性能，同时获得了大约一半的功率。在Max-P模式下，TX2平均功耗是15W。这大约是Jetson TX1最大时钟频率的两倍。3.按键接口和TX1的对比自带的软件包配置JetPack 3.0这款功能强大的开发者套件能够使主板的硬件功能和接口充分发挥效用，预装 Linux 开发环境。同时，它还支持 NVIDIA Jetpack SDK，包括 BSP、深度学习库、计算机视觉、GPU 计算、多媒体处理等众多功能。JetsonDevelopment Pack(JetPack) 是一个按需提供的一体化软件包，捆绑并安装了适用于 NVIDIA Jetson 嵌入式平台的所有开发用软件工具。NVIDIA JetPack SDK是构建AI应用程序的最全面的解决方案。它捆绑了所有Jetson平台软件，包括TensorRT，cuDNN，CUDA工具包，VisionWorks，Streamer和OpenCV，这些都是基于LTS Linux内核的L4T。JetPack 包括:深度学习: TensorRT、cuDNN、NVIDIADIGITS™ 工作流程计算机视觉: NVIDIA VisionWorks、OpenCVGPU 计算: NVIDIA CUDA、CUDA 库多媒体: ISP 支持、摄像头图像、视频 CODEC同时，它还包括 ROS 兼容性、OpenGL、高级开发者工具等等。CUDACUDA® 是NVIDIA 创造的一个并行计算平台和编程模型。它利用图形处理器 (GPU)能力，实现计算性能的显著提高。借助目前已售出的数百万支持 CUDA 的 GPU，软件开发人员、科学家和研究人员正在各种各样的应用程序中使用 GPU 加速计算。CUDA工具包为C和C ++开发人员构建提供了一个全面的开发环境 ，GPU加速的应用程序。该工具包包括用于NVIDIAGPU的编译器，数学库以及用于调试和优化应用程序性能的工具。参考 http://www.nvidia.cn/object/cudazone-cn.htmlOpenCVOpenCV是计算机视觉，图像处理和机器学习领域的领先开源库，现在提供GPU加速以实现实时操作。OpenCV被广泛的应用，包括：街景图像拼接，自动检查和监视，机器人和无人驾驶的汽车导航和控制，医学图像分析，视频/图像搜索和检索，电影 – 3D，交互艺术设备VisionWorksVisionWorks是用于计算机视觉（CV）和图像处理的软件开发包。VisionWorks™实现并扩展了Khronos OpenVX标准，并针对具有CUDA功能的GPU和SOC进行了优化，使开发人员能够在可扩展的灵活平台上实现CV应用程序。它包括VPI（视觉编程接口），这是CUDA开发人员使用的一组优化的CV原语。NVX库允许直接访问VPI，OVX库允许通过OpenVX框架间接访问VPI。VisionWorks的核心功能是针对以下方面的解决方案而设计的：机器人和无人机，自动驾驶，智能视频分析，增强现实该工具包通过添加您自己的算法和处理管道，帮助您解锁基于GPU的CV系统的可能性。将VisionWorks与OpenCV等其他API结合使用，可以访问许多开源CV算法。CUDA加速了OpenVX API和NVIDIA扩展原语适用于高级开发人员的直接CUDA视觉编程接口线程安全的API示例/示例管道代码文档包括带有发行说明的工具包参考指南，安装指南，教程和API参考。参考：https://developer.nvidia.com/embedded/visionworks应用场景方面，TX2主要部署在终端应用上，包括智能化的工厂机器人、商用无人机和智能摄像头等等。Jetson系列也都是为了推动终端的智能化。本文参考：https://blog.csdn.net/hongyanxuyanfei/article/details/79108039http://www.zhongkerd.com/news/content-1758.htmlhttps://blog.csdn.net/hanxuexiaoma/article/details/78968445"}
{"content2":"作机器视觉和图像处理方面的研究工作，最重要的两个问题：其一是要把握住国际上最前沿的内容；其二是所作工作要具备很高的实用背景。解决第一个问题的办法就是找出这个方向公认最高成就的几个超级专家(看看他们都在作什么)和最权威的出版物(阅读上面最新的文献)，解决第二个问题的办法是你最好能够找到一个实际应用的项目，边做边写文章。做好这几点的途径之一就是利用网络资源，利用权威网站和专家们的个人主页。依照下面目录整理：[1]研究群体(国际国内)[2]专家主页[3]前沿国际国内期刊与会议[4]搜索资源[5]GPL软件资源一、研究群体用来搜索国际知名计算机视觉研究组(CV Groups)：国际计算机视觉研究组清单http://peipa.essex.ac.uk/info/groups.html美国计算机视觉研究组清单 http://peipa.essex.ac.uk/info/groups.html#USAhttp://www-2.cs.cmu.edu/~cil/vision.html或 http://www.cs.cmu.edu/~cil/vision.html这是卡奈基梅隆大学的计算机视觉研究组的主页，上面提供很全的资料，从发表文章的下载到演示程序、测试图像、常用链接、相关软硬件，甚至还有一个搜索引擎。著名的有人物Tomasi， Kanade等。卡内基梅隆大学双目实验室http://vision.middlebury.edu/stereo/卡内基梅隆研究组http://www.cs.cmu.edu/~cil/v-groups.html还有几个实验室：Calibrated Imaging Laboratory 图像Digital Mapping Laboratory 映射Interactive Systems Laboratory 互动Vision and Autonomous Systems Center视觉自适应http://www.via.cornell.edu/康奈尔大学的计算机视觉和图像分析研究组，好像是电子和计算机工程系的。侧重医学方面的研究，但是在上面有相当不错资源，关键是它正在建设中，能够跟踪一些信息。Cornell University——Robotics and Vision grouphttp://www-cs-students.stanford.edu/ 斯坦福大学计算机系主页1. http://white.stanford.edu/2. http://vision.stanford.edu/3. http://ai.stanford.edu/美国斯坦福大学人工智能机器人实验室The Stanford AI Lab (SAIL) is the intellectual home for researchers in the Stanford Computer Science Department whose primary research focus is Artificial Intelligence. The lab is located in the Gates...Vision and Imaging Science and Technologyhttp://www.fmrib.ox.ac.uk/analysis/主要研究：Brain Extraction Tool， Nonlinear noise reduction， Linear Image Registration， Automated Segmentation， Structural brain change analysis， motion correction， etc.http://www.cse.msu.edu/prip/—密歇根州立大学计算机和电子工程系的模式识别--图像处理研究组，它的FTP上有许多的文章(NEW)。美国密歇根州大学认知模型和图像处理实验室The Pattern Recognition and Image Processing (PRIP) Lab faculty and students investigate the use of machines to recognize patterns or objects. Methods are developed to sense objects， to discover which...http://www.cse.msu.edu/rgroups/prip/http://pandora.inf.uni-jena.de/p/e/index.html德国的一个数字图像处理研究小组，在其上面能找到一些不错的链接资源。柏林大学 http://www.cv.tu-berlin.de/德国波恩大学视觉和认识模型小组Computer Vision Group located within the Division III of the Computer Science Department in the University of Bonn in Germany. This server offers information on topics concerning our computer vision http://www-dbv.informatik.uni-bonn.de/http://www-staff.it.uts.edu.au/~sean/CVCC.dir/home.htmlCVIP(used to be CVCC for Computer Vision and Cluster Computing) is a research group focusing on cluster-based computer vision within the Spiral Architecture.http://cfia.gmu.edu/The mission of the Center for Image Analysis is to foster multi-disciplinary research in image， multimedia and related technologies by establishing links between academic institutes， industry and government agencies， and to transfer key technologies to help industry build next generation commercial and military imaging and multimedia systems.英国的Bristol大学的Digital Media Group在高级图形图像方面不错。主要就是涉及到场景中光线计算的问题，比如用全局光照或是各种局部光照对高动态图的处理，还有近似真实的模拟现实环境 (照片级别的)，还有用几张照片来建立3D模型(人头之类的)。另外也有对古代建筑模型复原。http://www.cs.bristol.ac.uk/Research/Digitalmedia/而且根据Times全英计算机排名在第3， 也算比较顶尖的研究了http://www.cmis.csiro.au/IAP/zimage.htm这是一个侧重图像分析的站点，一般。但是提供一个Image Analysis环境---ZIMAGE and SZIMAGE。麻省理工视觉实验室MIT http://groups.csail.mit.edu/vision/welcome/AI Laboratory Computer Vision groupCenter for Biological and Computational LearningMedia Laboratory， Vision and Modeling GroupPerceptual Science groupUC Berkeley http://0-vision.berkeley.edu.ilstest.lib.neu.edu/vsp/index.htmlhttp://www.cs.berkeley.edu.ilste ... n/vision_group.html加州大学伯克利分校视觉实验室David A. Forsyth：http://www.cs.berkeley.edu/~daf/UCLA(加州大学洛杉矶分校) http://vision.ucla.edu/视觉实验室英国牛津的A.Zisserman：http://www.robots.ox.ac.uk/~az/ 机器人实验室美国南加州大学智能机器人和智能系统研究所University of Southern California， Los AngelesIRIS is an interdepartmental unit of USC's School of Engineering with ties to USC's Information Sciences Institute (ISI). Members include faculty， graduate students， and research staff associated with... http://iris.usc.edu/ Computer Vision 实验室美国南加州大学计算机视觉实验室介绍：Computer Vision Laboratory at the University of Southern California is one of the major centers of computer vision research for thirty years. they conduct research in a number of basic and applied are...http://iris.usc.edu/USC-Computer-Vision.html英国约克大学高级计算机结构神经网络小组The Advanced Computer Architecture Group has had a thriving research programme in neural networks for over 10 years. The 15 researchers， led by Jim Austin， focus their work in the theory and applicati...http://www.cs.york.ac.uk/arch/neural/瑞士戴尔莫尔感知人工智能研究所IDIAP is a research institute established in Martigny in the Swiss Alps since 1991. Active in the areas of multimodal interaction and multimedia information management， the institute is also the leade...http://www.idiap.ch/英国萨里大学视觉，语言和信号处理中心The Centre for Vision， Speech and Signal Processing (CVSSP) is more than 60 members strong， comprising 12 academic staff， 18 research fellows and more than 44 research students. The activities of the ...http://www.ee.surrey.ac.uk/Research/VSSP/美国阿默斯特马萨诸塞州立大学计算机视觉实验室The Computer Vision Laboratory was established in the Computer Science Department at the University of Massachusetts in 1974 with the goal of investigating the scientific principles underlying the con...http://vis-www.cs.umass.eduUniversity of Massachusetts——Computer Vision Laboratory for Perceptual Robotics美国芝加哥伊利诺伊斯大学贝克曼研究中心智能机器人和计算机视觉实验室Includes the following groups: Professor Seth Hutchinson's Research Group Professor David Kriegman's Research Group Professor Jean Ponce's Research Group Professor Narendra Ahuja's Research Gro...http://www-cvr.ai.uiuc.edu/Computer Vision and Robotics LaboratoryVision Interfaces and Systems Laboratory (VISLab)英国伯明翰大学计算机科学学校视觉研究小组The vision group at the School of Computer Science (a RAE 5 rated department) performs research into a wide variety of computer vision and image understanding areas. Much of this work is performed in ...http://www.cs.bham.ac.uk/research/vision/微软研究院机器学习与理解研究小组 / 计算机视觉小组The research group focuses on the development of more advanced and intelligent computer systems through the exploitation of statistical methods in machine learning and computer vision. The site lists ...http://research.microsoft.com/mlp/http://research.microsoft.com/en-us/groups/vision/微软公司的文献：http://research.microsoft.com/research/pubs微软亚洲研究院：http://research.microsoft.com/asia/，值得关注Harry Shum， Jian Sun， Steven Lin， Long Quan(兼职HKUST)etc.瑞典隆德大学数学系视觉组：http://www.maths.lth.se/matematiklth/personal/andersp/感觉国外搞视觉的好多是数学系出身，大约做计算机视觉对数学要求很高吧。澳大利亚国立大学：http://users.rsise.anu.edu.au/~hartley/美国北卡大学：http://www.cs.unc.edu/~marc/法国INRIA：http://www-sop.inria.fr/odyssee/team/ 由Olivier.Faugeras领衔的牛人众多。比利时鲁汶大学的L.Van Gool： www.esat.kuleuven.ac.be/psi/visics/据说在这个只有中国一个小镇大小的地方的鲁汶大学在欧洲排行top10，名列世界top100，还出了几个诺贝尔奖，视觉研究也很强.美国明德http://vision.middlebury.edu/stereo/以下含有非顶尖美国学校研究组，没有链接(个别的上面已经提到)，供参考。"}
{"content2":"转自论坛http://www.ieee.org.cn/dispbbs.asp?BoardID=62&replyID=31567&id=29962&star=1&skin=0作者好像是南大周志华老师下面同分的按字母序排列:IJCAI (1+): AI最好的综合性会议, 1969年开始, 每两年开一次, 奇数年开. 因为AI 实在太大, 所以虽然每届基本上能录100多篇（现在已经到200多篇了），但分到每个领域就没几篇了，象machine learning、computer vision这么大的领域每次大概也就10篇左右, 所以难度很大. 不过从录用率上来看倒不太低,基本上20%左右, 因为内 行人都会掂掂分量, 没希望的就别浪费reviewer的时间了. 最近中国大陆投往国际会议的文章象潮水一样, 而且因为国内很少有能自己把关的研究组, 所以很多会议都在complain说中国的低质量文章严重妨碍了PC的工作效率. 在这种情况下, 估计这几年国际会议的录用率都会降下去. 另外, 以前的IJCAI是没有poster的, 03年开始, 为了减少被误杀的好人, 增加了2页纸的poster.值得一提的是, IJCAI是由貌似一个公司\"IJCAI Inc.\"主办的(当然实际上并不是公司, 实际上是个基金会), 每次会议上要 发几个奖, 其中最重要的两个是IJCAI Research Excellence Award 和 Computer& Thoughts Award, 前者是终身成就奖, 每次一个人, 基本上是AI的最高奖(有趣的是, 以AI为主业拿图灵奖的6位中, 有2位还没得到这个奖), 后者是奖给35岁以下的青年科学家, 每次一个人. 这两个奖的获奖演说是每次IJCAI的一个重头戏.另外，IJCAI 的 PC member 相当于其他会议的area chair, 权力很大, 因为是由PC member 去找 reviewer 来审, 而不象一般会议的PC member其实就是 reviewer. 为了制约这种权力, IJCAI的审稿程序是每篇文章分配2位PC member, primary PC member去找3位reviewer, second PC member 找一位.AAAI (1): 美国人工智能学会AAAI的年会. 是一个很好的会议, 但其档次不稳定, 可以给到1+, 也可以给到1-或者2+, 总的来说我给它\"1\". 这是因为它的开法完全受IJCAI制约: 每年开, 但如果这一年的IJCAI在北美举行, 那么就停开. 所以, 偶数年里因为没有IJCAI, 它就是最好的AI综合性会议, 但因为号召力毕竟比IJCAI要小一些,特别是欧洲人捧AAAI场的比IJCAI少得多(其实亚洲人也是), 所以比IJCAI还是要稍弱一点, 基本上在1和1+之间; 在奇数年, 如果IJCAI不在北美, AAAI自然就变成了比IJCAI低一级的会议(1-或2+), 例如2005年既有IJCAI又有AAAI, 两个会议就进行了协调, 使得IJCAI的录用通知时间比AAAI的deadline早那么几天, 这样IJCAI落选的文章可以投往AAAI.在审稿时IJCAI 的 PC chair也在一直催, 说大家一定要快, 因为AAAI那边一直在担心IJCAI的录用通知出晚了AAAI就麻烦了.COLT (1): 这是计算学习理论最好的会议, ACM主办, 每年举行. 计算学习理论基本上可以看成理论计算机科学和机器学习的交叉, 所以这个会被一些人看成是理论计算机科学的会而不是AI的会. 我一个朋友用一句话对它进行了精彩的刻画: \"一小群数学家在开会\". 因为COLT的领域比较小, 所以每年会议基本上都是那些人. 这里顺便提一件有趣的事, 因为最近国内搞的会议太多太滥, 而且很多会议都是LNCS/LNAI出论文集, LNCS/LNAI基本上已经被搞臭了, 但很不幸的是, LNCS/LNAI中有一些很好的会议, 例如COLT.CVPR (1): 计算机视觉和模式识别方面最好的会议之一, IEEE主办, 每年举行. 虽然题目上有计算机视觉, 但个人认为它的模式识别味道更重一些. 事实上它应该是模式识别最好的会议, 而在计算机视觉方面, 还有ICCV与之相当. IEEE一直有个倾向, 要把会办成\"盛会\", 历史上已经有些会被它从quality很好的会办成\"盛会\"了. CVPR搞不好也要走这条路. 这几年录的文章已经不少了. 最近负责CVPR会议的TC的chair发信说, 对这个community来说, 让好人被误杀比被坏人漏网更糟糕, 所以我们是不是要减少好人被误杀的机会啊? 所以我估计明年或者后年的CVPR就要扩招了.ICCV (1): 介绍CVPR的时候说过了, 计算机视觉方面最好的会之一. IEEE主办.  ICCV逢奇数年开，开会地点以往是北美，欧洲和亚洲轮流，本来2003年定在北京，后来因Sars和原定05年的法国换了一下。ICCV'07年将首次在南美(巴西)举行.CVPR原则上每年在北美开, 如果那年正好ICCV在北美,则该年没有CVPR.ICML (1): 机器学习方面最好的会议之一. 现在是IMLS主办, 每年举行. 参见关于NIPS的介绍.NIPS (1): 神经计算方面最好的会议之一, NIPS主办, 每年举行. 值得注意的是, 这个会每年的举办地都是一样的, 以前是美国丹佛, 现在是加拿大温哥华; 而且它是年底开会, 会开完后第2年才出论文集, 也就是说, NIPS'05的论文集是06年出. 会议的名字是\"Advances in Neural Inxxxxation Processing Systems\", 所以, 与ICMLECML这样的\"标准的\"机器学习会议不同, NIPS里有相当一部分神经科学的内容, 和机器学习有一定的距离. 但由于会议的主体内容是机器学习, 或者说与机器学习关系紧密, 所以不少人把NIPS看成是机器学习方面最好的会议之一. 这个会议基本上控制在MichaelJordan的徒子徒孙手中, 所以对Jordan系的人来说, 发NIPS并不是难事, 一些未必很强的工作也能发上去, 但对这个圈子之外的人来说, 想发一篇实在很难, 因为留给\"外人\"的口子很小. 所以对Jordan系以外的人来说, 发NIPS的难度比ICML更大. 换句话说,ICML比较开放, 小圈子的影响不象NIPS那么大, 所以北美和欧洲人都认, 而NIPS则有些人(特别是一些欧洲人, 包括一些大家)坚决不投稿. 这对会议本身当然并不是好事,但因为Jordan系很强大, 所以它似乎也不太care. 最近IMLS(国际机器学习学会)改选理事, 有资格提名的人包括近三年在ICMLECMLCOLT发过文章的人, NIPS则被排除在外了. 无论如何, 这是一个非常好的会.ACL (1-): 计算语言学/自然语言处理方面最好的会议, ACL (Association ofComputational Linguistics) 主办, 每年开.KR (1-): 知识表示和推理方面最好的会议之一, 实际上也是传统AI(即基于逻辑的AI)最好的会议之一. KR Inc.主办, 现在是偶数年开.SIGIR (1-): 信息检索方面最好的会议, ACM主办, 每年开. 这个会现在小圈子气越来越重. 信息检索应该不算AI, 不过因为这里面用到机器学习越来越多, 最近几年甚至有点机器学习应用会议的味道了, 所以把它也列进来.SIGKDD (1-): 数据挖掘方面最好的会议, ACM主办, 每年开. 这个会议历史比较短,毕竟, 与其他领域相比,数据挖掘还只是个小弟弟甚至小侄儿. 在几年前还很难把它列在tier-1里面, 一方面是名声远不及其他的top conference响亮, 另一方面是相对容易被录用. 但现在它被列在tier-1应该是毫无疑问的事情了. 这几年来KDD的质量都很高. SIGKDD从2000年来full paper的录取率都在10%-12%之间，远远低于IJCAI和ICML.经常听人说，KDD要比IJICAI和ICML都要困难。IJICAI才6页，而KDD要10页。没有扎实系统的工作，很难不留下漏洞。有不少IJICAI的常客也每年都投KDD，可难得几个能经常中。UAI (1-): 名字叫\"人工智能中的不确定性\", 涉及表示推理学习等很多方面, AUAI(Association of UAI) 主办, 每年开.我知道的几个人工智能会议(二三流)(原创为lilybbs.us上的daniel)纯属个人看法, 仅供参考. tier-1的列得较全, tier-2的不太全, tier-3的很不全.同分的按字母序排列. 不很严谨地说, tier-1是可以令人羡慕的, tier-2是可以令人尊敬的,由于AI的相关会议非常多, 所以能列进tier-3的也是不错的.tier 2: tier-2的会议列得不全, 我熟悉的领域比较全一些.AAMAS (2+): agent方面最好的会议. 但是现在agent已经是一个一般性的概念,几乎所有AI有关的会议上都有这方面的内容, 所以AAMAS下降的趋势非常明显.ECCV (2+): 计算机视觉方面仅次于ICCV的会议, 因为这个领域发展很快, 有可能升级到1-去.ECML (2+): 机器学习方面仅次于ICML的会议, 欧洲人极力捧场, 一些人认为它已经是1-了. 我保守一点, 仍然把它放在2+. 因为机器学习发展很快, 这个会议的reputation上升非常明显.ICDM (2+): 数据挖掘方面仅次于SIGKDD的会议, 目前和SDM相当. 这个会只有5年历史, 上升速度之快非常惊人. 几年前ICDM还比不上PAKDD, 现在已经拉开很大距离了.SDM (2+): 数据挖掘方面仅次于SIGKDD的会议, 目前和ICDM相当. SIAM的底子很厚,但在CS里面的影响比ACM和IEEE还是要小, SDM眼看着要被ICDM超过了, 但至少目前还是相当的.ICAPS (2): 人工智能规划方面最好的会议, 是由以前的国际和欧洲规划会议合并来的. 因为这个领域逐渐变冷清, 影响比以前已经小了.ICCBR (2): Case-Based Reasoning方面最好的会议. 因为领域不太大, 而且一直半冷不热, 所以总是停留在2上.COLLING (2): 计算语言学/自然语言处理方面仅次于ACL的会, 但与ACL的差距比ICCV-ECCV和ICML-ECML大得多.ECAI (2): 欧洲的人工智能综合型会议, 历史很久, 但因为有IJCAI/AAAI压着,很难往上升.ALT (2-): 有点象COLT的tier-2版, 但因为搞计算学习理论的人没多少, 做得好的数来数去就那么些group, 基本上到COLT去了, 所以ALT里面有不少并非计算学习理论的内容.EMNLP (2-): 计算语言学/自然语言处理方面一个不错的会. 有些人认为与COLLING相当, 但我觉得它还是要弱一点.ILP (2-): 归纳逻辑程序设计方面最好的会议. 但因为很多其他会议里都有ILP方面的内容, 所以它只能保住2-的位置了.PKDD (2-): 欧洲的数据挖掘会议, 目前在数据挖掘会议里面排第4. 欧洲人很想把它抬起来, 所以这些年一直和ECML一起捆绑着开, 希望能借ECML把它带起来.但因为ICDM和SDM, 这已经不太可能了. 所以今年的PKDD和ECML虽然还是一起开, 但已经独立审稿了(以前是可以同时投两个会, 作者可以声明优先被哪个会考虑, 如果ECML中不了还可以被PKDD接受).tier 3: 列得很不全. 另外, 因为AI的相关会议非常多, 所以能列在tier-3也算不错了, 基本上能进到所有AI会议中的前30%吧ACCV (3+): 亚洲的计算机视觉会议, 在亚太级别的会议里算很好的了.DS (3+): 日本人发起的一个接近数据挖掘的会议.ECIR (3+): 欧洲的信息检索会议, 前几年还只是英国的信息检索会议.ICTAI (3+): IEEE最主要的人工智能会议, 偏应用, 是被IEEE办烂的一个典型. 以前的quality还是不错的, 但是办得越久声誉反倒越差了, 糟糕的是似乎还在继续下滑, 现在其实3+已经不太呆得住了.PAKDD (3+): 亚太数据挖掘会议, 目前在数据挖掘会议里排第5.ICANN (3+): 欧洲的神经网络会议, 从quality来说是神经网络会议中最好的, 但这个领域的人不重视会议,在该领域它的重要性不如IJCNN.AJCAI (3): 澳大利亚的综合型人工智能会议, 在国家/地区级AI会议中算不错的了.CAI (3): 加拿大的综合型人工智能会议, 在国家/地区级AI会议中算不错的了.CEC (3): 进化计算方面最重要的会议之一, 盛会型. IJCNN/CEC/FUZZ-IEEE这三个会议是计算智能或者说软计算方面最重要的会议, 它们经常一起开, 这时就叫WCCI  (World Congress on Computational Intelligence). 但这个领域和CS其他分支不太一样, 倒是和其他学科相似, 只重视journal, 不重视会议, 所以录用率经常在85%左右, 所录文章既有quality非常高的论文, 也有入门新手的习作.FUZZ-IEEE (3): 模糊方面最重要的会议, 盛会型, 参见CEC的介绍.GECCO (3): 进化计算方面最重要的会议之一, 与CEC相当，盛会型.ICASSP (3): 语音方面最重要的会议之一, 这个领域的人也不很care会议.ICIP (3): 图像处理方面最著名的会议之一, 盛会型.ICPR (3): 模式识别方面最著名的会议之一, 盛会型.IEA/AIE (3): 人工智能应用会议. 一般的会议提名优秀论文的通常只有几篇文章, 被提名就已经是很高的荣誉了, 这个会很有趣, 每次都搞1、20篇的优秀论文提名, 专门搞几个session做被提名论文报告, 倒是很热闹.IJCNN (3): 神经网络方面最重要的会议, 盛会型, 参见CEC的介绍.IJNLP (3): 计算语言学/自然语言处理方面比较著名的一个会议.PRICAI (3): 亚太综合型人工智能会议, 虽然历史不算短了, 但因为比它好或者相当的综合型会议太多, 所以很难上升."}
{"content2":"什么是深度学习？说明深度学习之前，先说明AI——人工智能。这是最近几年特别火的一个概念，从AlphaGo以四比一击败韩国顶尖围棋选手李世乭到如今数码产品领域的AI拍照、AI助手等等，都是AI带来的福利。人工智能分为多个领域：专家系统、机器学习、进化计算、模糊逻辑、计算机视觉、自然语言处理、推荐系统等。而我们所知道的深度学习，就是一种实现机器学习的技术，而机器学习是人工智能的一个分支。下展示了人工智能的分支及其衍生的分支，展示了人工智能、机器学习、深度学习的包含关系。图 1 人工智能及其衍生分支图 2 三者包含关系通过上面两张图，我们就能确定AI、Machine Learning、Deep Learning之间的关系了。总而言之，机器学习是实现人工智能的方法，深度学习是实现机器学习的一种技术。并且，在目前来看，深度学习将会淘汰其他的机器学习算法。深度学习就是让机器可以自我学习的技术。所以，我们需要先搞清楚，深度学习所要服务的机器学习是什么？机器学习：让机器根据一些训练资料，自动找出有用的函数。机器学习可以在语音处理上找出语音识别函数；在影响识别系统中可以找出影像识别函数，其它方面的应用，大致如此。这些函数共同点就是，它们涉及大量的数据和计算方式，人力无法企及，只有靠机器根据大量训练资料，才能得出。通常，机器学习分三步骤：一、人类提供一个函数集给机器；二、人类通过训练资料判定函数优劣；三、机器自动从函数集找出最佳函数。"}
{"content2":"光学字符识别（英语：Optical Character Recognition, OCR）是指对文本资料的图像文件进行分析处理，获取文字及版面信息的过程。 OCR的概念是在1929年由德国科学家Tausheck最先提出来，并申请了专利。后来美国科学家Handel也提出了利用技术对文字进行识别的想法 识别过程 图像输入、预处理 版面分析 字符切割 字符识别 版面恢复 后处理、校对一、验证码的基本知识1. 验证码的主要目的是强制人机交互来抵御机器自动化攻击的。2. 大部分的验证码设计者并不得要领，不了解图像处理，机器视觉，模式识别，人工智能的基本概念。3. 利用验证码，可以发财，当然要犯罪：比如招商银行密码只有6位，验证码形同虚设，计算机很快就能破解一个有钱的账户，很多帐户是可以网上交易的。4. 也有设计的比较好的，比如Yahoo,Google,Microsoft等。而国内Tencent的中文验证码虽然难，但算不上好。二、人工智能，模式识别，机器视觉，图像处理的基本知识1)主要流程：比如我们要从一副图片中，识别出验证码；比如我们要从一副图片中，检测并识别出一张人脸。 大概有哪些步骤呢？1.图像采集：验证码呢，就直接通过HTTP抓HTML，然后分析出图片的url，然后下载保存就可以了。 如果是人脸检测识别，一般要通过视屏采集设备，采集回来，通过A/D转操作，存为数字图片或者视频频。2.预处理：检测是正确的图像格式，转换到合适的格式，压缩，剪切出ROI，去除噪音，灰度化，转换色彩空间这些。3.检测：车牌检测识别系统要先找到车牌的大概位置，人脸检测系统要找出图片中所有的人脸（包括疑似人脸）；验证码识别呢，主要是找出文字所在的主要区域。4.前处理：人脸检测和识别，会对人脸在识别前作一些校正，比如面内面外的旋转，扭曲等。我这里的验证码识别，“一般”要做文字的切割5.训练：通过各种模式识别，机器学习算法，来挑选和训练合适数量的训练集。不是训练的样本越多越好。过学习，泛化能力差的问题可能在这里出现。这一步不是必须的，有些识别算法是不需要训练的。6.识别：输入待识别的处理后的图片，转换成分类器需要的输入格式，然后通过输出的类和置信度，来判断大概可能是哪个字母。识别本质上就是分类。2)关键概念：图像处理：一般指针对数字图像的某种数学处理。比如投影，钝化，锐化，细化，边缘检测，二值化，压缩，各种数据变换等等。1.二值化：一般图片都是彩色的，按照逼真程度，可能很多级别。为了降低计算复杂度，方便后续的处理，如果在不损失关键信息的情况下，能将图片处理成黑白两种颜色，那就最好不过了。2.细化：找出图像的骨架，图像线条可能是很宽的，通过细化将宽度将为1，某些地方可能大于1。不同的细化算法，可能有不同的差异，比如是否更靠近线条中间，比如是否保持联通行等。3.边缘检测：主要是理解边缘的概念。边缘实际上是图像中图像像素属性变化剧烈的地方。可能通过一个固定的门限值来判断，也可能是自适应的。门限可能是图 像全局的，也可能是局部的。不能说那个就一定好，不过大部分时候，自适应的局部的门限可能要好点。被分析的，可能是颜色，也可能是灰度图像的灰度。机器视觉：利用计算机来模式实现人的视觉。 比如物体检测，定位，识别。按照对图像理解的层次的差别，分高阶和低阶的理解。模式识别：对事物或者现象的某种表示方式（数值，文字，我们这里主要想说的是数值），通过一些处理和分析，来描述，归类，理解，解释这些事物，现象及其某种抽象。人工智能：这种概念比较宽，上面这些都属于人工智能这个大的方向。简单点不要过分学院派的理解就是，把人类的很“智能”的东西给模拟出来协助生物的人来处理问题，特别是在计算机里面。三、常见的验证码的破解分析以http://libcaca.zoy.org/wiki/PWNtcha这里PWNtcha项目中的资料为例分析，各种验证码的破解。（方法很多，仅仅从我个人乍看之下觉得可行的方法来分析）1)Authimage使用的反破解技巧：1.不连续的点组成字符2.有一定程度的倾斜设计不好的地方：1.通过纵横的直方图投影，可以找到字幕区域2.通过Hough变换，适当的参数，可以找到近似的横线，可以做倾斜矫正3.字符串的倾斜式面内的，没有太多的破解难度4.字母宽度一定，大小一定2)Clubic使用的反破解技巧：1.字符是手写体设计不好的地方：1.检测切割阶段没有任何技术含量，属于设计的比较丑的2.只有数字，而且手写体变化不大3.表面看起来对识别阶段有难度，仔细分析，发现几乎不用任何高级的训练识别算法，就固定的招某些像素点是否有色彩就够了3)linuxfr.org使用的反破解技巧：1.背景颜色块2.前景的横线或矩形设计不好的地方：1.背景色是单一色块，有形状，通过Region-Growth区域增长来很容易把背景给去掉2.前景色是标准的线条，色彩单一3.字母无粘连4.都是印刷体4)Ourcolony使用的反破解技巧：1.设计的太低级，不屑于去评价设计不好的地方：1.这种验证码，设计的最丑，但还是能把菜鸟搞定，毕竟学计算机的少，搞这个破解的更少，正所谓隔行如隔山5)LiveJournal使用的反破解技巧：1.这个设计略微好点，使用个随机噪音，而且作为前景2.字母位置粗细都有变化设计不好的地方：1.字母没有粘连2.噪音类型单一3.通过在X轴的直方图投影，能准确分割字幕4.然后在Y周作直方图投影,能准确定位高度5.识别阶段，都是印刷体，简单地很四、网上的一些高级验证码1)ICQ2)IMDb3)MS MVPS4)MVN Forum这些类型是被很多人认为比较难得类型，分析一下可以发现，字符检测，定位和分割都不是难。 唯一影响识别率的是IMDBb和MVPS这两类，字体变形略大。总体来说，这些类型的破解也不难，很容易做到50%以上的识别率。五、高级验证码的破解分析时间关系，我简单介绍如何利用图像处理和模式识别技术，自动识别比较高级的验证码。(以风头正劲的Google为例)1)至少从目前的AI的发展程度看，没有简单的做法能自动处理各种不同的验证码，即使能力很强，那么系统自然也十分复杂强大。所以，要想在很简单的算法实现比较高级的验证码破解，必须分析不同验证码算法的特点：作为一般的图像处理和计算机视觉，会考虑色彩，纹理，形状等直接的特征，同时也考虑直方图，灰度等统计特征，还考虑FFT，Wavelet等各种变换后的 特征。但最终目标都是Dimension Reduction（降维）然后利于识别，不仅仅是速度的考虑。从图像的角度看，很多系统都考虑转换为灰度级甚者黑白图片。Google的图片可以看出，颜色变化是虚晃一枪，不存在任何处理难度。难度是字体变形和字符粘连。如果能成功的分割字符，那么后期识别无论是用SVM等分类算法，还是分析笔顺比划走向来硬识别，都相对好做。2)图像处理和粘连分割代码中的part1目录主要完成图像预处理和粘连字符分割001：将图像从jpg等格式转换为位图便于处理002：采用Fix/Adaptive的Threshold门限算法，将图片Bin-Value二值化。（可用003算法）003：采用OSTU分水岭算法，将图片Bin-Value二值化。（更通用，大部分时候效果更好）005：获取ROI感兴趣的区域。006：Edge Trace边缘跟踪。007：Edge Detection边界检测。008：Thin细化去骨架。009：做了一些Tidy整理。（这个一般要根据特定的Captcha算法调整）010：做切割,注意图片中红色的交叉点。011：将边缘检测和骨干交叉点监测的图像合并。（合并过程可以做分析: 比如X坐标偏移门限分析，交叉点区域纹理分析，线条走势分析，等等各种方法，找出更可能的切分点和分离后部件的组合管理。）代码：（代码质量不高，从其他项目拷贝过来，简单修改的。）查看代码(./pstzine_09_01.txt)注： 在这里，我们可以看到，基本的部件（字母是分割开了，但可以造成统一字母的被切割成多个Component。 一种做法是：利用先验知识，做分割； 另外一种做法是，和第二部分的识别结合起来。 比如按照从左至右，尝试增加component来识别，如果不能识别而且component的总宽度，总面积还比较小，继续增加。 当然不排除拒识的可能性。 ）3)字符部件组合和识别。part2的代码展示了切割后的字母组合，和基于svm的字符识别的训练和识别过程。Detection.cpp中展示了ImageSpam检测过程中的 一些字符分割和组合，layout的分析和利用的简单技术。 而Google的验证码的识别，完全可以不用到，仅做参考。SVM及使用：本质上，SVM是一个分类器，原始的SVM是一个两类分类的分类器。可以通过1:1或者1:n的方式来组合成一个多类分类的分类器。 天生通过核函数的使用支持高维数据的分类。从几何意义上讲，就是找到最能表示类别特征的那些向量（支持向量SV）,然后找到一条线，能最大化分类的 Margin。libSVM是一个不错的实现。训练间断和识别阶段的数据整理和归一化是一样的。 这里的简单做法是：首先：#define SVM_MAX +0.999#define SVM_MIN +0.001其次：扫描黑白待识别字幕图片的每个像素，如果为0(黑色，是字母上的像素),那么svm中该位置就SVM_MAX,反之则反。最后：训练阶段，在svm的input的前面，为该类打上标记，即是那一个字母。识别阶段，当然这个类别标记是SVM分类出来。注意：如果是SVM菜鸟，最好找一个在SVM外边做了包装的工具，比如样本选择，交叉验证，核函数选择这些，让程序自动选择和分析。代码：通过ReginGrowth来提取单个单个的字符，然后开始识别。查看代码(./pstzine_09_02.txt)六、对验证码设计的一些建议1.在噪音等类型的使用上，尽力让字符和用来混淆的前景和背景不容易区分。尽力让坏人（噪音）长得和好人（字母）一样。2.特别好的验证码的设计，要尽力发挥人类擅长而AI算法不擅长的。 比如粘连字符的分割和手写体（通过印刷体做特别的变形也可以）。 而不要一味的去加一些看起来比较复杂的噪音或者其他的花哨的东西。即使你做的足够复杂，但如果人也难识别，显然别人认为你是没事找抽型的。3. 从专业的机器视觉的角度说，验证码的设计，一定要让破解者在识别阶段，反复在低阶视觉和高阶视觉之间多反复几次才能识别出来。 这样可以大大降低破解难度和破解的准确率。七、个人郑重申明1.这个问题，本身是人工智能，计算机视觉，模式识别领域的一个难题。我是虾米，菜得不能再菜的那种。作为破解者来说，是出于劣势地位。要做的很好，是很 难得。总体来说，我走的是比较学院派的线路，能真正的破解难度比较高的验证码，不同于网上很多不太入流的破解方法。我能做的只有利用有限的知识，抛砖引玉 而已。 很多OCR的技术，特别是离线手写体中文等文字识别的技术，个人了解有限的很，都不敢在这里乱写。"}
{"content2":"个人感觉，《深度学习实践计算机视觉》的特点是依托工业环境的实践经验，具备较强的实用性和专业性。适合于广大计算机视觉工程领域的从业者、深度学习爱好者、相关专业的大学生和研究生以及对计算机视觉感兴趣的爱好者使用。《深度学习实践计算机视觉》PDF，255页，带书签目录，文字可以复制，缪鹏 著。下载: https://pan.baidu.com/s/1ta-xAU2BX_RlBVmwxEsIJQ提取码: e5f8《深度学习实践:计算机视觉》主要关注计算机视觉领域，基于开源项目介绍最新的算法，各章主要内容如下：第1章对深度学习与计算机视觉进行简要介绍，也会简单介绍开发环境的搭建。第2章主要介绍OpenCV的基本操作及部分高级操作，包括人脸和人眼的检测与识别。第3章着重介绍目前常用的几类深度学习框架，包括PyTorch、Chainer、TensorFlow-Keras和MXNet-Gluon，另外本书中偶尔还会用到ChainerCV和GluonCV。第4章对图像分类进行了介绍，包括经典的网络类型（VGG、ResNet、Inception、Xception、DenseNet），并展示了部分实践操作。第5章对目标检测与识别进行了介绍，包括三种主流的网络结构：YOLO、SSD、Faster R-CNN，并展示了实践操作。第6章介绍图像分割技术，主要从前背景分割（Grab Cut）、语义分割（DeepLab与PSPNet）和实例分割（FCIS、Mask R-CNN、MaskLab、PANet）三个粒度阐述。第7章介绍图像搜索技术，主要指以图搜图方面（CBIR)，以及对应的实践展示。第8章主要介绍图像生成技术，包括三个大方向：Auto-Encoder、GAN和Neural Style Transfer。相比较而言，《深度学习之PyTorch实战计算机视觉》旨在帮助零基础或基础较为薄弱的读者入门深度学习，达到能够独立使用深度学习知识处理计算机视觉问题的水平。《深度学习之PyTorch实战计算机视觉》高清PDF，287页，带书签目录，文字可以复制。下载：https://pan.baidu.com/s/1P0-o29x0ZrXp8WotN7GzcA通过阅读本书，读者将学到人工智能的基础概念及 Python 编程技能，掌握PyTorch 的使用方法，学到深度学习相关的理论知识，比如卷积神经网络、循环神 经网络、自动编码器，等等。在掌握深度学习理论和编程技能之后，读者还会学到如何基于PyTorch 深度 学习框架实战计算机视觉。《深度学习之PyTorch实战计算机视觉》中的大量实例可让读者在循序渐进地学习的同时，不断地获得成就感。计算机视觉是深度学习较为重要的应用领域，学习过程中可以参考：《深度学习与计算机视觉 算法原理、框架应用》+《大数据架构详解：从数据获取到深度学习》下载：https://pan.baidu.com/s/12-s95JrHek82tLRk3UQO_w"}
{"content2":"编者按：2014年度计算机视觉方向的顶级会议CVPR上月落下帷幕。在这次大会中，微软亚洲研究院共有15篇论文入选。今年的CVPR上有哪些让人眼前一亮的研究，又反映出哪些趋势？来听赴美参加会议的微软亚洲研究院实习生胡哲的所见所闻。作者：胡哲微软亚洲研究院实习生计算机视觉（Computer Vision）是近十几年来计算机科学中最热门的方向之一，而国际计算机视觉与模式识别大会（Conference on Computer Vision and Pattern Recognition，简称CVPR）绝对是计算机视觉会议中的翘楚。今年的CVPR在美国俄亥俄州首府哥伦布市（Columbus）召开，地点有点前不着村后不着店的感觉，大多数人都只好老老实实开会。但即便在如此偏远的地方举行会议，CVPR的参会人数还是毫无缩减，这一点在中午领饭的时候可以深深体会到。当然，开会的核心绝对不在于地点和提供的饮食，虽然这也很重要。所有人千里迢迢从全世界过来汇聚在一起的主要目的还是感受流行的研究趋势以及与大家交流各自的研究突破和创新的想法。非常荣幸我这次能有三篇论文被大会录取，这既是对我个人在计算机视觉领域研究的巨大肯定，也让我得以有机会能够去往CVPR大会的现场去感受这个领域最前沿的研究成果和前瞻的趋势。接下来，我就与大家分享一下这次的参会感受。一、繁荣的深度学习深度学习（Deep Learning）是当下最热门的方向之一，今年的论文中标题带deep字眼的论文就有16篇（其中oral presentation 4篇——在CVPR等大型会议中，由于论文数量众多，大部分的论文都是以海报的形式作讲演。而大会的委员会在所有其中挑选少量出色的工作（占所有投稿的5%）面对所有研究者演讲）。深度学习热潮爆发以来，诸多研究者都在不懈地努力着，希望能够把它应用于解决计算机视觉的各种任务上，从高层次（high-level）的识别（recognition），分类（classification）到低层次（low-level）的去噪（denoising）。让人不禁联想起当年的稀疏表达（sparse representation）的热潮，而深度学习如今的风靡程度看上去是有过之而无不及。深度学习也有横扫high-level问题的趋势，high-level的很多方向都在被其不断刷新着数据。以往的改进都是1，2个点的增长，如今使用深度学习轻松刷出5，6点，这给很多非深度学习方法研究者巨大的压力。虽说深度学习是大热方向，可计算机视觉界的研究者对深度学习的态度也是很鲜明的两派——支持与观望，也给其他研究趋势带来了一些影响（原因接下来说）。作为强大的特征（feature）学习工具，获得大量的支持与推广自然不必说，很多原本观望的研究者们在目睹深度学习的优秀表现后也都开始投身于此。持观望态度的人们一部分可能仍并不了解深度学习的机理，另外一大部分相信是对深度学习将给计算机视觉带来的贡献持保守态度。虽然笔者赞叹于深度学习的强大能力，可对此也是持保守态度。诚然深度学习作为一个工具异常强大。在给定足够多的训练集的情况下，它可以帮助用户学习到这个任务下的具有很强分辨能力的特征。可是这个训练过程近乎黑箱，学习出的系统也很难给解决的问题带来更深刻的理解。二、为基础模型研究正名也许因为如此，我认为本次的评奖有些指引方向的感觉。本次大会的最佳论文颁给了研究camera motion和shape recovery关系的文章What Camera Motion Reveals About Shape with Unknown BRDF（single author!）, Honorable mention给了利用structured light研究shape的论文3D Shape and Indirect Appearance by Structured Light Transport。这两篇论文都可以算是研究3D几何模型的。不仅评奖如此，计算机视觉领域的前辈也亲自站出来力挺一下基础模型的研究，其中Jean Ponce亲自写了一篇论文(oral)来继续探讨trinocular geometry的传统假设不成立时如何保证三个相机visual rays相交的情况。这些论文无一不是对计算机视觉基础问题和基础模型的深入研究，考虑前人没有研究过的问题的系统分析。这些文章的获奖也是鼓励我们年轻研究人员静下心来做基础问题的研究，不轻易追赶当下热潮。做追赶浪潮的弄潮儿容易，可是怎么样保证研究工作不会轻易被遗忘在时间里，或者说怎么做对领域有贡献的研究工作，绝对是我们研究者们需要思考的问题。另外，并不要认为已经写入教科书的内容就已经板上钉钉没有研究价值了。有一些理论也是建立在理想的假设满足的前提下，所以它们仍然可以在质疑的眼光下去进行深造。三、尚未被深度学习渗透的Low-level Vision计算机视觉的问题可以根据他们的研究对象和目标分成三大类，low-level，mid-level, 和high-level。Low-level问题主要是针对图像本身及其内在属性的分析及处理，比如判断图片拍摄时所接受的光照，反射影响以及光线方向，进一步推断拍摄物体的几何结构；再如图片修复，如何去除图片拍摄中所遇到的抖动和噪声等不良影响。High-level问题主要是针对图像内容的理解和认知层面的，比如说识别与跟踪图像中的特定物体与其行为；根据已识别物体的深入推断，比如预测物体所处的场景和即将要进行的行为。Mid-level是介于以上两者之间的一个层面，个人理解是着重于特征表示，比如说如何描述high-level问题中的目标物体，使得这种描述有别于其他的物体。可以大致认为，low-level的内容可以服务于mid-level的问题，而mid-level的内容可以服务于high-level的问题。由于这种分类不是很严格，所以也会出现交叉的情况。深度学习在计算机视觉界主要是作为一种特征学习的工具，可以姑且认为是mid-level的。所以之前提到的high-level的问题受深度学习的影响很大就是这个原因。相比较而言low-level问题受到深度学习的冲击会小很多，当然也有深度学习用于去噪（denoise）和去模糊（deblur）等low-level问题的研究。对于受到深度学习良好表现困扰的年轻研究者们，也不妨来探寻low-level很多有意思的研究。这些年，MIT的Bill Freeman组就做了一些很有趣的low-level问题，比如放大视频中出现的肉眼难以察觉的细小变化（Eulerian Video Magnification for Revealing Subtle Changes in the World），还有这次CVPR的文章Camouflaging an Object from Many Viewpoints就是讲如何在自然环境中放置和涂染一个立方体，让其产生变色龙般的隐藏效果。诸如此类的研究也让研究这件事变得有趣和好玩。笔者目前也正专注于low-level中去模糊（deblur）的研究。去模糊的意思是借助某种方法将拍照中出现的模糊图像恢复成清晰图像。这个问题是一个已经被研究了很多年的问题——去卷积（deconvolution），自上世纪5，60年代起，就有很多知名研究工作出现。这方面研究到近十年取得了很多突破，在处理相机抖动引起的模糊中出现了不少有影响力的的工作。而Adobe公司2013年将这方面的算法作为一个重要特征放进了Photoshop中，更是成了鼓舞该领域的研究动力。美国FBI就有利用Photoshop的去模糊功能修复图片并帮助破案的例子，笔者去年在Adobe实习期间看到了FBI发来的感谢信。这次笔者被CVPR 2014录取的三篇文章都是关于去模糊的研究。一篇是针对模糊图像的一个主要来源——暗光照情况下的图像，设计的一个基于光斑（light streak）的去模糊算法（Deblurring Low-light Images with Light Streaks）。这个算法自动检测暗光情况下常见的光斑，并利用光斑作为模糊核（blur kernel）的约束。它对解决暗光下模糊图片非常有效，而且光斑这一现象不仅出现在低光下，在普通的模糊图像中也会出现，只需要场景中有与周边环境有颜色差别的小型物体出现。读者可以在我的个人主页上下载代码进行尝试。还有一篇是说从一张模糊图像中，我们不仅可以估计相机的抖动，还可以发掘出场景的深度（Joint Depth Estimation and Camera Shake Removal from Single）。这乍听上去像是不可能完成的任务，可实际上图像的模糊是同时包含了相机抖动和场景深度信息的。读者也可以这样认为，我们拍摄模糊图像的过程也可以看作是拍摄一小段video的过程，这样的话我们相当于拥有了一个多角度立体（stereo）的输入！第三篇是针对文字模糊图片设计的一个简单有效的算法，可以用于文字识别前的预处理（Deblurring Text Image via L0-Regularized Intensity and Gradient Prior）。四、Depth Sensor（深度传感器）及深度图像相关近几年来从Depth Sensor得到的深度图像的相关研究一直是学术界以及工业界重点关注的问题。特别是工业界，很多Depth Sensor相关的创业公司如雨后春笋般在业界涌现，他们也获得了广泛的关注和不菲的投资，这次赞助CVPR的就有多家这样的创业公司。不仅如此，很多大公司也都积极的投身于做自己的Depth Sensor，或者嵌入到自己的产品中。Depth Sensor为何有如此大的影响力，大家肯定早已有诸多见解。它作为一种新的输入数据，给了传统输入数据（2D）一个新的像素级别的维度——深度。这不仅给研究者们开拓了以RGBD输入数据为核心的旧问题新方向，而且由于深度图像的帮助下也让很多算法更加实用。这也让CV研究离工业界的产品更紧密了。Depth Sensor的成熟以及CV领域相关研究的发展，也提供给增强现实（Augmented Reality）这个未来科技感十足的方向一个重要的接口。所有的这些都昭示着Depth Sensor是一个非常有价值而且在一段时间内还将是非常热门的方向。微软亚洲研究院在这个方向上也有一篇利用depth sensor做手部跟踪的oral论文（Realtime and Robust Hand Tracking from Depth）。通过重新定义手的模型和能量方程，这个工作将手部跟踪做到了实时并且算法也很鲁棒。在PC上不用GPU也达到了25FPS（每秒显示帧数），而平均误差在测试数据上降低到10mm，相比其他方法提升50%左右。对手势的准确识别是现在很流行的一个问题。因为技术的进步已经让传统的输入方式（比如鼠标）处于更新换代的边缘了，如今通过depth sensor与手势来实现人机的实时交互将可能带来下一个输入方式的革命。所以这个工作是很有价值与深远影响的，也因此而获得了oral演讲的资格。另外，微软亚洲研究院在今年的CVPR发表的另外一篇oral论文也是应用很广的一个问题——人脸对准（Face Alignment at 3000 FPS via Regressing Local Binary Features）。通过采用局部学习的准则降低随机森林（random forest）的任务难度，以得到更好的局部特征（local feature）。同时，整体上的结构学习帮助算法更加鲁棒。这个项目实现了快速的人脸对准以及人脸跟踪。在相同精度下，它比以往的方法快了数十倍，在PC上单核3000FPS，手机上单核300FPS。这个结果很令人振奋，因为手机及移动设备已经很大程度的改变人们的生活方式，可是相比PC，手机的处理能力有限，那么就需要更加快速稳定的算法。这个工作就为在手机及移动设备对人脸的实时处理提供了坚实的基础。作者简介：胡哲，微软亚洲研究院实习生，本科毕业于浙江大学，目前在加州大学Merced分校攻读博士。曾在Adobe创新科技实验室实习，研究方向为计算机视觉和图像处理。在CVPR, ECCV, BMVC等知名国际会议上发表论文7篇（oral 2篇），并担任多家期刊及会议的审稿人，如TIP, ECCV, ACCV等。____________________________________________________________________________________相关阅读威尔士柯基犬，计算机视觉，以及深度学习的力量漫谈2014年人机交互（CHI）大会图形学创世纪——写在SIGGRAPH 40年的边上欢迎关注微软亚洲研究院人人网主页：http://page.renren.com/600674137微软亚洲研究院微博：http://t.sina.com.cn/msrafrom: http://blog.sina.com.cn/s/blog_4caedc7a0102uyjj.html"}
{"content2":"IBM、谷歌、微软、Facebook和亚马逊等公司正投入巨资进行研发，并纷纷收购在机器学习、神经网络、神经语言和图像处理等领域取得进展的初创公司。这些开源人工智能应用软件处在人工智能研究领域的最前沿。人工智能是技术研究领域最炙手可热的领域之一。IBM、谷歌、微软、Facebook和亚马逊等公司正投入巨资进行研发，并纷纷收购在机器学习、神经网络、神经语言和图像处理等领域取得进展的初创公司。考虑到人工智能如此受关注，斯坦福大学的专家最近撰写的一份智能研究报告得出结论：“现在到2030年人工智能可能会出现越来越有用的应用，有可能给我们的社会和经济带来深远的积极影响，”也就不足为奇了。我们在本文中专注于开源人工智能工具，着重介绍15个知名度最大的开源人工智能项目。1. CaffeCaffe是加州大学伯克利分校攻读博士学位者的杰作，这是一种深度学习框架，基于表达式架构和可扩展代码。速度快是它赖以成名的特点，因而在研究人员和企业用户当中都备受欢迎。据官方网站声称，仅仅使用一个英伟达K40 GPU，它在短短一天内就能够处理6000多万个图像。它由伯克利视觉和学习中心(BVLC)管理，英伟达和亚马逊等公司提供了拨款，支持它的发展。相关链接：http://caffe.berkeleyvision.org2. CNTKCNTK的全称是计算网络工具包，它是微软的开源人工智能工具之一。它声称拥有出众的性能，无论在只有CPU的系统上运行，在只有一个CPU的系统上运行，在拥有多个GPU的系统上运行，还是在拥有多个GPU的多台机器上运行，都是如此。微软主要用它来研究语音识别，但是它同样适用于其他应用领域，比如机器翻译、图像识别、图像字幕、文本处理、语言理解和语言建模。相关链接：https://www.cntk.ai3. Deeplearning4jDeeplearning4j是一种面向Java虚拟机(JVM)的开源深度学习库。它在分布式环境中运行，可与Hadoop和Apache Spark整合起来。它让用户可以配置深度神经网络，与Java、Scala及其他JVM语言兼容。该项目由一家名为Skymind的商业公司管理，该公司提供收费的支持、培训和Deeplearning4j的企业发行版。相关链接：http://deeplearning4j.org4. 分布式机器学习工具包与CNTK一样，分布式机器学习工具包(DMTK)是微软的开源人工智能工具之一。它是为大数据应用领域设计，旨在更快地训练人工智能系统。它包括三大部分：DMTK框架、LightLDA主题模型算法以及分布式(Multisense)单词嵌入算法。微软声称，在8个集群机器上，它能够“针对拥有1000多亿个权标的文档集合，训练拥有100万个主题和1000万个单词词汇表(共有10万亿个参数)的主题模型，”这个成绩是其他工具无法比拟的，这也证明了DMTK的速度有多快。相关链接：http://www.dmtk.io5. H2OH2O更加专注于人工智能在企业领域的应用，而不是在研究领域的应用，它的用户包括诸多大公司：第一资本、思科、尼尔森Catalina、贝宝及Transamerica。它声称让任何人都可以使用机器学习和预测分析的强大功能，解决业务问题。它可用于预测建模、风险及欺诈分析、保险分析、广告技术、医疗保健和客户情报。它有两种开源版本：标准的H2O和Sparkling Water，后者与Apache Spark集成起来。它还提供收费的企业支持。相关链接：http://www.h2o.ai6. MahoutMahout是Apache基金会下面的一个项目，是一种开源机器学习框架。据官方网站声称，它提供三种主要的特性：用于构建可扩展算法的编程环境、面向Spark和H2O等工具的预制算法，以及名为Samsara的向量数学试验环境。使用Mahout的公司包括：Adobe、埃森哲、Foursquare、英特尔、领英、推特、雅虎及其他许多公司。可通过官方网站上所列的第三方获得专业支持。相关链接：http://mahout.apache.org7. MLlibApache Spark以速度快著称，它已成为最流行的大数据处理工具之一。MLlib是Spark的可扩展机器学习库。它与Hadoop整合起来，可与NumPy和R协同操作。它包括一大批机器学习算法，可用于分类、回归、决策树、推荐、聚类、主题建模、特性转换、模型评估、机器学习管道构建、机器学习持久性、生存分析、频繁项集、顺序模式挖掘、分布式线性代数和统计。相关链接：https://spark.apache.org/mllib/8. NuPICNuPIC由一家名为Numenta的公司管理，这是一种开源人工智能项目，基于一种名为分层式即时记忆(即HTM)的理论。实际上，HTM试图建立一种模仿人类大脑新皮层而建的计算机系统。目的在于制造“处理许多认知任务时接近或胜过人类表现”的机器。除了开源许可证外，Numenta还提供采用商业许可证的NuPic，它还提供作为它技术底层的专利方面的许可证。相关链接：http://numenta.org9. OpenNNOpenNN为深入了解人工智能的研究人员和开发人员而设计，这是一种用于实现神经网络的C++编程库。主要特性包括：深度架构和卓越性能。官方网站上有全面的说明文档，包括解释神经网络基础知识的入门教程。可通过Artelnics获得OpenNN的收费支持，总部位于西班牙的这家公司主攻预测分析。相关链接：http://www.opennn.net10. OpenCycOpenCyc由一家名为Cycorp的公司开发，它让用户可以访问Cyc知识库和常识推理引擎。它包括239000多个术语、约2093000个三元组以及大约69000个owl:sameAs链接(指向外部语义数据命名空间)。它用于丰富域名建模、语义数据整合、文本理解、特定领域专家系统和游戏人工智能。这家公司还提供Cyc的另外两个版本：一个是非开源免费版本，面向研究人员;另一个面向企业用户，需要收费。相关链接：http://www.cyc.com/platform/opencyc/11. Oryx 2Oryx 2建立在Apache Spark和Kafka上，这是一种专门的应用开发框架，面向大规模的机器学习。它使用了一种独特的lambda架构，有三个层次。开发人员可使用Oryx 2来构建新的应用程序，它还包括一些预制应用程序，处理常见的大数据任务，比如协作过滤、分类、回归和聚类。大数据工具厂商Cloudera建立了最初的Oryx 1项目，一直大力参与持续开发工作。相关链接：http://oryx.io12. PredictionIO今年2月份，Salesforce收购了PredictionIO，后来在7月份，它把该平台连同商标一起捐献给了Apache基金会，该基金会将它列为孵化器项目。所以，虽然Salesforce使用PredictionIO技术来完善自己的机器学习功能，但是开源版本方面的工作也会继续下去。它可帮助用户构建拥有机器学习功能的预测引擎，这些功能可用来部署实时响应动态查询的Web服务。相关链接：https://prediction.io13. SystemMLSystemML最初由IBM开发，现在它是Apache旗下的一个大数据项目。它提供了一种高度可扩展的平台，可以实施用R或类似Python的语法编写的高级运算和算法。企业已经在用它来跟踪汽车维修方面的客户服务，引导机场客流量，或者将社交媒体数据与银行客户联系起来。它可以在Spark或Hadoop上运行。相关链接：http://systemml.apache.org14. TensorFlowTensorFlow是谷歌的开源人工智能工具之一。它提供了用于数字计算的库，使用数据流图。它可以在众多不同的搭载单一或多个CPU和GPU的系统上运行，甚至可以在移动设备上运行。它拥有深度灵活性、真正的可移植性、自动差分功能，并支持Python和C++。官方网站上列有非常丰富的教程和实用文章，可供有兴趣使用或扩展其功能的开发人员或研究人员使用。相关链接：https://www.tensorflow.org15. TorchTorch自称是“一种科学计算框架，广泛支持把GPU放在首位的机器学习算法。”这里的重点在于灵活性和速度。此外，很容易与众多方面的软件包结合使用：机器学习、计算机视觉、信号处理、并行处理、图像、视频、音频和网络。它依赖一种名为LuaJIT的脚本语言，这种语言基于Lua。相关链接：http://torch.ch"}
{"content2":"选取论文的原则：（1）期刊论文，主要来源于以下期刊：TPAMI，IJCV，TIP，CVIU，IVC，MVA，PR，JMIV，IJPRAI…（2）发表在2000年以后（3）SCI检索次数大于1000，来源于Web of Science数据库，2012年12月初的检索结果Top 20 榜单如下：[1] D. G. Lowe, “Distinctive image features from scale-invariant keypoints,” International Journal of Computer Vision, vol. 60, no. 2, pp. 91-110, Nov, 2004. (Cited=5663)[2] J. B. Shi, and J. Malik, “Normalized cuts and image segmentation,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 22, no. 8, pp. 888-905, Aug, 2000. (Cited=2165)[3] T. F. Chan, and L. A. Vese, “Active contours without edges,” IEEE Transactions on Image Processing, vol. 10, no. 2, pp. 266-277, Feb, 2001. (Cited=2153)[4] D. Comaniciu, and P. Meer, “Mean shift: A robust approach toward feature space analysis,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 24, no. 5, pp. 603-619, May, 2002. (Cited=1910)[5] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality assessment: From error visibility to structural similarity,” IEEE Transactions on Image Processing, vol. 13, no. 4, pp. 600-612, Apr, 2004. (Cited=1879)[6] A. W. M. Smeulders, M. Worring, S. Santini, A. Gupta, and R. Jain, “Content-based image retrieval at the end of the early years,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 22, no. 12, pp. 1349-1380, Dec, 2000. (Cited=1697)[7] P. Viola, and M. J. Jones, “Robust real-time face detection,” International Journal of Computer Vision, vol. 57, no. 2, pp. 137-154, May, 2004. (Cited=1634)[8] A. K. Jain, R. P. W. Duin, and J. C. Mao, “Statistical pattern recognition: A review,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 22, no. 1, pp. 4-37, Jan, 2000. (Cited=1546)[9] Z. Y. Zhang, “A flexible new technique for camera calibration,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 22, no. 11, pp. 1330-1334, Nov, 2000. (Cited=1516)[10] B. Zitova, and J. Flusser, “Image registration methods: a survey,” Image and Vision Computing, vol. 21, no. 11, pp. 977-1000, Oct, 2003. (Cited=1422)[11] S. Belongie, J. Malik, and J. Puzicha, “Shape matching and object recognition using shape contexts,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 24, no. 4, pp. 509-522, Apr, 2002. (Cited=1321)[12] P. J. Phillips, H. Moon, S. A. Rizvi, and P. J. Rauss, “The FERET evaluation methodology for face-recognition algorithms,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 22, no. 10, pp. 1090-1104, Oct, 2000. (Cited=1298)[13] Y. Boykov, O. Veksler, and R. Zabih, “Fast approximate energy minimization via graph cuts,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 23, no. 11, pp. 1222-1239, Nov, 2001. (Cited=1197)[14] D. Scharstein, and R. Szeliski, “A taxonomy and evaluation of dense two-frame stereo correspondence algorithms,” International Journal of Computer Vision, vol. 47, no. 1-3, pp. 7-42, Apr-Jun, 2002. (Cited=1174)[15] K. Mikolajczyk, and C. Schmid, “A performance evaluation of local descriptors,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 27, no. 10, pp. 1615-1630, Oct, 2005. (Cited=1166)[16] D. Comaniciu, V. Ramesh, and P. Meer, “Kernel-based object tracking,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 25, no. 5, pp. 564-577, May, 2003. (Cited=1109)[17] T. Ojala, M. Pietikainen, and T. Maenpaa, “Multiresolution gray-scale and rotation invariant texture classification with local binary patterns,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 24, no. 7, pp. 971-987, Jul, 2002. (Cited=1076)[18] C. Stauffer, and W. E. L. Grimson, “Learning patterns of activity using real-time tracking,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 22, no. 8, pp. 747-757, Aug, 2000. (Cited=1070)[19] M. H. Yang, D. J. Kriegman, and N. Ahuja, “Detecting faces in images: A survey,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 24, no. 1, pp. 34-58, Jan, 2002. (Cited=1032)[20] T. F. Cootes, G. J. Edwards, and C. J. Taylor, “Active appearance models,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 23, no. 6, pp. 681-685, Jun, 2001. (Cited=989)补充2篇TPAMI，山老师推荐的，南理工杨健老师的2DPCA和浙大何晓飞老师的LPP。[1] J. Yang, D. Zhang, A. Frangi, and J. Yang, “Two-dimensional PCA: a new approach to appearance-based face representation and recognition,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 26, no. 1, pp. 131-137, Jan, 2004. (Cited=625)[2] X. He, S. Yan, Y. Hu, P. Niyogi, and H. Zhang, “Face recognition using Laplacianfaces,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 27, no. 3, pp. 328-340, Mar, 2005. (Cited=724)简单小结：3篇IJCV，14篇TPAMI，2篇TIP，1篇IVC倒是有些意外，不过是综述性质的文章，也是情理之中。欢迎各位大牛对每篇文章进行点评。"}
{"content2":"前言现在是2018年3月，距离2016年3月AlphaGo在围棋大战中4:1战胜李世石已有两年时间。最近几年在谷歌等大公司的领带下，“人工智能”这个词汇反复出现在公众的视野中。随着人机大战的结束，“人工智能”这个大风口也被引爆到了极致。随处可见被冠以“智能”的物品：智能水杯、智能手表、各种智能家居等等。而本人也给这场大潮贡献了几百个大洋。深度学习“深度学习”从2006年开始取得突破，AlphaGO就是用了深度学习的算法才能在围棋上战胜人类。而深度学习算法的本质上还是一个神经网络，而神经网络早在20世代40年代就被提出，并且在80年代的时候就达到了研究的热潮。现在“深度学习”算法只不过是将神经网络的隐含层加多了，而本质上还是神经网络。下图中，左边是传统的神经网络算法的示意图，右边是现在的“深度学习”算法。而“神经网络”算法，所依赖的数学基础就是概率论。简单点说，语音识别算法在语料库中找到最接近的文本；自动驾驶算法算出最安全的行车路线；AlphaGo下棋时，总是算出胜率最大的落子等等。虽然人脑会基于概率去做某些判断，但绝对不是仅仅只基于概率。总结一句，人工智能的核心理论从上世纪80年代到现在没有本质上的进步仅仅用概率算法不可能实现具有意识的人工智能大数据现在“大数据”一词常和“人工智能”一起被提到，它有另一个很俗气的名字——“很多很多数据”比如用户数据（用户浏览过的网页、查看或购买过的物品、甚至是wx或qq的聊天内容）、交通数据、天气数据、股市数据等等。现在上网人数的爆炸，服务器存储能力和运算能力的提高，使存储、分析“很多很多数据”成为可能。在商业宣传价值上“很多很数据”这个词汇，完全不能和“大数据”三个字相比。“大数据”是从英文\"big data\"直译过来，如果这个概念最先从中国发明出来，可能就叫“宏数据”是不是很有格调？人工智能的层次按照现在业界的认可，人工智能有三个层次：一、运算智能快速计算和记忆存储能力。二、感知智能视觉、听觉、触觉等感知能力。人和动物都具备，能够通过各种智能感知能力与自然界进行交互。自动驾驶汽车，就是通过激光雷达等感知设备和人工智能算法，实现这样的感知智能的。三、认知智能通俗讲是“能理解会思考”。人类有语言，才有概念，才有推理，所以概念、意识、观念等都是人类认知智能的表现。其实上面1，2两个智能都只是在软件层面上实现的，只不过人机互相的接口做得更容易让人使用，也就是所谓的更“智能”了。商业的力量很可怕, 可以改变一个词汇的意思！！现在“智能”两字几乎等同于“好用”，不信？试试下面的说法——“智能电视”说成“好用的电视” “智能手表”说成“好用的手表” “智能手机”说成“好用的手机” “智能体重秤”说成“好用的体重秤”换成好用后，完全说得通，并且更好理解，只不过没有“智能”二字高大上而已，没有“智能”二字更具商业宣传价值而已。可能的突破人工智能，从图灵机-冯诺依曼体系结构到现在可以说是没有本质上的突破。在这里，本人大胆的预言可能对人工智能产生本质改变的四个突破点：基础物理学的突破。量子、弦论或M理论的完备。全新数学分支的创建。为实现有别于概率的智能算法提供理论支持。脑生物学的突破。彻底弄明白人脑结构、为何会产生意识，并且为之建立精准的数学模型。打破图灵机-冯诺依曼体系结构计算机的出现。量子、光子或生物计算机的完全成熟发展。作为一员技术男，还是挺希望看到人工智能有所突破。在看到电影《机器人之恋》、《超体》等超级智能体后总是兴奋不已。最后如果你恰好读完了本文，并且也同意本文中的大多数观点，下面有几个总结：1. 知道了最近几年的“人工智能”是商业词汇，但我们能做的确实不多。毕竟也是大佬们玩的游戏，作为吃瓜群众的我们不可能让人们去忘记“人工智能”这个词。不过，我们自己可以做到加强一下就这个词汇的抵抗力，在选购商品时如果出现“人工智能”家居之类的词汇，得看它是否真的是自己所需，还只是打了这个标签。2. 虽然现在这场“人工智能”热潮名不副实，但信息技术的大应用是实实在在的——这是一个不错的时代。3. 上文说的到“认知智能”层次的人工智能，至少在最近100年内不会出现，刚刚逝去不久的物理学家霍金就很担心人工智能会反过来统治人类。我想说的是，请好好放心吧，上面说的四个突破点中至少得完成两个才有可能到达“奇点”，但现在好像一个都没有。4. 当“人工智能机器人”对冠以“智能”二字的产品有购买欲望时，大概这时才能叫真正实现了人工智能吧！--END--"}
{"content2":"论文导读：介绍了人工智能的概念及其目前发展概况,对人工智能的几种类型及应用，如：模式识别、专家系统作了简要的介绍。并对人工智能今后的发展前景进行了分析。关键词：人工智能1引言人工智能(ArtificialIntelligence) ,英文缩写为AI，也称机器智能。“人工智能”一词最初是在1956年Dartmouth学会上提出的。它是计算机科学、控制论、信息论、神经生理学、心理学、语言学等多种学科互相渗透而发展起来的一门综合性学科。从计算机应用系统的角度出发，人工智能是研究如何制造智能机器或智能系统，来模拟人类智能活动的能力，以延伸人们智能的科学。2目前人工智能技术的研究和发展状况目前，人工智能技术在美国、欧洲和日本依然飞速发展。在AI技术领域十分活跃的IBM公司，已经为加州劳伦斯·利佛摩尔国家实验室制造了ASCIWhite 电脑，号称具有人脑的千分之一的智力能力，而正在开发的更为强大的新超级电脑—— “蓝色牛仔”（Blue Jean），据其研究主任保罗·霍恩称， “蓝色牛仔”的智力水平将大致与人脑相当。3技术应用随着AI的技术的发展，现代几乎各种技术的发展都涉及到了人工智能技术，可以说人工智能已经广泛应用到许多领域，其典型的应用包括：3.1符号计算计算机最主要的用途之一就是科学计算,科学计算可分为两类:一类是纯数值的计算,例如求函数的值; 另一类是符号计算,又称代数运算,这是一种智能化的计算, 处理的是符号。符号可以代表整数、有理数、实数和复数,也可以代表多项式,函数,集合等。随着计算机的普及和人工智能的发展,相继出现了多种功能齐全的计算机代数系统软件, 其中Mathematic和Maple 是它们的代表，由于它们都是用C 语言写成的, 所以可以在绝大多数计算机上使用。3.2模式识别模式识别就是通过计算机用数学技术方法来研究模式的自动处理和判读。这里,我们把环境与客体统称为“模式”。论文参考网。用计算机实现模式（文字、声音、人物、物体等）的自动识别，是开发智能机器的一个关键的突破口，也为人类认识自身智能提供线索。计算机识别的显著特点是速度快、准确性和效率高。识别过程与人类的学习过程相似。以“语音识别”为例： 语音识别就是让计算机能听懂人说的话，一个重要的例子就是七国语言（英、日、意、韩、法、德、中）口语自动翻译系统。该系统实现后，人们出国预定旅馆、购买机票、在餐馆对话和兑换外币时，只要利用电话网络和国际互联网，就可用手机、电话等与“老外”通话。3.3机器翻译机器翻译是利用计算机把一种自然语言转变成另一种自然语言的过程，用以完成这一过程的软件系统叫做机器翻译系统。目前，国内的机器翻译软件不下百种，根据这些软件的翻译特点，大致可以分为三大类：词典翻译类、汉化翻译类和专业翻译类。词典类翻译软件代表是“金山词霸”了，堪称是多快好省的电子词典，它可以迅速查询英文单词或词组的词义，并提供单词的发音，为用户了解单词或词组含义提供了极大的便利。汉化翻译软件的典型代表是“东方快车2000”，它首先提出了“智能汉化”的概念，使翻译软件的辅助翻译作用更加明显。3.4机器学习机器学习是机器具有智能的重要标志，同时也是机器获取知识的根本途径。有人认为，一个计算机系统如果不具备学习功能，就不能称其为智能系统。机器学习主要研究如何使计算机能够模拟或实现人类的学习功能。机器学习是一个难度较大的研究领域，它与认知科学、神经心理学、逻辑学等学科都有着密切的联系，并对人工智能的其他分支，如专家系统、自然语言理解、自动推理、智能机器人、计算机视觉、计算机听觉等方面，也会起到重要的推动作用。3.5问题求解人工智能的第一大成就是下棋程序,在下棋程度中应用的某些技术,今天的计算机程序已能够达到下各种方盘棋和国际象棋的锦标赛水平。但是,尚未解决包括人类棋手具有的但尚不能明确表达的能力。如国际象棋大师们洞察棋局的能力。论文参考网。另一个问题是涉及问题的原概念,在人工智能中叫问题表示的选择,人们常能找到某种思考问题的方法,从而使求解变易而解决该问题。到目前为止,人工智能程序已能知道如何考虑它们要解决的问题,即搜索解答空间,寻找较优解答。3.6逻辑推理与定理证明逻辑推理是人工智能研究中最持久的领域之一,其中特别重要的是要找到一些方法,只把注意力集中在一个大型的数据库中的有关事实上,留意可信的证明,并在出现新信息时适时修正这些证明。医疗诊断和信息检索都可以和定理证明问题一样加以形式化。因此,在人工智能方法的研究中定理证明是一个极其重要的论题。3.7自然语言处理自然语言的处理是人工智能技术应用于实际领域的典型范例,经过多年艰苦努力,这一领域已获得了大量令人注目的成果。目前该领域的主要课题是:计算机系统如何以主题和对话情境为基础,注重大量的常识——世界知识和期望作用,生成和理解自然语言。这是一个极其复杂的编码和解码问题。3.8分布式人工智能分布式人工智能在20世纪70年代后期出现，是人工智能研究的一个重要分支。分布式人工智能系统一般由多个Agent（智能体）组成，每一个Agent又是一个半自治系统，Agent之间以及Agent与环境之间进行并发活动，并通过交互来完成问题求解。3.9计算机视觉计算机视觉是一门用计算机实现或模拟人类视觉功能的新兴学科。其主要研究目标是使计算机具有通过二维图像认知三维环境信息的能力，这种能力不仅包括对三维环境中物体形状、位置、姿态、运动等几何信息的感知，而且还包括对这些信息的描述、存储、识别与理解。目前，计算机视觉已在人类社会的许多领域得到成功应用。例如，在图像、图形识别方面有指纹识别、染色体识字符识别等；在航天与军事方面有卫星图像处理、飞行器跟踪、成像精确制导、景物识别、目标检测等；在医学方面有图像的脏器重建、医学图像分析等；在工业方面有各种监测系统和生产过程监控系统等。"}
{"content2":"作者：张达衢  摘自中国论文网 原文地址：http://www.xzbu.com/4/view-8299582.htm【关键词】人工智能；发展现状；未来展望【中图分类号】TP18 【文献标志码】A 【文章编号】1673-1069（2017）04-0107-021 引言2016年年初，韩国围棋国手李在石与围棋程序Alpha Go对弈中首战失利，再一次将人工智能拉入了公众的视野，使其成为2016年度话题度最高的科技之一。不可否认，近些年来人工智能发展迅速，很多人工智能产品已经开始进入人们的家中，如扫地机器人、智能保姆等，虽然它们还没有美国大片《终结者》中所描述得那么先进，但从前遥不可及的人工智能概念正在一步步变为现实却是不争的事实。人工智能的现状如何，它又将如何发展，都是学界较为关注的课题。2 人工智能综述2.1 人工智能的概念人工智能即AI，其英文全称为Artificial Intelligence。人工智能的概念要从人工和智能两方面来了解，所谓人工就是指人工智能脱胎于人类的文明，是人类智慧的产物；而智能则是指具有人工智能的计算机或其他�子设备可以模拟人类的智能行为和思维方式，人工智能是计算机科学的一个分支，它的近期主要目标在于研究用机器来模仿和执行人脑的某些智能功能，并开发相关理论和技术。2.2 人工智能的现实应用如今的人工智能机器，可以在胜任一些复杂脑力劳动的同时，辅助人类进行记忆和逻辑运算等活动。现阶段学者已经研制出了一些可以模拟人类精神活动的电子机器，经过完善升级，这些电子机器将有希望超越人类的能力，协助人类完成一些执行难度较大的工作。但是目前研制出的自动化系统或者机器人虽然可以代替部分人类劳动，却还没有到达可以实现人类多方面协调和自我学习升级的智能水平，要制造出一款可以完全拥有人类智慧的机器，还需进一步深入研究。还有一些人工智能产物经常应用于各种商业用途，例如单位内部的客户信息系统，决策支持系统，以及我们在世面上可以看见的医学顾问、法津顾问等软件。3 人工智能发展现状3.1 智能接口技术研究现状人工智能接口研究就是为了实现人机交流，为此学者必须从理论和实践两方面努力，解决计算机对文字和语言的理解与翻译、对自我的表达等功能问题。由于智能接口技术的研究和应用，计算机技术的发展获得了极大的推动力，在运行速率和人机交流等方面都有巨大提升。3.2 数据挖掘技术研究现状数据挖掘技术主要是对各类模糊的、大量的应用数据、人未知的、潜在已经存在的数据进行整理挖掘进行细致的研究，寻找出对研究有用的数据。目前，数据库、人工智能、数理统计已经成为数据挖掘技术的三大技术支撑，以基础理论、发现算法、可视化技术、知识表示方法、半结构化等作为研究内容，为数据挖掘技术的发展提供理论和技术支持。3.3 主体系统研究现状主体系统可以实现机器意图和想法的生成，是一种智能方面更接近人类的自主性实体系统。自主系统可以完成一些相对独立、自主的任务，甚至可以通过调整自我状态，应对环境和特殊情况的变化，进而保证自身规划任务的完成。在多主体系统研究中，主要是从物理和逻辑思维方面对主体进行智能行为的分析研究。4 人工智能发展中面临的问题4.1 识别功能的困惑计算机识别技术研究在近些年取得了大量成果，其产品的实际应用范围较广，但不可否认的是，计算机识别的模式是基于一定的算法和程序设定的，其识别机制完全不同于人类的感官识别，因此，在计算机进行识别，尤其是图形识别时，对各种印刷体、文字、指纹等清晰图形可以快速识别，但对于相似度较高的物体，计算机识别能力相对较弱，识别失败的情况较为普遍。语音识别主要研究各种语音信号的分类。语音识别技术近年来发展很快，但是缺点是识别极易受到干扰，发音不标准的语音较易引发识别错误。4.2 GPS功能的局限性GPS是企图实现一种不依赖于领域知识求解人工智能问题的通用方法，但是问题内部的表达形式和领域知识是分不开的，用谓词逻辑进行定理归结或者人工智能通用方法GPS，都可以从分析表达能力上找出其局限性，这样就减少了人工智能的应用范围[1]。 　　5 人工智能的未来应用展望人工智能与人生活最息息相关的应用范围就是融入人们的衣食住行和教育等方面，这也是人工智能未来最普遍的应用方向。5.1 无人驾驶的汽车奔驰、丰田等很多大型汽车企业都在研究�o人驾驶的汽车，像007电影中的那种拥有自主辨别路况、自动驾驶等功能的汽车也许很快就会成为现实。自动驾驶的汽车要搭载的技术并不只人工智能一种，它还需要将自动控制和视觉计算等新型技术集成应用，改变现有汽车的体系结构，赋予其自动识别、分析和控制的能力。因此，自动驾驶汽车需要实现三方面的技术突破：其一，实现利用摄像设备、雷达和激光测距机来获得路况信息；其二，实现利用地图进行自动的车辆导航；其三，根据已有信息数据对车辆的速度和方向进行控制。未来的自动驾驶汽车还可以通过车辆之间的信息互通和互相感应，来协调车速和方向，避免车辆碰撞，实现自动驾驶车辆的安全行进。5.2 智能化的课堂当前已经有一些智能化的教学软件，教师们可以在这些软件上把教学课件传送给学生，并进行授课答题，学生还可以与教师弹幕互动，使课堂变得妙趣横生，方便了教师的授课活动。对于学生而言，能够在期末十分便捷地回顾上课的错题，甚至能够在几年后翻阅学习过的课件；对于教师而言，能够精细地知道学生对知识的掌握程度，甚至能够发现最积极和最懈怠的学生。未来的智能课堂将更具有时间延展性，学生不仅可以在课堂学习知识，还可以利用智能电子设备进行课前预习和课后复习，从而使学生可以在更加趣味性的氛围中进行自主学习安排。5.3 自动化的厨房今后的厨房将会更加智能化，当你做饭时，设定好你想要的菜谱，准备好所需的食材，烹调设备即可将饭菜制作得恰到好处。它会根据你食材的新鲜程度，为你推荐最适合的菜谱，并计算出其营养参考标准，并为你推荐其他食物，使膳食营养均衡。当你家中某样食材不足时，物流公司便会将时下最新鲜的这一食材送至你家中[2]。6 结语人工智能这一概念是在1956年提出的，在当时，人工智能还只是人们头脑中的一种幻想，而在60年后的今天，人工智能的梦想已经逐渐照进现实，它甚至渗透进了工业、医学、服务等多个领域，可以说人工智能正在改变着我们生活的世界。但对于人工智能这个人类创造出来的技术，人们也存在一定的担忧，人工智能将向何方发展？人工智能发展到极致会不会脱离人类的控制？人工智能会不会超越人类的智慧？在诸多问题围绕下，人工智能技术依然在迅猛发展，它的未来如何，让我们拭目以待。参考文献：【1】王宇楼.人工智能的现状及今后的发展趋势展望[J].科技展望，2016（22）：299.【2】吕泽宇.人工智能的历史、现状与未来[J].信息与电脑（理论版），2016（13）：166-167."}
{"content2":"AI： Artificial Intelligence（人工智能），它是研究、开发用于模拟和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学，上个世纪50年代一次学术讨论会议上，下图中几位著名的科学家从不同学科的角度探讨人类各种学习和其他职能特征，并研究如何在原理上进行精确的描述，探讨用机器模拟人类智能等问题，并首次提出了“人工智能”的术语。从此，人工智能这门新兴的学科诞生了，那场学术讨论会议，就是计算机科学史上著名的达特茅斯会议（Dartmouth Conferences），关于图灵测试是否是人工智能的评定标准，参见 （http://iscientists.baijia.baidu.com/article/466034）目录：应用场景2016AI榜单AI阶段与特级技术现状概念AR和MR该如何区分应用场景2016年AI榜单 (http://tech.qq.com/a/20161218/003272.htm?pgv_ref=aio2015_hao123news)2016.03谷歌DeepMind的深度学习领域开发的AlphaGo以4:1的比分击败了世界围棋冠军李世石,  (谷歌DeepMind部门招聘人工智能专家 预防AI毁灭人类 http://tech.qq.com/a/20161126/002425.htm?t=1480146883601)特斯拉Autopilot将血栓病人送到医院Swarm人工智能预测肯塔基赛马结果微软人工智能的语言理解能力超过人类人工智能预测美国大选人工智能诊断癌症AI阶段与等级技术现状微软提供的认知服务API如下图百度继天算、天像、天工后发布的天智平台，包含服务如下：概念VR：VitualReality，虚拟现实，是利用计算设备模拟产生一个三维的虚拟世界，提供用户关于视觉、听觉等感官的模拟，有十足的「沉浸感」与「临场感」。俗话说就是，你看到的所有东西都是计算机生成的，都是假的。典型的输出设备就是Oculus Rift、HTC Vive等等AR:AugmentedReality，增强现实，字面解释就是，「现实」就在这里，但是它被增强了，被谁增强了？被虚拟信息。两个典型的AR系统是车载系统和智能手机系统，被讨论最多的AR设备是Google Glass，应用如下图: 车载系统在其挡风玻璃上投射虚拟图像、比较火的FaceUAV: 增强虚拟是将真实环境中的特性加在虚拟环境中，手机中的赛车游戏与射击游戏，通过重力感应来调整方向和方位，那么就是通过重力传感器、陀螺仪等设备将真实世界中的「重力」「磁力」等特性加到了虚拟世界中MR：Mixed Reality，混合现实，又称Hybrid Reality，将真实世界和虚拟世界混合在一起，来产生新的可视化环境，环境中同时包含了物理实体与虚拟信息，并且必须是「实时的」。MR的两大代表设备就是Hololens与Magic LeapAR和MR该如何区分虚拟物体的相对位置，是否随设备的移动而移动。如果是，就是AR设备；如果不是，就是MR设备戴上Google Glass，它在你的左前方投射出一个「天气面板」，不管你怎样在屋子中走动，或者转动头部，天气面板一直都在你的左前方，它与你（或者Google Glass）的相对位置是不变的，你走到哪，就把它带到哪Hololens是怎样的呢，它也会在屋子墙壁上投射出一个天气面板，但是不同之处在于，不管你怎样在屋子中走动，或者转动头部，天气面板始终都在那面墙上，它不会由于你的移动而移动在理想状态下（数字光场没有信息损失），虚拟物体与真实物体是否能被区分AR设备创造的虚拟物体，是可以明显看出是虚拟的，比如FaceU在用户脸上打出的虚拟物品、Google Glass投射出的随你而动的虚拟信息；而Magic Leap，用户看到的虚拟物体和真实物体几乎是无法区分的总结：「AR设备」与「MR设备」的界限不是绝对的（甚至说这种界限是企业YY出来的）"}
{"content2":"人工智能能超越人类大脑吗是否能超越，我觉得可以从人脑的生理功能推测出答案。人脑有记忆和计算功能，除此之外并没有发现什么可证实的特别的地方。目前人类可以为人工智能提供足够的记忆空间和计算能力，要达到现在人脑的功能我们缺的是合理完整的理论而不是硬件条件。而目前深度学习在各领域都取得了很好的效果，这也说明了只要有合适的理论模型，人工智能可以做到十分出色。目前的人工智能还处于初级阶段，属于弱人工智能，将来一定能得到理论上的突破，提高归纳、推理能力，最终用记忆与计算衍生出情感与意识，这时候将迎来强人工智能时代。人工智能除了可以拥有比人脑更强大的物理硬件外，还有相对于人脑生理特性而形成的优势：一、能承受高强度长时间学习。只要有电等维持其运算的能量就可以全天候24小时不间断学习，它不怕累，而人脑需要休息。二、可以轻易快速超距离实现知识共享。人造科学有个好处就是很容易建立其标准框架，实现标准的交互接口，机器之间互相学习只需要简单的数据交互就能快速完成。而人脑之间的相互学习要通过语音文字等工具去理解，学习的效率和能力差得很远。三、能承受恶劣的环境。人脑的交流和学习很容易受外界和身体自身的影响，从噪音、气温、光线等外在环境，到饥饿、心情等自身的状况都会对人脑学习人脑之间的交流造成严重影响。而人工智能没有这么多麻烦，所能承受的环境要恶劣得多。总之霍金、比尔盖茨等人对高级人工智能的担忧不无道理，人工智能超越人类大脑是完全可能的。人工智能近期最有可能实现的是什么人工智能通过深度学习在自然语言处理NLP、图像识别取得了很大突破，还有卷积神经在计算机视觉的助力，让人工智能近期很可能让高精度实时翻译、可自由行动的家庭助手（想想google home与波士顿动力的机器狗结合）成为现实。人工智能近期最迫切需要实现的是什么人工智能也需要学习，而学习的来源来自于各种数据。文字作为人类知识智慧的最有力传承者，蕴含着最丰富的被提炼而成的数据，是最珍贵的知识宝藏。只有掌握学习人类社会最高形态的知识的方法，人工智能才能理解我们的世界，迈向高级人工智能。因此人工智能最迫切需要实现的是通过文字去自主学习的方法。就说到这吧，如果你对人工智能技术感兴趣，可以来这Q 群一起交流：170221295"}
{"content2":"原文：http://blog.csdn.NET/chenyusiyuan/article/details/5961769双目测距的基本原理如上图所示，双目测距主要是利用了目标点在左右两幅视图上成像的横向坐标直接存在的差异（即视差）与目标点到成像平面的距离Z存在着反比例的关系：Z=fT/d。假设目标点在左视图中的坐标为（x,y），在左右视图上形成的视差为d，目标点在以左摄像头光心为原点的世界坐标系中的坐标为（X,Y,Z），则存在上图所示的变换矩阵Q，使得 Q*[x y d 1]’ = [X Y Z W]’。“@scyscyao ：为了精确地求得某个点在三维空间里的距离Z，我们需要获得的参数有焦距f、视差d、摄像头中心距Tx。如果还需要获得X坐标和Y坐标的话，那么还需要额外知道左右像平面的坐标系与立体坐标系中原点的偏移cx和cy。其中f, Tx, cx和cy可以通过立体标定获得初始值，并通过立体校准优化，使得两个摄像头在数学上完全平行放置，并且左右摄像头的cx, cy和f相同(也就是实现中左右视图完全平行对准的理想形式)。而立体匹配所做的工作，就是在之前的基础上，求取最后一个变量：视差d(这个d一般需要达到亚像素精度)。从而最终完成求一个点三维坐标所需要的准备工作。在清楚了上述原理之后，我们也就知道了，所有的这几步：标定、校准和匹配，都是围绕着如何更精确地获得f, d, Tx, cx 和cy 而设计的 。 ”一、图像的获取1． 如何打开两个或多个摄像头？可以通过OpenCV的capture类函数或者结合DirectShow来实现双摄像头的捕获，具体可见我的读书笔记《OpenCV学习笔记（6）基于 VC+OpenCV+DirectShow 的多个摄像头同步工作 》。文中曾提及不能用cvCreateCameraCapture 同时读取两个摄像头，不过后来一位研友来信讨论说只要把摄像头指针的创建代码按照摄像头序号降序执行，就可以顺利打开多个摄像头，例如：[c-sharp] view plain copy print?CvCapture* capture2 = cvCreateCameraCapture( 1 );CvCapture* capture1 = cvCreateCameraCapture( 0 );采用DirectShow的方式读入时：[c-sharp] view plain copy print?camera2.OpenCamera(1, false, 640,480);camera1.OpenCamera(0, false, 640,480);这样就可以同时采集两个摄像头。我也验证过这种方法确实有效，而且还解决了我遇到的cvSetCaptureProperty调整帧画面大小速度过慢的问题。当摄像头的打开或创建代码按照摄像头序号从0开始以升序编写执行时，使用cvSetCaptureProperty就会出现第一个摄像头（序号为0）的显示窗口为灰色（即无图像）、且程序运行速度缓慢的现象。而改为降序编写执行后，则能正常、实时地显示各摄像头的画面。具体原因有待分析讨论。2． 如何实现多个摄像头帧画面的同步抓取？在单摄像头情况下用 cvQueryFrame 即可抓取一帧画面，实际上这个函数是由两个routine组成的：cvGrabFrame和cvRetrieveFrame（详见Learning OpenCV第103页）。cvGrabFrame将摄像头帧画面即时复制到内部缓存中，然后通过cvRetrieveFrame把我们预定义的一个IplImage型空指针指向缓存内的帧数据。注意这时我们并没有真正把帧数据取出来，它还保存在OpenCV的内部缓存中，下一次读取操作就会被覆盖掉。所以一般我们要另外定义一个IplImage来复制所抓取的帧数据，然后对这个新IplImage进行操作。由上面的解释也可以看出，cvGrabFrame的作用就是尽可能快的将摄像头画面数据复制到计算机缓存，这个功能就方便我们实现对多个摄像头的同步抓取，即首先用cvGrabFrame依次抓取各个CvCapture*，然后再用cvRetrieveFrame把帧数据取出来。例如：[c-sharp] view plain copy print?cvGrabFrame( lfCam );cvGrabFrame( riCam );frame1 = cvRetrieveFrame( lfCam );frame2 = cvRetrieveFrame( riCam );if( !frame1|| !frame2) break;cvCopyImage(frame1, image1);cvCopyImage(frame2, image2);二、摄像头定标摄像头定标一般都需要一个放在摄像头前的特制的标定参照物（棋盘纸），摄像头获取该物体的图像，并由此计算摄像头的内外参数。标定参照物上的每一个特征点相对于世界坐标系的位置在制作时应精确测定，世界坐标系可选为参照物的物体坐标系。在得到这些已知点在图像上的投影位置后，可计算出摄像头的内外参数。如上图所示，摄像头由于光学透镜的特性使得成像存在着径向畸变，可由三个参数k1,k2,k3确定；由于装配方面的误差，传感器与光学镜头之间并非完全平行，因此成像存在切向畸变，可由两个参数p1,p2确定。单个摄像头的定标主要是计算出摄像头的内参（焦距f和成像原点cx,cy、五个畸变参数（一般只需要计算出k1,k2,p1,p2，对于鱼眼镜头等径向畸变特别大的才需要计算k3））以及外参（标定物的世界坐标）。 OpenCV 中使用的求解焦距和成像原点的算法是基于张正友的方法（pdf ），而求解畸变参数是基于 Brown 的方法（pdf ）。1． 图像坐标系、摄像头坐标系和世界坐标系的关系摄像头成像几何关系，其中Oc 点称为摄像头（透镜）的光心，Xc 轴和Yc 轴与图像的x轴和Y轴平行，Zc 轴为摄像头的光轴，它与图像平面垂直。光轴与图像平面的交点O1 ，即为图像坐标系的原点。由点Oc 与Xc 、Yc 、Zc 轴组成的坐标系称为摄像头坐标系，Oc O1的距离为摄像头焦距，用f表示。图像坐标系是一个二维平面，又称为像平面，“@scyscyao ：实际上就是摄像头的CCD传感器的表面。每个CCD传感器都有一定的尺寸，也有一定的分辨率，这个就确定了毫米与像素点之间的转换关系。举个例子，CCD的尺寸是8mm X 6mm，帧画面的分辨率设置为640X480，那么毫米与像素点之间的转换关系就是80pixel/mm。”设CCD传感器每个像素点的物理大小为dx*dy，相应地，就有 dx=dy=1/80。2． 进行摄像头定标时，棋盘方格的实际大小 square_size （默认为 1.0f ）的设置对定标参数是否有影响？“@scyscyao ：当然有。在标定时，需要指定一个棋盘方格的长度，这个长度(一般以毫米为单位，如果需要更精确可以设为0.1毫米量级)与实际长度相同，标 定得出的结果才能用于实际距离测量。一般如果尺寸设定准确的话，通过立体标定得出的Translation向量的第一个分量Tx的绝对值就是左右摄像头的中心距。一般可以用这个来验证立体标定的准确度。比如我设定的棋盘格大小为270 (27mm)，最终得出的Tx大小就是602.8 (60.28mm)，相当精确。”3． 定标所得的摄像头内参数，即焦距和原点坐标，其数值单位都是一致的吗？怎么把焦距数值换算为实际的物理量？“@wobject ：是的，都是以像素为单位。假设像素点的大小为k x l，单位为mm，则fx = f / k， fy = f / (l * sinA)， A一般假设为 90°，是指摄像头坐标系的偏斜度（就是镜头坐标和CCD是否垂直）。摄像头矩阵（内参）的目的是把图像的点从图像坐标转换成实际物理的三维坐标。因此其中的fx, fy, cx, cy 都是使用类似上面的纲量。同样，Q 中的变量 f，cx, cy 也应该是一样的。”4． 棋盘图像数目应该取多少对摄像头定标比较适宜？OpenCV中文论坛上piao的帖子《在OpenCV中用cvCalibrateCamera2进行相机标定(附程序) 》中指出影响摄像头定标结果的准确性和稳定性的因素主要有三个：（1） 标定板所在平面与成像平面(image plane)之间的夹角；（2） 标定时拍摄的图片数目（棋盘图像数目）；（3） 图像上角点提取的不准确。感觉OpenCV1.2以后对图像角点的提取准确度是比较高的，cvFindChessboardCorners 和 cvFindCornerSubPix结合可以获得很好的角点检测效果（hqhuang1在《[HQ]角点检测(Corner Detection) cvFindCornerSubPix 使用范例 》中给出了相关的应用范例）。因此，影响定标结果较大的就是标定板与镜头的夹角和棋盘图像数目，在实际定标过程中，我感觉棋盘图像数目应该大于20张，每成功检测一次完整的棋盘角点就要变换一下标定板的姿态（包括角度、距离）。5． 单目定标函数cvCalibrateCamera2采用怎样的 flags 比较合适？由于一般镜头只需要计算k1,k2,p1,p2四个参数，所以我们首先要设置 CV_CALIB_FIX_K3；其次，如果所用的摄像头不是高端的、切向畸变系数非常少的，则不要设置 CV_CALIB_ZERO_TANGENT_DIST，否则单目校正误差会很大；如果事先知道摄像头内参的大概数值，并且cvCalibrateCamera2函数的第五个参数intrinsic_matrix非空，则也可设置 CV_CALIB_USE_INTRINSIC_GUESS ，以输入的intrinsic_matrix为初始估计值来加快内参的计算；其它的 flag 一般都不需要设置，对单目定标的影响不大。P.S. 使用OpenCV进行摄像机定标虽然方便，但是定标结果往往不够准确和稳定，最好是使用 Matlab标定工具箱 来进行定标，再将定标结果取回来用于立体匹配和视差计算。工具箱的使用官方主页 有图文并茂的详细说明，此外，有两篇博文也进行了不错的总结，推荐阅读：（1）分享一些OpenCV实现立体视觉的经验（2）Matlab标定工具箱使用的一些注意事项."}
{"content2":"手绘几何图形具有的特点是，随意、简单、可以迅速的表达设计师对产品的设想，有利于捕捉生活中得到的一些灵感。绘图工具中，需要在大量菜单，选项中选择所绘的图形，操作较为繁琐，在对手绘几何图形识别的过程中，通过计算机图形学，模式识别，人工智能，完成识别，重绘的过程，再完善人机交互的体验。一.手绘几何图形介绍手绘图识别分为联机和脱机两种脱机：通过特定的采集设备如摄像机、扫描仪将图形以图像的形式，采集并输入到计算机。联机：通过连接输入设备，如手写板，触摸屏，鼠标将笔的运动轨迹识别发送到计算机。识别方法不同：脱机：通常脱机识别的数据是用f(xi,yi)来描述，其中(xi,yi)为坐标(i,j)处的灰度值，所以在识别过程中，我们要对数据进行降噪，二值化等预处理；联机：联机识别中，获得的是一组二维数据，用来存放笔迹点的坐标，还可以存放笔的移动速度，加速度，笔压等信息，这些信息可以整合成多维的点的序列，比脱机识别更有优势。手绘几何图形识别和手写文字识别比较同属于模式识别和计算机视觉研究领域的分支。文字识别：将输入数据识别转化成词句，并显示出来。有固定的字符集合。对每个特定字符进行特征分析，在对整个字符集的整体特征进行分析，文字识别笔画形态相对简单，书写需要有严格的规范，便于建立鲁棒(Robust)的分类器。手绘几何图形识别：没有特定的字符集，笔画随意，特征不明显，但是图形具有很多几何特征，方向、尺寸、连接、交叉，这些几何特征可以在识别图形中进行判定。二.手绘图识别的研究方法手绘图识别主要的几类研究方法：模糊类方法：利用模糊逻辑，模糊知识，从笔迹的位置，方向，速度，加速度，来识别图形。几何方法：把图形作为整体识别，进行平滑处理，提取圆弧段，识别结点，分解出直线段，根据相邻3点的夹角角度作为圆弧和直线段的提取特征，找出实验阈值，进行分类。神经网类方法：通过提取图素几何形状的内角特征，用二进制突触的权重算法BSW(含一个隐层的前馈网)进行识别的方法。该方法以整体方式识别三角形、椭圆和矩形三种图素，但仅适用于绘图包和掌上电脑的自动草绘输入。图元分类：图元是组成图形的最小单元，其无法再被分割成其他几何线条。我们把基本的图元分为：直线，圆，圆弧，椭圆，椭圆弧。预处理：数据采集之后需要经过预处理操作，预处理是消除因输入设备的差异，或者是个人输入习惯  的不同产生的一些噪声，如图中，因为手的抖动，造成有很多毛刺，或者绘图着画的慢，造成有些点重复取样。这些噪声会影响后面识别的准确性。降低噪声的方法通常采用线性平滑算子或非线性平滑算子进行处理。可以通过每隔两点计算其中心点，然后连接这些中心点作为后续处理笔划的方法。也可以通过限定草图的点间距的方法过滤掉多余的草绘点。但方法可能会丢失草绘时的一些动态特征，如速度、加速度等。三.笔画分割笔画分割的目的是将用户画出的复杂图形分割成基本的图元。分割的好坏会直接影响到后面识别的部分。曲率：曲线上各点沿曲线方向的变化情况，借助曲率可以刻画曲线的几何特征。曲率几何特征连续零曲率直线段连续非零曲率弧线段局部最大曲率绝对值角点局部最大曲率正值凸角点局部最大曲率负值凹角点曲率过零点特征点1.曲率的计算公式：k=△φ/△S其中：△φ表示切线倾角变化值，△S表示曲线的弧长。2.绘制速度用户绘图过程中，在拐角处绘制速度会变慢，所以绘图时的绘制速度也是我们进行特征点判断的重要依据。由于在绘图的过程中是等时采样，任意点的绘制速度，可以用相邻两点间的距离代替。根据曲率及绘制速度来检测笔划中的特征点进行分割。介绍两种分割算法：均值滤波和尺度空间滤波法。均值滤波发：数据的极值可能是数据的噪声产生的，所以我没要消除噪声所产生的极值，需要设置合适的阈值。如果是固定阈值，可能某种情况很好，某种情况又非常不好，均值滤波起到了一个很好的解决方案。先计算数据的平均值，然后乘以一个固定因子，获得一个合适的阈值。可以把数据分成大于和小于阈值的两个部分。均值滤波需要运用在曲率和速度上，找出合适的特征点，具体步骤如下：1）遍历所有采样点，计算出采样点的切线与x轴夹角，计算出曲率和平均曲率Ave_Curvature；2）特征点在全部点钟占的比例较小，为了提高分割的准确性，所以对Ave_Curvature进行放大，作为曲率极值的判断阈值。3）遍历所有曲率数据，比较找出大于阈值的电，加入到曲率特征点链表中。4）同样遍历采样点，计算出速度数据和Ave_Speed。5）对速度进行缩小，作为速度极值判断的阈值。6）找出比阈值小的点，加入到速度特征点链表7）比较2个链表，将重合或相近的电作为特征点输出。尺度空间法：尺度空间思想最早由lijima于1962年提出，但当时并未引起计算机视觉领域研究者们的足够注意，直到上世纪八十年代，Witkin等人的奠基性工作使得尺度空间方法逐渐得到关注和发展。此后，随着非线性扩散方程、变分法和数学形态学等方法在计算机视觉领域中的广泛应用，尺度空间方法进入了快速发展阶段。尺度空间方法的基本思想是：在视觉信息(图像信息)处理模型中引入一个被视为尺度的参数，通过连续变化尺度参数获得不同尺度下的视觉处理信息，然后综合这些信息以深入地挖掘图像的本质特征。尺度空间方法将传统的单尺度视觉信息处理技术纳入尺度不断变化的动态分析框架中，因此更容易获得图像的本质特征。"}
{"content2":"https://mp.weixin.qq.com/s/kWw0xce4kdCx62AflY6AzQ1.  抢跑的nlpnlp发展的历史非常早，因为人从计算机发明开始，就有对语言处理的需求。各种字符串算法都贯穿于计算机的发展历史中。伟大的乔姆斯基提出了生成文法，人类拥有的处理语言的最基本框架，自动机(正则表达式)，随机上下文无关分析树，字符串匹配算法KMP，动态规划。nlp任务里如文本分类，成熟的非常早，如垃圾邮件分类等，用朴素贝叶斯就能有不错的效果。20年前通过纯统计和规则都可以做机器翻译了。相比，在cv领域，那时候mnist分类还没搞好呢。90年代，信息检索的发展提出BM25等一系列文本匹配算法，Google等搜索引擎的发展将nlp推向了高峰。相比CV领域暗淡的一些。2.  特征抽取困难的cvcv的前身就有一个领域叫图像处理，研究图片的压缩、滤波、边缘提取，天天摆弄着一个叫lenna的美女。早期的计算机视觉领域受困于特征提取的困难，无论是HOG还是各种手工特征提取，都没办法取得非常好的效果。大规模商业化应用比较困难。而同期nlp里手工特征➕svm已经搞的风生水起了。3.  深度学习的崛起- 自动特征提取近些年，非常火爆的深度学习模型简单可以概括为：深度学习 = 特征提取器➕分类器一下子解决cv难于手工提取特征的难题，所以给cv带来了爆发性的进展。深度学习的思路就是让模型自动从数据中学习特征提取，从而生成了很多人工很难提取的特征：4.  nlp的知识困境不是说nlp在这波深度学习浪潮下没有进展，而是说突破并没有cv那么巨大。很多文本分类任务，你用一个巨复杂的双向LTSM的效果，不见得比好好做手工feature + svm好多少，而svm速度快、小巧、不需要大量数据、不需要gpu，很多场景真不见得深度学习的模型就比svm、gbdt等传统模型就好用。而nlp更大的难题在于知识困境。不同于cv的感知智能，nlp是认知智能，认知就必然涉及到知识的问题，而知识却又是最离散最难于表示的。"}
{"content2":"http://cvrs.whu.edu.cn/武汉大学计算机视觉与遥感实验室（WHU-CVRS Lab）由学校高层次引进人才姚剑教授于2012年4月创建成立，目前成员包括1名博士后，20余名博士和硕士研究生，10多名本科生。自从成立至今，CVRS实验室参与和主持了一系列国家级和省部级科研项目，包括973计划、国家自然科学基金面上项目与重点项目、湖北省科技支撑项目、江苏省科技支撑项目、湖北省自然科学基金等，也与行业相关企业（腾讯、华为、阿里巴巴、中科院深圳先进院、立得空间、吉威时代、江苏智途、湖南桥康、顺德天行健等）开展了一系列横向科技合作和学生培养合作，目前主要研究方向包括：计算机视觉、机器学习、深度学习、模式识别、人工智能、数据挖掘、大数据分析；图像处理、医学影像处理、视频分析、智能视频监控；机器人技术、机器视觉智能检测、机器人视觉导航与定位、SLAM、室内导航与定位；三维场景建模与渲染、计算机图形学、虚拟现实；LiDAR点云数据处理与分析、三维变形监测与分析；地面移动遥感、高分辨率遥感目标提取与场景解译；高性能计算、GPU计算、多核CPU计算、GPU-CPU混合计算、云计算；软硬件系统集成、Web开发、APP开发等。申请博士后欢迎对我们的研究方向或科研项目有浓厚兴趣的即将毕业的博士生或已毕业的博士来武汉大学遥感信息工程学院测绘科学与技术学博士后流动站进一步从事博士后研究，除了国家和学校相关政策之外，我们另外酌情提供一定的科研经费和生活补助，有意者敬请电话或EMail联系 姚剑 教授。应聘材料的投递应聘者请将单个PDF文档包含以下内容（个人简历、研究工作简介、代表性论文以及三位推荐人的联系方式）发给姚剑教授（邮箱：jian.yao@whu.edu.cn），邮件主题请注明“应聘博士后”。待遇与福利(1) 博士后保证工资为每年30-50万元人民币，其他福利待遇参照武汉大学博士后标准，如博士后公寓、子女入学和五险一金等。实验室将提供充足科研经费并提供出国参加国际会议和交流访问机会；(2) 提供优越的科研条件和稳定的支持，鼓励博士后按照国家和地方相关政策申报博士后基金等各项基金。"}
{"content2":"1. 课程介绍2. 机器学习 （Machine Learning, ML)2.1 概念：多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。2.2 学科定位：人工智能(Artificial Intelligence, AI）的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演绎。2.3 定义：探究和开发一系列算法来如何使计算机不需要通过外部明显的指示，而可以自己通过数据来学习，建模，并且利用建好的模型和新的输入来进行预测的学科。Arthur Samuel (1959): 一门不需要通过外部程序指示而让计算机有能力自我学习的学科Langley（1996) ： “机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在经验学习中改善具体算法的性能”Tom Michell (1997):  “机器学习是对能通过经验自动改进的计算机算法的研究”2.4： 学习：针对经验E (experience) 和一系列的任务 T (tasks) 和一定表现的衡量 P，如果随之经验E的积累，针对定义好的任务T可以提高表现P，就说计算机具有学习能力例子： 下棋，语音识别，自动驾驶汽车等3. 机器学习的应用：语音识别自动驾驶语言翻译计算机视觉推荐系统无人机识别垃圾邮件4. Demo：人脸识别无人驾驶汽车电商推荐系统5. 置业市场需求：LinkedIn所有职业技能需求量第一：机器学习，数据挖掘和统计分析人才"}
{"content2":"这段时间一直在赶一个新项目，因此文章更新也就被搁置了。期间学习了一些关于人工智能的课程，因此从以下几点做了总结：1.1 物理预测的胜利与失效（1）数学---来自于生产试验土地测量学---古埃及 是物理学，数学，智能，大数据的学科基础 基于经验的数据(2) 数学模型----真正的数据A. 本轮与均轮旋转 ----托勒密模型 比较复杂B. 14~15世纪突飞猛进的发展 牛顿的微积分 以物理学的发展为代表基于天文学的发展牛顿三定律---物体是有惯性的受力相互作用---作用力与反作用力F=Gm1m2/r2C. 哈密顿方程=动能+势能（H+V） 类似于牛顿方程被定义于广义坐标中，角速度和速度的关系 单摆运动特征1.2 预测失效的原因随机性（不确定性） 如：微观世界里，牛顿量子力学混沌性（气象预测发现的） 当初始条件发生变化时，后面的变化很大反身性（股市） 当观测者想要预测时，行为会影响预测结果网络效应（大量简单物体发生反应时出现耦合，复杂化）历史事件依赖 此刻的状态取决于过去状态的影响1. 3 复杂系统引论（即：个体到群体的映射）由简单个体组成，通过相互作用产生的属性；个体层面不存在；个体是被制约的，宏观显现出来的（例如：铁磁铁 ---宏观物体，磁性如何产生？）外在环境会影响复杂系统信息（如生物进化的过程 当外界能量稀缺，外界能量进入生物内部，致使生物组织重构，形成更大的生物组织，调节生物变化适应环境）网络结构特性（如商业交易者，不同股票之间的联系，大脑，社交网络（存在点性结构）等）---存在连接度，兼顾稳定和效率1.4 生活实例与本章总结例如： 雪花 股市 鱼群、鸟群迁徙经济学现象---产业结构森林2. 1 大数据预测因为噪音失效例如：公安行侦 超市 谷歌等预测疾病，天气等系统2.2 大数据与机器学习算法----拯救预测失效的有效办法 如药物萃取的过程统计学习概念 如：行星的运动（物理模型）3.1 规则阶段故事：人工智能像个傻瓜人工智能发展史 包括符号，控制，连接学派1. 符号主义---人给机器规则，机器帮助人来决策人工智能之父阿兰图灵（同性恋患者 吃毒苹果自杀）理论框架---图灵机 （例如：毛毛虫---黑白格子纸上行动； 增加状态 饥饿或者吃饱 格子颜色根据状态变化 引申出可编程概念）2. 图灵停机的问题 量子计算机的概念3.2 机器学习阶段发展至连接主义阶段---机器学习规则 直接模拟人的思维硬件（大脑）统计学经济阶段 数据不能解决所有的问题统计算法阶段 各种统计算法模型（PCA 逻辑算法 决策树 贝叶斯网络）共同点：使用数据驱动，改变自身结构模拟人的思维，改变流程例如：声音翻译成语言 通过人脸说出人名（人脸识别）如何模拟人的大脑例如：神经元细胞模拟 输入（树突权重）---输出（为神经做决定） 感知机模型3.3 连接主义阶段发展至深度学习阶段神经网络之后进入低谷期，因为难以控制；2007年~2008年，出现了复杂的网络GPU用来玩游戏 训练神经网络的驯兽师公司例如：一张圈内包圈的图片用一刀切办法切开 这种算法叫做特征工程通过几次识别，算法提取；最终识别人脸。3.4 三个阶段总结与分析软件 硬件 神经网络---用神经元的数量来决定，神经元越复杂，越聪明应用：智商很低符号主义（规则）阶段经济学阶段深度学习----冰山一角 深度学习不等于人工智能，但是是划时代的3.5 人工智能的应用1APPLIcation例如：今日头条公司---文本分类 针对每个词做统计电影设计视觉类---物体的识别听觉类---声音信号 翻译成字母 应用于：智能家居，机器人对话翻译---谷歌翻译 深度学习AI设计师---可以不去商城试衣服 有很大商机人工智能艺术家---设计建筑AI商业模式----BI（商业智能）金融---人工智能使用最广泛的领域 如：炒股；机器交易单 ；青椒科技---AI金融农业---AI现代化机器 卫星图像---对地面扫描，确定土地状况3.6 人工智能的应用2围棋 疾病 无人车 阿法狗等开启人类的未来负责是可以对抗复杂的"}
{"content2":"1：OpenCv(计算机视觉必学的库，个人觉得其作用相当强悍）http://opencv.willowgarage.com/wiki/2：CVpaper 主页上推荐的开源视觉算法库，最全的了，也很新,强烈推荐大家去看看http://www.cvpapers.com/rr.html3：cmu的图像处理和计算机视觉软件库，很全，但有点老了，但都很经典，资源很丰富http://www.cs.cmu.edu/~cil/v-source.html4：机器学习Machine Learning 课程http://wenku.baidu.com/course/study/53b9fd0a79563c1ec5da7107http://v.163.com/special/opencourse/machinelearning.html5：国外人工智能界牛人主页http://caiqi1123.blog.163.com/blog/static/5736178120080213836366/6：opencv中文论坛（里面有非常多样例能够供刚開始学习的人学）http://www.opencv.org.cn/   老站点http://wiki.opencv.org.cn/index.php/Template:Install7：工业视觉应用网络DavidLowehttp://www.cs.ubc.ca/~lowe/vision.html8：Computer Vision: Algorithms and Applications（非常好的一本书）http://szeliski.org/Book/9:：C++学习站点http://www.cplusplus.com/10：总结了一些大牛的博客（非常值得学习）http://blog.csdn.net/xiaowei_cqu/article/details/803419511：MFC刚開始学习的人的学习站点（挺不错的）http://www.jizhuomi.com/software/257.html12：mattinghttp://mpac.ee.ntu.edu.tw/~sutony/vfx_matting/index.html13：OCR大牛http://yann.lecun.com/exdb/lenet/index.html14：开源计算机视觉社区https://www.google.com.hk/?gws_rd=ssl#newwindow=1&q=%E6%80%8E%E6%A0%B7%E5%BB%BA%E7%AB%8B%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%BD%AF%E4%BB%B6%E5%BA%93&safe=strict參考：http://www.cnblogs.com/kshenf/archive/2012/06/15/2550482.html后面会陆续更新"}
{"content2":"opencv-计算机视觉库opencv的安装在github上下载opencvrelease版本仓库安装依赖项sudo apt-get install build-essential sudo apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev sudo apt-get install python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev // option使用CMake编译安装cd ~/opencv mkdir build cd build cmake -D CMAKE_BUILD_TYPE = Release -D CMAKE_INSTALL_PREFIX = /usr/local .. make -j4 sudo make install注： 构建类型： CMAKE_BUILD_TYPE=Release\\Debug注：安装路径： CMAKE_INSTALL_PREFIX=/usr/local加载、显示和保存图片加载图像： cv::imread()创建图像窗口： cv::namedWindow()显示图像：cv::imshow()保存图像：cv::imwrite()#include <opencv2/opencv.hpp> //总的头文件 // #include <opencv2/core.hpp> // #include <opencv2/imgcodecs.hpp> // #include <opencv2/highgui.hpp> #include <iostream> #include <string> int main(int argc, char** argv) { std::string image_name = \"/home/liuzhiyang/test/color_image.png\"; cv::Mat color_image; color_image = cv::imread(image_name, cv::IMREAD_COLOR); if (color_image.empty()) { std::cout << \"fail to read image!\" << std::endl; return 0; } cv::namedWindow(\"color_image\"); cv::namedWindow(\"gray_image\"); cv::Mat gray_image; cv::cvtColor(color_image, gray_image, cv::COLOR_BGR2GRAY); cv::imshow(\"color_image\", color_image); cv::imshow(\"gray_image\", gray_image); cv::waitKey(0); cv::imwrite(\"/home/liuzhiyang/test/gray_image.png\", gray_image); return 0; }注： 读取灰色图像cv::imread(filename, IMREAD_GRAYSCALE);"}
{"content2":"3 月 18 日，由李飞飞担任所长之一的「以人为本人工智能研究所」（HAI）自启动以来不短的时间后，终于完成了正式成立的高光时刻。而正式上线的官网日前也更新了两条博文，一篇是详尽介绍 HAI 的文章；另一篇则是今天要给大家介绍的斯坦福 HAI 版人工智能简史图，图中涵盖的信息包括：人工智能大事记、全球人工智能初创公司布局、人工智能的普及度进展、人工智能的研究和教育进展以及斯坦福的人工智能突破性成果和人工智能课程。而之所以给此图加上「斯坦福 HAI 版」的前缀，则是因为本图笼罩着满满的「斯坦福」光环。本图虽然不够详尽，但是作为大家一窥人工智能历史以及目前相关进展的精简版素材，还是足够的。下面就一起逐个看看吧~人工智能大事记1955 年，在达特矛斯会议上，当时备受尊敬的计算机科学家约翰·麦卡锡首度提出「人工智能」这一概念。之后，本次会议也被视作人工智能正式诞生的标志，而提出者约翰·麦卡锡也被誉为「人工智能之父」。1963 年，「人工智能之父」约翰·麦卡锡创建了斯坦福人工智能实验室（SAIL，Stanford Artificial Intelligence Laboratory）。该实验室拥有多个领域的专家，涉及机器人技术、计算机视觉、机器学习、图像处理、自然语言处理等多个领域，代表人物包括一大批在人工智能领域闻名遐迩的人物，如 Christopher Manning 、吴恩达、李飞飞等。1966 年，MIT 计算机科学家 Joseph Weizenbaum 开发出首个自然语言处理程序——ELIZA，它是一个模拟罗杰斯心理治疗的聊天机器人。1967 年，斯坦福大学 E.A. Feigenbaum 领导开发出第一个「专家系统」——DENRAL， 使得人工智能的研究以推理算法为主转变为以知识为主。1969 年，阿瑟·布莱森（Arthur Bryson）和何毓琦（Yu-Chi Ho）提出机器学习领域最重要的算法——反向传播算法（Backpropagation）。这种算法可用于多层人工神经网络，2000 年至今深度学习的发展都离不开它的启发。1973 年，美国斯坦福国际研究所（Stanford Research Institute, SRI）研制出首台采用了人工智能学的移动机器人——Shakey。1979 年，汉斯·摩拉维克（Hans Moravec）在斯坦福大学就读研究生时期发明的 Stanford Cart，在无人干预的情况下自动穿过摆满椅子的房间并前后行驶了 5 小时。Stanford Cart 相当于早期无人驾驶汽车。1988 年，IBM 沃森研究中心发表《机器翻译的统计方法》（A STATISTICAL APPROACH TO LANGUAGE TRANSLATION），预示着基于规则的机器翻译方法开始转变为基于概率的方法，并反映了一个更为广泛的转变：从「理解」眼前的任务的「机器学习」方法转变为基于已知例子的统计分析方法。1991 年，蒂姆·伯纳斯－李（Tim Berners-Lee）发明的万维网首次上线。1997 年，IBM 研发的「深蓝」（Deep Blue）成为第一个击败人类象棋冠军 Garry Kasparov 的电脑程序。,1998 年，斯坦福大学教授肯尼斯·萨里斯伯里（Kenneth Salisbury）公开外科机器人（robotic surgery）专利。2005 年，斯坦福大学教授 Sebastian Thrun 联合斯坦福大学 AI 实验室发明的第一辆自动驾驶汽车完成了 132 英里的 Mojava 沙漠路线，在 DARPA 超级挑战赛（DARPA Grand Challenge）上一举夺冠。2009 年，李飞飞主导的 ImageNet 项目诞生了一个含有 1500 万张照片的数据库，涵盖了 22000 种物品。这个项目以及后来的一系列工作影响了整个计算机视觉领域发展。2010 年，苹果公司推出一款内建在苹果 iOS 系统中的人工智能助理软件 Siri。2011 年，IBM 开发的自然语言问答计算机沃森在美国老牌益智节目「危险边缘」（Jeopardy！）中击败人类。2012 年，杰夫·迪恩（Jeff Dean）和吴恩达（Andrew Ng）发布一份实验报告，他们给一个大型神经网络展示 1000 万张未标记的网络图片，发现神经网络能够识别出猫的形象。2014 年，亚马逊推出了智能音箱 Echo 以及智能语音助手 Alexa。2016 年，谷歌 DeepMind 研发的 AlphaGo 击败围棋世界冠军李世石。全球人工智能初创公司布局HAI 援引全球知名创投研究机构 Asgard CB Insights 的调查数据， 目前全球共涌现了 3600 余家人工智能初创公司，其中美国有 1393 家，占比 40％ 左右，排名第一；中国有 383 家，占比 11％ 左右，排名第二；而欧洲诸国中拥有 AI 公司数量最多的是英国，其有 245 家，占比 7% 左右；以色列有 362 家，占比 10％ 左右；加拿大仅有 131 家，占比 4% 左右。同时，Asgard CB Insights 还收集了美国各城市所拥有人工智能初创数量的数据，其中旧金山有 596 家，排名第一；纽约有 180 家，排名第二；波士顿有 102 家，排名第三；之后依次是洛杉矶、华盛顿、奥斯汀、西雅图，分别为 73 家、36 家、36 家以及 35 家。另外据 Pricewaterhousecoopers，2018 年美国初创公司从投资公司募集到的融资总额为 93 亿美元。另外，针对人工智能对全球经济的贡献，高德纳（Gartner）发布的报告预测，至 2030 年，人工智能将为全球经济贡献 15.7 万亿美元，这一数字将超过中国与印度两个国家目前的经济总量之和。人工智能的普及度进展针对人工智能的普及度，HAI 主要收集了美国国会提及「人工智能」的次数以及上市公司投资者所提及的科技术语次数两个方面的数据。随着人工智能的发展，美国国会提到「人工智能」的频率也日益增高，2017 年以前，美国国会提及「人工智能」的次数微乎其微，最高也不到 20 次，然而在 2017 年提及的次数竟飙升至近 100 次，这也从侧面展示了各国已经将人工智能布局提升到了国家战略布局的高度。而上市公司投资者所提及的科技术语次数方面，在 2011 年前，被提及最多的术语是「云计算」；2011 年至 2016 年是「大数据」；2016 年至 2017 年，「机器学习」与「人工智能」二者不相上下；而 2017 年后则是「人工智能」被提及的次数最多了。这些数据从侧面反映出了科技领域的发展趋势。人工智能的研究和教育进展在人工智能的研究和教育方面，HAI 援引 AI index 统计的学术论文发表数量和登记为「人工智能入门课程」的大学课程数量。其中学术论文发表数量方面，自 1996 年以来，关于「人工智能」主题的学术论文发表数量翻了 8 倍以上；而 2012 年至 2017 年，大学课程中登记为「人工智能入门课程」的课程数量的增长量高达 500%。这无疑彰显了学术界对于「人工智能」研究的极高热情。斯坦福的人工智能突破性成果而对于人工智能领域所取得的突破性成果，HAI 则重点列举了在人工智能领域具有代表性意义的高校——斯坦福大学的研究成果，包括：斯坦福大学 AI 实验室发明的自动驾驶汽车 Stanley 获得了 DARPA 超级挑战赛（DARPA Grand Challenge）冠军；斯坦福大学教授李飞飞主导的 ImageNet 改变了机器学习/AI 的发展路线，并引领了深度学习时代；斯坦福大学 AI 实验室共有 18 位成员获得 ACM 图灵奖；斯坦福大学联合斯坦福医学院开发出第一个用于医疗 AI 的超级计算机；斯坦福国际研究所（Stanford Research Institute, SRI）研制出第一台真正意义上的移动机器人；斯坦福大学于 2010 年发布领先的开源自然语言处理工具包——Stanford CoreNLP；斯坦福大学研制出人形潜水机器人——OceanOne，该机器人具有触觉反馈功能，可以高保真地探索海底世界。斯坦福人工智能课程在 2018 年，斯坦福开设的人工智能相关课程涵盖了 54 个研究主题，包括计算机视觉、自然语言处理、高级机器人以及计算基因组学等。其中最受欢迎的则要数吴恩达推出的《CS 221： 人工智能原理与技术》课程。雷锋网雷锋网本文转自：https://www.linuxprobe.com/stanford-hai-detailed.html"}
{"content2":"《Windows Azure Platform 系列文章目录》在上一节中Azure 认知服务 (2) 计算机视觉API - 分析图像，笔者介绍了如何使用API测试控制台进行调试。本章将介绍如何使用C#代码调用分析图像功能。我们需要准备：1.Azure China账户2.计算机视觉API的API Key3.分析的图片URL：https://leizhangstorage.blob.core.chinacloudapi.cn/azureblog/analyzeimagesample.jpg现在开始正文：1.我们可以访问：https://dev.cognitive.azure.cn/docs/services/56f91f2d778daf23d8ec6739/operations/56f91f2e778daf14a499e1fa可以看到最下面提供不同的开发语言Code Sample:2.我们复制出C# Code，这是一个Windows Console根据注释的内容，修改变量(1) API Key(2) JPG图片URL代码如下：static void Main(string[] args) { MakeRequest(); Console.WriteLine(\"Hit ENTER to exit...\"); Console.ReadLine(); } static async void MakeRequest() { var client = new HttpClient(); var queryString = HttpUtility.ParseQueryString(string.Empty); // Request headers // 这里输入API Key client.DefaultRequestHeaders.Add(\"Ocp-Apim-Subscription-Key\", \"{API key}\"); // Request parameters // 这里输入visual Features queryString[\"visualFeatures\"] = \"Categories,Tags,Description,Faces,ImageType,Color,Adult\"; queryString[\"details\"] = \"\"; queryString[\"language\"] = \"en\"; var uri = \"https://api.cognitive.azure.cn/vision/v1.0/analyze?\" + queryString; HttpResponseMessage response; // 这里输入使用的jpg图片路径 string s = @\"{\"\"url\"\":\" + @\"\"\"https://leizhangstorage.blob.core.chinacloudapi.cn/azureblog/analyzeimagesample.jpg\"\"}\"; // Request body byte[] byteData = Encoding.UTF8.GetBytes(s); using (var content = new ByteArrayContent(byteData)) { content.Headers.ContentType = new MediaTypeHeaderValue(\"application/json\"); response = await client.PostAsync(uri, content); var contents = await response.Content.ReadAsStringAsync(); } }"}
{"content2":"人工智能（计算机科学的一个分支）人工智能（Artificial Intelligence），英文缩写为AI。它是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。 人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。人工智能是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。人工智能从诞生以来，理论和技术日益成熟，应用领域也不断扩大，但没有一个统一的定义。人工智能是对人的意识、思维的信息过程的模拟。人工智能不是人的智能，但能像人那样思考、也可能超过人的智能。但是这种会自我思考的高级人工智能还需要科学理论和工程上的突破。人工智能是一门极富挑战性的科学，从事这项工作的人必须懂得计算机知识，心理学和哲学。人工智能是包括十分广泛的科学，它由不同的领域组成，如机器学习，计算机视觉等等，总的说来，人工智能研究的一个主要目标是使机器能够胜任一些通常需要人类智能才能完成的复杂工作。但不同的时代、不同的人对这种“复杂工作”的理解是不同的。"}
{"content2":"人工智能的发展史及应用开篇：人工智能无处不在人工智能的发展历程· 1945艾伦图灵在论文《计算机器不智能》中提出了著名的图灵测试，给人工智能的収展产生了深远的影响。· 1951年，马文·明斯基（Marvin Minsky）和迪恩·爱德蒙（Dean Edmunds）建立了\"随机神经网络模拟加固计算器\"SNARC。· 1955年8月31日，\"人工智能\"（artificial intelligence）一词在一份关于召开国际人工智能会议的提案中被提出，正式宣告人工智能作为一门学科的诞生。· 在1965年麻省理工学院约瑟夫·维森班（Joseph Weizenbaum）间建立了世界上第一个自然语言程序ELIZA。· 70年代开始 ，科学家的成果无法满足社会的期待，有限的计算机能力和快速增长的计算需求之间形成了尖锐的矛盾。人工智能进入第一个冬天。· 1981年，日本国际贸易和工业部提供8.5亿美元用于第亓代计算机项目研究。· 1986年10月，大卫·鲁梅尔哈特（David Rumelhart）、杰弗里·辛顿（Geoffrey Hinton）和罗纳德·威廉姆斯（Ronald Williams）収表了一篇具有里程碑意义的经典论文《通过误差反向传播学习表示》· 80年代后期 ，产业界収现对与家系统的开収不维护成本高昂，商业价值有限，在失望情绪的影响下，对人工智能投入大副削减，人工智能的収展再度步入冬天。· 2007年，李飞飞（Fei Fei Li ）和普林斯顿大学的同事开始建立ImageNet。这是一个大型注释图像数据库，旨在帮助视觉对象识别软件进行研究。· 2010年，ImageNet大规模视觉识别挑战赛（ILSVCR）正式举办，这项比赛是为了比较大家在影像辨识和分类斱面的运算能力.· 21世纪互联网蓬勃収展，人类进入\"大数据旪代\"，电脑芯片的计算能力持续调整增长，人工智能学者开始引入丌同学科的工具，一大批新的数学模型和算法被収展起来，人工智能厚积薄収，再造辉煌，迎来第三次浪潮· 2012年10月，多伦多大学设计的卷积神经网络在ImageNet大规模视觉识别挑战赛（ILSVCR）中实现了16%的错误率。比前一年的最佳水平（25%）有了明显提高。· 2016年3月，谷歌DeepMind研収的AlphaGo在围棋人机大战中击败韩国职业九段棋手李世乭。在2017年又战胜了当旪世界排名第一的中国棋手柯洁。· 2018年人工智能成为最热门的科技话题之一，未来商业价值显著，人才需求近一步扩大。人工智能的前景市场规模人工智能的前景国家政策人工智能的前景发展驱动力人工智能的前景人才培养与教育人工智能的前景AI巨头公司布局人工智能在各行各业的应用人工智能如何解决行业痛点· 安防：利用计算机视觉技术和大数据分析犯罪嫌疑人生活轨迹及可能出现的场所金融：利用语音识别、 语义理解等技术打造智能客服· 医疗：智能影像可以快速进行癌症早期筛查， 帮助患者更早収现病灶· 交通：无人驾驶通过传感器、 计算机视觉等技术解放人的双手和感知· 零售：利用计算机视觉、 语音/语义识别， 机器人等技术提升消费体验· 工业制造：机器人代替工人在危险场所完成工作 ,在流水线上高效完成重复工作详细内容可以结合视频学习直播地址：http://zhibo.huaweicloud.com/watch/2668613版权声名：部分图片素材来源网络,如有侵仅及时联系本人转载请注明来源作者：刘毅超 微信:yichao233作者：superba链接：https://juejin.im/post/5c9deab7f265da30784e01ea来源：掘金著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}
{"content2":"2017-01-18人民网-科普中国小字人民网北京1月18日电 说起人工智能你会想到什么?是最近横扫围棋界高手的Master?还是科幻电影《终结者》中具有自我意识的“天网”?你还觉得人工智能离我们遥不可及吗?清华大学计算机科学与技术系教授、博士生导师邓志东表示,人工智能正迎来发展的春天,或可引爆第四次工业革命!现在就让我们一起来盘点一下最新的人工智能大事件。一张图带你看完人工智能大事件(赵鹏 制图)2016年,人工智能迎来了春天邓志东表示,“2016年,以深度学习为主要标志的人工智能正迎来第三次伟大复兴,人工智能有望像19世纪的蒸汽机革命那样,彻底改变我们的生活。而且,人工智能给人类社会带来的影响有可能远远超过蒸汽机、电力和互联网带来的前三次工业革命。未来,人工智能将无处不在,甚至有可能代替人类从事部分脑力劳动。”在人工智能决策领域,2016年3月9日~15日,谷歌AlphaGo以4:1的成绩击败世界围棋冠军李世乭；2016年12月29日~2017年1月4日,谷歌Master(AlphaGo升级版)在30秒快棋网测中,以60胜0负1和的战绩,横扫柯杰、古力、聂卫平等数十位中日韩世界冠军与顶级高手；2017年1月,专家水平的人工智能首次战胜一对一无限注德州扑克人类职业玩家。在人工智能与无人驾驶领域,2016年9月14日,Uber 在美国匹兹堡市推出城区大范围无人驾驶出租车免费载客服务并试运行；2016年10月,Elon Musk宣布Tesla所有新车将安装具有完全自动驾驶功能的硬件系统Autopilot 2.0,并计划在2017年年底之前以完全自动驾驶模式让无人驾驶汽车从洛杉矶开往纽约；2016年11月15日,18辆“云骁”自动驾驶汽车亮相乌镇子夜路,在3.16公里的开放城区道路上自主行驶。此外,Google Waymo、沃尔沃、福特、宝马、百度、英特尔等全球近20家企业宣称,2021年将会是无人驾驶汽车元年。在人工智能识别领域,2016年12月,英国牛津大学、谷歌DeepMind等研发的自动唇读系统LipNet对Gird语料库实现了准确率为95.2%的唇语识别,其对BBC电视节目嘉宾进行唇语解读,准确率为46.8%,远远超过专业的人类唇语专家(仅为12.4%)；2017年1月6日,百度人工智能机器人“小度”利用其超强的人脸识别能力,以3:2的成绩战胜人类最强大脑代表王峰。另外,在人工智能语音合成领域,2016年9月19日,谷歌DeepMind推出WaveNet,实现了文本到美式英语或中国普通话的真实感语音合成；在人工智能速记领域,2016年10月17日,微软的语音识别系统实现了5.9%的词错率,媲美人类专业速记员；在人工智能翻译领域,2016年9月27日,Google的神经机器翻译系统(GNMT)实现的多语种翻译,与传统方法相比,其英语—西班牙语翻译错误率下降了87%,英语—汉语翻译错误率下降了58%,汉语—英语翻译错误率下降了60%,已接近人工翻译水平。邓志东在“刊媒惠”年度大会上发表演讲(赵鹏 摄)如何发展我国的人工智能技术与相关产业邓志东认为,发展我国的人工智能技术与相关产业需要做到以下五点:1、以深度卷积神经网络为核心,全面开展计算机视觉、语音识别和自然语言处理等人工智能产品的开发与大规模产业化应用。2、选定人工智能发展的垂直细分领域,并针对垂直细分领域建立大数据中心,同时建立相关公共基础设施与垂直领域知识库(知识图谱),实现大数据与大知识的采集、清理、标签、存储、管理与交易。3、开展人工智能芯片与硬件计算平台的研发,建立国家级人工智能超算中心。4、发展通用人工智能与认知智能前沿技术,加强与脑科学、认知科学、心理学等的多学科的交叉融合创新,推动原创性基础研究,为中国人工智能的应用与产业发展提供长远支撑。5、创新体制机制,抢占人工智能战略制高点,以此打通政、产、学、研、用各环节,进而保障技术与产业创新的实现。(实习生赵鹏)受访专家:清华大学计算机科学与技术系教授、博士生导师邓志东"}
{"content2":"当前人工智能领域玩的是热火朝天，导致AI人才供不应求。那怎么知道自己适不适合学AI技术呢？学AI，需要掌握Python、数学、算法，这些是跑不掉的。但是怎么知道自己适不适合学AI呢？看下面：AI课程目录模块一：计算机视觉1. 数字成像系统2. 视觉处理与分析3. 视觉处理算法基础4. 视觉特征提取5. 运动估计模块二：机器学习入门1. 机器学习简介2. 线性回归3. Logistic回归模块三：深度学习入门1. 神经网络2. 全连接神经网络介绍3. 深度学习基础4. 深度学习网络结构5. 深度学习训练与优化模块四：框架与环境1. TensorFlow框架模块五：机器学习基础算法1. SVM2. 决策树3. 集成机器学习4. 非监督学习5. 推荐系统模块六：位姿估计与三维重构1. 位姿估计2. 极线几何与立体视觉模块七：计算机视觉与神经网络1. 卷积神经网络2. 基于slim的神经网络模型训练模块八：工业级实战项目1. CTR预估2. 视频目标跟踪3. 全景拼接4. 车辆检测看完之后，你觉得自己适合学习AI吗？"}
{"content2":"1、Linear Algebra (线性代数) 和 Statistics (统计学) 是最重要和不可缺少的。这代表了Machine Learning中最主流的两大类方法的基础。一种是以研究函数和变换为重点的代数方法，比如Dimension reduction，feature extraction，Kernel等，一种是以研究统计模型和样本分布为重点的统计方法，比如Graphical model, Information theoretical models等。它们侧重虽有不同，但是常常是共同使用的，对于代数方法，往往需要统计上的解释，对于统计模型，其具体计算则需要代数的帮助。以代数和统计为出发点，继续往深处走，我们会发现需要更多的数学。2、Calculus (微积分)，只是数学分析体系的基础。其基础性作用不言而喻。Learning研究的大部分问题是在连续的度量空间进行的，无论代数还是统计，在研究优化问题的时候，对一个映射的微分或者梯度的分析总是不可避免。而在统计学中，Marginalization和积分更是密不可分——不过，以解析形式把积分导出来的情况则不多见。3、Partial Differential Equation （偏微分方程)，这主要用于描述动态过程，或者仿动态过程。这个学科在Vision中用得比Learning多，主要用于描述连续场的运动或者扩散过程。比如Level set, Optical flow都是这方面的典型例子。4、Functional Analysis (泛函分析)，通俗地，可以理解为微积分从有限维空间到无限维空间的拓展——当然了，它实际上远不止于此。在这个地方，函数以及其所作用的对象之间存在的对偶关系扮演了非常重要的角色。Learning发展至今，也在向无限维延伸——从研究有限维向量的问题到以无限维的函数为研究对象。Kernel Learning 和 Gaussian Process 是其中典型的例子——其中的核心概念都是Kernel。很多做Learning的人把Kernel简单理解为Kernel trick的运用，这就把kernel的意义严重弱化了。在泛函里面，Kernel (Inner Product)是建立整个博大的代数体系的根本，从metric, transform到spectrum都根源于此。5、Measure Theory (测度理论)，这是和实分析关系非常密切的学科。但是测度理论并不限于此。从某种意义上说，Real Analysis可以从Lebesgue Measure（勒贝格测度）推演，不过其实还有很多别的测度体系——概率本身就是一种测度。测度理论对于Learning的意义是根本的，现代统计学整个就是建立在测度理论的基础之上——虽然初级的概率论教科书一般不这样引入。在看一些统计方面的文章的时候，你可能会发现，它们会把统计的公式改用测度来表达，这样做有两个好处：所有的推导和结论不用分别给连续分布和离散分布各自写一遍了，这两种东西都可以用同一的测度形式表达：连续分布的积分基于Lebesgue测度，离散分布的求和基于计数测度，而且还能推广到那种既不连续又不离散的分布中去（这种东西不是数学家的游戏，而是已经在实用的东西，在Dirchlet Process或者Pitman-Yor Process里面会经常看到)。而且，即使是连续积分，如果不是在欧氏空间进行，而是在更一般的拓扑空间（比如微分流形或者变换群），那么传统的黎曼积分（就是大学一年级在微积分课学的那种）就不work了，你可能需要它们的一些推广，比如Haar Measure或者Lebesgue-Stieltjes积分。6、Topology（拓扑学)，这是学术中很基础的学科。它一般不直接提供方法，但是它的很多概念和定理是其它数学分支的基石。看很多别的数学的时候，你会经常接触这样一些概念：Open set / Closed set，set basis，Hausdauf, continuous function，metric space, Cauchy sequence, neighborhood, compactness, connectivity。很多这些也许在大学一年级就学习过一些，当时是基于极限的概念获得的。如果，看过拓扑学之后，对这些概念的认识会有根本性的拓展。比如，连续函数，当时是由epison法定义的，就是无论取多小的正数epsilon，都存在xxx，使得xxx。这是需要一种metric去度量距离的，在general topology里面，对于连续函数的定义连坐标和距离都不需要——如果一个映射使得开集的原像是开集，它就是连续的——至于开集是基于集合论定义的，不是通常的开区间的意思。这只是最简单的例子。当然，我们研究learning也许不需要深究这些数学概念背后的公理体系，但是，打破原来定义的概念的局限在很多问题上是必须的——尤其是当你研究的东西它不是在欧氏空间里面的时候——正交矩阵，变换群，流形，概率分布的空间，都属于此。7、Differential Manifold (微分流形)，通俗地说它研究的是平滑的曲面。一个直接的印象是它是不是可以用来fitting一个surface什么的——当然这算是一种应用，但是这是非常初步的。本质上说，微分流形研究的是平滑的拓扑结构。一个空间构成微分流形的基本要素是局部平滑：从拓扑学来理解，就是它的任意局部都同胚于欧氏空间，从解析的角度来看，就是相容的局部坐标系统。当然，在全局上，它不要求和欧氏空间同胚。它除了可以用于刻画集合上的平滑曲面外，更重要的意义在于，它可以用于研究很多重要的集合。一个n-维线性空间的全部k-维子空间(k8、Lie Group Theory (李群论)，一般意义的群论在Learning中被运用的不是很多，群论在Learning中用得较多的是它的一个重要方向Lie group。定义在平滑流形上的群，并且其群运算是平滑的话，那么这就叫李群。因为Learning和编码不同，更多关注的是连续空间，因为Lie group在各种群中对于Learning特别重要。各种子空间，线性变换，非奇异矩阵都基于通常意义的矩阵乘法构成李群。在李群中的映射，变换，度量，划分等等都对于Learning中代数方法的研究有重要指导意义。9、Graph Theory（图论)，图，由于它在表述各种关系的强大能力以及优雅的理论，高效的算法，越来越受到Learning领域的欢迎。经典图论，在Learning中的一个最重要应用就是graphical models了，它被成功运用于分析统计网络的结构和规划统计推断的流程。Graphical model所取得的成功，图论可谓功不可没。在Vision里面，maxflow (graphcut)算法在图像分割，Stereo还有各种能量优化中也广受应用。另外一个重要的图论分支就是Algebraic graph theory (代数图论)，主要运用于图的谱分析，著名的应用包括Normalized Cut和Spectral Clustering。近年来在semi-supervised learning中受到特别关注。"}
{"content2":"这是一个贝叶斯模型的计算机视觉小项目。希望大家通过这个简单的项目知道一般的计算机视觉项目是怎样操作的。我先讲题目放在这里希望有兴趣的童鞋花一周的时间思考并用python实现。一周以后我来发布我的详细操作细节和代码。希望大家能够通过这个简单的项目将自己学到的机器学习以及计算机视觉的知识应用到实践其中。基于OpenCV编写一个基于朴素贝叶斯的图片分类分类程序要求：[1] 在google的图片搜索引擎中输入\"flower\"和\"airplane\"，分别下载m(>100)张\"flower\"图片和n(>100)张\"sky\"图片，作为数据集的正负样本；(注意：这两类图片也能够随意选择，比方\"chair\"和\"car\")[2] 视觉词典的构建採用例如以下的方式：随机从每张训练集的图片中分割出p（p>500）个25x25的图片块，计算改图像块的颜色直方图特征（6x6x6个区间），然后将这些图像块（共同拥有p*（m+n））用kmeans聚成K(K>1000)个类，作为词典；注：kmeans能够直接调用opencv，或者用其它工具包来聚类。[3] 训练或者測试时，对待測试的图片进行密集採样，用Histogram Intersection距离（计算公式为）。採用近期邻的方法计算每一个单词的次数。[4] 使用5折交叉验证计算出平均的Precision和Recall；数据集大家能够考虑编写一个爬虫到谷歌下载图片（假设能够的  麻烦教我爬虫怎么操作哈）假设不行的能够找我索要数据集QQ 42 #14# 14# 626 。"}
{"content2":"最近打算学习一下AForge.NET，AForge.NET是一个专门为开发者和研究者基于C#框架设计的，这个框架提供了不同的类库和关于类库的资源，还有很多应用程序例子，包括计算机视觉与人工智能，图像处理，神经网络，遗传算法，机器学习，机器人等领域。这个框架由一系列的类库组成。主要包括有：AForge.Imaging —— 一些日常的图像处理和过滤器AForge.Vision —— 计算机视觉应用类库AForge.Neuro —— 神经网络计算库AForge.Genetic -进化算法编程库AForge.MachineLearning —— 机器学习类库AForge.Robotics —— 提供一些机器学习的工具类库AForge.Video —— 一系列的视频处理类库AForge.Fuzzy —— 模糊推理系统类库AForge.Controls—— 图像，三维，图表显示控件我这里打算就1）基于模糊系统的自动导航、2).运动检测、3）C#网络视频传输，这三个方面进行学习。会尽快把学习心得与大家分享。基于网络视频传输的部分，我参考别人的代码已经写出了示例程序，如图：因为下位机平台目前还没有搭建，所以无法进行传输视频的测试，等以后测试了再上传测试结果。因为我这里用的程序来自网络所以也就不把程序贴上来了。我把我学习中遇到的问题和解决方法给大家说说。问题1：安装AForge.net之后无法找到VideoSourcePlayer控件。方法：在VS2008  的工具中  右键->选择项-> .NET Framerwork  ->  点击命名空间进行排序，名称一列选择  VideoSourcePlayer，确定即可。问题2：定时器设定的时间为多少。答：  为1000ms1 private void timer1_Tick(object sender, EventArgs e) 2 { 3 // Interface 4 IVideoSource videoSource = videoSourcePlayer.VideoSource; 5 if (videoSource != null) 6 { 7 statCount[statIndex] = videoSource.FramesReceived; 8 if (++statIndex >= statLength) 9 statIndex = 0; 10 if (statReady < statLength) 11 statReady++; 12 float fps = 0; 13 for (int i = 0; i < statReady; i++) 14 { 15 fps += statCount[i]; 16 } 17 fps /= statReady; 18 statCount[statIndex] = 0; 19 fpsLabel.Text = fps.ToString(\"F2\") + \" fps\"; 20 } 21 }这段代码是用来计算刷新的帧数，首先private int[] statCount = new int[statLength];  statLength=15，用来储存已经刷新的次数。fps /= statReady;这个用来求刷新的平均值 ，因为时间为一秒，所以得到的结果也就是标准的刷新频率。问题3：?action=snapshot，?action=stream表示什么？答：?action=snapshot返回的是一幅静态图片，?action=stream返回的才是连续的mjpeg流。完整的代码大家可以从这个博客里阅读http://hi.baidu.com/lansessl/item/1e890df3e1dccf12d7ff8cab"}
{"content2":"前言自然语言处理（Natural Language Processing）是计算科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。自然语言处理是一门融语言学、计算机科学、数学于一体的科学。因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，所以它与语言学的研究有着密切的联系，但又有重要的区别。自然语言处理并不是一般地研究自然语言，而在于研制能有效地实现自然语言通信的计算机系统，特别是其中的软件系统。因而它是计算机科学的一部分（可耻的粘贴）。既然NLP是人工智能的一部分，那么我们就来简单说说人工智能。人工智能人工智能的目标推理自主学习 & 调度机器学习自然语言处理（NLP）计算机视觉处理机器人通用智能人工智能三大阶段机器学习：只能系统使用一系列算法从经验中进行学习机器智能：机器使用的一系列从经验中进行学习的高级算法，例如深度神经网络（深度学习），人工智能目前也处于现阶段机器意识：不需要外部数据就能从经验中自我学习。人工智能、机器学习、深度学习的关系机器学习：一种实现人工智能的方法深度学习：一种实现机器学习的技术人工智能的类型狭义人工智能（ANI）：它包含基础的、角色行任务。例如小爱、Siri、Alexa这样的聊天机器人，个人助手完成的任务。通用人工智能（AGI）：通用人工智能包含人类水平的任务，它涉及到机器的持续学习。强人工智（ASI）：强人工智能代指比人类更聪明的机器。怎样让机器智能化自然语言处理知识表示自动推理机器学习NLPNLP的目标NLP的目标是让计算机在理解语言方面像人类一样智能，最终的目标是弥补人类交流（自然语言）和计算机理解（机器语言）之间的差距。为什么需要NLP有了NLP，就可能完成自动语音、自动文本的编写等任务。让我们从大量的数据中解放出来，让计算机去执行。这些任务包括自动生成给定文本的摘要、机器翻译及其他的任务。NLP语言学分析现在，有一个给定的文本。NLP可以从下面三个不同等级来分析该给定文本：语法学：该文本语法的正确性语义学：该文本的含义是什么语用学：该文本的目的的什么除此之外，如果文本中含有音、视频，那么NLP又要从以下两方面来着手分析：音韵学：该语言中发音的系统化组织词态学：研究单词构成以及彼此之间的关系NLP中理解语义的方法：分布式：利用机器学习和深度学习的大规模统计策略框架式：句法不同，但语义相同的句子在数据结构被表示为程式化的情景理论式：思路是句子指代的正真的词结合句子的部分内容可表达全部含义交互式（学习）：它涉及大语用方法，在交互式学习环境中用户教计算机一步一步的学习语言NLP的流程NLP的机制涉及两个流程：自然语言理解自然语言生成自然语言理解（NLU）自然语言理解（Nature Language Understanding）是要理解给定文本的含义。比如文本内的每个单词的特性与结构需要被理解。在理解结构的基础上，NLU要理解自然语言中以下几个歧义：词法歧义性：单词有多重含义句法歧义性：语句有多重解析树语义歧义性：句子有多重含义回指歧义性：前文中的词语或句子在后面句子中有不同的含义自然语言生成（NLG）NLG是从结构化数据中以可读的方式自动生成文本的过程。自然语言生成可分为三个阶段：文本规划：完成结构化数据中基础内容的规划语句规划：从结构化数据中组合语句来表达信息流实现：生产语法通顺的语句来表达文本NLP的应用领域聊天机器人聊天机器人或者智能代理指的是，你能通过APP、聊天窗口、语音等方式进行交流的计算机程序。它的重要性在越来越多的地方得到体现：它对理解数字化客服和频繁咨询的常规问答领域中的变化至关重要它在一些特定场景下非常的有用及高效，特别是会被频繁问到的高度可预测的问题时聊天机器人的工作机制：基于知识：包含信息库，根据客户的问题回应相对的问题数据存储：包含与用户交流的历史信息NLP层：该层将用户的问题转译为信息，从而作出合适的回应应用层：用来与用户交互的应用接口NLP中深度学习的重要性它使用基于规则的方法将单词表示为one-hot编码向量传统的方法注重句法表征，而非语义表征词袋，分类模型不能够分别特定语境深度学习的三项能力：可表达性：该能力描述了机器如何能近似通用函数可训练性：深度学习系统学习问题的速度与能力可泛化性：在未训练过的数据上，机器做预测的能力除此之外，深度学习还有其他的能力，比如可解释性、模块性、可迁移性、延迟、对抗稳定性、安全方面等。日志中的NLP在日志分析和日志挖掘两方面，NLP在发挥着不可替代的作用。通过词语切分、词干提取、词形还原、解析等不同技术被用来将日志转换成结构化的形式。在日志分析中，NLP通过下列技术完成分析功能：模式识别：将日志信息与模式薄中的信息进行对比，从而过滤信息的技术标准化：日志信息的标准化将不同的信息转换为同样的格式。当不同来源的日志信息中有不同的疏于，但其含义相同时，需要进行标准化分类& 标签：不同日志信息的分类 & 标签涉及到对信息的排序，并用不同的关键词进行标注Artificial Ignorance：使用机器学习算法抛弃无用日志信息的技术。它也可被用来检测系统异常当日志以很好的形式组织起来之后，我们就能从日志中提取有用的信息。NLP的其他领域除了在大数据、日志挖掘和分析中，NLP还浪迹在其他的应用领域中：自动摘要：在给定文本的情况下，摒弃次要信息完成文本摘要情感分析：在给定文本中预测期主题，比如文本中是否包含批判、观点、评论等文本分类：按照其领域分类不同的、新闻报道、期刊等。比如流行的文本分类是垃圾邮件、基于写作风格可检测作者的姓名信息提取：建议电子邮件程序自动添加事件到日历see also： 人工智能、机器学习和深度学习的区别? | 自然语言处理(NLP)基础理解that's all 前言自然语言处理（Natural Language Processing）是计算科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。自然语言处理是一门融语言学、计算机科学、数学于一体的科学。因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，所以它与语言学的研究有着密切的联系，但又有重要的区别。自然语言处理并不是一般地研究自然语言，而在于研制能有效地实现自然语言通信的计算机系统，特别是其中的软件系统。因而它是计算机科学的一部分（可耻的粘贴）。既然NLP是人工智能的一部分，那么我们就来简单说说人工智能。人工智能人工智能的目标推理自主学习 & 调度机器学习自然语言处理（NLP）计算机视觉处理机器人通用智能人工智能三大阶段机器学习：只能系统使用一系列算法从经验中进行学习机器智能：机器使用的一系列从经验中进行学习的高级算法，例如深度神经网络（深度学习），人工智能目前也处于现阶段机器意识：不需要外部数据就能从经验中自我学习。人工智能、机器学习、深度学习的关系机器学习：一种实现人工智能的方法深度学习：一种实现机器学习的技术人工智能的类型狭义人工智能（ANI）：它包含基础的、角色行任务。例如小爱、Siri、Alexa这样的聊天机器人，个人助手完成的任务。通用人工智能（AGI）：通用人工智能包含人类水平的任务，它涉及到机器的持续学习。强人工智（ASI）：强人工智能代指比人类更聪明的机器。怎样让机器智能化自然语言处理知识表示自动推理机器学习NLPNLP的目标NLP的目标是让计算机在理解语言方面像人类一样智能，最终的目标是弥补人类交流（自然语言）和计算机理解（机器语言）之间的差距。为什么需要NLP有了NLP，就可能完成自动语音、自动文本的编写等任务。让我们从大量的数据中解放出来，让计算机去执行。这些任务包括自动生成给定文本的摘要、机器翻译及其他的任务。NLP语言学分析现在，有一个给定的文本。NLP可以从下面三个不同等级来分析该给定文本：语法学：该文本语法的正确性语义学：该文本的含义是什么语用学：该文本的目的的什么除此之外，如果文本中含有音、视频，那么NLP又要从以下两方面来着手分析：音韵学：该语言中发音的系统化组织词态学：研究单词构成以及彼此之间的关系NLP中理解语义的方法：分布式：利用机器学习和深度学习的大规模统计策略框架式：句法不同，但语义相同的句子在数据结构被表示为程式化的情景理论式：思路是句子指代的正真的词结合句子的部分内容可表达全部含义交互式（学习）：它涉及大语用方法，在交互式学习环境中用户教计算机一步一步的学习语言NLP的流程NLP的机制涉及两个流程：自然语言理解自然语言生成自然语言理解（NLU）自然语言理解（Nature Language Understanding）是要理解给定文本的含义。比如文本内的每个单词的特性与结构需要被理解。在理解结构的基础上，NLU要理解自然语言中以下几个歧义：词法歧义性：单词有多重含义句法歧义性：语句有多重解析树语义歧义性：句子有多重含义回指歧义性：前文中的词语或句子在后面句子中有不同的含义自然语言生成（NLG）NLG是从结构化数据中以可读的方式自动生成文本的过程。自然语言生成可分为三个阶段：文本规划：完成结构化数据中基础内容的规划语句规划：从结构化数据中组合语句来表达信息流实现：生产语法通顺的语句来表达文本NLP的应用领域聊天机器人聊天机器人或者智能代理指的是，你能通过APP、聊天窗口、语音等方式进行交流的计算机程序。它的重要性在越来越多的地方得到体现：它对理解数字化客服和频繁咨询的常规问答领域中的变化至关重要它在一些特定场景下非常的有用及高效，特别是会被频繁问到的高度可预测的问题时聊天机器人的工作机制：基于知识：包含信息库，根据客户的问题回应相对的问题数据存储：包含与用户交流的历史信息NLP层：该层将用户的问题转译为信息，从而作出合适的回应应用层：用来与用户交互的应用接口NLP中深度学习的重要性它使用基于规则的方法将单词表示为one-hot编码向量传统的方法注重句法表征，而非语义表征词袋，分类模型不能够分别特定语境深度学习的三项能力：可表达性：该能力描述了机器如何能近似通用函数可训练性：深度学习系统学习问题的速度与能力可泛化性：在未训练过的数据上，机器做预测的能力除此之外，深度学习还有其他的能力，比如可解释性、模块性、可迁移性、延迟、对抗稳定性、安全方面等。日志中的NLP在日志分析和日志挖掘两方面，NLP在发挥着不可替代的作用。通过词语切分、词干提取、词形还原、解析等不同技术被用来将日志转换成结构化的形式。在日志分析中，NLP通过下列技术完成分析功能：模式识别：将日志信息与模式薄中的信息进行对比，从而过滤信息的技术标准化：日志信息的标准化将不同的信息转换为同样的格式。当不同来源的日志信息中有不同的疏于，但其含义相同时，需要进行标准化分类& 标签：不同日志信息的分类 & 标签涉及到对信息的排序，并用不同的关键词进行标注Artificial Ignorance：使用机器学习算法抛弃无用日志信息的技术。它也可被用来检测系统异常当日志以很好的形式组织起来之后，我们就能从日志中提取有用的信息。NLP的其他领域除了在大数据、日志挖掘和分析中，NLP还浪迹在其他的应用领域中：自动摘要：在给定文本的情况下，摒弃次要信息完成文本摘要情感分析：在给定文本中预测期主题，比如文本中是否包含批判、观点、评论等文本分类：按照其领域分类不同的、新闻报道、期刊等。比如流行的文本分类是垃圾邮件、基于写作风格可检测作者的姓名信息提取：建议电子邮件程序自动添加事件到日历see also： 人工智能、机器学习和深度学习的区别? | 自然语言处理(NLP)基础理解that's all"}
{"content2":"在如今人工智能的浪潮下，无数模拟机器学习和深度学习的开发者工具倍出，其中在计算机图形学和计算机视觉里面最流行的一个库就是OpenCV库了.计算机图形学和计算机视觉学在我们的自动驾驶和仿生机器人当中有着举足轻重的地位，尤其是opencv库在里面的应用尤为广泛。今天我就分享给大家我们在python环境下，实现调用opencv库。用Python调用opencv库而不用C++的缘故是，第一：现在我们的高端处理器（如由中科大少年班的天才陈天石先生所创办的寒武纪科技有限公司的深度学习处理器）已经具备了强大的算力，除了谷歌的处理器就是地表最强的NPU了,因此我们并不需要这种高效率的语言来支撑。即使像我们普通学生使用的树莓派也可以跑得上Python，从而实现物联网。二：C++的代码过于繁琐，Python则可以用更短小的代码块来实现。在软件的开发周期上会变得更小。三：Python简单易学，笔者用三天学完了Python，学习C++则学习了接近一个星期。一：首先，您应该确保您的电脑应经安装了python的环境，笔者是安装的python3.6一个比较新的版本，以前的老版本python2.x对我们新技术的支持不是很好。同时我们python当中的各种库的版本也很有可能不相适应，这是计算机科学这门学科的特征。因此，建议安装更新版本的Python。二，如果您已经为您的Python配置了环境变量，那么请您使用win+R键，然后在输入cmd，进入Windows系统的命令行模式。三。如果您还没有为您的电脑配置环境变量的话，请为您的电脑配置环境变量，将Python的路径添加到您电脑的环境变量当中，这样就可以直接在命令行模式下进行编译Python文件了。比如这样：先在Python的命令行模式下输入：Python，然后再输入代码print（5），如果输出5成功，那么您的环境变量肯定是配置好了。关于具体该如何配置环境变量，笔者就不加累述了，这对学习计算机的朋友来说实在是小菜一碟，如果确实不知道该如何配置环境变量，是第一次接触非C语言的朋友，请您到网址https://jingyan.baidu.com/article/48206aeafdcf2a216ad6b316.html上自行学习。四：找到您以前已经安装过的Python文件的路径方便将我们的opencv库下载下来之后找到保存的地点。这时请继续保持打开我们刚才的命令行模式，输入以下代码：import sys >>> sys.path这样，计算机就会自动显示出您Python文件的安装路径了。这是十分重要的一步，没有这一步，您的计算机上是不可能安装好opencv库的。五：点击进入网址：https://www.lfd.uci.edu/~gohlke/pythonlibs/在这个国外的网站里汇聚了Python的各种类库，是美国加州大学（世界排名第三）所推出的。是我们做机器学习的人的天堂。此时，您应该找到numpy库，进行下载（因为如果您要安装opencv库的话，是必然得安装numpy库的），下载的时候记住：文件的后缀是whl，我们稍后将利用pip会进行安装。文件的前缀有cp两个英文，cp后表示的是Python的版本吗，笔者用的是3.6版本的Python，因此文件的前缀当中必然有cp36这个选项了。网站主页如图所示;六：将您刚刚所选择的numpy,opencv库的文件保存至Python安装文件的根目录下的scripts下，在这个文件的目录下，你还会发现有pip.exe以及east_install.exe的程序文件，这就对了。七：再次打开您的命令行模式：输入cd C:/xxxx(您Python文件安装目录的scripts文件夹),这改变的是您在命令行模式下的文件根目录，十分有效。八：为您的ython安装文件的根目录下的scripts配置环境变量。具体配置环境变量的方法还是和以前一样。不会的话，请参见网址：https://jingyan.baidu.com/article/48206aeafdcf2a216ad6b316.html：在命令行模式下输入：pip install wheel继续等待下载完成。如果出现了如下图像，则说明您安装成功了十：在命令行模式下输入pip install (您刚才现在的numpy文件的文件名).whl等待collect被安装，很快便会显示安装成功（笔者电脑CPU算力比较强大）安装成功后将会显示：上面这串英文应该很简单吧，正常人应该都会看得懂这是成功安装安装包的意思。最后安装opencv库。十一：安装opencv库的方法和第十步是完全一样的。十二：检测我们的OpenCV库是否安装成功。在命令行模式下输入：pip freeze这样我们就会看到您的Python除了标准库之外，您所安装的所有库文件了。笔者的由于还在做单纯的机器学习算法实现，因此，还安装了其他数据科学，和科学计算的库。不相关的库也被笔者所删除了。以上就是笔者所安装的所有库。今天的教程就到这里了，后面有关OpenCV库的Python代码实现，我将会在公众号的下一期当中发布具体的代码以及具体的使用方法。在下一期当中我们才会真正领略到opencv的强大威力。如果您有什么疑惑，可以在下面的留言区留言，我一定会尽我所能为您解答。如果我有什么说得不对的地方，也还请多多指教。"}
{"content2":"导读怎么样来理解近期异常火热的深度学习网络？深度学习有什么亮点呢？答案事实上非常简答。今年十月份有幸參加了深圳高交会的中科院院士论坛。IEEE fellow汤晓欧做了一场精彩的报告，这个问题被汤大神一语道破，他说深度学习网络说白了就是一个多层的神经网络。同20年前相比，计算机硬件性能提升非常多，有了实现处理大数据和并行运算的能力，deep learning才被又一次重视起来。这里，再反复一遍CNN的实质：CNN就是一个更深层次、具有很多其它节点的ANN网络。但与简单的ANN相比：CNN主要是卷积使得权值共享。减少连接的数量级。同一时候兼顾二维特征；从算法层面上。CNN的核心还是同BP网络一样权值正向传播误差反向传播，并利用误差来更新每一层的权值。DNN的背景DNN，deep neural network。近几年机器学习算法中崛起的旗舰方法，作为分类精度最高、处理高维大数据的算法，拯救了机器学习在半个世纪里发展缓慢的颓势，也为人工智能新领域的拓展起着重要作用，来看看近期一些IT巨头在深度学习领域中的开展的工作。2012年，《纽约时报》报道的google Brian项目，引起广泛关注，这个项目由机器学习领域大师级人物吴恩达和大规模计算机系统方面的世界顶级专家JeffDean主导，使用万为数量级的CPU数量搭建并行计算平台。模拟出10亿数量级的神经节点，让这个巨大的系统收看YouTube视频可以自学习和识别具有一个小孩的智力水平。微软在天津举办一次公开的活动展示了全自己主动的同声传译系统。演讲者用英语说话，后台计算机一气呵成地完毕语音识别、英中文翻译、语音合成，效果非常流畅。这个系统背后的关键技术就是DNN。或者说DL(Deep Learning)。2013年。李彦宏在百度年会上高调宣布成立“深度学习研究院”。Why DNN深度学习，体现出仿生生物学和人工智能等领域的光辉思想，借用获得诺贝尔生理学奖的“视觉分层”理论，深度学习採用多层的分层形式来模拟视觉系统的原理，从底层的像素获取形状方向。到大脑皮层不同区域抽象迭代获得局部特征。再到提取高层特征直到可以简单识别出模式。正由于深度学习神经网络中的深层次结构，可以无遗漏地提取样本特征，具有强大的分类能力。所以深度学习相比于其它机器学习的算法。可以减少误差率。成为近年来非常热门的算法。深度学习有着多层深层的层次结构，将底层的特征不断抽象和迭代出可标示的特征。可以发现大数据中的复杂结构，具有强大的分类能力，因而在视频图像识别、语音识别、海量文本分类等方面具有重要应用。深度学习的思想是通过深层网络的结构自己主动学习和提取海量数据的特征。假设将深度学习网络看成一个信号处理系统。输入是一堆数据（一批图片或一叠文本），通过多层网络 （i=1,2..）的处理后无损地得到输出，形象表示为： I=>S1=>S2=>…=>Sn => O，假设I和O相等，说明该系统可以无损处理输入信号。这样可以不断迭代训练网络得到一个多层网络 每层网络从低到高来表述特征。所以深度学习的目的就是训练这个多层网络，得到正确的分类參数。深度学习网络的眼下主流的模型主要有三种，深度信念网络。受限玻尔兹曼机和卷积神经网络。这里主要解说机器学习课程上的卷积神经网络模型。How CNN work这里从两个方面来阐述这个庞大的问题：- 理论方面比如。CNN怎样权值正向传播并进行误差反向？卷积过程？- 实例代码解说比如，怎样使用matlab工具箱构建CNN网络？怎样训练网络？Typical Example这里就机器学习方面的大师级人物LecunYann设计训练的CNN网络模型来说明问题，。CNN流程图具体解释"}
{"content2":"三者之间既有区别，又有联系，不确切的描述：计算机图形学≈画图计算机视觉≈看图数字图像处理≈看图前沐浴更衣焚香做好各种仪式，然后再看图计算机图形学（Computer Graphics）讲的是图形，也就是图形的构造方式，是一种从无到有的概念，从数据得到图像。是给定关于景象结构、表面反射特性、光源配置及相机模型的信息，生成图像。计算机视觉（Computer Vision）是给定图象，从图象提取信息，包括景象的三维结构，运动检测，识别物体等。数字图像处理（Digital Image Processing）是对已有的图像进行变换、分析、重构，得到的仍是图像。模式识别（PR）本质就是分类，根据常识或样本或二者结合进行分类，可以对图像进行分类，从图像得到数据。Computer Graphics和Computer Vision是同一过程的两个方向。Computer Graphics将抽象的语义信息转化成图像，Computer Vision从图像中提取抽象的语义信息。Image Processing探索的是从一个图像或者一组图像之间的互相转化和关系，与语义信息无关。总之，计算机图形学是计算机视觉的逆问题，两者从最初相互独立的平行发展到最近的融合是一大趋势。图像模式的分类是计算机视觉中的一个重要问题，模式识别中的许多方法可以应用于计算机视觉中。区别：Computer Graphics，简称 CG 。输入的是对虚拟场景的描述，通常为多边形数组，而每个多边形由三个顶点组成，每个顶点包括三维坐标、贴图坐标、rgb颜色等。输出的是图像，即二维像素数组。Computer Vision，简称 CV。输入的是图像或图像序列，通常来自相机或usb摄像头。输出的是对于图像序列对应的真实世界的理解，比如检测人脸、识别车牌。Digital Image Processing，简称 DIP。输入的是图像，输出的也是图像。Photoshop中对一副图像应用滤镜就是典型的一种图像处理。常见操作有模糊、灰度化、增强对比度等。联系：CG 中也会用到 DIP，现今的三维游戏为了增加表现力都会叠加全屏的后期特效，原理就是 DIP，只是将计算量放在了显卡端。CV 更是大量依赖 DIP 来打杂活，比如对需要识别的照片进行预处理。最后还要提到近年来的热点——增强现实（AR），它既需要 CG，又需要 CV，当然也不会漏掉 DIP。它用 DIP 进行预处理，用 CV 进行跟踪物体的识别与姿态获取，用 CG 进行虚拟三维物体的叠加。简单点说：1 计算机视觉，里面人工智能的东西更多一些，不仅仅是图像处理的知识，还涵盖了人工智能，机器学习等领域知识；2，计算机图形学，主要涉及图形成像及游戏类开发，如opengl等，还有就是视频渲染等；3，图像处理，这个主要针对图像图像的基本处理，如图像检索或则图像识别，压缩，复原等等操作。计算机图形学和数字图像处理是比较老的技术。计算机视觉要迟几十年才提出。计算机图形学和数字图像处理的区别在于图形和图像。图形是矢量的、纯数字式的。图像常常由来自现实世界的信号产生，有时也包括图形。而图像和图形都是数据的简单堆积，图像是像素的叠加，图形则是基本图元的叠加。计算机视觉要从图像中整理出一些信息或统计数据，也就是说要对计算机图像作进一步的分析。计算机图形学的研究成果可以用于产生数字图像处理所需要的素材，计算机视觉需要以数字图像处理作为基础。计算机视觉与数字图像处理的这种关系类似于物理学和数学的关系。另外，如果不是浙江大学的或者中科院计算所的，不建议做计算机图形学这一方向，难度太大（图形比图像虽然表面上只高一维，但实际上工作量大了好多倍；其次，图像，国内外差距目前已经很小，好发重要期刊；图形，除上面两个单位和微软外，国内外差距很大，不好发重要期刊）数字图像处理主要是对已有的图像，比如说可见光的图像、红外图像、雷达成像进行噪声滤除、边缘检测、图像恢复等处理，就像用ps 处理照片一样的。人脸识别啊、指纹识别啊、运动物体跟踪啊，都属于图像处理。去噪有各种滤波算法；其他的有各种时频变化算法，如傅里叶变化，小波变换等，有很多这方面的书籍。图形学主要研究如何生成图形的，像用autoCAD作图，就是图形学中算法的应用。各种动漫软件中图形算法的生成等。原博主链接为：http://www.cnblogs.com/lauzhishuai/p/6293735.html"}
{"content2":"人工智能（AI）—— 为机器赋予人的智能机器学习一种实现人工智能的方法深度学习一种实现机器学习的技术人工智能的知识图谱AI、ML和DL的关系由以上的关系图我们可以看出，总结的说就是机器学习是实现人工智能的一种方法,深度学习是机器学习一个分支机器学习的必要性很多软件无法靠人工编程:自动驾驶、计算机视觉、自然语言处理人类常会犯错(比如紧张、累了、困了),机器不容易犯错关于机器学习的定义有很多，比较晦涩的一个定义是“晦涩”的机器学习定义◆对某类任务T(Task)和性能度量P( Performance)◆通过经验E( Experience)改进后◆在任务T上由性能度量P衡量的性能有所提升简单的机器学习定义是：简单的机器学习定义◆机器学习:用数据来解答问题◆数据对应训练◆解答问题对应推测简单的说，机器学习相当于对人们难以处理的数据，使用机器进行一系列的处理之后，找到一个判定函数或者是模型，当再次遇到这种情况的时候，机器可以通过学习到的模型或者是与之相对应的函数进行一系列的人们难以进行的操作。机器学习的“关键三步”找一系列函数来实现预期的功能:建模问题找一组合理的评价标准,来评估函数的好坏:评价问题快速找到性能最佳的函数:优化问题（比如梯度下降就是这个目的）什么是深度学习？深度学习是机器学习的一个分支，是实现机器学习的一个方法。笼统的说就是，基于深度神经网络的学习研究就是深度学习。关于神经网络与深度神经网络？神经网络一般由输入层，隐含层，输出层三层结构组成，简单的神经网络是只有一个输入层，一个隐含层，一个输出层组成；深度神经网络是由一个输入层，多个隐含层，以及一个输出层组成。深度学习为什么兴起？随着科技的发展，人工智能的兴起，数据量不断的增加，相应的神经网络也在不断地发展。+++++++++++++++++++++++++++++++未+++++++ ++++++++++++++++"}
{"content2":"试想像一下，很多游客同时在不同角度拍摄Eiffel Tower(埃菲尔铁塔)，该如何用数学的方法来描述这一过程呢？首先要解决的问题就是定位，或者说坐标选定的问题，埃菲尔铁塔只有一座，如果按经、纬度来刻画，它的坐标是唯一确定的，但游客显然不关系这一点，他(她)只按自己的喜好选择角度和位置，因此，物体(景物)有物体的坐标系统，相机有相机的坐标系统，即便同一个相机，当调整参数时，在同样的位置、相同的角度，也可能得到不同的图像。为了统一描述，有必要引入世界坐标(或物体坐标)、相机坐标和像平面坐标。世界坐标用UVW记。相机坐标用XYZ记。中学物理告诉我们，物体与像是倒的关系，但作为数学分析，我们采用虚像。像平面用xoy记。而数字图像用(u,v)来表示，不弄混淆像平面和数字图像这两个概念，同一个像通过平移、拉伸等，可以得到不同的数学图像(u,v)。总体来看，就是我们需要用数学的语言来描述这一过程。先看中间部分。红框标注的部分是3D物体到2D像平面的透视投影(如果不明白透视投影的概念，需要补一下高等几何)显然，OP上的任一点的像都是p(x,y)，为了描述这一关系，需要引入齐次坐标。By convention, we specify that given (x’,y’,z’) we can recover the 2D point (x,y) as$ x=\\frac{x^'}{z^'} $     $ y=\\frac{y^'}{z^'}  $Note: (x,y) = (x,y,1) = (2x, 2y, 2) = (k x, ky, k)关于齐次坐标，更详细的介绍可参考高等几何。上述透视投影的过程可以描述为正如开头所言，不同游客会选择不同的位置和角度拍摄同一物体，因此，物体对于相机的关系各不相同，这就是物体到相机坐标变换的问题。上述红框部分描述的是从物体的坐标(称为世界坐标)到相机坐标变换的过程，它是一种刚体运动，可以用平移、旋转来描述。上图表示的是从世界坐标变换到相机坐标：$ P_c=R(P_w - C) $，写成矩阵形式平移是容易理解的，我们先讨论更简单的情形，即假设世界坐标系和相机坐标系的原点重合，则变换只剩下旋转了。旋转矩阵的元素也很容易确定。试想(U,V,W)=(1,0,0)，而它在相机坐标系中的坐标是(X,Y,Z)=(a,b,c)(同一物理点的不同坐标)则有：因此有：由于该旋转是刚体运动，因此它是正交变换，满足$ R^{-1}=R^T $，所以有：不难得出：看一个例子：由于物体的坐标到相机坐标的变换，相对于相机内部参数而言属于外部参数(External Parameters)，往往写作R和T，即总结本小节讲述了如何将3D世界坐标系中的点变换到相机坐标系中，然后经透视投影，变成2D像平面上的点(x,y).下一节：立体视觉基础3——内部参数描述"}
{"content2":"1 知识图谱和人工智能说起知识图谱，可能很多人不太了解，但要说到人工智能，大家就耳熟能详了。人工智能是指机器要像人一样可以思考，具有智慧。现在这个阶段，人工智能研究的人越来越多，在很多行业也实现了部分的人工智能，让机器代替了人进行简单重复性的工作。在这里我们可以把人工智能分为两个层次，一个是感知层次，也就是听觉、视觉、嗅觉、味觉等等，目前人工智能在听觉和视觉方面做的比较好，语音识别，图像识别，研究的人多，也有产品出来。但总起来说，感知层次的人工智能还没有体现人类的独有的智慧，其它动物可能在感知层次比人要好，比如鹰的眼睛，狼的耳朵，豹的速度，熊的力量等等。真正体现人工智能的还是第二个层次，也就是认知层次，能够认识这个客观世界。而认知世界是通过大量的知识积累实现的，小孩子见到狗和猫，见过几次就能分辨出狗和猫，让机器来分辨难度就比较大，当然现在通过大数据训练也在提升。这种认知能力是知识的运用，小孩子见到狗，他就会在潜意识中总结狗的特征，长耳朵，瘦脸，汪汪叫。猫的特征，短耳朵，圆脸，喵喵叫。这些知识会存储在人类的大脑中，作为经验知识，再次碰到类似的动物，人们马上就从记忆中想起该动物的特征，对号入座，马上判断出动物的类型。机器要想具有认知能力，也需要建立一个知识库，然后运用知识库来做一些事，这个知识库就是我们要说的知识图谱。从这个角度说，知识图谱是人工职能的一个重要分支，也是机器具有认知能力的基石，在人工智能领域具有非常重要的地位。2 知识图谱的由来知识图谱（Knowledge graph）首先是由Google提出来的，大家知道Google是做搜索引擎的，知识图谱出现之前，我们使用google、百度进行搜索的时候，搜索的结果是一堆网页，我们会根据搜索结果的网页题目再点击链接，才能看到具体内容，2012年google提出Google Knowldge Graph之后，利用知识图谱技术改善了搜索引擎核心，表现出来的效果就是我们现在使用搜索引擎进行搜索的时候，搜索结果会以一定的组织结构呈现，比如我们搜索比尔盖茨，结果如图所示这样的搜索结果，与知识图谱出现之前的结果有什么区别呢，辛格尔博士对知识图谱的介绍很简短，things，not string，抓住了知识图谱的核心，也点出了知识图谱加入之后搜索发生的变化，以前的搜索，都是将要搜索的内容看作字符串，结果是和字符串进行匹配，将匹配程度高的排在前面，后面按照匹配度依次显示。利用知识图谱之后，将搜索的内容不再看作字符串，而是看作客观世界的事物，也就是一个个的个体。搜索比尔盖茨的时候，搜索引擎不是搜索“比尔盖茨”这个字符串，而是搜索比尔盖茨这个人，围绕比尔盖茨这个人，展示与他相关的人和事，左侧百科会把比尔盖茨的主要情况列举出来，右侧显示比尔盖茨的微软产品和与他类似的人，主要是一些IT行业的创始人。一个搜索结果页面就把和比尔盖茨的基本情况和他的主要关系都列出来了，搜索的人很容易找到自己感兴趣的结果。3 知识图谱是什么知识图谱本质上是一种语义网络，用图的形式描述客观事物，这里的图指的是数据结构中的图，也就是由节点和边组成的，这也是知识图谱（Knowledge Graph）的真实含义。知识图谱中的节点表示概念和实体，概念是抽象出来的事物，实体是具体的事物；边表示事物的关系和属性，事物的内部特征用属性来表示，外部联系用关系来表示。很多时候，人们简化了对知识图谱的描述，将实体和概念统称为实体，将关系和属性统称为关系，这样就可以说知识图谱就是描述实体以及实体之间的关系。实体可以是人，地方，组织机构，概念等等，关系的种类更多，可以是人与人之间的关系，人与组织之间的关系，概念与某个物体之间的关系等等，以下是一个例子。4 知识图谱是怎么组织数据的知识图谱是由实体和实体的关系组成，通过图的形式表现出来，那么实体和实体关系的这些数据在知识图谱中怎么组织呢，这就涉及到三元组的概念，在知识图谱中，节点-边-节点可以看作一条记录，第一个节点看作主语，边看作谓语，第二个节点看作宾语，主谓宾构成一条记录。比如曹操的儿子是曹丕，曹操是主语，儿子是谓语，曹丕是宾语。再比如，曹操的小名是阿瞒，主语是曹操，谓语是小名，宾语是阿瞒。知识图谱就是由这样的一条条三元组构成，围绕着一个主语，可以有很多的关系呈现，随着知识的不断积累，最终会形成一个庞大的知识图谱，知识图谱建设完成后，会包含海量的数据，内涵丰富的知识。5 知识图谱的应用场景知识图谱构建完成之后，主要用在哪些地方，比较典型应用是语义搜索、智能问答、推荐系统等方面。知识图谱是一个具有本体特征的语义网络，可以看成是按照本体模式组织数据的知识库，以知识图谱为基础进行搜索，可以根据查询的内容进行语义搜索，查找需要找的本体或者本体的信息，这种语义搜索功能在google、百度、阿里巴巴等数据量大的公司里得到应用。智能问答，和语义搜索类似，对于提问内容，计算机首先要分析提问问题的语义，然后再将语义转换为查询语句，到知识图谱中查找，将最贴近的答案提供给提问者。推荐系统首先要采集用户的需求，分析用户的以往数据，提取共同特征，然后根据一定的规则，对用户提供推荐的产品。比如淘宝中记录用户经常购买的商品，经常浏览的商品，提取这些商品的共同特征，然后给这个用户打上标签，然后就给用户推荐具有类似特征的商品。知识图谱主要反映的事物之间的关系，对于和关系链条有关的场景，也可以用知识图谱解决，一些应用场景包括反欺诈、不一致性验证、异常分析、客户管理等"}
{"content2":"我的人工智能学习之路-从无到有精进之路 https://blog.csdn.net/sinox2010p1/article/details/80467475如何自学人工智能路径规划（附资源，百分百亲身经验）https://www.jianshu.com/p/2be801b101f2零基础自学人工智能，看这些资料就够了（300G资料免费送）https://www.jianshu.com/p/cb0db5a8a5d6如何自学人工智能路径规划（附资源，百分百亲身经验）https://www.jianshu.com/p/2be801b101f2自学人工智能之数学篇，数学入门并不难 https://www.jianshu.com/p/95585bc0059d人工智能之算法知识与实战篇 https://www.jianshu.com/p/cf0a298d05d1收藏 | 完备的 AI 学习路线，最详细的资源整理非计算机专业小白自学爬虫全指南（附资源）https://www.jianshu.com/p/eccaf43a0ce8莫烦Python、深度学习 https://morvanzhou.github.io/learning-steps/王小草的博客  https://blog.csdn.net/sinat_33761963/article/list/5?t=1&零基础入门深度学习：介绍概念原理性的东西ai大学https://www.aidaxue.com/深度学习：吴恩达深度学习视频：https://mooc.study.163.com/smartSpec/detail/1001319001.htm吴恩达深度学习学习笔记：https://blog.csdn.net/zchang81/article/details/77933537https://blog.csdn.net/koala_tree/article/details/79913655https://blog.csdn.net/julialove102123/article/details/80089018吴恩达机器学习视频：https://study.163.com/course/courseMain.htm?courseId=1004570029莫烦恼深度学习教程：https://morvanzhou.github.io/learning-steps/https://github.com/MorvanZhou/Tensorflow-Tutorial莫烦机器学习课程笔记： https://blog.csdn.net/jiandanjinxin/article/details/73320937莫烦tf教程笔记：https://blog.csdn.net/jiaoyangwm/article/details/79715826.机器学习包含了统计学，微积分，线性代数和概率论知识，关系如下：链接微积分告诉我们怎样优化线性代数让算法适用于大型数据集概率论帮我们预测某个特定输出的可能性统计学则表明是不是我们的目标机器学习基石笔记机器学习技法笔记GitHub 上有哪些有趣的关于 NLP 或者 DL 的项目？Reinforcement Learning: An Introduction:1. 原书籍地址：http://incompleteideas.net/sutton/book/bookdraft2017nov5.pd2. 课程代码地址：https://github.com/ShangtongZhang/reinforcement-learning-an-introduction3. 课程资料地址：http://incompleteideas.net/sutton/book/the-book-2nd.htmlai 学习 https://github.com/apachecn/AiLearning机器学习该怎么入门？https://www.zhihu.com/question/20691338/answer/248678328计算机视觉【AI白身境】计算机视觉都有哪些研究方向机器视觉/CV计算机视觉/图像处理-国际/国内顶尖学术网站小白入门计算机视觉：这是最全的一份CV技术学习之路"}
{"content2":"一. 前言这里记录了我Blog的所有目录结构，有的项是随笔分类项，有的是具体文章，先把目录写好，以后不断往里面装内容，以后方便整理查阅。当然，主页导航栏可以快速索引到下面的具体项目内容。但是，目前有很多板块还没具体内容更新，待后续接触到相关板块知识不断学习不断添加进来。然后每一项的链接后续再不断添加上。分享，使人快乐！希望以后这些内容可以帮助到各位小伙伴，我是一只小小菜鸡，希望在菜鸡摸爬滚打的路上能和大家一起成长一起进步！嘿嘿！二. Blog导航栏目录结构1. 博客园首页点击此目录链接到博客园首页编辑新随笔联系博主订阅博主博客管理2. 我的首页点击此目录链接到Blog主页面3. 全部分类知识框架Linux相关Python基础Python进阶项目实战人工智能阅读人生学习资源娱乐休闲4. 知识框架先看这里Python知识框架计算机组成原理人工智能知识框架哲学基础机器学习知识框架理论基础数据的搜索、挖掘和分析（思）计算机视觉（视）自然语言处理语音识别（听）语音合成（说）智能机器人（行）其他方法Web开发前端后端网络爬虫更多...5. Linux相关计算机组成原理认识LinuxLinux基础Linux进阶更多...6. Python基础认识PythonPython IDE数据类型文件操作迭代器生成器装饰器函数内置函数常用模块更多...7. Python进阶类和对象面向对象深入类和对象网络编程更多...8. 项目实战先看这里个人BlogDjango网站更多...9. 人工智能先看这里哲学基础机器学习数据的搜索、挖掘和分析计算机视觉自然语言处理智能机器人更多...10. 阅读人生学术专著诗词歌赋闲书杂文报刊新闻电影小说更多...11. 学习资源先看这里Python全栈web相关爬虫相关机器/深度学习Java相关一些学习网站python中文学习大本营总有你要的书单Bootstrap教程爬虫学习博客Django2.0官网Flask文档The Flask Mega-TutorialFlask extensions表设计工具REST framework官网数据库更多...12. 娱乐休闲云音乐QQ音乐bilibili双语阅读全景图欣赏微博NBA今日头条内涵段子"}
{"content2":"让计算机具有视觉,科学家与工程师们,作出了近40年的不懈努力:应该说,40年努力的进展是显著的,进展主要有两个方面:已经形成一些计算视觉的基本理论框架,如80年代初形成的以Marr为代表的视觉计算理论(有些学者称之为三维重建框架)和以后出现的基于模型的视觉( Model based vision)、主动视觉( Active Vision)等。现在看来,虽然我们仍然不清楚这些计算理论框架能否最终成为最理想的计算机视觉系统的基础,但有几点几乎是可以肯定的:一是迄今为止提出的各种理论框架虽然有方法论上的差异,有些甚至具有科学哲学思想的差异,但并没有本质上的相互排斥,而且是互补的。二是这些已有的视觉系统理论框架已经可以作为有一定程度视觉功能的实用视觉系统的基础。随着计算机性能价格比的指数增长,以现有视觉系统理论框架为基础的,针对特定任务的实用视觉系统将会广泛应用于现实生活中。三是与人工智能的其他许多领域类似,真正的突破要比当初想像的要困难得多。这里,“真正的突破”是指:当我们将当前的人工智能系统与人相比时,人的智能系统具有更强的通用性、自学习能力、自适应性和对噪声的鲁棒性。在计算机视觉另一方面的重要进展是,提出了大量的计算方法。尤其是90年代以来,为适应不同计算理论框架和为改进计算机视觉系统对噪声的鲁棒性,引进了许多数学方法和与之相对应的计算方法。几乎所有的数学分支,尤其是应用数学分支都要到计算机视觉领域来一显身手,使许多初学者,甚至搞了多年研究的学者都感到困惑。人们不禁要问,难道我们真需要这么多的复杂数学分支和计算方法来解决计算机视觉问题吗?事实上,这确实反映了当前的许多数学工具还不能有效解决“更强的通用性、自学习能力、自适应性和对噪声的鲁棒性”,另一方面,现在的许多数学方法,本质上是相通的。而我们缺少既对这些方法都精通,又对计算机视觉中所面临的实际问题有深入理解的理论工作者来对各种方法加以融会贯通。在上述视觉计算方法的研究中,基于几何的视觉计算方法,在90年代发展到了几乎是完美的程度。本书的作者既是这方面的先驱者,也在本书中作出了很好的总结与系统论述。基于几何的视觉计算方法,之所以引起很大关注是因为：1.计算机视觉的研究目标是使计算机具有通过二维图像认知三维环境信息的能力。这种能力将不仅使机器能感知三维环境中物体的几何信息,包括它的形状、位置、姿态、运动等,而且能对它们进行识别与理解。事实上,80年代形成的Marr的计算理论框架和其他计算理论框架中,绝大部分内容都涉及利用几何方法计算环境中的三维物体的形状、位置、姿态和运动。2.如果读者对欧几里德几何和近几百年来提出的各种几何,如本书中提到的射影几何、仿射几何等有些深入了解的话,应该理解“各种几何的本质是描述几何元素在不同变换群下的不变量”。由此,使用几何方法,不仅可以由二维图像重建( reconstruct)三维物体,还可以描述它们在摄像机变换下的不变量,从而达到识别的目的,也就是说,几何方法,可以贯穿计算视觉理论框架下的所有部分,有人称之为基于几何的计算机视觉。3.90年代以来,计算机视觉界将对应于射影几何、仿射几何、欧几里德几何的射影变换、仿射变换、欧几里德变换系统地引进到视觉计算方法中,三种变换都构成变换群,而且,后者为前者的子群,它们所对应的几何不变量,前者为后者的子集,这些性质比较完美地对应为视觉系统中对物体由粗到细的描述,在一些特定任务的计算机视觉系统中降低了对系统参数了解的要求(如本书中所描述的不需要对摄像机标定的三维重建),一定条件下提高了系统对噪声的鲁棒性,而这些确实是许多实用计算机视觉系统极为需要的品质。本书全面介绍了近10年来发展的基于几何的计算机视觉计算方法及其数学基础。除了上述内容外,其中多摄像机视图几何及其计算方法,值得读者关注,这是因为当前计算机的性能价格比大大提高,使人们有条件在视觉系统中使用更多的摄像机,以利用冗余的信息,来换取系统对噪声的鲁棒性。系统对噪声的鲁棒性一直是实用计算机视觉系统的瓶颈问题,解决该问题的可能的办法是:提高摄像机的分辨率、多摄像机方法和近年来大量引进的统计最优化鲁棒算法(本书许多章节也有描述)。安徽大学的老师们将本书译成中文,是一件很有益的工作,我曾长期讲授计算机视觉课程,深感我国工科大学研究生,缺乏现代几何的有关知识,对近10年来发展的基于几何的计算机视觉计算方法的本质接受较慢本书比较系统地介绍了射影几何,在各章节中也注意介绍有关数学基础,使即使缺少这方面系统知识的工科学生也能接受,应该对我国专门从事十算机视觉研究的读者有较好的参考价值。这本书对我国从事相关数学领域研究的人士也值得一读,计算机视觉涉及的数学,量大面广,是一个典型的数学工作者可有用武之地的领域,但比起其他国家来说,我国的数学家们基本不介入一些有相当实用背景的新兴学科,学科不能交叉,创新从何而来面？来自为知笔记(Wiz)"}
{"content2":"前言感觉是时候系统性的学一下人工智能啦！粗略大纲：1.贝叶斯与随机过程1.贝叶斯分析2.随机过程理论2.机器学习1.监督学习（重点）1.扫描各种算法1.数学推导2.程序清单1.安装2.介绍2.无监督学习3.强化学习3.深度学习（重点）1.公式2.实践（python）1.CNN（卷积网络）——对抗学习2.RNN（循环网络、递归网络）3.应用1.图像2.语言4.复杂系统1.统计力学2.非线性动力学3.与机器学习关联课程目录第 1 讲复杂系统第 2 讲大数据与机器学习第 3 讲人工智能的三个阶段第 4 讲高等数学—元素和极限第 5 讲复杂网络经济学应用第 6 讲机器学习与监督算法第 7 讲阿尔法狗与强化学习算法第 8 讲高等数学—两个重要的极限定理第 9 讲高等数学—导数第 10 讲贝叶斯理论第 11 讲高等数学—泰勒展开第 13 讲高等数学—积分第 14 讲高等数学—正态分布第 15 讲朴素贝叶斯和最大似然估计第 16 讲线性代数—线性空间和线性变换第 17 讲数据科学和统计学（上）第 18 讲线性代数—矩阵、等价类和行列式第 19 讲Python基础课程（上）第 20 讲线性代数—特征值与特征向量第 21 讲监督学习框架第 22 讲Python基础课程（下）第 23 讲PCA、降维方法引入第 24 讲数据科学和统计学（下）第 25 讲Python操作数据库、 Python爬虫第 26 讲线性分类器第 27 讲Python进阶（上）第 28 讲Scikit-Learn第 29 讲熵、逻辑斯蒂回归、SVM引入第 30 讲Python进阶（下）第 31 讲决策树第 32 讲数据呈现基础第 33 讲云计算初步第 34 讲D-Park实战第 35 讲第四范式分享第 36 讲决策树到随机森林第 37 讲数据呈现进阶第 38 讲强化学习（上）第 39 讲强化学习（下）第 40 讲SVM和神经网络引入第 41 讲集成模型总结和GDBT理解及其衍生应用第 42 讲神经网络第 43 讲监督学习－回归第 44 讲监督学习－分类第 45 讲神经网络基础与卷积网络第 46 讲时间序列预测第 47 讲人工智能金融应用第 48 讲计算机视觉深度学习入门目的篇第 49 讲计算机视觉深度学习入门结构篇第 50 讲计算机视觉深度学习入门优化篇第 51 讲计算机视觉深度学习入门数据篇第 52 讲计算机视觉深度学习入门工具篇第 53 讲个性化推荐算法第 54 讲Pig和Spark巩固第 55 讲人工智能与设计第 56 讲神经网络第 57 讲非线性动力学第 58 讲高频交易订单流模型第 59 讲区块链:一场革命第 60 讲统计物理专题（一）第 61 讲统计物理专题（二）61.1神奇公式.mp461.2信息熵（一）61.3信息熵（二）61.4Boltzmann分布61.5配分函数Z第 62 讲复杂网络简介第 63 讲ABM简介及金融市场建模第 64 讲用伊辛模型理解复杂系统第 65 讲金融市场的复杂性第 66 讲广泛出现的幂律分布第 67 讲自然启发算法第 68 讲机器学习的方法第 69 讲模型可视化工程管理第 70 讲Value Iteration Networks第 71 讲非线性动力学系统（上）第 72 讲非线性动力学系统（下）第 73 讲自然语言处理导入第 74 讲复杂网络上的物理传输过程第 75 讲RNN及LSTM第 76 讲漫谈人工智能创业第 77 讲深度学习其他主题第 78 讲课程总结关于学习笔记通过目录也可以看出，前三讲主要是说一些历史背景，行业状态，所以1.2.3讲就不做笔记了，从第4讲开始吧——！"}
{"content2":"课时26 图像分割与注意力模型（上）语义分割：我们有输入图像和固定的几个图像分类，任务是我们想要输入一个图像，然后我们要标记每个像素所属的标签为固定数据类中的一个使用卷积神经，网络为每个小区块进行分类，对在区块的中间打上标签，对图像的全部区块分类完毕，我们就可以得到每个像素所对应的标签，这个操作实际上非常耗时，因为一张图片将会被分割非常多的小块。如果这些神经网络具有相关的结构，通过这个图像金字塔方法的话，这些图像的输出将会有不同的感受野。语义分割的迭代精化我们有一个输入图像，他们被分割成三个通道，我们把输出从卷积神经网络中拿出来，同时把对应的下采样版本的图像拿出来。然后我们再重复这一过程，所以这个过程，其实是有点增加输出有效的感受野，同时也对输入图像有更多的处理。即时分割或者实时检测和分割：我们有一些分类需要去识别，给定一张图像需要输出对应图像的不同分类实例，而分实例我们也需要分割到具体每个像素是属于哪个实例。实例分割我们给他一副输入图像，CNN还要依赖于外部的候选区域，就是计算机视觉中低层次信息得离线处理操作，预测物体在图像中的具体的位置。每个分割候选我们可以抽取出一个bounding box，来提取边界框裁剪输入图像，喂给卷积神经网络去抽取特征，同时并行的执行RCNN，再一次我们拿到相应的块图像，进行裁剪，但这里我们实际上有候选的分割，我们为图像区域背景使用平均颜色做掩码，现在我们拿到这些掩过的输入图像并运行在独立的RCNN中，我们得到两个特征向量，一个是从原始图像框来的，而另一个是去除了背景颜色的图像框，我们把这两个结果联合起来就是我们在CNN做的一样，预测这个图像是属于哪个分类，同时还有一个区域细化的步骤，如果我们想要细化候选区域的话课时27 图像分割与注意力模型（下）语义分析通常使用这种深度卷积的实现方式，而实例分割使用的一整个处理流程更像是物体检测RNN只有一次机会去看输入图像，当他开始工作时，他是把整个图像作为输入的，并且只操作一次，如果他们可以看输入图像多次，这可能会更好，或者他每次曹锁时可以关注到原始的输入图像的不同部分也是更好的。我们有一个输入图像，并把他喂给卷积神经网络，但是代替原来使用全连接层，我们改使用全卷积层来抽取特征，这将给予我们L*D的特征，而不是原来的单特征向量，因为这个是从卷积层来的。你可以想象他是2D的特征向量，在这个2D向量中的每一个，特征都对应了原始输入图像的某一部分。现在我们用这些特征以某种方式来初始化神经网络的隐层，他现在是计算分布的不同位置，在我们的卷积特征图中，我们最终得到这个L维向量，给予我们一个不同位置可能的分布，现在我们拿着这个概率分布并且拿着他去得到这些特征向量在不同点的权重和，所以我们拿到这个特征权重的合集，然后拿着我们的2D向量并对之进行汇总到一个单向量，由于概率分布，给予了网络予能力去聚集图像的不同部分。概率分布，我们每次生成两种类型的概率分布，第一个是D向量，就像我们在普通的图片处理一样；第二个就是也会生成一个在原图中位置的概率分布，他们告诉我们下一次我们要看图片的哪个部分。使用注意力的两个动机首先使用注意力模型，可以得到可解释的结果；另一个动机是可以缓解计算压力，特别是你的输入非常非常大的时候，你可能需要大量计算，并且每次计算都是输入整个图像，如果使用注意力模型，每次只需要关注需要关注的那一块hard attention有做计算保存，并且使用的强学习化。soft attention这个模型的特征图有点固定限制，他只允许以一种固定大小的网络去看。空间变换网络有点像纹理映射。这个网络允许我们在输入图像中访问任意大小的部分以一种可微的方式，我们的网络只需要预测转换坐标数据，这个就可以允许我们以任意大小区域去访问输入图像。空间转换器接受一些输入，这可以是一个原始的输入图像，实际上执行了这个小的本地化网络，将会生成输出作为一个仿射变换坐标的数据。现在这个仿射变换坐标将会被用来计算采样网络，现在我们从本地化网络中预测这个仿射变换，我们映射输出中的每一个像素，输出中每一个像素的坐标把他作为一个输入，这是一个很好的平滑可微的函数。一旦我们有了这个采样网络，我们可以使用双线性插值，去计算输出中每个像素的值。"}
{"content2":"我是一名初学者，如果你发现文中有错误，请留言告诉我，谢谢图像的模糊和平滑是同一个层面的意思，平滑的过程就是一个模糊的过程。而图像的去噪可以通过图像的模糊、平滑来实现（图像去噪还有其他的方法）那么怎么才能对一幅图像进行模糊平滑呢？图像的模糊平滑是对图像矩阵进行平均的过程。相比于图像锐化（微分过程），图像平滑处理是一个积分的过程。图像平滑过程可以通过原图像和一个积分算子进行卷积来实现。下面介绍两种积分算子全1算子最简单的积分算子就是全1算子利用全1算子可以对图像进行模糊平滑操作，有一定的去噪能力。下面是python实例import numpy as np from PIL import Image import matplotlib.pyplot as plt import matplotlib.cm as cm suanzi = np.ones((3,3)) # 创建全1算子 # 打开图像并转化成灰度图像 image = Image.open(\"pika.jpg\").convert(\"L\") image_array = np.array(image) # 原图像与全1算子进行卷积 image2 = signal.convolve2d(image_array,suanzi,mode=\"same\") # 将结果灰度值转化到0-255 image2 = (image2/float(image2.max()))*255 # 显示图像 plt.subplot(2,1,1) plt.imshow(image_array,cmap=cm.gray) plt.axis(\"off\") plt.subplot(2,1,2) plt.imshow(image2,cmap=cm.gray) plt.axis(\"off\") plt.show()运行结果如下图，（为了看到效果，图像经过人工局部放大）上图为原图像，下图为经过模糊处理图像比较两幅图可以看出，全1算子有一定的模糊平滑效果高斯算子利用高斯算子进行模糊处理就是我们常听到的高斯模糊。标准差为σ的高斯分布如下式我们可以通过numpy模块的fromfunction()方法来生成高斯算子。import numpy as np # 乘以100是为了使算子中的数便于观察 # sigma指定高斯算子的标准差 def func(x,y,sigma=1): return 100*(1/(2*np.pi*sigma))*np.exp(-((x-2)**2+(y-2)**2)/(2.0*sigma**2)) # 生成标准差都2的5*5高斯算子 a = np.fromfunction(func,(5,5),sigma=2) print(a) # 结果 [[ 2.92749158 4.25947511 4.82661763 4.25947511 2.92749158] [ 4.25947511 6.19749972 7.02268722 6.19749972 4.25947511] [ 4.82661763 7.02268722 7.95774715 7.02268722 4.82661763] [ 4.25947511 6.19749972 7.02268722 6.19749972 4.25947511] [ 2.92749158 4.25947511 4.82661763 4.25947511 2.92749158]]对上面的5*5高斯算子每个元素进行四舍五入，可以得到下面矩阵看到有些地方直接用上面的矩阵对图像进行高斯模糊，实际上是运用的是标准差为2的高斯近似算子。利用高斯算子对图像进行模糊，程序如下import numpy as np from PIL import Image import matplotlib.pyplot as plt import matplotlib.cm as cm import scipy.signal as signal # 生成高斯算子的函数 def func(x,y,sigma=1): return 100*(1/(2*np.pi*sigma))*np.exp(-((x-2)**2+(y-2)**2)/(2.0*sigma**2)) # 生成标准差为2的5*5高斯算子 suanzi = np.fromfunction(func,(5,5),sigma=2) # 打开图像并转化成灰度图像 image = Image.open(\"pika.jpg\").convert(\"L\") image_array = np.array(image) # 图像与高斯算子进行卷积 image2 = signal.convolve2d(image_array,suanzi,mode=\"same\") # 结果转化到0-255 image2 = (image2/float(image2.max()))*255 # 显示图像 plt.subplot(2,1,1) plt.imshow(image_array,cmap=cm.gray) plt.axis(\"off\") plt.subplot(2,1,2) plt.imshow(image2,cmap=cm.gray) plt.axis(\"off\") plt.show()运行结果如下图，（为了看到效果，图像经过人工局部放大）上图为原图像，下图为经过高斯模糊处理图像对比高斯算子和全1算子，可以看出，高斯算子的模糊想过似乎更好。而且，我们可以通过更改高斯算子的标准差和维数来调整模糊效果一般来说，高斯算子标准差越大，维数越大，图像越模糊。参考列表1.《python计算机视觉编程》2.度娘，感谢那些热爱分享知识的朋友"}
{"content2":"图像配准广泛用于遥感，医学图像，计算机视觉等。通常，它的应用根据图像获取方式主要分为四组：不同视角(多视角分析)——从不同视角获取同一场景图像。其目的是为了获得更大的2D视图或者扫描场景的3D表示。应用示例：遥感-被检区域图像的拼接。计算机视觉-形状恢复(立体形状)。不同时间(多时分析)——从不同时间获取同一场景图像，通常是定期的，可能在不同条件下。其目的是找到和评价连续获得的图像之间场景的改变。应用示例：遥感-全球土地使用监督，景观规划。计算机视觉-安防自动改变检测，运动追踪。医学图像-愈合治疗监督，肿瘤进展监督。不同传感器(多模态分析)——从不同传感器获得同一场景图像。其目的是整合不同来源的信息来获得更复杂更细节的场景表示。应用示例：遥感-不同特征传感器信息融合，如有更好空间分辨率的全色图像，有更好光谱分辨率的彩色/多光谱图像，或与云层和光照无关的雷达图像。研究结果可应用于放射治疗和核医学领域。场景到模型的配准。一个场景的图像和场景的模型配准。模型可以是场景的计算机表示，例如GIS中的地图或数字海拔模型(DEM)，有相似内容的另一个场景，'平均'标本等。其目的是在场景/模型中定位获得的图像，并且/或者比较它们。应用示例：遥感-航空或者卫星数据到地图或者其它GIS层的配准。计算机视觉-目标模板匹配实时图像，自动质检。医学图像-病人图像和数字解剖集的比较，标本分类。由于配准图像的多样性和各种类型的退化，不能设计出适合所有配准任务的通用方法。每种方法不仅要考虑图像之间假定的几何变形类型，还要考虑辐射变形和噪声损坏，所需配准的准确率和应用数据特征。尽管如此，配准方法主要包含以下四步():特征检测。手动或者可能自动检测显著和独特的对象(闭合边界区域，边缘，轮廓，交线，角点等)。为了进一步处理，这些特征可以通过点来表示(重心，线尾，特征点)，这些点称为控制点(CP)。特征匹配。建立场景图像和参考图像特征之间的相关性。使用各种各样的特征描述符，相似性度量，连同特征的空间相关性。转换模型估计。估计将感测图像和参考图像对齐的所谓映射函数的类型和参数。映射函数的参数通过特征相关性计算。图像重采样和转换。使用映射函数转换感测图像。使用合适的插值技术计算非整数坐标的图像值。：图像配准四个步骤：上-特征检测。中-通过不变的描述符来特征匹配。左下-利用建立的相关性估计转换模型。右下-用合适的插值技术进行图像冲采样和转换。每个配准步骤的实现都有自己典型的问题。首先，必须决定哪种特征适合给定的任务。特征应该是有鉴别性的目标，并且在图像上广泛存在以及容易检测。通常，特征的物理可解释性是需要的。参考图像和感测图像上的特征必须有足够相同的元素，甚至在图像没有准确地覆盖同样的场景或者有物体阻挡又或者其它的改变的情况下。检测方法应该有好的定位准确性，并且应该对假定的图像退化不敏感。理想情况下，算法能在场景所有投影情况下检测到同样的特征，不管特定的图像变形。在特征匹配步骤，由不正确的特征检测或者图像退化造成的问题可能就出现了。由于不同的图像条件或者传感器的不同光谱敏感性，物理上的相关特征可能是不相似的。特征描述和相似性衡量的选择必须考虑到这些因素。对于假定的退化特征描述符应该是不变的。同时，它们必须有足够的可鉴别性来区分不同特征以及足够的稳定性，为了不被轻微的特征变化和噪音所影响。在不变量空间的匹配算法应该是鲁棒和高效的。在其它图像上没有相关对应的单一特征应该不影响性能。根据图像获取步骤和期望的图像退化先验知识来选择映射函数的类型。如果没有一个先验信息，模型应该足够灵活和通用来处理所有可能出现的退化。特征检测方法的准确性，特征相关性估计的可靠性，以及可接受的近似误差也需要考虑。此外，必须决定图像中哪种差异通过配准来移除。如果目标是变化检测，则不希望移除正在搜索的差异。这个问题是非常重要的并且很难。最终，合适的重采样技术的选择要取决于要求的插值准确性和计算复杂度来折中。在大多数情况下最近邻或双线性插值就够了；然而，一些应用要求更准确的方法。------参考文献：Image registration methods: a survey (2003)"}
{"content2":"人工智能（Artificial Intelligence），英文缩写为AI。它是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。人工智能从诞生以来，理论和技术日益成熟，应用领域也不断扩大，可以设想，未来人工智能带来的科技产品，将会是人类智慧的“容器”。人工智能可以对人的意识、思维的信息过程的模拟。人工智能不是人的智能，但能像人那样思考、也可能超过人的智能。人工智能是一门极富挑战性的科学，从事这项工作的人必须懂得计算机知识，心理学和哲学。人工智能是包括十分广泛的科学，它由不同的领域组成，如机器学习，计算机视觉等等，总的说来，人工智能研究的一个主要目标是使机器能够胜任一些通常需要人类智能才能完成的复杂工作。但不同的时代、不同的人对这种“复杂工作”的理解是不同的。 2017年12月，人工智能入选“2017年度中国媒体十大流行语”。人工智能现在能实现很多功能，比如语音识别——李开复博士当年做的工作奠定了很多当今识别系统的基础。这里忍不住说一下，Siri本身的技术并没有特别大的亮点，真正nb的是它的模式（语音识别直接与搜索引擎结合在一起，产品体验做得好。而且关键是这样的模式能采集到更多数据，使得系统的精度越来越高）自然语言理解——目前看到的最强的结果应该是IBM Watson。但其实我们现在用的搜索引擎、中文输入法、机器翻译（虽然其实还不怎么work）都和自然语言理解相关。这块儿不是我的专业，请 @段维斯 同学补充。数据挖掘——随着近年数据量的疯狂增长，数据挖掘也有了长足进步。最具有代表性的是前几年著名的Netflix challenge（Netflix公司公开了自己的用户评分数据，让研究者根据这些数据对用户没看过的电影预测评分，谁先比现有系统好10%，谁就能赢100万美元）最后这一比赛成绩较好的队伍，并非是单一的某个特别nb的算法能给出精确的结果，而是把大量刻画了不同方面的模型混合在一起，进行最终的预测。计算机视觉——目前越来越多的领域跟视觉有关。大家可能一开始想到的都是自动驾驶。虽然大家都在说googleX的无人车， 但实际上现在无论是商业上，还是技术整合上最成功的算法是Mobile Eye的辅助驾驶系统。这个公司也是目前computer vision领域最挣钱的公司。从实现新功能方面说，视觉的发展的趋势主要有两方面，A) 集成更多的模块，从问题的各种不同方面，解决同一个问题（比如Mobile Eye，就同时使用了数十种方法，放到一起最终作出决策） B) 使用新的信息，解决一个原来很难的问题。这方面最好的例子是M$的Kinect，这个产品最让人拍案叫绝的就是那个红外pattern投影仪。还有其他的功能，比如：机器视觉，指纹识别，人脸识别，视网膜识别，虹膜识别，掌纹识别，专家系统，自动规划，智能搜索，定理证明，博弈，自动程序设计，智能控制，机器人学，语言和图像理解，遗传编程等。人工智能的理论基础：这里说的是数学理论，是为实现功能解决问题而存在的。与人类的智能的联系在下一节说。从这个角度，我们已经有了很多强有力的数学工具，从高斯时代的最小二乘法，到现在比较火的凸优化，其实我们解决绝大多数智能问题的套路，都可以从某种意义上转换成一个优化问题。真正限制我们解这个优化问题的困难有以下三个：计算复杂度——能保证完美解的算法大都是NP-hard的。如何能让一个系统在当前的硬件下“跑起来”，就需要在很多细节取巧，这是很多learning paper的核心冲突。模型假设——所有模型都要基于一些假设，比如说，无人车会假设周围的汽车加速度有一个上限（至少不会瞬间移动吧，否则怎么闪避）绝大多数假设都不能保证绝对正确，我们只是制定那些在大多数时候合理的假设，然后基于这些假设建模（比如，在语音识别里，我们是否要假设存在背景噪声呢？如果有背景噪声，这个噪声应该符合什么特点呢？这时候无论你怎么定标准，总能找出“反例”）数据基础——任何学习过程都需要数据的支持，无论是人类学说话学写字，还是计算机学习汽车驾驶。但是就数据采集本身来说，成功的案例并不多。大概这个世界上最强的数据采集就是google了吧。每次你搜索一个关键词，然后点进去，google就自动记录了你的行为，然后以此数据来训练自己的算法。"}
{"content2":"图像处理，针对图像本身进行一些处理，这里可以是工业、医疗、娱乐、多媒体、广告等多个行业的，如常见的Photoshop也是图像处理软件，使用此软件从事相关工作的人也是图像处理人员。其它行业也有类似的效果，即将原始图像，通过一些算法、技术、手段等，转换成用户自己认为理想的图像，即把图像给处理了。计算机视觉，或者说是机器视觉（计算机视觉与机器视觉略有不同，不过更相近），则类似于人类的视觉效果，只不过是用到了机器、计算机上。这其中，大部分的机器视觉，都包含了图像处理的过程，只有图像处理过后，才能找到图像中需要的特征，从而更进一步的执行其它的指令动作，如机械手臂的运动、机台的移动等，这些应用在大学里主要表现在机器人上，如机器人踢球、下棋等，在工业上，则主要应用于工业机器人，完成自动生产、装配、检测等工作，富士康就有大量的机器人，在农业上，则表现在一些自动收割机，如棉花收割，自动分类机器。当然也有一些机器视觉是不需要图像处理的，如经过相机镜头等直接连接到显示器上观察的，结果好坏是由人来判断的，这时图像处理的过程是由人自己完成的，而不是计算机。还有一些图像传感器有固定的特性，如颜色传感器，那样只会有信号出来即可，也是没有图像处理的。计算机视觉，一定是包含计算机的，而机器视觉，则不一定需要计算机，可以是智能相机，也可以是图像传感器，当然也可以使用计算机完成。"}
{"content2":"计算机视觉库 OpenCVOpenCV是Intel®开源计算机视觉库。它由一系列 C 函数和少量 C++ 类构成，实现了图像处理和计算机视觉方面的很多通用算法。 OpenCV 拥有包括 300 多个C函数的跨平台的中、高层 API。它不依赖于其它的外部库——尽管也可以使用某些外部库。 OpenCV 对非商业...人脸识别 faceservice.cgifaceservice.cgi 是一个用来进行人脸识别的 CGI 程序， 你可以通过上传图像，然后该程序即告诉你人脸的大概坐标位置。faceservice是采用 OpenCV 库进行开发的。OpenCV的.NET版 OpenCVDotNetOpenCVDotNet 是一个 .NET 对 OpenCV 包的封装。人脸检测算法 jViolajonesjViolajones是人脸检测算法Viola-Jones的一个Java实现，并能够加载OpenCV XML文件。 示例代码：http://www.oschina.net/code/snippet_12_2033Java视觉处理库 JavaCVJavaCV 提供了在计算机视觉领域的封装库，包括：OpenCV、ARToolKitPlus、libdc1394 2.x 、PGR FlyCapture和FFmpeg。此外，该工具可以很容易地使用Java平台的功能。 JavaCV还带有硬件加速的全屏幕图像显示（CanvasFrame），易于在多个内核中执行并行代码（并...运动检测程序 QMotionQMotion 是一个采用 OpenCV 开发的运动检测程序，基于 QT。视频监控系统 OpenVSSOpenVSS - 开放平台的视频监控系统 - 是一个系统级别的视频监控软件视频分析框架（VAF）的视频分析与检索和播放服务，记录和索引技术。它被设计成插件式的支持多摄像头平台，多分析仪模块（OpenCV的集成），以及多核心架构。手势识别 hand-gesture-detection手势识别，用OpenCV实现人脸检测识别 mcvai-tracking提供人脸检测、识别与检测特定人脸的功能，示例代码 cvReleaseImage( &gray ); cvReleaseMemStorage(&storage); cvReleaseHaarClassifierCascade(&cascade);...人脸检测与跟踪库 asmlibraryActive Shape Model Library (ASMLibrary©) SDK, 用OpenCV开发，用于人脸检测与跟踪。Lua视觉开发库 libecvECV 是 lua 的计算机视觉开发库(目前只提供linux支持)OpenCV的.Net封装 OpenCVSharpOpenCVSharp 是一个OpenCV的.Net wrapper，应用最新的OpenCV库开发，使用习惯比EmguCV更接近原始的OpenCV，有详细的使用样例供参考。3D视觉库 fvision2010基于OpenCV构建的图像处理和3D视觉库。 示例代码： ImageSequenceReaderFactory factory; ImageSequenceReader* reader = factory.pathRegex(\"c:/a/im_%03d.jpg\", 0, 20); //ImageSequenceReader* reader = factory.avi(\"a.avi\"); if (reader == NULL) { ...基于QT的计算机视觉库 QVision基于 QT 的面向对象的多平台计算机视觉库。可以方便的创建图形化应用程序，算法库主要从 OpenCV，GSL，CGAL，IPP，Octave 等高性能库借鉴而来。图像特征提取 cvBlobcvBlob 是计算机视觉应用中在二值图像里寻找连通域的库.能够执行连通域分析与特征提取.实时图像/视频处理滤波开发包 GShowGShow is a real-time image/video processing filter development kit. It successfully integrates DirectX11 with DirectShow framework. So it has the following features: GShow 是实时 图像/视频 处理滤波开发包，集成DiretX11。...视频捕获 API VideoManVideoMan 提供一组视频捕获 API 。支持多种视频流同时输入（视频传输线、USB摄像头和视频文件等）。能利用 OpenGL 对输入进行处理，方便的与 OpenCV，CUDA 等集成开发计算机视觉系统。开放模式识别项目 OpenPRPattern Recognition project（开放模式识别项目），致力于开发出一套包含图像处理、计算机视觉、自然语言处理、模式识别、机器学习和相关领域算法的函数库。OpenCV的Python封装 pyopencvOpenCV的Python封装，主要特性包括： 提供与OpenCV 2.x中最新的C++接口极为相似的Python接口，并且包括C++中不包括的C接口 提供对OpenCV 2.x中所有主要部件的绑定：CxCORE (almost complete), CxFLANN (complete), Cv (complete), CvAux (C++ part almost...视觉快速开发平台 qcv计算机视觉快速开发平台，提供测试框架，使开发者可以专注于算法研究。图像捕获 libv4l2cam对函数库v412的封装，从网络摄像头等硬件获得图像数据，支持YUYV裸数据输出和BGR24的OpenCV  IplImage输出计算机视觉算法 OpenVIDIAOpenVIDIA projects implement computer vision algorithms running on on graphics hardware such as single or multiple graphics processing units(GPUs) using OpenGL, Cg and CUDA-C. Some samples will soon support OpenCL and Direct Compute API&apos;...高斯模型点集配准算法 gmmreg实现了基于混合高斯模型的点集配准算法，该算法描述在论文： A Robust Algorithm for Point Set Registration Using Mixture of Gaussians, Bing Jian and Baba C. Vemuri. ，实现了C++/Matlab/Python接口...模式识别和视觉库 RAVLRecognition And Vision Library (RAVL) 是一个通用 C++ 库，包含计算机视觉、模式识别等模块。图像处理和计算机视觉常用算法库 LTI-LibLTI-Lib 是一个包含图像处理和计算机视觉常用算法和数据结构的面向对象库，提供 Windows 下的 VC 版本和 Linux 下的 gcc 版本，主要包含以下几方面内容： 1、线性代数 2、聚类分析 3、图像处理 4、可视化和绘图工具OpenCV优化 opencv-dsp-acceleration优化了OpenCV库在DSP上的速度。C++计算机视觉库 Integrating Vision ToolkitIntegrating Vision Toolkit (IVT) 是一个强大而迅速的C++计算机视觉库，拥有易用的接口和面向对象的架构，并且含有自己的一套跨平台GUI组件，另外可以选择集成OpenCV计算机视觉和机器人技术的工具包 EGTThe Epipolar Geometry Toolbox (EGT) is a toolbox designed for Matlab (by Mathworks Inc.). EGT provides a wide set of functions to approach computer vision and robotics problems with single and multiple views, and with different vision se...OpenCV的扩展库 ImageNetsImageNets 是对OpenCV 的扩展，提供对机器人视觉算法方面友好的支持，使用Nokia的QT编写界面。libvideogfx视频处理、计算机视觉和计算机图形学的快速开发库。Matlab计算机视觉包 mVisionMatlab 的计算机视觉包，包含用于观察结果的 GUI 组件，貌似也停止开发了，拿来做学习用挺不错的。Scilab的计算机视觉库 SIPSIP 是 Scilab（一种免费的类Matlab编程环境）的图像处理和计算机视觉库。SIP 可以读写 JPEG/PNG/BMP 格式的图片。具备图像滤波、分割、边缘检测、形态学处理和形状分析等功能。STAIR Vision LibrarySTAIR Vision Library (SVL) 最初是为支持斯坦福智能机器人设计的，提供对计算机视觉、机器学习和概率统计模型的支持。"}
{"content2":"我们的大脑就像计算机，或者计算机像我们的大脑。总之两者，在某些地方都非常相似。例如，计算机接收输入，通过CPU在RAM中，高速运算，得到结果。也可以通过IO读取外部大容量存储设备的内容。我们的大脑也一样，通过视觉、听觉、触觉等接收现实世界中的输入，在大脑的神经中枢中分析、运算，与某一些记忆存储区交互，得到现实世界的感知，对行为作为判断。从上面可以看出，一个人要想变得聪明，只要尽量让自己的视觉、听觉等更加灵敏，或者加快大脑运算速度，提高联想能力、搜索能力，与已记忆的知识迅速交互的能力，或者尽可能记忆更多的知识，如果一个人这几个方面的能力都很强，他肯定是一个大神级别的人物。如此看来，让大脑保持高效工作，成为一个高效的人，聪明的人，或许并不困难。虽然，像计算机一样，有不同的配置，有的计算机是单核，有的是多核；有的是512M内存，有的是4G内存；有的是32G硬盘，有的是258G硬盘，配置不同，计算机性能也不同。这是计算机的硬件。人的大脑可能也有着天生的差别，或者生来人与人就有着智商的差别，这就是所谓的先天因素。这是大脑的硬件。很遗憾，这部分先天因素我们可能无法左右。但是，除去先天因素之后，我们是不是可以通过改变后天条件，来达到让我们大脑更高效的目的呢？当然可以。从计算机的高效利用上，我觉得很容易就可以找到答案。那就是软件。在计算机中，我们完全可以优化算法，优化程序流程，通过优秀的编程，让低配的计算机表现出高配的性能。在人脑中，也如此，当我们硬件配置不够的时候，就要想优化我们的软件配置，也就是如何更高效地利用大脑。一个行之有效的方法就是，单线程工作，一次只干一件事，减少任务切换时的资源耗费，聚焦有效输入，过滤无效输入。就是说，当我们做某件事之前，先让大脑忘记一切，保持空白，就像计算机清空内存一样。然后关闭无效的感觉接收接口，只需要视觉输入的时候，就关闭听觉和触觉，尽可能得把精力聚焦到视觉上，在视觉上，再把无效的视觉输入关闭，只保留有效的视觉输入，比如关闭没用的网页和软件。之后，就是专注地做一件事了，怎么做事也是有办法论的，比如我们最好先要知道做这件事的目的是什么，预期结果是什么，就像计算机的测试用例一样，先断言。然后更主要的是注重思考，分析做这件事需要什么资源，需要什么接口，什么资源可以完成它，这个时候完全可以不必过于关心细节。当我们对做这件事，有了整体架构之后，就说明我们完成了一次有效的思考。之后的具体实现，可能就更加按部就班，需要的只是时间问题了。上面的过程就是大脑高效工作的一次过程，我们要经常性地这样去处理事情，养成习惯之后，我们就能成为一个高效的人，变得更加聪明，在先天优势不足的情况下，通过后天努力，提升自己的竞争力，甚至超越那些先天优势明显的人。程序员肯定都知道，一段垃圾的代码，甚至跑在最高端的计算机上，也可能导致死机。而一段优秀简洁的代码，即便跑在最原始的计算机上，也依旧运行流畅。我感觉，人的大脑与计算机从本质上来说肯定是不同的，计算机只是看起来像人脑一样，所谓人工智能的最终幻想，让计算机完全模拟人脑工作，只能是人类幻想。计算机的硬件配置是死的，一旦形成了，不会自动升级，除非更换部件。而人的大脑是活的，人脑很有可能会自动更新升级，当你更好的利用它时，它就会变得更强大，神经元的组合可能是无穷多样的，这取决与你怎么使用它。也就是所谓的先天配置，我觉得是有可能改变的。随着人工智能发展，计算机的发展，计算机变得越来越像人，在很多方面确实可能会替代人类。而替代人类的这些方面，必然是人类怎么做也很难抵得过计算机，因此人类才会被淘汰。比如，存储，很明显的，计算机可以不断地扩展硬盘，存储无限多的知识。而我们的大脑显然不可能记忆到所有知识。计算机的搜索能力越来越强大，在海量数据中搜索想要的内容，也越来越快。这个时候，或许我们就不必费力自己存储知识。至少我们必要存储那么多长时记忆。我应该放弃爱记忆这个习惯，我的大脑只需要一块高速闪存，存储一些基本常识和操作系统，在处理问题的时候，可以快速启动操作系统，在高速内存中处理分析指令，得到结果。总结我应该怎么做？聚焦有效输入，过滤无效输入，不断锻炼，提升自己的感知能力深度思考，培养大脑的创造性思维、发散思维、联想思维，培养艺术细胞，单线程工作记忆科学世界的基本原理和本质概念，不要去耗费精力记忆人为规定的规则，语法，公式等人脑应该用来做创造性地思考，而非重复的机械式的思考，甚至不思考。实际上，重复机械式的思考根本不叫思考，根本就没用到人脑中具有发散功能地那块区域，而这块区域可能无法用计算机的部件来形容，因为目前的任何计算机都不具备这个功能。重复机械式的劳动就像是计算机寄存器中的一系列指令，在cpu的时钟内，一条一条地执行而已。"}
{"content2":"什么是计算机视觉计算机视觉，就是研究如何让计算机“看”的一门学问。也即通过分析图像，获取图像的具体信息( 如两点距离 )，甚至隐含信息( 如某个物体的运动速度 )。人和计算机的视觉类比下面通过对比人与计算机视觉的产生过程，加深对计算机视觉这一概念的认识。1. 物体投影于人眼对于计算机视觉，相当于摄像机捕获到物体的运动图像/视频( 实质上是一组组矩阵 )PS：以下 2 和 3 所说的信息专指视觉信息，也即“看到的”客观信息。2. 人从自己看到的事物中获取其关注的部分信息对于计算机视觉，相当于从获取的图像/视频中提取需要分析的数据。3. 人脑对这些信息进行分析对于计算机视觉，相当于用某些视觉算法( OpenCV中就封装好了许多相关的算法 )处理这些数据。4. 人基于自己的知识库以及3中处理后的数据进行决策对于计算机视觉，相当于结合3中的分析结果( 视觉信息 )和其他信息进行决策。"}
{"content2":"原文地址：http://blog.csdn.net/bigazrael/article/details/51016025通常会听到尺度变化等这类词语，看到的也总是一堆的数学公式，有时候真的不知道这到底有啥用，有啥意义，没有弄懂这些意义，当然就更不可能的理解，不可能去掌握应用它了，现在我才理解，小波变化其实也是一种尺度变化。今天我看到一篇南航数学系写的关于尺度空间解释的文章，感觉很通俗易懂，我们不从数学上来推倒什么是尺度空间，只是从生活常识方面来解释尺度空间的意义，意义懂了，数学方面自然就好理解了。好。首先我们提出一个问题，请看下面的图片，让我们我们人为的判断下所显示的左右图片中哪个的角尖锐？接下来我们慢慢的分析，在这其中我们会发现很有意思的事情。分析：(a)中，显然右边的角更尖锐，这是因为同左边的角相比其角度值较小。(b)中，也是右边的角更尖锐，这是因为同左边的角相比其曲率值较大。(c)中的第三组角，问题的答案则要困难得多。左边的角一方面具有较小的角度值（因此更尖锐），另一方面又具有较小的曲率值（因此更圆钝）。右边的角情形刚好相反，一方面因为具有较大的角度值更圆钝，另一方面又因为具有较大的曲率值显得更尖锐。事实上，本问题对于第一组角和第二组角来说是纯粹的数学问题，依据数学上的基本概念（即角度、曲率） 便可以做出判断。而第三组中两个角之间的比较已经不再是纯粹的数学问题，在数学上没有明确的答案。有意思吧。确切地说，这是一个尺度空间中的视觉问题，其答案取决于问题所在的“尺度”而不是某个数学指标。这里，“尺度”可以被直观地理解为观察窗口的大小。(c)中，我们观察两个角的窗口大小都是12*16。在(d)中，调整了观察窗口，其大小变成120*160（假设所比较的两个角都具有无限长的边）。在这个较大的尺度下，问题的答案变得非常明朗：左边的角更加尖锐。在(e)中，观察窗口的大小变更为6*8。在这个较小的尺度下，问题的答案发生了有趣的变化：此时右边的角更加尖锐。此例子的结果阐述了“尺度”对于解决视觉问题的重要性，即一个视觉问题的答案往往会依赖于其所在的尺度。在生活中这样的例子也比比皆是，比如要观察一棵树，所选择的尺度应该是“米”级；如果观察树上的树叶，所选择的尺度则应该是“厘米”级。一般来说，摄像设备所能呈现的画面分辨率是固定的。要以同样的分辨率分别观察树和树叶，我们需要调整摄像设备的摄像位置。因此，视觉问题中的“尺度”概念也可以被形象地理解为摄像设备与被观察物体之间的距离：较远的距离对应较大的尺度，较近的距离对应较小的尺度。接下来我们看看第二个例子吧，看看图二中显示的图片那些事角点？(a)呈现了一片雪花的形状轮廓，要求我们找出该形状上的角点。在很多计算机视觉任务中，角点都有着重要的作用。数学上，角点一般是指大曲率点或曲率无穷大点。在(b)中，雪花形状上所有曲率无穷大点都被确认为角点，一共有192个，如圆圈所标记。这个答案在数学上无疑是正确的、完美的令人惊奇的，但它对于完成一个视觉任务（比如理解和分析这个形状）来说并没有多大的意义。如果我们仅选择(c)中所标记出的48个点作为角点，感觉上要更好点。作为(b)中所标记的192个角点中的一部分，这48个角点在理解和分析雪花形状的结构时要比其余的角点具有更高的重要性。实际上，按照这一思路，我们不难发现在这48个角点中又有12个角点其重要性还要更高一些，如(d)中所标记。同例1 一样，本例中问题的答案依赖于问题所在的尺度。当我们非常靠近雪花形状观察它时（即在较小的尺度下），能够看清楚所有的细节，却不容易感知其整体轮廓，从而倾向于不加区分地选取(b)中所标记的192 个点作为角点。反过来，当我们从一个很远的距离观察雪花形状时（即在较大的尺度下），虽然轮廓的细节已经模糊不清，但却能够一眼看出其整体结构，从而倾向于选取(d) 中所标记的12 个点作为角点。此外，(c) 中所标记的角点对于理解雪花形状也很有帮助。事实上，如果我们不是保守地将自己固定在某个尺度下来观察物体，便能够获得充足的视觉信息。比如说(b)-2(d) 所呈现的三组角点已经很好地向我们展示了雪花形状的三个结构层次。这一效果是其中的任意一组角点都无法实现的。现实生活中视觉问题的复杂性也往往需要我们做到这一点：当我们去参观某处文化遗迹时，远远地就已经开始观察建筑物的外形，然后较近距离时开始注意到门窗、台阶、梁柱的建筑风格，最后会凑上前去细看门窗上的图案、石碑上的碑文等。当一部机器人也能够自主地做到这一点时，说明它已经具备了更高的人工智能。我们对尺度空间技术的研究也正是朝着这个方向努力。概括地说，“尺度空间”的概念就是在多个尺度下观察目标，然后加以综合的分析和理解。这里用的是图像来解释尺度，当然，对于抽象的信号，理解还是一样的，不过到时候我们看的工具不是我们人眼或者是摄像机从外表区分了，这时候我们用的工具也可能是时域的分析法，也可能是频率域的傅里叶变化等分析法，它是我们进一步发现我们感兴趣事物的工具。最后贴点数学公式吧，不然不完美：线性尺度空间技术：其实现途径是将一维信号（如曲线的曲率函数）或二维信号（如图象）与高斯函数g(x,t)=1/(sqrt(t*pi))*exp(-x^2/(4t))作卷积运算。以一维信号f(x)为例,f(x,t)=f(x)*g(x,t)=integration(f(u)g(x-u)du)其中t被当做尺度参数，其中尺度参数值t由小到大逐渐增加。在演化过程中，信号上的特征被提取并加以分析， 形成信号的尺度空间。"}
{"content2":"昨日，谷歌在上海举办了一年一度的Google中国开发者大会。在本届大会上，谷歌云首席科学家李飞飞宣布了一个重磅消息，即在北京将成立谷歌AI中国中心。对于这个即将成立的AI中心谷歌寄予厚望，希望与中国本土AI研发力量合作共同致力于人工智能领域的研究。该中心由李飞飞和 Google Cloud 研发负责人李佳博士共同领导。李飞飞将会负责中心的研究工作，也会统筹 Google Cloud AI, Google Brain 以及中国本土团队的工作。这是谷歌在亚洲的第一个AI中心。根据李飞飞教授的介绍，Google AI 中国中心的团队由目前全世界最优秀的 AI 研究员组成。除了发表自己的研究成果，Google AI 中国中心的重心也将向培养本土 AI 科研人才倾斜，开放与本土企业的合作，并且为更广大的学生及研究人员提供高质量 AI 及机器学习的教育支持。李飞飞表示：“我们很珍惜这次 Google 与中国顶尖 AI 人才合作的机会，这些人才，势必也是全球顶尖的 AI 人才。千里之行，始于足下，我们由衷希望，这将成为谷歌 AI 中国中心长期发展的第一步。在AI的世界里，中国早已觉醒，成为世界的领导者之一。我目睹了中国在AI基础研究、创业、发展等方面走到世界前列。在ImageNet图像识别大赛、Kaggle的AI编程竞技平台上以及基础科研等方面，都有巨大的成就。今天，我和团队回到中国，希望开始一段长久、真诚的工作，创造未来。这个中心将与我们在世界各地，包括纽约、多伦多、伦敦和苏黎世在内的AI研究小组一起，共同让人工智能更好地服务于全人类。‘’近年来中国投资资本纷纷涌向人工智能领域。截止目前，中国共拥有592家人工智能企业，在智能语音、自动驾驶、计算机视觉和图像处理领域有不错的发展。可以预见，未来AI行业将会为人类社会发展带来颠覆性的影响。未来在人工智能普及之后，将会有服务机器人、医疗机器人、农用背包机器人、无人农用机等自动化生产力。AI也将大量应用在翻译、助理、客服、会计、司机、家政、咨询等岗位。现在的中国市场，的确是人工智能的一片热土。自从将人工智能作为下一步的核心战略之后，百度又走起了技术流；阿里巴巴和腾讯虽然各有更加具有潜力的现金奶牛业务，但仍不愿减少对人工智能领域的投入；科大讯飞等本土初创企业的兴起，也仅仅是中国繁荣的人工智能市场的一个小小缩影。而且中国政府已经把发展人工智能作为国家的首要任务。人工智能发展形势一片大好，巨头们纷纷布局只为抢先机，而在AI这个领域中，中国互联网公司一点都不逊色美国，甚至要走的更超前一些。据中国网络招聘企业的最新报告，随着中国人工智能快速发展，AI创业潮涌现，今年第三季中国AI相关人才需求量是去年第一季的近三倍。人工智能的技术门槛很高，且难以通过短时间的学习掌握，具备学术知识以及实际操作经验的技术大牛是企业争抢的目标。知识型、技术型人才可复制性差，可替代性差，因此企业在追逐人才时通常处于被动状态，更出现高薪难求的状况。目前，全世界约有30万人掌握从无人驾驶汽车到家用机器人所需要的先进计算机科学技术，腾讯研究院在一份报告中说，全球市场对AI专家的需求在百万量级。AI的兴起被称为第四次工业革命，强大的超级计算机通过大量数据来辨识模式并自主学习，这让人们产生了失业的担忧。麦肯锡咨询公司预测，到2030年，全球可能有8亿个工作岗位随自动化的实现而消失。过去三百年轰轰烈烈的工业运动，体力劳动基本已经被取消;而近年来火爆的人工智能，则把白领的工作也要取代。人工智能在围棋方面超过人类，甚至可以写新闻，连记者这个行当也被取消。虽然人工智能已经存在很多年了，但我们今天所知道的人工智能仍处于起步阶段。围绕人工智能及其各种应用，从自动驾驶汽车到虚拟个人助理，以及其他许多需要人工智能的任务，都需要很长时间的部署与改进。人工智能还有很长的路要走。人工智能一门极富挑战性的科学，从事这项工作的人必须懂得计算机知识，心理学和哲学。人工智能是包括十分广泛的科学，它由不同的领域组成，如机器学习，计算机视觉等等，总的说来，人工智能研究的一个主要目标是使机器能够胜任一些通常需要人类智能才能完成的复杂工作。但不同的时代、不同的人对这种“复杂工作”的理解是不同的。2017年12月，人工智能入选“2017年度中国媒体十大流行语”。人工智能具有巨大意义，同时也有着巨大的商机，人类面对庞大的利益诱惑不可能停止人工智能的研究，所以，人工智能真正会走到哪步，就让时间来证实。文末福利：福利一：前端，Java，产品经理，微信小程序，Python等资源合集大放送：https://www.jianshu.com/p/e8197d4d9880福利二：微信小程序入门与实战全套详细视频教程image原文作者：祈澈姑娘原文链接：https://www.jianshu.com/u/05f416aefbe1创作不易，转载请告知90后前端妹子，爱编程，爱运营，爱折腾。坚持总结工作中遇到的技术问题，坚持记录工作中所所思所见，欢迎大家一起探讨交流。"}
{"content2":"今日CS.CV计算机视觉论文速览Thu, 28 Feb 2019Totally 31 papersDaily Computer Vision Papers[1] Title: Efficient Video Classification Using Fewer FramesAuthors:Shweta Bhardwaj, Mukundhan Srinivasan, Mitesh M. Khapra[2] Title: Recurrent MVSNet for High-resolution Multi-view Stereo Depth InferenceAuthors:Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang, Long Quan[3] Title: Attributes-aided Part Detection and Refinement for Person Re-identificationAuthors:Shuzhao Li, Huimin Yu, Wei Huang, Jing Zhang[4] Title: Fractional spectral graph wavelets and their applicationsAuthors:Jiasong Wu, Fuzhi Wu, Qihan Yang, Youyong Kong, Xilin Liu, Yan Zhang, Lotfi Senhadji, Huazhong Shu[5] Title: Generative Collaborative Networks for Single Image Super-ResolutionAuthors:Mohamed El Amine Seddik, Mohamed Tamaazousti, John Lin[6] Title: Flash Lightens Gray PixelsAuthors:Yanlin Qian, Song Yan, Joni-Kristian Kämäräinen, Jiri Matas[7] Title: Modulated binary cliquenetAuthors:Jinpeng Xia, Jiasong Wu, Youyong Kong, Pinzheng Zhang, Lotfi Senhadji, Huazhong Shu[8] Title: Fix Your Features: Stationary and Maximally Discriminative Embeddings using Regular Polytope (Fixed Classifier) NetworksAuthors:Federico Pernici, Matteo Bruni, Claudio Baecchi, Alberto Del Bimbo[9] Title: StyleRemix: An Interpretable Representation for Neural Image Style TransferAuthors:Hongmin Xu, Qiang Li, Wenbo Zhang, Wen Zheng[10] Title: Single-frame Regularization for Temporally Stable CNNsAuthors:Gabriel Eilertsen, Rafał K. Mantiuk, Jonas Unger[11] Title: FickleNet: Weakly and Semi-supervised Semantic Image Segmentation\\using Stochastic InferenceAuthors:Jungbeom Lee, Eunji Kim, Sungmin Lee, Jangho Lee, Sungroh Yoon[12] Title: Equi-normalization of Neural NetworksAuthors:Pierre Stock, Benjamin Graham, Rémi Gribonval, Hervé Jégou[13] Title: Cluster Regularized Quantization for Deep Networks CompressionAuthors:Yiming Hu, Jianquan Li, Xianlei Long, Shenhua Hu, Jiagang Zhu, Xingang Wang, Qingyi Gu[14] Title: Multi-loss-aware Channel Pruning of Deep NetworksAuthors:Yiming Hu, Siyang Sun, Jianquan Li, Jiagang Zhu, Xingang Wang, Qingyi Gu[15] Title: The Importance of Metric Learning for Robotic Vision: Open Set Recognition and Active LearningAuthors:Benjamin J. Meyer, Tom Drummond[16] Title: Spatio-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video CaptioningAuthors:Nayyer Aafaq, Naveed Akhtar, Wei Liu, Syed Zulqarnain Gilani, Ajmal Mian[17] Title: Aurora Guard: Real-Time Face Anti-Spoofing via Light ReflectionAuthors:Yao Liu, Ying Tai, Jilin Li, Shouhong Ding, Chengjie Wang, Feiyue Huang, Dongyang Li, Wenshuai Qi, Rongrong Ji[18] Title: A Simple Prior-Free Method for Non-rigid Structure-from-Motion Factorization : RevisitedAuthors:Suryansh Kumar[19] Title: Zero-shot Learning of 3D Point Cloud ObjectsAuthors:Ali Cheraghian, Shafin Rahman, Lars Petersson[20] Title: A Dictionary-Based Generalization of Robust PCA Part II: Applications to Hyperspectral DemixingAuthors:Sirisha Rambhatla, Xingguo Li, Jineng Ren, Jarvis Haupt[21] Title: Deep MR Fingerprinting with total-variation and low-rank subspace priorsAuthors:Mohammad Golbabaee, Carolin M. Pirkl, Marion I. Menzel, Guido Buonincontri, Pedro A. Gómez[22] Title: Learning Latent Scene-Graph Representations for Referring RelationshipsAuthors:Moshiko Raboh, Roei Herzig, Gal Chechik, Jonathan Berant, Amir Globerson[23] Title: From explanation to synthesis: Compositional program induction for learning from demonstrationAuthors:Michael Burke, Svetlin Penkov, Subramanian Ramamoorthy[24] Title: Road is Enough! Extrinsic Calibration of Non-overlapping Stereo Camera and LiDAR using Road InformationAuthors:Jiyong Jeong, Lucas Y. Cho, Ayoung Kim[25] Title: DeepLO: Geometry-Aware Deep LiDAR OdometryAuthors:Younggun Cho, Giseop Kim, Ayoung Kim[26] Title: Computing Nonlinear Eigenfunctions via Gradient Flow ExtinctionAuthors:Leon Bungert, Martin Burger, Daniel Tenbrinck[27] Title: Multi-task hypernetworksAuthors:Sylwester Klocek, Łukasz Maziarka, Maciej Wołczyk, Jacek Tabor, Marek Śmieja, Jakub Nowak[28] Title: TensorMap: Lidar-Based Topological Mapping and Localization via Tensor DecompositionsAuthors:Sirisha Rambhatla, Nikos D. Sidiropoulos, Jarvis Haupt[29] Title: Learning to See the Wood for the Trees: Deep Laser Localization in Urban and Natural Environments on a CPUAuthors:Georgi Tinchev, Adrian Penate-Sanchez, Maurice Fallon[30] Title: Unmasking Clever Hans Predictors and Assessing What Machines Really LearnAuthors:Sebastian Lapuschkin, Stephan Wäldchen, Alexander Binder, Grégoire Montavon, Wojciech Samek, Klaus-Robert Müller[31] Title: Deep Learning for Low-Dose CT DenoisingAuthors:Maryam Gholizadeh-Ansari, Javad Alirezaie, Paul BabynPapers from arxiv.org更多精彩请移步主页pic from pixels.com"}
{"content2":"原地址计算机视觉目标检测的框架与过程zouxy09@qq.comhttp://blog.csdn.net/zouxy09个人接触机器视觉的时间不长，对于机器学习在目标检测的大体的框架和过程有了一个初步的了解，不知道对不对，如有错误，请各位大牛不吝指点。目标的检测大体框架：目标检测分为以下几个步骤：1、训练分类器所需训练样本的创建：训练样本包括正样本和负样本；其中正例样本是指待检目标样本(例如人脸或汽车等)，负样本指其它不包含目标的任意图片（如背景等），所有的样本图片都被归一化为同样的尺寸大小(例如，20x20)。2、特征提取：由图像或波形所获得的数据量是相当大的。例如，一个文字图像可以有几千个数据，一个心电图波形也可能有几千个数据。为了有效地实现分类识别，就要对原始数据进行变换，得到最能反映分类本质的特征。这就是特征选择和提取的过程。一般我们把原始数据组成的空间叫测量空间，把分类识别赖以进行的空间叫做特征空间，通过变换，可把在维数较高的测量空间中表示的模式变为在维数较低的特征空间中表示的模式。3、用训练样本来训练分类器：这得先明白分类器是什么？百度百科的解释是：“使待分对象被划归某一类而使用的分类装置或数学模型。”我觉得可以怎么理解，举个例子：人脑本身也算一个分类器（只是它强大到超乎想象而已），人对事物的识别本身也是一个分类的过程。人在成长或者学习过程中，会通过观察A类事物的多个具体事例来得到对A类事物性质和特点的认识，然后以后遇到一个新的物体时，人脑会根据这个事物的特征是否符合A类事物性质和特点，而将其分类为A类或者非A类。（这里只是用简单的二分类问题来说明）。那么训练分类器可以理解为分类器（大脑）通过对正样本和负样本的观察（学习），使其具有对该目标的检测能力（未来遇到该目标能认出来）。从数学来表达，分类器就是一个函数y=f(x)，x是某个事物的特征，y是类别，通俗的说就是例如，你输入张三的特征x1，分类器就给你认出来这个是张三y1，你输入李四的特征x2，它就给你认出来这个是李四y2。那么分类器是个函数，它的数学模型是什么呢？一次函数y=kx+b？高次函数？等等好复杂的都有，我们需要先确定它的模型；确定了模型后，模型是不是由很多参数呢？例如上面的一次函数y=kx+b的k和b，高斯函数的均值和方差等等。这个就可以通过什么最小化分类误差、最小化惩罚啊等等方法来确定，其实训练分类器好像就是找这些参数，使得达到最好的分类效果。呵呵，不知道自己说得对不对。另外，为了使分类检测准确率较好，训练样本一般都是成千上万的，然后每个样本又提取出了很多个特征，这样就产生了很多的的训练数据，所以训练的过程一般都很耗时的。4、利用训练好的分类器进行目标检测：得到了分类器就可以用来对你输入的图像进行分类了，也就是在图像中检测是否存在你想要检测的目标。一般的检测过程是这样的：用一个扫描子窗口在待检测的图像中不断的移位滑动，子窗口每到一个位置，就会计算出该区域的特征，然后用我们训练好的分类器对该特征进行筛选，判定该区域是否为目标。然后因为目标在图像的大小可能和你训练分类器时使用的样本图片大小不一样，所以就需要对这个扫描的子窗口变大或者变小（或者将图像变小），再在图像中滑动，再匹配一遍。5、学习和改进分类器现在如果样本数较多，特征选取和分类器算法都比较好的情况下，分类器的检测准确度都挺高的了。但也会有误检的时候。所以更高级点的话就是加入了学习或者自适应，也就是说你把这张图分类错误了，我就把这张图拿出来，标上其正确的类别，再放到样本库中去训练分类器，让分类器更新、醒悟，下次别再给我弄错了。你怎么知道他弄错了？我理解是：大部分都是靠先验知识（例如目标本身存在着结构啊或者什么的约束）或者和跟踪（目标一般不会运动得太快）等综合来判断的。其实上面这个模式分类的过程是适合很多领域的，例如图像啊，语音识别等等。那么这整一个过程关键点在哪呢？（1）特征选取：感觉目标比较盛行的有：Haar特征、LBP特征、HOG特征和Shif特征等；他们各有千秋，得视你要检测的目标情况而定，例如：拳头：纹理特征明显：Haar、LBP（目前有将其和HOG结合）；手掌：轮廓特征明显：HOG特征（行人检测一般用这个）；（在博客中，我会参考各牛人的博客和资料来整理Haar特征、LBP特征、HOG特征和Shif特征等这些内容，具体见博客更新）（2）分类器算法：感觉目标比较盛行的有：SVM支持向量机、AdaBoost算法等；其中检测行人的一般是HOG特征+SVM，OpenCV中检测人脸的一般是Haar+AdaBoost，OpenCV中检测拳头一般是LBP+ AdaBoost；在计算机视觉领域，涉及到的特征啊，算法啊等等还是非常非常多的，不断有牛人在提出新的东西（简单的哲学+复杂的数学），也不断有牛人在改进以前的东西，然后随着岁月的脚步，科技在不停地狂奔着！"}
{"content2":"前言这个名字起的非常大，但是本文只能从一些概念和我自己的理解上介绍一下什么是人工智能。本文只是给从未接触过此块的人一个大致的印象和思路，其余人请直接略过。一、什么是人工智能人工智能这个概念最近非常火，其实什么是人工智能，无非是想让电脑拥有像人一样的智慧。以前电脑做事比较222，从来都是0和1两个数，你让电脑干什么电脑就干什么，像一个听话的奴隶一样，没有自己的思想，没有自己的创新，不会有多余的想法。看上去挺好，但是随着时代的发展，人们对这种电脑的表现已经完全不满意了，现在我们想让电脑不光完成我们交予他的任务，还希望电脑能够有点自己的想法，比如能够告诉我一件事情是好还是坏，顺便再帮我预测一下这件事的结果，这就是人工智能。举个具体的例子。我们有两张图片假设是两个人脸照片。那么在人工智能之前我们如果要让电脑判断这是不是同一个人，那就只能逐像素去比对，如果每个像素都一致，那么这两个照片肯定是同一个照片，人也肯定是同一个人。但是问题来了，如果两个照片是同一个人在不同情况下拍的两张照片，之前的技术手段是没有办法判断这是不是同一个的，因为环境不一样像素肯定不一样，这就必须要让电脑变的更聪明一点，无论什么环境拍的，无论化没化妆整没整容甚至不同年龄段的都要能够判断出来这是不是同一个人，这就是人工智能。二、人工智能能完成什么工作随着软硬件的发展，人工智能已经涉足到了方方面面。如人脸识别、猪脸识别（某东很擅长）、指纹识别、机器视觉、无人车、机器人、AlphaGO、天气预报、车牌识别、违章监控、保险收费、银行贷款等等。其实就是说人工智能已经基本上无所不能了，只要你能想到的地方都可以交给计算机去处理，只是必须要采集大量的数据经过大量的训练计算机才能明白你想让他干什么事情。三、人工智能的实现方式想法很美好，我也想让电脑把我所有的工作都代替掉，包括需要我思考的事情，但这明显是不可能的，那么究竟如何实现人工智能？考虑我们人类是如何对一件事情进行分析的。首先当我们面对一件事情的时候要先回忆自己之前有没有碰到过类似的事情，碰到的事情与这件事情的相似度是多少，所有有关的事情对此事情的影响程度，之前我是怎么处理的。脑中对如何做此事有一个大致的想法之后付诸实践，边做边修改。那么电脑基本类似，首先明确需要电脑完成什么事情，然后找到大量的以往案例，告诉计算机碰到这种事情如何处理，分析出来不同事情（输入）对此事影响的权重。而后交给他同样的事情，计算机便知道该如何处理此事，其实计算机对事情只是计算出了得到某个结果的概率（output），如两个照片是同一个人的概率是90%等等，人也基本类似。当然如上图所示，计算机需要根据输入完成很多层的参数设置最终计算得到输出，除了输入层和最后一层输出层，其他层均为hidden layer，隐层越多则计算量越大，最终的结果越准确（不是绝对的）。所以清晰明了，你告诉计算机越多的“经验”、训练量越大，则他判断一件事情的正确率就越高，这就是最近几年大数据和计算机硬件能力的发展对人工智能的积极影响。推荐大家观看吴恩达的深度学习课程对理论进行深入学习。四、人工智能框架TensorFlow简介有了上面的思路我们就能自己设计一套程序来完成一个训练模型，但是目前有很多成熟的开源深度学习框架，TensorFlow就是一个。TensorFlow™ 是一个采用数据流图（data flow graphs），用于数值计算的开源软件库。节点（Nodes）在图中表示数学操作，图中的线（edges）则表示在节点间相互联系的多维数据数组，即张量（tensor）。它灵活的架构让你可以在多种平台上展开计算，例如台式计算机中的一个或多个CPU（或GPU），服务器，移动设备等等。TensorFlow 最初由Google大脑小组（隶属于Google机器智能研究机构）的研究员和工程师们开发出来，用于机器学习和深度神经网络方面的研究，但这个系统的通用性使其也可广泛用于其他计算领域。通过此框架能够非常容易的设计并训练我们自己的深度学习网络。五、总结本文啰里啰唆的介绍了一下我自己对人工智能的理解，如果有不对的地方欢迎批评指正，后续我会介绍如何通过TensorFlow实现一些简单的案例以及一些深度学习网络模型。本文中部分内容和图片参考阮一峰博客。"}
{"content2":"机器学习笔记(一)名词解释ILP：Inductive Logic Programming的简称，即归纳逻辑程序设计。发展历程n  二十世纪五十年代初，出现机器学习的相关研究n  二十世纪五十年代中期， 基于神经网络的“连接主义学习开始出现n  在二十世纪六七十年代，基于逻辑表示的“符号主义”学习技术蓬勃发展。n  二十世纪五十年代 到七十年代初， 人工智能 处于“推理期”为只要能赋予机器逻辑推理能力，机器就能具有智能。n  二十世纪七十年代中期，人工智能进入“知识期”，大量专家系统问世。但渐渐意识到面临“知识功能瓶颈”n  二十世纪八十年代，机器学习成为一个独立的学科领域，各种机器学习技术百花绽放。机器学习作为“解决知识工程瓶颈问题的关键”而走上人工智能的主舞台。n  二十世纪九十年代中期之前，“从样例中学习”的另一个主流技术：基于神经网络的连接主义学习得到重视n  二十世纪九十年代中期，“统计学习”闪亮登场，迅速占领主流舞台。n  二十一世纪初，连接主义学习卷土重来，掀起了以深度学习为名的热潮。原因是数据大了，计算能力强了。相关材料推荐1)         第一本机器学习专业期刊：Machine Learning2)         人工智能领域的权威期刊：Artificial  Intelligence3)         第一本机器学习专门性教材：Mitchell, 19974)         出色的入门读物：Duda et al.,2001;  Alpaydin, 2004;  Flach, 2012;5)         进阶读物：Hastie et al. , 2009;6)         适合贝叶斯学习偏爱者：Bishop, 20067)         基于WEKA撰写的入门读物，有助于初学者通过WEKA实践快速掌握常用的机器学习算法：Witten et al.,20118)         国际机器学习会议：ICML9)         国际神经信息处理系统会议:NIPS10)     国际学习理论会议：COLT11)     国际学术期刊Journal of Machine Learning Research 和 Machine Learning12)     人工智能领域的重要会议：IJCAI, AAAI13)     人工智能领域重要期刊：Artifical Intelligence;  Journal of Artifical Intelligence Research14)     数据挖掘领域重要会议：KDD, ICDM15)     数据挖掘领域重要期刊：ACM Transactions on Knowledge Discovery from Data;  Data Mining and Knowledge Discovery16)     计算机视觉和模式识别领域的重要会议：CVPR17)     计算机视觉与模式识别领域的重要期刊：IEEE Transactions on Pattern  Analysis and Machine Intelligence18)     神经网络领域的重要期刊：Neural Computation, IEEE Transactions on Neural Networks and Learning System19)     统计学领域的重要期刊：Annals of Statistics20)     中国机器学习大会：CCML21)     中国“机器学习及其应用”研讨会：MLA"}
{"content2":"对人工智能的认识人工智能是计算机科学的一个研究分支，不过，这个分支下面又包含很多的研究方向，这个分支提出的时间很早，中间也热闹过几次，自从2012年以来又热闹了，所以看了一点综述文章对此进行简单的记录。1、人工智能如果从程序层面看其实是一些列的算法，或者包括了算法的sdk或者api，它需要集成到系统中去才能发挥作用，比如做一个具有智能的手机app、具有智能的web app、具有智能的设备（比如搜狗的翻译宝，儿童手表，会自动泊车的汽车等等）。2、人工智能包括六个方面：一、计算机视觉（算上模式识别、图像处理等问题）二、自然语言理解与交流（包括语音识别、合成等问题）三、认知与推理（包含各种物理和社会常识）四、机器人学（机械、控制、设计、运动规划、任务规划等）五、博弈与伦理六、机器学习（各种统计的建模、分析工具和计算的方法）3、人工智能目前还特别幼稚，基本没有啥智能，但是现在科技发展到了这个关口，科学家面临着必须向人工智能进军的问题，所以人工智能成为现实是必然的，只不过这个过程可能会比较漫长，也会充满曲折与挫折，需要大家共同面对、携手前进。4、给机器赋予智能这事一般是一个趋势，不以个人意志为转移的，它必然会来，不是你回避、不承认，它就不来，也不是你拥抱、欢迎，它就会更快来临，它回来，但是在变为现实的过程中也是艰辛和曲折的，因此这件事要不得机会主义，也要不得逃避主义，不能右，也不能左，还是那句话前途是光明的，道路是曲折的。越努力越幸福。5、人工智能的发展历史是从1956年开始的，20世纪80年代初有一股热潮，20世纪80年代末90年代初又有一股热潮，2012年由于深度学习的热潮而又引起一股人工智能的热潮。参考资料：1、谢耘的演讲2、朱松纯老师的文章——浅谈人工智能：现状、任务、架构和统一，正本清源"}
{"content2":"最近要求关注视觉注意的“热门研究方向”、“最新方法”等。boss建议CNKI、EI、或者SCI期刊。我有点纳闷，为什么不去读顶级会议上的论文？感觉在机器学习、计算机视觉和人工智能领域，顶级会议才是王道。有人会质疑这些会议都只是EI，确实，在中国的许多其它领域的会议都是盛会，比如society of neuroscience的会议，人山人海形容也不过分。但是，计算机几个领域的确非常特殊，顶级会议的重要性无论怎么强调都不为过。可以从以下几点说明：（1）因为机器学习、计算机视觉和人工智能领域发展非常迅速，新的工作层出不穷，如果把论文投到期刊上，一两年后刊出时就有点out 了。因此大部分最新的工作都首先发表在顶级会议上，这些顶级会议完全能反映“热门研究方向”、“最新方法”。（2）很多经典工作大家可能引的是某顶级期刊 上的论文，这是因为期刊论文表述得比较完整、实验充分。但实际上很多都是在顶级会议上首发。比如PLSA, Latent Dirichlet Allocation等。（3）如果注意这些领域大牛的pulications，不难发现他们很非常看重这些顶级会议，很多人是80%的会议+20%的期刊。即然大牛们把最新工作发在顶级会议上，有什么理由不去读顶级会议？(1)以下是几个顶级会议的列表（不完整的，但基本覆盖）机器学习顶级会议：NIPS, ICML, UAI, AISTATS;  （期刊：JMLR, ML, Trends in ML, IEEE T-NN）计算机视觉和图像识别：ICCV, CVPR, ECCV;  （期刊：IEEE T-PAMI, IJCV, IEEE T-IP）人工智能：IJCAI, AAAI; （期刊AI）另外相关的还有SIGRAPH, KDD, ACL, SIGIR, WWW等。特别是，如果做机器学习，必须地，把近4年的NIPS, ICML翻几遍；如果做计算机视觉，要把近4年的ICCV, CVPR, NIPS, ICML翻几遍。(2)另外补充一下：大部分顶级会议的论文都能从网上免费下载到，比如CV方面：http://www.cvpapers.com/index.htmlNIPS: http://books.nips.cc/JMLR(期刊): http://jmlr.csail.mit.edu/papers/COLT和ICML(每年度的官网): http://www.cs.mcgill.ca/~colt2009/proceedings.html希望这些信息对大家有点帮助。(3)说些自己的感受。对计算机视觉和计算神经科学领域，从方法和模型的角度看，统计模型（包括probabilistic graphical model和statistical learning theory）是主流也是非常有影响力的方法。有个非常明显的趋势：重要的方法和模型最先在NIPS或ICML出现，然后应用到CV,IR和MM。虽然具体问题和应用也很重要，但多关注和结合这些方法也很有意义。转自：http://blog.sina.com.cn/s/blog_6c41e2f30100v4bg.html"}
{"content2":"人工智能是一门极富挑战性的科学，从事这项工作的人必须懂得计算机知识，心理学和哲学。人工智能是包括十分广泛的科学，它由不同的领域组成，如机器学习，计算机视觉等等，总的说来，人工智能研究的一个主要目标是使机器能够胜任一些通常需要人类智能才能完成的复杂工作。但不同的时代、不同的人对这种“复杂工作”的理解是不同的2。简述。推理  学习   储存三者间的联系推理，逻辑学指思维的基本形式之一，推出新判断（的过程。学习包括知识的学习，行为学习和技术的学习，以及抽象逻辑的理解和空间思维的想象。存储就是根据不同的应用环境通过采取合理、安全、有效方式将数据保存。学习包括了推理且存储同时在进行，当一个人在学习的时候就发生在头脑里存储，实质是人。换一个角度从人工智能来说，计算机上存储时是通过人的学习和设计间接反映出来人的学习和推理，保存在计算机中的方法。学习和推理可以先在人的思维进行，再次放到计算机里存储的过程，也可以在计算机里按着人的设计在推出新的存储。推理、学习、存储本质需要人的思想来设计。3、   “警卫和囚犯”问题的过河方案，使用语义网络进行问题求解。模仿示例画出你的求解方案，并给出一共需要多少步可以成功过河？最优情况：所有情况："}
{"content2":"原文链接：关于感受野的总结论文链接：Understanding the Effective Receptive Field in Deep Convolutional Neural Networks一、感受野感受野被定义为卷积神经网络特征所能看到输入图像的区域，换句话说特征输出受感受野区域内的像素点的影响。下图展示了一个在输出层达到了7*7感受野的例子：感受野计算公式为：，如上例第一个隐层，，如果存在空洞卷积，公式变为。感受野计算的问题上文所述的是理论感受野，而特征的有效感受野（实际起作用的感受野）实际上是远小于理论感受野的，如下图所示。具体数学分析比较复杂，不再赘述，感兴趣的话可以参考论文：Understanding the Effective Receptive Field in Deep Convolutional Neural Networks。我们以下图为例，简要介绍有效感受野的问题：很容易可以发现， 只影响第一层feature map中的 ；而 会影响第一层feature map中的所有特征，即 。第一层的输出全部会影响第二层的 。于是 只能通过 来影响 ；而 能通过 来影响 。显而易见，虽然 和 都位于第二层特征感受野内，但是二者对最后的特征 的影响却大不相同，输入中越靠感受野中间的元素对特征的贡献越大。计算公式对于普通卷积：F(i, j-1) = (F(i, j) - 1)*stride + kernel_size其中 F(i,j)表示第i层对第j层的局部感受野。对于空洞卷积：F(i, j-1) = (F(i, j) - 1)*stride + dilation*(kernel_size - 1) + 1经实际演算，以上公式正确。二、感受野的应用分类Xudong Cao写过一篇叫《A practical theory for designing very deep convolutional neural networks》的technical report，里面讲设计基于深度卷积神经网络的图像分类器时，为了保证得到不错的效果，需要满足两个条件：Firstly, for each convolutional layer, its capacity of learning more complex patterns should be guaranteed; Secondly, the receptive field of the top most layer should be no larger than the image region.其中第二个条件就是对卷积神经网络最高层网络特征感受野大小的限制。目标检测现在流行的目标检测网络大部分都是基于anchor的，比如SSD系列，v2以后的yolo，还有faster rcnn系列。基于anchor的目标检测网络会预设一组大小不同的anchor，比如32x32、64x64、128x128、256x256，这么多anchor，我们应该放置在哪几层比较合适呢？这个时候感受野的大小是一个重要的考虑因素。放置anchor层的特征感受野应该跟anchor大小相匹配，感受野比anchor大太多不好，小太多也不好。如果感受野比anchor小很多，就好比只给你一只脚，让你说出这是什么鸟一样。如果感受野比anchor大很多，则好比给你一张世界地图，让你指出故宫在哪儿一样。《S3FD: Single Shot Scale-invariant Face Detector》这篇人脸检测器论文就是依据感受野来设计anchor的大小的一个例子，文中的原话是we design anchor scales based on the effective receptive field《FaceBoxes: A CPU Real-time Face Detector with High Accuracy》这篇论文在设计多尺度anchor的时候，依据同样是感受野，文章的一个贡献为We introduce the Multiple Scale Convolutional Layers(MSCL) to handle various scales of face via enrichingreceptive fields and discretizing anchors over layers"}
{"content2":"1、引言1什么是人工智能？人工智能是对程序系统的研究，该程序系统在一定程度上能模仿人类的活动，如感知、思考、学习和反应。2人工智能简史3图灵测试:这个测试提出了机器具有智能的一个定义。4智能体：是一个能够智能地感知环境、从环境中学习并与环境进行交互的系统。智能体可以分成两大类：软件智能体和物理智能体。5编程语言：虽然有些通用语言（如C、C++、Java）能用来编写智能软件，但是两种语言是特别为人工智能设计的：LISP和PROLOG2、知识表示我们描述四种常见的知识表示方法：1语义网2框架3谓词逻辑4基于规则的系统3、专家系统人工智能的一个目标是建立专家系统，完成通常需要人类专家经验的任务。它们可以用在人类专家缺少、昂贵或不可用等场合。专家系统体系结构：一个专家系统由7个部分构成：用户、用户界面、推理机、知识库、事实库、解释系统和知识库编辑器。1抽取知识→知识库2抽取事实→事实库4、感知人工智能的另一个目标是创造行为像普通人类的机器（平凡系统）。如果一个智能体要表现的像人类，那么它就应该有感知能力。人工智能已经初步完成两种感知：视觉和听觉。1图像处理这个目标的第一部分涉及图像处理或计算机视觉，这是处理对象感知的一个人工智能领域。步骤：①边缘探测②分段：方法：阈值化、分割和合并③查找深度：方法：立体视觉和运动④查找方向：两种技术：光照和纹理⑤对象识别应用：如制造业2语言理解这个目标的第二部分是自然语言的语言处理、分析和翻译。步骤：①语音识别②语法分析：两个工具：良好定义的文法和词法分析器③语义分析④语用分析：用来进一步明确句子的用途和消除歧义作用：意图和消除歧义5、搜索在人工智能中，问题求解的一种技术是搜索。搜索可以描述成使用一组状态（情形）求解一个问题。两种常用的搜索方法：1蛮力搜索：广度优先搜索和深度优先搜索2启发式搜索：使用它我们给每个节点赋一个成为启发值（h值）的定量值。6、神经网络如果智能体应该表现得像人一样，那么它可能就需要学习。已经使用的方法中有几种为未来建立了希望。大多数方法使用归纳学习和从例子中学习。一个通常的方法是使用神经网络，使用神经网络试图模仿人脑的学习过程。1生物神经元2感知器：一个类似于单个生物神经元的人工神经元。3多层网络：几个层次的感知器可以组合起来，形成多层神经网络。三层：输入层、隐藏层、输出层4应用两个证明神经网络有用的：光学字符识别（OCR)和信用赋值。"}
{"content2":"在网易云课堂上学习计算机视觉经典课程cs231n，觉得有必要做个笔记，因为自己的记性比较差，留待以后查看。每一堂课都对应一个学习笔记，下面就开始第一堂课。这堂课主要是回顾了计算机视觉的起源及其后来的发展，有助于学者理清计算机视觉的研究主题和发展脉络。首先介绍了计算机视觉的相关领域，因为它不仅仅关乎计算机科学，还涉及生物，物理，工程等等学科，是一门综合性很强的学科。接着从物种大爆炸讲起推测生物有了视觉后极大地扩张了活动区域，带来了生物的蓬勃发展。接着讲计算机视觉在近现代的发展，这里省略，直接跳到近些年有重要意义的学术成果。2001年Viola&Jones推出人脸识别算法haar检测器1999年David Lowe推出sift特征算法2005年和2009年分别出现HOG特征算法和DPM算法除了算法上的改进，数据规模的增大也是推动计算机视觉发展的主因，甚至到了深度学习时代，是各种数据集推动着算法的演化。以下分别是PASCAL VOC和ImageNet，PASCAL 包含20类物体，而ImageNet则有惊人的22000类物体，图片数量达到1千4百万张。那么计算机视觉究竟要解决哪些问题，比较重要的几个问题包括图像分类，目标识别，图像描述，图像分割，场景理解等，其中，场景理解可以说是终极目标，自然也是最具挑战性的问题。回到这门课本身，CNN对于计算机视觉有何帮助，可以说进入人工智能时代，CNN就是解决计算机视觉的全部。不过CNN的出现可是有些年头了，早在1998年，Lecun就成功地利用CNN来识别邮政数字了，不过直到2012年Alexnet的出现引发了这一潮流。下面左图就是二者的网络结构，可以看出并没有大的改变，所以不得不归功于今时今日计算机的性能和数据集的出现。右图是在ImageNet数据集上的历年冠军，从2012年往后，基本就是CNN的天下了。最后是一些基本要求吧，python，线性代数，微积分，还有CS229（机器学习）的基本知识，都要掌握。"}
