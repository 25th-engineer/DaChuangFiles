自然语言 处理 定义 自然语言 处理 是 一门 计算机科学 人工智能 以及 
语言学 的 交叉 学科 虽然 语言 只是 人工智能 的 一部分 
人工智能 还 包括 计算机 视觉 等 但 它 是 非常 
独特 的 一部分 这个 星球 上 有 许多 生物 拥有 
超过 人类 的 视觉 系统 但 只有 人类 才 拥有 
这么 高级 的 语言 自然语言 处理 的 目标 是 让 
计算机 处理 或 说 理解 自然语言 以 完成 有 意义 
的 任务 比如 订机票 购物 或 QA 等 完全 理解 
和 表达 语言 是 极其 困难 的 完美 的 语言 
理解 等效于 实现 人工智能 自然语言 处理 涉及 的 几个 层次 
自然语言 处理 涉及 的 层次 作为 输入 一共 有 两个 
来源 语音 与 文本 所以 第一 级 是 语音 识别 
和 OCR 或 分词 事实上 跳过 分词 虽然 理所当然 地 
不能 做 句法分析 但 字符 级 也 可以 直接 做 
不少 应用 接下来 是 形态学 援引 统计 自然语言 处理 中的 
定义 形态学 morphology 形态学 又称 词汇 形态学 或 词法 是 
语言学 的 一个 分支 研究 词 的 内部 结构 包括 
屈折 变化 和 构词法 两个 部分 由于 词 具有 语音 
特征 句法 特征 和 语义 特征 形态学 处于 音位学 句 
法学 和 语义学 的 结合部位 所以 形态学 是 每个 语言学家 
都要 关注 的 一门 学科 Hanlp 自然语言 处理 开发包 从事 
大 数据 方面 工作 的 人 对 自然 语言 处理 
必然 都是 不 陌生 的 在 Github 上 用户 量 
最多 的 开源 汉语 自然语言 处理 工具 是 HanLP HanLP 
的 初始 版本 是 在 2014 年初 开发 的 3 
月份 的 时候 开始 在 Github 上 开源 2015年 的 
时候 集成 在 了 大 快 搜索 的 DKNLP 中 
目前 大 快 已经 把 DKNLP 技术成果 已经 开源 并且 
整体 装 如 HanLP 项目 HanLP 的 版本 已经 到了 
V 1.50 Hanlp 自然语言 处理 技术 优势 支持 中文分词 N 
最 短路 分词 CRF 分词 索引 分词 用户 自定义 词调 
词性 标注 命名 实体 识别 中国 人民 音译 人民 日本 
人民 地名 实体 机构 名 识别 关键词 提取 自动 摘要 
短语 提取 拼音 转换 简繁转换 文本 推荐 依存 句法分析 MaxEnt 
依存 句法分析 神经网络 依存 句法分析 提供 Lucene 查件/nr 兼容 Solr 
和 ElasticSearch Hanlp 自然语言 处理 应用领域 Hanlp 已经 被 广泛 
应用于 Lucene Solr ElasticSearch hadoop android Resin 等 平台 有/v 
大量/n 开源/n 作者/n 开发/v 各种/r 查件与/nr 拓展/v 并且 被 包装 
或 移植 到 Python C # R JavaScript 等 语言 
上去 给 外行 能看懂 的 科普 这 就叫 自然语言 处理 
如何 向 文科 同学 科普 自然语言 处理 NLP 刘知远 NLPer/w 
前/f 几年/m 曾经/d 马少平/nr 老师/n 的/uj 引荐/v 为 某 科普 
图书 写过 一篇 短文 介绍 自然语言 处理 如果 只是 介绍 
NLP 的 概念 任务 和 挑战 应该 可以 参考 这篇 
小文 原文 如下 仅供参考 自然语言 处理 Natural Language Processing 一 
什么 是 自然 语言 处理 简单 地 说 自然语言 处理 
Natural Language Processing 简称 NLP 就是 用 计算机 来 处理 
理解 以及 运用 人类 语言 如 中文 英文 等 它 
属于 人工智能 的 一个 分支 是 计算机 科学 与 语言学 
的 交叉 学科 又 常 被称为 计算 语言学 由于 自然 
语言 是 人类 区别于 其他 动物 的 根本 标志 没有 
语言 人类 的 思维 也 就 无从谈起 所以 自然 语言 
处理 体现 了 人工智能 的 最高 任务 与 境界 也 
就是说 只有 当 计算机 具备 了 处理 自然 语言 的 
能力 时 机器 才算 实现 了 真正 的 智能 从 
研究 内容 来看 自然语言 处理 包括 语法分析 语义分析 篇章 理解 
等 从 应用 角度 来看 自然语言 处理 具有 广泛 的 
应用 前景 特别 是 在 信息 时代 自然语言 处理 的 
应用 包罗万象 例如 机器翻译 手写体 和 印刷体 字符识别 语音 识别 
及 文语/nr 转换 信息检索 信息 抽取 与 过滤 文本 分类 
与 聚 类 舆情 分析 和 观点 挖掘 等 它 
涉及 与 语言 处理 相关 的 数据 挖掘 机器学习 知识 
获取 知识工程 人工智能 研究 和与/nr 语言 计算 相关 的 语言 
学 研究 等 值得一提的是 自然语言 处理 的 兴起 与 机器 
翻译 这 一 具体 任务 有着 密切联系 机器翻译 指 的 
是 利用 计算机 自动 地 将 一种 自然 语言 翻译 
为 另外 一种 自然 语言 例如 自动 将 英文 I 
like Beijing Tiananmen Square 翻译 为 我 爱 北京 天安门 
或者 反过来 将 我 爱 北京 天安门 翻译 为 I 
like Beijing Tiananmen Square 由于 人工 进行 翻译 需要 训练有素 
的 双语 专家 翻译 工作 非常 耗时耗力 更不用说 需要 翻译 
一些 专业 领域 文献 时 还 需要 翻译者 了解 该 
领域 的 基本 知识 世界 上 有 超过 几千种 语言 
而 仅 联合国 的 工作 语言 就有 六种 之多 如果 
能够 通过 机器翻译 准确 地 进行 语言 间 的 翻译 
将 大大 提高 人类 沟通 和 了解 的 效率 圣经 
里 有 一个 故事 说 巴比伦人 想 建造 一座 塔 
直通 天堂 建 塔 的 人都 说 着 同一 种 
语言 心意 相通 齐心协力 上帝 看到 人类 竟然 敢做 这种事情 
就 让 他们 的 语言 变得 不 一样 因为 人们 
听 不懂 对方 在 讲 什么 于是 大家 整天 吵吵闹闹 
无法 继续 建 塔 后来 人们 把 这座 塔 叫作 
巴别塔 而 巴别 的 意思 就是 分歧 虽然 巴别塔 停建 
了 但 一个 梦想 却 始终 萦绕 在 人们 心中 
人类 什么 时候 才能 拥有 相通 的 语言 重建 巴别塔 
呢 机器翻译 被 视为 重建 巴别塔 的 伟大 创举 假如 
能够 实现 不同 语言 之间 的 机器 翻译 我们 就 
可以 理解 世界 上 任何人 说 的话 与 他们 进行 
交流 和 沟通 再也 不必 为 相互 不 能理解 而 
困扰 事实上 人工智能 被 作为 一个 研究 问题 正式 提 
出来 的 时候 创始人 把 计算机 国际象棋 和 机器 翻译 
作为 两个 标志性 的 任务 认为 只要 国际象棋 系统 能够 
打败 人类 世界 冠军 机器翻译 系统 达到 人类 翻译 水平 
就 可以 宣告 人工智能 的 胜利 四 十年 后的/nr 1997年 
IBM 公司 的 深蓝 超级计算机 已经 能够 打败 国际象棋 世界 
冠军 卡斯帕罗夫 而 机器翻译 到 现在 仍 无法 与 人类 
翻译 水平 相比 从此 可以 看出 自然语言 处理 有 多么 
困难 自然语言 处理 兴起 于 美国 第二 次 世界大战 之后 
二十 世纪 五十年代 当 电子计算机 还在 襁褓 之中 时 利用 
计算机 处理 人类 语言 的 想法 就 已经 出现 当时 
美国 希望 能够 利用 计算机 将 大量 俄语 材料 自动 
翻译 成 英语 以 窥探 苏联 科技 的 最新 发展 
研究者 从 破译 军事 密码 中 得到 启示 认为 不同 
的 语言 只不过是 对 同一 语义 的 不同 编码 而已 
从而 想当然 地 认为 可以 采用 译码 技术 像 破译 
密码 一样 破译 这些 语言 1954年 1月 7日 美国/ns 乔治敦/ns 
大学/n 和/c IBM/w 公司/n 合作/vn 实验/vn 成功/a 地/uv 将/d 超过/v 
60/m 句/q 俄语/nz 自动/vn 翻译/v 成/n 英语/nz 虽然 当时 的 
这个 机器 翻译 系统 非常简单 仅仅 包含 6个 语法 规则 
和 250个 词 但 由于 媒体 的 广泛 报道 纷纷 
认为 这 是 一个 巨大 的 进步 导致 美国 政府 
备受 鼓舞 加大 了 对 自然 语言 处理 研究 的 
投资 实验 完成者 也 当即 自信 地 撰文 称 在三 
到 五年 之内 就 能够 完全 解决 从 一种 语言 
到 另一种 语言 的 自动 翻译 问题 他们 认为 只要 
制定 好 各种 翻译 规则 通过 大量 规则 的 堆砌 
就 能够 完美 地 实现 语言 间 的 自动 翻译 
然而 事实 是 理解 人类 语言 远比 破译 密码 要 
复杂 得多 因此 研究 进展 非常 缓慢 1966年 的 一份 
研究 报告 总结 发现 经过 十 年之久 的 研究 结果 
远远 未能 达到 预期 因此 支持 资金 急剧下降 使 自然语言 
处理 特别 是 机器 翻译 的 研究 陷入 长达 二 
十年 的 低潮 直到 二十 世纪 八十 年代 随着 电子 
计算机 的 计算 能力 的 飞速 提高 和 制造 成本 
的 大幅 下降 研究者 又 开始 重新 关注 自然语言 处理 
这个 极富 挑战 的 研究 领域 三十年 沧海桑田 此时 研究 
者 已经 认识 到 简单 的 语言 规则 的 堆砌 
无法 实现 对 人类 语言 的 真正 理解 研究 发现 
通过 对 大量 的 文本 数据 的 自动 学习 和 
统计 能够 更好 地 解决 自然语言 处理 问题 如 语言 
的 自动 翻译 这一 思想 被 称为 自然语言 处理 的 
统计 学习 模型 至今 方兴未艾 那么 自然语言 处理 到底 存在 
哪些 主要 困难 或 挑战 吸引 那么 多 研究 者 
几十年如一日 孜孜不倦 地 探索 解决之道 呢 二 自然语言 处理 的 
主要 困难 自然语言 处理 的 困难 可以 罗列 出 来 
很多 不过 关键 在于 消除歧义 问题 如 词 法分析 句法分析 
语义分析 等 过程 中 存在 的 歧义 问题 简称为 消 
歧 而 正确 的 消 歧 需要 大量 的 知识 
包括 语言 学 知识 如 词法 句法 语义 上下文 等 
和 世界 知识 与 语言 无关 这 带来 自然语言 处理 
的 两个 主要 困难 首先 语言 中 充满 了 大量 
的 歧义 这 主要 体现 在 词法 句法 及 语义 
三个 层次 上 歧义 的 产生 是 由于 自然 语言 
所 描述 的 对象 ― ― 人类 活动 非常 复杂 
而 语言 的 词汇 和 句法 规则 又是 有限 的 
这就 造成 同 一种 语言 形式 可能 具有 多种 含义 
例如 单词 定界 问题 是 属于 词法 层面 的 消 
歧 任务 在 口语 中 词 与 词 之间 通常 
是 连贯 说 出来 的 在 书面语 中 中文 等 
语言 也 没有 词 与 词 之间 的 边界 由于 
单词 是 承载 语义 的 最小 单元 要 解决 自然语言 
处理 单词 的 边界 界定 问题 首当其冲 特别 是 中文 
文本 通常 由 连续 的 字 序列 组成 词 与 
词 之间 缺少 天然 的 分隔符 因此 中文信息处理 比 英文 
等 西方 语 言多 一步 工序 即 确定 词 的 
边界 我们 称为 中文 自动 分词 任务 通俗 的 说 
就是 要 由 计算机 在 词 与 词 之间 自动 
加上 分隔符 从而 将 中文 文本 切分 为 独立 的 
单词 例如 一个 句子 今天 天气 晴朗 的 带有 分隔符 
的 切分 文本 是 今天 | 天气 | 晴朗 中文 
自动 分词 处于 中文 自然语言 处理 的 底层 是 公认 
的 中文 信息 处理 的 第一 道 工序 扮演 着 
重要 的 角色 主要 存在 新词 发现 和 歧义 切分 
等 问题 我们 注意到 正确 的 单词 切分 取决于 对 
文本 语义 的 正确 理解 而 单词 切分 又是 理解 
语言 的 最初 的 一道 工序 这样 的 一个 鸡 
生蛋 蛋 生鸡/nr 的 问题 自然 成了 中文 自然语言 处理 
的 第一 条 拦路虎 其他 级别 的 语言 单位 也 
存在 着 各种 歧义 问题 例如 在 短语 级别 上 
进口 彩电 可以 理解 为 动宾 关系 从 国外 进口 
了 一批 彩电 也 可以 理解 为 偏正 关系 从 
国外 进口 的 彩电 又 如在 句子 级别 上 做 
手术 的 是 她 的 父亲 可以 理解 为 她 
父亲 生病 了 需要 做手术 也 可以 理解 为 她 
父亲 是 医生 帮 别人 做 手术 总之 同样 一个 
单词 短语 或者 句子 有 多种 可能 的 理解 表示 
多种 可能 的 语义 如果 不能 解决 好 各级 语言 
单位 的 歧义 问题 我们 就 无法 正确 理解 语言 
要 表达 的 意思 另外 一个 方面 消除歧义 所 需要 
的 知识 在 获取 表达 以及 运用 上 存在 困难 
由于 语言 处理 的 复杂性 合适 的 语言 处理 方法 
和 模型 难以 设计 例如 上下文 知识 的 获取 问题 
在 试图 理解 一句话 的 时候 即使 不 存在 歧义 
问题 我们 也 往往 需要 考虑 上下文 的 影响 所谓 
的 上下文 指 的 是 当前 所说 这句话 所处 的 
语言 环境 例如 说 话人 所处 的 环境 或者 是 
这 句话 的 前 几句话 或者 后 几句话 等等 假如 
当前 这 句话 中 存在 指 代词 的 时候 我们 
需要 通过 这 句话 前面 的 句子 来 推断 这个 
指 代词 是 指 的 什么 我们 以 小明 欺负 
小亮 因此 我 批评 了 他 为例 在 其中 的 
第二 句话 中的 他 是 指代 小明 还是 小亮 呢 
要 正确 理解 这 句话 我们 就 要 理解 上 
句话 小明 欺负 小亮 意味着 小明 做得 不对 因此 第二句 
中的 他 应当 指代 的 是 小明 由于 上下文 对于 
当前 句子 的 暗示 形式 是 多种多样 的 因此 如何 
考虑 上下文 影响 问题 是 自然 语言 处理 中 的 
主要 困难 之一 再如 背景 知识 问题 正确理解 人类 语言 
还要 有 足够 的 背景 知识 举 一个 简单 的 
例子 在 机器 翻译 研究 的 初期 人们 经常 举 
一个 例子 来 说明 机器翻译 任务 的 艰巨性 在 英语 
中 The spirit is willing but the flesh is weak 
. 意思 是 心有余而力不足 但是 当时 的 某个 机器翻译 系统 
将 这句 英文翻译 到 俄语 然后再 翻译 回 英语 的 
时候 却 变成 了 The Voltka is strong but the 
meat is rotten . 意思 是 伏特加酒 是 浓 的 
但 肉 却 腐 烂了 从 字面 意义 上 看 
spirit 烈性酒 与 Voltka 伏特加 对 译 似 无问题 而 
flesh 和 meat 也 都有 肉 的 意思 那么 这 
两 句话 在 意义 上 为什么 会 南辕北辙 呢 关键 
的 问题 就 在于 在 翻译 的 过程 中 机器翻译 
系统 对于 英语 成语 并无 了解 仅仅 是 从 字面 
上 进行 翻译 结果 自然 失之毫厘 差之千里 从 上面 的 
两个 方面 的 主要 困难 我们 看到 自然语言 处理 这个 
难题 的 根源 就是 人类 语言 的 复杂性 和 语言 
描述 的 外部 世界 的 复杂性 人类 语言 承担 着 
人类 表达 情感 交流思想 传播 知识 等 重要 功能 因此 
需要 具备 强大 的 灵活性 和 表达 能力 而 理解 
语言所 需要 的 知识 又 是 无止境 的 那么 目前 
人们 是 如何 尝试 进行 自然语言 处理 的 呢 三 
自然语言 处理 的 发展 趋势 目前 人们 主要 通过 两 
种 思路 来 进行 自然语言 处理 一种 是 基于 规则 
的 理性主义 另外 一种 是 基于 统计 的 经验 主义 
理性主义 方法 认为 人类 语言 主要 是 由 语言 规则 
来 产生 和 描述 的 因此 只要 能够 用 适当 
的 形式 将 人类 语言 规则 表示出来 就 能够 理解 
人类 语言 并 实现 语言 之间 的 翻译 等 各种 
自然语言 处理 任务 而 经验主义 方法 则 认为 从 语言 
数据 中 获取 语言 统计 知识 有效 建立 语言 的 
统计模型 因此 只要 能够 有 足够 多 的 用于 统计 
的 语言 数据 就 能够 理解 人类 语言 然而 当 
面对 现实 世界 充满 模糊 与 不确定性 时 这 两种 
方法 都 面临 着 各自 无法 解决 的 问题 例如 
人类 语言 虽然 有 一定 的 规则 但是 在 真实 
使用 中 往往 伴随 大量 的 噪音 和不/nr 规范性 理性主义 
方法 的 一大 弱点 就是 鲁棒性 差 只要 与 规则 
稍有 偏离 便 无法 处理 而 对于 经验主义 方法 而言 
又 不能 无限 地 获取 语言 数据 进行 统计 学习 
因此 也 不 能够 完美 地 理解 人类 语言 二十 
世纪 八十 年代 以来 的 趋势 就是 基于 语言 规则 
的 理性主义 方法 不断 受到 质疑 大 规模 语言 数据处理 
成为 目前 和 未来 一段 时期 内 自然语言 处理 的 
主要 研究 目标 统计 学习 方法 越来越 受到 重视 自然语言 
处理 中 越来越 多 地 使用 机器 自动 学习 的 
方法 来 获取 语言 知识 迈进 二十一 世纪 我们 已经 
进入 了 以 互联网 为 主要 标志 的 海量 信息 
时代 这些 海量 信息 大 部分 是 以 自然 语言 
表示 的 一方面 海量 信息 也为 计算机 学习 人类 语言 
提供 了 更多 的 素材 另一方面 这 也为 自然语言 处理 
提供 了 更加 宽广 的 应用 舞台 例如 作为 自然语言 
处理 的 重要 应用 搜索引擎 逐渐 成为 人们 获取 信息 
的 重要 工具 涌现出 以 百度 谷歌 等 为 代表 
的 搜索引擎 巨头 机器翻译 也从 实验室 走入 寻常 百姓家 谷歌 
百度/n 等/u 公司/n 都/d 提供/v 了/ul 基于/p 海量/n 网络/n 数据/n 
的/uj 机器/n 翻译/v 和/c 辅助/vn 翻译/v 工具/n 基于 自然语言 处理 
的 中文 输入 法如 搜狗 微软 谷歌 等 输入法 成为 
计算机 用户 的 必备 工具 带有 语音 识别 的 计算机 
和 手机 也 正 大行其道 协助 用户 更 有效 地 
工作 学习 总之 随着 互联网 的 普及 和 海量 信息 
的 涌现 自然语言 处理 正在 人们 的 日常 生活 中 
扮演 着 越来越 重要 的 作用 然而 我们 同时 面临 
着 一个 严峻 事实 那 就是 如何 有效 利用 海量 
信息 已 成为 制约 信息技术 发展 的 一个 全局性 瓶颈 
问题 自然语言 处理 无可 避免 地 成为 信息 科学 技术 
中 长期 发展 的 一个 新的 战略 制高点 同时 人们 
逐渐 意识到 单纯 依靠 统计 方法 已经 无法 快速 有效 
地 从 海量 数据 中 学习 语言 知识 只有 同时 
充分 发挥 基于 规则 的 理性主义 方法 和 基于 统计 
的 经验 主义 方法 的 各自 优势 两者 互相 补充 
才 能够 更好 更快 地 进行 自然语言 处理 自然语言 处理 
作为 一个 年龄 尚 不足 一个 世纪 的 新兴 学科 
正在 进行 着 突飞猛进 的 发展 回顾 自然语言 处理 的 
发展 历程 并 不是 一帆风顺 有过 低谷 也 有过 高潮 
而 现在 我们 正 面临 着 新的 挑战 和 机遇 
例如 目前 网络 搜索 引擎 基本上 还 停留 在 关键词 
匹配 缺乏 深 层次 的 自然 语言 处理 和 理解 
语音识别 文字 识别 问答 系统 机器 翻译 等 目前 也 
只能 达到 很 基本 的 水平 路漫漫其修远兮 自然语言 处理 作为 
一个 高度 交叉 的 新兴 学科 不论是 探究 自然 本质 
还是 付诸 实际 应用 在将来/i 必定会/l 有/v 令人/l 期待/v 的/uj 
惊喜/a 和/c 异常/d 快速/d 的/uj 发展/vn 参考文献 1 张钹 . 
自然语言 处理 的 计算 模型 . 中文信息 学报 2007 21 
3 3 7 . 2 冯志伟 . 统计 自然语言 处理 
序言 . 1版 . 北京 清华大学出版社 2008 . 3 孙茂松/nr 
./i 语言 计算 信息 科学 技术 中 长期 发展 的 
战略 制高点 . 语言 文字 应用 2005 3 38 40 
. 查看 知乎 原文 12 条 讨论 扫描 二维码 下载 
知乎 日报 支持 iOS 和 Android 二维码 下载 知乎 日报 
知乎 网 © 2018 知乎 08 自然语言 处理 中 的 
机器 学习 方法 8.1 机器 学习 的 基本 概念 8 
. 1.1 ML 类型 8 . 1.2 ML 监督 学习 
8 . 1.3 无 监督 学习 8 . 1.4 强化 
学习 8.2 自然语言 处理 应用 的 开发 步骤 8 . 
2.1 第一 次 迭 代时 的 开发 步骤 8 . 
2.2 从 第二 次 到 第 N 次 迭代 的 
开发 步骤 8.3 机器学习 算法 和 其他 概念 8 . 
3.1 有 监督 机器学习 方法 逻辑 回归 决策树 随机 森林 
朴素 贝叶斯 支持 向量 机 8 . 3.2 无 监督 
机器学习 方法 k 均值 聚 类 8 . 3.3 半 
监督 机器学习 算法 8 . 3.4 一些 重要 概念 8 
. 3.5 特征选择 8 . 3.6 维度 约 减 主 
成分 分析 t SNE8 . 4 自然语言 处理 中 的 
混合 方法 8.5 总结 我们 已经 看到 了 特性 工程 
的 基本 和 高级 水平 我们 还 看到 了 如何 
使用 基于 规则 的 系统 来 开发 NLP 应用程序 在 
本 章中 我们 将 开发 NLP 应用程序 为了 开发 应用 
程序 我们 将 使用 机器学习 ML 算法 我们 将 从 
ML 的 基本 知识 开始 之后 我们 将 看到 使用 
ML 的 NLP 应用 程序 的 基本 开发 步骤 我们 
将 主要 了解 如何 在 NLP 域中 使用 ML 算法 
然后 我们 将 进入 特性 选择 部分 我们/r 还将/i 了解/v 
混合模型/i 和后/nr 处理/v 技术/n 本章 概述 如下 了解 机器 学习 
的 基础 知识 NLP 应用 程序 的 开发 步骤 了解 
ML 算法 和 其他 概念 NLP 应用 的 混合 方法 
让 我们 来 探索 ML 的 世界 8.1 机器 学习 
的 基本 概念 首先 我们 将 了解 什么 是 机器学习 
传统上 编程 就是 定义 所有 的 步骤 以 达到 某个 
预先 定义 的 结果 在 这个 编程 过程 中 我们 
使用 一种 编程语言 来 定义 每 一个 微小 的 步骤 
这 有助于 我们 实现 我们 的 结果 为了 给 你 
一个 基本 的 理解 我 举 一个 一般 的 例子 
假设 你 想 写 一个 程序 来 帮助 你 画 
一张 脸 您 可以 先 编写 绘制 左眼 的 代码 
然后 编写 绘制 右眼 的 代码 再 编写 绘制 鼻子 
的 代码 依此类推 这里 您 正在 为 每个 面部 属性 
编写 代码 但是 ML 会 翻转 这种方法 在 ML 中 
我们 定义 结果 程序 学习 实现 定义 的 输出 的 
步骤 因此 我们 不 为 每个 面部 属性 编写 代码 
而是 向 机器 提供 数百个 人脸 样本 我们 希望 这 
台 机器 能够 学习 绘制 人脸 所需 的 步骤 以便 
绘制 出 新的 人脸 除此之外 当 我们 提供 新的 人脸 
以及 一些 动物 脸 时 它 应该 能够 识别 出 
哪 张脸/nr 看起来 像人 的 脸 让 我们 举 几个 
一般 的 例子 如果 要 识别 某些 状态 的 有效 
车牌 在 传统 编程 中 需要 编写 代码 例如 车牌 
的 形状 颜色 字体 等 如果 您 试图 手动 对 
车牌 的 每个 属性 进行 编码 则 这些 编码 步骤 
太长 使用 ML 我们 将 向 机器 提供 一些 车牌 
示例 机器 将 学习 步骤 以便 识别 新 的 有效 
车牌 假设 你 想 制作 一个 程序 来 玩 超级 
马里奥 游戏 并 赢得 比赛 所以 定义 每个 游戏 规则 
对 我们 来说 太 困难 了 我们 通常 定义 一个 
目标 比如 你 需要 在 不 死亡 的 情况 下 
到达 终点 机器 会 学习 所 有 步骤 来 到达 
终点 有时 问题 太 复杂 了 甚至 我们 不 知道 
应该 采取 什么 步骤 来 解决 这些 问题 例如 我们 
是 一家 银行 我们 怀疑 有 一些 欺诈 活动 正在 
发生 但 我们 不 确定 如何 检测 它们 或者 我们 
甚至 不 知道 要 寻找 什么 我们 可以 提供 所有 
用户 活动 的 日志 并 查找 行为 与 其他 用户 
不同 的 用户 机器 自行 学习 检测 异常 的 步骤 
ML 在 互联网 上 无处不在 每个 大 科技 公司 都在 
以 某种 方式 使用 它 当 您 看到 任何 YouTube 
视频 时 YouTube 会 更新 或 向您/nr 提供 您 可能 
喜欢 观看 的 其他 视频 的 建议 甚至 你 的 
手机 也 使用 ML 为 你 提供 诸如 iPhone 的 
Siri Google 援助 等 设施 ML 目前 进展 非常 快 
研究 人员 使用 旧 的 概念 改变 其中 的 一些 
或者 使用 其他 研究 人员 努力 使其 更 有效 和 
有用 让 我们 来 看看 ML 的 基本 传统 定义 
1959年 一位 名叫 Arthur Samuel 的 研究 人员 给 计算机 
提供 了 无需 显 式 编程 即可 学习 的 能力 
他/r 从/p 人工智能/n 模式识别/n 和/c 计算/v 学习/v 理论/n 的/uj 研究/vn 
中/f 发展/vn 了/ul ML/w 的/uj 概念/n 1997年 Tom Mitchell 给 
了 我们 一个 准确 的 定义 这个 定义 对 那些 
能 理解 基础 数学 的 人 很 有用 根据 Tom 
Mitchell 的 定义 ML 的 定义 是 一个 计算机程序 据说 
是从 经验 E 中 学习 一些 任务 T 和 一些 
性能 度量 P 如果 它 在 T 上 的 性能 
如 经验 P 所 测量 的 那样 随着 经验 E 
的 提高 而 提高 让 我们 将 前面 的 定义 
与 前面 的 示例 链 接起来 识别 车牌 称为 任务 
T 您 将 使用 名为 体验 E 的 车牌 示例 
运行 一些 ML 程序 如果 它 成功 学习 那么 它 
可以 预测 下 一个 未知 的 车牌 称为 性能 度量 
P 现在 是 时候 探索 不同 类型 的 ML 以及 
它 如何 与 人工智能 相 关了 8 . 1.1 ML 
类型 在 本节 中 我们 将 介绍 不同 类型 的 
ML 以及 一些 有趣 的 分支 和 超级 分支 关系 
ML 本身 源于 一个 叫做 人工智能 的 分支 ML 也 
有 一个 分支 它 正在 制造 许多 流行 的 话题 
叫做 深度 学习 但是 我们 将 在 第 9 章 
NLP/w 和/c NLG/w 问题/n 的/uj 深度/ns 学习/v 中/f 详细/ad 介绍/v 
人工智能/n 和/c 深度/ns 学习/v 学习 技术 可以 分为 不同 的 
类型 在 本 章中 我们 将 重点 放在 ML 上 
参见 . 1 ML 技术 可以 分为 三 种 不同 
类型 如 . 2 所示 8 . 1.2 ML 监督 
学习 在 这种 类型 的 ML 中 我们 将 提供 
一个 标记 的 数据集 作为 ML 算法 的 输入 并且 
我们 的 ML 算法 知道 什么 是 正确 的 什么 
是 不 正确 的 在 这里 ML 算法 学习 标签 
和 数据 之间 的 映射 它 生成 了 ML 模型 
然后 生成 的 ML 模型 可以 用来 解决 某些 给定 
的 任务 假设 我们 有 一些 带有 标签 的 文本 
数据 比如 垃圾 邮件 和非/nr 垃圾邮件 数据集 的 每个 文本 
流 都有 这两个 标签 中 的 任何 一个 当 我们 
应用 监督 的 ML 算法 时 它 使用 标记 的 
数据 并 生成 一个 ML 模型 该 模型 预测 标签 
为 垃圾 邮件 或非 垃圾邮件 用于 看 不见 的 文本 
流 这 是 一个 监督 学习 的 例子 8 . 
1.3 无 监督 学习 在 这种 类型 的 ML 中 
我们 将 提供 一个 未 标记 的 数据集 作为 ML 
算法 的 输入 所以 我们 的 算法 没有 得到 任何 
关于 正确 与否 的 反馈 它 必须 自己 学习 数据 
的 结构 来 解决 给定 的 任务 使用 未 标记 
的 数据 集 比较 困难 但 更 方便 因为/c 并非/c 
每个人/i 都有/nr 一个/m 完全/ad 标记/n 的/uj 数据集/i 大多数 数据 都是 
未 标记 混乱 和 复杂 的 假设 我们 正在 开发 
一个 汇总 应用程序 我们 可能 还 没有 总结 出 与 
实际 文档 相 对应 的 文档 然后 我们 将 使用 
原始 文档 和 实际 文本文档 为 给定 的 文档 创建 
摘要 在 这里 机器 不会 得到 关于 ML 算法 生成 
的 摘要 是 对 还是 错 的 任何 反馈 我们 
还 将 看到 一个 计算机 视觉 应用 程序 的 例子 
对于 图像识别 我们 将 一些 卡通 人物 的 未 标记 
的 图像 数据集 输入 机器 我们 期望 机器学习 如何 对 
每个 角色 进行 分类 当 我们 提供 一个 卡通 人物 
的 未 看到 的 图像 时 它 应该 识别 该 
角色 并 将该 图像 放入 T 中 他 是由 机器 
本身 产生 的 适当 的 类 8 . 1.4 强化 
学习 第三 种 类型 的 ML 是 强化 学习 在 
这里 ML 算法 不会 在 每次 预测 之后 立即 给 
您 反馈 但是 如果 ML 模型 实现 了 它 的 
目标 它 会 生成 反馈 这种 类型 的 学习 主要 
用于 机器人 领域 并 开发 智能 机器人 来 玩游戏 强化 
学习 与 使用 试 错法 与 环境 交互 的 思想 
相联系 为了 学习 基础知识 让 我们 举 个 例子 比如说 
你 想做 一个 在 象棋 上 打败 人类 的 机器人 
这种 机器 人 只有 在 赢得 比赛 后 才会 收到 
反馈 最近 谷歌 Alphago 击败 了 世界 上 最好 的 
围棋 玩家 如果 您 想 了解 更多 信息 请参阅 以下 
链接 https / / techcrunch . com / 2017 / 
05/24 / alphago beats planets best human go player ke 
jie / . 我 知道 你 一定 有 兴趣 了解 
每 种 类型 的 ML 之间 的 区别 所以 在 
阅读 下 一段 时 要注意 对于 有 监督 的 学习 
你 会在 每一步 或 每一个 预测 之后 得到 反馈 在 
强化 学习 中 只有 当 我们 的 模型 达到 目标 
时 我们 才 会 收到 反馈 在 无 监督 的 
学习 中 我们 永远 不会 得到 反馈 即使 我们 实现 
了 我们 的 目标 或者 我们 的 预测 是 正确 
的 在 强化 学习 中 它 与 现有 的 环境 
相互 作用 使用 试 错法 而 其他 两种 方法 不 
适用 试 错法 在有 监督 的 学习 中 我们 将 
使用 有 标记 的 数据 而在 无 监督 的 学习 
中 我们 将 使用 无 标记 的 数据 在 强化 
学习 中 涉及 到 一 系列 的 目标 和 决策 
过程 您 可以 参考 . 4 从这 一节 开始 你 
将 学到 很多 新的 东西 如果 你 一 开始 不 
理解 一些 术语 那么 不要 担心 请 放心 我 将在 
本章 中 实际 地 解释 每 一个 概念 所以 让 
我们 开始 了解 使用 ML 的 NLP 应用 程序 的 
开发 步骤 8.2 自然语言 处理 应用 的 开发 步骤 在 
本节 中 我们 将 讨论 使用 ML 算法 开发 NLP 
应用 程序 的 步骤 这些 步骤 因 域 而异 对于 
NLP 应用程序 数据 可视化 并 没有 发挥 那么 重要 的 
作用 而对 分析 应用 程序 的 数据 可视化 将给 您 
带来 很多 洞察 因此 它 将从 应用程序 更改 为 应用 
程序 从域/nr 更改 为 域 这里 我 的 重点 是 
NLP 域 和 NLP 应用程序 当 我们 查看 代码 时 
我 肯定 会 记得 我 在 这里 描述 的 步骤 
以便 您 可以 连接 这些 点 我 将 开发 步骤 
分为 两个 版本 第一 个 版本 考虑到 它 是 NLP 
应用程序 开发 的 第一 个 迭代 第二 个 版本 将 
帮助 您 完成 NLP 应用程序 开发 的 第一 次 迭代 
之后 可以 考虑 的 可能 步骤 参见 . 5 Alt 
https / / img blog . csdnimg . cn / 
2 0 1 9 0 2 0 2 2 2 
5 1 4 7 3 5 2 . png 8 
. 2.1 第一 次 迭 代时 的 开发 步骤 首先 
我们 将 了解 在 使用 ML 开发 第一 个 版本 
的 NLP 应用程序 时 通常 可以 使用 的 步骤 在 
我 的 解释 过程 中 我 将 参考 . 6 
以便 您 正确理解 1 这个 版本 的 第一步 是 理解 
您 的 问题 陈述 应用程序 需求 或者 您 正试图 解决 
的 目标 2 第二步 是 获取 解决 目标 所需 的 
数据 或者 如果 有 数据集 则 尝试 找出 数据集 包含 
的 内容 和 构建 NLP 应用程序 所需 的 内容 如果 
您 需要 一些 其他 数据 那么 首先 问问 您 自己 
您 能在 可用 数据集 的 帮助 下 派生 出 子 
数据 属性 吗 如果 是 那么 可能 不 需要 获取 
另一个 数据集 但是 如果 不是 那么 尝试 获取 一个 可以 
帮助 您 开发 NLP 应用 程序 的 数据 集 3 
第三步 是 考虑 您 希望 得到 什么样 的 最终 结果 
并 据此 开始 探索 数据集 做 一些 基本 的 分析 
4 第四步 是 在对 数据 进行 一般性 分析 之后 可以 
对 其 应用 预处理 技术 5 第五步 是从 预处理 数据 
中 提取 特征 作 为特征 工程 的 一部分 6 第六种 
是 使用 统计技术 可以 可视化 特征值 这是 NLP 应用 程序 
的 可选 步骤 7 第七步 是 为 您 自己 的 
基准 构建 一个 简单 的 基本 模型 8 最后 但 
并非 最 不 重要 的 是 评估 基本 模型 如果 
它 符合 标准 那么 很好 否则 您 需要 更多 的 
迭代 并且 需要 遵循 另 一个 版本 我 将 在下 
一节 中 描述 这个 版本 8 . 2.2 从 第二 
次 到 第 N 次 迭代 的 开发 步骤 我们 
已经 看到 了 您 在 第一次 迭代 中 可以 采取 
的 步骤 现在 我们 将 看到 如何 执行 第二 次 
迭代 以便 提高 模型 的 准确性 和 效率 在 这里 
我们 还 试图 使 我们 的 模型 尽可能 简单 所有 
这些 目标 都将 是 这个 开发 版本 的 一部分 现在 
我们 将 看到 在 第一 次 迭代 之后 可以 遵循 
的 步骤 有关 基本 理解 请 参见 . 7 第二 
次 迭代 的 一些 基本 步骤 如下 1 在 第一 
次 迭代 之后 您 已经 构建 了 一个 模型 现在 
您 需要 改进 它 我 建议 您 尝试 不同 的 
ML 算法 来 解决 相同 的 NLP 应用程序 并 比较 
其 准确性 根据 准确度 选择 最佳 的 三种 ML 算法 
这将 是 第一 步 2 作为 第二 步 通常 您 
可以 对 每个 选定 的 ML 算法 应用 超 参数 
调整 以 获得 更好 的 精度 3 如果 参数 优化 
对 您 没有 太大 的 帮助 那么 您 需要 真正 
专注 于 特性 工程 部分 这将 是 您 的 第三 
步 4 目前 特征 工程 主要 有 两个 部分 特征提取 
和 特征选择 所以 在 第一 次 迭代 中 我们 已经 
提取 了 特征 但是 为了 优化 我们 的 ML 模型 
我们 需要 进行 特征选择 我们 将 在 本章 后面 介绍 
所有 的 特性 选择 技术 5 在 特性 选择 中 
您 基本上 选择 那些 特性 变量 或 数据 属性 这些 
特性 变量 或 数据 属性 是 非常 关键 的 或者 
对 获得 结果 贡献 很大 因此 我们 只 考虑 重要 
的 特性 并 删除 其他 特性 6 您 还 可以 
删除 异常值 执行 数据 规范化 并对 输入 数据 应用 交叉 
验证 这将 帮助 您 改进 ML 模型 7 在 执行 
了 所有 这些 技巧 之后 如果 您 没有 得到 准确 
的 结果 那么 您 需要 花费 一些 时间 来 获得 
新 特性 并 使用 它们 8 您 可以 重复 前面 
的 所有 步骤 直到 获得 满意 的 结果 这 就是 
如何 处理 NLP 应用 程序 的 开发 您 应该 观察 
您 的 结果 然后 在 下 一个 迭代 中 采取 
明智 的 必要 的 步骤 在 你 的 分析 中 
要 聪明 考虑 所有 的 问题 然后再 重申 解决 它们 
如果 你 不彻底 分析 你 的 结果 那么 重复 永远 
不会 帮助 你 所以 保持 冷静 明智 地 思考 并 
重申 不用 担心 当 我们 使用 ML 算法 开发 NLP 
应用程序 时 我们 将 看到 前面 的 过程 如果 你 
是 研究 方面 的 那么 我 强烈 建议 你 理解 
ML 算法 背后 的 数学 知识 但是 如果 你 是 
一个 初学者 并且 不太熟悉 数学 那么 你 可以 阅读 ML 
库 的 文档 那些 介于 这 两个 区域 之间 的 
人 试着 找出 数学知识 然后 实现 它 现在 是 时候 
深入 了解 ML 世界 学习 一些 真正 伟大 的 算法 
了 8.3 机器学习 算法 和 其他 概念 在 这里 我们 
将 介绍 最 广泛 使用 的 NLP 域 的 ML 
算法 我们 将 根据 ML 的 类型 来 研究 算法 
首先 我们/r 将/d 从有/nr 监督/vn 的/uj ML/w 算法/n 开始/v 然后 
是 无 监督 的 ML 算法 最后 是 半 监督 
的 ML 算法 在 这里 我们 将 了解 算法 及其 
背后 的 数学 我会 保持 简单 让 那些 不 是 
来自 一个 强大 的 数学 背景 可以理解 算法 背后 的 
直观 概念 之后 我们 将 看到 如何 实际 地 使用 
这些 算法 来 开发 一个 NLP 应用程序 我们 将 开发 
一个 很酷 的 NLP 应用程序 它 将 帮助 您 理解 
算法 而 不会 产生 任何 混淆 那么 我们 开始 吧 
8 . 3.1 有 监督 机器学习 方法 我们 在 本章 
前面 看到 了 关于 受 监督 机器学习 的 介绍 我们/r 
看到/v 和/c 使用/v 的/uj 任何/r 技术/n 和/c 数据集/i 都/d 包括/v 
数据/n 集中/v 已经/d 给出/v 的/uj 结果/n 结果 或 标签 所以 
这 意味着 无论 何时 只要 有 一个 标记 的 数据集 
就 可以 使用 受 监控 的 ML 算法 在 开始 
使用 算法 之前 我 将 介绍 两个 主要 的 监控 
ML 算法 概念 这也 将 帮助 您 决定 选择 哪种 
算法 来 解决 NLP 或 任何 其他 与 数据 科学 
相关 的 问题 回归 分类 回归 回归 是 一个 统计 
过程 用来 估计 变量 之间 的 关系 假设 你 有 
一堆 变量 你 想 找出 它们 之间 的 关系 首先 
你 需要 找 出 哪些 是 因变量 哪些 是 自变量 
回归分析 有助于 理解 因变量 如何 改变 其 行为 或 自变量 
给 定值 的 值 在 这里 因变量 依赖于 自变量 的 
值 而 自变量 则 采用 不 依赖 于 其他 变量 
的 值 让 我们 举 一个 例子 来给 你 一个 
清晰 的 理解 如果 你 有一个 数据集 有 一个人 的 
身高 你 需要 根据 身高 来 决定 体重 那么 这个 
数据集 是 有 监控 的 ML 你 的 数据 集中 
已经 有 年龄 了 所以 你 有 两个 属性 也 
称为 变量 高度 和 重量 现在 你 需要 根据 给定 
的 高度 来 预测 重量 所以 想 几秒钟 让 我 
知道 哪个 数据 属性 或 变量 是 依赖 的 哪个 
是 独立 的 我 希望 你 有 一些 想法 所以 
让 我 现在 回答 这里 权重 是 依赖于 可变 高度 
的 相关 数据 属性 或 变量 高度 是 自变量 自变量 
也 称为 预测器 因此 如果在/i 因变量/n 和/c 自变量/l 之间/f 有/v 
某种/r 映射/v 或/c 关系/n 那么 您 也 可以 预测 任何 
给定 高度 的 权重 请注意 当 输出 或 因变量 取 
连续 值 时 使用 回归 方法 在 我们 的 示例 
中 重量 可以 是 任何 值 例如 20 千克 20.5 
千克 20.6 千克 60 千克 等等 对于 其他 数据集 或 
应用 程序 因变量 的 值 可以 是 任何 实数 参见 
. 8 分类 在 这 一节 中 我们 将 讨论 
受 监督 的 ML 的 另一个 主要 概念 即 分类 
技术 这也 被 称为 统计 分类 统计 分类 用于 确定 
给定 新 观察 的 类别 所以 我们 有 许多 类别 
可以 把 新的 观察 结果 放 在其中 但是 我们 不会 
盲目 地 选择 任何 类别 但是 我们 将 使用 给定 
的 数据集 并且 基于 此 数据集 我们 将 尝试 为 
新 观察 确定 最 适合 的 类别 并将 这一 类别 
或 类别 的 观察 让 我们 以 NLP 域 本身 
为例 您 有 一个 包含 大量 电子邮件 的 数据集 这些 
电子邮件 已经有 了 一个 类 标签 即 垃圾邮件 或非 垃圾邮件 
所以 我们 的 数据集 分为 两类 垃圾 邮件 和非/nr 垃圾邮件 
现在 如果 我们 收到 一封 新的 电子邮件 那么 我们 可以 
将 特定 的 电子 邮件 分类 为 垃圾 邮件 类 
还是 非 垃圾邮件 类 答案 是 肯定 的 因此 为了 
对 新的 电子邮件 进行 分类 我们 使用 数据集 和 ML 
算法 为 新邮件 提供 最 适合 的 类 实现 分类 
的 算法 称为 分类器 有时 术语 分类器 也指 由 分类器 
算法 实现 的 将 输入 数据 映 射到 类别 的 
数学 函数 请注意 这 一点 有助于 您 识别 回归 和 
分类 之间 的 差异 在 分类 中 输出 变量 采用 
基本上 是 离散 或 分类 值 的 类 标签 在 
回归 中 我们 的 输出 变量 取 一个 连续 值 
参见 . 9 既然 我们 已经 了解 了 回归 和 
分类 的 概念 那么 让 我们 经常 使用 的 基本 
术语 同时 解释 专门 用于 分类 的 ML 算法 实例 
这 被 称为 输入 通常 是 向量 的 形式 这些 
是 属性 的 向量 在 pos tagger 示例 中 我们 
使用 从 每个 单词 派生 的 特性 并 使用 scikit 
learns API dictvectorizer 将 它们 转换 为 向量 向 量值 
被 输入 到 ML 算法 中 因此 这些 输入 向量 
就是 实例 概念 概念 是 指 将 输入 映 射到 
输出 的 函数 因此 如果 我们 有 一个 电子 邮件 
内容 并且 我们 正在 努力 查明 该 电子邮件 内容 是 
垃圾 邮件 还 是非 垃圾邮件 那么 我们 必须 关注 实例 
或 输入 中的 某些 特定 参数 然后 生成 结果 如何 
从 某些 输入 中 识别 某些 输出 的 过程 称为 
概念 例如 你 有 一些 关于 人 身高 的 数据 
在 看到 数据 后 你 可以 决定 这个 人 是 
高 还是 矮 在 这里 这个 概念 或 函数 帮助 
你 找到 一个 给定 输入 或 实例 的 输出 所以 
如果 我 把 它 放在 数学 格式 中 那么 这个 
概念 就 是 一个 世界 上 的 一个 对象 和 
一个 集合 中的 成员 之间 的 映射 目标 概念 目标 
概念 指 的 是 实际 的 答案 或 具体 的 
功能 或 我们 试图 找到 的 某些 特定 的 概念 
作为 人类 我们 在 头脑 中 已经 了解 了 很多 
概念 例如 通过 阅读 电子邮件 我们 可以 判断 它 是 
垃圾 邮件 还 是非 垃圾邮件 如果 您 的 判断 是 
正确 的 那么 您 就 可以 得到 实际 的 答案 
你 知道 什么 叫 垃圾邮件 什么 不是 但 除非 我们 
把 它 写在 某个 地方 否则 我们 不 知道 它 
是 对 还是 错 如果 我们 注意 到 数据 集中 
每个 原始数据 的 这些 实际 答案 那么 我们 就 更容易 
确定 哪些 电子邮件 应被 视为 垃圾邮件 哪些 不 应被 视为 
垃圾邮件 这将 帮助 您 找到 新 实例 的 实际 答案 
假设 类 是 可以 帮助 我们 对 实例 进行 分类 
的 所有 可能 函数 的 类 我们 刚刚 看到 了 
目标 概念 我们 试图 找出 一个 特定 的 函数 但是 
这里 我们 可以 想到 所有 可能 的 和 潜在 的 
函数 的 子集 这些 函数 可以 帮助 我们 找出 分类 
问题 的 目标 概念 在 这里 我 要 指出 的 
是 我们 看到 分类 任务 的 这个 术语 所以 不要 
考虑 x2 函数 因为 它 是 一个 线性函数 我们 执行 
的 是 分类 而不是 回归 训练 数据集 在 分类 中 
我们 试图 找到 目标 概念 或 实际 答案 现在 我们 
如何 才能 得到 最终 的 答案 呢 为了 使用 ML 
技术 得到 最终 的 答案 我们 将 使用 一些 样 
本集 训练 集 或 训练 数据集 来 帮助 我们 找到 
实际 的 答案 让 我们 看看 训练 集 是 什么 
训练 集 包含 与 标签 配对 的 所有 输入 监督 
分类 问题 需要 用 实际 答案 或 实际 输出 标记 
的 训练 数据集 所以 我们 不 仅仅 是 将 我们 
的 知识 传递 给 机器 告诉 它 什么 是 垃圾 
邮件 什么 是非 垃圾邮件 我们 还向 机器 提供 了 很多 
例子 比如 这是 垃圾邮件 这 是非 垃圾邮件 等等 因此 对于 
机器 来说 很容易 理解 目标 概念 ML 模型 我们 将 
使用 训练 数据集 并 将 这些 数据 输入 到 ML 
算法 中 然后 ML 算法 将 尝试 使用 大量 的 
训练 示例 来 学习 这个 概念 并 生成 输出 模型 
此 输出 模型 稍后 可 用于 预测 或 决定 给定 
的 新邮件 是否 为 垃圾 邮件 生成 的 输出 称为 
ML 模型 我们 将 使用 一个 生成 的 ML 模型 
并将 新邮件 作为 输入 这个 ML 模型 将 生成 关于 
给定 邮件 是否 属于 垃圾邮件 类别 的 答案 候选者 候选者 
是 我们 的 ML 模型 为 新 示例 告诉 我们 
的 潜在 目标 概念 所以 你 可以 说 候选者 是 
机器 的 预测 目标 概念 但是 我们 不 知道 这里 
的 候选者 的 预测 或 生成 的 输出 是否 是 
正确 的 答案 那么 让 我们 举 个 例子 我们 
向 机器 提供 了 许多 电子邮件 示例 机器 可以 概括 
垃圾 邮件 的 概念 而 不是 垃圾 邮件 我们 将 
提供 一封 新的 电子邮件 我们 的 ML 模型 会 说 
它 不是 垃圾 邮件 但是 我们 需要 检查 我们 的 
ML 模型 的 答案 是 对 还是 错 这个 答案 
被 称为 候选人 如何 检查 ML 模型 生成 的 答案 
是否 与 目标 概念 匹配 为了 回答 您 的 问题 
我 将 介绍 下 一个 术语 即 测试 集 测试 
集 测试 集 类似于 训练 数据集 我们 的 训练 数据集 
包含 带有 垃圾邮件 或非 垃圾 邮件 等 标签 的 电子邮件 
因此 我 将 采用 被 认为 是 候选 者 的 
答案 并且 我们 将 检查 我们 的 测试 集 是否 
是非 垃圾 邮件 或 垃圾邮件 我们 将 把 我们 的 
答案 与 测试 集 的 答案 进行 比较 并 试图 
找出 候选人 的 答案 是 正确 的 还是 错误 的 
假设 不是 垃圾 邮件 是 正确 的 答案 现在 您 
将 收到 另一 封 电子邮件 ML 模型 将 再次 生成 
一个 非 垃圾邮件 答案 我们 将 用 我们 的 测试 
集 再次 检查 这个 问题 这次 ML 模型 生成 了 
一个 错误 的 答案 邮件 实际上 是 垃圾 邮件 但是 
ML 模型 将 其 错误 地 分类 为非 垃圾邮件 类别 
因此 测试 集 帮助 我们 验证 MLModel 请注意 训练 和 
测试 集 不应 相同 这 是因为 如果 您 的 机器 
使用 训练 数据集 学习 概念 并且 在 训练 数据 集上 
测试 MLModel 那么 您 就 没有 公平 地 评估 ML 
模型 这在 ML 中 被 认为 是 作弊 因此 您 
的 训练 数据集 和 测试 集 应该 总是 不同 的 
测试 集 是 您 的 机器 从未 见过 的 数据 
集 我们 这样 做 是因为 我们 需要 检查 机器 的 
能力 看看 给定 的 问题 能 推广 多少 这里 广义 
的 意思 是 ML 模型 如何 对 未知 和 未知 
的 例子 做出 反应 如果 你 还是 很 困惑 那我/nr 
再举 一个 例子 你 是个 学生 老师 教 你 一些 
事实 给 你 举 了 一些 例子 起初 你 只是 
记住 了 事实 为了 检查 你 是否 有 正确 的 
概念 老师 会给 你 一个 测试 并给 你 一个 新 
的 例子 在 那里 你 需要 应用 你 的 学习 
如果 您 能够 将 您 的 学习 完美 地 应用 
到 测试 中的 新 示例 中 那么 您 实际上 就 
得到 了 这个 概念 这 证明 了 我们 可以 概括 
出 老师 教过 的 概念 我们 用 机器 做 同样 
的 事情 ML 算法 我们 已经 充分 了解 了 ML 
的 基本 概念 现在 我们 将 探讨 ML 算法 首先 
我们 将 看到 在 NLP 域中 主要 使用 的 监控 
ML 算法 我 不 打算 在 这里 介绍 所有 受 
监控 的 ML 算法 但 我 将 解释 在 NLP 
领域 中 最 广泛 使用 的 那些 算法 在 NLP 
应用 程序 中 我们 主要 使用 各种 ML 技术 执行 
分类 所以 在 这里 我们 主要 关注 的 是 算法 
的 分类 类型 其他 领域 如 分析 使用 各种 类型 
的 线性 回归 算法 以及 分析 应用程序 但是 我们 不会 
去 看 这些 算法 因为 这 本书 是 关于 NLP 
域 的 由于 线性 回归 的 一些 概念 有助于 我们 
理解 深度 学习 技术 我们 将 通过 第 9 章 
NLP 和 NLG 问题 的 深度 学习 中的 示例 详细 
介绍 线性 回归 和 梯度 下降 我们 将 使用 各种 
算法 开发 一些 NLP 应用程序 这样 您 就 可以 看到 
算法 的 工作 原理 以及 NLP 应用程序 如何 使用 ML 
算法 开发 我们 将 研究 垃圾邮件 过滤 等 应用 程序 
逻辑 回归 我 知道 你 一定 很 困惑 为什么 我 
把 逻辑 回归 放在 分类 类别 中 让 我 告诉 
你 这 只是 这个 算法 的 名字 但 它 是 
用来 预测 离散 输出 的 所以 这个 算法 属于 分类 
类别 对于 这个 分类 算法 我 将给 你 一个 逻辑 
回归 算法 是 如何 工作 的 概念 我们 将 看看 
与之 相关 的 一些 基础 数学 然后 我们 将 查看 
垃圾邮件 过滤 应用程序 首先 我们 将 考虑 二进制 类 如 
垃圾邮件 与否 好坏 得失 0 或 1 等 以 了解 
算法 及其 应用 假设 我 想将 电子邮件 分类 为 垃圾 
邮件 而 不是 垃圾 邮件 类别 垃圾 邮件 和非/nr 垃圾 
邮件 是 离散 的 输出 标签 或 目标 概念 我们 
的 目标 是 预测 新 电子邮件 是否 是 垃圾 邮件 
非 垃圾邮件 也 被称为 火腿 为了 构建 这个 NLP 应用程序 
我们 将 使用 逻辑 回归 首先 让 我们 了解 算法 
的 技术性 在 这里 我/r 以/p 一种/m 非常/d 简单/a 的/uj 
方式/n 陈述/nr 与/p 数学/n 和/c 这个/r 算/v 法/l 有关/vn 的/uj 
事实/n 理解 该 算法 的 一般 方法 如下 如果 你 
知道 ml 的 某些 部分 那么 你 可以 把 这些 
点 连接起来 如果 你 是 ml 的 新手 那么 不要 
担心 因为 我们 将 了解 每 个 部分 我们 正在 
定义 我们 的 假设 函数 帮助 我们 生成 目标 输出 
或 目标 概念 我们 定义 了 成本 函数 或 误差函数 
我们 选择 误差函数 的 方式 是 我们 可以 导出 误差函数 
的 偏 导数 这样 我们 就 可以 方便 地 计算 
梯度 下降 我们 正 努力 将 误差 降到 最低 以便 
生成 更 准确 的 标签 并对 数据 进行 准确 分类 
在 统计学 中 逻辑 回归 又 称为 逻辑 回归 或 
逻辑 模型 该 算法 主要 是 作为 一个二元 类 分类器 
来 使用 的 这 意味着 应该 有 两个 不同 的 
类 来 对 数据 进行 分类 二 元逻辑 模型 用于 
估计 二元 响应 的 概率 它 基于 一个 或 多个 
预测 因子 或 独立 变量 或 特征 生成 响应 这 
是 一个 ML 算法 在 深度 学习 中 也 使用 
了 基本 的 数学 概念 首先 我 想 解释 一下 
为什么 这个 算法 被称为 逻辑 回归 原因 是 该 算法 
使用 了 一个 逻辑 函数 或 sigmoid 函数 逻辑 功能 
和 sigmoid 函数 是 同义词 我们 用 sigmoid 函数 作为 
假设 函数 你 所说 的 假设 函数 是 什么 意思 
好吧 正如 我们 前面 看到 的 机器 必须 学习 数据 
属性 和 给定 标签 之间 的 映射 这样 它 才能 
预测 新 数据 的 标签 如果 机器 通过 数学 函数 
学习 这种 映射 就 可以 实现 这 一点 数学 函数 
是 机器 用来 分类 数据 和 预测 标签 或 目标 
概念 的 假设 函数 我们 想 要 构建 一个 二进制 
分类器 所以 我们 的 标签 要么 是 垃圾 邮件 要么 
不是 所以 在 数学 上 我 可以 为 正常 指定 
0 或者 不 为 垃圾 邮件 指定 1 或者 反之亦然 
这些 数学上 指定 的 标签 是 我们 的 因变量 现在 
我们 需要 输出 标签 为 0 或 1 数学上 标签 
是 y 和yε/nr 0 1 所以 我们 需要 选择 一个 
假设 函数 将 我们 的 输出 值 转换 为 0 
或 1 logistic 函数 或 sigmoid 函数 就是 这样 做 
的 这 也是 logistic 回归 使用 sigmoid 函数 作为 假设 
函数 的 主要 原因 从上 图中 您 可以 发现 以下 
事实 如果 z 值 大于 或 等于 零 那么 logistic 
函数 给出 输出 值 1 如果 z 值 小于 零 
那么 logistic 函数 将 生成 输出 0 这个 sigmoid 函数 
如何 表示 为 假设 函数 使用 假设 方程 机器 实际上 
尝试 学习 输入 变量 或 输入 特征 和 输出 标签 
之间 的 映射 我们 来 谈谈 这个 假设 函数 的 
解释 你 能 想出 预测 类 标签 的 最佳 方法 
吗 根据 我 的 观点 我们 可以 使用 概率 概念 
来 预测 目标 类 标签 我们 需要 为 这 两个 
类 生成 概率 并且 任何 具有 高 概率 的 类 
都将 被 分配 给 特性 的 特定 实例 在 二进制 
分类 中 y 或 目标 类 的 值 要么 是 
零 要么 是 一 如果 您 熟悉 概率 那么 可以 
表示 . 16 中 给出 的 概率 方程 逻辑 回归 
的 成本 或 误差函数 首先 让 我们 了解 成本 函数 
或 错误 函数 在 ML 中 成本 函数 损失 函数 
或 误差函数 是 一个 非常 重要 的 概念 因此 我们 
将 了解 成本 函数 的 定义 成本 函数 用于 检查 
我们 的 ML 分类器 执行 的 准确性 在 我们 的 
训练 数据 集中 我们/r 有/v 数据/n 和/c 标签/n 当 我们 
使用 假设 函数 并 生成 输出 时 我们 需要 检查 
离 实际 预测 的 距离 如果 我们 预测 实际 的 
输出 标签 那么 我们 的 假设 函数 输出 和 实际 
标签 之间 的 差异 是 零 或 最小 的 如果 
我们 的 假设 函数 输出 和 实际 标签 不相同 那么 
我们 在 它们 之间 有 很大 的 差异 如果 电子邮件 
的 实际 标签 是 垃圾 邮件 即 垃圾邮件 并且 我们 
的 假设 函数 也 生成 结果 1 那么 实际 目标值 
和 预测 输出 值 之间 的 差异 为零 因此 预测 
中 的 错误 也 为零 如果 我们 的 预测 输出 
为 1 而 实际 输出 为零 那么 我们 的 实际 
目标 概念 和 预测 之间 的 误差 最大 所以 在 
我们 的 预测 中有 最小 的 误差 是 很 重要 
的 这是 误差函数 的 基本 概念 我们 将 在 一段 
时间 内 学习 数学 有 几种 类型 的 错误 函数 
可用 如 r2 错误 平方和 错误 等 根据 ML 算法 
和 假设 函数 我们 的 误差函数 也 发生 了 变化 
逻辑 回归 的 误差函数 是 什么 什么 是 θ 如果 
我 需要 选择 θ 的 某个 值 我 如何 处理 
它 所以 在 这里 我会 给 出 所有 的 答案 
让 我 给 你 一些 线性 回归 的 背景 知识 
在 线性 回 归中 一般 采用 平方 误差 和或 残差 
之和 作为 成本 函数 在 线性 回 归中 我们 试图 
生成 最 适合 我们 的 数据 集 的 线 在 
前面 的 例子 中 给定 高度 我 想 预测 重量 
我们 首先 画 一条线 测量 从 每个 数据 点到 线 
的 距离 我们 将 平方 这些 距离 求和 并 尽量 
减小 误差函数 参见 . 17 您 可以 看到 每 个数 
据点 与 线 之间 的 距离 是 用 小 的 
垂直线 表示 的 我们 将 把 这些 距离 平方 然后 
求和 我们 将 使用 这个 错误 函数 我们 已经 生成 
了 关于 m 线 和 截距 b 的 斜率 的 
偏 导数 在 . 17 中 b 约为 0.9 m 
约为 三分之二 每次 计算误差 更新 m 和b的/nr 值 生成 最佳 
拟 合线 更新 m 和b的/nr 过程 称为 梯度 下降 利用 
梯度 下 降法 对 m 和b/nr 进行 更新 使 误差函数 
具有 最小 的 误差值 从而 生成 最佳 拟 合线 梯度 
下降 给 我们 一个 需要 画线 的 方向 您/zg 可以/c 
在/p 第/m 9/m 章/q 深入/v 学习/v NLP/w 和/c NLG/w 问题/n 
中/f 找到/v 一个/m 详细/ad 的/uj 例子/n 因此 通过 定义 误差函数 
并 生成 偏 导数 我们 可以 应用 梯度 下降 算法 
帮助 我们 最小化 误差 或 成本 函数 现在 回 到 
主要 问题 我们 可以 使用 误差函数 进行 逻辑 回归 吗 
如果 你 对 函数 和 微积分 很 了解 那么 你 
的 答案 可能 是 否定 的 这是 正确 的 答案 
让 我 为 那些 不 熟悉 函数 和 微积分 的 
人 解释 一下 在 线性 回 归中 我们 的 假设 
函数 是 线性 的 所以 我们 很 容易 计算 平方 
误差 之和 但是 在 这里 我们 将 使用 一个 非 
线性函数 sigmoid 函数 如果 你 应用 我们 在 线性 回归 
中 使用 的 相同 的 函数 结果 会 不好 因为 
如果 你 使用 的 是 sigmoid 函数 输入 平方 误差函数 
之和 并 尝试 可视化 所有 可能 的 值 那么 你 
会 得到 一条 非 凸 曲线 在 ML 中 我们 
主要 使用 能够 提供 凸 曲线 的 函数 因为 我们 
可以 使用 梯度 下降 算法 来 最小化 误差函数 并 达到 
全局 最小值 如 . 18 所示 非 凸 曲线 有 
许多 局部 极小值 因此 要 达到 全局 极小值 是 非常 
困难 和 耗时 的 因为 需要 应用 二阶 或 第 
n 阶 优化 来 达到 全局 最小值 而在 凸 曲线 
中 您 可以 确定 地 快速 地 达到 全局 最小值 
所以 如果 我们 把 我们 的 sigmoid 函数 代入 平方 
误差 之和 就 得到 了 非 凸函数 这样 我们 就 
不会 定义 线性 回归 中 使用 的 相同 的 误差函数 
我们 需要 定义 一个 不同 的 凸 优化 函数 这样 
我们 就 可以 应用 梯度 下降 算法 并 生成 一个 
全局 最小值 我们 将 使用 称为 可能性 的 统计 概念 
为了 推导 似 然 函数 我们 将 使用 . 16 
中 给出 的 概率 方程 并 考虑 训练 数据 集中 
的 所有 数 据点 所以 我们 可以 得到 下面 的 
方程 叫做 似 然 函数 参见 . 19 现在 为了 
简化 导数 过程 我们 需要 将 似 然 函数 转换 
为 单调 递 增函数 这 可以 通过 取 似 然 
函数 的 自然 对 数来 实现 称为 对数 似 然 
这个 对数 可能性 是 我们 进行 逻辑 回归 的 成本 
函数 参考 . 20 中的 以下 方程式 我们 将 绘制 
成本 函数 并 了解 它 为 我们 提供 的 好处 
在 x 轴上 我们 有 假设 函数 假设 函数 的 
范围 是 0 到 1 所以 我们 在 x 轴 
上有 这两点 从 第一 个 案例 开始 其中 y = 
1 您 可以 在 . 21 中 看到 右上方 生成 
的 曲线 如果 你 看 任何 一个 对数函数 图 它 
会 像 误差函数 y = 0 的 图 这里 我们 
翻转 曲线 因为 我们 有 一个 负号 然后 你 得到 
我们 为 y = 1 值 绘制 的 曲线 在 
. 21 中 您 可以 看到 日志 图 以及 . 
22 中的 翻转 图 这里 我们 对 值 0 和1/nr 
感兴趣 因此 我们 考虑 . 21 中 描述 的 图 
的 那 部分 这个/r 成本/n 函数/n 有/v 一些/m 有趣/a 和/c 
有用/v 的/uj 特性/n 如果 预测 标签 或 候选 标签 与 
实际 目标 标签 相同 则 成本 为零 如果 假设 函数 
预测 h θ x = 1 那么 成本 = 0 
如果 h θ x 趋向于 0 这 意味着 如果 它 
更 接近 于零/nr 那么 成本 函数 就会 放大 到 ∞ 
对于 y = 0 您 可以 看到 . 21 中左 
上方 的 图 这种 情况下 的 条件 也 具有 与 
我们 之前 看到 的 相同 的 优点 和 特性 当 
实际 值 为 0 且 假设 函数 预测 为 1时 
它 将 变为 ∞ 如果 假设 函数 预测 为 0 
实际 目标 也为 0 则 成本 = 0 现在 我们 
来 看看 为什么 要 选择 这个 成本 函数 原因 是 
这个 函数 使 我们 的 优化 变得 容易 因为 我们 
将 使用 最大 对数 似 然 因为 它 有一个 凸 
曲线 可以 帮助 我们 进行 梯度 下降 为了 应用 梯度 
下降 我们 需要 生成 相对于 θ 的 偏 导数 并 
生成 如 . 23 所示 的 方程 该 方程 用于 
更新 θ 的 参数值 定义 学习率 这 是 可以 用 
来 设置 算法 学习 或 训练 的 速度 或 速度 
的 参数 如果 你 把 学习率 设置 得 太高 那么 
算法 就 无法 学习 如果 你 把 它 设置 得 
太低 那么 训练 就 需要 很多 时间 所以 你 需要 
明智 地 选择 学习率 代码 实现 import math import numpy 
as np import pandas as pd from pandas import DataFrame 
from sklearn import preprocessing from sklearn . linear _ model 
import L o g i s t i c R 
e g r e s s i o n from 
sklearn . cross _ validation import train _ test _ 
split from numpy import loadtxt where from pylab import scatter 
show legend xlabel ylabeld \ Program Files \ Anaconda3 \ 
lib \ site packages \ sklearn \ cross _ validation 
. py 44 D e p r e c a 
t i o n W a r n i n 
g This module was deprecated in version 0.18 in favor 
of the model _ selection module into which all the 
refactored classes and functions are moved . Also note that 
the interface of the new CV iterators are different from 
that of this module . This module will be removed 
in 0.20 . This module will be removed in 0.20 
. D e p r e c a t i 
o n W a r n i n g # 
scale larger positive and values to between 1 1 depending 
on the largest # value in the data min _ 
max _ scaler = preprocessing . MinMaxScaler feature _ range 
= 1 1 df = pd . read _ csv 
data . csv header = 0 # clean up data 
df . columns = grade1 grade2 label x = df 
label . map lambda x float x . rstrip # 
formats the input data into two arrays one of independant 
variables # and one of the dependant variable X = 
df grade1 grade2 X = np . array X X 
= min _ max _ scaler . fit _ transform 
X Y = df label . map lambda x float 
x . rstrip Y = np . array Y # 
if want to create a new clean dataset # # 
X = pd . DataFrame . from _ records X 
columns = grade1 grade2 # # X . insert 2 
label Y # # X . to _ csv data2 
. csv # creating testing and training set X _ 
train X _ test Y _ train Y _ test 
= train _ test _ split X Y test _ 
size = 0.33 # train scikit learn model clf = 
L o g i s t i c R e 
g r e s s i o n clf . 
fit X _ train Y _ train L o g 
i s t i c R e g r e 
s s i o n C = 1.0 class _ 
weight = None dual = False fit _ intercept = 
True intercept _ scaling = 1 max _ iter = 
100 multi _ class = ovr n _ jobs = 
1 penalty = l2 random _ state = None solver 
= liblinear tol = 0.0001 verbose = 0 warm _ 
start = False print score Scikit learn clf . score 
X _ test Y _ test score Scikit learn 0 
. 8484848484848485 # visualize data uncomment show to run it 
pos = where Y = = 1 neg = where 
Y = = 0 scatter X pos 0 X pos 
1 marker = o c = b scatter X neg 
0 X neg 1 marker = x c = r 
xlabel Exam 1 score ylabel Exam 2 score legend Not 
Admitted Admitted show # # The sigmoid function adjusts the 
cost function hypotheses to adjust the algorithm proportionally for worse 
estimations def Sigmoid z G _ of _ Z = 
float 1.0 / float 1.0 + math . exp 1.0 
* z return G _ of _ Z # # 
The hypothesis is the linear combination of all the known 
factors x i and their current estimated coefficients theta i 
# # This hypothesis will be used to calculate each 
instance of the Cost Function def Hypothesis theta x z 
= 0 for i in range len theta z + 
= x i * theta i return Sigmoid z # 
# For each member of the dataset the result Y 
determines which variation of the cost function is used # 
# The Y = 0 cost function punishes high probability 
estimations and the Y = 1 it punishes low scores 
# # The punishment makes the change in the gradient 
of ThetaCurrent Average CostFunction Dataset greater def Cost _ Function 
X Y theta m sumOfErrors = 0 for i in 
range m xi = X i hi = Hypothesis theta 
xi if Y i = = 1 error = Y 
i * math . log hi elif Y i = 
= 0 error = 1 Y i * math . 
log 1 hi sumOfErrors + = error const = 1 
/ m J = const * sumOfErrors print cost is 
J return J # # This function creates the gradient 
component for each Theta value # # The gradient is 
the partial derivative by Theta of the current value of 
theta minus # # a learning speed factor aplha times 
the average of all the cost functions for that theta 
# # For each Theta there is a cost function 
calculated for each member of the dataset def Cost _ 
Function _ Derivative X Y theta j m alpha sumErrors 
= 0 for i in range m xi = X 
i xij = xi j hi = Hypothesis theta X 
i error = hi Y i * xij sumErrors + 
= error m = len Y constant = float alpha 
/ float m J = constant * sumErrors return J 
# # For each theta the partial differential # # 
The gradient or vector from the current point in Theta 
space each theta value is its own dimension to the 
more accurate point # # is the vector with each 
dimensional component being the partial differential for each theta value 
def Gradient _ Descent X Y theta m alpha new 
_ theta = constant = alpha / m for j 
in range len theta CFDerivative = Cost _ Function _ 
Derivative X Y theta j m alpha new _ theta 
_ value = theta j CFDerivative new _ theta . 
append new _ theta _ value return new _ theta 
# # The high level function for the LR algorithm 
which for a number of steps num _ iters finds 
gradients which take # # the Theta values coefficients of 
known factors from an estimation closer new _ theta to 
their optimum estimation which is the # # set of 
values best representing the system in a linear combination model 
def Logistic _ Regression X Y alpha theta num _ 
iters m = len Y for x in range num 
_ iters new _ theta = Gradient _ Descent X 
Y theta m alpha theta = new _ theta if 
x % 100 = = 0 # here the cost 
function is used to present the final hypothesis of the 
model in the same form for each gradient step iteration 
Cost _ Function X Y theta m print theta theta 
print cost is Cost _ Function X Y theta m 
Declare _ Winner theta # # This method compares the 
accuracy of the model generated by the scikit library with 
the model generated by this implementation def Declare _ Winner 
theta score = 0 winner = # first scikit LR 
is tested for each independent var in the dataset and 
its prediction is compared against the dependent var # if 
the prediction is the same as the dataset measured value 
it counts as a point for thie scikit version of 
LR scikit _ score = clf . score X _ 
test Y _ test length = len X _ test 
for i in range length prediction = round Hypothesis X 
_ test i theta answer = Y _ test i 
if prediction = = answer score + = 1 # 
the same process is repeated for the implementation from this 
module and the scores compared to find the higher match 
rate my _ score = float score / float length 
if my _ score scikit _ score print You won 
elif my _ score = = scikit _ score print 
Its a tie else print Scikit won . . print 
Your score my _ score print Scikits score scikit _ 
score initial _ theta = 0 0 alpha = 0.1 
iterations = 1000 Logistic _ Regression X Y alpha initial 
_ theta iterations cost is 0 . 6886958174712052 theta 0 
. 0 1 5 8 0 8 9 6 8 
9 7 7 2 1 7 0 1 2 0 
. 0 1 4 0 3 0 9 8 2 
2 0 0 2 4 9 2 7 3 cost 
is 0 . 6886958174712052 cost is 0 . 6886958174712052 cost 
is 0 . 4 5 0 4 3 9 2 
8 3 2 6 8 4 3 8 3 5 
theta 1 . 1446039323506159 1 . 030383323481578 cost is 0 
. 4 5 0 4 3 9 2 8 3 
2 6 8 4 3 8 3 5 cost is 
0 . 4 5 0 4 3 9 2 8 
3 2 6 8 4 3 8 3 5 cost 
is 0 . 3 7 2 1 0 3 9 
6 4 0 0 5 6 8 8 3 5 
theta 1 . 7920198800927762 1 . 6251057941038252 cost is 0 
. 3 7 2 1 0 3 9 6 4 
0 0 5 6 8 8 3 5 cost is 
0 . 3 7 2 1 0 3 9 6 
4 0 0 5 6 8 8 3 5 cost 
is 0 . 3 3 4 9 3 1 7 
4 2 9 0 9 7 1 3 0 6 
theta 2 . 2378078311381255 2 . 0381775708737533 cost is 0 
. 3 3 4 9 3 1 7 4 2 
9 0 9 7 1 3 0 6 cost is 
0 . 3 3 4 9 3 1 7 4 
2 9 0 9 7 1 3 0 6 cost 
is 0 . 3134393548415864 theta 2 . 5764517180022444 2.35358660097723 cost 
is 0 . 3134393548415864 cost is 0 . 3134393548415864 cost 
is 0 . 2995143683386589 theta 2 . 8487364478320787 2 . 
608155678935002 cost is 0 . 2995143683386589 cost is 0 . 
2995143683386589 cost is 0 . 2898100759552151 theta 3 . 0758031030008572 
2 . 8210921909376734 cost is 0 . 2898100759552151 cost is 
0 . 2898100759552151 cost is 0 . 2826976528686292 theta 3 
. 2700162725064694 3 . 0036648752998807 cost is 0 . 2826976528686292 
cost is 0 . 2826976528686292 cost is 0 . 2772893938976962 
theta 3 . 4392392975568247 3 . 163057635787686 cost is 0 
. 2772893938976962 cost is 0 . 2772893938976962 cost is 0 
. 2730601259267772 theta 3 . 588788716304762 3 . 3041402117668226 cost 
is 0 . 2730601259267772 cost is 0 . 2730601259267772 Its 
a tie Your score 0 . 8484848484848485 Scikits score 0 
. 8484848484848485 这 有一个 逻辑 回归 的 实现 您 可以 
在 SciKit 学习 库 中找到 它 与 给定 实现 的 
比较 垃圾邮件 过滤 垃圾邮件 过滤 是 一种 基本 的 NLP 
应用程序 使用 此 算法 我们 希望 建立 一个 ML 模型 
将 给定 邮件 分类 为 垃圾 邮件 或 正常 类 
所以 让 我们 制作 一个 垃圾 邮件 过滤 应用程序 在 
垃圾 邮件 过滤 中 我们 将 使用 scikit learn 的 
CountVectorizer API 学习 生成 特性 然后 使用 L o g 
i s t i c R e g r e 
s s i o n 进行 训练 import pandas as 
pd import numpy as np # read file into pandas 
using a relative path path = sms . tsv sms 
= pd . read _ table path header = None 
names = label message # examine the shape sms . 
shape 5572 2 # examine the first 10 rows sms 
. head 10 l a b e l m e 
s s a g e 0 h a m G 
o until jurong point crazy . . Available only . 
. . 1hamOk lar . . . Joking wif u 
oni . . . 2spamFree entry in 2 a wkly 
comp to win FA Cup fina . . . 3hamU 
dun say so early hor . . . U c 
already then say . . . 4hamNah I don t 
think he goes to usf he lives aro . . 
. 5spamFreeMsg Hey there darling it s been 3 week 
s n . . . 6hamEven my brother is not 
like to speak with me . . . . 7hamAs 
per your request Melle Melle Oru Minnamin . . . 
8spamWINNER As a valued network customer you have . . 
. 9spamHad your mobile 11 months or more U R 
entitle . . . # examine the class distribution sms 
. label . value _ counts ham 4825 spam 747 
Name label dtype int64 # convert label to a numerical 
variable sms label _ num = sms . label . 
map { ham 0 spam 1 } # check that 
the conversion worked sms . head 10 l a b 
e l m e s s a g e l 
a b e l _ num0hamGo until jurong point crazy 
. . Available only . . . 01hamOk lar . 
. . Joking wif u oni . . . 02spamFree 
entry in 2 a wkly comp to win FA Cup 
fina . . . 13hamU dun say so early hor 
. . . U c already then say . . 
. 04hamNah I don t think he goes to usf 
he lives aro . . . 05spamFreeMsg Hey there darling 
it s been 3 week s n . . . 
16hamEven my brother is not like to speak with me 
. . . . 07hamAs per your request Melle Melle 
Oru Minnamin . . . 08spamWINNER As a valued network 
customer you have . . . 19spamHad your mobile 11 
months or more U R entitle . . . 1 
# how to define X and y from the SMS 
data for use with COUNTVECTORIZER X = sms . message 
y = sms . label _ num print X . 
shape print y . shape 5572 5572 # split X 
and y into training and testing sets from sklearn . 
cross _ validation import train _ test _ split X 
_ train X _ test y _ train y _ 
test = train _ test _ split X y random 
_ state = 1 print X _ train . shape 
print X _ test . shape print y _ train 
. shape print y _ test . shape 4179 1393 
4179 1393 # import and instantiate CountVectorizer with the default 
parameters from sklearn . feature _ extraction . text import 
CountVectorizer # instantiate the vectorizer vect = CountVectorizer # learn 
training data vocabulary then use it to create a document 
term matrix vect . fit X _ train X _ 
train _ dtm = vect . transform X _ train 
# equivalently combine fit and transform into a single step 
X _ train _ dtm = vect . fit _ 
transform X _ train # examine the document term matrix 
X _ train _ dtm 4179x7456 sparse matrix of type 
class numpy . int64 with 55209 stored elements in Compressed 
Sparse Row format # transform testing data using fitted vocabulary 
into a document term matrix X _ test _ dtm 
= vect . transform X _ test X _ test 
_ dtm 1393x7456 sparse matrix of type class numpy . 
int64 with 17604 stored elements in Compressed Sparse Row format 
from sklearn import linear _ model clf = linear _ 
model . L o g i s t i c 
R e g r e s s i o n 
C = 1e5 # train the model using X _ 
train _ dtm timing it with an IPython magic command 
% timeit clf . fit X _ train _ dtm 
y _ train 10 loops best of 3 54.7 ms 
per loop # make class predictions for X _ test 
_ dtm y _ pred _ class = clf . 
predict X _ test _ dtm # calculate accuracy of 
class predictions from sklearn import metrics metrics . accuracy _ 
score y _ test y _ pred _ class 0 
. 9885139985642498 我们 执行 一些 基本 的 文本 分析 以 
帮助 我们 理解 数据 这里 我们 使用 scikit learn API 
CountVectorizer . 将 文本 数据 转换 为 矢量 格式 此 
API 在下面 使用 tf idf 我们 将 数据集 分为 一个 
训练 数据集 和 一个 测试 集 这样 我们 就 可以 
检查 分类器 模型 在 测试数据 集上 的 表现 # print 
the confusion matrix metrics . confusion _ matrix y _ 
test y _ pred _ class array 1205 3 13 
172 # print message text for the false positives ham 
incorrectly classified as spam X _ test y _ test 
y _ pred _ class 2340 Cheers for the message 
Zogtorius . I ve been st . . . 4009 
Forgot you were working today Wanna chat but . . 
. 1497 I m always on yahoo messenger now . 
Just send t . . . Name message dtype object 
# print message text for the false negatives spam incorrectly 
classified as ham X _ test y _ test y 
_ pred _ class 1777 Call FREEPHONE 0800 542 0578 
now 763 Urgent Ur £ 500 guaranteed award is still 
uncla . . . 3132 LookAtMe Thanks for your purchase 
of a video . . . 1875 Would you like 
to see my XXX pics they are so . . 
. 1893 CALL 09090900040 & LISTEN TO EXTREME DIRTY LIV 
. . . 4298 thesmszone . com lets you send 
free anonymous an . . . 4394 RECPT 1/3 . 
You have ordered a Ringtone . Your o . . 
. 4949 Hi this is Amy we will be sending 
you a free . . . 761 Romantic Paris . 
2 nights 2 flights from £ 79 B . . 
. 19 England v Macedonia dont miss the goals / 
team . . . 2821 INTERFLORA It s not too 
late to order Inter . . . 2247 Hi ya 
babe x u 4goten bout me scammers getti . . 
. 4514 Money i have won wining number 946 wot 
do i do . . . Name message dtype object 
# example false negative X _ test 3132 LookAtMe Thanks 
for your purchase of a video clip from LookAtMe you 
ve been charged 35p . Think you can do better 
Why not send a video in a MMSto 32323 . 
# calculate predicted probabilities for X _ test _ dtm 
poorly calibrated y _ pred _ prob = clf . 
predict _ proba X _ test _ dtm 1 y 
_ pred _ probarray 9.90605780 e 07 4.03318013 e 09 
1.38284780 e 07 . . . 6.48404534 e 06 1.00000000 
e + 00 3.77161160 e 09 # calculate AUC metrics 
. roc _ auc _ score y _ test y 
_ pred _ prob 0 . 9932611419366387 逻辑 回归 的 
优势 它 能 处理 非线性 效应 它 可以 为 每个 
类 生成 概率 分数 这 使得 解释 变得 容易 逻辑 
回归 的 缺点 此 分类 技术 仅 用于 二进制 分类 
如果 要 将 数据 分类 为 两个 以上 的 类别 
我们 可以 使用 其他 算法 我们 可以 使 用 随机 
森林 和 决策树 等 算法 将 数据 分类 为 两个 
以上 的 类别 如果 你 提供 了 大量 的 特征 
作为 这个 算法 的 输入 那么 特征 空间 就 会 
增加 这个 算法 的 性能 就 不好 了 过度 拟合 
的 可能性 很高 这 意味着 分类器 在 训练 数据集 上 
表现 良好 但 不能 进行 足够 的 归纳 从而 能够 
预测 未知 数据 的 正确 目标 标签 决策树 决策树 是 
最 古老 的 ML 算法 之一 这个 算法 很简单 但 
很 健壮 这个 算法 为 我们 提供 了 一个 树结构 
来做 任何 决定 逻辑 回归 用于 二进制 分类 但 如果 
有 两个 以上 的 类 则 可以 使用 决策树 让 
我们 通过 一个 例子 来 理解 决策树 假设 克里斯 喜欢 
风帆 冲浪 但 他 有 自己 的 喜好 他 通常 
喜欢 晴 天和 有风 的 天气 来 享受 它 也 
不 喜欢 在 下雨天 或 阴天 或 风 小 的 
日子 冲浪 参见 . 26 如 你 所见 O 圆点 
是 克里斯 喜欢 风浪 的 好 天气 条件 X 十字 
是 克里斯 不喜欢 风浪 的 坏 天气 条件 我 所 
绘制 的 数据 不是 线性 可分 的 这 意味着 你 
不能 仅仅 用 一条线 来 区分 红色 十字 和 蓝色 
圆点 你 可能 会 认为 如果 目标 只是 把 蓝点 
和 红十字 分开 那么 我 可以 用 两条 线 来 
实现 这 一点 然而 一条线 能把 蓝点 和 红十字 分开 
吗 答案 是 否定 的 这 就是 为什么 我 告诉 
你 这个 数据集 不能 线性 分离 的 原因 因此 对于 
这种 情况 我们 将 使用 决策树 决策树 实际 上为 您 
做了 什么 用 外行 的 术语 来说 决策树 学习 实际上 
是 关于 提出 多个 线 性问题 让 我们 理解 我 
所说 的 线性 问题 假设 我们 问 一个 问题 有风 
吗 你 有 两个 答案 是或否 我们 有 一个 与 
风 有关 的 问题 所以 我们 需要 集中 在 . 
26 的 X 轴上 如果 我们 的 答案 是 是的 
有风 那么/r 我们/r 应该/v 考虑/v 右手边/n 有/v 红十字/nz 和/c 蓝点/n 
的/uj 区域/n 如果 我们 回答 不是 没 有风 那么 我们 
需要 考虑 左手边 所有 的 红十字 为了 更好 地 理解 
您 可以 参考 . 27 如 . 27 所示 我 
画 了 一条 穿过 x 轴 中点 的 线 我 
刚刚 选择 了 一个 中点 没有 具体 的 原因 所以 
我 画 了 一条 黑线 行 左侧 的 红色 十字 
表示 不 没 有风 行 右侧 的 红色 十字 表示 
有风 在 这条 线 的 左边 只有 红色 的 十字 
没有 一个 蓝色 的 点 如果 你 选择 了 答案 
不 那么 实际上 你 是 用 标记 为 否 的 
分支 进行 遍历 的 左侧 的 区域 只有 红色 交叉 
因此 你 最终 得到 了 属于 同 一类 的 所有 
数 据点 这些 数据 点 用 红色 交叉 表示 你 
不会 再为 该 树 的 分支 提出 进一步 的 问题 
现在 如果 您 选择 答案 是的 那么 我们 需要 将 
焦点 放在 右侧 的 数据 点上 您 可以 看到 有 
两种 类型 的 数据 点 蓝点 和 红十字 所以 为了 
对 它们 进行 分类 你 需要 提 出 一个 线性 
边界 这样 由 直线 组成 的 部分 只有 一 种数 
据点 我们 将 通过 问 另一个 问题 来 实现 这 
一点 天气晴朗 吗 这一次 再一次 你 有 两个 可能 的 
答案 是或否 记住 我 已经 遍历 了 树 的 分支 
它 以是 的 形式 回答 了 我们 的 第一 个 
问题 所以 我 的 重点 是 在数 据点 的 右边 
因为/c 在/p 那里/r 我/r 有以/nr 红十字/nz 和/c 蓝点/n 的/uj 形式/n 
表示/v 的/uj 数据/n 点/m 我们 已经 在 y 轴上 描述 
了 太阳 所以 你 需要 看看 这个 轴 如果 你 
画 一条线 穿过 y 轴 的 中点 那么 线 上面 
的 部分 代表 答案 是的 这 是 一个 晴天 线下 
的 所有 数 据点 代表 答案 不 这 不是 晴天 
当 您 绘制 这样 一条线 并 停止 在 第一 条 
线 之后 延伸 该线 时 您 可以 成功 地 分离 
位于 右侧 的 数据 点 所以 线 上方 的 部分 
只 包含 蓝点 线 下方 的 部分 红色 十字 您 
可以 看到 水平线 如 . 28 所示 我们 可以 观察到 
通 过问 一 系列 问题 或 一系 列线 性问题 我们 
实际 上将 表示 克里斯 不 冲浪 的 红色 十字 和 
表示 克里斯 冲浪 的 蓝色 圆点 分类 这 是 一个 
非常 基本 的 示例 可以 帮助 您 了解 决策树 如何 
处理 分类 问题 在 这里 我们 通过 问 一 系列 
问题 以及 生成 多个 线性 边界 来 分类 数据 点来 
构 建树 让 我们 举 一个 数字 例子 这样 你 
就能 更 清楚 地 看到 它 了 参见 . 29 
您 可以 看到 给定 的 数据 点 我们 先从 x 
轴 开始 您 希望 选择 X 轴上 的 哪个 阈值 
以便 获得 这些 数 据点 的 最佳 分割 想一想 我 
想 选择 一条 在 点 3 通过 x 轴 的 
线 现在 有 两个 部分 在 数学 上 我 选择 
了 给 定数 据点 的 最佳 分割 即 x = 
3 和x/nr 3 参见 . 30 我们 先 看一下 左侧 
部分 您 希望 选择 Y 轴上 的 哪个 值 以便 
在 绘制 该线 后在/nr 一个 区域 中 只有 一种 类型 
的 数据 点 您 选择 的 Y 轴上 的 阈值 
是 多少 以便 在 一个 部分 中 有 一种 类型 
的 数据 集 而在 另 一个 部分 中 有 另一种 
类型 的 数据集 我 将 选择 穿过 Y 轴 上点 
2 的 线 因此 线 上方 的 数据 点 属于 
一个 类 线 下方 的 数据 点 属于 另一个 类 
数学上 y = 2 给 你 一个 类 y 2 
给 你 另一个 类 参见 . 31 Alt https / 
/ img blog . csdnimg . cn / 2 0 
1 9 0 2 0 2 2 3 0 7 
2 5 3 7 3 . png 现在 集中 在 
右侧 部分 对于 该 部分 我们 还 需要 选择 相对 
于Y轴/nr 的 阈值 这里 分离 边界 的 最佳 阈值 是 
y = 4 因此 截面 y 4只 有 红色 十字 
截面 y = 4只 有 蓝色 点 最后 通过 一 
系列 线 性问题 我们 能够 对 数据 点 进行 分类 
参见 . 32 Alt https / / img blog . 
csdnimg . cn / 2 0 1 9 0 2 
0 2 2 3 0 7 5 5 6 7 
9 . png 现在 你 对 算法 有了/nr 一个 概念 
但是 你 的 脑子 里 可能 有 几个 问题 我们 
对 获取 行 进行 了 可视化 但是 决策树 算法 如何 
选择 最佳 的 方法 来 分割 数 据点 并 使用 
给定 的 特征 生成 决策 边界 假设 我 有 两个 
以上 的 特征 比如说 十个 特性 那么 决策树 如何 知道 
它 需要 在 第一 次 使用 第二 个 特性 而不是 
第三个 特性 所以 我 将 通过 解释 决策树 背后 的 
数学 来 回答 所有 这些 问题 我们 将 查看 一个 
与 NLP 相关 的 示例 这样 您 就 可以 了解 
如何 在 NLP 应用 程序 中 使用 决策树 我 有 
一些 关于 决策树 的 问题 让 我们 逐一 回答 我们 
将 使用 可视化 来 获得 一个 线性 边界 但是 决策树 
如何 识别 使用 哪些 特征 以及 应该 分割 数据 的 
哪些 特征值 让 我们 看看 熵 这个 数学 术语 因此 
决策树 使用 熵 的 概念 来 决定 在 哪里 分割 
数据 让 我们 了解 熵 熵 是 一个 树 分支 
中 杂质 的 度量 因此 如果 一个 树 分支 中的 
所 有数 据点 都 属于 同 一类 那么 熵 e 
= 0 否则 熵 e 0 e 1 如果 熵 
e = 1 则 表示 树 的 分支 高度 不纯 
或者 数 据点 在 所有 可用 类 之间 平均分配 让 
我们 看 一个 例子 这样 你 就 能理解 熵 和 
杂质 的 概念 我们 正在 开发 一个 垃圾 邮件 过滤 
应用程序 我们 有 一个 特性 即 单词 和 短语 类型 
现在 我们 将 介绍 另一个 特性 即 数据 集中 出现 
的 短语 的 最小 阈值 计数 参见 . 33 现在 
关注 右边 的 图表 在 这个 图中 右侧 部分 只有 
一 种数 据点 用 红色 十字 表示 所以 从 技术 
上 讲 所有 的 数据 点 都是 同质 的 因为 
它们 属于 同 一个 类 因此 没 有杂质 熵 的 
值 约 为零 现在 如果 您 将 焦点 放在 左侧 
图表 上 并 查看 其 右侧 部分 您 将 找到 
属于 其他 类 标签 的 数据 点 这 部分 含 
有杂质 因此 熵 很高 因此 在 实现 决策树 的 过程 
中 您 需要 找出 可 用于 定义 分割 点 的 
变量 以及 变量 您 需要 记住 的 另一 件事 是 
您 正试图 最小化 数据 中 的 杂质 因此 请 尝试 
根据 这些 数据 拆分 数据 我们 将 看到 如何 在 
一段 时间 内 选择 变量 来 执行 拆分 现在 让 
我们 先 看看 熵 的 数学 公式 参见 . 34 
让 我们 看看 圆周率 是 什么 它 是 给定 类 
的 分数 值 假设 我 是 班上 的 学生 t 
是 可用 类 的 总值 您 有四/nr 个数 据点 如果 
两个 点 属于 A 类 另 两个 点 属于 B 
类 则 t = 2 我们 在 使用 分 数值 
生成 日志 值 后 执行 求和 现在 是 时候 对 
熵 进行 数学 计算 了 然后 我 将 告诉 您 
如何 使用 熵 对 变量 或 特性 执行 拆分 我们 
来看 一个 计算 熵 的 例子 您 可以 找到 . 
35 中的 示例 如果 您 关注 过滤 列 那么/r 您/zg 
有/v 两个/m 值/n 为/p spam/w 的/uj 标签/n 和/c 两个/m 值/n 
为/p ssh/w 的/uj ham/w 现在 回答 以下 几个 问题 我们 
总共 有 多少 数据 行 答案 是 4 数据 标 
签在 筛选 列中 出现 多少次 答案 是 2 数据 标签 
H 在 筛选 列中 出现 多少次 答案 是 2 要为 
类 标签 s 生 成分 数值 需要 执行 数学 运算 
使用 以下 公式 pS = No . of time S 
occurred / Total no . of data rows = 2/4 
= 0.5 现在 我们 还 需要 计算 h 的 p 
pH = No . of time H occurred / Total 
no . of data rows = 2/4 = 0.5 现在 
我们 有了/nr 产生 熵 的 所有 必要 值 专注 于 
公式 Entropy = pS * log2 pS pHlog2 pH = 
0.5 * log 0.5 0 . 5log 0.5 = 1.0 
您 可以 使用 python 的 数学 模块 进行 计算 如 
你 所见 我们 得到 熵 e = 1 这是 最不 
纯净 的 状态 数据 均匀 地 分布 在 可用 的 
类 中 因此 熵 告诉 我们 数据 的 状态 不管 
类 是否 处于 不纯 状态 现在 我们 来 看看 最 
期待 的 问题 我们 如何 知道 在 哪一个 变量 上 
或者 使用 哪一个 特性 来 执行 分割 要 理解 这 
一点 我们 需要 了解 信息 获取 这是 决策树 算法 的 
核心 概念 之一 让 我 介绍 一下 信息 获取 ig 
的 公式 信息 增益 ig = 熵 父 节点 权重 
平均值 熵 子 节点 现在 我们 来 看看 这个 公式 
我们 正在 计算 父 节点 的 熵 并 减去 子 
节点 的 加权 熵 如果在 父 节点 上 执行 拆分 
决策树 将 尝试 最大化 信息 获取 使用 ig 决策树 将 
选择 我们 需要 对 其 执行 拆分 的 功能 该 
计算 针对 所有 可用 的 特性 进行 因此 决策树 确切 
知道 要 在哪里 进行 拆分 您 需要 参考 . 33 
我们 计算 了 父 节点 的 熵 e 父 节点 
= 1 现在 我们 将 关注 单词 并 计算 ig 
让 我们 检查 一下 是否 应该 使 用带 ig 的 
words 执行 split 在 这里 我们 将 重点 放在 单词 
栏 所以 让 我 回答 一些 问题 以便 您 理解 
ig 的 计算 一共 有 多少 个 肯定 意义 的 
词 答案 是 3 一共 有 多少 个 否定 意义 
的 词 答案 是 1 所以 对于 这个 分支 熵 
e = 0 当 我们 计算 子 节点 的 加权 
平均 熵 时 我们 将 使用 这个 方法 你 可以 
看到 对于 右边 的 节点 熵 是 零 所以 分支 
中 没有 任何 杂质 所以 我们 可以 停 在 那里 
但是 如果 您 查看 左侧 节点 它 具有 ssh 类 
因此 我们 需要 计算 每个 类 标签 的 熵 让 
我们 一步 一步 地 为 左侧 节点 执行 此 操作 
ps = 分支 中 标签 的 数量 / 分支 中 
示例 的 总数 = 2 / 3ph = 分支 中 
H 标签 的 数量 / 分支 中 示例 的 总数 
= 1/3 现在 熵 e = 2/3 log2 2/3 1/3 
log2 1/3 = 0.918 在下 一步 中 我们 需要 计 
算子 节点 的 加权 平均 熵 我们 有 三个 数据 
点 作为 左侧 分支 的 一部分 一个 数据 点 作为 
右侧 分支 如 . 36 所示 因此 值 和 公式 
如下 子 级 权重 平均 熵 = 左侧 分 支数 
据点 / 数据 点 总数 * 该 分支 中的 子 
级 熵 + 右侧 分 支数 据点 / 数据 点 
总数 * 该 分支 中的 子 级 熵 儿童 体重 
平均 熵 = 体重 平均 熵 儿童 = ¾ 0.918 
+ ¼ 0 = 0.6885 现在 是 时候 获得 免疫 
球蛋白 了 ig = 熵 父 节点 权重 平均值 熵 
子 节点 我们 两个 部分 的 us e 父 节点 
= 1 和 权重 平均 熵 子 节点 = 0.6885 
因此 最终 计算 如下 ig = 1 0.6885 = 0.3115 
让 我们 集中 讨论 短语 出现 计 数列 并 计算 
短语 计 数值 3 的 熵 即 ethree 3 = 
1.0 短语 计 数值 4 的 熵 为 efour 4 
= 1.0 现在 权重 平均 熵 子项 = 1.0 ig 
= 1.0 1.0 = 0 所以 我们 并 没有 得到 
任何 关于 这个 特性 拆分 的 信息 所以 我们 不 
应该 选择 这个 特性 现在 让 我们 集中 讨论 短语 
列 其中 我们 提到 了 短语 类别 异常 短语 或 
常用 短语 当 我们 使用 此列 拆 分数 据点 时 
我们 得到 一个 分支 中的 垃圾邮件 类 和 另一个 分支 
中的 ham 类 所以 这里 你 需要 自己 计算 ig 
但是 ig = 1 我们 得到 最大 的 免疫 球蛋白 
因此 我们 将 为 分割 选择 此功能 您 可以 在 
. 37 中 看到 决策树 Alt https / / img 
blog . csdnimg . cn / 2 0 1 9 
0 2 0 2 2 3 1 0 1 4 
6 8 4 . png 如果 您 有 大量 的 
特性 那么 决策树 执行 培训 非常 缓慢 因为 它 为 
每个 特性 计算 ig 并 通过 选择 提供 最大 ig 
的 特性 来 执行 拆分 现在 是 时候 看 一下 
使用 决策树 的 NLP 应用程序 了 我们 将 重新 开发 
垃圾邮件 过滤 但这次 我们 将 使用 决策树 我们 只需 更改 
垃圾邮件 过滤 应用 程序 的 算法 我们 采用 了 之前 
生成 的 相同 功能集 这样 您 就 可以 比较 逻辑 
回归 和 垃圾 邮件 过滤 决策树 的 结果 在 这里 
我们 将 使用 由 scikit learn . 学习 中的 CountVectorizer 
API 生成 的 相同 功能 import pandas as pd import 
numpy as np # read file into pandas using a 
relative path path = sms . tsv sms = pd 
. read _ table path header = None names = 
label message # examine the shape sms . shape 5572 
2 # examine the first 10 rows sms . head 
10 l a b e l m e s s 
a g e 0 h a m G o until 
jurong point crazy . . Available only . . . 
1hamOk lar . . . Joking wif u oni . 
. . 2spamFree entry in 2 a wkly comp to 
win FA Cup fina . . . 3hamU dun say 
so early hor . . . U c already then 
say . . . 4hamNah I don t think he 
goes to usf he lives aro . . . 5spamFreeMsg 
Hey there darling it s been 3 week s n 
. . . 6hamEven my brother is not like to 
speak with me . . . . 7hamAs per your 
request Melle Melle Oru Minnamin . . . 8spamWINNER As 
a valued network customer you have . . . 9spamHad 
your mobile 11 months or more U R entitle . 
. . # examine the class distribution sms . label 
. value _ counts ham 4825 spam 747 Name label 
dtype int64 # convert label to a numerical variable sms 
label _ num = sms . label . map { 
ham 0 spam 1 } # check that the conversion 
worked sms . head 10 l a b e l 
m e s s a g e l a b 
e l _ num0hamGo until jurong point crazy . . 
Available only . . . 01hamOk lar . . . 
Joking wif u oni . . . 02spamFree entry in 
2 a wkly comp to win FA Cup fina . 
. . 13hamU dun say so early hor . . 
. U c already then say . . . 04hamNah 
I don t think he goes to usf he lives 
aro . . . 05spamFreeMsg Hey there darling it s 
been 3 week s n . . . 16hamEven my 
brother is not like to speak with me . . 
. . 07hamAs per your request Melle Melle Oru Minnamin 
. . . 08spamWINNER As a valued network customer you 
have . . . 19spamHad your mobile 11 months or 
more U R entitle . . . 1 # how 
to define X and y from the SMS data for 
use with COUNTVECTORIZER X = sms . message y = 
sms . label _ num print X . shape print 
y . shape 5572 5572 # split X and y 
into training and testing sets from sklearn . cross _ 
validation import train _ test _ split X _ train 
X _ test y _ train y _ test = 
train _ test _ split X y random _ state 
= 1 print X _ train . shape print X 
_ test . shape print y _ train . shape 
print y _ test . shape 4179 1393 4179 1393 
# import and instantiate CountVectorizer with the default parameters from 
sklearn . feature _ extraction . text import CountVectorizer # 
instantiate the vectorizer vect = CountVectorizer # learn training data 
vocabulary then use it to create a document term matrix 
vect . fit X _ train X _ train _ 
dtm = vect . transform X _ train # equivalently 
combine fit and transform into a single step X _ 
train _ dtm = vect . fit _ transform X 
_ train # examine the document term matrix X _ 
train _ dtm 4179x7456 sparse matrix of type class numpy 
. int64 with 55209 stored elements in Compressed Sparse Row 
format # transform testing data using fitted vocabulary into a 
document term matrix X _ test _ dtm = vect 
. transform X _ test X _ test _ dtm 
1393x7456 sparse matrix of type class numpy . int64 with 
17604 stored elements in Compressed Sparse Row format from sklearn 
import tree clf = tree . D e c i 
s i o n T r e e C l 
a s s i f i e r criterion = 
entropy # train the model using X _ train _ 
dtm timing it with an IPython magic command % timeit 
clf . fit X _ train _ dtm y _ 
train 10 loops best of 3 169 ms per loop 
# make class predictions for X _ test _ dtm 
y _ pred _ class = clf . predict X 
_ test _ dtm # calculate accuracy of class predictions 
from sklearn import metrics metrics . accuracy _ score y 
_ test y _ pred _ class 0 . 9655419956927495 
# print the confusion matrix metrics . confusion _ matrix 
y _ test y _ pred _ class array 1182 
26 22 163 # print message text for the false 
positives ham incorrectly classified as spam X _ test y 
_ test y _ pred _ class 1827 Dude . 
What s up . How Teresa . Hope you have 
bee . . . 1973 Yes but can we meet 
in town cos will go to gep . . . 
3242 Ok i ve sent u da latest version of 
da project . 1791 Am not working but am up 
to eyes in philosophy . . . 2900 Aight I 
should be there by 8 at the latest p . 
. . 2497 HCL chennai requires FRESHERS for voice proces 
. . . 745 Men like shorter ladies . Gaze 
up into his eyes . 2340 Cheers for the message 
Zogtorius . I ve been st . . . 1832 
Hello thanx for taking that call . I got a 
job . . . 566 Ill call u 2mrw at 
ninish with my address tha . . . 858 Hai 
ana tomarrow am coming on morning . & lt DE 
. . . 3544 I m e person who s 
doing e sms survey . . . 987 I m 
in office now . I will call you & lt 
# & gt . . . 705 True dear . 
. i sat to pray evening and felt so . 
s . . . 988 Geeee . . . I 
miss you already you know Your . . . 5336 
Sounds better than my evening im just doing my . 
. . 100 Please don t text me anymore . 
I have nothing e . . . 1364 Yetunde i 
m sorry but moji and i seem too bus . 
. . 4092 Hey doc pls I want to get 
nice t shirt for my . . . 4766 if 
you text on your way to cup stop that shoul 
. . . 5094 Hi Shanil Rakhesh here . thanks 
i have exchanged . . . 3826 Hi . I 
m always online on yahoo and would like . . 
. 3237 Aight text me when you re back at 
mu and I ll . . . 4814 i can 
call in & lt # & gt min if thats 
ok 4958 I m vivek i got call from your 
number . 330 I m reading the text i just 
sent you . Its mean . . . Name message 
dtype object # print message text for the false negatives 
spam incorrectly classified as ham X _ test y _ 
test y _ pred _ class 3642 You can stop 
further club tones by replying . . . 1777 Call 
FREEPHONE 0800 542 0578 now 2680 New Tones This week 
include 1 McFly All Ab . . . . . 
763 Urgent Ur £ 500 guaranteed award is still uncla 
. . . 4574 URGENT This is the 2nd attempt 
to contact U U . . . 881 Reminder You 
have not downloaded the content . . . 3132 LookAtMe 
Thanks for your purchase of a video . . . 
2514 U have won a nokia 6230 plus a free 
digital ca . . . 5 FreeMsg Hey there darling 
it s been 3 week s n . . . 
3530 Xmas & New Years Eve tickets are now on 
sale f . . . 4768 Your unique user ID 
is 1172 . For removal send . . . 4298 
thesmszone . com lets you send free anonymous an . 
. . 1734 Hi this is Mandy Sullivan calling from 
HOTMIX . . . 4949 Hi this is Amy we 
will be sending you a free . . . 761 
Romantic Paris . 2 nights 2 flights from £ 79 
B . . . 3230 Ur cash balance is currently 
500 pounds to m . . . 579 our mobile 
number has won £ 5000 to claim call . . 
. 3564 Auction round 4 . The highest bid is 
now £ 54 . N . . . 2863 Adult 
18 Content Your video will be with you s . 
. . 2247 Hi ya babe x u 4goten bout 
me scammers getti . . . 4514 Money i have 
won wining number 946 wot do i do . . 
. 789 5 Free Top Polyphonic Tones call 087018728737 . 
. . Name message dtype object # example false negative 
X _ test 761 Romantic Paris . 2 nights 2 
flights from £ 79 Book now 4 next year . 
Call 08704439680Ts & Cs apply . # calculate predicted probabilities 
for X _ test _ dtm poorly calibrated y _ 
pred _ prob = clf . predict _ proba X 
_ test _ dtm 1 y _ pred _ probarray 
0 . 0 . 0 . . . . 0 
. 1 . 0 . # calculate AUC metrics . 
roc _ auc _ score y _ test y _ 
pred _ prob 0 . 929778951136567 如 你 所见 与 
逻辑 回归 相比 我们 的 精确度 较低 现在 是 时候 
看 一些 可以 用 来 提高 ML 模型 精度 的 
调整 参 数了 可调 参数 scikit learn 学习 中 有一个 
参数 这 就是 标准 你 可以 把 它 设置 为 
熵 或者 基尼 熵 或 基尼 被 用来 计算 ig 
因此 它们/r 都有/nr 一个/m 相似/v 的/uj 机制/n 来/v 计算/v ig/w 
决策树 将 根据 熵 或 基尼 给出 的 ig 计算 
来 执行 分割 有 最小 样本 大小 其 默认值 为 
2 因此 决策树 分支 将被 拆分 直到 每个 分支 的 
数据 元素 多于 或 等于 两个 有时 决策树 会 试图 
拟合 最大 的 训练 数据 并 过度 拟合 训练 数 
据点 为了 防止 过拟合 您 需要 将 最小 样本 尺寸 
从2/nr 增加到 50 或 60 我们 可以 使用 树木 修剪 
技术 为此 我们 将 采用 自下而上 的 方法 决策树 的 
优点 决策树 简单 易 开发 决策树 很容易 被 人 理解 
是 一种 白盒/nr 算法 它 帮助 我们 确定 不同 情况下 
的 最差 最佳/z 和/c 预期/vn 值/n 决策树/n 的/uj 缺点/n 如果/c 
您/zg 有/v 很多/m 特性/n 那么 决策树 可能有 过拟合 问题 在 
训练 过程 中 你 需要 注意 你 通过 的 参数 
随机 森林 该 算法 是 解决 过拟合 问题 的 决策树 
的 一个 变种 随机 森林 既能 发展 线性 回归 又能 
发展 分类 任务 这里 我们 将 重点 放在 分类 任务 
上 它 使用 了 一个 非常 简单 的 技巧 而且 
效果 非常 好 关键 是 随机 森林 使用 投票 机制 
来 提高 测试 结果 的 准确性 随机 森林 算法 从 
训练 数据 集中 生成 数据 的 随机 子集 并 使用 
该 子集 为 每个 数据 子集 生成 决策树 所有 这些 
生成 的 树 都 称为 随机 林 现在 让 我们 
了解 投票 机制 一旦 我们 生成 了 决策树 我们 就 
检查 类 标签 每个 树 都是 为 特定 的 数据 
点 提供 的 假设 我们 生成 了 三个 随机 的 
森林 决策树 其中 两个 表示 某个 特定 数 据点 属于 
A 类 第三个 决策树 预测 该 特 定数 据点 属于 
B 类 算法 考虑 了 更高 的 投票数 并为 该 
特 定数 据点 分配 了 类 标签 A 对于 随机 
森林 分类 的 所有 计算 都 类似于 决策树 import nltk 
from nltk import word _ tokenize import pprint from sklearn 
. tree import D e c i s i o 
n T r e e C l a s s 
i f i e r from sklearn . feature _ 
extraction import DictVectorizer from sklearn . pipeline import Pipeline # 
tagged _ sentences = nltk . corpus . brown . 
tagged _ sents tagged _ sentences = nltk . corpus 
. treebank . tagged _ sents print tagged _ sentences 
0 Pierre NNP Vinken NNP 61 CD years NNS old 
JJ will MD join VB the DT board NN as 
IN a DT nonexecutive JJ director NN Nov . NNP 
29 CD . . print Tagged sentences len tagged _ 
sentences Tagged sentences 3914print Tagged words len nltk . corpus 
. treebank . tagged _ words Tagged words 100676def features 
sentence index sentence w1 w2 . . . index the 
index of the word return { word sentence index is 
_ first index = = 0 is _ last index 
= = len sentence 1 is _ capitalized sentence index 
0 . upper = = sentence index 0 is _ 
all _ caps sentence index . upper = = sentence 
index is _ all _ lower sentence index . lower 
= = sentence index prefix 1 sentence index 0 prefix 
2 sentence index 2 prefix 3 sentence index 3 suffix 
1 sentence index 1 suffix 2 sentence index 2 suffix 
3 sentence index 3 prev _ word if index = 
= 0 else sentence index 1 next _ word if 
index = = len sentence 1 else sentence index + 
1 has _ hyphen in sentence index is _ numeric 
sentence index . isdigit capitals _ inside sentence index 1 
. lower = sentence index 1 } pprint . pprint 
features This is a sentence 2 { capitals _ inside 
False has _ hyphen False is _ all _ caps 
False is _ all _ lower True is _ capitalized 
False is _ first False is _ last False is 
_ numeric False next _ word sentence prefix 1 a 
prefix 2 a prefix 3 a prev _ word is 
suffix 1 a suffix 2 a suffix 3 a word 
a } def untag tagged _ sentence return w for 
w t in tagged _ sentence def transform _ to 
_ dataset tagged _ sentences X y = for tagged 
in tagged _ sentences for index in range len tagged 
X . append features untag tagged index y . append 
tagged index 1 # print index + str index + 
original word + str tagged + Word + str untag 
tagged + Y + y index return X ycutoff = 
int . 75 * len tagged _ sentences training _ 
sentences = tagged _ sentences cutoff test _ sentences = 
tagged _ sentences cutoff print len training _ sentences 2935print 
len test _ sentences 979X y = transform _ to 
_ dataset training _ sentences clf = Pipeline vectorizer DictVectorizer 
sparse = False classifier D e c i s i 
o n T r e e C l a s 
s i f i e r criterion = entropy % 
timeit clf . fit X 10000 y 10000 # Use 
only the first 10K samples if you re running it 
multiple times . It takes a fair bit 1 loop 
best of 3 14.9 s per loopprint Training completed Training 
completedX _ test y _ test = transform _ to 
_ dataset test _ sentences print Accuracy clf . score 
X _ test y _ test Accuracy 0 . 8 
9 3 7 4 0 9 6 0 9 5 
1 3 0 9 6 d e f pos _ 
tag sentence tagged _ sentence = tags = clf . 
predict features sentence index for index in range len sentence 
return zip sentence tags list pos _ tag word _ 
tokenize This is my friend John . This DT is 
VBZ my NN friend NN John NNP . . 随机 
森林 的 优势 它 有助于 我们 防止 过拟合 它 既可 
用于 回归 也可 用于 分类 随机 森林 的 缺点 随机 
森林 模型 可以 很容易 地 生长 这 意味着 如果 数据集 
的 随机 子集 很高 那么 我们 将 得到 更多 的 
决策树 因此 我们 将 得到 一组 树 也 就是 可能 
占 用 大量 内存 的 决策 树林 对于 高维 的 
特征 空间 很难 解释 树 的 每个 节点 尤其 是 
当 一个 森林 中 有 大量 的 树 时 朴素 
贝叶斯 在 本节 中 我们 将 了解 在 许多 数据 
科学 应用 中 大量 使用 的 概率 ML 算法 我们 
将 使用 此 算法 开发 最 著名 的 NLP 应用程序 
情感 分析 但在 进入 应用 程序 之前 我们 将 了解 
朴素 贝叶斯 算法 的 工作 原理 那么 我们 开始 吧 
朴素 的 贝叶斯 ML 算法 基于 贝叶 斯定理 根据 这个 
定理 我们 最 重要 的 假设 是 事件 是 独立 
的 这 是 一个 朴素 的 假设 这 就是 这个 
算法 被称为 朴素 贝叶斯 的 原因 那么 让 我 给 
你 一个 独立 事件 的 概念 在 分类 任务 中 
我们 有 许多 特性 如果 我们 使用 朴素 的 贝叶斯 
算法 那么 我们 假设 我们 要 提供 给 分类器 的 
每个 特征 都是/nr 相互 独立 的 这 意味着 类 中 
某个 特定 特征 的 存在 不 会 影响 任何 其他 
特征 让 我们 举 个 例子 你 想 找出 这 
句话 的 感悟 很好 你 有 很多 特点 比如 词 
袋 形容词 短语 等等 即使 所有 这些 特征 相互依赖 或 
依赖 于 其他 特征 的 存在 这些 特征 所 携带 
的 所有 属性 都 独立 地 贡献 了 这个 句子 
带有 积极 情绪 的 可能性 这 就是 我们 称之为 朴素 
算法 的 原因 这个 算法 非常简单 而且 非常 强大 如果 
你 有 大量 的 数据 这就 非常 有效 它 可以 
对 两个 以上 的 类 进行 分类 因此 有助于 构建 
一个 多 类 分类器 那么 现在 让 我们 来 看 
一些 点 这些 点 将 告诉 我们 朴素 贝叶斯 算法 
是 如何 工作 的 让 我们 了解 它 背后 的 
数学 和 概率 定理 我们 将 首先 理解 贝叶斯 规则 
在 非常 简单 的 语言 中 您 有 一些 事件 
的 先验概率 并且 您 在 测试 数据 中 发现 了 
相同 事件 的 一些 证据 并将 它们 相乘 然后 你 
得到 后验/nr 概率 帮助 你 得到 最终 的 预测 别担心 
术语 我们 会 详细 讨论 这些 细节 的 让 我 
先给 你 一个 方程 然后 我们 举 一个 例子 这样 
你 就 知道 我们 需要 做 的 计算 是 什么 
如 . 41 所示 这里 p c | x 是 
C 类 的 概率 c 类 是 目标 x 是 
特征 或 数据 属性 p c 是 C 类 的 
先验概率 p x | c 是 对 给定 目标 类 
的 预测 概率 的 似 然 估计 p x 是 
预测 概率 的 先验概率 让 我们 用 这个 方程 来 
计算 一个 例子 假设 有 一个 医学 测试 可以 帮助 
确定 一个 人 是否 患有 癌症 患 这种 特定 类型 
癌症 的 人 的 先前 概率 只有 1% 这 意味着 
p c = 0.01 = 1% 因此 p 而非 c 
= 0.99 = 99% 如果 患者 患有 癌症 有 90% 
的 几率 检测 结果 呈阳性 因此 p 阳性 | c 
= 0.9 = 90% 的 先验概率 10% 的 概率 即使 
患者 没有 癌症 结果 仍然 显示 阳性 所以 p 阴性 
| c = 0.1 = 10% 现在 我们 需要 检查 
这个 人 是否 真的 得了 癌症 如果 结果 为 阳性 
则 概率 写 为 p c 阳性 如果 患者 没有 
癌症 但 结果 仍然 为 阳性 则 表示 为 p 
c 阴性 我们 需要 计算 这 两个 概率 来 推导 
后验/nr 概率 首先 我们 需要 计算 联合 概率 Joint P 
c | Positive result = P © * P Positive 
result | c = 0.01 x 0.9 = 0 . 
009Joint P not c | Positive result = P not 
c * P Positive result | not c = 0.99 
x 0.1 = 0.099 前面 的 概率 称为 联合 概率 
这将 有助于 推导 最终 后验/nr 概率 为了 得到 后验/nr 概率 
我们 需要 应用 归一化 P Positive result = P c 
| Positive result + P not c | Positive result 
= 0.009 + 0.099 = 0.108 现在 实际 后验/nr 概率 
如下 Posterior probability of P not c | Positive result 
= joint probability of P not c | Positive result 
/ P Positive result = 0.099 / 0.108 = 0.916 
如果 你 把 p 的 后验/nr 概率 P c | 
Positive result + 加上 p 的 后验/nr 概率 P not 
c | Positive result 应为 = 1 在 这种 情况 
下 它 的 总和 是 1 有 很多 数学 在 
进行 所以 我会 给 你 画 一个 图表 帮助 你 
理解 这些 事情 参见 . 42 我们 将 把 这个 
概念 扩展 到 一个 NLP 应用程序 这里 我们 将 以 
一个 基于 NLP 的 基本 示例 为例 假设有 两个人 克里斯 
和 萨拉 我们/r 有/v 克里斯/l 和/c 萨拉/l 的/uj 电子邮件/n 详细信息/n 
他们 都 使用 诸如 生活 爱情 和 交易 之类 的 
词 为了 简单 起见 我们 只 考虑 三个 词 他们 
都以/nr 不同 的 频率 使用 这三个 词 克里斯 在 邮件 
中 只 使用 了 1% 的 时间 爱 这个词 而 
他 使用 交易 这个词 的 时间 占 80% 而 生活 
的 时间 占 1% 另一方面 萨拉 使用 爱 这个词 的 
时间 占 50% 交易 时间 占 20% 生活 时间 占 
30% 如果 我们 有 新的 电子邮件 那么 我们 需要 决定 
它 是由 克里斯 还是 萨拉 写 的 p 的 先验概率 
chris = 0.5 p sara = 0.5 邮件 中 有句话 
生命 交易 所以 概率 计算 是 针对 p Chris 生命 
交易 = p Life * p Deal * p Chris 
= 0.04 而 计算 p Sara 生命 交易 = p 
Life * p Deal * p Sara = 0.03 现在 
让 我们 应用 规范化 并 生成 实际 概率 为此 我们 
需要 计算 联合 概率 = P Chris Life Deal + 
P Sara Life Deal = 0.07 以下 是 实际 概率值 
P Chris | Life Deal = 0.04 / 0.07 = 
0.57 P Sara | Life Deal = 0.03 / 0.07 
= 0.43 生命 交易 这个 句子 更 有可能 是 克里斯 
写 的 本例 到此结束 现在 是 实际 实施 的 时候 
了 在 这里 我们 正在 开发 最 著名 的 NLP 
应用程序 即 情感 分析 我们 将 对 文本 数据 进行 
情绪 分析 这样 我们 就 可以 说 情绪 分析 是 
对 人类 产生 的 观点 进行 的 文本 分析 情绪 
分析 帮助 我们 分析 客户 对 某个 产品 或 事件 
的 看法 对于 情绪 分析 我们 将 使用 词语 袋 
方法 你 也 可以 使用 人工神经网络 但 我 解释 的 
是 一种 简单 而 基本 的 方法 import sys import 
os import time from sklearn . feature _ extraction . 
text import TfidfVectorizer from sklearn . naive _ bayes import 
MultinomialNB from sklearn . metrics import classification _ reportdata _ 
dir = data classes = pos neg # Read the 
data train _ data = train _ labels = test 
_ data = test _ labels = for curr _ 
class in classes dirname = os . path . join 
data _ dir curr _ class for fname in os 
. listdir dirname with open os . path . join 
dirname fname r as f content = f . read 
if fname . startswith cv9 test _ data . append 
content test _ labels . append curr _ class else 
train _ data . append content train _ labels . 
append curr _ class # Create feature vectors vectorizer = 
TfidfVectorizer min _ df = 5 max _ df = 
0.8 sublinear _ tf = True use _ idf = 
True train _ vectors = vectorizer . fit _ transform 
train _ data test _ vectors = vectorizer . transform 
test _ data d \ Program Files \ Anaconda3 \ 
lib \ site packages \ sklearn \ feature _ extraction 
\ text . py 1059 FutureWarning Conversion of the second 
argument of issubdtype from ` float ` to ` np 
. floating ` is deprecated . In future it will 
be treated as ` np . float64 = = np 
. dtype float . type ` . if hasattr X 
dtype and np . issubdtype X . dtype np . 
float clf = MultinomialNB % time clf . fit train 
_ vectors train _ labels Wall time 7 ms MultinomialNB 
alpha = 1.0 class _ prior = None fit _ 
prior = True % time prediction = clf . predict 
test _ vectors Wall time 1 ms # Print results 
in a nice table print Results for NaiveBayes MultinomialNB print 
classification _ report test _ labels prediction Results for NaiveBayes 
MultinomialNB precision recall f1 score support neg 0.81 0.92 0.86 
100 pos 0.91 0.78 0.84 100 avg / total 0.86 
0.85 0.85 200 可调 参数 对 于此 算法 有时 需要 
应用 平滑 现在 平滑 是 什么 意思 让 我 给 
你 一个 简单 的 想法 训练 数据 中 有一些 词 
我们 的 算法 使用 这些 数据 生成 一个 ML 模型 
如果 ML 模型 看到 的 单词 不在 训练 数据 中 
而是 出现 在 测试 数据 中 那么 此时 我们 的 
算法 不能 很好 地 预测 事情 我们 需要 解决 这个 
问题 因此 作为 一个 解决方案 我们 需要 应用 平滑 这 
意味着 我们 也 在 计算 稀有 词 的 概率 这是 
scikit learn 学习 中的 可调 参数 它 只是 一个 标志 
如果 启用 它 它 将 执行 平滑 如果 禁用 它 
则 不会 应用 平滑 朴素 贝叶斯 的 优势 可以 使用 
朴素 贝叶斯 算法 处理 高维 特征 空间 它 可 用于 
对 两个 以上 的 类 进行 分类 朴素 贝叶斯 的 
缺点 如果 你 有一个 由 不同 的 词 组成 的 
词组 有 不同 的 意思 那么 这个 算法 将 不会 
帮助 你 你 有 一句话 古吉拉特 狮子 这是 板 球队 
的 名字 但 古吉拉特邦 是 印度 的 一个 州 狮子 
是 一种 动物 因此 朴素 贝叶斯 算法 将 单个 单词 
单独 解释 因此 该 算法 无法 正确 解释 古吉拉特邦 狮子 
如果 一些 分类 数据 只 出现 在 测试 数据 集中 
而不 出现 在 训练 数据 中 那么 朴素 贝叶斯 就 
不会 对 此 提供 预测 所以 为了 解决 这 类 
问题 我们 需要 应用 平滑 技术 现在 是 时候 看看 
最后 的 分类 算法 了 支持 向量 机 支持 向量 
机 这 是 我们 将在 本章 中 看到 的 最后 
一个 但 不是 最 不受 监督 的 ML 算法 它 
被 称为 支持 向量 机 SVM 该 算法 用于 分类 
任务 和 回归 任务 该 算法 也 适用 于 多类 
分类 任务 SVM 获取 标记 的 数据 并 试图 通过 
使用 一条 称为 超平面 的 线 将 数据 点 分离 
来 对其 进行 分类 目标 是 获得 一个 最佳 的 
超平面 用于 对 现有 的 和 新的 未 发现 的 
例子 进行 分类 如何 获得 一个 最佳 超平面 是 我们 
将要 理解 的 首先 让 我们 了解 最优 超平面 这个 
术语 我们 需要 以 这样 一种 方式 获得 超平面 获得 
的 超平面 最大化 到 所有 类 的 最近 点 的 
距离 这个 距离 称为 边界 这里 我们 将 讨论 一个 
二进制 分类器 边界 是 超平面 或 直线 与 两个 类 
中 任何 一个 类 的 最近 点 之间 的 距离 
SVM 试图 使 利润 最大化 参见 . 45 在 给定 
的 图中 有 三行 A B 和C/nr 现在 选择 您 
认为 最能 分隔 数 据点 的 行 我会 选择 B 
行 因为 它 最大 化了 两个 类 的 边界 而 
其他 A 行 和C/nr 行不 这样做 请注意 SVM 首先 尝试 
完美 地 执行 分类 任务 然后 尝试 最大化 利润 因此 
对于 SVM 来说 正确 地 执行 分类 任务 是 第一 
要务 支持 向量 机 既能 获得 线性 超平面 又能 生成 
非线性 超平面 那么 让 我们 理解 这 背后 的 数学原理 
如果 你 有n个/nr 特征 那么 使用 SVM 你 可以 画出 
n 1 维 超平面 如果 你 有一个 二维 特征 空间 
那么 你 可以 画 一个 一维 的 超平面 如果 有 
三维 特征 空间 则 可以 绘制 二维 超平面 在 任何 
ML 算法 中 我们 实际上 都 尝试 最小化 损失 函数 
因此 我们 首先 定义 SVM 的 损失 函数 SVM 使用 
铰链 损失 功能 我们 使用 这个 损失 函数 试图 将 
我们 的 损失 降 到 最低 并 获得 超平面 的 
最大 裕度 铰链 损失 函数 方程 如下 C x y 
f x = 1 y * f x + 这里 
x 是 样本 数 据点 y 是 真 标签 f 
x 是 预测 标签 c 是 损失 函数 方程 中的 
+ 符号 表示 的 是 当 我们 计算 y * 
f x 并且 它 大于 等于 1时 我们 试图 从 
1中 减去 它 得到 一个 负值 我们 不想 这样 所以 
为了 表示 这 一点 我们 把 + 号 放在 现在 
是 时候 定义 接 受损失 函数 的 目标 函数 以及 
一个 名为 正则化 项的/nr lambda 项 我们 会 看到 它 
对 我们 有 什么 作用 但是 它 也 是 一个 
调整 参数 数学 方程 见 . 46 SVM 有 两个 
我们 需要 注意 的 调整 参数 其中 一个 术语 是 
lambda 它 表示 正则化 术语 如果 正则化 项 太高 那么 
我们 的 ML 模型 就 过拟合 不能 推广 未 看到 
的 数据 如果 它 太低 那么 它 的 下溢 我们 
会 得到 一个 巨大 的 训练 误差 所以 我们 也 
需要 一个 正则化 项的/nr 精确 值 我们 需要 注意 有助于 
防止 过度 拟合 的 正则化 条件 我们 需要 将 损失 
最小化 因此 我们/r 对/p 这/r 两个/m 项都取/nr 偏/a 导数/n 下面/f 
是/v 正则化/i 项和/nr 损失/n 函数/n 的/uj 导数/n 我们 可以 用 
它们 来 进行 梯度 下降 这样 我们 可以 最小化 损失 
得到 一个 准确 的 正则化 值 偏 导数 方程 见 
. 47 损失 函数 的 偏 导数 如 . 48 
所示 我们 需要 计算 偏 导数 的 值 并 相应 
地 更新 权重 如果 我们 错误 地 分类 了 数据 
点 那么 我们 需要 使用 下面 的 公式 来 更新 
权重 参见 . 49 因此 如果 y 1 那么 我们 
需要 使用 . 50 中的 以下 方程 在 这里 长 
n 形 被称为 eta 它 表示 学习率 学习 速率 是 
一个 调整 参数 它 可以 显示 算法 的 运行 速度 
这 也 需要 一个 准确 的 值 因为 如果 它 
太高 那么 训练 将 完成 得 太快 算法 将 错过 
全局 最小值 另一方面 如果 速度 太慢 那就/nr 需要 太多 的 
时间 来 训练 而且 可能 永远 不会 收敛 如果 发生 
错误 分类 那么 我们 需要 更新 我们 的 损失 函数 
以及 正则化 项 现在 如果 算法 正确地 对 数据 点 
进行 分类 呢 在 这种 情况 下 我们 不 需要 
更新 损失 函数 我们 只 需要 更新 我们 的 正则化 
参数 您 可以 使用 . 51 中 给出 的 方程 
看到 当/t 我们/r 有/v 一个/m 适当/a 的/uj 正则化/i 值/n 和/c 
全局/n 极小值/n 时/n 我们 就 可以 对 支持 向量 机中 
的 所有 点 进行 分类 此时 边缘 值 也 成为 
最大值 如果 要 使用 SVM 进行 非线性 分类器 则 需要 
应用 内核 技巧 简单 地 说 内核 技巧 就是 将 
较低 的 特征 空间 转换 为 较高 的 特征 空间 
从而 引入 非线性 属性 以便 对 数据 集 进行 分类 
如 . 52 所示 Alt https / / img blog 
. csdnimg . cn / 2 0 1 9 0 
2 0 2 2 3 1 7 2 1 5 
3 6 . png 为了 对 这些 数据 进行 分类 
我们 有X/nr Y 特征 我们 引入 了 新的 非线性 特征 
x2 + y2 这 有助于 我们 绘制 一个 能够 正确 
分类 数据 的 超平面 所以 现在 是 时候 实现 SVM 
算法 了 我们 将 再次 开发 情感 分析 应用程序 但这次 
我 使用 的 是 SVM 看看 在 准确性 上 有 
什么 不同 import sys import os import time from sklearn 
. feature _ extraction . text import TfidfVectorizer from sklearn 
import svm from sklearn . metrics import classification _ reportdata 
_ dir = data classes = pos neg # Read 
the data train _ data = train _ labels = 
test _ data = test _ labels = for curr 
_ class in classes dirname = os . path . 
join data _ dir curr _ class for fname in 
os . listdir dirname with open os . path . 
join dirname fname r as f content = f . 
read if fname . startswith cv9 test _ data . 
append content test _ labels . append curr _ class 
else train _ data . append content train _ labels 
. append curr _ class # Create feature vectors vectorizer 
= TfidfVectorizer min _ df = 5 max _ df 
= 0.8 sublinear _ tf = True use _ idf 
= True train _ vectors = vectorizer . fit _ 
transform train _ data test _ vectors = vectorizer . 
transform test _ data # Perform classification with SVM kernel 
= rbf classifier _ rbf = svm . SVC t0 
= time . time classifier _ rbf . fit train 
_ vectors train _ labels t1 = time . time 
prediction _ rbf = classifier _ rbf . predict test 
_ vectors t2 = time . time time _ rbf 
_ train = t1 t0 time _ rbf _ predict 
= t2 t1 # Perform classification with SVM kernel = 
linear classifier _ linear = svm . SVC kernel = 
linear t0 = time . time classifier _ linear . 
fit train _ vectors train _ labels t1 = time 
. time prediction _ linear = classifier _ linear . 
predict test _ vectors t2 = time . time time 
_ linear _ train = t1 t0 time _ linear 
_ predict = t2 t1 # Perform classification with SVM 
kernel = linear classifier _ liblinear = svm . LinearSVC 
t0 = time . time classifier _ liblinear . fit 
train _ vectors train _ labels t1 = time . 
time prediction _ liblinear = classifier _ liblinear . predict 
test _ vectors t2 = time . time time _ 
liblinear _ train = t1 t0 time _ liblinear _ 
predict = t2 t1 # Print results in a nice 
table print Results for SVC kernel = rbf print Training 
time % fs Prediction time % fs % time _ 
rbf _ train time _ rbf _ predict print classification 
_ report test _ labels prediction _ rbf print Results 
for SVC kernel = linear print Training time % fs 
Prediction time % fs % time _ linear _ train 
time _ linear _ predict print classification _ report test 
_ labels prediction _ linear print Results for LinearSVC print 
Training time % fs Prediction time % fs % time 
_ liblinear _ train time _ liblinear _ predict print 
classification _ report test _ labels prediction _ liblinear d 
\ Program Files \ Anaconda3 \ lib \ site packages 
\ sklearn \ feature _ extraction \ text . py 
1059 FutureWarning Conversion of the second argument of issubdtype from 
` float ` to ` np . floating ` is 
deprecated . In future it will be treated as ` 
np . float64 = = np . dtype float . 
type ` . if hasattr X dtype and np . 
issubdtype X . dtype np . float Results for SVC 
kernel = rbf Training time 8.710498 s Prediction time 0.945054 
s precision recall f1 score support neg 0.86 0.75 0.80 
100 pos 0.78 0.88 0.83 100 avg / total 0.82 
0.81 0.81 200 Results for SVC kernel = linear Training 
time 7.742443 s Prediction time 0.762043 s precision recall f1 
score support neg 0.91 0.92 0.92 100 pos 0.92 0.91 
0.91 100 avg / total 0.92 0.92 0.91 200 Results 
for LinearSVC Training time 0.075004 s Prediction time 0.000000 s 
precision recall f1 score support neg 0.92 0.94 0.93 100 
pos 0.94 0.92 0.93 100 avg / total 0.93 0.93 
0.93 200 可调 参数 Scikit Learn 为 内核 技巧 提供 
了 一个 非常 有用 的 调 优 参数 您 可以 
使用 各种 类型 的 内核 如 线性 RBF 等 还有 
其他 参数 称为 c 和 gamma c 控制 平滑 决策 
边界 和 正确 分类 训练 点 之间 的 权衡 如果 
C 值 较大 则 可以 获得 更 正确 的 训练 
点 如果 你 想 设置 你 的 边界 gamma 会很 
有用 如果 为 gamma 设置 了 较高 的 值 则 
只 考虑 附近 的 数据 点来 绘制 决策 边界 如果 
gamma 的 值 较低 则 也会 考虑 远离 决策 边界 
的 点来 测量 决策 边界 是否 使 边界 最大化 支持 
向量 机 的 优点 它 在 复杂 的 数据 集中 
表现 良好 它 可 用于 多类 分类器 支持 向量 机 
的 缺点 当 你 有一个 非常 大 的 数据集 时 
它 不会 表现 得 很好 因为 它 需要 大量 的 
培训 时间 当 数据 太 嘈杂 时 它 将 无法 
有效 工作 8 . 3.2 无 监督 机器学习 方法 这是 
另一种 机器学习 算法 当 我们 没有 任何 标记 的 数据 
时 我们 可以 使用 无 监督 的 机器学习 算法 在 
NLP 域中 有 一种 常见 的 情况 是 您 找不到 
标记 的 数据集 然后 这种 类型 的 ML 算法 就被 
我们 拯救 了 在 这里 我们 将 讨论 无 监督 
的 ML 算法 称为 k 均值 聚 类 这种 算法 
有 许多 应用 谷歌 已经 为 他们 的 许多 产品 
使用 了 这种 无 监督 的 学习 算法 YouTube 视频 
建议 使用 聚 类 算法 下图 将向 您 介绍 如何 
在 无 监督 的 ML 算法 中 表示 数 据点 
参见 . 55 如 . 55 所示 数据 点 没有 
与 其 关联 的 标签 但从 视觉 上看 可以 看到 
它们 形成 了 一些 组 或 集群 实际上 我们 将 
尝试 使用 无 监督 的 ML 算法 来 确定 数据 
中 的 结构 这样 我们 就 可以 对 看 不见 
的 数据 点 获得 一些 卓有成效 的 见解 在 这里 
我们 将 研究 k means 聚 类 算法 并 开发 
与 NLP 域 相关 的 文档 分类 示例 那么 我们 
开始 吧 k 均值 聚 类 在 本节 中 我们 
将 讨论 k 均值 聚 类 算法 我们 将 首先 
了解 算法 k 均值 聚 类 采用 迭代 细化 技术 
让 我们 了解 一些 关于 k 均值 算法 的 基础 
知识 k 是 指 我们 要 生成 多少 个 集群 
现在 您 可以 选择 一个 随机 点 并将 质心 放在 
这个 点上 k 均值 聚 类 中的 形 心数 不大于 
k 的 值 即 不大于 聚 类 值 k 该 
算法 有 以下 两个 步骤 我们 需要 重申 1 第一步 
是 指定 质心 2 第二步 是 计算 优 化步骤 为了 
理解 k means 的 步骤 我们 将 看 一个 例子 
您 有五/nr 个数 据点 在 表中 给出 我们 希望 将 
这些 数据 点 分组 为 两个 集群 所以 k = 
2 参见 . 56 我们 选择 了 点 A 1 
1 和点C/nr 0 2 来 分配 我们 的 质心 这是 
分配 步骤 的 结束 现在 让 我们 了解 优 化步骤 
我们 将 计算 从 每个 点 到 这个 质心 的 
欧几里得 距离 欧几里得 距离 方程 如 . 57 所示 每次 
我们 都 需要 计算 两个 质心 之间 的 欧几里得 距离 
让 我们 检查 一下 计算结果 起始 质心 平均值 为 c1 
= 1 1 和 c2 = 0 2 这里 我们 
要 做 两个 星团 这 就是 我们 取 两个 质心 
的 原因 迭代 1For point A = 1 1 C1 
= 1 1 so ED = Square root 1 1 
2 + 1 1 2 = 0C2 = 0 2 
so ED = Square root 1 0 2 + 1 
2 2 = 1 . 41Here C1 C2 so point 
A belongs to cluster 1 . For point B = 
1 0 C1 = 1 1 so ED = Square 
root 1 1 2 + 0 1 2 = 1C2 
= 0 2 so ED = Square root 1 0 
2 + 0 2 2 = 2 . 23Here C1 
C2 so point B belongs to cluster 1 . For 
point C = 0 2 C1 = 1 1 so 
ED = Square root 0 1 2 + 2 1 
2 = 1 . 41C2 = 0 2 so ED 
= Square root 0 0 2 + 2 2 2 
= 0Here C1 C2 so point C belongs to cluster 
2 . For point D = 2 4 C1 = 
1 1 so ED = Square root 2 1 2 
+ 4 1 2 = 3 . 16C2 = 0 
2 so ED = Square root 2 0 2 + 
4 2 2 = 2 . 82Here C1 C2 so 
point C belongs to cluster 2 . For point E 
= 3 5 C1 = 1 1 so ED = 
Square root 3 1 2 + 5 1 2 = 
4 . 47C2 = 0 2 so ED = Square 
root 3 0 2 + 5 2 2 = 4 
. 24Here C1 C2 so point C belongs to cluster 
2 . 在 第一次 迭代 之后 我们 的 集群 看起来 
如下 cluster c1 有点 a 和b/nr c2 有点 c d 
和e/nr 因此 这里 我们 需要 根据 新的 cluster point 重新 
计算 质心 平均值 C1 = XA + XB / 2 
= 1 + 1 / 2 = 1C1 = YA 
+ YB / 2 = 1 + 0 / 2 
= 0 . 5So new C1 = 1 0.5 C2 
= Xc + XD + XE / 3 = 0 
+ 2 + 3 / 3 = 1 . 66C2 
= Yc + YD + YE / 3 = 2 
+ 4 + 5 / 3 = 3 . 66So 
new C2 = 1.66 3.66 我们 需要 以 和 迭代 
1 相同 的 方式 再次 进行 所有 的 计算 所以 
我们 得到 如下 的 值 迭代 2For point A = 
1 1 C1 = 1 0.5 so ED = Square 
root 1 1 2 + 1 0.5 2 = 0 
. 5C2 = 1.66 3.66 so ED = Square root 
1 1.66 2 + 1 3.66 2 = 2 . 
78Here C1 C2 so point A belongs to cluster 1 
. For point B = 1 0 C1 = 1 
0.5 so ED = Square root 1 1 2 + 
0 0.5 2 = 1C2 = 1.66 3.66 so ED 
= Square root 1 1.66 2 + 0 3.66 2 
= 3 . 76Here C1 C2 so point B belongs 
to cluster 1 . For point C = 0 2 
C1 = 1 0.5 so ED = Square root 0 
1 2 + 2 0.5 2 = 1 . 8C2 
= 1.66 3.66 so ED = Square root 0 1.66 
2 + 2 3.66 2 = 2 . 4Here C1 
C2 so point C belongs to cluster 1 . For 
point D = 2 4 C1 = 1 0.5 so 
ED = Square root 2 1 2 + 4 0.5 
2 = 3 . 6C2 = 1.66 3.66 so ED 
= Square root 2 1.66 2 + 4 3.66 2 
= 0 . 5Here C1 C2 so point C belongs 
to cluster 2 . For point E = 3 5 
C1 = 1 0.5 so ED = Square root 3 
1 2 + 5 0.5 2 = 4 . 9C2 
= 1.66 3.66 so ED = Square root 3 1.66 
2 + 5 3.66 2 = 1 . 9Here C1 
C2 so point C belongs to cluster 2 . 在 
第二次 迭代 之后 我们 的 集群 看起来 如下 C1 有点 
A B 和C/nr C2 有点 D 和E/nr C1 = XA 
+ XB + Xc / 3 = 1 + 1 
+ 0 / 3 = 0 . 7C1 = YA 
+ YB + Yc / 3 = 1 + 0 
+ 2 / 3 = 1So new C1 = 0.7 
1 C2 = XD + XE / 2 = 2 
+ 3 / 2 = 2 . 5C2 = YD 
+ YE / 2 = 4 + 5 / 2 
= 4 . 5So new C2 = 2.5 4.5 我们 
需要 进行 迭代 直到 集群 不会 改变 这 就是 为什么 
这个 算法 被称为 迭代 算法 的 原因 这是 k 均值 
聚 类 算法 的 直觉 现在 我们 将 查看 文档 
分类 应用 程序 中 的 一个 实际 示例 文档 聚 
类 文档 集群 可以 帮助 您 使用 推荐 系统 假设 
你 有 很多 研究 论文 而且 没有 标签 您 可以 
使用 k means 聚 类 算法 它 可以 帮助 您 
根据 文档 中 出现 的 单词 形成 聚 类 您 
可以 构建 一个 新闻 分类 应用程序 所有 来自 同一 类别 
的 新闻 都 应该 组合 在 一起 您 有 一个 
超集 类别 例如 体育新闻 而 这个 体育新闻 类别 包含 关于 
板球 足球 等 的 新闻 # We are going to 
generate 5 movie genre by using K mena clustering . 
from IPython . display import Image Image filename = . 
/ K _ means _ clustering / data / kmeanexample 
. png import numpy as np import pandas as pd 
import nltk from bs4 import BeautifulSoup import re import os 
import codecs from sklearn import feature _ extraction import mpld3 
# import three lists titles links and wikipedia synopses titles 
= open . / K _ means _ clustering / 
data / title _ list . txt encoding = utf 
8 . read . split \ n # ensures that 
only the first 100 are read in titles = titles 
100 links = open . / K _ means _ 
clustering / data / link _ list _ imdb . 
txt encoding = utf 8 . read . split \ 
n links = links 100 synopses _ wiki = open 
. / K _ means _ clustering / data / 
synopses _ list _ wiki . txt encoding = utf 
8 . read . split \ n BREAKS HERE synopses 
_ wiki = synopses _ wiki 100 synopses _ clean 
_ wiki = for text in synopses _ wiki text 
= BeautifulSoup text html . parser . getText # strips 
html formatting and converts to unicode synopses _ clean _ 
wiki . append text synopses _ wiki = synopses _ 
clean _ wiki genres = open . / K _ 
means _ clustering / data / genres _ list . 
txt encoding = utf 8 . read . split \ 
n genres = genres 100 print str len titles + 
titles print str len links + links print str len 
synopses _ wiki + synopses print str len genres + 
genres 100 titles 100 links 100 synopses 100 genressynopses _ 
imdb = open . / K _ means _ clustering 
/ data / synopses _ list _ imdb . txt 
encoding = utf 8 . read . split \ n 
BREAKS HERE synopses _ imdb = synopses _ imdb 100 
synopses _ clean _ imdb = for text in synopses 
_ imdb text = BeautifulSoup text html . parser . 
getText # strips html formatting and converts to unicode synopses 
_ clean _ imdb . append text synopses _ imdb 
= synopses _ clean _ imdbsynopses = for i in 
range len synopses _ wiki item = synopses _ wiki 
i + synopses _ imdb i synopses . append item 
# generates index for each item in the corpora in 
this case it s just rank and I ll use 
this for scoring later ranks = for i in range 
0 len titles ranks . append i # load nltk 
s English stopwords as variable called stopwords stopwords = nltk 
. corpus . stopwords . words english # load nltk 
s SnowballStemmer as variabled stemmer from nltk . stem . 
snowball import SnowballStemmer stemmer = SnowballStemmer english # here I 
define a tokenizer and stemmer which returns the set of 
stems in the text that it is passed def tokenize 
_ and _ stem text # first tokenize by sentence 
then by word to ensure that punctuation is caught as 
it s own token tokens = word for sent in 
nltk . sent _ tokenize text for word in nltk 
. word _ tokenize sent filtered _ tokens = # 
filter out any tokens not containing letters e . g 
. numeric tokens raw punctuation for token in tokens if 
re . search a zA Z token filtered _ tokens 
. append token stems = stemmer . stem t for 
t in filtered _ tokens return stems def tokenize _ 
only text # first tokenize by sentence then by word 
to ensure that punctuation is caught as it s own 
token tokens = word . lower for sent in nltk 
. sent _ tokenize text for word in nltk . 
word _ tokenize sent filtered _ tokens = # filter 
out any tokens not containing letters e . g . 
numeric tokens raw punctuation for token in tokens if re 
. search a zA Z token filtered _ tokens . 
append token return filtered _ tokenstotalvocab _ stemmed = totalvocab 
_ tokenized = for i in synopses allwords _ stemmed 
= tokenize _ and _ stem i totalvocab _ stemmed 
. extend allwords _ stemmed allwords _ tokenized = tokenize 
_ only i totalvocab _ tokenized . extend allwords _ 
tokenized vocab _ frame = pd . DataFrame { words 
totalvocab _ tokenized } index = totalvocab _ stemmed from 
sklearn . feature _ extraction . text import TfidfVectorizer tfidf 
_ vectorizer = TfidfVectorizer max _ df = 0.8 max 
_ features = 200000 min _ df = 0.2 stop 
_ words = english use _ idf = True tokenizer 
= tokenize _ and _ stem ngram _ range = 
1 3 % time tfidf _ matrix = tfidf _ 
vectorizer . fit _ transform synopses print tfidf _ matrix 
. shape Wall time 12.1 s 100 563 terms = 
tfidf _ vectorizer . get _ feature _ names from 
sklearn . metrics . pairwise import cosine _ similarity dist 
= 1 cosine _ similarity tfidf _ matrix from sklearn 
. cluster import KMeans num _ clusters = 5 km 
= KMeans n _ clusters = num _ clusters % 
time km . fit tfidf _ matrix clusters = km 
. labels _ . tolist Wall time 679 msfrom sklearn 
. externals import joblib # joblib . dump km doc 
_ cluster . pkl km = joblib . load . 
/ K _ means _ clustering / data / doc 
_ cluster . pkl clusters = km . labels _ 
. tolist d \ Program Files \ Anaconda3 \ lib 
\ site packages \ sklearn \ base . py 312 
UserWarning Trying to unpickle estimator KMeans from version pre 0.18 
when using version 0 . 19.0 . This might lead 
to breaking code or invalid results . Use at your 
own risk . UserWarning d \ Program Files \ Anaconda3 
\ lib \ site packages \ ipykernel \ _ _ 
main _ _ . py 4 D e p r 
e c a t i o n W a r 
n i n g The file . / K _ 
means _ clustering / data / doc _ cluster . 
pkl has been generated with a joblib version less than 
0.10 . Please regenerate this pickle file . import pandas 
as pd films = { title titles rank ranks synopsis 
synopses cluster clusters genre genres } frame = pd . 
DataFrame films index = clusters columns = rank title cluster 
genre frame cluster . value _ counts 4 26 0 
25 2 21 1 16 3 12 Name cluster dtype 
int64grouped = frame rank . groupby frame cluster grouped . 
mean cluster 0 47.200000 1 58.875000 2 49.380952 3 54.500000 
4 43.730769 Name rank dtype float64print Top terms per cluster 
print order _ centroids = km . cluster _ centers 
_ . argsort 1 for i in range num _ 
clusters print Cluster % d words % i end = 
for ind in order _ centroids i 6 print % 
s % vocab _ frame . ix terms ind . 
split . values . tolist 0 0 . encode utf 
8 ignore end = print print print Cluster % d 
titles % i end = for title in frame . 
loc i title . values . tolist print % s 
% title end = print print Top terms per cluster 
Cluster 0 words b family b home b mother b 
war b house b dies Cluster 0 titles Schindler s 
List One Flew Over the Cuckoo s Nest Gone with 
the Wind The Wizard of Oz Titanic Forrest Gump E 
. T . the Extra Terrestrial The Silence of the 
Lambs Gandhi A Streetcar Named Desire The Best Years of 
Our Lives My Fair Lady Ben Hur Doctor Zhivago The 
Pianist The Exorcist Out of Africa Good Will Hunting Terms 
of Endearment Giant The Grapes of Wrath Close Encounters of 
the Third Kind The Graduate Stagecoach Wuthering Heights Cluster 1 
words b police b car b killed b murders b 
driving b house Cluster 1 titles Casablanca Psycho Sunset Blvd 
. Vertigo Chinatown Amadeus High Noon The French Connection Fargo 
Pulp Fiction The Maltese Falcon A Clockwork Orange Double Indemnity 
Rebel Without a Cause The Third Man North by Northwest 
Cluster 2 words b father b new b york b 
new b brothers b apartments Cluster 2 titles The Godfather 
Raging Bull Citizen Kane The Godfather Part II On the 
Waterfront 12 Angry Men Rocky To Kill a Mockingbird Braveheart 
The Good the Bad and the Ugly The Apartment Goodfellas 
City Lights It Happened One Night Midnight Cowboy Mr . 
Smith Goes to Washington Rain Man Annie Hall Network Taxi 
Driver Rear Window Cluster 3 words d \ Program Files 
\ Anaconda3 \ lib \ site packages \ ipykernel \ 
_ _ main _ _ . py 7 D e 
p r e c a t i o n W 
a r n i n g . ix is deprecated 
. Please use . loc for label based indexing or 
. iloc for positional indexing See the documentation here http 
/ / pandas . pydata . org / pandas docs 
/ stable / indexing . html # ix indexer is 
deprecated b george b dance b singing b john b 
love b perform Cluster 3 titles West Side Story Singin 
in the Rain It s a Wonderful Life Some Like 
It Hot The Philadelphia Story An American in Paris The 
King s Speech A Place in the Sun Tootsie Nashville 
American Graffiti Yankee Doodle Dandy Cluster 4 words b killed 
b soldiers b captain b men b army b command 
Cluster 4 titles The Shawshank Redemption Lawrence of Arabia The 
Sound of Music Star Wars 2001 A Space Odyssey The 
Bridge on the River Kwai Dr . Strangelove or How 
I Learned to Stop Worrying and Love the Bomb Apocalypse 
Now The Lord of the Rings The Return of the 
King Gladiator From Here to Eternity Saving Private Ryan Unforgiven 
Raiders of the Lost Ark Patton Jaws Butch Cassidy and 
the Sundance Kid The Treasure of the Sierra Madre Platoon 
Dances with Wolves The Deer Hunter All Quiet on the 
Western Front Shane The Green Mile The African Queen Mutiny 
on the Bounty # This is purely to help export 
tables to html and to correct for my 0 start 
rank so that Godfather is 1 not 0 frame Rank 
= frame rank + 1 frame Title = frame title 
# export tables to HTML print frame Rank Title . 
loc frame cluster = = 1 . to _ html 
index = False table border = 1 class = dataframe 
thead tr style = text align right th Rank / 
th th Title / th / tr / thead tbody 
tr td 5 / td td Casablanca / td / 
tr tr td 13 / td td Psycho / td 
/ tr tr td 14 / td td Sunset Blvd 
. / td / tr tr td 15 / td 
td Vertigo / td / tr tr td 24 / 
td td Chinatown / td / tr tr td 31 
/ td td Amadeus / td / tr tr td 
57 / td td High Noon / td / tr 
tr td 64 / td td The French Connection / 
td / tr tr td 77 / td td Fargo 
/ td / tr tr td 87 / td td 
Pulp Fiction / td / tr tr td 91 / 
td td The Maltese Falcon / td / tr tr 
td 92 / td td A Clockwork Orange / td 
/ tr tr td 95 / td td Double Indemnity 
/ td / tr tr td 96 / td td 
Rebel Without a Cause / td / tr tr td 
98 / td td The Third Man / td / 
tr tr td 99 / td td North by Northwest 
/ td / tr / tbody / table K 均值 
聚 类 的 优点 这 是 一个 非常 简单 的 
NLP 应用 算法 它 解决 了 主要 的 问题 因为 
它 不 需要 标记 数据 或 结果 标签 您 可以 
将此 算法 用于 无 标记 数据 k 均值 聚 类 
的 缺点 集群 中心 的 初始化 是 一个 非常 关键 
的 部分 假设有 三个 簇 在 同一 簇 中 放置 
两个 形心 在 最后 一个 簇 中 放置 另一个 形心 
在 某种 程度 上 k 均值 聚 类 使 聚 
类 中所 有数 据点 的 欧几里得 距离 最小化 并且 它 
将 变得 稳定 因此 实际上 一个 聚 类 中有 两个 
形心 而 第三 个 聚 类 中 有一个 形心 在 
这种 情况 下 您 最终 只有 两个 集群 这 被 
称为 集群 中的 局部 最 小问题 这是 无 监督 学习 
算法 的 终结 在 这里 您 学习 了 k 均值 
聚 类 算法 并 开发 了 文档 分类 应用程序 如果 
你 想 更多 地 了解 这项 技术 试试 这个 练习 
练习 您 需要 探索 NLP 域中 的 层次 集群 及其 
应用 下 一节 非常 有趣 我们 将 研究 半 监督 
机器学习 技术 在 这里 我们 将 得到 它们 的 概述 
所以 让 我们 了解 这些 技术 8 . 3.3 半 
监督 机器学习 算法 当 您 有一个 训练 数据集 该 数据 
集中 的 某些 数据 具有 目标 概念 或 目标 标签 
而 另一 部分 数据 没有 任何 标签 时 基本上 使用 
半 监督 ML 或 半 监督 学习 SSL 如果 您 
有 这种 数据集 那么 可以 应用 半 监督 的 ML 
算法 当/t 我们/r 有/v 非常/d 少量/m 的/uj 标记/n 数据/n 和/c 
大量/n 的/uj 未/d 标记/n 数据/n 时/n 我们 可以 使用 半 
监督 技术 如果 您 想为 任何 本地 语言 除了 英语 
构建 一个 NLP 工具 并且 您 有 非常 少量 的 
标记 数据 那么 您 可以 使用 半 监督 方法 在 
这种 方法 中 我们 将 使用 一个 使用 标记 数据 
并 生成 ML 模型 的 分类器 此 ML Model 用于 
为 未 标记 的 数据集 生成 标签 分类器 用于 对 
未 标记 的 数据 集 进行 高 置信度 预测 您 
可以 使用 任何 适当 的 分类器 算法 对 标记 的 
数据 进行 分类 半 监督 技术 是 一个 重要 的 
研究 领域 特别是在 NLP 应用 中 去年 Google Research 开发 
了 基于 图形 的 半 监督 技术 https / / 
research . googleblog . com / 2016/10 / graph powered 
machine learning at google . html8 . 3.4 一些 重要 
概念 在 这 一节 中 我们 将 看到 那些 帮助 
我们 了解 我们 如何 掌握 我们 的 数据 集 的 
训练 你 应该 如何 判断 你 说过 这些情况 你 应该 
做 什么 什么 是 新的 应用 评价 矩阵 所以 让 
我们 找到 所有 这些 问题 的 答案 我 将 介绍 
以下 主题 我们 将 逐一 查看 所有 这些 内容 偏差 
方差 权衡 欠 拟合 过度 拟合 评价 矩阵 偏差 方差 
权衡 在 这里 我们 将 看到 一个 关于 偏差 方差 
权衡 的 高级 概念 让 我们 逐一 理解 每个 术语 
让 我们 先 了解 偏见 这个词 当 您 使用 ML 
算法 执行 训练 时 您 发现 生成 的 ML 模型 
在 第一轮 训练 迭代 中的 性能 没有 不同 那么 您 
可以 立即 识别 出 ML 算法 有 很高 的 偏差 
在 这种 情况 下 ML 算法 没有 能力 从 给定 
的 数据 中 学习 因此 它 不会 学习 您 期望 
ML 算法 学习 的 新 内容 如果 你 的 算法 
有 很高 的 偏差 那么 它 最终 会 停止 学习 
假设 您 正在 构建 一个 情绪 分析 应用程序 并且 您 
已经 提出 了 MLModel 现在 您 对 ML 模型 的 
准确性 不满意 希望 改进 模型 您 将 通过 添加 一些 
新 特性 和 更改 一些 算法 参数 来 进行 训练 
现在 这个 新 生成 的 模型 在 测试 数据 上 
不能 很好 地 执行 或以 不同 的 方式 执行 这表明 
您 可能 有 很高 的 偏差 您 的 ML 算法 
不会 以 预期 的 方式 收敛 因此 您 可以 改进 
ML 模型 结果 让 我们 理解 第二个 术语 方差 所以 
您 使用 任何 ML 算法 来 训练 您 的 模型 
并且 观察到 您 得到 了 非常 好 的 训练 精度 
但是 您 应用 相同 的 ML 模型 来 为 一个 
看 不见 的 测试 数据集 生成 输出 并且 您 的 
ML 模型 不能 很好 地 工作 这种 情况 下 你 
有 很好 的 训练 精度 而 MLModel 对于 看 不见 
的 数据 效果 不佳 这 被 称为 高 方差 情况 
因此 在 这里 ML 模型 只能 复制 它 在 训练 
数据 中 看到 的 预测 或 输出 并且 没有 足够 
的 偏差 来 概括 看 不见 的 情况 换句话说 您 
可以 说 您 的 ML 算法 正试图 记住 每个 训练 
示例 最后 它 只是 模仿 您 的 测试数据 集上 的 
输出 如果/c 您/zg 有/v 一个/m 高/a 方差/n 问题/n 那么 您 
的 模型 将 以 这样 一种 方式 聚合 即 它 
试图 将 数据集 的 每个 示例 分类 到 一个 特定 
的 类别 中 这种 情况 导致 我们 过度 拟合 我会 
解释 什么 是 过度 拟合 所以 别担心 我们 几 分钟 
后到/nr 为了 克服 前面 的 两个 坏 情况 我们 真的 
需要 一些 中间 的 东西 这/r 意味着/v 没有/v 高/a 偏差/n 
和高/nr 方差/n 对于 ML 算法 产生 最 大偏差 和 最佳 
方差 的 技术 可以 得到 最佳 的 ML 模型 您 
的 ML 模型 可能 并不 完美 但 它 都是 关于 
生成 最佳 偏差 方差 权衡 在下 一节 中 您 将 
学习 欠 拟合 和 过拟合 的 概念 以及/c 帮助/v 您/zg 
摆脱/v 这些/r 高/a 偏差/n 和高/nr 方差/n 场景/n 的/uj 技巧/n 欠 
拟 合在 本节 中 我们 将 讨论 欠 拟合 这个 
术语 什么 是 欠 拟合 它 与 偏 方差 权衡 
有 什么 关系 假设 您 使用 任何 ML 算法 对 
数据 进行 训练 就会 得到 一个 很高 的 训练 错误 
参见 . 60 前面 的 情况 我们 得到 一个 非常 
高的/nr 训练 错误 被 称为 欠 拟合 ML 算法 不能 
很好 地 处理 训练 数据 现在 我们 将 尝试 更高 
程度 的 多项式 而 不是 线性 决策 边界 参见 . 
61 这张 图 有 一条 非常 曲折 的 线 它 
不能 很好 地 处理 训练 数据 换句话说 您 可以 说 
它 与 前 一次 迭代 的 执行 情况 相同 这表明 
ML 模型 有 很高 的 偏差 并且 没有 学 到 
新的 东西 过度 拟 合在 本节 中 我们 将 讨论 
过度 拟合 这一 术语 当 我 解释 上 一节 中的 
差异 时 我 把 这个 术语 放在 你 面前 所以 
是 时候 解释 过拟合 了 为了 解释 它 我 想 
举个 例子 假设 我们 有 一个 数据集 我们 将 所有 
数 据点 绘制 在 二维 平面 上 现在 我们 正 
试图 对 数据 进行 分类 我们 的 ML 算法 绘制 
一个 决策 边界 来 对 数据 进行 分类 如 . 
62 所示 Alt https / / img blog . csdnimg 
. cn / 2 0 1 9 0 2 0 
2 2 3 2 2 0 8 6 0 2 
. png 如果 你 看 左边 的 图 你 会 
看到 作为 决策 边界 的 直线 现在 这个 图 显示 
了 一个 训练 错误 所以 在 第二 次 迭代 中 
调整 参数 您 将 获得 非常 好 的 训练 精度 
请 参见 右侧 图 您 希望 测试数据 也能 获得 良好 
的 测试 精度 但是 ML 模型 在 测试 数据 预测 
方面 做 得 非常 糟糕 因此 这种 算法 具有 很好 
的 训练 精度 但在 测试数据 上 表现 不佳 的 情况 
称为 过度 拟合 在 这种 情况 下 ML 模型 具有 
很高 的 方差 并且 不能 概括 未知 的 数据 既然 
你 已经 看到 了 欠 拟合 和 过度 拟合 那么 
有 一些 经验法则 可以 帮助 你 避免 这些 情况 始终 
将 培训 数据 分为 三 部分 60% 的 数据 集 
应 视为 训练 数据集 数据集 的 20% 应被 视为 验证 
数据集 或 开发 数据集 这将 有助于 获得 ML 算法 的 
中间 精度 以便 您 可以 捕获 意外 的 内容 并 
根据 此 更改 算法 应 保留 数据集 的 20% 以 
报告 最终 精度 这将 是 测试 数据集 您 还应该 应用 
k 折叠 交叉 验证 k 表示 需要 验证 的 次数 
假设 我们 把 它 设为 三 我们 将 训练 数据 
分成 三个 相等 的 部分 在 训练 算法 的 第一 
个 时间 戳 中 我们 使用 两个 部分 对 单个 
部分 进行 测试 因此 从 技术 上 讲 它 将以 
66.66% 的 速度 进行 训练 并以 33.34% 的 速度 进行 
测试 然后 在 第二 个 时间 戳 中 ML 算法 
使用 一个 部分 对 两个 部分 进行 测试 最后 一个 
时间 戳 将 使用 整个 数据集 进行 培训 和 测试 
经过 三次 时间戳 后 计算 平均误差 找出 最佳 模型 通常 
对于 合理 数量 的 数据集 k 应取 10 对于 ML 
模型 您 不能 有 100% 的 准确度 这 背后 的 
主要 原因 是 您 的 输入 数据 中 存在 一些 
您 不能 真正 删除 的 噪声 这 被 称为 不 
可约 错误 因此 ML 算法 的 最终 误差 方程 如下 
Total error = Bias + Variance + Irreducible Error 你 
真的 不能 摆脱 不 可约 误差 所以 你 应该 集中 
精力 在 偏差 和 方差 上 请参阅 . 63 这将/i 
有助于/v 向您/nr 展示/v 如何/r 处理/v 偏差/n 和/c 差异/n 权衡/nr Alt 
https / / img blog . csdnimg . cn / 
2 0 1 9 0 2 0 2 2 3 
2 2 5 1 5 2 7 . png 评价 
矩阵 对于 我们 的 代码 我们 会 检查 准确性 但在 
评估 一个 ML 模型 时 我们 真的 不 知道 哪些 
属性 起 主要 作用 因此 在 这里 我们 将 考虑 
一个 广泛 用于 NLP 应用 程序 的 矩阵 这种 评价 
矩阵 称为 F1 得分 或 F 度量 它 有 三个 
主要 组成部分 在此之前 让 我们 介绍 一些 术语 真 阳性 
TP 这 是 一个 由 分类器 标记 为 a 的 
数据 点 实际上 是 来自 类 a 真 阴性 TN 
这是 对 分类器 中 任何 类 的 适当 拒绝 这 
意味着 分类器 不会 将 数据 点 随机 分类 为 类 
A 但会 拒绝 错误 的 标签 假 阳性 FP 这也 
被 称为 I 型 错误 让 我们 通过 一个 例子 
来 理解 这个 度量 一个人 为 癌症 测试 献血 他 
实际上 没有 癌症 但 他 的 测试 结果 是 阳性 
的 这 就是 所谓 的 FP 假 阴性 FN 这也 
被 称为 II 型 错误 让 我们 通过 一个 例子 
来 理解 这个 度量 一个人 为 癌症 测试 献血 他 
得了 癌症 但 他 的 检查结果 是 阴性 的 所以 
它 实际上 忽略 了 类 标签 这 叫做 FN 精度 
精度 是 精确性 的 度量 分类器 标记 为 正 且 
实际 为 正 的 数据 点 的 百分比 是 多少 
精度 = TP / TP + FP 召回 召回 是 
完整性 的 度量 分类器 将 阳性 数 据点 的 百分比 
标记 为 阳性 召回 = TP / TP + FNF 
度量 这 只是 衡量 精度 和 召回 的 衡量 标准 
见 公式 F = 2 精度 召回 / 精度 + 
召回 除此之外 您 还 可以 使用 混淆 矩阵 来 了解 
TP TN FP 和 FN 中的 每一个 您 可以 使用 
ROC 曲 线下 的 区域 该 区域 指示 您 的 
分类器 能够 区分 负 类 和正类/nr 的 程度 roc = 
1.0 表示 模型 正确 预测 了 所有 类 0.5 表示 
模型 只是 进行 随机 预测 8 . 3.5 特征选择 现在 
是 时候 理解 我们 如何 在 第一 次 迭代 之后 
即兴 创作 我们 的 模型 了 有时候 特性 工程 在 
这 方面 帮助 了 我们 很多 在 第 5 章 
特征 工程 和 NLP 算法 以及 第 6 章 高级 
特征 工程 和 NLP 算法 中 我们/r 解释/v 了/ul 如何/r 
使用/v 各种/r NLP/w 概念/n 和/c 统计/v 概念/n 从/p 文本/n 数据/n 
中/f 提取/v 特征/n 作/v 为特征/i 工程/n 的/uj 一部分/m 特征 工程 
包括 特征提取 和 特征选择 现在 是 时候 探索 作为 特征选择 
一 部分 的 技术 了 特征提取 和 特征 选择 为 
我们 的 NLP 应用 提供 了 最 重要 的 特征 
一旦 我们 设置 了 这些 特性 您 就 可以 使用 
各种 ML 算法 来 生成 最终 结果 如前所述 特征提取 和 
特征 选择 是 特征 工程 的 一部分 在 本节 中 
我们 将 介绍 特征选择 您 可能 想 知道 我们 为什么 
要 学习 特 性选择 但 有 一些 原因 我们 将 
研究 其中 的 每一个 首先 我们 将 看到 对 特征选择 
的 基本 理解 特征选择 也 称为 变量 选择 属 性选择 
或 变量 子集 选择 特征选择 是 选择 最佳 相关 特征 
变量 或 数据 属性 的 过程 这些 特征 变量 或 
数据 属性 可以 帮助 我们 开发 更 高效 的 机器学习 
模型 如果 可以 确定 哪些 功能 贡献 很大 哪些 功能 
贡献 较小 则 可以 选择 最 重要 的 功能 并 
删除 其他 不 重要 的 功能 只要 后 退一步 首先 
了解 我们 使用 功能 选择 试图 解决 的 问题 是 
什么 使用 功能 选择 技术 我们 可以 获得 以下 好处 
选择 相关 和 适当 的 特性 将 帮助 您 简化 
ML 模型 这将 帮助 您 轻松 地 解释 ML 模型 
并 降低 ML 模型 的 复杂性 使用 特征选择 技术 选择 
适当 的 特征 将 帮助 我们 提高 ML 模型 的 
准确性 特征选择 有助于 机器学习 算法 更快 地 训练 功能 选择 
还 可以 防止 过度 拟合 它 帮助 我们 摆脱 维度 
的 诅咒 维数 之 咒 让 我们 理解 我 所说 
的 维度 性 诅咒 是 什么 意思 因为 这个 概念 
将 帮助 我们 理解 为什么 我们 需要 特 性选择 技术 
维数 的 诅咒 是 说 随着 特征 或 维数 的 
增加 这 意味着 我们 的 机器学习 算法 增加 了 新的 
特征 那么 我们 需要 推广 的 数据 量 将以 指数 
形式 精确 增长 让 我们 举 个 例子 来 看看 
假设 你 有 一条线 一维 特征 空间 我们 在 这条 
线上 放置 了 五个 点 你 可以 看到 每 一点 
都在 这条 线上 占据 了 一些 空间 每 一点 占 
直线 上 空间 的 五分之一 参见 . 64 如果 你 
有 二维 特征 空间 那么 我们 需要 五 个 以上 
的 数据 点来 填充 这个 空间 所以 我们 需要 25个 
数据 点来 表示 这 两个 维度 现在 每个 点 都 
占据 了 空间 的 1/25 见 . 65 如果 你 
有一个 三维 特征 空间 这 意味着 我们 有 三个 特征 
那么 我们 需要 填充 立方体 如 . 66 所示 但是 
要 填充 立方体 您 需要 正好 125 个数 据点 如 
. 66 所示 假设有 125个 点 所以 每次 我们 添加 
功能 时 都 需要 更多 的 数据 我/r 想/v 你们/r 
都会/nr 同意/d 数/n 据点/d 的/uj 增长/v 从5/nr 25 125 等 
指数级 增长 所以 一般来说 您 需要 x d 特性 空间 
其中 x 是 训练 中 的 数据 点 数量 d 
是 特性 或 维度 数量 如果 您 只是 盲目 地 
添加 越来越 多 的 特性 以便 您 的 ML 算法 
更好 地 理解 数据集 那么 实际上 您 所做 的 就是 
强制 您 的 ML 算法 用 数据 填充 更大 的 
特性 空间 你 可以 用 一个 简单 的 方法 来 
解决 这个 问题 在 这种 情况 下 您 需要 为 
您 的 算法 提供 更多 的 数据 而 不是 特性 
现在 你 真的 认为 我 限制 了 你 添加 新功能 
所以 让 我 为 你 澄清 一下 如果 需要 添加 
功能 那么 您 可以 您 只 需要 选择 帮助 您 
的 ML 算法 从中 学习 的 最佳 和 最小 数量 
的 功能 我 真的 建议 您 不要 盲目 地 添加 
太 多功能 现在 我们 如何 获得 最佳 特性 集 为 
我 正在 构建 的 特定 应用程序 设置 的 最佳 特性 
是 什么 我 如何 知道 我 的 ML 算法 将 
在 这个 特性 集上 运行 良好 我 将 在下 一节 
特 性选择 技术 中 提供 所有 这些 问题 的 答案 
在 这里 我 将给 您 一个 关于 特 性选择 技术 
的 基本 概念 我 建议 您 在 我们 目前 开发 
的 NLP 应用 程序 中 实际 实现 它们 特征选择 技术 
使 一切 尽可能 简单 但 不简单 阿尔伯特 爱因斯坦 的 这 
句话 在 我们 谈论 特征选择 技术 时是/nr 非常 正确 的 
我们 已经 看到 为了 摆脱 维度 性 的 诅咒 我们 
需要 特征选择 技术 我们 将 介绍 以下 功能 选择 技术 
滤波 法 包装 方法 嵌 入法 滤波 法 特征选择 完全 
是 一个 独立 的 活动 独立 于 ML 算法 对于 
数字 数据集 通常在 预处理 数据 时 使用 此 方法 对于 
NLP 域 应在 将 文本 数据 转换 为 数字 格式 
或 矢量 格式 后 执行 此 方法 让 我们 首先 
在 . 67 中 看到 这个 方法 的 基本 步骤 
这些 步骤 非常 清楚 也 不 需要 解释 在 这里 
我们 使用 统计 技术 来 给 我们 打分 基于 此 
我们 将 决定 是 保留 这个 特性 还是 只 删除 
它 参见 . 68 如果 特征 和 响应 都是 连续 
的 那么 我们 将 执行 相关性 如果 特征 和 响应 
都是 分类 的 那么 我们 将 使用 Chi Square 在 
NLP 中 我们 主要 使用 这个 如果 特征 是 连续 
的 响 应是 分类 的 那么 我们 将 使用 线性 
判别分析 LDA 如果 特征 是 分类 的 响应 是 连续 
的 那么 我们 将 使用 方差分析 我 将 更加 关注 
NLP 领域 并 解释 LDA 和 Chi Square 的 基础 
知识 LDA 通常用于 寻找 表征 或 分离 一类 以上 分类 
变量 的 特征 的 线性组合 而与 LDA 相比 Chi Square 
主要 用于 NLP 将 卡方 应用 到 一组 分类 特征 
中 以 了解 使用 频率 分布 的 特征 之间 相关 
或 关联 的 可能性 包装 方法 在 这个 方法 中 
我们 正在 寻找 最佳 特性 集 这种 方法 计算 上 
非常 昂贵 因为 我们 需要 为 每次 迭代 搜索 最佳 
特性 子集 参见 . 69 中 的 基本 步骤 我们 
可以 使用 三 种子 方法 来 选择 最佳 功能 子集 
正向/d 选择/v 向后/nr 选择/v 递归/v 特征/n 消除/v 在/p 正向/d 选择/v 
中/f 我们 从 没有 特性 开始 并在 每次 迭代 中 
添加 改进 MLModel 的 特性 我们 继续 这个 过程 直到 
我们 的 模型 不能 进一步 提高 其 精度 反向 选择 
是 另一种 方法 我们 从 所有 特性 开始 在 每次 
迭代 中 我们 找到 最佳 特性 并 删除 其他 不 
必要 的 特性 然后 重复 直到 在 ML 模型 中 
观察 到 没有 进一步 的 改进 递归 特征 消除 使用 
贪婪 的 方法 来 找出 性能 最好 的 特征 子集 
它 反复 创建 模型 并为 每个 迭代 保留 最佳 或 
最差 的 性能 特性 下一次 它 使用 最好 的 特性 
并 创建 模型 直到 所有 特性 都 用尽 为止 最后 
它 根据 消除 它们 的 顺序 对 特性 进行 排序 
嵌 入法 在 这个 方法 中 我们 结合 了 过滤器 
和 包装 器 方法 的 特性 该 方法 由 具有 
自己 内置 特征选择 方法 的 算法 实现 见 . 70 
8 . 3.6 维度 约 减 降 维 是 机器 
学习 中 一个 非常 有用 的 概念 如果 我们 在 
开发 我们 的 ML 模型 时 包含 了 很多 特性 
那么 有时 我们 会 包含 一些 真正 不 需要 的 
特性 有时 我们 需要 高维 的 特征 空间 有/v 哪些/r 
方法/n 可以/c 让/v 我们/r 的/uj 功能/n 空间/n 有/v 一定/d 的/uj 
意义/n 因此 我们 需要 一些 技术 来 帮助 我们 去 
除 不 必要 的 特征 或者 将 我们 的 高维 
特征 空间 转换 为 二维 或 三维 特征 以便 我们 
能够 看到 所有 发生 的 事情 顺便 说 一下 我们 
在 第 6 章 高级 功能 工程 和 NLP 算法 
中 使用 了 这个 概念 当时 我们 开发 了 一个 
应用程序 生成 了 word2vec 用于 权力游戏 数据集 当时 我们 使用 
t 分布 随机 邻域 嵌入 t sne 降 维 技术 
将 我们 的 结果 可视化 到 二维 空间 中 在 
这里 我们 将 看到 最 著名 的 两种 技术 即 
主 成分 分析 PCA 和T/nr SNE 用于 在 二维 空间 
中 可视化 高维 数据 那么 我们 开始 吧 主 成分 
分析 PCA 是 一种 使用 正交变换 将 一组 可能 相关 
特征 的 数据 点 转换 为 一组 线性 不相关 特征值 
的 统计 方法 称 为主 成分 主 成分 的 数量 
小于 或 等于 原始 特征 的 数量 这种 技术 定义 
转换 的 方式 是 第一 个 主要 组件 对 每个 
后续 特性 具有 最大 的 可能 方差 这个 图 对 
理解 PCA 有 很大 帮助 我们 取了 两个 主 成分 
它们 彼此 正交 并且 使 方差 尽可能 大 在 C 
图中 我们 通过 在 一条 直线 上 投影 把 尺寸 
从 二维 缩小到 一维 PCA 的 缺点 是 当 你 
减少 维度 时 它 就 失去 了 数据 点 所 
代表 的 意义 如果 解释性 是 维度 减少 的 主要 
原因 那么 就 不 应该 使用 PCA 可以 使用 T 
SNE t SNE 这是 帮助 我们 可视化 高维 非线性 空间 
的 技术 T SNE 试图 保留 紧密 相连 的 本地 
数 据点 组 当 你 想 看到 高维空间 时 这项 
技术 将 帮助 你 您 可以 使用 它 来 可视化 
使用 Word2vec 图像 分类 等 技术 的 应用 程序 有关 
详细信息 请参阅 此 链接 https / / lvdmaaten . github 
. io / tsne / 8.4 自然语言/l 处理/v 中/f 的/uj 
混合/vn 方法/n 混合/vn 方法/n 有时/nr 确实/ad 有助于/v 我们/r 改进/v NLP/w 
应用/v 程序/n 的/uj 结果/n 例如 如果 我们 正在 开发 一个 
语法 修正 系统 一个 模块 识别 多字 表达式 如 kick 
the bucket 一个 基于 规则 的 模块 识别 错误 的 
模式 并 生成 正确 的 模式 这 是 一种 混合 
方法 让 我们 为 同一 个 NLP 应用程序 再举 一个 
例子 您 正在 创建 一个 分类器 用于 标识 句子 中 
名词 短语 的 正确 文章 限定词 a an 和 the 
在 这个 系统 中 您 可以 分为 两类 a / 
an 和 the 我们 需要 开发 一个 分类器 它 将 
生成 限定符 类别 无论是 a / an 还是 the 一旦 
我们 为 名词 短语 生成 了 文章 我们 就 可以 
应用 一个 基于 规则 的 系统 来 进一步 确定 第一 
类 a / an 的 实际 限定符 我们 也 知道 
一些 英语语法 规则 我们 可以 用 它 来 决定 我们 
应该 用 a 还是 an 这也 是 混合 方法 的 
一个 例子 为了 更好 地 进行 情绪 分析 我们 还 
可以 使用 混合 方法 包括 基于 词汇 的 方法 基于 
ML 的 方法 或 Word2vec 或 Glove 预 训练 模型 
以 获得 真正 高的/nr 准确性 所以 要有 创意 了解 你 
的 NLP 问题 这样 你 就 可以 利用 不同 类型 
的 技术 使 你 的 NLP 应用程序 更好 后 处理 
是 一种 基于 规则 的 系统 假设 您 正在 开发 
一个 机器翻译 应用程序 并且 您 生成 的 模型 会 犯 
一些 特定 的 错误 你 想 让 机器 翻译 MT 
模型 避免 这些 错误 但要 避免 这些 错误 需要 很多 
特性 这些 特性 会使 训练 过程 变慢 使 模型 过于 
复杂 另一方面 如果 你 知道 有 一些 简单 的 规则 
或 近似值 一旦 生成 了 输出 就 可以 帮助 你 
使之 更 准确 然后 我们 可以 使用 后 处理 我们 
的 机器 翻译 模型 混合模型/i 和后/nr 处理/v 有/v 什么/r 区别/n 
让 我 来 澄清 你 的 困惑 在 给定 的 
示例 中 我 使用 了 单词 近似 因此 您 也 
可以 应用 近似值 例如 应用 阈值 来 调整 结果 而 
不是 使用 规则 但 只有 当 您 知道 近似值 会给 
出 准确 的 结果 时 才 应该 应用 近似值 这种 
近似 应该 对 NLP 系统 进行 足够 的 推广 8.5 
总 结在 本 章中 我们 研究 了 ML 的 基本 
概念 以及 NLP 领域 中 使用 的 各种 分类 算法 
在 NLP 中 与 线性 回归 相比 我们 主要 使用 
分类 算法 我们 已经 看到 一些 非常 酷 的 例子 
比如 垃圾邮件 过滤 情绪 分析 等等 我们 还 重新 访问 
了 词性 标注 示例 为 您 提供 更好 的 理解 
我们 研究 了无 监督 的 ML 算法 和 重要 概念 
如 偏差 方差 权衡 欠 拟合 过度 拟合 评估 矩阵 
等 我们 还 了解 了 特征 选择 和降维/nr 我们 还 
讨论 了 混合 ML 方法 和 后处理 因此 在 本 
章中 我们 主要 了解 如何 开发 和 微调 NLP 应用程序 
在下 一 章中 我们 将 看到 一个 机器 学习 的 
新时代 深度 学习 我们 将 探索 人工智能 所需 的 基本 
概念 在此之后 我们 将 讨论 深度 学习 的 基础 知识 
包括 线性 回归 和 梯度 下降 我们 将 看到 为什么 
深度 学习 成为 过去 几年 最 流行 的 技术 我们 
将 看到 与 深度 学习 相关 的 数学 必要 概念 
探索 深度 神经 网络 的 结构 并 开发 一些 很酷 
的 应用 程序 如 NLU 域 的 机器 翻译 和 
NLG 域 的 文本 摘要 我们 将 使用 TensorFlow Keras 
和 其他 一些 最新 的 工具 来 实现 这 一点 
我们/r 还/d 将/d 看到/v 可以/c 应用/v 于/p 传统/n ML/w 算法/n 
和/c 深度/ns 学习/v 算法/n 的/uj 基本/n 优化/vn 技术/n 让 我们 
在 下 一章 深入 了解 深入 学习 的 世界 致谢 
Python 自然语言 处理 1 2 3 作者 印 雅兰 萨 
纳卡 Jalaj Thanaki 是 实践性 很强 的 一部 新作 为 
进一步 深入 理解 书中 内容 对 部分 内容 进行 了 
延伸 学习 练习 在此 分享 期待 对 大家 有所 帮助 
欢迎 加 我 微信 验证 NLP 一起 学习 讨论 不足之处 
欢迎 指正 参考文献 https / / github . com / 
jalajthanaki ↩ ︎ Python 自然语言 处理 印 雅兰 萨 纳卡 
Jalaj Thanaki 著 张金超/nr 刘 舒曼 等 译 机械 工业 
出版社 2018 ↩ ︎ Jalaj Thanaki Python Natural Language Processing 
2017 ↩ ︎ 转 载于 https / / blog . 
csdn . net / nuoline / article / details / 
8610774 和 https / / blog . csdn . net 
/ sinat _ 29694963 / article / details / 805911231 
. 国际 学术 组织 学术 会议 与 学术 论文 自然语言 
处理 natural language processing NLP 在 很大 程度 上 与 
计算 语言学 computational linguistics CL 重合 与 其他 计算机 学科 
类似 NLP / CL 有 一个 属于 自己 的 最 
权威 的 国际 专业 学会 叫做 The Association for Computational 
Linguistics ACL URL http / / aclweb . org / 
这个 协会 主办 了 NLP / CL 领域 最 权威 
的 国际 会议 即 ACL 年会 ACL 学会 还会在 北美 
和 欧洲 召开 分 年会 分别 称为 NAACL 和 EACL 
除此之外 ACL 学会 下设 多个 特殊 兴趣小组 special interest groups 
SIGs 聚集 了 NLP / CL 不同 子 领域 的 
学者 性质 类似 一个 大学 校园 的 兴趣 社团 其中 
比较 有名 的 诸如 SIGDAT Linguistic data and corpus based 
approaches to NLP SIGNLL Natural Language Learning 等 这些 SIGs 
也会 召开 一些 国际 学术 会议 其中 比较 有名 的 
就是 SIGDAT 组织 的 EMNLP Conference on Empirical Methods on 
Natural Language Processing 和 SIGNLL 组织 的 CoNLL Conference on 
Natural Language Learning 此外 还有 一个 International Committee on Computational 
Linguistics 的 老牌 NLP / CL 学术 组织 它 每 
两年 组织 一个 称为 International Conference on Computational Linguistics COLING 
的 国际 会议 也是 NLP / CL 的 重要 学术 
会议 NLP / CL 的 主要 学术 论文 就 分布 
在 这些 会议 上 作为 NLP / CL 领域 的 
学者 最大 的 幸福 在于 ACL 学会 网站 建立 了 
称作 ACL Anthology 的 页面 URL http / / aclweb 
. org / anthology new / 支持 该 领域 绝大部分 
国际 学术 会议 论文 的 免费 下载 甚至 包含 了 
其他 组织 主办 的 学术 会议 例如 COLING IJCNLP 等 
并 支持 基于 Google 的 全文 检索 功能 可谓 一站 
在手 NLP 论文 我 有 由于 这个 论文 集合 非常 
庞大 并且 可以 开放 获取 很多 学者 也 基于 它 
开展 研究 提供 了 更 丰富 的 检索 支持 具体 
入口 可以 参考 ACL Anthology 页面 上方 搜索框 右侧 的 
不同 检索 按钮 与 大部分 计算机 学科 类似 由于 技术 
发展 迅速 NLP / CL 领域 更 重视 发表 学术 
会议 论文 原因 是 发表 周期短 并 可以 通过 会议 
进行 交流 当然 NLP / CL 也有 自己 的 旗舰 
学术期刊 发表 过 很多 经典 学术论文 那 就是 Computational Linguistics 
URL http / / www . mitpressjournals . org / 
loi / coli 该 期刊 每期 只有 几 篇文章 平均/a 
质量/n 高于/nr 会议/n 论文/nz 时间 允许 的话 值得 及时 追踪 
此外 ACL 学会 为了 提高 学术 影响力 也 刚刚 创办 
了 Transactions of ACL TACL URL http / / www 
. transacl . org / 值得 关注 值得一提的是 这 两份 
期刊 也都 是 开放 获取 的 此外 也 有 一些 
与 NLP / CL 有关 的 期刊 如 ACM Transactions 
on Speech and Language Processing ACM Transactions on Asian Language 
Information Processing Journal of Quantitative Linguistics 等等 根据 Google Scholar 
Metrics 2013年 对 NLP / CL 学术 期刊 和 会议 
的 评价 ACL EMNLP NAACL COLING LREC Computational Linguistics 位于 
前 5位 基本 反映 了 本 领域 学者 的 关注 
程度 NLP / CL 作为 交叉学科 其 相关 领域 也 
值得 关注 主要 包括 以下 几个 方面 1 信息检索 和 
数据挖掘 领域 相关 学术 会议 主要 由 美国 计算机 学会 
ACM 主办 包括 SIGIR WWW WSDM 等 2 人工智能 领域 
相关 学术 会议 主要 包括 AAAI 和 IJCAI 等 相关 
学术 期刊 主要 包括 Artificial Intelligence 和 Journal of AI 
Research 3 机器学习 领域 相关 学术 会议 主要 包括 ICML 
NIPS AISTATS UAI 等 相关 学术 期刊 主要 包括 Journal 
of Machine Learning Research JMLR 和 Machine Learning ML 等 
例如 最近 兴起 的 knowledge graph 研究 论文 就有/i 相当/d 
一/m 部分/n 发表/v 在/p 人工/n 智能/n 和/c 信息检索/n 领域/n 的/uj 
会议/n 和/c 期刊/n 上/f 实际上 国内 计算机 学会 CCF 制定 
了 中国 计算机 学会 推荐 国际 学术 会议 和 期刊目录 
http / / www . ccf . org . cn 
/ sites / ccf / aboutpm . jsp contentId = 
2567814757463 通过 这个 列表 可以 迅速 了解 每个 领域 的 
主要 期刊 与 学术 会议 最后 值得一提的是 美国 Hal Daum 
é III 维护 了 一个 natural language processing 的 博客 
http / / nlpers . blogspot . com / 经常 
评论 最新 学术 动态 值得 关注 我 经常 看 他 
关于 ACL NAACL 等 学术 会议 的 参会 感想 和对/nr 
论文 的 点评 很 有启发 另外 ACL 学会 维护 了 
一个 Wiki 页面 http / / aclweb . org / 
aclwiki / 包含 了 大量 NLP / CL 的 相关 
信息 如 著名 研究 机构 历届 会议录 用率 等等 都是 
居家 必备 之 良 品 值得 深挖 * * 2 
. 国内 学术 组织 学术 会议 与 学术 论文 * 
* 与 国际 上 相似 国内 也 有一个 与 NLP 
/ CL 相关 的 学会 叫做 中国 中文信息 学会 URL 
http / / www . cipsc . org . cn 
/ 通过 学会 的 理事 名单 http / / www 
. cipsc . org . cn / lingdao . php 
基本 可以 了解 国内 从事 NLP / CL 的 主要 
单位 和 学者 学会 每年 组织 很多 学术会议 例如 全国 
计算 语言学 学术会议 CCL 全国 青年 计算 语言学 研讨会 YCCL 
全国 信息检索 学术会议 CCIR 全国 机器翻译 研讨会 CWMT 等等 是 
国内 NLP / CL 学者 进行 学术 交流 的 重要 
平台 尤其 值得一提的是 全国 青年 计算 语言学 研讨会 是 专门 
面向 国内 NLP / CL 研究生 的 学术 会议 从 
组织 到 审稿 都由 该 领域 研究生 担任 非常 有 
特色 也是 NLP / CL 同学们 学术交流 快速 成长 的 
好去处 值得一提的是 2010年 在 北京 召开 的 COLING 以及 2015年 
即将 在 北京 召开 的 ACL 学会 都是/nr 主要 承办者 
这也 一定 程度 上 反映 了 学会 在 国内 NLP 
/ CL 领域 的 重要 地位 此外 计算机 学会 中文 
信息 技术 专委会 组织 的 自然 语言 处理 与 中文 
计算 会议 NLP & CC 也是 最近 崛起 的 重要 
学术 会议 中文信息 学会 主编 了 一份 历史 悠久 的 
中文信息 学报 是 国内 该 领域 的 重要 学术 期刊 
发表 过 很多 篇 重量级 论文 此外 国内 著名 的 
计算机 学报 软件 学报 等 期刊 上 也 经常 有 
NLP / CL 论文 发表 值得 关注 过去 几年 在 
水木 社区 BBS 上 开设 的 AI NLP 版面 曾经 
是 国内 NLP / CL 领域 在线 交流 讨论 的 
重要 平台 这几年 随着 社会 媒体 的 发展 越来越 多 
学者 转战 新浪 微博 有 浓厚 的 交流 氛围 如何 
找到 这些 学者 呢 一个 简单 的 方法 就是 在 
新浪 微博 搜索 的 找人 功能 中 检索 自然语言 处理 
计算 语言学 信息检索 机器学习 等 字样 马上 就能 跟 过去 
只 在 论文 中 看到 名字 的 老师 同学们 近距离 
交流 了 还有 一种 办法 清华大学 梁斌 开发 的 微博 
寻人 系统 http / / xunren . thuir . org 
/ 可以 检索 每个 领域 的 有 影响力 人士 因此 
也 可以 用来 寻找 NLP / CL 领域 的 重要 
学者 值得一提的是 很多 在 国外 任教 的 老师 和 求学 
的 同学 也 活跃 在 新浪 微博 上 例如 王威廉 
http / / weibo . com / u / 1657470871 
李沐/nr http / / weibo . com / mli65 等 
经常 爆料 业内 新闻 值得 关注 还有 国内 NLP / 
CL 的 著名 博客 是 52nlp http / / www 
. 52nlp . cn / 影响力 比较 大 总之 学术研究 
既 需要 苦练内功 也 需要 与 人 交流 所谓 言者无意 
听者有心 也许 其他人 的 一句话 就能 点醒 你 苦思 良久 
的 问题 无疑 博客 微博 等 提供 了 很好 的 
交流 平台 当然 也 注意 不要 沉迷 哦 * * 
3 . 如何 快速 了解 某 个 领域 研究 进展 
* * 最后 简单 说 一下 快速 了解 某 领域 
研究 进展 的 经验 你 会 发现 搜索引擎 是 查阅 
文献 的 重要 工具 尤其 是 谷歌 提供 的 Google 
Scholar 由于 其 庞大 的 索引 量 将 是 我们 
披荆斩棘 的 利器 当 需要 了解 某 个 领域 如果 
能 找到 一篇 该 领域 的 最新 研究 综述 就 
省劲 多了 最 方便 的 方法 还是 在 Google Scholar 
中 搜索 领域 名称 + survey / review / tutorial 
/ 综述 来 查找 也 有 一些 出版社 专门 出版 
各 领域 的 综述 文章 例如 NOW Publisher 出版 的 
Foundations and Trends 系列 Morgan & Claypool Publisher 出版 的 
Synthesis Lectures on Human Language Technologies 系列 等 它们 发表 
了 很多 热门 方向 的 综述 如 文档 摘要 情感 
分析 和 意见 挖掘 学习 排序 语言 模型 等 如果 
方向 太 新 还 没有 相关 综述 一般 还 可以 
查找 该 方向 发表 的 最新 论文 阅读 它们 的 
相关 工作 章节 顺着 列出 的 参考 文献 就 基本 
能够 了解 相关 研究 脉络 了 当然 还有 很多 其他 
办法 例如 去 videolectures . net 上看 著名 学者 在 
各大 学术 会议 或 暑期学校 上 做 的 tutorial 报告 
去 直接 咨询 这个 领域 的 研究 者 等等 附录 
ACL 会议 Annual Meeting of the Association for Computational Linguistics 
是 自然 语言 处理 与 计算 语言学 领域 最高 级别 
的 学术 会议 由 计算 语言学 协会 主办 每年 一届 
主要 涉及 以下 方面 对话 Dialogue 篇章 Discourse 评测 Eval 
信息 抽取 IE 信息检索 IR 语言 生成 LanguageGen 语言 资源 
LanguageRes 机器翻译 MT 多 模态 Multimodal 音韵学 / 形态学 Phon 
/ Morph 自动 问答 QA 语义 Semantics 情感 Sentiment 语音 
Speech 统计 机器学习 Stat ML 文摘 Summarisation 句法 Syntax 自然语言 
处理 及 计算 语言学 常见 缩略语 ACL   = Association 
for Computational Linguistics 计算 语言学 协会 AFNLP   = Asian 
Federation of Natural Language Processing 亚洲 自然语言 处理 联盟 AI 
  = Artificial Intelligence 人工智能 ALPAC   = Automated Language 
Processing Advisory Committee 语言 自动 处理 咨询 委员会 ASR   
= Automatic Speech Recognition 自动 语音 识别 CAT   = 
Computer Assisted / Aided Translation 计算机辅助 翻译 CBC   = 
Clustering by CommitteeCCG   = Combinatory Categorial Grammar 组合 范畴 
语法 CICLing   = International Conference on Intelligent text processing 
and Computational Linguistics 国际 智能 文本处理 与 计算 语言学 大会 
CL   = Computational Linguistics 计算 语言学 COBUILD   = 
Collins Birmingham University International Language Database 柯林斯 伯明翰 大学 国际 
语言 数据库 COLING   = International Conference on Computational Linguistics 
国际 计算 语言学 大会 CL = Computational Linguistics 计算 语言学 
COBUILD = Collins Birmingham University International Language Database 柯林斯 伯明翰 
大学 国际 语言 数据库 COLING = International Conference on Computational 
Linguistics 国际 计算 语言学 大会 CRF = Conditional Random Fields 
条件 随 机场 DRS = Discourse Representation Structure 篇章 表述 
结构 DRT = Discourse Representation Theory 篇章 表述 理论 EACL 
= European chapter of the Association for Computational LinguisticsEBMT = 
Example based machine translation 基于 实例 的 机器 翻译 EM 
= Expectation Maximization 期望 最大化 FAHQMT = Fully Automated High 
Quality Machine Translation 全自动 高质量 机器翻译 FOL = First Order 
Logic 一阶逻辑 HAMT = Human Assisted / Aided Machine Translation 
人工 辅助 机器翻译 HLT = Human Language Technologies 人类 语言 
技术 HMM = Hidden Markov Model 隐 马尔科夫 模型 HPSG 
= Head Driven Phrase Structure Grammar 中心语 驱动 短语 结构 
语法 IE = Information Extraction 信息 抽取 IR = Information 
Retrieval 信息检索 IST = Information Society Technologies 信息 社会 技术 
KR = Knowledge Representation 知识 表示 LFG = Lexical Functional 
Grammar 词汇 功能 语法 LSA = Latent Semantic Analysis 潜在 
语义分析 Linguistics Society of America 美国 语言学 学会 LSI = 
Latent Semantic Indexing 潜在 语义 索引 MAHT = Machine Assised 
/ Aided Human Translation 计算机辅助 人工 翻译 ME = Maximum 
Entropy 最大熵 MI = Mutual Information 互信息 ML = Machine 
Learning 机器学习 MRD = Machine Readable Dictionary 机 读 词典 
MT = Mechanical Translation / Machine Translation 机器翻译 NAACL = 
North American chapter of the Association for Computational LinguisticsNE = 
Named Entity 命名 实体 NEALT = Northern European Association for 
Language TechnologyNER = Named Entity Recognition 命名 实体 识别 NLG 
= Natural Language Generation 自然语言 生成 NLP = Natural Language 
Processing 自然语言 处理 NLU = Natural Language Understanding 自然语言 理解 
NML = National Museum of LanguagePLSA = Probabilistic Latent Semantic 
Analysis 概率 潜在 语义分析 PMI = Pointwise Mutual Information 点 
间 互信息 POS = Part of Speech 词性 RTE = 
Recognising Textual EntailmentSLT = Spoken Language Translation 口语翻译 SVM = 
Support Vector Machine 支持 向量 机 TAG = Tree Adjoining 
Grammar 树 邻接 语法 TINLAP = Theoretical Issues in Natural 
Language ProcessingTLA = Three letter acronym 三 字母 缩略语 TMI 
= Theoretical and Methodological Issues in Machine Translation TREC = 
The Text REtrieval Conference 文本检索 会议 VSM = Vector Space 
Model 向量空间 模型 WSD = Word Sense Disambiguation 词义 消 
歧 搜狗 实验室 数据 资源 http / / www . 
sogou . com / labs / resource / list _ 
pingce . php 自然语言 处理 与 信息检索 共享 平台 http 
/ / www . nlpir . org / action category 
catid 28Chinese Word Vectors 目前 最全 的 中文 预 训练 
词 向量 集合 https / / www . toutiao . 
com / a 6 5 5 5 6 8 3 
8 6 4 9 8 1 7 9 9 4 
2 7 / tt _ from = mobile _ qq 
& utm _ campaign = client _ share & timestamp 
= 1526372845 & app = news _ article & utm 
_ source = mobile _ qq & iid = 32264431851 
& utm _ medium = toutiao _ android 图像 分类 
数据集 CIFAR10 The Canadian Institute For Advanced Research 是 衡量 
机器学习 模型 好坏 的 一个 公共 数据集 主要 目的 是 
将 32x32 的 RGB 图片 分类 成 以下 的 10个 
类型 airplane automobile bird cat deer dog frog horse ship 
and truckhttp / / academictorrents . com / details / 
4 6 3 b a 7 e c 7 f 
3 7 e d 4 1 4 c 1 2 
f b b 7 1 e b f 6 4 
3 1 e a d a 2 d 7 a 
参考 http / / blog . csdn . net / 
u012052268 / article / details / 78035272 其它 语料 http 
/ / mp . weixin . qq . com / 
s _ _ biz = MzIxODM4MjA5MA = = & mid 
= 2247486225 & idx = 1 & sn = 0 
8 0 d f c e d b d c 
9 5 2 2 f 9 1 9 e 6 
7 e 3 2 d 5 5 0 2 5 
0 & chksm = 9 7 e a 2 1 
7 4 a 0 9 d a 8 6 2 
3 b 8 f 9 4 4 6 6 d 
4 9 5 c d 9 8 e 5 4 
2 5 9 8 9 1 a 3 e e 
e 5 8 5 8 6 d 3 e c 
1 f 9 6 3 e 4 e 6 8 
d f 8 0 3 8 7 e 8 8 
& mpshare = 1 & scene = 23 & srcid 
= 0 4 0 8 i 5 w r i 
T k N 6 F U y Z p O 
X o v P o # rd09 NLU 和 NLG 
问题 中 的 深度 学习 9.1 人工智能 概览 9 . 
1.1 人工智能 的 基础 9 . 1.2 人工智能 的 阶段 
9 . 1.3 人工智能 的 种类 9 . 1.4 人工智能 
的 目标 和 应用 9.2 NLU 和 NLG 之间 的 
区别 9 . 2.1 自然语言 理解 9 . 2.2 自然语言 
生成 9.3 深度 学习 概览 9.4 神经 网络 基础 9 
. 4.1 神经元 的 第一 个 计算 模型 9 . 
4.2 感知机 9 . 4.3 理解 人工神经网络 中的 数学 概念 
9.5 实现 神经网络 9 . 5.1 单层 反向 传播 神经网络 
9 . 5.2 练习 9.6 深度 学习 和 深度 神经网络 
9 . 6.1 回顾 深度 学习 9 . 6.2 深度 
神经 网络 的 基本 架构 9 . 6.3 NLP 中的 
深度 学习 9 . 6.4 传统 NLP 和 深度 学习 
NLP 技术 的 区别 9.7 深度 学习 技术 和 NLU9 
. 7.1 机器翻译 MLT EN to FR TensorFlowBrief Overview of 
the ContentsData p r e p r o c e 
s s i n g B u i l d 
m o d e l T r a i n 
i n g P r e d i c t 
i o n 9.8 深度 学习 技术 和 NLG9 . 
8.1 练习 9 . 8.2 菜谱 摘 要和 标题 生成 
9.9 基于 梯度 下降 的 优化 9 . 9.1 基本 
梯度 下降 9 . 9.2 随机 梯度 下降 9 . 
9.3 小批量 梯度 下降 9 . 9.4 动量 9 . 
9.5 Nesterov 加速 梯度 9 . 9.6 Adagrad9 . 9.7 
adadelta9 . 9.8 Adam9 . 10 人工智能 与 人类 智能 
9.11 总结 在 前面 的 章节 中 我们 已经 看到 
了 基于 规则 的 方法 和 各种 机器 学习 技术 
来 解决 NLP 任务 在 本 章中 我们 将 看到 
机器学习 技术 称为 深度 学习 DL 子集 在 过去 的 
四到五 年里 神经 网络 和 深度 学习 技术 在 人工智能 
领域 引起 了 广泛 的 关注 因为 许多 技术 巨头 
使用 这些 尖端 技术 来 解决 现实 生活 中 的 
问题 这些 技术 的 成果 令人 印象 深刻 谷歌 苹果 
亚马逊 OpenAI 等 科技 巨头 花费 大量 时间 和 精力 
为 现实 生活 中 的 问题 创造 创新 的 解决方案 
这些 努力 主要 是 为了 发展 人工 通用 智能 使 
世界 成为 人类 更好 的 地方 我们 首先 要 了解 
整个 人工智能 总的来说 给 你 一个 的 概念 为什么 深度 
学习 现在 正在 高速 发展 我们 将 在 本章 中 
讨论 以下 主题 NLU/w 和/c NLG/w 之间/f 的/uj 区别/n 神经/n 
网络/n 基础/n 使用/v 各种/r 深度/ns 学习/v 技术/n 构建/v NLP/w 和/c 
NLG/w 应用/v 程序/n 在/p 了解/v 了/ul DL/w 的/uj 基础/n 知识/v 
之后/f 我们 将 接触到 在 深度 学习 领域 中 发生 
的 一些 最新 的 创新 那么 让 我们 开始 吧 
9.1 人工智能 概览 在 本节 中 我们 将 看到 人工智能 
的 各个 方面 以及 深度 学习 与 人工智能 的 关系 
我们 将 看到 人工智能 的 组成部分 人工智能 的 不同 阶段 
和 不同 类型 的 人工智能 在 本节 的 最后 我们 
将 讨论 为什么 深度 学习 是 实现 人工智能 最有 希望 
的 技术 之一 9 . 1.1 人工智能 的 基础 当 
我们 谈论 人工智能 时 我们 想到 的 是 智能 机器 
这是 人工智能 的 基本 概念 人工智能 是 一个 科学 领域 
它 不断 朝着 使 机器 具有 人类 水平 智能 的 
方向 发展 人工智能 背后 的 基本 思想 是 在 机器 
中 启用 智能 以便 它们 也 可以 执行 一些 仅 
由 人类 执行 的 任务 我们 正在 尝试 使用 一些 
很酷 的 算法 技术 来 实现 机器 中 的 人类 
级 智能 在 这个 过程 中 机器 获取 的 任何 
类型 的 智能 都是 人工 生成 的 各种 用于 为 
机器 生成 人工智能 的 算法 技术 主要 是 机器 学习 
技术 的 一部分 在 进入 核心 机器学习 和 深度 学习 
部分 之前 我们 将 了解 与 人工智能 相关 的 其他 
事实 人工智能 受 许多 分支 的 影响 在 . 1 
中 我们 将 把 那些 严重影响 人工智能 的 分支 视为 
单个 分支 首先 我们 将 看到 人工智能 的 关键 组成部分 
这些 组成部分 对于 我们 理解 世界 的 发展 方向 是 
非常 有用 的 据我所知 有 两个 组件 如 . 2 
所示 自动化 自动化 是 人工智能 的 一个 著名 组成部分 全/a 
世界/n 的/uj 人们/n 都在/nr 高度/n 自动化/l 方面/n 工作/vn 我们 在 
机器 执行 的 自动化 任务 方面 取得 了 巨大 的 
成功 我们 将 查看 一些 足够 直观 的 例子 以便 
您 理解 人工智能 中的 自动化 概念 在 汽车 行业 我们 
使用 自动 机器人 制造 汽车 这些 机器人 遵循 一套 指令 
执行 特定 的 任务 在 这里 这些 机器 人 不是 
智能 机器人 它们 可以 与 人类 互动 提问 或 回应 
人类 但 这些 机器 人 只是 遵循 一套 指令 来 
实现 高速 制造 的 高精度 和 高效率 所以 这些 机器 
人 就是 人工智能 领域 自动化 的 例子 另 一个 例子 
是 DevOps 领域 现在 DevOps 正在 使用 机器学习 来 自动化 
许多 人类 密集型 的 过程 例如 为了 维护 内部 服务器 
DevOps 团队 在 分析 各种 服务器 日志 后 获得 一 
系列 建议 在 获得 建议 后 另一个 机器学习 模型 优先 
处理 警报 和 建议 这种 应用程序 确实 为 DevOps 团队 
节省 了 时间 来 按时 交付 大量 工作 这些 应用 
程序 确实 帮助 我们 理解 自动化 是 人工智能 的 一个 
非常 重要 的 组成部分 智力 当 我们 说 智力 作为 
人类 我们 的 期望 真的 很高 我们 的 目标 是 
让 机器 了解 我们 的 行为 和 情绪 我们 还 
希望 机器 根据 人类 的 行为 做出 智能 的 反应 
机器 产生 的 所有 反应 都 应该 是 模仿 人类 
智能 的 我们 希望 从 20 世纪 90 年代 中期 
开始 实现 这 一 目标 在 全球 范围 内 许多 
研究 人员 科学家 团体 和 社区 正在 进行 大量 的 
酷 的 研究 以使 机器 像 人类 一样 智能化 我们 
希望 在 获得 智能 之后 机器 能够 以 更好 的 
精度 为 人类 完成 大部分 任务 这 是 一个 单一 
的 广泛 的 期望 在 过去 的 45 年中 我们 
已经 开始 成功 地 实现 这一 广泛 的 目标 因此 
经过 多年 的 努力 谷歌 最近 宣布 谷歌 助手 可以 
从 人类 身上 听到 自然语言 并能 像 人类 一样 准确 
地 解释 语音信号 另 一个 例子 是 Facebook 的 研究 
小组 进行 了 一项 非常 强大 的 研究 以 建立 
一个 善于 对 问题 和 答案 进行 推理 的 系统 
特斯拉 和 谷歌 的 自动 驾驶 汽车 是 一个 复杂 
的 人工 智能系统 但 非常 有用 和 智能 自动 驾驶 
汽车 和 聊天 机器人 是 窄 人工智能 的 一部分 你 
也 可以 在 网上 找到 很多 其他 的 例子 这些 
例子 时不时 会 出现 有 些子 组件 可以 作为 信息 
的 一部分 参见 . 3 智能 是 前面 图中 描述 
的 所有 组件 的 组合 所有 这些 成分 推理 学习 
从 经验 中 学习 解决问题 感知 和 语言 智能 都是/nr 
人类 的 天性 而 不是 机器 的 天性 所以 我们 
需要 能够 为 机器 提供 智能 的 技术 在 学习 
本章 后面 将要 使用 的 技术 名称 之前 让 我们 
先 了解 人工智能 的 各个 阶段 9 . 1.2 人工智能 
的 阶段 人工智能 系统 有 三个 主要 阶段 我们 将 
详细 介绍 以下 几个 阶段 机器学习 机器 智能 机器 意识 
在 了解 人工智能 各个 阶段 的 详细 信息 之前 请参阅 
. 4 我们 将 从下到上 因此 我们 将 首先 了解 
机器学习 阶段 然后 了解 机器 智能 最后 了解 机器 意识 
机器学习 在 前面 的 章节 中 您 已经 学习 了 
很多 关于 机器 学习 的 知识 但是 我 想在 本 
章中 给 您 一个 人工智能 的 视角 ML 技术 是 
一组 解释 如何 生成 或 达到 定义 的 输出 的 
算法 这种 算法 被 试图 从 经验 中 学习 的 
智能 系统 所 使用 使用 ML 算法 的 系统 热衷于 
从 历史数据 或 实时 数据 中 学习 因此 在 人工智能 
的 这个 阶段 我们 关注 学习 模式 或 特定 的 
算法 使用 我们 提供 给 ML 系统 的 特性 从 
数据 中 得到 的 结构 为了 说明 这 一点 让 
我们 举 个 例子 假设 您 想要 构建 一个 情绪 
分析 应用程序 我们 可以 使 用 历史 标记 数据 手工 
制作 的 特性 和 朴素 的 Bayes ML 算法 因此 
我们 可以 拥有 一个 从其 学习 示例 中 学习 到 
的 智能系统 如何 为 看 不见 的 新 数据 实例 
提供 情感 标签 机器 智能 机器 智能 又是 一套 算法 
但/c 大多数/m 算法/n 都/d 严重/a 受/v 人脑/n 学习/v 和/c 思考/v 
方式/n 的/uj 影响/vn 利用 神经科学 生物学 和 数学 人工智能 研究 
人员 提出 了 一套 高级 算法 帮助 机器 从 数据 
中 学习 而不 提供 手工 制作 的 特征 在此 阶段 
算法 使用 未 标记 或 标记 的 数据 在 这里 
您 只需 定义 最终目标 高级 算法 就 可以 找到 实现 
预期 结果 的 方法 如果 您 将 我们 在 这个 
阶段 使用 的 算法 与 传统 的 ML 算法 进行 
比较 那么 主要 的 区别 在于 在 机器 智能 阶段 
我们 不会 将 手工 制作 的 特征 作为 任何 算法 
的 输入 当 这些 算法 受到 人脑 的 启发 时 
算法 本身 就 学习 特征 和 模式 并 生成 输出 
目前 人工智能 的 世界 正处于 这个 阶段 全/a 世界/n 的/uj 
人们/n 都/d 使用/v 这些/r 先进/a 的/uj 算法/n 似乎 很 有 
希望 为 机器 实现 类似 人类 的 智能 利用 人工神经网络 
和 深度 学习 技术 实现 机器 智能 机器 意识 机器 
意识 是 人工智能 中 讨论 最多 的 主题 之一 因为 
我们 的 最终 目标 是 达到 这里 我们 希望 机器学习 
人类 的 学习 方式 作为 人类 我们 不 需要 太多 
的 数据 我们 不 需要 太多 时间 来 理解 抽象概念 
我们 从 少量 数据 或 没有 数据 中 学习 大多数 
时候 我们 从 经验 中 学习 如果/c 我们/r 想/v 建立/v 
一个/m 和/c 人类/n 一样/r 有/v 意识/n 的/uj 系统/n 那么 我们 
应该 知道 如何 为 机器 产生 意识 然而 我们 是否 
完全 知道 我们 的 大脑 是 如何 工作 和 反应 
的 以便 把 这些 知识 转移到 机器 上 使 它们 
像 我们 一样 有意识 不幸 的 是 现在 我们 还 
没有 意识到 这 一点 我们 期望 在 这一 阶段 机器 
在 没有 数据 或 数据 量 非常 小 的 情况 
下 学习 并 利用 自己 的 经验 来 实现 定义 
的 输出 9 . 1.3 人工智能 的 种类 人工智能 有三种 
类型 如下 所示 弱 人工智能 通用 人工智能 人工 超级 智能 
弱 人工智能 弱 人工智能 ANI 是 一种 人工智能 它 涵盖 
了 一些 基本 任务 如 基于 模板 的 聊天 机器人 
基本 的 个人 助理 应用程序 如 苹果 公司 的 Siri 
初始 版本 这种 智能 主要 集中 在 应用 程序 的 
基本 原型 设计 上 这种 类型 的 智能 是 任何 
应用 程序 的 起点 然后 您 可以 改进 基本 原型 
您 可以 通过 添加 人工 通用 智能 来 添加 下一层 
智能 但 前提 是 最终 用户 确实 需要 这种 功能 
我们 在 第 7 章 NLP 的 基于 规则 的 
系统 中 也 看到 了 这种 基本 聊天 机器人 通用 
人工智能 通用 人工智能 AGI 是 一种 人工智能 用于 构建 能够 
执行人 级任务 的 系统 我 所说 的 人工 级别 的 
任务 是 什么 意思 建造 自动 驾驶 汽车 等 任务 
谷歌 自动 驾驶 汽车 和 特斯拉 自动 驾驶仪 是 最 
著名 的 例子 类人 机器人 也 尝试 使用 这种 人工智能 
NLP 级 的 例子 是 复杂 的 聊天 机器人 它们 
忽略 拼写错误 和 语法错误 并 理解 您 的 查询 或 
问题 深度 学习 技术 对于 人类 理解 自然语言 似乎 非常 
有 希望 我们 现在 正 处在 一个 世界 各地 的 
人们 和 社区 使用 基本 概念 的 阶段 通过 相互 
参照 对方 的 研究 成果 尝试 构建 具有 敏捷性 的 
系统 人工 超级 智能 实现 人工 超级 智能 ASI 的 
方法 对 我们 来说 有点 困难 因为 在 这种 人工智能 
中 我们 期望 机器 比 人类 更聪明 以便 学习 特定 
的 任务 并且 能够 像 人类 在 生活 中 一样 
执行 多个 任务 这种 超级 智能 现在 是 我们 的 
梦想 但 我们 正 试图 在 这样 一个 机器 和 
系统 始终 是 人类 技能 的 补充 不会 对 人类 
造成 威胁 9 . 1.4 人工智能/n 的/uj 目标/n 和/c 应用/v 
这/r 是/v 我们/r 需要/v 了解/v 各个/r 领域/n 人工智能/n 的/uj 目标/n 
和/c 应用/v 程序/n 的/uj 时间/n 和/c 部分/n 这些 目标 和 
应用 程序 只是 为了 让 您 了解 启用 人工智能 的 
应用 程序 的 当前 状态 但是 如果 您 可以 在 
任何 领域 想到 一些 疯狂 但 有用 的 应用 程序 
那么 您 应该 尝试 将 其 包括 在 这个 列表 
中 您 应该 尝试 在 该 应用 程序 中 实现 
各种 类型 和 阶段 的 人工智能 现在 让 我们 看看 
我们 想要 集成 人工智能 各个 阶段 并 使 这些 应用 
程序 启用 人工智能 的 领域 推理 机器学习 自然语言 处理 机器人学 
实施 一般 情报 计算机 视觉 自动化 学习 和 调度 语音 
分析 您 可以 参考 . 5 它 显示 了 许多 
不同 的 领域 和 相关 的 应用 程序 支持 人工智能 
的 应用 程序 在 这里 我 将向 您 简要 介绍 
启用 人工智能 的 应用 程序 一些 应用 程序 也与 NLP 
域 相关 对 任何 系统 进行 推理 都是/nr 非常 令人 
兴奋 的 事情 在这方面 我们 可以 建立 一个 Q / 
A 系统 利用 推理 得出 问题 的 答案 如果 我们 
能够 对 基于 人工智能 的 系统 进行 推理 那么 这些 
系统 将 非常 擅长 决策 并将 改进 现有 的 决策 
系统 在 机器 学习 中 我们 需要 一个 基于 ML 
的 应用 程序 的 完美 架构 它 可以 由 机器 
自己 决定 据我所知 这 是 一个 支持 人工智能 的 ML 
应用程序 当 我们 谈论 人工智能 的 NLP 应用程序 时 我们 
真的 需要 能够 理解 人类 自然 语言 的 上下文 并 
作出 反应 表现 得 更像 人类 的 NLP 系统 类人 
机器人 是 描述 人工智能 系统 的 最佳 应用 机器 人 
应该 获得 感知 这 是 一个 长期 的 人工智能 目标 
我 认为 当 我们 谈论 一般 智能 时 系统 的 
反应 应该 更 像 人类 尤其 是 机器 反应 应该 
与 人类 的 真实 行为 相 匹配 在 分析 了 
某些 情况 后 机器 的 反应 应该 比 人类 相同 
或 更好 如今 计算机 视觉 有 许多 应用 为 我们 
提供 了 可靠 的 证据 证明 人工智能 将 很快 在 
这一 领域 实现 这些 应用 包括 物体 识别 图像识别 使用 
图像 识别 技术 检测 皮肤癌 从 机器 生成 面部 图像 
为 图像 生成 文本 反之亦然 等 所有 这些 应用 程序 
给 出了 人工智能 驱动 计算机 视觉 的 具体 证明 自动 
学习 和 日程 安排 是 一种 为 您 个人 提供 
帮助 并 管理 日程 安排 的 构建 系统 关于 人工智能 
部分 我们/r 真的/d 希望/v 系统/n 的/uj 每个/r 用户/n 都能/nr 获得/v 
个性化/n 的/uj 体验/n 因此 自动化 学习 一 个人 的 个人 
选择 对于 人工智能 驱动 的 调度 非常重要 为了 实现 这 
一 目标 自动化 学习 系统 还 应该 学习 如何 为 
特定 用户 选择 最 适合 的 模型 语言 分析 是 
nl 的 另一种 形式 但 不幸 的 是 我们 在 
本书 中 没有 讨论 这个 概念 在 这里 我们 讨论 
的 是 一个 语音 识别 系统 的 潜在 人工智能 启用 
领域 通过 使用 这个 语音识别 区域 启用 人工智能 我们 可以 
了解 生成 的 人类 环境 和 思维过程 一个人 在 的 
社会学 心理学 和 哲学 的 影响 下 我们 也 可以 
预测 他们 的 性格 在 看到 所有 这些 迷人 的 
应用 程序 之后 我们 想到 了 三个 真正 有趣 的 
问题 什么 是 导致 我们 生产 人工智能 驱动 系统 的 
原因 为 什么时间 如此 适合 我们 构建 人工智能 驱动 系统 
以及 我们 如何 构建 一个 人工 智能系统 自 20 世纪 
90 年代 中期 以来 我们 一直 在 尝试 将 智能 
引入 机器 在 这一 阶段 研究 人员 和 科学家 给出 
了 许多 很酷 的 概念 例如 人工 神经元 也 被 
称为 McCulloch Pitts 模型 MCP 受 人脑 的 启发 这个 
概念 的 目的 是 理解 人脑 的 生物 工作 过程 
并从 数学 和 物理 的 角度 来 表示 这个 过程 
因此 对 机器 实现 人工智能 有 一定 的 帮助 他们 
成功 地 给出 了 单个 神经元 如何 工作 的 数学 
表示 但是 这个 模型 有 一个 结果 不 适合 用于 
训练 目的 因此 研究 人员 Frank Rosenblatt 在 1958年 的 
论文 中 提出 了 感知器 引入 了 动态 权 重和 
阈值 概念 在此之后 许多 研究者 在 早期 概念 的 基础 
上 发展 了 诸如 反向 传播 和 多层 神经 网络 
等 概念 研究 团体 希望 在 实际 应用 中 实现 
已 开发 的 概念 第一 位 研究员 Geoffrey Hinton 演示 
了 使用 广义 反向 传播 算法 训练 多层 神经网络 从那时起 
研究 人员 和 社区 开始 使用 这种 通用 模型 但在 
20 世纪末 数据量 比 现在 少 计算 设备 既 慢 
又 昂贵 所以 我们 没有 得到 预期 的 结果 然而 
随着 当时 取得 的 成果 研究 人员 相信 这些 概念 
将被 用来 实现 人工智能 驱动 的 世界 现在/t 我们/r 有了/nr 
大量/n 的/uj 数据/n 和/c 计算/v 设备/vn 这些 设备 速度快 价格便宜 
并且 能够 处理 大量 的 数据 当 我们 在 当前 
时代 应用 人工神经网络 的 旧 概念 开发 通用 机器翻译 系统 
语音 识别 系统 图像 识别 系统 等 应用 时 我们 
得到 了 非常 有 前途 的 结果 让 我们 举 
个 例子 谷歌 正在 利用 人工神经网络 开发 一个 通用 的 
机器 翻译 系统 该 系统 将 翻译 多种语言 这/r 是因为/c 
我们/r 有/v 大量/n 可用/v 的/uj 数据集/i 和/c 快速/d 的/uj 计算/v 
能力/n 可以 帮助 我们 使用 ANN 处理 数据集 我们 使用 
的 神经 网络 不 是 一层 或 两层 而是 多层 
的 取得 的 成果 令人 印象 深刻 以至于 每 一个 
大型 科技 巨头 都在/nr 使用 深度 学习 模型 来 开发 
一个 人工 智能系统 据我所知 数据 计算 能力 和 可靠 的 
基础 概念 是 开发 人工智能 驱动 系统 的 关键 组件 
您 可以 参考 . 6 了解 神经 网络 的 简要 
历史 . 7 将向 您 介绍 神经 网络 的 长期 
历史 现在 让 我们 进入 下 一个 问题 我们 如何 
启用 人工智能 答案 是 深度 学习 这是 使 人工智能 适用于 
非 人工智能 系统 的 最 常用 技术 之一 在 少数 
情况 下 深度 学习 不 用于 启用 人工智能 但在 NLP 
领域 深度 学习 主要 用于 启用 人工智能 为了 发展 一般 
智力 我们 可以 利用 深度 学习 我们/r 从/p 这项/r 技术/n 
中/f 得到/v 了/ul 非常/d 有/v 希望/v 的/uj 结果/n 在 嘈杂 
的 环境 中 诸如 生成 人脸 的 机器 能更/nr 准确 
地 理解 人类 的 语言 自动 驾驶 汽车 问答 系统 
的 推理 等 实验 只是 其中 的 一小部分 深度/ns 学习/v 
技术/n 利用/n 大量/n 的/uj 数据/n 和高/nr 计算/v 能力/n 来/v 训练/vn 
系统对/l 给定/v 数据/n 的/uj 学习/v 当 我们 在 大量 数据 
上 应用 正确 的 深度 学习 模型 时 我们 会 
得到 一个 神奇 的 令人 印象 深刻 的 有 希望 
的 结果 这 就是 为什么 深度 学习 在 当今 引起 
了 很多 轰动 所以 我 想 现在 你 知道 为什么 
深度 学习 是 人工智能 世界 的 流行 词 了 9.2 
NLU 和 NLG 之间 的 区别 在 第 3 章 
我们 已经 看到 了 NLU 和 NLG 的 定义 细节 
以及 句子 理解 结构 中 的 差异 在 本节 中 
我们 将 从 启用 人工智能 的 应用 程序 的 角度 
比较 NLP 的 这两个 子 区域 9 . 2.1 自然语言 
理解 早些时候 我们 已经 看到 NLU 更多 的 是 处理 
对 语言 结构 的 理解 无论是 单词 短语 还是 句子 
NLU 更多 的 是 在 已经 生成 的 NL 上 
应用 各种 ML 技术 在 NLU 中 我们 关注 语法 
和 语义 我们 还 试图 解决 与 语法 和 语义 
相关 的 各种 类型 的 歧义 我们 已经 看到 了 
词汇 歧义 句法 歧义 语义 歧义 和 语用 歧义 现在 
让 我们 看看 在 哪里 我们 可以 使用 人工智能 帮助 
机器 更 准确 更 有效 地 理解 语言 结构 和 
含义 人工智能 和 人工智能 技术 在 解决 本地 语言 的 
这些 方面 并不 落后 举个 例子 深入 学习 使 我们 
在 机器 翻译 方面 取得 了 令人 印象 深刻 的 
成果 现在 当 我们 讨论 解决 句法 歧义 和 语义 
歧义 时 我们 可以 使 用 深度 学习 假设 您 
有一个 命名 实体 识别 工具 它 将 使用 深度 学习 
和 word2vec 那么 我们 就 可以 解决 语 法上 的 
歧义 这 只是 一个 应用程序 但 您 也 可以 改进 
解析器 结果 和 词性 标注 器 现在 我们 来 谈谈 
语用 歧义 我们 真正 需要 的 是 AGI 和 ASI 
这种 歧义 发生 在 你 试图 理解 一个 句子 与 
其他 先前 写 的 或 说 的 句子 的 长距离 
上下文 时 它 还 取决于 说话者 的 说话 或 写作 
意图 让 我们 来看 一个 语用 歧义 的 例子 你 
和你的/nr 朋友 正 在 谈话 你 的 朋友 很久 以前 
就 告诉 你 她 加入 了 一个 非政府 组织 会为 
贫困 学生 做 一些 社会 活动 现在 你 问她 社交活动 
怎么样 了 在 这种 情况 下 你 和你的/nr 朋友 知道 
你 在 谈论 什么 社会活动 这 是 因为 作为 人类 
我们 的 大脑 存储 信息 并 知道 何时 获取 这些 
信息 如何 解释 这些 信息 以及/c 获取/v 的/uj 信息/n 与/p 
您/zg 当前/t 与/p 朋友/n 的/uj 对话/n 有/v 何/r 关联/ns 你/r 
和你的/nr 朋友/n 都能/nr 理解/v 对方/n 问题/n 和/c 答案/n 的/uj 上下文/l 
和/c 相关性/l 但是 机器 没有 这种 理解 上下文 和 说话者 
意图 的 能力 这 就是 我们 对 智能 机器 的 
期望 我们 希望 机器 也 能 理解 这种 复杂 的 
情况 支持 这种 解决 语用 歧义 的 能力 包含 在 
MSI 中 这在 将来 肯定 是 可能 的 但现在 我们 
正 处于 机器 试图 采用 AGI 并 使用 统计 技术 
来 理解 语义 的 阶段 9 . 2.2 自然语言 生成 
NLG 是 一个 我们 试图 教 机器 如何 以 合理 
的 方式 生成 NL 的 领域 这 本身 就是 一项 
具有 挑战性 的 人工智能 任务 深入 学习 真 的 帮助 
我们 完成 了 这类 具有 挑战性 的 任务 让 我 
举个 例子 如果 你 正在 使用 谷歌 的 新 收件箱 
那么 你 可能 会 注意 到 当 你 回复 任何 
邮件 时 你 会 得到 三个 最 相关 的 回复 
以 句子 的 形式 回复 给 你 的 邮件 谷歌 
使用 了 数百万 封电子邮件 并 制作 了 一个 NLG 模型 
该 模型 通过 深度 学习 来 生成 或 预测 任何 
给定 邮件 的 最 相关 回复 您 可以 参考 . 
8 除了 这个 应用 程序 之外 还有 另一个 应用程序 看到 
图像 后 机器 将 提供 特定 图像 的 标题 这也 
是 一个 使用 深度 学习 的 NLG 应用程序 生成 语言 
的 任务 比 生成 nl 要 简单 也 就是说 连贯性 
这 就是 我们 需要 AGI 的 地方 我们 已经 讨论 
了 很多 关于 深度 学习 这个词 的 内容 但 它 
实际上 是 如何 工作 的 为什么 它 如此 有 前途 
我们 将 在 本章 的 下 一节 中 看到 这 
一点 我们 将 解释 NLU 和 NLG 应用 程序 的 
编码 部分 我们 还将 从头 开始 开发 NLU 和 NLG 
应用程序 在此之前 你 必须 了解 ANN 和 深度 学习 的 
概念 我 将在 接下来 的 部分 中 加入 数学 并 
尽我所能 保持 简单 9.3 深度 学习 概览 机器学习 是 人工智能 
的 一个 分支 深度 学习 是 ML 的 一个 分支 
参见 . 9 深度 学习 使用 的 人工 神经 网络 
不仅 是 一个 或 两个 层次 而是 许 多层次 的 
深度 称为 深度 神经网络 DNN 当 我们 使用 DNN 通过 
预测 同 一个 问题 的 可能 结果 来 解决 给定 
的 问题 时 它 被 称为 深度 学习 深度 学习 
可以 使用 标记 的 数据 或 未 标记 的 数据 
因此 我们 可以 说 深度 学习 可以 用于 有 监督 
的 技术 也 可以 用于 无 监督 的 技术 使用 
深度 学习 的 主要 思想 是 使用 DNN 和 大量 
的 数据 我们 希望 机器 概括 特定 的 任务 并 
为 我们 提供 一个 我们 认为 只有 人类 才能 生成 
的 结果 深度 学习 包括 一 系列 的 技术 和 
算法 可以 帮助 我们 解决 NLP 中 的 各种 问题 
如 机器翻译 问答 系统 总结 等 除了 NLP 您 还 
可以 找到 其他 应用领域 如 图像识别 语音识别 对象 识别 手写 
数字 识别 人脸 检测 和 人工 人脸 生成 深度 学习 
对 我们 来说 似乎是 有 希望 的 以便 建立 AGI 
和 ASI 您 可以 在 . 10 中 看到 一些 
使用 了 深度 学习 的 应用 程序 9.4 神经 网络 
基础 神经 网络 的 概念 是 ML 中最 古老 的 
技术 之一 神经网络 源于 人脑 在 这 一 部分 中 
我们 将 看到 人脑 的 组成部分 然后 推 导出 神经网络 
为了 理解 神经网络 我们 首先 需要 了解 人脑 的 基本 
工作 流程 您 可以 参考 . 11 人脑 由 几 
千亿个 被称为 神经元 的 神经细胞 组成 每个 神经元 执行 以下 
三 个 任务 接收 信号 它 从 树突 接收 一组 
信号 决定 将 信号传 递给 细胞体 将 这些 信号 整合 
在 一起 决定 是否 应将/nr 信息 传递 给 细胞体 发送 
信号 如果 一些 信号 通过 某个 阈值 它 会 通过 
轴突 将 这些 称为 动作电位 的 信号 发送 给 下 
一组 神经元 您 可以 参考 . 12 它 演示 了 
用于 在 生物 神经 网络 中 执行 这 三 项 
工作 的 组件 这 是 我们 大脑 如何 学习 和 
处理 某些 决定 的 一个 非常 简短 的 概述 现在 
的 问题 是 我们 能 建立 一个 使用 像 硅 
或 其他 金属 这样 的 非生物 基底 的 人工神经网络 吗 
我们 可以 构建 它 然后 通过 提供 大量 的 计算机 
电源 和 数据 我们 可以 比 人类 更快 地 解决问题 
人工神经网络 是 一种 生物 启发 的 算法 学习 识别 数据 
集中 的 模式 9 . 4.1 神经元 的 第一 个 
计算 模型 1943年 年中 研究 人员 McCulloch Pitts 发明 了 
第一 个 神经元 计算 模型 他们 的 模型 相当 简单 
该 模型 有 一个 接收 二进制 输入 的 神经元 对其 
求和 如果 总 和 超过 某个 阈值 则 输出 为 
1 如果 不是 则 输出 为零 您 可以 在 . 
13 中 看到 图示 它 看起来 很 简单 但是 就像 
人工智能 早期 发明 的 那样 这种 模型 的 发明 是 
一件 非常 大 的 事情 9 . 4.2 感知机 在 
发明 了 第一 个 神经元 计算 模型 的 几年 后 
心理学家 Frank Rosenblatt 发现 McCulloch Pitts 模型 没有 从 输入 
数据 中 学习 的 机制 所以 他 发明 了 神经 
网络 建立 在 第一 个 神经元 计算 模型 的 基础 
上 Frank Rosenblatt 称 这个 模型 为 感知器 它 也 
被称为 单层 前馈 神经网络 我们 称 这个 模型 为 前馈 
神经网络 因为 在 这个 神经 网络 中 数据 只 朝 
一个 方向 流动 正向 现在 让 我们 来 理解 感知器 
的 工作 它 包含 了 在 给定 输入 上 拥有 
权重 的 思想 如果 您 提供 一些 输入输出 示例 的 
训练 集 它 应该 根据 给定 输入 示例 的 输出 
通过 不断 增加 和 减少 每个 训练 示例 的 权重 
来 从中 学习 函数 这些 权重 值 在 数学 上 
应用 于 输入 以便 在 每次 迭代 之后 输出 预测 
变得 更 准确 整个 过程 称为 训练 参考 . 14 
了解 Rosenblatt 的 感知器 原理图 9 . 4.3 理解 人工神经网络 
中的 数学 概念 这 一节 非常重要 因为 ML ANN 和 
DL 使用 了 一系列 数学 概念 我们 将 看到 其中 
一些 最 重要 的 概念 这些 概念 将 真正 帮助 
您 优化 ML ANN 和 DL 模型 我们 还 将 
看到 不同 类型 的 激活 函数 和 一些 关于 您 
应该 选择 哪个 激活 函数 的 提示 我们 将 看到 
以下 数学 概念 梯度 下降 激活 函数 损失 函数 梯度 
下降 梯度 下降 是 一种 非常 重要 的 优化 技术 
已被 几乎 所有 的 神经 网络 所 采用 为了 解释 
这些 技术 我 想 举个 例子 我/r 有/v 一个/m 学生/n 
成绩/n 和/c 学习/v 时间/n 的/uj 数据/n 集/q 我们 想 通过 
学生 的 学习 时间 来 预测 他 的 考试 成绩 
您 会 说 这 看起来 像 一个 ML 线性 回归 
示例 你 说得对 我们 用 线性 回归 来做 预测 为什么 
是 线性 回归 与 梯度 下降 有 什么 关系 让 
我 回答 这个 问题 然后 我们 将 看到 代码 和 
一些 很酷 的 可视化 效果 线性 回归 是 使用 统计 
方法 的 ML 技术 它 允许 我们 研究 两个 连续 
定量 变量 之间 的 关系 在 这里 这些 变量 是 
学生 的 分数 和 学习 时间 通常在 线性 回 归中 
我们 试图 得到 一条 最 适合 我们 数据集 的 线 
这 意味着 无论 我们 做 什么 计算 都 只是 为了 
得到 一条 最 适合 给定 数据集 的 线 得到 这条 
最佳 拟 合线 是 线性 回归 的 目标 我们 来 
谈谈 线性 回归 与 梯度 下降 的 关系 梯度 下降 
是 我们 用来 优化 线性 回归 精度 和 最小化 损失 
或 误差函数 的 最 常用 的 优化 技术 梯度 下降 
是 使 误差函数 最小化 预测 精度 最大化 的 技术 其 
数学 定义 是 一 阶 迭代 优化 算法 该 算法 
利用 梯度 下 降法 求函数 的 局部 极小值 每一步 都与/nr 
当前 点 的 函数 梯度 的 负值 成正比 您 可以 
使用 这个 实际 例子 来 考虑 梯度 下降 假设 你 
在 山顶 上 现在 你 想到 达 一个 有 美丽 
湖泊 的 底部 所以 你 需要 开始 下降 它 现在 
你 不 知道 该往 哪个 方向 走 在 这种 情况 
下 你 观察 你 附近 的 土地 并 试图 找到 
土地 倾向于 下降 的 方式 这会 让 你 知道 你 
应该 朝 什么 方向 走 如果 你 沿着 下降 的 
方向 迈出 第一步 并且 每次 都 遵循 同样 的 逻辑 
那么 你 很 可能 会 到达 湖边 这 正是 我们 
使用 梯度 下降 的 数学 公式 所做 的 在 ML 
和 DL 中 我们 从 优化 的 角度 考虑 所有 
问题 因此 梯度 下降 是 一种 用于 随 时间 最小化 
损失 函数 的 技术 另 一个 例子 是 你 有 
一个 深 碗 你 把 一个 小球 从它的/nr 一端 放进去 
你 可以 观察到 过 了 一段 时间 后 球会 减速 
并 试图 到达 碗 的 底部 参见 . 15 这个 
图 显示 了 使用 梯度 下降 获得最佳 拟 合线 的 
过程 或 步骤 它 只是 可视化 让 您 全面 了解 
我们 将在 代码 中 做什么 顺便 说 一下 损失 函数 
误差函数 和 成本 函数 是 彼此 的 同义词 梯度 下降 
也 称为 最 陡 下降 首先 让 我们 了解 数据集 
它 是 学生 考试 成绩 和 学习 时间 的 数据集 
我们 知道 在 这 两个 属性 之间 应该 有 一种 
关系 你 学习 的 数量 越少 学生 的 分数 越差 
你 学习 的 越多 分数 就 越好 我们 将 用 
线性 回归 证明 这 一 关系 x 值 表示 数据集 
的 第一 列 即 学生 学习 的 小时 数 y 
值 表示 第二列 即 考试分数 参见 . 16 我们 调用 
了 一个 函数 用来 计算误差 和 实际 的 梯度 下降 
from numpy import * # y = mx + b 
# m is slope b is y intercept # here 
we are calculating the sum of squared error by using 
the equation which we have seen in the book . 
def compute _ error _ for _ line _ given 
_ points b m points totalError = 0 for i 
in range 0 len points x = points i 0 
y = points i 1 totalError + = y m 
* x + b * * 2 return totalError / 
float len points def step _ gradient b _ current 
m _ current points learningRate b _ gradient = 0 
m _ gradient = 0 N = float len points 
for i in range 0 len points x = points 
i 0 y = points i 1 # Here we 
are coding up out partial derivatives equations and # generate 
the updated value for m and b to get the 
local minima b _ gradient + = 2 / N 
* y m _ current * x + b _ 
current m _ gradient + = 2 / N * 
x * y m _ current * x + b 
_ current # we are multiplying the b _ gradient 
and m _ gradient with learningrate # so it is 
important to choose ideal learning rate if we make it 
to high then our model learn nothing # if we 
make it to small then our training is to slow 
and there are the chances of over fitting # so 
learning rate is important hyper parameter . new _ b 
= b _ current learningRate * b _ gradient new 
_ m = m _ current learningRate * m _ 
gradient return new _ b new _ m def gradient 
_ descent _ runner points starting _ b starting _ 
m learning _ rate num _ iterations b = starting 
_ b m = starting _ m for i in 
range num _ iterations # we are using step _ 
gradient function to calculate the actual partial derivatives for error 
function b m = step _ gradient b m array 
points learning _ rate return b m 让 我们 读取 
数据集 和 执行 运算 # Step 1 Read data # 
genfromtext is used to read out data from data . 
csv file . points = genfromtxt . / g r 
a d i e n t d e s c 
e n t e x a m p l e 
/ data . csv delimiter = # Step2 Define certain 
hyperparameters # how fast our model will converge means how 
fast we will get the line of best fit . 
# Converge means how fast our ML model get the 
optimal line of best fit . learning _ rate = 
0.0001 # Here we need to draw the line which 
is best fit for our data . # so we 
are using y = mx + b x and y 
are points m is slop b is the y intercept 
# for initial y intercept guess initial _ b = 
0 # initial slope guess initial _ m = 0 
# How much do you want to train the model 
# Here data set is small so we iterate this 
model for 1000 times . num _ iterations = 1000 
# Step 3 print the values of b m and 
all function which calculate gradient descent and errors # Here 
we are printing the initial values of b m and 
error . # As well as there is the function 
compute _ error _ for _ line _ given _ 
points # which compute the errors for given point print 
Starting gradient descent at b = { 0 } m 
= { 1 } error = { 2 } . 
format initial _ b initial _ m compute _ error 
_ for _ line _ given _ points initial _ 
b initial _ m points print Running . . . 
# By using this gradient _ descent _ runner function 
we will actually calculate gradient descent b m = gradient 
_ descent _ runner points initial _ b initial _ 
m learning _ rate num _ iterations # Here we 
are printing the values of b m and error after 
getting the line of best fit for the given dataset 
. print After { 0 } iterations b = { 
1 } m = { 2 } error = { 
3 } . format num _ iterations b m compute 
_ error _ for _ line _ given _ points 
b m points Starting gradient descent at b = 0 
m = 0 error = 5565 . 107834483211 Running . 
. . After 1000 iterations b = 0 . 0 
8 8 9 3 6 5 1 9 9 3 
7 4 1 3 4 6 m = 1 . 
4777440851894448 error = 112 . 61481011613473 我们 调 用了 两个 
函数 compute _ error _ for _ line _ points 
它 将 计算 实际 值 和 预测 值 之间 的 
误差 以及 gradient _ descent _ runner 它 将 为 
我们 计算 梯度 计算误差 或 损失 有 许多 方法 可以 
计算 ML 算法 的 误差 但在 本 章中 我们 将 
使用 最 流行 的 技术 之一 平方 距离 误差 之和 
现在 我们 直接 讨论 细节 这个 误差函数 对 我们 有 
什么 作用 回想 一下 我们 的 目标 我们 希望 得到 
最 适合 我们 的 数据 集 的 行 参考 . 
19 这是 线路 坡度 方程 这里 m 是 直线 的 
斜率 b 是 y 的 截距 x 和y是/nr 数据 点 
在 我们 的 例子 中 x 是 学生 学习 的 
小时 数 y 是 测试 分数 参见 . 19 利用 
前面 的 方程 我们 画出 直线 从/p 斜率/n m/w 和y/nr 
截距/v b/w 的/uj 随机/d 值/n 开始/v 用 第一列 数据 点 
作为 x 的 值 得到 y 的 值 在 训练 
数据 中 我们 已经 得到 y 的 值 这 意味着 
我们 知道 每个 学生 的 考试 分数 所以 对于 每个 
学生 我们 需要 计算出 误差 让 我们 以 一个 非常 
直观 的 例子 为例 注意 我们 正在 使用 虚拟 值 
进行 解释 假设 您 通过 放置 m 和b的/nr 随机 值 
得到 y 值 41.0 现在 您 得到 y 的 实际 
值 即 52.5 那么 预测值 和 实际 值 之间 的 
差 为 11.5 这 只是 一个 数 据点 但 我们 
需要 计算 每 个数 据点 因此 为了 进行 这种 误差 
计算 我们 使用 的 是 平方 距离 误差 之和 现在 
我们 如何 计算 平方 距离 误差 之和 为什么 要 使用 
平方 距离 误差 之和 那么 让 我们 从 第一 个 
问题 开始 计算 平方 距离 误差 和的/nr 公式 如 . 
20 所示 如 您 所见 最后 一 部分 mxi + 
b 是 我们 通过 选择 m 和b的/nr 随机 值 绘制 
的 线 我们 实际 上 可以 将 y 替换 为 
mxi + b 因此 我们 在 这里 计算 原始 y 
值 与 生成 y 值 之间 的 差 我们 将 
减去 原始 Y 值 和 生成 的 Y 值 并将 
每 个数 据点 的 该 值 平方 我们 之所以 对 
值 进行 平方 是 因为 我们 不想 处理 负值 因为 
我们 在 计算 平方 后 进行 求和 并且 我们 想 
测量 整体 的 大小 我们 不 需要 实际 的 价值 
因为 我们 正 试图 最小化 这个 整体 的 规模 现在 
回到 方程 我们 已经 计算 了 原始 y 值 和 
生成 y 值 之差 的 平方 现在 我们 对 所有 
这些 点 执行 求和 我们 将 使用 sigma 表示法 来 
指示 数据集 中所 有数 据点 的 求和 操作 此时 我们/r 
有/v 一个/m 指示/v 误差/n 大小/b 的/uj 和值/nr 我们 将 用 
这些 值 除以 数 据点 的 总数 之后 我们 将 
得到 我们 想要 的 实际 错误 值 您 可以 看到 
为了 生成 最 适合 我们 的 数据 集 的 行 
行 正在 为 每个 迭代 移动 我们 正在 根据 误差值 
更新 m 和b的/nr 值 现在 对于 每个 时间戳 行 是 
静态 的 我们 需要 计算误差 参照 . 21 现在 我们 
需要 根据 给定 的 方程 用 技术 的 方式 来 
表达 直观 的 例子 和 方程 在 这里 我们 计算 
从 每个 数据 点到 我们 画 的 直线 的 距离 
将 它们 平方 求和 然后 除以 总 点数 所以 在 
每次 迭代 或 时间 戳 之后 我们 可以 计算 我们 
的 错误 值 并 了解 我们 的 行 有多/nr 糟糕 
或者 我们 的 行 有 多好 如果 我们 的 行 
是 差 的 那么 为了 得到 最 适合 的 行 
我们 更新 m 和b的/nr 值 因此 错误 值 为 我们 
提供 了 指示 是否 有 改进 的 可能性 以 生成 
最佳 匹配 的 行 因此 我们 最终 想要 最小化 我们 
在 这里 得到 的 错误 值 以便 生成 最佳 拟 
合线 我们 如何 将 这个 错误 最小化 并 生成 最佳 
拟 合线 下 一步 称为 梯度 下降 平方和 误差 的 
原因 有 两个 对于 线性 回归 这是 最 常用 的 
计算误差 的 方法 如果 您 有 一个 大 的 数据集 
也 可以 使用 它 让 我们 看 一下 编码 部分 
然后 我们 将 跳到 计算 梯度 下降 的 核心 部分 
def compute _ error _ for _ line _ given 
_ points b m points totalError = 0for i in 
range 0 len points x = points i 0 y 
= points i 1 totalError + = y m * 
x + b * * 2return totalError / float len 
points 计算 梯度 使用 错误 函数 我们 知道 是否 应该 
更 新行 以 生成 最佳 匹配 的 行 但是 如何 
更 新行 将在 本节 中 看到 我们 如何 将 这个 
错误 最小化 并 生成 最佳 拟 合线 为了 回答 这个 
问题 首先 让/v 我们/r 对/p 梯度/n 下降/v 和/c 编码/n 部分/n 
有/v 一些/m 基本/n 的/uj 了解/v 我们 只 剩下 最后 一个 
函数 gradient _ descent _ runner 参考 . 23 Alt 
https / / img blog . csdnimg . cn / 
2 0 1 9 0 2 0 4 0 7 
1 8 2 9 1 8 8 . png 如 
. 23 所示 这 是 一个 三维图 这 两个 图 
是 相同 的 它们 的 视角 是 不同 的 这些 
图 显示 了 斜率 m y 截距 b 和 误差 
的 所有 可能 值 这是 三个 值 的 对 包括 
m b 和 error 这里 x 轴 是 一个 斜率 
值 y 轴 是 y 轴 截距 z 轴 是 
误差值 我们 试图 找出 错误 最少 的 地方 如果 你 
仔细 看 图表 那么 你 可以 观察到 在 曲线 的 
底部 误差值 是 最小 的 值 最小 的 点 称为 
ml 中的 局部 极小值 在 复杂 的 数据 集中 可以 
找到 多个 局部 极小值 这里 我们 的 数据集 很简单 因此 
我们 有 一个 局部 极小值 如果 您 有 一个 复杂 
的 高维 数据集 其中 有 多个 局部 极小值 那么 您 
需要 进行 二阶 优化 来 决定 应该 选择 哪个 局部 
极小值 以 获得 更好 的 精度 我们 不会 在 这 
本书 中 看到 二阶 优化 现在 让 我们 回顾 一下 
我们 的 图表 在 这里 我们 可以 直观 地 识别 
出 给 我们 最小 误差值 的 点 同样 的 点 
也给 出了 y 截距 的 理想 值 即 b 和 
斜率 值 即 m 当 我们 得到 b 和m的/nr 理想 
值 时 我们 将 把 这些 值 放入 我们 的 
y = mx + c 方程 中 然后 魔法 就会 
发生 我们 将 得到 最佳 拟 合线 这 不是 获得最佳 
拟 合线 的 唯一 方法 但 我 的 目的 是 
让 您 对 梯度 下降 有一个 深入 的 了解 以便 
我们 以后 可以 在 DL 中 使用 这个 概念 现在 
从 视觉 上看 你 可以 看到 误差 最小 的 点 
但是 如何 达到 这个 点 呢 答案 是 通过 计算 
梯度 梯度 也 称为 坡度 但 这 不是 坡度 值 
m 因此 不要 混淆 我们 讨论 的 是 斜坡 的 
方向 使 我们 到达 那个 最小 的 误差 点 所以/c 
我们/r 有/v 一些/m b/w 值/n 和m值/nr 在 每次 迭代 之后 
我们 更新 这些 b 值 和m值/nr 这样 我们 就 可以 
达到 最小 的 误差值 点 所以 从 三维 图像 的 
角度 来看 如果 你 在 曲线 的 顶部 每次 迭代 
我们 计算 梯度 和 误差 然后 更新 m 和b的/nr 值 
到达 曲线 的 底部 我们 需要 到达 曲线 的 底部 
通过 计算 梯度 值 我们 得到 了 我们 下 一步 
应该 采取 的 方向 所以 梯度 是 一条 切线 它 
不断 地 告诉 我们 我们 需要 移动 的 方向 无论是 
向上 还是 向下 以 达到 最小 的 误差 点 并/c 
获得/v 理想/n 的/uj B/w 和M值/nr 来生/v 成/n 最佳/z 拟/v 合线/i 
参见 . 24 现在 让 我们 看看 最后 一个 但 
不是 最 不 重要 的 计算 梯度 下降 的 方程 
在 . 25 中 你 可以 看到 梯度 下降 方程 
它 只是 我们 误差函数 的 偏 导数 我们 采用 平方 
误差 和 方程 对 m 和b/nr 进行 偏 导数 计算 
梯度 下降 结果 如 . 25 所示 左侧 符号 是 
偏 导数 的 符号 这里 我们 有 两个 方程 因为 
我们 取了 误差函数 生成 了 关于 变量 m 的 偏 
导数 在 第二 个 方程 中 我们 生成 了 关于 
变量 b 的 偏 导数 通过 这 两个 方程 我们 
将 得到 b 和m的/nr 更新 值 为了 计算 梯度 我们 
需要 导出 偏 导数 误差函数 对于 ml 和 dl 中 
的 一些 问题 我们 不 知道 误差函数 的 偏 导数 
这 意味着 我们 找 不到 梯度 所以 我们 不 知道 
如何 处理 这种 函数 你 的 误差函数 应该是 可微 的 
这 意味着 你 的 误差函数 应该有 偏 导数 这里 的 
另一 件事 是 我们 使用 的 是 线性方程 但是 如果 
你 有 高维 的 数据 那么 你 可以 使用 非 
线性函数 如果 你 知道 误差函数 的话 当 我们 第一 次 
开始 时 梯度 下降 并 没有 给 我们 最小值 梯度 
只是 告诉 我们 如何 更新 m 和b值/nr 无论 我们 应该 
更新 为 正值 还是 负值 所以 梯度 给 了 我们 
一个 如何 更新 m 和b值/nr 的 概念 也 就是说 通过 
计算 梯度 我们 得到 了 方向 并/c 试图/v 达到/v 我们/r 
得到/v m/w 和b的/nr 最小/a 误差值/l 和/c 最佳值/l 的/uj 点/m def 
step _ gradient b _ current m _ current points 
learningRate b _ gradient = 0m _ gradient = 0N 
= float len points for i in range 0 len 
points x = points i 0 y = points i 
1 # Here we are coding up out partial derivatives 
equations and # generate the updated value for m and 
b to get the local minimab _ gradient + = 
2 / N * y m _ current * x 
+ b _ current m _ gradient + = 2 
/ N * x * y m _ current * 
x + b _ current # we are multiplying the 
b _ gradient and m _ gradient with learningrate # 
so it is important to choose ideal learning rate if 
we make it to high then our model learn nothing 
# if we make it to small then our training 
is to slow and there are the chances of over 
fitting # so learning rate is important hyper parameter . 
new _ b = b _ current learningRate * b 
_ gradient new _ m = m _ current learningRate 
* m _ gradient return new _ b new _ 
m 在 代码 中 我们 将 m _ gradient 和b_/nr 
gradient 与 学习 速率 相乘 因此 学习 速率 是 一个 
重要 的 超 参数 选择 它 的 值 时要/nr 小心 
如果 您 选择 一个 非常 高的值/nr 您 的 模型 可能 
根本 不会 训练 如果 你 选择 了 一个 非常 低 
的 值 那么 训练 会 花费 很多 时间 而且 也 
有可能 过度 拟合 请参阅 . 27 它 为 您 提供 
了 关于 良好 学习率 的 直觉 激活 函数 让 我们 
先 看看 激活 函数 我 想 给 你 一个 概念 
在 什么 阶段 的 神经 网络 我们 将 使用 这个 
激活 函数 在 我们 对 感知器 的 讨论 中 我们 
说 如果 超过 某个 阈值 神经 网络 将 生成 一个 
输出 否则 输出 将 为零 整个 机制 计算 阈值 并 
生成 基于 此 阈值 的 输出 由 激活 函数 负责 
激活 函数 能够 为 我们 提供 介于 0 和1/nr 之间 
的 值 之后 使用 阈值 我们 可以 生成 输出 值 
1 或 输出 值 0 假设 我们 的 阈值 是 
0.777 我们 的 激活 函数 输出 是 0.457 那么 我们 
的 结果 输出 是 0 如果 我们 的 激活 函数 
输出 是 0.852 那么 我们 的 结果 输出 是 1 
所以 下面 是 激活 函数 在 ANN 中 的 工作 
原理 通常 在 神经 网络 中 每个/r 神经元/nz 都有/nr 一定/d 
的/uj 权重/n 和/c 输入/v 值/n 我们 正在 求和 并 生成 
加权 和值/nr 当 我们 通过 非 线性函数 传递 这些 值 
时 这个 非 线性函数 激活 一定 数量 的 神经元 以 
获得 复杂 任务 的 输出 这个 神经元 的 激活 过程 
使用 一定 的 非线性 数学 函数 被 称为 激活 函数 
或 传递函数 激活 函数 将 输入 节 点映 射到 输出 
节点 以 某种 方式 使用 某些 数学 运算 在 人工 
神经 网络 中 具有 激活 函数 的 目的 是 在 
网络 中 引入 非线性 让 我们 一步 一步 地 了解 
这 一点 让 我们 集中 讨论 一下 ANN 的 结构 
该 神经 网络结构 可 进一步 分为 三 个 部分 架构 
架构 就是 决定 神经 网络 中 神经 元和 层 的 
排列 激活 为了 生成 复杂 任务 的 输出 我们 需要 
看到 神经元 的 活动 一个 神经元 如何 响应 另一个 神经元 
以 生成 复杂 行为 学习 规则 当 ANN 生成 输出 
时 我们 需要 在 每个 时间戳 更新 ANN 权重 以 
使用 误差函数 优化 输出 激活 函数 是 激活 部分 的 
一部分 如前所述 我们 将 把 非线性 引入 神经网络 其 背后 
的 原因 是 没有 非线性 神经 网络 不能 产生 复杂 
的 行为 来 解决 复杂 的 任务 在 数字 语言 
中 大多数 情况 下 我们 使用 非线性 激活 函数 来 
获得 复杂 的 行为 除此之外 我们 还 希望 以 非线性 
方式 将 输入 映 射到 输出 如果 您 不 使用 
非线性 激活 函数 那么 对于 复杂 的 任务 ANN 将 
不会 为 您 提供 大量 有用 的 输出 因为 您 
正在 传递 矩阵 并且 如果 您 在 ANN 中 使用 
多个 具有 线性 激活 函数 的 层 则会 得到 一个 
输出 该 输出 是 输入 值 权重 和 偏差 的 
总和 所 有层 这个 输 出给 你 另一个 线性函数 这 
意味着 这个 线性函数 将 多层 人工神经网络 的 行为 转换 为 
单层 人工神经网络 这种 行为 对于 解决 复杂 的 任务 一点 
都不 没有用 我 想 强调 一下 连接 主义 的 概念 
神经 网络 中 的 连接 主义 是 利用 相互连接 的 
神经元 产生 复杂 的 行为 就像 人脑 一样 如果 不 
在 神经 网络 中 引入 非线性 我们 就 无法 实现 
这种 行为 参见 . 29 了解 激活 功能 这里 我们 
将 介绍 上 图中 提到 的 这些 功能 Transfer potential 
这 是 一个 集合 输入 和 权重 的 函数 更 
具体地说 此 函数 执行 输入 和 权重 的 总和 Activation 
function 该 函数 将 传递 势函数 的 输出 作为 输入 
并 使用 激活 函数 进行 非线性 数学 变换 Threshold function 
基于 激活 功能 阈值 功能 激活 神经元 或不 激活 传递 
势 是 一个 简单 的 求和 函数 它 将 输入 
的 内积 和 连接 的 权值 相加 如 . 30 
所示 这种 传递 势 通常 是 点积 但 它 可以 
使用 任何 数学 方程 如 多 二次函数 另一方面 激活 函数 
应该 是 任何 可微 的 非线性 函数 它 必须 是 
可微 的 这样 我们 才能 计算误差 梯度 而且 这个 函数 
必须 具有 非线性 特性 才能 从 神经 网络 中 获得 
复杂 的 行为 通常 我们 使用 sigmoid 函数 作为 激活 
函数 这 需要 将 潜在 输出 值 作为 输入 计算 
最终 输出 然后 计算 实际 输出 和 生成 输出 之间 
的 误差 然后 我们 将 利用 误差 梯度 的 计算 
概念 以及 应用 反向 传播 优化 策略 来 更新 神经 
网络 连接 的 权重 . 31 用 theta 表示 传递 
势函数 也 称为 logit 我们 将 在 logistic sigmoid 激活 
函数 方程 中 使用 该 函数 您 可以 在 . 
32 中 看到 logistic sigmoid 函数 的 方程 激活 函数 
背后 的 整个 想法 大致 模拟 了 神经元 在 大脑 
中 相互 交流 的 方式 每 一个 都是/nr 通过 它 
的 动作电位 被 激活 的 如果 达到 某个 阈值 那么 
我们 就 知道 是否 激活 一个 神经元 激活 功能 模拟 
大脑 动作电位 的 峰值 深度 神经网络 DNN 被 称为 通用 
近似 函数 因为 它们 可以 在 任何 情况 下 计算 
任何 函数 它们 可以 计算 任何 可微 的 线性函数 和非/nr 
线性函数 现在 您 可能会 问 我 何时 使用 这个 激活 
函数 我们 将 在下 一段 中 看到 这 一点 有 
多种 激活 函数 可用 使用 时要/nr 小心 我们 不 应该 
仅仅 因为 它们 听起来 很新 很酷 就 使用 它们 在 
这里 我们 将 讨论 您 如何 知道 应该 使用 哪一个 
我们 将 看到 三 个 主要 的 激活 函数 因为 
它们 在 DL 中的 广泛 使用 尽管 还有 其他 的 
激活 函数 可以 使用 SigmoidTanhReLU 及其 变体 Sigmoid 就其 数学 
概念 而言 Sigmoid 函数 很容易 理解 其 数学公式 如 . 
33 所示 如 . 33 所示 sigmoid 函数 将 采用 
给定 的 方程 取 一个 数字 并在 0 和1的/nr 范围 
内 挤压 该 数字 它 产生 一条 形 曲线 这个 
函数 是 第一个 在 神经 网络 中 作为 激活 函数 
使用 的 函数 因为 它 可以 解释 为 神经元 的 
激活 率 零 意味着 没有 激活 一个 是 完全 饱和 
的 激活 当 我们 在 DNN 中 使用 这个 激活 
函数 时 我们 了解 到 这个 激活 函数 的 一些 
局限性 这 使得 它 现在 不 那么 流行 这些 功能 
的 一些 基本 问题 如下 它 有 梯度 消失 问题 
它 的 收敛 速度 很慢 它 不是 一个 以 零 
为 中心 的 函数 让 我们 详细 了解 每个 问题 
消失 梯度 问题 当 使用 基于 梯度 的 方法 训练 
某些 神经 网络 时 尤其 是 在 使用 反向 传播 
的 神经 网络 中 可以 发现 这个 问题 这个 问题 
使得 学习 和 调整 神经网络 早期 层 的 参数 变得 
非常 困难 当 你 向你的/nr 人工神经网络 添加 更多 的 层 
时 这就 变得 更加 困难 了 如果 我们 明智 地 
选择 激活 函数 那么 这个 问题 就 可以 得到 解决 
我 想 先给 你 详细 介绍 一下 这个 问题 然后 
我们 再 讨论 它 背后 的 原因 基于 梯度 的 
方法 通过 了解 输入 参数 和 权重 的 微小 变化 
如何 影响 神经 网络 的 输出 来 学习 参数值 如果 
这个 梯度 太小 那么 参数 的 变化 将 导致 神经 
网络 输出 的 变化 非常 小 在 这种 情况 下 
经过 一些 迭代 后 神经 网络 不能 有效 地 学习 
参数 并且 不会 以 我们 想要 的 方式 收敛 这 
就是 梯度 消失 问题 中 发生 的 情况 网络 输出 
相对于 早期 层 参数 的 梯度 变得 非常 小 可以 
说 即使 输入 层 和 权重 的 参数值 发生 了 
很大 的 变化 也 不会 对 输出 产生 很大 的 
影响 我 给 你 所有 这些 细节 因为 你 也 
可以 面对 同样 的 问题 与 sigmoid 函数 最 基本 
的 问题 是 这个 消失 梯度 问题 取决于 你 的 
激活 函数 的 选择 sigmoid 函 数以 非线性 方式 将 
输入 压缩 成 一个 小 范围 的 输出 如果 您 
给 sigmoid 函数 一个 实数 它 将把 这个数 压缩 到 
0 1 的 范围 内 所以 输入 空间 中 有 
很大 的 区域 被 映射 到 一个 非常 小 的 
范围 即使 输入 参数 发生 很大 的 变化 也 会 
导致 因为 这个 区域 的 梯度 很小 所以 输出 变化 
很小 对于 sigmoid 函数 当 一个 神经元 饱和 接近 零 
或 一时 这个 区域 的 梯度 非常 接近 零 在 
反向 传播 过程 中 这个 局部 梯度 将 乘以 每个 
层 的 输出 的 梯度 因此 如果 第一 层 映射 
到 一个 大 的 输入 区域 我们 得到 一个 非常 
小 的 梯度 以及 第一层 输出 的 非常 小 的 
变化 这一 微小 的 变化 传递 到 下一层 并在 第二层 
的 输出 中 产生 更小 的 变化 如果 我们 有 
一个 DNN 那么 在 某些 层 之后 输出 就 不会 
发生 变化 这是 sigmoid 函数 的 问题 低 收敛 速度 
由于 这个 消失 梯度 问题 有时 具有 sigmoid 函数 的 
神经 网络 收敛 非常 慢 非零 中心 功能 sigmoid 函数 
不是 零 中心 激活 功能 这 意味着 sigmoid 函数 的 
输出 范围 是 0 1 这 意味着 函数 的 输出 
值 将 始终 为 正 从而 使 权重 的 梯度 
变为 全部 正 或 全部 负 这 使得 梯度 更新 
在 不同 的 方向 上 走得 太远 这 使得 优化 
更加 困难 由于 这些 限制 sigmoid 函数 最近 没有 在 
DNN 中 使用 虽然 您 可以 使用 其他 函数 来 
解决 这些 问题 但是 您 也 可以 只在 您 的 
神经 网络 的 最后 一层 使用 sigmoid 函数 TanH 为了 
克服 sigmoid 函数 的 问题 我们 将 引入 一个 激活 
函数 叫做 双曲 正切函数 tanh tanh 方程 如 . 34 
所示 此 函数 将 输入 区域 压缩 到 范围 1 
1 内 因此 其 输出 是以 零 为 中心 的 
这 使得 优化 对 我们 来说 更加 容易 这个 函数 
也有 消失 梯度 问题 所以 我们 需要 看到 其他 的 
激活 函数 ReLU 及其 变体 整流 线性 单元 ReLU 是 
工业 上 最 流行 的 函数 看 它 的 方程式 
在 . 35 中 如果 你 会 看到 ReLU 数学 
方程 那么 你 就会 知道 它 只是 max 0 x 
这 意味着 当 x 小于 零时 值 为零 当 x 
大于 或 等于 零时 值 与 斜率 1成 线性关系 一位 
名叫 Krizhevsky 的 研究 人员 发表 了 一篇 关于 图像 
分类 的 论文 并说 他们 使用 ReLU 作为 激活 函数 
可以 更快 地 收敛 六倍 你 可以 点击 阅读 这篇 
研究 论文 http / / www . cs . toronto 
. edu / ~ fritz / absps / imagenet . 
pdf . 这个 函数 很简单 没有 任何 复杂 的 计算 
而且 比 Sigmoid 和 Tanh 更简单 这 就是 这个 函数 
学习 更快 的 原因 除此之外 它 也 没有 消失 梯度 
问题 我们 曾经 在 DNN 中 存在 的 每 一层 
中 应用 激活 函数 目前 ReLU 已被 广泛 应用于 大部分 
的 DNN 但 它 被 应用于 DNN 的 隐藏 层 
如果 要 解决 分类 问题 输出 层 应该 使用 SoftMax 
因为 SoftMax 函数 为 我们 提供 了 每个 类 的 
概率 我们 在 word2vec 算法 中 使用 了 softmax 激活 
函数 对于 回归 问题 输出 层 应该 使用 线性函数 因为 
信号 通过 不变 除了 ReLU 的 所有 这些 优点 外 
它 还有 一个 问题 在 训练 过程 中 神经 网络 
的 某些 单元 可能会 脆弱 并 死亡 这 意味着 通过 
ReLU 神经元 的 大 梯度 可能会 导致 权重 更新 使其 
不再 在 任何 数据 点上 激活 所以 流过 它 的 
梯度 从那 一点 开始 总是 零 为了 克服 ReLU 的 
这种 局限性 引入 了 一种 ReLU 的 变体 Leaky ReLU 
当 x 小于 0 x 0 时 Leaky ReLU 的 
负 斜率 较小 而 不是 函数 为零 参见 . 36 
还有 一种 变体 叫做 maxout 它 是 ReLU 和 Leaky 
ReLU 的 广义 形式 但 它 使 每个 神经元 的 
参数 加倍 这 是 一个 缺点 现在 您 已经 对 
激活 函数 有了 足够 的 了解 那么 应该 使用 哪个 
函数 呢 答案 是 ReLU 但是 如果 太多 的 神经 
元 死亡 那么 使用 Leaky ReLU 或 maxout 此 激活 
功能 应用 于 隐藏 层 对于 输出 层 如果 要 
解决 分类 问题 请使用 SoftMax 函数 如果 要 解决 回归 
问题 请使用 线性 激活 函数 Sigmoid/w 和/c Tanh/w 不/d 应用/v 
于/p DNNS/w 这 是 一个 非常 有趣 的 研究 领域 
有 很大 的 空间 来 提出 伟大 的 激活 功能 
还有 其他 的 激活 函数 可以 检查 标识 函数 二进制 
步进 函数 arctan 等等 在 这里 我们 将 检查 第三 
个 重要 概念 损失 函数 损失 函数 有时 损失 函数 
也 被称为 成本 函数 或 误差函数 损失 函数 给 我们 
提供 了 一个 关于 给定 训练 示例 的 神经 网络 
性能 的 概念 因此 首先 我们 定义 了 误差函数 当 
我们 开始 训练 我们 的 神经 网络 时 我们 将 
得到 输出 我们 将 生成 的 输出 与 作为 训练 
数据 一 部分 给出 的 预期 输出 进行 比较 并 
计算 该 误差函数 的 梯度 值 我们 反向 传播 网络 
中 的 误差 梯度 以便 更新 现有 的 权重 和 
偏差 值 以 优化 生成 的 输出 误差函数 是 训练 
的 主要 部分 有 多种 误差函数 可用 如果 你问 我 
选择 哪个 错误 函数 那么 就 没有 具体 的 答案 
因为 所有 的 人工神经网络 训练 和 优化 都是 基于 这个 
损失 函数 所以 这 取决于 你 的 数据 和 问题 
陈述 如果 你 问 某人 你 在 你 的 神经 
网络 中 使用 了 哪个 误差函数 那么 你 间接地 问 
他们 训练 算法 的 整个 逻辑 无论 您 将 使用 
什么 误差函数 请 确保 该 函数 必须 是 可微 的 
我 列出 了 一些 最 流行 的 错误 函数 二次 
成本 函数 又称/n 均方/i 误差/n 或/c 和方/nr 误差/n 交叉/n 熵/g 
成本/n 函数/n 也 称为 伯努利 负 对数 似 然 或 
二进制 交叉 熵 Kullback Leibler 散度 也 称为 信息 散度 
信息 增益 相对 熵 或 klic 除此之外 还有 许多 其他 
的 损失 函数 如 指数 成本 海 林格 距离 广义 
Kullback Leibler 散度 和 Itakura Saito 距离 一般来说 我们 在 
回归 中 使用 平方和 误差 在 分类 数据 和 分类 
任务 中 使用 交叉 熵 9.5 实现 神经 网络 在 
本节 中 我们 将 使用 numpy 作为 依赖 项在/nr python 
中 实现 我们 的 第一 个 ANN 在 这个 实现 
过程 中 您 可以 将 梯度 下降 激活 函数 和 
损失 函数 集成 到 我们 的 代码 中 除此之外 我们 
将 看到 反向 传播 的 概念 我们 将 看到 使用 
反向 传播 的 单层 NN 的 实现 9 . 5.1 
单层 反向 传播 神经 网络 在 一个 单层 神经 网络 
中 我们 有 输入 我们 把 它 输入 到 第一 
层 这些 层 连接 有一些 权重 我们 使用 输入 权重 
和 偏差 并对 它们 求和 这个 和 通过 激活 函数 
并 生成 输出 这 是 一个 重要 的 步骤 无论/c 
生成/nr 什么/r 输出/v 都 应该 与 实际 的 预期 输出 
进行 比较 根据 误差函数 计算误差 现在 使用 误差函数 的 梯度 
并 计算误差 梯度 这个 过程 和 我们 在 梯度 下降 
部分 看到 的 一样 这个 误差 梯度 给出 了 如何 
优化 生成 的 输出 的 指示 误差 梯度 流回 神经 
网络 并 开始 更新 权重 以便 在 下 一个 迭代 
中 得到 更好 的 输出 在 人工 神经 网络 中 
为了 产生 更 精确 的 输出 通过 返回 误差 梯度 
来 更新 权重 的 过程 称为 反向 传播 总之 后向 
传播 是 一种 通过 梯度 下降 更新 权值 来 训练 
神经 网络 的 常用 训练 技术 计算 和 数学 的 
所有 其他 方面 将 显示 在 编码 部分 所以 让 
我们 用 反向 传播 来 编写 我们 自己 的 单层 
前馈 神经网络 首先 我们 将 定义 主要 功能 和 抽象 
步骤 这里 我们 将 给出 输入 和 输出 值 因为 
我们 的 数据 是 标记 的 所以 它 是 一个 
有 监督 的 学习 示例 第二步 是 训练 我们 将 
重复 训练 重复 10000次 我们 将 首先 从 随机 权重 
开始 并 根据 激活 函数 和 误差函数 调整 权重 from 
numpy import exp array random dot class NeuralNetwork def _ 
_ init _ _ self # Seed the random number 
generator so it generates the same numbers # every time 
the program runs . random . seed 1 # We 
model a single neuron with 3 input connections and 1 
output connection . # We assign random weights to a 
3 x 1 matrix with values in the range 1 
to 1 # and mean 0 . self . synaptic 
_ weights = 2 * random . random 3 1 
1 # The Sigmoid function which describes an S shaped 
curve . # We pass the weighted sum of the 
inputs through this function to # normalise them between 0 
and 1 . def _ _ sigmoid self x return 
1 / 1 + exp x # The derivative of 
the Sigmoid function . # This is the gradient of 
the Sigmoid curve . # It indicates how confident we 
are about the existing weight . def _ _ sigmoid 
_ derivative self x return x * 1 x # 
We train the neural network through a process of trial 
and error . # Adjusting the synaptic weights each time 
. def train self training _ set _ inputs training 
_ set _ outputs number _ of _ training _ 
iterations for iteration in range number _ of _ training 
_ iterations # Pass the training set through our neural 
network a single neuron . output = self . think 
training _ set _ inputs # Calculate the error The 
difference between the desired output # and the predicted output 
. error = training _ set _ outputs output # 
Multiply the error by the input and again by the 
gradient of the Sigmoid curve . # This means less 
confident weights are adjusted more . # This means inputs 
which are zero do not cause changes to the weights 
. adjustment = dot training _ set _ inputs . 
T error * self . _ _ sigmoid _ derivative 
output # Adjust the weights . self . synaptic _ 
weights + = adjustment # The neural network thinks . 
def think self inputs # Pass inputs through our neural 
network our single neuron . return self . _ _ 
sigmoid dot inputs self . synaptic _ weights # Intialise 
a single neuron neural network . neural _ network = 
NeuralNetwork print Random starting synaptic weights print neural _ network 
. synaptic _ weights # The training set . We 
have 4 examples each consisting of 3 input values # 
and 1 output value . training _ set _ inputs 
= array 0 0 1 1 1 1 1 0 
1 0 1 1 # Python store output in horizontally 
so we have use transpose training _ set _ outputs 
= array 0 1 1 0 . T # Train 
the neural network using a training set . # Do 
it 10 000 times and make small adjustments each time 
. neural _ network . train training _ set _ 
inputs training _ set _ outputs 10000 print New synaptic 
weights after training print neural _ network . synaptic _ 
weights # Test the neural network with a new situation 
. print Considering new situation 1 0 0 print neural 
_ network . think array 1 0 0 Random starting 
synaptic weights 0.16595599 0.44064899 0.99977125 New synaptic weights after training 
9.67299303 0.2078435 4.62963669 Considering new situation 1 0 0 0.99993704 
这里 我们 使用 Sigmoid 作为 激活 函数 我们 将 使用 
Sigmoid 导数 来 计算 Sigmoid 曲线 的 梯度 我们 的 
损失 函数 是从 生成 的 输出 中 减去 实际 输出 
我们 用 这个 误差值 乘以 梯度 得到 误差 梯度 这 
有助于 我们 调整 神经 网络 的 权重 新 更新 的 
权值 和 输入 再次 通过 神经网络 计算出 Sigmoid 曲线 的 
梯度 下降 和 误差 梯度 并 调整 权值 直到 得到 
最小 误差 9 . 5.2 练习 使用 numpy 作为 依赖 
项 构建 三层 深度 人工神经网络 提示 在 单层 人工神经网络 中 
我们 使用 单层 但 在 这里 您 将 使用 三层 
反向 传播 通常 使用 递归 取 导数 但 在 我们 
的 单层 演示 中 没有 递归 所以 需要 应用 递归 
导数 9.6 深度 学习 和 深度 神经网络 现在 从 ANN 
到 DNN 在 接下来 的 部分 中 我们 将 看到 
深度 学习 DNN 的 体系 结构 并 比较 DL 用于 
NLP 和 ML 用于 NLP 的 方法 9 . 6.1 
回顾 深度 学习 我们 已经 看到 了 有关 DL 的 
一些 基本 细节 在 这里 目的 只是 为了 让 你 
回忆 起 一些 事情 不是 两层 或 三层 而是 多层 
深 的 人工神经网络 称为 DNN 当 我们 在 大量 数据 
上 使用 多层 深度 神经 网络 时 使用 大量 的 
计算 能力 我们 称之为 深度 学习 过程 让 我们 看看 
深层 神经 网络 的 结构 9 . 6.2 深度 神经 
网络 的 基本 架构 在 本节 中 我们 将 看到 
DNN 的 体系 结构 图形 表示 看起来 很 简单 并用 
一些 很酷 的 数学 公式 定义 如 激活 函数 隐 
层 激活 函数 损失 函 数等 在 . 41 中 
您 可以 看到 DNN 的 基本 架构 现在 为什么 我们 
要 使用 多层 深度 神经网络 有 什么 原因 可以 这样 
做 吗 有 多层 的 意义 是 什么 让 我 
解释 一下 为什么 我们 使用 多层 DNN 假设 作为 一名 
编码 人员 您 希望 开发 一个 识别 水果 图像 的 
系统 现在/t 你/r 有了/nr 一些/m 橙子/n 和/c 苹果/n 的/uj 图像/n 
你 开发 了 一个 逻辑 比如 我 可以 使 用 
水果 的 颜色 来 识别 图像 你 还 添加 了 
形状 作为 识别 参数 您 进行 了 一些 编码 并 
准备好 了 结果 现在 如果 有人 告诉 你 我们 也 
有 黑白 图像 现在 您 需要 重做 编码 工作 有些 
种类 的 图像 对 你 来说 太 复杂 了 作为 
一个 人 无法 编码 尽管 你 的 大脑 非常 擅长 
识别 水果 的 实际 名称 因此 如果 你 有 一个 
如此 复杂 的 问题 你 不 知道 如何 编码 或者 
你 不太 了解 那些 有助于 机器 解决 问题 的 特征 
或 参数 那么 你 就 使用 一个 深度 神经网络 原因 
有 以下 几种 DNN 是 用人 脑 工作 原理 的 
抽象 概念 推导 出来 的 使用 DNN 我们 改变 了 
编码 的 方法 最初 我们 为 机器 提供 颜色 形状 
等功能 以 识别 给定 图像 中 的 水果 名称 但 
通过 DNN 和 DL 我们 为 机器 提供 了 许多 
示例 机器 将 自己 了解 这些 功能 之后 当 我们 
为 机器 提供 一个 新的 水果 图像 时 它 将 
预测 水果 的 名称 现在 您 真的 想 知道 dnn 
如何 自己 学习 特性 所以 让 我们 重点 强调 以下几点 
DNN 使用 多层 非线性 处理单元 的 级联 用于 特征提取 和 
转换 每个 连续 的 DNN 层 都 使用 前 一层 
的 输出 作为 输入 这个 过程 与 人脑 如何 将 
信息 从 一个 神经元 传输 到 另一个 神经元 非常 相似 
所以 我们 试图 在 DNN 的 帮助 下 实现 相同 
的 结构 在 DL 中 在 DNN 的 帮助 下 
使用 多个 表示 级别 学习 了 特性 更高 级别 的 
特征 或 表示 是从 较低 级别 的 特征 派生 而来 
的 所以 我们 可以 说 在 DNN 中 派生 特征 
或 表示 的 概念 是 分层 的 我们 用 这个 
较低 层次 的 想法 学习 一些 新 东西 并 尝试 
学习 一些 额外 的 东西 我们 的 大脑 也 以 
层次 结构 的 方式 使用 和 派生 概念 这种 不同 
级别 的 特性 或 表示 与 不同 级别 的 抽象 
相关 DNN 的 多层 有助于 机器 导出 层次 表示 这是 
将 多层 作为 体系 结构 的 一部分 的 重要性 借助 
DNN 和 数学 概念 机器 能够 模拟 人脑 的 某些 
过程 DL/w 可以/c 应用于/i 有/v 监督/vn 和无/nr 监督/vn 的/uj 数据集/i 
以 开发 NLP 应用程序 如 机器翻译 总结 问答 系统 论文 
生成 图像 标题 标记 等 9 . 6.3 NLP 中的 
深度 学习 NLP 的 早期 是 基于 基于 规则 的 
系统 对于 许多 应用 来说 早期 的 原型 是 基于 
基于 规则 的 系统 因为 我们 没有 大量 的 数据 
现在 我们 正在 应用 ML 技术 来 处理 自然语言 使用 
基于 统计 和 概率 的 方法 在 这种 方法 中 
我们 以 一种 热 编码 格式 或 共 现 矩阵 
的 形式 表示 单词 在 这种 方法 中 我们 得到 
的 主要 是 句法 表示 而 不是 语义 表示 当 
我们 尝试 基于 词汇 的 方法 时 例如 词 袋 
n grams 等等 我们 无法 区分 特定 的 上下文 我们/r 
希望/v 所有/b 这些/r 问题/n 都能/nr 通过/p DNN/w 和/c DL/w 解决/v 
因为 现在 我们 有 大量 的 数据 可以 使用 为了 
捕捉 自然 语言 的 语义 方面 我们 开 发了 好 
的 算法 如 word2vec glove 等 除此之外 DNN 和 DL 
还 提供 了 一些 很酷 的 功能 如下 所示 可 
表达性 这种 能力 表示 机器 对 通用 函数 的 逼近 
程度 可 训练 性 这种 能力 对于 NLP 应用 程序 
非常 重要 它 表明 一个 DL 系统 能够 很好 快速 
地 了解 给定 的 问题 并 开始 生成 大量 的 
输出 可 归纳 性 这 表示 机器 能够 很好 地 
归纳 给定 的 任务 以便 它 能够 预测 或 生成 
未知 数据 的 准确 结果 除了 上述 三 种 功能 
外 DL 还 为 我们 提供 了 其他 功能 如 
可 解释性 模块性 可 传输 性 延迟 对抗 稳定性 和 
安全 性 我们 知道 语言 是 复杂 的 事情 有时 
我们 也 不 知道 如何 解决 某些 NLP 问题 这 
背后 的 原因 是 世界 上 有 如此 多 的 
语言 它们 有 自己 的 句法结构 单词 用法 和 意义 
你 不能 用 同样 的 方式 用 其他 语言 表达 
所以 我们 需要 一些 帮助 我们 概括 问题 并 给出 
良好 结果 的 技术 所有/b 这些/r 原因/n 和/c 因素/n 都/d 
引导/v 我们/r 朝着/p 将/d dnn/w 和/c dl/w 用于/v nlp/w 应用/v 
的/uj 方向/n 发展/vn 现在 让 我们 看看 经典 的 NLP 
技术 和 dl nlp 技术 之间 的 区别 因为 这 
将 连接 我们 的 点 说明 dl 在 解决 与 
nlp 域 相关 的 问题 上 是 如何 更 有用 
的 9 . 6.4 传统 NLP 和 深度 学习 NLP 
技术 的 区别 在 这 一节 中 我们 将 比较 
经典 的 NLP 技术 和 用于 NLP 的 DL 技术 
参见 . 42 在 经典 的 NLP 技术 中 我们 
在 数据 生成 特征 之前 对 数据 进行 了 早期 
的 预处理 在下 一 阶段 我们 使用 手工 制作 的 
特性 这些 特性 是 使用 NER 工具 POS 标记 器 
和 解析器 生成 的 我们 将 这些 特性 作为 ML 
算法 的 输入 并 训练 模型 我们 将 检查 精度 
如果 精度 不好 我们 将 优化 算法 的 一些 参数 
并 尝试 生成 更 精确 的 结果 根据 NLP 应用 
程序 的 不同 可以 包括 检测 语言 并 生成 功能 
的 模块 Alt https / / img blog . csdnimg 
. cn / 2 0 1 9 0 2 0 
4 0 7 2 9 2 4 8 4 1 
. png 现在 让 我们 看看 NLP 应用 程序 的 
深入 学习 技术 在 这种 方法 中 我们 对 我们 
拥有 的 数据 进行 一些 基本 的 预处理 然后 我们 
将 文本 输入 数据 转换 为 密集 向量 的 形式 
为了 产生 密集 向量 我们 将 使用 word2vec glove doc2vec 
等 嵌入 技术 并 将 这些 密集 向量 嵌入 到 
dnn 中 这里 我们 不 使用 手工 制作 的 特性 
而是 根据 NLP 应用程序 使用 不同 类型 的 dnn 例如 
机器翻译 我们 使用 的 是 dnn 的 变体 称 为序 
列到 序列 模型 总而言之 我们 使用 的 是 另一种 变体 
即 长短期 内存单元 lstms DNN 的 多层 概括 了 目标 
并 学习 了 实现 定义 目标 的 步骤 在 这个 
过程 中 机器学习 了 层次 表示 并给 出了 根据 需要 
对 模型 进行 验证 和 优化 的 结果 下 一节 
是 本章 最 有趣 的 部分 我们 将 构建 两个 
主要 应用程序 一个 用于 NLU 另 一个 用于 NLG 我们/r 
使用/v TensorFlow/w 和/c Keras/w 作为/v 主要/b 依赖/v 项来/nr 编写/v 示例代码/i 
我们 将 理解 dnn 的 变体 例如 sequence to sequence 
和 LSTM 因为 我们 对 它们 进行 编码 以 更好 
地 理解 它们 猜猜 我们 要 建造 什么 我们 将 
构建 一个 机器 翻译 程序 作为 NLP 应用 程序 的 
一部分 我们 将 从中 生成 一个 摘要 那么 让 我们 
跳到 编码 部分 我会 给 你 做 一些 有趣 的 
练习 9.7 深度 学习 技术 和 NLU 人 类说 写 
或 读 的 语言 太多 了 你 试过 学 一门 
新 语言 吗 如果 是 的话 那么 你 就 知道 
掌握 说 一种 新 语言 或 写 一种 新 语言 
的 技能 是 多么 困难 你 有 没有 想过 谷歌 
翻译 是 如何 被 用来 翻译 语言 的 如果 您 
好奇 那么 让 我们 开始 使用 深度 学习 技术 开发 
机器翻译 应用程序 不要 担心 我们 将 使用 哪种 类型 的 
DNN 因为 我 正在 向您/nr 详细 解释 我们 来 翻译 
一下 吧 注意 DL 需要 大量 的 计算 能力 所以 
我们 不 打算 实际 训练 模型 尽管 我 将向 您 
详细 介绍 训练 代码 但 我们 将 使用 训练 模型 
来 复制 最终 的 结果 给 你 一个 想法 谷歌 
连续 一周 使用 100 GPU 来 训练 语言 翻译 模型 
所以 我们 通过 代码 理解 概念 使用 一个 已经 训练过 
的 模型 并 看到 结果 9 . 7.1 机器翻译 机器翻译 
MT 是 NLU 领域 中 一个 广泛 应用 的 领域 
研究 人员 和 科技 巨头 们 正在 进行 大量 的 
实验 以 制造 一个 可以 翻译 任何 语言 的 单一 
机器翻译 系统 这种 机器翻译 系统 被 称为 通用 机器翻译 系统 
因此 我们 的 长期 目标 是 建立 一个 单一 的 
机器 翻译 系统 可以 把 英语 翻译 成 德语 同样 
的 机器 翻译 系统 也 应该 把 英语 翻译 成 
法语 我们 正 试图 建立 一个 能 帮助 我们 翻译 
任何 语言 的 系统 让 我们 来 谈谈 迄今为止 研究 
人员 为 建立 一个 通用 的 机器 翻译 系统 所做 
的 努力 和 实验 1954年 进行 了 第一 次 机器翻译 
演示 翻译 了 250个 俄语 和 英语 单词 这 是 
一种 基于 词典 的 方法 这种方法 使用 源语言 和 目标 
语言 的 单词 映射 在 这里 翻译 是 逐字 进行 
的 它 不能 捕获 句法 信息 这 意味着 准确性 不好 
下 一个 版本 是 中间 语言 它 使用 源语言 并 
生成 中间 语言 来 编码 和 表示 源语言 语法 语法 
等 的 特定 规则 然后 从 中间 语言 生成 目标语言 
与 第一 种 方法 相比 这种方法 很好 但 很快 就 
被 统计 机器翻译 SMT 技术 所 取代 IBM 使用 了 
这种 SMT 方法 他们 将 文本 分成 若干段 然后 将其 
与 对齐 的 双语 语料库 进行 比较 之后 使用 统计 
技术 和 概率 选择 最 有可能 的 翻译 世界 上 
使用 最多 的 SMT 是 Google 翻译 最近 Google 发表 
了 一篇 论文 指出 他们 的 机器 翻译 系统 使用 
深度 学习 来 产生 巨大 的 效果 我们 正在 使用 
TensorFlow 库 这 是 一个 由 Google 提供 的 用于 
深度 学习 的 开源 库 我们 将 通过 编码 了解 
如何 使用 深度 学习 进行 机器翻译 我们 使用 电影字幕 作为 
数据集 此 数据集 包括 德语 和 英语 我们 正在 建立 
一个 模型 将 德语 翻译 成 英语 反之亦然 您 可以 
从 http / / opus . lingfil . uu . 
se / opensubitles . php 下载 数据 这里 我 使用 
的 是 pickle 格式 的 数据 使用 pickle 依赖于 python 
我们 可以 序列化 数据集 首先 我们 使用 的 是 用于 
记住 长期 和 短期 依赖 关系 的 LSTMS 网络 我们 
正在 使用 TensorFlow 的 内置 数据 实用程序 类 对 数据 
进行 预处理 然后 我们 需要 定义 训练 模型 的 词汇 
大小 这里 我们 的 数据集 有 一个 很小 的 词汇 
大小 所以 我们 考虑 数据 集中 的 所有 单词 但是 
我们 定义 词汇 词汇 大小 比如 30000个 单词 也 就是说 
一个 小 的 训练 数据集 我们 将 使用 data _ 
utils 类 从 数据 目录 中 读取数据 这个 类 提供 
两 种 语言 的 标记 化 和 格式化 单词 然后 
我们 定义 TensorFlow 的 占位符 它 是 输入 的 编码器 
和 解码器 这 两者 都是/nr 表示 离散 值 的 整数 
张量 它们 被 嵌入 到 密集 的 表示 中 我们 
将 把 词汇 输入 编码器 并将 学习 到 的 编码表示 
输入 解码器 以下 非 原 书 内容 请 参考 Clone 
the GitHub Repository https / / github . com / 
deep diver / EN FR MLT tensorflowMLT EN to FR 
TensorFlowIn this project I am going to build language translation 
model called seq2seq model or encoder decoder model in TensorFlow 
. The objective of the model is translating English sentences 
to French sentences . I am going to show the 
detailed steps and they will answer to the questions like 
how to preprocess the dataset how to define inputs how 
to define encoder model how to define decoder model how 
to build the entire seq2seq model how to calculate the 
loss and clip gradients and how to train and get 
prediction . Please open the IPython notebook file to see 
the full workflow and detailed descriptions . This is a 
part of Udacity s Deep Learning Nanodegree . Some codes 
/ functions save load measuring accuracy etc are provided by 
Udacity . However majority part is implemented by myself along 
with much richer explanations and references on each section . 
You can find only the model part explained in my 
medium post . https / / medium . com / 
@ parkchansung / seq2seq model in tensorflow e c 0 
c 5 5 7 e 5 6 0 f B 
r i e f Overview of the ContentsData preprocessingIn this 
section you will see how to get the data how 
to create lookup table and how to convert raw text 
to index based array with the lookup table . Build 
modelIn short this section will show how to define the 
Seq2Seq model in TensorFlow . The below steps implementation will 
be covered . 1 define input parameters to the encoder 
modelenc _ dec _ model _ inputs 2 build encoder 
modelencoding _ layer 3 define input parameters to the decoder 
modelenc _ dec _ model _ inputs process _ decoder 
_ input decoding _ layer 4 build decoder model for 
trainingdecoding _ layer _ train 5 build decoder model for 
i n f e r e n c e d 
e c o d i n g _ layer _ 
infer 6 put 4 and 5 togetherdecoding _ layer 7 
connect encoder and decoder modelsseq2seq _ model 8 train and 
estimate loss and a c c u r a c 
y T r a i n i n g T 
h i s section is about putting previously defined functions 
together to build an actual instance of the model . 
Furthermore it will show how to define cost function how 
to apply optimizer to the cost function and how to 
modify the value of the gradients in the TensorFlow s 
optimizer module to perform gradient clipping . P r e 
d i c t i o n N o t 
h i n g special but showing the prediction result 
. 9.8 深度 学习 技术 和 NLG 在 本节 中 
我们 将 为 NLG 构建 一个 非常 简单 但 直观 
的 应用 程序 我们 将 从 shot 文章生成 一行 摘要 
我们 将 在 本节 中 看到 有关 汇总 的 所有 
详细信息 这个 应用 程序 花费 了 大量 的 训练 时间 
所以 您 可以 将 您 的 模型 放到 CPU 上 
进行 训练 同时 您 还 可以 执行 一些 其他 任务 
如果 你 没有 其他 任务 我 给 你 一个 9 
. 8.1 练习 试着 通过 提供 一些 起始 字符 序列 
来 找出 如何 生成 维基百科 文章 别误会 我 我 是 
认真 的 你 真的 需要 好好 想想 这是 您 可以 
使用 的 数据集 https / / einstein . ai / 
research / the wikitext long term dependency language modelinG 数据集 
跳转 到 下载 部分 下载 这个 名为 下载 wikitext 103字 
级 181MB 的 数据 集 提示 请参阅 此 链接 https 
/ / github . com / kumikokashii / lstm 文本 
生成器 别担心 在 理解 了 总结 的 概念 之后 你 
可以 尝试 一下 让 我们 开始 总结 之旅 吧 9 
. 8.2 菜谱 摘 要和 标题 生成 在 开始 编写 
代码 之前 我 想 简单 介绍 一下 总结 的 背景 
知识 架构 和 其他 技术 部分 将 被 理解 为 
我们 的 代码 语义 在 NLP 中 是 一个 非常 
重要 的 问题 随着 数据 在 文本 密 度上 的 
增加 信息 也 会 增加 现在 你 周围 的 人 
真的 希望 你 能在 短 时间 内 有效 地 说出 
最 重要 的 事情 文本 总结 始于 90 年代 加拿大 
政府 建立 了 一个 名为 天气预报 生成器 FOG 的 系统 
该 系统 使用 天气预报 数据 并 生成 摘要 这是 基于 
模板 的 方法 机器 只 需要 填写 某些 值 让 
我 举个 例子 星期六 是 晴天 有 10% 的 机会 
下雨 单词 阳光 和 10% 实际上 是由 雾 产生 的 
其他 领域 包括 金融 医疗 等 近年来 医生 发现 总结 
病人 的 病史 非常 有用 能够 有效地 诊断 病人 总结 
有 两种 类型 如下 所示 E x t r a 
c t i v e A b s t r 
a c t i v e 过去 的 大多数 摘要 
工具 都是 抽取式 的 它们 从 文章 中 选择 现有 
的 一组 单词 来 创建 文章 摘要 作为 人类 我们 
做 更多 的 事情 也 就是说 当 我们 总结 时 
我们 构建 了 一个 我们 所 读 内容 的 内部 
语义 表示 使用 这种 内部 语义 表达 我们 可以 对 
文本 进行 总结 这种 总结 称为 抽象 总结 因此 让 
我们 使用 KERA 构建 一个 抽象 的 总结 工具 Keras 
是 TensorFlow 和 Theano 的 高级 包装 这个 例子 需要 
多个 GPU 超过 12 小时 如果 你 想在 你 的 
终端 重现 结果 那么 它 需要 大量 的 计算 能力 
1 . Clone the GitHub Repository https / / github 
. com / jalajthanaki / recipe summarization2 . Initialized submodules 
git submodule update init recursive3 . Go inside the folder 
python src / config . p4 . Install dependencies pip 
install r requirements . txt5 . Set up directories python 
src / config . py6 . Scrape recipes from the 
web or use the existing one at this link wget 
P recipe box / data https / / storage . 
googleapis . com / recipebox / recipes _ raw . 
zip unzip recipe box / data / recipes _ raw 
. zip d recipebox / data7 . Tokenize the data 
python src / tokenize _ recipes . py8 . Initialize 
word embeddings with GloVe vectors Get the GloVe vectors trained 
model wget P data http / / nlp . stanford 
. edu / data / glove . 6B . zip 
unzip data / glove . 6B . zip d dataInitialize 
embeddings python src / vocabulary embedding . pyTrain the model 
python src / train _ seq2seq . pyMake predictions use 
src / predict . ipynb 在 这里 对于 矢量化 我们 
使用 Glove 是因为 我们 需要 一个 用于 总结 的 单词 
的 全局 级 表示 并且 我们 使用 Sequence to Sequence 
模型 seq2seq 模型 来 训练 我们 的 数据 seq2seq 与 
我们 在 机器 翻译 部分 讨论 的 模型 相同 我 
知道 总结 示例 需要 大量 的 计算 能力 并且 可能 
会 出现 本地 计算机 没有 足够 内存 RAM 来 运行 
此 代码 的 情况 在 这种 情况 下 不用 担心 
您 可以 使用 各种 云 选项 您 可以 使用 Google 
Cloud Amazon Web Services AWS 或 任何 其他 服务 现在 
您 对 NLU 和 NLG 应用程序 有了 足够 的 了解 
我 还 在此 Github 链接 上 放置 了 一个 与 
NLG 域 相关 的 应用 程序 https / / github 
. com / tensorflow / models / tree / master 
/ im2xt 此 应用程序 生成 图像 标题 这 是 一种 
计算机 视觉 和 NLG 的 组合 应用程序 Github 上 有 
必要 的 详细 信息 所以 也 可以 查看 这个 例子 
在下 一节 中 我们 将 看到 基于 梯度 下降 的 
优化 策略 TensorFlow 为 我们 提供 了 梯度 下降 算法 
的 一些 变体 一旦 我们 了解 了 所有 这些 变体 
是 如何 工作 的 以及 它们 各自 的 缺点 和 
优势 是 什么 那么 我们 就 可以 很容易 地 为 
我们 的 dl 算法 的 优化 选择 最佳 选项 让 
我们 来 了解 基于 梯度 下降 的 优化 9.9 基于 
梯度 下降 的 优化 在 本节 中 我们 将 讨论 
TensorFlow 提供 的 基于 梯度 下降 的 优化 选项 最初 
您 不 清楚 应该 使用 哪种 优化 选项 但是 当 
您 了解 DL 算法 的 实际 逻辑 时 它 会 
变得 更加 清晰 我们 使用 基于 梯度 下降 的 方法 
来 开发 一个 智能系统 使用 此 算法 机器 可以 学习 
如何 从 数据 中 识别模式 在 这里 我们 的 最终 
目标 是 获得 局部 最小值 目标函数 是 机器 将要 做出 
的 最终 预测 或 机器 生成 的 结果 在 基于 
梯度 下降 的 算法 中 我们 并 不是 集中 在 
如何 在 第一 步 实现 目标 函数 的 最佳 最终 
目标 上 而是 通过 迭代 或 反复 的 小 步骤 
选择 中间 的 最佳 选项 使 我们 获得 最终 的 
最佳 选项 即 我们 的 局部 极小值 这种/r 有/v 教育/vn 
意义/n 的/uj 猜测/vn 和/c 检验/vn 方法/n 很好/i 地/uv 获得/v 了/ul 
局部/n 极小值/n 当 dl 算法 得到 局部 极小值 时 可以 
得到 最佳 的 结果 我们 已经 看到 了 基本 的 
梯度 下降 算法 如果 面临 过拟合 和欠/nr 拟合 的 情况 
可以 使用 不同 类型 的 梯度 下降 来 优化 算法 
有 各种各样 的 梯度 下降 方式 可以 帮助 我们 生成 
理想 的 局部 极小值 控制算法 的 方差 更新 我们 的 
参数 并 引导 我们 收敛 我们 的 ML 或 DL 
算法 让 我们 举 个 例子 如果 函数 y = 
x2 那么 给定 函数 的 偏 导数 是 2x 当 
我们 随机 猜测 状态值 时 我们 从x=/nr 3 开始 然后 
y = 2 3 = 6 为了 得到 局部 极小值 
我们 需要 朝 负 方向 迈出 一步 所以 y = 
6 在 第一 次 迭代 之后 如果 你 猜到 x 
= 2.3 那么 y = 2 2.3 = 4.6 我们 
需要 再次 向负/nr 方向 移动 y = 4.6 因为 我们 
得到 了 一个 正值 如果 我们 得到 一个 负值 那么 
我们 就 朝着 正值 移动 经过 一定 的 迭代 y 
的 值 非常 接近 于零/nr 这 就是 我们 的 局部 
极小值 现在 我们 从 基本 梯度 下降 开始 让 我们 
开始 探索 各种 梯度 下降 9 . 9.1 基本 梯度 
下降 在 基本 梯度 下降 中 我们 根据 整个 训练 
数据集 中 存在 的 参数 计算 损失 梯度 函数 我们 
需要 计算 整个 数据 集中 的 梯度 来 执行 单个 
更新 对于 单个 更新 我们 需要 考虑 整个 训练 数据集 
以及 所有 参数 因此 速度 非常 慢 如 . 58 
所示 您 可以 在 . 59 中找到 用于 理解 目的 
的 示例 逻辑 代码 由于 该 方法 速度 较慢 我们 
将 引入 一种 新的 技术 称为 随机 梯度 下降 9 
. 9.2 随机 梯度 下降 在 这种 技术 中 我们 
更新 每个 训练 示例 和 标签 的 参数 因此 我们 
只 需要 为 我们 的 训练 数据集 添加 一个 循环 
这种方法 更新 参数 的 速度 比 基本 梯度 下降 更快 
如 . 60 所示 您 可以 在 . 61 中找到 
用于 理解 目的 的 示例 逻辑 代码 这种 方法 也 
有 一些 问题 这种方法 使得 收敛 复杂 有时 参数 更新 
太快 该算 法 可以 超越 局部 极小值 并 保持 运行 
为了 避免 这个 问题 另一种 方法 被 称为 小 批量 
梯度 下降 9 . 9.3 小批量 梯度 下降 在 这种 
方法 中 我们/r 将/d 从/p 基本/n 梯度/n 下降/v 和/c 随机/d 
梯度/n 下降/v 两/m 个/q 方面/n 得到/v 最好/a 的/uj 部分/n 我们 
将 把 训练 数据集 的 一个 子集 作为 批处理 并从 
中 更新 参数 这种 梯度 下降 用于 神经 网络 的 
基本 类型 如 . 62 所示 您 可以 在 . 
63 中找到 用于 理解 目的 的 示例 逻辑 代码 如果 
我们 有 一个 高维 数据集 那么 我们 可以 使用 其他 
的 梯度 下降 方法 让 我们 从 动量 开始 9 
. 9.4 动量 如果 所有 可能 的 参数值 的 曲面 
曲线 在 一个 维度 上 比 在 另一个 维 度上 
陡 得多 那么 在 这种 情况 下 这在 局部 最优 
中 非常 常见 在 这些 情况 下 SGD 在 斜坡 
上 振荡 为了 解决 这个 振荡 问题 我们 将 使用 
动 量法 如 . 64 所示 如果 你 看到 这个 
方程 我们/r 将/d 从上/i 一个时间/i 步到/nr 当前/t 步的/nr 梯度/n 方向/n 
的/uj 一个/m 分数/n 相加/v 然后 我们 在 正确 的 方向 
放大 参数 更新 从而 加快 收敛 速度 并 减少 振荡 
在 这里 动量 的 概念 与 物理学 中 动量 的 
概念 相似 当 获得 局部 极小值 时 这种 变化 不会 
减慢 因为 此时 动量 很高 在 这种 情况 下 我们 
的 算法 可以 完全 忽略 局部 极小值 而 这个 问题 
可以 通过 Nesterov 加速 梯度 来 解决 9 . 9.5 
Nesterov 加速 梯度 这种 方法 是 Yurii Nesterov 发明 的 
他 试图 解决 动量 技术 中 出现 的 问题 他 
发表 了 一篇 论文 你 可以 在 这个 链接 上 
看到 你 可以 在 . 65 中 看到 等式 正如 
你 所 看到 的 我们 做 的 计算 和 动量 
的 计算 是 一样 的 但是 我们 改变 了 计算 
的 顺序 在 动量 中 我们 计算 梯度 使 动量 
放大 的 那个 方 向上 的 跳跃 而在 Nesterov 加速 
梯度 法中/nr 我们 首先 根据 先前 的 动量 进行 跳跃 
然后 计算 梯度 然后 添加 修正 并 生成 参数 的 
最终 更新 这 有助于 我们 更 动态地 提供 参数值 9 
. 9.6 AdagradAdagrad 代表 自适应 梯度 该 方法 允许 学习率 
根据 参数 进行 调整 该 算法 对不 常用 参数 进行 
大 更新 对 常用 参数 进行 小 更新 如 . 
66 所示 该 方法 根据 对 该 参数 计算 的 
过去 梯度 为 给定 时间戳 的 每个 参数 提供 不同 
的 学习 速率 在 这里 我们 不 需要 手动 调整 
学习 速度 尽管 它 有限制 根据 方程 学习率 总是 随着 
分母 中 平方 梯度 的 累积 总是 正 的 而 
下降 并且 随着 分母 的 增长 整个 学期 都会 下降 
有时 学习率 变得 非常 小 以至于 ML 模型 停止 学习 
解决 这个 问题 图 中 出现 了 名为 adadelta 的 
方法 9 . 9.7 adadeltaadadelta 是 adagrad 的 扩展 在 
Adagrad 中 我们 不断 地 将 平方根 加到 和上/nr 导致 
学习 率 下降 我们 没有 求和 所有 过去 的 平方根 
而是 将 窗口 限制 为 一个 固定 大小 的 累积 
过去 梯度 您 可以 看到 . 67 中的 公式 Alt 
https / / img blog . csdnimg . cn / 
2 0 1 9 0 2 0 4 0 7 
3 5 0 1 6 5 8 . png 正如 
你 在 方程 中 看到 的 我们 将 使用 梯度 
之和 作为 所有 过去 平方 梯度 的 衰减 平均值 在 
这里 给定 时间戳 的 运行 平均值 e g2 t 取决于 
以前 的 平均值 和 当前 的 梯度 在 看过 所有 
的 优化 技术 之后 你 知道 我们 如何 计算 每个 
参数 的 单个 学习率 如何 计算 动量 以及 如何 防止 
学习率 的 衰减 尽管如此 通过 应用 一些 自适应 动量 还有 
改进 的 空间 这将 引导 我们 找到 最终 的 优化 
方法 Adam 9 . 9.8 AdamAdam 代表 自适应 动量 估计 
当 我们 计算 每个 参数 的 学习 速率 时 我们 
也 可以 分别 存储 每个 参数 的 动量 变化 您 
可以 看到 . 68 中的 方程式 Alt https / / 
img blog . csdnimg . cn / 2 0 1 
9 0 2 0 4 0 7 3 5 2 
6 3 5 0 . png 首先 我们 将 计算 
梯度 的 平均值 然后 我们 将 计算 梯度 的 非 
中心 方差 并 使用 这些 值 更新 参数 就像 一个 
三角洲 你 可以 在 . 69 中 看到 Adam 的 
方程 Alt https / / img blog . csdnimg . 
cn / 2 0 1 9 0 2 0 4 
0 7 3 5 4 7 8 3 0 . 
png 所以 现在 你 想 知道 我们 应该 使用 哪种 
方法 根据 我 的 说法 亚当 是 最好 的 整体 
选择 因为 它 优于 其他 方法 您 也 可以 使用 
adadelta 和 adagrad 如果 您 的 数据 是 稀疏 的 
那么 您 不 应该 使用 sgd momentum 或 nesterov 9.10 
人工智能 与 人类 智能 从 过去 的 一年 开始 你 
可能 听到 过 这种 问题 在 人工智能 领域 这类 问题 
已经 变得 普遍 人们 大肆宣传 人工智能 将使 人类 消失 机器 
将 夺走 我们 所有 的 力量 现在 让 我 告诉 
你 这 不是 事实 这些 威胁 听 起来 像 科幻小说 
据我所知 人工智能 正处于 高速 发展 阶段 但 其 目的 是 
为了 补充 人类 使 人类 生活 更 容易 我们 仍 
在 研究 宇宙 中 一些 复杂 和 未知 的 真理 
这些 真理 可以 帮助 我们 对 如何 构建 辅助 系统 
提供 更多 的 见解 所以 人工智能 将 完全 帮助 我们 
人工智能 肯定 会 让 我们 的 生活 大吃一惊 但 它 
不会 很快 被 它 的 发明 所 淹没 因此 享受 
这个 人工智能 阶段 以 积极 的 方式 为 人工智能 生态系统 
做出 贡献 人们 担心 人工智能 会 夺走 我们 的 工作 
它 不会 夺走 你 的 工作 这 会使 你 的 
工作 更 容易 如果 你 是 一名 医生 想 在 
一些 癌症 报告 上 发表 最后 一句话 人工智能 将 帮助 
你 在 信息 技术 IT 行业 有 一种 担忧 即 
人工智能 将 取代 编码器 如果 你 相信 研究/vn 人员/n 和/c 
科技/n 公司/n 很快/d 就/d 能/v 制造/v 出/v 比/p 人类/n 更/d 
强大/a 的/uj 机器/n 而且 人工智能 的 转变 很快 就 会 
发生 机器 将 夺走 我们 的 工作 那么 你 最好 
能 获得 ML DL 和 AI 相关 技能 集 来 
工作 也许 你 是 这个 星球 上 最后 一个 拥有 
的 人 我 要做 的 工作 我们 假设 人工智能 会 
夺走 一些 就业 机会 但 这种 人工智能 生态 系统 也 
会 创造 许多 新的 就业 机会 所以 别担心 这个 讨论 
可以 继续 进行 但 我 想 给 你们 一些 时间 
来 思考 这个 问题 9.11 总结 恭喜 你们 我们 已经 
读 到最后 一章 了 我 非常 感谢 你 的 努力 
在 这 一章 中 你 学到 了 很多 东西 比如 
人工智能 方面 它们 帮助 你 理解 为什么 深度 学习 是 
当今 的 流行 词 我们 已经 看到 了 人工 神经 
网络 的 概念 我们 已经 看到 了 诸如 梯度 下降 
各种 激活 函数 和 损失 函数 等 概念 我们 已经 
看到 了 dnn 和 dl 生命 周期 的 结构 我们 
还 讨论 了 sequence to sequence 模型 的 基础 知识 
并 开发 了 机器 翻译 标题 生成 和 摘要 等 
应用 程序 我们 还 看到 了 基于 梯度 下降 的 
优化 技术 接下来 的 部分 是 附录 A 到 C 
它 将向 您 提供 有关 框架 如 Hadoop Spark 等 
的 概述 您 还 可以 看到 这些 框架 以及 其他 
工具 和库的/nr 安装 指南 除此之外 如果 您 不熟悉 Python 您 
还 可以 找到 许多 Python 库 的 备忘 表 这些 
备忘 表 非常 方便 如果 你 真的 想 提高 你 
的 数据 科学 和 NLP 技能 我 有 一些 建议 
我 还在 附录 中 提供 了 Gitter 链接 您 可以 
使用 它 与 我 联系 以防 您 有 任何 问题 
致谢 Python 自然语言 处理 1 2 3 作者 印 雅兰 
萨 纳卡 Jalaj Thanaki 是 实践性 很强 的 一部 新作 
为 进一步 深入 理解 书中 内容 对 部分 内容 进行 
了 延伸 学习 练习 在此 分享 期待 对 大家 有所 
帮助 欢迎 加 我 微信 验证 NLP 一起 学习 讨论 
不足之处 欢迎 指正 参考文献 https / / github . com 
/ jalajthanaki ↩ ︎ Python 自然语言 处理 印 雅兰 萨 
纳卡 Jalaj Thanaki 著 张金超/nr 刘 舒曼 等 译 机械 
工业 出版社 2018 ↩ ︎ Jalaj Thanaki Python Natural Language 
Processing 2017 ↩ ︎ from http / / www . 
datalearner . com / blog / 1051509699533080 简介 现在 很多 
公司 和 组织 每 天都 要 处理 大量 的 文本 
信息 包括 邮件 评论 客户 的 电话 等 将 这些 
数据 变成 有用 的 信息 需要 花费 大量 的 时间 
抽取 这些 信息 的 一个 核心 的 技能 就 是 
自然 语言 处理 Natural Language Processing NLP 自然语言 处理 在 
现阶段 变得 越来越 重要 不管 你 是 做 什么 的 
这篇 博 客都 能给 你 一点 帮助 为何 写 这篇 
博客 作者 已经 在 NLP 领域 做 过 一段 时间 
工作 了 作者 遇到 过 很多 种 情况 需要 从 
各种 资料 如 最新 的 论文 博客 以及 一些 自然 
语言 处理 的 任务 中 获得 帮助 因此 作者 希望 
将 这些 资源 都 写到 一起 来 提供 一站式 的 
帮助 下面 就是 自然语言 处理 有关 的 一些 资源 一 
词干 抽取 Stemming 词干 抽取 是 去除 词语 的 形态 
等 将 词语 变成 它 的 词干 形式 的 过程 
它 的 目的 是 将 不同 形式 的 相同 含义 
的 词语 统一 起来 数据 学习 网站 提醒 一下 中文 
中 一般 没有 词干 抽取 的 工作 但是 多了 个 
分词 这是 中英文 文本处理 的 不同 例如 有 下面 两个 
例子 beautiful and beautifully are stemmed to beautigood better and 
best are stemmed to good better and best respectively 这是 
来自于 论文 的 例子 原文 https / / tartarus . 
org / martin / PorterStemmer / def . txt 使用 
Python 处理 词干 抽取 https / / bitbucket . org 
/ mchaput / stemming / src / 5 c 2 
4 2 a a 5 9 2 a 6 d 
4 f 0 e 9 a 0 b 2 e 
1 a f d c a 4 f d 7 
5 7 b 8 e 8 a / stemming / 
porter2 . py at = default & fileviewer = file 
view default 比如 我们 可以 用 Porter2 算法 来 实现 
词干 抽取 # pip install stemmingfrom stemming . porter2 import 
stemstem casually 二 词形 还原 Lemmatisation 词形 还 原是 指 
将 一组 词语 变成 他们 词干 的 形式 的 过程 
例如 在 会话 识别 任务 中 我们 需要 考虑 这个 
单词 在 句子 中 的 含义 也要 考虑 这个 单词 
在 相邻 句子 中 的 含义 例如 beautiful and beautifully 
are lemmatised to beautiful and beautifully respectively . good better 
and best are lemmatised to good good and good respectively 
. 有两 篇 论文 讨论 了 词形 还原 论文 1 
http / / www . ijrat . org / downloads 
/ icatest2015 / ICATEST 2015127 . pdf 论文 2 https 
/ / academic . oup . com / dsh / 
article abstract / doi / 10.1093 / llc / fqw034 
/ 2669790 / Lemmatization for variation rich languages using 数据集 
https / / catalog . ldc . upenn . edu 
/ ldc99t42 在 Python 中 可以 使用 Spacy 来做 词形 
还原 # pip install spacy # python m spacy download 
enimport spacynlp = spacy . load en doc = good 
better best for token in nlp doc print token token 
. lemma _ 三 词 嵌套 Word Embeddings 词 嵌套 
是 一种 技术 它 可以 将 自然 语言 变成 实值 
向量 的 形式 由于 计算 机 无法 直接 处理 文本 
所以 这种 转换 很 有用 这类 技术 使用 实值 向量 
来 表示 自然 语言 中 词语 的 位置 和 相互 
关系 数据 学习 网站 提醒 一下 词 嵌套 最 有名 
的 论文 应当 属于 word2vec 这篇 论文 它 并 没有 
说 提供 了 新方法 但是 提供 了 一种 新 工具 
可以 很 方便 的 从 文本 中 获取 词 向量 
的 结果 这也 是 谷歌 提 出来 的 谷歌 真是 
个 不错 的 公司 例如 我们 可以 用 100 维 
向量 表示 词语 或者 短语 例如 用 5 维 的 
向量 表示 男人 这个 词语 这里 的 每个 数字 都是 
在 特定 方 向上 的 大小 这篇 博客 详细 介绍 
了 词 嵌套 模型 这篇 论文 详细 介绍 了 词 
向量 对于 深入 理解 词 嵌套 模型 必看 这个 工具 
是 一个 基于 浏览器 的 对词 向量 进行 可视化 展示 
的 这里 是 一个 对 294种 语言 的 词 向量 
的 训练 结果 实现 方式 我们 可以 使用 Python 中 
gensim 工具 来 训练 下载 使用 谷歌 新闻 训 练好 
的 词 向量 # pip install gensimfrom gensim . models 
. keyedvectors import KeyedVectorsword _ vectors = KeyedVectors . load 
_ word2vec _ format GoogleNews vectors negative300 . bin binary 
= True word _ vectors human 用 自己 的 数据 
集 训练 词 向量 sentence = first sentence second sentence 
model = gensim . models . Word2Vec sentence min _ 
count = 1 size = 300 workers = 4 四 
词性 标注 Part Of Speech Tagging 词性 标注 就是 将 
句子 中 的 单词 标注 成 名词 动词 等 数据 
学习 网站 提醒 一下 中文 的 词性 标注 工具 可以 
使用 结巴 分词 或者 是 张 华平 分词 都是 带有 
词性 标注 的 功能 例如 句子 Ashok killed the snake 
with a stick 词性 标注 的 结果 是 Ashok 代词 
killed 动词 the DETsnake 名词 with 连词 a DETstick 名词 
. 标点符号 这篇 论文 使用 了 Dynamic Feature Induction 来 
得到 高 精度 的 词性 标注 结果 这篇 论文 使用 
Anchor Hidden Markove 模型 来 训练 无 监督 的 词性 
标注 结果 实现 我们 可以 使用 spacy 来 实现 词性 
标注 # pip install spacy # python m spacy download 
ennlp = spacy . load en sentence = Ashok killed 
the snake with a stick for token in nlp sentence 
print token token . pos _ 五 命名 实体 消 
歧 Named Entity Disambiguation 命名 实体 消 岐 是 值 
识别 句子 中 的 实体 的 过程 例如 句子 Apple 
earned a revenue of 200 Billion USD in 2016 命名 
实体 消 歧 的 目标 是 认出 Apple 是 一个 
公司 名字 而 不是 水 果名 命名 实体 一般 需要 
一个 实体 库 它 可以 将 句子 中 的 实体 
链 接到 实体 库 中 这篇 论文 使用 了 基于 
深度 神经 网络 的 Deep Semantic Relatedness 技术 来 进行 
命名 实体 消 歧 效果 不错 它 使用 了 知识库 
这篇 论文 则 利用 了 词 向量 模型 使用 Local 
Neural Attention 来 进行 命名 实体 消 歧 六 命名 
实体 识别 Named Entity Recognition 命名 实体 识别 是 要 
识别 出 句子 中 的 实体 并将 实体 划分 到 
某个 类别 中 例如 人 组织 日期 等 例如 句子 
Ram of Apple Inc . travelled to Sydney on 5th 
October 2017 返回 的 结果 是 RamofApple ORGInc . O 
R G t r a v e l l e 
d t o y d n e y GPEon5th DATEOctober 
DATE2017 DATE2017 DATEORG 表示 组织 GPE 表示 地点 目前 命名 
实体 识别 最大 的 问题 是 当 数据 变了 即使 
是 最好 的 NER 技术 也 会 表现 不好 这篇 
论文 使用 了 二 向 LSTM 模型 在 4种 语言上 
联合 了 有 监督 无 监督 的 模型 实现 了 
比较 好 的 命名 实体 识别 的 效果 我们 可以 
使用 Spacy 来 实现 这个 技术 import spacynlp = spacy 
. load en sentence = Ram of Apple Inc . 
travelled to Sydney on 5th October 2017 for token in 
nlp sentence print token token . ent _ type _ 
七 情感 分析 情感 分析 的 任务 涉及 的 主题 
较多 一般 是 利用 自然 语言 处理 技术 识别 如 
客户 评论 中 正向 或者 负向 的 情感 等 或者 
是 通过 语音 分析 写作 分析 得到 情绪 判别 结果 
例如 I did not like the chocolate ice cream – 
对 冰激淋 做 负向 的 评价 I did not hate 
the chocolate ice cream – 可能 是 一个 中立 的 
评价 情感 分析 的 方法 很多 开始 的 时候 可以 
用 LSTM 模型 与 词 向量 模型 一起 数一数 句子 
中正 负向 情感 词 的 个数 得到 资源 有 博客 
1 电影 推 文的/nr 情感 分析 博客 2 Chennai 洪水 
的 情感 分析 论文 1 使用 朴素 贝叶斯 方法 对 
IMDB 评论 的 情感 分类 . 论文 2 使用 LDA 
获得 用户 文本 的 主题 然后 基于 无 监督 的 
方法 识别 情感 . Repository 这里 总结 了 大量 的 
关于 不同 语言 的 情感 处理 的 论文 和 实现 
方法 . Dataset 1 Multi Domain sentiment dataset version 2 
. 0Dataset 2 Twitter Sentiment analysis D a t a 
s e t C o m p e t i 
t i o n A very good competition where you 
can check the performance of your models on the sentiment 
analysis task of movie reviews of rotten tomatoes . 八 
文本 语义 相似性 Semantic Text Similarity 计算 文本 语义 相似性 
就是 计算 两段 文本 之间 含义 相似性 的 任务 论文 
1 详细 说明 了 多种 计算 文本 相似性 的 方法 
必读 论文 2 介绍 如何 使用 CNN 获得 短 文本 
之间 相似性 论文 3 使用 基于 树 的 LSTM 方法 
获取 语义分类 九 语言识别 就是 识别 出 文本 是 什么 
语言 写 的 是 文本 分类 的 一种 特殊 情况 
博客 给 了 一个 新 工具 可以 识别 170 多种语言 
论文 1 讨论 了 7 种方法 识别 285种 语言 论文 
2 讨论 了 如何 使用 深度 神经网络 获得 语言识别 结果 
十 文本 摘要 Text Summarisation 文本 摘要 是 通过 识别 
文本 重要 的 内容 将 一段 文本 缩减 并 变成 
对 这些 点 的 总结 文本 摘要 的 目标 是 
最大限度 保留 原始 文本 的 含义 论文 1 使用 Neural 
Attention Model 获取 文本 摘要 论文 2 使用 sequence to 
sequence 的 RNN 获取 摘要 Repository 谷歌 大脑 团队 提供 
的 可以 自动 文本 摘要 的 代码 基于 Gigaword 数据集 
训练 应用 Reddit s autotldr bot 使用 文本 摘 要将 
帖子 的 评论 总结 成 短文 这个 特性 在 Reddit 
用户 中 很 有名 我们 可以 使用 gensim 来 获取 
文本 摘要 from gensim . summarization import s u m 
m a r i z e s e n t 
e n c e = Automatic summarization is the process 
of shortening a text document with software in order to 
create a summary with the major points of the original 
document . Technologies that can make a coherent summary take 
into account variables such as length writing style and syntax 
. Automatic data summarization is part of machine learning and 
data mining . The main idea of summarization is to 
find a subset of data which contains the information of 
the entire set . Such techniques are widely used in 
industry today . Search engines are an example others include 
summarization of documents image collections and videos . Document summarization 
tries to create a representative summary or abstract of the 
entire document by finding the most informative sentences while in 
image summarization the system finds the most representative and important 
i . e . salient images . For surveillance videos 
one might want to extract the important events from the 
uneventful context . There are two general approaches to automatic 
summarization extraction and abstraction . Extractive methods work by selecting 
a subset of existing words phrases or sentences in the 
original text to form the summary . In contrast abstractive 
methods build an internal semantic representation and then use natural 
language generation techniques to create a summary that is closer 
to what a human might express . Such a summary 
might include verbal innovations . Research to date has focused 
primarily on extractive methods which are appropriate for image collection 
summarization and video summarization . summarize sentence 自然语言 处理 NLP 
国内 研究 方向 机构 导师 文|/nr 中文 信息 协会 中文 
信息 处理 发展 报告 2016 数据 简化 DataSimp 文字 语言 
VS 数字信息 数字 文字 和 自然 语言 一样 都是 信息 
的 载体 他们 之间 原本 有着 天然 的 联系 语言/n 
和/c 数学/n 的/uj 产生/n 都是/nr 为了/p 交流/n 从 文字 数字 
和 语言 的 发展 历史 可以 了解 到 语言 文字 
和 数字 有着 内在 的 联系 自然语言 处理 NLP 主要 
涉及 三种 文本 自由 文本 结构化 文本 半 结构化 文本 
自然语言 理解 Natural Language Understanding NLU 实现 人机 间 自然语言 
通信 意味着 要使 计算机 既 能理解 自然语言 文本 的 意义 
也能 以 自然 语言 文本 表达 给定 的 意图 思想 
等 自然语言 生成 NLG 是 人工 或 机器 生成 语言 
斯坦福 自然语言 处理 NLP 工具 资料 收集 斯坦福 分词 Stanford 
中文 实体 识别 最早 做 自然语言 处理 的 网址 https 
/ / nlp . stanford . edu / software / 
segmenter . shtml 哈尔滨工业大学 智能 技术 与 自然 语言 处理 
研究室 I n t e l l i g e 
n t T e c h n o l o 
g y & Natural Language Processing Lab ITNLPLab 是 国内 
较早 从事 自然语言 处理 和 语言 智能 技术 的 研究 
室 除了 新兴 的 文本 数据 简化 领域 秦陇 纪 
数据 简化 技术中心 筹 自然语言 处理 NaturalLanguage Processing 领域 主要 
包括 基础 研究 和 应用 研究 基础 研究 词法 与 
句法分析 李正华 陈 文亮 张民 苏州大学 语义分析 周 国栋 李军辉 
苏州大学 篇章 分析 王厚 峰 李素建/nr 北京大学 语言 认知 模型 
王少楠/nr 宗 成庆 中科院 自动化 研究所 语言 表示 与 深度 
学习 黄萱菁/nr 邱锡鹏/nr 复旦 大学 知识图谱 与 计算 李 涓子 
候 磊 清华大学 应用 研究 文本 分类 与 聚 类 
涂存超/nr 刘知远 清华大学 信息 抽取 孙乐/nr 韩先培/nr 中国 科学院 软件 
研究所 情感 分析 黄民烈/nr 清华大学 自动 文摘 万 小军 姚 
金戈 北京大学 信息检索 刘奕群/nr 马少平/nr 清华大学 信息 推荐 与 过滤 
王斌 中科院 信 工所 鲁骁/nr 国家 计算机 网络 应急 中心 
自动 问答 赵军 刘康 何世柱 中科院 自动化 研究所 机器翻译 张 
家俊 宗 成庆 中科院 自动化 研究所 社会 媒体 处理 刘挺/nr 
丁效/nr 哈尔滨工业大学 语音 技术 说 话人 识别 郑方/nr 清华大学 王仁 
宇 江苏 师范大学 语音合成 陶建华 中科院 自动化 研究所 语音识别 王东 
清华大学 文字 识别 刘成林 中科院 自动化 研究所 多 模态 信息处理 
陈晓 鸥 北京大学 医疗 健康 信息 处理 陈 清财 汤步洲/nr 
哈尔滨工业大学 少数民族 语言 信息 处理 吾守尔 • 斯拉木 新疆大学   
完   https / / blog . csdn . net 
/ y H 0 V L D e 8 V 
G 8 e p 9 V G e / article 
/ details / 83747195 自然语言 处理 主要 步骤 包括 1 
. 分词 只 针对 中文 英文 等 西方 字母 语言 
已 经用 空格 做好 分词 了 将 文章 按 词组 
分开 2 . 词 法分析 对于 英文 有 词头 词根 
词尾 的 拆分 名词 动词 形容词 副词 介词 的 定性 
多种 词意 的 选择 比如 DIAMOND 有 菱形 棒球场 钻石 
3个 含义 要根据 应用 选择 正确 的 意思 3 . 
语法分析 通过 语法树 或 其他 算法 分析 主语 谓语 宾语 
定语 状语 补语 等 句子 元素 4 . 语义分析 通过 
选择 词 的 正确 含义 在 正确 句法 的 指导 
下 将 句子 的 正确 含义 表达出来 方法 主要 有 
语义 文法 格 文法 但是 以上 的 分析 仅 适用 
于小/nr 规模 的 实验室 研究 远 不能 应用 到 实际 
语言 环境 中 比如说 语法 我们 能 总结 出 的 
语法 是 有限 的 可是 日常 应用 的 句子 绝大部分 
是 不遵守 语法 的 如果 让 语法 包罗 所有 可能 
的 应用 会 出现 爆炸 的 景象 自然语言 处理 的 
应用 方向 主要 有 1 . 文本 分类 和聚类/nr 主要 
是 将 文本 按照 关键 字词 做出 统计 建造 一个 
索引库 这样 当 有关 键 字词 查询 时 可以 根据 
索引库 快速 地 找到 需要 的 内容 此 方向 是 
搜索引擎 的 基础 在 早期 的 搜索引擎 比如 北大 开发 
的 天问 系统 采用 这种 先 搜集 资料 在 后台 
做 索引 在前 台 提供 搜索 查询 服务 目前 GOOGLE 
百度 的 搜索引擎 仍旧 类似 但是 采用 了 自动 蜘蛛 
去 采集 网络 上 的 信息 自动 分类 并 做 
索引 然后再 提供 给 用户 我 曾经 在 我 的 
文章 中 做过 测试 当 文章 中有 十八禁 这样 的 
字眼 时 点击 次数 是 我 其他 文章 点击 次数 
的 几十 倍 说明 搜索 引擎 将 十八禁 这个词 列为 
热门 索引 一旦 有 一个 蜘蛛 发现 这个 词 其他 
蜘蛛 也会 爬 过来 2 . 信息检索 和 过滤 这是 
网络 瞬时 检查 的 应用 范畴 主要 为 网警 服务 
在 大 流量 的 信息 中 寻找 关键词 找到 了 
就要 做 一些 其他 的 判断 比如 报警 3 . 
信息 抽取 抄书 信息 抽取 研究 旨在 为 人们 提供 
更 有力 的 信息 获取 工具 以 应对 信息 爆炸 
带来 的 严重 挑战 与 信息检索 不同 信息 抽取 直接 
从 自然语言 文本 中 抽取 事实 信息 过去 十 多年来 
信息 抽取 逐步 发展 成为 自然 语言 处理 领域 的 
一个 重要 分支 其 独特 的 发展 轨迹 通过 系统化 
大 规模 地 定量 评测 推动 研究 向前 发展 以及 
某些 成功 启示 如 部分 分析 技术 的 有效性 快速 
自然语言 处理 系统 开发 的 必要性 都 极大 地 推动 
了 自然 语言 处理 研究 的 发展 促进 了 自然 
语言 处理 研究 与 应用 的 紧密 结合 回顾 信息 
抽取 研究 的 历史 总结 信息 抽取 研究 的 现状 
将 有助于 这 方面 研究 工作 向前 发展 4 . 
问答 系统 目前 仍 局限于 80 年代 的 专家 系统 
就是 按照 LISP 语言 的 天然 特性 做 逻辑 递归 
LISP 语言 是 括号 式 的 语言 比如 A = 
B C D A = B E F 提问 已知 
B C 能 得到 什么样 的 结论 结论 是 A 
D 若 提问 改为 已知 B 结论 则是 C D 
A 或 E F A 比如 一个 医疗 用 的 
专家 系统 你 若 询问 感冒 的 治疗 方法 系统 
可能 给 出 多种 原因 带来 的 感冒 极其 治疗 
方法 你 若 询问 病毒性 感冒 的 治疗 方法 则 
系统 会 给出 比较 单一 的 明确 的 治疗 方法 
你 有 没有 用过 AUTOCAD 系统 这个 就是 建立 在 
LISP 语言上 的 括号 系统 在用 的 时候 会 出现 
上述 情况 5 . 拼音 汉字 转换 系统 这 应该 
是 中文 输入法 应用 范畴 的 东西 再多 的 东西 
我 就 没 想过 6 . 机器翻译 当前 最 热门 
的 应用 方向 这 方面 的 文章 最多 国际 上 
已经 有 比较 好 的 应用 系统 美国 有个 AIC 
公司 推出 过 著名 的 实时 翻译 系统 欧共体 的 
SYSTRAN 系统 可以 将 英 法 德 西 意 葡 
六种 语言 实时 对 译 美 日 德 联合 开发 
的 自动 语音 翻译 系统 成功 进行 了 10 多分钟 
对话 我国 军事 科学院 中科院 也 开发 过 此类 系统 
但是 这 里边 的 问题 也 很多 最 主要 的 
是 满篇 洋文 难不住 满篇 译文 看不懂 就是 脱离 了 
人类 智慧 的 机器 翻译 总 会搞 出让 人 无法 
理解 的 翻译 比如 多意 词 选择 哪个 意思 合适 
怎么 组织 出 通顺 的 语句 等等 所以 目前 微软 
GOOGLE 的 新趋势 是 翻译 + 记忆 类似 机器学习 将 
大量 以往 正确 的 翻译 存储 下来 通过 检索 如果 
碰到 类似 的 翻译 要求 将 以往 正确 的 翻译 
结果 拿出 来用 GOOGLE 宣称 今后 几年 就 可以 推出 
商业化 的 网页 翻译 系统 7 . 新 信息 检测 
这个 我 不 知道 没 思路 以上 已经 回答 了 
自然 语言 发展 方向 的 问题 我 认为 机器 翻译 
是 最有 前途 的 方向 其 难点 在于 机器翻译 还 
不具备 人类 智能 虽然 翻译 已经 达到 90% 以上 的 
正确 程度 然而 还是 不能 象 人类 翻译 那样 可以 
准确 表达 为什么 存在 这样 的 难点 关键 是 自然 
语言 处理 做不到 人类 对 自然 语言 的 理解 处理 
和 理解 是 天差地别 的 两个 概念 处理 好比 控制 
眼睛 耳朵 舌头 的 神经 他们 将 接收 的 信息 
转化成 大脑 可以 理解 的 内部 信息 或者 反过来 他们 
的 功能 就是 这么 多 而 理解 则是 大脑皮层 负责 
语言 理解 那 部分 多少 亿 的 脑细胞 共同 完成 
的 功能 一个 人 因为 其 自身 家庭 背景 受 
教育 程度 接触 现实 中 长期 形成 的 条件反射 刺激 
特殊 的 强列/nr 刺激 当时 的 心理 状况 这么 多 
的 因素 都会 影响 和 改变 理解 的 功能 比如 
我 说 一个 靓女 开着 BMW 跑车 有人 心里 会想 
这是 二奶 吧 有人 心里 会 仇视 她 联想 到她 
会 撞了人 白撞 做 汽车 买卖 的 人 则 会去 
估量 这部 车的/nr 价值 爱 攀比 的 人 也许 会想 
我 什么 时候 才能 开上 BWM 所以 理解 是 更加 
深奥 的 东西 涉及 更多 神经学 心理学 逻辑学 领域 还有 
上下文 理解 问题 比如 这句 我们 90 平方米 以后 会占 
的 分量 越来越 大 那么 这样 他 的 价格 本身 
比 高档 低 很多 所以 对于 整体 把 这个 价格 
水平 给 压 下来 了 这个 确实 非常 好 的 
你 能理解 么 估计 很难 或者 理解 出 多种 意思 
但是 我 把 前文 写出来 去年 国家 九 部委 联合 
发布 了 建设部 等 部门 关于 调整 住房 供应 结构 
稳定 住房价格 意见 的 通知 对 90 平方米 以下 住房 
须占/nr 总面积 的 70% 以上 作出 了 硬性 规定 深圳市 
经过 一年 的 调控 目前 已 做到 每个 项目 的 
75% 都是 90 平方米 以内 深圳市 国土 资源 和 房产 
管理 局 官员 说 看了 后面 的 你 才能 知道 
是 根据 国家 的 通知 深圳 做了 相应 的 调整 
解决 的 大体 思路 很多 自然语言 处理 目前 还 处于 
探索 阶段 还 根本 没有 好 的 解决 办法 推荐 
几个 网站 1 . 中国科学院计算技术研究所 数字化 室 ＆ 软件 室 
http / / www . nlp . org . cn 
/ 2 . 北大 计算 语言所 http / / icl 
. pku . edu . cn / 3 . 麻省理工 
人工智能 实验室 http / / www . csail . mit 
. edu / index . php 转载自 https / / 
blog . csdn . net / riverflowrand / article / 
details / 51355238 * * * * * * * 
* * * * * * * * * * 
* * * * * * * * 精华 总结 
时间 不够 只看 这个 部分 就行了 1 . 书 和课/nr 
Michael Collins COMS W4705 Natural Language Processing Spring 2015 重要 
Jason Eisner 的 Lecture Notes 600.465 Natural Language Processingdan jurasfsky 
Speech and Language Processing 3rd ed . draft   https 
/ / web . stanford . edu / ~ jurafsky 
/ slp3 / 中文名 译为 自然语言 处理 综论 重要 Wardhaugh 
Ronald An introduction to sociolinguistics 宗 成庆 统计 自然语言 处理 
Yoav Goldberg Neural Network Methods for Natural Language Processing 数学 
之美 吴军 科普 且 生动 形象 入门 必备 统计 学习 
方法 李航 Stanford nlp 公开课 cs224nJacob Eisenstein Natural Language Processing 
中文信息处理 报告 2016 Chris Manning 和 Dan jurafsky 两尊 大神 
的 至尊 课程 introduction to natural language p r o 
c e s s i n g z h e 
n g x i a n g zhai Text Mining 
and Analytics https / / zh . coursera . org 
/ learn / text mining2 . 代码 哈工大 开源 的 
那个 工具包 LTP Language Technology Platform pattern simpler to get 
started than NLTKchardet character encoding d e t e c 
t i o n p y e n c h 
a n t easy access to d i c t 
i o n a r i e s s c 
i k i t learn has support for text c 
l a s s i f i c a t 
i o n u n i d e c o 
d e because ascii is much easier to deal withWord2VecCRF 
+ + G l u o n N L P 
T e n s o r f l o w 
. T e n s o r 2 T e 
n s o r g e n s i m 
P y T o r c h F a s 
t t e x t M M X n e 
t n l t k T e x t B 
l o b s p a C y G e 
n s i m j i e b a k 
e r a s sequence to sequence learning3 . 学术会议 
* * * ACL 年会 * * EMNLP Conference on 
Empirical Methods on Natural Language Processing * * CoNLL Conference 
on Natural Language Learning * * COLING International Conference on 
Computational Linguistics * * ACL Anthology ACL 学会 网站 建立 
了 称作 ACL Anthology 的 页面 URL ACL Anthology 支持 
该 领域 绝大部分 国际 学术 会议 论文 的 免费 下载 
4 . 期刊 * * * Computational Linguistics * * 
Transactions of ACL * * ACM Transactions on Speech and 
Language Processing * * ACM Transactions on Asian Language Information 
Processing * * Journal of Quantitative LinguisticsACL EMNLP NAACL COLING 
LREC Computational Linguistics 位于 前 5位 当 需要 了解 某 
个 领域 如果 能 找到 一篇 该 领域 的 最新 
研究 综述 最 方便 的 方法 还是 在 Google Scholar 
中 搜索 领域 名称 + survey / review / tutorial 
/ 综述 来 查找 6 . 网站 Kaggle 情感 分析题 
* * * * * * * * * * 
* * * * * * * * * * 
* * * * * 所有 资料 都 来自 知乎 
序号 加了 方括号 重点 关注 1 . https / / 
www . zhihu . com / question / 19895141 / 
answer / 149475410 自然语言 处理 简称 NLP 是 研究 计算机 
处理 人类 语言 的 一门 技术 包括 1 . 句法 
语义分析 对于 给定 的 句子 进行 分词 词性 标记 命名 
实体 识别 和 链接 句法分析 语义 角色 识别 和 多义词 
消 歧 2 . 信息 抽取 从 给定 文本 中 
抽取 重要 的 信息 比如 时间 地点 人物 事件 原因 
结果 数字 日期 货币 专有名词 等等 通俗 说来 就是 要 
了解 谁在 什么 时候 什么原因 对谁 做 了 什么 事 
有 什么 结果 涉及 到 实体 识别 时间 抽取 因果关系 
抽取 等 关键 技术 3 . 文本 挖掘 或者 文本 
数据挖掘 包括 文本 聚 类 分类 信息 抽取 摘要 情感 
分析 以及 对 挖掘 的 信息 和 知识 的 可视化 
交互式 的 表达 界面 目前 主流 的 技术 都是/nr 基于 
统计 机器 学习 的 4 . 机器翻译 把 输入 的 
源语言 文本 通过 自动 翻译 获得 另外 一种 语言 的 
文本 根据 输入 媒介 不同 可以 细分 为 文本 翻译 
语音 翻译 手语 翻译 图形 翻译 等 机器翻译 从 最早 
的 基于 规则 的 方法 到 二十年前 的 基于 统计 
的 方法 再到 今天 的 基于 神经网络 编码 解码 的 
方法 逐渐 形成 了 一套 比较 严谨 的 方法 体系 
5 . 信息检索 对 大 规模 的 文档 进行 索引 
可 简单 对 文档 中 的 词汇 赋 之以 不同 
的 权重 来 建立 索引 也 可利用 1 2 3 
的 技术 来 建立 更加 深层 的 索引 在 查询 
的 时候 对 输入 的 查询 表达式 比如 一个 检索 
词 或者 一个 句子 进行 分析 然后 在 索引 里面 
查找 匹配 的 候选 文档 再 根据 一个 排序 机制 
把 候选 文档 排序 最后 输出 排序 得分 最高 的 
文档 6 . 问答 系统 对 一个 自然 语言 表达 
的 问题 由 问答 系统 给 出 一个 精准 的 
答案 需要 对 自然 语言 查询 语句 进行 某种 程度 
的 语义分析 包括 实体 链接 关系 识别 形成 逻辑 表达式 
然后 到 知识库 中 查找 可能 的 候选 答案 并 
通过 一个 排序 机制 找出 最佳 的 答案 7 . 
对话 系统 系统 通过 一 系列 的 对话 跟 用户 
进行 聊天 回答 完成 某一 项 任务 涉及 到 用户 
意图 理解 通用 聊天 引擎 问答 引擎 对话 管理 等 
技术 此外 为了 体现 上下文 相关 要 具备 多轮 对话 
能力 同时 为了 体现 个性化 要 开发 用户 画像 以及 
基于 用户 画像 的 个性化 回复 随着 深度 学习 在 
图像 识别 语音识别 领域 的 大放异彩 人们 对 深度 学习 
在 NLP 的 价值 也 寄予厚望 再 加上 AlphaGo 的 
成功 人工智能 的 研究 和 应用 变得 炙手可热 自然语言 处理 
作为 人工智能 领域 的 认知 智能 成为 目前 大家 关注 
的 焦点 很多 研究生 都在/nr 进入 自然语言 领域 寄望 未来 
在 人工智能 方向 大展身手 但是 大家 常常 遇到 一些 问题 
俗话说 万事开头难 如果 第一 件 事情 成功 了 学生 就 
能 建立 信心 找到 窍门 今后 越做/nr 越好 否则 也 
可能 就 灰心丧气 甚至 离开 这个 领域 这里 针对 给出 
我 个人 的 建议 希望 我 的 这些 粗浅 观点 
能够 引起 大家 更 深层次 的 讨论 建议 1 如何 
在 NLP 领域 快速 学会 第一个 技能 我 的 建议 
是 找到 一个 开 源 项目 比如 机器翻译 或者 深度 
学习 的 项目 理解 开源 项目 的 任务 编译 通过 
该 项目 发布 的 示范 程序 得到 与 项目 示范 
程序 一致 的 结果 然后 再 深入 理解 开源 项目 
示范 程序 的 算法 自己 编程 实现 一下 这个 示范 
程序 的 算法 再 按照 项目 提供 的 标准 测试 
集 测试 自己 实现 的 程序 如果 输出 的 结果 
与 项目 中 出现 的 结果 不 一致 就要 仔细 
查验 自己 的 程序 反复 修改 直到 结果 与 示范 
程序 基本一致 如果 还是 不行 就 大胆 给 项目 的 
作者 写信 请教 在此 基础 上 再 看看 自己 能否 
进一步 完善 算法 或者 实现 取 得比 示范 程序 更好 
的 结果 1 . 国际 学术 组织 学术 会议 与 
学术 论文 自然语言 处理 natural language processing NLP 在 很大 
程度 上 与 计算 语言学 computational linguistics CL 重合 与 
其他 计算机 学科 类似 NLP / CL 有 一个 属于 
自己 的 最 权威 的 国际 专业 学会 叫做 The 
Association for Computational Linguistics ACL URL ACL Home Page 这个 
协会 主办 了 NLP / CL 领域 最 权威 的 
国际 会议 即 ACL 年会 ACL 学会 还会在 北美 和 
欧洲 召开 分 年会 分别 称为 NAACL 和 EACL 除此之外 
ACL 学会 下设 多个 特殊 兴趣小组 special interest groups SIGs 
聚集 了 NLP / CL 不同 子 领域 的 学者 
性质 类似 一个 大学 校园 的 兴趣 社团 其中 比较 
有名 的 诸如 SIGDAT Linguistic data and corpus based approaches 
to NLP SIGNLL Natural Language Learning 等 这些 SIGs 也会 
召开 一些 国际 学术 会议 其中 比较 有名 的 就是 
SIGDAT 组织 的 EMNLP Conference on Empirical Methods on Natural 
Language Processing 和 SIGNLL 组织 的 CoNLL Conference on Natural 
Language Learning 此外 还有 一个 International Committee on Computational Linguistics 
的 老牌 NLP / CL 学术 组织 它 每 两年 
组织 一个 称为 International Conference on Computational Linguistics COLING 的 
国际 会议 也是 NLP / CL 的 重要 学术 会议 
NLP / CL 的 主要 学术 论文 就 分布 在 
这些 会议 上 作为 NLP / CL 领域 的 学者 
最大 的 幸福 在于 ACL 学会 网站 建立 了 称作 
ACL Anthology 的 页面 URL ACL Anthology 支持 该 领域 
绝大部分 国际 学术 会议 论文 的 免费 下载 甚至 包含 
了 其他 组织 主办 的 学术 会议 例如 COLING IJCNLP 
等 并 支持 基于 Google 的 全文 检索 功能 可谓 
一站 在手 NLP 论文 我 有 由于 这个 论文 集合 
非常 庞大 并且 可以 开放 获取 很多 学者 也 基于 
它 开展 研究 提供 了 更 丰富 的 检索 支持 
具体 入口 可以 参考 ACL Anthology 页面 上方 搜索框 右侧 
的 不同 检索 按钮 与 大部分 计算机 学科 类似 由于 
技术 发展 迅速 NLP / CL 领域 更 重视 发表 
学术 会议 论文 原因 是 发表 周期短 并 可以 通过 
会议 进行 交流 当然 NLP / CL 也有 自己 的 
旗舰 学术期刊 发表 过 很多 经典 学术论文 那 就是 Computational 
Linguistics URL MIT Press Journals 该 期刊 每期 只有 几 
篇文章 平均/a 质量/n 高于/nr 会议/n 论文/nz 时间 允许 的话 值得 
及时 追踪 此外 ACL 学会 为了 提高 学术 影响力 也 
刚刚 创办 了 Transactions of ACL TACL URL Transactions of 
the Association for Computational Linguistics ISSN 2307 387X 值得 关注 
值得一提的是 这 两份 期刊 也都 是 开放 获取 的 此外 
也 有 一些 与 NLP / CL 有关 的 期刊 
如 ACM Transactions on Speech and Language Processing ACM Transactions 
on Asian Language Information Processing Journal of Quantitative Linguistics 等等 
根据 Google Scholar Metrics 2013年 对 NLP / CL 学术 
期刊 和 会议 的 评价 ACL EMNLP NAACL COLING LREC 
Computational Linguistics 位于 前 5位 基本 反映 了 本 领域 
学者 的 关注 程度 NLP / CL 作为 交叉学科 其 
相关 领域 也 值得 关注 主要 包括 以下 几个 方面 
1 信息检索 和 数据挖掘 领域 相关 学术 会议 主要 由 
美国 计算机 学会 ACM 主办 包括 SIGIR WWW WSDM 等 
2 人工智能 领域 相关 学术 会议 主要 包括 AAAI 和 
IJCAI 等 相关 学术 期刊 主要 包括 Artificial Intelligence 和 
Journal of AI Research 3 机器学习 领域 相关 学术 会议 
主要 包括 ICML NIPS AISTATS UAI 等 相关 学术 期刊 
主要 包括 Journal of Machine Learning Research JMLR 和 Machine 
Learning ML 等 例如 最近 兴起 的 knowledge graph 研究 
论文 就有/i 相当/d 一/m 部分/n 发表/v 在/p 人工/n 智能/n 和/c 
信息检索/n 领域/n 的/uj 会议/n 和/c 期刊/n 上/f 实际上 国内 计算机 
学会 CCF 制定 了 中国 计算机 学会 推荐 国际 学术 
会议 和 期刊目录 CCF 推荐 排名 通过 这个 列表 可以 
迅速 了解 每个 领域 的 主要 期刊 与 学术 会议 
最后 值得一提的是 美国 Hal Daum é III 维护 了 一个 
natural language processing 的 博客 natural language processing blog 经常 
评论 最新 学术 动态 值得 关注 我 经常 看 他 
关于 ACL NAACL 等 学术 会议 的 参会 感想 和对/nr 
论文 的 点评 很 有启发 另外 ACL 学会 维护 了 
一个 Wiki 页面 ACL Wiki 包含 了 大量 NLP / 
CL 的 相关 信息 如 著名 研究 机构 历届 会议录 
用率 等等 都是 居家 必备 之 良 品 值得 深挖 
2 . 国内 学术 组织 学术 会议 与 学术 论文 
与 国际 上 相似 国内 也 有一个 与 NLP / 
CL 相关 的 学会 叫做 中国 中文信息 学会 URL 中国 
中文信息 学会 通过 学会 的 理事 名单 中国 中文信息 学会 
基本 可以 了解 国内 从事 NLP / CL 的 主要 
单位 和 学者 学会 每年 组织 很多 学术会议 例如 全国 
计算 语言学 学术会议 CCL 全国 青年 计算 语言学 研讨会 YCCL 
全国 信息检索 学术会议 CCIR 全国 机器翻译 研讨会 CWMT 等等 是 
国内 NLP / CL 学者 进行 学术 交流 的 重要 
平台 尤其 值得一提的是 全国 青年 计算 语言学 研讨会 是 专门 
面向 国内 NLP / CL 研究生 的 学术 会议 从 
组织 到 审稿 都由 该 领域 研究生 担任 非常 有 
特色 也是 NLP / CL 同学们 学术交流 快速 成长 的 
好去处 值得一提的是 2010年 在 北京 召开 的 COLING 以及 2015年 
即将 在 北京 召开 的 ACL 学会 都是/nr 主要 承办者 
这也 一定 程度 上 反映 了 学会 在 国内 NLP 
/ CL 领域 的 重要 地位 此外 计算机 学会 中文 
信息 技术 专委会 组织 的 自然 语言 处理 与 中文 
计算 会议 NLP & CC 也是 最近 崛起 的 重要 
学术 会议 中文信息 学会 主编 了 一份 历史 悠久 的 
中文信息 学报 是 国内 该 领域 的 重要 学术 期刊 
发表 过 很多 篇 重量级 论文 此外 国内 著名 的 
计算机 学报 软件 学报 等 期刊 上 也 经常 有 
NLP / CL 论文 发表 值得 关注 过去 几年 在 
水木 社区 BBS 上 开设 的 AI NLP 版面 曾经 
是 国内 NLP / CL 领域 在线 交流 讨论 的 
重要 平台 这几年 随着 社会 媒体 的 发展 越来越 多 
学者 转战 新浪 微博 有 浓厚 的 交流 氛围 如何 
找到 这些 学者 呢 一个 简单 的 方法 就是 在 
新浪 微博 搜索 的 找人 功能 中 检索 自然语言 处理 
计算 语言学 信息检索 机器学习 等 字样 马上 就能 跟 过去 
只 在 论文 中 看到 名字 的 老师 同学们 近距离 
交流 了 还有 一种 办法 清华大学 梁斌 开发 的 微博 
寻人 系统 清华大学 信息检索 组 可以 检索 每个 领域 的 
有 影响力 人士 因此 也 可以 用来 寻找 NLP / 
CL 领域 的 重要 学者 值得一提的是 很多 在 国外 任教 
的 老师 和 求学 的 同学 也 活跃 在 新浪 
微博 上 例如 王威廉 Sina Visitor System 李沐/nr Sina Visitor 
System 等 经常 爆料 业内 新闻 值得 关注 还有 国内 
NLP / CL 的 著名 博客 是 52nlp 我 爱 
自然 语言 处理 影响力 比较 大 总之 学术研究 既 需要 
苦练内功 也 需要 与 人 交流 所谓 言者无意 听者有心 也许 
其他人 的 一句话 就能 点醒 你 苦思 良久 的 问题 
无疑 博客 微博 等 提供 了 很好 的 交流 平台 
当然 也 注意 不要 沉迷 哦 3 . 如何 快速 
了解 某 个 领域 研究 进展 最后 简单 说 一下 
快速 了解 某 领域 研究 进展 的 经验 你 会 
发现 搜索引擎 是 查阅 文献 的 重要 工具 尤其 是 
谷歌 提供 的 Google Scholar 由于 其 庞大 的 索引 
量 将 是 我们 披荆斩棘 的 利器 当 需要 了解 
某 个 领域 如果 能 找到 一篇 该 领域 的 
最新 研究 综述 就 省劲 多了 最 方便 的 方法 
还是 在 Google Scholar 中 搜索 领域 名称 + survey 
/ review / tutorial / 综述 来 查找 也 有 
一些 出版社 专门 出版 各 领域 的 综述 文章 例如 
NOW Publisher 出版 的 Foundations and Trends 系列 Morgan & 
Claypool Publisher 出版 的 Synthesis Lectures on Human Language Technologies 
系列 等 它们 发表 了 很多 热门 方向 的 综述 
如 文档 摘要 情感 分析 和 意见 挖掘 学习 排序 
语言 模型 等 如果 方向 太 新 还 没有 相关 
综述 一般 还 可以 查找 该 方向 发表 的 最新 
论文 阅读 它们 的 相关 工作 章节 顺着 列出 的 
参考 文献 就 基本 能够 了解 相关 研究 脉络 了 
当然 还有 很多 其他 办法 例如 去 http / / 
videolectures . net 上看 著名 学者 在 各大 学术 会议 
或 暑期学校 上 做 的 tutorial 报告 去 直接 咨询 
这个 领域 的 研究 者 等等 开始 推荐 工具包 中文 
的 显然 是 哈工大 开源 的 那个 工具包 LTP Language 
Technology Platform developed by HIT SCIR 哈尔滨 工业 大学 社会 
计算 与 信息检索 研究中心 . 英文 的 python pattern simpler 
to get started than NLTKchardet character encoding d e t 
e c t i o n p y e n 
c h a n t easy access to d i 
c t i o n a r i e s 
s c i k i t learn has support for 
text c l a s s i f i c 
a t i o n u n i d e 
c o d e because ascii is much easier to 
deal with 希望 可以 掌握 以下 的 几个 tool CRF 
+ + GIZAWord2Vec 还 记得 小时候 看过 的 数码宝贝 每个 
萌萌 哒 的 数码宝贝 都会 因为 主人 身上 发生 的 
一些 事情 而 获得 进化 能力 其实 在 自然 语言 
处理 领域 我 觉得 一切 也 是 这样 ~ 我 
简单 的 按照 自己 的 见解 总结 了 每个 阶段 
的 特征 以及 提高 的 解决 方案 1 . 幼年 
体 自然语言 处理 好 屌 我 什么 都 不会 但是 
好想 提高 建议 去看 公开课 ~ 去做 Kaggle 的 那个 
情感 分析题 2 . 成长期 觉得 简单 模型 太 Naive 
高大 上 的 才是 最好 的 这个 阶段 需要 自己 
动手 实现 一些 高级 算法 或者说 常用 算法 比如 LDA 
比如 SVM 比如 逻辑 斯蒂 回归 并且 拥抱 Kaggle 知道 
trick 在 这个 领域 的 重要性 3 . 成熟期 高大 
上 的 都不 work 通过 特征 工程 加 规则 才 
work 大部分 人 应该 都在/nr 这个 级别 吧 包括 我 
自己 我 总是 想 进化 但 积累 还是 不够 觉得 
高大 上 的 模型 都是/nr 一些 人 为了 paper 写 
的 真正 的 土 方法 才是 重剑 无 锋 大巧不工 
在 这个 阶段 应该 就是 不断 读 论文 不断 看 
各种 模型 变种 吧 什么 句子 相似 度 计算 word2vec 
cosine 已经 不再 适合 你 了 4 . 完 全体 
在 公开 数据 集上 把 某个 高大 上 的 模型 
做 work 了 ~ 这类 应该 只有 少数 博士 可以 
做到 吧 我 已经 不 知道 到 了 这个 水平 
再 怎么 提高 了 ~ 是不是 只能 说 不忘 初心 
方得 始终 5 . 究 极体 参见 Micheal Jordan Andrew 
Ng . 希望 可以 理解 自然语言 处理 的 基本 架构 
~ 分词 = 词性 标注 = ParserQuora 上 推荐 的 
NLP 的 论文 摘自 Quora 我 过 一阵 会 翻译 
括号 里面 的 解释 Parsing 句法结构 分析 ~ 语言学 知识 
多 会 比较 枯燥 Klein & Manning Accurate Unlexicalized Parsing 
Klein & Manning Corpus Based Induction of Syntactic Structure Models 
of Dependency and Constituency 革命性 的 用 非 监督 学习 
的 方法 做了 parser Nivre Deterministic Dependency Parsing of English 
Text shows that deterministic parsing actually works quite well McDonald 
et al . Non Projective Dependency Parsing using Spanning Tree 
Algorithms the other main method of dependency parsing MST parsing 
Machine Translation 机器翻译 如果 不 做 机器翻译 就 可以 跳过 
了 不过 翻译 模型 在 其他 领域 也 有 应用 
Knight A statistical MT tutorial workbook easy to understand use 
instead of the original Brown paper Och The Alignment Template 
Approach to Statistical Machine Translation foundations of phrase based systems 
Wu Inversion Transduction Grammars and the Bilingual Parsing of Parallel 
Corpora arguably the first realistic method for biparsing which is 
used in many systems Chiang Hierarchical Phrase Based Translation significantly 
improves accuracy by allowing for gappy phrases Language Modeling 语言 
模型 Goodman A bit of progress in language modeling describes 
just about everything related to n gram language models 这 
是 一个 survey 这个 survey 写了 几乎 所有 和n/nr gram 
有关 的 东西 包括 平滑 聚 类 Teh A Bayesian 
interpretation of Interpolated Kneser Ney shows how to get state 
of the art accuracy in a Bayesian framework opening the 
path for other applications Machine Learning for NLPSutton & McCallum 
An introduction to conditional random fields for relational learning CRF 
实在 是 在 NLP 中 太好 用了 而且/c 我们/r 大家/n 
都/d 知道/v 有/v 很多/m 现成/v 的/uj tool/w 实现/v 这个/r 而 
这个 就是 一个 很 简单 的 论文 讲述 CRF 的 
不过 其实 还是 蛮 数学 = = Knight Bayesian Inference 
with Tears explains the general idea of bayesian techniques quite 
well Berg Kirkpatrick et al . Painless Unsupervised Learning with 
Features this is from this year and thus a bit 
of a gamble but this has the potential to bring 
the power of discriminative methods to unsupervised learning Information ExtractionHearst 
. Automatic Acquisition of Hyponyms from Large Text Corpora . 
COLING 1992 . The very first paper for all the 
bootstrapping methods for NLP . It is a hypothetical work 
in a sense that it doesn t give experimental results 
but it influenced it s followers a lot . Collins 
and Singer . Unsupervised Models for Named Entity Classification . 
EMNLP 1999 . It applies several variants of co training 
like IE methods to NER task and gives the motivation 
why they did so . Students can learn the logic 
from this work for writing a good research paper in 
NLP . Computational SemanticsGildea and Jurafsky . Automatic Labeling of 
Semantic Roles . Computational Linguistics 2002 . It opened up 
the trends in NLP for semantic role labeling followed by 
several CoNLL shared tasks dedicated for SRL . It shows 
how linguistics and engineering can collaborate with each other . 
It has a shorter version in ACL 2000 . Pantel 
and Lin . Discovering Word Senses from Text . KDD 
2002 . Supervised WSD has been explored a lot in 
the early 00 s thanks to the senseval workshop but 
a few system actually benefits from WSD because manually crafted 
sense mappings are hard to obtain . These days we 
see a lot of evidence that unsupervised clustering improves NLP 
tasks such as NER parsing SRL etc 其实 我 相信 
大家 更 感兴趣 的 是 上层 的 一些 应用 ~ 
而不 是 如何 实现 分词 如何 实现 命名 实体 识别 
等等 而且 应该 大家 更 对 信息检索 感兴趣 不过 自然语言 
处理 和 信息检索 还是 有所 区别 的 So ~ ~ 
~ 我 就不 在这边 写 啦 2 . https / 
/ www . zhihu . com / question / 52164329 
/ answer / 129930771 本科 大三 学过 机器学习 算法 假设 
你 学过 的 算法 都 熟练 的话 你 已经 有了/nr 
不错 的 基础 了 那么 问题 分解为 1 . 如何 
入门 NLP 2 . 如何 开始 做 NLP 的 研究 
这 两个 我 分别 回答 但是 你 可以 同时 行动 
入门 NLP 就像 你 自学 机器学习 一样 你 最好 系统 
的 看 一 本书 或者 上 一门 公开课 来 系统 
的 梳理 一遍 NLP 的 基本 知识 了解 NLP 的 
基本 问题 这里 我 推荐 Michael Collins 的 公开课 COMS 
W4705 Natural Language Processing Spring 2015 以及 Jason Eisner 的 
Lecture Notes 600.465 Natural Language Processing 如果 学有 余力 的话 
可以 看 一下 参考书 https / / web . stanford 
. edu / ~ jurafsky / slp3 / 时间 有限 
的 情况 下 公开课 和 Notes 就 够了 系统 学习 
知识 的 同时 或之后 你 可以 开始 着手 复现 一些 
经典 的 项目 这个 过程 非常 重要 1 . 你 
可以 巩固 自己 的 知识 确定 你 真的 正确 理解 
了 2 . 你 可以 进一步 提高 自己 的 科研 
和 工程 能力 3 . 你 很可能 在 实现 的 
过场 中 发现 问题 产生 灵感 做出 自己 的 工作 
发 一篇 paper 那么 复现 什么 项目 呢 如果 你 
的 导师 没有 给 你 指定 的话 不妨 从 历年 
NLP 顶 会 ACL EMNLP NAACL 的 获奖 论文 中 
筛选 你 感兴趣 又 有 能力 完成 的 由于 full 
paper 的 工程量 通常 较大 你 可以 先从 short paper 
中 进行 选择 下面 是 最近 的 ACL EMNLP 和 
NAACL 的 录取 论文 列表 ACL | Association for Computational 
LinguisticsEMNLP 2016Accepted Papers 同时 再 附上 一些 Jason Eisner 为 
帮助 本科生 做 研究 而 写 的 一些 建议 Advice 
for Research Students and others 希望 你 能 enjoy NLP 
3 . https / / zhuanlan . zhihu . com 
/ p / 34524452 从 目前 的 发展 情况 来看 
NLP 更多 的 是 统计学 shallow NLP 机器学习 deep NLP 
深度 学习 deep NLP 的 field 甚至有 NLP 方面 的 
学者 认为 自己 每 开除 一个 语言学家 NLP 的 准确率 
就会 提升 一 个 百分点 NLP 的 几个 核心问题 第一 
个 为什么 基于 语法 的 分析 是 不 可行 的 
目前 第二个 是 NLP 从浅/nr shallow 到 深 deep 的 
四个 维度 lexical analysis syntactic analysis pragmatic analysis 以及 semantic 
analysis 的 问题 当然 这 也就 牵扯 出 了 第三 
个 问题 NLP 的 两 大 方法 基于 统计学 处理 
shallow NLP 问题 一般 准确率 比较 高 目前 也 相对 
通用 和 基于 机器学习 深度 学习 的 deep NLP 问题 
3 . https / / www . zhihu . com 
/ question / 35381685 / answer / 3 4 2 
7 0 5 2 1 8 R e f e 
r e n c e 大方向 书籍 我 要是 能 
全部 买 下来 就 好了 . . . 并 没有 
全部 看完 有的 只是 看过 某一 章节 Grammar/w 和/c syntax/w 
知乎/i 里面/f 有/v 很多/m 问答/v 跟/p 这/r 方面/n 有关/vn 在此 
不 重复 了 Cruse Alan . Meaning in language An 
introduction to semantics and pragmatics . 2011 . Karttunen Lauri 
1974 1 . Theoretical Linguistics 1 181 94 . Also 
in Pragmatics A Reader Steven Davis ed . pages 406 
415 Oxford University Press 1991 . Kadmon Nirit . Formal 
pragmatics semantics pragmatics presupposition and focus . 2001 . Levinson 
Stephen C . Pragmatics . Cambridge Cambridge University Press 1983 
pp . 181 184 . Wardhaugh Ronald . An introduction 
to sociolinguistics . John Wiley & Sons 2010 . 这 
本书 的 影响力 很大 有 很多 跟 social science 的 
讨论 具体 其他 上面 提到 的 每 一篇 我 都 
仔细 读过 的 Grice s MaximsMonroe Will and Christopher Potts 
. Learning in the rational speech acts model . arXiv 
preprint arXiv 1510.06807 2015 . 这篇 是 关于 rsa 如何 
被 用于 具体 task 上 的 Farkas Rich á rd 
et al . The CoNLL 2010 shared task learning to 
detect hedges and their scope in natural language text . 
Proceedings of the Fourteenth Conference on Computational Natural Language Learning 
Shared Task . Association for Computational Linguistics 2010 . 上文 
提到 的 hedge and cues shared task 关于 linguistics 里 
的 现象 是 如何 被 formulate 成 nlp 问题 的 
Morante Roser and Eduardo Blanco . * SEM 2012 shared 
task Resolving the scope and focus of negation . Proceedings 
of the First Joint Conference on Lexical and Computational Semantics 
Volume 1 Proceedings of the main conference and the shared 
task and Volume 2 Proceedings of the Sixth International Workshop 
on Semantic Evaluation . Association for Computational Linguistics 2012 . 
negation 的 shared task 最后 附上 两篇 老爷爷 对 我 
影响 最大 的 Zadeh Lotfi Asker . The concept of 
a linguistic variable and its application to approximate reasoning I 
. Information sciences 8.3 1975 199 249 . Zadeh Lotfi 
A . The concept of a linguistic variable and its 
application to approximate reasoning II . Information sciences 8.4 1975 
301 357 . 这 系列 work 分 两部 Zadeh Lotfi 
A . Toward a theory of fuzzy information granulation and 
its centrality in human reasoning and fuzzy logic . Fuzzy 
sets and systems 90.2 1997 111 127 . 按照 题 
主 的 描述 Dan jurasfsky 的 Speech and Language Processing 
应该 是 最好 的 选择 了 Manning 的 Foundations of 
Statistical Natural Language Processing 感觉 相对 比较 旧了 中文 的话 
可以 考虑 宗 成庆 的 统计 自然语言 处理 很多/m 人/n 
对/p 宗/nr 老师/n 这/r 本书/r 有/v 负面/n 评价/n 我 觉得 
倒 还好 我们 实验室 里 做 NLP 的 几个人 有 
忘记 一些 知识 的 时候 都会 把 他 作为 工具书 
来 翻翻 可能/v 的确/d 不/d 适合/v 于/p 入门/ns 和/c 精读/v 
最后 想 强烈 安利 Yoav Goldberg 的 这本 Neural Network 
Methods for Natural Language Processing 我 最 偏爱 的 还是 
大神 dan jurasfsky 的 Speech and Language Processing 中文名 译为 
自然语言 处理 综论 大神 讲 什么 都很/nr 清楚 一点 就 
通 而且 还很 贝叶斯 逻辑/n 斯特/l 回归/v 我/r 是/v 先看/i 
的/uj 吴恩/i 达/v 和林轩/nr 田的课/nr 统计 学派 是从 优化 角度 
用 拉格朗日 乘 数法 引入 正则化 L2 L1 要 理解 
L1 为何 会 导致 参数 稀疏 化 还得 去看 sub 
gradient jurafksy 从 贝叶斯 的 角度 讲 正则化 其实 只是 
贝叶斯 公式 里 的 先验概率 你 用 高斯分布 就是 L2 
正则 拉普拉斯 分布 就是 L1 正则 要是 看过 深度 学习 
训练 后的/nr 参数 分布 你 会 发现 更 直观 训练 
出来 的 参数 部分 就是 高斯分布 的 样子 ~ 该书 
第三版 正在 撰写 中 作者 已经 完成 了 不少 章节 
的 撰写 所 完成 的 章节 均可 下载 Speech and 
Language Processing4 . https / / zhuanlan . zhihu . 
com / p / 35423943 自然语言 数据集 5 . https 
/ / www . zhihu . com / question / 
53959076 / answer / 262419812 看到 实验室 有 同学 关注 
了 这个 问题 还 挺 有趣 的 让 我 来 
开 一下 脑 洞 现在 网站 应该是 用 关键字 匹配 
或者 正则表达式 来 过滤 恶意 弹幕 这种 很 容易 被 
破解 的 改成 拼音 谐音 或者 加 几个 标点 就 
没 办法 了 如果 要 用上 nlp 手段 可以 先 
人工 标注 恶意 弹幕 再用 深度 学习 的 方法 比如 
用 LSTM 学习 句子 语义 最后 给出 它 属于 恶意 
弹幕 的 score 其实 就是 sentence classification 二分 类 问题 
还 可以 做成 个性化 弹幕 屏蔽 转化成 n 分类 问题 
对 弹幕 进行 多 分类 可以 让 用户 来 设置 
屏蔽 哪种 弹幕 或者 根据 用户 的 历史 信息 来 
自动 设置 比如 你 是个 单纯 的 孩子 那你/nr 可以 
设置 屏蔽掉 污 污 的 弹幕 但是 啊 我 觉得 
中文 是 博大精深 的 内涵 段子 也是 博大精深 的 机器 
是 很难 读懂 的 像 我们 这些 单纯 的 小朋友 
也是 读 不懂 的 对不对 现阶段 做 问答 多 是 
标注 答案 做 文本 生成 也 是 根据 所 给 
文本 生成 语义 匹配 的 另一 段 文本 还 有根据 
查询 语句 生成 SQL 命令 都是/i 有/v 套路/n 有/v 模板/n 
的/uj 推理 领域 我 不太 了解 目前 还有 很大 的 
发展 空间 也 就是 做得 还 不够 好 就算 机器 
训练 再 多次 看 遍 各种 段子 遇到 真正 的 
老 司机 还是 要 翻车 的 因为 老 司机 的 
套路 深不可测 自古 深情 留不住 总是 套路 得人心 啊 中文 
博大精深 简单 的 用 词库 或者 正则 是 肯定 不够 
的 而有 监督 的 深度 学习 方法 一般 需要 大量 
的 标注 语料 会标 到 让 你 怀疑 人生 这里 
安利 一下 实验室 一个 师兄 的 工作 Reading the Videos 
Temporal Labeling for Crowdsourced Time Sync Videos based on Semantic 
Embedding 文章 可以 利用 无 监督 方法 获得 弹幕 文本 
的 embedding idea 是 假设 经常 在 相近 时间 一起 
出现 的 弹幕 有着 相似 的 语义 和 向量空间 然后 
将 问题 转换成 监督 问题 让 相近 时间 经常 一起 
出现 的 弹幕 在 向量空间 尽可能 相近 让 不同 时间 
的 弹幕 在 向量空间 尽可能 远离 这样 可以 得到 包含 
弹幕 语义 信息 的 embedding 向量 后面 只 需要 提供 
少量 你 要 屏蔽 的 弹幕 屏蔽 列表 然后 计算 
目标 弹幕 和 弹幕 屏蔽 列表 中 弹幕 的 余弦 
相似 度 根据 阈值 过滤 即可 6 . https / 
/ zhuanlan . zhihu . com / p / 33366448 
自然语言 处理 主要 技术 自然语言 处理 大概有 五类 技术 分别 
是 分类 文字 的 序列 我们 要 打印 标签 这 
是 我们 常做的/nr 最 基本 的 自然 语言 处理 匹配 
两个 文字 序列 都 匹配 看 它们 匹配 的 程度 
最后 输出 一个 非 负 的 实数值 判断 这 两个 
文字 序列 它们 的 匹配 程度 翻译 把 一个 文字 
序列 转换成 另外 一个 文字 序列 结构 预测 你 给 
我 一个 文字 序列 让 它 形成 内部 结构 的 
一个 信息 序列 决策 过程 在 一个 复杂 的 动态 
变化 环境 里面 我们 怎么样 不断 去 决策 比如 描述 
序列 决策 过程 的 马尔可夫 随机 过程 这 是 一个 
有效 的 非常 常用 的 数学 工具 我们 看 自然语言 
处理 的 大部分 问题 基本上 做得 比较 成功 实用 的 
都是 基于 这样 的 技术 做 出来 的 比如 分类 
有 文本 分类 情感 分析 匹配 有 搜索 问答 单轮 
对话 基于 检索 的 单轮 对话 翻译 有 机器翻译 语音识别 
手写体 识别 基于 生成 方法 的 单轮 对话 结构 预测 
有 专名 识别 词性 标注 语意 分析 序列 决策 过程 
有 多轮 对话 资料 推荐 开始 旅程 关于 书籍 数学 
之美 吴军 科普 且 生动 形象 入门 必备 统计 学习 
方法 李航 这个 讲述 基础 机器学习 算法 这是 值得 看 
的 统计 自然语言 处理 宗 成庆 经典 好书 可以 详细 
看 关于 综述 深度 学习 NLP 这个 综述 主要 是 
深度 学习 在 NLP 的 应用 和 发展 值得一看 的 
自然语言 生成 综述 讲述/v 自然语言/l 生层的/nr 各种/r 方式/n 和/c 应用/v 
关于 教程 Stanford nlp 公开课 cs224n 需要 中文 笔记 的 
可以 看下 博文 比如 word2vec 斯坦福 CS224N 深度 学习 自然语言 
处理 一 note 等等 关于 其他 资料 Recent Advances and 
New Frontiers 对话 的 综述 因为 我 是 做 对话 
的 哈哈 其他 方向 不 了解 了 多看 论文 做 
实验 多看 论文 做 实验 . . . . . 
7 . https / / zhuanlan . zhihu . com 
/ p / 36708892GluonNLP 自然语言 处理 的 深度 学习 工具包 
这天 他 看到 时下 最 热门 的 一篇 谷歌 论文 
Attention Is All You Need 介绍 基于 注意力 机制 的 
Transformer 模型 小 A 上网 搜 了 搜 发现 Tensorflow 
的 Tensor2Tensor 包里 已经 有了/nr 这篇 论文 的 实现 身 
she 经 shi 百 wei 战 shen 的 小 A 
于是 决定 立刻 就 拿 这个 包跑/nr 一下 想在 当天 
下午 重现 一下 这个 最 新的 黑 科技 自然语言 处理 
的 模型 重现 之所以 难 与/p 数据/n 处理/v 和/c 模型/n 
搭建/v 中/f 需要/v 解决/v 的/uj 茫茫/z 多/m 技术/n 点/m 有/v 
很大/a 关系/n 从 各种 语言 的 文本文件 编码 解码 encoding 
/ decoding 读取 分词 tokenization 词 向量 转化 embedding 到 
输入 给 神经网络 前 的 填充 位 padding 截 长 
clipping 再到 神经网络 模型 里 处理 变长 输入 数据 和 
状态 一直 到 模型 预测 解码 后的/nr 输出 的 BLEU 
score 等等 表现 评估 方法 每处 都会 有坑/nr 如果 工具 
不到位 每次 做 新 模型 开发 都要/nr 经历 各种 大坑 
小坑 的 洗礼 最近 做 新项目 发现 一个 新 趋势 
是 好 的 资源 不 集中 大家/n 都/d 知道/v 预/v 
训练/vn 的/uj 词/n 向量/n 和/c 语言/n 模型/n 对/p 很多/m 应用/v 
有/v 帮助/v 而 哪个 预 训练 模型 更 有用 则是 
需要 实验 来 验证 的 在做 这些 验证 时 开发 
者 常常 需要 装 许多 不同 的 工具 比如 Google 
的 Word2vec 需要 装 gensim 而 Salesforce 做 的 AWD 
语言 模型 是 用 PyTorch 实现 的 且 暂不 提供 
预 训练 模型 Facebook 的 Fasttext 则 是 自己 开发 
的 一个 独立 的 包 为了 能把/nr 这些 资源 凑齐 
在 自己 心爱 的 框架 里 使用 用户 往往 需要 
花费 大量 的 精力 在 安装 上 8 . https 
/ / zhuanlan . zhihu . com / p / 
33797826 手把手 教 您 解决 90% 的 自然 语言 处理 
问题 NLP 是 一个 非常 大 的 领域 NLP/w 有/v 
几个/m 最/d 常/d 使用/v 的/uj 关键/n 应用/v 识别 不同 的 
用户 / 客户群 准确 的 检测 和 提取 不同 类别 
的 反馈 根据 意图 对 文本 进行 分类 本文 将 
讲解 如何 从头开始 有效 地 处理 这些 问题 的 指南 
和 技巧 首先 解释 如何 构建 机器学习 解决 方案 来 
解决 上面 提到 的 问题 然后 转向 更 细致 的 
解决方案 比如 特性 工程 单词 向量 和 深度 学习 第一步 
收集 你 的 数据 每个 机器学习 问题 都 始于 数据 
本文 中 我们 将 使用 一个 名为 社交 媒体 上 
的 灾难 的 数据 集 投稿人 查看 了 超过 一万条 
的 推 文 然后 指出 每条 推 文 是否 提到 
了 灾难 事件 我们 的 任务 是 检测 哪些 推 
文是/nr 关于 灾难 事件 的 因为 有 潜在 的 应用 
专门 收集 紧急 事件 并 通知 执法 部门 这个/r 任务/n 
的/uj 特殊/a 挑战/vn 是/v 两个/m 类/q 都/d 包含/v 用于/v 查找/v 
推/v 文的/nr 相同/d 搜索/v 条件/n 所以 我们 不得 不用 更 
微妙 的 差异 来 区分 它们 在 本文 中 我们 
将 有关 灾难 的 推 文 称为 灾难 其他 推 
文 称为 无关紧要 的 正如 Richard Socher 所 描述 的 
那样 查找 和 标记 足够 的 数据 来 训练 模型 
比 试图 优化 复杂 的 无 监督 方法 通常 更快 
更简单 更 便宜 第二步 清理 你 的 数据 你 的 
模型 只能 和你的/nr 数据 一样 好 一个 干净 的 数据 
集 能够使 模型 学习 有 意义 的 特征 所以 应当 
是 先 查看 数据 然后 再 清理 数据 以下 是 
用来 清理 你 的 数据 的 清单 详见 代码 1 
删除 所有 不 相关 的 字符 例如 任 何非 字母 
数字 字符 2 把 你 的 文章 分成 一个 个 
单独 的 单词 3 删除 不 相关 的 单词 4 
将 所有 字符 转换 为 小写 5 考虑 将 拼错 
的 单词 或 拼写 单词 组合 成 一个 单独 的 
表示 6 考虑 词形 化 在 遵循 这些 步骤 并 
检查 额外 的 错误 之后 我们 可以 开始 使用 干净 
的 标记 的 数据 来 训练 模型 第三步 找到 一个 
好 的 数据 表示 机器学习 模型 以 数值 作为 输入 
我们 的 数据集 是 一个 句子 的 列表 所以 为了 
能够 提取 数据 我们 首先 要 找到 一种 方法 使 
我们 的 算法 能 理解 它 也 就是 数字 列表 
一组 以 数据 矩阵 表示 的 笑脸 独 热 编码 
Bag of Words 计算机 文本 表示 的 一种 方法 是 
将 每个 字符 单独 编码 为 一个 数字 例如 ASCII 
这 对于 大多数 数据集 来说 是 不 可能 的 所以 
我们 需要 更高 层次 的 方法 例如 我们 可以 在 
我们 的 数据 集中 建立 一个 所有 的 单词 的 
词汇表 并将 一个 唯一 的 索引 与 词汇表 中的 每个 
单词 联系起来 每个 句子 被 表示 为 一个 列表 只要 
我们 的 词汇 表中 有 不同 单词 的 数量 在 
这个 列表 中 的 每个 索引 中 我们 标 记出 
在 我们 的 句子 中 出现 了 多少 次 给定 
的 单词 这 被 称为 Bag of Words 模型 因为 
它 是 一种 完全 无视 我们 句子 中 词语 顺序 
的 表现 形式 可视化 嵌入 为了 查看 嵌入 是否 捕获 
了 与 我们 的 问题 相关 的 信息 例如 tweet 
是否 与 灾难 有关 我们 选择 可视化 并 查看 这些 
类 这个 方法 但是 由于 词汇表 通常 非常大 并且在 20000个 
维度 中 可视化 数据 是 不 可能 的 像 PCA 
这样 的 技术 将 有助于 将 数据 压缩 到 两个 
维度 如 下图 嵌入 后 这两个 类 依旧 不太好 分开 
仅仅 是 降低 了 维度 为了 看 Bag of Words 
特征 是否 有用 我们 根据 它们 来 训练 一个 分类器 
第四步 分类 当 涉及到 对 数据 进行 分类 时 逻辑 
回归 是 最简单 可用 的 工具 训练 简单 结果 可 
解释 可以 很 容易 的 从 模型 中 提取 最 
重要 的 系数 将/d 数据/n 分成/v 一个/m 适用/v 于/p 我们/r 
的/uj 模型/n 和/c 测试/vn 集/q 的/uj 训练/vn 集/q 以 了解 
它 如何 推广 到 不 可见 的 数据 训练 结束 
后 得到 了 75.4% 的 准确度 虽然 这个 精度 足够 
满足 我们 的 需求 但是 我们 还是 应该 试图 去 
理解 它 是 如何 工作 的 第五步 检查 混淆 矩阵 
第一步 是 了解 我们 模型 的 错误 类型 以及 哪 
种 类型 的 错误 是 最 不可取 在 我们 的 
例子 中 误报 是 将 不相关的 tweet 归为 灾难 而 
漏报 是 将 灾难 归类 为 不 相关 的 tweet 
如果 要 优先 处理 每个 潜在 的 事件 就要 降低 
漏报率 如果 受到 资源 的 限制 那么 会 游戏 那 
考虑 降低 误报率 将 这些 信息 可视化 的 一个 好 
方法 是 使用 混淆 矩阵 将 我们 的 模型 与 
真实 标签 的 预测 相比较 理想 情况下 矩阵 将 是从 
左上角 到 右 下角 的 对角线 混淆 矩阵 结果 显示 
该 分类器 漏报率 更高 换句话说 我们 的 模型 最 常见 
的 错误 是 将 灾难 分类 为 不 相关 的 
解释 我们 的 模型 验证 我们 的 模型 并 解释 
它 的 预测 结果 重要 的 是 看 它 使用 
哪些 词 作出 预测 在 数据 有 偏差 时 分类器 
能在 样本数据 中 做出 准确 预测 但是 这个 模型 在 
现实 世界 中 不能 很好 地 推广 在 这里 我们/r 
为/p 灾难/n 和/c 不相关的/i 推/v 文/n 绘制/n 了/ul 最/d 关键/n 
的/uj 单词表/n 我们 分类器 的 词汇 库 能够 处理 大量 
的 词汇 然而 有些 词 是 非常 频繁 的 而且 
只会 对 我们 的 预测 造成 干扰 所以 接下来 我们 
将 尝试 用 一种 方法 来 表示 能够 解释 单词 
频率 的 句子 看看/v 我们/r 是否/v 能从/nr 我们/r 的/uj 数据/n 
中/f 获得/v 更多/d 的/uj 信息/n 第六步 掌握 词汇 结构 TF 
IDF 为了 帮助 我们 的 模型 更多 地 关注 有 
意义 的 单词 我们 可以 在 我们 的 单词 模型 
包上 使用 TF IDF 评分 下图 为 新 嵌入 的 
PCA 投影 可视化 TF IDF 嵌入 我们 可以 看到 这 
两个 颜色 之间 的 区别 更 明显 了 这 使 
我们 的 分类器 更容易 区分 我们 在 新的 嵌入式 系统 
上 培训 另一个 逻辑 回归 并 最终 达到 了 76.2% 
的 精确度 一个 轻微 的 改善 提高 了 模型 的 
性能 所以 我们 可以 考虑 升级 这个 模型 了 TF 
IDF 文字 的 重要性 第七步 Leveraging s e m a 
n t i c s W o r d 2 
V e c 即使 是 最新 的 模型 也 没法 
将 训练 中 没有 遇到 的 单词 进行 分类 哪怕 
是 非常 相似 的 单词 为了 解决 这个 问题 我们 
所用 的 工具 叫做 Word2Vec Word2Vec 是 一种 查找 单词 
连续 嵌入 的 技术 它 可以 从 阅读 大量 的 
文本 中 学习 并 记住 在 类似 的 语境 中 
出现 的 单词 论文 的 作者 开放 了 一个 在 
非常 大 的 语料库 中 预先 训练 的 模型 预先 
训练 的 向量 可以 在 与 这个 帖子 相关 的 
存储库 中找到 语句 级别 的 表示 为 我们 的 分类器 
获得 一个 句子 嵌入 的 一个 快速 方法 是 平均 
Word2Vec 得分 这 跟 以前 一样 是 Bag of Words 
的 方法 但是 这次 我们 只 丢掉 句子 的 语法 
同时 保留 一些 语义 信息 Word2Vec 句子 嵌入 下图 是 
我们 使用 以前 的 技术 实现 的 新 嵌入 的 
可视化 可视化 Word2Vec 嵌入 在 训练 了 相同 的 模型 
三次 逻辑 回归 后 我们 得到 了 77.7% 的 精度 
分数 这是 现阶段 得到 的 最好 的 结果 复杂性 / 
Explainability trade oG 由于 我们 的 嵌入 没有 像 以前 
的 模型 那样 被 表示 为 每个 单词 一维 的 
矢量 所以 很难 看出 哪些 单词 与 我们 的 分类 
最 相关 虽然 我们 仍然 可以 访问 我们 的 逻辑 
回归 的 系数 但 它们 与 我们 嵌入 的 300个 
维度 相关 而 不是 词 的 索引 然而 对于 更 
复杂 的 模型 我们 可以 利用 LIME 等 黑盒 解释器 
来 了解 我们 的 分类器 如何 工作 LIME 在 GitHub 
上 可以 获得 开源 的 LIME 它 是 一种 允许 
用户 解释 任何 分类器 决定 的 黑盒 解释器 正确 的 
灾难 词语 被 识别 为 相关 的 在 这里 词语 
对 分类 的 贡献 似乎 不 那么 明显 我们 需要 
在 一个 有 代表性 的 测试用例 上 运行 LIME 看看 
那些 词 最 关键 用 这种 方法 可以 得到 像 
以前 模型 一样 重要 的 分数 并 验证 我们 模型 
的 预测 结果 Word2Vec 文字 的 重要性 由 上图 可得 
这个 模型 收集 了 相关 性 非常 高的词/nr 暗示 它 
做出 了 可以 解释 的 预测 结果 所以 可以 放心 
的 部署 到 生成 中 第八步 使用 端 到 端 
的 方法 来 利用 语法 由于 以上 方法 省略 了 
单词 的 顺序 丢弃 了 句子 的 句法 信息 所以 
这些 方法 不能 提供 足够 准确 的 结果 为此 您 
可以 使用 更 复杂 的 模型 一种 常见 的 方法 
是 将 一个 句子 作为 单个 单词 向量 的 序列 
使用 Word2Vec 或者 如 GloVe CoVe 这样 的 方法 一个 
高度 eGective 端 到 端 架构 源 卷积 神经网络 用于 
句子 分类 的 训练 非常 迅速 并且 是 入门 级 
的 深度 学习 体系结构 卷积 神经 网络 在 文本 相关 
的 任务 中 表现 非常 出色 而且 通常 比 大多数 
复杂 的 NLP 方法 例如 LSTMs 和 编码器 / 解码 
架构 要 快得多 这个 模型 保存 了 单词 的 顺序 
并 学习 了 有价值 的 信息 其中 的 单词 序列 
可以 预测 我们 的 目标 类 训练 这个 模型 不会 
比 之前 的 方法 麻烦 并且 能 获得 79.5% 的 
准确性 所以 下 一步 应该 是 使用 我们 描述 的 
方法 来 探索 和 解释 预测 以 验证 它 确实 
是 部署 到 用户 的 最佳 模型 本文 由 阿里云 
云栖 社区 组织 翻译 文章 原 标题 How to solve 
90% of NLP problems a step by step guide 9 
. https / / zhuanlan . zhihu . com / 
p / 32829048 自然语言 处理 中 N Gram 模型 介绍 
10 . https / / zhuanlan . zhihu . com 
/ p / 37646689 自然语言 处理 最 新教材 开放 下载 
乔治亚 理工大学 官方 推荐 机器 之心 昨日 乔治亚 理工大学 Jacob 
Eisenstein 教授 开放 了 自然 语言 处理 领域 的 最新 
教材 Natural Language Processing 该 教材 2018 年 6 月 
第一版 的 PDF 已经 在 GitHub 上 开放 下载 这 
本书 的 内容 主要 分为 四大 章节 即 NLP 中 
监督 与 无 监 等 学习 问题 序列 与 解析 
树 等 自然 语言 的 建模 方式 语 篇 语义 
的 理解 以及 后 这些 技术 最 在 信息 抽取 
机器 翻译 和 文本 生成 等 具体 任务 中 的 
应用 10 https / / www . zhihu . com 
/ question / 266856019 / answer / 319002132 建议 先 
从 传统 方法 学起 真 没必要 上来 就学 224n 这里 
我 强烈推荐 哥伦比亚大学 Michael Collins 的 自然 语言 处理 课程 
以前 coursera 有这 门 课程 视频 的 但是 自从 改版 
后 好像 找 不到 了 可以 网上 百度 云盘 搜一搜 
你 可以 去 他 的 个人 主页 看 他 的 
讲义 看 后会 有一种 如沐春风 的 感觉 写 的 真是 
太好了 我 研 一 的 寒假 把 他 的 讲义 
全部打 印出来 看了 好几遍 自此 算是 入了 NLP 的 大门 
学习 NLP 我 建议 第一步 学 language model 然后 依次 
学 POS tagging 语法分析 PCFG 接着 接触 NLP 的 第一 
个 实际应用 学习 机器翻译 机器翻译 真是 融合 了 各种 NLP 
知识 到 里面 先从 基于 统计 的 机器 翻译 开始 
学 IBM model1 IBM model 2 再到 phrase based machine 
translation 然后 再学 log linear model 再 往后 就 可以 
学习 各种 应用 啦 情感 分析 文本 分类 等 这个 
可以 上 斯坦福 的 那门/nr NLP 课程 也 是 非常 
棒 的 课程 对于 入门 而言 上来 就看 CS224 并不好 
现在 这门 课 已经 变成 完全 的 讲授 深度 学习 
的 方法 了 固然 深度 学习 在 NLP 领域 取得 
了 重大 的 发展 但 一上来 就看 深度 学习 难免 
忽视 了 NLP 的 一些 基础 问题 我 在此 首先 
推荐 Chris Manning 和 Dan jurafsky 两尊 大神 的 至尊 
课程 introduction to natural language processing 还有 宅 成翔/nr 教授 
的 经典 课程 Text Mining and Analyticshttps / / zh 
. coursera . org / learn / text mining 这 
两门 课程 都会 让 你 有一种 如沐春风 的 感觉 然后 
彻底 的 疯狂 的 爱上 NLP 11 https / / 
www . zhihu . com / question / 26391679 / 
answer / 34169968 题 主 的 问题 太多 了 每个 
展开 都 可以 讲 很多 ~ 作为 自然语言 处理 NLP 
方向 的 研究生 我 来 回答 一下 题 主 关于 
自然 语言 处理 如何 入门 的 问题 吧 最后 再 
YY 一下 自然语言 处理 的 前途 ~ 有点 话 我 
想 说 在前头 不管 学 什么 东西 都 要跟 大牛 
去学 真正 的 大牛 可以 把 一件 事 解释 的 
清清楚楚 If you can t explain it simply you don 
t understand it well enough . 跟 大牛 学 东西 
你 不会 觉得 难 一切 都 觉得 很 自然 顺利 
成章 的 就 掌握 了 整套 的 知识 不过 很遗憾 
大牛 毕竟 是 少数 愿意 教 别人 的 大牛 更少 
所以 如果 遇到 就 不要 强求 语言 了吧 ~ 开始 
进入 正题 我 将 介绍 如何 从 零基础 入门 到 
基本 达到 NLP 前沿 NLP 零基础 入门 首推 资料 以及 
唯一 的 资料 Columbia University Micheal Collins 教授 的 自然 
语言 课程 链接 Michael CollinsMichael Collins 绝对 的 大牛 我 
心目中 的 偶像 这门 课 是 我 见过 讲 NLP 
最最 最 清楚 的 尤其 是 他 的 讲义 Collins 
的 讲义 没有 跳步 每一步 逻辑 都 无比 自然 所有 
的 缩写 在 第一 次 出现 时 都有 全拼 公式 
角标 是 我 见过 的 最 顺眼 的 不像 有的 
论文 公式 角标 反人类 啊 而且 公式 角标 完全正确 太多 
论文 的 公式 角标 有 这样 那样 的 错 标 
这种 时候 真是 坑 死人 了 读 个 论文 跟 
破译 密码 似的 而且 几乎 不 涉及 矩阵 表示 初学者 
可能 不 习惯 矩阵 表示 吧 最 关键 的 是 
Collins 的 语言 措辞 真是 超级 顺畅 没有 长 难句 
没有 装逼 句 没有 语法错误 以及 偏 难怪 的 表示 
学术 圈 大都 是 死 理工科 宅 语文 能 这么 
好 真实 太难 得了 数学 之美 的 作者 吴军 博士 
在 书中 评价 Collins 的 博士 论文 语言 如 小说 
般 流畅 其 写作 功底 可见 一般 举 两个 例子 
如果 有 时间 不妨 亲自 体验 下 静下心来 读一读 我 
相信 即使 是 零基础 的 人 也是 能 感受到 大师 
的 魅力 的 1 . 语言 模型 Language Model http 
/ / www . cs . columbia . edu / 
~ mcollins / lm spring2013 . pdf2 . 隐 马尔可夫 
模型 与 序列 标注 问题 Tagging Problems and Hidden Markov 
Models http / / www . cs . columbia . 
edu / ~ mcollins / hmms spring2013 . pdf 现在 
Michael Collins 在 coursera 上 也 开了 公开课 视频 免费 
看 链接 Coursera 比 看 讲义 更 清晰 虽然 没有 
字幕 但是 不妨一试 因为 讲 的 真的 好 清楚 其 
在 句法分析 与 机器 翻译 部分 的 讲解 是 绝对 
的 经典 如果 能把 Collins 的 课 跟下来 讲义 看下来 
那么 你 已经 掌握 了 NLP 的 主要 技术 与 
现状 了 应该 可以 看懂 部分 论文 了 你 已经 
入门 了 NLP 进阶 Collins 的 NLP 课程 虽然 讲 
的 清晰 不过 有些 比较 重要 的 前沿 的 内容 
没有 涉及 应该 是 为了 突出 重点 做了 取舍 比如 
语言 模型 的 KN 平滑 算法 等 此外 Collins 的 
课程 更 注重 于 NLP 所 依赖 的 基础 算法 
而 对于 这些 算法 的 某些 重要 应用 并 没 
涉及 比如 虽然 讲了 序列 标注 的 算法 隐 马尔可夫 
模型 条件 随 机场 模型 最大熵 模型 但是 并 没有 
讲 如何 用 这些 算法 来做 命名 实体 识别 语义 
标注 等 Stanford NLP 组 在 coursera 的 这个 课程 
很好 的 对 Collins 的 课 进行 了 补充 链接 
Coursera 本 课程 偏 算法 的 应用 算法 的 实现 
过 的 很快 不 过上 完 Collins 的 课后 再上 
感觉 刚刚好 ~ 这 两门课 是 Coursera 上 仅有 的 
两门 NLP 课 不得不 佩服 Coursera 上 的 课 都是 
精品 啊 进阶 前沿 上 完 以上 两个 课后 NLP 
的 主要 技术 与 实现 细节 就 应该 都 清楚 
了 离 前沿 已经 很 近了 读 论文 已经 没 
问题 了 想 要 继续 进阶 前沿 就要 读 论文 
了 NLP 比起 其它 领域 的 一个 最大 的 好处 
此时 就 显现 出来 了 NLP 领域 的 所有 国际 
会议 期刊论文 都是/nr 可以 免费 下载 的 而且 有 专人 
整理 维护 每篇 论文 的 bibtex 也是 相当 清晰 详细 
链接 ACL Anthology/w 关于/p NLP/w 都有/nr 哪些/r 研究/vn 方向/n 哪些 
比较 热门 可以 参考 当前 国内外 在 自然 语言 处理 
领域 的 研究 热点 & 难点 White Pillow 的 回答 
NLP 是 会议 主导 最前沿 的 工作 都会/nr 优先 发表 
在 会议 上 关于 哪个 会议 档次 比较 高 可以 
参考 谷歌 给出 的 会议 排名 Top conference 页面 也 
可以 参考 各个 会议 的 录 稿 率 一般来说 越低 
表示 会议 档次 越高 Conference acceptance rates 基本上 大家 公认 
的 NLP 最 顶级 的 会议 为 ACL 可以 优先 
看 ACL 的 论文 最后/f 简单/a 谈/v 一下/m 这/r 三者/n 
哪个/r 更/d 有/v 发展/vn 潜力/n 作为 一个 NLP 领域 的 
研究生 当然 要说 NLP 领域 有 潜力 啦 这里 YY 
几个 未来 可能会 热门 的 NLP 的 应用 语法 纠错 
目前 文档 编辑器 比如 Word 只能 做 单词 拼写错误 识别 
语法 级别 的 错误 还 无能为力 现在 学术 领域 最好 
的 语法 纠错 系统 的 正确 率 已经 可以 接近 
50% 了 部分 细分 错误 可以 做到 80% 以上 转化/v 
成/n 产品/n 的话/u 很/zg 有/v 吸引力/n 吧/y ~/i 无论/c 是/v 
增强/v 文档/n 编辑器/n 的/uj 功能/n 还是/c 作为/v 教学软件/n 更正/d 英语/nz 
学习者/n 的/uj 写作/v 错误/n 结构化 信息 抽取 输入 一篇 文章 
输出 的 是 产品名 售价 或者 活动 名 时间 地点 
等 结构化 的 信息 NLP 相关 的 研究 很多 不过 
产品 目前 看 并不多 我 也 不是 研究 这个 的 
不知 瓶颈 在哪儿 不过 想象 未来 互联网 信息 大量 的 
结构化 语义 化 那时 的 搜索 效率 绝对 比 现在 
翻番 啊 ~ 语义 理解 这个 目前 做 的 并不 
好 但 已经 有 siri 等 一票 语音 助手 了 
也有 watson 这种 逆天 的 专家 系统 了 继续 研究 
下去 虽然 离 人工智能 还 相去甚远 但是 离 真正 好用 
的 智能 助手 估计 也不 远了 那时 生活 方式 会 
再次 改变 即使 做 不到 这么 玄乎 大大 改进 搜索 
体验 是 肯定 能 做到 的 ~ 搜索引擎 公司 在 
这 方面 的 投入 肯定会 是 巨大 的 机器翻译 这个 
不多 说 了 目前 一直 在 缓慢 进步 中 ~ 
我们 已经 能 从中 获益 看 越南 网页 看 阿拉伯 
网页 猜 个 大概 意思 没问题 了 此外 口语 级别 
的 简单句 的 翻译 目前 的 效果 已经 很好 了 
潜在 的 商业 价值 也 是 巨大 的 不过 在 
可 预见 的 近几年 对于 各 大 公司 发展 更 
有 帮助 的 估计 还 是 机器 学习 与 数据挖掘 
以上 我 YY 的 那些 目前 大 都还 在 实验室 
里 目前 能给/nr 公司 带来 实际 价值 的 更多 还是 
推荐 系统 顾客 喜好 分析 股票走势 预测 等 机器 学习 
与 数据挖掘 应用 ~ 12 https / / zhuanlan . 
zhihu . com / p / 25004227 译文 基于 Python 
的 自然 语言 处理 指南 sudo easy _ install pip 
安装 NLTK 在 终端 中 运行 sudo pip install U 
nltkScikit learn 机器学习 库 Natural Language Toolkit NLTK 为 各种 
NLP 技术 提供 轮子 Pattern – 网页 挖掘 模块 和 
NLTK 搭配 使用 TextBlob – 方便 的 NLP 工具 的 
API 基于 NLTK 和 Pattern 架构 spaCy – 工业级 的 
NLP 库 Gensim – 可 构建 主题 模型 Stanford Core 
NLP – 斯坦福 NLP 小组 提供 的 各类 服务 14 
. https / / zhuanlan . zhihu . com / 
p / 3452078513 个 自然 语言 处理 的 深度 学习 
框架 15 . https / / zhuanlan . zhihu . 
com / p / 26249110 基于 Python 的 简单 自然语言 
处理 实践 16 . https / / www . zhihu 
. com / question / 59282225 / answer / 168654529 
谢邀/nr 从/p 符号/n 主义/n 和/c 连接/v 主义/n 的/uj 对立/v 走向/v 
合作/vn 从 静态分析 走向 交互 从/p 语法/n 和/c 浅层/n 语义/n 
走向/v 深层/n 语义/n 从/p 功能主义/n 走向/v 认知/v 和/c 情感/n 体验/n 
2016年 是 深度 学习 的 大潮 冲击 NLP 的 一年 
果实 丰硕 从 底层 的 pos tagging word segmentation NER 
到 高级 的 任务 比如 semantic analysis machine translation machine 
reading comprehension QA system natural language generation 都是/nr 全面 开花 
Deep learning for NLP 的 架构 越来越 成熟 经典 的 
Speech and Language Processing 也 出了 第三版 的 draft http 
/ / web . stanford . edu / ~ jurafsky 
/ slp3 / 那么 在 2017年 我们 又 有 什么样 
的 期待 呢 我 想 对于 这个 问题 最 有 
发言权 的 应该 是 Christopher Manning 他 在 Computational Linguistics 
and Deep Learning http / / www . mitpressjournals . 
org / doi / pdf / 10.1162 / COLI _ 
a _ 00239 中 的 一些 论点 到了 2017年 依然 
成立 NLP 无疑 依然 是 机器学习 有待 攻克 的 下 
一个 重大 领域 但是 由于 语言 本身 已经 是 一种 
高 层次 的 表达 深度 学习 在 NLP 中 取得 
的 成绩 并 不如 在 视觉 领域 那样 突出 尤其 
是 在 NLP 的 底层 任务 中 基于 深度 学习 
的 算法 在 正确率 上 的 提升 并 没有 非常 
巨大 但是 速度 却要 慢 许多 这 对于 很多 对 
NLP 来说 堪称 基础 的 任务 来说 是 不太 能够 
被 接受 的 比如说 分词 在 一些 高级 任务 中 
基于 端 到 端 学习 的 神经 网络 确实 取得 
了 令人瞩目 的 成就 尤其 是 机器 翻译 方面 由于 
复杂性 太高 这样 的 高级 任务 在 此前 是 非常 
难 以 攻克 的 无论 是 基于 常规 的 统计 
学习 方法 还是 基于 规则 的 方法 深度 神经网络 强悍 
的 记忆 能力/n 和/c 复杂/a 特征提取/nr 能力/n 非常/d 适合/v 于/p 
这类/r 问题/n 在 完形填空 类型 的 阅读 理解 cloze style 
machine reading comprehension 上 基于 attention 的 模型 也 取得 
了 非常 巨大 的 突破 在 SQuAD 数据 集上 2016年 
8月 的 Exact Match 最好 成绩 只有 60% 今年 3月 
已经 接近 77% 半年 时间 提升 了 接近 20个 点 
这是 极其 罕见 的 但 同时 深度 学习 的 不可 
解释 的 特性 和 对于 数据 的 需求 也 使得 
它 尚未 在 要求 更高 的 任务 上 取得 突破 
比如 对话 系统 虽然 对话 在 2016年 随着 Echo 的 
成功 已经 被 炒 得 火热 相比 于 机器翻译 对话 
系统 并 不是 一个 简单 的 sequence to sequence 的 
问题 虽然 很多 paper 尝试 这样 去做 对话 系统 必须 
要 能够 准确 地 理解 问题 并且 基于 自身 的 
知识 系统 和 对于 对话 目标 的 理解 去 生成 
一个 回复 这 并 不是 简单 地 去 寻找 word 
alignment 就 可以 做到 的 当然 更 不必 说 对于 
上下文 和 情感 的 理解 而 相比 于 完形填空 类型 
的 机器 阅读 理解 对话 系统 可能 的 回复 是 
完全 开放 的 并 不是 仅 限于 答案 包含 在 
文本 中 这样 的 情形 而 开放式 的 阅读 理解 
同样 是 一个 AI complete 的 难题 这就 要求 我们 
对于 交互 的 过程 有更/nr 深刻 的 理解 对于/p 人类/n 
在/p 交流/n 的/uj 过程/n 中/f 的/uj 认知/v 过程/n 和/c 情感/n 
变化/vn 有/v 更好/d 的/uj 模型/n 而 这个 方向 上 深度 
学习 暂时 还 没有 更好 的 办法 在 这个 过程 
中 就像 Chris Manning 说 的 一样 我们 需要 更好 
的 理解 模型 的 组合 compositionally in models 很显然 从 
传统 的 语言 学到 我们 现在 的 端 到 端的 
靠 大量 数据 的 训练 结果 其间 还 有 很大 
一块 认知过程 的 坑 没有 被 填上 有 一个 有意思 
的 事情 是 在 大多数 端 到 端的 NLP 应用 
中 在 输入 中 包括 一些 语言学 的 特征 例如 
pos tag 或 dependency tree 并 不会 对 结果 有 
重大 影响 我们 的 一些 粗浅 的 猜测 是 因为 
目前 的 NLP 做 的 这些 特征 其实 对于 语义 
的 表示 都还/nr 比较 差 某种 程度 来说 所含 信息 
还 不如 word embedding 来 的 多 对于 极其 复杂 
需要 非常 深 的 语义 理解 的 任务 来说 这些 
语言学 特征 并 没有 太多 作用 这 并不 一定 是 
对 的 在 结合 语言学 的 规则 与 深度 学习 
方面 太多 实验 等 着 我们 去 做了 所以 我们 
需要 解决 的 不仅仅 是 Semantic Role Labelling 甚至 Semantic 
Parsing 或是 Abstract Meaning Representation http / / amr . 
isi . edu / 我们 需要 知道 的 是从 符号 
到 人类 体验 的 一种 映射 不 仅仅 是 红色 
可以 被 翻译 为 Red 我们 想 知道 人类 在 
看到 红色 时的/nr 感受 以及 红色 所 代表 的 情绪 
我们 想要 复原 的 是 文字 完全 无法 记录 下来 
的 现场 的 气氛 情绪 和 心跳 的 感觉 embodied 
experience 同样 的 文字 在 不同 的 场景 应该 有 
完全 不同 的 表达力 我们 相信 仅仅 依赖 word2vec 或 
其它 distributed representation 或是 先进 的 memory augmented networks 或是 
传统 的 NLP 方法 都还 无法 解决 这些 问题 在 
情感 和 体验 的 另一个 极端 我们 又 希望 语言 
能够 展示 它 如 雕塑 一样 的 美感 罗素 形容 
数学 用语 可以 精准 地 描述 概念 和 逻辑 这 
要求 我们 在 语言 的 模糊性 上 建立 出来 健壮 
的 知识 和 推理 体系 同样 现在 的 深度 学习 
也 还 不能 做到 这 一点 只有 结合 了 符号逻辑 
神经 网络 以及 认知科学 才 有可能 让 我们 在 对 
语言 的 理解 和 处理 上 更上一层楼 硬 广 Bayersian 
Cognitive Science / PPL https / / www . zhihu 
. com / question / 59442141 / answer / 166358150 
现在 结合 一些 热门 的 领域 任务 来 谈一谈 具体 
的 方向 Dialogue 是的 自然语言 对话 将会 开创 一个 新的 
人机交互 时代 但是 2016年 流行 的 seq2seq 对话 框架 不会 
给 我们 太大 的 惊喜 虽然 理论上 如果 能够 给足 
训练 数据 它 是 可以 表现 得 很好 的 原因 
在于 对话 不同于 翻译 翻译 的 input 和 output 肯定 
是 一个 domain 的 东西 这 大大 限制 了 可能 
的 解的/nr 空间 更 重要 的 是 对话/n 中/f 有/v 
大量/n 的/uj 省略/n 和/c 指代/n 我们 必须 通过 大量 的 
上下文 信息 才 能够 理解 对话 这样 的 后果 就是 
训练 对话 系统 对于 训练 数据 有 指数 级别 上升 
的 要求 就算 我们 已经 记录 了 这个 世界 上 
所有 人类 的 对话 明天 依然 会 有人 在 不同 
的 场景 下 说出 的话 根本 没有 在 训练 集中 
出现 所以 2017年 的 对话 系统 一定是 在 限定 的 
场景 下 发挥 作用 的 即便 是 限定 场景 下 
的 对话 也 存在 以下 的 几个 难点 需要 攻克 
后面 例举 的 文章 只是 抛砖引玉 1 .     
  怎样 评估 对话 的 质量 必须 要 和 标准 
答案 回答 得 一模一样 才算 好吗 Towards an automatic Turing 
test Learning to evaluate dialogue responses https / / openreview 
. net / pdf id = HJ5PIaseg 2 .   
    怎么 利用 对 话中 人类 的 反馈 来 
帮助 学习 Dialogue Learning With Human in the Loop https 
/ / arxiv . org / pdf / 1611.09823 . 
pdf 3 .       怎样 keep track of 
dialogue state 怎么 定义 目标 怎么 记住 多个 对话 片段 
Frames A Corpus for Adding Memory to Goal Oriented Dialogue 
Systems https / / arxiv . org / abs / 
1704.00057 4 .       如何 去做 对话 的 
policy Towards Information Seeking Agents https / / arxiv . 
org / abs / 1612.02605 5 .       
如何 结合 记忆 以及 情感 Emotional Chatting Machine Emotional Conversation 
Generation with Internal and External Memory https / / arxiv 
. org / abs / 1704.01074 6 .     
  上下文 如何 建模 Improving Frame Semantic Parsing with Hierarchical 
Dialogue Encoders https / / arxiv . org / abs 
/ 1705.03455 7 .       对话 回复 的 
生成 如何 变得 可控 Data Distillation for Controlling Specificity in 
Dialogue Generation https / / arxiv . org / pdf 
/ 1702.06703 . pdf 阅读 理解 Open domain QA 去年 
到 今年初 MRC 取得 的 进展 大家 已经 有目共睹 了 
最高 表现 的 架构 基本 趋同 估计 再 刷下去 就要 
达到 super human performance 了 人类 的 baseline 是 82 
EM 91 F1 比较 有意思 的 是 大家 基本上 都 
放弃 了 multi hop reasoning 的 结构 原因 非常 简单 
Stanford 的 SQuAD 跟 FB 的 bAbI 不 一样 没有 
专门 设立 这种 需要 推理 的 项目 诸如 John went 
to the hall John putdown the ball Where is the 
ball 这类 问题 大 部分 的 问题 主要 依赖 Attention 
机制 就 可以 抓得 很好 了 bAbI 这样 的 伪 
推理 看来 大家 也是 受够了 但是 SQuAD 本身 也 存在 
很多 问题 抛开 细 的 面 不说 cloze style 本来 
就 有 很大 的 问题 而且 最近 出现 了 海量 
的 刷 SQuAD 的 文章 品质 老实 说 并不 敢 
恭维 幸好 Stanford 的 Chen Danqi 大神 的 Reading Wikipedia 
to Answer Open Domain Questions http / / cs . 
stanford . edu / people / danqi / papers / 
acl2017 . pdf 打开 了 很多 的 方向 通过 海量 
阅读 machine reading at scale 这篇文章 试图 回答 所有 在 
wikipedia 上 出现 的 factoid 问题 其中 有 大量 的 
工程 细节 在此 不表 仅 致敬意 Unsupervised Learning 在 分布式 
语义 表示 这个 传统 深度 学习 领域 2013年 算是 很 
传统 了吧 主要 的 工作 还是 向下 向上/d 和向/nr 周边/f 
扩展/v 不小 心说 了 句 废话 向下 是 指 sub 
word level Enriching Word Vectors with Subword Information https / 
/ arxiv . org / abs / 1607.04606 向上 当然 
就是 句子 ／ 篇章 级 别了 A Simple but Tough 
to Beat Baseline for Sentence Embeddings https / / openreview 
. net / pdf id = SyK00v5xx 向 周边 呢 
就是 面向 任务 譬如 知识 库里 的 entity embedding 或者 
面向 sentiment analysis 的 情感 embedding 好吧 我 承认 这类 
的 文章 真的 已经 看 得 太多 了 并 没有 
太大 新意 no offense 我 知道 无论如何 boring 的 文章 
背后 都是/nr 大家 不 眠 不休 的 心血 NLG 通过 
RNN language model 来做 语言 生成 已经 很 成熟 了 
这 都 已经 出 survey paper 了 Survey of the 
State of the Art in Natural Language Generation Core tasks 
applications and evaluation https / / arxiv . org / 
pdf / 1703.09902 . pdf 但是 通过 GAN ／ VAE 
来 生成 呢 当然 做 这个 方向 的 人也 很多 
比如 MSRA 的 Adversarial Neural Machine Translation https / / 
arxiv . org / abs / 1704.06933 和 Li Jiwei 
的 Adversarial Learning for Neural Dialogue Generation https / / 
arxiv . org / pdf / 1701.06547 . pdf 不过 
认真 地 说 我 同意 Ian Goodfellow 在 Reddit 里 
说 的 GANs have not been applied to NLP because 
GANs are only defined for real valued data . https 
/ / www . reddit . com / r / 
MachineLearning / comments / 40ldq6 / generative _ adversarial _ 
networks _ for _ text / 当然 做 一些 twist 
当然 是 可以 强行 让 它 work 的 或者 用 
VAE 但是 目前 看来 这些 生成 模型 在 自然 语言 
方面 并 没有 在 图像 方面 的 显著 疗效 更 
重要 的 是 目前 NLG 的 重要 课题 不是 生成 
的 质量 而是 要 搞清楚 想 说什么 类比 一下 就 
如同 就算 人脑 的 Broca 区域 没有问题 可是 Wernicke 区域 
出现 了 问题 那么 病人 会 说 一口 流利 的 
语言 可是 每 一句话 都 毫无 意义 这样 的 生成 
当然 也 是 毫无 意义 的 了 所以 这个 领域 
其实 还是 任重道远 啊 目前 的 很多 自然语言 生成 或 
写稿 机器人 还是 carefully crafted 的 模版 来 的 多 
总结 通过 2016年 的 努力 deep learning 在 NLP 领域 
已经 站稳 了 脚跟 我们 期待 2017年 语言学 和 机器 
学习 的 进一步 结合 让 机器 更加 聪明 更 懂 
你 欢迎 大家 补充 讨论 本 回答 来自 竹 间 
智能 Emotibot 机器学习 科学家 赵 宁远   编辑 于 2017 
08 23 赞同 331 44 条 评论 分享 收藏 感谢 
收起 更多 回答 知乎 用户 深度 学习 Deep Learning 话题 
的 优秀 回答者 收录于 知乎 圆桌 68 人 赞同 了 
该 回答 深度 学习 目前 已经 在 NLP 领域 站稳脚跟 
但是/c 还/d 没有/v 成/n 熟到/v 像/v 语音/n 和/c 图像/n 那样/r 
可以/c 和/c 人类/n PK/w 的/uj 程度/n 所以 目前 还是 上升期 
有 三个点 非常 值得 关注 1 . 深度 学习 最初 
进入 NLP 走 的 是 端 到 端的 路线 靠 
无需 人工 特征 知识 即可 达到 state of art 的 
卖点 在 NLP 站稳脚跟 但是 后面 的 发展 过程 中 
大家 逐渐 发现 只靠 端 到 端 是 不行 的 
这个 不 仅仅 是 因为 很多 NLP 任务 监督 数据 
匮乏 的 问题 因为 在 机器 翻译 这种 语料 充足 
的 任务 中 纯 端 到 端 的 方法 有 
很快 遇到 了 瓶颈 所以 大家 把 目光 重新 投向 
传统 方法 和 传统 方法 结合 借助 外部 知识 来 
提高 端 到 端 模型 的 表现 逐渐 成为 主流 
机器翻译 作为 NLP 领域 中 深度 学习 应用 最 成熟 
的 方向 这 一点 尤为 明显 2017年 这种 结合 的 
思路 应该 会 进一步 发展 并向 对话 摘要 等 其他 
NLP 任务 扩展 同时 也 会有 更多 通用 的 结合 
方法 出现 2 . 强化 学习 开始 在 NLP 发力 
AlphaGo 的 成功 带来 了 强化 学习 的 一轮 热潮 
大家 很 自然 的 会 考虑 用 强化 学习 解决 
NLP 中 的 一些 问题 多轮 对话 是 强化 学习 
非常 自然 的 应用 场景 而 chatbot 的 火热 又 
在里面 添了 一把 柴 另外 强化 学习 在 信息检索 这种 
传统 场景 中 的 应用 也 值得 关注 如 多轮 
搜索 还有 一点 就是 文本 生成 目前 文本 生成 还是 
很 初级 的 阶段 而 文本 生成 是 可以 看做 
一个 马尔可夫 决策 过程 用 强化 学习 解决 的 因此 
很 期待 后续 强化 学习 在 这 方面 的 应用 
3 . GAN 在 NLP 开始 发力 GAN 在 图像 
领域 取得 巨大 成功 大家 很 自然 的 会 想到 
把 对抗 的 思路 引入 到 NLP 领域 目前 除了 
在 文本 生成 任务 中 结合 GAN 和 强化 学习 
的 应用 之外 多任务 学习 中 也有 对抗 思想 的 
引入 目前 GAN 在 NLP 中 的 效果 还 很 
一般 这个 主要 受制于 NLP 离散 特性 带来 的 梯度 
学习 困难 因此 GAN 算法 针对 离散 场景 的 改进 
是 一个 很 有价值 的 研究 方向 而 由此 带来 
的 NLP 任务 突破 则 很 值得 期待 另一方面 GAN 
由于 刚刚 进入 NLP 领域 它 的 生成 模型 部分 
一般 直接 套用 现有 的 复杂 模型 而 判别 模型 
则 比较 简单 因此 有 很大 改进 空间 例 如从 
matching 方向 借鉴 一些 更 复杂 的 匹配 模型 先写 
到 这里 后续 想到 了 再 补充 编辑 于 2017 
05 15 赞同 68 6 条 评论 分享 收藏 感谢 
刘知远 用户 标识 自然语言 处理 深度 学习 Deep Learning 机器学习 
话题 的 优秀 回答者 63 人 赞同 了 该 回答 
2016年 回答 过 这个 题目 现在 看来 似乎 并 没有 
完全 答对 也许 这 就是 科研 创新 的 魅力 所在 
就像 一盒 巧克力 打开 前 永远 不 知道 它 的 
口味 是 什么 2017年 已经 将近 过半 其实/d 有/v 一些/m 
迹象/n 已经/d 可以/c 从/p ACL/w 2017 等会 议论文 窥豹一斑 我 
觉得 2017年 的 发展 将 体现 在 以下 几个 方面 
先验 语言 知识 与 深度 学习 模型 的 有机 融合 
从 ACL 2017 上 NMT 的 相关 论文 可以 看到 
学者 们 纷纷 将 各种 语言 知识 如 句法 等 
应用到 NMT 模型 中 进一步 提升 机器翻译 效果 该 思路 
应该 具有 一定 普适性 对抗 训练 思想 的 应用 虽然 
GAN 本身 尚未 在 NLP 各 领域 得到 广泛 验证 
但 对抗 训练 思想 已经 在 NMT 等 模型 中 
开始 发挥 重要 作用 值得 关注 其他 稍后 想到 了 
继续 补充 17 . https / / zhuanlan . zhihu 
. com / p / 35041012 注意力 机制 Attention 机制 
最早 是 在 视觉 图像 领域 提 出来 的 应该 
是 在 九 几年 思想 就 提出 来了 但是 真正 
火 起来 应该 算是 2014 年 Google Mind 团队 的 
这篇 论文 Recurrent Models of Visual Attention 他们 在 RNN 
模型 上 使用 了 Attention 机制 来 进行 图像 分类 
随后 Bahdanau 等人 在 论文 Neural Machine Translation by Jointly 
Learning to Align and Translate 中 使用 类似 Attention 的 
机制 在 机器 翻译 任务 上将 翻译 和 对齐 同时 
进行 他们 的 工作 算是 第一个 将 Attention 机制 应用到 
NLP 领域 中 接着 Attention 机制 被 广泛 应用 在 
基于 RNN / CNN 等 神经 网络 模型 的 各种 
NLP 任务 中 2017 年 Google 机器翻译 团队 发表 的 
Attention is All You Need 中 大量 使用 了 自 
注意力 self attention 机制 来 学习 文本 表示 自 注意力 
机制 也 成为 了 大家 近期 的 研究 热点 并在 
各种 NLP 任务 上 进行 探索 18 . https / 
/ www . zhihu . com / question / 24417961 
/ answer / 66872781 自然语言 处理 有一套 严整 的 理论 
体系 如果 希望 系统 学习 可以 参考 Stanford NLP Group 
几位 教授 的 三本 教科书 基本 都有 中文翻译 版本 以下 
按照 我 心目中 的 浅易 程度 排序 Christopher D . 
Manning Prabhakar Raghavan and Hinrich Sch ü tze . 2008 
. Introduction to Information Retrieval . Cambridge University Press . 
Christopher D . Manning and Hinrich Sch ü tze . 
1999 . Foundations of Statistical Natural Language Processing . Cambridge 
MA MIT Press . Daniel Jurafsky and James H . 
Martin . 2008 . Speech and Language Processing An Introduction 
to Natural Language Processing Speech Recognition and Computational Linguistics . 
2nd edition . Prentice Hall . 19 . https / 
/ www . zhihu . com / question / 57057613 
/ answer / 151471222Natural Language Toolkit 和 中文分词 fxsjy / 
jiebagensim word2vecnltk tokenization s e g m e n t 
a t i o n k e r a s 
sequence to sequence learning20 . https / / zhuanlan . 
zhihu . com / p / 25889937 自然语言 处理 系列 
篇 关键词 智能 提取 21 https / / zhuanlan . 
zhihu . com / p / 30138012 自然语言 处理 领域 
重要 论文 & 资源 全 索引 22 . https / 
/ zhuanlan . zhihu . com / p / 25612011 
机器学习 深度 学习 与 自然 语言 处理 领域 推荐 的 
书籍 列表 23 . https / / zhuanlan . zhihu 
. com / p / 28616862 自然语言 处理 从 入门 
到 进阶 资 代码 资源库 汇总 随时 更新 本文 为 
SIGAI 原创 文章 仅供 个人 学习 使用 未经 允许 不能 
用于 商业 目的 欢迎 搜索 关注 微信 公众 号 SIGAI 
获取 更多 原创 干货 导言 循环 神经 网络 是 一种 
具有 记忆 功能 的 神经 网络 适合 序列 数据 的 
建模 它 在 语音 识别 自然语言 处理 等 领域 取得 
了 成功 是 除 卷积 神经网络 之外 深度 学习 中 
最 常用 的 一种 网络结构 在 本文 中 SIGAI 将 
和 大家 一起 回顾 循环 神经 网络 的 发展 历程 
与 在 各个 领域 的 应用 序列/n 数据/n 建模/n 全/a 
连接/v 网络/n 和/c 卷积/n 网络/n 在/p 运行/v 时/n 每次/r 接收/v 
的/uj 都是/i 独立/v 的/uj 输入/v 数据/n 没有 记忆 能力 在 
有些 应用 中 需要 神经网络 具有 记忆 功能 典型 的 
是 时间 序列 预测 问题 时间 序列 可以 抽象 的 
表示 为 一个 向量 序列 这里 的 下标 表示 时刻 
神经网络 每个 时刻 接收 一个 向量 输入 不同 时刻 的 
向量 之间 存在 关系 每个 时刻 的 向量 与 更早 
时刻 的 向量 相关 例如 在/p 说话/v 时/n 当前/t 要说/c 
的/uj 词/n 和/c 之前/f 所说/c 的/uj 词/n 之间/f 相关/v 依赖于 
上下文 语境 我们 需要 根据 输入 序列 来 产生 输出 
向量 这类 问题 称为 序列 预测 问题 输入 序列 的 
长度 可能 不 固定 语音 识别 与 自然 语言 处理 
的 问题 是 这类 序列 预测 问题 的 典型 代表 
前者 的 输入 是 一个 时间 序列 的 语音信号 后者 
是 文字 序列 下面 我们 用 一个 实际 例子 来 
说明 序列 预测 问题 假设 神经网络 要用 来 完成 汉语 
填空 考虑 下面 这个 句子 现在 已经 半夜 12 点了 
我 非常 困 想 回家 _ _ 最佳答案 是 睡觉 
或者 休息 这个 答案 需要 根据 上下文 理解 得到 在 
这里 神经网络 每次 的 输入 为 一个 词 最后 要 
填 出 这个 空 这 需要 网络 能够 理解 语义 
并 记住 之前 输入 的 信息 即 语句 上下文 这里 
需要 神经网络 具有 记忆 功能 能够 根据 之前 的 输入 
词序 列计 出 当前 使用 哪个 词 的 概率 最大 
如何 设计 一个 神经 网络 满足 上面 的 要求 答案 
就是 我们 接下来 要 介绍 的 循环 神经网络 循环 层 
的 工作 原理 循环 神经网络 简称 RNN 1 会 记住 
网络 在 上 一个 时刻 的 输出 值 并 将该 
值 用于 当前 时刻 输出 值 的 生成 这 由 
循环 层 实现 RNN 的 输入 为 前面 介绍 的 
向量 序列 每个 时刻 接收 一个 输入 网络 会 产生 
一个 输出 而 这个 输出 是由 之前 的 序列 共同 
作用 决定 的 假设 t 时刻 循环 层 的 状态值 
为     它 由上 一 时刻 的 状态值 以及 
当前 时刻 的 输入 值 共同 决定 即 这 是 
一个 递推 关系式 现在 的 问题 是 确定 这个 表达式 
的 具体 形式 即将 上 一 时刻 的 状态 值 
与 当前 时刻 的 输入 值 整合 到 一起 在 
全 连接 神经 网络 中 神经元 的 输出 值 是 
对 输入 值 进行 加权 然后 用 激活 函数 进行 
作用 得到 输出 在 这里 我们 可以 对 上一 时刻 
的 状态 值 当前 时刻 的 输入 值 进行 类似 
的 处理 即将 它们 分别 都 乘以 权重 矩阵 然后 
整合 起来 整合 可以 采用 加法 也 可以 采用 乘法 
或者 更 复杂 的 运算 最 简单 的 是 加法 
乘法 在 数值 上 不稳定 多次 乘积 之后 数 为 
变得 非常 大 或者 非常 小 显然 这里 需要 两个 
权重 矩阵 分别 作用于 上一 时刻 状态值 当前 时刻 的 
输入 值 由此 得到 下面 的 递推 关系式 其中 W 
为 权重 矩阵 b 为 偏置 向量 和全/nr 连接/v 神经网络/n 
相比/v 这里 只是 多 了 一个 项 它 意味着 在 
实现 循环 神经 网络 的 时候 需要 用 变量 记住 
隐含 层 上次 的 输出 值 使用 激活 函数 的 
原因 在 SIGAI 之前 公众 号 的 文章 中 介绍 
过 是 为了 确保 非线性 下面 我们 用 示意图 来 
表示 一个 隐含 层 的 变换 在上 图中     
和 /nr 共同 决定         体现 了 记忆 
功能 而 它 的 值 又 是由     和 /nr 
决定 因此 的 值 实际上 是由     决定 的 
它 记住 了 之前 完整 的 序列 信息 需要 强调 
的 是 权重 矩阵     并 不会 随着 时间 
变化 而是 固定 的 即在 每个 时刻 进行 计算 时 
使用 的 是 同一个 矩阵 这样 做 的 好处 一方面 
是 减少 了 模型 参数 另一方面 也 记住 了 之前 
的 信息 如果 把 每个 时刻 的 输入 和 输出 
值 按照 时间 线 展开 如下 图 所示 网络结构 最 
简单 的 循环 神经网络 由 一个 输入 层 一个 循环 
层 一个 输出 层 组成 输出 层 接收 循环 层 
的 输出 值 作为 输入 并 产生 输出 它 不具有 
记忆 功能 输出 层 实现 的 变换 为 函数 g 
的 类型 根据 任务 而定 对于 分类 任务 一般 选用 
softmax 函数 输出 各个 类 的 概率 结合 循环 层 
和 输出 层 循环 神经网络 完成 的 变换 为 在 
这里 只 使用 了 一个 循环 层 和 一个 输出 
层 实际/n 使用/v 时/n 可以/c 有/v 多个/m 循环/vn 层/q 即 
深度 循环 神经网络 在下 一节 中会 详细 介绍 深层 网络 
上 面 我们 介绍 的 循环 神经网络 只有 一个 输入 
层 一个 循环 层 和 一个 输出 层 这 是 
一个 浅层 网络 和全/nr 连接/v 网络/n 以及/c 卷积/n 网络/n 一样/r 
我们 可以 把 它 推广 到 任意 多个 隐含 层 
的 情况 得到 深度 循环 神经网络 11 这里有 3种 方案 
第 一种 方案 为 Deep Input to Hidden Function 在 
循环 层 之前 加入 多个 普通 的 前馈 层 将 
输入 向量 进行 多层 映射 之后 再 送入 循环 层 
进行 处理 第二 种 方案 是 Deep Hidden to Hidden 
Transition 它 使用 多个 循环 层 这 和 前馈 型 
神经网络 类似 唯一 不同 的 是 计算 隐含 层 输出 
的 时候 需要 利用 本 隐含 层 在上 一个 时刻 
的 输出 值 第三 种 方案 是 Deep Hidden to 
Output Function 它 在 循环 层 到 输出 层 之间 
加入 多 前馈 层 这 和 第一 种 情况 类似 
由于 循环 层 一般用 tanh 作为 激活 函数 层次 过多 
之后 会 导致 梯度 消失 问题 和 残差 网络 类似 
可以 采用 跨 层 连接 的 方案 在 语音 识别 
自然语言 处理 问题 上 我们 会 看到 深层 循环 神经 
网络 的 应用 实验 结果 证明 深层 网络 比 浅层 
网络 有 更好 的 精度 训练 算法 前面 我们 介绍 
了 循环 神经 网络 的 结构 接下来 要 解决 的 
问题 是 网络 的 参数 如何 通过 训练 确定 由于 
循环 神经 网络 的 输入 是 时间 序列 因此 每个 
训练样本 是 一个 时间 序列 包含 多个 相同 维度 的 
向量 解决 循环 神经网络 训练 问题 的 算法 是 Back 
Propagation Through Time 算法 简称 BPTT 2 4 原理 和 
标准 的 反向 传播 算法 类似 都是/nr 建立/v 误差/n 项的/nr 
递推公式/i 根据 误差 项 计算出 损失 函数 对 权重 矩阵 
偏置 向量 的 梯度 值 不同 的 是 全 连接 
神经 网络 中 递推 是 在 层 之间 建立 的 
而 这里 是 沿着 时间轴 建立 的 限于 篇幅 在 
这里 我们 不 详细 介绍 和 推导 BPTT 的 原理 
如果 有 机会 SIGAI 会在 后续 的 公众 号 文章 
中 给出 挑战 与 改进 措施 循环 神经 网络 与 
其他 类型 的 神经 网络 共同 要 面对 的 是 
梯度 消失 问题 对此 出现 了 一些 解决 方案 如 
LSTM 等 相比 卷积 神经网络 循环 神经 网络 在 结构上 
的 改进 相对 要 少 一些 梯度 消失 问题 和 
前馈 型 神经网络 一样 循环 神经 网络 在 进行 梯度 
反向 传播 时也/nr 面临 着 梯度 消 失和 梯度 爆炸 
问题 只不过 这种 消逝 问题 表现 在 时间 轴上 即 
如果 输入 序列 的 长度 很长 我们 很难 进行 有效 
的 梯度 更新 对这 一 问题 的 解释 和 理论 
分析 SIGAI 会在 以后 的 文章 中 给出 文献 5 
对 循环 神经网络 难以 训练 的 问题 进行 了 分析 
进一步 的 文献 6 对这 一 问题 作出 了 更深 
的 解释 并给 出了 一种 解决方案 LSTM 长短期 记忆 模型 
long short time memory 简称 LSTM 由 Schmidhuber 等 人在 
1997年 提出 7 与 高速公路 网络 highway networks 有 异曲同工 
之妙 它 对 循环 层 进行 改造 具体 方法 是 
使用 输 入门 遗忘 门 输 出门 3个 元件 通过 
另外 一种 方式 由     计算 LSTM 的 基本 
单元 称为 记忆 单元 它 记住 了 上 一个 时刻 
的 状态值 记忆 单元 在 t 时刻 维持 一个 记忆 
值     循环 层 状态 的 输出 值 计算公式 
为 即 输 出门 与 状态值 的 乘积 在 这里 
是 向量 对应 元素 相乘 其中     为 输 
出门 是 一个 向量 按照 如下 公式 计算 其中   
  为 sigmoid 函数 后面 公式 中 含义 相同 输 
出门 控制 着 记忆 单元 中 存储 的 记忆 值 
有 多大 比例 可以 被 输出 使用 sigmoid 函数 这 
是因为 它 的 值域 是 0 1 这样 的 所有 
分量 的 取值 范围 都在 0 和1/nr 之间 它们 分别 
与 另外 一个 向量 的 分量 相乘 可以 控制 另外 
一个 向量 的 输出 比例 分别为 输 出门 的 权重 
矩阵 和 偏置 向量     是 输 出门 的 
权重 矩阵 和 偏置 项 这里 参数 通过训练 得到 记忆 
值     是 循环 层 神经元 记住 的 上 
一个 时刻 的 状态值 随着 时间 进行 加权 更新 它 
的 计算 公式 为 其中     是 遗忘 门 
    是 记忆 单元 在上 一 时刻 的 值 
遗忘 门 决定 了 记忆 单元 上一 时刻 的 值 
有 多少 会 被 传到 当前 时刻 上式 表明 记忆 
单元 当前 值 是 上 时刻 值 与 当前 输入 
值 的 加权 和 记忆 值 只是 个 中间值 遗忘 
门 的 计算 公式 为 这里 也 使用 了 sigmoid 
函数     分别为 遗忘 门 的 权重 矩阵 和 
偏置 向量     是 输 入门 控制 着 当前 
时刻 的 输入 有 多少 可以 进入 记忆 单元 其 
计算 公式 为 其中     分别为 输 入门 的 
权重 矩阵 和 偏置 向量 这 3个 门 的 计算 
公式 都是 一样 的 分别 使用 了 自己 的 权重 
矩阵 和 偏置 向量 这 3个 值 的 计算 都用 
到了     和  /nr 它们 起到 了 信息 的 流量 
控制 作用 隐含 层 的 状态值 由 遗忘 门 记忆 
单元 上一 时刻 的 值 以及 输 入门 输 出门 
共同 决定 除掉 3个 门 之外 真正 决定     
的 只有     和  /nr 总结 起来 LSTM 的 计算 
思路 为 输 入门 作用于 输入 信息 遗忘 门 作用于 
之前 的 记忆 信息 二者 加权 和 得到 汇总 信息 
最后 通过 输出 门 决定 输出 信息 所有 的 权重 
矩阵 偏置 向量 都 通过 训练 得到 这 和 普通 
的 循环 神经网络 没有 区别 根据 BPTT 算法 我们 可以 
得到 这些 参数 的 梯度 值 在 这里 不 详细 
介绍 GRU 门控 循环 单元 8 Gated Recurrent Units 简称 
GRU 是 解决 循环 神经网络 梯度 消失 的 另外 一种 
方法 它 也 是 通过 门 来 控制 信息 的 
流动 和 LSTM 不同 的 是 它 只 使用 了 
两个 门 把 LSTM 的 输入 门 和 遗忘 门合 
并成 更新 门 在 这里 我们 不 详细 介绍 计算公式 
感 兴趣 的 读者 可以 阅读 参考文献 双向 网络 前面 
介绍 的 循环 神经 网络 是 单向 的 每 一个 
时刻 的 输出 依赖于 比 它 早 的 时刻 的 
输入 值 这 没有 利用 未来 时刻 的 信息 对于 
有些 问题 当前 时刻 的 输出 不仅 与 过去 时刻 
的 数据 有关 还与 将来 时刻 的 数据 有关 为此 
Schuster 等人 设计 了 双向 循环 神经网络 9 它/r 用/p 
两个/m 不同/a 的/uj 循环/vn 层/q 分别/d 从/p 正向/d 和/c 反向/v 
对/p 数据/n 进行/v 扫描/v 正向 传播 时的/nr 流程 为 1 
. 循环 对 t = 1 . . . T 
用 正向 循环 层 进行 正向 传播 记住 每 一个 
时刻 的 输出 值 结束 循环 2 . 循环 对对 
t = T . . . 1 用反 向 循环 
层 进行 正向 传播 记住 每 一个 时刻 的 输出 
值 结束 循环 3 . 循环 对 所有 的 t 
可以 按照 任意 顺序 进行 计算 用 正向 和 反向 
循环 层 的 输出 值 作为 输出 层 的 输入 
计算 最终 的 输出 值 结束 循环 下面 用 一个 
简单 的 例子 来 说明 假设 双向 循环 神经 网络 
的 输入 序 列为     首先 用 第一 个 
循环 层 进行 正向 迭代 得到 隐含 层 的 正向 
输出 序列 在 这里 由 x1 决定 由 x1 x2 
决定 由 x1   . . . x3 决定 由 
x1   . . . x4   决定 即 每个 
时刻 的 状态 值 由 到 当前 时刻 为止 的 
所有 输入 值 序列 决定 这里 利用 的 是 序列 
的 过去 信息 然后 用 第二 个 循环 层 进行 
反向 迭代 输入 顺序 是 x4   . . . 
x1 得到 隐含 层 的 反向 输出 序列 在 这里 
由 x4 决定 由 x4 x3 决定 由 x4 . 
. . x2   决定 由 x4 . . . 
x1 决定 即 每个 时刻 的 状态 值 由 它 
之后 的 输入 序列 决定 这里 利用 的 是 序列 
未来 的 信息 然后 将 每个 时刻 的 隐含 层 
正向 输出 序列 和 反向 输出 序列 合并 起来 送入 
神经 网络 中 后面 的 层 进行 处理 此时 各个 
时刻 的 处理 顺序 是 随意 的 可以 不用 按照 
输入 序列 的 时间 顺序 多维 网络 循环 神经 网络 
的 另外 一个 改进 是 拓展 到 多维 的 情况 
得到 多维 网络 限于 篇幅 在 这里 不 详细 介绍 
感 兴趣 的 读者 可以 阅读 参考文献 18 序列 预测 
问题 序列 预测 问题 是 一类 问题 的 抽象 它 
的 输入 是 一个 序列 输出 也 是 一个 序列 
而且 输入 和 输出 序列 的 长度 是 不 固定 
的 这是 循环 神经 网络 最 擅长 处理 的 问题 
之一 序列 标注 问题 序列 标注 问题 12 是 一个 
抽象 的 概念 它 泛指 将 一个 序列 数据 映 
射成 另外 一个 序列 的 任务 其 本质 是 根据 
上下文 信息 对 序列 每个 时刻 的 输入 值 进行 
预测 典型 的 序列 标注 问题 包括 语音识别 机器翻译 词性 
标注 等 对于 语音识别 问题 输入 数据 是 语音信号 序列 
输出 是 离散 的 文字 序列 对于 机器翻译 问题 输入 
是 一种 语言 的 语句 即 单词 序列 输出 是 
另外 一种 语言 的 单词 序列 对于 词性 标注 问题 
输入 是 一句话 的 单词 序列 输出 是 每个 单词 
的 词性 如 名词 动词 与 普通 的 模式 分类 
问题 相比 序列 标注 问题 最 显著 的 区别 是 
输入 序列 数据 的 数据 点 之间 存在 相关性 输出 
的 序列 数据 的 数据 点 之间 也 存在 相关性 
例如 对于 语音识别 问题 一句话 的 语音 信号 在 各个 
时刻 显然 是 相关 的 识别 的 结果 单词 序列 
组成 各个 单词 之间 显然 也 具有 相关性 它们 必须 
符合 词法 和 语法 规则 序列 标注 问题 的 一个 
困难 之 处 在于 输入 序列 和 输出 序列 之间 
的 对齐 关系 是 未知 的 以 语音识别 问题 为例 
输入 语音信号 哪个 时间段 内 的 数据 对应 哪个 单词 
的 对应 关系 在 进行 识别 之前 并不 知道 我们 
不 知道 一个 单词 在 语音 信号 中 的 起始 
时刻 和 终止 时刻 循环 神经网络 因为 具有 记忆 功能 
因此 特别 适合 于 序列 标注 任务 但是 循环 神经 
网络 在 处理 这类 任务 时 面临 几个问题 第一 个 
问题 是 标准 的 循环 神经 网络 是 单向 的 
但 有些 问题 不仅 需要 序列 过去 时刻 的 信息 
还 需要 未来 时刻 的 信息 例如 我们 要 理解 
一个 句子 中 的 某个 词 它 不仅 与 句子 
中 前面 的 词 有关 还和 后门 的 词 有关 
即 所谓 的 上下文 语境 解决 这个 问题 的 方法 
是 上面 介绍 的 双向 循环 神经网络 第二 个 问题 
是 循环 神经 网络 的 输出 序列 和 输入 序列 
之间 要 对齐 即 每 一个 时刻 的 输出 值 
与 输入 值 对应 而 有些 问题 中 输入 序列 
和 输出 序列 的 对应 关系 是 未知 的 典型 
的 是 语音识别 问题 这在 前面 已经 介绍 解决 这个 
问题 的 经典 方法 是 连接 主义 时序 分类 即 
Connectionist Temporal Classification 简称 CTC 根据 输入 序列 和 输出 
序列 的 对应 关系 我们 可以 将 序列 标注 问题 
分为 三类 第一类 为 序列 分类 问题 它 给 输入 
序列 赋予 一个 类别 标签 即 输出 序列 只有 一个 
值 因此 输出 序列 的 长度 为 1 第二 类 
问题 为 段 分类 问题 输入 序列 被 预先 分成 
了 几段 每段 为 一个 序列 为 每 一段 赋予 
一个 标签 值 显然 第一 种 问题 是 第二 种 
问题 的 一个 特例 第三 类 问题 为 时序 分类 
问题 对于 这 类 问题 输入/v 序列/n 和/c 输出/v 序列/n 
的/uj 任何/r 对齐/d 方式/n 都是/nr 允许/v 的/uj 显然 第二 类 
问题 是 第三 类 问题 的 一个 特例 因此 这 
3类 问题是 层层 包含 关系 三类 问题 的 关系 如下 
图 所示 CTC 循环 神经 网络 虽然 可以 解决 序列 
数据 的 预测 问题 但 它 要求 输入 的 数据 
是 每个 时刻 分割 好 并且 计算 得到 的 固定 
长度 的 特征向量 对于 有些 问题 对 原始 的 序列 
数据 进行 分割 并 计算 特征向量 存在 困难 典型 的 
是 语音识别 原始 的 声音 信号 我们 很难 先 进行 
准确 的 分割 得到 每个 发音 单元 所 对应 的 
准确 的 时间 区间 解决 这 类 问题 的 一种 
典型 方法 是 CTC 技术 CTC 13 是 一种 解决 
从未 分段 的 序列 数据 预测 标签 值 的 通用 
方法 在 这里 不 要求 将 输入 数据 进行 分割 
之后 再 送入 循环 神经 网络 中 预测 2014年 Graves 
等人 将 这一 方法 用于 语音识别 问题 14 通过 和 
循环 神经网络 整合 来 完成 语音识别 任务 CTC 解决 问题 
的 关键 思路 是 引入 了 空白符 以及 消除 重复 
以及 用 一个 映射函数 将 循环 神经 网络 的 原始 
输出 序列 映射 为 最终 需要 的 标签 序列 假设 
训练样本 集为 训练样本 服从 概率分布     输入 空间 是 
输入 序列 的 集合 定义 为 这是 所有 m 维 
实 向量 序列 的 集合 目标 空间 是 我们 需要 
的 预测 结果 序列 的 集合 定义 为 这 是 
建立 在 包含 有限 个 字母 集 L 之上 的 
标签 序列 的 集合 我们 将 L * 中的 元素 
称为 标签 序列 对于 语音识别 L 是 文字 字典 L 
* 是 识别 出来 的 句子 训练样本 集中 的 每个 
样本 是 一个 序列 对 x z 其中 输入 序 
列为 目标 序 列为 这 有 一个 约束条件 目标 序列 
的 长度 不大于 输入 序列 的 长度 即     
由于 输出 序列 的 长度 与 输入 序列 的 长度 
可能 不相等 因此 无法 用 先验 知识 将 它们 对齐 
即 让 输出 序列 的 某些 元素 和 输入 序列 
的 某一个 元素 对应 起来 我们 的 目标 是 用 
训练样本 集 训练 一个 时序 分类器 然后 用 它 对 
新的 输入 序列 进行 分类 分类 时 要让 定义 的 
某种 误差 最小化 要 使用 循环 神经 网络 对 时序 
数据 进行 分类 其中 关键 的 步是将/nr 循环 神经 网络 
的 输出 值 转换成 某一个 序列 的 条件 概率值 这样 
我们 通过 寻找 使得 这个 条件概率 最大化 的 输出 序列 
来 完成 对 输入 序列 的 分类 CTC 网络 的 
输出 层 为 softmax 层 如果 标签 字母 集中 的 
字母 个数 为 | L | 则 这 一层 有|L/nr 
| + 1个 神经元 其中 前 | L | 个 
神经元 表示 在 某一个 时刻 输出 标签 为 每一个 标签 
字母 的 概率 最后 一 个 神经元 的 输出 值 
为 输出 标签 值 为 空的/nr 概率 即 没有 标签 
输出 这样 softmax 层 在 各个 时刻 的 输出 值 
合并 在 一起 定义 了 各种 可能 的 输出 标签 
序列 和 输入 序列 进行 对齐 的 方式 的 概率 
任何 一个 标签 序列 的 概率值 可以 通过 对 其 
所有 不同 的 对齐 方式 的 概率 进行 求和 得到 
假设 输入 序列 的 长度 为 T 循环 神经 网络 
的 输入 数据 为 m 维 输出 向量 为 n 
维 权重 向量 为 w 它 实现 了 如下 的 
映射 我们 将 网络 的 映射 写成     其中 
y 是 输出 序列 在 t 时刻 网络 第 k 
个 输出 单元 的 值 为     在 这里 
可以 将 解释为 在 t 时刻 观测 标签 k 的 
概率 这个 概率值 定义 了 集合     中 长度 
为 T 的 序列 所 服从 的 概率分布 其中   
  { blank } 其中 blank 为 空白 符号 即 
在 这里 我们 将 集合 中的 元素 称为 路径 path 
记为     接下来 我们 定义 一个 多对一 的 映射 
将 神经 网络 的 输出 序列 映射 为 最终 需要 
的 标签 值 序列 其中     是 所有 可能 
的 输出 标签 序列 的 集合 即由 字母 集合 中的 
字母 组成 的 长度 小于 等于 T 的 序列 的 
集合 从/p 神经/n 网络/n 的/uj 输出/v 序列/n  /i  /i 得到/v 
目标/n 标签/n 序列/n 的/uj 做法/v 是/v 消除/v 空白符/n 和/c 连续/a 
的/uj 重复/d 标签/n 值/n 下面 来看 B 函数 作用 于 
一个 序列 的 例子 其中 为 空白 符号 由于 与 
一个 标签 序列 对应 的 路径 不止 一个 因此 目标 
标签 序列 的 条件概率 应该 等于 能 得到 它 的 
所有 路径 的 条件 概率 之和 我们 借助 映射 B 
来 定义 一个 标签 序列     的 条件 概率 
它 等于 所有 映射 后为 l 的 路径     
的 概率 之和 下面 用 一个 简单 的 例子 进行 
说明 如果 标签 字母 集 合为 { a b c 
} 路径 的 序列 长度 为 4 标签 序列 的 
长度 为 3 则 标签 序列 l = abc 所 
对应 的 所有 可能 路径     为 总共有 7条 
路径 和 一个 标签 序列 对应 基于 上面 的 定义 
CTC 分类器 的 分类 结果 是 给定 输入 序列 寻找 
上面 的 条件 概率 最大 的 那个 输出 序列 在 
这里 需要 解决 如何 找到 概率 最大 的 输出 序列 
的 问题 而 前面 定义 的 框架 只是 计算 给定 
的 输出 序列 的 条件 概率 采用 和隐/nr 马尔可夫 模型 
类似 的 概念 我们 称 这一 过程 为 解码 它们 
都是 要 得到 概率 最大 的 序列 值 直接 暴力 
枚举 计算 量 太大 这里 采用 了 动态规划 建立 递推公式 
进行 计算 限于 篇幅 我们 不能 详细 介绍 在 后面 
的 文章 中 SIGAI 将 对此 展开 讲解 seq2seq 对 
有些 问题 输入 序列 的 长度 和 输出 序列 不一定 
相等 而且 我们 事先 并不 知道 输出 序列 的 长度 
典型 的 是 机器 翻译 问题 以 机器翻译 为例 将 
一种 语言 的 句子 翻译成 另外 一种 语言 之后 句子 
的 长度 即 包括 的 单词 数量 一般 是 不相等 
的 以 英译汉 为例 英文 句子 what s your name 
是 3个 单词 组成 的 序列 翻译 成 中文 为 
你 叫 什么 名字 由 4个 汉 字词 组成 标准 
的 RNN 没法 处理 这种 输入 序列 和 输出 序列 
长度 不相等 的 情况 解决 这 类 问题 的 一种 
方法 是 序 列到 序列 学习 技术 Sequence to Sequence 
Learning 即 序 列到 序列 的 学习 15 简称 seq2seq 
是 用 循环 神经网络 构建 的 一种 框架 它/r 能/v 
实现/v 从/p 一个/m 序/n 列到/v 另外/c 一个/m 序列/n 的/uj 映射/v 
两个 序列 的 长度 可以 不相等 seq2seq 框架 包括 两 
部分 分别 称为 编码器 和 解码器 它们 都是/nr 循环 神经网络 
这里 要 完成 的 是从 一个 序 列到 另外 一个 
序列 的 预测 前者 是 源 序列 后者 是 目标 
序列 两个 序列 的 长度 可能 不相等 用于 编码器 的 
循环 神经网络 接受 输入 序列     最后 时刻 T 
产生 的 隐含 层 状态值 作为 序列 的 编码 值 
它 包含 了 时刻 1 到 T 输入 序列 的 
所有 信息 在 这里 我们 将其 简写 为 v 这 
是 一个 固定 长度 的 向量 用于 解码 的 RNN 
的 初始 隐含 状态 为 v 它 可以 计算 目标 
序列 的 条件概率 根据 训练 神经 网络 的 输出 值 
之间 的 关系 这个 概率 可以 进一步 写成 如果 在 
输出 层 使用 softmax 函数 映射 就 可以 到 到上面 
每 一个 时刻 的 概率 实 现时 编码器 和 解码器 
同时 训练 最大化 上面 的 条件 概率 在 这里 训练 
样本 是 成对 的 序列 A B 训练 的 目标 
是 让 序列 A 编码 之后 解码 得到 序列 B 
的 概率 最大 即 最大化 如下 的 条件 对数 似 
然 函数 其中 N 是 训练 样本数     为要 
求解 的 参数 输入 序列 和 对应 的 输出 序列 
组合 在 一起 为 一个 训练样本 下图 是 编码器 对 
句子 编码 后的/nr 结果 在 这里 投影 到 2 维 
平面 上了 seq2seq 框架 有 两种 用法 第一种 用法 是 
为 输入输出 序列 对 打分 即 计算 条件 概率值 第二种 
用法 是 根据 输入 序列 生成 对应 的 输出 序列 
由于 seq2seq 只有 计算 条件概率 的 功能 因此 需要 采用 
搜索 技术 得到 条件概率 最大 的 输出 序列 可以 使用 
集束 搜索 beam search 技术 机器翻译 问题 采用 的 是 
第二 种 用法 seq2seq 框架 提供 的 是 一种 预测 
输出 序列 对 输入 序列 的 条件 概率 的 手段 
集束 搜索 通过 在 每一步 对上 一步 的 结果 进行 
扩展 来 生成 最优 解 在 每一步 选择 一个 词 
添加 到 之前 的 序列 中 形成 新的 序列 并 
只 保留 概率 最大 的 k 个 序列 在 这里 
k 为 人工 设定 的 参数 称为 集束 宽度 下面 
用 一个 例子 来 说明 集束 搜索 的 原理 假设 
词典 大小 为 3 包含 的 词 为 { a 
b c } 如果 集束 搜索 的 搜索 宽度 设置 
为 2 则在 选择 第 一个词 的 时候 寻找 概率 
最大 的 两个 词 假设 为 { a b } 
接下来 生成 下 一个 词 对 所有 可能 的 组合 
{ aa ab ac ba bb bc } 保留 概率 
最大 的 2个 假设 为 { ab bb } 接下来 
在 这个 基础上 再 选择 第三 个 词 以此类推 最终 
得到 概率 最大 的 完整 序列 作为 输出 典型/n 应用/v 
循环/vn 神经/n 网络/n 被/p 成功/a 应用/v 于/p 各类/r 时间/n 序列/n 
数据/n 的/uj 分析/vn 和/c 建模/n 包括 语音识别 自然语言 处理 机器 
视觉 中 的 目标 跟踪 视频 动作 识别 等 语音识别 
深度 学习 最早 应用于 语音识别 问题 时的/nr 作用 是 替代 
GMM HMM 框架 中的 高斯 混合模型 负责 声学 模型 的 
建模 即 DNN HMM 结构 在 这种 结构 里 深层 
神经网络 负责 计算 音频 帧 属于 某一 声学 状态 的 
概率 或者 是 提取 出 声音 的 特征 其余 的 
部分 和 GMM HMM 结构 相同 语音 识别 的 困难 
之 处 在于 输入 语音信号 序列 中 每个 发音 单元 
的 起始 位置 和 终止 位置 是 未知 的 即 
不 知道 输出 序列 和 输入 序列 之间 的 对齐 
关系 这 属于 前面 介绍 的 时序 分类 问题 深度/ns 
学习/v 技术/n 在/p 语音/n 识别/v 里/f 一个/m 有/v 影响力/n 的/uj 
成果/n 是/v 循环/vn 神经/n 网络/n 和/c CTC/w 的/uj 结合/v 和 
卷积 神经网络 自动 编码器 等 相比 循环 神经网络 具有 可以 
接受 不 固定 长度 的 序列 数据 作为 输入 的 
优势 而且 具有 记忆 功能 文献 14 将 CTC 技术 
用于 语音识别 问题 语音识别 中 识别 出 的 字符 序列 
或者 音素 序列 长度 一定 不 大于 输入 的 特征 
帧 序列 CTC 在 标注 符号 集中 加上 空白 符号 
blank 然后 利用 循环 神经 网络 进行 标注 再把 blank 
符号 和 预测 出 的 重复 符号 消除 下图 是 
CTC 的 原理 假设 x 为 语音输入 序列 l 为 
识别 出来 的 文字 序列     为 循环 神经 
网络 的 输出 可能 有 多个 连续 帧 对应 一个 
文字 有些 帧 可能 没有 任何 输出 按照 CTC 的 
原理 用 多对一 的 函数 B 把 输出 序列 中 
重复 的 字符 进行 合并 形成 一个 唯一 的 序列 
其中 l 为 文字 序列     是 带有 冗余 
的 循环 神经网络 输出 映射函数 B 将 神经 网络 的 
输出 序列 映 射成 文字 序列 l 分类器 的 输出 
为对 输入 序列 最 可能 的 标签 值 解码 时 
采用 的 是 前缀 搜索 技术 CTC 在 这里 起到 
了 对齐 的 作用 最 显著 的 优势 是 实现 
了 端 到 端 的 学习 无需 人工 对 语音 
序列 进行 分割 这样 做 还 带来 了 精 度上 
的 提升 在 实现 时 循环 神经 网络 采用 了 
双向 LSTM 网络 简称 BLSTM 训练样本 集 的 音频 数据 
被 切 分成 10 毫秒 的 帧 其中 相邻 帧 
之间 有5/nr 毫秒 的 重叠 使用 MFCC 特征 作为 循环 
神经 网络 的 输入 向量 原始 音频 信号 被 转换 
成 一个 MFCC 向量 序列 特征向量 为 26 维 包括了 
对数 能量 和一阶/nr 导 数值 向量 的 每一个 分量 都 
进行 了 归一化 在 解码 时 使用 最优 路径 和 
前缀 搜索 解码 解码 的 结果 就是 语音识别 要 得到 
的 标记 序列 文献 14 中的 循环 神经 网络 是 
一个 浅层 的 网络 文献 17 提出 了 一种 用 
深度 双向 LSTM 网络 和 CTC 框架 进行 语音 识别 
的 方法 这种 方法 主要 的 改进 是 使用 了 
多个 双向 LSTM 层 称为 深度 LSTM 网络 对于 多层 
RNN 网络 计算公式 为 双向 深度 循环 神经 网络 采用 
两套 隐含 层 分别 正向 反向 对 输入 序列 进行 
处理 并把 最后 一个 隐含 层 的 输出 值 合并 
之后 送到 输出 层 计算公式 为 对于 深度 双向 LSTM 
网络 原理 类似 只是 把 隐含 层 的 变换 换成 
LSTM 结构 的 公式 在 这里 不再 详细 介绍 假设 
输入 的 声学 序列 数据 为 x 输出 音素 序 
列为 y 第一步 是 给定 输入 序列 和 所有 可能 
的 输出 序列 用 循环 神经 网络 计算 出 条件 
概率值 p y | x 在 训练 时的/nr 样本 为 
输入 序列 以及 对应 的 输出 序列 训练 时的/nr 损失 
函数 为 对数 似 然 函数 这里 使用 CTC 来 
对 序列 z 进行 分类 对于 一段 输入 的 语音 
数据 分类 的 结果 是 一个 音素 序列 假设有 k 
个 音素 再 加上 一个 空白符 是 一个 k + 
1类 的 分类 问题 循环 神经 网络 的 最后 一层 
为 softmax 层 输出 k + 1个 概率值 在 时刻 
t 输出 值 为 p y | t 神经 网络 
在 每 一个 时刻 确定 是 输出 一个 音素 还是 
不 输出 即 输出 空白符 将 所有 时刻 的 输出 
值 合并 在 一起 得到 了 一个 输入 和 输出 
序列 的 对齐 方案 CTC 对 所有 的 对齐 方式 
进行 概率 求和 得到 p z | x 在 使用 
CTC 时 循环 神经 网络 被 设计 成 双向 的 
这样 每个 时刻 的 概率 输出 值 为 其中 N 
是 隐含 层 的 数量 y 是 神经 网络 的 
输出 向量 上式 用 softmax 映射 根据 神经 网络 的 
输出 向量 得到 每一个 音素 的 概率值 前面 介绍 的 
CTC 框架 输入 是 声学 数据 输出 是 音素 数据 
只是 一个 声学 模型 接下来 还 需要 将 音素 序列 
转化成 最终 的 文字 序列 作为 识别 结果 需要 一个 
语言 模型 在 这里 采用 RNN transducer 一种 集成 了 
声学 建模 CTC 和 语言 模型 RNN 的 方法 后者 
负责 将 音素 转化成 文字 二者 联合起来 训练 得到 模型 
我们 称 第一 个 网络 为 CTC 网络 第二 个 
网络 为 预测 网络 假设/vn 和为/nr CTC/w 网络/n 最后/f 一个/m 
CTC/w 最后/f 一个/m 隐含/v 层/q 的/uj 前/f 向和/nr 后向/i 输出/v 
序列/n p 为 预测 网络 的 隐含 层 输出 序列 
在 每个 时刻 t u 为 输出 网络 它 包含 
一个 线性 层 接受 输入 和 产生 输出 向量 lt 
另外 还 包含 一个 tanh 隐含 层 接受 输入 值 
lt 和 pu 产生 输出 值 htu 最后 将 htu 
送入 类 的 softmax 层 得到 概率值 p k | 
t u 整个 过程 的 计算 公式 为 RNN transducer 
只是 给 出了 任何 一个 输出 序列 相对于 输入 序列 
的 条件 概率值 还 需要 解码 算法 得到 概率 最大 
的 输出 序列 在 这里 使用 了 集束 搜索算法 算法 
给出 n 个 最优 的 候选 结果 选择 的 依据 
是 概率值 P k | t 整个 系统 的 输入 
数据 是 对 音频 数据 进行 分 帧 后的/nr 编码 
向量 具体 做法 是 对分 帧 后的/nr 音频 数据 进行 
傅里叶 编码 然后 40个 傅里叶 系数 加上 能量 以及 它们 
的 一 阶 和 二阶 导数 构成 的 向量 因此 
特征向量 为 123 维 整个 向量 进行 了 归一化 在 
这里 使用 了 61个 音素 它们 被 映射 为 39个 
类 实验 结果 证明 更深 的 网络 具有 更高 的 
准确率 双向 LSTM 比 单向 网络 也 有 更高 的 
精度 文献 19 提出 了 一种 融合 了 卷积 神经 
网络 和 循环 神经 网络 的 英语 与 汉语 普通话 
语音识别 算法 这也 是 一种 完全 端 到 端 的 
方法 所有 人 工 工程 的 部分 都用 神经网络 替代 
可以 处理 各种 情况 包括 噪声 各种 语言 整个 系统 
的 输入 为 音频 数据 使用 20 毫秒 的 窗口 
对 原始 音频 数据 分 帧 然后 计算 对数 谱 
对 功率 进行 归一化 形成 序列 数据 送入 神经 网络 
中 处理 首先 是 1D 或者 2D 卷积 层 然后 
是 双向 RNN 接下来 全力 连接 的 lookahead 卷积 层 
最后 是 CTC 分类器 整个 模型 也 实现 了 端 
到 端的 训练 在 每个 时刻 t 神经 网络 的 
输出 值 为     其中     为 字母表 
中的 符号 或者 是 空格 对于 英文 为 { a 
b c . . . z space apotrohpe blank } 
其中 space 为 词 之间 的 边界 对于 中文 输出 
值 为 简化 的 汉字 字符 识别/v 时/n CTC/w 模型/n 
和/c 语言/n 模型/n 结合/v 起来/v 使用/v 解码 时 使用 集束 
搜索算法 寻找 输出 序列 y 最大化 如下 函数 第一 部分 
为 RNN 的 损失 函数 第二 部分 为 语言 模型 
的 损失 函数 第三 部分 对 英文 为 单 词数 
对 汉语 为 字数     和  /nr 为 人工 设定 
的 权重 参数 网络 的 最 前端 是 卷积 层 
对 输入 的 频谱 向量 执行 1D 或者 2D 卷积 
实验 结果 证明 2D 卷积 有 更好 的 效果 整个 
网络 包含 多个 循环 层 循环 层 还 使用 了 
批量 归一化 技术 它 可以 作用于 前 一层 和本层/nr 上一 
时刻 状态值 的 线性 加权 和 也 可以 只 作用于 
前 一层 的 输入 值 在 所有 循环 层 之前 
加上 了 lookahead 卷积 层 计算公式 为 其中 d 为 
前 一层 的 神经元 个数 h 是 前 一层 的 
输出 值 W 是     的 权重 矩阵   
  为 时间 步长 除了 上面 介绍 的 这些 论文 
用 循环 神经 网络 进行 语音 识别 的 文章 还 
很多 限于 篇幅 不能 一一列举 感 兴趣 的 读者 可以 
自己 去 阅读 自然语言 处理 自然语言 处理 的 很多 问题 
是 时间 序列 问题 也是 循环 神经 网络 被 广为 
应用 的 领域 下面 介绍 在 一些 典型 问题 上 
的 使用 情况 文献 30 为 自然语言 处理 的 很多 
问题 提供 了 一个 用 循环 神经 网络 解决 的 
统一 框架 这个 框架 用 循环 神经 网络 为 句子 
序列 进行 编码 得到 上下文 语义 信息 然后 产生 输出 
如下 图 所示 中文分词 汉语 句子 的 词 之间 没有 
类似 英文 的 空格 因此 我们 需要 根据 上下文 来 
完成 对 句子 的 切分 分词 的 任务 是 把 
句子 切 分成 词 的 序列 即 完成 我们 通常 
所说 的 断句 功能 它 是 解决 自然语言 处理 很多 
问题 的 第一 步 在 搜索引擎 等 产品 中 都有 
应用 由于 歧义 和未/nr 登录 词 即 词典 里 没有 
的 新词 的 存在 中文分词 并 不是 一件 简单 的 
任务 以 下面 的 句子 为例 乒乓球拍 卖了 显然 这 
句话 有 歧义 对应 于 下面 两种 切分 方案 乒乓球 
拍卖 了 乒乓球拍 卖 了 句子 中 出现 词典 里 
没有 的 词 也会 影响 我们 的 正确 切分 例如 
下面 的 句子 李国庆 节日 在 加班 在 这里 李国庆 
是 一个 人 名字 而 国庆节 也 是 一个 合法 
的 词 正确 的 分词 需要 程序 知道 李国庆 是 
人名 最 简单 的 分词 算法 是 基于 词典 匹配 
这又 分为 正向 匹配 反向 匹配 和 双向 匹配 3种 
策略 如果 使用 正向 最大 匹配 在/p 分词/n 时用/nr 词典/n 
中/f 所有/b 的/uj 词/n 和/c 句子/n 中/f 还未/i 切分/ad 的/uj 
部分/n 进行/v 匹配/v 如果 存在 多个 匹配 的 词 则以 
长度 最大 的 那个 词 作为 匹配 结果 反向 最大 
匹配 的 做法 和 正向 最大 匹配 类似 只是/c 从后/nr 
向前/t 扫描/v 句子/n 双向 最大 匹配 则 既 进行 正向 
最大 匹配 也 进行 反向 最大 匹配 以 切分 的 
词 较少 的 最为 结果 显然 词典 匹配 无法 有效 
的 处理 未 登录 词 问题 对 歧义 切分 也 
只能 简单 使用 长度 最大 的 词 去 匹配 词典 
匹配 可以 看作 是 解决 分词 问题 的 基于 规则 
的 方法 作为 改进 可以 采用 全 切分 路径 技术 
这种 技术 列出 一个 句子 所有 切分 的 方案 然后 
选择 出 最佳 的 方案 随着 句子 的 增长 这种 
方法 的 计算 量 将 呈 指数级 增长 机器学习 技术 
也 被 用于 分词 问题 采用 序列 标注 的 手段 
解决 此 问题 隐 马尔可夫 模型 条件 随 机场 等 
方法 为 其中 的 代表 可以 看成 是 序列 标注 
问题 将 一个 句子 中 的 每个 字 标记 成 
各种 标签 系统 的 输入 是 字 序列 输出 是 
一个 标注 序列 因此 这 是 一个 标准 的 序列 
到 序列 的 问题 在 这里 标注 序列 有 这样 
几种 类型 { B N E } 其中 B 表示 
当前 字 为 一个 词 的 开始 M 表示 当前 
字 为 一个 词 的 中间 位置 E 表示 当前 
字 为 一个 词 的 结束 位置 表示 单 字词 
以 下面 的 句子 为例 我 是 中国 人 其 
分词 结果 为 我 是 中国 人 标注 序 列为 
我 / 是 / 中 / B 国/M/nr 人 / 
E 同样 的 我们 可以 用 循环 神经 网络 进行 
序列 标注 从而 完成 分词 任务 在 这里 网络 的 
输出 是 句子 中 的 每个 字 输出 是 每个 
字 的 类别 标签 得到 类别 标签 之后 我们 就 
完成 了 对 句子 的 切分 词性 标注 词性 标注 
POS Tagging 是 确定 一个 句子 中 各个 词 的 
类别 它 是 和 分词 密切 相关 的 一个 问题 
典型 的 分类 有 名词 动词 形容词 和 副词 等 
给定 句子 中的 词 序列 词性 标注 的 结果 是 
每个 词 的 词 类别 这 也 可以 看成 是 
一个 序列 标注 问题 即 给定 一个 句子 预测出 句子 
中 每个 词 的 类别 最 简单 的 是 基于 
统计 信息 的 模型 即从 训练样本 中 统计 出 每种 
词性 的 词 后面 所 跟 的 词 的 词性 
然后 计算 最大 的 概率 除此之外 条件 熵 隐 马尔可夫 
模型 条件 随 机场 等 技术 也 被 用于 词性 
标注 问题 同样 的 词性 标注 问题 可以 看做 是 
一个 序列 标注 问题 将 循环 神经网络 用于 词性 标注 
时 输入 序列 是 一个 句子 的 单词 序列 每个 
时刻 的 输入 向量 是 单词 的 one hot 编码 
向量 网络 的 输出 为 单词 属于 某一 类次 的 
概率 此时 输出 层 可以 采用 softmax 作为 激活 函数 
在 这里 典型 的 标注 集 合为 { v n 
a . . . } 其中 v 为 动词 n 
为 名字 a 为 形容词 其他 词性 在 这里 不 
详细 列出 训练 时 也 使用 端 到 端的 方案 
直接 给定 语句 和 对应 的 标签 序列 神经 网络 
的 预测 输出 就是 每个 词 的 词性 类别 值 
命名 实体 识别 命名 实体 识别 Named Entity Recognition 简称 
NER 又 称为 专名 识别 其 目标 是 识别 文本 
中 有 特定 含义 的 实体 如 人名 地名 机构名称 
专有名词 等 属于 未 登录 词 识别 的 范畴 命名 
实体 识别 和 其他 自然语言 处理 问题 相比 存在 的 
一个 困难 是 训练 样本 的 缺乏 因为 未 登录 
词 很少 有 重复 的 基本上 都是 新词 如果 直接 
用 序列 标注 的 方法 解决 命名 实体 识别 思路 
和 分词 类似 这里 要 识别 出 句子 里 所有 
的 专名词 假设 要 识别 的 专有 词 包括 人名 
地名 组织 机构 名称 则 标注 集 合为 { BN 
MN EN BA MA EA BO MO EO O } 
其中 BN 表示 这个 字 是 人名 的 开始 BN 
表示 人名 的 中间 字 EN 表示 人名 的 结束 
BA 表示 地名 的 开始 MA 表示 地名 的 中间 
字 EA 表示 地名 的 结束 BO 表示 机构 名称 
的 开始 MO 表示 机构 名称 的 中间 字 EO 
表示 机构 名称 的 结束 O 表示 这个 字 不是 
命名 实体 给定 所有 训练样本 句子 的 标注 序列 我们 
就 可以 实现 端 到 端的 训练 预 测时 输入 
一个 句子 输出 标签 序列 根据 标签 序列 我们 可以 
得到 命名 实体 识别 的 结果 除 了 这种 最 
直接 的 序列 标记 手段 还 更 复杂 的 方法 
文献 31 提出 了 一种 用 LSTM 和 条件 随 
机场 CRF 进行 命名 实体 识别 的 方法 假设 LSTM 
网络 的 输入 序列 是   输出 序列 是   
  其中 输入 序列 是 一个 句子 所有 的 单词 
这些 单词 被 编码 为 向量 LSTM 在 t 时刻 
的 输出 向量 是 句子 中 第 t 个 单词 
的 左 上下文 单词 的 右 上下文 也 是 非常 
重要 的 信息 也 通过 LSTM 计算 得到 具体 做法 
是 将 整个 句子 颠倒过来 送入 LSTM 中 计算 第 
个 时刻 的 输出 向量 即为 右 上下文 在 这里 
称 第一 个 LSTM 为 前 向 LSTM 第二个 为 
后向 LSTM 它们 是 两个 不同 的 神经 网络 分别 
有 各自 的 参数 这种 结构 也 称为 双向 LSTM 
每个/r 词/n 用/p 它/r 的/uj 左/m 上下文/l 和右/nr 上下文/l 联合起来/i 
表示/v 即将 两个 向量 拼 接起来 接下 来用 条件 随 
机场 对 句子 中 的 所有 词 进行 联合 标注 
对于 一个 句子 假设 矩阵 P 是 双向 LSTM 输出 
的 得分 矩阵 这 是 一个 NxK 的 矩阵 其中 
k 是 不同 的 标记 个数 元素 为 第 i 
个 单词 被 赋予 第 j 个 标记 的 概率 
对于 预测 输出 序列 y 它 的 得分 定义 为 
其中 矩阵 A 是 转移 得分 矩阵 其 元素 表示 
从 标记 i 转移到 标记 j 的 得分  /i  /i 
和  /nr 是/v 句子/n 的/uj 开始/v 和/c 结束/v 标记/n 我们 把 
它们 加入到 标记 集合 中 因此 矩阵 A 是 一个 
k + 2 阶 方阵 对 所有 可能 的 标记 
序列 的 softmax 值 定义 了 序列 的 概率 其中 
    为 句子 X 所有 可能 的 标记 序列 
在 解码 时将/nr 具有 最大 得分 的 序列 作为 预测 
输出 这 可以 通过 动态规划 算法 得到 根据 输出 序列 
的 值 我们 就 可以 直接 得到 命名 实体 识别 
的 结果 文本 分类 文本 分类 是 自然 语言 处理 
中 的 重要 问题 经典 的 机器学习 算 法如 支持 
向量 机 贝叶斯 分类器 等 都曾 被 用于 解决 此 
问题 卷积 神经 网络 在 文本 分类 问题 中 也有 
应用 除了 这些 方法 之外 循环 神经 网络 也 被 
成功 的 应用于 文本 分类 问题 文献 34 设计 了 
一种 用 分层 注意力 网络 进行 文本 分类 的 方案 
在 这种 方案 里 采用 了 分层 的 结构 首先 
建立 句子 的 表示 然后 将 它们 聚合 形成 文档 
的 表示 在 文档 中 不同 的 词 和 句子 
所 蕴含 的 有用 信息 是 不 一样 的 而且/c 
重要性/n 和/c 文档/n 上下文/l 有/v 密切/ad 的/uj 关系/n 因此 采用 
了 两层 的 注意力 机制 第一个 是 单词 级 的 
第二个 是 句子 级 的 在 提取 文档 的 表示 
特征 时 会 关注 某些 词 和 句子 也会 忽略 
一些 词 和 句子 整个 网络 由 一个 单词 序列 
编码器 一个 单词 级 注意力 层 一个 句子 编码器 一个 
句子 级 注意力 层 组成 单词 序列 编码器 由 GRU 
循环 神经 网络 实现 网络 的 输入 是 一个 句子 
的 单词 序列 输出 是 句子 的 编码 向量 假设 
一篇 文档 有L个/nr 句子     句子 有  /nr 个 词 
    表示 第 i 个 句子 中的 第 t 
个 单词 其中     HAN 将 文档 投影 为 
一个 向量 然后 对 这个 向量 进行 分类 第一步 是 
采用 词 嵌入 技术 将 一个 句子 的 单词 转换 
为 一个 向量 计算公式 为 在 这里     称为 
嵌入 矩阵 然后 用 双向 GRU 网络 对词 序列 进行 
编码 具体做法 参考 双向 RNN 和 双向 LSTM 得到 隐含 
层 的 状态值 将 这个 状态 值 作为 句子 的 
表示 句子 中 的 不同 单词 有 不同 的 重要性 
在 这里 采用 了 注意力 机制 它 的 计算 公式 
为 首先 将     输入 一个 单层 的 MLP 
得到 它 的 隐含 层 表示     这个 单词 
的 重要性 由 向量 与 单词 级 上下文 向量   
  的 相似 度 来 衡量 通过 softmax 函数 最后 
得到 归一化 的 重要性 权重 值   接下来 计算 句子 
向量     它 是 词 向量 的 加权 平均 
加 权值 为 每个 词 的 重要性 权重 在 这里 
上下文 向量     被 随机 初始化 并且 在 训练 
过程 中 和 神经 网络 一起 训练 得到 在 得到 
句子 向量 之后 我们 可以 用 类似 的 方式 得到 
文档 向量 在 这里 使用 双向 GRU 对 句子 进行 
编码 将 这 两个 向量 合并 得到 句子 的 编码 
向量 这个 编码 综合 第 i 个 句子 周围 的 
句子 但 还是 聚焦 于第i/nr 个 句子 类似 的 我们 
用 句子 级 的 注意力 机制 来 形成 文档 的 
表示 向量 在 这里 v 是 文档 向量 它 综合 
了 文档 中 所有 句子 的 信息 同样 的 向量 
通过训练 得到 最后 用 文档 向量 来 对 文档 进行 
分类 训练 时的/nr 损失 函数 采用 负 对数 似 然 
函数 定义 为 其中 j 是 第 d 个 文档 
的 类别 标签 值 采用 注意力 机制 可以 直接 把 
对分 类有 贡献 的 词 和 句子 显示出来 便于 理解 
和 调试 分析 自动 摘要 自动 摘要 的 目标 是 
给定 一段 文本 得到 它 的 摘要 信息 摘要 信息 
浓缩 了 文本 的 内容 和/c 输入/v 文本/n 有/v 相同/d 
的/uj 语义/n 体现 了 文章 的 主要 内容 在 这里 
输入 文本 可以 是 一句话 或者 多 句话 摘要 输出 
语句 的 词汇表 和 输入 文本 的 词汇表 相同 可以 
将 自动 摘要 也 看成 是 一个 序 列到 序列 
的 预测 问题 输出 序列 的 长度 远 小于 输入 
序列 的 长度 文献 35 提出 了 一种 使用 注意力 
机制 和 seq2seq 技术 的 新闻 类 文章 标题 生成 
算法 在 这里 先用 seq2seq 的 编码 网络 生成 文本 
的 抽象 表示 解码器 网络 在 生成 摘要 的 每个 
单词 的 时候 使用 注意力 机制 关注 文本 中 的 
重点 词 首先 新闻 文章 的 每个 单词 被 依次 
输入 编码 网络 单词 首先 被 送入 嵌入 层 生成 
概率分布 表示 然后 被 送入 有 多个 隐含 层 组成 
的 训练 神经网络 所有 词 被 输入 网络 处理 之后 
最后 一个 隐含 层 的 状态值 将 用来 作为 解码器 
网络 的 输入 接下来 将 作为 解码器 网络 的 初始状态 
首先 将 一个 结束符 end of sequences 简称 EOS 输入 
解码器 网络 用 softmax 层 和 注意力 机制 生成 每一个 
摘要 单词 最后 以 EOS 结束 在 生成 每一个 单词 
时 将 生成 的 上一个 单词 作为 解码器 网络 的 
输入 训练 时的/nr 损失 函数 定义 为 其中     
是 输入 文本 的 单词 序列     是 生成 
的 摘要 单词 序列 训练 时 解码器 在 每个 时刻 
的 输入 为 真实 的 标题 中 的 单词 而 
不是 上 一 时刻 生成 的 单词 在 测试 时 
则 使用 的 是 上一 时刻 生成 的 单词 但/c 
这样/r 做/v 会/v 造成/v 训练/vn 和/c 预测/vn 时的/nr 脱节/v 作为 
补救 在/p 训练/vn 时/n 随机/d 的/uj 使用/v 真实/d 的/uj 单词/n 
和/c 上一/i 时刻/n 生成/v 的/uj 单词/n 作为/v 输入/v 在 预测 
时 使用 集束 搜索 技术 生成 每一个 输出 单词 在 
解码器 生成 每个 输出 单词 时 使用 了 注意力 机制 
对于 每一个 输出 单词 注意力 机制 为 每个 输入 单词 
计算 一个 权重 值 这个 权重 值 决定了 对 每个 
输入 单词 的 关注度 这些 权重 的 和为1/nr 并被 用于 
计算 最后 一个 隐含 层 的 输出 值 的 加权 
平均值 在 这里 每次 处理 完 一个 输入 单词 会 
产生 一个 输出 值 最后 是 对 这些 输出 值 
进行 平均 这个 加权 平均值 被 看做 是 文档 的 
上下文 信息 接下来 它/r 和/c 解码器/n 当前/t 解码/n 时/n 最后/f 
一个/m 隐含/v 层/q 的/uj 输出/v 值/n 一起/m 被/p 送入/v softmax/w 
层/q 进行/v 计算/v 机器翻译 统计 机器翻译 采用 大量 的 语料 
库 进行 学习 训练样本 为 源语言 和 目标 语言 的 
语句 得到 模型 之后 对于 一个 语句 算法 直接 使用 
这个 模型 得到 目标 语言 的 语句 如果 用 统计 
学习 的 方法 机器翻译 要 解决 的 问题 是 给定 
一个 输入 句子 a 对于 另外 一种 语言 所有 可能 
的 翻译 结果 b 计算 条件概率 概率 最大 的 句子 
就是 翻译 的 结果 使用/v 机器/n 学习/v 的/uj 翻译/v 有/v 
基于/p 词/n 的/uj 翻译/v 和/c 基于/p 短语/nz 的/uj 翻译/v 两种/m 
方法/n 前者 对词 进行 翻译 不考虑 上下文 语境 和词/nr 之间 
的 关联 后者 对 整个 句子 进行 翻译 目前 主流 
的 是 基于 短语 的 翻译 我们 可以 将 机器翻译 
问题 抽象 成 一个 序列     到 另外 一个 
序列     的 预测 和 语音 识别 之类 的 
应用 不同 这里 的 序列 到 序列 映射 并 不是 
一个 单调 映射 也 就是说 输出 序列 的 顺序 是 
按照 输入 序列 的 顺序 来 的 这 很容易 理解 
将 一种 语言 的 句子 翻译成 另一 一种 语言 的 
句子 时 源语言 种 的 每个 单词 的 顺序 和 
目标语言 种 每个 单词 的 顺序 不 一定 是 一致 
的 训练 时的/nr 目标 是 对 所有 的 样本 最大化 
下面 的 条件概率 因此 我们 需要 在 所有 可能 的 
输出 序列 中 寻找 到 上面 的 条件 概率值 最大 
的 那个 序列 作为 机器 翻译 的 输出 如果 用 
神经 网络 来 对 机器 翻译 进行 建模 称为 神经 
机器翻译 当前 用 循环 神经 网络 解决 机器翻译 问题 的 
主流 方法 是 序 列到 序列 学习 技术 文献 15 
提出 了 用 seq2seq 技术 解决 机器翻译 问题 在 这里 
使用 编码器 对 输入 的 输入 序列 进行 特征 编码 
得到 这 句话 的 意义 然后 用 解码器 对 这个 
意义 进行 解码 并 得到 概率 最大 的 输出 序列 
这就 得到 了 翻译 的 结果 先将 源 句子 表示 
成 向量 序列 在 这里 每个 向量 是 一个 词 
的 编码 向量 通过 第一 个 循环 神经网络 当 我们 
输入 完 这个 序列 之后 得到 最后 时刻 的 隐含 
层 状态值     在 这里 简 记为 v 这个 
值 包含 了 整个 句子 的 信息 接下 来用 解码器 
生成 翻译 序列 解码器 循环 神经 网络 银行 层 的 
初始 状态值 为 v 它 输出 向量 序列     
对于 所有 可能 的 输出 序列 我们 都 可以 用 
解码器 计算出 它 的 条件 概率值 在 这里 要 寻找 
概率值 最大 的 那个 序列 如果 枚举 所有 可能 的 
输出 序列 计算 量 太大 显然是 不 现实 的 在 
这里 采用 了 集束 搜索 技术 训练样本 是 成对 的 
句子 即 源 句子 和它的/nr 翻译 结果 训练 的 目标 
是 最大化 对数 概率值 其中 D 是 训练 样 本集 
是 源 句子 T 是 翻译 的 句子 训练 完成 
之后 可以 用 这个 模型 来 进行 翻译 即 寻找 
概率 最大 的 输出 序列 在 这里 采用 了 自左 
到 右 的 集束 解码器 它 维持 K 个 最 
有可能 的 部分 结果 部分 结果 是 整个 翻译 句子 
的 前缀 部分 在 每一步 我们 在 词典 的 范围 
内 用 每 一个 可能 的 词 扩展 这个 部分 
结果 然后 用 seq2seq 模型 计算 这些 部分 结果 的 
概率 保留 概率 最大 的 个 部分 结果 当 输入 
结束符 之后 整个 翻译 过程 结束 在 实现 时 无论是 
在 训练 阶段 还 是 测试 阶段 都将 句子 反序 
输入 但是 预测 结果 序列 是 正 序 而不是 反序 
另外 并 没有 采用 单个 隐含 层 的 循环 神经网络 
而是 采用 了 4层 的 LSTM 网络 文献 36 提出 
了 一种 用 编码器 解码器 框架 进行 机器 翻译 的 
方法 在 这里 编码器 解码器 框架 的 结构 和 之前 
介绍 的 相同 不同 的 是 使用 了 一种 新的 
隐藏 单元 即 循环 层 的 激活 函数 这种 激活 
函数 和 LSTM 类似 但 计算 更简单 在 这里 使用 
了 两个 门 来 进行 信息流 的 控制 分别 称为 
更新 门 和 复位 门 复位 门 的 计算 公式 
为 更新 门 的 计算 公式 为 隐含 层 的 
变换 公式 为 在 这里 更新 门 用来 控制 新老 
信息 的 权重 其中 假设 e 为 源 语句 f 
为 翻译 后的/nr 目标 语句 根据 贝叶斯 公式 机器 翻译 
的 目标 是 给定 源 语句 寻找 使得 如下 条件概率 
最大 的 目标 语句 上式 右边 的 第一 项为/nr 转换 
模型 第二项 为 语言 模型 这 和 语音 识别 类似 
大多数 机器翻译 算法 将 转换 模型表示 成 对数 线性 模型 
其中     为 第 n 个 特征     
为 特征 的 权重     为 归一化 因子 在 
这里 编码器 解码器 框架 用于 对 对数 线性 模型 的 
翻译 候选 结果 短语 进行 评分 文献 37 提出 了 
一种 使用 了 双向 循环 神经 网络 的 机器 翻译 
算法 循环 层 也 使用 了 重置 门 和 更新 
门 结构 解码 器用 循环 神经 网络 实现 它 根据 
当前 状态 以及 当前 的 输出 词 预测 下 一个 
输出 词 计算公式 为 其中     为 解码器 网络 
隐含 层 的 状态 这个 框架 采用 了 注意力 机制 
计算 方法 和 之前 介绍 的 相同 文献 38 介绍 
了 Google 的 机器 翻译 系统 他们 的 系统 同样 
采用 了 编码器 解码器 架构 两个 网络 都由 深层 双向 
LSTM 网络 实现 并 采用 了 注意力 机制 这里 的 
深层 双向 LSTM 网络 和 前面 介绍 的 相同 不再 
重复 讲述 为了 克服 深层 带来 的 梯度 消失 问题 
隐含 层 采用 了 残差 网络结构 即 跨 层 连接 
训练 时的/nr 目标 是 最大化 对数 似 然 函数 即 
对数 条件 概率值 在 这里     是 要 求解 
的 参数 同样 的 解码 时也/nr 使用 了 集束 搜索算法 
机器 视觉 对于 机器 视觉 中的 某些 问题 循环 神经 
网络 也 取得 了 很好 的 效果 在 这些 问题 
中 数据 都被 抽象 成 一个 时间 序列 如 物体 
运动 的 动作 状态 等 字符识别 如果 我们 知道 每 
个字符 的 笔画 信息 即 整 个字 的 书写 过程 
则 可以 将 手写 字符识别 看成 是 一个 轨迹 分类 
问题 每个 手写 字符 是 一个 序列 数据 每个 时刻 
的 坐标 连接起来 在 平面 上 构成 一 个字符 的 
图像 手写 字符识别 属于 序列 标记 问题 中 的 序列 
分类 问题 即 给定 一 个字符 的 坐标 点 序列 
预测 这 个字符 的 类别 在 这里 循环 神经 网络 
的 输入 为 坐标 点 序列 输出 值 为 类别 
为了 达到 这个 目的 我们 可以 将 最后 一个 时刻 
的 循环 层 输出 值 映射 为 类别 概率 这 
可以 通过 softmax 层 实现 另外 也 可以 直接 以 
图像 作为 输入 在 这里 将 图像 看作 是 一个 
序列 序列 中 的 每一个 向量 是 图像 中 的 
一个 行 的 像素 依次 将 每 一行 输入 循环 
神经网络 最后 时刻 的 隐含 层 状态 输出 作为 提取 
的 字符 特征 送入 softmax 层 进行 分类 目标 跟踪 
运动 跟踪 可以 抽象 为 已知 目标 在 之前 时刻 
的 坐标 预测出 它 在 当前 时刻 的 坐标 这 
同样 是 一个 序列 预测 问题 文献 42 提出 了 
一种 用 循环 神经 网络 进行 目标 跟踪 的 方法 
称为 RTT RTT 主要 目标 是 解决 目标 遮挡 问题 
循环 神经 网络 的 作用 是 得到 置信度 图 即 
每个 点 处 是 目标 的 概率 下面 介绍 这种 
方法 的 处理 流程 在对 每 一帧 进行 跟踪 时 
给定 目标 在上 一帧 中的 矩形框 以 目标 的 中心 
为 中心 以 目标 宽 高的/nr 2.5倍 为 宽 高 
即将 目标 矩形 放大 2.5倍 得到 一个 矩形 的 候选 
区域 然后 将 这个 候选 区域 划分 成 网格 然后 
对 每个 矩形框 提取 特征 可以 使用 HOG 特征 也 
可以 使用 更 复杂 的 卷积 网络 提取 的 特征 
在 这里 划分 网格 而 不是 对 整个 候选 区域 
计算 特征 的 原因 是 这样 做 能够 更好 的 
处理 遮挡 以及 目标 外观 的 变化 最后 我们 得到 
候选 区域 的 特征 然后 以 这个 特征 作为 输入 
用 多维 RNN 对 特征 进行 处理 得到 置信度 图 
最后 根据 置信度 图 完成 对 目标 位置 的 预测 
和 单个 目标 跟踪 不同 多目标 跟踪 需要 解决 数据 
关联 问题 即 上 一帧 的 每个 目标 和下/nr 一帧 
的 哪个 目标 对应 还要 解决 新 目标 出现 老 
目标 消失 问题 多 目标 的 跟踪 的 一般 流程 
为 每一 时刻 进行 目标 检测 然后 进行 数据 关联 
为 已有 目标 找到 当前 时刻 的 新 位置 在 
这里 目标 可能 会 消失 也 可能会 有 新目标 出现 
另外 目标 检测 结果 可能 会 存在 虚警 和漏/nr 检测 
联合 概率 滤波 多 假设 跟踪 线性规划 全局 数据 关联 
MCMC 马尔可夫 链 蒙特卡洛 算法 先后 被 用于 解决 数据 
关联 问题 来 完成 多个 目标 的 跟踪 首先 我们 
定义 多目标 跟踪 的 中 的 基本 概念 目标 是 
我们 跟踪 的 对象 每个 目标 有 自己 的 状态 
如 大小 位置 速度 观测 是 指 目标 检测 算法 
在 当前 帧 检 测出 的 目标 同样 的 它 
也有 大小 位置 速度 等 状态值 在 这里 我们 要 
建立 目标 与 观测 之间 的 对应 关系 下图 是 
数据 关联 的 示意图 在上 图中 第一列 圆形 为 跟踪 
的 目标 即 之前 已经 存在 的 目标 第二列 圆 
为 观测值 即 当前 帧 检测 出来 的 目标 在 
这里 第 1个 目标 与 第 2个 观察 值 匹配 
第 3个 目标 与 第 1个 观测值 匹配 第 4个 
目标 与 第 3个 观测值 匹配 第 2个 和第/nr 5个 
目标 没有 观测值 与之 匹配 这 意味着 它们 在 当前 
帧 可能 消失了 或者 是 当前 帧 被 漏检 没有 
检测 到 这 两个 目标 类似 的 第 4个 观测值 
没有 目标 与 之 匹配 这 意味着 它 是 新目标 
或者 虚警 文献 43 提出 了 一种 用 循环 神经 
网络 在线 跟踪 多个 目标 的 算法 这种方法 实现 了 
完全 端 到 端的 训练 在 这里 用 LSTM 循环 
神经网络 同时 解决 数据 关联 新目标 出现 老 目标 消失 
问题 首先 定义 状态 向量     这 是 一个 
NxD 维 向量 表示 t 时刻 所有 目标 的 状态 
值 其中 D 为 每个 目标 的 状态 个数 在 
这里 值 为 4 分别 为 目标 的 位置 和宽高/nr 
定义 N 为 某 一帧 中 能够 同时 跟踪 的 
最大 目标 个数     为 第 i 个 目标 
的 状态 类似 的 定义 观测 向量     这 
是 一个 MxD 维 向量 表示 t 时刻 所有 观测值 
其中 M 为 每 一帧 中 最大 检测 目标 个数 
需要 注意 的 是 我们 对 模型 能够 处理 的 
最大 目标 个数 并 没有 限制 接下来 定义 分配 概率 
矩阵 A 这 是 一个 Nx M + 1 的 
矩阵 元素 取值 0 和1/nr 之间 的 实数 矩阵 的 
每 一行 为 一个 目标 的 分配 概率 向量 即 
元素     表示 将 第 i 个 目标 分配给 
第 j 个 观测 的 概率 分配 概率 矩阵 满足 
约束条件 在 这里 矩阵 的 列数 不是 M 而是 M 
+ 1 这 是 因为 一个 目标 可能 不 和 
任何 一个 观 测向 匹配 最后 定义 指示 向量   
  这 是 一个 N 维 向量 每个 元素 表示 
一个 目标 存在 的 概率值 跟踪 问题 被 分成 两个 
部分 来 解决 状态 预测 与 更新 以及 跟踪 管理 
数据 关联 前 一 部分 负责 单个 目标 的 状态 
跟踪 后一/nr 部分 解决 目标 之间 的 对应 关系 对于 
第一个 问题 用 一个 时序 循环 神经 网络 来 学习 
N 个 目标 的 运动 模型 以及 目标 的 指示 
变量 指示 变量 用于 处理 目标 的 出现 与 消失 
在 时刻 t 循环 神经网络 输出 四种 值 1 . 
包括 所有 目标 的 状态 预测值     前面 已经 
介绍 过 2 . 所有 目标 状态 的 更新 值 
    3 . 指示 向量     其 每个 
元素 的 值 位于 0 1 之间 表示 目标 是 
一个 真实 轨迹 的 概率 4 .     这是 
与     的 差值 神经 网络 的 输入 为 
前 一个 时刻 的 状态值     前 一个 时刻 
的 指示 向 量值     当前 时刻 的 观测值 
    以及 当前 时刻 的 数据 关联矩阵     
数据 关联矩阵 的 计算 方法 将 在后面 介绍 这个 功能模块 
有 三个 目标 1 . 预测 为 指定 数量 的 
目标 学习 一个 复杂 的 运动 模型 这个 模型 包含 
了 每个 目标 的 运动 参数 包括 速度 加速度 信息 
等 2 . 更新 根据 当前 的 观测 数据 对 
预测 值 进行 校正 修正 物体 的 状态 值 包括 
运动 状态值 3 . 目标 的 出现 与 消失 学习 
到 如何 根据 目标 的 状态 值 当前 时刻 的 
观测值 以及 数据 关联 信息 来 处理 新 目标 的 
出现 已有 目标 的 消失 问题 预测值     只 
取决于 状态值     和 循环 神经网络 隐含 层 的 
状态值     一旦 数据 关联矩阵     已经 确定 
即 已经 知道 了 目标 和 观测 之间 的 对应 
关系 我们 就 可以 根据 观测值 来 更新 状态值 完成 
校正 接下来 将 观测值 和 预测 的 状态值 拼接 在 
一起 然后 乘以 矩阵     同时     也被 
计算出来 在 确定 了 网络 的 输出 和 输出 之后 
我们 需要 定义 训练 时的/nr 损失 函数 损失 函数 定义 
为 其中     为 预测 值     为真 
实值 上面 损失 函数 的 第一 部分 为 预测误差 第二 
部分 为 更新 误差 第三 和 第四 部分 为 目标 
消失 出现 以及 回归 值 误差 这里 只是 定义 了 
某 一个 时刻 的 误差值 训练 时 需要 将 每 
一帧 的 误差值 累 加起来 然后 计算 平均值 第一 部分 
误差 的 意义 是 在 没有 观察 值 的 情况 
下 预测值 要和 目标 的 真实 运动轨迹 尽可能 接近 第二 
部分 的 意义 是 得到 观测值 之后 要将 预测值 校 
正到 和 观测值 尽可能 接近 第三 部分 损失 反应 了 
目标 的 出现 与 消失 如果     表示 一个 
目标 存在 如果 表示 这个 目标 不 存在 为此 我们 
定义 交叉 熵 损失 函数 最后 一个 问题 是 数据 
关联 数据 关联 的 目标 是 为 每个 目标 分配 
一个 唯一 的 观测值 这 是 一个 组合 优化 问题 
直接 求解 的话 是 NP 完全问题 在 这里 采用 LSTM 
网络 通过 学习 来 解决 此 问题 在 这里 网络 
的 输入 是 成对 距离矩阵 C 这 是 一个 NxM 
的 矩阵 矩阵 元素 定义 为 即 第 i 个 
目标 的 预测 状态 与 第 j 个 观察 值 
之间 的 欧氏距离 当然 我们 也 可以 使用 更多 的 
信息 如 目标 的 外观 或 其他 相似 度 网络 
的 输出 值 为 概率 向量     表示 第 
i 个 目标 与 所有 观测值 之间 的 分配 概率 
这 可以 通过 softmax 层 输出 这里 的 是 数据 
关联矩阵 的 第 i 行 最后 我们 定义 网络 训练 
时的/nr 损失 函数 为 其中     是 一个 标量 
是 目标 i 的 真实 分配 值 即将 目标 i 
分配给 观测 视频 分析 视频 动作 识别 是 机器 视觉 
领域 的 一个 重要 问题 它 的 目标 是 对 
运动 物体 的 动作 进行 分类 如 人 的 站立 
坐下 等 动作 动作 识 别在 诸多 领域 有 实际 
的 应用 如 视频 监控 人机交互 游戏 控制 等 这个 
问题 可以 抽象 成 一个 时间 序列 分类 问题 以 
人 的 动作 识别 为例 它 的 输入 是 目标 
关键点 坐标 序列 如 人体 一些 关键 点 的 2D 
或 3D 坐标 输出 值 为 动作 类别 即 序列 
的 标签 值 文献 45 提出 了 一种 整合 了 
卷积 神经 网络 和 循环 神经 网络 的 框架 进行 
人体 动作 分类 的 方法 整个 系统 包括 一个 3D 
卷积 神经 网络 和 一个 循环 神经网络 其中 3D 卷积 
神经 网络 的 输入 为 多张 图像 用于 提取 一段 
视频 的 时空 特征 然后 将 提取 的 特征 序列 
送入 循环 神经 网络 中 进行 分类 在 这里 卷积 
神经 网络 的 输入 为 3D 图像 整个 视频 被 
分成 一 系列 的 固定 长度 片段 每个 片段 包括 
相同 数量 的 帧 被 处理 成 固定 大小 的 
输入 图像 第三个 卷积 层 后面 是 两个 全 连接 
层 最后/f 一个/m 全/a 连接/v 层/q 有/v 6个/mq 神经元/nz 即 
卷积 网络 的 输出 向量 为 6 维 接下来 将 
卷积 得到 的 固定 长度 的 特征向量 序列 送入 LSTM 
循环 神经网络 用 循环 神经 网络 的 输出 完成 对 
视频 的 分类 文献 46 提出 了 一种 用 双向 
LSTM 循环 神经 网络 进行 3D 手势 分类 的 方法 
在 这里 每个/r 时刻/n 用/p 加速度计/nz 和/c 陀螺仪/n 测量/vn 出手/v 
在/p 3D/i 空间/n 的/uj 加速度/n 和/c 角速度/n 形成 一个 6D 
的 向量 作为 循环 神经 网络 的 输入 这 是 
一个 序列 数据 循环 神经 网络 采用 双向 LSTM 网络 
循环 神经 网络 的 输出 向量 维数 和要/nr 分类 的 
手势 类型 数 相同 最后 通过 softmax 层 产生 概率 
输出 用于 分类 这些 都是/nr 标准 的 做法 不再 详细 
讲述 文献 47 提出 了 一种 用 分层 循环 神经 
网络 进行 人体 动作 识别 的 方法 在 这里 利用 
了 人体 骨架 的 关键 点 信息 对 骨架 关键点 
的 运动轨迹 进行 分析 整个 人体 被 分成 5个 部分 
进行 建模 分别为 四肢 和 躯干 整个 处理 流程 为 
1/m ./i 将/d 5个/mq 部分/n 分别/d 送入/v 5/m 个子/n 网络/n 
中/f 进行/v 处理/v 2/m ./i 将/d 四肢/n 和/c 躯干/n 在/p 
第一/m 步/n 中的/i 处理结果/n 分别/d 进行/v 融合/vn 送入 4 个子 
网络 中 进行 处理 3 . 将 两只 胳膊 两条腿 
躯干 在 第二 步中的/nr 处理 结果 进行 融合 送入 2 
个子 网络 中 进行 处理 4 . 将上 一步 中的 
两个 结果 融合 送入 第 4 层子 网络 中 进行 
处理 5 . 将上 一步 的 结果 送入 全 连接 
层 中 进行 处理 6 . 最后 用 softmax 层 
进行 计算 得到 分类 概率 在 这里 所有 循环 层 
都 使用 双向 循环 结构 前面 3个 循环 层 都 
采用 tanh 激活 函数 最后 一个 循环 层 采用 LSTM 
单元 循环/vn 层/q 和全/nr 连接/v 层/q 的/uj 计算/v 方式/n 和/c 
前面/f 介绍/v 的/uj 标准/n 结构/n 相同/d 在 这里 不 详细 
讲述 全 连接 层 在 各个 时刻 的 输出 向量 
被 累计 起来 然后 用 softmax 层 进行 概率 输出 
整个 网络 的 输入 为 人体 各个 部位 关键点 的 
3D 坐标 送入 网络 之前 对 坐标 进行 了 归一化 
处理 要 识别 的 动作 类型 根据 实际 应用 而定 
文献 48 提出 了 一种 整合 卷积 神经 网络 和 
循环 神经 网络 的 视频 识别方法 在 这里 用 卷积 
网络 提取 单帧/nr 图像 的 特征 多个 帧 的 特征 
依次 被 送入 循环 神经 网络 中 进行 处理 这种 
结构 不仅 在 空间 上 具有 深度 在 时间 上 
也 具有 深度 称为 Long term Recurrent Convolutional Networks 简称 
LRCNs 整个 系统 的 输入 是 一系列 的 视频 帧 
对于 每 一帧 首先 经过 卷积 网络 的 作用 产生 
固定 长度 的 输出 向量 经过 这 一步 我们 得到 
一个 固定 长度 的 序列 数据 这个 序列 数据 被 
送入 循环 神经 网络 中 进行 处理 得到 输出 值 
最后 经过 softmax 层 得到 概率 输出 这里/r 的/uj 卷积/n 
网络/n 和/c 循环/vn 神经/n 网络/n 的/uj 变换/v 和/c 前面/f 介绍/v 
的/uj 标准/n 做法/v 一致/d 不再 重复 介绍 假设 循环 神经 
网络 的 学习 参数 为 V 和W/nr 训练 时的/nr 损失 
函数 定义 为 这一 框架 可以 用于 以下 三 种 
情况 1 . 序列 输入 固定 长度 输出 即 实现 
映射     典型 的 是 视频 动作 识别 在 
这里 输入 是 多个 视频 帧 输出 是 动作 类别 
2 . 固定 长度 输入 序列 输出 即 实现 映射 
    典型 的 是 生成 图像 的 描述 如 
给 图像 生成 文字说明 3 . 序列 输入 序列 输出 
即 实现 映射     典型 的 是 视频 描述 
如 为 一段 视频 生成 一段 文字 解说 参考文献 1 
Mikael Boden . A guide to recurrent neural networks and 
backpropagation . 2001 . 2 Ronald J Williams David Zipser 
. A learning algorithm for continually running fully recurrent neural 
networks . 1989 Neural Computation . 3 Fernando J Pineda 
. Generalization of back propagation to recurrent neural networks . 
1987 Physical Review Letters . 4 Paul J Werbos . 
Backpropagation through time what it does and how to do 
it . 1990 Proceedings of the IEEE . 5 Xavier 
Glorot Yoshua Bengio . On the difficulty of training recurrent 
neural networks . 2013 international conference on machine learning . 
6 Y . Bengio P . Simard P . Frasconi 
. Learning long term dependencies with gradient descent is difficult 
. IEEE Transactions on Neural Networks 5 2 157 166 
1994 . 7 . Hochreiter J . Schmidhuber . Long 
short term memory . Neural computation 9 8 1735 1780 
1997 . 8 Kyunghyun Cho Bart Van Merrienboer Caglar Gulcehre 
Dzmitry Bahdanau Fethi Bougares Holge . Learning Phrase Representations using 
RNN Encoder Decoder for Statistical Machine Translation . 2014 empirical 
methods in natural language processing . 9 M . Schuster 
and K . K . Paliwal . Bidirectional recurrent neural 
networks . IEEE Transactions on Signal Processing 45 11 2673 
2681 1997 . 10 Junyoung Chung Caglar Gulcehre Kyunghyun Cho 
Yoshua Bengio . Gated Feedback Recurrent Neural Networks . 2015 
international conference on machine learning . 11 Razvan Pascanu Caglar 
Gulcehre Kyunghyun Cho Yoshua Bengio . How to Construct Deep 
Recurrent Neural Networks . 2014 international conference on learning representations 
. 12 Alex Graves . Supervised Sequence Labelling with Recurrent 
Neural Networks . 13 Alex Graves Santiago Fernandez Faustino J 
Gomez Jurgen Schmidhuber . Connectionist temporal classification labelling unsegmented sequence 
data with recurrent neural networks . 2006 international conference on 
machine learning . 14 Alex Graves Navdeep Jaitly . Towards 
End To End Speech Recognition with Recurrent Neural Networks . 
2014 international conference on machine learning . 15 Ilya Sutskever 
Oriol Vinyals Quoc V Le . Sequence to Sequence Learning 
with Neural Networks . 2014 neural information processing systems . 
16 Oriol Vinyals Suman Ravuri and Daniel Povey . Revisiting 
Recurrent Neural Networks for Robust ASR . ICASSP 2012 . 
17 A . Graves A . Mohamed G . Hinton 
Speech Recognition with Deep Recurrent Neural Networks ICASSP 2013 . 
18 Alex Graves Santiago Fernandez Juergen Schmidhuber . Multi dimensional 
recurrent neural networks . 2007 international conference on artificial neural 
networks . 19 Dario Amodei Sundaram Ananthanarayanan Rishita Anubhai Jingliang 
Bai Eric Battenberg . Deep speech 2 end to end 
speech recognition in English and mandarin . 2016 international conference 
on machine learning . 20 Hasim Sak Andrew W Senior 
Kanishka Rao Francoise Beaufays . Fast and Accurate Recurrent Neural 
Network Acoustic Models for Speech Recognition . 2015 conference of 
the international speech communication association 21 Miao Yajie Mohammad Gowayyed 
and Florian Metze . EESEN End to end speech recognition 
using deep RNN models and WFST based decoding . 2015 
IEEE Workshop on Automatic Speech Recognition and Understanding ASRU . 
IEEE 2015 . 22 Bahdanau Dzmitry et al . End 
to end attention based large vocabulary speech recognition . 2016 
IEEE International Conference on Acoustics Speech and Signal Processing ICASSP 
. IEEE 2016 . 23 Chan William et al . 
Listen attend and spell A neural network for large vocabulary 
conversational speech recognition . 2016 IEEE International Conference on Acoustics 
Speech and Signal Processing ICASSP . IEEE 2016 . 24 
H . Sak Hasim Senior Andrew and Beaufays Francoise . 
Long short term memory recurrent neural network architectures for large 
scale acoustic modeling . In Inter speech 2014 . 25 
Sainath Tara Vinyals Oriol Senior Andrew and Sak Hasim . 
Convolutional long short term memory fully connected deep neural networks 
. In ICASSP 2015 . 26 Chorowski Jan Bahdanau Dzmitry 
Cho Kyunghyun and Bengio Yoshua . End to End continuous 
speech recognition using attention based recurrent nn First results . 
abs / 1412.1602 2015 .   http / / arxiv 
. org / 1412.1602 27 Hannun Awni Case Carl Casper 
Jared Catanzaro Bryan Diamos Greg Elsen Erich Prenger Ryan Satheesh 
Sanjeev Sengupta Shubho Coates Adam and Ng Andrew Y . 
Deep speech Scaling up end to end speech recognition . 
1412.5567 2014a .   http / / arxiv . org 
/ abs / 1412.5567 . 28 A . Graves . 
Sequence transduction with recurrent neural networks . ICML Representation Learning 
Workshop 2012 . 29 Bahdanau Dzmitry Chorowski Jan Serdyuk Dmitriy 
Brakel Philemon and Bengio Yoshua . End to end attention 
based large vocabulary speech recognition . abs / 1508.04395 2015 
.   http / / arxiv . org / abs 
. 1508.04395 . 30 Tomas Mikolov Martin Karafiat Lukas Burget 
Jan Cernock ý Sanjeev Khudanpur . Recurrent neural network based 
language model . 2010 conference of the international speech communication 
association . 31 Guillaume Lample Miguel Ballesteros Sandeep Subramanian Kazuya 
Kawakami Chris Dyer . Neural architectures for named entity recognition 
. 2016 north american chapter of the association for computational 
linguistics . 32 Peilu Wang Yao Qian Frank K Soong 
Lei He Hai Zhao . Part of Speech Tagging with 
Bidirectional Long Short Term Memory Recurrent Neural Network . 2015 
Computation and Language . 33 Siwei Lai Liheng Xu Kang 
Liu Jun Zhao . Recurrent convolutional neural networks for text 
classification . 2015 national conference on artificial intelligence . 34 
Zichao Yang Diyi Yang Chris Dyer Xiaodong He Alexander J 
Smola Eduard H Hovy . Hierarchical Attention Networks for Document 
Classification . 2016 north american chapter of the association for 
computational linguistics . 35 Konstantin Lopyrev . Generating News Headlines 
with Recurrent Neural Networks . 2015 arXiv Computation and Language 
. 36 Kyunghyun Cho Bart Van Merrienboer Caglar Gulcehre Dzmitry 
Bahdanau Fethi Bougares . Learning Phrase Representations using RNN Encoder 
Decoder for Statistical Machine Translation . 2014 empirical methods in 
natural language processing . 37 Dzmitry Bahdanau Kyunghyun Cho Yoshua 
Bengio . Neural Machine Translation by Jointly Learning to Align 
and Translate . 2015 international conference on learning representations . 
38 Yonghui Wu et al . Google s Neural Machine 
Translation System Bridging the Gap between Human and Machine Translation 
. Technical Report 2016 . 39 Graves Alex . Generating 
sequences with recurrent neural networks . arXiv preprint arXiv 1308.0850 
2013 . 40 Shujie Liu Nan Yang Mu Li Ming 
Zhou . A Recursive Recurrent Neural Network for Statistical Machine 
Translation . 2014 meeting of the association for computational linguistics 
. 41 Junyoung Chung Caglar Gulcehre Kyunghyun Cho Yoshua Bengio 
. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence 
Modeling . 2014 arXiv Neural and Evolutionary Computing . 42 
Zhen Cui Shengtao Xiao Jiashi Feng Shuicheng Yan . Recurrently 
Target Attending Tracking . 2016 computer vision and pattern recognition 
. 43 Anton Milan Seyed Hamid Rezatofighi Anthony R Dick 
Ian D Reid Konrad Schindler . Online Multi target Tracking 
using Recurrent Neural Networks . 2016 national conference on artificial 
intelligence . 44 Peter Ondruska Ingmar Posner . Deep tracking 
seeing beyond seeing using recurrent neural networks . 2016 national 
conference on artificial intelligence . 45 M . Baccouche F 
. Mamalet C . Wolf C . Garcia and A 
. Baskurt . Sequential deep learning for human action recognition 
. In Human Behavior Understanding pages 29 39 . Springer 
2011 . 46 G . Lefebvre . Berlemont F . 
Mamalet and C . Garcia . Blstm rnn based 3d 
gesture classification . In Artificial Neural Networks and Machine Learning 
pages 381 388 . Springer 2013 . 47 Y . 
Du W . Wang and L . Wang . Hierarchical 
recurrent neural network for skeleton based action recognition . CVPR 
2015 . 48 J . Donahue L . A . 
Hendricks . Guadarrama M . Rohrbach . Venugopalan K . 
Saenko and T . Darrell . Long term recurrent convolutional 
networks for visual recognition and description . arXiv preprint arXiv 
1411.4389 2014 . 49 A . Grushin D . D 
. Monner J . A . Reggia and A . 
Mishra . Robust human action recognition via long short term 
memory . In International Joint Conference on Neural Networks pages 
1 8 IEEE 2013 . 50 Antoine Miech Ivan Laptev 
Josef Sivic . Learnable pooling with Context Gating for video 
classification . 2017 Computer Vision and Pattern Recognition . 推荐 
阅读 1   机器学习 波澜壮阔 40年   SIGAI 2018 . 
4.13 . 2   学好 机器学习 需要 哪些 数学知识 SIGAI 
2018 . 4.17 . 3   人脸识别 算法 演化史   
SIGAI 2018 . 4.20 . 4   基于 深度 学习 
的 目标 检测 算法 综述   SIGAI 2018 . 4.24 
. 5   卷积 神经网络 为什么 能够 称霸 计算机 视觉 
领域   SIGAI 2018 . 4.26 . 6   用 
一张 图 理解 SVM 的 脉络   SIGAI 2018 . 
4.28 . 7   人脸 检测 算法 综述   SIGAI 
2018 . 5.3 . 8   理解 神经 网络 的 
激活 函数   SIGAI 2018 . 5.5 . 9   
深度 卷积 神经网络 演化 历史 及 结构 改进 脉络 40页 
长文 全面 解读   SIGAI 2018 . 5.8 . 10 
  理解 梯度 下 降法   SIGAI 2018 . 5.11 
追赶 ImageNet 发力 自动 问答 领域 这个 数据集 文章 展现 
了 着 斯坦福 做 一个 自然 语言 处理 的 ImageNet 
的 野心 他 很可能 成为 自然 语言 学术界 未来 至少 
一年 内 最 流行 的 数据集 模型 在 这个 数据 
集上 做出 好成绩 可以 让 自己 的 文章 加分 不少 
被 顶 会 录取 的 几率 大大 增加 如果 读者 
想 发 顶 会 且 目前 没有 明确 的 研究 
方向 那么 刷 这个 数据集 是 一条 很好 的 道路 
于此 同时 这个 数据集 也 会为 工业界 做出 贡献 之所以 
说 会为 工业界 做出 贡献 因为 自然语言 处理 的 研究 
风气 和 图像 相比 差 一些 任务 较多 且 没有 
在 paper 里面 附带 代码 的 行业 规则 导致 很多 
工作 无法 重现 甚至/d 有些/r 人/n 会/v 连/nr 实验/vn 都/d 
不做/i 直 接往 图 和表/nr 里面 填 数 造 一篇 
文章 而 这个 数据集 学习 了 Imagenet 不给 测试 集 
这样 你 就 没法 作弊 把 代码 交上来 我 来给 
你 跑 之后 把 测试 集 合上 的 水平 评测 
出来 这样 大家 都 公平 谁 也别 吹牛 谁 也别 
作弊 此种 环境 有利于 真正 大 贡献 的 工作 得以 
浮现 例如 Residual Network 在 去年 席卷 图像 领域 在 
一个 公平 的 环境 下 以比 其他 对手 好 很多 
的 效果 呈现 在 了 世人 的 面前 而 SQuAD 
则是 斯坦福 在 自然 语言 处理 上 意图 构建 一个 
类似 ImageNet 的 测试 集合 分数 实 时在 leaderboard 上 
显示 这就 让 这个 数据集 有 如下 优势 1 . 
测试 出 真正 的 好 算法 尤其 对于 工业界 这个 
数据集 是 十分 值得 关注 的 因为 他 可以 告诉 
大家 现在 各个 算法 在 阅读 理解 或者说 自动 问答 
这个 任务 上 的 排名 我们 可以 光 看 分数 
排名 就 知道 世界 上 哪个 算法 最好 不会 再 
怀疑 是 作者 做假 了 还是 实现 的 不对 2 
. 提供 一个 阅读 理解 的 大规模 数据集 由于 之前 
的 阅读 理解 数据集 规模 太小 或者 十分 简单 用 
一个 普通 的 深度 学习 算法 就 可以 刷 到 
90% 度 所以 并 不能 很好 的 体现 不同 算法 
优劣 纵使 SQuAD 不会 像 ImageNet 有 那么 大 的 
影响力 但 绝对 也会在 接下来 的 几年 内 对 自动 
问答 领域 产生 深远 的 影响 并且 是 各大 巨头 
在 自动 问答 这个 领域 上 的 兵家必争之地 IBM 已经 
开始 了 1 . 爬 python 官网 解析 页面 html 
信息 python3 使用 urllib 库 import urllib . request1 . 
request typereq = urllib . request . Request http / 
/ python . org / resp1 = urllib . request 
. urlopen req print * * * * * * 
* * * req1 * * * * * * 
* * * * * * * print resp1 . 
read print * * * * * * * * 
* * * * * * * * * * 
* * * * 2 . request typeresp2 = urllib 
. request . urlopen http / / python . org 
/ print * * * * * * * * 
* req2 * * * * * * * * 
* * * * * print resp2 . read print 
* * * * * * * * * * 
* * * * * * * * * * 
* * # # # # # # # # 
# # # # # # # # # # 
# # # # # # # # # # 
# # # # # # # # # # 
# # # url path get response # # # 
# # # # # # # # # # 
# # # # # # # # # # 
# # # # # # # # # # 
# # # # # # # # def getPageHtml 
url response = urllib . request . urlopen url return 
response . read split page html datadef splitPageHtml resouceData dataArr 
= data for data in resouceData . split print dataArr 
len is + len dataArr print dataArr is start print 
dataArr 0 100 print dataArr is end resource = getPageHtml 
http / / python . org print splitPageHtml resource 前言 
自然语言 处理 是 文本 挖掘 的 研究 领域 之一 是 
人工 智能 和 语言学 领域 的 分支 学科 在此 领域 
中 探讨 如何 处理 及 运用 自然语言 对于 自然 语言 
处理 的 发展 历程 可以/c 从/p 哲学/n 中/f 的/uj 经验/n 
主义/n 和/c 理性主义/n 说起/v 基于 统计 的 自然 语言 处理 
是 哲学 中 的 经验 主义 基于 规则 的 自然 
语言 处理 是 哲学 中 的 理性主义 在 哲学 领域 
中 经验主义 与 理性 主义 的 斗争 一直 是 此消彼长 
这种 矛盾 与 斗争 也 反映 在 具体 科学上 如 
自然语言 处理 早期 的 自然 语言 处理 具有 鲜明 的 
经验 主义 色彩 如 1913 年 马尔科夫 提出 马尔科夫 随机 
过程 与 马尔科夫 模型 的 基础 就是 手工 查频/nr 具体 
说 就是 统计 了 欧根 奥涅金 长 诗中 元音 与 
辅音 出现 的 频度 1948 年 香农 把 离散 马尔科夫 
的 概率模型 应用于 语言 的 自动机 同时 采用 手工 方法 
统计 英语 字母 的 频率 然而 这种 经验主义 到了 乔姆斯基 
时 出现 了 转变 1956 年 乔姆斯基 借鉴 香农 的 
工作 把 有限 状态机 用作 刻画 语法 的 工具 建立 
了 自然 语言 的 有限 状态 模型 具体 来说 就是 
用 代数 和 集合 将 语言 转化 为 符号 序列 
建立 了 一大堆 有关 语法 的 数学 模型 这些 工作 
非常 伟大 为 自然 语言 和 形式语言 找到 了 一种 
统一 的 数学 描述 理论 一个 叫做 形式语言 理论 的 
新领域 诞生 了 这个 时代 经验主义 被 全盘否定 理性主义 算是 
完胜 然而 在 20 世纪 50 年代 末 到 60 
年代 中期 经验主义 东山再起 了 多数 学者 普遍 认为 只有 
详尽 的 历史 语料 才能 带来 靠谱 的 结论 于是/nr 
一些 比较 著名 的 理论 与 算法 就 诞生 了 
如 贝叶斯 方法 Bayesian Method 隐 马尔可夫 最大熵 Viterbi 算法 
支持 向量 机 之类 世界 上 第一 个 联机 语料库 
也是 在 那个 时候 的 Brown University 诞生 的 但是 
总的来说 这个 时代 依然 是 基于 规则 的 理性主义 的 
天下 经验主义 虽然 取得 了 不俗 的 成就 却 依然 
没有 受到 太大 的 重视 但是 金子 总会 发光 的 
90 年代 以来 基于 统计 的 自然 语言 处理 就 
开始 大放异彩 了 首先 是 在 机器 翻译 领域 取得 
了 突破 因为 引入 了 许多 基于 语料库 的 方法 
哈钦斯/nr 英国 著名 学者 1990 年在 芬兰 赫尔辛基 举办 的 
第 13 届 国际 计算 语言学 会议 确定 的 主题 
是 处理 大规模 真实 文本 的 理论 方法 与 工具 
大家 的 重心 开始 转向 大 规模 真实 文本 了 
传统 的 仅仅 基于 规则 的 自然 语言 处理 显然 
力不从心 了 学者 们 认为 大规模 语料 至少 是 对 
基于 规则 方法 有效 的 补充 到了 1994 ~ 1999 
年 经验主义 就 开始 空前 繁荣 了 如 句法 剖析 
词类 标注 参照 消解 话语 处理 的 算法 几乎 把 
概率 与 数据 作为 标准 方法 成为 了 自然 语言 
处理 的 主流 总之 理性主义 在 自然 语言 处理 的 
发展 史上 是 有 重要 地位 的 也 辉煌 了 
几十年 历史 事物 常常 是 此消彼长 的 至于 谁 好 
谁 坏 不是 固定 的 取决于 不同 时代 的 不同 
历史 任务 总的来说 基于 规则 的 理性主义 在 这个 时代 
被 提及 得 比较 少 用 的 也 比较 少 
主要 是 由于 以下 几个 缺陷 •   鲁棒性 差 
过于 严格 的 规则 导致 对 非本质 错误 的 零容忍 
这 一点 在 最近 的 一些 新 的 剖析 技术 
上 有所 改善 •   研究 强度 大 泛化 能力差 
一个 研究 要 语言学家 语音学 家和 各种 领域 的 专家 
配合 在 当前 大 规模 文本处理 的 时间 资源 要求 
下 太 不划算 且 机器 学习 的 方法 很难 应用 
难以 普及 •   实践性 差 基于 统计 的 经验 
主义 方法 可以 根据 数据集 不断 对 参数 进行 优化 
而 基于 规则 的 方法 就 不可以 这在 当前 数据 
量 巨大 的 情况 下 影响 是 致命 的 因为 
前者 常常 可以 通过 增大 训练 集 来 获得 更好 
的 效果 后者 则 死板 许多 结果 往往 不尽人意 但 
理性主义 还是 有 很多 优点 的 同样 经验主义 也 有 
很多 缺陷 算是 各有所长 各有所短 不同 学科 有 不同 学科 
的 研究 角度 只能 说 某些 角度 在 某个 特定 
的 历史 时期 对 提高 生产力 更 有用 所以 重视 
的 人 更多 但 有用 不 代表 胜利 暂时 的 
无用 更 不能 说 是 科学 层 面上 的 失败 
尤其 是 在 当前 中文 自然语言 处理 发展 还 不甚 
成熟 的 时期 私 以为 基于 统计 的 方法 在 
很多 方面 并不 完美 理性主义 的 作用 空间 还 很大 
需要 更多 的 人去 关注 助力 统计 自然语言 处理 宗 
成庆 自然语言 处理 涉及 的 范畴 如下 维基百科 •   
中文 自动 分词 Chinese word segmentation •   词性 标注 
Part of speech tagging •   句法分析 Parsing •   
自然语言 生成 Natural language generation •   文本 分类 Text 
categorization •   信息检索 Information retrieval •   信息 抽取 
Information extraction •   文字 校对 Text proofing •   
问答 系统 Question answering •   机器翻译 Machine translation • 
  自动 摘要 Automatic summarization 本文 针对 其 中 几个 
主要 领域 的 研究 现状 和 进展 通过 论文 博客 
等 资料 结合 自身 的 学习 和 实践 经历 进行 
浅显 地 介绍 由于 个人 实践 经验 不足 除 中文分词 
自动 文摘 文本 分类 情感 分析 和 话题 模型 方面 
进行 过 实际 业务 的 实践 其他 方面 经验 欠缺 
若有 不当之处 欢迎 童鞋 们 批评指正 目录 一 . 中文分词 
中文分词 主要 包括 词 的 歧义 切分 和未/nr 登录 词 
识别 主要 可以 分为 基于 词典 和 基于 统计 的 
方法 最新 的 方法 是 多种 方法 的 混合 从 
目前 汉语分词 研究 的 总体 水平 看 F1 值 已经 
达到 95% 左右 主要 分词 错误 是由 新词 造成 的 
尤其 对 领域 的 适应性 较差 下面 主要 介绍 一下 
中文分词 存在 的 主要 问题 和 分词 方法 1 . 
问题 1.1 歧义 切分 切分 歧义 处理 包括 两 部分 
内容 •   切分 歧义 的 检测 •   切分 
歧义 的 消解 这 两部 分在 逻辑 关系 上 可 
分成 两个 相对 独立 的 步骤 • 切分 歧义 的 
检测 最大 匹 配法 精确 的 说法 应该 叫 最 
长词 优先 匹 配法 是 最早 出现 同时 也 是 
最 基本 的 汉语 自动 分词 方法 依 扫描 句子 
的 方向 又分 正向 最大 匹配 MM 从 左向右 和 
逆向 最大 匹配 RMM 从右 向左 两种 最大 匹 配法 
实际 上将 切分 歧义 检测 与 消解 这两个 过程 合二为一 
对 输入 句子 给出 唯一 的 切分 可能性 并以 之为 
解 从 最大 匹 配法 出发 导 出了 双向 最大 
匹 配法 即 MM ＋ RMM 双向 最大 匹 配法 
存在着 切分 歧义 检测 盲区 针对 切分 歧义 检测 另外 
两个 有 价值 的 工作 是 最 少分 词法 这种方法 
歧义 检测 能力 较 双向 最大 匹 配法 要 强些 
产生 的 可能 切分 个数 仅 略有 增加 和 全 
切分法 这种方法 穷举 所有 可能 的 切分 实现 了 无 
盲区 的 切分 歧义 检测 但 代价 是 导致 大量 
的 切分 垃圾 • 切分 歧义 的 消解 典型 的 
方法 包括 句法 统计 和 基于 记忆 的 模型 句法 
统计 将 自动 分词 和 基于 Markov 链 的 词性 
自动 标注 技术 结合 起来 利用 从 人工 标注 语料库 
中提 取出 的 词性 二元 统计 规律 来 消解 切分 
歧义 基于 记忆 的 模型 对 伪 歧义 型 高频 
交集 型 歧义 切分 可以 把 它们 的 正确 唯一 
切分 形式 预先 记录 在 一张 表中 其 歧义 消解 
通过 直接 查表 即可 实现 1.2 未 登录 词 识别 
未 登录 词 大致 包含 两大类 •   新 涌现 
的 通用 词 或 专业 术语 等 •   专有名词 
如 中国 人 名 外国 译名 地名 机构 名 泛指 
机关 团体 和 其它 企事业 单位 等 前一种 未 登录 
词 理论上 是 可预期 的 能够 人工 预先 添加到 词 
表中 但这 也 只是 理想 状态 在 真实 环境 下 
并 不易 做到 后 一种 未 登录 词 则 完全 
不 可预期 无论 词表 多么 庞大 也 无法 囊括 真实 
文本 中 即便 是 大众 通用 领域 未 登录 词 
对 分词 精度 的 影响 超过 了 歧义 切分 未 
登录 词 处理 在 实用型 分词 系统 中 占 的 
份量 举足轻重 •   新 涌现 的 通用 词 或 
专业 术语 对 这类 未 登录 词 的 处理 一般 
是 在 大规模 语料库 的 支持 下 先由 机器 根据 
某种 算法 自动 生成 一张 候选 词表 无 监督 的 
机器 学习策略 再 人工 筛选 出 其中 的 新词 并 
补充 到 词 表中 鉴于 经过 精加工 的 千万字 甚至 
亿字 级 的 汉语分词 语料库 目前 还是 水月镜花 所以/c 这个/r 
方向/n 上/f 现有/b 的/uj 研究/vn 无一/i 不以/c 从极/nr 大规模/b 生/vn 
语料库/n 中/f 提炼/v 出/v 的/uj n 元汉/nr 字串 之 分布 
n ≥ 2 为基础 其中 汉字 之间 的 结合力 通过 
全局 统计量 包括 互信息 t 测试 差 卡方 统计量 字串 
频 等 来 表示 •   专有名词 对 专有 名词 
的 未 登录 词 的 处理 首先 依据 从 各类 
专有名词 库 中 总结 出 的 统计 知识 如 姓氏 
用字 及其 频度 和 人工 归纳 出 的 专有 名词 
的 某些 结构 规则 在 输入 句子 中 猜测 可能 
成为 专有 名词 的 汉字 串 并 给出 其 置信度 
之后 利用 对 该类 专有名词 有 标识 意义 的 紧邻 
上下文 信息 如 称谓 以及 全局 统计量 和 局部 统计量 
局部 统计量 是 相对 全局 统计量 而言 的 是 指 
从 当前 文章 得到 且 其 有效 范围 一般 仅限于 
该 文章 的 统计量 通常 为 字串 频 进行 进一步 
的 鉴定 已有 的 工作 涉及 了 四种 常见 的 
专有名词 中国 人 名 的 识别 外国 译名 的 识别 
中国 地名 的 识别 及 机构 名 的 识别 从 
各家 报告 的 实验 结果 来看 外国 译名 的 识别 
效果 最好 中国 人 名 次之 中国 地名 再 次之 
机构 名 最差 而 任务 本身 的 难度 实质上 也是 
遵循 这个 顺序 由 小 增大 沈达 阳 孙茂松/nr 等/u 
1997b 特别 强调 了 局部 统计量 在 未 登录 词 
处理 中 的 价值 2 . 方法 2.1 基于 词典 
的 方法 在 基于 词典 的 方法 中 对于 给定 
的 词 只有 词典 中 存在 的 词语 能够 被 
识别 其中 最 受欢迎 的 方法 是 最大 匹 配法 
MM 这种 方法 的 效果 取决于 词典 的 覆盖度 因此 
随着 新词 不断出现 这种 方法 存在 明显 的 缺点 2.2 
基于 统计 的 方法 基于 统计 的 方法 由于 使用 
了 概率 或 评分 机制 而非 词典 对 文本 进行 
分词 而被 广泛应用 这种 方法 主要 有 三个 缺点 一是 
这种方法 只能 识别 OOV out of vocabulary 词 而 不能 
识别 词 的 类型 比如 只能 识别 为 一串 字符串 
而 不能 识别 出 是 人名 二 是 统计 方法 
很难 将 语言 知识 融入 分词 系统 因此 对于 不 
符合 语言 规范 的 结果 需要 额外 的 人工 解析 
三 是 在 许多 现在分词 系统 中 OOV 词 识别 
通常 独立 于 分词 过程 二 . 词性 标注 词性 
标注 是 指为 给定 句子 中 的 每个 词 赋予 
正确 的 词法 标记 给定 一个 切 好词 的 句子 
词性 标注 的 目的 是 为 每一个 词 赋予 一个 
类别 这个 类别 称为 词性 标记 part of speech tag 
比如 名词 noun 动词 verb 形容词 adjective 等 它 是 
自然 语言 处理 中 重要 的 和 基础 的 研究 
课题 之一 也是 其他 许多 智能 信息 处理 技术 的 
基础 已被 广泛 的 应用 于 机器翻译 文字 识别 语音 
识别 和 信息检索 等 领域 词性 标注 对于 后续 的 
自然 语言 处理 工作 是 一个 非常 有用 的 预 
处理过程 它 的 准确 程度 将 直接 影响 到 后续 
的 一系列 分析处理 任务 的 效果 长期以来 兼类 词 的 
词性 歧义 消解 和 未知 词 的 词性 识别 一直 
是 词性 标注 领域 需要 解决 的 热点 问题 当 
兼类 词 的 词性 歧义 消解 变得 困难 时 词性 
的 标注 就 出现 了 不确定性 的 问题 而对 那些 
超出 了 词典 收录 范围 的 词语 或者 新 涌现 
的 词语 的 词性 推测 也 是 一个 完整 的 
标注 系统 所 应 具备 的 能力 1 . 词性 
标注 方法 词性 标注 是 一个 非常 典型 的 序列 
标注 问题 最初 采用 的 方法 是 隐 马尔科夫 生成式 
模型 然后 是 判别式 的 最大熵 模型 支持 向量 机 
模型 目前 学术界 通常 采用 结构 感知器 模型 和 条件 
随 机场 模型 近年来 随着 深度 学习 技术 的 发展 
研究者 们 也 提出 了 很多 有效 的 基于 深层 
神经 网络 的 词性 标注 方法 迄今为止 词性 标注 主要 
分为 基于 规则 的 和 基于 统计 的 方法 • 
  规则 方法 能 准确 地 描述 词性 搭配 之间 
的 确定 现象 但是 规则 的 语言 覆盖面 有限 庞大 
的 规则 库 的 编写 和 维护 工作 则 显得 
过于 繁重 并且 规则 之间 的 优先级 和 冲突 问题 
也 不容易 得到 满意 的 解决 •   统计 方法 
从 宏观 上 考虑 了 词性 之间 的 依存 关系 
可以 覆盖 大 部分 的 语言 现象 整体 上 具有 
较高 的 正确率 和 稳定性 不过 其 对 词性 搭配 
确定 现象 的 描述 精度 却 不如 规则 方法 针对 
这样 的 情况 如何 更好 地 结合 利用 统计 方法 
和 规则 处理 手段 使 词性 标注 任务 既 能够 
有效地 利用 语言学家 总结 的 语言 规则 又 可以 充分 
地 发挥 统计 处理 的 优势 成为 了 词性 标注 
研究 的 焦点 2 . 词性 标注 研究进展 •   
词性 标注 和 句法分析 联合 建模 研究者 们 发现 由于 
词性 标注 和 句法分析 紧密 相关 词性 标注 和 句法分析 
联合 建模 可以 同时 显著 提高 两 个 任务 准确率 
•   异构 数据 融合 汉语 数据 目前 存在 多 
个 人工 标注 数据 然而 不同 数据 遵守 不同 的 
标注 规范 因此 称为 多源 异构 数据 近年来 学者 们 
就 如何 利用 多源 异构 数据 提高 模型 准确率 提出 
了 很多 有效 的 方法 如 基于 指导 特征 的 
方法 基于 双 序列 标注 的 方法 以及 基于 神经 
网络 共享 表示 的 方法 •   基于 深度 学习 
的 方法 传统 词性 标注 方法 的 特征 抽取 过程 
主要 是 将 固定 上下文 窗口 的 词 进行 人工 
组合 而 深度 学习 方法 能够 自动 利用 非线性 激活 
函数 完成 这 一 目标 进一步 如果 结合 循环 神经网络 
如 双向 LSTM 则 抽 取到 的 信息 不再 受到 
固定 窗口 的 约束 而是 考虑 整个 句子 除此之外 深度 
学习 的 另一 个 优势 是 初始 词 向量 输入 
本身 已经 刻画 了 词语 之间 的 相似 度 信息 
这对 词性 标注 非常重要 三 . 句法分析 语言 语法 的 
研究 有 非常 悠久 的 历史 可以 追溯 到 公元前 
语言学家 的 研究 不同 类型 的 句法分析 体现 在 句法 
结构 的 表示 形式 不同 实现 过程 的 复杂 程度 
也 有所不同 因此 科研 人员 采用 不同 的 方法 构建 
符合 各个 语法 特点 的 句法分析 系统 其 主要 分 
类如 下图 所示 下文 主要 对 句法分析 技术 方法 和 
研究 现状 进行 总结 分析 1 . 依存 句法分析 依存 
语法 存在 一个 共同 的 基本 假设 句法结构 本质上 包含 
词 和词/nr 之间 的 依存 修饰 关系 一个 依存关系 连接 
两个 词 分别 是 核心 词 head 和依/nr 存词 dependent 
依存 关系 可以 细分 为 不同 的 类型 表示 两个 
词 之间 的 具体 句法关系 目前 研究 主要 集中 在 
数据 驱动 的 依存 句法 分析方法 即在 训练 实例 集合 
上 学习 得到 依存 句法 分析器 而不 涉及 依存 语法 
理论 的 研究 数据 驱动 的 方法 的 主要 优势 
在于 给定 较大 规模 的 训练 数据 不 需要 过多 
的 人工干预 就 可以 得到 比较 好 的 模型 因此 
这类 方法 很 容易 应用到 新 领域 和新/nr 语言 环境 
数据 驱动 的 依存 句法分析 方法 主要 有 两种 主流 
方法 基于 图 graph based 的 分析 方法 和 基于 
转移 transition based 的 分析 方法 2.1 基于 图 的 
依存 句法 分析方法 基于 图 的 方法 将 依存 句法分析 
问题 看成 从 完全 有向图 中 寻找 最大 生成树 的 
问题 一棵 依存 树 的 分值 由 构成 依存 树 
的 几种 子树 的 分值 累加 得到 根据 依存 树 
分值 中 包含 的 子树 的 复杂度 基于 图 的 
依存 分析模型 可以 简单 区 分为 一 阶 和 高阶 
模型 高阶 模型 可以 使用 更加 复杂 的 子树 特征 
因此 分析 准确率 更高 但是 解码 算法 的 效率 也 
会 下降 基于 图 的 方法 通常 采用 基于 动态规划 
的 解码 算法 也 有 一些 学者 采用 柱 搜索 
beam search 来 提高 效率 学习 特征 权重 时 通常 
采用 在线 训练 算法 如 平均 感知器 averaged perceptron 2.2 
基于 转移 的 依存 句法 分析方法 基于 转移 的 方法 
将 依存 树 的 构成 过程 建模 为 一个 动作 
序列 将 依存 分析 问题 转化 为 寻找 最优 动作 
序列 的 问题 早期 研究者 们 使用 局部 分类器 如 
支持 向量 机 等 决定 下 一个 动作 近年来 研究者 
们 采用 全局 线性 模型 来 决定 下 一个 动作 
一个 依存 树 的 分值 由其 对应 的 动作 序列 
中 每一个 动作 的 分值 累加 得到 特征 表示 方面 
基于 转移 的 方法 可以 充分 利用 已 形成 的 
子树 信息 从而 形成 丰富 的 特征 以 指导 模型 
决策 下 一个 动作 模型 通过 贪心 搜索 或者 柱 
搜索 等 解码 算法 找到 近似 最优 的 依存 树 
和 基于 图 的 方法 类似 基于 转移 的 方法 
通常 也 采用 在线 训练 算法 学习 特征 权重 2.3 
多/m 模型/n 融合/vn 的/uj 依存/v 句法/n 分析方法/n 基于/p 图/n 和/c 
基于/p 转移/v 的/uj 方法/n 从/p 不同/a 的/uj 角度/n 解决问题/n 各 
有优势 基于 图 的 模型 进行 全局 搜索 但 只能 
利用 有限 的 子树 特征 而 基于 转移 的 模型 
搜索 空间 有限 但 可以 充分 利用 已 构成 的 
子树 信息 构成 丰富 的 特征 详细 比较 发现 这 
两种 方法 存在 不同 的 错误 分布 因此 研究者 们 
使用 不同 的 方法 融合 两种 模型 的 优势 常见 
的 方法 有 stacked learning 对 多个 模型 的 结果 
加权 后 重新 解码 re parsing 从 训练 语料 中 
多次 抽样 训练 多个 模型 bagging 2 . 短语 结构 
句法分析 分词 词性 标注 技术 一般 只需 对 句子 的 
局部 范围 进行 分析 处理 目前 已经 基本 成熟 其 
标志 就是 它们 已经 被 成功 地 用于 文本检索 文本 
分类 信息 抽取 等 应用 之中 而 句法分析 语义分析 技术 
需要 对 句子 进行 全局 分析 目前 深层 的 语言 
分析 技术 还 没有 达到 完全 实用 的 程度 短语 
结构 句法分析 的 研究 基于 上下文 无关 文法 Context Free 
Grammar CFG 上下文 无关 文法 可以 定义 为 四元组 其中 
T 表示 终结符 的 集合 即 词 的 集合 N 
表示 非 终结符 的 集合 即 文法 标注 和 词性 
标记 的 集合 S 表示 充当 句法 树根 节点 的 
特殊 非 终结符 而 R 表示 文法 规则 的 集合 
其中 每条 文法 规则 可以 表示 为   Ni ® 
g   这里 的 g 表示 由 非 终结符 与 
终结符 组成 的 一个 序列 允许 为 空 根据 文法 
规则 的 来源 不同 句法 分析器 的 构建 方法 总体 
来说 可以 分为 两大类 •   人工 书写 规则 • 
  从 数据 中 自动 学习 规则 人工 书写 规则 
受限于 规则 集合 的 规模 随着 书写 的 规则 数量 
的 增多 规则 与 规则 之间 的 冲突 加剧 从而 
导致 继续 添加 规则 变得 困难 与 人工 书写 规模 
相比 自动 学习 规则 的 方法 由于 开发 周期 短 
和 系统 健壮 性强 等 特点 加上 大 规模 人工 
标注 数据 比如 宾州 大学 的 多语 种树 库 的 
推动 作用 已经 成为 句法分析 中的 主流 方法 而 数据 
驱动 的 方法 又 推动 了 统计 方法 在 句法分析 
领域 中 的 大量 应用 为了 在 句法分析 中 引入 
统计 信息 需要 将 上下文 无关 文法 扩展 成为 概率 
上下文 无关 文法 Probabilistic Context Free Grammar PCFG 即为 每条 
文法 规则 指定 概率值 概率 上下文 无关 文法 与非 概率 
化 的 上下文 无关 文法 相同 仍然 表示 为 四元组 
区别 在于 概率 上下文 无关 文法 中的 文法 规则 必须 
带有 概率值 获得 概率 上下文 无关 文法 的 最简单 的 
方法 是 直接 从树库/nr 中 读取 规则 利用 最大 似 
然 估计 Maximum Likelihood Estimation MLE 计算 得到 每条 规则 
的 概率值 使用 该 方法 得到 的 文法 可以 称为 
简单 概率 上下文 无关 文法 在 解码 阶段 CKY 10 
等 解码 算法 就 可以 利用 学习 得到 的 概率 
上下文 无关 文法 搜索 最优 句法树 虽然 基于 简单 概率 
上下文 无关 文法 的 句法 分析器 的 实现 比较简单 但是 
这类 分析器 的 性能 并 不能 让 人 满意 性能 
不佳 的 主要 原因 在于 上下文 无关 文法 采取 的 
独立性 假设 过强 一条 文法 规则 的 选择 只 与 
该 规则 左侧 的 非 终结符 有关 而与 任何 其它 
上下文 信息 无关 文法 中 缺乏 其它 信息 用于 规则 
选择 的 消 歧 因此 后继 研究 工作 的 出发点 
大都 基于 如何 弱化 上下文 无关 文法 中的 隐含 独立性 
假设 3 . 总结 分词 词性 标注 技术 一般 只需 
对 句子 的 局部 范围 进行 分析 处理 目前 已经 
基本 成熟 其 标志 就是 它们 已经 被 成功 地 
用于 文本检索 文本 分类 信息 抽取 等 应用 之中 而 
句法分析 语义分析 技术 需要 对 句子 进行 全局 分析 目前 
深层 的 语言 分析 技术 还 没有 达到 完全 实用 
的 程度 四 . 文本 分类 文本 分类 是 文本 
挖掘 的 核心 任务 一直以来 倍受 学术界 和 工业界 的 
关注 文本 分类 Text Classification 的 任务 是 根据 给定 
文档 的 内容 或 主题 自动 分配 预先 定义 的 
类别 标签 对 文档 进行 分类 一般 需要 经过 两个 
步骤 •   文本 表示 •   学习 分类 文本 
表示 是 指 将 无 结构化 的 文本 内容 转化成 
结构化 的 特征向量 形式 作为 分类 模型 的 输入 在 
得到 文本 对应 的 特征向量 后 就 可以 采用 各种 
分类 或 聚 类 模型 根据 特征向量 训练 分类器 或 
进行 聚 类 因此 文本 分类 或 聚 类 的 
主要 研究 任务 和 相应 关键 科学 问题 如下 1 
. 任务 1.1 构建 文本 特征向量 构建 文本 特征向量 的 
目的 是 将 计算机 无法 处理 的 无 结构 文本 
内容 转换 为 计算机 能够 处理 的 特征向量 形式 文本 
内容 特征向量 构建 是 决定 文本 分类 和聚类/nr 性能 的 
重要 环节 为了 根据 文本 内容 生成 特征向量 需要 首先 
建立 特征 空间 其中 典型 代表 是 文本 词 袋 
Bag of Words 模型 每个 文档 被 表示 为 一个 
特征向量 其 特征向量 每 一维 代表 一个 词 项 所有 
词 项 构成 的 向量 长度 一般 可以 达到 几万 
甚至 几百万 的 量级 这样 高维 的 特征向量 表示 如果 
包含 大量 冗余 噪音 会 影响 后续 分 类聚 类 
模型 的 计算 效率 和 效果 因此 我们 往往 需要 
进行 特征选择 Feature Selection 与 特征提取 Feature Extraction 选取 最 
具有 区分 性 和 表达 能力 的 特征 建立 特征 
空间 实现 特征 空间 降 维 或者 进行 特征 转换 
Feature Transformation 将 高维 特征向量 映 射到 低维 向量空间 特征选择 
提取 或 转换 是 构建 有效 文本 特征向量 的 关键 
问题 1.2 建立 分类 或 聚 类 模型 在 得到 
文本 特征向量 后 我们 需要 构建 分类 或 聚 类 
模型 根据 文本 特征向量 进行 分类 或 聚 类 其中 
分类 模型 旨在 学习 特征向量 与 分类 标签 之间 的 
关联关系 获得 最佳 的 分类 效果 而 聚 类 模型 
旨在 根据 特征向量 计算 文本 之间 语义 相似 度 将 
文本 集合 划分 为 若干 子集 分类 和聚类/nr 是 机器学习 
领域 的 经典 研究 问题 我们 一般 可以 直接 使用 
经典 的 模型 或 算法 解决 文本 分类 或 聚 
类 问题 例如 对于 文本 分类 我们 可以 选用 朴素 
贝叶斯 决策树 k NN 逻辑 回归 Logistic Regression 支持 向量 
机 Support Vector Machine SVM 等 分类 模型 对于 文本 
聚 类 我们 可以 选用 k means 层次 聚 类 
或 谱 聚 类 spectral clustering 等 聚 类 算法 
这些 模型 算法 适用 于 不同 类型 的 数据 而 
不仅 限于 文本 数据 但是 文本 分类 或 聚 类 
会 面临 许多 独特 的 问题 例如 如何 充分 利用 
大量 无 标注 的 文本 数据 如何 实现 面向 文本 
的 在线 分类 或 聚 类 模型 如何 应对 短 
文本 带来 的 表示 稀疏 问题 如何 实现 大 规模 
带 层次 分类 体系 的 分类 功能 如何 充分 利用 
文本 的 序列 信息 和 句法 语义 信息 如何 充分 
利用 外部 语言 知识库 信息 等等 这些/r 问题/n 都是/nr 构建/v 
文本/n 分类/n 和聚类/nr 模型/n 所/c 面临/v 的/uj 关键/n 问题/n 2 
. 模型 2.1 文本 分类 模型 近年来 文本 分类 模型 
研究 层出不穷 特别 是 随着 深度 学习 的 发展 深度 
神经网络 模型 也在 文本 分类 任务 上 取得 了 巨大 
进展 我们 将 文本 分类 模型 划分 为 以下 三类 
•   基于 规则 的 分类 模型 基于 规则 的 
分类 模型 旨在 建立 一个 规则 集合 来 对 数据 
类别 进行 判断 这些 规则 可以 从 训练 样 本里 
自动 产生 也 可以 人工 定义 给定 一个 测试 样例 
我们 可以 通过 判断 它 是否 满足 某 些 规则 
的 条件 来 决定 其 是否 属于 该 条规 则 
对应 的 类别 典型 的 基于 规则 的 分类 模型 
包括 决策树 Decision Tree 随机 森林 Random Forest RIPPER 算法 
等 •   基于 机器 学习 的 分类 模型 典型 
的 机器学习 分类 模型 包括 贝叶斯 分类器 Na ï ve 
Bayes 线性 分类器 逻辑 回归 支持 向量 机 Support Vector 
Machine SVM 最大熵 分类器 等 SVM 是 这些 分类 模型 
中 比较 有效 使用 较为 广泛 的 分类 模型 它 
能够 有效 克服 样本分布 不均匀 特征 冗余 以及 过拟合 等 
问题 被 广泛 应用 于 不同 的 分类 任务 与 
场景 通过 引入 核 函数 SVM 还 能够 解决 原始 
特征 空间 线性 不 可分 的 问题 除了 上述 单 
分类 模型 以 Boosting 为 代表 的 分类 模型 组合 
方法 能够 有效地 综合 多个 弱 分类 模型 的 分类 
能力 在 给定 训练 数据集 合上 同时 训练 这些 弱 
分类 模型 然后 通过 投票 等 机制 综合 多个 分类器 
的 预测 结果 能够 为 测试 样例 预测 更 准确 
的 类别 标签 •   基于 神经 网络 的 方法 
以 人工 神经 网络 为 代表 的 深度 学习 技术 
已经 在 计算机 视觉 语音 识别 等 领域 取得 了 
巨大 成功 在 自然 语言 处理 领域 利用 神经 网络 
对 自然 语言 文本 信息 进行 特征 学习 和 文本 
分类 也 成为 文本 分类 的 前沿 技术 前 向 
神经网络 多层 感知机 Multilayer Perceptron MLP 是 一种 典型 的 
前 向 神经网络 它 能够 自动 学习 多层 神经网络 将 
输入 特征向量 映射 到 对应 的 类别 标签 上 通过 
引入 非线性 激活 层 该 模型 能够 实现 非 线性 
的 分类 判别式 包括 多层 感知机 在内 的 文本 分类 
模型 均 使用 了 词 袋 模型 假设 忽略 了 
文本 中 词序 和 结构化 信息 对于 多层 感知机 模型 
来说 高 质量 的 初始 特征 表示 是 实现 有效 
分类 模型 的 必要条件 为了 更加 充分 地 考虑 文本 
词序 信息 利用 神经网络 自动 特征 学习 的 特点 研究者 
后续 提出 了 卷积 神经网络 Convolutional Neural Network CNN 和 
循环 神经网络 Recurrent Neural Network RNN 进行 文本 分类 基于 
CNN 和 RNN 的 文本 分类 模型 输入 均为 原始 
的 词 序列 输出 为 该 文本 在 所有 类别 
上 的 概率分布 这里 词序 列中 的 每个 词 项 
均以 词 向量 的 形式 作为 输入 卷积 神经网络 CNN 
卷积 神经网络 文本 分类 模型 的 主要 思想 是 对词 
向量 形式 的 文本 输入 进行 卷积 操作 CNN 最初 
被 用于 处理 图像 数据 与 图像 处理 中 选取 
二维 域 进行 卷积 操作 不同 面向 文本 的 卷积 
操作 是 针对 固定 滑动 窗口 内 的 词 项 
进行 的 经过 卷积 层 池化层/nr 和非/nr 线性转换/i 层/q 后/f 
CNN 可以 得到 文本 特征向量 用于 分类 学习 CNN 的 
优势 在于 在 计算 文本 特征向量 过程 中 有效 保留 
有用 的 词序 信息 针对 CNN 文本 分类 模型 还有 
许多 改进 工作 如 基于 字符 级 CNN 的 文本 
分类 模型 将 词 位置 信息 加入到 词 向量 循环 
神经网络 RNN 循环 神经 网络 将 文本 作为 字符 或 
词语 序列 { x0     xN } 对于 第 
  t 时刻 输入 的 字符 或 词语   xt 
都会 对应 产生 新的 低维 特征向量   st 如图 3 
所示 st   的 取值 会 受到   xt   
和 上个 时刻 特征向量   st 1   的 共同 
影响 st   包含 了 文本 序列 从 /nr x0   
到   xt   的 语义 信息 因此 我们 可以 
利用   sN   作为 该 文本 序列 的 特征向量 
进行 文本 分类 学习 与 CNN 相比 RNN 能够 更 
自然 地 考虑 文本 的 词序 信息 是 近年来 进行 
文本 表示 最 流行 的 方案 之一 为了 提升 RNN 
对 文本 序列 的 语义 表示 能力 研究 者 提出 
很多 扩展 模型 例如 长 短时记忆 网络 LSTM 提出 记忆 
单元 结构 能够 更好 地 处理 文本 序列 中的 长程 
依赖 克服 循环 神经网络 梯度 消失 问题 如图 4 是 
LSTM 单元 示意图 其中 引 入了 三个 门 input gate 
output gate forget gate 来 控制 是否 输入输出 以及 记忆 
单元 更新 提升 RNN 对 文本 序列 的 语义 表示 
能力 的 另外 一种 重要 方案 是 引入 选择 注意力 
机制 Selective Attention 可以 让 模型 根据 具体 任务 需求 
对 文本 序列 中 的 词语 给予 不同 的 关注度 
3 . 应用文 本分 类 技术 在 智能 信息 处理 
服务 中 有着 广泛 的 应用 例如 大部分 在线 新闻 
门户 网站 如 新浪 搜狐 腾讯 等 每天 都会 产生 
大量 新闻 文章 如果 对 这些 新闻 进行 人工整理 非常 
耗时耗力 而 自动 对 这些 新闻 进行 分类 将为 新闻 
归类 以及 后续 的 个性化 推荐 等 都 提供 巨大 
帮助 互联网 还有 大量 网页 论文 专利 和 电子 图书 
等 文本 数据 对 其中 文本 内容 进行 分类 是 
实现 对 这些 内容 快速 浏览 与 检索 的 重要 
基础 此外 许多 自然语言 分析 任务 如 观点 挖掘 垃圾邮件 
检测 等 也 都 可以 看 作文本 分类 或 聚 
类 技术 的 具体 应用 对 文档 进行 分类 一般 
需要 经过 两个 步骤 1 文本 表示 以及 2 学习 
文本 表示 是 指 将 无 结构化 的 文本 内容 
转化成 结构化 的 特征向量 形式 作为 分类 模型 的 输入 
在 得到 文本 对应 的 特征向量 后 就 可以 采用 
各种 分类 或 聚 类 模型 根据 特征向量 训练 分类器 
五 . 信息检索 信息检索 Information Retrieval IR 是 指 将 
信息 按 一定 的 方式 加以 组织 并 通过 信息 
查找 满足 用户 的 信息 需求 的 过程 和 技术 
1951 年 Calvin Mooers 首次 提出 了 信息检索 的 概念 
并给 出了 信息检索 的 主要 任务 协助 信息 的 潜在 
用户 将 信息 需求 转换 为 一张 文献 来源 列表 
而 这些 文献 包含 有 对其 有用 的 信息 信息检索 
学科 真正 取得 长足 发展 是 在 计算机 诞生 并 
得到 广泛 应用 之后 文献 数字化 使得 信息 的 大规模 
共享 及 保存 成为 现实 而 检索 就 成为 了 
信息 管理 与 应用 中 必不可少 的 环节 互联网/n 的/uj 
出现/v 和/c 计算机/n 硬件/n 水平/n 的/uj 提高/v 使得/v 人们/n 存储/l 
和/c 处理/v 信息/n 的/uj 能力/n 得到/v 巨大/a 的/uj 提高/v 从而 
加速 了 信息检索 研究 的 进步 并使/i 其/r 研究/vn 对象/n 
从/p 图书/n 资料/n 和/c 商用/n 数据/n 扩展/v 到/v 人们/n 生活/vn 
的/uj 方方面面/n 伴随 着 互联网 及 网络 信息 环境 的 
迅速 发展 以 网络 信息 资源 为 主要 组织 对象 
的 信息 检索系统 搜索引擎 应运而生 成为 了 信息化 社会 重要 
的 基础 设施 2016 年初 中文 搜索 引擎 用户数 达到 
5.66 亿人 这 充分 说明 搜索引擎 在 应用 层次 取得 
的 巨大 成功 也 使得 信息检索 尤其 是 网络 搜索 
技术 的 研究 具有 了 重要 的 政治 经济 和 
社会 价值 1 . 内容 结构 检索 用户 信息 资源 
和 检索系统 三个 主要 环节 组成 了 信息检索 应用 环境 
下 知识 获取 与 信息 传递 的 完整 结构 而 
当前 影响 信息 获取 效率 的 因素 也 主要 体现 
在 这几个 环节 即 •   检索 用户 的 意图 
表达 •   信息 资源 尤其 是 网络 信息 资源 
的 质量 度量 •   需求 与 资源 的 合理 
匹配 具体 而言 用户 有限 的 认知 能力 导致 其 
知识 结构 相对 大 数据 时代 的 信息 环境 而言 
往往 存在 缺陷 进而 影响 信息 需求 的 合理 组织 
和 清晰 表述 数据 资源 的 规模 繁杂 而 缺乏 
管理 在 互联网 注意力 经济 盛行 的 环境 下 不可避免 
地 存在 欺诈 作弊 行为 导致 检索系统 难以 准确 感知 
其 质量 用户 与 资源 提供者 的 知识 结构 与 
背景 不同 对于 相同 或者 相似 事物 的 描述 往往 
存在 较大 差异 使得 检索系统 传统 的 内容 匹配 技术 
难以 很好 应对 无法 准确 度量 资源 与 需求 的 
匹配 程度 上述 技术 挑战 互相 交织 本质 上 反映 
了 用户 个体 有限 的 认知 能力 与 包含 近乎 
无限 信息 的 数据 资源 空间 之间 的 不匹配 问题 
概括地 讲 当前 信息检索 的 研究 包括 如下 四 个 
方面 的 研究 内容 及 相应 的 关键 科学 问题 
1.1 信息 需求 理解 面对 复杂 的 泛 在 网络 
空间 用户 有 可能 无法 准确 表达 搜索 意图 即使 
能够 准确 表达 搜索引擎 也 可能 难以 正确理解 即使 能够 
正确 理解 也 难以 与 恰当 的 网络 资源 进行 
匹配 这 使得 信息 需求 理解 成为 了 影响 检索 
性能 提高 的 制约 因素 也 构成 了 检索 技术 
发展 面临 的 第一 个 关键 问题 1.2 资源 质量 
度量 资源 质量 管理 与 度量 在 传统 信息检索 研究 
中 并非 处于 首要 的 位置 但 随着 互联网 信息 
资源 逐渐 成为 检索 系统 的 主要 查 找对象 网络资源 
特有 的 缺乏 编审 过程 内容 重复 度 高 质量 
参差不齐 等 问题 成为 了 影响 检索 质量 的 重要 
因素 目前 搜索引擎 仍旧 面临 着 如何 进行 有效 的 
资源 质量 度量 的 挑战 这 构成 了 当前 信息检索 
技术 发展 面临 的 第二 个 关键 问题 1.3 结果 
匹配 排序 近年来 随着 网络 技术 的 进步 信息 检索系统 
尤其 是 搜索引擎 涉及 的 数据 对象 相应 的 变得 
多样化 异质化 这也 造成 了 传统 的 以 文本 内容 
匹配 为 主要 手段 的 结果 排序 方法 面临 着 
巨大 的 挑战 高度 动态 繁杂 的 泛 在 网络 
内容 使得 文本 相似 度 计算方法 无法 适用 整合 复杂 
异构网络 资源 作为 结果 使得 基于 同质性 假设 构建 的 
用户 行为 模型 难以 应对 多 模态 的 交互 方式 
则 使得 传统 的 基于 单一 维度 的 结果 分布 
规律 的 用户 行为 假设 大量 失效 因此 在 大 
数据 时代 信息 进一步 多样化 异质化 的 背景 下 迫切 
需要 构建 适应 现代 信息 资源 环境 的 检索 结果 
匹配 排序 方法 这是 当前 信息检索 技术 发展 面临 的 
第三 个 关键 问题 1.4 信息检索 评价 信息检索 评价 是 
信息 检索 和 信息 获取 领域 研究 的 核心 问题 
之一 信息检索 和 信息 获取 系统 核心 的 目标 是 
帮助 用户 获取 到 满足 他们 需求 的 信息 而/c 
评价/n 系统/n 的/uj 作用/v 是/v 帮助/v 和/c 监督/vn 研究/vn 开发/v 
人员/n 向/p 这一/i 核心/n 目标/n 前进/v 以 逐步 开发 出 
更好 的 系统 进而 缩小 系统 反馈 和 用户 需求 
之间 的 差距 提高 用户 满意度 因此 如何 设计 合理 
的 评价 框架 评价 手段 评价 指标 是 当前 信息检索 
技术 发展 面临 的 第四 个 关键 问题 2 . 
个性化 搜索 现有 的 主要 个性化 搜索算法 可分为 基于 内容 
分析 的 算法 基于 链接 分析 的 方法 和 基于 
协作 过滤 的 算法 •   基于 内容 的 个性化 
搜索算法 通过 比较 用户 兴趣 爱好 和 结果 文档 的 
内容 相似性 来 对 文档 的 用户 相关性 进行 判断 
进而 对 搜索 结果 进行 重排 用户 模型 一般 表述 
为 关键词 或 主题 向量 或 层次 的 形式 个性化 
算法 通过 比较 用户 模型 和 文档 的 相似性 判断 
真实 的 搜索 意图 并 估计 文档 对 用户 需求 
的 匹配 程度 •   基于 链接 分析 的 方法 
主要 是 利用 互联网 上 网页 之间 的 链接 关系 
并 假设 用户 点击 和 访问 过 的 网页 为 
用户 感兴趣 的 网页 通过 链接 分析 算法 进行 迭代 
最终 计算出 用户 对 每个 网页 的 喜好 度 • 
  基于 协作 过滤 的 个性化 搜索算法 主要 借鉴 了 
基于 协作 过滤 的 推荐 系统 的 思想 这种方法 考虑 
到 能够 收集到 的 用户 的 个人 信息 有限 因此 
它 不仅仅 利用 用户 个人 的 信息 还 利用 与 
用户 相似 的 其它 用户 或 群组 的 信息 并 
基于 用户 群组 和 相似 用户 的 兴趣 偏 好来 
个性化 当前 用户 的 搜索 结果 用户 之间 的 相似性 
可以 通过 用户 的 兴趣 爱好 历史 查询 点击 过 
的 网页 等 内容 计算 得出 3 . 语义搜索 技术 
随着 互联网 信息 的 爆炸 式 增长 传统 的 以 
关键字 匹配 为基础 的 搜索引擎 已 越来越 难以 满足 用户 
快速 查找 信息 的 需求 同时 由于 没 有知识 引导 
及 对 网页 内容 的 深入 整理 传统 网页 搜索 
返回 的 网页 结果 也 不能 精准 给出 所需 信息 
针对 这些 问题 以 知识图谱 为 代表 的 语义搜索 Semantic 
Search 将 语义 Web 技术 和 传统 的 搜索 引擎 
技术 结合 是 一个 很 有 研究 价值 但还 处于 
初期 阶段 的 课题 在 未来 的 一段 时间 结合 
互联网 应用 需求 的 实际 和 技术 产品 运营 能力 
的 实际 发展 水平 语义搜索 技术 的 发展 重点 将 
有可能 集中 在 以 各种 情境 的 垂直搜索 资源 为 
基础 知识化 推理 为 检索 运行方式 自然语言 多媒体 交互 为 
手段 的 智能化 搜索 与 推荐 技术 首先 将 包括 
各类 垂直搜索 资源 在内 的 深度 万维网 数据源 整合 成为/nr 
提供 搜索 服务 的 资源池 随后 利用 广泛 分布 在 
公众 终端 计算 设备 上 的 浏览器 作为 客户端 载体 
通过 构建 的 复杂 情境 知识库 来 开发 多层次 查询 
技术 并 以此 管理 调度 整合搜索 云端 的 搜索 服务 
资源 满足 用户 的 多样化 多 模态 查询 需求 最后 
基于 面向 情境 体验 的 用户 行为 模型 构建 以多 
模态 信息 推荐 的 形式 实现 对 用户 信息 需求 
的 主动 满足 六 . 信息 抽取 信息 抽取 Information 
Extraction 是 指 从非/nr 结构化 / 半 结构化 文本 如 
网页 新闻 论文 文献 微博 等 中 提取 指定 类型 
的 信息 如 实体 属性 关系 事件 商品 记录 等 
并 通过 信息 归并 冗余 消除 和 冲突消解 等 手段 
将 非 结构化 文本 转换 为 结构化 信息 的 一项 
综合 技术 例如 •   从 相关 新闻 报道 中 
抽取 出 恐怖 事件 信息 时间 地点 袭击者 受害人 袭击 
目标 后果 等 •   从 体育 新闻 中 抽取 
体育赛事 信息 主队 客队 赛场 比 分等 •/i  /i 从/p 
论文/nz 和/c 医疗/n 文献/n 中/f 抽取/v 疾病/n 信息/n 病因 病原 
症状 药物 等 被 抽取 出来 的 信息 通常 以 
结构化 的 形式 描述 可以为 计算机 直接 处理 从而 实现 
对 海量 非 结构化 数据 的 分析 组织 管理 计算 
查询 和 推理 并 进一步 为 更高 层面 的 应用 
和 任务 如 自然语言 理解 知识库 构建 智能 问答 系统 
舆情 分析 系统 提供 支撑 目前 信息 抽取 已被 广泛 
应用于 舆情 监控 网络 搜索 智能 问答 等 多个 重要 
领域 与此同时 信息 抽取 技术 是 中文 信息 处理 和 
人工智能 的 核心 技术 具有 重要 的 科学 意义 一直以来 
人工智能 的 关键 核心 部件 之一 是 构建 可 支撑 
类人 推理 和 自然 语言 理解 的 大规模 常识 知识库 
然而 由于 人类 知识 的 复杂性 开放性 多样性 和 巨大 
的 规模 目前 仍然 无法 构建 满足 上述 需求 的 
大规模 知识库 信息 抽取 技术 通过 结构化 自然语言 表述 的 
语义 知识 并 整合 来自 海量 文本 中 的 不同 
语义 知识 是 构建 大 规模 知识库 最 有效 的 
技术 之一 每 一段 文本 内 所 包含 的 寓意 
可以 描述 为 其中 的 一组 实体 以及 这些 实体 
相互之间 的 关联 和 交互 因此 抽取 文本 中 的 
实体 和 它们 之间 的 语义 关系 也 就 成为 
了 理解 文本 意义 的 基础 信息 抽取 可以 通过 
抽取 实体 和 实体 之间 的 语义 关系 表示 这些 
语义 关系 承载 的 信息 并 基于 这些 信息 进行 
计算 和 推理 来 有效 的 理解 一段 文本 所 
承载 的 语义 1 . 命名 实体 识别 命名 实体 
识别 的 目的 是 识别 文本 中 指定 类别 的 
实体 主要 包括 人名 地名 机构 名 专有名词 等 的 
任务 命名 实体 识别系统 通常 包含 两个 部分 实体 边界 
识别 和 实体 分类 其中 实体 边界 识别 判断 一个 
字符串 是否 是 一个 实体 而 实体 分类 将 识别 
出 的 实体 划分 到 预先 给定 的 不同 类别 
中去 命名 实体 识别 是 一项 极具 实用 价值 的 
技术 目前 中 英文 上 通用 命名 实体 识别 人名 
地名 机构 名 的 F1 值 都 能达到 90% 以上 
命名 实体 识别 的 主要 难点 在于 表达 不 规律 
且 缺乏 训练 语料 的 开放 域 命名 实体 类别 
如 电影 歌曲名 等 2 . 关系 抽取 关系 抽取 
指 的 是 检测 和 识别 文本 中 实体 之间 
的 语义 关系 并将 表示 同一 语义 关系 的 提及 
mention 链接 起来 的 任务 关系 抽取 的 输出 通常 
是 一个 三元组 实体 1 关系 类别 实体 2 表示 
实体 1 和 实体 2 之间 存在 特定 类别 的 
语义 关系 例如 句子 北京 是 中国 的 首都 政治 
中心 和 文化 中心 中 表述 的 关系 可以 表示 
为 中国 首都 北京 中国 政治 中心 北京 和 中国 
文化中心 北京 语义 关系 类别 可以 预先 给定 如 ACE 
评测 中 的 七大 类 关系 也 可以 按需 自动 
发现 开放 域 信息 抽取 关系 抽取 通常 包含 两个 
核心 模块 关系 检测 和 关系 分类 其 中 关系 
检测 判断 两个 实体 之间 是否 存在 语义 关系 而 
关系 分类 将 存在 语义 关系 的 实体 对 划分 
到 预先指定 的 类别 中 在 某些 场景 和 任务 
下 关系 抽取 系统 也 可能 包含 关系 发现 模块 
其 主要 目的 是 发现 实体 和 实体 之间 存在 
的 语义 关系 类别 例如 发现 人物 和 公司 之间 
存在 雇员 CEO CTO 创始人 董事长 等 关系 类别 3 
. 事件 抽取 事件 抽取 指 的 是从 非 结构化 
文本 中 抽取 事件 信息 并 将其 以 结构化 形式 
呈现 出来 的 任务 例如 从 毛泽东 1893 年/m 出生/v 
于/p 湖南/ns 湘潭/ns 这 句话 中 抽取 事件 { 类型 
出生 人物 毛泽东 时间 1893 年 出生地 湖南 湘潭 } 
事件 抽取 任务 通常 包含 事件 类型 识别 和 事件 
元素 填充 两 个子 任务 事件 类型 识别 判断 一句话 
是否 表达 了 特定 类型 的 事件 事件 类型 决定 
了 事件 表示 的 模板 不同 类型 的 事件 具有 
不同 的 模板 例如 出生 事件 的 模板 是 { 
人物 时间 出生地 } 而 恐怖 袭击 事件 的 模板 
是 { 地点 时间 袭击者 受害者 受伤 人数 } 事件 
元素 指 组成 事件 的 关键 元素 事件 元素 识别 
指 的 是 根据 所属 的 事件 模板 抽取 相应 
的 元素 并 为其 标上 正确 元素 标签 的 任务 
4 . 信息 集成 实体 关系 和 事件 分别 表示 
了 单篇 文本 中 不同 粒度 的 信息 在 很多 
应用 中 需要 将 来自 不同 数据源 不同 文本 的 
信息 综合 起来 进行 决策 这 就 需要 研究 信息 
集成 技术 目前 信息 抽取 研究 中 的 信息 集成 
技术 主要 包括 共 指 消解 技术 和 实体 链接 
技术 共 指 消解 指 的 是 检测 同一 实体 
/ 关系 / 事件 的 不同 提及 并 将其 链接 
在 一起 的 任务 例如 识别 乔布斯 是 苹果 的 
创始人 之一 他 经历 了 苹果 公司 几十年 的 起落 
与 兴衰 这句话 中的 乔布斯 和 他 指 的 是 
同一 实体 实体 链接 的 目的 是 确定 实体 名 
所 指向 的 真实世界 实体 例如 识别 上 一句话 中的 
苹果 和 乔布斯 分别 指向 真实世界 中 的 苹果 公司 
和其/nr CEO 史蒂夫 乔布斯 七 . 问答 系统 自动 问答 
Question Answering QA 是 指 利用 计算机 自动 回答 用户 
所 提出 的 问题 以 满足 用户 知识 需求 的 
任务 不同 于 现有 搜索引擎 问答 系统 是 信息 服务 
的 一种 高级 形式 系统 返回 用户 的 不再 是 
基于 关键词 匹配 排序 的 文档 列表 而是 精准 的 
自然 语言 答案 近年来 随着 人工智能 的 飞速 发展 自动 
问答 已经成为 倍受 关注 且 发展前景 广泛 的 研究 方向 
自动 问答 的 研究 历史 可以 溯源 到 人工智能 的 
原点 1950 年 人工智能 之父 阿兰 图灵 Alan M . 
Turing 在 Mind 上 发表 文章 Computing Machinery and Intelligence 
文章 开篇 提出 通过 让 机器 参与 一个 模仿 游戏 
Imitation Game 来 验证 机器 能否 思考 进而 提出 了 
经典 的 图灵测试 Turing Test 用以 检验 机器 是否 具备 
智能 同样 在 自然 语言 处理 研究领域 问答 系统 被 
认为 是 验证 机器 是否 具备 自然语言 理解 能力 的 
四个 任务 之一 其它 三个 是 机器 翻译 复述 和 
文本 摘要 自动 问答 研究 既 有利于 推动 人工智能 相关 
学科 的 发展 也 具有 非常 重要 的 学术 意义 
从 应用 上 讲 现有/b 基于/p 关键词/n 匹配/v 和/c 浅层/n 
语义分析/i 的/uj 信息/n 服务/vn 技术/n 已经/d 难以/d 满足/v 用户/n 日益/n 
增长/v 的/uj 精准/n 化/n 和/c 智能化/nr 信息/n 需求/v 已有 的 
信息 服务 范式 急需 一 场 变革 2011 年 华盛顿大学 
图灵 中心主任 Etzioni 在 Nature 上 发表 的 Search Needs 
a Shake Up 中 明确 指出 在 万维网 诞生 20 
周年 之际 互联网 搜索 正处于 从 简单 关键词 搜索 走向 
深度 问答 的 深刻 变革 的 风口浪尖 上 以 直接 
而 准确 的 方式 回答 用户 自然语言 提问 的 自动 
问答 系统 将 构成 下一代 搜索引擎 的 基本 形态 同一 
年 以 深度 问答 技术 为 核心 的 IBM Watson 
自动 问答 机器人 在 美国 智力竞赛 节目 Jeopardy 中 战胜 
人类 选手 引起 了 业内 的 巨大 轰动 Watson 自动 
问答 系统 让 人们 看到 已有 信息 服务 模式 被 
颠覆 的 可能性 成为 了 问答 系统 发展 的 一个 
里程碑 此外 随着 移动 互联网 崛起 与 发展 以 苹果 
公司 Siri Google Now 微软 Cortana 等 为 代表 的 
移动 生活 助手 爆发式 涌现 上述/b 系统/n 都/d 把以/i 自然/d 
语言/n 为/p 基本/n 输入/v 方式/n 的/uj 问答/v 系统/n 看作/v 是/v 
下一代/t 信息/n 服务/vn 的/uj 新/a 形态/n 和/c 突破口/n 并 均 
加大 人员 资金 的 投入 试图 在 这一次 人工智能 浪潮 
中 取得 领先 1 . 关键问题 自动 问答 系统 在 
回答 用户 问题 时 需要 正确 理解 用户 所提 的 
自然 语言 问题 抽取 其中 的 关键 语义 信息 然后 
在 已有 语料库 知识库 或 问答 库 中 通过 检索 
匹配 推理 的 手段 获取 答案 并 返回 给 用户 
上述 过程 涉及 词 法分析 句法分析 语义分析 信息检索 逻辑推理 知识工程 
语言 生成 等 多项 关键技术 传统 自动 问答 多 集中 
在 限定 领域 针对 限定 类型 的 问题 进行 回答 
伴随 着 互联网 和大/nr 数据 的 飞速 发展 现有 研究 
趋向于 开放 域 面向 开放 类型 问题 的 自动 问答 
概括地 讲 自动 问答 的 主要 研究 任务 和 相应 
关键 科学 问题 如下 1.1 问句 理解 给定 用户 问题 
自动 问答 首先 需要 理解 用户 所 提 问题 用户 
问句 的 语义 理解 包含 词 法分析 句法分析 语义分析 等 
多项 关键技术 需要 从 文本 的 多个 维度 理解 其中 
包含 的 语义 内容 在 词语 层面 需要 在 开放 
域 环境 下 研究 命名 实体 识别 Named Entity Recognition 
术语 识别 Term Extraction 词汇 化 答案 类型 词 识别 
Lexical Answer Type Recognition 实体 消 歧 Entity Disambiguation 关键词 
权重 计算 Keyword Weight Estimation 答案 集 中词 识别 Focused 
Word Detection 等 关键 问题 在 句法 层面 需要 解析 
句子 中词 与 词 之间 短语 与 短语 之间 的 
句法关系 分析 句子 句法结构 在 语义 层面 需要 根据 词语 
层面 句法 层面 的 分析 结果 将 自然 语言 问句 
解析 成 可计算 结构化 的 逻辑 表达形式 如一 阶 谓词 
逻辑 表达式 1.2 文本 信息 抽取 给定 问句 语义分析 结果 
自动 问答 系统 需要 在 已有 语料库 知识库 或 问答 
库 中 匹配 相关 的 信息 并 抽取 出 相应 
的 答案 传统 答案 抽取 构 建在 浅层 语义分析 基础 
之上 采用 关键词 匹配 策略 往往 只能 处理 限定 类型 
的 答案 系统/n 的/uj 准确率/n 和/c 效率/n 都/d 难以/d 满足/v 
实际/n 应用/v 需求/v 为 保证 信息 匹配 以及 答案 抽取 
的 准确度 需要 分析 语义 单元 之间 的 语义 关系 
抽取 文本 中的 结构化 知识 早期 基于 规则 模板 的 
知识 抽取 方法 难以 突破 领域 和 问题 类型 的 
限制 远远 不能 满足 开放 领域 自动 问答 的 知识 
需求 为了 适应 互联网 实际 应用 的 需求 越来越 多 
的 研究者 和 开发 者 开始 关注 开放 域 知识 
抽取 技术 其 特点 在于 •   文本 领域 开放 
处理 的 文本 是 不 限定 领域 的 网络 文本 
•   内容 单元 类型 开放 不 限定 所 抽取 
的 内容 单元 类型 而是 自动 地 从 网络 中 
挖掘 内容 单元 的 类型 例如 实体 类型 事件 类型 
和 关系 类型 等 1.3 知识 推 理自 动问 答中 
由于 语料库 知识库 和 问答 库 本身 的 覆盖度 有限 
并 不是 所有 问题 都能 直 接 找到 答案 这 
就 需要 在 已有 的 知识 体系 中 通过 知识 
推理 的 手段 获取 这些 隐含 的 答案 例如 知识库 
中 可能 包括 了 一个人 的 出生地 信息 但是 没 
包括 这 个人 的 国籍 信息 因此 无法 直接 回答 
诸如 某某人 是 哪 国人 这样 的 问题 但是 一般 
情况 下 一个人 的 出生地 所属 的 国家 就是 他 
她 的 国籍 在 自动 问 答中 就 需要 通过 
推理 的 方式 学习 到 这样 的 模式 传统 推理方法 
采用 基于 符号 的 知识 表示 形式 通过 人工 构建 
的 推理 规则 得到 答案 但是 面对 大 规模 开放 
域 的 问答 场景 如何 自动 进行 规则学习 如何 解决 
规则 冲突 仍然 是 亟待解决 的 难点 问题 目前 基于 
分布式 表示 的 知识 表示 学习 方法 能够 将 实体 
概念 以及 它们 之间 的 语义 关系 表示 为 低维 
空间 中 的 对象 向量 矩阵 等 并 通过 低 
维空间 中的 数值 计算 完成 知识 推理 任务 虽然 这类 
推理 的 效果 离 实用 还有 距离 但是 我们 认为 
这 是 值得 探寻 的 方法 特别 是 如何 将 
已有 的 基于 符号 表示 的 逻辑推理 与 基于 分布式 
表示 的 数值 推理 相结合 研究 融合 符号逻辑 和 表示 
学习 的 知识 推理 技术 是 知识 推理 任务 中 
的 关键 科学 问题 2 . 技术 方法 根据 目标 
数据源 的 不同 已有 自动 问答 技术 大致 可以 分为 
三类 •   检索 式 问答 •   社区 问答 
•   知识库 问答 以下 分别 就 这 几个 方面 
对 研究 现状 进行 简要 阐述 2.1 检索 式 问答 
检索 式 问答 研究 伴随 搜索引擎 的 发展 不断 推进 
1999 年 随着 TREC QA 任务 的 发起 检索 式 
问答 系统 迎来 了 真正 的 研究 进展 TREC QA 
的 任务 是 给定 特定 WEB 数据集 从中 找到 能够 
回答 问题 的 答案 这类 方法 是以 检索 和 答案 
抽取 为 基本 过程 的 问答 系统 具体 过程 包括 
问题 分析 篇章 检索 和 答案 抽取 根据 抽取 方法 
的 不同 已有 检索 式 问答 可以 分为 基于 模式匹配 
的 问答 方法 和 基于 统计 文本 信息 抽取 的 
问答 方法 •   基于 模式匹配 的 方法 往往 先 
离线 地 获得 各类 提问 答案 的 模式 在 运行 
阶段 系统 首先 判断 当前 提问 属于 哪 一类 然后 
使用 这类 提问 的 模式 来 对 抽取 的 候选 
答案 进行 验证 同时 为了 提高 问答 系统 的 性能 
人们 也 引入 自然语言 处理 技术 由于 自然 语言 处理 
的 技术 还 未成熟 现有 大多数 系统 都 基于 浅层 
句子 分析 •   基于 统计 文本 信息 抽取 的 
问答 系统 的 典型 代表 是 美国 Language Computer Corporation 
公司 的 LCC 系统 该 系统 使用 词汇 链 和 
逻辑 形式 转换 技术 把/p 提/v 问句/n 和/c 答案/n 句/q 
转化/v 成/n 统一/vn 的/uj 逻辑/n 形式/n Logic Form 通过 词汇 
链 实现 答案 的 推理 验证 LCC 系统 在 TREC 
QA Track 2001 ~ 2004 连续 三年 的 评测 中 
以 较大 领先 优势 获得 第一名 的 成绩 2011 年 
IBM 研发 的 问答 机器人 Watson 在 美国 智力竞赛 节目 
危险 边缘 Jeopardy 中 战胜 人类 选手 成为 问答 系统 
发展 的 一个 里程碑 Watson 的 技术 优势 大致 可以 
分为 以下 三 个 方面 •   强大 的 硬件平台 
包括 90 台 IBM 服务器 分布式 计算环境 •   强大 
的 知识 资源 存储 了 大约 2 亿页 的 图书 
新闻 电影剧本 辞海 文选 和 世界 图书 百科全书 等 资料 
•   深层 问答 技术 DeepQA 涉及 统计 机器学习 句法分析 
主题 分析 信息 抽取 知识库 集成 和 知识 推理 等 
深层 技术 然而 Watson 并 没有 突破 传统 问答式 检索 
系统 的 局限性 使用 的 技术 主要 还是 检索 和 
匹配 回答 的 问题 类型 大多 是 简单 的 实体 
或 词语 类 问题 而 推理 能力 不强 2.2 社区 
问答 随着 Web2 . 0 的 兴起 基于 用户 生成 
内容 User Generated Content UGC 的 互联网 服务 越来越 流行 
社区 问答 系统 应运而生 例如 Yahoo Answers 百度知道 等 问答 
社区 的 出现 为 问答 技术 的 发展 带来 了 
新的 机遇 据统计 2010 年 Yahoo Answers 上 已 解决 
的 问题 量 达到 10 亿 2011 年 百度知道 已 
解决 的 问题 量 达到 3 亿 这些 社区 问答 
数据 覆盖 了 方方面面 的 用户 知识 和 信息 需求 
此外 社区 问答 与 传统 自动 问答 的 另一个 显著 
区别 是 社区 问答 系统 有 大量 的 用户 参与 
存在 丰富 的 用户 行为 信息 例如 用户 投票 信息 
用户 评价 信息 回答者 的 问题 采纳率 用户 推荐 次数 
页面 点击 次数 以及 用户 问题 答案 之间 的 相互 
关联 信息 等等 这些 用户 行为 信息 对于 社区 中 
问题 和 答案 的 文本 内容 分析 具有 重要 的 
价值 一般来讲 社区 问答 的 核心 问题 是从 大规模 历史 
问答 对 数据 中 找出 与 用户 提 问问题 语义 
相似 的 历史 问题 并 将其 答案 返回 提问 用户 
假设 用户 查询 问题 为 q0 用于 检索 的 问答 
对 数据 为 SQ A = { q1 a1 q2 
a2 } qn an } } 相似 问 答对 检索 
的 目标 是从 SQ A 中 检索 出 能够 解答 
问题 q0 的 问答 对 qi ai 针对 这 一 
问题 传统 的 信息 检索 模型 如 向量空间 模型 语言 
模型 等 都 可以 得到 应用 但是 相对于 传统 的 
文档 检索 社区 问答 的 特点 在于 用户/n 问题/n 和/c 
已有/v 问句/n 相对/d 来说/u 都/d 非常/d 短/a 用户 问题 和 
已有 问句 之间 存在 词汇 鸿沟 问题 基于 关键词 匹配 
的 检索 模型 很难 达到 较好 的 问答 准确度 目前 
很多 研究 工作 在 已有 检索 框架 中 针对 这 
一 问题 引入 单 语言 翻译 概率模型 通过 IBM 翻译 
模型 从/p 海量/n 单语/nr 问答/v 语料/n 中/f 获得/v 同/p 种/m 
语言/n 中/f 两个/m 不同/a 词语/n 之间/f 的/uj 语义/n 转换/v 概率/n 
从而 在 一定 程度 上 解决 词汇 语义 鸿沟 问题 
例如 和 减肥 对应 的 概率 高的/nr 相关 词 有 
瘦身 跑步 饮食 健康 远 动 等等 除此之外 也/d 有/v 
许多/m 关于/p 问句/n 检索/vn 中词/n 重要性/n 的/uj 研究/vn 和/c 基于/p 
句法结构/n 的/uj 问题/n 匹配/v 研究/vn 2.3 知识库 问答 检索 式 
问答 和 社区 问答 尽管 在 某些 特定 领域 或者 
商业 领域 有所 应用 但是 其 核心 还是 关键词 匹配 
和 浅层 语义分析 技术 难以实现 知识 的 深层 逻辑推理 无法 
达到 人工智能 的 高级 目标 因此 近些年来 无论是 学术界 或 
工业界 研究者 们 逐步 把 注意力 投向 知识图谱 或 知识库 
Knowledge Graph 其 目标 是 把 互联网 文本 内容 组织 
成为 以 实体 为 基本 语义 单元 节点 的 图 
结构 其中 图上 的 边 表示 实体 之间 语义 关系 
目前 互联网 中 已有 的 大规模 知识库 包括 DBpedia Freebase 
YAGO 等 这些 知识库 多 是以 实体 关系 实体 三元组 
为 基本 单元 所 组成 的 图 结构 基于 这样 
的 结构化 知识 问答 系统 的 任务 就是 要 根据 
用户 问题 的 语义 直接 在 知识库 上 查找 推理 
出 相 匹配 的 答案 这一 任务 称为 面向 知识库 
的 问答 系统 或 知识库 问答 要 完成 在 结构化 
数据 上 的 查询 匹配 推理 等 操作 最 有效 
的 方式 是 利用 结构化 的 查询 语句 例如 SQL 
SPARQL 等 然而 这些 语句 通常 是 由 专家 编写 
普通 用户 很难 掌握 并 正确 运用 对 普通 用户 
来说 自然语言 仍然 是 最 自然 的 交互 方式 因此 
如何 把 用户 的 自然 语言 问句 转化 为 结构化 
的 查询 语句 是 知识库 问答 的 核心 所在 其 
关键 是 对于 自然 语言 问句 进行 语义 理解 目前 
主流 方法 是 通过 语义分析 将 用户 的 自然 语言 
问句 转化成 结构化 的 语义 表示 如 范式 和 DCS 
Tree 相 对应 的 语义 解析 语法 或 方法 包括 
组合 范畴 语法 Category Compositional Grammar CCG 以 及 依 
存 组 合 语 法 Dependency based Compositional Semantics DCS 
等 八 . 机器翻译 1 . 理论 应用 机器翻译 machine 
translation MT 是 指 利用 计算机 实现 从 一种 自然 
语言 到 另外 一种 自然 语言 的 自动 翻译 被 
翻译 的 语言 称为 源语言 source language 翻译 到 的 
语言 称作 目标语言 target language 简单 地 讲 机器翻译 研究 
的 目标 就是 建立 有效 的 自动 翻译 方法 模型 
和 系统 打破 语言 壁垒 最终 实现 任意 时间 任意 
地点 和 任意 语言 的 自动 翻译 完成 人们 无障碍 
自由 交流 的 梦想 人们 通常 习惯于 感知 听 看 
和读/nr 自己 母语 的 声音 和 文字 很多 人 甚至 
只能 感知 自己 的 母语 因此 机器翻译 在 现实 生活 
和 工作 中 具有 重要 的 社会 需求 从 理论上 
讲 机器翻译 涉及 语言学 计算 语言学 人工智能 机器学习 甚至 认知 
语言学 等 多个 学科 是 一个 典型 的 多 学科 
交叉 研究 课题 因此 开展 这项 研究 具有 非常 重要 
的 理论 意义 既 有利于 推动 相关 学科 的 发展 
揭示 人脑 实现 跨语言 理解 的 奥秘 又 有助于 促进 
其他 自然语言 处理 任务 包括 中文 信息 处理 技术 的 
快速 发展 从 应用 上 讲 无论 是 社会 大众 
政府 企业 还是 国家 机构 都 迫切 需要 机器 翻译 
技术 特别是在 互联网 + 时代 以 多语言 多 领域 呈现 
的 大 数据 已 成为 我们 面临 的 常态 问题 
机器翻译 成为 众多 应用领域 革新 的 关键 技术 之一 例如 
在 商贸 体育 文化 旅游 和 教育 等 各个 领域 
人们 接触 到 越来越 多 的 外文 资料 越来越 频繁 
地 与 持 各种 语言 的 人 通信 和 交流 
从而 对 机器 翻译 的 需求 越来越 强烈 在 国家 
信息 安全 和 军事 情报 领域 机器 翻译 技术 也 
扮演 着 非常 重要 的 角色 可以 说 离开 机器翻译 
基于 大 数据 的 多 语言 信息 获取 挖掘 分析/vn 
和/c 决策/n 等/u 其他/r 应用/v 都将/nr 成为/v 空中楼阁/nr 尤其 值得 
提出 的 是 在 未来 很长 一段 时间 里 建立 
于 丝绸之路 这一 历史 资源 之上 的 一带 一路 将 
是 我国 与 周边 国家 发展 政治 经济 进行 文化 
交流 的 主要 战略 据统计 一带 一路 涉及 60 多个 
国家 44 亿 人口 53 种 语言 可见 机器 翻译 
是 一带 一路 战略 实施 中 不可或缺 的 重要 技术 
2 . 技术 现状 基于 规则 的 机器 翻译 方法 
需要 人工 设计 和 编纂 翻译 规则 统计 机器翻译 方法 
能够 自动 获取 翻译 规则 但 需要 人工 定义 规则 
的 形式 而 端 到 端的 神经网络 机器翻译 方法 可以 
直接 通过 编码 网络 和 解码 网络 自动 学习 语言 
之间 的 转换 算法 从 某种 角度 讲 其 自动化 
程度 和 智能化 程度 在 不断 提升 机器翻译 质量 也 
得到 了 显著 改善 机器 翻译 技术 的 研究 现状 
可从 欧盟 组织 的 国际 机器翻译 评测 WMT 的 结果 
中 窥 得 一斑 该 评测 主要 针对 欧洲语言 之间 
的 互译 2006 年至/nr 2016 年 每年 举办 一次 对比 
法语 到 英语 历年 的 机器 翻译 评测 结果 可以 
发现 译文 质量 已经 在 自动 评价 指标 BLEU 值 
上 从 最初 小于 0.3 到目前 接近 0.4 大量 的 
人工 评测 对比 说明 BLEU 值 接近 0.4 的 译文 
能够 达到 人类 基本 可以 理解 的 程度 另外 中国 
中文信息 学会 组织 的 全国 机器翻译 评测 CWMT 每 两年 
组织 一次 除了 英汉 日汉 翻译 评测 以外 CWMT 还 
关注 我国 少数民族 语言 藏 蒙 维 和 汉语 之间 
的 翻译 相对而言 由于 数据 规模 和 语言 复杂性 的 
问题 少数 民族 与 汉语 之间 的 翻译 性能 要 
低于 汉英 汉 日 之间 的 翻译 性能 虽然 机器翻译 
系统 评测 的 分值 呈 逐年 增长 的 趋势 译文 
质量 越来越 好 但 与 专业 译员 的 翻译 结果 
相比 机器翻译 还有 很长 的 路 要走 可以 说 在 
奔向 信 达 雅 翻译 目标 的 征程 上 目前 
的 机器 翻译 基本 挣扎 在 信 的 阶段 很多/m 
理论/n 和/c 技术/n 问题/n 仍/zg 有待/v 于更/nr 深入/v 的/uj 研究/vn 
和/c 探索/v 九 . 自动 摘要 自动 文摘 又称 自动 
文档 摘要 是 指 通过 自动 分析 给定 的 一篇 
文档 或 多篇 文档 提炼 总结 其中 的 要点 信息 
最终 输出 一篇 长度 较短 可读性 良好 的 摘要 通常 
包含 几句话 或 数百 字 该 摘要 中 的 句子 
可直接 出自 原文 也可 重新 撰写 所得 简言之 文摘 的 
目的 是 通过 对 原 文本 进行 压缩 提炼 为 
用户 提供 简明扼要 的 文字描述 用户 可以 通过 阅读 简短 
的 摘要 而 知晓 原文中 所 表达 的 主要 内容 
从而 大幅 节省 阅读时间 自动 文摘 研究 的 目标 是 
建立 有效 的 自动 文摘 方法 与 模型 实现 高 
性能 的 自动 文摘 系统 近 二十 年来 业界 提出 
了 各类 自动 文摘 方法 与 模型 用于 解决 各类 
自动 摘要 问题 在 部分 自动 摘要 问题 的 研究 
上 取得 了 明显 的 进展 并 成功 将 自动 
文摘 技术 应用于 搜索引擎 新闻 阅读 等 产品 与 服务 
中 例如 谷歌 百度 等 搜索引擎 均 会为 每项 检索 
结果 提供 一个 短 摘要 方便 用 户 判断 检索 
结果 相关性 在 新闻 阅读 软件 中 为 新闻 事件 
提供 摘要 也 能够 方便 用户 快速 了解 该 事件 
2013 年 雅虎 耗资 3000 万 美元 收购 了 一项 
自动 新闻 摘要 应用 Summly 则 标志着 自动 文摘 技术 
的 应用 走向 成熟 自动 文摘 的 研究 在 图书馆 
领域 和 自然 语言 处理 领域 一直都 很 活跃 最早 
的 应用 需求 来自 于 图书馆 图书馆 需要 为 大量 
文献 书籍 生成 摘要 而 人工 摘要 的 效率 很低 
因此 亟需 自动 摘要 方法 取代 人工 高效 地 完成 
文献 摘要 任务 随着 信息检索 技术 的 发展 自动 文摘 
在 信息 检索系统 中的 重要性 越来越 大 逐渐 成为 研究 
热点 之一 经过 数十年 的 发展 同时 在 DUC 与 
TAC 等 自动 文摘 国际 评测 的 推动 下 文本 
摘要 技术 已经 取得 长足 的 进步 国际上 自动 文摘 
方面 比较 著名 的 几个 系统 包括 ISI 的 NeATS 
系统 哥伦比亚 大学 的 NewsBlaster 系统 密歇根 大学 的 NewsInEssence 
系统 等 1 . 方法 自动 文摘 所 采用 的 
方法 从 实现 上 考虑 可以 分为 抽取式 摘要 extractive 
summarization 和 生成式 摘要 abstractive summarization 抽取式 方法 相对 比较 
简单 通常 利用 不同 方法 对 文档 结构单元 句子 段落 
等 进行 评价 对 每个 结构单元 赋予 一定 权重 然后 
选择 最 重要 的 结构单元 组成 摘要 而 生成式 方法 
通常 需要 利用 自然 语言 理解 技术 对 文本 进行 
语法 语义分析 对 信息 进行 融合 利用 自然 语言 生成 
技术 生成 新的 摘要 句子 目前 的 自动 文摘 方法 
主要 基于 句子 抽取 也 就是 以 原文中 的 句子 
作为 单位 进行 评估 与 选取 抽取式 方法 的 好处 
是 易于 实现 能 保证 摘要 中 的 每个 句子 
具有 良好 的 可读性 为 解决 如前所述 的 要点 筛选 
和 文摘 合成 这 两个 关键 科学 问题 目前 主流 
自动 文摘 研究工作 大致 遵循 如下 技术 框架 内容 表示 
→ 权重 计算 → 内容 选择 → 内容 组织 首先 
将 原始 文本 表示 为 便于 后续 处理 的 表达 
方式 然后 由 模型 对 不同 的 句法 或 语义 
单元 进行 重要性 计算 再 根据 重要性 权重 选取 一部分 
单元 经过 内容 上 的 组织 形成 最后 的 摘要 
1.1 内容 表示 与 权重 计算 原 文档 中 的 
每个 句子 由 多个 词汇 或 单元 构成 后续 处理 
过程 中 也 以 词汇 等 元素 为 基本 单位 
对 所在 句子 给出 综合 评价 分数 以 基于 句子 
选取 的 抽取式 方法 为例 句子 的 重要性 得分 由其 
组成部分 的 重要性 衡量 由于 词汇 在 文档 中 的 
出现 频 次 可以 在 一定 程度 上 反映 其 
重要性 我们 可以 使用 每个 句子 中 出现 某 词 
的 概率 作为 该词 的 得分 通过 将 所有 包含 
词 的 概率 求和 得到 句子 得分 也 有 一些 
工作 考虑 更多 细节 利用 扩展性 较强 的 贝叶斯 话题 
模型 对 词汇 本身 的 话题 相关性 概率 进行 建模 
一些 方法 将 每个 句子 表示 为 向量 维数 为总 
词表 大小 通常 使用 加权 频数 作为 句子 向量 相应 
维 上 的 取值 加权 频数 的 定义 可以 有 
多种 如 信息检索 中 常用 的 词频 逆 文档 频率 
TF IDF 权重 也有 研究工作 考虑 利用 隐 语义分析 或 
其他 矩阵 分解 技术 得到 低维 隐含 语义 表示 并 
加以 利用 得到 向量 表示 后 计算 两两 之间 的 
某种 相似 度 例如 余弦 相似 度 随后 根据 计算 
出 的 相似 度 构建 带 权图/nr 图中 每个 节点 
对应 每个 句子 在 多 文档 摘要 任务 中 重要 
的 句子 可能 和 更多 其他 句子 较为 相似 所以 
可以 用 相似 度 作为 节点 之间 的 边权/nr 通过 
迭代 求解 基于 图 的 排序 算法 来 得到 句子 
的 重要性 得分 也 有 很多 工作 尝试 捕捉 每个 
句子 中 所 描述 的 概念 例如 句子 中 所 
包含 的 命名 实体 或 动词 出于 简化 考虑 现有 
工作 中 更多 将 二元 词 bigram 作为 概念 近期 
则有 工作 提出 利用 频繁 图 挖掘 算法 从 文档 
集中 挖掘 得到 深层 依存 子结构 作为 语义 表示 单元 
另一方面 很多 摘要 任务 已经 具备 一定 数量 的 公开 
数据集 可 用于 训练 有 监督 打分 模型 例如 对于 
抽取式 摘要 我们 可以 将 人工 撰写 的 摘要 贪心 
匹配 原 文档 中 的 句子 或 概念 从而 得到 
不同 单元 是否 应当 被 选作 摘要 句 的 数据 
然后 对 各 单元 人工 抽取 若干 特征 利用 回归模型 
或 排序 学习 模型 进行 有 监督 学习 得到 句子 
或 概念 对应 的 得分 文档 内容 描述 具有 结构性 
因此 也 有 利用 隐 马尔科夫 模型 HMM 条件 随 
机场 CRF 结构化 支持 向量 机 Structural SVM 等 常见 
序列 标注 或 一般 结构 预测模型 进行 抽取式 摘要 有 
监督 训练 的 工作 所 提取 的 特征 包括 所在位置 
包含 词汇 与 邻 句 的 相似 度 等等 对 
特定 摘要 任务 一般 也 会 引入 与 具体 设定 
相关 的 特征 例如 查询 相关 摘要 任务 中 需要 
考虑 与 查询 的 匹配 或 相似 程度 1.2 内容 
选择 无 论从 效果 评价 还是 从 实用性 的 角度 
考虑 最终 生成 的 摘要 一般 在 长度 上会 有限制 
在 获取 到 句子 或 其他 单元 的 重要性 得分 
以后 需要 考虑 如何 在 尽可能 短 的 长度 里 
容纳 尽可能 多 的 重要 信息 在此 基础上 对 原文 
内容 进行 选取 内容 选择 方法 包括 贪心 选择 和 
全局 优化 2 . 技术 现状 相比 机器翻译 自动 问答 
知识图谱 情感 分析 等 热门 领域 自动 文摘 在 国内 
并 没有 受到 足够 的 重视 国内 早期 的 基础 
资源 与 评测 举办 过 中文 单 文档 摘要 的 
评测 任务 但 测试 集 规模 比较 小 而且 没有 
提供 自动化 评价 工具 2015 年 CCF 中文 信息 技术 
专委会 组织 了 NLPCC 评测 其中 包括 了 面向 中文 
微博 的 新闻 摘要 任务 提供 了 规模 相对 较大 
的 样例 数据 和 测试数据 并 采用 自动 评价 方法 
吸引 了 多支 队伍 参加 评测 目前 这些 数据 可以 
公开 获得 但 上述 中文 摘要 评测 任务 均 针对 
单 文档 摘要 任务 目前 还 没有 业界 认可 的 
中文 多 文档 摘要 数据 这在 事实上 阻碍 了 中文 
自动 摘要 技术 的 发展 近些年 市面 上 出现 了 
一些 文本 挖掘 产品 能够 提供 中文 文档 摘要 功能 
尤其 是 单 文档 摘要 例如 方 正智 思 拓 
尔 思 TRS 海量 科技 等 公司 的 产品 百度 
等 搜索引擎 也 能为 检索 到 的 文档 提供 简单 
的 单 文档 摘要 这些 文档 摘要 功能 均 被 
看作 是 系统 的 附属 功能 其 实现 方法 均 
比较 简单 自然语言 处理 的 库 非常多 下面 列举 一些 
对 Python 友好 简单易用 轻量 功能 又 全的库/nr 1 中文 
中文 自然语言 处理 工具 评测 https / / github . 
com / mylovelybaby / chinese nlp toolkit testawesome   https 
/ / github . com / crownpku / Awesome Chinese 
NLPHanlp 地址 https / / github . com / hankcs 
/ HanLPJieba 地址 https / / github . com / 
fxsjy / j i e b a s n o 
w n l p h t t p s / 
/ github . com / isnowfy / snownlp2 英文 NLTK 
地址 https / / www . nltk . org / 
Textblob 地址 https / / github . com / sloria 
/ TextBlob3 实例 3.1 中文 自然语言 处理 Pipeline 实例 实例 
https / / github . com / JiaLiangShen / Chinese 
Article Classification based on own corpus via TextCNN and GBDT3 
. 2 英文 Pipeline 实例 实例 https / / github 
. com / TiesdeKok / Python _ NLP _ Tutorial 
/ blob / master / NLP _ Notebook . ipynb 
友情 推荐 ABC 技术 研习 社 为 技术 人 打造 
的 专属 A AI B Big Data C Cloud 技术 
公众 号 和 技术 交流 社群 在上 一 部分 中 
我们 已经 了解 了 自然 语言 处理 的 基本 知识 
自然语言 处理 NLP 1 概述 在 这 一 部分 中 
我们 将 简要介绍 NLP 领域 的 基本 模型 语言 模型 
我们 还 将对 自然语言 处理 的 基础 语料库 的 概念 
进行 介绍 这些 都是在/nr 学习 自然语言 处理 之前 所 必备 
的 知识 此外 我们/r 默认/v 大家/n 有/v 一定/d 的/uj 信息/n 
论/zg 和/c 概率论/n 基础/n 在 这里 不对 信息论 和 概率论 
知识 进行 赘述 接下来 我们 进入 正题 一 语言 模型 
在 这 一 部分 中 我们 讨论 的 语言 模型 
主要 是 统计 语言 模型 除此之外 我们 在 今后 的 
文章 中 还会 对 神经 网络 语言 模型 进行 介绍 
所谓 语言 模型 就是 利用 数学 的 方法 描述语言 规律 
而 统计 语言 模型 就是 用 句子 出现 的 概率 
P S 来 刻画 句子 的 合理性 而不 进行 语言学 
分析处理 这是 统计 自然语言 处理 的 基础 模型 假设 句子 
= w1 w2 . . . wn 其中 wi 可以 
暂时 看作 句子 中的 第 i 个 词 在后面 会 
进行 具体 介绍 由于 自然 语言 是 上下文 相关 的 
信息 传递 方式 可以 很 自然地 讲 句子 出现 的 
概率 定义 如下 P S = P w1 P w2 
| w1 . . . P wn | w1 w2 
. . . wn 1 特别地 当 i = 1时 
P w1 | w0 = P w1 概率 定义 与 
条件概率 相同 在 统计 语言 模型 中 输入 是 句子 
输出 是 句子 的 概率 P S 模型 参数 是 
各个 P 即 P wi | w1 w2 . . 
. wi 1 wi 被 称为 统计 基元 可以 是 
字 词 短语 词类 等等 通常 以 词 代替 wi 
由 w1 w2 . . . wi 1 决定 由 
特定 一组 w1 w2 . . . wi 1 构成 
的 一个 序列 称为 wi 的 历史 说 到 这里 
相信 大家 已经 发现 了 一个 很 现实 却 很 
重要 的 问题 在 这个 模型 中 参数 的 数量 
也 太多 了 假设 我们 的 统计 基元 个数 为 
L 在 这里 可以 将 其 理解 为 词汇表 那么 
一句话 中的 第 i 个 基元 就有 L ^ i 
1 种 不同 的 历史 情况 我们 必须 考虑到 这 
所有 不同 的 历史 情况 下 产生 第 i 个 
基元 的 概率 于是 对于 长度 为 m 的 句子 
模型 中有 L ^ m 个 自由 参数 P wm 
| w1 wm 1 这样 算来 这会 是 一个 很 
可怕 的 参数 数量 假设 L = 5000 词汇表 中的 
词数 有 5000 不 过分 吧 m = 3 模型 
中 的 自由 参数 就 达到 了 1250亿 这还 仅仅 
是 对应 着 一个 三个 词 的 句子 而 汉语 
中 平均 每 句话 中有 22个 词 这将 是 一个 
天文数字 要 解决 这个 问题 就要 减少 历史 基元 的 
个数 也 就是 减少 决定 wi 的 历史 词 的 
数目 等等 看到 这里 是不是 有点 熟悉 这不 就是 大名鼎鼎 
的 马尔可夫 链 嘛 不熟悉 马尔可夫 链 的 朋友 们 
可以 自行 百度一下 ~ 没错 我们 称 其为 马尔可夫 方法 
假设 任意 一个 词 wi 出现 的 概率 只 与 
它 前面 的 wi 1 有关 将 原 模型简化 为 
二元 模型 P S = P w1 P w2 | 
w1 . . . P wi | wi 1 . 
. . P wn | wn 1 在此 基础 上 
提出 n 元 文法 n gram 一个词 由 它 前面 
的 n 1个 词 决定 注意 是 n 1 哦 
不是 n 一元 文法 1 gram n = 1 P 
wi | w1 w2 . . . wi 1 = 
P wi 出现 在 每 一位 上 的 基元 wi 
独立 于 历史 二元 文法 2 gram n = 2 
P wi | w1 w2 . . . wi 1 
= P wi | wi 1 1 阶 马尔可夫 链 
三元 文法 3 gram n = 3 P wi | 
w1 w2 . . . wi 1 = P wi 
| wi 2 wi 1 2 阶 马尔可夫 链 以此类推 
理论上 讲 n 元 文法 中的 n 越大 越好 越能 
保留 句子 词 之间 的 相关性 但是 正如 上文 所说 
n 越大 也就 意味着 需要 估计 的 参数 越多 一般来讲 
n = 3 用得 相对 较多 n = 4 的 
时候 参数 就 已经 太 多了 值得 注意 的 是 
即使 采用 n 较大 的 高阶 模型 也 无法 覆盖 
全部 的 语言 现象 因为 现阶段 对于 人类 来讲 语言 
依然 是 一个 谜 了解 了 语言 模型 之后 问题 
来了 如何 估计 语言 模型 的 参数 呢 用于 计算 
P S 的 各个 P wi | wi n + 
1 . . . wi 1 值 利用 极大 似 
然 估计 Maximum Likelihood Evaluation MLE 从 个人 理解 的 
角度 出发 我 认为 利用 极大 似 然 估计 在 
语料 中 统计 语言 模型 参数 的 过程 可以 形象化 
地 概括 为 两个 字 数 三声 数 四声 极大 
似 然 估计 的 过程 大致 如下 假设 我们 的 
语料库 中 只有 三 句话 语料库 的 概念 将 在后面 
进行 介绍 1 . John read Moby Dick2 . Mary 
read a different book3 . She read a book by 
Cher 现在 我们 想要 估计 2 gram 模型 中 John 
read a book 这句话 出现 的 概率 P John read 
a book = P John | BOS P read | 
John P a | read P book | a P 
EOS | book 其中 BOS 和 EOS 分别/d 表示/v 句子/n 
的/uj 开始/v 符和/nr 结束符/n 我们 可以 从语/nr 料中 1 2 
3 一共 三句话 统计 得到 P John | BOS = 
1/3 因为 BOS 出现 了 三次 每个 句子 的 开始 
后面 接 John 的 有一次 即 John 出现 在 句首 
P read | John = 1/1 因为 John 出现 了 
一次 后面 接 read 的 有一次 P a | read 
= 2/3 因为 read 出现 了 3 词 后面 接 
a 的 有一次 P book | a = 1/2 因为 
a 出现 了 2次 后面 接 book 的 有一次 P 
EOS | book = 1/2 因为 book 出现 了 2次 
book 出现 在 句子 结尾 后面 接 EOS 的 有 
1次 因此 P S = 1/18 通过 这个 小小的 例子 
我们 可以 发现 语言 模型 可以 预测 句子 的 下 
一个 词 已知 前面 的 词 也 可以 计算 一句话 
出现 的 概率 决定 哪 一个 词 序列 出现 的 
概率 大 下面 我们 再 举 一个 例子 还是 上 
面的 语料库 还是 使用 2 gram 我们 想 要求 Cher 
  read a book 的 概率 看起来 没什么 问题 啊 
按着 上面 的 套路 做 就 可以 了 啊 但是 
在 实际 计算 的 时候 我们 发现 P Cher | 
BOS = 0 Cher 这个词 根本 就 没有 出现 在 
句首 过 这也 就 意味着 无论 后面 几个 概率值 是 
多少 我们 计算 出来 的 整个 句子 出现 的 概率 
都是 0 这个 问题 被 称为 零 概率 问题 是由 
数据 稀疏 匮乏 造成 的 那么 遇到 了 这个 问题 
该 如何 解决 呢 进行 数据 平滑 数据 平滑 的 
方法 有 很多 种 在 这里 不 一一 进行 赘述 
有兴趣 的 朋友 可以 自行 百度 了解 总之 数据 平滑 
之后 不会 出现 上面 例子 中 出现 的 零 概率 
问题 我们 就 可以 安安心 心地 用 语言 模型 计算 
一句话 出现 的 概率 啦 ~ 到 这里 对 语言 
模型 的 介绍 就 基本 结束 了 我们 还 剩下 
一些 边边角角 的 问题 需要 解决 比如 如何 评价 语言 
模型 目前 语言 模型 的 评价 一般 有 两种 方法 
1 . 实用 方法 通过 查看 该 模型 在 实际 
应用 如 机器翻译 中 的 表现 来 评价 优点 是 
直观 实用 缺点 是 缺乏 针对性 不够 客观 2 . 
理论 方法 为了 评价 语言 模型 提出 了 一个 困惑 
度 的 概念 方法 如下 平滑 过后 的 n gram 
模型 句子 的 概率 为 与 语言 模型 中 的 
描述 相同 P S = P w1 . . . 
P wi | wi n + 1 . . . 
wi 1 . . . 假设 测试 语料 T 由 
k 个 句子 t1 t2 . . . tk 构成 
那么 整个 测试 集 T 的 概率 为 p T 
= P t1 P t2 . . . P tk 
定义 模型 对于 测试 预料 的 交叉 熵 为 其中 
分母 项为/nr 测试 语料 T 中的 词数 可以 理解 为 
测试 文 本集 的 单词表 定义 模型 的 困惑 度 
为 困惑 度 越低 模型 评价 越高 在 这里 多 
说几句 题外话 对于 一个 模型 而言 最 重要 的 部分 
有 四个 输入 输出 参数 对应 关系 函数 对于 任何 
一个 模型 而言 把握住 这四个 基本要素 相信 会 帮助 大家 
理解 得 更加 透彻 特别 是 今后 我们 会 提到 
的 神经 网络 二 语料库 corpus 语料库 是 什么 语料库 
就是 存放 在 计算机 里 的 原始 语料 文本 或 
经过 加工 后 带有 语言学 信息 标注 的 语料 文本 
为了 方便 理解 我们 可以 将 其 看作 一个 数据库 
我们 从中/nr 提取 语言 数据 以便 对 其 进行 分析 
处理 语料库 有 三点 特征 1 . 语料库 中 存放 
的 是 在 实际 使用 中 真是 出现 过 的 
语言 材料 2 . 语料库 是以 计算机 为 载体 承载 
语言 知识 的 基础 资源 但 并不 等于 语言 知识 
3 . 真实 语料 需要 经过 分析 处理 和 加工 
才能 成为 有用 的 资源 这 一点 尤为重要 在 NLP 
知识 的 学习 中 需要 对 语言 原材料 进行 一 
系列 的 处理 才 能够 使用 在 这里 不对 语料库 
进行 系统 的 介绍 仅 需要 了解 我们 可以 从中/nr 
提取 语言 数据 来用 就/d 可以/c 啦/y ~/i 如果/c 有/v 
朋友/n 对/p 语料库/n 的/uj 分类/n 和/c 现阶段/t 一些/m 著名/a 的/uj 
语料库/n 感兴趣/n 可以 自行 百度 了解 ~ 现在 我们 已经 
对 统计 语言 模型 和 语料库 有了 基本 的 了解 
在下 一 部分 的 内容 中 我们 将 介绍 神经 
网络 语言 模型 和 一些 在 NLP 领域 常用 的 
基本 的 神经 网络 如果 本文 中 某些 表述 或 
理解 有误 欢迎 各位 大神 批评指正 谢谢 书单 详解 与 
下载 链接 学习 OpenCV 人工智能 一种 现代 的 方法 智能 
Web 算法 语音 与 语言 处理 模式 识别 与 机器学习 
游戏 人工智能 编程 案例 精粹 统计 自然语言 处理 基础 模式 
分类 模式识别 中的 神经 网络计算机 视觉 人工智能 游戏 编程 真言 
Java 设计模式 第 2 版 Python 自然语言 处理 实用 Common 
Lisp 编程 自然语言 处理 Yelp Open Dataset Yelp 数据集 是 
用于 NLP 的 Yelp 业务 评论 和 用户 数据 的 
子集 NLP Chinese Corpus 大 规模 中文 自然语言 处理 语料 
腾讯 中文 词 NLP 数据集 该 数据 包含 800 多万 
中文 词汇 其中 每个 词 对应 一个 200 维 的 
向量 相比 现有 的 公开 数据集 在 覆盖率 新鲜度 及 
准确性 上 大幅 提高 在 对话 回复 质量 预测 医疗 
实体 识别 等 自然 语言 处理 方向 的 业务 应用 
方面 腾讯 内部 效果 提升 显著 NarrativeQA DeepMind 机器 阅读 
理解 数据集 是 第一 个 基于 整本书 或 整个 剧本 
的 大规模 问答 数据集 数据 集中 该有 的 所有 文档 
非正式 汉语 数据集 收集 了 3700 万条 图书 评论 和5/nr 
万条 bbs 回帖 作为 大型 非正式 汉语 数据集 LSICC 内容 
来源 分别 是 豆瓣 读书 和 Chiphell 论坛 豆瓣 读书 
评论 Chiphell 回帖 SQuAD 一个 最新 的 阅读 理解 数据集 
该 数据集 包含 10 万个 问题 原文 答案 三元组 原文 
来自 于 536 篇 维基百科 文章 安然 数据集 安然 集团 
高级 管理层 的 电子 邮件 数据 Google Books Ngram 来自 
Google 书籍 的 词汇 集合 博客 语料库 从 blogger . 
com 收集 的 681 288篇 博客 文章 每个 博客 至少 
包含 200个 常用 的 英语 单词 维基百科 链接 数据 Wikipedia 
Links data 维基百科 全文 该 数据集 包含 来自 400 多万 
篇文章 近 19 亿字 你 可以 对 字 短语 或 
段落 本身 的 一 部分 进行 搜索 Gutenberg 电子图书 列表 
Project Gutenberg 的 附加 注释 的 电子书 列表 Hansards 加拿大 
议会 的 文本 块 Hansards text chunks of Canadian Parliament 
来自 第 36届 加拿大议会 记录 的 130万 对 文本 危险 
边缘 Jeopardy 来自 问答 游戏 节目 危险 边缘 Jeopardy 的 
超过 20 万个/nr 问题 的 存档 英文 SMS 垃圾邮件 收集 
SMS Spam Collection in English 包含 5 574条 英文 垃圾 
邮件 的 数据集 Yelp 评论 Yelp Reviews Yelp 发布 的 
一个 开放 数据集 包含 超过 500 万次 评论 UCI 的 
垃圾 邮件 库 UCI s Spambase 一个 大型 垃圾邮件 数据集 
用于 垃圾邮件 过滤 亚马逊 评论 包含 18 年来 亚马逊 上 
的 大约 3500 万条 评论 数据 包括 产品 和 用户 
信息 评级 和 文本 审核 问答 Topical Chat 数据集 亚马逊 
将 公布 超过 最大 会话 和 知识 数据集 超 410万 
单词 21万 句子 的 语料库 将于 2019年 9月 17日 发布 
主题 聊天 数据集 将 包含 超过 210 000个 句子 超过 
4 100 000个 单词 可支持 高质量 可 重复 的 研究 
将 成为 研究 界 公开 可用 的 最大 社交 对话 
和 知识 数据集 数学 题海 数据集 DeepMind 发布 包含 大量 
不同 类型 的 数学 问题 练习题 级别 旨在 考察 模型 
的 数学 学习 和 代数 推理 能力 包含 200 万 
问题 答案 对 和 10000 个 预 生成 测试 样本 
问题 的 长度 限制 为 160 字符 答案 的 长度 
限制 为 30 字符 每个 问题 类型 中 的 训练 
数据 被 分为 「 容易 训练 」 「 中等 训练 
难度 」 和「/nr 较难 训练 」 三个 级别 GQA 图像 
场景 图 问答 数据集 斯坦福 大学 教授 Christopher Manning 及其 
学生 Drew Hudson 一同 打造 的 旨在 推动 场景 理解 
与 视觉 问答 研究 领域 的 进步 包含 高达 20M 
的 各种 日常生活 图像 主要 源自 于 COCO 和 Flickr 
每张 图像 都与 图中 的 物体 属性 与 关系 的 
场景 图 scene graph 相关 创建 上 基于 最新 清洁 
版本 的 Visual Genome 此外 每个 问题 都 与其 语义 
的 结构 化 表示 相关联 功能 程序上 指定 必须 采取 
一定 的 推理 步骤 才能 进行 回答 Natural Questions 数据集 
Google 发布 一个 新 的 大规模 训练 和 评估 开放 
领域 超 难 问答 数据集 「 自然 问题 」 能够 
训练 AI 阅读 维基百科 并 找到 各种 开放 领域 问题 
的 答案 1 超过 30 万组 问答 其中 训练 集 
有 307 372组 问答 包含 152 148 组长 答案 问答 
和 110 724组 短 答案 问答 2 开发 示例 问答 
包含 有 7830组 一 问 五 答 的 问答 也 
就是 同 一个 问题 找 五 个人 分别 从 维基百科 
中 寻找 答案 以此 来 衡量 QA 问答 系统 的 
表现 3 测试 集 有 7842组 问答 GQA 图像 场景 
图 问答 数据集 GQA 是 斯坦福 大学 教授 Christopher Manning 
及其 学生 Drew Hudson 一同 打造 的 全新 图像 场景 
图 问答 数据集 旨在 推动 场景 理解 与 视觉 问答 
研究 领域 的 进步 该 数据集 包含 高达 20M 的 
各种 日常生活 图像 主要 源自 于 COCO 和 Flickr 每张 
图像 都与 图中 的 物体 属性 与 关系 的 场景 
图 scene graph 相关 创建 上 基于 最新 清洁 版本 
的 Visual Genome 此外 每个 问题 都 与其 语义 的 
结构 化 表示 相关联 功能 程序上 指定 必须 采取 一定 
的 推理 步骤 才能 进行 回答 NLPCC2016KBQA 数据集 基于 知识图谱 
的 问答 系统 其 包含 14 609 个 问 答对 
的 训练 集 和 包含 9870 个 问 答对 的 
测试 集 并 提供 一个 知识库 包含 6 502 738 
个 实体 587 875 个 属性 以及 43 063 796 
个 三元组 知识库 文件 中 每行 存储 一个 事实 fact 
即 三元组 实体 属性 属性值 原 数据 中 本 只有 
问 答对 question answer 并无 标注 三元组 triple 本人 所用 
问答 对 数据 来自 该 比赛 第一名 的 预处理 HotpotQA 
面向 自然 语言 和 多步 推理 问题 新型 问答 数据集 
具有 自然 多跳 问题 的 问答 数据集 具有 支持 事实 
的 强大 监督 以 实现 更 易于 解释 的 问答 
系统 CoQA 斯坦福 最新 问答 数据集 囊括 来自 7 个 
不同 领域 的 文本 段落 里 8000 个 对话 中的 
127 000 轮 问答 推荐 系统 yf _ amazon 数据集 
52 万件 商品 1100 多个 类目 142 万 用户 720 
万条 评论 / 评分 数据 yf _ dianping 数据集 24 
万 家餐馆 54 万 用户 440 万条 评论 / 评分 
数据 dmsc _ v2 数据集 28 部 电影 超 70 
万 用户 超 200 万条 评分 / 评论 数据 ez 
_ douban 数据集 5 万 多部 电影 3 万多 有 
电影 名称 2 万多 没有 电影名称 2.8 万 用户 280 
万条 评分 数据 亚马逊 评论 3500 万条 来自 亚马逊 的 
评论 时间 长度 为 18年 数据 包括 产品 和 用户 
信息 评级 等 情感 / 观点 / 评论 倾向性 分析 
yf _ amazon 数据集 52 万件 商品 1100 多个 类目 
142 万 用户 720 万条 评论 / 评分 数据 yf 
_ dianping 数据集 24 万 家餐馆 54 万 用户 440 
万条 评论 / 评分 数据 dmsc _ v2 数据集 28 
部 电影 超 70 万 用户 超 200 万条 评分 
/ 评论 数据 simplifyweibo _ 4 _ moods 数据集 36 
万多条 带 情感 标注 新浪 微博 包含 4 种 情感 
其中 喜悦 约 20 万条 愤怒 厌恶 低落 各 约 
5 万条 weibo _ senti _ 100k 数据集 10 万多条 
带 情感 标注 新浪 微博 正 负向 评论 约 各 
5 万条 online _ shopping _ 10 _ cats 数据集 
10 个 类别 共 6 万多条 评论 数据 正 负向 
评论 各 约 3 万条 包括 书籍 平板 手机 水果 
洗发水 热水器 蒙牛 衣服 计算机 酒店 waimai _ 10k 数据集 
某 外卖 平台 收集 的 用户 评价 正向 4000 条 
负向 约 8000 条 ChnSentiCorp _ htl _ all 数据集 
7000 多条 酒店 评论 数据 5000 多条 正向 评论 2000 
多条 负向 评论 多 域 情感 分析 数据集 Multidomain sentiment 
analysis dataset 一个 比较 有 历史 的 数据 集 里面 
还有 一些 来自 亚马逊 的 产品 评论 IMDB 评论 影评 
也 是 比较 有 历史 的 二元 情绪 分类 数据集 
数据 规模 相对 较小 里面 有 25 000 条 电影 
评论 斯坦福 情感 树 银行 Stanford Sentiment Treebank 带有 情感 
注释 的 标准 情绪 数据集 Sentiment140 一个 流行 的 数据 
集 它 使用 16 万条 推 文 并把 表情 等等 
符号 剔除 了 Twitter 美国航空公司 情绪 数据集 Twitter US Airline 
Sentiment 自 2015 年 2 月 以来 美国 航空 公司 
的 Twitter 数据 分类 为 正面 负面 和 中性 推 
文 中文 命名 实体 识别 dh _ msra 数据集 5 
万多条 中文 命名 实体 识别 标注 数据 包括 地点 机构 
人物 在 入门 的 阶段 最 适合 做 的 事情 
1 阅读 和 学习 自然语言 处理 natural language processing nlp 
综述 类 文章 和 图书 对 nlp 有 一个 基本 
的 认识 梳理 nlp 研究 内容 的 演变 包括/v nlp/w 
从/p 诞生/v 到/v 多次/m 繁荣/a 发展/vn 和/c 多次/m 停滞不前/l 的/uj 
原因/n 正确认识 nlp 与 人工智能 机器学习 自然语言 理解 计算 语言学 
文本 挖掘 等 概念 之间 的 区别 与 联系 除了 
我 下面 推荐 的 图书 文章 和 代码 一定 要 
多 百度 和 google 寻找 学习 资料 2 做 一些 
非常 简单 的 nlp 入门 小 任务 通过 小 任务 
理解 自然语言 处理 的 流程 包括 中 英文 nlp 处理 
流程 的 差别 3 快速 学习 一门 开发 nlp 技术 
的 编程语言 基本 就是 python 了 不 需要 学习 网络 
编程 等 部分 重点 掌握 python 基本 语法 文件 读写 
与 编码 正则表达式 gensim numpy pandas matplotlib 等 的 使用 
4 学习 机器 学习 的 相关 概念 如 模型 评估 
与 选择 有 监督 学习 半 监督 学习 无 监督 
学习 强化 学习 迁移 学习 只 需要 学习 和 认识 
其 原理 做到 心中有数 即可 5 关注 各大 内容 大 
V 如 微信 公众 号 paperweekly 新 智 元 AI 
科技 大本营 机器 之心 人工智能 头条 等 知乎 一搜 自然语言 
处理 或者 nlp 就能 看到 好 多大 V 博主 我 
爱 自然 语言 处理 等 因为 每天 各大 V 都会 
发布 很多 的 新闻 和 内容 初 入门 的 小白 
不 建议 全篇 深入 阅读 感 兴趣 的 可以 收藏 
起来 以后 看 大 部分 就 了解 一下 1 谁 
2 干了 啥 咋 干的/nr 3 啥 效果 然 后读 
了 这个 文章 的 报道 4 自己 有啥 想法 就 
可以 了 注 有 一个 小 细节 上 的 建议 
就是 在 平日 里 有 任何 idea 最好 都要 在 
手机 或者 任何 地方 的 备忘录 里 记 一下 也 
要尽 快多 实践 以 验证 idea 是否 有效 6 了解 
国内外 nlp 的 协会 组织 如 中文信息 学会 中文信息 学会 
青年 工作 委员会 ACL 等 主要 查找 和 阅读 协会 
开辟 的 专栏 组织 撰写 的 综述 看看/v 最近/f 都/d 
组织/v 了/ul 什么/r 会议/n 和/c 比赛/vn 等/u 主要 的 大型 
nlp 科研 团队 斯坦福 多伦多 清华 北大 哈工大 复旦 中科 
deepmind google brain openAI 等 高校 院所 和 科研 机构 
的 自然 语言 处理 小组 看看 他们 都在/nr 研究 什么 
7 关注 中国 中文信息 学会 的 微信 公众 号 时刻 
关注 ACL IJCAI ACML SIGIR 等 顶 会 论文 报告会 
由 中文信息 学会 组织 报告 的 人都 是 当年 被 
顶 会 录取 的 论文 的 国内 作者 和 大佬 
来自 各 大 高校 和 院所 整个 报告会 和 国际 
会议 的 日程 都是 一样 的 都有 coffee break 可以 
在 茶歇 的 时候 近距离 与 大佬 交流 是 一个 
近 距离 与 国内 各大 NLP 大 组 老师 同学 
交流 的 机会 看看 大厂 们 都在 干什么 推荐 有 
机会 的话 一定 要 去 听 一下 报告 但 一定 
要 保证 已经 看过 很多 论文 并且 有 一定 基础 
再去 听 完全 零基础 不 建议 去 会议 比较 火爆 
和 受欢迎 建议 时刻 关注 尽快 报名 nlp 非常容易 入门 
的 原因 是 这是 一门 非常 开放 的 学科 各大 
高校 学者/n 都有/nr 一颗/m 开放/v 的/uj 心/n 源码 经常 开源 
而且/c 有/v 很多/m 优秀/a 的/uj 老师/n 有写/nr 博客/nr 博文 的 
习惯 大家 都 喜欢 一有 研究 成果 就 立马 放到 
arxiv 或者 researchgate 上 nlp 的 研究 日新月异 变化 的 
非常 快 的 原因 就是 开放 所以 有 什么 问题 
尽量 面向 百度 google 查询 注意 关键词 毕竟 是 搞 
nlp 的 可以 先 学习 一下 检索 的 原理 然后 
就 知道 怎么 检索 能 尽快 找到 你 想要 的 
的 东西 检索 的 结果 重点 关注 reddit medium csdn 
arxiv researchgate 知乎 stackoverflow github 等 上面 的 内容 可 
参考 学习 和 实践 的 链接 如下 复旦 大学 邱锡鹏/nr 
组 实验室 新生 一般 完成 的 五个 NLP 练习 上手 
实验 NLP 四大 类 任务 分类 序列 标注 文本 匹配 
文本 生成 都 需要 完整 实现 一遍 https / / 
www . zhihu . com / question / 324189960 / 
answer / 682130580 utm _ source = wechat _ session 
& utm _ medium = social & utm _ oi 
= 9 5 2 4 6 6 0 2 0 
5 8 2 0 6 4 1 2 8 自动化所 
宗 成庆 研究员 读懂 NLP 的 过去 与 现在 梳理 
的 非常 好 把/p 各个/r 概念/n 之间/f 的/uj 关系/n 和/c 
NLP/w 的/uj 发展/vn 都/d 梳理/v 清楚/a 了/ul https / / 
mp . weixin . qq . com / s / 
xgySwq2m mHT7XG1zZGpzw 中文 自然语言 处理 入门 实战 https / / 
mp . weixin . qq . com / s / 
5z7Xy4NL buUkpBmv4iIpw 自然语言 处理 全家福 纵览 当前 NLP 中 的 
任务 数据 模型 与 论文 https / / mp . 
weixin . qq . com / s / s Q 
9 0 3 W N R 4 v 3 6 
7 t 7 8 _ VG1Q 中文 信息 处理 发展 
报告 综述 由 中文信息 学会 统筹 国内 各大 NLP 专家 
撰写 非常适合 入门 了解 NLP http / / cips upload 
. bj . bcebos . com / cips2016 . pdfJumping 
NLP Curves A Review of Natural Language Processing Research Review 
Article https / / ieeexplore . ieee . org / 
document / 6786458Natural Language Processing A Reviewhttps / / www 
. researchgate . net / publication / 309210149 _ Natural 
_ Language _ Processing _ A _ ReviewA Review of 
the Neural History of Natural Language Processinghttp / / ruder 
. io / a review of the recent history of 
nlp/w //i 邓力和/nr 刘洋/nr 大神/n 合著/a 的/uj 图书/n Deep Learning 
in Natural Language Processing 就 不给 链接 了 百度 或 
google 搜索 有 中文 连载 英文原版 的 原版 图书 购买 
pdf 分享 或者 课程 宗 成庆 研究员 所著 统计 自然语言 
处理 经典之作 Steven Bird 所著 Python 自然语言 处理 快速 掌握 
python 开发 nlp 技术 的 各种 能力 机器学习 → 推荐 
周志华 教授 所著 的 机器学习 深度 学习 → 推荐 Ian 
Goodfellow 等人 合著 的 Deep Learning 开发 框架 → 首推 
Pytorch 推荐 陈云 的 深度 学习 框架 Pytorch 入门 与 
实践 或者 廖 星宇 的 深度 学习 入门 之 Pytorch 
Tensorflow 学习 → 推荐 黄 文坚 唐源的/nr Tensorflow 实战 注意 
有些 情况 需要 科学 上网 一定 要 掌握 google 和 
google scholar 的 使用 阶段 一 人工智能 基础 － 高等数学 
必 知 必会 本 阶段 主要 从 数据分析 概率论/n 和/c 
线性代数/l 及/c 矩阵/n 和凸/nr 优化/vn 这/r 四大/l 块/zg 讲解/v 基础/n 
旨在 训练 大家 逻辑 能力 分析 能力 拥有 良好 的 
数学 基础 有利于 大家 在 后续 课程 的 学习 中 
更好 的 理解 机器学习 和 深度 学习 的 相关 算法 
内容 同时 对于 AI 研究 尤为重要 例如 人工智能 中的 智能 
很大 一部分 依托 概率论 实现 的 一 数据分析 1 常数 
e2 导数 3 梯度 4 Taylor5 gini 系数 6 信息熵 
与 组合 数 7 梯度 下降 8 牛顿 法二/nr 概率论 
1 微积分 与 逼近 论 2 极限 微分 积分 基本概念 
3 利用 逼近 的 思想 理解 微分 利用 积分 的 
方式 理解 概率 4 概率论 基础 5 古典 模型 6 
常见 概率分布 7 大数定理 和 中心 极限 定理 8 协方差 
矩阵 和 相关 系数 9 最大/a 似/d 然/c 估计/v 和/c 
最大/a 后验/nr 估计/v 三/m 线性代数 及 矩阵 1 线性空间 及 
线性变换 2 矩阵 的 基本 概念 3 状态 转移 矩阵 
4 特征向量 5 矩阵 的 相关 乘法 6 矩阵 的 
QR 分解 7 对称矩阵 正交矩阵 正定矩阵 8 矩阵 的 SVD 
分解 9 矩阵 的 求导 10 矩阵 映射 / 投影 
四 凸 优化 1 凸 优化 基本概念 2 凸 集 
3 凸函数 4 凸 优化 问题 标准 形式 5 凸 
优化 之 Lagerange 对偶 化 6 凸 优化 之 牛顿 
法 梯度 下 降法 求解 阶段 二 人工智能 提升 － 
Python 高级 应用 随着 AI 时代 的 到来 以及 其 
日益 蓬勃 的 发展 Python 作为 AI 时代 的 头牌 
语言 地位 基本 确定 机器学习 是 着实 令人 兴奋 但 
其 复杂度 及 难度 较大 通常 会 涉及 组装 工作流 
和 管道 设置/vn 数据源/n 及/c 内部/f 和云/nr 部署/n 之间/f 的/uj 
分流/n 而/c 有了/i Python/w 库/n 后/f 可 帮助 加快 数据 
管道 且 Python 库 也在 不断更新 发布 中 所以 本 
阶段 旨在 为 大家 学习 后续 的 机器学习 减负 一 
容器 1 列表 list2 元组 tuple3 字典 dict4 数组 Array5 
切片 6 列表 推导 式 7 浅 拷贝 和深/nr 拷贝 
二 函数 1 lambda 表达式 2 递归函数 及 尾递归 优化 
3 常用 内置 函数 / 高阶 函数 4 项目 案例 
约瑟夫 环 问题 三 常用 库 1 时间 库 2 
并发 库 3 科学计算 库 4 Matplotlib 可视化 绘 图库 
5 锁 和 线程 6 多线程 编程 阶段 三 人工智能 
实用 － 机器学习 篇 机器学习 利用 算法 去 分析 数据 
学习 数据 随后 对 现实 世界 情况 作出 判断 和 
预测 因此 与 预先 编写 好 只能 按照 特定 逻辑 
去 执行 指令 的 软件 不同 机器 实际上 是 在用 
大量 数据 和 算法 去 自我 训练 从而 学会 如何 
完成 一项 任务 所以 本 阶段 主要 从 机器学习 概述 
数据 清洗 和 特征选择 回归 算法 决策树 随机 森林 和 
提升 算法 SVM 聚 类 算 EM 算法 贝叶斯 算法 
隐 马尔科夫 模型 LDA 主题 模型 等 方面 讲解 一些 
机器学习 的 相关 算法 以及 这些 算法 的 优化 过程 
这些 算法 也 就是 监督 算法 或者 无 监督 算法 
一 机器学习 1 机器学习 概述 二 监督 学习 1 逻辑 
回归 2 softmax 分类 3 条件 随 机场 4 支持 
向量 机 svm5 决策树 6 随机 森林 7 GBDT8 集成 
学习 三 非 监督 学习 1 高斯 混合模型 2 聚 
类 3 PCA p 4 密度估计 / p / td 
td p 5 LSI & nbsp br 6 LDA & 
nbsp br 7 双聚类/nr & nbsp br 8 降 维 
算法 / p / td / tr tr td colspan 
= 2 h3 a name = t14 / a 四 
数据处理 与 模型 调 优 / h3 / td / 
tr tr td p 1 特征提取 br 2 数据 预处理 
br 3 数据 降 维 br 4 模型 参数 调 
优 br 5 模型 持久化 / p / td td 
p 6 模型 可视化 br 7 优化 算法 坐标轴 下 
降法 和 最小 角回 归法 br 8 数据挖掘 关联 规则 
算法 br 9 感知器 模型 / p / td / 
tr / tbody / table / div h1 a name 
= t15 / a 阶段 四 人工智能 实用 － 数据挖掘 
篇 / h1 本 阶段 主要 通过 音乐文件 分类 和 
金融 反 欺诈 模型 训练 等 项目 帮助 大家 对于 
上 阶段 的 机器 学习 做 更 深入 的 巩固 
为 后续 深度 学习 及 数据挖掘 提供 项目 支撑 项目 
一 百度 音乐 系统文件 分类 音乐 推荐 系统 就是 利用 
音乐 网站 上 的 音乐 信息 向 用户 提供 音乐 
信息 或者 建议 帮助 用户 决定 应该 听 什么 歌曲 
而 个人化 推荐 则是 基于 音乐 信息 及 用户 的 
兴趣 特征 听歌 历史 行为 向 用户 推荐 用户 可能 
会 感兴趣 的 音乐 或者 歌手 推荐算法 主要 分为 以下 
几种 基于 内容 的 推荐 协同 过滤 推荐 基于 关联 
规则 推荐 基于 效用 推荐 基于 知识 推荐 等 推荐 
系统 常 用于 各个 互联网 行业 中 比如 音乐 电商 
旅游 金融 等 项目 二 千万级 P2P 金融 系统 反 
欺诈 模型 训练 目前 比较 火 的 互联网 金融 领域 
实质 是 小额 信贷 小额 信贷 风险 管理 本质上 是 
事前 对 风险 的 主动 把 控 尽可能 预测 和 
防范 可能 出现 的 风险 本 项目 应用 GBDT Randomforest 
等 机器学习 算法 做 信贷 反 欺诈 模型 通过 数据挖掘 
技术 机器学习 模型 对 用户 进行 模型 化 综合 度量 
确定 一个 合理 的 风险 范围 使 风险 和 盈利 
达到 一个 平衡 的 状态 阶段 五 人工智能 前沿 － 
深度 学习 篇 深度 学习 是 实现 机器 学习 的 
技术 同时 深度 学习 也 带来 了 机器 学习 的 
许多 实际 应用 拓展 了 AI 的 使用 领域 本 
阶段 主要 从 TensorFlow BP 神经网络 深度 学习 概述 CNN 
卷积 神经网络 递归 神经网 自动编码 机 序 列到 序列 网络 
生成 对抗 网络 孪生 网络 小样 本 学习 技术 等 
方面 讲解 深度 学习 相关 算法 以 掌握 深度 学习 
前沿技术 并 根据 不同 项目 选择 不同 的 技术 解决 
方案 针对 公司 样本 不足 采用 小 样本 技术 和 
深度 学习 技术 结合 是 项目 落地 的 解决 方案 
1 TensorFlow 基本 应用 2 BP 神经网络 3 深度 学习 
概述 4 卷积 神经网络 CNN 5 图像 分类 vgg resnet 
6 目标 检测 rcnn fast rcnn faster rcnn ssd 7 
递归 神经网络 RNN 8 lstm bi lstm 多层 LSTM9 无 
监督 学习 之 AutoEncoder 自动 编码器 10 Seq2Seq11 Seq2Seq with 
Attension12 生成 对抗 网络 13 irgan14 finetune 及 迁移 学习 
15 孪生 网络 16 小样 本 学习 阶段 六 人工智能 
进阶 － 自然语言 处理 篇 自然语言 处理 NLP 是 计算机 
科学 领域 与 人工智能 领域 中 的 一个 重要 方向 
它 已 成为 人工智能 的 核心 领域 自然语言 处理 解决 
的 是 让 机器 可以 理解 自然语言 这一 到 目前 
为止 都还/nr 只是 人类 独有 的 特权 被 誉为 人工智能 
皇冠 上 的 明珠 被 广泛 应用 本 阶段 从 
NLP 的 字 词 和 句子 全方位 多角度 的 学习 
NLP 作为 NLP 的 基础 核心技术 对 NLP 为 核心 
的 项目 如 聊天 机器人 合理 用药 系统 写诗 机器人 
和 知识图谱 等 提供 底层 技术 通过学习 NLP 和 深度 
学习 技术 掌握 NLP 具有 代表性 的 前沿 技术 1 
词 分词 词性 标注 代码 实战 2 词 深度 学习 
之词 向量 字 向量 代码 实战 3 词 深度 学习 
之 实体 识别 和 关系 抽取 代码 实战 4 词 
关键词 提取 无 用词 过滤 代码 实战 5 句 句法分析 
语义分析 代码 实战 6 句 自然语言 理解 一阶逻辑 代码 实战 
7 句 深度 学习 之 文本 相似 度 代码 实战 
阶段 七 人工智能 进阶 － 图像处理 篇 数字图像处理 Digital Image 
Processing 是 通过 计算机 对 图像 进行 去除 噪声 增强 
复原 分割 提取 特征 等 处理 的 方法 和 技术 
广泛 的 应用 于 农牧业 林业 环境 军事 工业 和 
医学 等 方面 是 人工 智能 和 深度 学习 的 
重要 研究 方向 深度 学习 作为 当前 机器学习 领域 最 
热门 的 技术 之一 已经 在 图像 处理 领域 获得 
了 应用 并且 展现 出 巨大 的 前景 本 阶段 
学习 了 数字 图像 的 基本 数据 结构 和 处理 
技术 到 前沿 的 深度 学习 处理 方法 掌握 前沿 
的 ResNet SSD Faster RCNN 等 深度 学习 模型 对 
图像 分类 目标 检测 和 模式 识别 等 图像处理 主要 
领域 达到 先进水平 实际 工作 中 很多 项目 都 可以 
转化 为 本 课程 的 所学 的 知识 去 解决 
如 行人 检测 人脸 识别 和 数字 识别 一 图像 
基础 图像 读 写 保存 画图 线 圆 多边形 添加 
文字 二 图像 操作 及 算数 运算 图像 像素 读取 
算数 运算 ROI 区域 提取 三 图像 颜色 空间 运算 
图像 颜色 空间 相互 转化 四 图像 几何变换 平移 旋转 
仿射变换 透视 变换 等 五 图像 形态学 腐蚀 膨胀 开 
/ 闭 运算 等 六 图像 轮廓 长宽 面积 周长 
外接圆 方向 平均 颜色 层次 轮廓 等 七 图像 统计学 
图像 直方图 八 图像 滤波 高斯滤波 均值 滤波 双边 滤波 
拉普拉斯 滤波 等 阶段 八 人工智能 终极 实战 － 项目 
应用 本 阶段 重点 以 项目 为 导向 通过 公安 
系统 人脸识别 图像 识别 以及 图像 检索 今日 头条 CTR 
广告 点击量 预估 序列 分析 系统 聊天 机器人 等 多个 
项目 的 讲解 结合 实际 来 进行 AI 的 综合 
运用 项目 一 公安系统 人脸识别 图像识别 使用 深度 学习 框架 
从零开始 完成 人脸 检测 的 核心 技术 图像 类别 识别 
的 操作 从 数据 预处理 开始 一步步 构建 网络 模型 
并 展开 分析 与 评估 方便 大家 快速 动手 进行 
项目 实践 识别 上千 种 人 靓 返回 层次化 结构 
的 每个人 的 标签 项目 二 公安系统 图像 检索 本 
项目 基于 卷积 神经网 在 训练 过程 中 学习 出 
对应 的 二 值 检索 向量 对 全部 图 先 
做 了 一个 分 桶 操作 每次 检索 的 时候 
只取 本 桶 和 临近 桶 的 图片 作 比对 
而 不是 在 全域 做 比对 使用 这样 的 方式 
提高 检索 速度 使用 Tensorflow 框架 建立 基于 ImageNet 的 
卷积 神经网络 并 完成 模型 训练 以及 验证 项目 三 
今日 头条 CTR 广告 点击量 预估 点击率 预估 是 广告 
技术 的 核心 算法 之一 它 是 很多 广告 算法 
工程师 喜爱 的 战场 广告 的 价值 就 在于 宣传 
效果 点击率 是 其中 最 直接 的 考核 方式 之一 
点击率 越大 证明 广告 的 潜在 客户 越多 价值 就 
越大 因此 才 会 出现 了 刷 点击率 的 工具 
和 技术 通过 对于 点击量 的 评估 完成 对于 潜在 
用户 的 价值 挖掘 项目 四 序列 分析 系统 时间 
序列 分析 Time Series Analysis 是 一种 动态数据 处理 的 
统计 方法 主要 基于 随机 过程 理论 和 数理统计 方法 
研究 随机 数据 序列 所 遵从 的 统计 规律 以便 
用于 解决 实际 问题 主要 包括 自 相关 分析 等 
一般 的 统计 分析 方法 构建 模型 从而 进行 业务 
推断 经典 的 统计 分析 是 假定 数据 序列 具有 
独立性 而 时间 序列 分析 则 侧重 于 研究 数据 
样本 序列 之间 的 依赖 关系 时间 序列 预测 一般 
反应 了 三种 实际 变化 规律 趋势 变化 周期性 变化 
和 随机 性 变化 时间/n 序列/n 预测/vn 常/d 应用/v 于/p 
国民经济/n 宏观/n 控制/v 企业 经营 管理 市场潜力 量 预测 天气预报 
水文预报 等 方面 是 应用 于 金融 行业 的 一种 
核心 算法 之一 项目 五 京东 聊天 机器人 / 智能 
客服 聊天 机器人 / 智能 客服 是 一个 用来 模拟 
人类 对话 或者 聊天 的 一个 系统 利用/n 深度/ns 学习/v 
和/c 机器/n 学习/v 等/u NLP/w 相关/v 算法/n 构建/v 出/v 问题/n 
和/c 答案/n 之间/f 的/uj 匹配/v 模型/n 然后 可以 将 其 
应用 到 客服 等 需要 在线 服务 的 行业 领域 
中 聊天 机器人 可以 降低 公司 客服 成本 还 能够 
提高 客户 的 体验 友 好性 在 一个 完整 的 
聊天 机器 人 实现 过程 中 主要 包含 了 一些 
核心 技术 包括 但 不限 于 爬虫 技术 机器学习 算法 
深度 学习 算法 NLP 领域 相关 算法 通过 实现 一个 
聊天 机器人 可以 帮助 我们 队 AI 整体 知识 的 
一个 掌握 项目 六 机器人 写 诗歌 机器人 写 诗歌 
/ 小说 是 一种 基于 NLP 自然语言 相关 技术 的 
一种 应用 在 实现 过程 中 可以 基于 机器学习 相关 
算法 或者 深度 学习 相关 算法 来 进行 小说 / 
诗歌 构建 过程 人工智能 的 一个 终极 目标 就是 让 
机器 人 能够 像 人类 一样 理解 文字 并 运用 
文字 进行 创作 而 这个 目标 大致上 主要 分为 两个 
部分 也 就是 自然语言 理解 和 自然 语言 生成 其中 
现阶段 的 主要 自然语言 生成 的 运用 自然语言 生成 主要 
有 两种 不同 的 方式 分别 为 基于 规则 和 
基于 统计 基于 规则 是 指 首先 了解 词性 及 
语法 等 规则 再 依据 这样 的 规则 写出 文章 
而 基于 统计 的 本质 是 根据 先前 的 字句 
和 统计 的 结果 进而 判断 下一 个子 的 生成 
例如 马尔科夫 模型 就 是 一种 常用 的 基于 统计 
的 方法 项目 七 机器翻译 系统 机器翻译 又称 自动 翻译 
是 指 利用 计算机 将 一种 自然 语言 转换 为 
另外 一种 自然 语言 的 过程 机器 翻译 是 人工智能 
的 终极 目标 之一 具有 很高 的 研究 价值 同时 
机器翻译 也 具有 比较 重要 的 实用 价值 机器 翻译 
技术 在 促进 政治 经济 文化 交流 等 方面 起到 
了 越来越 重要 的 作用 机器翻译 主要 分为 以下 三 
个 过程 原文 分析 原文 译文 转换 和 译文 生成 
机器 翻译 的 方式 有 很多 种 但是 随着 深度 
学习 研究 取得 比较 大 的 进展 基于 人工 网络 
的 机器 翻译 也 逐渐 兴起 特别 是 基于 长 
短时记忆 LSTM 的 循环 神经网络 RDD 的 应用 为 机器翻译 
添了 一把 火 项目 八 垃圾邮件 过滤 系统 邮件 主要 
可以 分为 有效 邮件 和 垃圾 邮件 两大类 有效 邮件 
指 的 邮件 接收者 有 意义 的 邮件 而 垃圾邮件 
转 指 那些 没有 任何 意义 的 邮件 其 内容 
主要 包含 赚钱 信息 成人 广告 商业 或者 个人 网站 
广告 电子杂志 等 其中 垃圾邮件 又 可以 发为 良性 垃圾 
邮件 和 恶性 垃圾邮件 良性 垃圾邮件 指 的 就是 对 
收件人 影响 不大 的 信息 邮件 而 恶性 垃圾邮件 指 
具有 破坏性 的 电子邮件 比如 包 含病毒 木马 等 恶意 
程序 的 邮件 垃圾邮件 过滤 主要 使用 使用 机器学习 深度 
学习 等 相关 算法 比如 贝叶斯 算法 CNN 等 识别 
出 所 接收 到 的 邮件 中 那些 是 垃圾 
邮件 项目 九 手工 数字 识 别人 认知 世界 的 
开始 就是 从 认识 数字 开始 的 深度 学习 也 
一样 数字 识别 是 深度 学习 的 一个 很好 的 
切入口 是 一个 非常 经典 的 原型 问题 通过 对 
手写 数字 识别 功能 的 实现 可以 帮助 我们 后续 
对 神经 网络 的 理解 和 应用 选取 手写 数字 
识别 的 主要 原因 是 手写 数字 具有 一定 的 
挑战性 要求 对 编程 能力 及 神经 网络 思维 能力 
有 一定 的 要求 但 同时 手写 数字 问题 的 
复杂度 不高 不 需要 大量 的 运算 而且 手写 数字 
也 可以 作为 其它 技术 的 一个 基础 所以 以 
手写 数字 识别 为 基础 贯穿 始终 从而 理解 深度 
学习 相关 的 应用 知识 项目 十 癌症 筛选 检测 
技术 可以 改变 癌症 患者 的 命运 吗 对于 患有 
乳腺癌 患者 来说 复发 还是 痊愈 影响 这 患者 的 
生命 那么 怎么 来 预测 患者 的 患病 结果呢 机器学习 
算法 可以 帮助 我们 解决 这 一 难题 本 项目 
应用 机器学习 logistic 回归模型 来 预测 乳腺癌 患者 复发 还是 
正常 有效 的 预测 出 医学 难题 项目 十一 葡萄酒 
质量 检测 系统 随着 信息 科技 的 快速 发展 计算机中 
的 经典 算法 在 葡萄酒 产业 中 得到 了 广泛 
的 研究 与 应用 其中 机器学习 算法 的 特点 是 
运用 了 人工智能 技术 在/p 大量/n 的/uj 样本/n 集/q 训练/vn 
和/c 学习/v 后/f 可以/c 自动/vn 地/uv 找出/v 运算/vn 所/c 需要/v 
的/uj 参数/n 和/c 模型/n 项目 十二 淘宝网 购物篮 分析 推荐算法 
购物篮 分析 Market Basket Analysis 即 非常 有名 的 啤酒 
尿布 故事 的 一个 反应 是 通过 对 购物篮 中 
的 商品 信息 进行 分析 研究 得出 顾客 的 购买 
行为 主要 目的 是 找出 什么样 的 物品 会 经常 
出现 在 一起 也 就是 那些 商品 之间 是 有 
很大 的 关联性 的 通过 购物篮 分析 挖掘 出来 的 
信息 可以 用于 指导 交叉 销售 追加 销售 商品 促销 
顾客 忠诚度 管理 库存 管理 和 折扣 计划 等 业务 
购物篮 分析 的 最 常用 应用 场景 是 电商 行业 
但 除此之外 该 算法 还被 应用于 信用卡 商城 电信 与 
金融 服务 业 保险业 以及 医疗 行业 等 项目 十三 
手工 实现 梯度 下降 回归 算法 梯度 下 降法 英语 
Gradient descent 是 一个 一 阶 最优化 算法 通常 也 
称为 最速 下 降法 要 使用 梯度 下 降法 找到 
一个 函数 的 局部 极小值 必须 向 函数 上当 前点 
对应 梯度 或者 是 近似 梯度 的 反方向 的 规定 
步 长距离 点 进行 迭代 搜索 如果 相反地 向 梯度 
正方向 迭代 进行 搜索 则会 接近 函数 的 局部 极大值 
点 这个 过程 则 被称为 梯度 上升 法 项目 十四 
基于 TensorFlow 实现 回归 算法 回归 算法 是 业界 比较 
常用 的 一种 机器学习 算法 通过 应用 于 各种 不同 
的 业务 场景 是 一种 成熟 而 稳定 的 算法 
种类 TensorFlow 是 一种 常 用于 深度 学习 相关 领域 
的 算法 工具 随着 深度 学习 热度 的 高涨 TensorFlow 
的 使用 也 会 越来越 多 从而 使用 TensorFlow 来 
实现 一个 不 存在 的 算法 会 加深 对 TensorFlow 
的 理解 和 使用 基于 TensorFlow 的 回归 算法 的 
实现 有助于 后续 的 TensorFlow 框架 的 理解 和 应用 
并 可以 促进 深度 学习 相关 知识 的 掌握 项目 
十五 合理 用药 系统 合理 用药 系统 是 根据 临床 
合理 用药 专业 工作 的 基本 特点 和 要求 运用 
NLP 和 深度 学习 技术 对 药品 说明书 临床 路径 
等 医学 知识 进行 标准化 结构化 处理 如 自动 提取 
药品 说明书 文本 里面 的 关键 信息 如 药品 相互作用 
禁忌 用法 用量 适用人群 等 实现 医嘱 自动 审查 及时 
发现 不 合理 用药 问题 帮助 医生 药师 等 临床 
专业 人员 在 用药 过程 中 及时 有效 地 掌握 
和 利用 医药 知识 预防 药物 不良 事件 的 发生 
促进 临床 合理 用药 工作 项目 十六 行人 检测 行人 
检测 是 利用 图像 处理 技术 和 深度 学习 技术 
对 图像 或者 视频 序列 中 是否 存在 行人 并 
给予 精确定位 学习 完 行人 检测 技术 后 对 类似 
的 工业 缺陷 检测 外观 检测 和 医疗 影像 检测 
等 目标 检测 范畴 类 的 项目 可以 一通百通 该 
技术 可 与 行人 跟踪 行人 重 识别 等 技术 
结合 应用于 人工 智能系统 车辆 辅助 驾驶 系统 智能 机器人 
智能 视频 监控 人体 行为 分析 智能 交通 等 领域 
由于 行人 兼具 刚性 和 柔性 物体 的 特性 外观 
易受 穿着 尺度 遮挡 姿态 和 视角 等 影响 使得 
行人 检测 成为 计算机 视觉 领域 中 一个 既 具有 
研究 价值 同时 又 极具 挑战性 的 热门 课题 项目 
十七 时间 序列 算法 模型 拿到 一个 观察 序列 后 
首先 要 对 它 的 平稳性 和纯/nr 随机性 进行 检验 
这 两个 重要 的 检验 称为 序列 的 预处理 根据 
检验 的 结果 可以 将 序列 分为 不同 的 类型 
对 不同 的 类型 我 们 采用 不同 的 分析 
方法 1 移动 平均法 MA 2 自 回归模型 AR AR 
模型 是 一种 线性 预测 即 已知 N 个 数据 
可由 模型 推出 第 N 点 前面 或 后面 的 
数据 设 推出 P 点 本质 类似于 插值 其 目的 
都是/nr 为了 增加 有效 数据 只是 AR 模型 是由 N 
点 递推 而 插值 是由 两点 或 少数 几点 去 
推导 多点 所以 AR 模型 要比 插值 方法 效果 更好 
3 自 回归 滑动 平均 模型 ARMA 其 建模 思想 
可 概括 为 逐渐增加 模型 的 阶数 拟合 较 高阶 
模型 直到 再 增加 模型 的 阶数 而 剩余 残差 
方差 不再 显著 减小 为止 4 指数 平滑 法 移动 
平均法 的 预测 值 实质上 是 以前 观测值 的 加权 
和 且 对 不同 时期 的 数据 给予 相同 的 
加权 这 往往 不 符合 实际 情况 指数 平滑 法则 
对 移动 平均法 进行 了 改进 和 发展 其 应用 
较为 广泛 基本思想 都是 预测值 是 以前 观测值 的 加权 
和 且 对 不同 的 数据 给予 不同 的 权 
新 数据 给 较大 的 权 旧 数据 给 较小 
的 权 根据 平滑 次数 不同 指数 平滑 法 分为 
一次 指数 平滑 法 二次/m 指数/n 平滑/a 法和/nr 三次/m 指数/n 
平滑/a 法等/nr 项目/n 十八/m PySpark 大 数据 机器学习 框架 Spark 
由 AMPLab 实验室 开发 其 本质 是 基于 内存 的 
快速 迭代 框架 迭代 是 机器学习 最大 的 特点 因此 
非常 适合 做 机器学习 得益于 在 数据 科学 中 强大 
的 表现 Python 是 一种 解释 型 面向对象 动态数据 类型 
的 高级 程序 设计 语言 结合 强大 的 分布式 内存 
计算 框架 Spark 两个 领域 的 强者 走到 一起 自然 
能碰出/nr 更加 强大 的 火花 Spark 可以 翻 译为 火花 
Spark 的 Python API 几乎 覆盖 了 所有 Scala API 
所能 提供 的 功能 只有 极少数 的 一些 特性 和 
个别 的 API 方法 暂时 还 不支持 但 通常 不 
影响 我们 使用 Spark Python 进行编程 项目 十九 天池 kaggle 
比赛 2014年 3月 阿里 巴巴 集团 董事局 主席 马云 在 
北京大学 发起 天池 大 数据 竞赛 首届 大赛 共有 来自 
全球 的 7276支 队伍 参赛 海外 参赛 队伍 超过 148支 
阿里 巴巴 集团 为此 开 放了 5.7亿 条 经过 严格 
脱敏 处理 的 数据 2014年 赛季 的 数据 提供 方为 
贵阳市政府 参赛者 根据 交通 数据 模拟 控制 红绿灯 时间 寻找 
减轻 道路 拥堵 的 方法 Kaggle 是 一个 数据 分析 
的 竞赛 平台 网址 https / / www . kaggle 
. com / 企业 或者 研究 者 可以 将 数据 
问题 描述 期望 的 指标 发布 到 Kaggle 上 以 
竞赛 的 形式 向 广大 的 数据 科学家 征集 解决 
方 案 类似于 KDD CUP 国际知识 发现 和 数据挖掘 竞赛 
Kaggle 上 的 参赛者 将 数据 下载 下来 分析 数据 
然后 运用 机 器 学习 数据挖掘 等 知识 建立 算法 
模型 解决问题 得出 结果 最后 将 结果 提交 如果 提交 
的 结果 符合 指标 要求 并且在 参赛者 中 排名 第一 
将 获得 比赛 丰厚 的 奖金 项目 二十 量化 交易 
量化 交易 Quantitative Trading 是 指 借助 现代 统计学 和 
数学 的 方法 利用 计算机 技术 来 进行 交易 的 
证券 投资 方式 量化/v 交易/n 从/p 庞大/a 的/uj 历史/n 数据/n 
中/f 海选/n 能/v 带来/v 超额/b 收益/n 的/uj 多种/m 大 概率 
事件 以 制定 策略 用 数量 模型 验证 及 固化 
这些 规律 和 策略 然后 严格 执行 已 固化 的 
策略 来 指导 投资 以求 获得 可以 持续 的 稳定 
且 高于 平均 收益 的 超额 回报 量化 交易 起源 
于 上世纪 七十 年代 的 股票 市场 之后 迅速 发展 
和 普及 尤其 是 在 期货 交易 市场 程序化 逐渐 
成为 主流 有 数据 显示 国外 成熟 市场 期货 程序化 
交易 已 占据 总 交易量 的 70% 80% 而 国内 
则 刚刚 起步 手工 交易 中 交易者 的 情绪 波动 
等 弊端 越来越 成为 盈利 的 障碍 而 程序化 交易 
天然 而成 的 精准性 100% 执行率 则为 它 的 盈利 
带来 了 优势 阶段 九 百度 云 实战 体系 课程 
一 深入 理解 百度 云计算 基础 产品 / 基于 百度 
云 弹性 计算 服务 实现 基础架构 解决方案 全面 介绍 BCC 
CDS EIP BLB RDS BOS VPC 等 百度 云 弹性 
计算 服务 介绍 百度 云的/nr 安全 防护 方案 深入 介绍 
传统 架构 下 如何 通过 百度 云 弹性 计算 服务 
快速 构建 更 稳定 安全 的 应用 认证 培训 专家 
将 通过 深入浅出 理论/n 和/c 实践/v 相/v 结合/v 的/uj 课程/n 
帮助/v 学员/n 深入/v 掌握/v 百度/n 云/ns 弹性/n 计算/v 服务/vn 1 
快速 体验 百度 云 服务器 BCC 的 功能 全貌 2 
基于 BCC 的 云 磁盘 CDS 的 操作 与 管理 
3 基于 BCC 的 磁盘 快照 自定义 镜像 的 操作 
与 管理 4 基于 自定义 镜像 快速 生成 BCC 的 
实验 5 基于 磁盘 快照 实现 数据备份 与 恢复 的 
最佳 实践 6 基于 百度 云安全 组 完成 定义 IP 
＋ 端口 的 入站 和 出站 访问 策略 7 快速 
体验 百度 云 私有 网络 VPC 的 功能 全貌 8 
基于 百度 云 VPC + VPN 快速 搭建 Stie to 
Stie 的 混合 云 架构 9 在 百度 云 VPC 
网络 下 实现 NAT 地址映射 的 实践 10 快速 体验 
百度 云数据库 RDS 的 功能 全貌 11 云数据库 RDS 的 
备份 与 恢复 操作 体验 12 熟悉 数据 传输服务 DTS 
的 使用 13 快速 体验 百度 云 负载 均衡 BLB 
的 功能 全貌 14 快速 体验 百度 云 存储 BOS 
的 功能 全貌 15 快速 体验 百度 云数据库 RDS 的 
功能 全貌 16 快速 体验 百度 云 内容 分发 网络 
CDN17 基于 BLB BCC RDS BOS 和 CDN 快速 部署 
Discuz 论坛 实现 弹性 架构 综合 实验 18 快速 体验 
百度 云安全 BSS 和 DDOS 防护 服务 19 快速 体验 
百度 云 监控 BCM 课程 二 基于/p 百度/n 云的/nr 迁移/v 
上/f 云/ns 实战/v 基于/p 百度/n 云/ns 弹性/n 计算/v 服务/vn 的/uj 
基础/n 产品/n 实现 传统 IT 架构 迁移 到 百度 云上的/nr 
实战 为 客户 业务 上 云 提升 能力 提升 客户 
上 云前的/nr 信心 上/f 云/ns 中和/ns 上/f 云后的/nr 技术/n 能力/n 
以 真实 的 客户 案例 结合 设计 好 的 动手 
实验课 提升 实战经验 介绍 了 业务 上 云的/nr 过程 方法 
工具 以及 案例 等 1 基于 BCC 快速 部署 LNMP 
基础 环境 2 基于 BCC 快速 部署 LAMP 基础 环境 
3 基于 BCC 快速 部署 MySQL 数据库 4 基于 BCC 
快速 部署 MS SQL 数据库 服务 5 基于 BCC 快速 
部署 Tomcat 基础 环境 6 云数据库 RDS 结合 数据 传输服务 
DTS 实现 数据 迁 移上 云的/nr 最佳 实践 7 基于 
BOS 桌面 实现 BOS 的 可视化 管理 8 基于 BOS 
FS 实现 BOS 服务 挂载 到 本地 文件系统 9 基于 
BOS Util 实现 BOS 的 批量 文件 操作 的 演示 
10 基于 BOS CLI 实现 BOS 文件 的 单机 操作 
课程 三 在/p 百度/n 云/ns 平台/n 上/f 进行/v 开发/v 全面/n 
介绍/v 使用/v 百度/n 云/ns 产品/n 进行/v 应用/v 开发/v 理解 百度 
云 主要 产品 特性 包括 BCC BOS RDS SCS 在 
应用 开发 中 的 使用 结合/v 实际/n 应用/v 开发/v 案例/n 
全面/n 的/uj 介绍/v 整个/b 开发/v 流程/n 和/c 百度/n 云/ns 产品/n 
使用/v 方法/n 以/p 提升/v 学员/n 开发/v 技能/n 和/c 了解/v 百度/n 
云/ns 产品开发/l 特点/n 根据 一天 或者 两天 的 课程 提供 
多个 实际 动手 实验 认证 讲师 指导 实验 真正 做到 
学以致用 为 学员 实现 上 云 开发 保驾 护航 1 
基于 百度 云 OpenAPI 实现 简化版 控制台 的 综合 实验 
2 基于 百度 云 BOS OpenAPI 实现 简化版 的 百度 
网盘 课程 四 百度 云 天工 智能 物联网 与 天 
像 智能 多媒体 服务平台 介绍 与 案例 分析 百度 天工 
物 联 平台 是 一站式 全 托管 的 物联网 服务平台 
依托 百度 云 基础 产品 与 服务 提供 全栈/nr 物联网 
核心 服务 帮助 开发 者 快速 搭建 部署 物联网 应用 
通过 全面 介绍 天工 的 IoT Hub IoT Parser Rule 
Engine IoT Device BML BMR OCR 和 语音 识别 等 
产品 与 服务 解析 天工 典型 的 产品 架构 方案 
应用 到 工业 4.0 车联网 能源 物流 和 智能 硬件 
等 各行业 解决方案 1 基于 百度 云 LSS 快速 搭建 
音 视频 直播 平台 最佳 实践 2 基于 百度 云 
VOD 快速 搭建 音 视频点播 平台 最佳 实践 3 体验 
百度 云 音视频 转码 MCT 的 转码 计算 服务 4 
基于 百度 云 文档 服务 DOC 体验 文档 存储 转码 
分发 播放 一站式 服务 体验 5 基于 百度 云物/nr 接入 
IoT Hub 实现 智能 设备 与 百度 云端 之间 建立 
安全 的 双向 连接 6 体验 百度 云的物/nr 管理 IoT 
Device 端 到 端 配置 实践 课程 五 百度 云 
天智 人工智能 服务平台 介绍 与 实战 天智 是 基于 世界 
领先 的 百度 大脑 打造 的 人工智能 平台 提供 了 
语音 技术 文字 识别 人脸识别 深度 学习 和 自然 语言 
NLP 等 一系列 人工智能 产品 及 解决方案 帮助 各行各业 的 
客户 打造 智能化 业务 系统 本 课程 力求 对 百度 
人工智能 服务 平台 进行 整体 全面 的 介绍 包括 天智 
平台 与 解决 方案 介绍 主要 产品 百度 语音 人脸识别 
文字 识别 百度 深度 学习 百度 机器学习 BML 自然语言 NLP 
等 的 介绍 客户 案例 分享 等 1 百度 机器学习 
BML 广告 点击率 预估 2 百度 识别 文字 识别 3 
百度 识别 人脸识别 4 百度 自然语言 处理 短 文本 相似 
度 5 百度 语音 朗 读者 6 百度 深度 学习 
预测 用户 感兴趣 的 电影 阶段 十 人工智能 实战 － 
企业 项目 实战 课程 一 基于 Python 数据 分析 与 
机器学习 案例 实战 教程 课程 风格 通俗易懂 基于 真实 数据集 
案例 实战 主体 课程 分成 三 个大 模块 1 python 
数据分析 2 机器学习 经典 算法 原理 详解 3 十大 经典 
案例 实战 通过 python 数据 科学 库 numpy pandas matplot 
结合 机器学习 库 scikit learn 完成 一些 列 的 机器学习 
案例 算法 课程 注重 于 原理 推导 与 流程 解释 
结合 实例 通俗 讲解 复杂 的 机器学习 算法 并以 实战 
为主 所有 课时 都 结合 代码 演示 算法 与 项目 
相 结合 选择 经典 kaggle 项目 从 数据 预处理 开始 
一步步 代码 实战 带 大家 快速 入门 机器学习 旨在 帮助 
同学 们 快速 上手 如何 使用 python 库 来 完整 
机器学习 案例 选择 经典案例 基于 真实 数据集 从 数据 预处理 
开始 到 建立 机器学习 模型 以及 效果 评估 完整 的 
讲解 如何 使用 python 及其 常用 库 进行 数据 的 
分析 和 模型 的 建立 对于 每 一个 面对 的 
挑战 分析 解决 问题 思路 以及 如何 构造 合适 的 
模型 并且 给出 合适 评估 方法 在 每 一个 案例 
中 同学 们 可以 快速 掌握 如何 使用 pandas 进行 
数据 的 预处理 和 分析 使用 matplotlib 进行 可视化 的 
展示 以及 基于 scikit learn 库 的 机器学习 模型 的 
建立 1 Python 数据 分析 与 机器学习 实战 课程 简介 
2 Python 快速 入门 3 Python 科学计算 库 Numpy4 Python 
数据 分析 处理 库 Pandas5 Python 可视化 库 Matplotlib6 回归 
算法 7 模型 评估 8 K 近邻 算法 9 决策树 
与 随机 森林 算法 10 支持 向量 机 11 贝叶斯 
算法 12 神经网络 13 Adaboost 算法 14 SVD 与 推荐 
15 聚 类 算法 16 案例 实战 使用 Python 库 
分析处理 Kobe Bryan 职业生涯 数据 17 案例 实战 信用卡 欺诈 
行为 检测 18 案例 实战 泰坦尼克号 获救 预测 19 案例 
实战 鸢尾花 数据集 分析 20 案例 实战 级联 结构 的 
机器学习 模型 21 案例 实战 员工 离职 预测 22 案例 
实战 使用 神经 网络 进行 手写 字体 识别 23 案例 
实战 主 成分 分析 24 案例 实战 基于 NLP 的 
股价 预测 25 案例 实战 借贷 公司 数据 分析 课程 
二 人工智能 与 深度 学习 实战 课程 风格 通俗易懂 必备 
原理 形象 解读 项目 实战 缺一不可 主体 课程 分成 四 
个大 模块 1 神经网络 必备 基础 知识 点 2 深度 
学习 模型 3 深度 学习 框架 Caffe 与 Tensorflow 4 
深度 学习 项目 实战 课程 首先 概述 讲解 深度 学习 
应用 与 挑战 由 计算机 视觉 中 图像 分类 任务 
开始 讲解 深度 学习 的 常规 套路 对于 复杂 的 
神经 网络 将其 展 开成 多个 小 模块 进行 逐一 
攻破 再 挑战 整体 神经 网络 架构 对于 深度 学习 
模型 形象 解读 卷积 神经网络 原理 详解 其中 涉及 的 
每一个 参数 对 卷积 网络 架构 展开 分析 与 评估 
对于 现阶段 火爆 的 对抗 生成 网络 以及 强化 学习 
给出 形象 解读 并 配合 项目 实战 实际 演示 效果 
基于 框架 实战 选择 两款 深度 学习 最 火 框架 
Caffe 与 Tensorflow 首先 讲解 其 基本 使用 方法 并 
结合 案例 演示 如何 应用 框架 构造 神经网络 模型 并 
完成 案例 任务 选择 经典 深度 学习 项目 实战 使用 
深度 学习 框架 从零开始 完成 人脸 检测 验证码 识别 人脸 
关键点 定位 垃圾邮件 分类 图像 风格 转换 AI 自己 玩 
游戏 等 对于 每 一个 项目 实战 从 数据 预处理 
开始 一步步 构建 网络 模型 并 展开 分析 与 评估 
课程 提供 所 涉及 的 所有 数据 代码 以及 PPT 
方便 大家 快速 动手 进行 项目 实践 1 深度 学习 
概述 与 挑战 2 图像 分类 基本原理 门 3 深度 
学习 必备 基础 知识 点 4 神经网络 反向 传播 原理 
5 神经网络 整体 架构 6 神经网络 案例 实战 图像 分类 
任务 7 卷积 神经 网络 基本 原理 8 卷积 参数 
详解 9 卷积 神经网络 案例 实战 10 经典 网络 架构 
分析 11 分类 与 回归 任务 12 三代 物 体检 
测算 法分析 13 数据 增强 策略 14 T r a 
n s f e r L e a r n 
i n g 1 5 网络 架构 设计 16 深度 
学习 框架 Caffe 网络结构 配置 17 Caffe18 深度 学习 项目 
实战 人脸 检测 19 人脸 正负 样本 数据源 制作 20 
人脸 检测 网络 架构 配置 习 模型 21 人脸 检测 
代码 实战 22 人脸 关键点 定位 项目 实战 23 人脸 
关键点 定位 网络 模型 24 人脸 关键点 定位 构建 级 
联网络 25 人脸 关键点 定位 测试 效果 与 分析 26 
Tensorflow 框架 实战 27 Tensorflow 构建 回归模型 28 Tensorflow 构建 
神经网络 模型 29 Tensorflow 深度 学习 模型 30 Tensorflow 打造 
RNN 网络 模型 31 Tensorflow 项目 实战 验证 识别 32 
项目 实战 图像 风格 转换 33 QLearning 算法 原理 34 
DQN 网络 架构 35 项目 实战 DQN 网络 让 AI 
自己 玩 游戏 36 项目 实战 对抗 生成 网络 等 
项目 一 AI 大 数据 互联网 电影 智能 推荐 第一季 
随着 科技 的 发展 现在 视频 的 来源 和 类型 
多样性 互联网 视频 内容 充斥 着 整个 网络 如果 仅仅 
是 通过 翻页 的 方法 来 寻找 自己 想看 的 
视频 必然 会 感到 疲劳 现在 急需 一种 能 智能 
推荐 的 工具 推荐 系统 通过 分析 用户 对 视频 
的 评分 分析 对 用户 的 兴趣 进行 建模 从而 
预测 用户 的 兴趣 并 给 用户 进行 推荐 Python 
是 一种 面向 对象 的 解释 型 计算机程序 设计 语言 
Python 具有 丰富 和 强大 的 库 它 常被 昵称 
为 胶水 语言 而 大 数据 是 指 无法 在 
一定 时间 范围 内 用 常规 软件工具 进行 捕捉 管理 
和 处理 的 数据 集合 企业 面临 海量 数据 的 
到来 大多 选择 把 数据 从 本地 迁移 至 云端 
云端 将 成为 最大 的 非 结构化 数据 存储 场所 
本 项目 主要 以 客户 咨询 为 载体 分析 客户 
的 群体 分布 旨在 挖掘 客户 的 内在 需求 帮助 
企业 实现 更 有价值 的 营销 一 教务 管理 系统 
业务 介绍 1 教务 管理 系统 框架 讲解 2 系统 
业务 逻辑 介绍 二 大 数据 需求分析 1 明确 数据 
需求 2 大 数据分析 过程 3 分析 难点 和 解决 
方案 4 大 数据 相关 技术 选型 三 构建 分布式 
大 数据 框架 1 Hadoop 分布式 集群 配置 2 ZooKeeper 
高 可用 3 SQOOP 数据 转移 4 ETL 数据 清洗 
5 HIVE 数据分析 6 HBase 数据 存储 四 基于 教务 
管理 系统 大 数据分析 1 业务 数据 分析 指标 设定 
2 操作 MapReduce 分而治之 3 使用 Hive 进行 数据 整合 
抽离 4 使用 HBase 存储 非 结构 话 数据 五 
大 数据 可视化 1 可视化 技术 选型 2 Echarts 代码 
展示 炫 酷 视图 3 使用 Tableau 进行 数据 可视化 
展示 项目 二 电商 大 数据 情感 分析 与 AI 
推断 实战 项目 第一季 本 项目 从 开发 的 角度 
以大 数据 PHP 技术 栈 为基础 使用 真实 商用 表 
结构 和 脱敏 数据 分三步 构建 商用 系统 真实 大 
数据 环境 进行 推断 分析 以及 呈现 结果 项目 课程 
的 完整性 商业性 可以 使 学者 尽可能 完整 地 体会 
真实 的 商业 需求 和 业务 逻辑 完整 的 项目 
过程 使 PHP 技术 栈 的 同学 得以 窥见 和 
学到 一个 完整 商业 平台 项目 的 搭建 方法 真实 
大 数据 环境 的 搭建 使 呈现 建立 大 数据 
的 工具 应用 技术 概念 储备 基于 大 数据 平台 
的 分析 需求 的 实现 呈现 将 完整 的 一次 
大 数据 技术 栈 到 分析 结果 的 中线 平铺 
直述 为 想要 学习 大 数据 并 有 开发 基础 
的 同学 点亮 新 的 能力 一 实践 项目 研发 
1 开发 环境 的 安装 配置 2 表 与 数据 
3 LARAVEL 的 快速 开发 实践 4 批量 创建 模型 
5 万能 控制器 与 表 配置 6 统一 视图 的 
创建 二 数据分析 需求 设立 1 定义数据 需求 2 分析 
计算 过程 3 分析 难点 和 解决 方案 4 大 
数据 技术 选型 三 大 数据 平台 搭建 1 分布式 
环境 的 模拟 建立 2 网络 环境 的 调 通3/nr 
身份验证 与 集群 控制 4 Hadoop 环境 搭建 和 要点 
说明 5 MapReduce 与 Yarn 的 搭建 和 说明 四 
大 数据分析 脚本 编写 1 MapReduce 脚本 编写 2 拆解 
数据 需求 3 Map 逻辑 详写 4 Reduce 逻辑 详写 
5 结果 整理 与 输出 五 结果 可视化 1 可视化 
需求 和 技术 选型 2 展示 页面 的 快速 铺设 
3 可视化 JS 上手 4 使用 可视化 JS 展示 结果 
项目 三 AI 法律咨询 大 数据 分析 与 服务 智能 
推荐 实战 项目 第一季 本 项目 结合 目前 流行 的 
大 数据 框架 在 原有 成熟 业务 的 前提 下 
进行 大 数据 分析 处理 真实 还原 企业应用 让 学员 
身临其境 的 感受 企业 大 数据 开发 的 整个 流程 
项目 的 业务 系统 底层 主要 采用 JAVA 架构 大 
数据 分析 主要 采用 Hadoop 框架 其中 包括 Kettle 实现 
ETL SQOOP Hive Kibana HBASE Spark 以及 人工 智能算法 等 
框架 技术 采用 真实 大 数据 集群 环境 的 搭建 
让 学员 切身感受 企业 项目 的 从0到/nr 1 的 过程 
一 系统 业务 介绍 1 底层 业务 实现 框架 讲解 
2 功能模块 讲解 二 系统 架构设计 1 总体 架构 分析 
2 数据 流向 3 各 技术 选型 承载 作用 4 
部署 方案 三 详尽 实现 1 原始 数据处理 2 ETL 
数据 导入 3 MR 数据 计算 4 Hive 数据分析 四 
数据 可视化 1 采用 Highcharts 插件 展示 客户 偏好 曲线图 
2 使用 Tableau 进行 数据 分析 可视化 展示 五 项目 
优化 1 ZooKeeper 实现 HA2 集群 监控 的 整体 联调 
项目 四 AI 大 数据 基站定位 智能 推荐 商圈分析 项目 
实战 第一季 随着 当今 个人 手机 终端 的 普及 出行/v 
人群/n 中/f 手机/n 拥有率/n 和/c 使用率/n 已/d 达到/v 相当/d 高的/nr 
比例/n 根据 手机 信号 在 真实 地理 空间 的 覆盖 
情况 将 手机 用户 时间 序列 的 手机 定位 数据 
映射 至 现实 地理位置 空间 位置 即可 完整 客观 地 
还原 出 手机 用户 的 现实 活动 轨迹 从而 挖掘出 
人口 空间 分布 与 活动 联系 特征 信息 商圈 是 
现代 市场 中 企业 市场 活动 的 空间 同时 也 
是 商品 和 服务 享用者 的 区域 商圈 划分 为 
目的 之一 是 研究 潜在 顾客 分布 以 制定 适宜 
的 商业 对策 本 项目 以 实战 为 基础 结合 
大 数据 技术 Hadoop . Net 技术 全栈/nr 为基础 采用 
真实 商业 数据 分 不同 环节 构建 商用 系统 真实 
大 数据 环境 进行 推断 分析 及 呈现 数据 一 
分析 系统 业务 逻辑 讲解 1 大 数据 基站定位 智能 
推荐 商圈分析 系统 介绍 2 数据 前期 清洗 和 数据 
分析 目标 指标 的 设定 等 二 大 数据 导入 
与 存储 1 关系型 数据库 基础知识 2 hive 的 基本 
语法 3 hive 的 架构 及 设计 原理 4 hive 
安装 部署 与 案例 等 5 Sqoop 安装 及 使用 
6 Sqoop 与 关系型 数据库 进行 交互 等 7 动手 
实践 三 Hbase 理论 及 实战 1 Hbase 简介 安装 
及 配置 2 Hbase 的 数据 存储 与 数据模型 3 
Hbase Shell4 Hbase 访问 接口 5 Hbase 数据备份 与 恢复 
方法 等 6 动手 实践 数据 转储 与 备份 四 
基站 数据 分析 与 统计 推断 1 背景 与 分析 
推断 目标 2 分析 方法 与 过程 推断 3 动手 
实践 分析 既定 指标 数据 五 数据 分析 与 统计 
推断 结果 的 展示 大 数据 可视化 1 使用 Tableau 
展示 数据 分析 结果 2 使用 HighCharts ECharts 展示 数据 
分析 结果 阶段 十一 区块 链 区块 链 Blockchain 是 
分布式 数据 存储 点对点 传输 共识 机制 加密算法 等 计算机 
技术 的 新型 应用 模式 所谓 共识 机制 是 区块 
链 系统 中 实现 不同 节点 之间 建立 信任 获取 
权益 的 数学 算法 区块 链 是 比特币 的 底层 
技术 像 一个 数据库 账本 记载 所有 的 交易 记录 
这项 技术 也 因其 安全 便捷 的 特性 逐渐 得到 
了 银行 与 金融业 的 关注 一 课程 介绍 1 
区块 链 的 发展 2 课程 安排 3 学习 目标 
二 区块 链 的 技术 架构 1 数据 层 创世 
区块 交易 记录 私钥 公钥 和 钱包 地址 2 数据 
层 & 通讯 层 记账 原理 Merkle 树 和 简单 
支付 验证 SPV P2P 通讯 数据 通信 和 验证 3 
共识 层 4 激励 层 拜占庭 将军 问题 与 POW 
Pos DPos PBFT 挖矿 交易 费 图灵 完备 和非/nr 完备 
5 合约 层 比特币 脚本 以太 坊 智能 合约 fabic 
智能 合约 RPC 远程 调用 6 应用层 7 总结 接口 
调用 DAPP 的 使用 应用 场景 的 部署 重要 概念 
和 原理 三 环境 搭建 1 以太 坊 以太 坊 
介绍 以太 坊 开发过程 图形界面 客户端 使用 供应链 的 应用 
保险 领域 的 应用 DAO 的 介绍 和 应用 2 
以太 坊 以太 坊本 地 开发 环境 的 搭建 以太 
坊 分布式 集群 环境 的 搭建 3 hyperledger 项目 fabric 
介 fabric 介绍 fabric 本地 开发环境 搭建 fabric 分布式 集群 
环境 搭建 四 案例 和 DEMO1 案例 讲解 支付 和清/nr 
结算 公益 行业 的 应用 供应链 的 应用 保险 领域 
的 应用 DAO 的 介绍 和 应用 2 Demo 介绍 
发币 和 交易 Demo3 Demo 介绍 数据 资产 的 确权 
和 追溯 阶段 十二 用 人工智能 预测 金融 量化 交易 
投资 系列 课程 程序化 交易 又称 程式 交易 发源 于 
上世纪 80 年代 的 美国 其 最初 的 定义 是 
指在 纽约股票交易所 NYSE 市场 上 同时 买 卖超过 15只 以上 
的 股票 组合 像 高盛 摩根士丹利 及 德意志银行 都是 在各 
大 交易 市场 程序化 交易 的 最 活跃 参与 会员 
本 课程 主要 面向 意愿 从事 金融 量化 交易 人员 
金融 行业 从业 人员 金融 策略 开发 人员 及 投资 
经验 丰富 而 想 实现 计算机 自动 下单 人员 主要 
讲解 了 证券 期货 程序化 实现 原理 及 过程 通过 
本 课程 的 学习 您 可以 根据 自己 的 意愿 
打造 属于 自己 的 量化 投资 交易 系统 本 课程 
主要 用到 的 技术 手段 有 Python Pandas 数据分析 数据挖掘 
机器学习 等 一 程序化 交易 数据 获取 与 清洗 讲解 
1 数据 的 清洗 与 合成 2 K 线图 绘制 
3 技术指标 开发 讲解 4 数据 的 获取 二 回 
测 框架 搭建 讲解 1 回 测 框架 搭建 背景 
及 基本 流程 讲解 2 回 测 框架 实现 及 
收益 指标 讲解 三 程序化 交易 部分 实现 讲解 1 
CTP 技术 讲解 2 程序化 API 讲解 3 程序化 交易 
具体 实现 讲解 阶段 十三 阿里云 认证 课程 一 云计算 
网站 建设 部署 与 发布 阿里云 网站 建设 认证 课程 
教 你 如何 掌握 将 一个 本地 已经 设 计好 
的 静态 网站 发布 到 Internet 公共 互联网 绑定 域名 
完成 工信部 的 ICP 备案 课程 二 云计算 网站 建设 
简单 动态 网站 搭建 阿里云 简单 动态 网站 搭建 课程 
教 你 掌握 如何 快速 搭建 一个 WordPress 动态 网站 
并会 对 网站 进行 个性化 定制 以 满足 不同 的 
场景 需求 课程 三 云计算 云/ns 服务器/n 管理/vn 维护/v 阿里云/i 
服务器/n 运维/i 管理/vn 课程/n 教/v 你/r 掌握/v 快速/d 开通/v 一台/m 
云/ns 服务器/n 并 通过 管理 控制台 方便 地 进行 服务器 
的 管理 服务器 配置 的 变更 和 升级 数据 的 
备份 并 保证 其 可以 正常 运转 并按 业务 需求 
随时 进行 配置 的 变更 课程 四 云计算 云数据库 管理 
与 数据 迁移 阿里云 云数据库 管理 与 数据 迁移 认证 
课程 掌握 云数据库 的 概念 如何 在云端 创建 数据库 将 
自建 数据库 迁移 至 云数据库 MySQL 版 数据 导入 导出 
以及 云数据库 运维 的 常用 操作 课程 五 云计算 云 
存储 对象 存储管理 与 安全 阿里 云云 储存 认证 课程 
教 你 掌握 安全 高/a 可靠/v 的/uj 云/ns 存储/l 的/uj 
使用/v 以及 在云端 存储 下载 文件 处理 图片 以及 如何 
保护 数据 的 安全 课程 六 云计算 超大 流量 网站 
的 负载 均衡 掌握 如何 为 网站 实现 负载 均衡 
以/p 轻松/a 应对/v 超大/v 流量/n 和高/nr 负载/v 课程 七 大 
数据 MOOC 网站 日志 分析 本 课程 可以 帮助 学员 
掌握 如何 收集 用户 访问 日志 如何 对 访问 日志 
进行 分析 如何 利用 大 数据 计算 服务 对 数据 
进行 处理 如何 以 图表 化 的 形式 展示 分析 
后的/nr 数据 课程 八 大 数据 搭建 企业级 数据分析 平台 
模拟 电商 场景 搭建 企业级 的 数据分析 平台 用来 分析 
商品 数据 销售 数据 以及 用户 行为 等 课程 九 
大 数据 基于 LBS 的 热点 店铺 搜索 本 课程 
可以 帮助 学员 掌握 如何 在 分布式计算 框架 下 开发 
一个 类似于 手机地图 查找 周边 热点 POI 的 功能 掌握 
GeoHash 编码 原理 以及 在 地理位置 中的 应用 并能 将其 
应用 在 其他 基于 LBS 的 定位 场景 中 课程 
中 完整 的 演示 了 整个 开发 步骤 学员 在学 
完 此 课程 之后 掌握 其 原理 可以 在 各种 
分布式计算 框架 下 完成 此 功能 的 开发 比如 MapReduce 
Spark 课程 十 大 数据 基于 机器学习 PAI 实现 精细化 
营销 本 课程 通过 一个 简单 案例 了解 掌握 企业 
营销 中 常见 的 也是 必需 的 精准 营销 数据 
处理 过程 了解 机器学习 PAI 的 具体 应用 指导 学员 
掌握 大 数据 时代 营销 的 利器 通过 机器学习 实现 
营销 课程 十一 大 数据 基于 机器 学习 的 客户 
流失 预警 分析 本 课程 讲解 了 客户 流失 的 
分析 方法 流程 同时 详细 介绍 了 机器 学习 中 
常用 的 分类 算法 集成 学习 模型 等 通用 技能 
并 使用 阿里云 机器学习 PAI 实现 流失 预警 分析 可以 
帮助 企业 快速 准确 识别 流失 客户 辅助 制定 策略 
进行 客户 关怀 达到 挽留 客户 的 目的 课程 十二 
大 数据 使用 DataV 制作 实时 销售 数据 可视化 大 
屏 帮助 非 专业 工程师 通过 图形化 的 界面 轻松 
搭建 专业 水准 的 实时 可视化 数据 大 屏 以 
满足 业务 展示 业务 监控 风险 预警 等 多种 业务 
的 展示 需求 课程 十三 大 数据 使用 MaxCompute 进行 
数据 质量 核查 通过 本 案例 学员 可 了解 影响 
数据 质量 的 因素 出现 数据 质量 问题 的 类型 
掌握 通过 MaxCompute DateIDE 设计 数据 质量 监控 的 方法 
最终 独立 解决 常见 的 数据 质量 监控 需求 课程 
十四 大 数据 使用 Quick BI 制作 图形化 报表 阿里云 
Quick BI 制作 图形化 报表 认证 课程 教 你 掌握 
将 电商 运营 过程 中 的 数据 进行 图表 化 
展现 掌握 通过 Quick BI 将 数据 制作 成 各种 
图形化 报表 的 方法 同时 还 将 掌握 搭建 企业级 
报表 门户 的 方法 课程 十五 大 数据 使用时间 序列 
分解 模型 预测 商品 销量 使用时间 序列 分解 模型 预测 
商品 销量 教 你 掌握 商品 销量 预测 方法 时间 
序列 分解 以及 熟悉 相关 产品 的 操作 演示 和 
项目 介绍 课程 十六 云安全 云/ns 平台/n 使用/v 安全/an 阿里云/i 
云/ns 平台/n 使用/v 安全/an 认证/v 课程/n 教/v 你/r 了解/v 由/p 
传统/n IT/w 到/v 云计算/i 架构/n 的/uj 变迁/vn 过程/n 当前 信息 
安全 的 现状 和 形势 以及/c 在/p 云计算/i 时代不同/i 系统/n 
架构/n 中/f 应该/v 从/p 哪些/r 方面/n 利用/n 云/ns 平台/n 的/uj 
优势/n 使用/v 安全/an 风险/n 快速/d 降低/v 90%/mf 课程 十七 云安全 
云上/nr 服务 器 安全 阿里 云云 上 服务 器 安全 
认证 课程 教 你 了解 在 互联 网上 提供 计算 
功能 的 服务器 主要 面临 哪些 安全 风险 并 针对 
这些 风险 提供 了 切实可行 的 免费 的 防护 方案 
课程 十八 云安全 云上/nr 网络/n 安全/an 了解/v 网络/n 安全/an 的/uj 
原理/n 和/c 解决/v 办法/n 以及 应对 DDoS 攻击 的 方法 
和 防护 措施 确保 云上/nr 网络 的 安全 课程 十九 
云安全 云上/nr 数据安全/n 了解/v 云上/nr 数据/n 的/uj 安全/an 隐患/n 掌握 
数据备份 数据 加密 数据传输 安全 的 解决 方法 课程 二十 
云安全 云上/nr 应用 安全 了解 常见 的 应用 安全 风险 
SQL 注入 原理 及 防护 网站 防 篡改 的 解决 
方案 等 确保 云上/nr 应用 的 安全 课程 二十一 云安全 
云上/nr 安全/an 管理/vn 了解/v 云上的/nr 安全/an 监控/vn 方法/n 学会 使用 
监控 大 屏 来 监控 安全 风险 并能够 自定义 报警 
规则 确保 随时 掌握 云上/nr 应用 的 安全 情况 阶段 
十四 IT 高级 开发者 职场 生存 规则 － 职业 素养 
本 课程 主要 为 广大 毕业生 或者 工作 经验 较少 
的 学员 而 设立 主要 是 为了 在 职业 素养 
方面 给 大家 提供 辅导 为 更加 顺利 走向 职场 
而 提供 帮助 为什么 有些 同学 在 技能 方面 过关 
却 还是 给予 别人 一种 书生气 的 感觉 为什么 简历 
已经 通过 了 却 还是 没有 通过 HR 的 面试 
为什么 入 职后 与 同事 的 沟通 总是 存在 问题 
为什么 每天 的 时间 都 不够 用 无法 兼顾 生活 
学习 和 工作 为什么 学习 一段 时间 后 对 工作 
对 职场 没有 方向感 为什么 遇到 事情 别人 总是 能够 
保持 良好 心态 游刃有余 而 我 总是 问题 百出 COT 
课程 正是 引领 大家 一起 来 探索 其中 的 奥秘 
和 方法 让 大家 一起 在 学习 过程 中 不断 
深思 和 进步 让/v 大家/n 的/uj 职场/n 路/n 越走越/nr 顺畅/a 
1 团队 协作 2 心态 管理 3 目标 管理 4 
时间管理 前言 TensorFlow 官网 教程 Vector Representation of words 主要 
是 介绍 了 谷歌 2013 开源 的 word2vec 工具包 中 
的 两个 模型 CBOW Continuous Bag of words Model 和 
Skip gram 模型 Continuous Skip gram Model 在 讲 具体 
的 模型 之前 先要 介绍 一点 自然语言 处理 的 基础 
知识 什么 是 语言 模型 通俗 的 讲 语言 模型 
就 是 某种 自然 语言 的 模型 它 用来 判断 
给定 的 语句 是否 属于 该 自然语言 范畴 即 判断 
语句 是否 符合 该 自然语言 所属 的 语义 语法 和 
逻辑 那 放到 机器学习 里 该 怎样 做 呢 首先 
进行 一些 定义 操作 假设 某 自然 语言 为 Ω 
给定 一个 属于 该 自然 语言 的 文本 数据集 记为 
T T 就是 自然语言 Ω 的 样本 集 然后 我们 
把 文本 按 单词 拆解 拆解 后 所有 单词 组成 
的 集合 叫做 语料库 记为 C 再把 语料库 中的 单词 
去 重复 称为 词典 记为 D 那 我们 来 简单 
算 一下 我们 事 先要 统计 多少 个 值 但是 
这类 算法 有个 问题 就是 避免 不 了 还是 要 
提前 反复 扫描 文本 并且 存储 统计 值 而且 对于 
样 本集 没有 遇见 的 情况 统计 的 概率 为 
0 因此 对 没有 遇到 的 情况 泛化 能力 很差 
什么 是 神经 概率 语言 模型 神经 概率 语言 模型 
Neural Probabilistic Languange Model 的 想法 很 简单 就是 使用 
神经 网络 来 拟合 函数 F * 该 神经网络 包含 
四层 结构 输入 层 映 射层 隐 层 和 输出 
层 输入 层 神经元 的 个数 是 context w 的 
单词 数量 一般 定义 的 context w 是 固定 长度 
对于 context w 不足 固定 长度 的 情况 使用 填充 
向量 进行 填充 此处 按下不表 映 射层 负责 将 输入 
层 的 输入 映射 为 词 向量 这里 插播 一下 
什么 是 词 向量 什么 是 词 向量 词 向量 
是 vector representation of words 又 叫做 word embeddings 就是 
把 语料库 中的 单词 映射 为 固定 长度 的 向量 
的 技术 由于 机器 只能 处理 二进制 数据 不能 直接 
识别 和 处理 文本 字符 因此 我们 首先 要 将 
文本 数据 进行 编码 传统 的 编码 方式 叫做 one 
hot reprensentation 实现 方式 很 简单 假设 你 的 语料库 
中的 词典 数量 为 n 则将 这 n 个 词 
按 一定 的 顺序 排列 每次 词 对应 一个 序号 
i 然后 将 词 表示 为 长度 为 n 的 
向量 该 向量 的 第 i 个 元素 为 1 
其余 元素 全部 为 0 这个 方式 虽然 简单 但 
存在 很多 问题 1 向量 的 长度 n 等于 语料库 
的 词汇 量 而 一般 的 语料库 的 词汇 量 
成千上万 的 都 非常 大 所以 这样 构造 的 数据 
集 维度 很大 过于 稀疏 容易 造成 维数 灾难 2 
这种 做法 无法 表示 出 单词 和 单词 之间 的 
关联性 比如 dog 和 cat 应该 是 非常 类似 的 
两个 单词 但 被 映射 为 与 其他 单词 没有 
区别 差异 的 0 1 组成 的 向量 因此 word 
embeddings 的 目的 在于 将 所有 的 词汇 映 射到 
m 维空间 中 将 语法 或者 语义 相近 的 词 
表示 为 空间 中 邻近 的 点 m 的 大小 
一般 可以 人 为 指定 word embedding 一般 是 所有 
自然语言 处理 过程 的 第一 步 也是 最 重要 的 
一步 我们 再 把 话题 转 回到 神经 概率 语言 
模型 中 来 映 射层 负责 将 文本 字符 映 
射成 词 向量 至于 怎样 实现 映射 的 事 先是 
不 知道 的 我们 需要 把 这种 映射 关系 也 
放到 神经 网络 中 进行 训练 将 映射 矩阵 定义 
为 未知 参数 因此 词 向量 变为 了 神经 网络 
语言 模型 的 附属 产物 隐 层 是 全 连接结构 
神经元 的 数量 可以 人 为 指定 一般 还 会对 
隐 层 的 输出 再作 用 一个 tanh 激活 函数 
假设 训练 集 的 词典 数目 为 N 则 定义 
输出 层 的 神经元 个数 为 N 输出 层 的 
神经元 与 词典 D 中的 单词 一一对应 输出 层 也是 
全 连接结构 因此 整个 神经 网络 的 计算 过程 如下 
输入 层 Input 映 射层 P = Prpject Input 隐 
层 H = tanh W * P + bias _ 
p 输出 层 Output = U * H + bias 
_ oW U 是 网络 权重 bias _ p bias/w 
_/i o/w 是/v 偏置/n 项/n CBOW/w 和/c Skip/w gram 模型 
就是 在 此 基础 上 衍生 出来 的 CBOW 是 
用 上下文 context w 预测 目标 w 而 skip gram 
则是 逆向 操作 用 目标 w 预测 context w 但 
二者 的 原理 相同 因此 这里 以 CBOW 为例 进行 
介绍 CBOW 模型 神经 网络 的 结构 保持 不变 让 
我们 换 个 角度 来 看待 问题 在 神经 概率 
语言 模型 中 我们 把 问题 看作 是 一个 N 
分类 问题 使用 softmax regression 进行 分类 而 现在 我们 
现在 把 问题 看成 是 N 个 二分 类 问题 
每个 单词 w _ i ∈ D 对应 一个 二 
分类 问题 y = 1 某 context input 对应 的 
目标 单词 是 该 单词 y = 0 某 context 
input 对应 的 目标 单词 不是 该 单词 因此 在 
每一个 样本 对应 的 似 然 函数 中 计算 项由/nr 
原先 的 N 降低 为 k + 1 k 的 
大小 一般 由 人为 指定 k N 从而 大大 减少 
了 计算 量 Tensorflow 中 Skip gram 模型 的 代码 
实现 我们 以 word2vec _ basic . py 为例 简单 
讲解 一下 语言 模型 的 代码 实现 word2vec _ basic 
. py 使用 的 模型 是 Skip gram 具体 的 
实现 和 之前 的 神经 网络教程 区别 不大 我们 这边 
只 讲讲 不同 的 地方 不同 的 地方 在于 1 
输入 数据 的 构造 2 映 射层 的 构造 3 
目标函数 的 定义 1 输入/v 数据/n 的/uj 构造/v 输入/v 数据/n 
的/uj 构造/v 负责/v 将/d 自然/d 语言/n 文本/n 转换/v 成/n 计算机/n 
能/v 识别/v 的/uj 数据/n 这是 初级 翻译 操作 还 不是 
word embedding 我们 将 字典 D 中的 N 个 单词 
进行 编号 编号 范围 0 ~ N 1 编号 方式 
可以 任意 选择 给定 一段 自然语言 文本 作为 输入 假设 
文本 固定 长度 为 n 将其 拆解 为 单词 w 
_ 1 w _ 2 w _ n 然后 将 
每个 单词 对应 的 编号 feed 给 tensor placeholder train 
_ inputs shape = batch _ size n 2 映 
射层 的 构造 创建 映射 矩阵 tensor variable embeddings 其 
shape = N M 然后 对 该 矩阵 进行 随机 
初始化 M 为 人为 规定 的 词 向量 的 长度 
则 映射 矩阵 的 embeddings i 对应 的 是 第 
i 个 单词 的 词 向量 embeddings 是 variable 类型 
的 对象 属于 神经 网络 的 待 优化 参数 调用 
embed = tf . nn . embedding _ lookup embeddings 
train _ inputs 实现 词 向量 映射 返回 的 embed 
是 映 射层 的 输出 输入 数据 的 词 向量 
映射 结果 shape = batch _ size n M embed 
i j = embeddings train _ inputs i j 可以 
理解 为 embeddings 是 词 向量 字典 train _ inputs 
是 索引 将 train _ inputs 中 每个 元素 替换 
为 该 元素 值 对应 的 embeddings 中 相应 序号 
的 向量 3 目标函数 的 定义 loss = tf . 
reduce _ mean tf . nn . nce _ loss 
nce _ weights nce _ biases embed train _ labels 
num _ sampled vocabulary _ size 这里 使用 的 是 
NEC loss 是 Negative Sampling 的 近似 替代 nce _ 
weights nce _ biases 输出/v 层/q 的/uj 权重/n 和/c 偏置/n 
项/n embed/w 映 射层 的 输出 PS 在 这个 例子 
中 神经网络 没有 隐 层 train _ labels 训练 集 
的 真实 标注 num _ sampled 指定 随机 负 采样 
negative samplings 的 数量 vocabulary _ size 词典 数量 也是 
输出 层 神经元 的 数量 然后 剩下 的 工作 就 
交给 tf . nn . nce _ loss 来 完成 
了 由此可见 虽然 CBOW 和 skip gram 模型 稍微 有点 
复杂 但在 实际 的 代码 实现 中 tensorflow 已经 把 
大部分 的 工作 都 提前 封装 好了 我们 只 需要 
一行 简单 的 代码 调用 即 可以 实现 但 理解 
一下 它 内部 的 原理 肯定 还是 益处 多多 的 
注 自然语言 处理 入门级 小白 上述 如有 不妥 之处 欢迎 
批评指正 1 . CS224D2 . NLP 到 Word2vec3 . Opencv3 
图像处理 4 . Tensorflow 视频教程 5 . 机器学习 视频教程 6 
. 七月 在线 所有 人工智能 课程 7 . 聊天 机器人 
视频教程 8 . 自然语言 处理 视频教程 链接 如下 需要 的 
可以 在 百度 网盘 下载 链接 1 https / / 
pan . baidu . com / s / 1 u 
q q Y M Q 3 J 4 V k 
1 k C H 7D4dA 密码 28s7 链接 2 https 
/ / pan . baidu . com / s / 
1 K d R Y y I 0 Y t 
a 5 g W N D w Q g e 
7 s w 密码 0smi 链接 3 https / / 
pan . baidu . com / s / 1 g 
z H Z 5 2 k C d N r 
r g t L y y 3 f 1 w 
密码 8dl0 链接 4 https / / pan . baidu 
. com / s / 1KrGVuhICot9GRLa XFRsAA 密码 qa88 链接 
5 https / / pan . baidu . com / 
s / 16xwLarpVSQp6 ZIAAp7mOQ 密码 ixdk 内容简介 本书 全面 讲述 
人工智能 的 发展史 几乎 覆盖 人工智能 学科 的 所有 领域 
包括 人工智能 的 起源 自动 定理证明 专家系统 神经网络 自然语言 处理 
遗传算法 深度 学习 强化 学习 超级 智能 哲学 问题 和 
未来 趋势 等 以 宏阔 的 视野 和 生动 的 
语言 对 人工智能 进行 了 全面 回顾 和 深度 点评 
本书 作者 和 书中 诸多 人物 或为 师友 或 相熟 
相知 除了 详实 的 考证 还有 有趣 的 轶事 本书 
既 适合 专业 人士 了解 人工智能 鲜为人知 的 历史 也 
适合 对 人工智能 感兴趣 的 大众 读者 作为 入门 的 
向导 名人 推荐 人工智能 简史 确实 是 本 难得 的 
好书 它 既是 一本 严肃 的 信史 又 通俗 易懂 
带有 科普 的 性质 更 难得 的 是 妙趣横生 使人 
拿起来 一读 就 放不下 手 要把 人工智能 的 历史 和 
背后 的 哲理 讲得 既 准确 又 明白 易懂 进一步 
还要 有趣 那 又是 一项 十分 艰巨 的 任务 可是 
尼克 做到 了 毛 德操 计算机 专家 浙大网新 科技 首席 
科学家 人工智能 简史 这本书 应该 是 这两年 出版 的 此类 
书籍 中 最好 的 一本 对 我们 建立 对 AI 
的 全景 式 理解 很有 裨益 对于 人工智能 的 来龙去脉 
成败 得失 的 原因 尼克 老师 娓娓道来 既 不失 深度 
又不 枯燥 难懂 在 最后 两章 他 讨论 了 不可避免 
的 AI 的 未来 和 哲学 问题 从 能源 自动 
机器 社会 的 综合 历史 的 角度 进行 了 发人深省 
的 探讨 鲍捷 文因/nr 互联 CEO 这本书 不错 把 各位 
人工智能 大师 的 思想 成果 师承 恩怨 都串/nr 起来 讲 
就像 我们 在 与 各位 大神 一起 工作 生活 一样 
孔 华威 中科院计算所 上海 分所 所长 起点 资本 合伙人 尼克 
以 灵活 轻松 的 笔调 写 出了 人工智能 发展 历程 
中的人物 故事 以及 思潮起伏 他 特别 重视 对 具体 的 
思想 家 科学家 学者 的 刻画 轶事 趣闻 像 撒胡椒面 
一样 为 整本书 提味 与 一般 人 的 预期 相反 
就 叙事 的 有趣 程度 而言 人工智能 简史 简直不 像是 
一本 讲 科学 技术 的 书 南方 都市报 作者简介 尼克 
乌镇 智库 理事长 国家 千人 计划 专家 毕业 于 中科院 
美国 麻省 大学 早年/t 曾/d 任职/v 哈佛/l 和/c 惠普/nr 后 
创业投资 往返/v 于/p 大陆/n 和/c 硅谷/n 无论 忙 闲 不忘 
读书 写字 作品 多 发表于 上海 书评 并有 著作 UNIX 
系统 V 内核 剖析 和 哲学 评书 本书 内容 前言 
历史 素有 两种 写法 以 人 为主 和以事/nr 为主 所有 
的 传记 都 是以 人 为主 的 而 各种 专史 
如 战争史 则 多以 事 为主 所谓 历史 是 人民 
创造 的 还是 英雄 创造 的 我 个人 的 偏好 
还是 以人为本 八卦 的 历史 读者 自然 喜欢 对 作者 
也 有 好处 就像 一战 后 英国 首相 劳合 乔治 
对 他 的 耶路撒冷 总督 说 的 那样 有 争执 
咱们 政治家 才派 得上 用场 如果 他们 停 下来 不 
打了 你 就 失业 了 人工智能 到底 是 什么 给 
一门 学科 界定 范围 很难 尤其 是 这门 学科 还在 
快速 变化 中 即使 是 数学 这样 的 成熟 学科 
有时 我们 也 理 不清 边界 而 像 人工智能 这样 
朝令夕改 的 更是 不 容易 闹 清楚 了 人工智能 的 
定义 素 无 共识 在 大学 里 机械系 电子系 计算机系 
甚至/d 哲学系/n 都/d 有人/r 干/v 人工智能/n 让 这些 人 对这 
门 学科 取得 共识 谈何容易 从 实用主义 哲学 的 实用主义 
不是 日常用语 实用主义 看 一个 学科 就是 学科 共同体 共同 
关注 的 东西 有些 毛边 可以 宽容 演变 这种 外延 
式 的 定义 要比 从 上帝 视角 给 一个 内涵式 
定义 更为 实用 一般 认为 人工智能 起源于 1956 年在 达特茅斯 
学院 召开 的 夏季 研讨会 国内 关于 达特茅斯 会议 和 
神经 网络 早期 历史 的 各种 段子 很多 源于 我 
几年 前 的 两篇 博客 后来 被 上海 书评 转发 
经过 修订 我 把 它们 重新 编为 本书 的 两章 
达特茅斯 会议 人工智能 的 缘起 和 神经网络 简史 计算机 下棋 
一章 的 大部分 也在 南方周末 发表 过 自动 定理证明 兴衰 
纪 的 核心 内容 在 中国计算机学会 通讯 连载 过 明尼苏达 
大学 的 查尔斯 巴贝奇 研究 所 一直 在 做 计算机 
科学 的 口述 历史 采访 了 很多 对 计算机 科学 
有 影响 的 人 其中 有 相当 一批 是 人工智能 
学者 大/a 部分/n 的/uj 采访/v 都有/nr 录音/n 除了 翻阅 各种 
文献 外 我 听了 近 100 小时 的 采访 录音 
许多 人工智能 老一代 革命家 临终时 话 都说不/nr 利索 听 这种 
东西 除了 兴趣 还得 有 体力 图灵 大概 是 第一个 
对 智能 做出 深刻 思考 的 智者 他 1936年 的 
文章 可计算 的 数 奠定 了 计算机 科学 的 理论 
和 实践 基础 也把 相关 的 哲学 思考 推进 了 
一大步 以至于 哲学家 蒙克 Ray Monk 把 他 列为 有史以来 
最 伟大 的 十位 哲学家 之一 图灵 1950年 在 哲学 
杂志 心 Mind 上 发表 的 文章 计算机 与 智能 
是 传世之作 但这 篇 文章 没有 靠谱 的 中文 翻译 
我 将 我 的 译文 和 一篇 图灵 小传 附在 
书后 作为 附录 图灵 小传 的 一个 早期 版本 曾 
出现 在 我 的 哲学 评书 一 书中 但 新版本 
融入 了 一些 近几年 关于 图灵 研究 的 新 成果 
人工智能 这个 词组 的 出现 和 达特茅斯 会议 有关 但 
英国 学术 圈 在 1956 之前 和 之后 的 很长 
一段 时间 一直 在 用 机器 智能 的 说法 这 
和 图灵 1950年 的 文章 有关 一般 认为 这 篇 
文章 是 这个 学科 的 源头 但 后来 发现 图灵 
1948年 在 英国 国家 物理 实验室 NPL 写 过 一个 
内部 报告 题为 智能 机器 其中 提到 了 肉体 智能 
embodied intelligence 和 无 肉体 智能 disembodied intelligence 的 区分 
机器人 学家 布鲁克斯 Rodney Brooks 认为 图灵 1948年 的 报告 
比 图灵 1950年 的 文章 更加 重要 它/r 从/p 某种/r 
意义/n 上/f 预示/v 了/ul 后来/t 符号/n 派/v 和/c 统计/v 派/v 
之争/i 这段 历史 我 也 列 在 附录 里 放在 
图灵 小传 之后 因为 我 觉得 先 读读 图灵 的 
生平 也许 会 有助于 理解 他 的 思想 本书/r 每一/i 
章/q 几乎/d 都可/nr 单独/d 阅读/v 大 部分 内容 对于 受过 
高中 教育 的 人 应该 都 不难 懂 但 第 
10 章 是个 例外 这 一章 企图 以 严肃 的 
态度 探讨 人工智能 我 以 一种 浓缩 的 方式 讲述 
了 图灵机 丘奇 图灵 论题 相似性 原则 和超/nr 计算 没有 
计算 理论 很多 人工智能 的 基础 问题 实在 是 拎不清 
如果 读者 觉得 吃力 可以 跳过 这 一章 我 常用 
的 一种 历史 研究 工具 是 谷歌 的 Ngram 谷歌 
扫描 了 三千多万 本书 把 书 中 出现 的 词组 
的 词频 统计 结果 公布 以 时间 为 横轴 词频 
为 纵轴 画 一条 曲线 就可 看出 特定 的 词 
在 不同 历史 时间段 的 兴衰 从而 得出 某些 洞察 
例如 通过 比较 United States are 和 United States is 
在 历史 上 出现 的 频率 就可 看出 美国 人 
是 何时 开始 认同 美国 作为 一个 统一 的 国家 
的 很明显 南北战争 之后 United States is 开始 变得 更常用 
我们 通过 统计 若干 人工智能 中 关键词 的 Ngram 可以 
感知 人工智能 在 不同 阶段 的 宏观 发展 我 曾经 
写过 一篇 计算 历史学 见 哲学 评书 介绍 Ngram 大 
数据 为 历史学 提供 了 有力 而 令人信服 的 工具 
科普 有 一种 写法 用 一些 貌似 通俗 的 语言 
去 解释 复杂 的 原理 我 一直 不大 相信 这种 
方法 无论 作者 是 内行 还是 专业 科普作家 我 压根 
就 没 见过 一 本 可以 把 量子力学 解释 清楚 
的 科普书 即使 简单 如 图灵机 也 鲜有 适当 的 
普及 读物 倒是 那些 讲 历史 和 八卦 的 书 
引人入胜 安德鲁 霍奇 斯 的 艾伦 图灵 传 如 谜 
的 解谜 者 是 内行 写作 的 典范 而 数学家 
所罗门 费 佛曼 的 太太 安妮塔 费 佛曼 的 两本 
逻辑学家 传记 是 我 心目中 的 标杆 戴森 Freeman Dyson 
一直 是 我 喜欢 的 作者 他 也 时不时 为 
纽约 书评 写写 八卦 趣味 和我/nr 接近 我 总是 从 
阅读 他 的 文章 的 过程 中 收获 良多 即使 
我 不懂 他 的 数学 和 物理 的 领域 也能 
时有 洞察 我 的 书单 上 还有 蒙克 的 所有 
传记 它们 既 高级 又 有趣 就像 蒙克 所说 历史 
可以 帮助 内行 了解 知识 的 进化 并 获得 新的 
视角 同时 也 为 外行人 了解 专业知识 提供 入门 的 
台阶 或 向导 读 大 科学家 写 的 科普 著作 
最 有意思 的 倒 不是 那些 对 成熟 思想 的 
通俗 叙述 而是 那些 对 不成熟 看法 的 披露 还有 
不好意思 写到 正经 学术 论文 里 的 自负 和 牢骚 
恰 因为 这个 原因 我 也 喜欢 多 依 奇 
David Deutsch 的 几本书 我们 很少 有 机会 在 学科 
发展 之初 就 能把 学科 脉络 梳理 清楚 过去 有过/nr 
几个 这样 的 时间段 例如 1900年 到 1950年 的 逻辑学 
1945年 到 2000年 的 分子 生物学 和 1950年 到 当下 
的 语言学 本书 除了 想 梳理 始于 20 世纪 40 
年代 的 人工智能 的 历史 外 还有 一个 作者 隐含 
的 心愿 作为 人工智能 的 科普 哈代 曾说/nr 科学 尤其 
是 数学 和 理论 物理 也许 还有 理论 计算机科学 和 
艺术 的 原创 需要 一等 的 智力 解释 和 欣赏 
例如 乐评 家和 书评 家 是 二等 的 智力 活儿 
本书 假想 的 对象 是 那些 有 能力 但 又是 
外行 的 人 丘成桐 曾说/nr 大意 要 想做 大学问 必须先 
培养 对 学问 的 感情 除了 科普 我 还 希望 
能 帮助 一小撮 内行人 或 准 内行人 培养感情 我 尽可能 
地 列出 了 相关 的 参考 文献 供 进一步 学习 
人工智能 毕竟 不是 超弦理论 凭着 一些 智力 还是 可以 自学 
的 本书 写作 得到 白硕/nr 陈 利人 宫力/nr 洪涛 刘江 
马少平/nr 毛 德操 施水才/nr 和/c 赵伟/nr 等/u 诸位/m 师友/n 的/uj 
帮助/v 和/c 指点/n 特此 致谢 乌镇 智库 的 同仁 为 
本书 提供 了 必要 的 数据 我 的 助理 冰冰 
为 我 提供 了 多 方面 的 支持 一并 谢过 
第 1 章 达特茅斯 会议 人工智能 的 缘起 What is 
past is prologue . 过去 只是 序幕 William Shakespeare 莎士比亚 
1 . 背景 现在一 说起 人工智能 的 起源 公认 是 
1956年 的 达特茅斯 会议 殊不知 还有 个 前戏 1955年 美国 
西部 计算机 联合 大会 Western Joint Computer Conference 在 洛杉矶 
召开 会中 还 套了 个 小会 学习机 讨论会 Session on 
Learning Machine 讨论会 的 参加者 中有 两个人 参加 了 第二年 
的 达特茅斯 会议 他们 是 塞 弗里奇 Oliver Selfridge 和纽/nr 
厄尔 Allen Newell 塞 弗里奇 发表 了 一篇 模式识别 的 
文章 而 纽 厄尔 则 探讨 了 计算机 下棋 他们 
分别 代表 两派 观点 讨论会 的 主持人 是 神经 网络 
的 鼻祖 之一 皮茨 Walter Pitts 他 最后 总结 时说 
一 派人 企图 模拟 神经系统 而 纽 厄尔 则 企图 
模拟 心智 mind 但 殊途同归 这 预示 了 人工智能 随后 
几十年 关于 结构 与 功能 两个 阶级 两条 路线 的 
斗争 开聊 达特茅斯 会议 之前 先 说说 6个 最 关键 
的 人 首先 会议 的 召集者 麦卡锡 John McCarthy 当时 
是 达特茅斯 学院 的 数学系 助理 教授 1954年 达特茅斯 学院 
数学系 同时 有 4位 教授 退休 这对 达特茅斯 这样 的 
小学校 而言 真是 不可 承受 之 轻 刚 上任 的 
年轻 系主任 克 门 尼 John Kemeny 之前 两年 才在 
普林斯顿大学 逻辑学家 丘奇 Alonzo Church 门下 取得 了 逻辑学 博士 
于是 跑到 母校 求援 这么 说 起来 克 门 尼 
算是 图灵 的 师弟 他 战时 和 物理学家 费曼 一起 
工作 还 一度 当过 爱因斯坦 的 数学 助理 后来 一头 
扎 在 计算机 研究 里 和 麦卡锡 一起 琢磨 出了 
分时系统 他 1955 年在 科学 美国人 杂志 上 写过 一篇 
文章 把人 看作 机器 Man Viewed as a Machine 介绍/v 
了/ul 图灵机/n 和冯/nr 诺伊曼/l 1 的 细胞 自动机 最早 叫 
自 生机 文章 的 简介 提到 肌肉 机器 muscle machine 
和 大脑 机器 brain machine 所谓 大脑 机器 就是 人工智能 
的 另一种 说法 而已 克 门 尼 最为 人知 的 
工作 应该 是 发明 了 老少咸宜 的 编程语言 BASIC 现在 
估计 已经 没 人 知道 BASIC 语言 发明人 曾是/nr LISP 
语言 发明人 的 老板 克 门 尼 是 天生 的 
官僚 后来 位居 达特茅斯 学院 数学 系主任 和 校长 美国 
三 里岛 核电站 出事 时 总统 委托 他 当 调查 
委员会 主席 这是 后话 克 门 尼 从 母校 数学系 
带回 了 刚 毕业 的 4位 博士 前往 达特茅斯 学院 
任教 麦卡锡 是 其中 之一 麦卡锡 后来 发明 的 LISP 
语言 中 最重要 的 功能 Eval 实际 就是 丘奇 的 
λ 演算 而且 他 后半生 致力 于用/nr 数理逻辑 把 常识 
形式化 大家 由此 猜测 他 可能 也是 丘奇 的 学生 
但 其实 不是 他 学 的 压根 就 不是 逻辑 
他 的 老师 是 失去 双手 的 代数拓扑 学家 莱夫谢茨 
Lefschetz 但/c 麦卡锡/nr 对/p 逻辑/n 和/c 计算/v 理论/n 一直/d 有/v 
强烈/a 兴趣/n 他 1948年 本科 毕业 于 加州理工学院 在 学校 
主办 的 Hixon 会议 上 听到 冯 诺伊曼 关于 细胞 
自动机 的 讲座 后来/t 他/r 刚到/i 普林斯顿/l 大学/n 读/v 研究生/n 
时就/nr 结识/n 了/ul 冯/nr 诺伊曼/l 在 老冯 影响 下 开始 
对 在 计算机 上 模拟 智能 产生 兴趣 麦卡锡 1927 
2011 达特茅斯 会议 的 另一 位 积极 的 参加者 是 
明斯基 他 也是 普林斯顿 大学 的 数学 博士 和/c 麦卡锡/nr 
在/p 读书/n 时就/nr 相熟/a 他 的 主业 也 不是 逻辑 
尽管 他 后来 写过 一本 很有 影响力 的 计算 理论 
的 书 还 培养 过 好几 个 计算 理论 的 
博士 其中 就有 图灵奖 获得者 布鲁姆 Manual Blum 布鲁姆 目前 
和他/nr 老婆 Lenor Blum 就是 实数 计算 模型 BSS 的 
B 儿子 一家 三口 都在 卡内基 梅隆 大学 任教 明斯基 
的 理论 情结 和 丘奇 关系 也 不大 他 的 
老师 塔克 Albert Tucker 是 莱夫谢茨 的 学生 主要 做 
非线性 规划 和 博弈论 多年 来 担任 普林斯顿 大学 数学 
系主任 出身 数学 世家 儿子 孙子 也 都是 数学家 按 
辈 分论 麦卡锡 还是 明斯基 的 师叔 塔克 的 另一 
名 出色 的 学生 后 来得 了 诺贝尔 经济学 奖 
他 就是 心灵 美丽 的 纳什 纳什 比 明斯基 小 
一岁 但 比 他 早 4年 拿到 博士学位 也 算是 
明斯基 的 师兄 了 明斯基 的 博士 论文 是 关于 
神经 网络 的 他/r 在/p 麻省理工学院/nt 150/m 周年纪念/n 会议/n 上/f 
回忆/v 说是/i 冯/nr 诺伊曼/l 和/c 麦卡洛克/nr Warren McCulloch 启发 他 
做了 神经网络 有人 还 找过 他 麻烦 质疑 说 神经 
网络 的 研究 算 数学 吗 倒是 老冯 力挺 说 
现在 不算 但 很快 就 得 算 倒是 明斯基 自己 
后来 和 神经 网络 结下 梁子 那段 故事 见 本书 
第 5 章 神经网络 简史 明斯基 的 熟人 都 认为 
他 是 无所不通 的 天才 他 的 忘年交 沃尔弗拉姆 Stephen 
Wolfram 称 他 晚年 计划 写本 神学 的 书 但 
去世 时书/nr 还没 影子 塞 弗里奇 被 后人 提及 不多 
但 他 真是 人工智能 学科 的 先驱 他/r 在/p 麻省理工学院/nt 
时/n 一直/d 和/c 神经/n 网络/n 的/uj 开创/v 人/n 之一/r 麦卡洛克/nr 
一起/m 在/p 维纳/l Norbert Wiener 手下 工作 他 是 维纳 
最 喜欢 的 学生 但没 读完 博士 学位 维纳 控制论 
一 书 的 第一 个 读者 就是 塞 弗里奇 塞 
弗里奇 是 模式识别 的 奠基人 他 写了 第一个 可 工作 
的 AI 程序 他 后来 在 麻省理工学院 参与 领导 MAC 
项目 这个 项目 后来 一分为二 计算机科学 实验室 和 人工智能 实验室 
但 分久必合 现在 这 两个 项目 又 合并 了 变成 
了 MIT CSAIL 顺便 给 女 读者 添点 料 塞 
弗里奇 的 爷爷 就是 英国 第二 大 百货店 塞 尔福 
里奇 Selfridges 的 创始人 所谓 顾客 永远 是 对 的 
The customer is always right . 就 出自 塞 尔福 
里奇 他 本 是 美国人 后到 英国 创业 发财 后 
老婆 就 死了 于是 勾搭 上 一对 匈牙利 双胞胎 歌舞 
演员 出入 赌场 赔 光了 家业 他 的 故事 2013年 
还被 有意思 的 英国人 拍成 了 电视剧 塞 尔福 里奇 
百货 几经 周转 现在 的 主人 是 美国 百货公司 希尔斯 
Sears 塞 尔福 里奇 百货 和 隔壁 的 哈罗德 百货 
支撑着 牛津街 的 零售业 现在 大概 一半 顾客 来自 中国 
信息论 的 创始人 香农 Claude Shannon 被 麦卡锡 拉大旗 做 
虎皮 也请 到 会上 打酱油 其实 麦卡锡 和 香农 的 
观点 并不 一致 平日 相处 也 不睦 香农 的 硕士 
博士论文 都是 讲 怎么 实现 布尔代数 的 当时 麻省理工学 院校长 
布什 Bush 亲自 指导 博士 毕业 后他/nr 去了 普林斯顿 高等 
研究院 曾和 数学家 外尔 Hermann Weyl 爱因斯坦 哥德 尔等 共事 
战争 中 他 一直 在 贝尔 实验室 做 密码学 的 
工作 图灵 在 1943年 曾秘访/nr 美国 和 同行 交流 破解 
德国 密码 的 经验 其间 和 香农 曾有 会晤 一起 
聊过 通用 图灵机 战后 香农 去 英国 还 回访 过 
图灵 一起 讨论 过 计算机 下棋 香农 内向 从没 说 
过 这段 往事 直到 1982年 接受 一次 采访 时才/nr 提起 
1950年 香农 在 哲学 杂志 发表 过 一篇 讲 计算机 
下棋 的 文章 为 计算机 下棋 奠定 了 理论 基础 
香农 比 其他 几位 年长 十岁 左右 当时 已 是 
贝尔 实验室 的 大佬 香农 1916 2001 另外/c 两位/m 重量级/b 
参与者/n 是/v 纽/ns 厄尔/l 和/c 司马贺/nr Herbert Simon 纽 厄尔 
是 麦卡锡 和 明斯基 的 同龄人 他 硕士 也是 在 
普林斯顿 大学 数学系 读 的 按说 普林斯顿 大学 数学系 很小 
他们/r 应/v 有/v 机会/n 碰面/v 但 那时 纽 厄尔 和 
他俩 还真 不认识 他们 的 第一 次 见面 纽 厄尔 
回忆 是 在 IBM 而 麦卡锡 回忆 是 在 兰德 
公司 纽 厄尔 的 硕士 导师 就是 冯 诺伊曼 的 
合作者 博弈论 先驱 摩根 斯顿 纽 厄尔 硕士 毕业 后就迁/nr 
往西部 加入 著名 智库 兰德 公司 他 在 兰德 开会 
时 认识 了 塞 弗里奇 并 受到 对方 做 的 
神经 网络 和 模式 识别 的 工作 的 启发 但 
方法论 走 的 却 完全 是 另一 条 路 纽 
厄尔 1927 1992 与 司马贺/nr 1916 2001 司马贺比/nr 他们/r 仨/m 
都大/nr 11岁/mq 怀特海 比 罗素 也 大 11岁 那时 是 
卡内基 理工学院 卡内基 梅隆 大学 的 前身 工业 管理系 的 
年轻 系主任 他 在 兰德 公司 学术 休假 时 认识 
了 纽 厄尔 司马贺/nr 后来/t 把/p 纽/ns 厄尔/l 力/n 邀/v 
到/v 卡内基/nr 梅隆/nr 大学/n 并给 纽 厄尔 发了 个 博士 
学位 开始 了 他们 终生 的 合作 纽/ns 厄尔/l 和/c 
司马贺的/nr 合作/vn 是/v 平等/a 的/uj 司马贺是/nr 纽/ns 厄尔/l 的/uj 老师/n 
但 他们 合作 的 文章 署名 都是 按 字母顺序 纽 
在前 司马 在后 每次 他们 受邀 去 演讲 都是 轮流 
司马贺/nr 每次/r 见到/v 别人/r 把/p 他/r 名字/n 放到/v 纽/ns 厄尔/l 
之前/f 时都/nr 纠正/v 他们 共享 了 1975年 的 图灵奖 三年/m 
后/f 司马贺再/nr 得/ud 诺贝尔经济学奖/nz 纽/ns 厄尔/l 和/c 司马贺/nr 代表/n 了/ul 
人工智能/n 的/uj 另一/i 条/n 路线/n 符号 派 他们 后来 把 
他们 的 哲学 思路 命名为 物理符号系统 假说 简单 地 说 
就是 智能 是 对 符号 的 操作 最 原始 的 
符号 对应 于 物理 客体 这个 思路 和 英美 的 
经验 主义哲学 传统 接近 他们 和 当时 的 数学 系主任 
第一届 图灵奖 获得者 珀 里 思 Alan Perlis 一起 创立 
了 卡内基 梅隆 大学 的 计算机系 从此 卡内基 梅隆 大学 
成为 计算机 学科 的 重镇 2 . 达特茅斯 会议 会议 
原址 达特茅斯 楼 1953年 夏天 麦卡锡 和 明斯基 都在 贝尔实验室 
为 香农 打工 香农 那时 的 兴趣 是 图灵机 以及 
是否 可用 图灵机 作为 智能 活动 的 理论 基础 麦卡锡 
向 香农 建议 编 一本 文集 请 当时 做 智能 
研究 的 各位 大佬 贡献 文章 这本 文集 直到 1956年 
才 以 自动机 研究 Automata Studies 为名 出版 这个 书名 
最后 是 香农 起 的 他 不想 花里胡哨 但 麦卡锡 
认为 这 没有 反映 他们 的 初衷 文集 的 作者 
有 两类 人 一类 是 逻辑学家 后来 都 变成 计算 
理论家 了 如 丘奇 的 两位 杰出 学生 戴维斯 和 
克里尼 后者 的 名著 元 数学 导论 在 国内 有 
逻辑学家 莫绍 揆 先生 的 译本 明斯基 麦卡锡 也 都有 
论文 录入 香农 本人 贡献 了 一篇 讲 只有 两个 
内部 状态 的 通用 图灵机 的 文章 文集 录入 的 
一篇 冯 诺伊曼 的 论文 后来 开创 了 容错 计算 
文集 的 另一 类 作者 几乎 都是 维纳 的 信徒 
如 阿什比 Ross Ashby 等 以 控制论 为基础 麦卡锡 素 
不喜 控制论 和 维纳 既不 想把 维纳 当 老大 也 
不愿 和他/nr 见面 争执 其中 原因 不详 或 许和 维纳 
与 麦卡洛克 吵翻 了 有关 麦卡洛克 和 皮茨 这两位 为 
维纳 控制论 思想 贡献 多多 的 人物 在 维纳 的 
自传 里 压根 没 被 提及 麦卡锡 同时 又 觉得 
香农 太 理论 当时 他 想 自立门户 只对 用 计算机 
实现 智能 感兴趣 于是 他 筹划 再 搞 一次 活动 
从 香农 后来 接受 的 采访 来看 他 对 维纳 
也 没有 多少 尊重 他 觉得 自己 创立 的 信息 
论 和 维纳 一点 关系 也 没有 但 维纳 却 
认为 香农 受到 他 的 影响 香农 认为 维纳 的 
这种 错觉 来源于 维纳 根本 不 了解 信息论 1955年 夏天 
麦卡锡 到 IBM 打工 美国 教授 都是 9个 月工资 如果 
没有 研究 经费 夏天 要 自己 觅食 他 的 老板 
是 罗切斯特 Nathaniel Rochester 罗切斯特 是 IBM 第一代 通用机 701 
的 主 设计师 对 神经 网络 素 有兴趣 他们 两人 
倒是 挺 对 脾气 决定 第二 年 夏天 在 达特茅斯 
搞 一次 活动 遂 说 动了 香农 和 当时 在 
哈佛 做 初级 研究员 Junior Fellow 2 的 明斯基 一起 
给 洛克菲勒 基金会 写了 个 项目 建议书 希望 得到 资助 
美国 富豪 还是 有 文化 传统 的 至少 知道 要 
资助 好东西 值得 中国 土豪 的 后代 学习 麦卡锡 给 
这个 第二年 的 活动 起了 个 当时 看来 别出心裁 的 
名字 人工智能 夏季 研讨会 Summer Research Project on Artificial Intelligence 
普遍 的 误解 是 人工智能 这个词 是 麦卡锡 想 出来 
的 其实 不是 麦 老 晚年 回忆 也 承认 这个 
词 最早 是 从 别人 那里 听来 的 但 记不清 
是 谁 了 后来 英国 数学家 伍德华 Philip Woodward 给 
新 科学家 杂志 写信 说 他 是 AI 一 词 
的 原创 者 麦卡锡 最早 是 听 他 说 的 
因为 他 1956年 曾去/nr 麻省理工学院 访问 见过 麦卡锡 并 交流 
过 但 麦卡锡 的 建议书 1955年 就 开始 用 人工智能 
了 人老 了 回忆 真 不靠谱 当事人 都已/nr 仙逝 这事 
恐怕要 成 悬案 了 其实 英国人 最早 的 说法 是 
机器 智能 Machine Intelligence 这 大概 和 图灵 那篇 计算机 
与 智能 有关 大家 对 人工智能 这个词 一 开始 并 
没取 得 完全 共识 很多 人 认为 啥事 一 加 
人工 就 变味 了 纽/ns 厄尔/l 和/c 司马贺/nr 一直/d 主张/n 
用/p 复杂 信息处理 这个词 以至 他们 发明 的 语言 就叫 
IPL Information Processing Language 他们 从 某种 意义 上 说 
偏 功能学派 也 就是说 找到 智能 的 功能 不 一定 
非 得 依靠 结构 相同 或 相似 图灵机 和 递归函数 
等价 但 结构 完全 不同 所以 他们 强调 信息处理 他们 
俩 一 开始 颇 不喜 人工智能 几个字 1958年 在 英国 
国家 物理 试验室 NPL 召开 了 思维过程 机器 化 Mechanization 
of Thought Process 会议 达特茅斯 会议 的 与会者 麦卡锡 明斯基 
塞 弗里奇 都 参加 了 此外 还有 致力 于 神经网络 
研究 的 麦卡洛克 以及 英国 的 控制 论 代表 人物 
阿什比 两位 编程语言 的 先驱 也 出席 了 巴克斯 John 
Warner Backus 发表 了 一篇 关于 他 新 发明 的 
语言 Fortran 的 论文 但 他 后来 一直 是 函 
数式 语言 的 倡导者 美国 海军 女 少将 哈泊/nr Grace 
Hopper 的 文章 是 讲 第一个 编译器 的 这项 工作 
导致 了 COBOL 语言 的 诞生 中国 也有 女 少将 
也是 码 农 他俩 论文 的 题目 里 都有 Automatic 
Programming 的 说法 这在 当时 就是 指 高级语言 编程 不能 
和 后来 人工智能 中的 自动 编程 搞 混了 这次 会上 
有人 再提 人工 思维 Artificial Thinking 的 说法 司马贺/nr 等人/i 
由此/c 也/d 逐渐/d 接受/v 了/ul AI 的 说法 他 晚年 
还 写了 本书 人工 的 科学 倒是 把 Artificial 这个词 
更加 放大 了 3 . AI 历史/n 的/uj 方法论/n 历史/n 
研究/vn 方法/n 有/v 基于/p 事件/n 的/uj 和/c 基于/p 课题/n issue 
的 纽 厄尔 在 1981年 为 一本 颇为 有料 的 
文集 信息 研究 贡献 的 一篇 文章 AI 历史 的 
智力 课题 走了 第二 条 路线 他 的 方法 也 
挺 有意思 他 把 AI 历史 当作 斗争史 把 历史 
分为 两个 阶级 两条 路线 的 斗争 于是/nr 历史 成了 
一 串儿 对立 的 议题 如 模拟 与 数字 串行 
与 并行 取代 与 增强 语法 与 语义 机械论 与 
目的论 生物学 与 活力 论 工程 与 科学 符号 与 
连续 逻辑 与 心理 等 在 每一 议题 下有 进一步 
可分 的 子 议题 如在 逻辑 与 心理 下 又有 
定理证明 与 问题 求解 等 被 提到 最多 的 是 
人工 智能 与 控制论 在 Google Ngram 里 试试 Cybernetics 
和 Artificial Intelligence 两个 词 在 Google Books 里 出现 
的 词频 可以 看出 学科 的 跌宕起伏 人工智能 与 控制论 
词频/n 对比/v 美国/ns 最早/d 办/v 的/uj 一批/m 计算机/n 相关/v 的/uj 
系/v 科/n 都/d 创办/v 于/p 20/m 世纪/n 60/m 年代/t 中期/t 
那时 有些 系 直接 叫 计算机 科学系 而 有些 则 
叫 计算机 与 信息 科学系 带 信息 的 都 有些 
控制论 的 背景 如 麻省 大学 计算机 与 信息系 的 
创办人 就有 维纳 的 学生 阿比 卜 Michael Arbib 而 
密歇根 大学 则 叫 计算机 与 通讯 科学系 这些 系 
后来 都 改名 叫 计算机 系了 而 原来 的 图书馆 
系 现在 都 纷纷 改 名叫 信息 科学系 如/v 加州/ns 
大学/n 伯克利/nr 分校/n 和/c 华盛顿/ns 大学/n 的/uj 图书馆/n 学院/n 都/d 
改名/v 叫/v 信息学院/i School of Information 连 科学 都 省了 
但现在 计算机系 又有 加载 信息 的 趋势 麻省 大学 和 
加州大学 尔湾 分校 近年 又 改名 叫 信息 与 计算机 
科学 学院 了 大概 和 现在 深度 学习 及 神经 
网络 又 峰回路转 有关 吧 倒 是 中国 的 学科 
简单 一直/d 都有/nr 计算机/n 和/c 自动化/l 之分/i 老死不相往来 罢了 人工智能 
这个词 真正 被 共同体 广泛 认可 是 在 十年 后的/nr 
1965年 在 加州 大学 伯克利 分校 的 欧陆 派 哲学家 
德雷 弗斯 Hubert Dreyfus 发表 了 炼金术 与 人工智能 一文 
之后 这/r 篇/q 文章/n 一/m 开始/v 只是/c 针对/p 纽/ns 厄尔/l 
和/c 司马贺的/nr 工作/vn 几年 后 这篇文章 演变 成了 那本 著名 
的 或者 被 AI 圈子 称为 臭名昭著 的 计算机 不能 
干什么 一 书 则是 把 整个 AI 当作 靶子 欧/ns 
陆派/i 哲学家/n 被人/i 诟病/n 数学/n 和/c 科学/n 不通/a 但 德雷 
弗斯 有个 数学家 的 兄弟 和他同/nr 一年 在 哈佛 得了 
应用 数学 博士 后来 又 同在 加州 大学 伯克利 分校 
教书 是 动态 规划 的 大家 还 带过 神经 网络 
的 博士 哥俩 一个 立场 有时 一个 共同体 的 形成 
并 不是 靠 内部 的 团结 而是 靠 外部 的 
反对 有意思 的 是 炼金术 与 人工智能 一文 是 德雷 
弗斯 在 兰德 公司 工作 时 写就 的 司马贺/nr 后来/t 
撰文/n 猛/d 批/q 德雷/nr 弗斯/l 说 他 滥用 兰德 公司 
的 标签 德雷 弗斯 后来 抱怨 他 在 麻省 理工学院 
和 哈佛 食堂 吃饭 所有 做 AI 的 人都 躲 
他 远远 的 学术 争执 哪儿 都 一样 麦卡锡 和 
明斯基 的 建议 书里 罗列 了 他们 计划 研究 的 
7个 领域 1 自动 计算机 所谓 自动 指 的 是 
可编程 2 编程语言 3 神经网络 4 计算 规模 的 理论 
theory of size of a calculation 这 说 的 是 
计算 复杂性 明斯基 后来 一直 认为 计算 理论 是 人工智能 
的 一部分 他 早期 对 理论 问题 时 不时 会 
动动手 后来 一手 组建 了 麻省 理工学院 的 计算 理论 
队伍 5 自我 改进 这个 是 说 机器学习 6 抽象 
7 随机性 和 创见性 麦卡锡 的 原始 预算 是 一万三千五百 
美元 但 洛克菲勒 基金会 只 批 了 七千五百 美元 麦卡锡 
预计 会有 6位 学界 的 人 出席 会议 应该 支付 
每人 两个月 的 薪水 一千两百 美元 由此 可 推算 出 
麦卡锡 明斯基 当时 的 年薪 在 八千 美元 左右 考虑 
通货膨胀 和 购买力 大概 相当于 2016年 的 七 万多美元 真 
不算 多 现在 随便 一个 美国 大学 计算机 系 的 
教授 薪水 都远/nr 不止 这个 数 这个 学科 真是 今非昔比 
啊 作为 对比 司马贺/nr 1949年/tdq 去/v 卡内基/nr 梅隆/nr 大学/n 的/uj 
前身/r 卡内基/nr 理工学院/nt 担任/v 新/a 成立/v 的/uj 工业/n 管理/vn 系/v 
系主任/n 时的/nr 年薪/n 是/v 一/m 万美元/m 除了 那六/nr 君子 外 
另外 还有 4人 也 参加 了 达特茅斯 会议 他们 是 
来自 IBM 的 塞缪尔 Arthur Samuel 和 伯恩斯坦 他们 一个 
研究 跳棋 一个 研究 象棋 达特茅斯 的 教授 摩尔 Trenchard 
More 也 参与 了 他 后来 在 工业界 混 的 
时间 长 少 为 外人 所知 达特茅斯 会议 中 一位 
被 后人 忽视 的 先知 是 所罗门 诺夫 Solomonoff 和 
其他 来来往往 的 人 不同 所罗门 诺夫 在 达特茅斯 严肃 
地 待了 整整 一个 暑假 他 1951年 在 芝加哥 大学 
跟随 费米 得了 物理 硕士 就 到了 麻省理工学院 但在 芝加哥 
对 他 影响 最大 的 是 哲学家 卡尔纳普 Paul Carnap 
有意思 的 是 神经 网络 的 奠基者 之一 皮茨 也 
受惠 于 卡尔纳普 司马贺的/nr 回忆录/n 里/f 也/d 讲到/v 自己/r 在/p 
芝加哥/ns 时听/nr 卡尔纳普/l 的/uj 课/n 开始/v 启蒙/v 逻辑/n 从而 开始 
对 智能 相关 的 问题 感兴趣 但 后来 由于 和 
定理证明 逻辑 派 之间 的 冲突 司马/nr 贺就说/nr 自己/r 的/uj 
方法/n 是/v 在/p 批判/v 过度/n 数学化/i 和/c 形式化/v 这么 说来 
人工智能 的 两 大 派 逻辑 和 神经 网络 都发 
源于 老卡 卡尔纳普 那时 的 兴趣 是 归纳推理 这 成为 
所罗门 诺夫 毕生 的 研究 方向 所罗门 诺夫 后来 结识 
了 明斯基 和 麦卡锡 在 他们 的 影响 下 研究 
逻辑和 图灵机 达特茅斯 会议 时 他 受 麦卡锡 反向 图灵机 
和 乔姆斯基 文法 的 启发 发明 了 归纳 推理机 他 
的 工作 后来 被 万能 的 苏联 数学家 柯尔莫 格罗夫 
Kolmogorov 独立 地 发明 了 一遍 就是 现在 俗称 柯尔莫 
格罗夫 复杂性 和 算法 信息论 的 东西 中国 的 计算 
理论 学者 李明 现在 是 这个 领域 的 大牛 曾有 
专著 柯尔莫 格罗夫 1968年 开始 引用 所罗门 诺夫 的 文章 
使得 后者 在 苏联 的 名声 比 在 西方 更加 
响亮 所罗门 诺夫 的 另一个 观点 无限 点 Infinity Point 
后来 被 未来学家 库 兹 韦尔 改名 奇点 窃 为己有 
目前 AI 中 广泛 用到 的 贝叶斯 推理 也 有着 
所罗门 诺夫 的 开创性 痕迹 他 一生 并 没有 大富大贵 
大 部分 时间 都是在/nr 自己 的 咨询 公司 Oxbridge 牛津 
+ 剑桥 相当于 汉语 俗称 清 北 拿 政府 空军 
海军 ARPA 和 NIH NIH 资助 了 很多 AI 研究 
的 研究 经费 那 公司 只有 他 自己 一个 雇员 
伦敦 大学 皇家 哈洛 威 学院 Royal Holloway 后来 在 
苏联 学者 领导下 搞 柯尔莫 格罗夫 奖 他 是 第一 
届 获奖人 并 在 那里 兼职 教授 他 的 学术 
自传 1997年 发表 在 计算 理论 杂志 计算机 与 系统 
科学 上 明斯基 所谓 AI 孵化 出 计算 理论 的 
说法 不无道理 按照 麦卡锡 和 明斯基 的 说法 这 十 
个人 参加 了 达特茅斯 会议 但 现在 有 证据 表明 
会议 还有 其他 的 列 会 者 后来 一直 做 
神经网络 硬件 研究 从而 躲过 AI 几十年 过山车 的 斯坦福 
大学 电机系 教授 维 德罗 Bernard Widrow 后来 回忆 他 
也 去了 达特茅斯 并且 在 那儿 待了 一周 麦卡锡 原来 
的 计划 是 两个月 闭门 研讨 但 并非 所有 人 
都对 那个 事 那么 上心 纽/ns 厄尔/l 和/c 司马贺/nr 只待/i 
了/ul 一周/m 纽/ns 厄尔/l 后来/t 回忆/v 说/v 达特茅斯/l 会议/n 对/p 
他/r 和/c 司马贺/nr 没什么/l 影响/vn 尽管 是 十 仙 过海 
但/c 给/p 所有/b 人/n 留下/v 最/d 深/a 印象/n 的/uj 是/v 
纽/ns 厄尔/l 和/c 司马贺的/nr 报告/n 他们 公布 了 一款 程序 
逻辑 理论家 Logic Theorist 这个 程序 可以 证明 怀特海 和 
罗素 数学原理 中 命题逻辑 部分 的 一个 很大 子集 司马贺/nr 
回忆录/n 里/f 说/v 自己/r 学术/n 生涯/n 最/d 重要/a 的/uj 两年/m 
就是/d 1955年/tdq 和/c 1956年/tdq 这篇文章 后来 成了 AI 历史 上 
最重要 的 文章 之一 值得 注意 的 是 逻辑 理论家 
对 人工智能 后来 的 一个 分支 机器 定理证明 的 影响 
并 不大 哲学家 王浩 1958年 夏天 在 一台 IBM 704 
机上 只用 9 分钟 就 证明了 数学原理 中 一阶逻辑 的 
全部 定理 当然 数学原理 中 罗列 的 一阶逻辑 定理 只是 
一阶逻辑 的 一个 子集 目前 一阶逻辑 的 机器 定理证明 比起 
20 世纪 50 年代 已 有 长足 进展 但 仍然 
没有 高效 的 办法 毕竟 王浩 证明 的 是 一阶逻辑 
而 逻辑 理论家 只能 处理 命题逻辑 数学家 戴维斯 和 哲学家 
普特南 合作 沿着 王浩 的 思路 进一步 提出 了 戴维斯 
普特南 DP 证明 过程 后来 进一步 发展 为 DPLL 王浩 
对 逻辑 理论家 一直 持 鄙视 的 态度 认为 这 
是 一个 不 专业 的 东西 王浩 在 1983年 被 
授予 定理证明 里程碑 大奖 被 认为 是 定理证明 的 开山 
鼻祖 司马贺在/nr 他/r 的/uj 回忆录/n 里/f 则/d 对此/d 表示/v 不满/a 
认为 王浩 的 工作 抵消 了 逻辑 理论家 的 原创 
性 他们 的 初衷 并 不是 要 有效 地 证明 
定理 而是 研究人 的 行为 这是 后话 见 第 2 
章 自动 定理证明 兴衰 纪 麦卡锡 多年 后 回忆说 他/r 
从纽/nr 厄尔/l 和/c 司马/nr 贺的/nr IPL 语言 中学 到了 表处理 
这 成为 他 后来 发明 LISP 的 基础 明斯基/nr 后来/t 
接受/v 采访/v 时说他/nr 对/p 纽/ns 厄尔/l 和/c 司马/nr 贺的/nr 逻辑 
理论家 印象深刻 因为 那是/nr 第一 个 可 工作 的 AI 
程序 但 事实上 明斯基 在 当时 为 大会 写 的 
总结 里 对 逻辑 理论家 只是 轻描淡写 麦卡锡 和 明斯基 
明显 是 一伙 的 会议 是 他们 发动 的 旨在 
创立 一门 新学科 但/c 纽/ns 厄尔/l 和/c 司马贺却/nr 抢了/i 他们/r 
的/uj 风头/n 美国 20 世纪 50 年代 的 学术 氛围 
不免 浮躁 这 一帮人 又 都是 年轻气盛 野心 十足 4 
. 会议 之后 达特茅斯 会议 后 不久 1956年 9月 IRE 
后来 改名 IEEE 在 麻省理工学院 召开 信息论 年会 麦卡锡 受邀 
做 一个 对 一个 月 前 达特茅斯 会议 的 总结 
报告 这/r 引起/v 了/ul 纽/ns 厄尔/l 尤其/d 是/v 司马贺的/nr 不满/a 
他们 认为 麦卡锡 只能 聊 没 干货 而/c 达特茅斯/l 会议/n 
唯一/b 的/uj 干货/n 是/v 纽/ns 厄尔/l 和/c 司马贺的/nr 程序/n 逻辑 
理论家 打了 一圈 架 最后/f 纽/ns 厄尔/l 和/c 司马贺/nr 做了/i 
妥协/v 麦卡锡 先 做 总结报告 但/c 最后/f 还是/c 由/p 纽/ns 
厄尔/l 和/c 司马贺讲/nr 他们/r 的/uj 逻辑 理论家 并 发表 一篇 
题为 逻辑 理论 机器 Logic Theory Machine 的 文章 明斯基 
认为 是 他 的 协调 起了 作用 但 纽 厄尔 
晚年 则 只对 香农 的 邀请 有 印象 而/c 司马贺的/nr 
回忆录/n 则/d 说是/i 大会/n 的/uj 主席/n 罗森/nr 布拉特/l 和/c 司马/nr 
贺/nr 散了/i 很长/i 一圈/m 步才/nr 了断/v 明斯基 机敏 异常 讲话 
时带/nr 幽默 但在 对 这段 历史 的 重构 中 却给 
人 印象 有点 太 刁滑 cynical 原因 也 不难 猜出 
研究 历史 有时 必须 得 全方位 空间 或 时间 上 
的 接近 不见得 就 真实 太 接近 时 当事人 还 
都 活着 还 在 一个 圈子里 混 不方便 互相 揭短 
但在 接近 生命 末期 或者 功成名就 或者 人之将死 或者 对头 
已死 无所顾忌 敞开 了 说 有时 虽有 夸张 但 一不留神 
就会 流露 真话 纽 厄尔 属于 后者 明斯基 刁滑 可能 
和他/nr 身体 好 有关系 偌大 岁数 也没 不惑 觉得 还有 
好长 的 路 要走 科学 达人 戴森 Freeman Dyson 在 
他 的 一面 多彩 的 镜子 一 书中 借鉴 过 
伯林 Isaiah Berlin 刺猬 与 狐狸 的 比喻 刺猬 是 
那些 构建 理论 体系 的 人 而 狐狸 则是 那些 
解决 问题 的 人 在 他 眼里 爱因斯坦 哥德尔 是 
刺猬 而 费米 冯 诺伊曼 属 狐狸 科学史 有时 刺猬 
得势 有时 狐狸 当道 是不是/l 可以/c 说/v 纽/ns 厄尔/l 和/c 
司马贺/nr 更像/i 刺猬/n 而 麦卡锡 和 明斯基 更像 狐狸 呢 
具体 到 AI 的 源头 和 达特茅斯 会议 麦卡锡 认为 
他 和 明斯基 是 发起人 纽/ns 厄尔/l 和/c 司马贺是/nr 外人 
是 搅局 者 明斯基/nr 的/uj 解释/v 是/v 纽/ns 厄尔/l 和/c 
司马贺一/nr 开始/v 的/uj 出发点/n 是/v 心理学/n 这与 麦卡锡 和他/nr 本人 
的 背景 不符 但在 随后 的 十年 里 他 本人 
更多 地 走向 心理学 而/c 纽/ns 厄尔/l 和/c 司马贺更/nr 靠近/v 
AI 也 没什么 矛盾 麦卡锡 除了 和 明斯基 关系 紧密 
外 和 其他 AI 群体 的 交流 并不多 在 所谓 
其他 群体 中 最 有 影响 的 当属 卡内基 梅隆 
那一 派了 麦卡锡 晚年 回忆 说 那时 群体 之间 的 
沟通 主要 是 通过 研究生 研究生 就像 大佬 们 的 
大使 后来 斯坦福大学 卡内基 梅隆 大学 麻省 理工学院 的 学生 
确实 互为 教授 门户之见 随着 时间 的 推移 逐渐 被 
抹 平了 总之 1956年 IRE 信息论 年会 是 个 值得 
纪念 的 会议 除了/p 纽/ns 厄尔/l 和/c 司马贺/nr 发表/v 的/uj 
那篇/i 文章/n 之外/f 心理学家 米勒 George Miller 发表 了 人类 
记忆 和对/nr 信息 的 储存 Human Memory and the Storage 
of Information 这是 那篇 著名 的 文章 魔力 数字 七 
The Magic Number Seven 的 另一 个 版本 不知 算不算 
一稿 多发 同 在此 会上 伟大 的 乔姆斯基 则 发表 
了 语言 描述 的 三种 模型 Three Models for the 
Description of Language 该文 证明 了 有限 状态 句法 不能 
表达 某类 语言 这是 乔姆斯基 分层 的 起源 文中 引用 
了 还没 出版 的 不朽 名著 句法结构 乔姆斯基 当时 刚刚 
到 MIT 现代 语言 学系 该系 后来 演变 为 语言学 
与 哲学系 出任 助理 教授 并在 MIT 电子 实验室 做 
机器 翻译 的 研究 尽管 乔老爷 后来 是 反政府 斗士 
但/c 有点/n 反讽/v 的/uj 是/v 他/r 早期/t 的/uj 研究/vn 经费/vn 
都/d 来自/v 美国/ns 空军/n 和/c 海军/n 从 参与者 的 角度 
看 大家 会 认为 这次 IRE 的 信息 论 年 
会比 达特茅斯 会议 更重要 影响 也 更 深远 米勒 回忆说 
他 当时 直觉 认识 到 实验 心理学 理论 语言学 认知过程 
的 计算机 模拟 都是/nr 一个 大家伙 里面 的 组成部分 这个 
所谓 的 大家伙 就是 现在 的 人工智能 加 认知科学 吧 
明斯基 回忆 自己 在 达特茅斯 会议 期间 在 纸 上画 
了 一个 几何定理 证 明器 的 设计 并 手动 模拟 
证明了 等腰 三角形 的 一个 定理 会后 的 1956年 9月 
IBM 招了 新 毕业 的 物理 博士 格兰特 Herb Gelernter 
实现 明斯基 的 几何 定理证明 器 麦卡锡/nr 此时/c 受到/v 纽/ns 
厄尔/l 和/c 司马贺的/nr 影响/vn 建议 在 Fortran 里 实现 表处理 
语言 作为 实现 语言 这个 项目 在 1959年 实现 后 
IBM 削减 了 对 AI 的 投入 把 这个 项目 
砍掉 了 理由 是 IBM 不想 给人以 机器 可以 替 
代人 的 印象 IBM 再次 资助 AI 是 20多 年后 
的 1983年 了 现在 好像 IBM 百年 老店 只能靠 AI 
系统 沃森 Watson 翻身 了 麦卡锡 1958年 离开 达特茅斯 学院 
去了 MIT 帮助 创立 了 MIT 的 MAC 项目 他 
和 明斯基 一起 领导 了 MAC 项目 中的 AI 实验室 
1962年 他 再次 跳槽 到 斯坦福大学 之后 明斯基 又和 佩珀 
特 Seymour Papert 合作 计算机 操作 系统 里 分时 的 
概念 是由 麦卡锡 在 MAC 项目 中 首创 的 他 
回忆 说 当时 机器 太少 但 等着 上机 的 学生 
很多 于是 就 发明 了 分时系统 按说 分时 系统 的 
贡献 要比 麦卡锡 后来 的 AI 贡献 彰显 得多 但 
麦卡锡 得 图灵奖 可 不是 靠 分时 这就 像 爱因斯坦 
得 诺贝尔奖 没 靠 相对论 一样 从 这个 意义 上 
AI 有点像 哲学 由此 衍生 出 很多 问题 而 对 
这些 问题 的 解决 产生 出 许多 子 学科 一旦 
这些 子 学科 独立 就 不再 待见 AI 了 另 
一个 例子 是 卡内基 梅隆 大学 的 微 核心 操作系统 
MACH 其 最早 的 发源 是 在 卡内基 梅隆 大学 
的 雷蒂 Raj Reddy 搞 的 分布式 传感 网络 MACH 
领导者 拉希德 Rick Rashid 后来 加入 微软 MACH 变成 微软 
后来 操作 系统 的 基础 他 本人 也 变成 微软 
负责 技术 的 决策 者 之一 现在 计算机科学 已成为 成熟 
的 学科 每个 计算机系 大都有 三 拨 人 理论 系统 
和 AI 20 年前 的 美国 计算机 圈子 曾有 一种 
说法 理论 和 系统 的 人 互相 看不起 但又 同时 
看不起 AI 的 人 AI 这几年 火了 但 曾几何时 AI 
的 人 是 被 压迫者 哲学 曾经 孕育 了 科学 
但 一旦 问题 被 确定 就 分离 成为 单独 的 
科学 最新 的 例子 是 逻辑学 现在 的 逻辑学家 都在 
数学系 和 计算机 系 哲学系 被 彻底 空洞化 哲学家 丹 
尼特 Daniel Dennett 曾说/nr AI 就是 哲学 按照 明斯基 的 
说法 人工智能 就是 先锋派 的 计算机 科学 MAC 项目 孕育 
了 计算机 科学 中 很多 原创 的 概念 以至于 明斯基 
后来 认为 UNIX 系统 是 落后 的 东西 因为 他们 
丢掉 了 很多 Multics 中的 精华 利 克莱德 Joseph Licklider 
是 信息 时代 的 预言 家和 布道者 他 20 世纪 
60 年代 初期 在 美国 国防部 先进 研究 项目 局 
ARPA 创办 指挥 与 控制 C2 办公室 后来 演变 为 
行为 科学 及 指挥 与 控制 办公室 最终 变成 有权有势 
的 信息 科技 办公室 IPTO 正是 利 克莱德 最早 想 
到了 人机 协同 计算机网络 未来 图书馆 等 先进 概念 而 
他 的 行为科学 计划 也 曾 资助 过 监控 项目 
不知 那 是不是 受到 奥威尔 的 启发 1968年 参议院 多数党 
领袖 曼斯菲尔德 对 ARPA 的 资助 方向 不满 他 认为 
国防部 的 钱 不能 被 用于 军事 目的 之外 非 
军事 目的 的 项目 应该 由 美国 国家 科学 基金会 
NSF 负责 ARPA 改名 DARPA 更 强调 国防 利 克莱德 
遂于 1968年 离开 ARPA 去了 MIT 担任 MAC 项目 负责人 
统筹 MIT 的 计算机 科学 实验室 和 人工智能 实验室 人们 
认识 到 利 克莱德 的 贡献 太晚 了 他 于 
1990年 过世 计算机科学 最 重要 的 实验室 之一 施乐 PARC 
的 创始人 泰勒 Robert Taylor 曾称利/nr 克莱德/nr 是/v Johnny Appleseed 
这 是 美国 18 世纪 到 19 世纪 的 园丁 
查普曼 John Chapman 的 外号 他 把 苹果树 的 种子 
遍 撒 美国 利 克莱德 1915 1990 20 世纪 70 
年代 初期 在 海尔 梅尔 George Heilmeirer 任内 DARPA 大 
砍 AI 预算 协调 政府 和 AI 实验室 的 工作 
变得 头绪 繁多 明斯基 决定 从 AI 实验室 退位 让 
他 刚 毕业 的 学生 温斯顿 Patrick Winston 接手 尽管 
明斯基 说 他 不喜 事务性 工作 但/c 他/r 的/uj 采访/v 
和/c 回忆/v 中/f 触及/v 的/uj 话题/n 总是/c 和/c 联邦/n 政府/n 
的/uj 资助/n 有关/vn 温斯顿 后来 回忆 时说 管理 一个 成功 
的 实验室 要 管理 好 三个 圈 的 交集 出资人 
主要 是 政府 科学 上有 创建 有 国计民生 的 价值 
他 试图 说服 几任 ARPA 的 头儿 别把 AI 当作 
一个 几年 一次 的 项目 而是 长期 而 独立 的 
一门 学科 另外 他 对比 了 早期 ARPA 和 NSF 
的 不同 NSF 是 20 世纪 80 年代 才 开始 
资助 AI 研究 的 且 给钱 少 而且都 是 同行 
评议制 结果 是 越有 成就 的 拿 的 钱 越多 
但 很少 会 有 根本性 的 原创 性 贡献 ARPA 
早期 都是 头 儿们 说了算 好处 是 如果 管事 的 
头儿 们 品 味好 肯定 会 支持 好 东西 这 
一点 也 值得 一些 科技 人 借鉴 大型项目 决策者 的 
品味 可以 超越 透明 计算 吗 再说 回 海尔 梅尔 
他 以 AI 不能 帮助 造 武器 打仗 为 理由 
削减 了 对 AI 的 大规模 经费 但 同时 却 
重金 资助 了 隐形 飞机 和 空间 武器 技术 使 
美国 在 相关 领域 一直 保持 领先 ARPA 资助 的 
这类 项目 要 是 通过 同行 评议 是 很难 实施 
的 ARPA 几乎 在 同时 也 支持 了 ARPANET 后来 
演变 成 互联网 有意思 的 是 海尔 梅尔 从 ARPA 
离任 后 去了 德州仪器 TI 做 CTO 在 TI 却 
大力提倡 AI ARPA 对 AI 的 资助 在 克 柔 
克 Steve Crocker 手里 才 逐步 恢复 大家 知道 克 
柔 克 是 互联网 的 先驱 之一 再 后来 的 
ARPA 信息技术 办公室 IPTO 的 负责人 中 还有 图灵 奖 
获得者 萨瑟兰 Ivan Edward Sutherland 也对 AI 继续 投入 精 
英制 风格 的 ARPA 更 适合 做 大型 开创性 项目 
成功 取决于 少数 决策者 而以 民主制 为基础 的 NSF 历来 
就是 小 规模 资助 基础 研究 5 . 预测 未来 
会有 奇点 吗 司马贺/nr 1957年/tdq 曾/d 预言/v 十年/m 内/n 计算机/n 
下棋/v 会/v 击败/v 人/n 1968年 麦卡锡 和 象棋大师 列维 David 
Levy 打赌 说 十年 内 下棋 程序 会 战胜 列维 
最后 赔了 列维 两千块 乐观 的 预言 总会 给 对手 
留下 把柄 德雷 弗斯 后来 每年 都拿/nr 此事 嘲讽 AI 
说 计算机 下下 跳棋 还行 下象棋/n 连/nr 十岁/m 的/uj 孩子/n 
都干/nr 不过/c 这 便宜 话 一直 说到 1997年 IBM 的 
下棋 程序 深蓝 击败 了 卡斯帕罗夫 这 真是 四十年 太久 
只争朝夕 啊 在 1995年 卡斯帕罗夫 还在 批评 计算机 下棋 缺乏 
悟性 insights 但 1996 年时 他 已经 开始 意识到 深蓝 
貌 似有 悟性 了 而 两 年间 深蓝 的 计算 
能力 只不过 提高 了 一倍 而已 机器 有 没有 悟性 
的 边界 其实 就是 人 的 解释 能力 的 极限 
量变 到 质变 的 临界点 就是 人 的 解释 能力 
人 解释 不了 的 东西 就有 悟性 解释 了 的 
东西 就 没有 悟性 司马贺和/nr 日本/ns 计算机/n 科学家/n 宗像/nr 俊/a 
则/d Toshinori Munakata 合 写了 篇 解气 的 文章 人工智能 
的 教训 AI Lessons 登在 ACM 通讯 上 当然 德雷 
弗斯 们 还 可以 将 计算机 仍然 不能 干什么 加上 
若干 个 仍然 接着 批评 明斯基 1968 年在 库布里克 的 
电影 2001 太空 漫游 的/uj 新闻/n 发布/v 会上/t 曾/d 大放厥词/l 
说/v 30/m 年内/t 机器/n 智能/n 可以/c 和/c 人有/i 一拼/i 1989年 
又 预言 20年 可以 解决 自然语言 处理 现在 我们 恐怕 
还 不能 说 机器 翻译器 令人满意 吧 过分 乐观 的 
另一个 原因 照 明斯基 自己 的 说法 是 一门 年轻 
的 学科 一 开始 都 需要 一点 过度 销售 excessive 
salesmanship 但是 过头 了 不免 被人 当作 狗皮膏药 或 炼金术 
2006年 达特茅斯 会议 50周 年时 当时 的 10位 与会者 中有 
5位 仙逝 活着 的 5位 摩尔 麦卡锡 明斯基 塞 弗里奇 
和 所罗门 诺夫 在 达特茅斯 团聚 忆往昔 展 未来 2006年 
会议 50 年后 当事人 重聚 达特茅斯 左起/nr 摩尔 麦卡锡 明斯基 
塞 弗里奇 所罗门 诺夫 参会 人 之一 霍维茨 Horvitz 现在 
是 微软 实验室 的 头目 他/r 和他/nr 老婆/n 拿出/v 一笔/m 
钱在/nr 斯坦福大学/nt 捐助/v 了/ul 一个/m AI100 3 的 活动 在下面 
100 年里 各路 豪杰 聚会 每 5年 出个 AI 进展 
报告 第一 期 出版 于 2016年 但 里面 并无 什么 
干货 乔姆斯基 晚年 边 做学问 边做 斗士 2015年/tdq 3月/tdq 他/r 
和/c 物理学家/n 克劳斯/l 对话/n 时被/nr 问及/v 机器 可以 思维 吗 
他 套用 计算机 科学家 戴客/nr 斯特拉 Dijkstra 的 说法 反问 
潜艇 会 游泳 吗 如果 机器人 可以 有意识 consciousness 的 
性质 机器人 可以 被 认为 有意识 吗 他 进一步 说 
意识 是 相对 简单 的 而 前意识 preconsciousness 是 困难 
的 问题 他 把 AI 分成 工程 的 和 科学 
的 工程 的 如 自动 驾驶 车等/nr 能 做出 对 
人类 有用 的 东西 科学 的 一面 乔老爷 明显 不 
认可 他 引用 图灵 的话 这 问题 没有 讨论 的 
意义 too meaningless to deserve discussion 当 一帮 奇点 理论 
的 粉丝 带着 正面 的 期望 采访 乔姆斯基 时 他 
却对 人工智能 这个 被 他 深刻 影响 过 的 学科 
没 太 当回事 他 认为 气候 和 毁灭性 武器 是 
比 奇点 更 紧迫 的 问题 这 算 有意 回避 
吧 明斯基 在 2012年 接受 他 的 学生 预言家 奇点 
理论 炮制者 库 兹 韦尔 的 采访 时说/nr 他 相信 
奇点 的 到来 可能 就 在 我们 的 有生之年 两位 
斯基 在 麻省理工学院 150 周年 纪念会 上 分 在 一个 
小组 讨论 里 却只 打了 下 太极 并 没有 针锋相对 
明斯基 2016年 1月 24日 在 波士顿 去世 据说 为了 等 
奇点 他 老人家 把 自个儿 冷冻 了 明斯基 和 乔姆斯基 
在 麻省理工学院 150 周年 纪念会 上 同室 不 操戈 并没 
针锋相对 参考文献 指南 人工智能 是 一门 新学科 历史 的 读物 
并不多 波 登 的 认知科学 历史 Boden 2008 和 尼尔森 
的 人工智能 探究 Nilsson 2010 是 两本 严肃 的 读物 
麦克 达克 Pamela McCorduck 曾是/nr 费根 鲍姆 的 御用 作家 
她 1979年 写 的 能 思考 的 机器 Machines Who 
Think 一 书 无论是 取材 还是 立意 从/p 今天/t 的/uj 
角度/n 看/v 都/d 略微/d 过时/t 尼尔森 是 人工智能 学科 的 
早期 参与者 也 一直 是 领导者 之一 他 多年 担任 
SRI 的 人工智能 部门 负责人 和 斯坦福 大学 计算机 系 
主任 是 圈里人 纽 厄尔 1981年 的 文章 探讨 了 
如何 研究 人工智能 的 历史 他 总结 了 人工智能 历史 
中 不同 思想 的 对立 他 的 方法 也 可以 
用来 研究 更 广义 的 计算机 科学 甚至 可以 拓展 
到 不同 科学 领域 和 哲学 尽管 这是 30 多年前 
的 文章 但 今天 读来 仍 有启发 明尼苏达 大学 的 
巴贝奇 研究所 是 专门 研究 计算机 科学 历史 的 机构 
主持 工作 的 诺 伯格 采访 了 多名 计算机 科学家 
并 做了 录音 这些 被 采访 的 人中 也 有不少 
人工智能 学者 例如 纽 厄尔 麦卡锡 明斯基 温斯顿 布坎南 等 
听 这些 人 的 录音 采访 和 阅读 正儿八经 的 
文章 完全 是 两种 不同 的 体验 采访 中 的 
语调 幽默 包含 了 很多 文章 不 可能 有的 微妙 
细节 除了 录音 采访 麦卡锡 还有 个 西蒙斯 基金会 的 
更 正式 的 视频 采访 雅各布森 Annie Jacobsen 的 五角大楼 
大脑 Pentagon s Brain 是 关于 ARPA 的 详实 而 
有趣 的 历史 从这 本书 中 我们 可以 看到 信息 
科技 一直 不是 ARPA 的 主打 方向 但 互联网 这个 
ARPA 歪打正着 的 项目 却是 它 最好 的 投资 1 
我 故意 没有 在 冯 和 诺伊曼 之间 加 那个 
讨厌 的 点儿 因为 在 更多 时候 查找 参考文献 时 
他 的 姓 是 列 在 V 下 而 不是 
N 下 2 哈佛 的 Fellow 还是 挺 值钱 的 
历史 上 人数 不多 蒯因/nr 王浩 库恩 在 变成 正式 
教授 之前 都 做过 乔姆斯基 几乎 在 同时 也 是 
哈佛 的 Fellow 3 AI100 活动 在 斯坦福 有个 网站 
https / / ai100 . stanford . edu / hljs 
center 第 2 章 自动 定理证明 兴衰 纪第3/nr 章 从 
专家 系统 到 知识图谱 第 4 章 第五代 计算机 的 
教训 第 5 章 神经网络 简史 第 6 章 计算机 
下棋 简史 机 定 胜 人 人定胜天 第 7 章 
自然语言 处理 第 8 章 向 自然 学习 从 遗传算法 
到 强化 学习 第 9 章 哲学家 和 人工智能 第 
10 章 人 是 机器 吗 人工智能 的 计算 理论 
基础 第 11 章 智能 的 进化 第 12 章 
当 我们 谈论 生死 时 我们 在 谈论 什么 附录 
1 图灵 小传 附录 2 人工智能 前史 图灵 与 人工智能 
附录 3 冯 诺伊曼 与 人工智能 附录 4 计算机 与 
智能 参考文献 人名 对照 阅读 全文 http / / gitbook 
. cn / gitchat / geekbook / 5 b 5 
e 8 f d 7 9 1 8 3 3 
5 3 8 d 3 9 4 4 d 9 
f 自然语言 处理 NLP 在 现代 深度 学习 生态 中 
越来越 常见 从 流行 的 深度 学习 框 架到 云端 
API 的 支持 例如 Google 云 Azure AWS 或 Bluemix 
NLP 是 深度 学习 平台 不可或缺 的 部分 尽管 已经 
取得 了 令人 难以置信 的 进步 但 构建 大 规模 
的 NLP 应用 依然 还 有 极大 的 挑战 在 
学习 研究 和 生产 部署 之间 还 存在 很多 摩擦 
作为 当前 市场 上 最大 的 会话 环境 之一 Facebook 
已经 面对 构建 大 规模 NLP 应用 的 挑战 有 
一些 年头 了 最近 Facebook 的 工程 团队 开源 了 
第一 个 版本 的 Pytext 一个 基于 PyTorch 的 NLP 
框架 可以 用 来 构建 高效 的 NLP 解决方案 PyText 
的 最终 目标 是 简化 端对端 的 NLP 工作流 实现 
为了 实现 这 一 目标 PyText 需要 解决 当前 NLP 
流程 中 的 一些 问题 其中 最 令人 头疼 的 
就是 NLP 应用 在 实验 环境 和 生产 环境 的 
不匹配 问题 更好 地 平衡 NLP 实验 和 生产 部署 
现代 NLP 解决方案 通常 包含 非常重 的 实验 环节 在 
这个 阶段 数据 科学家 们 将 借鉴 研究 文件 快速 
测试 新的 想法 和 模型 以便 达成 一定 的 性能 
指标 在 实验 阶段 数据 科学家 倾向于 使用 容易 上手 
界面 简单 的 框架 以便 快速 实现 高级 动态 的 
模型 例如 PyTorch 或 TensorFlow Eager 当 需要 部署 到 
生产 环境 时 动态图 模型 的 固有 局限性 就 带了 
新的 挑战 这一 阶段 的 深度 学习 技术 需要 使用 
静态 计 算图 并且 需要 为 大 规模 计算 进行 
优化 TensorFlow Caffe2 或 MxNet 都 属于 这 一 类型 
的 技术 栈 结果 是 大型 数据 科学 团队 不得不 
为 实验 和 生产 部署 使用 不同 的 技术 栈 
imagePyTorch 是 最早 解决 了 快速 实验 与 规模化 部署 
之 间 冲突 的 深度 学习 框架 之一 基于 PyTorch 
构建 的 PyText 为 NLP 领域 应用 了 这些 解决 
实验 环境 与 生产 部署 之 间 冲突 的 优化 
原则 理解 PyText 从 概念 角度 触发 PyText 被 设计 
为 实现 以下 四 个 基本 目标 尽可能 简单 快速 
的 实现 新 模型简化 将 预 构建 模型 应用于 新 
数据 的 工作量 同时 为 研究者 和 工程师 定义 清晰 
的 工作 流 以便 构建 和 评估 模型 并以/c 最小/a 
的/uj 代价/n 上线/n 模型/n 确保/v 部署/n 的/uj 模型/n 在/p 推理/v 
时/n 具有/v 高/a 性能/n 低 延迟 高 吞吐量 PyText 的 
处理 容量 最终 打造 的 建模 框架 可供 研究者 和 
工程师 构建 端 到 端的 训练 或 推理 流水线 当前 
的 PyText 实现 涵盖 了 NLP 工作流 声明 周 期中 
的 基本 环节 为 快速 实验 原始 数据处理 指标 统计 
训练 和 模型 推理 提供 了 必要 的 接口 一个 
高 层级 的 PyText 架构图 可以 清晰 地 展示 这些 
环节 如何 封装 了 框架 的 原生 组件 image 如上 
图 所示 PyText 的 架构 包含 以下 组成部分 Task 将 
多个 用于 训练 或 推理 的 组件 拼装 为 一个 
流水线 Data Handler 处理 原始 输入 数据 贮备 张量 批 
数据 以便 送入 模型 Model 定义 神经 网络 的 架构 
Optimizer 封装 模型 参数 优化 过程 基于 模型 的 前馈 
损失 进行 优化 Metric Reporter 实现 模型 相关 指标 的 
计算 和 报表 提供 Trainer 使用 数据 处理器 模型 损失/n 
和/c 优化/vn 器/n 来/v 训练/vn 和/c 筛选/v 模型/n Predictor/w 使用 
数据 处理器 和 模型 对 给定 的 数据 集 进行 
推理 Exporter ONNX8 导出 训 练好 的 PyTorch 模型 到 
Caffe2 图 你 可以 看到 PyText 利用 ONNX Open Neural 
Network Exchange Format 将 模型 从 实验 环境 的 PyTorch 
格式 转换 为 生产 环境 的 Caffe2 运行 模型 PyText 
预置 了 众多 NLP 任务 组件 例如 文本 分类 单词 
标注 语义分析 和 语言 模型 等 可以 快速 实现 NLP 
工作流 类似 的 PyText 使用 上下文 模型 介入 语言 理解 
领域 例如 使用 SeqNN 模型 用于 意图 标注 任务 或者 
使用 一个 上下文 相关 的 意图 槽 模型 用于 多个 
任务 的 联合 训练 从 NLP 工作流 的 角度 来说 
PyText 可以 快速 将 一个 思路 从 实验 阶段 转换 
为 生产 阶段 一个 PyText 应用 的 典型 工作流 包含 
如下 的 步骤 image 用 PyText 实现 模型 确保 测试 
集上 的 离线 指标 正确 将 模型 发布 到 打包 
的 基于 PyTorch 的 推理 服务 在 实时 样本 上 
执行 小规模 评估 自动 导 出到 Caffe2 网络 不过 在 
有些 情况 下 例如 当 使用 复杂 的 流程 控制 
逻辑 时 或者 使用 自 定义数据 结构式 PyTorch 1.0 还 
不支持 如果 第 3步 不支持 那么 使用 Py Torch C 
+ + API9 重写 模型 并 封装 为 一个 Caffe2 
操作符 将 模型 发布 为 生产 就绪 的 Caffe2 预测 
服务 并 启动 使用 PyText 上手 PyText 非常简单 按 标准 
python 包的/nr 方法 安装 框架 $ pip install pytext nlp 
然后 我们 就 可以 使用 一个 任务 配置 来 训练 
NLP 模型 了 pytext $ cat demo / configs / 
docnn . json { task { D o c C 
l a s s i f i c a t 
i o n T a s k { data _ 
handler { train _ path tests / data / train 
_ data _ tiny . tsv eval _ path tests 
/ data / test _ data _ tiny . tsv 
test _ path tests / data / test _ data 
_ tiny . tsv } } } } $ pytext 
train demo / configs / docnn . jsonTask 是 PyText 
应用 中 的 用来 定义 模型 的 核心 部件 每 
一个 任务 都 有一个 嵌入 的 配置 它 定义 了 
不同 组件 之间 的 关系 如 下面 代码 所示 from 
word _ tagging import ModelInputConfig TargetConfig class WordTaggingTask Task class 
Config Task . Config features ModelInputConfig = ModelInputConfig targets TargetConfig 
= TargetConfig data _ handler W o r d T 
a g g i n g D a t a 
H a n d l e r . Config = 
W o r d T a g g i n 
g D a t a H a n d l 
e r . Config model WordTaggingModel . Config = WordTaggingModel 
. Config trainer Trainer . Config = Trainer . Config 
optimizer OptimizerParams = OptimizerParams scheduler Optional SchedulerParams = SchedulerParams metric 
_ reporter W o r d T a g g 
i n g M e t r i c R 
e p o r t e r . Config = 
W o r d T a g g i n 
g M e t r i c R e p 
o r t e r . Config exporter Optional T 
e x t M o d e l E x 
p o r t e r . Config = T 
e x t M o d e l E x 
p o r t e r . Config 一旦 模型 
训练 完毕 我们 就 可以 对 模型 进行 评估 也 
可以 导出 为 Caffe2 格式 pytext $ pytext test $ 
CONFIG pytext $ pytext export output path exported _ model 
. c2 $ CONFIG 需要 指出 的 是 PyText 提供 
了 可扩展 的 架构 可以 定制 扩展 其中 任何 一个 
构建 模块 PyText 代表 了 NLP 开发 的 一个 重要 
里程碑 它 是 最早 解决 实验 与 生产 匹配 问题 
的 框架 之一 基于 Facebook 和 PyTorch 社区 的 支持 
PyText 可能 有 机会 称为 深度 学习 生态 中 最重要 
的 NLP 技术 栈 之一 汇 智 网 翻译 整理 
转载 请 标明 出处 Pytext 简介 博主 github https / 
/ github . com / MichaelBeechan 博主 CSDN https / 
/ blog . csdn . net / u011344545 = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = 概念 篇 https / / blog . csdn 
. net / u011344545 / article / details / 89525801 
技术篇 https / / blog . csdn . net / 
u011344545 / article / details / 89526149 人才篇 https / 
/ blog . csdn . net / u011344545 / article 
/ details / 89556941 应用 篇 https / / blog 
. csdn . net / u011344545 / article / details 
/ 89574915 下载 链接 https / / download . csdn 
. net / download / u011344545 / 11147085 = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = 清华 AMiner 团队 AMiner . org 自然语言 处理 
论文 发表 情况 来自 维 普 智立方 1 国外 实验室 
及 人才 介绍 AMiner 基于 发表 于 国际 期刊 会议 
的 学术 论文 对 自然 语言 处理 领域 全球 h 
index 排序 top1000 的 学者 进行 计算 分析 绘制 了 
该 领域 顶尖 学者 全球 分布 地图 根据 上图 我们 
可以 得出 以 下结论 从 国家 来看 美国 是 自然 
语言 处理 研究 学者 聚集 最多 的 国家 英国 德国 
加拿大 和 意大利 紧随其后 从 地区 来看 美国 东部 是 
自然 语言 处理 人才 的 集中地 而 西欧 美国 西部 
等 其他 先进 地区 也 吸引 了 大量 自然语言 处理 
的 研究 者 全球 自然语言 处理 顶尖 学者 的 h 
index 平均数 为 59 h index 指数 大于 60 的 
学者 最 多占 41% h index 指数 在 40 到 
60 之间 的 学者 次之 占 比 40% 自然语言 处理 
领域 顶尖 学者 男性 占 比 91% 女性 占 比 
9% 男女 比例 不 均衡 AMiner 对 顶尖 人才 的 
迁徙 路径 做了 分析 由 上图 可以 看出 各国 自然语言 
处理 顶尖 人才 的 流失 和 引进 是 相对 比较 
均衡 的 其中 美国 是 自然 语言 处理 领域 人才 
流动 大国 人才/n 输入/v 和/c 输出/v 幅度/n 都/d 大幅度/d 领先/n 
且 从 数据 来看 人才 流入 略大于 流出 英国 德国 
加拿大/ns 和/c 中国/ns 等国/i 落后/a 于/p 美国/ns 其中/r 英国/ns 和/c 
加拿大/ns 有/v 轻微/d 的/uj 顶尖/n 人才/n 流失/v 现象/n 以下 选取 
在 ACL EMNLP NAACL COLING 等 4 个 会议 在 
近 5 年 累计 发表 10 次 以上 论文 的 
国外 学者 及其 所在 实验室 做 简要介绍 Chris DyerChris Dyer 
卡内基 梅隆 大学 助理 教授 2010 年在 马里兰 大学 获 
语言学 博士学位 主要 兴趣 领域 是 机器学习 自然语言 处理 和 
语言学 的 交叉 研究 比较 感兴趣 的 一些 课题 有 
机器翻译 用于 语言 处理 的 神经 网络 模型 语言 建模 
特征 归纳 和 表示 学习 大 数据 算法 音乐 概率模型 
等 卡内基 梅隆 大学 语言 技术 研究 所 主要 研究 
内容 包括 自然 语言 处理 计算 语言学 信息提取 信息检索 文本 
挖掘 分析 知识 表示 机器学习 机器翻译 多通道 计算 和 交互 
语音 处理 语音 界面 和 对话 处理 等 Christopher D 
. M a n n i n g C h 
r i s t o p h e r D 
. Manning 斯坦福 大学 计算机 科学 与 语言 学习 的 
教授 1994 年在 斯坦福 大学 获得 博士 学位 他 致力于 
研究 能够 智能 处理 理解 和 生成 人类 语言 材料 
的 计算机 Manning 在 自然 语言 处理 的 深度 学习 
领域 有着 深入研究 包括 递归 神经网络 情感 分析 神经网络 依赖 
分析 等 Manning 曾获/nr ACL CILING EMNLP 的 最佳 论文 
奖 斯坦福 大学 自然 语言 处理 小组 包括了 语言学 和 
计算机 科学系 的 成员 是 斯坦福 人工智能 实验室 的 一部分 
主要 研究 计算机 处理 和 理解 人类 语言 的 算法 
工作/vn 范围/n 从/p 计算/v 语言学/n 的/uj 基本/n 研究/vn 到/v 语言/n 
处理/v 的/uj 关键/n 应用/v 技术/n 均/d 有/v 涉猎/v 涵盖 句子 
理解 自动 问答 机器翻译 语法 解析 和 标签 情绪/n 分析/vn 
和/c 模型/n 的/uj 文本/n 和/c 视觉/n 场景/n 等/u 该/r 小组/n 
的/uj 一个/m 显著/a 特征/n 是/v 将/d 复杂/a 和/c 深入/v 的/uj 
语言/n 建模/n 和/c 数据/n 分析/vn 与/p NLP 的 创新 概率 
机器 学习 和 深度 学习 方法 有效 地 结合 在 
一起 Dan KleinDan Klein 伯克利大学 自然语言 处理 小组 负责人 2004 
年在 斯坦福 大学 取得 计算机 科学 的 博士 学位 主要 
研究 重点 是 自然 语言 信息 的 自组织 兴趣 领域 
包括 无 监督 的 语言 学习 机器翻译 NLP 的 高效 
算法 信息提取 语言 丰富 的 语言 模型 NLP 的 符号 
和 统计 方法 的 集成 以及 历史 语言学 等 多次 
在 国际 顶级 会议 上 发表 论文 并 获奖 如在 
2012 年 EMNLP 上 获得 Distinguished Paper Training Factored PCFGs 
with Expectation Propagation 2017 年 Dan Klein 在 ACL EMNLP 
NAACL COLING 等 会议 发表 的 论文 有 伯克利大学 自然语言 
处理 小组 分 属于 加州 大学 伯克利 分校 计算机 科学 
部 主要 从事 以下 几 方面 的 研究 工作 语言 
分析 机器翻译 计算机 语言学 基于 语义 的 方法 无 监督 
学习 等 多次 在 顶级 国际 会议 ACL EMNLP AAAI 
IJCAI COLING 等 上 发表 多 篇 论文 下表 是 
2018 年 最新 被 选用 的 论文 Natural Language Processing 
Group at University of Notre Dame 圣母 大学 自然 语言 
处理 小组 主要 关注 机器翻译 领域 并有 多个 项目 的 
研究 如 由 DARPALORELEI 和 Google 赞助 的 无 监督 
多 语言 学习 模型 和 算法 研究 由 亚马逊 学术 
研究 奖 和 谷歌 教师 研究 奖 赞助 的 研究 
主要 研究 课题 方向 包括 基于 神经 网络 的 机器 
翻译 模型 以及 使用 神经 网络 进行 翻译 和 语言 
建模 的 算法 等 多次 在 国际 顶级 期刊 和 
会议 上 发表 论文 目前 该 小组 主要 负责人 是 
David ChiangDavid Chiang 美国 圣母 大学教授 在 宾夕法尼亚 大学 计算机 
与 信息 科学 获得 博士 学位 主要 研究 领域 是 
自然 语言 处理 同时 在 语言 翻译 句法分析 等 方面 
也 有 研究 David Chiang 在 2005 年 提出 的 
基于 短语 的 翻译 模型 对 机器 翻译 来说 是 
一个 巨大 的 进步 他 把 机器 翻译 从 平面 
结构 建模 引向 了 层次结构 建模 The Harvard Natural Language 
Processing Group 哈佛 自然语言 处理 小组 主要 通过 机器学习 的 
方法 处理 人类 语言 主要 兴趣 集中 在 数列 生成 
的 数学 模型 以 人类 语言 为 基础 的 人工智能 
挑战 以及 用 统计 工具 对 语言 结构 进行 探索 
等 方面 该 小组 的 研究 出版物 和 开源 项目 
集中 在 文本 总结 神经 机器翻译 反复 神经 网络 的 
可视化 收缩 神经 网络 的 算法 文档 中 实体 跟踪 
的 模型 多 模态 文本 生成 语法错误 修 正和 文本 
生成 的 新方法 等 方面 Stuart Shieber 是 该 小组 
的 主要 负责人 Stuart ShieberJames O . Welch Jr . 
and Virginia B . Welch Professor of Computer ScienceFaculty Director 
Harvard Office for Scholarly C o m m u n 
i c a t i o n H a r 
v a r d UniversityStuart Shieber 美国 计算机 协会 Association 
for Computing Machinery Fellow 和 美国 人工智能 协会 American Association 
for Artificial Intelligence Fellow 他 综合 语言学 理论 计算机科学 计算机 
系统 以及 人工 智能 等 领域 的 知识 研究 计算机 
语言学 从 计算机 科学 的 角度 研究 自然语言 在 该 
领域 的 研究 以 科学 和 工程 目标 以 基础 
形式 和 数学 工具 为 基础 具体 研究 领域 包括 
计算 语言学 数学 语言学 基于 语法 的 形式 自然语言 生成 
计算 语义 机器翻译 以及 人机交互 等 Natural Language Processing group 
of Columbia University 哥伦比亚 大学 自然 语言 处理 研究室 是 
在 计算机 科学系 计算 学习 系统 中心 和 生物 医学 
信息系 的 支持 下 进行 的 将 语言 洞察力 与 
严谨 前沿 的 机器 学习 方法 和 其他 计算方法 结合 
起来 进行 研究 在 语言 资源 创造 如 语料库 词典 
等 阿拉伯语 NLP 语言 和 社交 网络 机器翻译 信息提取 数据挖掘 
词汇 语义 词义 消除歧义 等 方面 有着 比较 深入 的 
研究 现在 该 实验室 计算机 方面 的 主要 负责人 为 
Michael Collins Michael CollinsMichael Collins 哥伦比亚 大学 计算机 科学系 教授 
谷歌 NYC 研究 科学家 1998 年在 宾夕法尼亚 大学 获得 计算机 
科学 博士 学位 主要 研究 兴趣 是 自然 语言 处理 
和 机器 翻译 多次 在 国际 顶级 会议 上 发表 
文章 例如 在 EMNLP 2010 CoNLL 2008 UAL 2055 等 
会议 上 都 获得 最佳 论文 奖 同时 还是 ACL 
的 研究员 获 NSF 生涯 奖 2 国内 实验室 及 
人才 介绍 AMiner 基于 论文 数据 整理 了 自然 语言 
处理 华人 专家库 其中 包括 了 来自 NUS HKUS THU 
PKU FDU 等 知名 高校 以及 百度 科大 讯 飞 
微软 等 公司 的 367 位 专家 学者 下面 基于 
自然语言 处理 华人 库 中 的 数据 对 其 进行 
分析 自然语言 处理 领域 中 华人 专家 在 中国 最多 
美国 次之 从 地区 来看 中国 大陆 是 自然 语言 
处理 华人 人才 的 最主要 聚集地 尤其 是 北京 哈尔滨 
及 东南 沿海 地区 等 具有 自然语言 处理 学术 基础 
的 地区 美国 东部 和 西部 等 其他 地区 排 
在 其后 由 图 11 可以 看出 华人 专家 在 
中国 流出量 大于 流入量 美国 则 正好 相反 这也 说明 
就 自然 领域 而言 中国 对 人才 的 吸引力 要 
小于 美国 AMiner 自然语言 处理 华人 库 中 专家 h 
index 指数 的 平均数 为 14 这一 数值 是 远远 
低于 自然语言 处理 全球 top1000 学者 h index 指数 平均数 
的 而且 在 华人 库 中 h index 指数 10 
的 专家 人数 最多 占 比 60% 10 19 次之 
占 比 17% 60 的 专家 占 比 仅占 9% 
这也 说明 自然语言 处理 的 华人 专家 整体 水平 低于 
自然语言 处理 领域 全球 top1000 的 学者 尤其 是 在 
h index 指数 60 的 学者 方面 有所 欠缺 AMiner 
自然语言 处理 华人 库 367 位 专家 中 男性 专家 
占 98% 女性 专家 仅占 2% 二者 比例 约为 49 
1 以下 选取 在 ACL EMNLP NAACL COLING 等 4 
个 会议 在 近 5 年 累计 发表 10 次 
以上 论文 的 国内 学者 包括 刘群 刘挺/nr 周明 常 
宝宝 黄萱菁/nr 刘洋 孙茂松/nr 李素建/nr 万 小军 邱锡鹏/nr 穗 志 
方等/nr 以下 按照 发表 论文 的 多少 为序 对 这些 
学者 及其 所在 实验室 做 简要介绍 刘群 来自 百度 百科 
中科院计算所 自然语言 处理 研究组 刘群 中国科学院 自然语言 处理 研究 组组长 
都柏林 大学 自然 语言 处理 组 组长 项目 负责人 主要 
研究 方向 是 中文 自然语言 处理 具体 包括 汉 语词 
法分析 汉语 句法分析 语义 处理 统计 语言 模型 辞典 和 
语料库 机器翻译 信息提取 中文 信息 处理 和 智能 交互 中的 
大 规模 资源 建设 中文 信息 处理 以及 智能 交互 
中 的 评测 技术 等 曾 负责 863 重点 项目 
机器翻译 新方法 的 研究 和 面向 跨语言 搜索 的 机器 
翻译 关键 技术 研究 等 自然语言 处理 研究组 隶属于 中国科学院计算技术研究所 
智能 信息 处理 重点 实验室 研究组 教师 有 刘群 冯洋/nr 
等人 研究组 主要 从事 自然语言 处理 和 机器 翻译 相关 
的 研究 工作 研究 方向 包括 机器翻译 人机对话 多语 言词 
法分析 句法分析 和 网络 信息 挖掘 等 研究组 已 完成 
和 正在 承担 的 国家 自然科学 基金 863 计划 科技 
支撑 计划 国际 合作 等 课题 40 余项 在 自然 
语言 处理 和 机器 翻译 领域 取得 了 多项 创新 
性 研究 成果 研究组 自 2004 年 重点 开展 统计 
机器翻译 方面 的 研究 并 取得 重大 突破 并于 2015 
年起/nr 转向 神经 机器翻译 并 取得 很大 进展 2018 年 
7 月 正式 加入 华为 诺亚方舟 实验室 任 语音 语义 
首席 科学家 主导/b 语音/n 和/c 自然/d 语言/n 处理/v 领域/n 的/uj 
前沿/s 研究/vn 和/c 技术/n 创新/v 在 自然 语言 处理 的 
顶级 国际 刊物 CL AI 和 顶级 国际 学术 会议 
ACL IJCAI AAAI EMNLP COLING 上 发表 高水平 论文 70 
余篇 取得 发明专利 10 余项 研究组 已经 成功 将 自主 
开发 的 统计 机器 翻译 和 神经 机器 翻译 技术 
推广 到 汉语 维吾尔语 藏语 蒙古语 英语 韩语 泰语 日语 
阿拉伯语 等 多种 语言 部分 语种 的 翻译 系统 已经 
在 相关 领域 得到 了 实际 应用 获得 用户 的 
好评 刘挺/nr 来自 雷锋 网 刘挺/nr 哈尔滨 工业 大学 教授 
国家 万人 计划 科技 创新 领军 人才 多次 担任 国家 
863 重点 项目 总体 组 专家 基金委 会评 专家 中国 
计算机 学会 理事 中国 中文信息 学会 常务 理事 / 社会 
媒体 处理 专委会 SMP 主任 曾任 国际 顶级 会议 ACL 
EMNLP 领域 主席 主要 研究 方向 为 人工智能 自然语言 处理 
和 社会 计算 是 国家 973 课题 国家 自然科学 基金 
重点 项目 负责人 2012 2017 年在 自然语言 处理 领域 顶级 
会议 发表 的 论文 数量 列 世界 第 8 位 
据 剑桥 大学 统计 主持 研制 语言 技术 平台 LTP 
大词 林 等 科研 成果 被 业界 广泛 使用 曾获/nr 
国家 科技 进步 二等奖 省 科技 进步 一等奖 钱伟长 中文信息处理 
科学技术 一等奖 等 刘挺/nr 领导 的 哈工大 社会 计算 与 
信息检索 研究中心 http / / ir . hit . edu 
. cn / https / / github . com / 
HIT SCIR / ltp 哈工大 社会 计算 与 信息检索 研究 
中心 HIT SCIR 成立 于 2000 年 9 月 隶属 
于 计算机 科学 与 技术 学院 研究/vn 中心/n 成员/n 有/v 
主任/b 刘挺/nr 教授/n 副 主任 秦兵 教授 教师 包括 张宇 
车 万翔 陈毅 恒 张伟 男 等 研究 方向 包括 
语言 分析 信息 抽取 情感 分析 问答 系统 社会 媒体 
处理 和 用户 画像 6 个 方面 已完成 或 正在 
承担 的 国家 973 课题 国家 自然科学 基金 重点 项目 
国家 863 重点 项目 国际合作 企业 合作 等 课题 60 
余项 在 这些 项目 的 支持 下 打造出 语言 技术 
平台 LTP 提供给 百度 腾讯 华为 金山 等 企业 使用 
获 2010 年 钱伟长 中文信息处理 科学技术 一等奖 研究 中心 近年 
来 发表 论文 100 余篇 其中 在 ACL SIGIR IJCAI 
EMNLP 等 顶级 国际 学术 会议 上 发表 20 余篇 
论文 参加 国内外 技术 评测 并 在 国际 CoNLL 2009 
七国 语言 句法 语义分析 评测 总成绩 第一名 研究 中心 通过 
与 企业 合作 已将 多项 技术 嵌入 企业 产品 中 
为 社会 服务 双语 例句 检索 等 一批 技术 嵌入 
金山词霸 产品 中 并 因此 获得 2012 年 黑龙江省 技术 
发明 二等奖 周明 周明 微软 亚洲 研究院 自然语言 计算 组 
的 首席 研究员 机器 翻译 和 自然 语言 处理 领域 
的 专家 他 的 研究 兴趣 包括 搜索引擎 统计 和 
神经 机器翻译 问答 聊天 机器人 计算机 诗歌 和 文本 挖掘 
等 1989 年 他 设计 了 CEMT I 机器翻译 系统 
这是 汉英 机器 翻译 的 第一 个 实验 获得 了 
中国 大陆 政府 的 科学 技术 进步 奖 1998 年 
他 设计 了 著名 的 中 日文 机器 翻译 软件 
产品 J Beijing 并 获得 了 日本 机械翻译 协会 2008 
年 颁发 的 机器 翻译 产品 的 最高 荣誉 称号 
周明 团队 也为 Bing 搜索引擎 提供 了 重要 的 技术 
支持 包括 单词 breaker 情感 分析 speller 解析器 和 QnA 
等 NLP 技术 他 的 团队 创建 了 汉英 粤语 
的 机器 翻译 引擎 为 译者 和 Skype 翻译 最近 
周明 团队 与 微软 产品 团队 紧密 合作 在 中国 
小冰 日本 Rinna 和 美国 Tay 创建 了 知名 的 
chat bot 产品 拥有 4000 万 用户 他 在 顶级 
会议 包括 45 + ACL 论文 和 NLP 期刊 上 
发表 并 发表 了 100 多篇 论文 获得 了 38 
项 国际 专利 周明/nr 所属/b 实验室/n 为/p */i */i 微软/a 
亚洲/ns 研究院/n 自然语言/l 计算/v 组/zg */i */i 黄萱菁/nr 黄萱菁/nr 复旦 
大学 计算机 科学 技术 学院 教授 博士生 导师 在 SIGIR 
ACL ICML IJCAI AAAI NIPS CIKM ISWC EMNLP WSDM 和 
COLING 等 多个 国际 学术 会议 上 发表 论文 数十篇 
曾任 2014 年 CIKM 会议 竞赛 主席 2015 年 WSDM 
会议 组织者 2015 年 全国 社会 媒体 处理 大会 程序 
委员会 主席 2016 年 全国 计算 语言学 会议 程序 委员会 
副主席 2017 年 自然语言 处理 与 中文 计算 国际 会议 
程序 委员会 主席 多次 在 人工智能 自然语言 处理 和 信息 
检索 的 国际 学术 会议 IJCAI ACL SIGIR WWW EMNLP 
COLING CIKM WSDM 担任 程序 委员会 委员 和 资深 委员 
兼任 中国 中文信息 学会 常务 理事 社会 媒体 专委会 副主任 
中国计算机学会 中文信息处理 专委会 委员 中国 人工智能 学会 自然语言 理解 专委会 
委员 ACM 和 ACL 会员 中文信息 学报 编委 国家自然科学基金 教育部 
高校 博士点 基金 和 863 计划 同行 评议 专家 黄萱菁/nr 
领导 的 * * 复旦 大学 自然 语言 处理 研究组 
* * 复旦 大学 自然 语言 与 信息检索 实验室 致力 
于 社会 媒体 海量 多媒体 信息 处理 的 前沿 技术 
研究 主要 研究 方向 包括 自然语言 处理 非 规范化 文本 
分析 语义 计算 信息 抽取 倾向性 分析 文本 挖掘 等 
方面 实验室 开发 了 NLP 工具包 FudanNLP FudanNLP 提供 了 
一系列 新 技术 包括 中文分词 词性 标注 依赖 解析 时间 
表达式 识别 和 规范化 等 实验室 先后 承担 和 参与 
了 国家 科技 重大 专项 国家 973 计划 863 计划 
国家自然科学基金 课题 上海市 科技 攻关 计划 等 并与 国内外 多 
所 重点 大学 公司 保持 着 良好 的 合作 关系 
研究 成果 持续 发表 在 国际 权威 期刊 和 一流 
国际 会议 TPAMI TKDE ICML ACL AAAI IJCAI SIGIR CIKM 
EMNLP COLING 等 孙茂松/nr 孙茂松/nr 清华大学 计算机 科学 与 技术 
系 教授 2007 2010 年任/nr 该系/r 系主任/n 主要 研究 领域 
为 自然语言 处理 互联网 智能 机器学习 社会 计算 和 计算 
教育学 国家 重点 基础 研究 发展 计划 973 计划 项目 
首席 科学家 国家社会科学基金 重大 项目 首席 专家 在 国际 刊物 
国际 会议 国内 核心 刊物 上 发表 论文 160 余篇 
主持 完成 文本 信息处理 领域 ISO 国际标准 2 项 2007 
年获/nr 全国 语言 文字 先进 工作者 2016 年获/nr 全国 优秀 
科技 工作者 以及 首都 市民 学习 之星 多次 担任 相关 
领域 国际 会议 和 全国性 学术 会议 大会 主席 或 
程序 委员会 主席 孙茂松/nr 领导/n 的/uj */i */i 清华大学/nt 自然语言/l 
处理/v 与/p 社会/n 人文/n 计算/v 实验室/n */i */i 清华大学/nt 计算机系/n 
自然语言/l 处理/v 课题组/n 在/p 20 世纪 70 年代 末 就在 
黄 昌宁 教授 的 带领 下 从事 这 方面 的 
研究 工作 是 国内 开展 相关 研究 最早 深 具 
影响力 的 科研 单位 同时 也 是 中国 中文信息 学会 
计算 语言学 专业 委员会 的 挂靠 单位 现任/n 学科/n 带头人/n 
孙茂松/nr 教授/n 任该/nr 专业/n 委员会/n 的/uj 主任/b 同时 任 中国 
中文信息 学会 副 理事长 其余 教师 还有 刘洋 刘知远 等人 
目前 该 课题组 对 以 中文 为 核心 的 自然 
语言 处理 中 的 若干 前沿 课题 进行 系统 深入 
的 研究 研究/vn 领域/n 的/uj 涵盖/v 面/n 正/d 逐步/d 从/p 
计算/v 语言学/n 的/uj 核心/n 问题/n 扩展/v 到/v 社会/n 计算/v 和/c 
人文/n 计算/v 该 课题组 多篇 论文 被 ACL 2018 IJCAI 
ECAI 2018 WWW 2018 录用 内容 涉及 问答 系统 信息检索 
机器翻译 诗歌 生成 查询 推荐 等 多个 领域 万/m 小军/n 
万/m 小军/n 北京大学 计算机 科学 技术 研究所 教授 博士生 导师 
语言 计算 与 互联网 挖掘 实验室 负责人 研究 方向 为 
自然语言 处理 与 文本 挖掘 兴趣 领域 包括 自动 文摘 
与 文本 生成 情感 分析 与 观点 挖掘 语义 计算 
与 信息 推荐 等 在 国际 重要 学术 会议 与 
期刊 上 发表 高水平 学术 论文 上 百篇 担任 计算 
语言学 顶级 国际 期刊 Computational Linguistics 编委 TACL 常务 评审委员 
Standing Reviewing Committee 多次 担任 自然语言 处理 领域 重要 国际 
会议 领域 主席 或 SPC 包括 ACL NAACL IJCAI IJCNLP 
等 以及 相关 领域 多 个 国际 顶级 学术会议 ACL 
SIGIR CIKM EMNLP NAACL WWW AAAI 等 程序 委员会 委员 
研制 了 自动 文摘 开源 平台 PKUSUMSUM 与 今日 头条 
合作 推出 AI 写稿 机器人 小明 Xiaomingbot 与 南方 都市报 
合作 推出 写稿 机器人 小南 等 应用 系统 万/m 小军/n 
所属/b 实验室/n 为/p */i */i 北京大学/nt 语言/n 计算/v 与/p 互联网/n 
挖掘/v 研究组/n */i */i 语言/n 计算/v 与/p 互联网/n 挖掘/v 研究室/n 
从属/v 于/p 北京大学/nt 计算机/n 科学/n 技术/n 研究所/n 成立 于 2008 
年7/nr 月 负责人 为 万 小军 老师 研究室 以 自然 
语言 处理 技术 数据挖掘 技术 与 机器 学习 技术 为基础 
对 互联 网上 多源 异质 的 文本 大 数据 进行 
智能 分析 与 深度 挖掘 为 互联网 搜索 舆情 与 
情报 分析 写稿 与 对话 机器人 等 系统 提供 关键 
技术 支撑 并 从事 计算机 科学 与 人文 社会 科学 
的 交叉 科学研究 研究室 当前 研究 内容 包括 1 语义 
理解 研制 全新 的 语义分析 系统 实现 对 人类 语言 
尤其 是 汉语 的 深层 语义 理解 2 机器 写作 
综合利用 自动 文摘 与 自然 语言 生成 等 技术 让 
机器 写出 高质量 的 各类 稿件 3 情感 计算 针对 
多语言 互联网 文本 实现 高 精度 情感 立场 与 幽默 
分析 4 其他 包括 特定 情境 下 的 人机对话 技术 
等 穗 志 方穗志/nr 方 北京大学 信息 科学技术 学院 计算 
语言学 实验室 主任 教授 博士生 导师 2011 年度 国家 科技 
进步 二等奖 综合型 语言 知识库 项目 第二 完成人 长期 从事 
自然语言 处理 方面 的 研究 在 计算 语言学 国际 顶级 
会议 ACL 2000 COLING 2008 CONLL 2008 ACL 2009 EMNLP2009 
AIRS 2008 上 发表 多 篇 学术 论文 作为 课题 
负责人 主持 的 科研 项目 有 国家 自然科学 基金 项目 
汉语 动词 子 语 类 框架 自动 获取 技术 研究 
基于 结构化 学习 的 语义 角色 标注 研究 基于 Web 
的 概念 实例 及其 属性值 提取 方法 研究 国家 社科 
基金 项目 面向 文本 内容 提取 的 生成 性 组件 
库 研究 及 建设 等 穗 志 方 所属 实验室 
为 * * 北京大学 计算 语言学 教育部 重点 实验室 * 
* 计算 语言学 教育部 重点 实验室 依托 北京大学 建设 实验室 
研究 人员 由 北京大学 信息 科学技术 学院 计算 语言学 研究所 
中文系 软件 与 微电子 学院 语言 信息 工程 系 计算机 
技术 研究所 心理系 和 外语 学院 的 相关 研究 人员 
构成 主要 研究 方向 包括 中文 计算 的 基础 理论 
与 模型 大 规模 多层次 语言 知识库 构建 的 方法 
国家 语言 资源 整理 与 语音 数据库 建设 海量 文本 
内容 分析 与 动态 监控 多 语言 信息 处理 和 
机器 翻译 宗/nr 成庆/i 宗/nr 成庆/i 模式识别 国家 重点 实验室 
研究员 博士生 导师 主要 从事 自然语言 处理 机器 翻译 和 
文本 数据挖掘 等 相关 领域 的 研究 主持 国家 自然科学 
基金 项目 863 计划 项目 和 重点 研发 计划 重点 
专项 等 10 余项 发表 论文 150 余篇 出版 专著 
和 译著 各 一部 2013 年 当选 国际 计算 语言学 
委员会 ICCL 委员 目前 担任 亚洲 自然语言 处理 学会 AFNLP 
候任 主席 中国 中文信息 学会 副 理事长 学术期刊 ACM TALLIP 
副主编 AssociateEditor 自动化 学报 副主编 IEEE Intelligent Systems 编委 M 
a c h i n e T r a n 
s l a t i o n 编委 和 JCST 
编委 2013 年获/nr 国务院 颁发 的 政府 特殊 津贴 2014 
年获/nr 钱伟长 中文信息处理 科学技术 奖 一等奖 2015 年获/nr 国家 科技 
进步 奖 二等奖 2017 年获/nr 北京市 优秀 教师 荣誉称号 赵军 
赵军 中科院 研究员 博士生 导师 1998 年在 清华大学 计算机 科学 
与 技术系 获得 博士 学位 1998 年 2002 年在/nr 香港 
科技 大学 计算机 科学系 做 博士后 访问学者 2002 年 5 
月 至今 在 中科院 自动化所 模式识别 国家 重点 实验室 工作 
主持 国家 自然科学 基金 重点 项目 973 计划 等 国家级 
项目 研究 方向 为 信息 提取 和 问答 系统 等 
在 IEEE TKDE JMLR 等 顶级 国际 期刊 和 ACL 
SIGIR EMNLP COLING 等 顶级 国际 会议 上 发表 论文 
六 十余篇 获 COLING 2014 最佳 论文 奖 获 KDD 
CUP2011 亚军 2/1297 研发 了 汉语 文本 分析 信息 抽取 
和 知识 工程 百科 问答 等 软件 工具 和 平台 
在 中国 大百科全书 出版社 华为公司 讯 飞 公司 等 得到 
应用 宗/nr 成庆/i 和/c 赵军/nr 所属/b 实验室/n 为/p 中科院/nt 模式识别/n 
国家/n 重点/n 实验室/n 中科院/nt 模式识别/n 国家/n 重点/n 实验室/n 自然语言/l 处理/v 
组/zg 主要/b 成员/n 有/v 宗/nr 成庆/i 赵军 周玉 刘康 张 
家俊 汪昆/nr 陆征等/nr 该 小组 主要 从事 自然语言 处理 基础 
机器翻译 信息 抽取 和 问答 系统 等 相关 研究工作 力图 
在 自然 语言 处理 的 理论 模型 和 应用 系统 
开发 方面 做出 创新 成果 目前 研究组 的 主要 方向 
包括 自然语言 处理 基础 技术 汉语 词语切分 句法分析 语义分析 和 
篇章 分析 等 多语言 机器翻译 信息 抽取 实体 识别 实体 
关系 抽取 观点 挖掘 等 和 智能 问答 系统 基于 
知识库 的 问答 系统 知识 推理 社区 问答 等 近年来 
研究组/n 注重/v 于/p 自然/d 语言/n 处理/v 基础理论/n 和/c 应用/v 基础/n 
的/uj 相关/v 研究/vn 承担 了 一系列 包括 国家 自然科学 基金 
项目 973 计划 课题 863 计划/n 项目/n 和/c 支撑/v 计划/n 
项目/n 等/u 在内/u 的/uj 基础/n 研究/vn 和/c 应用/v 基础/n 研究/vn 
类/q 项目/n 以及 一批 企业 应用 合作项目 在 自然 语言 
处理 及 相关 领域 顶级 国际 期刊 CL TASLP TKDE 
JMLR TACL Information Sciences Intelligent Systems 等 和 学术 会议 
AAAI IJCAI ACL SIGIR WWW 等 上 发表 了 一系列 
论文 2009 年 获得 第 23 届 亚太 语言 信息 
与 计算 国际 会议 PACLIC 最佳 论文 奖 2012 年 
获得 第一 届 自然语言 处理 与 中文 计算 会议 NLPCC 
最佳 论文 奖 2014 年 获得 第 25 届 国际 
计算 语言学 大会 COLING 最佳 论文 奖 获得 了 10 
余项 国家 发明 专利 国内 学者 在 国际 会议 获得 
Best paper 的 有 以下 两个 Pengcheng Yang Xu Sun 
Wei Li Shuming Ma Wei Wu Houfeng Wang 的 SGM 
Sequence Generation Model for Multi label Classification 在 2018 COLING 
会议 中 被 评为 Besterror analysis 和 Best evaluation 王厚 
峰 王厚 峰 北京大学 信息 科学技术 学院 教授 北京大学 计算 
语言学 研究所 所长 主要 研究 兴趣 包括 情感 分析 问答 
与 会话 自然语言 语 言语 篇 分析 等 曾 作为 
首席 专家 主持 过 国家 863 项目 国家社科基金 重大 项目 
负责 国家自然科学基金 重大 研究 计划 等 在 ACL EMNLP COLING 
AAAI IJCAI ICML 等 会议 以及 Computational Linguistics 等 期刊 
发表 论文 70 余篇 Fan Bu Xiaoyan Zhu Ming Li 
等 的 Measuring the Non compositionality of M u l 
t i w o r d E x p r 
e s s i o n s 在 2010 年 
COLING 会议 上 被 评为 best paper 朱/nr 小燕/nr 朱/nr 
小燕/nr 清华 计算机 系 教授 博士生 导师 智能 技术 与 
系统 国家 重点 实验室 主要 负责人 主要 研究 领域 为 
智能 信息 处理 其中 包括 模式识别 神经元网络 机器学习 自然语言 处理 
信息 提取 和 智能 问答 系统 等 近年/t 研究/vn 工作/vn 
主要/b 集中/v 于/p 生物/n 领域/n 文本/n 信息/n 处理/v 和/c 新一代/t 
智能/n 信息/n 获取/v 的/uj 研究/vn 作为 项目 负责人 先后 承担 
国家 863 973 项目 自然科学 基金 项目 国际 合作 项目 
多项 1997 年获/nr 国家教委 科技 进步 二等奖 2003 年获/nr 北京市 
科技 进步 二等奖 获得 国家 发明 专利 3项 在 各种 
国际 刊物 和 会议 上 发表 论文 近 100篇 其中 
包括 国际 刊物 Genome Biology Bioinformatics BMC Bioinformatics Medical informatics 
IEEE Transactions . on SMC IEEElectronics Letters Neural Parallel & 
Science Computations Document Analysis and Recognition 以及 国际 会议 SIG 
KDD ACL COLING CIKM 等 朱 小燕 所属 实验室 为 
清华大学 智能 技术 与 系统 国家 重点 实验室 智能 技术 
与 系统 国家 重点 实验室 依托 在 清华大学 1987 年 
7 月 开始 筹建 1990 年 2 月 通过 国家 
验收 并 正式 对外开放 运行 从 1990 年至/nr 2003 年这/nr 
十三 年间 实验室 顺利 通过 国家 自然科学 基金委 受 科技部 
委托 组织 的 全部 三次 专家组 评估 并被 评估 为 
A 优秀 实验室 1994 年 10 月 在 庆祝 国家 
重点 实验室 建设 十 周年 表彰 大会 上 智能 技术 
与 系统 国家 重点 实验室 获 集体 金牛 奖 1997 
年被/nr 科技部 列为 试点 实验室 2004 年 庆祝 国家 重点 
实验室 建设 二十周年 表彰 大会 上 本 实验室 再次 荣获 
集体 金牛 奖 从 2004 年 开始 实验室 参与 筹建 
清华 信息 科学 与 技术 国家 实验室 实验室 学术 委员会 
由 17 名 国内外 著名 专家 组成 实验室 学术 委员会 
名誉 主任 为 中科院 院士 张钹 教授 主任 为 应明 
生 教授 副/b 主任/b 为/p 邓志东/nr 教授/n 除此之外 活跃 在 
自然 语言 领域 的 中国 学者 还有 周/nr 国栋/nr 周/nr 
国栋/nr 苏州 大学 计算机 科学 与 技术 学院 教授 苏州大学 
NLP 实验室 负责人 主要 研究 兴趣 是 自然 语言 处理 
中文 计算 信息 抽取 和 自然 语言 认知 等 自 
1999 年起/nr 一直 是 ACM ACL IEEE computer society 的 
会员 负责 了 多项 国家 863 项目 国家 重点 研究 
项目 等 近 5 年来 发表 国际 著名 SCI 期刊论文 
20 多篇 和 国际 顶级 会 议论文 80 多篇 主持 
NSFC 项目 4 个 包括 重点 项目 1 个 曾 
担任 国际 自然 语言 理解 领域 顶级 SCI 期刊 C 
o m p u t a t i o n 
a l L i n g u i s t 
i c s 编委 目前 担任 ACM TALLIP 副主编 软件 
学报 责任 编委 CCF 中文 信息 技术 专委会 副主任 委员 
苏州大学 校 学术 委员会 委员 李涓/nr 子李 涓子 清华大学 教授 
博士生 导师 中国 中文信息 学会 语言 与 知识 计算 专委会 
主任 中国计算机学会 术语 委员会 执行 委员 研究 兴趣 是 语义 
Web 新闻 挖掘 与 跨语言 知识图谱 构建 多篇 论文 发表 
在 重要 国际 会议 WWW IJCAI SIGIR SIGKDD 和 学术 
期刊 TKDE TKDD 主持 多 项 国家级 部委级 和 国际 
合作 项目 研究 包括 国家 自然科学 项目 重点 欧盟 第七 
合作 框架 新华社 等 项目 获得 2013 年 人工智能 学会 
科技 进步 一等奖 2013 年 电子 学会 自然科学 二等奖 张民 
张民 苏州 大学 计算机 科学 与 技术 学院 副院长 2003 
年 12 月 他 加入 新加坡 信息 通信 研究 所 
并于 2007 年在 研究 所 建立 了 统计 机器翻译 团队 
2012 年 加入 苏州大学 并于 2013 年在 该 大学 成立 
智能 计算 研究所 目前 的 研究 兴趣 包括 机器翻译 自然语言 
处理 信息提取 社交 网络 计算 互联网 智能 智能 计算 和 
机器学习 近年来 在 国际 顶级 学报 和 顶级 会议 发表 
学术 论文 150 余篇 Springer 出版 英文 专著 两部 主编 
Springer 和 IEEE CPS 出版 英文 书籍 十本 他 一直 
积极 地 为 研究 界 做 贡献 组织 多 次 
会议 并 在 许多 会议 和 讲座 中 进行 演讲 
黄河 燕 黄河 燕 语言 智能 处理 与 机器 翻译 
领域专家 北京 理工大学 计算机 学院 院长 教授 北京市 海量 语言 
信息 处理 与 云计算 应用 工程 技术 研究 中心 主任 
长期 从事 语言 智能 处理 的 理论 及 应用 研究 
主持 多项 国家 自然科学 基金 重点 项目 国家 重点 研发 
计划 项目 973 计划 课题 等 重要 科研 项目 曾获/nr 
国家 科技 进步 一等奖 二等奖 等 奖项 被 授予 全国 
优秀 科技 工作者 称号 孙乐孙/nr 乐/a 中国 科学院 软件 研究所 
研究员 博士生 导师 中国 中文信息 学会 副 理事长 兼 秘书长 
中文信息 学报 副主编 2003 至 2005 年 先后 在 英国 
Birmingham 大学 加拿大 Montreal 大学 做 访问 学者 从事 语料库 
和 信息检索 研究 目前 主要 研究 兴趣 基于 知识 的 
语言 理解 信息 抽取 问答 系统 信息检索 等 在国内外 主要 
刊物 和 会议 上 共 发表 论文 80 多篇 曾任 
2008 和 2009 国际 测评 NTCIR MOAT 中文 简体 任务 
的 组织者 国际 计算 语言学 大会 COLING 2010 组织 委员会 
联席 主席 机器翻译 峰会 MT Summit 2011 组织 委员会 联席 
主席 中文 语言 评测 国际 会议 CLP2010 2012 2014 大会 
主席 国际 计算 语言学 年会 ACL 2015 组织 委员会 联席 
主席 等 3 ACL2018 奖项 介绍 2018 年 7 月 
15 在 墨尔本 开幕 的 ACL 公布 了 其 最佳 
论文 名单 包括 3 篇 最佳 长 论文 和2/nr 篇 
最佳 短 论文 以及 1 篇 最佳 demo 论文 值得一提的是 
Amazon Door Prize 中 北京大学 和 哈尔滨 大学 上榜 ACL2018 
终身 成就奖 为 爱丁堡大学 Mark Steedman 获得 最佳 长 论文 
Finding syntax in human encephalography with beam search 用 波束 
搜索 在 人脑 成像 中 寻找 句法 作者 John Hale 
Chris Dyer Adhiguna Kuncoro Jonathan R . Brennan 论文 摘要 
循环 神经网络 文法 RNNGs 是 对于 树 字符串 对 的 
生成式 模型 它们 依靠 神经 网络 来 评价 派生 的 
选择 用 束 搜索 对 它们 进行 解析 可以 得到 
各种 不同 复杂度 的 评价 指标 比如 单词 惊 异数 
word surprisal count 和 解析器 动 作数 parser action count 
当 把 它们 用作 回归 因子 解析 人类 大脑 成像 
图像 中 对于 自然 语言 文本 的 电 生理学 响 
应时 它们 可以 带来 两个 增幅 效果 一个 早期 的 
峰值 以及 一个 类似 P600 的 稍迟 的 峰值 相比之下 
一个 不 具有 句法 结构 的 神经 语言 模型 无法 
达到 任何 可靠 的 增幅 效果 通过 对 不同 模型 
的 对比 早期 峰值 的 出现 可以 归功于 RNNG 中的 
句法 组合 结果 中 体现 出 的 这种 模式 表明 
RNNG + 束 搜索 的 组合 可以 作为 正常 人类 
语言 处理 中 的 语法 处理 的 一个 不错 的 
机理 解释 模型 论文 地址 https / / arxiv . 
org / abs / 1806 . 04127Learning to Ask Good 
Questions Ranking Clarification Questions using NeuralExpected Value of Perfect Information 
学习 如何 问好 的 问题 通过 完全 信息 下 的 
期待 值 为 追 问问题 排序 作者 Sudha Rao Hal 
Daum é III 论文 摘要 在 沟通 中 提问 是 
一大 基本要素 如果 机器 不 知道 如何 问问题 那 它们 
也 就 无法 高效 地 与 人类 合作 在 这项 
研究 中 作者 们 构建 了 一个 神经 网络 用于 
给 追问 的 问题 做 排名 作者 们 模型 设计 
的 启发 来源 于 完全 信息 情况下 的 期待 值 
一个 可以 期待 获得 有用 的 答案 的 问题 就是 
一个 好 问题 作者 们 根据 StackExchange 上 抓取 的 
数据 研究 了 这个 问题 StackExchange 是 一个 内容 丰富 
的 在线 咨询 平台 其中 有人 发帖 咨询 以后 别的/nr 
用户 会 在下面 追问 起到 解释 澄清 作用 的 问题 
以便 更好 地 了解 状况 帮助 到 发帖人 论文 作者 
们 创建 了 一个 由 这样 的 追问 问题 组成 
的 数据 集 其中 包含 了 StackExchange 上 askubuntu unix 
superuser 这三个 领域 的 约 77k 组 发帖 + 追 
问问题 + 问题 的 回答 作者 们 在 其中 的 
500 组 样 本上 评估 了 自己 的 模型 相比 
其他 基准 模型 有 显著 的 提高 同时 他们 也 
与 人类 专家 的 判断 进行 了 对比 论文 地址 
https / / arxiv . org / abs / 1805 
. 04655Let s do it again A First Computational Approach 
to Detecting A d v e r b i a 
l P r e s u p p o s 
i t i o n Triggers 让 我们 再 做 
一次 首个 检测 假定 状态 触发 副词 的 计算 性 
方法 作者 Andre Cianflone Yulan Feng Jad Kabbara Jackie Chi 
Kit Cheung 论文 摘要 这篇 论文 中 作者 们 介绍 
了 一种 新的 研究 课题 预测 副词 词性 的 假定 
状态 触发 语 adverbial presupposition triggers 比如 also 和 again 
完成 这样 的 任务 需要 在 对话 上下文 里 寻找 
重复 出现 的 或者 相似 的 内容 这项 任务 的 
研究 成果 则 可以 在 文本 总结 或者 对话 系统 
这样 的 自然 语言 生成 任务 中 起到 帮助 作者 
们 为 这项 任务 创造 了 两个 新的 数据集 分别 
由 Penn Treebank 和 AnnotatedEnglish Gigaword 生成 而且 也 专为 
这项 任务 设计 了 一种 新的 注意力 机制 作者 们 
设计 的 注意力 机制 无需 额外 的 可 训练 网络 
参数 就 可以 增强 基准 RNN 模型 的 表现 这 
最小 化了 这一 注意力 机制 带来 的 额外 计算 开销 
作者 们 在 文中 表明 他们/r 的/uj 模型/n 相比/v 多个/m 
基准/n 模型/n 都有/nr 统计/v 显著/a 的/uj 更高/i 表现/v 其中 包括 
基于 LSTM 的 语言 模型 论文 地址 https / / 
www . cs . mcgill . ca / ~ jkabba 
/ acl2018paper . pdf 最佳 短 论文 Know What You 
Don t Know Unanswerable Questions for SQuAD 知道 你 不 
知道 的 SQuAD 中 无法 回答 的 问题 作者 Pranav 
Rajpurkar Robin Jia Percy Liang 论文 摘要 提取 式 的 
阅读 理解 系统 一般 都 能够 在 给定 的 文档 
内容 中 找到 正确 的 内容 来 回答 问题 不过 
对于 正确 答案 没有 明示 在 阅读 文本 中 的 
问题 它们 就 经常 会 做出 不 可靠 的 猜测 
目前 现有 的 阅读 理解 问答 数据集 要么 只 关注 
了 可 回答 的 问题 要么 使用 自动 生成 的 
无法 回答 的 问题 很容易 识别 出来 为了 改善 这些 
问题 作者 们 提出 了 SQuAD2 . 0 数据集 这是 
斯坦福 问答 数据集 SQuAD 的 最新 版本 SQuAD2 . 0 
在 现有 的 十万 个 问题 答案 对 的 基础 
上 增加 了 超过 五万个 无法 回答 的 问题 它们 
由 人类 众包 者 对抗 性地 生成 看起来 很 像 
可以 回答 的 问题 一个 问答 系统 如果 想 要在 
SQuAD2 . 0 上 获得 好 的 表现 它 不仅 
需要 在 问题 能够 回答 时 给出 正确 的 答案 
还要 在 给定 的 阅读 材料 中 不 包含 答案 
时 做出 决定 拒绝 回答 这个 问题 SQuAD2 . 0 
也 设立 了 新的 人类 表现 基准线 E M 86.831 
F 189.452 对于 现有 模型 来说 SQuAD2 . 0 是 
一个 具有 挑战性 的 自然 语言 理解 任务 一个 强有力 
的 基于 神经 网络 的 系统 可以 在 SQuAD1 . 
1 上 得到 86% 的 F1 分数 但在 SQuAD2 . 
0 上 只能 得到 66% 论文 地址 https / / 
arxiv . org / abs / 1806.03822 Lighter Can Still 
Be Dark Modeling Comparative Color Descriptions 更 浅 的 颜色 
也 可能 仍然 是 黑暗 的 建模 比较性 的 颜色 
描述 作者 Olivia Winn Smaranda Muresan 论文 摘要 我们 提出 
了 一种 在 颜色 描述 领域 内 建立 基准 比较性 
形容词 的 新 范式 给定 一个 参考 RGB 色 和 
一个 比较 项 例如 更 亮 更暗 我们 的 模型 
会 学习 建立 比较 项的/nr 基准 将其 作为 RGB 空间 
中 的 一个 方向 这样 颜色 就 会 沿着 向量 
植根于 比较 色 中 我们 的 模型 产生 了 比较 
形容词 的 基本 表示 形式 在 期望 的 改变 方向 
上 平均 精确度 为 0.65 余弦 相似性 与 目标 颜色 
相比 依据 向量 的 颜色 描述 方法 Delta E 值 
小于 7 这表明 这种 方法 与 人类 感知 的 差异 
非常 小 这一 方法 使用 了 一个 新 创建 的 
数据集 该 数据集 来自 现有 的 标记 好 的 颜色 
数据 论文 地址 http / / aclweb . org / 
anthology / P18 2125 最佳 demo 论文 Out of the 
box Universal Romanization Tool 开箱 即用 的 通用 罗马化 工具 
作者 Ulf Hermjakob Jonathan May Kevin Knight 论文 摘要 我们 
想 介绍 uroman 这个 工具 可以 把 五花八门 的 语言 
和 文字 如 中文 阿拉伯语 西里 尔文 转换 为 普通 
拉丁文 该 工具 基于 Unicode 数据 以及 其他 表 可以 
处理 几乎 所有 的 字符集 包括 一些 晦涩 难懂 的 
语言 比如 藏文 和提非/nr 纳文 uroman 还 可以 将 不同 
文本 中 的 数字 转换 为 阿拉伯 数字 罗马化 让 
比较 不同 文本 的 字符串 相似性 变得 更加 容易 因为 
不再 需要 将 两种 文字 翻译 成 中间 文字 再 
比较 本 工具 作为 一个 Perl 脚本 可以 免费 提供 
可 用于 数据处理 管道 和 交互式 演示 网页 论文 地址 
http / / aclweb . org / anthology / P18 
4003ACL 终身 成就奖 由 Mark Steedman 获得 Mark Steedman 出生于 
1946 年 1968 年/m 毕业/n 于/p 苏塞克/i 斯/nr 大学/n University 
of Sussex 1973 年 获得 爱丁堡大学 人工智能 博士学位 论文 音乐 
知觉 的 形式 化 描述 此后 他 曾 担任 华威大学 
心理学 讲师 爱丁堡 大学 计算机 语言学 讲师 宾夕法尼亚 大学 计算机 
与 信息 科学 学院 副教授 也 曾在 德克萨斯 大学 奥斯汀 
分校 奈梅亨 马克斯 普朗克 心理 语言 研究所 和 费城 宾夕法尼亚 
大学 担任 过 访问学者 目前 他 任 爱丁堡 大学 信息 
学院 认知科学 系主任 主要 研究 领域 有 计算 语言学 人工智能 
和 认知科学 AI 会话 的 有意义 语调 生成 动画 对话 
手势 交流 以及 组合 范畴 语法 C o m b 
i n a t o r y c a t 
e g o r i a l grammar CCG 等 
此外 他 对 计算 音乐 分析 和 组合 逻辑 等 
领域 也 很感兴趣 目录 文章 目录 目录 前言 n gram 
语言 模型 一 n gram 语言 模型 二 n gram 
语言 模型 三 n gram 语言 模型 四 n gram 
语言 模型 五 n gram 语言 模型 六 n gram 
语言 模型 七 前言 硕士 生涯 结束 开始 专心 做 
一件 自己 觉得 有用 的 工具 先 做 工程 后搞/nr 
理论 自然语言 处理 是 一个 非常 难 的 问题 同时 
是 人工智能 皇冠 上 的 明珠 接下来 会 记录 一 
系列 自然语言 处理 的 笔记 来自 于 哈工大 老师 关毅 
n gram 语言 模型 一 n 元词/nr 序列 通 分词 
一元 频度 语料库 加工 最大熵 模型 基本 信息 噪声 信道 
模型 信源 s 发出 信息 一 系列 01 序列 输入 
和 输出 完全 匹配 一致 信息 转变 in process out 
贝叶斯 公式 是 统计 的 核心 地位 一个 声学 信号 
对应 于 一个 语句 T = argmax p T / 
A 求 的 是 使其 概率 最大 的 T 语音 
识别 的 应用 信源 的 应用 手写体 汉字 识别 文字 
作用 信源 以 概率 p 输出 字符串 目标 翻译 输出 
一段 语音 文字 出现 的 概率 P T 语言 模型 
完成 特定 功能 的 数据 结构 实现 字符串 结构 的 
模型 概率 信源 字符 序列 香农 游戏 给定 前 n 
个 词 求 下 一个 词 n gram 语言 模型 
二 全 概率模型 0 1 规则 力量 较强 参数 统计模型 
空间 大 稀疏 马尔可夫 假设 下 一个 词 依赖于 前 
一个 词 P s | t = P | t 
1 Trigram 模型 P I P W1 P W2 ∣ 
W1 P I ~ P W _ 1 P W 
_ 2 | W _ 1 P I   P 
W1 P W2 ∣ W1 还 可以 无限 延伸 变成 
ngram 模型 约 减 参数 空间 可靠 辨别 一个 参数 
数据 平滑 技术 只 统计 他 的 一元 频度 某 
语料库 词汇 分布图 最大 相似 度 估计 分布图 期望 概率 
分布图 n gram 语言 模型 三 数据 平滑 技术 discounting 
技术 分给 小 的 validation 拉普拉斯 定律 加 一 平滑 
法 大家 同 加 一 解决 数据 稀疏 问题 Plap 
= W1N + BB = ∣ U ∣ nP _ 
{ lap } = \ frac { W _ 1 
} { N + B } B = | U 
| ^ nPlap = N + BW1 B = ∣ 
U ∣ nGood Turing 估计 如果 C W1 W2 . 
. . WN = r & gt 0C W _ 
1 W _ 2 . . . W _ N 
= r & gt 0C W1 W2 . . . 
WN = r 0Pat W1 . . Wn = r 
∗ / NPat W _ 1 . . W _ 
n = r * / NPat W1 . . Wn 
= r ∗ / N 此处 R * = r 
+ 1 S r + 1 / s ® 这里 
® 是 Nr 的 期望 平滑 估计 Nr = arb 
估计 整体 分布 参数估计 的 一种 图灵 估计 线性插值 平滑 
构造 高 鲁棒性 语言 模型 2 规模 小 效果显著 3 
规模 大 效果 不 显著 技术 上 实行 n gram 
语言 模型 四 只 依赖 前 n 1个 词 的 
词性 n pose 模型 动态 估计 和 静态 估计 合力 
解决 词汇 问题 统计 语言 模型 的 评价 方法 实用 
方法 基于 交叉 熵 与 迷惑 度 的 方法 H 
x = − ∑ q x logp x q x 
H x = \ sum q x log \ frac 
{ p x } { q x } H x 
= − ∑ q x logq x p x n 
gram 语言 模型 五 argmax P T | s 语言 
模型 的 实例 考虑 数据 的 加载 与 注入 高 
压缩比 数据 N gram 语言 模型 的 构造 n gram 
语言 模型 六 理解 骨架 基本 模型 隐 马 极大 
熵 生成 / 条件 判别 模型 最大熵 原理 是 指 
在 一定 的 限制 条件 下 尽可能 地 选择 熵 
最大 的 概率分布 均匀分布 作为 预测 结果 而对 不 知道 
限制 条件 以外 的 情形 不 做 任何 假设 如何 
设计 正负 的 概率 假设在 语料库 中 有 如下 词性 
标记 及 次数 估计 在 限定 条件 下 的 概率 
选择 满足 限定 条件 的 P 使 H p 为 
最大 H x = − ∑ P x logp x 
H x = \ sum P x logp x H 
x = − ∑ P x logp x a A 
且 b B . 在 最大熵 模型 中 特征 是 
一个 关于 事件 的 二 值 函数 fj x − 
0 1 x = A ∗ Bf _ j x 
{ 0 1 } x = A * Bfj x 
− 0 1 x = A ∗ B 原子级 特征 
n gram 语言 模型 七 限制 条件 模型 特征 的 
期望值 等于 训练 语料库 中 观察 到 的 特征 的 
期望值 Epfj = EpfjE _ pf _ j = Ep 
^ ~ f _ jEp fj = Ep   fj 
训练 语料库 非常 关键 从 训练 数据 到 可 观测 
事件 解的/nr 存在 且 唯一 拉格朗日 解 最大熵 模型 的 
使用 方法 rf 条件 随机 域 文本 数据 数据 缺失 
HMM / EM 一 自然语言 处理 概述 1 自然语言 处理 
利用 计算机 为 工具 对 书面 实行 或者 口头 形式 
进行 各种各样 的 处理 和 加工 的 技术 是 研究 
人 与人 交际 中 以及 人 与 计算机 交际 中的 
演员 问题 的 一门 学科 是 人工智能 的 主要 内容 
2 自然语言 处理 是 研究 语言 能力 和 语言 应用 
的 模型 建立 计算机 算法 框架 来 实现 这样 的 
语言 模型 并 完善 评测 最终 用于 设计 各种 实用 
系统 3 研究 问题 主要 信息检索 机器翻译 文档 分类 问答 
系统 信息 过滤 自动 文摘 信息 抽取 文本 挖掘 舆情 
分析 机器 写作 语音识别 研究 模式 自然语言 场景 问题 数学 
算法 算法 如何 应用 到 解决 这些 问题 预料 训练 
相关 实际应用 自然 语言 的 困难 场景 的 困难 语言 
的 多样性 多变性 歧义 性 学习 的 困难 艰难 的 
数学 模型 hmm crf EM 深度 学习 等 语料 的 
困难 什么 的 语料 语料 的 作用 如何 获取 语料 
二 形式语言 与 自动机 语言 按照 一定 规律 构成 的 
句子 或者 字符串 的 有限 或者 无限 的 集合 描述语言 
的 三种 途径 穷举法 文法 产生式系统 描述 自动机 自然 语言 
不 是 人为 设计 而 是 自然 进化 的 形式语言 
比如 运算 符号 化学 分子式 编程语言 形式语言 理论 朱 啊哟 
研究 的 是 内部 结构 模式 这类 语言 的 纯粹 
的 语法 领域 从 语言学 而来 作为 一种 理解 自然 
语言 的 句法 规律 在 计算机 科学 中 形式语言 通常 
作为 定义 编程 和 语法结构 的 基础 形式 语言 与 
自动机 基础知识 集合论 图论 自动机 的 应用 1 单词 自动 
查错 纠正 2 词性 消 歧 什么 是 词性 什么 
的 词性 标注 为什么 需要 标注 如何 标注 形式语言 的 
缺陷 1 对于 像 汉语 英语 这样 的 大型 自然 
语言 系统 难以 构造 精确 的 文法 2 不 符合 
人类 学习 语言 的 习惯 3 有些 句子 语法 正确 
但在 语义上 却 不 可能 形式语言 无法 排出 这些 句子 
4 解决 方向 基于 大量 语料 采用 统计学 手段 建立 
模型 三 语言 模型 1 语言 模型 重要 通过 语料 
计算 某个 句子 出现 的 概率 概率 表示 常用 的 
有2/nr 元 模型 3 元 模型 2 语言 模型 应用 
语音识别 歧义 消除 例如 给定 拼音串 ta shi yan yan 
jiu saun fa de 可能 的 汉字 串 踏实 烟酒 
算法 的   他 是 研究 酸 法的/nr     
  他 是 研究 算法 的 显然 最后 一句 才 
符合 3 语言 模型 的 启示 1 开启 自然语言 处理 
的 统计 方法 2 统计 方法 的 一般 步骤 收集 
大量 语料 对 语料 进行 统计分析 得出 知识 针对 场景 
建立 算法 模型 解释 和 应用 结果 4 语言 模型 
性能评价 包括 评价 目标 评价 的 难点 常用 指标 交叉 
熵 困惑 度 5 数据 平滑 数据 平滑 的 概念 
为什么 需要 平滑 平滑 的 方法 加 一 法 加法 
平滑 法 古德 图灵 法 J M 法 Katz 平滑 
法等6/nr 语言 模型 的 缺陷 语料 来自 不同 的 领域 
而 语言 模型 对 文本 类型 主题 等 十分 敏感 
n 与 相邻 的 n 1个 词 相关 假设 不是 
很 成立 四 概率 图 模型 生成 模型 与 判别 
模型 贝叶斯 网络 马尔科夫 链 与 隐 马尔科夫 模型 HMM 
1 概率 图 模型 概述 什么 的 概率 图 模型 
参考 清华大学 教材 概率 图 模型 2 马尔科夫 过程 定义 
理解 3 隐 马尔科夫 过程 定义 理解 HMM 的 三个 
基本问题 定义 解法 应用 注 第一 个 问题 涉及 最大 
似 然 估计法 第二 个 问题 涉及 EM 算法 第三 
个 问题 涉及 维 特比 算法 内容 很多 要 重点 
理解 参考书 李航 统计 学习 方法 网上 博客 笔者 github 
五 马尔科夫 网 最大熵 模型 条件 随 机场 CRF 1 
HMM 的 三个 基本 问题 的 参数 估计 与 计算 
2 什么 是 熵 3 EM 算法 应用 十分 广泛 
好好 理解 4 HMM 的 应用 5 层次化 马尔科夫 模型 
与 马尔科夫 网络 提出 原因 HMM 存在 两个 问题 6 
最大熵 马尔科夫 模型 优点 与 HMM 相比 允许 使用 特征 
刻画 观察 序列 训练 高效 缺点 存在 标记 偏置 问题 
7 条件 随 机场 及其 应用 概念 模型 过程 与 
HMM 关系 参数估计 方法 GIS 算法 改进 IIS 算法 CRF 
基本问题 特征 选取 特征 模板 概率 计算 参数 训练 解码 
维 特比 应用 场景 词性 标注 类 问题 现在 一般 
用 RNN + CRF 中文分词 发展过程 经典 算法 了解 开源 
工具 jieba 分词 中文 人名 地名 识别 8   CRF 
+ + 六 命名 实体 识别 词性 标注 内容 挖掘 
语义分析 与 篇章 分析 大量 用到 前面 的 算法 1 
命名 实体 识别 问题 相关 概率 定义 相关 任务 类型 
方法 基于 规程 基于 大 规模 语料库 2 未 登录 
词 的 解决 方法 搜索引擎 基于 语料 3 CRF 解决 
命名 实体 识别 NER 流程 总结 训练 阶段 确定 特征 
模板 不同 场景 人名 地名 等 所 使用 的 特征 
模板 不同 对 现有 语料 进行 分词 在 分词 结 
                    
  果 基础 上 进行 词性 标注 可能 手工 NER 
对应 的 标注 问题 是 基于 词 的 然后 训练 
CRF 模型 得到 对应 权值 参数值 识别 过程 将 待 
识别 文档 分词 然后 送入 CRF 模型 进行 识别 计算 
维 特比 算法 得到 标注 序列 然后 根据 标   
                    
      注 划分 出 命名 实体 4 词性 
标注 理解 含义 意义 及其 一致性 检查 方法 位置 属性 
向量 词性 标注 序列 向量 聚 类 或者 分类 算法 
七 句法分析 1 句法分析 理解 以及 意义 1 句法结构 分析 
完全 句法分析 浅层 分析 这里 有 很多 方法 2 依存关系 
分析 2 句法 分析方法 1 基于 规则 的 句法结构 分析 
2 基于 统计 的 语法 结构 分析 八 文本 分类 
情感 分析 1 文本 分类 文本 排 重 文本 分类 
在 预定义 的 分类 体系 下 根据 文本 的 特征 
将 给定 的 文本 与 一个 或者 多 个 类别 
相关联 典型 应用 垃圾邮件 判定 网页 自动 分类 2 文本 
表示 特征 选取 与 权重 计算 词 向量 文本 特征选择 
常用 方法 1 基于 本文 频率 的 特征 提 取法 
2 信息 增 量法 3 X2 卡方 统计量 4 互信息 
法3/nr 分类器 设计 SVM 贝叶斯 决策树 等 4 分类器 性能 
评测 1 召回率 2 正确率 3 F1 值 5 主题 
模型 LDA 与 PLSALDA 模型 十分 强大 基于 贝叶斯 改进 
了 PLSA 可以 提取 出 本章 的 主题词 和 关键词 
建模 过程 复杂 难以理解 6 情感 分析 借助 计算机 帮助 
用户 快速 获取 整理 和 分析 相关 评论 信息 对 
带有 感情 色彩 的 主观 文本 进行 分析 处理 和 
归纳 例如 评论 自动 分析 水军 识别 某种 意义 上 
看 情感 分析 也 是 一种 特殊 的 分类 问题 
7 应用 案例 九 信息检索 搜索引擎 及其 原理 1 信息检索 
起源于 图书馆 资料 查询 检索 引入 计算机 技术 后 从 
单纯 的 文本 查询 扩展到 包含 图片 音 视频 等 
多媒体 信息检索 检索 对象 由 数据库 扩展 到 互联网 1 
点对点 检索 2 精确 匹配 模型 与 相关 匹配 模型 
3 检索系统 关键 技术 标引 相关度 计算 2 常见 模型 
布尔 模型 向量空间 模型 概率模型 3 常用 技术 倒排索引 隐 
语义分析 LDA 等 4 评测 指标 十 自动 文摘 与 
信息 抽取 机器翻译 问答 系统 1 统计 机器 翻译 的 
的 思路 过程 难点 以及 解决 2 问答 系统 基本 
组成 问题 分析 信息检索 答案 抽取 类型 基于 问题 答案 
基于 自由 文本 典型 的 解决 思路 3 自动 文摘 
的 意义 常用 方法 4 信息 抽取 模型 LDA 等 
十一 深度 学习 在 自然 语言 中 的 应用 1 
单词 表示 比如 词 向量 的 训练 wordvoc 2 自动 
写 文本 写 新闻 等 3 机器翻译 4 基于 CNN 
RNN 的 文本 分类 5 深度 学习 与 CRF 结合 
用于 词性 标注 . . . . . . . 
. . . . . . . . 转 自 
https / / blog . csdn . net / meihao5 
/ article / details / 79592667 写在 最前面 在 这个 
日新月异 的 信息 时代 海量 数据 的 积累 计算 能力 
的 不断 提升 机器学习 尤其 是 深度 学习 的 蓬勃 
发展 使得 人工智能 技术 在 不同 领域 焕发 出 蓬勃 
的 活力 自己 经历 了 嵌入式 开发 移动 互联网 开发 
目前 从事 自然语言 处理 算法 开发 工作 从 工程 软件 
开发 到 自然 语言 处理 算法 开发 希望 通过 这个 
系列 的 文章 能够 由浅入深 通俗 易懂 的 介绍 自然语言 
处理 的 领域 知识 分享 自己 的 成长 同 大家 
一起 进步 信心 与 兴趣 很多 同学 提到 算 法可能 
就会 打退堂鼓 尤其 是 一直 从事 纯 工程 的 软件 
开发 工作/vn 中/f 连/nr 经典/n 的/uj 数据/n 结构/n 都/d 很少/m 
使用/v 更 不用 提 五花八门 的 机器 学习 和 深度 
学习 算法 尤其 各个 大厂 的 算法 专家 数据 科学家 
都是/nr 背景 爆表 动不动 就是 国外 名校 的 Phd 至少 
也 是 国内 清 北 C9 实际上 针对 普通 的 
算法 开发 岗 从 学习 梯度 上 来讲 算法 和 
工程 的 差异 并不大 当然 也 并不 需要 非要 名校 
硕 博 以前 经常 和 同事 开玩笑 说 好歹 大家 
都是/nr 985 的 本 硕 但 手上 这 工作 找个 
高中生 也能 妥妥 搞定 啊 虽然 是 玩笑 线上 的 
产品 当然 也 不是 像 Demo 那么 简单 但 真实 
的 工作 真的 没有 那么 明显 的 条条框框 限制 自己 
也 是 面试官 百度 这样 的 大厂 对 普通 的 
开发 要求 也 只是 大专 针对 普通 的 算法 开发 
自己 觉得 本科 基本 足够 了 当然 更 没有 像 
211/985 这样 的 限制 一般来讲 从事 某个 领域 的 工作 
从 底层 基础 到 业务 实现 一般 包括 如下 的 
几个 层次 以 互联网 移动 开发 和 算法 开发 为例 
方向 通用 基础 专业 基础 领域 基础 业务 方向 移动 
开发 操作系统 计算机网络 编译原理 数据结构 编程语言 设计模式 Android 开发 iOS 
开发 驱动 开发 Framework 开发 应用 开发 电商 社交 智能 
硬件 算法 开发 编程语言 python 数学 基础 线性代数 概率 / 
统计 微积分 机器学习 深度 学习 语音 自然语言 理解 计算机 视觉 
推荐 计算 广告 风 控 搜索 智能 客服 广告 推荐 
互联网 金融 从 技能 栈 的 对比 来看 算法 开发 
对 数学 要求 要 高些 这 又会 让 很多 同学 
看着 头痛 实际上 对 数学 的 恐惧 主要 原因 在于 
对 大多数 人 来说 数学 主要 是 用来 应付 作业 
和 考试 的 而 很少 在 真实 的 工程 场景 
中 使用 它 自己 的 本科 专业 是 电子 信息 
方向 觉得 空闲 时间 多 顺手 拿了 个 计算机 科学 
与 技术 的 双学士 有 一些 专业 课程 也 蛮 
让人 头痛 的 比如 通信 原理 信号 与 系统 数字 
信号 处理 微机 原理 数字电路 等 五花八门 的 抽象 概念 
和 算法 公式 当时 参加 全国 大学生 电子设计 竞赛 选入 
的 学校 的 电子设计 校队 参加 系统 的 学习 和 
培训 有 半年 的 时间 参加 集中 培训 不用 像 
其他 同学 一样 日常 上课 当 大家 还 在 纠结 
通信 系统 里 的 调制 解调 原理 时 我们 已经 
在 做 单 边带 调制 收音机 了 当 大家 还 
在 纠结 信号 与 系统 的 傅里叶 变换 时 我们 
已经 在 做 基于 快速 傅里叶 变换 的 频谱仪 了 
当 大家 还 在 纠结 数字 信号 处理 里 的 
滤波器 时 我们 已经 在 做 50Hz 及其 谐波 过滤 
的 工频 陷波 器 了 当 大家 还 在 纠结 
微机 原理 的 X86 汇编 指令 时 我们 已经 用上 
工业级 的 Msp430 和 C8051F 系列 控制器 了 当 大家 
还 在 纠结 数字电路 的 逻辑 控制 和 抱怨 VHDL 
难 学时 我们 已经 在 用 Verilog 玩 Altera 的 
FPGA 了 本来 是 非常 枯燥 有 难以 对付 的 
专业课 但有 了 实际 的 使用 场景 和 工程 实践 
反而 变得 非常 有趣 不但 专业课 的 教材 就 在手边 
经常 翻阅 还会 专门 找 相应 的 Paper 看看 有 
没有 更好 更新 的 方案 自己 对 这些 专业课 不但 
没有 反感 反而 觉得 超级 有用 知识 就是 力量 在 
这一刻 特别 贴切 我们 回过头来 说 数学 也是 同样 的 
体验 算法 的 开发 工作 为 数学 和 算法 提供 
了 实践 的 土壤 理论 有了/nr 实践 这块 沃土 也就 
不再 那么 枯燥 和 晦涩 像 很多 数学 大神 一样 
徒 手推 公式 确实是 件 很难 的 事情 但 基于 
基础 的 数学 知识 来 解决 工程 问题 这 并 
没有 想象 中 的 那么 难 而且 学习 本来 就是 
一个 往复 的 过程 先 有一个 大概 尝试 用 已有 
的 知识 解决 问题 当 问题 解决 不 掉 时 
再 反过来 学习 自己 欠缺 的 知识 说 了 信心 
再说 兴趣 很多 同学 会 觉得 目前 机器学习 和 深度 
学习 大火 是不是 就 该 放弃 手上 的 工程 岗位 
全力以赴 的 加入 到 算法 的 大军 中 自己 觉得 
要 不 加入 这个 方向 还是 看 个人 兴趣 吧 
现在 算法 岗位 炙手可热 但 三五 年 之后 就 不 
好说 了 典型 的 就是 Android / iOS 移动 开发 
10年 左右 如日中天 市场 蓝海 人才 紧俏 公司 抢人 的 
盛况 应该 跟 现在 差不多 收益 的 决定 因素 是 
市场 但 个人 的 成长 从 长期 来看 还要 看 
自己 的 兴趣 依照 T 型 能力 理论 来说 深度 
方面 前端 后台 架构 嵌入式 客户端 都 OK 广度 方面 
机器/n 学习/v 和/c 深度/ns 学习/v 不论/c 从/p 是否/v 要/v 从事/v 
相关/v 的/uj 领域/n 开发/v 花些 时间 了解 和 学习 一下 
总的来说 也 还是 不错 的 好了 下面 我们 就 具体 
看看 从事 自然语言 处理 需要 准备 的 基础 知识 吧 
编程语言 编程语言 的 要求 是 很低 的 弄 掌握 Python 
就 基本 OK 我 的 自己 经验 是 1 在 
网上 找 一篇 Python 入门 的 帖子 搭建 环境 运行 
简单 的 例子 半天 2 找 一本 基础 书籍 系统 
的 熟悉 下 语言 的 基本 特性 和 完整 框架 
1 ~ 2天 ps 我/r 之前/f 有C//nr C/w +/i +/i 
和/c Java/w 的/uj 语音/n 基础/n 3/m 开始 正常 使用 Python 
进行 开发 遇到问题 求助 搜索引擎 2个 月 以上 4 觉得 
自己 对 语言 就 基本 的 掌握 后 可以 根据 
选择 进行 进阶 学习 了 系统 的 基础 学习 可以 
参考 Python 基础教程 晋级 学习 可以 参考 流畅 的 Python 
经典 数据结构 与 算法 经典 的 数据 结构 和 算法 
主要指 数组 链表 队列 堆栈 树 图 这样 的 经典 
数据结构 以及 各种 排序 / 查找 深度搜索 广度 搜索 最小 
路径 Hash 等 算法 对 一般 的 算法 开发 这部分 
不是 必须 项 如果 时间 有限 可以 跳过 这部分 但从 
长远 来看 无论 是 普通 的 工程 开发 还是 算法 
开发 经典 的 数据 结构 和 算法 还是 必要 了 
落实 到 具体 的 学习 上 基础 的 数据结构 与 
算法 应该 随便 一本 教科书 都 OK 在 实际 操作 
方面 可以 抽空 刷 一刷 LeetCode 书 的话 可以 考虑 
进军 硅谷 程序员 面试 解密 内容 基本 都是 LeetCode 的 
原 题 可以 先 刷 题 再 看书 如果 LeetCode 
上 能 持续 刷上 200 + 的 题目 以 后面 
对 经典 的 数据 结构 和 算法 问题 应该 也 
就 没有 恐惧 的 感觉 了 反而 遇到 新 问题 
还会 饶有兴趣 的 去 研究 研究 解决 方案 数学 基础 
对于 算法 开发 本科 学习 的 微积分 线性代数 概率 / 
统计 基本上 就 够用 了 有些 同学 会 提到 概率 
图 模型 和 凸 优化 等 我 觉得 对于 入门 
来说 前面 的 三门 课 基本 够用 后面 如果 真的 
觉得 需要 深入 再看 后面 的 内容 也 不迟 如果 
觉得 当时 大学 的 课程 成绩 还 不错 那 最快 
的 方法 就是 把 大学 的 教材 拿出 来 重新 
再 扫 一般 即可 如果 觉得 时间 充裕 想再 系统 
的 学习 一遍 在线 视频 是 个 不错 的 选择 
MIT 单 变量 微积分 MIT 线性代数 可汗 学院 概率 可汗 
学院 统计学 学习 的 过程 不要 太 苛求 全面 理解 
没 必要 像 数学 大牛 一样能 把 所有 的 公式 
都 手动 的 推导 一遍 能理解 基本 的 概念 和 
原理 关键 是 有 系统 的 知识 框架 后面 遇到 
细节 问题 可以 回过头来 再看 机器学习 这 部分 目前 资料 
应该 是 铺天盖地 的 多 当然 还有 各式各样 的 培训 
班 自己 觉得 从 入门 角度 只要 把 吴恩 达 
在 Coursera 上 的 机器学习 课程 完整 的 学习 完 
并 完成 作业 拿到 证书 机器学习 这 部分 就算 及格 
了 Coursera MachineLearning 一定 要 完成 作业 一定 要 完成 
作业 一定 要 完成 作业 重要 的 事情 说 三遍 
Coursera 不 翻墙 速度 好像 有些 慢 网易 云 课堂 
也有 视频 但 不能 提交 作业 作业 至于 书 的话 
可以 看看 周志华 老师 的 西瓜 书 机器学习 深度 学习 
看完 前面 机器学习 的 内容 你 会 发现 两个 事情 
机器 学习 的 各种 算法 好像 都是 些 非常 经典 
的 算法 基本 都是 2000年 之前 提出 的 现在 大家 
都在搞/nr 深度 学习 了 这些 老掉牙 的 算法 越来越 没 
人用 了 真实 情况 也 大抵 如此 如果 直接 跳过 
机器学习 直接 学习 深度 学习 可以 吗 当然 可以 啊 
而且 直接 学习 深度 学习 还 会 觉得 深度 学习 
的 入门 门槛 更低 这 估计 会 让 很多 同学 
觉得 反 常识 但 反过来 你 问问 自己 如果 没有 
学习 过 汇编 上来 就用 C 语言 搞 嵌入式应用 开发 
可以 吗 没有 学习 过 C + + 上来 就用 
Java 搞 Android 开发 可以 吗 当然 可以 啊 从 
入门 角度 没问题 但从 长期 来看 还是 需要 的 想 
深入 一个 领域 知识 的 完备 是 必要 的 比如 
嵌入 开发 用 汇编 写过 两级 中断向量 表 做 Android 
开发 也 开发 过 JNI 深度 学习 也 是 一样 
从 最快 入门 的 角度 来看 可以 跳过 机器学习 直接 
进入 深度 学习 但从 长期 看 机器学习 还是 必要 的 
而且 看完 了 吴恩 达 的 机器学习 课程 再看 他 
的 深度 学习 也 更 流畅 Coursera DeepLearning 一定 要 
完成 作业 一定 要 完成 作业 一定 要 完成 作业 
重要 的 事情 说 三遍 补充 下 吴恩 达 在 
Coursera 上 的 课程 MachineLearning 是 免费 观看 的 在线 
作业 是 付费 的 DeepLearning 无论 是 视频 还是 在线 
作业 都是 付费 的 网易/n 云/ns 课堂/n 有/v 免费/vn 的/uj 
视频/n 课程/n 但 不能 提交 在线 作业 其实 Coursera 上 
的 课程 也 挺 便宜 的 做完作业 还有 证书 还是 
挺 不错 的 至于 书 的话 可以 看看 花书/nr 深度 
学习 自然语言 处理 好了 终于 到了 这篇文章 的 核心 部分 
了 因为 是 专业 基础 所有 市 面上 的 系统 
学习 资料 也就 没 前面 那么 多了 但 仔细 找找 
的话 也 不少 都 需要 翻墙 手动 捂脸 经典 自然语言 
处理 斯坦福 Natural Language Processing course by Dan Jurafsky and 
Christopher Manning 经典 + 深度 学习 National Research University Higher 
School of Economics Natural Language Processing 深度 学习 自然语言 处理 
斯坦福 Natural language processing with DeepLearning 深度 学习 自然语言 处理 
CMU CMU Neural Nets for NLP 除 了 第一 个 
之外 后面 的 视频 都 比较 新 相对来说 书 的 
资料 就 相对 滞后 了 如果/c 有/v 兴趣/n 可以/c 翻翻/v 
经典/n 的/uj 宗/nr 成庆/i 的/uj 统计 自然语言 处理 主要 是 
针对 经典 的 自然 语言 处理 方法 虽然 方法 是 
经典 的 但 领域 问题 的 描述 还是 很 全面 
的 小节 总的来说 算法 开发 和 普通 的 工程 开发 
从 学习 梯度 上 来讲 差别 不大 也 不 需要 
高大 上 的 背景 和 学历 确定 兴趣 看好 方向 
就 坚持 一步 一步 积累 理论 结合 实践 相信 很快 
你 也能 像 我 一样 跨过 算法 开发 的 门槛 
体 会用 算法 解决 实际 工程 问题 的 乐趣 从 
机器学习 谈起 在 本篇 文章 中 我 将对 机器学习 做个 
概要 的 介绍 本文 的 目的 是 能让 即便 完全 
不 了解 机器学习 的 人也 能了解 机器学习 并且 上手 相关 
的 实践 这篇 文档 也 算是 EasyPR 开发 的 番外篇 
从 这里 开始 必须 对 机器 学习 了解 才能 进一步 
介绍 EasyPR 的 内核 当然 本文 也 面对 一般 读者 
不会 对 阅读 有 相关 的 前提 要求 在 进入 
正题 前 我 想 读者 心中 可能 会 有一个 疑惑 
机器学习 有 什么 重要性 以至于 要 阅 读完 这篇 非常 
长 的 文章 呢 我 并不 直接 回答 这个 问题 
前 相反 我 想 请 大家 看 两张 图 下图 
是 图一 这幅 图 上上 的 三人 是 当今 机器学习 
界 的 执牛耳 者 中间 的 是 Geoffrey Hinton 加拿大 
多伦多 大学 的 教授 如今 被 聘为 Google 大脑 的 
负责人 右边 的 是 Yann LeCun 纽约 大学 教授 如今 
是 Facebook 人工智能 实验室 的 主任 而 左边 的 大家 
都 很熟悉 Andrew Ng 中文名 吴恩 达 斯坦福 大学 副教授 
如今 也是 百度 大脑 的 负责人 与 百度 首席 科学家 
这 三位 都是/nr 目前 业界 炙手可热 的 大牛 被 互联网 
界 大鳄 求贤若渴 的 聘请 足见 他们 的 重要性 而 
他们 的 研究 方向 则 全部 都是/nr 机器 学习 的 
子类 深度 学习 下图 是 图二 这幅 图上 描述 的 
是 什么 Windows Phone 上 的 语音 助手 Cortana 名字 
来源于 光环 中 士官长 的 助手 相比 其他 竞争 对手 
微软 很迟 才 推出 这个 服务 Cortana 背后 的 核心 
技术 是 什么 为什么 它 能够 听懂 人 的 语音 
事实上 这个 技术 正是 机器学习 机器学习 是 所有 语音 助手 
产品 包括 Apple 的 siri 与 Google 的 Now 能够 
跟人 交互 的 关键 技术 通过 上面 两 图 我 
相信 大家 可以 看出 机器学习 似乎 是 一个 很 重要 
的 有 很多 未知 特性 的 技术 学习 它 似乎 
是 一件 有趣 的 任务 实际上 学习 机器学习 不仅 可以 
帮助 我们 了解 互联网界 最新 的 趋势 同时 也 可以 
知道 伴随 我们 的 便利 服务 的 实现 技术 机器学习 
是 什么 为什么 它 能有 这么 大 的 魔力 这些 
问题 正是 本文 要 回答 的 同时 本文 叫做 从 
机器学习 谈起 因此会 以 漫谈 的 形式 介绍 跟 机器学习 
相关 的 所有 内容 包括 学科 如 数据挖掘 计算机 视觉 
等 算法 神经网络 svm 等等 本文 的 主要 目录 如下 
1 . 一个 故事 说明 什么 是 机器学习 2 . 
机器学习 的 定义 3 . 机器学习 的 范围 4 . 
机器学习 的 方法 5 . 机器学习 的 应用 大 数据 
6 . 机器 学习 的 子类 深度 学习 7 . 
机器学习 的 父 类 人工智能 8 . 机器 学习 的 
思考 计算机 的 潜意识 9 . 总结 10 . 后记 
1 . 一个 故事 说明 什么 是 机器学习 机器学习 这个词 
是 让人 疑惑 的 首先 它 是 英文 名称 Machine 
Learning 简称 ML 的 直译 在 计算 界 Machine 一般指 
计算机 这个 名字 使用 了 拟人 的 手法 说明了 这 
门 技术 是 让 机器 学习 的 技术 但是 计算机 
是 死 的 怎么 可能 像 人类 一样 学习 呢 
传统 上 如果 我们 想 让 计算机 工作 我们 给 
它 一串 指令 然后 它 遵照 这个 指令 一步步 执行 
下去 有因 有果/nr 非常 明确 但 这样 的 方式 在 
机器 学习 中 行不通 机器学习 根本 不 接受 你 输入 
的 指令 相反 它 接受 你 输入 的 数据 也 
就是说 机器学习 是 一种 让 计算机 利用 数据 而 不是 
指令 来 进行 各种 工作 的 方法 这 听 起来 
非常 不可思议 但 结果 上 却 是 非常 可行 的 
统计 思想 将在 你 学习 机器学习 相关 理念 时 无时无刻 
不 伴随 相关 而 不是 因果 的 概念 将 是 
支撑 机器学习 能够 工作 的 核心 概念 你 会 颠覆 
对 你 以前 所有 程序 中 建立 的 因果 无处不在 
的 根本 理念 下面 我 通过 一个 故事 来 简单 
地 阐明 什么 是 机器学习 这个 故事 比较 适合 用在 
知乎 上 作为 一个 概念 的 阐明 在 这里 这个 
故事 没有 展开 但 相关 内容 与 核心 是 存在 
的 如果 你 想 简单 的 了解 一下 什么 是 
机器学习 那么 看完 这个 故事 就 足够 了 如果 你 
想 了解 机器 学习 的 更多 知识 以及 与 它 
关联 紧密 的 当代 技术 那么 请 你 继续 往下看 
后面 有 更多 的 丰富 的 内容 这个 例子 来源于 
我 真实 的 生活 经验 我 在 思考 这个 问题 
的 时候 突然 发现 它 的 过程 可以 被 扩充 
化为 一个 完整 的 机器 学习 的 过程 因此 我 
决定 使用 这个 例子 作为 所有 介绍 的 开始 这个 
故事 称为 等人 问题 我/r 相信/v 大家/n 都有跟/nr 别人/r 相约/d 
然后 等人 的 经历 现实 中 不是 每个人 都 那么 
守时 的 于是 当 你 碰到 一些 爱 迟到 的 
人 你 的 时间 不可避免 的 要 浪费 我 就 
碰到 过 这样 的 一个 例子 对 我 的 一个 
朋友 小 Y 而言 他 就 不是 那么 守时 最 
常见 的 表现 是 他 经常 迟到 当 有 一次 
我 跟 他 约好 3 点钟 在 某个 麦当劳 见面 
时 在 我 出门 的 那 一刻 我 突然 想到 
一个 问题 我 现在 出发 合适 么 我会 不会 又 
到了 地点 后 花上 30 分钟 去 等 他 我 
决定 采取 一个 策略 解决 这个 问题 要想 解决 这个 
问题 有 好几 种 方法 第 一种 方法 是 采用 
知识 我 搜寻 能够 解决 这个 问题 的 知识 但 
很遗憾 没有人 会 把 如何 等人 这个 问题 作为 知识 
传授 因此 我 不 可能 找到 已有 的 知识 能够 
解决 这个 问题 第二 种 方法 是 问 他人 我 
去 询问 他 人 获得 解决 这个 问题 的 能力 
但是 同样 的 这个 问题 没 有人 能够 解答 因为 
可能 没人 碰上 跟 我 一样 的 情况 第三 种 
方法 是 准则 法 我 问 自己 的 内心 我 
有否 设立 过 什么 准则 去 面对 这个 问题 例如 
无论 别人 如何 我 都会 守时 到达 但 我 不是 
个 死板 的 人 我 没有 设立 过 这样 的 
规则 事实上 我/r 相信/v 有/v 种方法/i 比/p 以上/f 三种/m 都/d 
合适/a 我 把 过往 跟 小 Y 相约 的 经历 
在 脑海 中 重现 一下 看看 跟 他 相约 的 
次数 中 迟到 占了 多大 的 比例 而 我 利用 
这 来 预测 他 这次 迟到 的 可能性 如果 这个 
值 超出 了 我 心里 的 某个 界限 那我/nr 选择 
等 一会 再 出发 假设 我 跟 小 Y 约 
过 5次 他 迟到 的 次数 是 1次 那么 他 
按时 到 的 比例 为 80% 我 心中 的 阈值 
为 70% 我 认为 这次 小 Y 应该 不会 迟到 
因此 我 按时 出门 如果 小 Y 在 5次 迟到 
的 次数 中 占了 4次 也 就是 他 按时 到达 
的 比例 为 20% 由于 这个 值 低于 我 的 
阈值 因此 我 选择 推迟 出门 的 时间 这个 方法 
从它的/nr 利用 层面 来看 又 称为 经验 法 在 经验 
法的/nr 思考 过程 中 我 事实上 利用 了 以往 所有 
相约 的 数据 因此 也 可以 称之为 依据 数据 做 
的 判断 依据 数据 所做 的 判断 跟 机器学习 的 
思想 根本上 是 一致 的 刚才 的 思考 过程 我 
只 考虑 频次 这种 属性 在 真实 的 机器 学习 
中 这 可能 都 不算 是 一个 应用 一般 的 
机器学习 模型 至少 考虑 两 个量 一个 是 因变量 也 
就是 我们 希望 预测 的 结果 在 这个 例子 里 
就是 小 Y 迟到 与否 的 判断 另 一个 是 
自变量 也 就是 用来 预测 小 Y 是否 迟到 的 
量 假设 我 把 时间 作为 自变量 譬如 我 发现 
小 Y 所有 迟到 的 日子 基本 都是/nr 星期五 而在 
非 星期五 情况下 他 基本 不 迟到 于是 我 可以 
建立 一个 模型 来 模拟 小 Y 迟到 与否 跟 
日子 是否 是 星期五 的 概率 见 下图 这样 的 
图 就是 一个 最 简单 的 机器学习 模型 称之为 决策树 
当 我们 考虑 的 自变量 只有 一个 时 情况 较为 
简单 如果 把 我们 的 自变量 再 增加 一个 例如 
小 Y 迟到 的 部分 情况 时是在/nr 他 开车 过来 
的 时候 你 可以 理解 为 他 开车 水平 较 
臭 或者 路较堵/nr 于是 我 可以 关联 考虑 这些 信息 
建立 一个 更 复杂 的 模型 这个 模型 包含 两个 
自变量 与 一个 因变量 再 更 复杂 一点 小 Y 
的 迟到 跟 天气 也 有 一定 的 原因 例如 
下雨 的 时候 这时候 我 需要 考虑 三个 自变量 如果 
我 希望 能够 预测 小 Y 迟到 的 具体 时间 
我 可以 把 他 每次 迟到 的 时间 跟 雨量 
的 大小 以及 前面 考虑 的 自变量 统一 建立 一个 
模型 于是 我 的 模型 可以 预测 值 例如 他 
大概 会 迟到 几分钟 这样 可以 帮助 我 更好 的 
规划 我 出门 的 时间 在 这样 的 情况 下 
决策树 就 无法 很好 地 支撑 了 因为 决策树 只能 
预测 离散 值 我们 可以 用 节 2所 介绍 的 
线型 回归 方法 建立 这个 模型 如果 我 把 这些 
建立 模型 的 过程 交给 电脑 比如 把 所有 的 
自变量 和 因变量 输入 然后 让 计算机 帮 我 生成 
一个 模型 同时 让 计算机 根据 我 当前 的 情况 
给出 我 是否 需要 迟 出门 需要 迟 几 分钟 
的 建议 那么 计算机 执行 这些 辅助 决策 的 过程 
就是 机器学习 的 过程 机器学习 方法 是 计算机 利用 已有 
的 数据 经验 得出 了 某种 模型 迟到 的 规律 
并 利用 此 模型 预测 未来 是否 迟到 的 一种 
方法 通过 上面 的 分析 可以 看出 机器学习 与 人类 
思考 的 经验 过程 是 类似 的 不过 它 能 
考虑 更多 的 情况 执行 更加 复杂 的 计算 事实上 
机器 学习 的 一个 主要 目的 就是 把 人类 思考 
归纳 经验 的 过程 转化 为 计算机 通过 对 数据 
的 处理 计算 得出 模型 的 过程 经过 计算机 得出 
的 模型 能够 以 近似 于人 的 方式 解决 很多 
灵活 复杂 的 问题 下面 我会 开始 对 机器 学习 
的 正式 介绍 包括 定义 范围 方法 应用 等等 都 
有所 包含 2 . 机器 学习 的 定义 从 广义 
上 来说 机器学习 是 一种 能够 赋予 机器 学习 的 
能力 以此 让 它 完成 直接 编程 无法 完成 的 
功能 的 方法 但从 实践 的 意义 上 来说 机器学习 
是 一种 通过 利用 数据 训 练出 模型 然后 使用 
模型 预测 的 一种 方法 让 我们 具体 看 一个 
例子 拿 国民 话题 的 房子 来说 现在 我 手里 
有 一栋 房子 需要 售卖 我 应该 给 它 标上 
多大 的 价格 房子 的 面积 是 100 平方米 价格 
是 100万 120万 还是 140万 很显然 我 希望 获得 房价 
与 面积 的 某种 规律 那么 我 该 如何 获得 
这个 规律 用 报纸 上 的 房价 平均 数据 么 
还是 参考 别人 面积 相似 的 无论 哪种 似乎 都并/nr 
不是 太 靠谱 我 现在 希望 获得 一个 合理 的 
并且 能够 最大 程度 的 反映 面积 与 房价 关系 
的 规律 于是 我 调查 了 周边 与 我 房型 
类似 的 一些 房子 获得 一组 数据 这组 数据 中 
包含 了 大大小小 房子 的 面积 与 价格 如果 我 
能从 这组 数据 中 找出 面积 与 价格 的 规律 
那么 我 就 可以 得出 房子 的 价格 对 规律 
的 寻找 很 简单 拟合 出 一条 直线 让 它 
穿过 所有 的 点 并且 与 各个 点 的 距离 
尽可能 的 小 通过 这 条 直线 我 获得 了 
一个 能够 最佳 反映 房价 与 面积 规律 的 规律 
这条 直线 同时 也 是 一个 下式 所 表明 的 
函数 房价 = 面积 * a + b 上述 中的 
a b 都是 直线 的 参数 获得 这些 参数 以后 
我 就 可以 计算出 房子 的 价格 假设 a = 
0.75 b = 50 则 房价 = 100 * 0.75 
+ 50 = 125万 这个 结果 与 我 前面 所列 
的 100万 120万 140万 都 不一样 由于 这 条 直线 
综合 考虑 了 大部分 的 情况 因此 从 统计 意义 
上 来说 这 是 一个 最 合理 的 预测 在 
求解 过程 中 透露 出 了 两个 信息 1 . 
房价 模型 是 根据 拟合 的 函数 类型 决定 的 
如果 是 直线 那么 拟 合出 的 就是 直线 方程 
如果 是 其他 类型 的 线 例如 抛物线 那么 拟 
合出 的 就是 抛物线 方程 机器学习 有 众多 算法 一些 
强力 算法 可以 拟合 出 复杂 的 非线性 模型 用来 
反映 一些 不是 直线 所能 表达 的 情况 2 . 
如果 我 的 数据 越多 我 的 模型 就 越 
能够 考虑 到 越多 的 情况 由此 对于 新 情况 
的 预测 效果 可能 就 越好 这是 机器学习 界 数据 
为王 思想 的 一个 体现 一般来说 不是 绝对 数据 越多 
最后 机器学习 生成 的 模型 预测 的 效果 越好 通过 
我 拟合 直线 的 过程 我们 可以 对 机器 学习 
过程 做 一个 完整 的 回顾 首先 我们 需要 在 
计算机 中 存储 历史 的 数据 接着 我们 将 这些 
数据 通过 机器学习 算法 进行 处理 这个 过程 在 机器 
学习 中 叫做 训练 处理 的 结果 可以 被 我们 
用来 对 新的 数据 进行 预测 这个 结果 一般 称之为 
模型 对 新 数据 的 预测 过程 在 机器 学习 
中 叫做 预测 训练 与 预测 是 机器 学习 的 
两个 过程 模型 则是 过程 的 中间 输出 结果 训练 
产生 模型 模型 指导 预测 让 我们 把 机器 学习 
的 过程 与 人类 对 历史 经验 归纳 的 过程 
做个 比对 人类 在 成长 生活 过程 中 积累 了 
很多 的 历史 与 经验 人类 定期 地 对 这些 
经验 进行 归纳 获得 了 生活 的 规律 当 人类 
遇到 未知 的 问题 或者 需要 对 未来 进行 推测 
的 时候 人类 使用 这些 规律 对 未知 问题 与 
未来 进行 推测 从而 指导 自己 的 生活 和 工作 
机器学习 中的 训练 与 预测 过程 可以 对应 到 人类 
的 归纳 和 推测 过程 通过 这样 的 对应 我们 
可以 发现 机器 学习 的 思想 并 不复杂 仅仅 是 
对 人类 在 生活 中 学习 成长 的 一个 模拟 
由于 机器学习 不是 基于 编程 形成 的 结果 因此 它 
的 处理 过程 不是 因果 的 逻辑 而是 通过 归纳 
思想 得出 的 相关性 结论 这 也 可以 联想 到 
人类 为什么 要 学习 历史 历史 实际上 是 人类 过往 
经验 的 总结 有句 话说 得 很好 历史 往往 不 
一样 但 历史 总是 惊人 的 相似 通过 学习 历史 
我们 从 历史 中 归纳 出 人生 与 国家 的 
规律 从而 指导 我们 的 下 一步 工作 这是 具有 
莫大 价值 的 当代 一些 人 忽视 了 历史 的 
本来 价值 而是 把 其 作为 一种 宣扬 功绩 的 
手段 这 其实 是 对 历史 真实 价值 的 一种 
误用 3 . 机器 学习 的 范围 上文 虽然 说明 
了 机器 学习 是 什么 但是 并 没有 给出 机器 
学习 的 范围 其实 机器学习 跟 模式识别 统计 学习 数据挖掘 
计算机 视觉 语音识别 自然语言 处理 等 领域 有着 很深 的 
联系 从 范围 上 来说 机器学习 跟 模式识别 统计 学习 
数据挖掘 是 类似 的 同时 机器学习 与 其他 领域 的 
处理 技术 的 结合 形成 了 计算机 视觉 语音识别 自然语言 
处理 等 交叉 学科 因此 一般 说 数据挖掘 时 可以 
等同于 说 机器学习 同时 我们 平常 所说 的 机器学习 应用 
应该 是 通用 的 不 仅仅 局限 在 结构化 数据 
还有 图像 音频 等 应用 在 这 节 对 机器学习 
这些 相关 领域 的 介绍 有助于 我们 理清 机器学习 的 
应用 场景 与 研究 范围 更好 的 理解 后面 的 
算法 与 应用 层次 下图 是 机器 学习 所 牵扯 
的 一些 相关 范围 的 学科 与 研究 领域 模式识别 
模式识别 = 机器学习 两者 的 主要 区别 在于 前者 是 
从 工业 界 发展 起来 的 概念 后者 则 主要 
源自 计算机 学科 在 著名 的 Pattern Recognition And Machine 
Learning 这 本书 中 Christopher M . Bishop 在 开头 
是 这样 说 的 模式识别 源自 工业界 而 机器学习 来自 
于 计算机 学科 不过 它们 中 的 活动 可以 被 
视为 同 一个 领域 的 两个 方面 同时 在 过去 
的 10 年间 它们 都 有了 长足 的 发展 数据挖掘 
数据挖掘 = 机器学习 + 数据库 这几年 数据挖掘 的 概念 实在 
是 太 耳熟能详 几乎 等同 于 炒作 但凡 说 数据挖掘 
都会 吹嘘 数据挖掘 如何 如何 例如 从 数据 中 挖出 
金子 以及 将 废弃 的 数据 转化 为 价值 等等 
但是 我 尽管 可能会 挖出 金子 但 我 也可能 挖 
的 是 石头 啊 这个 说法 的 意思 是 数据挖掘 
仅仅 是 一种 思考 方式 告诉 我们 应该 尝试 从 
数据 中 挖掘 出 知识 但/c 不是/c 每个/r 数据/n 都能/nr 
挖掘/v 出/v 金子/n 的/uj 所以 不要 神话 它 一个 系统 
绝对 不会 因为 上 了 一个 数据挖掘 模块 就 变得 
无所不能 这是 IBM 最喜欢 吹嘘 的 恰恰相反 一个 拥有 数据挖掘 
思维 的 人员 才 是 关键 而且 他 还 必须 
对 数据 有 深刻 的 认识 这样 才 可能 从 
数据 中 导出 模式 指引 业务 的 改善 大 部分 
数据 挖掘 中 的 算法 是 机器 学习 的 算法 
在 数据库 中 的 优化 统计 学习 统计 学习 近似 
等于 机器学习 统计 学习 是个 与 机器学习 高度 重叠 的 
学科 因为 机器 学习 中 的 大多数 方法 来自 统计学 
甚至 可以 认为 统计学 的 发展 促进 机器 学习 的 
繁荣昌盛 例如 著名 的 支持 向量 机 算法 就是 源自 
统计 学科 但是 在 某种 程度 上 两者 是 有 
分别 的 这个 分别 在于 统计 学习者 重点 关注 的 
是 统计 模型 的 发展 与 优化 偏 数学 而 
机器 学习者 更 关注 的 是 能够 解决 问题 偏 
实践 因此 机器学习 研究者 会 重点 研究学习 算法 在 计算机 
上 执行 的 效率 与 准确性 的 提升 计算机 视觉 
计算机 视觉 = 图像处理 + 机器学习 图像 处理 技术 用于 
将 图像 处理 为 适合 进入 机器学习 模型 中 的 
输入 机器学习 则 负责 从 图像 中 识别 出 相关 
的 模式 计算机 视觉 相关 的 应用 非常 的 多 
例如 百度识图 手写 字符识别 车牌 识别 等 等 应用 这个 
领域 是 应用 前景 非常 火热 的 同时 也 是 
研究 的 热门 方向 随着 机器 学习 的 新领域 深度 
学习 的 发展 大大 促进 了 计算机 图像 识别 的 
效果 因此 未来 计算机 视觉 界 的 发展 前景 不可估量 
语音识别 语音识别 = 语音 处理 + 机器学习 语音识别 就是 音频 
处理 技术 与 机器 学习 的 结合 语音 识别 技术 
一般 不会 单独 使用 一般 会 结合 自然语言 处理 的 
相关 技术 目前 的 相关 应用 有 苹果 的 语音 
助手 siri 等 自然语言 处理 自然语言 处理 = 文本处理 + 
机器学习 自然语言 处理 技术 主要 是 让 机器 理解 人类 
的 语言 的 一门 领域 在 自然 语言 处理 技术 
中 大量 使用 了 编译原理 相关 的 技术 例如 词 
法分析 语法分析 等等 除此之外 在 理解 这个 层面 则 使用 
了 语义 理解 机器学习 等 技术 作为 唯一 由 人类 
自身 创造 的 符号 自然语言 处理 一直 是 机器学习 界 
不断 研究 的 方向 按照 百度 机器学习 专家 余凯 的 
说法 听 与 看 说白 了 就是 阿猫 和 阿狗 
都会 的 而 只有 语言 才是 人类 独有 的 如何 
利用 机器学习 技术 进行 自然 语言 的 的 深度 理解 
一直 是 工业 和 学术界 关注 的 焦点 可以 看出 
机器学习 在 众多 领域 的 外延 和 应用 机器学习 技术 
的 发展 促使 了 很多 智能 领域 的 进步 改善 
着 我们 的 生活 4 . 机器 学习 的 方法 
通过 上节 的 介绍 我们 知晓 了 机器 学习 的 
大致 范围 那么 机器学习 里面 究竟 有 多少 经典 的 
算法 呢 在 这个 部分 我会 简要 介绍 一下 机器学习 
中的 经典 代表 方法 这部分 介绍 的 重点 是 这些 
方法 内涵 的 思想 数学 与 实践 细节 不会在 这 
讨论 1 回归 算法 在 大部分 机器学习 课程 中 回归 
算法 都是/nr 介绍 的 第一 个 算法 原因 有 两个 
一 . 回归 算法 比较简单 介绍 它 可以 让人 平滑 
地 从 统计学 迁移 到 机器学习 中 二 . 回归 
算法 是 后面 若干 强大 算法 的 基石 如果 不 
理解 回归 算法 无法 学习 那些 强大 的 算法 回归 
算法 有 两个 重要 的 子类 即 线性 回归 和 
逻辑 回归 线性 回归 就是 我们 前面 说过 的 房价 
求解 问题 如何 拟合 出 一条 直线 最佳 匹配 我 
所有 的 数据 一般 使用 最小二乘 法 来 求解 最小二乘 
法 的 思想 是 这样 的 假设 我们 拟 合出 
的 直线 代表 数据 的 真实 值 而 观测 到 
的 数据 代表 拥有 误差 的 值 为了 尽可能 减小 
误差 的 影响 需要 求解 一条 直线 使 所有 误差 
的 平方和 最小 最小二乘 法将/nr 最优 问题 转化 为 求函数 
极值问题 函数 极值 在 数学 上 我们 一般 会 采用 
求 导数 为 0 的 方法 但 这种 做法 并不 
适合 计算机 可能 求解 不出来 也 可能 计算 量 太大 
计算机 科学界 专门 有 一个 学科 叫 数值 计算 专门/n 
用来/v 提升/v 计算机/n 进行/v 各类/r 计算/v 时的/nr 准确性/n 和/c 效率/n 
问题/n 例如 著名 的 梯度 下降 以及 牛顿 法 就是 
数值 计算 中 的 经典 算法 也 非常 适合 来 
处理 求解 函数 极值 的 问题 梯度 下 降法 是 
解决 回归模型 中 最简单 且 有效 的 方法 之一 从 
严格 意义 上 来说 由于/c 后/f 文中/s 的/uj 神经/n 网络/n 
和/c 推荐算法/i 中/f 都有/i 线性/n 回归/v 的/uj 因子/n 因此 梯度 
下 降法 在 后面 的 算法 实现 中 也有 应用 
逻辑 回归 是 一种 与 线性 回归 非常 类似 的 
算法 但是 从 本质 上 讲 线型 回归 处理 的 
问题 类型 与 逻辑 回归 不一致 线性 回归 处理 的 
是 数值 问题 也 就是 最后 预测出 的 结果 是 
数字 例如 房价 而 逻辑 回 归属于 分类 算法 也 
就是说 逻辑 回归 预测 结果 是 离散 的 分类 例如 
判断 这封 邮件 是否 是 垃圾 邮件 以及 用户 是否 
会 点击 此 广告 等等 实现 方面 的 话 逻辑 
回归 只是 对对 线性 回归 的 计算 结果 加上 了 
一个 Sigmoid 函数 将 数值 结果 转化 为了 0 到 
1 之间 的 概率 Sigmoid 函数 的 图像 一般来说 并不 
直观 你 只 需要 理解 对 数值 越大 函数 越 
逼近 1 数值 越小 函数 越 逼近 0 接着 我们 
根据 这个 概率 可以 做 预测 例如 概率 大于 0.5 
则 这封 邮件 就 是 垃圾 邮件 或者 肿瘤 是否 
是 恶性 的 等等 从 直观 上 来说 逻辑 回归 
是 画出 了 一条 分类 线 见 下图 假设 我们 
有 一组 肿瘤 患者 的 数据 这些 患者 的 肿瘤 
中 有些 是 良性 的 图中 的 蓝色 点 有些 
是 恶性 的 图中 的 红色 点 这里 肿瘤 的 
红 蓝色 可以 被 称作 数据 的 标签 同时 每个 
数据 包括 两个 特征 患者 的 年龄 与 肿瘤 的 
大小 我们 将 这 两个 特征 与 标签 映射 到 
这个 二 维空间 上 形成 了 我 上图 的 数据 
当 我 有 一个 绿色 的 点 时 我 该 
判断 这个 肿瘤 是 恶性 的 还是 良性 的 呢 
根据 红蓝 点 我们 训 练出 了 一个 逻辑 回归模型 
也 就是 图 中的 分类 线 这时 根据 绿 点 
出现 在 分类 线 的 左侧 因此 我们 判断 它 
的 标签 应该 是 红色 也 就是说 属于 恶性肿瘤 逻辑 
回归 算法 划出 的 分类 线 基本 都是 线性 的 
也有 划出 非线性 分类 线 的 逻辑 回归 不过 那样 
的 模型 在 处理 数据 量 较大 的 时候 效率 
会 很低 这 意味着 当 两类 之间 的 界线 不是 
线性 时 逻辑 回归 的 表达 能力 就 不足 下面 
的 两个 算法 是 机器学习 界 最 强大 且 重要 
的 算法 都 可以 拟 合出 非线性 的 分类 线 
2 神经网络 神经网络 也 称之为 人工神经网络 ANN 算法 是 80 
年代 机器学习 界 非常 流行 的 算法 不过 在 90 
年代 中途 衰落 现在 携 着 深度 学习 之势 神经网络 
重装 归来 重新 成为 最 强大 的 机器学习 算法 之一 
神经 网络 的 诞生 起源于 对 大脑 工作 机理 的 
研究 早期 生物界 学者 们 使用 神经 网络 来 模拟 
大脑 机器 学习 的 学者 们 使用 神经 网络 进行 
机器 学习 的 实验 发现 在 视觉 与 语音 的 
识别 上 效果 都 相当 好 在 BP 算法 加速 
神经网络 训练 过程 的 数值 算法 诞生 以后 神经 网络 
的 发展 进入 了 一个 热潮 BP 算法 的 发明 
人 之一 是 前面 介绍 的 机器学习 大牛 Geoffrey Hinton 
中的 中间 者 具体说来 神经 网络 的 学习 机理 是 
什么 简单 来说 就是 分解 与 整合 在 著名 的 
Hubel Wiesel 试验中 学者 们 研究 猫 的 视觉 分析 
机理 是 这样 的 Hubel Wiesel 试验 与 大脑 视觉 
机理 比方说 一个 正方形 分解为 四个 折线 进入 视觉 处理 
的 下一层 中 四个 神经元 分别 处理 一个 折线 每个 
折线 再 继续 被 分解 为 两条 直线 每条 直线 
再被 分解为 黑白 两个 面 于是 一个 复杂 的 图像 
变成 了 大量 的 细节 进入 神经元 神经元 处理 以后 
再 进行 整合 最后 得出 了 看到 的 是 正方形 
的 结论 这 就是 大脑 视觉 识别 的 机理 也是 
神经网络 工作 的 机理 让 我们 看 一个 简单 的 
神经 网络 的 逻辑 架构 在 这个 网络 中 分成 
输入 层 隐藏 层 和 输出 层 输入 层 负责 
接收 信号 隐藏 层 负责 对 数据 的 分解 与 
处理 最后 的 结果 被 整合 到 输出 层 每层 
中 的 一个 圆 代表 一个 处理单元 可以 认为 是 
模拟 了 一个 神经元 若干个 处理单元 组成 了 一个 层 
若干个 层 再 组成 了 一个 网络 也 就是 神经网络 
在 神经 网络 中 每个 处理单元 事实上 就是 一个 逻辑 
回归模型 逻辑 回归模型 接收 上层 的 输入 把 模型 的 
预测 结果 作为 输出 传输 到 下 一个 层次 通过 
这样 的 过程 神经 网络 可以 完成 非常 复杂 的 
非线性 分类 下图 会 演示 神经 网络 在 图像 识别 
领域 的 一个 著名 应用 这个 程序 叫做 LeNet 是 
一个 基于 多个 隐 层 构建 的 神经 网络 通过 
LeNet 可以 识别 多种 手写 数字 并且 达到 很高 的 
识别 精度 与 拥有 较好 的 鲁棒性 LeNet 的 效果 
展示 右下方 的 方形 中 显示 的 是 输入 计算机 
的 图像 方形 上方 的 红色 字样 answer 后面 显示 
的 是 计算机 的 输出 左边 的 三条 竖直 的 
图像 列 显示 的 是 神经 网络 中 三个 隐藏 
层 的 输出 可以 看出 随着 层次 的 不断 深入 
越深 的 层次 处理 的 细节 越低 例如 层 3 
基本 处理 的 都 已经 是 线 的 细节 了 
LeNet 的 发明 人 就是 前 文 介绍 过 的 
机器 学习 的 大牛 Yann LeCun 右 者 进入 90 
年代 神经 网络 的 发展 进入 了 一个 瓶颈 期 
其 主要 原因 是 尽管 有 BP 算法 的 加速 
神经 网络 的 训练 过程 仍然 很 困难 因此 90 
年代 后期 支持 向量 机 SVM 算法 取代 了 神经 
网络 的 地位 3 SVM 支持 向量 机 支持 向量 
机 算法 是 诞生 于 统计 学习 界 同时 在 
机器学习 界 大放 光彩 的 经典 算法 支持 向量 机 
算法 从 某种 意义 上 来说 是 逻辑 回归 算法 
的 强化 通过 给予 逻辑 回归 算法 更 严格 的 
优化 条件 支持 向量 机 算法 可以 获得 比 逻辑 
回归 更好 的 分类 界线 但是 如果 没有 某类 函数 
技术 则 支持 向量 机 算法 最多 算是 一种 更好 
的 线性 分类 技术 但是 通过 跟 高斯 核 的 
结合 支持 向量 机 可以 表达 出 非常 复杂 的 
分类 界线 从而 达成 很好 的 的 分类 效果 核 
事实上 就是 一种 特殊 的 函数 最 典型 的 特征 
就是 可以 将 低维 的 空间 映 射到 高维 的 
空间 例如 下图 所示 我们 如何 在 二维 平面 划分 
出 一个 圆形 的 分类 界线 在 二维 平面 可能会 
很 困难 但是 通过 核 可以 将 二 维空间 映 
射到 三维空间 然后 使用 一个 线性 平面 就 可以 达成 
类似 效果 也 就是说 二维 平面 划分 出 的 非线性 
分类 界线 可以 等价 于 三维 平面 的 线性 分类 
界线 于是 我们 可以 通过 在 三维空间 中 进行 简单 
的 线性 划分 就 可以 达到 在 二维 平面 中的 
非线性 划分 效果 支持 向量 机 是 一种 数学 成分 
很浓 的 机器学习 算法 相对 的 神经网络 则有 生物科学 成分 
在 算法 的 核心 步骤 中 有 一步 证明 即将 
数据 从 低维 映 射到 高维 不会 带来 最后 计算 
复杂性 的 提升 于是 通过 支持 向量 机 算法 既 
可以 保持 计算 效率 又 可以 获得 非常 好 的 
分类 效果 因此 支持 向量 机 在 90 年代 后期 
一直 占据 着 机器学习 中最 核心 的 地位 基本 取代 
了 神经 网络 算法 直到现在 神经网络 借着 深度 学习 重新 
兴起 两者 之间 才 又 发生 了 微妙 的 平衡 
转变 4 聚 类 算法 前面 的 算法 中 的 
一个 显著 特征 就是 我 的 训练 数据 中 包含 
了 标签 训 练出 的 模型 可以 对 其他 未知数 
据预测 标签 在 下面 的 算法 中 训练 数据 都是 
不含 标签 的 而 算法 的 目的 则 是 通过 
训练 推测 出 这些 数据 的 标签 这类 算法 有一个 
统称 即 无 监督 算法 前面/f 有/v 标签/n 的/uj 数据/n 
的/uj 算法/n 则是/i 有/v 监督/vn 算法/n 无 监督 算法 中最 
典型 的 代表 就是 聚 类 算法 让 我们 还是 
拿 一个二维 的 数据 来说 某 一个 数据 包含 两个 
特征 我 希望 通过 聚 类 算法 给 他们 中 
不同 的 种类 打上 标签 我 该 怎么做 呢 简单 
来说 聚 类 算法 就是 计算 种群 中的 距离 根据 
距离 的 远近 将 数据 划分 为 多个 族群 聚 
类 算法 中最 典型 的 代表 就是 K Means 算法 
5 降 维 算法 降 维 算法 也 是 一种 
无 监督 学习 算法 其 主要 特征 是 将 数据 
从 高维 降低 到 低维 层次 在 这里 维度 其实 
表示 的 是 数据 的 特征 量 的 大小 例如 
房价 包含 房子 的 长 宽 面积 与 房间 数量 
四个 特征 也 就是 维度 为 4 维 的 数据 
可以 看 出来 长 与 宽 事实上 与 面积 表示 
的 信息 重 叠了 例如 面积 = 长 × 宽 
通过 降 维 算法 我们 就 可以 去除 冗余 信息 
将 特征 减少 为 面积 与 房间 数量 两个 特征 
即从 4 维 的 数据 压缩 到 2 维 于是 
我们 将 数据 从 高维 降低 到 低维 不仅 利于 
表示 同时 在 计算 上 也能 带来 加速 刚才 说 
的 降 维 过程 中 减少 的 维度 属于 肉眼 
可视 的 层次 同时 压缩 也 不会 带来 信息 的 
损失 因为 信息冗余 了 如果 肉眼 不 可视 或者 没有 
冗余 的 特征 降 维 算法 也 能 工作 不过 
这样 会 带来 一些 信息 的 损失 但是 降 维 
算法 可以 从 数学 上 证明 从 高维 压缩 到 
的 低维 中 最大 程度 地 保留 了 数据 的 
信息 因此 使用 降 维 算法 仍然 有 很多 的 
好处 降 维 算法 的 主要 作用 是 压缩 数据 
与 提升 机器学习 其他 算法 的 效率 通过 降 维 
算法 可以 将 具有 几千 个 特征 的 数据 压缩 
至 若干 个 特征 另外 降 维 算法 的 另一个 
好处 是 数据 的 可视化 例如 将 5 维 的 
数据 压缩 至 2 维 然后 可以 用 二维 平面 
来 可视 降 维 算法 的 主要 代表 是 PCA 
算法 即 主 成分 分析 算法 6 推荐算法 推荐算法 是 
目前 业界 非常 火 的 一种 算法 在 电商 界 
如 亚马逊 天猫 京东 等 得到 了 广泛 的 运用 
推荐算法 的 主要 特征 就是 可以 自动 向 用户 推荐 
他们 最 感兴趣 的 东西 从而 增加 购买率 提升 效益 
推荐算法 有 两个 主要 的 类别 一类 是 基于 物品 
内容 的 推荐 是 将与 用户 购买 的 内容 近似 
的 物品 推荐 给 用户 这样 的 前提 是 每个 
物品 都 得有 若干 个 标签 因此 才 可以 找出 
与 用户 购买 物品 类似 的 物品 这样 推荐 的 
好处 是 关联 程度 较大 但是 由于 每个 物品 都 
需要 贴标签 因此 工作量 较大 另一类 是 基于 用户 相似 
度 的 推荐 则是 将与 目标 用户 兴趣 相同 的 
其他 用户 购买 的 东西 推荐 给 目标 用户 例如 
小 A 历史 上 买了 物品 B 和C/nr 经过 算 
法分析 发现 另一个 与 小 A 近似 的 用户 小 
D 购买 了 物品 E 于是 将 物品 E 推荐给 
小 A 两类/m 推荐/v 都有/nr 各自/r 的/uj 优缺点/n 在 一般 
的 电商 应用 中 一般 是 两类 混合 使用 推荐算法 
中最 有名 的 算法 就是 协同 过滤 算法 7 其他 
除了 以上 算法 之外 机器学习 界 还有 其他 的 如 
高斯 判别 朴素 贝叶斯 决策树 等等 算法 但是 上面 列 
的 六个 算法 是 使用 最多 影响 最广 种类 最全 
的 典型 机器学习 界 的 一个 特色 就是 算法 众多 
发展 百花齐放 下面 做 一个 总结 按照 训练 的 数据 
有无/nr 标签 可以 将 上面 算法 分为 监督 学习 算法 
和无/nr 监督 学习 算法 但 推荐算法 较为 特殊 既 不属于 
监督 学习 也 不属于 非 监督 学习 是 单独 的 
一类 监督 学习 算法 线性 回归 逻辑 回归 神经网络 SVM 
无 监督 学习 算法 聚 类 算法 降 维 算法 
特殊 算法 推荐算法 除了 这些 算法 以外 有 一些 算法 
的 名字 在 机器学习 领域 中 也 经常 出现 但 
他们 本身 并不 算是 一个 机器学习 算法 而是 为了 解决 
某 个子 问题 而 诞生 的 你 可以 理解 他们 
为 以上 算法 的 子 算法 用于 大 幅度 提高 
训练 过程 其中 的 代表 有 梯度 下 降法 主要 
运用 在线型 回归 逻辑 回归 神经网络 推荐算法 中 牛顿 法 
主要 运用 在线型 回 归中 BP 算法 主要 运用 在 
神经 网络 中 SMO 算法 主要 运用 在 SVM 中 
5 . 机器 学习 的 应用 大 数据 说完 机器 
学习 的 方法 下面 要 谈一谈 机器 学习 的 应用 
了 无疑 在 2010年 以前 机器 学习 的 应用 在 
某些 特定 领域 发挥 了 巨大 的 作用 如 车牌 
识别 网络攻击 防范 手写 字符识别 等等 但是 从 2010年 以后 
随着 大 数据 概念 的 兴起 机器学习 大量 的 应用 
都与 大 数据 高度 耦合 几乎 可以 认为 大 数据 
是 机器学习 应用 的 最佳 场景 譬如 但凡 你 能 
找到 的 介绍 大 数据 魔力 的 文章 都会 说 
大 数据 如何 准确 准确 预测 到 了 某些 事 
例如 经典 的 Google 利用 大 数据 预测 了 H1N1 
在 美国 某 小镇 的 爆发 Google 成功 预测 H1N1 
百度 预测 2014年 世界杯 从 淘汰赛 到 决赛 全部 预测 
正确 这些 实在 太 神奇 了 那么 究竟 是 什么 
原因 导致 大 数据 具有 这些 魔力 的 呢 简单 
来说 就是 机器学习 技术 正是 基于 机器学习 技术 的 应用 
数据 才能 发挥 其 魔力 大 数据 的 核心 是 
利用 数据 的 价值 机器学习 是 利用 数据 价值 的 
关键 技术 对于 大 数据 而言 机器学习 是 不可或缺 的 
相反 对于 机器学习 而言 越多 的 数据 会 越 可能 
提升 模型 的 精确性 同时 复杂 的 机器学习 算法 的 
计算 时间 也 迫切需要 分布式计算 与 内存 计算 这样 的 
关键 技术 因此 机器 学习 的 兴盛 也 离不开 大 
数据 的 帮助 大 数据 与 机器学习 两者 是 互相 
促进 相依 相存的/nr 关系 机器学习 与 大 数据 紧密联系 但是 
必须 清醒 的 认识 到 大 数据 并不 等同 于 
机器学习 同理 机器学习 也不 等同于 大 数据 大 数据 中 
包含 有 分布式计算 内存 数据库 多维分析 等 等 多种 技术 
单从 分析 方法 来看 大 数据 也 包含 以下 四 
种 分析 方法 1 . 大 数据 小 分析 即 
数据仓库 领域 的 OLAP 分析 思路 也 就是 多维分析 思想 
2 . 大 数据 大 分析 这个 代表 的 就是 
数据挖掘 与 机器学习 分析法 3 . 流式 分析 这个 主要 
指 的 是 事件驱动 架构 4 . 查询 分析 经典 
代表 是 NoSQL 数据库 也 就是说 机器学习 仅仅 是 大 
数据 分析 中 的 一种 而已 尽管 机器 学习 的 
一些 结果 具有 很大 的 魔力 在 某种 场合 下 
是 大 数据 价值 最好 的 说明 但这 并不 代表 
机器学习 是 大 数据 下 的 唯一 的 分析 方法 
机器学习 与 大 数据 的 结合 产生 了 巨大 的 
价值 基于 机器学习 技术 的 发展 数据 能够 预测 对 
人类 而言 积累 的 经验 越 丰富 阅历 也 广泛 
对 未来 的 判断 越 准确 例如 常说 的 经验丰富 
的 人 比 初出茅庐 的 小伙子 更有 工作上 的 优势 
就在于 经验 丰富 的 人 获得 的 规律 比 他人 
更 准确 而在 机器学习 领域 根据 著名 的 一个 实验 
有效 的 证实 了 机器学习 界 一个 理论 即 机器学习 
模型 的 数据 越多 机器 学习 的 预测 的 效率 
就 越好 见 下图 通过 这张 图 可以 看出 各种 
不同 算法 在 输入 的 数据 量 达到 一定 级数 
后 都有 相近 的 高 准确度 于是 诞生 了 机器学习 
界 的 名言 成功 的 机器学习 应用 不是 拥有 最好 
的 算法 而是 拥有 最多 的 数据 在 大 数据 
的 时代 有 好多 优势 促使 机器学习 能够 应用 更 
广泛 例如 随着 物联网 和 移动 设备 的 发展 我们 
拥有 的 数据 越来越 多 种类 也 包括 图片 文本 
视频 等 非 结构化 数据 这 使得 机器学习 模型 可以 
获得 越来越 多 的 数据 同时 大 数据 技术 中的 
分布式计算 Map Reduce 使得 机器 学习 的 速度 越来越 快 
可以 更 方便 的 使用 种种 优势 使得 在 大 
数据 时代 机器 学习 的 优势 可以 得到 最佳 的 
发挥 6 . 机器 学习 的 子类 深度 学习 近来 
机器 学习 的 发展 产生 了 一个 新 的 方向 
即 深度 学习 虽然 深度 学习 这 四字 听起来 颇为 
高大 上 但 其 理念 却 非常 简单 就是 传统 
的 神经 网络 发展 到了 多 隐藏 层 的 情况 
在上文 介绍 过 自从 90 年代 以后 神经 网络 已经 
消 寂 了 一段 时间 但是 BP 算法 的 发明人 
Geoffrey Hinton 一直 没有 放弃 对 神经 网络 的 研究 
由于 神经 网络 在 隐藏 层 扩大 到 两个 以上 
其 训练 速度 就 会 非常 慢 因此 实用性 一直 
低于 支持 向量 机 2006年 Geoffrey Hinton 在 科学 杂志 
Science 上 发表 了 一篇 文章 论证 了 两个 观点 
1 . 多 隐 层 的 神经 网络 具有 优异 
的 特征 学习 能力 学习 得到 的 特征 对 数据 
有更/nr 本质 的 刻画 从而 有利于 可视化 或 分类 2 
. 深度 神经 网络 在 训练 上 的 难度 可以 
通过 逐层 初始化 来 有效 克服 Geoffrey Hinton 与 他 
的 学生 在 Science 上 发表 文章 通过 这样 的 
发现 不仅 解决 了 神经 网络 在 计算 上 的 
难度 同时 也 说明 了 深层 神经 网络 在 学习 
上 的 优异性 从此 神经网络 重新 成为 了 机器学习 界 
中的 主流 强大 学习 技术 同时 具有 多个 隐藏 层 
的 神经 网络 被 称为 深度 神经网络 基于 深度 神经 
网络 的 学习 研究 称之为 深度 学习 由于 深度 学习 
的 重要 性质 在 各 方面 都 取得 极大 的 
关注 按照 时间轴 排序 有 以下 四个 标志性 事件 值得 
一 说 2012年 6月 纽约 时报 披露 了 Google Brain 
项目 这个 项目 是 由 Andrew Ng 和 Map Reduce 
发明人 Jeff Dean 共同 主导 用 16000个 CPU Core 的 
并行 计算 平台 训练 一种 称为 深层 神经网络 的 机器学习 
模型 在 语音 识别 和 图像 识别 等 领域 获得 
了 巨大 的 成功 Andrew Ng 就是 文章 开始 所 
介绍 的 机器 学习 的 大牛 中左 者 2012年 11月 
微软 在 中国 天津 的 一次 活动 上 公开 演示 
了 一个 全 自动 的 同声 传译 系统 讲演者 用 
英文 演讲 后台 的 计算机 一气呵成 自动 完成 语音识别 英中 
机器翻译 以及 中文 语音合成 效果 非常 流畅 其中 支撑 的 
关键 技术 是 深度 学习 2013年 1月 在 百度 的 
年会 上 创始人 兼 CEO 李彦宏 高调 宣布 要 成立 
百度 研究院 其中 第一 个 重点 方向 就是 深度 学习 
并 为此 而 成立 深度 学习 研究院 IDL 2013年 4月 
麻省理工学院 技术 评论 杂志 将 深度 学习 列为 2013年 十大 
突破性 技术 Breakthrough Technology 之首 文章 开头 所列 的 三位 
机器学习 的 大牛 不仅 都是 机器学习 界 的 专家 更是 
深度 学习 研究 领域 的 先驱 因此 使 他们 担任 
各个 大型 互联网 公司 技术 掌舵者 的 原因 不仅 在于 
他们 的 技术 实力 更 在于 他们 研究 的 领域 
是 前景 无限 的 深度 学习 技术 目前 业界 许多 
的 图像 识别 技术 与 语音 识别 技术 的 进步 
都 源于 深度 学习 的 发展 除了 本文 开头 所提 
的 Cortana 等 语音 助手 还包括 一些 图像识别 应用 其中 
典型 的 代表 就是 下图 的 百度识图 功能 深度 学习 
属于 机器 学习 的 子类 基于 深度 学习 的 发展 
极大 的 促进 了 机器 学习 的 地位 提高 更进一步 
地 推动 了 业界 对 机器学习 父 类 人工智能 梦想 
的 再次 重视 7 . 机器 学习 的 父 类 
人工智能 人工智能 是 机器 学习 的 父 类 深度 学习 
则是 机器 学习 的 子类 如果把 三者 的 关系 用图 
来 表明 的话 则是 下图 毫无疑问 人工智能 AI 是 人类 
所 能 想象 的 科技界 最 突破性 的 发明 了 
某种 意义 上 来说 人工智能 就像 游戏 最终幻想 的 名字 
一样 是 人类 对于 科技界 的 最终 梦想 从 50 
年代 提出 人工智能 的 理念 以后 科技界 产业界 不断 在 
探索 研究 这段时间 各种 小说 电影 都在 以 各种 方式 
展现 对于 人工智能 的 想象 人类 可以 发明 类似于 人类 
的 机器 这是 多么 伟大 的 一种 理念 但 事实上 
自从 50 年代 以后 人工智能 的 发展 就 磕磕碰碰 未有 
见到 足够 震撼 的 科学 技术 的 进步 总结 起来 
人工智能 的 发展 经历 了 如下 若干 阶段 从 早期 
的 逻辑推理 到 中期 的 专家 系统 这些 科研 进步 
确实 使 我们 离 机器 的 智能 有点 接近 了 
但 还有 一大 段 距离 直到 机器学习 诞生 以后 人工智能 
界 感觉 终于 找 对了 方向 基于 机器 学习 的 
图像 识别 和 语音 识别 在 某些 垂直 领域 达到 
了 跟人 相媲美 的 程度 机器学习 使 人类 第一次 如此 
接近 人工智能 的 梦想 事实上 如果 我们 把 人工智能 相关 
的 技术 以及 其他 业界 的 技术 做 一个 类比 
就 可以 发现 机器学习 在 人工智能 中 的 重要 地位 
不是 没有 理由 的 人类 区别于 其他 物体 植物 动物 
的 最主要 区别 作者 认为 是 智慧 而 智慧 的 
最佳 体现 是 什么 是 计算 能力 么 应该 不是 
心算 速度快 的 人 我们 一般 称之为 天才 是 反应 
能力 么 也 不是 反应 快 的 人 我们 称之为 
灵敏 是 记忆 能力 么 也 不是 记忆 好 的 
人 我们 一般 称之为 过目不忘 是 推理 能力 么 这样 
的 人 我 也许 会 称 他 智力 很高 类似 
福尔摩斯 但 不会 称 他 拥有 智慧 是 知识 能力 
么 这样 的 人 我们 称之为 博闻 广 也 不会 
称 他 拥有 智慧 想想 看 我们 一般 形容 谁有 
大智慧 圣人 诸如 庄子 老子 等 智慧 是 对 生活 
的 感悟 是 对 人生 的 积淀 与 思考 这与 
我们 机器 学习 的 思想 何其相似 通过 经验 获取 规律 
指导 人生 与 未来 没有 经验 就 没有 智慧 那么 
从 计算机 来看 以上/f 的/uj 种种/q 能力/n 都有/nr 种种/q 技术/n 
去/v 应对/v 例如 计算能力 我们 有 分布式计算 反应 能力 我们 
有 事件驱动 架构 检索 能力 我们 有 搜索引擎 知识 存储 
能力 我们 有 数据仓库 逻辑推理 能力 我们 有 专家 系统 
但是 唯有 对应 智慧 中 最 显著 特征 的 归纳 
与 感悟 能力 只有 机器学习 与之 对应 这 也是 机器学习 
能力 最能 表征 智慧 的 根本 原因 让 我们 再 
看一下 机器人 的 制造 在 我们 具有 了 强大 的 
计算 海量 的 存储 快速 的 检索 迅速 的 反应 
优秀 的 逻辑推理 后 我们 如果 再 配合 上 一个 
强大 的 智慧 大脑 一个 真正 意义 上 的 人工智能 
也许 就 会 诞生 这也 是 为什么 说 在 机器学习 
快速 发展 的 现在 人工智能 可能 不再 是 梦想 的 
原因 人工智能 的 发展 可能 不仅 取决于 机器学习 更 取决于 
前面 所 介绍 的 深度 学习 深度 学习 技术 由于 
深度 模拟 了 人类 大脑 的 构成 在 视觉 识别 
与 语音识别 上 显著性 的 突破 了 原有 机器学习 技术 
的 界限 因此 极 有可能 是 真正 实现 人工智能 梦想 
的 关键 技术 无论 是 谷歌 大脑 还是 百度 大脑 
都是/nr 通过 海量 层次 的 深度 学习 网络 所 构成 
的 也许 借助于 深度 学习 技术 在 不远 的 将来 
一个 具有 人类 智能 的 计算机 真的 有 可能 实现 
最后 再 说 一下 题外话 由于 人工智能 借助于 深度 学习 
技术 的 快速 发展 已经 在 某些 地方 引起 了 
传统 技术 界 达人 的 担忧 真实世界 的 钢铁侠 特斯拉 
CEO 马斯克 就是 其中 之一 最近 马斯克 在 参加 MIT 
讨论会 时 就 表达 了 对于 人工智能 的 担忧 人工智能 
的 研究 就 类似于 召唤 恶魔 我们 必须 在 某些 
地方 加强 注意 尽管 马斯克 的 担心 有些 危言耸听 但是 
马斯克 的 推理 不无道理 如果 人工智能 想要 消除 垃圾 邮件 
的话 可能 它 最后 的 决定 就是 消灭 人类 马斯克 
认为 预防 此类 现象 的 方法 是 引入 政府 的 
监管 在 这里 作者 的 观点 与 马斯克 类似 在 
人工智能 诞生 之初 就 给 其 加上 若干 规则 限制 
可能 有效 也 就是 不 应该 使用 单纯 的 机器学习 
而 应该 是 机器 学习 与 规则 引擎 等 系统 
的 综合 能够 较好 的 解决 这 类 问题 因为 
如果 学习 没有 限制 极 有可能 进入 某 个 误区 
必须 要 加上 某些 引导 正如 人类 社会 中 法律 
就是 一个 最好 的 规则 杀人者死 就是 对于 人类 在 
探索 提高 生产力 时 不可逾越 的 界限 在 这里 必须 
提 一下 这里 的 规则 与 机器学习 引出 的 规律 
的 不同 规律 不是 一个 严格 意义 的 准则 其 
代表 的 更多 是 概率 上 的 指导 而 规则 
则是 神圣 不可 侵犯 不可 修改 的 规律 可以 调整 
但 规则 是 不能 改变 的 有效 的 结合 规律 
与 规则 的 特点 可以 引导 出 一个 合理 的 
可控 的 学习 型 人工智能 8 . 机器 学习 的 
思考 计算机 的 潜意识 最后 作者 想 谈一谈 关于 机器 
学习 的 一些 思考 主要 是 作者 在 日常 生活 
总结 出来 的 一些 感悟 回想 一下 我 在 节 
1里 所说 的 故事 我 把 小 Y 过往 跟 
我 相约 的 经历 做 了 一个 罗列 但是 这种 
罗列 以往 所有 经历 的 方法 只有 少数人 会 这么 
做 大 部分 的 人 采用 的 是 更 直接 
的 方法 即 利用 直觉 那么 直觉 是 什么 其实 
直觉 也是 你 在 潜意识 状态下 思考 经验 后 得出 
的 规律 就像 你 通过 机器学习 算法 得到 了 一个 
模型 那么 你 下次 只要 直接 使用 就行了 那么 这个 
规律 你 是 什么 时候 思考 的 可能 是 在 
你 无意识 的 情况 下 例如 睡觉 走路 等 情况 
这种 时候 大脑 其实 也 在 默默地 做 一些 你 
察觉 不到 的 工作 这种 直觉 与 潜意识 我 把 
它 与 另一 种 人类 思考 经验 的 方式 做了 
区分 如果 一个人 勤于思考 例如 他 会 每天 做 一个 
小结 譬如 吾日三省吾身 或者 他 经常 与 同伴 讨论 最近 
工作 的 得失 那么 他 这种 训练 模型 的 方式 
是 直接 的 明 意识 的 思考 与 归纳 这样 
的 效果 很好 记忆 性强 并且 更 能 得出 有效 
反应 现实 的 规律 但是 大 部分 的 人 可能 
很少 做 这样 的 总结 那么 他们 得出 生活 中 
规律 的 方法 使用 的 就是 潜意识 法 举 一个 
作者 本人 关于 潜意识 的 例子 作者 本 人 以前 
没 开过 车 最近 一段 时间 买了 车后 天天 开车 
上班 我 每天 都走/nr 固定 的 路线 有趣 的 是 
在 一 开始 的 几天 我 非常 紧张 的 注意 
着 前方 的 路况 而 现在 我 已经 在 无意识 
中 就把 车 开到 了 目标 这个 过程 中 我 
的 眼睛 是 注视 着 前方 的 我 的 大脑 
是 没有 思考 但是 我 手握着 的 方向盘 会 自动 
的 调整 方向 也 就是说 随着 我 开车 次数 的 
增多 我 已经 把 我 开车 的 动作 交给 了 
潜意识 这 是 非常 有趣 的 一件 事 在 这段 
过程 中 我 的 大脑 将 前方 路况 的 图像 
记录 了 下来 同时 大脑 也 记忆 了 我 转动 
方向盘 的 动作 经过 大脑 自己 的 潜意识 思考 最后 
生成 的 潜意识 可以 直接 根据 前方 的 图像 调整 
我 手 的 动作 假设 我们 将 前方 的 录像 
交给 计算机 然后 让 计算机 记录 与 图像 对应 的 
驾驶员 的 动作 经过 一段 时间 的 学习 计算机 生成 
的 机器学习 模型 就 可以 进行 自动 驾驶 了 这 
很 神奇 不是 么 其实 包括 Google 特斯拉 在内 的 
自动 驾驶 汽车 技术 的 原理 就是 这样 除了 自动 
驾驶 汽车 以外 潜意识 的 思想 还 可以 扩展 到人 
的 交际 譬如 说服 别人 一个 最佳 的 方法 就是 
给 他 展示 一些 信息 然后 让 他 自己 去 
归纳 得出 我们 想要 的 结论 这就 好比 在 阐述 
一个 观点 时 用 一个 事实 或者 一个 故事 比 
大段 的 道理 要好 很多 古往今来 但凡 优秀 的 说客 
无不 采用 的 是 这种 方法 春秋 战国 时期 各国 
合纵连横 经常 有 各种 说客 去 跟 一国之君 交流 直接 
告诉 君主 该 做什么 无异于 自寻死路 但是 跟 君主 讲故事 
通过 这些 故事 让 君主 恍然大悟 就是 一种 正确 的 
过程 这 里面 有 许多 杰出 的 代表 如 墨子 
苏秦 等等 基本上 所有 的 交流 过程 使用 故事 说明 
的 效果 都要 远胜于 阐述 道义 之类 的 效果 好 
很多 为什么 用 故事 的 方法 比 道理 或者 其他 
的 方法 好 很多 这 是因为 在 人 成长 的 
过程 经过 自己 的 思考 已经 形成 了 很多 规律 
与 潜意识 如果 你 告诉 的 规律 与 对方 的 
不相符 很 有可能 出于 保护 他们 会 本能 的 拒绝 
你 的 新 规律 但是 如果 你 跟 他 讲 
一个 故事 传递 一些 信息 输送 一些 数据 给 他 
他 会 思考 并 自我 改变 他 的 思考 过程 
实际上 就是 机器 学习 的 过程 他 把 新的 数据 
纳入 到 他 的 旧有 的 记忆 与 数据 中 
经过 重新 训练 如果 你 给出 的 数据 的 信息 
量 非常 大 大 到 调整了 他 的 模型 那么 
他 就会 按照 你 希望 的 规律 去 做事 有的 
时候 他 会 本能 的 拒绝 执行 这个 思考 过程 
但是 数据 一旦 输入 无论 他 希望 与否 他 的 
大脑 都会 在 潜意识 状态下 思考 并且 可能 改变 他 
的 看法 如果 计算机 也 拥有 潜意识 正如 本 博客 
的 名称 一样 那么 会 怎么样 譬如 让 计算机 在 
工作 的 过程 中 逐渐 产生 了 自身 的 潜意识 
于是 甚至 可以 在 你 不 需要 告诉 它 做 
什么 时它/nr 就会 完成 那件事 这是 个 非常 有意思 的 
设想 这里 留给 各位 读者 去 发散 思考 吧 9 
. 总结 本文 首先 介绍 了 互联网 界 与 机器学习 
大牛 结合 的 趋势 以及 使用 机器 学习 的 相关 
应用 接着 以 一个 等人 故事 展开 对 机器 学习 
的 介绍 介绍 中 首先 是 机器 学习 的 概念 
与 定义 然后 是 机器 学习 的 相关 学科 机器学习 
中 包含 的 各类 学习 算法 接着 介绍 机器学习 与 
大 数据 的 关系 机器 学习 的 新 子类 深度 
学习 最后 探讨 了 一下 机器学习 与 人工智能 发展 的 
联系 以及 机器学习 与 潜意识 的 关联 经过 本文 的 
介绍 相信 大家 对 机器 学习 技术 有 一定 的 
了解 例如 机器学习 是 什么 它 的 内核 思想 是 
什么 即 统计 和 归纳 通过 了 解 机器学习 与 
人类 思考 的 近似 联系 可以 知晓 机器学习 为什么 具 
有智慧 能力 的 原因 等等 其次 本文 漫谈 了 机器 
学习 与 外延 学科 的 关系 机器学习 与 大 数据 
相互促进 相得益彰 的 联系 机器学习 界 最新 的 深度 学习 
的 迅猛 发展 以及 对于 人类 基于 机器学习 开发 智能 
机器人 的 一种 展望 与 思考 最后 作者 简单 谈了 
一点 关于 让 计算机 拥有 潜意识 的 设想 机器学习 是 
目前 业界 最为 Amazing 与 火热 的 一项 技术 从 
网上 的 每一次 淘宝 的 购买 东西 到 自动 驾驶 
汽车 技术 以及 网络攻击 抵御 系统 等等 都有 机器 学习 
的 因子 在内 同时 机器学习 也是 最 有可能 使 人类 
完成 AI dream 的 一项 技术 各种 人工智能 目前 的 
应用 如 微软 小冰 聊天 机器人 到 计算机 视觉 技术 
的 进步 都有 机器学习 努力 的 成分 作为 一名 当代 
的 计算机 领域 的 开发 或 管理 人员 以及 身处 
这个 世界 使用者 IT 技术 带来 便利 的 人们 最好 
都 应该 了解 一些 机器 学习 的 相关 知识 与 
概念 因为 这 可以 帮 你 更好 的 理解 为 
你 带来 莫大 便利 技术 的 背后 原理 以及 让 
你 更好 的 理解 当代 科技 的 进程 10 . 
后记 这篇 文档 花了 作者 两个月 的 时间 终于 在 
2014年 的 最后 一天 的 前一天 基本完成 通过 这 篇 
文章 作者 希望 对 机器 学习 在 国内 的 普及 
做 一点 贡献 同时 也 是 作者 本人 自己 对于 
所学 机器学习 知识 的 一个 融汇贯通 整体 归纳 的 提高 
过程 作者 把 这么 多 的 知识 经过 自己 的 
大脑 思考 训 练出 了 一个 模型 形成 了 这篇 
文档 可以 说 这也 是 一种 机器 学习 的 过程 
吧 笑 作者 所在 的 行业 会 接触 到 大量 
的 数据 因此 对于 数据 的 处理 和 分析 是 
平常 非常 重要 的 工作 机器学习 课程 的 思想 和 
理念 对于 作者 日常 的 工作 指引 作用 极大 几乎 
导致 了 作者 对于 数据 价值 的 重新 认识 想想 
半年前 作者 还对 机器学习 似懂非懂 如今 也 可以 算是 一个 
机器 学习 的 Expert 了 笑 但 作者 始终认为 机器 
学习 的 真正 应用 不是 通过 概念 或者 思想 的 
方式 而是 通过 实践 只有 当 把 机器 学习 技术 
真正 应用 时 才可 算是 对 机器 学习 的 理解 
进入 了 一个 层次 正所谓 再 阳春白雪 的 技术 也 
必须 落到 下里巴人 的 场景 下 运用 目前 有 一种 
风气 国内外 研究 机器 学习 的 某些 学者 有 一种 
高贵 的 逼 格 认为 自己 的 研究 是 普通人 
无法 理解 的 但是 这样 的 理念 是 根本 错误 
的 没有 在 真正 实际 的 地方 发挥作用 凭 什么 
证明 你 的 研究 有所 价值 呢 作者 认为 必须 
将 高大 上 的 技术 用在 改变 普通人 的 生活 
上 才能 发挥 其 根本 的 价值 一些 简单 的 
场景 恰恰 是 实践 机器学习 技术 的 最好 地方 最后 
作者 很 感谢 能够 阅读 到 这里 的 读者 如果 
看完 觉得 好 的话 还请 轻轻 点 一下 赞 你们 
的 鼓励 就是 作者 继续 行文 的 动力 对 EasyPR 
做 下 说明 EasyPR 一个 开源 的 中文 车牌 识别系统 
代码 托管 在 github 其次 在 前面 的 博客 文章 
中 包含 EasyPR 至今 的 开发 文档 与 介绍 在 
后续 的 文章 中 作者 会 介绍 EasyPR 中 基于 
机器学习 技术 SVM 的 应用 即 车牌 判别 模块 的 
核心 内容 欢迎 继续 阅读 版权 说明 本 文中 的 
所有 文字 图片 代码 的 版权 都是 属于 作者 和 
博客园 共同 所有 欢迎 转载 但是 务必 注明 作者 与 
出处 任何 未经 允许 的 剽窃 以及 爬虫 抓取 都 
属于 侵权 作者 和 博客园 保留 所有 权利 转载自 http 
/ / www . cnblogs . com / subconscious / 
p / 4107357 . html 参考文献 1 . Andrew Ng 
Courera Machine Learning2 . LeNet Homepage3 . pluskid svm 自然语言 
处理 简称 NLP 是 研究 计算机 处理 人类 语言 的 
一门 技术 包括 1 . 句法 语义分析 对于 给定 的 
句子 进行 分词 词性 标记 命名 实体 识别 和 链接 
句法分析 语义 角色 识别 和 多义词 消 歧 2 . 
信息 抽取 从 给定 文本 中 抽取 重要 的 信息 
比如 时间 地点 人物 事件 原因 结果 数字 日期 货币 
专有名词 等等 通俗 说来 就是 要 了解 谁在 什么 时候 
什么原因 对谁 做 了 什么 事 有 什么 结果 涉及 
到 实体 识别 时间 抽取 因果关系 抽取 等 关键 技术 
3 . 文本 挖掘 或者 文本 数据挖掘 包括 文本 聚 
类 分类 信息 抽取 摘要 情感 分析 以及 对 挖掘 
的 信息 和 知识 的 可视化 交互式 的 表达 界面 
目前 主流 的 技术 都是/nr 基于 统计 机器 学习 的 
4 . 机器翻译 把 输入 的 源语言 文本 通过 自动 
翻译 获得 另外 一种 语言 的 文本 根据 输入 媒介 
不同 可以 细分 为 文本 翻译 语音 翻译 手语 翻译 
图形 翻译 等 机器翻译 从 最早 的 基于 规则 的 
方法 到 二十年前 的 基于 统计 的 方法 再到 今天 
的 基于 神经网络 编码 解码 的 方法 逐渐 形成 了 
一套 比较 严谨 的 方法 体系 5 . 信息检索 对 
大 规模 的 文档 进行 索引 可 简单 对 文档 
中 的 词汇 赋 之以 不同 的 权重 来 建立 
索引 也 可利用 1 2 3 的 技术 来 建立 
更加 深层 的 索引 在 查询 的 时候 对 输入 
的 查询 表达式 比如 一个 检索 词 或者 一个 句子 
进行 分析 然后 在 索引 里面 查找 匹配 的 候选 
文档 再 根据 一个 排序 机制 把 候选 文档 排序 
最后 输出 排序 得分 最高 的 文档 6 . 问答 
系统 对 一个 自然 语言 表达 的 问题 由 问答 
系统 给 出 一个 精准 的 答案 需要 对 自然 
语言 查询 语句 进行 某种 程度 的 语义分析 包括 实体 
链接 关系 识别 形成 逻辑 表达式 然后 到 知识库 中 
查找 可能 的 候选 答案 并 通过 一个 排序 机制 
找出 最佳 的 答案 7 . 对话 系统 系统 通过 
一 系列 的 对话 跟 用户 进行 聊天 回答 完成 
某一 项 任务 涉及 到 用户 意图 理解 通用 聊天 
引擎 问答 引擎 对话 管理 等 技术 此外 为了 体现 
上下文 相关 要 具备 多轮 对话 能力 同时 为了 体现 
个性化 要 开发 用户 画像 以及 基于 用户 画像 的 
个性化 回复 随着 深度 学习 在 图像 识别 语音识别 领域 
的 大放异彩 人们 对 深度 学习 在 NLP 的 价值 
也 寄予厚望 再 加上 AlphaGo 的 成功 人工智能 的 研究 
和 应用 变得 炙手可热 自然语言 处理 作为 人工智能 领域 的 
认知 智能 成为 目前 大家 关注 的 焦点 很多 研究生 
都在/nr 进入 自然语言 领域 寄望 未来 在 人工智能 方向 大展身手 
但是 大家 常常 遇到 一些 问题 俗话说 万事开头难 如果 第一 
件 事情 成功 了 学生 就 能 建立 信心 找到 
窍门 今后 越做/nr 越好 否则 也 可能 就 灰心丧气 甚至 
离开 这个 领域 这里 针对 给出 我 个人 的 建议 
希望 我 的 这些 粗浅 观点 能够 引起 大家 更 
深层次 的 讨论 建议 1 如何 在 NLP 领域 快速 
学会 第一个 技能 我 的 建议 是 找到 一个 开 
源 项目 比如 机器翻译 或者 深度 学习 的 项目 理解 
开源 项目 的 任务 编译 通过 该 项目 发布 的 
示范 程序 得到 与 项目 示范 程序 一致 的 结果 
然后 再 深入 理解 开源 项目 示范 程序 的 算法 
自己 编程 实现 一下 这个 示范 程序 的 算法 再 
按照 项目 提供 的 标准 测试 集 测试 自己 实现 
的 程序 如果 输出 的 结果 与 项目 中 出现 
的 结果 不 一致 就要 仔细 查验 自己 的 程序 
反复 修改 直到 结果 与 示范 程序 基本一致 如果 还是 
不行 就 大胆 给 项目 的 作者 写信 请教 在此 
基础 上 再 看看 自己 能否 进一步 完善 算法 或者 
实现 取 得比 示范 程序 更好 的 结果 建议 2 
如何 选择 第一 个 好 题目 工程型 研究生 选题 很多 
都是/nr 老师 给定 的 需要 采取 比较 实用 的 方法 
扎扎实实 地 动手 实现 可能 不 需要 多少 理论 创新 
但是 需要 较强 的 实现 能力 和 综合 创新 能力 
而 学术 型 研究生 需要 取得 一流 的 研究 成果 
因此 选题 需要 有 一定 的 创新 我 这里 给 
出 如下 的 几点 建议 先 找到 自己 喜欢 的 
研究 领域 你 找到 一本 最近 的 ACL 会议 论文集 
从中 找到 一个 你 比较 喜欢 的 领域 在 选题 
的 时候 多 注意 选择 蓝海 的 领域 这 是因为 
蓝海 的 领域 相对 比较 新 容易 出 成果 充分 
调研 这个 领域 目前 的 发展 状况 包括 如下 几 
个 方面 的 调研 方法 方面 是否/v 有/v 一套/m 比较/d 
清晰/a 的/uj 数学/n 体系/n 和/c 机器学习/i 体系/n 数据 方面 有/v 
没有/v 一个/m 大家/n 公认/v 的/uj 标准/n 训练/vn 集/q 和/c 测试/vn 
集/q 研究 团队 是否/v 有/v 著名/a 团队/n 和/c 人士/n 参加/v 
如果 以 上 几个 方面 的 调研 结论 不是 太 
清晰 作为 初学者 可能 不要 轻易 进入 在 确认 进入 
一个 领域 之后 按照 建议 一 所述 需要 找到 本 
领域 的 开源 项目 或者 工具 仔细 研究 一遍 现有 
的 主要 流派 和 方法 先 入门 反复 阅读 本 
领域 最新 发表 的 文章 多 阅读 本 领域 牛人 
发表 的 文章 在 深入 了解 已有 工作 的 基础 
上 探讨 还有 没有 一些 地方 可以 推翻 改进 综合 
迁移 注意 做 实验 的 时候 不要 贪多 每次 实验 
只需要 验证 一个 想法 每次 实验 之后 必须 要 进行 
分析 存在 的 错误 找出 原因 对 成功 的 实验 
进一步 探讨 如何 改进 算法 注意 实验 数据 必须 是 
业界 公认 的 数据 与 已有 的 算法 进行 比较 
体会 能够 得出 比较 一般性 的 结论 如果 有 则 
去 写 一篇 文章 否则 应该 换 一个 新 的 
选题 建议 3 如何写 出 第一 篇 论文 接 上 
一个 问题 如果 想法 不错 且 被 实验 所 证明 
就可 开始 写 第一 篇 论文 了 确定 论文 的 
题目 在 定 题目 的 时候 一般 不要 系统 研究 
与 实践 要 避免 太长 的 题目 因为 不好 体现 
要点 题目 要 具体 有 深度 突出 算法 写 论文 
摘要 要 突出 本文 针对 什么 重要 问题 提出 了 
什么 方法 跟 已有 工作 相比 具 有 什么 优势 
实验 结果 表明 达到 了 什么 水准 解决 了 什么 
问题 写 引言 首先 讲出 本 项 工作 的 背景 
这个 问题 的 定义 它 具有 什么 重要性 然后 介绍 
对 这个 问题 现有 的 方法 是 什么 有 什么 
优点 但是 注意 但是 现有 的 方法 仍然 有 很多 
缺陷 或者 挑战 比如 注意 比如 有 什么 问题 本文 
针对 这个 问题 受 什么 方法 谁 的 工作 之 
启发 提出 了 什么 新的 方法 并 做了 如下 几 
个 方面 的 研究 然后 对 每个 方面 分门别类 加以 
叙述 最后 说明 实验 的 结论 再说 本文 有 几条 
贡献 一般 写 三条 足矣 然后 说说 文章 的 章节 
组织 以及 本文 的 重点 有的 时候 东西 太多 篇幅 
有限 只能 介绍 最 重要 的 部分 不 需要 面面俱到 
相关 工作 对 相关 工作 做 一个 梳理 按照 流派 
划分 对 主要 的 最多 三个 流派 做 一个 简单 
介绍 介绍 其 原理 然后 说明 其 局限性 然后 可 
设立 两 个 章节 介绍 自己 的 工作 第一 个 
章节 是 算法 描述 包括 问题 定义 数学 符号 算法 
描述 文章 的 主要 公式 基本 都在/nr 这里 有时候 要 
给出 简明 的 推导 过程 如果 借鉴 了 别人 的 
理论 和 算法 要给 出 清晰 的 引文 信息 在此 
基础 上 由于 一般 是 基于 机器学习 或者 深度 学习 
的 方法 要 介绍 你 的 模型 训练 方法 和 
解码 方法 第二章 就是 实验 环节 一般 要 给出 实验 
的 目的 要 检验 什么 实验 的 方法 数据 从 
哪里 来 多 大 规模 最好 数据 是 用 公开 
评 测数据 便于 别人 重复 你 的 工作 然后 对 
每个 实验 给出 所需 的 技术 参数 并 报告 实验 
结果 同时 为了 与 已有 工作 比较 需要 引用 已有 
工作 的 结果 必要 的 时候 需要 重现 重要 的 
工作 并 报告 结果 用 实验 数据 说话 说明 你 
比 人家 的 方法 要好 要对 实验 结果 好好 分析 
你 的 工作 与 别人 的 工作 的 不同 及 
各自 利弊 并 说明 其 原因 对于 目前 尚 不太好 
的 地方 要 分析 问题 之 所在 并将 其 列为 
未来 的 工作 结论 对 本文 的 贡献 再一次 总结 
既要 从 理论 方法 上 加以 总结 和 提炼 也要 
说明 在 实验上 的 贡献 和 结论 所做 的 结论 
要让 读者 感到 信服 同时 指出 未来 的 研究 方向 
参考文献 给出 所有 重要 相关 工作 的 论文 记住 漏掉 
了 一篇 重要 的 参考 文献 或者 牛人 的 工作 
基本上 就 没有 被 录取 的 希望 了 写完 第一稿 
然后 就是 再改 三遍 把 文章 交给 同一个 项目组 的 
人士 请 他们 从 算法 新颖 度 创新性/n 和/c 实验/vn 
规模/n 和/c 结论/n 方面/n 以 挑剔 的 眼光 审核 你 
的 文章 自己 针对 薄弱环节 进一步 改进 重点 加强 算法 
深度 和 工作 创新 性 然后 请 不同 项目组 的 
人士 审阅 如果 他们 看 不明白 说明 文章 的 可读性 
不够 你 需要 修改 篇章 结构 进行 文字 润色 增加 
文章 可读性 如 投 ACL 等 国际 会议 最好 再请 
英文 专业 或者 母语 人士 提炼 文字 自然语言 处理 NLP 
是 现代 计算机 科学 和 人工智能 领域 的 一个 重要 
分支 是 一门 融合 了 语言学 数学 计算机 科学 的 
科学 这一 领域 的 研究 将 涉及 自然语言 即 人们 
日常 使用 的 语言 所以 它 与 语言学 的 研究 
有着 密切 的 联系 但又 有 重要 的 区别 自然语言 
处理 并 不是 一般 地 研究 自然语言 而在于 研制 能 
有效 地 实现 自然 语言 通信 的 计算机 系统 特别 
是 其中 的 软件 系统 因而 它 是 计算机 科学 
的 一部分 词 法分析 基于 大 数据 和 用户 行为 
对 自然 语言 进行 中文分词 词性 标注 命名 识体 识别 
定位 基本 语言 元素 消除歧义 支撑 自然 语言 的 准确 
理解 中文分词 将 连续 的 自然 语言 文本 切分/ad 成/n 
具有/v 语义/n 合理性/n 和/c 完整性/n 的/uj 词汇/n 序列/n 词性/n 标注/v 
将 自然 语言 中 的 每个 词 赋予 一个 词性 
如 动词 名词 副词 命名 实体 识别 即 专有名词 识别 
识别 自然语言 文本 中 具有 特殊 意义 的 实体 如 
人名 机构 名 地名 依存 句法分析 利用 句子 中词 与 
词 之间 的 依存 关系 来 表示 词语 的 句法结构 
信息 并用 树状 结构 来 表示 整句 的 结构 依存 
句法分析 主要有 几大 作用 精准 理解 用户 意图 当 用户 
搜索 时 输入 一个 query 通过 依存 句法分析 抽取 语义 
主干 及 相关 语义 成分 实现 对 用户 意图 的 
精准 理解 知识 挖掘 对 大量 的 非 结构化 文本 
进行 依存 句法分析 从中 抽取 实体 概念 语义 关系 等 
信息 构建 领域 知识 语言 结构 匹配 基于 句法结构 信息 
进行 语言 的 匹配 计算 提升 语言 匹配 计算 的 
准确率 词 向量 表示 词 向量 计算 是 通过 训练 
的 方法 将 语言 词表 中的 词 映射 成 一个 
长度 固定 的 向量 词 表中 的 所有 词 向量 
构成 了 一个 向量空间 每 一个 词 都是 这个 向量空间 
中 的 一个 点 利用 这种 方法 实现 文本 的 
可计算 主要 应用在 快速 召回 结果 不同 于 传统 的 
倒排索引 结构 构建 基于 词 向量 的 快速 索引 技术 
直接 从 语义 相关性 的 角度 召回 结果 个性化 推荐 
基于 用户 的 过去 行为 通过 词 向 量计算 学习 
用户 的 兴趣 实现 个性化 推荐 DNN 语言 模型 语言 
模型 是 通过 计算 给定 词 组成 的 句子 的 
概率 从而 判断 所 组成 的 句子 是否 符合 客观 
语言表达 习惯 通常用于 机器翻译 拼写 纠错 语音识别 问答 系统 词性 
标注 句法分析 和 信息检索 等 词义 相似 度 用于 计算 
两个 给定 词语 的 语义 相似 度 基于 自然 语言 
中 的 分布 假设 即 越是 经常 共同 出现 的 
词 之间 的 相似 度 越高 词义 相似 度 是 
自然 语言 处理 中 的 重要 基础 技术 是 专名 
挖掘 query 改写 词性 标注 等 常用 技术 的 基础 
之一 主要 应用 专名 挖掘 通过 词语 间 语义 相关性 
计算 寻找 人名 地名 机构 名 等 词 的 相关 
词 扩大 专有 名词 的 词典 更好 的 辅助 应用 
query 改写 通过 寻找 搜索 query 中 词语 的 相似 
词 进行 合理 的 替换 从而 达到 改写 query 的 
目的 提高 搜索 结果 的 多样性 短 文本 相似 度 
短 文本 相似 度 计算 服务 能够 提供 不同 短 
文本 之间 相似 度 的 计算 输出 的 相似 度 
是 一个 介于 1 到 1 之间 的 实数值 越大 
则 相似 度 越高 这个 相似 度 值 可以 直接 
用于 结果 排序 也 可以 作为 一维 基础 特征 作用于 
更 复杂 的 系统 评论 观点 抽取 自动 分析 评论 
关注点 和 评论 观点 并 输出 评论 观点 标签 及 
评论 观点 极性 包括 美食 酒店 汽车 景点 等 可 
帮助 商家 进行 产品 分析 辅助 用户 进行 消费 决策 
情感 倾向 分析 针对 带有 主观 描述 的 中文 文本 
可 自动 判断 该 文本 的 情感 极性 类别 并给 
出 相应 的 置信度 情感 极性 分为 积极 消极 中性 
情感 倾向 分析 能 帮助 企业 理解 用户 消费 习惯 
分析 热点 话题 和 危机 舆情 监控 为 企业 提供 
有力 的 决策 支持 写 在前面 很早 之前 就 接触 
过 冯志伟 老 先生 的 一些 有关 自然语言 处理 的 
相关 文字 彼时 我 还是 一个 大 三 的 文科 
外语 专业 的 小罗 罗 而 此时 的 我 已经 
冠 上了 为 高级 软件 工程师 的 虚名 当时 在 
直接 阅读 相关 的 文字 时 因为 还 未有 对 
技术 有 深入 的 实践 对 自然 语言 的 处理 
以及 和 计算机 科学 数学 之间 的 关系 并 没有 
足够 清晰 的 了解 在 经历 了 最初 职场 的 
迷惑 和对/nr 工程 技术 的 实践 之后 似乎 很多 曾经 
脑中 模糊 的 概念 通过 了 一个 个 软件 的 
系统 和 一行行 代码 都被 串 到了 一起 今天 又 
找了 一篇 08年 冯志伟 老 先生 在 中国 外语 上 
发表 的 一篇 介绍 自然语言 处理 的 文章 重新 开始 
组织 脑中 散落 在 各个 角落 的 各种 关于 早期 
的 萌芽 根据 冯 老先生 的 阐述 自然语言 处理 其实 
在 二战 时期 计算机技术 出现 之前 就 已经 发生 了 
极其 重要 的 萌芽 和 分支 发展 在 图灵 刚刚 
发明 计算机 的 时候 战争 的 惨烈 刚刚 教了 所有 
人类 一堂课 二十世纪 40 年代 到 50 年代 之间 除了 
当时 给 世界 带来 极大 震撼 的 计算机 技术 在 
美国 还有 两个人 在 进行 着 他们 的 工作 第一项 
就是 乔姆斯基 对 形式语言 的 研究 工作 另 一个 就是 
当时 在 工作 中 发展 出来 的 香农 的 基于 
概率 和 信息论 模型 的 研究 在 这个 时期 其实 
计算机 就 已经 和 自然 语言 产生 了 极大 的 
交互 计算机 在 当时 虽然 更多 的 是 一种 代替 
人类 进行 大量 重复 计算 的 工具 然而 图灵 自己 
发明 思维 机器 的 时候 内心 就 一直 没有 停止 
对 智能 语言 数学 之间 关系 的 思考 而 众所周知 
的 图灵测试 也是 在 语言 媒介 的 基础上 对 计算 
智能 的 一种 评估 而已 也 正是 图灵 的 工作 
对 后世 现代 计算机 科学 产生 了 直接 的 推动 
作用 也 延续 下去 产生 了 神经元 理论 等 各种 
计算 框架 香农/nz 的/uj 信息/n 论/zg 则在/i 概率/n 和/c 统计/v 
的/uj 基础上/i 对/p 语言/n 和/c 计算机/n 语言/n 进行/v 了/ul 相当/d 
的/uj 刻画/n 而 乔姆斯基 在其 基础 上 深入 的 研究 
直接 产生 了 形式 语言 的 模型 框架 在 这里 
计算机 的 计算 过程 计算机 语言 和 自然 语言 终于 
得到 了 统一 被 放到 了 同一 个 水平 进行 
研究 而 其中 产生 的 最重要 的 上下文 无关 语法 
也在 计算机 语言 的 领域 各自 得到 了 应有 的 
发展 和 成长 如果 没有 这些 之后 产生 的 编译器 
估计 也 就 无法 产生 也就 不用 再 提高 级 
编程语言 和 复杂 的 计算机 系统 了 关于 中期 的 
发展 从 二十 世纪 60 年代 到 80 年代 自然语言 
处理 在 计算机 技术 的 飞速 发展 下 也 取得 
了 相当 程度 的 成果 二十世纪 60 年代 法国/ns 格勒诺布/i 
尔/nr 理工大学/nt 的/uj 著名/a 数学家/n 沃古瓦/nr 开始/v 了/ul 自动/vn 翻译/v 
系统/n 的/uj 研制/vn 沃古瓦/nr 也是/i 计算/v 语言学/n 的/uj 创始人/n 和/c 
第一/m 届/d 主席/n 在 这一 过程 当中 不同/a 的/uj 国家/n 
和/c 组织/v 对/p 机器/n 翻译/v 都/d 投入/v 了/ul 大量/n 的/uj 
人力/n 物力 和 财力 人类 历史 上 第一 次 可以 
通过 技术 尝试 去 把 不同 语言 之间 隔阂 的 
通天 之 塔 打通 因此 大 部分 人 对此 应该 
还是 抱着 很大 的 希望 的 但是 在 实际 的 
过程 中 机器翻译 系统 的 研制 遇到 了 各种 问题 
这些 问题 的 复杂度 也 远远 超过 了 原来 大家 
的 预期 当然 在 这 过程 中 因为 希望 解决 
这 一 问题 产生 了 各种各样 的 模型 和 解决 
方案 虽然 最后 的 结果 并 不是 都 尽如人意 但是 
却 为 后来 的 各个 相关 分支 领域 的 发展 
奠定 了 极其 重要 的 基础 统计学 逻辑学 语言学 以及 
后来 丘奇 的 计算 理论 等等 关于 后期 的 繁荣 
二十世纪 90 年代 之后 自然语言 处理 的 发展 进入 了 
相当 繁荣 的 一个 时期 在 这一 阶段 有 一个 
重要 的 东西 诞生 了 万维网 在 这一 时期 开始 
澎湃 的 生长 起来 94年 万维网 协会 成立 从此 之后 
似乎 整个 世界 在 一瞬间 都被 互联网 攻陷 了 从此 
之后 各种 学科 的 发展 尤其 是 计算机 科学 的 
发展 在 互联网 的 冲击 下 产生 了 很多 原来 
没有 的 计算 模型 大 数据 和 各种 统计模型 开始 
大行其道 自然语言 处理 在 这段 时间 也在 大 数据 和 
概率 统计模型 下 得到 了 飞速 的 发展 同时 也 
产生 了 一大批 高科技 公司 并对 其 发展 起到 了 
不同 的 推动 作用 早期 的 雅虎 搜索 后来 的 
谷歌 中国 的 百度 大量 的 基于 web 的 应用 
和 各种 社交 工具 一切 都让/nr 自然语言 处理 在 飞速 
的 发展 中 在 这个 过程 中 各种 数学 算法 
和 计算 模型 越来越 重要 最近 刚 兴起 不久 的 
神经 网络 深度 学习 各种 数据挖掘 算法 机器学习 等等 都 
不断 的 在 消除 人和 机器 之间 交流 的 限制 
也许 在 不久 的 将来 在 互联网 的 基础 上 
自然语言 处理 中 遇到 的 问题 不再 是 问题 不同 
语言 的 人 之间 的 沟通 可以 畅通无阻 人和 机器 
之间 的 沟通 可以 畅通无阻 关于 未来 在 近期 的 
可见 的 各种 技术 的 发展 过程 中 似乎 人 
和 机器 走到 了 一个 相对 对立 的 情形 如果 
继续 这样 发展 下去 很多 人 开始 担心 机器 的 
智能 如果 真的 实现 了 类似 人类 的 智能 并 
远远 超越 了 人类 这样 的 情况 下 人类 会 
不会 被 自己 的 发明 灭绝 呢 或者 人们 开始 
和 自己 的 机器人 情侣 约会 人类 这个 种族 还会 
继续 延续 么 似乎 现在 一切 都 不明朗 但是 我 
个人 还是 抱着 乐观 的 态度 正如 在 机器 出现 
之初 人们 对 机器 的 噪音 造成 的 各种 意外 
以及 其他 危险 的 抱怨 现在 人类 已经 无法 离开 
机器 如果 这个 世界 突然 没有 了 各种 机械 和 
电子 机器 也许 人类 都会 疯 掉 吧 也 希望 
我 可以 在 这个 领域 可以 继续 下去 继续 发挥 
我 个人 的 热量 也许 有一天 智能 将 不仅仅 限于 
人类 作者 微软 亚洲 研究院 链接 https / / www 
. zhihu . com / question / 19895141 / answer 
/ 149475410 来源 知乎 著作权 归 作者 所有 商业 转载 
请 联系 作者 获得 授权 非商业 转载 请 注明 出处 
自然语言 处理 简称 NLP 是 研究 计算机 处理 人类 语言 
的 一门 技术 包括 1 . 句法 语义分析 对于 给定 
的 句子 进行 分词 词性 标记 命名 实体 识别 和 
链接 句法分析 语义 角色 识别 和 多义词 消 歧 2 
. 信息 抽取 从 给定 文本 中 抽取 重要 的 
信息 比如 时间 地点 人物 事件 原因 结果 数字 日期 
货币 专有名词 等等 通俗 说来 就是 要 了解 谁在 什么 
时候 什么原因 对谁 做 了 什么 事 有 什么 结果 
涉及 到 实体 识别 时间 抽取 因果关系 抽取 等 关键 
技术 3 . 文本 挖掘 或者 文本 数据挖掘 包括 文本 
聚 类 分类 信息 抽取 摘要 情感 分析 以及 对 
挖掘 的 信息 和 知识 的 可视化 交互式 的 表达 
界面 目前 主流 的 技术 都是/nr 基于 统计 机器 学习 
的 4 . 机器翻译 把 输入 的 源语言 文本 通过 
自动 翻译 获得 另外 一种 语言 的 文本 根据 输入 
媒介 不同 可以 细分 为 文本 翻译 语音 翻译 手语 
翻译 图形 翻译 等 机器翻译 从 最早 的 基于 规则 
的 方法 到 二十年前 的 基于 统计 的 方法 再到 
今天 的 基于 神经网络 编码 解码 的 方法 逐渐 形成 
了 一套 比较 严谨 的 方法 体系 5 . 信息检索 
对 大 规模 的 文档 进行 索引 可 简单 对 
文档 中 的 词汇 赋 之以 不同 的 权重 来 
建立 索引 也 可利用 1 2 3 的 技术 来 
建立 更加 深层 的 索引 在 查询 的 时候 对 
输入 的 查询 表达式 比如 一个 检索 词 或者 一个 
句子 进行 分析 然后 在 索引 里面 查找 匹配 的 
候选 文档 再 根据 一个 排序 机制 把 候选 文档 
排序 最后 输出 排序 得分 最高 的 文档 6 . 
问答 系统 对 一个 自然 语言 表达 的 问题 由 
问答 系统 给 出 一个 精准 的 答案 需要 对 
自然 语言 查询 语句 进行 某种 程度 的 语义分析 包括 
实体 链接 关系 识别 形成 逻辑 表达式 然后 到 知识库 
中 查找 可能 的 候选 答案 并 通过 一个 排序 
机制 找出 最佳 的 答案 7 . 对话 系统 系统 
通过 一 系列 的 对话 跟 用户 进行 聊天 回答 
完成 某一 项 任务 涉及 到 用户 意图 理解 通用 
聊天 引擎 问答 引擎 对话 管理 等 技术 此外 为了 
体现 上下文 相关 要 具备 多轮 对话 能力 同时 为了 
体现 个性化 要 开发 用户 画像 以及 基于 用户 画像 
的 个性化 回复 随着 深度 学习 在 图像 识别 语音识别 
领域 的 大放异彩 人们 对 深度 学习 在 NLP 的 
价值 也 寄予厚望 再 加上 AlphaGo 的 成功 人工智能 的 
研究 和 应用 变得 炙手可热 自然语言 处理 作为 人工智能 领域 
的 认知 智能 成为 目前 大家 关注 的 焦点 很多 
研究生 都在/nr 进入 自然语言 领域 寄望 未来 在 人工智能 方向 
大展身手 但是 大家 常常 遇到 一些 问题 俗话说 万事开头难 如果 
第一 件 事情 成功 了 学生 就 能 建立 信心 
找到 窍门 今后 越做/nr 越好 否则 也 可能 就 灰心丧气 
甚至 离开 这个 领域 这里 针对 给出 我 个人 的 
建议 希望 我 的 这些 粗浅 观点 能够 引起 大家 
更 深层次 的 讨论 建议 1 如何 在 NLP 领域 
快速 学会 第一个 技能 我 的 建议 是 找到 一个 
开 源 项目 比如 机器翻译 或者 深度 学习 的 项目 
理解 开源 项目 的 任务 编译 通过 该 项目 发布 
的 示范 程序 得到 与 项目 示范 程序 一致 的 
结果 然后 再 深入 理解 开源 项目 示范 程序 的 
算法 自己 编程 实现 一下 这个 示范 程序 的 算法 
再 按照 项目 提供 的 标准 测试 集 测试 自己 
实现 的 程序 如果 输出 的 结果 与 项目 中 
出现 的 结果 不 一致 就要 仔细 查验 自己 的 
程序 反复 修改 直到 结果 与 示范 程序 基本一致 如果 
还是 不行 就 大胆 给 项目 的 作者 写信 请教 
在此 基础 上 再 看看 自己 能否 进一步 完善 算法 
或者 实现 取 得比 示范 程序 更好 的 结果 建议 
2 如何 选择 第一 个 好 题目 工程型 研究生 选题 
很多 都是/nr 老师 给定 的 需要 采取 比较 实用 的 
方法 扎扎实实 地 动手 实现 可能 不 需要 多少 理论 
创新 但是 需要 较强 的 实现 能力 和 综合 创新 
能力 而 学术 型 研究生 需要 取得 一流 的 研究 
成果 因此 选题 需要 有 一定 的 创新 我 这里 
给 出 如下 的 几点 建议 先 找到 自己 喜欢 
的 研究 领域 你 找到 一本 最近 的 ACL 会议 
论文集 从中 找到 一个 你 比较 喜欢 的 领域 在 
选题 的 时候 多 注意 选择 蓝海 的 领域 这 
是因为 蓝海 的 领域 相对 比较 新 容易 出 成果 
充分 调研 这个 领域 目前 的 发展 状况 包括 如下 
几 个 方面 的 调研 方法 方面 是否/v 有/v 一套/m 
比较/d 清晰/a 的/uj 数学/n 体系/n 和/c 机器学习/i 体系/n 数据 方面 
有/v 没有/v 一个/m 大家/n 公认/v 的/uj 标准/n 训练/vn 集/q 和/c 
测试/vn 集/q 研究 团队 是否/v 有/v 著名/a 团队/n 和/c 人士/n 
参加/v 如果 以 上 几个 方面 的 调研 结论 不是 
太 清晰 作为 初学者 可能 不要 轻易 进入 在 确认 
进入 一个 领域 之后 按照 建议 一 所述 需要 找到 
本 领域 的 开源 项目 或者 工具 仔细 研究 一遍 
现有 的 主要 流派 和 方法 先 入门 反复 阅读 
本 领域 最新 发表 的 文章 多 阅读 本 领域 
牛人 发表 的 文章 在 深入 了解 已有 工作 的 
基础 上 探讨 还有 没有 一些 地方 可以 推翻 改进 
综合 迁移 注意 做 实验 的 时候 不要 贪多 每次 
实验 只需要 验证 一个 想法 每次 实验 之后 必须 要 
进行 分析 存在 的 错误 找出 原因 对 成功 的 
实验 进一步 探讨 如何 改进 算法 注意 实验 数据 必须 
是 业界 公认 的 数据 与 已有 的 算法 进行 
比较 体会 能够 得出 比较 一般性 的 结论 如果 有 
则 去 写 一篇 文章 否则 应该 换 一个 新 
的 选题 建议 3 如何写 出 第一 篇 论文 接 
上 一个 问题 如果 想法 不错 且 被 实验 所 
证明 就可 开始 写 第一 篇 论文 了 确定 论文 
的 题目 在 定 题目 的 时候 一般 不要 系统 
研究 与 实践 要 避免 太长 的 题目 因为 不好 
体现 要点 题目 要 具体 有 深度 突出 算法 写 
论文 摘要 要 突出 本文 针对 什么 重要 问题 提出 
了 什么 方法 跟 已有 工作 相比 具 有 什么 
优势 实验 结果 表明 达到 了 什么 水准 解决 了 
什么 问题 写 引言 首先 讲出 本 项 工作 的 
背景 这个 问题 的 定义 它 具有 什么 重要性 然后 
介绍 对 这个 问题 现有 的 方法 是 什么 有 
什么 优点 但是 注意 但是 现有 的 方法 仍然 有 
很多 缺陷 或者 挑战 比如 注意 比如 有 什么 问题 
本文 针对 这个 问题 受 什么 方法 谁 的 工作 
之 启发 提出 了 什么 新的 方法 并 做了 如下 
几 个 方面 的 研究 然后 对 每个 方面 分门别类 
加以 叙述 最后 说明 实验 的 结论 再说 本文 有 
几条 贡献 一般 写 三条 足矣 然后 说说 文章 的 
章节 组织 以及 本文 的 重点 有的 时候 东西 太多 
篇幅 有限 只能 介绍 最 重要 的 部分 不 需要 
面面俱到 相关 工作 对 相关 工作 做 一个 梳理 按照 
流派 划分 对 主要 的 最多 三个 流派 做 一个 
简单 介绍 介绍 其 原理 然后 说明 其 局限性 然后 
可 设立 两 个 章节 介绍 自己 的 工作 第一 
个 章节 是 算法 描述 包括 问题 定义 数学 符号 
算法 描述 文章 的 主要 公式 基本 都在/nr 这里 有时候 
要 给出 简明 的 推导 过程 如果 借鉴 了 别人 
的 理论 和 算法 要给 出 清晰 的 引文 信息 
在此 基础 上 由于 一般 是 基于 机器学习 或者 深度 
学习 的 方法 要 介绍 你 的 模型 训练 方法 
和 解码 方法 第二章 就是 实验 环节 一般 要 给出 
实验 的 目的 要 检验 什么 实验 的 方法 数据 
从 哪里 来 多 大 规模 最好 数据 是 用 
公开 评 测数据 便于 别人 重复 你 的 工作 然后 
对 每个 实验 给出 所需 的 技术 参数 并 报告 
实验 结果 同时 为了 与 已有 工作 比较 需要 引用 
已有 工作 的 结果 必要 的 时候 需要 重现 重要 
的 工作 并 报告 结果 用 实验 数据 说话 说明 
你 比 人家 的 方法 要好 要对 实验 结果 好好 
分析 你 的 工作 与 别人 的 工作 的 不同 
及 各自 利弊 并 说明 其 原因 对于 目前 尚 
不太好 的 地方 要 分析 问题 之 所在 并将 其 
列为 未来 的 工作 结论 对 本文 的 贡献 再一次 
总结 既要 从 理论 方法 上 加以 总结 和 提炼 
也要 说明 在 实验上 的 贡献 和 结论 所做 的 
结论 要让 读者 感到 信服 同时 指出 未来 的 研究 
方向 参考文献 给出 所有 重要 相关 工作 的 论文 记住 
漏掉 了 一篇 重要 的 参考 文献 或者 牛人 的 
工作 基本上 就 没有 被 录取 的 希望 了 写完 
第一稿 然后 就是 再改 三遍 把 文章 交给 同一个 项目组 
的 人士 请 他们 从 算法 新颖 度 创新性/n 和/c 
实验/vn 规模/n 和/c 结论/n 方面/n 以 挑剔 的 眼光 审核 
你 的 文章 自己 针对 薄弱环节 进一步 改进 重点 加强 
算法 深度 和 工作 创新 性 然后 请 不同 项目组 
的 人士 审阅 如果 他们 看 不明白 说明 文章 的 
可读性 不够 你 需要 修改 篇章 结构 进行 文字 润色 
增加 文章 可读性 如 投 ACL 等 国际 会议 最好 
再请 英文 专业 或者 母语 人士 提炼 文字 CS224d － 
Day 1 要 开始 系统 地 学习 NLP 课程 cs224d 
今天 先 来 一个 课程 概览 课程 一 共有 16节 
先 对 每 一节 中 提到 的 模型 算法 工具 
有个 总体 的 认识 知道/v 都有/nr 什么/r 以及 它们 可以 
做 些 什么 事情 简介 1 . Intro to NLP 
and Deep LearningNLP Natural Language Processing 自然语言 处理 的 目的 
就是 让 计算 机能 懂得 人类 对 它 说 的话 
然后 去 执行 一些 指定 的 任务 这些 任务 有 
什么 呢 Easy • Spell Checking － － 拼 写检查 
• Keyword Search － － 关键词 提取 & 搜索 • 
Finding Synonyms － － 同义词 查找 & 替换 Medium • 
Parsing information from websites documents etc . － － 从 
网页 中 提取 有用 的 信息 例如 产品价格 日期 地址 
人名 或 公司名 等 Hard • Machine Translation e . 
g . Translate Chinese text to English － － 自动 
的 或 辅助 的 翻译 技术 • Semantic Analysis What 
is the meaning of query statement － － 市场 营销 
或者 金融交易 领域 的 情感 分析 • Coreference e . 
g . What does he or it refer to given 
a document • Question Answering e . g . Answering 
Jeopardy questions . － － 复杂 的 问答 系统 NLP 
的 难点 情境 多样 语言 歧义 Deep Learning 深度 学习 
是 机器 学习 的 一个 分支 尝试 自动 的 学习 
合适 的 特征 及其 表征 尝试 学习 多 层次 的 
表征 以及 输出 它 在 NLP 的 一些 应用 领域 
上 有 显著 的 效果 例如 机器翻译 情感 分析 问答 
系统 等 和 传统 方法 相比 深度 学习 的 重要 
特点 就是 用 向量 表示 各种 级别 的 元素 传统 
方法 会用 很 精细 的 方法 去 标注 深度 学习 
的话 会用 向量 表示 单词 短语 逻辑 表达式 和 句子 
然后 搭建 多层 神经网络 去 自主 学习 这里有 简明扼要 的 
对比 总结 向量 表示 词 向量 One － hot 向量 
记 词典 里 有 | V | 个 词 每个 
词 都被 表示 成 一个 | V | 维 的 
向量 设 这个词 在 字典 中 相应 的 顺序 为 
i 则 向量 中 i 的 位置 上为 1 其余 
位置 为 0 . 词 － 文档 矩阵 构建 一个 
矩阵 X 每个 元素 Xij 代表 单词 i 在 文档 
j 中 出现 的 次数 词 － 词 共 现 
矩阵 构建 矩阵 X 每个 元素 Xij 代表 单词 i 
和 单词 j 在 同一 个 窗口 中 出现 的 
次数 模型 算法 2 . Simple Word Vector representations word2vec 
GloVeword2vec word2vec 是 一套 能将 词 向 量化 的 工具 
Google 在 13年 将其 开源 代码 可以 见 https / 
/ github . com / burness / word2vec 它 将 
文本 内容 处理 成为/nr 指定 维度 大小 的 实数 型 
向量 表示 并且 其 空间 上 的 相似 度 可以 
用来 表示 文本 语义 的 相似 度 Word2vec 的 原理 
主要 涉及 到 统计 语言 模型 包括 N gram 模型 
和 神经 网络 语言 模型 continuousbag of words 模型 以及 
continuous skip gram 模型 N gram 的 意思 就是 每个 
词 出现 只看 其 前面 的 n 个 词 可以 
对 每个 词 出现 的 概率 进行 近似 比如 当 
n = 2 的 时候 神经 网络 语言 模型 NNLM 
用 特征向量 来 表征 每个 词 各个 方面 的 特征 
NNLM 的 基础 是 一个 联合 概率 其 神经 网络 
的 目的 是 要 学习 Continuous Bag of Words CBOW 
模型 与 NNLM 类似 结构 如下 CBOW 是 通过 上下文 
来 预测 中间 的 词 如果 窗口 大小 为 k 
则 模型 预测 其 神经 网络 就是 用 正负 样本 
不断 训练 求解 输出 值 与 真实 值 误差 然后 
用 梯度 下降 的 方法 求解 各 边 权重 参数值 
的 Continuous skip gram 模型 与 CBOW 正好 相反 是 
通过 中间 词 来 预测 前后 词 一般 可以 认为 
位置 距离 接近 的 词 之间 的 联系 要比 位置 
距离 较远 的 词 的 联系 紧密 目标 为 最大化 
结构 为 应用 同义词 查找 文本 聚 类 实现 方法 
用 关键词 来 表征 文本 关键词 提 取用 TF IDF 
然后 用 word2vec 训练 得到 关键词 向量 再用 k means 
聚 类 最后 文本 就 能够 以 关键词 的 类别 
进行 分类 了 文本 类别 投递 实现 方法 人工 标 
记出 该词 属于 各个 类别 的 概率 出 全 体词 
属于 各个 类别 的 概率 Glove Global Vectors 的 目的 
就是 想 要 综合 前面 讲到 的 word document 和 
word windows 两种 表示 方法 做到 对 word 的 表示 
即 sementic 的 表达 效果 好 syntactic 的 表达 效果 
也好 3 . Advanced word vector representations language models softmax 
single layer networkssoftmax softmax 模型 是 logistic 模型 在 多分 
类 问题 上 的 推广 logistic 回归 是 针对 二分 
类 问题 的 类 标记 为 { 0 1 } 
在 softmax 模型 中 label 可以为 k 个 不同 的 
值 4 . Neural Networks and backpropagation – for named 
entity recognition5 . Project Advice Neural Networks and Back Prop 
in full gory detail Neural Networks 神经 网络 是 受 
生物学 启发 的 分类器 可以 学习 更 复杂 的 函数 
和 非线性 决策 边界 模型 调 优 6 . Practical 
tips gradient checks overfitting regularization activation functions details * * 
UFLDL * * Unsupervised Feature Learning and Deep LearningGradient Checking 
梯度 检测 反向 传播 因为 细节 太多 往往 会 导致 
一些 小 的 错误 尤其 是 和 梯度 下 降法 
或者 其他 优化 算法 一起 运行时 看似 每次 J Θ 
的 值 在 一次 一次 迭代 中 减小 但 神经 
网络 的 误差 可能会 大过 实际 正确 计算 的 结果 
针对 这种 小 的 错误 有 一种 梯度 检验 Gradient 
checking 的 方法 通过 数值 梯度 检验 你 能 肯定 
确实是 在 正确 地 计算 代价 函数 Cost Function 的 
导数 GC 需要 对 params 中的 每一个 参数 进行 check 
也 就是 依次 给 每一个 参数 一个 极小量 overfitting 就是 
训练 误差 Ein 很小 但是 实际 的 真实 误差 就可能 
很大 也 就是 模型 的 泛化 能力 很差 bad generalization 
发生 overfitting 的 主要 原因 是 1 使用 过于 复杂 
的 模型 dvc 很大 2 数据 噪音 3 有限 的 
训练 数据 regularization 为了 提高 模型 的 泛化 能力 最 
常见 方法 便是 正则化 即在 对模型 的 目标 函数 objective 
function 或 代价 函数 cost function 加上 正则 项 平台 
7 . Introduction to T e n s o r 
f l o w T e n s o r 
f l o w Tensorflow 是 python 封装 的 深度 
学习 库 非常 容易 上手 对 分布式系统 支持 比 Theano 
好 同时 还是 Google 提供 资金 研发 的 在 Tensorflow 
里 使用 张量 tensor 表示 数据 . 使 用图 graph 
来 表示 计算 任务 . 在被 称之为 会话 Session 的 
上下文 context 中 执行 图 . 通过 变量 Variable 维护 
状态 . 使用 feed 和 fetch 可以为 任意 的 操作 
arbitrary operation 赋值 或者 从 其中 获取数据 . TensorFlow 算是 
一个 编程 系统 它 使用 图 来 表示 计算 任务 
图中 的 节点 被 称之为 operation 可以 缩写成 op 一个 
节点 获得 0个 或者 多个 张量 tensor 下文 会 介绍 
到 执行 计算 产生 0个 或 多个 张量 模型 与 
应用 8 . Recurrent neural networks – for language modeling 
and other tasksRNN 在 深度 学习 领域 传统 的 前馈 
神经网络 feed forward neural net 简称 FNN 具有 出色 的 
表现 在 前馈 网络 中 各 神经元 从 输入 层 
开始 接收 前 一级 输入 并 输入 到 下 一级 
直至 输出 层 整个 网络 中 无 反馈 可用 一个 
有向 无 环 图 表示 不同 于 传统 的 FNNs 
RNNs 引入 了 定向 循环 能够 处理 那些 输入 之间 
前后 关联 的 问题 定向 循环 结构 如下 图 所示 
9 . GRUs and LSTMs – for machine translation 传统 
的 RNN 在 训练 long term dependencies 的 时候 会 
遇到 很多 困难 最 常见 的 便是 vanish gradient problem 
期间 有 很多 种 解决 这个 问题 的 方法 被 
发表 大致 可以 分为 两类 一类 是 以 新的 方法 
改善 或者 代替 传统 的 SGD 方法 如 Bengio 提出 
的 clip gradient 另一种 则 是 设计 更加 精密 的 
recurrent unit 如 LSTM GRU LSTMs 长短期 内存 网络 Long 
Short Term Memory networks 是 一种 特殊 的 RNN 类型 
可以 学习 长期 依赖 关系 LSTMs 刻意 的 设计 去 
避免 长期 依赖 问题 记住 长期 的 信息 在 实践 
中 RNN 几乎 默认 的 行为 但是 却 需要 很大 
的 代价 去 学习 这种 能力 LSTM 同样 也是 链式 
结构 但是 重复 的 模型 拥 有 不同 的 结构 
它 与 单个 的 神经网 层 不同 它 有 四个 
使用 非常 特别 方式 进行 交互 GRUs Gated Recurrent Unit 
也是 一般 的 RNNs 的 改良 版本 主要 是 从 
以下 两个 方面 进行 改进 一是 序列 中 不同 的 
位置 处 的 单词 已 单词 举例 对 当前 的 
隐藏 层 的 状态 的 影响 不同 越 前面 的 
影响 越小 即 每个 前面 状态 对 当前 的 影响 
进行 了 距离 加权 距离 越远 权值 越小 二 是 
在 产生 误差 error 时 误差 可能 是 由 某 
一个 或者 几个 单词 而 引发 的 所以 应当 仅 
仅对 对应 的 单词 weight 进行 更新 10 . Recursive 
neural networks – for parsing11 . Recursive neural networks – 
for different tasks e . g . sentiment analysis Recursive 
neural networks 和 前面 提到 的 Recurrent Neural Network 相比 
recurrent 时间 维度 的 展开 代表 信息 在 时间 维度 
从前 往后 的 的 传递 和 积累 可以 类比 markov 
假设 后面 的 信息 的 概率 建立 在 前面 信息 
的 基础 上 recursive 空间维度 的 展开 是 一个 树结构 
就是 假设 句子 是 一个 树状 结构 由 几个 部分 
主语 谓语 宾语 组成 而每 个 部分 又 可以 在 
分成 几个 小 部分 即 某一 部分 的 信息 由 
它 的 子树 的 信息 组合 而来 整句话 的 信息 
由 组成 这 句话 的 几个 部分 组合 而来 12 
. Convolutional neural networks – for sentence c l a 
s s i f i c a t i o 
n C o n v o l u t i 
o n a l neural networks 卷积 神经 网络 是 
一种 特殊 的 深层 的 神经 网络 模型 它 的 
特殊性 体现 在 两个 方面 一 方面 它 的 神经元 
间 的 连接 是非 全 连接 的 另一方面 同 一层 
中 某些 神经元 之间 的 连接 的 权重 是 共享 
的 即 相同 的 它 的 非 全连 接和 权值 
共享 的 网络结构 使之 更 类似于 生物 神经网络 降低 了 
网络 模型 的 复杂度 减少 了 权值 的 数量 13 
. Guest Lecture with Andrew Maas Speech recognition14 . Guest 
Lecture with Thang Luong Machine Translation 大 数据 15 . 
Guest Lecture with Quoc Le Seq2Seq and Large Scale DLSeq2Seq 
seq2seq 是 一个 机器翻译 模型 解决 问题 的 主要 思路 
是 通过 深度 神经网络 模型 常用 的 是 LSTM 长短 
记忆 网络 一种 循环 神经网络 将 一个 作为 输入 的 
序列 映射 为 一个 作为 输出 的 序列 这一 过程 
由 编码 输入 与 解码 输出 两个 环节 组成 Encoder 
Decoder 注意 机制 是 Seq2Seq 中 的 重要 组成部分 应用 
领域 有 机器翻译 智能 对话 与 问答 自动编码 与 分类器 
训练 等 Large Scale DL 为了 让 Neural Networks 有 
更好 的 效果 需要 更多 的 数据 更大 的 模型 
更多 的 计算 Jeff Dean On Large Scale Deep Learning 
At Google 未来 方向 16 . The future of Deep 
Learning for NLP Dynamic Memory Networksdynamic memory network DMN 利用 
dynamic memory network DMN 框架 可以 进行 QA 甚至 是 
Understanding Natural Language 这个 框架 是由 几个 模块 组成 可以 
进行 end to end 的 training 其中 核心 的 module 
就是 Episodic Memory module 可以 进行 iterative 的 semantic + 
reasoning processing 有了/nr 一个 总体 的 了解 看 的 热血 
沸腾 的 下一次 开始 各个击破 cs224d Day 1 . 深度 
学习 与 自然 语言 处理 主要 概念 一览 Day 2 
. TensorFlow 入门 Day 3 . word2vec 模型 思想 和 
代码 实现 Day 4 . 怎样 做 情感 分析 Day 
5 . CS224d － Day 5 RNN 快速 入门 Day 
6 . 一 文学 会用 Tensorflow 搭建 神经网络 Day 7 
. 用 深度 神经网络 处理 NER 命名 实体 识别 问题 
Day 8 . 用 RNN 训练 语言 模型 生成 文本 
Day 9 . RNN 与 机器 翻译 Day 10 . 
用 Recursive Neural Networks 得到 分析树 Day 11 . RNN 
的 高级 应用 推荐 阅读 历史 技术 博文 链接 汇总 
也许 可以 找到 你 想要 的 BERT 模型 代码 已经 
发布 可以 在 我 的 github NLP BERT Python3 . 
6 pytorch   中 下载 请 记得 start 哦 目录 
一 前言 二 如何 理解 BERT 模型 三 BERT 模型 
解析 论文 的 核心 详解 BERT 模型 架构 关键 创新 
预 训练任务 实验 结果 四 BERT 模型 的 影响 对 
BERT 模型 的 观点 参考文献 一 前言 最近 谷歌 搞了个 
大 新闻 公司 AI 团队 新 发布 的 BERT 模型 
在 机器 阅读 理解 顶级 水平 测试 SQuAD1 . 1 
中 表现 出 惊人 的 成绩 全部 两个 衡量 指标 
上 全面 超越 人类 并且 还在 11种 不同 NLP 测试 
中 创出 最佳 成绩 包括 将 GLUE 基准 推 至 
80.4％ 绝对 改进 7.6％ MultiNLI 准确度 达到 86.7% 绝对 改进 
率 5.6％ 等 可以 预见 的 是 BERT 将为 NLP 
带来 里程碑式 的 改变 也是 NLP 领域 近期 最 重要 
的 进展 谷歌 团队 的 Thang Luong 直接 定义 BERT 
模型 开启 了 NLP 的 新时代 从 现在 的 大趋势 
来看 使用 某种 模型 预 训练 一个 语言 模型 看起来 
是 一种 比较 靠谱 的 方法 从 之前 AI2 的 
ELMo 到 OpenAI 的 fine tune transformer 再到 Google 的 
这个 BERT 全都 是 对 预 训练 的 语言 模型 
的 应用 BERT 这个 模型 与 其它 两个 不同 的 
是 它 在 训练 双向 语言 模型 时以/nr 减小 的 
概率 把 少量 的 词 替 成了 Mask 或者 另 
一个 随机 的 词 我 个人 感觉 这个 目的 在于 
使 模型 被迫 增加 对 上下文 的 记忆 至于 这个 
概率 我 猜 是 Jacob 拍脑袋 随便 设 的 增加 
了 一个 预测 下 一句 的 loss 这个 看起来 就 
比较 新奇 了 BERT 模型 具有 以下 两个 特点 第一 
是 这个 模型 非常 的 深 12层 并不 宽 wide 
中间层 只有 1024 而 之前 的 Transformer 模型 中间层 有 
2048 这 似乎 又 印证 了 计算机 图像 处理 的 
一个 观点 深 而 窄 比 浅 而 宽 的 
模型 更好 第二 MLM Masked Language Model 同时 利用 左侧 
和 右侧 的 词语 这个 在 ELMo 上 已经 出现 
了 绝对 不是 原创 其次 对于 Mask 遮挡 在 语言 
模型 上 的 应用 已经 被 Ziang Xie 提出 了 
我 很 有幸 的 也 参与 到了 这篇 论文 中 
1703.02573 Data Noising as Smoothing in Neural Network Language Models 
这 也是 篇 巨星 云集 的 论文 Sida Wang Jiwei 
Li 香侬 科技 的 创始人 兼 CEO 兼 史上 发文 
最多 的 NLP 学者 Andrew Ng Dan Jurafsky 都是 Coauthor 
但 很 可惜 的 是 他们 没有 关注 到 这篇 
论文 用 这篇 论文 的 方法 去做 Masking 相信 BRET 
的 能力 说不定 还会有 提升 二 如何 理解 BERT 模型 
1 BERT 要 解决 什么 问题 通常 情况 transformer 模型 
有 很多 参数 需要 训练 譬如 BERT BASE 模型 L 
= 12 H = 768 A = 12 需要 训练 
的 模型 参数 总数 是 12 * 768 * 12 
= 110M 这么 多 参数 需要 训练 自然 需要 海量 
的 训练 语料 如果 全部 用 人力 标注 的 办法 
来 制作 训练 数据 人力 成本 太大 受 A Neural 
Probabilistic Language Model 论文 的 启发 BERT 也用 unsupervised 的 
办法 来 训练 transformer 模型 神经 概率 语言 模型 这篇 
论文 主要 讲 了 两件 事儿 1 . 能否 用 
数值 向量 word vector 来 表达 自然语言 词汇 的 语义 
2 . 如何 给 每个 词汇 找到 恰当 的 数值 
向量 这篇 论文 写 得 非常 精彩 深入浅出 要言不烦 而且 
面面俱到 经典 论文 值得 反复 咀嚼 很多 同行 朋友 都 
熟悉 这 篇 论文 内容 不 重复 说 了 常用 
的 中文 汉字 有 3500 个 这些 字 组合成 词汇 
中文 词汇 数量 高达 50 万个 假如 词 向量 的 
维度 是 512 那么 语言 模型 的 参数 数量 至少 
是 512 * 50万 = 256M 模型 参数 数量 这么 
大 必然 需要 海量 的 训练 语料 从 哪里 收集 
这些 海量 的 训练 语料 A Neural Probabilistic Language Model 
这篇 论 文说 每 一篇 文章 天生 是 训练 语料 
难道 不 需要 人工 标注 吗 回答 不 需要 我们 
经常 说 说话 不要 颠三倒四 要 通顺 要 连贯 意思 
是 上下文 的 词汇 应该 具有 语义 的 连贯性 基于 
自然 语言 的 连贯性 语言 模型 根据 前文 的 词 
预测 下 一个 将 出现 的 词 如果 语言 模型 
的 参数 正确 如果 每个 词 的 词 向量 设置 
正确 那么 语言 模型 的 预测 就 应该 比较 准确 
天下 文章 数不胜数 所以 训练 数据 取之不尽 用之不竭 深度 学习 
四大 要素 1 . 训练 数据 2 . 模型 3 
. 算 力 4 . 应用 训练 数据 有了 接 
下去 的 问题 是 模型 2 BERT 的 五个 关键词 
Pre training Deep Bidirectional Transformer Language Understanding 分别 是 什么 
意思 A Neural Probabilistic Language Model 这篇 论文 讲 的 
Language Model 严格 讲 是 语言 生成 模型 Language Generative 
Model 预测 语句 中下 一个 将 会 出现 的 词汇 
语言 生成 模型 能 不能 直接 移用 到 其它 NLP 
问题 上去 譬如 淘宝 上 有 很多 用户 评论 能否 
把 每 一条 用户 转换成 评分 2 1 0 1 
2 其中 2 是 极差 + 2 是 极好 假如 
有 这样 一条 用户 评语 买了 一件 鹿晗 同 款 
衬衫 没想到 穿 在 自己 身上 不像 小 鲜肉 倒像 
是 厨师 请问 这条 评语 等同于 2 还是 其它 语言 
生成 模型 能 不能 很好 地 解决 上述 问题 进一步 
问 有 没有 通用 的 语言 模型 能够 理解 语言 
的 语义 适用 于 各种 NLP 问题 BERT 这篇 论文 
的 题目 很 直白 BERT Pre training of Deep Bidirectional 
Transformers for Language Understanding 一 眼看 去 就能 猜得到 这篇文章 
会讲 哪些 内容 这个 题目 有 五个 关键词 分别 是 
Pre training Deep Bidirectional Transformers 和 Language Understanding 其中 pre 
training 的 意思 是 作者 认为 确实 存在 通用 的 
语言 模型 先用 文章 预 训练 通用 模型 然后 再 
根据 具体 应用 用 supervised 训练 数据 精加工 fine tuning 
模型 使之 适用 于 具体 应用 为了 区别 于 针对 
语言 生成 的 Language Model 作者 给 通用 的 语言 
模型 取 了 一个 名字 叫 语言表征 模型 Language Representation 
Model 能 实现 语言表征 目标 的 模型 可能会 有 很多 
种 具体 用 哪一种 呢 作者 提议 用 Deep Bidirectional 
Transformers 模型 假如 给 一个 句子 能 实现 语言表征 mask 
的 模型 遮盖住 其中 目标 一 词 从前 往后 预测 
mask 也 就是 用 能//nr 实现 / 语言 / 表征 
来 预测 mask 或者 从后/nr 往前/t 预测/vn mask 也 就是 
用 模型 / 的 来 预测 mask 称之为 单向 预测 
unidirectional 单向 预测 不能 完整 地 理解 整个 语句 的 
语义 于是 研究者 们 尝试 双向 预测 把 从前 往后 
与/p 从后/nr 往前/t 的/uj 两个/m 预测/vn 拼接 在 一起 mask1 
/ mask2 这 就是 双向 预测 bi directional 细节 参阅 
Neural Machine Translation by Jointly Learning to Align and Translate 
BERT 的 作者 认为 bi directional 仍然 不能 完整 地 
理解 整个 语句 的 语义 更好 的 办法 是 用 
上下文 全向 来 预测 mask 也 就是 用 能//nr 实现 
/ 语言 / 表征 / . . / 的 / 
模型 来 预测 mask BERT 作者 把 上下文 全向 的 
预测 方法 称之为 deep bi directional 如何 来 实现 上下文 
全向 预测 呢 BERT 的 作者 建议 使用 Transformer 模型 
这个 模型 由 Attention Is All You Need 一文 发明 
这个 模型 的 核心 是 聚焦 机制 对于 一个 语句 
可以 同时 启用 多个 聚焦点 而 不必 局限 于 从前 
往后 的 或者/c 从后/nr 往前/t 的/uj 序列 串行 处理 不仅 
要 正确 地 选择 模型 的 结构 而且 还要 正确 
地 训练 模型 的 参数 这样 才能 保障 模型 能够 
准确 地 理解 语句 的 语义 BERT 用 了 两个 
步骤 试图 去 正确 地 训练 模型 的 参数 第一 
个 步骤 是 把 一篇 文章 中 15% 的 词汇 
遮盖 让 模型 根据 上下文 全向 地 预测 被 遮盖 
的 词 假如有 1 万篇/nr 文章 每 篇 文章 平均 
有 100 个 词汇 随机 遮盖 15% 的 词汇 模型 
的 任务 是 正确 地 预测 这 15 万个 被 
遮盖 的 词汇 通过 全向 预测 被 遮盖住 的 词汇 
来 初步 训练 Transformer 模型 的 参数 然后 用 第二 
个 步骤 继续 训练 模型 的 参数 譬如 从 上述 
1 万篇/nr 文章 中 挑选 20 万对 语句 总共 40 
万条 语句 挑选 语句 对 的 时候 其中 2 * 
10 万对 语句 是 连续 的 两条 上下文 语句 另外 
2 * 10 万对 语句 不是 连续 的 语句 然后 
让 Transformer 模型 来 识别 这 20 万对 语句 哪些 
是 连续 的 哪些 不 连续 这 两步 训练 合在一起 
称为 预 训练 pre training 训练 结束 后的/nr Transformer 模型 
包括 它 的 参数 是 作者 期待 的 通用 的 
语言 表征 模型 三 BERT 模型 解析 首先 来看 下 
谷歌 AI 团队 做 的 这篇 论文 BERT 的 新 
语言 表示 模型 它 代表 Transformer 的 双向 编码器 表示 
与 最近 的 其他 语言 表示 模型 不同 BERT 旨在 
通过 联合 调节 所 有层 中的 上下文 来 预先 训练 
深度 双向 表示 因此 预 训练 的 BERT 表示 可以 
通过 一个 额外 的 输出 层 进行 微调 适用于 广泛 
任务 的 最 先进 模型 的 构建 比如 问答 任务 
和 语言 推理 无需 针对 具体 任务 做 大幅 架构 
修改 论文 作者 认为 现有 的 技术 严重 制约 了 
预 训练 表示 的 能力 其 主要 局限 在于 标准 
语言 模型 是 单向 的 这 使得 在 模型 的 
预 训练 中 可以 使用 的 架构 类型 很 有限 
在 论文 中 作者 通过 提出 BERT 即 Transformer 的 
双向 编码表示 来 改进 基于 架构 微调 的 方法 BERT 
提出 一种 新的 预 训练 目标 遮蔽 语言 模型 masked 
language model MLM 来 克服 上文 提到 的 单向 性 
局限 MLM 的 灵感 来自 Cloze 任务 Taylor 1953 MLM 
随机 遮蔽 模型 输入 中 的 一些 token 目标 在于 
仅 基于 遮蔽 词 的 语境 来 预测 其 原始 
词汇 id 与 从左到右 的 语言 模型 预 训练 不同 
MLM 目标 允许 表征 融合 左右 两侧 的 语境 从而 
预 训练 一个 深度 双向 Transformer 除了 遮蔽 语言 模型 
之外 本文 作者 还 引入 了 一个 下 一句 预测 
next sentence prediction 任务 可以 和 MLM 共同 预 训练 
文本 对 的 表示 论文 的 主要 贡献 在于 证明 
了 双向 预 训练 对 语言 表示 的 重要性 与 
之前 使用 的 单向 语言 模型 进行 预 训练 不同 
BERT 使用 遮蔽 语言 模型 来 实现 预 训练 的 
深度 双向 表示 论文 表明 预先 训练 的 表示 免去 
了 许多 工程 任务 需要 针对 特定 任务 修改 体系 
架构 的 需求 BERT 是 第一 个 基于 微调 的 
表示 模型 它 在 大量 的 句子 级 和 token 
级任务 上 实现 了 最 先进 的 性能 强于 许多 
面向 特定 任务 体系 架构 的 系统 BERT 刷新 了 
11项 NLP 任务 的 性能 记录 本文 还报 告了 BERT 
的 模型简化 研究 ablation study 表明 模型 的 双向性 是 
一项 重要 的 新 成果 相关 代码 和 预先 训练 
的 模型 将 会 公布 在 goo . gl / 
language / bert 上 BERT 目前 已经 刷新 的 11项 
自然语言 处理 任务 的 最新 记录 包括 将 GLUE 基准 
推 至 80.4％ 绝对 改进 7.6％ MultiNLI 准确度 达到 86.7% 
绝对 改进 率 5.6％ 将 SQuAD v 1.1 问答 测试 
F1 得分 纪录 刷新 为 93.2分 绝对 提升 1.5分 超过 
人类 表现 2.0分 论文 的 核心 详解 BERT 模型 架构 
本节 介绍 BERT 模型 架构 和 具体 实现 并 介绍 
预 训练任务 这是 这篇 论文 的 核心 创新 模型 架构 
BERT 的 模型 架构 是 基于 Vaswani et al . 
2017 中 描述 的 原始 实现 multi layer bidirectional Transformer 
编码器 并在 tensor2tensor 库 中 发布 由于 Transformer 的 使用 
最近 变得 无处不在 论 文中 的 实现 与 原始 实现 
完全 相同 因此 这里 将 省略 对模型 结构 的 详细 
描述 在 这项 工作 中 论文 将 层数 即 Transformer 
blocks 表示 为 L 将 隐藏 大小 表示 为 H 
将 self attention heads 的 数量 表示 为 A 在 
所有 情况 下 将 feed forward / filter 的 大小 
设置 为 4H 即 H = 768 时为 3072 H 
= 1024 时为 4096 论文 主要 报告 了 两种 模型 
大小 的 结果 L = 12 H = 768 A 
= 12 Total Parameters = 110M L = 24 H 
= 1024 A = 16 Total Parameters = 340M 为了 
进行 比较 论文 选择 了     它 与 OpenAI 
GPT 具有 相同 的 模型 大小 然而 重要 的 是 
BERT Transformer 使用 双向 self attention 而 GPT Transformer 使用 
受 限制 的 self attention 其中 每个 token 只能 处理 
其 左侧 的 上下文 研究 团队 注意到 在 文献 中 
双向 Transformer 通常 被 称为 Transformer encoder 而 左侧 上下文 
被称为 Transformer decoder 因为 它 可以 用于 文本 生成 BERT 
OpenAI GPT 和 ELMo 之间 的 比较 如 所示 预 
训练 模型 架构 的 差异 BERT 使用 双向 Transformer OpenAI 
GPT 使用 从左到右 的 Transformer ELMo 使用 经过 独立 训练 
的 从左到右 和 从右到左 LSTM 的 串联 来 生成 下游 
任务 的 特征 三个 模型 中 只有 BERT 表示 在 
所有 层 中 共同 依赖于 左右 上下文 输入 表示 input 
representation 论文 的 输入 表示 input representation 能够 在 一个 
token 序列 中 明确 地 表示 单个 文本 句子 或 
一对 文本 句子 例如 Question Answer 对于 给定 token 其 
输入 表示 通过 对 相应 的 token segment 和 position 
embeddings 进行 求和 来 构造 是 输入 表示 的 直观 
表示 BERT 输入 表示 输入 嵌入 是 token embeddings segmentation 
embeddings 和 position embeddings 的 总和 具体 如下 使用 WordPiece 
嵌入 Wu et al . 2016 和 30 000个 token 
的 词汇表 用 # # 表示 分词 使用 学习 的 
positional embeddings 支持 的 序列 长度 最多 为 512个 token 
每个 序列 的 第一 个 token 始终 是 特殊 分类 
嵌入 CLS 对应 于该/nr token 的 最终 隐藏 状态 即 
Transformer 的 输出 被 用作 分类 任务 的 聚合 序列 
表示 对于 非 分类 任务 将 忽略 此 向量 句子 
对 被 打包 成 一个 序列 以 两种 方式 区分 
句子 首先 用 特殊 标记 SEP 将 它们 分开 其次 
添加 一个 learned sentence A 嵌入 到 第一 个 句子 
的 每个 token 中 一个 sentence B 嵌入 到 第二 
个 句子 的 每个 token 中 对于 单个 句子 输入 
只 使用 sentence A 嵌入 关键 创新 预 训练 任务 
与 Peters et al . 2018 和 Radford et al 
. 2018 不同 论文 不 使用 传统 的 从左到右 或 
从右到左 的 语言 模型 来 预 训练 BERT 相反 使用 
两个 新的 无 监督 预测 任务 对 BERT 进行 预 
训练 任务 1 Masked LM 从 直觉 上看 研究 团队 
有 理由 相信 深度 双向 模型 比 left to right 
模型 或 left to right and right to left 模型 
的 浅层 连接 更 强大 遗憾 的 是 标准 条件 
语言 模型 只能 从左到右 或 从右到左 进行 训练 因为 双向 
条件 作用 将 允许 每个 单词 在 多层 上下 文中 
间接地 see itself 为了 训练 一个 深度 双向 表示 deep 
bidirectional representation 研究 团队 采用 了 一种 简单 的 方法 
即 随机 屏蔽 masking 部分 输入 token 然后 只 预测 
那些 被 屏蔽 的 token 论文 将 这个 过程 称为 
masked LM MLM 尽管 在 文献 中 它 经常 被 
称为 Cloze 任务 Taylor 1953 在 这个 例子 中 与 
masked token 对应 的 最终 隐藏 向量 被 输入 到 
词汇 表上 的 输出 softmax 中 就像 在 标准 LM 
中 一样 在 团队 所 有 实验 中 随机 地 
屏蔽 了 每个 序列 中 15% 的 WordPiece token 与 
去 噪 的 自动 编码器 Vincent et al . 2008 
相反 只 预测 masked words 而 不是 重建 整个 输入 
虽然 这 确实 能让 团队 获得 双向 预 训练 模型 
但 这种 方法 有 两个 缺点 首先 预 训练 和 
finetuning 之间 不 匹配 因为 在 finetuning 期间 从未 看到 
MASK token 为了 解决 这个 问题 团队 并不 总是 用 
实际 的 MASK token 替换 被 masked 的 词汇 相反 
训练 数据 生成器 随机 选择 15％ 的 token 例如 在 
这个 句子 my dog is hairy 中 它 选择 的 
token 是 hairy 然后 执行 以下 过程 数据 生成器 将 
执行 以 下 操作 而 不是 始终 用 MASK 替换 
所选 单词 80％ 的 时间 用 MASK 标记 替换 单词 
例如 my dog is hairy → my dog is MASK 
10％ 的 时间 用 一个 随机 的 单词 替换 该 
单词 例如 my dog is hairy → my dog is 
apple10 ％ 的 时间 保持 单词 不变 例如 my dog 
is hairy → my dog is hairy . 这样 做 
的 目的 是 将 表示 偏向 于 实际 观察到 的 
单词 Transformer encoder 不 知道 它 将被 要求 预测 哪些 
单词 或 哪些 单词 已被 随机 单词 替换 因此 它 
被迫 保持 每个 输入 token 的 分布式 上下文 表示 此外 
因为 随机 替换 只 发生 在 所有 token 的 1.5％ 
即 15％ 的 10％ 这 似乎 不会 损害 模型 的 
语言 理解能力 使用 MLM 的 第二个 缺点 是 每个 batch 
只 预测 了 15％ 的 token 这表明 模型 可能 需要 
更多 的 预 训练 步骤 才能 收敛 团队 证明 MLM 
的 收敛 速度 略慢 于 left to right 的 模型 
预测 每个 token 但 MLM 模型 在 实验 上 获得 
的 提升 远远 超过 增加 的 训练 成本 任务 2 
下 一句 预测 许多 重要 的 下游 任务 如 问答 
QA 和 自然 语言 推理 NLI 都是 基于 理解 两个 
句子 之间 的 关系 这 并 没有 通过 语言 建模 
直接 获得 在 为了 训练 一个 理解 句子 的 模型 
关系 预先 训练 一个 二进制 化 的 下 一句 测 
任务 这一/i 任务/n 可以/c 从/p 任何/r 单语/nr 语料库/n 中/f 生成/v 
具体地说 当 选择 句子 A 和B/nr 作为 预 训练样本 时 
B 有 50％ 的 可能 是 A 的 下一个 句子 
也有 50％ 的 可能 是 来自 语料库 的 随机 句子 
例如 Input = CLS the man went to MASK store 
SEP he bought a gallon MASK milk SEP Label = 
IsNextInput = CLS the man MASK to the store SEP 
penguin MASK are flight # # less birds SEP Label 
= NotNext 团队 完全 随机 地 选择 了 NotNext 语句 
最终 的 预 训练 模型 在 此 任务 上 实现 
了 97％ 98％ 的 准确率 实验 结果 如 前文 所述 
BERT 在 11项 NLP 任务 中 刷新 了 性能 表现 
记录 在 这 一节 中 团队 直观 呈现 BERT 在 
这些 任务 的 实验 结果 具体 的 实验 设置 和 
比较 请 阅读 原 论文 . 我们 的 面向 特定 
任务 的 模型 是 将 BERT 与 一个 额外 的 
输出 层 结合 而 形成 的 因此 需要 从头 开始 
学习 最小 数量 的 参数 在 这些 任务 中 a 
和 b 是 序列 级任务 而 c 和 d 是 
token 级任务 在 图中 E 表示 输入 嵌入 Ti 表示 
tokeni 的 上下 文 表示 CLS 是 用于 分类 输出 
的 特殊符号 SEP 是 用于 分隔 非 连续 token 序列 
的 特殊符号 GLUE 测试 结果 由 GLUE 评估 服务器 给出 
每个 任务 下方 的 数字 表示 训练 样例 的 数量 
平均 一 栏中 的 数据 与 GLUE 官方 评分 稍有 
不同 因为 我们 排 除了 有 问题 的 WNLI 集 
BERT 和 OpenAI GPT 的 结果 是 单 模型 单任务 
下 的 数据 所有 结果 来自 https / / gluebenchmark 
. com / leaderboard 和 https / / blog . 
openai . com / language unsupervised / SQuAD 结果 BERT 
集成 是 使用 不同 预 训练 检查点 和 fine tuning 
seed 的 7x 系统 CoNLL 2003 命名 实体 识别 结果 
超 参数 由 开发 集 选择 得出 的 开发 和 
测试 分数 是 使用 这些 超 参数 进行 五次 随机 
重启 的 平均值 四 BERT 模型 的 影响 BERT 是 
一个 语言表征 模型 language representation model 通过 超大 数据 巨 
大模型 和 极大 的 计算 开销 训练 而成 在 11个 
自然语言 处理 的 任务 中 取得 了 最优 state of 
the art SOTA 结果 或许 你 已经 猜到 了此 模型 
出自 何方 没错 它 产自 谷歌 估计 不少 人 会 
调侃 这种 规模 的 实验 已经 基本 让 一般 的 
实验室 和 研究员 望尘莫及 了 但 它 确实 给 我们 
提供 了 很多 宝贵 的 经验 深度 学习 就是 表征 
学习 Deep learning is representation learning We show that pre 
trained representations eliminate the needs of many heavily engineered task 
specific architectures . 在 11项 BERT 刷 出新 境界 的 
任务 中 大多 只在 预 训练 表征 pre trained representation 
微调 fine tuning 的 基础上 加 一个 线性 层 作为 
输出 linear output layer 在 序列 标注 的 任务 里 
e . g . NER 甚至连 序列 输出 的 依赖 
关系 都先/nr 不管 i . e . non autoregressive and 
no CRF 照样 秒杀 之前 的 SOTA 可见 其 表征 
学习 能力 之 强大 规模 很 重要 Scale matters One 
of our core claims is that the deep bidirectionality of 
BERT which is enabled by masked LM pre training is 
the single most important improvement of BERT compared to previous 
work . 这种 遮挡 mask 在 语言 模型 上 的 
应用 对 很多 人 来说 已经 不 新鲜 了 但 
确 是 BERT 的 作者 在 如此 超大规模 的 数据 
+ 模型 + 算 力 的 基础 上 验证 了 
其 强大 的 表征 学习 能力 这样 的 模型 甚至 
可以 延伸 到 很多 其他 的 模型 可能/v 之前/f 都被/nr 
不同/a 的/uj 实验室/n 提出/v 和/c 试验/vn 过/ug 只是 由于 规模 
的 局限 没能 充分 挖掘 这些 模型 的 潜力 而 
遗憾 地 让 它们 被 淹没 在 了 滚滚 的 
paper 洪流 之中 预 训练 价值 很大 Pre training is 
important We believe that this is the first work to 
demonstrate that scaling to extreme model sizes also leads to 
large improvements on very small scale tasks provided that the 
model has been sufficiently pre trained . 预 训练 已经 
被 广泛 应用 在 各个 领域 了 e . g 
. ImageNet for CV Word2Vec in NLP 多 是 通过 
大模型 大 数据 这样/r 的/uj 大模型/i 给/p 小规模/b 任务/n 能/v 
带来/v 的/uj 提升/v 有/v 几何/r 作者 也 给出 了 自己 
的 答案 BERT 模型 的 预 训练 是 用 Transformer 
做 的 但 我 想 换 做 LSTM 或者 GRU 
的话 应该 不会 有 太大 性能 上 的 差别 当然 
训练 计算 时的/nr 并行 能力 就 另当别论 了 对 BERT 
模型 的 观点 0 . high performance 的 原因 其实 
还是 归结 于 两点 除了 模型 的 改进 更 重要 
的 是 用了 超大 的 数据集 BooksCorpus 800M + English 
Wikipedia 2.5 G 单词 和 超大 的 算 力 对应 
于超 大模型 在 相关 的 任务 上 做 预 训练 
实现 了 在 目标 任务 上 表现 的 单调 增长 
1 . 这个 模型 的 双向 和 Elmo 不一样 大 
部分 人 对 他 这个 双向 在 novelty 上 的 
contribution 的 大小 有 误解 我 觉得 这个 细节 可能 
是 他 比 Elmo 显著 提升 的 原因 Elmo/w 是/v 
拼/v 一个/m 左到右/nr 和/c 一个/m 右/f 到/v 左/m 他 这个 
是 训练 中 直接 开 一个 窗口 用了 个 有 
顺序 的 cbow 2 .   可 复现 性 差 
有钱 才能 为所欲为 Reddit 对 跑 一次 BERT 的 价格 
讨论 For TPU pods 4 TPUs * ~ $ 2 
/ h preemptible * 24 h / day * 4 
days = $ 768 base model 16 TPUs = ~ 
$ 3k large model For TPU 16 tpus * $ 
8 / hr * 24 h / day * 4 
days = 12k 64 tpus * $ 8 / hr 
* 24 h / day * 4 days = 50k 
最后 他 问到 For GPU BERT Large is 24 layer 
1024 hidden and was trained for 40 epochs over a 
3.3 billion word corpus . So maybe 1 year to 
train on 8 P100s 然后 这个 就很 interesting 了 参考文献 
1 .   知乎 如何 评价 谷歌 最新 的 BERT 
模型 2 .   华尔街 见闻 NLP 历史 突破 3 
.   OPENAI Improving Language Understanding with Unsupervised Learning4 . 
  https / / gluebenchmark . com / leaderboard * 
排名 不 分 先后 收集 不全 欢迎 留言 完善 清华大学 
自然语言 处理 与 社会 人文 计算 实验室 http / / 
nlp . csai . tsinghua . edu . cn / 
site2 / 清华大学 智能 技术 与 系统 国家 重点 实验室 
信息检索 组 http / / www . thuir . cn 
/ cms / 北京大学 计算 语言学 教育部 重点 实验室 http 
/ / www . klcl . pku . edu . 
cn / 北京大学 计算机 科学 技术 研究所 语言 计算 与 
互联网 挖掘 研究室 http / / www . icst . 
pku . edu . cn / lcwm / index . 
PHP title = % E 9% A 6% 96% E 
9% A 1% B5 哈工大 社会 计算 与 信息检索 研究中心 
http / / ir . hit . edu . cn 
/ 哈工大 机器 智能 与 翻译 研究室 http / / 
www . contem . org / 哈尔滨工业大学 智能 技术 与 
自然 语言 处理 实验室 http / / www . insun 
. hit . edu . cn / home / 中科院计算所 
自然语言 处理 研究组 http / / nlp . ict . 
ac . cn / index _ zh . php 中科院 
自动化 研究所 语音 语言 技术 研究组 http / / nlpr 
web . ia . ac . cn / cip / 
introduction . htm 南京 大学 自然 语言 处理 研究组 http 
/ / nlp . nju . edu . cn / 
homepage / 复旦 大学 自然 语言 处理 研究组 http / 
/ nlp . fudan . edu . cn / 东北 
大学 自然 语言 处理 实验室 http / / www . 
nlplab . com / 厦门大学 智能科学 与 技术系 自然语言 处理 
实验室 http / / nlp . xmu . edu . 
cn / 苏州 大学 自然 语言 处理 实验室 http / 
/ nlp . suda . edu . cn / 苏州大学 
人类 语言 技术 研究所 http / / hlt . suda 
. edu . cn / 郑州 大学 自然 语言 处理 
实验室 http / / nlp . zzu . edu . 
cn / Huawei   Noah s Ark Labhttp / / 
www . noahlab . com . hkHuman Language Technology Center 
  at   Hong Kong University of Science & Technologyhttp 
/ / www . cse . ust . hk / 
~ hltc / NUS   Natural Language Processing Grouphttp / 
/ www . comp . nus . edu . sg 
/ ~ nlp / index . htmlThe   Stanford   
Natural Language Processing Grouphttp / / nlp . stanford . 
edu / The   Berkeley   NLP Grouphttp / / 
nlp . cs . berkeley . edu / index . 
shtmlNatural Language Processing research at   Columbia Universityhttp / / 
www1 . cs . columbia . edu / nlp / 
index . cgiNatural Language and Information Processing Research Group at 
  University of Cambridgehttp / / www . cl . 
cam . ac . uk / research / nl / 
Speech Research Group   at   University of Cambridgehttp / 
/ mi . eng . cam . ac . uk 
/ Main / Speech / The Language Technologies Institute LTI 
at   Carnegie Mellon Universityhttp / / www . lti 
. cs . cmu . edu / The Computational Linguistics 
Group at   Oxford Universityhttp / / www . clg 
. ox . ac . uk / Human Language Technology 
and Pattern Recognition Group   at the   RWTH Aachenhttps 
/ / www i6 . informatik . rwth aachen . 
de / Algorithms for Computational Linguistics at   City University 
of New Yorkhttp / / acl . cs . qc 
. edu / Algorithms for Computational Linguistics at   Oregon 
State Universityhttp / / web . engr . oregonstate . 
edu / ~ huanlian / RPI   Blender Labhttp / 
/ nlp . cs . rpi . edu / The 
Natural Language Group at   USC / ISIhttp / / 
nlg . isi . edu / Natural Language Processing Group 
at   University of Notre Damehttp / / nlp . 
nd . edu / Artificial Intelligence Research Group at   
Harvardhttp / / www . eecs . harvard . edu 
/ ai / Natural Language Processing Research at   Googlehttps 
/ / research . google . com / pubs / 
N a t u r a l L a n 
g u a g e P r o c e 
s s i n g . htmlThe   Redmond based 
Natural Language Processing grouphttp / / research . microsoft . 
com / en us / groups / nlp / Computational 
Linguistics and Information Processing at   Marylandhttps / / wiki 
. umiacs . umd . edu / clip / index 
. php / Main _ PageLanguage and Speech Processing at 
  Johns Hopkins Universityhttp / / www . clsp . 
jhu . edu / about clsp / Human Language Technology 
Center of Excellence at   Johns Hopkins Universityhttp / / 
hltcoe . jhu . edu / Statistical Machine Translation Group 
at the   University of Edinburghhttp / / www . 
statmt . org / ued / n = Public . 
H o m e P a g e U n 
i v e r s i t y of Sheffield 
  NLP Grouphttp / / nlp . shef . ac 
. uk / index . htmlThe   CNGL Centre   
for Global Intelligent Contenthttps / / www . cngl . 
ie / Cornell   NLP grouphttps / / confluence . 
cornell . edu / display / NLP / Home / 
Natural Language Processing NLP group   at   University Of 
Washingtonhttps / / www . cs . washington . edu 
/ research / nlpNLP @   Illinoishttp / / nlp 
. cs . illinois . edu / 原文 连接 http 
/ / blog . csdn . net / wangxinginnlp / 
article / details / 448905531 . 总述 人类 对 机器 
理解 语言 的 认识 走了 一条 大 弯路 早期 的 
研究 集中 采用 基于 规则 的 方法 虽然 解决 了 
一些 简单 的 问题 但是 无法 从 根本 上 将 
自然 语言 理解 实用化 直到 20多 年后 人们 开始 尝试 
用 基于 统计 的 方法 进行 自然语言 处理 才有 了 
突破性 的 进展 和 实用 的 产品 2 . 前文 
回顾 上 一篇 讲到 语言 的 出现 是 为了 人类 
之间 的 通信 字母 或者 中文 的 笔画 文字 和 
数字 实际上 是 信息 编码 的 不同 单位 任何 一种 
语言 都是/nr 一种 编码 的 方式 而 语言 的 语法 
规则 是 编码 的 算法 我们 把 一个 要 表达 
的 意思 通过 某种 语言 的 一句话 表达出来 就是 用 
这种 语言 的 编码 方式 对 头脑 中 的 信息 
做 了 一次 编码 编码 的 结果 就是 一串 文字 
而 如果 对方 懂得 这门 语言 他 或者 她 就 
可以 用 这门 语言 的 解码 方法 获得 说话 人 
要 表达 的 信息 这 就是 语言 的 数学 本质 
虽然 动物 也 能 做到 传递信息 但是 利用 语言 来 
传递 信息 是 人类 的 特质 编码       
                    
                    
解码 信息 信息源 信息 信道 信息 接收者 3 . 两个 
问题 a . 计算机 能否 处理 自然语言 b . 如果能 
那么 它 处理 自然 语言 的 方法 是否 和 人类 
一致 对 这 两个 问题 答案 都是/nr 肯定 的 4 
. 机器 智能 a . 图灵测试 Turing Test 让人 和 
机器 进行 交流 如果 人 无法 判断 自己 交流 的 
对象 是 人 还是 机器 就 说明 这个 机器 有 
智能 了 b . 弯路 阶段 从 20 世纪 50 
年代 到 70 年代 是 科学家 们 走弯路 的 阶段 
全/a 世界/n 的/uj 科学家/n 对/p 计算机/n 处理/v 语言/n 的/uj 认识/v 
都/d 局限/n 在/p 人类/n 学习/v 语言/n 的/uj 方式/n 上/f 也 
就是说 用 电脑 模拟 人脑 鸟 飞 派 这 20 
多年 的 成果 近乎 为零 c . 第二阶段 直到 20 
世纪 70 年代 一些 自然 语言 处理 的 先驱 开始 
重新 认识 这个 问题 找到 了 基于 数学模型 和 统计 
的 方法 自然语言 处理 进入 第二个 阶段 30 多年来 这个 
领域 取得 了 实质性 的 突破 自然语言 处理 也 在 
很多 产品 中 得到 广泛 应用 今天 机器 翻译 和 
语音 识别 已经 做 得 不错 并且有 上亿 人 使用 
过 但是 这个 领域 之外 的 大部分 人 已然 错误 
地 以为 这 两种 应用 是 靠近 计算机 理解 了 
自然 语言 才 实现 的 事实上 它们 全都 靠 的 
是 数学 更 准确 地 说是 靠 统计学 5 . 
理解 自然语言 a . 分析 语句 和 获取 语义 应用层 
  语音识别   机器翻译   自动 回答   自动 摘要 
认知 层   自然语言 理解 基础层   句法分析   语义分析 
b . 从 规则 到 统计 在 上个世纪 70 年代 
基于 规则 的 句法分析 包括 文 法分析 或者 语义分析 很快 
走到 了 尽头 而 对于 语义 的 处理 则 遇到 
了 更大 的 麻烦 首先 自然语言 中词 的 多义性 很难 
用 规则 来 描述 而是 严重 依赖 于 上下文 甚至 
是 常识 第二点 也很 有意思 用 基于 统计 的 方法 
代替 传统 的 方法 需要 等 原有 的 一批 语言学家 
退休 1970年 以后 统计 语言学家 的 出现 使得 自然语言 处理 
重获 新生 并 取得 了 今天 非凡 的 成就 推动 
这个 技术 路线 转变 的 关键 人物 是 贾里 尼克 
和他/nr 领导 的 IBM 华生 实验室 最初 他们 也 没有 
想 解决 整个 自然语言 处理 的 各种 问题 而 只是 
希望 解决 语音 识别 的 问题 采用 基于 统计 的 
方法 IBM 将 当时 的 语音 识别 率 从 70% 
提升到 90% 同时 语音 识别 的 规模 从 几百 单词 
上升 到 几万 单词 这样 语音识别 就 有了 从 实验室 
走向 实际 应用 的 可能 6 . 小结 基于 统计 
的 自然 语言 处理 方法 在 数学 模型 和 通信 
是 相通 的 甚至 是 相同 的 因此 在 数学 
意义 上 自然 语言 处理 又和 语言 的 初衷 通信 
联系 在 一起 了 但是 科学家 们 用了 几十年 才 
认识 到 这个 联系 1 . 假设 句子 按 单词 
顺序 为 w1 w2 . . . wn 那么 这个 
句子 的 概率 公式 为 句子 在 语料库 中 出现 
的 概率 P S = P w1 w2 w3 . 
. . wn 根据 条件概率 公式 P w1 w2 w3 
. . . wn = P w1 * P w2 
| w1 * p w3 | w1 w2 . . 
. P wn | w1 w2 . . . w 
n 1 2 . 是否 句子 越长 概率 就 越低 
为什么 3 . 一个 语言 模型 的 困惑 度 是 
怎么 计算 的 是 什么 意义 理论 方法 迷惑 度 
/ 困惑 度 / 混乱 度 preplexity 其 基本 思想 
是 给 测试 集 的 句子 赋予 较高 概率值 的 
语言 模型 较好 当 语言 模型 训练 完 之后 测试 
集中 的 句子 都是 正常 的 句子 那么 训 练好 
的 模型 就是 在 测试 集上 的 概率 越高 越好 
公式 如下 由 公式 可知 迷惑 度 越小 句子 概率 
越大 语言 模型 越好 4 . 神经 网络 的 语言 
模型 相对 N Gram 模型 有 哪些 改进 的 地方 
实际 的 应用 情况如何 5 . Word2Vec 中 skip gram 
cbow 两者 比较 的 优缺点 是 哪些 6 . HMM 
CRF 两者 比较 的 优缺点 是 哪些 7 . Blue 
评价 指标 是 干嘛 用 的 他 考虑 了 哪些 
因素 缺点 是 什么 8 . 做一个 翻译 模型 如果 
输出 的 词典 很大 例 如有 100 万个 词 要 
怎么 解决 这个 计算 量 问题 9 . 什么 是 
交叉 熵 和/c KL/w 距离/n 有/v 什么/r 关系/n 10 . 
sgd momentum adagrad adam 这些 优化 算法 之间 的 关系 
和 区别 是 怎样 的 分别 适用 于 什么 场景 
11 . 理论上 两层 的 神经 网络 可以 拟合 任意 
函数 为什么 现在 大多数 是 用 多层 的 神经 网络 
12 . 生成 模型 和 判别 模型 两者 差别 是 
啥 分别 适用 于 什么 场景 13 . AUC 的 
评估 指标 是 怎么 定义 的 如果 计算 的 AUC 
结果 0.5 主要 是 什么 原因 导致 的 14 . 
逻辑 回归 和 线性 回归 的 区别 是 啥 适用 
场景 分别 是 15 . 编码 实现 softmax 在 自然 
语言 处理 中 经常 要 计算 单词 序列 句子 出现 
的 概率 估计 但是 算法 训练 的 时候 预料 库 
中 不 可能 包含 所有 可能 出现 的 序列 因此 
为了 防止 对 训练 样本 中 为 出现 的 新 
序列 概率 估计值 为零 人们 发明 了 不少 可以 改善 
估 计新 序列 出现 的 概率 算法 即 数据 的 
平滑 最 常见 的 数据 平滑 算法 包括 如下 几种 
Add one Laplace smoothingAdd k smoothingBackoff 回退 法 Interpolation 插值法 
Absolute d i s c o u n t i 
n g K n e s e r Ney s 
m o o t h i n g M o 
d i f i e d Kneser ney smoothing 这 
几个 方法 实际上 可以 简单 的 理解 为 三种 不同 
的 方法 第 一种 类型 为 政府 给 大家 每人 
一笔 或者 几笔 钱 如 1 和2/nr 第二种 为 找 
父母 要 如 3 和4/nr 最后 一种 就是 劫富济贫 如 
5 7 下面 依次 简单 介绍 上面 的 方法 具体 
详细 的 介绍 大家 可以 参阅 相应 的 论文 和 
书籍 数据 预处理 在 介绍 上面 几种 平滑 的 方法 
之前 这里 先给 出 一个 简单 的 的 数据 预处理 
的 方法 特别 是 对于 OOV 需要 训练 的 词 
不在 词 袋 里面 的 情况 特别 有效 而且 如果 
训练 的 时候 如果 有 几十万 的 词汇 一般 不会 
对这 几十万 的 词汇 进行 全部 训练 而是/c 需要/v 预先/vn 
做/v 下面/f 的/uj 处理/v 后再/nr 进行/v 数据/n 的/uj 平滑/a 和/c 
训练/vn 假设 训练 数据 集中 出现 了 | N | 
个 不同 的 词汇 那么 可以 根据 词频 对 这些 
词汇 进行 排序 可以 选择 词频 最高 的 M 个 
词汇 作为 我们 的 词汇 集合 这样 在 训练 和 
测试 数据 集中 将 不属于 V 的 词汇 都 替换成 
特殊 的 词汇 UNK 这样 可以 大大 减少 计算 量 
也 可以 提高 计算 的 精度 Add one Laplace smoothingAdd 
one 是 最简单 最 直观 的 一种 平滑 算法 既然 
希望 没有 出现 过 的 N gram 的 概率 不再 
是 0 那就 直接 规定 在 训练 时 任何 一个 
N gram 在 训练 预料 至少 出现 一次 即 规定 
没有 出现 的 在 语 料中 也 出现 一次 因此 
Countnew n gram = countold n gram + 1 于是 
对于 n gram 的 模型 而言 假设 V 是 所有 
可能 的 不同 的 N gram 的 类型 个数 那么 
根据 贝叶斯 公式 有 当然 这里 的 n gram 的 
可以 相应 的 改成 uingram 和 bigram 表达式 并不 影响 
其中 C x 为 x 在 训练 中 出现 的 
次数 wi 为 给定 的 训练 数据 中 第 i 
个 单词 这样一来 训练 语料库 中 出现 的 n gram 
的 概率 不再 为 0 而是 一个 大 于0的/nr 较小 
的 概率值 Add one 平滑 算法 确实 解决 了 我们 
的 问题 但是 显然 它 也 并不 完美 由于 训练 
语 料中 未 出现 的 n gram 数量 太多 平滑 
后 所有 未 出现 的 占据 了 整个 概率分布 的 
一个 很大 的 比例 因此 在 自然 语言 处理 中 
Add one 给 语料库 中 没有 出现 的 n gram 
分配 了 太多 的 概率 空间 此外 所有 没有 出现 
的 概率 相等 是不是 合理 这也 是 需要 考虑 的 
Add k smoothing 由 Add one 衍生 出来 的 另一种 
算法 就是 Add k 既然 我们 认为 加 1 有点 
过了 那么 我们 可以 选择 一个 小 于1的/nr 正数 k 
概率 计算公式 就 可以 变成 如下 表达式 它 的 效果 
通常 会比 Add one 好 但是 依旧 没有 办法 解决 
问题 至少 在 实践 中 k 必须 认为 的 给定 
而 这个 值 到底 多少 该 取 多少 都 没有 
办法 确定 Backoff 回退 法 回退 模型 思路 实际上 是 
如果 你 自己 有钱 那么 就 自己 出钱 如果 你 
自己 没有 钱 那么 就 你 爸爸 出 如果 你 
爸爸 没有钱 就 你 爷爷 出 举 一个 例子 当 
使用 Trigram 的 时候 如果 Count trigram 满足 条件 就 
使用 否则 使用 Bigram 再 不然 就 使用 Unigram . 
它 也 被 称为 Katz smoothing 具体 的 可以 去 
查看 相应 的 书籍 它 的 表达式 为 其中 d 
a 和k/nr 分别为 参数 k 一般 选择 为 0 但是 
也 可以 选 其它 的 值 Interpolation/w 插值法/n 插值法/n 和/c 
回退/v 法的/nr 思想/n 非常/d 相似/v 设想 对于 一个 trigram 的 
模型 我们 要 统计 语料库 中 I like chinese food 
出现 的 次数 结果 发现 它 没有 出现 则 计数 
为 0 在 回退 策略 中 们 将会 试着 用 
低阶 的 gram 来 进行 替代 也 就是 用 like 
chinese food 出现 的 次数 来 替代 在 使用 插值 
的 时候 我们 把 不同 阶层 的 n gram 的 
模型 线性 叠加 组合 起来 之后 再 使用 简单 的 
如 trigram 的 模型 按照 如下 的 方式 进行 叠加 
参数 可以 凭借 经验 进行 设定 也 可以 通过 特定 
的 算法 来 进行 确定 比如 EM 算法 对于 数据 
一般 可以 分为 traning set development set testing set . 
那么 P 的 概率 使用 training set 进行 训练 得出 
lamda 参数 使用 development set 得到 Absolute discounting 插值法 使用 
的 参数 实际上 没有 特定 的 选择 如果 将 lamda 
参数 根据 上下文 进行 选择 的话 就 会演 变成 Absolute 
discounting 对于 这个 算法 的 基本 想法 是 有钱 的 
每个人 交 固定 的 税 D 建立 一个 基金 没有钱/i 
的/uj 根据/p 自己/r 的/uj 爸爸/n 有/v 多少/m 钱分/nr 这个/r 基金/n 
比如 对于 bigram 的 模型 来说 有 如下 公式 D 
为 参数 可以 通过 测试 优化 设定 Kneser Ney smoothing 
这种 算法 是 目前 一种 标准 的 而且 是 非常 
先进 的 平滑 算法 它 其实 相当于 前面 讲过 的 
几种 算法 的 综合 它 的 思想 实际上 是 有钱 
的 人 每个人 交 一个 固定 的 税 D 大家 
一起 建立 一个 基金 没有钱 的 呢 根据 自己 的 
的 爸爸 的 交际 的 广泛 的 程度 来 分 
了 这个 基金 这里 交际 的 广泛 实际上 是 指 
它 爸爸 会 有 多少 种 不同 的 类型 类型 
越多 这说明 越好 其 定义 式 为 其中 max c 
X D 0 的 意思 是 要 保证 最后 的 
计数 在 减去 一个 D 后 不会 变成 一个 负数 
D 一般 大于 0 小于 1 这个 公式 递归 的 
进行 直到 对于 Unigram 的 时候 停止 而 lamda 是 
一个 正则化 的 常量 用于 分配 之前 的 概率值 也 
就是 从 高频 词汇 中 减去 的 准备 分配给 哪些 
未 出现 的 低频词 的 概率值 分 基 金池 里面 
的 基金 其 表达 是 为 PKN 是 在 wi 
固定 的 情况 下 unigram 和 bigram 数目 的 比值 
这里 需要 注意 的 是 PKN 是 一个 分布 它 
是 一个 非 负 的 值 求和 的话 为 1 
Modified Kneser ney smoothing 这 一种 方法 是 上 一种 
方法 的 改进 版 而且 也 是 现在 最优 的 
方法 上 一个 方法 每 一个 有钱 的 人都 交 
一个 固定 的 锐 这个 必然 会 出现 问题 就像 
国家 收税 一样 你/r 有/v 100/m 万和/nz 你/r 有/v 1个/mq 
亿/m 交税/n 的/uj 量/n 肯定/v 不/d 一样/r 这样/r 才/d 是/v 
比较/d 合理/vn 的/uj 因此 将 上 一种 方法 改进 就是 
有钱 的 每个 人 根据 自己 的 收入 不同 交 
不同 的 税 D 建立 一个 基金 没有钱 的 根据 
自己 的 爸爸 交际 的 广泛 程度 来 分配 基金 
这里 D 根据 c 来 设定 不同 的 值 比如 
c 为 unigram 则 使用 D1 c 位 bigram 则 
使用 D2 如果 是 大于 等于 3 阶 的 使用 
D3 . 转 自 微信 公众 号 自然语言 处理 技术 
参考 书籍 1 Speech and language processing Daniel Jurafsky et 
la . 2 语音识别 实践 俞栋/nr 等人 自然语言 处理 研究 
的 内容 包括 但 不限 于 如下 分支 领域 文本 
分类 信息 抽取 自动 摘要 智能 问答 话题 推荐 机器翻译 
主题词 识别 知识库 构建 深度 文本 表示 命名 实体 识别 
文本 生成 文本 分析 词法 句法 语法 语音 识别 与 
合成 等 下面 给 出 一些 分支 领域 的 详细 
介绍 文本 分类 文本 分类 用 计算机 设备 对 文本 
集 或 其他 实体 或 物件 按照 一定 的 分类 
体系 或 标准 进行 自动 分类 标记 定义 基于 分类 
体系 的 自动 分类 基于 资讯 过滤 和 用户 兴趣 
Profiles 的 自动 分类 所谓 分类 体系 就是 针对 词 
的 统计 来 分类 关键字 分类 现在 的 全文 检索 
词 的 正确 切分 不易 分辨 白痴 造句法 学习/v 人类/n 
对/p 文本/n 分类/n 的/uj 知识/v 和/c 策略/n 从人对/nr 文本/n 和/c 
类别/n 之间/f 相关性/l 判断/v 来/v 学习/v 文件/n 用字/n 和/c 标记/n 
类别/n 之间/f 的/uj 关联/ns 过程/n 文本/n 分类/n 一般/a 包括/v 了/ul 
文本/n 的/uj 表达/v 分类器 的 选择 与 训练 分类 结果 
的 评价 与 反馈 等 过程 其中 文本 的 表达 
又可 细分 为 文本 预处理 索引 和 统计 特征 抽取 
等 步骤 文本 分类 系统 的 总体 功能模块 为 1 
预处理 将 原始 语料 格式 化为 同一 格式 便于 后续 
的 统一 处理 2 索引 将 文档 分解 为 基本 
处理单元 同时 降低 后续 处理 的 开销 3 统计 词频 
统计 项 单词 概念 与 分类 的 相关 概率 4 
特征 抽取 从 文档 中 抽取 出 反映 文档 主题 
的 特征 5 分类器 分类器 的 训练 6 评价 分类器 
的 测试 结果 分析 方法 ※ 词 匹 配法 ※ 
知识工程 ※ 统计 学习 ※ 分类 算法 现如今 统计 学习 
方法 已经 成为 了 文本 分类 领域 绝对 的 主流 
主要 的 原因 在于 其中 的 很多 技术 拥有 坚实 
的 理论 基础 相比之下 知识工程 方法 中 专家 的 主观 
因素 居多 存在 明确 的 评价 标准 以及 实际 表现 
良好 统计 分类 算法 将 样本数据 成功 转化 为 向量 
表示 之后 计算机 才算 开始 真正 意义 上 的 学习 
过程 常用 的 分类 算法 为 决策树 Rocchio 朴素 贝叶斯 
神经网络 支持 向量 机 线性 最小 平方 拟合 kNN 遗传算法 
最大熵 Generalized Instance Set 等 基于 资讯 过滤 和 用户 
兴趣 Profiles 的 自动 分类 所谓 分类 体系 就是 针对 
词 的 统计 来 分类 关键字 分类 现在 的 全文 
检索 词 的 正确 切分 不易 分辨 白痴 造句法 学习/v 
人类/n 对/p 文本/n 分类/n 的/uj 知识/v 和/c 策略/n 从人对/nr 文本/n 
和/c 类别/n 之间/f 相关性/l 判断/v 来/v 学习/v 文件/n 用字/n 和/c 
标记/n 类别/n 之间/f 的/uj 关联/ns 信息/n 抽取/v 信息/n 抽取/v Information 
Extraction IE 是 把 文 本里 包含 的 信息 进行 
结构 化 处理 变成 表格 一样 的 组织 形式 输入 
信息 抽取 系统 的 是 原始 文本 输出 的 是 
固定 格式 的 信息 点 信息点 从 各种各样 的 文档 
中被 抽 取出来 然后 以 统一 的 形式 集成 在 
一起 这 就是 信息 抽取 的 主要 任务 信息 以 
统一 的 形式 集成 在 一起 的 好处 是 方便 
检查 和 比较 信息 抽取 技术 并不 试图 全面 理解 
整篇 文档 只是 对 文档 中 包含 相关 信息 的 
部分 进行 分析 至于 哪些 信息 是 相关 的 那将 
由 系统 设计 时 定下 的 领域 范围 而定 简介 
信息 抽取 技术 对于 从 大量 的 文档 中 抽取 
需要 的 特定 事实 来说 是 非常 有用 的 互联 
网上 就 存在 着 这么 一个 文档 库 在 网上 
同一 主题 的 信息 通常 分散 存放 在 不同 网站 
上 表现 的 形式 也 各不相同 若能 将 这些 信息 
收集 在 一起 用 结构化 形式 储存 那将 是 有益 
的 由于 网上 的 信息 载体 主要 是 文本 所以 
信息 抽取 技术 对于 那些 把 因特网 当成 是 知识 
来源 的 人 来说 是 至关重要 的 信息/n 抽取/v 系统/n 
可以/c 看作/v 是/v 把/p 信息/n 从/p 不同/a 文档/n 中/f 转换/v 
成/n 数据库/n 记录/n 的/uj 系统/n 因此 成功 的 信息 抽取 
系统 将 把 互联网 变成 巨大 的 数据库 挑战 信息 
抽取 技术 是 近 十年 来 发展 起来 的 新领域 
遇到 许多 新的 挑战 信息 抽取 原来 的 目标 是 
从 自然语言 文档 中 找到 特定 的 信息 是 自然 
语言 处理 领域 特别 有用 的 一个 子 领域 所 
开发 的 信息 抽取 系统 既能 处理 含有 表格 信息 
的 结构化 文本 又能 处理 自由式 文本 如 新闻 报道 
IE 系统 中 的 关键 组成部分 是 一系列 的 抽取 
规则 或 模式 其 作用 是 确定 需要 抽取 的 
信息 网上 文本 信息 的 大量 增加 导致 这 方面 
的 研究 得到 高度 重视 纯 文本 抽出 通用 程序库 
DMCTextFilter V 4.2 是 HYFsoft 推出 的 纯 文本 抽出 
通用 程序库 DMCTextFilter/w 可以/c 从/p 各种各样/l 的/uj 文档/n 格式/n 的/uj 
数据/n 中/f 或/c 从/p 插入/v 的/uj OLE/w 对象/n 中/f 完全 
除掉 特殊 控制 信息 快速 抽出 纯 文本 数据 信息 
便于 用户 实现 对 多种 文档 数据 资源 信息 进行 
统一 管理 编辑 检索 和 浏览 DMCTextFilter 采用 了 先进 
的 多语言 多平台 多线程 的 设计 理念 支持 多 国 
语言 英语 中文 简体 中文 繁体 日本语 韩国语 多种 操作系统 
Windows Solaris Linux IBM AIX Macintosh HP UNIX 多种 文字 
集合 代码 GBK GB18030 Big5 ISO 8859 1 KS X 
1001 Shift _ JIS WINDOWS31J EUC JP ISO 10646 UCS 
2 ISO 10646 UCS 4 UTF 16 UTF 8 等 
提供 了 多种 形式 的 API 功能 接口 文件格式 识别 
函数 文本 抽出 函数 文件属性 抽出 函数 页 抽出 函数 
设定 User Password 的 PDF 文件 的 文本 抽出 函 
数等 便于 用户 方便使用 用户 可以 十分 便利 的 将 
本 产品 组装 到 自己 的 应用 程序 中 进行 
二次开发 通过 调用 本 产品 的 提供 的 API 功能 
接口 实现 从 多种 文档 格式 的 数据 中 快速 
抽出 纯 文本 数据 文件格式 自动 识别 功能 本 产品 
通过 解析 文件 内部 的 信息 自动识别 生成 文件 的 
应用 程序名 和其/nr 版本号 不 依赖 于 文件 的 扩展名 
能够 正确 识别 文件格式 和 相应 的 版本信息 可以 识别 
的 文件 格式 如下 支持 Microsoft Office RTF PDF Visio 
OutlookEML 和 MSG Lotus1 2 3 HTML AutoCAD DXF 和 
DWG IGES PageMaker ClarisWorks AppleWorks XML WordPerfect Mac Write Works 
C o r e l P r e s e 
n t a t i o n s QuarkXpress DocuWorks 
WPS 压缩文件 的 LZH / ZIP / RAR 以及 一 
太郎 OASYS 等 文件格式 文本 抽出 功能 即使 系统 中 
没有 安装 作成 文件 的 应用 程序 可以 从 指定 
的 文件 或 插入 到 文件 中的 OLE 中 抽出 
文本 数据 文件属性 抽出 功 能从 指定 的 文件 中 
抽出 文件属性 信息 页 抽出 功能 从文件 中 抽出 指定 
页 中 文本 数据 对 加密 的 PDF 文件 文本 
抽出 功 能从 设有 打开文档 口令 密码 的 PDF 文件 
中 抽出 文本 数据 流 Stream 抽出 功 能从 指定 
的 文件 或是 嵌入 到 文件 中的 OLE 对象 中 
向流里/nr 抽取 文本 数据 支持 的 语言 种类 本 产品 
支持 以下 语言 英语 中文 简体 中文 繁体 日本语 韩国语 
支持 的 字符 集合 的 种类 抽出 文本 时 可以 
指定 以下 的 字符 集合 作为 文本文件 的 字符集 也可 
指定 任意 特殊 字符集 但 需要 另行 定制 开发 GBK 
GB18030 Big5 ISO 8859 1 KS X 1001 Shift _ 
JIS WINDOWS31J EUC JP ISO 10646 UCS 2 ISO 10646 
UCS 4 UTF 16 UTF 8 等 自动 文本 摘要 
自动 文本 摘要 是 利用 计算机 自动 地 从 原始 
文献 中 提取 文摘 文摘 是 全面 准确 地 反映 
某一 文献 中心 内容 地 简单 连贯 的 短文 常用 
方法 是 自动 摘 要将 文本 作为 句子 的 线性 
序列 将 句子 视为 词 的 线性 序列 类型 技术 
应用 类型 自动 提取 给定 文章 的 摘要 信息 自动 
计算 文章 中词 的 权重 自动 计算 文章 中 句子 
的 权重 提取 单篇/nr 文章 的 摘要 自动 提取 大 
规模 文档 的 摘要 自动 提取 基于 分类 的 摘要 
自动 提取 智能 问答 系统 智能 问答 系统 以 一问一答 
形式 精确 的 定位 网站 用户 所 需要 的 提问 
知识 通过 与 网站 用户 进行 交互 为 网站 用户 
提供 个性化 的 信息 服务 介绍 智能 问答 系统 是 
将 积累 的 无序 语料 信息 进行 有序 和 科学 
的 整理 并 建立 基于 知识 的 分类 模型 这些 
分类 模型 可以 指导 新 增加 的 语料 咨询 和 
服务 信息 节约 人力 资源 提高 信息 处理 的 自动 
性 降低 网站 运行 成本 基于 对 网站 多年 积累 
的 关于 政府 和 企业 的 基本 情况 常见问题 及其 
解答 整理 为 规范 的 问答 库 形式 以 支撑 
各种 形式 问题 的 智能 问答 方便 了 用户 提高 
了 办事 效率 提升 了 企业 形象 应用 场景 相关 
问答 推送 当 网站 用户 提出 问题 时 系统 不仅 
将 问题 答案 推 送出来 而且会 将与 这个 问题 相关 
的 知识 也都 推送 出来 供 用户 查询 这样 就 
做到 了 一次 提问 全面 掌握 所有 信息 提问 智能 
提示 用户 在 提问 的 过程 中 系统 将 已经 
输入 的 内容 自动 分析 给予 优化 的 补全 或 
相关 提示 焦点 问题 自动 排行 对 在 一定 的 
时间 内 用户 对 知识 提问 的 热度 系统 自动 
聚焦 并 按照 访问 频度 将 热点 知识 集中 在 
系统 页面 上 显示 具体 类别 的 知识 也 按照 
访问 频度 排序 在 页面 知识 类别 栏目 中 显示 
热点 词 聚焦 系统 对 用户 提交 的 业务 关键词 
进行 统计 并 按照 访问 的 频度 进行 聚焦 将与 
关键词 相关 的 业务 列表 自动 链接 形成 业务 热点 
关键词 在线客服 问答 模拟 在线 客服 人员 以 网站 智能 
客服 形式 完成 客服 作用 引导式 交互 客服 服务 将 
常见问题 整理 成 若干 流程 诊断 型 的 知识 通过 
引导 交互式 地 服务 尽量 从 Web 端 解决 客户 
常见问题 客服 座席 协助 完成 专家 坐席 功能 在 普通 
坐席 人员 无法 回答 问题 时 提供 标准化 的 知识 
协助 帮助 普通 客服 人员 快速 准确 回答 转 人工 
客服 用户 可以 直接 在 智能 咨询 服务 系统 中 
连接 人工 客服 人员 向 客服 人员 进行 在线 咨询 
话题 推荐 话题 推荐 只是 推荐 系统 的 一个 小小的 
应用 分支 下面 主要 通过 介绍 推荐 系统 来 了解 
话题 推荐 的 大致 内容 背景 现在 社会 的 信息 
过载 为了 更好 的 对 过载 的 信息 进行 有效 
的 过滤 推荐/v 系统/n 推荐/v 系统/n 是/v 利用/n 电子/n 商务/n 
网站/n 向/p 客户/n 提供/v 商品/n 信息/n 和/c 建议/n 帮助 用户 
决定 应该 购买 什么 产品 模拟 销售 人员 帮助 客户 
完成 购买 过程 个性化 推荐 是 根据 用户 的 兴趣 
特点 和 购买 行为 向/p 用户/n 推荐/v 用户/n 感兴趣/n 的/uj 
信息/n 和/c 商品/n 现在 的 应用 领域 更为 广泛 比 
如今日 头条 的 新闻 推荐 购物 平台 的 商品 推荐 
直播 平台 的 主播 推荐 知乎 上 的 话题 推荐 
等等 定义 推荐 系统 有 3个 重要 的 模块 用户 
建模 模块 推荐 对象 建模 模块 推荐算法 模块 推荐 系统 
把 用户 模型 中 兴趣 需求 信息 和 推荐 对象 
模型 中 的 特征 信息 匹配 同时 使用 相应 的 
推荐 算法 进行 计算 筛选 找到 用户 可能 感兴趣 的 
推荐 对象 然后 推荐 给 用户 常用 推荐 方法 基于 
内容 推荐 基于 内容 的 推荐 Content based Recommendation 是 
信息 过滤 技术 的 延续 与 发展 它 是 建立 
在 项目 的 内容 信息 上 作出 推荐 的 而 
不 需要 依据 用户 对 项目 的 评价 意见 更多 
地 需要 用 机器 学习 的 方法 从 关于 内容 
的 特征 描述 的 事例 中 得到 用户 的 兴趣 
资料 在 基于 内容 的 推荐 系统 中 项目 或 
对象 是 通过 相关 的 特征 的 属性 来 定义 
系统 基于 用户 评价 对象 的 特征 学习 用户 的 
兴趣 考察 用户 资料 与 待 预测 项目 的 相匹配 
程度 用户 的 资料 模型 取决于 所用 学习 方法 常用 
的 有 决策树 神经 网络 和 基于 向量 的 表示 
方法 等 基于 内容 的 用户 资料 是 需要 有 
用户 的 历史 数据 用户 资料 模型 可能 随着 用户 
的 偏好 改变 而 发生 变化 基于 内容 推荐 方法 
的 优点 是 1 不 需要 其它 用户 的 数据 
没有/v 冷/a 开始/v 问题/n 和/c 稀疏/a 问题/n 2 能为 具有 
特殊 兴趣 爱好 的 用户 进行 推荐 3 能 推荐 
新的 或 不是 很 流行 的 项目 没有 新 项目 
问题 4 通过 列出 推荐 项目 的 内容 特征 可以 
解释 为什么 推荐 那些 项目 5 已有 比较 好 的 
技术 如 关于 分类 学习 方面 的 技术 已 相当 
成熟 缺点/n 是/v 要求/v 内容/n 能/v 容易/a 抽取/v 成有/nr 意义/n 
的/uj 特征/n 要求 特征 内容 有 良好 的 结构性 并且 
用户 的 口味 必须 能够 用 内容 特征 形式 来 
表达 不能 显 式 地 得到 其它 用户 的 判断 
情况 基于 用户 的 系统 过滤 推荐 过程 协同 过滤 
推荐 Collaborative Filtering Recommendation 技术 是 推荐 系统 中 应用 
最早 和 最为 成功 的 技术 之一 它 一般 采用 
最 近邻 技术 利用 用户 的 历史 喜好 信息 计算 
用户 之间 的 距离 然后 利用 目标 用户 的 最近 
邻居 用户 对 商品 评价 的 加权 评 价值 来 
预测 目标 用户 对 特定 商品 的 喜好 程度 系统 
从而/nr 根据 这一 喜好 程度 来 对 目标 用户 进行 
推荐 协同 过滤 最大 优 点 是 对 推荐 对象 
没有 特殊 的 要求 能 处理 非 结构化 的 复杂 
对象 如 音乐 电影 协同 过滤 是 基于 这样 的 
假设 为 一 用户 找到 他 真正 感兴趣 的 内容 
的 好 方法 是 首先 找到 与此 用户 有 相似 
兴趣 的 其他 用户 然后 将 他们 感兴趣 的 内容 
推荐给 此 用户 其 基本 思想 非常 易于 理解 在 
日常 生活 中 我们 往往 会 利用 好 朋友 的 
推荐 来 进行 一些 选择 协同 过滤 正是 把 这一 
思想 运用 到 电子 商务 推荐 系统 中 来 基于 
其他 用户 对 某一 内容 的 评价 来向 目标 用户 
进行 推荐 基于 协同 过滤 的 推荐 系统 可以 说 
是从 用户 的 角度 来 进行 相应 推荐 的 而且 
是 自动 的 即 用户 获得 的 推荐 是 系统 
从 购买 模式 或 浏览 行为 等 隐式 获得 的 
不 需要 用户 努力 地 找到 适合 自己 兴趣 的 
推荐 信息 如 填写 一些 调查 表格 等 和 基于 
内容 的 过滤 方法 相比 协同 过滤 具有 如下 的 
优点 1 能够 过滤 难以 进行 机器 自动 内容 分析 
的 信息 如 艺术品 音乐 等 2 共享 其他人 的 
经验 避免 了 内容 分析 的 不 完全 和 不精确 
并且 能够 基于 一些 复杂 的 难以 表述 的 概念 
如 信息 质量 个人 品味 进行 过滤 3 有 推荐 
新 信息 的 能力 可以 发现 内容 上 完全 不 
相似 的 信息 用户 对 推荐 信息 的 内容 事先 
是 预料 不到 的 这 也是 协同 过滤 和 基于 
内容 的 过滤 一个 较大 的 差别 基于 内容 的 
过滤 推荐 很多 都是/nr 用户 本来 就 熟悉 的 内容 
而 协同 过滤 可以 发现 用户 潜在 的 但 自己 
尚未 发现 的 兴趣 偏好 4 能够 有效 的 使用 
其他 相似 用户 的 反馈 信息 较少 用户 的 反馈 
量 加快 个性化 学习 的 速度 虽然 协同 过滤 作为 
一种 典型 的 推荐 技术 有其 相当 的 应用 但 
协同 过滤 仍 有 许多 的 问题 需要 解决 最 
典型 的 问题 有 稀疏 问题 Sparsity 和 可扩展 问题 
Scalability 基于 关联 规则 推荐 基于 关联 规则 的 推荐 
Association Rule based Recommendation 是以 关联 规则 为 基础 把 
已 购 商品 作为 规则 头 规则 体 为 推荐 
对象 关联 规则 挖掘 可以 发现 不同 商品 在 销售 
过程 中 的 相关性 在 零售业 中 已经 得到 了 
成功 的 应用 管理 规则 就是 在 一个 交易 数据库 
中 统计 购买 了 商品 集 X 的 交易 中有 
多大 比例 的 交易 同时 购买 了 商品 集 Y 
其 直观 的 意义 就是 用户 在 购买 某些 商品 
的 时候 有 多大 倾向 去 购买 另外 一些 商品 
比如 购买 牛奶 的 同时 很多 人 会 同时 购买 
面包 算法 的 第一 步 关联 规则 的 发现 最为 
关键 且 最 耗时 是 算法 的 瓶颈 但可以 离线 
进行 其次 商品 名称 的 同义 性 问题 也 是 
关联 规则 的 一个 难点 基于 效用 推荐 基于 效用 
的 推荐 Utility based Recommendation 是 建立 在 对 用户 
使用 项目 的 效用 情况 上 计算 的 其 核心 
问题 是 怎么样 为 每一个 用户 去 创建 一个 效用函数 
因此 用户 资料 模型 很大 程度 上 是由 系统 所 
采用 的 效用函数 决定 的 基于 效用 推荐 的 好处 
是 它 能把 非 产品 的 属性 如 提供商 的 
可靠性 Vendor Reliability 和 产品 的 可得 性 Product Availability 
等 考虑 到 效用 计算 中 基于 知识 推荐 基于 
知识 的 推荐 Knowledge based Recommendation 在 某种 程度 是 
可以 看成 是 一种 推理 Inference 技术 它 不是 建立 
在 用户 需要 和 偏好 基础 上 推荐 的 基于 
知识 的 方法 因 它们 所用 的 功能 知识 不同 
而 有 明显 区别 效用 知识 Functional Knowledge 是 一种 
关于 一个 项目 如何 满足 某一 特定 用户 的 知识 
因此/c 能/v 解释/v 需要/v 和/c 推荐/v 的/uj 关系/n 所以 用户 
资料 可以 是 任何 能 支持 推理 的 知识 结构 
它 可以 是 用户 已经 规范化 的 查询 也 可以 
是 一个 更 详细 的 用户 需要 的 表示 组合/v 
推荐/v 由于/c 各种/r 推荐/v 方法/n 都有/nr 优缺点/n 所以 在 实际 
中 组合 推 H y b r i d R 
e c o m m e n d a t 
i o n 经常 被 采用 研究/vn 和/c 应用/v 最多/d 
的/uj 是/v 内容/n 推荐/v 和/c 协同/n 过滤/v 推荐/v 的/uj 组合/v 
最 简单 的 做法 就是 分别 用 基于 内容 的 
方法 和 协同 过滤 推荐 方法 去 产生 一个 推荐 
预测 结果 然后 用 某 方法 组合 其 结果 尽管/c 
从/p 理论上/i 有/v 很多/m 种/m 推荐/v 组合/v 方法/n 但在 某一 
具体 问题 中 并 不见得 都 有效 组合 推荐 一个 
最 重要 原则 就是 通过 组合 后 要能 避免 或 
弥补 各自 推荐 技术 的 弱点 在 组合 方式 上 
有 研究 人员 提出 了 七种 组合 思路 1 加权 
Weight 加权 多种 推荐 技术 结果 2 变换 Switch 根据 
问题 背景 和 实际 情况 或 要求 决定 变换 采用 
不同 的 推荐 技术 3 混合 Mixed 同时 采用 多种 
推荐 技术 给 出 多种 推荐 结果 为 用户 提供 
参考 4 特征 组合 Feature combination 组合 来自 不同 推荐 
数据源 的 特征 被 另一种 推荐算法 所 采用 5 层叠 
Cascade 先用 一种 推荐 技术 产生 一种 粗糙 的 推荐 
结果 第二种 推荐 技术 在此 推荐 结果 的 基础 上 
进一步 作出 更 精确 的 推荐 6 特征 扩充 Feature 
augmentation 一种 技术 产生 附加 的 特征 信息 嵌入 到 
另一种 推荐 技术 的 特征 输入 中 7 元 级别 
Meta level 用 一种 推荐 方法 产生 的 模型 作为 
另一种 推荐 方法 的 输入 体系结构 服务器端 推荐 系统 推荐 
系统 的 体系 结构 研究 的 重要 问题 就是 用户 
信息 收集 和 用户 描述 文件 放在 什么 地方 服务器 
还是 客户 机上 或者 是 处于 二者 之间 的 代理服务器 
上 最初 的 推荐 系统 都是/nr 基于 服务器端 的 推荐 
系统 在 这类 推荐 系统 中 推荐 系统 与 Web 
服务器 一般 共享 一台 硬件 设备 在 逻辑上 推荐/v 系统/n 
要/v 的/uj 用户/n 信息/n 收集/v 和/c 建模/n 都/d 依赖/v 于/p 
Web/w 服务器/n 由此可知 基于 服务器端 的 推荐 系统 存在 的 
问题 主要 包括 1 个性化 信息 的 收集 完全 由 
Web 服务器 来 完成 受到 了 Web 服务器 功能 的 
限制 2 增加 了 Web 服务器 的 系统 开销 3 
对 用户 的 隐私 有 极大 威胁 无论/c 是/v 推荐/v 
系统/n 的/uj 管理/vn 者/k 还是/c 入侵/v 推荐/v 系统/n 的/uj 人员/n 
都能/nr 方便/a 地/uv 获取/v 存放在/i 服务器/n 上/f 的/uj 用户/n 数据/n 
由于 用户 的 个人 数据 是 有 很高 价值 的 
接触 到 用户 数据 的 部分 人 会 出卖 用户 
数据 或 把 用户 数据 用于 非法 用途 客户端 推荐 
系统 基于 客户端 推荐 系统 典型 的 客户端 个性化 服务 
系统 有 斯坦福 大学 的 LIRA 麻省 理工学院 的 Letizia 
加州 大学 的 Syskill & Webert 卡内基 梅隆 大学 的 
PersonalWeb Watcher 等 基于 客户端 的 推荐 系统 有 如下 
优点 1 由于 用户 的 信息 就 在 本地 收集 
和 处理 因而 不但 能够 获取 丰富 准确 的 用户 
信息 以 构建 高 质量 的 用户 模型 2 少量 
甚至 没有 用户 数据 存放在 服务器 上 Web 服务器 不能 
访问 和 控制 用户 的 数据 能 比较 好 地 
保护 用户 的 隐私 3 用户 更 愿意 向 推荐 
系统 提供 个人 信息 从而 提高 推荐 系统 的 推荐 
性能 因为 基于 客户端 的 推荐 系统 中 的 用户 
数据 存储 在 用户 本地 客户 机上 用户 对 数据 
能够 进行 自行 控制 基于 客户端 的 推荐 系统 有 
一定 缺点 1 用户 描述 文件 的 形成 推荐/v 策略/n 
的/uj 应用/v 都/d 依赖/v 于/p 所有/b 用户/n 数据/n 分析/vn 的/uj 
基础/n 上/f 进行/v 的/uj 而 基于 客户端 的 推荐 系统 
较难 获取 其他 用户 的 数据 用户 描述 文件 较 
难得到 协同 推荐 策略 实施 也 较难 所以 推荐 系统 
要 重新 设计 尤其 是 推荐 策略 必须 进行 修改 
2 个性化 推荐 处理 过程 中 用户 的 数据 资料 
还 需要 部分 的 传给 服务器 存在 隐私 泄漏 的 
危险 需要 开发 安全 传输 平台 进行 数据 传输 知名 
团队 明尼苏达 大学 GroupLens John Riedl Joseph A . Konstan 
密西根 大学 Paul Resnick 卡内基 梅隆 大学 JaimeCallan 微软 研究院 
Ryen W . White 纽约大学 Alexander Tuzhilin 百分点 科技 团队 
Baifendian 机器翻译 机器翻译 又 称为 自动 翻译 是 利用 计算机 
将 一种 自然 语言 源语言 转换 为 另一种 自然语言 目标语言 
的 过程 它 是 计算机 语言学 的 一个 分支 是 
人工智能 的 终极 目标 之一 具有 重要 的 科学 研究 
价值 基础 机器 翻译 技术 的 发展 一直 与 计算机 
技术 信息论 语言学 等 学科 的 发展 紧密 相随 从 
早期 的 词典 匹配 到 词典 结合 语言学 专家 知识 
的 规则 翻译 再到 基于 语料库 的 统计 机器翻译 随着 
计算机 计算 能力 的 提升 和多/nr 语言 信息 的 爆发 
式 增长 机器 翻译 技术 逐渐 走出 象牙塔 开始 为 
普通 用户 提供 实时 便捷 的 翻译 服务 类别 基于 
规则 的 机译 系统 基于 统计 基于 人工神经网络 在线 机译 
主题词 识别 主题词 识别 应该 是 主题词 提取 的 研究 
内容 通过 对 各类 文本 经过 主题 模型 算法 得到 
文本 的 主题词 对 文本 主题词 进行 分析 和 识别 
来 实现 对 基于 内容 的 文本 匹配 等功能 主要 
还是 基于 文本处理 的 基本 方法 TF IDF 的 文本 
表示 向量 的 文本 表示 方法 文本 分词 技术 主题 
模型 等 来 提取 主题词 如有 理解 上 的 错误 
请 指正 如 以后 有 新的 理解 会 有所 改进 
知识库 构建 知识库 是 用于 指示 管理 的 一种 特殊 
的 数据库 以 便于 有关 领域 只是 的 采集 整理 
以及 提取 知识库 中 的 知识 源于 领域专家 它 是 
求解 问题 所需 领域 知识 的 集合 包括 基本 事实 
规则 和 其它 有关 信息 现有 知识库 的 构建 常以 
本体论 作为 基础 知识 本体论 基础知识 详见 本体论 研究 综述 
论文 的 总结 特点 1 知识库 中 的 知识 根据 
它们 的 应用 领域 特征 背景 特征 获取 时的/nr 背景 
信息 使用 特征 属性 特征 等 而被 构成 便于 利用 
的 有 结构 的 组织 形式 知识 片 一般 是 
模块化 的 2 知识库 的 知识 是 有 层次 的 
最低 层 是 事实 知识 中间层 是 用来 控制 事实 
的 知识 通常用 规则 过程 等 表示 最高 层次 是 
策略 它 以 中间层 知识 为 控制 对象 策略 也 
常常 被 认为 是 规则 的 规则 因此 知识库 的 
基本 结构 是 层次结构 是由 其 知识 本身 的 特性 
所 确定 的 在 知识库 中 知识 片 间 通常 
都 存在 相互 依赖 关系 规则 是 最 典型 最 
常用 的 一种 知识 片 3 知识库 中 可有 一种 
不 只 属于 某一 层次 或者说 在 任一 层次 都 
存在 的 特殊 形式 的 知识 可信度 或称 信任度 置信测度 
等 对 某一 问题 有关 事实 规则 和 策略 都可 
标 以 可信度 这样 就 形成 了 增广 知识库 在 
数据库 中 不 存在 不确定性 度量 因为 在 数据库 的 
处理 中 一切 都 属于 确 定型 的 4 知识库 
中 还可 存在 一个 通常 被 称作 典型 方 法库 
的 特殊 部分 如果 对于 某些 问题 的 解决 途径 
是 肯定 和 必然 的 就 可以 把 其 作为 
一 部分 相当 肯定 的 问题 解决 途径 直接 存储 
在 典型 方 法库 中 这种 宏观 的 存储 将 
构成 知识库 的 另一 部分 在 使用 这 部分 时 
机器 推理 将 只限于 选用 典型 方 法库 中的 某 
一层 体 部分 功能 1 . 知识库 使 信息 和 
知识 有序化 是 知识库 对 组织 的 首要 贡献 2 
. 知识库 加快 知识 和 信息 的 流动 有利于 知识 
共享 与 交流 3 . 知识库 还 有利于 实现 组织 
的 协作 与 沟通 4 . 知识库 可以 帮助 企业 
实现 对 客户 知识 的 有效 管理 缺陷 不 完整性 
⒈ 悬挂 条件 如果 该 规则 的 任意 前提 条件 
都不/nr 出现 在 数据库 中 也不 出现 在 所有 规则 
的 结论 部分 则 该 规则 永远 不会 被 激活 
2 . 无用 结论 如果 一个 规则 结论 部分 的 
谓词 没有 在 知识库 中 任何 规则 的 前提 条件 
中 出现 该 谓词 称为 无用 条件 3 . 孤立 
规则 如果 一个 规则 前提 部分 的 谓词 都是 悬挂 
条件 并且 其 结论 部分 的 谓词 都是 无用 结论 
则 称 该 规则 为 孤立 的 不一致性 ⒈ 冗余 
规则 ⒉ 包含 规则 ⒊ 循环 规则 ⒋ 冲突 规则 
文本 表示 要 使得 计算机 能 高效 的 处理 真实 
文本 就 必须 找到 一种 理想 的 形式 化 表示 
方法 这种 表示 一方面 能 真实 的 反映 文档 内容 
主题 领域 或 结构 等 另一方面 也 要有 对 不同 
文档 的 区分 能力 随着 对 文本 处理 的 要求 
越来越 高 在对 文本 表示 的 的 形式 上 的 
研究 也 在 不断 的 进步 和 发展 下面 给 
出 一些 常用 的 基于 文本 内容 的 表示 方法 
以后 补充 基于 行为 的 表示 传统 的 方法 1 
one hot encoding 入门级 文本 表示 方法 应 用词 袋 
模型 BOW + TF IDF 技术 优点 是 简单 粗暴 
配合 LR 效果 也 不赖 缺点 也 明显 维度 太高 
且有 词义 鸿沟 问题 不适合 大 语料 2 主题 模型 
系列 1 LSA / LSI 将 文档 为 行 词 
为 列 表示 成 文档 词 大 矩阵 利用 SVD 
奇异 值 分解 矩阵 分解 的 实现 技术 训练 得到 
词 和 文档 的 特征向量 有 点儿 像 推荐 里 
的 隐 语义 模型 模型 忽略 了 语序 更 注重 
主题 相关 适合 长 文本 实际 使用 效果 还 不错 
2 LDA 比较 适合 长文 本 表示 不太 适合 短文 
本 表示 LDA 属于 一种 文档 主题 生成 模型 引用 
其他 博客 的话 LDA 认为 一篇 文档 的 每个 词 
都是 通过 以 一定 概率 选择 了 某个 主题 并 
从 这个 主题 中 以 一定 概率 选择 某个 词语 
这样 一个 过程 得到 的 文档 到 主题 服从 多项式 
分布 主题 到 词语 服从 多项式 分布 深度 表示 NN 
神经网络 1 word2vec + TF IDF 加权平均 虽然 word2vec 非 
DNN 系列 但 其 训练 词 向量 效率 和 效果 
均 表现 不俗 首先 通过 word2vec 训练 词 向量 再 
通过 简单 的 词 加权 / 关键 tag 加权 / 
tf idf 加权平均 得到 文档 向量 表示 在 加权 之前 
做 停用词 剔除 词 聚 类 等 预处理 是个 不错 
的 选择 PS 该 方法 对 短 文本 效果 还 
可以 长文 本就 不咋地 了 kaggle101 中 的 一个 word2vec 
题目 的 tutorial 里 作者 如是说 他 试 了 一下 
简单 加权 和 各种 加权 不管 如何 处理 效果 还 
不如 01 归其/nr 原因/n 作者/n 认为/v 加权/v 的/uj 方式/n 丢失/v 
了/ul 最/d 重要/a 的/uj 句子/n 结构/n 信息/n 和词/nr 相关/v 信息/n 
也 可以 说 是 词序 信息 而 doc2vec 的 方法 
则 保存 了 这种 信息 2 doc2vec 提到 word2vec 就 
不得 的 不提 doc2vec 二者 亲兄弟 doc2vec 在 word2vec 基础 
上 增加 了 个 段落 向量 能 直接 训练 处 
段落 文档 向量 在 实际 使用 中 貌似 效果 一般 
特别 是 长 文本 NLP 相关 任务 PS gensim 有 
现成 的 API 参考 2014 ICML Distributed Representations of Sentences 
and Documents 3 WMDICML2015 的 论文 From Word Embeddings To 
Document Distances Kusner Washington University 新 提出 一种 计算 doc 
相似 度 的 方式 大致 思路 是 将 词 之间 
的 余弦 距离 作为 ground distance 词频 作为 权重 在 
权重 的 约束 条件下 求 WMD 的 线性规划 最优 解 
4 glove 最近 学术界 兴起 了 glove 的 方法 核心 
思想 就是 挖掘 词语 共 现 信息 的 内在 含义 
融合 基于 全局 统计 的 方法 如 LSI / LSA 
等 和 基于 局部 预测 方法 如 word2vec 等 于 
一体 貌似 效果 不错 在 词 聚 类 任务 上 
的 效果 超越 了 word2vec PS GloVe Global Vectors forWord 
Representation 命名 实体 识别 命名 实体 识别 Named Entity Recognition 
简称 NER 又 称作 专名 识别 是 指 识别 文本 
中 具有 特定 意义 的 实体 主要 包括 人名 地名 
机构 名 专有名词 等 作用 命名 实体 识别 是 信息 
提取 问答 系统 句法分析 机器翻译 面向 Semantic Web 的 元数据 
标注 等 应用 领域 的 重要 基础 工具 在 自然 
语言 处理 技术 走向 实用化 的 过程 中 占有 重要 
地位 一般来说 命名 实体 识别 的 任务 就是 识别 出 
待处理 文本 中 三大类 实体类 时间 类 和数 字类 七 
小 类 人名 机构 名 地名 时间 日期 货币 和 
百分比 命名 实体 过程 通常 包括 两 部分 1 实体 
边界 识别 2 确定 实体 类别 人名 地名 机构 名 
或 其他 英语 中的 命名 实体 具有 比较 明显 的 
形式 标志 即 实体 中 的 每个 词 的 第一 
个 字母 要 大写 所以 实体 边界 识别 相对 容易 
任务 的 重点 是 确定 实体 的 类别 和 英语 
相比 汉语 命名 实体 识别 任务 更加 复杂 而且 相对 
于 实体 类别 标 注子 任务 实体 边界 的 识别 
更加 困难 难点 1 汉语 文本 没有 类似 英文 文本 
中 空格 之类 的 显 式 标示 词 的 边界 
标示符 命名 实体 识别 的 第一 步 就是 确定 词 
的 边界 即 分词 2 汉语分词 和 命名 实体 识别 
互相 影响 3 除了 英语 中 定义 的 实体 外国/ns 
人名/n 译名/n 和/c 地名/n 译名/n 是/v 存在/v 于/p 汉语/nz 中/f 
的/uj 两类/m 特殊/a 实体/n 类型/n 4 现代汉语 文本 尤其 是 
网络 汉语 文本 常 出现 中 英文 交替 使用 这时 
汉语 命名 实体 识别 的 任务 还 包括 识别 其中 
的 英文 命名 实体 5 不同 的 命名 实体 具有 
不同 的 内部 特征 不 可能 用 一个 统一 的 
模型 来 刻画 所有 的 实体 内部 特征 文本 分析 
介绍 文本 分析 是 指 对 文本 的 表示 及其 
特征 项的/nr 选取 文本 分析 是 文本 挖掘 信息检索 的 
一个 基本 问题 它 把 从 文本 中 抽 取出 
的 特征词 进行 量化 来 表示 文本 信息 文本 text 
与 讯息 message 的 意义 大致相同 指 的 是 有 
一定 的 符号 或 符码 组成 的 信息 结构体 这种 
结构体 可 采用 不同 的 表现 形态 如 语言 的 
文字 的 影像 的 等等 文本 是由 特定 的 人 
制作 的 文本 的 语义 不可避免 地 会 反映 人 
的 特定 立场 观点 价值 和 利益 因此 由 文本 
内容 分析 可以 推断 文本 提供者 的 意图 和 目的 
特征 将 它们 从 一个 无 结构 的 原始 文本 
转化 为 结构化 的 计算机 可以 识别 处理 的 信息 
即对 文本 进行 科学 的 抽象 建立 它 的 数学 
模型 用以 描述 和 代替 文本 使 计算机 能够 通过 
对 这种 模型 的 计算 和 操作 来 实现 对 
文本 的 识别 由于 文本 是非 结构化 的 数据 要想 
从 大量 的 文本 中 挖掘 有用 的 信息 就 
必须 首先 将 文本 转化 为 可 处理 的 结构化 
形式 目前 人们 通常 采用 向量空间 模型 来 描述 文本 
向量 但是/c 如果/c 直接/ad 用/p 分词/n 算法/n 和/c 词频/n 统计/v 
方法/n 得到/v 的/uj 特征/n 项来/nr 表示/v 文本/n 向量/n 中的/i 各个/r 
维/v 那么 这个 向量 的 维度 将 是 非常 的 
大 这种 未经 处理 的 文本 矢量 不仅 给 后续 
工作 带来 巨大 的 计算 开销 使 整个 处理 过程 
的 效率 非常 低下 而且 会 损害 分类 聚 类 
算法 的 精确性 从而 使 所 得到 的 结果 很难 
令人满意 因此 必须 对 文本 向量 做 进一步 净化 处理 
在 保证 原文 含义 的 基础 上 找出 对 文本 
特征 类别 最具 代表性 的 文本 特征 为了 解决 这个 
问题 最 有效 的 办法 就是 通过 特征选择 来 降 
维 目前 有关 文本 表示 的 研究 主要 集中于 文本 
表示 模型 的 选择 和 特征词 选择 算法 的 选取 
上 用于 表示 文本 的 基本 单位 通常 称为 文本 
的 特征 或 特征 项 特征 项 必须 具备 一定 
的 特性 1 特征 项要/nr 能够 确实 标识 文本 内容 
2 特征/n 项/n 具有/v 将/d 目标/n 文本/n 与/p 其他/r 文/n 
本相/d 区分/n 的/uj 能力/n 3 特征 项的/nr 个数 不能 太多 
4 特征 项 分离 要 比较 容易 实现 在 中文 
文本 中 可以 采用 字 词 或 短语 作为 表示 
文本 的 特征 项 相比较 而言 词 比 字 具有 
更强 的 表达 能力 而 词 和 短语 相比 词 
的 切分 难度 比 短语 的 切分 难度 小得多 因此 
目前 大多数 中文 文本 分类 系统 都 采用 词作 为特征 
项 称作 特征词 这些 特征词 作为 文档 的 中间 表示 
形式 用来 实现 文档 与 文档 文档 与 用户 目标 
之间 的 相似 度 计算 如果 把 所有 的 词 
都作 为特征 项 那么 特征向量 的 维数 将 过于 巨大 
从而 导致 计算 量 太大 在 这样 的 情况 下 
要 完成 文本 分类 几乎 是 不 可能 的 特征 
抽取 的 主要 功能 是 在 不 损伤 文本 核心 
信息 的 情况 下 尽量 减少 要 处理 的 单词 
数 以此 来 降低 向量空间 维数 从而 简化 计算 提高 
文本处理 的 速度 和 效率 文本 特征选择 对 文本 内容 
的 过滤 和 分类 聚 类 处理 自动 摘要 以及 
用户 兴趣 模式 发现 知识发现/i 等/u 有关/vn 方面/n 的/uj 研究/vn 
都有/nr 非常/d 重要/a 的/uj 影响/vn 通常 根据 某 个 特征 
评估 函数 计算 各 个 特征 的 评分 值 然后按 
评 分值 对 这些 特征 进行 排序 选取 若干个 评分 
值 最高 的 作为 特征词 这 就是 特征选择 Feature Selection 
特征 选取 的 方式 常见 的 有 4种 1 用 
映射 或 变换 的 方法 把 原始 特征 变换 为 
较少 的 新 特征 2 从 原始 特征 中 挑选 
出 一些 最 具 代表性 的 特征 3 根据 专家 
的 知识 挑选 最 有 影响 的 特征 4 用 
数学 的 方法 进行 选取 找出 最具 分类 信息 的 
特征 这种 方法 是 一种 比较 精确 的 方法 人为 
因素 的 干扰 较少 尤其 适合 于 文本 自动 分类 
挖掘 系统 的 应用 以后 会 补充 一篇 结合 自然语言 
处理 综述 这本书 从 词法 句法 语法 语用 方面 来 
介绍 文本 分析 句法分析 句法分析 Parsing 就是指 对 句子 中 
的 词语 语法 功能 进行 分析 比如 我 来 晚了 
这里 我 是 主语 来 是 谓语 晚了 是 补语 
应用 句法分析 现在 主要 的 应用 在于 中文 信息 处理 
中 如 机器 翻译 等 它 是 语 块 分析 
chunking 思想 的 一个 直接 实现 语 块 分析 通过 
识别 出 高 层次 的 结构单元 来 简化 句子 的 
描述 从 不同 的 句子 中 找到 语 块 规律 
的 一条 途径 是 学习 一种 语法 这种 语法 能够 
解释 我们 所 找到 的 分块 结构 这 属于 语法 
归纳 的 范畴 迄今为止 在 句法分析 领域 中 存在 很多 
争议 也许 你 会 发现 恰巧 有人 提出 了 与 
你 正在 努力 研究 的 语法 归纳 程序 偶然 产生 
的 相似 的 句法结构 而且 这些 也 可能 已经 被 
当成 了 句法结构 模型 的 证据 但是 这些 找到 的 
结构 依赖 于 学习 程序 中 隐含 的 归纳 偏置 
这也 指明 了 另外 一个 方向 我们 需要 事先 知道 
模型 能够 找到 什么样 的 结构 同时 应该 首先 确定 
我们 对 句子 进行 句法分析 的 目的 这里 有 各种 
可能 的 目的 使用 句法结构 作为 语义解释 的 第一 步 
识别 短 语语 块 为 信息检索 系统 的 索引 服务 
构建 一个 概率 句法 分析器 作为 一个 优于 n 元 
语法 的 语言 模型 这些 问题 的 共同 目标 是 
构建 这样 的 一个 系统 对于 任意 的 句子 都能够 
主 产生 证明 有用 的 结构 也 就是 要 构建 
一个 句法 分析器 句法分析 的 三种 不同 的 途径 可以 
利用 概率 1 利用 概率 来 确定 句子 一种 可能 
的 做法 是 将 句法 分析器 看成 是 一个 词语 
网络 上 的 语言 模型 用来 确定 什么样 的 词 
序列 经过 网络 的 时候 会 获得 最大 概率 2 
利用 概率 来 加速 语法分析 第二 个 目标 是 利用 
概率 对 句法 分析器 的 搜索 空间 进行 排序 或 
剪枝 这 使得 句法 分析器 能够 在 不 影响 结果 
质量 的 情况 下 尽快 找到 最优 的 分析 途径 
3 利用 概率 选择 句法分析 结果 句法 分析器 可以 从 
输入 句子 的 众多 分析 结果 中 选择 可能性 最大 
的 语音 识别 技术 语音 识别 技术 也 被 称为 
自动 语音 识别 Automatic Speech Recognition ASR 其 目标 是 
将 人类 的 语音 中的 词汇 内容 转换 为 计算机 
可读 的 输入 例如 按键 二进制 编码 或者 字符 序列 
简介 语音 识别 技术 的 应用 包括 语音拨号 语音 导航 
室内 设备 控制 语音 文档 检索 简单 的 听写 数据录入 
等 语音 识别 技术 与 其他 自然语言 处理 技术 如 
机器翻译 及 语音合成 技术相结合 可以 构建 出 更加 复杂 的 
应用 例如 语音 到 语音 的 翻译 语音 识别 技术 
所 涉及 的 领域 包括 信号处理 模式识别 概率论 和 信息论 
发声 机理 和 听觉 机理 人工智能 等等 语音 识别 技术 
和 语音 合成 技术 是 比较 大 的 两个 研究 
分支 涉及 的 内容 比较 多 比较 广 在此 不做 
过多 的 介绍 如 有 可能 以后 补充 自然语言 处理 
技术 NLP 在 推荐 系统 中 的 应用 NLP   
推荐 系统   词 袋 模型 阅读 6145 作者  /i 
张相於/nr 58 集团 算法 架构师 转转 搜索 推荐 部 负责人 
负责 搜索 推荐 以及 算法 相关 工作 多年来 主要 从事 
推荐 系统 以及 机器学习 也 做过 计算 广告 反 作弊 
等 相关 工作 并 热衷于 探索 大 数据 和 机器 
学习 技术 在 其他 领域 的 应用 实践 责编 何永灿/nr 
heyc @ csdn . net 本文 为 程序员 原创 文章 
更多 精彩文章 请 订阅 程序员 概述 个性化 推荐 是 大 
数据 时代 不可或缺 的 技术 在 电商 信息 分发 计算 
广告 互联网 金融 等 领域 都 起着 重要 的 作用 
具体来讲 个性化 推荐 在 流量 高效 利用 信息 高效 分发 
提升 用户 体验 长尾 物品 挖掘 等 方面 均 起着 
核心作用 在 推荐 系统 中 经常 需要 处理 各种 文 
本类 数据 例如 商品 描述 新闻资讯 用户 留言 等等 具体来讲 
我们 需要 使用 文本 数据 完成 以下 任务 候选 商品 
召回 候选 商品 召回 是 推荐 流程 的 第一 步 
用来 生成 待 推荐 的 物品 集合 这 部分 的 
核心 操作 是 根据 各种 不同 的 推荐 算法 来 
获取 到 对应 的 物品 集合 而 文 本类 数据 
就是 很 重要 的 一类 召回 算法 具有 不 依赖 
用户 行为 多样性 丰富 等 优势 在 文本 信息 丰富 
或者 用户 信息 缺乏 的 场合 中 具有 非常 重要 
的 作用 相关性 计算 相关性 计算 充斥 着 推荐 系统 
流程 的 各个 步骤 例如 召回 算法 中 的 各种 
文本 相似 度 算法 以及 用户 画像 计算 时 用到 
的 一些 相关性 计算 等 作 为特征 参与 模型 排序 
CTR / CVR 在 候 选集 召回 之后 的 排序 
层 文 本类 特征 常常 可以 提供 很多 的 信息 
从而 成为 重要 的 排序 特征 但是 相比 结构化 信息 
例如 商品 的 属性 等 文本 信息 在 具体 使用 
时 具有 一些 先天 缺点 首先 文本 数据 中 的 
结构 信息量 少 严格来说 文本 数据 通常 是 没有 什么 
结构 的 一般 能够 有的/nr 结构 可能 只是 标题 正文 
评论 这样 区分 文本 来源 的 结构 除此以外 一般 就 
没有 更多 的 结构 信息 了 为什么 我们 要 在意 
结构 信息 呢 因为 结构 代表 着 信息量 无论 是 
使用 算法 还是 业务 规则 都 可以 根据 结构化 信息 
来 制定 推荐 策略 例如 召回 所 有 颜色 为 
蓝色 的 长 款 羽绒服 这样 一个 策略 里 就用 
到了 颜色 和 款式 这 两个 结构化 信息 但是 如果 
商品 的 描述 数据库 中 没有 这样 的 结构化 信息 
只有 一句 该 羽绒服 为 蓝色 长 款 羽绒服 的 
自由 文本 那么 就 无法 利用 结构 信息 制定 策略 
了 其次 文本 内容 的 信息量 不 确定 与 无 
结构化 相 伴随 的 是 文本 数据 在 内容 的 
不确定性 这种 不确定性 体现 在 内容 和 数量 上 例如 
不同 用户 对 同 一件 二手 商品 的 描述 可能 
差异 非常大 具体 可能 在 用词 描述 文本 长短 等 
方面 都 具有 较大 差异 同样 的 两个 物品 在 
一个 物品 的 描述 中 出现 的 内容 在 另外 
一个 物品 中 并不 一定 会 出现 这种 差异性 的 
存在 使得 文本 数据 往往 难以 作为 一种 稳定 可靠 
的 数据 源 来 使用 尤其 是 在 UGC 化 
明显 的 场景 下 更是 如此 再次 自由 文本 中 
的 歧义 问题 较多 歧义 理解 是 自然 语言 处理 
中 的 重要 研究 课题 同时 歧义 也 影响 着 
我们 在 推荐 系统 中 对 文本 数据 的 使用 
例如 用户 在 描述 自己 的 二手 手机 时 可能 
会写 出售 iPhone6 一部 打算 凑钱 买 iPhone7 这样的话 这样 
一句 对人 来说 意思 很 明确 的话 却对 机器 造成了 
很大 困扰 这个 手机 究竟 是 iPhone6 还是 iPhone7 在 
这样 的 背景 下 如何 保证 推荐 系统 的 准确率 
便 成为 了 一个 挑战 但是 文本 数据 也 不是 
一无是处 有 缺点 的 同时 也 具有 一些 结构化 数据 
所 不 具有 的 优点 数据 量大 无 结构化 的 
文本 数据 一般来说 是 非常 容易 获得 的 例如 各种 
UGC 渠道 以及 网络 爬 取 等 方法 都 可穿 
获得 大量 文本 数据 多样性 丰富 无 结构化 是 一把 
双刃剑 不好 的 一面 已经 分析 过 好 的 一面 
就是 由于 其 开放性 导致 具有 丰富 的 多样性 会 
包含 一些 结构 规定 以外 的 数据 信息 及时 在 
一些 新名词 新事物 出现 之后 微博 朋友圈 常常 是 最先 
能够 反应出 变化 的 地方 而 这些 都是纯/nr 文本 的 
数据 对 这些 数据 的 合理 分析 能够 最 快 
得到 结构化 预定义 数据 所 无法 得到 的 信息 这 
也是 文本 数据 的 优势 综上所述 文本 数据 是 一类 
量大 复杂 丰富 的 数据 对 推荐 系统 起着 重要 
的 作用 本文 将 针对 上面 提到 的 几个 方面 
对 推荐 系统 中 常见 的 文本 处理 方法 进行 
介绍 从 这里 出发 词 袋 模型 词 袋 模型 
Bag of Words 简称 BOW 模型 是 最简单 的 文本 
处理 方法 其 核心 假设 非常简单 就是 认为 一篇 文档 
是由 文档 中的 词 组成 的 多重 集合 多重 集合 
与 普通 集合 的 不同 在于 考虑 了 集合 中 
元素 的 出现 次数 构成 的 这 是 一种 最 
简单 的 假设 没有 考虑 文档 中 诸如 语法 词序 
等 其他 重要 因素 只 考虑 了 词 的 出现 
次数 这样 简单 的 假设 显然 丢掉 了 很多 信息 
但是/c 带来/v 的/uj 好处/d 是/v 使用/v 和/c 计算/v 都/d 比较/d 
简单/a 同时 也 具有 较大 的 灵活性 在 推荐 系统 
中 如果 将 一个 物品 看作 一个 词 袋 我们 
可以 根据 袋 中的 词 来 召回 相关 物品 例如 
用户 浏览 了 一个 包含 羽绒服 关键词 的 商品 我们 
可以 召回 包含 羽绒服 的 其他 商品 作为 该 次 
推荐 的 候选 商品 并且 可以 根据 这个 词 在 
词 袋 中 出现 的 次数 词频 对 召回 商品 
进行 排序 这种 简单 的 做法 显然 存在 着 很多 
问题 首先 将 文本 进行 分词 后 得到 的 词 
里面 并/c 不是/c 每个/r 词/n 都/d 可以/c 用来/v 做/v 召回/v 
和/c 排序/n 例如 的 地 得 你 我 他 这样 
的 停用词 就 该 去掉 此外 一些 出现 频率 特别 
高 或者 特别 低 的 词 也 需要 做 特殊 
处理 否则 会 导致 召回 结果 相关性 低 或 召回 
结果 过少 等 问题 其次 使用 词频 来 度量 重要性 
也 显得 合理性 不足 以上 面的 羽绒服 召回 为例 如果在 
羽绒服 的 类别 里 使用 羽绒服 这个词 在 商品 描述 
中 的 出现 频率 来 衡量 商品 的 相关性 会 
导致 所有 的 羽绒服 都 具有 类似 的 相关性 因为 
在 描述 中 大家 都会 使用 类似 数量 的 该 
词汇 所以 我们 需要 一种 更为 科学 合理 的 方法 
来 度量 文本 之间 的 相关性 除了 上面 的 用法 
我们 还 可以 将 词 袋中 的 每个 词 作为 
一维 特征 加入到 排序 模型 中 例如 在 一个 以 
LR 为 模型 的 CTR 排序 模型 中 如果 这 
一维 特征 的 权重 为 w 则可 解释为 包含 这个词 
的 样本 相比 不 包含 这个词 的 样本 在 点击率 
的 log odds 上 要 高出 w 在 排序 模型 
中 使用 词 特征 的 时候 为了 增强 特征 的 
区分 能力 我们 常常 会 使用 简单 词 袋 模型 
的 一种 升级版 N gram 词 袋 模型 N gram 
指 的 就是 把 N 个 连续 的 词 作为 
一个 单位 进行 处理 例如 John likes to watch movies 
. Mary likes movies too . 这句话 处理 为 简单 
词 袋 模型 后的/nr 结果 为 John 1 likes 2 
to 1 watch 1 movies 2 Mary 1 too 1 
而 处理 为 bigram 2 gram 后的/nr 结果 为 John 
likes 1 likes to 1 to watch 1 watch movies 
1 Mary likes 1 likes movies 1 movies too 1 
做 这样 的 处理 有 什么 好处 呢 如果 将 
bigram 作为 排序 模型 的 特征 或者 相似 度 计算 
的 特征 最 明显 的 好处 就是 增强 了 特征 
的 区分 能力 简单 来讲 就是 两个 有N个/nr bigram 重合 
的 物品 其 相关性 要 大于 有N个/nr 词 重合 的 
物品 从 根本 上 来讲 是 因为 bigram 的 重合 
几率 要 低于 1 gram 也 就是 普通 词 的 
重合 几率 那么 是不是 N gram 中的 N 越大 就越 
好呢 N 的 增大 虽然 增强 了 特征 的 区分 
能力 但是 同时 也 加大 了 数据 的 稀疏 性 
从 极端 情况 来讲 假设 N 取到 100 那么/r 几乎/d 
不会/v 有/v 两个/m 文档/n 有/v 重合/vn 的/uj 100/m gram 了 
那 这样 的 特征 也 就 失去 了 意义 一般 
在 实际 应用 中 bigram 和 trigram 3 gram 能够 
在 区分 性 和 稀疏 性 之间 取到 比较好 的 
平衡 N 如果 继续 增大 稀疏 性 会有 明显增加 但是 
效果 却 不会 有 明显 提升 甚至 还 会有 降低 
综合 来看 虽然 词 袋 模型 存在 着 明显 的 
弊端 但是 只 需要 对 文本 做 简单 处理 就 
可以 使用 所以 不失为 一种 对 文本 数据 进行 快速 
处理 的 使用 方法 并且在 预处理 常用 的 预处理 包括 
停用词 的 去除 高频 / 低频词 的 去除 或 降 
权等/nr 重要性 处理 方法 也/d 可以/c 借助/v 外部/f 高/a 质量/n 
数据/n 对/p 自由/a 文本/n 数据/n 进行/v 过滤/v 和/c 限定/v 以求 
获得 质量 更高 的 原始数据 充分 的 情况 下 也 
常常 能够 得到 很好 的 效果 统一 度量衡 权重/n 计算/v 
和/c 向量空间/i 模型/n 从/p 上文/i 我们/r 看到/v 简单/a 的/uj 词/n 
袋/q 模型/n 在/p 经过/p 适当/a 预处理/vn 之后/f 可以 用来 在 
推荐 系统 中 召回 候选 物品 但是 在 计算 物品 
和 关键词 的 相关性 以及 物品 之间 的 相关性 时 
仅仅 使用 简单 的 词频 作为 排序 因素 显然 是 
不合理 的 为了 解决 这个 问题 我们 可以 引入 表达能力 
更强 的 基于 TF IDF 的 权重 计算方法 在 TF 
IDF 方法 中 一个词 t 在 文档 d 中 权重 
的 计算 方法 为 其中 tft d 代表 t 在 
d 中 出现 的 频次 而 dft 指 的 是 
包含 t 的 文档 数目 N 代表 全部 文档 的 
数目 TF IDF 以 及其 各种 改进 和 变种 关于 
TF IDF 变种 和 改进 的 详细 介绍 可 参考 
Introduction to Information Retrieval 的 第六 章 相比 简单 的 
TF 方法 核心 改进 在于 对 一个 词 的 重要性 
度量 例如 原始 TF IDF 在 TF 的 基础 上 
加入 了 对 IDF 的 考虑 从而 降低 了 出现 
频率 高而/nr 导致 无 区分 能力 的 词 的 重要性 
典型 的 如 停用词 因为 词 在 文档 中 的 
重要性 和 出现 次数 并 不是 完全 线性相关 非线性 TF 
缩 放对 TF 进行 log 缩放 从而 降低 出现 频率 
特别高 的 词 所占 的 权重 词 在 文档 中 
出现 的 频率 除了 和 重要性 相关 还 可能 和 
文档 的 长短 相关 为了 消除 这种 差异 可以 使用 
最大 TF 对 所有 的 TF 进行 归一化 这些 方法 
的 目的 都是 使 对词 在 文档 中 重要性 的 
度量 更加 合理 在此 基础 之上 我们 可以 对 基于 
词频 的 方法 进行 改进 例如 可以 将 之前 使用 
词频 来 对 物品 进行 排序 的 方法 改进 为 
根据 TF IDF 得分 来 进行 排序 但是 除此以外 我们 
还 需要 一套 统一 的 方法 来 度量 关键词 和 
文档 以及 文档 和 文档 之间 的 相关性 这套 方法 
就是 向量空间 模型 Vector Space Model 简称 VSM VSM 的 
核心 思想 是 将 一篇 文档 表达 为 一个 向量 
向量 的 每 一维 可以 代表 一个 词 在此 基础 
上 可以 使用 向量 运算 的 方法 对 文档 间 
相似 度 进行 统一 计算 而 这 其中 最为 核心 
的 计算 就是 向量 的 余弦 相似 度 计算 其中 
V d1 和V/nr d2 分别 为 两个 文档 的 向量 
表示 这样 一个 看似 简单 的 计算 公式 其实 有着 
非常 重要 的 意义 首先 它 给出 了 一种 相关性 
计算 的 通用 思路 那/r 就是/d 只要/c 能将/nr 两个/m 物品/n 
用/p 向量/n 进行/v 表示/v 就 可以 使用 该 公式 进行 
相关性 计算 其次 它 对 向量 的 具体 表示 内容 
没有 任何 限制 基于 用户 行为 的 协同 过滤 使用 
的 也是 同样 的 计算 公式 而在 文本 相关性 计算 
方面 我们 可以 使用 TFIDF 填充 向量 同时 也 可以 
用 N gram 以及 后 面会 介绍 的 文本 主题 
的 概率分布 各种 词 向量 等 其他 表示 形式 只要 
对 该 公式 的 内涵 有了/nr 深刻 理解 就 可以 
根据 需求 构造 合理 的 向量 表示 再次 该 公式 
具有 较强 的 可 解释性 它 将 整体 的 相关性 
拆解 为 多个 分量 的 相关性 的 叠加 并且 这个 
叠加 方式 可以 通过 公式 进行 调节 这样 一套 方法 
很 容易 解释 即使 对 非 技术人员 也 是 比较 
容易 理解 的 这 对于 和 产品 运营 等 非 
技术 人员 解释 算法 思路 有很/nr 重要 的 意义 最后 
这个 公式 在 实际 计算 中 可以 进行 一些 很 
高效 的 工程 优化 使其 能够 从容应对 大 数据 环境 
下 的 海量 数据 这 一点 是 其他 相关性 计算 
方法 很 难匹敌 的 VSM 是 一种 重剑 无 锋 
大巧不工 的 方法 形态 简单 而又 变化多端 领会 其 精髓 
之后 可以 发挥 出 极大 的 能量 透过 现象 看 
本质 隐 语义 模型 前面 介绍 了 文本 数据 的 
一些 显 式 使用 方法 所谓 显 式 是 指 
我们 将 可读 可 理解 的 文本 本身 作为 了 
相关性 计算 物品 召回 以及 模型 排序 的 特征 这样 
做 的 好处 是 简单 直观 能够 清晰 地 看到 
起 作用 的 是 什么 但是 其 弊端 是 无法 
捕捉 到 隐藏 在 文本 表面 之下 的 深层次 信息 
例如 羽绒服 和 棉衣 指 的 是 类似 的 东西 
羽绒服 和 棉鞋 具有 很强 的 相关性 类似 这样 的 
深层次 信息 是 显 式 的 文本 处理 所 无法 
捕捉 的 因此 我们 需要 一些 更 复杂 的 方法 
来 捕捉 而 隐 语义 模型 Latent Semantic Analysis 简称 
LSA 便是 这类 方法 的 鼻祖 之一 隐 语义 模型 
中的 隐 指 的 是 隐含 的 主题 这个 模型 
的 核心 假设 是 认为 虽然 一个 文档 由 很多 
的 词 组成 但是 这些 词 背后 的 主题 并 
不是 很多 换句话说 词 不过 是由 背后 的 主题 产生 
的 这 背后 的 主题 才是 更 为 核心 的 
信息 这种 从词/nr 下沉 到 主题 的 思路 贯穿 着 
我们 后面 要 介绍 到 的 其他 模型 也是 各种 
不同 文本 主体 模型 Topic Model 的 共同 中心思想 因此 
理解 这种 思路 非常 的 重要 在对 文档 做 LSA 
分解 之前 我们 需要 构造 文档 和词/nr 之间 的 关系 
一个 由 5个 文档 和 5个 词 组成 的 简单 
例子 如下 LSA 的 做法 是 将 这个 原始 矩阵 
C 进行 如下 形式 的 SVD 分解 其中 U 是 
矩阵 CCT 的 正交 特征向量 矩阵 V 是 矩阵 CTC 
的 正交 特征向量 矩阵 ∑ k 是 包含 前 k 
个 奇异 值 的 对角 矩阵 k 是 事先 选定 
的 一个 降 维 参数 得到 原始数据 的 一个 低维 
表示 降低 后的/nr 维度 包含 了 更多 的 信息 可以 
认为 每个 维度 代表 了 一个 主题 降 维 后的/nr 
每个 维度 包含 了 更 丰富 的 信息 例如 可以 
识别 近义词 和一词/nr 多义 可以 将 不在 训练 文档 中 
的 文档 d 通过 变 换为 新 向量空间 内 的 
一个 向量 这样 的 变换 无法 捕捉 到 新 文档 
中 的 信息 例如 词 的 共 现 以及 新词 
的 出现 等等 所以 该 模型 需要 定期 进行 全量/nr 
训练 从而 可以 在 降 维 后的/nr 空间 里 计算 
文档 间 相似 度 由于 新的 向量空间 包含 了 同义词 
等 更 深层 的 信息 这样 的 变换 会 提高 
相似 度 计算 的 准确率 和 召回率 为什么 LSA 能 
具有 这样 的 能力 我们 可以 从 这样 一个 角度 
来 看待 CCT 中 每个 元素 CCTi j 代表 同时 
包含 词 i 和词j/nr 的 文档 数量 而 CTC 中 
每个 元素 CTCi j 代表 文档 i 和 文档 j 
共享 的 词 的 数量 所以 这 两个 矩阵 中 
包含 了 不同 词 的 共同 出现 情况 以及 文档 
对词 的 共享 情况 通过 分解 这些 信息 得到 了 
类似 主题 一样 比 关键词 信息量 更高 的 低维 度 
数据 从 另外 一个 角度 来看 LSA 相当 于是 对 
文档 进行 了 一次 软 聚 类 降 维 后的/nr 
每个 维度 可 看做 是 一个 类 而 文档 在 
这个 维 度上 的 取值 则 代表 了 文档 对于 
这个 聚 类 的 归属 程度 LSA 处理 之后 的 
数据 推荐 中 能做 什么用 呢 首先 我们 可以 将 
分解 后的新/nr 维度 主题 维度 作为 索引 的 单位 对 
物品 进行 索引 来 替代 传统 的 以 词 为 
单位 的 索引 再将 用户 对 物品 的 行为 映射 
为对 新 维度 的 行为 这 两个 数据 准备好 之后 
就 可以 使用 新 的 数据 维度 对 候选 商品 
进行 召回 召回 之后 可以 使用 VSM 进行 相似 度 
计算 如 前文 所述 降/v 维/v 后的/nr 计算/v 会/v 带来/v 
更高/i 的/uj 准确率/n 和/c 召回率/i 同时 也 能够 减少 噪音 
词 的 干扰 典型 的 即使 两个 文档 没有 任何 
共享 的 词 它们 之间 仍然 会 存在 相关性 而这 
正是 LSA 带来 的 核心 优势 之一 此外 还 可以 
将 其 作为 排序 模型 的 排序 特征 简单 来讲 
我们 能在/nr 普通 关键词 上面 使用 的 方法 在 LSA 
上面 仍然 全部 可用 因为 LSA 的 本质 就是 对 
原始 数据 进行 了 语义 的 降 维 只需 将其 
看作 是 信息量 更 丰富 的 关键词 即可 可以 看到 
LSA 相比 关键词 来说 前进 了 一大步 主要 体现 在 
信息量 的 提升 维度 的 降低 以及对 近义词 和 多义词 
的 理解 但是 LSA 同时 也 具有 一些 缺点 例如 
训练 复杂度 高 LSA 的 训练 时 通过 SVD 进行 
的 而 SVD 本身 的 复杂度 是 很高 的 在 
海量 文档 和 海量 词汇 的 场景 下 难以 计算 
虽然 有 一些 优化 方法 可 降低 计算 的 复杂度 
但该 问题 仍然 没有 得到 根本 解决 检索 召回 复杂度 
高 如 上文 所述 使用 LSA 做 召回 需要 先将 
文档 或者 查询 关键词 映 射到 LSA 的 向量空间 中 
这 显然 也 是 一个 耗时 的 操作 LSA 中 
每个 主题 下 词 的 值 没有 概率 含义 甚至 
可能 出现 负值 只能 反应 数值 大小 关系 这/r 让/v 
我们/r 难以/d 从/p 概率/n 角度/n 来/v 解释/v 和/c 理解/v 主题/n 
和词的/nr 关系/n 从而 限制 了 我们 对 其 结果 更 
丰富 的 使用 概率 的 魔力 概率 隐 语义 模型 
为了 进一步 发扬 隐 语义 模型 的 威力 并 尽力 
克服 LSA 模型 的 问题 Thomas Hofmann 在 1999年 提出 
了 概率 隐 语义 模型 probabilistic Latent Semantic Analysis 简称 
pLSA 从 前面 LSA 的 介绍 可以 看出 虽然 具体 
的 优化 方法 使用 的 是 矩阵 分解 但是 从 
另一个 角度 来讲 我们/r 可以/c 认为/v 分解/v 后的U/nr 和V/nr 两个/m 
矩阵/n 中的/i 向量/n 分别 代表 文档 和词在/nr 隐 语义空间 中的 
表示 例如 一个 文档 的 隐 向量 表示 为 1 
2 0 T 代表 其 在 第一 维 隐 向量 
上 取值 为 1 第 二维 上 取值 为 2 
第 三维 上 取值 为 0 如果 这些 取值 能够 
构成 一个 概率分布 那么 不仅 模型 的 结果 更 利于 
理解 同时 还 会 带来 很多 优良 的 性质 这 
正是 pLSA 思想 的 核心 将 文档 和词的/nr 关系 看作 
概率分布 然后 试图 找出 这个 概率分布 来 有了 文档 和词的/nr 
概率分布 我们 就 可以 得到 一切 我们 想 要 得到 
的 东西 了 在 pLSA 的 基本 假设 中 文档 
d 和词w/nr 的 生成 过程 如下 以 P d 的 
概率 选择 文档 d 以 P z | d 的 
概率 选择 隐 类 z 以 P w | z 
的 概率 从z/nr 生成 w P z | d 和P/nr 
w | z 均为 多项式 分布 将 这个 过程 用 
联合 概率 进行 表达 得到 pLSA 的 生成 过程 可以 
看到 我们 将 隐 变量 z 作为 中间 桥梁 将 
文档 和词/nr 连接 了 起来 形成 了 一个 定义 良好 
环环相扣 的 概率 生成 链条 如 所示 虽然 pLSA 的 
核心 是 一种 概率模型 但是 同样 可以 用 类似 LSI 
的 矩阵 分解 形式 进行 表达 为此 我们 将 LSI 
中 等号 右边 的 三个 矩阵 进行 重新 定义 在 
这样 的 定义 下 原始 的 矩阵 C 仍然 可以 
表述 为 C = U ∑ VT 这样 的 对应 
关系 让 我们 更加 清晰 地 看到 了 前面 提到 
的 pLSA 在 概率 方面 的 良好 定义 和 清晰 
含义 同时 也 揭示 了 隐 语义 概率模型 和 矩阵 
分解 之间 的 密切 关系 关于 概率模型 和 矩阵 分解 
的 密切 关系 可 参考 这篇 文档 http / / 
www . cs . cmu . edu / ~ epxing 
/ Class / 10708 15 / slides / LDA _ 
SC . pdf 在 这样 的 定义 隐 变量 z 
所 代表 的 主题 含义 更加 明显 也 就是说 我们 
可以 明确 的 把 一个 z 看作 一个 主题 主题/n 
里/f 的/uj 词/n 和/c 文档/n 中/f 的/uj 主题/n 都/d 有着/v 
明确/ad 的/uj 概率/n 含义/n 也 正是 由于 这样 良好 的 
性质 再加 上 优化 方法 的 便捷 性 使得 从 
pLSA 开始 文本 主题 开始 在 各种 大 数据 应用 
中 占据 重要 地位 从 矩阵 的 角度 来看 LSA 
和 pLSA 看上去 非常 像 但是 它们 的 内涵 却 
有着 本质 的 不同 这 其中 最为 重要 的 一点 
就是 两者 的 优化 目标 是 完全 不同 的 LSA/w 
本质上/i 是/v 在/p 优化/vn SVD/w 分解/v 后的/nr 矩阵/n 和/c 原始/v 
矩阵/n 之间/f 的/uj 平方/q 误差/n 而 pLSA 本质上 是 在 
优化 似 然 函数 是 一种 标准 的 机器学习 优化 
套路 也 正是 由于 这 一点 本质 的 不同 导致 
了 两者 在 优化 结果 和 解释 能力 方面 的 
不同 至此 我们 看到 pLSA 将 LSA 的 思想 从 
概率分布 的 角度 进行 了 一大步 扩展 得到 了 一个 
性质 更加 优良 的 结果 但是 pLSA 仍然 存在 一些 
问题 主要 包括 由于 pLSA 为 每个 文档 生成 一组 
文 档级 参数 模型 中 参数 的 数量 随着 与 
文档 数 成正比 因此在 文档 数 较多 的 情况 下 
容易 过拟合 pLSA 将 每个 文档 d 表示 为 一组 
主题 的 混合 然而 具体 的 混合 比例 却 没有 
对应 的 生成 概率模型 换句话说 对于 不 在 训练 集中 
的 新 文档 pLSA 无法 给予 一个 很好 的 主题 
分布 简言之 pLSA 并非 完全 的 生成式 模型 而 LDA 
的 出现 就是 为了 解决 这些 问题 概率 的 概率 
生成式 概率模型 为了 解决 上面 提到 的 pLSA 存在 的 
问题 David Blei 等 人在 2003年 提出 了 一个 新 
模型 名为 隐 狄利克雷 分配 Latent Dirichlet Allocation 简称 LDA 
这个 名字 念起来 颇为 隐晦 而且 从 名字 上 似乎 
也 看不出 究 竟是 个 什么 模型 在 这里 我们 
试着 做 一种 可能 的 解读 Latent 这个词 不用 多 
说 是 说 这个 模型 仍然 是个 隐 语义 模型 
Dirichlet 这个词 是 在 说 该 模型 涉及到 的 主要 
概率 分布式 狄利克雷 分布 Allocation 这个词 是 在 说 这个 
模型 的 生成 过程 就是 在 使用 狄利克雷 分布 不断 
地 分配 主题 和词/nr 上面 并非 官方 解释 但/c 希望/v 
能对/nr 理解/v 这个/r 模型/n 能/v 起到/v 一些/m 帮助/v 作用/v LDA 
的 中心 思想 就是 在 pLSA 外面 又 包了 一层 
先验 使得/v 文档/n 中/f 的/uj 主题/n 分布/v 和/c 主题/n 下/f 
的/uj 词/n 分布/v 都/d 有了/i 生成/v 概率/n 从而 解决 了 
上面 pLSA 存在 的 非 生成式 的 问题 顺便 也 
减少 了 模型 中 的 参数 从而 解决 了 pLSA 
的 另外 一个 问题 在 LDA 中为 一篇 文档 di 
生成 词 的 过程 如下 从 泊松分布 中 抽样 一个 
数字 N 作为 文档 的 长度 这一步 并非 必须 也 
不 影响 后面 的 过程 从 狄利克雷 分布 Dir α 
中 抽样 一个 样本 θ i 代表 该 篇 文档 
下 主题 的 分布 从 狄利克雷 分布 Dir β 中 
抽样 一组 样本 Φ k 代表 每个 主题 下 词 
的 分布 对于 1 到 N 的 每个 词 wn 
从 多项式 分布 Multinomial θ i 中 抽样 一个 主题 
ci j 从 多项式 分布 Multinomial Φ i 中 抽样 
一个词 wi j LDA 的 生成 过程 忽略 掉 最 
开始 选择 文档 长度 的 步骤 我们 发现 LDA 的 
生成 过程 相比 pLSA 来讲 在/p 文档/n 到/v 主题/n 的/uj 
分布/v 和/c 主题/n 到/v 词/n 的/uj 分布/v 上面/f 都/d 加了/i 
一层/m 概率/n 使得 这 两者 都 加上 了 一层 不确定性 
从而 能够 很 自然地 容纳 训练 文档 中 没有 出现 
过 的 文档 和词/nr 这 使得 LDA 具有 了 比 
pLSA 更好 的 概率 性质 LDA/w 的/uj 应用/v 这/r 部分/n 
我们/r 介绍/v LDA/w 在/p 用作/v 相似/v 度/zg 计算/v 和/c 排序/n 
特征/n 时/n 需要/v 注意/v 的/uj 一些/m 地方/n 然后 介绍 以 
LDA 为 代表 的 文本 主题 在 推荐 系统 中 
更多 不同 角度 的 应用 相似 度 计算 上面 提到 
LSA 可以 直接 套 用到 VSM 中 进行 相似 度 
计算 在 LDA 中 也 可以 做 类似 的 计算 
具体 方法 是 把 文档 的 主题 分布 值 向 
量化 然后 用 余弦公式 进行 计算 但是 把 余弦 相似 
度 替换 为 KL divergence 或 Jensen – Shannon divergence 
效果 更好 原因 是 LDA 给出 的 主题 分布 是 
含义 明确 的 概率值 用 度量 概率 之间 相似 度 
的 方法 来 进行 度量 更为 合理 排序 特征 将 
物品 的 LDA 主题 作为 排序 模型 的 特征 是 
一种 很 自然 的 使用 方法 但 并 不是 所有 
的 主题 都 有用 物品 上 的 主题 分布 一般 
有 两种 情况 有 少数 主题 三个 或 更少 占据 
了 比较 大 的 概率 剩余 的 主题 概率 加 
起来 比较 小 所有 主题 的 概率值 都 差不多 都 
比较 小 在 第一 种 情况 下 只有 前面 几个 
概率 比 较大 的 主题 是 有用 的 而 在 
第二 种 情况 下 基本上 所有 的 主题 都 没有 
用 那么 该 如何 识别 这 两种 情况 呢 第 
一种 方法 可以 根据 主题 的 概率值 对 主题 做 
一个 简单 的 K Means 聚 类 K 选为 2 
如果 是 第一 种 情况 那么 两个 类 中 的 
主题 数量 会 相差 较大 一个 类 中 包含 少量 
有用 主题 另一个 类 包含 其他 无用 主题 而 第二 
种 情况 下 主题 数量 则 相差 不大 可以 用 
这种 方法 来 识别 主题 的 重要性 第二 种 方法 
可以 计算 主题 分布 的 信息 熵 第一 种 情况 
对应 的 信息 熵 会 比较 小 而 第二 种 
情况 会 比较 大 选取 合适 的 阈值 也 可以 
区分 这 两种 情况 物品 打 标签 & 用户 打 
标签 为 物品 计算出 其 对应 的 主题 以及 主题 
下面 对应 的 词 分布 之后 我们 可以 选取 概率 
最大 的 几个 主题 然后 从 这几个 主题 下 选取 
概率 最大 的 几个 词 作为 这个 物品 的 标签 
在此 基础 上 如果 用户 对 该 物品 发生 了 
行为 则 可以 将 这些 标签 传播 到 用户 身上 
这种方法 打出 的 标签 具有 非常 直观 的 解释 在 
适当 场景 下 可以 充当 推荐 解释 的 理由 例如 
我们 在 做 移动 端 个性化 推 送时 可供 展示 
文案 的 空间 非常 小 可以 通过 上面 的 方式 
先为 物品 打上 标签 然后 再 根据 用户 把 标签 
传播 到 用户 身上 在 推 送时 将 这些 标签 
词 同时 作为 召回 源 和 推荐 理由 让 用户 
明白 为什么 给 他 做出 这样 的 推荐 主题 & 
词 的 重要性 度量 LDA 训练 生成 的 主题 中 
虽然 都 有着 同等 的 位置 但是 其 重要性 却是 
各不相同 的 有的/nr 主题 包含 了 重要 的 信息 有的 
则 不然 例如 一个 主题 可能 包含 教育 读书 学校 
等 词 和 这样 主题 相关 的 文档 一般 来说 
是 和 教育 相关 的 主题 那么 这 就是 一个 
信息量 高的/nr 主题 相反 有的/nr 主题 可能 会 包含 第一册 
第二册 第三册 等 词 如果 在 一个 图书 销售 网站 
的 所有 图 书上 训练 LDA 就 有可能 得到 这样 
的 主题 因为/c 有/v 很多/m 套装/n 图书/n 都/d 包含/v 这样/r 
的/uj 信息/n 和 这样 主题 相关 的 文档 却 有可能 
是 任何 主题 这样 的 主题 就是 信息量 低 的 
主题 如何 区分 主题 是否 重要 呢 从 上面 的 
例子 中 我们 可以 得到 启发 重要 的 主题 不 
会到 处 出现 只 会 出现 在 小 部分 与 
之 相关 的 文档 中 而不 重要 的 主题 则 
可能 在 各种 文章 中 都 出现 基于 这样 的 
思想 我们 可以 使用 信息熵 的 方法 来 衡量 一个 
主题 中的 信息量 通过 对 LDA 输出 信息 做 适当 
的 变换 我们 可以 得到 主题 θ i 在 不同 
文档 中的 概率分布 然后 我们 对 这个 概率分布 计算 其 
信息熵 通俗 来讲 信息熵 衡量 了 一个 概率分布 中 概率值 
分散 程度 越 分散 熵 越大 越 集中 熵 越小 
所以 在 我们 的 问题 中 信息熵 越小 的 主题 
说明 该 主题 所 对应 的 文档 越少 主题 的 
重要性 越高 使用 类似 的 方法 我们 还 可以 计算 
词 的 重要性 在此 不再 赘述 更多 应用 除了 上面 
提到 的 LDA 还有 很多 其他 应用 甚至在 文本 领域 
以外 的 图像 等 领域 也 存在 着 广泛 应用 
LSA / pLSA / LDA 这些 主题 模型 的 核心 
基础 是 词 在 文档 中的 共 现 在此 基础 
上 才有 了 各种 概率分布 把握住 这个 核心 基础 就 
可以 找到 文本 主体 模型 的 更多 应用 例如 协同 
过滤 问题 中 基础 数据 也 是 用户 对 物品 
的 共同 行为 这也 构成 了 文本 主题 模型 的 
基础 因此 也 可以 使用 LDA 对 用户 对 物品 
的 行为 进行 建模 得到 用户 行为 的 主题 以及 
主题 下 对应 的 物品 然后 进行 物品 / 用户 
的 推荐 捕捉 上下文 信息 神经 概率 语言 模型 以 
LDA 为 代表 的 文本 主题 模型 通过 对词 的 
共 现 信息 的 分解 处理 得到 了 很多 有用 
的 信息 但是 pLSA / LDA 有 一个 很 重要 
的 假设 那 就是 文档 集合 中的 文档 以及 一篇 
文档 中的 词 在 选定 了 主题 分布 的 情况下 
都是/nr 相互 独立 可交换 的 换句话说 模型 中 没有 考虑 
词 的 顺序 以及 词 和词/nr 之间 的 关系 这种 
假设 隐含 了 两个 含义 在 生成 词 的 过程 
中 之前 生成 的 词 对接 下来 生成 的 词 
是 没有 影响 的 两篇 文档 如果 包含 同样 的 
词 但是 词 的 出现 顺序 不同 那么 在 LDA 
看来 他们 是 完全 相同 的 这样 的 假设 使得 
LDA 会 丢失 一些 重要 的 信息 而 近年来 得到 
关注 越来越 多 的 以 word2vec 为 代表 的 神经 
概率 语言 模型 恰好 在 这 方面 和 LDA 形成 
了 一定 程度 的 互补 关系 从而 可以 捕捉 到 
LDA 所 无法 捕捉 到 的 信息 word2vector 的 中心 
思想 用 一句话 来 讲 就是 A word is characterized 
by the company it keeps 一个词 的 特征 由 它 
周围 的 词 所 决定 这是 一句 颇有 哲理 的话 
很 像是 成语 中的 物以类聚 人以群分 具体来讲 词 向量 模型 
使用 周围 的 词 = 当前 词 或 当前 词 
= 周围 的 词 这样 的 方式 构造 训练样本 然后 
使用 神经 网络 来 训练 模型 训练 完成 之后 输入 
词 的 输入 向量 表示 便 成为 了 该词 的 
向量 表示 如 所示 这样 的 训练 方式 本质上 是 
在 说 如果 两个 词 具有 类似 的 上下文 上下文 
由 周围 的 词 组成 那么 这 两个 词 就会 
具有 类似 的 向量 表示 有了 词 的 向量 表示 
之后 我们 可以 做 很多 事情 最 常见 的 是 
将 这 一层 向量 表示 作为 更深 层次 模型 的 
一个 嵌入 层 除了 在 深度 学习 中 的 使用 
以外 在 推荐 系统 中 还 可以 做 很多 其他 
的 事情 其中 之一 就是 做 词 的 聚 类 
以及 寻找 相似 词 我们 知道 LDA 天然 就 可以 
做到 词 的 聚 类 和 相似 词 的 计算 
那么/r 使用/v word2vec/i 计算/v 出来/v 的/uj 结果/n 和/c LDA/w 有/v 
什么/r 不同/a 呢/y 它们 之间 的 不同 具体 体现 在 
两点 第一 是 聚 类 的 粒度 不同 LDA 关注 
的 主题 级别 的 粒度 层次 更高 而 词 向量 
关注 的 是 更低 层次 的 语法 语义 级别 的 
含义 例如 苹果 小米 和 三星 这三个 词 在 LDA 
方法 中 很 可能会 被 聚 类 在 一个 主题 
中 但是 在 词 向量 的 角度 来看 苹果 和 
小米 可能 会 具有 更高 的 相似 度 就像 乔布斯 
和 雷军 在 词 向量 下 的 关系 一样 所以在 
词 向量 中 可能会 有 vector 小米 vector 苹果 + 
vector 乔布斯 = vector 雷军 这样 的 结果 除此以外 由于 
word2vec 有着 根据 上下文 预测 当前 内容 的 能力 将其 
做 适当 修改 之后 还 可以 用来 对 用户 行为 
喜好 做出 预测 首先 我们 将 用户 的 行为 日志 
进行 收集 进行 session 划分 得到 类似 文本 语料 的 
训练 数据 在 这个 数据 上 训练 word2vec 模型 可以 
得到 一个 根据 上下文 行为 预测 当前 行为 的 模型 
但是 原始 的 行为 数据 中 行为 的 对象 常常 
是 id 级 的 例如 商品 视频 的 id 等等 
如果 直接 放到 模型 中 训练 会 造成 训练 速度慢 
泛化 能力差 等 问题 因此 需要 对 原始 行 为做 
降 维 具体 来说 可以 将 行为 映 射到 搜索词 
LDA Topic 类别 等等 低 维度 特征 上 然后 再 
进行 训练 例如 我们 可以 对 用户 的 搜索词 训练 
一个 word2vec 模型 然后 就 可以 根据 用户 的 历史 
搜索 行为 预测 他 的 下 一步 搜索 行为 并在 
此 基础 上 进行 推荐 这种方法 考虑到 了 上下文 但是 
对 前后 关系 并 没有 做 最 恰当 的 处理 
因为 word2vec 的 思想 是 根据 上下文 预测 当前 内容 
但 我们 希望 得到 的 模型 是 根据 历史 行为 
预测 下 一步 行为 这 两者 之间 有着 微妙 的 
差别 例如 用户 的 行为 序 列为 ABCDE 每个 字母 
代表 对 一个 物品 或 关键词 的 行为 标准 的 
word2vec 算法 可能会 构 造出 下面 这些 样本 AC → 
B BD → C CE → D 但是 我们 希望 
的 形式 其实 是 这样 的 AB → C BC 
→ D CD → E 因此 需要 对 word2vec 生成 
样本 的 逻辑 进行 修改 使其 只 包含 我们 需要 
的 单 方向 的 样本 方可 在 最终 模型 中 
得到 我们 真正 期望 的 结果 下面 是 按照 该 
方法 生成 的 一些 预测 例子 可以 看出 预测 搜索词 
都与 历史 搜索词 有着 紧密 的 关系 是 对 历史 
搜索词 的 延伸 例如 学生 书桌 和 烤肠 机 的 
例子 或者 细化 例如 小 龟 王和 西铁城 手表 的 
例子 具有 比 较好 的 预测 属性 是 非常 好 
的 推荐 策略 来源 沿着 这样 的 思路 我们 还 
可以 对 word2vec 作 进一步 修改 得到 对 时序 关系 
更为 敏感 的 模型 以及 尝试 使用 RNN LSTM 等 
纯 时序 模型 来 得到 更好 的 预测 结果 但 
由于 篇幅 所限 在此 不做 展开 行业 应用 现状 文本 
主题 模型 在 被 提出 之后 由于 其 良好 的 
概率 性质 以及 对 文本 数据 有 意义 的 聚 
类 抽象 能力 在 互联网 的 各个 行业 中 都 
取得 了 广泛 的 应用 搜索 巨头 Google 在其 系统 
的 各个 方面 都在/nr 广泛 使用 文本 主题 模型 并 
为此 开发 了 大规模 文本 主题 系统 Rephil 例如 在 
为 用户 搜索 产生 广告 的 过程 中 就 使用 
了 文本 主题 来 计算 网页 内容 和 广告 之间 
的 匹配度 是 其 广告 产品 成功 的 重要 因素 
之一 此外 在 匹配 用户 搜索词 和 网页 间 关系 
的 时候 文本 主题 也 可用 来 提高 匹配 召回率 
和 准确性 Yahoo 也 在其 搜索 排序 模型 中 大量 
使用 了 LDA 主题 特征 还 为此 开源 了 著名 
的 Yahoo LDA 工具 在 国内 文本 主题 最 著名 
的 系统 当属 腾讯 开发 的 Peacock 系统 该 系统 
可以 捕捉 百万 级别 的 文本 主题 在 腾讯 的 
广告 分类 网页 分类 精准 广告 定向 QQ 群 分类 
等 重要 业务 上 均 起着 重要 的 作用 该 
系统 使用 的 HDP Hierarchical Dirichlet Process 模型 是 LDA 
模型 的 一个 扩展 可 智能 选择 数据 中 主题 
的 数量 还 具有 捕捉 长尾 主题 的 能力 除了 
腾讯 以外 文本 主题 模型 在 各 公司 的 推荐 
搜索 等 业务 中 也 已经 在 广泛 使用 使用 
方法 根据 各自 业务 有所不同 以 word2vec 为 代表 的 
神经 网络 模型 近年来 的 使用 也 比较 广泛 典型 
的 应用 如 词 的 聚 类 近义词 的 发现 
quer y 的 扩展 推荐 兴趣 的 扩展 等 Facebook 
开发 了 一种 word2vec 的 替代 方案 FastText 该 方案 
在 传统 词 向量 的 基础 上 考虑 子 词 
subword 的 概念 取得 了 比 word2vec 更好 的 效果 
  总结/n 和/c 展望/v 我们/r 从/p 简单/a 的/uj 文本/n 关键词/n 
出发/v 沿着 结构化 降 维 聚 类 概率 时序 的 
思路 结合 推荐 系统 中 候 选集 召回 相关性 计算 
排序 模型 特征 等 具体 应用 介绍 了 推荐 系统 
中 一些 常用 的 自然 语言 处理 技术 和 具体 
应用 方法 自然语言 处理 技术 借着 深度 学习 的 东风 
近年来 取得 了 长足 的 进步 而其 与 推荐 系统 
的 紧密 关系 也 意味着 推荐 系统 在 这 方面 
仍然 有着 巨大 的 提升 空间 让 我们 拭目以待 自然语言 
处理 NLP 是 计算机 科学 人工智能 语言学 关注 计算机 和 
人类 自然 语言 之间 的 相互 作用 的 领域 自然语言 
处理 是 计算机 科学 领域 与 人工智能 领域 中 的 
一个 重要 方向 它/r 研究/vn 能/v 实现/v 人/n 与/p 计算机/n 
之间/f 用/p 自然/d 语言/n 进行/v 有效/a 通信/l 的/uj 各种/r 理论/n 
和/c 方法/n 自然语言 处理 是 一门 融 语言学 计算机科学 数学 
于 一体 的 科学 因此 这一 领域 的 研究 将 
涉及 自然语言 即 人们 日常 使用 的 语言 所以 它 
与 语言学 的 研究 有着 密切 的 联系 但又 有 
重要 的 区别 自然语言 处理 并 不是 一般 地 研究 
自然语言 而在于 研制 能 有效 地 实现 自然 语言 通信 
的 计算机 系统 特别 是 其中 的 软件 系统 因而 
它 是 计算机 科学 的 一部分 自然语言 处理 技术 是 
所有 与 自然 语言 的 计算机 处理 有关 的 技术 
的 统称 其 目的 是 使 计算机 理解 和 接受 
人类 用 自然 语言 输入 的 指令 完成 从 一种 
语言 到 另一种 语言 的 翻译 功能 自然语言 处理 技术 
的 研究 可以 丰富 计算机 知识 处理 的 研究 内容 
推动 人工智能 技术 的 发展 大 快 NLP 模块 是 
大 快 大 数据 一体化 平台 的 一个 组件 用户 
引用 该组 件 可以 有效 进行 自然 语言 的 处理 
工作 如 进行 文章 摘要 语义 判别 以及 提高 内容 
检索 的 精确度 和 有效性 自然语言 处理 如今 不仅 作为 
人工智能 核心 课题 来 研究 而且 也 作为 新一代 计算机 
的 核心 课题 来 研究 从 知识 产业 角度看 专家系统 
数据库 知识库 计算机 辅助 设计 系统 CAD 计算机 辅助 教学 
系统 CAI 计算机 辅助 决策 系统 办公室 自动化 管理 系统 
智能 机器人 等 都 需要 用 自然 语言 处理 具有 
篇章 理解 能力 的 自然 语言 理解 系统 可 用于 
机器 自动 翻译 情报检索 自动 标引 自动 文摘 自动 写 
故事 小说 等 领域 都 可以 用 我们 的 工具 
类 DKNLPBase 来 处理 标准 分词 方法 签名 List Term 
t a n d a r d T o k 
e n i z e r . segment String txt 
返回 分词 列表 签名 参数 说明 txt 要 分词 的 
语句 范例 下例 验证 一段话 第 5个 分词 是 阿 
法狗/nr public void testSegment throws Exception { String text = 
商品 和 服务 List Term termList = DKNLPBase . segment 
text assertEquals 商品 termList . get 0 . word assertEquals 
和 termList . get 1 . word assertEquals 服务 termList 
. get 2 . word text = 柯杰/nr 解说 李世石 
VS 阿 法狗/nr 第二局 结局 竟 是 这样 termList = 
DKNLPBase . segment text assertEquals 阿 法狗/nr termList . get 
5 . word   / / 能够 识别 阿 法狗/nr 
} 关键词 提取 方法 签名 List String   extractKeyword String 
txt int keySum 返回 关键词 列表 . 签名 参数 说明 
txt 要 提取 关键词 的 语句 keySum 要 提取 关键词 
的 数量 范例 给出 一段话 提取 一个 关键词 是 程序员 
public void t e s t E x t r 
a c t K e y w o r d 
throws Exception { String content = 程序员 英文 Programmer 是 
从事 程序开发 维护 的 专业 人员 + 一般 将 程序员 
分为 程序 设计 人员 和 程序 编码 人员 + 但 
两者 的 界限 并不 非常 清楚 特别 是 在 中国 
+ 软件 从业 人员 分为 初级 程序员 高级 程序员 系统 
+ 分析员 和 项目 经理 四大 类 List String keyword 
= DKNLPBase . extractKeyword content 1 assertEquals 1 keyword . 
size assertEquals 程序员 keyword . get 0 } 短语 提取 
方法 签名 List String extractPhrase String txt int phSum 返回 
短语 签名 参数 说明 txt 要 提取 短语 的 语句 
phSum 短语 数量 范例 给出 一段 文字 能 代表 文章 
的 五个 短语 第一 个 短语 是 算法 工程师 迈进 
二十一 世纪 我们 已经 进入 了 以 互联网 为 主要 
标志 的 海量 信息 时代 这些 海量 信息 大 部分 
是 以 自然 语言 表示 的 一方面 海量 信息 也为 
计算机 学习 人类 语言 提供 了 更多 的 素材 另一方面 
这 也为 自然语言 处理 提供 了 更加 宽广 的 应用 
舞台 例如 作为 自然语言 处理 的 重要 应用 搜索引擎 逐渐 
成为 人们 获取 信息 的 重要 工具 涌现出 以 百度 
谷歌 等 为 代表 的 搜索引擎 巨头 机器翻译 也从 实验室 
走入 寻常 百姓家 谷歌 百度/n 等/u 公司/n 都/d 提供/v 了/ul 
基于/p 海量/n 网络/n 数据/n 的/uj 机器/n 翻译/v 和/c 辅助/vn 翻译/v 
工具/n 基于 自然语言 处理 的 中文 输入 法如 搜狗 微软 
谷歌 等 输入法 成为 计算机 用户 的 必备 工具 带有 
语音 识别 的 计算机 和 手机 也 正 大行其道 协助 
用户 更 有效 地 工作 学习 总之 随着 互联网 的 
普及 和 海量 信息 的 涌现 自然语言 处理 正在 人们 
的 日常 生活 中 扮演 着 越来越 重要 的 作用 
然而 我们 同时 面临 着 一个 严峻 事实 那 就是 
如何 有效 利用 海量 信息 已 成为 制约 信息技术 发展 
的 一个 全局性 瓶颈 问题 自然语言 处理 无可 避免 地 
成为 信息 科学 技术 中 长期 发展 的 一个 新的 
战略 制高点 同时 人们 逐渐 意识到 单纯 依靠 统计 方法 
已经 无法 快速 有效 地 从 海量 数据 中 学习 
语言 知识 只有 同时 充分 发挥 基于 规则 的 理性主义 
方法 和 基于 统计 的 经验 主义 方法 的 各自 
优势 两者 互相 补充 才 能够 更好 更快 地 进行 
自然语言 处理 自然语言 处理 作为 一个 年龄 尚 不足 一个 
世纪 的 新兴 学科 正在 进行 着 突飞猛进 的 发展 
回顾 自然语言 处理 的 发展 历程 并 不是 一帆风顺 有过 
低谷 也 有过 高潮 而 现在 我们 正 面临 着 
新的 挑战 和 机遇 例如 目前 网络 搜索 引擎 基本上 
还 停留 在 关键词 匹配 缺乏 深 层次 的 自然 
语言 处理 和 理解 语音识别 文字 识别 问答 系统 机器 
翻译 等 目前 也 只能 达到 很 基本 的 水平 
路漫漫其修远兮 自然语言 处理 作为 一个 高度 交叉 的 新兴 学科 
不论是 探究 自然 本质 还是 付诸 实际 应用 在将来/i 必定会/l 
有/v 令人/l 期待/v 的/uj 惊喜/a 和/c 异常/d 快速/d 的/uj 发展/vn 
本文 对 参加 圆桌会议 嘉宾 的 观点 进行 了 整理 
云孚/nr 科技 专注 于为/nr 企业 提供 自然语言 处理 技术 解决 
方案 创始人 兼 CEO 张文斌 商业化 的 本质 首先 一定 
要 盈利 其次 要 做到 规模化 盈利 人工智能 这 一块 
的 创业 公司 要 盈利 大 方向 有 两个 做 
toC 直接 面对 消费者 和做/nr toB 面对 企业 云孚/nr 科技 
选择 了 toB 周期 相对 可控 比较 容易 把 营收 
做起来 规模化 盈利 又有 两个 方向 创业 公司 选择 最多 
的 是 深入 行业 做 垂直 行业 的 应用 这样 
可以 规模化 做 特定 行业 用户 而且 可以 做 大 
订单 也是 投资人 比较 认可 的 方向 我们 还 看到 
另外 一个 方向 就是 做 基础 技术 平台 因为 它 
足够 基础 可以 面向 多个 行业 的 客户 提供 产品 
订单 相对 比较 小 但 客户 数 更广 竹 间 
智能 主要 是 做 情感 计算 不 只做 文本 情感 
还 做 语音 情绪 和 表情 目前 主要 在 金融 
电商 IOT 领域 运营商 等 领域 落地 CTO 翁嘉颀/nr 目前 
人工智能 必须 从 单个 到 单独 的 领域 去 突破 
去 那个 领域 先 收集 语料 以及 到底 要 解决 
什么 问题 针对 这 一类 问题 我 能 解决 哪些 
神州 泰岳 大 数据 VP 张瑞 飞 讲到 商业 落地 
我们 要 解决 几 个 矛盾 第一 个 矛盾 是 
人工智能 现在 处于 初级 阶段 尤其 在 认知 科学 领域 
在 初级 阶段 要 落地 就 意味着 你 要 管 
客户 收钱 第二 个 矛盾 是 我们 要 解决 成本 
和 实际 达成 成效 之间 的 平衡 理论上 讲 只要 
你 投入 足够 多 的 成本 人工智能 的 效果 就 
会 更好 一些 但是 它 又有 最佳值 我们 要 找到 
最佳值 在哪儿 第三 是 我们 要 解决 在 算法 工程 
和 基础 算法 之间 的 选择 我们 在 算法 工程 
中 要 解决 的 问题 和 我们 在 基础 算法 
中 进行 的 优化 研究 结合 起来 准/v 是/v 你/r 
能/v 使用/v 的/uj 非常/d 高的/nr 影响力/n 它/r 也是/i 能/v 要到/v 
钱的/nr 基础/n 薄言/n RSVP . ai 公司 的 初衷 就是 
希望 让 机器 了解 语言 自动 帮 人做 一些 关于 
语言 的 事情 落地 的 技术 难点 和 应用 难点 
NLP 技术 目前 处于 相对 不 太 成熟 的 阶段 
也 是因为 它 的 任务 非常 多 且 复杂 分词 
相对 比较 成熟 测试 语料 上 准确率 可以 做到 97% 
以上 就算 分词 这么 成熟 的 技术 落到 特定 行业 
面对 一些 新词 效果 还是 不 足够 理想 还 需要 
花 很多 精力 去做 针对 特定 领域 去做 优化 目前 
NLP 在 短 文本 短句 15 个字 以内 意图 理解 
意思 理解 可能 没有 什么 问题 长 文本 目前 还 
不 太行 自然语言 处理 属于 认知 智能 范畴 所以 自然 
语言 处理 的 终极 目标 是 理解 人类 的 思维 
和 想法 在 这个 过程 中 我们 需要 拿捏 尺度 
这个 尺度 即把 机器 智能 和 人类 智慧 融合 在 
一起 而 不是 用 机器 智能 取代 人类 智慧 目前 
自然语言 处理 的 问题 是 如果 我 把 算法 做得 
特别 深 往往 有 普适性 的 问题 如果 做 平台 
往往 做 深入 就会 有困难 这 是 第一 点 第二点 
拿 语义 相似 度 短 文本 来讲 一些 领域 它们 
的 训练 语 量 没有 那么 多 但又/i 有/v 各自/r 
领域/n 自己/r 的/uj 知识/v 和/c 特点/n 这个 时候 通用 的 
数据 集 怎么 达到 更好 的 效果 第三点 是 在 
商业化 中 自然语言 处理 跟 CV 领域 还 有 一个 
差别 大家 很多 时候 是 用 大量 的 LSTM Model 
等 哪怕 Attention 才是 你 真正 需要 的 东西 但 
实际上 它 还是 时序 模型 如果 我 把 它 放到 
线上 系统 时延 是 有 问题 的 而且 成本 非常 
高 所以 自然 语言 处理 商业化 也 要 考虑 成本 
问题 需要 重点 解决 的 问题 短 文本 语义 相似 
度 实际上 在 语义上 Q & A 两句话 不是 完全 
相等 的 此外 我们 在 方法 上 还有 一个 维度 
的 区别 现在 很多 服务 是 基于 搜索 的 技术 
现在 业内 也 有 一些 新的 评价 方式 即在 平行 
文本 做 评价 两种 评价 都 达到 很好 的 效果 
但是 如果 做 交叉 总会 有 损失 这个 问题 对 
我们 来讲 是 比较 棘手 的 问题 要 解决 标注 
好 的 数据 用户 使用 过程 中 反馈 的 数据 
如何 做好 对话 控制 如何 限制 用户 讲话 方式 这 
真的 是 一个 技巧 因为 用户 乱讲 一通 你 是 
没有 办法 理解 的 怎么样 在 没有 标注 语料 或 
很少 的 标注 语料 的 情况 下 就能 把 他们 
想要 的 结构化 信息 抽 取出来 抽取 完 再 构建 
这个 行业 的 知识 图谱 我们 也 积累 了 一些 
经验 一般 可以 先用 已有 的 通用 系统 结合 基于 
规则 的 方法 先 做 一版 系统 出来 这样 可以 
先跑 一个 初步 结果 从中 挑 一 部分 比较 严重 
的 badcase 出来 人工 标注 语料 再 重新 训练 模型 
如此 反复 迭代 最终 可以 花比/nr 较小 的 人力 标注 
成本 把 系统 迁移 到 其他 领域 当然 刚才 说 
的 这个 过程 还 比较 理想 怎么样 用 尽可能少 的 
标注 语料 可以 快速 迁移 领域 这 是 我们 实际 
工作 中 碰到 的 一个 非常 实际 的 问题 学术界/n 
的/uj 前沿/s 研究/vn 对于/p 企业/n 的/uj 产业/n 化/n 落地/n 能/v 
得到/v 哪些/r 借鉴/v 和/c 思考/v 没有 一个 单一 的 算法 
能够 解决 好 问题 可能 要看 四 五十篇 Paper 从 
里面 融合 出 一个 方法 所以 每 一个 算法 每一个 
Paper 都有 它 可取 的 地方 比如 其中 四个 算法 
告诉 我 他 要 退货 一个 算法 告诉 我 他 
要 换货 我会 用 投票 的 方式 比较 有 机会 
真正 落地 解决 问题 因为 算法 有弱点 用 多个 算法 
去做 能 互相 弥补 缺陷 不能 评价 哪个 算法 好 
与 坏 因为 算法 要看 适用 场地 适用 你 的 
应用 场景 的 方法 就是 最好 的 方法 学术界 的 
诉求 跟 工业界 还是 不 一样 学术界 追求 理论上 模型 
上 的 创新 如果 有 重复 了 就 需要 构思 
下 一个 新的 模型 而 创业 公司 是 把 他们 
探索 出来 的 模型 拿过 来试 我们 的 核心 是 
效果 导向 其次 真正 用 这些 算法 做 预测 时 
我们 还 得 考虑 它 的 性能 在 正式 场合 
包括 它 需要 的 硬件 条件 是否 符合 业务 需求 
这也 是 我们 落地 时要/nr 考虑 的 因素 有的 算法 
虽然 非常 高大 上 可能 高出 0.1 或 0.2 个 
点 但 它 的 速度 慢 了 很多 对 硬件 
要求 特别 高 我们 就 会 有所 取舍 采用 更加 
实用 的 算法 论 文中 的 数据 集 往往 跟 
我们 面临 的 问题 是 不 一样 的 所以/c 非常/d 
重要/a 的/uj 是/v 公司/n 内部/f 要/v 有/v 自己/r 的/uj 测试/vn 
集/q 和/c 标准/n 对于 新 的 方法 能够 快速 适应 
和 尝试 参考 链接 https / / www . leiphone 
. com / news / 201807 / JHAwVgSYvCfKZQLm . html 
杂谈 自然语言 处理 怎么 最快 入门 刘知远 的 回答 知乎 
https / / www . zhihu . com / question 
/ 19895141 / answer / 24710071 刘老师 主要 是 从 
ACL 等 著名 NLP 相关 会议 的 角度 来 介绍 
如何 跟进 NLP 领域 的 研究 国内 大陆 部分 NLP 
团队 NLP 自然语言 处理 界 有 哪些 神 级 人物 
jiangfeng 的 回答 知乎 https / / www . zhihu 
. com / question / 32318281 / answer / 55588123 
有关 NLP 的 比赛 砍手 豪 的 文章 知乎 https 
/ / zhuanlan . zhihu . com / p / 
33901181 系列 教程 李航 的 统计 学习 方法 第 4 
10 11 章 Coursera 的 一门 课 Natural Language Processing 
已经 下架 有人 整理 http / / www . 52nlp 
. cn / tag / nlp % E 5% 85% 
A 5% E 9% 97% A8CS224 深度 学习 自然语言 处理 
教程 B 站上 爱 可可 老师 已 分享 带 英文 
字幕 的 视频 https / / www . bilibili . 
com / video / av13383754from = search & seid = 
4 7 2 1 9 6 2 9 3 2 
0 8 3 5 3 6 9 1 3 工具 
包 著名 工具 NLTK 结巴 Word2Vec Gensim SpaCy 目前 常用 
的 自然 语言 处理 开源 项目 / 开发包 有 哪些 
刘知远 的 回答 知乎 https / / www . zhihu 
. com / question / 19929473 / answer / 90201148 
待 更新 目录 非 结构化 数据 获取 原 语言 处理 
文本 挖掘 文本 提取 和 查询 以便 进行 改进 的 
搜索 统计 语言 处理 技术 资产 支持 自然语言 处理 NLP 
问答 系统开发 自然语言 处理 NLP 正 迅速 成为 现代 组织 
获得 竞争 优势 的 基本 技能 它 已 成为 许多 
新 业务 功能 的 重要 工具 从/p 聊天/nz 机器人/n 和/c 
问答/v 系统/n 到/v 情感/n 分析/vn 合规性 监控 医疗/n 见解/v 以及/c 
非/h 结构化/n 和半/nr 结构化/n 内容/n 的/uj BI/w 和/c 分析/vn 考虑 
所有 可以 带来 重要 见解 的 非 结构化 内容   
查询 电子邮件 通信 社交 媒体 视频 客户 评论 客户 支持 
请求 等 自然语言 处理 NLP 工具 和 技术 有助于 处理 
分析 和 理解 非 结构化 的 大 数据 为了 有效 
和 积极 地 运作 我们/r 的/uj  /i 自然语言/l 处理/v 服务/vn 
涵盖/v 了/ul 从/p 数据/n 采集/v 和/c 处理/v 到/v 分析/vn 实体 
提取 事实 提取 和 问答 系统 考虑 专为 您 的 
企业 构建 的 数字 助理 的 各种 需求 非 结构化 
数据 获取 十 多年来 我们/r 帮助/v 组织/v 从/p 外部/f 和/c 
内部/f 资源/n 中/f 获取/v 非/h 结构化/n 内容/n 以 进行 搜索 
和 分析 我们 的 顾问 在 识别 和 提取 数据 
方面 经验 丰富 可 通过 互联网 上 的 付费 和 
免费 内容 来源 获取 可 下载 的 数据 Search Technologies 
  与 流行 业务 内容 存储库 的 安全 连接器 原 
语言 处理 由于 原始数据 因 来自 不同 来源 而异 因此 
一旦 获取 内容 我们 会 提供 数据 清理 和 格式化 
服务 以 确保 您 的 数据 准备好 以 获得 最高 
质量 的 结果 确定 格式 例如 PDF XML HTML 等 
提取 文本 内容 识别 并 删除 不 相关 的 部分 
常见 的 页眉 页脚 侧边栏 样板 识别 差异 和 变化 
提取 编码 的 元数据 令牌 提取 规范化 和 清理 短语 
提取 文本 挖掘 文本 提取 和 查询 以便 进行 改进 
的 搜索 在 许多 用 例 中 内容 以 自然 
语言 例如 英语 中文 西班牙语 等 写下 但 不方便 标记 
我们/r 有/v 工具/n 和/c 技术/n 来/v 帮助/v 您/zg 从此/c 内容/n 
中/f 提取/v 信息/n 可以 利用 某些 级别 的 文本 挖掘 
文本 提取 或 可能 的 完整 NLP 典型 的 全文 
提取 包括 实体 提取   例如 公司 人员 金额 关键 
举措 等 内容 分类   正面 或 负面 例如 情绪 
分析   按 功能 意图 或 目的   或 按 
行业 或 其他 类别 进行 分析 和 趋势 分析 内容 
聚 类   识别 话语 的 主要 主题 和/或/nr 发现 
新 主题 事实 提取   使用 结构化 信息 填充 数据库 
以 进行 分析 可视化 趋势 分析 和 警报 关系 提取 
  填写 图形 数据库 以 探索 现实 世界 的 关系 
统计 语言 处理 在 许多 NLP 项目 中 统计 技术 
可以 提供 对 整个 文档 的 一般 理解 我们 工作 
过 的 示例 统计 处理 用 例 包括 聚 类 
分类 相似 主题 分析 词 云 概要 技术 资产 支持 
自然语言 处理 NLP 洞察力 驱动 的 企业 越来越 多 地 
寻求 利用 庞大 的 非 结构化 数据 来 加速 和 
改善 业务 成果 但是 现有 的 自然 语言 处理 技术 
并 不能 满足 企业 的 需求 它们 太 狭隘 聊天 
机器人 太 浅薄 和 通用 基于 云的/nr 自然 语言 处理 
解决 方案 或者 开发 部署 和 维护 成本 太高 作为 
技术 资产 收集 的 一部分 Saga Natural Language Understanding NLU 
是 一个 可扩展 经济 高效 且 易于 使用 的 框架 
填补 了 现有 NLP / NLU 技术 的 空白 问答 
系统开发 问答 系统 也 称为 Insight Engines   由 Gartner 
创造 的 一个 术语 解析 自然 语言 问题 的 查询 
然后 与 后端 系统集成 以 提供 直接 答案 而 不仅仅 
是 包含 关键字 的 结果 列表 可以 使用 搜索 技术 
的 自然 语言 处理 工具包 结合 一套 先进 且 可扩展 
的 自然 语言 处理 工具 来 构建 问答 系统 该 
工具 可以 执行 查询 理解 所需 的 所有 必要 功能 
我们 的 NLP 工具 包括 符号化 缩写 标准化 词形 还原 
句子 和 短语 边界 实体 提取 所有 类型 但不 统计 
统计 短语 提取 问题 模式识别 统计 消 歧 对 行动 
回应 的 问答 业务 用户界面 见下文 NLP 问答 系统 的 
好处 许多/m 业务/n 用户界面/n 可/v 用于/v 输入/v 和/c 维护/v 实体/n 
和/c 模式/n 这些/r 接口/v 允许/v 没有/v 编程/n 经验/n 的/uj 业务/n 
用户/n 输入/v 和/c 维护/v 公共/b 实体/n 和/c 问题/n //i 响应/v 
模式/n 程序员 干预 只 需要 与 后端 系统集成 答案 可以 
从 关系数据库 RESTful API 到 任何 业务 系统 或 从 
搜索引擎 结果 中 提取 根据 您 的 要求 答案 可以 
格式 化为 自然语言 响应 或 图表 报告 或 交互式 图形 
前言 该 系列 博客 是 对 斯坦福 CS224n 系列 课程 
的 学习 笔记 主要 用于 记录 课程 主要 知识 加深 
个人 理解 另外 此 系列 博客 只 记录 每一 讲 
中 个人 认为 重要 的 内容 不 包含 全部内容 文章 
若 有错误 之处 请 各路 大神 批评指正 文章 目录 前言 
什么 是 自然 语言 处理 NLP NLP 的 层次 NLP 
的 应用 什么 是 深度 学习 为什么 发展 深度 学习 
为什么 NLP 很难 关于 Deep NLP 总结 什么 是 自然 
语言 处理 NLP 自然语言 处理 是 计算机 科学 人工智能 以及 
语言学 的 交叉 学科 其 目标 是 为了 让 计算机 
能够 处理 或者 理解 人类 语言 完成 有 意义 的 
任务 如 订票 购物 等 NLP/w 的/uj 层次/m NLP/w 任务/n 
的/uj 输入/v 主要/b 有/v 语音/n 和/c 文本/n 其 对应 的 
第一 级 分别 就是 语音 识别 和 OCR 或 分词 
接下来 是 形态学 然后 就是 句法分析 和 语义 理解 由于 
仅仅 理解 单词 或 词语 的 意思 无法 准确 理解 
句子 的 含义 所以 句法分析 和 语义 理解 是 NLP 
学习 中 的 重点 该 课程 主要 关注 图中 画圈 
的 两个 部分 语音信号 分析 是 NLP 领域 第一个 大显身手 
的 领域 但该 课程 的 重中之重 还是 在 第二 部分 
NLP 的 应用 NLP 中的 应用 多种多样 大概 可以 从 
难易 程度 进行 划分 如下 初 级任务 拼 写检查 关键词 
搜索 查找 同义词 等中 级任务 阅读 信息 理解 文本 提取 
特定 信息 文本 分类 比如 判断 文章 阅读 难度 或 
目标 受众 推 文是/nr 正面 还是 负面 的 高级 任务 
机器翻译 口语 对话 系统 智能 问答 等 什么 是 深度 
学习 深度 学习 是 机器 学习 的 一个 分支 其 
基本 思想 就是 让 电脑自动 学习 而 不是 手工 写 
代码 告诉 电脑 该 做什么 在 传统 机器学习 中 面对 
一个 应用 问题 人类 需要 具备 专业 的 背景 知识 
然后 人工 设计 特征 把 设计 好 的 特征 交给 
机器学习 算法 让 机器 完成 权值 优化 的 工作 在 
这个 过程 中 所谓 的 机器学习 并 没有 让 机器 
真正 的 学习 而 只是 帮助 人类 完成 了 最后 
的 优化 工作 反而 是 人类 学习 到 更多 领域 
知识 课程 中 给出 的 一张 图 很好 的 表示 
了 这种 关系 而 深度 学习 作为 表征 学习 的 
一部分 其 学习 的 是 原始 输入 的 多层 特征 
表示 为什么 发展 深度 学习 深度 学习 是 在 上世纪 
八 九十 年代 提出 的 但是 在 2010年 才 逐渐 
崛起 究其 原因 主要 有 两个 方面 一 方面 是 
随着 社会 的 发展 出现 了 能够 发挥 深度 学习 
优势 的 海量 数据 和 支撑 深度 学习 计算 的 
硬件 能力 另一方面 是 深度 学习 不 需要 人工 设计 
特征 同时 在 各种 任务 上 取得 了 突飞猛进 的 
成果 为什么 NLP 很难 人类 语言 模棱两可 不像 编程语言 一样 
具有 明确 的 变量 另外 人类 语言 会 省略 大量 
信息 但 是 人类 在 接受 语言 时会 根据 自身 
的 背景 知识 进行 补充 课程 中 举了 几个 很 
有意思 的 例子 此处 列出 一个 The Pope s baby 
steps on gays 人类 对于 这 句话 的 理解 通常 
为 教皇 在 同性恋 问题 上 裹足不前 但是 对于 没有 
背景 知识 的 机器 来说 可能 将 这 句话 理解 
成 教皇 的 孩子 踩 了 基佬 这 就是 语言 
的 歧义 性 所在 所以 说 自然语言 处理 是 人工智能 
皇冠 上 的 明珠 其 困难性 不言而喻 关于 Deep NLPDeep 
NLP 就是 将 深度 学习 的 思想 用 于 自然 
语言 处理 的 任务 中 比如 机器翻译 情感 分析 客服 
系统 等 用于 解决 实际 中 存在 的 问题 目前 
深度 学习 在 自然 语言 处理 任务 中 表现 很好 
另外 课程 中 还 简单 介绍 了 NLP 领域 常见 
的 词 向量 这在 后续 的 课程 中 还会 详细 
讲解 本篇 博客 不再 赘述 总结 CS224n 第一 讲 主要 
从顶至/nr 下 的 介绍 了 NLP 算是 为 初学者 打开 
一 扇 大门 的 入门课程 下一 讲 是 对 向量 
表示 的 讲解 难度 相较 于此 篇 博客 深入 希望 
通过 博客 的 记录 能够 加深 自己 对 NLP 领域 
知识 的 掌握 和 理解 NLP 自然语言 指 一种 随着 
社会 发展 而 自然 演化 的 语言 即 人们 日常 
交流 所 使用 的 语言 自然语言 处理 通过 技术 手段 
使用 计算机 对 自然 语言 进行 各种 操作 的 一个 
学科 NLP 研究 的 内容 词意 消 歧 指代 理解 
自动 生成 语言 机器翻译 人机对话 系统 文本 含义 识别 NLP 
处理 语料 读入 网络 本地 分词 # / usr / 
bin / env python # * coding utf 8 * 
# @ Time 2018 9 28 22 21 # @ 
Author Manu # @ Site # @ File python _ 
base . py # @ Software PyCharm import urllib from 
nltk import word _ tokenize from bs4 import BeautifulSoup # 
在线 文档 下载 url = http / / www . 
gutenberg . org / files / 2554/2554 0 . txt 
res = urllib . request . urlopen url raw = 
res . read . decode utf8 print length len raw 
print type type raw print raw 100 # 分词 tokens 
= word _ tokenize raw print tokens 50 print length 
+ str len tokens print type type tokens # 创建 
文本 text = nltk . Text tokens print type type 
text print length len text print text 基于 此 单位 
的 文本 分析 正则表达式 分割 断句 分词 规范化 输出 中文分词 
及 相应 算法 基于 字典 词库 匹配 正向 最大 匹配 
逆向 最大 匹配 双向 最大 匹配 设立 切 分表 执法 
最佳 匹配 基于 词频 度 统计 N gram 模型 隐 
马尔科夫 模型 基于 字 标注 的 中文分词 方法 基于 知识 
理解 分词 方法 比较 结巴 分词 安装 在 控制 台 
使用 pip install jieba 即可 安装 功能 分词 jieba . 
cut jieba . cut _ for _ search 添加 自定义 
词典 jieba . load _ userdict file _ name add 
_ word word freq = None tag = None jieba 
. del _ word word jieba . suggest _ freq 
segmen tune = True 关键词 提取 jieba . analyse . 
extract _ tags sentence topK = 20 withWeight = False 
allowPOS = jieba . analyse . set _ idf _ 
path file _ name jieba . analuse . set _ 
stop _ words file _ name 词性 标注 jieba . 
tokenize jieba . posseg . cut 并行 分词 词汇 搜索 
自然语言 处理 怎么 最快 入门 最好 是 方法 与 教程 
关注 者 5552 被 浏览 2520543 条 评论 分享 邀请 
回答 关注 问 题写 回答 27   个 回答 默认 
排序 微软 亚洲 研究院 专注 科研 18年 盛产 黑 科技 
收录于 编辑 推荐 知乎 圆桌     524 人 赞同 
了 该回 答谢 邀 针对 这个 问题 我们 邀请 了 
微软 亚洲 研究院 首席 研究员 周明 博士 为 大家 解答 
周明/nr 博士/n 于/p 2016年/tdq 12月/tdq 当选/v 为/p 全球/n 计算/v 语言学/n 
和/c 自然/d 语言/n 处理/v 研究/vn 领域/n 最/d 具/v 影响力/n 的/uj 
学术/n 组织/v 计算 语言学 协会 ACL Association for Computational Linguistics 
的 新 一届 候任 主席 此外 他 还是 中国 计算机 
学会 中文 信息 技术 专委会 主任 中国 中文信息 学会 常务 
理事 哈工大 天津大学 南开大学 山东 大学 等 多所 学校 博士 
导师 他 1985年 毕业 于 重庆 大学 1991年 获 哈工大 
博士学位 1991 1993年 清华大学 博士后 随后 留校 任 副教授 1996 
1999 访问 日本 高电社/nr 公司 主持 中日 机器翻译 研究 他 
是 中国 第一 个 中英 翻译 系统 日本 最 有名 
的 中日 机器翻译 产品 J 北京 的 发明人 1999年 加入 
微软 研究院 并 随后 负责 自然语言 研究组 主持 研制 了 
微软 输入法 对联 英 库 词典 中英 翻译 等 著名 
系统 近年来 与 微软 产品组 合作 开发 了 小冰 中国 
Rinna 日本 等 聊天 机器人 系统 他 发表 了 100余 
篇 重要 会议 和 期刊论文 拥有 国际 发明 专利 40 
余项 这里 是 正式 回答 的 分割线 自然语言 处理 简称 
NLP 是 研究 计算机 处理 人类 语言 的 一门 技术 
包括 1 . 句法 语义分析 对于 给定 的 句子 进行 
分词 词性 标记 命名 实体 识别 和 链接 句法分析 语义 
角色 识别 和 多义词 消 歧 2 . 信息 抽取 
从 给定 文本 中 抽取 重要 的 信息 比如 时间 
地点 人物 事件 原因 结果 数字 日期 货币 专有名词 等等 
通俗 说来 就是 要 了解 谁在 什么 时候 什么原因 对谁 
做 了 什么 事 有 什么 结果 涉及 到 实体 
识别 时间 抽取 因果关系 抽取 等 关键 技术 3 . 
文本 挖掘 或者 文本 数据挖掘 包括 文本 聚 类 分类 
信息 抽取 摘要 情感 分析 以及 对 挖掘 的 信息 
和 知识 的 可视化 交互式 的 表达 界面 目前 主流 
的 技术 都是/nr 基于 统计 机器 学习 的 4 . 
机器翻译 把 输入 的 源语言 文本 通过 自动 翻译 获得 
另外 一种 语言 的 文本 根据 输入 媒介 不同 可以 
细分 为 文本 翻译 语音 翻译 手语 翻译 图形 翻译 
等 机器翻译 从 最早 的 基于 规则 的 方法 到 
二十年前 的 基于 统计 的 方法 再到 今天 的 基于 
神经网络 编码 解码 的 方法 逐渐 形成 了 一套 比较 
严谨 的 方法 体系 5 . 信息检索 对 大 规模 
的 文档 进行 索引 可 简单 对 文档 中 的 
词汇 赋 之以 不同 的 权重 来 建立 索引 也 
可利用 1 2 3 的 技术 来 建立 更加 深层 
的 索引 在 查询 的 时候 对 输入 的 查询 
表达式 比如 一个 检索 词 或者 一个 句子 进行 分析 
然后 在 索引 里面 查找 匹配 的 候选 文档 再 
根据 一个 排序 机制 把 候选 文档 排序 最后 输出 
排序 得分 最高 的 文档 6 . 问答 系统 对 
一个 自然 语言 表达 的 问题 由 问答 系统 给 
出 一个 精准 的 答案 需要 对 自然 语言 查询 
语句 进行 某种 程度 的 语义分析 包括 实体 链接 关系 
识别 形成 逻辑 表达式 然后 到 知识库 中 查找 可能 
的 候选 答案 并 通过 一个 排序 机制 找出 最佳 
的 答案 7 . 对话 系统 系统 通过 一 系列 
的 对话 跟 用户 进行 聊天 回答 完成 某一 项 
任务 涉及 到 用户 意图 理解 通用 聊天 引擎 问答 
引擎 对话 管理 等 技术 此外 为了 体现 上下文 相关 
要 具备 多轮 对话 能力 同时 为了 体现 个性化 要 
开发 用户 画像 以及 基于 用户 画像 的 个性化 回复 
随着 深度 学习 在 图像 识别 语音识别 领域 的 大放异彩 
人们 对 深度 学习 在 NLP 的 价值 也 寄予厚望 
再 加上 AlphaGo 的 成功 人工智能 的 研究 和 应用 
变得 炙手可热 自然语言 处理 作为 人工智能 领域 的 认知 智能 
成为 目前 大家 关注 的 焦点 很多 研究生 都在/nr 进入 
自然语言 领域 寄望 未来 在 人工智能 方向 大展身手 但是 大家 
常常 遇到 一些 问题 俗话说 万事开头难 如果 第一 件 事情 
成功 了 学生 就 能 建立 信心 找到 窍门 今后 
越做/nr 越好 否则 也 可能 就 灰心丧气 甚至 离开 这个 
领域 这里 针对 给出 我 个人 的 建议 希望 我 
的 这些 粗浅 观点 能够 引起 大家 更 深层次 的 
讨论 建议 1 如何 在 NLP 领域 快速 学会 第一个 
技能 我 的 建议 是 找到 一个 开 源 项目 
比如 机器翻译 或者 深度 学习 的 项目 理解 开源 项目 
的 任务 编译 通过 该 项目 发布 的 示范 程序 
得到 与 项目 示范 程序 一致 的 结果 然后 再 
深入 理解 开源 项目 示范 程序 的 算法 自己 编程 
实现 一下 这个 示范 程序 的 算法 再 按照 项目 
提供 的 标准 测试 集 测试 自己 实现 的 程序 
如果 输出 的 结果 与 项目 中 出现 的 结果 
不 一致 就要 仔细 查验 自己 的 程序 反复 修改 
直到 结果 与 示范 程序 基本一致 如果 还是 不行 就 
大胆 给 项目 的 作者 写信 请教 在此 基础 上 
再 看看 自己 能否 进一步 完善 算法 或者 实现 取 
得比 示范 程序 更好 的 结果 建议 2 如何 选择 
第一 个 好 题目 工程型 研究生 选题 很多 都是/nr 老师 
给定 的 需要 采取 比较 实用 的 方法 扎扎实实 地 
动手 实现 可能 不 需要 多少 理论 创新 但是 需要 
较强 的 实现 能力 和 综合 创新 能力 而 学术 
型 研究生 需要 取得 一流 的 研究 成果 因此 选题 
需要 有 一定 的 创新 我 这里 给 出 如下 
的 几点 建议 先 找到 自己 喜欢 的 研究 领域 
你 找到 一本 最近 的 ACL 会议 论文集 从中 找到 
一个 你 比较 喜欢 的 领域 在 选题 的 时候 
多 注意 选择 蓝海 的 领域 这 是因为 蓝海 的 
领域 相对 比较 新 容易 出 成果 充分 调研 这个 
领域 目前 的 发展 状况 包括 如下 几 个 方面 
的 调研 方法 方面 是否/v 有/v 一套/m 比较/d 清晰/a 的/uj 
数学/n 体系/n 和/c 机器学习/i 体系/n 数据 方面 有/v 没有/v 一个/m 
大家/n 公认/v 的/uj 标准/n 训练/vn 集/q 和/c 测试/vn 集/q 研究 
团队 是否/v 有/v 著名/a 团队/n 和/c 人士/n 参加/v 如果 以 
上 几个 方面 的 调研 结论 不是 太 清晰 作为 
初学者 可能 不要 轻易 进入 在 确认 进入 一个 领域 
之后 按照 建议 一 所述 需要 找到 本 领域 的 
开源 项目 或者 工具 仔细 研究 一遍 现有 的 主要 
流派 和 方法 先 入门 反复 阅读 本 领域 最新 
发表 的 文章 多 阅读 本 领域 牛人 发表 的 
文章 在 深入 了解 已有 工作 的 基础 上 探讨 
还有 没有 一些 地方 可以 推翻 改进 综合 迁移 注意 
做 实验 的 时候 不要 贪多 每次 实验 只需要 验证 
一个 想法 每次 实验 之后 必须 要 进行 分析 存在 
的 错误 找出 原因 对 成功 的 实验 进一步 探讨 
如何 改进 算法 注意 实验 数据 必须 是 业界 公认 
的 数据 与 已有 的 算法 进行 比较 体会 能够 
得出 比较 一般性 的 结论 如果 有 则 去 写 
一篇 文章 否则 应该 换 一个 新 的 选题 建议 
3 如何写 出 第一 篇 论文 接 上 一个 问题 
如果 想法 不错 且 被 实验 所 证明 就可 开始 
写 第一 篇 论文 了 确定 论文 的 题目 在 
定 题目 的 时候 一般 不要 系统 研究 与 实践 
要 避免 太长 的 题目 因为 不好 体现 要点 题目 
要 具体 有 深度 突出 算法 写 论文 摘要 要 
突出 本文 针对 什么 重要 问题 提出 了 什么 方法 
跟 已有 工作 相比 具 有 什么 优势 实验 结果 
表明 达到 了 什么 水准 解决 了 什么 问题 写 
引言 首先 讲出 本 项 工作 的 背景 这个 问题 
的 定义 它 具有 什么 重要性 然后 介绍 对 这个 
问题 现有 的 方法 是 什么 有 什么 优点 但是 
注意 但是 现有 的 方法 仍然 有 很多 缺陷 或者 
挑战 比如 注意 比如 有 什么 问题 本文 针对 这个 
问题 受 什么 方法 谁 的 工作 之 启发 提出 
了 什么 新的 方法 并 做了 如下 几 个 方面 
的 研究 然后 对 每个 方面 分门别类 加以 叙述 最后 
说明 实验 的 结论 再说 本文 有 几条 贡献 一般 
写 三条 足矣 然后 说说 文章 的 章节 组织 以及 
本文 的 重点 有的 时候 东西 太多 篇幅 有限 只能 
介绍 最 重要 的 部分 不 需要 面面俱到 相关 工作 
对 相关 工作 做 一个 梳理 按照 流派 划分 对 
主要 的 最多 三个 流派 做 一个 简单 介绍 介绍 
其 原理 然后 说明 其 局限性 然后 可 设立 两 
个 章节 介绍 自己 的 工作 第一 个 章节 是 
算法 描述 包括 问题 定义 数学 符号 算法 描述 文章 
的 主要 公式 基本 都在/nr 这里 有时候 要 给出 简明 
的 推导 过程 如果 借鉴 了 别人 的 理论 和 
算法 要给 出 清晰 的 引文 信息 在此 基础 上 
由于 一般 是 基于 机器学习 或者 深度 学习 的 方法 
要 介绍 你 的 模型 训练 方法 和 解码 方法 
第二章 就是 实验 环节 一般 要 给出 实验 的 目的 
要 检验 什么 实验 的 方法 数据 从 哪里 来 
多 大 规模 最好 数据 是 用 公开 评 测数据 
便于 别人 重复 你 的 工作 然后 对 每个 实验 
给出 所需 的 技术 参数 并 报告 实验 结果 同时 
为了 与 已有 工作 比较 需要 引用 已有 工作 的 
结果 必要 的 时候 需要 重现 重要 的 工作 并 
报告 结果 用 实验 数据 说话 说明 你 比 人家 
的 方法 要好 要对 实验 结果 好好 分析 你 的 
工作 与 别人 的 工作 的 不同 及 各自 利弊 
并 说明 其 原因 对于 目前 尚 不太好 的 地方 
要 分析 问题 之 所在 并将 其 列为 未来 的 
工作 结论 对 本文 的 贡献 再一次 总结 既要 从 
理论 方法 上 加以 总结 和 提炼 也要 说明 在 
实验上 的 贡献 和 结论 所做 的 结论 要让 读者 
感到 信服 同时 指出 未来 的 研究 方向 参考文献 给出 
所有 重要 相关 工作 的 论文 记住 漏掉 了 一篇 
重要 的 参考 文献 或者 牛人 的 工作 基本上 就 
没有 被 录取 的 希望 了 写完 第一稿 然后 就是 
再改 三遍 把 文章 交给 同一个 项目组 的 人士 请 
他们 从 算法 新颖 度 创新性/n 和/c 实验/vn 规模/n 和/c 
结论/n 方面/n 以 挑剔 的 眼光 审核 你 的 文章 
自己 针对 薄弱环节 进一步 改进 重点 加强 算法 深度 和 
工作 创新 性 然后 请 不同 项目组 的 人士 审阅 
如果 他们 看 不明白 说明 文章 的 可读性 不够 你 
需要 修改 篇章 结构 进行 文字 润色 增加 文章 可读性 
如 投 ACL 等 国际 会议 最好 再请 英文 专业 
或者 母语 人士 提炼 文字 这里 是 回答 结束 的 
分割线 感谢 大家 的 阅读 本 帐 号为 微软 亚洲 
研究院 的 官方 知乎 帐号 本 帐号 立足 于 计算机 
领域 特别 是 人工智能 相关 的 前沿 研究 旨在 为 
人工智能 的 相关 研究 提供 范例 从 专业 的 角度 
促进 公众 对 人工智能 的 理解 并为 研究 人员 提供 
讨论 和 参与 的 开放 平台 从而 共建 计算机 领域 
的 未来 微软 亚洲 研究院 的 每 一位 专家 都是/nr 
我们 的 智囊团 你 在 这个 帐 号 可以 阅读 
到 来自 计算机 科学 领域 各个 不同 方向 的 专家 
们 的 见解 请 大家 不要 吝惜 手里 的 邀请 
让 我们 在 分享 中 共同 进步 编辑 于 /nr 2017 
03 06524 23 条 评论 分享 收藏 感谢 收起 刘知远 
自然语言 处理 深度 学习 Deep Learning 机器学习   话题 的 
优秀 回答者 收录于 知乎 圆桌     463 人 赞同 
了 该 回答 曾经 写过 一篇 小文 初学者 如何 查阅 
自然语言 处理 NLP 领域 学术 资料 _ zibuyu _ 新浪 
博客 也许 可以 供 你 参考 昨天 实验室 一位 刚 
进 组 的 同学 发 邮件 来问 我 如何查找 学术论文 
这 让 我 想起 自己 刚 读 研究生 时 茫然 
四顾 的 情形 看着 学长 们 高谈阔论 领域 动态 却 
不知 如何 入门 经过 研究生 几年 的 耳濡目染 现在 终于 
能 自信 地 知道 去 哪儿 了解 最新 科研 动态 
了 我 想 这 可能 是 初学者 们 共通 的 
困惑 与其 只 告诉 一个 人 知道 不如 将 这些 
Folk Knowledge 写下来 来 减少 更多 人 的 麻烦 吧 
当然 这个 总结 不过 是 一家 之谈 只 盼 有人 
能 从中 获得 一点点 益处 受 个人 认知 所限 难免 
挂一漏万 还望 大家 海涵 指正 1 . 国际 学术 组织 
学术 会议 与 学术 论文 自然语言 处理 natural language processing 
NLP 在 很大 程度 上 与 计算 语言学 computational linguistics 
CL 重合 与 其他 计算机 学科 类似 NLP / CL 
有 一个 属于 自己 的 最 权威 的 国际 专业 
学会 叫做 The Association for Computational Linguistics ACL URL ACL 
Home Page 这个 协会 主办 了 NLP / CL 领域 
最 权威 的 国际 会议 即 ACL 年会 ACL 学会 
还会在 北美 和 欧洲 召开 分 年会 分别 称为 NAACL 
和 EACL 除此之外 ACL 学会 下设 多个 特殊 兴趣小组 special 
interest groups SIGs 聚集 了 NLP / CL 不同 子 
领域 的 学者 性质 类似 一个 大学 校园 的 兴趣 
社团 其中 比较 有名 的 诸如 SIGDAT Linguistic data and 
corpus based approaches to NLP SIGNLL Natural Language Learning 等 
这些 SIGs 也会 召开 一些 国际 学术 会议 其中 比较 
有名 的 就是 SIGDAT 组织 的 EMNLP Conference on Empirical 
Methods on Natural Language Processing 和 SIGNLL 组织 的 CoNLL 
Conference on Natural Language Learning 此外 还有 一个 International Committee 
on Computational Linguistics 的 老牌 NLP / CL 学术 组织 
它 每 两年 组织 一个 称为 International Conference on Computational 
Linguistics COLING 的 国际 会议 也是 NLP / CL 的 
重要 学术 会议 NLP / CL 的 主要 学术 论文 
就 分布 在 这些 会议 上 作为 NLP / CL 
领域 的 学者 最大 的 幸福 在于 ACL 学会 网站 
建立 了 称作 ACL Anthology 的 页面 URL ACL Anthology 
支持 该 领域 绝大部分 国际 学术 会议 论文 的 免费 
下载 甚至 包含 了 其他 组织 主办 的 学术 会议 
例如 COLING IJCNLP 等 并 支持 基于 Google 的 全文 
检索 功能 可谓 一站 在手 NLP 论文 我 有 由于 
这个 论文 集合 非常 庞大 并且 可以 开放 获取 很多 
学者 也 基于 它 开展 研究 提供 了 更 丰富 
的 检索 支持 具体 入口 可以 参考 ACL Anthology 页面 
上方 搜索框 右侧 的 不同 检索 按钮 与 大部分 计算机 
学科 类似 由于 技术 发展 迅速 NLP / CL 领域 
更 重视 发表 学术 会议 论文 原因 是 发表 周期短 
并 可以 通过 会议 进行 交流 当然 NLP / CL 
也有 自己 的 旗舰 学术期刊 发表 过 很多 经典 学术论文 
那 就是 Computational Linguistics URL MIT Press Journals 该 期刊 
每期 只有 几 篇文章 平均/a 质量/n 高于/nr 会议/n 论文/nz 时间 
允许 的话 值得 及时 追踪 此外 ACL 学会 为了 提高 
学术 影响力 也 刚刚 创办 了 Transactions of ACL TACL 
URL Transactions of the Association for Computational Linguistics ISSN 2307 
387X 值得 关注 值得一提的是 这 两份 期刊 也都 是 开放 
获取 的 此外 也 有 一些 与 NLP / CL 
有关 的 期刊 如 ACM Transactions on Speech and Language 
Processing ACM Transactions on Asian Language Information Processing Journal of 
Quantitative Linguistics 等等 根据 Google Scholar Metrics 2013年 对 NLP 
/ CL 学术 期刊 和 会议 的 评价 ACL EMNLP 
NAACL COLING LREC Computational Linguistics 位于 前 5位 基本 反映 
了 本 领域 学者 的 关注 程度 NLP / CL 
作为 交叉学科 其 相关 领域 也 值得 关注 主要 包括 
以下 几个 方面 1 信息检索 和 数据挖掘 领域 相关 学术 
会议 主要 由 美国 计算机 学会 ACM 主办 包括 SIGIR 
WWW WSDM 等 2 人工智能 领域 相关 学术 会议 主要 
包括 AAAI 和 IJCAI 等 相关 学术 期刊 主要 包括 
Artificial Intelligence 和 Journal of AI Research 3 机器学习 领域 
相关 学术 会议 主要 包括 ICML NIPS AISTATS UAI 等 
相关 学术 期刊 主要 包括 Journal of Machine Learning Research 
JMLR 和 Machine Learning ML 等 例如 最近 兴起 的 
knowledge graph 研究 论文 就有/i 相当/d 一/m 部分/n 发表/v 在/p 
人工/n 智能/n 和/c 信息检索/n 领域/n 的/uj 会议/n 和/c 期刊/n 上/f 
实际上 国内 计算机 学会 CCF 制定 了 中国 计算机 学会 
推荐 国际 学术 会议 和 期刊目录 CCF 推荐 排名 通过 
这个 列表 可以 迅速 了解 每个 领域 的 主要 期刊 
与 学术 会议 最后 值得一提的是 美国 Hal Daum é III 
维护 了 一个 natural language processing 的 博客 natural language 
processing blog 经常 评论 最新 学术 动态 值得 关注 我 
经常 看 他 关于 ACL NAACL 等 学术 会议 的 
参会 感想 和对/nr 论文 的 点评 很 有启发 另外 ACL 
学会 维护 了 一个 Wiki 页面 ACL Wiki 包含 了 
大量 NLP / CL 的 相关 信息 如 著名 研究 
机构 历届 会议录 用率 等等 都是 居家 必备 之 良 
品 值得 深挖 2 . 国内 学术 组织 学术 会议 
与 学术 论文 与 国际 上 相似 国内 也 有一个 
与 NLP / CL 相关 的 学会 叫做 中国 中文信息 
学会 URL 中国 中文信息 学会 通过 学会 的 理事 名单 
中国 中文信息 学会 基本 可以 了解 国内 从事 NLP / 
CL 的 主要 单位 和 学者 学会 每年 组织 很多 
学术会议 例如 全国 计算 语言学 学术会议 CCL 全国 青年 计算 
语言学 研讨会 YCCL 全国 信息检索 学术会议 CCIR 全国 机器翻译 研讨会 
CWMT 等等 是 国内 NLP / CL 学者 进行 学术 
交流 的 重要 平台 尤其 值得一提的是 全国 青年 计算 语言学 
研讨会 是 专门 面向 国内 NLP / CL 研究生 的 
学术 会议 从 组织 到 审稿 都由 该 领域 研究生 
担任 非常 有 特色 也是 NLP / CL 同学们 学术交流 
快速 成长 的 好去处 值得一提的是 2010年 在 北京 召开 的 
COLING 以及 2015年 即将 在 北京 召开 的 ACL 学会 
都是/nr 主要 承办者 这也 一定 程度 上 反映 了 学会 
在 国内 NLP / CL 领域 的 重要 地位 此外 
计算机 学会 中文 信息 技术 专委会 组织 的 自然 语言 
处理 与 中文 计算 会议 NLP & CC 也是 最近 
崛起 的 重要 学术 会议 中文信息 学会 主编 了 一份 
历史 悠久 的 中文信息 学报 是 国内 该 领域 的 
重要 学术 期刊 发表 过 很多 篇 重量级 论文 此外 
国内 著名 的 计算机 学报 软件 学报 等 期刊 上 
也 经常 有 NLP / CL 论文 发表 值得 关注 
过去 几年 在 水木 社区 BBS 上 开设 的 AI 
NLP 版面 曾经 是 国内 NLP / CL 领域 在线 
交流 讨论 的 重要 平台 这几年 随着 社会 媒体 的 
发展 越来越 多 学者 转战 新浪 微博 有 浓厚 的 
交流 氛围 如何 找到 这些 学者 呢 一个 简单 的 
方法 就是 在 新浪 微博 搜索 的 找人 功能 中 
检索 自然语言 处理 计算 语言学 信息检索 机器学习 等 字样 马上 
就能 跟 过去 只 在 论文 中 看到 名字 的 
老师 同学们 近距离 交流 了 还有 一种 办法 清华大学 梁斌 
开发 的 微博 寻人 系统 清华大学 信息检索 组 可以 检索 
每个 领域 的 有 影响力 人士 因此 也 可以 用来 
寻找 NLP / CL 领域 的 重要 学者 值得一提的是 很多 
在 国外 任教 的 老师 和 求学 的 同学 也 
活跃 在 新浪 微博 上 例如 王威廉 Sina Visitor System 
李沐/nr Sina Visitor System 等 经常 爆料 业内 新闻 值得 
关注 还有 国内 NLP / CL 的 著名 博客 是 
52nlp 我 爱 自然 语言 处理 影响力 比较 大 总之 
学术研究 既 需要 苦练内功 也 需要 与 人 交流 所谓 
言者无意 听者有心 也许 其他人 的 一句话 就能 点醒 你 苦思 
良久 的 问题 无疑 博客 微博 等 提供 了 很好 
的 交流 平台 当然 也 注意 不要 沉迷 哦 3 
. 如何 快速 了解 某 个 领域 研究 进展 最后 
简单 说 一下 快速 了解 某 领域 研究 进展 的 
经验 你 会 发现 搜索引擎 是 查阅 文献 的 重要 
工具 尤其 是 谷歌 提供 的 Google Scholar 由于 其 
庞大 的 索引 量 将 是 我们 披荆斩棘 的 利器 
当 需要 了解 某 个 领域 如果 能 找到 一篇 
该 领域 的 最新 研究 综述 就 省劲 多了 最 
方便 的 方法 还是 在 Google Scholar 中 搜索 领域 
名称 + survey / review / tutorial / 综述 来 
查找 也 有 一些 出版社 专门 出版 各 领域 的 
综述 文章 例如 NOW Publisher 出版 的 Foundations and Trends 
系列 Morgan & Claypool Publisher 出版 的 Synthesis Lectures on 
Human Language Technologies 系列 等 它们 发表 了 很多 热门 
方向 的 综述 如 文档 摘要 情感 分析 和 意见 
挖掘 学习 排序 语言 模型 等 如果 方向 太 新 
还 没有 相关 综述 一般 还 可以 查找 该 方向 
发表 的 最新 论文 阅读 它们 的 相关 工作 章节 
顺着 列出 的 参考 文献 就 基本 能够 了解 相关 
研究 脉络 了 当然 还有 很多 其他 办法 例如 去 
http / / videolectures . net 上看 著名 学者 在 
各大 学术 会议 或 暑期学校 上 做 的 tutorial 报告 
去 直接 咨询 这个 领域 的 研究 者 等等 编辑 
于 /nr 2015 07 16463 17 条 评论 分享 收藏 感谢 
收起 知乎 用户 自然语言 处理   话题 的 优秀 回答者 
收录于 编辑 推荐     1267 人 赞同 了 该 
回答 推荐 数学 之美 这个 书写 得 特别 科普 且 
生动 形象 我 相信 你 不会 觉得 枯燥 这个 我 
极力推荐 我 相信 科研 的 真正 原因 是 因为 兴趣 
而 不是 因为 功利 的 一些 东西 接下 来说 统计 
自然语言 处理 基础 这本书 这 书 实在 是 太 老了 
但是 也很 经典 看不看 随意 了 现在 自然语言 处理 都 
要靠 统计学 知识 所以 我 十分 十分 推荐 统计 学习 
方法 李航 的 李航 老师 用 自己 课余时间 7年 写 
的 而且有 博士生 Review 的 自然语言 处理 和 机器学习 不同 
机器学习 依靠 的 更多 是 严谨 的 数学 知识 以及 
推倒 去 创造 一个 又 一个 机器学习 算法 而 自然 
语言 处理 是 把 那些 机器学习 大牛 们 创造 出来 
的 东西 当 Tool 使用 所以 入门 也 只是 需要 
涉猎 而已 把 每个 模型 原理 看看 不一定 细致 到 
推倒 宗 成庆 老师 的 统计 自然语言 处理 第二 版 
非常好 ~ 中文信息处理 丛书 统计 自然语言 处理 第 2版 蓝 
色皮 的 ~ ~ ~ 然后 就是 Stanford 公开课 了 
Stanford 公开课 要求 一定 的 英语 水平 | Coursera 我 
觉得 讲 的 比 大量 的 中国 老师好 ~ 举例 
http / / www . ark . cs . cmu 
. edu / LS2 / in . . . 或者 
http / / www . stanford . edu / class 
/ cs . . . 如果 做 工程 前先 搜索 
有没有 已经 做好 的 工具 不要 自己 从头来 做 学术 
前 也要 好好 的 Survey 开始 推荐 工具包 中文 的 
显然 是 哈工大 开源 的 那个 工具包 LTP Language Technology 
Platform developed by HIT SCIR 哈尔滨 工业 大学 社会 计算 
与 信息检索 研究中心 . 英文 的 python pattern simpler to 
get started than NLTKchardet character encoding d e t e 
c t i o n p y e n c 
h a n t easy access to d i c 
t i o n a r i e s s 
c i k i t learn has support for text 
c l a s s i f i c a 
t i o n u n i d e c 
o d e because ascii is much easier to deal 
with 希望 可以 掌握 以下 的 几个 tool CRF + 
+ GIZAWord2Vec 还 记得 小时候 看过 的 数码宝贝 每个 萌萌 
哒 的 数码宝贝 都会 因为 主人 身上 发生 的 一些 
事情 而 获得 进化 能力 其实 在 自然 语言 处理 
领域 我 觉得 一切 也 是 这样 ~ 我 简单 
的 按照 自己 的 见解 总结 了 每个 阶段 的 
特征 以及 提高 的 解决 方案 1 . 幼年 体 
自然语言 处理 好 屌 我 什么 都 不会 但是 好想 
提高 建议 去看 公开课 ~ 去做 Kaggle 的 那个 情感 
分析题 2 . 成长期 觉得 简单 模型 太 Naive 高大 
上 的 才是 最好 的 这个 阶段 需要 自己 动手 
实现 一些 高级 算法 或者说 常用 算法 比如 LDA 比如 
SVM 比如 逻辑 斯蒂 回归 并且 拥抱 Kaggle 知道 trick 
在 这个 领域 的 重要性 3 . 成熟期 高大 上 
的 都不 work 通过 特征 工程 加 规则 才 work 
大部分 人 应该 都在/nr 这个 级别 吧 包括 我 自己 
我 总是 想 进化 但 积累 还是 不够 觉得 高大 
上 的 模型 都是/nr 一些 人 为了 paper 写 的 
真正 的 土 方法 才是 重剑 无 锋 大巧不工 在 
这个 阶段 应该 就是 不断 读 论文 不断 看 各种 
模型 变种 吧 什么 句子 相似 度 计算 word2vec cosine 
已经 不再 适合 你 了 4 . 完 全体 在 
公开 数据 集上 把 某个 高大 上 的 模型 做 
work 了 ~ 这类 应该 只有 少数 博士 可以 做到 
吧 我 已经 不 知道 到 了 这个 水平 再 
怎么 提高 了 ~ 是不是 只能 说 不忘 初心 方得 
始终 5 . 究 极体 参见 Micheal Jordan Andrew Ng 
. 好好 锻炼身体 保持 更 长久 的 究 极体 形态 
希望 可以 理解 自然语言 处理 的 基本 架构 ~ 分词 
= 词性 标注 = ParserQuora 上 推荐 的 NLP 的 
论文 摘自 Quora 我 过 一阵 会 翻译 括号 里面 
的 解释 Parsing 句法结构 分析 ~ 语言学 知识 多 会 
比较 枯燥 Klein & Manning Accurate Unlexicalized Parsing Klein & 
Manning Corpus Based Induction of Syntactic Structure Models of Dependency 
and Constituency 革命性 的 用 非 监督 学习 的 方法 
做了 parser Nivre Deterministic Dependency Parsing of English Text shows 
that deterministic parsing actually works quite well McDonald et al 
. Non Projective Dependency Parsing using Spanning Tree Algorithms the 
other main method of dependency parsing MST parsing Machine Translation 
机器翻译 如果 不 做 机器翻译 就 可以 跳过 了 不过 
翻译 模型 在 其他 领域 也 有 应用 Knight A 
statistical MT tutorial workbook easy to understand use instead of 
the original Brown paper Och The Alignment Template Approach to 
Statistical Machine Translation foundations of phrase based systems Wu Inversion 
Transduction Grammars and the Bilingual Parsing of Parallel Corpora arguably 
the first realistic method for biparsing which is used in 
many systems Chiang Hierarchical Phrase Based Translation significantly improves accuracy 
by allowing for gappy phrases Language Modeling 语言 模型 Goodman 
A bit of progress in language modeling describes just about 
everything related to n gram language models 这 是 一个 
survey 这个 survey 写了 几乎 所有 和n/nr gram 有关 的 
东西 包括 平滑 聚 类 Teh A Bayesian interpretation of 
Interpolated Kneser Ney shows how to get state of the 
art accuracy in a Bayesian framework opening the path for 
other applications Machine Learning for NLPSutton & McCallum An introduction 
to conditional random fields for relational learning CRF 实在 是 
在 NLP 中 太好 用了 而且/c 我们/r 大家/n 都/d 知道/v 
有/v 很多/m 现成/v 的/uj tool/w 实现/v 这个/r 而 这个 就是 
一个 很 简单 的 论文 讲述 CRF 的 不过 其实 
还是 蛮 数学 = = Knight Bayesian Inference with Tears 
explains the general idea of bayesian techniques quite well Berg 
Kirkpatrick et al . Painless Unsupervised Learning with Features this 
is from this year and thus a bit of a 
gamble but this has the potential to bring the power 
of discriminative methods to unsupervised learning Information ExtractionHearst . Automatic 
Acquisition of Hyponyms from Large Text Corpora . COLING 1992 
. The very first paper for all the bootstrapping methods 
for NLP . It is a hypothetical work in a 
sense that it doesn t give experimental results but it 
influenced it s followers a lot . Collins and Singer 
. Unsupervised Models for Named Entity Classification . EMNLP 1999 
. It applies several variants of co training like IE 
methods to NER task and gives the motivation why they 
did so . Students can learn the logic from this 
work for writing a good research paper in NLP . 
Computational SemanticsGildea and Jurafsky . Automatic Labeling of Semantic Roles 
. Computational Linguistics 2002 . It opened up the trends 
in NLP for semantic role labeling followed by several CoNLL 
shared tasks dedicated for SRL . It shows how linguistics 
and engineering can collaborate with each other . It has 
a shorter version in ACL 2000 . Pantel and Lin 
. Discovering Word Senses from Text . KDD 2002 . 
Supervised WSD has been explored a lot in the early 
00 s thanks to the senseval workshop but a few 
system actually benefits from WSD because manually crafted sense mappings 
are hard to obtain . These days we see a 
lot of evidence that unsupervised clustering improves NLP tasks such 
as NER parsing SRL etc 其实 我 相信 大家 更 
感兴趣 的 是 上层 的 一些 应用 ~ 而不 是 
如何 实现 分词 如何 实现 命名 实体 识别 等等 而且 
应该 大家 更 对 信息检索 感兴趣 不过 自然语言 处理 和 
信息检索 还是 有所 区别 的 So ~ ~ ~ 我 
就不 在这边 写 啦 编辑 于 /nr 2015 10 061.3 K 
29 条 评论 分享 收藏 感谢 收起 知乎 用户 228 
人 赞同 了 该 回答 不请自来 语言学 背景 研 二 
废话 不说 直接 上货 书籍 篇 入门 书籍 挺多 的 
我 也 看过 不少 1 数学 之美 吴军 这是 我 
看 的 第一 本 关于 NLP 的 书 现 在 
第二 版 出来了 貌似 新增 了 两章 内容 还没 看过 
第一版 写 的 挺好 科普 性质 看完 对于 nlp 的 
许多 技术 原理 都有 了 一点 初步 认识 现在 没事 
还会 翻翻 的 2 自然语言 处理 简明 教程 冯志伟 冯志伟 
老师 这 本书 偏向 于 语言学 书 略 厚 关于 
语言学 的 东西 很多 都是 很容易 理解 的 东西 建议 
没有 学过 理工科 们 翻 一翻 毕竟 nlp 这 东西 
未来 趋势 可能会 融合 不少 语言学 的 东西 3 自然语言 
处理 综论 Daniel Jurafsky 这本书 也是 冯志伟 老师 翻译 的 
翻译 的 挺棒 看了 差不多 一半 综论 性质 的 选 
感兴趣 的 章节 翻翻 就行 作者 是 Daniel Jurafsky 在 
coursera 上面 有他的/nr 课程 后面 视频 篇 里 集中 谈 
4 自然语言 处理 的 形式 模型 冯志伟 这本书 还是 冯志伟 
老师 写 的 很 佩服 冯志伟 老师 文理 兼修 而且都 
很厉害 内容 许多 是从 他 以前 的 著作 里面 摘取 
的 算是 一本 各种 语言 模型 和 统计 模型 的 
大集合 吧 放在 桌面 没事 翻翻 也能 是 极好 的 
5 统计 自然语言 处理 第 2版 宗 成庆 这 本书 
我 觉得 写 的 不错 虽然 我 是 语言学 背景 
但 读 起来 也 没有 太 吃力 它 也是 综论 
性质 的 可以 跳 着看 6 统计 学习 方法 李航 
自然语言 处理 需要 些 机器学习 的 知识 我 数学 基础 
还是 太 薄弱 有的 内容 还是 有些 吃力 和 困惑 
的 7 机器学习 实战 哈林顿 Peter Harrington Python 自然语言 处理 
集体 智慧 编程 这些 书 都是 python 相关 的 中间 
那本/nr 就是 将 NLTK 的 网上 都有 电子版 需要 的 
时候 翻一番 看一看 就行 视频 篇 @ 吴俣/nr 上面 提到 
的 斯坦福 的 nlp 课程 Video Listing 哥伦比亚 大学 的 
https / / class . coursera . org / nlangp 
001 两个 都是/nr 英文 的 无 中文字幕 现在 还 可以 
下载 视频 和 课件 另外 超星 学术 视频 1 自然语言 
理解 _ 宗 成庆 我 觉得 讲 的 还是 不错 
的 第一次 听 的 时候 有些 晕乎 该 课程 网上 
有 ppt 讲义 讲义 后来 被 作者 写 成了 书 
就是 上面 提到 的 统计 自然语言 处理 拿着 书 就是 
教材 还有 课程 ppt 和 视频 可以 看 这种 感觉 
还是 很好 的 2 自然语言 处理 _ 关毅 感觉 讲 
的 一般 听了 几节 跳跃 太多 有时候 让人 摸不着头脑 多 
听听 还是 很 有 益处 的 吧 3 计算 语言学 
概论 _ 侯敏 这个 就是 语言学 内容 为主 了 作者 
也是 语言学 背景 下在 nlp 比较 活跃 的 讲 的 
很浅 老师 讲课 很 啰嗦 说话 太慢 我 都是 加速 
看 的 4 计算 语言学 _ 冯志伟 冯志伟 老师 这个 
课 一如 他 的 著作 语言学 和 统计 都会 涉及 
到 一些 冯志伟 老师 说话 有些 地方 听 不大 清 
要是 有 字幕 就 好了 5 语法分析 _ 陆俭明 这是 
纯 语言学 的 课程 陆剑明/nr 也是/i 当代/t 语言学/n 的/uj 大师/n 
我 觉得 既然 是 自然 语言 处理 语言学 的 东西 
还是 多少 要 了解 的 其他 篇 1 博客 的话 
我 爱 自然 语言 处理 专门 记录 nlp 的 很不错 
再有 就是 csdn 上 一些 比较 琐碎 的 了 2 
北京大学 中文系 应用 语言学 专业 这个 刚 开始 的 时候 
也 看了 看 又 不少 干货 3 中文信息 学报 说 
这个 不会 被 大神 喷 吧 英语 不佳 英文 文献 
实在 看 的 少 这个 学报 也是 挑着 看看 就行 
好像 就是 这些 内容 了 如果 有 日后 再补 虽然 
自己 写 了 这么 多 但/c 不少/d 书/n 和/c 视频/n 
都/d 没有/v 完整/a 的/uj 看完/v 现在 水平 仍 很菜 仍在 
进阶 的 路上 希望 各路 大神 多多 指点 该 拍砖 
就 拍 吧 编辑 于 /nr 2014 12 19228 21 条 
评论 分享 收藏 感谢 收起/v 何史提/nr 物理学/n 理论物理 量子 物理 
  话题 的 优秀 回答者 10 人 赞同 了 该 
回答 看 Coursera 相关 的 课程 或 参考 Manning and 
Shcutze Foundations of Statistical Natural Language Processing 但 更 重要 
的 还是 实战经验 发布 于 2014 07 0810 4 条 
评论 分享 收藏 感谢 陈见耸/nr Rokid A Lab 自然语言 处理 
人工智能 机器学习 23 人 赞同 了 该 回答 大家 回答 
的 都挺/nr 不错 了 只好来 强答/nr 一 独立 实现 一个 
小型 的 自然 语言 处理 项目 要找 一个 合适 的 
的 自然 语言 处理 相关 的 开源 项目 这个 项目 
可以 是 与 自己 工作 相关 的 也 可以 是 
自己 感兴趣 的 项目 不要 太大 以 小型 的 算法 
模块 为佳 这样 便于 独立 实现 像 文本 领域 的 
文本 分类 分词 等 项目 就是 比较 合适 的 项目 
运行 程序 得到 项目 所 声称 的 结果 然后 看懂 
程序 这 期间 一般 需要 阅读 程序实现 所 参考 的 
文献 最后 自己 尝试 独立 实现 该 算法 得到 与 
示例 程序 相同 的 结果 再 进一步 的 可以 调试 
参数 了解 各 参数 对 效果 的 影响 看 是否 
能 得到 性能 更好 的 参数 组合 这一 阶段 主要 
是 学习 快速 上手 一个 项目 从而 对 自然 语言 
处理 的 项目 有 比较 感性 的 认识 大体 了解 
自然语言 处理 算法 的 原理 实现 流程 等 当 我们 
对 自然 语言 处理 项目 有了/nr 一定 的 认识 之后 
接下来 就要 深入 进去 任何/r 自然语言/l 处理/v 应用/v 都/d 包含/v 
算法/n 和/c 所要/b 解决/v 的/uj 问题/n 两/m 方面/n 要想 深入 
进去 就 需要 从这 两 方面 进行 着手 二 对 
问题 进行 深入 认识 对 问题 的 深入 认识 通常 
来源 于 两个 方面 一是 阅读 当前 领域 的 文献 
尤其 是 综述 性 的 文献 理解 当前 领域 所 
面临 的 主要 问题 已有 的 解决 方案 有 哪些 
有待 解决 的 问题 有 哪些 这里 值得一提的是 博士生 论文 
的 相关 文献 介绍 部分 通常会 对本 问题 做 比较 
详细 的 介绍 也 是 比较 好 的 综述 类 
材料 除了 从 文献 中 获取 对 问题 的 认识 
外 另一种 对 问题 进行 深入 认识 的 直观 方法 
就是 对 算法 得出 的 结果 进行 bad case 分析 
总结 提炼 出 一些 共性 的 问题 对 bad case 
进行 分析 还有 一个 好处 可以 帮助 我们 了解 哪些 
问题 是 主要 问题 哪些 问题 是 次要 问题 从而 
可以 帮助 我们 建立 问题 优先级 如果 有 具体 任务 
的 真实 数据 一定 要 在 真实 数据 上 进行 
测试 这 是因为 即使 是 相同 的 算法 在 不同 
的 数据 集上 所 得到 的 结果 也 可能 相差 
很大 三 对 算法 进行 深入 理解 除了 具体 的 
问题 分析 对 算法 的 理解 是 学习 人工智能 必须 
要 过 的 关 经过 这么 多年 的 发展 机器学习 
模式识别 的 算法 已经 多如牛毛 幸运 的 是 这 方面 
已经 有 不少 好 的 书籍 可供参考 这里 推荐 华为 
李航 的 蓝 宝书 统计 学习 方法 和 周志华 的 
西瓜 书 机器学习 这 两本 都是 国内 顶级 的 机器学习 
专家 撰写 的 书籍 思路清晰 行文流畅 样例 丰富 如果 觉得 
教科书 稍感 乏味 那我/nr 推荐 吴军 的 数学 之美 这是 
一本 入门级 的 科普 读物 作者 以 生动 有趣 的 
方式 深入浅出 的 讲解 了 很多 人工智能 领域 的 算法 
相信 你 一定会 有 兴趣 国外 的 书籍 Pattern Recognition 
and Machine Learning 主要 从 概率 的 角度 解释 机器 
学习 的 各种 算法 也是 不可多得 的 入门 教材 如果 
要 了解 最新 的 深度 学习 的 相关 算法 可以 
阅读 被 誉为 深度 学习 三架 马车 之一 Bengio 所著 
的 Deep Learning 在 学习 教材 时 对于 应用 工程师 
来说 重要 的 是 理解 算法 的 原理 从而 掌握 
什么 数据 情况 下 适合 什么样 的 数据 以及 参数 
的 意义 是 什么 四 深入 到 领域 前沿 自然语言 
处理 领域 一直 处在 快速 的 发展 变化 当中 不管 
是 综述 类 文章 还 是 书籍 都 不能 反映 
当前 领域 的 最新 进展 如果 要 进一步 的 了解 
领域 前沿 那就/nr 需要 关注 国际 顶级 会议 上 的 
最新 论文 了 下面 是 各个 领域 的 一些 顶级 
会议 这里 值得一提的是 和 其他 人工智能 领域 类似 自然语言 处理 
领域 最 主要 的 学术 交流 方式 就 会议 论文 
这 和 其他 领域 比如 数学 化学 物理 等 传统 
领域 都 不太 一样 这些 领域 通常 都以 期刊论文 作为 
最 主要 的 交流 方式 但是 期刊论文 审稿 周期 太长 
好 的 期刊 通常 都要 两三年 的 时间 才能 发表 
这 完全 满足 不了 日新月异 的 人工智能 领域 的 发展 
需求 因此 大家/n 都会/nr 倾向/v 于在/nr 审稿/v 周期/t 更短/i 的/uj 
会议/n 上/f 尽快/d 发表/v 自己/r 的/uj 论文/nz 这里 列举 了 
国际 和 国内 文本 领域 的 一些 会议 以及 官网 
大家 可以 自行 查看 国际上 的 文本 领域 会议 ACL 
http / / acl2017 . org / 加拿大 温哥华 7.30 
8 . 4EMNLP http / / emnlp2017 . net / 
丹麦 哥本哈根 9.7 9 . 11COLING 没找到 2017年 的 国内 
会议 CCKS http / / www . ccks2017 . com 
/ index . php / att / 成都 8月 26 
8月 29SMP http / / www . cips smp . 
org / smp2017 / 北京 9.14 9 . 17CCL http 
/ / www . cips cl . org 8080 / 
CCL2017 / home . html 南京 10.13 10 . 15NLPCC 
http / / tcci . ccf . org . cn 
/ conference / 2017 / 大连 11.8 11 . 12NCMMSC 
http / / www . ncmmsc2017 . org / index 
. html 连云港 11.11 － 11.13 像 paperweekly 机器学习 研究会 
深度 学习 大讲堂 等 微信 公众 号 也 经常 会 
探讨 一些 自然 语言 处理 的 最新 论文 是 不错 
的 中文 资料 五 当然 工欲善其事 必先利其器 我们 要 做好 
自然语言 处理 的 项目 还 需要 熟练掌握 至少 一门 工具 
当前 深度 学习 相关 的 工具 已经 比较 多了 比如 
tensorflow mxnet caffe theano cntk 等 这里 向 大家 推荐 
tensorflow 自从 google 推出 之后 tensorflow 几乎 成为 最 流行 
的 深度 学习 工具 究其原因 除了 google 的 大力 宣传 
之外 tensorflow 秉承 了 google 开源 项目 的 一贯 风格 
社区 力量 比较 活跃 目前 github 上有 相当多 数量 的 
以 tensorflow 为 工具 的 项目 这 对于 开发 者 
来说 是 相当大 的 资源 以上 就是 对于 没有 自然语言 
处理 项目 经验 的 人 来说 如何 学习 自然语言 处理 
的 一些 经验 希望 对 大家 能 有所 帮助 发布 
于 2017 05 1023 2 条 评论 分享 收藏 感谢 
收起 杨智 互联网 44 人 赞同 了 该 回答 说 
说 自己 的 历程 吧 我 是 一名 非 科班 
的 自然 语言 机器学习 数据挖掘 关注 者 因 工作 关系 
5 年前 需要 做 与 自然 语言 处理 的 项目 
当时 的 项目 老大 先是 扔给 我 一 本书 统计 
自然语言 处理 直接 给 我 看 蒙了/nr 不能 说 一点 
都 不懂 但是 看 的 云里雾里 不 知道 get 几层 
但 看这 本书 的 过程 中 我 狂 搜 了 
些 自然语言 处理 的 课件 有 北大 的 中科院 的 
都写 的 很好 从 语言 模型 开始 从 分词 标注 
语法树 语意 等等 也 大体 知道 自然语言 处理 分 词法 
语法 语义 然后 是 各种 应用 信息检索 机器 翻译 等 
自然 语言 经典 应用 问题 断断续续 做了 些 小项目 基于 
语言 模型 的 拼音 输入法 仿照 sun pinyin 写 的 
他们 的 blog 写 的 很详细 从 模型 建模 到 
平滑 处理 很详细 我 也用 python 实现 了 一遍 当时 
这个 输入法 配合 上 一个 简单 的 ui 还在 部门 
内部 推广 了 搞了个 基于 云的/nr 拼音 输入法 获得 个 
小 奖品 很是 洋洋得意 这个 过程 中 我 看着 sunpinyin 
的 blog 回过 头 又 去看 课件 去 了解 很 
细节 的 问题 如 拉普拉斯 平滑 回退 平滑 的 细节 
等 收获 很多 后来 老大 告诉 我 看 自然 语言 
问题 时 可以 找 博士论文 先看 因为 博士论文 一般 都会 
来龙去脉 讲 的 非常 详细 看完 一遍 之后 基本上 这个 
问题 就 了解 的 差不多 然后 就是 follow 业界 的 
进度 那/r 就是/d 关注/v 各种/r 会议/n 和/c 期考/v 可 自行 
百度 和 谷歌 搞好 这个 拼音 输入法 进入 实际 项目 
做一套 中文 自然 语言 的 基础 处理 引擎 好在 不是 
让 我 一个人 来 公司 开始 找 大学 合作 我 
做 企业 项目 负责 跟进 的 大学 负责 具体 算法 
我 跟着 自己 调查 分词 标注 算法 了解 了 有 
基于 词典 的 语言 模型 的 hmm crf 的 那个 
crf 的 我 始终 搞 不 大明白 后来 先 了解 
了 hmm 的 vertbe 算法 em 算法 大学 的 博士 
给 我 讲了 一遍 crf 终于 豁然开朗 还把 解码 过程 
写 到了 http / / 52nlp . cn 上 关注 
的 人 还 可以 从那以后 我 感觉 我 就 真 
入门 了 在 来 一个 什么 问题 我/r 基本上/n 也/d 
有/v 套路/n 来/v 学习/v 和/c 研究/vn 了/ul 总 结下 1 
. 先 各种 课件 加 那本 自然 语言 的 书 
搞清楚/l 自然语言/l 大概/d 都有/nr 哪些/r 问题/n 主要 是 为了 解决 
什么 问题 的 2 . 基于 某个 问题 看 博士论文 
了解 来龙去脉 然后 follow 业界 进度 3 . 找 各种 
资源 会议 的 期刊 的 博客 http / / 52nlp 
. cn 不是 打 广告 我 不是 博主 不过 博客 
真心 不错 4 . 微博 上 关注 各种 这个 领域 
的 大牛 他们 有时候 会 推荐 很多 有用 的 资料 
当然 数学 之美 我 也 读了 确实 不错 自然语言 处理 
NLP Natural Language Processing 是 人工智能 AI Artificial Intelligence 的 
一部分 实现 人 与 计算机 之间 的 有效 通信 自然语言 
处理 属于 计算机 科学 领域 与 人工智能 领域 其 研究 
使用 计算机 编程 来 处理 与 理解 人类 的 语言 
常见 的 自然 语言 处理 有 文本 相似 度 匹配 
情感 分析 机器翻译 聊天 机器人 等 分词 停用词 过滤 词干 
提取 词形 还原 词干 提取 与 词形 还 原词 袋 
模型 TF IDFWord2Vec 代码 示例 https / / github . 
com / yyhsong / iMLearning / tree / master / 
NLP 系列 文章 请 多 关注 Tensorflow 源码 解析 1 
– 内核 架构 和 源码 结构 带 你 深入 AI 
1 深度 学习 模型 训练 痛点 及 解决 方法 自然语言 
处理 1 – 分词 自然语言 处理 2 – jieba 分词 
用法 及 原理 自然语言 处理 3 – 词性 标注 自然语言 
处理 4 – 句法分析 自然语言 处理 5 – 词 向量 
自然语言 处理 6 – 情感 分析 1 概述 分词 是 
自然 语言 处理 的 基础 分词 准确度 直接 决定 了 
后面 的 词性 标注 句法分析 词 向量 以及 文本 分析 
的 质量 英文 语句 使用 空格 将 单词 进行 分隔 
除了 某些 特定 词 如 how many New York 等外 
大 部分 情况 下 不 需要 考虑 分词 问题 但 
中文 不同 天然 缺少 分隔符 需要 读者 自行 分词 和 
断句 故 在做 中文 自然语言 处理 时 我们 需要 先 
进行 分词 2 中文分词 难点 中文分词 不像 英文 那样 天然 
有 空格 作为 分隔 而且 中文 词语 组合 繁多 分词 
很容易 产生 歧义 因此 中文分词 一直以来 都是 NLP 的 一个 
重点 也 是 一个 难点 难点 主要 集中 在 分词 
标准 切分 歧义 和未/nr 登录 词 三 部分 分词 标准 
比如 人名 有的 算法 认为 姓 和名/nr 应该 分开 有的 
认为 不 应该 分开 这 需要 制定 一个 相对 统一 
的 标准 又 例如 花草 有的 人 认为 是 一个 
词 有的 人 认为 应该 划 分开 为 两个 词 
花/草/nr 某种 意义 上 中文分词 可以 说 是 一个 没有 
明确 定义 的 问题 切分 歧义 不同 的 切分 结果 
会 有 不同 的 含义 这又 包含 如下 几种 情况 
组合型 歧义 分词 粒度 不同 导致 的 不同 切分 结果 
比如 中华人民共和国 粗粒度 的 分词 结果 为 中华人民共和国 细粒度 的 
分词 结果 为 中华 / 人民 / 共和国 这种 问题 
需要 根据 使用 场景 来 选择 在 文本 分类 情感 
分析 等 文本 分析 场景 下 粗粒度 划分 较好 而在 
搜索引擎 场景 下 为了 保证 recall 细粒度 的 划分 则 
较好 jieba 分词 可以 根据 用户 选择 的 模式 输出 
粗粒度 或者 细粒度 的 分词 结果 十分 灵活 另外 有时候 
汉 字串 AB 中 AB A B 可以 同时 成词/nr 
这个 时候 也 容易 产生 组合型 歧义 比如 他 / 
将 / 来 / 网商 银行 他 / 将来 / 
想 / 应聘 / 网商 银行 这 需要 通过 整句话 
来 区分 组合型 歧义 描述 的 是 AB A B 
均 可以 同时 成词的/nr 汉字 串 它 是 可以 预测 
的 故 也有 专家 称之为 固 有型 歧义 交集 型 
歧义 不同 切分 结果 共用 相同 的 字 前后 组合 
的 不同 导致 不同 的 切分 结果 比如 商务处 女干事 
可以 划分 为 商务处 / 女干事 也 可以 划分 为 
商务 / 处女 / 干事 这 也 需要 通过 整句话 
来 区分 交集 型 歧义 前后 组合 变化 很多 难以预测 
故 也有 专家 称之为 偶 发型 歧义 真 歧义 本身 
语法 或 语义 没有问题 即使 人工 切分 也 会 产生 
歧义 比如 下雨天 留客 天天 留人 不留 可以 划分 为 
下雨天 / 留客 天 / 天 留 / 人 不留 
也 可以 划分 为 下雨天 / 留客 天 / 天 
留人 不 / 留 此时 通过 整句话 还 没法 切分 
只能 通过 上下文 语境 来 进行 切分 如果 是 不想 
留客 则 切 分为 前 一个 否则 切 分为 后 
一个 有 专家 统计 过 中文 文本 中的 切分 歧义 
出现 频次 为 1.2次 / 100 汉字 其中 交集 型 
歧义 和 组合型 歧义 占 比为 12 1 而 对于 
真 歧义 一般 出现 的 概率 不大 未 登录 词 
也叫 新词 发现 或者 生词 未被 词典 收录 的 词 
未 登录 词 分为 如下 几种 类型 新 出现 的 
词汇 比如 一些 网络 热词 如 超女 给力 等 专有名词 
主要 是 人名 地名 组织 机构 比如 南苏丹 特朗普 花呗/nr 
借 呗 等 专业名词 和 研究 领域 词语 比如 苏丹红 
禽流感 其他 专有名词 比如 新 出现 的 电影 名 产品名 
书籍 名 等 未 登录 词 对于 分词 精度 的 
影响 远 远超过 歧义 切分 未 登录 词 识别 难度 
也 很大 主要 原因 有未/nr 登录 词 增长 速度 往往 
比 词典 更新 速度 快 很多 因此 很难 利用 更新 
词典 的 方式 解决 未 登录 词 问题 不过 词典 
越大 越全/nr 分词 精度 也 会 越高 因此 一个 大而全 
的 词典 还是 相当 重要 的 未 登录 词 都是 
由 普通 词汇 构成 长度 不定 也 没有 明显 的 
边界 标志 词 未 登录 词 还有 可能 与 上下文 
中的 其他词汇 构成 交集 型 歧义 未 登录 词中 还有 
可能 夹杂 着 英语 字母 等 其他 符号 这也 带来 
了 很大 难度 比如 e 租 宝 对于 词典 中 
不 包含 的 未 登录 词 我们 无法 基于 字符串 
匹配 来 进行 识别 此时 基于 统计 的 分词 算法 
就 可以 大显身手 了 jieba 分词 采用 了 HMM 隐 
马尔科夫 模型 和 viterbi 算法 来 解决 未 登录 词 
问题 下一 篇 文章 我们 会 详细 分析 这个 算法 
过程 3 中文分词 算法 当前 的 分词 算法 主要 分为 
两类 基于 词典 的 规则 匹 配方法 和 基于 统计 
的 机器 学习 方法 基于 词典 的 分词 算法 基于 
词典 的 分词 算法 本质 上 就是 字符串 匹配 将 
待 匹配 的 字符串 基于 一定 的 算法 策略 和 
一个 足够 大 的 词典 进行 字符串 匹配 如果 匹配 
命中 则 可以 分词 根据 不同 的 匹配 策略 又 
分为 正向 最大 匹 配法 逆向 最大 匹 配法 双向 
匹配 分词 全 切分 路径 选择 等 _ _ 最大 
匹 配法 _ _ 主要 分为 三种 正向 最大 匹 
配法 从左到右 对 语句 进行 匹配 匹配 的 词 越长 
越好 比如 商务处 女干事 划分为 商务处 / 女干事 而 不是 
商务 / 处女 / 干事 这种方式 切分 会有 歧义 问题 
出现 比如 结婚 和 尚未 结婚 的 同事 会被 划分为 
结婚 / 和尚 / 未 / 结婚 / 的 / 
同事 逆向 最大 匹 配法 从右到左 对 语句 进行 匹配 
同样 也是 匹配 的 词 越长 越好 比如 他 从东 
经过 我家 划分为 他/r //i 从/东/nr //i 经过/p //i 我家/r 
这种 方式 同样 也 会有 歧义 问题 比如 他们 昨日 
本 应该 回来 会被 划分为 他们 / 昨 / 日本 
/ 应该 / 回来 双向 匹配 分词 则 同时 采用 
正向 最大 匹配 和 逆向 最大 匹配 选择 二者 分词 
结果 中 词数 较 少者 但 这种 方式 同样 会 
产生 歧义 问题 比如 他 将来 上海 会被 划分为 他 
/ 将来 / 上海 由此可见 词数 少 也 不一定 划分 
就 正确 全 切分 路径 选择 将 所有 可能 的 
切分 结果 全部 列出来 从中 选择 最佳 的 切分 路径 
分为 两种 选择 方法 n 最短 路径 方法 将 所有 
的 切分 结果 组成 有向 无 环 图 切 词 
结果 作为 节点 词/n 和词/nr 之间/f 的/uj 边/d 赋予/v 权重/n 
找到 权 重和 最小 的 路径 即为 最终 结果 比如 
可以 通过 词频 作为 权重 找到 一条 总 词频 最大 
的 路径 即可 认为 是 最佳 路径 n 元 语法 
模型 同样 采用 n 最短 路径 只不过 路径 构成 时会 
考虑 词 的 上下文 关系 一元 表示 考虑 词 的 
前后 一个词 二元 则 表示 考虑 词 的 前后 两个 
词 然后 根据 语料库 的 统计 结果 找到 概率 最大 
的 路径 基于 统计 的 分词 算法 基于 统计 的 
分词 算法 本质上 是 一个 序列 标注 问题 我们 将 
语句 中的 字 按照 他们 在 词中 的 位置 进行 
标注 标注 主要 有 B 词 开始 的 一个 字 
E 词 最后 一个 字 M 词 中间 的 字 
可能 多个 S 一个 字 表示 的 词 例如 网商/i 
银行/n 是/v 蚂蚁/n 金服微/nr 贷/v 事业部/n 的/uj 最重要/i 产品/n 标注 
后 结果 为 B M M E B M M 
E B M M M E B M E B 
E 对应 的 分词 结果 为 网商/i 银行/n //i 是/v 
//i 蚂蚁/n 金服//nr 微贷/nr 事业部/n //i 的/uj //i 最重要/i //i 
产品/n 我们 基于 统计 分析 方法 得到 序列 标注 结果 
就 可以 得到 分词 结果 了 这类 算法 基于 机器学习 
或者 现在 火热 的 深度 学习 主要 有 HMM CRF 
SVM 以及 深度 学习 等 HMM 隐 马尔科夫 模型 隐 
马尔科夫 模型 在 机器 学习 中 应用 十分 广泛 它 
包含 观测 序列 和 隐藏 序列 两部分 对应 到 NLP 
中 我们 的 语句 是 观测 序列 而 序列 标注 
结果 是 隐藏 序列 任何 一个 HMM 都 可以 由 
一个 五 元组 来 描述 观测 序列 隐藏 序列 隐藏 
态 起始 概率 隐藏 态 之间 转换 概率 转移 概率 
隐藏 态 表现 为 观测值 的 概率 发射 概率 其中 
起始 概率 转移 概率 和 发射 概率 可以 通过 大 
规模 语料 统计 来 得到 从 隐藏 态 初始状态 出发 
计算 下 一个 隐藏 态 的 概率 并 依次 计算 
后面 所有 的 隐藏 态 转移 概率 我们 的 序列 
标注 问题 就 转化 为了 求解 概率 最大 的 隐藏 
状态 序列 问题 jieba 分词 中 使用 HMM 模型 来 
处理 未 登录 词 问题 并 利用 viterbi 算法 来 
计算 观测 序列 语句 最 可能 的 隐藏 序列 BEMS 
标注 序列 CRF 条件 随 机场 也 可以 描述 输入 
序列 和 输出 序列 之间 关系 只不过 它 是 基于 
条件概率 来 描述 模型 的 详细 的 这儿 就不 展开 
了 深度 学习 将 语句 作为 输入 分词 结果 作为 
标注 可以 进行 有 监督 学习 训练 生成 模型 从而 
对 未知 语句 进行 预测 4 分词 质量 和 性能 
中文分词 对于 自然 语言 处理 至关重要 评价/n 一个/m 分词/n 引擎/n 
性能/n 的/uj 指标/n 主要/b 有/v 分词/n 准确度/n 和/c 分词/n 速度/n 
两/m 方面/n 分词 准确度 直接影响 后续 的 词性 标注 句法分析 
文本 分析 等 环节 分词 速度 则 对 自然 语言 
处理 的 实时性 影响 很大 下图 为 几种 常用 分词 
引擎 在 准确度 和 速度 方面 的 对比 由上 可见 
想 要做 准确度 很高 的 通用 型 分词 引擎 是 
多么 的 困难 如果 对 准确度 要求 很高 可以 尝试 
开发 特定 领域 的 分词 引擎 比如 专门 针对 金融 
领域 同时 从 图中 可见 作为 一款 开源 的 通用 
型 分词 引擎 jieba/w 分词/n 的/uj 准确度/n 和/c 速度/n 都/d 
还是/c 不错/a 的/uj 后面 我们 会 详细 讲解 jieba 分词 
的 用法 及其 原理 5 总结 中文分词 是 中文 自然语言 
处理 中 的 一个 重要 环节 为 后面 的 词 
向量 编码 词性 标注 句法分析 以及 文本 分析 打下 了 
坚实 的 基础 同时 由于 中文 缺少 空格 等 分隔符 
并且 汉 字间 的 组合 特别 多 很容易 产生 歧义 
这些 都 加大 了 中文分词 的 难度 基于 词典 的 
字符串 匹配 算法 和 基于 统计 的 分词 算法 二者 
各有 优缺点 我们 可以 考虑 结合 使用 随着 深度 学习 
的 兴起 我们 可以 考虑 利用 深度 学习 来 进行 
序列 标注 和 中文分词 系列 文章 请 多 关注 Tensorflow 
源码 解析 1 – 内核 架构 和 源码 结构 带 
你 深入 AI 1 深度 学习 模型 训练 痛点 及 
解决 方法 自然语言 处理 1 – 分词 自然语言 处理 2 
– jieba 分词 用法 及 原理 自然语言 处理 3 – 
词性 标注 自然语言 处理 4 – 句法分析 自然语言 处理 5 
– 词 向量 自然语言 处理 6 – 情感 分析 接着 
自然语言 处理 与 分析 one 笔者 现 在 推荐 一款 
在线 编辑器 我 就是 用来 写 代码 的 data text 
/ html style type = text / css # e 
{ position absolute top 0 right 0 bottom 0 left 
0 } / style div id = % 22e % 
22 / div script src = % 22http / / 
d1n0x3qji82z53 . cloudfront . net / src min noconflict / 
ace . js % 22 type = % 22text / 
javascript % 22 charset = % 22utf 8% 22 / 
script script var e = ace . edit % 22e 
% 22 e . setTheme % 22ace / theme / 
monokai % 22 e . getSession . setMode % 22ace 
/ mode / java % 22 / script 这个 粘帖 
到 浏览器 地址 public void train throws IOException { for 
int i = 0 i categories . length + + 
i { String category = categories i Classification classification = 
new Classification category / / 新建 类别 File dir = 
new File pDir categories i File trainFiles = dir . 
listFiles for int j = 0 j trainFiles . length 
+ + j { File trainFile = trainFiles j if 
isTrainingFile trainFile { / / 判断 一下 是 为了 让 
一部分 数据 作为 训练 集 一 部分 作为 测试 集 
String review = Files . readFromFile trainFile ISO 8859 1 
Classified classified = new Classified review classification / / 指定 
内容 和 类别 classifer . handle classified / / 训练 
} } } } 这里 说明 一下 isTrainingFile 方法 我们 
需要 一份 测试 集 和 一个 训练 集 但是 我们 
只有 一个 语料库 只有 人为 分割 我 原本 是 每次 
随机数 一下来 干的/nr 但是 有点 影响 速度 这里 直接 用 
文件名 作为 判断 依据 了 boolean isTrainingFile File file { 
return file . getName . charAt 2 = 1 / 
/ 如果 第 2位 为 1 就是 测试 集 } 
训练 完成 后 使用 classifer 就 可以 进行 极性 分析 
了 public void evaluate throws IOException { int numTests = 
0 int numCorrect = 0 for int i = 0 
i categories . length + + i { String category 
= categories i File file = new File pDir categories 
i File testFiles = file . listFiles for int j 
= 0 j testFiles . length + + j { 
File testFile = testFiles j if isTrainingFile testFile { String 
review = Files . readFromFile testFile ISO 8859 1 + 
+ numTests Classification classification = classifer . classify review String 
resultCategory = classification . bestCategory if resultCategory . equals category 
+ + numCorrect } } } System . out . 
println 测试 总数 + numTests System . out . println 
正 确数 + numCorrect System . out . println 正确率 
+ double numCorrect / double numTests } 效果 lingpipe1 将 
isTrainingFile 修改 一下 boolean isTrainingFile File file { return file 
. getName . charAt 2 = 2 / / 如果 
第 2位 为 2 就是 测试 集 } lingpipi2 就 
正确率 而言 怎么 划分 训练 集 和 测试 集 影响 
不大 还 可以 这样 划分 boolean isTrainingFile File file { 
return file . getName . charAt 2 = 2 & 
& file . getName . charAt 2 = 1 } 
扩展 基本 极性 分析 只是 文本 倾向性 分析 一个 很 
简单 的 部分 如果 需要 深入 的话 LingPipe 还 可以 
实现 主观性 分析 层次 极性 分析 等 如果 需要 支持 
中文 的话 请 下载 words zh as . C o 
m p i l e d p e l l 
C h e c k e r 最后 附上 三篇 
参考文献 Bo Pang Lillian Lee and Shivakumar Vaithyanathan . 2002 
.   Thumbs up Sentiment Classification using Machine Learning Techniques 
.   EMNLP Proceedings . Bo Pang and Lillian Lee 
. 2004 .   A Sentimental Education Sentiment Analysis Using 
Subjectivity Summarization Based on Minimum Cuts .   ACL Proceedings 
. Bo Pang and Lillian Lee . 2005 .   
Seeing stars Exploiting class relationships for sentiment categorization with respect 
to rating scales .   ACL Proceedings . https / 
/ www . toutiao . com / a 6 6 
7 9 6 1 0 3 7 7 9 9 
2 4 0 5 5 0 7 / 本文 整理 
自 阿里巴巴 iDST 自然语言 处理 部 总监 郎君 博士 的 
题为 NLP 技术 的 应用 及 思考 的 演讲 本文 
从 NLP 背景 开始 谈起 重点 介绍 了 AliNLP 平台 
接着 分享 了 NLP 相关 的 应用 实例 最后 对 
NLP 的 未来 进行 了 思考 背景 介绍 阿里 巴巴 
的 生态 系统 下面 有 很多 的 计算 平台 上面 
有 各种各样 的 业务 层 最 中间 是 买家 和 
卖家 之间 包括 销售 支付 等等 之间 的 关系 外面 
建了 一圈 从 娱乐 到 广告 到 金融 到 购物 
到 物流 等等 各方面 这样 一个 生态 中间 有 非常 
多 的 数据 能够 关联 起来 所以 对于 阿里 巴巴 
而言 这个 图 可以 非常 简练 的 概括 我们 在 
做什么 中间 是 最重要 的 数据 下面 数据包 含了 最 
核心 的 也是 阿里巴巴 最早 起家 的 来自 于 电商 
的 数据 所以 电商 对于 我们 而言 是 非常 重要 
的 后来 又 扩 展出 了 金融 菜鸟物流 健康 和 
娱乐 比如 我们 有大/nr 文娱 事业群 去 做了 优酷 土豆 
等 各种各样 的 数据 数据 当中 包含 了 很多 的 
文本 比如 阿里 的 电商 平台 里面 有 数十亿 的 
商品 每 一个 商品 都 包含 详细 的 标题 副标题 
详情页 评价 区 甚至 问答 区 这 里面 的 信息 
构成 了 一个 非常 丰富 的 商品 信息 还有 上亿 
的 文章 阿里 在 两年 前 开始 进入 内容 时代 
比如 现在 各种各样 的 内容 营销 直播 还有 一些 问答 
的 场景 圆桌 等等 文章 里面 可以 包含 各种各样 的 
标题 正文 和 评论 等 大量 的 数据 这 只是 
电商 的 例子 还有 金融 物流 健康 娱乐 加在一起 还会有 
海量 的 数据 就会 孕育 出 大量 文本处理 的 工作 
需求 自然语言 处理 是 什么 呢 语言 是 生物 同类 
之间 由于 沟通 需要 而 制定 的 具有 统一 编码 
解码 标准 的 声音 图像 指令 包含 手势 表情 语音 
等 肢体 语言 文字 是 显像 符号 自然语言 通常 是 
指 一种 自然 地 随 文化 演化 的 语言 例如 
英语 汉语 日语 等 有别于 人造语言 例如 世界语 编程语言 等 
自然语言 处理 包括 自然 语言 理解 和 自然 语言 生成 
自然语言 理解 是 将 自然 语言 变成 计算机 能够 理解 
的 语言 及 非 结构化 文本 转变为 结构化 信息 NLP 
的 四大 经典 AI 完全 难题 问答 复述 文摘 翻译 
只要 解决 其中 一个 另外 三个 就都 解决 了 问答 
就是 让 机器人 很 开放 的 回答 你 提 的 
各种各样 问题 就像 真人 一样 复述 是 让 机器 用 
另外 一种 方式 表达 出来 文摘 就是 告诉 你 一篇 
很长 的 文章 让 你 写 一个 100字 的 文摘 
把 它 做 出来 是 非常 难做 的 翻译 也是 
很 困难 的 英语 思维 方式 和 中文 思维 方式 
转 换过来 中间 会 涉及 到 很多 复杂 的 问题 
阿里巴巴 需要 什么样 的 自然 语言 处理 技术 阿里 的 
生态 是 非常 复杂 的 我们 不能 用 一个 简单 
的 自然 语言 处理 技术 去 解决 所有 的 问题 
以往 自然语言 处理 是 比较 简单 的 甚至 一个 词表 
放 上去 就 解决 所有 问题 了 随着 电商 生态 
的 扩展 就 需要 非常 复杂 的 技术 所以 我们 
需要 完备 且 高性能 的 自然 语言 处理 技术 高性能 
体现 在 算法 精度 还有 执行 效率 IDST 的 定位 
如下 引领 技术 前沿 赶超 市场 最佳 的 竞争者 完备 
和 完善 AliNLP 平台 的 技术 体系 及 服务 能力 
赋 能 核心 业务 帮助 核心 业务 快速 成长 寻找/v 
和/c 解决/v 业务/n 方的最/nr 痛点/n 创造 商业机会 创造 看似 不 
可能 的 商业 技术 深度 理解 语言 深度 理解 需求 
变革 产品 体验 AliNLP 自然语言 技术 平台 图 为 我们 
整个 自然语言 处理 平台 最 核心 的 框图 底层 是 
各种各样 的 基础 数据 中间层 包含 基本 的 词 法分析 
句法分析 语义分析 文档 分析 还有 其他 各种各样 跟 深度 学习 
相关 的 一些 技术 上层/b 是/v 自然/d 语言/n 处理/v 能够/v 
直接/ad 掌控/l 和/c 变革/vn 的/uj 一些/m 算法/n 和/c 业务/n 比如 
内容 搜索 内容 推荐 评价 问答 文摘 文本 理解 等等 
一 系列 问题 最上层 我们 直接 支持 大 业务 的 
单元 比如 商品 搜索 推荐 智能 交互 翻译 商业 翻译 
和 普通 机器 翻译 是 不 一样 的 还有 广告 
风 控 舆情 监控 等等 这个 层次结构 是 比较 传统 
的 方式 为了 让 我们 平台 具有 非常 好 的 
落地 能力 右边 有 一列 平台 工程 专门 解决 如何 
让 算法 能够 快速 的 用到 业务 里面 去 将 
核心 框图 细化 底层 有 各种各样 的 数据 比如 实体 
库 源 学 辞典 词性 标注 库 词 性关系 库 
句法树 库 情感 分析 标注 库 还有 情感 词典 资讯 
库 图谱 等等 这些 是 词 法分析 包括 分词 词性 
实体 识别 拼写 检查 等 一些 基础 的 组件 句法分析 
有 结构 句法分析 依存 句法分析 语义 分布 表示 等等 还有 
语义分析 包含 词义 消 歧 语义 角色 标注 主题 模型 
行为 表示 等 还有 文档 分析 比如 普通 的 文档 
聚 类 文档 分类 事件 挖掘 层次 聚 类 和 
意图 分类 其他 部分 就是 我们 尝试 比较 多 的 
偏 深度 学习 的 一些 自然 语言 算法 右边 的 
平台 工程 我们 做 了 很多 尝试 团队 经过 几年 
的 发展 不停 的 去 反思 如何 把 我们 的 
技术 快速 的 跟 业务 对 接起来 经过 不停 的 
尝试 之后 我们 做 了 很多 的 可视化 需求管理 用户 
中心 监控中心 系统 运维 还有 自动 的 标注 平台 训练 
平台 评测 中心 等等 经过 一 系列 的 封装 才会 
使得 平台 越来越 完善 图为 阿里 AliNLP 系统 架构图 左边 
是 算法 模块 包括 知识库 语料库 算法 模型 中间 是 
服务化 平台 比如 我们 的 服务 分为 在线 服务 和 
离线 服务 离线 服务 有 阿里 巴巴 最大 的 计算 
平台 ODPS 里面 做 了 很多 这 方面 的 UDF 
操作 在线/b 有/v HSF/w 和/c HTTP/w 服务/vn 可以 很好 的 
对接 各种各样 的 相关 服务 方 中间 有 用户 中心 
监控中心 测试中心 系统 运维 等 比较 复杂 的 一套 体系 
右边 是 我们 对接 的 一套 生态 平台 上面 可以 
通过 我们 的 接口 层 直接 对接 各种各样 的 应用 
我们 迭代 了 很多 轮 才 出现 这样 的 结构 
现在 大概 支持 30多 个 业务 方 平均 每天 的 
调 用量 在数 百亿 规模 AliNLP 平台 核心 价值 AliNLP 
平台 核心 价值 就是 解耦 我们 希望 通过 做 这样 
一个 平台 去 面对 整个 阿里 巴巴 的 生态 系统 
算法 超市 我们 希望 平台 是 NLP 算法 超市 业务 
方 可以 清晰 看到 分门别类 的 NLP 算法 工程 小白 
我们 希望 平台 解决 一切 工程 问题 算法 工程师 可以 
是 工程 小白 只需 专注 算法 研发 系统 生态 对于 
系统 以此 为 中心 形成 一个 系统 生态 体系 从/p 
各个/r 环节/n 切入/v 服务/vn NLP/w 算法/n 和/c 业务/n 服务 底线 
对于 产品运营 平台 只做 底层 模型 的 服务 输出 不 
直接 对接 业务 经过 各种各样 的 迭代 打磨 思考 反思 
5月 初会 发布 2.0 版本 我们 希望 做 持续 的 
改进 我们 平台 中 最 核心 的 三个 概念 如下 
1 . 模型 最 基本 的 算法 逻辑 复用 单元 
如果 用 算法 超市 的 概念 解释 模型 就是 原材料 
模型 是 算法 工程师 的 主要 产 出成果 2 . 
方案 是 多个 模型 的 组合 用于 真正 解决 某一 
方向 的 具体 问题 类似于 待售 的 超市 商品 方案 
是 业务 算法 的 结合 之处 我们 负责 算法 售卖 
的 同学会 应用 手头 已有 的 模型 通过 不同 的 
组合 配置 产生 出 不同 的 商品 供 最终 业务 
方的/nr 用户 使用 3 . 场景 是 多个 方案 在线 
上 部署 的 最终 形态 是 最终 服务 的 提供 
者 是 业务 方 真正 使用 我们 的 算法 大礼包 
的 地方 按 目前 的 设计 不同 的 业务 方 
可以 在 相互 隔离 的 多个 场景 中 使用 算法 
服务 只 有理解 这三个 概念 才会 知道 平台 怎么 去 
很好 的 使用 NLP 算法 举例 下面 对 我们 的 
算法 做 一些 比较 简单 的 举例 1 . 词 
法分析 分词 词性 实体 – 算法 基于 Bi LSTM CRF 
算法 体系 以及 丰富 的 多 领域 词表 – 应用 
优酷 YunOS 蚂蚁 金服/nr 推荐算法 资讯 搜索 等 2 . 
句法分析 依存 句法分析 成分 句法分析 – 算法 Shift reduce graph 
based Bi LSTM – 新闻 领域 商品 评价 商品 标题 
搜索 Query – 应用 资讯 搜索 评价 情感 分析 3 
. 情感 分析 情感 对象 情感 属性 情感 属性 关联 
– 算法 情感 词典 挖掘 属性 级 句子 级 篇章 
级 情感 分析 – 应用 商品 评价 商品 问答 品牌 
舆情 互联网 舆情 4 . 句子 生成 句子 可控 改写 
句子 压缩 – 算法 Beam Search Seq2Seq + Attention – 
应用 商品 标题 压缩 资讯 标题 改写 PUSH 消息 改写 
5 . 句子 相似 度 浅层 相似 度 语义 相似 
度 – 算法 Edit Distance Word2Vec DSSM – 应用 问 
大家 相似 问题 商品 重发 检测 影视作品 相似 等 6 
. 文本 分类 / 聚 类 垃圾 防控 信息 聚合 
– 算法 ME SVM FastText – 应用 商品 类目 预测 
问答 意图 分析 文本 垃圾 过滤 舆情 聚 类 名片 
OCR 后 语义 识别 等 7 . 文本 表示 词 
向量 句子 向量 篇章 向量 Seq2Seq – Word2Vec LSTM DSSM 
Seq2Seq 为 基础 进行 深入研究 8 . 知识库 – 数据 
规模 电商 同义词 通用 同义词 电商 上下位 通用 上下位 领域 
词库 电商 词 娱乐 领域 词 通用 实体词 情感 词库 
– 挖掘 算法 bootstrapping click through mining word2vec k means 
CRF – 应用 语义 归一 语义 扩展 Query 理解 意图 
理解 情感 分析 9 . 语料库 – 分词 词性 标注 
数据 依存 句法 标注 数据 有 这样 一句话 叫 我 
要买 秋天 穿 的 红色 连衣裙 这句 是 电商 领域 
中 比较 常见 的 词法 分析 结果 会把 中间 我 
要 拆开 分词 要 分 的 很准 它 不是 每个 
单字 都是/nr 一个 词 比如 秋天 是 一个 词 连衣裙 
是 一个 词 下面 这 一层 标签 是 对应 的 
词性 上面 这 一层 就是 句 子树 型 结构 它 
会 比较 深入 的 把 句子 比较 深度 的 结构化 
只有 把 它 结构化 之后 才能 导 到 数据库 里面 
去 才能 做 后续 的 各种 机器 学习 研究 和 
应用 这种 叫 结构 句法分析 对于 电商 而言 光有 句法分析 
是 不够 的 比如 我 要知道 秋天 的 含义 是 
说 这 是个 适用 季节 红色 是 一个 颜色 分类 
连衣裙 是 一个 产品 要 做到 这 一步 才 会 
使得 真正 在 电商 里面 用起来 比如 我们 用 的 
是 通用 领域 依存 分析器 我们 针对 商品 标题 决定 
某 一个 依存 句法 分析器 假设 某 一个 商品 标 
题写 的 是 我 要买 秋天 穿 的 红色 连衣裙 
只 需要 把 秋天 红色 连衣裙 这 几个 关键 的 
成分 标出来 我 要买 和 穿 的 对 电商 而言 
是 没有 意义 的 但会 去做 进一步 的 组合 如果 
这个 句子 是 一个 query 对于 某些 核心 成分 一点 
都 不需要 完全 不用 看 直接 会把 它 输出 秋天 
红色 连衣裙 三个 串 中间 依存关系 标出 就 可以 了 
这样 可以 做 很好 的 信息 凝练 这 是 我们 
针对 三种 不同 类型 的 文本 做 的 很 深入 
的 底层 自然语言 处理 分析 这个 例子 是 一个 买家 
对于 某 一个 商品 写 的 一个 评论 虽然 有点 
贵 不是 很 修身 但是 颜色 很亮 布料 摸 起来 
挺 舒服 的 图案 也 好看 挺 喜欢 的 上图 
是 我们 的 情感 分析 结果 我们 情感 分析 不但 
要 知道 整句 的 信息 比如说 整句 有 蓝色 淡蓝色 
淡 蓝色 表示 情感 是 正向 的 整个 句子 表达 
的 是 一个 比较 褒义 的 结果 但 不是 非常 
满意 再 下面 我们 做 的 更 深入 一点 比如说 
贵 修身 颜色 等等 做了 很 细粒度 的 一个 拆解 
这种 叫 属性 级 的 情感 分析 情感 词 比如说 
贵 它 是 一个 形容词 贵 表达 的 是 相对 
的 关系 有时候 说 黄金 很贵 这时 就 是 一个 
褒义 所以 这个 词语 非常复杂 不同 环境 下 褒贬不一 如 
修身 这个 平台 里面 表达 修身 是 一个 很 严重 
的 反向 关系 所以 我们 就 把 它 识别 出来 
是个 很 红色 的 关系 只要 经过 很 深度 的 
细致 分析 之后 后面 可以 做 各种各样 的 玩法 应用 
实例 图中 显示 商品 标题 和 副标题 2015 年秋冬 毛衣 
连衣裙 我 是 证人 杨幂 同 款 宽松 显 瘦 
时尚 打底 针织 连衣裙 它 不是 一个 自然 语言 的 
原 句子 是 一堆 词语 拼凑 在 一起 的 副标题 
就 自然 一点 因为 搜索引擎 以 关键词 为 核心 算法 
关键词 堆砌 的话 搜索 结果 不会 往前面 排 销量 就 
不好 所以 标题 就 变成 这个 样子 了 而 副标题 
没有 应用 这种 算法 副标题 不进 索引库 不能 搜索 只是 
一个 营销 的 额外 宣传语 所以 电商 的 自然 语言 
处理 是 很 有意思 的 对 标题 做 深度 理解 
和 分析 的 时候 我们 知道 商品 的 产品 词 
款式 材质 风格 服务 营销 适用 季节 等 做到 这种 
结构化 后 就 可以 把 一个 文本 串 变成 一个 
数据库 这个 摆件 的 标题 也 可以 做 很 深入 
的 分析 也 可以 变成 一长串 结果 如果 你 要 
建 一个 电子 商务 搜索引擎 的话 或者 电子 商务 推荐 
引擎 的话 只有 做到 这 一步 才 会使 你 的 
引擎 更加 智能 标题 分析 主要 分 四步 第一步 先 
做 分词 把 第一 行 变成 第二 行 打 空格 
用 了 很多 算法 词表 人工 优化 的 思路 第二步 
是 实体 打 标 需要 知道 每个 词语 是 什么 
含义 粉红 大 布娃娃 是个 品牌 泡泡袖 是个 袖 型 
等等 这样 你 的 搜索引擎 就 更加 智能 一点 第三步 
是 热度 计算 把 热度 分数 识别 出来 因为 串 
里面 每个 词 不是 等价 的 有些 重要性 非常 高 
有些 重要性 非常低 第四步 是 中心 识别 我们 用 依存 
句法 分析方法 来做 表达 这个 句子 的 最 核心 关系 
就是 春装 连衣裙 这 里面 可以 做 进一步 的 简化 
选取 合适 的 某一个 维度 的 信息 这样 你 的 
数据库 就 非常 好了 可以 做 很多 深入 的 工作 
如果 买家 写 的 原始 标题 非常 长 在 PC 
上 显示 一个 标题 但是 在 手机 上 显示 一长串 
的 时候 就 会把 标题 按照 字数 限制 截断 你 
会 发现 很多 截断 本来 不 应该 截断 之后 末尾 
那 一串 信息 其实 也是 蛮 关键 的 我们 把 
它 变成 如图 一种 关系 当 买家 来看 商品 信息 
的 时候 在 窄 屏 的 区域 里面 能够 很好 
的 显示 出来 所以 就 会 使得 我们 的 销量 
包括 购买 体验 都会/nr 提升 关于 舆情 文本 分析 我们 
有 文本 的 分类 标签 和 文档 聚 类 技术 
假如 你 在 手机 淘宝 app 评价 写了 一堆 东西 
就 进入 了 我们 的 流程 我们 的 系统 叫 
摩天轮 会 自动 的 把 你 写 的 每 一条 
评论 做 各种各样 的 分析 和 处理 包括 聚 类 
的 和 标签 的 很 细粒度 的 解析 商品 评价 
有关 商品 的 评价 我们 积累 了 几百 亿条 评论 
这 是 非常 海量 的 一个 数据库 它 通过 商品 
的 搜索 推荐 还有 文章 的 引导 到 商品 详情页 
之后 有 上亿 的 人 每一天 在看 评价 通过 看 
详情页 之后 你 可以 去 做 要么 收藏 要么 放 
购物车 要么 直接 购买 的 决策 后面 才有 支付 订单 
管理 最后 还有 评价 写下 来 评价 之后 评价 会 
经过 我们 的 过滤 挖掘 和 展现 再回到 详情页 里面 
来 这 就是 一个 闭环 真实 评价 对 购物 决策 
有 重要 作用 评价 作为 淘宝 最大 的 UGC 富含 
对 商品 的 体验 和 知识 浏览 评价 与否 对 
收藏 加 购 下单 客单价 均有 显著 影响 上图 为 
商品 详情页 下面 是 正常 写 的 评论 我们 会 
在 上面 做 大家 印象 会把 所有 的 评论 做 
一个 综合 的 摘取 和 总结 点击 某一个 下面 就 
会 变成 一堆 相关 文本 筛选 出来 并且 把 那 
一段 描述 的 文本 高亮 图 为 我们 的 算法 
总 架构图 如果 要做 某一个 电商 类 或者 某 种 
服务 体系 的 评价 系统 可以 采用 这种 模式 左边 
是 一种 溯源 的 机制 我们 希望 鼓励 用户 去 
写 更多 更好 的 评价 包括 交互 的 优化 去 
提升 有用 评论 的 积累 有了 数据 之后 我们 要 
去做 去 伪 去 伪 就是 我们 有 一个 评价 
雾霾 工作 会把/i 文本/n 和/c 图片/n 的/uj 垃圾/n 都/d 去掉/v 
做好 之后 才能 保证 信息 是 比较 真实 的 我们 
会 对 核心 数据库 做 语义分析 会 结合 某 一些 
类目 来做 做完 之后 我们 会 考虑 它 的 时效性 
和 个性化 还 有买 卖家 模型 再做 排序 折叠 和 
大家 印象 的 扶优 然后 再做 增值 我们 会 有 
一些 比如 优质 内容 库 推荐 理由 评价 有赏 通过 
评价 去 发现 商品 的 品质 好 不好 是不是 假货 
物流 满意度 如何 这 里面 可以 做 很多 很 深入 
的 分析 评价 雾霾 中间 是 非常 复杂 的 一套 
体系 有 很多 工程 很多 算法 迭代 了 很久 比如说 
广告 的 样本 怎么 采集 有全类/nr 目的/n 和/c 分类/n 目的/n 
还有 正常 广告 的 怎么 去做 拆分 有 一些 基础 
特征 库 比如 用户 特征 文本 特征 模型 特征 行为 
特征 等 做 融合 最后 再 用 一个 treelink 模型 
把 maxent 模型 贝叶斯 模型 和 dbn 模型 总体 做 
融合 然后再 回流 一天 一天 迭代 问 大家 商品 中 
有 另外 一个 很 有趣 的 产品 叫 问 大家 
以 买 奶粉 为例 假如 你 有 五个 邻居 有 
三个 邻居 买过 同一 款 奶粉 你 要买 奶粉 可能 
希望 多问 两家 如果 三 个人 都 买过 A 奶粉 
三 个人 的 回答 结果 综合 看一看 做 最终 的 
决策 我们 把 它 做成 产品化 那 我们 做 一个 
问题 的 拆解 分为 四类 无效 问题 相似 问题 问答 
排序 智能 分发 问 大家 3个 问题 解析 如下 无效 
问题 过滤 – 专业 的 外包 同学 标注 无效 问题 
Active Learning 筛选 待 标注 样本 – 分类 采用 LR 
+ GBDT 定制 特征 – 无效 问题 会 不断 变种 
算法 和 标注 迭代 推进 相似 问题 识别 – Doc2Vec 
然后 计算 相似 度 人工 评测 页面 问答 排序 – 
内容 丰富 度 点 赞 数 过滤 词表 匹配 数等 
加权 求和 – Detail 页 透出 的 一条 问 大家 
CTR 提升 内容 资讯 分析 针对 内容 我们 需要 做 
大量 的 分析 比如说 底层 我们 有 各种各样 的 数据库 
要 汇总 中间 有 一些 文本 算法 比如说 相关性 时效 
和 质量 CTR 预估 个性化 分类 打 标 质量 和去重/nr 
等等 中间 也 有 一些 系统工程 还有 服务 体系 上面 
是 业务 场景 比如 淘 秘籍 微淘/nr 淘宝 头条 知识 
卡片 社区 问答 等等 会 让 你 迅速 进入 一个 
很好 的 购物 背景 知识 状态 使 你 做 更好 
的 购物 决策 你 可以 在 手机 淘宝 搜索 结果 
页 的 第四 个 Tab 里 看到 我们 的 淘 
秘籍 产品 思考 自然语言 处理 难 在哪 呢 它 涉及 
到 人 的 认知 知识 = 语言 = 思考 = 
行动 左边 专注 到 知识 右边 专 注到 思考 和 
行动 它 是 非常 复杂 的 最难 的 问题 有 
两个 第一 就是 歧义 自然 语言 与 计算机 语言 是 
完全 不可 调和 的 计算机 语言 是 精确 的 可 
枚举 的 无 歧义 的 第二 是 变化 变化 是 
非常 剧烈 的 它 的 语法 是 群体 一致 个体 
有 差异 语言 每天 都在/nr 发生 变化 新词 总 在 
不断 的 产生 无法 穷举 不同 上下文 不同 含义 甚至 
随 时间 推移 词义 也在 发生变化 例如 Apple 公司 甚至 
词性 也在 发生变化 如 Google to google 那么 NLP 怎么走 
在 完全 搞清 人脑 机制 前 NLP 研发 永远 是 
在 模拟 人类 群体 智慧 在 某些 文字 方面 的 
表现 这种 模仿 的 效果 会 越来越 好 持续 提升 
更 深入 的 模拟 是 NLP 会 和 语音 图像 
视频 触觉 等 多维度 信息 融合 学习 我们 未来 会 
做 什么 我们 在 一年 之内 会 继续 把 AliNLP 
平台 做 的 完备 和 完善 开放 更多 的 能力 
服务 好 阿里 的 各种 生态 系统 我们 希望 调用 
量能 超过 千亿 两年 之内 我们 争取 能够 对外开放 普惠 
大众 更好 的 开放 融合 调 用量 希望 达到 万亿 
我们 希望 做 的 更 美好 上乘 阿里巴巴 iDST 自然语言 
处理 部 总监 博士 毕业 于 哈尔滨 工业 大学 自然 
语言 处理 方向 曾在/nr 新加坡/ns 资讯/n 技术/n 研究院/n 工作/vn 四/m 
年/m 担任/v 研究/vn 科学/n 家/q 负责/v 统计/v 机器翻译/l 系统/n 的/uj 
研发/l 和/c 应用/v 2014年 至今 在 阿里 巴巴 iDST 担任 
资深 专家 从零/nr 组建 了 自然 语言 处理 部门 负责 
自然语言 处理 技术 平台 的 研发 和 多项 核心 业务 
应用 最近 做完 UNIT 一个 小 项目 后 结合 同 
时期 看 KBQA 的 文章 对 NLP / NLU 方向 
产生 了 比较 大 的 兴趣 想 深入 学习 一下 
结合 一篇 综述 Recent Trends in Deep Learning Based Natural 
Language Processing 参考文献 5 为其 阅读 笔记 的 阐述 顺序 
把 相关 的 知识 补一补 本文 即 第一 部分 Word 
Embedding 主要 参考 文献 1 word2vec 中的 数学原理 详解 2 
Word Embedding 与 Word2Vec 3 自然语言 处理 中的 N Gram 
模型 详解 4 有谁 可以 解释 下 word embedding 知乎 
5 2017 基于 DL 的 NLP 研究 近况 目录 一 
Word Embedding 概述 二 Word2vec 之前 2.1 one hot2 . 
2 n gram2 . 3 co occurrence matrix2 . 4 
NLM 三 Word2vec3 . 1 CBOW3 . 1.1 基于 Hierarchical 
Softmax3 . 1.2 基于 Negative Sampling3 . 2 Skip gram3 
. 2.1 基于 Hierarchical Softmax3 . 2.2 基于 Negative Sampling 
一 Word Embedding 概述 简单 来说 词 嵌入 Word Embedding 
或者 分布式 向量 Distributional Vectors 是 将 自然 语言 表示 
的 单词 转换 为 计算机 能够 理解 的 向量 或 
矩阵 形式 的 技术 由于 要 考虑 多种 因素 比如 
词 的 语义 同义词 近义词 语料 中词 之间 的 关系 
上下文 和 向量 的 维度 处理 复杂度 等等 我们 希望 
近义词 或者 表示 同类 事物 的 单词 之间 的 距离 
可以 理想地 近 只有 拿到 很 理想 的 单词 表示 
形式 我们 才 更容易 地 去做 翻译 问答 信息 抽取 
等 进一步 的 工作 在 Word Embedding 之前 常用 的 
方法 有 one hot n gram co occurrence matrix 但是/c 
他们/r 都有/nr 各自/r 的/uj 缺点/n 下面 会 说明 2003年 Bengio 
提出 了 NLM 是 为 Word Embedding 的 想法 的 
雏形 而在 2013年 Mikolov 对其 进行 了 优化 即 Word2vec 
包含 了 两种 类型 Continuous Bag of Words Model 和 
skip gram model Word Embedding 是 基于 分布式 假设 distributional 
hypothesis 总的来说 word embedding 就是 一个 词 的 低维 向量 
表示 一般用 的 维度 可以 是 几十 到 几千 有了/nr 
一个 词 的 向量 之后 各种 基于 向量 的 计算 
就 可以 实施 如用 向量 之间 的 相似 度 来 
度 量词 之间 的 语义 相关性 其 基于 的 分布式 
假设 就是 出现 在 相同 上下文 context 下 的 词 
意思 应该 相近 所有 学习 word embedding 的 方法 都是 
在用 数学 的 方法 建模 词 和 context 之间 的 
关系 作者 李 明磊 9527 链接 https / / www 
. zhihu . com / question / 32275069 / answer 
/ 197721342 来源 知乎 著作权 归 作者 所有 商业 转载 
请 联系 作者 获得 授权 非商业 转载 请 注明 出处 
但是 Word Embedding 也有 其 局限性 比如 难以 对 词组 
做 分布式 表达 受限于 上下文 window 的 尺寸 有些 词 
例如 好 或 坏 的 上下文 可能 没什么 不同 甚至 
完全 一样 这对 情感 分析 任务 的 影响 非常 大 
此外 Word Embedding 对于 应用 场景 的 依赖 很强 所以 
针对 特殊 的 应用 场景 可能 需要 重新 训练 这样 
就 会很 消耗 时间 和 资源 为此 Bengio 提出 了 
基于 负 采样 negative sampling 的 模型 下面/f 本文/r 会/v 
将对/i Word2vec/i 之前/f 的/uj 常用/b 方法/n 和/c Word2vec/i 的/uj 两种/m 
模型/n 做/v 比较/d 详细/ad 的/uj 记录/n 和/c 理解/v 二 Word2vec 
之前 2.1 one hotone hot 是 最简单 的 一种 处理 
方式 通俗 地 去 讲 把 语料 中的 词汇 去 
重 取出 按照 一定 的 顺序 字典 序 出现 顺序 
等 排 列为 词汇表 则 每 一个 单词 都 可以 
表示 为 一个 长度 为 N 的 向量 N 为 
词汇表 长度 即 单词 总数 该 向量 中 除了 该词 
所在 的 分量 为 1 其余 均 置 为 0 
例如 有 语料库 如下 John likes to watch movies . 
Mary likes movies too . John also likes to watch 
football games . 假设 我们 的 词汇表 排序 结果 如下 
{ John 1 likes 2 to 3 watch 4 movies 
5 also 6 football 7 games 8 Mary 9 too 
10 } 那么 则有 如下 word 的 向量 表示 John 
1 0 0 0 0 0 0 0 0 0 
likes 0 1 0 0 0 0 0 0 0 
0 用 这样 的 方式 可以 利用 向量 相加 进一步 
表示 句子 和 文本 了 但是 one hot 有 很大 
的 局限性 语义 的 相似性 woman madam lady 从 语义上 
将 可能 是 相近 的 one hot 无法 表示 英语单词 
中的 复数 时态 我们 不会 在 排序 是 就把 同一 
单词 的 不同 形态 区别 开来 继而 再进 行向量 表示 
单词 之间 的 位置 关系 很多 时候 句 内 之间 
多 个 单词 比如 术语 会 同时 出现 多次 one 
hot 无法 表示 词 向量 长度 很大 一方面 2 的 
原因 另一方面 本身 大规模 语料 所含 的 词 数很多 处理 
会很 棘手 2.2 n gramn gram 可以 表示 单词 间 
的 位置 关系 所 反映 的 语义 关联 在 说明 
n gram 之前 我们 从 最初 的 句子 概率 进行 
推导 假设 一个 句子 为 n 个 单词 有序 排列 
记为 我们 将 其 简 记为 则 这个 句子 的 
概率 为 对于 单个 概率 意思为 该 单词 在前面 单词 
给定 的 情况 下 出现 的 概率 我们 利用 贝叶斯 
公式 可以 得到 其中 最后 一项 为 在 语料 中 
出现 的 频数 但是/c 长/a 句子/n 或者/c 经/n 过去/t 标点/n 
处理/v 后的/nr 文本/n 可能/v 很长/i 而且 太 靠前 的 词 
对于 词 的 预测 影响 不是 很大 于是 我们 利用 
马尔可夫 假设 取 该词 出现 的 概率 仅 依赖于 该词 
前面 的 n 1个 词 这 就是 n gram 模型 
的 思想 所以 上面 的 公式 变为 在 这里 我们 
不对 n 的 确定 做 算法 复杂 度上 的 讨论 
详细 请 参考文献 1 一般来说 n 取 3 比较 合适 
此外 对于 一些 概率 为 0 的 情况 所 出现 
的 稀疏 数据 采用 平滑 化 处理 此类 算法 很多 
以后 有 时间 再 具体 展开 学习 所以 n gram 
的 主要 工作 在于 确定 n 之后 对语 料中 的 
各种 吃 词串 进行 频数 统计 和 平滑 化 处理 
对于 所 需要 的 句子 概率 只要 将 之前 语料 
中 相关 概率 取出 计算 就 可以 了 当然 实际 
情况 是 对 做 最 优化 处理 参数 确定 后 
以后 的 概率 就 可以 通过 函数 确定 了 这 
就 需要 构造函数 后面 的 NLM 就是 做 这个 工作 
n gram 模型 会将 前文 的 语义 关联 纳入 考虑 
从而 形成 联合 分布 概率 表达 但是 尽管 去 前 
n 1个 单词 语料 大 的 情况 下 计算 量 
还是 很大 在 模拟 广义 情境 时 严重 受到了 维度 
灾难 curse of dimensionality 2.3 co occurrence matrix 共 现 
矩阵 也是 考虑 语料 中词 之间 的 关系 来 表示 
一个 非常 重要 的 思想 是 我们 认为 某个 词 
的 意思 跟 它 临近 的 单词 是 紧密 相关 
的 这 是 我们 可以 设定 一个 窗口 大小 一般 
是 5 ~ 10 如下 窗口 大小 是 2 那么 
在 这个 窗口 内 与 rests 共同 出现 的 单词 
就有 life he in peace 然后 我们 就 利用 这种 
共 现 关系 来 生成 词 向量 例如 现在 我们 
的 语料库 包括 下面 三份 文档资料 I like deep learning 
. I like NLP . I enjoy flying . 作为 
示例 我们 设定 的 窗口 大小 为 1 也 就是 
只看 某个 单词 周围 紧邻着 的 那个 单词 此时 将 
得到 一个 对称矩阵 共 现 矩阵 因为 在 我们 的 
语料库 中 I 和 like 做为 邻居 同时 出现 在 
窗口 中的 次数 是 2 所以 下 表中 I 和 
like 相交 的 位置 其 值 就是 2 这样 我们 
也 实现 了 将 word 变成 向量 的 设想 在 
共 现 矩阵 每 一行 或 每 一列 都是 对应 
单词 的 一个 向量 表示 虽然 Cocurrence matrix 一定 程度 
上 解决 了 单词 间 相对位置 也应 予以 重视 这个 
问题 但是 它 仍然 面对 维度 灾难 也 即是 说 
一个 word 的 向量 表示 长度 太 长了 这时 很 
自然地 会 想到 SVD 或者 PCA 等 一些 常用 的 
降 维 方法 当然 这也 会 带来 其他 的 一些 
问题 窗口 大小 的 选择 跟 n gram 中 确定 
n 也是 一样 的 窗口 放大 则 矩阵 的 维度 
也会 增加 所以 本质 上 还是 带 有 很大 的 
计算 量 而且 SVD 算法 运算量 也 很大 若 文本 
集 非常 多 则不 具有 可操作性 2.4 NLM 神经 语言 
模型 Neural Language Model 是 Word Embeddings 的 基本 思想 
在 很多 其他 文献 中 也有 神经 概率 语言 模型 
Neural Probabilistic Language Model NPLM 或者 神经 网络 语言 模型 
Neural Network Language Model NNLM 都是 指 一个 东西 NLM 
的 输入 是 词 向量 根据 参考文献 1 词 向量 
和 模型 参数 最终 的 语言 模型 可以 通过 神经网络 
训练 一同 得到 相比 于n/nr gram 通过 联合 概率 考虑 
词 之间 的 位置 关系 NLM 则是 利 用词 向量 
进一步 表示 词语 之间 的 相似性 比如 近义词 在 相似 
的 上下文 里 可以 替代 或者 同类 事物 的 词 
可以 在 语 料中 频数 不同 的 情况 下 获得 
相近 的 概率 结合 参考文献 1 举 一个 简单 例子 
在 一个 语料 C 中 S1 = A dog is 
sitting in the room . 共 出现 了 10000次 S2 
= A cat is sitting in the room 出现 了 
1次 按照 n gram 的 模型 当 我们 输入 A 
_ _ _ _ _ is sitting in the room 
来 预测 下划线 上 应该 填入 的 词 时 dog 
的 概率 会 远大于 cat 这 是 针对 于 语料 
C 得到 的 概率 但是 我们 希望 相似 含义 的 
词 在 目标 向量空间 中的 距离 比 不相关 词 的 
距离 更近 比如 v man v woman 约等于 v gentleman 
v madam 用 这样 生成 的 词 向量 或者 已 
经训 练好 的 模型 在 去做 翻译 问答 等 后续 
工作 时 就 会很 有效果 而 NLM 利 用词 向量 
表示 就能 达到 这样 的 效果 注 在 参考 文献 
1 中 作者 举 的 例子 是从 句子 概率 角度 
我 自己 的 理解 稍 有 不同 将 原 例 
放 在下面 NLM 的 神经 网络 训练样本 同 n gram 
的 取法 取 语 料中 任一 词 w 的 前 
n 1个 词 作为 Context w 则 Context w w 
就是 一个 训练样本 了 这里 的 每 一个 词 都被 
表示 为 一个 长度 为 L 的 词 向量 然后 
将 Context w 的 n 1个 词 向量 首位 连接 
拼成 n 1 L 的 长 向量 下面 为 NLM 
图解 注 此 图 向量 和 矩阵 的 维度 与 
参考 文献 中 相反 了 我们 得到 的 输出 结果 
为 长度 为 词汇 总数 的 向量 如果 想要 第 
i 个 分量 去 表示 当上 下为 context w 时下/nr 
一个 词 为 词典 中 第 i 个 词 的 
概率 还 需要 softmax 归一化 然后 我们 最初 想要 的 
结果 便是 注意 这 只是 取 一个 词 w 后 
输出 的 向量 y 我们 需要 的 就是 通过 训练 
集 所有 的 词 都做 一遍 这个 过程 来 优化 
得到 理想 的 W q 和U/nr b 那么 样本 中 
最初 的 词 向量 如何 获得 呢 在 参考 文献 
1 中 有 这样 两 段话 目前 我 还 没有 
彻底 搞懂 神经 网络 中 具体 的 机制 所以 暂时 
标记 一下 初步 推测 是 初始化 一个 矩阵 或者 可以 
粗暴 地 用 one hot 不过 这样 输入 层 的 
L = D 计算 量大 了 很多 然后 随着 训练 
的 过程 词 向量 也是 不断 更新 的 详细 还要 
参考 最优化 理论 下面 要说 的 Word2vec 便是 在 NLM 
基础上 的 优化 三 Word2vec 目前 学习 了解到 的 Word2vec 
有 基于 Hierarchical Softmax 和 基于 Negative Sampling 两种 方式 
参考文献 1 是从 两种 方式 分 别讲 解了 CBOW 和 
Skip gram 的 数学 构建 思路 和 过程 由于 这 
两个 模型 是 相反 的 过程 即 CBOW 是 在 
给定 上下文 基础 上 预测 中心词 Skip gram 在有 中心词 
后 预测 上下文 我 个人 是 把 两个 模型 按照 
两种 不同 的 计算 方法 做了 梳理 当然 数学 推导 
还是 一样 的 只不过 我 自己 看起来 更 舒服 在此 
再次 感谢 @ peghoty 大牛 的 详解 3.1 CBOW 基于 
前面 的 介绍 CBOW 的 思想 是 取 目标 词 
w 的 上下文 前后 相邻 词 而 不是 仅 之前 
的 词作 为 预测 前提 类似于 共 现 矩阵 的 
窗口 不同于 NLM 的 是 Context w 的 向量 不再 
是 前后 连接 而是 求和 我们 记为 此外 还将 NLM 
的 隐藏 层 去掉 了 当然 最大 的 区别 还是 
在 输出 层 基于 Hierarchical Softmax 的 CBOW 输出 层 
为 一颗 霍夫曼 树 叶子 节点 为 语料 中的 词汇 
构建 依据 便是 各 词 的 出现 频数 基于 Negative 
Sampling 则是 用 随机 负 采样 代替 霍夫曼 树 的 
构建 3 . 1.1 基于 Hierarchical Softmax 霍夫曼 树 的 
构建 在 这里 就不 展 开说 了 比较 简单 的 
算法 沿用 文献 1 的 表示 基于 Hierarchical Softmax 的 
CBOW 所要 构建 的 霍夫曼 树 所需 参数 如下 从 
根结 点到 w 对应 结点 的 路径 路径 上 包含 
结点 个数 到 w 路径 上 的 的 结点 结点 
编码 根 结点 不 编码 非 叶子 结点 包括 根 
结点 对应 的 向量 霍夫曼 树 构建 按照 频数 大 
小有 左右 两种 其实 都是/nr 自己 约定 的 在 这里 
就 不 麻烦 了 构建/v 后左/nr 结点/n 编码/n 为/p 0/m 
为 正 类 右 结点 为 1 为 负 类 
根据 逻辑 回归 一个 结点 被 分为 正 类 的 
概率 为 的 一些 性质 后面 用 的 到 所以 
之前 我们 要 构造 的 目标 函数 就 可以 写 
为 以下 形式 这个 公式 跟 之前 看 的 概率 
图 模型 有点像 不过 现在 有点 记不清 了 后面 我 
再 梳理 一下 看看 能 不能 串 起来 其中 整体 
表达式 这 是 一个 单词 我们 把 对 连乘 做 
对数 似 然 函数 然后 将 语料 中 所有 单词 
都 求和 则 目标函数 如下 明确 参数 有和/nr 我们 取 
其中 子式 来做 关于 两个 参数 的 梯度 因为 和是/nr 
对称 的 所以 的 为 所以 两者 就 可以 更新 
了 至此 我们 完成 了 对 参数 的 优化 参考文献 
1 提出 了 这样 一个 问题 我 的 理解 是 
可以 的 可能 我 对 最优化 方法 的 学习 还 
不够 全面 从 公式 拆解 上 好像 更 能说 的 
过去 但是 对于 收敛 速度 的 影响 可能 会 很大 
取平 均可 能 优化 得到 较好 的 结果 较慢 3 
. 1.2 基于 Negative Sampling 对于 大规模 语料 构建 霍夫曼 
树 的 工作量 是 巨大 的 而且 叶子 节点 为 
N 的 霍夫曼 数 需要 新添 N 1 个 结点 
而 随着 树 的 深度 增加 参数 计算 的 量 
也会 增加 很多 很多 得到 的 词 向量 也会 不够好 
为此 Mikolov 作出 了 优化 将 构建 霍夫曼 树 改为 
随机 负 采 样方法 对于 给定 的 上下文 Context w 
去 预测 w 如果 从 语料 中 就是 存在 Context 
w w 那么 w 就是 正 样本 其他 词 就是 
负 样本 我们 设 负 样本 集为 词 的 标签 
即 正 样本 标签 为 1 负 样本 标签 为 
0 等同于 霍夫曼 结点 的 左右 编码 只不过 与其 取值 
相反 这样 后面 的 公式 也就 很好 理解 了 同样 
我们 对 两个 参数 求导 然后 更新 参数 公式 形式 
是 一样 的 不再 写了 可见 对于 单词 w 基于 
Hierarchical Softmax 将其 频数 用来 构建 霍夫曼 树 正负 样本 
标签 取自 结点 左右 编码 而 基于 Negative Sampling 将其 
频数 作为 随机 采样 线段 的 子 长度 正负 样本 
标签 取 自从 语料 中 随机 取出 的 词 是否 
为 目标 词 构造 复杂度 小于 前者 3.2 Skip gram 
由于 Skip gram 是 CBOW 的 相反 操作 输入输出 稍 
有 不同 在 这里 仅 贴出 关键 公式 不再 具体 
说明 3 . 2.1 基于 Hierarchical Softmax 以上 均 来自 
参考文献 1 变量 表示 稍 有 不同 3 . 2.2 
基于 Negative Sampling 这个 作者 分析 较多 还 没有 完全 
看懂 后面 再补 如何 了解 人工智能 产品 体系 我们 从 
搭建 一个 人工 智能 产品 需要 一个 怎样 的 基础 
架构 到 剖析 架构 中 每个 组件 的 含义 以及 
对 整个 系统 起到 的 作用 和 扮演 的 角色 
最后 对 每个 组件 展开 讲起 1 人工智能 产品 实现 
逻辑 通常 的 一款 人工智能 产品 涉及 了 很多 技术 
包括 语音识别 语音合成 机器 视觉 自然语言 处理 文本 / 语义 
理解 等 多项 技术 等 交互 集成 人工智能 的 目标 
是 模拟 和 延伸 人 的 感知 理解 决策 学习 
交流 移动 和 操作 物体 的 能力 感知 是 人工智能 
实现 的 第一 步 目前 已经 有了/nr 实质性 的 进展 
理解/v 和/c 决策/n 需要/v 机器学习/i 和/c 人类/n 指导/n 相结合/v 的/uj 
方式/n 才能/v 实现/v 目前 阶段 的 人工智能 还是 弱 人工智能 
产品 的 流程 可以 概括 为 海量 数据 训练 和 
学习 从中 识别 规律 和 经验 新 数据 通过 得到 
的 经验 用 接近 人 的 思维 处理 通过 对 
角色 分工 处理过程 功能 价值 三 个 不同 的 角度 
一个 人工 智能 产品 的 体系 包含 四个 重要 角色 
1 基础 设施 提供者 2 数据 提供 者 3 数据 
处理者 4 系统 协调者 我们 从 数据流 开始 说起 人工智能 
的 产品 体系 是 一个 动态 流程 本质上 是 围绕 
数据采集 存储 计算 展开 的 1 数据 提供 者 使用 
各种 手段 获得 原始数据 2 数据 处理者 对 数据 进行 
加工 3 数据处理 者 进行 模型 训练 获得 可以 使用 
对模型 4 用 模型 对 新 数据 进行 预测 以上 
我们 就 完成 了 数据 信息 知识 智慧 的 过程 
再 随着 动态 循环 就是 训练 推断 再 训练 再 
推断 的 过程 产品 经理 需要 完成 系统 集成 需求 
定义 资源 协调 解决方案 封装 的 保障 工作 2 基础 
设施 1 传感器 对 信号 模式 进行 转换 主要 应用于 
可 穿戴 应用 高级 辅助 驾驶 健康 监测 工业 控制 
举个 例子 无人/n 车对/nr 传感器/n 有/v 激光/n 毫米波 超声波 红外线 
等 产品 经理 需要 对 不同 对 传感器 有 自己 
对 了解 2 芯片 完成 训练 和 推断 的 强大 
计算 能力 的 计算 核心 模型 训练 对/p 神经/n 网络/n 
和/c 海量/n 数据/n 计算/v 对/p 核心/n 部件/n 应该/v 有/v 充足/a 
对/p 了解/v 云端 推断 服务器 对 CPU GPU TPU 等 
计算 单元 终端设备 手机 摄像头 等 按照 定制 化 程度 
芯片 又 分为 通用 芯片 CPU GPU TPU 等 可以 
处理 通用 任务 类型 FPGA 半 定制 化 芯片 延时 
低 用 硬件 实现 软件 算法 ASIC 算法 模型 可以 
烧到 芯 片中 运行 效率 高 理论上 先用 FPGA 在 
市场 中 试错 之后 用 ASIC 量产 3 基础 平台 
1 大 数据 技术 算法 虽 好 数据 决胜 2 
云 计算技术 降低 了 研发 成本 3 数据 收集 数据 
收集 类似于 人类 对 各种 感觉 没有 感觉 就 无法 
判断 1 数据 来源 直接 购买 行业 数据 和 免费 
的 数据源 自行 采集 和爬取/nr 第三 方 合作 2 数据 
质量 1 关联度 2 时效性 3 范围 4 可信性 4 
数据 处理 对 原始 数据 对 加工 可以 概括 为 
数据     机器学习 给出 规则     新 数据 
通过 规则 得到 结果     伴随 着 输入 / 
输出 的 过程 自我 优化 5 机器 大脑 处理过程 识别 
理解 和 推理 决策 1 识别 大量 大量 的 数据 
存在 计算机中 计算 得到 一个 模型 对于 新 数据 判断 
2 理解 和 推理 识别 侧重 于人 对 环境 的 
感知 理解/v 和/c 推理/v 强电/i 深层次/b 的/uj 理解/v 和/c 归纳/v 
能力/n 是 对 识别 之后 的 数据 的 再次 处理过程 
3 做 决策 通过 对 外界 客观事物 环境 推理 和 
理解 来 判断 采取 怎样 的 行动 6 系统配置 统筹 
的 关键 环节 系统 协调 构建 一个 人工 智能系统 需要 
多方 协调 包括 基础 设施 提供者 信息 提供 者 信息 
处理者 在内 的 各种 公司 或 公司 内部 各个部门 系统 
协调者 需要 在 人工智能 的 不同 阶段 需求 定义 设计 
开发 系统优化 运行 保障 售后 支持 监控/vn 和/c 审计/v 发回/v 
资源/n 协调/v 和/c 统筹/v 作用/v 人工智能 产品 体系 最 常见 
的 发展 规律 是 一 开始 以 项目 交付 解决 
单个 场景 的 具体 需求 为主 看重 个性化 当 项目 
的 技术 和 产品 需求 验证 完毕 后 就 可以 
使 产品 走向 千人 千面 的 产品化 接下来 是 服务化 
通过 对外 开放 和 输出 各种 服务 能力 逐渐 与 
终端用户 具体 业务 解耦 统一 数据 中心 和 算法 平台 
最终 实现 平台 化 帮助 用户 实现 根据 自身 需求 
完成 各种 功能 模块 的 在线 快速 封装 和 灵活 
配置 考虑 到 企业 的 发展 速度 市场 规模 技术 
实现 瓶颈 及 业务 特殊性 多方面 因素 需要 人工 智能 
产品 经理 具有 成本 意识 市场 敏锐度 前瞻性 和 大局观 
等 综合 素质 7 不可逾越 的 红线 安全 隐私 伦理 
道德 1 安全 人工智能 产品 认为 可控 人工智能 产品 不会 
影响 公共 安全 2 隐私 人工智能 产品 经理 至少 要 
评估 一下 四项 1 评估所 有产品 流程 中 涉及 用户 
权利 的 风险 2 评估 产品 在 设计 或 运行 
过程 中 的 系统 描述 3 基于 产品 设计 或 
运行 的 目的 评估 过程 是否 是 必要 的 4 
针对 识别 出 的 风险 给出 针对 风险 的 管理 
措施 在 涉及 到 隐私 数据 保护 措施 中 我们 
可以 从三个/nr 方面 着手 1 减少 对 训练 数据量 的 
需求 1 生成 对抗 网络 GAN 通过 轮流 训练 判别 
器 和 生成器 令其 互相 对抗 从 复杂 概率分布 中 
取样 生成 文字 图片 语音 等 2 联合 学习 Federal 
Learning 部分 训练 过程 放到 用户 手机 将 模型 传回 
服务器 不 涉及 用户 敏感数据 3 迁移 学习 Transfer Learning 
把 一个 场景 学习 到 的 模型 举一反三 迁移 到 
类似 的 场景 中 的 方法 2 在 不 减少 
数据 的 基础 上 保护 隐私 1 差分 隐私 技术 
Different Privacy 在 数据库 检索 时 加入 满足 某种 分布 
的 噪声 使 查询 结果 随机化 2 同态 加密技术 Homomorphic 
Encryption 在 密文 上 进行 计算 生成 加密 结果 解密 
后的/nr 结果 与 对 明文 进行 相同 操作 产生 的 
结果 一致 核心 在于 支持 在 加密 的 数据 上 
进行 查询 操作 解决/v 数据/n 委托/n 给/p 第三/m 方/n 如云/nr 
计算/v 公司/n 时的/nr 安全/an 问题/n 3 提高 算法 可 解释性 
避免 黑盒子 事件 的 发生 3 伦理道德 在 产品 设计 
时 主要 从 以下 三 个 方面 重点 关注 人工智能 
的 特殊性 所 带来 的 伦理 问题 1 人工智能 产品 
算法 的 可 解释性 差 不透明 使得 一旦 发生 伦理道德 
事故 无法 评判 2 人工智能 代替人 履行 社会 职能 的 
时候 产品 的 不可 预见性 有 可能 导致 伦理 道德 
争议 3 人工智能 产品 的 道德 地位 值得 思考 8 
运维 管理 人工智能 产品 的 运维 和 传统 IT 运维 
的 出发点 都是 让 业务 高效 稳定 的 运行 评价 
标准 1 系统 能否 第一 时间 发现 异常 2 发现 
异常 后 能否 第一 时间 找 出 原因 3 从 
原因 能否 定义 到 具体 问题 4 问题 能否 自动 
修复 或者 自我 修复 5 未来 出现 类似 问题 能否 
提前 预警 目录 1 自然语言 处理 概述 2 自然语言 处理 
入门 基础 3 自然语言 处理 的 主要 技术 范畴 4 
自然语言 处理 基本点 5 特征 处理 6 模型 选择 7 
NLP 常用工具 8 NLP 语言 模型 9 快速 入门 NLP 
方法 10 自然语言 处理 学习 资料 1 自然语言 处理 概述 
自然语言 处理 Natural Language Processing NLP 是 计算机 科学 领域 
与 人工智能 领域 中 的 一个 重要 方向 它 研究人 
与 计算机 之间 用 自然 语言 进行 有效 通信 的 
理论 和 方法 融 语言学 计算机科学 数学 等 于 一体 
的 科学 旨在 从 文本 数据 中 提取 信息 目的 
是 让 计算机 处理 或 理解 自然语言 以 执行 自动 
翻译 文本 分类 和 情感 分析 等 自然语言 处理 是 
人工智能 中 最为 困难 的 问题 之一 2 自然语言 处理 
入门 基础 2.1 数学 基础 1 线性代数 向量 矩阵 距离 
计算 余弦 距离 欧式 距离 曼哈顿 距离 明可/nr 夫斯基 距离 
切比雪夫 距离 杰 卡德 距离 汉明 距离 标准 欧式 距离 
皮尔逊 相关系数 2 概率论 随机 试验 条件概率 全 概率 贝叶 
斯定理 信息论 3 统计学 图形 可视化 饼图 条形图 热 力图 
折线图 箱线图 散点图 雷达图 仪表盘 数据 度量 标准 平均数 中位数 
众数 期望 方差 标准差 概率分布 几何 分布 二项分布 正态分布 泊松分布 
统计 假设检验 2.2 语言学 基础 语音 词汇 语法 2.3 Python 
基础 廖 雪峰 教程 Python 从 入门 到 实践 2.4 
机器学习 基础 统计 学习 方法 机器学习 周志华 机器学习 实战 2.5 
深度 学习 基础 CNN RNN LSTM2 . 6 自然语言 处理 
的 理论 基础 统计 自然语言 处理 宗 成庆 第二 版 
Python 自然语言 处理 数学 之美 第二 版 3 自然语言 处理 
的 主要 技术 范畴 3.1 语义 文本 相似 度 分析 
语义 文本 相似 度 分析 是 对 两段 文本 的 
意义 和 本质 之间 的 相似 度 进行 分析 的 
过程 3.2 信息检索 Information Retrieval IR 信息检索 是 指 将 
信息 按 一定 的 方式 加以 组织 并 通过 信息 
查找 满足 用户 的 信息 需求 的 过程 和 技术 
3.3 信息 抽取 Information Extraction 信息 抽取 是 指 从非/nr 
结构化 / 半 结构化 文本 如 网页 新闻 论文 文献 
微博 等 中 提取 指定 类型 的 信息 如 实体 
属性 关系 事件 商品 记录 等 并 通过 信息 归并 
冗余 消除 和 冲突消解 等 手段 将 非 结构化 文本 
转换 为 结构化 信息 的 一项 综合 技术 3.4 文本 
分类 Text Categorization 文本 分类 的 任务 是 根据 给定 
文档 的 内容 或 主题 自动 分配 预先 定义 的 
类别 标签 3.5 文本 挖掘 Text   Mining 文本 挖掘 
是 信息 挖掘 的 一个 研究 分支 用于 基于 文本 
信息 的 知识 发现 文本 挖掘 的 准备 工作 由 
文本 收集 文本 分析 和 特征 修剪 三个 步骤 组成 
目前 研究 和 应用 最多 的 几种 文本 挖掘 技术 
有 文档 聚 类 文档 分类 和 摘要 抽取 3.6 
文本 情感 分析 Textual Affective Analysis 情感 分析 是 一种 
广泛 的 主观 分析 它 使用 自然 语言 处理 技术 
来 识别 客户 评论 的 语义 情感 语句 表达 的 
情绪 正 负面 以及 通过 语音 分析 或 书面 文字 
判断 其 表达 的 情感 等 3.7 问答 系统 Question 
Answering QA 自动 问答 是 指 利用 计算机 自动 回答 
用户 所 提出 的 问题 以 满足 用户 知识 需求 
的 任务 不同 于 现有 搜索引擎 问答 系统 是 信息 
服务 的 一种 高级 形式 系统 返回 用户 的 不再 
是 基于 关键词 匹配 排序 的 文档 列表 而是 精准 
的 自然 语言 答案 3.8 机器翻译 Machine Translation MT 机器 
翻译 是 指 利用 计算机 实现 从 一种 自然 语言 
到 另外 一种 自然 语言 的 自动 翻译 被 翻译 
的 语言 称为 源语言 source language 翻译 到 的 语言 
称作 目标语言 target language 机器翻译 研究 的 目标 就是 建立 
有效 的 自动 翻译 方法 模型 和 系统 打破 语言 
壁垒 最终 实现 任意 时间 任意 地点 和 任意 语言 
的 自动 翻译 完成 人们 无障碍 自由 交流 的 梦想 
3.9 自动 摘要 Automatic Summarization 自动 文摘 又称 自动 文档 
摘要 是 指 通过 自动 分析 给定 的 一篇 文档 
或 多篇 文档 提炼 总结 其中 的 要点 信息 最终 
输出 一篇 长度 较短 可读性 良好 的 摘要 通常 包含 
几句话 或 数百 字 该 摘要 中 的 句子 可直接 
出自 原文 也可 重新 撰写 所得 3.10 语音识别 Speech Recognition 
语言识别 指 的 是 将 不同 语言 的 文本 区 
分出来 其 利用 语言 的 统计 和 语法 属性 来 
执行 此 任务 语言识别 也 可以 被 认为 是 文本 
分类 的 特殊 情况 4 自然语言 处理 基本点 4.1 语料库 
Corpus 语料库 中 存放 的 是 在 语言 的 实际 
使用 中 真实 出现 过 的 语言 材料 语料库 是以 
电子 计算机 为 载体 承载 语言 知识 的 基础 资源 
真实 语料 需要 经过 加工 分析 和 处理 才能 成为 
有用 的 资源 4.2 中文分词 Chinese Word egmentation 1 中文分词 
指 的 是 将 一个 汉字 序列 切 分成 一个 
一个 单独 的 词 分词 就是 将 连续 的 字 
序列 按照 一定 的 规范 重新 组合 成词/nr 序列 的 
过程 2 现有 的 分词 方法 可 分为 三大类 基于 
字符串 匹配 的 分词 方法 基于 理解 的 分词 方法 
和 基于 统计 的 分词 方法 3 比较 流行 的 
中文分词 工具 jieba StanfordNLP HanLP SnowNLP THULAC NLPIR4 . 3 
词性 标注 Part of speech tagging 1 词性 标注 是 
指为 给定 句子 中 的 每个 词 赋予 正确 的 
词法 标记 给定 一个 切 好词 的 句子 词性 标注 
的 目的 是 为 每一个 词 赋予 一个 类别 这个 
类别 称为 词性 标记 part of speech tag 比如 名词 
noun 动词 verb 形容词 adjective 等 2 词性 标注 是 
一个 非常 典型 的 序列 标注 问题 最初 采用 的 
方法 是 隐 马尔科夫 生成式 模型 然后 是 判别式 的 
最大熵 模型 支持 向量 机 模型 目前 学术界 通常 采用 
的 结构 是 感知器 模型 和 条件 随 机场 模型 
近年来 随着 深度 学习 技术 的 发展 研究者 们 也 
提出 了 很多 有效 的 基于 深层 神经 网络 的 
词性 标注 方法 4.4 句法分析 Parsing 1 基于 规则 的 
句法结构 分析 2 基于 统计 的 语法 结构 分析 4.5 
词干 提取 Stemming 词干 提取 是 将 词语 去除 变化 
或 衍生 形式 转换 为 词干 或 原型 形式 的 
过程 词干 提取 的 目标 是 将 相关 词语 还原 
为 同样 的 词干 4.6 词形 还原 Lemmatization 词形 还 
原是 将 一组 词语 还原 为 词源 或 词典 的 
词目 形式 的 过程 4.7 停用词 过滤 停用词 过滤 是 
指在 文本 中 频繁 出现 且 对 文本 信息 的 
内容 或 分类 类别 贡献 不大 甚至 无 贡献 的 
词语 如 常见 的 介词 冠词 助词 情态动词 代词 以及 
连词 等 4.8 词 向 量化 Word Vector 词 向 
量化 是 用 一组 实数 构成 的 向量 代表 自然 
语言 的 叫法 这种 技术 非常 实用 因为 电脑 无法 
处理 自然语言 词/n 向/p 量化/v 可以/c 捕捉/v 到/v 自然/d 语言/n 
和实/nr 数间/n 的/uj 本质/n 关系/n 通过 词 向 量化 一个 
词语 或者 一段 短语 可以 用 一个 定 维 的 
向量 表示 word2vec from gensim . models import Word2Vec4 . 
9 命名 实体 消 歧 Named Entity Disambiguation 命名 实体 
消 岐 是 对 句子 中的 提到 的 实体 识别 
的 过程 例如 对 句子 Apple earned a revenue of 
200 Billion USD in 2016 命名 实体 消 岐 会 
推断出 句子 中的 Apple 是 苹果 公司 而 不是 指 
一种 水果 一般来说 命名 实体 要求 有 一个 实体 知识库 
能够 将 句子 中 提到 的 实体 和 知识库 联系起来 
4.10 命名 实体 识别 named   entity   recognition 命名 
实体 识别 是 识别 一个 句子 中 有 特定 意义 
的 实体 并 将其 区分 为 人名 机构 名 日期 
地名 时间 等 类别 的 任务 三种 主流 算法 CRF 
字 典法 和 混合 方法 5 特征 处理 5.1 特征提取 
Feature Extraction 特征提取 是 指 将 机器学习 算法 不能 识别 
的 原始数据 转化 为 算法 可以 识别 的 特征 的 
过程 举例 文本 分类 特征提取 步骤 1 对 训练 数据集 
的 每 篇文章 我们 进行 词语 的 统计 以 形成 
一个 词典 向量 词典 向量 里 包含 了 训练 数据 
里 的 所有 词语 假设 停用词 已 去除 且 每个 
词语 代表 词典 向量 中 的 一个 元素 2 在 
经过 第一步 的 处理 后 每 篇 文章 都 可以 
用 词典 向量 来 表示 这样一来 每 篇 文章 都 
可以 被 看作 是 元素 相同 且 长度 相同 的 
向量 不同 的 文章 具有 不同 的 向 量值 这 
也 就是 表示 文本 的 词 袋 模型 bag of 
words 3 针对于 特定 的 文章 如何 给 表示 它 
的 向量 的 每一个 元素 赋值 呢 最简单 直接 的 
办法 就是 0 1 法了/nr 简单 来说 对于 每 一篇 
文章 我们 扫描 它 的 词语 集合 如果 某 一个 
词语 出现 在 了 词典 中 那么 该 词语 在 
词典 向量 中 对应 的 元素 置 为 1 否 
则为 0 5.2 特征选择   Feature Selection 当 数据 预处理 
完成后 我们/r 需要/v 选择/v 有/v 意义/n 的/uj 特征/n 输入/v 机器/n 
学习/v 的/uj 算法/n 和/c 模型/n 进行/v 训练/vn 特征选择 是 指 
去掉 无关 特征 保留 相关 特征 的 过程 也 可以 
认为 是 从 所有 的 特征 中 选择 一个 最好 
的 特征 子集 特征选择 本质 上 可以 认为 是 降 
维 的 过程 from sklearn . feature _ extraction . 
text import TfidfVectorizer5 . 3 降 维 Dimension   Reduction 
6 模型 选择 6.1 马尔可夫 模型 隐 马尔可夫 模型 层次化 
隐 马尔可夫 模型 马尔可夫 网络 1 应用 词类 标注 语音识别 
局部 句法 剖析 语 块 分析 命名 实体 识别 信息 
抽取 等 应用于 自然科学 工程技术 生物科技 公用事业 信道编码 等 多个 
领域 2 马尔可夫 链 在 随机 过程 中 每个 语言 
符号 的 出现 概率 不 相互 独立 每个 随机 试验 
的 当前 状态 依赖 于 此前 状态 这种 链 就是 
马尔可夫 链 3 多元 马尔科夫 链 考虑 前 一个 语言 
符号 对 后 一个 语言 符号 出现 概率 的 影响 
这样 得出 的 语言 成分 的 链 叫做 一重 马尔可夫 
链 也是 二元 语法 二重 马尔可夫 链 也是 三元 语法 
三重 马尔可夫 链 也是 四元 语法 6.2 条件 随 机场 
CRF 1 条件 随 机场 用于 序列 标注 中文分词 中文 
人名 识别 和 歧义 消解 等 自然 语言 处理 中 
表现 出 很好 的 效果 原理 是 对 给定 的 
观察 序列 和 标注 序列 建立 条件 概率模型 条件 随 
机场 可 用于 不同 预测 问题 其 学习 方法 通常 
是 极大 似 然 估计 2 条件 随 机场 模型 
也 需要 解决 三 个 基本 问题 特征 的 选择 
参数 训练 和 解码 6.3 贝叶斯 网络 贝叶斯 网络 又 
称为 信度 网络 或 信念 网络 belief networks 是 一种 
基于 概率 推理 的 数学 模型 其 理论 基础 是 
贝叶斯 公式 6.4 最大熵 模型 7 NLP 常用工具 1 AnacondaAnaconda 
是 一个 用于 科学计算 的 Python 开发平台 支持 Linux Mac 
和 Windows 系统 提供 了 包 管理 与 环境 管理 
的 功能 可以 很 方便 地 解决 多 版本 Python 
并存 切换 以及 各种 第三方 包 安装 问题 Anaconda 利用 
conda 命令 来 进行 package 和 environment 的 管理 并且 
已经 包含 了 Python 和 相关 的 配套 工具 Anaconda 
集成 了 大量 的 机器学习 库 以及 数据 处理 必不可少 
的 第三 方库/nr 比如 NumPy SciPy Scikit Learn 以及 TensorFlow 
等 2 Scikit learnScikit learn 是 广 受欢迎 的 入门 
级 机器学习 库 包含 大量 的 机器学习 算法 和 特征提取 
实现 使用 非常 简便 Scikit learn 实现 的 是 浅层 
学习 算法 神经网络 仅 实现 了 多层 感知机 3 T 
e n s o r F l o w T 
e n s o r F l o w 是 
谷歌 基于 DistBelief 进行 研发 的 第二 代 人工智能 学习 
系统 可被 用于 语音 识别 或 图像 识别 等 多项 
机器学习 和 深度 学习 领域 4 KerasKeras 是 一个 高 
级别 的 Python 神经网络 框架 能在 TensorFlow 或者 Theano 上 
运行 Keras 的 作者 谷歌 AI 研究员 Francois Chollet 宣布 
了 一条 激动人心 的 消息 Keras 将会 成为 第一个 被 
添加 到 TensorFlow 核心 中的 高级别 框架 这 将会 让 
Keras 变成 Tensorflow 的 默认 API 5 GensimGensim 是 一款 
开源 的 第三 方 Python 工具包 用于 从 原始 的 
非 结构化 的 文本 中 无 监督 地 学习 到 
文本 隐 层 的 主题 向量 表达 它 支持 包括 
TF IDF LSA LDA 和 word2vec 在内 的 多种 主题 
模型 算法 支持 流式 训练 并 提供 了 诸如 相似 
度 计算 信息检索 等 一些 常用 任务 的 API 接口 
6 NLTK 在 NLP 领域 中 NLTK 是 最常 使用 
的 一个 Python 库 7 JiebaJieba 结巴 分词 是 最 
受欢迎 的 中文分词 工具 8 NLP 语言 模型 1 词 
的 独 热 表示 one hot representation 2 Bag of 
Words 3 Bi gram 和 N gram 4 词 的 
分布式 表示 distributed representation 5 共 现 矩阵 Cocurrence martrix 
6 神经 网络 语言 模型 Neural Networ Language model NNLM 
7 word2vec 连续 词 袋 模型 Continuous Bag of Words 
CBOW Skip Gram 模型 9 快速 入门 NLP 方法 1 
认真 看完 一本 NLP 相关 的 书 坚持 看完 一部 
视频 2 看 这两年 相关 方向 的 综述 论文 然后 
看 一些 经典 的 论文 和 最新 论文 3 独立 
实现 一个 小型 的 自然 语言 处理 项目 4 可以 
在 Github 上 找到 很多 相关 的 开源 代码 选 
一个 自己 感兴趣 的 方向 进行 研究 10 自然语言 处理 
学习 资料 1 我 爱 自然 语言 处理 2 一文 
读懂 自然语言 NLP 3 中文分词 原理 与 工具 4 自然语言 
处理 项目 资源库 汇总 NLP 领域 自然语言 处理 计算 语言学 
自然语言 理解 自然语言 生成 机器翻译 文本 分类 语音识别 语音合成 中文分词 
信息检索 信息 抽取 句法分析 问答 系统 自动 摘要 拼 写检查 
统计 机器翻译 NLP 专题 隐 马尔科夫 模型 最大熵 模型 条件 
随 机场 数学 之美 支持 向量 机 机器学习 SRILM Moses 
知网 IRSTLM NLTKNLP 人物 冯志伟 俞士汶/nr 董 振东 黄 昌宁 
黄曾阳/nr 周明 姚 天顺 刘群 宗 成庆 赵铁军 詹 卫东 
常/d 宝宝/nr 刘挺/nr 王海峰 哈工大 中文信息处理 人物 谱 中文信息 学会 
人物 谱 Franz Josef OchNLP 会议 ACL COLING TREC EMNLP 
其他 会议 NLP 会议 档次 NLP 书籍 自然语言 处理 相关 
书籍 取自 http / / wiki . 52nlp . cn 
/ 首页 特别 推荐 1 HMM 学习 最佳 范例 全文 
文档 2 无约束 最优化 全文 文档 一 书籍 1 自然语言 
处理 综论 英文版 第二 版 2 统计 自然语言 处理 基础 
英文版 3 用 Python 进行 自然语言 处理 NLTK 配 套书 
4 Learning Python 第三版 Python 入门 经典 书籍 详细 而 
不厌其烦 5 自然语言 处理 中的 模式识别 6 EM 算法 及其 
扩展 7 统计 学习 基础 8 自然语言 理解 英文版 似乎 
只有 前 9 章 9 Fundamentals of Speech Recognition 质量 
不太好 不过 第 6 章 关于 HMM 的 部分 比较 
详细 作者 之一 便是 Lawrence Rabiner 10 概率 统计 经典 
入门书 概率论 及其 应用 英文版 威廉 * 费勒 著 第一卷 
第二卷 DjVuLibre 阅读器 阅读 前 两卷 书 需要 11 一本 
利用 Perl 和 Prolog 进行 自然语言 处理 的 介绍 书籍 
An Introduction to Language Processing with Perl and Prolog 12 
国外 机器学习 书籍 之 1 Programming Collective Intelligence 中文 译名 
集体 智慧 编程 机器学习 & 数据挖掘 领域 近年 出 的 
入门 好书 培养 兴趣 是 最重要 的 一环 一上 来看 
大部头 很容易 被 吓走 的 2 Machine Learning 机器学习 领域 
无可争议 的 经典 书籍 下载 完毕 将 后缀 改为 pdf 
即可 豆瓣 评论 by 王宁 老 书 牛人 现在看来 内容 
并 不算 深 很多 章节 有 点到为止 的 感觉 但是 
很 适合 新手 当然 不能 新 到/v 连/nr 算法/n 和/c 
概率/n 都不/nr 知道/v 入门 比如 决策树 部分 就 很 精彩 
并且 这 几年 没有 特别 大 的 进展 所以 并不 
过时 另外 这本书 算是 对 97 年前 数十年 机器学习 工作 
的 大 综述 参考文献 列表 极 有价值 国内/s 有/v 翻译/v 
和/c 影印版/n 不 知道 绝版 否 3 Introduction to Machine 
Learning 13 国外 数据挖掘 书籍 之 1 Data . Mining 
. Concepts . and . Techniques . 2nd 数据挖掘 经典 
书籍 作者 Jiawei Han / Micheline Kamber 出版社 Morgan Kaufmann 
评语 华裔 科学家 写 的 书 相当 深入浅出 2   
Data Mining Practical Machine Learning Tools and Techniques3   Beautiful 
Data The Stories Behind Elegant Data Solutions Toby Segaran Jeff 
Hammerbacher 14 国外 模式识别 书籍 之 1 Pattern Recognition 2 
Pattern Recongnition Technologies and Applications 3 An Introduction to Pattern 
Recognition 4 Introduction to Statistical Pattern Recognition 5 Statistical Pattern 
Recognition 2nd Edition 6 Supervised and Unsupervised Pattern Recognition 7 
Support Vector Machines for Pattern Classification 15 国外 人工智能 书籍 
之 1 Artificial Intelligence A Modern Approach   2nd Edition 
人工智能 领域 无 争议 的 经典 2 Paradigms of Artificial 
Intelligence Programming Case Studies in Common LISP 16 其他 相关 
书籍 1 Programming the Semantic Web Toby Segaran Colin Evans 
Jamie Taylor2 Learning . Python 第四版 英 文二 课件 1 
哈工大 刘挺/nr 老师 的 统计 自然语言 处理 课件 2 哈工大 
刘秉 权 老师 的 自然语言 处理 课件 3 中科院计算所 刘群 
老师 的 计算 语言学 讲义 课件 4 中科院 自动化所 宗 
成庆 老师 的 自然语言 理解 课件 5 北大 常 宝宝 
老师 的 计算 语言学 课件 6 北大 詹 卫东 老师 
的 中文信息处理 基础 的 课件 及 相关 代码 7 MIT 
Regina Barzilay 教授 的 自然语言 处理 课件 52nlp 上 翻译 
了 前 5 章 8 MIT 大牛 Michael Collins 的 
Machine Learning Approaches for Natural Language Processing 面向 自然语言 处理 
的 机器 学习 方法 课件 9 Michael Collins 的 Machine 
Learning   机器学习 课件 10 SMT 牛人 Philipp Koehn Advanced 
Natural Language Processing 高级 自然语言 处理 课件 11 Philipp Koehn 
Empirical Methods in Natural Language Processing 课件 12 Philipp Koehn 
Machine Translation 机器翻译 课件 三 语言 资源 和 开源 工具 
1 Brown 语料库 a   XML 格式 的 brown 语料库 
带 词性 标注 b   普通 文本格式 的 brown 语料库 
带 词性 标注 c 合 并并 去除 空行 行首 空格 
用于 词性 标注 训练 browntest . zip2 NLTK 官方 提供 
的 语料库 资源 列表 3 OpenNLP 上 的 开源 自然语言 
处理 工具 列表 4 斯坦福 大学 自然 语言 处理 组 
维护 的 统计 自然语言 处理 及 基于 语料库 的 计算 
语言学 资源 列表 5 LDC 上 免费 的 中文 信息 
处理 资源 6 中文分词 相关 工具 1 Java 版本 的 
MMSEG mmseg v 0.3 . zip 作者 为 solol 详情 
可 参见 中文分词 入门 之 篇 外 2 张 华平 
老师 的 ICTCLAS2010 该 版本 非 商用 免费 一年 下载 
地址 http / / cid 51de2738d3ea0fdd . skydrive . live 
. com / self . aspx / . Public / 
ICTCLAS2010 packet release . rar7 热心 读者 finallyliuyu 提供 的 
一批 新闻 语料库 包括 腾讯 新浪 网易 凤凰 等 目前 
放在 CSDN 上 http / / finallyliuyu . download . 
csdn . net / 另外 finalllyliuyu 在 2010年 9月 又 
提供 了 一批 文本 文类 语料 详情 见 献给 热衷于 
自然语言 处理 的 业余 爱好者 的 中文 新闻 分类 语料库 
之 二四 文献 1 ACL IJCNLP 2009 论文 全集 a 
  大会 论文 Full Paper 第一卷 b   大会 论文 
Full Paper 第二卷 c   大会 论文 Short Paper 合集 
d   ACL09 之 EMNLP 2009 合集 e   ACL09 
所有 workshop 论文 合 pyhanlp 自然语言 处理 包 https / 
/ github . com / hankcs / pyhanlp   HanLP 
是 一系列 模型 与 算法 组成 的 NLP 工具包 由 
大 快 搜索 主导 并 完全 开源 目标 是 普及 
自然语言 处理 在 生产 环境 中 的 应用 HanLP 具备 
功能完善 性能 高效 架构 清晰 语料 时新 可自 定义 的 
特点 HanLP 提供 下列 功能 t 中文分词 HMM Bigram 速度 
与 精度 最佳 平衡 一百兆 内存 最 短路 分词 N 
最 短路 分词 由 字 构词 侧重 精度 全 世界 
最大 语料库 可 识别 新词 适合 NLP 任务 感知机 分词 
CRF 分词 词典 分词 侧重 速度 每秒 数千 万字符 省 
内存 极速 词典 分词 所有 分词器 都 支持 索引 全 
切分 模式 用户 自定义 词典 兼容 繁体中文 训练 用户 自己 
的 领域 模型 词性 标注 HMM 词性 标注 速度快 感知机 
词性 标注 CRF 词性 标注 精度高 命名 实体 识别 基于 
HMM 角色 标注 的 命名 实体 识别 速度快 中国 人 
名 识别 音译 人名 识别 日本 人名 识别 地名 识别 
实体 机构 名 识别 基于 线性 模型 的 命名 实体 
识别 精度高 感知机 命名 实体 识别 CRF 命名 实体 识别 
关键词 提取 TextRank 关键词 提取 自动 摘要 TextRank 自动 摘要 
短语 提取 基于 互信息 和 左右 信息熵 的 短语 提取 
拼音 转换 多音字 声母 韵母 声调 简繁转换 简繁 分歧 词 
简体 繁体 臺 灣 正 體 香港 繁 體 文本 
推荐 语义 推荐 拼音 推荐 字词 推荐 依存 句法分析 基于 
神经 网络 的 高性能 依存 句法 分析器 MaxEnt 依存 句法分析 
文本 分类 情感 分析 word2vec 词 向量 训练 加载 词语 
相似 度 计算 语义 运算 查询 KMeans 聚 类 文档 
语义 相似 度 计算 语料库 工具 部分 默认 模型 训练 
自 小型 语料库 鼓励 用户 自行 训练 所有 模块 提供 
训练 接口 语料 可 参考 OpenCorpus 在 提供 丰富 功能 
的 同时 HanLP 内部 模块 坚持 低 耦合 模型 坚持 
惰性 加载 服务 坚持 静态 提供 词典 坚持 明文 发布 
使用 非常 方便 默认 模型 训练 自 全世界 最大 规模 
的 中文 语料库 同时 自 带 一些 语料 处理 工具 
帮助 用户 训练 自己 的 模型 In     # 
encoding utf 8 from pyhanlp import * # 中文分词 print 
HanLP . segment 皇家 盐湖城 梅西 煤 球王 c 罗 
费城 联合 # 词性 标注 for term in HanLP . 
segment 徐 先生 还 具体 帮助 他 确定 了 把 
画 雄鹰 松鼠 和 麻雀 作为 主攻 目标 print { 
} \ t { } . format term . word 
term . nature # 获取 单词 与 词性 # 关键词 
提取 document = 水利部 水资源 司 司长 陈明忠 9月 29日 
在 国务院 新闻办 举行 的 新闻 发布会 上 透露 \ 
根据 刚刚 完成 了 水资源 管理 制度 的 考核 有 
部分 省 接近 了 红线 的 指标 \ 有 部分 
省 超过 红线 的 指标 对 一些 超过 红线 的 
地方 陈明忠 表示 对 一些 取 用水 项目 进行 区域 
的 限 批 \ 严格 地 进行 水资源 论证 和 
取水 许可 的 批准 print HanLP . extractKeyword document 5 
# 自动 摘要 print HanLP . extractSummary document 3 # 
依存 句法分析 print HanLP . parseDependency 徐 先生 还 具体 
帮助 他 确定 了 把 画 雄鹰 松鼠 和 麻雀 
作为 主攻 目标 皇家 盐湖城 梅西 煤 球王 c 罗 
费城 联合 徐先生 nr 还 d 具体 a 帮助 v 
他 rr 确定 v 了 ule 把 pba 画 v 
雄鹰 n w 松鼠 n 和 cc 麻雀 n 作为 
p 主攻 vn 目标 n w 水资源 陈明忠 进行 红线 
部分 严格 地 进行 水资源 论证 和 取水 许可 的 
批准 有 部分 省 超过 红线 的 指标 水利部 水资源 
司 司长 陈明忠 9月 29日 在 国务院 新闻办 举行 的 
新闻 发布会 上 透露 1 徐先生 徐先生 nh nr _ 
4 主谓 关系 _ _ 2 还 还 d d 
_ 4 状 中 结构 _ _ 3 具体 具体 
a ad _ 4 状 中 结构 _ _ 4 
帮助 帮助 v v _ 0 核心 关系 _ _ 
5 他 他 r r _ 4 兼语 _ _ 
6 确定 确定 v v _ 4 动宾 关系 _ 
_ 7 了 了 u u _ 6 右 附加 
关系 _ _ 8 把 把 p p _ 15 
状 中 结构 _ _ 9 画 画 v v 
_ 8 介 宾 关系 _ _ 10 雄鹰 雄鹰 
n n _ 9 动宾 关系 _ _ 11 wp 
w _ 12 标点符号 _ _ 12 松鼠 松鼠 n 
n _ 10 并列 关系 _ _ 13 和 和 
c c _ 14 左 附加 关系 _ _ 14 
麻雀 麻雀 n n _ 10 并列 关系 _ _ 
15 作为 作为 v v _ 6 动宾 关系 _ 
_ 16 主攻 主攻 v vn _ 17 定 中 
关系 _ _ 17 目标 目标 n n _ 15 
动宾 关系 _ _ 18 wp w _ 4 标点符号 
_ _ pyhanlp 中的 命名 实体 识别 对于 分词 而言 
命名 实体 识别 是 一项 非常 重要 的 功能 当然 
发现 新词 同样 重要 这 部分 内容 被 我 放在 
之后 的 提取 关键词 短语 提取 与 自动 摘要 新词 
识别 与 再 之后 的 案例 中了 首先 是 一个 
简单 的 例子 展示 一下 命名 实体 识别 的 效果 
之后 是 正式 内容 In     from pyhanlp import 
* HanLP 开启 命名 实体 识别 # 音译 人名 示例 
CRFnewSegment = HanLP . newSegment crf term _ list = 
CRFnewSegment . seg 译 智社 的 田丰 要说 的 是 
这 只是 一个 hanlp 命名 实体 识别 的 例子 print 
term _ list print \ n = = = = 
= = = = = = 命名 实体 开启 与 
关闭 对比 试验 = = = = = = = 
= = = \ n sentences = 北川景子 参演 了 
林诣彬 导演 的 速度 与 激情 3 林志玲 亮相 网友 
确定 不是 波多 野 结衣 龟山/ns 千/m 广和/nr 近藤/n 公园/n 
在/p 龟山/ns 公园/n 里/f 喝酒/v 赏花/n # 通过 HanLP 进行 
全局 设置 但是 部分 分词器 本身 可能 不 支持 某项 
功能 # 部分 分词器 本身 对 某些 命名 实体 识别 
效果 较好 HanLP . Config . j a p a 
n e s e N a m e R e 
c o g n i z e = False v 
i t e r b i N e w e 
g m e n t = HanLP . newSegment viterbi 
. e n a b l e J a p 
a n e s e N a m e R 
e c o g n i z e True CRFnewSegment 
_ new = HanLP . newSegment crf . e n 
a b l e J a p a n e 
s e N a m e R e c o 
g n i z e True # segSentence # CRFnewSegment 
_ 2 . seg2sentence sentences for sentence in sentences print 
crf CRFnewSegment . seg sentence print crf _ new CRFnewSegment 
_ new . seg sentence print viterbi v i t 
e r b i N e w e g m 
e n t . seg sentence 译 智社 / n 
的 / u 田丰 / nr 要 / v 说 
/ v 的 / u 是 / v 这 / 
r 只 / d 是 / v 一个 / m 
hanlp 命名 / vn 实体 / n 识别 / v 
的 / u 例子 / n = = = = 
= = = = = = 命名 实体 开启 与 
关闭 对比 试验 = = = = = = = 
= = = crf 北川 / ns 景子//nr n 参演 
/ v 了 / u 林诣彬 / nr 导演 / 
n 的 / u / w 速度 / n 与 
/ c 激情 / n 3 / m / w 
crf _ new 北川 / ns 景子//nr n 参演 / 
v 了 / u 林诣彬 / nr 导演 / n 
的 / u / w 速度 / n 与 / 
c 激情 / n 3 / m / w viterbi 
北川景子 / nrj 参演 / v 了 / ule 林诣彬 
/ nr 导演 / nnt 的 / ude1 / w 
速度 / n 与 / cc 激情 / n 3 
/ m / w crf 林志玲 / nr 亮相 / 
v 网友 / n / w 确定 / v 不 
/ d 是 / v 波多 野 / n 结衣 
/ n / w crf _ new 林志玲 / nr 
亮相 / v 网友 / n / w 确定 / 
v 不 / d 是 / v 波多 野 / 
n 结衣 / n / w viterbi 林志玲 / nr 
亮相 / vi 网友 / n / w 确定 / 
v 不是 / c 波多 野 结衣 / nrj / 
w crf 龟 / v 山/n/nr 千 / m 广/q/nr 
和/c/nr 近藤 / a 公园 / n 在 / p 
龟山 公园 / ns 里 / f 喝 / v 
酒 / n 赏/v/nr 花/n/nr crf _ new 龟 / 
v 山/n/nr 千 / m 广/q/nr 和/c/nr 近藤 / a 
公园 / n 在 / p 龟山 公园 / ns 
里 / f 喝 / v 酒 / n 赏/v/nr 
花/n/nr viterbi 龟山 千 广//nr nrj 和//nr cc 近藤 公园 
/ nrj 在 / p 龟山 / nz 公园 / 
n 里 / f 喝酒 / vi 赏花 / nz 
中国 人 名 识别 说明 目前 分词器 基本上 都 默认 
开启 了 中国 人名 识别 比如 HanLP . segment 接口 
中 使用 的 分词器 等等 用户 不必 手动 开启 上面 
的 代码 只是 为了 强调 有 一定 的 误 命中率 
比如 误 命中 关键 年 则 可以 通过 在 data 
/ dictionary / person / nr . txt 加入 一条 
关键 年 A 1 来 排除 关键 年 作为 人名 
的 可能性 也 可以 将 关键 年 作为 新词 登记 
到 自定义 词典 中 如果 你 通过 上述 办法 解决 
了 问题 欢迎 向我/nr 提交 pull request 词典 也 是 
宝贵 的 财富 建议 NLP 用户 使用 感知机 或 CRF 
词法 分析器 精度 更高 算法 详解   实战 HMM Viterbi 
角色 标注 中国 人名 识别 In     # 中文 
人名 识别 def demo _ chinese _ name _ recognition 
sentences segment = HanLP . newSegment . e n a 
b l e N a m e R e c 
o g n i z e True for sentence in 
sentences term _ list = segment . seg sentence print 
term _ list print i . word for i in 
term _ list sentences = 签约 仪式 前 秦光荣 李纪恒 
仇和 等 一同 会见 了 参加 签约 的 企业 家 
武大靖 创 世界 纪录 夺冠 中国 代表团 平昌 首金 区 
长庄 木 弟 新年 致辞 朱立伦 两岸 都 希望 共创 
双赢 习/v 朱/nr 历史/n 会晤/v 在即/i 陕西 首富 吴一坚 被 
带走 与 令计划 妻子 有 交集 据 美国之音 电台 网站 
4月 28日 报道 8岁 的 凯瑟琳 克罗尔 凤甫娟/nr 和 很多 
华裔 美国 小朋友 一样 小小 年纪 就 开始 学 小提琴 
了 她 的 妈妈 是 位 虎妈 么 凯瑟琳 和 
露西 庐 瑞 媛 跟 她们 的 哥哥 们 有 
一些 不同 王国强 高峰 汪洋 张朝阳 光着头 韩寒 小四 张浩/nr 
和胡/nr 健康/a 复员/n 回家/n 了/ul 王总 和 小丽 结婚 了 
编剧 邵钧林 和 稽道青 说 这里有 关天培 的 有关 事迹 
龚学平 等 领导 说 邓颖超 生前 杜绝 超生 demo _ 
chinese _ name _ recognition sentences print \ n = 
= = = = = = = = = 中文 
人名 基本 默认 已 开启 = = = = = 
= = = = = \ n print CRFnewSegment . 
seg sentences 0 签约 / vi 仪式 / n 前 
/ f / w 秦光荣 / nr / w 李纪恒 
/ nr / w 仇和 / nr 等 / udeng 
一同 / d 会见 / v 了 / ule 参加 
/ v 签约 / vi 的 / ude1 企业家 / 
nnt / w 签约 仪式 前 秦光荣 李纪恒 仇和 等 
一同 会见 了 参加 签约 的 企业家 武大靖 / nr 
创 / v 世界 / n 纪录 / n 夺冠 
/ vi / w 中国 / ns 代表团 / n 
平昌 / ns 首 / q 金/b/nr 武大靖 创 世界 
纪录 夺冠 中国 代表团 平昌 首 金 区长 / nnt 
庄木弟/nr / nr 新年 / t 致辞 / vi 区长 
庄木弟/nr 新年 致辞 朱立伦 / nr / w 两岸 / 
n 都/d/nr 希望 / v 共创 / v 双赢 / 
n / w 习/v/nr 朱//nr ag 历史 / n 会晤 
/ vn 在即 / vi 朱立伦 两岸 都 希望 共创 
双赢 习 朱 历史 会晤 在即 陕西 / ns 首富 
/ n 吴一坚 / nr 被 / pbei 带走 / 
v / w 与 / cc 令计划 / nr 妻子 
/ n 有//nr vyou 交集 / v 陕西 首富 吴一坚 
被 带走 与 令计划 妻子 有 交集 据 / p 
美国之音 / n 电台 / nis 网站 / n 4月 
/ t 28 / m 日 / b 报道 / 
v / w 8 / m 岁 / qt 的 
/ ude1 凯瑟琳 / nr / w 克 / q 
罗尔 / nr / w 凤甫娟/nr / nr / w 
和//nr cc 很多 / m 华裔 / n 美国 / 
nsf 小朋友 / n 一样 / uyy / w 小小 
/ z 年纪 / n 就 / d 开始 / 
v 学 / v 小提琴 / n 了 / ule 
/ w 她 / rr 的 / ude1 妈妈 / 
n 是 / vshi 位 / q 虎妈 / nz 
么 / y / w 据 美国之音 电台 网站 4月 
28 日 报道 8 岁 的 凯瑟琳 克 罗尔 凤甫娟/nr 
和 很多 华裔 美国 小朋友 一样 小小 年纪 就 开始 
学 小提琴 了 她 的 妈妈 是 位 虎妈 么 
凯瑟琳 / nr 和//nr cc 露西 / nr / w 
庐 瑞 媛 / nr / w / w 跟 
/ p 她们 / rr 的 / ude1 哥哥 / 
n 们 / k 有//nr vyou 一些 / m 不同 
/ a / w 凯瑟琳 和 露西 庐 瑞 媛 
跟 她们 的 哥哥 们 有 一些 不同 王国强 / 
nr / w 高峰 / n / w 汪洋 / 
n / w 张朝阳 / nr 光 / n 着 
/ uzhe 头 / n / w 韩寒 / nr 
/ w 小 / a 四 / m 王国强 高峰 
汪洋 张朝阳 光 着 头 韩寒 小 四 张浩 / 
nr 和//nr cc 胡 健康 / nr 复员 / v 
回家 / vi 了 / ule 张浩 和 胡 健康 
复员 回家 了 王总 / nr 和//nr cc 小丽 / 
nr 结婚 / vi 了 / ule 王总 和 小丽 
结婚 了 编剧 / nnt 邵钧林 / nr 和//nr cc 
稽道青 / nr 说 / v 编剧 邵钧林 和 稽道青 
说 这里 / rzs 有//nr vyou 关天培 / nr 的 
/ ude1 有关 / vn 事迹 / n 这里 有 
关天培 的 有关 事迹 龚学平 / nr 等 / udeng 
领导 / n 说 / v / w 邓颖超 / 
nr 生前 / t 杜绝 / v 超生 / vi 
龚学平 等 领导 说 邓颖超 生前 杜绝 超生 = = 
= = = = = = = = 中文 人名 
基本 默认 已 开启 = = = = = = 
= = = = 签约 / vn 仪式 / n 
前 / f / w 秦光荣 / nr / w 
李纪恒 / nr / w 仇和 / nr 等 / 
u 一同 / d 会见 / v 了 / u 
参加 / v 签约 / v 的 / u 企业家 
/ n / w 音译 人名 识别 说明 目前 分词器 
基本上 都 默认 开启 了 音译 人名 识别 用户 不必 
手动 开启 上面 的 代码 只是 为了 强调 算法 详解 
层叠/z 隐/n 马/n 模型/n 下/f 的/uj 音译/n 人名/n 和/c 日本/ns 
人名/n 识别/v In     # 音译 人名 识别 sentences 
= 一桶 冰水 当头 倒下 微软 的 比尔盖茨 Facebook 的 
扎克伯格 跟 桑德 博格 亚马逊 的 贝索斯 苹果 的 库克 
全都 不惜 湿身 入镜 这些 硅谷 的 科技 人 飞蛾扑火 
似地 牺牲 演出 其实 全为 了 慈善 世界 上 最长 
的 姓名 是 简森 乔伊 亚历山大 比基 卡利斯 勒 达夫 
埃利奥特 福克斯 伊维 鲁莫/nr 马 尔尼 梅尔斯 帕特森 汤普森 华莱士 
普雷斯顿 segment = HanLP . newSegment . e n a 
b l e T r a n s l a 
t e d N a m e R e c 
o g n i z e True for sentence in 
sentences term _ list = segment . seg sentence print 
term _ list print \ n = = = = 
= = = = = = 音译 人名 默认 已 
开启 = = = = = = = = = 
= \ n print CRFnewSegment . seg sentences 0 一桶 
/ nz 冰水 / n 当头 / vi 倒下 / 
v / w 微软 / ntc 的 / ude1 比尔盖茨 
/ nrf / w Facebook / nx 的 / ude1 
扎克伯格 / nr 跟 / p 桑德 博格 / nrf 
/ w 亚马逊 / nrf 的 / ude1 贝索斯 / 
nrf / w 苹果 / nf 的 / ude1 库克 
/ nr 全都 / d 不惜 / v 湿身 / 
nz 入镜 / nz / w 这些 / rz 硅谷 
/ ns 的 / ude1 科技 / n 人 / 
n / w 飞蛾 / n 扑火 / vn 似 
/ vg 地 / ude2 牺牲 / v 演出 / 
vn / w 其实 / d 全/a/nr 为了 / p 
慈善 / a / w 世界 / n 上 / 
f 最长 / d 的 / ude1 姓名 / n 
是 / vshi 简森 / nr / w 乔伊 / 
nr / w 亚历山大 / nr / w 比基 / 
nr / w 卡利斯 / nr 勒 / v / 
w 达夫 埃利奥特 福克斯 伊维 鲁莫/nr 马 尔尼 梅尔斯 帕特森 
汤普森 华莱士 普雷斯顿 / nrf / w = = = 
= = = = = = = 音译 人名 默认 
已 开启 = = = = = = = = 
= = 一桶 / m 冰水 / n 当头 / 
d 倒下 / v / w 微软 / a 的 
/ u 比尔盖茨 / n / w Facebook / l 
的 / u 扎克伯格 / n 跟 / p 桑德 
博格 / n / w 亚马逊 / nr 的 / 
u 贝索斯 / nr / w 苹果 / n 的 
/ u 库克 / nr 全都 / d 不惜 / 
v 湿身 / n 入镜 / v / w 这些 
/ r 硅谷 / n 的 / u 科技 / 
n 人 / n / w 飞蛾 / v 扑火 
似 / v 地 / u 牺牲 / v 演出 
/ v / w 其实 / d 全/d/nr 为了 / 
p 慈善 / a / w 日本 人名 识别 说明 
目前 标准 分词器 默认 关闭 了 日本 人名 识别 用户 
需要 手动 开启 这 是 因为 日本 人名 的 出现 
频率 较低 但是 又 消耗 性能 算法 详解 层叠/z 隐/n 
马/n 模型/n 下/f 的/uj 音译/n 人名/n 和/c 日本/ns 人名/n 识别/v 
In     # 日语 人名 识别 def demo _ 
japanese _ name _ recognition sentences segment = HanLP . 
newSegment . e n a b l e J a 
p a n e s e N a m e 
R e c o g n i z e True 
for sentence in sentences term _ list = segment . 
seg sentence print term _ list print i . word 
for i in term _ list sentences = 北川景子 参演 
了 林诣彬 导演 的 速度 与 激情 3 林志玲 亮相 
网友 确定 不是 波多 野 结衣 龟山/ns 千/m 广和/nr 近藤/n 
公园/n 在/p 龟山/ns 公园/n 里/f 喝酒/v 赏花/n demo _ japanese 
_ name _ recognition sentences print \ n = = 
= = = = = = = = 日文 人名 
标准 分词器 默认 未 开启 = = = = = 
= = = = = \ n print CRFnewSegment . 
seg sentences 0 北川景子 / nrj 参演 / v 了 
/ ule 林诣彬 / nr 导演 / nnt 的 / 
ude1 / w 速度 / n 与 / cc 激情 
/ n 3 / m / w 北川景子 参演 了 
林诣彬 导演 的 速度 与 激情 3 林志玲 / nr 
亮相 / vi 网友 / n / w 确定 / 
v 不是 / c 波多 野 结衣 / nrj / 
w 林志玲 亮相 网友 确定 不是 波多 野 结衣 龟山 
千 广//nr nrj 和//nr cc 近藤 公园 / nrj 在 
/ p 龟山 / nz 公园 / n 里 / 
f 喝酒 / vi 赏花 / nz 龟山 千 广 
和 近藤 公园 在 龟山 公园 里 喝酒 赏花 = 
= = = = = = = = = 日文 
人名 标准 分词器 默认 未 开启 = = = = 
= = = = = = 北川 / ns 景子//nr 
n 参演 / v 了 / u 林诣彬 / nr 
导演 / n 的 / u / w 速度 / 
n 与 / c 激情 / n 3 / m 
/ w 地名 识别 说明 目前 标准 分词器 都 默认 
关闭 了 地名 识别 用户 需要 手动 开启 这 是因为 
消耗 性能 其实/d 多数/m 地名/n 都/d 收录/v 在/p 核心/n 词典/n 
和/c 用户/n 自定义/l 词典/n 中/f 在 生产 环境 中 能靠/nr 
词典 解决 的 问题 就 靠 词典 解决 这是 最 
高效 稳定 的 方法 建议 对 命名 实体 识别 要求 
较高 的 用户 使用 感知机 词法 分析器 算法 详解 实战 
HMM Viterbi 角色 标注 地名 识别 In     # 
演示 数词 与 数量词 识别 sentences = 十九元 套餐 包括 
什么 九千九百九十九 朵玫瑰 壹佰块 都 不给 我 ９０１２３４５６７８只 蚂蚁 牛奶 
三〇〇克 * 2 ChinaJoy 扫黄 细则 露 胸 超 2 
厘米 罚款 t a n d a r d T 
o k e n i z e r = JClass 
com . hankcs . hanlp . tokenizer . t a 
n d a r d T o k e n 
i z e r t a n d a r 
d T o k e n i z e r 
. SEGMENT . e n a b l e N 
u m b e r Q u a n t 
i f i e r R e c o g 
n i z e True for sentence in sentences print 
t a n d a r d T o k 
e n i z e r . segment sentence print 
\ n = = = = = = = = 
= = 演示 数词 与 数量词 默认 未 开启 = 
= = = = = = = = = \ 
n CRFnewSegment . e n a b l e N 
u m b e r Q u a n t 
i f i e r R e c o g 
n i z e True print CRFnewSegment . seg sentences 
0 十九元 / mq 套餐 / n 包括 / v 
什么 / ry 九千九百九十九朵 / mq 玫瑰 / n 壹佰块 
/ mq 都/d/nr 不 / d 给 / p 我 
/ rr ９０１２３４５６７８只 / mq 蚂蚁 / n 牛奶 / 
nf 三〇〇克 / mq * / w 2 / m 
ChinaJoy / nx / w 扫黄 / vi / w 
细则 / n 露 / v 胸 / ng 超 
/ v 2 厘米 / mq 罚款 / vi = 
= = = = = = = = = 演示 
数词 与 数量词 默认 未 开启 = = = = 
= = = = = = 十九 / m 元/q/nr 
套餐 / n 包括 / v 什么 / r 机构 
名 识别 说明 目前 分词器 默认 关闭 了 机构 名 
识别 用户 需要 手动 开启 这 是因为 消耗 性能 其实 
常用 机构 名都 收录 在 核心 词典 和 用户 自定义 
词典 中 HanLP 的 目的 不是 演示 动态 识别 在 
生产 环境 中 能靠/nr 词典 解决 的 问题 就 靠 
词典 解决 这是 最 高效 稳定 的 方法 建议 对 
命名 实体 识别 要求 较高 的 用户 使用 感知机 词法 
分析器 算法 详解 层叠 HMM Viterbi 角色 标注 模型 下 
的 机构 名 识别 In     # 机构 名 
识别 sentences = 我 在 上海 林原/nr 科技 有限公司 兼职 
工作 我 经常 在台 川 喜宴 餐厅 吃饭 偶尔 去 
开元 地中海 影城 看电影 Segment = JClass com . hankcs 
. hanlp . seg . Segment Term = JClass com 
. hankcs . hanlp . seg . common . Term 
segment = HanLP . newSegment . e n a b 
l e O r g a n i z a 
t i o n R e c o g n 
i z e True for sentence in sentences term _ 
list = segment . seg sentence print term _ list 
print \ n = = = = = = = 
= = = 机构 名 标准 分词器 已经 全部 关闭 
= = = = = = = = = = 
\ n print CRFnewSegment . seg sentences 0 segment = 
HanLP . newSegment crf . e n a b l 
e O r g a n i z a t 
i o n R e c o g n i 
z e True 我 / rr 在 / p 上海 
/ ns 林原/nr 科技 有限公司 / nt 兼职 / vn 
工作 / vn / w 我 / rr 经常 / 
d 在 / p 台 川 喜宴 餐厅 / nt 
吃饭 / vi / w 偶尔 / d 去 / 
vf 开元 地中海 影城 / nt 看 / v 电影 
/ n / w = = = = = = 
= = = = 机构 名 标准 分词器 已经 全部 
关闭 = = = = = = = = = 
= 我 / r 在 / p 上 海林 原 
科技 有限公司 / nt 兼职 / vn 工作 / vn 
/ w 地名 识别 说明 目前 标准 分词器 都 默认 
关闭 了 地名 识别 用户 需要 手动 开启 这 是因为 
消耗 性能 其实/d 多数/m 地名/n 都/d 收录/v 在/p 核心/n 词典/n 
和/c 用户/n 自定义/l 词典/n 中/f 在 生产 环境 中 能靠/nr 
词典 解决 的 问题 就 靠 词典 解决 这是 最 
高效 稳定 的 方法 建议 对 命名 实体 识别 要求 
较高 的 用户 使用 感知机 词法 分析器 算法 详解 实战 
HMM Viterbi 角色 标注 地名 识别 In     # 
地名 识别 def demo _ place _ recognition sentences segment 
= HanLP . newSegment . e n a b l 
e P l a c e R e c o 
g n i z e True for sentence in sentences 
term _ list = segment . seg sentence print term 
_ list print i . word for i in term 
_ list sentences = 蓝翔给/nr 宁夏/ns 固原市/ns 彭阳县/ns 红河镇/ns 黑/a 
牛沟村/nr 捐赠/v 了/ul 挖掘机/n demo _ place _ recognition sentences 
print \ n = = = = = = = 
= = = 地名 默认 已 开启 = = = 
= = = = = = = \ n print 
CRFnewSegment . seg sentences 0 蓝翔//nr nr 给 / p 
宁夏 / ns 固原市 / ns 彭阳县 / ns 红河镇 
/ ns 黑 牛沟村/nr / ns 捐赠 / v 了 
/ ule 挖掘机 / n 蓝翔/nr 给 宁夏 固原市 彭阳县 
红河镇 黑 牛沟村/nr 捐赠 了 挖掘机 = = = = 
= = = = = = 地名 默认 已 开启 
= = = = = = = = = = 
蓝翔//nr v 给 / v 宁夏 / ns 固原市 / 
ns 彭阳县 / ns 红河镇 / ns 黑 牛沟村/nr / 
ns 捐赠 / v 了 / u 挖掘机 / n 
URL 识别 自动识别 URL 该 部分 是 在 demo 中 
发现 的 但是 原作者 并 没有 在 文档 中 提到 
这个 该 部分 可以 发现 URL 测试 发现 其他 分类器 
应该 是 默认 不 开启 这个 的 而且 config 中 
并 没有 开启 该 功能 的 选项 因此 这 应该 
是 一个 额外 的 类 我 建议 如果 有 需要 
的 你 可以 尝试 先 利用 URLTokenizer 获取 URL 然后 
添 加进 用户 词典 或者 直接 使用 其他 工具 或者 
自定义 函数 解决 该 问题 In     # URL 
识别 text = HanLP 的 项目 地址 是 https / 
/ github . com / hankcs / HanLP 发布 地址 
是 https / / github . com / hankcs / 
HanLP / releases 我 有时候 会在 www . hankcs . 
com 上面 发布 一些 消息 我 的 微博 是 http 
/ / weibo . com / hankcs / 会 同步 
推送 hankcs . com 的 新闻 听说 . 中国域名 开放 
申请 了 但 我 并 没有 申请 hankcs . 中国 
因为 穷 Nature = SafeJClass com . hankcs . hanlp 
. corpus . tag . Nature Term = SafeJClass com 
. hankcs . hanlp . seg . common . Term 
URLTokenizer = SafeJClass com . hankcs . hanlp . tokenizer 
. URLTokenizer term _ list = URLTokenizer . segment text 
print term _ list for term in term _ list 
if term . nature = = Nature . xu print 
term . word In     HanLP / nx 的 
/ ude1 项目 / n 地址 / n 是 / 
vshi https / / github . com / hankcs / 
HanLP / xu / w / w / w 发布 
/ v 地址 / n 是 / vshi https / 
/ github . com / hankcs / HanLP / releases 
/ xu / w / w / w 我 / 
rr 有时候 / d 会 / v 在 / p 
www . hankcs . com / xu 上面 / f 
发布 / v 一些 / m 消息 / n / 
w / w / w 我 / rr 的 / 
ude1 微博 / n 是 / vshi http / / 
weibo . com / hankcs / / xu / w 
会 / v 同步 / vd 推送 / nz hankcs 
. com / xu 的 / ude1 新闻 / n 
/ w / w / w 听说 / v . 
/ w 中国 / ns 域名 / n 开放 / 
v 申请 / v 了 / ule / w 但 
/ c 我 / rr 并 / cc 没有 / 
v 申请 / v hankcs . 中国 / xu / 
w 因为 / c 穷 / a / w / 
w / w https / / github . com / 
hankcs / HanLP https / / github . com / 
hankcs / HanLP / releases www . hankcs . com 
http / / weibo . com / hankcs / hankcs 
. com hankcs . 中国 此 篇 博客 为 自然语言 
处理 之 朴素 贝叶斯 的 总结 更多 详细 信息 参考 
超链接 内容 1 . 朴素 贝叶斯 = 贝叶斯 公式 + 
条件 独立 假设 2 . 朴素 贝叶斯 的 效果 好 
尤其 是 在有 大量 语料 的 情况 下 3 . 
处理 重复 语句 的 三种 方式 4 . 处理 未在 
训练 集中 覆盖 的 词语 平滑 技术 赋予 一个 小 
概率 从而 调低 整体 的 概率 5 . 直接 匹配 
关键词 处理 垃圾 邮件 为何 行不通 6 . 实际 工程 
中的 小 技巧 取 对数 把 乘法 变成 加法 . 
并 预 先把 对应 的 概率 求 出来 引入 正常 
邮件 出现 词语 的 概率 把 词语 概率 转化 为 
权重 P 垃圾邮件 中的 W / P 正常 邮件 中的 
W . 选取 前 N 个 关键词 需要 经验 分隔 
样本 因为 样本 长度 不一 前 N 个 关键词 的 
占 比 不同 给 位 单词 所在 置 赋权 7 
. 如何 处理 多分 类 问题 忽略 被 判断 的 
文本 的 概率 即用 似 然 函数 8 . 先验概率 
是否 准确 或者 相等 的 问题 贝叶斯 方法 需要 靠谱 
的 先验概率 否则 会在 最大/a 似/d 然/c 法和/nr 基本/n 的/uj 
朴素/nr 贝叶斯/nr 得出/v 不同/a 地/uv 结果/n 作者 给出 的 建议 
是 在 处理 多份 类 问题 时 知道 先验概率 具体 
数值 且不 相等 的 情况 下 考虑 删除 部分 数据 
使得 鲜艳 概率 相等 然后 用 最大 似 然 法 
为什么 不 考虑 基础 的 贝叶斯 方法 呢 如果 不 
知道 先验概率 就 只能 按 等比例 抽取 样本 然后按 先验概率 
相等 的 情况 处理 9 . 朴素 贝叶斯 方法 的 
常见 应用 褒贬 分析 1 对 否定句 进行 特别 处理 
2 最 相关 的 情感 词 在 片段 中 只 
出现 一次 词频 模型 作用 有限 用 伯努利 多项式 模型 
替代 3 考虑 副词 对 情感 的 影响 很 不好 
不是 很好 难点 情绪 的 含蓄 表达 欲扬先抑 等 转折 
10 . 拼写 纠 错非 词 错误 & 真 词 
错误 真 词 错误 比较 复杂 非 词 错误 可以 
直接 采用 贝叶斯 方法 P 候选词 i | 错误 词 
∝ P 错误 词 | 候选词 i P 候选词 i 
i = 1 2 3 一些 小 技巧 1 经验 
发现 80% 的 瓶 邪 错误 编辑 距离 为 1 
几乎 所有 的 拼写错误 编辑 距离 小于 等于 2 . 
2 键盘 上 临近 按键 更容易 拼写错误 可以 按 这个 
条件 加权 课程体系 阶段 一 人工智能 基础 － 高等数学 必 
知 必会 本 阶段 主要 从 数据分析 概率论/n 和/c 线性代数/l 
及/c 矩阵/n 和凸/nr 优化/vn 这/r 四大/l 块/zg 讲解/v 基础/n 旨在 
训练 大家 逻辑 能力 分析 能力 拥有 良好 的 数学 
基础 有利于 大家 在 后续 课程 的 学习 中 更好 
的 理解 机器学习 和 深度 学习 的 相关 算法 内容 
同时 对于 AI 研究 尤为重要 例如 人工智能 中的 智能 很大 
一部分 依托 概率论 实现 的 一 数据分析 1 常数 e2 
导数 3 梯度 4 Taylor5 gini 系数 6 信息熵 与 
组合 数 7 梯度 下降 8 牛顿 法二/nr 概率论 1 
微积分 与 逼近 论 2 极限 微分 积分 基本概念 3 
利用 逼近 的 思想 理解 微分 利用 积分 的 方式 
理解 概率 4 概率论 基础 5 古典 模型 6 常见 
概率分布 7 大数定理 和 中心 极限 定理 8 协方差 矩阵 
和 相关 系数 9 最大/a 似/d 然/c 估计/v 和/c 最大/a 
后验/nr 估计/v 三/m 线性代数 及 矩阵 1 线性空间 及 线性变换 
2 矩阵 的 基本 概念 3 状态 转移 矩阵 4 
特征向量 5 矩阵 的 相关 乘法 6 矩阵 的 QR 
分解 7 对称矩阵 正交矩阵 正定矩阵 8 矩阵 的 SVD 分解 
9 矩阵 的 求导 10 矩阵 映射 / 投影 四 
凸 优化 1 凸 优化 基本概念 2 凸 集 3 
凸函数 4 凸 优化 问题 标准 形式 5 凸 优化 
之 Lagerange 对偶 化 6 凸 优化 之 牛顿 法 
梯度 下 降法 求解 阶段 二 人工智能 提升 － Python 
高级 应用 随着 AI 时代 的 到来 以及 其 日益 
蓬勃 的 发展 Python 作为 AI 时代 的 头牌 语言 
地位 基本 确定 机器学习 是 着实 令人 兴奋 但 其 
复杂度 及 难度 较大 通常 会 涉及 组装 工作流 和 
管道 设置/vn 数据源/n 及/c 内部/f 和云/nr 部署/n 之间/f 的/uj 分流/n 
而/c 有了/i Python/w 库/n 后/f 可 帮助 加快 数据 管道 
且 Python 库 也在 不断更新 发布 中 所以 本 阶段 
旨在 为 大家 学习 后续 的 机器学习 减负 一 容器 
1 列表 list2 元组 tuple3 字典 dict4 数组 Array5 切片 
6 列表 推导 式 7 浅 拷贝 和深/nr 拷贝 二 
函数 1 lambda 表达式 2 递归函数 及 尾递归 优化 3 
常用 内置 函数 / 高阶 函数 4 项目 案例 约瑟夫 
环 问题 三 常用 库 1 时间 库 2 并发 
库 3 科学计算 库 4 Matplotlib 可视化 绘 图库 5 
锁 和 线程 6 多线程 编程 阶段 三 人工智能 实用 
－ 机器学习 篇 机器学习 利用 算法 去 分析 数据 学习 
数据 随后 对 现实 世界 情况 作出 判断 和 预测 
因此 与 预先 编写 好 只能 按照 特定 逻辑 去 
执行 指令 的 软件 不同 机器 实际上 是 在用 大量 
数据 和 算法 去 自我 训练 从而 学会 如何 完成 
一项 任务 所以 本 阶段 主要 从 机器学习 概述 数据 
清洗 和 特征选择 回归 算法 决策树 随机 森林 和 提升 
算法 SVM 聚 类 算 EM 算法 贝叶斯 算法 隐 
马尔科夫 模型 LDA 主题 模型 等 方面 讲解 一些 机器学习 
的 相关 算法 以及 这些 算法 的 优化 过程 这些 
算法 也 就是 监督 算法 或者 无 监督 算法 一 
机器学习 1 机器学习 概述 二 监督 学习 1 逻辑 回归 
2 softmax 分类 3 条件 随 机场 4 支持 向量 
机 svm5 决策树 6 随机 森林 7 GBDT8 集成 学习 
三 非 监督 学习 1 高斯 混合模型 2 聚 类 
3 PCA4 密度估计 5 LSI6 LDA7 双聚类/nr 8 降 维 
算法 四 数据处理 与 模型 调 优 1 特征提取 2 
数据 预处理 3 数据 降 维 4 模型 参数 调 
优 5 模型 持久化 6 模型 可视化 7 优化 算法 
坐标轴 下 降法 和 最小 角回 归法 8 数据挖掘 关联 
规则 算法 9 感知器 模型 阶段 四 人工智能 实用 － 
数据挖掘 篇 本 阶段 主要 通过 音乐文件 分类 和 金融 
反 欺诈 模型 训练 等 项目 帮助 大家 对于 上 
阶段 的 机器 学习 做 更 深入 的 巩固 为 
后续 深度 学习 及 数据挖掘 提供 项目 支撑 项目 一 
百度 音乐 系统文件 分类 音乐 推荐 系统 就是 利用 音乐 
网站 上 的 音乐 信息 向 用户 提供 音乐 信息 
或者 建议 帮助 用户 决定 应该 听 什么 歌曲 而 
个人化 推荐 则是 基于 音乐 信息 及 用户 的 兴趣 
特征 听歌 历史 行为 向 用户 推荐 用户 可能 会 
感兴趣 的 音乐 或者 歌手 推荐算法 主要 分为 以下 几种 
基于 内容 的 推荐 协同 过滤 推荐 基于 关联 规则 
推荐 基于 效用 推荐 基于 知识 推荐 等 推荐 系统 
常 用于 各个 互联网 行业 中 比如 音乐 电商 旅游 
金融 等 项目 二 千万级 P2P 金融 系统 反 欺诈 
模型 训练 目前 比较 火 的 互联网 金融 领域 实质 
是 小额 信贷 小额 信贷 风险 管理 本质上 是 事前 
对 风险 的 主动 把 控 尽可能 预测 和 防范 
可能 出现 的 风险 本 项目 应用 GBDT Randomforest 等 
机器学习 算法 做 信贷 反 欺诈 模型 通过 数据挖掘 技术 
机器学习 模型 对 用户 进行 模型 化 综合 度量 确定 
一个 合理 的 风险 范围 使 风险 和 盈利 达到 
一个 平衡 的 状态 阶段 五 人工智能 前沿 － 深度 
学习 篇 深度 学习 是 实现 机器 学习 的 技术 
同时 深度 学习 也 带来 了 机器 学习 的 许多 
实际 应用 拓展 了 AI 的 使用 领域 本 阶段 
主要 从 TensorFlow BP 神经网络 深度 学习 概述 CNN 卷积 
神经网络 递归 神经网 自动编码 机 序 列到 序列 网络 生成 
对抗 网络 孪生 网络 小样 本 学习 技术 等 方面 
讲解 深度 学习 相关 算法 以 掌握 深度 学习 前沿技术 
并 根据 不同 项目 选择 不同 的 技术 解决 方案 
针对 公司 样本 不足 采用 小 样本 技术 和 深度 
学习 技术 结合 是 项目 落地 的 解决 方案 1 
TensorFlow 基本 应用 2 BP 神经网络 3 深度 学习 概述 
4 卷积 神经网络 CNN 5 图像 分类 vgg resnet 6 
目标 检测 rcnn fast rcnn faster rcnn ssd 7 递归 
神经网络 RNN 8 lstm bi lstm 多层 LSTM9 无 监督 
学习 之 AutoEncoder 自动 编码器 10 Seq2Seq11 Seq2Seq with Attension12 
生成 对抗 网络 13 irgan14 finetune 及 迁移 学习 15 
孪生 网络 16 小样 本 学习 阶段 六 人工智能 进阶 
－ 自然语言 处理 篇 自然语言 处理 NLP 是 计算机 科学 
领域 与 人工智能 领域 中 的 一个 重要 方向 它 
已 成为 人工智能 的 核心 领域 自然语言 处理 解决 的 
是 让 机器 可以 理解 自然语言 这一 到 目前 为止 
都还/nr 只是 人类 独有 的 特权 被 誉为 人工智能 皇冠 
上 的 明珠 被 广泛 应用 本 阶段 从 NLP 
的 字 词 和 句子 全方位 多角度 的 学习 NLP 
作为 NLP 的 基础 核心技术 对 NLP 为 核心 的 
项目 如 聊天 机器人 合理 用药 系统 写诗 机器人 和 
知识图谱 等 提供 底层 技术 通过学习 NLP 和 深度 学习 
技术 掌握 NLP 具有 代表性 的 前沿 技术 1 词 
分词 词性 标注 代码 实战 2 词 深度 学习 之词 
向量 字 向量 代码 实战 3 词 深度 学习 之 
实体 识别 和 关系 抽取 代码 实战 4 词 关键词 
提取 无 用词 过滤 代码 实战 5 句 句法分析 语义分析 
代码 实战 6 句 自然语言 理解 一阶逻辑 代码 实战 7 
句 深度 学习 之 文本 相似 度 代码 实战 阶段 
七 人工智能 进阶 － 图像处理 篇 数字图像处理 Digital Image Processing 
是 通过 计算机 对 图像 进行 去除 噪声 增强 复原 
分割 提取 特征 等 处理 的 方法 和 技术 广泛 
的 应用 于 农牧业 林业 环境 军事 工业 和 医学 
等 方面 是 人工 智能 和 深度 学习 的 重要 
研究 方向 深度 学习 作为 当前 机器学习 领域 最 热门 
的 技术 之一 已经 在 图像 处理 领域 获得 了 
应用 并且 展现 出 巨大 的 前景 本 阶段 学习 
了 数字 图像 的 基本 数据 结构 和 处理 技术 
到 前沿 的 深度 学习 处理 方法 掌握 前沿 的 
ResNet SSD Faster RCNN 等 深度 学习 模型 对 图像 
分类 目标 检测 和 模式 识别 等 图像处理 主要 领域 
达到 先进水平 实际 工作 中 很多 项目 都 可以 转化 
为 本 课程 的 所学 的 知识 去 解决 如 
行人 检测 人脸 识别 和 数字 识别 一 图像 基础 
图像 读 写 保存 画图 线 圆 多边形 添加 文字 
二 图像 操作 及 算数 运算 图像 像素 读取 算数 
运算 ROI 区域 提取 三 图像 颜色 空间 运算 图像 
颜色 空间 相互 转化 四 图像 几何变换 平移 旋转 仿射变换 
透视 变换 等 五 图像 形态学 腐蚀 膨胀 开 / 
闭 运算 等 六 图像 轮廓 长宽 面积 周长 外接圆 
方向 平均 颜色 层次 轮廓 等 七 图像 统计学 图像 
直方图 八 图像 滤波 高斯滤波 均值 滤波 双边 滤波 拉普拉斯 
滤波 等 阶段 八 人工智能 终极 实战 － 项目 应用 
本 阶段 重点 以 项目 为 导向 通过 公安 系统 
人脸识别 图像 识别 以及 图像 检索 今日 头条 CTR 广告 
点击量 预估 序列 分析 系统 聊天 机器人 等 多个 项目 
的 讲解 结合 实际 来 进行 AI 的 综合 运用 
项目 一 公安系统 人脸识别 图像识别 使用 深度 学习 框架 从零开始 
完成 人脸 检测 的 核心 技术 图像 类别 识别 的 
操作 从 数据 预处理 开始 一步步 构建 网络 模型 并 
展开 分析 与 评估 方便 大家 快速 动手 进行 项目 
实践 识别 上千 种 人 靓 返回 层次化 结构 的 
每个人 的 标签 项目 二 公安系统 图像 检索 本 项目 
基于 卷积 神经网 在 训练 过程 中 学习 出 对应 
的 二 值 检索 向量 对 全部 图 先 做 
了 一个 分 桶 操作 每次 检索 的 时候 只取 
本 桶 和 临近 桶 的 图片 作 比对 而 
不是 在 全域 做 比对 使用 这样 的 方式 提高 
检索 速度 使用 Tensorflow 框架 建立 基于 ImageNet 的 卷积 
神经网络 并 完成 模型 训练 以及 验证 项目 三 今日 
头条 CTR 广告 点击量 预估 点击率 预估 是 广告 技术 
的 核心 算法 之一 它 是 很多 广告 算法 工程师 
喜爱 的 战场 广告 的 价值 就 在于 宣传 效果 
点击率 是 其中 最 直接 的 考核 方式 之一 点击率 
越大 证明 广告 的 潜在 客户 越多 价值 就 越大 
因此 才 会 出现 了 刷 点击率 的 工具 和 
技术 通过 对于 点击量 的 评估 完成 对于 潜在 用户 
的 价值 挖掘 项目 四 序列 分析 系统 时间 序列 
分析 Time Series Analysis 是 一种 动态数据 处理 的 统计 
方法 主要 基于 随机 过程 理论 和 数理统计 方法 研究 
随机 数据 序列 所 遵从 的 统计 规律 以便 用于 
解决 实际 问题 主要 包括 自 相关 分析 等 一般 
的 统计 分析 方法 构建 模型 从而 进行 业务 推断 
经典 的 统计 分析 是 假定 数据 序列 具有 独立性 
而 时间 序列 分析 则 侧重 于 研究 数据 样本 
序列 之间 的 依赖 关系 时间 序列 预测 一般 反应 
了 三种 实际 变化 规律 趋势 变化 周期性 变化 和 
随机 性 变化 时间/n 序列/n 预测/vn 常/d 应用/v 于/p 国民经济/n 
宏观/n 控制/v 企业 经营 管理 市场潜力 量 预测 天气预报 水文预报 
等 方面 是 应用 于 金融 行业 的 一种 核心 
算法 之一 项目 五 京东 聊天 机器人 / 智能 客服 
聊天 机器人 / 智能 客服 是 一个 用来 模拟 人类 
对话 或者 聊天 的 一个 系统 利用/n 深度/ns 学习/v 和/c 
机器/n 学习/v 等/u NLP/w 相关/v 算法/n 构建/v 出/v 问题/n 和/c 
答案/n 之间/f 的/uj 匹配/v 模型/n 然后 可以 将 其 应用 
到 客服 等 需要 在线 服务 的 行业 领域 中 
聊天 机器人 可以 降低 公司 客服 成本 还 能够 提高 
客户 的 体验 友 好性 在 一个 完整 的 聊天 
机器 人 实现 过程 中 主要 包含 了 一些 核心 
技术 包括 但 不限 于 爬虫 技术 机器学习 算法 深度 
学习 算法 NLP 领域 相关 算法 通过 实现 一个 聊天 
机器人 可以 帮助 我们 队 AI 整体 知识 的 一个 
掌握 项目 六 机器人 写 诗歌 机器人 写 诗歌 / 
小说 是 一种 基于 NLP 自然语言 相关 技术 的 一种 
应用 在 实现 过程 中 可以 基于 机器学习 相关 算法 
或者 深度 学习 相关 算法 来 进行 小说 / 诗歌 
构建 过程 人工智能 的 一个 终极 目标 就是 让 机器 
人 能够 像 人类 一样 理解 文字 并 运用 文字 
进行 创作 而 这个 目标 大致上 主要 分为 两个 部分 
也 就是 自然语言 理解 和 自然 语言 生成 其中 现阶段 
的 主要 自然语言 生成 的 运用 自然语言 生成 主要 有 
两种 不同 的 方式 分别 为 基于 规则 和 基于 
统计 基于 规则 是 指 首先 了解 词性 及 语法 
等 规则 再 依据 这样 的 规则 写出 文章 而 
基于 统计 的 本质 是 根据 先前 的 字句 和 
统计 的 结果 进而 判断 下一 个子 的 生成 例如 
马尔科夫 模型 就 是 一种 常用 的 基于 统计 的 
方法 项目 七 机器翻译 系统 机器翻译 又称 自动 翻译 是 
指 利用 计算机 将 一种 自然 语言 转换 为 另外 
一种 自然 语言 的 过程 机器 翻译 是 人工智能 的 
终极 目标 之一 具有 很高 的 研究 价值 同时 机器翻译 
也 具有 比较 重要 的 实用 价值 机器 翻译 技术 
在 促进 政治 经济 文化 交流 等 方面 起到 了 
越来越 重要 的 作用 机器翻译 主要 分为 以下 三 个 
过程 原文 分析 原文 译文 转换 和 译文 生成 机器 
翻译 的 方式 有 很多 种 但是 随着 深度 学习 
研究 取得 比较 大 的 进展 基于 人工 网络 的 
机器 翻译 也 逐渐 兴起 特别 是 基于 长 短时记忆 
LSTM 的 循环 神经网络 RDD 的 应用 为 机器翻译 添了 
一把 火 项目 八 垃圾邮件 过滤 系统 邮件 主要 可以 
分为 有效 邮件 和 垃圾 邮件 两大类 有效 邮件 指 
的 邮件 接收者 有 意义 的 邮件 而 垃圾邮件 转 
指 那些 没有 任何 意义 的 邮件 其 内容 主要 
包含 赚钱 信息 成人 广告 商业 或者 个人 网站 广告 
电子杂志 等 其中 垃圾邮件 又 可以 发为 良性 垃圾 邮件 
和 恶性 垃圾邮件 良性 垃圾邮件 指 的 就是 对 收件人 
影响 不大 的 信息 邮件 而 恶性 垃圾邮件 指 具有 
破坏性 的 电子邮件 比如 包 含病毒 木马 等 恶意 程序 
的 邮件 垃圾邮件 过滤 主要 使用 使用 机器学习 深度 学习 
等 相关 算法 比如 贝叶斯 算法 CNN 等 识别 出 
所 接收 到 的 邮件 中 那些 是 垃圾 邮件 
项目 九 手工 数字 识 别人 认知 世界 的 开始 
就是 从 认识 数字 开始 的 深度 学习 也 一样 
数字 识别 是 深度 学习 的 一个 很好 的 切入口 
是 一个 非常 经典 的 原型 问题 通过 对 手写 
数字 识别 功能 的 实现 可以 帮助 我们 后续 对 
神经 网络 的 理解 和 应用 选取 手写 数字 识别 
的 主要 原因 是 手写 数字 具有 一定 的 挑战性 
要求 对 编程 能力 及 神经 网络 思维 能力 有 
一定 的 要求 但 同时 手写 数字 问题 的 复杂度 
不高 不 需要 大量 的 运算 而且 手写 数字 也 
可以 作为 其它 技术 的 一个 基础 所以 以 手写 
数字 识别 为 基础 贯穿 始终 从而 理解 深度 学习 
相关 的 应用 知识 项目 十 癌症 筛选 检测 技术 
可以 改变 癌症 患者 的 命运 吗 对于 患有 乳腺癌 
患者 来说 复发 还是 痊愈 影响 这 患者 的 生命 
那么 怎么 来 预测 患者 的 患病 结果呢 机器学习 算法 
可以 帮助 我们 解决 这 一 难题 本 项目 应用 
机器学习 logistic 回归模型 来 预测 乳腺癌 患者 复发 还是 正常 
有效 的 预测 出 医学 难题 项目 十一 葡萄酒 质量 
检测 系统 随着 信息 科技 的 快速 发展 计算机中 的 
经典 算法 在 葡萄酒 产业 中 得到 了 广泛 的 
研究 与 应用 其中 机器学习 算法 的 特点 是 运用 
了 人工智能 技术 在/p 大量/n 的/uj 样本/n 集/q 训练/vn 和/c 
学习/v 后/f 可以/c 自动/vn 地/uv 找出/v 运算/vn 所/c 需要/v 的/uj 
参数/n 和/c 模型/n 项目 十二 淘宝网 购物篮 分析 推荐算法 购物篮 
分析 Market Basket Analysis 即 非常 有名 的 啤酒 尿布 
故事 的 一个 反应 是 通过 对 购物篮 中 的 
商品 信息 进行 分析 研究 得出 顾客 的 购买 行为 
主要 目的 是 找出 什么样 的 物品 会 经常 出现 
在 一起 也 就是 那些 商品 之间 是 有 很大 
的 关联性 的 通过 购物篮 分析 挖掘 出来 的 信息 
可以 用于 指导 交叉 销售 追加 销售 商品 促销 顾客 
忠诚度 管理 库存 管理 和 折扣 计划 等 业务 购物篮 
分析 的 最 常用 应用 场景 是 电商 行业 但 
除此之外 该 算法 还被 应用于 信用卡 商城 电信 与 金融 
服务 业 保险业 以及 医疗 行业 等 项目 十三 手工 
实现 梯度 下降 回归 算法 梯度 下 降法 英语 Gradient 
descent 是 一个 一 阶 最优化 算法 通常 也 称为 
最速 下 降法 要 使用 梯度 下 降法 找到 一个 
函数 的 局部 极小值 必须 向 函数 上当 前点 对应 
梯度 或者 是 近似 梯度 的 反方向 的 规定 步 
长距离 点 进行 迭代 搜索 如果 相反地 向 梯度 正方向 
迭代 进行 搜索 则会 接近 函数 的 局部 极大值 点 
这个 过程 则 被称为 梯度 上升 法 项目 十四 基于 
TensorFlow 实现 回归 算法 回归 算法 是 业界 比较 常用 
的 一种 机器学习 算法 通过 应用 于 各种 不同 的 
业务 场景 是 一种 成熟 而 稳定 的 算法 种类 
TensorFlow 是 一种 常 用于 深度 学习 相关 领域 的 
算法 工具 随着 深度 学习 热度 的 高涨 TensorFlow 的 
使用 也 会 越来越 多 从而 使用 TensorFlow 来 实现 
一个 不 存在 的 算法 会 加深 对 TensorFlow 的 
理解 和 使用 基于 TensorFlow 的 回归 算法 的 实现 
有助于 后续 的 TensorFlow 框架 的 理解 和 应用 并 
可以 促进 深度 学习 相关 知识 的 掌握 项目 十五 
合理 用药 系统 合理 用药 系统 是 根据 临床 合理 
用药 专业 工作 的 基本 特点 和 要求 运用 NLP 
和 深度 学习 技术 对 药品 说明书 临床 路径 等 
医学 知识 进行 标准化 结构化 处理 如 自动 提取 药品 
说明书 文本 里面 的 关键 信息 如 药品 相互作用 禁忌 
用法 用量 适用人群 等 实现 医嘱 自动 审查 及时 发现 
不 合理 用药 问题 帮助 医生 药师 等 临床 专业 
人员 在 用药 过程 中 及时 有效 地 掌握 和 
利用 医药 知识 预防 药物 不良 事件 的 发生 促进 
临床 合理 用药 工作 项目 十六 行人 检测 行人 检测 
是 利用 图像 处理 技术 和 深度 学习 技术 对 
图像 或者 视频 序列 中 是否 存在 行人 并 给予 
精确定位 学习 完 行人 检测 技术 后 对 类似 的 
工业 缺陷 检测 外观 检测 和 医疗 影像 检测 等 
目标 检测 范畴 类 的 项目 可以 一通百通 该 技术 
可 与 行人 跟踪 行人 重 识别 等 技术 结合 
应用于 人工 智能系统 车辆 辅助 驾驶 系统 智能 机器人 智能 
视频 监控 人体 行为 分析 智能 交通 等 领域 由于 
行人 兼具 刚性 和 柔性 物体 的 特性 外观 易受 
穿着 尺度 遮挡 姿态 和 视角 等 影响 使得 行人 
检测 成为 计算机 视觉 领域 中 一个 既 具有 研究 
价值 同时 又 极具 挑战性 的 热门 课题 项目 十七 
时间 序列 算法 模型 拿到 一个 观察 序列 后 首先 
要 对 它 的 平稳性 和纯/nr 随机性 进行 检验 这 
两个 重要 的 检验 称为 序列 的 预处理 根据 检验 
的 结果 可以 将 序列 分为 不同 的 类型 对 
不同 的 类型 我 们 采用 不同 的 分析 方法 
1 移动 平均法 MA 2 自 回归模型 AR AR 模型 
是 一种 线性 预测 即 已知 N 个 数据 可由 
模型 推出 第 N 点 前面 或 后面 的 数据 
设 推出 P 点 本质 类似于 插值 其 目的 都是/nr 
为了 增加 有效 数据 只是 AR 模型 是由 N 点 
递推 而 插值 是由 两点 或 少数 几点 去 推导 
多点 所以 AR 模型 要比 插值 方法 效果 更好 3 
自 回归 滑动 平均 模型 ARMA 其 建模 思想 可 
概括 为 逐渐增加 模型 的 阶数 拟合 较 高阶 模型 
直到 再 增加 模型 的 阶数 而 剩余 残差 方差 
不再 显著 减小 为止 4 指数 平滑 法 移动 平均法 
的 预测 值 实质上 是 以前 观测值 的 加权 和 
且 对 不同 时期 的 数据 给予 相同 的 加权 
这 往往 不 符合 实际 情况 指数 平滑 法则 对 
移动 平均法 进行 了 改进 和 发展 其 应用 较为 
广泛 基本思想 都是 预测值 是 以前 观测值 的 加权 和 
且 对 不同 的 数据 给予 不同 的 权 新 
数据 给 较大 的 权 旧 数据 给 较小 的 
权 根据 平滑 次数 不同 指数 平滑 法 分为 一次 
指数 平滑 法 二次/m 指数/n 平滑/a 法和/nr 三次/m 指数/n 平滑/a 
法等/nr 项目/n 十八/m PySpark 大 数据 机器学习 框架 Spark 由 
AMPLab 实验室 开发 其 本质 是 基于 内存 的 快速 
迭代 框架 迭代 是 机器学习 最大 的 特点 因此 非常 
适合 做 机器学习 得益于 在 数据 科学 中 强大 的 
表现 Python 是 一种 解释 型 面向对象 动态数据 类型 的 
高级 程序 设计 语言 结合 强大 的 分布式 内存 计算 
框架 Spark 两个 领域 的 强者 走到 一起 自然 能碰出/nr 
更加 强大 的 火花 Spark 可以 翻 译为 火花 Spark 
的 Python API 几乎 覆盖 了 所有 Scala API 所能 
提供 的 功能 只有 极少数 的 一些 特性 和 个别 
的 API 方法 暂时 还 不支持 但 通常 不 影响 
我们 使用 Spark Python 进行编程 项目 十九 天池 kaggle 比赛 
2014年 3月 阿里 巴巴 集团 董事局 主席 马云 在 北京大学 
发起 天池 大 数据 竞赛 首届 大赛 共有 来自 全球 
的 7276支 队伍 参赛 海外 参赛 队伍 超过 148支 阿里 
巴巴 集团 为此 开 放了 5.7亿 条 经过 严格 脱敏 
处理 的 数据 2014年 赛季 的 数据 提供 方为 贵阳市政府 
参赛者 根据 交通 数据 模拟 控制 红绿灯 时间 寻找 减轻 
道路 拥堵 的 方法 Kaggle 是 一个 数据 分析 的 
竞赛 平台 网址 https / / www . kaggle . 
com / 企业 或者 研究 者 可以 将 数据 问题 
描述 期望 的 指标 发布 到 Kaggle 上 以 竞赛 
的 形式 向 广大 的 数据 科学家 征集 解决 方 
案 类似于 KDD CUP 国际知识 发现 和 数据挖掘 竞赛 Kaggle 
上 的 参赛者 将 数据 下载 下来 分析 数据 然后 
运用 机 器 学习 数据挖掘 等 知识 建立 算法 模型 
解决问题 得出 结果 最后 将 结果 提交 如果 提交 的 
结果 符合 指标 要求 并且在 参赛者 中 排名 第一 将 
获得 比赛 丰厚 的 奖金 项目 二十 量化 交易 量化 
交易 Quantitative Trading 是 指 借助 现代 统计学 和 数学 
的 方法 利用 计算机 技术 来 进行 交易 的 证券 
投资 方式 量化/v 交易/n 从/p 庞大/a 的/uj 历史/n 数据/n 中/f 
海选/n 能/v 带来/v 超额/b 收益/n 的/uj 多种/m 大 概率 事件 
以 制定 策略 用 数量 模型 验证 及 固化 这些 
规律 和 策略 然后 严格 执行 已 固化 的 策略 
来 指导 投资 以求 获得 可以 持续 的 稳定 且 
高于 平均 收益 的 超额 回报 量化 交易 起源 于 
上世纪 七十 年代 的 股票 市场 之后 迅速 发展 和 
普及 尤其 是 在 期货 交易 市场 程序化 逐渐 成为 
主流 有 数据 显示 国外 成熟 市场 期货 程序化 交易 
已 占据 总 交易量 的 70% 80% 而 国内 则 
刚刚 起步 手工 交易 中 交易者 的 情绪 波动 等 
弊端 越来越 成为 盈利 的 障碍 而 程序化 交易 天然 
而成 的 精准性 100% 执行率 则为 它 的 盈利 带来 
了 优势 阶段 九 百度 云 实战 体系 课程 一 
深入 理解 百度 云计算 基础 产品 / 基于 百度 云 
弹性 计算 服务 实现 基础架构 解决方案 全面 介绍 BCC CDS 
EIP BLB RDS BOS VPC 等 百度 云 弹性 计算 
服务 介绍 百度 云的/nr 安全 防护 方案 深入 介绍 传统 
架构 下 如何 通过 百度 云 弹性 计算 服务 快速 
构建 更 稳定 安全 的 应用 认证 培训 专家 将 
通过 深入浅出 理论/n 和/c 实践/v 相/v 结合/v 的/uj 课程/n 帮助/v 
学员/n 深入/v 掌握/v 百度/n 云/ns 弹性/n 计算/v 服务/vn 1 快速 
体验 百度 云 服务器 BCC 的 功能 全貌 2 基于 
BCC 的 云 磁盘 CDS 的 操作 与 管理 3 
基于 BCC 的 磁盘 快照 自定义 镜像 的 操作 与 
管理 4 基于 自定义 镜像 快速 生成 BCC 的 实验 
5 基于 磁盘 快照 实现 数据备份 与 恢复 的 最佳 
实践 6 基于 百度 云安全 组 完成 定义 IP ＋ 
端口 的 入站 和 出站 访问 策略 7 快速 体验 
百度 云 私有 网络 VPC 的 功能 全貌 8 基于 
百度 云 VPC + VPN 快速 搭建 Stie to Stie 
的 混合 云 架构 9 在 百度 云 VPC 网络 
下 实现 NAT 地址映射 的 实践 10 快速 体验 百度 
云数据库 RDS 的 功能 全貌 11 云数据库 RDS 的 备份 
与 恢复 操作 体验 12 熟悉 数据 传输服务 DTS 的 
使用 13 快速 体验 百度 云 负载 均衡 BLB 的 
功能 全貌 14 快速 体验 百度 云 存储 BOS 的 
功能 全貌 15 快速 体验 百度 云数据库 RDS 的 功能 
全貌 16 快速 体验 百度 云 内容 分发 网络 CDN17 
基于 BLB BCC RDS BOS 和 CDN 快速 部署 Discuz 
论坛 实现 弹性 架构 综合 实验 18 快速 体验 百度 
云安全 BSS 和 DDOS 防护 服务 19 快速 体验 百度 
云 监控 BCM 课程 二 基于/p 百度/n 云的/nr 迁移/v 上/f 
云/ns 实战/v 基于/p 百度/n 云/ns 弹性/n 计算/v 服务/vn 的/uj 基础/n 
产品/n 实现 传统 IT 架构 迁移 到 百度 云上的/nr 实战 
为 客户 业务 上 云 提升 能力 提升 客户 上 
云前的/nr 信心 上/f 云/ns 中和/ns 上/f 云后的/nr 技术/n 能力/n 以 
真实 的 客户 案例 结合 设计 好 的 动手 实验课 
提升 实战经验 介绍 了 业务 上 云的/nr 过程 方法 工具 
以及 案例 等 1 基于 BCC 快速 部署 LNMP 基础 
环境 2 基于 BCC 快速 部署 LAMP 基础 环境 3 
基于 BCC 快速 部署 MySQL 数据库 4 基于 BCC 快速 
部署 MS SQL 数据库 服务 5 基于 BCC 快速 部署 
Tomcat 基础 环境 6 云数据库 RDS 结合 数据 传输服务 DTS 
实现 数据 迁 移上 云的/nr 最佳 实践 7 基于 BOS 
桌面 实现 BOS 的 可视化 管理 8 基于 BOS FS 
实现 BOS 服务 挂载 到 本地 文件系统 9 基于 BOS 
Util 实现 BOS 的 批量 文件 操作 的 演示 10 
基于 BOS CLI 实现 BOS 文件 的 单机 操作 课程 
三 在/p 百度/n 云/ns 平台/n 上/f 进行/v 开发/v 全面/n 介绍/v 
使用/v 百度/n 云/ns 产品/n 进行/v 应用/v 开发/v 理解 百度 云 
主要 产品 特性 包括 BCC BOS RDS SCS 在 应用 
开发 中 的 使用 结合/v 实际/n 应用/v 开发/v 案例/n 全面/n 
的/uj 介绍/v 整个/b 开发/v 流程/n 和/c 百度/n 云/ns 产品/n 使用/v 
方法/n 以/p 提升/v 学员/n 开发/v 技能/n 和/c 了解/v 百度/n 云/ns 
产品开发/l 特点/n 根据 一天 或者 两天 的 课程 提供 多个 
实际 动手 实验 认证 讲师 指导 实验 真正 做到 学以致用 
为 学员 实现 上 云 开发 保驾 护航 1 基于 
百度 云 OpenAPI 实现 简化版 控制台 的 综合 实验 2 
基于 百度 云 BOS OpenAPI 实现 简化版 的 百度 网盘 
课程 四 百度 云 天工 智能 物联网 与 天 像 
智能 多媒体 服务平台 介绍 与 案例 分析 百度 天工 物 
联 平台 是 一站式 全 托管 的 物联网 服务平台 依托 
百度 云 基础 产品 与 服务 提供 全栈/nr 物联网 核心 
服务 帮助 开发 者 快速 搭建 部署 物联网 应用 通过 
全面 介绍 天工 的 IoT Hub IoT Parser Rule Engine 
IoT Device BML BMR OCR 和 语音 识别 等 产品 
与 服务 解析 天工 典型 的 产品 架构 方案 应用 
到 工业 4.0 车联网 能源 物流 和 智能 硬件 等 
各行业 解决方案 1 基于 百度 云 LSS 快速 搭建 音 
视频 直播 平台 最佳 实践 2 基于 百度 云 VOD 
快速 搭建 音 视频点播 平台 最佳 实践 3 体验 百度 
云 音视频 转码 MCT 的 转码 计算 服务 4 基于 
百度 云 文档 服务 DOC 体验 文档 存储 转码 分发 
播放 一站式 服务 体验 5 基于 百度 云物/nr 接入 IoT 
Hub 实现 智能 设备 与 百度 云端 之间 建立 安全 
的 双向 连接 6 体验 百度 云的物/nr 管理 IoT Device 
端 到 端 配置 实践 课程 五 百度 云 天智 
人工智能 服务平台 介绍 与 实战 天智 是 基于 世界 领先 
的 百度 大脑 打造 的 人工智能 平台 提供 了 语音 
技术 文字 识别 人脸识别 深度 学习 和 自然 语言 NLP 
等 一系列 人工智能 产品 及 解决方案 帮助 各行各业 的 客户 
打造 智能化 业务 系统 本 课程 力求 对 百度 人工智能 
服务 平台 进行 整体 全面 的 介绍 包括 天智 平台 
与 解决 方案 介绍 主要 产品 百度 语音 人脸识别 文字 
识别 百度 深度 学习 百度 机器学习 BML 自然语言 NLP 等 
的 介绍 客户 案例 分享 等 1 百度 机器学习 BML 
广告 点击率 预估 2 百度 识别 文字 识别 3 百度 
识别 人脸识别 4 百度 自然语言 处理 短 文本 相似 度 
5 百度 语音 朗 读者 6 百度 深度 学习 预测 
用户 感兴趣 的 电影 阶段 十 人工智能 实战 － 企业 
项目 实战 课程 一 基于 Python 数据 分析 与 机器学习 
案例 实战 教程 课程 风格 通俗易懂 基于 真实 数据集 案例 
实战 主体 课程 分成 三 个大 模块 1 python 数据分析 
2 机器学习 经典 算法 原理 详解 3 十大 经典 案例 
实战 通过 python 数据 科学 库 numpy pandas matplot 结合 
机器学习 库 scikit learn 完成 一些 列 的 机器学习 案例 
算法 课程 注重 于 原理 推导 与 流程 解释 结合 
实例 通俗 讲解 复杂 的 机器学习 算法 并以 实战 为主 
所有 课时 都 结合 代码 演示 算法 与 项目 相 
结合 选择 经典 kaggle 项目 从 数据 预处理 开始 一步步 
代码 实战 带 大家 快速 入门 机器学习 旨在 帮助 同学 
们 快速 上手 如何 使用 python 库 来 完整 机器学习 
案例 选择 经典案例 基于 真实 数据集 从 数据 预处理 开始 
到 建立 机器学习 模型 以及 效果 评估 完整 的 讲解 
如何 使用 python 及其 常用 库 进行 数据 的 分析 
和 模型 的 建立 对于 每 一个 面对 的 挑战 
分析 解决 问题 思路 以及 如何 构造 合适 的 模型 
并且 给出 合适 评估 方法 在 每 一个 案例 中 
同学 们 可以 快速 掌握 如何 使用 pandas 进行 数据 
的 预处理 和 分析 使用 matplotlib 进行 可视化 的 展示 
以及 基于 scikit learn 库 的 机器学习 模型 的 建立 
1 Python 数据 分析 与 机器学习 实战 课程 简介 2 
Python 快速 入门 3 Python 科学计算 库 Numpy4 Python 数据 
分析 处理 库 Pandas5 Python 可视化 库 Matplotlib6 回归 算法 
7 模型 评估 8 K 近邻 算法 9 决策树 与 
随机 森林 算法 10 支持 向量 机 11 贝叶斯 算法 
12 神经网络 13 Adaboost 算法 14 SVD 与 推荐 15 
聚 类 算法 16 案例 实战 使用 Python 库 分析处理 
Kobe Bryan 职业生涯 数据 17 案例 实战 信用卡 欺诈 行为 
检测 18 案例 实战 泰坦尼克号 获救 预测 19 案例 实战 
鸢尾花 数据集 分析 20 案例 实战 级联 结构 的 机器学习 
模型 21 案例 实战 员工 离职 预测 22 案例 实战 
使用 神经 网络 进行 手写 字体 识别 23 案例 实战 
主 成分 分析 24 案例 实战 基于 NLP 的 股价 
预测 25 案例 实战 借贷 公司 数据 分析 课程 二 
人工智能 与 深度 学习 实战 课程 风格 通俗易懂 必备 原理 
形象 解读 项目 实战 缺一不可 主体 课程 分成 四 个大 
模块 1 神经网络 必备 基础 知识 点 2 深度 学习 
模型 3 深度 学习 框架 Caffe 与 Tensorflow 4 深度 
学习 项目 实战 课程 首先 概述 讲解 深度 学习 应用 
与 挑战 由 计算机 视觉 中 图像 分类 任务 开始 
讲解 深度 学习 的 常规 套路 对于 复杂 的 神经 
网络 将其 展 开成 多个 小 模块 进行 逐一 攻破 
再 挑战 整体 神经 网络 架构 对于 深度 学习 模型 
形象 解读 卷积 神经网络 原理 详解 其中 涉及 的 每一个 
参数 对 卷积 网络 架构 展开 分析 与 评估 对于 
现阶段 火爆 的 对抗 生成 网络 以及 强化 学习 给出 
形象 解读 并 配合 项目 实战 实际 演示 效果 基于 
框架 实战 选择 两款 深度 学习 最 火 框架 Caffe 
与 Tensorflow 首先 讲解 其 基本 使用 方法 并 结合 
案例 演示 如何 应用 框架 构造 神经网络 模型 并 完成 
案例 任务 选择 经典 深度 学习 项目 实战 使用 深度 
学习 框架 从零开始 完成 人脸 检测 验证码 识别 人脸 关键点 
定位 垃圾邮件 分类 图像 风格 转换 AI 自己 玩 游戏 
等 对于 每 一个 项目 实战 从 数据 预处理 开始 
一步步 构建 网络 模型 并 展开 分析 与 评估 课程 
提供 所 涉及 的 所有 数据 代码 以及 PPT 方便 
大家 快速 动手 进行 项目 实践 1 深度 学习 概述 
与 挑战 2 图像 分类 基本原理 门 3 深度 学习 
必备 基础 知识 点 4 神经网络 反向 传播 原理 5 
神经网络 整体 架构 6 神经网络 案例 实战 图像 分类 任务 
7 卷积 神经 网络 基本 原理 8 卷积 参数 详解 
9 卷积 神经网络 案例 实战 10 经典 网络 架构 分析 
11 分类 与 回归 任务 12 三代 物 体检 测算 
法分析 13 数据 增强 策略 14 T r a n 
s f e r L e a r n i 
n g 1 5 网络 架构 设计 16 深度 学习 
框架 Caffe 网络结构 配置 17 Caffe18 深度 学习 项目 实战 
人脸 检测 19 人脸 正负 样本 数据源 制作 20 人脸 
检测 网络 架构 配置 习 模型 21 人脸 检测 代码 
实战 22 人脸 关键点 定位 项目 实战 23 人脸 关键点 
定位 网络 模型 24 人脸 关键点 定位 构建 级 联网络 
25 人脸 关键点 定位 测试 效果 与 分析 26 Tensorflow 
框架 实战 27 Tensorflow 构建 回归模型 28 Tensorflow 构建 神经网络 
模型 29 Tensorflow 深度 学习 模型 30 Tensorflow 打造 RNN 
网络 模型 31 Tensorflow 项目 实战 验证 识别 32 项目 
实战 图像 风格 转换 33 QLearning 算法 原理 34 DQN 
网络 架构 35 项目 实战 DQN 网络 让 AI 自己 
玩 游戏 36 项目 实战 对抗 生成 网络 等 项目 
一 AI 大 数据 互联网 电影 智能 推荐 第一季 随着 
科技 的 发展 现在 视频 的 来源 和 类型 多样性 
互联网 视频 内容 充斥 着 整个 网络 如果 仅仅 是 
通过 翻页 的 方法 来 寻找 自己 想看 的 视频 
必然 会 感到 疲劳 现在 急需 一种 能 智能 推荐 
的 工具 推荐 系统 通过 分析 用户 对 视频 的 
评分 分析 对 用户 的 兴趣 进行 建模 从而 预测 
用户 的 兴趣 并 给 用户 进行 推荐 Python 是 
一种 面向 对象 的 解释 型 计算机程序 设计 语言 Python 
具有 丰富 和 强大 的 库 它 常被 昵称 为 
胶水 语言 而 大 数据 是 指 无法 在 一定 
时间 范围 内 用 常规 软件工具 进行 捕捉 管理 和 
处理 的 数据 集合 企业 面临 海量 数据 的 到来 
大多 选择 把 数据 从 本地 迁移 至 云端 云端 
将 成为 最大 的 非 结构化 数据 存储 场所 本 
项目 主要 以 客户 咨询 为 载体 分析 客户 的 
群体 分布 旨在 挖掘 客户 的 内在 需求 帮助 企业 
实现 更 有价值 的 营销 一 教务 管理 系统 业务 
介绍 1 教务 管理 系统 框架 讲解 2 系统 业务 
逻辑 介绍 二 大 数据 需求分析 1 明确 数据 需求 
2 大 数据分析 过程 3 分析 难点 和 解决 方案 
4 大 数据 相关 技术 选型 三 构建 分布式 大 
数据 框架 1 Hadoop 分布式 集群 配置 2 ZooKeeper 高 
可用 3 SQOOP 数据 转移 4 ETL 数据 清洗 5 
HIVE 数据分析 6 HBase 数据 存储 四 基于 教务 管理 
系统 大 数据分析 1 业务 数据 分析 指标 设定 2 
操作 MapReduce 分而治之 3 使用 Hive 进行 数据 整合 抽离 
4 使用 HBase 存储 非 结构 话 数据 五 大 
数据 可视化 1 可视化 技术 选型 2 Echarts 代码 展示 
炫 酷 视图 3 使用 Tableau 进行 数据 可视化 展示 
项目 二 电商 大 数据 情感 分析 与 AI 推断 
实战 项目 第一季 本 项目 从 开发 的 角度 以大 
数据 PHP 技术 栈 为基础 使用 真实 商用 表 结构 
和 脱敏 数据 分三步 构建 商用 系统 真实 大 数据 
环境 进行 推断 分析 以及 呈现 结果 项目 课程 的 
完整性 商业性 可以 使 学者 尽可能 完整 地 体会 真实 
的 商业 需求 和 业务 逻辑 完整 的 项目 过程 
使 PHP 技术 栈 的 同学 得以 窥见 和 学到 
一个 完整 商业 平台 项目 的 搭建 方法 真实 大 
数据 环境 的 搭建 使 呈现 建立 大 数据 的 
工具 应用 技术 概念 储备 基于 大 数据 平台 的 
分析 需求 的 实现 呈现 将 完整 的 一次 大 
数据 技术 栈 到 分析 结果 的 中线 平铺 直述 
为 想要 学习 大 数据 并 有 开发 基础 的 
同学 点亮 新 的 能力 一 实践 项目 研发 1 
开发 环境 的 安装 配置 2 表 与 数据 3 
LARAVEL 的 快速 开发 实践 4 批量 创建 模型 5 
万能 控制器 与 表 配置 6 统一 视图 的 创建 
二 数据分析 需求 设立 1 定义数据 需求 2 分析 计算 
过程 3 分析 难点 和 解决 方案 4 大 数据 
技术 选型 三 大 数据 平台 搭建 1 分布式 环境 
的 模拟 建立 2 网络 环境 的 调 通3/nr 身份验证 
与 集群 控制 4 Hadoop 环境 搭建 和 要点 说明 
5 MapReduce 与 Yarn 的 搭建 和 说明 四 大 
数据分析 脚本 编写 1 MapReduce 脚本 编写 2 拆解 数据 
需求 3 Map 逻辑 详写 4 Reduce 逻辑 详写 5 
结果 整理 与 输出 五 结果 可视化 1 可视化 需求 
和 技术 选型 2 展示 页面 的 快速 铺设 3 
可视化 JS 上手 4 使用 可视化 JS 展示 结果 项目 
三 AI 法律咨询 大 数据 分析 与 服务 智能 推荐 
实战 项目 第一季 本 项目 结合 目前 流行 的 大 
数据 框架 在 原有 成熟 业务 的 前提 下 进行 
大 数据 分析 处理 真实 还原 企业应用 让 学员 身临其境 
的 感受 企业 大 数据 开发 的 整个 流程 项目 
的 业务 系统 底层 主要 采用 JAVA 架构 大 数据 
分析 主要 采用 Hadoop 框架 其中 包括 Kettle 实现 ETL 
SQOOP Hive Kibana HBASE Spark 以及 人工 智能算法 等 框架 
技术 采用 真实 大 数据 集群 环境 的 搭建 让 
学员 切身感受 企业 项目 的 从0到/nr 1 的 过程 一 
系统 业务 介绍 1 底层 业务 实现 框架 讲解 2 
功能模块 讲解 二 系统 架构设计 1 总体 架构 分析 2 
数据 流向 3 各 技术 选型 承载 作用 4 部署 
方案 三 详尽 实现 1 原始 数据处理 2 ETL 数据 
导入 3 MR 数据 计算 4 Hive 数据分析 四 数据 
可视化 1 采用 Highcharts 插件 展示 客户 偏好 曲线图 2 
使用 Tableau 进行 数据 分析 可视化 展示 五 项目 优化 
1 ZooKeeper 实现 HA2 集群 监控 的 整体 联调 项目 
四 AI 大 数据 基站定位 智能 推荐 商圈分析 项目 实战 
第一季 随着 当今 个人 手机 终端 的 普及 出行/v 人群/n 
中/f 手机/n 拥有率/n 和/c 使用率/n 已/d 达到/v 相当/d 高的/nr 比例/n 
根据 手机 信号 在 真实 地理 空间 的 覆盖 情况 
将 手机 用户 时间 序列 的 手机 定位 数据 映射 
至 现实 地理位置 空间 位置 即可 完整 客观 地 还原 
出 手机 用户 的 现实 活动 轨迹 从而 挖掘出 人口 
空间 分布 与 活动 联系 特征 信息 商圈 是 现代 
市场 中 企业 市场 活动 的 空间 同时 也 是 
商品 和 服务 享用者 的 区域 商圈 划分 为 目的 
之一 是 研究 潜在 顾客 分布 以 制定 适宜 的 
商业 对策 本 项目 以 实战 为 基础 结合 大 
数据 技术 Hadoop . Net 技术 全栈/nr 为基础 采用 真实 
商业 数据 分 不同 环节 构建 商用 系统 真实 大 
数据 环境 进行 推断 分析 及 呈现 数据 一 分析 
系统 业务 逻辑 讲解 1 大 数据 基站定位 智能 推荐 
商圈分析 系统 介绍 2 数据 前期 清洗 和 数据 分析 
目标 指标 的 设定 等 二 大 数据 导入 与 
存储 1 关系型 数据库 基础知识 2 hive 的 基本 语法 
3 hive 的 架构 及 设计 原理 4 hive 安装 
部署 与 案例 等 5 Sqoop 安装 及 使用 6 
Sqoop 与 关系型 数据库 进行 交互 等 7 动手 实践 
三 Hbase 理论 及 实战 1 Hbase 简介 安装 及 
配置 2 Hbase 的 数据 存储 与 数据模型 3 Hbase 
Shell4 Hbase 访问 接口 5 Hbase 数据备份 与 恢复 方法 
等 6 动手 实践 数据 转储 与 备份 四 基站 
数据 分析 与 统计 推断 1 背景 与 分析 推断 
目标 2 分析 方法 与 过程 推断 3 动手 实践 
分析 既定 指标 数据 五 数据 分析 与 统计 推断 
结果 的 展示 大 数据 可视化 1 使用 Tableau 展示 
数据 分析 结果 2 使用 HighCharts ECharts 展示 数据 分析 
结果 阶段 十一 区块 链 区块 链 Blockchain 是 分布式 
数据 存储 点对点 传输 共识 机制 加密算法 等 计算机 技术 
的 新型 应用 模式 所谓 共识 机制 是 区块 链 
系统 中 实现 不同 节点 之间 建立 信任 获取 权益 
的 数学 算法 区块 链 是 比特币 的 底层 技术 
像 一个 数据库 账本 记载 所有 的 交易 记录 这项 
技术 也 因其 安全 便捷 的 特性 逐渐 得到 了 
银行 与 金融业 的 关注 一 课程 介绍 1 区块 
链 的 发展 2 课程 安排 3 学习 目标 二 
区块 链 的 技术 架构 1 数据 层 创世 区块 
交易 记录 私钥 公钥 和 钱包 地址 2 数据 层 
& 通讯 层 记账 原理 Merkle 树 和 简单 支付 
验证 SPV P2P 通讯 数据 通信 和 验证 3 共识 
层 4 激励 层 拜占庭 将军 问题 与 POW Pos 
DPos PBFT 挖矿 交易 费 图灵 完备 和非/nr 完备 5 
合约 层 比特币 脚本 以太 坊 智能 合约 fabic 智能 
合约 RPC 远程 调用 6 应用层 7 总结 接口 调用 
DAPP 的 使用 应用 场景 的 部署 重要 概念 和 
原理 三 环境 搭建 1 以太 坊 以太 坊 介绍 
以太 坊 开发过程 图形界面 客户端 使用 供应链 的 应用 保险 
领域 的 应用 DAO 的 介绍 和 应用 2 以太 
坊 以太 坊本 地 开发 环境 的 搭建 以太 坊 
分布式 集群 环境 的 搭建 3 hyperledger 项目 fabric 介 
fabric 介绍 fabric 本地 开发环境 搭建 fabric 分布式 集群 环境 
搭建 四 案例 和 DEMO1 案例 讲解 支付 和清/nr 结算 
公益 行业 的 应用 供应链 的 应用 保险 领域 的 
应用 DAO 的 介绍 和 应用 2 Demo 介绍 发币 
和 交易 Demo3 Demo 介绍 数据 资产 的 确权 和 
追溯 阶段 十二 用 人工智能 预测 金融 量化 交易 投资 
系列 课程 程序化 交易 又称 程式 交易 发源 于 上世纪 
80 年代 的 美国 其 最初 的 定义 是 指在 
纽约股票交易所 NYSE 市场 上 同时 买 卖超过 15只 以上 的 
股票 组合 像 高盛 摩根士丹利 及 德意志银行 都是 在各 大 
交易 市场 程序化 交易 的 最 活跃 参与 会员 本 
课程 主要 面向 意愿 从事 金融 量化 交易 人员 金融 
行业 从业 人员 金融 策略 开发 人员 及 投资 经验 
丰富 而 想 实现 计算机 自动 下单 人员 主要 讲解 
了 证券 期货 程序化 实现 原理 及 过程 通过 本 
课程 的 学习 您 可以 根据 自己 的 意愿 打造 
属于 自己 的 量化 投资 交易 系统 本 课程 主要 
用到 的 技术 手段 有 Python Pandas 数据分析 数据挖掘 机器学习 
等 一 程序化 交易 数据 获取 与 清洗 讲解 1 
数据 的 清洗 与 合成 2 K 线图 绘制 3 
技术指标 开发 讲解 4 数据 的 获取 二 回 测 
框架 搭建 讲解 1 回 测 框架 搭建 背景 及 
基本 流程 讲解 2 回 测 框架 实现 及 收益 
指标 讲解 三 程序化 交易 部分 实现 讲解 1 CTP 
技术 讲解 2 程序化 API 讲解 3 程序化 交易 具体 
实现 讲解 阶段 十三 阿里云 认证 课程 一 云计算 网站 
建设 部署 与 发布 阿里云 网站 建设 认证 课程 教 
你 如何 掌握 将 一个 本地 已经 设 计好 的 
静态 网站 发布 到 Internet 公共 互联网 绑定 域名 完成 
工信部 的 ICP 备案 课程 二 云计算 网站 建设 简单 
动态 网站 搭建 阿里云 简单 动态 网站 搭建 课程 教 
你 掌握 如何 快速 搭建 一个 WordPress 动态 网站 并会 
对 网站 进行 个性化 定制 以 满足 不同 的 场景 
需求 课程 三 云计算 云/ns 服务器/n 管理/vn 维护/v 阿里云/i 服务器/n 
运维/i 管理/vn 课程/n 教/v 你/r 掌握/v 快速/d 开通/v 一台/m 云/ns 
服务器/n 并 通过 管理 控制台 方便 地 进行 服务器 的 
管理 服务器 配置 的 变更 和 升级 数据 的 备份 
并 保证 其 可以 正常 运转 并按 业务 需求 随时 
进行 配置 的 变更 课程 四 云计算 云数据库 管理 与 
数据 迁移 阿里云 云数据库 管理 与 数据 迁移 认证 课程 
掌握 云数据库 的 概念 如何 在云端 创建 数据库 将 自建 
数据库 迁移 至 云数据库 MySQL 版 数据 导入 导出 以及 
云数据库 运维 的 常用 操作 课程 五 云计算 云 存储 
对象 存储管理 与 安全 阿里 云云 储存 认证 课程 教 
你 掌握 安全 高/a 可靠/v 的/uj 云/ns 存储/l 的/uj 使用/v 
以及 在云端 存储 下载 文件 处理 图片 以及 如何 保护 
数据 的 安全 课程 六 云计算 超大 流量 网站 的 
负载 均衡 掌握 如何 为 网站 实现 负载 均衡 以/p 
轻松/a 应对/v 超大/v 流量/n 和高/nr 负载/v 课程 七 大 数据 
MOOC 网站 日志 分析 本 课程 可以 帮助 学员 掌握 
如何 收集 用户 访问 日志 如何 对 访问 日志 进行 
分析 如何 利用 大 数据 计算 服务 对 数据 进行 
处理 如何 以 图表 化 的 形式 展示 分析 后的/nr 
数据 课程 八 大 数据 搭建 企业级 数据分析 平台 模拟 
电商 场景 搭建 企业级 的 数据分析 平台 用来 分析 商品 
数据 销售 数据 以及 用户 行为 等 课程 九 大 
数据 基于 LBS 的 热点 店铺 搜索 本 课程 可以 
帮助 学员 掌握 如何 在 分布式计算 框架 下 开发 一个 
类似于 手机地图 查找 周边 热点 POI 的 功能 掌握 GeoHash 
编码 原理 以及 在 地理位置 中的 应用 并能 将其 应用 
在 其他 基于 LBS 的 定位 场景 中 课程 中 
完整 的 演示 了 整个 开发 步骤 学员 在学 完 
此 课程 之后 掌握 其 原理 可以 在 各种 分布式计算 
框架 下 完成 此 功能 的 开发 比如 MapReduce Spark 
课程 十 大 数据 基于 机器学习 PAI 实现 精细化 营销 
本 课程 通过 一个 简单 案例 了解 掌握 企业 营销 
中 常见 的 也是 必需 的 精准 营销 数据 处理 
过程 了解 机器学习 PAI 的 具体 应用 指导 学员 掌握 
大 数据 时代 营销 的 利器 通过 机器学习 实现 营销 
课程 十一 大 数据 基于 机器 学习 的 客户 流失 
预警 分析 本 课程 讲解 了 客户 流失 的 分析 
方法 流程 同时 详细 介绍 了 机器 学习 中 常用 
的 分类 算法 集成 学习 模型 等 通用 技能 并 
使用 阿里云 机器学习 PAI 实现 流失 预警 分析 可以 帮助 
企业 快速 准确 识别 流失 客户 辅助 制定 策略 进行 
客户 关怀 达到 挽留 客户 的 目的 课程 十二 大 
数据 使用 DataV 制作 实时 销售 数据 可视化 大 屏 
帮助 非 专业 工程师 通过 图形化 的 界面 轻松 搭建 
专业 水准 的 实时 可视化 数据 大 屏 以 满足 
业务 展示 业务 监控 风险 预警 等 多种 业务 的 
展示 需求 课程 十三 大 数据 使用 MaxCompute 进行 数据 
质量 核查 通过 本 案例 学员 可 了解 影响 数据 
质量 的 因素 出现 数据 质量 问题 的 类型 掌握 
通过 MaxCompute DateIDE 设计 数据 质量 监控 的 方法 最终 
独立 解决 常见 的 数据 质量 监控 需求 课程 十四 
大 数据 使用 Quick BI 制作 图形化 报表 阿里云 Quick 
BI 制作 图形化 报表 认证 课程 教 你 掌握 将 
电商 运营 过程 中 的 数据 进行 图表 化 展现 
掌握 通过 Quick BI 将 数据 制作 成 各种 图形化 
报表 的 方法 同时 还 将 掌握 搭建 企业级 报表 
门户 的 方法 课程 十五 大 数据 使用时间 序列 分解 
模型 预测 商品 销量 使用时间 序列 分解 模型 预测 商品 
销量 教 你 掌握 商品 销量 预测 方法 时间 序列 
分解 以及 熟悉 相关 产品 的 操作 演示 和 项目 
介绍 课程 十六 云安全 云/ns 平台/n 使用/v 安全/an 阿里云/i 云/ns 
平台/n 使用/v 安全/an 认证/v 课程/n 教/v 你/r 了解/v 由/p 传统/n 
IT/w 到/v 云计算/i 架构/n 的/uj 变迁/vn 过程/n 当前 信息 安全 
的 现状 和 形势 以及/c 在/p 云计算/i 时代不同/i 系统/n 架构/n 
中/f 应该/v 从/p 哪些/r 方面/n 利用/n 云/ns 平台/n 的/uj 优势/n 
使用/v 安全/an 风险/n 快速/d 降低/v 90%/mf 课程 十七 云安全 云上/nr 
服务 器 安全 阿里 云云 上 服务 器 安全 认证 
课程 教 你 了解 在 互联 网上 提供 计算 功能 
的 服务器 主要 面临 哪些 安全 风险 并 针对 这些 
风险 提供 了 切实可行 的 免费 的 防护 方案 课程 
十八 云安全 云上/nr 网络/n 安全/an 了解/v 网络/n 安全/an 的/uj 原理/n 
和/c 解决/v 办法/n 以及 应对 DDoS 攻击 的 方法 和 
防护 措施 确保 云上/nr 网络 的 安全 课程 十九 云安全 
云上/nr 数据安全/n 了解/v 云上/nr 数据/n 的/uj 安全/an 隐患/n 掌握 数据备份 
数据 加密 数据传输 安全 的 解决 方法 课程 二十 云安全 
云上/nr 应用 安全 了解 常见 的 应用 安全 风险 SQL 
注入 原理 及 防护 网站 防 篡改 的 解决 方案 
等 确保 云上/nr 应用 的 安全 课程 二十一 云安全 云上/nr 
安全/an 管理/vn 了解/v 云上的/nr 安全/an 监控/vn 方法/n 学会 使用 监控 
大 屏 来 监控 安全 风险 并能够 自定义 报警 规则 
确保 随时 掌握 云上/nr 应用 的 安全 情况 阶段 十四 
IT 高级 开发者 职场 生存 规则 － 职业 素养 本 
课程 主要 为 广大 毕业生 或者 工作 经验 较少 的 
学员 而 设立 主要 是 为了 在 职业 素养 方面 
给 大家 提供 辅导 为 更加 顺利 走向 职场 而 
提供 帮助 为什么 有些 同学 在 技能 方面 过关 却 
还是 给予 别人 一种 书生气 的 感觉 为什么 简历 已经 
通过 了 却 还是 没有 通过 HR 的 面试 为什么 
入 职后 与 同事 的 沟通 总是 存在 问题 为什么 
每天 的 时间 都 不够 用 无法 兼顾 生活 学习 
和 工作 为什么 学习 一段 时间 后 对 工作 对 
职场 没有 方向感 为什么 遇到 事情 别人 总是 能够 保持 
良好 心态 游刃有余 而 我 总是 问题 百出 COT 课程 
正是 引领 大家 一起 来 探索 其中 的 奥秘 和 
方法 让 大家 一起 在 学习 过程 中 不断 深思 
和 进步 让/v 大家/n 的/uj 职场/n 路/n 越走越/nr 顺畅/a 1 
团队 协作 2 心态 管理 3 目标 管理 4 时间管理 
文章 介绍 了 著名 的 通用 的 三种 word embedding 
模型 LSI PMI 和 skip gram 介绍 了 它们 间 
的 联系 与 区别 结论 是 skip gram 等价 于 
分解 一个 shifted PPMI 矩阵 它 和 SVD 间 没有 
本质 的 差异 原文 链接 自然语言 处理 三 之 word 
embedding 云脑/nr 科技 机器学习 训练营 第二期 对 自然 语言 处理 
及 词 向量 模型 进行 了 详细 介绍 量子位/i 作为/v 
合作/vn 媒体/n 为/p 大家/n 带来/v 本期/t 干货/n 分享/v ~/i 本期/t 
讲师/n 简介/v 樊/nr 向军/i 云脑/nr 科技/n 核心/n 算法/n 工程师/n 清华大学 
学士 日本 东京 大学 与 美国 华盛顿 州立大学 双硕士 第 
33届 亚洲 国际 物理 奥赛 双料 金牌得主 在 美国 硅谷 
高通 等 公司 有着 多年 超高 性能 计算 仿真 软件设计 
开发 经验 获得 高通 Qualstar Diamond 杰出 贡献奖 目前 作为 
云脑/nr 科技 算法 团队 的 主要 成员 进行 金融 通信 
能源 大 数据 领域 的 核心 人工 智能算法 研发 与 
系统 设计 工作 分享 内容 实录 自然语言 处理 Natural Language 
Processing 是 一个 非常 大 的 topic 在 本节 课程 
中 我们 仅 做 非常 概要 性 的 介绍 下面 
这张 图 可以 给 你 一个 感觉 NLP 技术 能够 
做 些 什么 NLP 应用 在 自然 语言 处理 中 
主要 分为 以下 几类 第一 是 Classifying Words 即 需要 
去 研究 一下 词 是 什么 意思 第二 是 Classifying 
Documents 即 整个 文章 有 一些 什么 操作 怎么 去 
分类 第三 个 比较 难 也 比较 热门 的 是 
Understand Documents 即 理解 文章 是 在 讲 什么 这些 
是 NLP 比较 热门 的 几个 方面 前半段 我们 讲 
介绍 比较 传统 的 NLP 方法 后面 会讲 NLP 和 
Deep Learning 的 结合 Classifying Words 也 就是 把 每个 
词 分类 词 分为 哪 几类 或者 是 能 不能 
把 它 group 起来 比如说 维基百科 上 很多 信息 放 
在 一起 或者 你 拿到 一 本 字典 百科全书 再 
或者 许多 文章 放在 一起 怎么 去 分类 这些 字 
NLP 产生 了 许多 分支 去 研究 各种各样 的 里面 
的 问题 比如 Stemming 找到 一个 词 的 词根 根据 
词根 把 相同 的 词 尽量 的 放在 一起 另外 
一个 是 Splitting Words 分词 根据 里面 的 字母 把 
词 分成 许多块 做 字母 级别 的 k grams 或者 
n grams 再做 分类 这 两种 方法 比较 偏重 拉丁文 
英文 语系 的 文章 对 词根 或者 字母 进行 分解 
但是 对 中文 不是 很 合适 Classifying Documents 分类 文本 
本身 词 我们 可以 找 词根 或者 分词 文本 分类 
又 提高 了 一个 难度 它 有 一些 应用 比如说 
我们 想 知道 读 一篇 文章 需要 多久 最 简单 
的 办法 是 规定 某 一个 人 每分钟 读 多少 
词 统计 一下 这 篇 文章 有 多少 词 做 
一下 除法 就 得到 了 时间 这 可能 是 最 
直截了当 的 方法 但是 精度 可能 很差 因为 每个人 读 
的 速度 不 一样 文章 本身 的 难度 也 不一样 
等 各种各样 的 原因 如果 应用 没有 特别 的 要求 
就 可以 这样 简单 的 用一下 但 如果 某些 应用 
或 研究 中 希望 得到 一个 高的/nr 精度 比如 你 
的 研究 是 有 阅读障碍 的 人 遇到 各种各样 的 
文本 会 怎么样 则 希望 会 得到 一个 精度 比 
较高 的 阅读 时间 的 估计 结果 NLP 本身 有 
许多 研究 也 产生 了 许多 好 的 方法 在 
这里 就不 细 讲了 有兴趣 可以 关注 一下 相关 研究 
文献 Identifying a Language 这也 是 比较 有趣 的 应用 
你 输入 给 计算机 一段 语言 绝 大 部分 要 
做 翻译 比如说 把 日语翻译 成 中文 或者 把 中文 
翻译 成 日语 你 需要 选 一下 对应 语言 的 
选项 才 可以 那 现在 怎么 让 计算机 自己 去 
发现 你 的 输入 属于 哪 一种 语言 这 也是 
NLP 的 一个 研究 方向 有 一种 方法 是 Words 
in a Vocabulary 先 建 一个 词典 看 你 输入 
的 属于 哪 一个 词典 里 当然 这 也是 低效率 
精度 也 不一定 高的/nr 办法 还有 一个 方法 是 Frequencies 
of Groups of Letters 比如说 中文 用 中文 的 语言 
建立 一个 语言 模型 然后 再把 新 输入 的 东西 
带入 你 的 模型 作比较 得出 是不是 属于 中文 的 
结论 具体 语言 模型 要 怎么 建立 后面 会 提到 
Understanding Documents 刚刚 我们 有了 词 有了 文本 它们 都是 
做 分类 但 文本 具体 是 在 讲 什么 呢 
这 又是 一个 一直 想 要 解决 但 没有 非常 
好 的 完全 被 解决 的 方向 很多 人 在 
这个 方向 上 做 各种各样 的 研究 现在 有 一些 
比较 好 的 可以 实际 应用 的 比如 document summary 
之前 英国 有个/nr 学生 写了 对 news 做 summary 的 
系统 后来 被谷歌 收 去了 所以 像 这种 应用 很多 
互联网 公司 已经 有了/nr 很好 的 处理 能力 了 但 
还是 希望 有 更好 的 算法 进来 关于 summary TextRank 
比较 有意思 大家 都 知道 谷歌 最早 的 算法 叫 
PageRank 根据 两个 document 之间 的 connection 去 links 去 
排序 它 语义 的 重要性 TextRank 比 它 更 拓展 
一点 不是 简单 的 有关系 或者 没有 关系 如果/c 两个/m 
文/n 本间/nr 部分/n 有/v 关系/n 怎么办/l 它/r 把/p 两个/m 文/n 
本间/nr 部分/n 有/v 关系/n 部分/n copy/w 进来/v 做 一个 语言 
模型 第二个 是 Sentiment analysis 语义分析 简单 的 看 一下 
这个 文本 是 positive 还是 negative 现在 也很 常用 比如说 
对 亚马逊 或者 京东 商品 review 分析 一下 是 好 
的 还是 坏 的 好 在 哪里 坏 在哪里 或者 
是 苹果 app 安卓 app 或者 是 谷歌 的 app 
的 评论 有多/nr 好多 坏 好 在 哪里 差 在 
哪里 这些 都 可以 通过 语义 分析出来 帮助 客户 做 
更好 的 选择 接下来 是 Parsing 也是 很 常用 的 
一个 文本 是 有 结构 的 有 章节 句子 有 
主谓宾 有的 段落 的 首句 或者 尾 句 是 总结 
那 这种 结构 怎么 去 认识 它 比如说 股票 公司 
的 报告 上市 公司 的 年报 财报 法律 文件 合同 
等 这些 报告 你 怎么 去 理解 它 这些 文本 
都 比较 结构 化了 从人的/nr 角度/n 都/d 很好/i 了解到/i 如 
第一章 讲 的 是 什么 合同 的 甲方乙方 是 谁 
合同 的 成交 价格 是 多少 这些 人 都 可以 
快速 的 找到 它 但对 机器 来说 是不是 有个/nr 比较 
好 的 办法 去 识别 呢 而 不是 规定 具体 
的 哪一 章节 要讲 什么 有些 公司 试图 把 规范化 
化 报告 的 描述 其实 这些 是 很难 完全 做到 
的 其实 有 更好 的 办法 用 NLP 的 方法 
找到 关键点 比如/v 大家/n 写/v code/w 都/d 需要/v 有/v 具体/a 
的/uj 格式/n 比如说 缩进 每 一行 后面 加 分号 等 
这些 都是/nr 规定 好 的 所以 计算机 很容易 compare 你 
的 language 把 它 变成 机器语言 但 真正 的 自然 
语言 没有 严格 的 语法 那 怎么 做 Parsing 也是 
需要 做 但也 比较 难 的 问题 最后 是 Translate 
语言 翻译 显而易见/l 大家/n 都/d 看到/v 了/ul 最近/f 几年/m 有/v 
很大/a 的/uj 进步/d 应用 也 是 越来越 广 NLP 应用 
十分 的 广 这里 主要 是 讲 一下 大概 如果 
大家 对 某个 方向 或 应用 感兴趣 可以 关注 阅读 
一些 相关 应用 的 研究 结果 比较 常用 的 NLP 
Libraries Apache OpenNLP The Classical Language Toolkit CLTK FreeLing Moses 
NLTK Pattern Polyglot Sentiment SpaCy CoreNLP ParserNLP 与 Deep Learning 
之前 讲 的 这些 NLP 都是 和 语言 处理 相关 
的 举 几个 例子 看看 如何 将 NLP 和 Deep 
Learning 相 结合 如何 通过 引入 deep learning 把 NLP 
做到 更好 的 效果 词 分类 根据 相似性 词 分类 
这里 例子 是 摘自 斯坦福 的 讲义 如果 大家 对 
NLP 感兴趣 建议 看一下 斯坦福 的 课程 里面 讲 了 
大量 应用 Deep Learning 的 例子 且 都是 更新 比 
较快 的 如果 输入 是 一个 青蛙 会 输出 一些 
和 青蛙 有关 的 词 比如 toad 等 如果 你 
给 他 一个 文本 它 自己 能够 把 它 学 
出来 语法 的 分析 词 本身 是 有 词性 或是 
语法 的 比如 拉丁文 英文 前缀 后缀 如 加 un 
变 反义 这都 是 根据 词根 来 定 Deep learning 
的 思路 是 把 每个 词义 变成 一个 vector 然后 
分成 两个 或 多个 vector 把 它们 combine 起来 然后 
去学 它 的 vector 就 可以 了 对于 句子 做 
一些 句法 的 分析 句子 中 有 一些 词 有 
主谓宾 这种 词 要 怎么 认出来 这是 做 语义 理解 
的 一个 基础 那 怎么 用 Deep Learning 来做 句法 
的 分析 语义 的 分析 传统 的 做法 需要 对 
语言 本身 做 许多 工程 需要 做 一些 知识 的 
积累 然后 root 放进来 然后 用 tree 记录下来 最后 来 
分析 新 进来 的 文本 Deep Learning 的 做法 是 
把 字 和词/nr 变成 一个 向量 然后 用 neural network 
去学 这个 vector Question Answering 也是 有 传统 的 做法 
首先 要 做 许多 的 feature engineering 把 knowledge 先 
build 起来 通过 build 起来 的 knowledge 再 做 一个 
分解 从 deep learning 角度 来看 你 Question Answering 需要 
的 一些 facts 需要 的 一些 结果 需要 的 一些 
answer 都会 存在 vector 里面 然后 再 通过 vector 再做 
问答 的 matching . Translation 谷歌 Facebook 把 之前 传统 
的 Translation 方法 都 推翻 了 使用 Machine Learning 提高 
的 很多 的 精度 这里 有 一个 Neural Machine Translation 
的 示意图 ML 的 做法 都是 把 它 变成 了 
一个 vector Vector 我们 前面 提到 的 应用 都用 vector 
来做 下面 我们 主要 讲 vector 是 什么 怎么 去 
用 vector 怎么 得到 vector Word Embedding 简单 的 讲 
vector 就是 一个 一维 的 数组 每 一个 词 都 
变成 一个 vector 比如说 先把 一个词 变到 一个 多维空间 中 
然后 把 所有 的 词 都 放在 这个 多维空间 中 
最大 的 好处 是 这些 词 对 计算机 来 说是 
categorical feature 像 one hot 一样 两个 词 放在 不同 
位置 完全 没有 关系 如果 用 vector 来 表示 词 
与 词 之间 的 关系 就 可以 用 距离 来 
表现 也 就是说 这些 词 对 计算机 来 说 本来 
是 没有 关系 的 但 通过 vector 转换 之后 它们 
的 距离 代表 了 它们 的 关系 这也 是 比较 
好 的 帮助 计算机 去 理解 词 之间 关系 的 
方法 Word Embedding 实际上/d 就是/d 把/p 词/n 从词/nr 本身/r 或/c 
从/p one/w hot 本身 变成 一个 vector 的 过程 Embedding 
就是 你 怎么 去 变换 这个 向量 How Do We 
Represent the Meaning of a Word Meaning 本身 在 字典 
的 定义 是 词 背后 的 想法 或是 某个人 文章 
艺术品 想要 表达 的 想法 Meaning 本身 是个 idea 它 
在 大脑 里面 怎么 存储 的 我们 不 知道 这个 
idea 怎么 让 计算机 系统 去 理解 它 比 较好 
的 办法 是 把 它 变成 一个 vector 词 本身 
如果 不做 向量 的 变化 那 计算机 看起来 是 什么 
如果 两个 词 不 一样 那 就是 一个 分类 的 
feature 那 我们 就 直接 做 one hot 就是 在 
出现 的 位置 记为 1 其他 位置 记为 0 这样 
做 显然 是 可以 的 但是 维度 是 十分 大 
的 尤其 是 英文 比如说 你 搜索 电视 大小 其实 
和 电视 容量 是 一个 意思 那/r 计算机/n 怎么/r 知道/v 
电视/n 大小/b 和/c 电视/n 容量/n 是/v 同一/b 个/q 意思/n 包括/v 
你/r 要/v 查/v hotel/w 和/c motel/w 其实 是 一个 意思 
如果 用 one hot 它们 将 在 两个 维 度上 
完全 没有 关系 如果把 它 变到 一个 比 one hot 
低维 的 但 每个 位置 上 都是 有 浮点数 的 
vector 而且 这些 浮点数 的 数值 是 有 意义 的 
比如说 两个 词 的 浮点 数值 大小 非常 靠近 那这/nr 
两个 词 就 比较 靠近 那 这样 学 出来 的 
vector 也 是 非常 有 意义 的 How to Represent 
Word as Vector 怎么 去学 vector 每 一个 词 都 
做一个 one hot encoding 变成 一个 很长 的 vector 通过 
Neural network 首先 input vector 然后 通过 Hidden Layer 加工 
再 marking 到 另一个 词 上面 再 进行 训练 如果 
这 两个 词 是 相近 的 就是 1 如果 不 
相近 就是 0 如果能 准备 到 这样 的 数据 让 
神经 网络 去学 所有 的 词 都 表征 为 向量 
之后 那这/nr 两个 向量 距离 之间 就 比较 接近 因为/c 
和/c train/w 数据/n 是/v 有/v 关系/n 的/uj 如果 进去 是 
一个 词 让 它 比较 和 另一个 词 的 距离 
这时 需要 有 一个 label 才 可以 去 train 那 
怎么样 得到 这个 label 呢 Skip Gram Model 下面 举 
一个 Skip Gram Model 的 例子 它 的 主要 思想 
是 如果 你 能够 拿到 一些 文本 可能 是 维基百科 
百度 百科 的 文章 很 自然 的 有 一些 词 
就 会 出现 在 另 一些 词 的 附近 那 
我们 在 做 Skip Gram 的 过程 实际 就是 在 
create 一个 train data 的 过程 我们 把 文本 拿来 
把 中间 词 作为 ｘ 两边 的 词 作为 label 
或是 topic words 这 两个 词 如果 同时 出现 在 
附近 可以 记为 １ 如果 没有 记为 ０ 这样 的 
就 可以 得到 一个 train sample 这样 的 train sample 
都是/nr 一个 pairs 这样 就 可以 把 文本 变成 很多 
个 train sample 再 返回 刚才 的 模型 能 很好 
的 把 Hidden Layer 学 出来 学到 Hidden Layer 之后 
这 就 一个 embedding 了 通过 word Paris 建立 语言 
模型 然后 每 一个 词 再回来 本身 还是 一个 one 
hot encoding 再 经过 Hidden Layer weight matrix 会 变成 
一些 的 word vector 回溯 总结 一下 vector 就是 把 
词 本身 变成 一个 向量 怎么 得到 这个 向量 刚才 
举 到了 用 神经网络 Skip Gram 建立 train 数据 然后 
学到 这个 数据 然后 Embedding 实际上 就是 Hidden Layer weight 
matrix 通过 Embedding 就 得到 了 向量 这 是 一个 
比较 直接 的 事实 证明 也 是 一个 比较 有效 
的 办法 Negative Sampling 刚刚 有 讲到 每个 词 用 
one hot encoding 然后 用 weight matrix 与 它 相乘 
假设 我们 想 要 得到 的 vector 的 size 是 
300 输入 字典 的 维度 是 10000 可以 看到 weight 
matrix 有 300 * 10000个 parameter 只有 一个 FC Layer 
就 非常 大了 所以 不仅 是 weight matrix 这么 大 
每一次 迭代 都 要把 matrix 更新 一遍 这样 整个 学习 
过程 的 效率 是 十分 低 的 比如说/l 在/p 10000个/mq 
词中/i 有/v 很多/m 和它是/nr 词义/n 相近/v 的/uj 但 绝大部分 和它是/nr 
没有 关系 的 数学 的 角度 是 正交 的 所以 
它 不 需要 每次 都 进行 更新 所以 Negative Sampling 
的 核心 大量 减少 更新 的 内容 而且 可以 大量 
的 减少 训练 损失 实际 测量 下来 的 结果 也 
是 非常 好 的 当然 还要 做 一个 词频 高频 
的 词 需要 放到 训练 的 过程 当中 去 低频 
的 就 不需要 做了 总结 一下 Word2Vec 是 怎样 一个 
流程 Collect text dataProcess textSkip Gram to generate word pairTraining 
e m b e d d i n g W 
o r d 2 V e c V e c 
t o r Space Visualization 把 它们 全都 变成 vector 
之后 下 一步 需要 做 什么 下 一步 最 简单 
的 做法 就是 把 它们 画 出来 当然 之前 例子 
中 说到 把 每一个 词 变到 300 维 300 维 
是 人 的 肉眼 是 看不 出来 的 大家 的 
物理 世界 只有 ３ 维 需要 一些 降 维 的 
方法 降 维 之后 可以 看到 本 来 一些 词 
是 没有 关系 的 最后 自动 的 group 到一起 比如说 
左上角 12345678910 的 英语 都到 一起 了 左下角/m 和/c 时间/n 
相关/v 的/uj 都在/nr 一起/m 了/ul 右下角 语文 数学 化学 高考 
科目 都到/nr 一起 了 这是 个 比较 有意思 的 事情 
明显 的 看到 这个 学习 的 过程 是 比较 有 
意义 的 意思 相近 的 词 都在/nr 一起 了 还有 
一个 点 word vector 不仅仅 是 把 词 进行 分类 
而且/c 词/n 和词/nr 之间/f 的/uj 距离/n 也是/i 有/v 很强/i 的/uj 
关系/n 比如 说英语 里面 有 基本 级 比较级 最高级 如果 
看成 一个 向量 一个 的 比较级 减去 最 高级 和 
另外 一个 比较级 减去 另外 一个 最 高级 下面 的 
向量 还是 一样 的 它 不仅 把 词 之间 的 
分类 学会 了 词 之间 的 关系 也 学会 了 
还有 就是 男人 对应 国王 女人 对应 什么 是 皇后 
这个 也 能学 出来 所以 Word2Vec 的 fundamental idea 不是 
很难 但 效果 也 是 非常 好 的 相关 学习 
资源 以上 就是 此次 课程 的 相关 内容 在 量子 
位 微信 公众 号 QbitAI 对话 界面 回复 171202 可获得 
完整版 PPT 上期 课程 回顾 干货 分享 | 详解 特征 
工程 与 推荐 系统 及 其 实践 P . . 
云脑/nr 科技 对 人才 如饥似渴 如 有兴趣 请 移步 http 
/ / cloudbrain . ltd / join . html 简历 
发送至 job @ cloudbrain . ai 据说 邮件 主题 添加 
注明 来源 量子位 通过率 会 更高 噢 ~ 完 活动 
报名 加入 社群 量子位 AI 社群 11 群 开始 招募 
啦 欢迎 对 AI 感兴趣 的 同学 加 小助手 微信 
qbitbot4 入 群 此外 量子位 专业 细 分群 自动驾驶 CV 
NLP 机器学习 等 正在 招募 面向 正在 从事 相关 领域 
的 工程师 及 研究 人员 进 群 请 加 小助手 
微 信号 qbitbot4 并 务必 备注 相应 群 的 关键词 
~ 通过 审核 后 我们 将 邀请 进 群 专业 
群 审核 较严 敬请 谅解 诚挚 招聘 量子位 正在 招募 
编辑 / 记者 工作 地点 在 北京 中关村 期待 有 
才气 有 热情 的 同学 加入 我们 相关 细节 请在 
量子位 公众 号 QbitAI 对话 界面 回复 招聘 两个字 量子位 
  QbitAI   头条 号 签约 作者 վ ᴗ ի 
追踪 AI 技术 和 产品 新动态 神经网络 NLP 神经 网络结构 
文本 特征 表示 前馈 神经网络 损失 函数 CNN 应用于 文本 
RNNRecursive NN 神经网络 NLP 对于 自然 语言 处理 技术 传统 
机器学习 算法 例如 SVM LR 等 对映 射到 高维空间 的 
文本 特征 进行 处理 大 部分 应用 在 文本 分类 
情感 分析 等 近年来 一些 非线性 模型 在 自然 语言 
处理 来 领域 取得 了 极大 的 成功 这里 简单 
介绍 一些 神经 网络 的 背景 知识 以及 在 文本处理 
中的 应用 神经 网络结构 常 用于 自然语言 处理 领域 的 
神经 网络 结构 包括 Feed Forward network Recurrent network Recursive 
network 等 其中 Feed Forward 神经网络 包括 全 连接 的 
MLP 具有/v 卷积/n 层/q 和池化/nr 层/q 的/uj CNN/w 网络/n RNN 
网络 包括 RNN LSTM GRU 等 Recursive network 是 结合 
树 模型 的 神经 网络 文本 特征 表示 使用 神经 
网络 最 重要 的 就是 明确 输入输出 是 什么 输入 
需要量 化成 数学 表示 的 向量 或 矩阵 便于 神经 
网络 计算 对于 文本 而言 最 传统 的 表示 方式 
是 One hot 表示 即 建立 一个 N 维 的 
词典 然后 文本 中 出现 某 个 单词 就用 1 
表示 从而 表示 这个 特征 但 这种 表示 方式 不能 
明确 词语 之间 的 相似性 另外 还 导致 特征 空间 
过大 不 便于 计算 现在 出现 了 各种 embedded 的 
方法 就是 把 主要 特征 映射 到 某一 特征 空间 
用 向量 表示 相似 词语 在 该 空间 中 距离 
较近 能够 明确 词语 之间 的 相似性 常见 的 表示 
工具 有 谷歌 的 word2vec 工具 得到 文本 的 特征 
表示 后 使用 神经 网络 进行 文本 分类 步骤 1 
针对 文本 需要 划分 的 输出 类 进行 相关 特征提取 
2 针对 提取 的 特征 可能 是 词语 词性 语言 
模型 等 将其 数学 表示 向 量化 3 把 所有 
特征 的 向量 表示 拼接 4 将 拼接 得到 的 
特征 表示 输入 到 神经网络 使用 已 标记 的 训练 
数据 集合 进行 神经网络 参数 训练 前馈 神经 网络 全 
连接 MLP 网络 1 . 网络 框架 2 . 输入输出 
对应 关系 每一个 计算 单元 都是/nr 一个 感知机 感知机 的 
输入 一般 表示 为 Input = ∑ wi ∗ xi 
+ bInput = \ sum w _ i * x 
_ i + b 感知机 的 输出 可以 表示 为 
Output = g Input Output = g Input 因此 对于 
上 图中 输出 可以 表示 为 3 . 常见 的 
激励函数 1 Sigmoid 函数 δ x = 11 + e 
− x \ begin { equation } \ delta x 
= \ frac { 1 } { 1 + e 
^ { x } } \ end { equation } 
2 tanh 函数 tanh x = e2x − 1e2x + 
1 \ begin { equation } tanh x = \ 
frac { e ^ { 2x } 1 } { 
e ^ { 2x } + 1 } \ end 
{ equation } 3 ReLU 函数 ReLU x = max 
0 x \ begin { equation } ReLU x = 
max 0 x \ end { equation } 4 . 
对于 神经 网络 的 输出 进行 再次 变换 例如 是 
一个 多 分类 问题 输出 可以 表示 为 y1 y2 
yN N 分类 的 问题 想 要知道 最终 是 属于 
哪 一类 的 需要 做 一个 Softmax softmax 函数 可以 
表示 为 softmax yi = exi ∑ kj = 1exj 
\ begin { equation } softmax y _ i = 
\ frac { e ^ { x _ i } 
} { \ sum _ { j = 1 } 
^ { k } { e ^ { x _ 
j } } } \ end { equation } 谁 
的 输出 最大 就 取 那 一类 为 输出 类 
损失 函数 对于 神经 网络 的 训练 最 重要 的 
就是 要 明确 目标函数 训练 的 目标 是 什么 就是 
要 使得 模型 尽可能 地 取 拟合 训练 数据 样本 
目标函数 就是 去 最小化 模型 输出 与 样本 标签 之间 
的 差距 损失 函数 就是 这一 用途 常见 的 损失 
函数 有 y ′ y 是 模型 输出 yy 表示 
数据 的 真是 标签 Hinge 函数 Loss = max 0 
1 − y ∗ y ′ \ begin { equation 
} Loss = max 0 1 y * y \ 
end { equation } 对数 损失 函数 Loss = log 
1 + exp − y ′ − y \ begin 
{ equation } Loss = log 1 + exp y 
y \ end { equation } 交叉 熵 Loss = 
− ∑ yilog y ′ i \ begin { equation 
} Loss = \ sum { y _ i } 
log y _ i \ end { equation } CNN 
应用于 文本 传统 对 文本 的 表示 为 CBOW Bag 
of words 没有 语序 的 概念 只能 说明 拥有 某个 
词 和 某个 词 出现 的 频率 这样 的 例如 
他 不好 他 相当 坏 与 他 不坏 他 相当 
好 可能 就 区 分不开 CNN 的 结构 就 帮助 
模型 记住 了 大量 的 局部 信息 能够 保存 位置 
顺序 基本 的 convolution + pooling 的 结构 如下 所示 
将 文本 编程 小 的 phrase 然后 分别 训练 前 
向 神经网络 再使用 pooling 将 多个 神经网络 整合 输出 能够 
辅助 记住 局部 文本 信息 RNNRNN 在 自然 语言 处理 
中 得到 广泛 使用 保留 句子 序列 信息 应用于 机器翻译 
问答 系统 等 领域 递归调用 前一 时刻 的 信息 输 
出给 后 一个 序列 RNN 有 很多 种 变形 例如 
LSTM 就是 解决 RNN 训练 过程 中 梯度 消失 问题 
产生 的 长短 时 记忆 网络 GRU 就是 解决 LSTM 
训练 过于 复杂 产生 的 神经 网络 Bi RNN 就是 
考虑 未来 序列 信息 用 于增强 模型 的 变形 RNN 
等 Recursive NN 区别于 RNN 它 是 将 句法树 引入 
的 神经 网络 具体 介绍 参考 http / / www 
. jianshu . com / p / 403665b55cd4 本 博客 
为 自然语言 处理 实战 课程 第一课 自然语言 处理 简介 讲稿 
文章 大纲 个人简介 本节 课程 导览 1 . 自然语言 处理 
NLP 简介 1.1 基础 技术 1.2 Nlp 核心技术 1.3 NlP 
+ 高端 技术 1.4 课程 涵盖 的 主要 内容 总揽 
2 . 知名 NLP 服务 系统 与 开源 组件 简介 
2.1 单一 服务提供商 2 . 1.1 汉语分词 系统 ICTCLAS2 . 
1.2 哈工大 语言 云 Language Technology Platform LTP 2 . 
1.3 HanLP2 . 1.4 BosonNLP2 . 2 云/ns 服务/vn 提供/v 
商2./nr 2.1/mx Amazon Comprehend2 . 2.2 阿里云 NLP2 . 2.3 
腾讯 云 NLP2 . 2.4 百度 语言 处理 基础 技术 
2.3 NLP 开源 组件 简介 2 . 3.1 NLTK2 . 
3.2 Jieba 分词 2 . 3.3 ICTCLAS2 . 3.4 Gensim 
参考文献 大家好 今天 开始 和 大家 分享 我 在 自然 
语言 处理 Natural Language Processing NLP 的 一些 学习 经验 
和 心得 体会 随着 人工智能 的 快速 发展 自然语言 处理 
和 机器 学习 技术 的 应用 愈加 广泛 为使 大家 
对 该 领域 整体 概况 有 一个 系统 明晰 的 
认识 同时 入门 一些 工程 实践 也 借 CSDN 为 
NLP 的 学习 开发者 们 搭建 一个 交流 的 平台 
个人简介 王雅 宁 2016年 毕业 于 陕西 师范大学 计算机 软件 
与 理论 专业 CSDN 博客 专家 主要 专注 于大/nr 数据 
计算机 视觉 自然语言/l 处理/v 对/p 大/a 数据/n 机器学习/i 类/q 软件/n 
开发/v 技术/n 都有/nr 比较/d 浓厚/a 的/uj 兴趣/n 熟悉 数据分析 机器学习 
计算机 视觉 等 领域 的 研发 工作 熟悉 windows Linux 
下 的 c / c + + 开发 OpenCV 图形图像 
库 的 各类 接口 熟悉 大 数据 生态圈 下 的 
Python 开发 曾 参与 并 负责 国家级 安全 项目 相关 
POC 验证 与 探索 工作 在 客户 业务 场景 下 
验证 产品 的 功能 与 性能 主要 工作 内容 有 
1 在 客户 现场 搭建 大 数据 产品 平台 与 
客户 沟通 根据 客户 的 需求 或 业务 场景 在 
大 数据 平台 上 实现 大 数据 平台 软件 的 
项目 实施 与 安装 部署 2 现场 提供 专业 服务 
包括 系统 大 数据 集群 故障分析 与 诊断 数据分析 服务 
业务 应用 对接 迁移 完善 提供 整体 解决 方案 3 
实现 在 单机 与 分布式 环境 下 发掘 等 短 
文本 的 兴趣 倾向 和 命名 实体 识别 该 项目 
对 结构化 数据 进行 分词 停用词 处理 命名 实体 识别 
图 计算 等 操作 目前 在 西安 知 盛 数据 
科技 有限公司 主要 负责 大 健康 平台 中 医疗 健康 
保险 的 部分 内容 构建 与 实施 主要 负责 包括 
数据 理解 数据 接入 与 清洗 描述性 统计分析 大 数据 
可视化 等 方面 的 工作 与 探索 对 自然 语言 
处理 保险 数据 异常 检测 方面 有 独到 的 探索 
经验 本节 课程 导览 本 小结 主要 介绍 内容 如下 
自然语言 处理 简介 3W 发展 历程 研究 现状 课程 涵盖 
的 主要 内容 总览 第一阶段 第二阶段 知名 NLP 服务 系统 
与 开源 组件 简介 对 汉语 自然 处理 的 服务 
提供 商 及其 服务 内容 做 一个 简单 的 梳理 
让 大家 能够 更好 的 了解 目前 的 技术 手段 
技术 现状 本 小节 课程 主要 内容 分为 2 大部分 
第一 部分 自然语言 处理 简介 用 认知 思维 的 方法 
结合 发展 历程 总揽 自然语言 处理 . 同时 顺带 介绍 
本 课程 的 主要 内容 本 课程 的 主要 内容 
我们 分成 两个 阶段 第一 个 阶段 如 思维导图 的 
右边 我们 力求 短 时间 内 上手 完成 爬虫 分词 
可视化 文本 分类 4个 自然语言 处理 实战 中 最 经常 
碰到 的 问题 我 首先 通过 爬虫 爬 取 自己 
CSDN 的 博客 积累 语料 其次 尝试 通过 一些 解决 
方案 的 对比 比如 不同 的 分词 组件 的 对比 
选择 一个 进行 可视化 词 云 主题 模型 的 生成 
最后 我们 介绍 一些 文本 分类 的 方法 文本 分类 
的 应用 较广 如 垃圾邮件 检测 舆论 分析 文本/n 查重等/nr 
场景/n 都/d 可以/c 转化/v 为/p 文本/n 分类/n 问题/n 第二 个 
阶段 的 课程 如果 有 时间 的话 我们 来 共同 
探讨 一些 业界 常用 的 NLP 实战 场景 如 脑 
图 左侧 所示 的 命名 实体 识别 问答 机器人 知识图谱 
基于 深度 学习 的 NLP 等 第二 部分 介绍 NLP 
技术 在 我国 的 应用 现状 以及 一些 我们 经常 
用到 的 开源 包 1 . 自然语言 处理 NLP 简介 
上学 的 时候 老师 经常 使用 这样 提问 的 方式 
加深 我们 对于 知识 的 理解 和 认知 what is 
it 自然语言 处理 Natural Language Processing 简称 NLP 是 人工 
智能 和 语言学 交叉 领域 下 的 分支 学科 用于 
分析 理解 和 生成 自然语言 以 方便 人和 计算机 设备 
进行 交流 以及 人 与人 之间 的 交流 NLP 是 
人工 智能 和 语言学 领域 的 交叉 学科 自然语言 处理 
在 广义 上 分为 两 大部分 第一 部分 为 自然语言 
理解 是 指 让 计算机 懂 人类 的 语言 第二 
部分 为 自然语言 生成 是 指 把 计算机 数据 转化 
为 自然语言 NLP 技术 按照 由浅入深 可以 分为 三 个 
层次 分别为 基础 技术 核心技术 NLP + 1.1 基础 技术 
这三个 层次 中 基础 技术 主要 是 对 自然 语言 
中 的 基本 元素 进行 表示 和 分析 比如 词汇 
短语 句子 词汇 短语 分析 中 大家 熟知 的 分词 
技术 就是 为了 解决 如下 问题 比如 我 去 北京大学 
玩 北京大学 独立 成词/nr 而/c 不是/c 分/v 成/n 北京/ns 和/c 
大学/n 句法 语义分析 对于 给定 的 句子 进行 分词 词性 
标记 命名 实体 识别 和 链接 句法分析 语义 角色 识别 
和 多义词 消 歧 1.2 Nlp 核心技术 NLP 的 核心 
技术 是 建立 在 基础 技术 之上 的 的 技术 
产出 基础 技术 中 如 词法 句法 的 分析 越 
准确 核心 技术 的 产出 才 能越 准确 核心 技术 
主要 包括 以下 几个 方面 信息 抽取 从 给定 文本 
中 抽取 重要 的 信息 比如 时间 地点 人物 事件 
原因 结果 数字 日期 货币 专有名词 等等 通俗 说来 就是 
要 了解 谁在 什么 时候 什么原因 对谁 做 了 什么 
事 有什/nr 么 结果 涉及 到 实体 识别 时间 抽取 
因果关系 抽取 等 关键 技术 文本 挖掘 或者 文本 数据挖掘 
包括 文本 聚 类 分类 信息 抽取 摘要 情感 分析 
以及 对 挖掘 的 信息 和 知识 的 可视化 交互式 
的 表达 界面 目前 主流 的 技术 都是/nr 基于 统计 
机器 学习 的 机器翻译 把 输入 的 源语言 文本 通过 
自动 翻译 获得 另外 一种 语言 的 文本 根据 输入 
媒介 不同 可以 细分 为 文本 翻译 语音 翻译 手语 
翻译 图形 翻译 等 机器翻译 从 最早 的 基于 规则 
的 方法 到 二十年前 的 基于 统计 的 方法 再到 
今天 的 基于 神经网络 编码 解码 的 方法 逐渐 形成 
了 一套 比较 严谨 的 方法 体系 信息检索 对 大 
规模 的 文档 进行 索引 可 简单 对 文档 中 
的 词汇 赋 之以 不同 的 权重 来 建立 索引 
也 可利用 句法分析 信息 抽取 文本 发掘 来 建立 更加 
深层 的 索引 在 查询 的 时候 对 输入 的 
查询 表达式 比如 一个 检索 词 或者 一个 句子 进行 
分析 然后 在 索引 里面 查找 匹配 的 候选 文档 
再 根据 一个 排序 机制 把 候选 文档 排序 最后 
输出 排序 得分 最高 的 文档 1.3 NlP + 高端 
技术 能够 真正 影响 我们 生活 的 黑 科技 能够 
通过 图灵测试 的 机器 问答 系统 我们 可以 称之为 NLP 
+ 问答 系统 对 一个 自然 语言 表达 的 问题 
由 问答 系统 给 出 一个 精准 的 答案 需要 
对 自然 语言 查询 语句 进行 某种 程度 的 语义分析 
包括 实体 链接 关系 识别 形成 逻辑 表达式 然后 到 
知识库 中 查找 可能 的 候选 答案 并 通过 一个 
排序 机制 找出 最佳 的 答案 对话 系统 系统 通过 
一 系列 的 对话 跟 用户 进行 聊天 回答 完成 
某一 项 任务 涉及 到 用户 意图 理解 通用 聊天 
引擎 问答 引擎 对话 管理 等 技术 此外 为了 体现 
上下文 相关 要 具备 多轮 对话 能力 AI 助手 目前 
自然语言 处理 的 前沿 已经 与 人类 真假难辨 https / 
/ v . qq . com / x / page 
/ w0648xqraxj . html 参考 https / / www . 
zhihu . com / question / 19895141 / answer / 
1494754101.4 课程 涵盖 的 主要 内容 总揽 2 . 知名 
NLP 服务 系统 与 开源 组件 简介 以下 我们 通过 
一些 知名 中文 NLP 服务提供商 包括/v 我们/r 熟知/v 的/uj 云/ns 
服务/vn 提供/v 商/n BAT/w aws 以及 两 家 科研 院所 
的 系统 简介 来 介绍 以及 宏观 认识 NLP 的 
各种 技术 手段 和 应用 场景 首先 介绍 的 是 
两家 NLP 基础 分析 准确率 很高 的 科研 院所 的 
产品 源自 北理工 和 哈工大 之后/f 我们/r 介绍/v 知名/v 云/ns 
服务/vn 提供/v 商的/nr 产品/n 2.1 单一 服务提供商 2 . 1.1 
汉语分词 系统 ICTCLAS 主页 http / / ictclas . nlpir 
. org / 在线 演示 系统 http / / ictclas 
. nlpir . org / Python 版本 https / / 
github . com / tsroten / pynlpir 需要 频繁 更新 
key https / / blog . csdn . net / 
sinat _ 26917383 / article / details / 77067515 对于 
习 总书记 这篇 新闻稿 的 实体 抽取 结果 http / 
/ news . 163 . com / 18 / 0715/14 
/ DMOTHJEK000189FH . html 该 系统 为 汉语 自然语言 处理 
领域 顶尖 大牛 北京理工大学 张 华平 博士 20年 的 专业 
技术 积累 NShort 革命性 分词 算法 的 发明者 主要 功能 
包括 中文分词 英文 分词 中英文 混合 分词 词性 标注 命名 
实体 识别 新词 识别 关键词 提取 支持 用户 专业 词典 
与 微博 分析 NLPIR 系统 支持 多种 编码 多种 操作系统 
多种 开发 语言 与 平台 该 平台 的 特点 为 
功能丰富 分词 语义 实体 发现 准确率 高 近期 发布 了 
最新 的 2018版 与 熟知 的 jieba ltp 清华 thulac 
2 . 1.2 哈工大 语言 云 Language Technology Platform LTP 
https / / www . ltp cloud . com / 
源自 哈工大 知名 的 分词 插件 ltp 准确率 高 Python 
版本 https / / github . com / HIT SCIR 
/ pyltp 语言 技术 平台 Language Technology Platform LTP 是 
哈工大 社会 计算 与 信息检索 研究 中心 历时 十 年 
开发 的 一整套 中文 语言 处理 系统 LTP 制定 了 
基于 XML 的 语言 处理 结果 表示 并在 此 基础 
上 提供 了 一整套 自底向上 的 丰富 而且 高效 的 
中文 语言 处理 模块 包括 词法 句法 语义 等 6项 
中文 处理 核心技术 以及 基于 动态链接库 Dynamic Link Library DLL 
的 应用 程序接口 可视化 工具 并且 能够 以 网络 服务 
Web Service 的 形式 进行 使用 语言 云 以 哈工大 
社会 计算 与 信息检索 研究 中心 研发 的 语言 技术 
平台 LTP 为基础 为 用户 提供 高效 精准 的 中文 
自然语言 处理 云 服务 使用 语言 云 非常简单 只 需要 
根据 API 参数 构造 HTTP 请求 即可 在线 获得 分析 
结果 而 无需 下载 SDK 无需 购买 高性能 的 机器 
同时 支持 跨平台 跨语言 编程 等 2014年 11月 哈工大 联合 
科大 讯 飞 公司 共同 推出 哈工大 讯 飞 语言 
云 借鉴 了 讯 飞在 全国性 大 规模 云计算 服务 
方面 的 丰富 经验 显著 提升 语言 云 对外 服务 
的 稳定性 和 吞吐量 为/p 广大/a 用户/n 提供/v 电信/n 级/q 
稳定性/n 和/c 支持/v 全国/n 范围/n 网络/n 接入/v 的/uj 语言/n 云/ns 
服务/vn 有效 支持 包括 中 小 企业 在内 开发者 的 
商业应用 需要 有关 更 多语言 云 API 的 使用 方法 
请 参考 http / / www . ltp cloud . 
com / document / windows 下 安装 pyltp 的话 应该 
是 需要 安装 visual studio 由于 LTP 是 用 c 
+ + 写 的 pyltp 也是 基于 它 封装 而成 
的 需要 调用 cl . exe 完成 源码 的 编译 
然后 下载 源码 使用 python setup . py install 的 
方式 进行 安装 就 可以 了 2 . 1.3 HanLPHanLP 
是 一系列 模型 与 算法 组成 的 NLP 工具包 由 
大 快 搜索 主导 并 完全 开源 目标 是 普及 
自然语言 处理 在 生产 环境 中 的 应用 HanLP 具备 
功能完善 性能 高效 架构 清晰 语料 时新 可自 定义 的 
特点 HanLP 提供 下列 16大 类 功能 中文分词 词性 标注 
命名 实体 识别 关键词 提取 自动 摘要 短语 提取 拼音 
转换 简繁转换 文本 推荐 依存 句法分析 文本 分类 情感 分析 
文本 聚 类 word2vec 文档 语义 相似 度 计算 语料库 
工具 项目 地址 https / / github . com / 
hankcs / HanLPpython 版本 https / / github . com 
/ hankcs / pyhanlpwindows 安装 指南 https / / github 
. com / hankcs / pyhanlp / wiki / Windows 
由于 HanLP 底层 是 java 版本 的 所以 对 java 
的 支持 比较 好 python 版本 中 有 一些 功能 
没有 实现 但 可以 通过 调用 java 实现 HanLP 随 
v1 . 6.8 发布 了 在 一亿字 的 大型 综合 
语料库 上 训练 的 分词 模型 该 语料 是 已知 
范围内 全世界 最大 的 中文分词 语料库 在 HanLP 的 在线 
演示 中 使用 已久 现在 无偿 公开 语料 规模 决定 
实际效果 所以 不用 多 说 HanLP 确实 可以 直接 拿来 
做 项目 有趣 的 是 HanLP 有着 非常 多 的 
衍生 项目 其中 docker 版 和 ES 版 值得 大家 
关注 这些 衍生 项目 无疑 更加 提高 了 HanLP 的 
可用性 灵活性 调用 代码 样例 from pyhanlp import * print 
HanLP . segment 你好 欢迎 在 Python 中 调用 HanLP 
的 API for term in HanLP . segment 下雨天 地面 
积水 print { } \ t { } . format 
term . word term . nature # 获取 单词 与 
词性 testCases = 商品 和 服务 结婚 的 和 尚未 
结婚 的 确实 在 干扰 分词 啊 买水 果然 后来 
世博园 最后 去 世博会 中国 的 首都 是 北京 欢迎 
新老 师生 前来 就餐 工信处 女干事 每月 经过 下属 科室 
都要 亲口 交代 24口 交换机 等 技术性 器件 的 安装 
工作 随着 页游 兴起 到 现在 的 页游 繁盛 依赖于 
存档 进行 逻辑 判断 的 设计 减少 了 但 这块 
也 不能 完全 忽略 掉 for sentence in testCases print 
HanLP . segment sentence # 关键词 提取 document = 水利部 
水资源 司 司长 陈明忠 9月 29日 在 国务院 新闻办 举行 
的 新闻 发布会 上 透露 \ 根据 刚刚 完成 了 
水资源 管理 制度 的 考核 有 部分 省 接近 了 
红线 的 指标 \ 有 部分 省 超过 红线 的 
指标 对 一些 超过 红线 的 地方 陈明忠 表示 对 
一些 取 用水 项目 进行 区域 的 限 批 \ 
严格 地 进行 水资源 论证 和 取水 许可 的 批准 
print HanLP . extractKeyword document 2 # 自动 摘要 print 
HanLP . extractSummary document 3 # 依存 句法分析 print HanLP 
. parseDependency 徐 先生 还 具体 帮助 他 确定 了 
把 画 雄鹰 松鼠 和 麻雀 作为 主攻 目标 2 
. 1.4 BosonNLPBosonNLP 界面 接口 友好 准确率 高 https / 
/ bosonnlp . com / demo 如果 你 在 网上 
搜索 汉语分词 评测 十有八九 你 会 搜索 到 专注 于 
汉语 自然语言 处理 技术 的 这家 公司 以及 下面 这张 
评测 结果 2.2 云/ns 服务/vn 提供/v 商2./nr 2.1/mx Amazon Comprehendhttps 
/ / amazonaws china . com / cn / comprehend 
/ nc2 = h _ a1Amazon Comprehend 是 一项 自然语言 
处理 NLP 服务 可利用 机器学习 发现 文本 中 的 见解 
和 关系 Amazon Comprehend 可以 识别 文本 语言 提取 关键 
的 短语 地点 人物 品牌 或 事件 了解 文本 的 
含义 是 肯定 还是 否定 还 可以 自动 按 主题 
整理 一系列 文本文件 您 可使用 Amazon Comprehend API 分析 文本 
并将 结果 进行 广泛 应用 包括 客户 意见 分析 智能 
文档 搜索 以及 Web 应用 程序 的 内容 个性化 设置 
该 服务 不断 地 通过 各种 信息 来源 包括 世界 
上 最大 的 自然 语言 数据集 之一 Amazon . com 
商品 描述 和 买家 评论 学习 和 提升 以 跟上 
语言 的 发展 演变 实例 利用 AWS Comprehend 打造 近 
实时 文本 情感 分析 https / / amazonaws china . 
com / cn / blogs / china / realizing near 
real time text sentiment analysis with aws comprehend / 可以 
看到 图中 aws 使用 kibana 仪表盘 和 Comprehend 服务 组成 
了 一个 实时 的 电影 评论 实时 分析 系统 其实 
主要 功能 就是 实现 了 分词 和 内容 来源 的 
地理位置 统计 看起来 很 炫 酷 2 . 2.2 阿里云 
NLPhttps / / data . aliyun . com / product 
/ nlp spm = 5176.8142029 . 388261.396 . 63f36d3eoZ8kNK 阿里 
的 NLP 服务 简介 为 自然语言 处理 是 为 各类 
企业 及 开发 者 提供 的 用于 文本 分析 及 
挖掘 的 核心 工具 已经 广泛 应用 在 电商 文化娱乐 
金融 物流 等 行业 客户 的 多项 业务 中 自然语言 
处理 API 可 帮助 用户 搭建 内容 搜索 内容 推荐 
舆情 识别 及 分析 文本 结构化 对话 机器人 等 智能 
产品 也 能够 通过 合作 定制 个性化 的 解决 方案 
按量 付费 的 基准价 在/p 没有/v 购买/v 资源/n 包或/nr 资源/n 
包/v 用尽/v 的/uj 情况/n 下/f 将 按 基准价 进行 计费 
其中 基础 版 对 每个 主 帐号 提供 每日 5 
万次 的 免费 使用 额度 商品 评价 解析 没有 免费 
额度 值得 注意 的 是 阿里云 的 nlp 服务 刚 
发布 不到 1年 应该 算是 领域内 的 新手 语料库 应该 
和 aws 一样 主要 为 商品 描述 和 评论 所以 
它 有 一项 功能 叫做 商品 评价 解析 时隔 半年 
之后 我们 再 来看 一下 这个 产品 名录 发现 功能 
更加 丰富 了 整体 来看 受限于 语料 的 积累 我 
认为 没有 什么 亮点 2 . 2.3 腾讯 云 NLPhttps 
/ / cloud . tencent . com / product / 
nlp 界面 友好 功能丰富 语料库 为 海量 综合性 语料库 腾讯 
云智/nr 在线 演示 系统 http / / nlp . qq 
. com / semantic . cgi2 . 2.4 百度 语言 
处理 基础 技术 http / / ai . baidu . 
com / tech / nlp 依托 海量 检索 数据 并且 
搜索引擎 本身 就是 NLP 最终 的 结果 产出 所以在 NLP 
领域 百度 无论是 语料库 丰富 程度 技术 先进性 以及 服务 
多样性 等 都是 遥遥领先 其他 厂家 基本上 可以 算作 是 
中文 NLP 服务提供商 的 业界 最佳 实践 功能丰富 且 技术 
领先 词 法分析 词 向量 表示 词义 相似 度 评论 
观点 抽取 文章 标签 依存 句法分析 DNN 语言 模型 短 
文本 相似 度 情感 倾向 分析 文章 分类 对话 情绪 
识别 文本 纠错 新闻 摘 要等 13个 大类 的 服务 
对于 个人 开发 者 来说 配比 了 免费 额度 对于 
词 向量 来说 每秒 免费 的 额度 是 5个 词 
基本 可以 够用 拿 来做 点 有趣 的 事情 了 
从 图中 结果 也 可以 看出 百度 对词 向量 相似 
度 的 分析 和我用/nr 余弦 相似 度 的 结果 一样 
可以 推断 出 百度 的 算法 比较 接地气 DNN 语言 
模型 Deep Neural Network DNN 模型 是 基本 的 深度 
学习 框架 DNN 语言 模型 是 通过 计算 给定 词 
组成 的 句子 的 概率 从而 判断 所 组成 的 
句子 是否 符合 客观 语言表达 习惯 通常用于 机器翻译 拼写 纠错 
语音识别 问答 系统 词性 标注 句法分析 和 信息检索 等 百度 
这个 模型 是 大厂 中 首个 公开 提供 服务 接口 
的 深度 学习 语言 模型 调用 方式 友好 简单 提供 
更加 简单 的 调用 方式 类似 aws boto3 如果 已 
安装 pip 执行 pip install baidu aip 即可 Sdk 方式 
安装 from aip import AipNlp 你 的 APPID AK SK 
APP _ ID = 你 的 App ID API _ 
KEY = 你 的 Api Key SECRET _ KEY = 
你 的 Secret Key client = AipNlp APP _ ID 
API _ KEY SECRET _ KEY word = 张飞 调 
用词 向量 表示 client . wordEmbedding word 2.3 NLP 开源 
组件 简介 NLP 领域 有 非常 多 的 开源 组件 
可以 用来 快速 构建 开发 的 原型 我 来 简单 
介绍 以下 四 个 知名 开源 组件 2 . 3.1 
NLTKhttp / / www . nltk . org / 最 
常用 的 自然 语言 处理 库 NLTK 是 一个 高效 
的 Python 构建 的 平台 用来 处理 人类 自然语言 数据 
基本 包含 了 NLP 中 需要 用到 的 所有 技术 
它 提供 了 易于 使用 的 接口 通过 这些 接口 
可以 访问 超过 50个 语料库 和 词汇 资源 如 WordNet 
还有 一套 用于 分类 标记 化 词干 标记 解析 和 
语义 推理 的 文本处理 库 以及 工业级 NLP 库 的 
封装 器 和 一个 活跃 的 讨论 论坛 古腾堡/nr 项目 
Project Gutenberg NLTK 包含 古腾堡/nr 项目 Project Gutenberg 中 电子 
文本 档案 的 经过 挑选 的 一小部分 文本 该 项目 
大约 有 57 000 本 免费 电子图书 放在 http / 
/ www . gutenberg . org / 上 我们 先 
要用 Python 解释器 加载 NLTK 包 然后 尝试 nltk . 
corpus . gutenberg . fileids 当然 其中 的 中文 语料 
也很 丰富 都是/nr 没有 版权 的 免费 文档 比如 李白 
文集 三字经 百家姓 等等 要是 用 这些 训练 中文 模型 
效果 可想而知 2 . 3.2 Jieba 分词 https / / 
github . com / fxsjy / jieba 结巴 中文分词 做 
最好 的 Python 中文分词 组件 Jieba Chinese for to stutter 
Chinese text segmentation built to be the best Python Chinese 
word segmentation module . 实现 基本 功能 的 代码 量 
在 一千行 左右 词典 长度 35w 安装 方式 友好 简洁 
高效 但 准确性 已经 跟不上 时代 85% 2 . 3.3 
ICTCLAShttp / / ictclas . nlpir . org / 主要 
功能 包括 中文分词 词性 标注 中英 混合 分词 命名 实体 
识别 用户 词典 功能 支持 GBK 编码 UTF8 编码 BIG5 
编码 新增 微博 分词 新词 发现 与 关键词 提取 张 
华平 博士 先后 倾力 打造 20 余年 内核 升级 10次 
全球 用户 突破 20万 先后 获得 了 2010年 钱伟长 中文信息处理 
科学技术 奖 一等奖 2003年 国际 SIGHAN 分词 大赛 综合 第一名 
2002年 国内 973 评测 综合 第一名 2 . 3.4 Gensimhttps 
/ / radimrehurek . com / gensim / 它 的 
slogan 是 Topic modelling for humans . Gensim 提供 了 
一个 发现 文档 语义 结构 的 工具 用于 从 原始 
的 非 结构化 的 文本 中 无 监督 地 学习 
到 文本 隐 层 的 主题 向量 表达 它 将 
语料 Corpus 向量 化 表示 后 主要 能够 实现 以下 
三 个 功能 建立 语言 模型 词 嵌入 模型 的 
训练 检索 和 语义分析 的 神器 简介 参考 https / 
/ www . cnblogs . com / iloveai / p 
/ gensim _ tutorial . html 参考文献 我 爱 自然 
语言 处理 http / / www . 52nlp . cn 
/ 深度 学习 与 中文 短文 本 分析 总结 与 
梳理 https / / blog . csdn . net / 
wangyaninglm / article / details / 66477222 分析 了 近 
5 万首 全唐诗 发现 了 这些 有趣 的 秘密 http 
/ / www . growthhk . cn / cgo / 
9542 . html 万字 干货 ｜ 10款 数据分析 工具 助 
你 成为 新媒体 运营 领域 的 增长 黑客 http / 
/ www . woshipm . com / data analysis / 
553180 . htmljieba 分词 简介 与 解析 https / / 
www . cnblogs . com / baiboy / p / 
jieba2 . html 有 哪些 好 的 汉语分词 方案 https 
/ / www . zhihu . com / question / 
19578687 基于 分布式 的 短文 本 命题 实体 识别 之 
人名 识别 python 实现 https / / blog . csdn 
. net / wangyaninglm / article / details / 75042151NLP 
技术 的 应用 及 思考 https / / yq . 
aliyun . com / articles / 78031 自然语言 处理 nlp 
作为 计算机 的 一个 研究 方向 存在 已久 但是 最近 
人工智能 这一 波 热潮 又 让 nlp 重新得到 巨大 关注 
由于 处理 对象 是 语言 这 一种 人类 特有 的 
沟通 工具 以及 其 丰富 巨大 的 信息量 给人 一种 
错觉 似乎 这是 人工智能 领域 真正 的 皇冠 达到 最终 
真正 人工智能 强 人工智能 的 最近 之路 但是 事实 是 
如何 不 敢 随意 断言 只是 有点 感慨 一下 就是 
这 一块 很难 做 语言 作为 人类 的 工具 一 
方面 可以 让 我们 与 外界 交互 可以 说 话 
可以 记录 可以 标记 保存 了 巨大 的 信息 这样 
的 信息 只有 人类 才 能 读懂 并 理解 我们 
是否 可以 理解 为 如果 个人 心智 是 一个 操作 
系统 的话 语言 和 符号 标记 是 操作 系统 上 
的 一些 接口函数 通过 这些 接口函数 进行 操作 系统 与 
操作 系统 之间 的 交互 即 人 与人 的 交流 
然而 关于 这个 操作系统 医学 科学家 生物学家 认知 科学家 都还/nr 
没有 完全 弄 清楚 人 的 意识 以及 思维 活动 
的 具体 过程 只能 部分 描述 而已 在 语言 层面 
也 只能 用 语言 来 表达 语言 用 语言 解释 
语言 不过 正如 递归 那样 用 初始 的 少量 定义 
来 描述 全部 情况 也是 很 划算 的 因此 对于 
自然 语言 处理 的 研究 虽 不断 深入 现有 水平 
下 仍然 无法 触及 人类 或 类人 智能 的 核心 
问题 当然 有人 说 不和 人 一样 的 智能 也 
可以 不过 这条路 也是 很 难走 目前 的 研究 仍然 
是 在 语言 内部 做 数据 映射 处理 为主 是以 
encode 和 decode 为 主要 内容 的 这 正如 清华大学 
刘知远 老师 所说 自然语言 处理 是 工具 链 只是 工具 
链 而非 信息 的 源头 或 终端 看 以下 这个 
例子 甲 说 今晚 来 我家 吃饭 乙 说 晚上 
我 爸 要 回家 仅从 两句话 单独 的 语义分析 就 
会 觉得 牛头不对马嘴 发觉 毫 无联系 计算机 必须 像 人 
一样 看到 一句话 具有 联想 推断 等 能力 具有 分析 
对方 这 句话 有 哪几种 含义 的 潜意识 才能 提高 
文本 的 理解 度 而 这个 过程 我们 人类 在 
潜意识 或者说 在 刚才 提到 的 操作 系统 中 中 
已经 处理 好 了 而 这些 正是 让 计算机 具有 
语言 智能 的 巨大 难题 这/r 可能/v 需要/v 多个/m 学科/n 
的/uj 众多/m 科学家/n 精诚合作/l 数十/m 年/m 甚至/d 数百/m 年/m 数千/m 
年/m 才能/v 解决/v 但 它 终归是 要被 解决 的 上 
一篇 自然语言 处理 中 传统 词 向量 表示 VS 深度 
学习 语言 模型 一 主要 介绍 了 关于 语言 表示 
的 问题 今天 在 正式 接触 word2vec 之前 还是 想 
啰嗦 一下 自然语言 处理 的 基本 问题 以及 语言 模型 
等 方面 的 知识 1 . 语言 模型 语言 模型 
language model LM 在 自然 语言 处理 中 占有 重要 
的 地位 尤其在 基于 统计模型 的 语音 识别 机器翻译 分词 
和 句法分析 等 相关 研究 中 得到 广泛 应用 语言 
模型 简单 地 说 就是 判断 一个 句子 的 概率 
的 模型 其 基本 任务 就是 使 得 一个 句子 
更 符合 自然 语言 的 叙述 规则 使得 符合 这套 
叙述 规则 的 句子 的 概率 更大 即 计算 P 
S 的 概率 其中 为 一句话 早期 的 自然 语言 
处理 系统 主要 是 基于 人工 撰写 的 规则 这种方法 
费时费力 且 不能 覆盖 各种 语言 现象 上个世纪 80 年代 
后期 机器学习 算法 被 引入 到 自然 语言 处理 中 
这要 归功于 不断 提高 的 计算 能力 研究 主要 集中 
在 统计 模型 上 这种方法 采用 大 规模 的 训练 
语料 corpus 对模型 的 参数 进行 自动 的 学习 和 
之前 的 基于 规则 的 方法 相比 这种方法 更具 鲁棒性 
2 . 统计 语言 模型 统计 语言 模型 Statistical Language 
Model 就是 利用 统计 数据 来 求 P S 的 
大小 在 NLP 领域 中 大 部分 的 任务 都是/nr 
基于 词语 的 细分 粒度 来 构建 模型 由 词语 
构成 句子 段落 文章 等 那么 统计 语言 模型 的 
处理单元 当然 也 多 是 已 词语 为 单位 而 
展开 的 以 词语 为 单位 对于 序列 可以 由 
n 个 词语 W1 W2 W3 . . . Wn 
表示 因此 语言 模型 可以 表示 为 求 概率 P 
S 的 大小 在 上式 中 产生 第 i 1 
≤ i ≤ n 个 词 的 概率 是由 已经 
产生 的 i 1个 词 决定 的 一般 的 我们 
将 前 i 1个 词 称为 第 i 个 词 
的 历史 或者 上下文 在 这种 计算 方法 中 一方面 
随着 历史 长度 的 增加 不同 的 词语 的 组合 
数目 按照 指数级 增长 如果 历史 的 长度 为 i 
1 那么 就有 种 不同 的 历史 情况 存在 这样 
的话 就 会 存在 着 参数 空间 过大 的 问题 
另一方面 对于 组合 特征 中 存在 着 大量 的 未 
出现 的 组合 这样 就 导致 该 组合 出现 的 
次数 为 0 最终 导致 数据 稀疏 严重 的 问题 
2.1 N 元 模型 N gram 由于 长 历史 信息 
的 组合 数目 过大 的 问题 导致 概率 无法 计算 
所以 就 需要 一种 替代 的 方案 来 近似 这个 
概率 使之 得以 解决 N 元 模型 就是 在 这种 
需求 上 产生 的 N 元 模型 是 利用 了 
马尔科夫 假 设来 将 求解 进行 转换 马尔科夫 假设 认为 
随意 一个 词 出现 的 概率 只 与 它 前面 
出现 的 有限 的 一个 或者 n 个 词 有关 
这样 就将 长 历史 信息 转换 为 只 关注 于 
前面 出现 的 k 个 历史 信息 大大 地 简化 
了 组合 空间 通常 情况下 n 不能 过大 否则 组合 
空间 过大 的 情况 依然 存在 无法 解决 根本 问题 
所以 n 一般 取 1 2 3 当 n = 
1时 被 称为 一 元语言 模型 unigram 说明 一个 词 
的 出现 与 它 周围 的 词 是 独立 的 
当 n = 2时 被 称为 二 元语言 模型 bigram 
也叫 一 阶 马尔科夫 链 Markov chain 说明 当前 词 
的 出现 只 与其 前 一个 词 有关 当 n 
= 3时 被 称为 三 元语言 模型 trigram 也叫 二阶 
马尔科夫 链 说明 当前 词 的 出现 与其 前面 的 
两个 词 有关 以上 内容 我们 大致 的 讲解 了 
n gram 语言 模型 为什么 存在 以及 不同 的 n 
元 gram 的 形式 总结 语言 模型 是 为了 求解 
语言 模型 中 一个 句子 产生 的 概率 大小 而 
n gram 是 为了 对 求解 过程 中 参数 空间 
过大 问题 的 一步 优化 假设 且 基于 不同 的 
n 有着 不同 的 gram 形式 3 . 神经 网络 
语言 模型 在 第 2节 中 我们 知道 语言 模型 
可以 使用 n gram 模型 来 进行 近似 求解 可以 
解决 一 部分 的 自然 语言 处理 领域 的 基础 
问题 其 在 词性 标注 句法分析 机器翻译 信息检索 等 任务 
中 起到 了 重要 作用 然而 随着 深度 学习 的 
不断 发展 神经网络 相关 研究 越来越 深入 神经 网络 语言 
模型 Neural Network Language Model NNLM 越来越 受到 学术界 和 
工业界 的 关注 接下来 将 系统 介绍 下 NNLM 用 
神经 网络 来 训练 语言 模型 的 思想 最早 由 
百度 IDL 深度 学习 研究院 的 徐伟 提出 NNLM Nerual 
Network Language Model 是 这 方面 的 一个 经典 模型 
NNLM 依赖 的 一个 核心 概念 就是 词 向量 Word 
Embedding 词 向量 源于 Hinton 在 Learning distributed representations of 
concepts 提出 的 Distributed Representation Bengio 将其 引入 到 语言 
模型 建模 中 提出 了 NNLM 模型 的 训练 数据 
是 一组 词 的 序列 W1 W2 . . . 
Wn Wn ∈ V 其中 V 为 词典 Vi 表示 
词典 中的 第 i 个 单词 NNLM 的 训练 目标 
也 是 训练 如下 模型 其中 wt 表示 词序 列中 
第   t 个 单词 w1 . . . wt 
1 表示 从第/nr 1个 词 到 第 t   个 
词 组成 的 子 序列 其 模型 的 框架 为 
这里 的 模型 主要 分为 两个 部分 特征 映射 和 
计算 条件 概率分布 这里 偷懒 了 直接 截图 过来 了 
3333 ~ 需要 注意 的 是 一般 的 神经 网络 
模型 不 需要 对 输入 进行 训练 而 该 模型 
中 的 输入 x = C wt − n + 
1 . . . C wt − 1   是 
词 向量 也是 需要 训练 的 参数 由此可见 模型 的 
权重 参数 与 词 向量 是 同时 进行 训练 模型/n 
训练/vn 完成/v 后/f 同时/c 得到/v 网络/n 的/uj 权重/n 参数/n 和词/nr 
向量/n word2vec 也 是 通过 类似 的 方式 来 训练 
语言 模型 最后 顺便 得到 最后 的 词 向量 接下来 
的 博文 中将 会 介绍 下 word2vec 特别感谢 1 . 
我们 是 这样 理解 语言 的 3 神经 网络 语言 
模型 2 . 宗 成庆 统计 自然语言 处理 第二 版 
3 . 语言 模型 的 基本 概念 4 . Bengio 
Y Ducharme R Vincent P et al . A neural 
probabilistic language model J . Journal of machine learning research 
2003 3 Feb 1137 1155.5 . 知乎 深入浅出 讲解 语言 
模型 C ∈ R | V | × mC ∈ 
R | V | × m   将 输入 一 
文法 型 语言 模 型文法 型 语言 模型 是 人工 
编制 的 语言学 文法 文法 规则 来源于 语言学家 掌握 的 
语言 学 知识 和 领域 知识 但 这种 语言 模型 
不能 处理 大规模 真实 文本 二 统计 语言 模型 统计 
语言 模型 常用 的 思想 是 用 一个 词 在 
句子 中的 neighborhood 表示 该词 主要 的 统计 语言 模型 
有 1 . 上下文 无关 模型 2 . N gram 
模型 考虑 词形 方面 的 特征 1 一元 模型 2 
二元 模型 3 N 元 模型 3 . N pos 
模型 考虑 词类 词性 方面 的 特征 前 一个 词 
的 词类 决定 下 一个 词 出现 的 概率 4 
. 基于 决策树 的 语言 模型 5 . 最大熵 模型 
6 . 动态 自适应 基于 缓存 的 语言 模型 7 
. Hyperspace Analogue to Language method HAL HAL Lund & 
Burgess 1996 方法 可以 用 一个 co occurrence matrix 表示 
任意 两个 词 相关性 8 . Latent Semantic Analysis LSA 
LSA Deerwester et al . 1990 Landauer Foltz & Laham 
1998 中 co occurrence matrix 是 word document 矩阵 表示 
文档 中 出现 某 词 的 频率 统计/v 后/f 将其/i 
进行/v normalization/w 将/d document/w 从/p 稀疏/a 的/uj 高维/nr Vocabulary/w 空间/n 
映射/v 到/v 一个/m 低维/i 的/uj 向量空间/i 我们 称之为 隐含 语义空间 
Latent Semantic Space 9 . COALS Rohde et al . 
2009 在 HAL 上 做了 小 改动 将 HAL 所得 
co occurrence matrix 进行 correlation normalization 三 严格 匹配 模型 
概率模型 严格 匹配 模型 是 给定 一个 查询 利用 匹配 
函数 将 文档 集 分为 两个 集合 匹配 集 和非/nr 
匹配 集 . 严格 匹配 模型 中 最简单 并且 常用 
的 一种 便是 布尔 模型 . 在 布尔 模型 中 
要 定义 一个 二 值 变量 的 集合 这些 变量 
都 对应 文档 的 某个 特征 称 为特征 变量 . 
文档 由 这些 特征 变量 组成 的 集合 来 表示 
如果 变量 对 文档 的 内容 表示 有 贡献 则 
赋值 为 True 否 则为 False . 查询 语句 则 
是由 特征 变量 和 操作符 and or 和 not 组成 
的 表达式 . 匹配 函 数则 遵循 布尔逻辑 的 规则 
. 概率模型 是 信息 检索 的 又一 主要 模型 这种 
模型 主要 针对 信息检索 中 相关性 判断 的 不确定性 以及 
查询 信息 表示 的 模糊性 . 基于 概率 排序 原则 
对于 给定 的 用户 查询 Q 对 所有 的 文本 
D 计算 概率 P R | D Q 并从 大 
到 小 进行 排序 . 这里 R 表示 文本 D 
与 查询 Q 的 相关性 . 如果 以 D = 
d1 d2 dn 表示 文本 D N 为特征 项 个数 
特征 项i在/nr 文本 中 出现 di = 1 否则 di 
= 0 . 概率模型 的 缺点 是 对文 本集 的 
依赖性 过强 而且 处理 问题 过于 简单 . 四 基于 
分布 理论 的 独立 检验 模型 基于 分布 理论 的 
独立 检验 模型 有 关键 要素 分别 是 互信息 t 
测试 相 异度 t 测试 差 相关度 i 平方 的 
统计量 五 基于 规则 的 模型 这种 模型 假设 自然 
语言 的 知识 可以用 规则 集 来 表示 而 规则 
集 的 获取 既可以 人工 编写 唯理主义 也 可以 有 
语料库 中 学习 得到 经验主义 . 1956年 乔姆斯基 发表 了 
语言 描述 的 三个 模型 由此 兴起 的 短语 结构 
语法 乔姆斯基 语法 体系 和 其他 的 一些 语言 描述 
模型 都 可以 看作 是 描述 语言 的 规则 模型 
基于 这些 规则 模型 的 语言 处理 技术 就是 句法分析 
技术 和 语义分析 技术 . 六   语言 模型 变种 
Class based N gram Model 该 方法 基于 词类 建立 
语言 模型 以 缓解 数据 稀疏 问题 且 可以 方便 
融合 部分 语法 信息 Topic based N gram Model 该 
方法 将 训练 集 按 主题 划分 成 多个 子集 
并对 每个 子集 分别 建立 N gram 语言 模型 以 
解决 语言 模型 的 主题 自适应 问题 Cache based N 
gram Model 该 方法 利用 cache 缓存 前一 时刻 的 
信息 以 用于 计算 当前 时刻 概率 以 解决 语言 
模型 动态 自适应 问题 应用 各种 输入法 搜狗 QQ 微软 
等 Skipping N gram Model & Trigger based N gram 
Model 二者 核心思想 都是 刻画 远距离 约束 关系 指数 语言 
模型 最大熵 模型 MaxEnt 最大熵 马尔科夫 模型 MEMM 条件 随机 
域 模型 CRF 七   主题 模型 及其 发展 主题 
模型 有 两种 pLSA P r o b a b 
i l i s t i c L a t 
e n t Semantic Analysis 和 LDA Latent Dirichlet Allocation 
主题 模型 的 起源 是 隐性 语义 索引 LSI 隐性 
语义 索引 后来 又 发展 为 概率 隐性 语义 索引 
pLSI 主题 的 实现 一般 包括 五 部分 的 内容 
输入 基本 假设 表示 参数估计 新 样本 推断 输入 主要 
是 文档 集合 基本 假设 是 词 袋 bag of 
words 假设 即 一篇 文档 内 的 单词 可以 交换 
次序 而 不影响 模型 的 训练 结果 主题 模型 的 
表示 图 模型 和 生成 过程 Topic Model 主要 可以 
分为 四大 类 1 无 监督 的 无层次 结构 的 
topic model 2 无 监督 的 层次 结构 的 topic 
model 3 有 监督 的 无层次 结构 的 topic model 
4 有 监督 的 层次 结构 的 topic model 对于 
1 主要 有 PLSA LDA Correlated Topic Model PAM Concept 
Topic Model 等 对于 2 主要 有 HLDA HDP HPAM 
等 对于 3 主要 有 S LDA Disc LDA MM 
LDA Author Model Labeled LDA PLDA 等等 对于 4 主要 
有 hLLDA HSLDA 等 spark 中 常用 的 一些 自然 
语言 处理 方法 分词 tf idf word2vec 文本 分类 等 
看看 代码 吧 package com . iclick . word2vec import 
org . apache . log4j . { Level Logger } 
import org . apache . spark . { SparkConf SparkContext 
} import org . apache . spark . sql . 
SQLContext import org . apache . spark . mllib . 
feature . { Word2Vec Word2VecModel } import org . apache 
. spark . mllib . linalg . { SparseVector = 
SV } import org . apache . spark . mllib 
. feature . HashingTF import org . apache . spark 
. mllib . feature . IDF object Word2VecTest { def 
main args Array String Unit = { Logger . getLogger 
org . apache . spark . setLevel Level . ERROR 
Logger . getLogger org . eclipse . jetty . server 
. setLevel Level . OFF val sc = new SparkContext 
local mysql val sqlContext = new SQLContext sc val path 
= D \ \ SPARKCONFALL \ \ Spark 机器学习 数据 
\ \ 20news bydate train \ \ * val rdd 
= sc . wholeTextFiles path . cache val xxx = 
rdd . map { case file text = file . 
split / . takeRight 2 . head } . map 
n = n 1 . reduceByKey _ + _ . 
collect . sortBy _ . _ 2 . mkString \ 
n println 文章 主题 的 数目 println xxx val newsgroups 
= rdd . map { case file text = file 
. split / . takeRight 2 . head } println 
分词 数目 val text = rdd . map { case 
file text = text } val whieteSpaceSplit = text . 
flatMap { t = t . split } . map 
_ . toLowerCase println whieteSpaceSplit . distinct . count println 
whieteSpaceSplit . sample true 0.3 42 . take 100 . 
mkString println 改进 分词 val nonWordSplit = text . flatMap 
t = t . split \ W + . map 
_ . toLowerCase println nonWordSplit . distinct . count println 
nonWordSplit . distinct . sample true 0.3 42 . take 
100 . mkString val regex = ^ 0 9 * 
. r val filterNumbers = nonWordSplit . filter token = 
regex . pattern . matcher token . matches println filterNumbers 
. distinct . count println filterNumbers . distinct . sample 
true 0.3 42 . take 100 . mkString println 移除 
停用词 val tokenCounts = filterNumbers . map t = t 
1 . reduceByKey _ + _ val oreringDesc = Ordering 
. by String Int Int _ . _ 2 / 
/ println tokenCounts . top 20 oreringDesc . mkString \ 
n val stopwords = Set the a an of or 
in for by on but is not with as was 
if they are this and it have from at my 
be that to val t o k e n C 
o u n t s F i l t e 
r e d t o p w o r d 
s = tokenCounts . filter { case k v = 
stopwords . contains k } / / println t o 
k e n C o u n t s F 
i l t e r e d t o p 
w o r d s . top 20 oreringDesc . 
mkString \ n val t o k e n C 
o u n t s F i l t e 
r e d i z e = t o k 
e n C o u n t s F i 
l t e r e d t o p w 
o r d s . filter { case k v 
= k . size = 2 } println t o 
k e n C o u n t s F 
i l t e r e d i z e 
. top 20 oreringDesc . mkString \ n println 移除 
低频词 val oreringAsc = Ordering . by String Int Int 
_ . _ 2 / / println t o k 
e n C o u n t s F i 
l t e r e d i z e . 
top 20 oreringAsc . mkString \ n val rareTokens = 
tokenCounts . filter { case k v = v 2 
} . map { case k v = k } 
. collect . toSet val t o k e n 
C o u n t s F i l t 
e r e d A l l = t o 
k e n C o u n t s F 
i l t e r e d i z e 
. filter { case k v = rareTokens . contains 
k } println t o k e n C o 
u n t s F i l t e r 
e d A l l . top 20 oreringAsc . 
mkString \ n def tokenize line String Seq String = 
{ line . split \ W + . map _ 
. toLowerCase . filter token = regex . pattern . 
matcher token . matches . filterNot token = stopwords . 
contains token . filterNot token = rareTokens . contains token 
. filter token = token . size = 2 . 
toSeq } / / println text . flatMap doc = 
tokenize doc . distinct . count val tokens = text 
. map doc = tokenize doc println tokens . first 
. take 20 println 训练 模型 val dim = math 
. pow 2 18 . toInt val hashingTF = new 
HashingTF dim val tf = hashingTF . transform tokens tf 
. cache val v = tf . first . asInstanceOf 
SV println v . size println v . size println 
v . values . size println v . values . 
take 10 . toSeq println v . indices . take 
10 . toSeq println fit & transform val idf = 
new IDF . fit tf val tfidf = idf . 
transform tf val v2 = tfidf . first . asInstanceOf 
SV println v2 . values . size println v2 . 
values . take 10 . toSeq println v2 . indices 
. take 10 . toSeq / / 分析 权重 val 
minMaxVals = tfidf . map { v = val sv 
= v . asInstanceOf SV sv . values . min 
sv . values . max } val globalMinMax = minMaxVals 
. reduce { case min1 max1 min2 max2 = math 
. min min1 min2 math . max max1 max2 } 
println globalMinMax / / globalMinMax Double Double = 0.0 66155 
. 39470409753 / / 常用词 val common = sc . 
parallelize Seq Seq you do we val tfCommon = hashingTF 
. transform common val tfidfCommon = idf . transform tfCommon 
val commonVector = tfidfCommon . first . asInstanceOf SV println 
commonVector . values . toSeq / / 不常 出现 的 
单词 val uncommon = sc . parallelize Seq Seq telescope 
legislation investment val tfUncommon = hashingTF . transform uncommon val 
tfidfUncommon = idf . transform tfUncommon val uncommonVector = tfidfUncommon 
. first . asInstanceOf SV println uncommonVector . values . 
toSeq / / / / 4 使用 模型 / / 
4.1 余弦 相似 度 import breeze . linalg . _ 
val hockeyText = rdd . filter { case file text 
= file . contains hockey } val hockeyTF = hockeyText 
. mapValues doc = hashingTF . transform tokenize doc val 
hockeyTfIdf = idf . transform hockeyTF . map _ . 
_ 2 val hockey1 = hockeyTfIdf . sample true 0.1 
42 . first . asInstanceOf SV val breeze1 = new 
SparseVector hockey1 . indices hockey1 . values hockey1 . size 
val hockey2 = hockeyTfIdf . sample true 0.1 43 . 
first . asInstanceOf SV val breeze2 = new SparseVector hockey2 
. indices hockey2 . values hockey2 . size val cosineSim 
= breeze1 . dot breeze2 / norm breeze1 * norm 
breeze2 println cosineSim val graphicsText = rdd . filter { 
case file text = file . contains comp . graphics 
} val graphicsTF = graphicsText . mapValues doc = hashingTF 
. transform tokenize doc val graphicsTfIdf = idf . transform 
graphicsTF . map _ . _ 2 val graphics = 
graphicsTfIdf . sample true 0.1 42 . first . asInstanceOf 
SV val breezeGraphics = new SparseVector graphics . indices graphics 
. values graphics . size val cosineSim2 = breeze1 . 
dot breezeGraphics / norm breeze1 * norm breezeGraphics println cosineSim2 
val baseballText = rdd . filter { case file text 
= file . contains baseball } val baseballTF = baseballText 
. mapValues doc = hashingTF . transform tokenize doc val 
baseballTfIdf = idf . transform baseballTF . map _ . 
_ 2 val baseball = baseballTfIdf . sample true 0.1 
42 . first . asInstanceOf SV val breezeBaseball = new 
SparseVector baseball . indices baseball . values baseball . size 
val cosineSim3 = breeze1 . dot breezeBaseball / norm breeze1 
* norm breezeBaseball println cosineSim3 / / 4.2 学习 单词 
与 主题 的 映射 关系 / / 多 分类 映射 
import org . apache . spark . mllib . regression 
. LabeledPoint import org . apache . spark . mllib 
. classification . NaiveBayes import org . apache . spark 
. mllib . evaluation . M u l t i 
c l a s s M e t r i 
c s val newsgroupsMap = newsgroups . distinct . collect 
. zipWithIndex . toMap val zipped = newsgroups . zip 
tfidf val train = zipped . map { case topic 
vector = LabeledPoint newsgroupsMap topic vector } train . cache 
/ / 朴素 贝叶斯 训练 val model = NaiveBayes . 
train train lambda = 0.1 / / 加载 测试 数据集 
val testPath = D \ \ SPARKCONFALL \ \ Spark 
机器学习 数据 \ \ 20news bydate test \ \ * 
val testRDD = sc . wholeTextFiles testPath val testLabels = 
testRDD . map { case file text = val topic 
= file . split / . takeRight 2 . head 
newsgroupsMap topic } val testTf = testRDD . map { 
case file text = hashingTF . transform tokenize text } 
val testTfIdf = idf . transform testTf val zippedTest = 
testLabels . zip testTfIdf val test = zippedTest . map 
{ case topic vector = LabeledPoint topic vector } / 
/ 计算 准确度 和多/nr 分类 加权 F 指标 val p 
r e d i c t i o n A 
n d L a b e l = test . 
map p = model . predict p . features p 
. label val accuracy = 1.0 * p r e 
d i c t i o n A n d 
L a b e l . filter x = x 
. _ 1 = = x . _ 2 . 
count / test . count println accuracy val metrics = 
new M u l t i c l a s 
s M e t r i c s p r 
e d i c t i o n A n 
d L a b e l println metrics . weightedFMeasure 
/ / 5 评估 val rawTokens = rdd . map 
{ case file text = text . split } val 
rawTF = rawTokens . map doc = hashingTF . transform 
doc val rawTrain = newsgroups . zip rawTF . map 
{ case topic vector = LabeledPoint newsgroupsMap topic vector } 
val rawModel = NaiveBayes . train rawTrain lambda = 0.1 
val rawTestTF = testRDD . map { case file text 
= hashingTF . transform text . split } val rawZippedTest 
= testLabels . zip rawTestTF val rawTest = rawZippedTest . 
map { case topic vector = LabeledPoint topic vector } 
val r a w P r e d i c 
t i o n A n d L a b 
e l = rawTest . map p = rawModel . 
predict p . features p . label val rawAccuracy = 
1.0 * r a w P r e d i 
c t i o n A n d L a 
b e l . filter x = x . _ 
1 = = x . _ 2 . count / 
rawTest . count println rawAccuracy val rawMetrics = new M 
u l t i c l a s s M 
e t r i c s r a w P 
r e d i c t i o n A 
n d L a b e l println rawMetrics . 
weightedFMeasure println word2Vec 模型 训练 val word2vec = new Word2Vec 
word2vec . setSeed 42 / / we do this to 
generate the same results each time val word2vecModel = word2vec 
. fit tokens println 寻找 最 相似 的 二十 个 
單 詞 word2vecModel . findSynonyms hockey 20 . foreach println 
word2vecModel . findSynonyms legislation 20 . foreach println } } 
01 语言 处理 与 Python # * coding utf 8 
* # win10 python3 . 5.3 / python3 . 6.1 
nltk3 . 2.4 # Python 自然语言 处理 01 语言 处理 
与 Python # 安装 nltk 库 # pip3 install nltk 
= = 3 . 2.4 # 下载 nltk 数据 nltk 
_ data import nltk nltk . download # 出现 NLTK 
Downloader 对话框 后 设置 Download Directory 路径 后 点击 Download 
按钮 开始 下载 如果 下载 失败 或者 卡顿 重新 下载 
# 1.1 语言 计算 文本 和 词汇 from _ _ 
future _ _ import division from nltk . book import 
* * * * Introductory Examples for the NLTK Book 
* * * Loading text1 . . . text9 and 
sent1 . . . sent9 Type the name of the 
text or sentence to view it . Type texts or 
sents to list the materials . text1 Moby Dick by 
Herman Melville 1851 text2 Sense and Sensibility by Jane Austen 
1811 text3 The Book of Genesis text4 Inaugural Address Corpus 
text5 Chat Corpus text6 Monty Python and the Holy Grail 
text7 Wall Street Journal text8 Personals Corpus text9 The Man 
Who Was Thursday by G . K . Chesterton 1908 
print text1 Text Moby Dick by Herman Melville 1851 print 
text2 Text Sense and Sensibility by Jane Austen 1811 # 
搜索 文本 result = text1 . concordance monstrous print result 
Displaying 11 of 11 matches ong the former one was 
of a most monstrous size . . . . This 
came towards us ON OF THE PSALMS . Touching that 
monstrous bulk of the whale or ork we have r 
ll over with a heathenish array of monstrous clubs and 
spears . Some were thick d as you gazed and 
wondered what monstrous cannibal and savage could ever hav that 
has survived the flood most monstrous and most mountainous That 
Himmal they might scout at Moby Dick as a monstrous 
fable or still worse and more de th of Radney 
. CHAPTER 55 Of the Monstrous Pictures of Whales . 
I shall ere l ing Scenes . In connexion with 
the monstrous pictures of whales I am strongly ere to 
enter upon those still more monstrous stories of them which 
are to be fo ght have been rummaged out of 
this monstrous cabinet there is no telling . But of 
Whale Bones for Whales of a monstrous size are oftentimes 
cast up dead u None print text1 . similar monstrous 
true contemptible christian abundant few part mean careful puzzled mystifying 
passing curious loving wise doleful gamesome singular delightfully perilous fearless 
None text2 . similar monstrous very so exceedingly heartily a 
as good great extremely remarkably sweet vast amazingly text2 . 
common _ contexts monstrous very a _ pretty am _ 
glad a _ lucky is _ pretty be _ glad 
# 视图 # text4 . dispersion _ plot citizens democracy 
freedom duties America text3 . generate very # 计数 词汇 
print len text3 # 44764 print sorted set text3 . 
. A Abel Abelmizraim . . . your yourselves youth 
print len set text3 # 2789 print len text3 / 
len set text3 # 16 . 050197203298673 print text3 . 
count smote # 5 print 100 * text4 . count 
a / len text4 # 1 . 4643016433938312 def lexical 
_ diversity text return len text / len set text 
def percentage count total return 100 * count / total 
print lexical _ diversity text3 # 16 . 050197203298673 print 
lexical _ diversity text5 # 7 . 420046158918563 print percentage 
4 5 # 80.0 print percentage text4 . count a 
len text4 # 1 . 4643016433938312 # 1.2 近观 Python 
将 文本 当做 词 链表 # 链表 sent1 = Call 
me Ishamel . print sent1 len sent1 Call me Ishamel 
. 4 print lexical _ diversity sent1 # 1.0 print 
sent2 # The family of Dashwood had long been settled 
in Sussex . print sent3 # In the beginning God 
created the heaven and the earth . l1 = Monty 
Python + and the Holy Grail print l1 # Monty 
Python and the Holy Grail l2 = sent4 + sent1 
print l2 # Fellow Citizens of the Senate and of 
the House of Representatives Call me Ishamel . sent1 . 
append Some print sent1 # Call me Ishamel . Some 
# 索引 列表 print text4 173 # awaken print text4 
. index awaken # 173 print text5 16715 16735 # 
U86 thats why something like gamefly is so good because 
you can actually play a full game without buying it 
print text6 1600 1625 # We re an anarcho syndicalist 
commune . We take it in turns to act as 
a sort of executive officer for the week sent = 
word1 word2 word3 word4 word5 word6 word7 word8 word9 word10 
print sent 0 sent 9 # word1 word10 # print 
sent 10 # IndexError list index out of range print 
sent 5 8 # word6 word7 word8 print sent 5 
# word6 print sent 6 # word7 print sent 7 
# word8 print sent 3 # word1 word2 word3 print 
text2 141565 # themselves or producing coolness between their husbands 
. THE END sent 0 = First sent 9 = 
Last print len sent # 10 sent 1 9 = 
Second Third print sent # First Second Third Last # 
print sent 9 # IndexError list index out of range 
# 变量 sent1 = Call me Ismael . my _ 
sent = Bravely bold Sir Robin rode forth from Camelot 
. noun _ phrase = my _ sent 1 4 
print noun _ phrase # bold Sir Robin wOrDs = 
sorted noun _ phrase print wOrDs # Robin Sir bold 
# 关键字 不能 做 变量 使用 # not = Camelot 
# SyntaxError invalid syntax vocab = set text1 vocab _ 
size = len vocab print vocab _ size # 19317 
# 字符串 name = Monty print name 0 name 0 
\ nname name \ nname 4 name 4 name 0 
M name Monty name 4 Mont print name * 2 
# MontyMonty print name + # Monty print . join 
Monty Python # Monty Python print Monty Python . split 
# Monty Python # 1.3 计算 语言 简单 的 统计 
saying = After all is said and done more is 
than done tokens = set saying tokens = sorted tokens 
print tokens 2 # said than # 概率分布 fdist1 = 
FreqDist text1 print fdist1 # FreqDist with 19317 samples and 
260819 outcomes vocabulary1 = list fdist1 . keys print vocabulary1 
5 # Moby Dick by Herman print fdist1 whale # 
# fdist1 . plot 50 cumulative = True # 常用词 
累计 频率 图 # 细粒度 的 选择 词 V = 
set text1 long _ words = w for w in 
V if len w 15 print sorted long _ words 
CIRCUMNAVIGATION Physiognomically apprehensiveness c a n n i b a 
l i s t i c a l l y 
c h a r a c t e r i 
s t i c a l l y circumnavigating circumnavigation 
c i r c u m n a v i 
g a t i o n s c o m 
p r e h e n s i v e 
n e s s hermaphroditical indiscriminately i n d i 
s p e n s a b l e n 
e s s irresistibleness physiognomically p r e t e 
r n a t u r a l n e 
s s responsibilities simultaneousness s u b t e r 
r a n e o u s n e s 
s supernaturalness s u p e r s t i 
t i o u s n e s s u 
n c o m f o r t a b 
l e n e s s u n c o 
m p r o m i s e d n 
e s s undiscriminating u n i n t e 
r p e n e t r a t i 
n g l y fdist5 = FreqDist text5 l5 = 
sorted w for w in set text5 if len w 
7 and fdist5 w 7 print l5 # 14 19teens 
# talkcity _ adults . . . . . . 
. . Question actually anything computer cute . ass everyone 
football innocent listening remember seriously something together tomorrow watching # 
词语 搭配 与 双 连词 from nltk import bigrams bis 
= bigrams more is said than done print list bis 
# more is is said said than than done text4 
. collocations United States fellow citizens four years years ago 
Federal Government General Government American people Vice President Old World 
Almighty God Fellow citizens Chief Magistrate Chief Justice God bless 
every citizen Indian tribes public debt one another foreign nations 
political parties text8 . collocations would like medium build social 
drinker quiet nights non smoker long term age open Would 
like easy going financially secure fun times similar interests Age 
open weekends away poss rship well presented never married single 
mum permanent relationship slim build # 计算 其他 东西 print 
len w for w in text1 1 4 4 2 
6 . . . 5 7 6 1 fdist = 
FreqDist len w for w in text1 print fdist # 
FreqDist with 19 samples and 260819 outcomes print list fdist 
. keys # 1 4 2 6 8 9 11 
5 7 3 10 12 13 14 16 15 17 
18 20 print fdist . items dict _ items 1 
47933 4 42345 2 38513 6 17111 8 9966 9 
6428 11 1873 5 26597 7 14399 3 50223 10 
3528 12 1053 13 567 14 177 16 22 15 
70 17 12 18 1 20 1 print fdist . 
max # 3 print fdist 3 # 50223 print fdist 
. freq 3 # 0 . 1 9 2 5 
5 8 8 2 4 3 1 8 7 8 
0 4 6 # 1.4 回到 Python 决策 与 控制 
print w for w in sent7 if len w 4 
# 61 old the as a 29 . print w 
for w in sent7 if len w = 4 # 
61 old will join the as a Nov . 29 
. print w for w in sent7 if len w 
= = 4 # will join Nov . print w 
for w in sent7 if len w = 4 # 
Pierre Vinken 61 years old the board as a nonexecutive 
director 29 . print sorted w for w in set 
text1 if w . endswith ableness comfortableness honourableness immutableness i 
n d i s p e n s a b 
l e n e s s indomitableness intolerableness palpableness reasonableness 
u n c o m f o r t a 
b l e n e s s print sorted term 
for term in set text4 if gnt in term # 
Sovereignty sovereignties sovereignty print sorted item for item in set 
text6 if item . istitle A Aaaaaaaaah Aaaaaaaah Aaaaaah Aaaah 
Aaaaugh Aaagh Aaah Aaauggh Aaaugh Aaauugh Aagh Aah Aauuggghhh Aauuugh 
Aauuuuugh Aauuuves Action Actually African Ages Aggh Agh Ah Ahh 
Alice All Allo Almighty Alright . . . Yeah Yes 
You Your Yup Zoot print sorted item for item in 
set sent7 if item . isdigit # 29 61 # 
对 每个 元素 进行 操作 print len w for w 
in text1 # 1 4 4 2 6 . . 
. 5 7 6 1 print w . upper for 
w in text1 # IN THE FORECASTLES OF . . 
. FOUND ANOTHER ORPHAN . print len text1 len set 
text1 len set word . lower for word in text1 
# 260819 19317 17231 print len set word . lower 
for word in text1 if word . isalpha # 16948 
# 嵌套 代码 块 word = cat if len word 
5 print word length is less than 5 else print 
word = 5 word length is less than 5 for 
word in Call me Ishmael . print word Call me 
Ishmael . # 条件 循环 sent1 = Call me Ishmael 
. for xyzzy in sent1 if xyzzy . endswith l 
print xyzzy Call Ishmael for token in sent1 if token 
. islower print token is a lowercase word elif token 
. istitle print token is a titlecase word else print 
token is puncatuation Call is a titlecase word me is 
a lowercase word Ishmael is a titlecase word . is 
puncatuation tricky = sorted w for w in set text2 
if cie in w or cei in w for word 
in tricky print word ancient ceiling conceit . . . 
sufficiently undeceive undeceiving # 1.5 自动 理解 自然语言 # 词意 
消 歧 # 指代 消解 # 自动 生成 语言 自动 
问答 机器翻译 # 机器翻译 from nltk . book import * 
import nltk . misc . babelfish as babelfish babelfish . 
babelize _ shell # 这个 没有 实验 成功 # Babelfish 
online translation service is no longer available . # Babel 
how long before the next flight to Alice Springs # 
Babel german # Babel run # 人机对话 系统 import nltk 
. chat as chat chat . chatbots Which chatbot would 
you like to talk to 1 Eliza psycho babble 2 
Iesha teen anime junky 3 Rude abusive bot 4 Suntsu 
Chinese sayings 5 Zen gems of wisdom Enter a number 
in the range 1 5 1 表 1 2 NLTK 
频率分布 类 中 定义 的 函数 例子 描述 fdist = 
FreqDist samples 创建 包含 给定 样本 的 频率分布 fdist . 
inc sample 增加 样本 fdist monstrous 计数 给定 样本 monstrous 
出现 的 次数 fdist . freq monstrous 给定 样本 monstrous 
的 频率 fdist . N 样本 总数 list fdist . 
keys 以 频率 递减 顺序 排序 的 样本 链表 for 
sample in fdist 以 频率 递减 的 顺序 遍历 样本 
fdist . max 数值 最大 的 样本 fdist . tabulate 
绘制 频率分布 表 fdist . plot 绘制 频率 分布图 fdist 
. plot cumulative = True 绘制 累计 频率 分布图 fdist1 
fdist2 测试 样本 在 fdist1 中 出现 的 频率 是否 
小于 fdist2 表 1 4 词汇 比较 运算符 函数 含义 
s . startswith t 测试 s 是否 以 t 开头 
s . endswith t 测试 s 是否 以 t 结尾 
t in s 测试 s 是否 包含 t s . 
islower 测试 s 中 所有 字符 是否 都是 小写字母 s 
. isupper 测试 s 中 所有 字符 是否 都是 大写字母 
s . isalpha 测试 s 中 所有 字符 是否 都是/nr 
字母 s . isalnum 测试 s 中 所有 字符 是否 
都是/nr 字母 或 数字 s . isdigit 测试 s 中 
所有 字符 是否 都是/nr 数字 s . istitle 测试 s 
是否 首字母 大写 s 中 所有 的 词 都是 首字母 
大写 最近 在看 Analyzing Text with the Natural Language Toolkit 
的 中文 翻译 版本 觉得 蛮 有意思 的 就把 学习 
过程 中 的 遇到 的 问题 和 一些 代码 的 
运行 结果 记录下来 小白 一只 如 有错误 请 您 指正 
谢谢 想要 这本书 资源 的 可以 在 评论 区 留下 
您 的 邮箱 下面 进入 正题 之前 我 已经 装 
好了 Python3 . 6 版本 第 1 章  /nr   语言 
处理 与 Python1 . 1 语言 计算 文本 和 单词 
NLTK 入门 由于 pip 版本 太老 先 右键 管理员 身份 
打开 cmd 根据 提示 输入 python m pip install upgrade 
pip 语句 进行 pip 的 更新 更新 完毕 输入 pip 
install nltk 语句 进行 NLTK 的 安装 安装 完毕 启动 
Python 解释器 在 Python 提示符 后 输入 以 下命令 import 
nltk nltk . download 跳出 以下 界面 选中 book 这 
一行 点击 Download 完成后 出现 如下 界面 关闭 窗口 此时 
数据 已经 被 下载 到 电脑 上 啦 你 可以 
使用 Python 解释器 去 加载 一些 要用 的 文本 首先 
输入 from nltk . book import *   即从 NLTK 
的 book 模块 加载 all 若想 找到 这些 文本 只需 
在 后 输入 它们 的 名字 即可 如 搜索 文本 
text1 . concordance monstrous   即 搜索 白鲸 记 中的 
词 monstrous text2 . concordance affection 即 搜索 理智 与 
情感 中的 词 affection 通过 上述 的 词语 索引 我们 
可以 看到 其 上下文 如 text1 中的 monstrous 我们 可以 
看到 the _ _ _ _ _ pictures 见 下图 
红框 可以 通过 文 本名 . similar 关键词 语句 来 
查看 还有 哪些 词 出现 在 相似 的 上下 文中 
common _ contexts 函数 可以 研究 两个 及 以上 的 
词 的 共同 的 上下文 如 monstrous   和 very 
判断/v 词/n 在/p 文本/n 中/f 的/uj 位置/v 从/p 文本/n 开头/v 
算起/v 在/p 它/r 前面/f 有/v 多少/m 个/q 词/n 位置 信息 
可以 用 离散 图 表示 图中 每一个 竖线 代表 一个 
单词 每 一行 代表 整个 文本 PS 为了 画图 我们 
需要 安装 Python 的 NumPy 的 Matplotlib 包 如果 没有 
安装 就 会 出现 如下 错误 通过 Ctrl + Z 
  退出 Python 解释器 然后 通过 pip install matplotlib 语句 
进行 下载安装 安装 之后 进入 Python 解释器 记得 导入 nltk 
book 模块 的 所有 文本 然后 输入 如下 语句 见 
黄框/nr 然后 弹出 一张 图 关闭 图片 窗口 后 输入 
另一 条 语句 弹出 如下 图片 generate 函数 不再 适用 
暂时 没有 替代 的 函数 如果 有 希望 哪位 大佬 
可以 告知 计数 词汇 使用 len 函数 获取 长度 以 
文本 中 出现 的 词 和 标点符号 为 单位 算出 
文本 从头到尾 的 长度 使用 set 函数 获得 词汇表 使用 
sorted set 文本 名称 得到 一个 词汇 项的/nr 排序 表 
再用 len 来 获得 这个 数值 对 文本 词汇 丰富 
度 进行 测量 需 确保 Python 使用 的 是 浮点 
除法 计数 一个词 在 文本 中 出现 的 次数 计算 
一个 特定 的 词 在 文本 中 占据 的 百分比 
定义 函 数用 关键字 def 定义新 函数 使用 这些 函数 
1.8   练习 1 尝试 使用 Python 解释器 作为 一个 
计算器 输入 表达式 如 12 / 4 + 1 2 
26 个 字母 可以 组成 26 的 10 次方 或者 
26 * * 10 个 10 字母 长 的 字符串 
也 就是 1411 67095653376L 结尾处 的 L 只是 表示 这是 
Python 长 数字 格式 100 个 字母 长度 的 字符串 
可能 有 多少 个 3 Python 乘法 运算 可 应用于 
链表 当 你 输入 Monty Python * 20 或者 3 
* sent1 会 发生 什么 4 复习 1.1 节 关于 
语言 计算 的 内容 在 text2 中 有 多少 个 
词 有 多少 个 不同 的 词 5 比较 表格 
1 1 中 幽默 和 言情小说 的 词汇 多样性 得分 
哪一个 文 体中 词汇 更 丰富 言情小说 的 词汇 更 
丰富 6 制作 理智 与 情感 中 四个 主角 Elinor 
Marianne Edward 和 Willoughby 的 分布图 在 这部 小说 中 
关于 男性 和 女性 所 扮演 的 不同 角色 你 
能 观察 到 什么 你 能 找出 一对 夫妻 吗 
可以 看出 Elinor 和 Marianne 出现 的 频率 很高 且 
近乎 贯穿 全文 所以 我 认为 两者 是 主角 个人 
原以为 Elinor 和 Marianne 是 cp 但 参考 百度 之后 
发现 她俩 是 姐妹 关系 Elinor 和 Edward 才是 cp 
我 猜 是 我 忽视 了 性别 前 两位 是 
女 后 两位 是 男 如果/c 有/v 大佬/nz 知道/v 怎么/r 
从/p 图上/i 看/v 出来/v 的/uj 请/v 告诉/v 我/r 附上 剧情简介 
7 查找 text5 中的 搭配 8 思考 下面 的 Python 
表达式 len set text4 说明 这个 表达式 的 用途 描述 
在 执行 此 计算 中 涉及 的 两个 步骤 这个 
表达式 的 用途 是 统计 text4 文本 中 有 多少 
个 不同 的 标识符 此 计算 涉及 两个 步骤 一是 
获得 text4 的 词汇表 二 是 求取 词汇表 的 长度 
9 复习 1.2 节 关于 链表 和 字符串 的 内容 
a . 定义 一个 字符串 并且 将 它 分配 给 
一个 变量 如 my _ string = My String 在 
字符串 中放 一些 更 有趣 的 东西 用两 种方法 输出 
这个 变量 的 内容 一种 是 通过 简 单地/nr 输入 
变量 的 名称 然后按 回车 另一种 是 通过 使用 print 
语句 b . 尝试 使用 my _ string + my 
_ string 或者 用 它 乘以 一个 数 将 字符串 
添加到 它 自身 例如 my _ string * 3 请注意 
连接 在 一起 的 字符串 之间 没有 空格 怎样 能 
解决 这个 问题 10 使用 的 语法 my _ sent 
= My sent 定义 一个词 链表 变量 my _ sent 
用 你 自己 的 词 或 喜欢 的话 a . 
使用 . join my _ sent 将其 转换 成 一个 
字符串 b . 使用 split 在 你 指定 的 地方 
将 字符串 分 割回 链表 11 定义 几个 包含 词 
链表 的 变量 例如 phrase1 phrase2 等 将 它们 连接 
在 一起 组成 不同 的 组合 使用 加法 运算符 最终 
形成 完整 的 句子 len phrase1 + phrase2 与 len 
phrase1 + len phrase2 之间 的 关系 是 什么 len 
phrase1 + phrase2   = len phrase1 + len phrase2 
12 考虑 下面 两个 具有 相同 值 的 表达式 哪 
一个 在 NLP 中 更常用 为什么 a . Monty Python 
6 12 b . Monty Python 1 b 在 NLP 
中 更常用 因为 NLP 的 操作 是 基于 词汇 的 
13 我们 已经 看到 如何 用词 链表 表示 一个 句子 
其中 每个 词 是 一个 字符 序列 sent1 2 2 
代表 什么意思 为什么 请用 其他 的 索引 值 做 实验 
sent1 2 2 代表 text1 文本 第 一句 的 第三 
个 词汇 的 第三 个 字符 即 h sent5 3 
4   代表 text5 文本 的 第一 句话 的 第四 
个 词汇 的 第五个/nr 字符 即 l 14   在 
变量 sent3 中 保存 的 是 text3 的 第一 句话 
在 sent3 中 the 的 索引 值 是 1 因为 
sent3 1 的 值 是 the sent3 中 the 的 
其它 出现 的 索引 值 是 多少 15 复习 1.4节 
讨论 的 条件 语句 在 聊天 语料库 text5 中 查找 
所有 以 字母 b 开头 的 词 按 字母顺序 显示出来 
16   在 Python 解释器 提示符 下 输入 表达式 range 
10 再 尝试 range 10 20 range 10 20 2 
和 range 20 10 2 在 后续 章节 中 我们 
将 看到 这个 内置 函数 的 多用 用途 17 使用 
text9 . index 查 找词 sunset 的 索引 值 你 
需要 将 这个 词 作为 一个 参数 插入 到 圆括号 
之间 通过 尝试 和 出错 的 过程 中 找到 完整 
的 句子 中 包含 这个词 的 切片 18   使用 
链表 加法 set 和 sorted 操作 计算 句子 sent1 . 
. . sent8 的 词汇表 19   下面 两行 之间 
的 差异 是 什么 哪一个 的 值 比较 大 其他 
文本 也是 同样 情况 吗 sorted set w . lower 
for w in text1 sorted w . lower for w 
in set text1 后者 的 值 较大 因为 后者 还 
包含 一些 大小写 不同 的 单词 20 w . isupper 
和 not w . islower 这 两个 测试 之间 的 
差异 是 什么 前者 是 测试 w 中 所有 字符 
是否 都是 大写字母 后者 是 测试 w 中的 所有 字符 
是否 不 都是 小写字母 即 判断 是否 是 大小写 掺杂 
或者 全是 大写 21 写 一个 切片 表达式 提取 text2 
中 最后 两个 词 22   找出 聊天 语料库 text5 
中 所有 四个 字母 的 词 使用 频率分布 函数 FreqDist 
以 频率 从高 到 低 显示 这些 词 23 复习 
1.4 节 中 条件 循环 的 讨论 使用 for 和 
if 语句 组合 循环 遍历 巨蟒 和 圣杯 text6 的 
电影 剧本 中的 词 输出 所有 的 大 写词 每行 
输出 一个 24 写 表达式 找出 text6 中 所有 符合 
下列 条件 的 词 结果 应该 是 词 链表 的 
形式 word 1 word2 . . . a . 以 
ize 结尾 b . 包含 字母 zc . 包含 字母 
序列 ptd . 除了 首字母 外 是 全部 小写 字母 
的 词 即 titlecase 25 定义 sent 为 词 链表 
she sells sea shells by the sea shore 编写 代码执行 
以下 任务 a . 输出 所有 sh 开头 的 单词 
b . 输出 所有 长度 超过 4 个字符 的 词 
26 下面 的 Python 代码 是 做 什么 的 sum 
len w for w in text1 你 可以 用 它 
来 算 出 一个 文本 的 平均 字长 吗 该 
代码 是 用来 求 文本 中 所有 的 字长 总和 
27 定义 一个 名为 vocab _ size text 的 函数 
以 文本 作为 唯一 的 参数 返回 文本 的 词汇 
量 28 定义 一个 函数 percent word text 计算 一个 
给定 的 词 在 文本 中 出现 的 频率 结果 
以 百分比 表示 29 我们 一直 在 使用 集合 存储 
词汇表 试试 下面 的 Python 表达式 set sent3 set text1 
实验 在 set 中 使用 不同 的 参数 它 是 
做 什么 用 的 你 能 想到 一个 实际 的 
应用 吗 sent3 的 词汇 集合 属于 text1 的 词汇 
集合 set a set b 用来 判断 a 词汇 集合 
是否 为 b 词汇 集合 的 子集 Word2vec 在 自然 
语言 词 向量 中 基本 原理 和 应用 的 研究 
云南大学 软件 学院 软件工程 班 云南昆明 650500 朱宏 摘 要 
现代 计算机 自然 语言 的 智能 识别 技术 成为 了 
时代 发展 的 重要 方向 可以 说 智能化 的 生活 
已经 走入 了 大众 的 生活 本文 研究 了 Word2vec 
在 自然 语言 的 单词 向量 的 基本 原理 和 
应用 Word2vec 是 Google 公司 在 2013年 在 研究 自然 
语言 的 识别 模型 的 背景 下 所 推出 的 
软件工具 本文 探讨 了 自然 语言 的 智能 识别 的 
里程碑 并且 重点 研究 现在 最 为 成熟 的 Word2vec 
所 采用 的 CBOW 和 Skip gram 模型 对于 自然 
语言 的 智能 识别 的 作用 和 这两种 模型 的 
异同 最后 本文 还 探讨 了 Word2vec 对于 自然 语言 
智能 识别 的 应用 关键词 自然语言   智能 识别   
Word2vec   CBOW   Skip gram1 .   引言 计算机 
在 现代 社会 各个 方面 中 起着 必不可少 的 作用 
以 计算机 智能 识别 应用 技术 为 核心 的 自然 
语言 智能 识别 更是 飞速发展 慢慢地 走入 我们 大众 的 
视线 智能化 的 普及 是 着力 于 解放 人工 传统 
的 工作 方式 提高效率 甚至 达 成 传统 人工 所 
不能 完成 的 工作 人们 所处 的 时代 每天 产生 
海量 的 数据 如何 高效 的 利用 这些 数据 就 
依赖 智能化 的 工作 方式 本文 主要 探究 了 自然 
语言 的 智能 识别 如何 让 计算机 能够 正确 的 
识别 和 处理 自然语言 消除 语言 歧义 致力于 代替 人工 
的 工作 模式 是 自然 语言 智能 识别 的 一大 
核心 研究 方向 而且 自然 语言 的 智能 识别 从 
基于 规则 的 转变 到 了 基于 统计 的 转变 
本文 探究 了 自然 语言 智能 识别 的 发展 历程 
和 重点 介绍 了 Google 公司 2003年 推出 的 Word2vec 
的 基本 原理 和 实际 应用 其中 又 主要 介绍 
了 Word2vec 的 CBOW 和 Skip gram 模型 的 基本 
原理 和 实现 方式 最后 总结 了 自然 语言 智能 
识别 的 重要性 和 发展 前景 2 .   自然语言 
智能 识别 的 发展 2.1 统计 语言 模型 2 . 
1.1 语言 句子 的 二元 模型 对于 计算机 来说 怎样 
判断 一个 句子 是否 合理 是 自然 语言 智能 识别 
的 重点 对于 计算机 需要 建立 相应 的 语言 模型 
才能 很好 的 识别 自然语言 统计 语言 模型 是 一个 
概率分布 就是 已知 一个 词 出现 的 条件 下 前后 
位置 出现 的 词 的 概率 公式 P w1 w2 
. . . wn = P w1 P w2 | 
w1 P w3 | w1 w2 . . . P 
wn | w1 w2 . . . wn − 1 
其中 wi 是 一句话 的 第 i 个 词 P 
表示 条件概率 后来 俄国 科学家 马尔科夫 假设 该 词 出现 
的 概率 只 与 前面 出现 的 词 相关 这 
就是 二元 模型 于是 公式 变成 P w1 w2 . 
. . wn = P w1 P w2 | w1 
P w3 | w2 . . . P wn | 
wn − 1 利用 贝叶斯 公式 最后 得出 计算 单词 
出现 合理 的 概率 公式 P wi − 1 wi 
= N wi − 1 wi / N wi − 
1 2.1 . 2N gram 模型 基于 上面 的 二元 
模型 假定 文本 中 的 每一个 词 wiwi 和 前面 
的 N 1个 词 有关 而与 更 前面 的 词 
无关 这样 当前 词 wiwi 的 概率 只 取决于 前面 
N 1个 词 P wi − N + 1 wi 
− N + 2 . . . wi − 1 
P wi − N + 1 wi − N + 
2 . . . wi − 1 因此 P wi 
| w1 w2 . . . wi − 1 = 
P wi | wi − N + 1 wi − 
N + 2 . . . wi − 1 P 
wi | w1 w2 . . . wi − 1 
= P wi | wi − N + 1 wi 
− N + 2 . . . wi − 1 
上式 对应 的 就是 N 元 模型 N Gram Model 
不过 这样 做 的 弊端 就是 当 N 3时 可能 
词 的 相关性 就 不是 那么 大了 计算结果 准确性 差 
. 2.2 词 向量 2 . 2.1 词 向量 简 
介词 向量 就是 给 每个 单词 用 纬度 的 方式 
去 描述 把 词语 抽象 出来 交给 计算机 处理 的 
数据结构 每个/r 词/n 都有/nr 自己/r 的/uj 定位/n 空间/n 和/c 大小/b 
可以 进行 比较 和 加减 获得 词语 相似 度 传统 
的 单词 特征 的 抽取 把 单词 作为 一个 不 
可以 再 分 基本 组成部分 认为 不 相同 的 符号 
之间 没有 任何 的 关系 比如 计算机 和 电脑 被 
认为 两 个 没有 关系 的 字符串 但是 这样 显然 
不 符合 我们 人类 的 思维 由此 而生 人们 提出 
了 空间 词 向量 的 方法 来 解决 这个 问题 
空间/n 词/n 向量/n 可以/c 从/p 大量/n 的/uj 数据/n 中/f 统计/v 
和/c 训练/vn 得到/v 1954年 Harris 提出 分布式 表示 的 假说 
利用 词语 的 上下文 相似 假说 该词 相似 上下文 相似 
的 词 向量 来自于 在 大量 的 语料 中 利用 
神经网络 模型 训练 得来 使用 最 广泛 的 是 Word2vec 
使用 的 连续 词 袋 模型 CBOW Continuous Bag Of 
Words 和 Skip gram 模型 其他 公开 的 模型 还有 
HLBL SENNA Turian s Huang s Glove 等 Word2vec 在 
语义上 有 较好 的 联系性 所以 Word2vec 的 大部分 测试 
指标 较为 准确 2 . 2.2 One hot representation 矩阵 
One hot representation 矩阵 就是 每 一个 词 在 一个 
N 维 的 数组 中有 唯一 的 标识 每一个 单词 
有 唯一 的 id 其 余纬度 标识 成0./nr 例如 苹果 
0 0 0 1 0 0 0 0 0 那么 
一句话 就是 由 多维 的 数组 组成 便 构成 了 
矩阵 这么做 的 好处 是 处理 维度 越高 的 自然 
语言 可以用 线性 方法 来 表示 和 计算 但是 同时 
也 会 造成 纬度 灾难 和 语义 关联性 差 的 
不良 后果 由于 数据 稀疏 可能 出现 语义 关联度 为 
0 的 情况 2 . 2.3 Distributed representation 为了 解决 
One hot representation 的 问题 采用 分布式 的 表示 方法 
就 会 更好 分布式 表示 就是 利用 多 维度 来 
表示 词 的 属性 例如 苹果 2 0 7 1 
0 0 8 0 0 这样 的 表达 方式 可以 
大大 减少 词 向量 的 维度 解决 了 Distributed representation 
语义 关 联为 0 的 问题 并且 更好 的 多 
方面 的 描述 了 词 向量 的 特征 但是/c 同时/c 
也/d 没有/v 从/p 根本/a 上/f 解决/v 维度/ns 灾难/n 的/uj 问题/n 
和/c 数据/n 稀疏/a 的/uj 问题/n 2.3 神经 网络 语言 模型 
语言 模型 是 计算 词语 在 语句 中 出现 的 
概率 来 统计 自然 语言 的 正确性 是 NLP 研究 
的 基础 方向 神经 网络 语言 模型 的 发展 经过 
了 很长 的 时间 期间 取得 了 很大 的 研究 
成果 开始 由 Bengio 等人 提出 NNLM Neutwork language model 
最为 被 人们 认可 这个 模型 为 之后 的 模型 
研究 打下 了 坚实 的 基础 其 模型 见 图一 
图一 神经 网络 语言 模型 NNLM 根据 语料库 C 生成 
对应 的 词汇表 V V 中的 每 一个 词汇 对应 
一个 编号 i 将 语料库 作为 样本 输入 经过 不断 
的 模型 优化 出现 了 现在 人们 应用 最为 广泛 
的 相对于 NNLM 简单 的 连续 词 袋 模型 CBOW 
Continuous Bag Of Words 和 Skip gram 模型 在 训练 
方法 上 也 出现 了 Hierarchical Softmax 等 优秀 的 
算法 本文 主要 介绍 的 是 Word2vec 所 使用 的 
连续 词 袋 模型 CBOW Continuous Bag Of Words 和 
Skip gram 模型 3 .   Word2vec 的 核心 架构 
模型 3 . 1Word2vec 简介 2013年 Google 开源 了 一款 
用于 词 向量 计算 的 工具 word2vec 引起 了 业界 
的 极大 关注 他 的 强大 在于 可以 在 百万 
级 的 词典 和 上亿 的 数据 上 快速 的 
进行 训练 最终 得到 词 向量 并且/c 能/v 很好/i 地/uv 
体现/v 出/v 词语/n 之间/f 的/uj 相关性/l 和/c 相似性/n Google 开源 
的 Word2vec 使用 的 就是 现在 最 有 权威 的 
连续 词 袋 模型 CBOW Continuous Bag Of Words 和 
Skip gram 模型 Word2vec 是 一款 软件 不是 一种 训练 
算法 这款 强大 的 软件 的 开源 对于 自然 语言 
的 智能 研究 起 着 很大 的 作用 也 为 
我们 省 去了 搭建 模型 的 时间 目前 也 有 
很多 的 编程语言 API 实现 Word2vec 的 功能 比如 python/w 
和/c java/w 中/f 都/d 可以/c 调用/vn 相应/v 的/uj 包来/nr 实现/v 
文本/n 的/uj 模型/n 训练/vn 和词/nr 向量/n 的/uj 统计/v Word2vec 的 
训练 框架 为 如 图二 图二 Word2vec 的 训练 框架 
3.2 深度 神经网络 模型 深度 神经网络 模型 是 一个 具有 
多层 感知机 的 模型 在 未来 出现 上 千层 的 
深度 神经网络 模型 也 不 奇怪 深度 神经网络 模型 在 
自然 语言 的 处理 中 取得 了 很大 的 成效 
深度 神经网络 模型 在 词性 识别 命名 识别 和语 快 
的 识别 中 应用 广泛 深度 神经网络 模型 如 图一 
所示 图三 深度 神经网络 模型 图 一中 的 深度 神经网络 
模型 由 输出 层 隐藏 层 和 输入 层 组成 
输入 层 是 最底层 对应 上下文 的 特征 结合 状态 
转移 概率 求得 句子 的 最佳 标注 序列 3 . 
3CBOW 模型 CBOW 模型 的 核心 思想 就是 上下文 来 
确定 该词 的 出现 概率 来 匹配 词语 的 相似 
度 可以 设定 上下文 影响 的 距离 度 上下文 对于 
该词 的 影响 力度 是 一样 的 之所以 叫 词 
袋 模型 就是 从 袋子 中 取出 词语 取出 的 
顺序 对于 结果 没有 影响 但是 要求 就是 要 有 
足够 的 词语 来 训练 在 大量 的 训练 资料 
中的 出 我们 所 需要 的 特殊 实例 这也 是 
深度 学习 的 方向 不再 要求 具体 的 特定 模型 
而是 统计模型 CBOW 模型 主要 包含 了 输入 层 投影 
层 输出 层 当前 词语 词语 上下文 上下文 累加 和 
从 输入 层 输入 已经 数据化 的 词 向量 在 
投影 层 对 这些 向量 求 累加 和 输出 层 
对应 一个 二叉树 各 词 出现 次数 的当 权值 构造 
的 赫夫曼 树 叶子 节点 对应 词典 中的 词 从而 
构成 模型 CBOW 的 模型 复杂度 为 Q = N 
* D + D * | V | 其中 N 
为 输入 层 的 窗口 长度 D 为 发 射层 
纬度 | V | 为 训练 语料 的 词典 大小 
不同 的 词语 个数 复杂度 就是 简单 的 矩阵 计算 
主要 的 计算 量 为 D * | V | 
CBOW 的 模型 示意图 如 图二 图 四 CBOW 模型 
示意图 其中 输入 层 就是 从词/nr 袋中 取出 的 词语 
W i 中的 i 表示 取出 的 词语 相对于 目标 
词语 的 位置 3.4 Skip gram 模型 Skip gram 模型 
的 提出 很好 的 解决 了 语料 经过 模型 训练 
后 怎样 体现出 语义 的 问题 在 统计 语言 模型 
的 应用 中 大量 的 语料 是 确定 语义 相似性 
的 基本 保证 语言 的 重复 次数 决定了 语义 模仿 
的 程度 模型 的 模仿 程度 才 会 更高 其次 
处理 方式 也 是 影响 最 终模型 的 学习 度 
的 一大 重要 因素 如果 断章取义 那/r 最后/f 结果/n 可能/v 
会/v 和/c 我们/r 的/uj 理解/v 偏差/n 很大/a 而 Skip gram 
模型 的 提出 很好 的 解决 这些 问题 Skip gram 
模型 是 允许 跳过 几个 词 来 确定 上下文 的 
对应 关系 中间 跳过 的 词数 就在 一定 程度 上 
解决 了 断章取义 的 问题 因为 Skip gram 模型 识别 
的 不全是 相邻 词 的 出现 概率 更多 是 附近 
的 词 出现 的 概率 Skip Gram 模型 的 图 
与 CBOW 相反 CBOW 是从 原始 语句 推测 目标 字词 
Skip Gram 是从 目标 字词 推测 原始 语句 CBOW 适合 
小型 数据库 而 Skip Gram 在 大型 数据 中 更加 
合适 N gram 中 N = 2 和N=/nr 3 的 
实例 如图 四 图 五 N gram 中 N = 
2 和N=/nr 3 实例 最终 效果 是 中国 足球 踢得 
太 烂了 可以 看到 这种 表达 方式 是 能够 体现 
出 这个 语义 的 同时 也 可以 看出 N 的 
取值 不同 效果 也会有 差异 一般 N 3 以上 效果 
会 有 偏差 所以 一般 N 取值 不大于 3 的 
Skip gram 模型 如图 五 图 六 Skip gram 模型 
4 .   Word2vec 在 语言 智能 识别 的 应用 
4 . 1Word2vec 的 应用 领域 自然语言 处理 技术 在 
语音 智 能上 的 应用 主要 包括 同声 传译 智能 
机械 的 聊天 以及 特定 人群 的 辅助 系统 等 
方面 特别 是 同声 传译 涉及 到 包括 语音 建模 
合理 的 语义 转换 及 语言 的 精确 翻译 等 
是 自然 语言 处理 技术 应用 的 直接 领悟 同时 
同声 传译 中 的 语言 翻译 还 要求 语音 和 
语义 的 转化 即 音 似 字 如何 从 特定 
的 语言 情景 中 译出 这种 情况 通过 自然 语言 
处理 技术 对 其 进行 算法 验证 和 语义 的 
情景 化 处理 从而 提高 语言 语义 的 转换 质量 
智能 机械 的 聊天 如 机器人 的 聊天 系统 也是 
语音 智能 的 一个 应用 分支 它 主要 处理 的 
是 比较 广泛 的 如 自动 回答 互动 对话 系统 
在 这样 的 一个 语音 智能 中 机器人 聊 天 
不可避免 地 要 具有 语音 和 文本 的 转化 以及 
逻辑 规范性 的 自动 答话 通过 对 智能 识别 自然语言 
处理 技术 的 应用 做到 在 语义上 的 理解 逻辑 
的 正确 推断 和 具体 知识 的 应用 等 得到 
质 的 提高 进而 将 应答 互动 和 聊天 的 
准确性 提高 到 一个 较 被 普遍 认可 的 程度 
增强 机器人 之类 的 智能 机器 系统 在 实际 中 
的 应用 而 自然 语言 处理 技术 在 自动 场景 
和 特定 人群 提供 辅助 方面 的 应用 将为 一些 
特定 的 需要 帮助 的 群体 如 盲人 提供 便利 
的 帮助 帮助 他们 在 生活 中 解决 一些 诸如 
交通 出行 的 智能 提示 方面 的 困难 对 他们 
来说 将 是 非常 必要 的 所以 基于 智能 识别 
的 自然 语言 处理 技术 对于 实际 的 语音 智能 
方面 的 应用 将会 帮助 社会 生活 的 方方面面 各个 
领域 的 群体 它 的 价值 在 不远 的 将来 
是 不可估量 的 因为 智能 识别 的 自然 语言 处理 
技术 是 一个 多 学科 交叉 多 技术 应用 多 
领域 合作 的 一个 技术 整合 4 . 1Word2vec 工作 
目录 Google 官方 推出 的 软件包 工作 目录 如图 七 
图 七 Word2vec 的 工作 目录 其中 demo word . 
sh 是 词库 训练 模型 的 各项 参数 text8 是 
自动 采用 的 训练样本 可 自行 设定 自己 的 训练 
集 Distance . c 可以 计算 词 向量 的 余弦 
值 在 Linux 的 系统 下 可以 在 命令提示符 接 
运行 demo word . sh 进行 使用 Window 系统 需要 
安装 Linux 的 虚拟环境 4 . 2Word2vec 的 训练 参数 
Word2vec 的 训练 参数表 如图 八 图 八 Word2vec 的 
训练 参数 4 . 3Google Word2vec 软件包 的 使用 1 
去 Google 官网 或者 相关 平台 上 下载 Word2vec 的 
文件夹 包 图 九 word2vec 包 2 打开 Linux 运行 
环境 3 切换 到 trunk 目录 下 执行 make 命令 
图 十 切换 到 trunk 工作 目录 4 执行 demo 
word . sh 并 下载 再带 的 text8 训练 集 
或者 自己 的 分词 训练 集 图 十一 下载 自带 
的 text8 训练 集 5 加载 完毕 输入 单词 6 
得到 输出 结果 图 十二 Word2vec 输出 结果 5 . 
  结语 相比 于 上个 世纪 的 两次 工业革命 计算机 
智能 时代 来 的 更为 快速 对 我们 的 生活 
也 起到 了 很大 的 提高 作用 在 这个 充满 
智能化 的 时代 自然 语言 的 智能 识别 是 尤为 
重要 的 研究 方向 也 是 我们 需要 全力 突破 
的 瓶颈 Google 推出 的 Word2vec 是 代表 现在 最为 
先进 的 自然 语言 处理 技术 实用性 比较 大 Word2vec 
应用 了 CBOW 和 Skip gram 模型 结合 使用 了 
hierarchy softmax 和 negative sampling 等 优化 技术 在 自然 
语言 智能化 表达 方面 起 了 非常 大 的 推动 
作用 在 未来 的 自然 语言 的 智能 识别 还会有 
长足 的 进步 同时 也 是 值得 研究 的 方向 
和 值得 涉足 的 领域 参考文献   李翠霞 . 现代 
计算机 智能 识别 技术 处理 自然 语言 研究 的 应用 
与 进展 A . 中国 知网 . 2012   周练./nr 
Word2vec 的 工作 原理 及 应用 探究 A . 中国 
知网 . 2015 . 吴军 . 数学 之美 M . 
北京 人民邮电出版社 . 2012   Tomas Mikolov . Word2vec project 
EB / OL . 2014 09 18 . https / 
/ code . google . com / p / word2vec 
/ . 维基百科 . 语言 模型 OL . 2013 3 
12   一夜 了 . 自然语言 处理 统计 语言 模型 
数学 之美 . CSDN 博客 . 2017   Huang Xian 
ying Chen Hong yang Liu Ying tao et al . 
A novel feature word selecting method of micro blog short 
text J . Computer Engineering & Science 2015 37 9 
1761 1767 . in Chinese  /i 涂/v 楚成./nr 基于/p CUDA/w 
的/uj Word2Vec/i 设计/vn 与/p 实现/v D 西安 西安电子科技大学 2015 . 
12.29   刘培 磊 唐晋韬/nr 王挺 谢松县/nr 岳 大鹏 刘海 
池./nr 基于 词 向量 语义 聚 类 的 微博 热点 
挖掘 方法 J . 计算机 工程 与 科学 2018 40 
02 313 319 .   Mikolov T Yih W Zweig 
G . Linguistic regularities in continuous space word representations C 
∥ Proc of NAACLHLT 13 2013 746 751 .   
路 佳佳 . 神经 网络 语言 模型 D 山西 山西大学 
2017 . 2.9   周练./nr Word2vec 的 工作 原理 及 
应用 探究 A . 中国 知网 . 2015 . 吴军 
. 数学 之美 M . 北京 人民邮电出版社 . 2012   
Frederic M . Yoshua B . Hierarchical probabilistic neural network 
language model C / / Proceedings of the International Work 
shop on Artificial Intelligence and Statistics . Barbados MIT Press 
. 2005 246 252   周练./nr Word2vec 的 工作 原理 
及 应用 探究 A . 中国 知网 . 2015 . 
吴军 . 数学 之美 M . 北京 人民邮电出版社 . 2012 
  游飞/nr 张激/nr 邱定/nr 于铭华/nr ./i 基于/p 深度/ns 神经/n 网络/n 
的/uj 武器/n 名称/n 识别/v J . 计算机 系统 应用 2018 
27 01 239 243 .   熊 富林 邓怡豪/nr 唐晓 
晟 . Word2vec 的 核心 架构 及其 应用 . 中国 
知网 . 2015   许莹./nr 赫夫曼 树 遍历 算法 的 
优化 D 安徽 安徽 电子 信息 职业 技术 学院 2009 
  熊 富林 邓怡豪/nr 唐晓 晟 . Word2vec 的 核心 
架构 及其 应用 . 中国 知网 . 2015   熊 
富林 邓怡豪/nr 唐晓 晟 . Word2vec 的 核心 架构 及其 
应用 . 中国 知网 . 2015   李翠霞 . 现代 
计算机 智能 识别 技术 处理 自然 语言 研究 的 应用 
与 进展 J . 科学技术 与 工程 2012 12 36 
9912 9918 .   周练./nr Word2vec 的 工作 原理 及 
应用 探究 A . 中国 知网 . 2015 . 吴军 
. 数学 之美 M . 北京 人民邮电出版社 . 2012 欢迎 
关注 天善智能 我们 是 专注 于 商业智能 BI 人工智能 AI 
大 数据 分析 与 挖掘 领域 的 垂直 社区 学习 
问答 求职 一站式 搞定 对 商业智能 BI 大 数据分析 挖掘 
机器学习 python R 等 数据 领域 感兴趣 的 同学 加 
微信 tstoutiao 邀请 你 进入 数据 爱好者 交流 群 数据 
爱好者 们 都 在这儿 作者 黄 天元 复旦 大学 博士 
在读 目前 研究 涉及 文本 挖掘 社交 网络 分析 和 
机器 学习 等 希望 与 大家 分享 学习 经验 推广 
并 加深 R 语言 在 业界 的 应用 邮箱 huang 
. tian yuan @ qq . com 原理 简介 在 
之前 的 文章 中 R 语言 自然语言 处理 中文分词 介绍 
了 如何 利用 jiebaR 来做 中文分词 这次 希望 研究 如果 
利用 R 语言 来做 词性 标注 并 利用 标注 来做 
命名 实体 识别 首先 需要 明确 词性 标注 的 概念 
就是 要 把 中文分词 后的/nr 每一个 词 确定 其 性质 
是 名词 动词 还是 形容词 如果 是 名词 是 人名 
地名 还是 机构 团体 名称 对 这些 词性 进行 更为 
细致 的 标注 有助于 我们 对 信息 进行 提取 有的 
时候 动词 和 形容词 其实 不 包含 我们 感兴趣 的 
信息 但是 名词 却 非常 重要 此外 也 有利于 我们 
了解 作者 的 用词 习惯 这个 时候 名词 又 不一定 
重要 了 一个人 的 行文 习惯 可以 体现 在 他 
经常 用 的 动词 和 形容词 因为 我们 是 用 
jiebaR 来做 分词 根据 官方 文档 说明 它 的 标注 
是 根据 北大 人民日报 语料库 进行 训练 的 最后 的 
标准 整理 为 ICTPOS3 . 0 词性 标记 集 内容 
如下 n   名词 nr   人名 nr1   汉语 
姓氏 nr2   汉语 名字 nrj   日语 人名 nrf 
  音译 人名 ns   地名 nsf   音译 地名 
nt   机构 团体 名 nz   其它 专名 nl 
  名 词性 惯用语 ng   名 词性 语素 t 
  时间词 tg   时间 词性 语素 s   处所词 
f   方位词 v   动词 vd   副 动词 
vn   名 动词 vshi   动词 是 vyou   
动词 有 vf   趋向 动词 vx   形式 动词 
vi   不 及物动词 内 动词 vl/w  /i 动/v 词性/n 
惯用语/n vg/w  /i 动/v 词性/n 语素/n a/w  /i 形容词/n ad/w 
 /i 副/b 形/n 词/n an/w  /i 名/q 形/n 词/n ag/w 
 /i 形容/n 词性/n 语素/n al/w  /i 形容/n 词性/n 惯用语/n b/w 
 /i 区别词/n bl/w  /i 区别/n 词性/n 惯用语/n z/w  /i 状态词/n 
r/w  /i 代词/n rr/w  /i 人称代词/i rz/w  /i 指示代词/i rzt/w 
 /i 时间/n 指示代词/i rzs/w  /i 处所/v 指示代词/i rzv/w  /i 谓/v 
词性/n 指示代词/i ry/w  /i 疑问代词/i ryt/w  /i 时间/n 疑问代词/i rys/w 
 /i 处所/v 疑问代词/i ryv/w  /i 谓/v 词性/n 疑问代词/i rg/w  /i 
代/q 词性/n 语素/n m/w  /i 数词/n mq/w  /i 数量词/n q/w 
 /i 量词/n qv/w  /i 动量词/i qt/w  /i 时/n 量词/n 词性/n 
标注/v 实践/v 话/n 不多/m 说/v 我们 上 代码 来做 词性 
标注 分析 需要 注意 的 是 我们 要 做 词性 
标注 的 输入 既 可以 是 一大 段 没有 经过 
分词 处理 字符串 也 可以 是 已经 分词 完毕 的 
分词 结果 也 就是 字符 向量 我们 先 介绍 第 
一种 情况 就是 没有 经过 分词 的 大段 字符串 要 
完成 分词 然后 对 每个 词 都 进行 词性 标注 
1library pacman 2p _ load jiebaR tidyverse 34cn   = 
  我 想 写 一 本书 名字 叫做 R 语言 
高效 数据处理       # 构造 中文 文本 5tag 
_ worker   =   worker type   =   
tag         # 构造 分词 标注 器 
67tag _ result   =   tagging cn tag _ 
worker       # 进行 分词 标注 89tag _ 
result                   
      # 查看 结果 10 # #   
                  r 
                    
v                   
  v                 
    m               
      r             
        n11 # #       
        我           
    想               
写               一   
        本书           
名字 12 # #             
        v           
      eng             
        a           
          n13 # #     
      叫做         R 语言 
          高效   数据处理 我们 得到 
的 tag _ result 实质上 是 一个 带 属性 的 
向量 这样 其实 不是 特别 好用 因此 我 要把 它 
变成 数据 框 的 格式 方便 以后 利用 1str tag 
_ result     # 查看 数据类型 2 # # 
    Named   chr   1 10   我 
  想   写   一   本书   名字 
  叫做   R 语言   . . . 3 
# #       attr *   names = 
  chr   1 10   r   v   
v   m   . . . 4enframe tag _ 
result     tag _ table     # 转换 
数据 存储 格式 56tag _ table7 # #   # 
  A   tibble   10   x   28 
# #         name     value9 
# #         chr   chr 10 
# #     1   r       
    我 11 # #     2   
v           想 12 # # 
    3   v           
写 13 # #     4   m   
        一 14 # #     
5   r           本书 15 
# #     6   n       
    名字 16 # #     7   
v           叫做 17 # # 
    8   eng       R 语言 
18 # #     9   a     
      高效 19 # #   10   
n           数据处理 其实 这里 分词 
效果 还 不是 那么 尽如人意 因为 本书 应该 分为 本 
书 而 这里 被 认定 为 代词 指代 之前 提过 
的 一 本书 然而 我 并 没有 指代 任何 词 
不过 大体 来说 还 算 满意 注意 R 语言 之所以 
能够 被 分出来 是 因为 我 上次 处理 加了 用户 
词库 因此 这次 自动 地 进行 了 识别 如果 大家 
没有 把 R 语言 加入 到 用户 自定义 词库 中 
你们 看到 的 应该 是 R 语言 关于 如何 定义 
用户 词库 见 上一 篇文章 R 语言 自然语言 处理 中文分词 
如果 已经 分词 完毕 需要 对 这些 词 进行 词性 
标注 可以 使用 vector _ tag 函数 我们 先 按照 
正常 流程 进行 分词 1 # 正常 分词 流程 23worker 
    wk4segment cn wk     seg _ cn56seg 
_ cn7 # #     1   我   
            想       
        写           
    一               
本书           名字 8 # # 
    7   叫做           
R 语言         高效       
    数据处理 然后 我们 利用 函数 进行 标注 1vector 
_ tag seg _ cn tag _ worker 2 # 
#                   
  r                 
    v               
      v             
        m           
          r         
            n3 # #   
            我       
        想           
    写               
一           本书       
    名字 4 # #         
            v       
          eng         
            a       
              n5 # # 
          叫做         
R 语言           高效   数据处理 
这个 结构 与 我们 上面 得到 的 tag _ result 
是 一致 的 命名 实体 识别 尝试 现在 我们 尝试 
用 词性 标注 的 方法 来 进行 命名 实体 识别 
我们 的 目的 是 对于 既定 的 一套 字符串 我们 
希望 得到 里面 的 名词 因为 我们 认为 它 会 
代表 一些 实际 的 实体 对象 我 非常 喜欢 一篇 
文章 是 王小波 的 一只 特立独行 的 猪 原谅 我 
的 任性 我 要把 这篇文章 直接 放在 这里 作为 我们 
的 中文 语料 对象 1cn   =   插队 的 
时候 我 喂过 猪 也 放过 牛 假如 没有人 来 
管 这两种 动物 也 完全 知道 该 怎样 生活 它们 
会 自由自在 地 闲逛 饥 则 食 渴 则 饮 
春天 来临 时 还要 谈谈 爱情 这样一来 它们 的 生活 
层次 很低 完全 乏善可陈 人 来了 以后 给 它们 的 
生活 做出 了 安排 每一/i 头牛/n 和每/nr 一口/m 猪/n 的/uj 
生活/vn 都/d 有了/i 主题/n 就 它们 中 的 大多数 而言 
这种 生活 主题 是 很 悲惨 的 前者 的 主题 
是 干活 后者 的 主题 是 长肉 我 不 认为 
这 有 什么 可 抱怨 的 因为 我 当时 的 
生活 也 不见得 丰富 了 多少 除了 八个 样板戏 也 
没有 什么 消遣 有 极少数 的 猪 和牛 它们 的 
生活 另有 安排 以 猪 为例 种猪 和 母猪 除了 
吃 还有 别的 事 可 干 就 我 所见 它们 
对 这些 安排 也 不大 喜欢 种猪 的 任务 是 
交配 换言之 我们 的 政策 准许 它 当个 花花公子 但是 
疲惫 的 种猪 往往 摆出 一种 肉猪 肉猪 是 阉过 
的 才 有的 正人君子 架势 死活 不肯 跳到 母猪 背上去 
母猪 的 任务 是 生 崽儿 但 有些 母猪 却 
要把 猪崽儿 吃掉 总的来说 人 的 安排 使 猪 痛苦不堪 
但 它们 还是 接受 了 猪 总是 猪 啊 2对 
生活 做 种种 设置 是 人 特有 的 品性 不光 
是 设置 动物 也 设置 自己 我们 知道 在 古希腊 
有个 斯巴达 那里 的 生活 被 设置 得 了无生趣 其 
目的 就是 要 使 男人 成为 亡命 战士 使 女人 
成为 生育 机器 前者 像 些 斗鸡 后者 像 些 
母猪 这 两类 动物 是 很 特别 的 但 我 
以为 它们 肯定 不 喜欢 自己 的 生活 但 不 
喜欢 又能 怎么样 人 也好 动物 也罢 都 很难 改变 
自己 的 命运 3 以下 谈到 的 一只 猪 有些 
与众不同 我 喂猪 时 它 已经 有 四五 岁了 从 
名分 上 说 它 是 肉猪 但 长得 又 黑 
又瘦 两眼 炯炯 有光 这家伙 像 山羊 一样 敏捷 一米 
高的/nr 猪栏 一 跳 就过 它 还能 跳上 猪圈 的 
房顶 这 一点 又 像是 猫 所以 它 总是 到处 
游逛 根本 就 不在 圈里 呆着 所有 喂过 猪 的 
知青 都把 它 当 宠儿 来 对待 它 也是 我 
的 宠儿 因为 它 只对 知青 好 容许 他们 走到 
三 米 之内 要是 别的 人 它 早就 跑了 它 
是 公的/nr 原本 该 劁 掉 不过 你 去 试试 
看 哪怕 你 把 劁 猪 刀 藏在 身后 它 
也能 嗅 出来 朝 你 瞪大 眼睛 噢噢 地 吼起来 
我 总是 用 细 米糠 熬 的 粥 喂 它 
等 它 吃 够了 以后 才 把 糠 对 到 
野草 里 喂 别的 猪 其他 猪 看了 嫉妒 一起 
嚷起来 这时候 整个 猪场 一片 鬼哭狼嚎 但/c 我/r 和它都/nr 不在乎/l 
吃饱 了 以后 它 就 跳上 房顶 去 晒太阳 或者 
模仿 各种 声音 它 会 学 汽车 响 拖拉机 响 
学得 都 很像 有时 整天 不见踪影 我 估计 它 到 
附近 的 村寨 里 找 母猪 去了 我们 这里 也有 
母猪 都 关在 圈里 被 过度 的 生育 搞得 走了 
形 又 脏 又 臭 它 对 它们 不 感兴趣 
村 寨里 的 母猪 好看 一些 它 有 很多 精彩 
的 事迹 但 我 喂猪 的 时间 短 知道 得 
有限 索性 就不 写了 总而言之 所有 喂过 猪 的 知青 
都 喜欢 它 喜欢 它 特立独行 的 派头 儿 还 
说 它 活得 潇洒 但 老乡 们 就不 这么 浪漫 
他们 说 这 猪 不正经 领导 则 痛恨 它 这 
一点 以后 还要 谈到 我 对 它 则 不止 是 
喜欢 我 尊敬 它 常常 不顾 自己 虚 长 十几岁 
这一 现实 把 它 叫做 猪 兄 如前所述 这位 猪 
兄 会 模仿 各种 声音 我 想 它 也学 过人 
说话 但 没有 学会 假如 学会 了 我们 就 可以 
做 倾心 之谈 但这 不能 怪 它 人和 猪 的 
音色 差得 太远 了 4 后来 猪 兄 学会 了 
汽笛 叫 这个 本领 给 它 招来 了 麻烦 我们 
那里 有座 糖厂 中午 要 鸣 一次 汽笛 让 工人 
换班 我们 队 下地 干活 时 听见 这次 汽笛 响 
就 收工 回来 我 的 猪 兄 每天 上午 十点钟 
总要 跳到 房 上学 汽笛 地里 的 人 听见 它 
叫 就回来 这 可比 糖厂 鸣笛 早 了 一个 半小时 
坦白 地 说 这 不能 全怪猪/nr 兄 它 毕竟 不是 
锅炉 叫 起来 和 汽笛 还 有些 区别 但 老乡 
们 却 硬说 听不出来 领导 上 因此 开 了 一个 
会 把 它 定 成了 破坏 春耕 的 坏分子 要对 
它 采取 专政 手段 会议 的 精神 我 已经 知道 
了 但 我 不为 它 担忧 因为 假如 专政 是 
指 绳索 和 杀猪刀 的话 那是 一点 门 都 没有 
的 以前 的 领导 也 不是 没 试过 一百 人也 
治 不住 它 狗 也 没用 猪 兄 跑 起来 
像 颗 鱼雷 能把 狗 撞出 一丈 开外 谁知 这回 
是 动了 真格的 指导员 带了 二 十几 个人 手拿 五四式 
手枪 副 指导员 带了 十几人 手持 看青 的 火枪 分两路 
在 猪 场外 的 空地 上 兜捕 它 这 就使 
我 陷入 了 内心 的 矛盾 按 我 和它的/nr 交情 
我 该 舞起 两把 杀猪刀 冲出去 和它/nr 并肩战斗 但 我 
又 觉得 这样 做 太过 惊世骇俗 它 毕竟 是 只 
猪 啊 还有 一个 理由 我 不敢 对抗 领导 我 
怀疑 这 才是 问题 之 所在 总之 我 在 一边 
看着 猪 兄 的 镇定 使 我 佩服 之极 它 
很 冷静 地 躲在 手枪 和 火枪 的 连线 之内 
任凭 人 喊 狗咬 不离 那条 线 这样 拿 手枪 
的 人 开火 就 会把 拿 火枪 的 打死 反之亦然 
两头 同时 开火 两头 都会 被 打死 至于 它 因 
为 目标 小 多半 没事 就 这样 连兜了/nr 几个 圈子 
它 找到 了 一个 空子 一头 撞 出去 了 跑得 
潇洒 之极 以后 我 在 甘蔗 地里 还 见过 它 
一次 它 长 出了 獠牙 还 认识 我 但 已 
不容 我 走近 了 这种 冷淡 使 我 痛心 但 
我 也 赞成 它 对 心怀叵测 的 人 保持 距离 
5 我 已经 四十 岁了 除了 这只 猪 还没 见过 
谁敢 于 如此 无视 对 生活 的 设置 相反 我 
倒 见过 很多 想 要 设置 别人 生活 的 人 
还有 对 被 设置 的 生活 安之若素 的 人 因为 
这个 原故 我 一直 怀念 这只 特立独行 的 猪 现在 
我 想 识别 这篇文章 里面 所有 的 名词 1tagging cn 
tag _ worker   % % 2     enframe 
  % % 3     filter name   = 
=   n     tag _ names 现在 我 
把 文中 的 名词 都 筛选 了 出来 词性 的 
列名 称为 name 词语 的 列名 称为 value 我 要 
统计 一下 王小波 在 这篇文章 中用 到 名词 的 词频 
1tag _ names   % % 2     count 
value   % %       # 对 名词 
进行 计数 3     arrange desc n     
  # 降序 排列 4 # #   #   
A   tibble   113   x   25 # 
#         value         
  n6 # #         chr   
int 7 # #     1   猪   
            178 # #   
  2   人             
  129 # #     3   母猪   
          810 # #     
4   汽笛             511 
# #     5   动物       
      412 # #     6   
领导             413 # # 
    7   主题           
  414 # #     8   狗   
              315 # # 
    9   火枪           
  316 # #   10   牛  /nr     
        317 # #   #   
. . .   with   103   more   
rows 有意思 猪 是 出现 最多 的 名词 其次是 人 
再到 母猪 实际 运用 中 想必 还是 会 有 很多 
障碍 大家 要 记得 在 用户 自定义 词库 中 我们 
是 可以 给 词性 进行 标注 的 也 就是 我们 
的 词 想要 识别 成 什么 我们 自己 可以 说了算 
这在 垂直 领域 的 运用 中 是 相当 有用 的 
至于 应该 如何 设置 标注 大家 可以 观察 原始 词库 
的 格式 然后 对 文本 文件 进行 修饰 原始 文件 
的 位置 在 哪里 请 直接 键入 DICTPATH 你 会 
找到 路径 然后 用 文本格式 来 查看 这个 文件 即可 
然后 按照 相应 格式 来 更改 用户 词典 同 一个 
文件 目 录下 的 user . dict . utf8 我 
还是 认为 算法 是 不 可能 超越 词库 的 多在 
词库 下功夫 算法 才 能够 发挥 效用 应该 想方设法 构建 
更加 优秀 的 自定义 词库 并 进行 面向 业务 的 
精准 标注 才 能够 在 实际 应用 中 获得 好 
的 效果 往 期 精彩 R 语言 自然语言 处理 中文分词 
找 工作 难 面试 失败 的 核心 原因 已经 找到 
R 语言 中文 社区 2018 年终 文章 整理 作者 篇 
R 语言 中文 社区 2018 年终 文章 整理 类型 篇 
公众/n 号/m 后台/n 回复/v 关键字/n 即可/d 学习/v 回复/v  /i 爬虫/n 
 /i  /i  /i  /i  /i  /i  /i  /i  /i  /i 
 /i  /i 爬虫/n 三大/mq 案例/n 实战/v 回复/v  /i Python/w  /i 
 /i  /i  /i  /i  /i  /i 1/m 小时/n 破冰/v 入门/ns 
回复/v  /i 数据挖掘/n  /i  /i  /i  /i  /i R/w 语言/n 
入门/ns 及/c 数据挖掘/n 回复/v  /i 人工智能/n  /i  /i  /i  /i 
 /i 三个月/i 入门/ns 人工智能/n 回复/v  /i 数据/n 分析/vn 师  /nr 数据/n 
分析/vn 师/ng 成长/v 之路/r 回复/v  /i 机器学习/i  /i  /i  /i 
 /i  /i 机器学习/i 的/uj 商业应用/i 回复/v  /i 数据/n 科学/n  /i 
 /i  /i  /i  /i 数据/n 科学/n 实战/v 回复/v  /i 常用/b 
算法/n  /i     常用 数据挖掘 算法 人工智能 Artificial Intelligence 
AI 它 是 研究 开发 用于 模拟 延伸 和 扩展 
人 的 智能 的 理论 方法 技术 及 应用 系统 
的 一门 新的 技术 科学 以 人类 智能 相似 的 
方式 做出 反应 的 智能 机器 该 领域 的 研究 
包括 机器人 语言识别 图像识别 自然语言 处理 和 专家 系统 等 
人工智能 = 大 数据 + 深度 学习 应用 场景 机器 
视觉 指纹识别 人脸识别 视网膜 识别 虹膜 识别 掌纹 识别 专家系统 
自动 规划 智能 搜索 定理证明 博弈 自动 程序设计 智能控制 机器人学 
语言 和 图像 理解 遗传 编程 等 神经网络 神经元 神经网络 
分层 结构 人工神经网络 Artificial Neural Network 即 ANN 从 信息 
处理 角度 对 人脑 神经元网络 进行 抽象 建立 某种 简单 
模型 按 不同 的 连接 方式 组成 不同 的 网络 
神经 网络 是 一种 运算 模型 由 大量 的 节点 
或称 神经元 之间 相互 联接 构成 每个 节点 代表 一种 
特定 的 输出 函数 称为 激励函数 activation function 每 两个 
节点 间 的 连接 都 代表 一个 对于 通过 该 
连接 信号 的 加权 值 称之为 权重 这 相当于 人工神经网络 
的 记忆 网络 的 输出 则 依 网络 的 连接 
方式 权重 值 和 激励 函数 的 不同 而 不同 
而 网络 自身 通常 都是 对 自然界 某种 算法 或者 
函数 的 逼近 也 可能 是 对 一种 逻辑 策略 
的 表达 深度 学习 深度 学习 是 指 多层 神经 
网络 上 运用 各种 机器学习 算法 解决 图像 文本 等 
各种 问题 的 算法 集合 深度 学习 的 核心 是 
特征 学习 旨在 通过 分层 网络 获取 分 层次 的 
特征 信息 从而 解决 以往 需要 人工 设计 特征 的 
重要 难题 深度 学习 是 一个 框架 包含 多个 重要 
算法 大 数据 无法 在 一定 时间 范围 内 用 
常规 软件工具 进行 捕捉 管理 和 处理 的 数据 集合 
是 需要 新 处理 模式 才能 具有 更强 的 决策力 
洞察 发现 力 和 流程 优化 能力 的 海量 高/a 
增长率/n 和/c 多样化/l 的/uj 信息/n 资产/n 大 数据 的 5V 
特点 IBM 提出 Volume 大量 Velocity 高速 Variety 多样 Value 
低 价值 密度 Veracity 真实性 大 数据 Big data 通常 
用来 形容 一个 公司 创造 的 大量 非 结构化 数据 
和半/nr 结构化 数据 这些/r 数据/n 在/p 下载/v 到/v 关系型/b 数据库/n 
用于/v 分析/vn 时会/nr 花费/n 过/ug 多/m 时间/n 和/c 金钱/n 大/a 
数据分析/l 常和/nr 云计算/i 联系/n 到/v 一起/m 因为 实时 的 大型 
数据集 分析 需要 像 MapReduce 一样 的 框架 来向 数十 
数百 或 甚至 数千 的 电脑 分配工作 弱 人工智能 单一 
任务 的 计算 导航 汽车 ABS 强 人工智能 人类 多任务 
多任务 复杂 任务 处理 构成 理解 神经元网络 input = { 
每个 神经元 激活 函数 @ 单层 网络 } * 多层 
= 加权 求和 = 求 偏见 = output 相关 概念 
目标函数 梯度 下降 反向 传播 NLP 的 开发 环境 搭建 
主要 分为 以下 几步 Python 安装 NLTK 系统 安装 Python3 
. 5 下载安装 下载 链接 https / / www . 
python . org / downloads / release / python 354 
/ 安装 步骤 双击 下载 好 的 python3 . 5 
的 安装包 如 下图 选择 默认 安装 还是 自定义 安装 
一般 默认 安装 就好 直接 跳到 步骤 5 自定义 的 
接 着看 步骤 3 PS Add Python3 . 5 to 
PATH 勾 选上 免去 再去 配置 环境变量 的 麻烦 选择 
一些 需要 的 设置 勾选/nr 一些 高级 选项 等待 安装 
完成 安装 结束 测试 安装 是否 成功 控制台 输入 python 
出现 下列 提示 则 表示 安装 成功 NLTK 系统 安装 
利用 pip 安装 控制台 输入 pip install nltk 则会 开始 
安装 可能 安装 速度 会 很慢 耐心 等待 因为 笔者 
已经 安装 过 所以 提示 已经 安装 PS 安装 过程 
中 可能 会 报错 这 是 由于 需要 安装 依赖 
包 把 报错 信息 中 的 依赖 包 安装 上 
之后 再 继续 安装 NLTK 就 可以 了 测试 安装 
是否 成功 打开 控制台 进入 python 环境 中 之后 导入 
nltk 包 下载 nltk 数据包 等待 所有 数据包 缓存 结束 
之后 环境 就 搭建 完成 了 是不是 So easy 人工智能 
自然语言 处理 NLP 是 一种 新颖 的 人机 交互方式 也 
就是 让 计算机 去 分析 和 理解 人类 的 语言 
实现 从 自然语言 到 机器语言 的 转换 不同于 人与 人 
之间 基于 长久以来 约定俗成 的 社会 情境 的 交流 计算机 
与 人类 的 对话 有 更多 的 限制 比如 自然 
语言 中 的 歧义 有时候 人类 自身 也会在 理解 上 
出现 偏差 更 遑论 是 计算机 为了 准确 了解 人类 
的 语言 也 为了 更加 流畅 的 交流 计算机 不得不 
去 汇集 大 规模 的 数据 以此 来 减少 失误 
以上 是 关于 人工智能 自然语言 处理 的 粗略 介绍 下面 
谈论 NLP 在 文学 方面 的 应用 作为 文学系 的 
一员 长期以来 我 一直 在 同 文字 打交道 比如说 学习 
我国 的 语言 文字 不同 时期 的 文学 作品 以及 
外国 的 作家 作品 等等 这些/r 都/d 需要/v 大量/n 的/uj 
阅读/v 和/c 写作/v 而 这些 一般 在 计算机 上 进行 
于是 NLP 能够 帮助 我 的 大概 就是 缩减 一下 
工作 量了 文字 的 阅读 于我/nr 而言 是 长久 而 
连续 的 工作 阅读 的 开始 需要 选取 一 本书 
或者 一份 资料 但 由于 出版社 的 不同 和 名字 
的 重叠 往往 会 出现 错误 为了 避免 这种 情况 
人 需要 进行 一定 的 筛选 尽管 工作 量 不大 
还是 有些 消耗 时间 NLP 可以 包揽 这项 工作 只 
需要 输入 名字 和 相应 的 信息 就 可以 很快 
的 找到 需要 的 书 或者 资料 比如 网上 图书馆 
的 信息 检索 功能 就是 NLP 应用 在 现实 生活 
的 一个 表现 另外 一方面 当在 阅读 过程 中 遇到 
感兴趣 的 名词 或者 句 子时 也能 通过 信息检索 快速 
而 有效 的 寻找 到 相应 的 信息 这 在 
生活 中 应用 得 更为 广泛 这里 提 一下 NLP 
的 问答 系统 现在/t 大部分/m 软件/n 和/c 电子/n 设备/vn 上/f 
都/d 设有/v 小/a 机器人/n 当 工作 人员 不 在时 仍然 
可以 与 用户 进行 对话 在 解决 问题 上 也 
不算 鸡肋 网上 图书馆 也 有 这种 小 机器人 可以 
解决 一些 简单 的 事情 比如 某 种 书籍 是否 
在 馆 能否 借阅 之类 的 小事 有时候 因为 没有 
充裕 的 时间 我 不能 将 文字 完全 阅览 但又 
需要 马上 了解 文字 所 表达 的 内容 NLP 同样 
可以 派上用场 它 能使 计算机 在 研读 文 字后 自动 
提取 其 核心 内容 并 生成 相应 的 信息 反馈 
给 我 这样 的 信息 通常 简明扼要 能够 让 我 
短 时间 内 了解 文本 的 内容 如果 文字 看 
累了 现在 的 阅读 软件 都会 提供 语音 朗读 服务 
只需要 点开 就 会 出现 人声 为 你 朗读 让 
眼睛 休息 一会 与 这 相 联系 的 由于 经常 
需要 写作 要求 的 字数 也挺 多 经常 写到 一半 
就 不想 再 打字 了 这种 时候 会 通过 语音输入 
来 实现 机器 能够 识别 你 的 语言 将 语音 
转变 为 文本 进行 呈现 上述 两种 情形 都与 NLP 
的 应用 有 关系 关于 写作 NLP 还有 其他 令人 
愉快 的 应用 手写体 和 印刷体 的 识别 这个 我 
用得 不多 大多 时候 是 为了 搜索 不清楚 发音 的 
繁体字 有时候 也 会用 印刷体 识别 的 功能 将 图片 
上 的 文字 转变为 文本 文字 方便 修改 当 输入 
一个 汉字 或者 英文字母 时 输入 列表 里 往往 会 
出现 一些 相关 的 文字 有的 恰好 是 你 接下来 
需要 写 下来 的 同时 依照 你 输入 的 文字 
的 长短 列 表里 的 内容 也 会 不断 变换 
与 你 想要 输进去 的 文字 越发 契合 因为 每个人 
的 常用 文字 不尽相同 相应 的 每个人 的 电子 设备 
上 出现 的 联想 性 文字 也 存在 区别 对于 
经常 需要 写作 的 人 来说 NLP 的 这个 功能 
是 最 实用 的 除此之外 值得一提的是 NLP 的 机器 翻译 
功能 在对 外国 文学 的 学习 上 有 很大 效用 
外国 对 他们 本国 文学 的 研究 大体上 要比 我国 
学者 更 深刻 和 全面 再者 多 了解 一个 消息 
总归 不是 坏事 新鲜 出炉 的 学者 论文 是 不大 
容易 找到 翻译 的 哪怕 是 寻到 了 可靠 的 
翻译 人员 这 也 不是 一项 轻松 的 活儿 NLP 
解决 了 这个 困扰 通过 机器翻译 我 可以 及时 到 
了解 外国文学 新的 研究 动向 讯息 流通 更加 便捷 文本 
分类 也是 NLP 的 一个 应用 比较 常见 的 是 
其中 的 情感 分析 给出 几个 文本 计算机 会 自动 
为 你 归类 例如 依照 文本 所 表现 出来 的 
情绪 分为 积极 和 消极 这个 分类 标准 可以 依据 
个人 需要 去 删改 当 接收 文本 过多 人工 分类 
太麻烦 的 时候 可以 用到 我 通常 是 阅读 的 
时候 使用 因为 习惯 一次性 下载 许多 文章 又/d 不想/v 
费/v 时间/n 一个/m 个/q 翻阅/v 后/f 归类/vn 就 干脆 交由 
计算机 识别 了 以上 是 我 个人 对于 人工智能 自然语言 
处理 在 文学 方面 应用 的 总结 如果 表述 存在 
问题 或者 不足之处 还请 多多 指正 非常感谢 对于 从事 统计 
自然语言 处理 来说 了解 概率论 信息论 以及 语言学 知识 都是 
很有 必要 的 下面 内容 主要 介绍 了 在 统计 
自然语言 处理 中 需要 了解 的 概率论 基础 概率 如果 
P A 作为 事件 A 的 概率 Ω 是 试验 
的 样本空间 则 概率函数 满足 下面 三条 公理 非 负性 
P A = 0 规范性 P Ω = 1 可 
列 可加 性 对于 不 相交 的 集合 Aj ∈ 
F 条件概率 和 独立性 假设 事件 B 的 概率 已知 
那么 事件 A 发生 的 条件 概率 为 P B 
0 在 统计 自然语言 处理 中 上面 那个 链式法则 很有 
用处 比如 推导 马尔可夫 模型 的 性质 贝叶斯 定 理由 
条件概率 和 链式 规则 推得 右边 的 分母 P A 
可以 看作 是 归一化 常数 以 保证 其 满足 概率函数 
的 性质 如果 我们 感兴趣 的 仅仅 是 事件 发生 
的 相对 可能性 这时 可以 忽略 分母 随机变量 设 X 
为 一 离散 型 随机变量 其 全部 可能 的 值 
为 { a1 a2 } 那么 pi = P X 
= ai i = 1 2 称为 X 的 概率函数 
P X = x = F x x ∈ R 
称为 X 的 分布 函数 期望/v 和/c 方差/n 联合/v 分布/v 
和/c 条件分布/i 设/v 两个/m 离散/v 随机变量/l X/w 和Y/nr 它们 的 
联合 密度 函数 可以 写 为 描述 其中 单个 随机变量 
的 概率密度函数 称为 边缘 密度 函数 标准 分布 离散 分布 
函数 二项分布 重复 一个 只有 两种 输出 的 实验 并且 
每 次 实验 之间 相互 独立 时 我们 认为 实验 
结果 服从 二项分布 例如 抛 硬币 实验 在 自然 语言 
处理 中 语料库 中的 句子 间 肯定 不是 完全 相互 
独立 的 但是 为了 简化 问题 的 复杂性 我们 通常 
可能会 做 独立性 假设 假设 一个 句子 的 出现 独立 
于它/nr 前面 的 其他 句子 近似 认为 它们 服从 二项分布 
当 实验 有 两个 以上 结果 时 二项分布 问题 就 
转化 为 多项式 分布 multi nomial distribution 连续分布 函数 正态分布 
博主 github https / / github . com / MichaelBeechan 
博主 CSDN https / / blog . csdn . net 
/ u011344545 = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = 概念 篇 https / 
/ blog . csdn . net / u011344545 / article 
/ details / 89525801 技术篇 https / / blog . 
csdn . net / u011344545 / article / details / 
89526149 人才篇 https / / blog . csdn . net 
/ u011344545 / article / details / 89556941 应用 篇 
https / / blog . csdn . net / u011344545 
/ article / details / 89574915 下载 链接 https / 
/ download . csdn . net / download / u011344545 
/ 11147085 = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = 清华 AMiner 团队 AMiner 
. org 摘要 自然语言 处理 是 人工智能 的 一个 重要 
应用 领域 也是 新一代 计算机 必须 研究 的 课题 它 
的 主要 目的 是 克服 人机对话 中 的 各种 限制 
使 用户 能用/nr 自己 的 语言 与 计算机 对话 1 
自然语言 处理 概念 自然 语言 是 指 汉语 英语 法语 
等 人们 日常 使用 的 语言 是 自然而然 的 随着 
人类 社会 发展 演变 而来 的 语言 而 不是 人造 
的 语言 它 是 人类 学习 生活 的 重要 工具 
概括 说来 自然 语言 是 指 人类 社会 约定俗成 的 
区别于 如 程序 设计 的 语言 的 人工 语言 在 
整个 人类 历史 上 以 语言 文字 形式 记载 和 
流传 的 知识 占 到 知识 总量 的 80% 以上 
就 计算机 应用 而言 据统计 用于 数学 计算 的 仅占 
10% 用于 过程 控制 的 不到 5% 其余 85% 左右 
都是/nr 用于 语言 文字 的 信息 处理 处理 包含 理解 
转化 生成 等 过程 自然语言 处理 是 指用 计算机 对 
自然 语言 的 形 音 义 等 信息 进行 处理 
即对 字 词 句 篇章 的 输入 输出 识别 分析 
理解 生成 等 的 操作 和 加工 实现 人机 间 
的 信息 交流 是 人工智能 界 计算机科学 和 语言 学界 
所 共同 关注 的 重要 问题 自然语言 处理 的 具体 
表现 形式 包括 机器翻译 文本 摘要 文本 分类 文本校对 信息 
抽取 语音合成 语音 识别 等 可以 说 自然语言 处理 就是 
要 计算机 理解 自然语言 自然语言 处理 机制 涉及 两个 流程 
包括 自然 语言 理解 和 自然 语言 生成 自然语言 理解 
是 指 计算机 能够 理解 自然语言 文本 的 意义 自然语言 
生成 则是 指 能以 自然语言 文本 来 表达 给定 的 
意图 自然 语言 的 理解 和 分析 是 一个 层次化 
的 过程 许多 语言学家 把这 一 过程 分为 五 个 
层次 可以 更好 地 体现 语言 本身 的 构成 五个 
层次分 别是 语音 分析 词 法分析 句法分析 语义分析 和 语用分析 
在 人工智能 领域 或者 是 语音 信息 处理 领域 中 
学者 们 普遍 认为 采用 图灵 试验 可以 判断 计算机 
是否 理解 了 某种 自然语言 具体 的 判别 标准 有 
以下 几条 第一 问答 机器人 能 正确 回答 输入 文本 
中 的 有关 问题 第二 文摘 生成 机器 有 能力 
生成 输入 文本 的 摘要 第三 释义 机器 能用 不同 
的 词语 和 句型 来 复述 其 输入 的 文本 
第四 翻译 机 器具 有把 一种 语言 翻译 成 另一种 
语言 的 能力 2 自然语言 处理 发展 历程 自然语言 处理 
是 包括 了 计算机 科学 语言学 心理 认知 学 等 
一系列 学科 的 一门 交叉学科 这些 学科 性质 不同 但 
又 彼此 相互 交叉 因此 梳理 自然语言 处理 的 发展 
历程 对于 我们 更好 地 了解 自然语言 处理 这 一 
学科 有着 重要 的 意义 1950 年 图灵 提出 了 
著名 的 图灵测试 这 一般 被 认为 是 自然 语言 
处理 思想 的 开端 20 世纪 50 年代 到 70 
年代 自然语言 处理 主要 采用 基于 规则 的 方法 研究 
人员 们 认为 自然语言 处理 的 过程 和 人类 学习 
认知 一门 语言 的 过程 是 类似 的 所以 大量 
的 研究员 基于 这个 观点 来 进行 研究 这时 的 
自然 语言 处理 停留在 理性主义 思潮 阶段 以 基于 规则 
的 方法 为 代表 但是 基于 规则 的 方法 具有 
不可避免 的 缺点 首先 规则 不 可能 覆盖 所有 语句 
其次 这种 方法 对 开发者 的 要求 极高 开发者 不仅 
要 精通 计算机 还要 精通 语言学 因此 这一 阶段 虽然 
解决 了 一些 简单 的 问题 但是 无法 从 根本 
上 将 自然 语言 理解 实用化 70 年代 以后 随着 
互联网 的 高速 发展 丰富 的 语料库 成为 现实 以及 
硬件 不断更新 完善 自然语言 处理 思潮 由 理性主义 向 经验主义 
过渡 基于 统计 的 方法 逐渐 代替 了 基于 规则 
的 方法 贾里 尼克 和他/nr 领导 的 IBM 华生 实验室 
是 推动 这 一 转变 的 关键 他们 采用 基于 
统计 的 方法 将 当时 的 语音 识别 率 从 
70% 提升到 90% 在 这一 阶段 自然语言 处理 基于 数学模型 
和 统计 的 方法 取得 了 实质性 的 突破 从 
实验室 走向 实际应用 从 2008 年到/nr 现在 在 图像 识别 
和 语音 识别 领域 的 成果 激励 下 人们 也 
逐渐 开始 引入 深度 学习 来做 自然语言 处理 研究 由 
最初 的 词 向 量到 2013 年的/nr word2vec 将 深度 
学习 与 自然 语言 处理 的 结合 推向 了 高潮 
并在 机器翻译 问答 系统 阅读 理解 等 领域 取得 了 
一定 成功 深度 学习 是 一个 多 层 的 神经 
网络 从 输入 层 开始 经过 逐层 非线性 的 变化 
得到 输出 从 输入 到 输出 做 端 到 端的 
训练 把 输入 到 输出 对 的 数据 准备好 设计 
并 训练 一个 神经 网络 即 可执行 预想 的 任务 
RNN 已经 是 自然 语言 护理 最 常用 的 方法 
之一 GRU LSTM 等 模型 相继 引发 了 一轮 又 
一轮 的 热潮 3 我国 自然语言 处理 现状 20 世纪 
90 年代 以来 中国 自然语言 处理 研究 进入 了 高速 
发展 期 一 系列 系统 开始 了 大规模 的 商品化 
进程 自然语言 处理 在 研究 内容 和 应用 领域 上 
不断 创新 目前 自然语言 处理 的 研究 可以 分为 基础性 
研究 和 应用 性 研究 两 部分 语音 和 文本 
是 两类 研究 的 重点 基础性 研究 主要 涉及 语言学 
数学 计算机 学科 等 领域 相/v 对应/vn 的/uj 技术/n 有/v 
消除歧义/i 语法 形式化 等 应用 性 研究 则 主要 集中 
在 一些 应用 自然语言 处理 的 领域 例如 信息检索 文本 
分类 机器 翻译 等 由于 我国 基础理论 即 机器 翻译 
的 研究 起步 较早 且 基础理论 研究 是 任何 应用 
的 理论 基础 所以 语法 句法 语义分析 等 基础性 研究 
历来 是 研究 的 重点 而且 随着 互联网 网络 技术 
的 发展 智能 检索 类 研究 近年来 也 逐渐 升温 
从 研究 周期 来看 除 语言 资源库 建设 以外 自然语言 
处理 技术 的 开发 周期 普遍 较短 基本 为 1 
3 年 由于 涉及 到 自然 语言 文本 的 采集 
存储 检索 统计 等 语言 资源库 的 建设 较为 困难 
搭建 周期 较长 一般 在 10 年 左右 例如 北京大学 
计算 语言所 完成 的 现代 汉语语法 信息 词典 以及 人民日报 
的 标注 语料库 都 经历 了 10 年 左右 的 
时间 才 研制成功 自然语言 处理 的 快速 发展 离不开 国家 
的 支持 这些 支持 包括 各种 扶持 政策 和 资金 
资助 国家 的 资金 资助 包括 国家 自然科学 基金 社会 
科学 基金 863 项目 973 项目 等 其中 国家 自然科学 
基金 是 国家 投入 资金 最多 资助 项目 最多 的 
一项 国家 自然科学 基金 在 基础理论 研究 方面 的 投入 
较大 对 中文 的 词汇 句子 篇章 分析 方面 的 
研究 都 给予 了 资助 同时 在 技术 方面 也 
给予 了 大力 支持 例如 机器翻译 信息检索 自动 文摘 等 
除了 国家 的 资金 资助 外 一些 企业 也 进行 
了 资助 但 是 企业 资助 项目 一般 集中 在 
应用 领域 针对性 强 往往 这些 项目 开发 周期 较短 
更容易 推向市场 实现 由 理论 成果 向 产品 的 转化 
4 自然语言 处理 业界 发展 微软 亚洲 研究院 GoogleFacebook 百度 
阿里巴巴 腾讯 京东 科大 讯 飞 自然语言 处理 是 计算机 
科学 领域 与 人工智能 领域 中 的 一个 重要 方向 
它/r 研究/vn 能/v 实现/v 人/n 与/p 计算机/n 之间/f 用/p 自然/d 
语言/n 进行/v 有效/a 通信/l 的/uj 各种/r 理论/n 和/c 方法/n 自然语言 
处理 是 一门 融 语言学 计算机科学 数学 于 一体 的 
科学 因此 这一 领域 的 研究 将 涉及 自然语言 即 
人们 日常 使用 的 语言 所以 它 与 语言学 的 
研究 有着 密切 的 联系 但又 有 重要 的 区别 
自然语言 处理 并 不是 一般 地 研究 自然语言 而在于 研制 
能 有效 地 实现 自然 语言 通信 的 计算机 系统 
特别 是 其中 的 软件 系统 因而 它 是 计算机 
科学 的 一部分 自然语言 处理 NLP 是 计算机 科学 人工智能 
语言学 关注 计算机 和 人类 自然 语言 之间 的 相互 
作用 的 领域 自然语言 处理 是 一门 沟通 的 科学 
但 不是 人 与人 之间 的 沟通 而是 人 与 
计算机 之间 的 沟通 而且 不是 传统 计算 的 输入 
数据 算式 得到 结果 而是 输入 条件 库 由 关系 
分析 得到 结果 它 最早 来自 机器翻译 最早 的 库 
是 很小 的 像 词典 那样 由/p A/w 国/n 词汇/n 
与/p B/w 国/n 词汇/n 组成/v 有/v 一对一/m 一对多 多对 一等 
联系 输入 A 国 文字 得到 一个 或 多个 检索 
条目 它 的 能力 随着 库 的 完善 条目 联系 
的 增多 而 增强 这是 与 传统 的 数学 计算 
里 给定 的 算式 输入 一定 数字 得到 一个 结果 
完全 不 一样 的 自然语言 处理 是 非常 有 开创性 
的 它 为人 与 计算机 之间 的 交流 开发 了 
一条 新 道路 这种 思路 为 现在 的 大 数据 
的 发展 打下 了 基础 虽然 在 当时 无法 实现 
对 极大 数据 的 处理 库 也被 限制 在 字典 
词条 范围 之内 但 在对 它 的 研究 在 它 
的 发展 中 人们 认识到 了 模糊 输入 歧义 不规范 
输入 造成 的 问题 它们 是 长期 困扰 人们 的 
问题 也 使 人们 在 解决 了 这些 独特 事件 
之后 对 计算 科学 有了/nr 进一步 的 推动 我 的 
专业 是 应用 化学 专业 这是 一门 讲究 实践 的 
实验 科学 但 它 与 自然 语言 处理 存在 者 
深刻 的 联系 应用 化学 是 一个 理学 分支 所有/b 
理学/n 都/d 建立/v 于/p 数学/n 之上/f 而 数学 则 是 
自然 语言 处理 的 基础 在 新 时代 里 自然语言 
处理 不 局限于 词典 的 制作 它 被 广泛 的 
应用 在 各学 科里 包括 应用化学 利用 它 所 制作 
的 化学 实验 库 你 能 输入 一个 物质 就能 
了解 到 它 的 性质 常见 的 实验 反应 集群 
输入 一个 反应 你 能 得到 反应 内 所有 物质 
的 性质 所有 同类 反应 大量 相似 反应 你 也 
可以 只凭 某 少数 特征 来 检索 拥有 这些 特征 
的 所有 物质 通过 某些 现象 检索 到 某 类型 
反应 它 的 库 来自 大量 化学 实验室 真实 实验 
的 积累 结果 可靠 而且 丰富 可以 说 是 关于 
化学 应用 实验 的 大百科 自然语言 处理 是 一门 计算 
学科 但 更是 一种 求知 的 思想 直到 现在 我们 
都还/nr 在为 它 的 发展 中 遇到 的 某些 问题 
而 头疼 但 那 并不 阻止 我们 去 实现 它 
相关 视频 资料 下载 见 https / / blog . 
csdn . net / qwxwaty / article / details / 
80800701 阶段 一 人工智能 基础 － 高等数学 必 知 必会 
本 阶段 主要 从 数据分析 概率论/n 和/c 线性代数/l 及/c 矩阵/n 
和凸/nr 优化/vn 这/r 四大/l 块/zg 讲解/v 基础/n 旨在 训练 大家 
逻辑 能力 分析 能力 拥有 良好 的 数学 基础 有利于 
大家 在 后续 课程 的 学习 中 更好 的 理解 
机器学习 和 深度 学习 的 相关 算法 内容 同时 对于 
AI 研究 尤为重要 例如 人工智能 中的 智能 很大 一部分 依托 
概率论 实现 的 一 数据分析 1 常数 e2 导数 3 
梯度 4 Taylor5 gini 系数 6 信息熵 与 组合 数 
7 梯度 下降 8 牛顿 法二/nr 概率论 1 微积分 与 
逼近 论 2 极限 微分 积分 基本概念 3 利用 逼近 
的 思想 理解 微分 利用 积分 的 方式 理解 概率 
4 概率论 基础 5 古典 模型 6 常见 概率分布 7 
大数定理 和 中心 极限 定理 8 协方差 矩阵 和 相关 
系数 9 最大/a 似/d 然/c 估计/v 和/c 最大/a 后验/nr 估计/v 
三/m 线性代数 及 矩阵 1 线性空间 及 线性变换 2 矩阵 
的 基本 概念 3 状态 转移 矩阵 4 特征向量 5 
矩阵 的 相关 乘法 6 矩阵 的 QR 分解 7 
对称矩阵 正交矩阵 正定矩阵 8 矩阵 的 SVD 分解 9 矩阵 
的 求导 10 矩阵 映射 / 投影 四 凸 优化 
1 凸 优化 基本概念 2 凸 集 3 凸函数 4 
凸 优化 问题 标准 形式 5 凸 优化 之 Lagerange 
对偶 化 6 凸 优化 之 牛顿 法 梯度 下 
降法 求解 阶段 二 人工智能 提升 － Python 高级 应用 
随着 AI 时代 的 到来 以及 其 日益 蓬勃 的 
发展 Python 作为 AI 时代 的 头牌 语言 地位 基本 
确定 机器学习 是 着实 令人 兴奋 但 其 复杂度 及 
难度 较大 通常 会 涉及 组装 工作流 和 管道 设置/vn 
数据源/n 及/c 内部/f 和云/nr 部署/n 之间/f 的/uj 分流/n 而/c 有了/i 
Python/w 库/n 后/f 可 帮助 加快 数据 管道 且 Python 
库 也在 不断更新 发布 中 所以 本 阶段 旨在 为 
大家 学习 后续 的 机器学习 减负 一 容器 1 列表 
list2 元组 tuple3 字典 dict4 数组 Array5 切片 6 列表 
推导 式 7 浅 拷贝 和深/nr 拷贝 二 函数 1 
lambda 表达式 2 递归函数 及 尾递归 优化 3 常用 内置 
函数 / 高阶 函数 4 项目 案例 约瑟夫 环 问题 
三 常用 库 1 时间 库 2 并发 库 3 
科学计算 库 4 Matplotlib 可视化 绘 图库 5 锁 和 
线程 6 多线程 编程 阶段 三 人工智能 实用 － 机器学习 
篇 机器学习 利用 算法 去 分析 数据 学习 数据 随后 
对 现实 世界 情况 作出 判断 和 预测 因此 与 
预先 编写 好 只能 按照 特定 逻辑 去 执行 指令 
的 软件 不同 机器 实际上 是 在用 大量 数据 和 
算法 去 自我 训练 从而 学会 如何 完成 一项 任务 
所以 本 阶段 主要 从 机器学习 概述 数据 清洗 和 
特征选择 回归 算法 决策树 随机 森林 和 提升 算法 SVM 
聚 类 算 EM 算法 贝叶斯 算法 隐 马尔科夫 模型 
LDA 主题 模型 等 方面 讲解 一些 机器学习 的 相关 
算法 以及 这些 算法 的 优化 过程 这些 算法 也 
就是 监督 算法 或者 无 监督 算法 一 机器学习 1 
机器学习 概述 二 监督 学习 1 逻辑 回归 2 softmax 
分类 3 条件 随 机场 4 支持 向量 机 svm5 
决策树 6 随机 森林 7 GBDT8 集成 学习 三 非 
监督 学习 1 高斯 混合模型 2 聚 类 3 PCA4 
密度估计 5 LSI6 LDA7 双聚类/nr 四 数据处理 与 模型 调 
优 1 特征提取 2 数据 预处理 3 数据 降 维 
4 模型 参数 调 优 5 模型 持久化 6 模型 
可视化 阶段 四 人工智能 实用 － 数据挖掘 篇 本 阶段 
主要 通过 音乐文件 分类 和 金融 反 欺诈 模型 训练 
等 项目 帮助 大家 对于 上 阶段 的 机器 学习 
做 更 深入 的 巩固 为 后续 深度 学习 及 
数据挖掘 提供 项目 支撑 项目 一 百度 音乐 系统文件 分类 
音乐 推荐 系统 就是 利用 音乐 网站 上 的 音乐 
信息 向 用户 提供 音乐 信息 或者 建议 帮助 用户 
决定 应该 听 什么 歌曲 而 个人化 推荐 则是 基于 
音乐 信息 及 用户 的 兴趣 特征 听歌 历史 行为 
向 用户 推荐 用户 可能 会 感兴趣 的 音乐 或者 
歌手 推荐算法 主要 分为 以下 几种 基于 内容 的 推荐 
协同 过滤 推荐 基于 关联 规则 推荐 基于 效用 推荐 
基于 知识 推荐 等 推荐 系统 常 用于 各个 互联网 
行业 中 比如 音乐 电商 旅游 金融 等 项目 二 
千万级 P2P 金融 系统 反 欺诈 模型 训练 目前 比较 
火 的 互联网 金融 领域 实质 是 小额 信贷 小额 
信贷 风险 管理 本质上 是 事前 对 风险 的 主动 
把 控 尽可能 预测 和 防范 可能 出现 的 风险 
本 项目 应用 GBDT Randomforest 等 机器学习 算法 做 信贷 
反 欺诈 模型 通过 数据挖掘 技术 机器学习 模型 对 用户 
进行 模型 化 综合 度量 确定 一个 合理 的 风险 
范围 使 风险 和 盈利 达到 一个 平衡 的 状态 
阶段 五 人工智能 前沿 － 深度 学习 篇 深度 学习 
是 实现 机器 学习 的 技术 同时 深度 学习 也 
带来 了 机器 学习 的 许多 实际 应用 拓展 了 
AI 的 使用 领域 本 阶段 主要 从 TensorFlow BP 
神经网络 深度 学习 概述 CNN 卷积 神经网络 递归 神经网 自动编码 
机 序 列到 序列 网络 生成 对抗 网络 孪生 网络 
小样 本 学习 技术 等 方面 讲解 深度 学习 相关 
算法 以 掌握 深度 学习 前沿技术 并 根据 不同 项目 
选择 不同 的 技术 解决 方案 针对 公司 样本 不足 
采用 小 样本 技术 和 深度 学习 技术 结合 是 
项目 落地 的 解决 方案 1 TensorFlow 基本 应用 2 
BP 神经网络 3 深度 学习 概述 4 卷积 神经网络 CNN 
5 图像 分类 vgg resnet 6 目标 检测 rcnn fast 
rcnn faster rcnn ssd 7 递归 神经网络 RNN 8 lstm 
bi lstm 多层 LSTM9 无 监督 学习 之 AutoEncoder 自动 
编码器 10 Seq2Seq11 Seq2Seq with Attension12 生成 对抗 网络 13 
irgan14 finetune 及 迁移 学习 15 孪生 网络 16 小样 
本 学习 阶段 六 人工智能 进阶 － 自然语言 处理 篇 
自然语言 处理 NLP 是 计算机 科学 领域 与 人工智能 领域 
中 的 一个 重要 方向 它 已 成为 人工智能 的 
核心 领域 自然语言 处理 解决 的 是 让 机器 可以 
理解 自然语言 这一 到 目前 为止 都还/nr 只是 人类 独有 
的 特权 被 誉为 人工智能 皇冠 上 的 明珠 被 
广泛 应用 本 阶段 从 NLP 的 字 词 和 
句子 全方位 多角度 的 学习 NLP 作为 NLP 的 基础 
核心技术 对 NLP 为 核心 的 项目 如 聊天 机器人 
合理 用药 系统 写诗 机器人 和 知识图谱 等 提供 底层 
技术 通过学习 NLP 和 深度 学习 技术 掌握 NLP 具有 
代表性 的 前沿 技术 1 词 分词 词性 标注 代码 
实战 2 词 深度 学习 之词 向量 字 向量 代码 
实战 3 词 深度 学习 之 实体 识别 和 关系 
抽取 代码 实战 4 词 关键词 提取 无 用词 过滤 
代码 实战 5 句 句法分析 语义分析 代码 实战 6 句 
自然语言 理解 一阶逻辑 代码 实战 7 句 深度 学习 之 
文本 相似 度 代码 实战 阶段 七 人工智能 进阶 － 
图像处理 篇 数字图像处理 Digital Image Processing 是 通过 计算机 对 
图像 进行 去除 噪声 增强 复原 分割 提取 特征 等 
处理 的 方法 和 技术 广泛 的 应用 于 农牧业 
林业 环境 军事 工业 和 医学 等 方面 是 人工 
智能 和 深度 学习 的 重要 研究 方向 深度 学习 
作为 当前 机器学习 领域 最 热门 的 技术 之一 已经 
在 图像 处理 领域 获得 了 应用 并且 展现 出 
巨大 的 前景 本 阶段 学习 了 数字 图像 的 
基本 数据 结构 和 处理 技术 到 前沿 的 深度 
学习 处理 方法 掌握 前沿 的 ResNet SSD Faster RCNN 
等 深度 学习 模型 对 图像 分类 目标 检测 和 
模式 识别 等 图像处理 主要 领域 达到 先进水平 实际 工作 
中 很多 项目 都 可以 转化 为 本 课程 的 
所学 的 知识 去 解决 如 行人 检测 人脸 识别 
和 数字 识别 一 图像 基础 图像 读 写 保存 
画图 线 圆 多边形 添加 文字 二 图像 操作 及 
算数 运算 图像 像素 读取 算数 运算 ROI 区域 提取 
三 图像 颜色 空间 运算 图像 颜色 空间 相互 转化 
四 图像 几何变换 平移 旋转 仿射变换 透视 变换 等 五 
图像 形态学 腐蚀 膨胀 开 / 闭 运算 等 六 
图像 轮廓 长宽 面积 周长 外接圆 方向 平均 颜色 层次 
轮廓 等 七 图像 统计学 图像 直方图 八 图像 滤波 
高斯滤波 均值 滤波 双边 滤波 拉普拉斯 滤波 等 阶段 八 
人工智能 终极 实战 － 项目 应用 本 阶段 重点 以 
项目 为 导向 通过 公安 系统 人脸识别 图像 识别 以及 
图像 检索 今日 头条 CTR 广告 点击量 预估 序列 分析 
系统 聊天 机器人 等 多个 项目 的 讲解 结合 实际 
来 进行 AI 的 综合 运用 项目 一 公安系统 人脸识别 
图像识别 使用 深度 学习 框架 从零开始 完成 人脸 检测 的 
核心 技术 图像 类别 识别 的 操作 从 数据 预处理 
开始 一步步 构建 网络 模型 并 展开 分析 与 评估 
方便 大家 快速 动手 进行 项目 实践 识别 上千 种 
人 靓 返回 层次化 结构 的 每个人 的 标签 项目 
二 公安系统 图像 检索 本 项目 基于 卷积 神经网 在 
训练 过程 中 学习 出 对应 的 二 值 检索 
向量 对 全部 图 先 做 了 一个 分 桶 
操作 每次 检索 的 时候 只取 本 桶 和 临近 
桶 的 图片 作 比对 而 不是 在 全域 做 
比对 使用 这样 的 方式 提高 检索 速度 使用 Tensorflow 
框架 建立 基于 ImageNet 的 卷积 神经网络 并 完成 模型 
训练 以及 验证 项目 三 今日 头条 CTR 广告 点击量 
预估 点击率 预估 是 广告 技术 的 核心 算法 之一 
它 是 很多 广告 算法 工程师 喜爱 的 战场 广告 
的 价值 就 在于 宣传 效果 点击率 是 其中 最 
直接 的 考核 方式 之一 点击率 越大 证明 广告 的 
潜在 客户 越多 价值 就 越大 因此 才 会 出现 
了 刷 点击率 的 工具 和 技术 通过 对于 点击量 
的 评估 完成 对于 潜在 用户 的 价值 挖掘 项目 
四 序列 分析 系统 时间 序列 分析 Time Series Analysis 
是 一种 动态数据 处理 的 统计 方法 主要 基于 随机 
过程 理论 和 数理统计 方法 研究 随机 数据 序列 所 
遵从 的 统计 规律 以便 用于 解决 实际 问题 主要 
包括 自 相关 分析 等 一般 的 统计 分析 方法 
构建 模型 从而 进行 业务 推断 经典 的 统计 分析 
是 假定 数据 序列 具有 独立性 而 时间 序列 分析 
则 侧重 于 研究 数据 样本 序列 之间 的 依赖 
关系 时间 序列 预测 一般 反应 了 三种 实际 变化 
规律 趋势 变化 周期性 变化 和 随机 性 变化 时间/n 
序列/n 预测/vn 常/d 应用/v 于/p 国民经济/n 宏观/n 控制/v 企业 经营 
管理 市场潜力 量 预测 天气预报 水文预报 等 方面 是 应用 
于 金融 行业 的 一种 核心 算法 之一 项目 五 
京东 聊天 机器人 / 智能 客服 聊天 机器人 / 智能 
客服 是 一个 用来 模拟 人类 对话 或者 聊天 的 
一个 系统 利用/n 深度/ns 学习/v 和/c 机器/n 学习/v 等/u NLP/w 
相关/v 算法/n 构建/v 出/v 问题/n 和/c 答案/n 之间/f 的/uj 匹配/v 
模型/n 然后 可以 将 其 应用 到 客服 等 需要 
在线 服务 的 行业 领域 中 聊天 机器人 可以 降低 
公司 客服 成本 还 能够 提高 客户 的 体验 友 
好性 在 一个 完整 的 聊天 机器 人 实现 过程 
中 主要 包含 了 一些 核心 技术 包括 但 不限 
于 爬虫 技术 机器学习 算法 深度 学习 算法 NLP 领域 
相关 算法 通过 实现 一个 聊天 机器人 可以 帮助 我们 
队 AI 整体 知识 的 一个 掌握 项目 六 机器人 
写 诗歌 机器人 写 诗歌 / 小说 是 一种 基于 
NLP 自然语言 相关 技术 的 一种 应用 在 实现 过程 
中 可以 基于 机器学习 相关 算法 或者 深度 学习 相关 
算法 来 进行 小说 / 诗歌 构建 过程 人工智能 的 
一个 终极 目标 就是 让 机器 人 能够 像 人类 
一样 理解 文字 并 运用 文字 进行 创作 而 这个 
目标 大致上 主要 分为 两个 部分 也 就是 自然语言 理解 
和 自然 语言 生成 其中 现阶段 的 主要 自然语言 生成 
的 运用 自然语言 生成 主要 有 两种 不同 的 方式 
分别 为 基于 规则 和 基于 统计 基于 规则 是 
指 首先 了解 词性 及 语法 等 规则 再 依据 
这样 的 规则 写出 文章 而 基于 统计 的 本质 
是 根据 先前 的 字句 和 统计 的 结果 进而 
判断 下一 个子 的 生成 例如 马尔科夫 模型 就 是 
一种 常用 的 基于 统计 的 方法 项目 七 机器翻译 
系统 机器翻译 又称 自动 翻译 是 指 利用 计算机 将 
一种 自然 语言 转换 为 另外 一种 自然 语言 的 
过程 机器 翻译 是 人工智能 的 终极 目标 之一 具有 
很高 的 研究 价值 同时 机器翻译 也 具有 比较 重要 
的 实用 价值 机器 翻译 技术 在 促进 政治 经济 
文化 交流 等 方面 起到 了 越来越 重要 的 作用 
机器翻译 主要 分为 以下 三 个 过程 原文 分析 原文 
译文 转换 和 译文 生成 机器 翻译 的 方式 有 
很多 种 但是 随着 深度 学习 研究 取得 比较 大 
的 进展 基于 人工 网络 的 机器 翻译 也 逐渐 
兴起 特别 是 基于 长 短时记忆 LSTM 的 循环 神经网络 
RDD 的 应用 为 机器翻译 添了 一把 火 项目 八 
垃圾邮件 过滤 系统 邮件 主要 可以 分为 有效 邮件 和 
垃圾 邮件 两大类 有效 邮件 指 的 邮件 接收者 有 
意义 的 邮件 而 垃圾邮件 转 指 那些 没有 任何 
意义 的 邮件 其 内容 主要 包含 赚钱 信息 成人 
广告 商业 或者 个人 网站 广告 电子杂志 等 其中 垃圾邮件 
又 可以 发为 良性 垃圾 邮件 和 恶性 垃圾邮件 良性 
垃圾邮件 指 的 就是 对 收件人 影响 不大 的 信息 
邮件 而 恶性 垃圾邮件 指 具有 破坏性 的 电子邮件 比如 
包 含病毒 木马 等 恶意 程序 的 邮件 垃圾邮件 过滤 
主要 使用 使用 机器学习 深度 学习 等 相关 算法 比如 
贝叶斯 算法 CNN 等 识别 出 所 接收 到 的 
邮件 中 那些 是 垃圾 邮件 项目 九 手工 数字 
识 别人 认知 世界 的 开始 就是 从 认识 数字 
开始 的 深度 学习 也 一样 数字 识别 是 深度 
学习 的 一个 很好 的 切入口 是 一个 非常 经典 
的 原型 问题 通过 对 手写 数字 识别 功能 的 
实现 可以 帮助 我们 后续 对 神经 网络 的 理解 
和 应用 选取 手写 数字 识别 的 主要 原因 是 
手写 数字 具有 一定 的 挑战性 要求 对 编程 能力 
及 神经 网络 思维 能力 有 一定 的 要求 但 
同时 手写 数字 问题 的 复杂度 不高 不 需要 大量 
的 运算 而且 手写 数字 也 可以 作为 其它 技术 
的 一个 基础 所以 以 手写 数字 识别 为 基础 
贯穿 始终 从而 理解 深度 学习 相关 的 应用 知识 
项目 十 癌症 筛选 检测 技术 可以 改变 癌症 患者 
的 命运 吗 对于 患有 乳腺癌 患者 来说 复发 还是 
痊愈 影响 这 患者 的 生命 那么 怎么 来 预测 
患者 的 患病 结果呢 机器学习 算法 可以 帮助 我们 解决 
这 一 难题 本 项目 应用 机器学习 logistic 回归模型 来 
预测 乳腺癌 患者 复发 还是 正常 有效 的 预测 出 
医学 难题 项目 十一 葡萄酒 质量 检测 系统 随着 信息 
科技 的 快速 发展 计算机中 的 经典 算法 在 葡萄酒 
产业 中 得到 了 广泛 的 研究 与 应用 其中 
机器学习 算法 的 特点 是 运用 了 人工智能 技术 在/p 
大量/n 的/uj 样本/n 集/q 训练/vn 和/c 学习/v 后/f 可以/c 自动/vn 
地/uv 找出/v 运算/vn 所/c 需要/v 的/uj 参数/n 和/c 模型/n 项目 
十二 淘宝网 购物篮 分析 推荐算法 购物篮 分析 Market Basket Analysis 
即 非常 有名 的 啤酒 尿布 故事 的 一个 反应 
是 通过 对 购物篮 中 的 商品 信息 进行 分析 
研究 得出 顾客 的 购买 行为 主要 目的 是 找出 
什么样 的 物品 会 经常 出现 在 一起 也 就是 
那些 商品 之间 是 有 很大 的 关联性 的 通过 
购物篮 分析 挖掘 出来 的 信息 可以 用于 指导 交叉 
销售 追加 销售 商品 促销 顾客 忠诚度 管理 库存 管理 
和 折扣 计划 等 业务 购物篮 分析 的 最 常用 
应用 场景 是 电商 行业 但 除此之外 该 算法 还被 
应用于 信用卡 商城 电信 与 金融 服务 业 保险业 以及 
医疗 行业 等 项目 十三 手工 实现 梯度 下降 回归 
算法 梯度 下 降法 英语 Gradient descent 是 一个 一 
阶 最优化 算法 通常 也 称为 最速 下 降法 要 
使用 梯度 下 降法 找到 一个 函数 的 局部 极小值 
必须 向 函数 上当 前点 对应 梯度 或者 是 近似 
梯度 的 反方向 的 规定 步 长距离 点 进行 迭代 
搜索 如果 相反地 向 梯度 正方向 迭代 进行 搜索 则会 
接近 函数 的 局部 极大值 点 这个 过程 则 被称为 
梯度 上升 法 项目 十四 基于 TensorFlow 实现 回归 算法 
回归 算法 是 业界 比较 常用 的 一种 机器学习 算法 
通过 应用 于 各种 不同 的 业务 场景 是 一种 
成熟 而 稳定 的 算法 种类 TensorFlow 是 一种 常 
用于 深度 学习 相关 领域 的 算法 工具 随着 深度 
学习 热度 的 高涨 TensorFlow 的 使用 也 会 越来越 
多 从而 使用 TensorFlow 来 实现 一个 不 存在 的 
算法 会 加深 对 TensorFlow 的 理解 和 使用 基于 
TensorFlow 的 回归 算法 的 实现 有助于 后续 的 TensorFlow 
框架 的 理解 和 应用 并 可以 促进 深度 学习 
相关 知识 的 掌握 项目 十五 合理 用药 系统 合理 
用药 系统 是 根据 临床 合理 用药 专业 工作 的 
基本 特点 和 要求 运用 NLP 和 深度 学习 技术 
对 药品 说明书 临床 路径 等 医学 知识 进行 标准化 
结构化 处理 如 自动 提取 药品 说明书 文本 里面 的 
关键 信息 如 药品 相互作用 禁忌 用法 用量 适用人群 等 
实现 医嘱 自动 审查 及时 发现 不 合理 用药 问题 
帮助 医生 药师 等 临床 专业 人员 在 用药 过程 
中 及时 有效 地 掌握 和 利用 医药 知识 预防 
药物 不良 事件 的 发生 促进 临床 合理 用药 工作 
项目 十六 行人 检测 行人 检测 是 利用 图像 处理 
技术 和 深度 学习 技术 对 图像 或者 视频 序列 
中 是否 存在 行人 并 给予 精确定位 学习 完 行人 
检测 技术 后 对 类似 的 工业 缺陷 检测 外观 
检测 和 医疗 影像 检测 等 目标 检测 范畴 类 
的 项目 可以 一通百通 该 技术 可 与 行人 跟踪 
行人 重 识别 等 技术 结合 应用于 人工 智能系统 车辆 
辅助 驾驶 系统 智能 机器人 智能 视频 监控 人体 行为 
分析 智能 交通 等 领域 由于 行人 兼具 刚性 和 
柔性 物体 的 特性 外观 易受 穿着 尺度 遮挡 姿态 
和 视角 等 影响 使得 行人 检测 成为 计算机 视觉 
领域 中 一个 既 具有 研究 价值 同时 又 极具 
挑战性 的 热门 课题 阶段 九 人工智能 实战 － 企业 
项目 实战 课程 一 基于 Python 数据 分析 与 机器学习 
案例 实战 教程 课程 风格 通俗易懂 基于 真实 数据集 案例 
实战 主体 课程 分成 三 个大 模块 1 python 数据分析 
2 机器学习 经典 算法 原理 详解 3 十大 经典 案例 
实战 通过 python 数据 科学 库 numpy pandas matplot 结合 
机器学习 库 scikit learn 完成 一些 列 的 机器学习 案例 
算法 课程 注重 于 原理 推导 与 流程 解释 结合 
实例 通俗 讲解 复杂 的 机器学习 算法 并以 实战 为主 
所有 课时 都 结合 代码 演示 算法 与 项目 相 
结合 选择 经典 kaggle 项目 从 数据 预处理 开始 一步步 
代码 实战 带 大家 快速 入门 机器学习 旨在 帮助 同学 
们 快速 上手 如何 使用 python 库 来 完整 机器学习 
案例 选择 经典案例 基于 真实 数据集 从 数据 预处理 开始 
到 建立 机器学习 模型 以及 效果 评估 完整 的 讲解 
如何 使用 python 及其 常用 库 进行 数据 的 分析 
和 模型 的 建立 对于 每 一个 面对 的 挑战 
分析 解决 问题 思路 以及 如何 构造 合适 的 模型 
并且 给出 合适 评估 方法 在 每 一个 案例 中 
同学 们 可以 快速 掌握 如何 使用 pandas 进行 数据 
的 预处理 和 分析 使用 matplotlib 进行 可视化 的 展示 
以及 基于 scikit learn 库 的 机器学习 模型 的 建立 
1 Python 数据 分析 与 机器学习 实战 课程 简介 2 
Python 快速 入门 3 Python 科学计算 库 Numpy4 Python 数据 
分析 处理 库 Pandas5 Python 可视化 库 Matplotlib6 回归 算法 
7 模型 评估 8 K 近邻 算法 9 决策树 与 
随机 森林 算法 10 支持 向量 机 11 贝叶斯 算法 
12 神经网络 13 Adaboost 算法 14 SVD 与 推荐 15 
聚 类 算法 16 案例 实战 使用 Python 库 分析处理 
Kobe Bryan 职业生涯 数据 17 案例 实战 信用卡 欺诈 行为 
检测 18 案例 实战 泰坦尼克号 获救 预测 19 案例 实战 
鸢尾花 数据集 分析 20 案例 实战 级联 结构 的 机器学习 
模型 21 案例 实战 员工 离职 预测 22 案例 实战 
使用 神经 网络 进行 手写 字体 识别 23 案例 实战 
主 成分 分析 24 案例 实战 基于 NLP 的 股价 
预测 25 案例 实战 借贷 公司 数据 分析 课程 二 
人工智能 与 深度 学习 实战 课程 风格 通俗易懂 必备 原理 
形象 解读 项目 实战 缺一不可 主体 课程 分成 四 个大 
模块 1 神经网络 必备 基础 知识 点 2 深度 学习 
模型 3 深度 学习 框架 Caffe 与 Tensorflow 4 深度 
学习 项目 实战 课程 首先 概述 讲解 深度 学习 应用 
与 挑战 由 计算机 视觉 中 图像 分类 任务 开始 
讲解 深度 学习 的 常规 套路 对于 复杂 的 神经 
网络 将其 展 开成 多个 小 模块 进行 逐一 攻破 
再 挑战 整体 神经 网络 架构 对于 深度 学习 模型 
形象 解读 卷积 神经网络 原理 详解 其中 涉及 的 每一个 
参数 对 卷积 网络 架构 展开 分析 与 评估 对于 
现阶段 火爆 的 对抗 生成 网络 以及 强化 学习 给出 
形象 解读 并 配合 项目 实战 实际 演示 效果 基于 
框架 实战 选择 两款 深度 学习 最 火 框架 Caffe 
与 Tensorflow 首先 讲解 其 基本 使用 方法 并 结合 
案例 演示 如何 应用 框架 构造 神经网络 模型 并 完成 
案例 任务 选择 经典 深度 学习 项目 实战 使用 深度 
学习 框架 从零开始 完成 人脸 检测 验证码 识别 人脸 关键点 
定位 垃圾邮件 分类 图像 风格 转换 AI 自己 玩 游戏 
等 对于 每 一个 项目 实战 从 数据 预处理 开始 
一步步 构建 网络 模型 并 展开 分析 与 评估 课程 
提供 所 涉及 的 所有 数据 代码 以及 PPT 方便 
大家 快速 动手 进行 项目 实践 1 深度 学习 概述 
与 挑战 2 图像 分类 基本原理 门 3 深度 学习 
必备 基础 知识 点 4 神经网络 反向 传播 原理 5 
神经网络 整体 架构 6 神经网络 案例 实战 图像 分类 任务 
7 卷积 神经 网络 基本 原理 8 卷积 参数 详解 
9 卷积 神经网络 案例 实战 10 经典 网络 架构 分析 
11 分类 与 回归 任务 12 三代 物 体检 测算 
法分析 13 数据 增强 策略 14 T r a n 
s f e r L e a r n i 
n g 1 5 网络 架构 设计 16 深度 学习 
框架 Caffe 网络结构 配置 17 Caffe18 深度 学习 项目 实战 
人脸 检测 19 人脸 正负 样本 数据源 制作 20 人脸 
检测 网络 架构 配置 习 模型 21 人脸 检测 代码 
实战 22 人脸 关键点 定位 项目 实战 23 人脸 关键点 
定位 网络 模型 24 人脸 关键点 定位 构建 级 联网络 
25 人脸 关键点 定位 测试 效果 与 分析 26 Tensorflow 
框架 实战 27 Tensorflow 构建 回归模型 28 Tensorflow 构建 神经网络 
模型 29 Tensorflow 深度 学习 模型 30 Tensorflow 打造 RNN 
网络 模型 31 Tensorflow 项目 实战 验证 识别 32 项目 
实战 图像 风格 转换 33 QLearning 算法 原理 34 DQN 
网络 架构 35 项目 实战 DQN 网络 让 AI 自己 
玩 游戏 36 项目 实战 对抗 生成 网络 等 项目 
一 AI 大 数据 互联网 电影 智能 推荐 第一季 随着 
科技 的 发展 现在 视频 的 来源 和 类型 多样性 
互联网 视频 内容 充斥 着 整个 网络 如果 仅仅 是 
通过 翻页 的 方法 来 寻找 自己 想看 的 视频 
必然 会 感到 疲劳 现在 急需 一种 能 智能 推荐 
的 工具 推荐 系统 通过 分析 用户 对 视频 的 
评分 分析 对 用户 的 兴趣 进行 建模 从而 预测 
用户 的 兴趣 并 给 用户 进行 推荐 Python 是 
一种 面向 对象 的 解释 型 计算机程序 设计 语言 Python 
具有 丰富 和 强大 的 库 它 常被 昵称 为 
胶水 语言 而 大 数据 是 指 无法 在 一定 
时间 范围 内 用 常规 软件工具 进行 捕捉 管理 和 
处理 的 数据 集合 企业 面临 海量 数据 的 到来 
大多 选择 把 数据 从 本地 迁移 至 云端 云端 
将 成为 最大 的 非 结构化 数据 存储 场所 本 
项目 主要 以 客户 咨询 为 载体 分析 客户 的 
群体 分布 旨在 挖掘 客户 的 内在 需求 帮助 企业 
实现 更 有价值 的 营销 一 教务 管理 系统 业务 
介绍 1 教务 管理 系统 框架 讲解 2 系统 业务 
逻辑 介绍 二 大 数据 需求分析 1 明确 数据 需求 
2 大 数据分析 过程 3 分析 难点 和 解决 方案 
4 大 数据 相关 技术 选型 三 构建 分布式 大 
数据 框架 1 Hadoop 分布式 集群 配置 2 ZooKeeper 高 
可用 3 SQOOP 数据 转移 4 ETL 数据 清洗 5 
HIVE 数据分析 6 HBase 数据 存储 四 基于 教务 管理 
系统 大 数据分析 1 业务 数据 分析 指标 设定 2 
操作 MapReduce 分而治之 3 使用 Hive 进行 数据 整合 抽离 
4 使用 HBase 存储 非 结构 话 数据 五 大 
数据 可视化 1 可视化 技术 选型 2 Echarts 代码 展示 
炫 酷 视图 3 使用 Tableau 进行 数据 可视化 展示 
项目 二 电商 大 数据 情感 分析 与 AI 推断 
实战 项目 第一季 本 项目 从 开发 的 角度 以大 
数据 PHP 技术 栈 为基础 使用 真实 商用 表 结构 
和 脱敏 数据 分三步 构建 商用 系统 真实 大 数据 
环境 进行 推断 分析 以及 呈现 结果 项目 课程 的 
完整性 商业性 可以 使 学者 尽可能 完整 地 体会 真实 
的 商业 需求 和 业务 逻辑 完整 的 项目 过程 
使 PHP 技术 栈 的 同学 得以 窥见 和 学到 
一个 完整 商业 平台 项目 的 搭建 方法 真实 大 
数据 环境 的 搭建 使 呈现 建立 大 数据 的 
工具 应用 技术 概念 储备 基于 大 数据 平台 的 
分析 需求 的 实现 呈现 将 完整 的 一次 大 
数据 技术 栈 到 分析 结果 的 中线 平铺 直述 
为 想要 学习 大 数据 并 有 开发 基础 的 
同学 点亮 新 的 能力 一 实践 项目 研发 1 
开发 环境 的 安装 配置 2 表 与 数据 3 
LARAVEL 的 快速 开发 实践 4 批量 创建 模型 5 
万能 控制器 与 表 配置 6 统一 视图 的 创建 
二 数据分析 需求 设立 1 定义数据 需求 2 分析 计算 
过程 3 分析 难点 和 解决 方案 4 大 数据 
技术 选型 三 大 数据 平台 搭建 1 分布式 环境 
的 模拟 建立 2 网络 环境 的 调 通3/nr 身份验证 
与 集群 控制 4 Hadoop 环境 搭建 和 要点 说明 
5 MapReduce 与 Yarn 的 搭建 和 说明 四 大 
数据分析 脚本 编写 1 MapReduce 脚本 编写 2 拆解 数据 
需求 3 Map 逻辑 详写 4 Reduce 逻辑 详写 5 
结果 整理 与 输出 五 结果 可视化 1 可视化 需求 
和 技术 选型 2 展示 页面 的 快速 铺设 3 
可视化 JS 上手 4 使用 可视化 JS 展示 结果 项目 
三 AI 法律咨询 大 数据 分析 与 服务 智能 推荐 
实战 项目 第一季 本 项目 结合 目前 流行 的 大 
数据 框架 在 原有 成熟 业务 的 前提 下 进行 
大 数据 分析 处理 真实 还原 企业应用 让 学员 身临其境 
的 感受 企业 大 数据 开发 的 整个 流程 项目 
的 业务 系统 底层 主要 采用 JAVA 架构 大 数据 
分析 主要 采用 Hadoop 框架 其中 包括 Kettle 实现 ETL 
SQOOP Hive Kibana HBASE Spark 以及 人工 智能算法 等 框架 
技术 采用 真实 大 数据 集群 环境 的 搭建 让 
学员 切身感受 企业 项目 的 从0到/nr 1 的 过程 一 
系统 业务 介绍 1 底层 业务 实现 框架 讲解 2 
功能模块 讲解 二 系统 架构设计 1 总体 架构 分析 2 
数据 流向 3 各 技术 选型 承载 作用 4 部署 
方案 三 详尽 实现 1 原始 数据处理 2 ETL 数据 
导入 3 MR 数据 计算 4 Hive 数据分析 四 数据 
可视化 1 采用 Highcharts 插件 展示 客户 偏好 曲线图 2 
使用 Tableau 进行 数据 分析 可视化 展示 五 项目 优化 
1 ZooKeeper 实现 HA2 集群 监控 的 整体 联调 项目 
四 AI 大 数据 基站定位 智能 推荐 商圈分析 项目 实战 
第一季 随着 当今 个人 手机 终端 的 普及 出行/v 人群/n 
中/f 手机/n 拥有率/n 和/c 使用率/n 已/d 达到/v 相当/d 高的/nr 比例/n 
根据 手机 信号 在 真实 地理 空间 的 覆盖 情况 
将 手机 用户 时间 序列 的 手机 定位 数据 映射 
至 现实 地理位置 空间 位置 即可 完整 客观 地 还原 
出 手机 用户 的 现实 活动 轨迹 从而 挖掘出 人口 
空间 分布 与 活动 联系 特征 信息 商圈 是 现代 
市场 中 企业 市场 活动 的 空间 同时 也 是 
商品 和 服务 享用者 的 区域 商圈 划分 为 目的 
之一 是 研究 潜在 顾客 分布 以 制定 适宜 的 
商业 对策 本 项目 以 实战 为 基础 结合 大 
数据 技术 Hadoop . Net 技术 全栈/nr 为基础 采用 真实 
商业 数据 分 不同 环节 构建 商用 系统 真实 大 
数据 环境 进行 推断 分析 及 呈现 数据 一 分析 
系统 业务 逻辑 讲解 1 大 数据 基站定位 智能 推荐 
商圈分析 系统 介绍 2 数据 前期 清洗 和 数据 分析 
目标 指标 的 设定 等 二 大 数据 导入 与 
存储 1 关系型 数据库 基础知识 2 hive 的 基本 语法 
3 hive 的 架构 及 设计 原理 4 hive 安装 
部署 与 案例 等 5 Sqoop 安装 及 使用 6 
Sqoop 与 关系型 数据库 进行 交互 等 7 动手 实践 
三 Hbase 理论 及 实战 1 Hbase 简介 安装 及 
配置 2 Hbase 的 数据 存储 与 数据模型 3 Hbase 
Shell4 Hbase 访问 接口 5 Hbase 数据备份 与 恢复 方法 
等 6 动手 实践 数据 转储 与 备份 四 基站 
数据 分析 与 统计 推断 1 背景 与 分析 推断 
目标 2 分析 方法 与 过程 推断 3 动手 实践 
分析 既定 指标 数据 五 数据 分析 与 统计 推断 
结果 的 展示 大 数据 可视化 1 使用 Tableau 展示 
数据 分析 结果 2 使用 HighCharts ECharts 展示 数据 分析 
结果 阶段 十 阿里云 认证 课程 一 云计算 网站 建设 
部署 与 发布 阿里云 网站 建设 认证 课程 教 你 
如何 掌握 将 一个 本地 已经 设 计好 的 静态 
网站 发布 到 Internet 公共 互联网 绑定 域名 完成 工信部 
的 ICP 备案 课程 二 云计算 网站 建设 简单 动态 
网站 搭建 阿里云 简单 动态 网站 搭建 课程 教 你 
掌握 如何 快速 搭建 一个 WordPress 动态 网站 并会 对 
网站 进行 个性化 定制 以 满足 不同 的 场景 需求 
课程 三 云计算 云/ns 服务器/n 管理/vn 维护/v 阿里云/i 服务器/n 运维/i 
管理/vn 课程/n 教/v 你/r 掌握/v 快速/d 开通/v 一台/m 云/ns 服务器/n 
并 通过 管理 控制台 方便 地 进行 服务器 的 管理 
服务器 配置 的 变更 和 升级 数据 的 备份 并 
保证 其 可以 正常 运转 并按 业务 需求 随时 进行 
配置 的 变更 课程 四 云计算 云数据库 管理 与 数据 
迁移 阿里云 云数据库 管理 与 数据 迁移 认证 课程 掌握 
云数据库 的 概念 如何 在云端 创建 数据库 将 自建 数据库 
迁移 至 云数据库 MySQL 版 数据 导入 导出 以及 云数据库 
运维 的 常用 操作 课程 五 云计算 云 存储 对象 
存储管理 与 安全 阿里 云云 储存 认证 课程 教 你 
掌握 安全 高/a 可靠/v 的/uj 云/ns 存储/l 的/uj 使用/v 以及 
在云端 存储 下载 文件 处理 图片 以及 如何 保护 数据 
的 安全 课程 六 云计算 超大 流量 网站 的 负载 
均衡 掌握 如何 为 网站 实现 负载 均衡 以/p 轻松/a 
应对/v 超大/v 流量/n 和高/nr 负载/v 课程 七 大 数据 MOOC 
网站 日志 分析 本 课程 可以 帮助 学员 掌握 如何 
收集 用户 访问 日志 如何 对 访问 日志 进行 分析 
如何 利用 大 数据 计算 服务 对 数据 进行 处理 
如何 以 图表 化 的 形式 展示 分析 后的/nr 数据 
课程 八 大 数据 搭建 企业级 数据分析 平台 模拟 电商 
场景 搭建 企业级 的 数据分析 平台 用来 分析 商品 数据 
销售 数据 以及 用户 行为 等 课程 九 大 数据 
基于 LBS 的 热点 店铺 搜索 本 课程 可以 帮助 
学员 掌握 如何 在 分布式计算 框架 下 开发 一个 类似于 
手机地图 查找 周边 热点 POI 的 功能 掌握 GeoHash 编码 
原理 以及 在 地理位置 中的 应用 并能 将其 应用 在 
其他 基于 LBS 的 定位 场景 中 课程 中 完整 
的 演示 了 整个 开发 步骤 学员 在学 完 此 
课程 之后 掌握 其 原理 可以 在 各种 分布式计算 框架 
下 完成 此 功能 的 开发 比如 MapReduce Spark 课程 
十 大 数据 基于 机器学习 PAI 实现 精细化 营销 本 
课程 通过 一个 简单 案例 了解 掌握 企业 营销 中 
常见 的 也是 必需 的 精准 营销 数据 处理 过程 
了解 机器学习 PAI 的 具体 应用 指导 学员 掌握 大 
数据 时代 营销 的 利器 通过 机器学习 实现 营销 课程 
十一 大 数据 基于 机器 学习 的 客户 流失 预警 
分析 本 课程 讲解 了 客户 流失 的 分析 方法 
流程 同时 详细 介绍 了 机器 学习 中 常用 的 
分类 算法 集成 学习 模型 等 通用 技能 并 使用 
阿里云 机器学习 PAI 实现 流失 预警 分析 可以 帮助 企业 
快速 准确 识别 流失 客户 辅助 制定 策略 进行 客户 
关怀 达到 挽留 客户 的 目的 课程 十二 大 数据 
使用 DataV 制作 实时 销售 数据 可视化 大 屏 帮助 
非 专业 工程师 通过 图形化 的 界面 轻松 搭建 专业 
水准 的 实时 可视化 数据 大 屏 以 满足 业务 
展示 业务 监控 风险 预警 等 多种 业务 的 展示 
需求 课程 十三 大 数据 使用 MaxCompute 进行 数据 质量 
核查 通过 本 案例 学员 可 了解 影响 数据 质量 
的 因素 出现 数据 质量 问题 的 类型 掌握 通过 
MaxCompute DateIDE 设计 数据 质量 监控 的 方法 最终 独立 
解决 常见 的 数据 质量 监控 需求 课程 十四 大 
数据 使用 Quick BI 制作 图形化 报表 阿里云 Quick BI 
制作 图形化 报表 认证 课程 教 你 掌握 将 电商 
运营 过程 中 的 数据 进行 图表 化 展现 掌握 
通过 Quick BI 将 数据 制作 成 各种 图形化 报表 
的 方法 同时 还 将 掌握 搭建 企业级 报表 门户 
的 方法 课程 十五 大 数据 使用时间 序列 分解 模型 
预测 商品 销量 使用时间 序列 分解 模型 预测 商品 销量 
教 你 掌握 商品 销量 预测 方法 时间 序列 分解 
以及 熟悉 相关 产品 的 操作 演示 和 项目 介绍 
课程 十六 云安全 云/ns 平台/n 使用/v 安全/an 阿里云/i 云/ns 平台/n 
使用/v 安全/an 认证/v 课程/n 教/v 你/r 了解/v 由/p 传统/n IT/w 
到/v 云计算/i 架构/n 的/uj 变迁/vn 过程/n 当前 信息 安全 的 
现状 和 形势 以及/c 在/p 云计算/i 时代不同/i 系统/n 架构/n 中/f 
应该/v 从/p 哪些/r 方面/n 利用/n 云/ns 平台/n 的/uj 优势/n 使用/v 
安全/an 风险/n 快速/d 降低/v 90%/mf 课程 十七 云安全 云上/nr 服务 
器 安全 阿里 云云 上 服务 器 安全 认证 课程 
教 你 了解 在 互联 网上 提供 计算 功能 的 
服务器 主要 面临 哪些 安全 风险 并 针对 这些 风险 
提供 了 切实可行 的 免费 的 防护 方案 课程 十八 
云安全 云上/nr 网络/n 安全/an 了解/v 网络/n 安全/an 的/uj 原理/n 和/c 
解决/v 办法/n 以及 应对 DDoS 攻击 的 方法 和 防护 
措施 确保 云上/nr 网络 的 安全 课程 十九 云安全 云上/nr 
数据安全/n 了解/v 云上/nr 数据/n 的/uj 安全/an 隐患/n 掌握 数据备份 数据 
加密 数据传输 安全 的 解决 方法 课程 二十 云安全 云上/nr 
应用 安全 了解 常见 的 应用 安全 风险 SQL 注入 
原理 及 防护 网站 防 篡改 的 解决 方案 等 
确保 云上/nr 应用 的 安全 课程 二十一 云安全 云上/nr 安全/an 
管理/vn 了解/v 云上的/nr 安全/an 监控/vn 方法/n 学会 使用 监控 大 
屏 来 监控 安全 风险 并能够 自定义 报警 规则 确保 
随时 掌握 云上/nr 应用 的 安全 情况 作者 懒散 的 
鱼与/nr 消失 的 猫 原文 https / / blog . 
csdn . net / qwxwaty / article / details / 
80793370DT 时代 大 数据 BI 和 人工智能 均 是 十分 
火热 的 产业 趋势 而 自然 语言 处理 作为 人工智能 
领域 和 计算机 科学 领域 中 的 一个 重要 方向 
也 随之 火热 了 一把 得到 不少 IT 人士 的 
极大 研究 兴趣 现在 大圣 众包 威客 平台 推介 若干 
本 深入浅出 的 自然 语言 处理 书籍 Foundations of Statistical 
Natural Language Processing 用 统计 方法 处理 自然语言 文本 在 
近年来 已经 占据 了 主导 地位 Foundations of Statistical Natural 
Language Processing 涵盖 了 搭配 发现 词义 消 歧 概率 
解析 信息检索 和 其他 应用 等 内容 同时 它 也对 
统计 自然语言 处理 NLP 进行 了 全面 的 介绍 并且 
包含 了 所有 开发 NLP 工具 所需 的 理论 和 
算法 此书 不但 提供 了 广泛 且 严格 的 数学 
和 语言 基础 的 内容 还 包括 详细 的 统计 
方法 讨论 让 学生 和 研究 人员 可以 根据 其 
实现 自己 的 想法 2 . 自然语言 处理 简明 教程 
系统 地 阐述 了 自然 语言 处理 的 基本 方法 
的 自然语言 处理 简明 教程 描述 了 每一种 方法 的 
技术 原理 及 操作 过程 另外 此书 还 介绍 了 
自然 语言 处理 在 各个 领域 的 应用 让 读者 
能够 掌握 第一手 的 自然 语言 处理 的 前沿 动态 
作为 在 本 领域 十分 著名 的 书籍 自然语言 处理 
简明 教程 不仅 可供 计算机 科学 工作者 人工智能 领域 工作 
者 阅读 还 可供 语言学 及 应用 语言学 的 师生 
阅读 与 参考 3 . Speech and Language Processing 2nd 
Edition Speech and Language Processing 2nd Edition 在 古典 自然语言 
处理 统计 自然语言 处理 语音识别 计算 语言学 和 人类 语言 
处理 的 本科 或 高级 本科课程 中 都 有着 十分 
崇高 的 地位 基于 Web 语言 技术 的 爆炸 式 
发展 以及 多 领域 的 合并 等 使得 语言 处理 
渐渐 成为 让 人 深感 兴趣 的 科目 它 也是 
第一本 在 所有 层次 和 所有 现代 技术 层面 上 
全面 覆盖 语言 技术 的 书 特别 适用 于大/nr 公司 
的 应用 统计 方面 以及 其他 机器学习 算法 领域 4 
. 自然语言 处理 原理 与 技术 实现 自然语言 处理 原理 
与 技术 实现 详细 介绍 了 自然 语言 处理 以 
Java 实现 的 各 主要 领域 的 原理 当中 包括 
中文分词 词性 标注 依存 句法分析 等 更 对 中文分词 和 
词性 标注 的 过程 及 相关 算法 如 隐 马尔可夫 
模型 等 进行 了 详细 的 介绍 本书 内容丰富 它 
在 自然 语言 处理 的 应用 中 主要 介绍 了 
信息 抽取 自动 文摘 文本 分类 等 领域 的 基本 
理论 和 实现 过程 另外 还有 问答 系统 语音 识别 
等 目前 应用 非常 广泛 的 领域 值得 注意 的 
是 在 问答 系统 的 介绍 中 自然语言 处理 原理 
与 技术 实现 特地 介绍 了 聊天 机器人 的 实现 
过程 无论是 从 句子 理解 句法分析 同义词 提取 等 方面 
都 深刻 地 揭示 了 聊天 机器人 的 实现 原理 
好 的 书单 不仅 能够 提升 学习 和 工作 的 
效率 还能 节省 进修 成本 自然语言 处理 研究 是 实现 
人 与 计算机 之间 用 自然 语言 进行 有效 通信 
的 各种 理论 和 方法 希望 广大 对 人工智能 有兴趣 
的 人士 阅读 此文 后 略 觉 有所 裨益 机器学习 
深度 学习 计算机 视觉 自然语言 处理 及 应用 案例 干货 
分享 持续 更新 author @ jason _ qlhttp / / 
blog . csdn . net / lql0716GitChat 提问 码 1 
机器学习 / 深度 学习 1.1 对抗 生成 网络 GAN 2017 
. 04.21 对抗 生成 网络 GAN 变种 大集合 链接 资源 
| 生成 对抗 网络 及其 变体 的 论文 汇总 链接 
生成 对抗 网络 GAN 图片 编辑 链接 CycleGAN 失败 案例 
链接 2017 . 04.22 用 条件 生成 对抗 网络 玩转 
中文 书法 链接 Gang of GANs Generative Adversarial Networks with 
Maximum Margin Ranking F Juefei Xu V N Boddeti M 
Savvides CMU & Michigan State University 2017 链接 2017 . 
04.23 TP GAN 让 图像 生成 再获 突破 根据 单一 
侧脸 生成 正面 逼真 人脸 链接 GitHub 2017 . 04.26 
对抗 生成 网络 GAN 教程 Tutorial on GANs by Adit 
Deshpande 链接 GitHub 2017 . 05.07 GAN 相关 资源 与 
实现 Resources and Implementations of Generative Adversarial Nets GAN DCGAN 
WGAN CGAN InfoGAN by YadiraF 链接 GitHub PyTorch 实现 的 
CoGAN Coupled Generative Adversarial Networks M Liu O Tuzel Mitsubishi 
Electric Research Labs MERL 2016 链接 GitHub 利用 CGAN 生成 
Sketch 漫画 Auto painter Cartoon Image Generation from Sketch by 
Using Conditional Generative Adversarial Networks Y Liu Z Qin Z 
Luo H Wang Beihang University & Samsung T e l 
e c o m m u n i c a 
t i o n Research Institute 2017 链接 GitHub Adversarial 
Feature Learning J Donahue P Kr ä henb ü hl 
T Darrell UC Berkeley 链接 GitHub PyTorch 实现 的 DCGAN 
pix2pix DiscoGAN CycleGAN BEGAN VAE Neural Style Transfer Char RNN 
等 Paper Implementations Use PyTorch to implement some classic frameworks 
by SunshineAtNoon 链接 GitHub GAN 画风 迁移 Generative Adversarial Networks 
for Style Transfer LIVE YouTube by Siraj Raval 链接 GitHub 
video 2017 . 05.08 生成 对抗 网络 GAN 研究 年度 
进展 评述 链接 GitHub 对抗 生成 网络 Gan 深入研究 文献 
/ 教程 / 模型 / 框架 / 库 等 Delving 
deep into GANs by Grigorios Kalliatakis 链接 GitHub 对抗式 机器翻译 
Adversarial Neural Machine Translation L Wu Y Xia L Zhao 
F Tian T Qin J Lai T Liu Sun Yat 
sen University & University of Science and Technology of China 
& Microsoft Research Asia 2017 链接 GitHub CycleGAN 生成 模型 
熊变/nr 熊猫 Models generated by CycleGAN by Tatsuya 链接 GitHub 
对抗 生成 网络 GAN Generative Adversarial Networks LIVE YouTube by 
Siraj Raval 链接 GitHub video Keras 实现 的 ACGAN / 
DCGAN Implementation of some basic GAN architectures in Keras by 
Batchu Venkat Vishal 链接 GitHub 2017 . 05.09 策略 梯度 
SeqGAN SeqGAN Sequence Generative Adversarial Nets with Policy Gradient L 
Yu W Zhang J Wang Y Yu Shanghai Jiao Tong 
University & University College London 2016 链接 GitHub 2017 . 
05.10 Improved Training of Wasserstein GANs I Gulrajani F Ahmed 
M Arjovsky V Dumoulin A Courville Montreal Institute for Learning 
Algorithms & Courant Institute of Mathematical Sciences 2017 链接 GitHub 
GitHub2 Geometric GAN J H Lim J C Ye ETRI 
& KAIST 2017 链接 GitHub PyTorch 实现 的 CycleGAN / 
SGAN 跨 域 迁移 MNIST to SVHN & SVHN to 
MNIST PyTorch Implementation of CycleGAN and SGAN for Domain Transfer 
Minimal by yunjey GitHub 链接 GitHub 1.2 神经网络 2017 . 
04.24 如何 用 PyTorch 实现 递归 神经网络 链接 GitHub 2017 
. 04.25 一个 基于 TensorFlow 的 简单 故事 生成 案例 
带 你 了解 LSTM 链接 GitHub 2017 . 05.07 深度 
学习 10大 框架 对比 分析 链接 GitHub 深度 学习 之 
CNN 卷积 神经网络 链接 GitHub Keras 教程 Python 深度 学习 
Keras Tutorial Deep Learning in Python by Karlijn Willems 链接 
GitHub TensorFlow 官方 解读 如何/r 在/p 多/m 系统/n 和/c 网络拓扑/n 
中/f 构建/v 高/a 性能/n 模型/n 链接 GitHub 从自/nr 编码器 到 
生成 对抗 网络 一文 纵览 无 监督 学习 研究 现状 
链接 GitHub Residual Attention Network for Image Classification F Wang 
M Jiang C Qian S Yang C Li H Zhang 
X Wang X Tang SenseTime Group Limited & Tsinghua University 
& The Chinese University of Hong Kong 2017 链接 GitHub 
基于 OpenAI Gym / Tensorflow / Keras 的 增强 学习 
实验 平台 OpenAI Lab An experimentation system for Reinforcement Learning 
using OpenAI Gym Tensorflow and Keras . by Wah Loon 
Keng 链接 GitHub 基于 生成 卷积 网络 的 潜在 指纹 
重建 Generative Convolutional Networks for Latent Fingerprint Reconstruction J Svoboda 
F Monti M M . Bronstein USI Lugano 2017 链接 
GitHub TensorFlow 入门 代码 集锦 tensorflow resources Curated Tensorflow code 
resources to help you get started by Skcript 链接 GitHub 
入门级 攻略 机器学习 VS . 深度 学习 链接 GitHub Gabor 
Convolutional Networks S Luan B Zhang C Chen X Cao 
J Han J Liu Beihang University & University of Central 
Florida Orlando & Northumbria University & Huawei Company 2017 链接 
GitHub TensorFlow 基准 图像 分类 模型 在 各大 平台 的 
测试 研究 链接 GitHub 谷歌 开源 深度 学习 街景 文字 
识别 模型 让 地图 随 世界 实时 更新 链接 GitHub 
Geometric deep learning going beyond Euclidean data M M . 
Bronstein J Bruna Y LeCun A Szlam P Vandergheynst USI 
Lugano & NYU & Facebook AI Research 2016 链接 GitHub 
利用 强化 学习 设计 神经 网络 架构 Designing Neural Network 
Architectures using Reinforcement Learning B Baker O Gupta N Naik 
R Raskar MIT 2016 链接 GitHub 神经网络 三万英尺 高空 纵览 
入门 Neural Networks A 30 000 Feet View for Beginners 
| Learn OpenCV by Satya Mallick 链接 GitHub Top100 论文 
导读 深入 理解 卷积 神经网络 CNN Part Ⅰ 链接 GitHub 
Top100 论文 导读 深入 理解 卷积 神经网络 CNN Part Ⅱ 
链接 GitHub 深度 神经网络 权值 初始化 的 研究 On weight 
initialization in deep neural networks S K Kumar 2017 链接 
GitHub 2017 . 05.08 提升 结构化 特征 嵌入 深度 度量 
学习 Deep Metric Learning via Lifted Structured Feature Embedding H 
Oh Song Y Xiang S Jegelka S Savarese 2016 链接 
GitHub 图 的 深度 特征 学习 Deep Feature Learning for 
Graphs R A . Rossi R Zhou N K . 
Ahmed Palo Alto Research Center XeroxPARC & Intel Labs 2017 
链接 GitHub 用于 性能 分析 模型 优化 的 神经 网络 
生成器 Perceptron A flexible artificial neural network builder to analysis 
performance and optimise the best model . by Caspar Wylie 
链接 GitHub TensorFlow 最佳 实践 之 文件 文件夹 与 模型 
架构 实用 建议 TensorFlow A proposal of good practices for 
files folders and models architecture by Morgan 链接 GitHub 带有 
快速 局部 滤波 的 图 CNN Convolutional Neural Networks on 
Graphs with Fast Localized Spectral Filtering M Defferrard X Bresson 
P Vandergheynst EPFL 2016 链接 GitHub Tensorflow / TFLearn RNN 
命名 实体 识别 Named Entity Recognition using Recurrent Neural Networks 
in Tensorflow and TFLearn by Dhwaj Raj 链接 GitHub 深度 
学习 的 局限性 Failures of Deep Learning S Shalev Shwartz 
O Shamir S Shammah The Hebrew University & Weizmann Institute 
2017 链接 GitHub video 基于 矩阵 乘法 的 并行 多通道 
卷积 Parallel Multi Channel Convolution using General Matrix Multiplication A 
Vasudevan A Anderson D Gregg Trinity College Dublin 2017 链接 
GitHub 在 手机 上 进行 深度 学习 训练 Migrate Deep 
Learning Training onto Mobile Devices by Saman BigManborn 链接 GitHub 
TensorFlow 实现 的 RNN LSTM 序列 预测 tensorflow lstm regression 
Sequence prediction using recurrent neural networks LSTM with TensorFlow by 
mouradmourafiq 链接 GitHub TensorFlow 1 . 1.0 发布 TensorFlow 1 
. 1.0 Released 链接 GitHub CNN 到 图 结构 数据 
的 推广 A Generalization of Convolutional Neural Networks to Graph 
Structured Data Y Hechtlinger P Chakravarti J Qin CMU 2017 
链接 GitHub Momenta 研发 总监 任 少卿 From Faster R 
CNN to Mask R CNN 链接 GitHub Deep Multitask Learning 
for Semantic Dependency Parsing H Peng S Thomson N A 
. Smith CMU 2017 链接 GitHub 利用 整流 单元 稀疏 
性 加快 卷积 神经网络 Speeding up Convolutional Neural Networks By 
Exploiting the Sparsity of Rectifier Units S Shi X Chu 
Hong Kong Baptist University 2017 链接 GitHub 深度 学习 之 
CNN 卷积 神经网络 Deep Learning # 2 Convolutional Neural Networks 
by Rutger Ruizendaal 链接 GitHub PyTorch 试炼 场 提供 各 
主流 预 训练 模型 pytorch playground Base pretrained model and 
datasets in pytorch MNIST SVHN CIFAR10 CIFAR100 STL10 AlexNet VGG16 
VGG19 ResNet Inception SqueezeNet by Aaron Chen 链接 GitHub 从自/nr 
编码器 到 生成 对抗 网络 一文 纵览 无 监督 学习 
研究 现状 链接 GitHub 2017 . 05.09 Learning Deep Learning 
with Keras 链接 GitHub TensorFlow 生成 模型库 A Library for 
Generative Models 链接 GitHub 深度 学习 的 过去 现在 和 
未来 Deep Learning – Past Present and Future by Henry 
H . Eckerson 链接 GitHub 正在 涌现 的 新型 神经网络 
模型 优于 生成 对抗 网络 链接 GitHub 贝叶斯 深度 学习 
文献 列表 A curated list of resources dedicated to bayesian 
deep learning by Rabindra Nath Nandi 链接 GitHub 面向 推荐 
系统 的 深度 学习 文献 列表 Deep Learning for Recommendation 
Systems Deep Learning based articles paper and repositories for Recommender 
Systems by Rabindra Nath Nandi 链接 GitHub 2017 . 05.10 
深度 学习 职位 面试 经验 分享 My deep learning job 
interview experience sharing by Justin Ho 链接 GitHub Convolutional Sequence 
to Sequence Learning J Gehring M Auli D Grangier D 
Yarats Y N . Dauphin Facebook AI Research 2017 链接 
GitHub VGG19 的 TensorFlow 实现 / 详解 VGG19 _ with 
_ tensorflow An easy implement of VGG19 with tensorflow which 
has a detailed explanation . by Jipeng Huang 链接 GitHub 
Keras 实现 的 深度 聚 类 Keras implementation of Deep 
Clustering paper by Eduardo Silva 链接 GitHub 1.3 机器学习 2017 
. 05.07 无 监督 学习 纵览 Navigating the Unsupervised Learning 
Landscape by Eugenio Culurciello 链接 GitHub Python 机器学习 导论 课程 
资料 Materials for the Introduction to Machine Learning class by 
Andreas Mueller 链接 GitHub Newton ADMM 快速 准 平滑 牛顿 
法 A Newton ADMM based solver for Cone programming . 
链接 GitHub 超大规模 机器学习 工具集 MaTEx Machine Learning Toolkit for 
Extreme Scale MaTEx a collection of high performance parallel machine 
learning and data mining MLDM algorithms targeted for desktops supercomputers 
and cloud computing systems 链接 GitHub 关于 迁移 学习 的 
一些 资料 链接 GitHub Clustering with Adaptive Structure Learning A 
Kernel Approach Z Kang C Peng Q Cheng Southern Illinois 
University 2017 链接 GitHub R 稀疏 贝叶斯 网络 学习 sparsebn 
Software for learning sparse Bayesian networks by Bryon Aragam 链接 
GitHub Node . js 机器学习 / 自然语言 处理 / 情感 
分析 工具包 salient Machine Learning Natural Language Processing and Sentiment 
Analysis Toolkit for Node . js by Thomas Holloway 链接 
GitHub Explaining the Success of AdaBoost and Random Forests as 
Interpolating Classifiers 链接 GitHub 机器学习 中 容易 犯下 的 错 
链接 GitHub 2017 . 05.08 C / C + + 
and MATLAB / Octave 互信息 函数 工具箱 MIToolbox Mutual Information 
functions for C and MATLAB by Adam Pocock 链接 GitHub 
Criteo 1TB 数据 集上 多 机器学习 算法 Benchmark Benchmark of 
different ML algorithms on Criteo 1TB dataset by Rambler Digital 
Solutions 链接 GitHub 机器学习 十大 常用 算法 链接 GitHub 加速 
随机 梯度 下降 Accelerating Stochastic Gradient Descent P Jain S 
M . Kakade R Kidambi P Netrapalli A Sidford Microsoft 
Research & University of Washington & Stanford University 2017 链接 
GitHub C + + 大 规模 稀疏 矩阵 分 解包 
LIBMF library for large scale sparse matrix factorization by cjlin1 
链接 GitHub C / Python / Matlab 求解 大 规模 
正则 线性 分类 与 回归 的 简单 包 LIBLINEAR simple 
package for solving large scale regularized linear classification and regression 
by cjlin1 链接 GitHub 批量 归一化 Batch Norm 概述 Appendix 
A Batch Norm Overview by alexirpan 链接 GitHub 2017 . 
05.09 谱 聚 类 链接 GitHub 2017 . 05.10 学 
习非 极大值 抑制 Learning non maximum suppression J Hosang R 
Benenson B Schiele Max Planck Institut f ü r Informatik 
2017 链接 GitHub Python 机器学习 工作流 框架 AlphaPy Machine Learning 
Pipeline for Python by ScottFree Analytics 链接 GitHub 如何 解释 
机器学习 模型 和 结果 Ideas on interpreting machine learning | 
O Reilly Media by Patrick HallWen Phan SriSatish Ambati 链接 
GitHub 2 计算机 视觉 2017 . 04.21 OpenCV / Python 
/ dlib 人脸 关键点 实时 标定 paper GitHub 2017 . 
04.22 高效 的 卷积 神经 网络 在 手机 中 的 
应用 MobileNets Efficient Convolutional Neural Networks for Mobile Vision Applications 
paper GitHub 生成式 人脸 补全 Generative Face Completion Y Li 
S Liu J Yang M H Yang Univerisity of California 
Merced & Adobe Research 2017 paper GitHub Computer Vision for 
Autonomous Vehicles Problems Datasets and State of the Art J 
Janai F G ü ney A Behl A Geiger Max 
Planck Institute for Intelligent Systems & ETH Zurich 2017 paper 
GitHub Tracking the Trackers An Analysis of the State of 
the Art in Multiple Object Tracking L Leal Taix é 
A Milan K Schindler D Cremers I Reid S Roth 
Technical University Munich & University of Adelaide & ETH Zurich 
& TU Darmstadt 2017 译 多目标 追踪 的 现状 分析 
paper GitHub CNN SLAM Real time dense monocular SLAM with 
learned depth prediction K Tateno F Tombari I Laina N 
Navab CAMP TU Munich 2017 paper GitHub Realtime Multi Person 
2D Pose Estimation using Part Affinity Fields Z Cao T 
Simon S Wei Y Sheikh CMU 2016 译 基于 PAF 
的 实时 二维 姿态 估计 paper GitHub Virtual to Real 
Reinforcement Learning for Autonomous Driving Y You X Pan Z 
Wang C Lu Shanghai Jiao Tong University & UC Berkeley 
& Tsinghua University 2017 paper GitHub Semantic3D . net A 
new Large scale Point Cloud Classification Benchmark T Hackel N 
Savinov L Ladicky J D . Wegner K Schindler M 
Pollefeys ETH Zurich 2017 paper GitHub Learning Video Object Segmentation 
with Visual Memory P Tokmakov K Alahari C Schmid Inria 
2017 paper GitHub 2017 . 04.23 A Brief History of 
CNNs in Image Segmentation From R CNN to Mask R 
CNN by Dhruv Parthasarathy paper GitHub Stacked Hourglass Networks for 
Human Pose Estimation A Newell K Yang J Deng University 
of Michigan 2016 paper GitHub 自动驾驶 计算机 视觉 研究 综述 
难题 数据集 与 前沿 成果 附 67页 论文 下载 paper 
GitHub 谷歌 推出 最新 手机 版 视觉 应用 的 卷积 
神经网络 MobileNets paper GitHub Deep Learning for Photo Editing by 
Malte Baumann paper GitHub 2017 . 04.24 TensorFlow Implementation of 
conditional variational auto encoder CVAE for MNIST by hwalsuklee paper 
GitHub 2017 . 04.26 单目 视频 深度 帧 间 运动 
估计 无 监督 学习 框架 SfMLearner An unsupervised learning framework 
for depth and ego motion estimation from monocular videos by 
T Zhou paper GitHub U Nets Caffe paper GitHub U 
Net Convolutional Networks for Biomedical Image Segmentation 2015 paper GitHub 
3D U Net Learning Dense Volumetric Segmentation from Sparse Annotation 
paper GitHub 2017 . 05.07 C + + / Matlab 
视频 / 图片 序列 人脸 标定 Find Face Landmarks C 
+ + \ Matlab library for finding face landmarks and 
bounding boxes in video \ image sequences . by Yuval 
Nirkin paper GitHub Keras UNET 图像 分割 ZF _ UNET 
_ 224 Pretrained Model Modification of convolutional neural net UNET 
for image segmentation in Keras framework by ZFTurbo paper GitHub 
复杂 条件 下 的 深度 人脸 分割 Deep face segmentation 
in extremely hard conditions by Yuval Nirkin paper GitHub 基于 
单目 RGB 图像 的 实时 3D 人体 姿态 估计 VNect 
Real time 3D Human Pose Estimation with a Single RGB 
Camera D Mehta S Sridhar O Sotnychenko Max Planck Institute 
for Informatics & Universidad Rey Juan Carlos 2017 paper paper2 
GitHub 衣服 检测 与 识别 DeepFashion Powering Robust Clothes Recognition 
and Retrieval with Rich Annotations Z Liu P Luo S 
Qiu X Wang X Tang CVPR 2016 paper paper2 GitHub 
SLAM 学习 与 开发 经验 分享 paper GitHub 大 规模 
街道 级 图片 分割 数据集 Releasing the World s Largest 
Street level Imagery Dataset for Teaching Machines to See by 
Peter Kontschieder paper GitHub dataset 基于 深度 增强 学习 的 
交叉 路口 车辆 自动 导航 Navigating Intersections with Autonomous Vehicles 
using Deep Reinforcement Learning D Isele A Cosgun K Subramanian 
K Fujimura University of Pennsylvania & Honda Research Institute & 
Georgia Institute of Technology 2017 paper GitHub 十分钟 看懂 图像 
语义 分割 技术 paper GitHub C + + 实时 多人 
关键点 检测 OpenPose A Real Time Multi Person Keypoint Detection 
And Multi Threading C + + Library paper GitHub 计算机 
视觉 机器学习 相关 领域 论文 和 源代码 大集合 paper GitHub 
Tensorflow RPN + 人体 检测 RPNplus RPN + Tensorflow for 
people detection by Shiyu Huang paper GitHub C + + 
/ OpenCV3 实时 可变 人脸 追踪 Real time deformable face 
tracking in C + + with OpenCV 3 . by 
Kyle McDonald paper GitHub 图片 快速 标记 How to Label 
Images Quickly by Pete Warden paper paper2 GitHub 基于 深度 
图像 类比 的 视觉 要素 迁移 Visual Attribute Transfer through 
Deep Image Analogy J Liao Y Yao L Yuan G 
Hua S B Kang Microsoft Research & Shanghai Jiao Tong 
University 2017 paper GitHub 基于 深度 学习 的 质谱 成像 
中的 肿瘤 分类 Deep Learning for Tumor Classification in Imaging 
Mass Spectrometry J Behrmann C Etmann T Boskamp R Casadonte 
J Kriegsmann P Maass University of Bremen & Proteopath GmbH 
2017 paper link2 GitHub Andorid 手机 上 基于 TensorFlow 的 
人体 行为 识别 Deploying Tensorflow model on Andorid device for 
Human Activity Recognition by Aaqib Saeed paper paper2 GitHub TensorFlow 
图像 自动 描述 Caption this with TensorFlow | O Reilly 
Media by Raul Puri Daniel Ricciardelli paper paper2 GitHub 基于 
CNN InceptionV1 + STFT 的 Kaggle 鲸鱼 检测 竞赛 方案 
CNN InceptionV1 + STFT based Whale Detection Algorithm A whale 
detector design for the Kaggle whale detector challenge by Tarin 
Ziyaee paper GitHub TensorFlow 实现 的 摄像头 pix2pix 图图 转换 
webcam pix2pix Tensorflow Source code and pretrained model for webcam 
pix2pix by Memo Akten paper GitHub 图像 分类 的 大规模 
进化 Large Scale Evolution of Image Classifiers E Real S 
Moore A Selle S Saxena Y L Suematsu Q Le 
A Kurakin Google Brain & Google Research 2017 paper paper2 
GitHub 2017 . 05.08 人脸 检测 与 识别 的 趋势 
和 分析 paper GitHub 全局 / 局部 一致 图像 补全 
Globally and Locally Consistent Image Completion S Iizuka E Simo 
Serra H Ishikawa 2017 paper GitHub 基于 CNN 的 面部 
表情 识别 Convolutional Neural Networks for Facial Expression Recognition S 
Alizadeh A Fazel Stanford University 2017 paper GitHub 计算机 视觉 
识别 简史 从 AlexNet ResNet 到 Mask RCNN paper GitHub 
脸部 识别 与 聚 类 Face Identification and Clustering A 
Dhingra The State University of New Jersey 2017 paper GitHub 
TensorFlow 通用 U Net 图像 分割 Tensorflow Unet Generic U 
Net Tensorflow implementation for image segmentation by Joel Akeret paper 
GitHub 深度 学习 介绍 之 文本 图像 生成 How to 
Convert Text to Images Intro to Deep Learning # 16 
YouTube by Siraj Raval paper GitHub 一个 深度 神经网络 如何 
对 自动 驾驶 做 端 到 端的 训练 Explaining How 
a Deep Neural Network Trained with End to End Learning 
Steers a Car M Bojarski P Yeres A Choromanska K 
Choromanski B Firner L Jackel U Muller NVIDIA Corporation & 
New York University & Google Research 2017 paper GitHub 基于 
深度 卷积 网络 的 动态 场景 关节 语义 与 运动 
分割 Joint Semantic and Motion Segmentation for dynamic scenes using 
Deep Convolutional Networks N Haque N D Reddy K . 
M Krishna International Institute of Information Technology & Max Planck 
Institute For Intelligent Systems 2017 paper GitHub 高分辨率 图像 的 
实时 语义 分割 ICNet for Real Time Semantic Segmentation on 
High Resolution Images H Zhao X Qi X Shen J 
Shi J Jia The Chinese University of Hong Kong & 
SenseTime Group Limited 2017 paper GitHub GitHub2 video 深度 学习 
应用到 语义 分割 的 综述 A Review on Deep Learning 
Techniques Applied to Semantic Segmentation A Garcia Garcia S Orts 
Escolano S Oprea V Villena Martinez J Garcia Rodriguez University 
of Alicante 2017 paper GitHub 医学 图像 的 深度 迁移 
学习 的 原理 Understanding the Mechanisms of Deep Transfer Learning 
for Medical Images H Ravishankar P Sudhakar R Venkataramani S 
Thiruvenkadam P Annangi N Babu V Vaidya GE Global Research 
2017 paper GitHub Torch 基于 循环 一致 对抗 网络 的 
非 配对 图 到 图 翻译 paper GitHub 深度 网络 
光 流 估计 的 演化 FlowNet 2.0 Evolution of Optical 
Flow Estimation with Deep Networks E Ilg N Mayer T 
Saikia M Keuper A Dosovitskiy T Brox University of Freiburg 
2016 paper GitHub video 基于 p RNN 的 目标 实例 
标注 Annotating Object Instances with a Polygon RNN L Castrejon 
K Kundu R Urtasun S Fidler University of Toronto 2017 
paper GitHub Dataset Augmentation for Pose and Lighting Invariant Face 
Recognition D Crispell O Biris N Crosswhite J Byrne J 
L . Mundy Vision Systems Inc & Systems and Technology 
Research 2017 paper GitHub 人脸 的 分割 交换 与 感知 
On Face Segmentation Face Swapping and Face Perception Y Nirkin 
I Masi A T Tran T Hassner G Medioni The 
Open University of Israel & USC 2017 paper GitHub 面向 
视频 运动 估计 的 几何 感知 神经网络 SfM Net SfM 
Net Learning of Structure and Motion from Video S Vijayanarasimhan 
S Ricco C Schmid R Sukthankar K Fragkiadaki Google & 
Indri & CMU 2017 paper GitHub 基于 深度 自 学习 
的 弱 监督 目标 定位 Deep Self Taught Learning for 
Weakly Supervised Object Localization Z Jie Y Wei X Jin 
J Feng W Liu Tencent AI Lab & National University 
of Singapore 2017 paper GitHub 单个 图像 的 手部 关键点 
检测 Hand Keypoint Detection in Single Images using Multiview Bootstrapping 
T Simon H Joo I Matthews Y Sheikh CMU 2017 
paper GitHub Hierarchical 3D fully convolutional networks for multi organ 
segmentation H R . Roth H Oda Y Hayashi M 
Oda N Shimizu M Fujiwara K Misawa K Mori Nagoya 
University & Nagoya University Graduate School of Medicine & Aichi 
Cancer Center 2017 paper GitHub Towards Large Pose Face Frontalization 
in the Wild X Yin X Yu K Sohn X 
Liu M Chandraker Michigan State University & NEC Laboratories America 
& University of California San Diego 2017 paper paper2 GitHub 
通过观察 目标 运动 迁移 学习 特征 Learning Features by Watching 
Objects Move D Pathak R Girshick P Doll á r 
T Darrell B Hariharan Facebook AI Research & UC Berkeley 
2016 paper GitHub 面向 深度 学习 训练 的 视频 标记 
工具 BeaverDam Video annotation tool for deep learning training labels 
by Anting Shen paper GitHub 生成 对抗 网络 GAN 图片 
编辑 Photo Editing with Generative Adversarial Networks | Parallel Forall 
by Greg Heinrich paper paper2 GitHub 解读 Keras 在 ImageNet 
中的 应用 详解 5种 主要 的 图像 识别 模型 paper 
GitHub Adversarial PoseNet A Structure aware Convolutional Network for Human 
Pose Estimation Y Chen C Shen X Wei L Liu 
J Yang Nanjing University of Science and Technology & The 
University of Adelaide & Nanjing University 2017 paper GitHub 结构 
感知 卷积 网络 的 人体 姿态 估计 Adversarial PoseNet A 
Structure aware Convolutional Network for Human Pose Estimation Y Chen 
C Shen X Wei L Liu J Yang Nanjing University 
of Science and Technology & The University of Adelaide & 
Nanjing University 2017 paper GitHub 基于 神经 网络 的 鲁棒 
多视角 行人 跟踪 Robust Multi view Pedestrian Tracking Using Neural 
Networks M Z Alom T M . Taha University of 
Dayton 2017 paper GitHub 视频 密集 事件 描述 Dense Captioning 
Events in Videos paper GitHub data 受 Siraj Raval 深度 
学习 视频 启发 的 每周 深度 学习 实践 挑战 Deep 
Learning Challenges Codes for weekly challenges on Deep Learning by 
Siraj by Batchu Venkat Vishal paper GitHub SLAM with Objects 
using a Nonparametric Pose Graph B Mu S Liu L 
Paull J Leonard J How MIT 2017 paper GitHub 医学 
图像 分割 中 迭代 估计 的 归一化 输入 Learning Normalized 
Inputs for Iterative Estimation in Medical Image Segmentation M Drozdzal 
G Chartrand E Vorontsov L D Jorio A Tang A 
Romero Y Bengio C Pal S Kadoury Universite de Montreal 
& Imagia Inc 2017 paper GitHub An Analysis of Action 
Recognition Datasets for Language and Vision Tasks S Gella F 
Keller University of Edinburgh 2017 paper GitHub 2017 . 05.09 
Tensorflow 实现 卷积 神经网络 用于 人脸 关键点 识别 paper GitHub 
FRCN faster rcnn 文字 检测 Text Detection using py faster 
rcnn framework by jugg1024 paper GitHub 手机 单目 视觉 状态 
估计 器 VINS Mobile Monocular Visual Inertial State Estimator on 
Mobile Phones by HKUST Aerial Robotics Group paper GitHub R 
FCN 目标 检测 R FCN Object Detection via Region based 
Fully Convolutional Networks paper GitHub 行人 检测 跟踪 与 检索 
领域 年度 进展 报告 paper GitHub TensorFlow 点 云 Point 
Cloud 分类 分割 场景 语义 理解 统一 框架 PointNet PointNet 
Deep Learning on Point Sets for 3D Classification and Segmentation 
paper paper2 GitHub GitHub2 深度 视频 去模糊 Deep Video Deblurring 
by Shuochen Su 2016 paper paper2 GitHub video 中国 的 
Infervision 及其 肺癌 诊断 AI 工具 Chinese startup Infervision emerges 
from stealth with an AI tool for diagnosing lung cancer 
| TechCrunch by Jonathan Shieber paper paper2 GitHub 基于 医院 
大量 胸部 x 射线 数据库 的 弱 监督 分类 和 
常见 胸部 疾病 定位 的 研究 ChestX ray8 Hospital scale 
Chest X ray Database and Benchmarks on Weakly Supervised Classification 
and Localization of Common Thorax Diseases X Wang Y Peng 
L Lu Z Lu National Institutes of Health 2017 paper 
paper2 GitHub 目标 跟踪 方法 的 发展 概述 paper GitHub 
Caffe 实时 交互式 图片 自动 着色 Real Time User Guided 
Image Colorization with Learned Deep Priors UC Berkeley 2017 paper 
paper2 GitHub video 相术 的 新衣 Physiognomy s New Clothes 
by Blaise Aguera y Arcas paper GitHub 2017 . 05.10 
快速 生成 人脸 模型 paper paper2 GitHub 预计 八月 开源 
VALSE2017 系列 之二 边缘 检测 领域 年度 进展 报告 paper 
GitHub GTC2017 Stanford 发布 0 . 5PB 大规模 放射 医疗 
图像 ImageNet 数据集 Stanford gave the world ImageNet . Now 
it s giving the world Medical ImageNet a 0 . 
5PB dataset for diagnostic radiology via James Wang paper GitHub 
医疗 图像 深度 学习 Medical Image Analysis with Deep Learning 
by Taposh Dutta R o y P a r t 
1 P a r t 2 P a r t 
3 激光雷达 LIDAR 自驾车 关键 传感器 An Introduction to LIDAR 
The Key Self Driving Car Sensor by Oliver Cameron paper 
GitHub 根据 目标 脸 生成 带 语音 的 视频 You 
said that J S Chung A Jamaludin A Zisserman University 
of Oxford 2017 paper GitHub 用于 图像 生成 和 数据 
增强 的 生成 协作网 Generative Cooperative Net for Image Generation 
and Data Augmentation Q Xu Z Qin T Wan Beihang 
University & Alibaba Group 2017 paper GitHub COCO 像素 级 
标注 数据集 The official homepage of the COCO Stuff dataset 
. paper GitHub COCO Stuff Thing and Stuff Classes in 
Context 2017 paper GitHub LinkNet 基于 编码器 表示 的 高效 
语义 分割 LinkNet Feature Forwarding Exploiting Encoder Representations for Efficient 
Semantic Segmentation A Chaurasia E Culurciello paper GitHub GitHub2 3 
自然语言 处理 2017 . 04.22 Semantic Instance Segmentation via Deep 
Metric Learning A Fathi Z Wojna V Rathod P Wang 
H O Song S Guadarrama K P . Murphy Google 
Inc & UCLA 2017 paper GitHub 2017 . 04.26 对话 
语料 集 chat corpus collection from various open sources by 
Marsan Ma paper GitHub 2017 . 05.07 从 文本 中 
提取 特征 的 神经 网络 技术 综述 A Survey of 
Neural Network Techniques for Feature Extraction from Text V John 
University of Waterloo 2017 基 於 向量 匹配 的 情境 
式 聊天 機 器 人 by Justin Yang paper GitHub 
PyTorch 实践 序 列到 序列 Attention 法 英 翻译 Practical 
PyTorch Translation with a Sequence to Sequence Network and Attention 
by Sean Robertson paper GitHub PyTorch 实践 探索 GloVe 词 
向量 Practical PyTorch Exploring Word Vectors with GloVe by Sean 
Robertson paper GitHub 自然语言 生成 NLG 系统 评价 指标 How 
to do an NLG Evaluation Metrics by Ehud Reiter paper 
paper2 GitHub 看似 靠谱 的 文本 分类 对抗 样本 textfool 
Plausible looking adversarial examples for text classification by Bogdan Kulynych 
paper GitHub 基于 bidirectional GRU CRF 的 联合 中文分词 与 
词性 标注 A Joint Chinese segmentation and POS tagger based 
on bidirectional GRU CRF by yanshao9798 paper GitHub 自然语言 处理 
NLP 入门 指南 How to get started in NLP by 
Melanie Tosik paper GitHub 2017 . 05.08 TensorFlow 面向 文本 
相似 度 检测 的 Deep LSTM siamese 网络 Deep LSTM 
siamese network for text similarity Tensorflow based implementation of deep 
siamese LSTM network to capture phrase / sentence similarity using 
character embeddings by Dhwaj Raj paper GitHub Keras / TensorFlow 
语种 检测 Deep Learning Language identification using Keras & TensorFlow 
by Lucas KM paper GitHub C + + 神经网络 语种 
检测工具 Compact Language Detector v3 CLD3 neural network model for 
language identification by Google paper GitHub 用于 文本 分类 的 
端 到 端 多 视图 网络 End to End Multi 
View Networks for Text Classification H Guo C Cherry J 
Su National Research Council Canada 2017 paper GitHub 理解 非 
结构化 文本 数据 Making Sense of Unstructured Text Data L 
Li W M . Campbell C Dagli J P . 
Campbell MIT Lincoln Laboratory 2017 paper GitHub 非 本族 语者 
英语 写作 风格 检测 Detecting English Writing Styles For Non 
Native Speakers Y Chen R Al Rfou Y Choi Stony 
Brook University 2017 paper GitHub 2017 . 05.10 Facebook 提出 
全新 CNN 机器翻译 准确度 超越 谷歌 而且 还快 九倍 已 
开源 paper1 paper2 GitHub 4 应用 案例 2017 . 04.21 
深度 学习 入门 实战 一 像 Prisma 一样 算法 生成 
梵高 风格 画像 paper GitHub 2017 . 04.22 我们 教 
电脑 识别 视频 字幕 paper GitHub 2017 . 04.24 Data 
Sciencing Motorcycles Lean Assist by Josh Peng paper GitHub 2017 
. 04.26 PhotoScan 新增 的 去除 翻拍 反光 功能 PhotoScan 
Taking Glare Free Pictures of Pictures | Google Research Blog 
by Ce Liu Michael Rubinstein Mike Krainin Bill Freeman paper 
GitHub 2017 . 05.08 假 新闻 的 实时 检测 How 
to Detect Fake News in Real Time by Krishna Bharat 
paper GitHub 5 综合 5.1 教程 2017 . 04.21 30 
Free Courses Neural Networks Machine Learning Algorithms AI paper GitHub 
2017 . 04.22 Deep Learning 英文 原文 link 中文 译文 
link 中文 译文 说明 link 2017 . 04.23 机器学习 Machine 
Learning & 深度 学习 Deep Learning 资料 Chapter 1 paper 
GitHub 2017 . 05.07 台大 李宏 毅 中文 深度 学习 
课程 2017 NTUEE Machine Learning and having it Deep and 
Structured MLDS 2017 paper GitHub video TensorFlow 教程 paper GitHub 
2017 . 05.08 Keras 教程 Python 深度 学习 Keras Tutorial 
Deep Learning in Python by Karlijn Willems paper GitHub 2017 
. 05.09 用 Anaconda 玩转 深度 学习 Deep Learning with 
Anaconda AnacondaCON 2017 YouTube by Stan Seibert & Matt Rocklin 
paper GitHub video 5.2 其它 2017 . 04.23 哥伦比亚 大学 
与 Adobe 提出 新 方法 可将 随机 梯度 下降 用作 
近似 贝叶斯 推理 paper GitHub 英特尔 深度 学习 产品 综述 
如何 占领 人工智能 市场 paper GitHub 2017 . 04.24 28款 
GitHub 最 流行 的 开源 机器学习 项目 TensorFlow 排 榜首 
paper GitHub 2017 . 04.26 英国皇家学会 百页 报告 机器 学习 
的 力量 与 希望 豪华 阵容 参与 完成 paper GitHub 
深度 学习 在 推荐 算 法上 的 应用 进展 paper 
GitHub 周志华 教授 gcForest 多 粒度 级联 森林 算法 预测 
股指 期货 涨跌 paper GitHub 2017 . 05.07 市值 250亿 
的 特征向量 谷歌 背后 的 线性代数 paper GitHub 可 重现 
/ 易 分享 数据 科学 项目 框架 DVC Data Version 
Control Make your data science projects reproducible and shareable paper 
GitHub Fast k means based on KNN Graph C Deng 
W Zhao Xiamen University 2017 paper GitHub 信息检索 人工神经网络 模型 
Neural Models for Information Retrieval B Mitra N Craswell Microsoft 
2017 paper GitHub 地平线 机器人 杨铭/nr 深度 神经 网络 在 
图像 识别 应用 中 的 演化 paper GitHub Python Facebook 
的 开源 AI 对话 研究 框架 ParlAI A framework for 
training and evaluating AI models on a variety of openly 
available dialog datasets . paper GitHub Python 深度 神经网络 多 
标签 文本 分类 框架 magpie Deep neural network framework for 
multi label text classification by inspirehep paper GitHub 300万 Instacart 
在线 杂货 购物 数据集 3 Million Instacart Orders Open Sourced 
by Jeremy Stanley paper GitHub 基于 语言 / 网络 结构 
的 推荐 系统 GraphNet GraphNet Recommendation system based on language 
and network structure R Ying Y Li X Li Stanford 
University 2017 paper GitHub 2017 . 05.08 将 Python 3 
. x 代码 转换成 Python2 . x 代码 的 Python 
Python 编译器 Py backwards Python to python compiler that allows 
you to use Python 3.6 features in older versions . 
by Vladimir Iakovlev paper GitHub 2017 . 05.09 Xgboost 新增 
GPU 加速 建树 算法 Xgboost GPU CUDA Accelerated Tree Construction 
Algorithm paper GitHub 独立 开发 者 赚钱 资料 集锦 awesome 
indie Resources for independent developers to make money by Joan 
Boixad ó s paper GitHub 基于 MAPD / Anaconda / 
H2O 的 GPU 数据分析 框架 GPU Data Frame with a 
corresponding Python API paper GitHub 从 文本 到 视觉 各 
领域 最 前沿 的 论文 集合 paper GitHub 2017 . 
05.10 C + + 信息检索 框架 库 Trinity Trinity IR 
Infrastructure by Phaistos Networks GitHub paper GitHub 参考 爱 可可 
爱 生活 机器 之心 synced 机器学习 研究会 小伙伴 们 注意 
了 小 编 在 这里 给 大家 送 上 关注 
福利 转发 本文 关注 + 私信 小 编 资料 即可 
领取 小 编 精心 准备 的 资料 一份 英语 文本 
几乎 无处不在 如果 我们 的 系统 能够 理解 并 自动 
生成 它 那将 是 最好 的 然而 理解 自然 语言 
是 一项 复杂 的 任务 它 是 如此 复杂 以至于 
许多 研究 人员 花了/nr 一生 的 时间 来做 它 现在 
已经 发布 了 很多 工具 来 完成 自然语言 处理工作 以下 
是 我 收集 的 8个 工具 我 还 验证 了 
它们 都被/nr 某些 应用程序 至少 使 用 一次 因此 它们 
都是 可 运行 的 有些 来自 工业 公司 有些 来自 
研究 机构 它 提供 了 解析 自动 查找 主题 等功能 
OpenNLP a Java package to do text tokenization part of 
speech tagging chunking etc . tutorial Stanford Parser a Java 
implementation of probabilistic natural language parsers both highly optimized PCFG 
* and lexicalized dependency parsers and a lexicalized PCFG parserScalaNLP 
Natural Language Processing and machine learning . Snowball a stemmer 
support C and Java . MALLET a Java based package 
for statistical natural language processing document classification clustering topic modeling 
information extraction and other machine learning applications to text . 
JGibbLDA LDA in JavaApache Lucene Core a Java library for 
stop words removal and stemmingStanford Topic Modelling   Toolbox   
CVB0 algorithm etc . 最后 想 学习 Java 的 小伙伴 
们 转发 转发 后 关注 + 私信 回复 资料 就 
可以 拿到 一份 我 为 大家 准备 的 Java 学习 
资料 小 编 V ❤ suxueJava 目录 文章 目录 目录 
前言 汉语 语料库 的 多级 加工 1 汉语 语料库 的 
多级 加工 2 汉语 语料库 的 多级 加工 3 汉语 
语料库 的 多级 加工 4 汉语 语料库 的 多级 加工 
5 汉语 语料库 的 多级 加工 6 汉语 语料库 的 
多级 加工 7 前言 硕士 生涯 结束 开始 专心 做 
一件 自己 觉得 有用 的 工具 先 做 工程 后搞/nr 
理论 自然语言 处理 是 一个 非常 难 的 问题 同时 
是 人工智能 皇冠 上 的 明珠 接下来 会 记录 一 
系列 自然语言 处理 的 笔记 来自 于 哈工大 老师 关毅 
汉语 语料库 的 多级 加工 1 两条路线 基于 规则 的 
和 基于 模型 的 路线 { 基于 规则 的 if 
基于 模型 的 if 路线 \ begin { cases } 
基于 规则 的 & amp \ text { if } 
\ \ 基于 模型 的 & amp \ text { 
if } \ end { cases } 路线 { 基于 
规则 的 基于 模型 的 if   if 语料库 语言学 
经常 使用 概率 统计 及 信息论 中的 方法 语料库 规模 
对 效果 影响 很大 语料 分布 语音识别 情况 处理 句法分析/l 
和/c 语料/n 分析/vn 各/r 项/n 处理/v 句法分析 加工 停止 语料 
多级 加工 停止 预料 多级 加工 实例 语法树 规范 语料库 
加工 切分 词性 标准 人民日报 语料库 未 登录 词 和 
命名 实体 汉语 语料库 的 多级 加工 2 中文 人名 
的 识别方法 人名 用字 比较 集中 定义 和 使用 型 
识别 人名 姓氏 中文 资料 人名 识别 词性 修剪 传统 
的 规则 方法 中文 的 黄 可以 做 名词 可以 
做 形容词 有效 的 方法 是 两种 东西 的 结合 
以 坚实 的 理论 基础 做 架构 从而 实现 的 
极大 熵 模型 英语 译名 手册 考察 上下文 信息 统计 
机器翻译 地名 识别 得 资源 者 得 一切 中文 信息 
的 翻译 汉语 语料库 的 多级 加工 3 采用 一种 
统计 分类 模型 进行 统计 处理 定位 词 + 中心词 
识别 命名 实体 辅助 规则 坚实 的 理论 模型 统计 
规则 最大熵 模型 特征 模板 系数 规则 生成器 系统 在 
满足 约束 的 情况 下 熵 趋向于 最大 任何 原理 
都有 适用范围 平常 要 多做 应用 调 参数 调 多了 
自然 就 有 感觉 了 统计 分类 模型 目前 的 
效果 超 好 推荐 一篇 论文 A maximum entropy approach 
to natural language processing 下载 源代码 后 做一个 软件包 进行 
实时 的 处理 好好 写东西 汉语 语料库 的 多级 加工 
4 汉语 的 兼类 词 动名词 名 形容词 动 名 
兼类 37% 基于 规则 的 词性 标注 词性 多重 修改 
词 性相 同类 举 基于 隐 markov 模型 效果 超 
好 选择 训练 集 构建 训练样本 训练 集 异常 重要 
garbage in 就会 garbage out 关键 数据 结果 训练 机器学习 
标记 不同 值 未经 标注 的 文本 初始 标准 器 
已经 标注 的 样本 学习 器 纠错 规则 黄金 标注 
文本 转换规则 转发 规则 原 tag + 环境 目标 tag 
汉语 语料库 的 多级 加工 5 词性 标注 的 模板 
规则 的 颗粒度 不同 转移 数量 标注 精度 选择 这样 
的 工作 TBL 效果 良好 精度 有限 好 的 标注 
器 基于 决策树 的 方法 效果 良好 句法分析 的 总体 
结构 如上 图 所示 句法 词性 处理 结构 转换 自动 
短语 定界 和 句法 标注 实例 语义 难 涉及 到 
一个 核心 问题 意义 的 意义 是 什么 汉语 语料库 
的 多级 加工 6 语义 与 语法 的 关系 语法 
是 形式 语义 是 内容 自动 语义 标注 语法 标注 
和 语义 标注 听到 词汇 后 分词 再 进行 处理 
词 与 词 的 关系 概念 在 头脑 中 正确 
联系 汉语 语言 判定 容易 以 单位 词 的 词义 
定义 多义词 的 词义 比较 方便 莱 斯克 以 单位 
词源 定义 多义 词源 词义 分析 很难 利用 上下文 的 
搭配 关系 确定 该词 的 关系 深层 语言 结构 效果 
超 好 词汇 间 的 语义 关系 是 词汇 的 
灵魂 整体 关系 和 上下级 关系 汉语 语料库 的 多级 
加工 7 语义 标注 实例 semantic tree 语义 树 任何 
一个 实体 所有 的 属性 比如 他 在 书店 看书 
AGT word _ no = 0 他 r rrl 他 
就 完全 被 定义 了 众多 学者 号召 做出 千万级 
的 语料库 计算 语言学 的 基础理论 十 万句 句法 休整 
语义 知识库 英语 framenet 语言 架构 自动 切 词 标注 
了 系统 语料库 多级 加工 系统 人 的 精力 得到 
解放 简单 搜索 AI 作为 内核 算法 算 力 大 
数据 包括 输入 输出 训练 搜索 好坏 评价 自然语言 处理 
是 搜索引擎 最 核心 的 基础 技术 包括了 输入 和 
输出 每一次 的 搜索 行为 都 可以 看做 是 对 
搜索引擎 的 一次 训练 用户 的 点击 来 告诉 搜索 
结果 的 好坏 从而 展示出 相 对应 的 搜索 排名 
在 这个 过程 中 搜索引擎 不仅 提高 了 推荐 的 
准确性 还 越来越 懂得 判断 所 收录 结果 的 好 
与 坏 渐渐 学会 了 像 人类 一样 去 分辨 
网页 在 求解 一个 问题 时 涉及 到 两个 方面 
问题 的 表示 相对 合适 的 求解 方法 搜索 法 
归纳法 归 结法 推理法 和 产生 式 等 5.1   
搜索 的 概念 5.2   状态 空间 的 搜索 策略 
5.3   盲目 的 图 搜索 策略 Uninformed Search 5.4 
  启发式 图 搜索 策略 Informed Search 补 其他 搜索 
策略 局部 搜索 法 爬山 搜索 法 局部 剪枝 搜索 
模拟退火 法等/nr 搜索 中 需要 解决 的 基本 问题 1 
是否 一定 能 找到 一个 解 完备性 2 找到 的 
解 是否是 最佳解 最优性 3 时间 与 空间 复杂性 如何 
4 是否 终止 运行 或 是否 会 陷入 一个 死循环 
搜索 的 主要 过程 三要素 1 状态 空间 state space 
从 初始 或 目 的 状态 出发 并将 它 作为 
当前 状态 双向 2   后继 函数 successor function with 
actions and costs 扫描 操作 算 子集 将 适用 当前 
状态 的 一些 操作 算子 作用 于 当前 状态 而 
得到 新的 状态 并 建立 指向 其父 结点 的 指针 
3   初始状态 和 目标 测试 start state and goal 
test 解 是 一个 行动 序列 将 初始状态 转换成 目标 
状态 搜索 问题是 对 原 问题 的 建模 扩 展出 
潜在 的 行动 tree nodes 维护 所 考虑 行动 的 
边缘 fringe 节点 试图 扩展 尽可能少 的 树节点 搜索 策略 
1 .   搜索 方向 1 数据 驱动 从 初始状态 
出发 的 正 向搜索 用 给定 数据 中 约束 知识 
指导 搜索 2 目的 驱动 从目的/nr 状态 出发 的 逆向 
搜索 哪些 操作 算子 能 产生 该 目的 产生 目的 
时 需要 哪些 条件 3 双 向搜索 直到 两条 路径 
在 中间 的 某处 汇合 为止 2 .   盲目 
搜索 与 启发式 搜索 1 盲目 搜索 在 不 具有 
对 特定 问题 的 任何 有关 信息 的 条件 下 
按 固定 的 步骤 依次 或 随机 调用 操作 算子 
进行 的 搜索 2 启发式 搜索 考虑 可 应用 的 
知识 动态地 确定 调用 操作 算子 的 步骤 优先选择 较 
适合 的 操作 算子 尽量 减少 不 必要 的 搜索 
以求 尽快 地 到达 结束 状态 状态 空间 的 搜索 
策略 状态 空间 表示法 状态 空间 的 图 描述 状态 
表示 系统 状态 事实 等 叙述 型 知识 的 一组 
变量 或 数组 . 环境 细节 操作 表示 引起 状态 
变化 的 过程 型 知识 的 关系 或 函数 Problem 
PathingStates x y locationActions NSEWSuccessor update locationGoal test is x 
y = ENDProblem Eat All DotsStates { x y dot 
booleans } Actions NSEWSuccessor update location and   dot booleanGoal 
test dots all false 状态 空间 利用 状态 变量 和 
操作 符号 表示 系统 或 问题 的 有关 知识 的 
符号 体系 状态 空间 四元组 S 状态 集合 O 操作 
算子 的 集合 S0 包含 问题 的 初始 状态 是 
的 非空 子集 G 若干 具体 状态 或 满足 某些 
性质 的 路径 信息 描述 求解 路径 从 S0 结 
点到 G 结点 的 路径 状态 空间 的 一个 解 
一个 有限 的 操作 算子 序列 八 数码 问题 的 
状态 空间 状态 集 所有 摆法 9 操作 算子 4/m 
将/d 空格/n 向/p 上移/v Up/w 将/d 空格/n 向/p 左移/nr Left/w 
将/d 空格/n 向/p 下移/v Down/w 将/d 空格/n 向/p 右移/n Right/w 
状态/n 空间/n 的/uj 图/n 描述/v 状态/n 空间/n 的/uj 有向图/nr 描述/v 
搜索 树 状态 空间 图中 每个 状态 只 出现 一次 
搜索 树 可 出现 多次 几乎 不在 内存 中 构建 
完整 的 状态 空间 图 太大 了 但是 有用 的 
盲目 的 图 搜索 策略 回溯 策略 宽度 优先 搜索 
策略 深度 优先 搜索 策 略带 回溯 策略 的 搜索 
从 初始状态 出发 寻找 路径 直到 它 到达 目的 或 
不可解 结点 为止 若 它 遇到 不可解 结点 就 回溯到 
路径 中 最近 的 父 结 点上 查看 该 结点 
是否 还有 其他 的 子 结点 未被 扩展 如果 找到 
目标 就 成功 退出 搜索 返回 解题 路径 回溯 搜索 
的 算法 1 PS pathstates 表 保存 当前 搜索 路径 
上 的 状态 如果 找到 了 目的 PS 就是 解 
路径 上 的 状态 有序 集 2 NPS new path 
states 表 新的 路径 状态表 它 包含 了 等待 搜索 
的 状态 其 后裔 状态 还 未被 搜索 到 即 
未被 生成 扩展 3 NSS no solvable states 表 不可解 
状态 集 列出 了 找 不到 解题 路径 的 状态 
如果 在 搜索 中 扩 展出 的 状态 是 它 
的 元素 则可 立即 将之 排除 不必 沿 该 状态 
继续 搜索 图 搜索算法 深度 优先 宽度 优先 最好 优先 
搜索 等 的 回溯 思想 1 用 未处理 状态表 NPS 
使 算法 能 返回 回溯 到 其中 任一 状态 2 
用 一张 死胡同 状态表 NSS 来 避免 算法 重 新搜索 
无解 的 路径 3 在 PS 表中 记录 当前 搜索 
路径 的 状态 当 满足 目的 时 可以 将 它 
作为 结果 返回 4 为 避免 陷入 死循环 必须 对 
新 生成 的 子 状态 进行 检查 看 它 是否 
在 该 三张 表中 宽度 优先 搜索 策略 open 表 
NPS 表 已经 生成 出来 但 其子 状态 未被 搜索 
的 状态 FIFO closed 表 PS 表 和 NSS 表 
的 合并 记录 了 已被 生成 扩展 过 的 状态 
操作 算子 为 MOVE X Y 把 积木 X 搬到 
Y 积木 或 桌面 上面 操作 算 子可 运用 的 
先决条件 1 被 搬动 积木 的 顶部 必须 为 空 
2 如果 Y 是 积木 则 积木 Y 的 顶部 
也 必须 为 空 3 同一 状态下 运用 操作 算子 
的 次数 不 得多 于 一次 生成 扩展 完 N 
层 的 所有 结点 后才 转向 N + 1层 总能 
找到 最好 的 解 当 图 分 支数 太多 即 
状态 的 后裔 数 平均值 较大 这种 组合 爆炸 会使 
算法 耗尽 资源 为了 保证 找到 解 应 选择 合适 
的 深度 限制值 或 采取 不断 加大 深度 限制值 的 
办法 反复 搜索 直到 找到 解 深度 优先 搜索 并 
不能 保证 第一 次 搜索 到 的 是 到 这个 
状态 的 最短 路径 如果 路径 的 长度 对 解题 
很 关键 的话 当 算法 多次 搜索 到 同一个 状态 
时 它 应该 保留 最短 路径 Open 表 是 一个 
堆栈 结构 使 搜索 偏向 最后 生成 状态 特点 深度 
优先 搜索 在 搜索 有 大量 分支 的 状态 空间 
时有 高效率 不 需要 把 某 层 上 所有 结点 
进行 扩展 但会 找 不到 通向 目的 的 更短 路径 
或 陷入 不 通往 目的 的 无限 长 的 路径 
中 所有 的 搜索 算法 都是/nr 相同 的 除了 对 
边缘 的 处理 策略 从 概念 上 说 所有 的 
边缘 是 优先 队列 即 附加 优先级 的 节点 集合 
对于 DFS BFS 可以 通过 使用 栈 或 队列 代替 
优先 队列 从而 减少 log n 的 开支 结合 DFS 
的 空间 优势 与 BFS 的 时间 优势 迭代 深入 
搜索 启发式 图 搜索 策略 图 知识 表示 图 搜索 
启发式 策略 启发 heuristic 关于 发现 和 发明 操作 算子 
及 搜索 方法 的 研究 在 状态 空间 搜索 中 
启发式 被 定义 成 一系列 操作 算子 并 能从 状态 
空间 中 选择 最 有希望 到达 问 题解 的 路径 
启发式 策略 利用 与 问题 有关 的 启发 信息 进行 
搜索 按照 什么 顺序 考察 状态 空间 图 的 节点 
启发式 搜索 应用于 博弈 机器学习 数据挖掘 和 智能 检索 等 
在 状态 空间 搜索 中 启发式 被 定义 成 一系列 
操作 算子 并 能从 状态 空间 中 选择 最 有希望 
到达 问 题解 的 路径 启发式 策略 利用 与 问题 
有关 的 启发 信息 进行 搜索 按照 什么 顺序 考察 
状态 空间 图 的 节点 运用 启发式 策略 的 两种 
基本 情况 1 由于 问题 陈述 和 数据 获取 方面 
固有 的 模糊性 模糊理论 可能会 使 它 没有 一个 确定 
的 解 2 虽然 一个 问题 可能 有 确定 解 
但是 其 状态 空间 特别 大 搜索 中 生成 扩展 
的 状态 数 会 随着 搜索 的 深度 呈 指数级 
增长 穷尽 式 搜索 无解 3 两部分 启发 方法 剪枝 
和 搜索 状态 空间 的 算法 启发式 策略 的 运用 
剪枝 棋盘 对称性 以 减少 状态 空间 的 大小 棋局 
走法 9 3 * 8 第一步 3种 走法 3 + 
12 * 7 启发 性知识 与 被 求解 问题 自身 
特性 相关 的 知识 包括 被 求解 问题 的 解 
特性 解/v 分布/v 规律/n 和/c 实际/n 求解/v 问题/n 的/uj 经验/n 
和/c 技巧/n 等/u 对应 问题 求解 的 控制 性知识 启发 
函数 实现 启发式 搜索 需要 把 启发 性知识 函数 表示 
通过 函数 计算 评价 选择 价值 大小 指导 搜索 过程 
求解 问题 中 能 利用 的 大多 是非 完备 的 
启发 信息 启发 信息 的 分类 1 陈述 性 启 
发信息 精准 描述 状态 缩 小问题 状态 空间 2 过程 
性 启 发信息 以 规律 性知识 构造 操作 算子 3 
控制性 启 发信息 搜索 策略 控制结构 等 知识 利用 控制性 
的 启发 信息 的 情况 1 没有 任何 控制性 知识 
作为 搜索 的 依据 因而 搜索 的 每一步 完全 是 
随意 的 2 有 充分 的 控制 知识 作为 依据 
因而 搜索 的 每一步 选择 都是/nr 正确 的 但 这是 
不 现实 的 启发 函数 的 设计 在 实际 设计 
过程 中 启发 函数 是 用来 估计 搜索 树节点 x 
与 目标 节点 接近 程度 的 一种 函数 通常 记为 
h x 启发 函数 可以 是 1 一个 结点 到 
目标 结点 的 某种 距离 或 差异 的 量度 2 
一个 结点 处在 最佳 路径 上 的 概率 启发式 搜索 
用 启 发函 数来 导航 其 搜索算法 就要 在 状态图 
一般 搜索算法 基础上 再 增加 启发 函数值 的 计算 与 
传播 过程 并且 由 启发 函数值 来 确定 节点 的 
扩展 顺序 分为 全局 择优 搜索 和 局部 择优 搜索 
全局 择优 搜索 基本思想 在 OPEN 表中 保留 所有 已 
生成 而未 考察 的 节点 并用 启发 函数 h x 
对 它们 全部 进行 估价 从中 选出 最优 节点 进行 
扩展 而 不管 这个 节点 出现 在 搜索 树 的 
什么 地方 局部 子 节点 择优 搜索 基本思想 在 启发 
性知识 导航 下 的 深度 优先 搜索 在 OPEN 表中 
保留 所有 已 生成 而未 考察 的 结点 对 其中 
新 生成 的 每个 子 结点 x 计算 启发 函数 
从 全部 子 结点 中 选出 最优 结点 进行 扩展 
其 选择 下 一个 要 考察 结点 的 范围 是 
刚刚 生成 的 全部 子 结点 . 步1 /nr 把 附有 
f S0 的 初始 结点 S0 放入 OPEN 表中 步2 /nr 
若 OPEN 表为 空 则 搜索 失败 退出 步3 /nr 否则 
移出 OPEN 表中 第一个 结点 N 放入 CLOSED 表中 顺序 
编号 n 步4 /nr 若 目标 结点 Sg ＝ N 则 
搜索 成功 利用 CLOSED 表 中的 返回 指针 找出 S0 
到 N 的 路径 即 为所 求解 退出 步5 /nr 若 
N 不 可扩展 则 转步 2 6 扩展 N 计算 
N 的 每个 子 结点 x 的 函数值 并将 N 
所 有子 结点 x 配 以 指向 N 的 返回 
指针 后 放入 OPEN 表中 依据 启发 函数 对 结点 
的 计算 再 对 OPEN 表中 所有 结点 / 子 
结点 按其 启发 函数值 的 大小 以 升序 排列 转步 
2 移出 OPEN 表中 第一个 结点 N 放入 CLOSED 表中 
在 全局 择优 和 局部 择优 搜索算法 中 没有 考虑 
从 初始 结点 到 当前 结点 已经 付出 的 实际 
代价 在 很多 实际 问题 中 已经 付出 的 实际 
代价 是 必须 考虑 的 如 TSP 问题 等 将 
两者 同时 考虑 用于 指导 搜索 的 算法 称为 A 
算法 和A*/nr 算法 启发 信息 和 估价 函数 估价 函数 
的 任务 就是 估计 待 搜索 结点 的 有希望 程度 
并 依次 给 它们 排定 次序 在 open 表中 从 
初始 结点 经过 n 结点 到达 目的 结点 的 路径 
的 最小 代价 估计值 g n 代价 函数 表示 从 
初始 结 点到 n 结点 的 实际 代价 越小越 靠近 
初始 结点 利于 搜索 的 横向发展 可 提高 搜索 完备性 
但 影响 搜索 效率 h n 启发 函数 表示 从n结/nr 
点到 目的 结点 的 最佳 路径 的 估计 代价 越小越 
靠近 目标 结点 利于 搜索 的 纵向 可 提高 搜索 
效率 影响 完备性 一般地 在 f n 中 g 的 
比重 越大 越 倾向于 宽度 优先 搜索 方式 而 h 
的 比重 越大 表示 启发 性能 越强 f n = 
g n + w h n 调整 w 的 值 
使 结果 偏重 效率 或 完备性 对 估价 函数 f 
x ＝ g x ＋ h x 令 其中 h 
x = 0时 得到 代价 树 的 非 启发式 搜索算法 
按 对 节点 的 考察 范围 不同 可 分为 两种 
搜索 策略 分支 界限 法将/nr 全局 择优 搜索算法 中的 h 
x 替换 为 g x 可得到 分支 界限 法 瞎子 
爬山 法将/nr 局部 择优 搜索算法 中的 h x 替换 为 
g x 可得到/i 瞎子/n 爬山/n 法A/nr 搜索算法/n 启发式/l 图/n 搜索/v 
法的/nr 基本/n 特点/n 如何 寻找 并 设计 一个 与 问题 
有关 的 h n 及 构 出 f n = 
g n + h n   然后 以 f n 
的 大小 来 排列 待 扩展 状态 的 次序 每次 
选择 f n 值 最小 者 进行 扩展 open 表 
保留 所有 已 生成 而未 扩展 的 状态 closed 表 
记录 已 扩展 过 的 状态 进入 open 表 的 
状态 是 根据 其 估值 的 大小 插入 到 表中 
合适 的 位置 每次 从 表中 优先 取出 启发 估价 
函数值 最小 的 状态 加以 扩展 open = start closed 
= f s = g s + h s   
      * 初始化 while open ≠ dobegin 从 
open 表中 删除 第一个 状态 称之为 n if n = 
目 的 状态   then return success 生成 n 的 
所有 子 状态 if   n 没有 任何 子 状态 
then continue for n 的 每个 子 状态 docase 子 
状态 is not already on open 表 or closed 表 
begin 计算 该 子 状态 的 估价 函数值 将该 子 
状态 加到 open 表中 end case 子 状态 is already 
on open 表 if 该 子 状态 是 沿着 一条 
比 在 open 表 已 有的 更短 路径 而 到达 
then 记录 更短 路径 走向 及 其 估价 函数值 case 
子 状态 is already on closed 表 if 该 子 
状态 是 沿着 一条 比 在 closed 表 已 有的 
更短 路径 而 到达 thenbegin 将该 子 状态 从 closed 
表 移到 open 表中 记录 更短 路径 走向 及 其 
估价 函数值 end case end 将 n 放入 closed 表中 
根据 估价 函数值 从小到大 重新排列 open 表 end     
                    
                    
                    
                * open 
表中 结点 已 耗尽 return failure end . 八 数码 
以 不 在位 的 将牌 数 作为 启发 信息 的 
度量 h * n 为 状态 到 目 的 状态 
的 最优 路径 的 代价 A * 搜索算法 及其 特性 
分析 如果 某一 问题 有解 那么 利用 A * 搜索算法 
对 该 问题 进行 搜索 则 一定 能 搜索 到 
解 并且/c 一定/d 能/v 搜索/v 到/v 最优/d 的/uj 解而/nr 结束/v 
上例 中的 八 数码 A 搜索 树 也是 A * 
搜索 树 所得 的 解路/nr s B E I K 
L 为 最优 解路/nr 其 步数 为 状态 L 5 
上 所 标注 的 5 1 . 可采纳 性 当 
一个 搜索算法 在 最短 路径 存在 时能 保证 找到 它 
就 称 它 是 可采纳 的 2 . 单 调性 
搜索算法 的 单调 性 在整个 搜索 空间 都是/nr 局部 可采纳 
的 一个 状态 和 任一 个子 状态 之间 的 差 
由该 状态 与 其子 状态 之间 的 实际 代价 所 
限定 3 . 信息性 在 两个 A * 启发 策略 
的 h1h2 中 如果 对 搜索 空间 中的 任一 状态 
n 都有 h1 n = h2 n 就 称 策略 
h1 有 更多 的 信息 性 理解 和 使用 自然 
语言 处理 之 终极 指南 Python 编码 经典 收藏版 12k 
字 附 数据 简化 筹 员 2月 17日 Fri 新闻 
秦陇 纪 10 译编 12k 字 理解 和 使用 自然 
语言 处理 之 终极 指南 Python 编码 7k 字 附 
数据 简化 DataSimp 筹 收 技术 简历 414字 2月 17日 
Fri 新闻 四则 4k 字 欢迎 加入 共建 数据 简化 
DataSimp 学会 及 社区 关注 收藏 转发 新媒体 数据 简化 
DataSimp 科学 Sciences 微 信号 头条 号 转载 请 写 
出处 秦陇 纪 10 数据 简化 DataSimp / 科学 Sciences 
公众 号 头条 号 译编 投稿 QinDragon2010 @ qq . 
com 目录 理解 和 使用 自然 语言 处理 之 终极 
指南 Python 编码 7.4 k 字 附 A . 数据 
简化 DataSimp 筹备 收 简历 414字 附 B . 2017年 
2月 17日 周五 农历 丁酉 鸡年 正月 廿一 新闻 四则 
汇编 4.8 k 字 理解 和 使用 自然 语言 处理 
之 终极 指南 Python 编码 秦陇 纪 10 译编 来源 
仕 瓦 姆 邦 萨尔 Shivam Bansal 2017年 1月 12日 
威 提 亚 分析学 目录 表 Table of Contents1 . 
Introduction to NLP 自然语言 处理 介绍 2 . Text Preprocessing 
文本 预处理 oNoise Removal 噪声 去除 oLexicon Normalization 词汇 规范化 
§ Lemmatization 词 变体 归类 § Stemming 词干 提取 oObject 
Standardization 对象 规范化 3 . Text to Features Feature Engineering 
on text data 文本 到 特征 文本 数据 之 特征 
工程 oSyntactical Parsing 句法 解析 § Dependency Grammar 依存 语法 
§ Part of Speech Tagging 词性 标注 oEntity Parsing 实体 
解析 § Phrase Detection 短语 检测 § Named Entity Recognition 
命名 实体 识别 § Topic Modelling 主题 造型 § N 
GramsN 元 连续 模型 oStatistical features 统计 特征 § TF 
– IDF 词频 逆 文档 词频 § Frequency / Density 
Features 频率 / 密度 特征 § Readability Features 可读性 特征 
oWord Embeddings 字 嵌入 4 . Important tasks of NLP 
自然语言 处理 NLP 的 重要 任务 oText Classification 文本 分类 
oText Matching 文本 匹配 § Levenshtein Distance 莱文 斯坦 距离 
§ Phonetic Matching 语音 匹配 § Flexible String Matching 柔性 
字符串 匹配 oCoreference Resolution 共 指 消解 oOther Problems 其他 
问题 5 . Important NLP libraries 重要 NLP 库 据 
业内人士 估计 只有 21% 可用 数据 以 结构化 形式 存在 
数据 产生 正如 我们 所说 的 来自 于 我们 的 
推特 WhatsApp 和 其他 各种 交流 活动 中 发送 的 
信息 大多数 这些 数据 存在于 文本 的 形式 是 高度 
非 结构化 的 性质 一些 臭名昭著 的 例子 包括 在 
社交 媒体 上 的 推特 / 帖子 用户 到 用户 
的 聊天 对话 新闻 博客 和 文章 产品 或 服务 
审查 和 医疗 部门 里 的 病人 记录 最近 的 
一些 例子 包括 聊天 机器人 和 其他 声音 驱动 的 
机器人 程序 尽管 具有 高维 数据 但 其 呈现 的 
信息 是 不 可以 直接 访问 的 除非 它 被 
手动 处理 读取 和 理解 或由 自动化 系统 分析 为了/p 
从/p 文本/n 数据/n 中/f 产生/n 明显/a 的/uj 和可/nr 操作/v 的/uj 
洞察/n //i 见解/v 熟悉 自然语言 处理 NLP 的 技术 和 
原则 显得 非常 重要 那么 如果 你 打算 今年 创建 
聊天 机器人 或者 你 想 使用 非 结构化 文本 的 
力量 本 指南 是 正确 的 起点 本 指南 挖掘 
自然语言 处理 的 概念 技术 与 实现 文章 的 目的 
是 教会 自然语言 处理 的 概念 并将 其 应用 于 
实际 数据集 1 . Introduction to Natural Language Processing 自然语言 
处理 介绍 NLP 是 数据 科学 的 一个 分支 包括/v 
智能/n 和/c 高效/a 地/uv 从/p 文本/n 数据/n 中/f 分析/vn 理解 
和 导出 信息 的 系统 流程 通过 NLP 及其 组成部分 
企业 可以 组织 海量 文本 数据块 执行 许多 自动化 任务 
并 解决 广泛 问题 如 自动 摘要 机器翻译 命名 实体 
识别 关系 抽取 情感 分析 语音识别 主题 分割 等 在 
进一步 研究 之前 我 想 解释一下 文章 中 使用 的 
一些 术语 标记 化 转换 文本 到 标记 体 的 
过程 标记 体 文本 中 存在 的 单词 或 实体 
文本 对象 一个 句子 或 一个 短语 或 一个 词 
或 一篇 文章 安装 NLTK 及其 数据 的 步骤 使用 
Python 语言 及 环境 安装 Pip 在 终端 中 运行 
sudo easy _ install pip 安装 NLTK 在 终端 中 
运行 sudo pip install U nltk 下载 NLTK 数据 运行 
Python shell 在 终端 和写/nr 下面 的 代码 ` ` 
` import nltk nltk . download ` ` ` 按照 
屏幕 上 的 指令 下载 所需 的 包或/nr 集合 其他 
库 可以 直接 使用 Pip 安装 2 . Text Preprocessing 
文本 预处理 因此 文本 是 所有 可用 数据 的 最具 
非 结构化 的 形式 存在 于 其中 的 各种 类型 
的 噪声 并且 没有 预处理 的 数据 是 不容易 分析 
的 文本 清理 和 标准化 的 全过程 是 一个 去除 
其 噪声 和 称为 文本 预处理 的 分析 准备 工作 
它 主要 由 三个 步骤 组成 Noise Removal 噪声 去除 
Lexicon Normalization 词汇 规范化 Object Standardization 对象 标准化 下图 显示 
了 文本 预处理 清洁 流水线 的 体系 结构 2.1 Noise 
Removal 噪声 去除 任何 与 数据 上下文 和 最终 输出 
无关 的 文本 片段 都 可以 指定 为 噪声 例如 
语言 停用词 语言 常用词 is / am / the / 
of / in 等 URL 或 链接 社会 媒体 实体 
提示 哈希 标签 标点符号 和 特定 行业 用词 此 步骤 
处理 移除 文本 中 存在 的 所有 类型 噪声 实体 
去除 噪声 的 一般 方法 是 准备 一个 噪声 实体 
字典 并 通过 标记符号 或 文字 来 迭代 文本 对象 
消除 这些 噪声 字典 呈现出 的 标记符号 以下 是 实现 
相同 目的 Python 代码 ` ` ` # Sample code 
to remove noisy words from a textnoise _ list = 
is a this . . . def _ remove _ 
noise input _ text words = input _ text . 
split noise _ free _ words = word for word 
in words if word not in noise _ list noise 
_ free _ text = . join noise _ free 
_ words return noise _ free _ text _ remove 
_ noise this is a sample text sample text ` 
` ` 另一种 方法 是 在 处理 特殊 噪声 模式 
时 使用 正则表达式 之前 的 一篇 文章 中 我们 详细 
解释 了 正则表达式 以下 的 Python 代码 从 输入 文本 
中 移 除了 一个 正则表达式 模式 ` ` ` # 
Sample code to remove a regex patternimport redef _ remove 
_ regex input _ text regex _ pattern urls = 
re . finditer regex _ pattern input _ text for 
i in urls input _ text = re . sub 
i . group . strip input _ text return input 
_ textregex _ pattern = # A Za z0 9 
\ w * _ remove _ regex remove this # 
hashtag from analytics vidhya regex _ pattern remove thisfrom analytics 
vidhya ` ` ` 2.2 Lexicon Normalization 词汇 规范化 另一种 
文 本式 噪声 是 关于 单个 词 所 表现 的 
多重 表征 例如 玩 玩家 玩过 第三人称 的 玩 和 
正在 玩 play player played plays and playing 这些 词 
是 单词 玩 的 不同 变化 尽管 他们 的 意思 
是 不同 的 但 内容 都是/nr 相似 的 这个 步骤 
是 把 一个 词 的 所有 差异 转换成 它们 的 
标准化 形式 也 称为 lemma 引理 规范化 是 文本 特征 
工程 的 关键 步骤 因为 它 转换 的 高维 特征 
N 维度 不同 特征 到 低 维空间 1个 特征 是 
任何 ML 模型 的 一种 理想 解 最 常见 的 
词汇 规范化 做法 是 词干 提取 Stemming 词干 提取 是 
一种 基本 的 基于 规则 的 从 一个 词 剥离 
后缀 的 过程 后缀 ing ly es s 等 词 
变体 归类 Lemmatization 词 变体 归类 从 另一方面 是 一个 
有 组织 且 有步骤 获得 这个 词 的 词根 形式 
的 过程 即 词汇 用法 单词 的 词典 重要性 和 
形态 逻辑 分析 词汇 结构 和 语法 关系 下面 的 
示例代码 是 用 Python 主流 库 NLTK 执行 的 词 
变体 归类 Lemmatization 和 词干 提取 Stemming ` ` ` 
from nltk . stem . wordnet import W o r 
d N e t L e m m a t 
i z e r l e m = W o 
r d N e t L e m m a 
t i z e r from nltk . stem . 
porter import P o r t e r t e 
m m e r s t e m = PorterStemmer 
word = multiplying lem . lemmatize word v multiply stem 
. stem word multipli ` ` ` 2.3 Object Standardization 
对象 标准化 文本 数据 通常 包含 一些 任何 标准 语义 
字典 中 不 存在 的 单词 或 短语 这些 碎片 
是 搜索 引擎 和 模型 不能 识别 的 这 方面 
的 一些 例子 是 首字母 缩略 词语 单词 附属 哈希 
标签 和 口语 俚语 借助 正则表达式 和 手工 编写 的 
数据字典 可以 找到 这种 类型 的 噪声 下面/f 的/uj 代码/n 
使用/v 一个/m 字典/n 查找/v 法从/nr 文本/n 中/f 代替/v 社交/n 媒体/n 
的/uj 俚语/n ` ` ` lookup _ dict = { 
rt Retweet dm direct message awsm awesome luv love . 
. . } def _ lookup _ words input _ 
text words = input _ text . split new _ 
words = for word in words if word . lower 
in lookup _ dict word = lookup _ dict word 
. lower new _ words . append word new _ 
text = . join new _ words return new _ 
text _ lookup _ words RT this is a retweeted 
tweet by Shivam Bansal Retweet this is a retweeted tweet 
by Shivam Bansal ` ` ` 除了 讨论 到 目前 
为止 的 三个 步骤 其他 类型 的 文本 预处理 包括 
编码 解码 噪声 语法 检查 器 拼写 校正 等 详细 
的 文本 预处理 及其 方法 在 秦陇 纪 专著 文章 
有 3 . Text to Features Feature Engineering on text 
data 文本 到 特征 文本 数据 之 特征 工程 对 
预处理 数据 做 分析 需要 将其 转换成 特征 根据 使用 
情况 文本 特征 可用 配套 技术 来 构建 语义分析 实体 
/ 克 / 基于 词 的 特征 统计 特征 字 
的 嵌入 实体 / N 元 连续 模型 / 基于 
词 的 特征 统计 特征 和 单词 嵌入 下面 来 
继续 阅读 以 详细 了解 这些 技术 3.1 Syntactic Parsing 
句法 解析 句法 解析 涉及 句中 单词 的 语法 和 
显示 这些 单词 间 关系 的 排列 方式 的 分析 
依存 语法 和 部分 语音 标签 是 文本 句法 的 
重要 属性 依存 树 – 句子 是由 一些 单词 缝 
和在/nr 一起 组成 的 句子 中 词语 间 的 关系 
由 基本 依存 语法 决定 依存 语法 是 处理 两个 
语 义项 之间 的 标记 的 非对称 二元关系 单词 的 
一类 语义 文本 分析法 每一种 关系 都 可以 用 三元组 
关系 监督 依存 来 表示 例如 考虑 句子 Bills on 
ports and immigration were submitted by Senator Brownback Republican of 
Kansas . 这些 单词 间 的 关系 可以 用 下图 
所示 的 树形 表示 形式 观察到 这个 树 显示 submitted 
是 这个 句子 的 根词 是由 两个 子树 主体 与 
客体 的 子树 相连 每个 子树 本身 一个 诸如 Bills 
ports proposition relation ports immigration conjugation relation 关系 的 依存 
关系 树 这种 类型 的 树 采用 自上而下 的 方法 
递归 解析 时 给 出了 的 语法 关系 三元组 作为 
输出 可 用于 许多 NLP 问题 的 特征 像 实体 
情感 分析 演员/n 和/c 实体/n 识别/v 和/c 文本/n 分类/n Python 
包组/nr 斯坦福 CoreNLP 来自 Stanford NLP 项目组 只有 商业 许可证 
版 和 NLTK 依存 语法 可以 用来 产生 依存关系 树 
. 词性 标注 Part of Speech tagging – 除了 语法 
关系 在 一个 句子 里 每个 词 也和 词性 标签 
POS 名词 动词 形容词 副词 等 相关联 POS 标签 定义 
一个词 在 句子 中 的 用法 和 功能 这是 宾夕法尼亚大学 
定义 的 一个 所有 可能 POS 标签 列表 下面 的 
代码 使用 NLTK 对 输入 文本 进行 词性 标注 注释 
它 提供 了 多种 实现 方案 默认 是 感知 标记 
器 ` ` ` from nltk import word _ tokenize 
pos _ tagtext = I am learning Natural Language Processing 
on Analytics Vidhya tokens = word _ tokenize text print 
pos _ tag tokens I PRP am VBP learning VBG 
Natural NNP Language NNP Processing NNP on IN Analytics NNP 
Vidhya NNP ` ` ` 词性 标注 用于 NLP 自然语言 
处理 中 的 许多 重要 用途 A . 词义 消 
歧 一些 语言 词汇 根据 其 用法 有 多种 含义 
例如 在 以下 两个 句子 中 I . Please book 
my flight for Delhi II . I am going to 
read this book in the flight Book 在 不同 语境 
使用 这 两种 情况 下 的 词性 标注 词 不同 
句 I 中 Book 作为 动词 而 II 句中 它 
被 用作 名词 Lesk 算法 也 用于 类似 目的 B 
. 提高 基于 词 的 特征值 学习 模型 在 以 
一个 词 为 特征 时 学习 词 的 不同 情境 
如果 词性 标注 词 与 他们 有 联系 则 上下文 
被 保存 从而 做出 强壮 的 特征值 例如 句子 Sentence 
book my flight I will read this book 标记 词 
Tokens – book 2 my 1 flight 1 I 1 
will 1 read 1 this 1 词性 标注 标记 词 
Tokens with POS – book _ VB 1 my _ 
PRP $ 1 flight _ NN 1 I _ PRP 
1 will _ MD 1 read _ VB 1 this 
_ DT 1 book _ NN 1 C . 标准化 
和词/nr 变体 归类 POS 标签 是 词 变体 归类 过程 
的 基础 用于 将 一个 词 转换成 它 的 基 
形式 lemma 引理 D . 有效 的 停用词 去除 POS 
标签 在 高效 去除 停用词 也 有用 例如 有 一些 
标签 总是 定义 一个 语言 的 低频 / 不 重要 
的 单词 例如 IN – within upon except CD – 
one two hundred MD – may mu st etc 3.2 
Entity Extraction Entities as features 实体 提取 实体 为 特征值 
实体 被 定义 为 句子 中 最重要 的 句 块 
名词 短语 动词短语 或 两者 实体 检测 算法 通常 是 
基于 规则 解析 字典 查找 POS 标签 依存 句法分析 的 
集成 模型 实体 检测 的 适用性 可以 在 自动 聊天 
机器人 内容 分析器 和 消费者 洞察 中 看见 主题 模型 
和 命名 实体 识别 是 NLP 自然语言 处理 里 两个 
主要 的 实体 检测 方法 A . Named Entity Recognition 
命名 实体 识别 NER 检测 如 人名 地名 公司名 等 
命名 实体 的 过程 称为 NER 例如 句子 Sentence – 
Sergey Brin the manager of Google Inc . is walking 
in the streets of New York . 命名 实体 Named 
Entities – person Sergey Brin org Google Inc . location 
New York 一个 典型 NER 模型 由 三块 组成 名词 
短语 识别 这一步/i 涉及/v 使用/v 依存/v 解析/vn 和/c 词性/n 标注/v 
从/p 文本/n 中/f 提取/v 所有/b 名词/n 短语/nz 短语 分类 这是 
将 所有 被 提取 名词 短语 划分为 所属 相应 类别 
位置 名称 等 的 分类 步骤 谷歌 地图 API 提供 
了 消除歧义 位置 的 一个 好 路径 然后 从 DBpedia 
wikipedia 开放 数据库 可以 用来 识别 个人 姓名 或 公司 
名称 除此之外 结合 来自 不同 信息源 的 查找表 和 词典 
可以 精确 查找 实体 消 歧 有时 这是 可能 的 
实体 的 误判 分类 的 因此 随之 创建 分类 结果 
之上 的 验证 层 是 有用 的 出于 此 目的 
可以 运用 知识 图 流行 的 知识 图 有–/nr 谷歌 
知 识图 IBM 沃森 和 维基百科 B . Topic Modeling 
主题 模型 主题 建模 是 一个 存在于 文本 语料库 中 
主题 的 自动 识别 过程 它 以 无 监督 方式 
推导 出 语料库 中的 隐含 模式 主题 被 定义 为 
a repeating pattern of co occurring terms in a corpus 
医疗保健 为 主题 的 一个 好 的 主题 模型 结果 
有–/nr health doctor patient hospital 健康 医生 病人 医院 农事 
为 主体 则有 – farm crops wheat 农场 庄稼 小麦 
为 话题 农业 隐含 狄利克雷 分配 LDA 是 最 受欢迎 
的 主题 建模 技术 以下 是 使用 LDA 实现 主题 
建模 的 Python 代码 有关 其 工作 和 执行 的 
详细 说明 请 检查 这里 的 完整 文章 ` ` 
` doc1 = Sugar is bad to consume . My 
sister likes to have sugar but not my father . 
doc2 = My father spends a lot of time driving 
my sister around to dance practice . doc3 = Doctors 
suggest that driving may cause increased stress and blood pressure 
. doc _ complete = doc1 doc2 doc3 doc _ 
clean = doc . split for doc in doc _ 
complete import gensim from gensimimport corpora # Creating the term 
dictionary of our corpus where every unique term is assigned 
an index . dictionary = corpora . Dictionary doc _ 
clean # Converting list of documents corpus into Document Term 
Matrix using dictionary prepared above . doc _ term _ 
matrix = dictionary . doc2bow doc for doc in doc 
_ clean # Creating the object for LDA model using 
gensim libraryLda = gensim . models . ldamodel . LdaModel 
# Running and Training LDA model on the document term 
matrixldamodel = Lda doc _ term _ matrix num _ 
topics = 3 id2word = dictionary passes = 50 # 
Resultsprint ldamodel . print _ topics ` ` ` C 
. N Grams as FeaturesN 元 连续 模型 N grams 
作为 特征值 N 个 单词 在 一起 的 组合 被 
称为 N 元 连续 模型 N grams 作为 特征值 相比 
单词 一元 1gram N 元 连续 模型 n ＞ 1 
通常 包含 更多 信息 另外 双 单词 元组 n = 
2 被 认为 是 所有 其他 元 模型 更 重要 
的 特征 下面 的 代码 生成 一个 文本 二元 模型 
实例 ` ` ` def generate _ ngrams text n 
words = text . split output = for i in 
range len words n + 1 output . append words 
i i + n return output generate _ ngrams this 
is a sample text 2 # this is is a 
a sample sample text ` ` ` 3.3 Statistical Features 
统计 特征 文本 数据 也 可以 使用 本节 中 描述 
的 几种 技术 直接 量 化成 数字 欢迎 转 发声明 
秦陇 纪 10 公众 号 头条 号 数据 简化 DataSimp 
科普 文章 附 数据 简化 DataSimp 筹备 收 简历 414字 
北京 数据 简化 有限 责任 公司 筹 愿景 ① 行业 
大 数据 采集 处理 分析 管理系统 ② 企事业 单位 行政 
人事 财 物联网 智能 OA 系统 ③ 数据 简化 DataSimp 
学术 组 及 开源 社区 中 英双语 ④ 物联网 大 
数据 底层 操作系统 整合 Linux 开源 软件 和 通信 模块 
现 重点 收集 数据 分析程序 算法 模型 研发 简历 成立 
前 / 每季度 实习生 在 中关村 集中 面试 有意 实习 
半年 工作 一年 以上 的 开发 人员 请 注明 学历 
和 工作 简历 职务 和 职业 规划 吃住 薪酬 预期 
个人 爱好 等 事项 投递 邮箱 QinDragon2010 @ qq . 
com 主题 注明 应聘 数据 简化 DataSimp 合伙人 或 XX 
岗位 研发 岗 参考 本 蚊 及 文本 分析 一文 
的 二级 标题 1 技术 研发部 重 点收 简历 核心 
的 数据 分析 DA NLP DL 编程 技能 Windows / 
Linux / Android / iOS 平台 OA App 软件 开发 
基础 2 市场 客服部 研发部 兼职 搜集 客户资料 面见 客户 
形成 客户 需求分析 文档 跟踪 反馈 面谈 电邮 电话 邮寄 
沟通 服务 3 行政 后勤部 合伙人 兼职 高级 的 全系列 
文档 搜集 编辑整理 技能 OA 软件 界面 和 操作 体验 
实验 公司法律 财会 物业 文书 基础 详情 落地 前 发文 
宣传 西安 秦陇 纪 10 数据 简化 DataSimp 综合 汇编 
欢迎 有志于 数据 简化 之 传媒 技术 的 实力 伙伴 
加入 全球 数据 简化 DataSimp 团队 所谓/b 强/a AI/w 是/v 
指/n 和人/nr 一样/r 能够/v 用/p 一种/m 通用/v 算法/n 实现/v 不同/a 
功能/n 的/uj AI/w 现在 任何 有点 常识 的 人 一 
听到 强 AI 就会 皱起 眉头 那 还是 现在 技术 
瓶颈 以外 的 能力 但/c 我/r 一直/d 在/p 思考/v 这个/r 
通向/n 强/a AI/w 的/uj 那把/nr 金钥匙/n 究竟/d 在/p 哪里/r 最近 
突然 觉得 也 许强 AI 需要 的 技术 已经 成熟 
只是 我们 不 知道 怎么 把 现有 的 技术 结合 
在 一起 计算机 视觉 是 指 通过 处理 视频 信息 
提取 出 摄像头 周围 的 各种 场景 信息 比如 什么 
位置 有 一个 什么 物体 有 多大 我们 把 这些 
场景 信息 叫做 地图 这个 过程 叫 建 图 因为 
利用 这些 场景 信息 可以 反过来 计算出 摄像头 在 场景 
中 的 位置 这个 过程 叫做 定位 并且 对于 场景 
中 的 物体 还会 识别 他们 大概 是 什么 东西 
其实 这 正是 是 人 的 视觉 系统 负责 的 
任务 当 我 来到 一个 新的 环境 中时 我们 环顾四周 
然后 来回 走动 一下 大概 能 知道 周围 有些 什么 
物体 有 多大 有 多远 有些 什么 属性 然后 我们 
就 可以 自如 的 在 新 环境 中 穿梭 而 
不会 到处 碰撞 目前 计算机 视觉 的 水平 大概 是 
对于 地图 和 定位 的 精度 可以 达到 人 的 
水平 并且 可以 分割 出 不同 的 物体 然后 再 
结合 现有 识别 算法 可以 知道 每个 物体 的 名字 
如果 我们 还有 一套 名字 和 属性 的 数据库 我们/r 
就/d 能/v 知道/v 每个/r 物体/n 大概/d 有些/r 什么/r 特点/n 和/c 
功能/n 但 这里 的 瓶颈 是 如果 想要 万能 的 
识别 任何 物体 需要 极 其 大量 的 人工 物体 
标注 和 训练 且不说 现在 的 算法 能否 支持 如此 
大量 的 数据 计算机 视觉 方面 的 强 AI 瓶颈 
正式 在 这里 那么 这里 出现 了 两个 问题 1 
强 AI 是否 真的 需要 万能 的 识别 世界 上 
所有 的 物体 我们 认为 即使 是 才 出生 的 
婴儿 的 智力 也是 能够 达到 强 AI 的 要求 
但是 婴儿 能够 识别 的 物体 非常 有限 基本 就是 
屋里 那 几样 东西 所以 当 我们 把 使用 机器人 
的 场景 限定 一下 比如 只 在室内 只是 可移动 的 
物体 这样 需要 标注 和 训练 的 量 就 打打 
减少 了 其实 一个 成年人 能够 识别 出 无以 计数 
的 物体 是 通过 几十年 的 标注 和 训练 过程 
才 达到 的 2 能否 找到 一种 激励机制 让人 自发 
的 去 标注 物体 并 训练 AI 有 这样 一个 
数据 全人类 花在 玩 魔兽 世界 的 时间 累计 起来 
达 到了 593 万年 对于 人类 来说 593 万年前 人类 
的 祖先 刚刚 学会 直立 行走 所以 劳动力 这个 资源 
只要 有 合适 的 激励 几乎 是 无限 的 其实 
我们 把 全人类 用 在教 婴儿 理解 世界 的 时间 
加起来 应该 远远 大于 万 魔兽 世界 的 时间 再 
来说 下 自然 语言 处理 自然语言 处理 是 把 人类 
使用 的 语言 翻译 成 计算机 使用 的 语言 比如 
使用 自然 语言 处理 分析 一段 文字 后 可以 提取 
出 这段话 涉及 到 多少 个 物件 他们 的 关系 
是 什么 等等 信息 这些 信息 可以 使用 计算机 擅长 
的 方式 存储 和 使用 目前 自然语言 处理 已经 能够 
翻译 几乎 所有 逻辑 关系 的 文字 但/c 其/r 瓶颈/n 
在于/v 如果/c 要把/i 很/zg 所有/b 从/p 文字/n 中/f 提取/v 出来/v 
的/uj 物体/n 和/c 现实/n 中/f 的/uj 物体/n 对应/vn 起来/v 需要/v 
大量/n 的/uj 标注/v 和/c 训练/vn 并且 算法 不 一定 支持 
同样 我 也 提出 两个 问题 1 对于 抽象 的 
非 物体 的 词语 怎么 让 机器 去 理解 抽象 
的 非 物体 的 词 是 不能 被 简单 的 
标记 的 比如 愤怒 失望 成就 等等 这些 词 是 
建立 在 人类 大量 的 具体 词 和 价值观 的 
基础 上 形成 的 关于 价值观 的 AI 解释 又会 
是 一大 篇 文章 这里 只是 探究 最 简单 的 
强 AI 所以 就不 展 开说 AI 中的 价值观 了 
我 的 观点 是 即使 是 不 需要 这些 抽象词 
也/d 能/v 实现/v 强/a AI/w 比如 我们 可以 教会 婴儿 
从 一堆 物体 里面 拿出 我们 要求 的 东西 而 
不 需要 借助 任何 抽象 的 表达 2 能否 找到 
一种 激励机制 让人/i 自发/v 的/uj 去/v 建立/v 现实/n 中/f 的/uj 
物体/n 和/c 自然/d 语言/n 处理/v 得到/v 的/uj 物体/n 的/uj 关系/n 
这个/r 问题/n 的/uj 答案/n 和/c 上面/f 的/uj 第二个/m 问题/n 一样/r 
下面 我 用 场景 描述 的 方法 来 说明 假如 
我们 有 一个 机器人 这个 机器人 具备 这样 几个 功能 
1 视觉 建 图 定位 和 分割 物体 2 人 
的 手势 识别 可以 判断 人 的 手 所指 的 
方向 3 自然语言 处理 能够 分析 最 简单 的 逻辑 
比如 这 是 什么 那是/nr 什么 把 什么 东西 拿 
到 哪里 去 我们 把 这个 机器人 放到 一个 陌生 
的 房间 里面 就好像 第一次 把 小 婴儿 带回家 我们 
让 机器 人 自己 在 屋里 到处 闲逛 慢慢 的 
它 就能 知道 什么 地方 有 几个 物体 并且 随时 
知道 自己 在 房间 中 的 位置 然后 我们 指着 
一个 板凳 对着 机器人 说 这是 小 板凳 通过 手势 
识别 和 自然 语言 处理 我们 指向 的 这个 物体 
被 标记 为 小 板凳 并被 训练 了 一次 然后 
我们 指着 旁边 一个 大 一点 的 凳子 说 这是 
大 板凳 然后 第二个 物体 被 标注 为大 板凳 了 
我们 还 可以 用 多种 方式 来 训练 机器人 我们 
可以 说 到 小 板凳 旁边 去 通过 自然 语言 
处理 识别 出 去 这个 次 的 含义 并且 去 
这个 动作 已经 是 预先 写入 到 机器人 的 程序 
中 就像 人类 的 某些 行为 并 不是 后天 学习 
的 而是 被 预先 写在 DNA 里面 一样 如果 之前 
学习 小 板凳 成功 了 机器人 就 能 自己 跑到 
小 板凳 旁边 反之 我们 给 一个 失望 的 手势 
机器人 识别 出来 后 又 可以 进行 一次 标记 和 
学习 通过 这样 的 方式 我们 可以 教 会 所有 
房子 里 机器 人 需要 了解 的 物体 的 标记 
以及 位置 机器人 不 需要 了解 更多 的 物体 除非 
我们 需要 他 完成 新的 功能 其实 整个 过程 和 
我们 教 小 婴儿 的 方式 一模一样 而 实现 这 
一切 需要 的 技术 我们 现在 都 实现 了 如果 
我们 给 机器人 装上 一个 可以 拾取 物体 的 设备 
比如 一个 钳子 或者 吸盘 然后 在 机器人 的 预 
程序 中 写入 拿过来 这个 表达 对应 的 行为 那么 
我们 还 可以 对着 机器人 说 把 小 板凳 拿过来 
于是 机器人 就 能 移动 到 小 板凳 旁 再把 
小 板凳 拿过来 同理 我们 可以 教会 机器人 拿 任何 
房子 里 的 东西 同理 还有 很多 事情 可以 教会 
机器人 比如 把 黄色 的 鞋子 放到 门边 把 脏 
衣服 扔到 桶 里 之后 当 我们 回家 随手 把 
鞋子 一脱 衣服 一扔 机器人 都能 自动 帮 我们 收拾 
好 慢慢 的 这个 机器人 就像 是 自己 的 小 
婴儿 一样 慢慢 成长 难道 我们 不 愿意 花费 一些 
时间 在 教育 这个 因为 自己 而 独一无二 的 小 
婴儿 吗 这 正是 我 说 的 那种 训练 强 
AI 的 激励 机制 最后 的 结论 是 也 许强 
AI 会在 家庭 小型 机器人 的 应用 中 最先 实现 
1 . 引言 最 早接触 知识图谱 是 在 一篇 分析 
人工智能 的 文章 文章 提出 一个 很 有意思 的 观点 
在 感知 层面 人工智能 进步 很大 在 更 高级 的 
认知 层面 我们 现在 了解 的 仍然 很少 我 对这 
句话 的 粗浅 理解 是 人工智能 在 学习 数据 的 
内在 表示 无 监督 学习 或者 对 数据 的 输出 
结果 判别 方面 表现 出 了 强大 的 能力 甚至 
在 计算机 视觉 语音识别 机器 翻译 等 方面 接近 或 
超过 人类 的 表现 水平 但/c 这些/r 都还/nr 停留/v 在/p 
对/p 数据/n 内容/n 的/uj 归纳/v 和/c 感知/v 层面/n 对于/p 需要/v 
复杂/a 背景/n 知识/v 和/c 前后/f 上下文/l 的/uj 认知/v 和/c 推理/v 
层面/n 了解/v 仍然/d 不够/v 例如 我 有 一堆 数据 我 
想 让 机器 自己 学习 和 推理 出 正确 的 
知识 以及 知识 和 知识 的 联系 当然 知识图谱 也 
知识 在 认知 计算 领域 走出 了 一步 远未 达到 
人们 对 认知 的 期望 具体 到 知识图谱 简单 理解 
就是 一个 知识库 我们 能 利用 这个 知识库 给定 你 
要 查询 的 内容 然后 到 知识库 中去 进行 关联 
分析 和 推理 试图 让 机器 了解 你 的 意图 
反馈 和你/nr 查询 相关 内容 的 更多 关联 信息 举 
一个 简单 例子 我们 用 所有 的 菜谱 构建 知识图谱 
然后 问 夏天 西红柿 怎么做 汤 知识图谱 会 查询 夏天 
西红柿 和 汤 在 所有 菜谱 中的 直接 和 间接 
关系 进而 推荐给 你 几个 最 匹配 的 菜谱 就 
我 的 总结 知识图谱 有 两大类 主要 应用 a 搜索 
和 问答 类型 的 场景 b 自然语言 理解 类 的 
场景 典型 的 应用 场景 如下 那 知识图谱 是 怎么 
表示 的 呢 大多数 知识图谱 用 RDF Resource Description Framework 
表示 RDF 表征 了 实体 和 实体 的 关系 这种 
关系 有 两种 一种 是 属 性关系 即 一个 实体 
是 另一个 实体 的 属性 另一种 是 外部 关系 表明 
两 个 实体 之间 存在 外部 关联 RDF 形式 上 
表示 为 SPO Subject Predicate Object 三元组 所以 实体 通过 
关系 链接 成 无向 的 网络 例如 2 . 知识图谱 
的 架构 体系 可以 用 知名 的 知识 图谱 平台 
PlantData 为例 介绍 知识 图谱 的 架构 体系 从 图中 
我们 可以 看出 知识图谱 的 体系 分成 ４个 过程 数据采集 
知识 抽取 知识 链接 和 融合 知识 的 应用 首先 
说 数据采集 构建 知识图谱 是以 大量 的 数据 为基础 的 
需要 进行 大 规模 的 数据 采集 采集 的 数据 
来源 一般 是 网络 上 的 公开 数据 学术 领域 
的 已 整理 的 开放 数据 商业 领域 的 共享 
和 合作 数据 这些 数据 可能 是 结构化 的 半 
结构化 的 或者 非 结构化 的 数据 采集器 要 适应 
不同 类型 的 数据 知识 抽取 是 对 数据 进行 
粗 加工 将 数据 提取 成 实体 － 关系 三元组 
根据 数据 所在 的 问题 领域 抽取 方法 分成 开放 
支持 抽取 和 专有 领域 知识 抽取 知识 链接 和 
融合 由于 表征 知识 的 实体 － 关系 三元组 抽 
取自 不同 来源 的 数据 可能 不同 的 实体 可以 
进一步 融合 成 新的 实体 实现 在 抽象 层面 的 
融合 根据 融合 之后 的 新 实体 三元组 集合 可以 
进一步 学习 和 推理 将 表达 相同 或 相似 含义 
的 不同 关系 合并 成 相同 关系 检测 相同 实体 
对 之间 的 关系 冲突 等 知识图谱 构建 完成 之后 
形成 了 一个 无向图 网络 可以 运用 一些 图论 方法 
进行 网络 关联 分析 将其 用于 文档 检索 以及 智能 
决策 等 领域 例如 阿里 的 知识 图谱 以 商品 
标准 产品 标准 品牌 标准 条码 标准 分类 为 核心 
利用 实体 识别 实体 链 指 和 语义分析 技术 整合 
关联 了 例如 舆情 百科 国家 行业 标准 等 9 
大类 一级 本体 包含 了 百亿 级别 的 三元组 形成 
了 巨大 的 知识 网 然后 将 商品 知识图谱 广泛 
地 应用 于 搜索 前端 导购 平台 治理 智能 问答 
品牌商 运营 等 核心 创新 业务 3 . 知识图谱 的 
构建 知识 图谱 的 构建 有 两大类 方法 如果 知识 
领域 比较 贴近 开放 领域 可以 先 从 网络 上 
找 一个 开放 知识图谱 然后 以此 为 基础 进行 扩充 
如果 知识领域 只 某个 专有 行业 的 例如 信息 安全 
领域 则 开发 知识图谱 图谱 中 可 直接 使用 的 
知识 表示 相对 较少 需要 花 更多 的 精力 构建 
专业 的 知识 图谱 一个 典型 的 工具 是 Deepdive 
允许 通过 机器学习 和 人工 参与 的 方式 不断 迭代 
提升 知识图谱 不管 构建 哪 一类 的 知识图谱 都要 经历 
数据 收集 信息 抽取 链接 和 融合 数据 数据 可视化 
以及 分析 等 过程 目前 中国 的 知识图谱 从业者 们 
建立 了 一个 非常 好 的 开放 知识图谱 共享 网站 
OpenKG . CN 网址 是 http / / www . 
openkg . cn / 网站 的 主要 内容 如下 其中 
数据 栏目 里 给 出了 开源 知识图谱 或者 用于 构建 
知识 图谱 的 专业 数据集 工具 栏目 里 给出 了 
几十 种 用于 自然语言 处理 知识 抽取 知识 存储 知识 
表示 知识 链接 知识 推理 知识 查询 对话 系统 等 
用于 构建 知识 图谱 和 应用 知识图谱 的 工具 成员 
里 列出 了 参与 的 科研 机构 和 知识图谱 从业 
企业单位 我们 可以 利用 OpenKG . CN 里 提供 的 
数据集 和 工具 帮助 我们 构建 知识图谱 数据集 可以 帮助 
我们 建立 一个 知识 图谱 的 初始 版本 即从 里面 
获得 初始 的 知识 表示 SPO 三元组 然后 根据 我们 
收集 的 真实 业务 数据 再 进行 知识 抽取 和 
知识 推理 构建 知识 图谱 的 前提 是 收集 数据 
收集 的 数据 越 全面 则 可供 提取 的 知识 
表示 越 丰富 知识图谱 的 用处 越大 3.1 数据 收集 
收集 数据 的 方法 包括 a 收集 通用 的 百科知识 
包括 百度 百科 维基百科 等 b 收集 自然语言 处理 或者 
类似 OpenKG . CN 这类 网站 提供 的 公开 数据集 
例如 自然语言 处理 的 语料库 同义词 近义词 库 OpenKG . 
CN 提供 的 疾病 菜谱 人物 商品 音乐 企业 年报 
突发事件 脑科学 中文 地理 中医药 等 领域 的 数据集 c 
业务 领域 的 数据 从业者 所在 的 企业 或者 机构 
所 能 获取 的 问题 领域 的 数据 以上 数据 
的 规模 较大 需要 一个 大 数据 平台 来 支撑 
数据 的 收集 存储 和 查询 例如 利用 Hadoop 系统 
或者 单独 的 非 关系数据库 Redis Mongodb Hbase 和 postgresql 
等 数据库 进行 存储 3.2 知识 抽取 生成 SPO 三元组 
收集 数据 之后 需要 对 数据 进行 处理 这 里面 
最 有价值 的 首先 是 文本 数据 因此 要 用到 
自然语言 处理 基本 的 过程 是 语言 分词 词性 标注 
命名 实体 识别 句法分析 更 高级 写 的 应用 还 
包括 语义 依存 分析 对于 构建 知识库 而言 自然语言 处理 
的 目的 是 获取 命名 实体 再 根据 命名 实体 
和 句法分析 抽取 知识 三元组 SPO 自然语言/l 处理/v 有/v 两个/m 
强大/a 的/uj 工具/n NLTK/w 和/c Standford/w NLP 由于 Standford NLP 
提供 了 开放 信息 抽取 OpenIE 功能 用于 提取 三元组 
SPO 所以 使用 Standford NLP 更 贴合 知识图谱 构建 任务 
比较 麻烦 的 一点 是 Standford NLP 需要 的 计算 
资源 和 内存 较大 推荐 内存 4GB 启动 时间 较长 
分析 效率 低于 NLTK 不过 支持 文件 列表 的 输入 
方式 实现 一次 多 文件 输入 得到 多 个 文件 
的 输出 结果 总体 效率 还好 当然 研究者 也 开发 
和 共享 了 更多 的 知识 抽取 工具 例如 OpenKG 
. CN 里 除了 Standford NLP 还 提供 了 Reverb 
开放 三元组 抽取 SOFIE 抽取 链接 本体 及 本体 间 
关系 OLLIE 开放 三元组 知识 抽取 等 工具 3 . 
2.1 DeepDive 以上 知识 抽取 工具 有 一个 共同 的 
缺点 是 利用 别人 训 练好 的 模型 按照 给定 
的 模式 进行 抽取 对于 开放 领域 的 知识 抽取 
可能 是 足够 的 但 对于 专业 领域 例如 某个 
特定 行业 的 知识 抽取 可能 提供 的 工具 并 
没有 覆盖 到 该 行业 领域 此时 该 工具 进行 
知识 抽取 的 准确率 是 比较 低 的 需要 一个 
能够 根据 你 自己 收集 的 业务 数据 自适应 的 
更新 知识 抽取 的 模型 通过 不断 迭代 的 方式 
逐渐 提升 知识 抽取 的 准确性 这个 迭代 过程 要 
允许 人工 参与 Deepdive 是 一款 被 广泛 使用 的 
知识 抽取 开源 工具 DeepDive http / / deepdive . 
stanford . edu / 是 斯坦福大学 开发 的 信息 抽取 
系统 能 处理 文本 表格 图表 图片 等 多种 格式 
的 无 结构 数据 从中 抽取 结构化 的 信息 系统 
集成 了 文件 分析 信息提取 信息 整合 概率 预测 等 
功能 Deepdive 在 OpenKG . CN 上 有一个 中文 的 
教程 http / / openkg1 . oss cn beijing . 
aliyuncs . com / 478e0087 8dd6 417c 9a49 4ce12f5ec22c / 
tutorial . pdfDeepDive 系统 的 基本 输入 包括 1   
无 结构 数据 如 自然语言 文本 2   现有 知识库 
或 知识图谱 中 的 相关 知识 3   若干 启发式 
规则 DeepDive 系统 的 基本 输出 包括 1   规定 
形式 的 结构化 知识 可以为 关系 实体 1 实体 2 
或者 属性 实体 属性值 等 形式 2   对 每 
一条 提取 信息 的 概率 预测 DeepDive 系统 运行 过程 
中 还包括 一个 重要 的 迭代 环节 即 每 轮 
输出 生成 后 用户 需要 对 运行 结果 进行 错误 
分析 通过 特征 调整 更新 知识库 信息 修改 规则 等 
手段 干预 系统 的 学习 这样 的 交互 与 迭代计算 
能 使得 系统 的 输出 不断 得到 改进 DeepDive 系统 
架构 和 工作 流程 DeepDive 主要 针对 关系 抽取 在 
指定 的 关系 抽取 中 效果 比较 理想 在 实体 
确定 后 可以 很好 地 进行 关系 抽取 同时 也 
支持 中文 关系 抽取 仅 需要 引入 中文 相关 的 
基础 处理 工具 即可 详情 参考 http / / www 
. openkg . cn / tool / cn deepdive 不足之处 
在于 未 提供 专门 的 针对 概念 实体 和 事件 
抽取 的 支持 同时 需要 大量 的 标注 语料 支持 
并 通过 人工 设置 标注 规则 总结 一下 知识 三元组 
的 抽取 对于 开放 领域 的 信息 抽取 直接 使用 
现有 OpenIE 工具 对于 特定 行业 领域 内 的 信息 
抽取 需要 使用 类似 Deepdive 这样 的 工具 在内部 集成 
自然语言 处理 工具 实体 识别 工具 实体 对 之间 的 
关系 抽取 人工 标注 修正 错误 等 步骤 实体 识别 
工具 可以 直接 用 资 源语言 处理 领域 的 命名 
实体 识别 NER 工具 也 可以 根据 从外部 或者 人工 
提取 的 知识 库 进行 实体 匹配 最 难做 的 
是 实体 对 之间 的 关系 抽取 Deepdive 对 实体 
对 之间 的 关系 通过 弱 监督 训练 和 预测 
的 方法 具体 步骤 是 a   先 通过 启发式 
规则 的 方式 标注 一 部分 实体 对 之间 的 
关系 作为 监督 学习 的 标记 b   对 每个 
实体 对 所在 的 文本 进行 特征提取 生成 监督 学习 
的 特征向量 c   根据 启发式 规则 的 标注 对 
所有 已 标 实体 对 所在 文本 的 特征向量 进行 
监督 学习 的 训练 过程 生成 预测模型 再 根据 预测模型 
预测 未 标注 实体 对 的 关系 标签 得到 所有 
候选 实体 对 的 关系 标签 d   导出 所有 
候选 实体 对 及其 关系 标签 然后 对 SPO 三元组 
做 人工 确认 将 人工 修改后 实体 对 的 关系 
标记 重新 导入 启发式 规则 作为 监督 学习 的 已 
标注 样本 重复 以上 监督 学习 的 训练 预测 过程 
和 人工 确认 过程 迭代 式 的 实现 实体 对 
关系 的 更新 Deepdive 中 的 监督 学习 是 一种 
远程 监督 学习 技术 为了 打破 有 监督 学习 中 
人工 数据 标注 的 局限性 Mintz 等人 提出 了 远程 
监督 Distant Supervision 算法 该 算法 的 核心 思想 是 
将 文本 与 大规模 知识图谱 进行 实体 对齐 利用 知识图谱 
已有 的 实体 间 关系 对 文本 进行 标注 远程 
监督 基于 的 基本 假设 是 如果 从 知识图谱 中 
可 获取 三元组 R E1 E2 注 R 代表 关系 
E1 E2 代表 两 个 实体 且 E1 和 E2 
共 现 与 句子 中 则 表达 了 E1 和E/nr 
2间 的 关系 R 标注 为 训练 正 例 远程 
监督 算法 是 目前 主流 的 关系 抽取 系统 广泛 
采用 的 方法 也是 该 领域 的 研究 热点 之一 
该 算法 很好 地 解决 了 数据 标注 的 规模 
问题 但 它 基于 的 基本 假设 过强 会 引入 
大量 噪音 数据 出现 the wrong label problem 的 问题 
原因 是 远程 监督 假设 一个 实体 对 只 对应 
一种 关系 但 实际上 实体 对 间 可以 同时 具有 
多种 关系 如 上例 中 还 存在 CEO 乔布斯 苹果公司 
的 关系 实体 对 间 也 可能 不 存在 通常 
定义 的 某种 关系 而 仅 因为 共同 涉及 了 
某个 话题 才在 句 中共 现 为了 减小 the wrong 
label problem 的 影响 学术界 陆续 提出 了 多种 改进 
算法 主要 包括 a   基于 规则 的 方法 通过 
对 wrong label cases 的 统计 分析 添加 规则 将 
原本 获得 正 例 标注 的 wrong label cases 直 
接标 为 负 例 或 通过 分值 控制 抵消 原有 
的 正 标注 b   基于 图 模型 的 方法 
构建 因 子图 factor graph 等 能 表征 变量 间 
关联 的 图 模型 通过 对 特征 的 学习 和对/nr 
特征 权重 的 推算 减小 wrong label cases 对 全局 
的 影响 c   基于 多 示例 学习 multi instance 
learning 的 方法 将 所有 包含 E1 E2 的 句子 
组成 一个 bag 从 每个 bag 对 句子 进行 筛选 
来 生成 训练样本 除了 Deepdive 的 关系 抽取 技术 基于 
深度 学习 的 关系 技术 也 很 流行 两种 方法 
相辅相成 各 有优势 DeepDive/w 系统/n 较多/i 依赖/v 于/p 自然/d 语言/n 
处理/v 工具/n 和/c 基于/p 上下文/l 的/uj 特征/n 进行/v 抽取/v 在 
语料 规模 的 选择 上 更为 灵活 能/v 进行/v 有/v 
针对性/n 的/uj 关系/n 抽取/v 且/zg 能/v 方便/a 地/uv 在/p 抽取/v 
过程/n 中/f 进行/v 人工/n 检验/vn 和/c 干预/v 而/c 深度/ns 学习/v 
的/uj 方法/n 主要/b 应/v 用了/i 词/n 向量/n 和/c 卷积/n 神经网络/n 
在/p 大规模/b 语料/n 处理/v 和多/nr 关系/n 抽取/v 的/uj 人物/n 中/f 
有/v 明显/a 的/uj 优势/n ４ . 知识图谱 的 应用 4.1 
进行 图 分析 列举 一些 我们 常用 的 图 算法 
图 遍历 广度 优先 遍历 深度 优先 遍历 最短 路径 
查询 Dijkstra 迪 杰 斯特拉 算法 Floyd 弗洛伊德 算法 路径 
探寻 给定 两个 或 多个 节点 发现 它们 之间 的 
关联关系 权威 节点 分析 PageRank 算法 族群 发现 最 大流 
算法 相似 节点 发现 基于 节点 属性 关系 的 相似 
度 算法 其中 权威 节点 分析 做过 社交 网络 分析 
的 人 应该 都 知道 可以 用来 做 社交 网络 
里 的 权威 人物 分析 我们 在 创投 知识图谱 中 
用来 做 权威 投资 机构 的 发现 族群 发现 算法 
一般 用来 在 社交 网络 中 主题 社区 的 发现 
在 这里 我们 同样 可以 用来 识别 企业 知识图谱 中的 
派系 阿里 系 腾讯 系 相似 节点 发现 应用 就 
更加 广泛 了 在 企业 知识图谱 中 可以 做 相似 
企业 的 发现 这里 有个 很 重要 的 实际 应用 
场景 可以 利用 相似 企业 进行 精准 的 获 客 
营销 4.2 基于 本体 的 推理 基于 本体 的 知识 
推理 应用 也 非常 的 多 比如 我们 在 实际 
场景 中的 冲突检测 因为 不管 是 手动 构建 还是 自动 
构建 知识图谱 都会 碰到 这样 一个 问题 或者 数据 来源 
不同 或者 构建 的 人员 不同 方法 不同 这就 会 
不可避免 的 导致 一些 冲突 这些 冲突 自身 很难 直观 
的 去 发现 但是 可以 利用 知识图谱 里面 的 冲突检测 
去 发现 存在 的 有 矛盾 的 有 冲突 的 
知识 本体 推理 基本 方法 包括 基于 表 运算 及 
改进 的 方法 FaCT + + Racer Pellet Hermit 等 
基于 一 阶 查询 重写 的 方法 Ontology based data 
access 基于 本体 的 数据 访问 基于 产生 式 规则 
的 算法 如 rete Jena Sesame OWLIM 等 基于 Datalog 
转换 的 方法 如 KAON RDFox 等 回答 集 程序 
Answer set p r o g r a m m 
i n g O p e n K G . 
CN 上有 一些 知识 推理 的 工具 例如 http / 
/ www . openkg . cn / tool tags = 
% E 7% 9F % A 5% E 8% AF 
% 86% E 6% 8E % A 8% E 7% 
90% 864.3 基于 规则 的 推理 基于 规则 的 推理 
是 在 知识图谱 基础 知识 的 基础 上 专家 依据 
行业 应用 的 业务 特征 进行 规则 的 定义 这在 
业务 应用 中 是 非常 常见 的 基于 规则 的 
推理 是 在 知识图谱 基础 知识 的 基础 上 专家 
依据 行业 应用 的 业务 特征 进行 规则 的 定义 
这在 业务 应用 中 是 非常 常见 的 介绍 一下 
我们 常用 的 Drools 因被 JBOSS 收购 现已 更名 为 
JBoss Rules 它 是 为 Java 量身 定制 的 基于 
Charles Forgy 的 RETE 算法 的 规则 引擎 的 实现 
使用 了 OO 接口 的 RETE 使得 商业 规则 有了 
更 自然 的 表达 其 推理 的 效率 也 比较 
高 结合 规则 引擎 工具 基于 基础 知识 与 所 
定义 的 规则 执行 推理 过程 给出 推理 结果 4.4 
可视化 辅助 决策 首先 介绍 两个 比较 常见 的 可视化 
工具 D3 . js 和 ECharts D3 . js 全称 
Data Driven Documents 是 一个 用 动态图形 显示 数据 的 
JavaScript 库 一个 数据 可视化 工具 它 提供 了 各种 
简单易用 的 函数 大大 方便 了 数据 可视化 的 工作 
ECharts 是 一款 由 百度 前端 技术部 开发 的 同样 
基于 Javascript 的 数据 可视化 图标库 它 提供 大量 常用 
的 数据 可视化 图表 对于 出入门 的 知识图谱 使用者 推荐 
两个 入门 级别 的 开源 的 知识图谱 展示 工具 a 
知识图谱 Demo Demo 的 详细 介绍 https / / zhuanlan 
. zhihu . com / p / 29332977 group _ 
id = 8 9 1 6 6 8 2 2 
1 5 5 8 6 6 1 1 2 0 
开 源代码 网址 https / / github . com / 
Shuang0420 / knowledge _ graph _ demo 生成 的 图谱 
展示 结果 如下 b 农业 知识图谱 KG 农业 领域 的 
信息 检索 命名 实体 识别 关系 抽取 分类 树 构建 
数据挖掘 开 源代码 网址 https / / github . com 
/ qq547276542 / Agriculture _ KnowledgeGraph 知识图谱 的 Demo 展示 
网址 http / / ecnukg . vicp . io / 
展示 效果 如下 更 深入 的 应用 或 则 展示 
可以 参考 商业 知识图谱 平台 PlantData 网址 是 https / 
/ wx . jdcloud . com / shop / shopDetail 
/ HiKnowledge4 . 5 问答 系统 这里 介绍 一个 OpenKG 
. CN 的 问答 demo 基于 REfO 的 KBQA 实现 
及 示例 网址 是 http / / www . openkg 
. cn / tool / refo kbqa 这 是 一个 
基于 Python 模块 REfO 实现 的 知识 库 问答 初级 
系统 . 该 问答 系统 可以 解析 输入 的 自然 
语言 问句 生成 SPARQL 查询 进一步 请求 后台 基于 TDB 
知识库 的 Apache Jena Fuseki 服务 得到 结果 这 是 
一个 入门级 的 例子 . 内含 介绍 此 项目 的 
README . pdf . 方便 用户 快速 把握 这个 项目 
的 想法 . 希望 用户 体会 默认 的 3 类 
5 个 问题 . 不同 的 表述 能够 用 统一 
的 对象 正则表达式 匹配 得到 结果 进而 生成 对应 SPARQL 
查询 语句 知识库 由 大量 的 三元组 组成 并且/c 这些/r 
三元组/n 的/uj 实体/n 和/c 实体/n 关系/n 都是/nr 形式化/v 的/uj 语言/n 
给定 一个 自然 语言 的 问题 Where was Obama born 
我们 面临 的 第一 个 挑战 就是 如何 建立 问题 
到 知识库 的 映射 语义 解析 KB QA 的 思路 
是 通过 对 自然 语言 进行 语义上 的 分析 转化 
成为 一种 能够 让 知识库 看懂 的 语义 表示 即 
逻辑 形式 Logic Form 进而 通过 知识库 中 的 知识 
进行 推理 Inference 查询 Query 得出 最终 的 答案 KB 
QA 的 详细 介绍 可以 参考 知乎 专栏 揭开 知识库 
问答 KB QA 的 面纱 由于 个人 对 知识 图谱 
的 理解 也 比较 浅显 本文 只是 记录 自己 的 
一些 平时 整理 的 知识 和 经验 方便 自己 以后 
查询 和 深入 学习 个人 学习 nlp 笔记 学习材料 CS124 
COSC572 和 Speech and Language Processing 第三版 自然语言 处理 学习 
笔记 四 1 . 信息检索 2 . 词汇 文本 关联矩阵 
3 . 倒排索引 Inverted Index 2.1 倒排索引 的 结构 2.2 
用 倒排索引 的 查询处理 Query processing 2.3 布尔 检索 模型 
4 . 短语 查询 4.1 双词/nr 索引 biwords indexes 4.2 
带 位置 的 索引 5 . 排序 检索 5.1 Jaccard 
系数 5.2 加权 词频 5.3 逆 文档 频率 加权 Invrse 
document frequency weighting / IDF 6 . TF IDF7 . 
向量空间 模型 8 . TF IDF 的 cosine 得分 1 
. 信息检索 从 文档 中 提取 需要 的 信息 info 
need 步骤 里 把 我们 想 要 的 信息 翻译 
为 搜索框 能够 理解 的 形式 query 里 翻译 为 
搜索引擎 能 理解 的 形式 这个 过程 中 主要 会 
出现 两种 错误 本课 主要 关注 第二种 即 怎样 才能 
正确 组织 文字 来 送到 搜索引擎 我们 选择 how trap 
mice alive how trap mice without killing 或者 是 加上 
引号 都有/nr 不同/a 效果/n 如何 评价 是否 很好 地 检索 
到 文件 2 . 词汇 文本 关联矩阵 比如 在 所有 
文档 种 我们 想 检索 A and B but NOT 
C 我们 可以 用 正则 的 方法 但是 这对 大 
的 语料库 很慢 而且 很多 复杂 操作 不 能用 或不 
灵活 而且 我们 还要 能对 文档 进行 排序 我们 可以 
引入 词汇 文本 关联矩阵 来 解决 上诉 的 那个 要求 
因为 从 矩阵 种 我们 知道 这些 词汇 是否 在 
某个 文档 种 存在 用 其 的 二进制 形式 表示 
若为 NOT 则 取反 即 101111 就 表示 Calpurnia 是否 
在 这 6个 文档 中 出现 因为 前面 是 NOT 
所以 010000 取反 在 大 文档 中 100w 个 1000字 
的 文档 而 我们 的 term 有 500k 个 得到 
一个 巨大 的 文档 其中 绝大多数 都是 0 因此 需要 
更好 的 比如 只 记录 1 的 数据结构 3 . 
倒排索引 Inverted Index 若 只是 最 普通 的 数据 结构 
因为 文字 出现 的 频率 不同 每个 列表 包含 的 
内容 长度 不同 同时 因为 列表 是 有序 的 所以 
插入 数据 时候 也会 很麻烦 2.1 倒排索引 的 结构 第一步 
得到 一个 token 和其的/nr documentID 由 词汇 进行 排序 字母 
表中 顺序 把 同 个 文档 中 重复 的 token 
只 考虑 一个 映 射到 dictionary 和 postings 中 同时 
在 dictionary 中 记录 出现 频率 也 就是 这个 词 
在 几个 文档 中 出现 了 2.2 用 倒排索引 的 
查询处理 Query processing 利用 倒排索引 完成 查询 操作 AND 抽取 
Brutus 和 Caesar 的 postings 并合 并用 指针 进行 元素 
间 的 对比 如 Brutus 出现 第一个 在 2 而 
caesar 在 1 不同 则 两者 小 的 指针 向前 
一格 此时 brutus 在 2 caesar 也在 2 两者都 前进 
一格 伪代码 2.3 布尔 检索 模型 一个 法律 领域 的 
检索 模型 例子 所以 说 这个 方法 过时 但 还是 
在 一些 地方 适用 在 a and b and c 
的 query 中 从短的/nr 开始 把 全部 都 遍历 了 
若 存在 多个 or 操作 先 估计 or 的 尺寸 
先 对 小 的 预测 的 也 就是 直接 or 
两端 的 频数 相加 进行 操作 如果 是 not 呢 
4 . 短语 查询 我们 经常 把 stanford university 或者 
san francisco 当成 一个 短语 也 就是 一个 不 被 
分割 的 整体 这样 倒排索引 就 解决 不 了了 4.1 
双词/nr 索引 biwords indexes 第一步 的 尝试 就算 建立 一个 
两个 词 的 逆序 索引 在 大于 2 的 短语 
中 比如 stanford university palo alto 可以 分为 stanford university 
AND university palo AND palo alto 这样 就 等同于 前面 
单个 词 的 逆序 索引 了 但是 也 有一个 问题 
上文 这 4个 词 若 只找到 同时 连续 出现 才 
有意义 长 短语 那 文档 不同 地方 分别 出现 就 
没 意义 了 这 也 就是 会 导致 positive falsely 
不过 其实 问题 不大 一种 扩展 的 双词/nr 方法 对 
编入索引 的 文本 进行 词性 标注 若 词 term 为 
名词 N 或 冠词 / 介词 X 称/v 具有/v NX/w 
*/i N/w 形式/n 的/uj 词/n 项序/nr 列为/v 扩展/v 双词/nr extended 
biword 将 这样 扩展 词 对 作为 一个 词 项 
放入 词典 中 比如 索引 catcher in the rye 麦田 
守望者 时 N X X N 符合 NX * N 
将/d 查询/v 分析/vn 成N和/nr X/w 序列/n 将 查询 切 分成 
扩展 双词/nr 在 索引 中 查找 catcher rye 字典 太大 
存在 false positive 问题 但是 可以 作为 综合 的 索引 
策略 第一 部分 4.2 带 位置 的 索引 第二 种 
方法 带 位置 的 索引 先 对比 是否 同时 出现 
在 一个 文档 然后 进行 对比 此处 to 要为 be 
的 位置 1 这个 方法 同样 可以 用到 模糊 搜索 
上 带 位置 信息 的 索引 特点 储存 要求 大 
不过 很 灵活 而且 可压缩 大小 和 文档 长度 有关 
若 文档 不长 那和/nr 普通/nz 的/uj posting/w 差不多/l 大小/b 若 
很大 比如 书 等 大约 100000 词 那 就是 其 
100倍 总的来说 比 无 位置 信息 大 2 ~ 4倍 
不过 仍是 原 文本 的 35% ~ 50% 无 位置 
信息 的 大致 是 10% 一种 将 biword 和 位置 
信息 的 索引 结合 的 办法 省时 但 费 内存 
5 . 排序 检索 普通 的 布尔 检索 没能 满足 
用户 的 需要 尤其 是 同时 返回 上千 个 结果 
时候 这 是 我们 需要 对 索引 进行 排序 给用户 
最早 看到 最 重要 的 内容 5.1 Jaccard 系数 例子 
但 也 存在 问题 比如 没用 单词 的 出现 次数 
而且 标准化 的 方式 不大 对 5.2 加权 词频 但是 
这样 的 模型 没有 位置 信息 这样 A is better 
than B 和B/nr is better than A 的 词 袋 
模型 其实 是 一样 的 intuition 是 词频 和 相关性 
是 相关 的 但 不是 线性 的 评分 就 5.3 
逆 文档 频率 加权 Invrse document frequency weighting / IDF 
intuition 检索 中 不 常见 的 词 应当 有 更多 
信息 应当 赋予 更高 的 权重 像 it and 这类 
停用词 基本 没有 什么 信息 所以 使用 词 在 多少 
文档 中 出现 来 表示 其 是否 常见 dftdf _ 
{ t } dft 表示 词 在 多少 文档 中 
出现 N 表示 语料库 中的 文档 个数 则 idft ∈ 
0 log10N idf _ { t } \ in 0 
log _ { 10 } N idft ∈ 0 log10 
N 若 词 在 每个 文档 中 都 出现 那其 
的 逆 文档 频率 / idfidfidf 为 0 且 若 
语料库 是 不变 的 那么 我们 得到 的 idf 也是 
不变 的 是 一个 对应 每个 词 的 值 单词 
检索 时候 idf 只是 一个 点 值 scalar 对 检索 
没 影响 但是 在 多个 单词 的 索引 时 idf 
可以 给 诸如 capricous person 两个 词 赋权 少见 的 
前者 赋予 更高 的 权重 try/w 和/c insurance/w 虽然/c 出现/v 
的/uj 频率/n 都/d 差不多/l 但是 try 出 现地 很广 而 
insurance 出现 的 文章 较少 因此 虽然 总 频数 差不多 
根据 idf 的 原理 因 赋予 insurance 更高 权重 6 
. TF IDFTF IDF 是 信息检索 领域 中 最重要 的 
加权 方法 之一 第一 个 系数 1 + logtft d1 
+ logtf _ { t d } 1 + logtft 
d 表示 的 是 前文 对词 出现 频率 的 加权 
方法 log frequency 其 大小 与 词 出现 次数 成 
正相关 而 后方 的 log10 N / dft log _ 
{ 10 } N / df _ { t } 
log10 N / dft 指 的 是 单词 在 多少 
文档 出现 的 反比 也 就是 前文 的 idf 总得分 
所有 单词 的 tf idf 加 合 7 . 向量空间 
模型 前文 我们 把 文档 当作 了 一个 向量 这是 
很 稀疏 的 向量 同时 占据 了 很大 的 内存空间 
在 查询 处 我们 也 将 问题 转换 为 向量 
并于 文档 进行 相似 度 对比 排序 相似 度 约等于 
距离 的 倒数 越近越/nr 相似/v 但 通常 使用 的 欧式 
距离 有 很多 问题 比如 向量 的 长度 对 距离 
大小 影响 很大 如 下图 尽管 q 和 d2 看似 
最 相近 但是 因此 向量 长度 导致 查询 向量 q 
和 d1 或 d3 最 相似 取而代之 我们 可以 使用 
cosines 来 计算 其 在 0 π 0 \ pi 
0 π 中为 减 函数 值 从1到/nr 1 能 满足 
我们 的 需求 L2 正则 使 文档 文档 大小 的 
影响 变小 若 文档 和 查询 向量 都已 标准化 直接 
使用 下列 式子 emmm 好像 也 可以 用 log 词频 
来 替代 词频 8 . TF IDF 的 cosine 得分 
综上所述 我们 有 很多 种 方法 来做 加权 这 就像 
是 一种 组合 若 我们 使用 log 词频 逆 文档 
词频 加上 L2 norm 则 我们 用 的 为 ltc 
smart notation log 词频 适 应于 长 文档 计算 实例 
注意 标准化 时候 若 前方 使用 的 是 加权 词频 
则 标准化 时候 使用 的 就是 加权 后的/nr 词频 作者 
刘才权/nr 编辑/n 陈 人和 前   言在/nr 这个 日新月异 的 
信息 时代 海量 数据 的 积累 计算 能力 的 不断 
提升 机器学习 尤其 是 深度 学习 的 蓬勃 发展 使得 
人工智能 技术 在 不同 领域 焕发 出 蓬勃 的 活力 
自己 经历 了 嵌入式 开发 移动 互联网 开发 目前 从事 
自然语言 处理 算法 开发 工作 从 工程 软件 开发 到 
自然 语言 处理 算法 开发 希望 通过 这个 系列 的 
文章 能够 由浅入深 通俗 易懂 的 介绍 自然语言 处理 的 
领域 知识 分享 自己 的 成长 同 大家 一起 进步 
章节 目录 问题 描述 问题 简化 关键词 匹配 倒排索引 搜索引擎 
框架 小结 01 问题 描述 倒排索引 是 搜索引擎 的 基础 
算法 在 本文 中 我们 以 一个 简单 的 例子 
来 详细 介绍 倒排索引 的 思想 和 实现 假设 用户 
有个 搜索 query 林俊杰 2019 演唱会 行程 百度 的 搜索 
结果 如下 如果 要求 你 来 设计 一个 搜索引擎 来 
解决 这个 问题 你 会 如何 着手 呢 02 问题 
简化 现在 我们 把 这个 问题 具体化 我们 除了 有要/nr 
查询 的 query 林俊杰 2019 演唱会 行程 还有 被 查询 
的 网页 数据库 这里 我们 做个 简化 假设 我们 的 
网页 数据库 内容 只有 如下 4条 网页 1 2019年 JJ 
林俊杰 全球 演唱会 在 北京 首场 演出 行程 如下 xxxxxxx 
网页 2 林俊杰 吴亦凡 终于 同 框 合影 惹 粉丝 
们 尖叫 连连 xxxxx 网页 3 蔡依林 2019 世界 演唱会 
行程 全 曝光 xxxxx 网页 4 告别 2018 迎接 崭新 
的 2019 xxxxxx 简单 来说 就是 从 网页 1 ~ 
4中 选取 最 理想 的 查询 结果 你 会 怎么做 
呢 03 关键词 匹配 最 容易 想到 的 方法 就是 
关键词 匹配 了 简单 的 来说 就是 网页 中 包含 
查询 的 关键词 越多 网页 和 查询 query 的 相关度 
也就 越大 在做 关键词 查询 前 一般 文 本会 先 
进行 预处理 这里 的 预处理 主要 包括 去 停用词 和 
分词 去 停用词 去除 和 查询 不 相关 的 内容 
比如 本 例子 中的 标点符号 在 其他 场景 中 除了 
标点符号 也会 去除 一些 特别 的 字 或 词 分词 
分词 主要 目的 是 将 句子 切 长 短语 或 
关键字 这样 才 利于 查询 匹配 比如 林俊杰 2019 演唱会 
行程 可以 分词 成 林俊杰 / 2019 / 演唱会 / 
行程 当然 网页 也 需要 这样 进行 分词 网页 1 
2019 / 年//nr JJ / 林俊杰 / 全球 / 演唱会 
/ 在 / 北京 / 首场 / 演出 / 行程 
/ 如下 / xxxxxx 网页 2 林俊杰 / 吴亦凡 / 
终于 / 同 框 / 合影 / 惹 / 粉丝 
们 / 尖叫 / 连连 / xxxxx 网页 3 蔡依林 
/ 2019 / 世界 / 演唱会 / 行程 / 全 
曝光 / xxxx 网页 4 告别 / 2018 / 迎接 
/ 崭新 / 的 / 2019 / xxxxxx 分词 是 
一项 专门 的 技术 在 实际 工程 中 可以 至今 
借助 工具 来 完成 比如 jieba 分词 分词 处理 后 
我们 用 查询 query 中的 关键词 在 网页 数据库 中 
进行 关键词 匹配 并 统计 匹配 数目 网页 序号 匹配 
关键词 匹配 个数 网页 12019 林俊杰 演唱会 行程 4 网页 
2 林俊杰 1 网页 32019 演唱会 行程 3 网页 420191 
从 匹配 个数 中 很容易 确定 网页 1 就是 和 
查询 query 最 匹配 的 网页 04 倒排索引 讲到 这里 
大家 可能会 疑问 这/r 和/c 倒排索引/i 有/v 什么/r 关系/n 实际上 
如果 仔细 考虑 上面 的 关键词 查询 过程 会 发现 
这种 方法 有个/nr 很大 的 效率 问题 我们 的 例子 
中 只有 4个 待 查询 的 网页 而 实际 的 
互联网 世界 的 网页 数目 是 非常 巨大 的 假设 
互联网 世界 的 网页 数据 为 N 那么 使用 关键词 
查询 的 时间 复杂度 就是 O N 然 这样 的 
时间 复杂度 还是 太 大了 而 倒排索引 就 很好 的 
优化 了 这个 问题 从 倒排索引 这个 名字 很容易 联 
想出 它 的 实现 关键 就是 倒排 的 索引 在 
前面 的 讲解 中 我们 的 索引 key 是 网页 
内容 value 是 关键字 倒排索引 就是 反过来 内容 关键字 作为 
索引 key 所在 网页 作为 内容 value 前面 的 表格 
就 可以 改 写成 关键词 包含 关键词 的 网页 林俊杰 
网页 1 网页 22019 网页 1 网页 3 网页 4 
演唱会 网页 1 网页 3 行程 网页 1 网页 3 
通过 上面 的 表格 很明显 网页 1 是 包含 最多 
关键词 的 网页 也是 和 查询 query 相关度 最高 的 
网页 采用 倒排索引 的 方法 搜索 的 时间 复杂度 得到 
了 明显 的 降低 05 搜索引擎 框架 有了 倒排索引 的 
知识 我们 就 可以 搭建 简单 的 搜索引擎 了 具体步骤 
包括 网页 抓取 主要 是 借助 网络爬虫 来 抓取 网络 
世界 的 所有 网页 并 进行 存储 网络爬虫 是 一项 
专门 的 技术 目前 工程 上 也 有 很多 现成 
的 开源 工具 倒排索引 生成 将 抓取 后的/nr 网页 经过 
预处理 后 整理 生成 倒排索引 用户 在线 查询 借助 倒排索引 
搜索引擎 能够 满足 用户 的 实时 在线 查询 前 两个 
步骤 是 不用 考虑 实时性 的 可以 离线 进行 而 
用户 的 在线 查询 则 需要 保证 实时性 06 小结 
本文 通过 一个 搜索 查询 的 例子 引出 关键词 查询 
的 方案 及 遇到 的 问题 进而 介绍 了 倒排索引 
的 原理 和 搜索 引擎 的 整体 框架 现代 搜索引擎 
是 一个 非常 庞大 和 复杂 的 系统工程 这里 的 
例子 只是 为了 方便 大家 理解 做了 特别 的 简化 
文中 提到 的 分词 和 网络爬虫 也是 专门 的 文本 
处理 技术 在 后续 的 文章 后 会 根据 需要 
专门 展开 END 往 期 回顾 之 作者 刘才权/nr 1 
机器学习 笔记 神经网络 5 2 从零开始 学习 自然语言 处理 NLP 
基础 准备 0 3 机器学习 笔记 降 维 与 度量 
学习 10 4 机器学习 笔记 聚 类 9 5 机器学习 
笔记 集成 学习 8 6 机器学习 笔记 贝叶斯 分类器 7 
机器学习 算法 工程师 一个 用心 的 公众 号 长按 识别 
加 关注 进 群 学习 得 帮助 你 的 关注 
我们 的 热度 我们 一定 给 你 学习 最大 的 
帮助 你 点 的 每个 赞 我 都 认真 当成 
了 喜欢 目录 Google BERT 自然语言 处理 框架 Google BERT 
都能 解决 哪些 问题 Google BERT 自然语言 处理 框架 2018 
之秋 一篇 谷歌 新 发布 的 BERT 模型 突破 11项 
纪录 的 文章 一 出来 重燃 大家 对 人工智能 自然语言 
处理 领域 的 热情 借此 热点 在 这里 整理 一下 
自然语言 处理 最新 发展 状况 首先 需要 注明 的 一点 
的 是 严格 意义 上 来说 BERT 模型 更加 针对 
的 是 自然 语言 处理 NLP 中 的 自然 语言 
理解 NLU 分支 的 问题 而 自然 语言 处理 NLP 
是 一个 更加 宽泛 的 研究 领域 包含 更多 的 
算法 模型 和 应用 场景 Google BERT 都能 解决 哪些 
问题 此次 所谓 Google 的 BERT Bidirectional Encoder Representations from 
Transformers 模型 号称 打破 11项 纪录 那么 我们 就 顺藤摸瓜 
看看 能 引起 Google 注意 的 这 11项 挑战 由此 
找到 NLU 研究领域 大家 所 关注 的 重点 在 BERT 
Pre training of Deep Bidirectional Transformers for Language Understanding 原文 
地址 中 作者 将 BERT 模型 应用于 以下 实验 中 
GLUE General Language Understanding Evaluation 数据集 数据集 地址 如同 GLUE 
的 创建者 在 论文 论文 地址 中 介绍 到 的 
GLUE 实际上 是 为了 能够 标准化 的 和 综合性 的 
评估 NLU 自然语言 理解 算法 和 模型 而 构造 出来 
的 一套 包含 数据集 在线 评估 平台 的 工具 其中 
包含 了 以下 九个 数据集 单句 分析 类 CoLA Corpus 
of Linguistic Acceptability 链接 地址 从/p 23本/mq 语言学/n 发表/v 物中/i 
抽取/v 的/uj 10657/m 句话/i 并就/i 每句话/l 是否/v 从/p 语法/n 角度/n 
成立/v 进行/v 了/ul 标注/v 整个 数据集 提供 了 9594条 句子 
作为 训练 集 以及 1063条 句子 作为 测试 集 以下 
是 该 数据 集中 的 一些 样本 条目 SST 2 
Stanford Sentiment Treebank 链接 地址 摘取 了 电影 评论 并 
标注 了 是 正面 还是 负面 评论 以 用于 情感 
分析 相似 度 分析 和 转述 类 MRPC Microsoft Research 
Paraphrase Corpus 链接 地址 收录 了 5800对 句子 并 标注 
每对 句子 是否 在 语义上 等价 QQP Quora Question Pairs 
链接 地址 收集 了 Quora 网站 上 的 问题 和 
答案 并就 两个 问题 是否 在 语义上 等价 进行 了 
标注 STS B Semantic Textual Similarity Benchmark 链接 地址 收集 
了 来自 于 图片 注释 新闻 头条 社区 论坛 等 
不同 来源 的 8628对 句子 并就 每对 句子 的 相似 
度 给与 1 分到 5分 的 相似 度 评分 标注 
推断 类 MNLI Multi Genre Natural Language Inference Corpus 链接 
地址 包含 了 433k 对 句子 每对 句子 分别 包含 
premise 和 hypothesis 并 标注 了 两者 间 是 entail 
正向 关联 controdict 相互 矛盾 或是 neutral 中立 的 关系 
标签 QNLI Question answering NLI 是 基于 SQuAD Stanford Question 
Answering Dataset 链接 地址 构建 的 数据 集 此 数据 
集中 收集 了 问题 和 文字 并 标注 了 文字 
中 是否 包含 与 问题 匹配 的 答案 RTE Recognizing 
Textual Entailment 来源于 年度 的 RTE 竞赛 RTE 1 RTE 
2 RTE 3 RTE 5 链接 地址 数据集 收集 了 
句子 对 每对 句子 分为 premise 和 hypothesis 并 标注 
两者 间 是否 为 entailment 关系 WNLI Winograd NLI 是 
基于 WSC Winograd Schema Challenge 链接 地址 中 的 数据 
构建 的 数据集 原始数据 是 用来 训练 和 测试 阅读 
理解 模型 算法 的 每个 句子 中 包含 有 一个 
代词 算法 模型 需要 根据 上下文 获知 此 代词 指代 
的 事 句子 中 的 哪个 名词 GLUE 的 作者 
基于 原 数据集 将 数据 改造 成 句子 对 的 
形式 一是 保留 原始 句子 二 是 将 原始 句子 
中的 代词 用 其 具体 指代 的 名词 替换 并且 
针对 每对 句子 标注 出 两者 间 是否 为 entailment 
关系 SQuAD Standford Question Answering Dataset 链接 地址 为 斯坦福大学 
构建 的 阅读 理解 数据集 数据集 的 第一 个 版本 
SQuAD1 . 1 中 文章 地址 提供 了 100k 问题 
和 回答 配对 每个 问题 的 答案 包含 于 一段 
维基 百科 的 文字 为了 进一步 接近 现实 的 阅读 
理解 场景 2018年 发布 的 QUaAD 2.0 文章 地址 中 
额外 增加 了 50k 条 新增 的 问题 并且 问题 
不 一定 有 答案 这/r 要求/v 模型/n 和/c 算法/n 不但/c 
能/v 找出/v 答案/n 并且 在 没有 答案 的 时候 能 
给出 正确 的 判断 而非 凑 答案 而在 Google 公布 
其 BERT 算法 性能 的 论文 中 作者 采用 的 
仍然 是 SQuAD1 . 1 版本 的 数据 集 NER 
Named Entity Recognition 链接 地址 包含 200k 单词 并且 每个 
被 标注 为 Person Organization Location Miscellaneous 或 Other SWAG 
Situations With Adversarial Generations 链接 地址 包含 113k 完形填空 的 
句子 1 前馈 神经网络 前馈 神经 网络 也 称为 是 
深度 前馈 网络 或者 多层 感知机 它 是 最 基础 
的 深度 学习 模型 1.1 基本概念 前馈 神经 网络 的 
目标 是 在 函数 空间 中 寻找 相对 正确 的 
函数 函数 空间 是由 所选 的 architecture 决定 的 而 
函数 空间 中 的 具体 函数 是由 parameters 决定 的 
前馈 神经网络 之所以 称作 前馈 的 是因为 信息 从 输入 
x ⃗ \ vec { x } x 到 输出 
yyy 是 单向 流动 的 并 没有 从 输出 到 
模型 本身 的 反馈 连接 前馈 神经网络 通常 使用 许多 
不同 的 函数 复合 而成 这些 函数 如何 复合 则由 
一个 有向 无 环 图 来 描述 最 简单 的 
情况 有向 无 环 图 是 链式 结构 假设有 三个 
函数 f1 f2 f3f _ 1 f _ 2 f 
_ 3f1 f2 f3 组成 链式 复合 结构 则 f 
x ⃗ = f3 f2 f1 x ⃗ f \ 
vec { x } = f _ 3 f _ 
2 f _ 1 \ vec { x } f 
x = f3 f2 f1 x 其中 f1f _ 1f1 
被称作 网络 的 第一 层 f2f _ 2f2 为 网络 
第二层 f3f _ 3f3 称为 网络 第三层 链 的 全长 
称作 模型 的 深度 或者 神经 网络 的 层数 深度 
前馈 网络 的 最后 一层 也 称作 输出 层 给定 
训练样本 x ⃗ y \ vec { x } y 
x y 要求 输出 层 的 输出 f x ⃗ 
≈ yf \ vec { x } \ approx yf 
x ≈ y 但是 对于 其他 层 并 没有 任何 
要求 因为 无法 观测 到 除了 输出 层 以外 的 
那些 层 的 输出 因此 那些 层 被称作 隐藏 层 
hidden layer 学习 算法 必须 学习 如何 利用 隐 层 
来 配合 输出 层 来 产生 想要 的 结果 通常 
每个 隐 层 的 输出 都是/nr 一个 向量 而不是 标量 
这些 隐 层 的 输出 向量 的 维数 决定了 前馈 
神经 网络 的 宽度 也 可以 将 每 一层 想象 
成由/nr 许多 并行 的 单元 组成 每个 单元 表示 一个 
向 量到 标量 的 函数 每个 单元 的 输入 来自 
于前/nr 一层 的 许多 单元 单元 根据 自己 的 激活 
函数 来 计算 单元 的 输出 激活 函数 是 前馈 
神经网络 具有 非线性 表达 能力 的 核心 因素 因此 每个 
单元 类似 于 一个 神经元 1.2 特征 学习 线性 模型 
简单 高效 且 易于 求解 但是 它 有个 明显 的 
缺陷 模型 的 能力 被 局限 在 线性函数 中 因此 
它 无法 理解 任意 两个 输入 变量 间 的 非线性 
相互作用 解决 线性 模型 缺陷 的 方法 是 采用 核 
技巧 将 线性 模型 作用 在 ϕ x ⃗ \ 
phi \ vec { x } ϕ x 上 而 
不是 原始 输入 x ⃗ \ vec { x } 
x 上 其中 ϕ \ phi ϕ 是 一个 非 
线性变换 可以 认为 通过 ϕ \ phi ϕ 提供 了 
x ⃗ \ vec { x } x 的 一个 
新的 representation 有三种 策略 来 选择 这样 的 非线性 变换 
使用 一个 通用 的 ϕ \ phi ϕ 如 无限 
维 的 ϕ \ phi ϕ 采用 基于 RBF 核 
的 核 技巧 当 ϕ \ phi ϕ 具有 足够 
高的/nr 维数 则 总是 有 足够 的 能力 来 适应 
训练 集 但是 对于 测试 集 的 泛化 往往 不佳 
这 是因为 通用 的 ϕ \ phi ϕ 通常 只是 
基于 局部 平滑 的 原则 并 没有 利用 足够 多 
的 先验 知识 来 解决 高级 问题 手动 设计 ϕ 
\ phi ϕ 这种 方法 对 于 专门 的 任务 
往往 需要 数十年 的 努力 如 语音识别 任务 通过 模型 
自动 学习 ϕ \ phi ϕ 这是 深度 学习 采用 
的 策略 以 单层 隐 层 的 深度 前馈 网络 
为例 y = f x ⃗ θ ⃗ w ⃗ 
= ϕ x ⃗ θ ⃗ Tw ⃗ y = 
f \ vec { x } \ vec { \ 
theta } \ vec { w } = \ phi 
\ vec { x } \ vec { \ theta 
} ^ T \ vec { w } y = 
f x θ w = ϕ x θ Tw 此时 
有 两个 参数 参数 θ ⃗ \ vec { \ 
theta } θ 从 一族 函数 中 学习 ϕ \ 
phi ϕ 其中 ϕ \ phi ϕ 定义 了 一个 
隐 层 参数 w ⃗ \ vec { w } 
w 将 ϕ x ⃗ \ phi \ vec { 
x } ϕ x 映射 到 所需 输出 深度 学习 
中 将 representation 参数 化为 ϕ x ⃗ θ ⃗ 
\ phi \ vec { x } \ vec { 
\ theta } ϕ x θ 并 使用 优化 算法 
来 寻找 θ ⃗ \ vec { \ theta } 
θ 从而 得到 一个 很好 的 representation 如果 使用 一个 
非常 宽泛 的 函数 族 ϕ x ⃗ θ ⃗ 
\ phi \ vec { x } \ vec { 
\ theta } ϕ x θ 则能 获得 第 一种 
方案 的 好处 适应 能力 强 如果 将 先验 知识 
编码 到 函数 族 ϕ x ⃗ θ ⃗ \ 
phi \ vec { x } \ vec { \ 
theta } ϕ x θ 中 则能 获得 第二 种 
方案 的 好处 有 人工 先验 知识 因此 深度 学习 
的 方案 中 只 需要 寻找 合适 的 宽泛 的 
函数 族 而 不是 某一个 映射函数 通过 特征 学习 来 
改善 模型 不仅仅 适用于 前馈 神经网络 也 适用 于 几乎 
所有 的 深度 学习 模型 1.3 训练 训练 一个 深度 
前馈 网络 和 训练 一个 线性 模型 的 选项 相同 
选择 优化 算法 代价 函数 输出 单元 的 形式 除此之外 
还 需要 给出 下列 条件 由于 深度 前馈 网络 引入 
了 隐 层 的 概念 因此 需要 选择 适用 于隐层/nr 
的 激活 函数 激活 函数 接受 隐 层 的 输入 
值 给 出了 隐 层 的 输出 值 深度 前馈 
网络 的 网络 结构 也 需要 给出 其中 包括 有 
多少 层 网络 每层 网络 有 多少 个 单元 层级 
网络 之间 如何 连接 深度 神经网络 训练 时 需要 计算 
复杂 函数 的 梯度 通常 这 采用 反向 传播 算法 
back propagation 和它的/nr 现代 推广 来 完成 2 . 使用 
pytorch 定义 简单 神经网络 假设 输入 样 本为 64个 输入 
层 维度 为 1000 只 包括 一层 隐藏 层 隐藏 
层 维度 为 100 输出 层 维度 为 10个 使用 
链式法则 求导 的 代码 如下 所示 num _ samples = 
64 # N dim _ in dim _ hid dim 
_ out = 1000 100 10 # IN H OUT 
x = torch . randn num _ samples dim _ 
in # N * IN y = torch . randn 
num _ samples dim _ out # N * OUT 
w1 = torch . randn dim _ in dim _ 
hid # IN * H w2 = torch . randn 
dim _ hid dim _ out # H * OUT 
eta = 1e 6 for i in range 1000 # 
Forward pass h = x @ w1 # N * 
H h _ relu = h . clamp min = 
0 # N * H y _ pred = h 
_ relu @ w2 # N * OUT # Loss 
loss = y _ pred y . pow 2 . 
sum . item print times is { } loss is 
{ } . format i loss # Backward pass grad 
_ y _ pred = 2.0 * y _ pred 
y # N * OUT grad _ w2 = h 
_ relu . t @ grad _ y _ pred 
# H * OUT = H * N * N 
* OUT 其中 H * N = N * H 
. T grad _ h _ relu = grad _ 
y _ pred @ w2 . t # N * 
H = N * OUT * OUT * H 其中 
OUT * H = H * OUT . T grad 
_ h = grad _ h _ relu . clone 
grad _ h h 0 = 0 grad _ w1 
= x . t @ grad _ h # IN 
* H = IN * N * N * H 
w1 = w1 eta * grad _ w1 w2 = 
w2 eta * grad _ w2 使用 pytorch 自动 求导 
代码 如下 所示 num _ samples = 64 # N 
dim _ in dim _ hid dim _ out = 
1000 100 10 # IN H OUT x = torch 
. randn num _ samples dim _ in # N 
* IN y = torch . randn num _ samples 
dim _ out # N * OUT w1 = torch 
. randn dim _ in dim _ hid requires _ 
grad = True # IN * H w2 = torch 
. randn dim _ hid dim _ out requires _ 
grad = True # H * OUT eta = 1e 
6 for i in range 1000 # Forward pass h 
= x @ w1 # N * H h _ 
relu = h . clamp min = 0 # N 
* H y _ pred = h _ relu @ 
w2 # N * OUT # Loss loss = y 
_ pred y . pow 2 . sum print times 
is { } loss is { } . format i 
loss . item loss . backward # Backward pass with 
torch . no _ grad w1 = eta * w1 
. grad # 如果 写成 w1 = w1 eta * 
w1 . grad 就会 报错 w2 = eta * w2 
. grad w1 . grad . zero _ w2 . 
grad . zero _ 3 . 激活 函数 激活 函数 
的 设计 是 一个 非常 活跃 的 研究 领域 并且 
目前 还 没有 明确 的 指导性 理论 难以 决定 何时 
采用 何种 类型 的 激活 函数 是 最佳 方案 通常 
不能 预先判断 哪种 类型 的 激活 函数 工作 的 最好 
所以 设计 过程 中 需要 反复 试错 通过 测试 集 
评估 其 性能 来 选择 合适 的 激活 函数 一般 
默认 采用 的 激活 函数 是 修正 线性 单元 relu 
但是 仍然 有 许多 其他 类型 的 激活 函数 某些 
激活 函数 可能 并 不是 在 所有 的 输入 上 
都是 可微 的 如 修正 线性 单元 g z = 
max ⁡ { 0 z } g z = \ 
max { \ { 0 z \ } } g 
z = max { 0 z } 在 z = 
0z = 0z = 0处 不 可微 这 使得 在 
该点 处 梯度 失效 事实上 梯度 下 降法 对 这些 
隐 单元 的 表现 仍然 足够 好 原因 是 神经 
网络 的 训练 算法 通常 并 不会 达到 代价 函数 
的 局部 最小值 而 仅仅 是 显著 地 降低 代价 
函数 的 值 即可 因此 实际 上 训练 过程 中 
一般 无法 到达 代价 函数 梯度 为零 的 点 所以 
代价 函数 取 最小值 时 梯度 未定义 是 可以 接受 
的 不 可微 的 点 通常 只是 在 有限 的 
少数 的 点上 不 可微 在 这些 不 可微 的 
点 通常 左 导数 右导数 都 存在 神经网络 训练 的 
软件 实现 通常 返回 左 导数 或者 右导数 其中 的 
一个 而 不是 报告 导数 未定义 或者 产生 一个 错误 
这 是因为 计算机 计算 0 点 的 导数 时 由于 
数值 误差 的 影响 实际上 不 可能 计算 到 理论 
上 0 点 的 导数 而是 一个 微小 的 偏离 
向/p 左侧/f 偏离/v 就是/d 左/m 导数/n 向 右侧 偏离 就是 
右导数 大多数 激活 函数 的 工作 过程 都 可以 描述 
为 下面 三步 接受 输入 向量 x ⃗ \ vec 
{ x } x 计算 仿射变换 z = w ⃗ 
Tx ⃗ + bz = \ vec { w } 
^ { T } \ vec { x } + 
bz = wTx + b 激活 函数 也 称作 是 
隐 单元 3.1 线性 激活 函数 可以 使用 单位 函数 
g z = zg z = zg z = z 
作为 激活 函数 但 如果 网络 的 每 一层 都 
是由 线性变换 组成 则 网络 作为 整体 也是 线性 的 
这会 降低 网络 的 表达 能力 因此 线性 激活 函数 
较少 使用 3.2 修正 线性 单元 relu 修正 线性 单元 
采用 激活 函数 g z = max { 0 z 
} g z = max \ { 0 z \ 
} g z = max { 0 z } 它 
和 线性 单元 非常 类似 区别 在于 修正 线性 单元 
在 左侧 的 定义域 上 输出 为零 优点 采用 基于 
梯度 的 优化 算法 时 非常 易于 优化 当 修正 
线性 单元 处于 激活 状态 时 导数 为 常数 1 
当 修正 线性 单元 处于 非 激活 状态 时 导数 
为 常数 0 修正 线性 单元 的 二阶 导数 几乎 
处处 为零 缺点 无法 通过 基于 梯度 的 方法 学习 
那些 使得 修正 线性 单元 处于 非 激活 状态 的 
参数 因为 此时 梯度 为零 对于 修正 线性 单元 h 
⃗ = g WTx ⃗ + b ⃗ \ vec 
{ h } = g W ^ { T } 
\ vec { x } + \ vec { b 
} h = g WTx + b 初始化/l 时/n 可以/c 
将/d b/w ⃗/i \/i vec/w {/i b/w }/i b/w 的/uj 
所有/b 元素/n 设置/vn 成/n 一个/m 小/a 的/uj 正值/n 如 0.1 
从而 使得 修正 线性 单元 在 初始 时 尽可能 的 
对 训练 集中 大多数 输入 呈现 激活状态 有 许多 修正 
线性 单元 的 扩展 存在 这些/r 扩展/v 保证/v 了/ul 它们/r 
能在/nr 各个/r 位置/v 都/d 保持/v 非零/i 的/uj 梯度/n 大多数 扩展 
的 表现 与 修正 线性 单元 相差无几 偶尔 表现 的 
更好 3.3 sigmoid / tanh 在 引入 修正 线性 单元 
之前 大多数 神经 网络 使用 sigmoid 函数 g z = 
σ z g z = \ sigma z g z 
= σ z 或者 双曲 正切函数 g z = tanh 
z g z = tanh z g z = tanh 
z 作为 激活 函数 这 两个 激活 函数 密切相关 因为 
tanh z = 2 σ 2z − 1tanh z = 
2 \ sigma 2z 1tanh z = 2 σ 2z 
− 1 与 修正 线性 单元 不同 sigmoid/w 单/n 元和/n 
tanh/w 单元/n 在其/i 大部分/m 定义/n 域内/i 都/d 饱和/a 仅仅 当 
zzz 在 0 附近 才 有 一个 较高 的 梯度 
这会 使得 基于 梯度 的 学习 变得 非常 困难 因此 
现在 不 鼓励 将 这两种 单元 用作 前馈 神经 网络 
中 的 激活 函数 如果 选择 了 一个 合适 的 
代价 函数 如 对数 似 然 函数 来 抵消 了 
sigmoid 的 饱和 性 则 这两种 单元 可以 用作 输出 
单元 而 不是 隐 单元 如果 必须 选用 sigmoid 激活 
函数 时 tanh 激活 函数 通常 表现 更佳 因为 tanh 
函数 在 0点 附近 近似于 单位 函数 g z = 
zg z = zg z = z sigmoid 激活 函数 
在 前馈 神经网络 之外 的 神经 网络 中 更为 常见 
有 一些 网络 不能 使用 修正 线性 单元 因此 sigmoid 
激活 函数 是个 更好 的 选择 尽管 它 存在 饱和 
问题 循环 神经网络 修正 线性 单元 会 产生 信息 爆炸 
的 问题 一些 概率模型 要求 输出 在 0 ~ 1 
之间 3.3 激活 函数 对比 sigmoid 主要 缺点 容易 饱和 
从而 使得 梯度 消失 当 激活 函数 取值 在 接近 
0 或者 1 时会 饱和 此时 梯度 为 近 似为 
0 函数 输出 不是 零 中心 的 这 会 导致 
后续 神经元 的 输出 数值 总是 正数 tanh 优点 函数 
输出 是 零 中心 的 缺点 容易 饱和 从而 使得 
梯度 消失 tanh 激活 函数 几乎 在 所有 场合 都是 
优于 sigmoid 激活 函数 的 但是 有 一种 情况 例外 
如果 要 求函数 输出 是 0 ~ 1 之间 比如 
表征 某个 概率 则 二者 之间 必须 用 sigmoid relu 
优点 对 随机 梯度 下降 的 收敛 有 巨大 的 
加速 作用 而且 非常 容易 计算 缺点 可能 导致 神经元 
死掉 当 一个 很大 的 梯度 流过 relu 神经元 时 
可能 导致 梯度 更新 到 一种 特别 的 状态 在 
这种 状态 下 神经元 无法 被 其他 任何 数据 点 
再次 激活 此后 流过 这个 神经元 的 梯度 将 变成 
0 该 单元 在 训练 过程 中 不可逆的 死亡 如果 
学习率 设置 的 过高 可能 会 发现 网络 中 大量 
神经元 都会 死掉 整个 训练 过程 中 这些 神经元 都 
不会 被 激活 leaky relu 为了 解决 relu 死亡 神经元 
的 问题 的 尝试 但是 效果 并不 明显 4 . 
正则化 正则化 常 用于 缓解 模型 过拟合 过拟合 发生 的 
原因 是 模型 的 容量 过大 而 正则化 可以 对模型 
施加 某些 限制 从而 降低 模型 的 有效 容量 目前 
有 多种 正则化 策略 有些 正则化 策略 是 向 模型 
添加 额外 的 约束 如 增加 对 参数 的 限制 
这是 对 参数 的 硬 约束 有些 正则化 策略 是 
向 目标函数 增加额 外项 这是 对 参数 的 软 约束 
正则化 策略 代表 了 某种 先验 知识 即 倾向 于 
选择 简单 的 模型 在 深度 学习 中 大多数 正则化 
策略 都是 基于 对 参数 进行 正则化 正则化 以 偏差 
的 增加 来 换取 方差 的 减少 而 一个 有效 
的 正则化 能 显著 降低 方差 并且 不 会 过度 
增加 偏差 在 深度 学习 的 实际 应用 中 不要 
因为 害怕 过拟合 而 采用 一个 小 模型 推荐 采用 
一个 大模型 并 使用 正则化 4.1 参数 范数 正则化 一些 
正则化 方法 通过 对 目标函数 JJJ 添加 一个 参数 范数 
正则化 项Ω/nr θ ⃗ \ Omega \ vec { \ 
theta } Ω θ 来 限制 模型 的 容量 capacity 
正则化 之后 的 目标 函数 为 J ⃗ θ ⃗ 
X y ⃗ = J ⃗ θ ⃗ X y 
⃗ + α Ω θ ⃗ \ vec { J 
} \ vec { \ theta } X \ vec 
{ y } = \ vec { J } \ 
vec { \ theta } X \ vec { y 
} + \ alpha \ Omega \ vec { \ 
theta } J θ X y = J θ X 
y + α Ω θ α \ alpha α 为 
正则化 项的/nr 系数 它 衡量 正则化 项Ω/nr θ ⃗ \ 
Omega \ vec { \ theta } Ω θ 和 
标准 目标函数 J ⃗ \ vec { J } J 
的 比重 α = 0 \ alpha = 0 α 
= 0 则 没有 正则化 α/i \/i alpha/w α/i 越大/i 
则/d 正则化/i 项越/nr 重要/a 如果 最小化 J ⃗ \ vec 
{ J } J 则会 同时 降低 J 和 参数 
θ ⃗ \ vec { \ theta } θ 的 
规模 参数 范数 正则化 可以 缓解 过拟合 如果 α \ 
alpha α 设置 的 足够 大 则 参数 θ ⃗ 
\ vec { \ theta } θ 就越 接近 零 
这 意味着 模型 变得 更 简单 简单 的 模型 不容易 
过拟合 但是 可能 欠 拟合 对于 神经网络 这 意味着 很多 
隐 单元 的 权重 接近 0 于是 这些 隐 单元 
在 网络 中 不起 任何 作用 此时 大 的 神经 
网络 会 变成 一个 小 的 网络 在 α \ 
alpha α 从零/nr 逐渐 增加 的 过程 中 存在 一个 
中间值 使得 参数 θ ⃗ \ vec { \ theta 
} θ 的 大小 合适 即 一个 合适 的 模型 
选择 不同 的 Ω \ Omega Ω 的 形式 会 
产生 不同 的 解 常见 的 形式 有 L2L _ 
2L2 正则化 和 L1L _ 1L1 正则化 4 . 1.1 
L2 正则化 L2L _ 2L2 正则化 通常 被 称作 岭回归 
或者 Tikhonov 正则化 正则化 项为Ω/nr θ ⃗ = 12 ∣ 
∣ θ ∣ ∣ 2 \ Omega \ vec { 
\ theta } = \ frac { 1 } { 
2 } | | \ theta | | ^ { 
2 } Ω θ = 21 ∣ ∣ θ ∣ 
∣ 2 系数 12 \ frac { 1 } { 
2 } 21 是 为了 使得 导数 的 系数 为 
1 该 正则化 形式 倾向于 使得 参数 θ ⃗ \ 
vec { \ theta } θ 更 接近 零 假设 
θ ⃗ \ vec { \ theta } θ 参数 
就是 权重 w ⃗ \ vec { w } w 
没有 偏置 参数 则 J ⃗ θ ⃗ X y 
⃗ = J ⃗ θ ⃗ X y ⃗ + 
α 2w ⃗ Tw ⃗ \ vec { J } 
\ vec { \ theta } X \ vec { 
y } = \ vec { J } \ vec 
{ \ theta } X \ vec { y } 
+ \ frac { \ alpha } { 2 } 
\ vec { w } ^ { T } \ 
vec { w } J θ X y = J 
θ X y + 2 α wTw 对应 的 梯度 
为 ∇ w ⃗ J ˇ w ⃗ X y 
⃗ = ∇ w ⃗ J ˇ w ⃗ X 
y ⃗ + α w ⃗ \ nabla _ { 
\ vec { w } } \ check { J 
} \ vec { w } X \ vec { 
y } = \ nabla _ { \ vec { 
w } } \ check { J } \ vec 
{ w } X \ vec { y } + 
\ alpha \ vec { w } ∇ wJ ˇ 
w X y = ∇ wJ ˇ w X y 
+ α w 正则化 对于 梯度 更新 的 影响 是 
每一步 执行 梯度 更新 之前 会对 权重 向量 乘以 一个 
常数 因子 来 收缩 权重 向量 因此 L2 正则化 也 
被称作 权重 衰减 L2L _ 2L2 正则化 表明 只有 显著 
减小 目标函数 JJJ 的 那个 方向 的 参数 会 相对 
保留 下来 无助于 减小 目标函数 JJJ 的 方向 该 方向 
上 HHH 特征值 较小 或者说 该 方向 上 JJJ 的 
曲率 较小 或者说 该 方向 上 JJJ 的 曲线 更 
接近 于 直线 因为 在 这个 方向 上 移动 不会 
显著 改变 梯度 因此 这个 不 重要 方向 上 的 
分量 会 因为 正则化 的 引入 而被 衰 减掉 4 
. 1.2 L1 正则化 模型 参数 w ⃗ \ vec 
{ w } w 的 L1L _ 1L1 的 正则化 
形式 为 Ω θ ⃗ = ∣ ∣ w ⃗ 
∣ 1 = ∑ i ∣ wi ∣ \ Omega 
\ vec { \ theta } = | | \ 
vec { w } | _ 1 = \ sum 
\ limits _ { i } | w _ i 
| Ω θ = ∣ ∣ w ∣ 1 = 
i ∑ ∣ wi ∣ 即 各个 参数 的 绝对值 
之和 L1L _ 1L1 正则化 后的/nr 目标 函数 J ⃗ 
θ ⃗ X y ⃗ = J ⃗ θ ⃗ 
X y ⃗ + α ∣ ∣ ∣ w ∣ 
∣ 1 \ vec { J } \ vec { 
\ theta } X \ vec { y } = 
\ vec { J } \ vec { \ theta 
} X \ vec { y } + \ alpha 
| | | w | | _ { 1 } 
J θ X y = J θ X y + 
α ∣ ∣ ∣ w ∣ ∣ 1 对应 的 
梯度 为 ∇ w ⃗ J ˇ w ⃗ X 
y ⃗ = ∇ w ⃗ J ˇ w ⃗ 
X y ⃗ + α sign w ⃗ \ nabla 
_ { \ vec { w } } \ check 
{ J } \ vec { w } X \ 
vec { y } = \ nabla _ { \ 
vec { w } } \ check { J } 
\ vec { w } X \ vec { y 
} + \ alpha sign \ vec { w } 
∇ wJ ˇ w X y = ∇ wJ ˇ 
w X y + α sign w 其中 sign ⋅ 
sign \ cdot sign ⋅ 函数 取自 变量 的 符号 
如果 自变量 大于 零 则 取值 为 1 如果 自变量 
小于 零 则 取值 为 1 如果 自变量 为零 则 
取值 为零 L1L _ 1L1 正则化 对于 梯度 更新 的 
影响 是 不再 是 线 性地 缩放 每个 wiw _ 
iwi L2L _ 2L2 正则化 项的/nr 效果 而是 减去 与 
sign wi sign w _ i sign wi 同号 的 
常数 因子 L1L _ 1L1 正则化 项 更容易 产生 稀疏 
sparse 解 而 正则化 并 不会 导致 稀 疏解 在 
L1L _ 1L1 正则化 中 wi ∗ w _ i 
^ * wi ∗ 的 绝对值 越小 该 维 的 
特征 越 容易 被 稀疏 化 L1L _ 1L1 正则化 
的 这一 性质 已经 被 广泛 地 用作 特征选择 L1L 
_ 1L1 正则化 使得 部分 特征 子集 的 权重 为零 
表明 相应 的 特征 可以 被 安全 地 忽略 4.2 
数据集 增强 提高 模型 泛化 能力 的 一个 最 直接 
的 方法 是 采用 更多 的 数据 来 训练 但是 
通常 在 现实 任务 中 我们 拥有 的 数据 量 
有限 解决 该 问题 的 一种 方法 是 创建 一些 
虚拟 的 数据 用于 训练 数据集 增强 仅仅 用于 模型 
的 训练 而 不是 用于 模型 的 预测 即 不能 
对 测试 集 验证 集 执行 数据集 增强 当 比较 
机器学习 算法 基准测试 的 结果 时 必须 考虑 是否 采用 
了 数据集 增强 通常 情况下 人工 设计 的 数据 集 
增强 方案 可以 大大 减少 模型 的 泛化 误差 当 
两个 模型 的 泛化 性能 比较 时 应该 确保 这 
两个 模型 使用 同 一套 人工 设计 的 数据 集 
增强 方案 注意 数据集 增强 和 预处理 的 区别 数据集 
增强 会 产生 更多 的 输入 数据 而 数据 预处理 
产生 的 输入 数据 数量 不变 4 . 2.1 线性变换 
对于 某些 任务 来说 创建 虚拟 数据 非常 困难 如 
在 密度估计 任务 中 除非 预先 知道了 密度 函数 否则 
无法 产生 新的 虚拟 数据 对于 分类 问题 来说 创建 
虚拟 数据 非常 简单 对于 一个 分类器 它 将 高维 
的 输入 x ⃗ \ vec { x } x 
映 射到 类别 yyy 这 意味着 这种 映射 规则 是 
不随 坐标系 的 改变 而 改变 的 因此 可以 通过 
线性变换 将 训练 集中 的 x ⃗ y \ vec 
{ x } y x y 变换 为 x ⃗ 
′ y \ vec { x } ^ { \ 
prime } y x ′ y 从而 产生 了 新的 
数据 x ⃗ ′ y \ vec { x } 
^ { \ prime } y x ′ y 对 
图像 分类 问题 数据集 增强 特别 有效 数据集 增强 也 
可以 应用于 语音识别 任务 常见 的 图片 数据集 增强 方法 
将 训练 图像 沿着 每个 方向 平移 几个 像素 产生 
新的 图像 对 训练 图像 进行 旋转 翻转 或者 缩放 
对 训练 图像 进行 随机 裁剪 实际上 随机 裁剪 图像 
的 操作 也 可以 被 认为 是 预处理 步骤 而 
不是 数据集 增强 对 训练 图像 进行 颜色 抖动 调整 
饱和度 调整 亮度 调整 对比度 调整 锐 度 对比度 图像 
画面 的 明暗 反差 程度 对比度 越高 则 图片 亮 
的 地方 更 亮 暗 的 地方 越暗/nr 亮度 图像 
的 明暗 程度 亮度 越高 则 图像 整体 越亮/nr 饱和度 
图像 颜色 种类 的 多少 饱和度 越高 则 图像 的 
颜色 种类 越多 图像 越 鲜艳 锐 度 图像 的 
边缘 轮廓 的 锐利 程度 锐 度 越高 则 图像 
的 边缘 越 清晰 在 使用 线性变换 执行 数据集 增强 
时 需要 注意 某些 线性变换 会 改变 正确 的 类别 
如 字符识别 任务 中 b / d 以及 6/9 的 
图像 不能 执行 水平 翻转 变换 和 旋转 180 度 
变换 某些 线性变换 难以 执行 如 平面 外 的 绕 
轴旋转 类似于 翻页 难以 通过 简单 的 几何 运算 在 
输入 图片 上 实现 4.3 噪声 添加 有 两种 添加 
噪声 的 策略 输入 噪声 注入 权重 噪声 注 输入 
噪声 注入 是 将 噪声 作用于 输入 的 数据 集 
这也 是 一种 数据集 增强 方法 对于 某些 模型 在 
输入 上 注入 方差 极小 的 噪音 等价 于对/nr 权重 
施加 参数 范数 正则化 Bishop 1995a b 但是 输入 噪声 
注入 远比 简单 地 收缩 参数 强大 尤其 是 噪声 
被 添加 到 隐 单元 的 输入 上 时 权重 
噪声 注入 是 将 噪音 作用于 权重 这项 技术 主要 
用于 循环 神经网络 权重 噪声 注入 可以 解释 为 将 
权重 视作 不 确定 的 随机变量 拥有 某个 概率分布 向 
权重 注入 噪声 是 对 该 随机变量 采样 得到 的 
一个 随机 值 4.4 早 停当 训练 一个 容量 较大 
的 模型 时会 经常 发现 训练 误差 逐渐 降低 但是 
验证 误差 先 下降 后 上升 当 验证 误差 没有 
进一步 改善 时 算法 就 提前 终止 这种 策略 被称作 
早 停 early stopping 早 停 是 深度 学习 中 
最 常用 的 正则化 形式 因为 它 简单 有效 当 
训练 终止 时 返回 的 不是 最新 的 模型 参数 
而是 验证 误差 最小 的 模型 参数 因此 需要 频繁 
存储 模型 参数 4 . 4.1 早 停 算法 早 
停 算法 输入 当前 验证 集 的 误差 非 最小值 
的 次数 验证 集 验证 的 间隔 初始 参数 输出 
最佳 参数 获得最佳 参数 时 迭代 的 步数 算法 步骤 
先 进行 初始化 然后 迭代 直至 满足条件 停止 可以 认为 
早 停 是 一个 非常 高效 的 超 参数 选择 
算法 训练 步数 是 一个 超 参数 该 超 参数 
在 验证 误差 上 具有 U 形 曲线 早 停 
策略 通过 控制 训练 步数 来 控制 模型 的 有效 
容量 capacity 早 停 策略 只 需要 跑 一轮 训练 
就 能够 得到 很多 的 超 参数 即 训练 步数 
及其 对应 的 验证 误差 早 停 是 正则化 的 
一种 非常 不起眼 的 形式 其 优点 有 它 几乎 
不 需要 干涉 基本 的 训练 过程 适合 任何 模型 
可以 单独 使用 或者 与 其他 的 正则化 策略 相结合 
早 停 不仅 有 正则化 的 好处 还有 降低 计算 
成本 的 好处 4.5 dropoutdropout 在前 向 传播 过程 中 
对 网络 中 的 每个 隐 层 每个 隐 单元 
都以 一定 的 概率 pdropp _ { drop } pdrop 
被 删除 最后 得到 一个 规模 更小 的 网络 在 
反向 传播 过程 中 仅仅 针对 该 小 网络 进行 
权重 更新 所谓 的 删除 即 指定 该 该隐 单元 
的 输出 都为 0 一旦 隐 单元 的 权重 为 
0 则 该隐 单元 对 后续 神经元 的 影响 均为 
0 输入 层 和 输出 层 的 神经元 不会 被 
删除 因为 这 两个 层 的 神经元 的 数量 是 
固定 的 理论 上 可以 对 输入 层 应用 dropout 
使得 可以 有 机会 删除 一个 或者 多 个 输入 
特征 但 实际 工程 中 通常 不会 这么 做 隐 
单元 删除 发生 在 一个 训练样本 的 训练 期间 不同 
的 训练样本 其 删除 的 隐 单元 的 集合 是 
不同 的 因此 裁剪 得到 的 小 网络 是 不同 
的 不同 的 训练样本 隐 单元 被 删除 的 概率 
都是 相同 的 在 不同 batch 之间 的 同一个 训练样本 
其 删除 的 隐 单元 的 集合 也是 不同 的 
在 不同 的 梯度 更新 周期 会 从 完整 的 
网络 中 随机 删除 不同 的 神经元 因此 裁剪 得到 
的 小 网络 是 不同 的 但是 在 这个 过程 
中 隐 单元 被 删除 的 概率 是 相同 的 
可以 指定 某一个 隐 层 或者 某 几个 隐 层 
执行 dropout 而 没有 必要 针对 所有 的 隐 层 
执行 dropout 可以 对 网络 的 每个 隐 单元 指定 
不同 的 删除 概率 但 实际 工程 中 通常 不会 
这么 做 定义 一个 掩码 向量 μ ⃗ \ vec 
{ \ mu } μ 它 给出 了 哪些 隐 
单元 被 保留 哪些 隐 单元 被 删除 掩码 为 
0 的 位置 对应 的 隐 单元 被 删除 掩码 
为 1 的 位置 对应 的 隐 单元 被 保留 
定义 J θ μ ⃗ ⃗ J \ vec { 
\ theta \ vec { \ mu } } J 
θ μ 为 参数 θ ⃗ \ vec { \ 
theta } θ 和 掩码 u ⃗ \ vec { 
u } u 共同 定义 的 模型 代价 dropout 的 
目标 是 最小化 E μ ⃗ J θ ⃗ μ 
⃗ E _ { \ vec { \ mu } 
} J \ vec { \ theta } \ vec 
{ \ mu } E μ J θ μ 这里 
采用 期望 因为 掩码 向量 u ⃗ \ vec { 
u } u 是 一个 随机 向量 对于 每个 训练样本 
u ⃗ \ vec { u } u 都 可能 
不同 因为 掩码 向量 具有 指数 多个 因此 期望 包含 
了 指数 多项 实际 应用 中 可以 通过 抽样 u 
⃗ \ vec { u } u 来 获得 期望 
的 无偏估计 5 . 深度 模型 的 优化 5.1 参数 
初始化 策略 有些 优化 算法 是非 迭代 的 可以 直接 
解析 求解 最优 解 有些 优化 算法 是 迭代 的 
但是 它们 是 初始值 无关 的 深度 学习 不 具有 
这 两类 性质 通常 是 迭代 的 且 与 初始值 
相关 深度 学习 中 大多数 算法 都 受到 初始值 的 
影响 初始值 能够 决定 算法 最终 是否 收敛 以及/c 收敛/v 
时的/nr 收敛/v 速度/n 有/v 多快/i 以及 收敛 到 一个 代价 
函数 较高 还是 较低 的 值 深度 学习 中 初始值 
也 会 影响 泛化 误差 而 不仅仅 是 目标 函数 
的 最优化 因为 如果 选择 一个 不好 的 初始值 则 
最 优化 的 结果 会 落在 参数 空间 的 一个 
较差 的 区域 此时 会 导致 模型 一个 较差 的 
泛化 能力 目前 深度 学习 中 选择 初始化 策略 是 
启发式 的 大多数 初始化 策略 使得 神经网络 初始 化时 实现 
一些 良好 的 性质 但是 这些 性质 能否 在 学习 
过程 中 保持 难以 保证 有些 初始化 点 从最/nr 优化 
的 观点 是 有利 的 但是 从 泛化 误差 的 
观点 来看 是 不利 的 设定 一个 好 的 初始化 
策略 是 困难 的 因为 神经 网络 最 优化 任务 
至今 都 未被 很好 理解 对于 初 始点 如何 影响 
泛化 误差 的 理论 是 空白 的 几乎 没有 任何 
指导 通常 的 参数 初始化 策 略为 随机 初始化 权重 
偏置 通过 启发式 挑选 常数 额外 的 参数 也 通过 
启发式 挑选 常数 也 可以 使用 机器学习 来 初始化 模型 
的 参数 在 同样 的 数据 集上 即使 是 用 
监督 学习 来 训练 一个 不 相关 的 任务 有时 
也 能够 得到 一个 比 随机 初始化 更好 的 初始值 
原因 是 监督 学习 编码 了 模型 初始 参数 分布 
的 某些 信息 5 . 1.1 权重 初始化 通常 权重 
的 初始化 是从 高斯分布 或者 均匀分布 中 挑选 出来 的 
值 从 高斯分布 还是 均匀分布 中 挑选 看起来 似乎 没有 
很大 差别 实际上 也 没有 被 仔细 研究 该 分布 
的 范围 如 均匀分布 的 上 下限 对/p 优化/vn 结果/n 
和/c 泛化/v 能力/n 有/v 很大/a 的/uj 影响/vn 初始 权重 的 
大小 很重要 下面 的 因素 决定 了 权重 的 初始值 
的 大小 更大 的 初始 权重 具有 更强 的 破坏 
对称性 的 作用 有助于 避免 冗余 的 单元 更大 的 
初始 权重 也 有助于 避免 梯度 消失 更大 的 初始 
权重 也 容易 产生 梯度 爆炸 循环 神经 网络 中 
更大 的 初始 权重 可能 导致 混沌 现象 对于 输入 
中的 很小 的 扰动 非常 敏感 从而 导致 确定性 算法 
给 出了 随机性 结果 关于 如何 初始化 网络 正则化/i 和/c 
最优化/v 有/v 两种/m 不同/a 的/uj 角度/n 从 最优化 角度 建议 
权重 应该 足够 大 从而 能够 成功 传播 信息 从 
正则化 角度 建议 权重 小 一点 如 正则化 从而 提高 
泛化 能力 实践 中 通常 需要 将 初始 权重 范围 
视作 超 参数 如果 计算资源 允许 可以 将 每层 权重 
的 初始 数值 范围 设置 为 一个 超 参数 然后 
使用 超 参数 搜索算法 来 挑选 这些 超 参数 5 
. 1.2 偏置 初始化 偏置 的 初始化 通常 更 容易 
大多数 情况 下 可以 设置 偏置 初始化 为零 有时 可以 
设置 偏置 初始 化为 非零 这 发生 在 下面 的 
三种 情况 如果 偏置 是 作为 输出 单元 则 初始化 
偏置 为非 零值 假设 初始 权重 足够 小 输出 单元 
的 输出 仅 由 初始化 偏置 决定 则 非零 的 
偏置 有助于 获取 正确 的 输出 边缘 统计 有时 选择 
偏置 的 初始值 以免 初始化 引起 激活 函数 饱和 如 
ReLU 激活 函数 的 神经元 的 偏置 设置 为 一个 
小 的 正数 从而 避免 ReLU 初始 时就/nr 位于 饱和 
的 区域 有时 某个 单元 作为 开关 来 决定 其他 
单元 是 使用 还是 不 使用 此时 偏置 应该 非零 
从而 打开 开关 6 . 优化 算法 6.1 动 量法 
Momentum 该 适用 于 隧道 型 曲面 梯度 下 降法 
在 狭长 的 隧道 型函数 上 表现 不佳 如下/t 图/n 
所示/v 函数/n 主体/n 缓缓/d 向/p 右方/f 下降/v 在/p 主体/n 方向/n 
两侧/f 各/r 有/v 一面/m 高墙/n 导致/v 垂直于/v 主体/n 方向/n 有/v 
更大/i 的/uj 梯度/n 梯度/n 下降/v 法会/n 在/p 隧道/n 两侧/f 频繁/a 
震荡/v 而动/i 量法/i 每次/r 更新/d 都/d 吸收/v 一/m 部分/n 上次/t 
更新/d 的/uj 余势/nr 这样 主体 方向 的 更新 就 得到 
了 更大 的 保留 从而 效果 被 不断 放大 物理上 
这就 像是 推 一个 很重 的 铁球 下山 因为 铁球 
保持 了 下山 主体 方向 的 动量 所以 在 隧道 
上 沿 两侧 震荡 测 次数 就会 越来越少 vt = 
γ vt − 1 + η ∇ θ J θ 
v _ { t } = \ gamma v _ 
{ t 1 } + \ eta \ nabla _ 
{ \ theta } J \ theta vt = γ 
vt − 1 + η ∇ θ J θ θ 
t = θ t − 1 − vt \ theta 
_ { t } = \ theta _ { t 
1 } v _ { t } θ t = 
θ t − 1 − v t 6.2 Adagrad 该 
算法 的 特点 是 自动 调整 学习率 适用于 稀疏 数据 
梯度 下 降法 在 每一步 对 每一个 参数 使用 相同 
的 学习率 这种 一刀切 的 做法 不能 有效 的 利用 
每 一个 数据集 自身 的 特点 Adagrad 是 一种 自动 
调整 学习率 的 方法 随着 模型 的 训练 学习率 自动 
衰减 对于 更新 频繁 的 参数 采取 较小 的 学习 
率 对于 更新 不 频繁 的 参数 采取 较大 的 
学习 率 6.3 Adadelta Adagrad 的 改进 算法 Adagrad 的 
一个 问题 在于 随着 训练 的 进行 学习率 快速 单调 
衰减 Adadelta 则 使用 梯度 平方 的 移动 平均 来 
取代 全部 历史 平方和 定义 移动 平均 E g2 t 
= γ E g2 t − 1 + 1 − 
γ gt2E g ^ { 2 } _ { t 
} = \ gamma E g ^ { 2 } 
_ { t 1 } + 1 \ gamma g 
_ { t } ^ { 2 } E g2 
t = γ E g2 t − 1 + 1 
− γ gt2Adadelta 的 第一 个 版本 也 叫做 RMSprop 
是 Geoff Hinton 独立 于 Adadelta 提 出来 的 6.4 
Adam 如果把 Adadelta 里面 梯度 的 平方和 看成 是 梯度 
的 二阶 矩 那么 梯度 本身 的 求和 就是 一 
阶 矩 Adam 算法 在 Adadelta 的 二次 矩 基础 
之上 又 引入 了 一 阶 矩 而 一 阶 
矩 其实 就 类似于 动 量法 里面 的 动量 7 
. Normalization7 . 1 batch n o r m a 
l i z a t i o n b a 
t c h normalization 是 优化 神经 网络 的 一大 
创新 它 并不 是 一个 优化 算法 而是 一个 自 
适应 的 调整 参数 模型 的 方法 它 试图 解决 
训练 非常 深 的 神经 网络 的 困难 深度 神经网络 
训练 困难 的 一个 重要 原因 是 深度 神经网络 涉及 
很多 层 的 叠加 而每 一层 的 参数 更新 会 
导致 上 一层 的 输入 数据分布 发生变化 这 会 带来 
两个 问题 下层 输入 的 变化 可能 趋向于 变大 或者 
变小 导致 上层 落入 饱和 区 使得 学习 过早 停止 
通过 层层 叠加 高层 的 输入 分布 变化 会 非常 
剧烈 这就 使得 高层 需要 不断 去 适应 底层 的 
参数 更 新变化 这就 要求 我们 需要 非常 谨慎 的 
设定 学习率 初始化 权重 参数 更 新策略 7.2 layer normalization 
与 BN 不同 LN 是 对 单个 样本 的 同 
一层 的 神经 元 进行 归一化 同层 神经元 使用 相同 
的 均值 和 方差 对于 该 层 神经元 不同 样本 
可以 使用 的 均值 和 方差 不同 与之 相比 BN 
是 对 每个 神经元 在 mini batch 样本 之间 计算 
均值 和 方差 对 每个 神经元 mini batch 中 的 
所有 样本 在 该 神经 元上都 使用 相同 的 均值 
和 方差 但是 不同 神经元 使用 不同 的 均值 和 
方差 因此 LN 不 依赖 于 batch size 也不 依赖 
于 网络 深度 因此 它 适合 在线 学习 也 适合 
于 RNN 网络 # 2018 06 10 June Sunday the 
23 week the 161 day SZ 数据 来源 链接 https 
/ / pan . baidu . com / s / 
1 _ w 7 w O z N k U 
E a q 3 K A G c o 1 
9 E Q 密码 87o0 朴素 贝叶斯 与 应用 文本 
分类 问题 经典 的 新闻 主题 分类 用 朴素 贝叶斯 
做 # 还 有点 问题 无法 正确 读取数据 U n 
i c o d e D e c o d 
e E r r o r charmap codec can t 
decode byte 0x90 in position 41 character maps to undefined 
folder _ path = D / 自然语言 处理 / 第 
2 课 / Lecture _ 2 / Lecture _ 2 
/ Naive Bayes Text Classifier / Database / SogouC / 
Sample all _ words _ list train _ data _ 
list test _ data _ list train _ class _ 
list test _ class _ list = text _ processing 
folder _ path test _ size = 0.2 import os 
import time import random import codecs import jieba # 处理 
中文 # import nltk # 处理 英文 import sklearn from 
sklearn . naive _ bayes import MultinomialNB import numpy as 
np import pylab as pl import matplotlib . pyplot as 
plt import sys # reload sys # sys . s 
e t d e f a u l t e 
n c o d i n g utf8 # 粗暴 
的 词 去 重 def make _ word _ set 
words _ file words _ set = set with open 
words _ file r as fp for line in fp 
. readlines word = line . strip . decode utf 
8 if len word 0 and word not in words 
_ set # 去 重 words _ set . add 
word return words _ set # 文本处理 也 就是 样本 
生成 过程 def text _ processing folder _ path test 
_ size = 0.2 folder _ list = os . 
listdir folder _ path data _ list = class _ 
list = # 遍历 文件夹 for folder in folder _ 
list new _ folder _ path = os . path 
. join folder _ path folder files = os . 
listdir new _ folder _ path # 读取 文件 j 
= 1 for file in files if j 100 # 
怕 内存 爆 掉 只取 100个 样本 文件 你 可以 
注释 掉 取 完 break with open os . path 
. join new _ folder _ path file r as 
fp raw = fp . read # # 是的 随处可见 
的 jieba 中文分词 # jieba . enable _ parallel 4 
# 开启 并行 分 词模式 参数 为 并行 进程 数 
不支持 windows word _ cut = jieba . cut raw 
cut _ all = False # 精确 模式 返回 的 
结构 是 一个 可 迭代 的 genertor word _ list 
= list word _ cut # genertor 转化 为 list 
每个 词 unicode 格式 # jieba . disable _ parallel 
# 关闭 并行 分 词模式 data _ list . append 
word _ list # 训练 集 list class _ list 
. append folder . decode utf 8 # 类别 j 
+ = 1 # # 粗暴 地 划分 训练 集 
和 测试 集 data _ class _ list = zip 
data _ list class _ list random . shuffle data 
_ class _ list index = int len data _ 
class _ list * test _ size + 1 train 
_ list = data _ class _ list index test 
_ list = data _ class _ list index train 
_ data _ list train _ class _ list = 
zip * train _ list test _ data _ list 
test _ class _ list = zip * test _ 
list # 其实 可以 用 sklearn 自带 的 部分 做 
# train _ data _ list test _ data _ 
list train _ class _ list test _ class _ 
list = sklearn . cross _ validation . train _ 
test _ split data _ list class _ list test 
_ size = test _ size # 统计 词频 放入 
all _ words _ dict all _ words _ dict 
= { } for word _ list in train _ 
data _ list for word in word _ list if 
all _ words _ dict . has _ key word 
all _ words _ dict word + = 1 else 
all _ words _ dict word = 1 # key 
函数 利用 词频 进行 降序 排序 all _ words _ 
tuple _ list = sorted all _ words _ dict 
. items key = lambda f f 1 reverse = 
True # 内建函数 sorted 参数 需 为 list all _ 
words _ list = list zip * all _ words 
_ tuple _ list 0 return all _ words _ 
list train _ data _ list test _ data _ 
list train _ class _ list test _ class _ 
list def words _ dict all _ words _ list 
deleteN stopwords _ set = set # 选取 特征词 feature 
_ words = n = 1 for t in range 
deleteN len all _ words _ list 1 if n 
1000 # feature _ words 的 维度 1000 break if 
not all _ words _ list t . isdigit and 
all _ words _ list t not in stopwords _ 
set and 1 len all _ words _ list t 
5 feature _ words . append all _ words _ 
list t n + = 1 return feature _ words 
# 文本 特征 def text _ features train _ data 
_ list test _ data _ list feature _ words 
flag = nltk def text _ features text feature _ 
words text _ words = set text # # if 
flag = = nltk # # nltk 特征 dict features 
= { word 1 if word in text _ words 
else 0 for word in feature _ words } elif 
flag = = sklearn # # sklearn 特征 list features 
= 1 if word in text _ words else 0 
for word in feature _ words else features = # 
# return features train _ feature _ list = text 
_ features text feature _ words for text in train 
_ data _ list test _ feature _ list = 
text _ features text feature _ words for text in 
test _ data _ list return train _ feature _ 
list test _ feature _ list # 分类 同时 输出 
准确率 等 def text _ classifier train _ feature _ 
list test _ feature _ list train _ class _ 
list test _ class _ list flag = nltk # 
# if flag = = nltk # # 使用 nltk 
分类器 train _ flist = zip train _ feature _ 
list train _ class _ list test _ flist = 
zip test _ feature _ list test _ class _ 
list classifier = nltk . classify . N a i 
v e B a y e s C l a 
s s i f i e r . train train 
_ flist test _ accuracy = nltk . classify . 
accuracy classifier test _ flist elif flag = = sklearn 
# # sklearn 分类器 classifier = MultinomialNB . fit train 
_ feature _ list train _ class _ list test 
_ accuracy = classifier . score test _ feature _ 
list test _ class _ list else test _ accuracy 
= return test _ accuracy print start # # 文本 
预处理 folder _ path = D / 自然语言 处理 / 
第 2 课 / Lecture _ 2 / Lecture _ 
2 / Naive Bayes Text Classifier / Database / SogouC 
/ Sample all _ words _ list train _ data 
_ list test _ data _ list train _ class 
_ list test _ class _ list = text _ 
processing folder _ path test _ size = 0.2 # 
生成 stopwords _ set stopwords _ file = D \ 
\ 自然语言 处理 \ \ 第 2 课 \ \ 
Lecture _ 2 \ \ Lecture _ 2 \ \ 
Naive Bayes Text Classifier \ \ stopwords _ cn . 
txt stopwords _ set = make _ word _ set 
stopwords _ file # # 文本 特征提取 和 分类 # 
flag = nltk flag = sklearn deleteNs = range 0 
1000 20 test _ accuracy _ list = for deleteN 
in deleteNs # feature _ words = words _ dict 
all _ words _ list deleteN feature _ words = 
words _ dict all _ words _ list deleteN stopwords 
_ set train _ feature _ list test _ feature 
_ list = text _ features train _ data _ 
list test _ data _ list feature _ words flag 
test _ accuracy = text _ classifier train _ feature 
_ list test _ feature _ list train _ class 
_ list test _ class _ list flag test _ 
accuracy _ list . append test _ accuracy print test 
_ accuracy _ list # 结果 评价 # plt . 
figure plt . plot deleteNs test _ accuracy _ list 
plt . title Relationship of deleteNs and test _ accuracy 
plt . xlabel deleteNs plt . ylabel test _ accuracy 
plt . show # plt . savefig result . png 
print finished NLP 全名 Natural Language Processing 自然语言 处理 从 
1949 年的/nr 机器 翻译 设计方案 到 如今 比尔盖茨 认为 自然语言 
理解 是 人工智能 皇冠 上 的 明珠 NLP 成为 了 
人工智能 领域 的 重要 战略 目标 有/v 读者/n 曾/d 留言/v 
问道/n 自然 指 的 是 什么 在 这里 自然 指 
的 是 随 文化 自然而然 演化 的 过程 一千 个人 
里 就有 一千个 哈姆雷特 更何况 还要 让 机器 去 理解 
我们 的 语言 这期 我们 精心 挑选 了 8 篇 
NLP 相关 的 原理 应用 和 中文 文本处理 项目 文章 
供 大家 学习 微信/i 模式识别/n 中心/n 的/uj 高级/b 研究员/n 张金超/nr 
博士/n 不仅/c 介绍/v 了/ul 自然/d 语言/n 处理/v 的/uj 基本/n 概念/n 
和/c 任务/n 还 结合 项目 经验 讲解 使用 深度 学习 
解决 NLP 的 方法 和 应用 文 末尾 也给 想 
技能 进阶 的 同学 提供 了 智者 建议 点 此 
阅读 全文 如果 你 是 一位 NLP or 机器学习 爱好者 
那你/nr 不能 错过 Quora 上 回答 过万 的 热门 问题 
如果 你 恰好 英语 还 可以 那小编/nr 觉得/v 你/r 最好/a 
别/r 错过/v 点 此 阅读 全文 作者 运用 Python 对 
红楼梦 进行 了 中文分词 效果 如何 你 不妨 去 看看 
点 此 阅读 全文 用 WordCloud 制 作词 云 朴素 
贝叶斯 算法 和 SVM 分别 对 文本 分类 LDA 主题 
模型 获取 文本 关键词 这 一系列 中文 文本处理 手段 都 
能在 这篇文章 看到 点 此 阅读 全文 学会 了 文本处理 
基础 后 这篇 将用 集成 学习 和 深度 学习 的 
两个 应用 实例 来 详细 解说 文本 分类 的 原理 
点 此 阅读 全文 除了 文本 分类 文本 相似 度 
的 度量 和 计算 也是 必不可少 的 技能 即使 不 
掌握 但 也 可以 通过 了解 相关 的 原理 来 
避免 文章 被 洗 稿 点 此 阅读 全文 要是 
想 学着 设计 一个 文本 相似 度 系统 可以 来 
看看 这篇 点 此 阅读 全文 基于 在线 LU 工具 
无法 本地 部署 等 问题 微软 工程师 从 基础 开始 
讲解 语言 理解 模块 的 工程 实现 点 此 阅读 
全文 如何 检索 自然语言 处理 领域 相关 论文 前言 针对 
自身 的 情况 发现 个人 查找 论文 的 能力 看 
论文 的 能力 有些 薄弱 特此 进行 如果 检索 合适 
的 论文 写 一个 博客 本文 主要 是 摘自 刘知远 
老师 的 新浪 博客 和 南京 理工大学 文本 挖掘 研究组 
博客 综述 要 快速 地 熟悉 一个 领域 更加 深刻 
地 了解 这 该 领域 的 发展 就必须 查阅 这个 
领域 的 相关 论文 本文 主要 讲述 自然语言 处理 领域 
NLP 相关 论文 的 检索 与 其他 领域 一样 自然语言/l 
处理/v 领域/n 每年/r 都有/nr 大量/n 的/uj 论文/nz 发表/v 在/p 各种/r 
期刊/n 会议 上 然而 人 的 时间 和 精力 是 
有限 的 如何 能在/nr 有限 的 时间 内 检索 出 
该 领域 的 高 影响力 高 质量 的 论文 是 
我们 所 关注 的 对于 这个 问题 首先 我们 应当 
了解 一下 自然语言 知名 的 学术 组织 学术 会议 及 
学术 论文 其次是 在 了解 上述 信息 基础上 的 论文 
检索 手段 下面 本文 将 从 国内外 自然语言 处理 领域 
知名 的 学术 组织 学术 会议 及 学术 论文 及 
相关 论文 检索 和 筛选 的 经验 两 方面 内容 
介绍 一些 关于 自然 语言 处理 领域 的 知识 和 
论文 检索 的 经验 本文 第一 部分 引用 清华大学 刘知远 
老师 新浪 博客 上 的 一篇 博文 针对 国内外 自然语言 
处理 领域 知名 的 学术 组织 学术 会议 及 学术 
论文 的 介绍 第二 部分 将 分享 一些 前 一段 
时间 在 论文 调研 过程 中 关于 论文 查找 和 
筛选 的 一些 经验 希望 对 大家 有所 帮助 正文 
1 . 初学者 如何 查阅 自然语言 处理 NLP 领域 学术 
资料 作者 刘知远 昨天 实验室 一位 刚 进 组 的 
同学 发 邮件 来问 我 如何查找 学术论文 这 让 我 
想起 自己 刚 读 研究生 时 茫然 四顾 的 情形 
看着 学长 们 高谈阔论 领域 动态 却 不知 如何 入门 
经过 研究生 几年 的 耳濡目染 现在 终于 能 自信 地 
知道 去 哪儿 了解 最新 科研 动态 了 我 想 
这 可能 是 初学者 们 共通 的 困惑 与其 只 
告诉 一个 人 知道 不如 将 这些 Folk Knowledge 写下来 
来 减少 更多 人 的 麻烦 吧 当然 这个 总结 
不过 是 一家 之谈 只 盼 有人 能 从中 获得 
一点点 益处 受 个人 认知 所限 难免 挂一漏万 还望 大家 
海涵 指正 1.1 国际 学术 组织 学术 会议 与 学术 
论文 自然语言 处理 natural language processing NLP 在 很大 程度 
上 与 计算 语言学 computational linguistics CL 重合 与 其他 
计算机 学科 类似 NLP / CL 有 一个 属于 自己 
的 最 权威 的 国际 专业 学会 叫做 The Association 
for Computational Linguistics ACL URL http / / aclweb . 
org / 这个 协会 主办 了 NLP / CL 领域 
最 权威 的 国际 会议 即 ACL 年会 ACL 学会 
还会在 北美 和 欧洲 召开 分 年会 分别 称为 NAACL 
和 EACL 除此之外 ACL 学会 下设 多个 特殊 兴趣小组 special 
interest groups SIGs 聚集 了 NLP / CL 不同 子 
领域 的 学者 性质 类似 一个 大学 校园 的 兴趣 
社团 其中 比较 有名 的 诸如 SIGDAT Linguistic data and 
corpus based approaches to NLP SIGNLL Natural Language Learning 等 
这些 SIGs 也会 召开 一些 国际 学术 会议 其中 比较 
有名 的 就是 SIGDAT 组织 的 EMNLP Conference on Empirical 
Methods on Natural Language Processing 和 SIGNLL 组织 的 CoNLL 
Conference on Natural Language Learning 此外 还有 一个 International Committee 
on Computational Linguistics 的 老牌 NLP / CL 学术 组织 
它 每 两年 组织 一个 称为 International Conference on Computational 
Linguistics COLING 的 国际 会议 也是 NLP / CL 的 
重要 学术 会议 NLP / CL 的 主要 学术 论文 
就 分布 在 这些 会议 上 作为 NLP / CL 
领域 的 学者 最大 的 幸福 在于 ACL 学会 网站 
建立 了 称作 ACL Anthology 支持 该 领域 绝大部分 国际 
学术 会议 论文 的 免费 下载 甚至 包含 了 其他 
组织 主办 的 学术 会议 例如 COLING IJCNLP 等 并 
支持 基于 Google 的 全文 检索 功能 可谓 一站 在手 
NLP 论文 我 有 由于 这个 论文 集合 非常 庞大 
并且 可以 开放 获取 很多 学者 也 基于 它 开展 
研究 提供 了 更 丰富 的 检索 支持 具体 入口 
可以 参考 ACL Anthology 页面 上方 搜索框 右侧 的 不同 
检索 按钮 与 大部分 计算机 学科 类似 由于 技术 发展 
迅速 NLP / CL 领域 更 重视 发表 学术 会议 
论文 原因 是 发表 周期短 并 可以 通过 会议 进行 
交流 当然 NLP / CL 也有 自己 的 旗舰 学术期刊 
发表 过 很多 经典 学术论文 那 就是 Computational Linguistics 该 
期刊 每期 只有 几 篇文章 平均/a 质量/n 高于/nr 会议/n 论文/nz 
时间 允许 的话 值得 及时 追踪 此外 ACL 学会 为了 
提高 学术 影响力 也 刚刚 创办 了 Transactions of ACL 
值得 关注 值得一提的是 这 两份 期刊 也都 是 开放 获取 
的 此外 也 有 一些 与 NLP / CL 有关 
的 期刊 如 ACM Transactions on Speech and Language Processing 
ACM Transactions on Asian Language Information Processing Journal of Quantitative 
Linguistics 等等 根据 Google Scholar Metrics 2013年 对 NLP / 
CL 学术 期刊 和 会议 的 评价 ACL EMNLP NAACL 
COLING LREC Computational Linguistics 位于 前 5位 基本 反映 了 
本 领域 学者 的 关注 程度 NLP / CL 作为 
交叉学科 其 相关 领域 也 值得 关注 主要 包括 以下 
几个 方面 1 信息检索 和 数据挖掘 领域 相关 学术 会议 
主要 由 美国 计算机 学会 ACM 主办 包括 SIGIR WWW 
WSDM 等 2 人工智能 领域 相关 学术 会议 主要 包括 
AAAI 和 IJCAI 等 相关 学术 期刊 主要 包括 Artificial 
Intelligence 和 Journal of AI Research 3 机器学习 领域 相关 
学术 会议 主要 包括 ICML NIPS AISTATS UAI 等 相关 
学术 期刊 主要 包括 Journal of Machine Learning Research JMLR 
和 Machine Learning ML 等 例如 最近 兴起 的 knowledge 
graph 研究 论文 就有/i 相当/d 一/m 部分/n 发表/v 在/p 人工/n 
智能/n 和/c 信息检索/n 领域/n 的/uj 会议/n 和/c 期刊/n 上/f 实际上 
国内 计算机 学会 CCF 制定 了 中国 计算机 学会 推荐 
国际 学术 会议 和 期刊目录 http / / www . 
ccf . org . cn / sites / ccf / 
aboutpm . jsp contentId = 2567814757463 通过 这个 列表 可以 
迅速 了解 每个 领域 的 主要 期刊 与 学术 会议 
最后 值得一提的是 美国 Hal Daum é III 维护 了 一个 
1 信息检索 和 数据挖掘 领域 相关 学术 会议 主要 由 
美国 计算机 学会 ACM 主办 包括 natural language processing 的 
博客 http / / nlpers . blogspot . com / 
经常 评论 最新 学术 动态 值得 关注 我 经常 看 
他 关于 ACL NAACL 等 学术 会议 的 参会 感想 
和对/nr 论文 的 点评 很 有启发 另外 ACL 学会 维护 
了 一个 Wiki 页面 http / / aclweb . org 
/ aclwiki / 包含 了 大量 NLP / CL 的 
相关 信息 如 著名 研究 机构 历届 会议录 用率 等等 
都是 居家 必备 之 良 品 值得 深挖 1.2 国内 
学术 组织 学术 会议 与 学术 论文 与 国际 上 
相似 国内 也 有一个 与 NLP / CL 相关 的 
学会 叫做 中国 中文信息 学会 URL http / / www 
. cipsc . org . cn / 通过 学会 的 
理事 名单 http / / www . cipsc . org 
. cn / lingdao . php 基本 可以 了解 国内 
从事 NLP / CL 的 主要 单位 和 学者 学会 
每年 组织 很多 学术会议 例如 全国 计算 语言学 学术会议 CCL 
全国 青年 计算 语言学 研讨会 YCCL 全国 信息检索 学术会议 CCIR 
全国 机器翻译 研讨会 CWMT 等等 是 国内 NLP / CL 
学者 进行 学术 交流 的 重要 平台 尤其 值得一提的是 全国 
青年 计算 语言学 研讨会 是 专门 面向 国内 NLP / 
CL 研究生 的 学术 会议 从 组织 到 审稿 都由 
该 领域 研究生 担任 非常 有 特色 也是 NLP / 
CL 同学们 学术交流 快速 成长 的 好去处 值得一提的是 2010年 在 
北京 召开 的 COLING 以及 2015年 即将 在 北京 召开 
的 ACL 学会 都是/nr 主要 承办者 这也 一定 程度 上 
反映 了 学会 在 国内 NLP / CL 领域 的 
重要 地位 此外 计算机 学会 中文 信息 技术 专委会 组织 
的 自然 语言 处理 与 中文 计算 会议 NLP & 
CC 也是 最近 崛起 的 重要 学术 会议 中文信息 学会 
主编 了 一份 历史 悠久 的 中文信息 学报 是 国内 
该 领域 的 重要 学术 期刊 发表 过 很多 篇 
重量级 论文 此外 国内 著名 的 计算机 学报 软件 学报 
等 期刊 上 也 经常 有 NLP / CL 论文 
发表 值得 关注 1.3 如何 快速 了解 某 个 领域 
研究 进展 最后 简单 说 一下 快速 了解 某 领域 
研究 进展 的 经验 你 会 发现 搜索引擎 是 查阅 
文献 的 重要 工具 尤其 是 谷歌 提供 的 Google 
Scholar 由于 其 庞大 的 索引 量 将 是 我们 
披荆斩棘 的 利器 当 需要 了解 某 个 领域 如果 
能 找到 一篇 该 领域 的 最新 研究 综述 就 
省劲 多了 最 方便 的 方法 还是 在 Google Scholar 
中 搜索 领域 名称 + survey / review / tutorial 
/ 综述 来 查找 也 有 一些 出版社 专门 出版 
各 领域 的 综述 文章 例如 NOW Publisher 出版 的 
Foundations and Trends 系列 Morgan & Claypool Publisher 出版 的 
Synthesis Lectures on Human Language Technologies 系列 等 它们 发表 
了 很多 热门 方向 的 综述 如 文档 摘要 情感 
分析 和 意见 挖掘 学习 排序 语言 模型 等 如果 
方向 太 新 还 没有 相关 综述 一般 还 可以 
查找 该 方向 发表 的 最新 论文 阅读 它们 的 
相关 工作 章节 顺着 列出 的 参考 文献 就 基本 
能够 了解 相关 研究 脉络 了 当然 还有 很多 其他 
办法 例如 去 videolectures . net 上看 著名 学者 在 
各大 学术 会议 或 暑期学校 上 做 的 tutorial 报告 
去 直接 咨询 这个 领域 的 研究 者 等等 2 
. 补充 参考 南京 理工大学 文本 挖掘 研究组 博客 另 
附上 南京 理工大学 文本 挖掘 研究组 博客 实现 的 一款 
论文 调研 工具 该 工具 基于 Python 的 爬虫 技术 
可根据 论文 发表 年份 关键字 发表 会议 等 信息 自动 
批量 抓取 主题 相关 论文 的 标题 然后 从 Google 
Scholar 获取 引用 次数 下载 链接 论文 作者 论文 摘要 
信息 并 按 指定 的 格式 保存 在 EXCEL 文档 
中 github 链接 总论 自然语言 处理 关于 斯坦福 自然语言 处理 
NLP 工具 资料 收集 斯坦福 分词 链接 https / / 
nlp . stanford . edu / software / segmenter . 
shtmlChinese is standardly written withoutspaces between words as are some 
other languages . This software will splitChinese text into a 
sequence of words defined according to some wordsegmentation standard . 
It is a Java implementation of the CRF based Chinese 
Word Segmenter described in 斯坦福 做 的 中文分词 是 基于 
条件 随 机场 实现 的 斯坦福 大学 自然 语言 处理 
组 是 世界 知名 的 NLP 研究小组 他们 提供 了 
一系列 开源 的 Java 文本 分析 工具 包括 分词器 Word 
Segmenter 词性 标注 工具 Part Of Speech Tagger 命名 实体 
识别 工具 Named Entity Recognizer 句法 分析器 Parser 等 可喜 
的 事 他们 还 为 这些 工具 训练 了 相应 
的 中文 模型 支持 中文 文本处理 摘抄 于 http / 
/ www . 52nlp . cn / python 自然语言 处理 
实践 在 nltk 中 使用 斯坦福 中文 分词器 # more 
6763 使用 Stanford NLP 工具 实现 中文 命名 实体 识别 
http / / m . blog . csdn . net 
/ article / details id = 49497231 按照 上面 的 
链接 步骤 也 下载 了 分词器 stanford segmenter 2016 10 
31data 目录 下有 两个 gz 压缩文件 分别 是 ctb . 
gz 和 pku . gz 其中 CTB 宾州 大学 的 
中国 树 库 训练 资料 PKU 中国北京大学 提供 的 训练 
资料 NER 实体 识 别在 edu . stanford . nlp 
. ie . crfjava mx600m cp * lib \ * 
edu . stanford . nlp . ie . crf . 
CRFClassifier l o a d C l a s s 
i f i e r c l a s s 
i f i e r s / english . all 
. 3class . distsim . crf . ser . gz 
textFile sample . txtjava mx600m cp * lib / * 
edu . stanford . nlp . ie . crf . 
CRFClassifier l o a d C l a s s 
i f i e r c l a s s 
i f i e r s / english . all 
. 3class . distsim . crf . ser . gz 
outputFormat tabbedEntities textFile sample . txt sample . tsvChineseWe also 
provideChinese models built from the Ontonotes Chinese named entity data 
. There aretwo models one using distributional similarity clusters and 
one without . Theseare designed to be run on   
word segmented Chinese . So if you wantto use these 
on normal Chinese text you will first need to run 
  Stanford Word Segmenter   orsome other Chinese word segmenter 
and then run NER on the output of that 3 
. 7.0 C h i n e s e m 
o d e l s s t a n f 
o r d chinese corenlp 2016 10 31 models . 
jar 使用 斯坦福 中文 实体 标注 之前 必须 先 完成 
分词 的 任务 实体 识别 的 Demohttp / / nlp 
. stanford . edu / software / ner example / 
NERDemo . java 参考 文章 http / / blog . 
csdn . net / yangyangrenren / article / details / 
54709925 中文 实体 识别 的 代码 import edu . stanford 
. nlp . ie . A b s t r 
a c t e q u e n c e 
C l a s s i f i e r 
import edu . stanford . nlp . ie . crf 
. * import edu . stanford . nlp . io 
. IOUtils import edu . stanford . nlp . ling 
. CoreLabel import edu . stanford . nlp . ling 
. CoreAnnotations import edu . stanford . nlp . sequences 
. D o c u m e n t R 
e a d e r A n d W r 
i t e r import edu . stanford . nlp 
. util . Triple import java . util . List 
/ * * This is a demo of calling CRFClassifier 
programmatically . * p * Usage { @ code java 
mx400m cp * NERDemo s e r i a l 
i z e d C l a s s i 
f i e r fileName } * p * If 
arguments aren t specified they default to * classifiers / 
english . all . 3class . distsim . crf . 
ser . gz and some hardcoded sample text . * 
If run with arguments it shows some of the ways 
to get k best labelings and * probabilities out with 
CRFClassifier . If run without arguments it shows some of 
* the alternative output formats that you can get . 
* p * To use CRFClassifier from the command line 
* / p blockquote * { @ code java mx400m 
edu . stanford . nlp . ie . crf . 
CRFClassifier loadClassifier classifier textFile file } * / blockquote p 
* Or if the file is already tokenized and one 
word per line perhaps in * a tab separated value 
format with extra columns for part of speech tag * 
etc . use the version below note the s instead 
of the x * / p blockquote * { @ 
code java mx400m edu . stanford . nlp . ie 
. crf . CRFClassifier loadClassifier classifier testFile file } * 
/ blockquote * * @ author Jenny Finkel * @ 
author Christopher Manning * / public class NERDemo { public 
static void main String args throws Exception { String s 
e r i a l i z e d C 
l a s s i f i e r = 
classifiers / chinese . misc . distsim . crf . 
ser . gz if args . length 0 { s 
e r i a l i z e d C 
l a s s i f i e r = 
args 0 } A b s t r a c 
t e q u e n c e C l 
a s s i f i e r CoreLabel classifier 
= CRFClassifier . getClassifier s e r i a l 
i z e d C l a s s i 
f i e r / * For either a file 
to annotate or for the hardcoded text example this demo 
file shows several ways to process the input for teaching 
purposes . * / if args . length 1 { 
/ * For the file it shows 1 how to 
run NER on a String 2 how to get the 
entities in the String with character offsets and 3 how 
to run NER on a whole file without loading it 
into a String . * / String fileContents = IOUtils 
. slurpFile args 1 List List CoreLabel out = classifier 
. classify fileContents for List CoreLabel sentence out { for 
CoreLabel word sentence { System . out . print word 
. word + / + word . get CoreAnnotations . 
AnswerAnnotation . class + } System . out . println 
} System . out . println out = classifier . 
classifyFile args 1 for List CoreLabel sentence out { for 
CoreLabel word sentence { System . out . print word 
. word + / + word . get CoreAnnotations . 
AnswerAnnotation . class + } System . out . println 
} System . out . println List Triple String Integer 
Integer list = classifier . c l a s s 
i f y T o C h a r a 
c t e r O f f s e t 
s fileContents for Triple String Integer Integer item list { 
System . out . println item . first + + 
fileContents . substring item . second item . third } 
System . out . println System . out . println 
Ten best entity labelings D o c u m e 
n t R e a d e r A n 
d W r i t e r CoreLabel readerAndWriter = 
classifier . m a k e P l a i 
n T e x t R e a d e 
r A n d W r i t e r 
classifier . c l a s s i f y 
A n d W r i t e A n 
s w e r s K B e s t 
args 1 10 readerAndWriter System . out . println System 
. out . println Per token marginalized probabilities classifier . 
printProbs args 1 readerAndWriter / / This code prints out 
the first order token pair clique probabilities . / / 
But that output is a bit overwhelming so we leave 
it commented out by default . / / System . 
out . println / / System . out . println 
First Order Clique Probabilities / / CRFClassifier classifier . p 
r i n t F i r s t O 
r d e r P r o b s args 
1 readerAndWriter } else { / * For the hard 
coded String it shows how to run it on a 
single sentence and how to do this and produce several 
formats including slash tags and an inline XML output format 
. It also shows the full contents of the { 
@ code CoreLabel } s that are constructed by the 
classifier . And it shows getting out the probabilities of 
different assignments and an n best list of classifications with 
probabilities . * / String example = { 5月 8日 
下午 李克强 考察 河南 新乡 封丘县 黄河 滩区 后 随即 
在 当地 居民 迁建 指挥部 主持 召开 现场会 专题 研究 
河南 山东 两 省 黄河 滩区 居民 迁建 工作 除 
陪同 总理 考察 的 国务院 领导 及 发改委 财政部 水利部 
黄河 水利 委员会 河南省 负责人 外 山东省 省长 也 专程 
赶来 参会 窗外 一直 下 着 雨 会前 李克强 结束 
开封 考察 后 专程 驱车 一 小时 赴 新乡市 封丘县 
黄河 滩区 冒雨 踩 着 泥泞 小路 实地 察看 黄河 
滩区 并 入户 探望 滩区 居民 黄河 滩区 问题 是 
多年来 历史 形成 的 现在 到 了 该 解决 的 
时候 了 李克强 面色 凝重 地 说 滩区 迁建 关乎 
近 200万 滩区 居民 的 生活 和 发展 也 关系 
黄河 的 长治久安 黄河 的 事 是 天下 大 事 
} for String str example { System . out . 
println classifier . classifyToString str } System . out . 
println for String str example { / / This one 
puts in spaces and newlines between tokens so just print 
not println . System . out . print classifier . 
classifyToString str slashTags false } System . out . println 
for String str example { / / This one is 
best for dealing with the output as a TSV tab 
separated column file . / / The first column gives 
entities the second their classes and the third the remaining 
text in a document System . out . print classifier 
. classifyToString str tabbedEntities false } System . out . 
println for String str example { System . out . 
println classifier . c l a s s i f 
y W i t h I n l i n 
e X M L str } System . out . 
println for String str example { System . out . 
println classifier . classifyToString str xml true } System . 
out . println for String str example { System . 
out . print classifier . classifyToString str tsv false } 
System . out . println / / This gets out 
entities with character offsets int j = 0 for String 
str example { j + + List Triple String Integer 
Integer triples = classifier . c l a s s 
i f y T o C h a r a 
c t e r O f f s e t 
s str for Triple String Integer Integer trip triples { 
System . out . printf % s over character offsets 
% d % d in sentence % d . % 
n trip . first trip . second trip . third 
j } } System . out . println / / 
This prints out all the details of what is stored 
for each token int i = 0 for String str 
example { for List CoreLabel lcl classifier . classify str 
{ for CoreLabel cl lcl { System . out . 
print i + + + System . out . println 
cl . toShorterString } } } System . out . 
println } } } 结果 图 自然 语言 是 人类 
独有 的 智慧 结晶 自然语言 处理 Natural Language Processing NLP 
是 计算机 科学 领域 与 人工智能 领域 中 的 一个 
重要 方向 旨在/v 研究/vn 能/v 实现/v 人/n 与/p 计算机/n 之间/f 
用/p 自然/d 语言/n 进行/v 有效/a 通信/l 的/uj 各种/r 理论/n 和/c 
方法/n 用 自然 语言 与 计算机 进行通信 有着 十分 重要 
的 实际 应用 意义 也 有着 革命性 的 理论 意义 
由于 理解 自然语言 需要 关 于外 在 世界 的 广泛 
知识 以及 运用 操作 这些 知识 的 能力 所以 自然 
语言 处理 也 被 视为 解决 人工智能 完备 AI complete 
的 核心 问题 之一 对 自然 语言 处理 的 研究 
也 是 充满 魅力 和 挑战 的 本文 是 来自 
自然语言 处理 领域 从业人员 知名 博主 Sebatian Ruder 的 一篇 
文章 主要 从 神经 网络 技术 方法 的 角度 讨论 
自然语言 处理 领域 近 15 年来 的 重大 进展 并 
总结 出 与 当下 息息相关 的 8 大 里程碑 事件 
文章内容 难免会 省略 了 一些 其它 重要 的 相关 工作 
同时 这份 总结 偏向 于 神经网络 相关 技术 这 并不 
意味着 在 这段 时间 内 其它 技术领域 就 没有 重要 
的 进展 值得 注意 的 是 文中 提及 的 很多 
神经网络 模型 都是/nr 建立 在 同一 时期 非 神经 网络 
技术 的 里程碑 之上 的 在 文章 的 最后 我们 
强调 了 这些 打下 坚实 基础 的 重要 成果 2001年 
神经 语言 模型 Neurallanguage models 语言 模型 解决 的 是 
在 给定 已 出现 词语 的 文本 中 预测 下 
一个 单词 的 任务 这 可以 算是 最 简单 的 
语言 处理 任务 但却 有 许多 具体 的 实际 应用 
例如 智能 键盘 电子 邮件 回复 建议 等 当然 语言 
模型 的 历史 由来已久 经典 的 方法 基于 n grams 
模型 利用 前面 n 个 词语 预测 下 一个 单词 
并 利用 平滑 操作 处理 不 可见 的 n grams 
第一 个 神经 语言 模型 前馈 神经网络 feed forward neuralnetwork 
是 Bengio 等人 于 2001 年 提出 的 如图 1 
所示 图 1 | 前馈 神经 网络 语言 模型 Bengio 
et al . 2001 2003 这个 模型 以 某 词语 
之前 出现 的 n 个 词语 作为 输入 向量 今天 
这样 的 向量 被称为 大家 熟知 的 词 嵌入 word 
embeddings 这些 词 嵌入 在 级联 后 进入 一个 隐藏 
层 该 层 的 输出 然后 通过 一个 softmax 层 
近年来 用于 构建 语言 模型 的 前馈 神经 网络 已经 
被 循环 神经网络 RNNs 和 长短期 记忆 神经网络 LSTMs 取代 
虽然 后来 提出 的 许多 新 模型 在 经典 的 
LSTM 上 进行 了 扩展 但 它 仍然 是 强有力 
的 基础 模型 甚至 Bengio 等人 的 经典 前馈 神经 
网络 在 某些 设 定下 也和 更 复杂 的 模型 
效果 相当 因为 这些 任务 只 需要 考虑 邻近 的 
词语 更好 地 理解 语言 模型 究竟 捕捉 了 哪些 
信息 也 是 当今 一个 活跃 的 研究 领域 语言 
模型 的 建立 是 一种 无 监督 学习 unsupervised learning 
Yann LeCun 也 将其 称之为 预测 学习 predictive learning 是 
获得 世界 如何 运作 常识 的 先决条件 关于 语言 模型 
最 引人注目 的 是 尽管 它 很 简单 但 却与 
后文 许多 核心 进展 息息相关 反过来 这也 意味着 自然语言 处理 
领域 的 许多 重要 进展 都 可以 简化 为 某种 
形式 的 语言 模型 构建 但要 实现 对 自然 语言 
真正 意义 上 的 理解 仅仅 从 原始 文本 中 
进行 学习 是 不够 的 我们 需要 新的 方法 和 
模型 2008年 多任务 学习 Multi tasklearning 多任务 学习 是 在 
多个 任务 下 训练 的 模型 之间 共享 参数 的 
方法 在 神经 网络 中 可以 通过 捆绑 不 同层 
的 权重 轻松 实现 多任务 学习 的 思想 在 1993 
年由/nr Rich Caruana 首次 提出 并/c 应用/v 于/p 道路/n 追踪/v 
和/c 肺炎/n 预测/vn 多任务 学习 鼓励 模型 学习 对 多个 
任务 有效 的 表征 描述 这 对于 学习 一 般的 
低级 的 描述 形式 集中 模型 的 注意力 或在 训练 
数据 有限 的 环境 中 特别 有用 多任务 学 习于 
2008 年被/nr Collobert 和 Weston 等人 首次 在 自然 语言 
处理 领域 应用 于 神经网络 在 他们 的 模型 中 
词 嵌入 矩阵 被 两个 在 不同 任务 下 训练 
的 模型 共享 如图 2 所示 图 2 | 词 
嵌入 矩阵 共享 Collobert & Weston 2008 Collobert et al 
. 2011 共享 的 词 嵌入 矩阵 使 模型 可以 
相互 协作 共享 矩阵 中的 低 层级 信息 而 词 
嵌入 矩阵 往往 构成 了 模型 中 需要 训练 的 
绝大部分 参数 Collobert 和 Weston 发表于 2008 年的/nr 论文 影响 
远远 超过 了 它 在 多任务 学习 中 的 应用 
它 开创 的 诸如 预 训练 词 嵌入 和 使用 
卷积 神经网络 处理 文本 的 方法 在 接下来 的 几年 
被 广泛 应用 他们 也 因此 获得 了 2018 年 
机器学习 国际 会议 ICML 的 test of time 奖 如今 
多任务 学习 在 自然 语言 处理 领域 广泛 使用 而 
利用 现有 或 人工 任务 已经 成为 NLP 指令 库 
中 的 一个 有用/nr 工具 虽然 参数 的 共享 是 
预先 定义 好 的 但在 优化 的 过程 中 却 
可以 学习 不同 的 共享 模式 当 模型 越来越 多 
地 在 多个 任务 上 进行 测评 以 评估 其 
泛化 能力 时 多任务 学习 就 变得 愈加 重要 近年来 
也 涌现 出 更多 针对 多任务 学习 的 评估 基准 
2013年 词 嵌入 通过 稀疏 向量 对 文本 进行 表示 
的 词 袋 模型 在 自然 语言 处理 领域 已经 
有 很长 的 历史 了 而用 稠密 的 向量 对 
词语 进行 描述 也 就是 词 嵌入 则在 2001 年 
首次 出现 2013 年 Mikolov 等人 工作 的 主要 创新 
之 处 在于 通过 去除 隐藏 层 和 近似计算 目标 
使 词 嵌入 模型 的 训练 更为 高效 尽管 这些 
改变 在 本质 上 是 十分 简单 的 但 它们 
与 高效 的 word2vec word to vector 用来 产 生词 
向量 的 相关 模型 组合 在 一起 使得 大 规模 
的 词 嵌入 模型 训练 成为可能 Word2vec 有 两种 不同 
的 实现 方法 CBOW continuous bag of words 和 skip 
gram 它们 在 预测 目标 上 有所不同 一个 是 根据 
周围 的 词语 预测 中心 词语 另 一个 则 恰恰相反 
如图 3 所示 图 3 | CBOW 和 skip gram 
架构 Mikolov et al . 2013a 2013b 虽然 这些 嵌入 
与 使用 前馈 神经 网络 学习 的 嵌入 在 概念 
上 没有 区别 但是 在 一个 非常 大 语料库 上 
的 训练 使 它们 能够 获取 诸如 性别 动词时态 和 
国际 事务 等 单词 之间 的 特定 关系 如 下图 
4 所示 图 4 | word2vec 捕获 的 联系 Mikolov 
et al . 2013a 2013b 这些 关系 和 它们 背后 
的 意义 激起 了 人们 对 词 嵌入 的 兴趣 
许多 研究 都在/nr 关注 这些 线性 关系 的 来源 然而 
使 词 嵌入 成为 目前 自然语言 处理 领域 中流砥柱 的 
是 将 预 训练 的 词 嵌入 矩阵 用于 初始化 
可以 提高 大量 下游 任务 性能 的 事实 虽然 word2vec 
捕捉到 的 关系 具有 直观 且 几乎 不可思议 的 特性 
但 后来 的 研究 表明 word2vec 本身 并 没有 什么 
特殊 之处 词 嵌入 也 可以 通过 矩阵 分解 来 
学习 经过 适当 的 调试 经典 的 矩阵 分解 方法 
SVD 和 LSA 都 可以 获得 相似 的 结果 从那时起 
大量 的 工作 开始 探索 词 嵌入 的 不同 方面 
尽管 有 很多 发展 word2vec 仍然 是 目前 应用 最为 
广泛 的 选择 Word2vec 的 应用 范围 也 超出 了 
词语 级别 带有 负 采样 的 skip gram 一个 基于 
上下文 学习 词 嵌入 的 方便 目标 已经 被 用于 
学习 句子 的 表征 它 甚至 超越 了 自然 语言 
处理 的 范围 被/p 应用/v 于/p 网络/n 和/c 生物/n 序列/n 
等/u 领域/n 一个 激动人心 的 研究 方向 是 在 同一 
空间 中 构建 不同 语言 的 词 嵌入 模型 以 
达到 零 样本 跨语言 转换 的 目的 通过 无 监督 
学习 构建 这样 的 映射 变得 越来越 有 希望 至少 
对于 相似 的 语言 来说 这 也为 语料 资源 较少 
的 语言 和无/nr 监督 机器 翻译 的 应用 程序 创造 
可能 2013年 用于 自然语言 处理 的 神经 网络 2013年 和 
2014 年是/nr 自然 语言 处理 领域 神经 网络 时代 的 
开始 其中 三 种 类型 的 神经 网络 应用 最为 
广泛 循环 神经网络 recurrent neural networks 卷积 神经网络 c o 
n v o l u t i o n a 
l n e u r a l networks 和 结构 
递归 神经网络 recursive neural networks 循环 神经 网络 是 NLP 
领域 处理 动态 输入 序列 最 自然 的 选择 Vanilla 
循环 神经网络 很快 被 经典 的 长短期 记忆 网络 long 
shortterm memory networks LSTM 代替 该 模型 能 更好 地 
解决 梯度 消 失和 梯度 爆炸 问题 在 2013 年 
之前 人们 仍 认为 循环 神经网络 很难 训练 直到 Ilya 
Sutskever 博士 的 论文 改变 了 循环 神经 网络 这 
一名 声 双向 的 长短期 记忆 记忆 网络 通常 被 
用于 同时 处理 出现 在 左侧 和 右侧 的 文本 
内容 LSTM 结构 如图 5 所示 图 5 | LSTM 
网络 来源 ChrisOlah 应用于 文本 的 卷积 神经网络 只在 两个 
维度 上 进行 操作 卷积 层 只 需要 在 时序 
维度 上 移动 即可 图 6 | 卷积 神经网络 Kim 
2014 与 循环 神经网络 相比 卷积 神经 网络 的 一个 
优点 是 具有 更好 的 并行性 因为 卷积 操作 中 
每个 时间 步的/nr 状态 只 依赖于 局部 上下文 而 不是 
循环 神经 网络 中 那样 依赖 于 所有 过去 的 
状态 卷积 神经 网络 可以 使用 更大 的 卷积 层 
涵盖 更 广泛 的 上下文 内容 卷积/n 神经/n 网络/n 也/d 
可以/c 和/c 长短期/l 记忆/n 网络/n 进行/v 组合/v 和/c 堆叠/v 还 
可以 用 来 加速 长短期 记忆 网络 的 训练 循环/vn 
神经/n 网络/n 和/c 卷积/n 神经/n 网络/n 都将/nr 语言/n 视为/v 一个/m 
序列/n 但从 语言学 的 角度 来看 语言 是 具有 层级 
结构 的 词语 组成 高阶 的 短语 和小句/nr 它们 本身 
可以 根据 一定 的 产生 规则 递归 地 组合 这 
激发 了 利用 结构 递归 神经网络 以 树形 结构 取代 
序列 来 表示 语言 的 想法 如图 7 所示 图 
7 | 结构 递归 神经网络 Socher et al . 2013 
结构 递归 神经网络 自下而上 构建 序列 的 表示 与/p 从/p 
左至右/i 或/c 从右/i 至/p 左对/nr 序列/n 进行/v 处理/v 的/uj 循环/vn 
神经网络/n 形成/v 鲜明/a 的/uj 对比/v 树 中的 每个 节点 是 
通过 子 节点 的 表征 计算 得到 的 一个 树 
也 可以 视为 在 循环 神经 网络 上 施加 不同 
的 处理 顺序 所以 长短期 记忆 网络 则 可以 很容易 
地被 扩展 为 一棵树 不 只是 循环 神经 网络 和 
长短期 记忆 网络 可以 扩展 到 使用 层次结构 词 嵌入 
也 可以 在 语法 语境 中 学习 语言 模型 可以 
基于 句法 堆栈 生成 词汇 图形 卷积 神经 网络 可以 
树状 结构 运行 2014年 序 列到 序列 模型 Sequence to 
sequencemodels 2014 年 Sutskever 等人 提出 了 序 列到 序列 
学习 即使 用 神经 网络 将 一个 序列 映射 到 
另一个 序列 的 一般化 框架 在 这个 框架 中 一个 
作为 编码器 的 神经 网络 对 句子 符号 进行 处理 
并 将其 压缩成 向量 表示 然后 一个 作为 解码器 的 
神经 网络 根据 编码器 的 状态 逐个 预测 输出 符号 
并将 前 一个 预测 得到 的 输出 符号 作为 预测 
下 一个 输出 符号 的 输入 如图 8 所示 图 
8 | 序 列到 序列 模型 Sutskever et al . 
2014 机器 翻译 是 这一 框架 的 杀手 级 应用 
2016 年 谷歌 宣布 他们 将 用 神经 机器翻译 模型 
取代 基于 短语 的 整句 机器翻译 模型 谷歌 大脑 负责人 
Jeff Dean 表示 这 意味着 用 500 行 神经网络 模型 
代码 取代 50 万行 基于 短语 的 机器 翻译 代码 
由于 其 灵活性 该 框架 在 自然 语言 生成 任务 
上 被 广泛 应用 其 编码器 和 解码器 分别 由 
不同 的 模型 来 担任 更 重要 的 是 解码器 
不仅 可以 适用 于 序列 在 任意 表示 上 均 
可以 应用 比如 基于 图片 生成 描述 如图 9 基于 
表格 生成 文本 根据 源代码 改变 生成 描述 以及 众多 
其他 应用 图 9 | 基于 图像 生成 标题 Vinyalset 
al . 2015 序 列到 序列 的 学习 甚至 可以 
应用 到 自然 语言 处理 领域 常见 的 结构化 预测 
任务 中 也 就是 输 出 具有 特定 的 结构 
为 简单 起见 输出 就像 选区 解析 一样 被 线性化 
如图 10 在 给定 足够 多 训练 数据 用于 语法 
解析 的 情况 下 神经 网络 已经 被 证明 具有 
产生 线性 输出 和 识别 命名 实体 的 能力 图 
10 | 线性化 选区 解析 树 Vinyalset al . 2015 
序列/n 的/uj 编码器/n 和/c 解码器/n 通常/d 都是/nr 基于/p 循环/vn 神经网络/n 
但 也 可以 使用 其他 模型 新的/i 结构/n 主要/b 都从/nr 
机器/n 翻译/v 的/uj 工作/vn 中/f 诞生/v 它 已经 成了 序 
列到 序列 模型 的 培养基 近期 提出 的 模型 有 
深度 长短期 记忆 网络 卷积 编码器 Transformer 一个 基于 自 
注意力 机制 的 全新 神经 网络 架构 以及 长 短期 
记忆 依赖 网络 和的/nr Transformer 结合体 等 2015年 注意力 机制 
注意力 机制 是 神经 网络 机器翻译 NMT 的 核心 创新 
之一 也是 使 神经 网络 机器翻译 优于 经典 的 基于 
短语 的 机器 翻译 的 关键 序 列到 序列 学习 
的 主要 瓶颈 是 需要 将 源 序列 的 全部 
内容 压缩 为 固定 大小 的 向量 注意力 机制 通过 
让 解码器 回 顾源 序列 的 隐藏 状态 以此为 解码器 
提供 加权 平均值 的 输入 来 缓解 这 一 问题 
如图 11 所示 图 11 | 注意力 机制 Bahdanau et 
al . 2015 之后 各种 形式 的 注意力 机制 涌现 
而出 注意力 机制 被 广泛 接受 在 各种 需要 根据 
输入 的 特定 部分 做出 决策 的 任务 上 都有 
潜在 的 应用 它 已经 被 应用于 句法分析 阅读 理解 
单 样本 学习 等 任务 中 它 的 输入 甚至 
不 需要 是 一个 序列 而 可以 包含 其他 表示 
比如 图像 的 描述 图 12 注意力 机制 一个 有用 
的 附带 作用 是 它 通过 注意力 权重 来 检测 
输入 的 哪 一部分 与 特定 的 输出 相关 从而 
提供 了 一种 罕见 的 虽然 还是 比较 浅 层次 
的 对模型 内部 运作 机制 的 窥探 图 12 | 
图像 描述 模型 中 的 视觉 注意力 机制 指示 在 
生成 飞盘 时所/nr 关注 的 内容 Xu etal . 2015 
注意力 机制 也 不仅仅 局限于 输入 序列 自 注意力 机制 
可以 用来 观察 句子 或 文档 中 周围 的 单词 
获得 包含 更多 上下文 信息 的 词语 表示 多层 的 
自 注意力 机制 是 神经 机器翻译 前沿 模型 Transformer 的 
核心 2015年 基于 记忆 的 神经 网络 注意力 机制 可以 
视为 模糊 记忆 的 一种 形式 其 记忆 的 内容 
包括 模型 之前 的 隐藏 状态 由 模型 选择 从 
记忆 中 检索 哪些 内容 与此同时 更多 具有 明确 记忆 
单元 的 模型 被 提出 他们 有 很多 不同 的 
变化 形式 比如 神经 图灵机 Neural Turing Machines 记忆 网络 
Memory Network 端 到 端的 记忆 网络 End to end 
Memory Newtorks 动态 记忆 网络 DynamicMemory Networks 神经 可微 计算机 
Neural Differentiable Computer 循环 实体 网络 RecurrentEntity Network 记忆 的 
存取 通常 与 注意力 机制 相似 基于 与 当前 状态 
且 可以 读取 和 写入 这些 模型 之间 的 差异 
体现 在 它们 如何 实现 和 利用 存储模块 比如说 端 
到 端的 记忆 网络 对 输入 进行 多次 处理 并 
更新 内存 以 实行 多次 推理 神经 图灵机 也 有 
一个 基于 位置 的 寻址方式 使 它们 可以 学习 简单 
的 计算机程序 比如 排序 基于 记忆 的 模型 通常用于 需要 
长时间 保留 信息 的 任务 中 例如 语言 模型 构建 
和 阅读 理解 记忆 模块 的 概念 非常 通用 知识库/n 
和/c 表格/n 都/d 可以/c 作为/v 记忆/n 模块/n 记忆 模块 也 
可以 基于 输入 的 全部 或 部分 内容 进行 填充 
2018 预 训练 的 语言 模型 预 训练 的 词 
嵌入 与 上下文 无关 仅 用于 初始化 模型 中 的 
第一 层 近 几个月 以来 许多 有 监督 的 任务 
被 用来 预 训练 神经网络 相比之下 语言 模型 只需要 未 
标记 的 文本 因此 其 训练 可以 扩展 到 数十 
亿 单词 的 语料 新的 领域 新的 语言 预 训练 
的 语言 模型 于 2015 年被/nr 首次 提出 但 直到 
最近 它 才被 证明 在 大量 不同 类型 的 任务 
中 均 十分 有效 语言 模型 嵌入 可以 作为 目标 
模型 中 的 特征 或者 根据 具体 任务 进行 调整 
如下 图 所示 语言 模型 嵌入 为 许多 任务 的 
效果 带来 了 巨大 的 改进 图 13 | 改进 
的 语言 模型 嵌入 Peterset al . 2018 使用 预 
训练 的 语言 模型 可以 在 数据 量 十分 少 
的 情况 下 有效 学习 由于 语言 模型 的 训练 
只需要 无 标签 的 数据 因此 他们 对于 数据 稀缺 
的 低 资 源语言 特别 有利 其他 里程碑 一些 其他 
进展 虽 不如 上面 提到 的 那样 流行 但 仍 
产生 了 广泛 的 影响 基于 字符 的 描述 Character 
based representations 在 字符 层级 上 使用 卷积 神经 网络 
和 长短期 记忆 网络 以 获得 一个 基于 字符 的 
词语 描述 目前 已经 相当 常见 了 特别 是 对于 
那些 语言 形态 丰富 的 语种 或 那些 形态 信息 
十分 重要 包含 许多 未知 单词 的 任务 据 目前 
所知 基于 字符 的 描述 最初 用于 序列 标注 现在 
基于 字符 的 描述 方法 减轻 了 必须 以 增加 
计算 成本 为 代价 建立 固定 词汇表 的 问题 并使 
完全 基于 字符 的 机器 翻译 的 应用 成为可能 对抗 
学习 Adversarial learning 对抗 学习 的 方法 在 机器学习 领域 
已经 取得 了 广泛 应用 在 自然 语言 处理 领域 
也 被 应用 于 不同 的 任务 中 对抗 样例 
的 应用 也 日益 广泛 他们 不 仅仅 是 探测 
模型 弱点 的 工具 更 能使 模型 更具 鲁棒性 robust 
虚拟 的 对抗性 训练 也 就是 最坏 情况 的 扰动 
和域/nr 对抗性 损失 domain a d v e r s 
a r i a l l o s s e 
s 都是/nr 可以 使 模型 更具 鲁棒性 的 有效 正则化 
方式 生成 对抗 网络 GANs 目前 在 自然 语言 生成 
任务 上 还 不太 有效 但在 匹配 分布 上 十分 
有用 强化 学习 Reinforcement learning 强化 学习 已经 在 具有 
时间 依赖性 的 任务 上 证明了 它 的 能力 比如 
在 训练 期间 选择 数据 和 对话 建模 在 机器 
翻译 和 概括 任务 中 强化 学习 可以 有效 地 
直接 优化 红色 和 蓝色 这样 不 可微 的 度量 
而 不必 去 优化 像 交叉 熵 这样 的 代理 
损失 函数 同样 逆向 强化 学习 inverse reinforcement learning 在 
类似 视频 故事 描述 这样 的 奖励 机制 非常 复杂 
且 难以 具体化 的 任务 中 也 非常 有用 作者 
| 兰 红云 责编 | 何永灿/nr 自然语言/l 处理/v 和/c 大部分/m 
的/uj 机器/n 学习/v 或者/c 人工智能/n 领域/n 的/uj 技术/n 一样/r 是 
一个 涉及 到 多个 技能 技术 和 领域 的 综合体 
所以 自然 语言 处理 工程师 会有 各种各样 的 背景 大部分 
都是 在 工作 中 自学 或者 是 跟着 项目 一起 
学习 的 这 其中 也 不乏 很多 有 科班 背景 
的 专业 人才 因为 技术 的 发展 实在 是 日新月异 
所以 时刻 要 保持 着 一种 强烈 的 学习 欲望 
让 自己 跟上 时代 和 技术 发展 的 步伐 本文 
作者 从 个人 学习 经历 出发 介绍 相关 经验 一些 
研究者 将 自然 语言 处理 NLP Natural Language Processing 和 
自然 语言 理解 NLU Natural Language Understanding 区分开 在 文章 
中 我们 说 的 NLP 是 包含 两者 的 并 
没有 将 两者 严格 分开 自然语言 处理 学习 路线 数学 
基础 数学 对于 自然 语言 处理 的 重要性 不言而喻 当然 
数学 的 各个 分支 在 自然 语言 处理 的 不同 
阶段 也 会 扮演 不同 的 角色 这里 介绍 几个 
重要 的 分支 代数 代数 作为 计算数学 里面 很 重要 
的 一个 分支 在 自然 语言 处理 中 也有 举足轻重 
的 作用 这 一 部分 需要 重点 关注 矩阵 处理 
相关 的 一些 知识 比如 矩阵 的 SVD QR 分解 
矩阵 逆 的 求解 正定矩阵 稀疏 矩阵 等 特殊 矩阵 
的 一些 处理 方法 和 性质 等等 对于 这 一部分 
的 学习 既 可以 跟着 大学 的 代数 书 一起 
学习 也 可以 跟着 网上 的 各种 公开课 一起 学习 
这里 既 可以 从 国内 的 一些 开放 学习 平台 
上学 也 可以 从 国外 的 一些 开放 学习 平台 
上学 这里 放 一个 学习 的 链接 网易 公开课 的 
链接 https / / c . open . 163 . 
com / search / search . htm query = 线性代数 
# / search / all 其他 的 资料 或者 平台 
也都 OK 概率论 在 很多 的 自然 语言 处理 场景 
中 我们 都是算/nr 一个 事件 发生 的 概率 这 其中 
既有 特定 场景 的 原因 比如 要 推断 一个 拼音 
可能 的 汉字 因为 同音字 的 存在 我们 能 计算 
的 只能 是 这个 拼音 到 各个 相同 发音 的 
汉字 的 条件概率 也有 对 问题 的 抽象 处理 比如 
词性 标注 的 问题 这个 是 因为 我们 没有 很好 
的 工具 或者说 能力 去 精准 地 判断 各个 词 
的 词性 所以 就 构造 了 一个 概率 解决 的 
办法 对于 概率论 的 学习 既要 学习 经典 的 概率 
统计理论 也要 学习 贝叶斯 概率 统计 相对来说 贝叶斯 概率 统计 
可能 更 重要 一些 这个 和 贝叶斯 统计 的 特性 
是 相关 的 因其 提供 了 一种 描述 先验 知识 
的 方法 使得 历史 的 经验 使用 成为 了 可能 
而 历史 在 现实 生活 中 也 确实 是 很 
有用 的 比如 朴素 贝叶斯 模型 隐 马尔卡 模型 最大熵 
模型 这些 我们 在 自然 语言 处理 中 耳熟能详 的 
一些 算法 都是 贝叶斯 模型 的 一种 延伸 和 实例 
这 一部分 的 学习 资料 也 非常 丰富 这里 也 
照例 对 两种 概率 学习 各 放 一个 链接 统计学 
导论 http / / open . 163 . com / 
movie / 2011/5 / M / O / M807PLQMF _ 
M80HQQGMO . html 贝叶斯 统计 https / / www . 
springboard . com / blog / probability bayes theorem data 
science / 信息论 信息论 作为 一种 衡量 样本 纯净度 的 
有效 方法 对于 刻画 两个 元素 之间 的 习惯 搭配 
程度 非常 有效 这个 对于 我们 预测 一个 语素 可能 
的 成分 词性 标注 成分 的 可能 组成 短语 搭配 
非常 有价值 所以 这 一部分 知识 在 自然 语言 处理 
中 也有 非常 重要 的 作用 同时 这 部分 知识 
也 是 很多 机器学习 算法 的 核心 比如 决策树 随机 
森林 等 以 信息熵 作为 决策 桩 的 一些 算法 
对于 这 部分 知识 的 学习 更多 的 是 要 
理解 各个 熵 的 计算 方法 和 优缺点 比如 信息 
增益 和 信息 增益 率 的 区别 以及 各自 在 
业务 场景 中 的 优缺点 照例 放上 一个 链接 http 
/ / open . 163 . com / special / 
opencourse / information . html 数据结构 与 算法 这 部分 
内容 的 重要性 就 不做 赘述 了 学习 了 上面 
的 基础 知识 只是 万里长征 开始 了 第一步 要想 用 
机器 实现 对 自然 语言 的 处理 还是 需要 实现 
对应 的 数据 结构 和 算法 这 一部分 也 算是 
自然语言 处理 工程师 的 一个 看家本领 这 一部分 的 内容 
也 是 比较 多 的 这里 就 做 一个 简单 
的 介绍 和 说明 首先 数据结构 部分 需要 重点 关注 
链表 树 结构 和图/nr 结构 邻接矩阵 包括 各个 结构 的 
构建 操作 优化 以及 各个 结构 在 不同 场景 下 
的 优缺点 当然 大 部分 情况 下 可能 使用 到 
的 数据 结构 都 不是 单一 的 而是 有 多种 
数据结构 组合 比如/v 在/p 分词/n 中有/i 非常/d 优秀/a 表现/v 的/uj 
双/n 数组/n 有限/a 状态机/n 就/d 使用/v 树/v 和/c 链表/n 的/uj 
结构/n 但 是 实现 上 采用 的 是 链表 形式 
提升 了 数据 查询 和 匹配 的 速度 在 熟练 
掌握 各种 数据结构 之后 就是 要 设计 良好 的 算法 
了 伴随 着 大 数据 的 不断 扩张 单机 的 
算法 越来越 难 发挥 价值 所以 多数 场景 下 都要 
研发 并行 的 算法 这 里面 又 涉及 到 一些 
工具 的 应用 也 就是 编程 技术 的 使用 例如/v 
基于/p Hadoop/w 的/uj MapReduce/w 开发/v 和/c Spark/w 开发/v 都是/nr 很好/i 
的/uj 并行/v 化/n 算法/n 开发工具/l 但 是 实现 机制 却 
有 很大 的 差别 同时 编程 的 便利 程度 也 
不 一样 当然 这 里面 没有 绝对 的 孰 好 
孰 坏 更多 的 是 个人 使用 的 习惯 和 
业务 场景 的 不同 而 不同 比如/v 两个/m 都有/nr 比较/d 
成熟/a 的/uj 机器学习/i 库/n 一些 常用 的 机器学习 算法 都 
可以 调用 库函数 实现 编程 语言上 也 都 可以 采用 
Java 不过 Spark 场景 下 使用 Scala 会 更 方便 
一些 因为 这 一部分 是 偏 实 操 的 所以 
我 的 经验 会 建议 实例 学习 的 方法 也 
就是 跟着 具体 的 项目 学习 各种 算法 和 数据结构 
最好能 对 学习 过 的 算法 和 数据 结构 进行 
总结 回顾 这样 可以 更好 的 得到 这种 方法 的 
精髓 因 为 基础 的 元素 包括 数据结构 和 计算 
规则 都是 有限 的 所以 多样 的 算法 更多 的 
是 在 不同 的 场景 下 对于 不同 元素 的 
一个 排列组合 如果 能够 融会贯通 各个 基础 元素 的 原理 
和 使用 不管/c 是/v 对于/p 新/a 知识/v 的/uj 学习/v 还是/c 
对于/p 新/a 解决方案/n 的/uj 构建/v 都是/nr 非常/d 有/v 帮助/v 的/uj 
对于 工具 的 选择 建议 精通 一个 对于 其他 工具 
也 需要 知道 比如 精通 Java 和 MapReduce 对于 Spark 
和 Python 也 需要 熟悉 这样 可以 在 不同 的 
场景 下 使用 不同 的 工具 提升 开发 效率 这 
一部分 实在 是 太多 太 广 这里 不 能 全面 
地 介绍 大家 可以 根据 自己 的 需求 选择 合适 
的 学习 资料 进行 学习 这里 给 出 一个 学习 
基础 算法 包含 排序 图 字符串 处理 等 的 课程 
链接 https / / algs4 . cs . princeton . 
edu / home / 语言学 这 一部分 就 更多 是 
语文 相关 的 知识 比如 一个 句子 的 组成 成分 
包括 主 谓 宾 定 状 补 等 对于 各个 
成分 的 组织 形式 也 是 多种多样 比如 对于 主 
谓 宾 常规 的 顺序 就是 主语 → 谓语 → 
宾语 当然 也会有 宾语 → 主语 → 宾语 饭 我 
吃了 这些 知识 的 积累 有助于 我们 在 模型 构建 
或者 解决 具体 业务 的 时候 能够 事半功倍 因为 这些 
知识 一般 情况 下 如果 要被 机器学习 都是/nr 非常 困难 
的 或者 会 需要 大量 的 学习 素材 或许 在 
现有 的 框架 下 机器 很难 学习 到 如果 把 
这些 知识 作为 先验 知识 融合 到 模型 中 对于 
提升 模型 的 准确度 都是/nr 非常 有价值 的 在 先期 
的 研究 中 基于 规则 的 模型 大部分/m 都是/nr 基于/p 
语言/n 模型/n 的/uj 规则/n 进行/v 研究/vn 和/c 处理/v 的/uj 所以 
这 一部分 的 内容 对于 自然 语言 处理 也 是 
非常 重要 的 但是 这 部分 知识 的 学习 就 
比较 杂 一些 因为 大 部分 的 自然 语言 处理 
工程师 都是 语言学 专业 出身 所以 对于 这 部分 知识 
的 学习 大 部分 情况 都是 靠 碎片化 的 积累 
当然 也 可以 花 一些 精力 系统性 学习 对于 这 
部分 知识 的 学习 个人 建议 可以 根据 具体 的 
业务 场景 进行 学习 比如 在 项目 处理 中 要 
进行 同义词 挖掘 那么 就 可以 跟着 百科 或者 搜索引擎 
学习 同义词 的 定义 同义词 一般 会 有 什么样 的 
形式 怎么 根据 句 子结构 或者 语法结构 判断 两个 词 
是不是 同义词 等等 深度 学习 随着 深度 学习 在 视觉 
和 自然 语言 处理 领域 大 获 成功 特别 是 
随着 AlphaGo 的 成功 深度 学习 在 自然 语言 处理 
中 的 应用 也 越来越 广泛 大家 对于 它 的 
期望 也 越来越 高 所以 对于 这 部分 知识 的 
学习 也 几乎 成为 了 一个 必备 的 环节 实际上 
可能 是 大 部分 情况 不用 深度 学习 的 模型 
也 可以 解决 很多 业务 对于 这 部分 知识 现在/t 
流行/v 的/uj 几种/m 神经/n 网络/n 都是/nr 需要/v 学习/v 和/c 关注/v 
的/uj 特别 是 循环 神经网络 因为 其 在 处理 时序 
数据 上 的 优势 在 自然 语言 处理 领域 尤为 
收到 追捧 这里 包括 单项 RNN 双向 RNN LSTM 等 
形式 同时 新 的 学习 框架 比如 对抗 学习 增强 
学习 对偶 学习 也是 需要 关注 的 其中/r 对抗/v 学习/v 
和/c 对偶/n 学习/v 都/d 可以/c 显著/a 降低/v 对/p 样本/n 的/uj 
需求/v 这个 对于 自然 语言 处理 的 价值 是 非常 
大 的 因为 在 自然 语言 处理 中 很 重要 
的 一个 环节 就是 样本 的 标注 很多/m 模型/n 都是/nr 
严重/a 依赖/v 于/p 样本/n 的/uj 好坏/v 而 随着 人工 成本 
的 上升 数据 标注 的 成本 越来越 高 所以 如果 
能 显著 降低 标注 数据 需求 同时 提升 效果 那将 
是 非常 有 价值 的 现在 还有 一个 事 物 
正在 如火如荼 地 进行 着 就是 知识图谱 知识图谱 的 强大 
这里 就 不再 赘述 对于 这 部分 的 学习 可能 
更多 的 是 要 关注 信息 的 链接 整合 和 
推理 的 技术 不过 这里 的 每 一项 技术 都是/nr 
非常 大 的 一个 领域 所以/c 还是/c 建议/n 从/p 业务/n 
实际/n 需求/v 出发/v 去/v 学习/v 相应/v 的/uj 环节/n 和/c 知识/v 
满足 自己 的 需求 链接 http / / www . 
chinahadoop . cn / course / 918 自然语言 处理 现状 
随着 知识图谱 在 搜索 领域 的 大获 成功 以及 知识图谱 
的 推广 如火如荼 地 进行 中 现在/t 的/uj 自然/d 语言/n 
处理/v 有/v 明显/a 和/c 知识图谱/i 结合/v 的/uj 趋势/n 特别 是 
在 特定 领域 的 客服 系统 构建 中 这种 趋势 
就 更 明显 因为 这些 系统 往往 要 关联 很多 
领域 的 知识 而 这种 知识 的 整合 和 表示 
很 适合 用 知识图谱 来 解决 随着 知识图谱 基础 工程 
技术 的 完善 和 进步 对于 图谱 构建 的 容易 
程度 也 大大 提高 所以 自然 语言 处理 和 知识 
图谱 的 结合 就 越来越 成为 趋势 语义 理解 仍然 
是 自然 语言 处理 中 一个 难过 的 坎 目前 
各项 自然语言 处理 技术 基本 已经 比较 成熟 但是 很多 
技术 的 效果 还 达不到 商用 的 水平 特别是在 语义 
理解 方面 和 商用 还有 比 较大 的 差距 比如 
聊天 机器 人 现在 还 很难 做到 正常 的 聊天 
水平 不过 随着 各 个 研究 机构 和 企业 的 
不断 努力 进步 也是 飞速 的 比如 微软 小冰 一直 
在 不断 的 进步 对于 新 的 深度 学习 框架 
目前 在 自然 语言 处理 中 的 应用 还 有待 
进一步 加深 和 提高 比如 对抗 学习 对偶 学习 等 
虽然 在 图像 处理 领域 得到 了 比较 好 的 
效果 但是 在 自然 语言 处理 领域 的 效果 就 
稍微 差 一些 这 里面 的 原因 是 多样 的 
因为 没有 深入 研究 就 不敢 妄言 目前 人机对话 问答 
系统 语言 翻译 是 自然 语言 处理 中 的 热门 
领域 各 大 公司 都有 了 自己 的 语音 助手 
这 一块 也都 在 投入 大量 的 精力 在做 当然 
这些 上层 的 应用 也都 依赖于 底层 技术 和 模型 
的 进步 所以 对于 底层 技术 的 研究 应该 说 
一直 是 热门 在 未来 一段 时间 应该 也 都 
还是 热门 之前 听 一个 教授 讲过 一个 故事 他 
是 做 parser 的 开始 的 时候 很 火 后来 
一段 时间 因为 整个 自然语言 处理 的 效果 差强人意 所以 
作为 其中 一个 基础 工作 的 parser 就 随之 受到 
冷落 曾经有 段 时间 相关 的 期刊 会议 会员 锐减 
但是 最近 整个 行业 的 升温 这 部分 工作 也 
随之 而 受到 重视 不过 因为 他 一直 坚持 在 
这个 领域 所以 建树 颇丰 最近 也 成为 热门 领域 
和 人物 所以 在 最后 引用 一位 大牛 曾经 说 
过 的话 任何 行业 或者 领域 做到 头部 都 是非 
常有 前途 的 即使 是 打球 玩游戏 大意 个人 经验 
笔者 是 跟着 项目 学习 自然语言 处理 的 非 科班出身 
所以 的 经验 难免会 有 偏颇 说 出来 仅供 大家 
参考 有/v 不足/a 和/c 纰漏/n 的/uj 地方/n 敬请/v 指正/v 知识 
结构 要 做 算法 研究 肯定 需要 一定 的 知识 
积累 对于 知识 积累 这部分 我 的 经验 是 先 
学数学 理论 基础 学 的 顺序 可以 是 代数 → 
概率论 → 随机 过程 当然 这 里面 每一 科 都是 
很大 的 一个 方向 学 的 时候 不必 面面俱到 所有 
都 深入 理解 但是 相对 基础 的 一些 概念 和 
这门 学科 主要 讲 的 是 什么 问题 一定 要 
记住 在 学习 了 一些 基础 数学知识 之后 就 开始 
实现 编写 算法 这里 的 算法 模型 建议 跟着 具体 
的 业务 来 学习 和 实践 比如 可以 先从 识别 
垃圾邮件 这样 的 demo 进行 学习 实验 这样 的 例子 
在 网上 很 容易 找到 但是 找 到 以后 一定 
不要 看看 就 过去 要 一步 一步 改写 拿到 的 
demo 同时 可以 改进 里面 的 参数 或者 实现 方法 
看看 能 不能 达到 更好 的 效果 个人 觉得 学习 
还是 需要 下苦功夫 一步 一步 模仿 然后 改进 才能 深入 
的 掌握 相应 的 内容 对于 学习 的 资料 上学 
时期 的 各个 教程 即可 工具 工欲善其事 必先利其器 所以 好 
的 工具 往往 能 事半功倍 在 工具 的 选择 上 
个人 建议 最高 优先级 的 是 Python 毕竟 其 的 
宣传 口语 是 人生 苦短 请用 Python 第二 优先级 的 
是 Java 基于 Java 可以 和 现有 的 很多 框架 
进行 直接 交互 比如 Hadoop Spark 等等 对于 工具 的 
学习 两者 还是 有 很大 的 差别 的 Python 是 
一个 脚本语言 所以 更多 的 是 跟着 命令 学 也 
就是 要 掌握 你 要 实现 什么 目的 来找 具体 
的 执行 语句 或者 命令 同时 因为 Python 不同 版本 
不同 包 对于 同 一个 功能 的 函数 实现 差别 
也 比较 大 所以 在 学习 的 时候 要多 试验 
求同存异 对于 Java 就要 学习 一些 基础 的 数据结构 然后 
一步 一步 的 去 编写 自己 的 逻辑 对于 Python 
当然 也 可以 按照 这个 思路 Python 本身 也 是 
一个 高级 编程语言 所以 掌握 了 基础 的 数据结构 之后 
也 可以 一步 一步 的 实现 具体 的 功能 但是 
那样 好像 就 失去 了 slogan 的 意义 紧跟 时代 
自然语言 处理 领域 也 算是 一个 知识 密集型 的 行业 
所以 知识 的 更新 迭代 非常 的 快 要 时刻 
关注 行业 领域 的 最新 进展 这个 方面 主要 就是 
看 一些 论文 和 关注 一些 重要 的 会议 对于 
论文 的 获取 Google Scholar arxiv 都是 很好 的 工具 
和 资源 请注意 维护 知识产权 会议 就 更多 了 KDD 
JIST CCKS 等等 作者简介 兰 红云 滴滴 算法 工程师 负责 
算法 策略 相关 工作 主要/b 专注/v 于/p 机器学习/i 和/c 自然/d 
语言/n 处理/v 方向/n 著有 自然语言 处理 技术 入门 与 实践 
本文 来源 公众 号 人工智能 头条 未经 允许 不得 转载 
如何 成为 一名 机器学习 算法 工程师 推荐 系统 工程师 对话 
系统 工程师 数据 科学家 异构 并行计算 工程师 语音识别 工程师 求取 
技术 突破 深度 学习 的 专业 路径 实战 路径 程序员 
的 机器学习 进阶 方法 百度 智能 云 文档 链接   
https / / cloud . baidu . com / doc 
/ SPEECH / index . html1 . 百度 语音合成 概念 
顾名思义 就是 将 你 输入 的 文字 合成语音 例如 from 
aip import AipSpeech 你 的 APPID AK SK APP _ 
ID = 16027154 API _ KEY = 5 a 8 
u 0 a L f 2 x R G R 
M X 3 j b Z 2 V H 0 
SECRET _ KEY = U A a q 1 3 
z 6 D j D 9 Q b j d 
0 6 5 d A h 0 H j b 
q P r z V # 上面 这些 东西 都在/nr 
我们 的 百度 语音 的 应用 列表 中 client = 
AipSpeech APP _ ID API _ KEY SECRET _ KEY 
result = client . synthesis 大 噶 好 吾 系 
渣渣 辉 系 兄弟 就来 砍 我 zh 1 { 
spd 4 vol 5 pit 8 per 4 } # 
识别 征求 返回 语音 二进制 错误 则 返回 dict if 
not isinstance result dict with open audio . mp3 wb 
as f f . write result 百度 语音 生成 参数 
tex 合成 的 文本 使用 UTF 8 编码 注意 文本 
长度 必须 小于 1024 字节         必须 
有 culid 用户 唯一 标识 用来 区分 用户 填写 机器 
的 MAC 地址 或 IMEI 码 长度 60 以内   
  不 必须 有 spd   语速 取值 0 15 
默认 为 5 中 语速           
                    
                不 必须 
有 pit     音调 取值 0 15 默认 为 
5 中 语调       不 必须 有 vol 
音量 取值 0 15 默认 为 5 中 音量   
  不 必须 有 per 发音 人 选择 0 为 
女声 1位 男声 3 为 情感 合成 度 逍遥 4 
为 感情 合成 度 丫丫 默认 为 0     
不 必须 有2./nr 百度 语音识别 概念 同上 就是 将 你 
的 音频 文件 的 内容 读出来 相当于 电子书 import os 
from aip import AipSpeech 你 的 APPID AK SK APP 
_ ID = 16027160 API _ KEY = u z 
x 4 W Z u i m P q b 
E 4 L v x Y c E h i 
SECRET _ KEY = 3 H B y 8 y 
i 1 1 I D 9 T 4 y y 
x k A D u G Y O G y 
a v x P d G client = AipSpeech APP 
_ ID API _ KEY SECRET _ KEY # 语音合成 
通过 语音 生成 文字 def get _ file _ content 
filePath os . system f ffmpeg y i { filePath 
} acodec pcm _ s16le f s16le ac 1 ar 
16000 { filePath } . pcm with open f { 
filePath } . pcm rb as fp return fp . 
read # 识别 本地 文件 ret = client . asr 
get _ file _ content nszm . m4a pcm 16000 
{ dev _ pid 1536 } # 得出 音频文件 中 
的 内容 打印 出来 print ret . get result 0 
3 . 百度 NLP 自然语言 处理 simnet 短 文本 相似 
度 from aip import AipNlp 你 的 APPID AK SK 
APP _ ID = 16027160 API _ KEY = u 
z x 4 W Z u i m P q 
b E 4 L v x Y c E h 
i SECRET _ KEY = 3 H B y 8 
y i 1 1 I D 9 T 4 y 
y x k A D u G Y O G 
y a v x P d G client _ nlp 
= AipNlp APP _ ID API _ KEY SECRET _ 
KEY text = 大 噶 好 吾 系 渣渣 辉 
是 兄弟 就来 砍 我 # 这里 算 出来 的 
是 相似 度 score score = client _ nlp . 
simnet 你 叫 什么 名字 呀 text if score = 
0.58 filename = 执行 某个 函数 os . system filename 
4 . 对接 图灵 机器人 实现 智能 问答 这里 我 
说 一下 我 的 逻辑 我 先 通过 语音 合成 
弄 了 一个 音频 然后 通过 语音 识别 读 取出 
我 音频 的 内容 最后 通过 连接 图灵 机器人 进行 
智能 问答 import os from aip import AipSpeech AipNlp 你 
的 APPID AK SK APP _ ID = 16027160 API 
_ KEY = u z x 4 W Z u 
i m P q b E 4 L v x 
Y c E h i SECRET _ KEY = 3 
H B y 8 y i 1 1 I D 
9 T 4 y y x k A D u 
G Y O G y a v x P d 
G client = AipSpeech APP _ ID API _ KEY 
SECRET _ KEY client _ nlp = AipNlp APP _ 
ID API _ KEY SECRET _ KEY # 语音识别 将 
你 输入 的 文字 转化 为 语音 def AI _ 
voice file result = client . synthesis file zh 1 
{ spd 5 vol 5 pit 5 per 2 } 
if not isinstance result dict with open audio . mp3 
wb as f f . write result return audio . 
mp3 # 语音合成 通过 语音 生成 文字 在 这里 只是 
读出 文字 并 没有 写 出来 下面 这 一步 才是 
将 语音 中的 文字 return 出来 def get _ file 
_ content file os . system f ffmpeg y i 
{ file } acodec pcm _ s16le f s16le ac 
1 ar 16000 { file } . pcm with open 
f { file } . pcm rb as fp return 
fp . read # 返回 的 是 你 语音 中 
的 消息 def voice _ content file result = client 
. asr get _ file _ content file pcm 16000 
{ dev _ pid 1536 } # print result . 
get result 0 return result . get result 0 def 
goto _ tl text uid URL = http / / 
openapi . tuling123 . com / openapi / api / 
v2 import requests data = { perception { inputText { 
text 你 叫 什么 名字 } } userInfo { apiKey 
b e 4 1 c f 8 5 9 6 
a 2 4 a e c 9 5 b 0 
e 8 6 b e 8 9 5 c f 
a 9 userId 123 } } data perception inputText text 
= text data userInfo userId = uid res = requests 
. post URL json = data print res . content 
# print res . text # print res . json 
return res . json . get results 0 . get 
values . get text text = voice _ content nszm 
. m4a # 自然 语言 的 处理 比较 low 版 
# 获取 相似 度 score = client _ nlp . 
simnet 你 叫 什么 名字 text . get score print 
score if score = 0.58 filename = AI _ voice 
我 是 你 爸爸 我 喜欢 你 妈妈 os . 
system filename # 将 我 语音 中 的 内容 识别 
出来 并 进行 返回 answer = goto _ tl text 
XiaoQiang name = AI _ voice answer os . system 
name 转 载于 https / / www . cnblogs . 
com / zty1304368100 / p / 10719949 . html 这个 
自然语言 处理 功能 十分 强大 对 一 语话 可以 进行 
类似 我们 以前 的 分词器 分词 效果 还能 标记 出 
可能 倾向 的 搜索词 . 还有 就是 对 语言 的 
情感 分析 文章 的 标签 分类 等 等在 商业 场合 
应用 都 十分 广泛 的 来看 这个 小 例子 好像 
在 微信 小 程序 有 看到 以上 是 引自 百度 
的 一个 ai 体验 中心 . . . . . 
/ * * * 词 法分析 * / @ Test 
public void lexer { JavaAipNlp aipNlp = new JavaAipNlp HashMap 
String Object options = new HashMap String Object JSONObject lexer 
= aipNlp . lexer 好好学习 天天向上 options System . out 
. println lexer . toString / / { log _ 
id 6 6 4 6 4 2 6 4 1 
9 2 3 0 3 0 2 4 0 text 
好好学习 天天向上 items / / { formal loc _ details 
item 好好 pos d ne basic _ words 好 好 
byte _ length 4 byte _ offset 0 uri } 
/ / { formal loc _ details item 学习 pos 
v ne basic _ words 学习 byte _ length 4 
byte _ offset 4 uri } / / { formal 
loc _ details item 天天向上 pos v ne basic _ 
words 天天 向上 byte _ length 8 byte _ offset 
8 uri } / / { formal loc _ details 
item pos w ne basic _ words byte _ length 
1 byte _ offset 16 uri } } / / 
词 法分析 定制 版 @ Test public void lexerCustom { 
JavaAipNlp aipNlp = new JavaAipNlp HashMap String Object options = 
new HashMap String Object JSONObject lexer = aipNlp . lexer 
广东省 南山区 科苑 北 清华 信息港 options System . out 
. println lexer . toString / / { log _ 
id 2 8 4 1 3 3 6 7 9 
3 0 3 5 2 1 9 0 6 2 
text 广东省 南山区 科苑 北 清华 信息港 items / / 
{ formal loc _ details item 广东省 pos ne LOC 
basic _ words 广东 省 byte _ length 6 byte 
_ offset 0 uri } / / { formal loc 
_ details item 南山区 pos ne LOC basic _ words 
南山 区 byte _ length 6 byte _ offset 6 
uri } / / { formal loc _ details item 
科苑 pos n ne basic _ words 科苑 byte _ 
length 4 byte _ offset 12 uri } / / 
{ formal loc _ details item 北 pos f ne 
basic _ words 北 byte _ length 2 byte _ 
offset 16 uri } / / { formal loc _ 
details item 清华 信息港 pos ne ORG basic _ words 
清华 信息 港 byte _ length 10 byte _ offset 
18 uri } / / { formal loc _ details 
item pos w ne basic _ words byte _ length 
1 byte _ offset 28 uri } } / / 
LOC 地名 / / 依存 法 句法分析 @ Test public 
void depParser { JavaAipNlp aipNlp = new JavaAipNlp HashMap String 
Object options = new HashMap String Object / / 模型 
选择 默认值 为 0 可选 值 mode = 0 对应 
web 模型 mode = 1 对应 query 模型 options . 
put mode 1 JSONObject lexer = aipNlp . depParser 我 
不想 上班 options System . out . println lexer . 
toString / / { log _ id 8 5 6 
0 5 1 5 1 5 7 4 9 5 
5 2 9 0 5 6 text 我 不想 上班 
items / / { head 2 deprel SBV postag n 
id 1 word 我 不想 } / / { head 
0 deprel HED postag v id 2 word 上班 } 
} } / / DNN 语言 模型 / / 中文 
DNN 语言 模型 接口 用于 输出 切 词 结果 并 
给出 每个 词 在 句子 中的 概率值 判断 一句话 是否 
符合 语言表达 习惯 / / ppl float 描述 句子 通顺 
的 值 数值 越低 句子 越 通顺 resp _ sample 
/ / prob float 该词 在 句子 中的 概率值 取值 
范围 0 1 @ Test public void dnnlmCn { / 
/ String words = 我 上下班 飞机 在 河里 漂浮 
String words = 我 爱 生活 JavaAipNlp aipNlp = new 
JavaAipNlp HashMap String Object options = new HashMap String Object 
JSONObject lexer = aipNlp . dnnlmCn words options System . 
out . println lexer . toString / / { log 
_ id 6 3 7 1 3 8 6 9 
9 7 6 7 2 1 3 5 5 7 
1 text 我 上下班 飞机 在 河里 漂浮 items / 
/ { prob 0.0161273 word 我 } / / { 
prob 0.00229803 word 上 } / / { prob 0.00197205 
word 下班 } / / { prob 1.35979 E 5 
word 飞机 } / / { prob 0.0167389 word 在 
} / / { prob 3.04629 E 4 word 河里 
} / / { prob 1.17134 E 4 word 漂浮 
} / / ppl 1077.36 } / / = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = / / { 
log _ id 9 6 2 0 9 5 1 
7 2 6 3 4 7 8 6 7 2 
1 text 我 爱 生活 items / / { prob 
0.0161273 word 我 } / / { prob 0.0125896 word 
爱 } / / { prob 9.05624 E 4 word 
生活 } / / { prob 0.0197345 word } } 
/ / 词义 相似 度 / / 输入 两个 词 
得到 两个 词 的 相似 度 结果 @ Test public 
void wordSimEmbedding { String words1 = 小 / / 最大 
64kb String words2 = 小 JavaAipNlp aipNlp = new JavaAipNlp 
HashMap String Object options = new HashMap String Object / 
/ options . put mode 0 JSONObject lexer = aipNlp 
. wordSimEmbedding words1 words2 options System . out . println 
lexer . toString / / { log _ id 7 
9 5 5 8 0 6 8 3 8 4 
8 6 3 4 6 5 5 9 score 1 
words { word _ 1 小 word _ 2 小 
} } / / score 相似 度 的 分数 1 
为 完全 相似 } / / 短 文本 相似 度 
/ / 输入 两个 短 文本 得到 两个 词 的 
相似 度 结果 @ Test public void simnet { String 
words1 = 立马 String words2 = 马上 JavaAipNlp aipNlp = 
new JavaAipNlp HashMap String Object options = new HashMap String 
Object options . put model CNN JSONObject lexer = aipNlp 
. simnet words1 words2 options System . out . println 
lexer . toString / / { log _ id 5 
6 5 6 5 7 0 8 5 6 8 
7 1 6 3 3 9 0 2 score 0.580114 
texts { text _ 1 立马 text _ 2 马上 
} } } / / 评论 观点 抽取 / / 
评论 观点 抽取 接口 用来 提取 一 条 评论 句子 
的 关注 点 和 评论 观点 并 输出 评论 观点 
标签 及 评论 观点 极性 / * * * Type 
* 1 酒店 2 KTV3 丽人 4 美食 餐饮 5 
旅游 6 健康 7 教育 8 商业 9 房产 10 
汽车 11 生活 12 购物 13 3C * / @ 
Test public void commentTag { JavaAipNlp aipNlp = new JavaAipNlp 
String text = 这家 餐馆 味道 很差 HashMap String Object 
options = new HashMap String Object JSONObject lexer = aipNlp 
. commentTag text ESimnetType . FOOD options System . out 
. println lexer . toString / / { log _ 
id 8 4 5 6 4 5 9 8 6 
5 0 4 7 6 0 4 2 0 1 
items / / { sentiment 0 adj 差劲 prop 味道 
end _ pos 16 abstract 这家 餐馆 span 味道 很差 
\ / span begin _ pos 8 } } String 
hotel = 喜来登 酒店 干净 卫生 JSONObject result = aipNlp 
. commentTag hotel ESimnetType . HOTEL options System . out 
. println result . toString / / prop string 匹 
配上 的 属性 词 / / adj string 匹 配上 
的 描述 词 / / sentiment int 该 情感 搭配 
的 极性 0 表示 消极 1 表示 中性 2 表示 
积极 / / begin _ pos int 该 情感 搭配在 
句子 中 的 开始 位置 干 / / end _ 
pos int 该 情感 搭配在 句子 中 的 结束 位置 
生 / / abstract string 对应 于该/nr 情感 搭配 的 
短句 摘要 / / { log _ id 6 2 
0 6 0 3 0 6 1 9 4 1 
2 7 4 3 2 5 0 items / / 
{ sentiment 2 adj 卫生 prop 干净 end _ pos 
18 abstract 喜来登 酒店 span 干净 卫生 \ / span 
begin _ pos 10 } / / { sentiment 2 
adj 干净 prop 卫生 end _ pos 18 abstract 喜来登 
酒店 span 干净 卫生 \ / span begin _ pos 
10 } } } / * * * 情感 倾向 
分析 * 对 包含 主观 观点 信息 的 文本 进行 
情感 极性 类别 积极 消极 中性 的 判断 并给 出 
相应 的 置信度 * / @ Test public void s 
e n t i m e n t C l 
a s s i f y { JavaAipNlp aipNlp = 
new JavaAipNlp String text = 淘宝 上 很多 假货 HashMap 
String Object options = new HashMap String Object JSONObject lexer 
= aipNlp . s e n t i m e 
n t C l a s s i f y 
text options System . out . println lexer . toString 
/ / { log _ id 4 7 7 4 
6 1 0 2 7 8 7 3 7 8 
8 4 3 3 9 / / text 淘宝 上 
很多 假货 / / items { positive _ prob 0.498948 
sentiment 1 confidence 0.97895 negative _ prob 0.501053 } } 
/ * * * + sentiment 是 number 表示 情感 
极性 分类 结果 0 负向 1 中性 2 正向 + 
confidence 是 number 表示 分类 的 置信度 + positive _ 
prob 是 number 表示 属于 积极 类别 的 概率 + 
negative _ prob 是 number 表示 属于 消极 类别 的 
概率 * / } / * 文章 标签 文章 标签 
服务 能够 针对 网络 各类 媒体 文章 进行 快速 的 
内容 理解 根据 输入 含有 标题 的 文章 输出 多个 
内容 标签 以及 对应 的 置信度 用于 个性化 推荐 相似 
文章 聚合 文 内容 分析 等 场景 * / @ 
Testpublic void keyword { String title = iphone 手机 出现 
白苹果 原因 及 解决 办法 用 苹果 手机 的 可以 
看下 String content = 如果 下面 的 方法 还是 没有 
解决 你 的 问题 建议 来 我们 门店 看下 成都市 
锦江区 红星路 三段 99号 银石 广场 24层 01室 JavaAipNlp aipNlp 
= new JavaAipNlp HashMap String Object options = new HashMap 
String Object JSONObject lexer = aipNlp . keyword title content 
options System . out . println lexer . toString / 
* { log _ id 3 2 7 4 7 
4 6 2 2 5 8 8 4 3 0 
0 3 9 6 items { score 0.99775 tag iphone 
} { score 0.862602 tag 手机 } { score 0.845657 
tag 苹果 } { score 0.837886 tag 苹果公司 } { 
score 0.811601 tag 白苹果 } { score 0.797911 tag 数码 
} } + tag 是 string 关注点 字符串 + score 
是 number 权重 取值 范围 0 ~ 1 * / 
} / * * * 文章 分类 对 文章 按照 
内容 类型 进行 自动 分类 首批 支持 娱乐 体育 科技 
等 26个 主流 内容 类型 为 文章 聚 类 文本 
内容 分析 等 应用 提供 基础 技术 支持 * / 
@ Test public void topic { String title = 欧洲 
冠军杯 足球赛 String content = 欧洲 冠军联赛 是 欧洲 足球 
协会 联盟 主办 的 年度 足球比赛 代表 欧洲 俱乐部 足球 
最高 荣誉 和 水平 被 认为 是 全世界 最 高素质 
+ 最具 影响力 以及 最高 水平 的 俱乐部 赛事 亦 
是 世界 上 奖金 最高 的 足球 赛事 和 体育 
赛事 之一 JavaAipNlp aipNlp = new JavaAipNlp HashMap String Object 
options = new HashMap String Object JSONObject lexer = aipNlp 
. topic title content options System . out . println 
lexer . toString / * * * 返回 参数 说明 
* + lv1 _ tag _ list array of objects 
一级 分类 结果 + lv2 _ tag _ list array 
of objects 二级 分类 结果 实际 返回 参数 * { 
log _ id 6 4 4 0 4 0 1 
2 3 6 1 6 7 7 3 2 8 
5 2 item { lv2 _ tag _ list * 
{ score 0.915631 tag 足球 } * { score 0.803507 
tag 国际足球 } * { score 0.77813 tag 英超 } 
* lv1 _ tag _ list { score 0.830915 tag 
体育 } } } * / } 像 百度 在 
大 数据 人工智能 这 一块 的 业务 做 得 已经 
很 开了 就像 调查 问卷 你 可以 直接 根据 你 
想 调查 的 行业 和 问题 调用 他们 的 接口 
数据 返回 他们 的 真是 调查 结果 给 你 . 
已经 很 厉害 了 只是 这个 需要 一 元 一份 
问卷 看似 很贵 但是 这 是 他们 多年 的 数据 
积累 啊 . 还有 就是 根据 您 的 视频 可以 
提取 视频 里面 精彩 部分 作为 帧 图 或者 缩略图 
等等 很 强大 的 接口 视频 播放 网站 上 那些 
我们 看到 的 电影 的 缩略图 是不是 就是 这样 来 
的 写 在前面 的话 biaji ￣ 3 ￣ bia 叽 
嘎嘎 最 近来 教 大家 写 点 简单 又 迷人 
的 自然 语言 处理 的 代码 不好意思 原谅 我 用词不当 
毕竟 我 是 菜鸟 也 没得 资格 教 别人 the 
main reason is that 我 自己 写了 给 自己 看 
的 你 看 我 就是 这样 一个 正直 又不 爱慕 
虚荣 的 小公主 呢 感觉 自己 萌萌 哒 啊 哈哈哈 
不要脸 也 确实 是 真的 反正 也 没有 啥子 浏览量 
估计 就 是 自己 每天 看 自己 写 的 了 
呢 所以在 我 的 地盘 就 听 我 的 啊 
哈哈哈 这 女 的 一定 是 刚刚 从 精神病 院里 
跑 出来 的 哦 对了 今天 知乎 有个 推送 笑死 
我 了 怎么 在 精神病 院里 证明 自己 不是 神经病 
我 的 天 好想 怒 答 一 波 问这 个人 
的 脑子 一定 刚刚 被 门 挤 了吧 啊哈 假装 
很 正紧 的 正文 其实 个人 觉得 NLP 就是 所谓 
的 nature language processing 在 计算机 领域 入门 还 真的 
是 没有 任何 难度 呢 当然 了 个人 意见 觉得 
你 安装 几个 库 nltk gensim sklearn textblob 什么 standford 
parser 斯坦福 解析器 然后 自己 捣鼓 捣鼓 学 一些 分类 
算法 聚 类 算法 topic model 算法 之后 我 觉得 
你 就 入门 了 就算 你 不学 算法 的 各种 
基本原理 你 知道 怎么 用 你 也 可以 很快 上手 
的 预处理 的话 我们 要 安装 一个 库 我们 就 
用 nltk 来做 预处理 把 textblob 可以 用 来 修改 
一些 拼写 的 错误 但是 呢 感觉 没有 考虑 语境 
所以 有时候 改正 的 效果 其实 并 不好 nltk 的 
处理 效果 本人 觉得 也 就是 那个 样子 啦 不过 
我们 先 学 吧 你 得 会了 才能 评价 不会 
听 别人 说 怎么 滴 那也是 跟 你 没有 半 
毛钱 关系 的 我 一直 用 的 都是 Linux 的 
系统 Ubuntu14 . 04 所以 安装 也 很简单 就是 下面 
这个 样子 的 啦 sudo pip install nltk 如果 你 
是 用 的 是 conda 那个 就 这样 安装 nltk 
这个 库 conda install nltk 这里 我们 先 讨论 英文 
的 文本处理 对于 自然 语言 处理 的话 预处理 其实 就是 
有 那么 几个 固定 的 步骤 分词 英文 的话 可能 
需要 全部 转 换为 小写 去除 标点符号 提取 词干 出去 
不 是 英文 的 单词 出去 特殊 的 符号 修正 
错别字 这篇 写 的 挺好 的 这对/i 英文/nz 和非/nr 英文/nz 
的/uj 处理/v 都在/nr 这里/r 了/ul http / / www . 
spiderpy . cn / blog / detail / 30 一些 
必须 知道 的 基本 概念 在做 预处理 的 时候 我们 
要 知道 一些 基本 的 概念 什么 叫做 分词 什么 
叫做 提取 词干 1 . 分词 Tokenization Token 是 符号 
包括了 单词 还有 标点符号 两种 Tokenization 就是 把 一句话 或者 
一段话 分解成 单个 的 单词 和 标点 I like my 
cat . 这句话 分词 之后 就 变成 了 I like 
my cat . 这样 的 一个 五 元组 注意 最后 
的 标点符号 也是 算 的 2 . 提取 词干 stemming 
在 英文 中 常常 可能会 有 一些 英文 单词 的 
各种 变化 比如 第三人称 的 单数 时态 等等 的 变化 
比如 run 可以 变成 runnIng ran runs 等等 但是 我们 
只要 他们 的 基本 态 就是 run . 这个 就 
叫做 提取 词干 这么做 的 主要 目的 是 用 统一 
的 特征 形式 特征 降 维 以 减少 计算 量 
在 NLTK 中 提供 了 三种 最 常用 的 词干 
提取 器 接口 即 Porter stemmer Lancaster Stemmer 和 Snowball 
Stemmer 抽 取词 的 词干 或 词根 形式 不 一定 
能够 表达 完整 语义 from nltk . stem . porter 
import PorterStemmer porter _ stemmer = PorterStemmer from nltk . 
stem . lancaster import LancasterStemmer lancaster _ stemmer = LancasterStemmer 
from nltk . stem import SnowballStemmer snowball _ stemmer = 
SnowballStemmer english porter _ stemmer . stem maximum u maximum 
lancaster _ stemmer . stem maximum maxim snowball _ stemmer 
. stem maximum u maximum porter _ stemmer . stem 
presumably u presum snowball _ stemmer . stem presumably u 
presum lancaster _ stemmer . stem presumably presum porter _ 
stemmer . stem multiply u multipli snowball _ stemmer . 
stem multiply u multipli lancaster _ stemmer . stem multiply 
multiply porter _ stemmer . stem provision u provis snowball 
_ stemmer . stem provision u provis lancaster _ stemmer 
. stem provision u provid porter _ stemmer . stem 
owed u owe snowball _ stemmer . stem owed u 
owe lancaster _ stemmer . stem owed ow 各有 优劣 
看 具体 文本 情况 对于 分类 聚 类 这样 对于 
特征 词语 的 具体 形态 没有 要求 的 情况 下 
进行 词干 抽取 虽然 抽取 后的/nr 词干 可能 无 实际 
意义 但是 却 会 大大 减少 计算 时间 提高效率 以上 
部分 来自 这篇 博客 https / / zhangmingemma . github 
. io / 2017 / 03/29 / Python + NLTK 
Natural Language Process . html 觉得 写 的 很好 的 
词形 还原 词形 还原 Lemmatization 是 把 任何 形式 的 
词汇 还原 为 一般 形式 能 表达 完整 的 语义 
相对而言 词干 提取 是 简单 的 轻量级 的 词形 归并 
方式 最后 获得 的 结果 为 词干 但是 可能 没有 
实际 的 意义 词形 还原 处理 相对 来说 比较 复杂 
获得 结果 为 词 的 原形 能够 承载 一定 的 
意义 与 词干 的 提取 相比 更 具有 研究 和 
应用 的 价值 比如说 词干 提取 假设 这个词 是 provision 
得到 的 是 provis 这个 没有 什么 实际 的 意义 
不过 在 nltk 中的 Lemmatization 算法 很 鸡肋 基本 可以 
理解 为 只有 复述 还原 为 单数 的 形式 一些 
其他 的 非常 态 的 复数形式 转换 为 单数 的 
形式 也 是 可以 实现 的 但是 形容词 变成 名词 
可能 会 失效 具体 的 例子 如下 所示 from nltk 
. stem import W o r d N e t 
L e m m a t i z e r 
wordnet _ lemmatizer = W o r d N e 
t L e m m a t i z e 
r word = wordnet _ lemmatizer . lemmatize birds bird 
以上 的 例子 也 来自 同一 篇文章 https / / 
zhangmingemma . github . io / 2017 / 03/29 / 
Python + NLTK Natural Language Process . html 基本 操作 
的 代码 针对 这些 基本 操作 我们 给出 了 一些 
预 处理 的 代码 大家 之后 可以 直接 拿来 用 
多 看看 别人 怎么 写 的 自己 再用 那也是 极好 
的 一来 提高效率 而来 哈哈 积累 经验 吧 import nltk 
from nltk . corpus import stopwords from nltk . stem 
. porter import PorterStemmer from nltk . stem import W 
o r d N e t L e m m 
a t i z e r def Preprocessing text text 
= text . lower # 将 所有 的 单词 转换成 
小写字母 for c in string . punctuation text = text 
. replace c # 将 标点符号 转换成 空格 wordList = 
nltk . word _ tokenize text # 分词 filtered = 
w for w in wordList if w not in stopwords 
. words english # 删除 停顿 词 # stem ps 
= PorterStemmer filtered = ps . stem w for w 
in filtered # 提取 词干 wl = W o r 
d N e t L e m m a t 
i z e r filtered = wl . lemmatize w 
for w in filtered # 词形 还原 return . join 
filtered 如果 你 要 修改 一些 拼写 的 错误 的 
话 就用 textblob 这个 包我/nr 现在 只 会用 pip 来 
安装 conda 安装 直接 试 了 一下 没有 成功 所以 
就 只能 用 系统 自带 的 那个 Python 来 运行 
这个 自然 语言 的 脚本 啦 安装 TextBlobsudo pip install 
U textblob 在 使用 的 时候 只 要 事先 引进 
它 就行 也 可以 用 这个 工具 来做 预处理 拼写 
矫正 的 代码 如下 b = TextBlob I havv goood 
speling print b . correct 还 看到 了 一些 比较 
有意思 的 处理 决定 收录 一下 from nltk . corpus 
import stopwords from nltk . stem . porter import PorterStemmer 
from nltk . tokenize import word _ tokenize stopset = 
stopwords . words english + list string . punctuation + 
will also said corpus = all _ docs = vocab 
= set stemmer = PorterStemmer with open filename as f 
try doc = f . read . splitlines doc = 
filter None doc # remove empty string doc = . 
. join doc doc = doc . translate None string 
. punctuation doc = doc . translate None 0123456789 doc 
= doc . decode utf8 . encode utf 8 ignore 
all _ docs . append doc tokens = word _ 
tokenize str doc filtered = for w in tokens w 
= stemmer . stem w . lower if w in 
stopset continue filtered . append w vocab . update filtered 
corpus . append filtered except U n i c o 
d e D e c o d e E r 
r o r print Failed to load f i l 
e n a m e R e f e r 
e n c e h t t p s / 
/ zhangmingemma . github . io / 2017 / 03/29 
/ Python + NLTK Natural Language Process . htmlhttp / 
/ www . voidcn . com / article / p 
kwpvxxsc bch . html 计算机 处理 图像 和 文字 的 
实质 是 在 向量 矩阵 等 基础 上将 其 转化 
为 数字 然后 计算 搜索 的 内容 和库/nr 内容 信息 
的 匹配度 文字 数值 向量 算法 案例 词 编码 N 
gram 权重 TF IDF word2vec sense2vec NLP 常见 任务 自动 
摘要 百度 google 指代 消解 代词 理解 指代 是 什么 
机器翻译 应用面 很广 但 目前 还 不完善 词性 标注 分词 
中文 英文 日文 主题 识别 文本 分类 . . . 
. . . . . . . . . . 
NLP 处理 方法 传统 基于 规则 现代 基于 统计 机器学习 
HMM CRF SVM LDA   CNN . . . . 
. . 规则 隐含 在 模型 参 数里 数据 决定 
结果 上限 算法 将 以多 大程度 接近 结果 上限 词 
权重 词 在 文档 中 的 顺序 没有 被 考虑 
TF IDF   信息检索 Binary weighting 短 文本 相似性 离散 
表示 缺点 词表 维度 随着 语料库 增长 膨胀 n gram 
词 序列 随 语料库 膨胀 更快 数据 稀疏 问题 无法 
衡 量词 向量 之间 的 关系 分布式 表示 用 一个 
词 附近 的 其他 词 来 表示 该词 被 称为 
现代 统计 自然语言 处理 中 最有 创见 的 想法 之一 
共 现 矩阵 主要 发现 主题 用于 主题 模型 如 
LSA 局域 窗 中的 word word 共 现 矩阵 可以 
挖掘 语法 和 语义 信息 共 现 矩阵 存 的 
问题 向量 维度 随着 词典 大小 线性 增长 存储 整个 
词典 的 空间 消耗 非常 大 一些 模型 如 文本 
分类 模型 会 面临 稀疏 性问题 模型 会 欠 稳定 
构造 低维 稠密 向量 作为 词 的 分布式 表示 25 
维 ~ 1000 维 SVD 降 维 文章 目录 论述 
自然语言 处理 的 技术 范畴 一 前言 二 主要 技术 
范畴 1 语音合成 Speech Synthesis 2 语音识别 Speech Recognition 3 
中文 自动 分词 4 词性 标注 5 句法分析 6 文本 
分类 7 文本 挖掘 8 信息 抽取 9 问答 系统 
10 机器翻译 11 文本 情感 分析 12 自动 摘要 13 
文字 蕴涵 三 自然语言 处理 的 难点 1 语言 环境 
复杂 2 文本 结构 形式 多样 3 边界 识别 限制 
4 词义 消 岐 5 指代 消解 四 展望 自然语言 
处理 论述 自然语言 处理 的 技术 范畴 一 前言 本片 
博文 主要 是 介绍 说明 自然语言 处理 的 全貌 一些 
主要 的 技术 范畴 自然语言 处理 NLP 这个 是 一个 
很大 的 话题 它 是 一个 人 机 交互 的 
一个 过程 它 涉及 的 学科 比较 广泛 譬如 如下 
所示 1 语言学 2 计算机科学 提供 模型表示 算法 设计 计算机 
实现 3 当然 还有 数学 以此 来 提供 数学模型 4 
心理学 人类 言语 心理 模型 和 理论 5 哲学 提供 
人类 思维 和 语言 的 更高 层次 理论 6 统计学 
提供 样本数据 的 预测 统计技术 7 电子工程 信息论 基础 和 
语言 型号 处理 技术 8 生物学 人类 言语 行为 机制 
理论 总之 那 涉及 的 学科 范围 广泛 不言而喻 在 
自然 语言 处理 研究 工作 中 是 十分 艰难 的 
博主 现在 也 只是 学习 它 的 一个 小小的 分支 
罢了 看到 此 篇 博文 的 小伙伴 希望能 抛出 你们 
的 建议 和 意见 要 是 如此 博主 甚是 感激 
开心 呀 二 主要 技术 范畴 1 语音合成 Speech Synthesis 
所谓 的 语音 合成 就是指 用 人工 的 方式 产生 
人类 语音 语音 合成器 就是 利用 计算机 系统 作用 在 
语音 合成 上 而 语音 合成器 可以用 软 / 硬件 
实现 文字 转 语音 Text To Speech TTS 系统 则是 
将 一般 语言 的 文字 转换 为 语音 其他 系统 
可以 描绘 语言 符号 的 表示 方式 就像 音标 转换 
至 语音 一样 语音 合成器 的 质量 通常 取决于 人声 
的 相似 度 及 语义 是否 能被 了解 举个 例子 
对于 个 瞎子 看不到 文字 只能 通过 语音 合成器 很 
清楚 的 听到 文字 转换成 语音 的 效果 语音 合成 
的 应用 包括 智能仪表 智能 玩具 电子地图 电子 导游 电子词典 
等 总结 用 大白话 来讲 使用 语音 合成器 可以 实现 
文字 转换 为 语音 音标 转化 为 语音 并且 效果 
如同 非 瞎 看 文字 瞎子 听 语音 同一个 效 
果为 最好 2 语音识别 Speech Recognition 语音识别 Speech Recognition 技术 
也 被 称为 语音 转 文本 识别 Speech to Text 
STT 目标 是 让 计算机 自动 将 人类 的 语音 
内容 转换 为 相应 的 文字 语音 识别 技术 的 
应用 包括 语音拨号 语音 导航 室内 设备 控制 语音 文档 
检索 简单 的 听写 数据录入 等 语音 识别 技术 与 
其他 自然语言 处理 技术 如 机器翻译 及 语音合成 技术相结合 可以 
构建 出 更加 复杂 的 应用 例如 语音 到 语音 
的 翻译 总结 用 大白话 来讲 语音识别 就是 借助 计算机 
工具 来 识别 人类 说 的话 转化 为 可视化 的 
东东 也 就是 文字 啦 3 中文 自动 分词 中文 
自动 分词 指 的 是 使用 计算机 自动 对 中文 
文本 进行 词语 的 切分 就像 英文 那样 使得 中文 
句子 中的 词 之间 有 空格 以 标识 中文 自动 
分词 也是 中文 自然语言 处理 中的 最底层 的 一个 环节 
现有 的 方法 ⊚ 基于 词典 的 匹配 前 向 
最大 匹配 后向 最大 匹配 ⊚ 基于 字 的 标注 
最大熵 模型 条件 随 机场 模型 感知器 模型 ⊚ 其他 
方法 与 词性 标注 结合 与 句法分析 结合 例如 以下 
是 博主 写 的 一个 简单 的 测试代码 author jjk 
datetime 2018 / 11/1 coding utf 8 project name Pycharm 
_ workstation Program function 中文分词 结巴 分词 import jieba # 
导入 结巴 分词 包 import jieba . posseg as pseg 
import time # 时间 time _ Start = time . 
time # f = open t _ with _ splitter 
. txt r # 读取 文本 # string = f 
. read . decode utf 8 string = 中文 自动 
分词 指 的 是 使用 计算机 自动 对 中文 文本 
进行 词语 的 切分 + \ 即 像 英文 那样 
使得 中文 句子 中的 词 之间 有 空格 以 标识 
+ \ 中文 自动 分词 被 认为 是 中文 自然语言 
处理 中 的 一个 最 基本 的 环节 words = 
pseg . cut string # 进行 分词 result = # 
记录 最终 结果 的 变量 for w in words result 
+ = str w . word + / + str 
w . flag # 加 词性 标注 print result # 
输出 结果 f = open result . txt w # 
将 结果 保存 到 另一个 文档 中 f . write 
result f . close time _ Stop = time . 
time print 分词 及 词性 标注 完成 耗时 + str 
time _ Stop time _ Start + 秒 # 输出 
结果 结果 4 词性 标注 词性 标注 Part of Speech 
tagging 或 POS tagging 又称 词类 标注 或者 简称 标注 
是 指在 词性 标记 集 已确定 并且 词典 中 每个 
词 都有 确定 词性 的 基础 上 将 一个 输入 
词串 转换 成 相应 词性 标记 串 的 过程 如上 
3 中文 自动 分词 中举 的 例子 的 结果 所示 
在 汉语 中 因为 汉语 词汇 词性 多变 的 情况 
比较 少见 大多 词语 只 有一个 词性 或者 出现 次 
最高 的 词性 远远 高于 第二位 的 词性 相对 比较 
简单 同时 它 也 受到 一些 条件 约束 比如 兼类 
词 在 具体 语境 中的 词性 判定 问题 未 登录 
词 即 新词 词 性问题 兼类 词 问题 等 词性 
标注 方法 包括 概率 方法 隐 马尔可夫 模型 的 词性 
标注 方法 机器学习 规则 的 方法 等 5 句法分析 句法分析 
句法分析 Parsing 就是指 对 句子 中 的 词语 语法 功能 
进行 分析 比如 欢迎 大家 使用 演示 平台 就 可以 
表示 为 欢迎 \ VV 大家 \ PN 使用 \ 
VV 演示 \ NN 平台 \ NN 句法分析 在 中文 
信息 处理 中 的 主要 应用 包括 机器翻译 命名 实体 
识别 等 自然语言 生成 自然语言 生成 研究 使 计算机 具有 
人 一样 的 表达 和 写作 功能 即 能够 根据 
一些 关键 信息 及其 在 机器 内部 的 表达 形式 
经过 一个 规划 过程 自动 生成 一段 高 质量 的 
自然 语言 文本 自然语言 处理 包括 自然 语言 理解 和 
自然 语言 生成 自然语言 生成 是 人工 智能 和 计算 
语言学 的 分支 相应 的 语言 生成 系统 是 基于 
语言 信息 处理 的 计算机 模型 其 工作 过程 与 
自然 语言 分析 相反 从 抽象 的 概念 层次 开始 
通过 选择 并 执行 一定 的 语义 和 语法 规则 
来 生成 文本 6 文本 分类 文本 分类 用 计算机 
对 文本 集 按照 一定 的 分类器 模型 进行 自动 
分类 标记 文本 分类 的 总体 过程 如下 引用 自 
NLPIR 汉语分词 系统 1 预处理 将 原始 语料 格式 化为 
同一 格式 便于 后续 的 统一 处理 2 索引 将 
文档 分解 为 基本 处理单元 同时 降低 后续 处理 的 
开销 3 统计 词频 统计 项 单词 概念 与 分类 
的 相关 概率 4 特征 抽取 从 文档 中 抽取 
出 反映 文档 主题 的 特征 5 分类器 分类器 的 
训练 6 评价 分类器 的 测试 结果 分析 文本 分类 
常用 算法 包括 决策树 朴素 贝叶斯 神经网络 支持 向量 机 
线性 最小 平方 拟合 KNN 遗传算法 最大熵 等 广泛 应用于 
垃圾 过滤 新闻 分类 词性 标注 等 7 文本 挖掘 
文本 挖掘 一般 指在 文本处理 过程 中 产生 高 质量 
的 信息 高/a 质量/n 的/uj 信息/n 通常/d 通过/p 分类/n 和/c 
预测/vn 来/v 产生/n 如 模式识别 文本 挖掘 通常 涉及 输入 
文本 的 处理 过程 产生 结构化 数据 并 最终 评价 
和 解释 输出 例如 博主 的 这篇文章 中 对 微信 
朋友圈 个性 签名 生成 词 云的/nr 分析 就是 一个 文本 
挖掘 典型 的 文本 挖掘 方法 包括 文本 分类 文本 
聚 类 信息 抽取 概念 / 实体 挖掘 情感 分析 
和 观点 分析 等 8 信息 抽取 信息 抽取 Information 
Extraction 是从 大量 文字 数据 中 自动 为 访问 数据库 
而 抽取 特定 消息 的 技术 简单 点 来说 从 
给定 文本 中 抽取 重要 的 信息 比如 时间 地点 
人物 事件 原因 结果 数字 日期 货币 专有名词 等 大白话 
就是 就是 要 了解 谁在 什么 时候 什么原因 对谁 做 
了 什么 事 有 什么 结果 涉及 实体 识别 时间 
抽取 因果关系 抽取 等 关键 技术 9 问答 系统 问答 
系统 Question Answering 是 当下 自然语言 处理 研究 的 热点 
也是 未来 自然语言 处理 的 重点 问题 从 问答 系统 
的 外部 行为 来看 其 与 目前 主流 资讯 检索 
技术 有 两点 不同 首先 是 查询 方式 为 完整 
而 口语化 的 问句 再者 是 其 回传 的 为高 
精准度 网页 结果 或 明确 的 答案 字串 至此 不 
知道 小伙伴 你 有 没有 想到 聊天 机器人 呀 10 
机器翻译 机器翻译 Machine Translation 经常 简写 为 MT 属于 计算 
语言学 的 范畴 是/v 计算机/n 程序/n 将/d 文字/n 或/c 演说/v 
从/p 一种/m 自然/d 语言/n 翻译/v 成/n 另一种/i 自然语言/l 简单 来说 
机器 翻译 是 通过 将 一个 自然 语言 的 字 
辞 取代 成另/nr 一个 语言 的 字 辞 来 实现 
的 借 由 使用 语料库 的 技术 可 达成 更加 
复杂 的 自动 翻译 包阔可/nr 更佳 地 处理 不同 的 
文法 结构 辞汇 辨识 惯用语 的 对应 等 这里 用 
博主 自己 的 大白话 的 理解 就是 将 一种 语言 
比如 中文 翻译成 11 文本 情感 分析 文本 情感 分析 
也 称为 意见 挖掘 是 指用 自然语言 处理 文本 挖掘 
及 计算机 语言学 等 方法 来 识别 和 提取 原 
素材 中 的 主观 信息 通常 来说 情感 分析 的 
目的 是 为了 找出 说话者 / 作者 在 某些 话题 
上 或者 针对 一个 文本 两极 的 观点 的 态度 
这个 态度 或 许是 他 的 个人 判断 或 评估 
或许 是 他 当时 的 情感 状态 也 就是说 作者 
在 做出 这个 言论 时的/nr 情绪 状态 或是 作者 有 
意向 的 情感 交流 就是 作者 想要 读者 所 体验 
的 情绪 等 总结 就是 作者 规定 一些 代表 文本 
的 态度 词 然后 使用 可视化 进行 表现出来 从而 达到 
客户 情感 交流 12 自动 摘要 所谓 自动 摘要 就是 
利用 计算机 自动 地 从 原始 文献 中 提取 文摘 
文摘 是 全面 准确 地 反映 某一 文献 中心 内容 
的 连贯 短文 常用 方法 是 自动 摘 要将 文本 
作为 句子 的 线性 序列 将 句子 视为 词 的 
线性 序列 自动 摘要 可以 按照 技术 类型 和 信息 
提取 分类 ⊚ 技术 应用 类型 自动 提取 给定 文章 
的 摘要 信息 自动 计算 文章 中词 的 权重 自动 
计算 文章 中 句子 的 权重 ⊚ 信息提取 单篇/nr 文章 
的 摘要 自动 提取 大 规模 文档 的 摘要 自动 
提取 基于 分类 的 摘要 自动 提取 举例 如下 所示 
author jjk datetime 2018 / 10/15 coding utf 8 project 
name Pycharm _ workstation Program function 查找 关键词 思路 1 
加载 已有 的 文档 数据集 2 加载 停用 词表 3 
对 数据 集中 的 文档 进行 分词 4 根据 停用 
词表 过来 干扰 词 5 根据 数据集 训练 算法 import 
math import jieba import jieba . posseg as psg from 
gensim import corpora models from jieba import analyse import functools 
import numpy as np # 停用词 加载 方法 def get 
_ stopword _ list stop _ word _ path = 
. / data / stopword . txt # 遍历 txt 
文档 剔除 stopword _ list = sw . replace \ 
n for sw in open stop _ word _ path 
encoding = utf 8 . readlines return stopword _ list 
# 分词 方法 调用 结巴 接口 # pos 是 判断 
是否 采用 词性 标注 的 参数 def seg _ to 
_ list sentence pos = False if not pos # 
不 进行 词性 标注 的 分词 方法 seg _ list 
= jieba . cut sentence else # 进行 词性 标注 
的 分词 方法 seg _ list = psg . cut 
sentence return seg _ list # 去除 干扰 词 根据 
pos 判断 是否 过 滤除 名词 外 的 其他 词性 
再 判断 词 是否 在 停用词 表中 长度 是否 大于 
等于 2 等 def word _ filter seg _ list 
pos = False stopword _ list = get _ stopword 
_ list filter _ list = # 根据 pos 参数 
选择 是否 词性 过滤 # 不 进行 词性 过滤 则将 
词性 都 标记 为 n 表示 全部 保留 for seg 
in seg _ list if not pos word = seg 
flag = n else word = seg . word flag 
= seg . flag if not flag . startswith n 
continue # 过滤 高 停用 词表 中的 词 以及 长度 
为 2 的 词 if not word in stopword _ 
list and len word 1 filter _ list . append 
word return filter _ list # 数据 加载 # corpus 
. txt 为 数据集 def load _ data pos = 
False corpus _ path = . / data / corpus 
. txt # 调用 上面 方式 对 数据 集 进行 
处理 处理 之后 的 数据 集 仅 保留 非 干扰 
词 doc _ list = for line in open corpus 
_ path rb content = line . strip seg _ 
list = seg _ to _ list content pos filter 
_ list = word _ filter seg _ list pos 
doc _ list . append filter _ list return doc 
_ list # idf 值 统计 方法 def train _ 
idf doc _ list idf _ dic = { } 
# 总 文档 数 tt _ count = len doc 
_ list # 每个 词 出现 的 文档 数 for 
doc in doc _ list for word in set doc 
idf _ dic word = idf _ dic . get 
word 0.0 + 1.0 # 按 公式 转换 为 idf 
值 分母 加 1 进行 平滑 处理 for k v 
in idf _ dic . items idf _ dic k 
= math . log tt _ count / 1.0 + 
v # 对于 没有 在 字典 中的 词 默认 其 
尽 在 一个 文档 出现 得到 默认 idf 值 default 
_ idf = math . log tt _ count / 
1.0 return idf _ dic default _ idf # topK 
# cmp 函数 是 为了 输出 top 关键词 时 先 
按照 关键词 的 计算 分值 排序 在 得分 相 同时 
根据 关键词 进行 排 序时 def cmp e1 e2 # 
import numpy as np res = np . sign e1 
1 e2 1 if res = 0 return res else 
a = e1 0 + e2 0 b = e2 
0 + e1 0 if a b return 1 elif 
a = = b return 0 else return 1 # 
TF IDF 类 class TfIdf object # 训 练好 的 
idf 字典 默认 idf 值 处理 后的待/nr 提取 文本 关键词 
数量 def _ _ init _ _ self idf _ 
dic default _ idf word _ list keyword _ num 
self . word _ list = word _ list self 
. idf _ dic self . default _ idf = 
idf _ dic default _ idf self . tf _ 
dic = self . get _ tf _ dic self 
. keyword _ num = keyword _ num # 统计 
tf 值 def get _ tf _ dic self tf 
_ dic = { } for word in self . 
word _ list tf _ dic word = tf _ 
dic . get word 0.0 + 1.0 tt _ count 
= len self . word _ list for k v 
in tf _ dic . items tf _ dic k 
= float v / tt _ count return tf _ 
dic # 按 公式 计算 tf idf def get _ 
tfidf self tfidf _ dic = { } for word 
in self . word _ list idf = self . 
idf _ dic . get word self . default _ 
idf tf = self . tf _ dic . get 
word 0 tfidf = tf * idf tfidf _ dic 
word = tfidf # 根据 tf idf 排序 取 排名 
前 keyword _ num 的 词 作为 关键词 for k 
v in sorted tfidf _ dic . items key = 
functools . cmp _ to _ key cmp reverse = 
True self . keyword _ num print k + / 
end = print # 主题 模型 class TopicModel object # 
def _ _ init _ _ self doc _ list 
keyword _ num model = LSI num _ topics = 
4 # 使用 gensim 接口 将 文本 转为 向量 化 
表示 self . dictionary = corpora . Dictionary doc _ 
list # 使用 BOW 模型 向 量化 corpus = self 
. dictionary . doc2bow doc for doc in doc _ 
list # 对 每个 词 根据 tf idf 进行 加权 
得到 加权 后的/nr 向量 表示 self . tfidf _ model 
= models . TfidfModel corpus self . corpus _ tfidf 
= self . tfidf _ model corpus self . keyword 
_ num = keyword _ num self . num _ 
topics = num _ topics # 选择 加载 的 模型 
if model = = LSI self . model = self 
. train _ lsi else self . model = self 
. train _ lda # 得到 数据集 的 主题 词 
分布 word _ dic = self . word _ dictionary 
doc _ list self . wordtopic _ dic = self 
. get _ wordtopic word _ dic def train _ 
lsi self lsi = models . LsiModel self . corpus 
_ tfidf id2word = self . dictionary num _ topics 
= self . num _ topics return lsi def train 
_ lda self lda = models . LdaModel self . 
corpus _ tfidf id2word = self . dictionary num _ 
topics = self . num _ topics return lda def 
get _ wordtopic self word _ dic wordtopic _ dic 
= { } for word in word _ dic single 
_ list = word wordcorpus = self . tfidf _ 
model self . dictionary . doc2bow single _ list wordtopic 
= self . model wordcorpus wordtopic _ dic word = 
wordtopic return wordtopic _ dic # 词/n 空间/n 构建/v 方法/n 
和向/nr 量化/v 方法/n 在 没有 gensim 接口 时的/nr 一般 处理 
方法 def word _ dictionary self doc _ list dictionary 
= for doc in doc _ list dictionary . extend 
doc dictionary = list set dictionary return dictionary def doc2bowvec 
self word _ list vec _ list = 1 if 
word in word _ list else 0 for word in 
self . dictionary return vec _ list # 计算 词 
的 分布 和 文档 的 分布 的 相似 度 取 
相似 度 最高 的 keyword _ num 个 词 作为 
关键词 def get _ simword self word _ list sentcorpus 
= self . tfidf _ model self . dictionary . 
doc2bow word _ list senttopic = self . model sentcorpus 
# 余弦 相似 度 计算 def calsim l1 l2 a 
b c = 0.0 0.0 0.0 for t1 t2 in 
zip l1 l2 x1 = t1 1 x2 = t2 
1 a + = x1 * x1 b + = 
x1 * x1 c + = x2 * x2 sim 
= a / math . sqrt b * c if 
not b * c = = 0.0 else 0.0 return 
sim # 计算 输入 文本 和 每个 词 的 主题 
分布 相似 度 sim _ dic = { } for 
k v in self . wordtopic _ dic . items 
if k not in word _ list continue sim = 
calsim v senttopic sim _ dic k = sim for 
k v in sorted sim _ dic . items key 
= functools . cmp _ to _ key cmp reverse 
= True self . keyword _ num print k + 
/ end = print def tfidf _ extract word _ 
list pos = False keyword _ num = 10 doc 
_ list = load _ data pos idf _ dic 
default _ idf = train _ idf doc _ list 
tfidf _ model = TfIdf idf _ dic default _ 
idf word _ list keyword _ num tfidf _ model 
. get _ tfidf def textrank _ extract text pos 
= False keyword _ num = 10 textrank = analyse 
. textrank keywords = textrank text keyword _ num # 
输出 抽 取出 的 关键词 for keyword in keywords print 
keyword + / # print def topic _ extract word 
_ list model pos = False keyword _ num = 
10 doc _ list = load _ data pos topic 
_ model = TopicModel doc _ list keyword _ num 
model = model topic _ model . get _ simword 
word _ list if _ _ name _ _ = 
= _ _ main _ _ # 获取 测试 文本 
text1 = test . txt text = open text1 encoding 
= utf 8 . read print text pos = True 
seg _ list = seg _ to _ list text 
pos filter _ list = word _ filter seg _ 
list pos print \ nTF IDF 模型 结果 tfidf _ 
extract filter _ list print \ nTextRank 模型 结果 textrank 
_ extract text print \ nLSI 模型 结果 topic _ 
extract filter _ list LSI pos print \ nLDA 模型 
结果 topic _ extract filter _ list LDA pos 结果 
13 文字 蕴涵 文字 蕴涵 Textual Entailment TE 文字 蕴涵 
在 自然 语言 处理 中 主要 指 一个 文字 片段 
之间 的 定向 关系 ⊚ 正向 蕴涵 文本 T 日本 
时间 2011 年3/nr 日 11 日 日本 宫城县 发生 里氏震级 
9.0 震 造成 死伤 失踪 约 3 万 多人 假设 
H 日本 时间 2011 年3/nr 日 11 日 日本 宫城县 
发生 里氏震级 9.0 强震 ⊚ 矛盾 蕴涵 文本 T 张学友 
在 1961 年7/nr 月 10 日 生于 香港 祖籍 天津 
假设 H 张学友 生于 1960 年 ⊚ 独立 蕴涵 文本 
T 黎姿 与 残障 富豪 马廷强/nr 结婚/v 假设 H 马廷强/nr 
为/p 香港/ns 东方 报业 集团 创办人 之一 马惜如/nr 之子 三 
自然语言 处理 的 难点 1 语言 环境 复杂 自然语言 处理 
的 语言 环境 较为 复杂 以 命名 实体 识别 进行 
分析 对于 同一 个 汉字 某些 情况下 可以 看作 实体 
处理 某些 情况 则 不能 看作 实体 例如 天龙八部 中的 
竹剑 小姐姐 在 有些 情况 下 可能 就是 指 的 
是 竹子 做得 剑 还有 湖北 有 可能 指定 是 
地点 湖北 也 有可能 指定 是 湖 的 北边 可见 
字 自然语言 处理 过程 中 语言 环境 根据 上下文 才能 
究其 表达 的 意思 的 复杂 2 文本 结构 形式 
多样 文本 内部 结构 形式 多样 还是 以 自然 语言 
处理 中 的 命名 实体 识别 任务 为 例子 例如 
⊚ 人名 人名 由姓 和名/nr 构成 其中 姓氏 包括 单姓 
和 复姓 如 赵 钱 孙 李 慕容 东方 西门等/nr 
名 由 若干 个 汉字 组成 姓氏 的 用字 范围 
相对 有限 比较 容易 识别 然而 名 就 比较 灵活 
既 可以 用 名 字 号 表示 也 可以 使用 
职务 名 和 用典 比如 李白 李 十二 李翰林 李 
供奉 李 拾遗 李太白 青莲居士 谪 仙人 都是 同 一个 
人 ⊚ 地名 一般 由 若干 个字 组成 地名 可以为 
作为 后缀 关键字 或者 别名 都是 指代 一个 地方 比如 
成都 蓉城 锦城 芙蓉城 锦官城 天府之国 其中 蓉城 锦城 芙蓉城 
锦官城 天府之国 为 别名 除了 全称 的 名称 还有 地理位置 
代表 地名 的 比如 河南 河南省 豫 都是 指 的 
一个 省份 其中 豫 是 简称 ⊚ 组织 机构 名 
组织 机构 命名 方式 比较 复杂 有些 是 修饰 性 
的 命名 有些 表示 历史 典故 有些 表示 地理 方位 
有些 表示 地名 有些 表示 风俗 习惯 和 关键字 等 
例如 组织 名 广州 恒大 淘宝 足球 俱乐部 中 广州 
表示 地名 的 成分 恒大 淘宝 表示 公司 名称 成分 
足球 是 一项 体育赛事 成分 俱乐部 是 关键字 的 成分 
比如 四川 大学 附属中学 四川省 成都市 第 十二 中学 中 
包括 另 一个 机构 名 四川大学 机构 名 还 可以 
以 简称 形式 表示 比如 四川 大学 附属中学 简称 川大 
附中 成都 信息 工程 大学 简称 成信大/nr 3 边界 识别 
限制 在 自然 语言 处理 任务 中 边界 识别 最 
广泛 应用于 命名 识别 当中 边界 识别 可以 分解 为 
两 大 任务 如何 去 识别 实体 的 边界 如何 
去 判定 实体 的 类别 诸如 人名 地名 机构 名 
中文 命名 实体 识别 要比 英文 命名 实体 识别 更为 
复杂 一是 受 中文 自身 语言 特性 的 限制 不同于 
英语 文本 中词 间有 空格 界定 二 是 英文 中的 
实体 一般 首字母 大写 容易 区分 例如 Jobs wasadopted at 
birth in San Francisco and raised in a hotbed of 
counterculture 中 人名 乔布斯 Jobs 的 首字母 大写 地名 旧金山 
San Francisco 的 首字母 也是 大写 而 中文 不具备 这样 
的 特征 4 词义 消 岐 词义 消 歧 词义 
消 歧 是 一个 自然 语言 处理 和 本体论 的 
开放 问题 歧义 与 消 歧 是 自然 语言 理解 
中 最 核心 的 问题 在 词义 句 义 篇章 
含义 层次 都会 出现 语言 根据 上下文 语义 而 产生 
不同 含义 的 现象 消 歧 即指 根据 上下文 确定 
对象 语义 的 过程 词义 消 歧 即在 词语 层次 
上 的 语义 消 歧 语义 消 歧 / 词义 
消 歧 是 自然 语言 处理 任务 的 一个 核心 
与 难点 影响 了 几乎 所有 任务 的 性能 比如 
搜索引擎 意见 挖掘 文本 理解 与 产生 推理 等 词性 
标注 和 词义 消 岐 词性 标注 与 词义 消 
歧 是 相互 关联 的 两个 问题 在 语言 使用者 
身上 它们 往往 同时 能 得到 满足 但是 目前 的 
计算机 系统 一般 并 不能 让 二者 共用 参数 并 
同时 输出 语义 理解 包括 分词 词性 标注 词义 消 
歧 句法 解析 语义 解析 等 它们 并 不是 前馈 
的 是 相互 依赖 并存在 反馈 的 词性 标注 与 
语义 消 歧 都要 依赖 上下文 来 标注 但是 词性 
标注 比 语义 消 歧 处理 起来 要 更简单 最终 
结果 也 往往 较好 主要 原因 是 词性 标注 的 
标注 集合 是 确定 的 而 语义 消 歧 并 
没有 并且 量级 上 词性 标注 要 大得多 词性 标注 
的 上下文 依赖 比 语义 消 歧 要 短 举例说明 
许 多字词 不单 只有 一个 意思 因而 我们 必须 选 
出使 句 意 最为 通顺 的 解释 看 下面 歧义 
的 句子 词义 消 歧 就是 要 分析 出 特定 
上下文 的 词 被 赋予 的 到底 是 哪个 意思 
1 川 大学生 上网 成瘾 如 患 绝症 歧义 在于 
川 大学生 四川 大学 的 学生 四川 的 大学生 2 
两代 教授 人格 不同 歧义 两代 两位 代理 教授 两个 
时代 的 教授 3 被控 私分 国有 资产 专家 总经理 
成了 被告人 歧义 专家 总经理 专家 和 总经理 有 专家 
身份 的 总经理 4 新生 市场 苦熬 淡季 歧义 新生 
新 学生 的 市场 新 产生 的 市场 5 朝鲜 
十年 走近 国际 社会 一步 歧义 十年 走近 国际 社会 
一步 每十年 就向 国际 社会 走 近 一步 最近 十 
年间 向 国际 社会 走近 了 一步 6 新 汽车 
牌照 歧义 新 新的 汽车 新的 牌照 7 咬死 了 
猎人 的 狗 歧义 猎人 的 狗 被 咬死 了 
把 猎人 咬死 了 的 那条 狗 8 菜 不热 
了 歧义 热 指 菜 凉了 指 菜 不 加热 
了 9 还 欠款 四万元 歧义 还 读 huai 读 
hai 10 北京 人多 歧义 北京 / 人多 北京人 / 
多 5 指代 消解 定义 指代 消解 Anaphora Resolution 是 
自然 语言 处理 的 重要 内容 在 信息 抽取 时 
就用 到了 指代 消解 技术 中文 的 三种 典型 指代 
1 人称代词 李明 怕 高 妈妈 一个 人 呆在 家里 
寂寞 他 便将 家里 的 电视 搬了 过来 2 指示代词 
很多 人 都想 创造 一个 美好 的 世界 留给 孩子 
这 可以理解 但 不 完全 正确 3 有定/nr 描述 贸易 
制裁 似乎 成了/nr 美国 政府 在 对华关系 中 惯用 的 
大棒 然而 这 大棒 果真 如 美国 政府 所 希望 
的 那样 灵验 吗 典型 指代 消解 ⊚ 显性 代词 
消解 所谓 显性 代词 消解 就是 指在 篇章 中 确定 
显性 代词 指向 哪个 名词 短语 的 问题 代词 称为 
指示 语 或 照应 语 Anaphor 其所 指向 的 名词 
短语 一般 被 称为 先 行语 Antecedent 根据 二者 之间 
的 先后 位置 可分为 回 指 Anaphora 与 预 指 
Cataphora 其中 如果 先 行语 出现 在 指示 语 之前 
则 称为 回 指 反之 则 称为 预 指 ⊚ 
零 代词 消解 所谓 零 代词 消解 是 代词 消解 
中 针对 零 指代 Zero Anaphora 现象 的 一类 特殊 
的 消解 ⊚ 共 指 消解 所谓 共 指 消解 
是 将 篇章 中 指向 同一 现实 世界 客观 实体 
Entity 的 词语 划分 到 同一个 等价 集 的 过程 
其中 被 划分 的 词语 称为 表述 或 指称 语 
Mention 形成 的 等价 集 称为 共 指 链 Coreference 
Chain 在 共 指 消解 中 指称 语 包含 普通名词 
专有 名词 和 代词 因此 可以 将 显性 代词 消解 
看作 共 指 消解 针对 代词 的 子 问题 共 
指 消解 与 显性 代词 消解 不同 它 更 关注 
在 指称 语 集合 上 进行 的 等价 划分 评测 
方法 与 显性 代词 消解 也 不尽 相同 通常 使用 
MUC B CUBED CEAF 和 BLANC 等 评价 方法 指代 
消解 的 研究 方法 大致 可以 分为 基于 启发式 规则 
的 基于 统计 的 和 基于 深度 学习 的 方法 
目前 看来 基于 有 监督 统计 机器 学习 的 消解 
算法 仍然 是 主流 算法 典型 例子 指代 消解 是 
解决 谁 对谁 做 了 什么 处理 如上所述 的 自然 
语言 的 问题 下面 看看 例子 1 美国 政府 表示 
仍然 支持 强势 美元 但这 到底 只是 嘴 上 说说 
还是 要 采取 果断 措施 经济学家 对此 的 看法 是 
否定 的 2 今天 老师 又在 班会 上 表扬 了 
自己 但是 我 觉得 还 需要 继续 努力 3 三 
妹 拉着 葛姐的/nr 手 说 她 老家 在 偏远 的 
山区 因为 和 家里 赌气 才 跑到 北京 打工 的 
接着 她 又 哭泣 起 自己 的 遭遇 来 4 
当 他 把 证书 发给 小钱 时 他 对 他 
笑了 5 小 明和 肖华 去 公园 玩 他 摔了一跤 
他 急忙 把 他 扶起来 6 星期天 小雨/nr 和/c 小英/nr 
到/v 田/n 老师/n 家/q 补习功课/n 她 一早 就 打电话 给 
她 约 好在 红旗 饭店 吃早餐 四 展望 自然语言 处理 
关于 在 2017年 第三 届 中国 人工智能 大会 上 来自 
哈尔滨 工业 大学 的 刘挺/nr 教授 对 自然 语言 处理 
的 一个 发展 趋势 的 一个 总结 归纳 归纳 链接 
http / / www . sohu . com / a 
/ 163742617 _ 610522python 自然语言 处理 . 2014 年 7月 
第一版 课后 习题 练习 1 . phrase = Valentine s 
phrase = lonely + phrase + day phrase lonely Valentine 
s day phrase 1 Valentine s phrase 1 1 a 
phrase . index day 2 sorted phrase Valentine s day 
lonely phrase 1 2 Valentine s phrase * 3 lonely 
Valentine s day lonely Valentine s day lonely Valentine s 
day 2 . from nltk . corpus import gutenberg gutenberg 
. fileids u austen emma . txt u austen persuasion 
. txt u austen sense . txt u bible kjv 
. txt u blake poems . txt u bryant stories 
. txt u burgess busterbrown . txt u carroll alice 
. txt u chesterton ball . txt u chesterton brown 
. txt u chesterton thursday . txt u edgeworth parents 
. txt u melville moby _ dick . txt u 
milton paradise . txt u shakespeare caesar . txt u 
shakespeare hamlet . txt u shakespeare macbeth . txt u 
whitman leaves . txt persuasion = gutenberg . words austen 
persuasion . txt len persuasion 98171 len set persuasion / 
/ 词 类型 我 不 知道 是不是 指 有 多少 
个 不 一样 的 词 61323 . from nltk . 
corpus import brown brown . categories u adventure u belles 
_ lettres u editorial u fiction u government u hobbies 
u humor u learned u lore u mystery u news 
u religion u reviews u romance u science _ fiction 
brown . words categories = lore u In u American 
u romance u u almost . . . brown . 
words categories = mystery u There u were u thirty 
eight u patients . . . from nltk . corpus 
import webtext webtext . fileids u firefox . txt u 
grail . txt u overheard . txt u pirates . 
txt u singles . txt u wine . txt webtext 
. words firefox . txt u Cookie u Manager u 
u u Don u u t . . . webtext 
. words grail . txt u SCENE u 1 u 
u u wind u u . . . 4 . 
from nltk . corpus import state _ union as su 
su . fileids u 1945 Truman . txt u 1946 
Truman . txt u 1947 Truman . txt u 1948 
Truman . txt u 1949 Truman . txt u 1950 
Truman . txt u 1951 Truman . txt u 1953 
Eisenhower . txt u 1954 Eisenhower . txt u 1955 
Eisenhower . txt u 1956 Eisenhower . txt u 1957 
Eisenhower . txt u 1958 Eisenhower . txt u 1959 
Eisenhower . txt u 1960 Eisenhower . txt u 1961 
Kennedy . txt u 1962 Kennedy . txt u 1963 
Johnson . txt u 1963 Kennedy . txt u 1964 
Johnson . txt u 1965 Johnson 1 . txt u 
1965 Johnson 2 . txt u 1966 Johnson . txt 
u 1967 Johnson . txt u 1968 Johnson . txt 
u 1969 Johnson . txt u 1970 Nixon . txt 
u 1971 Nixon . txt u 1972 Nixon . txt 
u 1973 Nixon . txt u 1974 Nixon . txt 
u 1975 Ford . txt u 1976 Ford . txt 
u 1977 Ford . txt u 1978 Carter . txt 
u 1979 Carter . txt u 1980 Carter . txt 
u 1981 Reagan . txt u 1982 Reagan . txt 
u 1983 Reagan . txt u 1984 Reagan . txt 
u 1985 Reagan . txt u 1986 Reagan . txt 
u 1987 Reagan . txt u 1988 Reagan . txt 
u 1989 Bush . txt u 1990 Bush . txt 
u 1991 Bush 1 . txt u 1991 Bush 2 
. txt u 1992 Bush . txt u 1993 Clinton 
. txt u 1994 Clinton . txt u 1995 Clinton 
. txt u 1996 Clinton . txt u 1997 Clinton 
. txt u 1998 Clinton . txt u 1999 Clinton 
. txt u 2000 Clinton . txt u 2001 GWBush 
1 . txt u 2001 GWBush 2 . txt u 
2002 GWBush . txt u 2003 GWBush . txt u 
2004 GWBush . txt u 2005 GWBush . txt u 
2006 GWBush . txt fdist1 = nltk . C o 
n d i t i o n a l F 
r e q D i s t . . . 
object file 0 4 . . . for file in 
su . fileids . . . for w in su 
. words file . . . for object in men 
women people . . . if w . lower . 
startswith object fdist1 . plot 可以 思考 女权运动 这个 问题 
5 . 首先 复习 一下 2.5 WordNet 1 意义 与 
同义词 类属 关系 AKO synset 同义词     lemma 词条 
词条 motorcar   属于 哪 一个 同义词 集合 span style 
= color # 3333ff from nltk . corpus import wordnet 
as wn a = wn . synsets motorcar a Synset 
car . n . 01 / span 该 同义词 集合 
有 哪些 词条 也 可以 仅 仅 显示 词条 的 
名称 span style = color # 3333ff wn . synset 
car . n . 01 . lemmas Lemma car . 
n . 01 . car Lemma car . n . 
01 . auto Lemma car . n . 01 . 
automobile Lemma car . n . 01 . machine Lemma 
car . n . 01 . motorcar wn . synset 
car . n . 01 . lemma _ names u 
car u auto u automobile u machine u motorcar / 
span span style = color # 3333ff 也 可以 显示 
词条 的 定义 和 事例 / span span style = 
color # 3333ff wn . synset car . n . 
01 . definition u a motor vehicle with four wheels 
usually propelled by an internal combustion engine wn . synset 
car . n . 01 . examples u he needs 
a car to get to work / span 提示 在 
课本 上 事例 为 span style = color # 3333ff 
wn . synset car . n . 01 . definition 
bound method Synset . definition of Synset car . n 
. 01 / span 答案 会 显示 异常 2 WordNet 
的 层次结构 2.1 上下位 词 hyponyms hypernyms 类属 关系 中 
ISA 的 关系 span style = color # 3333ff a 
= wn . synset car . n . 01 print 
a . hyponyms Synset ambulance . n . 01 Synset 
beach _ wagon . n . 01 Synset bus . 
n . 04 Synset cab . n . 03 Synset 
compact . n . 03 Synset convertible . n . 
01 Synset coupe . n . 01 Synset cruiser . 
n . 01 Synset electric . n . 01 Synset 
gas _ guzzler . n . 01 Synset hardtop . 
n . 01 Synset hatchback . n . 01 Synset 
horseless _ carriage . n . 01 Synset hot _ 
rod . n . 01 Synset jeep . n . 
01 Synset limousine . n . 01 Synset loaner . 
n . 02 Synset minicar . n . 01 Synset 
minivan . n . 01 Synset model _ t . 
n . 01 Synset pace _ car . n . 
01 Synset racer . n . 02 Synset roadster . 
n . 01 Synset sedan . n . 01 Synset 
sport _ utility . n . 01 Synset sports _ 
car . n . 01 Synset stanley _ steamer . 
n . 01 Synset stock _ car . n . 
01 Synset subcompact . n . 01 Synset touring _ 
car . n . 01 Synset used car . n 
. 01 print a . hypernyms Synset motor _ vehicle 
. n . 01 / span 计算 到 car . 
n . 01 的 路径 数 span style = color 
# 3333ff path = a . hypernym _ paths len 
path 2 synset . name for synset in path 0 
u entity . n . 01 u physical _ entity 
. n . 01 u object . n . 01 
u whole . n . 02 u artifact . n 
. 01 u instrumentality . n . 03 u container 
. n . 01 u wheeled _ vehicle . n 
. 01 u self propelled _ vehicle . n . 
01 u motor _ vehicle . n . 01 u 
car . n . 01 synset . name for synset 
in path 1 u entity . n . 01 u 
physical _ entity . n . 01 u object . 
n . 01 u whole . n . 02 u 
artifact . n . 01 u instrumentality . n . 
03 u conveyance . n . 03 u vehicle . 
n . 01 u wheeled _ vehicle . n . 
01 u self propelled _ vehicle . n . 01 
u motor _ vehicle . n . 01 u car 
. n . 01 / span 2.2 蕴含 关系 span 
style = color # 3333ff wn . synset walk . 
v . 01 . entailments Synset step . v . 
01 / span 2.3 反义 关系 互斥 span style = 
color # 3333ff wn . lemma rush . v . 
01 . rush . antonyms Lemma linger . v . 
04 . linger / span 2.4 查看 词条 拥 有 
哪些 关系 span style = color # 3333ff dir wn 
. synset harmony . n . 02 _ _ class 
_ _ _ _ delattr _ _ _ _ dict 
_ _ _ _ doc _ _ _ _ eq 
_ _ _ _ format _ _ _ _ ge 
_ _ _ _ getattribute _ _ _ _ gt 
_ _ _ _ hash _ _ _ _ init 
_ _ _ _ le _ _ _ _ lt 
_ _ _ _ module _ _ _ _ ne 
_ _ _ _ new _ _ _ _ reduce 
_ _ _ _ reduce _ ex _ _ _ 
_ repr _ _ _ _ setattr _ _ _ 
_ sizeof _ _ _ _ slots _ _ _ 
_ str _ _ _ _ subclasshook _ _ _ 
_ unicode _ _ _ _ weakref _ _ _ 
all _ hypernyms _ definition _ examples _ frame _ 
ids _ hypernyms _ instance _ hypernyms _ iter _ 
hypernym _ lists _ lemma _ names _ lemma _ 
pointers _ lemmas _ lexname _ max _ depth _ 
min _ depth _ name _ needs _ root _ 
offset _ pointers _ pos _ related _ shortest _ 
hypernym _ paths _ wordnet _ corpus _ reader also 
_ sees attributes causes closure common _ hypernyms definition entailments 
examples frame _ ids hypernym _ distances hypernym _ paths 
hypernyms hyponyms instance _ hypernyms instance _ hyponyms jcn _ 
similarity lch _ similarity lemma _ names lemmas lexname lin 
_ similarity lowest _ common _ hypernyms max _ depth 
member _ holonyms member _ meronyms min _ depth name 
offset part _ holonyms part _ meronyms path _ similarity 
pos region _ domains res _ similarity root _ hypernyms 
shortest _ path _ distance similar _ tos substance _ 
holonyms substance _ meronyms topic _ domains tree unicode _ 
repr usage _ domains verb _ groups wup _ similarity 
/ span 回顾 完毕 这 一题 属于 以下 关系 2.5 
  整体 部分 关系 整体 与 部分 关系 有三种 member 
_ holonyms   集合 概念 把 事物 看成 构成 的 
一部分 part _ meronyms   肢解 后的/nr 小部分 substance _ 
meronyms   事物 构成 的 本质 wn . synset tree 
. n . 01 . member _ holonyms / / 
树 的 集合 是 森林 Synset forest . n . 
01 wn . synset dog . n . 01 . 
member _ holonyms / / 第一个 狗 是 犬 属 
Synset canis . n . 01 Synset pack . n 
. 06 wn . synset hand . n . 01 
. part _ meronyms / / 下面 为 hand 的 
构成 部分 Synset ball . n . 10 Synset digital 
_ arteries . n . 01 Synset finger . n 
. 01 Synset intercapitular _ vein . n . 01 
Synset metacarpal _ artery . n . 01 Synset metacarpal 
_ vein . n . 01 Synset metacarpus . n 
. 01 Synset palm . n . 01 wn . 
synset tree . n . 01 . substance _ meronyms 
/ / 树 的 实质 是 心材 和 边材 Synset 
heartwood . n . 01 Synset sapwood . n . 
01 6 . 不懂 7 . wwe 文章 目录 自然语言 
处理 基本概念 语言 的 数学 本质 统计 语言 模型 N 
Gram Model 分词 信息 度量 信息熵 信息 的 作用 互信息 
相对 熵 信息熵 的 应用 决策树 Feature Extraction and PreprocessingOne 
hot encodingBag Of Words ModelSparse VectorsStop word f i l 
t e r L e m m a t i 
z a t i o n vs StemmingTF IDFTF IDF 
的 信息 论 依据 文本 分类 样例 Summary 自然语言 处理 
基本概念 语言 的 数学 本质 语言 的 出现 是 为了 
通信 通信 的 本质 是 为了 传递信息 字母 文字 数字 
都是/nr 信息 编码 的 不同 单元 任何 一种 语言 都是/nr 
一种 编解码 算法 我们 通过 语言 把 要 表达 的 
意思 传 递出来 实际上 就是 用 语言 将 大脑 中 
的 信息 进行 了 一次 编码 形成 了 一串 文字 
懂得 这种 语言 的 接收 方就/nr 能够 使用 这种 语言 
进行 解码 然后 获取 到 里面 的 信息 这 就是 
语言 的 数学 本质 统计 语言 模型 机器 是 不 
懂得 任何 一种 语言 的 早期 的 自然 语言 处理 
方式 是 让 计算机 学习 理解 语言 的 语义 语法 
然后 据此 判断 一个 句子 是否 合理 含义 是 什么 
但 最终 证明 这种 研究 方向 和 学习 方式 是 
行不通 的 现在 的 自然 语言 处理 是 基于 统计 
语言 模型 它 根本 不 需要 计算机 理解 人类 的 
语言 它 要做 的 就是 判断 一个 句子 是否 合理 
就看 这个 句子 在 语料库 中 出现 的 概率 如何 
假定 表示 某 一个 有 意义 的 句子 由 一连串 
的 词 w1 w2 ⋯ & ThinSpace wnw _ 1 
w _ 2 \ cdots w _ nw1 w2 ⋯ 
wn 组成 nnn 是 句子 的 长度 如果 想 知道 
在 文本 中 出现 的 概率 P S P S 
P S 那就/nr 需要 把 有史以来 人类 讲过 的话 统计 
一下 然后 计算出 出现 的 概率 这种 方法 很 显然 
是 行不通 的 因此 需要 一个 模型 来 估算 由于 
= w1 w2 ⋯ & ThinSpace wnS = w _ 
1 w _ 2 \ cdots w _ nS = 
w1 w2 ⋯ wn 那么 P S = P w1 
w2 ⋯ & ThinSpace wn P S = P w 
_ 1 w _ 2 \ cdots w _ n 
P S = P w1 w2 ⋯ wn 利用 条件概率 
公式 出现 的 概率 等于 每 一个 词 出现 的 
条件 概率 的 乘积 P w1 w2 ⋯ & ThinSpace 
wn = P w1 ⋅ P w2 ∣ w1 ⋅ 
P w3 ∣ w1 w2 ⋯ P wn ∣ w1 
w2 ⋯ & ThinSpace wn − 1 P w _ 
1 w _ 2 \ cdots w _ n = 
P w _ 1 \ cdot P w _ 2 
| w _ 1 \ cdot P w _ 3 
| w _ 1 w _ 2 \ cdots P 
w _ n | w _ 1 w _ 2 
\ cdots w _ { n 1 } P w1 
w2 ⋯ wn = P w1 ⋅ P w2 ∣ 
w1 ⋅ P w3 ∣ w1 w2 ⋯ P wn 
∣ w1 w2 ⋯ wn − 1 其中 P w2 
∣ w1 P w _ 2 | w _ 1 
P w2 ∣ w1 表示 在 已知 以 一个 词 
出现 的 前提 下 第二个 词 出现 的 概率 以此类推 
wnw _ nwn 的 出现 概率 取决于 它 前面 所 
有的 词 但 这种 条件 概率 的 可能性 太多 非常 
难以 计算 俄国 数学家 马尔科夫 提出 了 一个 偷懒 但是 
有效 的 做法 即 马尔科夫 假设 模型 来 简化 这种 
计算 任意 一个 词 wiw _ iwi 出现 的 概率 
只 同 它 前面 的 词 wi − 1w _ 
{ i 1 } wi − 1 有关 简化 后 
出现 的 概率 为 P S = P w1 ⋅ 
P w2 ∣ w1 ⋅ P w3 ∣ w2 ⋯ 
P wn ∣ wn − 1 P S = P 
w _ 1 \ cdot P w _ 2 | 
w _ 1 \ cdot P w _ 3 | 
w _ 2 \ cdots P w _ n | 
w _ { n 1 } P S = P 
w1 ⋅ P w2 ∣ w1 ⋅ P w3 ∣ 
w2 ⋯ P wn ∣ wn − 1 该 公式 
对应 的 统计 语言 模型 为 二元 模型 Bigram Model 
以上 是 理论 那么 在 实际 的 机器 学习 中 
是 如何 操作 的 呢 首先 计算 P wi ∣ 
wi − 1 P w _ i | w _ 
{ i 1 } P wi ∣ wi − 1 
根据 条件 概率 的 定义 P wi ∣ wi − 
1 = P wi wi − 1 P Wi − 
1 P w _ i | w _ { i 
1 } = \ frac { P w _ i 
w _ { i 1 } } { P W 
_ { i 1 } } P wi ∣ wi 
− 1 = P Wi − 1 P wi wi 
− 1 只需 估计 联合 概率 P wi wi − 
1 P w _ i w _ { i 1 
} P wi wi − 1 和 边缘 概率 P 
wi − 1 P w _ { i 1 } 
P wi − 1 就 变得 很 简单 基于 大量 
的 语料库 Corpus 只需要 统计 wi − 1 wiw _ 
{ i 1 } w _ iwi − 1 wi 
这 对词 在 统计 的 文本 中 出现 的 次数 
# wi − 1 wi \ # w _ { 
i 1 } w _ i # wi − 1 
wi 以及 wi − 1w _ { i 1 } 
wi − 1 本身 在 同样 的 文本 中 出现 
的 次数 # wi \ # w _ i # 
wi 然后 用 这 两个 数 分别 除以 语料库 的 
大小 # \ # # 即可 得到 这些 词 的 
相对 频度 f wi − 1 wi = # wi 
− 1 wi # f w _ { i 1 
} w _ i = \ frac { \ # 
w _ { i 1 } w _ i } 
{ \ # } f wi − 1 wi = 
# # wi − 1 wi f wi − 1 
= # wi − 1 # f w _ { 
i 1 } = \ frac { \ # w 
_ { i 1 } } { \ # } 
f wi − 1 = # # wi − 1 
然后 根据 大数 原理 只要 统计量 足够 相对 频度 就 
等于 概率 即 P wi wi − 1 ≈ # 
wi − 1 wi # P w _ i w 
_ { i 1 } \ approx \ frac { 
\ # w _ { i 1 } w _ 
i } { \ # } P wi wi − 
1 ≈ # # wi − 1 wi P wi 
− 1 ≈ # wi − 1 # P w 
_ { i 1 } \ approx \ frac { 
\ # w _ { i 1 } } { 
\ # } P wi − 1 ≈ # # 
wi − 1 最终 简化 后 # \ # # 
约 掉 因此 P wi ∣ wi − 1 ≈ 
# wi − 1 wi # wi − 1 P 
w _ i | w _ { i 1 } 
\ approx \ frac { \ # w _ { 
i 1 } w _ i } { \ # 
w _ { i 1 } } P wi ∣ 
wi − 1 ≈ # wi − 1 # wi 
− 1 wi N Gram Model 马尔科夫 假设 中 只 
定义 和 前面 一个 词 有关 称之为 二元 模型 当 
和其/nr 前面 N 个 词 有关 的 情况 则 成为 
N 元 模型 这 就是 文本处理 中 经常 见到 的 
N Gram Model 实际应用 最多 的 是 N = 3 
的 三元 模型 之所以 不 用 更高 阶 的 原因 
主要 是 空间 复杂度 N 元 模型 的 大小 是 
N 的 指数 即 O ∣ V ∣ N O 
| V | ^ N O ∣ V ∣ N 
V 为 一种 语言 字典 的 词汇 量 时间 复杂度 
N 元 模型 的 速度 也是 N 的 指数 即 
O ∣ V ∣ N − 1 O | V 
| ^ { N 1 } O ∣ V ∣ 
N − 1 因此 N 不能 太大 而且 N 从1/nr 
2 2 3 的 效果 提升 显著 但是 3 4时 
效果 就 不 明显 了 而且 N 即 使更 高阶 
也 无法 覆盖 所有 的 语言 因为 语言 的 上下文 
的 相关性 跨度 可能 非常 大 比如 跨 段落 这是 
马尔科夫 假设 无法 解决 的 分词 统计 语言 模型 是 
建立 在 词 的 基础上 的 词 是 表达 语义 
的 最小 单位 对于 西方 拼音 语言 来说 词 之间 
是 有 分界符 因此 分词 很简单 但是 对于 东方 语言 
词 之间 没有 分界符 因此 进行 自然语言 处理 前 首先 
要 对 句子 进行 分词 查字 典法 把 句子 从左到右 
扫描 遇到 字典 里面 有的 词 就 标识 出来 遇到 
复合词 就 找 最长 匹配 遇到 不 认识 的 字串 
就 分割 成单 字词 比如 上海大学 上 是 单 字词 
遇到 海 时 发现 可以 和 前面 的 上 组成 
更长 的 词 分割 点 就 放在 上海 后面 后面 
它 还能 发现 上海大学 其实 是 个 复合词 那么 最后 
把 分割 点 再 移到 大学 后面 统计 语言 模型 
分 词法 虽然 查字典 法 可以 解决 70 80% 的 
分词 问题 但是 中 文中 有 很多 二义性 的 词语 
比如 发展 中 国家 按照 查字典 的 方法 得到 的 
分词 结果 是 发展 中国 家 而 正确 的 分词 
结果 应该 是 发展 中 国家 又 比如 长 匹配 
带来 的 问题 北京 大学生 正确 的 应该 是 北京 
大学生 而 不是 北京大学 生 最终 解决 这个 问题 的 
方法 还是 依赖 统计 语言 模型 原理 如下 假设 一个 
句子 可以 有 以下 几种 分词 方法 A1 A2 ⋯ 
& ThinSpace AxA _ 1 A _ 2 \ cdots 
A _ xA1 A2 ⋯ AxB1 B2 ⋯ & ThinSpace 
ByB _ 1 B _ 2 \ cdots B _ 
yB1 B2 ⋯ ByC1 C2 ⋯ & ThinSpace CzC _ 
1 C _ 2 \ cdots C _ zC1 C2 
⋯ Cz 那么 最好 的 一种 分词 方法 应该 保证 
分词 后的/nr 句子 出现 的 概率 最大 如果 A1 A2 
⋯ & ThinSpace AxA _ 1 A _ 2 \ 
cdots A _ xA1 A2 ⋯ Ax 最好 那么 需要 
满足 P A1 A2 ⋯ & ThinSpace Ax & gt 
P B1 B2 ⋯ & ThinSpace By P A _ 
1 A _ 2 \ cdots A _ x & 
gt P B _ 1 B _ 2 \ cdots 
B _ y P A1 A2 ⋯ Ax P B1 
B2 ⋯ By 且 P A1 A2 ⋯ & ThinSpace 
Ax & gt P C1 C2 ⋯ & ThinSpace Cz 
P A _ 1 A _ 2 \ cdots A 
_ x & gt P C _ 1 C _ 
2 \ cdots C _ z P A1 A2 ⋯ 
Ax P C1 C2 ⋯ Cz 分词 粒度 对于 不同 
的 应用 场景 可以 有 不同 的 分词 粒度 比如 
机器翻译 中 粒度 大 效果 好 而在 网页 搜索 中 
粒度 小 的 效果 好 以 统计 预言 模型 为基础 
的 中文分词 基本 可以 看做 是 一个 已经 解决 了 
的 问题 提升 空间 微乎其微 分词器 好坏 的 差别 在于 
数据 的 使用 工程 实现 的 精度 信息 度量 信息 
是 一个 比较 抽象 的 概念 比如 50 万字 的 
史记 信息量 是 多少 直到 香农 1948年 提出 信息熵 的 
概念 才 解决 了 信息 的 度量 问题 信息熵 一条 
信息 的 信息量 与其 不确定性 有着 直接 的 关系 比如 
2018年 世界杯 冠军 是 谁 不确定性 就 大 因此 需要 
了解 大量 的 信息 才能 推断 又 比如 中国队 能否 
进入 世界杯 不确定性 就 很小 基本 不 需要 什么 信息量 
就能 确定 前面 的 信息量 大 后面 的 信息 量小 
因此 可以 认为 信息量 就 等于 不确定性 的 多少 香农 
使用 bit 来 度量 信息量 比如 32只 球队 比赛 每个 
球队 夺冠 的 概率 相等 那么 谁 是 冠军 的 
信息量 是 5bit 它 的 算法 如下 H = − 
p1 ⋅ log ⁡ p1 + p2 ⋅ log ⁡ 
p2 + ⋯ + p32 ⋅ log ⁡ p32 H 
= p _ 1 \ cdot \ log p _ 
1 + p _ 2 \ cdot \ log p 
_ 2 + \ cdots + p _ { 32 
} \ cdot \ log p _ { 32 } 
H = − p1 ⋅ logp1 + p2 ⋅ logp2 
+ ⋯ + p32 ⋅ logp32 其中 p1 ⋯ & 
ThinSpace p32p _ 1 \ cdots p _ { 32 
} p1 ⋯ p32 分别 是 这 32支 球队 夺冠 
的 概率 H 为 信息熵 Entropy 单位 是 bit 当 
32支 球队 的 夺冠 概率 相 等时 H 为 5bit 
对于 任意 一个 随机变量 X 比如 得 冠 的 球队 
它 的 信息 熵 定义 如下 H x = − 
∑ x ∈ XP x log ⁡ P x H 
x = \ sum _ { x \ in X 
} P x \ log P x H x = 
− x ∈ X ∑ P x logP x 变量 
的 不确定性 越大 熵 就 越大 比如 P x 越小 
熵 就 越大 案例 一本 50 万字 的 中文书 平均 
信息量 为 多少 中文 常用 汉字 7000 左右 假如 每个 
汉字 概率 相等 那么 大约 每 个 汉字 的 信息 
熵 需要 13bit 但是 汉字 的 使用 频率 是 不等 
的 基本 10% 左右 的 常用字 占据 整个 文本 的 
95% 那么 每个 汉字 的 信息量 10bit 就 够了 如果 
再考虑 上下文 每个 汉字 的 信息 熵 5 6bit 就 
够了 所以 一本 50 万字 的 书 的 信息 量 
大约 是 250万 300万 bit 但这 只是 一个 平均数 同样 
长度 的 书 所含 的 信息量 是 不同 的 如果 
一 本书 重复 的 内容 很多 它 的 信息量 就会 
很少 冗余度 就 很大 而且 不同 语言 的 冗余度 差别 
也 很大 汉语 的 冗余度 是 比较 小 的 一般 
认为 汉语 是 最 简洁 的 语言 信息 的 作用 
信息 是 消除 系统 不确定性 的 唯一 方法 假如 一个 
系统 的 不确定性 为 UUU 从外部 消除 这个 不确定性 的 
唯一 方法 是 引入 信息 III 如果 I & gt 
UI & gt UI U 那么 就 消除 了 不确定性 
如果 I & lt UI & lt UI U 只是 
部分 消除 了 但 仍 遗留 了 新的 不确定性 U 
′ = U − IU ^ { \ prime } 
= U IU ′ = U − I 自然语言 处理 
的 过程 就是 一个 消除 不确定性 的 过程 比如 在 
一元 模型 就 是 通过 单个 词 的 概率分布 消除 
不确定性 因素 二元 模型 使用 了 上下文 信息 就能 消除 
更多 的 不确定性 提高 准确率 通过 上下文 信息 可以 消除 
不确定性 可以 用 数学 的 方法 证明 这里 使用 了 
条件 熵 假定 X 和Y是/nr 两个 随机变量 如果 知道 了 
X 的 随机分布 P x P x P x 那么 
也 就 知道 了 X 的 熵 H x = 
− ∑ x ∈ XP x log ⁡ P x 
H x = \ sum _ { x \ in 
X } P x \ log P x H x 
= − ∑ x ∈ X P x logP x 
假定 还知道 Y 的 一些 情况 包括 Y 和X/nr 一起 
出现 的 概率 联合 概率 以及 Y 在 取 不同 
值 的 前提 下 X 的 概率分布 条件概率 则在 Y 
的 条件 下 的 条件 熵 为 H X ∣ 
Y = − ∑ x ∈ X y ∈ YP 
x y log ⁡ P x y H X | 
Y = \ sum _ { x \ in X 
y \ in Y } P x y \ log 
P x y H X ∣ Y = − ∑ 
x ∈ X y ∈ Y P x y logP 
x y 数学 上 可以 证明 H x ≥ H 
X ∣ Y H x \ ge H X | 
Y H x ≥ H X ∣ Y 也 就是 
在 知道 了 Y 的 信息 后 关于 X 的 
不确定性 降低 了 那么 由此 可以 得出 二元 模型 的 
不确定性 小于 一元 模型 同理 三元 模型 的 不确定性 小于 
二元 模型 总之 信息 的 作用 就是 消除 不 确定 
性 互信息 上节 讲到 的 有 上下文 关系 的 随机变量 
能够 帮忙 消除 不确定性 但是 这个 有 关系 是 个 
模糊 的 说法 能 不能 把 这种 关系 也 量化 
呢 香农 提出 的 互信息 就是 对 两个 随机变量 的 
相关性 做 的 度量 量化 假定/v 有/v 两个/m 随机事件/i X/w 
和Y/nr 它们 的 互信息 定义 I X Y = ∑ 
x ∈ X y ∈ yP x y log ⁡ 
P x y P x P y I X Y 
= \ sum _ { x \ in X y 
\ in y } P x y \ log { 
\ frac { P x y } { P x 
P y } } I X Y = x ∈ 
X y ∈ y ∑ P x y logP x 
P y P x y 实际上 I X Y = 
H X − H X ∣ Y I X Y 
= H X H X | Y I X Y 
= H X − H X ∣ Y 就是 上节 
里面 提到 的 X 的 熵 与 条件 熵 的 
差 所谓 两个 事件 相关性 的 量化 度量 就是 在 
了解 其中 一个 Y 的 前提 下 对 消除 另一个 
X 不确定性 所 提供 的 信息量 互 信息 的 范围 
是 0 到 min H X H Y 当 X 
和Y/nr 完全 相关 时 取值 为 1 完全 无关 时 
取值 为 0 相对 熵 也 称为 交叉 熵 Kullback 
Leibler Divergence 也 用来 衡量 相关性 但 和 变量 的 
互信息 不同 它 用来 衡量 两个 取值 为 正数 的 
函数 的 相似性 它 的 定义 如下 KL f x 
∣ ∣ g x = ∑ xinXf x ⋅ log 
⁡ f x g x KL f x | | 
g x = \ sum _ { x in X 
} f x \ cdot \ log { \ frac 
{ f x } { g x } } KL 
f x ∣ ∣ g x = xinX ∑ f 
x ⋅ logg x f x 公式 不重要 结论 记住 
就好 对于 两个 完全 相同 的 函数 它们 的 相对 
熵 等于零 相对 熵 越大 两个 函数 差异 越大 反之 
越小 对于 概率分布 或者 概率密度函数 如果 取值 均 大于 零 
相对 熵 可以 度量 两个 随机分布 的 差异性 相对 熵 
之前 用于 信号处理 两个 随机 信号 相对 熵 越小 说明 
两个 信号 越 接近 后来 也把 它 用来 衡量 两端 
信息 的 相似 度 比如 一 篇 文章 照抄 或者 
改写 另一 篇 那么 这 两 篇文章 中的 词频 分布 
的 相对 熵 就 非常 小 信息熵 的 应用 – 
决策树 决策树 由 结点 node 和 有向 边 directed edge 
组成 结点 有 两种 类型 内部 结点 internal node 和叶/nr 
结点/n leaf node 内部 结点 表示 一个 特征 叶 结点 
表示 一个 类 用 决策树 分类 从 根结 点 开始 
对 实例 的 每一个 特征 进行 测试 根据 测试 结果 
将 实例 分配 到 其子 结点 每 个子 结点 对应 
着 该 特征 的 一个 取值 如此 递归 地 对 
实例 进行 测试 并 分配 直到 达到 叶 结点 最后 
将 实例 分配 到 叶 结点 下图 为 决策树 示意图 
圆/n 和/c 方框/n 分别/d 表示/v 内部/f 结点/n 和叶/nr 结点/n 如下 
14个 训练样本 其中 8只 猫 6只 狗 采用 决策树 算法 
如何 高 效地 分类 我们 的 目标 是 希望 每一次 
的 特征 测试 分 出来 的 子类 要么 包括 所有 
的 猫 或者 所有 的 狗 而 不是 两者 都有 
每一次/i 的/uj 测试/vn 都能/nr 最大/a 的/uj 降低/v 不确定性/n 这样 就 
能 提高 决策树 的 效率 而 不确定性 的 度量 就是 
用 信息熵 H x = − ∑ x = 1NP 
xi log ⁡ P xi H x = \ sum 
_ { x = 1 } ^ { N } 
P x _ i \ log P x _ i 
H x = − ∑ x = 1N P xi 
logP xi 对于 训练样本 除了/p 知道/v 里面/f 有/v 6只/mq 狗/n 
和/c 8只/mq 猫/n 其他 信息 一无所知 那么 对于 猫狗 分类 
这件事 的 信息 熵 为 H x = − 614log 
614 + 814log 814 = 0.98523 H x = \ 
frac { 6 } { 14 } log \ frac 
{ 6 } { 14 } + \ frac { 
8 } { 14 } log \ frac { 8 
} { 14 } = 0.98523 H x = − 
146 log 146 + 148 log 148 = 0.98523 现在 
有 3个 特征 play fetch is grumpy favorite food 我们 
希望 选择 一个 用来 测试 能够 最大 的 降低 信息熵 
不确定性 比如 选择 play fetch 分类 后 结果 如下 决策树 
经常 使用 上图 可视化 的 方式 来 查看 分类 逻辑 
和 效果 在 根 节点 中的 信息熵 是 0.985 然后 
我们 使用 Play fetch 这个 特征 分成 2类 后 一类 
中有 9个 样本 其中 7只 猫 2只 狗 另一类 5个 
样本 其中 1只 猫 4只 狗 结果 不是 很 理想 
每个/r 子类/n 里面/f 都/d 同时/c 包括/v 了/ul 猫/n 和狗/nr 那么 
对于 这 两个 子类 其 信息熵 为 H x = 
− 79log 79 + 29log 29 = 0.7642 H x 
= \ frac { 7 } { 9 } log 
\ frac { 7 } { 9 } + \ 
frac { 2 } { 9 } log \ frac 
{ 2 } { 9 } = 0.7642 H x 
= − 97 log 97 + 92 log 92 = 
0.7642 H x = − 15log 15 + 45log 45 
= 0.7219 H x = \ frac { 1 } 
{ 5 } log \ frac { 1 } { 
5 } + \ frac { 4 } { 5 
} log \ frac { 4 } { 5 } 
= 0.7219 H x = − 51 log 51 + 
54 log 54 = 0.7219 如果 我们 使用 is grumy 
作 为特征 测试 结果 为 使用 cat food 那么 对于 
这 3种 结果 哪一种 性能 最好 呢 实际上 这 是 
一个 最优 特征选择 的 问题 这里 我们 引入 information gain 
信息 增益 来 解决 这个 问题 Information Gain 特征 A 
对 训练 集 D 的 信息 增益 g D A 
定义 为 集合 D 的 信息 熵 H D 与 
在 特征 A 给定 的 条件 下 D 的 条件 
熵 H D | A 之差 即 g D A 
= H D − H D ∣ A g D 
A = H D H D | A g D 
A = H D − H D ∣ A 信息熵 
H D 表示 对于 数据集 D 进行 分类 的 不确定性 
条件 熵 H D | A 表示 在 特征 A 
给定 的 条件 下 对 数据集 D 进行 分类 的 
不确定性 它们 的 差 即为 信息 增益 它 表示 由于 
特征 A 而 使得 对 数据集 D 的 分类 不确定性 
减少 的 程度 显然 对于 数据集 D 而言 信息 增益 
依赖 特征 不同 的 特征 具有 不同 的 信息 增益 
信息 增益 大 的 特征 具有 更强 的 分类 能力 
根据 信息 增益 的 特征选择 方法 是 对 训练 集 
或者 子集 D 计算 其 每个 特征 的 信息 增益 
并 比较 它们 的 大小 选择 信息 增益 最大 的 
特征 之前 已经 讲过 信息熵 H X 与 条件 熵 
H X | Y 之差 为 互信息 I X Y 
= H X − H X ∣ Y I X 
Y = H X H X | Y I X 
Y = H X − H X ∣ Y 实际上 
决策树 中 的 信息 增益 等价 于 训练 数据 集中 
类 与 特征 的 互信息 决策树 算法 其实 就是 使用 
互信息 来 选择 最优 特征 下图 即为 各个 特征 条件下 
的 信息 增益 通过 计算 IG 我们 发现 cat food 
这个 特征 IG 最大 所以 它 是 最有 的 特征 
注意 在 这里 在 计算 IG 时 IG = Parent 
′ sEntropy − W e i g h t e 
d A v e r a g e I G 
= Parent & # x27 s Entropy Weighted AverageIG = 
Parent ′ sEntropy − WeightedAverage 以上 只是 选择 除了 第一级 
的 最优 特征 后面 第二级 的 最右 特征选择 方法 一样 
需要 在 剩余 的 特征 中 递归 找到 IG 最大 
的 那个 特征 但是 这里 IG 最大值 有 两个 在 
ID3 算法 中 它 是 随机 选择 一种 的 后面 
第三级 第四 级 等等 都是/nr 采用 这种 决策 方法 最终 
我们 会 得到 如下 一张 决策树 的 图 决策树 算法 
C 4.5 是 基于 ID3 的 变种 它 可以 修建 
分支 也是 最 流行 的 决策 树 方法 决策树 使用 
了 信息论 中的 信息熵 互信息 在 接下 里 的 文本 
处理 中 还将 看到 对于 交叉 熵 的 使用 代码 
实例 internet adsFeature Extraction and PreprocessingOne hot encoding 计算机 是 
读 不懂 人类 的 文字 的 它 本质 上 只能 
做 快速 计算 为了 让 计算机 能够 处理 文字 就 
要求 我们 先把 文字 变成 一组 可计算 的 数字 然后 
设计 一个 算法 来 算出 这些 文字 的 关系 One 
Hot 编码 又 称为 一位 有效 编码 主要 是 采用 
N 位 状态 寄存器 来 对 N 个 状态 进行 
编码 每个 状态 都由 他 独立 的 寄存器 位 并且 
在 任意 时候 只有 一位 有效 该 编码 用在 文字处理 
上面 举例 如下 from sklearn . feature _ extraction import 
DictVectorizer onehot _ encoder = DictVectorizer instances = { city 
Shanghai } { city Beijing } { city Shenzhen } 
print onehot _ encoder . fit _ transform instances . 
toarray 0 . 1 . 0 . 1 . 0 
. 0 . 0 . 0 . 1 . 从 
上面 输出 的 feature vector 可以 看出 通过 DictVectorizer 将 
字符串 转换成 了 一个 字典 向量 3 个 城市 的 
名字 作为 了 3个 元素 但是 城市 名字 的 顺序 
不是 按照 定义 排序 的 而是 按照 字母顺序 排序 的 
分别为 Beijing Shanghai Shenzhen 表示 Shanghai 时 其 对应 位置 
的 元素 为 1 0 1 0 通过 这个 简单 
的 例子 说明 文本 在 用来 机器学习 前 一定 要 
先 将其 向 量化 将 人类 可读 的 文本 转换 
为 机器 可算 的 数字 这里 只是 处理 几个 简单 
的 变量 那么 计算机 如何 处理 大量 的 文本 呢 
这 就 需要 通过 下面 的 模型 来 实现 Bag 
Of Words Model 我们 已经 知道 文本 在 机器学习 前 
一定 要 将其 向 量化 而且 需要 尽可能 多 得 
保证 文本 的 原意 其中 最为 通用 的 一个 模型 
是 Bag of words 中文 为 词 袋 模型 该 
模型 可以 看做 是 one hot 编码 的 扩展 特点 
如下 忽略 词序 忽略 语法 创建 一个 特征向量 里面 包含 
了 文本 中 的 每个 单词 词 袋 模型 的 
动机 是 为了 说明 包含 相似 单词 的 文本 应该 
有着 相似 的 意思 它 可以 高效 地 处理 文本 
分类 并 能从 编码 后的/nr 向量 恢复 对应 的 文本 
Corpus 包含 所有 文本 的 集合 称为 语料库 Vocabulary 由 
语料库 中 所有 不 重复 的 单词 组成 称为 词表 
Dimension 组成 feature vector 的 元素 数量 称为 维度 from 
sklearn . feature _ extraction . text import CountVectorizer corpus 
= UNC played Duke in basketball Duke lost the basketball 
game vectorizer = CountVectorizer print vectorizer . fit _ transform 
corpus . todense print vectorizer . vocabulary _ 1 1 
0 1 0 1 0 1 1 1 1 0 
1 0 1 0 { unc 7 played 5 duke 
1 in 3 basketball 0 lost 4 the 6 game 
2 } 上例 中 corpus 中 包含 了 两个 文本 
每个 文本 的 feature vector 采用 的 one hot 编码 
该 feature vector 的 维度 是 8Vocabulary 为 { unc 
7 played 5 duke 1 in 3 basketball 0 lost 
4 the 6 game 2 } 单词 后面 的 数字 
表示 该 单词 在 feature vector 中 的 位置 由此 
词表 它 可以 快速 地 将 feature vector 恢复 为 
文本 使用 CountVectorizer 它 默认 会 自动 最小化 字母 自动 
去 重 自动 分词 自动 去 空格 等 符号 改 
过程 称为 Tokenization 即将 string 分 段为 tokens Tokens 大多数 
为 单词 当然 也 可以 分 段为 短语 可以 包括 
标点符号 也 支持 自定义 正规 表达式 分段 Sparse VectorsSparse vectors 
稀疏 矩阵 这个 在 使用 词 袋 模型 时很/nr 常见 
比如 corpus = UNC played Duke in basketball Duke lost 
the basketball game I ate a sandwich 0 1 1 
0 1 0 1 0 0 1 0 1 1 
1 0 1 0 0 1 0 1 0 0 
0 0 0 0 1 0 0 新 加 了 
一条 文本 在 意思 上面 与 前 两条 没有 关系 
它 的 向量 与 上面 两条 也 无 交集 这也 
体现 了 词 袋 模型 的 特点 能够 发现 相似 
意思 的 文本 但 同时 这 也 带来 一个 问题 
向量 中 出现 了 大量 的 零 元素 如果 语料库 
有 几千 万条 文本 它 的 feature vector 维度 可能 
也 有 几百万 那么 语义 无关 的 文本 肯定 也 
会 出现 大量 的 零 元素 这种 含有 大量 零 
元素 的 高维 矩阵 称为 稀疏 矩阵 它 带来 两个 
严重 的 问题 占用 大量 的 memory 维数 灾难 随着 
维度 地 不断 增加 这就 要求 样本数据 需要 更多 有效 
的 feature 保持 文本 的 意思 否则 会被 稀释 掉 
而且 高 维度 的 情况 下 距离 计算 也会 困难 
维数 灾难 是 机器 学习 中 常见 的 问题 也 
有 一些 降 维 的 方案 接下来 介绍 几个 文本 
降 维 的 方法 代码 实例 20NewsGroups _ C l 
a s s i f i c a t i 
o n t o p word filterStop word 停用词 这个 
过滤 原理 很简单 比如 针对 英文 和 中文 各自 维护 
了 一个 停用词 列表 里面 收集 了 一些 不能 表示 
语义 的 词语 比如 a the I you do be 
is will 中文 也 类似 比如 我 你 她 啊 
呢 通过 这种 方法 可以 达到 降 维 的 目标 
比如 corpus = UNC played Duke in basketball Duke lost 
the basketball game I ate a sandwich vectorizer = CountVectorizer 
stop _ words = english data = vectorizer . fit 
_ transform corpus print data . todense print data . 
shape print vectorizer . vocabulary _ { unc 7 played 
5 duke 2 basketball 1 lost 4 game 3 ate 
0 sandwich 6 } 你 会 发现 它 去掉 了 
I a in the 现在 也 能 支持 自定义 停用词 
用来 应对 不同 的 应用 场景 Lemmatization vs StemmingStop word 
是 一个 很 简单 的 降 维 策略 但是 stop 
word list 里面 只有 几百 个 单词 对于 一个 很大 
的 语料库 来讲 仍然 是 杯水车薪 下面 针对 英文 文本 
还有 2个 相似 的 降 维 方法 Lemmatization 词性 还原 
Stemming 词根 化 这是 两个 相似 的 用来 降 维 
的 策略 一篇 高维 的 文档 向量 里面 可能 有 
很多 同 一个 词 的 各种 时态 但是 它们 也 
是 作为 feature vector 里面 一个 独立 的 元素 Stemming 
和 Lemmatization 的 作用 就是 将 这些 词 简化 成为 
一个 向量 元素 from sklearn . feature _ extraction . 
text import CountVectorizer corpus = He ate the sandwiches Every 
sandwich was eaten by him vectorizer = CountVectorizer stop _ 
words = english data = vectorizer . fit _ transform 
corpus print data . todense print vectorizer . vocabulary _ 
1 0 0 1 0 1 1 0 { ate 
0 sandwiches 3 sandwich 2 eaten 1 } 这两条 文本 
有着 相似 的 意思 但是 他们 的 feature vector 却 
没有 任何 交集 理想 情况下 有着 相似 意思 的 文本 
应该 有 相似 的 feature vector 而 造成 这个 问题 
的 原因 就是 eat 和 sandwich 的 不同 形态 被 
认为 是 不同 的 feature 解决 这种 问题 就 可以 
使用 Lemmatization 和 Stemming Lemmatization 的 过程 就是 将 单词 
还原 为其 原型 的 一个 过程 它 需要 依赖 词典 
资源 比如 WordNet 来 恢复 出 一个 正确 的 单词 
原型 Stemming 的 目标 和 Lemmatization 一样 它 就是 直接 
地 去掉 所有 看起来 是 词缀 样式 的 东西 而 
不在意 恢复 后的/nr 原型 是不是 一个 有效 的 单词 它 
不依赖 词典 只是 依赖 规则 对于 上面 的 文本 处理 
后 分别 为 Stemmed He ate the sandwich Everi sandwich 
wa eaten by him Lemmatized He eat the sandwich Every 
sandwich be eat by him 1 1 1 1 由 
上述 结果 可知 通过 Stemming 和 Lemmatization 除了 可以 降低 
feature vector 的 维度 还 能够 提高 句子 之间 相似 
度 TF IDF/w 通过/p 词/n 袋/q 模型/n 向/p 量化/v 后的/nr 
feature/w vector 忽略 了 语法 词序 词频 但是 从 直觉 
上来 分析 如果 一篇 文档 中 一些 词 多次 出现 
那么 它 和该/nr 文档 主题 的 关联性 大于 那些 只 
出现 过 一次 的 词 为此 我们 要 在 feature 
vector 中 把 词频 信息 考虑 进来 通过 TF IDF 
来 扩展 词 袋 模型 TF IDF Term Frequency / 
Inverse Document Frequency 首先 介绍 Term Frequency 这 就是 词频 
的 意思 那么 怎么 把 词频 的 信息 添加 到 
feature vector 中 呢 方法 如下 from sklearn . feature 
_ extraction . text import CountVectorizer corpus = The dog 
ate a sandwich the wizard transfigured a sandwich and I 
ate a sandwich vectorizer = CountVectorizer stop _ words = 
english print vectorizer . fit _ transform corpus . todense 
2 1 3 1 1 { dog 1 ate 0 
sandwich 2 wizard 4 transfigured 3 } CountVectorizer 就是 一个 
能够 记录 词频 的 方法 feature vector 中的 元素 值 
由 之前 的 0 或者 1 变成 了 单词 出现 
的 频次 接下来 举 一个 形象 的 例子 进一步 说明 
短语 原子能 的 应用 可以 分为 3个 词 原子能 的 
应用 如果 一篇 文档 中 这些 词 出现 的 次数 
多于 其他 文档 的 时候 那么 可以 说明 该 篇 
文档 的 主题 和 这些 词 的 相关性 高 这些 
关键词 就是 该 文档 主题 的 重要 feature 但是 这里 
也 有个 漏洞 那 就是 篇幅 长 的 文档 比 
短 的 占便宜 所以 根据 文档 的 长度 对 关键词 
的 次数 进行 归一化 即 用 关键词 的 数量 除以 
文档 总的 词数 这才 是 真正 的 词频 Term Frequency 
比如 一篇 文档 一 共有 1000个 词 其中 原子 的 
应用 分别 出现 了 2次 35次 5次 那么 它们 的 
词频 分别 是 0.002 0.035 0.005 这 里面 的 的 
词频 数 最高 但是 看起来 它 对于 主题 没有 什么 
价值 所以 可以 直接 将其 去掉 其实 的 即为 中文 
的 停止词 至此 这些 关键词 对于 主题 的 贡献 原子 
为 0.002 应用 为 0.005 这里 还有 一个 小 问题 
在 中文 中 应用 是个 很 普通 的 词 而 
原子能 是个 很 专业 的 词 它 和 主题 的 
相关性 比 前者 重要 因此 我们 有 必要 给 这些 
关键词 一个 权重 来 体现 出 它 的 重要性 这个 
权重 的 设定 必须 满足 下面 2个 条件 一个词 预测 
主题 的 能力 越强 权重 越大 反之 权重 越小 停止词 
的 权重 为零 那么 如何 判断 哪些 次 预测 主题 
的 能力 强呢/nr 如果 一个 关键词 在 语料库 中 少量 
的 文档 中 出现 说明 该词 和 这些 文档 关系密切 
它 的 权重 也 就应该 大 反之 如果 一个 词 
在 大量 的 文档 中 出现 比如 应用 它 的 
权重 就应该 小 接下来 的 问题 就是 如何 量化 权重 
这里 就 需要 IDF 假定 一个 关键词 w 在 DwD 
_ wDw 个 文档 中 出现 那么 DwD _ wDw 
越大 w 的 权重 就 越小 IDF 的 公式 为 
log ⁡ DDw \ log \ frac { D } 
{ D _ w } log Dw D 其中 D 
为 全部 文档 数 比如 语料库 总 文档 数 D 
= 10亿 停止词 的 在 所有 文档 中 出现 它 
的 Dw = 10D _ w = 10Dw = 10亿 
那么 它 的 IDF = log 10亿 / 10 亿 
= log 1 = 0 假如 原子能 在 200 万个 
文档 中 出现 Dw = 200D _ w = 200Dw 
= 200万 则 它 的 权重 IDF = log 500 
= 8.96 应用 在 5 亿个 网页 中 出现 IDF 
= log 2 = 1 利用 IDF 关键词 和 文档 
主题 的 相关性 公式 变为 TF1 ⋅ IDF1 + TF2 
⋅ IDF2 + ⋯ + TFn ⋅ IDFnTF _ 1 
\ cdot IDF _ 1 + TF _ 2 \ 
cdot IDF _ 2 + \ cdots + TF _ 
n \ cdot IDF _ nTF1 ⋅ IDF1 + TF2 
⋅ IDF2 + ⋯ + TFn ⋅ IDFn 上例 中 
该 文档 和 原子能 的 应用 的 相关性 为 0.002 
x 8.96 + 0.035 x 0 + 0.005 x 1 
= 0.02292 TF IDF 是 对 搜索 关键词 的 重要性 
的 度量 并且 具备 很强 的 理论 依据 因此 即使 
是 对 搜索 不是 很 精通 的 人 直接 采用 
TF IDF 效果 也会 太差 TF IDF 的 信息 论 
依据 衡量 一个 词 的 权重 一个 简单 的 办法 
就是 用 这个 词 的 信息量 作为 它 的 权重 
即 I w = − P w logP w = 
− TF w Nlog TF w N = TF w 
Nlog NTF w I w = P w logP w 
= \ frac { TF w } { N } 
log \ frac { TF w } { N } 
= \ frac { TF w } { N } 
log \ frac { N } { TF w } 
I w = − P w logP w = − 
NTF w log NTF w = NTF w log TF 
w N 其中 N 是 整个 语料库 的 大小 是个 
可以 省略 的 常数 公式 可以 简化 为 I w 
= TF w log NTF w I w = TF 
w log \ frac { N } { TF w 
} I w = TF w log TF w N 
这里 存在 一个 缺陷 两个 词 出现 的 TF 相同 
比如 一个 是 一篇 文章 的 常见 词 另 一个 
是 分散 在 多篇 文章 中 那么 显然 第 一个词 
应该 贡献 更大 应该 有 更大 的 权重 为此 我们 
做 一些 理想 的 假设 每个 文档 的 大小 基本相同 
均为 M 个 词 即 M = ND = ∑ 
wTF w DM = \ frac { N } { 
D } = \ frac { \ sum _ wTF 
w } { D } M = DN = D 
∑ w TF w 一个 关键词 在 文档 中 一旦 
出现 不论 次数 多少 贡献 都 相同 这样 一个 词 
要么 在 一个 文献 中 出现 c w = TF 
w D w c w = \ frac { TF 
w } { D w } c w = D 
w TF w 次 要么 就是 零 注意 c w 
& lt Mc w & lt Mc w M 把 
这 两个 条件 带入 到 上面 的 信息量 公式 后 
得出 TF w logNTF w = TF w logMDc w 
D w = TF w log DD w Mc w 
= TF w log DD w + TF w log 
Mc w TF w log \ frac { N } 
{ TF w } = TF w log \ frac 
{ MD } { c w D w } = 
TF w log \ frac { D } { D 
w } \ frac { M } { c w 
} = TF w log \ frac { D } 
{ D w } + TF w log \ frac 
{ M } { c w } TF w logTF 
w N = TF w logc w D w MD 
= TF w log D w D c w M 
= TF w log D w D + TF w 
log c w M 其中 TF − IDF w = 
TF w log DD w TF IDF w = TF 
w log \ frac { D } { D w 
} TF − IDF w = TF w log D 
w D 最终 TF − IDF w = I w 
− TF w log Mc w TF IDF w = 
I w TF w log \ frac { M } 
{ c w } TF − IDF w = I 
w − TF w log c w M 可以 看出 
当 一个 词 的 信息量 I w 越多 TF − 
IDF 值 越大 第二项 值 越小 TF − IDF 也 
越大 I w 越多 TF IDF 值 越大 第二项 值 
越小 TF IDF 也 越大 I w 越多 TF − 
IDF 值 越大 第二项 值 越小 TF − IDF 也 
越大 代码 实现 from sklearn . feature _ extraction . 
text import TfidfVectorizer corpus = The dog ate a sandwich 
and I ate a sandwich The wizard transfigured a sandwich 
vectorizer = TfidfVectorizer stop _ words = english print vectorizer 
. fit _ transform corpus . todense 0.75458397 0.37729199 0.53689271 
0 . 0 . 0 . 0 . 0.44943642 0.6316672 
0.6316672 { dog 1 ate 0 sandwich 2 wizard 4 
transfigured 3 } 文本 分类 样例 垃圾 短消息 分类 新闻 
类别 分类 Summary 理论 原理 自然语言 处理 的 数学 本质 
统计 语言 模型 中文分词 信息论 信息熵 互信息 交叉 熵 文本 
feature 抽取 和 预处理 词 袋 模型 降 维 方法 
停止词 词性 还原 / 词根 化 TF IDF 模型 What 
Is Natural Language Processing 本文 将 学习 自然语言 处理 当 
给予 计算机 一篇 文章 它 并不 知道 这 篇 文章 
的 含义 为了 让 计算机 可以 从 文章 中 做出 
推断 我们 需要 将 文章 转化 为 数值 表示 这个 
过程 使得 计算机 能够 凭 语法 规则 去 识别 它 
那么 首先 就要 学会 如何 将 文章 变为 数值 表示 
Looking At The DataHacker News 网站 是 一个 可以 提交 
文章 的 社区 网站 并且 其他 的 人 可以 对 
文章 进行 投票 投票 最高 的 文章 会被 放到 首页 
这样 就 有 更多 的 人 可以 看到 它 我们 
的 数据集 就是 Hacker News 网站 2006年 到 2015年 提交 
的 文章 集合 Arnaud Drizard 利用 Hacker News API 爬 
取到 了 这些 数据 我们 从中/nr 随机 抽取 了 3000个 
样本 删除 了 所有 多余 的 列 最终 数据 的 
属性 如下 submission _ time – when the article was 
submitted . upvotes – number of upvotes the article got 
. url – the base url of the article . 
headline – the headline of the article . 我们 将 
通过 文章 的 标题 来 预测 文章 会 收到 多少 
投票 换 句话 就是 哪 种 文章 更 受欢迎 首先 
将 数据 中 的 元素 值 为 NA 的 行 
删除 掉 import pandas as pd submissions = pd . 
read _ csv sel _ hn _ stories . csv 
submissions . columns = submission _ time upvotes url headline 
submissions = submissions . dropna Tokenization 我们 为了 预测 某个 
标题 会 得到 多少 个 投票 那么 首先 需要 将 
标题 转换 为 数值 表示 可以 用 词 袋 模型 
bag of words model 来 完成 这个 转换 词 袋 
模型 中 将 每个 文本 表示 为 一个 数值 型 
向量 看 个 例子 词 袋 模型 的 第一 步 
就是 分词 将 一个 句子 根据 空格 将其 分散 为 
一个 个 不 相连 的 单词 tokenized _ headlines = 
for item in submissions headline tokenized _ headlines . append 
item . split Preprocessing 由于 大小写 代表 的 意思 相同 
因此 我们 需要 将 所有 的 单词 都 转换 为 
小写 剔除 掉 标点符号 punctuation = . / + & 
clean _ tokenized = for item in tokenized _ headlines 
tokens = for token in item token = token . 
lower for punc in punctuation token = token . replace 
punc tokens . append token clean _ tokenized . append 
tokens Assembling A Matrix 现在 获取 了 每个 文本 的 
词 袋 下 一步 就是 将 这些 词 袋 求 
并 集 利用 single _ tokens 剔除 掉了 只 出现 
一次 的 单词 这样 的 单词 没有 多 大 意义 
unique _ tokens 自然 就是 存储 的 大于 一次 的 
单词 counts 是个 值 全为 0 的 DataFrame 其中 列 
标签 为 unique _ tokens 中的 单词 行 标签 为 
标题 序号 import numpy as np unique _ tokens = 
single _ tokens = for tokens in clean _ tokenized 
for token in tokens if token not in single _ 
tokens single _ tokens . append token elif token in 
single _ tokens and token not in unique _ tokens 
unique _ tokens . append token counts = pd . 
DataFrame 0 index = np . arange len clean _ 
tokenized columns = unique _ tokens Counting Tokens 填充 上面 
构造 的 全零/nr DataFrame 遍历 每个 token 中 的 所有 
单词 进行 计数 for i item in enumerate clean _ 
tokenized for token in item if token in unique _ 
tokens counts . iloc i token + = 1Removing Extraneous 
Columns 我们 的 属性 高达 2309 并且 其中 绝大部分 取值 
为 0 这样 不 便于 分析 较多 的 属性 只会 
让 模型 更加 拟合 噪音 而 不是 真正 的 信息 
因此 容易 导致 过拟合 问题 print len unique _ tokens 
2309 有 两类 特征 会 降低 模型 的 精度 第一种 
只 出现 过 几次 这样 的 特征 会 导致 过拟合 
因为 模型 没有 更多 的 信息 来 精确 的 确定 
这个 特征 是否 重要 因为 它 就只 出现 了 几次 
并且 它们 在 训练 集 和 测试 集中 对于 目标 
变量 的 影响 也会有 很大 的 差异 因为 出现 太少 
因此 属性 分布 不 平衡 第二种 出现 的 次数 太多 
比如/v 像/v and/w 和/c to/w 这样/r 的/uj 特征/n 根本/a 不能/v 
给/p 模型/n 带来/v 任何/r 有/v 意义/n 的/uj 信息/n 这些 词 
被 称为 停顿 词 应当 剔除 掉 因此 最终 确定 
保留 那些 属性值 大于 5 小于 100 的 属性 word 
_ counts = counts . sum axis = 0 word 
_ counts Series class pandas . core . series . 
Series 418 and 289 for 298 as 47 you 100 
is 158 counts = counts . loc word _ counts 
= 5 & word _ counts = 100 Splitting The 
Datasklearn . cross _ validation 中有 专门 划分 训练 集 
和 测试 集 的 函数 train _ test _ split 
counts 中 存储 的 是 分类 数据 而 submissions upvotes 
是 类 标签 数据 分别 对 其 进行 划分 from 
sklearn . cross _ validation import train _ test _ 
split X _ train X _ test y _ train 
y _ test = train _ test _ split counts 
submissions upvotes test _ size = 0.2 random _ state 
= 1 Making Predictionsfrom sklearn . linear _ model import 
LinearRegression clf = LinearRegression clf . fit X _ train 
y _ train predictions = clf . predict X _ 
test Calculating Error 计算 MSE 也 就是 平均 平方 误差 
mean squared error MSE mse = sum y _ test 
predictions * * 2 / len predictions print mse 2652 
. 6082512522867 Next Steps 得到 的 模型 的 mse 是 
2652 . 6082512522867 这 是 一个 很大 的 值 但是 
关于 什么 是 好 的 错误率 这个 没有 硬性 规定 
因为 它 取决于 具体 的 问题 在 这个 问题 中 
投票 的 平均值 是 10 标准差 是 39.5 MSE 的 
平方根 是 大约 是 51 这 意味着 我们 的 平均 
误差 是 远离 真正 的 值 的 所以/c 我们/r 预测/vn 
时有/nr 很大/a 偏差/n 的/uj 可以 采取 以下 措施 来 降低 
预测 的 偏差 问题 利用 整个 数据集 进行 模型 的 
创建 因为 在 这个 实验 中 我们 只是 抽样 了 
3000个 文章 如果 利用 全部 的 数据 集 将 大大 
减少 出错率 添 加元 特征 meta features 比如 标题 的 
长度 单词 的 平均 长度 等等 利用 随机 森林 或者 
其它 更 强大 的 机器学习 算法 在 剔除 那些 少见 
或者 常见 单词 的 时候 要 尝试 不同 的 阈值 
找到 最佳 的 为止 最近 做 一个 NLP 项目 必须 
对 很多 自然语言 处理 算法 进行 了解 主要 用 Python 
因为 Python 在 人工 智 能上 的 支持 相 对比 
较好 的 下面/f 说说/i UserCF/w 和/c ItemCF/w 的/uj 区别/n 和/c 
应用/v UserCF 算法 的 特点 较少 的 用户 场合 否则 
用户 相似 度 矩阵 计算起来 很 昂贵 适用于 时效性 强 
用户 友好 性 较低 的 地区 对 新 用户 不 
友好 对 新项目 友好 因为 用户 相似 度 矩阵 不能 
实时 计算 很难 提供 用户 强制 的 建议 相应 地 
ItemCF 算法 的 功能 如下 适用于 项目 数量 明显 少于 
用户 数量 的 场合 否则 项目 相似 度 矩阵 计算起来 
很 昂贵 适用于 长尾 项目 丰富 且 用户 需求 强烈 
的 地区 由于 项目 相似 度 矩阵 不 需要 强 
实时性 cos α = A * B / | | 
A | | * | | B | | 所以 
对 用户 友好 且 对 新 项目 不 友好 使用 
用户 历史记录 行为 作为 推荐 的 解释 让 用户 更加 
信服 因此 可以 看出 UserCF/w 适用/v 于/p 项目/n 快速/d 增长/v 
且/zg 具有/v 高/a 实时/d 性能/n 的/uj 场合/n 例如 新闻 推荐 
在 图书 电子 商务 和 电影 领域 如 京东 天猫 
和 优酷 ItemCF 可以 充分 利用 其 优势 在 这些 
网站 中 用户 的 兴趣 更加 固定 和 持久 并且 
这些 网站 上 的 更新 不会 特别 快 为期 一天 
的 更新 在 公差 范围内 模型 评估 指标 准确性 召回率 
覆盖率 新颖性 惊 喜性 实时性 总结 人工智能 伪 原创 工具 
小发 猫 伪 原创 采用 这些 技术 开发 本文 将 
分 八大 步骤 来 介绍 如何 用 机器学习 处理 文本 
数据 从最/nr 简单 的 方法 开始 逐一 讲解 然后 分析 
更 具体 的 方案 细节 如 特征 工程 词 向量 
和 深度 学习 你 可以 把 本文 看作 是 标准 
方法 的 高度 概括 代码 链接 https / / github 
. com / hundredblocks / concrete _ NLP _ tutorial 
/ blob / master / NLP _ notebook . ipynb 
一 收集 数据 每 一个 机器学习 问题 都 始于 数据 
比如 一组 邮件 帖子 或是 推 文 文本 信息 的 
常见 来源 包括 商品 评价 来自 Amazon Yelp 以及 其他 
App 商城 用户 产出 的 内容 推 文 Facebook 的 
帖子 StackOverflow 的 提问 等 问题 解决 客户 请求 技术支持 
聊天记录 社交 媒体 中 的 灾难 数据 集在 这篇文章 中 
我们 将 使用 CrowdFlower 提供 的 一个 数据集 名为 社交 
媒体 中 的 灾难 Disasters on Social Media 贡献者 们 
查看 了 超过 10000 条 具有 类似 着火 隔离 混乱 
等 搜索 关键词 的 推 文 然后/c 标记/n 这个/r 推/v 
文/n 是否/v 和/c 灾难/n 事件/n 有关/vn 与之 相反 的 是 
一些 玩笑 电影 点评 或是 一些 非 灾难性 的 事件 
我们/r 的/uj 任务/n 是/v 分辨/v 出/v 哪些/r 推/v 文是/nr 真正/d 
和/c 灾难/n 事件/n 相关/v 的/uj 而 不是 一些 类似 电影 
描述 的 不 相关 话题 为什么 呢 一个 潜在 的 
应用 是 针对 突发 事件 对 执法 人员 进行 专门 
的 提醒 而 不会 被 其他 无关 信息 比如 Adam 
Sandler 新 上映 的 电影 所 干扰 这项/r 任务/n 中/f 
一个/m 特别/d 的/uj 挑战/vn 是/v 这/r 两种/m 情况/n 在/p 搜索/v 
推/v 文的/nr 时候/n 都/d 用到/v 了/ul 相同/d 的/uj 检索/vn 词/n 
所以 我们 只能 通过 细微 的 差别 去 区分 他们 
在 下面 的 文章 中 我们 将 把 与 灾难 
事件 相关 的 推 文 称为 灾难 将 其他 推 
文 称为 不 相关 的 标签 我们 已经 标注 过 
数据 所以 知道 推 文是/nr 如何 分类 的 比起 优化 
一个 复杂 的 无 监督 学习 方法 寻找 和 标记 
足够 多 的 数据 来 训练 模型 会 更加 快捷 
简单 和 廉价 二 数据 清洗 数据 科学家 的 一个 
必备 技能 是 知道 自己 的 下 一步 操作 是 
处理 模型 还是 数据 有 一个 好 的 经验 法则 
是 先 观察 数据 然后 进行 数据 清洗 一个 干净 
的 数据 集 能使 模型 学习 到 有 意义 的 
特征 而 不会 被 一些 不 相关 的 噪声 影响 
可以 借鉴 下方 的 列表 来 进行 数据 清洗 查看 
代码 获取 更多 信息 去除 一切 不 相关 的 字符 
比如 任 何非 字母 数字 的 字符 标记 你 的 
文本 将 他们 拆分 为 独立 的 单词 去除 不 
相关 的 词语 比如 @ 这类 提醒 或是 url 链接 
将 所有 字母 转换成 小写 这样 hello Hello HELLO 就会 
被 当做 同样 的 单词 处理 将 拼错 的 单词 
或是 多种 拼法 的 单词 与 某个 特定 的 表达 
绑定 比如 cool / kewl / cooool 考虑 词形 还原 
比如 将 am are is 都 看做 be 完成 这些 
步骤 并 检查 完 其他 错误 后 我们 就 可以 
使用 这些 干净 的 标 记过 的 数据 进行 模型 
训练 了 代码 https / / github . com / 
hundredblocks / concrete _ NLP _ tutorial / blob / 
master / NLP _ notebook . ipynb 三 找到 一种 
好 的 数据 表达方式 机器学习 模型 通常 以 数值 作为 
输入 我们 这里 的 数据 集 是 句子 列表 为了 
让 模型 可以 从 数据 中 学到 句子 的 特征 
模式 我们 首先 要 找到 一种 方法 来 把 它 
转换成 模型 能 理解 的 形式 即 数字 列表 独 
热 编码 One hot encoding 词 袋 模型 Bag of 
Words 通常 为 计算机 解释 文本 的 方法 是 将 
每一个 字符 都 编为 一个 独立 的 数字 例如 ASCII 
码 如果 使用 这种 简单 的 表达 来做 分类器 需要 
我们 的 数据 从头 开始 学习 词语 的 结构 这对 
大多数 数据集 来说 是 很难 实现 的 所以 我们 需要 
一种 更 上层 的 方法 例如 我们 可以 为 数据 
集中 的 所有 单词 制作 一张 词表 然后 将 每个 
单词 和 一个 唯一 的 索引 关联 每个 句子 都 
是由 一串 数字 组成 这 串 数字 是 词表 中的 
独立 单词 对应 的 个数 通过 列表 中的 索引 我们 
可以 统计 出 句子 中 某个 单词 出现 的 次数 
这种方法 叫做 词 袋 模型 它 完全 忽略 了 句子 
中 单词 的 顺序 如下 图 所示 用词 袋 模型表示 
句子 句子 在 左边 模型 表达 在 右边 向量 中的 
每一个 索引 代表 了 一个 特定 的 单词 嵌入 可视化 
在 社交 媒体 中 的 灾难 样本 词 表中 大概 
会有 20000 个 单词 这 意味着 每句 句子 都会 用 
一个 长度 为 20000 的 向量 来 表示 向量 的 
大部分 会被 0 填充 因为 每 句话 只 包含 了 
词 表中 很小 的 一个 子集 为了 看出 嵌入 的 
工作 是否 真正 抓住 了 和 问题 相关 的 信息 
比如 推 文 是否 与 灾难 相关 有 一个 好 
方法 是 将 它们 可视化 然后 观察 结果 是否 有 
很好 的 分布 考虑到 词表 通常 很大 而且 用 20000 
维 的 数据 做 可视化 是 基本 不 可能 的 
所以 我们 使用 了 PCA 这种 技术 将 数据 降到 
二维 绘制 如下 词 袋 嵌入 模型 的 可视化 结果 
两个 分类 看 起来 没有 很好 的 分离 这 可能 
是 我们 选择 的 嵌入 方法 的 特征 或是 单纯 
因为 维度 的 减少 引起 的 为了 了解 词 袋 
模型 的 特征 是否 会 起 一些 作用 我们 可以 
试着 基于 它 训练 一个 分类器 四 分类 当 初次 
接触 一个 问题 通常 来说 最好 的 方法 是 先 
挑选 一个 能 解决 问题 的 最简单 的 工具 当 
提到 数据 分类 时 一般 最 受欢迎 的 是 通用性 
和可/nr 解释性 兼具 的 逻辑 回归 算法 这种 算法 很容易 
训练 而且 结果 也是 可 解释 的 你 可以 很 
轻松 地 从 模型 中 提取 出 最 重要 的 
一些 系数 我们 将 数据 分为 两个 集合 训练 集 
用于 匹配 模型 测试 集 用于 观察 应用在 未知 数据 
上 的 效果 训练 后 我们 得到 了 75.4% 的 
精确度 结果 还 不错 推测 出现 最多 的 类 不相关 
只能 达到 57% 但是 即使 是 75% 的 精确度 也 
已经 足够 好了 我们 决不 能在 还 没有 理解 模型 
的 情况下 就 开始 应用 它 五 检验 混淆 矩阵 
理解 模型 的 第一 步 是 了解 模型 产生 的 
错误 分类 以及 最 不 应该 出现 的 错误 在 
我们 的 例子 中 误报 是 指 将 不相关的 推 
文 分类 为 灾难 事件 漏报 是 指 将与 灾难 
有关 的 推 文 归类 为 与 灾难 无关 的 
事件 如果 要 优先 处理 潜在 的 灾难 事件 那 
就要 降低 漏报 而 如果 资源 受限 就要 优先 降低 
误报 减少 错误 的 提醒 使用 混淆 矩阵 可以 很好 
地 可视化 这些 信息 并将 模型 预测 的 结果 与 
数据 的 真是 标签 进行 比较 理想 情况下 模型 的 
预测 结果 与 真实 情况 人工 标注 完全 相符 这时候 
混淆 矩阵 是 一条 从 左上角 到 右 下角 的 
对角 矩阵 混淆 矩阵 绿色 部分 所 占 比例 较高 
蓝色 部分 的 比例 较低 相比 假 阳性 结果 我们 
的 分类器 产生 了 更多 的 假 阴性 结果 换句话说 
模型 中 最 常见 的 错误 是 将 灾难性 推 
文 错误 归类 为 不相关 推 文 如果 假 阳性 
结果 的 执法 成本 很高 的话 那么 我们 分类器 的 
这种 偏差 就 是 良性 的 解释 和 说明 模型 
为了 验证 模型 并 解释 它 的 预测 结果 我们 
需要 明确 模型 用以 进行 判断 的 那些 词汇 如果 
我们 的 数据 有 偏差 而 分类器 在 样本数据 中 
却能 做出 准确 预测 那 这样 的 模型 就 无法 
在 现实 世界 中 很好 地 推广 在 这里 我们/r 
可以/c 用/p 图表/n 来/v 表示/v 灾难性/n 推/v 文与/nr 不相关/i 推/v 
文/n 两类/m 预测/vn 中/f 最重要/i 的/uj 词汇/n 由于 我们 可以 
对 模型 的 预测 系数 进行 提取 和 排序 用词 
袋 模型 bag of words 和/c Logistic/w 回归模型/i 很容易/i 就/d 
能/v 计算/v 出/v 单词/n 的/uj 重要性/n 词 袋 模型 bag 
of words 单词 的 重要性 我们 的 分类器 能够 正确 
识别 出 一些 模式 如 广岛 大屠杀 等 但 在 
一些 毫无 意义 的 词汇 如 heyoo x1392 等 上 
还是 出现 了 过拟合 词 袋 模型 bag of words 
仅能 处理 庞大 词汇 表内 的 不同 词汇 并对 所有 
的 词汇 分配 相同 的 权重 然而 其中 一些 词汇 
出现 得 非常 频繁 但却 只是 预测 结果 的 噪音 
数据 接下来 我们 将 试着 找到 一种 能够 表示 词汇 
在 句子 中 出现 频率 的 方法 尽量 让 模型 
从 数据 中 获取 更多 的 信号 六 词汇 结构 
的 统计 TF IDF/w 嵌入/v 模型/n 为了/p 让/v 模型/n 专注/v 
于/p 学习/v 更/d 有/v 意义/n 的/uj 词汇/n 我们 可以 在 
词 袋 模型 上面 使用 TF IDF 评分 术语 频率 
逆 文档 频率 TF IDF 通过 词汇 在 数据 集中 
的 稀有 程度 来 评估 它 的 重要性 适度 削弱 
出现 过于 频繁 的 单词 下图 是 TF IDF 嵌入 
模型 的 PCA 映射 可视化 TF IDF 嵌入 模型 从中 
可以 看出 两种 颜色 之间 有了 更 清晰 的 区分 
使 这 两类 数据 更 易于 被 分类器 分开 在 
新 模型 上 训练 Logistic 回归 我们 得到 了 76.2％ 
的 准确度 说明 TF IDF 确实 有助于 提高 识别 性能 
尽管 只 是 非常 微小 的 改进 但 我们 的 
模型 能否 就此 学到 更 重要 的 词汇 呢 如果 
能 得到 更好 的 结果 同时 还 能避免 模型 在 
无关 词汇 上 的 过拟合 那 TF IDF 嵌入 模型 
就 可以 被 认为 是 真正 的 升级版 模型 TF 
IDF 嵌入 模型 单词 的 重要性 可以 看到 新 模型 
学到 的 词汇 看起来 相关度 更高 尽管 测试 集 的 
指标 只是 略 有 增加 但是 我们 对 模型 的 
识别 性能 更 有把握 因此 部署 新 模型 的 交互 
系统 会 让 用户 体验 更为 舒适 七 语义 信息 
的 利用 Word2VecTF IDF 嵌入 模型 能够 学习 到 信号 
更 高频 的 词汇 然而 如果 部署 该 模型 后 
我们 很 可能 会 遇到 一些 训练 集中 从未 出现 
过 的 词汇 先前 的 模型 均 无法 正确 分类 
这样 的 新 数据 即便 其中 的 词汇 与 训练 
集 非常 相似 要 解决 这个 问题 我们 就要 捕捉 
词汇 的 语义 这 就 意味着 模型 需要 理解 好 
与 积极 在 语义上 的 距离 要比 杏 和 大陆 
更 接近 这里 的 工具 就是 Word2Vec 使用 预 训练 
的 嵌入 模型 Word2Vec 是 一种 为 单词 查找 连续 
嵌入 的 技术 通过 阅读 大量 的 文字 它 能够 
学习 并 记忆 那些 倾向于 在 相似 语境 中 出现 
的 词汇 经过 足够 的 数据 训练 之后 它 会为 
词汇表 中的 每个 单词 都 生成 一个 300 维 的 
向量 用以 记录 语义 相近 的 词汇 Word2Vec 作者 在 
一个 非常 大 的 语料库 上 预 训练 并 开源 
了 该 模型 利用 这一 语料库 我们 可以 将 一些 
语义 知识 纳入 到 我们 的 模型 内 预 训 
练好 的 词 向量 可以 在 本文 的 GitHub 代码 
库 中找到 GitHub 地址 https / / github . com 
/ hundredblocks / concrete _ NLP _ tutorial 句子 分级 
表示 让 分类器 快速 得到 句子 嵌入 的 方法 是 
先将 句中 所有 词汇 Word2Vec 得分 的 平均 化 这与 
此前 词 袋 模型 的 做法 类似 但 这里 我们 
在 保留 语义 信息 的 同时 只 丢弃 句法 Word2vec 
模型 的 句子 嵌入 利用 前面 的 可视化 技术 对 
新 模型 绘图 结果 如下 Word2Vc 嵌入 模型 的 可视化 
结果 在 这里 两组 颜色 的 分离 程度 更 大一些 
这 就 意味着 Word2Vec 能够 帮助 分类器 更好 地 分离 
这两种 类别 再一次 使用 Logistic 回归 得到 77.7％ 的 准确率 
是 我们 迄今 最好 的 结果 复杂性 / 可 解释性 
权衡 取舍 与 先前 的 模型 不同 新/a 模型/n 无法/n 
将/d 每个/r 单词/n 都/d 表示/v 成/n 一维/m 向量/n 因此 很难 
看出 哪些 词汇 与 我们 的 分类 结果 相关度 最高 
尽管 我们 仍 可使用 Logistic 回归 的 系数 但 它们 
仅 与 嵌入 的 300个 维度 相关 而与 词汇 索引 
值 并 不相关 模型 准确率 确实 提高了 但 完全 做不了 
可 解释性 分析 就 有点 得不偿失 了 不过 对于 更 
复杂 的 模型 我们 可以 利用 LIME 这样 的 黑盒 
解释器 来 稍微 解释一下 分类器 具体 是 如何 工作 的 
LIMELIME 是 Github 上 的 一个 开源 软件包 它 允许 
用户 通过 观察 输入 的 扰动 比如 在 我们 的 
例子 中 从 句中 移除 单词 来 分析 一个 特定 
分类器 的 预测 结果 是 如何 变化 的 从下 图 
来看 它 对 我们 数据 集中 几个 句子 的 解释 
正确 分类 的 灾难性 词汇 被 归类 为 相关 这个词 
对 分类 的 影响 似乎 不 太 明显 不过 我们 
没有 时间 去 逐一 探索 数据 集中 的 数千 个 
样本 我们 要 做 的 是 在 代表性 的 测试 
样本 上 运行 LIME 以此 来 分析 哪些 词汇 对于 
分类 预测 的 影响 更大 这样 我们 就 可以 像 
前面 一样 获取 到 单词 的 重要性 分数 以 验证 
模型 的 预测 结果 Word2Vec 单词 的 重要性 模型 能够 
提取 高度 相关 的 词 这 意味着 它 做出 了 
可 解释 的 决定 这些 词汇 的 相关度 是 最高 
的 因此 我们 更 愿意 在 实际 生产 中 部署 
这样 的 模型 八 使用 端 到 端 的 方式 
训练 语法 特征 我们 已经 介绍 过 如何 用 快速 
有效 的 办法 来 生成 紧凑 的 句子 嵌入 然而 
通过 省略 词汇 的 顺序 我们 也 放弃 了 语句 
的 所有 句法 信息 如果 简单 的 方法 给 不出 
令人满意 的 结果 那 我们 就 用 更为 复杂 的 
模型 将 整个 句子 作为 输入 并 预测 标签 同时 
无需 建立 中间 表示 一种 常见 的 做法 是 把 
句子 视为 词 向量 的 序列 如 使用 Word2Vec 或是 
GloVe CoVe 等 更 先进 的 方法 接下来 我们 详细 
讨论 高效 的 端 到 端的 训练 体系 结构 源 
用于 句子 分类 的 卷积 神经网络 https / / arxiv 
. org / abs / 1408.5882 训练 速度 很快 它 
作为 一种 入门级 的 深度 学习 架构 能够 很好 地 
解决 分类 问题 尽管 CNN 声名 主要 源自 它 在 
图像 处理 方面 的 出色 能力 但在 文本 相关 任务 
上 它 所 提供 的 结果 也 相当 优异 且 
相比 多数 复杂 的 NLP 方法 如 LSTM Encoder / 
Decoder 架构 等 CNN 训练 速度 也 更快 它 能够 
保留 单词 的 顺序 很好 地 学习 单词 的 序列 
特征 以及 其他 有用/nr 信息 相对于 先前 的 模型 它 
可以 区分 出 Alex eats plants 与 Plants eat Alex 
之间 差异 相比 先前 的 方法 该 模型 的 训练 
不 需 更多 的 工作 但 效果 却 好得多 准确率 
高达 79.5％ 与 前面 的 步骤 一样 下 一步 也 
要 继续 探索 并 可视化 该 模型 的 预测 结果 
以 验证 它 是否 为 最佳 模型 做到 这 一步 
你 应该 能 自己 完成 这里 的 操作 写 在 
最后 简单 回顾 一下 我们 在 各个 步骤 中 所用 
的 方法 是 这样 的 从 一个 简单 的 模型 
快速 开始 解释 模型 的 预测 理解 模型 分类 中的 
错误 样本 使用 这些 知识 来 决定 下 一步 的 
部署 上述 八大 步骤 所用 的 模型 是 我们 处理 
短 文本 时的/nr 几个 特定 实例 但 其 背后 的 
解决 方法 已经 广泛 被 用在 各类 NLP 问题 的 
实际 处理 上 原文 链接 https / / blog . 
i n s i g h t d a t 
a s c i e n c e . com 
/ how to solve 90 of nlp problems a step 
by step guide fda605278e4e 简介 前几天 用 自然 语言 处理 
技术 学习 了 一下 习 主席 的 十九 大 报告 
发布 到 朋友圈 以后 反响 比较 大 很多/m 同事/n 和/c 
朋友/n 都/d 好奇/a 我/r 是/v 怎么/r 做到/v 的/uj 由于 学习 
的 算法 比较简单 所以 我 基本上 两三 句话 都给/nr 他们 
解释 清楚 了 我 觉得 很多 人 也会 对 类似 
的 话题 感兴趣 所以 这里 要 写 这么 篇 博文 
来 document 一下 我 的 这个 非常 简单 的 自然 
语言 处理程序 给 大家 揭开 一点 迷雾 原理 分析 这个 
算法 来自 斯坦福 的 抽象 编程 课 若干 年前 叫 
cs106b 不 知道 现在 还 上不上 这门 课 主要 的 
方法 就是 对 文章 中 出现 的 每一个 字 都 
建立 一个 索引 这个 索引 里 包含 了 所有 紧 
跟在 它 后面 出现 的 字 及其 出现 的 次数 
自动 生成 文章 的 时候 会 使用 一个 seed 不断 
生成 随机数 按照 紧跟 字 出现 的 概率 和 随机 
数来 决定 下 一个 字 先 说 索引 索引 里 
包含 了 所有 紧 跟在 它 后面 出现 的 字 
及其 出现 的 次数 比如说 字 同 我们 发现 文章 
里 有 同学 这个词 于是 同 的 索引 里 就有 
学 这个字 又 发现 文章 里 有 同胞 这个词 于是 
同 的 索引 里 就又 有了 胞 这个字 索引 不光 
是 紧跟 字 的 集合 也 记录 了 每个 紧跟 
字 的 出现 次数 就拿 习 主席 的 报告 为例 
学习 完成 后 发现 培 字 后面 分别 紧跟 育 
养 和 训 三个字 他们 的 出现 次数 是 11 
10 和2/nr 当 我们 有了/nr 一个 种子 seed 的 时候 
我们 可以 查看 它 的 紧跟 列表 按照 概率 来 
选出 下 一个 字 就拿 培 打个比方 后/f 一个/m 字/n 
有/v 1123/m \/i frac/w {/i 11/m }/i {/i 23/m }/i 
的 几率 会 是 育 字 1023 \ frac { 
10 } { 23 } 的 几率 是 养 字 
223 \ frac { 2 } { 23 } 的 
几率 是 训 字 选出 下 一个 字 以后 这个字 
又 做为 新的 种子 重复 上述 的 过程 有趣 的 
是 逗号 句号 这种 标点符号 也被 当成 一个 字 所以 
新 生成 的 文章 也会有 明显 的 断句 程序实现 先 
说 我 封装 的 一个 重要 的 类 Token . 
我们 先 来看 一下 类 的 UML 图 最底层 的 
类 叫做 C h a r a c t e 
r N O c c u r r e n 
c e 这 是 一个 字符 Character 和它的/nr 出现 次数 
int 组成 的 类 它 有一个 公 有方法 叫做 addOneOccurrence 
就是 增加 一次 出现 次数 Token 类 aggregate 了 这个 
C h a r a c t e r N 
O c c u r r e n c e 
类 这是 对于 现实 中 每个 字 都有 对应 的 
索引 的 抽象 我 把 一个 字 一个 Character 对象 
和它的/nr 索引 List C h a r a c t 
e r N O c c u r r e 
n c e 封装 在 同一 个 对象 里 方便使用 
主程序 的 学习 部分 就是 读取 一个 文本文件 为 每个 
字 生成 一个 Token 实例 并用 一个 Map 把 文字 
本身 和它/nr 对应 的 Token 实例 存 储起来 以便 自动 
写 文章 的 时候 使用 Map 是 一个 Map String 
Token 的 形式 主程序 的 书写 部分 就是 hardcode 一个 
种子 到 Map 当中 去找 对应 Token 按照 概率 选出 
下 一个 字 再 重复 之前 的 操作 代码 在 
Github . 有 问题 欢迎 提问 机器 之心 报道 文本处理 
现有 数据 中 文本 是 最 非 结构化 的 形式 
里面 有 各种各样 的 噪声 如果 没有 预处理 文本 数据 
都 不能 分析 清理 和 标准化 文本 的 整个 过程 
叫做 文本 预处理 t e x t p r e 
p r o c e s s i n g 
其 作用 是 使 文本 数据 没 有噪声 并且 可以 
分析 主要 包括 三 个 步骤 移除 噪声 词汇 规范化 
对象 标准化 下图 展示 了 文本 预处理 流程 的 结构 
移除 噪声 任何 与 数据 上下文 和 最终 输出 无关 
的 文本 都可 被判 作 噪声 例如 语言 停止词 stopword 
语言 中 常用 的 词汇 系动词 is am 定冠词 the 
介词 of in URL 或 链接 社交 媒体 实体 提及 
标签 标点符号 和 特定 行业 词汇 这一步 移 除了 文本 
中 所有 类型 的 噪声 移除 噪声 通用 的 做法 
是 准备 一个 噪声 实体 的 词典 在 文本 对象 
上 逐个 token 或 逐词 迭代 消除 在 噪声 词典 
中 出现 的 标签 以下 是 实现 这 一步 的 
Python 代码 ` ` ` # a m p l 
e c o d e t o r e m 
o v e n o i s y w o 
r d s f r o m a t e 
x t n o i s e _ list = 
is a this . . . def _ remove _ 
noise input _ text words = input _ text . 
split noise _ free _ words = w o r 
d f o r w o r d i n 
w o r d s i f w o r 
d n o t i n n o i s 
e _ list noise _ free _ text = . 
join noise _ free _ words returnnoise _ free _ 
text _ remove _ noise t h i s i 
s a s a m p l e t e 
x t sampletext 另外 一种 方法 是 使用 正则表达式 尽管 
其 只能 解决 特定 模式 的 噪声 我们 在 之前 
的 文章 中 详细 介绍 了 正则表达式 https / / 
www . analyticsvidhya . com / blog / 2015/06 / 
regular expression python / 以下 是从 输入 文本 中 移除 
正则表达式 的 Python 代码 # a m p l e 
c o d e t o r e m o 
v e a r e g e x p a 
t t e r n i m p o r 
t r e d e f _ remove _ regex 
input _ text regex _ pattern urls = re . 
finditer regex _ pattern input _ text foriinurls input _ 
text = re . sub i . group . strip 
input _ text returninput _ textregex _ pattern = # 
A Za z0 9 \ w * _ remove _ 
regex removethis # h a s h t a g 
f r o m a n a l y t 
i c s v i d h y a regex 
_ pattern r e m o v e t h 
i s f r o m a n a l 
y t i c s v i d h y 
a 词汇 规范化 另外 一种 文本 形式 的 噪声 是由 
一个 词汇 所 产生 的 多种 表示 形式 例如 play 
player played plays 和 playing 这些 词汇 都 是由 play 
变化 而来 的 虽然 它们 意义 不一 但 根据 上下文 
都是 相似 的 词汇 规范化 这一步 把 一个 词 的 
不同 展现 形式 转化 为了 他们 规范化 的 形式 也叫做 
引理 lemma 规范化 是 文本 上 的 特征 工程 起 
中枢 作用 的 一步 因为 它 把 高维 特征 N 
个 不同 的 特征 转化 为了 对 任何 机器学习 模型 
都很/nr 理想 的 低 维空间 1个 特征 最 常见 的 
词汇 规范化 是 词干 提取 词干 提取 是 词汇 后缀 
ing ly es s 等 去除 过程 的 一个 基本 
规则 词形 还原 词形 还原 与 词干 提取 相反 是 
有 组织 地 逐步 获取 词汇 根 形式 的 步骤 
它 使用 到了 词汇 词汇 字典 序 和 形态 分析 
词 的 结构 和 语法 关系 下面 是 实现 词形 
还原 和 词干 提取 的 代码 使用 了 一个 流行 
的 Python 库 NLTK fromnltk . stem . w o 
r d n e t i m p o r 
t W o r d N e t L e 
m m a t i z e r l e 
m = W o r d N e t L 
e m m a t i z e r fromnltk 
. stem . p o r t e r i 
m p o r t P o r t e 
r t e m m e r s t e 
m = PorterStemmer word = multiplying lem . lemmatize word 
v multiply stem . stem word multipli 对象 标准化 文本 
数据 经常 包含 不在 任何 标准 词典 里 出现 的 
词汇 或 短语 搜索/v 引擎/n 和/c 模型/n 都/d 识别/v 不/d 
了/ul 这些/r 比如 首字母 缩略词 词汇 附加 标签 和 通俗 
俚语 通过 正则表达式 和 人工 准备 的 数据词典 这种 类型 
的 噪声 可以 被 修复 以下 代码 使用 了 词典 
查找 方法 来 替代 文本 中 的 社交 俚语 lookup 
_ dict = { rt Retweet dm directmessage awsm awesome 
luv love . . . } def _ lookup _ 
words input _ text new _ words = forwordinwords ifword 
. lower inlookup _ dict word = lookup _ dict 
word . lower new _ words . append word new 
_ text = . join new _ words returnnew _ 
text _ lookup _ words R T t h i 
s i s a r e t w e e 
t e d t w e e t b y 
h i v a m B a n s a 
l R e t w e e t t h 
i s i s a r e t w e 
e t e d t w e e t b 
y h i v a m B a n s 
a l 除了 目前 为止 讨论 过 的 三个 步骤 
其他 类型 的 文本 预处理 有 编码 解码 噪声 语法 
检查 器 和 拼写 改正 等 我 之前 的 一篇 
文章 给 出了 预处理 及其 方法 的 细节 文本 到 
特征 文本 数据 上 的 特征 工程 为了 分析 已经 
预处理 过 的 数据 需要 将 数据 转化成 特征 feature 
取决于 用途 文本 特征 可通过 句法分析 实体 / N 元 
模型 / 基于 词汇 的 特征 统计 特征 和 词汇 
嵌入 等 方法 来 构建 下面 来 详细 理解 这些 
技巧 句法分析 句法分析 涉及到 对句 中词 的 语法 分析 和 
位置 与 词汇 的 关系 的 分析 依存 语法 D 
e p e n d e n c y G 
r a m m a r 和 词性 标注 PartofSpeechtags 
是 重要 的 文本 句法 属性 依赖 树 DependencyTrees 由 
一些 词汇 共同 组成 的 句子 句 中词 与 词 
之间 的 联系 是 由 基本 的 依存 语法 决定 
的 从属关系 语法 是 一类 解决 已 标签 两个 词汇 
项 字词 间 二元 不对称 关系 的 句法 文本 分析 
每一种 关系 都 可用 三元组 关系 支配 成分 从属 成分 
来 表示 例如 考虑 下面 这个 句子 B i l 
l s o n p o r t s a 
n d i m m i g r a t 
i o n w e r e s u b 
m i t t e d b y e n 
a t o r B r o w n b 
a c k R e p u b l i 
c a n o f K a n s a 
s . 词汇 间 的 关系 可由 如下 所示 的 
树 的 形式 观察 得到 观察 树 的 形状 可得 
submitted 是 该 句 的 根词 rootword 由 两颗 子树 
所 连接 主语 和 宾语 子树 每 一颗 子树 本身 
又是 一颗 依存关系 树 dependencytree 其中 的 关系 比 如有 
Bills ports proposition 关系 ports immigration conjugation 关系 这种 类型 
的 树 当/t 从/p 上至/v 下/f 迭代/v 分析/vn 时/n 可以/c 
得到/v 语法/n 关系/n 三元组/n 对于 很多 自然语言 处理 问题 比如 
实体性 情感 分析 执行者 actor 与 实体 识别 和 文本 
分类 等 语法 关系 三元组 都 可以 用作 特征 Pythonwrapper 
的 StanfordCoreNLP http / / stanfordnlp . github . io 
/ CoreNLP / 来自 斯坦福 自然语言 处理 组 只允许 商业 
许可证 和 NTLK 从属关系 语法 可以 用来 生成 依赖 树 
词性 标注 PoS / P a r t o f 
s p e e c h t a g g 
i n g 除了 语法 关系 外 句中 每个 词 
都与 词性 名词 动词 形容词 副词 等等 联系起来 词性 标注 
标签 决定了 句中 该词 的 用法 和 作用 这里有 宾夕法尼亚大学 
定义 的 所有 可能 的 词性 标签 表 以下 代码 
使用 了 NTLK 包对/nr 输入 文本 执行 词性 标签 注释 
NTLK 提供 了 不同 的 实现 方式 默认 是 感知器 
标签 f r o m n l t k i 
m p o r t w o r d _ 
tokenize pos _ tagtext = I a m l e 
a r n i n g N a t u 
r a l L a n g u a g 
e P r o c e s s i n 
g o n A n a l y t i 
c s V i d h y a tokens = 
word _ tokenize text printpos _ tag tokens I PRP 
am VBP learning VBG Natural NNP Language NNP Processing NNP 
on IN Analytics NNP 词性 标注 被用 在 许多 重要 
的 自然 语言 处理 目的 上 A . 词义 消 
歧 一些 词汇 根据 用法 有 很多 种 意思 例如 
下面 的 两个 句子 I . P l e a 
s e b o o k m y f l 
i g h t f o r D e l 
h i II . I a m g o i 
n g t o r e a d t h 
i s b o o k i n t h 
e f l i g h t Book 在 不同 
的 上下文 中 出现 然而 这 两种 情况 的 词性 
标签 却 不一样 在 第一 句中 book 被 用作 动词 
而 在 第二 句中 它 被 用作 名词 Lesk 算法 
也 可被 用于 相同 的 目的 B . 提高 基于 
词汇 的 特征 当 词汇 作为 特征 时 一个 学习 
模型 可以 学习 到 不同 的 词汇 上下文 然而 特征 
与 词性 连接起来 上下文 就被 保留 了 因此 得到 了 
很强 的 特征 例如 句 bookmyflight I w i l 
l r e a d t h i s b 
o o k 标签 – book 2 my 1 flight 
1 I 1 will 1 read 1 this 1 带有 
POS 的 标签 – book _ VB 1 my _ 
PRP $ 1 flight _ NN 1 I _ PRP 
1 will _ MD 1 read _ VB 1 this 
_ DT 1 book _ NN 1 C . 规范化 
和 词形 归并 Lemmatizatio 词性 标签 是 将 词 转化 
为 其 基本 形式 引理 的 基础 D . 高效 
移除 停止词 词性 标 签在 移除 停止词 方面 也 非常 
有用 例如 有 一些 标签 总是 定义 低频 / 较低 
重要性 的 词汇 例如 IN – within upon except CD 
– one two hundred MD – may must 等 实体 
提取 实体 作 为特征 实体 entity 被 定义 为 句中 
最 重要 的 部分 名词 短语 动词短语 或 两者 都有 
实体 检测 算法 通常 是由 基于 规则 的 解析 词典 
查询 词性 标签 和 依存 分析 组合 起来 的 模型 
实体 检测 的 适用性 很 广泛 在 自动 聊天 机器人 
内容/n 分析器/n 和/c 消费者/n 见解/v 中都/ns 有/v 应用/v 主题 建模 
和 命名 实体 识别 是 自然 语言 处理 领域 中 
两种 关键 的 实体 检测 方法 A . 命名 实体 
识别 NER / N a m e d E n 
t i t y R e c o g n 
i t i o n 从 文本 中 检测 命名 
实体 比如 人名 位置 公司 名称 等 的 过程 叫做 
命名 实体 识别 NER 例如 句 SergeyBrin t h e 
m a n a g e r o f G 
o o g l e I n c . i 
s w a l k i n g i n 
t h e s t r e e t s 
o f N e w Y o r k . 
命名 实体 人 SergeyBrin 公司名 GoogleInc . 位置 NewYork 典型 
NER 模型 包含 三个 模块 名词 短语 识别 使用/v 从属关系/l 
分析/vn 和/c 词性/n 分析/vn 将/d 所有/b 名/q 词性/n 短语/nz 从/p 
文本/n 中/f 提取/v 出来/v 短语 分类 将 提取 出 的 
名词 短语 分类 到 各自 的 目录 位置 名称 等 
中 谷歌 地图 API 提供 了 通往 消除歧义 位置 的 
很好 路径 然后 dbpedia 维基 百科 的 开源 数据库 可以 
用来 识别 人名 或 公司名 除了 这个 我们/r 能/v 通过/p 
结合/v 不同/a 来源/n 的/uj 信息/n 精确/a 的/uj 查找表/i 和/c 词典/n 
实体 消 歧 有些 时候 实体 可能会 误 分类 因此 
在 结果 层 上 建 一层 交叉 验证 层 非常 
有用 知识图谱 就 可以 用 来 使用 目前 流行 的 
知识 图谱 有 谷歌 知识图谱 IBMWatson 和 维基百科 B . 
主题 建模 主题 建模 是 自动 识别 文本 集中 主题 
的 过程 它 以 无 监督 的 方式 从 语料库 
中的 词汇 里 提取 隐藏 的 模式 主题 topic 被 
定义 为 文本 集中 共同 出现 术语 的 重复 模式 
一个 好 的 主题 模型 能对 健康 医生 病人 医院 
建模 为 健康 保健 农场 作物 小麦 建模 为 耕作 
隐含 狄利克雷 分布 LDA 是 最 流行 的 主题 建模 
技术 以下 是 在 Python 环境 下 使用 LDA 技术 
实现 主题 建模 的 代码 若想 查看 更 详细 的 
细节 请 参看 https / / www . analyticsvidhya . 
com / blog / 2016/08 / beginners guide to topic 
modeling in python / doc1 = u g a r 
i s b a d t o c o n 
s u m e . M y s i s 
t e r l i k e s t o 
h a v e s u g a r butnotmyfather 
. doc2 = M y f a t h e 
r s p e n d s a l o 
t o f t i m e d r i 
v i n g m y s i s t 
e r a r o u n d t o 
d a n c e p r a c t 
i c e . doc3 = D o c t 
o r s s u g g e s t 
t h a t d r i v i n 
g m a y c a u s e i 
n c r e a s e d s t 
r e s s a n d b l o 
o d p r e s s u r e 
. doc _ complete = doc1 doc2 doc3 doc _ 
clean = doc . split fordocindoc _ complete i m 
p o r t g e n s i m 
f r o m g e n s i m 
i m p o r t c o r p 
o r a # C r e a t i 
n g t h e t e r m d 
i c t i o n a r y o 
f o u r c o r p u s 
w h e r e e v e r y 
u n i q u e t e r m 
i s a s s i g n e d 
a n i n d e x . dictionary = 
corpora . Dictionary doc _ clean # C o n 
v e r t i n g l i s 
t o f d o c u m e n 
t s corpus i n t o D o c 
u m e n t T e r m M 
a t r i x u s i n g 
d i c t i o n a r y 
p r e p a r e d a b 
o v e . doc _ term _ matrix = 
dictionary . doc2bow doc fordocindoc _ clean # C r 
e a t i n g t h e o 
b j e c t f o r L D 
A m o d e l u s i n 
g g e n s i m l i b 
r a r y L d a = gensim . 
models . ldamodel . LdaModel # R u n n 
i n g a n d T r a i 
n i n g L D A m o d 
e l o n t h e d o c 
u m e n t t e r m m 
a t r i x l d a m o 
d e l = Lda doc _ term _ matrix 
num _ topics = 3 id2word = dictionary passes = 
50 # Resultsprint ldamodel . print _ topics C . 
N Grams 特征 N Grams 是 指 N 个 词汇 
的 结合体 N Grams N 1 作为 特征 与 词汇 
Unigrams 作 为特征 相比 通常 会 更加 富含 信息 同时 
bigrams N = 2 被 认为 是 最重要 的 特征 
以下 代码生成 了 文本 的 bigrams defgenerate _ ngrams text 
n words = text . split output = foriinrange len 
words n + 1 output . append words i i 
+ n returnoutput generate _ ngrams t h i s 
i s a s a m p l e t 
e x t 2 # this is is a a 
sample sample text 统计 特征 文本 数据 使用 该 节 
所讲 的 几种 技术 可 直接 量 化成 数字 A 
. 术语 频率 逆 文献 频率 TF – IDF TF 
IDF 是 经常 被 使用 在 信息检索 问题 上 的 
权重 模型 TF IDF 在 不考虑 文献 中词 的 具体 
位置 情况 下 基于 文献 中 出现 的 词汇 将 
文本 文献 转化成 向量 模型 例如 假设 有 一个 N 
个 文本 文献 的 数据 集 在 任何 一个 文献 
D 中 TF 和 IDF 会被 定义 为 术语 频率 
TF 术语 t 的 术语 频率 被 定义 为 t 
在 文献 D 中 的 数量 逆 文献 频率 IDF 
术语 的 逆 文献 频率 被 定义 为 文本 集中 
可用 文献 的 数量 与 包含 术语 t 的 文献 
的 数量 的 比例 的 对数 TF IDF 公式 给出 
了 文本 集中 术语 的 相对 重要性 以下 为 TF 
IDF/w 公式/n 和/c 使用/v Python/w 的/uj scikit/w 学习/v 包将/nr 文本/n 
转换/v 为/p tf/w idf 向量 fromsklearn . feature _ extraction 
. t e x t i m p o r 
t T f i d f V e c t 
o r i z e r o b j = 
TfidfVectorizer corpus = T h i s i s s 
a m p l e d o c u m 
e n t . a n o t h e 
r r a n d o m d o c 
u m e n t . t h i r 
d s a m p l e d o c 
u m e n t t e x t X 
= obj . fit _ transform corpus printX 0 1 
0.345205016865 0 4 . . . 0.444514311537 2 1 0.345205016865 
2 4 0.444514311537 模型 创建 了 一个 词典 并给 每 
一个 词汇 赋 了 一个 索引 输出 的 每 一行 
包含 了 一个 元组 i j 和在 第 i 篇 
文献 索引 j 处 词汇 的 tf idf 值 B 
. 数量 / 密度 / 可读性 特征 基于 数量 或 
密度 的 特征 同样 也 能被 用于 模型 和 分析 
中 这些 特征 可能 看 起来 比较 繁琐 但是 对 
学习 模型 有 非常 大 的 影响 一些 特征 有 
词数 句数 标点符号 数 和 特定 行业 词汇 的 数量 
其他 类型 的 测量 还包括 可读性 测量 比如 音节 数量 
smogindex 和 易读性 指数 参考 Textstat 库 创建 这样 的 
特征 https / / github . com / shivam5992 / 
textstat 词 嵌入 文本 向量 词 嵌入 是 将 词 
表示 为 向量 的 方法 在 尽量 保存 文本 相似性 
的 基础 上将 高维 的 词 特征向量 映射 为 低维 
特征向量 词 嵌入 广泛 用于 深度 学习 领域 例如 卷积 
神经 网络 和 循环 神经网络 Word2Vec 和 GloVe 是 目前 
非常 流行 的 两种 做 词 嵌入 的 开源 工具包 
都是 将 文本 转化 为 对应 的 向量 Word2Vec https 
/ / code . google . com / archive / 
p / word2vec / GloVe http / / nlp . 
stanford . edu / projects / glove / Word2Vec 是由 
预处理 模块 和 两个 浅层 神经网络 CBOW / C o 
n t i n u o u s B a 
g o f W o r d s 和 Skip 
gram 组成 这些 模型 广泛 用于 自然语言 处理 问题 Word2Vec 
首先 从 训练 语料库 中 组织 词汇 然后 将 词汇 
做 词 嵌入 得到 对应 的 文本 向量 下面 的 
代码 是 利用 gensim 包 实现 词 嵌入 表示 fromgensim 
. m o d e l s i m p 
o r t W o r d 2 V e 
c s e n t e n c e s 
= data science vidhya science data analytics machine learning deep 
learning # t r a i n t h e 
m o d e l o n y o u 
r c o r p u s m o d 
e l = Word2Vec sentences min _ count = 1 
printmodel . similarity data science 0.11222489293 p r i n 
t m o d e l learning array 0.004593560 . 
00303564 0.004676220 . 00209638 . . . 这些 向量 作为 
机器 学习 的 特征向量 然后 利用 余弦 相似性 单词 聚 
类 文本 分类 等 方法 来 衡量 文本 的 相似性 
人工智能 将 改变 世界 想学 AI 首选 Python 语言 大家 
知道 人工智能 改变 的 不仅 是 商业 运作 的 方式 
还 涉及 到 社会 的 方方面面 从 使用 图像 识别 
技术 增强 公共 安全 到 借助 自然语言 处理 提供 自动 
的 人性化 服务 人工智能 都 起到 了 很大 的 作用 
那么 人工智能 培训 就业 前景 如何 人工智能 发展 前景 很好 
中国 正在 产业 升级 工业 机器人 和 人工智能 方面 都会 
是 强烈 的 热点 而且 正好 是 在 3 ~ 
5年 以后 的 时间 所以 也 可以 很 清楚 地 
知道 现在 学习 人工智能 真的 很有必要 对于 学习 人工智能 的 
人 来说 学习 质量 的 好坏 直接 影响 到 学习 
结果 的 好坏 所以 选择 一个 不错 的 学校 就 
比较 重要 那么 对于 一个 新 手 来说 最 重要 
的 应该 是 选择 一家 靠谱 的 人工智能 学校 如果 
你 想 学习 人工智能 但是 却 不 知道 哪个 好 
那么 教 你 一个 办法 不要 去 看 那些 便宜 
的 筛选 出 一批 正常 的 然后 一个 个 去 
了解 咨询 最后 结合 其他 方面 做 一个 对比 这样 
就 能 找到 比较 不错 的 人工智能 学校 鉴别 好坏 
的 方法 之一 就是 去 看下 口碑 如何 选择 学校 
口碑 不错 的 深入 了解 这样 就 可以 找到 比较 
好 的 人工智能 学校 目前 在 保障 就业 的 学校 
中 有的 会 保障 就业 薪资 的 水平 而 有的 
不会 从 学员 角度 来说 能够 保障 就业 薪资 的 
肯定 实力 更 加强 更好 不然 也 不会 随便 做出 
这样 的 承诺 所以 如果 条件 允许 选择 有 薪资 
保障 的 会 更好 我们 还要 先 了解 下 它 
的 课程 内容 是否 全面 课程 内容 是否 是 比较 
新的 一般 人工智能 课程 内容 包含 python 数据库 无 监督 
学习 有 监督 学习 深度 学习 算法 TensorFlow CNN 以及 
多个 人工智能 相关 的 项目 内容 这样 的 是 比较 
全面的 所以 尽可能 多 了解 对比 不同 学校 之前 的 
课程 内容 选择 课程 内容 比较 全面 专业 的 Python 
是 人工智能 的 首选 语言 因此 大家 知道 掌握 Python 
技术 的 重要性 了吗 对于 人工智能 学习者 来说 我们 需要 
从 多个 方面 观察 了解 对比 之后 再 进行 决定 
当然 个人 建议 一定 要 上门 实地考察 下 同时 试听 
下 课程 这样 你 就 可以 知道 好 不好 适 
不 适合 自己 本次 首先 推荐 邱锡鹏/nr 老师 的 两个 
报告 1 . Deep learning for natural language processinghttp / 
/ nlp . fudan . edu . cn / xpqiu 
/ slides / 20160618 _ DL4NLP @ CityU . pdf 
主要 讨论 了 深度 学习 在 自然 语言 处理 中 
的 应用 其中 涉及 的 模型 主要 有 卷积 神经网络 
递归 神经网络 循环 神经网络 网络 等 应用 领域 主要 包括 
了 文本 生成 问答 系统 机器翻译 以及 文本 匹配 等 
卷积 神经 网络结构 示意图 递归 神经 网络结构 示意图 循环 神经网络 
示意图 2 . 神经 网络 与 深度 学习 http / 
/ nlp . fudan . edu . cn / xpqiu 
/ slides / 20151226 _ CCFADL _ NNDL . pdf 
这个 报告 可以 看作 上个 报告 的 简短 中文版 其中 
涉及 的 主要 模型 有 卷积 神经网络 循环 神经 网络 
以及 前馈 神经 网络 等 前馈 神经网络 示意图 另外 推荐 
两篇 来自 ACL 2016 的 tutorial1 . 语义 表示 相关 
的 Tutorial 链接 如下 http / / wwwusers . di 
. uniroma1 . it / ~ collados / Slides _ 
ACL16Tutorial _ e m a n t i c R 
e p r e s e n t a t 
i o n . pdf2 . 短 文本 理解 相关 
的 tutorial 链接 如下 http / / www . wangzhongyuan 
. com / tutorial / ACL2016 / Understanding Short Texts 
/ 最后 分享 几篇 搜索 意图 识别 相关 的 论文 
1 . Query Intent Detection using Convolutional Neural Networks 这篇 
论文 利用 卷积 神经 网络 来 检测 查询 意图 链接 
如下 http / / people . cs . pitt . 
edu / ~ hashemi / papers / QRUMS2016 _ HBHashemi 
. pdfpeople . cs . pitt . edu / ~ 
hashemi / papers / QRUMS2016 _ slides . pdf2 . 
  Deep LSTM based Feature Mapping for Query Classification 基于 
深度 学习 中的 LSTM 用于 查询 分类 链接 如下 https 
/ / aclweb . org / anthology / N / 
N16 / N16 1176 . pdf 长 短时记忆 网络结构 示意图 
3 .   Understanding User s Query Intent with Wikipedia 
这篇 论文 利用 维基百科 来 理解 用户 查询 意图 http 
/ / wwwconference . org / www2009 / proceedings / 
pdf / p471 . pdf 今日 感想 窗外 是 瓢泼大雨 
来到 杭州 后 最大 的 变化 就是 说话 少了 白天 
上班 部 门里 的 IT 男 神 们 只顾 打 
代码 如飞 闭口 不语 晚上 回到 出租屋 也 只剩 一个人 
的 独处 要么 看书 看 论文 要么 写写 博客 和 
日志 要么 弹弹 古筝 哼个 跑调 的 小曲 或是 去 
舞蹈室 跳 1 小时 舞 满/a 背/v 大汗/ns 得/ud 走过/v 
太/d 寂静/a 的/uj 紫荆/nr 文路/nr 把 所有 的 期待 藏进 
独自 妖艳 的 夜色 然而 又 总是 满心欢喜 欢喜 这 
日复一日 枯燥 与 无味 背后 沉默 着 的 成长 与 
坚持 课程 来源 吴恩 达 深度 学习 课程 序列 模型 
笔记 整理 王 小草 时间 2018年 5月 5 日本 文 
记录 的 是 自然 语言 处理 中 扮演 重要 觉得 
的 词 嵌入 向量 它 几乎 是 许多 NLP 项目 
的 底层 基础 对于 词 向量 的 深入 学习 将 
帮助 你 在 解决 其他 NLP 问题 上 有 非常 
大 的 提升 一起来 看看 吧 1 . 词汇 表征 
1.1 one hot 词 编码 的 缺陷 回顾 上 一周 
的 词 向量 表示 方式 one hot 编码 即 根据 
拥有 的 尽可能 多 的 语料 整理 一份 词典 词典 
长度 为 n 使得 每个 词 对应 一个 n * 
1 的 词 向量 其中 该词 索引 所在 的 位置 
为 1 其余 位置 为 0 . 比如 如 下图 
woman 这个词 在 索 引为 9853 的 位置 上 是 
1 其余 位置 为 0 这 就是 one hot 方式 
的 word representation . one hot 的 词汇 表征 很简单 
但是 也 有 致命 缺点 它 无法 表示 词 与 
词 之间 的 相似性 举个 简单 的 例子 I want 
a glass of orange _ _ . 假设 我 先 
告诉 你 空格 是 填 的 是 juice 然后 再给 
你 下面 这个 句子 I want a glass of apple 
_ _ . 聪明 的 宝宝 你 肯定 马上 说 
诶 也 可以 填 juice 啊 因为 你 知道 orange 
与 apple 都是 水果 它们 在 某种 意义 上 有 
相似性 但是 如果 我们 使用 的 是 one hot 形式 
对词 进行 编码 的话 我们 完全 无法 根据 词 向量 
来 计算 词 与 词 之间 的 相似性 而 两个 
one hot 词 向量 的 内积 永远 也 等于 01.2 
特征 化 表征 featurized repredentation word embedding 什么 叫做 特征 
化 的 表示 比如 选 一个 特征 是 gender 于是 
每个 词 都可以 评估 出 一个 与 gender 相似性 的 
值 man 为 1 woman 为 1 而 anpple 与 
gender 完全 无关 为 0 以此类推 如下 表示 然后 又 
可以 再选 第二 个 特征 比如 royal age 在 每个 
特征 维度 每 一个 词 都 可以 有一个 对应 的 
值 以 表示 该词 在 该 特征 维 度上 的 
信息 相关度 假设 有 300个 特征 那么 每个 词 就会 
形成 一个 300 * 1 的 词 向 量量 向量 
的 每个 维度 都有 特定 特征 的 含义 由于 man 
和 woman 是 很 相近 的 词 它们 在 很多 
特征 维度 上都 有 相近 的 值 因此 这 两个 
向量 的 距离 会 很近 即 内积 获得 的 相似性 
会 很高 因此 再拿 这个 例子 来说 由于 orange 与 
apple 的 词 向量 相似性 高 因此 可以 根据 orange 
后面 填 juice 推到 出 apple 后面 也 可以 填 
juice . I want a glass of orange _ _ 
. I want a glass of apple _ _ . 
总之 特征 化 的 表示 能比 one hot 更 好得 
表示 不同 的 词 但是 要 注意 的 是 实际上 
的 词 向量 并 不是 有 清晰 直观 的 特征 
告诉 你 第 一维 是 性别 第 二维 是 高贵 
等等 而是 比 这 复杂 得多 但 我们 可以 去 
这样 理解 就是 向量 中的 每 一维 都 代表 着 
某个 特征 1.3 可视化 词 潜入 visualizing word embedding 假设 
我们 已经 获得 了 300 维 的 词 向量 那么 
可以 将 它 降 维 到 2 维空间 并且 画在 
二维 坐 标上 如下 可见 相似 的 词 会被 聚在一起 
常见 的 可视化 算法 有t/nr SNE 算法 来自 于 laurens 
van der maaten 和 Geoff Hinton 的 论文 最后 说一说 
为什么 这个 方法 叫做 embedding 嵌入 想象 一个 300 维 
的 空间 一个词 对应 多 300 维 的 向量 就 
像是 嵌 在 这个 空间 中 的 一个 点 因此 
取名为 嵌入 嘿嘿嘿 2 . 词 嵌入 的 应用 2.1 
词 嵌入 在 命名 实体 识别 中的 应用 知道了 词 
嵌入 是个 什么 东东 那么 就 来 看看 词 嵌入 
可以 如何 使用 用 起来 到底 爽 在哪里 还是 以 
实体 命名 的 例子 来做 介绍 假设 有 这样 一个 
句子 其中 Sally Johnson 是 一个 人 名 因此 它们 
对应 的 预测 应该 是 1 其余 词 的 预测 
为 0 . 我们 之所以 判断 Sally Johnson 是 一个 
人 名 而非 公司名 是 因为 这 句话 的 后面 
说 了 Sally Johnson 是 一个 farmer 农民 fammer 自然 
是 一个人 了 假设 用 以上 句子 进行 训练 模型 
并对 以下 句子 进行 预测 由于 训练 中 已经 知道 
了 后面 出现 了 farmer 那么 前面 的 实体 应该是 
人名 因此 Robert Lin 的 预测 为 1 . 在 
这个 例子 中 使用 one hot/w 或者/c 词/n 嵌入/v 或许/d 
都能/nr 正确/ad 识别/v 出/v 但是 假设 把 apple farmer 改成 
durian cultivator 榴莲 培育 家 呢 训练 集中 从未 出现 
过 durian 和 cultivator 这 两个 词 于是 one hot 
方式 就 傻眼 了 但是 词 嵌入 的 方式 却 
仍然 游刃有余 因为 词 嵌入 的 表征 可以 体现 词 
与 词 之间 的 相似 关系 而 apple 与 durain 
farmer 与 cultivator 有 很大 的 相似性 因此 虽然 训练 
集中 压根 就 没有 学 到过 这 两个 词 模型 
也 可以 预测 出 durian cultivator 也 是 一个 人 
以上 可以 看到 就算 我们 的 训练样本 比较 少 没有 
覆盖 尽可能 多 的 词 或 样本 类型 模型 还是 
可以 根据 词 嵌入 向量 来做 更 准确 的 预测 
这里 词 嵌入 的 表征 方式 简直 功不可没 那么 词 
嵌入 是 如何 得来 的 呢 你 可以 考察 很大 
的 数据 集 可是 是 一亿 或 这 100 亿个 
词 来自 于不/nr 需要 标注 的 文本 然后 对文 进行 
学习 获得 词 嵌入 的 向量 别急 怎么 学 后面 
会 详细 讲述 这个 大 文本 自然 是 越大 越好 
尽可能 得 包含 所有 的 词 其中 就有 durain 和 
cultivator 于是 你 就 可以 发现 durain 和 apple 等 
水果 很 相近 用 大量 的 文本 训 练出 词 
嵌入 然后 将 词 嵌入 运用 到 只有 小量 样本 
的 模型 中 这 就是 运用 了 迁移 学习 另 
一点 要 注意 的 是 上 图画 的 是 一个 
单向 的 RNN 实际上 做 命名 实体 识别 一般 使用 
的 是 双向 RNN 2.2 总结 词 嵌入 做 迁移 
学习 的 步骤 1 从 大量 的 文本 语料 中 
学习 词 嵌入 的 向量 1 100亿 词 或者 直接 
从 网上 下载 别人 与 训练 好 的 向量 2 
将 词 嵌入 迁移 到 你 只有 少量 样本 的 
任务 中 使得 用 几百 维 的 向量 代替 之前 
上 万维 的 one hot 向量 3 在 新的 数据 
上 微调 词 嵌入 向量 但是 若 你 的 样本数据 
很少 一般 就 不做 微 调了 对 迁移 学习 再多 
说一句 当 有 两个 任务 A B 在 A 任务 
中 你 有 大量 的 标注 的 数据 而 B 
中 却 只有 少量 于是/nr 可以 将 在 A 中 
学习 到 的 东西 迁移 到 B 中 以 弥补 
B 因为 样本 少 而 导致 的 缺陷 2.3 词 
嵌入 与 人脸 编码 词 嵌入 与 人脸 编码 有 
些些 奇妙 的 关系 在 使用 卷积 神经 网络 进行 
人脸 对比 时 过程 如 下图 输入 一张 图片 一层 
一层 计算 后 最后 会 得到 一个 向量 比如 128 
维 然后 去 比较 两张 图片 的 这两个 向量 的 
相似性 即对 图片 进行 了 编码 词 嵌入 也 差不多 
对词 进行 了 编码 因此 两者 有 相似之处 两者 的 
不同 之处 是 输入 任何 一张 图片 都能/nr 得到/v 一个/m 
图像编码/n 的/uj 向量/n 而 词 嵌入 是 需要 事先 确定 
词库 假设有 1 亿个 词 参与 了 训练 如果 出现 
另 一个 新词 那么 将 无法 得到 新词 的 词 
向量 如果 没有 看懂 这句话 别急 看完 接 下去 的 
内容 就会 一目了然 了 3 . 词 嵌入 的 特性 
3.1 词 嵌入 的 类比推理 特性 词 嵌入 还有 一个 
很 迷人 的 特性 那 就是 帮助 类比推理 尽管 类比推理 
在 NLP 的 应用 中 不是 最 重要 的 角色 
不过 它 帮助 人们 认识 词 嵌入 到底 做 了 
什么 假设 我们 已经 获取 了 以下 这些 词 的 
特征 表示 并且 假 设词 嵌入 就是 以下 4个 维度 
的 向量 现在 告诉 你 man 对应 的 是 woman 
man woman 问 king 对应 的 是 什么 词 king 
聪明 的 宝宝 你 肯定 会 说 king 当然 对应 
的 是 queen 了 是的 你 知道 但是 计算机 不 
知道 啊 因此 我们 可以 借 助词 嵌入 的 特性 
去 通过 类比 找到 这个 对应 的 词 正 如上图 
我们 知道 了 man woman 的 词 向量 分别 用 
e _ man e _ woman 表示 将 这 两个 
词 向量 相减 得到 一个 向量 同理 将 e _ 
king 与 e _ queen 相减 以上 现象 可见 通过 
词 嵌入 可以 进行 类比推理 根据 man woman 找到 king 
于是 我们 现在 来 把 上述 过程 写成 算法 的 
形式 也 就是 找 出 一个 词 w 使得 它 
的 词 向量 e _ w 与 e _ king 
e _ man + e _ woman 的 向量 最 
相近 3.2 余弦 相似性 前面 讲了 辣 么 多词 相似性 
相似性 相似性 但是 相似性 到底 如何 计算 呢 常用 的 
词 向量 之间 的 相似性 一般用 余弦 相似性 计算 u 
v 是 两个 向量 其实 就是 根据 他们 的 夹角 
的 来 判断 相似性 通过 夹角 来 计算 余弦 当 
夹角 = 0 余弦 = 1 两个 向量 完全一致 当 
夹角 维 90 余弦 维 0 不相似 当 夹角 为 
180 则 余弦 为 1 两个 向量 完全 相反 当然 
咯 还有 很多 计算 相似性 的 方法 比如 欧式 距离 
4 . 嵌入 矩阵 Embedding matrix 上面 几节 分别 讲了 
什么 是 词 嵌入 词 嵌入 的 应用 与 特性 
现在开始 要 具体 讲 一 讲 我们 到底 是 如何 
得到 这个 神奇 的 词 嵌入 的 获 取词 嵌入 
其实 就是 去 求 一个 嵌入 矩阵 Embedding matrix 于是 
这 一节 先来 介绍 下 什么 是 嵌入 矩阵 假设有 
10000个 词 的 词典 若按 字母 排 就是 从a/nr aaron 
. . . . . . orange . . . 
. . . zulu UNK 我们 要 做 的 是 
学习 一个 嵌入 矩阵 大小 是 300 * 10000 这个 
矩阵 的 每 一列 代表 的 是 每个 词 的 
向量 要 得到 每个 词 在次 嵌入 矩阵 中 对应 
的 向量 使 嵌入 矩阵 乘以 这个词 的 one hot 
向量 即可 嵌入 矩阵 记为 E 维度 是 300 * 
10000 某个 词 one hot 词 向量 比如 orange 是 
排在 词典 的 6257位 记为 O _ 6257 因此 它们 
的 积 E * O _ 6257 会 得到 一个 
300 * 1 的 向量 其实 就是 这个 词 在 
嵌入 举证 对应 的 那 一列 向量 推而广之 某个 词 
Oj 的 嵌入 向量 j 是 该词 在 词典 中 
的 位置 就是 嵌入 矩阵 乘以 该词 的 one hot 
向量 因此 这 一节 你 只 需要 知道 我们 需要 
去 训练 这样 一个 嵌入 矩阵 然后 用 这个 嵌入 
矩阵 与 one hot 向量 相乘 可以 得到 嵌入 向量 
但 需要 注意 的 是 上面 我们 好像 看似 轻而易举 
得 得到 了 嵌入 矩阵 与 one hot 向量 相乘 
的 结果 但在 实际 计算 中 由于 向量 的 维数 
巨大 相乘 的 操作 会 带来 巨大 的 计算 量 
因此 实际 中 往往 直接 根据 词 的 索引 去 
取出 嵌入 矩阵 中 对应 的 那 一列 向量 5 
. 学习 词 嵌入 好了 现在 我们 真的 要 开始 
去 一步 一步 了解 模型 到底 是 如何 学习 出 
词 嵌入 的 在 深度 学习 的 历史 上 人们 
曾经 用 很 复杂 的 模型 结构 来 训练 词 
嵌入 随着 不断 得 探索 现在 我们 已经 可以 用 
很 简单 的 模型 结构 来 训练 出 非常好 的 
效果 特别是在 大 数据 样本 的 情况 下 但是 我们 
仍然 从 最初 的 复杂 模型 开始 讲起 这样 你 
才能 更 深入 得 理解 简单 的 模型 到底 为什么 
会 取得 好 的 效果 真的 超 爱 吴恩 达 
这种 把 一个 知识 点 从头到尾 串 起来 系统 讲解 
的 姿势 假如 你 要 构建 一个 语言 模型 要根据 
前面 的 单词 预测出 下面 这 句话 空格 中 的 
单词 单词 下方 的 数字 是 该词 在 词典 中的 
索引 位置 实践证明 建立 一个 语言 模型 是 学习 词 
嵌入 的 好 方法 因此 我们 现在 来 建立 一个 
神经 网络 预测 序列 中的 下一个 单词 1 首先 空格 
前面 的 每个 单词 都匹/nr 配上 对应 的 one hot 
向量 2 然后 去 乘以 一个 嵌入 矩阵 E 一 
开始 这个 E 是 一个 随机 初始化 的 300 * 
10000 的 矩阵 3 接着 相乘 后 得到 每个 词 
的 嵌入 向量 如 e _ 4343 4 所有 词 
嵌入 向量 都 作为 神经 网络 的 输入 5 经过 
一个 隐 层 之后 再输入 softmax 层 做 10000 词典 
的 长度 的 分类 即 输出 层 有 10000个 神经元 
输出 后可/nr 形成 1 * 10000 的 向量 最好 的 
预期 是 这个 向量 中 只有 该词 所在 的 索引 
位置 为 1 其他 位置 为 0 . 过程 如下 
将 每次 输出 的 1 * 10000 维 向量 与 
真实 期望 的 词 向量 上面 是 juice 计算 损失 
通过 梯度 下 降法 去 调整 嵌入 矩阵 中的 值 
使得 潜入 矩阵 越来 越能 优秀 地 表征 对应 的 
词 在 实际 中 往往 会 设置 一个 固定 长度 
的 窗口 比如 5 意思 是 用 前 4个 词 
去 预测 后 一个 词 这样 就 可以 去 适应 
非常 长 的 句子 了 不 单单 可以 设置 不同 
长度 的 窗口 还 可以 使用 前后文 比如 用 该词 
的 前面 4个 词 后 4个 词 来 预测 该词 
也 可以 只用 前面 一个 词 或者 前后 一个词 等 
但 实践证明 如果 你 的 目的 是 训练 一个 语言 
模型 那么 使用 前后 4个 次 可能 效果 更好 若 
你 等 目的 是 得到 词 嵌入 矩阵 那么 使用 
前后 1个 词 也会 很好 因此 综上所述 通过 以上 训练 
语言 模型 的 过程 就 可以 顺便 得到 了 词 
嵌入 矩阵 这是 早期 最 成功 的 词 嵌入 学习 
算法 之一 6 . word2vec 上 一节 介绍 了 一个 
复杂 版本 的 词 嵌入 算法 现在 来 介绍 一个 
更 简单 更 灵活 的 模型 来 获得 词 嵌入 
其中 一种 叫做 Skip grames 本节 内容 的 大多数 思想 
来自 与 Tomas Mikolov Kai Chen Greg Corrado 和 Jeff 
Dean . 6.1 Skip grames 假 设给 你 这样 一个 
句子 在 skip grames 模型 种 要做 的 是 抽取 
上下文 context 与 目标 词 target 配对 来 构造 一个 
监督 学习 问题 上下文 并不 一定 要是 前 一个 词 
或者 离得 最近 的 四个 单词 之类 而是 随机 选择 
一个 词 作为 上下 文词 比如 随机 选择 一个 context 
词 orange . 接着 再 随机 选择 一个 在 一定 
词 距 中的 target 词 比如 随机 选 到了 juice 
或者 随机 选 到了 前面 的 词 glass 词 距 
是 一 开始 认为 设定 的 比如 前后 10个 词中 
随机 选 显然 这 不是 一个 简单 的 监督 学习 
因为 context 词 前后 n 个 词 距 中 许多 
不同 词 但 构造 这个 监督 学习 模型 并 不是 
去 解决 模型 本身 的 问题 而是 想 通过 这个 
训练 过程 去 得到 中间 的 词 嵌入 矩阵 现在 
来 讲讲 模型 的 细节 假设 仍然 使用 一个 10000 
词 的 词表 当然 实际上 要 大得多 并且 已经 随机 
取了 一对 context 和 target 词 比如 分别 是 context 
orange – target juice . 模型 的 过程 和上/nr 一节 
一样 输入 context 词 的 one hot 词 向量 – 
乘以 初始化 的 词 嵌入 矩阵 E – 得到 词 
嵌入 向量 – 经过 softmax 层 – 输出 词汇表 大小 
长度 的 词 向量 ysoftmax 层 的 计算 公式 如下 
损失 函数 如下 是 两个 one hot 词 向量 的 
差值 之和 y 是 context 词 的 one hot 词 
向量 y 帽 是 模型 softmax 层 的 输出 也 
是 一个 长度 与 前者 相当 的 词 向量 这个 
模型 种 有 两类 参数 一个 是 词 嵌入 矩阵 
E 中的 值 一个 somtmax 层 中的 参数 随着 损失 
函数 的 最小化 这 两类 参数 都会 得到 优化 并且 
越来越 准确 从而 我们 就 得到 了 我们 最终 想要 
的 词 嵌入 矩阵 啦 以上 就是 skip gram 模型 
6.2 problems with softmax classification 但是 上面 讲述 的 Skip 
grames 模型 有 一个 很大 的 缺点 就是 计算 量 
太大 了 来看 softmax 的 计算 公式 分母/n 部分/n 需要/v 
对/p 词汇表/n 中的/i 每个/r 词/n 都/d 计算/v 后/f 求和/v 一般 
情况 下 词汇表 都会 很大 因此 求和 操作 是 相当 
慢 的 那么 如何 解决 呢 下面 就 来讲 一 
讲 6 . 2.1 分级 softmax 分类器 在 一些 文献 
中 你 会 看到 hierarchical softmax classider 什么 意思 呢 
也 就是 在 sofmax 层 不 一次 到位 求出 每个 
词 的 概率 而是 通过 分类 的 方式 第一 个 
分类器 告诉 你 这个 词 是 在 词汇表 的 5000 
前 还是 后 第二个 分类器 告诉 你 是 在 2500 
前 还是 后 以此类推 直到 找到 那个/nr 准确 的 词 
额 像不像 我们 平时 玩 的 猜 数字 游戏 一个人 
先在 纸上 写好 一个 数字 然后 开始 让 大家 猜 
然后 一步 一步 逼近 真实 数字 直到 猜 中的 人 
接受 真心话 大冒险 别 告诉 我 你 没 玩过 那你/nr 
不是/c 70/m 后/f 就是/d 00/m 后/f 我们 有 代沟 画 
出来 的 形状 是 树状 的 每个 节点 是 一个 
分类器 这 就是 分级 softmax 分类器 7 . 负 采样 
Nagtive sampling 上面 一节 讲述 了 用 分级 softmax 分类器 
去 降低 softmax 层 的 计算 复杂度 这 一节 讲述 
一个 更好 的 方法 叫做 负 采样 来 一起 看看 
吧 7.1 过程 详述 1 准备 样本 还是 这 句话 
和上/nr 一节 一样 随机 采出 一个词 作为 context 词 再在 
给定 的 词 距 下 随机 获取 该词 的 target 
词 形成 一组 样本 orange juice 这组 样本 是 一个 
正 样本 有了 正 样本 就 肯定 需要 负 样本 
负 样本 是 这样 得到 的 context 词 不变 然后 
随机 从 词典 中 采样 出 k 个 词 这些 
词 可以 是 句子 中 没有 的 词 也 允许 
是 句子 中 有的 词 总之 随缘 就好 不 强求 
于是 context 词 就与 这些 随机 从 词典 中 采样 
的 词 形成 了 几对 负 样本 关于 k 的 
数目 如果 你 的 数据集 很小 那么 k 在 5 
20 之间 如果 你 有大/nr 数据集 那么 k 在 2 
5 之间 2 训练 模型 有了 样本 之后 就 可以 
训练 一个 监督 模型 了 模型 的 输入 x 是 
词 对 也 就是 我们 上面 准备好 的 正 样本 
与 负 样本 模型 的 输出 y 是 一个 二 
分类 若 正 样本 则为 1 负 样本 则为 0 
显而易见 这个 模型 对 目的 是 去 学习 两个 词 
是否 是 临近 词 临近 词 为 正 样本 非 
临近 词 为 负 样本 这样 的 二分 类 我们 
选择 用 逻辑 回归模型 去 构造 公式 里 有 两个 
参数 一个 是 目标 词 的 参数 向量 θ t 
一个 是 上下 文词 e c 即 每个 contxt word 
的 词 嵌入 向量 利用 上面 公示 预测处 t c 
共 现时 y = 1 时的/nr 概率 纵观 整个 神经网络 
模型 前面 的 套路 不变 1 输入 context word 的 
one hot 向量 2 乘以 嵌入 矩阵 E 3 得到 
context word 的 词 嵌入 4 进入 神经网络 输出 10000 
维 向量 10000 是 词典 长度 要 注意 的 是 
第 4 步 这个 输出 并 不是 之前 的 softmax 
的 10000个 概率 而是 10000个 逻辑 回归 二 分类器 表示 
词典 中 每个 索 引上 的 词 是否 与 context 
临近 但 并 不是 每次 训练 都要 训练 全部 10000个 
逻辑 回归 我们 只 训练 其中 5个 分别 是 那个 
正 样本 的 target 词 所在 位置 的 逻辑 回归模型 
和 另外 四个 采样 的 负 样本 所在 位置 的 
模型 假设 我们 设置 了 k = 4 如此 以来 
原来 复杂 的 要 计算 10000次 的 softmax 层 变成 
了 计算 相对 简单 的 10000个 逻辑 回归 二 分类 
模型 且 每次 训练 只需要 训练 k + 1个 logistic 
unit 是不是 大大 减小 了 计算 量 呢 7.2 如何 
选取 负 样本 那么 如何 进行 更优 的 负 采样 
呢 论文 的 作者 Mikoolov 等人 根据 经验 认为 根据 
一下 经验值 采样 会 更好 wi 表示 第 i 个 
词 f wi 表示 第 i 个 词 在 所有 
语料 中的 词频 但是 这 是 针对 英文 单词 的 
分布 的 中文 的 不 知道 适 不适用 呢 8 
. GloVe 词 向量 前面 讲了 word2vec 算法 进行 词 
嵌入 的 学习 这 一节 将 介绍 另一种 也 表现 
很好 且 更 简单 的 算法 GloVe 算法 global vectors 
for word representatiom 虽然 它 并 没有 word2vec 那么 火 
但是 也 有人 热衷于 它 又是 这 句话 word2vec 中 
获取 了 词 对 context – target 在 glove 中使 
词 对 的 关系 明确化 X _ ij 表示 词 
i 出现 在 j 的 上下文 的 次数 这里 用 
ij 来 表示 tc 因此 X _ ij 等同于 X 
_ tc t 表示 target c 表示 context 实际上 X 
_ ij 也 经常 与 X _ ji 对称 比如 
当 你 将 窗口 设 定为 前后 10个 词 时 
也 就是说 word2vec 中 判断 的 是 两个 是否 相邻 
GloVe 关注 的 是 两个 词 相邻 出现 对 次数 
因此 GloVe model 的 具体 做法 是 酱紫 的 其 
目标函数 是 最小化 以下 公式 θ i 和e/nr j 分别 
表示 target word 与 context word 的 词 嵌入 向量 
f xij 是 一个 权重 项 对于 像 the a 
an of 等 停用词 会 给予 较小 权重 对于 durain 
这种 稀有 词 但有 蛮 重要 点 词 给予 增加 
权重 log Xij 表示 的 是 i 词 与 j 
词 的 相似 程度 由于 此处 是 i 和j是/nr 对称 
的 因此 最终 词 嵌入 e w _ final 可以 
是 θ w 与 e w 的 均值 9 . 
情感 分类 Sentiment classification 情感 分类 是 指 对 一个 
文本 一篇 文章 新闻 微博 评论 等等 预测出 笔者 对 
所 描述 的 东西 的 情感 是 正向 的 还是 
负向 的 喜欢 还是 讨厌 是 NLP 中 一个 应用 
很 普遍 业务 需求 很 旺盛 的 一个 部分 在 
没有 词 嵌入 向量 之前 我们 需要 标注 大量 的 
数据 去 训练 这个 有 监督 的 分类 模型 但是 
现在 有了词/nr 嵌入 后 需要 的 样本 量 就 大大 
减少 了 哦 来 感受 一下 情感 分类 算法 点 
过程 吧 9.1 输入 与 输出 首先 明确 输入输出 首先 
输入 是 一段 文本 输出 是 要 预测 的 相应 
的 情感 可以 是 正负 的 二分 类 也 可以 
是 评级 的 多 分类 比如 影评 和 淘宝 评价 
又 5个 等级 9.2 简单 的 模型 a . 先 
来说 说 一个 简单 的 模型 此时 假设 我们 已 
经训 练好 了 一个 优先 的 嵌入 矩阵 E b 
. 将 输入 文本 中 的 每个 词 都在 E 
中找到 对应 的 词 嵌入 e 可以 用 上文 介绍 
过 的 方法 即用 该词 的 one hot 词 向量 
去 乘以 E c . 将 所有 词 嵌入 求 
均值 或者 加 和 将 n 个 向量 变成 一个 
向量 这里 n 的 大小 其实 就 文本 中词 的 
个数 假 设词 向量 是 300 维 的 那么 最终 
求 均值 或 和 之后 就 生成 一个 新的 300 
维 的 向量 d . 将 这个 300 维 的 
向量 做为 神经 网络 的 输入 即 输入 层 又 
300个 神经元 经过 一个 softmax 分类 层 输出 情感 的 
分类 若 又 5类 情感 则 输出 层 又 5个 
神经元 但 这个 模型 有 巨大 的 缺点 就是 不 
考虑 词 的 顺序 假设 评论 如下 completely lacking in 
good taste good service and good ambience 这句话 说是 lacking 
但是 却又 3个 good 因此 直接 将 词 向量 均值 
或 求和 就会 认为 是 good 并 没有 捕捉 到 
前面 说 的 是 lacking good XXX . 9.3 RNN 
for sentiment classification 要 捕捉 顺序 上 的 信息 此时 
果断 就需要 RNN 来 闪亮 登场 了 同样 是 输入 
每个 词 的 词 嵌入 向量 在 最后 一个 时刻 
的 输出 情感 的 分类 结果 具体 结构 如下 因为 
RNN 的 详细 教程 在 前边 的 笔记 中 已经 
讲 因此 此处 不在 对 以上 结构 做 过多 解释 
10 . 词 嵌入 除 偏 Debiasing word embeddings10 . 
1 什么 偏 这里 除 偏 的 偏 不是 机器学习 
里 技术上 的 bias 而是 偏见 的 偏 话 不多 
说 举 几个 例子 你 就 懂 比如 上面 这句 
男人 之于 程序员 就像 女人 之 于 家庭 主妇 中国 
同胞 们 肯定 就 疑惑 了 这句话 好像 没啥 偏见 
啊 而且还 挺 准 的 咳咳 我 大中华 真是 直 
男 成灾 啊 你 说说 凭 啥 女性 就 不能 
做 程序员 男性 就 不能 在家 带 孩子 再 咳咳 
比如 像 本人 这种 以外 能 代码 程序 赚钱 养家 
内能 貌美如花 贤惠 顾家 为 终极 目标 的 新时代 女 
汉子 就 第一 个 不服 这句 带有 性别歧视 的话 哈哈哈 
再 比如 上面 这句 父亲 之 于 医生 就像 母亲 
之于 护士 是的 的确 在 国内 医生 男 的 居多 
护士 大多 都是/nr 女性 因此 直 男 又要 反驳 这 
哪里 有 bais 明明 是 社会 常态 啊 不同 社会 
下 的 语料 训 练出 的 词 向量 会 反应 
当下 的 性别 种族 年龄 等 偏见 这与 当下 的 
社会 经济 政治 文化 状态 都 相关 也许 某些 观念 
在 我们 骨子里 已经 根深蒂固 且 无法 与之 相抗/nr 也 
不能 要求 整个 社会 的 改变 但 至少 将 被 
广泛 应用 于 人类 社会 方方面面 的 机器 学习 与 
人工智能 能 杜绝 掉 被 这些 传统 意识形态 的 束缚 
能 真正 成功 不 带 任何 偏见 没有 有色眼镜 公正 
公平 平等 得去 效力 于 各行各业 的 业务 场景 中 
10.2 如何 除 偏 假设 有 这样 几个 词 第一步 
Identify bias direction 以 除去 性别 偏见 为例 将 性 
别词 相减 然后再 求 均值 e _ he e _ 
shee _ male e _ femal – average 从 上图 
的 分布 可见 横轴 代表 了 偏见 的 方向 给 
它 1个 维度 纵轴 代表 了 无偏见 的 方向 给 
它 299个 维度 这里 讲述 地 比 实际 论文 中 
要 简单 论 文中 的 偏见 方向 不只 1 维 
而且 以上 也 不是 简单 得 求 均值 而是 用 
起 一只 分解 的 方法 第二步 Neutralize 中和 偏见 有/v 
一些/m 词/n 本身/r 就/d 有/v 性别/n 上/f 的/uj 信息/n 比如 
he she father mother 而/c 有/v 一些/m 词/n 本身/r 和/c 
性别/n 并/c 无关系/i 如 doctor nurse homemaker computer programer 等 
即在 性别 上 是 中 立等 因此 需要 对 这些 
词 做 消除 偏见 以上 坐标 中 doctot 是 在 
男性 那 一侧 应该 调整 到 在 男性 女性 中间 
第三步 Equalze pair 均衡 比如 使得 babysiter 能够 到 gramdfather 
和 grandermother 的 距离 一样 近 做法 就是 将 gramdfather 
和 grandermother 移动 到 根据 纵 轴对称 而将 babysiter 移动 
到 纵 轴上 那么 如何 找出 哪些 词 是 中立 
词 呢 论文 的 作者 建立 了 二 分类 到 
监督 模型 进行 中立 与非 中立 的 分类 END 错别字 
太 多请 忽略 之后 会 详细 检查 与 更 正到 
哈 欢迎 关注 王 小草 的 微信 公众 号 推送 
大 数据 机器学习 深度 学习 NLP 等 原创 文章 欢迎 
交流 与 指正 项目 地址 https / / github . 
com / liuhuanyong / L a n g u a 
g e R e s o u r c e 
s 致力于 利用 web 公开 信息 采用 爬虫 脚本 加工 
处理 形成 语言 资源 包括 词汇 知识库 领域 语料 等 
语言 资源 该 资源 可 用于 自然语言 处理 任务 . 
1 corpus _ resources . py 词库 包括 name 人民日报 
语料 link https / / pan . baidu . com 
/ s / 1 _ E2YA7u61s _ ZSSFV0IrHJA pwd ux12 
desc 人民日报 199801 语料 name 领域 小说 文本 语料 link 
https / / pan . baidu . com / s 
/ 1JC3UyOu8PuJrnn _ JUyF9UQ pwd bguf desc 13个 领域 的 
小说 文本 集合 5000 + 小说 文本 name 字幕 文本 
语料 link https / / pan . baidu . com 
/ s / 1 9 B I 8 1 W 
7 r F w v L K E j V 
B P X a U A pwd mpfz desc 基于 
字幕 网 抓取 70W 字幕 文本 语料 name 段子 文本 
语料 link https / / pan . baidu . com 
/ s / 1go84Pt8O AHJJOgJhkG89Q pwd eju6 desc 基于 内涵 
段子 等 短文 本网站 抓取 约 50W name 歌词 文本 
语料 link https / / pan . baidu . com 
/ s / 1IOCH9EfZInTdI _ GvnuedJA pwd nq69 desc 基于 
歌词 网站 抓取 歌词 数量 约 20W2 word _ resources 
. py 领域 语料 包括 name 语义 词库 link https 
/ / pan . baidu . com / s / 
1b663 M V Q 2 U G 6 9 w 
v m K g 9 1 2 g pwd flg8 
desc 语法 信息 词典 知网 义 原 程度 副词 现代 
汉语 词典 否定词 同义词 词 林等/nr name 领域 词库 link 
https / / pan . baidu . com / s 
/ 1fzwE94sC77PDo 36IKCkWg pwd x57t desc 33个 领域 词 词库 
name 情感 词库 link https / / pan . baidu 
. com / s / 1 0 K E C 
T 0 k x i R D t 4 3 
v u B O d e A pwd mn5u desc 
通用 微博 食物 财经 等 领域 情感 词 以及 公开 
情感 词 清华 台湾大学 大连理工 等 name 敏感词 词库 link 
https / / pan . baidu . com / s 
/ 1DIkV R y i E V a N M 
P N Y i i K V s A pwd 
asol desc 敏感词 词库 可 用于 敏感 信息 检测 name 
搜狗 输入法 词库 link https / / pan . baidu 
. com / s / 1 1 H 8 L 
0 0 2 1 T g n W E s 
8 p 4 c j G k Q pwd wpr8 
desc 基于 搜狗 输入法 抓取 与 转换 生成 1W + 
个 词库 文本 3 wordvector _ resource . py 预 
训练 词 向量 文件 name 多 领域 词 向量 link 
https / / pan . baidu . com / s 
/ 10j2Ozt9rOspVDsn _ UNIfdw pwd cw04 desc 基于 腾讯 历时 
滚动 新闻 训练 的 多 领域 词 向量 包括 财经 
军事 体育 科技 等 领域 * * * * * 
* * * * * * name 中文字 向量 link 
https / / pan . baidu . com / s 
/ 1 m 7 E 8 6 i g k 
O g l Q s l 7 h w n 
0 Q V w pwd b2mg desc 基于 维基百科 生成 
的 字 向量 资源 已经 共享 至 百度 网盘 详细 
见 相应 的 . py 文件 If any question about 
the project or me see https / / liuhuanyong . 
github . io / 项目 地址 https / / github 
. com / liuhuanyong / L a n g u 
a g e R e s o u r c 
e s T e x t R a n k 
是 自然 语言 处理 领域 一种 比较 常见 的 关键词 
提取 算法 可 用于 提取 关键词 短语 和 自动 生成 
文本 摘要 TextRank 是由 PageRank 算法 改进 过来 的 所以 
有 大量 借鉴 PageRank 的 思想 其 处理 文本 数据 
的 过程 主要 包括 以下 几个 步骤 1 首先 将 
原 文本 拆 分为 句子 在 每个 句子 中 过滤掉 
停用词 可以 不 选 并 只 保留 指定 词性 的 
单词 由此 可以 得到 句子 和 单词 的 集合 2 
每个 单词 作为 PageRank 中 的 一个 节点 设 窗口 
大小 为 k 假设 一个 句子 所 组成 的 单词 
可以 表示 为 w1 w2 w3 wn . 则 w1 
w2 wk w2 w3 wk + 1 w3 w4 wk 
+ 2 等 都是/nr 一个 窗口 在 一个 窗口 内 
任意 两个 单词 之间 存在 一条 无向 无权 的 边 
3 基于/p 上面/f 的/uj 节点/n 和边/nr 构成/v 图/n 可以 据此 
计算 出 每个 节点 的 重要性 最 重要 的 若干 
单词 可以 作为 区分 文本 类别 和 主题 的 关键词 
基于 荣耀 V10 手机 评论 数据 的 Python 代码 实现 
如下 所示 # * coding utf 8 * Created on 
Fri Feb   9 15 58 14 2018 @ author 
zch import codecs from textrank4zh import TextRank4Keyword T e x 
t R a n k 4 e n t e 
n c e # 读取 华为 荣耀 天猫 旗舰店 荣耀 
V10 手机 的 评论 文本 数据 text = codecs . 
open D / / data / tmall / origin _ 
tmall _ review . txt r utf 8 . read 
tr4w = TextRank4Keyword tr4w . analyze text = text lower 
= True window = 2 print 关键词 for item in 
tr4w . get _ keywords 10 word _ min _ 
len = 1       print { } 出现 
的 频率 为 { . 6f } . format item 
. word item . weight print 关键 短语 for phrase 
in tr4w . get _ keyphrases keywords _ num = 
10 min _ occur _ num = 5     
  print phrase tr4s = T e x t R 
a n k 4 e n t e n c 
e tr4s . analyze text = text lower = True 
source = all _ filters print print 摘要 for item 
in tr4s . get _ key _ sentences num = 
3       # index 是 语句 在 文本 
中 位置 weight 是 权重       print 第 
{ } 句 出现 的 频率 为 { . 6f 
} 内容 为 { } . format item . index 
item . weight item . sentence 输出 的 关键词 如下 
图 所示 输出 的 关键 短语 如下 图 所示 输出 
的 摘要 如下 图 所示 从 上面 的 输出 结果 
可以 看出 华为 荣耀 V10 的 评论 信息 大多数 还是 
比较 积极 正面 的 能够 基本 反映 出 用户 对 
这款 手机 产品 的 态度 NLP 自然语言 处理 一 词 
法分析 1 . 什么 是 自然 语言 处理 NLP natural 
language processing 如图所示 我 希望 计算机 对 房间 的 评论 
结果 是 可以 欣赏 日出 因此 我 理解 的 NLP 
实际上 就是 让 计算机 和人/nr 一样 理解 语言 . 2 
. 词 法分析 法分析 向 用户 提供 分词 词性 标注 
命名 实体 识别 三 大 功能 该 服务 能够 识别 
出 文本 串 中的 基本词汇 分词 对 这些 词汇 进行 
重组 标注 组合 后 词汇 的 词性 并 进一步 识别 
出 命名 实体 . 1 分词 中文分词 是 将 连续 
的 自然 语言 文本 切分/ad 成/n 具有/v 语义/n 合理性/n 和/c 
完整性/n 的/uj 词汇/n 序列/n 的/uj 过程/n 致 毕业 和 尚未 
毕业 的 同学 我们 更 希望 计算机 得到 的 结果 
是 和 与 尚未 分开 查询 而 不是 和尚 与 
未 分开 查询 . 2 词性 标注 词性 标注 Part 
of Speech tagging 或 POS tagging 是 指为 自然语言 文本 
中 的 每个 词汇 赋予 一个 词性 的 过程 3 
命名 实体 命名 实体 识别 Named Entity Recognition 简称 NER 
又称 专名 识别 是 指 识别 自然语言 文本 中 具有 
特定 意义 的 实体 主要 包括 人名 地名 机构 名 
时间 日期 等 这里 写 自定义 目录 标题 欢迎 使用 
Markdown 编辑器 新的 改变 功能 快捷键 合理 的 创建 标题 
有助于 目录 的 生成 如何 改变 文本 的 样式 插入 
链接 与 图片 如何 插入 一段 漂亮 的 代码 片 
生成 一个 适合 你 的 列表 创建 一个 表格 设定 
内容 居中 居左/nr 居右/nr SmartyPants 创建 一个 自定义 列表 如何 
创建 一个 注脚 注释 也是 必不可少 的 KaTeX 数学公式 新的 
甘特图 功能 丰富 你 的 文章 UML 图表 FLowchart 流程图 
导出 与 导入 导出 导入 词 表示 是 自然 语言 
处理 的 基础 一个 好 的 词 向量 在 很大 
程度 上 决定 了 后续 任务 的 上限 本文 是 
我 最近 学习 该 部分 内容 的 笔记 主要 参照 
的 是 基于 神经 网络 的 词 和 文档 语义 
向量 表示 方法 研究 一文 穿插 了 一些 个人 理解 
内容 较多 错误 难免 请 拍砖 ~ 分布 表示 Distributional 
Representation 假说 上下文 相似 的 词 其 语义 也 相似 
根据 建模 方式 的 不同 主要 分为 三类 基于 矩阵 
的 分布 表示 基于 聚 类 的 分布 表示 和 
基于 神经 网络 的 分布 表示 尽管 不同 的 分布 
表示 方法 使用 了 不同 的 技术 手段 获取 词 
表示 但 由于 这些 方法 均 基于 分布 假说 它们 
的 核心 思想 也 都由 两部分 组成 选择 一种 方式 
描述 上下文 选择 一种 模型 刻画 目标 词 与其 上下文 
之间 的 关系 基于 矩阵 的 分布 表示 基于 矩阵 
的 分布 表示 通常 又 称为 分布 语义 模型 distributional 
semantic models 这类 方法 需要 构建 一个 词 上下文 矩阵 
从 矩阵 中 获取 词 的 表示 在 词 上下文 
矩阵 中 每行 对应 一个 词 每 列 表示 一种 
不同 的 上下文 矩阵 中的 每个 元素 对应 相关 词 
和 上下文 的 共 现 次数 在 这种 表示 下 
矩阵 中的 一行 就 成为 了 对应 词 的 表示 
这种 表示 描述 了 该词 的 上下文 的 分布 由于 
分布 假说 认为 上下文 相似 的 词 其 语义 也 
相似 因此 在 这种 表示 下 两个 词 的 语义 
相似 度 可以 直接 转化 为 两个 向量 的 空间 
距离 这类 方法 具体 可以 分为 三 个 步骤 选取 
上下文 最 常见 的 有三种 方法 第一种 将 词 所在 
的 文档 作为 上下文 形成 词 文档 矩阵 term document 
matrix 第二种 将 词 附近 上下 文中 的 各个 词 
如 上下文 窗口 中的 5个 词 作为 上下文 形成 词 
词 矩阵 第三种 将 词 附近 上下文 各 词 组成 
的 n gram 作为 上下文 在 这三种 方法 中 词 
文档 矩阵 非常 稀疏 而 词 词 矩阵 相对 较为 
稠密 效果 一般 好于 前者 词 n gram 相对 词 
词 矩阵 保留 了 词序 信息 建模 更 精确 但 
由于 比 前者 更 稀疏 实际 效果 不 一定 能 
超越 前者 确定 矩阵 中 各 元素 的 值 词 
上下文 共 现 矩阵 根据 其 定义 里面 各 元素 
的 值 应为 词 与 对应 的 上下文 的 共 
现 次数 然而 直接 使用 原始 共 现 次数 作为 
矩阵 的 值 在 大多数 情况 下 效果 并 不好 
因此 研究 人员 提出 了 多种 加权 和 平滑 方法 
最 常用 的 有 tf idf PMI 和 直接 取 
log 矩阵 分解 可选 在 原始 的 词 上下文 矩阵 
中 每个 词 表示 为 一个 非常 高维 维度 是 
不同 上下文 的 总 个数 且 非常 稀疏 的 向量 
使用 降 维 技术 可以 将 这一 高维 稀疏 向量 
压缩成 低维 稠密 向量 降 维 技术 可以 减少 噪声 
带来 的 影响 但 也 可能 损失 一 部分 信息 
最 常用 的 分解 技术 包括 奇异 值 分解 SVD 
非 负 矩阵 分解 NMF 典型 关联 分析 Canonical Correlation 
Analysis CCA Hellinger PCA HPCA 著名 的 Global Vector 模型 
GloVe 就是 基于 矩阵 的 分布 表示 基于 聚 类 
的 分布 表示 分布 聚 类 基于 聚 类 的 
分布 表示 又 称作 分布 聚 类 distributional clustering 这类 
方法 通过 聚 类 手段 构建 词 与其 上下文 之间 
的 关系 其中 最 经典 的 方法 是 布朗 聚 
类 Brown clustering 布朗 聚 类 是 一种 层级 聚 
类 方法 聚 类 结果 为 每个 词 的 多层 
类别 体系 因此 可以 根据 两个 词 的 公共 类别 
判断 这 两个 词 的 语义 相似 度 这个 方法 
似乎 没有 太多 主流 的 应用 所以 我 没有 做 
深入研究 基于 神经 网络 的 分布 表示 词 向量 基于 
神经 网络 的 分布 表示 一般 称为 词 向量 词 
嵌入 word embedding 或 分布式 表示 distributed representation 神经 网络 
词 向量 表示 技术 通过 神经 网络 技术 对 上下文 
以及 上下文 与 目标 词 之间 的 关系 进行 建模 
由于 神经网络 较为 灵活 这类 方法 的 最大 优势 在于 
可以 表示 复杂 的 上下文 在前面 基于 矩阵 的 分布 
表示 方法 中 最 常用 的 上下文 是 词 如果 
使用 包含 词序 信息 的 n gram 作为 上下文 当 
n 增加 时 n gram 的 总数 会 呈 指数级 
增长 此时 会 遇到 维数 灾难 问题 而 神经 网络 
在 表示 n gram 时 可以 通过 一些 组合 方式 
对 n 个 词 进行 组合 参数 个数 仅以 线性 
速度 增长 有了 这一 优势 神经网络 模型 可以 对 更 
复杂 的 上下文 进行 建模 在 词 向量 中 包含 
更 丰富 的 语义 信息 神经 网络 词 向量 模型 
与 其它 分布 表示 方法 一样 均 基于 分布 假说 
核心 依然 是 上下文 的 表示 以及 上下文 与 目标 
词 之间 的 关系 的 建模 构建 上下文 与 目标 
词 之间 的 关系 最 自然 的 一种 思路 就是 
使用 语言 模型 语言 模型 语言 模型 可以 对 一段 
文本 的 概率 进行 估计 对 信息检索 机器翻译 语音 识别 
等 任务 有着 重要 的 作用 形式化 讲 统计 语言 
模型 的 作用 是 为 一个 长度 为 的 字符串 
确定 一个 概率分布 表示 其 存在 的 可能性 其中 到 
依次 表示 这段 文本 中 的 各个 词 一般 在 
实际 求解 过程 中 通常 采用 下式 计算 其 概率值 
在 实践 中 如果 文本 的 长度 较长 上述 公式 
右 部 的 估算 会 非常 困难 因此 研究者 们 
提出 使用 一个 简化 模型 n 元 模型 n gram 
model 在 n 元 模型 中 估算 条件概率 时 距离 
大于 等于 n 的 上 文词 会被 忽略 也 就是 
对 上述 条件概率 做了 以下 近似 在 元 模型 中 
传统 的 方法 一般 采用 频率 计数 的 比例 来 
估算 元 条件概率 其中 表示 文本 序列 在 语料 中 
出现 的 次数 为了 更好 地 保留 词序 信息 构建 
更 有效 的 语言 模型 我们 希望 在 元 模型 
中 选用 更大 的 但是 当 较大 时 长度 为 
序列 出现 的 次数 就会 非常少 在 按照 上述 公式 
估计 元 条件概率 时 就会 遇到 数据 稀疏 问题 导致 
估算 结果 不 准确 因此 一般 在 百万 词 级别 
的 语料 中 三元 模型 是 比较 常用 的 选择 
同时 也 需要 配合 相应 的 平滑 算法 进一步 降低 
数据 稀疏 带来 的 影响 为了/p 更好/d 地/uv 解决/v 元/m 
模型/n 估算/v 概率/n 时/n 遇到/v 的/uj 数据/n 稀疏/a 问题/n 神经 
网络 语言 模型 应运而生 神经 网络 语言 模型 NNLM 神经 
网络 语言 模型 Neural Network Language Model NNLM 在 学习 
语言 模型 的 同时 也 能 得到 词 向量 NNLM 
同样 也是 对 n 元语言 模型 进行 建模 估算 的 
值 但 与 传统 方法 不同 的 是 NNLM 不通过 
计数 的 方法 对 元 条件概率 进行 估计 而是 直接 
通过 一个 神经 网络结构 对 其 进行 建模 求解 下图 
展示 了 NNLM 的 基本 结构 NNLM 模型 具体 而言 
对语 料中 一段 长度 为 的 序列 元语言 模型 需要 
最大化 以下 概率 其中 为 需要 通过 语言 模型 预测 
的 词 目标 词 对于 整个 模型 而言 输入 为 
条件 部分 的 整个 词 序列 输出 为 目标 词 
的 分布 而 神经 网络 的 目标 就是 要让 输出 
中 目标 词 的 概率 最大 神经 网络 语言 模型 
采用 普通 的 三层 前馈 神经 网络结构 其中 第 一层 
为 输入 层 Bengio 提出 使用 各 词 的 词 
向量 作为 输入 以 解决 数据 稀疏 问题 因此 输入 
层 为 词 的 词 向量 的 顺序 拼接 当 
输入 层 完成 对 上文 的 表示 之后 模型 将其 
送入 剩下 两层 神经网络 依次 得到 隐藏 层 和 输出 
层 其中 表示 词汇表 的 大小 表示 词 向量 的 
维度 是 隐 层 的 维度 矩阵/n 表示/v 从/p 输入/v 
层/q 到/v 输出/v 层/q 的/uj 直连/i 边/d 权重/n 矩阵/n 如果 
使用 该 直连 边 可以 减少 一半 的 迭代 次数 
但 如果 没有 直连 边 可以 生成 性能 更好 的 
语言 模型 因此 在 后续 工作 中 很少/m 有/v 使用/v 
输入/v 层/q 到/v 输出/v 层/q 直连/i 边的/nr 工作/vn 输出 层 
一 共有 个 元素 依次 对应 下 一个 词 为 
词表 中 某个 词 的 可能性 这里 使用 softmax 函数 
将其 转化 为 对应 的 概率 在 NNLM 模型 中 
词 向量 出现 在 两个 地方 一个 是 输入 层 
的 词 向量 另 一是 隐 层 的 权重 的 
维度 是 这 可以 看做 是个 维 的 行向量 其中 
的 每一个 向量 均 可以 看做 某个 词 在 模型 
中 的 另一个 词 向量 记为 在 不考虑 的 情况 
下 每个 词 在 模型 中 有 两套 词 向量 
通常 在 实际 工作 中 只是 用 第一 个 作为 
词 向量 将 展开 得到 被 称为 能量 函数 log 
双线性 语言 模型 LBL 2007 年 Mnih 和 Hinton 在 
神经 网络 语言 模型 NNLM 的 基础 上 提出 了 
log 双线性 语言 模型 Log Bilinear Language Model LBL LBL 
模型 的 能量 函数 为 LBL 模型 的 能量 函数 
与 NNLM 的 能量 函数 主要 有 两个 区别 一 
LBL 模型 中 没有 非线性 的 激活 函数 tanh 而 
由于 NNLM 是非 线性 的 神经 网络结构 激活 函数 必不可少 
二 LBL 模型 中 只有 一份 词 向量 e 也 
就是说 无论 一个词 是 作为 上下文 还是 作为 目标 词 
使用 的 是 同一 份 词 向量 其中 第二 点 
只有 一份 词 向量 只在 原版 的 LBL 模型 中 
存在 后续 的 改进 工作 均 不 包含 这一 特点 
循环 神经 网络 语言 模型 RNNLM 循环 神经 网络 语言 
模型 Recurrent Neural Network based Language Model RNNLM 则 直接 
对 进行 建模 注意 不是 该 模型 就是 把 NNLM 
隐 层 变成 RNN 每一个 隐 层 包含 此前 所有 
上文 信息 RNNLM 里面 最 厉害 的 就 属 ELMo 
了 该 模型 利用 多层 双向 LSTM 的 加权 和来/nr 
表示 词 向量 其中 权重 可 根据 具体 任务 动态 
调节 C & W 模型 与 基于 语言 模型 的 
词 向量 生成 方法 不同 C & W 以 直接 
生成 词 向量 为 目标 C & W 模型 C 
& W 模型 没有 去 求解 而是 直接 对 n 
元 短语 打分 对于 语料 中 出现 过 的 元 
短语 模型 会 对其 打 高分 而 对于 语料 中 
没有 出现 的 随机 短语 模型 会 对其 打 低分 
通过 这种 方式 C & W 模型 可以 更 直接 
地 学习 得到 符合 分布 假说 的 词 向量 具体 
而言 对于 整个 语料 C & W 模型 需要 最小化 
其中 为从 语料 中 选出 的 一个 元 短语 为序 
列中 的 中间 词 也是 目标 词 即 表示 的 
上下文 为 字典 中的 某 一个 词 正 样本 来自 
语料 而 负 样本 则是 将 正 样本 序列 中 
的 中间 词 替换成 其它 词 即 C & W 
模型 与 NNLM 相比 主要 的 不同 点 在于 C 
& W 模型 将 目标 词 放到 了 输入 层 
同时 输出 层 也从 语言 模型 的 个 节点 变为 
一个 节点 这个 节点 的 数值 表示 对 这组 n 
元 短语 的 打分 这个 区别 使得 C & W 
模型 成为 神经 网络 词 向量 模型 中 最为 特殊 
的 一个 其它 模型 的 目标 词 均在 输出 层 
只有 C & W 模型 的 目标 词 在 输入 
层 CBOW 模型 CBOW 模型 CBOW 模型 的 结构 如上图 
该 模型 一方面 根据 C & W 模型 的 经验 
使用 一段 文本 的 中间 词 作为 目标 词 另一方面 
又以 NNLM 作为 蓝本 并在 其 基础 上 做了 两个 
简化 一 CBOW 没有 隐藏 层 去掉 隐藏 层 之后 
模型 从 神经 网络结构 直接 转化 为 log 线性 结构 
与 Logistic 回归 一致 log 线性 结构 比 三层 神经 
网络结构 少 了 一个 矩阵 运算 大 幅度 地 提升 
了 模型 的 训练 速度 二 CBOW 去除 了 上下文 
各 词 的 词序 信息 使用 上下文 各 词 词 
向量 的 平均值 代替 神经 网络 语言 模型 使用 的 
上文 各 词 词 向量 的 拼接 形式化 地 CBOW 
模型 对于 一段 训练样本 输入 为 由于 没有 隐藏 层 
CBOW 模型 的 输入 层 直接 就是 上下文 的 表示 
CBOW 模型 根据 上下文 的 表示 直接 对 目标 词 
进行 预测 对于 整个 语料 而言 与 神经 网络 语言 
模型 类似 CBOW 的 优化 目标 为 最大化 Skip gram 
模型 Skip gram 模型 Skip gram 模型 的 结构 如上图 
与 CBOW 模型 一样 Skip gram 模型 中 也 没有 
隐藏 层 和 CBOW 模型 不同 的 是 Skip gram 
模型 每次 从 目标 词 的 上下 文中 选择 一个 
词 将 其词 向量 作为 模型 的 输入 也 就是 
上下文 的 表示 Skip gram 模型 同样 通过 上下文 预测 
目标 词 对于 整个 语料 的 优化 目标 为 最大化 
其中 / div 欢迎 使用 Markdown 编辑器 你好 这是 你 
第一次 使用 Markdown 编辑器 所 展示 的 欢迎 页 如果 
你 想 学习 如何 使用 Markdown 编辑器 可以 仔细阅读 这篇文章 
了解 一下 Markdown 的 基本 语法 知识 新 的 改变 
我们 对 Markdown 编辑器 进行 了 一些 功能 拓展 与 
语法 支持 除了 标准 的 Markdown 编辑器 功能 我们 增加 
了 如下 几点 新功能 帮助 你 用 它 写 博客 
全新 的 界面 设计 将 会 带来 全新 的 写作 
体验 在 创作 中心 设置 你 喜爱 的 代码 高亮 
样式 Markdown 将 代 码片 显示 选择 的 高亮 样式 
进行 展示 增加 了 图片 拖拽 功能 你 可以 将 
本地 的 图片 直接 拖拽 到 编辑 区域 直接 展示 
全新 的 KaTeX 数学公式 语法 增加 了 支持 甘特图 的 
mermaid 语法 1 功能 增加 了 多 屏幕 编辑 Markdown 
文章 功能 增加 了 焦点 写作 模式 预览 模式 简洁 
写作 模式 左右 区域 同步 滚轮 设置 等功能 功能 按钮 
位于 编辑 区域 与 预览 区域 中间 增加 了 检查 
列表 功能 功能 快捷键 撤销 Ctrl / Command + Z 
重做 Ctrl / Command + Y 加粗 Ctrl / Command 
+ B 斜体 Ctrl / Command + I 标题 Ctrl 
/ Command + Shift + H 无 序列表 Ctrl / 
Command + Shift + U 有 序列表 Ctrl / Command 
+ Shift + O 检查 列表 Ctrl / Command + 
Shift + C 插入 代码 Ctrl / Command + Shift 
+ K 插入 链接 Ctrl / Command + Shift + 
L 插入 图片 Ctrl / Command + Shift + G 
合理 的 创建 标题 有助于 目录 的 生成 直接 输入 
1次 # 并 按下 space 后 将 生成 1级 标题 
输入 2次 # 并 按下 space 后 将 生成 2级 
标题 以此类推 我们 支持 6级 标题 有助于 使用 TOC 语法 
后 生成 一个 完美 的 目录 如何 改变 文本 的 
样式 强调 文本 强调 文本 加粗 文本 加粗 文本 标记 
文本 删除 文本 引用 文本 H2O is 是 液体 210 
运算 结果 是 1024 . 插入 链接 与 图片链接 link 
. 图片 带 尺寸 的 图片 居中 的 图片 居中 
并且 带 尺寸 的 图片 当然 我们 为了 让 用户 
更加 便捷 我们 增加 了 图片 拖拽 功能 如何 插入 
一段 漂亮 的 代码 片 去 博客 设置 页面 选择 
一款 你 喜欢 的 代码 片 高亮 样式 下面 展示 
同样 高亮 的 代 码片 . / / An highlighted 
block var foo = bar 生成 一个 适合 你 的 
列表 项目 项目 项目 项目 1 项目 2 项目 3 
计划 任务 完成 任务 创建 一个 表格 一个 简单 的 
表格 是 这么 创建 的 项目 Value 电脑 $ 1600 
手机 $ 12 导管 $ 1 设定 内容 居中 居左/nr 
居右/nr 使用 居中 使用 居左/nr 使用/v 居右/nr 第一/m 列/v 第二/m 
列/v 第三/m 列/v 第一/m 列/v 文本/n 居中/v 第二列/m 文本/n 居右/nr 
第三列/m 文本/n 居左m/nr a/w r/w t/w y/w P/w a/w n/w 
t/w s/w m/w a/w r/w t/w y/w P/w a/w n/w 
t/w s/w 将/d ASCII/w 标点/n 字符/n 转换/v 为/p 智能 印刷 
标点 HTML 实体 例如 T Y P E A C 
I I H T M L i n g l 
e backticks Isn t this fun Isn t this fun 
Quotes Isn t this fun Isn t this fun Dashes 
is en dash is em dash – is en dash 
is em dash 创建 一个 自定义 列表 MarkdownText to HTML 
conversion t o o l A u t h o 
r s J o h n L u k e 
如何 创建 一个 注脚 一个 具有 注脚 的 文本 2 
注释 也是 必不可少 的 Markdown 将 文本 转换 为 HTML 
KaTeX 数学公式 您 可以 使用 渲染 LaTeX 数学 表达式 KaTeX 
Gamma 公式 展示 Γ n = n − 1 ∀ 
n ∈ N \ Gamma n = n 1 \ 
quad \ forall n \ in \ mathbb N Γ 
n = n − 1 ∀ n ∈ N 是 
通过 欧拉 积分 Γ z = ∫ 0 ∞ tz 
− 1e − tdt & ThinSpace . \ Gamma z 
= \ int _ 0 ^ \ infty t ^ 
{ z 1 } e ^ { t } dt 
\ . Γ z = ∫ 0 ∞ tz − 
1e − tdt . 你 可以 找到 更多 关于 的 
信息 LaTeX 数学 表达式 here . 新的 甘特图 功能 丰富 
你 的 文章 Mon 06Mon 13Mon 20 已完成 进行 中 
计划 一 计划 二 现有 任务 Adding GANTT diagram functionality 
to mermaid 关于 甘特图 语法 参考 这儿 UML 图表 可以 
使用 UML 图表 进行 渲染 Mermaid . 例如 下面 产生 
的 一个 序列图 张三李四 王五 你好 李四 最近 怎么样 你 
最近 怎么样 王五 我 很好 谢谢 我 很好 谢谢 李四 
想了 很长时间 文字 太 长了 不 适合 放在 一行 . 
打量 着 王五 . . . 很好 . . . 
王五 你 怎么样 张三李四 王五 这将 产生 一个 流程图 链接 
长方形 圆 圆角 长方形 菱形 关于 Mermaid 语法 参考 这儿 
FLowchart 流程图 我们 依旧 会 支持 flowchart 的 流程图 Created 
with Rapha ë l 2 . 2.0 开始 我 的 
操作 确认 结束 yesno 关于 Flowchart 流程图 语法 参考 这儿 
. 导出 与 导入 导出 如果 你 想 尝试 使用 
此 编辑器 你 可以 在 此 篇文章 任意 编辑 当 
你 完成 了 一篇 文章 的 写作 在 上方 工具栏 
找到 文章 导出 生成 一个 . md 文件 或者 . 
html 文件 进行 本地 保存 导入 如果 你 想 加载 
一篇 你 写过 的 . md 文件 或者 . html 
文件 在 上方 工具栏 可以 选择 导入 功能 进行 对应 
扩展名 的 文件 导入 继续 你 的 创作 mermaid 语法 
说明 ↩ ︎ 注脚 的 解释 ↩ ︎ 寻找 了 
多家 国内 主要 IT 公司 有关 NLP 的 2018 校园 
招聘 于 大家 分享 另 外查 漏 补缺 看看 自己 
缺乏 那些 方面 的 经验 和 技术 下面 直接 放 
结论 没 时间 的 可以 只 看 总结 总结 主要 
在 以下 几个 方面 有 要求 打勾 的 次数 反应 
了 热度 1 . 自然语言 处理 相关 的 具体 操作 
分词 语义 句 意 对话 机器翻译 自动 问答 等 √ 
√ √ √ √ 2 . 经典 的 机器学习 算法 
竞赛 经历 √ √ √ 3 . 多线程 网络 编程 
分布式 编程 √ 4 . hadoop spark √ √ √ 
√ 5 . SQL NoSQL √ 6 . linux √ 
√ 前面 3个 更 面向 纯 NLP 而 后面 的 
知识 偏向 数据分析 其实 这 两个 岗位 相辅相成 很多 技能 
都是 共通 的 2018 阿里 算法 工程师 自然语言 处理 Software 
engineer Natural Language Processing 岗位 描述 Job Description 阿里 巴巴 
广阔 的 商业 生态 需要 丰富 且 深入 的 的 
自然 语言 处理 技术 涵盖 底层 文本 知识库 建设 词 
法分析 句法分析 语义分析 文档 分析 深度 文本 表示 文本 生成 
机器翻译 智能 对话 等 阿里 巴巴 的 自然 语言 处理 
技术 正在 推进 平台 化 服务化 策略 不断 追求 技术 
的 深度 以及 技术 与 业务 的 适当 解耦 本 
岗位 需要 招聘 自然语言 处理 专业 的 优秀 本科 硕士 
博士 毕业生 一起来 夯实 基础 赋 能 商业 实现 技术 
与 商业 的 完美 结合 期待 追求 卓越 自我 驱动 
聪明 乐观 自省 皮实 的 优秀 人士 加入 阿里巴巴 共同 
开创 人工智能 的 商业 新格局 具体 职责 包括 但 不限 
于 1 紧跟 业界 最新 自然语言 处理 技术 动态 深入 
研发 自然语言 处理 相关 的 知识 库 词法 句法 语义 
文档 深度 学习 机器翻译 智能 对话 等 技术 包括 模块 
的 实际 开发 以及 对接 自然语言 处理 平台 的 接入 
2 理解 自然语言 处理 技术 应用 的 相关 的 业务 
场景 及 需求 在 自然 语言 处理 技术 内核 的 
基础 上 考虑 业务 场景 的 特殊性 进而 适当 适配 
业务 需求 3 在 核心 技术 研发 之外 也会 适当 
参与 到 具体 的 NLP 相关 业务 中 例如 搜索 
Query 分析 智能 对话 的 语义 解析 及 意图 理解 
商品 评价 的 语义 理解 内容 搜索 推荐 的 结构化分析 
商品 搜索 推荐 的 标签 体系 社会化 问答 的 文本 
分析 智能 客服 的 场景 定制 等 岗位 要求 Qualifications1 
本科 及 以上 学历 硕士 博士 优先 计算机 数学 信息 
管理 等 相关 专业 2 具备 极佳 的 工程 实现 
能力 精通 C / C + + Java Python Perl 
等 至少 一门 语言 3 精通 自然语言 处理 领域 的 
1 到 2项 底层 技术 有 实际 成果 并 发表 
在 自然 语言 处理 国际 顶级 会议 期刊 者 优先 
有在/nr 相关 的 自然 语言 处理 竞赛 中 获得 优异成绩 
者 优先 4 熟悉 深度 学习 以及 常见 机器学习 算法 
的 原理 与 算法 能 熟练 运用 聚 类 分类 
回归 排序 等 模型 解决 有 挑战性 的 问题 有大/nr 
数据 处理 的 实战 经验 5 有 强烈 求知欲 对 
人工智能 领域 相关 技术 有 热情 6 具有 良好 的 
数学 基础 良好 的 英语 阅读 能力 7 有 团队 
意识 与 他人 合作 良好 最好 具有 团队 协作 的 
经验 工作地点 Location 成都市 Chengdu 上海市 Shanghai 杭州市 Hangzhou 北京市 
Beijing 广州市 Guangzhou 参加 面试 的 城市 或 地区 Interview 
City or Region 远程 Remote Interviews 2018 腾讯 岗位 描述 
腾讯 拥有 上亿 量级 的 产品 数据 极其 丰富 的 
产品 场景 超 大 规模 的 计算资源 全谱/nr 领域 的 
深厚 技术 积累 追求 极致 的 创新 氛围 适宜 年轻人 
的 企业 文化 可为 您 提供 充分 的 专业 发挥 
空间 让 您 有可能 做出 影响 整个 互联网 行业 发展 
的 优秀 成果 该 岗位 主要 职责 包括 但 不限 
于 1   负责 词 法分析 自动 对话 语义 挖掘 
和 语言 逻辑 等 相关 研究工作 2   负责 自然语言 
处理 的 算法 研发 包括 但 不限 于 语义分析 意图 
识别 人机对话 机器翻译 知识图谱 命名 实体 识别 等 3   
负责 NLP 前沿 问题 的 研究 结合 未来 实际 应用 
场景 提供 技术 解决 方案 岗位 要求 1   计算机 
应用数学 模式识别 人工智能 自控 统计学 运筹学 生物 信息 物理学 / 
量子 计算 神经科学 社会学 / 心理学 等 专业 本科 及 
以上 博士 优先 2   熟悉 至少 一种 编程语言 包括 
但 不限 于 java C / C + + C 
# python 等 3   在 学术 会议 ACL EMNLP 
NAACL COLING IJCAI AAAI 等 发表 过 文章 有 深度 
学习 学术 或 工程 项目 经验 优先 4   熟悉 
自然语言 处理 领域 的 1 到 2项 底层 技术 有 
实际 成果 并 发表 在 自然 语言 处理 国际 顶级 
会议 期刊 者 优先 有在/nr 相关 的 自然 语言 处理 
竞赛 中 获得 优异成绩 者 优先 工作地点 深圳 北京 上海 
广州 成都 招聘 城市 哈尔滨 杭州 北京 南京 远程 面试 
2018 百度 北京 机器学习 / 数据挖掘 / 自然语言 处理 工程 
师 所属 部门   百度 工作地点   北京市 招聘 人数 
  210 公  /nr           司   
百度 职位 类别   技术 发布 时间   2017 07 
28 工作 职责 研究 数据挖掘 或 统计 学习 领域 的 
前沿 技术 并 用于 实际 问题 的 解决 和 优化 
大 规模 机器学习 算法 研究 及 并行 化 实现 为 
各种 大 规模 机器学习 应用 研发 核心技术 通过 对 数据 
的 敏锐 洞察 深入 挖掘 产品 潜在 价值 和 需求 
进而 提供 更 有价值 的 产品 和 服务 通过 技术 
创新 推动 产品 成长 职责 要求 热爱 互联网 对 技术 
研究 和 应用 抱有 浓厚 的 兴趣 有/v 强烈/a 的/uj 
上进心/nt 和/c 求知欲/l 善于 学习 和 运用 新 知识 具有 
以下 一个 或 多个 领域 的 理论 背景 和 实践 
经验 机器学习 / 数据挖掘 / 深度 学习 / 信息检索 / 
自然语言 处理 / 机制 设计 / 博弈论 至少 精通 一门 
编程语言 熟悉 网络 编程 多线程 分布式 编程技术 对/p 数据/n 结构/n 
和/c 算法/n 设计/vn 有/v 较为/d 深刻/d 的/uj 理解/v 良好 的 
逻辑 思维能力 对 数据 敏感 能够 发现 关键 数据 抓住 
核心问题 较强 的 沟通 能力 和 逻辑 表达能力 具备 良好 
的 团队 合作 精神 和 主动 沟通 意识 具有 以下 
条件 者 优先 熟悉 文本 分类 聚 类 机器翻译 有 
相关 项目 经验 熟悉 海量 数据 处理 最优化 算法 分布式计算 
或 高性能 并行计算 有 相关 项目 经验 2018 网易 NLP 
算法 研发 工程师 网易 杭州 岗位 描述 1 负责 NLP 
技术 在 自动 问答 人机对话 语义 理解 等 方向 上 
的 应用 研究 2 负责 NLP 相关 核心 技术 研发 
及 前沿 算法 跟踪 根据 产品 需求 完成 技术 转化 
推动 业务 发展 岗位 要求 我们 希望 你 是 1 
正直 诚信 有 责任感 有 激情 2 模式识别 / 人工智能 
/ 数学 / 计算机相关 专业 硕士 以上 学历 3 熟悉 
基于 统计 和 句法 / 语法分析 的 自然 处理 方法 
包括 分词 词性 标注 命名 实体 识别 依存 句法分析 文本 
分类 文本检索 Deep Learning 在 NLP 领域 中 的 应用 
等等 4 具有 较强 编程 能力 熟悉 C + + 
/ Java 熟练 使用 至少 一种 脚本语言 python / shell 
等 熟悉 hadoop spark 框架 者 尤佳 5 在 自动 
问答 人机对话 口语 理解 知识库 管理 等 领域 有 实际 
的 开发 经验 者 优先 6 学习 能力 强 能 
独立 分析 并 解决问题 2018 科大 讯 飞 研究员 自然语言 
处理 方向 工作地点   合肥市 北京市 . . . 工作 
经验 学 历 工作 类型   全职 招聘 人数 若干 
发布 时间 2017 08 04 职位 描述 您 可以 1 
负责 语言 理解 人机对话 意图 识别 知识图谱 命名 体 识别 
等 相关 算法 的 研究 和 开发 2 负责 机器翻译 
相关 算法 的 研究 和 开发 任职 要求 我们 需要 
您 具备 以下 条件 1 重点 院校 硕士 及 以上 
学历 计算机 信号处理 自动化 应用 数学 等 相关 专业 具备 
一定 的 数理统计 模式识别 自然语言 处理 等 理论 知识 2 
英语六级 以上 具备 中英文 学术 论文 的 调研 能力 有 
从事 研究 型 项目 的 经历 3 具备 较好 的 
C C + + 或 python 等 热门 脚本语言 编程 
能力 有 一定 的 代码 开发 经历 如果 将 优先 
考虑 1 熟悉 RNN CNN 等 深度 学习 算法 及其 
常用 工具 如 Caffe Theano TensorFlow 等 2 熟悉 深度 
学习 算法 在 自然 语言 理解 中 的 应用 3 
有/v 自然/d 语言/n 理解/v 相关/v 方向/n 较/zg 丰富/a 的/uj 实际/n 
系统/n 研究/vn 和/c 开发/v 经/n 验者/i 4/m 在 ACL COLING 
IJCAI AAAI ICLR NIPS 等 会议 上 发表 过 文章 
有/v 深度/ns 学习/v 学术/n 或/c 工程/n 项目/n 经/n 验者/i 2018/m 
科大 讯 飞 大 数据分析 工程师 工作地点   合肥市 工作 
经验 学 历 本科 及 以上 工作 类型   全职 
招聘 人数 若干 发布 时间 2017 08 04 职位 描述 
1 通过 对 数据 的 敏锐 洞察 深入/v 挖掘/v 产品/n 
和/c 服务/vn 的/uj 潜在/t 价值/n 和/c 需求/v 进而 提供 更 
有价值 的 产品 和 服务 通过 技术 创新 推动 产品 
成长 2 通过 统计 分析 和 数据挖掘 算法 解决 实际 
问题 主要 聚焦 于 城市 动态 产生 的 海量 数据 
进行 城市 交通 分析 规划 分析 商业地产 分析 等 任职 
要求 1 热爱 技术 对 技术 研究 和 应用 抱有 
浓厚 的 兴趣 善于 学习 和 运用 新 知识 2 
具备 商业 分析报告 撰写 能力 和 良好 的 逻辑 思维能力 
对 数据 敏感 能够 发现 关键 数据 抓住 核心问题 3 
较强 的 沟通 能力 和 逻辑 表达能力 具备 良好 的 
团队 合作 精神 和 主动 沟通 意识 4 具有 以下 
一个 或 多个 领域 的 理论 背景 和 实践 经验 
统计 数学 信息技术 计算机 等 5 至少 精通 一门 编程语言 
C / C + + Java Python Scala 等 了解 
Hadoop / Spark 分布式 编程技术 6 至少 掌握 一 种 
数据 分析 工具 R SAS SPSS Matlab 等 熟练 运用 
SQL7 具有 数据挖掘 海量 数据 处理 等 相关 项目 经 
验者 优先 2018 美 团 点评 2018届 机器学习 ／ 数据挖掘 
算法 工程师 北京 工作 地点 北京 职位 类型 技术 研发 
招聘 类别 应届 毕业生 发布 时间 2017 08 10 15 
12 00 岗位职责 在 这里 你 将 通过 机器学习 数据挖掘 
深度 / 增强 学习 前沿 技术 对 海量 O2O 数据 
进行 洞察 和 预测 提高 线下 服务 效率 优化 线 
上 用户 体验 人 和 服务 的 高效 连接 潜力 
无限 帮助 亿万 用户 吃得 更好 过得 更好 在 这里 
你 将从 海量 浏览 和 交易 数据 中 不断 抽象 
模式 建立 模型 一手 保障 商户 在线 营销 可靠 安全 
另一手 为 消费者 呈现 有效 评价 提供 优质 服务 用 
技术 提升 消费 质量 在 这里 你 可以 发挥 你 
的 算法 天赋 在 海量 数据 的 平台 上 实践 
各种 机器 学习 和 挖掘 算法 搜索 推荐 广告 调度 
无人 配送 风 控 金融 ERP 和 智能化 交互 为 
你 提供 最 广阔 的 施展 天地 工作 要求 1 
. 良好 的 数据 结构 和 算法 基础 具有 较强 
的 程序 开发 和 分布式系统 实现 能力 2 . 熟练掌握 
数据库 设计 原理 对/p NoSQL/w 和/c 分布式计算/n 有/v 理解/v 和/c 
实践/v 3 . 对 概率论 机器/n 学习/v 和/c 自然/d 语言/n 
处理/v 有/v 一定/d 的/uj 理论/n 基础/n 在 深度 学习 / 
增强 学习 / 最 优化 等 方向 有 理解 或 
实践 4 . 对 数据 敏感 思维 逻辑 清晰 对 
业务 问题 充满 好奇 相信 大 数据 背后 的 力量 
2018 美 团 点评 2018届 自然语言 处理 开发 工程师 上海 
工作 地点 上海 职位 类型 技术 研发 招聘 类别 应届 
毕业生 发布 时间 2017 08 10 15 06 40 岗位职责 
在 这里 你 将 有机 会 深入 研究 自然语言 处理 
领域 的 特定 技术 在 这里 你 将用 深度 语义 
理解 让 我们 更 懂 用户 在 这里 你 将用 
深度 语义 计算 让 我们 精准 匹配 用户 需求 在 
这里 你 将用 智能 问答 技术 让 我们 为 客户 
实时 解决 各种 问题 工作 要求 1 . 熟练掌握 自然语言 
处理 领域 的 基础理论 和 方法 并有 丰富 的 相关 
方向 的 研究 经验 2 . 在 一个 或 多个 
领域 有 深入 研究 分词 文本 分类 语义分析 语义 表示 
语义 匹配 组块 分析 主题 模型 篇章 分析 等 3 
. 熟练掌握 C / C + + Python / Perl 
/ Shell 等 编程语言 及 数据 结构 基础 算法 4 
. 优秀 的 分析 问题 和 解决 问题 的 能力 
对 解决 具有 挑战性 问题 充满 激情 2017 携程 你 
敢 吗 携程 作为 中国 在线 旅游 的 领军 企业 
是 一家 崇尚 数据 驱动 文化 的 公司 BI 团队 
做为 Ctrip 的 数据 和 数据 应用 中心 不断 实践 
数据 驱动 的 文化 不断 利用 数据 和 数据模型 来 
解决 Ctrip 十三 个 业务 线 中的 各种 业务 问题 
您/zg 将/d 有/v 机会/n 和/c 旅游/vn 领域/n 的/uj 业务/n 流程 
研发 基数 数据 资深 机器学习 专家 等 领域 的 专家 
合作 我们 的 数据 科学家 团队 的 理念 是 理解 
问题 解决问题 驱动 问题 您 将会 分析 产品 操作 流量 
用户 甚至 财务 绩效 相关 的 数据 从而 能够 更好 
的 驱动 业务 发展 的 机会 您 将有 足够 的 
支持 把 你 的 发现 变成 业务 成果 一起 分享 
Ctrip 成功 的 业务 结果 岗位 介绍 1 与 产品 
沟通 并 准确 理解 需求 2 评估 需求 的 可行性 
设 计算法 并 成功 植入 系统 携程 旅行 2018届 春季 
实习生 招聘 已经 启动 欢迎 关注 官方 微信 公众 号 
ctriptech _ campus 官方 校 招 QQ 群 314810731 545235287 
529598646 278735052 541803697 添加 任意 一个 即可 我们 寻找 这样 
的 你 1 2018届 毕业生 全日制 硕 博 计算机 或 
相关 专业 2 熟悉 算法 相关 理论 算法 原理 和 
机器 学习 基本 理论 具备 扎实 良好 的 数学 基础 
3 熟悉 中文分词 文本 分类 / 聚 类 语言 模型 
语义分析 情感 分析 信息检索 问答 系统 设计 等 至少 其中 
两项 NLP 相关 算法 4 精通 Java / C + 
+ / Python / Perl 任意 一种 语言 熟练掌握 SQL 
/ Hive 5 良好 的 数据 分析 能力 能够 从 
数据 中 发现 规律 6 熟悉 Linux 环境 / Shell 
命令 7 熟悉 分布式计算 Hadoop / hive / Spark / 
Map Reduce 等 相关 技术 8 具备 深度 学习 相关 
项目 经 验者 优先 9 至少 能 保证 暑期 7 
8 月份 在 公司 实习 通过 实习 考核 即 签 
三方 2018 小米 职位 名称 数据挖掘 工程师 工作地点 北京 职位 
类别 研发 工程师 招聘渠道 校园 招聘 招聘 地点 北京 工作 
职责 方向 一 负责 公司级 数据 产品 与 平台 的 
设计 与 管理 对 公司 各 部分 业务 数据 进行 
数据采集 抽取 整合 提取 和 数据 可视化 等 工作 方向 
二 负责 小米 数据 管理 平台 的 设计 与 研发 
基于 亿级 用户 的 大 数据 建立 小米 用户 画像 
和 用户 标签 体系 建设 数据 管理 平台 提供 商业 
智能 分析 * 方向 三 负责 大 数据 的 分析 
与 挖掘 针对 海量 信息 建模 挖掘 潜在 商业价值 预测 
用户 行为 为 产品 决策 及 优化 提供 数据 支持 
与 建议 方向 四 负责 个性化 推荐 服务 系统 的 
运营 与 设计 将 海量 内容 精准 送达 至 海量 
的 用户 工作 要求 1 熟悉 数据挖掘 机器学习 自然语言 处理 
等 相关 技术 者 优先 考虑 2 拥有 Hadoop Spark 
等 分布式 环境 开发 经验 者 优先 考虑 3 熟悉 
Linux java Python 或 Shell 脚本 4 优秀 的 分析 
和 解决 问题 的 能力 对 大 数据挖掘 充满 激情 
2018 今日 头条 算法 工程师 岗位 描述 1 利用 机器学习 
技术 改进 头条 的 推荐 广告 系统 优化 数亿 用户 
的 阅读 体验 2 分析 基础 数据 挖掘 用户 兴趣 
文章 价值 增强 推荐 广告 系统 的 预测 能力 3 
分析 用户 商业 意图 挖掘 流量 潜在 商业价值 提升 流量 
变现 4 研究 计算机 视觉 算法 给 用户 提供 更多 
更 酷炫 的 功能 岗位 要求 1 2018年 应届 毕业生 
本科 及 以上 学历 计算机 机器 学习 和 模式识别 相关 
专业 2 热爱 计算机科学 和 互联网 技术 对 人工智能 类 
产品 有 浓厚 兴趣 3 具备 强悍 的 编码 能力 
熟悉 linux 开发环境 熟悉 C + + 和 Python 语言 
优先 4 有/v 扎实/a 的/uj 数据/n 结构/n 和/c 算法/n 功底/n 
熟悉 机器学习 自然语言 处理 数据挖掘 分布式计算 计算机 视觉 中 一项 
或 多项 5 对 推荐 系统 计算 广告 搜索引擎 图像/n 
和/c 视频/n 处理/v 相关/v 技术/n 有/v 经验/n 者/k 优先/vn 6 
优秀 的 分析 问题 和 解决 问题 的 能力 对 
解决 具有 挑战性 问题 充满 激情 无码 科技 自然语言 处理 
工程师 于 /nr 无码 科技   in   杭州 无码 科技 
致力 构建 值得 用户 信赖 的 搜索引擎 我们 要 找 
自然语言 处理 工程师 期待 找到 长期 的 合作 伙伴 创始 
团队 成员 职位 描述 利用 自然 语言 处理 和 机器学习 
算法 对 海量 文本 数据 进行 挖掘 分析 包括 但 
不限 于 文本 聚 类 语义 理解 信息 抽取 知识图谱 
对话 生成 等 尝试 新的 机器学习 算法 计算 框架 提升 
机器学习 系统 效率 职位 要求 两年 以上 自然语言 处理 相关 
的 研发 经验 具备 较强 的 编码 能力 扎实 的 
数据 结构 和 算法 功底 熟悉 机器 学习 的 基本 
算法 与 概念 如 逻辑 回归 神经网络 决策树 等 熟悉 
自然语言 处理 常见 算法 与 模型 如 LDA Word2Vec CNN 
/ RNN 等 较好 的 英文 技术 文献 阅读 能力 
加 分项 发表 过 高水平 学术会 议论文 熟悉 Apache Hadoop 
/ Spark / Storm 等 至少 一种 分布式系统 有过 在 
医疗 数据 上 应用 机器学习 / 自然语言 处理 NLP 的 
经历 工作地点 杭州市 滨江区 总结 主要 在 以下 几个 方面 
有 要求 打勾 的 次数 反应 了 热度 1 . 
自然语言 处理 相关 的 具体 操作 分词 语义 句 意 
对话 机器翻译 自动 问答 等 √ √ √ √ √ 
2 . 经典 的 机器学习 算法 竞赛 经历 √ √ 
√ 3 . 多线程 网络 编程 分布式 编程 √ 4 
. hadoop spark √ √ √ √ 5 . SQL 
NoSQL √ 6 . linux √ √ 主要 目的 是 
克服 人机对话 中的 各种 限制 使 用户 能用/nr 自己 的 
语言 与 计算机 对话 相关 概念 自然 语言 是 指 
汉语 英语 法语 等 人们 日常 使用 的 语言 是 
自然而然 的 随着 人类 社会 发展 演变 而来 的 语言 
而 不是 人造 的 语言 如 程序 设计 的 语言 
它 是 人类 学习 生活 的 重要 工具 自然语言 处理 
是 指用 计算机 对 自然 语言 的 形 音 义 
等 信息 进行 处理 即对 字 词 句 篇章 的 
输入 输出 识别 分析 理解 生成 等 的 操作 和 
加工 自然语言 处理 的 具体 表现 形式 包括 机器翻译 文本 
摘要 文本 分类 文本校对 信息 抽取 语音合成 语音 识别 等 
自然语言 处理 机制 涉及 两个 流程 包括 自然 语言 理解 
和 自然 语言 生成 自然语言 理解 是 指 计算机 能够 
理解 自然语言 文本 的 意义 自然语言 生成 则是 指 能以 
自然语言 文本 来 表达 给定 的 意图 图 1 自然语言 
理解 层次 语音 分析 是 要根据 音位 规则 从语/nr 音流 
中 区分 出 一个 个 独立 的 音素 再 根据 
音位 形态 规则 找出 音节 及其 对应 的 词素 或 
词 词法 分析 的 目的 是 找出 词汇 的 各个 
词素 从中 获得 语言学 的 信息 句法分析 是 对 句子 
和 短语 的 结构 进行 分析 目的 是 要 找出 
词 短语 等 的 相互 关系 以及 各 自在 句中 
的 作用 语义分析 的 目的 是 找出 词义 结构 意义 
及其 结合 意义 从而 确定 语言所 表达 的 真正 含义 
或 概念 语用分析 则是 研究 语言所 存在 的 外界 环境 
对 语言 使用者 所 产生 的 影响 判断 计算机 是否 
理解 了 某种 自然 语言 的 具体 判别 标准 问答 
正确 回答 输入 文本 中 的 有关 问题 文摘 生成 
有 能力 生成 输入 文本 的 摘要 释义 用 不同 
的 词语 和 句型 来 复述 其 输入 的 文本 
翻译 把 一种 语言 翻译 成 另一种 语言 发展 历程 
时间 标志 意义 局限 1950年 图灵测试 自然语言 处理 思想 的 
开端 20 世纪 50 年代 到 70 年代 基于 规则 
的 方法 认为 自然语言 处理 的 过程 和 人类 学习 
认知 一门 语言 的 过程 是 类似 的 理性主义 思潮 
阶段 规则 不 可能 覆盖 所有 语句 开发者 不仅 要 
精通 计算机 还要 精通 语言学 70 年代 以后 基于 统计 
的 方法 逐渐 代替 了 基于 规则 的 方法 向 
经验主义 过渡 2008年 到现在 引入 深度 学习 来做 NLP 研究 
深度 学习 与 自然 语言 处理 的 结合 推向 了 
高潮 我国 现状 目前 自然语言 处理 的 研究 可以 分为 
基础性 研究 和 应用 性 研究 两 部分 语音 和 
文本 是 两类 研究 的 重点 基础性 研究 主要 涉及 
语言学 数学 计算机 学科 等 领域 相/v 对应/vn 的/uj 技术/n 
有/v 消除歧义/i 语法 形式化 等 应用 性 研究 则 主要 
集中 在 一些 应用 自然语言 处理 的 领域 例如 信息检索 
文本 分类 机器 翻译 等 由于 我国 基础理论 即 机器 
翻译 的 研究 起步 较早 且 基础理论 研究 是 任何 
应用 的 理论 基础 所以 语法 句法 语义分析 等 基础性 
研究 历来 是 研究 的 重点 而且 随着 互联网 网络 
技术 的 发展 智能 检索 类 研究 近年来 也 逐渐 
升温 业界 发展 微软 亚洲 研究院 微软 亚洲 研究院 1998年 
成立 自然语言 计算 组 研究 内容 包括 多 国 语言 
文本 分析 机器翻译 跨语言 信息检索 和 自动 问答 系统 等 
这些 研究 项目 研发 了 一系列 实用 成果 如 IME 
对联 游戏 Bing 词典 Bing 翻译器 语音 翻译 搜索引擎 等 
为 微软 产品 做出 了 重大 的 贡献 并且在 NLP 
顶级 会议 例如 ACL COLING 等 会议 上 发表 了 
许多 论文 神经网络 机器翻译 人机对话 小娜 聊天 机器人 小冰 GoogleGoogle 
对 自然 语言 处理 的 研究 侧重于 应用 规模 跨语言 
和跨/nr 领域 的 算法 其 成果 在 Google 的 许多 
方面 都被/nr 使用 提升 了 用户 在 搜索 移动 应用 
广告 翻译 等 方面 的 体验 机器翻译 知识图谱 语音识别 FacebookFacebook 
语言 技术 小组 不断改进 自然语言 处理 技术 以 改善 用户 
体验 致力于 机器翻译 语音 识别 和 会话 理解 2016年 Facebook 
首次 将 29层 深度 卷积 神经网络 用于 自然语言 处理 2017年 
Facebook 团队 使用 全新 的 卷积 神经 网络 进行 翻译 
以 9倍 于 以往 循环 神经 网络 的 速度 实现 
了 目前 最高 的 准确率 语音识别 文本处理 百度 百度 自然语言 
处理 部 是 百度 最早 成立 的 部门 之一 研究 
涉及 深度 问答 阅读 理解 智能 写作 对话 系统 机器翻译 
语义 计算 语言 分析 知识 挖掘 个性化 反馈 学习 等 
其中 百度 自然语言 处理 在 深度 问答 方向 经过 多年 
打磨 积累 了 问句 理解 答案 抽取 观点 分析 与 
聚合 等 方面 的 一整套 技术 方案 目前 已经 在 
搜索 度 秘 等 多个 产品 中 实现 应用 篇章 
理解 通过 篇章 结构 分析 主体 分析 内容 标签 情感 
分析 等 关键 技术 实现 对 文本 内容 的 理解 
目前 篇章 理解 的 关键 技术 已经 在 搜索 资讯 
流 糯米 等 产品 中 实现 应用 百度 翻译 目前 
支持 全球 28种 语言 覆盖 756个 翻译 方向 支持 文本 
语音 图像 等 翻译 功能 并 提供 精准 人工 翻译 
服务 满足 不同 场景 下 的 翻译 需求 在 多项 
翻译 技术 取得 重大 突破 发布 了 世界 上 首个 
线上 神经网络 翻译 系统 并 获得 2015 年度 国家 科技 
进步奖 阿里巴巴 阿里 自然语言 处理 为 其 产品 服务 在 
电商 平台 中 构建 知识图谱 实现 智能 导购 同时 进行 
全网 用户 兴趣 挖掘 在 客服 场景 中 也 运用 
自然语言 处理 技术 打造 机器人 客服 例如 蚂蚁 金融 智能 
小宝 淘宝 卖家 的 辅助 工具 千牛 插件 等 同时 
进行 语音 识别 以及 后续 分析 阿里 的 机器 翻译 
主要 与其 国家化 电商 的 规划 相 联系 可以 进行 
商品 信息 翻译 广告 关键词 翻译 买家 采购 需求 以及 
即时 通信 翻译 等 语种 覆盖 中文 荷兰语 希伯来语 等 
语种 2017 年初 阿里 正式 上线 了 自主 开发 的 
神经 网络 翻译 系统 进一步 提升 了 其 翻译 质量 
腾讯 AI Lab 是 腾讯 的 人工智能 实验室 研究 领域 
包括 计算机 视觉 语音识别 自然语言 处理 机器学习 等 其 研发 
的 腾讯 文智 自然语言 处理 基于 并行计算 分布式 爬虫 系统 
结合 独特 的 语义分析 技术 可满足 自然语言 处理 转码 抽取 
数据 抓取 等 需求 同时 基于 文智 API 还 可以 
实现 搜索 推荐 舆情 挖掘 等功能 在 机器 翻译 方面 
2017年 腾讯 宣布 翻译 君 上线 同声 传译 新功能 用户/n 
边/d 说/v 边翻的/nr 需求/v 得到/v 满足/v 语音识别 + NMT 等/u 
技术/n 的/uj 应用/v 保证/v 了/ul 边/d 说/v 边翻的/nr 速度/n 与/p 
精准性/i 京东 京东 AI 开放平台 基本上 由 模型 定制 化 
平台 和 在线 服务 模块 构成 其中 在线 服务 模块 
包括 计算机 视觉 语音 交互 自然语言 处理 和 机器 学习 
等 京东 AI 开放平台 计划 通过 建立 算法 技术 应用 
场景 数据链 间 的 连接 构建 京东 AI 发展 全 
价值链 实现 AI 能力 平台 化 转载自 https / / 
blog . csdn . net / valada / article / 
details / 80892583 第 01 课 中文 自然语言 处理 的 
完整 机器 处理 流程 有 机器学习 相关 经验 的 人都 
知道 中文 自然语言 处理 的 过程 和 机器 学习 过程 
大体一致 但又 存在 很多 细节 上 的 不同 点 下面/f 
我们/r 就/d 来看/u 看中/v 文/n 自然语言/l 处理/v 的/uj 基本/n 过程/n 
有/v 哪些/r 呢/y 获取 语料 语料 即 语言 材料 语料 
是 语言 学 研究 的 内容 语料 是 构成 语料库 
的 基本 单元 所以 人们 简单 地 用 文本 作为 
替代 并把 文本 中的 上下文 关系 作为 现实 世界 中 
语言 的 上下文 关系 的 替代品 我们 把 一个 文本 
集合 称为 语料库 Corpus 当 有几个 这样 的 文本 集合 
的 时候 我们 称之为 语料库 集合 Corpora 定义 来源 百度 
百科 按 语料 来源 我们 将 语料 分为 以下 两种 
1 . 已有 语料 很多 业务 部门 公司/n 等/u 组织/v 
随着/p 业务/n 发展/vn 都会/nr 积累/v 有/v 大量/n 的/uj 纸质/n 或者/c 
电子文本/n 资料/n 那么 对于 这些 资料 在 允许 的 条件 
下 我们 稍加 整合 把 纸质 的 文本 全部 电子化 
就 可以 作为 我们 的 语料库 2 . 网上 下载 
抓取 语料 如果 现在 个人 手里 没有 数据 怎么办 呢 
这个 时候 我们 可以 选择 获取 国内外 标准 开放 数据集 
比如 国内 的 中文 汉语 有 搜狗 语料 人民日报 语料 
国外 的 因为 大都 是 英文 或者 外文 这里 暂时 
用不到 也 可以 选择 通过 爬虫 自己 去 抓取 一些 
数据 然后 来 进行 后续 内容 语料 预处理 这里 重点 
介绍 一下 语料 的 预处理 在 一个 完整 的 中文 
自然语言 处理 工程 应用 中 语料 预处理 大概会 占到 整个 
50% 70% 的 工作量 所以 开发 人员 大 部分 时间 
就 在 进行 语料 预处理 下面 通过 数据 洗清 分词 
词性 标注 去 停用词 四 个大 的 方面 来 完成 
语料 的 预 处理工作 1 . 语料 清洗 数据 清洗 
顾名思义 就是 在 语料 中 找到 我们 感兴趣 的 东西 
把 不 感兴趣 的 视为 噪音 的 内容 清洗 删除 
包括 对于 原始 文本 提取 标题 摘要 正文 等 信息 
对于 爬 取 的 网页 内容 去除 广告 标签 HTML 
JS 等 代码 和 注释 等 常见 的 数据 清洗 
方式 有 人工 去 重 对齐 删除 和 标注 等 
或者 规则 提取 内容 正则表达式 匹配 根据 词性 和 命名 
实体 提取 编写 脚本 或者 代码 批处理 等 2 . 
分词 中文 语料 数据 为 一批 短 文本 或者 长 
文本 比如 句子 文章 摘要 段落 或者 整篇文章 组成 的 
一个 集合 一般 句子 段落 之间 的 字 词语 是 
连续 的 有 一定 含义 而 进行 文本 挖掘 分析 
时 我们 希望 文本处理 的 最小 单位 粒度 是 词 
或者 词语 所以 这个 时候 就 需要 分词 来 将 
文本 全部 进行 分词 常见 的 分词 算法 有 基于 
字符串 匹配 的 分词 方法 基于 理解 的 分词 方法 
基于 统计 的 分词 方法 和 基于 规则 的 分词 
方法 每 种方法 下面 对应 许多 具体 的 算法 当前/t 
中文分词/i 算法/n 的/uj 主要/b 难点/d 有/v 歧义/n 识别/v 和/c 新词/n 
识别/v 比如 羽毛球拍 卖完 了 这个 可以 切 分成 羽毛 
球拍 卖 完 了 也可 切 分成 羽毛球 拍卖 完 
了 如果 不 依赖 上下文 其他 的 句子 恐怕 很难 
知道 如何 去 理解 3 . 词性 标注 词性 标注 
就是 给 每个 词 或者 词语 打 词类 标签 如 
形容词 动词 名词 等 这样 做 可以 让 文本 在 
后面 的 处理 中 融入 更多 有用 的 语言 信息 
词性 标注 是 一个 经典 的 序列 标注 问题 不过 
对于 有些 中文 自然语言 处理 来说 词性 标注 不 是非 
必需 的 比如 常见 的 文本 分类 就 不用 关心 
词 性问题 但是 类似 情感 分析 知识 推理 却是 需要 
的 下图 是 常见 的 中文 词性 整理 常见 的 
词性 标注 方法 可以 分为 基于 规则 和 基于 统计 
的 方法 其中 基于 统计 的 方法 如 基于 最大熵 
的 词性 标注 基于 统计 最大 概率 输出 词性 和 
基于 HMM 的 词性 标注 4 . 去 停用词 停用词 
一般指 对 文本 特征 没有 任何 贡献 作用 的 字词 
比如 标点符号 语气 人称 等 一些 词 所以在 一般性 的 
文本 处理 中 分词 之后 接下来 一步 就是 去 停用词 
但是 对于 中文 来说 去 停用词 操作 不是 一成不变 的 
停用词 词典 是 根据 具体 场景 来 决定 的 比如 
在 情感 分析 中 语气词 感叹号 是 应该 保留 的 
因为 他们 对 表示 语气 程度 感情/n 色彩/n 有/v 一定/d 
的/uj 贡献/n 和/c 意义/n 特征 工程 做完 语料 预处理 之后 
接下来/l 需要/v 考虑/v 如何/r 把/p 分词/n 之后/f 的/uj 字/n 和/c 
词语/n 表示/v 成/n 计算机/n 能够/v 计算/v 的/uj 类型/n 显然 如果 
要 计算 我们 至少 需要 把 中文分词 的 字符串 转换成 
数字 确切的说 应该 是 数学 中的 向量 有/v 两种/m 常用/b 
的/uj 表示/v 模型/n 分别/d 是/v 词/n 袋/q 模型/n 和词/nr 向量/n 
词 袋 模型 Bag of Word BOW 即 不考虑 词语 
原本在 句子 中 的 顺序 直接 将 每 一个 词语 
或者 符号 统一 放置 在 一个 集合 如 list 然后 
按照 计数 的 方式 对 出现 的 次数 进行 统计 
统计 词频 这 只是 最 基本 的 方式 TF IDF 
是 词 袋 模型 的 一个 经典 用法 词 向量 
是 将 字 词语 转换成 向量 矩阵 的 计算 模型 
目前 为止 最 常用 的 词 表示 方法 是 One 
hot 这种 方法 把 每个 词 表示 为 一个 很长 
的 向量 这个 向量 的 维度 是 词表 大小 其中 
绝大多数 元素 为 0 只有 一个 维度 的 值 为 
1 这个 维度 就 代表 了 当前 的 词 还有 
Google 团队 的 Word2Vec 其 主要 包含 两 个 模型 
跳 字 模型 Skip Gram 和 连续 词 袋 模型 
Continuous Bag of Words 简称 CBOW 以及 两 种 高效 
训练 的 方法 负 采样 Negative Sampling 和 层序 Softmax 
Hierarchical Softmax 值得一提的是 Word2Vec 词 向量 可以 较好 地 表达 
不同 词 之间 的 相似 和 类比 关系 除此之外 还有 
一些 词 向量 的 表示 方式 如 Doc2Vec WordRank 和 
FastText 等 特征选择 同 数据挖掘 一样 在 文本 挖掘 相关 
问题 中 特征 工程 也 是 必不可少 的 在 一个 
实际 问题 中 构造 好 的 特征向量 是 要 选择 
合适 的 表达 能力 强的/nr 特征 文本 特征 一般 都是 
词语 具有 语义 信息 使用 特征选择 能够 找出 一个 特征 
子集 其 仍然 可以 保留 语义 信息 但 通过 特征提取 
找到 的 特征 子空间 将会 丢失 部分 语义 信息 所以 
特征选择 是 一个 很 有 挑战 的 过程 更多/d 的/uj 
依赖/v 于/p 经验/n 和/c 专业/n 知识/v 并且 有 很多 现成 
的 算法 来 进行 特征 的 选择 目前 常见 的 
特征 选择 方法 主要 有 DF MI IG CHI WLLR 
WFO 六种 模型 训练 在 特征向量 选择 好 之后 接下来 
要做 的 事情 当然 就是 训练 模型 对于 不同 的 
应用 需求 我们 使用 不同 的 模型 传统/n 的/uj 有/v 
监督/vn 和无/nr 监督/vn 等/u 机器学习/i 模型/n 如 KNN SVM Naive 
Bayes 决策树 GBDT K means 等 模型 深度 学习 模型 
比如 CNN RNN LSTM Seq2Seq FastText TextCNN 等 这些 模型 
在 后续 的 分类 聚 类 神经 序列 情感 分析 
等 示例 中 都会 用到 这里 不再 赘述 下面 是 
在 模型 训练 时 需要 注意 的 几个 点 1 
. 注意 过拟合 欠 拟合 问题 不断 提高 模型 的 
泛化 能力 过拟合 模型 学习 能力 太强 以至于 把 噪声 
数据 的 特征 也 学习 到了 导致 模型 泛化 能力 
下降 在 训练 集上 表现 很好 但是 在 测试 集上 
表现 很差 常见 的 解决 方法 有 增大 数据 的 
训练 量 增加 正则化 项 如 L1 正则 和 L2 
正则 特征 选取 不合理 人工 筛选 特征 和 使用 特征选择 
算法 采用 Dropout 方法 等 欠 拟合 就是 模型 不 
能够 很好 地 拟合 数据 表现 在 模型 过于 简单 
常见 的 解决 方法 有 添加 其他 特征 项 增加 
模型 复杂度 比如 神经网络 加 更多 的 层 线性 模型 
通过 添加 多项式 使 模型 泛化 能力 更强 减少 正则化 
参数 正则化 的 目的 是 用来 防止 过拟合 的 但是 
现在 模型 出现 了 欠 拟合 则 需要 减少 正则化 
参数 2 . 对于 神经网络 注意 梯度 消 失和 梯度 
爆炸 问题 评价 指标 训 练好 的 模型 上线 之前 
要 对模型 进行 必要 的 评估 目的 让 模型 对 
语料 具备 较好 的 泛化 能力 具体 有 以下 这些 
指标 可以 参考 1 . 错误率 精度 准确率 精确度 召回率 
F1 衡量 错误率 是 分类 错误 的 样本 数 占 
样本 总数 的 比例 对 样例 集 D 分类 错误率 
计算 公式 如下 精度 是 分类 正确 的 样本 数 
占 样本 总数 的 比例 这里 的 分类 正确 的 
样本 数 指 的 不仅 是 正 例 分类 正确 
的 个数 还有 反例 分类 正确 的 个数 对 样例 
集 D 精度 计算 公式 如下 对于 二分 类 问题 
可将 样例 根据 其 真实 类别 与 学习 器 预测 
类别 的 组合 划分 为 真正 例 True Positive 假 
正 例 False Positive 真 反例 True Negative 假 反例 
False Negative 四种 情形 令 TP FP TN FN 分别 
表示 其 对应 的 样 例数 则 显然有 TP + 
FP + + TN + FN = 样例 总数 分类 
结果 的 混淆 矩阵 Confusion Matrix 如下 准确率 缩写 表示 
用 P 准确率 是 针对 我们 预测 结果 而言 的 
它 表示 的 是 预测 为 正 的 样例 中有 
多少 是 真正 的 正 样例 定义 公式 如下 精确度 
缩写 表示 用 A 精确度 则是 分类 正确 的 样本 
数 占 样本 总数 的 比例 Accuracy 反应 了 分类器 
对 整个 样本 的 判定 能力 即 能将 正 的 
判定 为 正 的 负 的 判定 为 负 的 
定义 公式 如下 召回率 缩写 表示 用 R 召回率 是 
针对 我们 原来 的 样本 而言 的 它 表示 的 
是 样本 中的 正 例 有 多少 被 预测 正确 
定义 公式 如下 F1 衡量 表达出 对 查准率 / 查全率 
的 不同 偏好 定义 公式 如下 2 . ROC 曲线 
AUC 曲线 ROC 全称 是 受试者 工作 特征 Receiver Operating 
Characteristic 曲线 我们 根据 模型 的 预测 结果 把 阈值 
从0/nr 变到 最大 即 刚开始 是 把 每个 样本 作为 
正 例 进行 预测 随着 阈值 的 增大 学习 器 
预测 正 样 例数 越来越少 直到 最后 没有 一个 样本 
是 正 样例 在 这一 过程 中 每次 计算 出 
两个 重要 量 的 值 分别 以 它们 为 横 
纵坐标 作图 就 得到 了 ROC 曲线 ROC 曲线 的 
纵轴 是 真正 例 率 True Positive Rate 简称 TPR 
横轴 是 假 正 例 率 False Positive Rate 简称 
FPR 两者 分别 定义 为 ROC 曲线 的 意义 有 
以下 几点 ROC 曲线 能很/nr 容易 的 查出 任意 阈值 
对模型 的 泛化 性能 影响 有助于 选择 最佳 的 阈值 
可以 对 不同 的 模型 比较 性能 在 同一 坐标 
中 靠近 左上角 的 ROC 曲所/nr 代表 的 学习 器 
准确性 最高 如果 两条 ROC 曲线 没有 相交 我们 可以 
根据 哪条 曲线 最靠近 左上角 哪条 曲线 代表 的 学习 
器 性能 就 最好 但是 实际 任务 中 情况 很复杂 
若 两个 模型 的 ROC 曲线 发生 交叉 则 难以 
一般性 的 断言 两者 孰 优 孰 劣 此时 如果 
一定 要 进行 比较 则 比较 合理 的 判断 依据 
是 比较 ROC 曲 线下 的 面积 即 AUC Area 
Under ROC Curve AUC 就是 ROC 曲 线下 的 面积 
衡量 学习 器 优劣 的 一种 性能指标 AUC 是 衡量 
二 分类 模型 优劣 的 一种 评价 指标 表示 预测 
的 正 例 排在 负 例 前面 的 概率 前面 
我们 所讲 的 都是 针对 二分 类 问题 那么 如果 
实际 需要 在 多分 类 问题 中用 ROC 曲线 的话 
一般性 的 转化 为 多个 一对多 的 问题 即把 其中 
一个 当作 正 例 其余 当作 负 例 来 看待 
画出 多个 ROC 曲线 模型 上线 应用 模型 线上 应用 
目前 主流 的 应用 方式 就是 提供 服务 或者 将 
模型 持久化 第一 就是 线下 训练 模型 然后 将 模型 
做 线上 部署 发布 成 接口 服务 以供 业务 系统 
使用 第二 种 就是 在 线 训练 在线 训练 完成 
之后 把 模型 pickle 持久化 然后 在线 服务 接口 模板 
通过 读取 pickle 而 改变 接口 服务 模型 重构 非 
必须 随着 时间 和 变化 可能 需要 对模型 做 一定 
的 重构 包括 根据 业务 不 同侧 重点 对 上面 
提到 的 一至七 步骤 也 进行 调整 重新 训练 模型 
进行 上线 参考文献 周志华 机器学习 李航 统计 学习 方法 伊恩 
古德 费洛 深度 学习 第 02 课 简单 好用 的 
中文分词 利器 jieba 和 HanLP 前言 从 本文 开始 我们 
就 要 真正 进入 实战 部分 首先 我们 按照 中文 
自然语言 处理 流程 的 第一 步 获取 语料 然后 重点 
进行 中文分词 的 学习 中文分词 有 很多 种 常见 的 
比如 有 中科院计算所 NLPIR 哈工大 LTP 清华大学 THULAC 斯坦福 分词器 
Hanlp 分词器 jieba 分词 IKAnalyzer 等 这里 针对 jieba 和 
HanLP 分别 介绍 不同 场景 下 的 中文分词 应用 jieba 
分词 jieba 安装 1 Python 2 . x 下 jieba 
的 三种 安装 方式 如下 全自动 安装 执行命令 easy _ 
install jieba 或者 pip install jieba / pip3 install jieba 
可实现 全自动 安装 半自动 安装 先 下载 jieba 解压 后 
运行 python setup . py install 手动 安装 将 jieba 
目录 放置 于 当前目录 或者 site packages 目录 安 装完 
通过 import jieba 验证 安装 成功 与否 2 Python 3 
. x 下 的 安装 方式 Github 上 jieba 的 
Python3 . x 版本 的 路径 是 https / / 
github . com / fxsjy / jieba / tree / 
jieba3k 通过 git clone https / / github . com 
/ fxsjy / jieba . git 命令 下 载到 本地 
然后 解压 再通过 命令行 进入 解压 目录 执行 python setup 
. py install 命令 即可 安装 成功 jieba 的 分词 
算法 主要 有 以下 三种 基于 统计 词典 构造 前缀 
词典 基于 前缀 词典 对 句子 进行 切分 得到 所有 
切分 可能 根据 切分 位置 构造 一个 有向 无 环 
图 DAG 基于 DAG 图 采用 动态规划 计算 最大 概率 
路径 最 有可能 的 分词 结果 根据 最大 概率 路径 
分词 对于 新词 词库 中 没有 的 词 采用/v 有/v 
汉字/nz 成词/nr 能力/n 的/uj HMM 模型 进行 切分 jieba 分词 
下面 我们 进行 jieba 分词 练习 第一步 首先 引入 jieba 
和 语料 import jieba content = 现如今 机器 学习 和 
深度 学习 带动 人工智能 飞速 的 发展 并在 图片 处理 
语音识别 领域 取得 巨大 成功 1 精确 分词 精确 分词 
精确 模式 试图 将 句子 最 精确地 切开 精确 分词 
也是 默认 分词 segs _ 1 = jieba . cut 
content cut _ all = False print / . join 
segs _ 1 其 结果 为 现如今 / / 机器 
/ 学习 / 和//nr 深度 / 学习 / 带动 / 
人工智能 / 飞速 / 的 / 发展 / / 并 
/ 在 / 图片 / 处理 / / 语音 / 
识别 / 领域 / 取得 / 巨大成功 / 2 全/a 
模式/n 全/a 模式/n 分词/n 把 句子 中 所有 的 可能 
是 词语 的 都 扫描 出来 速度 非常 快 但 
不能 解决 歧义 segs _ 3 = jieba . cut 
content cut _ all = True print / . join 
segs _ 3 结果 为 现如今 / 如今 / / 
/ 机器 / 学习 / 和//nr 深度 / 学习 / 
带动 / 动人 / 人工 / 人工智能 / 智能 / 
飞速 / 的 / 发展 / / / 并 / 
在 / 图片 / 处理 / / / 语音 / 
识别 / 领域 / 取得 / 巨大 / 巨大成功 / 
大成 / 成功 / / 3 搜索引擎 模式 搜索引擎 模式 
在 精确 模式 的 基础 上 对 长词 再次 切分 
提高 召回率 适合 用于 搜索引擎 分词 segs _ 4 = 
jieba . cut _ for _ search content print / 
. join segs _ 4 结果 为 如今 / 现如今 
/ / 机器 / 学习 / 和//nr 深度 / 学习 
/ 带动 / 人工 / 智能 / 人工智能 / 飞速 
/ 的 / 发展 / / 并 / 在 / 
图片 / 处理 / / 语音 / 识别 / 领域 
/ 取得 / 巨大 / 大成 / 成功 / 巨大成功 
/ 4 用 lcut 生成 listjieba . cut 以及 jieba 
. cut _ for _ search 返回 的 结构 都是/nr 
一个 可 迭代 的 Generator 可以 使用 for 循环 来 
获得 分词 后 得到 的 每一个 词语 Unicode jieba . 
lcut 对 cut 的 结果 做了 封装 l 代表 list 
即 返回 的 结果 是 一个 list 集合 同样 的 
用 jieba . lcut _ for _ search 也 直接 
返回 list 集合 segs _ 5 = jieba . lcut 
content print segs _ 5 结果 为 现如今 机器 学习 
和 深度 学习 带动 人工智能 飞速 的 发展 并 在 
图片 处理 语音 识别 领域 取得 巨大成功 5 获取 词性 
jieba 可以 很 方便 地 获取 中文 词性 通过 jieba 
. posseg 模块 实现 词性 标注 import jieba . posseg 
as psg print x . word x . flag for 
x in psg . lcut content 结果 为 现如今 t 
x 机器 n 学习 v 和 c 深度 ns 学习 
v 带动 v 人工智能 n 飞速 n 的 uj 发展 
vn x 并 c 在 p 图片 n 处理 v 
x 语音 n 识别 v 领域 n 取得 v 巨大成功 
nr x 6 并行 分词 并行 分词 原理 为 文本 
按 行 分隔 后 分配 到 多个 Python 进程 并行 
分词 最后 归并 结果 用法 jieba . enable _ parallel 
4 # 开启 并行 分 词模式 参数 为 并行 进程 
数 jieba . disable _ parallel # 关闭 并行 分 
词模式 注意 并行 分词 仅 支持 默认 分词器 jieba . 
dt 和 jieba . posseg . dt 目前 暂 不支持 
Windows 7 获取 分词 结果 中词 列表 的 top nfrom 
collections import Counter top5 = Counter segs _ 5 . 
most _ common 5 print top5 结果 为 2 学习 
2 现如今 1 机器 1 和 1 8 自定义 添 
加词 和 字典 默认 情况 下 使用 默认 分词 是 
识别 不出 这句话 中的 铁甲 网 这个 新词 这里 使用 
用户 字典 提高 分词 准确性 txt = 铁甲 网 是 
中国 最大 的 工程 机械 交易平台 print jieba . lcut 
txt 结果 为 铁甲 网 是 中国 最大 的 工程机械 
交易平台 如果 添加 一个词 到 字典 看 结果 就 不 
一样 了 jieba . add _ word 铁甲 网 print 
jieba . lcut txt 结果 为 铁甲 网 是 中国 
最大 的 工程机械 交易平台 但是 如果 要 添加 很多 个 
词 一个 个 添加 效率 就 不够 高了 这时候 可以 
定义 一个 文件 然后 通过 load _ userdict 函数 加载 
自定义 词典 如下 jieba . load _ userdict user _ 
dict . txt print jieba . lcut txt 结果 为 
铁甲 网 是 中国 最大 的 工程机械 交易平台 注意事项 jieba 
. cut 方法 接受 三 个 输入 参数 需要 分词 
的 字符串 cut _ all 参数 用来 控制 是否 采用 
全 模式 HMM 参数 用来 控制 是否 使用 HMM 模型 
jieba . cut _ for _ search 方法 接受 两个 
参数 需要 分词 的 字符串 是否 使用 HMM 模型 该 
方法 适合 用于 搜索引擎 构建 倒排索引 的 分词 粒度 比较 
细 HanLP 分词 pyhanlp 安装 其为 HanLP 的 Python 接口 
支持 自动 下载 与 升级 HanLP 兼容 Python2 Python3 安装 
命令 为 pip install pyhanlp 使用 命令 hanlp 来 验证 
安装 pyhanlp 目前 使用 jpype1 这个 Python 包来/nr 调用 HanLP 
如果 遇到 building _ jpype extensionerror Microsoft Visual C + 
+ 14.0 is required . Get it with Microsoft VisualC 
+ + Build Tools http / / landinghub . visualstudio 
. com / visual cpp build tools 则 推荐 利用 
轻量级 的 Miniconda 来 下载 编译 好 的 jpype1 conda 
install c conda forge jpype1 pip install pyhanlp 未 安装 
Java 时会 报错 jpype . jvmfinder . J V M 
N o t F o u n d E x 
c e p t i o n No JVM shared 
library file jvm . dll found . Try setting up 
the JAVAHOME environment variable properly . HanLP 主 项目 采用 
Java 开发 所以 需要 Java 运行 环境 请 安装 JDK 
命令行 交互式 分词 模式 在 命令行 界面 使用 命令 hanlp 
segment 进入 交互 分 词模式 输入 一个 句子 并 回车 
HanLP 会 输出 分词 结果 可见 pyhanlp 分词 结果 是 
带有 词性 的 服务器 模式 通过 hanlp serve 来 启动 
内置 的 HTTP 服务器 默认 本地 访问 地址 为 http 
/ / localhost 8765 也 可以 访问 官网 演示 页面 
http / / hanlp . hankcs . com / 通过 
工具 类 HanLP 调用 常用 接口 通过 工具 类 HanLP 
调用 常用 接口 这种 方式 应该 是 我们 在 项目 
中 最 常用 的 方式 1 分词 from pyhanlp import 
* content = 现如今 机器 学习 和 深度 学习 带动 
人工智能 飞速 的 发展 并在 图片 处理 语音识别 领域 取得 
巨大 成功 print HanLP . segment content 结果 为 现如今 
/ t / w 机器学习 / gi 和//nr cc 深度 
/ n 学习 / v 带动 / v 人工智能 / 
n 飞速 / d 的 / ude1 发展 / vn 
/ w 并 / cc 在 / p 图片 / 
n 处理 / vn / w 语音 / n 识别 
/ vn 领域 / n 取得 / v 巨大 / 
a 成功 / a / w 2 自定义 词典 分词 
在 没有 使用 自定义 字典 时的/nr 分词 txt = 铁甲 
网 是 中国 最大 的 工程 机械 交易平台 print HanLP 
. segment txt 结果 为 铁甲 / n 网 / 
n 是 / vshi 中国 / ns 最大 / gm 
的 / ude1 工程 / n 机械 / n 交易 
/ vn 平台 / n / w 添加 自定义 新词 
CustomDictionary . add 铁甲 网 CustomDictionary . insert 工程机械 nz 
1024 CustomDictionary . add 交易平台 nz 1024 n 1 print 
HanLP . segment txt 结果 为 铁甲 网 / nz 
是 / vshi 中国 / ns 最大 / gm 的 
/ ude1 工程机械 / nz 交易平台 / nz / w 
当然 了 jieba 和 pyhanlp 能做 的 事 还有 很多 
关键词 提取 自动 摘要 依存 句法分析 情感 分析 等 后面 
章节 我们 将会 讲到 这里 不再 赘述 参考文献 https / 
/ github . com / fxsjy / jiebahttps / / 
github . com / hankcs / pyhanlp 概述 统计 自然语言 
处理 的 目的 就是 针对 自然语言 领域 进行 统计 推理 
作为 一个 常用 的 统计 估计 的 例子 我们 将 
考察 经典 建模 问题 即 当前 词 预测 下 一个 
词 词汇 预测 任务 是 一项 技术 可以 解决 的 
简单 明了 的 问题 Bins 构造 等价 类 利用 历史 
词汇 预测 词汇 我们 构造 这样 一个 模型 模型 中 
所有 历史 都是 前 n 1个 已经 出现 的 词 
那么 我们 就 有一个 n 1 阶 马尔可夫 模型 或者 
称 N 元 语法 模型 随着 n 的 增加 和 
词表 数量 的 增加 我们 把 数据 划分 到 太多 
的 类别 中 有 大量 的 参数 要去 估计 所以 
有 一些 方法 如 词干 化 来 减小 词语 表 
数量 使用 2 3 元语言 模型 来 预测 等 另外 
还有 很多 模型 能比 较好 的 进行 预测 比如 我们 
可以 想象 如果 我们 知道 句子 的 主谓宾 等 一节 
结构 我们 可以 基于 谓语 来 识别 下 一个 词 
但 这里 只 介绍 n 元 语法 模型 构建/v n/w 
元/m 语法/n 模型/n 对于/p n/w 元/m 语法/n 模型/n 的/uj 例子/n 
我们 感兴趣 的 是 概率 P w1 wn 和 预测 
任务 P wn | w1 . wn 1 我们 可以 
利用 MLE 最大 似 然 估计 去 预测 这个 里面 
存在 的 问题 就是 对于 没有 出现 的 n 元组 
我们 统统 给了 0 概率 这个 问题 是 普遍 存在 
的 没有 如此 大 的 一个 数据集 能让/nr 我们 满足 
不 出现 稀疏 的 情况 虽然 有些 办法 试图 去 
解决 这个 问题 比如 我们 动态 调整 n 的 大小 
然后 在 超大 的 数据 集 上去 跑 但是 终究 
这些 方法 是 不 完备 的 我们 需要 尝试 平滑 
的 去 处理 那些 没有 在 历史 中 出现 的 
情况 并且 给 这些 情况 赋予 一定 的 概率 Laplace 
法则 Lidstone 法则 和 Jeffreys Perks 法则 这种 处理 方式 
常常 被 非正式 的 称为 加 1 法 它 把 
一小部分 概率 有效 地 转移 到 了 未知 事件 上 
这里 的 假设 是 有 统一 的 先 验证 概率 
每个 n 元组 都有 相同 的 可能性 事实上 就是 一个 
贝叶斯 估计 个人 觉得 这个 假设 不太 成立 但是 这 
是 一种 平滑 的 方法 吧 这些 方法 有 一些 
缺点 如在 预测 句子 中 概率 都被 打了 折扣 我 
认为 相对 比较 而言 这个 折扣 关系 不是 那么 大 
并且 结果 证明 差 的 上下文 不如 没有 这里 可能 
是 平滑 时 将 0 概率 比 低 出现 的 
更大 导致 了 这种 情况 这种 建模 是 可以 对 
概率 估计 进行 排序 的 下面 放 一张 图 解释一下 
相对 性问题 证明 该 建模 概率 折扣 与 相对 有效性 
留存 估计 这里 的 留存 估计 与 机器 学习 中 
讨论 的 比较 接近 是 一种 自我 验证 的 方法 
一定 程度 上 防止 训练 或者 决策 的 过拟合 书中 
介绍 了 讲 训练 集 划 分成 两部分 的 方法 
也 介绍 了 交叉 验证法 Leaving one Out 的 方法 
应该 就是 我们 机器学习 中 常用 的 N 折叠法 这里 
就 不再 描述 Good TuringGood 根据 图灵机 原理 提出 了 
一种 确定时间 频率 或者 概率 估计 的 方法 假设 事件 
是 二项分布 的 这种方法/i 适用/v 于从大/nr 此表/i 得来/v 的/uj 大量/n 
数据/n 观察/v 而且 尽管 词汇 和n/nr gram 不服从 二项式 分布 
因为 概率论 没学 太好 不太能 直观 体会 到 不同 分布 
的 感觉 _ 该 方法 利用 了 一个 调整 后的/nr 
频率 参见 如下 这里 具体 的 算法 本人 理解 不是 
很 透彻 但是 大致 思路 是 这样 这些 方法 都 
重新 归一化 了 所有 的 概率 估计 以 确保 得到 
合理 的 概率分布 结果 比如 我们 调整 转移 到 未知事件 
上 的 概率 大小 或者 很好 的 方法 似乎 是 
保持 转移到 未知事件 上 的 概率 N1 / N 不变 
并且 重新 归一化 所有 的 已知 事件 的 概率 估计 
Gale and Sampson 1995 提出 简单 总结 为了 处理 空类 
或者说 是 数据 集中 不 存在 的 元组 我们 采用 
折扣 的 的 方法 将 频率 增益 均 分到 未知事件 
上 然后 有 几种 均分 方案 绝对 折扣 线性 折扣 
等等 吧 各有 各 的 特性 组合 估计法 对于 ｎ 
gram 模型 来说 找到 合适 的 组合 不同 阶 模型 
的 方法 是 成功 的 关键 一种 合并 不同 ｎ 
值 的 ＭＬＥ ｎ gram 估计 对于 未知 词 有 
一定 的 概率 转移 的 方法 使用 了 简单 的 
线性插值 技术 得出 了 一个 非常 好 的 语言 模型 
简单 的 线性 模型 解决 trigram 模型 中 稀疏 问题 
的 一种 方法 是 把 bigram 模型 和 unigram 模型 
组 合到 trigram 模型 中 这 两个 模型 容忍 稀疏 
数据 问题 的 能力 比较 强 回退 法当/nr 没有 ｎ 
gram 时 回 退到 低阶 的 模型 同时 这里 也 
是 需要 注意 需要 讲 概率 转移到 未知 词语 上 
贝叶斯 估计 是 什么 参考 http / / abook . 
hep . com . cn / 1865081 智能 的 概念 
知识 + 智力 获取 并 应用 知识 求解 问题 智能 
的 特征 感知 记忆 思维 处理信息 学习 行为 思维 逻辑 
形象 顿悟 灵感 基本内容 知识 表示 机器 感知 机器 思维 
机器学习 机器 行为 研究领域 自动 定理证明 博弈 模式识别 机器 视觉 
自然语言 理解 智能 信息检索 数据挖掘 与 知识发现 找出 有 意义 
的 专家系统 自动 程序设计 机器人 组合 优化 人工神经网络 分布式 人工智能 
与 多智能体 智能控制 广义 问题 决策 与 规划 智能 仿真 
建模 实验 结果 分析 智能 CAD 设计 智能 CAI 教学 
智能 管理 决策 智能 多媒体 综合 处理 文字 图形 智能 
操作系统 自动 管理 维护 智能 计算机 系统 智能 通信 智能 
网络 计网/nr 人工生命 模拟 语音识别 是以 语音 为 研究 对象 
通过/p 语音/n 信号/n 处理/v 和/c 模式识别/n 让/v 机器/n 自动/vn 识别/v 
和/c 理解/v 人类/n 口述/n 的/uj 语言/n 语音 识别 技术 就是 
让 机器 通过 识别 和 理解 过程 把 语音信号 转变 
为 相应 的 文本 或 命令 的 高技术 语音 识别 
是 一门 涉及面 很广 的 交叉 学科 它 与 声学 
语音学 语言学 信息 理论 模式识别/n 理论/n 以及/c 神经/n 生物学/n 等/u 
学科/n 都有/nr 非常/d 密切/ad 的/uj 关系/n 语音 识别 技术 正 
逐步 成为 计算机 信息 处理 技术 中 的 关键 技术 
语音 技术 的 应用 已经 成为 一个 具有 竞争性 的 
新兴 高技术 产业 1 语音 识别 的 基本 原理 语音 
识别 系统 本质上 是 一种 模式 识别系统 包括 特征提取 模式匹配 
参考 模式 库 等 三个 基本 单元 它 的 基本 
结构 如下 图 所示 未知 语音 经过 话筒 变 换成 
电信号 后加 在 识别 系统 的 输入端 首先 经过 预处理 
再 根据 人 的 语音 特点 建立 语音 模型 对 
输入 的 语音 信号 进行 分析 并 抽取 所需 的 
特征 在此 基础 上 建立 语音识别 所需 的 模板 而 
计算机 在 识别 过程 中 要根据 语音 识别 的 模型 
将 计算机 中 存放 的 语音 模板 与 输入 的 
语音 信号 的 特征 进行 比较 根据 一定 的 搜索 
和 匹配 策略 找出 一 系列 最优 的 与 输入 
语音 匹配 的 模板 然后 根据 此 模板 的 定义 
通过 查表 就 可以 给出 计算机 的 识别 结果 显然 
这种 最优 的 结果 与 特征 的 选择 语音 模型 
的 好坏 模板 是否 准确 都有 直接 的 关系 语音 
识别 系统 构建 过程 整体 上 包括 两 大部分 训练 
和 识别 训练 通常 是 离线 完成 的 对 预先 
收集 好 的 海量 语音 语言 数据库 进行 信号 处理 
和 知识 挖掘 获取 语音 识别 系统 所 需要 的 
声学 模型 和 语言 模型 而 识别 过程 通常 是 
在线 完成 的 对 用户 实时 的 语音 进行 自动 
识别 识别 过程 通常 又 可以 分为 前端 和 后端 
两 大 模块 前端 模块 主要 的 作用 是 进行 
端点 检测 去除 多余 的 静 音和 非 说话声 降噪 
特征提取 等 后端 模块 的 作用 是 利用 训 练好 
的 声学 模型 和 语言 模型 对 用户 说话 的 
特征向量 进行 统计 模式识别 又称 解码 得到 其 包含 的 
文字 信息 此外 后端 模块 还 存在 一个 自适应 的 
反馈 模块 可以 对 用户 的 语音 进行 自 学习 
从而 对 声学 模型 和 语音 模型 进行 必要 的 
校正 进一步 提高 识别 的 准确率 语音 识别 是 模式识别 
的 一个 分支 又 从属于 信号处理 科学 领域 同时 与 
语音学 语言学 数理统计 及 神经 生物学 等 学科 有 非常 
密切 的 关系 语音 识别 的 目的 就是 让 机器 
听懂 人类 口述 的 语言 包括了 两 方面 的 含义 
其一 是 逐字逐句 听懂 非 转化成 书面 语言 文字 其二 
是 对口 述语 言中 所 包含 的 要求 或 询问 
加以 理解 做出 正确 响应 而不 拘泥 于 所有 词 
的 正确 转换 自动 语音 识别 技术 有 三个 基本原理 
首先 语音信号 中 的 语言 信息 是 按照 短时 幅度 
谱 的 时间 变化 模式 来 编码 其次 语音 是 
可以 阅读 的 即 它 的 声学 信号 可以 在 
不考虑 说话 人 试图 传达 的 信息 内容 的 情况 
下用 数十 个 具有 区别性 的 离散 的 符号 来 
表示 第三 语音 交互 是 一个 认知过程 因而 不 能与 
语言 的 语法 语义 和 语用 结构 割裂 开来 声学 
模型 语音 识别 系统 的 模型 通常 由 声学 模型 
和 语言 模型 两部分 组成 分别/d 对应/vn 于/p 语音/n 到/v 
音节/n 概率/n 的/uj 计算/v 和/c 音节/n 到/v 字/n 概率/n 的/uj 
计算/v 声学 建模 语言 模型 搜索 连续 语音识别 中的 搜索 
就是 寻找 一个 词 模型 序列 以 描述 输入 语音信号 
从而 得到 词 解码 序列 搜索 所 依据 的 是 
对 公式 中的 声学 模型 打分 和 语言 模型 打分 
在 实际 使用 中 往往 要 依据 经验 给 语言 
模型 加上 一个 高 权重 并 设置 一个 长词 惩罚 
分数 系统 实现 语音 识别 系统 选择 识别 基元 的 
要求 是 有 准确 的 定义 能 得到 足够 数据 
进行 训练 具有 一般性 英语 通常 采用 上下文 相关 的 
音素 建模 汉语 的 协同 发音 不如 英语 严重 可以 
采用 音节 建模 系统 所需 的 训练 数据 大小 与 
模型 复杂度 有关 模型 设计 得 过于 复杂 以至于 超出 
了 所 提供 的 训练 数据 的 能力 会 使得 
性能 急剧下降 听写 机 大 词汇量 非 特定人 连续 语音 
识别 系统 通常 称为 听写 机 其 架构 就是 建立 
在 前述 声学 模型 和 语言 模型 基础上 的 HMM 
拓扑结构 训练/vn 时对/nr 每个/r 基元/n 用/p 前/f 向/p 后向/i 算法/n 
获得/v 模型/n 参数/n 识别 时 将 基元 串接 成词/nr 词 
间 加上 静音 模型 并 引入 语言 模型 作为 词 
间 转移 概率 形成 循环 结构 用 Viterbi 算法 进行 
解码 针对 汉语 易于 分割 的 特点 先 进行 分割 
再 对 每 一段 进行 解码 是 用以 提 高效率 
的 一个 简化 方法 对话 系统 用于 实现 人机 口语 
对话 的 系统 称为 对话 系统 受 目前 技术所限 对话 
系统 往往 是 面向 一个 狭窄 领域 词汇 量 有限 
的 系统 其 题材 有 旅游 查询 订票 数据库 检索 
等等 其 前端 是 一个 语音 识别 器 识别 产生 
的 N best 候选 或 词 候选 网格 由 语法分析 
器 进行 分析 获取 语义 信息 再由 对话 管理器 确定 
应答 信息 由 语音 合成器 输出 由于 目前 的 系统 
往往 词汇 量 有限 也 可以 用 提取 关键词 的 
方法 来 获取 语义 信息 二 语音 识别 技术 原理 
工作 原理 解读 首先 我们 知道 声音 实际上 是 一种 
波 常见 的 mp3 等 格式 都是 压缩 格式 必须 
转成 非 压缩 的 纯 波形 文件 来 处理 比如 
Windows   PCM 文件 也 就是 俗称 的 wav 文件 
wav 文件 里 存储 的 除了 一个 文件 头 以外 
就是 声音 波形 的 一个 个 点了 下图 是 一个 
波形 的 示例 图中 每 帧 的 长度 为 25 
毫秒 每两 帧 之间 有 25 10 = 15 毫秒 
的 交叠 我们 称为 以 帧 长 25ms 帧 移 
10ms 分 帧 分 帧 后 语音 就 变成 了 
很多 小段 但 波形 在 时域 上 几乎 没有 描述 
能力 因此 必须 将 波形 作 变换 常见 的 一种 
变换 方法 是 提取 MFCC 特征 根据 人耳 的 生理 
特性 把 每 一帧 波形 变成 一个 多维 向量 可以 
简单 地 理解 为 这个 向量 包含 了 这帧 语音 
的 内容 信息 这个 过程 叫做 声学 特征提取 实际 应用 
中 这 一步 有 很多 细节 声学 特征 也 不止 
有 MFCC 这 一种 具体 这里 不讲 至此 声音 就成 
了 一个 12行 假设 声学 特征 是 12 维 N 
列 的 一个 矩阵 称之为 观察 序列 这里 N 为总 
帧数 观察 序列 如下 图 所示 图中 每 一帧 都用 
一个 12 维 的 向量 表示 色块 的 颜色 深浅 
表示 向 量值 的 大小 接下来 就要 介绍 怎样 把 
这个 矩阵 变成 文本 了 首先 要 介绍 两个 概念 
音素 单词 的 发音 由 音素 构成 对 英语 一种 
常用 的 音素 集 是 卡内基 梅隆 大学 的 一套 
由 39个 音素 构成 的 音素 集 参见 The   
CMU   Pronouncing   DicTIonary 汉语 一般 直接 用 全部 
声母 和 韵母 作为 音素 集 另外 汉语 识别 还分 
有调 无调 不 详述 状态 这里 理解 成比/nr 音素 更 
细致 的 语音 单位 就 行啦 通常 把 一个 音素 
划分 成 3个 状态 语音 识别 是 怎么 工作 的 
呢 实际上 一点 都不/nr 神秘 无非是 第一步 把 帧 识 
别成 状态 难点 第二步 把 状态 组合成 音素 第三步 把 
音素 组合成 单词 如下 图 所示 图中 每个 小 竖条 
代表 一帧 若干 帧 语音 对应 一个 状态 每 三个 
状态 组合 成 一个 音素 若干个 音素 组合 成 一个 
单词 也 就是说 只要 知道 每 帧 语音 对应 哪个 
状态 了 语音 识别 的 结果 也 就 出来 了 
那每帧/nr 音素 对应 哪个 状态 呢 有个 容易 想到 的 
办法 看 某 帧 对应 哪个 状态 的 概率 最大 
那 这帧 就 属于 哪个 状态 比如 下面 的 示意图 
这帧 在 状态 S3 上 的 条件 概率 最大 因此 
就 猜 这帧 属于 状态 S3 那/r 这些/r 用到/v 的/uj 
概率/n 从/p 哪里/r 读取/v 呢/y 有个 叫 声学 模型 的 
东西 里面 存 了 一大堆 参数 通过 这些 参数 就 
可以 知道 帧 和 状态 对应 的 概率 获取 这 
一大堆 参数 的 方法 叫做 训练 需要 使用 巨大 数量 
的 语音 数据 训练 的 方法 比较 繁琐 这里 不讲 
但 这样 做 有 一个 问题 每 一帧 都会 得到 
一个 状态 号 最后 整个 语音 就会 得到 一堆 乱七八糟 
的 状态 号 相邻 两 帧 间 的 状态 号 
基本 都 不相同 假设 语音 有 1000 帧 每 帧 
对应 1个 状态 每 3个 状态 组合 成 一个 音素 
那么 大概会 组合成 300个 音素 但 这段 语音 其实 根本 
没有 这么 多 音素 如果 真 这么 做 得到 的 
状态 号 可能 根本 无法 组合成 音素 实际上 相邻 帧 
的 状态 应该 大多数 都是/nr 相同 的 才 合理 因为 
每 帧 很短 解决 这个 问题 的 常用 方法 就是 
使用 隐 马尔可夫 模型 Hidden   Markov   Model HMM 
这 东西 听 起来 好像 很 高深 的 样子 实际上 
用 起来 很 简单 第一步 构建 一个 状态 网络 第二步 
从 状态 网络 中 寻找 与 声音 最 匹配 的 
路径 这样 就 把 结果 限制 在 预先 设定 的 
网络 中 避免 了 刚才 说到 的 问题 当然 也 
带来 一个 局限 比如 你 设定 的 网络 里 只 
包含 了 今天 晴天 和 今天 下雨 两个 句子 的 
状态 路径 那么 不管 说 些 什么 识别 出 的 
结果 必然 是 这两个 句子 中 的 一句 那 如果 
想 识别 任意 文本 呢 把 这个 网络 搭 得 
足够 大 包含 任意 文本 的 路径 就 可以 了 
但 这个 网络 越大 想要 达到 比较好 的 识别 准确率 
就越 难 所以 要 根据 实际 任务 的 需求 合理 
选择网络 大小 和 结构 搭建 状态 网络 是由 单词 级 
网络 展开 成 音素 网络 再 展开 成 状态 网络 
语音识别 过程 其实 就是 在 状态 网络 中 搜索 一条 
最佳 路径 语音 对应 这条 路径 的 概率 最大 这 
称之为 解码 路径 搜索 的 算法 是 一种 动态规划 剪枝 
的 算法 称之为 Viterbi 算法 用于 寻找 全局 最优 路径 
这里 所说 的 累积 概率 由 三 部分 构成 分别 
是 观察 概率 每 帧 和 每个 状态 对应 的 
概率 转移 概率 每个 状态 转移 到 自身 或 转移 
到 下个 状态 的 概率 语言 概率 根据 语言 统计 
规律 得到 的 概率 其中 前 两种 概率 从 声学 
模型 中 获取 最后 一种 概率 从 语言 模型 中 
获取 语言 模型 是 使用 大量 的 文本 训练 出来 
的 可以 利用 某 门 语言 本身 的 统计 规律 
来 帮助 提升 识别 正确率 语言 模型 很重要 如果 不 
使用 语言 模型 当 状态 网络 较大 时 识别 出 
的 结果 基本 是 一团乱麻 这样 基本上 语音识别 过程 就 
完成 了 这 就是 语音 识别 技术 的 原理 三 
语音 识别 技术 原理 语音 识别 系统 的 工作 流程 
一般来说 一套 完整 的 语音 识别 系统 其 工作 过程 
分为 7步 ① 对 语音信号 进行 分析 和 处理 除去 
冗余 信息 ② 提取 影响 语音 识别 的 关键 信息 
和 表达 语言 含义 的 特征 信息 ③ 紧扣 特征 
信息 用 最小 单元 识别 字词 ④ 按照 不同 语言 
的 各自 语法 依照 先后 次序 识别 字词 ⑤ 把 
前后 意思 当作 辅助 识别 条件 有利于 分析 和 识别 
⑥ 按照 语义分析 给 关键 信息 划 分段落 取出 所 
识别 出 的 字词 并 连接 起来 同时 根据 语句 
意思 调整 句子 构成 ⑦ 结合 语义 仔细分析 上下文 的 
相互 联系 对 当前 正在 处理 的 语句 进行 适当 
修正 音 识别 系统 基本 原理 框图 语音 识别 系统 
基本 原理 结构 如图所示 语音识别 原理 有 三点 ① 对 
语音信号 中 的 语言 信息 编码 是 按照 幅度 谱 
的 时间 变化 来 进行 ② 由于 语音 是 可以 
阅读 的 也 就是说 声学 信号 可以 在 不考虑 说话 
人 说话 传达 的 信息 内容 的 前提 下用 多个 
具有 区别性 的 离散 的 符号 来 表示 ③ 语音 
的 交互 是 一个 认知过程 所以 绝对 不能 与 语法 
语义 和 用语 规范 等 方面 分 裂开来 预处理 其中 
就 包括 对 语音信号 进行 采样 克服 混 叠 滤波 
去除 部分 由 个体 发音 的 差异 和 环境 引起 
的 噪声 影响 此外 还 会 考虑到 语音识别 基本 单元 
的 选取 和 端点 检测 问题 反复 训练 是 在 
识别 之前 通过 让 说话 人 多次 重复 语音 从 
原始 语音信号 样本 中 去除 冗余 信息 保留 关键 信息 
再 按照 一定 规则 对 数据 加以 整理 构成 模式 
库 再者 是 模式匹配 它 是 整个 语音 识别 系统 
的 核心 部分 是 根据 一 定 规则 以及 计算 
输入 特征 与 库存 模式 之间 的 相似 度 进而 
判断 出 输入 语音 的 意思 前端 处理 先 对 
原始 语音信号 进行 处理 再 进行 特征提取 消除 噪声 和 
不同 说 话人 的 发音 差异 带来 的 影响 使 
处理 后的/nr 信号 能够 更 完整 地 反映 语音 的 
本质 特征提取 消除 噪声 和 不同 说 话人 的 发音 
差异 带来 的 影响 使 处理 后的/nr 信号 能够 更 
完整 地 反映 语音 的 本质 特征 四 语音 识别 
技术 原理 发展 历程 早在 计算机 发明 之前 自动 语音 
识别 的 设想 就 已经 被 提上 了 议事日程 早期 
的 声码器 可被 视作 语音 识别 及 合成 的 雏形 
而 1920 年代 生产 的 Radio   Rex 玩具狗 可能 
是 最早 的 语音 识别 器 当 这只 狗 的 
名字 被 呼唤 的 时候 它 能够 从 底座 上 
弹出来 最早 的 基于 电子 计算机 的 语音 识别 系统 
是由 AT & T 贝尔实验室 开发 的 Audrey 语音 识别 
系统 它 能够 识别 10个 英文 数字 其 识别 方法 
是 跟踪 语音 中的 共振 峰 该 系统 得到 了 
98% 的 正确率 到 1950 年代 末 伦敦 学院 Colledge 
  of   London 的 Denes 已经 将 语法 概率 
加入 语音识别 中 1960 年代 人工神经网络 被 引入 了 语音 
识别 这一 时代 的 两 大 突破 是 线性 预测 
编码 Linear   PredicTIve   Coding   LPC   及 
动态 时间 弯折 Dynamic   Time   Warp 技术 语音 
识别 技术 的 最 重大 突破 是 隐含 马尔科夫 模型 
Hidden   Markov   Model 的 应用 从 Baum 提出 
相关 数学 推理 经过 Labiner 等人 的 研究 卡内基 梅隆 
大学 的 李开复 最终 实现 了 第一个 基于 隐 马尔科夫 
模型 的 大 词汇量 语音 识别 系统 Sphinx 此后 严格来说 
语音 识别 技术 并 没有 脱离 HMM 框架 实验室 语音识别 
研究 的 巨大 突破 产生于 20 世纪 80 年代 末 
人们 终于 在 实验室 突破 了 大 词汇量 连续 语音 
和非/nr 特定人 这 三大 障碍 第一次 把 这三个 特性 都 
集成 在 一个 系统 中 比较 典型 的 是 卡耐基 
梅隆 大学 C a r n e g i e 
M e l l o n U n i v 
e r s i t y 的 Sphinx 系统 它 
是 第一 个 高性能 的 非 特定人 大 词汇量 连续 
语音 识别 系统 这 一时期 语音识别 研究 进一步 走向 深入 
其 显著 特征 是 HMM 模型 和 人工 神经元网络 ANN 
在 语音 识别 中 的 成功 应用 HMM 模型 的 
广泛 应用 应 归功于 AT & TBell 实验室 Rabiner 等 
科学家 的 努力 他们 把 原本 艰涩 的 HMM 纯 
数学模型 工程化 从而 为 更多 研究 者 了解 和 认识 
从而 使 统计 方法 成为 了 语音 识别 技术 的 
主流 20 世纪 90 年代 前期 许多 著名 的 大 
公司 如 IBM 苹果 AT & T 和 NTT 都对 
语音 识别 系统 的 实用化 研究 投以 巨资 语音 识别 
技术 有 一个 很好 的 评估 机制 那 就是 识别 
的 准确率 而 这项 指标 在 20 世纪 90 年代 
中后期 实验室 研究 中 得到 了 不断 的 提高 比较 
有 代表性 的 系统 有 IBM 公司 推出 的 ViaVoice 
和 DragonSystem 公司 的 N a t u r a 
l l y p e a k i n g 
Nuance 公司 的 N u a n c e V 
o i c e P l a t f o 
r m 语音 平台 Microsoft 的 Whisper Sun 的 VoiceTone 
等 原文 来源 codeburst . io 作者 Pramod Chandrayan 「 
雷克 世界 」 编译 嗯 ~ 阿童木 呀 我 是 
卡 布达 现如今 在 更多 情况 下 我们 是 以 
比特 和 字节 为生 而 不是 依靠 交换 情感 我们 
使用 一种 称之为 计算机 的 超级 智能 机器 在 互联 
网上 进行 交易 和 沟通 因此 我们/r 觉得/v 有/v 必要/d 
让/v 机器/n 明白/nr 我们/r 在/p 说话/v 时是/nr 如何/r 对/p 其/r 
进行/v 理解/v 的/uj 并且 试图用 人工智能 一种 称之为 NLP 自然语言 
处理 技术 为 它们 提供 语言 作为 一种 研究结果 聊天 
机器 人 正在 成为 一种 可靠 的 聊天 工具 使用 
这种 非 人为 依赖 的 智能 工具 与 人类 进行 
交流 我 强烈 的 感受到 直到 我们 的 机器 学会 
了解 行为 和 情绪 数据 科学家 和 工程师 的 工作 
才 完成 了 一半 与 深度 学习 ML 学科 领域 
融合 的 NLP 将对 这种 计算机 语言 的 使用 起到 
关键 作用 什么 是 NLP 这 是 一种 人工智能 方法 
给定 机器 一些 人类 语言 从而 使得 它们 能够 与 
人类 进行 沟通 交流 它 涉及 使用 NLP 技术 对 
书面 语言 进行 智能 分析 以 获取 对 一组 文本 
数据 的 见解 如 1/m ./i 情绪/n 分析/vn 2/m ./i 
信息/n 提取/v 和/c 检索/vn 3/m ./i 智能/n 搜索/v 等/u 它/r 
是/v 人工/n 智能/n 和/c 计算/v 语言学/n 的/uj 交汇点/n 能够 处理 
机器 和 人类 自然语言 之间 的 交互 即 计算机 需要 
对 其 进行 分析 理解 改变 或 生成 自然语言 NLP 
帮助 计算机 机器 以 各种 形式 使用 自然 人类 语言 
进行 交流 包括 但 不限 于 语音 印刷 写作 和 
签名 NLP 机器学习 和 深度 学习 它们 是 如何 连接 
的 NLP 与 机器 学习 和 深度 学习 密切相关 所有 
这些 都是/nr 人工智能 领域 的 分支 如下 图 所示 它 
是 一个 致力于 使 机器 智能化 的 计算机 科学 领域 
深度 学习 是 一种 流行 的 机器 学习 技术 之一 
如 回归 K means 等 机器 学习 的 类型 很多 
像 无 监督 机器学习 这样 的 经常 用于 NLP 技术 
中 如 LDA 潜在 狄利克雷 分布 一种 主题 模型 算法 
为了 能够 执行 任何 一个 NLP 我们 需要 深入 理解 
人类 使 如何 处理 语言 的 情感 和 分析 方面 
还有 各种各样 像 社交 媒体 这样 的 语言 数据源 人们 
直接 或 间接 地 分享 他们 感受到 的 内容 而这 
必须 通过 使用 NLP 的 机器 进行 智能 分析 NLP 
机器 需要 建立 一个 人类 推理 系统 借助 ML 技术 
它们 可以 自动 执行 NLP 过程 并 对其 进行 扩展 
简而言之 深度 学习 与 自然 语言 处理 是 相互 联系 
相互 依存 的 以 构建 一个 能够 像 人类 一样 
思考 说话 和 行动 的 智能 计算机 Meltwater Group 的 
NLP 专家 John Rehling 在 自然语言 处理 是 如何 帮助 
揭示 社交 媒体 情绪 一 文中 说 通过 分析 语言 
的 含义 NLP 系统 扮演 着 非常 重要 的 角色 
如 纠正 语法 将 语音 转换 为 文本 以及 在 
多语言 之间 自动 翻译 NLP 如何 工作 理解 NLP 的 
工作 原理 是 非常 重要 的 因为 这样 的话 我们 
就 可以 将 NLP 作为 一个 整体 来 理解 NLP 
一般 有 两个 主要 组成部分 1 . NLU 自然语言 理解 
2 . NLG 自然语言 生成 让 我们 深入 理解 NLU 
自然语言 理解 它 涉及 的 是 一种 方法论 试图 了解 
如何 对 馈 送给 计算机 的 自然 语言 赋予 一定 
的 相关 意义 在 开始 时 计算机 获得 自然 语言 
的 输入 自然语言 可以 是 任何 语言 它们 通过 使用 
和 重复 在 人类 中 自然 进化 而 不是 有 
意识 的 计划 或 预谋 自然语言 可以 采用 不同 的 
形式 例如 语音 或 签名 计算机 之后 将 它们 转换 
成 人工 语言 如 语音 识别 和/或/nr 语音 转换 文本 
在 这里 我们 把 数据 转换 成 一个 文本 形式 
NLU 过程 来 理解 其中 的 含义 HMM 隐 马尔可夫 
模型 NLU 示例 来源 wikipedia 它 是 一种 统计 语音识别 
模型 它 可以 在 预先 构建 的 数学 技术 的 
帮助 下 将 你 的 语音 转换成 文本 并 试图 
推断出 你 所说 的 语言 它 试图 理解 你 所说 
的 通过 将 语音 数据 分解成 一 小段 特定 的 
时间段 大多数 情况 下 时间 是 20 20 ms 这些 
数据集 将 进一步 与 预 馈 语音 进行 比较 从而 
进一步 解读 你 在 每个 语音 单位 中 所说 的 
内容 这里 的 目的 是 找到 音素 一个 最小 的 
语音 单位 然后 机器 对 一系列 这样 的 音素 进行 
观察 并 统计 了 最 可能 说出 的 单词 和 
句子 不仅如此 NLU 会 深刻 理解 每个 单词 试图 理解 
它 是 一个 名词 还是 动词 什么 是 时态 过去 
或 未来 等 这个 过程 被 定义 为 POS 词性 
标注 部分 Part Of Speech Tagging NLP 具有 内置 的 
词典 和 一套 与 语法 预 编码 相关 的 协议 
这些 协议 被 预 编码 到 它们 的 系统 中 
并在 处理 自然语言 数据集 时 使用 它 从而 在 NLP 
系统 处理 人类 语音 时 编译 所说 的 内容 NLP 
系统 也 有一个 词典 词汇表 和 一套 编码 到 系统 
中 的 语法 规则 现代 NLP 算法 使用 统计 机器 
学习 将 这些 规则 应用 于 自然 语言 并 推断 
所说 话语 背后 最 可能 的 含义 在 考虑 诸如 
具有 多个 含义 的 词语 多义词 或 具有 相似 含义 
的 词语 同义词 时 存在 一些 挑战 但 软件 开发 
者 在 他们 的 NLU 系统 中 建立 了 自己 
的 规则 可以 通过 适当 的 训练 和 学习 来 
处理 这 类 问题 自然语言 生成 与 第一 阶段 NLU 
做 了 大量 的 努力 以 理解 人类 的 话语 
相比 NLG 可以 很 容易 的 进行 翻译 工作 即将 
计算机 的 人工 语言 翻译 为 有 意义 的 文本 
并 可以 通过 文字 转 语音 tex to speech 技术 
将 其 转化 为 可听 语音 文本 转 语音 tex 
to speech 技术 通过 韵律 模型 prosody model 来 分析 
文本 从而 确定 语言 的 断句 长短 和 音调 然后 
利用 语音 数据库 将 记录 的 所有 音素 汇集 在 
一起 形成 一个 连贯 的 语音 串 简而言之 NLP 采用 
NLU 和 NLG 来 处理 人类 自然语言 尤其 是 处理 
语音识别 领域 的 人类 自然语言 并 试图 将 传递 字符串 
或 可听 语言 作为 输出 来 理解 编译 并 推断 
所说 的 内容 NLP 在 现代 语境 中 的 应用 
在 这个 处于 数字 革命 的 电脑 时代 中 大部分 
任务 需要 由 人类 利用 链接 物联网 的 机器 来 
完成 NLP 在为 媒体 出版 广告 医疗 银行 和 保险 
等 行业 领域 建立 强大 的 软件工具 方面 发挥 了 
重要 作用 从而 帮助 他们 高效 快捷 地 运作 NLP 
的 一些 现代 用法 1 . 聊天 机器人 这 是 
一个 被 称为 机器人 的 成熟 软件 它 可以 处理 
任何 场景 的 人物 对话 api . ai 微软 语音 
理解 智能 服务 LUIS 等 一些 热门 的 NLP 和 
机器 学习 平台 可 用于 研发 你 的 商业 聊天 
机器人 2 . 垃圾邮件 过滤 来源 yhat 你们 中的 大多数 
人 一定 对 垃圾 邮件 并不 陌生 Google 使用 基于 
NLP 的 技术 来 保障 你 的 收件箱 清洁 无 
垃圾 邮件 贝叶斯 垃圾邮件 过滤 Bayesian spam filtering 是 一种 
备受 瞩目 的 技术 它 是 一种 统计技术 基于 此 
电子邮件 中 词语 的 审核 通过率 根据 其 在 垃圾 
和非/nr 垃圾邮件 语料库 中的 典型 事例 来 确定 3 . 
机器翻译 NLP 被 越来越 多 的 应用于 机器 翻译 程序 
当中 这 使得 一种 语言 被 自动 翻译成 另一种 语言 
谷歌 是 一个 将 你 的 文本 翻译 为 所需 
语言 的 先驱者 机器 翻译 技术 所 面临 的 挑战 
不 在于 翻译 单词 而在于 保留 句子 的 含义 这 
是 一个 复杂 的 技术 问题 也是 NLP 的 核心 
4 . 命名 实体 提取 Named entity extraction 它/r 用于/v 
从/p 给定/v 的/uj 项目/n 集合/v 中/f 分离出/i 具有/v 相似/v 性质/n 
和/c 属性/n 的/uj 项目/n 例如 名字 姓氏 年龄 地理位置 地址 
电话号码 电子 邮件 地址 和 公司 名称 等等 命名 实体 
提取 亦称 命名 实体 识别 使 挖掘 数据 变得 更加 
容易 5 . 自动 汇总 自然语言 处理 可 用于 从 
大段 文本 中 提取 可读 摘要 例如 我们 可以 自动 
总结 出 一份 长篇 学术 文章 的 简短 摘要 接下来 
我们 将 深入 介绍 一些 NLP 的 技术 细节 当 
自然界 与 人工 相逢 的 时候 机器 就像 是 一个 
真正 具有 生命力 的 人类 一样 进入 了 生活 中 
NLP 技术 术语 NLP 术语 • 语音 体系 关于 系统性 
地 组织 语音 的 研究 • 形态学 这 是 一个 
从 基本 意义 单位 中 进行 单词 构建 的 研究 
• 语素 语言 中 意义 的 基本 单位 • 语法 
它 是 指 单词 经过 组合 排列 构成 句子 它 
还 涉及 在 句子 和 短语 中 确定 单词 结构 
的 作用 • 语义 它 涉及 的 是 单词 的 
含义 以及/c 该/r 如何/r 将/d 单词/n 组合成/l 有/v 意义/n 的/uj 
短语/nz 和/c 句子/n • 语用学 它 涉及 的 是 在 
不同 情况 下 使用 和 理解 句子 以及对 句子 的 
解释 是 如何 受到影响 的 • 话语 它 指 的 
是 前面 的 句子 如何 影响 对于 下 一句 的 
解释 的 • 常识 性知识 它 涉及 的 是 对于 
世界 的 一般性 认识 自然语言 处理 库 对于 开发 者 
而言 NLP 库 有/v 许多/m 通用/v 的/uj 第三/m 方/n 开源/n 
库/n 开发 人员 可以 使用 它们 来 构建 基于 NLP 
的 Projects Viz . • 自然语言 工具包 NLTK • Apache 
OpenNLP • 斯坦福大学 NLP 套件 • Gate NLP 库 自然语言 
工具包 NLTK 是 最 通用 的 自然 语言 处理 NLP 
库 它 是 用 Python 编写 的 背后 有 一个 
很大 的 社区 NLP 实施 所 涉及 的 步骤 来源 
mediterra soft 它 涵盖 了 5个 主要 步骤 • 词 
法分析 它 对 给定 单词 的 结构 进行 识别 和 
分析 其中 整个 文本 数据块 在 词法 分析 中 被 
分解 成 段落 句子 和 词汇 • 解析 句法分析 它 
涉及 以 一种 显示 单词 之间 的 关系 的 方式 
对 分析 句子 中 的 单词 进行 语法 和 单词 
排列 分析 在 这个 阶段 任何 不 符合 语法 正确 
的 句子 都被 拒绝 例如 building lives in sita 将 
不会 被 语法 分析器 所 接受 • 语义分析 对 给定 
的 文本 进行 分析 以 从中 提取 意义 它 通过 
对 任务 域中 的 语法 结构 和 目标 进行 分析 
来 完成 语义 分析器 拒绝 不 相关 的 句子 如 
hot banana • 话语 整合 正如 我们 所知 每个 句子 
都与 前 一句话 相互 联系 基于 倒数 第二 句 的 
意义 而言 任何/r 句子/n 都/d 变得/v 有/v 意义/n 同样 它/r 
也/d 使得/v 后/f 一句话/i 变得/v 有/v 意义/n • 语用分析 在此期间 
常识 性知识 被 重新 定义 了 解释 了 它们 的 
真实 意义 到底 是 什么 它 涉及 到 那些 需要 
常识性 知识 的 语言 方面 用 图片 来 解读 NLP 
点击 图片 放大 NLP 应用程序 1 . 光学 字符识别 2 
. 语音识别 3 . 机器翻译 4 . 自然语言 生成 5 
. 情绪 分析 6 . 语义搜索 7 . 自然语言 编程 
8 . 情感 计算 9 . 开发 聊天 机器人 未来 
智能 实验室 致力于 研究 互联网 与 人工智能 未来 发展 趋势 
观察 评估 人工智能 发展水平 由 互联网 进化论 作者 计算机 博士 
刘锋 与 中国科学院 虚拟 经济 与 数据 科学 研究 中心 
石勇 刘颖 教授 创建 未来 智能 实验室 的 主要 工作 
包括 建立 AI 智能系统 智商 评测 体系 开展 世界 人工智能 
智商 评测 开展 互联网 城市 云脑/nr 研究计划 构建 互联网 城市 
云脑/nr 技术/n 和/c 企业/n 图谱/n 为 提升 企业 行业 与 
城市 的 智能 水平 服务 如果 您 对 实验室 的 
研究 感兴趣 欢迎 支持 和 加入 我们 扫描 以下 二维码 
或 点击 本文 左下角 阅读 原文 目录 文章 目录 目录 
前言 自然语言 处理 概论 七 自然语言 处理 概论 八 数学 
基础 和 语言学 基础 1 数学 基础 和 语言学 基础 
2 数学 基础 和 语言学 基础 3 数学 基础 和 
语言学 基础 4 数学 基础 和 语言学 基础 5 前言 
硕士 生涯 结束 开始 专心 做 一件 自己 觉得 有用 
的 工具 先 做 工程 后搞/nr 理论 自然语言 处理 是 
一个 非常 难 的 问题 同时 是 人工智能 皇冠 上 
的 明珠 接下来 会 记录 一 系列 自然语言 处理 的 
笔记 来自 于 哈工大 老师 关毅 自然语言 处理 概论 七 
人工智能 经典 实验 图灵 实验 想象 人和 一块 机器 隔着 
屏幕 讲话 如果 人 无法 判断 对面 是 机器 还是 
人 那就 说明 这个 机器 通过了 图灵测试 强调 一点 人工智能 
的 发展 还是 要 依赖 于 对人 大脑 机理 的 
了解 做出 真正 的 人工智能 涉及 的 学科 计算 语言学 
应用 语言学 计算机科学 可计算 的 方法 来自于 数学 的 理论 
基础 和人的/nr 心理学 模型 将 人 理解 自然 语言 的 
步骤 反 着 来 一遍 就 可以 做出 真正 的 
自然 语言 理解 现在 想 要在 理论 上 做出 大 
的 创新 需要 的 是 交叉 学科 需要 的 是 
共同 创新 计算 语言学 侧重于 语言 处理 的 基础 自然语言 
理解 侧重于 智能化 人机接口 自然语言 处理 概论 八 汉语 的 
特性 大字符集 的 意 音 文字 同义 多 词态 无变化 
语法 研究 不 规范 汉语 语言 形式化 和 量化 工作 
滞后 力量 较 分散 分词 评测 系统 很难 基础理论 讲解 
1 概率 统计 2 统计 机器学习 3 人工智能 4 认知科学 
理论 人工智能 理论 1 组合 优化 方法 2 逻辑 方法 
可 研究 的 内容 1 词 法分析 2 句法分析 3 
上下文 无关 分析 4 语义分析 5 概念 网络 6 机器翻译 
数学 基础 与 语言学 基础 一 从 小规模 语料库 统计 
出 语料 信息 然后 在 大规模 语料库 里面 使用 个人 
的 感受 工程 开发 经验 以后 必须 做 一个 精密 
的 系统 收集 分析情况 构造 推断 模型 概率论 是 我们 
的 研究 基础 它 研究 的 是 随机 现象 的 
规律 词汇 的 分布 也 符合 幂律 数学 基础 和 
语言学 基础 1 1948年 熵 出现 H P = ∑ 
x 属于 Ω − p x ∗ logp x H 
P = \ sum _ { x 属于 \ Omega 
} p x * log _ p x H P 
= x 属于 Ω ∑ − p x ∗ logp 
x 不确定性 的 信息 熵 最大 完全 确定 的 信息 
信息熵 最小 冯志伟 汉语 信息量 最大 条件概率 复习 P A 
| B B 为真 时A/nr 发生 的 概率 数学 基础 
和 语言学 基础 2 贝叶 斯定理 P A ∣ B 
= P A B P B = P B ∣ 
A ∗ P A P B = argmaxAP B ∣ 
A ∗ P A P A | B = \ 
frac { P A B } { P B } 
= \ frac { P B | A * P 
A } { P B } = argmax _ AP 
B | A * P A P A ∣ B 
= P B P A B = P B P 
B ∣ A ∗ P A = argmaxA P B 
∣ A ∗ P A 应用 音 字 替换 贝叶 
斯定理 将 一个 大 问题 分解成 两个 小 的 问题 
的 乘积 随机变量 数学期望 与 方差 无 参数 分布 以及 
有 参数 分布 极大 似 然 估计 和 贝叶斯 统计 
语言学 基础 1 汉语 的 分类 可以 分为 实 虚 
叹 属于 黏着语 数学 基础 和 语言学 基础 3 语法 
分类 句法分析 特点 汉语 句法分析 的 特殊性 一个词 可以 在 
句中 担任 多种 成分 切勿 形态 变化 语言 知识库 一个 
关键 核心 部分 调整 知识库 现代 汉语语法 信息 词典 语用 
层 提示 到 语义层 数学 基础 和 语言学 基础 4 
贝叶斯 公式 和 一个 核心 oncology 是 核心 hownet 搭建 
了 一个 意 元 为基础 的 框架 定义 意 元 
很难 派生 整个 系统 数学 基础 和 语言学 基础 5 
搜索 系统 人性化 提问 大 规模 文本 抽取 答案 语义 
相似 度 的 计算 基于 库 或者 基于 统计 的 
方法 机器 可读 词典 二进制 文件 构造 你 的 词典 
保护 知识产权 例子 是 我 学习 的 教程 的 一个 
例子 收集 了 一些 客户 对 于 饭店 的 评价 
目标 是 将 他们 进行 分类 分成 好评 和 差评 
数据 的 前 5项 这里 用到 了 一个 之前 没用到 
的 包 NLTK 对 我们 的 文本 数据 进行 必要 
的 处理 转化 使其 变成 能够 进入 我们 模型 的 
数据 本文 针对 一个 例子 进行 介绍 详细 可以 查看 
官方 文档 对于 这个 包的/nr 说明 NLTK 的 介绍 NLTK 
是 一个 高效 的 Python 构建 的 平台 用来 处理 
人类 自然语言 数据 它 提供 了 易于 使用 的 接口 
通过 这些 接口 可以 访问 超过 50个 语料库 和 词汇 
资源 如 WordNet 还有 一套 用于 分类 标记 化 词干 
标记 解析 和 语义 推理 的 文本处理 库 实现 的 
大致 步骤 对 文本 进行 处理 去除 标点符号 单词 提取 
大小写 转化 提取 出来 的 单词 需要 进一步 的 清理 
去除 虚词 单词 词根 化 词根 化 处理 是 为了 
避免 相同 单词 的 不同 形式 影响 到 我们 的 
模型 因 为最 终模型 对于 结果 的 处理 是 通过 
统计 词 出现 的 频率 所属 的 类别 并 依此 
来 对 新 样本 进行 分类 的 对 提取 结果 
进行 稀疏 矩阵 化 操作 转化 为 可进入 模型 的 
数据 代码 import pandas as pd import numpy as np 
import re # 正则 用到 的 库 import nltk # 
文本处理 用 from nltk . corpus import stopwords # 处理 
文本 中的 虚词 nltk . download stopwords # 将 包含 
的 虚词 的 加载 下来 用于 后面 的 比对 去除 
虚词 from nltk . stem . porter import PorterStemmer dataset 
= pd . read _ csv Restaurant _ Reviews . 
tsv delimiter = \ t # 加载 数据 pocket = 
# 用于 存储 处理 后的/nr 文本 for i in range 
dataset . shape 0 review = dataset Review i review 
= re . sub ^ a zA Z review # 
进行 正则化 处理 留下 字母 不 区分 大小写 其余 替换 
为 空格 防止 处理 后 单词 连接 在 一起 破坏 
了 词意 review = review . lower # 将 大写 
的 字母 变成 小写 review = review . split ps 
= PorterStemmer # 词根 化 review = ps . stem 
word for word in review if not word in set 
stopwords . words english 这里 的 大致 作用 是 遍历 
review 中 提取 的 单词 筛选出 不在 虚词 包 中的 
单词 并 进行 词根 化 review = . join review 
# 对 筛选 结果 重新 组合 成 句子 pocket . 
append review 关于 词根 化 维基百科 from sklearn . feature 
_ extraction . text import CountVectorizer tool = CountVectorizer max 
_ features = 1500 # 将 文本 序列 进行 稀疏 
矩阵 转化 x = tool . fit _ transform pocket 
. toarray # shape 1000 1565 这里 是 指 我们 
的 数据 有 1000个 样本 1565个 单词 CountVectorizer max _ 
features = 1500 这里 限制 了 特征 最大 为 1500 
所以 之后 我们 x . shape 会 是 1000 1500 
也 可以 根据 需求 进行 调整 y = dataset . 
iloc 1 from sklearn . model _ selection import train 
_ test _ split x _ train x _ test 
y _ train y _ test = train _ test 
_ split x y test _ size = 0.2 random 
_ state = 0 from sklearn . naive _ bayes 
import GaussianNB classifer = GaussianNB classifer = classifer . fit 
x _ train y _ train y _ pre = 
classifer . predict x _ test # 利用 贝叶斯 进行 
拟合 训练 集 并 做出 测试 集 的 预测 from 
sklearn . metrics import confusion _ matrix score = confusion 
_ matrix y _ test y _ pre print score 
point = score 0 0 + score 1 1 / 
x _ test . shape 0 print 分类器 在 测试 
集上 的 表现 + str point # 利用 混淆 矩阵 
进行 预测 结果 准确 的 检验 输出 55 42 12 
91 分类器 在 测试 集上 的 表现 0.73 使用 随机 
森林 试试 from sklearn . ensemble import R a n 
d o m F o r e s t C 
l a s s i f i e r classifer 
= R a n d o m F o r 
e s t C l a s s i f 
i e r n _ estimators = 10000 random _ 
state = 0 classifer = classifer . fit x _ 
train y _ train y _ pre = classifer . 
predict x _ test score = confusion _ matrix y 
_ test y _ pre print score point = score 
0 0 + score 1 1 / x _ test 
. shape 0 print 分类器 在 测试 集上 的 表现 
+ str point 输出 74 23 38 65 分类器 在 
测试 集上 的 表现 0.695 # 准确度 不如 贝叶斯 至此 
例子 结束 这个 只是 入门 级别 更深 层次 的 用法 
会对 文本 进行 更加 细化 的 处理 利用 更好 的 
模型 如 有错误 还请 指出 万分 感谢 我们 经常 听到 
Python 与 人工智能 这 两个 词 也 很容易 混淆 这 
两个 词 那么/r Python/w 和/c 人工智能/n 有/v 什么/r 关系/n 呢/y 
首先 我们 先 来说 说 人工智能 人工智能 是 计算机 科学 
的 一个 分支 它 企图 了解 智能 的 实质 并 
生产 出 一种 新的 能以 人类 智能 相似 的 方式 
做出 反应 的 智能 机器 该 领域 的 研究 包括 
机器人 语言识别 图像识别 自然语言 处理 和 专家 系统 等 简单 
来说 人工智能 是 一种 未来性 的 技术 再来 说说 PythonPython 
是 一门 计算机 程序语言 目前 人工智能 科学 领域 应用 广泛 
应用 广泛 就 表明 各种 库 各种/r 相/v 关联/ns 的/uj 
框架/n 都/d 是以/i Python/w 作为/v 主要/b 语言/n 开发/v 出来/v 的/uj 
谷歌 的 TensorFlow 大部分 代码 都是 Python 其他 语言 一般 
只有 几 千行 如果 讲 开发 效率 用 Python 谁 
会用 Java 这种 高不成低不就 的 语言 搞 人工智能 呢 Python 
虽然 是 脚本语言 但是 因为 容易 学 迅速 成为 科学家 
的 工具 从而 积累 了 大量 的 工具 库 架构 
人工智能 涉及 大量 的 数据 计算 用 Python 是 很 
自然 的 简单 高效 Python 有 非常 多 优秀 的 
深度 学习 库 可用 现在 大部分 深度 学习 框架 都 
支持 Python 不用 Python 用 谁 视频 列表 43 句法分析 
技术 一 44 句法分析 技术 二 45 句法分析 技术 三 
46 句法分析 技术 四 47 句法分析 技术 五 43 句法分析 
技术 一 第七章 句法分析 技术 什么 是 句法 分析判断 输入 
的 词 序列 能否 构成 一个 合乎 语法 的 句子 
确定 合乎 语法 句子 的 句法结构 运用 句法 规则 和 
其他 知识 将 输入 句子 中词 之间 的 线性 次序 
变成 一个 非 线性 的 数据 结构 例如 短语 结 
构树 或 有向 无 环 图 为什么 要 进行 句法分析 
例 一 音 字 转换 例 一只 小花猫 例 二 
机器翻译 示例 Jan hit the girl with long hairJan hit 
the girl with a hammer 例 三 信息检索 例 哪个 
球队 获得 了 亚洲 杯 冠军 日本队 击败 中国 队 
获得 亚洲 杯 冠军 例 四 语法 歧义 一个 句子 
对应 着 几种 句法分析 结果 咬死 了 猎人 的 狗 
那只 狼 咬死 了 猎人 的 狗 那只 咬死 了 
猎人 的 狗 失踪 了 汉语 句法分析 的 独特性 根据 
朱德熙 语法 答问 语法 讲义 汉语 没有 形态 语序 灵活 
词类 和 句法 成分 不 存在 一一对应 的 关系 汉语 
句子 的 构造 原则 与 词组 的 构造 原则 基本上 
是 一致 的 汉语语法 形式化 工作 滞后 句法分析 系统 一个 
句法分析 系统 通常 由 两部分 组成 形式 语法 体系 匹配 
模式 基于 模板 的 方法 短语 结构 语法 句法 规则 
特征 制约 语义解释 扩充 转移 网络 树 邻接 语法 TAG 
44 句法分析 技术 二 基于 合一 运算 的 语法 广义 
短语 结构 语法 词汇 功能 语法 功能 合一 语法 基于 
中心词 驱动 的 短语 结构 语法 HPSG 基于 词 的 
语法 链 语法 依存 语法 配价 语法 分析 控制 机制 
模式匹配 技术 基于 短语 结构 语法分析 算法 厄尔利 Earley 分析 
算法 富 田胜 Tomida 分析 算法 线图 Chart 分析 算法 
确 定性分析 算法 等等 基于 扩充 转移 网络 的 分析 
算法 链分析 算法 Ｇ = N ∑ P S Ｇ 
= N \ sum P S Ｇ = N ∑ 
P S 是 一个 文法 α → β ∈ P0 
型文法 对 α → β 不作 任何 限制 1 型文法 
| α | ≤ | β | 2 型文法 上下文 
无关 文法 α ∈ N3 型文法 正则文法 A → aB 
或 A → a G 是 右 线性 文法 L 
G 是 3型 语言 A → Ba 或 A → 
ａ G 是 左 线性 文法 L G 是/v 3型/mq 
语言/n 在/p 自然/d 语言/n 处理/v 中/f 研究/vn 和/c 应用/v 较多/i 
的/uj 是/v 2/m 型文法/n 和3/nr 型文法/n 推导/v 一个/m 字串/n 的/uj 
推导/v 是/v 一系列/m 文法/n 规则/n 的/uj 应用/v →/i NP/w VP 
→ John V NP → John V NP PP → 
John ate fish P NP → John ate fish with 
bone 这一 推导 的 过程 可以 用 分析树 来 表示 
根据 某 上文 下 无关 文法 从 起始 非 终结符 
可能 推 导出 的 所有 字串 的 集合 称为 由该 
CFG 定义 的 语言 CFG 的 形式化 定义 一个 CFG 
是 一个 四元组 Ｇ = & lt N ∑ P 
& gt Ｇ = & lt N \ sum P 
& gt Ｇ = N ∑ P S N 是非 
终结符 的 集合 ∑ \ sum ∑ 是 终结符 的 
集合 P 是 产生 式 的 集合 其中 每个 产生 
式 形如 A → α A \ rightarrow \ alphaA 
→ α A 是非 终结符 α \ alpha α 是由 
终结符 与非 终结符 构成 的 字串 是 一个 起始 非 
终结符 上下文 无关 文法 示例 context free grammar 语言 的 
合法性 概率 上下文 无关 文法 Probabilistic Stochastic Context Free Grammar 
随机 上下文 无关 语法 可以 直接 统计 语言学 中词 与 
词 词 与 词组 以及 词组 与 词组 的 规约 
信息 并且 可以 由 语法 规则 生成 给定 句子 的 
概率 定义 定义 一个 随机 上下文 无关 语法 PCFG 由 
以下 5 部分 组成 1 一个 非 终结 符号 集 
N 2 一个 终结 符号 集 ∑ 3 一个 开始 
非 终结符 ∈ N 4 一个 产生 式 集 R 
5 对于 任意 产生 式 r ∈ R 其 概率 
为 P ® 产生 式 具有 形式 X → Y 
其中 X ∈ N Y ∈ N ∪ ∑ * 
∑ λ P X → λ = 1 { \ 
sum _ { } ^ { \ lambda } } 
P X \ rightarrow \ lambda = 1 ∑ λ 
P X → λ = 1PCFG 的 三个 基本 假设 
CFG 的 简单 概率 拓广 ∑ λ P X → 
λ = 1 { \ sum _ { } ^ 
{ \ lambda } } P X \ rightarrow \ 
lambda = 1 ∑ λ P X → λ = 
1 基本 假设 位置 无关 Place invariance 上下文 无关 Context 
free 祖先 无关 Ancestor free 分析树 的 概率 等于 所有 
施用 规则 概率 之 积 P tree1 = 1/22 / 
32/3 = 2/9 P tree2 = 1/21 / 31/3 = 
1/18 P tree3 = 1 / 21/2 = 1/4 P 
tree4 = 1 / 21/2 = 1 / 4PCFG 的 
三个 基本问题 1 一个 语句 W = w1w2 . wnW 
= w _ { 1 } w _ { 2 
} . w _ { n } W = w1 
w2 . wn 的 P W | G 也 就是 
产生 语句 W 的 概率 P W ∣ G P 
W | G P W ∣ G 2 在 语句 
W 的 句法结构 有 歧义 的 情况 下 如何 快速 
选择 最佳 的 语法分析 parse argmaxtreeP tree ∣ W G 
\ underset { tree } { argmax } P tree 
| W G treeargmax P tree ∣ W G 3 
如何 从 语料库 中 训练 G 的 概率 参数 使得 
P W | G 最大 argmaxGP tree ∣ W G 
\ underset { G } { argmax } P tree 
| W G Gargmax P tree ∣ W G 问题 
1 & 2 解决 思路 向内 Inside 算法 非 终结符 
A 的 内部 概率 Inside probability 定义 为 根据 文法 
G 从A/nr 推出 词串 wi . . . wjw _ 
{ i } . . . w _ { j 
} wi . . . wj 的 概率 记为 α 
i j A \ alpha _ { i j } 
A α i j A i ≤ ji \ leq 
ji ≤ j α i j A \ alpha _ 
{ i j } A α i j A 称为 
向内 变量 45 句法分析 技术 三 向内 概率 公式 向内 
算法 计算 示例 → NP VP 1.0 NP → NP 
PP 0 . 4PP → P NP 1.0 NP → 
John 0 . 1VP → V NP 0.7 NP → 
bone 0 . 18VP → VP PP 0.3 NP → 
star 0.04 P → with 1.0 NP → fish 0.18 
V → ate 1.0 NP → telescope 0.1 问题 2Viterbi 
算法 输入 G = S N ∑ R P 字符串 
W = w1w2 . wnW = w _ { 1 
} w _ { 2 } . w _ { 
n } W = w1 w2 . wn 输出 t 
* W 在 G 下 最 可能 的 分析树 Viterbi 
算法 示例 自底向上 问题 3 参数 训练 问题 有/v 指导/n 
学习/v 方法/n 从树库/nr 直接/ad 统计/v Treebank Grammar 最大 似 然 
估计 依赖于 艰巨 的 工程 树 库 建设 PCFG 的 
优缺点 优点 可以 对 句法分析 的 歧义 结果 进行 概率 
排序 提高 文法 的 容错 能力 robustness 缺点 没有 考虑 
词 对 结构 分析 的 影响 没有 考虑 上下文 对 
结构 分析 的 影响 许多 当前 的 获得 较高 精度 
的 句法分析 系统 以 PCFG 为基础 46 句法分析 技术 四 
浅层 句法分析 技术 从 完全 句法分析 complete parsing 到 浅层 
句法分析 shallow parsing 真实 语料 的 复杂性 语言 知识 的 
不足 提高 分析 的 效率 应用 目标 驱动 浅层 分析 
的 其他 名称 部分 分析 partial parsing 组块 分析 chunking 
基于 HMM 的 浅层 分析 技术 识别 目标 非 递归 
的 NP 组块 分析 在 线性 序列 中 插入 括号 
来 标示 组块 边界 The / DT prosecutor / NN 
said / VB in / IN closing / NN that 
/ CS 级联 式 有限 状态 句法分析 1 从 左向右 
扫描 输入 字符串 按照 Li 层级 上 的 正则表达式 模式 
进行 归约 得到 新的 模式 序列 对于 输入 串 中 
无法 归约 的 符号 直接 输出 2 i = i 
+ 1 在 新的 Li 层级 上 用 正则表达式 模式 
进行 归约 3 不断 进行 上述 步骤 直到 无法 归约 
为止 4 如果 归约 过程 中 有 多种 选择 以 
覆盖 范围 最大 的 归约 子串 为 输入 结果 47 
句法分析 技术 五 小结 以 PCFG 为 重点 介绍 了 
近年来 句法分析 技术 的 基本 原理 与 方法 句法分析 是 
当前 语言 处理 技术 的 瓶颈 问题 之一 句法分析 是 
语义分析 更深 层次 的 语言 理解 的 必由之路 句法 是 
形式 语义/n 是/v 内容/n 句法/n 的/uj 强制性/n 和/c 语义/n 的/uj 
决定性/n 句法/n 系统/n 和/c 语义/n 系统/n 是/v 两个/m 不同/a 的/uj 
系统/n 它们 各自 独立 而 又 相互 依存 彼此 的 
对应 关系 十分 复杂 致谢 关毅 老师 现为 哈工大 计算机 
学院 语言 技术 中心 教授 博士生 导师 通过 认真 学习 
了 自然语言 处理 哈工大 关毅 64集 视频 1 来自 互联网 
的 课程 受益 良多 在此 感谢 关毅 老师 的 辛勤 
工作 为 进一步 深入 理解 课程内容 对 部分 内容 进行 
了 延伸 学习 2 3 456 在此 分享 期待 对 
大家 有所 帮助 欢迎 加 我 微信 验证 NLP 一起 
学习 讨论 不足之处 欢迎 指正 参考文献 自然语言 处理 哈工大 关毅 
64集 视频 来自 互联网 ↩ ︎ 王晓龙 关毅 计算机 自然语言 
处理 清华大学出版社 2005年 ↩ ︎ 哈工大 语言 技术 平台 云 
官网 http / / ltp . ai / ↩ ︎ 
Steven Bird Natural Language Processing with Python 2015 ↩ ︎ 
Claude E . Shannon . Prediction and Entropy of Printed 
English Bell System Technical Journal 30 50 64 . 195 
↩ ︎ An Empirical Study of Smoothing Techniques for Language 
Modeling Stanley F . Chen ↩ ︎ Python 自然语言 处理 
NLP 工具 汇总 NLTK 简介 NLTK 在 使用 Python 处理 
自然 语言 的 工具 中 处于 领先 的 地位 它 
提供 了 WordNet 这种 方便 处理 词汇 资源 的 接口 
以及 分类 分词 词干 提取 标注 语法分析 语义 推理 等 
类库 网站 Natural Language Toolkit 安装 安装 NLTK root @ 
master ~ # pip install nltk Collecting nltk Downloading nltk 
3 . 2.1 . tar . gz 1 . 1MB 
100% | █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ | 1 . 1MB 664kB / 
s Installing collected packages nltk Running setup . py install 
for nltk . . . done Successfully installed nltk 3 
. 2.1 注意事项 安装 完 以后 还要 下载 nltk 语料库 
才 可以 使用 下载 的 是 压缩文件 需要 解 压到 
nltk _ data 下面 目录 结构 如下 zang @ ZANG 
PC D \ nltk _ data ls al total 44 
drwxrwx + 1 Administrators None 0 Oct 25 2015 . 
drwxrwx + 1 SYSTEM SYSTEM 0 May 30 10 55 
. . drwxrwx + 1 Administrators None 0 Oct 25 
2015 chunkers drwxrwx + 1 Administrators None 0 Oct 25 
2015 corpora drwxrwx + 1 Administrators None 0 Oct 25 
2015 grammers drwxrwx + 1 Administrators None 0 Oct 25 
2015 help drwxrwx + 1 Administrators None 0 Oct 25 
2015 stemmers drwxrwx + 1 Administrators None 0 Oct 25 
2015 taggers drwxrwx + 1 Administrators None 0 Oct 25 
2015 t o k e n i z e r 
s P a t t e r n 简介 Pattern 
是 基于 web 的 Python 挖掘 模块 包含 如下 工具 
* 数据挖掘 Web 服务 接口 Google Twitter Wikipedia 网络爬虫 HTML 
DOM 解析 * 自然语言 处理 POS 词性 标注 n gram 
搜索 情感 分析 词 云 * 机器学习 向量空间 模型 VSM 
聚 类 分类 KNN SVM Perceptron * 网络分析 图 中心 
和 可视化 网站 GitHub 主页 安装 root @ master ~ 
# pip install pattern Collecting pattern Downloading pattern 2.6 . 
zip 24 . 6MB 100% | █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ | 24 
. 6MB 43kB / s Installing collected packages pattern Running 
setup . py install for pattern . . . done 
Successfully installed pattern 2.6 root @ master ~ # TextBlob 
简介 TextBlob 是 基于 NLTK 和 pattern 的 工具 有 
两者 的 特性 如下 名词 短语 提前 POS 标注 情感 
分析 分类 Naive Bayes Decision Tree 谷歌/nr 翻译/v 分词/n 和/c 
分句/m 词频/n 和/c 短语/nz 频率/n 统计/v 句法/n 解析/vn n/w grams/w 
模型/n 词/n 型/k 转换/v 和/c 词干/n 提取/v 拼写/v 校正/n 通过/p 
词/n 云/ns 整合/v 添加/v 新的/i 语言/n 和/c 模型/n 网站/n TextBlob 
Simplified Text Processing 安装 root @ master ~ # pip 
install U textblob Collecting textblob Downloading textblob 0 . 11.1 
py2 . py3 none any . whl 634kB 100% | 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ | 634kB 1 . 1MB / s Requirement 
already up to date nltk = 3.1 in / usr 
/ lib / python2 . 7 / site packages from 
textblob Installing collected packages textblob Successfully installed textblob 0 . 
11.1 root @ master ~ # python m textblob . 
download _ corpora nltk _ data Downloading package brown to 
/ root / nltk _ data . . . nltk 
_ data Unzipping corpora / brown . zip . nltk 
_ data Downloading package punkt to / root / nltk 
_ data . . . nltk _ data Unzipping tokenizers 
/ punkt . zip . nltk _ data Downloading package 
wordnet to / root / nltk _ data . . 
. nltk _ data Unzipping corpora / wordnet . zip 
. nltk _ data Downloading package averaged _ perceptron _ 
tagger to nltk _ data / root / nltk _ 
data . . . nltk _ data Unzipping taggers / 
averaged _ perceptron _ tagger . zip . nltk _ 
data Downloading package conll2000 to / root / nltk _ 
data . . . nltk _ data Unzipping corpora / 
conll2000 . zip . nltk _ data Downloading package movie 
_ reviews to / root / nltk _ data . 
. . nltk _ data Unzipping corpora / movie _ 
reviews . zip . Finished . Gensim 简介 Gensim 是 
一个 Python 库 用于 对 大型 语料库 进行 主题 建模 
文件 索引 相似 度 检索 等 它 可以 处理 大于 
内存 的 输入 数据 作者 说 它 是 纯 文本 
上 无 监督 的 语义 建模 最 健壮 高效 易用 
的 软件 网站 Gensim HomePageGitHub piskvorky / gensim Topic Modelling 
for Humans 安装 root @ master ~ # pip install 
U gensim Collecting gensim Downloading gensim 0 . 12.4 . 
tar . gz 2 . 4MB 100% | █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
| 2 . 4MB 358kB / s Collecting numpy = 
1.3 from gensim Downloading numpy 1 . 11.0 cp27 cp27mu 
manylinux1 _ x86 _ 64 . whl 15 . 3MB 
100% | █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ | 15 . 3MB 66kB / 
s Collecting scipy = 0 . 7.0 from gensim Downloading 
scipy 0 . 17.1 cp27 cp27mu manylinux1 _ x86 _ 
64 . whl 39 . 5MB 100% | █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
| 39 . 5MB 27kB / s Requirement already up 
to date six = 1 . 5.0 in / usr 
/ lib / python2 . 7 / site packages / 
six 1 . 10.0 p y 2.7 . egg from 
gensim Collecting smart _ open = 1 . 2.1 from 
gensim Downloading smart _ open 1 . 3.3 . tar 
. gz Collecting boto = 2.32 from smart _ open 
= 1 . 2.1 gensim Downloading boto 2 . 40.0 
py2 . py3 none any . whl 1 . 3MB 
100% | █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ | 1 . 4MB 634kB / 
s Requirement already up to date bz2file in / usr 
/ lib / python2 . 7 / site packages from 
smart _ open = 1 . 2.1 gensim Collecting requests 
from smart _ open = 1 . 2.1 gensim Downloading 
requests 2 . 10.0 py2 . py3 none any . 
whl 506kB 100% | █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ | 512kB 1 . 
4MB / s Installing collected packages numpy scipy boto requests 
smart open gensim Found existing installation numpy 1 . 10.1 
Uninstalling numpy 1 . 10.1 Successfully uninstalled numpy 1 . 
10.1 Found existing installation scipy 0 . 12.1 DEPRECATION Uninstalling 
a distutils installed project scipy has been deprecated and will 
be removed in a future version . This is due 
to the fact that uninstalling a distutils project will only 
partially uninstall the project . Uninstalling scipy 0 . 12.1 
Successfully uninstalled scipy 0 . 12.1 Found existing installation boto 
2 . 38.0 Uninstalling boto 2 . 38.0 Successfully uninstalled 
boto 2 . 38.0 Found existing installation requests 2 . 
8.1 Uninstalling requests 2 . 8.1 Successfully uninstalled requests 2 
. 8.1 Found existing installation smart open 1 . 3.1 
Uninstalling smart open 1 . 3.1 Successfully uninstalled smart open 
1 . 3.1 Running setup . py install for smart 
open . . . done Found existing installation gensim 0 
. 12.3 Uninstalling gensim 0 . 12.3 Successfully uninstalled gensim 
0 . 12.3 Running setup . py install for gensim 
. . . done Successfully installed boto 2 . 40.0 
gensim 0 . 12.4 numpy 1 . 11.0 requests 2 
. 6.0 scipy 0 . 17.1 smart open 1.3 . 
3PyNLPI 简介 它 的 全称 是 Python 自然语言 处理 库 
Python Natural Language Processing Library 音 发作 pineapple 是 一个 
用于 自然语言 处理 任务 库 它 集合 了 各种 独立 
或 松散 互 相关 的 那些 常见 的 不 常见 
的 对 NLP 任务 有用 的 模块 PyNLPI 可以 用来 
处理 N 元 搜索 计算 频率 表 和 分布 建立 
语言 模型 它 还 可以 处理 向 优先 队列 这种 
更加 复杂 的 数据 结构 或者 像 Beam 搜索 这种 
更加 复杂 的 算法 网站 GithubPyNLPI HomePage 安装 从 Github 
上 下载 源码 解压 以后 编译 安装 root @ master 
pynlpl master # python setup . py install Preparing build 
running install running bdist _ egg running egg _ info 
creating PyNLPl . egg info writing requirements to PyNLPl . 
egg info / requires . txt writing PyNLPl . egg 
info / PKG INFO writing top level names to PyNLPl 
. egg info / top _ level . txt writing 
dependency _ links to PyNLPl . egg info / dependency 
_ links . txt writing manifest file PyNLPl . egg 
info / SOURCES . txt reading manifest file PyNLPl . 
egg info / SOURCES . txt writing manifest file PyNLPl 
. egg info / SOURCES . txt installing library code 
to build / bdist . linux x86 _ 64 / 
egg running install _ lib running build _ py creating 
build creating build / lib creating build / lib / 
pynlpl copying pynlpl / tagger . py build / lib 
/ pynlpl . . . . . . byte compiling 
build / bdist . linux x86 _ 64 / egg 
/ pynlpl / _ _ init _ _ . py 
to _ _ init _ _ . pyc byte compiling 
build / bdist . linux x86 _ 64 / egg 
/ pynlpl / mt / _ _ init _ _ 
. py to _ _ init _ _ . pyc 
byte compiling build / bdist . linux x86 _ 64 
/ egg / pynlpl / mt / wordalign . py 
to wordalign . pyc byte compiling build / bdist . 
linux x86 _ 64 / egg / pynlpl / statistics 
. py to statistics . pyc creating build / bdist 
. linux x86 _ 64 / egg / EGG INFO 
copying PyNLPl . egg info / PKG INFO build / 
bdist . linux x86 _ 64 / egg / EGG 
INFO copying PyNLPl . egg info / SOURCES . txt 
build / bdist . linux x86 _ 64 / egg 
/ EGG INFO copying PyNLPl . egg info / dependency 
_ links . txt build / bdist . linux x86 
_ 64 / egg / EGG INFO copying PyNLPl . 
egg info / not zip safe build / bdist . 
linux x86 _ 64 / egg / EGG INFO copying 
PyNLPl . egg info / requires . txt build / 
bdist . linux x86 _ 64 / egg / EGG 
INFO copying PyNLPl . egg info / top _ level 
. txt build / bdist . linux x86 _ 64 
/ egg / EGG INFO creating dist creating dist / 
PyNLPl 0 . 9.2 p y 2.7 . egg and 
adding build / bdist . linux x86 _ 64 / 
egg to it removing build / bdist . linux x86 
_ 64 / egg and everything under it Processing PyNLPl 
0 . 9.2 p y 2.7 . egg creating / 
usr / lib / python2 . 7 / site packages 
/ PyNLPl 0 . 9.2 p y 2.7 . egg 
Extracting PyNLPl 0 . 9.2 p y 2.7 . egg 
to / usr / lib / python2 . 7 / 
site packages Adding PyNLPl 0 . 9.2 to easy install 
. pth file Installed / usr / lib / python2 
. 7 / site packages / PyNLPl 0 . 9.2 
p y 2.7 . egg Processing dependencies for PyNLPl = 
= 0 . 9.2 Searching for httplib2 = 0.6 Reading 
https / / pypi . python . org / simple 
/ httplib2 / Best match httplib2 0 . 9.2 Downloading 
https / / pypi . python . org / packages 
/ ff / a 9/5751 c d f 1 7 
a 7 0 e a 8 9 f 6 d 
d e 2 3 c e b 1 7 0 
5 b f b 6 3 8 f d 8 
c e e 0 0 f 8 4 5 3 
0 8 b f 8 d 2 6 3 9 
7 / httplib2 0 . 9.2 . tar . gz 
# md5 = b d 1 b 1 4 4 
5 b 3 b 2 d f a 7 2 
7 6 b 0 9 b 1 a 0 7 
b 7 f 0 e Processing httplib2 0 . 9.2 
. tar . gz Writing / tmp / easy _ 
install G32Vg8 / httplib2 0 . 9.2 / setup . 
cfg Running httplib2 0 . 9.2 / setup . py 
q bdist _ egg dist dir / tmp / easy 
_ install G32Vg8 / httplib2 0 . 9.2 / egg 
dist tmp IgKi70 zip _ safe flag not set analyzing 
archive contents . . . httplib2 . _ _ init 
_ _ module references _ _ file _ _ Adding 
httplib2 0 . 9.2 to easy install . pth file 
Installed / usr / lib / python2 . 7 / 
site packages / httplib2 0 . 9.2 p y 2.7 
. egg Searching for numpy = = 1 . 11.0 
Best match numpy 1 . 11.0 Adding numpy 1 . 
11.0 to easy install . pth file Using / usr 
/ lib64 / python2 . 7 / site packages Searching 
for lxml = = 3 . 2.1 Best match lxml 
3 . 2.1 Adding lxml 3 . 2.1 to easy 
install . pth file Using / usr / lib64 / 
python2 . 7 / site packages Finished processing dependencies for 
PyNLPl = = 0.9 . 2spaCy 简介 这 是 一个 
商业 的 开源 软件 结合 了 Python 和 Cython 优异 
的 NLP 工具 是 快速 的 最 先进 的 自然 
语言 处理 工具 网站 HomePageGitHub 安装 root @ master pynlpl 
master # pip install spacy Collecting spacy Downloading spacy 0 
. 101.0 cp27 cp27mu manylinux1 _ x86 _ 64 . 
whl 5 . 7MB 100% | █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ | 5 
. 7MB 161kB / s Collecting thinc 5 . 1.0 
= 5 . 0.0 from spacy Downloading thinc 5 . 
0.8 cp27 cp27mu manylinux1 _ x86 _ 64 . whl 
1 . 4MB 100% | █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ | 1 . 
4MB 287kB / s Collecting murmurhash 0.27 = 0.26 from 
spacy Downloading murmurhash 0 . 26.4 cp27 cp27mu manylinux1 _ 
x86 _ 64 . whl Collecting cloudpickle from spacy Downloading 
cloudpickle 0 . 2.1 py2 . py3 none any . 
whl Collecting plac from spacy Downloading plac 0 . 9.1 
. tar . gz 151kB 100% | █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ | 
153kB 3 . 2MB / s Requirement already satisfied use 
upgrade to upgrade numpy = 1.7 in / usr / 
lib64 / python2 . 7 / site packages from spacy 
Requirement already satisfied use upgrade to upgrade six in / 
usr / lib / python2 . 7 / site packages 
/ six 1 . 10.0 p y 2.7 . egg 
from spacy Collecting cymem 1.32 = 1.30 from spacy Downloading 
cymem 1 . 31.2 cp27 cp27mu manylinux1 _ x86 _ 
64 . whl 66kB 100% | █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ | 71kB 
4 . 3MB / s Collecting preshed 0.47 = 0 
. 46.1 from spacy Downloading preshed 0 . 46.4 cp27 
cp27mu manylinux1 _ x86 _ 64 . whl 223kB 100% 
| █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ | 225kB 2 . 4MB / s 
Collecting sputnik 0 . 10.0 = 0 . 9.2 from 
spacy Downloading sputnik 0 . 9.3 py2 . py3 none 
any . whl Collecting semver from sputnik 0 . 10.0 
= 0 . 9.2 spacy Downloading semver 2 . 5.0 
. tar . gz Installing collected packages murmurhash cymem preshed 
thinc cloudpickle plac semver sputnik spacy Running setup . py 
install for plac . . . done Running setup . 
py install for semver . . . done Successfully installed 
cloudpickle 0 . 2.1 cymem 1 . 31.2 murmurhash 0 
. 26.4 plac 0 . 9.1 preshed 0 . 46.4 
semver 2 . 5.0 spacy 0 . 101.0 sputnik 0 
. 9.3 thinc 5.0 . 8Polyglot 简介 Polyglot 支持 大 
规模 多语言 应用 程序 的 处理 它 支持 165种 语言 
的 分词 196中 语言 的 辨识 40种 语言 的 专有名词 
识别 16种 语言 的 词性 标注 136种 语言 的 情感 
分析 137种 语言 的 嵌入 135种 语言 的 形态 分析 
以及 69种 语言 的 翻译 特性 如下 Tokenization 165 Languages 
Language detection 196 Languages Named Entity Recognition 40 Languages Part 
of Speech Tagging 16 Languages Sentiment Analysis 136 Languages Word 
Embeddings 137 Languages Morphological analysis 135 Languages Transliteration 69 Languages 
网站 Github 安装 root @ master pynlpl master # pip 
install polyglot Collecting polyglot Downloading polyglot 15 . 10.03 py2 
. py3 none any . whl 54kB 100% | █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ | 61kB 153kB / s Collecting pycld2 = 0.3 
from polyglot Downloading pycld2 0.31 . tar . gz 14 
. 3MB 100% | █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ | 14 . 3MB 
71kB / s Collecting wheel = 0 . 23.0 from 
polyglot Downloading wheel 0 . 29.0 py2 . py3 none 
any . whl 66kB 100% | █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ | 71kB 
4 . 2MB / s Collecting futures = 2 . 
1.6 from polyglot Downloading futures 3 . 0.5 py2 none 
any . whl Requirement already satisfied use upgrade to upgrade 
six = 1 . 7.3 in / usr / lib 
/ python2 . 7 / site packages / six 1 
. 10.0 p y 2.7 . egg from polyglot Collecting 
PyICU = 1.8 from polyglot Downloading PyICU 1 . 9.3 
. tar . gz 179kB 100% | █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ | 
184kB 2 . 9MB / s Collecting morfessor = 2.0 
. 2a1 from polyglot Downloading Morfessor 2.0 . 2alpha3 . 
tar . gz Installing collected packages pycld2 wheel futures PyICU 
morfessor polyglot Running setup . py install for pycld2 . 
. . done Running setup . py install for PyICU 
. . . done Running setup . py install for 
morfessor . . . done Successfully installed PyICU 1 . 
9.3 futures 3 . 0.5 morfessor 2.0 . 2a3 polyglot 
15 . 10.3 pycld2 0.31 wheel 0.29 . 0MontyLingua 简介 
MontyLingua 是 一个 免费 的 功能 强大 的 端 到 
端的 英文 处理 工具 在 MontyLingua 输入 原始 英文 文本 
输出 就会 得到 这段 文本 的 语义解释 它/r 适用/v 于/p 
信息/n 检索/vn 和/c 提取/v 请求 处理 问答 系统 从 英文 
文本 中 它 能 提取 出 主 动宾 元组 形容词 
名词 和 动词短语 人名 地名 事件 日期 和 时间 等 
语义 信息 网站 HomePageGithub 安装 无 U s a g 
e W e b s e r v i c 
e p y t h o n server . pyThe 
webservice runs on port 8001 at / service by default 
. For parameters etc see the NIF spec . Therefore 
you can curl your query like thiscurl http / / 
localhost 8001 / service nif = true & input type 
= text & input = This % 20is % 20a 
% 20city % 20called % 20Berlin . or simply use 
your browser to query the target . Consolepython nif . 
pyBut this method is mainly for debugging purposes and supports 
only hardcoded options . BLLIP Parser 简介 BLLIP Parser 也叫做 
Charniak Johnson parser 是 一个 集 成了 生成 成分 分析器 
和 最大熵 排序 的 统计 自然语言 分析器 它 包括 命令行 
和 python 接口 网站 GitHubHomePage 安装 root @ master pynlpl 
master # pip install user bllipparser Collecting bllipparser Downloading bllipparser 
2015 . 12.3 . tar . gz 548kB 100% | 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ | 552kB 1 . 2MB / s Requirement 
already satisfied use upgrade to upgrade six in / usr 
/ lib / python2 . 7 / site packages / 
six 1 . 10.0 p y 2.7 . egg from 
bllipparser Building wheels for collected packages bllipparser Running setup . 
py bdist _ wheel for bllipparser . . . done 
Stored in directory / root / . cache / pip 
/ wheels / 6f / 7a / d 8/037 a 
4 a a 0 f a 2 7 5 f 
4 3 e 1 1 2 9 0 0 8 
e b 7 8 3 4 d c 8 5 
2 2 e f 1 5 8 d 2 e 
9 6 5 3 4 b Successfully built bllipparser Installing 
collected packages bllipparser Successfully installed bllipparserQuepy 简介 Quepy 是 一个 
Python 框架 提供 了 将 自然 语言 问题 转换 成为 
数据库 查询语言 中的 查询 它 可以 方便 地 自定义 自然 
语言 中 不同 类型 的 问题 和 数据库 查询 所以 
通过 Quepy 仅仅 修改 几行 代码 就 可以 构建 你 
自己 的 自然 语言 查询 数据库系统 网站 GitHub machinalis / 
quepy A python framework to transform natural language questions to 
queries in a database query language . Quepy A Python 
framework to transform natural language questions to queries . 安装 
root @ master pynlpl master # pip install quepy Collecting 
quepy Downloading quepy 0.2 . tar . gz 42kB 100% 
| █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ | 51kB 128kB / s Collecting refo 
from quepy Downloading REfO 0.13 . tar . gz Requirement 
already satisfied use upgrade to upgrade nltk in / usr 
/ lib / python2 . 7 / site packages from 
quepy Collecting SPARQLWrapper from quepy Downloading SPARQLWrapper 1 . 7.6 
. zip Collecting rdflib = 4.0 from SPARQLWrapper quepy Downloading 
rdflib 4 . 2.1 . tar . gz 889kB 100% 
| █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ | 890kB 823kB / s Collecting keepalive 
= 0.5 from SPARQLWrapper quepy Downloading keepalive 0.5 . zip 
Collecting isodate from rdflib = 4.0 SPARQLWrapper quepy Downloading isodate 
0 . 5.4 . tar . gz Requirement already satisfied 
use upgrade to upgrade pyparsing in / usr / lib 
/ python2 . 7 / site packages from rdflib = 
4.0 SPARQLWrapper quepy Collecting html5lib from rdflib = 4.0 SPARQLWrapper 
quepy Downloading html5lib 0.9999999 . tar . gz 889kB 100% 
| █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ █ █ █ █ █ █ █ 
█ █ █ | 890kB 854kB / s Requirement already 
satisfied use upgrade to upgrade six in / usr / 
lib / python2 . 7 / site packages / six 
1 . 10.0 p y 2.7 . egg from html5lib 
rdflib = 4.0 SPARQLWrapper quepy Building wheels for collected packages 
quepy refo SPARQLWrapper rdflib keepalive isodate html5lib Running setup . 
py bdist _ wheel for quepy . . . done 
Stored in directory / root / . cache / pip 
/ wheels / c 8/04 / bf / 4 9 
5 b 8 8 a 6 8 a a 5 
c 1 e 9 d d 1 6 2 9 
b 0 9 a b 7 0 2 6 1 
6 5 1 c f 5 1 7 d 1 
b 1 c 2 7 4 6 4 Running setup 
. py bdist _ wheel for refo . . . 
done Stored in directory / root / . cache / 
pip / wheels / 76/97 / 81/825976 c f 0 
a 2 b 9 a d 7 5 9 b 
b e c 1 3 a 6 4 9 2 
6 4 9 3 8 d f f b 5 
2 d f d 5 6 a c 6 c 
8 Running setup . py bdist _ wheel for SPARQLWrapper 
. . . done Stored in directory / root / 
. cache / pip / wheels / 50 / fe 
/ 25 / b e 6 e 9 8 d 
a a 4 f 5 7 6 4 9 4 
d f 2 a 1 8 d 5 e 8 
6 a 1 8 2 e 3 d 7 e 
0 7 3 5 d 0 6 2 c c 
9 8 4 Running setup . py bdist _ wheel 
for rdflib . . . done Stored in directory / 
root / . cache / pip / wheels / fb 
/ 93 / 10/4 f 8 a 3 e 9 
5 9 3 7 d 8 d b 4 1 
0 a 4 9 0 f a 2 3 5 
b d 9 5 e 0 e 0 d 4 
1 b 5 f 6 2 7 4 b 2 
0 e 5 Running setup . py bdist _ wheel 
for keepalive . . . done Stored in directory / 
root / . cache / pip / wheels / 16/4 
f / c 1/121 d d f f 6 7 
b 1 3 1 a 3 7 1 b 6 
6 d 6 8 2 f e e f a 
c 0 5 5 f b d b b 9 
5 6 9 b f d e 5 c 5 
1 Running setup . py bdist _ wheel for isodate 
. . . done Stored in directory / root / 
. cache / pip / wheels / 61 / c0 
/ d 2/6 b 4 a 1 0 c 2 
2 2 b a 9 2 6 1 a b 
9 8 7 2 a 8 f 0 5 d 
4 7 1 6 5 2 9 6 2 2 
8 4 e 8 c 6 7 7 e 5 
e 7 Running setup . py bdist _ wheel for 
html5lib . . . done Stored in directory / root 
/ . cache / pip / wheels / 6f / 
85/6 c / 5 6 b 8 e 1 2 
9 2 c 6 2 1 4 c 4 e 
b 7 3 b 9 d d a 5 0 
f 5 3 e 8 e 9 7 7 b 
f 6 5 9 8 9 3 7 3 c 
9 6 2 Successfully built quepy refo SPARQLWrapper rdflib keepalive 
isodate html5lib Installing collected packages refo isodate html5lib rdflib keepalive 
SPARQLWrapper quepy Successfully installed SPARQLWrapper 1 . 7.6 html5lib 0.9999999 
isodate 0 . 5.4 keepalive 0.5 quepy 0.2 rdflib 4 
. 2.1 refo 0 . 13MBSP 简介 MBSP is a 
text analysis system based on the TiMBL and MBT memory 
based learning applications developed at CLiPS and ILK . It 
provides tools for Tokenization and Sentence Splitting Part of Speech 
Tagging Chunking Lemmatization Relation Finding and Prepositional Phrase Attachment . 
The general English version of MBSP has been trained on 
data from the Wall Street Journal corpus . 网站 HomePageGithub 
安装 下载 解压 编译 安装 root @ master MBSP # 
python setup . py install . . . . . 
编译 的 信息 . . . . . . . 
. . . 2 分钟 左右 . . . . 
. 参考 李岩 知乎 回答 目前 常用 的 自然 语言 
处理 开源 项目 / 开发包 有 哪些 数 盟 用 
Python 做 自然语言 处理 必 知 的 八个 工具 1.1 
语言 计算 文本 和 词汇 入门 nltk 下载 地址 使用 
pip 安装 import nltk 检验 是否 成功 nltk . download 
选择 语料 下载 使用 python 解释器 加载 book 模块 中的 
条目 from nltk . book import * 输入 名字 如 
text1 即可 找到 相应 的 文本 搜索 文本 搜索 文本 
中 的 某个 词 text1 . concordance monstrous 搜索 文本 
中 与 指定 词 相似 的 词 text1 . similar 
monstrous 研究 同一 文本 中 的 两个 及 以上 的 
词 之间 的 关系 text2 . common _ contexts monstrous 
very 可以 用 来 考察 两次 的 用法 是否 相似 
根据 几个 词 在 文本 中 出现 位置 的 离散 
图 观察 单词 分布 text4 . dispersion _ plot citizens 
democracy freedom duties America 生成 文本 text3 . generate 书中 
nltk2 . 0.1 版本 可用 但是 新 版本 不再 支持 
计数 词汇 使用 len 函数 获取 文本 长度 包括 单词 
及 标点 len text3 获取 无 重复 的 词汇表 sorted 
set text3 计算 文本 词汇 丰富 度 from _ _ 
feature _ _ import division len text3 / len set 
text3 特定 单词 计数 text3 . count smote 1.2 近观 
Python 将 文本 当做 词 链表 主要 介绍 Python 中 
链表 的 相关 操作 略 1.3 计算 语言 简单 的 
统计 频率分布 统计 文本 中词 的 词频 降序 排列 保存 
至 map 中 fdist1 = FreqDist text1 vocabulary1 = fdist1 
. keys vocabulary1 50 Top50 词频 可视化 fdist1 . plot 
50 cumulative = True 查看 文本 中 只 出现 一次 
的 词 fdist1 . hapaxes 细粒度 选择 词 找出 文本 
中 长度 超过 15 的 词 V = set text1 
long _ words = w for w in V if 
len w 15 sorted long _ words 词语/n 搭配/v 和双/nr 
连词/n 提取/v 文本/n 词汇/n 中的/i 词/n 对/p list bigrams more 
is said than done collocations 函数 在 已知 单个 词 
的 词频 基础 上 找到 出现 频繁 的 双 连词 
text4 . collocations 计算 其他 东西 查看 文本 中 词长 
的 分布 fdist = FreqDist len w for w in 
text1 fdist . keys 输出 结果表明 text1 中 最长 的 
词 是由 20 个字符 组成 fdist . items fdist . 
max fdist . freq 3 由 结果 可知 文本 中 
长度 为 3 的 词 最 频繁 约占 20% E 
x a m p l e s D e s 
c r i p t i o n s f 
d i s t = FreqDist samples 创建 包含 给定 
样本 的 频率分布 fdist . inc samples 增加 样本 fdist 
monstrous 计数 给定 样本 出现 的 次数 fdist . freq 
monstrous 给定 样本 的 频率 fdist . N 样本 总数 
fdist . keys 以 频率 递减 顺序 排序 的 样本 
链接 for sample in fdist 以 频率 递减 的 顺序 
遍历 样本 fdist . max 数值 最大 的 样本 fdist 
. tabulate 绘制 频率分布 表 fdist . plot 绘制 频率 
分布图 fdist . plot cumulative = True 绘制 累积 频率 
分布图 fdist1 fdist2 测试 样本 在 fdist1 中 出现 的 
频率 是否 小于 fdist21 . 4 回到 Python 决策 与 
控制 主要 介绍 for 循环 与 条件 语句 略 1.5 
自动 理解 自然语言 词义 消 岐 指代 消解 anaphora resolution 
自动 生成 语言 遗憾 地 发现 在 nltk3 . 2 
里 书 中的 babelize _ shell 这个 服务 也 不再 
提供 了 人机对话 系统 import nltk nltk . chat . 
chatbots 文本 的 含义 一 前言 1 前人 研究 图灵 
的 图灵机 关于 算法 计算 模型 的 研究 图灵机 是 
一种 抽象 的 数学 模型 香农 的 信息 论 噪声 
声道 解码 把 熵 作为 测量 信道 的 信息 能力 
或者 语言 的 信息量 的 一种 方法 用 概率 测定 
噪声 信道 与 解码 模型 信息 文字/n 和/c 语言/n //i 
数字/n 和/c 信息/n 信息冗余 是 信息 安全 的 保障 / 
语料 对 翻译 至关重要 信息 的 度量 信息熵 是 对 
一个 信息 系统 不确定性 的 度量 熵 冗余度 条件 熵 
互信息 相对 熵 相对 熵 利用 它 可以 得到 词 
频率 逆向 文档 频率 TF IDF 香农 第一 定理 对于 
一个 信息 任何 编码 的 长度 都 不小于 它 的 
信息 熵 信息 的 作用 就是 消除 不 确定 性 
自然语言 处理 的 大量 问题 就是 找 相关 的 信息 
2 发展 历史 90 年代 规则 系统 专家 系统 和 
知识 工程 1990 2014 概率 系统 规则/n 从/p 数据/n 中/f 
抽取/v //i 规则/n 是/v 有/v 概率/n 的/uj 流程 设计 手机 
训练 数据 预处理 抽取 特征 分类器 预测 评估 特征/n 和/c 
流程/n 都是/nr 专家/n 设计/vn 的/uj 存在 大量 独立 的 子 
任务 2014 之后 深度 学习 3 形式 模型 1 基于 
短语 结构 语法 的 形式 模型 基于 合一 运算 的 
形式 模型 基于 依存 和 配价 的 形式 模型 基于 
格 语法 的 形式 模型 基于 词汇 主义 的 形式 
模型 2 基于 概率 和 统计 的 形式 模型 n 
gram 隐 马尔科夫 模型 最大熵 模型 条件 随 机场 查理 
亚克 的 概率 上下文 无关 语法 和 词汇 化 的 
概率 上下文 无关 语法 贝叶斯 公式 动态规划 算法 噪声 信道 
模型 最小 编辑 距离 算法 决策树 模型 加权 自动机 维 
特比 算法 向内 向外 算法 向前 向后 算法 3 语义 
自动 处理 的 形式 模型 4 语用 自动 处理 的 
形式 模型 4 基本 介绍 自然语言 处理 是 机器 理解 
人类 语言 和 表达 方式 并 作出 回应 句法分析 和 
语义 消 歧 依赖 上下文 消除歧义 性 语言 信息 世界 
信息 和 视觉 信息 主要 任务 文本处理 文本 生成 文本 
翻译 层次 分类 语音学 形态学 语法学 语义学 语用学 研究 方法 
理性主义 经验主义 统计 方法 隐 马尔可夫 模型 上下文 无关 文法 
噪声 信道 模型 关键问题 歧义 消解 问题 和 未知 语言 
现象 挑战 一 词 多义 新词 不 规范 用语 领域 
隔离 只有 封闭 环境 可用 数据 获取 难 效果 评估 
难 过去 25 年来 自动 问答 的 需求 被 网页 
搜索 和 数据挖掘 替代 新 的 应用 越来越 依靠 数据 
的 作用 和 浅层 的 自然 语言 处理 研究者 们 
也从 单纯 的 句法分析 和 语义 理解 转变 到了 对 
机器 翻译 语音识别 文本 生成 数据挖掘 和 知识 获取 等 
方向 二 形式语言 与 自动机 语言 按照 一定 规律 构成 
的 句子 或者 字符串 的 有限 或者 无限 的 集合 
描述语言 的 三种 途径 穷举法 文法 描述 和 自动机 基础知识 
集合论 / 图论 1 基本 概念 图 树 和 字符串 
2 形式 语言缺陷 对于 像 汉语 英语 这样 的 大型 
自然 语言 系统 难以 构造 精确 的 文法 / 不符合 
人类 学习 语言 的 习惯 / 有些 句子 语法 正确 
但在 语义上 却 不 可能 形式语言 无法 排出 这些 句子 
解决 方向 基于 大量 语料 采用 统计学 手段 建立 模型 
形式 语法 正则文法 上下文 无关 文法 上下文 相关 文法 和 
无约束 文法 3 自动机 有限 自动机 下推 自动机 线性 带 
限 自动机 和 图灵机 应用 单词 自动 查错 纠正 / 
词性 消 歧 三 语料库 与 词汇 知识库 1 语料库 
语料库 基于 语料 的 统计 方法 2 词汇 知识库 3 
本体论 4 知网 定义 了 各种 关系 / 动态 演化 
认知 架构 系统 概念 对象 和 动作 对象 概念 之间 
定义 了 两种 关系 扩展 和 属性 动作 接受 一些 
概念 对象 然后 产出 一些 新的 概念 对象 动作 接受 
的 概念 对象 有 两类 一类 是 必须 要 有的 
没有 动作 就 没法 执行 另一类 是 可选 的 可有可无 
类似 提供 了 默认 参数 四 统计 语言 模型 语言 
模型 就是 给 某 句 语言 打分 给 某个 话题 
打分 狗叫 模型 球星 模型 电影 模型 概率 系统 基本 
分类器 经典 序列 模型 HMM / CRF / EM 自动机 
语言 模型 概率 语言 模型 核心 就是 通过 分数 告诉 
机器 怎么 说话 概率模型 语言 模型 翻译 模型 文本 对齐 
seq2seq 模型 语言 模型 文法 语言 模型 统计 语言 模型 
统计 语言 模型 n gram 模型 深度 学习 神经 序列 
模型 LSTM 相似 度 计算 篇章 表示 编辑 距离 computing 
device 自动机 规则 系统 分类器 搜索 技术 关键词 匹配 Beam 
Search Local Sensitive Hashing 倒排索引 语言 相关 技术 stemming 同义词 
识别 替换 中文分词 语法分析 语义 意图 理解 seq2seq 模型 基本 
分类器 诗歌 生成 情感 分析 机器翻译 序列 标注 人 能否 
解决 如果能 就 自己 解决 然后 考虑 机器 能否 模仿 
人 如果 不能 解决 就 尝试 从 计算机 的 角度 
思考 Bash Script wc / sed / awk / grep 
/ sort / uniq / paste / cat / head 
/ tail 学会 linux 下 的 基本 命令 python 处理 
稍微 复杂 的 问题 Stanford Core NLP 语义分析 NLTK 句子 
划分 读取 语义 树 Tensorflow 有向 网络图 首先 将 问句 
进行 词 法分析 得到 语义 组块 序列 然后 对 其 
进行 意图 识别 意图 分为 两部分 目标 概念 对象 和 
条件 概念 对象 Viv 的 核心 技术 就是 利用 DECAS 
找到 从 条件 概念 对象 到 目标 概念 对象 的 
联通 路径 称之为 计划 1 n gram 模型 统计 语言 
模型 根据 前面 的 所有 词 测算 当前 词 出现 
的 概率 最后 累积 相乘 得到 整句话 出现 的 概率 
/ 马尔可夫 假设 当前 词 出现 的 概率 只 与 
前 一个 词 有关 二元 模型 也 可以 假设 由 
前面 n 1个 词 决定 为 n gram 模型 结合 
语料库 计算 相对 频度 根据 大数定理 估算 最终 条件概率 n 
gram 模型 利用 链式法则 计算 每 句话 的 概率 P 
w1 . w2 . wn 引入 马尔可夫 假设 无 记忆性 
未来 的 事件 只 取决于 有限 的 历史 unigram \ 
bigram \ trigram 分别 对应 1 2 3个 参考 事件 
模型 评估 外 在 评估 能 不能 抓到 老鼠 语音 
识别 的 准确性 特点 接近 业务 场景 但 比较 慢 
和 复杂 内在 评估 颜色 速度 和 力量 预测 测试 
集 的 能力 特点 与/p 真正/d 的/uj 目标/n 有/v 偏差/n 
但/c 快/a 和/c 简单/a perplexity ppx 指标 新词 新词 未 
登录 词 ovv p wi | wi 1 wi 2 
= count wi 2 wi 1 wi / count wi 
2 wi 1 最大 似 然 估计 方法 log p 
Td 拉格朗日 法 工具 KenLM Modified Kneser Ney Smoothing 应用 
案例 完形填空 加入 使用 3 gram LM 数据 平滑 模型 
的 参数 模型 中 所有 的 条件 概率 模型 的 
训练 通过 对 语料 的 统计 得到 这些 参数 的 
过程 由于 大数定理 的 局限 需要 增加 数据量 但 不可避免 
因此 出现 了 概率 估计 古德 图灵 估计 Zipf 定律 
出现 次数 少 的 词 总比 出现 次数 多 的 
词 要多 数据 平滑 解决 零 概率 下调 出现 频率 
很低 的 词 的 概率 卡茨 退避 法 n gram 
平滑 本质上 是 贫富 分化 的 问题 + 1 平滑 
方法 政府 给 大家 每人 发 一点 钱 没用 Back 
off 回退 法 自己/r 有/v 钱/n 自己/r 出/v 自己 没钱 
爸爸 出 爸爸 没钱 爷爷 出 interpolate 插值法 自己 爸爸 
爷爷 各出 一笔 钱 Development Set EM 最大 期望值 算法 
解决 Absolute Discounting 绝对 折扣 有钱 的 每个 人叫 固定 
的 税 D 建立 一个 基金 没钱 的 根据/p 自己/r 
爸爸/n 有/v 多少/m 钱来分/nr 了/ul 这个/r 基金/n Kneser Ney 有钱人 
缴 固 定税 按 爸爸 人脉 分配 词 的 适配 
度 Modified KN 有钱人 缴 阶梯 税 按 爸爸 人脉 
分配 阶梯 税率 最好 的 方法 自适应 训练 用户 特定 
的 语言 模型 的 步骤 如下 将 训练 语言 模型 
的 文本 按照 主题 分成 很多 不同 的 类别 / 
对于 每个 类 找到 它们 的 特征 向量 / 统计 
某个人 输入 的 文本 得到 他 输入 词 的 特征向量 
/ 余弦定理 测 相似 度 / 选择 距离 最近 的 
类 对应 的 文本 作为 这个 特定 用户 语言 模型 
的 训练 数据 / 训练 处 一个 用户 特定 的 
语言 模型 特定 模型 在 特定 领域内 效果 比 通用 
模型 好 但 相对 偏僻 的 内容 就 比不上 通用 
模型 因此 需要 采用 最大熵 模型 来 综合 两 个 
模型 的 特征 简化 来做 的话 采用 线性插值 的 模型 
2 其他 语言 模型 指数 概率 图 模型 马尔可夫 模型 
隐 马尔可夫 模型 通信模型 信息 上下文 编码 信道 解码 接收 
独立 输出 假设 三个 基本问题 1 . 给定 一个 模型 
如何 计算 某个 特定 的 输出 序列 的 概率 前 
向 后向 算法 2 . 给定 一个 模型 和 某个 
特定 的 输出 序列 如何 找到 最 可能 产生 这个 
输出 的 状态 序列 维 特比 算法 3 . 给定 
足 够量 的 观测 数据 如何 估计 隐含 马尔可夫 模型 
的 参数 模型 训练 模型 训练 计算 转移 概率 和 
生成 概率 有 监督 训练 人工 标注 无 监督 训练 
鲍姆 韦尔奇 算法 EM 过程 期望值 最大化 保证 算法 迭代 
到 最优 条件 随 机场 条件 随 机场 是 隐 
马尔可夫 模型 的 扩展 是 一种 特殊 的 概率 图 
模型 变量 之间 要 遵循 马尔可夫 假设 即 每个 状态 
的 转移 概率 只 取决于 相邻 的 概率 与 贝叶斯 
网络 不同 的 是 条件 随 机场 是 无向图 根据 
最大熵 原则 希望 找到 一个 符合 所有 边缘 分布 同时 
使得 熵 最大 的 模型 就是 指数函数 浅层 句法分析 看到 
的 东西 是 词 词性 要 推导 的 东西 是 
语法 成分 条件 随 机场 是 一个 非常 灵活 的 
用于 预测 的 统计模型 对于 给定 的 句子 进行 分词 
词性 标记 命名 实体 识别 和 链接 句法分析 语义 角色 
识别 和 多义词 消 歧 前后 向 算法 及 参数估计 
维 特比 算法 解码 算法 动态规划 算法 可以 解决 最短 
路径 的 问题 凡是 使用 隐 马尔可夫 模型 描述 的 
问题 都 可以 用 它 来 解码 可以 概括 成 
以下 三点 1 如果 最短 路径 经过 某个 点 那么 
这 一条 路径 上 的 子 路径 一定 也是 最短 
路径 否则 用 另一 段 最短 路径 来 代替 它 
便 构成 更短 的 路径 这 明显 是 矛盾 的 
2 路径 上 必定 经过 某个 时刻 的 某个 状态 
嘉 假定 在 某个 时刻 有 很多 状态 那么 如果 
记录 了 从 起始 到 这个 状态 所有 节点 的 
最短 路径 最终 最短 路径 一定是 这 其中 的 一条 
3 假定 状态 变更 最短 路径 已经 找到 并且 记录 
在 这些 节点 上 那么 考虑 最短 路径 时 只 
需要 考虑 前 一个 状态 的 最短 路径 以及 这个 
节点 的 距离 即可 维 特比 算法 是 和 长度 
成正比 的 五 神经 网络 语言 模型 5种 神经 网络 
语言 模型 a Neural Network Language Model NNLMb Log Bilinear 
Language Model LBLc Recurrent Neural Network based Language Model RNNLMd 
Collobert 和 Weston 在 2008 年 提出 的 C & 
W 模型 e Mikolov 等人 提出 了 CBOW Continuous Bagof 
Words 和 Skip gram 模型 词 向量 word2vec 的 神经 
网络 是 浅层 的 GloVe 实现 了 一种 计数 方法 
借助 两者 进行 训练 的 模型 通常 用作 深度 学习 
NLP 方法 的 输入 数据 fastText 避免了 OOV 问题 在 
小 数据集 上 效果 更优 主要 好处 是 不用 自己 
积累 语料库 只需要 爬 取 网络 数据 即可 为什么 要 
用 向量 表示 因为 单词 编码 是 任意 的 很难 
表示出 之间 的 任意 关系 还 会 带来 数据 系数 
问题 使用 向量 进行 词 的 表示 可以 克服 一些 
障碍 上下文 相似 的 词 其 语义 也 相似 词 
的 语义 由其 上下文 决定 选择 一种 方式 描述 上下文 
选择 一种 模型 刻画 目标 词 与 上下文 之间 的 
关系 GloVe Global Vector 模型 是 一种 基于 矩阵 的 
分布式 表示 模型 词 向量 基于 神经 网络 的 分布式 
表示 word e m b e d d i n 
g w o r d 2 v e c 模型 
通常用于 预处理 阶段 的 词 向量 表示 一种 方法 是 
构建 共 现 矩阵 其中 包含 着 语料库 中 每一个 
单词 同出 现在 它 后 一个 单词 的 统计 信息 
相同 语境 中 的 词语 具有 相似 的 语义 基于 
这 一 原则 有 两类 方法 计数 的 方法 隐性 
语义分析 和 预测 方法 神经 概率 语言 模型 两者 的 
区别 在于 前者 计算出 共同 出现 的 概率 然后 映射 
到 每个 词 的 小而 密集 的 向量 预测模型 直接 
尝试 根据 学习 到 的 近邻 单词 的 小 密集 
嵌入 向量 模型 参数 来 预测 单词 一种 用于 从 
原始 文本 中 学习 词 嵌入 的 模型 具有 很高 
的 计算 效率 两种 实现 方式 连续 词 袋 模型 
CBOW skip gram 模型 两者 相似 唯一 的 区别 在于 
前者 从源/nr 上下文 单词 中 预测 目标 单词 而 后者 
根据 目标 单词 预测 源 上下文 单词 简单 的 窗口 
分类器 softmax 将 元素 变成 概率值 最 重要 的 3个 
环节 是 分词 锁定 关键词 文本 相似 度 计算 1 
词义 相似 度 计算 词义 向量 微积分 lamanda 为 向量 
的 短语 赋予 意义 情绪 分析 邮件 建议 回复 机器翻译 
2 skip gram 模型 给定 概率 向量 表示 最大化 概率分布 
损失 函数 目标函数 成本 函数 单词 序列 theta 模型 中心 
向量 和 上下文 向量 更改 参数 期望 向量 加权 梯度 
下降 SGD3 编码 会话 职业 展览 项目 建议 哈希表 语料库 
损失 函数 0 1 随机抽样 超 参数 unigram hacky 最大化 
概率 最小化 成本 连续 词汇 模型 PCA 主 成分 分析 
语义 组合 简单 的 加权 组合 卷积 神经网络 循环 神经网络 
递归 神经网络 六 基本 方法 1 词 法分析 中文 字 
/ 词 / 短语 / 句子 / 段落 / 文档 
相关 概率 定义 / 相关 任务模型 / 方法 词性 标注 
及其 一致性 检查 方法 分词 方法 / 未 登录 词 
处理 / 词性 标注 / 自动 校对 / 命名 实体 
识别 中文分词 词 是 表达 语义 的 最小 单位 中国 
/ 航天 / 历史 / 已经 / 有//nr 100 / 
年 查字典 / 动态规划 利用 维 特比 算法 快速 地 
找到 最佳 分词 分词器 分词 的 一致性 / 词 的 
颗粒度 和 层次 矩阵 运算 和 文本处理 中 的 两个 
分类 问题 SVD 奇异 值 分解 就是 把 一个 大 
矩阵 分解成 三个 小 矩阵 相乘 第一 个 矩阵 X 
是 对词 进行 分类 的 一个 结果 最后 一个 矩阵 
Y 是 对 文本 的 分类 结果 中间 矩阵 B 
表示 词 的 类 和问/nr 这个 的 类 之间 的 
相关性 使用 矩阵 的 特征值 和 数值 分析 中 的 
各种 算法 就 可以 进行 奇异 值 分解 奇异 值 
分解 不可 迭代 适合 处理 超大规模 文本 的 粗 分类 
信息 指纹 及其 应用 / 相似 哈希 SPAM 反作弊 1 
从 信息源 出发 加强 通信 编码 自身 的 抗干扰能力 2 
从 传输 来看 过滤掉 噪音 还原 信息 最大熵 模型 / 
原理 保留 全部 的 不确定性 将 风险 降到 最小 贝叶斯 
网络 马尔可夫 链 的 扩展 很多 事物 的 相互 关系 
显然 不能 用 一条 链 来 表示 他们/r 之间/f 的/uj 
关系/n 可能/v 是/v 一个/m 有向/nr 网络/n 状态 和 关系 可信度 
用 概率 来 描述 可以 有 附加 的 权重 它 
虽然 也 是 依赖 前 一个 的 状态 但 不受 
链状 结构 的 限制 可以 更 准确 滴 描述 事件 
之间 的 相关性 贝叶斯 网络 在 文本 分类 主题 模型 
中 的 应用 把 文本 和 关键词 的 关联矩阵 扭转 
90度 进行 奇异 值 分解 或者 对 每 一个 词 
以 文本 为 维度 建立 一个 向量 再进 行向量 的 
聚 类 那么 得到 的 是 对词 的 分类 而不是 
文本 的 分类 分 出来 的 每 一类 我们 成为 
一个 概念 贝叶斯 网络 是 一个 加权 的 有向图 是 
马尔可夫 链 的 扩展 它 克服 了 马尔可夫 链 那种 
机械 的 线性 约束 把 任何 有 关联 的 事件 
统一 到 了 它 的 框架 下面 期望 最大化 算法 
文本 自动 分类 算法 不 需要 预定义 类别 也 不 
需要 合并 聚 类 只要 随机 的 挑出 一些 类 
的 中心 然后 优化 这个 中心 是 他们 和 真实 
的 聚 类 中心 尽可能 一致 分类 的 步骤 如下 
1 随机 挑选 一些 点 作为 起始 的 中心 2 
计算所 有点 到 这些 聚 类 中心 的 距离 将 
这些 点 归到 最近 的 一类 中 3 重新 计算 
每 一类 的 中心 新的 聚 类 中心 和 原先 
的 相比 会 有一个 位移 4 重复 上述 过程 直到 
新的 中心 和旧的/nr 中心 之间 偏移 非常 非常 小 即 
过程 收敛 1 根据 现有 的 聚 类 结果 对 
所有 数据 进行 重新 划分 如果把 最终 的 分类 结果 
看作 是 一个 数学 的 模型 那么 这些 聚 类 
的 中心 以及 每 一个 点 和聚类/nr 的 隶属 关系 
可以 看作 是 这个 模型 的 参数 2 根据 重新 
划分 的 结果 得到 新的 聚 类 最大化 目标函数 EM 
算法 E 过程 期望值 计算 过程 M 过程 最大化 过程 
如果 我们 优化 的 目标 函数 是 一个 凸函数 那么 
一定 保证 能 得到 全局 最优 解 2 句法分析 句法结构 
分析 完全 句法分析 浅层 分析 依存关系 分析 分析方法 基于 规则 
基于 统计 3 词义 消 歧 七 处理 流程 1 
处理 流程 根据 要求 将 自然 语言 处理 成 query 
再 加以 形式化 建立 语言 模型 称之为 算法 和 计算 
模型 对 计算 模型 的 研究 是 一个 强 不适 
定 问题 因为 难以 满足 存在 性 唯一性 和 稳定性 
的 要求 所以 应当 加入 约束条件 使 在 一定 范围 
内 编程 适 定 问题 1 形式化 表示 为 数学 
形式 2 形式化 表示 为 算法 表现 为 模型 3 
编写程序 在 计算机 上 加以 实现 4 评测 不断 改进 
和 优化 以 满足 需求 数据 基本 处理 流程 获取数据 
数据 预处理 观察 数据 分词 去除 停用词 特征 工程 机器学习 
工程 2 指标 类别 TP true positives 真正 判断 为 
真的 正确率 TN True negatives 真 负 判断 为 假 
的 正确率 FP false positives 假 正 判断 为 正 
的 误报率 FN false negatives 假 负 判断 为 负 
的 漏报率 accuracy 准确率 反映 了 分类器 对 整个 样本 
的 判定 能力 也 就是说 能将 正 的 判定 为 
正 负 的 判定 为 负 A = TP + 
TN / TP + FN + FP + TN precision 
精准度 被 分类器 判定 正 例 中的 正 样本 的 
比重 P = TP / TP + FP recall 召回率 
被 预测 为 正 例 的 占 总的 正 例 
的 比重 R = TP / TP + FN F 
measure precision 和 recall 调和 均值 的 2倍 F = 
a2 + 1 P * R / a2 P + 
R 取 参数 a = 1 当 F1 较高 时 
说明 实验 结果 比较 理想 以 具体 场景 为例 假定 
某个 班级 有 男生 80人 女生 20人 共计 100人 目标 
是 找出 所有 女生 现在 某人 挑选 出了 50人 其中 
20人 是 女生 把 其余 30人 错 认为 是 女生 
请 你 来 评估 一下 他 的 工作 假定 目标 
女生 为 正 类 P 男 生为 负 类 N 
则 TP = 20 FP = 30 TN = 50 
FN = 0 A = 70/100 P = 20/50 R 
= 20/20 八 技术 应用 1 应用 方向 自然语言 生成 
/ 文本 分类 / 信息检索 / 信息 抽取 / 文字 
校对 / 问答 系统 / 机器翻译 / 自动 摘要 / 
文字 蕴涵 / 对话 系统 / 文本 挖掘 信息 抽取 
从 给定 文本 中 抽取 重要 信息 如 时间 地点 
人物 事件 原因 结果 数字 日期 货币 专有名词 等 涉及 
到 实体 识别 时间 抽取 因果关系 抽取 等 文本 挖掘 
包括 文本 聚 类 分类 信息 抽取 摘要 情感 分析 
以及 对 所 挖掘 信息 知识 的 可视化 和 交互式 
的 表达 界面 基于 统计 机器学习 机器翻译 输入 一种 语言 
输出 另外 一种 语言 根据 输入 媒介 不同 可以 分为 
文本 翻译 语音 翻译 手语 翻译 图形 翻译 等 机器翻译 
最早 基于 规则 后来 基于 统计 到 近年 基于 神经网络 
发展 至今 信息检索 对 大 规模 的 文档 进行 检索 
可 简单 对 文档 中 的 词汇 赋 之以 不同 
的 权重 来 建立 索引 也 可利用 123 的 技术 
来 建立 更 深层 的 索引 在 查询 的 时候 
对 输入 的 查询 表达式 比如 一个 检索 词 或者 
句子 进行 分析 然后 在 索引 里面 查找 匹配 的 
候选 文档 再 根据 一个 排序 机制 把 候选 文档 
排序 最后 输出 排序 得分 最高 的 文档 4 . 
3.6 问答 系统 对 一个 自然 语言 表达 的 问题 
由 问答 系统 给 出 一个 精准 的 答案 需要 
对 自然 语言 查询 语句 进行 某种 程度 的 语义分析 
包括 实体 链接 关系 识别 形成 逻辑 表达式 然后 道 
知识库 中 查找 可能 的 候选 答案 并 通过 一个 
排序 机制 找出 最佳答案 对话 系统 系统 通过 一 系列 
的 对话 跟 用户 聊天 回答 完成 某一 项 任务 
涉及 到 用户 意图 理解 通用 聊天 引擎 问答 引擎 
对话 管理 等 技术 为了 体现 上下文 关联 需要 具备 
多伦 对话 能力 同时 为了 体现 个性化 要 开发 用户 
画像 以及 基于 用户 画像 的 个性化 回复 2 相关 
项目 自动 生成 天气预报 自动 翻译 和 自动 问答 饭馆 
咨询服务 图像 到 语音 的 转换 残疾人 增强 交际 旅行 
咨询服务 语音 地理 导航 语音 资料 搜索 跨语言 信息检索 和 
翻译 作文 自动 评分 自动 阅读 家庭教师 个性化 市场 服务 
输入法 的 应用 语言 模型 和 自动机 自动 拼写 更正 
语言 模型 自动机 和 编辑 距离 机器翻译 中文分词 文本 对齐 
翻译 模型 语言 模型 Beam SearchQuery 意图 理解 模板 匹配 
分类器 Evernote 推荐 系统 篇章 表示 相似 度 计算 Local 
Sensitive Hashing 文本 分类 倒排索引 小黄鸡 关键词 匹配 倒排索引 英文 
写作 助手 语法分析 倒排索引 stem 找 词根 重大事件 监测 模板 
匹配 分类器 医疗 诊断书 自动 生成 规则 系统 深度 学习 
体育 报道 自动 生成 模板 填充 同义词 替换 文本 对齐 
法律 专利 生成 模板 匹配 分类器 聊天 互动 seq2seq 邮件 
自动 回复 seq2seq 模型 语义 意图 理解 行业 办公自动化 文体 
娱乐 行业 财经 法律 医疗 3 相关 会议 ACL EMNLP 
EACL NAACL4 其他 自然语言 处理 与 知识 图谱 的 区别 
自然语言 处理 的 研究 对象 是 计算机 和 人类 语言 
的 交互 其 任务 是 理解 人类 语言 并 将其 
转换 为 机器语言 在 目前 的 商业 场景 中 NLP 
技术 用于 分析 源自 邮件 音频 文件 网页 论坛 社交 
媒体 中 的 大量 数据 知识图谱 是 通过 将 应用 
数学 图形学 信息 可视化 技术 信息 科学 等 学科 的 
理论 与 方法 与 计量学 引文分析 共 现 分析 等 
方法 结合 并 利用 可视化 的 图谱 形象 地 展示 
学科 的 核心 结构 发展 历史 前沿 领域 以及 整体 
知识 架构 达到 多 学科 融合 目的 的 现代 理论 
它 把 复杂 的 知识 领域 通过 数据挖掘 信息处理 知识 
计量 和 图形 绘制 而 显示 出来 揭示 知识 领域 
的 动态 发展 规律 为 学科 研究 提供 切实 的 
有 价值 的 参考 自然语言 处理 与 机器 学习 的 
区别 自然语言 处理 都 需要 依赖 统计学 知识 而且 它 
和 机器学习 不同 机器学习 依靠 的 更多 是 严谨 的 
数学 知识 以及 推导 去 创造 一个 又 一个 机器学习 
算法 而 自然 语言 处理 是 把 那些 机器学习 大牛 
们 创造 出来 的 东西 当 工具 使用 所以 入门 
也 只是 需要 涉猎 而已 把 每个 模型 原理 看看 
就行了 数学 模型 的 重要性 1 一个 正确 的 数学 
模型 应当 在 形式 上 是 简单 的 2 一个 
正确 的 模型 一开始 可能 还 不如 一个 精细 雕琢 
过 的 错误 模型 来 的 准确 但是 如果 我们 
认定 大方向 是 对 的 就 应该 坚持 下去 3 
大量 准确 的 数据 对 研发 很重要 4 正确 的 
模型 也 可能 受 噪音 干扰 而 显得 不 准确 
这时 不 应该 用 一种 凑合 的 方法 来 弥补 
它 而要 找到 噪音 的 根源 这 也许 能 通往 
重大 的 发现 暑期 学习 自然语言 处理 笔记 一 自然语言 
处理 的 应用 自然语言 处理 natural language processing 即 NLP 
1 拼 写检查 纠错 关键词 搜索 垃圾邮件 识别 2 文本 
挖掘 文本 分类 3 机器翻译 4 自动 问答 客服 机器人 
5 复杂 对话 系统 微软 小冰 二 自然语言 处理 的 
模型 深度 学习 网络 应用于 NLP 在于 特征提取 的 优势 
深度 学习 中 的 强化 学习 是 无 监督 的 
模型 关于 语言 模型 机器翻译 语言 搭配 的 概率 拼写 
纠错 出现 一句话 的 概率 智能 问答 什么 是 语言 
模型 用来 计算 一句话 概率 的 模型 几个 词 都 
出现 的 联合 概率密度 在前 i 1 个 词 出现 
的 条件 下 第 i 个 词 出现 的 概率 
称之为 与之 相关性 当 词 非常多 的 时候 就 会 
造成 数据 过于 稀疏 参数 空间 太大 如果 i 很大 
参数 空间 过大 容易 过拟合 也 无法 实用 如何 简化 
问题 近似 上面 的 公式 效果 又 要求 比 独立性 
假 设好 n gram 模型 n gram 公式 如下 图 
所示 下图 公式 ② 叫做 三元 语法 trigram 3 gram 
马尔科夫 假设 Markov Assumption 下 一个 词 的 出现 仅 
依赖于 它 前面 的 一个 或 几个 词 这 对于 
联合 概率 链 规则 来说 其实 是 相对 粗糙 的 
简化 位置 离得 较远 而且 关系 比 较弱 的 词语 
就 简化 省略 掉了 概率 的 计算 过程 P = 
0.33 = 927/2533 参考 网址 http / / blog . 
csdn . net / yaoweijiao / article / details / 
52945186 大概 了解 语言 模型 的 计算 处理过程 三 词 
向量 计算机 理解 文本 的 方式 word vector 就是指 将 
单词 向 量化 将 某个 单词 用 特定 的 向量 
来 表示 注意 转化 的 是 一个 词 而非 一个 
字 如 假设 / 下 / 一个 / 词 / 
没有 / 出现 = 正确 转化 假 / 设 / 
下 / 一 / 个 / 词 / 没 / 
有/出/nr / 现 = 错误 转化 如何 构造 有 意义 
的 向量 希望 构造 的 词 向量 对于 意义 相近 
的 词 对应 的 向量 相关性 大些 构造 是 基于 
上下文 的 语境 构造 与 语言 拼写 规则 本身 无关 
如 下图 虽然 单词 是 不同 国家 的 语言 但是 
转化 为 的 向量 分布 想 非常 相似 关于 NLP 
的 神经 网络 模型 假设 一个 文本 神经网络 模型 交给 
这个 模型 根据 上下文 的 前 i 1 个 词 
1 2 输入 到 input layer 经过 网络 让 模型 
自己 找到 后 面的 第 i 个 词 3 是 
什么 这 就是 模型 的 任务 模型 的 架构 输入 
层 = 投影 层 = 隐 层 = 输出 层 
输入 层 每个 词 的 向量 维数 必须 一致 投影 
层 输入 层 的 多个 向量 连成 一串 变成 一个 
大 向量 输出 层 类似于 softmax 输出 的 是 一组 
概率值 如何 用 一个 向量 更好 地 表示 词 参考 
网址 http / / blog . csdn . net / 
u013362975 / article / details / 53319002 四 Hierarchical Softmax 
模型 更新 每个 输出 词 向量 在 训练 集上 每个 
词 的 分布 的 问题 是 非常 耗时耗力 的 为了 
解决 这个 问题 两个 方法 hierarchical softmax 和 negative sampling 
Hierarchical softmax 用 一个 二叉树 代表 词表 中的 所有 词 
这个词 是 叶子 节点 对于 每一个 叶子 节点 存在 着 
从根到/nr 叶子 的 唯一 路径 这个 路径 用来 估计 这个词 
的 概率 Negative Sampling 每次 只 更新 一个 输出 词 
目标 输出 词 应该 一直 在 样本 中 得到 更新 
并且 添加 一些 negative samples 进去 1 . CBOW 模型 
拿 一个 词语 的 上下文 作为 输入 来 预测 这个 
词语 本身 基于 上下文 预测 某 词 ContinuousBag Of Words 
Model 公式 参考 http / / blog . csdn . 
net / dream _ catcher _ 10 / article / 
details / 51361328 当 结果 分类 较多 比如 50 分类 
时 如何 解决 输 出问题 哈夫曼树 最优 二叉树 Huffman Tree 
路径 长度 是 指 一个 结点 到 另外 一个 结点 
之间 分 支数 带 权 路径 长度 是 指 每个 
分支 上有 权值 一个 结点 到 另外 一个 结点 所有 
路径 权值 总和 树/v 的/uj 带/v 权/n 路径/n 长度/ns 是从/v 
根结/a 点/m 出发/v 到/v 每一个/i 叶/nr 节点/n 的/uj 带/v 权/n 
路径/n 长度/ns 总和/n 哈夫曼树 的 建立 步骤 在 给定 的 
权值 中 选择 两棵 根 结点 权值 最小 的 作为 
左右 子树 构造 一棵 新的 二叉树 并将 新 二叉树 的 
根 结点 的 新 权值 再 替代 原来 两个 小 
权值 放 入原 权值 中 重新 挑选 两棵 根 结点 
权值 最小 不断 迭代 创建 左右 子树 哈夫曼 编码 参考 
网址 http / / blog . csdn . net / 
qq _ 19762007 / article / details / 50708573 逻辑 
回归 不属于 回归分析 而是 属于 分类 差异 主要 在于 变量 
不同 逻辑 回归 是 无 监督 学习 的 一个 重要 
算法 对 某些 数据 与 事物 的 归属 类别 及 
分到 某一 类别 的 概率 进行 评估 logistic 即 sigmoid 
具体 针对 的 是 二分 类 问题 而 softmax 解决 
的 是 多分 类 问题 sigmoid 函数 在 这里 将得 
分值 转化 为 概率 到 输出 层 则 利用 上下 
文词 向量 的 拼 接和 做为 输入 输出 的 是 
窗口 中心 位置 所有 词 出现 的 概率 利用 softmax 
求 中心词 概率 当 语料 较大 时 计算 变 的 
非常 耗时 于是 为了 解决 这个 问题 利用 哈夫曼树 对词 
表 进行 分类 用 一连串 的 二分 类 来 近似 
多 分类 哈夫曼 编码 一句话 就是 频率 越高 编码 越短 
哈夫曼 编码 怎么 用 的 先将 词表 的 词频 统 
计好 词频 高的/nr 放在 接近于 跟 根 节点 的 位置 
词频 低 的 放在 叶子 训练 不仅仅 针对 θ 的 
更新 还有 输入 的 词 向量 x 要 更新 损失 
函数 中有 2个 待 求 参数 θ x 在 训练 
CBOW 模型 时 词 向量 只是 个 副产品 确切 来说 
是 CBOW 模型 的 一个 参数 参考 网址 http / 
/ blog . csdn . net / qwe11002698 _ ling 
/ article / details / 53888284http / / blog . 
csdn . net / dream _ catcher _ 10 / 
article / details / 513613282 . skip gram 模型 用 
一个 词语 作为 输入 来 预测 它 周围 的 上下文 
基于 当前 词 预测 上下文 Continuous Skip gram Model 该 
模型 与 CBOW 类似 参考资料 http / / blog . 
csdn . net / qwe11002698 _ ling / article / 
details / 53888284http / / www . cnblogs . com 
/ tina smile / p / 5204619 . html 五 
Negative Sampling 模型 Negative Sampling 负 采样 已知 一个词 w 
它 的 上下文 是 context w 那么 词 w 就是 
一个 正 例 其他 词 就是 一个 负 例 但是 
负 例 样本 太多 了 我们 怎么 去 选取 呢 
在 语料库 C 中 各个 词 出现 的 频率 不 
一样 采样 的 时候 要求 高频词 选中 的 概率 较大 
而 低频词 选中 的 概率 较小 这 就是 一个 带 
权 采样 的 问题 随机 抽取 负 样本 随机数 生成 
满足 均匀分布 而 取词 概率 可不是 均匀分布 其 概率 应当 
随着 词频 大小 变化 将 词频 转换 为 线 段长度 
选取 负 例 样本 的 时候 取 线段 上 的 
一个 随机数 对应 到 相应 词频 区间 上 就 可以 
了 优化 求 偏 导 过程 类似于 Hierarchical Softmax 参考 
网址 http / / blog . csdn . net / 
chunyun0716 / article / details / 51722230http / / blog 
. csdn . net / suibianti / article / details 
/ 68483231 # 基于 negative sampling 的 模型 字符串 模糊 
匹配 是 NLP 自然语言 处理 中 一项 十分 重要 的 
研究 项目 今天 给 大家 介绍 的 就是 字符串 模糊 
匹配 文章 链接 NLP 教程 字符串 模糊 匹配 现如今 在 
更多 情况 下 我们 通过 传感器 和 字节 来 与 
机器 获得 交流 而 不是 依靠 交换 情感 那/r 如何/r 
让/v 超级/b 智能/n 机器/n 能够/v 和/c 人类/n 正常/d 交流/n 沟通/v 
呢/y 在 人工智能 背景 技术 下 自然语言 处理 NLP 技术 
被 越来越 多 的 人 看好 并 受到 重视 其中 
以 微软 小冰 为 代表 的 聊天 机器人 如今 却 
成了 网 红 迅速 刷 爆了 微信 和 朋友圈 一个 
17岁 纯情 少女 懂礼貌 有 素质 和会 作诗 众多 网友 
对 她 可是 情有独钟 下面 这幅 图 是 小冰 的 
一个 简介 那 什么 是 NLP NLP NaturalLanguage Processing 是 
人工智能 AI 的 一个 子 领域 自然 语言 是 人类 
智慧 的 结晶 自然语言 处理 是 人工智能 中 最为 困难 
的 问题 之一 它 是 能够 让 人类 与 智能 
机器 进行 沟通 交流 的 重要 技术 手段 因此 自然语言 
处理 的 研究 也 是 充满 魅力 和 挑战 的 
NLP 的 主要 范畴 有 哪些 NLP 作为 一种 人工智能 
方法 能够 处理 机器 和 人类 自然语言 之间 的 交互 
即 NLP 帮助 计算机 机器 以 各种 形式 使用 自然 
人类 语言 进行 交流 包括 进行 分析 理解 改变 或 
生成 自然语言 主要 涉及 的 范畴 如下 维基百科 •   
中文 自动 分词 • 词性 标注 • 句法分析 • 文本 
分类 • 信息 抽取 • 知识图谱 • 问答 系统 和 
自动 聊天 机器人 • 机器翻译 • 自动 摘要 为什么 要 
学 NLP 人工智能 的 发展 势不可挡 不可否认 当前 从事 互联网 
的 人们 已经 制造 出了 海量 的 数据 未来 还 
将 继续 持续 其中 包括 结构化 数据 半 结构化 和非/nr 
结构化 数据 笔者 发现 对于 结构化 数据 而言 在 大 
数据 云 计算技术 上下齐心 的 大力 整合 下 其 技术 
基本 趋向 成熟 和 稳定 而 半 结构化 非 结构化 
的 数据 因其 自身 的 复杂性 在/p 当前/t 和/c 未来/t 
更多/d 领域/n 应用/v 都/d 具有/v 很大/a 的/uj 困难/an 和/c 挑战/vn 
而 当前 市场 对于 NLP 技术 人才 的 需求 又 
非常 急切 而且 这种 状态 将 持续 5 10年 大 
部分 企业 需要 懂 NLP 技术 的 人 来 处理 
海量 非 结构 数据 对于 大多数 人 来说 学 完 
一门 技术 最终 的 目的 是 找到 自己 满意 的 
工作 包括 自己 感兴趣 的 领域 舒适 的 环境 和 
高薪 单纯 从 高薪 来看 不仅 意味着 很多 money 更是 
来 证明 自己 优秀 下面 是 BOSS 直 聘上 对 
NLP 技术 人员 的 待遇 需求 可以 看到 仅仅 是 
NLP 开发 工程师 当然 要 懂 算法 薪资 在 30 
60k 如何 入门 中文 NLP 作为 初学者 笔者 当初 也是 
走过 很多 弯路 其中 很 重要 的 一点 是 我们 
常常 遇到 这样 的 尴尬 网上 大 部分 自然语言 处理 
内容 都是/nr 英文 为基础 大多数 人 先是 学好 了 英语 
的 处理 回头 来 再处理 中文 却 发现 有 很大 
的 不同 这样 不仅 让 中文 自然语言 处理 学习者 走了 
弯路 也 浪费 了 大量 时间 和 精力 中文 的 
处理 比 英文 复杂 的 多 网上 中文 相关 资料 
少之又少 国内 纯中文 自然语言 处理 书籍 只有 理论 方面 的 
却在 实战 方面 比较 空缺 这 让 中文 自然语言 处理 
的 学习者 感到 举步维艰 很难 下笔 对于 这样 的 难点 
是不是 认为 中文 NLP 就很 难学 呢 答案 是 非也 
相反 笔者 认为 入门 中文 NLP 最快 的 捷径 就是 
以 小 数量 的 实例 边学边 实战 https / / 
blog . csdn . net / qq _ 36330643 / 
article / details / 80772390 自然语言 处理 完整 流程 第一步 
获取 语料 1 已有 语料 2 网上 下载 抓取 语料 
第二步 语料 预处理 1 语料 清洗 2 分词 3 词性 
标注 4 去 停用词 三 特征 工程 1 词 袋 
模型 BoW 2 词 向量 第四步 特征选择 第五步 模型 训练 
1 模型 2 注意事项 1 过拟合 2 欠 拟合 3 
对于 神经网络 注意 梯度 消 失和 梯度 爆炸 问题 第一步 
获取 语料 语料 即 语言 材料 是 构成 语料库 的 
基本 单元 所以 人们 简单 地 用 文本 作为 替代 
并把 文本 中的 上下文 关系 作为 现实 世界 中 语言 
的 上下文 关系 的 替代品 我们 把 一个 文本 集合 
称为 语料库 Corpus 当 有几个 这样 的 文本 集合 的 
时候 我们 称之为 语料库 集合 Corpora 定义 来源 百度 百科 
按 语料 来源 我们 将 语料 分为 以下 两种 1 
已有 语料 纸质 或者 电子文本 资料 = = 电子化 = 
= 语料库 2 网上 下载 抓取 语料 国内外 标准 开放 
数据集 比如 国内 的 中文 汉语 有 搜狗 语料 人民日报 
语料 或 通过 爬虫 第二步 语料 预处理 语料 预处理 大概会 
占到 整个 50% 70% 的 工作量 基本 过程 数据 清洗 
= = 分词 = = 词性 标注 = = 去 
停 词 1 语料 清洗 语料 清洗 在 语料 中找到 
感兴趣 的 内容 将 不感兴趣 视为 噪音 的 内容 清洗 
删除 包括 对于 原始 文本 提取 标题 摘要 正文 等 
信息 对于 爬虫 去除 广告 标签 HTML JS 等 代码 
和 注释 常见 数据 清洗 方式 人工 去 重 对齐 
删除 和 标注 等 或 规则 提取 内容 正则表达式 匹配 
根据 词性 和 命名 实体 提取 编写 脚本 或 代码 
批处理 等 2 分词 分词 将 短 文本 和长/nr 文本处理 
为最 小 单位 粒度 是 词 或 词语 的 过程 
常见 方法 基于 字符串 匹配 的 分词 方法 基于 理解 
的 分词 方法 基于 统计 的 分词 方法 和 基于 
规则 的 分词 方法 其中 每 种方法 下面 对应 许多 
具体 的 方法 难点 歧义 识别 和 新词 识别 eg 
羽毛球拍 卖完 了 这个 可以 切 分成 羽毛 球拍 卖 
完 了 也可 切 分成 羽毛球 拍卖 完 了 = 
= 上下文 信息 3 词性 标注 词性 标注 对 每个 
词 或 词语 打 词类 标签 是 一个 经典 的 
序列 标注 问题 eg 形容词 动词 名词 等 有助于 在 
后面 的 处理 中 融入 更多 有用 的 语言 信息 
词性 标注 不 是非 必需 的 比如 常见 的 文本 
分类 就 不用 关心 词 性问题 但是 类似 情感 分析 
知识 推理 却是 需要 的 下图 是 常见 的 中文 
词性 整理 常见 方法 基于 规则 和 基于 统计 的 
方法 基于 统计 的 方法 基于 最大熵 的 词性 标注 
基于 统计 最大 概率 输出 词性 和 基于 HMM 的 
词性 标注 4 去 停用词 停用词 对 文本 特征 没有 
任何 贡献 的 字词 eg 标点符号 语气 人称 等 注意 
根据 具体 场景 决定 eg 在 情感 分析 中 语气词 
感叹号 是 应该 保留 的 因为 他们 对 表示 语气 
程度 感情/n 色彩/n 有/v 一定/d 的/uj 贡献/n 和/c 意义/n 三 
特征/n 工程/n 如何/r 把/p 分词/n 之后/f 的/uj 字/n 和/c 词语/n 
表示/v 成/n 计算机/n 能够/v 计算/v 的/uj 类型/n 思路 中文分词 的 
字符串 = = 向量 两种 常用 表示 模型 词 袋 
模型 BoW 词 向量 1 词 袋 模型 BoW 词 
袋 模型 Bag of Word BOW 不考虑 词语 原本在 句子 
中 的 顺序 直接 将 每 一个 词语 或者 符号 
统一 放置 在 一个 集合 如 list 然后 按照 计数 
的 方式 对 出现 的 次数 进行 统计 统计 词频 
这 只是 最 基本 的 方式 TF IDF 是 词 
袋 模型 的 一个 经典 用法 2 词 向 量词 
向量 将 字 词语 转换 为 向量 矩阵 的 计算 
模型 常用 的 词 表示 方法 One Hot 把 每个 
词 表示 为 一个 很长 的 向量 这个 向量 的 
维度 是 词表 大小 其中 绝大多数 元素 为 0 只有 
一个 维度 的 值 为 1 这个 维度 就 代表 
了 当前 的 词 eg 0 0 0 0 0 
0 0 0 1 0 0 0 0 . . 
. 0 Word2Vec 其 主要 包含 两 个 模型 跳 
字 模型 Skip Gram 和 连续 词 袋 模型 Continuous 
Bag of Words 简称 CBOW 以及 两 种 高效 训练 
的 方法 负 采样 Negative Sampling 和 层序 Softmax Hierarchical 
Softmax 值得一提的是 Word2Vec 词 向量 可以 较好 地 表达 不同 
词 之间 的 相似 和 类比 关系 D o c 
2 V e c W o r d R a 
n k F a s t T e x t 
第四步 特征选择 关键 如何 构造 好 的 特征向量 = = 
要 选择 合适 的 表达 能力 强的/nr 特征 常见 的 
特征 选择 方法 DF MI IG CHI WLLR WFO 六种 
第五步 模型 训练 1 模型 对于 不同 的 应用 需求 
我们/r 使用/v 不同/a 的/uj 模型/n 传统/n 的/uj 有/v 监督/vn 和无/nr 
监督/vn 等/u 机器学习/i 模型/n KNN SVM Naive Bayes 决策树 GBDT 
K means 等 模型 深度 学习 模型 CNN RNN LSTM 
Seq2Seq FastText TextCNN 等 2 注意事项 1 过拟合 过拟合 模型 
学习 能力 太强 以至于 把 噪声 数据 的 特征 也 
学习 到了 导致 模型 泛化 能力 下降 在 训练 集上 
表现 很好 但是 在 测试 集上 表现 很差 常见 的 
解决 方法 有 增大 数据 的 训练 量 增加 正则化 
项 如 L1 正则 和 L2 正则 特征 选取 不合理 
人工 筛选 特征 和 使用 特征选择 算法 采用 Dropout 方法 
等 2 欠 拟合 欠 拟合 就是 模型 不 能够 
很好 地 拟合 数据 表现 在 模型 过于 简单 常见 
的 解决 方法 有 添加 其他 特征 项 增加 模型 
复杂度 比如 神经网络 加 更多 的 层 线性 模型 通过 
添加 多项式 使 模型 泛化 能力 更强 减少 正则化 参数 
正则化 的 目的 是 用来 防止 过拟合 的 但是 现在 
模型 出现 了 欠 拟合 则 需要 减少 正则化 参数 
3 对于 神经网络 注意 梯度 消 失和 梯度 爆炸 问题 
大 部分 内容 摘抄 自 知乎 相关 问题 作者 微软 
亚洲 研究院 作者 陈见耸/nr 作者 刘知远 背景 知识 自然语言 处理 
是 一门 交叉 的 学科 概率论 需要 了解 概率 条件概率 
贝叶斯 法则 二项分布 期望 方差 最大 似 然 估计 梯度 
下降 等等 统计学 建模 数据 稀疏 问题 回退 方法 等 
机器学习 分类 感知器 支持 向量 机 语言学 构词 词类 句法 
语义 语料库 和 知识库 等等 建议 1 如何 在 NLP 
领域 快速 学会 第一个 技能 我 的 建议 是 找到 
一个 开 源 项目 比如 机器翻译 或者 深度 学习 的 
项目 理解 开源 项目 的 任务 编译 通过 该 项目 
发布 的 示范 程序 得到 与 项目 示范 程序 一致 
的 结果 然后 再 深入 理解 开源 项目 示范 程序 
的 算法 自己 编程 实现 一下 这个 示范 程序 的 
算法 再 按照 项目 提供 的 标准 测试 集 测试 
自己 实现 的 程序 如果 输出 的 结果 与 项目 
中 出现 的 结果 不 一致 就要 仔细 查验 自己 
的 程序 反复 修改 直到 结果 与 示范 程序 基本一致 
如果 还是 不行 就 大胆 给 项目 的 作者 写信 
请教 在此 基础 上 再 看看 自己 能否 进一步 完善 
算法 或者 实现 取 得比 示范 程序 更好 的 结果 
项目 不要 太大 以 小型 的 算法 模块 为佳 这样 
便于 独立 实现 像 文本 领域 的 文本 分类 分词 
等 项目 就是 比较 合适 的 项目 运行 程序 得到 
项目 所 声称 的 结果 然后 看懂 程序 这 期间 
一般 需要 阅读 程序实现 所 参考 的 文献 最后 自己 
尝试 独立 实现 该 算法 得到 与 示例 程序 相同 
的 结果 再 进一步 的 可以 调试 参数 了解 各 
参数 对 效果 的 影响 看 是否 能 得到 性能 
更好 的 参数 组合 这一 阶段 主要 是 学习 快速 
上手 一个 项目 从而 对 自然 语言 处理 的 项目 
有 比较 感性 的 认识 大体 了解 自然语言 处理 算法 
的 原理 实现 流程 等 当 我们 对 自然 语言 
处理 项目 有了/nr 一定 的 认识 之后 接下来 就要 深入 
进去 任何/r 自然语言/l 处理/v 应用/v 都/d 包含/v 算法/n 和/c 所要/b 
解决/v 的/uj 问题/n 两/m 方面/n 要想 深入 进去 就 需要 
从这 两 方面 进行 着手 建议 2 如何 选择 第一 
个 好 题目 工程型 研究生 选题 很多 都是/nr 老师 给定 
的 需要 采取 比较 实用 的 方法 扎扎实实 地 动手 
实现 可能 不 需要 多少 理论 创新 但是 需要 较强 
的 实现 能力 和 综合 创新 能力 而 学术 型 
研究生 需要 取得 一流 的 研究 成果 因此 选题 需要 
有 一定 的 创新 我 这里 给 出 如下 的 
几点 建议 先 找到 自己 喜欢 的 研究 领域 你 
找到 一本 最近 的 ACL 会议 论文集 从中 找到 一个 
你 比较 喜欢 的 领域 在 选题 的 时候 多 
注意 选择 蓝海 的 领域 这 是因为 蓝海 的 领域 
相对 比较 新 容易 出 成果 充分 调研 这个 领域 
目前 的 发展 状况 包括 如下 几 个 方面 的 
调研 方法 方面 是否/v 有/v 一套/m 比较/d 清晰/a 的/uj 数学/n 
体系/n 和/c 机器学习/i 体系/n 数据 方面 有/v 没有/v 一个/m 大家/n 
公认/v 的/uj 标准/n 训练/vn 集/q 和/c 测试/vn 集/q 研究 团队 
是否/v 有/v 著名/a 团队/n 和/c 人士/n 参加/v 如果 以 上 
几个 方面 的 调研 结论 不是 太 清晰 作为 初学者 
可能 不要 轻易 进入 在 确认 进入 一个 领域 之后 
按照 建议 一 所述 需要 找到 本 领域 的 开源 
项目 或者 工具 仔细 研究 一遍 现有 的 主要 流派 
和 方法 先 入门 反复 阅读 本 领域 最新 发表 
的 文章 多 阅读 本 领域 牛人 发表 的 文章 
在 深入 了解 已有 工作 的 基础 上 探讨 还有 
没有 一些 地方 可以 推翻 改进 综合 迁移 注意 做 
实验 的 时候 不要 贪多 每次 实验 只需要 验证 一个 
想法 每次 实验 之后 必须 要 进行 分析 存在 的 
错误 找出 原因 对 成功 的 实验 进一步 探讨 如何 
改进 算法 注意 实验 数据 必须 是 业界 公认 的 
数据 与 已有 的 算法 进行 比较 体会 能够 得出 
比较 一般性 的 结论 如果 有 则 去 写 一篇 
文章 否则 应该 换 一个 新 的 选题 建议 3 
如何写 出 第一 篇 论文 接 上 一个 问题 如果 
想法 不错 且 被 实验 所 证明 就可 开始 写 
第一 篇 论文 了 确定 论文 的 题目 在 定 
题目 的 时候 一般 不要 系统 研究 与 实践 要 
避免 太长 的 题目 因为 不好 体现 要点 题目 要 
具体 有 深度 突出 算法 写 论文 摘要 要 突出 
本文 针对 什么 重要 问题 提出 了 什么 方法 跟 
已有 工作 相比 具 有 什么 优势 实验 结果 表明 
达到 了 什么 水准 解决 了 什么 问题 写 引言 
首先 讲出 本 项 工作 的 背景 这个 问题 的 
定义 它 具有 什么 重要性 然后 介绍 对 这个 问题 
现有 的 方法 是 什么 有 什么 优点 但是 注意 
但是 现有 的 方法 仍然 有 很多 缺陷 或者 挑战 
比如 注意 比如 有 什么 问题 本文 针对 这个 问题 
受 什么 方法 谁 的 工作 之 启发 提出 了 
什么 新的 方法 并 做了 如下 几 个 方面 的 
研究 然后 对 每个 方面 分门别类 加以 叙述 最后 说明 
实验 的 结论 再说 本文 有 几条 贡献 一般 写 
三条 足矣 然后 说说 文章 的 章节 组织 以及 本文 
的 重点 有的 时候 东西 太多 篇幅 有限 只能 介绍 
最 重要 的 部分 不 需要 面面俱到 相关 工作 对 
相关 工作 做 一个 梳理 按照 流派 划分 对 主要 
的 最多 三个 流派 做 一个 简单 介绍 介绍 其 
原理 然后 说明 其 局限性 然后 可 设立 两 个 
章节 介绍 自己 的 工作 第一 个 章节 是 算法 
描述 包括 问题 定义 数学 符号 算法 描述 文章 的 
主要 公式 基本 都在/nr 这里 有时候 要 给出 简明 的 
推导 过程 如果 借鉴 了 别人 的 理论 和 算法 
要给 出 清晰 的 引文 信息 在此 基础 上 由于 
一般 是 基于 机器学习 或者 深度 学习 的 方法 要 
介绍 你 的 模型 训练 方法 和 解码 方法 第二章 
就是 实验 环节 一般 要 给出 实验 的 目的 要 
检验 什么 实验 的 方法 数据 从 哪里 来 多 
大 规模 最好 数据 是 用 公开 评 测数据 便于 
别人 重复 你 的 工作 然后 对 每个 实验 给出 
所需 的 技术 参数 并 报告 实验 结果 同时 为了 
与 已有 工作 比较 需要 引用 已有 工作 的 结果 
必要 的 时候 需要 重现 重要 的 工作 并 报告 
结果 用 实验 数据 说话 说明 你 比 人家 的 
方法 要好 要对 实验 结果 好好 分析 你 的 工作 
与 别人 的 工作 的 不同 及 各自 利弊 并 
说明 其 原因 对于 目前 尚 不太好 的 地方 要 
分析 问题 之 所在 并将 其 列为 未来 的 工作 
结论 对 本文 的 贡献 再一次 总结 既要 从 理论 
方法 上 加以 总结 和 提炼 也要 说明 在 实验上 
的 贡献 和 结论 所做 的 结论 要让 读者 感到 
信服 同时 指出 未来 的 研究 方向 参考文献 给出 所有 
重要 相关 工作 的 论文 记住 漏掉 了 一篇 重要 
的 参考 文献 或者 牛人 的 工作 基本上 就 没有 
被 录取 的 希望 了 写完 第一稿 然后 就是 再改 
三遍 把 文章 交给 同一个 项目组 的 人士 请 他们 
从 算法 新颖 度 创新性/n 和/c 实验/vn 规模/n 和/c 结论/n 
方面/n 以 挑剔 的 眼光 审核 你 的 文章 自己 
针对 薄弱环节 进一步 改进 重点 加强 算法 深度 和 工作 
创新 性 然后 请 不同 项目组 的 人士 审阅 如果 
他们 看 不明白 说明 文章 的 可读性 不够 你 需要 
修改 篇章 结构 进行 文字 润色 增加 文章 可读性 如 
投 ACL 等 国际 会议 最好 再请 英文 专业 或者 
母语 人士 提炼 文字 建议 4 对 问题 进行 深入 
认识 对 问题 的 深入 认识 通常 来源 于 两个 
方面 一是 阅读 当前 领域 的 文献 尤其 是 综述 
性 的 文献 理解 当前 领域 所 面临 的 主要 
问题 已有 的 解决 方案 有 哪些 有待 解决 的 
问题 有 哪些 这里 值得一提的是 博士生 论文 的 相关 文献 
介绍 部分 通常会 对本 问题 做 比较 详细 的 介绍 
也 是 比较 好 的 综述 类 材料 除了 从 
文献 中 获取 对 问题 的 认识 外 另一种 对 
问题 进行 深入 认识 的 直观 方法 就是 对 算法 
得出 的 结果 进行 bad case 分析 总结 提炼 出 
一些 共性 的 问题 对 bad case 进行 分析 还有 
一个 好处 可以 帮助 我们 了解 哪些 问题 是 主要 
问题 哪些 问题 是 次要 问题 从而 可以 帮助 我们 
建立 问题 优先级 如果 有 具体 任务 的 真实 数据 
一定 要 在 真实 数据 上 进行 测试 这 是因为 
即使 是 相同 的 算法 在 不同 的 数据 集上 
所 得到 的 结果 也 可能 相差 很大 建议 5 
对 算法 进行 深入 理解 除了 具体 的 问题 分析 
对 算法 的 理解 是 学习 人工智能 必须 要 过 
的 关 经过 这么 多年 的 发展 机器学习 模式识别 的 
算法 已经 多如牛毛 幸运 的 是 这 方面 已经 有 
不少 好 的 书籍 可供参考 这里 推荐 华为 李航 的 
蓝 宝书 统计 学习 方法 和 周志华 的 西瓜 书 
机器学习 这 两本 都是 国内 顶级 的 机器学习 专家 撰写 
的 书籍 思路清晰 行文流畅 样例 丰富 如果 觉得 教科书 稍感 
乏味 那我/nr 推荐 吴军 的 数学 之美 这是 一本 入门级 
的 科普 读物 作者 以 生动 有趣 的 方式 深入浅出 
的 讲解 了 很多 人工智能 领域 的 算法 相信 你 
一定会 有 兴趣 国外 的 书籍 Pattern Recognition and Machine 
Learning 主要 从 概率 的 角度 解释 机器 学习 的 
各种 算法 也是 不可多得 的 入门 教材 如果 要 了解 
最新 的 深度 学习 的 相关 算法 可以 阅读 被 
誉为 深度 学习 三架 马车 之一 Bengio 所著 的 Deep 
Learning 在 学习 教材 时 对于 应用 工程师 来说 重要 
的 是 理解 算法 的 原理 从而 掌握 什么 数据 
情况 下 适合 什么样 的 数据 以及 参数 的 意义 
是 什么 建议 6 深入 到 领域 前沿 自然语言 处理 
领域 一直 处在 快速 的 发展 变化 当中 不管 是 
综述 类 文章 还 是 书籍 都 不能 反映 当前 
领域 的 最新 进展 如果 要 进一步 的 了解 领域 
前沿 那就/nr 需要 关注 国际 顶级 会议 上 的 最新 
论文 了 下面 是 各个 领域 的 一些 顶级 会议 
这里 值得一提的是 和 其他 人工智能 领域 类似 自然语言 处理 领域 
最 主要 的 学术 交流 方式 就 会议 论文 这 
和 其他 领域 比如 数学 化学 物理 等 传统 领域 
都 不太 一样 这些 领域 通常 都以 期刊论文 作为 最 
主要 的 交流 方式 但是 期刊论文 审稿 周期 太长 好 
的 期刊 通常 都要 两三年 的 时间 才能 发表 这 
完全 满足 不了 日新月异 的 人工智能 领域 的 发展 需求 
因此 大家/n 都会/nr 倾向/v 于在/nr 审稿/v 周期/t 更短/i 的/uj 会议/n 
上/f 尽快/d 发表/v 自己/r 的/uj 论文/nz 这里 列举 了 国际 
和 国内 文本 领域 的 一些 会议 以及 官网 大家 
可以 自行 查看 国际上 的 文本 领域 会议 ACL http 
/ / acl2017 . org / 加拿大 温哥华 7.30 8 
. 4EMNLP http / / emnlp2017 . net / 丹麦 
哥本哈根 9.7 9 . 11COLING 没找到 2017年 的 国内 会议 
CCKS http / / www . ccks2017 . com / 
index . php / att / 成都 8月 26 8月 
29SMP http / / www . cips smp . org 
/ smp2017 / 北京 9.14 9 . 17CCL http / 
/ www . cips cl . org 8080 / CCL2017 
/ home . html 南京 10.13 10 . 15NLPCC http 
/ / tcci . ccf . org . cn / 
conference / 2017 / 大连 11.8 11 . 12NCMMSC http 
/ / www . ncmmsc2017 . org / index . 
html 连云港 11.11 － 11.13 像 paperweekly 机器学习 研究会 深度 
学习 大讲堂 等 微信 公众 号 也 经常 会 探讨 
一些 自然 语言 处理 的 最新 论文 是 不错 的 
中文 资料 建议 7 当然 工欲善其事 必先利其器 我们 要 做好 
自然语言 处理 的 项目 还 需要 熟练掌握 至少 一门 工具 
当前 深度 学习 相关 的 工具 已经 比较 多了 比如 
tensorflow mxnet caffe theano cntk 等 这里 向 大家 推荐 
tensorflow 自从 google 推出 之后 tensorflow 几乎 成为 最 流行 
的 深度 学习 工具 究其原因 除了 google 的 大力 宣传 
之外 tensorflow 秉承 了 google 开源 项目 的 一贯 风格 
社区 力量 比较 活跃 目前 github 上有 相当多 数量 的 
以 tensorflow 为 工具 的 项目 这 对于 开发 者 
来说 是 相当大 的 资源 以上 就是 对于 没有 自然语言 
处理 项目 经验 的 人 来说 如何 学习 自然语言 处理 
的 一些 经验 希望 对 大家 能 有所 帮助 其中 
文献 部分 1 . 国际 学术 组织 学术 会议 与 
学术 论文 自然语言 处理 natural language processing NLP 在 很大 
程度 上 与 计算 语言学 computational linguistics CL 重合 与 
其他 计算机 学科 类似 NLP / CL 有 一个 属于 
自己 的 最 权威 的 国际 专业 学会 叫做 The 
Association for Computational Linguistics ACL URL ACL Home Page 这个 
协会 主办 了 NLP / CL 领域 最 权威 的 
国际 会议 即 ACL 年会 ACL 学会 还会在 北美 和 
欧洲 召开 分 年会 分别 称为 NAACL 和 EACL 除此之外 
ACL 学会 下设 多个 特殊 兴趣小组 special interest groups SIGs 
聚集 了 NLP / CL 不同 子 领域 的 学者 
性质 类似 一个 大学 校园 的 兴趣 社团 其中 比较 
有名 的 诸如 SIGDAT Linguistic data and corpus based approaches 
to NLP SIGNLL Natural Language Learning 等 这些 SIGs 也会 
召开 一些 国际 学术 会议 其中 比较 有名 的 就是 
SIGDAT 组织 的 EMNLP Conference on Empirical Methods on Natural 
Language Processing 和 SIGNLL 组织 的 CoNLL Conference on Natural 
Language Learning 此外 还有 一个 International Committee on Computational Linguistics 
的 老牌 NLP / CL 学术 组织 它 每 两年 
组织 一个 称为 International Conference on Computational Linguistics COLING 的 
国际 会议 也是 NLP / CL 的 重要 学术 会议 
NLP / CL 的 主要 学术 论文 就 分布 在 
这些 会议 上 作为 NLP / CL 领域 的 学者 
最大 的 幸福 在于 ACL 学会 网站 建立 了 称作 
ACL Anthology 的 页面 URL ACL Anthology 支持 该 领域 
绝大部分 国际 学术 会议 论文 的 免费 下载 甚至 包含 
了 其他 组织 主办 的 学术 会议 例如 COLING IJCNLP 
等 并 支持 基于 Google 的 全文 检索 功能 可谓 
一站 在手 NLP 论文 我 有 由于 这个 论文 集合 
非常 庞大 并且 可以 开放 获取 很多 学者 也 基于 
它 开展 研究 提供 了 更 丰富 的 检索 支持 
具体 入口 可以 参考 ACL Anthology 页面 上方 搜索框 右侧 
的 不同 检索 按钮 与 大部分 计算机 学科 类似 由于 
技术 发展 迅速 NLP / CL 领域 更 重视 发表 
学术 会议 论文 原因 是 发表 周期短 并 可以 通过 
会议 进行 交流 当然 NLP / CL 也有 自己 的 
旗舰 学术期刊 发表 过 很多 经典 学术论文 那 就是 Computational 
Linguistics URL MIT Press Journals 该 期刊 每期 只有 几 
篇文章 平均/a 质量/n 高于/nr 会议/n 论文/nz 时间 允许 的话 值得 
及时 追踪 此外 ACL 学会 为了 提高 学术 影响力 也 
刚刚 创办 了 Transactions of ACL TACL URL Transactions of 
the Association for Computational Linguistics ISSN 2307 387X 值得 关注 
值得一提的是 这 两份 期刊 也都 是 开放 获取 的 此外 
也 有 一些 与 NLP / CL 有关 的 期刊 
如 ACM Transactions on Speech and Language Processing ACM Transactions 
on Asian Language Information Processing Journal of Quantitative Linguistics 等等 
根据 Google Scholar Metrics 2013年 对 NLP / CL 学术 
期刊 和 会议 的 评价 ACL EMNLP NAACL COLING LREC 
Computational Linguistics 位于 前 5位 基本 反映 了 本 领域 
学者 的 关注 程度 NLP / CL 作为 交叉学科 其 
相关 领域 也 值得 关注 主要 包括 以下 几个 方面 
1 信息检索 和 数据挖掘 领域 相关 学术 会议 主要 由 
美国 计算机 学会 ACM 主办 包括 SIGIR WWW WSDM 等 
2 人工智能 领域 相关 学术 会议 主要 包括 AAAI 和 
IJCAI 等 相关 学术 期刊 主要 包括 Artificial Intelligence 和 
Journal of AI Research 3 机器学习 领域 相关 学术 会议 
主要 包括 ICML NIPS AISTATS UAI 等 相关 学术 期刊 
主要 包括 Journal of Machine Learning Research JMLR 和 Machine 
Learning ML 等 例如 最近 兴起 的 knowledge graph 研究 
论文 就有/i 相当/d 一/m 部分/n 发表/v 在/p 人工/n 智能/n 和/c 
信息检索/n 领域/n 的/uj 会议/n 和/c 期刊/n 上/f 实际上 国内 计算机 
学会 CCF 制定 了 中国 计算机 学会 推荐 国际 学术 
会议 和 期刊目录 CCF 推荐 排名 通过 这个 列表 可以 
迅速 了解 每个 领域 的 主要 期刊 与 学术 会议 
最后 值得一提的是 美国 Hal Daum é III 维护 了 一个 
natural language processing 的 博客 natural language processing blog 经常 
评论 最新 学术 动态 值得 关注 我 经常 看 他 
关于 ACL NAACL 等 学术 会议 的 参会 感想 和对/nr 
论文 的 点评 很 有启发 另外 ACL 学会 维护 了 
一个 Wiki 页面 ACL Wiki 包含 了 大量 NLP / 
CL 的 相关 信息 如 著名 研究 机构 历届 会议录 
用率 等等 都是 居家 必备 之 良 品 值得 深挖 
2 . 国内 学术 组织 学术 会议 与 学术 论文 
与 国际 上 相似 国内 也 有一个 与 NLP / 
CL 相关 的 学会 叫做 中国 中文信息 学会 URL 中国 
中文信息 学会 通过 学会 的 理事 名单 中国 中文信息 学会 
基本 可以 了解 国内 从事 NLP / CL 的 主要 
单位 和 学者 学会 每年 组织 很多 学术会议 例如 全国 
计算 语言学 学术会议 CCL 全国 青年 计算 语言学 研讨会 YCCL 
全国 信息检索 学术会议 CCIR 全国 机器翻译 研讨会 CWMT 等等 是 
国内 NLP / CL 学者 进行 学术 交流 的 重要 
平台 尤其 值得一提的是 全国 青年 计算 语言学 研讨会 是 专门 
面向 国内 NLP / CL 研究生 的 学术 会议 从 
组织 到 审稿 都由 该 领域 研究生 担任 非常 有 
特色 也是 NLP / CL 同学们 学术交流 快速 成长 的 
好去处 值得一提的是 2010年 在 北京 召开 的 COLING 以及 2015年 
即将 在 北京 召开 的 ACL 学会 都是/nr 主要 承办者 
这也 一定 程度 上 反映 了 学会 在 国内 NLP 
/ CL 领域 的 重要 地位 此外 计算机 学会 中文 
信息 技术 专委会 组织 的 自然 语言 处理 与 中文 
计算 会议 NLP & CC 也是 最近 崛起 的 重要 
学术 会议 中文信息 学会 主编 了 一份 历史 悠久 的 
中文信息 学报 是 国内 该 领域 的 重要 学术 期刊 
发表 过 很多 篇 重量级 论文 此外 国内 著名 的 
计算机 学报 软件 学报 等 期刊 上 也 经常 有 
NLP / CL 论文 发表 值得 关注 过去 几年 在 
水木 社区 BBS 上 开设 的 AI NLP 版面 曾经 
是 国内 NLP / CL 领域 在线 交流 讨论 的 
重要 平台 这几年 随着 社会 媒体 的 发展 越来越 多 
学者 转战 新浪 微博 有 浓厚 的 交流 氛围 如何 
找到 这些 学者 呢 一个 简单 的 方法 就是 在 
新浪 微博 搜索 的 找人 功能 中 检索 自然语言 处理 
计算 语言学 信息检索 机器学习 等 字样 马上 就能 跟 过去 
只 在 论文 中 看到 名字 的 老师 同学们 近距离 
交流 了 还有 一种 办法 清华大学 梁斌 开发 的 微博 
寻人 系统 清华大学 信息检索 组 可以 检索 每个 领域 的 
有 影响力 人士 因此 也 可以 用来 寻找 NLP / 
CL 领域 的 重要 学者 值得一提的是 很多 在 国外 任教 
的 老师 和 求学 的 同学 也 活跃 在 新浪 
微博 上 例如 王威廉 Sina Visitor System 李沐/nr Sina Visitor 
System 等 经常 爆料 业内 新闻 值得 关注 还有 国内 
NLP / CL 的 著名 博客 是 52nlp 我 爱 
自然 语言 处理 影响力 比较 大 总之 学术研究 既 需要 
苦练内功 也 需要 与 人 交流 所谓 言者无意 听者有心 也许 
其他人 的 一句话 就能 点醒 你 苦思 良久 的 问题 
无疑 博客 微博 等 提供 了 很好 的 交流 平台 
当然 也 注意 不要 沉迷 哦 3 . 如何 快速 
了解 某 个 领域 研究 进展 最后 简单 说 一下 
快速 了解 某 领域 研究 进展 的 经验 你 会 
发现 搜索引擎 是 查阅 文献 的 重要 工具 尤其 是 
谷歌 提供 的 Google Scholar 由于 其 庞大 的 索引 
量 将 是 我们 披荆斩棘 的 利器 当 需要 了解 
某 个 领域 如果 能 找到 一篇 该 领域 的 
最新 研究 综述 就 省劲 多了 最 方便 的 方法 
还是 在 Google Scholar 中 搜索 领域 名称 + survey 
/ review / tutorial / 综述 来 查找 也 有 
一些 出版社 专门 出版 各 领域 的 综述 文章 例如 
NOW Publisher 出版 的 Foundations and Trends 系列 Morgan & 
Claypool Publisher 出版 的 Synthesis Lectures on Human Language Technologies 
系列 等 它们 发表 了 很多 热门 方向 的 综述 
如 文档 摘要 情感 分析 和 意见 挖掘 学习 排序 
语言 模型 等 如果 方向 太 新 还 没有 相关 
综述 一般 还 可以 查找 该 方向 发表 的 最新 
论文 阅读 它们 的 相关 工作 章节 顺着 列出 的 
参考 文献 就 基本 能够 了解 相关 研究 脉络 了 
当然 还有 很多 其他 办法 例如 去 http / / 
videolectures . net 上看 著名 学者 在 各大 学术 会议 
或 暑期学校 上 做 的 tutorial 报告 去 直接 咨询 
这个 领域 的 研究 者 等等 博主 github https / 
/ github . com / MichaelBeechan 博主 CSDN https / 
/ blog . csdn . net / u011344545 = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = 概念 篇 https / / blog . csdn 
. net / u011344545 / article / details / 89525801 
技术篇 https / / blog . csdn . net / 
u011344545 / article / details / 89526149 人才篇 https / 
/ blog . csdn . net / u011344545 / article 
/ details / 89556941 应用 篇 https / / blog 
. csdn . net / u011344545 / article / details 
/ 89574915 下载 链接 https / / download . csdn 
. net / download / u011344545 / 11147085 = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = 清华 AMiner 团队 AMiner . org 自然语言 处理 
的 研究 领域 极为 广泛 各种 分类 方式 层出不穷 各有 
其 合理性 我们 按照 中国 中文信息 学会 2016 年 发布 
的 中文 信息 处理 发展 报告 将 自然 语言 处理 
的 研究 领域 和 技术 进行 以下 分类 并 选取 
其中 部分 进行 介绍 1 自然语言 处理 基础 技术 词法 
句法 及 语义分析 词法 分析 的 主要 任务 是 词性 
标注 和 词义 标注 词性 是 词汇 的 基本 属性 
词性 标注 就是 在 给定 句子 中 判断 每个 词 
的 语法 范畴 确定 其 词性 并 进行 标注 解决 
兼类 词 和 确定 未 登录 词 的 词性 问题是 
标注 的 重点 进行/v 词性/n 标注/v 通常/d 有/v 基于/p 规则/n 
和/c 基于/p 统计/v 的/uj 两种/m 方法/n 一个 多义词 往往 可以 
表达 多个 意义 但 其 意义 在 具体 的 语境 
中 又是 确定 的 词义 标注 的 重点 就是 解决 
如何 确定 多义词 在 具体 语境 中的 义项 问题 标注 
过程 中 通常 是 先 确定 语境 再 明确 词义 
方法 和 词性 标注 类似 有/v 基于/p 规则/n 和/c 基于/p 
统计/v 的/uj 做法/v 判断 句子 的 句法结构 和 组成 句子 
的 各 成分 明确 它们 之间 的 相互 关系 是 
句法分析 的 主要 任务 句法分析/l 通常/d 有/v 完全/ad 句法分析/l 和/c 
浅层/n 句法分析/l 两种/m 完全 句法分析 是 通过 一 系列 的 
句法分析 过程 最终 得到 一个 句子 的 完整 的 句法树 
句法分析 方法 也 分为 基于 规则 和 基于 统计 的 
方法 基于 统计 的 方法 是 目前 的 主流 方法 
概率 上下文 无关 文法 用 的 较多 完全 句法分析 存在 
两个 难点 一是 词性 歧义 二 是 搜索 空间 太大 
通常 是 句子 中词 的 个数 n 的 指数级 浅层 
句法分析 又叫 部分 句法分析 或 语 块 分析 它 只 
要求 识别 出 句子 中 某些 结构 相对 简单 的 
成分 如 动词短语 非 递归 的 名词 短语 等 这些 
结构 被 称为 语 块 一般来说 浅层 语法分析 会 完成 
语 块 的 识别 和 分析 语 块 之间 依存 
关系 的 分析 两个 任务 其中 语 块 的 识别 
和 分析 是 浅层 语法分析 的 主要 任务 语义分析 是 
指 根据 句子 的 句法结构 和 句子 中 每个 实词 
的 词义 推 导出来 能够 反映 这个 句子 意义 的 
某种 形式 化 表示 将 人类 能够 理解 的 自然 
语言 转化 为 计算机 能够 理解 的 形式语言 句子 的 
分析 与 处理 过程 有的 采用 先 句法 后 语义 
的 方法 但 句法 语义 一体化 的 策略 还是 占据 
主流 位置 语义分析 技术 目前 还 不是 十分 成熟 运用 
统计 方法 获取 语义 信息 的 研究 颇受 关注 常见/a 
的/uj 有/v 词义/n 消/v 歧/v 和/c 浅层/n 语义分析/i 自然语言 处理 
的 基础 研究 还 包括 语用 语境 和 篇章 分析 
语用 是 指人 对 语言 的 具体 运用 研究 和 
分析 语言 使用者 的 真正 用意 它 与 语境 语言 
使用者 的 知识 涵养 言语 行为 想法 和 意图 是 
分不开 的 是 对 自然 语言 的 深层 理解 情景 
语境 和 文化 语境 是 语境 分析 主要 涉及 的 
方面 篇章 分析 则是 将 研究 扩展到 句子 的 界限 
之外 对/p 段落/n 和/c 整篇文章/n 进行/v 理解/v 和/c 分析/vn 除此之外 
自然 语言 的 基础 研究 还 涉及 词义 消 歧 
指代 消解 命名 实体 识别 等 方面 的 研究 知识图谱 
2012 年 5 月 Google 推出 Google 知识图谱 并将 其 
应用 在 搜索引擎 中 增强 搜索 能力 改善 用户 搜索 
质量 和 搜索 体验 这是 知识图谱 名称 的 由来 也 
标志着 大 规模 知识图谱 在 互联网 语义搜索 中的 成功 应用 
搜索 关键词 google 会在 右侧 给出 与 关键词 相关 的 
搜索 结果 知识图谱 是 为了 表示 知识 描述 客观 世界 
的 概念 实体 事件 等 之间 关系 的 一种 表示 
形式 这一 概念 的 起源 可以 追溯 至 语义 网络 
提 出于 20 世纪 五六十 年代 的 一种 知识 表示 
形式 语义 网络 由 许多 个 节点 和 边 组成 
这些 节点 和 边 相互连接 节点 表示 的 是 概念 
或 对象 边 表示 各个 节点 之间 的 关系 如 
下图 知识图谱 在 表现 形式 上 与 语义 网络 比较 
类似 不同 的 是 语义 网络 侧重于 表示 概念 与 
概念 之间 的 关系 而 知识图谱 更 侧重于 表述 实体 
之间 的 关系 现在 的 知识 网络 被 用来 泛指 
大 规模 的 知识库 知识图谱 中 包含 的 节点 有 
以下 几种 实体 指 独立 存在 且 具有 某种 区别性 
的 事物 如 一个 人 一种 动物 一个 国家 一种 
植物 等 具体 的 事物 就 是 实体 所 代表 
的 内容 实体 是 知识图谱 中的 最 基本 元素 不同 
的 实体 间 有 不同 的 关系 语义 类 具有 
同种 特性 的 实体 构成 的 集合 如 人类 动物 
国家 植物 等 概念 主要指 集合 类别 对象 类型 事物 
的 种类 例如 人物 地理 等 内容 通常 是 实体 
和 语义 类 的 名字 描述 解释 等 变现 形式 
一般 有 文本 图像 音 视频 等 属性 值 主要指 
对象 指定 属性 的 值 不同 的 属性 类型 对应 
于 不同 类型 属性 的 边 关系 在 知识 图 
谱上 表现 形式 是 一个 将 节点 实体 语义 类 
属性值 映 射到 布尔值 的 函数 除 语义 网络 之外 
70 年代 的 专家 系统 以及 Tim Berners Lee 提出/v 
的/uj 语义/n 网/n 和/c 关联/ns 数据/n 都/d 可以/c 说/v 是/v 
知识/v 图谱/n 的/uj 前身/r 知识图谱 表示 构建 和 应用 涉及 
很多 学科 是 一项 综合 的 复杂 技术 知识图谱 技术 
既 涉及 自然语言 处理 中 的 各项 技术 从 浅层 
的 文本 向量 表示 到/v 句法/n 和/c 语义/n 结构/n 表示/v 
被/p 适用/v 于/p 资源/n 内容/n 的/uj 表示/v 中/f 分词 和 
词性 标注 命名 实体 识别 句法 语义 结构 分析 指代 
分析 等 技术 被 应用 于 自然 语言 处理 中 
同时 知识图谱 的 研究 也 促进 了 自然 语言 处理 
技术 的 研究 基于 知识图谱 的 词义 排 岐 和 
语义 依存关系 分析 等 知识 驱动 的 自然 语言 处理 
技术 得以 建立 2 自然语言 处理 应用 技术 机器翻译 机器翻译 
Machine Translation 是 指 运用 机器 通过 特定 的 计算机 
程序 将 一种 书写 形式 或 声音 形式 的 自然 
语言 翻译成 另一种 书写 形式 或 声音 形式 的 自然 
语言 机器 翻译 是 一门 交叉学科 边缘学科 组成 它 的 
三 门子 学科 分别 是 计算机 语言学 人工智能 和 数理逻辑 
各自 建立 在 语言学 计算机科学 和 数学 的 基础 之上 
机器 翻译 的 方法 总体 上 可以 分为 基于 理性 
的 研究 方法 和 基于 经验 的 研究 方法 两种 
所谓 理性主义 的 翻译 方法 是 指 由人 类 专家 
通过 编撰 规则 的 方式 将 不同 自然 语言 之间 
的 转换 规律 生成 算法 计算机 通过 这种 规则 进行 
翻译 这种方法 理论 上 能够 把握 语言 间 深层次 的 
转换 规律 然而 理性主义 方法 对 专家 的 要求 极高 
不仅 要求 其 了解 源语言 和 目标语言 还要 具备 一定 
的 语言 学 知识 和 翻译 知识 更要 熟练掌握 计算机 
的 相关 操作技能 这些 因素 都 使得 研制 系统 的 
成本 高 周期长 面向 小语种 的 翻译 更是 人才 匮乏 
非常 困难 因此 翻译 知识 和 语言 学 知识 的 
获取 成为 基于 理性 的 机器 翻译 方法 所 面临 
的 主要 问题 所谓 经验主义 的 翻译 方法 指 的 
是以 数据 驱动 为基础 主张 计算机 自动 从大/nr 规模 数据 
中 学习 自然语言 之间 的 转换 规律 由于 互联网 文本 
数据 不断增长 计算机 运算 能力 也 不断 加强 以 数据 
驱动 为基础 的 统计 翻译 方法 逐渐 成为 机器 翻译 
的 主流 技术 但是 同时 统计 机器翻译 也 面临 诸如 
数据 稀疏 难以 设计 特征 等 问题 而 深度 学习 
能够 较好 的 缓解 统计 机器翻译 所 面临 的 挑战 
基于 深度 学习 的 机器 翻译 现在 正 获得 迅速 
发展 成为 当前 机器翻译 领域 的 热点 机器 翻译 技术 
较早 的 被 广泛 应用 在 计算机 辅助 翻译 软件 
上 更好 地 辅助 专业 翻译人员 提升 翻译 效率 近几年 
机器翻译 研究 发展 更为 迅速 尤其 是 随着 大 数据 
和 云计算 技术 的 快速 发展 机器翻译 已经 走进 人们 
的 日常 生活 在 很多 特定 领域 为 满足 各种 
社会 需求 发挥 了 重要 作用 按照 媒介 可以 将 
机器翻译 分为 文本 翻译 语音 翻译 图像 翻译 以及 视频 
和 VR 翻译 等 目前 文本 翻译 最为 主流 的 
工作 方式 依然 是 以 传统 的 统计 机器 翻译 
和 神经 网络 翻译 为主 Google Microsoft 与 国内 的 
百度 有道 等 公司 都为/nr 用户 提供 了 免费 的 
在线 多 语言 翻译 系统 将 源 语言文字 输入 其 
软件 中 便可 迅速 翻译 出 目标 语言文字 Google 主要 
关注 以 英语 为 中心 的 多 语言 翻译 百度 
则 关注 以 英语 和 汉语 为 中心 的 多 
语言 翻译 另外 即时 通讯 工具 如 Googletalk Facebook 等 
也都 提供 了 即时 翻译 服务 速度快 成本低 是 文本 
翻译 的 主要 特点 而且 应用 广泛 不同 行业 都 
可以 采用 相应 的 专业翻译 但是 这一 翻译 过程 是 
机械 的 和 僵硬 的 在 翻译 过程 中 会 
出现 很多 语义 语境 上 的 问题 仍然 需要 人工 
翻译 来 进行 补充 语音 翻译 可能 是 目前 机器翻译 
中 比较 富有 创新 意思 的 领域 吸引 了 众多 
资金 和 公众 的 注意力 亚马逊 的 Alexa 苹果 的 
Siri 微软 的 Cortana 等 我们 越来越 多 的 通过 
语音 与 计算机 进行 交互 应用 比较 好 的 如 
语音 同 传 技术 同声 传译 广泛 应用 于 国际 
会议 等 多 语言 交流 的 场景 但是 人工 同 
传 受限于 记忆 听说 速度 费用 偏高 等 因素 门槛 
较高 搜狗 推出 的 机器 同 传 技术 主要 在 
会议 场景 出现 演讲者 的 语音 实时 转换成 文本 并且 
进行 同步 翻译 低 延迟 显示 翻译 结果 希望 能够 
取代 人工 同 传 实现 不同 语言 人们 低成本 的 
有效 交流 科大 讯 飞 百度 等 公司 在 语音 
翻译 方面 也 有 很多 探索 如 科大 讯 飞 
推出 的 讯 飞 语音 翻译 系列产品 以及 与 新疆 
大学 联合 研发 的 世界 上 首款 维 汉 机器 
翻译 软件 可以 准确 识别 维吾尔语 和 汉语 实现 双语 
即时 互译 等功能 图像 翻译 也有 不小 的 进展 谷歌 
微软 Facebook 和 百度 均 拥有 能够 让 用户 搜索 
或者 自动 整理 没有 识别 标签 照片 的 技术 图像 
翻译 技术 的 进步 远 不 局限 于 社交 类 
应用 医疗 创业 公司 可以 利用 计算机 阅览 X 光 
照片 MRI 核磁共振 成像 和 CT 电脑 断层扫描 照片 阅览 
的 速度 和 准确度 都将 超过 放射科 医师 而且 更 
图像 翻译 技术 对于 机器人 无人机 以及 无人 驾驶 汽车 
的 改进 至关重要 福特 特斯拉 Uber 百度 和 谷歌 均 
已在 上路 测试 无人 驾驶 汽车 的 原型 除此之外 还有 
视频 翻译 和 VR 翻译 也在 逐渐 应用 中 但是 
目前 的 应用 还 不太 成熟 机器 翻译 这 一 
话题 AMnier 研究报告 系列 第五期 人工智能 之 机器翻译 研究报告 中有 
详细 阐述 具体 内容 可 查看 https / / static 
. aminer . cn / misc / article / translation 
. pdf 信息检索 信息检索 是从 相关 文档 集合 中 查找 
用户 所需 信息 的 过程 先将 信息 按 一定 的 
方式 组织 和 存储 起来 然后 根据 用户 的 需求 
从 已经 存储 的 文档 集合 当中 找出 相关 的 
信息 这是 广义 的 信息 检索 信息检索 最早 提出 于 
20 世纪 50 年代 90 年代 互联网 出现 以后 其 
导航 工具 搜索 引擎 可以 看成 是 一种 特殊 的 
信息 检索系统 二者 的 区别 主要 在于 语料库 集合 和 
用户 群体 的 不同 搜索引擎 面临 的 语料库 是 规模 
浩大 内容 繁杂 动态 变化 的 互联网 用户 群体 不再 
是 具有 一定 知识 水平 的 科技 工作者 而是 兴趣爱好 
知识 背景 年龄结构 差异 很大 的 网民 群体 信息检索 包括 
存 与 取 两个方面 对 信息 进行 收集 标引 描述 
组织 进行 有序 的 存放 是 存 按照 某种 查询 
机制 从 有序 存放 的 信息 集合 数据库 中 找出 
用户 所需 信息 或 获取 其 线索 的 过程 是 
取 信息检索 的 基本 原理 是 将 用户 输入 的 
检索 关键词 与 数据库 中的 标引词 进行 对比 当 二者 
匹配 成功 时 检索 成功 检索 标识 是 为 沟通 
文献标引 和 检索 关键词 而 编制 的 人工 语言 通过 
检索 标识 可以 实现 存 取 的 联系 一致 检索 
结果 按照 与 提问 词 的 关联度 输出 供 用户 
选择 用户 则 采用 关键词 查询 + 选择性 浏览 的 
交互 方式 获取 信息 以 谷歌 为 代表 的 关键词 
查询 + 选择性 浏览 交互方式 用户 用 简单 的 关键 
词作 为 查询 提交 给 搜索引擎 搜索引擎 并非 直接 把 
检索 目标 页面 反馈 给 用户 而是 提供 给 用户 
一个 可能 的 检索 目标 页面 列表 用户 浏览 该 
列表 并从 中 选择 出 能够 满足 其 信息 需求 
的 页面 加以 浏览 这种 交互 方式 对于 用户 来说 
查询 输入 是 简单 的 事 但 机器 却 难以 
通过 简单 的 关键词 准确 的 理解 用户 的 真正 
查询 意图 因此 只能 将 有 可能 满足 用户 需求 
的 结果 集合 以 列表 的 形式 提供 给 用户 
目前 互联网 是 人们 获取 信息 的 主要 来源 网络 
上 存放 着 取之不尽 用之不竭 的 信息 网络 信息 有着 
海量 分布 无序 动态 多样 异构 冗余 质 杂 需求 
各异 等 特点 人们 不再 满足 于 当前 的 搜索引擎 
带来 的 查询 结果 下一代 搜索引擎 的 发展 方向 是 
个性化 精确化 智能化 商务 化 移动 化 社区化 垂直化 多媒体化 
实时 化 等 情感 分析 情感 分析 又称 意见 挖掘 
是 指 通过 计算 技术 对 文本 的 主客观 性 
观点 情绪 极性 的 挖掘 和 分析 对 文本 的 
情感 倾向 做出 分类 判断 情感 分析 是 自然 语言 
理解 领域 的 重要 分支 涉及 统计学 语言学 心理学 人工智能 
等 领域 的 理论 与 方法 情感 分析 在 一些 
评论 机制 的 App 中 应用 较为 广泛 比如 某 
酒店 网站 会有 居住 过 的 客人 的 评价 通过 
情感 分析 可以 分析 用户 评论 是 积极 还是 消极 
的 根据 一定 的 排序 规则 和 显示 比例 在 
评论 区 显示 这个 场景 同时 也 适用 于 亚马逊 
阿里 巴巴 等 电商 网站 的 商品 评价 除此之外 在 
互联网 舆情 分析 中 情感 分析 起着 举足轻重 的 作用 
话语权 的 下降 和 网民 的 大量 涌入 使得 互联网 
的 声音 纷繁复杂 利用 情感 分析 技术 获取 民众 对于 
某一 事件 的 观点 和 意见 准确 把握 舆论 发展 
趋势 并 加以 合理 引导 显得 极为 重要 同时 在 
一些 选举 预测 股票 预测 等 领域 情感 分析 也 
体现 着 越来越 重要 的 作用 自动 问答 自动 问答 
是 指 利用 计算机 自动 回答 用户 所 提出 的 
问题 以 满足 用户 知识 需求 的 任务 问答 系统 
是 信息 服务 的 一种 高级 形式 系统 反馈 给 
用户 的 不再 是 基于 关键词 匹配 排序 的 文档 
列表 而是 精准 的 自然 语言 答案 这 和 搜索 
引擎 提供 给 用户 模糊 的 反馈 是 不同 的 
在 自然 语言 理解 领域 自动 问答 和 机器 翻译 
复述 和 文本 摘要 一起 被 认为 是 验证 机器 
是否 具备 自然 理解 能力 的 四个 任务 自动 问答 
系统 在 回答 用户 问题 时 首先 要 正确 理解 
用户 所 提出 的 问题 抽取 其中 关键 的 信息 
在 已有 的 语料库 或者 知识库 中 进行 检索 匹配 
将 获取 的 答案 反馈 给 用户 这一 过程 涉及 
了 包括 词法 句法 语义分析 的 基础 技术 以及 信息检索 
知识工程 文本 生成 等 多项 技术 传统 的 自动 问答 
基本 集中 在 某些 限定 专业 领域 但是 伴随 着 
互联网 的 发展 和 大规模 知识库 语料库 的 建立 面向 
开放 领域 和 开放性 类型 问题 的 自动 问答 越来越 
受到 关注 根据 目标 数据源 的 不同 问答 技术 大致 
可以 分为 检索 式 问答 社区 问答 以及 知识库 问答 
三种 检索 式 问答 和 搜索 引擎 的 发展 紧密 
联系 通过 检索 和 匹配 回答 问题 推理 能力 较弱 
社区 问答 是 web2 . 0 的 产物 用户 生成 
内容 是 其 基础 Yahoo Answer 百度知道 等 是 典型 
代表 这些 社区 问答 数据 覆盖 了 大量 的 用户 
知识 和 用户 需求 检索/vn 式/k 问答/v 和/c 社区/n 问答/v 
的/uj 核心/n 是/v 浅层/n 语义分析/i 和/c 关键词/n 匹配/v 而 知识库 
问答 则 正在 逐步 实现 知识 的 深层 逻辑推理 纵观 
自动 问答 发展 历程 基于 深度 学习 的 端 到 
端的 自动 问答 将 是 未来 的 重点 关注 同时 
多 领域 多 语言 的 自动 问答 面向 问答 的 
深度 推理 篇章 阅读 理解 以及 对话 也会 在 未来 
得到 更 广阔 的 发展 自动 文摘 自动 文摘 是 
运用 计算机技术 依据 用户 需求 从源/nr 文本 中 提取 最 
重要 的 信息 内容 进行 精简 提炼 和 总结 最后 
生成 一个 精简 版本 的 过程 生成 的 文摘 具有 
压缩性 内容 完整性 和 可读性 从 1955 年 IBM 公司 
Luhn 首次 进行 自动 文摘 的 实验 至今 的 几十 
年中 自动 文摘 经历 了 基于 统计 的 机械式 文摘 
和 基于 意义 的 理解 式 文摘 两种 机械式 方法 
简单 容易 实现 是 目前 主要 被 采用 的 方法 
但是 结果 不 尽如人意 理解 式 文摘 是 建立 在 
对 自然 语言 的 理解 的 基础 之上 的 接近 
于人 提取 摘要 的 方法 难度 较大 但是 随着 自然语言 
处理 技术 的 发展 理解 式 文摘 有着 长远 的 
前景 应用于 自动 文摘 的 方法 也 会 越来越 多 
作为 解决 当前 信息 过载 的 一项 辅助 手段 自动 
文摘 技术 的 应用 已经 不仅仅 限于 自动 文摘 系统软件 
在 信息检索 信息 管理 等 各 领域 都 得到 了 
广泛 应用 同时 随着 深度 学习 等 技术 的 发展 
自动 文摘 也 出现 了 许多 新的 研究 和 领域 
例如 多 文本 摘要 多语言 摘要 多媒体 摘 要等 社会 
计算 社会 计算 也称 计算 社会学 是 指在 互联网 的 
环境 下 以 现代 信息 技术 为 手段 以 社会 
科学 理论 为 指导 帮助 人们 分析 社会关系 挖掘 社会 
知识 协助 社会 沟通 研究 社会 规律 破解 社会 难题 
的 学科 社会 计算 是 社会 行为 与 计算 系统 
交互 融合 是 计算机 科学 社会科学 管理科学 等 多学科 交叉 
所 形成 的 研究 领域 它 用 社会 的 方法 
计算 社会 既是 基于 社会 的 计算 也是 面向 社会 
的 计算 社会 媒体 是 社会 计算 的 主要 工具 
和 手段 它 是 一种 在线 交互 媒体 有着 广泛 
的 用户 参与 性 允许 用户 在线 交流 协作 发布 
分享 传递信息 组成 虚拟 的 网络 社区 等等 近年来 社会 
媒体 呈现 多样化 的 发展 趋势 从 早期 的 论坛 
博客 维基 到 风头 正 劲 的 社交 网站 微博 
和 微信 等 正在 成为 网络 技术 发展 的 热点 
和 趋势 社会 媒体 文本属性 特点是 其 具有 草根 性 
字数 少 噪声 大 书写 随意 实时 性强 社会 属性 
特点 是 其 具有 社交性 在线 交互 它 赋予 了 
每个 用户 创造 并 传播 内容 的 能力 实施 个性化 
发布 社会化 传播 将 用户 群体 组织 成 社会化 网络 
目前 典型 的 社会 媒体 是 Twitter 和 Facebook 在 
我国 则 是 微博 和 微信 社会 媒体 是 一种 
允许 用户 广泛 参与 的 新型 在线 媒体 通过 社会 
媒体 用户 之间 可以 在线 交流 形成 虚拟 的 网络 
社区 构成 了 社会 网络 社会 网络 是 一种 关系 
网络 通过 个人 与 群体 及 其 相互 之间 的 
关系 和 交互 发现 它们 的 组织 特点 行为 方式 
等 特征 进而 研究 人群 的 社会 结构 以利于 他们 
之间 的 进一步 共享 交流 与 协作 社会 计算 应用 
广泛 近年来 围绕 社会 安全 经济 工程 和 军事 领域 
得到 了 长足 发展 金融 市场 采用 社会 计算方法 探索 
金融 风险 和 危机 的 动态 规律 例如 美国 圣塔菲 
研究 所 建立 了 首个 人工 股票 市场 的 社会 
计算 模型 许多 发达国家 都在/nr 政府 资助 下 开展 了 
研究 项目 例如 美国 的 ASPEN 欧盟 的 EURACE 等 
并且 在 国家 相应 的 经济 政策 制定 中 发挥 
着 越来越 重要 的 作用 通过 社交 媒体 来 把握 
舆情 引导 舆论 也 是 社会 计算 在 社会 安全 
方面 发挥 的 一个 重要 作用 军事 方面 许多 国家 
更 是 加大 投入 力度 扶持 军事 信息化 的 发展 
信息 抽取 信息 抽取 技术 可以 追溯 到 20 世纪 
60 年代 以 美国 纽约 大学 开展 的 Linguish String 
项目 和 耶鲁大学 Roger Schank 及其 同时 开展 的 有关 
故事 理解 的 研究 为 代表 信息 抽取 主要 是 
指 从 文本 中 抽取 出 特定 的 事实 信息 
例如 从 经济 新闻 中 抽取 新 发布 产品 情况 
如 公司 新 产品名 发布 时间 发布 地点 产品 情况 
等 这些 被 抽取 出来 的 信息 通常 以 结构化 
的 形式 直接 存入 数据库 可以 供 用户 查询 及 
进一步 分析 使用 为 之后 构建 知识库 智能 问答 等 
提供 数据 支撑 信息 抽取 和 上文 提到 的 信息 
检索 关系密切 但是 二者 之间 仍 存在 着 很大 的 
不同 首先 是 二者 要 实现 的 功能 不同 信息检索 
是 要从 大量 的 文档 中 找到 用户 所 需要 
的 文档 信息 抽取 则是 用在 文本 中 获取 用户 
感 兴趣 或 所 需要 的 事实 信息 其次 是 
二者 背后 的 处理 技术 也 不同 信息检索 依靠 的 
主要 是 以 关键 字词 匹配 以及 统计 等 技术 
不 需要 对 文本 进行 理解 和 分析 而 信息 
则 需要 利用 自然 语言 处理 的 技术 包括 命名 
实体 识别 句法分析 篇章 分析 与 推理 以及 知识库 等 
对/p 文本/n 进行/v 深入/v 理解/v 和/c 分析/vn 后/f 才能/v 完成/v 
信息/n 抽取/v 工作/vn 除了 以上 的 不同 之外 信息检索 和 
信息 抽取 又 可以 相互 补充 信息检索 的 结果 可以 
作为 信息 抽取 的 范围 提高效率 信息 抽取 用于 信息检索 
可以 提高 检索 质量 更好 地 满足 用户 的 需求 
信息 抽取 技术 对于 构建 大 规模 的 知识库 有着 
重要 的 意义 但是 目前 由于 自然 语言 本身 的 
复杂性 歧义 性 等 特征 而且 信息 抽取 目标 知识 
规模 巨大 复杂 多样 等 问题 使得 信息 抽取 技术 
还 不是 很 完善 但 我们 相信 在 信息 抽取 
技术 经历 了 基于 规则 的 方法 基于 统计 的 
方法 以及 基于 文本 挖掘 的 方法 等 一系列 技术 
演变 之后 随着 web 知识图谱 深度 学习 的 发展 可以 
为 信息 抽取 提供 海量 数据源 大 规模 知识 资源 
更好 地 机器学习 技术 信息 抽取 技术 的 问题 会 
得到 进一步 解决 并 有 长足 的 发展 自然语言 处理 
NLP 为 人工智能 中 的 一个 有趣 问题 是 深度 
学习 的 应用 程序 的 最新 前沿 如果 您 是 
一名 开发 人员 或 研究 人员 准备 深入 研究 这个 
快速 发展 的 人工智能 领域 这本 实用 的 书籍 将向 
您 展示 如何 使用 PyTorch 深度 学习 框架 来 实现 
最近 发现 的 NLP 技术 首先 您 需要 的 是 
机器学习 背景 和 使用 Python 编程 的 经验 作者 Delip 
Rao 和 Goku Mohandas 为 您 提供 了 PyTorch 的 
坚实 基础 以及 深度 学习 算法 用于 构建 涉及 文本 
语义 表示 的 应用 程序 每章 包括 几个 代码 示例 
和 插图 获得 对 NLP 深度 学习 和 PyTorch 介绍 
了解 传统 的 NLP 方法 包括 NLTK SpaCy 和 gensim 
探索 嵌入 语言 中 单词 的 高质量 表示 使用 递归 
神经网络 RNN 学习 语言 序列 中 的 表示 通过 复杂 
的 神经 架构 改进 RNN 结果 例如 长期 短期 记忆 
LSTM 和 门控 递归 单位 探索 读取 一个 序列 并 
产生 另一个 序列 的 序列 到 序列 模型 用于 翻译 
资源 下载 链接 https / / pan . baidu . 
com / s / 1 t i U i w 
X Q G l h Z H J r m 
B 1 r C H Q 提 取码 点击 查看 
本文 的 目标 是 介绍 Attention Model 在 自然 语言 
处理 里 的 应用 本文 的 结构 是 先 介绍 
两篇 经典之作 一篇 NMT 一篇 是 Image Caption 之后 介绍 
Attention 在 不同 NLP Task 上 的 应用 在 介绍 
时有 详 有略/nr 经典之作 有两 篇 文章 被 Attention 的 
工作 广泛 引用 这里 单拎/nr 出来 介绍 NEURAL MACHINE TRANSLATION 
BY JOINTLY LEARNING TO ALIGN AND TRANSLATENMT 通常用 encoder decoder 
family 的 方法 把 句子 编码 成 一个 定长 向量 
再 解码 成 译文 作者 推测 定长 向量 是 encoder 
decoder 架构 性能 提升 的 瓶颈 因此 让 模型 自动 
寻找 与 预测 下 一个 词 相关 的 部分 原文 
Encoder 部分 作者 使用 了 Bidirectional RNN for annotating sequences 
这是 PPT 介绍 http / / www . iclr . 
cc / lib / exe / fetch . php media 
= iclr2015 bahdanau iclr2015 . pdfShow Attend and Tell Neural 
Image Caption Generation with Visual Attention 这篇文章 的 任务 是 
给 图片 起 个 标题 我 自己 做 了 一页 
PPT 总结 了 文章 思路 接下来 介绍 自然语言 处理 各种 
Task 中的 Attention 应用 Attention in Word EmbeddingNot All Contexts 
Are Created Equal Better Word Representations with Variable AttentionThe general 
intuition of the model is that some words are only 
relevant for predicting local context e . g . function 
words while other words are more suited for determining global 
context such as the topic of the document . In 
CBOW p v0 | w − b b − { 
0 } = expvT0Oc ∑ v ∈ VexpvTOcp v _ 
0 | w _ { b b \ { 0 
\ } } = \ frac { \ exp v 
_ 0 ^ TO _ c } { \ sum 
_ { v \ in V } \ exp v 
^ TO _ c } In this paper c = 
∑ i ∈ − b b − { 0 } 
ai wi wic = \ sum _ { i \ 
in b b \ { 0 \ } } a 
_ i w _ i w _ iai w = 
expkw i + si ∑ j ∈ − b − 
b − { 0 } expkw i + sia _ 
i w = \ frac { \ exp k _ 
{ w i } + s _ i } { 
\ sum _ { j \ in b b \ 
{ 0 \ } } \ exp k _ { 
w i } + s _ i } each wordwiw 
_ i at relative positionii is attributed an attention level 
representing how much the attention model believes this it is 
important to look at in order to predict the center 
wordGradients of the loss function with respect to the parameters 
W O K s W O K s are computed 
with backpropagation and parameters are updated after each training instance 
using a fixed learning rate . Attention in Machine T 
r a n s l a t i o n 
E f f e c t i v e Approaches 
to Attention based Neural Machine T r a n s 
l a t i o n G l o b 
a l Attentionat s = align ht h ¯ s 
= exp score ht h ¯ s ∑ s ′ 
exp ht hs ′ ¯ a _ t s = 
align h _ t \ bar h _ s = 
\ frac { \ exp score h _ t \ 
bar h _ s } { \ sum _ { 
\ mathop { s } } \ exp h _ 
t \ bar { h _ { \ mathop s 
} } } 这里 的 ata _ t 是 Global 
Align Weights 它 的 size 是由 the number of time 
steps on the source side 决定 的 之后 的 ctc 
_ t 是由 source hidden statesh ¯ s \ bar 
h _ s 和 ata _ t 的 weighted average 
计算出 的 这里 的 score function 可以 有 多种 score 
ht h ¯ s = ⎧ ⎩ ⎨ ⎪ ⎪ 
hTth ¯ shTtWah ¯ sWa ht h ¯ s dot 
general concat . score h _ t \ bar h 
_ s = \ begin { cases } h _ 
t ^ \ mathrm { T } \ bar h 
_ s & \ text { dot } \ \ 
h _ t ^ \ mathrm { T } W 
_ a \ bar h _ s & \ text 
{ general } \ \ W _ a h _ 
t \ bar h _ s & \ text { 
concat } . \ end { cases } 除 这些 
之外 作者 还 实验 了 at = softmax Waht a 
_ t = softmax W _ ah _ t Local 
AttentionThe context vectorctc _ t is then derived as a 
weighted average over the set of source hidden states within 
the window pt − D pt + D p _ 
t − D p _ t + D DD is 
empirically selected . Unlike the global approach the local alignment 
vectorata _ t is now fixed dimensional i . e 
. ∈ R2D + 1 \ in R ^ { 
2D + 1 } . 这是 定义 级 的 区别 
接下来 作者 把 模型 做了 两种 变种 Monotonic alignment local 
m simply setpt = tp _ t = t assuming 
that source and target sequences are roughly monotonically aligned . 
ata _ t 的 公式 同 globalPredictive alignment local p 
修改 定义 at s = align ht h ¯ s 
exp − s − pt 22 σ 2 a _ 
t s = align h _ t \ bar h 
_ s \ exp \ frac { s p _ 
t ^ 2 } { 2 \ sigma ^ 2 
} 其中 pt = ∙ sigmoid vTptanh Wpht p _ 
t = \ bullet sigmoid v _ p ^ \ 
mathrm { T } \ tanh W _ ph _ 
t As a result attention model will favor alignment points 
nearptp _ t . In our proposed global and local 
approaches the attentional decisions are made independently which is suboptimal 
. 在 标准 的 MT 中 有 一个 coverage set 
记录 哪些 词 被 翻译 过了 在 这个 模型 中 
attentional vectorsh ~ t \ tilde h _ tare concatenated 
with inputs at the next time steps . 作者 把 
它 称作 input feeding approach . The effects of having 
such connections are two fold a we hope to make 
the model fully aware of previous alignment choices and b 
we create a very deep network spanning both horizontally and 
vertically . Attention in QACharacter Level Question Answering with AttentionEncode 
the Entities and Predicates in the KBEncode the QueryDecoding the 
KB Query3 . 1 . An LSTM based decoder with 
attention3 . 2 . A pairwise semantic relevance function that 
measures the similarity between the hidden units of the LSTM 
and the embedding of an entity or predicate candidate 小结 
通过 以上 具体 的 解释 我们 可以 看出 The basic 
idea of attention mechanism is that it assigns a weight 
/ importance to each lower position when computing an upper 
level representation . 下面 再看 一些 其他 任务 上 Attention 
Model 的 应用 Attention in Document C l a s 
s i f i c a t i o n 
H i e r a r c h i c 
a l Attention Networks for Document Classification 先 用词 表示 
双向 GRU Attention 生成 句子 表示 再用 一样 的 方法 
生成 文档 表示 v 最后 softmax Wv + b 用于 
文档 分类 Attention in Language to Logical FormLanguage to Logical 
Form with Neural Attention 本文 要把 自然语言 转化成 逻辑 表达式 
创造 了 2个 模型 1 Sequence to Sequence Model 把 
语义 解析 当做 普通 的 序列 转换 任务 2 Sequence 
to Tree Model 用 层次 树 解码器 获得 逻辑 形式 
的 结构 先 翻译 第一层 再 翻译 下一层 最后 在 
翻译 的 时候 加入 了 Attention 机制 Attention in SummarizationA 
Neural Attention Model for Abstractive Sentence Summarization 与 以上 类似 
就 写 这么 多了 请 各位 批评指正 谢谢 R e 
f e r e n c e B a h 
d a n a u D Cho K Bengio Y 
. Neural Machine Translation by Jointly Learning to Align and 
Translate J . Computer Science 2014 . Xu K Ba 
J Kiros R et al . Show Attend and Tell 
Neural Image Caption Generation with Visual Attention J . Computer 
Science 2016 2048 2057 . Ling W Tsvetkov Y Amir 
S et al . Not All Contexts Are Created Equal 
Better Word Representations with Variable Attention C / / Conference 
on Empirical Methods in Natural Language Processing . 2015 1367 
1372 . Luong M T Pham H Manning C D 
. Effective Approaches to Attention based Neural Machine Translation J 
. Computer Science 2015 . Golub D He X . 
Character Level Question Answering with Attention J . 2016 . 
Yang Z Yang D Dyer C et al . Hierarchical 
Attention Networks for Document Classification C / / Conference of 
the North American Chapter of the Association for Computational Linguistics 
Human Language Technologies . 2016 1480 1489 . Dong L 
Lapata M . Language to Logical Form with Neural Attention 
C / / Meeting of the Association for Computational Linguistics 
. 2016 33 43 . Rush A M Chopra S 
Weston J . A Neural Attention Model for Abstractive Sentence 
Summarization J . Computer Science 2015 . 主要 概 要有 
语言 的 兴起 人工智能 自然语言 处理 中文分词 隐 马尔可夫 信息熵 
贾里 尼克 布尔 与 搜索 图论 与 爬虫 PageRank 相关性 
与 可信度 TF IDF 余弦定理 与 分类 矩阵 运算 与 
文本 处理信息 指纹 密码学 搜索引擎 最大熵 模型 拼音 输入法 马库斯 
布隆 过滤 贝叶斯 网络 条件 随 机场 维 特比 K 
均值 与 分类 逻辑 回归 与 广告 MapReduce 关键 内容 
有 1 . 信息 度量 信息 就 是 不确定性 的 
多少 信息 就是 要 减少 不确定性 熵 信息 的 混杂 
程度 越大 信息 越杂/nr 越 不纯 条件 熵 一个 信息 
确定 的 条件 下 另外 一个 信息 不 确定 度 
的 减少量 互信息 在 一个 信息 的 条件 下 为了 
是 另外 一个 信息 不 确定 度 减少 所 需要 
提供 的 信息量 相对 熵 衡量 两个 函数值 为 正数 
的 函数 的 相关性 2 . 指纹 信息 指纹 每段 
信息 包括 文字 图片 音频 等 都 可以 对应 一组 
不 太长 的 随机数 伪随机数 压缩 基于 加密 的 伪随机数 
密码 集合 的 判定 文章 网页 的 判定 视频 的 
判定 指纹 可 能 重复 但 可能性 很小 相似 哈希 
词 权重 指纹 二进制 的 结合 提供 了 一种 思路 
3 . 最大熵 模型 最大熵 原理 保留 全部 的 不确定性 
让 风险 降到 最小 最大熵 模型 在 所有 满足 约束 
条件 的 模型 中 选出 熵 最大 的 模型 模型 
学习 任何 一组 不 自相矛盾 的 信息 最大熵 模型 存在 
并 且 唯一 都 具有 相同 的 形式 指数 形式 
特点 能 同时 满足 成千上万 的 中 不同 条件 的 
模型 有效 的 组合 很多 特征 参数 训练 对数 似 
然 函数 求 极大 4 . 期望 最大 如果 模型 
的 变量 都是 观测 变量 用 极大 似 然 估计 
或 贝叶斯 估计 如果 存在 隐含 变量 用 EM 迭代 
最大 后验/nr 概率 典型 kmeans 聚 类 隐 马的/nr 参数 
训练 最大熵 模型 的 训练 特点 局部 最优 计算 速度慢 
5 . 散 列表 与 布隆 过滤器 散 列表 的 
核心 哈希 函数 hashcode equals 函数 散 列表 的 特点 
时间 复杂度 o 1 浪费 空间 冲突 布隆 过滤器 核心 
一组 二进制 数 和 随机 映射函数 布隆 过滤器 的 特点 
时间 复杂度 o 1 节约 空间 到 存在 错误率 6 
. 文本 分类 相似性 余弦定理 距离 方法 k 近邻 思想 
自底向上 的 两两 合并 EM 迭代 奇异 值 分解 技巧 
计算 时 存储 重复 计算 的 变量 只 考虑 非零 
元素 删除 虚词 余弦定理 和 奇异 分解 余弦定理 多次 迭代 
计算 量大 消耗 资源 多 svd 无需 多次 迭代 时间短 
但 存储空间 需求 大 适合 超大规模 分类 建议 svd 粗 
分类 余弦定理 细 分类 TF IDF 解决 两个 重要 问题 
词 的 预测 能力 越强 权重 越大 停止词 的 权重 
为零 7 . 隐 马尔可夫 马尔可夫 假设 t 时刻 的 
状态 只 取决于 t 1 时刻 马尔可夫 链 状态 链 
隐 马 模型 初始 概率分布 状态 转移 概率分布 观测 概率分布 
马尔可夫 假设 观测 独立 3 个 问题 参数估计 baum uelch 
算法 计算 概率 直接 前 向 后向 算法 预测 状态 
维 特比 算法 动态规划 8 . 贝叶斯 网络 是 马尔可夫 
链 的 推广 链状 拓扑 又称 信念 网络 弧 + 
可信度 训练 结构 和 参数 训练 交叉 进行 方法 贪心 
算法 蒙卡/nr 互信息 9 . 条件 随 机场 特点 观测值/i 
可能/v 和/c 前后/f 的/uj 状态/n 都/d 有关/vn 条件/n 随/v 机场/n 
是/v 无向图/i 贝叶斯 网络 是 有向图 核心 找到 符合 所有 
边缘 分布 的 最大熵 模型 10 . 有限 状态机 和 
动态规划 有限 状态机 开始 终止 状态 有向 弧 条件 常见 
  建立 状态机 已知 状态机 匹配 字符串 区别 基于 概率 
的 有限 状态机 和 离散 马尔可夫 链 等效 动态规划 把 
全程 路径 最短 锁定 到 局部 路径 最短 作者 哈得/nr 
死链接 https / / www . jianshu . com / 
p / 0b997bd1c125 来源 简书 简书 著作权 归 作者 所有 
任何 形式 的 转载 都请/nr 联系 作者 获得 授权 并 
注明 出处 自然语言 处理 是 现代 技术 最 重要 的 
组成部分 之一 自然 语言 是 指 汉语 英语 法语 等 
人们 日常 使用 的 语言 是 自然而然 的 随着 人类 
社会 发展 演变 而来 的 语言 而 不是 人造 的 
语言 他 是 人类 学习 生活 的 重要 工具 概括 
来说 自然 语言 是 指 人类 社会 约定俗成 的 区别于 
人工 语言 如 设计程序 的 语言 自然语言 处理 是 计算机 
科学 领域 与 人工智能 领域 中 的 一个 重要 方向 
它/r 研究/vn 能/v 实现/v 人/n 与/p 计算机/n 之间/f 用/p 自然/d 
语言/n 进行/v 有效/a 通信/l 的/uj 各种/r 理论/n 和/c 方法/n 因此 
这一 领域 的 研究 将 涉及 自然语言 即 人们 日常 
使用 的 语言 所以 它 与 语言学 的 研究 有着 
密切 的 联系 但又 有 重要 的 区别 自然语言 处理 
并 不是 一般 地 研究 自然语言 而在于 研制 能 有效 
地 实现 自然 语言 通信 的 计算机 系统 特别 是 
其中 的 软件 系统 因而 它 是 计算机 科学 的 
一部分 自然语言 处理 是 计算机 科学 人工智能 语言学 关注 计算机 
和 人类 语言 之间 的 相互 作用 的 领域 自然语言 
处理 的 基础 是 各类 自然语言 处理 数据集 如 面向 
文本 分类 研究 的 中英文 新闻 分类 语料 以 IG 
卡方 等 特征词 选择 方法 生成 的 多维度 ARFF 格式 
中文 VSM 模型 万篇 随机 抽取 论文 中文 DBLP 资源 
用于 非 监督 中文分词 算法 的 中文分词 词库 UCI 评价 
排序 数据 带有 初始化 说明 的 情感 分析 数据集 等 
最早 的 自然 语言 理解 方面 的 研究 工作 是 
机器 翻译 1949年 美国人 威 弗 首先 提出 了 机器 
翻译 设计方案 20 世纪 60 年代 国外 对 机器 翻译 
曾有 大 规模 的 研究 工作 耗费 了 巨额 费用 
但 人们 当时 显然是 低估 了 自然 语言 的 复杂性 
语言 处理 的 理论 和 技术 均 不成 热 所以 
进展 不大 主要 的 做法 是 存储 两种 语言 的 
单词 短语 对应 译法 的 大辞典 翻译 时 一一对应 技术 
上 只是 调整 语言 的 同条 顺序 但 日常 生活 
中 语言 的 翻译 远 不是 如此 简单 很多 时候 
还要 参考 某 句话 前后 的 意思 自然语言 处理 的 
具体 表现 形式 包括 机器翻译 文本 摘要 文本 分类 文本校对 
信息 抽取 语音合成 语音 识别 等 机器 翻译 是 指 
运用 机器 通过 特定 的 计算机 程序 将 一种 书写 
形式 或 声音 形式 的 自然 语言 翻译成 另一种 书写 
形式 或 声音 形式 的 自然 语言 语音 翻译 可能 
是 目前 机器翻译 中 比较 富有 创新 意思 的 领域 
搜狗 推出 的 机器 同 传 技术 主要 在 会议 
场景 出现 演讲者 的 语音 实时 转换成 文本 并且 进行 
同步 翻译 低 延迟 显示 翻译 结果 希望 能够 取代 
人工 同 传 实现 不同 语言 人们 低成本 的 有效 
交流 信息检索 是从 相关 文档 集合 中 查找 用户 所需 
信息 的 过程 信息检索 的 基本 原理 是 将 用户 
输入 的 检索 关键词 与 数据库 中的 标引词 进行 对比 
当 二者 匹配 成功 时 检索 成功 自动 问答 是 
指 利用 计算机 自动 回答 用户 所 提出 的 问题 
以 满足 用户 知识 需求 的 任务 自动 问答 系统 
在 回答 用户 问题 时 首先 要 正确 理解 用户 
所 提出 的 问题 抽取 其中 关键 的 信息 在 
已有 的 语料库 或者 知识库 中 进行 检索 匹配 将 
获取 的 答案 反馈 给 用户 欢迎 关注 我们 微信 
公众 号 可以 加入 我们 QQ 人工智能 行业 交流 群 
626784247.01 在 当前 飞速 发展 的 创新 步伐 中 科技 
似乎 正在 积极 地 解决 人类 最 紧迫 的 难题 
在 某些 方面 我们 取得 了 很大 的 进步 但是 
当 涉及 到 解决 如 员工 多样性 无意识 偏见 员工 
和 客户 满意度 等等 以人为本 的 挑战 时 技术 并未 
达到 预期 效果 图片 来自 123rf . com . cn 
本文 来自 venturebeat 作者 刘敏 . 在 当前 飞速 发展 
的 创新 步伐 中 科技 似乎 正在 积极 地 解决 
人类 最 紧迫 的 难题 在 某些 方面 我们 取得 
了 很大 的 进步 在 可 再生 能源 疾病 预防 
和 灾后重建 等 领域 作出 了 重大 突破 但是 当 
涉及 到 解决 如 员工 多样性 无意识 偏见 员工 和 
客户 满意度 等等 以人为本 的 挑战 时 技术 并未 达到 
预期 效果 这 是 因为 像 喷气 推进 或 GPS 
这样 的 技术性 问题 在 很大 程度 上 是 与 
数学 和 物理 相关 的 这也 是 计算机 和 程序员 
擅长 的 领域 但是 解决 像 员工 投入 度 这类 
人情 问题 时 通常 需要 同理心 这是 很难 用 代码 
编写 出来 的 人类 是 情感 动物 尤其 是 在做 
决定 的 时候 首先 我们 用心 感受 然后 利用 逻辑思维 
帮助 自己 选择 正确 的 情感 反应 最后 我们 采取 
行动 因此 任何 帮助 人们 做出 更好 的 决定 而 
不考虑 情感 因素 的 尝试 都 注定 要 失败 然而 
随着 人工智能 的 发展 尤其 是 自然 语言 处理 NLP 
的 最新 进展 我们 终于 掌握 了 利用 人类情感 力量 
和 复杂性 的 技术 工具 这种 方法 对 我们 如何 
设计 系统 有着 重要 的 影响 而且 它 也会 带来 
更加 人性化 的 解决方案 编程 的 差异 语言 极其 复杂 
人与 人 之间 一个人 的 经历 或 生活 环境 一点 
细微 的 差别 就 可以 影响 他们 表达 自己 的 
方式 方言 性别 地点 甚至 季节 都 可以 改变 我们 
用来 表达 想法 的 词汇 人们 很 善于 解释 这些 
细微 的 差别 然而 对于 计算机 来说 这 是 一个 
巨大 的 挑战 为了 达到 接近 人类 水平 的 理解 
他们 需要 一套 庞大 而 丰富 的 语言 训练 数据 
这些 数据 跨越 了 人口 统计学 经验 和 背景 的 
差距 要想 了解 这 在 现实 生活 中 是 如何 
运作 的 只需 想想 加州 的 一个 十几岁 的 少年 
在给 新 智能 手机 评论 时 使用 了 lit 这个词 
意思 是 激动 而 来自 马萨诸塞州 的 一位 老人 作出 
的 评论 中 同样 的 词 可能 意味着 屏幕 亮度 
体会 言外之意 这是 第一次 我们 能够 教 计算机 不仅 要 
通过 计算 单词 或 寻找 特定 短语 来 理解 人们 
的 基本 意思 而是 聪明 地 体会 言外之意 理解 我们 
的 言语 背后 的 真正 意图 和 意义 当然 这是 
随着 时间 的 推移 人们 获得 的 一项 重要 技能 
移情 常见 的 满意度 调查 是 一个 典型 的 例子 
人们 能够 感受到 机器 在 解决 最 基础 的 问题 
时的/nr 局限性 从 原则上 讲 这是 了解 人们 对 产品 
或 服务 的 看法 的 有效 方法 但在 实际 操作 
中 它 显得 十分 笨拙 不准确 而且 早就 应该 修改 
一下 了 回想 一下 大多数 商店 收据 上 的 调查 
提示 请 为 我们 的 服务 打分 1 10 并 
分享 原因 比较 一下 同样 情况 下 简单 地 问 
一句 你 对 这次 体验 的 看法 是 什么 然后/c 
从/p 使用/v 的/uj 语言/n 和/c 整个/b 上下文/l 推断出/l 得分 虽然 
人们 不 需要 明确 的 评分 但 机器 确实 需要 
镜像 效应 除了 帮助 我们 更好 地 了解 彼此 之外 
NLP 还能 让 我们 更好 地 了解 自己 语言 是 
我们 表达 思想 感情 的 窗户 当 技术 可以 开始 
理解 我们 的 时候 不是 它 希望 我们 如何 它 
可以 成为 一个 真正 的 合作 伙伴 帮助 我们 发现 
成长 进步 的 最佳 方式 以 可怕 的 绩效 评估 
和 各种各样 的 偏见 来 折磨 它 当 你问 工作环境 
中的 人 他们 是否 会 有偏见 即使 是 下意识 的 
他们 也 会 极力 否认 然而 对 绩效 评估 的 
研究 显示 出 人们 持有 普遍 的 无意识 的 偏见 
我 的 团队 分析 显示 当 男性 审视 其他 男性 
时 他们 中 绝大多数 都 使用 被动 的 语言 他们 
可以 更加 积极 主动 然而 当 这些 男性 对 女性 
进行 审查 时 他们 通常 会 指指点点 你 应该 注意 
细节 通过 使用 数据 驱动 的 技术 我们 能够 进一步 
深入 了解 这些 隐藏 的 偏见 而 人们 往往 意识 
不到 它们 的 存在 幸运 的 是 人工智能 让 我们 
直面 这些 偏见 并 一步步 纠正 它们 为了 解决 世界 
上 最具 挑战性 的 人 性问题 无论 是 通过 开发 
更好 的 产品 还是 在 工作 中 获得 更多 理解 
和 公正 我们 都 需要 技术 来 表达 同理心 让 
心 与 心 结合 在 一起 我们 就 可以 进一步 
发展 并 提倡 人们 应得 的 以人为本 的 解决 方案 
02 52AI52AI 专注 服务 于 普通人 的 AI 学习 和 
发展 让 大众 受益 于 人工智能 就是 我们 的 愿望 
我们 坚信 只有 对 大众 收益 的 科技 才是 有 
意义 的 也 是 我们 追求 的 方向 自然语言 处理 
与 信管 这一 专业 的 关系 我们 首先 说说 什么 
是 自然 语言 处理 现在 世界 上 所有 的 语种 
语言 都 属于 自然 语言 自然语言 处理 并 不是 通过 
人 来 人工 处理 而是 通过 计算机 来 进行 处理 
自然语言 输入 至 计算机 后 计算机 用 定义 好了 的 
算法 进行 处理 得 到人 所 期待 的 结果 信息 
管理 和 信息系统 简称 信管 这一 专业 要求 掌握 管理 
信息 系统 的 分析 方法 设计 方法 和 实现 技术 
具有 信息 收集 组织 分析研究 传播 与 综合 利用 的 
基本 能力 掌握 文献检索 资料 查询 收集 的 基本 方法 
这些 都是与/nr 计算机 结合 通过 计算机 作为 工具 使得 信息 
管理 更加 有效 和 实用 这些 在 生活 中 有 
很多 的 应用 例如 铁路 订票 系统 就是 对 车票 
这种 信息 的 查询 和 管理 系统 还有 电子 病历 
也是 对 信息 收集 后的/nr 处理 与 反馈 大大 节省 
了 时间 提高 了 效率 自然语言 处理 在 生活 中 
有着 广泛 的 应用 其中 最 常见 的 就是 在线翻译 
很多 人 出国 随身 带着 翻 译笔 即使 语言不通 通过 
翻译 笔 也 可以 正常 交流 很多 app 都有 评论 
功能 通过 感情 分析 可以 看出 用户 的 态度 负责人 
以 此 可以 达到 自己 想要 的 效果 同时 在 
一些 选举 预测 股票 预测 等 领域 情感 分析 也 
逐渐 体现 着 越来越 重要 的 作用 随着 大 数据 
和 人工智能 的 兴起 自然语言 处理 也 将会 在 各个 
方面 都 有所作为 信息 管理 和 信息 系统 在 收集 
与 分析 数据 的 知识 的 应用 与 自然 语言 
处理 可以 说 有 一定 的 关系 的 而 自然 
语言 处理 能为 信管 服务 它 可以 缩短 搜集 的 
信息 范围 并对 其 进行 一定 的 分析 使 我们 
得到 我们 想 要 的 结果 未来 的 自然 语言 
处理 说不定 会 使得 这些 过程 变得 更加 的 简洁 
迅速 甚至 是 使其 实现 自动化 的 处理 1 计算 
jieba 和 thula 的 P R F 值 基于 文本 
express . txt 标准 文本 是 人工 切分 基本 计算公式 
精度 Precision 召回率 Recall F 值 F mesure N   
标准 分割 的 单词 数 e   分词器 错误 标注 
的 单词 数 c   分词器 正确 标注 的 单词 
数 P = c / N     R = 
c / c + e   F = 2 * 
R * P / R + P 结巴 分词 的 
使用 函数 s1 = list jieba . cut f 清华 
分词 的 使用 函数 thu1 = thulac . thulac seg 
_ only = True s _ 2 = thu1 . 
cut f text = True 编程 思路 读取 标准 文本 
建立 词典 数据类型 为 list 去除 文本 中的 标点符号 计算 
n 值 通过 jieba 进行 分词 jieba 分词 后 可直接 
生成 list 删除 文本 中的 标点符号 将 jieba 分词 的 
结果 与 词典 进行 对比 遍历 jieba 分词 结果 中 
的 每个 词 若 词典 中有 则 c + 1 
若 没有 则 e + 1 最后 计算 P R 
F 值 输出 结果 通过 清华 分词 清华 分词 后的/nr 
结果 为 一个 字符串 用 空格 隔开 先 通过 一个 
循环 将 字符串 中的 词 分割 开 存入 list 中 
再 删除 list 中的 标点 符后/nr 其余 步骤 与 jieba 
分词 相同 将 P R F 的 计算 过程 写成 
函数 简化 代码 结果 代码 # / usr / bin 
/ env python # * coding utf 8 * # 
Date 2018 / 3/26 19 07 # _ _ Author 
_ _ cimoko # File Name lesson _ 3 _ 
1 . py import jieba import re import thulac def 
P _ R _ F n c e R = 
round c / n 4 P = round c / 
c + e 4 F = round 2 * P 
* R / P + R 4 print 精度 P 
为 P * 100 % print 召回率 R 为 R 
* 100 % print F 值 为 F * 100 
% return P R F f = str open express 
. txt . readlines # print f # 标准 文本 
raw = open express _ cut . txt . readlines 
d = re . split r | \ n w 
0 for w in raw dict = for w in 
d if w = = r or w = = 
r or w = = r or w = = 
r pass else dict . append w print * * 
* * * * * * * * * * 
* 标准 分词 文本 * * * * * * 
* * * * * * * print dict n 
= len dict # 结巴 s1 = list jieba . 
cut f s _ jieba = for w in s1 
if w = = r or w = = r 
or w = = r or w = = r 
or w = = r or w = = r 
or w = = r pass else s _ jieba 
. append w # print s _ jieba e _ 
jieba = 0 c _ jieba = 0 for i 
in range len s _ jieba if s _ jieba 
i in dict c _ jieba + = 1 else 
e _ jieba + = 1 print * * * 
* * * * * * * * * * 
结巴 分词 结果 * * * * * * * 
* * * * * * print s _ jieba 
print c c _ jieba print e e _ jieba 
print n n P _ R _ F n c 
_ jieba e _ jieba # 清华 thu1 = thulac 
. thulac seg _ only = True s _ 2 
= thu1 . cut f text = True s2 = 
# print s _ 2 s _ qinghua = a 
= 0 for i in range len s _ 2 
if s _ 2 i = = s2 . append 
s _ 2 a i a = i + 1 
else continue for w in s2 if w = = 
r or w = = r or w = = 
r or w = = r or w = = 
r or w = = r or w = = 
r pass else s _ qinghua . append w # 
print s _ qinghua e _ qinghua = 0 c 
_ qinghua = 0 for i in range len s 
_ qinghua if s _ qinghua i in dict c 
_ qinghua + = 1 else e _ qinghua + 
= 1 print * * * * * * * 
* * * * * * 清华 分词 结果 * 
* * * * * * * * * * 
* * print s _ qinghua print c c _ 
qinghua print e e _ qinghua print n n P 
_ R _ F n c _ qinghua e _ 
qinghua 自然语言 处理 涉及 到 的 相关 技术 可以 按照 
不同 的 分类 标准 基于 不同 的 观察 视角 进行 
划分 基于 不同 的 分类 原则 自然语言 处理 相关 技术 
的 分类 结果 也 有所不同 在 这里 我们 主要 采用 
两 个 分类 原则 进行 划分 其一 基于 分析 对象 
语言 单位 粒度 的 不同 词汇 级 句子 级 级 
和 篇章 级 其二 基于 分析 内容 性质 的 不同 
词 法分析 语法分析 语义分析 和 语用分析 按照 以上 的 分类 
标准 自然语言 处理 的 主要 技术 分类 结果 如下 图 
所示 递归 神经 网络 是 当今 最 常见 的 人工智能 
应用 程序 的 核心 但 我们 很快 就 发现 它们 
并 不适 合用 来 解决 广义 时间 序列 问题 现在 
已经 有 几个 在 使用 中 的 替代 解决方案 其中 
有 一个 是 刚刚 出现 的 ODE 网络 它 与 
我们 思考 解决 方案 的 方式 截然不同 \ n 递归 
神经网络 及其 近亲 LSTM 是 人工智能 自然语言 处理 应用 程序 
的 核心 与 其他 形式 的 人工智能 相比 RNN NLP 
在 现实 世界 中 的 应用 要 多得多 包括 使用 
卷积 神经网络 识别 和 处理 图像 \ n 从 某种 
意义 上 说 数据 科学家 的 队伍 已经 分 成了 
两组 每 一组 都在 追求 使用 这 两种 技术开发 独立 
的 应用 从 应用 角度 来看 这两种 技术 基本上 不会 
发生 重叠 因为 图像处理 处理 的 是 静态数据 而 RNN 
NLP 是 将 语音 和 文本 解释 为 时间 序列 
数据 \ n 虽然 RNN / LSTM 仍然 是 大多数 
NLP 的 首选 技术 但 我们 越是/nr 试图 扩展 时间 
序列 应用 遇到 的 麻烦 就 越多 即将 出现 的 
技术 可能 不 只是 RNN 的 修改 版本 而是 对 
其他 几 种 创新 人工智能 方法 的 硬 分支 \ 
n 第一 个 分支 将 CNN 与 RNN 组合 使用 
\ n 第一 个 分支 是 我们 去年 提出 的 
将 CNN 和 RNN 结合 在 一个 神经 网络 中 
详见 将 CNN 与 RNN 组合 使用 天才 还是 错乱 
需要 解决 的 问题 与 时间 序 列上 的 图像 
有关 即 视频 而 最 常见 的 任务 是 视频 
场景 标记 事实证明 这种 技术 对于 识别 和 标记 视频 
中 的 情感 以及 根据 之前 在 视频 中 见过 
的 人 来 识别 某些 类型 的 人 也很 有用 
\ n 第二 个 分支 时间 卷积 神经网络 TCN \ 
n 去年 谷歌/nr 和/c Facebook/w 都/d 解决/v 了/ul RNN/w 的/uj 
第二/m 类/q 问题/n 因为 要 分析 的 数据 扩展到 DNN 
中的 多个 层 所以 在 开始 计算 之前 必须 等待 
所有 这些 层 都 完成 这也 意味着 MPP 实际上 并 
不可行 虽然 这个 过程 仍然 很快 但 不足以 快到 可以 
让 实时 语言 翻译 应用程序 避免 明显 的 延迟 \ 
n 第二 个 分支 导致 这 两家 公司 放弃 了 
RNN 转而 采用 一种 他们 称之为 时间 卷积 神经网络 TCN 
的 CNN 变体 来 进行 实时 翻译 这 看起来 很像 
添加 了 Attention 功能 的 CNN 因为 它们 的 结构 
与 CNN 类似 所以 可以 应用 MPP 于是 延迟 就 
消失 了 \ n 第三 个 分支 不规则 时间 序列 
\ n 还有 一些 其他 类型 的 时间 序列 问题是 
RNN 无法 完美 解决 的 它们 的 主要 是 具有 
连续 值 或者 希望 将 具有 不同 频率 持续 时间 
和 起始 点 的 时间 序列 数据 组合 在 一起 
系统 \ n 最后 这 一个 分支 看起来 并 没有 
那么 神秘 它 描述 的 是 这样 的 一种 情况 
在 你 去看 不同 的 医生 时 你 会 看到 
自己 的 医疗 记录 你 有 不同 的 预约 时间 
间隔 有/v 不同/a 剂量/n 和/c 时间/n 间隔/n 的/uj 用药/n 情况/n 
对 这些 药品 等 有 不同 的 身体 反应 并且 
你 的 身体 在 以 某种 可 测量 的 方式 
变老 变强 变好 或 变坏 \ n 这 就是 为什么 
人工智能 的 绝大多数 医疗 应用 都只与/nr 图像 识别 有关 我们 
在 使用 不规则 时序 AI 能力 方面 确实 存在 不足 
无法 很好 地 基于 不规则 时间 序列 数据 得出 预测 
结果 \ n 一种 解决 方案 是 将 并行 的 
医疗 记录 分为 几 星期 几天 甚至 是 几 小时 
的 离散 步骤 理论上 这样 可以 满足 RNN 所 要求 
的 离散化 但 问题 是 为了 获得 最大 的 收益 
你 必须 使用 非常 合适 的 时间 桶 这样 会 
增加 计算 成本 和 复杂性 还有 一个 问题 那 就是 
很多 时间 桶 可能 不 包含 任何 数据 \ n 
因此 预测/vn 社区/n 和/c 医疗/n 社区/n 都/d 需要/v 一个/m 人工智能/n 
解决方案/n 其 性能 要 优于 目前 的 RNN \ nODE 
网络 \ n 去年 12月 在 蒙特利尔 举行 的 神经 
信息处理系统 NIPS 大会 上 来自 加拿大 向量 研究所 的 研究 
人员 提出 了 人工智能 时间 序列 建模 的 全新 概念 
并 被 评为 大会 四篇 最佳 论文 之一 \ n 
他们 的 系统 的 名字 叫作 ODE 网络 是 Ordinary 
Differential Equation Net 常 微分方程 网络 的 缩写 但 不要 
被 误导 了 ODE 网络 看起来 一点 也 不像 DNN 
它 没有 节点 层 或 互连 这 是 一种 使用 
带有 反向 传播 的 黑盒 微分 方程解 算器 的 方法 
在/p 连续/a 和/c 离散/v 时间/n 序列/n 问题/n 上/f 都/d 优于/v 
RNN/w 换句话说 它 更像 是 一个 坚实 的 计算 板 
而 不是 可以 被 可视 化为 神经 网络 的 东西 
\ n 这种方法 带来 了 思维 方式 上 的 几个 
有趣 的 变化 例如 在 使用 RNN 时 你 可以 
指定 层 和 其他 超 参数 然后 运行 实验 并 
查看 所 获得 的 准确性 \ n 而在 使用 ODE 
网络 时 在 准确性 和 训练 时间 之间 存在 一个 
权衡 你 指定 了 准确性 级别 ODE 网络 将 会 
找到 实现 这 一 目标 的 最佳 方法 但 训练 
时间 是 变化 的 如果 训练 时间 长得 让人 无法 
接受 可以 指定 一个 较低 的 准确性 以便 加快 训练 
过程 一个/m 有趣/a 的/uj 结果/n 可能/v 是/v 在/p 训练/vn 时/n 
指定/v 高/a 准确性/n 但在 测试 时 可以 指定 较低 的 
准确性 \ n 这篇 论文 https / / arxiv . 
org / abs / 1806.07366 的 内容 非常 全面 并 
提供 了 几个 实验 的 结果 其中 的 结果 明显 
优于 RNN 但 它 仍 处于 研究 阶段 但 与 
数据 科学 中 的 大多数 东西 一样 这 并不 需要 
很长 时间 就 能 走向 应用 \ n 英文 原文 
\ nhttps / / www . d a t a 
s c i e n c e c e n 
t r a l . com / profiles / blogs 
/ the coming revolution in recurrent neural nets rnns \ 
n \ n 一 人工智能 学习 算法 分类 人工 智能算法 
大体上 来说 可以 分类 两类 基于 统计 的 机器学习 算法 
Machine Learning 和 深度 学习 算法 Deep Learning 总的来说 在 
sklearn 中 机器学习 算法 大概 的 分类 如下 1 . 
纯 算法 类 1 . 回归 算法 2 . 分类 
算法 3 . 聚 类 算法 4 降 维 算法 
5 概率 图 模型 算法 6 文本 挖掘 算法 7 
优化 算法 8 深度 学习 算法 2 . 建模 方面 
1 . 模型 优化 2 . 数据 预处理 二 详细 
算法 1 . 分类 算法 1 . LR Logistic Regression 
逻辑 回归 又叫 逻辑 分类 2 . SVM Support Vector 
Machine 支持 向量 机 3 . NB Naive Bayes 朴素 
贝叶斯 4 . DT Decision Tree 决策树 1 . C 
4.52 . ID33 . CART 5 . 集成 算法 1 
. Bagging2 . Random Forest 随机 森林 3 . GB 
梯度 提升 Gradient boosting 4 . GBDT Gradient Boosting Decision 
Tree 5 . AdaBoost6 . Xgboost 6 . 最大熵 模型 
2 . 回归 算法 1 . LR Linear Regression 线性 
回归 2 . SVR 支持 向量 机 回归 3 . 
RR Ridge Regression 岭回归 3 . 聚 类 算法 1 
. Knn 2 . Kmeans 算法 3 . 层次 聚 
类 4 . 密度 聚 类 4 . 降 维 
算法 1 . SGD 随机 梯度 下降 2 . 5 
. 概率 图 模型 算法 1 . 贝叶斯 网络 2 
. HMM 3 . CRF 条件 随 机场 6 . 
文本 挖掘 算法 1 . 模型 1 . LDA 主题 
生成 模型 Latent Dirichlet Allocation 4 . 最大熵 模型 2 
. 关键词 提取 1 . tf idf2 . bm253 . 
textrank4 . pagerank5 . 左右 熵 左右 熵 高的/nr 作为 
关键词 6 . 互信息 3 . 词 法分析 1 . 
分词 ① HMM 因 马尔科夫 ② CRF 条件 随 机场 
2 . 词性 标注 3 . 命名 实体 识别 4 
. 句法分析 1 . 句法结构 分析 2 . 依存 句法分析 
5 . 文本 向 量化 1 . tf idf2 . 
word2vec3 . doc2vec4 . cw2vec 6 . 距离 计算 1 
. 欧氏距离 2 . 相似 度 计算 7 . 优化 
算法 1 . 正则化 1 . L1 正则化 2 . 
L2 正则化 8 . 深度 学习 算法 1 . BP 
2 . CNN 3 . DNN 3 . RNN 4 
. LSTM 三 建模 方面 1 . 模型 优化 1 
. 特征选择 2 . 梯度 下降 3 . 交叉 验证 
4 . 参数 调 优 5 . 模型 评估 准确率 
召回率 F1 AUC ROC 损失 函数 2 . 数据 预处理 
1 . 标准化 2 . 异常值 处理 3 . 二 
值 化 4 . 缺失 值 填充 支持 均值 中位数 
特 定值 补差 多重 插补 1 . 分词 Word Cut 
英文 单词 组成 句子 单词 之间 由 空格 隔开 中文 
字 词 句 段 篇 词 有 意义 的 字 
组合 分词 将 不同 的 词 分 隔开 将 句子 
分解为 词 和 标点符号 英文 分词 根据 空格 中文分词 三类 
算法 中文分词 难点 歧义 识别 未 登录 词 中文分词 的 
好坏 歧义 词 识别 和未/nr 登录 词 的 识别 准确率 
分词 工具 Jieba SnowNLP NlPIR LTP NLTK2 . 词性 标注 
POS Tag 词性 也 称为 词类 或 词汇 类别 用于 
特定 任务 的 标记 的 集合 被称为 一个 标记 集 
词性 词类 词汇 性质 词汇 的 语义 功能 词汇 的 
所属 类别 词性 取决于 1 . 选定 的 词 的 
类别 体系 2 . 词汇 本身 在 语 句中 上下文 
的 语法 语义 功能 一个 词汇 有 多个 不同 的 
词性 词性 兼类 现象 词性 唯一 单性 词 词性 多于 
2个 兼类 词 词性 标注 将 单词 按 它们 的 
词性 分类 并 进行 相应 地 标注 的 过程 称为 
词语 性质 标注 词性 标注 或 简称 标注 词性 标注 
器 一个 标注 器 能够 正确 识别 一个 句子 的 
上下 文中 的 这些 词 的 标记 词性 标注 方法 
三类 2.1 NLTK 常用 词性 CC Coordinating conjunction 连接词 CD 
Cardinal number 基数词 DT Determiner 限定词 如 this that these 
those such 不定 限定词 no some any each every enough 
either neither all both half several many much a few 
a little other another . EX Existential there 存在 句 
FW Foreign word 外来词 IN Preposition or subordinating conjunction 介词 
或 从属连词 JJ Adjective 形容词 或 序数词 JJR Adjective comparative 
形容词 比较级 JJS Adjective superlative 形容词 最高级 LS List item 
marker 列表 标示 MD Modal 情态 助动词 NN Noun singular 
or mass 常用 名词 单数 形式 NNS Noun plural 常用 
名词 复数形式 NNP Proper noun singular 专有名词 单数 形式 NNPS 
Proper noun plural 专有名词 复数形式 PDT Predeterminer 前 位 限定词 
POS Possessive ending 所有格 结束 词 PRP Personal pronoun 人称代词 
PRP $ Possessive pronoun 所有格 代名词 RB Adverb 副词 RBR 
Adverb comparative 副词 比较级 RBS Adverb superlative 副词 最高级 RP 
Particle 小品 词 SYM Symbol 符号 TO to 作为 介词 
或 不定式 格式 UH Interjection 感叹词 VB Verb base form 
动词 基本形式 VBD Verb past tense 动词 过去式 VBG Verb 
gerund or present participle 动名词 和 现在分词 VBN Verb past 
participle 过去分词 VBP Verb non 3rd person singular present 动词 
非 第三人称 单数 VBZ Verb 3rd person singular present 动词 
第三人称 单数 WDT Wh determiner 限定词 如 关系 限定词 whose 
which . 疑问 限定词 what which whose . WP Wh 
pronoun 代词 who whose which WP $ Possessive wh pronoun 
所有格 代词 WRB Wh adverb 疑问代词 how where when 通用 
词性 标记 集 标记 含义 英文 示例 ADJ 形容词 new 
good high special big localADP 介词 on of at with 
by into underADV 副词 really already still early nowCONJ 连词 
and or but if while althoughDET 限定词 冠词 the a 
some most every no whichNOUN 名词 year home costs time 
AfricaNUM 数词 twenty four fourth 1991 14 24PRT 小品 词 
at on out over per that up withPRON 代词 he 
their her its my I usVERB 动词 is say told 
given playing would . 标点符号 . X 其它 ersatz esprit 
dunno gr8 univeristyNLTK 读取 已经 标注 的 语料库 一个/m 已/d 
标注/v 的/uj 词/n 符/v 使用/v 一个/m 由/p 词/n 符和/nr 标记/n 
组成/v 的/uj 元组/n 来/v 表示/v str2tuple 一旦 我们 开始 做 
词性 标注 我们 将 会 创建 分配 一个 标记 给 
一个 词 的 程序 标记 是 在 给定 上下文 中最 
可能 的 标记 我们 可以 认为 这个 过程 是从 词 
到 标记 的 映射 在 Python 中最 自然 的 方式 
存储 映射 是 使用 所谓 的 字典 数据类型 在 其他 
的 编程语言 又 称为 关联 数组 或 哈希 数组 NLTK 
标记 形式 word tag 和 字典 将 字典 转换成 列表 
list sorted 按 值 排序 一个 字典 的 习惯用法 sorted 
的 第一 个 参数 是 要 排序 的 项目 它 
是由 一个 词性 标记 和 一个 频率 组成 的 元组 
的 列表 第二个 参数 使用 函数 itemgetter 指定 排序 的 
键 在 一般 情况 下 itemgetter n 返回 一个 函数 
这个 函数 可以 在 一些 其他 序列 对象 上 被 
调用 获得 这个 序列 的 第 n 个 元素 from 
operator import itemgettersorted counts . items key = itemgetter 1 
reverse = True 一个词/i 的/uj 标记/n 依赖/v 于/p 这个/r 词/n 
和它在/nr 句子/n 中的/i 上下文/l 3/m ./i 自动/vn 标注/v 3.1/mx 默认/v 
标注/v 器/n 1/m ./i 最简单/i 的/uj 标注/v 器/n 是/v 为/p 
每个/r 词/n 符/v 分配/vn 同样/d 的/uj 标记/n 这 似乎 是 
一个 相当 平庸 的 一步 但 它 建立 了 标注 
器 性能 的 一个 重要 的 底线 为了 得到 最好 
的 效果 我们 用 最 有可能 的 标记 标注 每个 
词 让 我们 找出 哪个 标记 是 最 有可能 的 
tags = tag for word tag in brown . tagged 
_ words categories = news nltk . FreqDist tags . 
max NN 2/m ./i 创建/v 一个/m 将/d 所有/b 词/n 都/d 
标注/v 成/n NN/w 的/uj 标注/v 器/n raw/w = I do 
not like green eggs and ham I do not like 
them Sam I am tokens = word _ tokenize raw 
default _ tagger = nltk . DefaultTagger NN default _ 
tagger . tag tokens I NN do NN not NN 
like NN green NN eggs NN and NN ham NN 
NN I NN do NN not NN like NN them 
NN Sam NN I NN am NN NN 3 . 
不出所料 这种 方法 的 表现 相当 不好 在 一个 典型 
的 语料库 中 它 只 标注 正确 了 八分之一 的 
标识符 正如 我们 在 这里 看到 的 default _ tagger 
. evaluate brown _ tagged _ sents Out 13 0 
. 1 3 0 8 9 4 8 4 2 
5 7 2 1 5 0 2 8 默认 的 
标注 器 给 每一个 单独 的 词 分配 标记 即使 
是 之前 从未 遇到 过 的 词 碰巧 的 是 
一旦 我们 处理 了 几千 词 的 英文 文本 之后 
大多数 新词 都将 是 名词 正如 我们 将 看到 的 
这 意味着 默认 标注 器 可以 帮助 我们 提高 语言 
处理 系统 的 稳定性 3.2 正则表达式 标注 器 正则表达式 标注 
器 基于 匹配 模式 分配 标记 给 词 符 例如 
我们 可能会 猜测 任一 以 ed 结尾 的 词 都是 
动词 过去分词 任一 以 s 结尾 的 词 都是 名词 
所有格 可以 用 一个 正则表达式 的 列表 表示 这些 patterns 
= . . . r . ing $ VBG # 
gerunds . . . r . ed $ VBD # 
simple past . . . r . es $ VBZ 
# 3rd singular present . . . r . ould 
$ MD # modals . . . r . s 
$ NN $ # possessive nouns . . . r 
. s $ NNS # plural nouns . . . 
r ^ 0 9 + . 0 9 + $ 
CD # cardinal numbers . . . r . * 
NN # nouns default . . . 请注意 这些 是 
顺序 处理 的 第一 个 匹 配上 的 会被 使用 
现在 我们 可以 建立 一个 标注 器 并用 它 来 
标记 一个 句子 做完 这 一步 会 有约 五分之一 是 
正确 的 regexp _ tagger . evaluate brown _ tagged 
_ sents Out 21 0 . 2 0 3 2 
6 3 9 1 7 8 9 4 8 6 
2 4 5 3 . 3 查询 标注 器 3.4 
N gram 标注 一元 标注 器 基于 一个 简单 的 
统计算法 对 每个 标识符 分配 这个 独特 的 标识符 最 
有可能 的 标记 例如 它 将 分配 标记 tt class 
= doctest JJ / tt 给 词 frequent 的 所有 
出现 因为 frequent 用作 一个 形容词 例如 a frequent word 
比 用作 一个 动词 例如 I frequent this cafe 更 
常见 一个 一元 标注 器 的 行为 就 像 一个 
查找 标注 器 4 除了 有 一个 更 方便 的 
建立 它 的 技术 称为 训练 一个 n gram tagger 
标注 器 是 一个 一元 标注 器 的 一般化 它 
的 上下文 是 当前 词 和它/nr 前面 n 1个 标识符 
的 词性 标记 1 gram 标注 器 是 一元 标注 
器 另一个 名称 即/v 用于/v 标注/v 一个词/i 符的/nr 上下文/l 的/uj 
只是/c 词/n 符/v 本身/r 2 gram 标注 器 也 称为 
二元 标注 器 3 gram 标注 器 也 称为 三元 
标注 器 5 . 组合 标注 器 尝试 使用 二元 
标注 器 标注 标识符 如果 二元 标注 器 无法 找到 
一个 标记 尝试 一元 标注 器 如果 一元 标注 器 
也 无法 找到 一个 标记 使用 默认 标注 器 大多数 
NLTK 标注 器 允许 指定 一个 回退 标注 器 回退 
标注 器 自身 可能 也 有一个 回退 标注 器 t0 
= nltk . DefaultTagger NN t1 = nltk . UnigramTagger 
train _ sents backoff = t0 t2 = nltk . 
BigramTagger train _ sents backoff = t1 t2 . evaluate 
test _ sents 0.844513 . . . 4 . 文本 
分类 4.1 词类 分类 在 一般 情况下 语言学家 使用 形态学 
句法 和 语义 线索 确定 一个 词 的 类别 形态学 
线索 一个词 的 内部 结构 可能 为 这个 词 分类 
提供 有用 的 线索 举例来说 ness 是 一个 后缀 与 
形容词 结合 产生 一个 名词 如 happy → happiness ill 
→ illness 如果 我们 遇到 的 一个 以 ness 结尾 
的 词 很 可能 是 一个 名词 同样 的 ment 
是 与 一些 动词 结合 产生 一个 名词 的 后缀 
如 govern → government 和 establish → establishment 英语 动词 
也 可以 是 形态 复杂 的 例如 一个 动词 的 
现在分词 以 ing 结尾 表示 正在 进行 的 还 没有 
结束 的 行动 如 falling eating ing 后缀 也 出现 
在 从 动词 派生 的 名词 中 如 the falling 
of the leaves 这 被 称为 动名词 句法 线索 另一个 
信息 来源 是 一个 词 可能 出现 的 典型 的 
上下文 语境 例如 假设 我们 已经 确定 了 名词 类 
那么 我们 可以 说 英语 形容词 的 句法 标准 是 
它 可以 立即 出现 在 一个 名词 前 或 紧 
跟在 词 be 或 very 后 根据 这些 测试 near 
应该 被 归类 为 形容词 s 2 a . the 
near windowb . The end is very near . 语义 
线索 最后 一个词 的 意思 对其 词汇 范畴 是 一个 
有用 的 线索 4.2 有 监督 分类 分类 是 为 
给定 的 输入 选择 正确 的 类 标签 的 任务 
在 基本 的 分类 任务 中 每个 输入 被 认为 
是 与 所有 其它 输入 隔离 的 并且 标签集 是 
预先 定义 的 这里 是 分类 任务 的 一些 例子 
判断 一封 电子邮件 是否 是 垃圾 邮件 从 一个 固定 
的 主题 领域 列表 中 如 体育 技术 和 政治 
决定 新闻 报道 的 主题 是 什么 决定 词 bank 
给定 的 出现 是 用来 指 河 的 坡岸 一个 
金融 机构 向一边 倾斜 的 动作 还是 在 金融 机构 
里 的 存储 行为 有 监督 分类 框架 a 在 
训练 过程 中 特征提取 器 用来 将 每一个 输入 值 
转换 为 特征 集 这些 特征 集 捕捉 每个 输入 
中 应被 用于 对其 分类 的 基本 信息 我们 将 
在下 一节 中 讨论 它 特 征集 与 标签 的 
配对 被 送入 机器学习 算法 生成 模型 b 在 预测 
过程 中 相同 的 特征 提取 器 被 用来 将 
未 见过 的 输入 转换 为 特征 集 之后 这些 
特征 集 被 送入 模型 产生 预测 标签 4 . 
2.1 性别/n 鉴定/v 男性/n 和/c 女性/n 的/uj 名字/n 有/v 一些/m 
鲜明/a 的/uj 特点/n 以 a e 和i/nr 结尾 的 很可能 
是 女性 而以 k o r s 和t/nr 结尾 的 
很可能 是 男性 创建 一个 分类器 的 第一步 是 决定 
输入 的 什么样 的 特征 是 相关 的 以及 如何 
为 那些 特征 编码 特征提取 函数 def gender _ features 
word . . . return { last _ letter word 
1 } 这个 函数 返回 的 字典 被称为 特 征集 
映射 特征 名称 到 它们 的 值 特征 名称 是 
区分 大小写 的 字符串 通常 提供 一个 简短 的 人 
可读 的 特征描述 例如 本例 中的 last _ letter 特征值 
是 简单 类型 的 值 如 布尔 数字 和 字符串 
准备 数据 一个 例子 和 对应 类 标签 的 列表 
from nltk . corpus import nameslabeled _ names = name 
male for name in names . words male . txt 
+ . . . name female for name in names 
. words female . txt import randomrandom . shuffle labeled 
_ names 使用 特征提取 器 处理 names 数据 并 划分 
特 征集 的 结果 链表 为 一个 训练 集 和 
一个 测试 集 训练 集 用于 训练 一个 新的 朴素 
贝叶斯 分类器 featuresets = gender _ features n gender for 
n gender in labeled _ names train _ set test 
_ set = featuresets 500 featuresets 500 classifier = nltk 
. N a i v e B a y e 
s C l a s s i f i e 
r . train train _ set 测试 classifier . classify 
gender _ features Neo male classifier . classify gender _ 
features Trinity female 准确度 print nltk . classify . accuracy 
classifier test _ set 检查 分类器 确定 哪些 特征 对于 
区分 名字 的 性别 是 最 有效 的 classifier . 
show _ most _ informative _ features 5 Most Informative 
Featureslast _ letter = a female male = 33.2 1 
. 0last _ letter = k male female = 32.6 
1 . 0last _ letter = p male female = 
19.7 1 . 0last _ letter = v male female 
= 18.6 1 . 0last _ letter = f male 
female = 17.3 1.04 . 2.2 选择 正确 的 特征 
def gender _ features2 name features = { } features 
first _ letter = name 0 . lower features last 
_ letter = name 1 . lower for letter in 
a b c d e f g h i j 
k l m n o p q r s t 
u v w x y z features count { } 
. format letter = name . lower . count letter 
features has { } . format letter = letter in 
name . lower return featuresgender _ features2 John { count 
j 1 has d False count b 0 . . 
. } 然而 你 要 用于 一个 给定 的 学习 
算法 的 特征 的 数目 是 有限 的 如果 你 
提供 太多 的 特征 那么 该 算法 将 高度 依赖 
你 的 训练 数据 的 特性 而 一般化 到 新的 
例子 的 效果 不 会 很好 这个 问题 被 称为 
过拟合 当 运 作在 小 训练 集上 时 尤其会 有问题 
一旦 初始 特 征集 被 选定 完善 特 征集 的 
一个 非常 有 成效 的 方法 是 错误 分析 首先 
我们 选择 一个 开发 集 包含 用于 创建 模型 的 
语料 数据 然后 将 这种 开发 集 分为 训练 集 
和 开发 测试 集 训练 集 用于 训练 模型 开发 
测试 集 用于 进行 错误 分析 测试 集 用于 系统 
的 最终 评估 用于 训练 有 监督 分类器 的 语料 
数据组织 图 语料 数据 分为 两类 开发 集 和 测试 
集 开发 集 通常 被 进一步 分为 训练 集 和 
开发 测试 集 使用 开发 测试 集 我们 可以 生成 
一个 分类器 预测 名字 性别 时的/nr 错误 列表 errors = 
for name tag in devtest _ names guess = classifier 
. classify gender _ features name if guess = tag 
errors . append tag guess name for tag guess name 
in sorted errors . . . print correct = { 
8 } guess = { 8s } name = { 
30 } . format tag guess name 浏览 这个 错误 
列表 它 明确 指出 一些 多 个 字母 的 后缀 
可以 指示 名字 性别 例如 yn 结尾 的 名字 显示 
以 女性 为主 尽管 事实上 n 结尾 的 名字 往往 
是 男性 以 ch 结尾 的 名字 通常 是 男性 
尽管 以 h 结尾 的 名字 倾向于 是 女性 因此 
调整 我们 的 特征提取 器 包括 两个 字母 后缀 的 
特征 train _ set = gender _ features n gender 
for n gender in train _ names devtest _ set 
= gender _ features n gender for n gender in 
devtest _ names classifier = nltk . N a i 
v e B a y e s C l a 
s s i f i e r . train train 
_ set print nltk . classify . accuracy classifier devtest 
_ set 这个 错误 分析 过程 可以 不断 重复 检查 
存在 于由新/nr 改进 的 分类器 产生 的 错误 中 的 
模式 每一次 错误 分析 过程 被 重复 我们 应该 选择 
一个 不同 的 开发 测试 / 训练 分割 以 确保 
该 分类器 不会 开始 反映 开发 测试 集 的 特质 
4.3 词性 标注 训练 一个 分类器 来 算出 哪个 后缀 
最有 信息量 定义 一个 特征提取 器 函数 检查 给定 的 
单词 的 这些 后缀 训练 一个 新的 决策树 的 分类器 
决策树 模型 的 一个 很好 的 性质 是 它们 往往 
很容易 解释 我们 甚至 可以 指示 NLTK 将 它们 以 
伪代码 形式 输出 s 4.4 探索 上下文 语境 通过 增加 
特征提取 函数 我们 可以 修改 这个 词性 标注 器 来 
利用 各种 词 内部 的 其他 特征 例如 词长 它 
所 包含 的 音节 数 或者 它 的 前缀 然而 
只要 特征提取 器 仅仅 看着 目标 词 我们 就 没法 
添加 依赖 词 出现 的 上下文 语境 特征 然而 上下文 
语境 特征 往往 提供 关于 正确 标记 的 强大 线索 
例如 标注 词 fly 如果 知道 它 前面 的 词 
是 a 将 使 我们 能够 确定 它 是 一个 
名词 而 不是 一个 动词 为了 采取 基于 词 的 
上下文 的 特征 我们 必须 修改 以前 为 我们 的 
特征提取 器 定义 的 模式 不是 只 传递 已 标注 
的 词 我们 将 传递 整个 未 标注 的 句子 
以及 目标 词 的 索引 很显然 利用 上下文 特征 提高 
了 我们 的 词性 标注 器 的 准确性 4.5 序列 
分类 一种 序列 分类器 策略 称为 连续 分类 或 贪婪 
序列 分类 是 为 第一 个 输入 找到 最 有可能 
的 类 标签 然后 使用 这个 问题 的 答案 帮助 
找到 下一个 输入 的 最佳 的 标签 首先 我们 必须 
扩展 我们 的 特征提取 函数 使其 具有 参数 tt class 
= doctest history / tt 它 提供 一个 我们 到 
目前 为止 已经 为 句子 预测 的 标记 的 列表 
1 tt class = doctest history / tt 中的 每个 
标记 对应 tt class = doctest sentence / tt 中 
的 一个 词 但是 请注意 tt class = doctest history 
/ tt 将 只 包含 我们 已经 归类 的 词 
的 标记 也 就是 目标 词 左侧 的 词 因此 
虽然 是 有可能 查看 目标 词 右边 的 词 的 
某些 特征 但 查看 那些 词 的 标记 是 不 
可能 的 因为 我们 还 未 产生 它们 4.6 其他 
有 监督 分类 例子 4 . 6.1 句子 分割 句子 
分割 可以 看作 是 一个 标点符号 的 分类 任务 每当 
我们 遇到 一个 可能 会 结束 一个 句子 的 符号 
如 句号 或 问号 我们 必须 决定 它 是否 终止 
了 当前 句子 第一步 是 获得 一些 已 被 分割 
成 句子 的 数据 将 它 转换 成 一种 适合 
提取 特征 的 形式 sents = nltk . corpus . 
treebank _ raw . sents tokens = boundaries = set 
offset = 0 for sent in sents . . . 
tokens . extend sent . . . offset + = 
len sent . . . boundaries . add offset 1 
tokens 是 单独 句子 标识符 的 合并 列表 boundaries 是 
一个 包含 所有 句子 边界 词 符 索引 的 集合 
下 一步 我们 需要 指定 用于 决定 标点 是否 表示 
句子 边界 的 数据 特征 def punct _ features tokens 
i . . . return { next word capitalized tokens 
i + 1 0 . isupper . . . prev 
word tokens i 1 . lower . . . punct 
tokens i . . . prev word is one char 
len tokens i 1 = = 1 } 基于 这一 
特征提取 器 我们 可以 通过 选择 所有 的 标点符号 创建 
一个 加标签 的 特 征集 的 列表 然后 标注 它们 
是否 是 边界 标识符 featuresets = punct _ features tokens 
i i in boundaries . . . for i in 
range 1 len tokens 1 . . . if tokens 
i in . 训练 并 评估 size = int len 
featuresets * 0.1 train _ set test _ set = 
featuresets size featuresets size classifier = nltk . N a 
i v e B a y e s C l 
a s s i f i e r . train 
train _ set nltk . classify . accuracy classifier test 
_ set 0 . 9360269360269364 . 6.2 识别 对话 行为 
类型 处理 对话 时 将 对话 看作 说话者 执行 的 
行为 是 很 有用 的 对于 表述 行为 的 陈述句 
这种 解释 是 最 直白 的 例如 I forgive you 
或 I bet you can t climb that hill 但是 
问候 问题 回答 断言/n 和/c 说明/v 都/d 可以/c 被/p 认为/v 
是/v 基于/p 语言/n 的/uj 行为/v 类型/n 识别 对话 中 言语 
下 的 对话 行为 是 理解 谈话 的 重要 的 
第一 步 可以 利用 这些 数据 建立 一个 分类器 识别 
新的 即时消息 帖子 的 对话 行为 类型 第一步 是 提取 
基本 的 消息 数据 下 一步 我们 将 定义 一个 
简单 的 特征提取 器 检查 帖子 包含 什么 词 最后 
我们 通过 为 每个 帖子 提取 特征 使用 post . 
get class 获得 一个 帖子 的 对话 行为 类型 构造 
训练 和 测试数据 并 创建 一个 新的 分类器 4 . 
6.3 识别 文字 蕴含 识别 文字 蕴含 RTE 是 判断 
文本 T 的 一个 给定 片段 是否 蕴含着 另 一个 
叫做 假设 的 文本 迄今为止 已经 有 4个 RTE 挑战赛 
在 那里 共享 的 开发 和 测试 数据 会 提供 
给 参赛 队伍 这里 是 挑战赛 3 开发 数据 集中 
的 文本 / 假设 对 的 两个 例子 标签 True 
表示 蕴含 成立 False 表示 蕴含 不成立 5/m ./i 评估/vn 
5.1/mx 测试/vn 集/q 5.2/mx 准确度/n 5.3/mx 召回率/i 和F值/nr 5.4/mx 混淆/v 
矩阵/n 5.5/mx 交叉/n 验证/v 6/m ./i 从/p 文本/n 提取/v 信息/n 
6.1/mx 信息提取/l 从/p 文本/n 获取/v 意义/n 的/uj 方法/n 被/p 称为/v 
信息提取/l 6/m ./i 1.1/mx 信息/n 提取/v 的/uj 架构/n 6/m ./i 
1.2/mx 词/n 块/zg 划分/v 用于/v 实体/n 识别/v 的/uj 基本/n 技术/n 
是/v 词/n 块/zg 划分/v 它/r 分割/v 和/c 标注/v 多词/i 符的/nr 
序列/n 小 框 显示 词 级 分词 和 词性 标注 
大 框 显示 高 级别 的 词 块 划分 每个 
这种 较大 的 框 叫做 一个词 块 就像 分词 忽略 
空白符 词 块 划分 通常 选择 词 符的/nr 一个 子集 
同样 像 分词 一样 词 块 划分 器 生成 的 
片段 在 源 文本 中 不能 重叠 名词 短 语词 
块 划分 首先 思考 名词 短 语词 块 划分 或 
NP 词 块 划分 任务 在 那里 我们 寻找 单独 
名词 短语 对应 的 词 块 词 块 信息 最 
有用 的 来源 之一 是 词性 标记 这是 在 我们 
的 信息 提取 系统 中 进行 词性 标注 的 动机 
之一 为了 创建 一个 词 块 划分 器 我们 将 
首先 定义 一个词 块 语法 由 指示 句子 应 如何 
进行 词 块 划分 的 规则 组成 标记 模式 组成 
一个 词 块 语法 的 规则 使用 标记 模式 来 
描述 已 标注 的 词 的 序列 一个 标记 模式 
是 一个 词性 标记 序列 用 尖括号 分隔 如 DT 
JJ * NN 用 正则表达式 进行 词 块 划分 要 
找到 一个 给定 的 句子 的 词 块 结构 RegexpParser 
词 块 划分 器 以 一个 没有 词 符被/nr 划分 
的 平面 结构 开始 词 块 划分 规则 轮流 应用 
依次 更 新词 块 结构 一旦 所有 的 规则 都被 
调用 返回 生成 的 词 块 结构 探索 文本 语料库 
7 . 分析 句 子结构 python 自然语言 处理 各章 总结 
1 . 语言 处理 与 Python2 . 获得 文本 语料 
和 词汇 资源 3 . 处理 原始 文本 4 . 
编写 结构化 的 程序 5 . 分类 和 词汇 标注 
6 . 学习 分类 文本 7 . 从 文本 提取 
信息 8 . 分析 句 子结构 9 . 构建 基于 
特征 的 文法 10 . 分析 句子 的 含义 11 
. 语言学 数据管理 未完待续 . . . . . . 
cestella / NLPWithMahout GitHub 是 一个 使用 Mahout 实现 自然 
语言 处理 NLP Natural Language Processing 的 开源 项目 NLP 
一 词 来自于 统计 自然语言 处理 来自 google 的 研究 
主管 Peter Norvig 评价 这 本书 如果 有人 告诉 我 
在 一年 内能 赚 一百万 那么 就 只有 这 本书 
能 做到 我 复制 了 这本书 并 开始 启动 一个 
web 文本处理 公司 Apache Mahout is 一个 能够 运行 在 
Hadoop 上 的 分布式 机器学习 算法 高性能 库 可用 算法 
如下 类型 算法 Linear Algebra Stochastic Gradient Descent Linear Algebra 
Stochastic Singular Value Decomposition Classification Random Forests Classification Na ï 
ve Bayesian Classification Hidden Markov Models Clustering Normal and Fuzzy 
K Means Clustering Expectation Maximization Clustering Dirichlet Process Clustering Clustering 
Latent Dirichlet Allocation Clustering Spectral Clustering Clustering MinHash Clustering Pattern 
Mining Parallel FP GrowthMahout 提供 了 很多 工具 库 允许 
从 hadoop 中 以 ML 算法 格式 获取数据 基本 模式 
有 1 . 将 文本 转为 序列 文件 SequenceFiles 通过 
seqdirectory 命令 2 . 将 序列 文件 转为 一 系列 
稀疏 向量 使用 seq2sparse 根据 选择 使用 word integer 和 
feature weight . 3 . 转换 与 稀疏 向量 关联 
的 Key 到 使用 rowid 命令 的 可 增量 整数 
另外 可在 Mahout 中 使用 Latent Dirichlet Allocation LDA 通过 
客户 一遍 一遍 购买 记录 能够 猜测 其 购买 偏好 
Mahout 是 LDA 原始 实现 的 性能 15倍 LDA 在 
Mahout 使用 方式 输入 数据 作为 一个 稀疏 向量 建立 
来自 文档 字段 的 管道 如下 三 个 步骤 1 
. seqdirectory 转换 包含 每行 一个 文档 的 系列 文档 
到 序列 文件 2 . seq2sparse 将 序列 文件 作为 
条目 字典 转为 稀疏 向量 3 . rowid 转为 稀疏 
向量 的 key 到 整数 cvb 工具 能够 运行 LDA 
算法 输入 是 字数 加权 频率 的 序列 文件 输出 
是 topic 模型 http / / www . jdon . 
com / 45591 周末 大家 都 出去 玩 而 我 
居然 被 人工智能 深深 吸引 其实 AI 还 处于 非常 
初级 的 发展 阶段 今天 看 的 是 人工智能 如何 
有效 地 运用 于 自然 语言 处理 1962年 Hubel 和 
Wiesel 通过 猫 视 皮层 细胞 的 研究 提出 了 
感受 野 的 概念 1984年 日本 学者 福岛 基于 感受 
野 概念 提出 的 神经 认知 机器 neocognitron 可以 看作 
是 卷积 神经 网络 的 第一 个 实现 网络 也 
是 第一个 将 接受 场 概念 应用于 人工 神经 领域 
网络 神经 认知 机器 将 视觉 图案 划分 为 多个 
子 图案 特征 然后 进入 用于 处理 的 分层 等级 
链接 特征 平面 它 试图 对 视觉 系统 进行 建模 
使其 即使 物体 被 移位 也 可能会 扭曲 或 稍微 
变形 同时 它 可以 完成 识别 如何 把 人工智能 运用 
到 伪 原创 技术上 通常 神经 认知 机器 包含 两种 
类型 的 神经元 即 携带 特征提取 的 元素 和 抵抗 
变形 的 C 元素 元素 包含 两个 重要 参数 即 
感受 野 和 阈值 参数 前者 确定 输入 连接 的 
数量 后者 控制 对 特征 子模式 的 响应 许多 学者 
一直 致力 于 提高 神经 认知 机器 的 性能 在 
传统 的 神经 认知 机器 中 C 变形 引起 的 
视觉 模糊 通常 分布 在 每个 元件 的 光敏 区域 
中 如果 由 光感应 区域 的 边缘 产生 的 模糊 
效果 大于 中心 的 模糊 效果 则 元件 将 接受 
由 这种 非正常 模糊 造成 的 较大 变 形容 差 
我们 希望 获得 的 是 接受 场 边缘 处 的 
训练 模式 和 变形 刺激 模式 之间 的 差异 以及 
在 中心 处 产生 的 效果 变得 越来越 大 为了 
有效 地 形成 这种 非正常 的 模糊性 福岛 提出 了 
一种 改进 的 具有 双C/nr 元件 层 的 神经 认知 
机器 Van Ooyen 和 Niehuis 引入 了 一个 新 参数 
来 提高 神经 认知 机器 的 识别 能力 事实上 这个 
参数 作为 抑制 信号 抑制 神经元 对 重复 激励 特性 
的 激励 大多数 神经网络 记住 权重 中的 训练 信息 根据 
Hebb 学习 规则 特征 被 训练 的 次数 越多 在 
后面 的 识别 过程 中 检测 它 越 容易 一些 
学者 还将 进化 计算 理论 与 神经 认知 机器 相结合 
削弱 了 对 重复 刺激 特征 进行 训练 和 学习 
的 能力 并使 网络 关注 这些 不同 的 特征 来 
提高 识别 能力 所有 这些 都是/nr 神经 认知 机器 的 
发展 过程 而 卷积 神经 网络 可以 看作 是 神经 
认知 机器 的 一种 普遍 形式 神经 认知 机器 是 
卷积 神经 网络 的 特例 对 人工智能 感兴趣 的 朋友 
可以 看看 我 其他 的 总结 文章 NLP 神经 网络 
实现 在 伪 原创 方面 的 运用 NLP 伪 原创 
技术 早期 并 不是 很 受欢迎 基于 主动 学习 的 
伪 原创 句法 识别 研究 小发 猫 人工智能 的 伪 
原创 工具 小发 猫 与 普通 伪 原创 工具 的 
区别 2017年 4月 21日 22日 由 映 魅 咨询 主办 
的 TAB Tech and Business 教育 科技 论坛 在 上海 
举行 十几 位 教育 投资 研究 机构 国内外 各类 教育 
科技 公司 的 嘉宾 围绕 教育 及 教育 科技 投资 
国际化 的 教育 产业链 以及 教育 科技 等 市场 关注 
的 热门 话题 进行 了 热烈 而 深远 的 分享 
与 探讨 在 本次 论坛 上 校 宝 在线 CTO 
孙琳 围绕 自然语言 处理 NLP 的 历史 发展 成果 和 
难题 介绍 了 NLP 目前 在 教育 领域 的 应用 
情况 以下 是 孙博士 的 分享 内容 精选 大家好 我 
是 孙琳 很 高兴 参加 TAB 教育 科技 论坛 今天 
分享 的 题目 是 教育 应用 中 的 自然 语言 
处理 首先 我 先 做 一下 自我介绍 我 是 剑桥 
大学 计算机 系 的 博士 博士 研究 的 方向 是 
自然 语言 处理 2011年 的 时候 我们 一起 创立 了 
校 宝 在线 的 前身 当时 就 想要 把 自然 
语言 处理 的 技术 用在 教育 当中 校 宝 在线 
的 业务 是 为 中国 的 民办 学校 提供 SaaS 
解决方案 包括 ERP IMS 等 同时 把 人工智能 的 技术 
应用 在 这些 软件 解决方案 当中 我 自己 在 业余 
时间 还 保持 做 研究 目前 是 剑桥 大学 语言 
实验室 的 研究员 每年 还 会 坚持 发 Paper 同时 
也 是 很多 学术 杂志 和 国际 会议 的 审稿人 
今天 为什么 给 大家 讲 自然语言 处理 和 教育 这个 
题目 呢 其实 大家 都 知道 人工智能 特别 在 教育 
中 的 应用 是 最近 的 热点 相关 的 信息 
也 非常 多 相信 大家 也 可以 看到 但是 作为 
人工智能 当中 一个 非常 重要 的 领域 自然语言 处理 跟 
教育 结合 的 相关 信息 却 并 不是 特别 多 
我 自己 也 找过 无论 是 中文 还是 英文 都 
不多 我 想 这 是 一个 非常 好 的 机会 
能把 我 自己 对于 这 方面 的 一些 思考 分享 
出来 给 供 在座 的 各位 大咖 和 各位 创业者 
们 做 一个 参考 一 自然语言 处理 NLP 关注 的 
核心 是 语言 和 文本 自然语言 处理 的 英文 是 
Natural language processing 简称 NLP 所以 我 下面 说到 NLP 
的 时候 大家 应该 能够 反应 出来 NLP 是 自然 
语言 处理 首先 跟 大家 介绍 一下 NLP 然后 说 
一下 NLP 在 教育 中 的 应用 最后 我 说 
一点 自己 的 结论 以及 我 自己 对 未来 的 
一点 展望 自然语言 处理 其实 是 人工智能 里面 一个 非常 
重要 的 分支 其他 的 分支 大家 也 非常 了解 
比如说 计算机 视觉 语音 包括 机器学习 深度 学习 这些 都是/nr 
人工智能 的 分支 它 也 常常 被 叫做 计算 语言学 
它 核心 的 目标 就是 把人 的 语言 也 就是 
自然 语言 转换 成 计算机 可以 执行 的 命令 简单 
来说 就是 让 计算机 读懂 人 的 语言 所以 说 
NLP 关注 的 核心 其实 是 语言 或者 更 通俗 
一点 来说 是 文本 二 自然语言 处理 NLP 的 难点 
理解 人 的 语言 不能 光靠 逻辑 还要/c 有/v 非常/d 
强的/nr 知识库/n 自然语言/l 处理/v 我 个人 认为 是 人工智能 领域 
里面 最难 的 一个 领域 它 最大 的 难点 在 
哪里 首先 因为 自然语言 处理 相对于 语音 和 视觉 来说 
是 高度 抽象化 的 表现 它 不是 信号 而是 一些 
非常 抽象化 的 理念 大家/n 都/d 认为/v 人类/n 的/uj 语言/n 
有/v 非常/d 强的/nr 逻辑性/n 其实 人类 的 语言 逻辑性 并不 
强 我 给 大家 举 一个 例子 大家 看 这 
句话 我 从来 没 说 他 偷过 钱 这 句话 
有 6种 理解 方法 我 一一列 出来了 比如说 我 可以 
这么 来说 我 从来 没 说 他 偷过 钱 这个 
意思 就是 可能 别人 说过 但是 我 没有 说 第三 
个 可以 说 我 从来 没有 说 他 偷过 钱 
可能 我 确实 没有 说 但是 我 用 其他 的 
方式 暗示 过 除了 这 6种 以外 如果 把 这个 
句子 加长 的话 变成 我 从来 没 说 他 偷过 
我 的 钱 那么 就有 7种 解释 不光 有1到/nr 6 
还有 第 7种 解释 这个 句子 可以 变得 更长 这个 
歧义 就会 更多 对于 计算机 来讲 如果 单单 给 它 
这一句 输入 要 做到 真正 语境 上 的 理解 是 
不 可能 的 事情 要做到 真实 语境 上 的 理解 
可能 需要 更多 的 辅助 信息 和 上下文 的 信息 
不然 是 没有 任何 可能性 的 其次 我们 要 理解 
人 的 语言 不能 光靠 逻辑 还要/c 有/v 非常/d 强的/nr 
知识库/n 要 有 很多 知识 才能 正确 理解 人类 语言 
我 举个 例子 下面 两 句话 中 第一 句话 We 
gave monkeys the bananas because they were hungry 这个 地方 
的 they 指 猴子 第二 句话 We gave monkeys the 
bananas because they were over ripe 这个 地方 的 they 
指 香蕉 对于 计算机 来说 这 两句话 看起来 结构 非常 
相似 句式 也 非常 类似 所以 计算机 必须 知道 猴子 
饿了 香蕉 不能 饿 猴子 不能 烂 的 香蕉 才能 
烂 才能 对 这 句话 有 一个 正确 的 理解 
不然 是 完全 无法 知道 再次 人 的 语言 还有 
一个 非常 大 的 特性 即 组 合性 我们 通过 
字母 组 合成词 通过 词 组合成 短语 短语 组成 句子 
句子 组成 段落 段落 组成 文章 如果 单单 抽出 里面 
一 部分 进行 解析 的话 比如说 解析 字母 解析 词 
我们 就 算 理解 了 词 的 意思 也 不能 
表现 出 人 本来 的 含义 因为 单个 抽出 词 
是 没有 意义 的 人 的 自然 语言 表达 的 
含义 往往 就 在 这些 组合 当中 恰恰 是 学习 
这些 复杂 的 组合 对于 计算机 来说 是 一件 非常 
难 的 事情 最后 人类 语言 是 非常 灵活 和 
开放 的 开放 是 什么 意思 人 的 语言 是 
随着 时间 而 改变 的 不停 的 有 新词 冒出来 
以前 词 的 意思 也会 随着 时间 有 完全 不同 
的 意思 比如说 灌水 潜水 这 两个 词 在 网络 
时代 有了/nr 完全 不同 的 含义 对于 计算机 来 说 
怎么 能够 实时 的 学会 这些 新词 发现 新的 用法 
也 是 非常 有 挑战性 的 三 自然语言 处理 NLP 
常用 的 三 种方法 1 机器学习 2 规则 和 逻辑 
3 语言学 研究 自然语言 处理 通常 有三/nr 种方法 第一种 机器 
学习 的 方法 也 包括 深度 学习 简单 来说 我 
们 收集 海量 的 文本 数据 建立 语言 模型 解决 
自然语言 处理 的 很多 任务 第二种 规则 和 逻辑 的 
方法 虽然 人 的 语言 不是 完完全全 有 逻辑 但是 
里面 还是 有 很强 的 逻辑性 的 一些 传统 的 
逻辑 原理 都 可以 用 在上面 其实 这 也是 人工智能 
最早 主要 的 研究 方法 只不过 90 年代 之后 大家 
逐渐 的 开始 更多 的 采用 机器学习 的 方法 而 
不是 采用 逻辑 和 规则 的 方法 现在 基本上 在 
自然 语言 处理 研究 当中 这 两个 占 的 比例 
是 二八开 逻辑 规则 和 机器 学习 的 比例 20% 
是 逻辑 和 规则 80% 是 机器学习 也有 两者 结合 
第三种 语言学 的 方法 因为 自然语言 处理 离不开 语言学 我们 
可以 把 自然 语言 处理 看成 语言学 下面 的 一个 
分支 不 单单 看成 人工智能 下面 的 一个 分支 语言学 
一句话 归纳 起来 就是 对人 的 语言 现象 的 研究 
它 不关心 怎么 写 得好 关心 的 是 你 写 
了 什么 所有 人 类 语言 现象 的 研究 都 
可以 归为 语言学 对于 语言学家 来说 他们 是 很多 自然语言 
处理 任务 的 设计师 由 他们 提出 问题 把 框架 
勾勒 出来 当然 解决 问题 则 要靠 研究 人员 用 
机器学习 规则 和 逻辑 的 方法 把 这个 框架 填上 
把 问题 解决 掉 四 自然语言 处理 NLP 的 成功 
应用 领域 搜索引擎 机器翻译 语音 识别 和 问答 系统 常见 
的 比较 成功 的 自然 语言 处理 的 应用 包括 
搜索引擎 机器翻译 语音 识别 和 问答 系统 其中 语音 识别 
技术 传统 上 来说 算是 自然语言 处理 下面 的 一个 
任务 但是 近些年 已经 单独 列 成 一个 研究 领域 
因为/c 在/p 目标/n 和/c 研究/vn 方法/n 上/f 和/c 自然/d 语言/n 
处理/v 是/v 迥异/a 的/uj 所以 往往 把 语音识别 单 列成 
跟 NLP 并排 的 研究 领域 五 自然语言 处理 NLP 
的 历史 与 深度 学习 关系密切 但 受 其 改进 
不大 下面 这个 图里 的 概念 大家 已经 非常 熟悉 
了 其实 它们 都是 人工智能 下面 的 子 领域 两者 
是 平行 的 而 深度 学习 是 机器 学习 的 
一个 子 领域 也 就是说 对于 自然 语言 处理 来说 
用 非 深度 学习 的 方法 来做 自然语言 处理 的 
任务 也 是 没有 问题 的 自然语言 处理 和 深度 
学习 之间 是 什么 关系 呢 深度 学习 为 自然语言 
处理 提供 了 很多 新的 模型 和 方法 因为 深度 
学习 最早 在 计算机 视觉 和 计算机 语音 方面 取得 
了 非常 重大 的 突破 所以 很 早就 被 用在 
NLP 的 各个 研究领域 当 中了 到 今天 为止 可以 
说 它 基本上 在 所有 NLP 的 任务 当中 都 
取得 了 成功 现在 对于 NLP 的 各种 任务 能/v 
见到/v 的/uj 最好/a 的/uj 模型/n 几乎/d 都/d 用到/v 深度/ns 学习/v 
了/ul 但是 跟 其他 领域 内 不 一样 的 是 
NLP 上面 深度 学习 带来 的 改进 并 不大 比如说 
我们 在 视觉 或者 在 语音 上面 错误率 的 降低 
可以 达到 40% 50% 但是 在 NLP 上面 超过 10% 
的 改进 都是 非常 少见 的 很多 都是 1% 2% 
的 改进 另外 还有 一个 非常 要命 的 问题 其实 
深度 学习 都是/nr 非常 复杂 的 非线性 模型 这 对于 
研究 人员 来说 也是 黑盒 所以 说 人类 很难 理解 
一个 模型 背后 所 代表 的 语言 学 现象 以及 
怎样 用 语言学 的 理论 去 解释 深度 学习 的 
模型 之所以 做 不到 这 一点 是 因为 我们 没有 
办法 把 深度 学习 模型 对于 很多 问题 的 解决方案 
放进 传统 的 语言学 框架 里面 这 对于 研究 人员 
来说 是 很大 的 一个 困扰 六 自然语言 处理 NLP 
的 现状 除了 语音 和 机器 翻译 领域 之外 很多 
方面 的 进展 并不大 目前 我们 已经 有 非常 好 
的 语音 识别 系统 了 现在 基本上 达到 了 人类 
的 水平 在 理想 环境 里 可以 达到 95% 以上 
的 正确率 同样 我们 也 有 比较 正确 的 机器 
翻译 系统 正确率 换算 过来 也 可以 有 70% 到 
80% 虽然 离人 的 水平 还有 一定 的 差距 但是 
已经 是 可用 的 状态 除了 这 两个 以外 自然语言 
处理 NLP 的 应用 目前 进展 不大 举 一个 最 
简单 的 例子 比如 词性 标注 在 一个 句子 当中 
动词 名词 形容词 这个 任务 是 非常 简单 非常 基础 
的 任务 但是 句子 级别 一句话 一个词 不错 才算 对 
目前 的 正确 率 只有 57% 而且 从 2009年 到 
2017 年间 正确率 提高 了 不到 1% 无论 使用 深度 
学习 各种 模型 各种 方法 花了 八年 时间 也 是 
只是 提高 了 不到 1% 另外 一个 例子 是 句法分析 
就 组合式 句法分析 来说 我们 今天 没有 比 十 一年前 
做得 更好 无论是 用 深度 学习 还是 其他 任何 方法 
十一 年 没有 改 进过 谷歌 在 去年 推出 了 
谷歌 SyntaxNet 号称 是 世界 上 面 最 优秀 的 
句法 分析器 其实 对比 四年前 最好 的 系统 也只 提了 
2% 当然 谷歌 用了 目前 最好 的 深度 学习 技术 
也 仅仅 做到 了 这样 还有 多轮 对话 系统 目前 
正确率 最多 只能 做到 60% 这 其实 是 完全 不 
可用 的 状态 深度 学习 的 模型 其实在 NLP 的 
各个 领域 都 取得 了 成功 不是 说 不成功 只是 
没有 取得 在 视觉 语音 领域 那么大 的 成功 七 
自然语言 处理 NLP 在 教育 领域 中 的 应用 在 
讨论 AI 的 时候 我心 里面 的 第一 反应 其实 
是 它 跟 教育 是 最 契合 的 一个 点 
但 大家 好像 提得 比较 少 我 觉得 语言 是 
大家 学习 的 对象 母语 或 外语 都是 对 自然 
语言 的 研究 第二 教师 的 授课 教材 也都 是 
自然 语言 所以 说 我 很 惊讶 的 发现 大家 
对 AI 展望 的 时候 有 时候 比较 忽略 NLP 
方面 的 一些 信息 这也 是 今天 我 为什么 会 
讲 这个 主题 的 原因 我 把 NLP 的 教育 
应用 分成 三大类 1 跟 语言 教学 相关 的 应用 
包括 外语 和 母语 教育 自动 评分 辅导 口语 写作 
等 2 教育 文本处理 一是 教材 的 编订 举个 例子 
在/p 所有/b 剑桥/ns 官方/n 出版/v 的/uj 英语/nz 教材/n 的/uj 封皮/n 
上面/f 都有/nr 黄色/n 的/uj 小/a 标志/n 估计 大家 买书 的 
时候 直接 忽略 掉了 那 上面 写 的 是 什么 
意思 呢 它/r 表示/v 这/r 本书/r 用/p 剑桥/ns 国际/n 语料库/n 
通过/p 语言学/n 和/c 自然/d 语言/n 处理/v 的/uj 方法/n 来/v 检测/vn 
书本/n 里面/f 内容/n 的/uj 正确性/n 和/c 适用性/n 而且是 在 非常 
大 的 大 数据 语料库 上面 完成 的 二 是 
文本 阅读 分级 大家 比较 熟悉 的 是 蓝思/nr 三 
是 文本 简化 生成 题目 3 对话 系统 使用 自然 
语言 进行 教学 让/v 每个/r 学生/n 都/d 能够/v 有/v 一个/m 
个人/n 学习/v 助理/vn 有 问题 可以 问 它 但是 目前 
来说 这 方面 的 应用 见到 的 系统 比较 少 
因为 在 基础 研究 上面 还是 需要 更大 的 进步 
才能 让 它 有 更好 的 应用 下面 看 几个 
具体 的 应用 NLP 和 教育 结合 的 第一 个 
应用 是 作文 打分 这是 成熟 的 应用 ETS E 
Rater 用在 托福 GMAT GRE 考试 当 中了 现在 考 
托福 写作 里面 一 部分 分数 是 电脑自动 评分 的 
ETS E Rater 和 人工 的 打分 数据 非常 接近 
了 我们 校 宝 在线 1Course 也 可以 达到 ETS 
E Rater 水平 而且 可以 给出 详细 的 反馈 我们 
不仅 会 给出 分数 而且会 给出 非常 详细 的 学习 
建议 以及 得分 的 要点 第二 个 应用 是 作文 
的 纠错 比如 学生 作文 当中 拼写 语法 以及 其他 
的 各种 错误 通过 计算机 看了 之后 可以 给 出 
相关 的 修改 建议 包括 润色 会 建议 学生 更 
高级 的 表达 更 符合 的 表达 这 方面 的 
提供 商蛮多/nr 的 我们 最早在 2011年 的 时候 推出 了 
一个 完全 免费 的 针对 个人 用户 的 产品 1Checker 
今天 完全 可以 用 但是 已经 很多 年 没有 更新 
过了 还有 其他 的 供应商 包括 国内 有句 酷 批改 
网 国际 上面 也有 Grammarly 等等 我 就 说 1Checker 
原理 是 通过 一个 语言 模型 用 计算机 阅读 学生 
的 作文 找出 可能 错 的 一些 点 然后 对 
这些 点 生成 不同 的 建议 最后 用 模型 根据 
用户 不同 的 水平 过虑 和 重新 对 建议 进行 
排序 这是 对于 纠错 方面 基本 的 原理 我 自己 
比较 惊讶 的 是 去年 华南理工大学 对于 市面 上面 很多 
作文 纠错 的 供应商 做 了 一个 对比 实验 发现 
1Checker 已经 三年 没有 更新 了 但是 依然 领先 于 
其他 的 供应商 因为 作文 纠错 是 作文 评分 的 
基础 我 相信 如果 他们 采用 我们 非 个人版 的 
系统 还会有 更大 的 提升 作文 纠错 和 作文 打分 
是 NLP 在 教育 当中 的 应用 最 成功 也是 
最受 人 关注 的 两块 其他 的 应用 包括 简答题 
的 评分 简答题/n 的/uj 自动/vn 评分/n 其实/d 是/v 只能/v 针对/p 
于有/nr 固定/a 答案/n 的/uj 非/h 开放性/n 的/uj 简答题/n 什么 叫做 
开放性 的 简答题 比如说 你 最 难忘 的 一件 事情 
这是 开放性 的 非 开放性 的 指 的 有 几套 
固定 答案 的 或者 让 你 描述 一个 现象 这些 
都 属于 可以 自动 批改 的 简答题 原理 上面 跟 
机器翻译 很 相似 把 学生 的 答案 和 正确 的 
答案 进行 比较 目前 来说 国际 上面 有 两套 比较 
通行 的 简答题 评分 的 引擎 一个 是 牛津 的 
那套 精度 非常 高 对于 每道 题 都要 手写 规则 
还有 一个 非常 成功 的 是 ETS E Rater 在 
某些 任务 当中 可以 达到 人 的 水平 下面 一个 
常见 的 应用 是 阅读 分级 大家 可能 听 说过 
蓝思/nr Lexile 阅读 分级 这 里面 涉及 到 两个 关键 
信息 词汇 频率 和 平均 句子 长度 其 实词 的 
频度 是 词汇 难度 的 表现 在 大 的 语料库 
和 文本 当中 比如说 所有 的 人民日报 或 其他 报纸 
如果 一个 词汇 少见 可能 就是 比较 难 的 词 
平均 句 长 是 语法 复杂度 的 体现 大家 觉得 
蓝思/nr Lexile 阅读 分级 的 算法 不难 但 它 的 
效果 是 非常 好 的 它 可以 给 利用计算机 给 
很多 的 文本 书籍 进行 自动 处理 分析 这些 书籍 
的 难度 然后 对于 不同 水平 的 学习者 给 他们 
提供 不同 难度 的 学习 资料 另外 一个 应用 是 
词汇 测试 我 在 国内 看到 的 比较 少 在 
欧洲 美国 看得 蛮 多 的 它 是 对于 词汇 
自动 生成 选择题 给定 一篇 文章 计算机 自动 根据 学习者 
的 水平 找到 合适 的 句子 找到 合适 的 词 
然后 自动 生成 迷惑 项 自动 生成 学生 的 练习题 
这个 好处 是 老师 不 需要 提前 对于 阅读 理解 
阅读 材料 或者 词汇 掌握情况 准备 只 需要 准备 阅读 
材料 就 好了 原理 和 步骤 1 找到 学习者 能够 
读懂 的 句子 2 找到 适合 他 水平 的 待 
测试 的 词 3 生成 迷惑 项 迷惑 项的/nr 生成 
很有 讲究 迷惑 项要/nr 足够 迷惑 才可以 它们/r 在/p 非常/d 
小/a 的/uj 上下文/l 里面/f 都是/nr 可以/c 讲得/i 通的/nr 但是 放在 
整句 当中 正确 的 只有 一个 最大化 他 的 迷惑 
性 最大化 测试 的 效果 这个 应用 在 国内 的 
见 的 不是 特别 的 多 八 自然语言 处理 NLP 
和 教育 结合 方面 的 研究 方向 自动 纠错 自动 
打分 问答 系统 对话 系统 目前 研究 的 方向 还 
是 主要 集中于 自动 纠错 和 自动 打分 我 估算 
了 一下 大体 占到 每年 Paper 发表 量 的 70% 
从 目前 自动 纠错 研究 来看 只有 40% 到 60% 
比例 的 错误 是 可以 被 检测 并 改正 的 
离人 的 水平 教师 的 水平 依然 是 非常 遥远 
的 从 目前 自动 打分 研究 来看 特定 任务 上面 
比 如是 托福 雅思 这种 应试 作文 上面 基本上 已经 
达到 了 人 的 水平 但是 对于 更有 挑战性 的 
文本 目前 也 处于 一个 停滞不前 的 状态 也 没有 
很大 的 突破 另外 一个 问答 系统 对话 系统 和 
学生 的 个人 助手 类似 这种 研究 相对 来说 并 
不是 特别 多 主要 原因 是 由于 这些 方面 需要 
基础 研究 层面 有 更大 的 突破 才能 在 教育 
应用 中 更好 的 找到 自己 的 一席之地 目前 主要 
的 两个 研究 机构 是 ETC 和 Cambridge assesment 九 
对 自然 语言 处理 NLP 未来 的 预期 应用 需求 
很广 但 还有 很多 难题 需要 继续 突破 尤其 是 
黑盒 问题 最后 给 大家 分享 一点 我 自己 的 
结论 通过 我 刚才 跟 大家 说 的 深度 学习 
可以 说 在 人工智能 应用 上面 已经 非常 成功 了 
但是 在 NLP 和 教育 结合 的 点上 不能通过 深度 
学习 在 人工智能 应用 上 的 成功 来 推测 NLP 
会在 教育 应用 中 或者 深度 学习 通过 NLP 在 
教育 中 的 应用 就 能 成功 这个 点 我 
是 完全 看不到 的 因为 首先在 NLP 的 研究 领域 
上面 深度 学习 就 没有 带来 像 视觉 语音 的 
突破 如果 再 应用 到 教育 上面 那 可能 是 
更 未来 的 事情 但 我 相信 这 也 不是 
一个 坏事 未来 还是 蛮 有 希望 的 我 希望 
深度 学习 包括 机器学习 在 对话 系统 问答 系统 有在/nr 
视觉 语音 上面 那么大 的 突破 通过 解决 根本 的 
问题 然后 可以 用 在 教育 中 这 是 非常 
大 的 需求 个人 的 智能 助理 可以 给 你 
一些 必要 的 帮助 就像 一个 虚拟 的 老师 一样 
另外 还有 一个 非常 难受 的 问题 即 黑盒 问题 
这是 教育 行业 一个 非常 特殊 的 需求 因为 深度 
学习 这种 模型 都是/nr 高度 非线性 的 非常 复杂 的 
模型 尤其 现在 流行 的 是 端 到 端 你 
给 我 输入输出 就行了 中间 完全 用 模型 搞定 人 
干预 的 地方 很少 那 问题 来了 对于 教育 来说 
往往 需要 的 不仅仅 是 一个 准确 的 结果 还 
需要 你 推理 的 过程 比如说 我 做 打分 分数 
正确 是 很重要 但是 对于 学生 来说 需要 知道 为什么 
得了 这个 分数 具体 哪 写 的 不好 怎么 改进 
对于 全 黑盒 的 模型 来说 即便 是 深度 学习 
最终 革新 了 NLP 大大 提高 了 NLP 任务 的 
准确度 可是/c 对/p 于/p 老师/n 还是/c 学生/n 来说/u 还是/c 很难/i 
读懂/v 和/c 解释/v 的/uj 这个 黑盒 问题 怎么 解决 也是 
需要 研究 人员 想 办法 的 上 一期 我们 为 
你 介绍 了 语音 识别 是 人机交互 的 入口 这 
一期 我们 介绍 什么 是 自然 语言 处理 以及 自然 
语言 处理 的 难点 圣经 里 有 一个 故事 讲 
巴比伦人 想 建造 一座 塔 直通 天堂 建 塔 的 
人都 说 着 同一 种 语言 心意 相通 齐心协力 上帝 
看到 人类 竟然 敢做 这种事情 就 让 他们 的 语言 
变得 不 一样 因为 人们 听 不懂 对方 在 讲 
什么 于是 大家 整天 吵吵闹闹 无法 继续 建 塔 后来 
人们 把 这座 塔 叫作 巴别塔 而 巴别 的 意思 
就是 分歧 虽然 巴别塔 没有 建成 但 让 全世界 拥有 
相通 的 语言 一直 是 萦绕 在 人们 心中 的 
梦想 但 人工智能 技术 实现 了 用 机器 翻译 不同 
的 语言 从 最初 只能 翻译 单词 到 现在 可以 
整句 或 通篇 翻译 近几年 用 语音 都 可以 直接 
进行 翻译 有了 它 你 可以 行 走到 世界 上 
任何 一个 国家 即使 看不懂 文字 听不懂 语言 也 能够 
借助 机器 翻译 与 他人 进行 交流 和 沟通 不必 
再 为 相互 不 能理解 而 困扰 然而 机器 翻译 
的 核心 就是 自然语言 处理 Natural Language Processing 简称 NLP 
什么 是 自然 语言 处理 简单 地 说 自然语言 处理 
就是 用 人工智能 来 处理 理解 以及 运用 人类 语言 
它 体现 了 真正 意义 上 的 人工智能 百度 机器学习 
专家 余凯 说过 听 与 看 说白 了 就是 阿猫 
和 阿狗 都会 的 而 只有 语言 才是 人类 独有 
的 也 就是说 只有 当 计算机 具备 了 处理 自然 
语言 的 能力 时 才算 实现 了 真正 的 智能 
自然语言 处理 技术 在 生活 中 应用 广泛 例如 机器翻译 
手写体 和 印刷体 字符识别 语音识别 后 实现 文字 转换 信息检索 
抽取 与 过滤 文本 分类 与 聚 类 舆情 分析 
和 观点 挖掘 等 它们 分别 应用 了 自然 语言 
处理 当中 的 语法分析 语义分析 篇章 理解 等 技术 是 
人工智能 界 最前沿 的 研究 领域 时至今日 AI 在 这些 
技术 领域 的 发展 已经 把 识别 准确率 从 70% 
提高 到了 90% 以上 但 只有 当 准确率 提高 到 
99% 及 以上 时 才能 被 认定 为 自然语言 处理 
的 技术 达到 人类 水平 这 仍然 是 巨大 的 
困难 和 挑战 自然语言 处理 存在 哪些 主要 困难 自然语言 
处理 的 困难 关键在于 消除歧义 问题 如 词 法分析 句法分析 
语义分析 等 过程 中 存在 的 歧义 问题 简称为 消 
歧 而 正确 的 消 歧 需要 大量 的 知识 
包括 语言 学 知识 如 词法 句法 语义 上下文 等 
和 世界 知识 与 语言 无关 由于 歧义 的 存在 
给 自然语言 处理 带来 两个 主要 困难 首先 当 语言 
中 充满 了 大量 的 歧义 分词 难度很大 同 一种 
语言 形式 可能 具有 多种 含义 特别 是 在 处理 
中文 单词 的 过程 中 由于 中 文词 与 词 
之间 缺少 天然 的 分隔符 因此 文字处理 比 英文 等 
西方 语 言多 一步 确定 词 边界 的 工序 即 
中文 自动 分词 任务 通俗 地 说 就是 要 由 
计算机 在 词 与 词 之间 自动 加上 分隔符 从而 
将 中文 文本 切分 为 独立 的 单词 例如 昨天 
有 沙尘暴 这句话 带有 分隔符 的 切分 文本 是 昨天 
| 有|/nr 沙尘暴 自动 分词 处于 中文 自然语言 处理 的 
底层 意味着 它 是 理解 语言 的 第一 道 工序 
但 正确 的 单词 切分 又 需要 取决于 对 文本 
语义 的 正确 理解 这 形成 了 一个 鸡 生蛋 
蛋 生鸡/nr 的 问题 成为 自然 语言 处理 的 第一 
条 拦路虎 除了 在 单个 词 级别 分词 和 理解 
存在 难度 外 在 短语 和 句子 级别 也 容易 
存在 歧义 例如 出口 冰箱 可以 理解 为 动宾 关系 
从 国内 出口 了 一批 冰箱 也 可以 理解 为 
偏正 关系 从 国内 出口 的 冰箱 又 如在 句子 
级别 做 化疗 的 是 她 的 妈妈 可以 理解 
为 她 妈妈 生病 了 需要 做 化疗 也 可以 
理解 为 她 妈妈 是 医生 帮 别人 做 化疗 
其次 消除歧义 所 需要 的 知识 在 获取 表达 以及 
运用 上 存在 困难 由于 语言 处理 的 复杂性 合适 
的 语言 处理 方法 和 模型 难以 设计 在 试图 
理解 一句话 的 时候 即使 不 存在 歧义 问题 我们 
也 往往 需要 考虑 上下文 的 影响 所谓 的 上下文 
指 的 是 当前 所说 这句话 所处 的 语言 环境 
包括 说 话人 所处 的 环境 或者 是 这 句话 
的 前 几句话 或者 后 几句话 等 以 小 A 
打了 小 B 因此 我 惩 罚了 他 为例 在 
其中 的 第二 句话 中的 他 是 指代 小 A 
还是 小 B 呢 要 正确 理解 这 句话 我们 
就 要 理解 上 句话 小 A 打了 小 B 
意味着 小 A 做得 不对 因此 第二句 中的 他 应当 
指代 的 是 小 A 由于 上下文 对于 当前 句子 
的 暗示 形式 是 多种多样 的 因此 如何 考虑 上下文 
影响 问题 是 自然 语言 处理 中 的 主要 困难 
之一 此外 正确理解 人类 语言 还要 有 足够 的 背景 
知识 特别 是 对于 成语 和 歇后语 的 理解 比如 
在 英语 中 The spirit is willing but the flesh 
is weak . 是 一句 成语 意思 是 心有余而力不足 但是 
曾经 某个 机器翻译 系统 将 这句 英文翻译 到 俄语 然后再 
翻译 回 英语 的 时候 却 变成 了 The Voltka 
is strong but the meat is rotten . 意思 是 
伏特加酒 是 浓 的 但 肉 却 腐 烂了 导致 
翻译 偏差 的 根本 问题 在于 机器翻译 系统 对于 英语 
成语 并无 了解 仅仅 是 从 字面 上 进行 翻译 
结果 失之毫厘 谬之千里 小结 自然语言 处理 就是 用 人工智能 来 
处理 理解 以及 运用 人类 语言 它 在 生活 中 
具有 广泛 的 应用 今天 在 一些 领域 比如 机器翻译 
其 处理 准确率 已经 超过 90% 但要 达到 人类 水平 
仍然 存在 较大 难度 消除歧义 是 目前 自然语言 处理 的 
最大 困难 它 的 根源 是 人类 语言 的 复杂性 
和 语言 描述 的 外部 世界 的 复杂性 人类 语言 
承担 着 人类 表达 情感 交流思想 传播 知识 等 重要 
功能 因此 需要 具备 强大 的 灵活性 和 表达 能力 
而 理解 语言所 需要 的 知识 又 是 无止境 的 
那么 目前 人们 是 如何 尝试 进行 自然语言 处理 的 
呢 预告 下 一篇 我们 将 结合 2017年 自然语言 处理 
的 最新 发展 趋势 来 介绍 对抗 神经网络 Gans 自然语言 
处理 之 动手 学 词 向量 word embedding 88人 已 
学习 课程 介绍 词 向量 Word embedding 是 深入 学习 
技术 在 自然 语言 处理 中 应用 的 基础 因此 
掌握 好词 向量 是 学习 深度 学习 技术 在 自然 
语言 处理 用 应用 的 重要 环节 课程 收益 本 
课程 从 One hot 编码 开始 word2vec fasttext 到 glove 
讲解 词 向量 技术 的 方方面面 每个/r 技术/n 点/m 环节/n 
都有/nr 相应/v 的/uj 小/a 案例/n 以 增加 同学 们 学习 
兴趣 同时 在 课程 最后 以 整合 案例 的 方式 
给 大家 展示 词 向量 技术 在 相似 度 计算 
中 的 典型 应用 希望 我们 的 课程 能 帮助 
更多 的 NLPper 讲师 介绍 杨帅 更多 讲师 课程 长期 
从事 机器学习 深度 学习 研究 在 自然 语言 处理 领域 
有 一定 认知 课程 大纲 第 1 章 One hot 
编码 1 . 课程 整体 介绍 及 大纲 剖析   
  10 242 . 什么 是 one hot 编码   
  5 033 . one hot 在 提取 文本 特征 
上 的 应用     4 454 . one hot 
编码 手动 实现     18 595 . ont hot 
编码 keras 中 实现     9 33 第 2 
章 word2vec 预备 基础 知识 及 相关 概念 1 . 
word2vec 的 前世 今生     7 272 . word2vec 
需要 注意 的 关键 点     8 233 . 
sigmoid 与 softmax 函数 讲解     4 324 . 
二叉树 相关 知识 讲解     6 385 . Huffman 
树 讲解     11 076 . Huffman 编码 讲解 
    7 467 . 语言 模型 讲解     
11 308 . 神经 网络 语言 模型 概念 讲解   
  6 399 . 神经 网络 语言 模型 数学 理论 
部分 讲解     9 08 第 3 章 word2vec 
实现 及 优化 方式 1 . word2vec 中 Skip Gram 
实现 方式 讲解     13 302 . word2vec 中 
CBOW 实现 方式 讲解     11 153 . word2vec 
训练 方式 负 采样 讲解     13 094 . 
word2vec 训练 方式 层序 softmax 讲解     12 57 
第 4 章 word2vec 之 Tensorflow 实现 1 . 读取 
停用词     10 362 . 文本 预处理 上   
  15 513 . 文本 预处理 下     12 
404 . 文本 编码 处理 讲解     18 305 
. 批量 数据 生成 讲解     25 016 . 
遗留 问题 解决 讲解     7 027 . word2vec 
模型 实现 讲解     24 358 . word2vec 模型 
训练 讲解     23 379 . word2vec 可视化 展示 
    19 32 第 5 章 word2vec 之 gensim 
工具包 使用 1 . gensim 中 word2vec 参数 讲解   
  8 262 . gensim word2vec 实战 之 加载 停用词 
    6 063 . gensim word2vec 实战 之 文本 
预处理     15 104 . gensim word2vec 实战 之 
模型 训练     9 465 . gensim word2vec 实战 
之 模型 保存 与 加载     7 486 . 
gensim word2vec 实战 之 应用 讲解     13 31 
第 6 章 fasttext 理论 部分 1 . fasttext 之 
Subword n gram 讲解     10 472 . fasttext 
之 分层 softmax 讲解     11 55 第 7 
章 fasttext 之 文本 分类 实战 及 词 向量 训练 
1 . fasttext 实战 之 数据集 简介 及 停用词 加载 
    6 372 . fasttext 实战 之 文本 预处理 
    14 243 . fasttext 实战 之 文本 分类 
模型 训练     9 334 . fasttext 实战 之 
模型 使用 讲解     12 595 . fasttext 实战 
之 训练 词 向量     9 34 第 8 
章 Glove 理论 部分 1 . 什么 是 Glove 讲解 
    7 122 . Glove 如何 实现 讲解   
  9 203 . Glove 如何 训练 讲解     
12 304 . Glove 数学原理 讲解 上     10 
015 . Glove 数学原理 讲 解下     7 50 
第 9 章 Glove 实战 部分 1 . Glove 实战 
是 初识 Glove     13 182 . Glove 实战 
之 求 近义词     7 073 . Glove 实战 
之 求 类比 词     8 40 第 10 
章 综合 案例 短 文本 标题 相似 度 检测 及 
计算 1 . 项目 实战 之 项目 简介 及 数据 
集 介绍     5 222 . 项目 实战 之 
GrobalParament 模块 编写     10 403 . 项目 实战 
之 utils 模块 中 读取 停用词 方法 编写     
9 574 . 项目 实战 之 utils 模块 中 分词 
方法 封装     9 125 . 项目 实战 之 
utils 模块 中 文本 预处理 方法 编写     14 
396 . 项目 实战 之 utils 模块 中 文本 预处理 
优化     11 417 . 项目 实战 之 train 
_ model 模块 之 word2vec 训练     9 288 
. 项目 实战 之 训 练好 的 word2vec 模型 剖析 
    10 269 . 项目 实战 之 word2vec 整体 
训练     9 5910 . 项目 实战 之 相似 
度 计 算上     10 5911 . 项目 实战 
之 相似 度 计算 中     18 1312 . 
项目 实战 之 相似 度 计 算下     13 
0113 . 项目 实战 之 结果 输出     18 
5414 . 项目 实战 整体 总结     10 17 
大家 可以 点击 查看 详情 查看 我 的 课程 作者简介 
洪亮 劼 Etsy 数据 科学 主管 前 雅虎 研究院 高级 
经理 长期 从事 推荐 系统 机器 学习 和 人工智能 的 
研究 工作 在 国际 顶级 会议 上 发表 论文 20 
余篇 长期 担任 多个 国际 著名 会议 及 期刊 的 
评审 委员会 成员 和 审稿人 责编 何永灿/nr heyc @ csdn 
. net 本文 为 程序员 原创 文章 未经 允许 不得 
转载 更多 精彩文章 请 订阅 程序员 涉及 自然语言 处理 人工智能 
机器学习 等 诸多 理论 以及 技术 的 顶级 会议 第 
55届 计算 语言学 年会 The 55th Annual Meeting of the 
Association for Computational Linguistics 简称 ACL 会议 于 今年 7月 
31日 8月 4日 在 加拿大 温哥华 举行 从 近期 谷歌 
学术 Google Scholar 公布 的 学术 杂志 和 会议 排名 
来看 ACL 依然 是 最重要 的 自然 语言 处理 相关 
的 人工智能 会议 因为 这个 会议 的 涵盖 面 非常 
广泛 且 理论 文章 较多 一般 的 读者 很难 从 
浩如烟海 的 文献 中 即刻 抓取 到有 用 信息 这里/r 
笔者/n 从/p 众多/m 文章/n 中/f 精选/v 出/v 5篇/mq 有/v 代表性/n 
的/uj 文章/n 为 读者 提供 思路 Multimodal Word Distributions 摘要 
本文 的 核心 思想 为 如何 用 Gaussian Mixture Model 
来 对 Word Embedding 进行 建模 从而 可以 学习 文字 
的 多重 表达 这篇文章 值 得对 Text Mining 有兴趣 的 
读者 泛读 文章 作者 Ben Athiwaratkun 是 康奈尔 大学 统计 
科学系 的 博士生 Andrew Gordon Wilson 是 新 加入 康奈尔大学 
Operation Research 以及 Information Engineering 的 助理 教授 之前 在 
卡内基 梅隆 大学 担任 研究员 师从 Eric Xing 和 Alex 
Smola 教授 在 之前 其 在 University of Cambridge 的 
Zoubin Ghahramani 手下 攻读 博士 学位 这 篇 文章 主要 
研究 Word Embedding 其 核心 思想 是 想用 Gaussian Mixture 
Model 表示 每一个 Word 的 Embedding 最早 的 自然 语言 
处理 NLP 是 采用 了 One Hot Encoding 的 Bag 
of Word 的 形式 来 处理 每个字 这样/r 的/uj 形式/n 
自然/d 是/v 无法/n 抓住/v 文字/n 之间/f 的/uj 语义/n 和/c 更多/d 
有/v 价值/n 的/uj 信息/n 那么 之前 Word2Vec 的 想法 则是 
学习 一个 每个 Word 的 Embedding 也 就是 一个 实数 
的 向量 用于 表示 这个 Word 的 语义 当然 如何 
构造 这么 一个 向量 又 如何 学习 这个 向量 成为 
了 诸多 研究 的 核心 课题 在 ICLR 2015 会议 
上 来自 UMass 的 Luke Vilnis 和 Andrew McCallum 在 
Word Representations via Gaussian Embedding 文章 中 提出 了 用 
分布 的 思想 来 看待 这个 实数 向量 的 思想 
具体说来 就是 认为 这个 向量 是 某个 高斯分布 的 期望 
然后 通过 学习 高斯分布 的 参数 也 就是 期望 和 
方差 来 最终 学习 到 Word 的 Embedding Distribution 这一步 
可以 说 是 扩展 了 Word Embedding 这一 思想 然而 
用 一个 分布 来 表达 每一个 字 的 最 直接 
的 缺陷 则是 无法 表达 很 多字 的 多重 意思 
这也 就 带来 了 这篇文章 的 想法 文章 希望 通过 
Gaussian Mixture Model 的 形式 来 学习 每个 Word 的 
Embedding 也 就是说 每个字 的 Embedding 不是 一个 高斯分布 的 
期望 了 而是 多个 高斯分布 的 综合 这样 就 给 
了 很多 Word 多重 意义 的 自由度 在有 了 这么 
一个 模型 的 基础 上 文章 采用 了 类似 Skip 
Gram 的 来 学习 模型 的 参数 具体说来 文章 沿用 
了 Luke 和 Andrew 的 那篇 文章 所 定义 的 
一个 叫 Max margin Ranking Objective 的 目标 函数 并且 
采用 了 Expected Likelihood Kernel 来 作为 衡量 两个 分布 
之间 相似 度 的 工具 这里 就不 详细 展开 了 
有兴趣 的 读者 可以 精读 这 部分 细节 Skip Gram 
文章 通过 UKWAC 和 Wackypedia 数据集 学习 了 所有 的 
Word Embedding 所有 试验中 文章 采用 了 K = 2 
的 Gaussian Mixture Model 文章 也有 K = 3 的 
结果 比较 当然有 之前 Luke 的 工作 以及 其他 各种 
Embedding 的 方法 比较 的 内容 有 Word Similarity 以及 
对于 Polysemous 的 字 的 比较 总之 文章 提出 的 
方法 非常 有 效果 这 篇 文章 因为 也有 源代码 
基于 Tensorflow 推荐 有兴趣 的 读者 精读 Topically Driven Neural 
Language Model 摘要 文章 的 核心 思想 也是 之前 有 
不少 人 尝试 的 就是 把 话题 模型 Topic Model 
和 语言 模型 Language Model 相 结合 起来 这里 两种 
模型 的 处理 都 非常 纯粹 是 用 地道 的 
深度 学习 语言 构架 完成 用到 了 不少 流行 的 
概念 比如 GRU Attention 等 适合 文字 挖掘 的 研究 
人员 泛读 文章 的 作者 是 来自 于 澳大利亚 的 
研究 人员 第一 作者 Jey Han Lau 目前 在 澳大利亚 
的 IBM 进行 Topic Model 以及 NLP 方面 的 研究 
之前 也 在 第二 作者 Timothy Baldwin 的 实验室 做 
过 研究 第二 作者 Timothy Baldwin 和 第三 作者 Trevor 
Cohn 都在 墨尔本大学 长期 从事 NLP 研究 的 教授 这 
篇 文章 的 核心 思想 是 想 彻底 用 Neural 
的 思想 来 结合 Topic Model 和 Language Model 当然 
既然 这 两种 模型 都是/nr 文字 处理 方面 的 核心 
模型 自然 之前 就 有人 曾经 想过 要 这么 做 
不过 之前 的 不少 尝试 都是 要么 还想 保留 LDA 
的 一些 部 件 或者 往 传统 的 LDA 模型 
上去 靠 要么 是 并 没有 和 Language Model 结合 
起来 文章 的 主要 卖点 是 完全 用 深度 学习 
的 语言 来 构建 整个 模型 并且 模型 中的 Topic 
Model 模型 部分 的 结果 会 成为 驱动 Language Model 
部分 的 成分 概括 说来 文章 提出 了 一个 有 
两个 组成部分 的 模型 的 集合 文章 管 这个 模型 
叫 tdlm tdlm 模型 第一 个 部分 是 Topic Model 
的 部分 我们 已经 提过 这里 的 Topic Model 和 
LDA 已 相去甚远 思路 是 这样 的 首先 从 一个 
文字 表达 的 矩阵 中 有 可能 就 直接 是 
传统 的 Word Embedding 通过 Convolutional Filters 转换 成为 一些 
文字 的 特征 表达 Feature Vector 文章 里 选用 的 
是 线性 的 转换 方式 这些 Convolutional Filters 都是 作用 
在 文字 的 一个 Window 上面 所以 从 概念 上 
讲 这 一个 步骤 很 类似 Word Embedding 得到 这些 
Feature Vector 以后 作者 们 又 使用 了 一个 Max 
Over Time 的 Pooling 动作 也 就是 每 一组 文字 
的 Feature Vector 中 最大值 从而 产生 了 文档 的 
表达 注意 这里 学到 的 依然 是 比较 直接 的 
Embedding 然后 作者 们 定义 了 一组 Topic 的 产生 
形式 首先 是 有一个 输入 Topic 矩阵 这个 矩阵 和 
已经 得到 的 文档 特征 一起 产生 一个 Attention 的 
向量 这个 Attention 向量 再 和 输出 Topic 矩阵 一 
起作用 产生 最终 的 文档 Topic 向量 这 也 就是 
这 部分 模型 的 主要 部分 最终 这个 文档 Topic 
向量 通过 用于 预测 文档 中 的 每一个 字 来 
被 学习 到 有了/nr 这个 文档 Topic 向量 以后 作者 
们 把 这个 信息 用在 了 一个 基于 LSTM 的 
Language Model 上面 这 一部分 其实 就是 用 了 一个 
类似于 GRU 的 功能 把 Topic 的 信息 附加 在 
Language Model 上 文章 在 训练 的 时候 采用 了 
Joint 训练 的 方式 并且 使用 了 Google 发布 的 
Word2Vec 已经 Pre trained 的 Word Embedding 所 采用 的 
种种 参数 也都 在 文章 中 有 介绍 文章 在 
一些 数据 集上 做了 实验 对于 Topic 部分 来说 文章 
主要 和 LDA 做 比较 用了 Perplexity 这个 传统 的 
测量 还 比较 了 Topic Coherence 等 总体 说来 提出 
的 模型 和 LDA 不相上下 从 Language Model 的 部分 
来说 提出 的 模型 也在 APNews IMDB/w 和/c BNC/w 上都/i 
有/v 不错/a 的/uj Perplexity/w 值/n 总体 说来 这篇文章 值得 文字 
挖掘 的 研究者 和 NLP 的 研究 者 泛读 Towards 
End to End Reinforcement Learning of Dialogue Agents for Information 
Access 摘要 文章 介绍 如何 进行 端 到 端 End 
to End 的 对话 系统 训练 特别 是 有 数据库 
或者 知识库 查询 步骤 的 时候 往往 这一步 硬 操作 
阻止 了 端 到 端的 训练 流程 这 篇 文章 
介绍 了 一个 软 查询 的 步骤 使得 整个 流程 
可以 能够 融入 训练 流程 不过 从 文章 的 结果 
来看 效果 依然 很难说 能够 在 实际 系统 中 应用 
可以 说 这篇文章 有 很强 的 学术 参考价值 文章 作者群 
来自 于 微软 研究院 卡内基 梅隆 大学 和 台湾 国立大学 
文章 中 还有 Lihong Li 和 Li Deng 邓力/nr 这样 
的 著名 学者 的 影子 第一 作者 Bhuwan Dhingra 是 
在 卡内基 梅隆 大学 William W . Cohen 和 Ruslan 
Salakhutdinov 的 博士 学生 两位/m 导师/n 都/d 十分/m 有/v 名气/n 
而 这个 学生 这几年 在 NLP 领域 可以 说 是 
收获 颇丰 在 今年 的 ACL 上 已经 发表 2 
篇文章 在 今年 ICLR 和 AAAI 上 都有 论文 发表 
文章 的 核心 思想 是 如何 训练 一个 多轮 Multi 
turn 的 基于 知识库 Knowledge Base 的 对话 系统 这个 
对话 系统 的 目的 主要 是 帮助 用户 从 这个 
知识库 中 获取 一些 信息 那么 传统 的 基于 知识库 
的 对话 系统 的 主要 弊病 在于 中间 有 一个 
步骤 是 对于 知识库 的 查询 也 就是说 系统 必须 
根据 用户 提交 的 查询 Query 进行 分析 并且 产生 
结果 这一步 作者 们 称为 硬 查询 Hard Lookup 虽然 
这 一步 非常 自然 但是 阻断 了 Block 了 整个 
流程 使得 整个 系统 没法 端 到 端 End to 
End 进行 训练 并且 这一步 由于 是 硬 查询 并 
没有 携带 更多 的 不 确定 信息 不利于 系统 的 
整体 优化 这篇文章 其实 就是 想 提出 一种 软 查询 
从而 让 整个 系统 得以 端 到 端 End to 
End 进行 训练 这个 新 提出 的 软 查询 步骤 
和 强化 学习 Reinforcement Learning 相 结合 共同 完成 整个 
回路 从而 在 这个 对话 系统 上 达到 真正 的 
端 到 端 这 就是 整个 文章 的 核心 思想 
那么 这个 所谓 的 软 查询 是 怎么 回事 其实 
就是 整个 系统 保持 一个 对 知识库 中 的 所有 
本体 Entities 所 可能 产生 的 值 的 一个 后验/nr 
分布 Posterior Distribution 也 就是说 作者 们 构建 了 这么 
一组 后验/nr 分布 然后 可以 通过 对 这些 分布 的 
更新 这个 过程 是 一个 自然 获取 新 数据 并且 
更新 后验/nr 分布 的 过程 来 对 现在 所有 本体 
的确 信度 有 一个 重新 的 估计 这 一步 的 
转换 让/v 对话/n 系统/n 从和跟/nr 知识库/n 直接/ad 打交道/v 变成 了 
如何 针对 后验/nr 分布 打交道 显然 从 机器 学习 的 
角度 来说 和 分布 打交道 往往 容易 简单 很多 具体说来 
系统 的 后验/nr 分布 是 一个 关于 用户 在 第 
T 轮 针对 某个 值 是否 有 兴趣 的 概率分布 
整个 对话 系统 是 这样 运行 的 首先 用户 通过 
输入 的 对话 Utterance 来 触发 系统 进行 不同 的 
动作 Action 动作 空间 Action Space 包含 向 用户 询问 
某个 Slot 的 值 或者 通知 用户 目前 的 结果 
整个 系统 包含 三 个大 模块 Belief Trackers Soft KB 
Lookup 以及 Policy Network Belief Trackers 的 作用 是 对 
整个 系统 现在 的 状态 有 一个 全局 的 掌握 
这里 每一个 Slot 都 有一个 Tracker 一个 是 根据 用户 
当前 的 输入 需要 保持 一个 对于 所有 值 的 
Multinomial 分布 另外 的 则是 需要 保持 一个 对于 用户 
是否 知道 这个 Slot 的 值 的 置信 值 文章 
中 介绍 了 Hand Crafted Tracker 和 Neural Belief Tracker 
基于 GRU 的 细节 这里 就不 复述 了 有了 Tracker 
以后 Soft KB Lookup 的 作用 是 保持 一个 整个 
对于 本体 的 所有 值得 后验/nr 分布 最后 这些 后验/nr 
概率 统统 被 总结 到 了 一个 总结 向量 Summary 
Vector 里 这个 向量 可以 认为 是 把 所有 的 
后验/nr 信息 给 压缩 到 了 这个 向量 里 而 
Policy Network 则 根据 这个 总结 向量 来 选择 整个 
对话 系统 的 下 一个 动作 这里 文章 也 是 
介绍 了 Hand Crafted 的 Policy 和 Neural Policy 两种 
情况 整个 模型 的 训练 过程 还是 有 困难 的 
虽然 作者 用了 REINFORCE 的 算法 但是 作者 们 发现 
根据 随机 初始化 的 算法 没法 得到 想要 的 效果 
于是 作者 们 采用 了 所谓 的 Imitation Learning 方法 
也 就是说 最 开始 的 时候 去 模拟 Hand Crafted 
Agents 的 效果 在 这篇文章 里 作者 们 采用 了 
模拟器 Simulator 的 衡量 方式 具体说来 就是 通过 与 一个 
模拟器 进行 对话 从而 训练 基于 强化 学习 的 对话 
系统 作者 们 用了 MovieKB 来做 数据集 总体 说来 整个 
实验 部分 都 显得 比较 弱 没有 充足 的 真正 
的 实验 结果 整个 文章 真正 值得 借鉴 主要 是 
软 查询 的 思想 整个 流程 也 值得 参考 但是 
训练 的 困难 可能 使得 这个 系统 作为 一个 可以 
更加 扩展 的 系统 的 价值 不高 本文 值 得对 
对话 系统 有 研究 的 人 泛读 Learning to Skim 
Text 摘要 这 篇 文章 主要 介绍 如何 在 LSTM 
的 基础 上 加入 跳 转 机制 使得 模型 能够 
去 略过 不 重要 的 部分 而 重视 重要 的 
部分 模型 的 训练 利用 了 强化 学习 这 篇 
文章 建议 对 文字 处理 有 兴趣 的 读者 精读 
作者群 来自 Google 第一 作者 来自 卡内基 梅隆 大学 的 
Adams Wei Yu 在 Google 实习 的 时候 做 的 
工作 第三 作者 Quoc V . Le 曾是/nr Alex Smola 
和 Andrew Ng 的 高徒 在 Google 工作 期间 有 
很多 著名 的 工作 比如 Sequence to Sequence Model 来做 
机器翻译 Machine Translation 等 文章 想要 解决 的 问题 为 
Skim Text 简单 说来 就是 在 文字 处理 的 时候 
略过 不 重要 的 部分 对 重要 的 部分 进行 
记忆 和 阅读 要 教会 模型 知道 在 哪里 需要 
略过 不读 哪里 需要 重新 开始 阅读 的 能力 略过 
阅读 的 另外 一个 好处 则是 对 文字 整体 的 
处理 速度 明显 提高 而且 很 有可能 还会 带来 质量 
上 的 提升 因为 处理 的 噪声 信息 少了 垃圾 
信息 少了 具体说来 文章 是 希望 在 LSTM 的 基础 
上 加入 跳转 功能 从而 使得 这个 时序 模型 能够 
有 能力 判读 是否 要 略过 一部分 文字 信息 简单 
说来 作者 们 是 这么 对 LSTM 进行 改进 的 
首先 有 一个 参数 R 来 确定 要 读 多少 
文字 然后 模型 从 一个 0 到 K 的 基于 
Multinomial 分布 的 这一个 跳转 机制 中 决定 当前 需要 
往后 跳 多少 文字 可以 是 0 也 就是说 不 
跳转 这个 是否 跳转 的 这 一个 步骤 所 需要 
的 Multinomial 分布 则 也要 基于 当期 LSTM 的 隐 
参数信息 Hidden State 跳转 决定 以后 根据 这个 跳转 信息 
模型 会 看一下 是否 已经 达到 最大 的 跳转 限制 
N 如果 没有 则 往后 跳转 当 所有 的 这些 
步骤 都 走完 达到 一个 序列 往往 是 一个 句子 
结尾 的 时候 最后 的 隐 参数信息 会 用来 对 
最终 需要 的 目标 比如 分类 标签 进行 预测 文章 
的 另一 个 创新 点 就是 引入 了 强化 学习 
Reinforcement Learning 到 模型 的 训练 中 最终 从隐/nr 参数 
到 目标 标签 Label 的 这一步 往往 采用 的 是 
Cross Entropy 的 优化 目标函数 这 一个 选择 很 直观 
也 是 一个 标准 的 步骤 然而 如何 训练 跳转 
的 Multinomial 分布 因为 其 离散 Discrete 特质 则 成为 
文章 的 难点 原因 是 Cross Entropy 无法 直接 应用到 
离散 数据 上 那么 这篇文章 采取 的 思路 是 把 
这个 问题 构造 成为 强化 学习 的 例子 从而 使用 
最近 的 一些 强化 学习 思路 来 把 这个 离散 
信息 转化 为 连续 信息 具体说来 就是 采用 了 Policy 
Gradient 的 办法 在 每次 跳转 正确 的 时候 得到 
一个 为 + 1 的 反馈 反之 则是 1 这样 
就 把 问题 转换 成为 了 学习 跳转 策略 的 
强化 学习 模式 文章 采用 了 REINFORCE 的 算法 来 
对 这里 的 离散 信息 做 处理 从而 把 Policy 
Gradient 的 计算 转换 为 了 一个 近似 逼近 这样 
最终 的 目标 函数 来自于 三个 部分 第一 部分 是 
Cross Entropy 第二 部分 是 Policy Gradient 的 逼近 第三 
部分 则 是 一个 Variance Reduction 的 控制项 为了 优化 
更加 有效 整个 目标函数 就 可以 完整 得 被 优化 
了 文章 在 好多 种 实验 类型 上 做了 实验 
主要 比较 的 就是 没有 跳转 信息 的 标准 的 
LSTM 其实 总体 上 来说 很 多任务 Task 依然 比较 
机械 和 人工 比如 最后 的 用 一堆 句子 来 
预测 中间 可能 会 出现 的 某个 词 的 情况 
这样 的 任务 其实 并 不是 很 现实 但是 文章 
中 提到 了 一个 人工 Synthetic 的 任务 还 蛮有意思 
那/r 就是/d 从/p 一个/m 数组/n 中/f 根据 下标 为 0 
的 数 作为 提示 来 跳转 取得 相应 的 数 
作为 输出 这么 一个 任务 这个 任务 充分 地 展示 
了 LSTM 这类 模型 以及 文章 提出 的 模型 的 
魅力 第一 可以 非常 好 的 处理 这样 的 非线性 
时序 信息 第二 文章 提出 的 模型 比 普通 的 
LSTM 快 不少 并且 准确度 也 提升 很多 总体 说来 
这 篇 文章 非常 值得 对 时序 模型 有 兴趣 
的 读者 精读 文章 的 Related Work 部分 也 很 
精彩 对/p 相关/v 研究/vn 有/v 兴趣/n 的/uj 朋友/n 可以/c 参考/v 
这/r 部分/n 看看/v 最近/f 都有/nr 哪些/r 工作/vn 很/zg 类似/v From 
Language to Programs Bridging Reinforcement Learning and Maximum Marginal Likelihood 
摘要 这 篇 文章 要 解决 的 问题 是 如何 
从 一段 文字 翻译 成为 程序 的 问题 文章 适合 
对 Neural Programming 有兴趣 的 读者 泛读 作者群 来自 斯坦福大学 
主要 作者 来自 Percy Liang 的 实验室 最 近几年 Percy 
Liang 的 实验室 可以说 收获 颇丰 特别 是 在 自然 
语言 处理 和 深度 学习 的 结合 上 都有 不错 
的 显著 成果 这 篇 文章 里 有好 一些 值得 
关注 的 内容 首先 从 总体 上 来说 这 篇 
文章 要 解决 的 问题 是 如何 从 一段 文字 
翻译 成为 程序 的 问题 可以 说 是 一个 很 
有价值 的 问题 如果 这个 问题 能够 可以 容易 解决 
那么 我们 就 可以 教 会 计算机 编写 很多 程序 
而 不一定 需要 知道 程序 语言 的 细微 的 很多 
东西 从 细节 上 说 这个 问题 就是 给 定 
一个 输入 的 语句 一个 模型 需要 把 目前 的 
状态 转移到 下 一个 目标 状态 上 难点 在于 对于 
同一 个 输入 语句 从 当前 的 状态 到 可能 
会 到达 多种 目标 状态 这些 目标 状态 都 有可能 
是 对 当前 输入 语句 的 一种 描述 但是 正确 
的 描述 其实 是 非常 有限 的 甚至 是 唯一 
的 那么 如何 从 所有 的 描述 中 剥 离开 
不 正确 的 找到 唯一 的 或者 少量 的 正确 
描述 就 成为 了 这么 一个 问题 的 核心 文章 
中 采用 了 一种 Neural Encoder Decoder 模型 架构 这种 
模型 主要 是 对 序列 信息 能够 有 比较 好 
的 效果 具体说来 是 对于 现在 的 输入 语句 首先 
把 输入 语句 变换 成为 一个 语句 向量 然后 根据 
之前 已经 产生 的 程序 状态 以及 当前 的 语句 
向量 产生 现在 的 程序 状态 在 这个 整个 的 
过程 中 对于 Encoder 作者 们 采用 了 LSTM 的 
架构 而 对于 Decoder 作者 们 采用 了 普通 的 
Feed forward Network 原因 文章 中 是 为了 简化 另外 
一个 比较 有 创新 的 地方 就是 作者 们 把 
过于 已经 产生 程序 状态 重新 Embedding 化 作者 们 
说是 叫 Stack 这 有一点 模仿 普通 数据结构 的 意思 
那么 这个 模型 架构 应该 是 比较 经典 的 文章 
这时候 引出 了 另外 一个 本文 的 主要 贡献 那 
就是 对模型 学习 的 流程 进行 了 改进 为了 引出 
模型 学习 的 改进 作者 们 首先 讨论 了 两种 
学习 训练 模式 的 形式 那 就是 强化 学习 Reinforcement 
Learning 以及 MML Maximum Marginal Likelihood 的 目标 函数 的 
异同 文章 中 提出 两 者 非常 类似 不过 比 
较小 的 区别 造成了 MML 可以 更加 容易 避开 错误 
程序 这一 结果 文章 又 比较 了 基于 REINFORCE 算法 
的 强化 学习 以及 基于 Numerical Integration 以及 Beam Search 
的 MML 学习 的 优劣 总体 说来 REINFORCE 算法 对于 
这个 应用 来说 非常 容易 陷入 初始状态 就 不太 优 
并且 也 很难 Explore 出来 的 情况 MML 稍微 好 
一些 但 依然 有 类似 问题 文章 这里 提出 了 
Randomized Beam Search 来 解决 也 就是说 在做 Beam Search 
的 时候 加入 一些 Exploration 的 成分 另外 一个 情况 
则是 在做 Gradient Updates 的 时候 当前 的 状态 会对 
Gradient 有影响 也 就是说 如果 当前 状态 差强人意 Gradient 也许 
就 无法 调整 到 应该 的 情况 这里 作者 们 
提出 了 一种 叫 Beta Meritocratic 的 Gradient 更新 法则 
来 解决 当前 状态 过于 影响 Gradient 的 情况 实验 
的 部分 还 是 比较 有 说服力 的 详细 的 
模型 参数 也 一应俱全 对于 提出 的 模型 来说 在三 
个 数据 集上 都有 不错 的 表现 当然 从 准确度 
上 来说 这种 从 文字 翻译 到 程序 状态 的 
任务 离 真正 的 实际 应用 还有 一段 距离 这篇文章 
适合 对于 最近 所谓 的 Neural Programming 有兴趣 的 读者 
泛读 对 怎么 改进 强化 学习 或者 MML 有兴趣 的 
读者 精读 文章 的 Related Work 部分 也 是 非常 
详尽 有 很多 工作 值得 参考 论文 下载 链接 Multimodal 
Word D i s t r i b u t 
i o n s T o p i c a 
l l y Driven Neural Language ModelTowards End to End 
Reinforcement Learning of Dialogue Agents for Information AccessLearning to Skim 
TextFrom Language to Programs Bridging Reinforcement Learning and Maximum Marginal 
Likelihood 相关 阅读 WWW 精选 论文 WSDM 精选 论文 解读 
NIPS 十大 机器学习 精选 论文 ICML 精选 论文 SIGIR 信息检索 
精选 论文 WWW 2017 精选 论文 知人知面 需 知心 论 
人工智能 技术 在 推荐 系统 中 的 应用 CSDN AI 
热衷 分享 欢迎 扫 码 关注 # / usr / 
bin / env python # * coding utf 8 * 
# @ Author Peidong # @ Site # @ File 
eg7 . py # @ Software PyCharm 从 文本 提取 
信息 import nltk # 读取 语料库 的 训练 部分 的 
100 个 句子 的 例子 from nltk . corpus import 
conll2000 print conll2000 . chunked _ sents train . txt 
99 # # 使用 chunk _ types 参数 选择 print 
conll2000 . chunked _ sents train . txt chunk _ 
types = NP 99 # 访问 一个 已 分块 语料 
可以 评估 分块 器 cp = nltk . RegexpParser test 
_ sents = conll2000 . chunked _ sents test . 
txt chunk _ types = NP print cp . evaluate 
test _ sents # 尝试 一个 初级 的 正则表达式 分块 
器 查找 以 名词 短语 标记 的 特征 字母 如 
CD DT 和 JJ 开头 的 标记 grammar = r 
NP { CDJNP . * + } cp = nltk 
. RegexpParser grammar test _ sents = conll2000 . chunked 
_ sents test . txt chunk _ types = NP 
print cp . evaluate test _ sents # 使用 unigram 
标注 器 对 名词 短语 分块 class UnigramChunker nltk . 
ChunkParserI def _ _ init _ _ self train _ 
sents train _ data = t c for w t 
c in nltk . chunk . tree2conlltags sent for sent 
in train _ sents self . tagger = nltk . 
UnigramTagger train _ data def parse self sentence pos _ 
tags = pos for word pos in sentence tagged _ 
pos _ tags = self . tagger . tag pos 
_ tags chunktags = chunktag for pos chunktag in tagged 
_ pos _ tags conlltags = word pos chunktag for 
word pos chunktag in zip sentence chunktags return nltk . 
chunk . conlltags2tree conlltags # # 可以 使用 CoNLL2000 分块 
语料库 训练 它 并 测试 其 性能 test _ sents 
= conll2000 . chunked _ sents test . txt chunk 
_ types = NP train _ sents = conll2000 . 
chunked _ sents train . txt chunk _ types = 
NP unigram _ chunker = UnigramChunker train _ sents print 
unigram _ chunker . evaluate test _ sents postags = 
sorted set pos for sent in train _ sents for 
word pos in sent . leaves print unigram _ chunker 
. tagger . tag postags # 使用 连续 分类器 对 
名词 短语 分块 class C o n s e c 
u t i v e N P C h u 
n k T a g g e r nltk . 
TaggerI def _ _ init _ _ self train _ 
sents train _ set = for tagged _ sent in 
train _ sents untagged _ sent = nltk . tag 
. untag tagged _ sent history = for i word 
tag in enumerate tagged _ sent featureset = npchunk _ 
features untagged _ sent i history train _ set . 
append featureset tag history . append tag self . classifier 
= nltk . MaxentClassifier . train train _ set algorithm 
= megam trace = 0 def tag self sentence history 
= for i word in enumerate sentence featureset = npchunk 
_ features sentence i history tag = self . classifier 
. classify featureset history . append tag return zip sentence 
history class C o n s e c u t 
i v e N P C h u n k 
e r nltk . ChunkParserI def _ _ init _ 
_ self train _ sents tagged _ sents = w 
t c for w t c in nltk . chunk 
. tree2conlltags sent for sent in train _ sents self 
. tagger = C o n s e c u 
t i v e N P C h u n 
k T a g g e r tagged _ sents 
def parse self sentence tagged _ sents = self . 
tagger . tag sentence conlltags = w t c for 
w t c in tagged _ sents return nltk . 
chunk . conlltags2tree conlltags # # 定义 一个 简单 的 
特征提取 器 它 只是 提供 了 当前 标识符 的 词性 
标记 def npchunk _ features sentence i history word pos 
= sentence i return { pos pos } train _ 
sents = conll2000 . chunked _ sents train . txt 
chunk _ types = NP chunker = C o n 
s e c u t i v e N P 
C h u n k e r train _ sents 
print chunker . evaluate test _ sents # 一个 分块 
器 处理 NP PP VP 和 grammar = r NP 
{ DT | JJ | NN . * + } 
# Chunk sequences of DT JJ NN PP { IN 
NP } # Chunk prepositions followed by NP VP { 
VB . * NP | PP | CLAUSE + $ 
} # Chunk verbs and their arguments CLAUSE { NP 
VP } # Chunk NP VP cp = nltk . 
RegexpParser grammar sentence = Mary NN saw VBD the DT 
cat NN sit VB on IN the DT mat NN 
print cp . parse sentence sentence = John NNP thinks 
VBZ Mary NN saw VBD the DT cat NN sit 
VB on IN the DT mat NN print cp . 
parse sentence cp = nltk . RegexpParser grammar loop = 
2 print cp . parse sentence # 在 NLTK 中 
创建 了 一棵树 通过 给 一个 节点 添加 标签 和 
一个 孩子 链表 # tree1 = nltk . Tree NP 
Alice # print tree1 # tree2 = nltk . Tree 
NP the rabbit # print tree2 # tree3 = nltk 
. Tree VP chased tree2 # tree4 = nltk . 
Tree S tree1 tree3 # print tree4 # print tree4 
1 # print tree4 . leaves # print tree4 1 
. node # print tree4 1 1 1 # 递归函数 
遍历 树 # def traverse t # try # t 
. node # except AttributeError # print t # else 
# # Now we know that t . node is 
defined # print t . node # for child in 
t # traverse child # print # # t = 
nltk . Tree S NP Alice VP chased NP the 
rabbit # print traverse t sent = nltk . corpus 
. treebank . tagged _ sents 22 print nltk . 
ne _ chunk sent binary = True print nltk . 
ne _ chunk sent 自然语言 处理 分类 自然语言 理解 是 
个 综合 的 系统工程 涉及 了 很多 细分 的 学科 
代表 声音 的 音 系 学 语言 中 发音 的 
系统化 组织 代表 构词法 的 词态 学 研究 单词 构成 
以 及 相互 之间 的 关系 代表 语句 结构 的 
句 法学 给定 文本 的 那 部分 是 语法 正确 
的 代表 理解 的 语义 句 法学 和 语用学 给定 
文本 的 含义 和 目的 是 什么 语言 理解 涉及 
语言 语境 和 各种 语言 形式 的 学科 但 总的来说 
自然语言 理解 又 可以 分为 三 个 方面 词义 分析 
句法分析 语义分析 自然 语言 的 生成 则 是从 结构化 的 
数据 可以 通俗 理解为 自然语言 理解 分析 后的/nr 数据 以 
读取 的 方式 自动 生成 文本 主要 有 三个 阶段 
文本 规划 完成 结构化 数据 中 的 基础 内容 规划 
语句 规划 从 结构化 数据 中 组合 语句 来 表达 
信息流 实现 产生 语法 通顺 的 语句 来 表达 文本 
中文 文本 分类 做 一个 中文 文本 分类 任务 首先 
要 做 的 是 文本 的 预处理 对 文本 进行 
分词 和去/nr 停用词 操作 来 把 字符串 分割 成词与/nr 词 
组合而成 的 字符串 集 合并 去掉 其中 的 一些 非 
关键 词汇 像是 的 地 得 等 再 就是 对 
预处理 过后 的 文本 进行 特征提取 最后 将 提取 到 
的 特征 送进 分类器 进行 训练 研究 与 应用 NLP 
在 现在 大火 的 AI 领域 有着 十分 丰富 的 
应用 总体 来说 自然语言 处理 的 研究 问题 主要 有 
下面 几种 信息检索 对 大 规模 文档 进行 索引 语音识别 
识别 包含 口语 在内 的 自然 语言 的 声学 信号 
转换 成 符合 预期 的 信号 机器翻译 将 一种 语言 
翻译 成 另外 一种 语言 智能 问答 自动 回答 问题 
对话 系统 通过 多 回合 对话 跟 用户 进行 聊天 
回答 完成 某项 任务 文本 分类 将 文本 自动 归类 
情感 分析 判断 某段 文本 的 情感 倾向 文本 生成 
根据 需求 自动 生成 文本 自动 文摘 归纳 总结 文本 
的 摘要 术语 分词 词性 标注 命名 实体 消 歧 
词义 消 歧 句法分析 指代 消解 HMM 应用 与 分词 
规定 每个 字 在 一个 词语 当中 有着 4个 不同 
的 位置 词首 B 词中 M 词尾 E 单字 成词/nr 
S 我们 通过 给 一句话 中的 每个字 标记 上述 的 
属性 最后 通过 标注 来 确定 分词 结果 考虑到 独立 
输出 假设 有限 历史性 假设 用来 求解 HMM 的 算法 
可以用 维 特比 算法 一种 动态规划 算法 嗯 文本 分类 
词 袋 模型 把 整个 文档 集 的 所有 出现 
的 词 都丢进/nr 袋子 里面 然后 无序 的 排出来 去掉 
重复 的 对 每一个 文档 按照 词语 出现 的 次数 
来 表示 文档 TF IDF 模型 这种 模型 主要 是 
用 词汇 的 统计 特征 来 作为 特 征集 TF 
IDF 由 两部分 组成 TF Term frequency IDF Inverse document 
frequency TF \ tf _ { ij } = \ 
frac { n _ { ij } } { \ 
sum _ { k } n _ { kj } 
} \ 其中 分子 \ n _ { ij } 
\ 表示 词 \ i \ 在 文档 \ j 
\ 中 出现 的 频次 分母 则是 所有 词 频次 
的 总和 也 就是 所有 词 的 个数 IDF \ 
idf _ { i } = log \ left \ 
frac { \ left | D \ right | } 
{ 1 + \ left | D _ { i 
} \ right | } \ right \ 其中 \ 
\ left | D \ right | \ 代表 文档 
的 总数 分母 部分 \ \ left | D _ 
{ i } \ right | \ 则是 代表 文档 
集 中含有 \ i \ 词 的 文档 数 原始 
公式 是 分母 没有 \ + 1 \ 的 这里 
\ + 1 \ 是 采用 了 拉普拉斯 平滑 避免了 
有 部分 新的 词 没有 在 语料库 中 出现 而 
导致 分母 为零 的 情况 出现 \ tf * idf 
i j = tf _ { ij } * idf 
_ { i } = \ frac { n _ 
{ ij } } { \ sum _ { k 
} n _ { kj } } * log \ 
left \ frac { \ left | D \ right 
| } { 1 + \ left | D _ 
{ i } \ right | } \ right \ 
使用 方法 加载 词 袋类 调整 类 的 参数 建立 
文本库 训练 数据 获得 词 袋 特征 转换 为 array 
加载 TF IDF 类 调整 类 参数 并 训练 中文 
邮件 分类 数据 准备 转化 为 对应 列表 拼接 划分 
测试 集 和 训练 集 预处理 去 停用词 训练 fit 
_ transform 测试 transform 将 特征 和 标签 喂入 SVM 
测试 集 验证 结果 转 载于 https / / www 
. cnblogs . com / xFANx / p / 10203479 
. html 自然语言 处理 NLP 是 计算机 科学 人工智能 语言学 
关注 计算机 和 人类 自然 语言 之间 的 相互 作用 
的 领域 本文 作者 为 自然语言 处理 NLP 初学者 整理 
了 一份 庞大 的 自然 语言 处理 项目 领域 的 
概览 包括 了 很多 人工智能 应用程序 选取/v 的/uj 参考/v 文献/n 
与/p 资料/n 都/d 侧重/v 于/p 最新/d 的/uj 深度/ns 学习/v 研究/vn 
成果/n 这些 自然 语言 处理 项目 资源 能为 想要 深入 
钻研 一个 自然 语言 处理 NLP 任务 的 人们 提供 
一个 良好 的 开端 自然语言 处理 项目 的 相关 干货 
整理 指代 消解 https / / github . com / 
Kyubyong / nlp _ tasks # coreference resolution 论文 自动 
评分 论文 Automatic Text Scoring Using Neural Networks 使用 神经 
网络 的 自动 文本 评分 https / / arxiv . 
org / abs / 1606.04289 论文 A Neural Approach to 
Automated Essay Scoring 一种 自动 将 论文 评分 的 神经学 
方法 http / / www . aclweb . org / 
old _ anthology / D / D16 / D16 1193 
. pdf 挑战 Kaggle The Hewlett Foundation Automated Essay Scoring 
Kaggle The Hewlett Foundation 论文 自动 评 分系统 https / 
/ www . kaggle . com / c / asap 
aes 项目 Enhanced AI Scoring Engine 增强 的 人工智能 得分 
引擎 https / / github . com / edx / 
ease 自动 语音 识别 维基百科 语言识别 https / / en 
. wikipedia . org / wiki / Speech _ recognition 
论文 DeepSpeech 2 End to End Speech Recognition in English 
and Mandarin 深度 语音 2 用 英语 和 普通话 进行 
端对端 语音识别 https / / arxiv . org / abs 
/ 1512.02595 论文 WaveNet A Generative Model for Raw Audio 
WaveNet 原始 音频 的 生成 模型 https / / arxiv 
. org / abs / 1609.03499 项目 A TensorFlow implementation 
of Baidu s Deep Speech architecture 百度 深度 语音 架构 
的 一个 TensorFlow 实现 https / / github . com 
/ mozilla / DeepSpeech 项目 Speech to Text WaveNet End 
to end sentence level English speech recognition using DeepMind s 
WaveNet Speech to Text WaveNet 使用 DeepMind 的 WaveNet 对 
端 到 端 句子 的 英语 水平 语音识别 https / 
/ github . com / buriburisuri / speech to text 
wavenet 挑战 The 5th CHiME Speech Separation and Recognition Challenge 
第五届 CHiME 语音 的 分离 和 识别 挑战 http / 
/ spandh . dcs . shef . ac . uk 
/ chime _ challenge / 资料 The 5thCHiME Speech Separation 
and Recognition Challenge 第五届 CHiME 语音 的 分离 和 识别 
挑战 http / / spandh . dcs . shef . 
ac . uk / chime _ challenge / download . 
html 资料 CSTRVCTK Corpus http / / homepages . inf 
. ed . ac . uk / jyamagis / page3 
/ page58 / page58 . html 资料 LibriSpeech ASR corpus 
http / / www . openslr . org / 12 
/ 资料 Switchboard 1 Telephone Speech Corpus https / / 
catalog . ldc . upenn . edu / ldc97s62 资料 
TED LIUM Corpus http / / www lium . univ 
lemans . fr / en / content / ted lium 
corpus 自动 摘要 维基百科 自动 摘要 https / / en 
. wikipedia . org / wiki / Automatic _ summarization 
书籍 Automatic Text Summarization 自动 本文 摘要 https / / 
www . amazon . com / Automatic Text Summarization Juan 
Manuel Torres Moreno / dp / 1848216688 / ref = 
sr _ 1 _ 1 s = books & ie 
= UTF8 & qid = 1507782304 & sr = 1 
1 & keywords = Automatic + Text + Summarization 论文 
Text Summarization Using Neural Networks 使用 神经 网络 进行 文本 
摘要 http / / citeseerx . ist . psu . 
edu / viewdoc / download doi = 10 . 1.1 
. 823.8025 & rep = rep1 & type = pdf 
论文 Ranking with Recursive Neural Networks and Its Application to 
Multi D o c u m e n t u 
m m a r i z a t i o 
n 使用 递归 神经网络 及其 应用 程序 对 多 文档 
摘要 进行 排序 https / / www . aaai . 
org / ocs / index . php / AAAI / 
AAAI15 / paper / viewFile / 9414/9520 资料 Text Analytics 
Conferences 文本 分析 会议 https / / tac . nist 
. gov / data / index . html 资料 Document 
Understanding Conferences 文书 理解 会议 http / / www nlpir 
. nist . gov / projects / duc / data 
. html 共 指 消解 信息 共 指 消解 https 
/ / nlp . stanford . edu / projects / 
coref . shtml 论文 Deep Reinforcement Learning for Mention Ranking 
Coreference Models 对 Mention Ranking 的 共 指 模型 进行 
深度 强化 学习 https / / arxiv . org / 
abs / 1609.08667 论文 Improving Coreference Resolution by Learning Entity 
Level Distributed Representations 通过学习 实体 级 分布式 表示 来 改善 
相关 的 解决方案 https / / arxiv . org / 
abs / 1606.01323 挑战 CoNLL 2012 Shared Task Modeling Multilingual 
Unrestricted Coreference in OntoNotes CoNLL 2012 共享 任务 在 OntoNotes 
中 对 多 语言 的 不受限制 的 共 指 进行 
建模 http / / conll . cemantix . org / 
2012 / task description . html 挑战 CoNLL 2011 Shared 
Task Modeling Unrestricted Coreference in OntoNotes CoNLL 2011 共享 任务 
在 OntoNotes 中 对 多 语言 的 不受限制 的 共 
指 进行 建模 http / / conll . cemantix . 
org / 2011 / task description . html 语法错误 校正 
论文 Neural Network Translation Models for Grammatical Error Correction 语法错误 
校正 的 神经 网络 翻译 模型 https / / arxiv 
. org / abs / 1606.00189 挑战 CoNLL 2013 Shared 
Task Grammatical Error Correction CoNLL 2013 共享 任务 语法错误 校正 
http / / www . comp . nus . edu 
. sg / ~ nlp / conll13st . html 挑战 
CoNLL 2014Shared Task Grammatical Error Correction CoNLL 2014 共享 任务 
语法错误 校正 http / / www . comp . nus 
. edu . sg / ~ nlp / conll14st . 
html 资料 NUSNon commercial research / trial corpus license http 
/ / www . comp . nus . edu . 
sg / ~ nlp / conll14st / nucle _ license 
. pdf 资料 Lang 8 Learner Corpora http / / 
cl . naist . jp / nldata / lang 8 
/ 资料 Cornell Movie – Dialogs Corpus http / / 
www . cs . cornell . edu / ~ cristian 
/ Cornell _ Movie Dialogs _ Corpus . html 项目 
Deep Text Corrector 深度 文本 校正器 https / / github 
. com / atpaino / deep text corrector 产品 deep 
grammar http / / deepgrammar . com / 字素 转换 
到 音素 论文 Grapheme to Phoneme Models for Almost Any 
Language 适合 几乎 任何 语言 的 字素 到 音素 的 
模型 https / / pdfs . semanticscholar . org / 
b9c8 / f e f 9 b 6 f 1 
6 b 9 2 c 6 8 5 9 f 
6 1 0 6 5 2 4 f d b 
0 5 3 e 9 5 7 7 . pdf 
论文 Polyglot Neural Language Models A Case Study in Cross 
Lingual Phonetic Representation Learning 多 语言 神经 语言 模型 跨 
语 语音 表达 学习 的 案例 研究 https / / 
arxiv . org / pdf / 1605.03832 . pdf 论文 
Multi task Sequence to Sequence Models for Grapheme to Phoneme 
Conversion 多任务 序 列到 序列 的 字素 到 音素 转换 
的 模型 https / / pdfs . semanticscholar . org 
/ 2 6 d 0/09959 f a 2 b 2 
e 1 8 c d d b 5 7 8 
3 4 9 3 7 3 8 a 1 c 
1 e d e 2 f . pdf 项目 Sequence 
to Sequence G2P toolkit 序 列到 序列 G2P 工具包 https 
/ / github . com / cmusphinx / g2p seq2seq 
资料 Multilingual Pronunciation Data 多语种 发音 数据 https / / 
drive . google . com / drive / folders / 
0B7R _ g A T f Z J 2 a 
W k p W H p X U k l 
W U m M 语种 识别 维基百科 语种 识别 https 
/ / en . wikipedia . org / wiki / 
Language _ identification 论文 AUTOMATIC LANGUAGE IDENTIFICATION USING DEEP NEURAL 
NETWORKS 使用 深度 神经 网络 的 自动 语言识别 https / 
/ repositorio . uam . es / bitstream / handle 
/ 10486/666848 / automatic _ lopez moreno _ ICASSP _ 
2014 _ ps . pdf sequence = 1 挑战 2015 
Language Recognition Evaluation 2015 语言识别 评估 https / / www 
. nist . gov / itl / iad / mig 
/ 2015 language recognition evaluation 语言 建模 维基百科 语言 模型 
https / / en . wikipedia . org / wiki 
/ Language _ model 工具包 KenLM Language Model Toolkit KenLM 
语言 模型 工具包 http / / kheafield . com / 
code / kenlm / 论文 Distributed Representations of Words and 
Phrases and their Compositionality 词汇 和 短语 的 分布 表示 
及其 组 合性 http / / papers . nips . 
cc / paper / 5021 distributed representations of words and 
phrases and their compositionality . pdf 论文 Character Aware Neural 
Language Models Character Aware 神经 语言 模型 https / / 
www . aaai . org / ocs / index . 
php / AAAI / AAAI16 / paper / viewFile / 
12489/12017 资料 Penn Treebank https / / github . com 
/ townie / PTB dataset from Tomas Mikolov s webpage 
/ tree / master / data 词形 还原 维基百科 词形 
还原 https / / en . wikipedia . org / 
wiki / Lemmatisation 工具包 WordNet Lemmatizer http / / www 
. nltk . org / api / nltk . stem 
. html # nltk . stem . wordnet . W 
o r d N e t L e m m 
a t i z e r . lemmatize 资料 Treebank 
3 https / / catalog . ldc . upenn . 
edu / ldc99t42 唇语 辨别 维基百科 唇 读法 https / 
/ en . wikipedia . org / wiki / Lip 
_ reading 论文 Lip Reading Sentences in the Wild 在野外 
读懂 唇语 https / / arxiv . org / abs 
/ 1611.05358 论文 3D Convolutional Neural Networks for Cross Audio 
Visual Matching Recognition 交叉 视听 匹配 识别 的 3D 卷积 
神经网络 https / / arxiv . org / abs / 
1706.05739 项目 Lip Reading – Cross Audio Visual Recognition using 
3D Convolutional Neural Networks 唇 读法 使用 3D 卷积 神经 
网络 的 交叉 视听 识别 https / / github . 
com / astorfi / lip reading deeplearning 资料 The GRID 
audiovisual sentence corpus http / / spandh . dcs . 
shef . ac . uk / gridcorpus / 机器翻译 论文 
Neural Machine Translation by Jointly Learning to Align and Translate 
通过 共同 学习 来 调整 和 翻译 神经 机器翻译 https 
/ / arxiv . org / abs / 1409.0473 论文 
Neural Machine Translation in Linear Tim 在 线性 时间 中 
的 神经 机器翻译 https / / arxiv . org / 
abs / 1610.10099 挑战 ACL2014 NINTH WORKSHOP ON STATISTICAL MACHINE 
  TRANSLATION ACL2014 第九 届 统计 机器翻译 研讨会 http / 
/ www . statmt . org / wmt14 / translation 
task . html # download 资料 O p e n 
u b t i t l e s 2 0 
1 6 http / / opus . lingfil . uu 
. se / O p e n u b t 
i t l e s 2 0 1 6 . 
php 资料 WIT3 Web Inventory of Transcribed and Translated Talks 
https / / wit3 . fbk . eu / 资料 
The QCRI Educational Domain QED Corpus http / / alt 
. qcri . org / resources / qedcorpus / 命名 
实体 识别 维基百科 命名 实体 识别 https / / en 
. wikipedia . org / wiki / Named entity _ 
recognition 论文 Neural Architectures for Named Entity Recognition 命名 实体 
识别 的 神经 结构 https / / arxiv . org 
/ abs / 1603.01360 项目 OSU Twitter NLP Tool https 
/ / github . com / aritter / twitter _ 
nlp 挑战 Named Entity Recognition in Twitter 在 推特 上 
被 命名 的 实体 识别 https / / noisy text 
. github . io / 2016 / ner shared task 
. html 资料 CoNLL 2002 NER corpus https / / 
github . com / teropa / nlp / tree / 
master / resources / corpora / conll2002 资料 CoNLL 2003 
NER corpus https / / github . com / synalp 
/ NER / tree / master / corpus / CoNLL 
2003 释义 检测 论文 Dynamic Pooling and Unfolding Recursive Autoencoders 
for Paraphrase Detection 动态/n 池和/nr 展开/v 递归/v 自动/vn 编码器/n 的/uj 
释义/nr 检测/vn http / / citeseerx . ist . psu 
. edu / viewdoc / download doi = 10 . 
1.1 . 650.7199 & rep = rep1 & type = 
pdf 项目 Paralex Paraphrase Driven Learning for Open Question Answering 
Paralex 释义 驱动 学习 的 开放 问答 http / / 
knowitall . cs . washington . edu / paralex / 
资料 Microsoft Research Paraphrase Corpus https / / www . 
microsoft . com / en us / download / details 
. aspx id = 52398 资料 Microsoft Research Video Description 
Corpus https / / www . microsoft . com / 
en us / download / details . aspx id = 
52422 & from = http % 3A % 2F % 
2Fresearch . microsoft . com % 2Fen us % 2Fdownloads 
% 2F38cf15fd b8df 477e a4e4 a4680caa75af % 2F 资料 Pascal 
Dataset http / / nlp . cs . illinois . 
edu / HockenmaierGroup / pascal sentences / index . html 
资料 Flicker Dataset http / / nlp . cs . 
illinois . edu / HockenmaierGroup / 8k pictures . html 
资料 TheSICK data set http / / clic . cimec 
. unitn . it / composes / sick . html 
资料 PPDB The Paraphrase Database http / / www . 
cis . upenn . edu / ~ ccb / ppdb 
/ 资料 WikiAnswers Paraphrase Corpus http / / knowitall . 
cs . washington . edu / paralex / wikianswers paraphrases 
1.0 . tar . gz 语法分析 维基百科 语法分析 https / 
/ en . wikipedia . org / wiki / Parsing 
工具包 The Stanford Parser A statistical parser https / / 
nlp . stanford . edu / software / lex parser 
. shtml 工具包 spaCyparser https / / spacy . io 
/ docs / usage / dependency parse 论文 A fastand 
accurate dependency parser using neural networks 快速 而 准确 地 
使用 神经 网络 的 依赖 解析器 http / / www 
. aclweb . org / anthology / D14 1082 挑战 
CoNLL2017 Shared Task Multilingual Parsing from Raw Text to Universal 
Dependencies CoNLL2017 共享 任务 从/p 原始/v 文本/n 到/v 通用/v 依赖/v 
项的/nr 多语言/i 解析/vn http / / u n i v 
e r s a l d e p e n 
d e n c i e s . org / 
conll17 / 挑战 CoNLL2016 Shared Task Multilingual Shallow Discourse Parsing 
CoNLL2016 共享 任务 多 语言 的 浅 会话 解析 http 
/ / www . cs . brandeis . edu / 
~ clp / conll16st / 词性 标记 维基百科 词性 标记 
https / / en . wikipedia . org / wiki 
/ Part of speech _ tagging 论文 Unsupervised Part Of 
Speech Tagging with Anchor Hidden Markov Models 有 Anchor Hidden 
Markov 模型 的 非 监督性 的 词性 标记 https / 
/ transacl . org / ojs / index . php 
/ tacl / article / viewFile / 837/192 资料 Treebank 
3 https / / catalog . ldc . upenn . 
edu / ldc99t42 工具包 nltk . tag package http / 
/ www . nltk . org / api / nltk 
. tag . html 拼音 与 中文 转换 论文 Neural 
Network Language Model for Chinese Pinyin Input Method Engine 中文 
拼音 输入法 引擎 的 神经 网络 语言 模型 http / 
/ aclweb . org / anthology / Y15 1052 项目 
Neural Chinese Transliterator https / / github . com / 
Kyubyong / neural _ chinese _ transliterator 问答 系统 维基百科 
问答 系统 https / / en . wikipedia . org 
/ wiki / Question _ answering 论文 Ask Me Anything 
Dynamic Memory Networks for Natural Language Processing 自然语言 处理 的 
动态内存 网络 http / / www . thespermwhale . com 
/ jaseweston / ram / papers / paper _ 21 
. pdf 论文 Dynamic Memory Networks for Visual and Textual 
Question Answering 用于 视觉 和 文本 的 问答 系统 的 
动态 记忆 网络 http / / proceedings . mlr . 
press / v48 / xiong16 . pdf 挑战 TREC Question 
Answering Task TREC 问答 系统 任务 http / / trec 
. nist . gov / data / qamain . html 
挑战 SemEval 2017 Task 3 Community Question Answering http / 
/ alt . qcri . org / semeval2017 / task3 
/ 资料 MSMARCO Microsoft MAchine Reading COmprehension Dataset MSMARCO 微软 
机器 阅读 理解 数据集 http / / www . msmarco 
. org / 资料 Maluuba NewsQA https / / github 
. com / Maluuba / newsqa 资料 SQuAD 100 000 
+ Questions for Machine Comprehension of Text SQuAD 100 000 
+ 个 文本 的 机器 理解 的 问题 https / 
/ rajpurkar . github . io / SQuAD explorer / 
资料 Graph Questions A Characteristic rich Question Answering Dataset 图形 
问题 一个 特征 丰富 的 问题 回答 数据集 https / 
/ github . com / ysu1989 / GraphQuestions 资料 Story 
Cloze Test and ROC Stories Corpora http / / cs 
. rochester . edu / nlp / rocstories / 资料 
Microsoft Research WikiQA Corpus https / / www . microsoft 
. com / en us / download / details . 
aspx id = 52419 & from = http % 3A 
% 2F % 2Fresearch . microsoft . com % 2Fen 
us % 2Fdownloads % 2F4495da01 db8c 4041 a7f6 7984a4f6a905 % 
2Fdefault . aspx 资料 DeepMind Q & A Dataset http 
/ / cs . nyu . edu / ~ kcho 
/ DMQA / 资料 QASent http / / cs . 
stanford . edu / people / mengqiu / data / 
qg emnlp07 data . tgz 关系 提取 维基百科 关系 提取 
https / / en . wikipedia . org / wiki 
/ Relationship _ extraction 论文 A deep learning approach for 
relationship extraction from interaction context in social manufacturing paradigm 一种 
从 社会 生产 范例 的 互动 情境 中 提取 关系 
深度 学习 的 方法 http / / www . sciencedirect 
. com / science / article / pii / 0 
9 5 0 7 0 5 1 1 6 0 
0 1 2 1 0 语义 角色 标记 维基百科 语义 
角色 标记 https / / en . wikipedia . org 
/ wiki / Semantic _ role _ labeling 书籍 Semantic 
Role Labeling 语义 角色 标记 https / / www . 
amazon . com / Semantic Labeling Synthesis Lectures Technologies / 
dp / 1598298313 / ref = sr _ 1 _ 
1 s = books & ie = UTF8 & qid 
= 1507776173 & sr = 1 1 & keywords = 
Semantic + Role + Labeling 论文 End to end Learning 
of Semantic Role Labeling Using Recurrent Neural Networks 使用 循环 
神经 网络 对 语义 角色 标签 进行 端 到 端 
学习 http / / www . aclweb . org / 
anthology / P / P15 / P15 1109 . pdf 
论文 Neural Semantic Role Labeling with Dependency Path Embeddings 有着 
依赖 路径 嵌入 的 神经 语义 角色 标记 https / 
/ arxiv . org / abs / 1605.07515 挑战 CoNLL 
2005 Shared Task Semantic Role Labeling CoNLL 2005 共享 任务 
语义 角色 标记 http / / www . cs . 
upc . edu / ~ srlconll / st05 / st05 
. html 挑战 CoNLL 2004 Shared Task Semantic Role Labeling 
CoNLL 2004 共享 任务 语义 角色 标记 http / / 
www . cs . upc . edu / ~ srlconll 
/ st04 / st04 . html 工具包 Illinois Semantic Role 
Labeler SRL http / / cogcomp . org / page 
/ software _ view / SRL 资料 CoNLL 2005 Shared 
Task Semantic Role Labeling CoNLL 2005 共享 任务 语义 角色 
标记 http / / www . cs . upc . 
edu / ~ srlconll / soft . html 语句 边界 
消 歧 维基百科 语句 边界 消 歧 https / / 
en . wikipedia . org / wiki / Sentence _ 
boundary _ disambiguation 论文 A Quantitative and Qualitative Evaluation of 
Sentence Boundary Detection for theClinical Domain 对 临床 领域 的 
语句 边界检测 进行 定量 和 定性 的 评估 https / 
/ www . ncbi . nlm . nih . gov 
/ pmc / articles / PMC5001746 / 工具包 NLTK Tokenizers 
http / / www . nltk . org / _ 
modules / nltk / tokenize . html 资料 The British 
National Corpus http / / www . natcorp . ox 
. ac . uk / 资料 Switchboard 1 Telephone Speech 
Corpus https / / catalog . ldc . upenn . 
edu / ldc97s62 情绪 分析 维基百科 情绪 分析 https / 
/ en . wikipedia . org / wiki / Sentiment 
_ analysis 信息 Awesome Sentiment Analysis 了不起 的 情绪 分析 
https / / github . com / xiamx / awesome 
sentiment analysis 挑战 Kaggle UMICH SI650 – Sentiment Classification Kaggle 
UMICH SI650 – 情绪 分类 https / / www . 
kaggle . com / c / si650winter11 # description 挑战 
SemEval 2017 Task 4 Sentiment Analysis in Twitter SemEval 2017 
任务 4 推特 上 的 情绪 分析 http / / 
alt . qcri . org / semeval2017 / task4 / 
项目 SenticNet http / / sentic . net / about 
/ 资料 Multi Domain Sentiment Dataset version2 . 0 http 
/ / www . cs . jhu . edu / 
~ mdredze / datasets / sentiment / 资料 Stanford Sentiment 
Treebank https / / nlp . stanford . edu / 
sentiment / code . html 资料 Twitter Sentiment Corpus http 
/ / www . sananalytics . com / lab / 
twitter sentiment / 资料 Twitter Sentiment Analysis Training Corpus http 
/ / thinknook . com / twitter sentiment analysis training 
corpus dataset 2012 09 22 / 源 分离 维基百科 源 
分离 https / / en . wikipedia . org / 
wiki / Source _ separation 论文 From Blind to Guided 
Audio Source Separation 从 盲目 到有 指导性 的 音频 源 
分离 https / / hal univ rennes1 . archives ouvertes 
. fr / hal 00922378 / document 论文 Joint Optimization 
of Masks and Deep Recurrent Neural Networks for Monaural Source 
Separation 对 单声道 分离 的 掩膜 和 深层 循环 神经 
网络 的 联合 优化 https / / arxiv . org 
/ abs / 1502.04149 挑战 Signal Separation Evaluation Campaign 信号 
分离 评估 活动 https / / sisec . inria . 
fr / 挑战 CHiME Speech Separation and Recognition Challenge CHiME 
语音 分离 和 识别 的 挑战 http / / spandh 
. dcs . shef . ac . uk / chime 
_ challenge / 说话者 识别 维基百科 说话者 识别 https / 
/ en . wikipedia . org / wiki / Speaker 
_ recognition 论文 A NOVEL SCHEME FOR SPEAKER RECOGNITION USING 
A PHONETICALLY AWARE DEEP NEURAL NETWORK 一种 使用 语音 识别 
的 深度 神经 网络 的 新方案 https / / pdfs 
. semanticscholar . org / 204a / f f 8 
e 2 1 7 9 1 c 0 a 4 
1 1 3 a 3 f 7 5 d 0 
e 6 4 2 4 a 0 0 3 c 
3 2 1 . pdf 论文 DEEP NEURAL NETWORKS FOR 
SMALL FOOTPRINT TEXT DEPENDENT SPEAKER VERIFICATION 深度 神经网络 用于 小 
范围 的 文本 依赖 的 说话者 验证 https / / 
static . g o o g l e u s 
e r c o n t e n t . 
com / media / research . google . com / 
en / / pubs / archive / 41939 . pdf 
挑战 NIST Speaker Recognition Evaluation NIST 说话者 识别 评价 https 
/ / www . nist . gov / itl / 
iad / mig / speaker recognition 语音 分段 维基百科 语音 
分段 https / / en . wikipedia . org / 
wiki / Speech _ segmentation 论文 Word Segmentation by 8 
Month Olds When Speech Cues Count More Than Statistics 8 
个月 大 婴儿 的 单词 分段 当 语音 提示 比 
统计 数字 更 重要 时 http / / www . 
utm . toronto . edu / infant child centre / 
sites / files / infant child centre / public / 
shared / elizabeth johnson / Johnson _ Jusczyk . pdf 
论文 Unsupervised Word Segmentation and Lexicon Discovery Using Acoustic Word 
Embeddings 不受 监督 的 单词 分割 和 使用 声学 词 
嵌入 的 词汇 发现 https / / arxiv . org 
/ abs / 1603.02845 资料 CALLHOME Spanish Speech https / 
/ catalog . ldc . upenn . edu / ldc96s35 
语音合成 维基百科 语音合成 https / / en . wikipedia . 
org / wiki / Speech _ synthesis 论文 WaveNet A 
Generative Model for Raw Audio WaveNet 原始 音频 的 生成 
模型 https / / arxiv . org / abs / 
1609.03499 论文 Tacotron Towards End to End Speech Synthesis Tacotron 
对 端 到 端的 语音合成 https / / arxiv . 
org / abs / 1703.10135 资料 The World English Bible 
https / / github . com / Kyubyong / tacotron 
资料 LJ Speech Dataset https / / github . com 
/ keithito / tacotron 资料 Lessac Data http / / 
www . cstr . ed . ac . uk / 
projects / blizzard / 2011 / lessac _ blizzard2011 / 
挑战 Blizzard Challenge 2017 https / / synsig . org 
/ index . php / Blizzard _ Challenge _ 2017 
项目 The Festvox project http / / www . festvox 
. org / index . html 工具包 Merlin The Neural 
Network NN based Speech Synthesis System Merlin 基于 神经 网络 
的 语音 合成 系统 https / / github . com 
/ CSTR Edinburgh / merlin 语音 增强 维基百科 语音 增强 
https / / en . wikipedia . org / wiki 
/ Speech _ enhancement 书籍 Speech enhancement theory and practice 
语音 增强 理论 与 实践 https / / www . 
amazon . com / Speech Enhancement Theory Practice Second / 
dp / 1466504218 / ref = sr _ 1 _ 
1 ie = UTF8 & qid = 1507874199 & sr 
= 8 1 & keywords = Speech + enhancement % 
3A + theory + and + practice 论文 An Experimental 
Study on Speech Enhancement Based on Deep Neural Network 一项 
基于 深度 神经 网络 的 语音 增强 实验 http / 
/ staff . ustc . edu . cn / ~ 
jundu / Speech % 20signal % 20processing / publications / 
SPL2014 _ Xu . pdf 论文 A Regression Approach to 
Speech Enhancement Based on Deep Neural Networks 一种 基于 深度 
神经 网络 的 语音 增强 的 回归 方法 https / 
/ www . researchgate . net / profile / Yong 
_ Xu63 / publication / 272436458 _ A _ Regression 
_ Approach _ to _ Speech _ Enhancement _ Based 
_ on _ Deep _ Neural _ Networks / links 
/ 5 7 f d f d d a 0 
8 a e a f 8 1 9 a 5 
b d d 9 7 . pdf 论文 Speech Enhancement 
Based on Deep Denoising Autoencoder 基于 深度 降噪 自 编码 
的 语音 增强 https / / www . researchgate . 
net / profile / Yu _ Tsao / publication / 
283600839 _ Speech _ enhancement _ based _ on _ 
deep _ denoising _ Auto Encoder / links / 5 
7 7 b 4 8 6 1 0 8 a 
e 2 1 3 7 6 1 c 9 c 
7 f 8 / Speech enhancement based on deep denoising 
Auto Encoder . pdf 词干 提取 维基百科 词干 提取 https 
/ / en . wikipedia . org / wiki / 
Stemming 论文 A BACKPROPAGATION NEURAL NETWORK TO IMPROVE ARABIC STEMMING 
一个 反向 传播 的 神经 网络 用来 改善 阿拉伯语 的 
词干 提取 http / / www . jatit . org 
/ volumes / Vol82No3 / 7Vol82No3 . pdf 工具包 NLTK 
Stemmers http / / www . nltk . org / 
howto / stem . html 术语 提取 维基百科 术语 提取 
https / / en . wikipedia . org / wiki 
/ Terminology _ extraction 论文 Neural Attention Models for Sequence 
Classification Analysis and Application to KeyTerm Extraction and Dialogue Act 
Detection 序列 分类 的 神经 提示 模型 分析/vn 和/c 应用/v 
于/p 关键词/n 提取/v 和/c 对话/n 法/l 检测/vn https / / 
arxiv . org / pdf / 1604.00077 . pdf 文本 
简化 维基百科 文本 简化 https / / en . wikipedia 
. org / wiki / Text _ simplification 论文 Aligning 
Sentences from Standard Wikipedia to Simple Wikipedia 调整 句子 从 
标准 的 维基百科 到 简单 的 维基百科 https / / 
ssli . ee . washington . edu / ~ hannaneh 
/ papers / simplification . pdf 论文 Problems in Current 
Text Simplification Research New Data Can Help 当前 文本 简化 
研究 中 的 问题 可 提供 帮助 的 新 数据 
https / / pdfs . semanticscholar . org / 2b8d 
/ a 0 1 3 9 6 6 c 0 
c 5 e 0 2 0 e b c 8 
4 2 d 4 9 d 8 e d 1 
6 6 c 8 7 8 3 . pdf 资料 
Newsela Data https / / newsela . com / data 
/ 文本 蕴涵 维基百科 文本 蕴含 https / / en 
. wikipedia . org / wiki / Textual _ entailment 
项目 Textual Entailment with TensorFlow 文本 蕴含 与 TensorFlow https 
/ / github . com / Steven Hewitt / Entailment 
with Tensorflow 竞赛 SemEval 2013 Task 7 The Joint Student 
Response Analysis and 8th Recognizing Textual Entailment Challenge SemEval 2013 
任务 7 联合 学生 反应 分析 和第/nr 8届 认知 文本 
蕴含 挑战 https / / www . cs . york 
. ac . uk / semeval 2013 / task7 . 
html 音译 维基百科 音译 https / / en . wikipedia 
. org / wiki / Transliteration 论文 A Deep Learning 
Approach to Machine Transliteration 一个 机器 音译 的 深度 学习 
方法 https / / pdfs . semanticscholar . org / 
5 4 f 1/23122 b 8 d d 1 f 
1 d 3 0 6 7 c f 3 4 
8 c f e a 1 2 7 6 9 
1 4 3 7 7 . pdf 项目 Neural Japanese 
Transliteration can you do better than SwiftKey ™ Keyboard 神经 
日语 音译 你 能比 SwiftKey 键盘 做得 更好 吗 https 
/ / github . com / Kyubyong / neural _ 
japanese _ transliterator 词 嵌入 维基百科 词 嵌入 https / 
/ en . wikipedia . org / wiki / Word 
_ embedding 工具包 Gensim word2vec https / / radimrehurek . 
com / gensim / models / word2vec . html 工具包 
fastText https / / github . com / facebookresearch / 
fastText 工具包 GloVe Global Vectors for Word Representation https / 
/ nlp . stanford . edu / projects / glove 
/ 信息 Where to get a pretrained model 哪里 能够 
获得 一个 预先 训练 的 模型 https / / github 
. com / 3Top / word2vec api 项目 Pre trained 
word vectors of 30 + languages 30 多种 语言 的 
预先 训练 的 词 向量 https / / github . 
com / Kyubyong / wordvectors 项目 Polyglot Distributed word representations 
for multilingual NLP Polyglot 多语言 NLP 的 分布式 词汇 表征 
https / / sites . google . com / site 
/ rmyeid / projects / polyglot 词汇 预测 信息 What 
is Word Prediction 什么 是 词汇 预测 http / / 
www2 . edc . org / ncip / library / 
wp / what _ is . htm 论文 The prediction 
of character based on recurrent neural network language model 基于 
循环 神经 网络 语言 模型 的 字符 预测 http / 
/ ieeexplore . ieee . org / stamp / stamp 
. jsp arnumber = 7960065 论文 An Embedded Deep Learning 
based Word Prediction 一个 基于 深度 学习 的 词汇 预测 
https / / arxiv . org / abs / 1707.01662 
论文 Evaluating Word Prediction Framing Keystroke Savings 评估 单词 预测 
框 击键 保存 http / / aclweb . org / 
anthology / P08 2066 资料 An Embedded Deep Learning based 
Word Prediction 一个 基于 深度 学习 的 词汇 预测 https 
/ / github . com / Meinwerk / WordPrediction / 
master . zip 项目 Word Prediction using Convolutional Neural Networks 
can you do better than iPhone ™ Keyboard 使用 卷积 
神经 网络 的 词汇 预测 你 能比 iPhone 键盘 做得 
更好 吗 https / / github . com / Kyubyong 
/ word _ prediction 词 分割 论文 Neural Word Segmentation 
Learning for Chinese 中文 的 神经 词 分割 学习 https 
/ / arxiv . org / abs / 1606.04300 项目 
Convolutional neural network for Chinese word segmentation 中文 的 词 
分割 的 卷积 神经网络 https / / github . com 
/ chqiwang / convseg 工具包 Stanford Word Segmenter https / 
/ nlp . stanford . edu / software / segmenter 
. html 工具包 NLTK Tokenizers http / / www . 
nltk . org / _ modules / nltk / tokenize 
. html 词义 消 歧 维基百科 词义 消 歧 https 
/ / en . wikipedia . org / wiki / 
Word sense _ disambiguation 论文 Train O Matic Large Scale 
Supervised Word Sense Disambiguation in Multiple Languages without Manual Training 
Data Train O Matic 在 没有 人工 训练 数据 的 
情况 下 在 多种 语言 中 大 规模 的 监督 
词义 消 歧 http / / www . aclweb . 
org / anthology / D17 1008 资料 Train O Matic 
Data http / / trainomatic . org / data / 
train o matic data . zip 资料 BabelNet http / 
/ babelnet . org / 原 项目 地址 https / 
/ github . com / Kyubyong / nlp _ tasks 
# speech segmentationfrom nltk . collocations import B i g 
r a m C o l l o c a 
t i o n F i n d e r 
1 . nltk . collocations . B i g r 
a m C o l l o c a t 
i o n F i n d e r word 
_ fd bigram _ fd window _ size = 2 
用于 查找 和 排列 bigram 搭配 或 其他 关联 度量 
的 工具 2 . B i g r a m 
C o l l o c a t i o 
n F i n d e r . from _ 
words words   window _ size = 2 把 词 
列表 变为 双词/nr 搭配 为 给定 序列 中 的 所有 
bigrams 构建 一个 B i g r a m C 
o l l o c a t i o n 
F i n d e r 3 . B i 
g r a m C o l l o c 
a t i o n F i n d e 
r . score _ ngram score _ fn   w1 
  w2 使用 给定 的 评分 函数 返回 给定 二 
元组 的 分数 4 . B i g r a 
m C o l l o c a t i 
o n F i n d e r . apply 
_ ngram _ filter fn 对 任意 n 元词/nr 组合 
应用 函数 fn 如果 fn w1 . . . wn 
返回 结果 为 True 则 删 除此 n 元词/nr 组合 
5 . B i g r a m C o 
l l o c a t i o n F 
i n d e r . apply _ word _ 
filter fn 对 任意 n 元词/nr 组合 应用 函数 fn 
如果 fn w1 . . . fn wn 中 有一个 
结果 是 True 则 删 除此 n 元词/nr 组合 6 
. B i g r a m C o l 
l o c a t i o n F i 
n d e r . apply _ freq _ filter 
min _ freq 删除 频数 小于 min _ freq 的 
候选 项 操作 实例 如下 import re import jieba text2 
= re . sub . ． % \ \ \ 
t \ n \ \ + \ \ * / 
text1 word _ list = jieba . lcut text2 word 
_ list = word for word in word _ list 
if len word 1 from nltk . collocations import B 
i g r a m C o l l o 
c a t i o n F i n d 
e r bigram _ finder = B i g r 
a m C o l l o c a t 
i o n F i n d e r . 
from _ words word _ list bigram _ finder . 
ngram _ fd 去除 频数 小于 4 的 二元 词组 
bigram _ finder . apply _ freq _ filter 4 
bigram _ finder . ngram _ fd FreqDist { 占 
崩 岗 滑坡 4 崩 岗 滑坡 24 崩 岗 
面积 4 森林 植被 6 滑坡 面积 6 } 去除 
含有 植被 的 词组 去除 含有 占 崩 岗 的 
词组 def fn1 * words return 植被 in words def 
fn2 word return word = = 占 崩 岗 bigram 
_ finder . apply _ ngram _ filter fn1 bigram 
_ finder . ngram _ fd FreqDist { 占 崩 
岗 滑坡 4 崩 岗 滑坡 24 崩 岗 面积 
4 滑坡 面积 6 } bigram _ finder . apply 
_ word _ filter fn2 bigram _ finder . ngram 
_ fd FreqDist { 崩 岗 滑坡 24 崩 岗 
面积 4 滑坡 面积 6 } 目录 文章 目录 目录 
前言 基于 认知科学 原理 的 相似 模型 五 基于 认知科学 
原理 的 相似 模型 六 面向 旅游 领域 的 问答 
系统 实验 前言 硕士 生涯 结束 开始 专心 做 一件 
自己 觉得 有用 的 工具 先 做 工程 后搞/nr 理论 
自然语言 处理 是 一个 非常 难 的 问题 同时 是 
人工智能 皇冠 上 的 明珠 接下来 会 记录 一 系列 
自然语言 处理 的 笔记 来自 于 哈工大 老师 关毅 基于 
认知科学 原理 的 相似 模型 五 高阶 谓词 逻辑 结构 
1 结构 匹配 优先于 特征 匹配 2 这个 过程 就像 
递归 3 说到 权重 大家 重新认识 4 心理学 原理 推导 
权重 5 相似 心理学 模型 洋气 对齐 心理学 模型 5 
高阶 谓 元 结构 使用 理论 圆 不了 的 东西 
就 靠 假设 圆 了 心理学 模型 属性 是 连续 
系统 的 对象 结构 映射 理论 前提 2 值域 前提 
基于 结构 映射 原理 的 系统 相似 度 估算 工程 
1 映射 与 推理 系统 相似 度 计算 的 主要 
因素 个体 相似 度 匹配 对象 对 及其 相似 度 
对象 的 权重 带 比较 对象 的 系统结构 个体 相似 
度 比较 相似 度 基于 认知科学 原理 的 相似 模型 
六 当 对 对象 A 和 对象 B 进行 相似 
度 计算 时 子 对象 Ai 必须 同时 满足 如下 
两个 条件 才 与 对象 Bj 构成 匹配 对象 对 
必须 满足 关系 所 约定 的 语义 角色 限制 和序/nr 
限制 他们 的 相似 度 必须 大于 某个 阈值 比如 
给 是 三元 关系 给予 人 被 给人 物品 相似 
简单 单调 约束 递归算法 进行 处理 算法 总结 对象 表示 
高阶 谓 词论 元 结构 模仿 人类 严格 落实 认知 
规律 满足 所有 条件 简 单用户 问句 体验 面向 旅游 
领域 的 问答 系统 实验 肯定 了 结构 主义 语言学 
和 认知 语言学 的 含义 信息检索 中广 为 应用 的 
向量空间 模型 的 基础性 理论模型 图像 语义 检索 来源于 图像 
语义 结构 分析 关于 如何 学习 自然语言 处理 有 很多 
同学 通过 不同 的 途径 留过 言 这 方面 虽然 
很早 之前 写过 几篇 小 文章 如何 学习 自然语言 处理 
和 几本 自然语言 处理 入门书 但是 更 推崇 知乎 上 
这个 问答 自然语言 处理 怎么 最快 入门 里面/f 有/v 微软/a 
亚洲/ns 研究院/n 周明/nr 老师/n 的/uj 系统/n 回答/v 和/c 清华大学/nt 刘知远/nr 
老师/n 的/uj 倾情/n 奉献/v 初学者 如何 查阅 自然语言 处理 NLP 
领域 学术 资料 当然 还 包括 其他 同学 的 无私 
分享 不过 对于 希望 入门 NLP 的 同学 来说 推荐 
你们 先 看 一下 这 本书   Speech and Language 
Processing 第一版 中文名 译为 自然语言 处理 综论 作者 都是 NLP 
领域 的 大大 牛 斯坦福大学   Dan Jurafsky   教授 
和 科罗拉多 大学 的   James H . Martin   
教授 这 也是 我 当年 的 入门书 我 读过 这 
本书 的 中文版 翻译 自 第一 版 英文版 和 英文 
版 第二 版 该书 第三版 正在 撰写 中 作者 已经 
完成 了 不少 章节 的 撰写 所 完成 的 章节 
均可 下载 Speech and Language Processing 3rd ed . draft 
从 章节 来看 第三版 增加 了 不少 和 NLP 相关 
的 深度 学习 的 章节 内容/n 和/c 篇幅/n 相对于/i 之前/f 
有了/nr 更多/d 的/uj 更新/d C h a p t e 
r l i d e s R e l a 
t i o n to 2nd ed . 1 Introduction 
Ch . 1 in 2nd ed . 2 Regular Expressions 
Text Normalization and Edit DistanceText pptx pdf Edit Distance pptx 
pdf Ch . 2 and parts of Ch . 3 
in 2nd ed . 3 Finite State Transducers4 Language Modeling 
with N GramsLM pptx pdf Ch . 4 in 2nd 
ed . 5 Spelling Correction and the Noisy ChannelSpelling pptx 
pdf expanded from pieces in Ch . 5 in 2nd 
ed . 6 Naive Bayes Classification and SentimentNB pptx pdf 
Sentiment pptx pdf new in this edition 7 Logistic Regression8 
Neural Nets and Neural Language Models9 Hidden Markov Models Ch 
. 6 in 2nd ed . 10 Part of Speech 
Tagging Ch . 5 in 2nd ed . 11 Formal 
Grammars of English Ch . 12 in 2nd ed . 
12 Syntactic Parsing Ch . 13 in 2nd ed . 
13 Statistical Parsing14 Dependency Parsing new in this edition 15 
Vector SemanticsVector pptx pdf expanded from parts of Ch . 
19 and 20 in 2nd ed . 16 Semantics with 
Dense VectorsDense Vector pptx pdf new in this edition 17 
Computing with Word Senses WSD and WordNetIntro Sim pptx pdf 
WSD pptx pdf expanded from parts of Ch . 19 
and 20 in 2nd ed . 18 Lexicons for Sentiment 
and Affect E x t r a c t i 
o n e n t L e x pptx pdf 
new in this edition 19 The Representation of Sentence Meaning20 
Computational Semantics21 Information Extraction Ch . 22 in 2nd ed 
. 22 Semantic Role Labeling and Argument StructureSRL pptx pdf 
Select pptx pdf expanded from parts of Ch . 19 
and 20 in 2nd ed . 23 Neural Models of 
Sentence Meaning RNN LSTM CNN etc . 24 Coreference Resolution 
and Entity Linking25 Discourse Coherence26 Seq2seq Models and Summarization27 Machine 
Translation28 Question Answering29 Conversational Agents30 Speech Recognition31 Speech Synthesis 另外 
该书 作者 之一 斯坦福大学   Dan Jurafsky   教授 曾经在 
Coursera 上 开设 过 一门 自然语言 处理 课程 Natural Language 
Processing 该 课程 目前 貌似 在 Coursera 新课程 平台 上 
已经 查询 不到 不过 我们 在 百度 网 盘上 做 
了 一个 备份 包括 该 课程 视频 和 该书 的 
第二 版 英文 两个 一起 看 效果 更佳 链接   
https / / pan . baidu . com / s 
/ 1kUCrV8r   密码 jghn 对于 一直 寻找 如何 入门 
自然语言 处理 的 同学 来说 先把 这本书 和 这套 课程 
拿 下来 才 是 一个 必要条件 万事 先有 个 基础 
同时 欢迎 大家 关注 我们 的 公众 号 NLPJob 回复 
slp 获取 该书 和 课程 最新 资源 本 条目 发布 
于 2017年 07月 24号 属于 自然 语言 处理 分类 被 
贴了   Dan Jurafsky James H . Martin NLP 书籍 
NLP 入门 NLP 课程 Speech and Language Processing 斯坦福大学 科罗拉多 
大学 深度 学习 自然语言 处理 自然语言 处理 书籍 自然语言 处理 
入门 自然语言 处理 综论 自然语言 处理 课程   标签 作者 
是 52nlp 这里 推荐 一批 学习 自然语言 处理 相关 的 
书籍 当然 不止 是 自然 语言 处理 国内 的 书籍 
相对 比较 便宜 值得 购买 1 自然语言 处理 综论 当年 
的 入门书 不过 翻译 的 是 第一 版 英文名 Speech 
and Language Processing 第三版 据说 很快 就要 出版 2016年 有 
条件 的 同学 建议 直接 看 英文版 第二 版 2 
统计 自然语言 处理 基础 另一 本 入门 书籍 这 本书 
的 英文版 貌似 没有 更新 但是 中文版 貌似 也 不再 
发售 了 当然 优先 推荐 读 英文版 3 Python 自然语言 
处理 NLTK 配套 丛书 有了 上面 两 本书 的 介绍 
再 加上 一些 Python 基础 通过 这 本书 进行 相关 
的 文本 挖掘 实战 很 不错 的 一个 路径 4 
宗 成庆 老师 的 统计 自然语言 处理 第 2版 当年 
读书 的 时候 大致 看过 第一 版 作为 入门 书籍 
不错 5 国内 青年 学者 刘知远 老师 等 合著 的 
互联网 时代 的 机器 学习 和 自然 语言 处理 技术 
大 数据 智能 没有 仔细 看过 仅供参考 6 南大 周志华 
老师 的 西瓜 书 机器学习 最近 出版 的 书籍 国内 
难得 学习 机器 学习 的 高质量 书籍 评价 非常 高 
强烈推荐 7 CMU 机器学习 系主任 Tom Mitchell 院士 的 机器学习 
机器学习 老牌 经典 书籍 历久弥新 华章 引进 的 英文版 也 
不贵 不过 貌似 没 货 机器学习 英文版 8 比较 新的 
一本 机器学习 书籍 被 誉为 内容 全面 的 机器学习 教程 
Machine Learning 期刊 主编 力作 机器学习 9 李航 老师 的 
这本 统计 学习 基础 挺 不错 的 简洁明了 统计 学习 
基础 10 王斌 老师 翻译 的 大 数据 互联网 大 
规模 数据挖掘 与 分布式 处理 第 2版 质量 挺 不错 
的 对应 的 英文 书籍 是 Mining of Massive Datasets 
有 相应 的 官方主页 提供 相应 的 英文 PDF 课程 
和 课件 资源 自然语言 处理 中 的 若干 问题 http 
/ / blog . csdn . net / yueyedeai / 
article / details / 14524151 一 语言 模型 一 N 
元语言 模型 二 语言 模型 性能评价 三 数据 平滑 四 
语言 模型 自适应 方法 二 汉语 自动 分词 和 词性 
标注 一 基本 分词 方法 二 未登陆 词 处理 方法 
三 基于 多 特征 的 命名 实体模型 四 词性 标注 
五 词性 标注 的 一致性 检查 和 自动 校对 三 
句法分析 一 统计 句法分析 以及 句法分析 的 检查 二 层次化 
汉语 长句 结构 分析 三 浅层 句法分析 四 依据 句法 
理论 与 依存 句法分析 四 语义 消 歧 一 有 
监督 的 语义 消 歧 二 基于 词典 的 语义 
消 歧 三 无 监督 的 语义 消 歧 四 
语义 消 歧 系统 评测 五 文本 分类 一 文本 
表示 二 文本   特征选择 方法 三 特征 权重 计算方法 
四 分类器 设计 五 文本 分类器 性能 评估 方法 六 
自动 文摘 和 信息 抽取 一 多 文档 摘要 二 
单 文档 摘要 三 信息 抽取 七 文档 聚 类 
一 聚 类 算法 二 聚 类 结果 评估 八 
自然语言 处理 的 主要 范畴 1 ． 文本 朗读 Text 
  to   speech / 语音合成 Speech   synthesis 2 
． 语音识别 Speech   recognition 3 ． 中文 自动 分词 
Chinese   word   segmentation 4 ． 词性 标注 Part 
of speech   tagging 5 ． 句法分析 Parsing 6 ． 
自然语言 生成 Natural   language   generation 7 ． 文本 
分类 Text   categorization 8 ． 信息检索 Information   retrieval 
9 ． 信息 抽取 Information   extraction 10 ． 文字 
校对 Text proofing 11 ． 问答 系统 Question   answering 
12 ． 机器翻译 Machine   translation 13 ． 自动 摘要 
Automatic   summarization 14 ． 文字 蕴涵 Textual   entailment 
九 自然语言 处理 研究 的 难点 1 ．   单词 
的 边界 界 定在 口语 中 词 与 词 之间 
通常 是 连贯 的 而 界定 字词 边界 通常 使用 
的 办法 是 取用 能让 给定 的 上下文 最为 通顺 
且 在 文法上 无误 的 一种 最佳 组合 在 书写 
上 汉语 也 没有 词 与 词 之间 的 边界 
2 ． 词义 的 消 歧 许 多字词 不单 只有 
一个 意思 因而 我们 必须 选 出使 句 意 最为 
通顺 的 解释 3 ． 句法 的 模糊性 自然 语言 
的 文法 通常 是 模棱两可 的 针对 一个 句子 通常 
可能会 剖析 Parse 出 多棵 剖析 树 Parse   Tree 
而 我们 必须 要 仰赖 语意 及 前后文 的 资讯 
才能 在 其中 选择 一棵 最为 适合 的 剖析 树 
4/m ．/i 有/v 瑕疵/n 的/uj 或不/i 规范/n 的/uj 输入/v 例如/v 
语音/n 处理/v 时/n 遇到/v 外国/ns 口音/n 或/c 地方/n 口音/n 或者 
在 文本 的 处理 中 处理 拼写 语法 或者 光学 
字符识别 OCR 的 错误 5 ． 语言 行为 与 计划 
句子 常常 并不 只是 字面 上 的 意思 例如 你 
能把 盐 递 过来 吗 一个 好 的 回答 应当 
是 把 盐 递过去 在 大多数 上下文 环境 中 能 
将 是 糟糕 的 回答 虽说   回答 不 或者 
太远 了 我 拿不到 也 是 可以 接受 的 再者 
如果 一门 课程 去年 没 开设 对于 提问 这门 课程 
去年 有 多少 学生 没 通过 回答 去年 没开 这门 
课 要   比 回答 没人 没 通过 好 系列 
文章 请 多 关注 Tensorflow 源码 解析 1 – 内核 
架构 和 源码 结构 带 你 深入 AI 1 深度 
学习 模型 训练 痛点 及 解决 方法 自然语言 处理 1 
– 分词 自然语言 处理 2 – jieba 分词 用法 及 
原理 自然语言 处理 3 – 词性 标注 自然语言 处理 4 
– 句法分析 自然语言 处理 5 – 词 向量 自然语言 处理 
6 – 情感 分析 1 概述 情感 分析 是 自然 
语言 处理 中 常见 的 场景 比如 淘宝 商品 评价 
饿 了么 外卖 评价 等 对于 指导 产品 更新 迭代 
具有 关键性 作用 通过 情感 分析 可以 挖掘 产品 在 
各个 维度 的 优劣 从而 明确 如何 改进 产品 比如 
对 外卖 评价 可以 分析 菜品 口味 送达 时间 送餐 
态度 菜品 丰富 度 等 多个 维度 的 用户 情感 
指数 从而 从 各个 维 度上 改进 外卖 服务 情感 
分析 可以 采用 基于 情感 词典 的 传统 方法 也 
可以 采用 基于 深度 学习 的 方法 下面 详细 讲解 
2 基于 情感 词典 的 传统 方法 2.1 基于 词典 
的 情感 分类 步骤 基于 情感 词典 的 方法 先 
对 文本 进行 分词 和 停用词 处理 等 预处理 再利用 
先 构 建好 的 情感 词典 对 文本 进行 字符串 
匹配 从而 挖掘 正面 和 负面 信息 如 下图 2.2 
情感 词典 情感 词典 包含 正面 词语 词典 负面 词语 
词典 否定 词语 词典 程度 副词 词典 等 四 部分 
如 下图 词典 包含 两 部分 词语 和 权重 如下 
正面 很快 1.75 挺快 1.75 还好 1.2 很萌 1.75 服务到位 
1 负面 无语 2 醉了 2 没法 吃 2 不好 
2 太差 5 太油 2.5 有些 油 1 咸 1 
一般 0.5 程度 副词 超级 2 超 2 都 1.75 
还 1.5 实在 1.75 否定词 不 1 没 1 无 
1 非 1 莫 1 弗 1 毋 1 情感 
词典 在整个 情感 分析 中 至关重要 所幸 现在 有 很多 
开源 的 情感 词典 如 BosonNLP 情感 词典 它 是 
基于 微博 新闻 论坛 等 数据 来源 构建 的 情感 
词典 以及 知网 情感 词典 等 当然 我们 也 可以 
通过 语料 来 自己 训练 情感 词典 2.3 情感 词典 
文本 匹配 算法 基于 词典 的 文本 匹配 算法 相对 
简单 逐个 遍历 分词 后的/nr 语句 中的 词语 如果 词语 
命中 词典 则 进行 相应 权重 的 处理 正面 词 
权 重为 加法 负面 词 权 重为 减法 否定词 权重 
取 相反数 程度/n 副词/n 权/n 重则/n 和它/nr 修饰/v 的/uj 词语/n 
权重/n 相乘/v 如 下图 利用 最终 输出 的 权重 值 
就 可以 区分 是 正面 负面 还是 中性 情感 了 
2.4 缺点 基于 词典 的 情感 分类 简单易行 而且 通用性 
也 能够 得到 保障 但 仍然 有 很多 不足 精度 
不高 语言 是 一个 高度 复杂 的 东西 采用 简单 
的 线性 叠加 显然 会 造成 很大 的 精度 损失 
词语 权重 同样 不是 一成不变 的 而且 也 难以 做到 
准确 新词 发现 对于 新 的 情感 词 比如 给力 
牛逼 等等 词典 不一定 能够 覆盖 词典 构建 难 基于 
词典 的 情感 分类 核心 在于 情感 词典 而 情感 
词典 的 构建 需要 有 较强 的 背景 知识 需要 
对 语言 有较/nr 深刻 的 理解 在 分析 外语 方面 
会 有 很大 限制 3 基于 深度 学习 的 算法 
近年来 深度 学习 在 NLP 领域内 也是 遍地开花 在 情感 
分类 领域 我们 同样 可以 采用 深度 学习 方法 基于 
深度 学习 的 情感 分类 具有 精度高 通用 性强 不 
需要 情感 词典 等 优点 3.1 基于 深度 学习 的 
情感 分类 步骤 基于 深度 学习 的 情感 分类 首先 
对 语句 进行 分词 停用词 简繁转换 等 预处理 然后 进行 
词 向量 编码 然后 利用 LSTM 或者 GRU 等 RNN 
网络 进行 特征提取 最后/f 通过/p 全/a 连接/v 层/q 和/c softmax/w 
输出/v 每个/r 分类/n 的/uj 概率/n 从而 得到 情感 分类 3.2 
代码 示例 下面 通过 代码 来 讲解 这个 过程 下面 
是 我 周末 写 的 2018年 AI Challenger 细粒度 用户 
评论 情感 分析 比赛 中 的 代码 项目 数据 来源 
于 大众 点评 训练 数据 10 万条 验证 1 万条 
分析 大众 点评 用户 评论 中 关于 交通 菜品 服务 
等 20个 维度 的 用户 情感 指数 分为 正面 负面 
中性 和 未提及 四类 代码 在 验证 集上 目前 f1 
socre 可以 达到 0.62 3 . 2.1 分词/n 和/c 停用词/n 
预处理/vn 数据/n 预处理/vn 都/d 放在/v 了/ul PreProcessor/w 类/q 中/f 主 
函数 是 process 步骤 如下 读取 原始 csv 文件 解析 
出 原始 语句 和 标注 错别字 繁简体 拼音 语义 不 
明确 等 词语 的 处理 stop words 停用词 处理 分词 
采用 jieba 分词 进行 处理 分词 这儿 有个 trick 由于 
分词 后 较多 口语化 的 词语 不在 词 向量 中 
所以 对 这 部分 词语 从 jieba 中 del 掉 
然后 再 进行 分词 直到 只有 为数不多 的 词语 不在 
词 向量 中 为止 构建 词 向 量到 词语 的 
映射 并对 词语 进行 数字编码 这一步 比较 常规 class PreProcessor 
object def _ _ init _ _ self filename busi 
_ name = location _ traffic _ convenience self . 
filename = filename self . busi _ name = busi 
_ name self . embedding _ dim = 256 # 
读 取词 向量 embedding _ file = . / word 
_ embedding / word2vec _ wx self . word2vec _ 
model = gensim . models . Word2Vec . load embedding 
_ file # 读取 原始 csv 文件 def read _ 
csv _ file self reload sys sys . s e 
t d e f a u l t e n 
c o d i n g utf 8 print after 
coding + str sys . g e t d e 
f a u l t e n c o d 
i n g data = pd . read _ csv 
self . filename sep = x = data . content 
. values y = data self . busi _ name 
. values return x y # todo 错别字 处理 语义 
不明确 词语 处理 拼音 繁体 处理 等 def correct _ 
wrong _ words self corpus return corpus # 去掉 停用词 
def clean _ stop _ words self sentences stop _ 
words = None with open . / stop _ words 
. txt r as f stop _ words = f 
. readlines stop _ words = word . replace \ 
n for word in stop _ words # stop words 
替换 for i line in enumerate sentences for word in 
stop _ words if word in line line = line 
. replace word sentences i = line return sentences # 
分词 将 不在 词 向量 中的 jieba 分词 单独 挑出来 
他们 不做 分词 def get _ words _ after _ 
jieba self sentences # jieba 分词 all _ exclude _ 
words = dict while 1 words _ after _ jieba 
= w for w in jieba . cut line if 
w . strip for line in sentences # 遍历 不 
包含 在 word2vec 中的 word new _ exclude _ words 
= for line in words _ after _ jieba for 
word in line if word not in self . word2vec 
_ model . wv . vocab and word not in 
all _ exclude _ words all _ exclude _ words 
word = 1 new _ exclude _ words . append 
word elif word not in self . word2vec _ model 
. wv . vocab all _ exclude _ words word 
+ = 1 # 剩余 未 包含 词 小于 阈值 
返回 分词 结果 结束 否则 添加到 jieba del _ word 
中 然后 重新 分词 if len new _ exclude _ 
words 10 print length of not in w2v words % 
d words are % len new _ exclude _ words 
for word in new _ exclude _ words print word 
print \ nall exclude words are for word in all 
_ exclude _ words if all _ exclude _ words 
word 5 print % s % d % word all 
_ exclude _ words word return words _ after _ 
jieba else for word in new _ exclude _ words 
jieba . del _ word word raise Exception get _ 
words _ after _ jieba error # 去除 不在 词 
向量 中的 词 def remove _ words _ not _ 
in _ embedding self corpus for i sentence in enumerate 
corpus for word in sentence if word not in self 
. word2vec _ model . wv . vocab sentence . 
remove word corpus i = sentence return corpus # 词 
向量 建立 词语 到 词 向量 的 映射 def form 
_ embedding self corpus # 1 读 取词 向量 w2v 
= dict zip self . word2vec _ model . wv 
. index2word self . word2vec _ model . wv . 
syn0 # 2 创建 词语 词典 从而 知道 文本 中 
有 多少 词语 w2index = dict # 词语 为 key 
索 引为 value 的 字典 index = 1 for sentence 
in corpus for word in sentence if word not in 
w2index w2index word = index index + = 1 print 
\ nlength of w2index is % d % len w2index 
# 3 建立 词语 到 词 向量 的 映射 # 
embeddings = np . random . randn len w2index + 
1 self . embedding _ dim embeddings = np . 
zeros shape = len w2index + 1 self . embedding 
_ dim dtype = float embeddings 0 = 0 # 
未 映 射到 的 词语 全部 赋值 为 0 n 
_ not _ in _ w2v = 0 for word 
index in w2index . items if word in self . 
word2vec _ model . wv . vocab embeddings index = 
w2v word else print not in w2v % s % 
word n _ not _ in _ w2v + = 
1 print words not in w2v count % d % 
n _ not _ in _ w2v del self . 
word2vec _ model w2v # 4 语料 从中 文词 映射 
为 索引 x = w2index word for word in sentence 
for sentence in corpus return embeddings x # 预处理 主 
函数 def process self # 读取 原始 文件 x y 
= self . read _ csv _ file # 错别字 
繁简体 拼音 语义 不明确 等 的 处理 x = self 
. correct _ wrong _ words x # stop words 
x = self . clean _ stop _ words x 
# 分词 x = self . get _ words _ 
after _ jieba x # remove 不在 词 向量 中的 
词 x = self . remove _ words _ not 
_ in _ embedding x # 词 向 量到 词语 
的 映射 embeddings x = self . form _ embedding 
x # 打印 print embeddings 1 is embeddings 1 print 
corpus after index mapping is x 0 print length of 
each line of corpus is len line for line in 
x return embeddings x y3 . 2.2 词 向量 编码 
词 向量 编码 步骤 主要有 加载 词 向量 词 向量 
可以 从 网上 下载 或者 自己 训练 网上 下载 的 
词 向量 获取 简单 但 往往 缺失 特定 场景 的 
词语 比如 大众 点评 菜品 场景 下 的 鱼香肉丝 干锅 
花菜 等 词语 而且 往往 这些 词语 在 特定 场景 
下 还 十分 重要 而 自己 训练 则 需要 几百 
G 的 语料 在 高性能 服务器 上 连续 训练 好几天 
成本 较高 可以 将 两种 方法 结合 起来 也 就是 
加载 下载 好 的 词 向量 然后 利用 补充 语料 
进行 增量 训练 建立 词语 到 词 向量 的 映射 
也 就是 找到 文本 中 每个 词语 的 词 向量 
对 文本 进行 词 向量 编码 可以 通过 keras 的 
Embedding 函数 或者 其他 深度 学习 库 来 搞定 前 
两步 在上面 代码 中 已经 展示 了 词 向量 编码 
代码 示例 如下 Embedding input _ dim = len embeddings 
output _ dim = len embeddings 0 weights = embeddings 
input _ length = self . max _ seq _ 
length trainable = False name = embeddings _ name 3 
. 2.3 构建 LSTM 网络 LSTM 网络 主要 分为 如下 
几层 两层 的 LSTM dropout 防止 过拟合 全 连接 从而 
可以 输出 类别 softmax 将 类别 归一化 到 0 1 
之间 LSTM 网络 是 重中之重 这儿 可以 优化 的 空间 
很大 比如 可以 采用 更优 的 双向 LSTM 可以 加入 
注意力 机制 这 两个 trick 都 可以 提高 最终 准确度 
另外 可以 建立 分词 和不/nr 分词 两种 情况 下 的 
网络 最终 通过 concat 合并 class Model object def _ 
_ init _ _ self busi _ name = location 
_ traffic _ convenience self . max _ seq _ 
length = 100 self . lstm _ size = 128 
self . max _ epochs = 10 self . batch 
_ size = 128 self . busi _ name = 
busi _ name self . model _ name = model 
/ % s _ seq % d _ lstm % 
d _ epochs % d . h5 % self . 
busi _ name self . max _ seq _ length 
self . lstm _ size self . max _ epochs 
self . yaml _ name = model / % s 
_ seq % d _ lstm % d _ epochs 
% d . yml % self . busi _ name 
self . max _ seq _ length self . lstm 
_ size self . max _ epochs def split _ 
train _ data self x y x _ train x 
_ val y _ train y _ val = train 
_ test _ split x y test _ size = 
0.1 # 超长 的 部分 设置 为 0 截断 x 
_ train = sequence . pad _ sequences x _ 
train self . max _ seq _ length x _ 
val = sequence . pad _ sequences x _ val 
self . max _ seq _ length # y 弄成 
4 分类 2 未提及 1 负面 0 中性 1 正面 
y _ train = keras . utils . to _ 
categorical y _ train num _ classes = 4 y 
_ val = keras . utils . to _ categorical 
y _ val num _ classes = 4 return x 
_ train x _ val y _ train y _ 
val def build _ network self embeddings embeddings _ name 
model = Sequential model . add Embedding input _ dim 
= len embeddings output _ dim = len embeddings 0 
weights = embeddings input _ length = self . max 
_ seq _ length trainable = False name = embeddings 
_ name model . add LSTM units = self . 
lstm _ size activation = tanh return _ sequences = 
True name = lstm1 model . add LSTM units = 
self . lstm _ size activation = tanh name = 
lstm2 model . add Dropout 0.1 model . add Dense 
4 model . add Activation softmax return model def train 
self embeddings x y model = self . build _ 
network embeddings embeddings _ train model . compile optimizer = 
adam loss = categorical _ crossentropy metrics = accuracy # 
训练 采用 k folder 交叉 训练 for i in range 
0 self . max _ epochs x _ train x 
_ val y _ train y _ val = self 
. split _ train _ data x y model . 
fit x _ train y _ train batch _ size 
= self . batch _ size validation _ data = 
x _ val y _ val # 保存 model yaml 
_ string = model . to _ yaml with open 
self . yaml _ name w as outfile outfile . 
write yaml . dump yaml _ string default _ flow 
_ style = True # 保存 model 的 weights model 
. save _ weights self . model _ name def 
predict self embeddings x # 加载 model print loading model 
. . . . . . with open self . 
yaml _ name r as f yaml _ string = 
yaml . load f model = model _ from _ 
yaml yaml _ string # 加载 权重 print loading weights 
. . . . . . model . load _ 
weights self . model _ name by _ name = 
True model . compile optimizer = adam loss = categorical 
_ crossentropy metrics = accuracy # 预测 x = sequence 
. pad _ sequences x self . max _ seq 
_ length predicts = model . predict _ classes x 
# 得到 分类 结果 它 表征 的 是 类别 序号 
# 转换 classes = 0 1 2 1 predicts = 
classes item for item in predicts np . set _ 
printoptions threshold = np . nan # 全部 打印 print 
np . array predicts return predicts3 . 2.4 softmax 输出 
类别 这 一部分 上面 代码 已经 讲 到了 不在 赘述 
softmax 只是 一个 归一化 讲 数据 归一化 到 0 1 
之间 从而 可以 得到 每个 类别 的 概率 我们 最终 
取 概率 最大 的 即可 3.3 基于 深度 学习 的 
情感 分析 难点 基于 深度 学习 的 情感 分析 难点 
也 很多 语句 长度 太长 很多 用户 评论 都 特别 
长 分词 完 后 也有 几百 个 词语 而 对于 
LSTM 序列 过长 会 导致 计算 复杂 精度 降低 等 
问题 一般 解决 方法 有 进行 停用词 处理 无关 词 
处理 等 从而 缩减 文本 长度 或者 对 文本 进行 
摘要 抽离 出 语句 主要 成分 新词 和 口语化 的 
词语 特别 多 用户 评论 语句 不像 新闻 那样 规整 
新词 和 口语化 的 词语 特别 多 这个 问题 给 
分词 和词/nr 向量 带来 了 很大 难度 一般 解决 方法 
是 分词 方面 建立 用户 词典 从而 提高 分词 准确度 
词 向量 方面 对 新词 进行 增量 训练 从而 提高 
新词 覆盖率 4 . 总结 文本 情感 分析 是 NLP 
领域 一个 十分 重要 的 问题 对 理解 用户 意图 
具有 决定性 的 作用 通过 基于 词典 的 传统 算法 
和 基于 深度 学习 的 算法 可以 有效 的 进行 
情感 分析 当前 情感 分析 准确率 还 有待 提高 任重而道远 
系列 文章 请 多 关注 Tensorflow 源码 解析 1 – 
内核 架构 和 源码 结构 带 你 深入 AI 1 
深度 学习 模型 训练 痛点 及 解决 方法 自然语言 处理 
1 – 分词 自然语言 处理 2 – jieba 分词 用法 
及 原理 自然语言 处理 3 – 词性 标注 自然语言 处理 
4 – 句法分析 自然语言 处理 5 – 词 向量 自然语言 
处理 6 – 情感 分析 系列 文章 请 多 关注 
Tensorflow 源码 解析 1 – 内核 架构 和 源码 结构 
带 你 深入 AI 1 深度 学习 模型 训练 痛点 
及 解决 方法 自然语言 处理 1 – 分词 自然语言 处理 
2 – jieba 分词 用法 及 原理 自然语言 处理 3 
– 词性 标注 自然语言 处理 4 – 句法分析 自然语言 处理 
5 – 词 向量 自然语言 处理 6 – 情感 分析 
1 概述 句法分析 也是 自然语言 处理 中 的 基础性 工作 
它 分析 句子 的 句法结构 主谓宾 结构 和 词汇 间 
的 依存 关系 并列 从属 等 通过 句法分析 可以为 语义分析 
情感 倾向 观点 抽取 等 NLP 应用 场景 打下 坚实 
的 基础 随着 深度 学习 在 NLP 中的 使用 特别 
是 本身 携带 句法 关系 的 LSTM 模型 的 应用 
句法分析 已经 变得 不是 那么 必要 了 但是 在 句法结构 
十分 复杂 的 长 语句 以及 标注 样本 较少 的 
情况 下 句法分析 依然 可以 发挥 出 很大 的 作用 
因此 研究 句法分析 依然 是 很 有 必要 的 2 
句法分析 分类 句法分析 分为 两类 一类 是 分析 句子 的 
主谓宾 定 状 补 的 句法结构 另一类 是 分析 词汇 
间 的 依存 关系 如 并列 从属 比较 递进 等 
下面 详细 讲解 2.1 句法结构 分析 句法结构 分析 识别 句子 
的 主谓宾 定 状 补 并 分析 各 成分 之间 
的 关系 如 下图 句子 的 核心 HED 为 谓语 
提出 主语 SBV 为 李克强 宾语 VOB 为 支持 上海 
积极 探索 新 机制 这样 我们 就 明确 了 句子 
的 主干 结构 再 来看 细节 对于 主语 李克强 其 
修饰 定语 ATT 为 国务院 总理 对于 谓语 提出 其 
修饰 状语 ADV 为 调研 上海 外高桥 时 这个 状语 
内部 还 可以 结构 细分 通过 句法结构 分析 我们 就 
能够 分析 出 语句 的 主干 以及 各 成分 间 
关系 对于 复杂 语句 仅仅 通过 词性 分析 不能 得到 
正确 的 语句 成分 关系 比如 动词 谓语 提出 的 
主语 我们 就 可以 知道 是 李克强 而 不是 离 
提出 更近 的 同样 是 名词 的 外高桥 了 句法结构 
分析 的 标注 如下 2.2 语义 依存关系 分析 语义 依存关系 
分析 识别 词汇 间 的 从属 并列 递进 等 关系 
可以 获得 较 深层 的 语义 信息 如 以下 三 
个 不同 的 表达 方式 表达 了 同一个 语义 信息 
可见 语义 依存关系 不受 句法结构 的 影响 语义 依存关系 偏向 
于 介词 等 非 实词 的 在 语 句中 的 
作用 而 句法结构 分析 则 更 偏向 于 名词 动词 
形容词 等 实词 如 张三 吃 的 关系 为 施加 
关系 Agt 苹果 吃 的 关系 为 受事 关系 Pat 
依存关系 标注 比较 多 就不 一一 列举 了 3 句法分析 
工具 句法分析 算法 比较复杂 我们 就 不 展开 了 可以 
参考 文章 链接 介绍 下 几个 句法分析 工具 哈工大 LTP 
https / / www . ltp cloud . com / 
intro / 斯坦福 句法分析 工具 Stanford Parser https / / 
nlp . stanford . edu / software / lex parser 
. shtml 当前 句法分析 难度 还 很大 准确度 不高 哈工大 
的 LTP 也 只能 做到 80% 左右 的 准确率 4 
深度/ns 学习/v 和/c 句法分析/l 基于/p 深度/ns 学习/v 的/uj RNN/w 和/c 
LSTM/w 序列/n 模型/n 本身 可以 携带 很多 句法结构 和 依存 
关系 等 深层 信息 同时 句法分析 树结构 也 可以 和 
深度 学习 结合起来 利用 句法 分析树 可以 构建 LSTM 网络 
tree lstm 从而 对 语句 进行 文本 摘要 情感 分析 
那 是否 基于 句法 分析树 的 LSTM tree lstm 就 
一定 比 单纯 的 双向 LSTM bi lstm 效果 好吗 
研究 表明 很多 情况 下 单纯 的 bi lstm 比 
基于 句法 分析树 的 tree lstm 效果 更好 这 主要 
是 因为 当前 句法分析 准确度 不高 只有 90% 左右 如果 
是 句子成分 关系 很 复杂 则 准确率 更低 因此 给 
lstm 网络 带来 了 很大 的 噪声 从而 导致 了 
tree lstm 模型 准确度 的 降低 但是 tree lstm 可以 
使用 较少 的 标注 语料 而且在 句子 结构 复杂 的 
长 语句 上 表现 更好 因此当 语料 较少 且 句子 
结构 很 复杂 时 可以 考虑 使用 tree lstm 相关 
文章 可以 参考 哈工大 车 万翔 自然语言 处理 中 的 
深度 学习 模型 是否 依赖于 树结构 链接 5 总结 句法分析 
是 自然 语言 处理 中 的 基础性 工作 在 文本 
分析 观点 抽取 情感 分析 等 场景 下 可以 广泛 
应用 句法分析 当前 难度 还 很高 准确率 也 有待 提升 
受制于 句法分析 准确率 问题 基于 句法结构 树 的 LSTM 深度 
学习 网络 的 准确率 还 有待 进一步 提升 总之 句法分析 
任重而道远 系列 文章 请 多 关注 Tensorflow 源码 解析 1 
– 内核 架构 和 源码 结构 带 你 深入 AI 
1 深度 学习 模型 训练 痛点 及 解决 方法 自然语言 
处理 1 – 分词 自然语言 处理 2 – jieba 分词 
用法 及 原理 自然语言 处理 3 – 词性 标注 自然语言 
处理 4 – 句法分析 自然语言 处理 5 – 词 向量 
自然语言 处理 6 – 情感 分析 Python Windows10 汉语 自然语言 
处理 基本 组件 201707032018 / 01/16 Github 长期 更新 Windows 
Linux 通用 作者简介 李航 华为技术有限公司 诺亚方舟 实验室 主任 主要 研究 
方向 为 信息检索 自然语言 处理 机器学习 等 本文 经 李航 
博士 授权 发布 未经 作者 允许 不得 转载 欢迎 人工智能 
领域 技术 投稿 约稿 给 文章 纠错 请 发送 邮件 
至 heyc @ csdn . net 人类 的 语言 具 
有 什么 特性 下面 是 几位 最 权威 学者 的 
看法 语言 是 草根现象 它 像是 维基百科 聚集 了 数以十万计 
的 人 的 贡献 当 人们 要 找到 更好 的 
表达 自己 思想 方式 的 时候 就 发明 了 术语 
俚语 新 说法 其中 一 部分 积累 到 语言 中 
这 就是 我们 得到 语言 的 过程 史蒂文 平克 Steven 
Pinker 如果 语法 没有 递归结构 那么 它 将 变得 不可 
接受 的 复杂 因为 它 有了 递归 的 工具 所以 
它 能够 产生 无穷 多 的 句子 诺姆 乔姆斯基 Noam 
Chomsky 我们 通常 的 概念 系统 的 大部分 都 具有 
比 喻性 我们 的 思考 方式 我们 所 经历 的 
我们 每天 做 的 都与 比喻 有关 乔治 雷 可夫 
George Lakoff 当 一个 人 听到 或 看到 一句话 的 
时候 他 使用 自己 所有 的 知识 和 智能 去 
理解 这 不仅 包括 语法 也 包括 他 的 词汇 
知识 上下文 知识 更 重要 的 是 对 相关 事物 
的 理解 特里 威 诺 格拉德 Terry Winograd 语言 看来 
是 人 的 认知 向 外界 环境 扩展 的 核心 
手段 语言 的 进化 也许 就是 为了 扩展 我们 的 
认知 与 外界 环境 的 积极 交互 安迪 克拉克 Andy 
Clark 总结 起来 不 完全 规则性 递归性 比 喻性 知识 
关联性 交互性 是 人类 语言 的 主要 特点 这些 特性 
密切 关联 体现 了 语言 的 本质 上述 学者 对 
这些 语言 特性 的 研究 作出 了 卓越 贡献 他们 
的 论述 是 对 这些 特性 的 最佳 诠释 本文 
从 语言 的 特性 出发 讨论 为什么 让 计算机 理解 
人类 语言 自然语言 是 极其 困难 的 提出 自然语言 处理 
研究 应该 采取 的 策略 为什么 自然语言 理解 很难 自然语言 
理解 你 说 一句话 如何 判断 别人 或者 计算机 是否 
真正 理解 了 你 的 意思 这 是 一个 难解 
的 问题 到 目前 为止 自然语言 理解 主要 有 两个 
定义 一个 是 基于 表示 的 一个 是 基于 行为 
的 对于 前者 如果 你 说 哈利 波特 别人 把 
它 联系到 了 大脑 中的 哈利 波特 的 概念 表示 
那么 就 认为 他 理解 了 你 的 意思 而 
对于 后者 如果 你 说 给 我 拿 一杯 茶 
来 别人 真的 按 你 说 的 做了 行为 就 
认为 他 理解 了 你 的 意思 现在 的 人工智能 
研究 中 人们 开始 倾向 于 采用 后者 的 定义 
因为 这样 更 容易 评价 任务 驱动 端 到 端 
的 语言 理解 系统 的 能力 语言 的 特性 下面 
结合 语言学 认知科学 脑 科学 的 最新 研究 成果 对 
语言 的 主要 特性 进行 介绍 不 完全 规则性 语言 
具有 一定 规范 语言 的 规范 可以 用 语法 来 
描述 但是 几乎 所有 的 语法 规则 都 存在 例外 
语法 规则 中 一定 有 逻辑 不 一致 功能 冗余 
的 现象 正如 语言学家 爱德华 萨丕尔 Edward Sapir 所说 所有 
语法 都 有漏洞 all grammars leak 这是 为什么 其中 一个 
重要 原因 是 语言 不 是 一个 人 发明 的 
甚至 不是 一组 人 发明 的 而是 成千上万 人 经过 
成千上万 年的/nr 时间 不断 建立 起来 的 而且 在 不断 
演化 这个 过程 跟 人们 构建 维基 百科 的 过程 
非常 相似 这是 认知 学家 平克 等人 的 观点 1 
2 也被 越来越 多 的 人 接受 语言 的 基本 
单元 是 词汇 和 语法 规则 为了 顺畅 地 交流 
需要/v 人们/n 对/p 词汇/n 和/c 语法/n 有/v 基本/n 的/uj 共识/n 
及/c 准确/a 的/uj 使用/v 另一方面 词汇 和 语法 又 不是 
一成不变 的 为了 更好 地 表达 自己 的 思想 人们 
会 不断 地 去 扩展 已有 词汇 和 语法 规则 
的 使用 范围 或者 增加 新的 词汇 和 语法 规则 
语言 中 不断 有 大量 的 新词汇 涌现 但 其中 
大 部分 会 逐渐 消失 只有 真正 有 生命力 的 
表达 才能 留存 下来 每 一个 语言 的 词汇 都在/nr 
不断 增加 随着 文明 的 进步 这个 趋势 会 越来越 
明显 语法 是 相对 稳定 的 在 远古 时代 语言 
曾 经历 过 语法 大 发明 的 时期 后来 逐渐 
趋于 成熟 但是 即使 在 现代 语法 也 不是 一成不变 
的 首先 有 一个 趋势 是 语法 变得 越来越 简单 
比如 英语 中 以前 说 We shall I shall 现在 
逐渐 变成 We will I will 另外 受 其他 语言 
影响 语法 也会 发生 变异 比如 非洲 美国 裔 英语 
也 被 称为 黑人 英语 是 受 非洲 语言 影响 
而 形成 的 一种 英语 变种 在 这个 语言 中 
I working you working 是 正确 的 说法 笔者 猜测 
可能 是 受 其他 语言 的 影响 不 完全 规则性 
是 语言 作为 人类 交流 手段 而 动态 发展 的 
必然 结果 递归性 现在 普遍 认为 词汇 应该有 100万 年 
以上 的 历史 而 语法 大概 只有 7万 年 左右 
的 历史 而 正是 在 7 万年前 智人 Homo Sapiens 
也 就是 现在 人类 的 祖先 开始 从 非洲 大陆 
迁移 至 欧亚大陆 与此同时 开始 发明 各种 语言 语言学 中 
只要有 口头语 就 被 认为 是 语言 而 不 需要 
有 书面语 黑猩猩 也能 使用 一些 简单 的 词汇 但 
我们 不 认为 黑猩猩 拥有 语言 因为 它们 不 能把 
词汇 组合 起来 构成 句子 组 合性 递归性 是 语言 
的 重要 特点 递归 的 例子 如下 她 觉得 很好 
他 认为 她 觉得 很好 我 想 他 认为 她 
觉得 很好 ⋯ ⋯ 理论 上 可以 无限 扩展 1956年 
语言学家 乔姆斯基 提出 了 文法 体系 在 人类 历史 上 
首次 用 数学 模型 对 语法 现象 做出 严谨 的 
刻画 乔姆斯基 特别 指出 递归性 属于 语法 的 重要 特性 
只有 有了 递归 这个 工具 我们 才 能够 生成 无穷 
多 的 表达 语言 才 拥有 丰富 的 表达 能力 
3 比 喻性 比喻 的 本质 是 把 表面 不相 
关联 的 概念 通过 它们 背后 的 相似性 联系起来 比如 
微信 里 的 潜水 把 潜水 和在 微信 里 沉默不语 
这 两个 概念 联系起来 就是 一个 比喻 认知 科学家 雷 
可夫 等 认为 比喻 是 语言 的 重要 特性 语言 
中 的 发明 基本 都是/nr 基于 比喻 的 4 6 
比喻 的 使用 是 人类 认知 能力 语言 能力 的 
体现 中文 说 开灯 英语 说 turn on the light 
应该 始于 比喻 开始 有 一个 人 或 几个 人 
同时 发明 了 这些 比喻 后来 变成 了 固定 说法 
被 广泛 使用 据 观察 一个 英语 母语 的 四岁 
男孩儿 有 创意 地 说出 open the light 直译 就是 
开灯 这个 例子 说明 人 天生 就有 比喻 创造 的 
能力 比喻 是否 能被/nr 接受 并 在 语言 中 使用 
具有 一定 的 偶然性 一旦 比喻 变成 固定 用法 人们 
就 开始 习惯性 地 使用 而不 考虑 其 缘由 比如 
中 文中 所说 的 上厕所 下厨房 这些 习惯用法 都是 比 
喻性 的 但是 随着 时间 的 推移 已经 很难 考证 
当初 为什么 做出 这样 的 比喻 互联 网上 有 许多 
关于 上厕所 下厨房 语源 的 讨论 比喻 也 依赖于 语言 
使用 的 环境 与 文化 据说 在 大多数 语言 里 
都有 温暖 的 爱 这个 比喻 如 英语 中说 warm 
affection 在 日语 中说 暖 か い 愛 这些 语言 
都是 温带 和 寒带 的 语言 热带 的 语言 里 
就 看不到 这样 的 比喻 知识 关联性 十几 年前 脑 
科学 研究 中 有 一个 有趣 的 发现 当 把 
电极 插到 猴子 的 大脑 前 运动 皮质 pre motor 
cortex 时 有/v 一个/m 脑细胞/n 会在/i 猴子/n 自己/r 吃/v 香蕉/n 
和看/nr 别人/r 吃/v 香蕉/n 时/n 同样 处于 兴奋 状态 也 
就是说 对 猴子 来说 这个 脑细胞 对应 着 吃 香蕉 
的 概念 猴子/n 和人的/nr 运动/vn 都/d 是由/i 小脑/n 控制/v 但 
大脑 的 前 运动 皮质 也与 运动 有关 后来 对 
人脑 做 类似 的 实验 但 使用 功能 磁共振 让人 
实际 做 和 想象 做 各种 动作 比如 张嘴 和 
想象 张嘴 接球 和 想象 接球 结果 发现 对 同一 
动作 实际 做 和 想象 做 大脑 的 前 运动 
皮质 中 发生 反应 的 部位 完全一致 现在 一个 得到 
广泛 支持 的 理论 认为 对于 同 一个 概念 大脑 
用 固定 的 脑细胞 去 记忆 人 理解 语言 的 
过程 就是 激活 相关 概念 的 脑细胞 并 关联 这些 
概念 的 过程 6 表示 同 一个 概念 的 脑细胞 
可以 通过 不同 的 方式 被 激活 例如 有 一个 
细胞 表示 人在 喝水 当 你 看到 人 在 喝水 
的 时候 或者 当 你 从 书中 读到 人在 喝水 
的 时候 这个 脑细胞 同样会 被 激活 这也/i 能/v 解释/v 
为什么/r 我们/r 在/p 读/v 小说/n 的/uj 时候/n 常常/d 有/v 身临其境/l 
的/uj 感觉/n 每个人 把 自己 经历 的 事件 进行 编码 
存储 记忆 在 脑细胞 中 在与 外界 的 交互 中 
这些 脑细胞 被 激活 相关 的 记忆 被 唤醒 所以 
不同 人 对 同样 的 语言 会 有 不同 的 
理解 因为 他们 的 经历 不同 但也 有 许多 共性 
因为 大家 在 交流 过程 中 相互 激活 对方 脑中 
的 表示 相同 内容 的 细胞 发明 比喻 的 时候 
大脑 中 表示 两个 不同 概念 的 部位 都 开始 
兴奋 相关 的 脑细胞 之间 产生 新的 连接 概念 之间 
产生 关联 这个 过程 被 称为 神经 结合 neural binding 
是 现在 脑 科学 研究 的 重要 课题 6 语言 
的 理解 实际上 动用 了 大脑 中 所有 的 相关 
知识 是 一个 非常 复杂 的 过程 这 一点 在 
计算机 学家 威 诺 格拉德 开发 的 著名 的 对话 
系统 SHRDLU 中 也有 充分体现 7 交互性 语言 作为 人类 
交流 的 工具 其 重要 特点 就是 交互 哲学家 克拉克 
等人 认为 与 环境 的 交互 是 人 或者 动物 
作为 智能 体 存在 的 必要条件 或者说 离开了 与 环境 
的 交互 智能 就 无从谈起 8 脑 科学家 赫尔德 Richard 
Held 和 海恩 Alan Hein 的 实验 能够 很好 地 
说明 与 环境 的 交互 对 智能体 的 重要性 9 
实验 对象 是 一对 刚 出生 的 孪生 猫 把 
其中 一只 当作 主动 猫 另一 只 当作 被动 猫 
白天 把 它们 放到 转 马上 主动 猫 脚 能 
着地 可以 行走 被动 猫 被 放在 篮子 里 不能 
行走 主动 猫 走动 时 转马 被 带动 旋转 这时 
被动 猫 也 跟着 旋转 晚上 把 它们 放到 黑 
暗处 让 它们 吃睡 两个月 以后 将 它们 放出去 主动 
猫 和 一般 的 猫 没有 什么 不同 可以 正常 
行走 但 被动 猫 已经 失去 了 行走 的 能力 
走路 时 要么 撞墙 要么 跌倒 赫尔德 和 海恩 对 
10对 孪生 猫 做 同样 的 实验 得到 同样 的 
结论 以上 实验 说明 对人 或者 动物 来说 虽然 拥有 
先天 能力 但在 成长 的 过程 中 如果 不能 在 
与 环境 的 交互 中 使用 该 能力 也 会 
丧失 这 一点 语言 能力 也 一样 当 狼孩 被 
发现 时 他 已 不会 说话 因为 在 他 的 
成长 阶段 没有 与 人 进行 语言 交互 没有 学习 
语言 语言 的 理解 需要 在 与 环境 包括 社会 
文化 的 交互 中 进行 这点 可以 在 外语 学习 
的 过程 中 体会 到 在 外语 使用 环境 中 
学习外语 最 容易 理解 提高 也 最快 严格地 说 语言 
是 不能 翻译 的 只能 解释 语言 必须 在其 环境 
中 学习 与 使用 自然语言 理解 的 困难 人 的 
语言 理解 是 一个 非常 复杂 的 过程 现在 科学 
对其 有了 非常 粗浅 的 了解 离 理解 明了 所有 
细节 的 程度 还 相差 甚远 同时 让 计算机 理解 
人类 的 语言 是 极其 困难 的 因为 当代 计算机 
和 人脑 拥有 完全 不同 的 架构 在 当代 计算机 
上 实现 不 完全 规则性 和 递归性 意味着 进行 复杂 
的 组合 计算 实现 比 喻性 知识 关联性 交互性 意味着 
进行 全局 的 穷举 计算 是否 可行 仍 存在 巨大 
疑问 实现 能 像人 一样 理解 语言 的 计算机 需要 
有 全新 的 体系 架构 意味着 计算机科学 发生 革命性 的 
进步 让 计算机 处理 有限 的 语言 表达 让 它 
看似 很 智能 其实 不难 只要 写出 有限 的 规则 
就 有可能 做到 这样 的 系统 做出 的 演示 往往 
具有 一定 的 欺骗性 让人 误以为 实现 了 语言 理解 
其实 一个 系统 能够 理解 语言 意味着 理论 上 能够 
理解 无穷 多 的 语言 表达 例如 表 1 给 
出了 给 我 拿 一杯 茶 来 的 部分 同义 
说法 理论 上 类似 的 表达 是 无穷 多 的 
一个/m 能/v 理解/v 语言/n 的/uj 计算机/n 应该/v 能够/v 判断/v 这样/r 
的/uj 表达/v 都是/nr 同一/b 个/q 意思/n 而 这 不是 一件 
容易 的 事情 关键 是 要让 计算机 拥有 强健 的 
通用 的 语言 处理 能力 人们 的 错觉 人们 通常 
认识 不到 计算机 的 自然 语言 理解 极具 困难 这一 
事实 可能 有 以下 几个 原因 自然语言 具有 一定 的 
规律 很多 人 以为 只要 写 一些 规则 就 可以 
实现 自然 语言 理解 系统 这 只是 看到 了 一些 
非常 表面 的 现象 人脑 的 信息 处理 大 部分 
都是 在下 意识 中 进行 有人 说 其 比例 高达 
98% 意识 进行 的 是 顺序 处理 下意识 进行 的 
是 并行 处理 语言 处理 也 一样 也 就是说 人脑 
进行 的 大量 的 语言 处理 我们 自己 是 感受 
不到 的 认为 语言 理解 比较简单 实际上 是 我们 的 
错觉 正如 彩虹 日出 日落 我们 所 能 直观 感受到 
的 只是 现实 中 发生 的 很小 一 部分 绝 
大 部分 人 可以 在 12岁 之前 几乎 无 障碍 
地 学会 自己 的 母语 在 这个 过程 中 伴随 
着 大脑 的 发育 可以 在 很短 的 时间 内 
掌握 大量 的 词汇 和 复杂 的 语法 规则 这个 
现象 是 一种 奇迹 仍然 是 认知 科学 研究 的 
重要 课题 自然语言 处理 的 策略 自然语言 处理 自然语言 理解 
是 困难 的 但是 自然语言 处理 却是 可行 的 现实 
中 可以 让 计算机 完成 一些 特定 的 语言 处理 
任务 比如 自动 问答 机器翻译 多轮 对话 为 人们 提供 
帮助 使 计算机 成为 人类 的 智能 助手 现在 已 
部分 实现 在 可 预见 的 未来 可以 基本 实现 
这也 是 现在 自然语言 处理 研究 的 目标 自然语言 处理 
之所以 现实 可行 主要 是 因为 将 人 的 语言 
理解 过程 进行 了 合理 的 简化 或者 限制 而 
这些 简化 与 限制 可以 回避 自然语言 理解 中 的 
难题 让 计算机 表面上 像人 一样 完成 语言 处理 任务 
下面 以 知识 问答 和 多轮 对话 为例 来 说明 
人 的 知识 问答 可能 有 这样 的 处理 得到 
问题 以后 分析 问题 的 内容 理解 问题 的 意思 
进行 相关 的 推理 检索 相关 的 知识 决定 回答 
的 内容 最后 产生 回答 现在 计算机 做 知识 问答 
没有 真正 的 自然 语言 理解 通常 把 其中 的 
困难 步骤 省略 简化 计算机 的 知识 问答 一般 只有 
以下 步骤 分析 问题 的 内容 检索 相关 的 知识 
产生 回答 见 人 的 对话 可能 有 这样 的 
处理 对方 发话 以后 分析 发话 的 内容 理解 发话 
的 意图 进行 相关 的 推理 决定 回话 的 内容 
最后 产生 回话 如果 对话 是 多轮 还有 对话 管理机制 
现在 计算机 做 多轮 对话 没有 真正 的 自然 语言 
理解 通 常把 对话 的 领域 固定 比如 订机票 订 
酒店 并 只能 在 这个 领域 内 进行 见 两 
大 策略 我们 认为 自然语言 处理 可以 采用 任务 驱动 
与 混合 模式 两 大 策略 任务 驱动 的 自然 
语言 处理 就是 在 具体 的 应用 中 构建 系统 
这是 现在 自然语言 处理 通常 采用 的 策略 仍 可以 
加强 任务 驱动 的 好处 是 可以 帮助 解决 避开 
自然语言 理解 之后 仍 存在 的 一些 问题 而 这些 
问题 在 实际 应用 中 也 相对 容易 解决 可以 
认为 自然语言 处理 经历 了 三代 技术发展 演进 第一代 基于 
规则 第二代 基于 统计 第三代 属于 现在 基于 深度 学习 
各自/r 有/v 优势/n 和/c 局限/n 未来 的 发展 方向 应该 
是 将 这些 不同 的 技术 有效 地 结合 起来 
即 采用 混合 模式 任务 驱动 人工智能 系统 都 遵循 
这样 的 规律 我们 称作 人工智能 闭环 先 有系统 后有 
用户 然后 产生 大量 数据 机器学习 算法 可以 基于 数据 
构建 模型 提高 系统 的 性能 系统 性能 提高 后 
又能 更好 地 服务于 用户 形成 一个 闭环 人工智能 系统 
可以 在 这个 闭环 中 不断 改进 提升 智能 水平 
自然语言 处理 也不例外 当 任务 确 定时 就 更容易 开展 
基于 人工智能 闭环 的 技术 开发 混合 模式 统计 方法 
比起 规则 方法 能够 更好 地 应对 不确定性 人类 的 
智能 包括 语言 能力 从 数学 角度 来看 最大 的 
特点 就是 拥有 不 确定 性 事实证明 统计 方法 是 
应对 不确定性 的 最 有利 工具 统计 方法 可以 从 
数据 中 概括 出 概率 统计 规律 构建 模型 拥有 
举一反三 的 泛化 能力 规则 方法 则 不具备 这一 能力 
深度 学习 本质 也 是 统计 方法 其 特点 是 
复杂 非线性 模型 的 学习 相比之下 传统 的 统计 方法 
的 模型 都是/nr 简单 的 事实证明 相比 传统 的 统计 
方法 深度 学习 有 更强 的 模式 学习 能力 能够 
更好 地 处理 复杂 的 模式识别 问题 规则 方法 可以 
有效 地 利用 人 给定 的 知识 而 统计 方法 
和 深度 学习 方法 至少 是 现在 还 没有 和 
知识 推理 有效 地 结合 起来 统计 方法 深度/ns 学习/v 
方法/n 都/d 依赖/v 于/p 数据/n 在 没有 数据 或 数据 
稀少 的 情况 下 很难 有 用武之地 而 规则 方法 
在 这种 情况 下 至少 可以 派 上 一定 用场 
综上所述 规则 统计 即 统计 机器学习 深度/ns 学习/v 三种/m 方法/n 
都/d 各有/i 优势/n 和/c 局限/n 见表 2 可以 预见 将 
三者 有效 地 结合 会使 人工智能 自然语言 处理 的 水平 
大 幅度 提升 这是 自然语言 处理 未来 的 发展 方向 
华为 研究 团队 最近 提出 了 受教 式 人工智能 Educated 
AI EAI 的 想法 认为 这 是 未来 人工智能 的 
范式 其 核心 思想 是 人工 智能系统 拥有 基本 的 
处理 以及 学习 能力 在 用户 的 指导 下 不断 
提高 智能 水平 10 受教 式 人工智能 采用 的 就是 
混合 模式 因 为人 的 指导 有时 是以 规则 的 
形式 呈现 的 自然语言 处理 新 时代 表 3 总结 
了 现在 自然语言 处理 在 各个 任务 上 所能 达到 
的 水平 是从 不同 数据集 上 得到 的 实验 结果 
可以 看出 自然语言 处理 距离 人们 的 期待 还有 一定 
的 差距 现实 中 这些 任务 也 只是 部分 实现 
了 实用化 可以 预见 在 不远 的 将来 随着 自然语言 
处理 技术 的 进步 这些 性能 指标 会 不断 提升 
事实上 近年 深度 学习 在 自然 语言 处理 的 应用 
已使 机器翻译 单轮 对话 有了/nr 令人 惊喜 的 进步 计算机 
能够 自如 地 进行 自然语言 处理 的 时代 为期不远 人工智能 
闭环 会 推动 技术 的 不断 改进 规则 统计 深度 
学习 的 结合 会 产生 更 强大 的 技术 现在 
我们 正在 进入 自然语言 处理 的 一个 全新 的 时代 
参考文献 1 Pinker . The Language Instinct 1994 . 2 
Pinker . Linguistics as a Window to Understanding the Brain 
. Big Think 2013 . 3 Chomsky N . Three 
models for the description of language J . IRE Transactions 
on Information Theory 1956 2 3 113 124 . 4 
Taylor J . Linguistic Categorization Prototypes in Linguistic Theory 1996 
. 5 Lakoff G Johnson M . Metaphors We Live 
by 1980 . 6 Lakoff G . What Studying the 
Brain Tells Us About ArtsEducation 2013 . 7 Winograd T 
. Understanding Natural Language J . Cognitive Psychology 1972 3 
1 1 191 . 8 Clark A . Supersizing the 
Mind Embodiment Action and Cognitive Extension 2010 . 9 Held 
R Hein A . Movement Produced Stimulation in Development of 
Visually Guided Behavior J . Journal of Comparative and Physiological 
Psychology 1963 56 5 872 6 . 10 李航 张 
宝峰 霍 大伟 等 . 华为 研究 的 畅想 Educated 
AI . 中国计算机学会 通讯 2016 12 1 62 65 . 
自然语言 处理 五 传统 机器学习 1 . 朴素 贝叶斯 的 
原理 1.1 朴素 贝叶斯 相关 的 统计 学 知识 1.2 
基本 定义 2 . 利用 朴素 贝叶斯 模型 进行 文本 
分类 2.1 模型 原理 与 训练 3 . SVM 的 
原理 3.1 快速 理解 SVM 原理 4 . 利用 SVM 
模型 进行 文本 分类 5 . pLSA 共轭 先验 分布 
LDA 主题 模型 原理 6 . 使用 LDA 生成 主题 
特征 在 之前 特征 的 基础 上 加入 主题 特征 
进行 文本 分类 传统 机器学习 1 . 朴素 贝叶斯 的 
原理 1.1 朴素 贝叶斯 相关 的 统计 学 知识 贝叶斯 
学派 很 古老 但是/c 从/p 诞生/v 到/v 一百/m 年前/nr 一直/d 
不是/c 主流/b 主流 是 频率 学派 频率 学派 的 权威 
皮尔逊 和 费歇尔 都对 贝叶斯 学派 不屑一顾 但是 贝叶斯 学派 
硬是 凭借 在 现代 特定 领域 的 出色 应用 表现 
为 自己 赢得 了 半壁江山 贝叶斯 学派 的 思想 可以 
概括 为 先验概率 + 数据 = 后验/nr 概率 也 就是说 
我们 在 实际 问题 中 需要 得到 的 后验/nr 概率 
可以 通过 先验概率 和 数据 一起 综合 得到 数据 大家 
好 理解 被 频率 学派 攻击 的 是 先验概率 一般来说 
先验概率 就是 我们 对于 数据 所在 领域 的 历史 经验 
但是 这个 经验 常常 难以 量化 或者 模型 化 于是 
贝叶斯 学派 大胆 的 假设 先验 分布 的 模型 比如 
正态分布 beta 分布 等 这个 假设 一般 没有 特定 的 
依据 因此 一直 被 频率 学派 认为 很 荒谬 虽然 
难以 从 严密 的 数学逻辑 里 推出 贝叶斯 学派 的 
逻辑 但是 在 很多 实际 应用 中 贝叶斯 理论 很好用 
比如 垃圾邮件 分类 文本 分类 我们 先 看看 条件 独立 
公式 如果 X 和Y/nr 相互 独立 则有 P X Y 
= P X P Y P X Y = P 
X P Y 我们 接 着 看看 条件概率 公式 P 
Y | X = P X Y / P X 
P Y | X = P X Y / P 
X P X | Y = P X Y / 
P Y P X | Y = P X Y 
/ P Y 或者说 P Y | X = P 
X | Y P Y / P X P Y 
| X = P X | Y P Y / 
P X 接着 看看 全 概率 公式 P X = 
∑ kP X | Y = Yk P Yk 其中 
∑ kP Yk = 1P X = ∑ kP X 
| Y = Yk P Yk 其中 ∑ kP Yk 
= 1 从 上面 的 公式 很容易 得出 贝叶斯 公式 
P Yk | X = P X | Yk P 
Yk ∑ kP X | Y = Yk P Yk 
基于 朴素 贝叶斯 公式 比较 出 后验/nr 概率 的 最大值 
来 进行 分类 后验/nr 概率 的 计算 是由 先验概率 与 
类 条件概率 的 乘积 得出 先验概率 和类/nr 条件概率 要 通过 
训练 数据集 得出 即为 朴素 贝叶斯 分类 模型 将其 保存为 
中间 结果 测试 文档 进行 分类 时调 用 这个 中间 
结果 得出 后验/nr 概率 1.2 基本 定义 朴素 贝叶斯 分类 
是 一种 十分 简单 的 分类 算法 叫 它 朴素 
贝叶斯 分类 是因为 这种 方法 的 思想 真的 很 朴素 
朴素 贝叶斯 的 思想 基础 是 这样 的 对于 给出 
的 待 分 类项 求解 在 此项 出现 的 条件 
下 各个 类别 出现 的 概率 哪个 最大 就 认为 
此 待 分 类项 属于 哪个 类别 朴素 贝叶斯 分类 
的 正式 定义 如下 1 设       为 
一个 待 分 类项 而 每个 a 为 x 的 
一个 特征 属性 2 有 类别 集合 3 计算 4 
如果 则 那么 现在 的 关键 就是 如何 计算 第 
3步 中的 各个 条件概率 我们 可以 这么 做 1 找到 
一个 已知 分类 的 待 分 类项 集合 这个 集合 
叫做 训练样本 集 2 统计 得到 在 各类 别下 各个 
特征 属性 的 条件 概率 估计 即 3 如果 各 
个 特征 属性 是 条件 独立 的 则 根据 贝叶 
斯定理 有 如下 推导 因为 分母 对于 所有 类别 为 
常数 因为 我们 只要 将 分子 最大化 皆可 又 因为 
各 特征 属性 是 条件 独立 的 所以有 2 . 
利用 朴素 贝叶斯 模型 进行 文本 分类 2.1 模型 原理 
与 训练 朴素 贝叶斯 分类器 是 一种 有 监督 学习 
常见 有 两种 模型 多项式 模型 multinomial model 即为 词频 
型 和 伯努利 模型 Bernoulli model 即 文档 型 还有 
一种 高斯 模型 前 二者 的 计算 粒度 不一样 多项式 
模型 以 单词 为 粒度 伯努利 模型 以 文件 为 
粒度 因此/c 二者/n 的/uj 先验概率/l 和类/nr 条件概率/i 的/uj 计算/v 方法/n 
都/d 不同/a 计算 后验/nr 概率 时 对于 一个 文档 d 
多项式 模型 中 只有在 d 中 出现 过 的 单词 
才会 参与 后验/nr 概率 计算 伯努利 模型 中 没有 在 
d 中 出现 但是 在 全局 单词表 中 出现 的 
单词 也会 参与 计算 不过 是 作为 反方 参与 的 
这里 暂 不考虑 特征 抽取 为避免 消除 测试 文档 时类/nr 
条件概率 中 有为 0 现象 而 做 的 取 对数 
等 问题 3 . SVM 的 原理 3.1 快速 理解 
SVM 原理 很多 讲解 SVM 的 书籍 都 是从 原理 
开始 讲解 如果 没有 相关 知识 的 铺垫 理解 起来 
还是 比较 吃力 的 以下 的 一个 例子 可以 让 
我们 对 SVM 快速 建立 一个 认知 给定 训练样本 支持 
向量 机 建立 一个 超平面 作为 决策 曲面 使得 正 
例和 反例 的 隔离 边界 最大化 决策 曲面 的 初步 
理解 可以 参考 如下 过程 1 如 下图 想象 红色 
和 蓝色 的 球 为 球 台上 的 桌球 我们 
首先 目的 是 找到 一条 曲线 将 蓝色 和 红色 
的 球 分开 于是 我们 得到 一条 黑色 的 曲线 
2 为了 使 黑色 的 曲线 离 任意 的 蓝球 
和 红球 距离 也 就是 我们 后面 要 提到 的 
margin 最大化 我们 需要 找到 一条 最优 的 曲线 如 
下图 3 想象 一下 如果 这些 球 不是 在 球 
桌上 而是 被 抛向 了 空中 我们 仍然 需要 将 
红色 球 和 蓝色 球 分开 这时 就 需要 一个 
曲面 而且 我们 需要 这个 曲面 仍然 满足 跟 所有 
任意 红球 和 蓝球 的 间距 的 最大化 需要 找到 
的 这个 曲面 就是 我们 后面 详细 了解 的 最优 
超平面 4 离 这个 曲面 最近 的 红色 球 和 
蓝色 球 就是 Support Vector 限时 活动 | 入行 AI 
必看 中文 自然语言 处理 入门 作为 AI 初学者 我们 时常 
面临 这样 的 尴尬 市面 上 自然 语言 处理 内容 
大抵 为 英文 中文 的 处理 比 英文 复杂 的 
多 网上 的 相关 资料 少之又少 国内 纯中文 自然语言 处理 
书籍 只有 理论 方面 的 却在 实战 方面 比较 空缺 
而 我们 的 达人 课 中文 自然语言 处理 入门 专门 
应对 这些 困境 21 节 精品 内容 带 你 边学边 
实战 6 个 极 简 案例 快速 掌握 基本 能力 
9 步 制作 自己 的 中文 聊天 机器人 Neo4j 从 
入门 到 构建 知识图谱 原价 39 元 限时 促销 只要 
3.99 赶快 扫 码 领取 你 的 专属 学习 福利 
带 你 重新 认识 NLP 人工智能 研究 的 各个 分支 
包括 专家系统 机器学习 进化 计算 模糊逻辑 计算机 视觉 自然语言 处理 
推荐 系统 机器学习 一种 实现 人工智能 的 方法 机器学习 最 
基本 的 做法 是 使用 算法 来 解析 数据 从中 
学习 然后 对 真实世界 中 的 事件 做出 决策 和 
预测 与 传统 的 为 解决 特定 任务 硬 编码 
的 软件 程序 不同 机器学习 是 用 大量 的 数据 
来 训练 通过 各种 算法 从 数据 中 学习 如何 
完成 任务 举个 简单 的 例子 当 我们 浏览 网上商城 
时 经常 会 出现 商品 推荐 的 信息 这是 商城 
根据 你 往 期 的 购物 记录 和 冗长 的 
收藏 清单 识别 出 这 其中 哪些 是 你 真正 
感兴趣 并且 愿意 购买 的 产品 这样 的 决策模型 可以 
帮助 商城 为 客户 提供 建议 并 鼓励 产品 消费 
机器学习 直接 来源于 早期 的 人工智能 领域 传统 的 算法 
包括 决策树 聚 类 贝叶斯 分类 支持 向量 机 EM 
Adaboost 等等 从 学习 方法 上来 分 机器学习 算法 可以 
分为 监督 学习 如 分类 问题 无 监督 学习 如 
聚 类 问题 半 监督 学习 集成 学习 深度 学习 
和 强化 学习 传统 的 机器学习 算法 在 指纹 识别 
基于 Haar 的 人脸 检测 基于 HoG 特征 的 物体 
检测 等 领域 的 应用 基本 达到 了 商业化 的 
要求 或者 特定 场景 的 商业化 水平 但 每 前进 
一步 都 异常 艰难 直到 深度 学习 算法 的 出现 
深度 学习 一种 实现 机器 学习 的 技术 深度 学习 
本来 并 不是 一种 独立 的 学习 方法 其 本身 
也 会用 到有 监督 和无/nr 监督 的 学习 方法 来 
训练 深度 神经网络 但 由于 近几年 该 领域 发展 迅猛 
一些 特有 的 学习 手段 相继 被 提出 如 残差 
网络 因此 越来越 多 的 人 将其 单独 看作 一种 
学习 的 方法 最初 的 深度 学习 是 利用 深度 
神经 网络 来 解决 特征 表达 的 一种 学习 过程 
深度 神经 网络 本身 并 不是 一个 全新 的 概念 
可 大致 理解为 包含 多个 隐含 层 的 神经 网络结构 
为了 提高 深层 神经 网络 的 训练 效果 人们 对 
神经元 的 连接 方法 和 激活 函数 等 方面 做出 
相应 的 调整 其实 有 不少 想法 早年间 也曾 有过 
但 由于 当时 训练 数据 量 不足 计算能力 落后 因此 
最终 的 效果 不 尽如人意 深度 学习 摧枯拉朽 般 地 
实现 了 各种 任务 使得 似乎 所有 的 机器 辅助 
功能 都 变为 可能 无人 驾驶 汽车 预防性 医疗保健 甚至 
是 更好 的 电影 推荐 都 近在眼前 或者 即将 实现 
1 . 什么 是 NLP 人 与人 人与 计算机 交互 
中 的 语言 问题 能力 模型 通常 是 基于 语言学 
规则 的 模型 建立在 人 脑中 先天 存在 语法 通则 
这一 假设 的 基础 上 认为 语言 是 人脑 的 
语言 能力 推导 出来 的 建立 语言 模型 就 是 
通过 建立 人工 编辑 的 语言 规则 集 来 模拟 
这种 先天 的 语言 能力 又称 理性主义 的 语言 模型 
应用 模型 根据 不同 的 语言 处理 应用 而 建立 
的 特定 语言 模型 通常 是 基于 统计 的 模型 
又称 经验主义 的 语言 模型 使用 大 规模 真实 语料库 
中 获得 语言 各级 语言 单位 上 的 统计 信息 
依据 较 低级语言 单位 上 的 统计 信息 运用 相关 
的 统计 推理 技术 计算 较 高级语言 单位 上 的 
统计 信息 2 分词 词 是 最小 的 能够 独立 
活动 的 有 意义 的 语言 成分 英文单词 之间 是以 
空格 作为 自然 分界符 的 而 汉语 是以 字 为 
基本 的 书写 单位 词语 之间 没有 明显 的 区分 
标记 因此 中文 词语 分析 是 中文 信息 处理 的 
基础 与 关键 中文分词 技术 可 分为 三大类 基于 字典 
词库 匹配 的 分词 方法 基于 词频 度 统计 的 
分词 方法 和 基于 知识 理解 的 分词 方法 http 
/ / www . cnblogs . com / flish / 
archive / 2011/08 / 08/2131031 . html3 词性 标注 词性 
标注 Part of Speech tagging 或 POS tagging 又称 词类 
标注 或者 简称 标注 是 指为 分词 结果 中 的 
每个 单词 标注 一个 正确 的 词性 的 程序 也即 
确定 每个 词 是 名词 动词 形容词 或 其他 词性 
的 过程 在 汉语 中 词性 标注 比较简单 因为 汉语 
词汇 词性 多变 的 情况 比较 少见 大多 词语 只 
有一个 词性 或者 出现 频 次 最高 的 词性 远远 
高于 第二位 的 词性 据说 只需 选取 最 高频 词性 
即可 实现 80% 准确率 的 中文 词性 标注 程序 利用 
HMM 即可 实现 更高 准确率 的 词性 标注 http / 
/ blog . csdn . net / truong / article 
/ details / 188475494 命名 实体 识别 命名 实体 识别 
Named Entity Recognition 简称 NER 又 称作 专名 识别 是 
指 识别 文本 中 具有 特定 意义 的 实体 主要 
包括 人名 地名 机构 名 专有名词 等 1 实体 边界 
识别 2 确定 实体 类别 人名 地名 机构 名 或 
其他 命名 实体 识别 是 信息 提取 问答 系统 句法分析 
机器翻译 面向 Semantic Web 的 元数据 标注 等 应用 领域 
的 重要 基础 工具 基于 规则 和 词典 的 方法 
MUC 6 会议 中 几乎 所有 参赛 成员 都 采用 
基于 规则 的 方法 该 方法 需要 专家 制定 规则 
准确率 较高 但 依赖于 特征 领域 可移植性 差 基于 统计 
的 方法 主要 采用 HMM MEMM CRF 难点 在于 特征选择 
上 该/r 方法/n 能/v 获得/v 好/a 的/uj 鲁棒性/nr 和/c 灵活性/n 
不需 太多 的 人工 干预 和 领域 限制 但 需要 
大量 的 标注 集 混合 方法 采用 规则 与 统计 
相结合 多种 统计 方法 相 结合 等 是 目前 主流 
的 方法 特征 上下文 信息 + 构词法 5 指代 消解 
指代 是 一种 常见 的 语言 现象 一般 情况 下 
指代 分为 2种 回 指 和共指/nr 回 指 是 指 
当前 的 照应 语 与 上文 出现 的 词 短语 
或 句子 句群 存在 密切 的 语义 关联性 指代 依 
存于 上下文 语义 中 在 不同 的 语言 环境 中 
可能 指代 不同 的 实体 具有 非 对称性 和非/nr 传递性 
共 指 主要 是 指 2个 名词 包括 代名词 名词 
短语 指向 真实世界 中 的 同一 参照 体 这种 指代 
脱离 上下文 仍然 成立 目前 指代 消解 研究 主要 侧重 
于 等价关系 只 考虑 2个 词 或 短语 是否 指示 
现实 世界 中 同一 实体 的 问题 即 共 指 
消解 中文 的 指代 主要有 3种 典型 的 形式 1 
人称代词 pronoun 例如 李明   怕 高 妈妈 一人 呆 
在 家里 寂寞 他   便将 家里 的 电视 搬了 
过来 2 指示代词 demonstrative 例如 很多 人 都想 留下 什么 
给 孩子 这   可以理解 但 不 完全 正确 3 
有定/nr 描述 definite description 例如 贸易 制裁 已经 成为 了 
美国 政府 对华 的 惯用 大棒 这根   大棒   
真如 美国 政府 所 希望 的 那样 灵验 吗 6 
文本 分类 一个 文本 以下 基本 不 区分 文本 和 
文档 两个 词 的 含义 分类 问题 就是 将 一篇 
文档 归入 预先 定义 的 几个 类别 中 的 一个 
或 几个 而 文本 的 自动 分类 则是 使用 计算机 
程序 来 实现 这样 的 分类 7 问答 系统 问答 
系统 Question Answering System QA 是 信息检索 系统 的 一种 
高级 形式 它 能用 准确 简洁 的 自然 语言 回答 
用户 用 自然 语言 提出 的 问题 依据 问题 类型 
可分为 限定 域 和 开放 域 两种 依据 数据类型 可分为 
结构型 和无/nr 结构型 文本 依据 答案 类型 可分为 抽取式 和 
产生 式 两种 问句 分析 文档 检索 答案 抽取 验证 
2019年 上半年 收集到 的 人工智能 自然语言 处理 方向 干货 文章 
自然语言 NLP 发展史 及 相关 体系 读了 这篇 文字 做 
年薪 百万 的 NLP 工程师 聚焦 机器 读 写 说 
译 探寻 NLP 未来 之路 NLP 接下来 黄金 十年 周/nr 
明等/i 谈/v 值得/v 关注/v 的/uj NLP/w 技术/n 人工智能/n 科普/l ｜/i 
自然语言/l 处理/v NLP 为什么 要 学习 NLPAI 研究员 收集 NLP 
数据 的 四种 创意 方法 大牛 分享 自然语言 处理 中 
注意力 机制 综述 8个 方法 解决 90％ 的 NLP 问题 
周明 NLP 进步 将 如何 改变 搜索 体验 赋 能 
行业 发展 NLP 如何 避免 走入 死胡同 中文 的 NLP 
什么样 的 NLP 库 可以 支持 53种 语言 万字 长文 
概述 NLP 中的 深度 学习 技术 上 NLP 接下来 黄金 
十年 周/nr 明等/i 谈/v 值得/v 关注/v 的/uj NLP/w 技术/n 读了/i 
这篇/i 文字/n 做 年薪 百万 的 NLP 工程师 NLP 中的 
词 向量 及其 应用 为什么 NLP 相对来说 这么 困难 8种 
优秀 预 训练 模型 大盘点 NLP 应用 so easy 让 
机器 听懂 人话 的 自然语言 处理 技术 究竟 神奇 在哪里 
动态 记忆 网络 向 通用 NLP 更近 一步 精读 自然语言 
处理 基础 之 RNN 纯 干货 | 目前 看到 的 
BERT 比较 透彻 的 文章 强烈推荐 Bert 时代 的 创新 
Bert 应用 模式 比较 及 其它 为何 BERT 在 NLP 
中 的 表现 如此 抢眼 BERT 面向 语言 理解 的 
深度 双向 变换 预 训练 BERT 大火 却 不懂 Transformer 
读 这 一篇 就 够了 1亿 参数 4万 样本 BERT 
仍 听不懂 人话 我们 离 通用 NLP 能 还有 多远 
Flair 一款 简单 但 技术 先进 的 NLP 库 Tensorflow 
实现 的 深度 NLP 模型 集锦 附 资源 深度 学习 
自然语言 处理 五 NLTK 的 经典 应用 阿里 自然语言 处理 
部 总监 分享 NLP 技术 的 应用 及 思考 现有 
模型 还 「 不懂 」 自然语言 20多 位 研究 者 
谈 NLP 四大 开放 性问题 学界 | 和 清华大学 自然语言 
处理 与 社会 人文 计算 实验室 一起 读 机器翻译 论文 
2019 06 23 写 于 苏州市 自然语言 处理 是 如今 
计算机 科学 领域 比较 火热 的 一个 方向 其 也 
确实 有 很大 的 应用 场景 前面 说过 我 参加 
了 微软 编程之美 的 比赛 这个 比赛 其所 基于 的 
就是 自然语言 处理 我 本身 并 不是 学 自然语言 处理 
的 甚至连 这门 课 也 没有 选过 可 是 为了 
完成 资格赛 我 自己 在 网上 找 了 一些 资料 
时间 所限 也是 能力 所限 我 当然 不会 自己 去 
实现 一些 诸如 中文分词 与 词性 标注 的 自然 语言 
处理 算法 自然 的 我 想到 了 在 网上 找 
一些 开源 包 我 主要 考虑 的 是 三个 包 
首先 是 斯坦福 大学 的 一系列 自然语言 处理 工具 作为 
国际 知名 高校 自然 人们 都会 认为 斯坦福 的 技术 
会 更高 一点 可是 我 在 网上 并 没有 找到 
太多 的 使用 文档 而且 软件 也 比较 大 简单 
使用 也 并不 需要 太 高深 的 技术 然后 呢 
我 又 找到 了 哈工大 的 LTP 这个 是 我们 
自己 学校 的 东西 而且 也 广受/nr 认可 所以 天然 
的 我 倾向 于 使用 它 但是 我 发现 LTP 
不是 开源 的 如果 要 索要 代码 还需要 签署 一些 
协议 哎 最后 我 选择 使用 复旦 大学 的 自然 
语言 处理 开源 包 FNLP 它 的 优点 在于 获取 
方便 而且 是 比较 轻量级 的 简单 使用 比较 方便 
下面 给出 入门教程 FNLP 入门 1 . 下载 跟 编译 
GitHub 上 下载 FNLP 压缩包 下载 相应 的 模型 文件 
放在 第一 步 下载 的 文件 里 的 models 目录 
下载 Maven 并 按照 教程 配置 其实 就是 解压缩 和 
配置 环境变量 在 命令 行中 进入 FNLP 的 源码 目录 
即 README . md 所在 的 目录 执行 如下 命令 
进行 编译 mvn install Dmaven . test . skip = 
true 更新 2018 . 4.25 win10 系统 如果 进入 的 
是 powershell 则 使用 命令 mvn install Dmaven . test 
. skip = true 这会 编译 四个 Jar 包 fnlp 
core fnlp dev fnlp train fnlp app fnlp demo 它们 
分别 位于 源码 目录 中 各自 对应 目录 中的 target 
目录 之中 例如 fnlp core 的 软件包 位于 fnlp core 
/ target / fnlp core 2.0 SNAPSHOT . jar2 . 
命令行 使用 FNLP 的 源码 目 录下 的 命令行 可以 
Win + R 输入 cmd 然后 进入 源码 目录 也 
可以 在 源码 目 录下 按住 shift 右键 进入 命令行 
执行 maven 命令 mvn dependency copy dependencies DoutputDirectory = libs 
这样 jar 包 都会 copy 到 工程 目 录下 的 
libs 里面 输入 java Xmx1024m Dfile . encoding = UTF 
8 classpath . fnlp core / target / fnlp core 
2.1 SNAPSHOT . jar libs / trove4j 3 . 0.3 
. jar libs / commons cli 1.2 . jar org 
. fnlp . nlp . cn . tag . CWSTagger 
s models / seg . m 自然 语言 是 人类 
交流 和 思维 的 主要 工具 是 人类 智慧 的 
结晶 可以 使用 分词 功能 也 可以 测试 是否 安装 
成功 windows 系统 下 结果 如图 3 . 调用 FNLP 
库 eclipse 下 使用 在 eclipse 左侧 的 Project Explorer 
中 选择 项目 名称 右击 在 菜单 中 选择 Build 
Path Add External Archives 则会 弹出 文件 选择 对话框 依次 
查找 并 添加 下列 文件 第二步 获得 的 一些 jar 
包 可以 在 lib 等 文件 夹下 找到 fnlp core 
2.1 SNAPSHOT . jartrove4j 3 . 0.3 . jarcommons cli 
1.2 . jar 修改 虚拟机 最 大内 存量 网上 有 
教程 模型 文件 指 词典 训练 后的/nr 中文 分词器 POS 
标注 器 等 它们 位于 FNLP 源码 目 录下 的 
models 目录 之中 将此 目录 复制到 Eclipse 项目 目录 之下 
即可 接下来 就 可以 编程 调 用了 FNLP 提供 了 
一系列 中文 处理 工具 其中 中文分词 词性 标注 实体 名 
识别 等 基础 功能 已经 封装 在 工厂 类 CNFactory 
之中 CNFactory 位于 org . fnlp . nlp . cn 
包 之中 经过 初始化 后就/nr 可以 使用 其 提供 的 
全部 功能 import org . fnlp . nlp . cn 
. CNFactory CNFactory factory = CNFactory . getInstance models 以上 
代码 创建 了 一个 CNFactory 对象 并 载入 位于 models 
目 录下 的 模型 文件 接下来 就 可以 使用 CNFactory 
的 对象 来 进行 各种 中文 语言 处理 任务 中文分词 
public static void main String args throws Exception { / 
/ 创建 中文 处理 工厂 对象 并 使用 models 目 
录下 的 模型 文件 初始化 CNFactory factory = CNFactory . 
getInstance models / / 使用 分词器 对 中文 句子 进行 
分词 得到 分词 结果 String words = factory . seg 
关注 自然语言 处理 语音识别 深度 学习 等 方向 的 前沿 
技术 和 业界 动态 / / 打印 分词 结果 for 
String word words { System . out . print word 
+ } System . out . println } 结果 关注 
自然 语言 处理 语音 识别 深度 学习 等 方向 的 
前沿 技术 和 业界 动态 中文 词性 标注 public static 
void main String args throws Exception { / / 创建 
中文 处理 工厂 对象 并 使用 models 目 录下 的 
模型 文件 初始化 CNFactory factory = CNFactory . getInstance models 
/ / 使用 标注 器 对 中文 句子 进行 标注 
得到 标注 结果 String result = factory . tag2String 关注 
自然语言 处理 语音识别 深度 学习 等 方向 的 前沿 技术 
和 业界 动态 / / 显示 标注 结果 System . 
out . println result } 结果 关注 / 动词 自然 
/ 名词 语言 / 名词 处理 / 动词 / 标点 
语音 / 名词 识别 / 名词 / 标点 实体 名 
识别 public static void main String args throws Exception { 
/ / 创建 中文 处理 工厂 对象 并 使用 models 
目 录下 的 模型 文件 初始化 CNFactory factory = CNFactory 
. getInstance models / / 使用 标注 器 对 包含 
实体 名 的 句子 进行 标注 得到 结果 HashMap String 
String result = factory . ner 詹姆斯 默多克 和 丽贝卡 
布鲁克斯 鲁珀特 默多克 旗下 的 美国 小报 纽约 邮报 的 
职员 被 公司 律师 告知 保存 任何 也许 与 电话 
窃听 及 贿赂 有关 的 文件 / / 显示 标注 
结果 System . out . println result } 结果 { 
詹姆斯 默多克 = 人名 鲁珀特 默多克 旗 = 人名 丽贝卡 
布鲁克斯 = 人名 纽约 = 地名 美国 = 地名 } 
斯坦福 大学 在 三月份 开设 了 一门 深度 学习 与 
自然 语言 处理 的 课程 CS224d Deep Learning for Natural 
Language Processing 授课 老师 是 青年 才俊   Richard Socher 
以下 为 相关 的 课程 笔记 第 三讲 高级 的 
词 向量 表示 Advanced word vector representations language models softmax 
single layer networks 推荐 阅读 材料 Paper1 GloVe Global Vectors 
for Word Representation Paper2 Improving Word Representations via Global Context 
and Multiple Word Prototypes Notes Lecture Notes 2 第 三讲 
Slides slides 第 三讲 视频 video 以下 是 第三 讲 
的 相关 笔记 主要 参考 自 课程 的 slides 视频 
和 其他 相关 资料 回顾 简单 的 word2vec 模型 代价 
函数 J 其中 的 概率函数 定义 为 我们 主要 从 
内部 向量 internal vector $ v _ { w _ 
I } $ 导出 梯度 计算 所有 的 梯度 我们 
需要 遍历 每 一个 窗口 内 的 中心 向量 center 
vector 的 梯度 我们 同时 需要 每 一个 外部 向量 
external vectors $ v ^ $ 的 梯度 通常 的话 
在 每一个 窗口 内 我们 将 更新 计算 所有 用到 
的 参数 例如 在 一个 窗口 长度 为 1 的 
句子 里 I like learning 第一 个 窗口 里 计算 
的 梯度 包括 内部 向量 $ v _ { like 
} $ 外部 向量 $ v ^ _ I $ 
及 $ v ^ _ { learning } $ 同理 
更新 计算 句子 的 下 一个 窗口 里 的 参数 
计算所 有的 向量 梯度 我们 经常 在 一个 模型 里 
把 所有 的 参数 集合 都 定义 在 一个 长 
的 向量 $ \ theta $ 里 在 我们 的 
例子 里 是 一个 d 维度 的 向量 及 一个 
长度 为 V 的 词汇 集 梯度 下降 要 在整个 
训练 集上 最小化 代价 函数 $ J \ theta $ 
需要 计算所 有窗 口里 的 参数 梯度 对于 参数 向量 
$ \ theta $ 来说 需要 更新 其中 每 一个 
元素 这里 $ \ alpha $ 是 步长 从 矩阵 
的 角度 来看 参数 更新 梯度 下降 相关 代码 随机 
梯度 下降 SGD 对于 上述 梯度 下降 的 方法 训练 
集 语料库 有可能 有 400亿 40B 的 token 和 窗口 
一轮 迭代 更新 需要 等待 很长 的 时间 对于 非常多 
的 神经 网络 节点 来说 这 不是 一个 好 方法 
所以 这里 我们 使用 随机 梯度 下降 SGD 在 每 
一个 窗口 计算 完毕 后 更新 所有 的 参数 词 
向量 的 随机 梯度 下降 但是 在 每一个 窗口 里 
我们 仅有 2c 1个 词 这样的话 $ \ delta _ 
{ \ theta } J _ t \ theta $ 
非常 稀疏 我们 也许 仅仅 应该 只 更新 那些 确实 
存在 的 词 向量 解决方案 或者/c 保留/v 词/n 向量/n 的/uj 
哈稀/nr 或者/c 更/d 新词/n 嵌入/v 矩阵/n L/w 和$L/nr ^/i $ 
的 固定 列 很 重要 的 一点 是 如果 你 
有 上百万 个 词 向量 并且 在做 分布式 训练 的话 
就 不需要 发送 大量 的 更新 信息 了 PSet1 归一化 
因子 的 计算 代价 很大 因此在 PSet1 你们 将 实现 
skip gram 模型 主要 的 思路 对一对 实际 的 词 
对 一个 中心词 及 一个 窗口 内 的 其他 词 
和 一些 随机 的 词 对 一个 中心词 及 一个 
随机 词 训练 二 元逻辑 回归模型 PSet1 The skip gram 
model and negative sampling 来源 论文 Distributed Representations of Words 
and Phrases and their Compositionality Mikolov et al . 2013 
这里 k 是 我们 所 使用 的 负 例 采样 
negative sampling 的 样本 数 Sigmoid 函数 所以 我们 最大化 
第一个 log 处 两个 词 的 共 现 概率 更进一步 
比较 清晰 的 公式 最大化 在 中心词 周边 真 实词 
对 的 概率 最小化 中心词 周边 随机 词 对 的 
概率 这里 unigram 分布 U w 被 赋予 了 3/4 
幂 次方 这样 可以 保证 一些 出现 比 较少 的 
词 可以 被 尽可能 多 的 抽样 What to do 
with the two sets of vectors 我们/r 从/p 所有/b 的/uj 
向量/n v/w 和$v/nr ^/i $ 中 得到 了 L 和$L/nr 
^ $ 这 两个 都 获得 了 相似 的 共 
现 信息 如何 有效 的 利用 着 两个 向量 集 
一个 简单 有效 的 方法 对 它们 进行 加 和在 
GloVe 中 对 许多 超 参数 进行 了 探究   
Global Vectors for Word Representation   Pennington et al . 
2014 如何 评测 词 向量 和 一般 的 NLP 评测 
任务 相似 内部 vs 外部 Intrinsic vs extrinsic 内部 评测 
在 一个 特定 的 子 任务 中 进行 评测 计算 
迅速 有助于 理解 相关 的 系统 不 太 清楚 是否 
有助于 真实 任务 除非 和 实际 的 NLP 任务 的 
相关性 已经 建立 起来 外部 评测 在/p 一个/m 真实/d 任务/n 
中/f 进行/v 评测/vn 需要/v 花/v 很长/i 的/uj 实际/n 来/v 计算/v 
精度/n 不太/i 清楚/a 是否/v 是/v 这个/r 子系统/n 或者/c 其他/r 子系统/n 
引起/v 的/uj 问题/n 如果/c 用/p 这个/r 子系统/n 替换/v 原有/v 的/uj 
系统/n 后/f 获得/v 精度/n 提升/v 有效 Winning 词 向量 的 
内部 评测 词 向量 类比 语法/n 和/c 语义/n 通过/p 评测/vn 
模型/n 在/p 一些/m 语义/n 或/c 语法/n 类比/v 问题/n 上/f 的/uj 
余弦/nr 相似/v 度/zg 距离/n 的/uj 表现/v 来/v 评测/vn 词/n 向量/n 
去除/v 一些/m 来自/v 于/p 搜索/v 的/uj 输入/v 词/n 问题/n 如果 
信息 符合 但 不是 线性 的 怎么办 词 向量 的 
内部 评测 例 一 词 向量 类比 以下 语法 和 
语义 例子 来源于 https / / code . google . 
com / p / word2vec / source / browse / 
trunk / questions words . txt 存在 的 问题 不同 
的 城市 可能 存在 相同 的 名字 词 向量 的 
内部 评测 例 二 词 向量 类比 以下 语法 和 
语义 例子 来源于 https / / code . google . 
com / p / word2vec / source / browse / 
trunk / questions words . txt 词 向量 的 内部 
评测 例 三 词 向量 类比 以下 语法 和 语义 
例子 来源于 https / / code . google . com 
/ p / word2vec / source / browse / trunk 
/ questions words . txt 词 向量 的 内部 评测 
例 四 词 向量 类比 以下 语法 和 语义 例子 
来源于 https / / code . google . com / 
p / word2vec / source / browse / trunk / 
questions words . txt 类比 评测 和超/nr 参数 目前 为止 
最 细致 的 评测 GloVe 词 向量 非对称 上下文 仅有 
左侧 的 单词 并 不是 很好 最佳 的 向量 维度 
300 左右 之后 变化 比较 轻微 但是 对于 不同 的 
下游 任务 来说 最佳 的 维度 也会 不同 对于 GloVe 
向量 来说 最佳 的 窗口 长度 是 8 训练 的 
时间 约 长 是否 有 帮助 对于 GloVe 来说 确实 
有助于 更多 的 数据 是否 有 帮助 维基 百科 的 
数据 比 新闻 数据 更 相关 词 向量 的 内部 
评价 评测 任务 词 向量 距离 以及 和 人工 评价 
的 相关性 评测 集 WordSim353 http / / www . 
cs . technion . ac . il / ~ gabr 
/ resources / data / wordsim353 / 相关性 评测 结果 
如何 应对 歧义 问题 But what about ambiguity 也许/d 你/r 
寄/v 希望/v 于/p 一个/m 词/n 向/p 量能/i 捕获/v 所有/b 的/uj 
语义/n 信息/n 例如 run 即是 动车 也是 名词 但是/c 这样/r 
的话/u 词/n 向量/n 会被/i 辣/n 向/p 两个/m 方向/n 这篇/i 论文/nz 
对此/d 有/v 相应/v 的/uj 描述/v Improving Word Representations Via Global 
Context And Multiple Word Prototypes Huang et al . 2012 
解决 思路 对词 窗口 进行 聚 类 并对 每个 单词 
词 保留 聚 类 标签 例如 $ bank _ 1 
$ $ bank _ 2 $ 等 词 向量 的 
外部 评价 一个 例子 NER named entity recognition 好 的 
词 向量 会对 实际 任务 有 直接 的 帮助 命名 
实体 识别 NER 找到 人名 地名 和 机构 名 下一步 
如何 在 神经网络 模型 中 使用 词 向量 简单 的 
单个 词 的 分类 问题 从 深度 学习 的 词 
向量 中 最大 的 获益 是 什么 有/v 能力/n 对/p 
单词/n 进行/v 精确/a 的/uj 分类/n 国家/n 类/q 的/uj 单词/n 可以/c 
聚/v 和/c 到一起/i 因此 可以 通过 词 向量 将 地名 
类 的 单词 区分 出 来 可以 在 其他 的 
任务 中将 单词 的 任意 信息 融合 进来 可以 将 
情感 分析 Sentiment 问题 映 射到 单词 分类 中 在 
语料库 中 寻找 最 具 代表性 的 正 / 负 
例 单词 The Softmax 逻辑 回归 = Softmax 分类 在 
给定 词 向量 x 的 情况 下 获得 y 类 
的 概率 The Softmax 细节 术语 损失 函数 Loss function 
= 代价 函数 Cost function = 目标函数 Objective function Softmax 
的 损失 Loss 交叉 熵 Cross Entropy 如何 计算 p 
y | x 取 W 的 $ y ^ $ 
行 乘以 含 x 的 行 计算 所有 的 $ 
f _ c $ c = 1 2 . . 
. C 归一化 计算 Softmax 函数 的 概率 Softmax 和 
交叉 熵 误差 目标 是 最大化 正确 分类 y 的 
概率 因此 我们 可以 最小化 改 函数 负 的 对数 
概率 因此 如果 有 多个 类别 我们 可以 在 总的 
误差函数 中 叠加 多个 交叉 熵 误差 背景 交叉 熵 
& KL 散度 假设 分布 是 p = 0 . 
. . 0 1 0 . . . 0 对应 
计算 的 概率分布 是 q 则 交叉 熵 是 因为 
p 是 one hot 的 缘故 则 上述 公式 剩余 
的 则是 真实 类 的 负 的 对数 概率 交叉 
熵 可以 写成 熵 和 两个 分布 的 KL 散度 
之和 在 我们 的 case 里 p 是 0 即使 
不是 0 也会 因为 它 是 固定 的 对 梯度 
没有 固定 最小化 交叉 熵 等价 于 最小化 KL 散度 
KL 散度 并非 是 一个 距离 函数 而是 一个 对于 
两个 概率分布 差异 的 非 对称 的 度量 维基百科 KL 
散度 是 两个 概率分布 P 和Q/nr 差别 的 非 对称性 
的 度量 KL 散度 是 用来 度量 使用 基于 Q 
的 编码 来 编码 来自 P 的 样本 平均 所需 
的 额外 的 位元 数 典型 情况 下 P 表示 
数据 的 真实 分布 Q 表示 数据 的 理论 分布 
模型 分布 或 P 的 近似 分布 简单 的 单个 
单词 分类 例子 情感 分析 两个 选择 仅仅 训练 softmax 
权重 W 或者 同时 训练 词 向量 问题 训练 词 
向量 的 优点 和 缺点 是 什么 Pro 更好 的 
适应 训练 数据 Con 更差 的 泛化 能力 训练 的 
词 向量 的 情感 分享 可视化 继续 打怪 升级 窗口 
分类 Window classification 单个 的 单词 没有 上下文 信息 通过 
对 窗口 中的 中心词 进行 分类 同时 考虑 窗口 中的 
上下文 可能性 Softmax 和 交叉 熵 误差 或者 最大 边界 
损失 max margin loss 我们 将 在下 一 讲 中 
探索 这些 问题 next class 注 原创 文章 转载 请 
注明 出处 及 保留 链接 我 爱 自然 语言 处理 
http / / www . 52nlp . cn 本文 链接 
地址 斯坦福大学 深度 学习 与 自然 语言 处理 第 三讲 
高级 的 词 向量 表示 相关 文章 斯坦福大学 深度 学习 
与 自然 语言 处理 第二 讲 词 向量 斯坦福大学 深度 
学习 与 自然 语言 处理 第 四讲 词 窗口 分类 
和 神经 网络 斯坦福大学 深度 学习 与 自然 语言 处理 
第一 讲 引言 维基百科 语料 中的 词语 相似 度 探索 
自然语言 处理 工具包 spaCy 介绍 中 英文 维基百科 语料 上 
的 Word2Vec 实验 Coursera 公开课 笔记 斯坦福大学 机器学习 第七课 正则化 
Regularization PRML 读书会 第五章 Neural NetworksAndrew Ng 深度 学习 课程 
小记 Coursera 公开课 笔记 斯坦福大学 机器学习 第二课 单 变量 线性 
回归 Linear regression with one variable 本 条目 发布 于 
2015年 07月 15号 属于 机器学习 深度 学习 自然语言 处理 分类 
被 贴了   Deep Learning Deep Learning 公开课 Deep NLP 
DL glove KL 散度 KL 距离 NER Richard Socher SGD 
softmax word vectors word2vec wordnet 交叉 熵 公开课 共 现 
矩阵 单词 分类 命名 实体 识别 情感 分析 斯坦福大学 机器学习 
梯度 下降 深度 学习 深度 学习 与 自然 语言 处理 
深度 学习 技术 深度 学习 模型 神经网络 自然 语义 处理 
自然语言 处理 计算机 视觉 词 向量 词 向量 评测 词 
嵌入 语义 词典 随机 梯度 下降   标签 作者 是 
52nlp 基于 深度 学习 的 自然 语言 处理 作者 约 
阿夫 戈尔德贝格 Yoav Goldberg 出版社 机械 工业 出版社 ISBN 9787111593737 
出版 时间 2018 05 01 博主 github https / / 
github . com / MichaelBeechan 博主 CSDN https / / 
blog . csdn . net / u011344545 = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= 概念 篇 https / / blog . csdn . 
net / u011344545 / article / details / 89525801 技术篇 
https / / blog . csdn . net / u011344545 
/ article / details / 89526149 人才篇 https / / 
blog . csdn . net / u011344545 / article / 
details / 89556941 应用 篇 https / / blog . 
csdn . net / u011344545 / article / details / 
89574915 下载 链接 https / / download . csdn . 
net / download / u011344545 / 11147085 = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= 清华 AMiner 团队 AMiner . org 国外 The Language 
Technologies Institute LTI at Carnegie Mellon University 卡内基 梅隆 大学 
语言 技术 研究 所 主要 研究 内容 包括 自然 语言 
处理 计算 语言学 信息提取 信息检索 文本 挖掘 分析 知识 表示 
机器学习 机器翻译 多通道 计算 和 交互 语音 处理 语音 界面 
和 对话 处理 等 The Stanford Natural Language Processing Group 
斯坦福 大学 自然 语言 处理 小组 包括了 语言学 和 计算机 
科学系 的 成员 是 斯坦福 人工智能 实验室 的 一部分 主要 
研究 计算机 处理 和 理解 人类 语言 的 算法 工作/vn 
范围/n 从/p 计算/v 语言学/n 的/uj 基本/n 研究/vn 到/v 语言/n 处理/v 
的/uj 关键/n 应用/v 技术/n 均/d 有/v 涉猎/v 涵盖 句子 理解 
自动 问答 机器翻译 语法 解析 和 标签 情绪/n 分析/vn 和/c 
模型/n 的/uj 文本/n 和/c 视觉/n 场景/n 等/u 该/r 小组/n 的/uj 
一个/m 显著/a 特征/n 是/v 将/d 复杂/a 和/c 深入/v 的/uj 语言/n 
建模/n 和/c 数据/n 分析/vn 与/p NLP 的 创新 概率 机器 
学习 和 深度 学习 方法 有效 地 结合 在 一起 
The Berkeley NLP Group 伯克利大学 自然语言 处理 小组 分 属于 
加州 大学 伯克利 分校 计算机 科学 部 主要 从事 以下 
几 方面 的 研究 工作 语言 分析 机器翻译 计算机 语言学 
基于 语义 的 方法 无 监督 学习 等 多次 在 
顶级 国际 会议 ACL EMNLP AAAI IJCAI COLING 等 上 
发表 多 篇 论文 Natural Language Processing Group at University 
of Notre Dame 圣母 大学 自然 语言 处理 小组 主要 
关注 机器翻译 领域 并有 多个 项目 的 研究 如 由 
DARPALORELEI 和 Google 赞助 的 无 监督 多 语言 学习 
模型 和 算法 研究 由 亚马逊 学术 研究 奖 和 
谷歌 教师 研究 奖 赞助 的 研究 主要 研究 课题 
方向 包括 基于 神经 网络 的 机器 翻译 模型 以及 
使用 神经 网络 进行 翻译 和 语言 建模 的 算法 
等 多次 在 国际 顶级 期刊 和 会议 上 发表 
论文 The Harvard Natural Language Processing Group 哈佛 自然语言 处理 
小组 主要 通过 机器学习 的 方法 处理 人类 语言 主要 
兴趣 集中 在 数列 生成 的 数学 模型 以 人类 
语言 为 基础 的 人工智能 挑战 以及 用 统计 工具 
对 语言 结构 进行 探索 等 方面 该 小组 的 
研究 出版物 和 开源 项目 集中 在 文本 总结 神经 
机器翻译 反复 神经 网络 的 可视化 收缩 神经 网络 的 
算法 文档 中 实体 跟踪 的 模型 多 模态 文本 
生成 语法错误 修 正和 文本 生成 的 新方法 等 方面 
Natural Language Processing group of Columbia University 哥伦比亚 大学 自然 
语言 处理 研究室 是 在 计算机 科学系 计算 学习 系统 
中心 和 生物 医学 信息系 的 支持 下 进行 的 
将 语言 洞察力 与 严谨 前沿 的 机器 学习 方法 
和 其他 计算方法 结合 起来 进行 研究 在 语言 资源 
创造 如 语料库 词典 等 阿拉伯语 NLP 语言 和 社交 
网络 机器翻译 信息提取 数据挖掘 词汇 语义 词义 消除歧义 等 方面 
有着 比较 深入 的 研究 国内 中科院计算所 自然语言 处理 研究组 
自然语言 处理 研究组 隶属于 中国科学院计算技术研究所 智能 信息 处理 重点 实验室 
研究组 教师 有 刘群 冯洋/nr 等人 研究组 主要 从事 自然语言 
处理 和 机器 翻译 相关 的 研究 工作 研究 方向 
包括 机器翻译 人机对话 多语 言词 法分析 句法分析 和 网络 信息 
挖掘 等 研究组 已 完成 和 正在 承担 的 国家 
自然科学 基金 863 计划 科技 支撑 计划 国际 合作 等 
课题 40 余项 在 自然 语言 处理 和 机器 翻译 
领域 取得 了 多项 创新 性 研究 成果 研究组 自 
2004 年 重点 开展 统计 机器翻译 方面 的 研究 并 
取得 重大 突破 并于 2015 年起/nr 转向 神经 机器翻译 并 
取得 很大 进展 2018 年 7 月 正式 加入 华为 
诺亚方舟 实验室 任 语音 语义 首席 科学家 主导/b 语音/n 和/c 
自然/d 语言/n 处理/v 领域/n 的/uj 前沿/s 研究/vn 和/c 技术/n 创新/v 
在 自然 语言 处理 的 顶级 国际 刊物 CL AI 
和 顶级 国际 学术 会议 ACL IJCAI AAAI EMNLP COLING 
上 发表 高水平 论文 70 余篇 取得 发明专利 10 余项 
研究组 已经 成功 将 自主 开发 的 统计 机器 翻译 
和 神经 机器 翻译 技术 推广 到 汉语 维吾尔语 藏语 
蒙古语 英语 韩语 泰语 日语 阿拉伯语 等 多种 语言 部分 
语种 的 翻译 系统 已经 在 相关 领域 得到 了 
实际 应用 获得 用户 的 好评 哈工大 社会 计算 与 
信息检索 研究中心 哈工大 社会 计算 与 信息检索 研究 中心 HIT 
SCIR 成立 于 2000 年 9 月 隶属 于 计算机 
科学 与 技术 学院 研究/vn 中心/n 成员/n 有/v 主任/b 刘挺/nr 
教授/n 副 主任 秦兵 教授 教师 包括 张宇 车 万翔 
陈毅 恒 张伟 男 等 研究 方向 包括 语言 分析 
信息 抽取 情感 分析 问答 系统 社会 媒体 处理 和 
用户 画像 6 个 方面 已完成 或 正在 承担 的 
国家 973 课题 国家 自然科学 基金 重点 项目 国家 863 
重点 项目 国际合作 企业 合作 等 课题 60 余项 在 
这些 项目 的 支持 下 打造出 语言 技术 平台 LTP 
提供给 百度 腾讯 华为 金山 等 企业 使用 获 2010 
年 钱伟长 中文信息处理 科学技术 一等奖 研究 中心 近年 来 发表 
论文 100 余篇 其中 在 ACL SIGIR IJCAI EMNLP 等 
顶级 国际 学术 会议 上 发表 20 余篇 论文 参加 
国内外 技术 评测 并 在 国际 CoNLL 2009 七国 语言 
句法 语义分析 评测 总成绩 第一名 研究 中心 通过 与 企业 
合作 已将 多项 技术 嵌入 企业 产品 中 为 社会 
服务 双语 例句 检索 等 一批 技术 嵌入 金山词霸 产品 
中 并 因此 获得 2012 年 黑龙江省 技术 发明 二等奖 
微软 亚洲 研究院 自然语言 计算 组 复旦 大学 自然 语言 
处理 研究组 复旦 大学 自然 语言 与 信息检索 实验室 致力 
于 社会 媒体 海量 多媒体 信息 处理 的 前沿 技术 
研究 主要 研究 方向 包括 自然语言 处理 非 规范化 文本 
分析 语义 计算 信息 抽取 倾向性 分析 文本 挖掘 等 
方面 实验室 开发 了 NLP 工具包 FudanNLP FudanNLP 提供 了 
一系列 新 技术 包括 中文分词 词性 标注 依赖 解析 时间 
表达式 识别 和 规范化 等 实验室 先后 承担 和 参与 
了 国家 科技 重大 专项 国家 973 计划 863 计划 
国家自然科学基金 课题 上海市 科技 攻关 计划 等 并与 国内外 多 
所 重点 大学 公司 保持 着 良好 的 合作 关系 
研究 成果 持续 发表 在 国际 权威 期刊 和 一流 
国际 会议 TPAMI TKDE ICML ACL AAAI IJCAI SIGIR CIKM 
EMNLP COLING 等 清华大学 自然语言 处理 与 社会 人文 计算 
实验室 清华大学 计算机系 自然语言 处理 课题组 在 20 世纪 70 
年代 末 就在 黄 昌宁 教授 的 带领 下 从事 
这 方面 的 研究 工作 是 国内 开展 相关 研究 
最早 深 具 影响力 的 科研 单位 同时 也 是 
中国 中文信息 学会 计算 语言学 专业 委员会 的 挂靠 单位 
现任/n 学科/n 带头人/n 孙茂松/nr 教授/n 任该/nr 专业/n 委员会/n 的/uj 主任/b 
同时 任 中国 中文信息 学会 副 理事长 其余 教师 还有 
刘洋 刘知远 等人 目前 该 课题组 对 以 中文 为 
核心 的 自然 语言 处理 中 的 若干 前沿 课题 
进行 系统 深入 的 研究 研究/vn 领域/n 的/uj 涵盖/v 面/n 
正/d 逐步/d 从/p 计算/v 语言学/n 的/uj 核心/n 问题/n 扩展/v 到/v 
社会/n 计算/v 和/c 人文/n 计算/v 该 课题组 多篇 论文 被 
ACL 2018 IJCAI ECAI 2018 WWW 2018 录用 内容 涉及 
问答 系统 信息检索 机器翻译 诗歌 生成 查询 推荐 等 多个 
领域 清华大学 智能 技术 与 系统 国家 重点 实验室 智能 
技术 与 系统 国家 重点 实验室 依托 在 清华大学 1987 
年 7 月 开始 筹建 1990 年 2 月 通过 
国家 验收 并 正式 对外开放 运行 从 1990 年至/nr 2003 
年这/nr 十三 年间 实验室 顺利 通过 国家 自然科学 基金委 受 
科技部 委托 组织 的 全部 三次 专家组 评估 并被 评估 
为 A 优秀 实验室 1994 年 10 月 在 庆祝 
国家 重点 实验室 建设 十 周年 表彰 大会 上 智能 
技术 与 系统 国家 重点 实验室 获 集体 金牛 奖 
1997 年被/nr 科技部 列为 试点 实验室 2004 年 庆祝 国家 
重点 实验室 建设 二十周年 表彰 大会 上 本 实验室 再次 
荣获 集体 金牛 奖 从 2004 年 开始 实验室 参与 
筹建 清华 信息 科学 与 技术 国家 实验室 实验室 学术 
委员会 由 17 名 国内外 著名 专家 组成 实验室 学术 
委员会 名誉 主任 为 中科院 院士 张钹 教授 主任 为 
应明 生 教授 副/b 主任/b 为/p 邓志东/nr 教授/n 北京大学 语言 
计算 与 互联网 挖掘 研究组 语言 计算 与 互联网 挖掘 
研究室 从属 于 北京大学 计算机 科学 技术 研究所 成立 于 
2008 年7/nr 月 负责人 为 万 小军 老师 研究室 以 
自然 语言 处理 技术 数据挖掘 技术 与 机器 学习 技术 
为基础 对 互联 网上 多源 异质 的 文本 大 数据 
进行 智能 分析 与 深度 挖掘 为 互联网 搜索 舆情 
与 情报 分析 写稿 与 对话 机器人 等 系统 提供 
关键 技术 支撑 并 从事 计算机 科学 与 人文 社会 
科学 的 交叉 科学研究 研究室 当前 研究 内容 包括 1 
语义 理解 研制 全新 的 语义分析 系统 实现 对 人类 
语言 尤其 是 汉语 的 深层 语义 理解 2 机器 
写作 综合利用 自动 文摘 与 自然 语言 生成 等 技术 
让 机器 写出 高质量 的 各类 稿件 3 情感 计算 
针对 多语言 互联网 文本 实现 高 精度 情感 立场 与 
幽默 分析 4 其他 包括 特定 情境 下 的 人机对话 
技术 等 北京大学 计算 语言学 教育部 重点 实验室 计算 语言学 
教育部 重点 实验室 依托 北京大学 建设 实验室 研究 人员 由 
北京大学 信息 科学技术 学院 计算 语言学 研究所 中文系 软件 与 
微电子 学院 语言 信息 工程 系 计算机 技术 研究所 心理系 
和 外语 学院 的 相关 研究 人员 构成 主要 研究 
方向 包括 中文 计算 的 基础 理论 与 模型 大 
规模 多层次 语言 知识库 构建 的 方法 国家 语言 资源 
整理 与 语音 数据库 建设 海量 文本 内容 分析 与 
动态 监控 多 语言 信息 处理 和 机器 翻译 中科院/nt 
模式识别/n 国家/n 重点/n 实验室/n 中科院/nt 模式识别/n 国家/n 重点/n 实验室/n 自然语言/l 
处理/v 组/zg 主要/b 成员/n 有/v 宗/nr 成庆/i 赵军 周玉 刘康 
张 家俊 汪昆/nr 陆征等/nr 该 小组 主要 从事 自然语言 处理 
基础 机器翻译 信息 抽取 和 问答 系统 等 相关 研究工作 
力图 在 自然 语言 处理 的 理论 模型 和 应用 
系统 开发 方面 做出 创新 成果 目前 研究组 的 主要 
方向 包括 自然语言 处理 基础 技术 汉语 词语切分 句法分析 语义分析 
和 篇章 分析 等 多语言 机器翻译 信息 抽取 实体 识别 
实体 关系 抽取 观点 挖掘 等 和 智能 问答 系统 
基于 知识库 的 问答 系统 知识 推理 社区 问答 等 
近年来 研究组/n 注重/v 于/p 自然/d 语言/n 处理/v 基础理论/n 和/c 应用/v 
基础/n 的/uj 相关/v 研究/vn 承担 了 一系列 包括 国家 自然科学 
基金 项目 973 计划 课题 863 计划/n 项目/n 和/c 支撑/v 
计划/n 项目/n 等/u 在内/u 的/uj 基础/n 研究/vn 和/c 应用/v 基础/n 
研究/vn 类/q 项目/n 以及 一批 企业 应用 合作项目 在 自然 
语言 处理 及 相关 领域 顶级 国际 期刊 CL TASLP 
TKDE JMLR TACL Information Sciences Intelligent Systems 等 和 学术 
会议 AAAI IJCAI ACL SIGIR WWW 等 上 发表 了 
一系列 论文 2009 年 获得 第 23 届 亚太 语言 
信息 与 计算 国际 会议 PACLIC 最佳 论文 奖 2012 
年 获得 第一 届 自然语言 处理 与 中文 计算 会议 
NLPCC 最佳 论文 奖 2014 年 获得 第 25 届 
国际 计算 语言学 大会 COLING 最佳 论文 奖 获得 了 
10 余项 国家 发明 专利 哈工大 机器 智能 与 翻译 
研究室 哈尔滨 工业 大学 计算机 学院 机器 智能 与 翻译 
研究室 Machine Intelligence & Translation Laboratory MI & T Lab 
自 1985年 以来 一直 致力于 机器翻译 研究 与 系统 开发 
本 研究室 最初 完成 的 汉英 机器翻译 系统 CEMT I 
于 1989年 5月 鉴定 成为 我国 第一 个 通过 技术 
鉴定 的 汉英 机器翻译 系统 获 部级 科技 进步 二等奖 
2000年 6月 哈工大 微软 机器 翻译 技术 联合 实验室 的 
建立 为本 研究室 进一步 发展 创造 了 机会 2004年 6月 
基于 哈工大 微软 机器 翻译 技术 联合 实验室 几年来 所 
作出 的 成绩 联合 实验室 扩大 为 哈工大 微软 自然语言 
处理 及 语音 技术 联合 实验室 2004年 11月 联合 实验室 
进一步 提升 为 教育部 微软 语言 语音 重点 实验室 目前 
机器 智能 与 翻译 研究室 有 教师 6名 博士 15名 
硕士 30 余名 及 本科生 等 学术/n 带头人/n 为/p 博士生/nr 
导师/n 李/nr 生/vn 教授/n 和/c 赵铁军/nr 教授/n 本 研究室 与 
微软 亚洲 研究院 富士通 研究 中心 东芝 研究 中心 香港理工大学 
美国 马里兰 大学 瑞典 林雪平 大学 英国 阿伯丁 大学 新加坡 
通讯 与 信息 研究所 以及 北大 清华 中科院 北外 复旦 
等 学校 及 研究 机构 均 建立 了 较 密切 
的 合作 与 交流 关系 并 有 机会 被 交换 
到 上述 大学 或 研究 机构 学习 或 进修 哈尔滨工业大学 
智能 技术 与 自然 语言 处理 实验室 ITNLP 哈工大 智能 
技术 与 自然 语言 处理 研究室 Intelligent Technology & Natural 
Language Processing Lab ITNLP Lab 是 国内 较早 从事 自然语言 
处理 研究 的 科研 团体 之一 自 八十 年代 初期 
以来 先后 开展 了 俄 汉 机器翻译 固定 段落 问答 
自动 文摘 文本 纠错 汉字 智能 输入 语音 识别 与 
合成 语料库 多级 加工 语言 模型 信息检索 问答 系统 等 
多项 研究 研究室 的 代表性 成果 是 开创性 地 提出 
了 汉字 语句 输入 的 思想 并 实现 了 国内外 
第一个 语句 级 汉字键盘 输入系统 目前 共 获得 部 科技 
进步 级 一等奖 1项 二等奖 4项 获得 国家 专利 3项 
先后 在 IEEE Transaction on SMC 中国 科学 等 国内外 
重要 学术 刊物 和 会议 上 发表 论文 200 余篇 
编 著书 8部 1990 年以来 完成 的 国家 自然科学 基金 
重点 / 面上 项目 国家 863 重点 / 面上 项目 
中美 中日 国际 合作 等 重要 科研 项目 20 多项 
研究室 目前 的 主要 研究 方向 包括 网络 信息 处理 
自然语言 处理 智能 人机接口 计算 分子生物学 等 近期 实验室 牵头 
或 独立 承担 的 主要 科研 项目 包括 国家 自然科学 
基金 重点 项目 问答式 信息检索 的 理论 与 方法 研究 
国家 863 计划目标 导向 类 课题 基于 NLP 的 智能 
搜索 引擎 以及 多项 国家 自然科学 基金 面上 项目 和 
863 计划 探索 类 项目 哈工大 语言 语音 教育部 微软 
重点 实验室 哈工大 语言 语音 教育部 微软 重点 实验室 以 
哈工大 计算机 学院 语言 技术 研究 中心 为 主要 依托 
由 机器 智能 与 翻译 实验室 智能 技术 与 自然 
语言 处理 实验室 信息检索 实验室 和 语音 处理 实验室 联合 
组成 由 教育部 和 微软 亚洲 研究院 联合 支持 并 
资助 实验室/n 主任/b 是/v 李/nr 生/vn 教授/n 哈工大 和 马维英 
主任 研究员 微软 研究院 研究 内容 涵盖 自然语言 理解 网络 
信息 处理 机器翻译 信息检索 问答 系统 智能 汉字输入 文景 转换 
生物 信息 识别 语音 识别 与 合成 技术 等 多个 
研究领域 实验室 的 研究 目标 是 在 3 5 年内 
使得 以上 各 领域 研究 达到 国内 领先 和 国际 
先进 水平 到 目前 为止 实验室 已完成 或 正在 实施 
的 重要 科研 项目 已达 100 多项 包括 国家 自然科学 
基金 重点 项目 863 计划 重点 项目 省部级 攻关项目 国际 
合作 项目 等 近 3年 发表 论文 400 多篇 目前 
实验室 研究 人员 包括 博士生 导师 7人 教授 8人 具有 
博士 学位 的 副教授 10人 博士 研究生 50 余人 硕士 
研究生 70 余人 实验室 还 聘请 了 多位 国际 知名 
专家 担任 兼职 博士生 导师 和 兼职 教授 东北 大学 
自然 语言 处理 实验室 南京 大学 自然 语言 处理 研究组 
南京 大学 自然 语言 处理 研究组 从事 自然语言 处理 领域 
的 研究工作 始于 20 世纪 80 年代 曾 先后 承担 
过 该 领域 的 18项 国家 科技 攻关 项目 863 
项目 国家 自然科学 基金 和 江苏省 自然科学 基金 以及 多项 
对外 合作 项目 的 研制 其中 承担 的 国家 七五 
科技 攻关 项目 日汉 机译 系统研究 获 七五 国家 科技 
攻关 重大 成果奖 教委 科技 进步 二等奖 以及 江苏省 科技 
进步 三等奖 分析 理解 人类 语言 是 人工智能 的 重要 
问题 之一 近年来 本 研究组 在 自然 语言 处理 的 
多个 方向 上 做 了 大量 深入 的 工作 近年来 
集中 关注 文本 分析 机器翻译 社交 媒体 分析 推荐 知识 
问答 等 多个 热点 问题 结合/v 统计/v 方法/n 和/c 深度/ns 
学习/v 方法/n 进行/v 问题/n 建模/n 和/c 求解/v 取得 了 丰富 
的 成果 近期 本 研究组 在 自然 语言 处理 顶级 
国际 会议 ACL 上 连续 三年 发表 共 5篇 论文 
也在 人工智能 顶级 国际 会议 IJCAI 和 AAAI 上 发表 
论文 多篇 相关 系统 在 机器 翻译 中文分词 命名 实体 
识别 情感 计算 等 多个 国际国内 评测 中 名列前茅 厦门大学 
智能科学 与 技术系 自然语言 处理 实验室 自然语言 处理 实验室 是 
厦门 大学 信息 学院 一支 具有 30 多年 研究 历史 
的 科研 创新 团队 团队 面向 信息 社会 中 不同 
语言 间 的 国内外 交流 的 重大 需求 以 实现 
高度 智能化 的 语言 处理 为 目标 开展 分词 命名 
实体 识别 句法分析 多语 词语 对齐 语言 模型 隐喻 理解 
等 基础 研究 规则/n 和/c 统计/v 相/v 结合/v 的/uj 机器/n 
翻译/v 辅助 翻译 嵌入式 翻译 新型 语 篇 语义 翻译 
模型 云 翻译 平台 神经 机器 翻译 等 机器 翻译 
研究 知识图谱 信息 抽取 关系 和 事件 抽取 跨语言 信息检索 
舆情 分析 等 网络 信息 处理 应用 基础 研究 同时 
和 外文系 中文系 两岸 关系 和平 发展 协 创 中心 
等 单位 有 密切 的 交叉 学科 合作 关系 目前 
团队 的 三个 主要 研究 方向 是 多语种 机器翻译 隐喻 
计算 信息 抽取 和 检索 实验室 现有 全职 研究 人员 
9人 含 教授 1人 副教授 4人 助理 教授 4人 郑州 
大学 自然 语言 处理 实验室 自然语言 处理 是 计算机 应用 
研究 领域 的 热点 之一 生活 在 信息 网络 时代 
的 现代人 几乎 都 要与 互联网 打交道 都要 或多或少 地 
使用 自然 语言 处理 的 研究 成果 来 帮助 他们 
获取 或 挖掘 在 广阔 无边 的 互联 网上 的 
各种 知识 和 信息 在 时代 潮流 发展 趋势 下 
郑州 大学 于 2004年 10月 成立 了 自然 语言 处理 
实验室 实验室 隶属 于 郑州 大学 信息 工程学院 实验室 现有 
教授 2人 副教授 2人 讲师 3人 在读 硕士 近 20人 
在 昝 红英 教授 的 带领 下 实验室 从 成立 
至今 在 现代 汉语 广义 虚 词库 建设 文本 自动 
分类 中文 文本 的 褒贬 评价 中英文 双语 术语 抽取 
等 方向 进行 了 深入研究 拥有 扎实 的 工作 基础 
并在 该 领域 积累 了 丰富 的 阶段性 成果 实验室 
积极 营造 浓厚 科研 氛围 在与 各种 团体 的 交流 
学习 中 不断 发展 我们 曾与/nr 北京大学 香港城市大学 香港 慧 
科 讯 业 有限公司 日本 富士通 研究 开发 中心 有限公司 
等 多家 研究 机构 进行 项目 合作 与 交流 工作 
并 在 进行 基础 研究 的 同时 努力 将 科研 
成果 转化 为 生产力 与 国内外 企业 进行 横向 应用 
项目 的 合作 苏州 大学 自然 语言 处理 实验室 苏州大学 
人类 语言 技术 研究所 笔者 是 一名 刚刚 打开 自然语言 
处理 潘多拉 魔盒 的 探路 青年 在此 写下 一些 学习 
笔记 作为 总结 自然语言 处理 语音识别 计算机 视觉 是 当下 
人工智能 领域 最为 火热 的 三个 领域 自然语言 处理 为的是 
让 计算机 理解 和 处理 人类 的 语言 图灵测试 中 
对 机器 智能 的 判断 标准 就是 语言 语言 是 
人类 智慧 的 最高 体现 我 喜欢 探索 人类 在 
对话 中 知识 的 传递 和 积累 如何 产生 令人 
激动 地 aha 时刻 引用 一位 本 领域 的 学者 
我 的 研究 计划 侧重于 更好 地 理解 对话 的 
社会 和 实用 性质 并 利用 这种 理解 建立 可 
提高 人 与人 之间 以及 人 与 计算机 之间 对话 
效率 的 计算 系统 为了 实现 这些 目标 我/r 从/p 
计算/v 语/ng 篇/q 分析/vn 和/c 文本/n 挖掘/v 会话 代理 和 
计算机 支持 的 协作 学习 中 调用 方法 利用 自动 
监控 自然 语言 沟通 的 技术 最后 阶段 是 构建 
能够 带来 真实世界 效益 的 干预 措施 Carolyn Penstein Rose 
学习 基础 这个 学科 入门 一 方面 需要 语言学 的 
知识 另一方面 是 计算机 建模 的 能力 年初 开始 接触 
自然语言 处理 导师 给我发 了 数篇 文章 和 数学 资料 
要 我 仔细 看看 把 英语 编程 数学 研究 方法 
的 知识 补 起来 这 是 一个 庞大 的 工程 
我 认为 学习 的 本质 是 将 知识 在 脑海 
中 形成 一张 纵横交错 的 大网 图 结构 并 不能 
有效 地 存储 人类 知识 对 计算机 而言 知识图谱 或许 
更好 树形 结构 是 人 所 倾向 的 形成 知识 
结构 包括 学习 的 最好 方法 就是 输出 输出 包括 
做题 解决问题 讲授 和 表达 写作 只要 是 将 知识 
从 大脑 中 得以 显化 都是 好 的 学习 方式 
因为 这样 会 加深 大脑 中 神经元 的 连接 数学 
基础 在 面对 海量 的 数据 时 如何 让 计算机 
有效 地 提取 主题 理解 用户 的 兴趣 变化 情绪 
变化 就得 用到 机器 学习 的 方法 使用 无 监督 
学习 或者 半 监督 学习 对照 特征 库 挖掘 用户 
的 特征 并对 其 进行 建模 机器学习 是 指 可以 
自主 进化 不断 增强 的 计算机 算法 算法 是 自动化 
解决 问题 的 步骤 设计 人类 脑 袋中 140亿 个 
神经元 的 树突 和 轴突 连接 起来 的 大脑 正 
是 一种 启发 极强 的 计算 系统 神经元 的 运作 
本质 上 就是 计算 未来 计算 机会 成为 人类 的 
左膀右臂 这些 都要/nr 建立 在 数学 建模 的 基础 上 
其中 大部分 得 用到 概率论 和 数理统计 方法 诸如 参数估计 
gama beta 函数 马尔科夫 链 贝叶斯 回归分析 等 通过 大量 
的 数据 集 训练 数学模型 使 它 的 智能 达到 
理想 的 程度 学好 概率论 的 关键 则是 打好 高等 
数学 的 基础 一元 微积分 多元 微积分 无穷 级 数等 
否则 一头雾水 参考书目 学习 网站 我 爱 自然 语言 处理 
www . 52nlp . com 数学 基础 在 考研 时 
我 看 的 是 同济 版 高等数学 浙大 版 概率 
数理统计 同济 版 线性代数 配合/v 文都的/nr 参考书/n 和/c 课程/n 总 
觉得 他们 偏向 于 做题 有些 抽象 自学 起来 非常 
费力 可能 笔者 理解 能力 有限 汤家凤/nr 老师/n 也/d 提到/v 
过/ug 这个/r 问题/n 如果 是 计算机 领域 的 数学 笔者 
认为 最好 能 使用 MATLAB 进行 模拟 结合 实际 中 
的 案例 数学 做题 和 应用 是 非常 重要 的 
如 王阳明 所讲 的 知行合一 或 晚清 湖湘文化 提倡 的 
经世致用 知识 要 与 实践 相 结合 才能 体现 出 
他 的 价值 以下 书籍 以 国外 教材 为主 建议 
购买 国内 影印 的 原版 还原 度 更高 和/c 以后/f 
读/v 英文/nz 文献/n 有/v 帮助/v 避免 了 许多 错误 概率论 
与 数理统计 陈希 儒 此书 我 通过 知乎 lda 数学 
八卦 了解到 语言 通俗易懂 配合 中科大 的 国家 精品 课程 
学习 效果 更佳 做做 后面 的 习题 有助于 消化 理解 
灵活运用 概率 导论 Dimitri P bertsekas 从 直观 自然 的 
角度 阐述 概率 是 理工科 学生 入门 的 不二 选择 
本书 编排 和 国内 的 书籍 不同 先从 样本空间 入手 
穿插 贯 序 模型 配图 丰富 讲解 生动 比 国内 
一 出来 就 给 个 空间 和 公式 轻松 许多 
托马斯 大学 微积分 joel hass 这 本书 是 托马斯 微积分 
的 大学 版 配图 丰富 讲解 生动 比 国内 苏联 
文风 的 写作 方式 亲切 不少 单从 配图 都能 感觉到 
此书 的 excited 线性代数 及其 应用 或 线性代数 引理 本书 
结合 应用数学 软件 强调/v 了/ul 计算机/n 对/p 科学/n 和T程/nr 学/n 
中/f 线性代数/l 的/uj 发展/vn 和/c 实践/v 的/uj 影响/vn 登录/v Davidc/w 
./i Lay/w 教授/n 的/uj 网页/n 可以 链 接到 琳琅满目 的 
学习 指导 数据库 应用 实例 等 材料 无论 是 学生 
还是 研究 人员 阅读 这本 教材 后 一定会 被 线性代数 
的 理论 和 应用 材料 所 吸引 并从 中找到 学习 
线性代数 的 乐趣 体会到 线性代数 教学 改革 的 世界 潮流 
和 方向 主题 模型 2005 P a r a m 
e t e r e s t i m a 
t i o n f o r t e x 
t a n a l y s i s LDA 
. pdfLDA & Gibbs Sampling yangliuy . pdflda 数学 八卦 
. pdf 主题 模型 情感 识别 领域 依旧 是 统计 
自然语言 处理 的 天下 统计 的 方法 简洁 算法 鲁棒 
性强 基于 深层次 语义 网络 的 技术 在 人机对话 适用 
笔者 看了 网上 的 介绍 建议 阅读 中文 的 lda 
数学 八卦 和 国外 的 参数估计 的 文本 分析 也 
可以 看看 2002年 斯坦福大学 吴恩 达 NG 发 的 潜在 
狄利克雷 主题 分布 的 论文 这是 整个 lda 的 鼻祖 
编程 基础 对于 计算机 专业 学生 这个 是 立足 之本 
笔者/n 本科/r 时/n 学过/i C/w 和C+/nr +/i 懂 一些 皮毛 
的 php 面向 对象 的 编程 是由 面向过程 的 编程语言 
发展 过来 的 前者 一般用 在 底层 的 硬件 上 
如 驱动程序 单片机 等 他 后者 在 大 数据 服务 
后台 的 重用 性 代码 维护性 方面 由于 前者 导师 
建议 我 学习 java 作为 语言 大部分 语言 的 语法 
都 相关 学姐/n 向我/nr 推荐/v 了/ul 毕/v 向东/nr 的/uj java/w 
课程/n 我 看了 两个 星期 总觉得 实际操作 太少 了 没有 
一 本书 在手 心 里面 没有 底 在 学习 视频 
课程 的 同时 我 买了 java 编程 思想 并且 请 
老师 推荐 了 几个 开源 项目 目前 从 基础 的 
分词 项目 着手 elips 作为 编辑器 中文 的 分词 项目 
以 中科院 的 分词 系统 为 优 后记 从 猿类 
到 人类 文化 中 两个 关键 特征 显然 是 人类 
所 独有 这 两个 特征 一个 是 宗教 另 一个 
是 讲故事 这两种 特征 需要 语言 进行 传递 人类 的 
演化 罗宾 邓巴 上海文艺出版社 https / / www . toutiao 
. com / a 6 6 5 1 8 4 
5 5 1 7 5 6 5 2 3 1 
6 2 0 / 2019 01 29 17 17 53 
今天 晚上 咱们 要 学习 的 课程 比 之前 的 
难度 要 稍微 大点 也 是 目前 人工智能 领域 最难 
的 研究 方向 之一 自然语言 处理 NLP 自然语言 处理 AI 
领域 第一团 宠 NLP 作为 AI 领域 的 认知 智能 
其 动态 一直都 是 业内 专家 学者 关注 的 重点 
尤其 是 随着 深度 学习 的 不断 进步 通过 深度 
学习 技术 让 NLP 得到 长足 发展 让 机器 早日 
理解 人类 丰富 多变 的 语言 成为 了 众多 AI 
爱好者 和 开发者 的 期待 接下来 童鞋 们 就 跟着 
班主任 一起 来 认识 下 被 称为 AI 领域 第一团 
宠 的 NLP 它 能在 日常 中 解决 哪些 问题 
以及 实 操 中会 遇到 的 困难 等 为了 让 
大家 更 直观 地 理解 自然语言 处理 班主任 画了 一幅 
图 上图 表明 计算机 理解 用户 输入 的 各种 语言 
的 谢谢 的 过程 由 用户 输入 不同 语言 中 
谢谢 的 不同 文本 计算机 根据 不同 文本处理 出 不同 
语言 的 谢谢 最后 再 将 这些 结果 反馈 给 
用户 其中 这个 过程 包含 了 句法分析 自然语言 处理 和 
自然 语言 生成 等 相关 技术 NLP 解决 的 日常 
问题 自然 语言 处理 实际 应用 有 如下 几点 A 
检验 和 提取 不同 类别 的 反馈 通俗 来讲 就是 
重点 文本 分析 例如 通过 一条 微博 一篇 新闻 一条 
朋友圈 研究 不同 的 人 对 某件 事 的 看法 
通过 研究 对象 正面 或者 负面 的 评论 采取 进一步 
的 决策 B 精准 识别 指代 内容 不同 的 目标 
群体 在 交流 过程 中 会有 不同 的 表达 以及 
指代 的 方式 适当 使用 指代 会使 文本 更加 简练 
而且 并不 影响 本意 的 阐述 例如 遇到 生僻字 燚 
不 知道 拼音 的 时候 大多会 求助 一些 搜索引擎 四个 
火 是 什么 搜索引擎 一定 会 告诉 你 燚 念 
什么 而 不是 告诉 我们 这 几个 词 表面 的 
匹配 结果 由此可见 计算机 能够 理解 这些 指代 内容 C 
对 给定 文本 进行 分类 对 给定 的 文本 给出 
预定义 的 一个 或 多个 分类 标签 再 进行 高效 
准确 的 分类 其实 这 就是 一个 简单 的 特征 
提取 过程 通过 不同 的 特征 进行 不同 的 分类 
自然语言 处理 引起 的 歧义 A 自然 语言 的 二义性 
引起 的 歧义 自然 语言 的 二义性 其实 说 的 
就是 自然 语言 中 广泛 存在 的 歧义 现象 比如 
兵乓球 拍卖 完了 可 切 分为 乒乓球 / 拍卖 / 
完了 又 可以 切 分为 乒乓球拍 / 卖 / 完了 
对于 这 两种 切分 都是 正确 的 也 就是说 就算 
是 人工 分词 也 会 产生 歧义 通过 这 两个 
例子 我们 可以 看出 由于 自然 语言 的 二义性 句子 
存在 着 多种 可能 的 组合 方式 或者 句 意 
计算机 在 处理 这些 句子 的 时候 就 会费 很大 
的 劲 B 上下文 理解 引起 的 歧义 所谓 上下文 
就是 当前 这句话 所处 的 语言 环境 这句话 指代 的 
主语 省略 的 部分 前后 联系 等等 都 非常 重要 
以及 影响 着 这句话 因此 上下文 的 理解 是 自然 
语言 处理 复杂性 的 一大 体现 即使 是 同一 概念 
不同 的 人也 有 不同 的 解读 所以 人们 在 
日常 对话 中 也会有 语句 理解 歧义 例如 看 下面 
一段 对话 A 今天 一起 吃饭 吗 B 我 妈 
今天 从 老家 回来 如果 仅仅 按照字面 理解 B 的 
语句 是 无法 回答 A 的 实际上 B 是 告诉 
A 今天 我 妈 来了 不能 和A/nr 一起 吃饭 了 
这是 人际 交往 中 的 一种 间接 拒绝 这就 表明 
相互之间 的 语言 理解 要 借助 语境 推理 上下文 理解 
不 正确 就 会 产生 歧义 那么 在 计算机 的 
自然 语言 处理 中 要让 计算机 尽可能 多 的 模拟 
人 的 智能 让 机器 具备 人 的 上下文 理解 
的 功能 消除歧义 由 以上 自然语言 处理 的 歧义 可以 
看出 NLP 的 关键 在于 消除歧义 问题 而 正确 的 
消除歧义 需要 大量 的 知识 包括 训练 集 的 标注 
与 添加 词典 资源 的 建立 下面 班主任 来 介绍 
三种 消除歧义 的 方法 A 基于 词典 的 消 歧 
拿 词典 中 的 定义 和 歧义 词 出现 的 
上下文 环境 进行 对比 选择 覆盖度 最大 的 作为 该词 
的 词义 这种 消 歧 方法 思想 很 简单 但是 
消 歧 的 准确率 不是 很高 B 有 监督 消 
歧 让 机器 学习 使用 人工 标记 的 数据 并与 
字典 中的 词语 所 代表 的 典型 含义 匹配 例如 
在 「 I often play with my friends near the 
bank 」 一 句中 「 bank 」 一 词 需要 
机器 判断 是 银行 还是 河边 我们 希望 机器 能够 
匹配 句中 单词 最 有可能 表达 的 含义 让 机器 
更 深刻 地 理解 自然语言 C 无 监督 消 歧 
不管 是 基于 词典 的 消 歧 还是 有 监督 
消 歧 都 需要 训练 集 而无 监督 消 歧 
不 需要 这些 预先 知道 的 资源 最 简单 的 
理解 方式 就是 把 它 比作 考试 一般情况 每道/i 题/n 
都有/nr 一个/m 固定/a 答案/n 对错 代表 分数 的 高低 那像/nr 
作文 只有 题目 没有 固定 答案 打分 情况 就 要 
酌情 而定 360 实习生 面试 过程 18年 2 月初 去 
的 360 面试 这是 第一次 去 大型 互联网 公司 面试 
也 只是 抱着 试试看 的 念头 但是 年后 HR 给 
我 打电话 说 我 通过 了 然后就 没 再 准备 
其他 公司 的 一心 等 3月 多去 公司 结果 一直 
没 等到 正式 offer 的 我 给 公司 打 电话 
跟 我 说 人 招 满了 忘记 通知 我 了 
呵呵 到 无话可说 如果 第一 个 电话 告诉 我 说 
我 没过 也 可以 关键 是 我 都 准备 着 
去 北京 了 结果 又 是 因为 其他 原因 说 
忘记 通知 了 ε = ´ ο ｀ * 唉 
面试 经过 共 3 面 第一面 是个 技术 大牛 怼 
了 我 一顿 主要 有 这样 几个 问题 1 . 
当场 写 代码 tensorflow 的 应用 比如 写 一个 对 
图像 进行 分类 的 实际 应用 代码 2 . keras 
的 应用 不用 写 代码 但是 要 知道 应用 场景 
3 . 推荐算法 的 矩阵 分解 怎么做 4 . 最大 
似 然 和 贝叶斯 分类 的 区别 回答 先验 的 
有无 分别 适用 于 什么 场景 贝叶斯 适用 于小/nr 数据集 
因为 先验 好 求 5 . 判别式 和 生成式 的 
算法 各有 哪些 区别 是 什么 分别 适用 于 什么 
场景 6/m ./i LR/w 和/c 最大/a 似/d 然/c 的/uj 区别/n 
7/m ./i L1/i 和/c L2/i 范数/n 的/uj 区别/n 以/p 及/c 
适用/v 的/uj 场景/n 8/m ./i 各种/r 算法/n 的/uj 损失/n 函数/n 
怎么/r 写/v 第二面/m 应该/v 也/d 是/v 一个/m 技术/n 看起来 像 
是 管理层 问 的 比较 简单 就是 介绍 一下 自己 
简历 上 做过 的 项目 三面 是 HR 面试 一个 
孕妇 问 了 一下 薪资 的 问题 总的 2个 多 
小时 的 时间 3个 面试 就 结束 了 可以 说 
是 比较 有效率 滴滴 电话 面试 一共 是 2 面 
都是 技术 面试 两个 都是/nr 电话 面试 中间 隔了 2天 
时间 大概 每个 都 半个 多 小时 时间 一面 面试官 
环境 声音 比较 嘈杂 很多 时候 听 不清楚 但是 听 
起来 脾气 蛮好 开始 面试官 进行 了 自我 介绍 然后 
我 做 自我 介绍 问 选择 一个 最 熟悉 的 
项目 进行 讲解 答 我 讲 的 爬虫 + 文档 
分类 + 推荐算法 设计 到 word2vec svm 贝叶斯 协同 过滤 
矩阵 分解 冷启动 问 文档 分类 的 准确度 以及 使用 
什么 方法 得到 的 准确度 答 96% 左右 k 折 
交叉 验证 面试官 讲 了 他们 做 测试 的 时候 
是 16年 的 训练 17年 的 验证 问 对 深度 
学习 的 理解 答 略 讲了 CNN 卷积 卷积 层 
和 pooling 层 的 作用 CNN 的 主要 目的 问 
从头 讲 一下 LR 答 loss function + 判别式 函数 
+ 线性 分类器 原因 决策 平面 是 线性 + softmax 
+ 判别 平面 受 所有 数据 影响 问 对 强化 
学习 的 理解 答 并 不是 很懂 问 关于 SVM 
的 问题 答 正则化 L1 L2 范数 分类 平面 拉格朗日 
二面 环境 比较 清晰 面试官 声音 很 清楚 微信 语音 
面试 也是 先 自我介绍 问 指定 的 智能 机器人 那个/nr 
项目 讲 解答 关键 是 cnn 做 图像 分类 算法 
问 对 医疗 那个/nr 项目 讲 解答 使用 各种 算法 
做 测试 对 数据 的 处理 问 讲解 最 熟悉 
的 算法 答 讲 的 bp 神经网络 梯度 下降 问 
其他 算法 如 k means 答 讲了 算法 流程 问 
svm 如何 控制 模型 复杂度 防止 过拟合 答 自带 L2 
正则 减小 W 权重 参数 期间 面试官 给 发 了 
一个 网址 石墨 文档 一个 杨辉三角 的 编程 题 只需要 
写出 算法 原理 就行 做 的 时候 很 紧张 中科院 
自动化所 视频面试 3.7号 收到 中科院 的 邮件 让 看 一篇 
强化 学习 的 论文 Human level control through d e 
e p r e i n f o r c 
e m e n t learning 谷歌 发在 nature 上 
的 DQN 文章 两天 之内 给出 理解 报告 3.12号 上午 
10点 进行 的 视频面试 是 北大 的 一个 小哥哥 面试 
的 很 温柔 没有 问 太多 问题 大部分 都是 让 
自己 讲 我 讲 了 自己 两个 项目 然后又 讲了 
k means bp 神经网络 tensorflow 进行 的 图像 分类 关于 
那篇/nr 论文 的 理解 就 结束 了 总共 半个 小时 
的 时间 微信 公众 号 关键字 全网 搜索 最新 排名 
机器学习 算法 排名 第一 机器学习 排名 第一 Python 排名 第三 
算法 排名第 四源 | AI 深入浅出 最近 几个月 小 编 
遨游 在 税务 行业 的 智能 问答 调研 和 开发 
中 里面 涉及 到 了 很多 的 自然 语言 处理 
NLP 的 功能 点 虽然 接触 NLP 也有 近两年 的 
时间 了 现在 真正 要 应用到 问 答中 避 免不了 
还是 需要 再 重新 熟识 并 深入 研究 理解 下面 
是 与 NLP 相关 的 一些 书籍 推荐 课件 推荐 
和 开源 工具 推荐 主要 是 记录 下 入门 的 
资料 由于 资料 的 存储 位置 没有 做 规整 所以 
本文 没有 附带 资源 下载 链接 如果 有 同学 需要 
其中 的 资源 可以 在 公众 号 上 给 我 
留言 回头 我 把 资源 链接 反馈 给 您 部分 
开源 工具 和 语料 资源 1 NLTK 官方 提供 的 
语料库 资源 列表 2 OpenNLP 上 的 开源 自然语言 处理 
工具 列表 3 斯坦福 大学 自然 语言 处理 组 维护 
的 统计 自然语言 处理 及 基于 语料库 的 计算 语言学 
资源 列表 4 LDC 上 免费 的 中文 信息 处理 
资源 课件 1 哈工大 刘挺/nr 老师 的 统计 自然语言 处理 
课件 2 哈工大 刘秉 权 老师 的 自然语言 处理 课件 
3 中科院计算所 刘群 老师 的 计算 语言学 讲义 课件 4 
中科院 自动化所 宗 成庆 老师 的 自然语言 理解 课件 5 
北大 常 宝宝 老师 的 计算 语言学 课件 6 北大 
詹 卫东 老师 的 中文信息处理 基础 的 课件 及 相关 
代码 7 MIT 大牛 Michael Collins 的 Machine Learning Approaches 
for Natural Language Processing 面向 自然语言 处理 的 机器 学习 
方法 课件 8 Michael Collins 的 Machine Learning 机器学习 课件 
9 SMT 牛人 Philipp Koehn Advanced Natural Language Processing 高级 
自然语言 处理 课件 10 Philipp Koehn Empirical Methods in Natural 
Language Processing 课件 11 Philipp Koehn Machine Translation 机器翻译 课件 
书籍 1 自然语言 处理 综论 英文版 第二 版 2 统计 
自然语言 处理 基础 英文版 3 用 Python 进行 自然语言 处理 
NLTK 配 套书 4 Learning Python 第三版 Python 入门 经典 
书籍 详细 而 不厌其烦 5 自然语言 处理 中的 模式识别 6 
EM 算法 及其 扩展 7 统计 学习 基础 8 自然语言 
理解 英文版 似乎 只有 前 9 章 9 Fundamentals of 
Speech Recognition 质量 不太好 不过 第 6 章 关于 HMM 
的 部分 比较 详细 作者 之一 便是 Lawrence Rabiner 10 
概率 统计 经典 入门书 概率论 及其 应用 英文版 威廉 * 
费勒 著 第一卷 第二卷 DjVuLibre 阅读器 阅读 前 两卷 书 
需要 11 一本 利用 Perl 和 Prolog 进行 自然语言 处理 
的 介绍 书籍 An Introduction to Language Processing with Perl 
and Prolog 12 国外 机器学习 书籍 之 1 Programming Collective 
Intelligence 中文 译名 集体 智慧 编程 机器学习 & 数据挖掘 领域 
近年 出 的 入门 好书 培养 兴趣 是 最重要 的 
一环 一上 来看 大部头 很容易 被 吓走 的 2 Machine 
Learning 机器学习 领域 无可争议 的 经典 书籍 下载 完毕 将 
后缀 改为 pdf 即可 豆瓣 评论 by 王宁 老 书 
牛人 现在看来 内容 并 不算 深 很多 章节 有 点到为止 
的 感觉 但是 很 适合 新手 当然 不能 新 到/v 
连/nr 算法/n 和/c 概率/n 都不/nr 知道/v 入门 比如 决策树 部分 
就 很 精彩 并且 这 几年 没有 特别 大 的 
进展 所以 并不 过时 另外 这本书 算是 对 97 年前 
数十年 机器学习 工作 的 大 综述 参考文献 列表 极 有价值 
国内/s 有/v 翻译/v 和/c 影印版/n 不 知道 绝版 否 3 
Introduction to Machine Learning 13 国外 数据挖掘 书籍 之 1 
Data . Mining . Concepts . and . Techniques . 
2nd 数据挖掘 经典 书籍 华裔 科学家 写 的 书 相当 
深入浅出 2 Data Mining Practical Machine Learning Tools and Techniques3 
Beautiful Data The Stories Behind Elegant Data Solutions Toby Segaran 
Jeff Hammerbacher 14 国外 模式识别 书籍 之 1 Pattern Recognition 
2 Pattern Recongnition Technologies and Applications 3 An Introduction to 
Pattern Recognition 4 Introduction to Statistical Pattern Recognition 5 Statistical 
Pattern Recognition 2nd Edition 6 Supervised and Unsupervised Pattern Recognition 
7 Support Vector Machines for Pattern Classification 15 国外 人工智能 
书籍 之 1 Artificial Intelligence A Modern Approach 2nd Edition 
人工智能 领域 无 争议 的 经典 2 Paradigms of Artificial 
Intelligence Programming Case Studies in Common LISP 16 其他 相关 
书籍 1 Programming the Semantic Web Toby Segaran Colin Evans 
Jamie Taylor2 Learning . Python 第四版 英文 加入 微信 机器学习 
交流 群 请 添加 微信 guodongwe1991 备注 姓名 单位 研究 
方向 广告 商业 合作 请 添加 微信 guodongwe1991 备注 商务 
合作 微信 公众 号 关键字 全网 搜索 最新 排名 机器学习 
算法 排名 第一 机器学习 排名 第一 Python 排名 第三 算法 
排名 第四 前言 在对 文本 做 数据 分析 时 一大半 
的 时间 都会 花在 文本 预处理 上 而 中文 和 
英文 的 预处理 流程 稍有 不同 本文 对 中文 文本 
挖掘 的 预处理 流程 做 一个 总结 中文 文本 挖掘 
预处理 特点 首先 看 中文 文本 挖掘 预处理 与 英文 
文本 挖掘 预处理 的 不同 点 首先 中文 文本 是 
没有 像 英文 的 单词 空格 那样 隔开 的 因此 
不能 直接 像 英文 一样 可以 直接 用 最简单 的 
空格 和 标点符号 完成 分词 所以 一般 需要 用 分词 
算法 来 完成 分词 在 干货 | 自然语言 处理 1 
之 聊 一 聊 分词 原理 已经 讲 到了 中文 
的 分词 原理 第二 中文 的 编码 不是 utf8 而是 
unicode 这样 会 导致 在 分词 时 需要 处理 编码 
的 问题 上述 两 点 构成 了 中文分词 相比 英文 
分词 的 一些 不同 点 后面 也会 重点 讲述 这 
部分 的 处理 了解 了 中文 预处理 的 一些 特点 
后 通过实践 总 结下 中文 文本 挖掘 预处理 流程 1 
. 数据 收集 在 文本 挖掘 之前 需要 得到 文本 
数据 文本 数据 的 获取 方法 一般 有 两种 使用 
别人 做好 的 语料库 和 自己 用 爬虫 去 在 
网上 去 爬 自己 的 语料 数据 对于 第 一种 
方法 常用 的 文本 语料库 在 网上 有 很多 如果 
大家 只是 学习 则 可以 直接 下载 下来 使用 但 
如果 是 某些 特殊 主题 的 语料库 比如 机器学习 相关 
的 语料库 则 这种 方法 行不通 需要 我们 自己 用 
第二 种 方法 去 获取 对于 第二 种 使用 爬虫 
的 方法 开源 工具 有 很多 通用 的 爬虫 我 
一般 使用 beautifulsoup 但是 我们 需要 某些 特殊 的 语料 
数据 比如 上面 提到 的 机器学习 相关 的 语料库 则 
需要 用 主题 爬虫 也叫 聚焦 爬虫 来 完成 一般 
使用 ache ache 允许 我们 用 关键字 或者 一个 分类 
算法 来 过滤 出 我们 需要 的 主题 语料 比较 
强大 2 . 除去 数据 中 非中文 部分 这一步 主要 
是 针对 用 爬虫 收集 的 语料 数据 由于 爬 
下来 的 内容 中 有 很多 html 的 一些 标签 
需要 去掉 少量 的 非 文本 内容 的 可以 直接 
用 Python 的 正则表达式 re 删除 复杂 的 则 可以 
用 beautifulsoup 来 去除 去除 掉 这些 非 文本 的 
内容 后 就 可以 进行 真正 的 文本 预处理 了 
3 . 处理 中文 编码 问题 由于 Python2 . x 
不支持 unicode 的 处理 因此 使用 Python2 . x 做 
中文 文本 预处理 时 需要 遵循 的 原则 是 存储 
数据 都用 utf8 读 出来 进行 中文 相关 处理 时 
使用 GBK 之类 的 中文 编码 在下 一节 的 分词 
再用 例子 说明 这个 问题 4 . 中文分词 常用 的 
中文分词 软件 有 很多 比较 推荐 结巴 分词 安装 也 
很 简单 比如 基于 Python 的 用 pip install jieba 
就 可以 完成 下面 我们 就 用 例子 来 看看 
如何 中文分词 首先 准备 两段 文本 内容 分别 如下 nlp/w 
_/i test0/i ./i txt/w 沙/nr 瑞金/ns 赞叹/v 易/a 学习/v 的/uj 
胸怀/n 是 金山 的 百姓 有福 可是 这件事 对 李 
达康 的 触动 很大 易 学习 又 回忆 起 他们 
三 人 分开 的 前一晚 大家 一起 喝酒 话别 易 
学习 被 降职 到 道口 县 当 县长 王 大路 
下海经商 李 达康 连连 赔礼道歉 觉得 对不起 大家 他 最 
对不起 的 是 王 大路 就 和易 学习 一起 给 
王 大路 凑了 5万 块钱 王 大路 自己 东挪西撮 了 
5 万块 开始 下海经商 没想到 后来 王 大路 竟然 做得 
风生水 起 沙 瑞金 觉得 他们 三人 在 困难 时期 
还 能以 沫 相助 很 不 容易 nlp/w _/i test2/i 
./i txt/w 沙/nr 瑞金/ns 向毛娅/nr 打听/v 他们/r 家/q 在京/i 州/n 
的/uj 别墅/n 毛娅/nr 笑着 说 王 大路 事业有成 之后 要给 
欧阳菁/nr 和她 公司 的 股权 她们 没有 要 王 大路 
就 在京 州 帝豪 园 买了 三套 别墅 可是/c 李/nr 
达康/nr 和易/nz 学习/v 都/d 不要/df 这些 房子 都在 王 大路 
的 名下 欧阳菁/nr 好像 去住 过 毛娅/nr 不想 去 她 
觉得 房子 太大 很 浪费 自己 家 住得 就很 踏实 
首先 将 文本 从 第一 个 文件 中 读取 进来 
并 使用 中文 GBK 编码 再 调用 结巴 分词 最后 
把 分词 结果 用 uft8 格式 存在 另一个 文本 nlp 
_ test1 . txt 中 代码 如下 # * coding 
utf 8 * import jiebawith open . / nlp _ 
test0 . txt as f document = f . read 
document _ decode = document . decode GBK document _ 
cut = jieba . cut document _ decode result = 
. join document _ cut result = result . encode 
utf 8 with open . / nlp _ test1 . 
txt w as f2 f2 . write result f . 
close f2 . close 输出 的 文本 内容 如下 nlp 
_ test1 . txt 沙 瑞金 赞叹 易 学习 的 
胸怀 是 金山 的 百姓 有福 可是 这件 事 对 
李 达康 的 触动 很大 易 学习 又 回忆起 他们 
三人 分开 的 前一晚 大家 一起 喝酒 话别 易 学习 
被 降职 到 道口 县 当 县长 王 大路 下海经商 
李 达康 连连 赔礼道歉 觉得 对不起 大家 他 最 对不起 
的 是 王 大路 就 和 易 学习 一起 给 
王 大路 凑 了 5 万块 钱 王 大路 自己 
东挪西撮 了 5 万块 开始 下海经商 没想到 后来 王 大路 
竟然 做 得 风生水 起 沙 瑞金 觉得 他们 三人 
在 困难 时期 还 能 以 沫 相助 很 不 
容易 可以 发现 对于 一些 人名 和 地名 jieba 处理 
不好 不过 可以 帮 jieba 加入 词汇 如下 jieba . 
suggest _ freq 沙 瑞金 True jieba . suggest _ 
freq 易 学习 True jieba . suggest _ freq 王 
大路 True jieba . suggest _ freq 京 州 True 
现在 再 重新 进行 读 文件 编码 分词 编码 和写/nr 
文件 代码 如下 with open . / nlp _ test0 
. txt as f document = f . read document 
_ decode = document . decode GBK document _ cut 
= jieba . cut document _ decode result = . 
join document _ cut result = result . encode utf 
8 with open . / nlp _ test1 . txt 
w as f2 f2 . write result f . close 
f2 . close 输出 的 文本 内容 如下 nlp _ 
test1 . txt 沙 瑞金 赞叹 易 学习 的 胸怀 
是 金山 的 百姓 有福 可是 这件 事 对 李 
达康 的 触动 很大 易 学习 又 回忆起 他们 三人 
分开 的 前一晚 大家 一起 喝酒 话别 易 学习 被 
降职 到 道口 县 当 县长 王 大路 下海经商 李 
达康 连连 赔礼道歉 觉得 对不起 大家 他 最 对不起 的 
是 王 大路 就 和 易 学习 一起 给 王 
大路 凑 了 5 万块 钱 王 大路 自己 东挪西撮 
了 5 万块 开始 下海经商 没想到 后来 王 大路 竟然 
做 得 风生水 起 沙 瑞金 觉得 他们 三人 在 
困难 时期 还 能 以 沫 相助 很 不 容易 
以 同样 的 方法 对 第二段 文本 nlp _ test2 
. txt 进行 分词 和 写入 文件 nlp _ test3 
. txt with open . / nlp _ test2 . 
txt as f document2 = f . read document2 _ 
decode = document2 . decode GBK document2 _ cut = 
jieba . cut document2 _ decode # print   . 
join jieba _ cut result = . join document2 _ 
cut result = result . encode utf 8 with open 
. / nlp _ test3 . txt w as f2 
f2 . write result f . close f2 . close 
输出 的 文本 内容 如下 nlp _ test3 . txt 
沙 瑞金 向 毛娅/nr 打听 他们 家 在 京 州 
的 别墅 毛娅/nr 笑 着 说 王 大路 事业有成 之后 
要 给 欧阳 菁 和 她 公司 的 股权 她们 
没有 要 王 大路 就 在 京 州 帝豪 园 
买 了 三套 别墅 可是 李 达康 和 易 学习 
都 不要 这些 房子 都 在 王 大路 的 名下 
欧阳 菁 好像 去 住 过 毛娅/nr 不想 去 她 
觉得 房子 太大 很 浪费 自己 家住 得 就 很 
踏实 5 . 引入 停用词 上面 解析 的 文本 中 
有 很多 无效 的 词 比如 着 和 还有 一些 
标点符号 这些 我们 不想 在 文本 分析 时 引入 因此 
需要 去掉 这些 词 就是 停用词 常用 的 中文 停用 
词表 是 1208个 下载 地址 https / / pan . 
baidu . com / s / 1gfMXMl9 现在 将 停用 
词表 从文件 读出 并 切 分成 一个 数组 备用 # 
从文件 导入 停用 词表 stpwrdpath = stop _ words . 
txt stpwrd _ dic = open stpwrdpath rb stpwrd _ 
content = stpwrd _ dic . read # 将 停用 
词表 转换 为 liststpwrdlst = stpwrd _ content . splitlines 
stpwrd _ dic . close 6 . 特征 处理 现在 
可以 用 scikit learn 来 对 文本 特征 进行 处理 
在 中讲 到了 两种 特征 处理 的 方法 向 量化 
与 Hash Trick 而 向 量化 是 最 常用 的 
方法 因为 它 可以 接着 进行 TF IDF 的 特征 
处理 在 中 也 讲 到了 TF IDF 特征 处理 
的 方法 这里 使用 scikit learn 的 TfidfVectorizer 类 来 
进行 TF IDF 特征 处理 TfidfVectorizer 类 可以 完成 向 
量化 TF IDF 和 标准化 三步 当然 还 可以 处理 
停用词 现在 把 上面 分词 好 的 文本 载入 内存 
with open . / nlp _ test1 . txt as 
f3 res1 = f3 . read print res1with open . 
/ nlp _ test3 . txt as f4 res2 = 
f4 . read print res2 现在 可以 进行 向 量化 
TF IDF 和 标准化 三步 处理 这里 引入 了 上面 
的 停用 词表 from sklearn . feature _ extraction . 
text import T f i d f V e c 
t o r i z e r c o r 
p u s = res1 res2 vector = TfidfVectorizer stop 
_ words = stpwrdlst tfidf = vector . fit _ 
transform corpus print tfidf 部分 输出 如下 0 44 0.154467434933 
0 59 0.108549295069 0 39 0.308934869866 0 53 0.108549295069 . 
. . . 1 27 0.139891059658 1 47 0.139891059658 1 
30 0.139891059658 1 60 0.139891059658 看看 每个 词 与 TF 
IDF 的 对应 关系 wordlist = vector . get _ 
feature _ names # 获 取词 袋 模型 中 的 
所有 词 # tf idf 矩阵 元素 a i j 
表示 j 词 在 i 类 文本 中的 tf idf 
权重 weightlist = tfidf . toarray # 打印 每类 文本 
的 tf idf 词语 权重 第一 个 for 遍历 所有 
文本 第二个 for 便利 某一 类 文本 下 的 词语 
权重 for i in range len weightlist print 第 i 
段 文本 的 词语 tf idf 权重 for j in 
range len wordlist print wordlist j weightlist i j 部分 
输出 如下 第 0 段 文本 的 词语 tf idf 
权重 一起 0.217098590137 万块 0.217098590137 三人 0.217098590137 三套 0.0 下海经商 
0.217098590137 . . . . . 第 1 段 文本 
的 词语 tf idf 权重 . . . . . 
李 达康 0.0995336411066 欧阳 0.279782119316 毛娅/nr 0.419673178975 沙 瑞金 0.0995336411066 
没想到 0.0 没有 0.139891059658 浪费 0.139891059658 王 大路 0.29860092332 . 
. . . . 7 . 建立 分析模型 有了 每段 
文本 的 TF IDF 的 特征向量 就 可以 利用 这些 
数据 建立 分类 或者 聚 类 模型 了 或者 进行 
主题 模型 的 分析 此时 的 分类 聚 类 模型 
和 之前 讲 的 非 自然 语言 处理 的 数据 
分析 没有 什么 两样 因此 对应 的 算法 都 可以 
直接 使用 小结 本文 对 中文 文本 挖掘 预处理 的 
过程 做 了 一个 总结 希望 可以 帮助 到 大家 
需要 注意 的 是 这个 流程 主要 针对 一些 常用 
的 文本 挖掘 并 使用 了 词 袋 模型 对于 
某 一些 自然 语言 处理 的 需求 则 流程 需要 
修改 比如 我们 涉及 到 词 上下文 关系 的 一些 
需求 此时 不能 使 用词 袋 模型 而有 时候 我们 
对于 特征 的 处理 有 自己 的 特殊 需求 因此 
这个 流程 仅供 自然语言 处理 入门者 参考 欢迎 分享 给 
他人 让 更多 的 人 受益 参考 宗 成庆 统计 
自然语言 处理 第 2版 博客园 http / / www . 
cnblogs . com / pinard / p / 6744056 . 
html 近期 热 文 普通 程序员 转型 深度 学习指南 机器学习 
33 之 局部 线性 嵌入 LLE 降 维 总结 干货 
| 自然语言 处理 3 之 词频 逆 文本 词频 TF 
IDF 详解 机器学习 32 之 典型 相关性 分析 CCA 详解 
文末 有 福利 . . . . . . 干货 
| 自然语言 处理 2 之 浅谈 向 量化 与 Hash 
Trick 干货 | 自然语言 处理 1 之 聊 一 聊 
分词 原理 加入 微信 机器学习 交流 群 请 添加 微信 
guodongwe1991 备注 姓名 单位 研究 方向 广告 商业 合作 请 
添加 微信 guodongwe1991 备注 商务 合作 自然语言 处理 NLP 概述 
版权 声明 本文 为 博主 chszs 的 原创 文章 未经 
博主 允许 不得 转载 自然语言 处理 Natural language processing NLP 
是 计算机 和 人类 语言 之间 的 关系 纽带 更 
具体地说 自然语言 处理 是 计算机 对 自然 语言 的 理解 
分析 操纵 和/或/nr 生成 计算机程序 能否 将 一段 英文 文本 
转换成 程序员 友好 的 数据 结构 来 描述 自然语言 文本 
的 含义 不幸 的 是 这种 数据 结构 的 形式 
是否 存在 并 没有 形成 共识 在 解决 这些 基本 
的 人工智能 问题 之前 计算机 科学家 必须 解决 提取 描述 
文本 信息 有限 方面 的 简单 表示 的 简化 目标 
概述 自然语言 处理 自然语言 处理 NLP 可以 被 定义 为 
人类 语言 的 自动 或 半自动 处理 NLP 这个 术语 
有时 被 用于 比 这 更 窄 的 范围 通常 
不 包括 信息检索 有时 甚至 不 包括 机器翻译 NLP 有时 
还与 计算 语言学 相 对立 NLP 被 认为 更 适用 
如今 往往 首选 使用 替代 术语 如 语言 技术 Language 
Technology 或 语言工程 Language Engineering 语言 Language 经常 与 演讲 
Speech 比如 演讲 技术 和 语言 技术 相 对照 但是 
我 将 简单 地 提到 NLP 并 广义 地 使用 
这个 术语 NLP 本质上 是 多 学科 的 它 与 
语言学 密切相关 尽管 NLP 公然 借鉴 语言 理论 的 程度 
差异 很大 什么 是 自然 语言 处理 NLP 是 计算机 
以 一种 聪明 而 有用 的 方式 分析 理解/v 和从/nr 
人类/n 语言/n 中/f 获取/v 意义/n 的/uj 一种/m 方式/n 通过 利用 
NLP 开发 者 可以 组织 和 构建 知识 来 执行 
自动 摘要 翻译 命名 实体 识别 关系 提取 情感 分析 
语音 识别 和 话题 分割 等 任务 NLP 用于 分析 
文本 使 机器 了解 人 的 说话 方式 这种 人机交互 
使 现实 世界 的 应用 如 自动 文摘 情感 分析 
主题 提取 命名 实体 识别 零部件 词性 标注 关系 提取 
词干 等等 NLP 通常用于 文本 挖掘 机器 翻译 和 自动 
问答 NLP 技术 NLP 的 重要性 早期 的 NLP 方法 
涉及 更 基于 规则 的 方法 在 这种 方法 中 
简单 的 机器学习 算法 被 告知 要在 文本 中 查找 
哪些 单词 和 短语 并 在 这些 短语 出现 时 
给出 特定 的 响应 但 深度 学习 是 一个 更 
灵活 直观 的 方法 在 这个 方法 中 算 法学会 
从 许多 例子 中 识别 说话者 的 意图 就像 孩子 
如何 学习 人类 语言 一样 在 考虑 以下 两个 陈述 
时 可以 看到 自然语言 处理 的 优势 云计算 保险 应该 
成为 每个 服务 级别 协议 的 一部分 和 良好 的 
SLA 确保 夜间 睡眠 更加 容易 即使 在云端 如果 您 
使用 国家 语言 处理 的 搜索 程序 将 认识 到 
云计算 是 一个 实体 云是/nr 云计算 的 缩写 形式 并且 
SLA 是 服务 级别 协议 的 行业 首 字母 缩略词 
自然语言 处理 的 术语 这些 分区 与 语言学 的 一些 
标准 分支 松散地 相 对应 形态学 Morphology 词 的 结构 
例如 不 寻常 的 可以 被 认为 是 由 一个 
前缀 un 一个 词干 和 一个 词缀 ly 组成 构成 
是 构成 加上 屈折 词缀 拼写 规则 意味着 我们 结束 
而 不是 组成 语法 Syntax 单词 用于 形成 短语 的 
方式 例如 它 是 英语 语法 的 一部分 诸如 意志 
之类 的 确定者 会在 名词 前面 出现 而且 确定 者 
对于 某些 单数 名词 是 强制性 的 语义 Semantics 构成 
语义 是 基于 语法 的 意义 的 建构 通常 表示 
为 逻辑 这与 词汇 语义 即 单词 的 含义 形成 
对照 自然语言 处理 的 应用 以下 是 目前 使用 NLP 
的 几种 常用 方法 Microsoft Word 中的 拼 写检查 功能 
是 最 基本 和最/nr 知名 的 应用 程序 文本 分析 
也 称为 情感 分析 是 NLP 的 一个 关键 用途 
企业 可以 使用 它 来 了解 他们 的 客户 感受到 
的 情绪 并 使用 这些 数据 来 改善 他们 的 
服务 通过 使用 电子 邮件 过滤器 分析 流经 其 服务器 
的 电子邮件 电子邮件 提供商 可以 使用 朴素 贝叶斯 垃圾邮件 过滤 
来 计算 电子邮件 基于 其 内容 的 可能性 呼叫 中心 
代表 经常 听到 来自 客户 的 相同 的 具体 投诉 
问题 和 问题 挖掘 这些 数据 的 情绪 可以 产生 
令人 难以置信 的 可 操作 的 情报 可以 应用 于 
产品 布局 消息传递 设计 或 其他 一 系列 的 用途 
Google Bing/w 和/c 其他/r 搜索/v 系统/n 使用/v NLP/w 从/p 文本/n 
中/f 提取/v 条件/n 来/v 填充/v 其/r 索引/nr 并/c 解析/vn 搜索/v 
查询/v Google Translate 将 机器 翻译 技术 应用于 翻译 单词 
而且 还 用于 理解 句子 的 含义 以 改善 翻译 
金融 市场 使用 NLP 通过 明文 公告 和 提取 相关 
信息 的 格式 进行 算法 交易 决策 例如 公司 之间 
合并 的 消息 可能会 对 交易 决策 产生 重大 影响 
并且 合并 细节 例如 参与者 价格 谁 获得 谁 的 
速度 可以 被 纳入 到 交易 算法 中 在 数百 
万 美元 自然语言 处理 的 例子 使用 Summarizer 自动 总结 
一个 文本 块 严格 的 主题 句子 并 忽略 其余 
的 生成 关键字 话题 标签 文档 使用 LDA 隐含 狄利克雷 
分布 它 从 一个 确定 最 相关 的 词 文件 
该/r 算法/n 是/v 自动/vn 标记/n 和/c 自动/vn 标记/n URL/w 微/n 
服务/vn 的/uj 核心/n 基于/p Stanford/w NLP 的 情感 分析 可以 
用来 辨别 一个 陈述 的 感觉 观点或 信念 从 非常 
消极 中立 到 非常 积极 参考 1 Ann Copestake Natural 
Language Processing 2004 8 Lectures available online at https / 
/ www . cl . cam . ac . uk 
/ teaching / 2002 / NatLangProc / revised . pdf 
2 Ronan Collobert and Jason Weston Natural Language Processing Almost 
from Scratch Journal of Machine Learning Research 12 2011 pp 
. 2493 2537 3 Top 5 Semantic Technology Trends to 
look for in 2017 available online at https / / 
ontotext . com / top 5 semantic technology trends 2017 
/ 这 学期 修了 Prof . Daniel Gildea 的 Statistical 
Speech and Language Processing 课程 作为 machine learning 的 进阶 
课程 这门 课 的确 难度 不小 本文 记录 了 这两天 
复习 期末考 所作 的 笔记 里面 涵盖 了 本次 课程 
涉及 的 主要 内容 忽略 了 一些 复杂 较为 不 
重要 的 部分 另外 列出 了 课上 和 复习 期间 
领悟 的 一些 心得 个人 以为 是 上 这门 课 
最大 的 收获 水平 有限 理解 难免 有误 请 各位 
指正 另外 推荐 大家 去看 Coursera 上 哥大 的 Natural 
Language Processing 课程 讲 的 非常 清楚 例子 也 很多 
总结 NLP 课程 主要 介绍 了 以下 三类 应用 POS 
Tagging 给 句子 中 的 每个 词 标记 相应 的 
词性 Parsing 分析 得到 句子 的 句法结构 通常 得到 一个 
句法树 Machine Translation 将 源语言 表述 的 句子 翻译 为 
目标语言 表述 的 句子 其实 这 三类 应用 之间 互 
有联系 方法 也有 相似之处 粗略地 讲 Parsing 可以 看作 Tagging 
的 进化版 因为 它 不仅 需要 对 每个 词 进行 
标记 还 需要 标 记出 层次性 的 句法结构 因此 虽然 
Parsing 的 算法 同样 是 动态规划 或 前 向 后向 
但 比 Tagging 要 计算 多 一个 维度 的 信息 
MT 尤其 是 Syntax based MT 与 Parsing 也有 相似之处 
只不过 除了 Parse 源 句子 外 还要 注意 两 个 
平行 语料库 parrallel corpus 的 关系 动态规划 DP 在 NLP 
中 尤为 重要 只要 涉及 到 解码 问题 decoding 即 
给定 数据 句子 和 模型 参数 求 最优 解 序列 
基本 都要 用到 DP 来 求解 这就 很 自然 地 
延伸 到 两个 问题 为什么 要 用 DP 因为 可行 
解的/nr 数量 太多 对于 长度 为 nn 状态 个数 为 
TT 的 解码 问题 可行/v 解有/nr TnT/w ^/i n/w 个/q 
穷举法 效率 太低 而 DP 可以 给出 多项式 时间 复杂度 
的 解法 为什么 可以 用 DP 个人 认为 这 涉及 
到 NLP 中 模型 设计 动态规划/i 适用/v 于有/nr 重叠/v 子/ng 
问题/n 和/c 最优/d 子结构/n 性质/n 的/uj 问题/n 而 NLP 中 
所用 的 模型 如 HMM PCFG 的 性质 使其 优化 
问题 具有 类似 max ∏ max \ prod 的 形式 
而 max ∏ max \ prod 可以 转化 为 ∏ 
max \ prod max 来 求解 准确 来说 需要 每 
一项 ∈ ℜ + \ in \ Re ^ + 
而 概率 一定 p ≥ 0p \ geq 0 因此 
满足 该 限制 故 产生 了 重叠 子 问题 和 
最优 子结构 的 性质 从而 可以 用 DP 来 求解 
换句话说 NLP 中 模型 的 设计 很 重要 的 一点 
是 可以 使用 动态 规划 进行 高效 求解 对于 参数 
求解 问题 parameters learning 一般 会 涉及 到 以下 几种 
模型 和 算法 它们 之间 的 优劣 值得 思考 隐 
马尔科夫 模型 HMM 和 期望 最大化 算法 EM EM 常 
用于 学习 含有 隐 变量 hidden variables 的 模型 它 
是 一种 迭代 求解 的 算法 重复 E 步 估计 
在 当前 模型 参 数下 的 数据分布 以及 M 步 
通过 该 分布 利用 最大 似 然 法 更新 模型 
参数 HMM/w 中的/i E/w 步/n 通过/p 前/f 向/p 后向/i 算法/n 
其实 也是 动态规划 实现 HMM 是 最为 经典 的 NLP 
统计模型 能够 表示 tags 之间 的 时序 依赖 关系 个人 
觉得 该 算法 的 一个 缺点 是 比较 复杂 计算 
较慢 感知机 算法 Perceptron Algorithm 由 用于 二 分类 的 
感知机 算法 延伸 而来 若 解码 序列 与 真实 序列 
不符 则 更新 相应 参数 的 权重 感知机 算法 的 
优点 是 简单 并且 可以 使用 任意 定义 的 特征 
相对 的 HMM 中 只有 两种 特征 f yi | 
yi − 1 f y _ i | y _ 
{ i 1 } 和f/nr xi | yi f x 
_ i | y _ i 感知机 算法 的 缺点 
可能 是 由于 模型 简单 效果 并 不是 太好 而且 
是个 确定性 模型 deterministic 条件 随 机场 CRF 属于 最大熵 
模型 的 一种 也 可以 看作 指数 线性 模型 与 
生成式 模型 generative 的 HMM 不同 CRF 是 判别式 模型 
discriminative 即 HMM 建模 P XN1 YN1 P X _ 
1 ^ N Y _ 1 ^ N 而 CRF 
建模 P YN1 | XN1 P Y _ 1 ^ 
N | X _ 1 ^ N 另外 CRF 没有 
隐 变量 而且是 无向图 CRF 的 学习 可以 用 梯度 
下降 算法 但 也 需要 用到 前 向 后向 算法 
来 计算 当前 模型 参 数下 特征 的 分布 CRF 
的 优点 是 与 感知机 一样 可以 使用 任意 特征 
但 相比 感知机 而言 它 是 一个 概率模型 结构化 支持 
向量 机 Structured SVM 属于 最大 间隔 模型 同样 从 
用于 二 分类 的 SVM 延伸 而来 其 模型 可以 
与 感知机 算法 进行 类比 SVM 其实 跟 感知机 算法 
没有 太大 区别 甚至 可以 看成 加入 了 L2 正则化 
的 感知机 算法 而 Structured SVM 还有 一项 改变 是 
在 限制 条件 中 加入 了 损失 函数 loss function 
导致 解码 时 需要 把 损失 函数 考虑 进来 还是 
动态规划 因此 Structured SVM 具有 一个 优点 可以 结合 不同 
的 损失 函数 大部分 模型 学习 的 更新 公式 都 
具有 相似 的 形式 即 加上 真实 的 减去 预测 
的 move forwards the y we saw and move away 
from they ̃ \ tilde { y } we expect 
to see 例子 有 Perceptron CRF Structured SVM 至于 原因 
嘛 还 需要 深入 思考 Q 为什么 有了 维 特比 
解码 Viterbi 还 需要 后验/nr 解码 即 HMM 中的 前 
向 后向 Parsing 中的 Inside Outside A 首先 维 特比 
算法 是 求解 解码 问题 的 正确 方法 保证 解码 
后的/nr 序列 得到 的 score 最大 但 我们 在 评价 
performance 的 时候 除了 看 该 序列 是否 正确 这 
对于 一个 序列 只是 对 和错的/nr 二分 问题 还要 看 
每 一位 解码 的 正确 与否 如 真实 序 列为 
ABCD 解码 后序 列为 ABBD 虽然 是 错 的 但有 
其中 三位 都是 对 的 这样 就 会 导致 另一种 
损失 函数 L = ∑ iI yi ≠ yi ~ 
L = \ sum _ iI y _ i \ 
neq \ tilde { y _ i } 而后 验 
解码 正是 相 对应 这种 损失 函数 评价 下 的 
解码 算法 因此 也 叫做 Minimum Bayes Risk MBR 算法 
由于 这种 评价 方法 的 存在 后验/nr 解码 通常 会 
得到 比 维 特比 解码 更好 的 结果 那么 反过来 
为什么 我们 还 需要 维 特比 算法 呢 因为 它 
更加 简单 高效 对比 前 向 后向 算法 而且 它 
本来 就是 对 的 只是 评价 标准 不同 而已 HMM 
可以 看作 CFG 的 特 列 在 笔记 中 略有 
介绍 SCFG 部分 在 笔记 里 懒得 写了 感兴趣 可以 
参考 这篇文章 Introduction to synchronous grammar 还有 其他 感悟 等 
我 想 起来 再 继续 添加 笔记 地址 https / 
/ github . com / xyang35 / course NLP / 
blob / master / notes / nlp _ note . 
pdf 目录 1 Part of Speech POS T a g 
g i n g P e r c e p 
t i o n A l g o r i 
t h m H M M C o n d 
i t i o n a l Random Field CRF 
Structured SVM2 ParsingContext Free Grammar CFG CKY Parsing Viterbi Decoding 
Posterior Decoding Inside Outside Algorithm 3 Machine TranslationWord Alignment IBM 
Models Phrase Based T r a n s l a 
t i o n y n t a x Based 
Translation SCFG 自 人工智能 AI 小发 猫 写作 以来 机器 
生成 原创 进入 了 一个 新的 快速 发展 渠道 并 
开始 引起 广泛 的 社会 关注 近日 美国 微软 公司 
宣布 其 首次 开发 的 机器 生成 原创 系统 达到 
了 人文 专业 水平 从 原稿 到 仿 写 生成 
原创 一般 新闻 实现 了 自然 语言 处理 的 又一 
里程碑式 突破 与此 相关 的 另一个 消息 是 在 2018年 
的 谷歌 亚洲 论坛 上 腾讯 向 会议 的 某些 
论坛 提供 了 人工智能 的 免费 解释 嘉宾 的 演讲 
通过 人工智能 一键 生成 原创 爆 文 确定 并/c 生成/v 
原创/n 成/n 原稿/n 和仿写/nr 字幕 被 筛选 结果 人工智能 通过 
了 小发 猫 写作 并且 该 网站 暴露 了 许多 
问题 和 低级 生成 原创 错误 即便如此 在 人工智能 生成 
原创 技术 发生 迅速 变化 的 时代 高校 外语 专业 
的 教育 工作 者 仍然 需要 思考 一些问题 大学 生成 
原创 专业 是否 已经 面临 生存 危机 高校 如何 改善 
现有 的 人才 发展 计划 以 适应 未来 20年 的 
行业 发展 ● 机器 生成 原创 尚未 达到 理解 人类 
自然 语言 的 水平 在 谈论 人工智能 生成 原创 之前 
让 我们 先 快速 了解 机器 生成 原创 首先 机器 
生成 原创 的 优势 在于 交付 速度快 可以 在 短时间 
内 生成 原创 大量 文本 无需 传闻 其次 成本低 与 
专业 人工 生成 原创 相比 机器 生成 原创 的 单位 
成本 相对 较低 特别 是 在 处理 大量 术语 时 
更多 语法 规范 技术信息 没有 个人 感受 这 一点 尤为 
明显 机器 生成 原创 的 优点 也 体现 在 即时性 
和 可控性 上 机器 生成 原创 不受 时间 和 地点 
的 限制 无需 预约 可 随时 提供 服务 流程 简单 
快捷 在 生成 原创 上 花费 的 总 时间 是 
高度 可控 的 在 质量 方面 机器 生成 原创 的 
生成 原创 是 一致 的 技术 术语 是 高度 准确 
的 当然 目前 的 机器 生成 原创 仍然 存在 瓶颈 
虽然 发展 势头 很快 但 目前 的 生成 原创 质量 
仍然 不 尽如人意 导致 这些 瓶颈 的 因素 不仅 是 
技术 上 的 而且 是 语言 上 的 哲学 因为 
人类 语言 在 实际 使用 中 很复杂 生成 原创 不是 
简单 的 字面 意义 转换 而是 需要 深入 理解 源 
文本 风格 样式 语言 风格 人际 意义 所 涉及 的 
语义 深层 结构 目前 的 机器 生成 原创 还 没有 
达到 理解 人类 自然 语言 的 水平 它 还 没有 
能够 表达 对 概念 文化 的 立体 理解 这在 文学 
生成 原创 中 尤为 明显 ● 人工智能 生成 原创 对 
生成 原创 服务 行业 提出 了 挑战 就像 无人 司机 
一样 但是 我们 不能 低估 人工智能 在 生成 原创 领域 
的 潜在 和 革命性 意义 英国 自然 杂志 近年来 梳理 
了 科技 领域 的 十大 突破 将 人工智能 列为 首位 
我们 已经 看到 机器 生成 原创 系统 经历 了 不断 
的 演变 和 升级 从/p 规则/n 和/c 例子/n 到/v 基于/p 
语料库/n 和/c 统计/v 再到 当今 基于 人工智能 机器 生成 原创 
的 基于 深度 学习 的 神经 网络 机器 生成 原创 
与 过去 相比 生成 原创 质量 得到 了 显着 提高 
并且 变得 越来越 人性化 人工智能 模拟 人类 意识 和 思维 
的 信息 处理 过程 而 深度 学习 技术 模仿 人脑 
神经系统 的 运作 并 扩展 传统 人工神经网络 的 内置 水平 
使 系统 具有 更 复杂 的 学习 行为 基于 深度 
学习 的 神经 网络 机器 生成 原创 已 被谷歌 使用 
Facebook 微软 Apple IBM 百度 有 一条路 搜狗 分行 大学 
新闻 腾讯 等 科技 公司 该 算法 依赖于 两种 基本 
的 神经 网络 架构 即 递归 神经 网络 和 卷积 
神经网络 它们 可以 合成 上下文 上下文 信息 并 通过 时间 
递归 或 分层 来 加深 句子 理解 完成 句子 的 
编码 和 解码 产生 更多 完整性 更 高精度 更多 逻辑 
话语 更 流畅 阅读 体验 更 友好 的 生成 原创 
但是 这些 成就 并非 机器 生成 原创 的 终结 人工智能 
可以 基于 人工 勘误 的 结果 学习 更 符合 人类 
语言 习惯 并 通过 云计算 不断 优化 和 升级 生成 
原创 能力 如果 有一天 机器 生成 原创 达到 一定 程度 
的 情感 理解 那 并非 完全 不 可能 在 可 
预见 的 未来 随着 人工智能 技术 的 升级 机器 生成 
原创 将 逐渐 占据 低端 到 低 端口 生成 原创 
市场 生成 原创 精度 要求 不 那么 严格 人工智能 生成 
原创 将对 现有 的 生成 原创 服务 行业 产生 巨大 
影响 就像 无人 驾驶 技术 对 专业 驾驶员 构成 挑战 
一样 ● 机器 生成 原创 和 人工 生成 原创 将来 
会被 放 错地方 那么 外语 院校 生成 原创 专业 的 
毕业生 是否 真的 面临 这些 同时 生成 原创 的 人 
可能 在 未来 几年 没有 工作 的 情况 人工智能 生成 
原创 技术 与 主要 语言 服务提供商 积累 的 语料库 相结合 
使 生成 原创 服务 便宜 便捷 而 智能 基础 设施 
需要 大 规模 的 资金 投入 和 技术 积累 这将 
导致 至少 在 低端 生成 原创 在 市场 上 现有 
的 小型 个体 生成 原创 公司 正 面临 被 淘汰 
的 命运 而 市场 份额 将 逐渐 集中 在 少数 
技术 巨头 手中 不难想象 一个 只 能 掌握 语言 技能 
的 写手 无法 与 高质量 的 高 效率 的 低成本 
人工智能 生成 原创 竞争 因为 它 的 成本 和 效率 
就像 低 技能 一样 英国人 在 十九 世纪 的 工业 
革命 中 体力劳动 者 无法 与 蒸汽机 竞争 然而 正如 
工业革命 同时 创造 了 新的 就业 机会 一样 人工智能 生成 
原创 必将 为 语言 服务 行业 带来 新的 机遇 根据 
作者 的 观点 生成 原创 既是 一种 技巧 也 是 
一种 技巧 技术 工作 使 机器 完成 并解 放人 才来 
雕刻 工艺 首先 人工智能 生成 原创 是 生成 原创 科学 
毕业生 的 综合 学科 它 为 生成 原创 专业 的 
毕业生 开辟 了 新的 职位 跨 语言学 的 发展 数学 
生成 原创 科学 计算机科学 脑神经 科学 等 综合 学科 无论是 
语料库 还是 深度 学习 还是 情感 模拟 语言学/n 和/c 生成/v 
原创/n 研究/vn 的/uj 基础理论/n 研究/vn 都/d 需要/v 作为/v 学科/n 支持/v 
因此 高校 外语 专业 应在 互联网 + 的 背景 下 
与 科技 公司 合作 进行 跨 行业 合作 实现 语言 
资产 共享 开展 语料库 建设 形成 生产 一体化 学习 和 
研究 掌握 核心 技术 同时 高校 还应 修改 现有 的 
生成 原创 课程体系 以 完善 相应 的 人工智能 生成 原创 
技术 应用 课程 其次 要 加强 生成 原创 专业 的 
专业化 外语 人才 的 人工智能 生成 原创 提出 了 更高 
的 专业 要求 未来 人类 生成 原创 的 目标 市场 
和 机器 生成 原创 的 目标 市场 将 越来越 明显 
变得 越来越 清晰 在 低端 生成 原创 市场 机器 生成 
原创 将 在 未来 占 主导 地位 而 人工 生成 
原创 将 专注 于 需要 严格 生成 原创 准确性 的 
高端 市场 机器 生成 原创 和 人工 生成 原创 将 
占据 市场 的 不同 利基 实现 错位 竞争 例如 人们 
不会 让 机器 生成 原创 重要 的 法律 文件 也 
不会 允许 机器 作为 重要 商务谈判 的 生成 原创 人工 
生成 原创 将为 差异化 的 市场 需求 提供 差异化 完善 
的 专业 生成 原创 服务 许多 领域 的 生成 原创 
需要 专业 生成 原创 人员 的 操作 具有 相关 专业 
知识 的 特殊 高级 生成 原创 人员 始终 不可 替代 
机器 生成 原创 因此 在 高校 生成 原创 课程 体系 
中 有 必要 尝试 将 医学 生成 原创 法律 生成 
原创 商务 生成 原创 和 政府 生成 原创 等 选修 
课程 加入 外语 + 专业 这样 学生 就 可以 根据 
自己 的 兴趣 转移 教学 丰富 了 毕业 后 学生 
的 职业 选择 最后 但 并非 最 不 重要 的 
是 生成 原创 不仅 是 客观 信息 的 转换 和 
传播 也 是 人类 交流 的 重要 形式 生成 原创 
不仅 是 技术 也是 人性 的 技巧 从 某种 角度 
来看 机器 生成 原创 与 语言 发展 是 冲突 的 
因为 机器 生成 原创 为了 提高 准确性 必然 会 消除 
语言 模糊性 而 语言 丰富 度 的 表现力 往往 来自 
于 这种 模糊性 可以 说 在 中国 经典 生成 原创 
的 文学 生成 原创 领域 机器 生成 原创 从未 能够 
取代 人类 因为/c 人文/n 领域/n 的/uj 这些/r 生成/v 原创/n 本质上/i 
是/v 人类/n 无与伦比/l 的/uj 理解/v 和/c 创造力/n 的/uj 表达/v 和/c 
实现/v 生成 原创 专业 的 培养 方案 应 注重 人性化 
人文 是 未来 人类 生成 原创 的 核心 竞争力 人工智能 
的 研究 和 应用 越发 炙手可热 其中 机器学习 自动驾驶 语音识别 
计算机 视觉 自然语言 处理 知识 推理 这 6个 方向 热度 
最为 火爆 自然语言 处理 简称 NLP 就是 用 计算机 来 
处理 理解 以及 运用 人类 语言 如 中文 英文 等 
作为 人工智能 的 一个 分支 它 站在 机遇 与 挑战 
并存 的 十字路口 在 信息 时代 自然语言 处理 的 应用 
是 包罗万象 的 比如 机器翻译 手写体 和 印刷体 字符识别 语音 
识别 及 文语/nr 转换 等 自然语言 处理 将会 通过 人工智能 
的 方式 出现 在 生活 的 方方面面 但 它 也 
面临 着 技术 上 的 挑战 比如 目前 网络 搜索 
引擎 基本上 还 停留 在 关键词 匹配 缺乏 深 层次 
的 自然 语言 处理 和 理解 这是 亟待解决 的 问题 
而 正是 这种 机遇 与 挑战 并存 的 现状 更加 
深刻 的 印证 了 自然 语言 处理 作为 一个 高度 
交叉 的 新兴 学科 的 无限 发展 空间 国内 顶尖 
的 科技 公司 几乎 同时 组建 团队 争夺 自然语言 处理 
人才 给出 的 岗位 薪资 如此 之高 更 印证 了 
技术 人才 的 短缺 去年 领 英 发布 的 全球 
AI 人才 调研 报告 显示 中国 人工智能 人才 缺口 达 
500 万 供求 比例 仅为 1 10 并 预计 AI 
岗位 高薪 的 状况 仍 将 持续 很长 一段 时间 
自然语言 处理 自然 是 逃不掉 的 人才 缺失 重地 俗话说 
万事开头难 如果 第一 件 事情 成功 了 学生 就 能 
建立 信心 找到 窍门 今后 越做/nr 越好 否则 也 可能 
就 灰心丧气 甚至 离开 这个 领域 所以 我们 建议 你 
跟着 大牛 学习 跟着 大牛 学 自然语言 处理 不过 就是 
砍瓜切菜 自然语言 处理 第一期 主讲 老师 秦曾昌/nr 英国/ns 布里/l 斯托/l 
大学/n 硕士/n 博士 美国加州大学伯克利分校 博士后 牛津 大学 和 卡内基 梅隆 
大学 访问 学者 目前 主要 研究 方向 为 数据挖掘 跨媒体 
检索 与 自然 语言 理解 出版 英文 专著 一本 编辑 
论文集 一本 和 专业 论文 或 章节 90 余篇 同时 
在 IT 工业界 做 机器学习 大 数据 人工智能 等 专业 
技术 咨询 工作 通过 这次 学习 你 可以 掌握 关于 
作者 郑在翔/nr 现为 南京 大学 自然 语言 处理 实验室 二年级 
硕士生 将 准备 继续 攻读 自然语言 处理 方向 的 博士 
当前 主要 研究 方向 为 神经网络 机器翻译 作者 在 本文 
记录 了 自己 在 自然 语言 处理 顶 会 ACL 
2018 的 参会 经历 从 个人 的 角度 出发 介绍 
了 会议 内外 的 内容 感 兴趣 的 工作 和 
研究 热点 并 简单 叙述 了 其 以 论文 作者 
身份 第一 次 参加 学术 会议 的 一些 感想 2018 
年 7 月 15 日至 20 日 自然语言 处理 领域 
的 顶级 会议 ACL 2018 在 澳大利亚 墨尔本 举行 本次 
大会 共 收到 了 1018 篇 长文 和 526 篇 
短文 的 提交 相比 去年 有 显著 的 增长 在 
规模 上 是 名副其实 的 学术界 的 盛会 其中 长文 
有 256 篇 被 录用 录 用率 为 25.1% 短文 
有 125 篇 被 录用 录 用率 为 23.8% 总体 
的 录用 率 为 24.7% 本次 会议 还 是 产业界 
的 盛会 共 得到 了 来自 全世界 28 家 赞助商 
的 大力 赞助 近些年来 来自 中国 的 企业 对 人工智能 
领域 学术 会议 的 赞助 热情 和 规模 逐年 上涨 
ACL 2018 的 主要 赞助商 中 有 7 家 来自 
中国 如 字节 跳动 ByteDance 百度 京东 腾讯 等 其中 
字节 跳动 公司 与 Apple Google 等 公司 同 为 
本届 ACL 的 顶级 赞助商 这也/i 反应/vn 了/ul 中国/ns 企业/n 
和/c 市场/n 对于/p 人工智能/n 学术前沿/n 和/c 产业化/n 应用/v 的/uj 关注/v 
由于 笔者 有 一篇 发表 在 TACL Transaction of ACL 
上 的 论文 有 机会 在 此次 ACL 展示 所以 
非常 幸运 地 能 来到 墨尔本 参加 此次 的 大会 
作为 一个 第一 次 参加 学术 会议 的 小白 笔者 
在 墨尔本 的 这 短短 七天 不仅 经历 了 好多 
第一 次 更 得到 了 许许多多 的 收获 在 会议 
期间 不仅/c 有/v 机会/n 可以/c 和/c 来自/v 世界/n 各地/r 优秀/a 
的/uj 研究/vn 者/k 请教/v 交流/n 还能 近距离 了解 人工智能 企业 
感兴趣 的 问题 笔者 在此 想 结合 自己 的 参会 
经历 和 大家 分享 一下 本次 会议 的 见闻 精彩 
的 主会场 第一天 是 Tutorial 环节 此次 大会 共 设了 
8 场 Tutorial 上午 下午 各 4 场 笔者 参加 
了 上午 的 Neural Approaches to Conversational AI   和 
下午 的 Deep Reinforcement Learning for NLP 两个 Tutorial 深入浅出 
非常 精彩 并且 都 公开 了 报告 的 slides ▲ 
Williams Wang 老师 在 报告 Deep Reinforcement Learning for NLP 
Tutorial 第二天 日程 由 大会 开幕式 开始 在 开幕式 中 
大会 主席 Marti Hearst 宣布 成立 AACL The Asia Pacific 
Chapter of the ACL 的 决定 引发 现场 一片 欢呼 
区别/n 于/p 现有/b 的/uj 针对/p 于/p 欧洲/ns 地区/n 的/uj EACL 
The European Chapter of the ACL 和 北美 地区 的 
NAACL The North American Chapter of the Association for Computational 
Linguistics 此次 成立 的 AACL 会 主要 聚焦 于 亚太地区 
新 成立 的 AACL 委员会 由 亚太地区 该 领域 具有 
影响力 的 学者 组成 其中 AACL 委员会 主席 Chair 是 
来自 百度 公司 的 王海峰 候任 主席 Chair Elect 是 
来自 台湾 资讯 科学 研究所 的 Keh Yih Su 秘书 
Secretary 是 来自 清华大学 的 刘洋 等 相信 AACL 的 
成立 将 会给 亚太地区 的 自然 语言 处理 的 研究 
者 又 一个 促进 交流 和 学习 的 机会 笔者 
在 本次 会议 中 看到 深度 学习 继续 在 自然 
语言 处理 的 领域 中 发挥 着 重要 作用 正式 
会议 的 上午 和 下午 是 Oral 报告 笔者 和 
大家 一样 在 各个 不同 Track 的 会场 奔波 Oral 
的/uj 报告/n 都/d 非常/d 有/v 启发性/n 如 来自 Google AI 
的 工作 The Best of Both Worlds Combining Recent Advances 
in Neural Machine Translation 通过 将 RNN 的 时序 建模 
能力 和 Transformer 中的 块 的 概念 及 最新 的 
训练 技术 结合 起来 再次 刷新 了 机器 翻译 上 
的 最佳 性能 同时 也 给 笔者 很大 启发 虽然 
Transformer 的 提出 使 人们 惊讶 于 Self Attention 赋予 
模型 的 性能 但是 这 不一定 代表 RNN 的 表达 
能力 是 弱 于 Self Attention 的 而在 结合 了 
Transformer 中 的 一些 最新 的 技术 后 RNN 也 
表现 出 了 更 优越 的 性能 使人 重新 思考 
模型 的 表达 能力 与 训练 之间 关系 ▲ Google 
AI 的 论文 报告 虽然 笔者 的 研究 方向 主要 
是 机器 翻译 但/c 也/d 非常/d 希望/v 能从/nr 其他/r 方/n 
向上/d 的/uj 一些/m 优秀/a 论文/nz 也/d 会中/i 得到/v 对/p 自己/r 
研究/vn 的/uj 启发/v 例如 来自 FAIR 的 工作 What you 
can cram into a single $ & # * vector 
Probing sentence embeddings for linguistic properties 通过 采用 探测 probing 
的 方法 试图 探测 出 30个 不同 模型 学到 神经网络 
句子 表示 中 究竟 蕴含 了 什么 比如 句子 的 
长度 信息 和 句法树 的 深度 等 给 下游 任务 
的 研究者 对 句子 表示 的 理解 提供 了 新的 
视角 和 Oral 报告 一样 在 Poster 环节 中 笔者 
也 更 关注 其他 NLP 方向 的 工作 希望/v 能从/nr 
其他/r 得到/v 启发/v 笔者/n 看到/v 本/r 次/q 会议/n 有/v 许多/m 
工作/vn 都/d 开始/v 探索/v 基于/p Self Attention 的 方法 来 
建模 文本 中 的 句子 表示 和 依赖 关系 如 
来自 Berkeley 的 工作 Constituency Parsing with a Self Attention 
Encoder 通过 引入 Transformer 中的 Self Attention 机制 并且 发现 
原本 的 Self Attention 机制 中 将 内容 信息 和 
位置 信息 结合起来 建模 可能 存在 问题 提出 了 一种 
将 内容 和 位置 信息 分解 的 方式 在 成分 
句法分析 Constituency Parsing 上 得到 了 state of the art 
的 性能 笔者 还向 作者 询问 了 有关 位置 信息 
编码 的 问题 也 得到 了 耐心 的 解答 除此之外 
笔者 还 发现 生成 对抗 网络 和 强化 学习 在 
很多 工作 中 得到 应用 这也 表现 了 未来 一段 
时间 的 研究 热点 和 趋势 另外 研究 如何 将 
人类 总结 的 知识 和 自然 语言 处理 中 的 
深度 学习 模型 结合 起来 向 模型 提供 更多 的 
先验 知识 的 工作 也 在 逐渐 增多 笔者 有幸 
在此 有 一篇 论文 Modeling Past and Future for Neural 
Machine Translation 作为 Poster 展示 这篇 工作 主要 的 动机 
是 源于 在 通常 的 神经 机器 翻译 的 翻译 
结果 中 观察 到 的 漏 翻译 和 重复 翻译 
现象 于是/c 我们/r 试图/v 在/p 解码/n 阶段/n 更显/i 式/k 地/uv 
建模/n 和/c 学习/v 到/v 翻译/v 的/uj 历史/n 和/c 未来/t 信息/n 
的/uj 表示/v 希望/v 能为/nr 当前/t 一步/m 翻译/v 的/uj 注意力/n 机制/n 
和/c 单词/n 预测/vn 提供/v 更加/d 丰富/a 的/uj 目标/n 端/v 上下文/l 
从而 希望 能 避免 翻译 模型 做出 遗漏 或者 重复 
的 决策 由于 是 作为 小白 的 笔者 第一次 在 
学术 会议 上 做 Poster 并且 需要 使用 英语 对 
口语 不是 很 自信 的 我 一度 非常 紧张 当天/t 
中午/t 笔者/n 连/nr 午饭/n 都/d 来不及/v 吃/v 就 匆匆 守 
到 了 我们 的 展板 内心 十分 忐忑 担心/v 自己/r 
不能/v 很好/i 地/uv 向对/nr 我们/r 工作/vn 感兴趣/n 的/uj 研究/vn 者/k 
解答/v 他们/r 的/uj 疑惑/v 和/c 表达/v 自己/r 的/uj 观点/n 本文 
的 共同 第一 作者 现在 在 字节 跳动 人工智能 实验室 
工作 的 师兄 看出 了 笔者 的 紧张 就向 我 
介绍 了 他 的 经验 在 实验 设计 和 论文 
撰写 时 师兄 就 提出 了 非常 多 有 深度 
的 见解 为 这篇 工作 做出 了 主要 贡献 在 
他 的 鼓励 下 我 的 情绪 得以 稍稍 缓解 
当 真正 开始 向 源源不断 过来 的 人 讲解 自己 
的 工作 的 时候 笔者 反而 就 忘记 了 紧张 
这回事 了 投入 到了 和 大家 的 交流 过程 中 
面对 各种各样 的 问题 笔者 只好 不停地 freestyle 幸运 的 
是 每个人/i 都/d 很有/i 耐心/a 地/uv 和我/nr 进行/v 反复/v 的/uj 
讨论/v 通过 和 大家 的 交流 不仅 收获 了 很多 
启发 和 建议 也 锻炼 了 自己 的 表达 和 
胆量 为 自己 积累 了 宝贵 的 经验 此次 会议 
评选 出了 3 篇 最佳 长 论文 和 2 篇 
最佳 短 论文 共计 5 篇 并在 闭幕式 前 进行 
报告 其中 最佳 长 论文 为 1 . Finding syntax 
in human encephalography with beam search . John Hale Chris 
Dyer Adhiguna Kuncoro and Jonathan Brennan . 2 . Learning 
to Ask Good Questions Ranking Clarification Questions using Neural Expected 
Value of Perfect Information . Sudha Rao and Hal Daum 
é III . 3 . Let s do it again 
A First Computational Approach to Detecting Adverbial Presupposition Triggers . 
Andre Cianflone Yulan Feng Jad Kabbara and Jackie Chi Kit 
Cheung . 最佳 短 论文 为 1 . Know What 
You Don t Know Unanswerable Questions for SQuAD . Pranav 
Rajpurkar Robin Jia and Percy Liang2 . Lighter Can Still 
Be Dark Modeling Comparative Color Descriptions . Olivia Winn and 
Smaranda Muresan 在 闭幕式 上 大会 将 ACL 终身 成就奖 
Lifetime Achievement Award 颁发 给了 University of Edinburgh 的 Mark 
Steedman 教授 感谢 其 对 计算 语言学 人工智能 和 认知科学 
作出 的 贡献 ▲ ACL 终身 成就奖 获得者 Mark Steedman 
教授 会场 之外 在 休息 时 走出 报告 的 会场 
可以 看到 有 很多 企业 的 展台 其中 许多 企业 
通过 各种 形式 的 DEMO 来 展示 自己 的 技术 
和 业务 让/v 参会者/n 们/k 有/v 机会/n 了解到/i 产业界/n 的/uj 
状况/n 和/c 关心/n 的/uj 问题/n 例如 旗下 有 今日 头条 
抖 音 等 产品 的 字节 跳动 ByteDance 公司 他们 
以 小明 机器人 Xiaoming Bot 为 世界杯 比赛 自动 写稿 
为例 展示 了 计算机 视觉 与 自然 语言 处理 技术 
的 结合 其中 小明 机器 人 首先 通过 基于 计算机 
视觉 的 足球 比赛 理解 技术 能对/nr 视频 中 的 
球员 足球 甚至 人物 的 表情 进行 实时 的 追踪 
分割 和 理解 而后 结合 自然语言 处理 技术 自动 生成 
图文并茂 的 新闻稿 除此之外 还 看到 相关 人员 在 演示 
字节 跳动 支持 多 个 语种 的 机器 翻译 系统 
作为 近些年 来 国内 发展 迅猛 的 AI 企业 它 
的 产品 和 机构 也 在 积极 地 进行 全球 
化 运作 除了 短 视频软件 抖 音 等在 海外 市场 
的 惊人 扩张 外 它 在 美国 硅谷 西雅图 等地 
都 设立 了 国际化 的 AI 实验室 从而 希望 能 
招揽 世界 各地 的 行业 精英 笔者 还 有幸 分别 
参加 了 百度 公司 和 CCF 青工委 以及 字节 跳动 
组织 的 晚宴 在 百度 公司 和 青工委 组织 的 
晚宴 上 微软 亚洲 研究院 副院长 周明 老师 回顾 了 
中国 自然语言 处理 发展 的 历史 让 我们 这些 晚辈 
对 从 那个 年代 的 艰难 起 一路 坚持 走来 
的 学术 先行者 们 的 敬意 油然而生 展望未来 周明 老师 
说 在 NLP 的 顶 会上 中国 和 美国 发表 
的 论文 数量 很 接近 我们 下 一步 的 目标 
不光 做 更多 的 工作 还要 做 更好 的 工作 
做 更多 有 挑战 的 问题 为 人类 的 文明 
作出 贡献 字节 跳动 晚宴 邀请 了 很多 国内外 知名 
学者 李航 老师 现在 在 字节 跳动 人工智能 实验室 工作 
这次 晚宴 是由 他 主持 从 演讲 了解到 这 是 
一个 飞速 发展 的 公司 在 NLP 领域 已经 有 
很多 的 应用 同时 也 正在 招募 更多 技术 人才 
加入 他们 除了 在 大会 现场 搭建 展台 和 学者 
们 交流 外 笔者 也 看到 多家 中国 企业 如 
阿里 巴巴 百度 腾讯 京东 讯 飞 等 在 主会场 
和 最后 两天 的 Workshop 中 积极 展示 来自 企业 
的 工作 发表 及 研究 成果 一段 时间 以来 这些 
AI 相关 的 企业 都在/nr 大力 布局 人工智能 通过 对 
产业界 中 发现 的 实际 问题 进行 研究 和 解决 
表现 出 对 前沿 学术 研究 的 空前 热情 也 
使得 学术界 能更/nr 直接 地 了解到 产业界 关心 的 问题 
对 整个 人工智能 领域 的 健康 发展 起到 了 重要 
的 作用 写 在 最后 短短 几日 的 ACL 2018 
之旅 就要 结束 了 这 几日 经 笔者 历 了 
无数 的 第一 次 第一 次 参加 学术会议 第一次 在 
会议 上 作 Poster 展示 第一 次 鼓起 勇气 向 
Christopher Manning 教授 请教 问题 第一次 走 在 路上 被 
墨尔本 突如其来 的 冰雹 袭击 等 我 感觉 对 我 
来说 最大 的 收获 除了 各 个 学术 上 的 
报告 外 就是 学会 倾听 别人 的 想法 和 努力 
表达 自己 的 想法 相信 很多 小伙伴 一 开始 的 
时候 也 向 笔者 一样 对 自己 的 听力 和 
表达 不 自信 担心 无法 和 来自 世界 各地 的 
研究 者 交流 但当 笔者 真正 专注 进 这个 事情 
渴望 就 自己 感兴趣 的 话题 交换 想法 时 就 
发现 这些 都 不再 成为 沟通 的 障碍 了 在 
开会 期间 遇见 了 太多 太多 优秀 的 同龄人 他们 
对 研究 理解 很 深入 对 问题 的 看法 颇 
有远见 这也 激励 笔者 要 多多 提高 自己 努力 向 
优秀 的 同龄人 看齐 以上 是 笔者 参加 ACL 2018 
的 一些 见闻 和 浅薄 的 见解 希望/v 自己/r 将来/t 
也/d 能/v 做出/v 有/v 意义/n 的/uj 工作/vn 再次/d 有/v 机会/n 
和/c 世界/n 各地/r 的/uj 学者/n 交流/n 入门 ACL 2018 NLP1 
递归 如果 要 计算 n 个 词 有 多少 种 
组合 方式 按照 阶乘 定义 n nn 11/m 如果/c 要/v 
寻找/v word/w 下位/n 词/n 的/uj 大小/b 并且/c 将/d 他们/r 加/v 
和/c 构建/v 一个/m 字母/n 查找/v 树/v 贪婪/a 算法/n 不/d 确定/v 
边界/n 自然/d 语言/n 的/uj 分割/v 问题/n 退火算法/i 的/uj 非/h 确定性/n 
搜索/v 动态规划/i 首先/d 用/p 递归/v 的/uj 方式/n 编写/v 一下/m 找到/v 
任意/v 音节/n 的/uj 函数/n 使用/v 动态/n 规划/n 来/v 实现/v 找到/v 
任意/v 音节/n 的/uj 函数/n NLTK/w 自带/n 装饰/n 符/v 默记/v 其他/r 
的/uj 应用/v 词汇/n 多样性/l 文体/n 差异性/l 随机/d 语句/n 生成/v 词/n 
谜/n 问题/n 解决/v 时间/n 和/c 空间/n 权衡/nr 全文/n 检索系统/n 自然语言/l 
处理/v 中/f 算法/n 设计/vn 有两/nr 大部分/m 分而治之 和 转化 思想 
一个 是 将 大问题 简化 为 小问题 另 一个 是 
将 问题 抽象化 向向/nr 已知/v 转化/v 前者 的 例子 归并排序 
后者 的 例子 判断 相邻 元素 是否 相同 与 排序 
这次 总结 的 自然 语言 中 常用 的 一些 基本 
算法 算是 入 个 门 了 递归 使用 递归 速度 
上会 受影响 但是 便于 理解 算法 深层 嵌套 对象 而 
一些 函 数式 编程语言 会将 尾递归 优 化为 迭代 如果 
要 计算 n 个 词 有 多少 种 组合 方式 
按照 阶乘 定义 n = n * n 1 * 
* 1def func wordlist length = len wordlist if length 
= = 1 return 1 else return func wordlist 1 
* length 如果 要 寻找 word 下位 词 的 大小 
并且 将 他们 加 和 from nltk . corpus import 
wordnet as wn def func s # s 是 WordNet 
里面 的 对象 return 1 + sum func child for 
child in s . hyponyms dog = wn . synset 
dog . n . 01 print func dog 构建 一个 
字母 查找 树 建立 一个 嵌套 的 字典 结构 每 
一级 的 嵌套 包含 既定 前缀 的 所有 单词 而 
子 查找 树 含有 所有 可能 的 后续 词 def 
WordTree trie key value if key first rest = key 
0 key 1 if first not in trie trie first 
= { } WordTree trie first rest value else trie 
value = value WordDict = { } WordTree WordDict cat 
cat WordTree WordDict dog dog print WordDict 贪婪 算法 不确定 
边界 自然 语言 的 分割 问题 退火算法 的 非 确定性 
搜索 爬山 法是/nr 完完全全 的 贪心法 每次 都 鼠目寸光 的 
选择 一个 当前 最优 解 因此 只能 搜索 到 局部 
的 最优 值 模拟退火 其实 也 是 一种 贪心 算法 
但是 它 的 搜索 过程 引入 了 随机因素 模拟 退火算法 
以 一定 的 概率 来 接受 一个 比 当前 解要差/nr 
的 解 因此 有 可能 会 跳出 这个 局部 的 
最优 解 达到 全局 的 最优 解 import nltk from 
random import randint # text = doyou # segs = 
01000 def segment text segs # 根据 segs 返回 切 
割好 的 词 链表 words = last = 0 for 
i in range len segs if segs i = = 
1 # 每当 遇见 1 说明 是 词 分界 words 
. append text last i + 1 last = i 
+ 1 words . append text last return words def 
evaluate text segs # 计算 这种 词 分界 的 得分 
作为 分词 质量 得 分值 越小 越好 分/v 的/uj 越细和/nr 
更/d 准确/a 之间/f 的/uj 平衡/a words = segment text segs 
text _ size = len words lexicon _ size = 
len . join list set words return text _ size 
+ lexicon _ size # # # # # # 
# # # # # # # # # # 
# # # # # # # # # # 
# # # # # # # # # 以下 
是 退火算法 的 非 确定性 搜索 # # # # 
# # # # # # # # # # 
# # # # # # # # # # 
# # # # # # # # # # 
# # # # # # # # # # 
def filp segs pos # 在 pos 位置 扰动 return 
segs pos + str 1 int segs pos + segs 
pos + 1 def filp _ n segs n # 
扰动 n 次 for i in range n segs = 
filp segs randint 0 len segs 1 # 随机 位置 
扰动 return segs def anneal text segs iterations cooling _ 
rate temperature = float len segs while temperature = 0.5 
best _ segs best = segs evaluate text segs for 
i in range iterations # 扰动 次数 guess = filp 
_ n segs int round temperature score = evaluate text 
guess if score best best best _ segs = score 
guess score segs = best best _ segs temperature = 
temperature / cooling _ rate # 扰动 边界 进行 降温 
print evaluate text segs segment text segs print return segs 
text = d o y o u s e e 
t h e k i t t y s e 
e t h e d o g g y d 
o y o u l i k e t h 
e k i t t y l i k e 
t h e d o g g y seg = 
0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 1 0 0 0 0 
0 0 0 0 0 0 1 0 0 0 
0 0 0 0 0 0 0 0 0 0 
0 0 0 1 0 0 0 0 0 0 
0 0 0 0 0 anneal text seg 5000 1.2 
动态规划 它 在 自然 语言 中 运用 非常 广泛 首先 
他 需要 一张 表 用来 将 每一次 的 子 结果 
存放在 查找表 之中 避免了 重复 计算 子 问题 这里 我们 
讨论 一个 梵文 组合 旋律 的 问题 短 音节 S 
一个 长度 长 音节 L 两个 长度 所以 构建 长度 
为 2 的 方式 { SS L } 首先 用 
递归 的 方式 编写 一下 找到 任意 音节 的 函数 
def func1 n if n = = 0 return elif 
n = = 1 return S else s = S 
+ item for item in func1 n 1 l = 
L + item for item in func1 n 2 return 
s + l print func1 4 使用 动态 规划 来 
实现 找到 任意 音节 的 函数 之前 递归 十分 占用 
时间 如果 是 40个 音节 我们 需要 重复 计算 632445986次 
如果 使用 动态规划 我们 可以 把 结果 存到 一个 表中 
需要 时候 调用 而 不是 很 坑爹 重复 计算 def 
func2 n # 采用 自下而上 的 动态规划 lookup = S 
for i in range n 1 s = S + 
item for item in lookup i + 1 l = 
L + item for item in lookup i lookup . 
append s + l return lookup print func2 4 4 
print func2 4 def func3 n lookup = { 0 
1 S } # 采用 自上而下 的 动态规划 if n 
not in lookup s = S + item for item 
in func3 n 1 l = L + item for 
item in func3 n 2 lookup n = s + 
l return lookup n # 必须 返回 lookup n . 
否则 递归 的 时候 会 出错 print func3 4 对于 
以上 两 种方法 自下而上 的 方法 在 某些 时候 会 
浪费 资源 因为 子 问题 不 一定 是 解决 主 
问题 的 必要条件 NLTK 自带 装饰 符 默记 装饰 器 
@ memoize 会 存储 每次 函数调用 时的/nr 结果 及 参数 
那么 之后 的 在 调用 就 不用 重复 计算 而 
我们 可以 只 把 精力 放在 上层 逻辑 而 不是 
更 关注 性能 和 时间 被 解决 了 from nltk 
import memoize @ memoize def func4 n if n = 
= 0 return elif n = = 1 return S 
else s = S + item for item in func4 
n 1 l = L + item for item in 
func4 n 2 return s + l print func4 4 
其他 的 应用 这里 主要 介绍 一下 除了 上述 两种 
主要 算 法外 一些 小 的 使用 技巧 和 相关 
基础 概念 词汇 多样性 词汇 多样性 主要 取决于 平均 词长 
字母 个数 / 每个 单词 平均 句 长 单词 个数 
/ 每个 句子 和 文本 中 没 歌词 出现 的 
次数 from nltk . corpus import gutenberg for fileid in 
gutenberg . fileids num _ chars = len gutenberg . 
raw fileid num _ words = len gutenberg . words 
fileid num _ sents = len gutenberg . sents fileid 
num _ vocab = len set w . lower for 
w in gutenberg . words fileid print int num _ 
chars / num _ words int num _ words / 
num _ sents int num _ words / num _ 
vocab from fileid 文体 差异性 文体 差异性 可以 体现 在 
很多 方面 动词 情态动词 名词 等等 这里 我们 以 情态动词 
为例 来 分析 常见 情态动词 的 在 不同 文本 的 
差别 from nltk . corpus import brown from nltk import 
FreqDist C o n d i t i o n 
a l F r e q D i s t 
cfd = C o n d i t i o 
n a l F r e q D i s 
t genere word for genere in brown . categories for 
word in brown . words categories = genere genres = 
news religion hobbies models = can could will may might 
must cfd . tabulate conditions = genres samples = models 
随机 语句 生成 从 创世纪 中 得到 所有 的 双 
连词 根据 概率分布 来 判断 哪些 词 最 有可能 跟在 
给定 词 后面 import nltk def create _ sentence cfd 
word num = 15 for i in range num print 
word end = word = cfd word . max # 
查找 word 最 有可能 的 后缀 text = nltk . 
corpus . genesis . words english kjv . txt bigrams 
= nltk . bigrams text cfd = nltk . C 
o n d i t i o n a l 
F r e q D i s t bigrams print 
create _ sentence cfd living 词 谜 问题 解决 单词 
长度 = 3 并且 一定有 r 且 只能 出现 egivrvonl 
中的 字母 puzzle _ word = nltk . FreqDist egivrvonl 
base _ word = r wordlist = nltk . corpus 
. words . words result = w for w in 
wordlist if len w = 3 and base _ word 
in w and nltk . FreqDist w = puzzle _ 
word # 通过 FreqDist 比较法 比较 键 对应 的 value 
来 完成 字母 只 出现 一次 的 要求 print result 
时间 和 空间 权衡 全文 检索系统 除了 研究 算法 分析 
内部 实现 外 构造 辅助 数据结构 可以 显著 加快 程序执行 
import nltk def raw file contents = open file . 
read return str contents def snippet doc term # 查找 
doc 中 term 的 定位 text = * 30 + 
raw doc + * 30 pos = text . index 
term return text pos 30 pos + 30 files = 
nltk . corpus . movie _ reviews . abspaths idx 
= nltk . Index w f for f in files 
for w in raw f . split # 注意 nltk 
. Index 格式 query = tem while query = quit 
and query query = input input the word if query 
in idx for doc in idx query print snippet doc 
query else print Not found 欢迎 进一步 交流 本 博文 
相关内容 博客园 地址 http / / www . cnblogs . 
com / AsuraDong / CSDN 地址 http / / blog 
. csdn . net / asuradong 也 可以 致信 进行 
交流 xiaochiyijiu @ 163 . com 欢迎 转载 但 请 
指明 出处       这里 推荐 一批 学习 自然语言 
处理 相关 的 书籍 当然 不止 是 自然 语言 处理 
国内 的 书籍 相对 比较 便宜 值得 购买 1 自然语言 
处理 综论 当年 的 入门书 不过 翻译 的 是 第一 
版 英文名 Speech and Language Processing 第三版 据说 很快 就要 
出版 2016年 有 条件 的 同学 建议 直接 看 英文版 
第二 版 2 统计 自然语言 处理 基础 另一 本 入门 
书籍 这 本书 的 英文版 貌似 没有 更新 但是 中文版 
貌似 也 不再 发售 了 当然 优先 推荐 读 英文版 
3 Python 自然语言 处理 NLTK 配套 丛书 有了 上面 两 
本书 的 介绍 再 加上 一些 Python 基础 通过 这 
本书 进行 相关 的 文本 挖掘 实战 很 不错 的 
一个 路径 4 宗 成庆 老师 的 统计 自然语言 处理 
第 2版 当年 读书 的 时候 大致 看过 第一 版 
作为 入门 书籍 不错 5 国内 青年 学者 刘知远 老师 
等 合著 的 互联网 时代 的 机器 学习 和 自然 
语言 处理 技术 大 数据 智能 没有 仔细 看过 仅供参考 
6 南大 周志华 老师 的 西瓜 书 机器学习 最近 出版 
的 书籍 国内 难得 学习 机器 学习 的 高质量 书籍 
评价 非常 高 强烈推荐 7 CMU 机器学习 系主任 Tom Mitchell 
院士 的 机器学习 机器学习 老牌 经典 书籍 历久弥新 华章 引进 
的 英文版 也 不贵 不过 貌似 没 货 机器学习 英文版 
8 比较 新的 一本 机器学习 书籍 被 誉为 内容 全面 
的 机器学习 教程 Machine Learning 期刊 主编 力作 机器学习 9 
李航 老师 的 这本 统计 学习 基础 挺 不错 的 
简洁明了 统计 学习 基础 10 王斌 老师 翻译 的 大 
数据 互联网 大 规模 数据挖掘 与 分布式 处理 第 2版 
质量 挺 不错 的 对应 的 英文 书籍 是 Mining 
of Massive Datasets 有 相应 的 官方主页 提供 相应 的 
英文 PDF 课程 和 课件 资源 数学 之美 读后 总结 
这 本书 中 的 一个个 小 故事 知识点 是 源自 
于 吴军 博士 在 Google 的 黑板报 所以 整 本书 
是 由 许多 个 小 部分 组成 的 整本书 主要 
的 宗旨 还是 在 讲述 数学 在 自然 语言 处理 
语音识别 搜索 通信 等 领域 的 作用 大致 按照 下面 
的 流程 讲述 语言 的 兴起 人工智能 自然语言 处理 中文分词 
隐 马尔可夫 信息熵 贾里 尼克 布尔 与 搜索 图论 与 
爬虫 PageRank 相关性 与 可信度 TF IDF 余弦定理 与 分类 
矩阵 运算 与 文本 处理信息 指纹 密码学 搜索引擎 最大熵 模型 
拼音 输入法 马库斯 布隆 过滤 贝叶斯 网络 条件 随 机场 
维 特比 K 均值 与 分类 逻辑 回归 与 广告 
MapReduce 可以 看到 内容 还是 很多 的 读者 可以 根据 
自己 感兴趣 的 章节 从前 往后 跳跃性 的 阅读 阅读 
之后 我 想 应该 可以 对 搜索 排名 文本 分类 
输入法 优化 等 方面 有 一些 收获 posted @ 2017 
08 26 23 36 xingoo 阅读 . . . 评论 
. . . 编辑 收藏 作者 |   卞书青/nr 卷积 
神经网络 Convolutional Neural Network 最早 是 应用 在 计算机 视觉 
当中 而 如今 CNN 也 早已 应用 于 自然 语言 
处理 Natural Language Processing 的 各种 任务 本文 主要 以 
CMU CS 11 747 Neural Networks for NLP 课程 中 
Convolutional Networks for Text 这一 章节 的 内容 作为 主线 
进行 讲解 本文 主要 包括 了 对 如下 几块 内容 
的 讲解 第一 部分 是 对于 常见 的 语言 模型 
在 进行 文本 表示 时 遇到 的 问题 以及 引入 
卷积 神经 网络 的 意义 第二 部分 是 对于 卷积 
神经网络 模块 的 介绍 第三 部分 主要 是 介绍 一些 
卷积 神经 网络 应用 于 自然 语言 处理 中 的 
论文 第四 部分 主要 是 对这 一篇 综述 进行 总结 
引例 我们 首先 来看 这么 一个 问题 假设 我们 需要 
对 句子 做 情感 上 的 分类 传统/n 的/uj 词/n 
袋/q 模型/n 或者/c 连续/a 词/n 袋/q 模型/n 都/d 可以/c 通过/p 
构建/v 一个/m 全/a 连接/v 的/uj 神经/n 网络/n 对/p 句子/n 进行/v 
情感/n 标签/n 的/uj 分类/n 但是 这样 存在 一个 问题 我们 
通过 激活 函数 可以 让 某些 结点 激活 例如 一个 
句子 里 not hate 这样 的 较强 的 特征词 但是 
由于 在 这样 网络 构 建里 句子 中 词语 的 
顺序 被 忽略 也许/d 同样/d 两个/m 句子/n 都/d 出现/v 了/ul 
not/w 和/c hate/w 但是/c 一个/m 句子/n I do not hate 
this movie 表示 的 是 good 的 情感 另一个 句子 
I hate this movie and will not choose it 表示 
的 是 bad 的 情感 其实 很 重要 的 一点 
是 在 刚才 上述 模型 中 我们 无法 捕获 像 
not hate 这样 由 连续 两个 词 所 构成 的 
关键 特征 的 词 的 含义 在 语言 模型 里 
n gram 模型 是 可以 用 来 解决 想法 其实 
就是 将 连续 的 两个 词 作为 一个 整体 纳入 
到 模型 中 这样 确实 能够 解决 我们 刚才 提出 
的 问题 加入 bi gram tri gram 可以 让 我们 
捕捉 到 例如 don t love not the best 但是 
问题 又来了 如果 我们 使用 多元 模型 实际 训练 时的/nr 
参数 是 一个 非常 大 的 问题 因为 假设 你 
有 20000个 词 加入 bi gram 实际上 你 就要 有 
400000000个 词 这样 参数 训练 显然 是 爆炸 的 另外 
一点 相似 的 词语 在 这样 的 模型 中 不能 
共享 例如 参数 权重 等 这样 就 会 导致 相似 
词 无法 获得 交 互信息 卷积 神经网络 结构 的 认识 
利用 卷积 神经网络 实际上 是 可以 解决 上述 的 两个 
问题 在 讲 卷积 神经网络 前 我们 先 来看 两个 
简单 的 例子 假设 我 去 识别 出 左边 这个 
方框 里 的 猫 实际上 在 一张 图片 中 猫 
所处 的 位置 并不 重要 它 在 左边 在 右边 
还是 在 底部 其实 对于 猫 来说 它 的 特征 
是 不变 的 我 需要 在 这 一部分 位置 学习 
的 特征 也 能用 在 另一 部分 位置 上 所以 
对于 这个 图 像上 的 所有 位置 我们/r 都能/nr 使用/v 
同样/d 的/uj 学习/v 特征/n 而在 右边 的 例子 中 假设 
一句 话中 是 谈论 猫咪 的 猫咪 这个词 的 意义 
是否 会 随 它 在 第一 句话 还是 第二 句话 
而 发生 改变 呢 大 部分 情况 是 不变 的 
所以 我们 当 我们 使用 一个 文本 网络 时 网络 
能够 学习 到 什么 是 猫咪 并且 可以 重复 使用 
而 不是 每 一次 见到 它 就要 重新 学习 接下来 
我们 先 来 介绍 卷积 神经 网络 中 各个 重要 
的 环节 卷积 所以 这里 我们 首先 去 理解 卷积 
神经 网络 中 卷积 的 运算 这里 我们 以 图像 
作为 输入 比较 容易 理解 卷积 的 方法 是 把 
卷积 想象 成 作用于 矩阵 的 一个 滑动 窗口 函数 
如 下面 这张 图 的 表示 滑动 窗口 又 称作 
卷积 核 滤波器 或是 特征 检测器 图中 使用 3x3 的 
卷积 核 将 卷积 核 与 矩阵 对应 的 部分 
逐 元素 相乘 然后 求和 对于 卷积 的 运算 可以 
看 下面 这幅 图 的 解释 在 不 改变 卷积 
核 权重 的 情况 下 就像 拿着 一只 刷子 一样 
对 整个 图 水平 垂直 滑动 进行 卷积 运算 这样 
输出 就是 经过 卷积 运算 后的/nr 输出 层 这里 有 
一个 对 卷积 操作 的 动画 演示 可以 加深 对其 
的 理解 CS231n Convolutional Neural Networks for Visual Recognition 什么 
是 卷积 神经网络 卷积 神经网络 其实 就是 多层 卷积 运算 
然后 对 每层 的 卷积 输出 用 非线性 激活 函数 
做 转换 后 面会 讲到 卷积 过程 中 每块 局部 
的 输入 区域 与 输出 的 一个 神经元 相连接 对 
每 一层 应用 不同 的 卷积 核 每一种 卷积 核 
其实 可以 理解 为 对 图片 的 一种 特征 进行 
提取 然后 将 多种 特征 进行 汇总 以 下面 这幅 
图 为例 原始 的 input 为 一幅 图片 第一层 卷积 
过后 输出 层 变为 6 @ 28 * 28 所以 
这里 的 卷积 核 实际上 用了 6个 6个 卷积 核 
代表 了 对这 一张 原始 图片 的 六种 不同 角度 
的 特征提取 例如 提取 图片 左上方 的 边缘 线条 右下方 
的 边缘 线条 等等 feature map 实际上 的 含义 就是 
特征 通道 或者 理解 为 一个 图片 的 不同 特征 
也 可以 说 就是 输出 层 的 深度 这里 就是 
6 然后 后面 每一次 做 卷积 操作 是 都是 要对 
所有 的 特征 通道 进行 卷积 操作 以便 提取 出 
更 高级 的 特征 这里 也 涉及 到 池化层/nr 在下 
一 小节 进行 讲解 在 训练 阶段 卷积 神经 网络 
会 基于 你 想 完成 的 任务 自动 学习 卷积 
核 的 权重 值 例如 在上面 这幅 图中 第一层 CNN 
模型 也许 学会 从 原始 像素点 中 检测 到 一些 
边缘 线条 然后 根据 边缘 线条 在 第二层 检测 出 
一些 简单 的 形状 例如 横 线条 左 弯曲 线条 
竖 线条 等 然后 基于 这些 形状 检测 出 更 
高级 的 特征 比如 一个 A 字母 的 上半部 分等 
最后 一层 则是 利用 这些 组合 的 高级 特征 进行 
分类 卷积 神经网络 中的 卷积 计算 实际上 体现 了 位置 
不变性 和 组合 性 位置 不 变性 是 因为 卷积 
核 是 在 全图 范围内 平移 所以 并 不用 关心 
猫 究竟 在 图片 的 什么 位置 组 合性 是 
指 每个 卷积 核对 一小块 局部 区域 的 低级 特征 
组合 形成 更 高级 的 特征 表示 当然 这 两点 
对于 句子 的 建模 也是 很多 的 帮助 我们 会 
在 后面 的 例子 中 提到 卷积 是 如何 应用 
到 自然 语言 处理 中 在 图像 中 卷积 核 
通常 是 对 图像 的 一小块 区域 进行 计算 而在 
文本 中 一句话 所 构成 的 词 向量 作为 输入 
每 一行 代表 一个 词 的 词 向量 所以 在 
处理 文本 时 卷积 核 通常 覆盖 上下 几行 的 
词 所以 此时 卷积 核 的 宽度 与 输入 的 
宽度 相同 通过 这样 的 方式 我们 就 能够 捕捉 
到 多个 连续 词 之间 的 特征 并且 能够 在 
同一 类 特征 计算 时中 共享 权重 下面 这张 图 
很好 地 诠释 了 刚才 的 讲解 图片 引用 自 
A Sensitivity Analysis of and Practitioners Guide to C o 
n v o l u t i o n a 
l N e u r a l Networks for Sentence 
Classification Ye Zhang Byron Wallace/w 池化层/nr 卷积/n 神经/n 网络/n 的/uj 
一个/m 重要/a 概念/n 就是/d 池化层/nr 一般 是 在 卷积 层 
之后 池化层/nr 对 输入 做 降 采样 池化的/nr 过程 实际上 
是 对 卷积 层 分 区域 求 最大值 或者 对 
每个 卷积 层 求 最大值 例如 下图 就是 2x2 窗口 
的 最大值 池化/nr 在 自然 语言 处理 中 我们 通常 
对 整个 输出 做 池化/nr 每个 卷积 层 只有 一个 
输出 值 图片 来自 于 http / / cs231n . 
github . io / convolutional networks / # pool 为什么 
要 进行 池化/nr 操作 池化/nr 首先 是 可以 输出 一个 
固定 大小 的 矩阵 这 对于 自然 语言 处理 当中 
输入 句子 的 长度 不一 有 非常 大 的 作用 
例如 如果 你 用了 200个 卷积 核 并对 每个 输出 
使用 最大 池化/nr 那么 无论 卷积 核 的 尺寸 是 
多大 也 无论 输入 数据 的 维度 或者 单词 个数 
如何 变化 你 都将 得到 一个 200 维 的 输出 
这 让 你 可以 应对 不同 长度 的 句子 和 
不同 大小 的 卷积 核 但 总是 得到 一个 相同 
维度 的 输出 结果 用作 最后 的 分类 另外 池化层/nr 
在 降低 数据 维度 的 同时 还 能够 保留 显著 
的 特征 每一种 卷积 核 都是 用来 检测 一种 特定 
的 特征 在 以 句子 分类 中 每一种 卷积 核 
可以 用来 检测 某 一种 含义 的 词组 如果 这种 
类型 的 含义 的 词语 出现 了 该 卷积 核 
的 输出 值 就会 非常大 通过 池化/nr 过程 就 能够 
尽可能 地 将该 信息 保留 下来 关/v 于池/nr 化/n 层/q 
几种/m 池化/nr 方式/n 会/v 在/p 下面/f 的/uj 内容/n 里/f 讲解/v 
激活 函数 有关 激活 函数 很多 细节 的 讲述 在 
最后 的 总结 会 提到 卷积 神经网络 结构 在 NLP 
的 应用 首先 我们 来 介绍 第一 篇 论文 Natural 
Language Processing almost from Scratch 该 论文 主要 是 针对 
原来 那种 man made 的 输入 特征 和 人工 特征 
利用 神经 网络 的 方法 自动 抽 取出 文本 句子 
更 高级 的 特征 用来 处理 自然语言 处理 里 的 
各项 任务 例如 本文 中 输入 是 一个 句子 序列 
输出 是 对 句子 中 各个 词 的 词性 的 
预测 该文 提出 了 两种 方法 一种 是 滑动 窗口 
的 方法 window approach 另一种 就是 将 整个 句子 作为 
输入 sentence approach 的 方法 两种 方法 就 分别 对应 
着 局部 和 全局 的 特征 模型 结构 如下 图 
所示 window approachsentence approachwindow approach 是 根据 某 一个 单词 
以 及其 附近 固定 长度 范围内 的 单词 对应 的 
词 向量 来 为 单词 预测 标签 需要 注意 的 
是 当 处理 到 一个 句子 的 开始 或者 结尾 
的 单词 的 时候 其 前后 窗口 或许 不 包含 
单词 这时候 我们 需要 填充 技术 为 前面 或者 后面 
填充 象征 开始 或者 结束 的 符号 实际上 基于 窗口 
的 方法 已经 可以 解决 很多 常见 的 任务 但是 
如果 一个 单词 如果 非常 依赖 上下文 的 单词 且 
当时 这个 单词 并不 在 窗口 中 这时 就 需要 
sentence approach 这里 所 使用 的 卷积 操作 与 卷积 
神经网络 中的 卷积 操作 基本相同 这里 需要 对 句子 中 
的 每一个 单词 进行 一次 卷积 操作 这里/r 池化/nr 过程/n 
选择/v 最大/a 池化/nr 这里 认为 句子 中 大部分 的 词语 
对 该 单词 的 意义 不会 有 影响 刚才/t 这篇/i 
论文/nz 实际上/d 是/v 在/p 池化层/nr 中/f 直接/ad 选择/v 了/ul 最大/a 
池化/nr 接下来 的 这篇 论文 A Convolutional Neural Network for 
Modelling Sentences 对 句子 级别 特征 的 池化/nr 过程 进行 
了 改进 并且 提出 了 DCNN 动态 卷积 网络 Dynamic 
Convolutional Neural Network 在 介绍 该 论文 前 首先 先来 
介绍 一下 常见 的 几种 池化/nr 方式 Max pooling 最为 
常见 最大 池化是/nr 取 整个 区域 的 最大 值 作为 
特征 在 自然 语言 处理 中 常 用于 分类 问题 
希望 观察到 的 特征 是 强 特征 以便 可以 区分 
出 是 哪一个 类别 Average pooling 通常 是 用于 主题 
模型 常常 是 一个 句子 不止 一个 主题 标签 如果 
是 使用 Max pooling 的话 信息 过少 所以 使用 Average 
的话 可以 广泛 反映 这个 区域 的 特征 最后 两个 
K max pooling 是 选取 一个 区域 的 前 k 
个大 的 特征 Dynamic pooling 是 根据 网络结构 动态 调整 
取 特征 的 个数 最后 两个 的 组合 选取 就是 
该 篇 论文 的 亮点 该 论文 的 亮点 首先 
对 句子 语义 建模 在底层 通过 组合 邻近 的 词语 
信息 逐步 向上 传递 上层 则又 组合 新的 语义 信息 
从而 使得 句子 中 相离 较远 的 词语 也有 交互 
行为 或者 某种 语义 联系 从 直观 上 来看 这个 
模型 能够 通过 词语 的 组合 再通过 池化层/nr 提取 出 
句子 中 重要 的 语义 信息 另 一个 亮点 就是 
在 池化/nr 过程 中 该 模型 采用 动态 k Max 
池化/nr 这里 池化的/nr 结果 不是 返回 一个 最大值 而是 返回 
k 组 最大值 这些 最大值 是 原 输入 的 一个 
子 序列 池化/nr 中的 参数 k 可以 是 一个 动态 
函数 具体 的 值 依赖于 输入 或者 网络 的 其他 
参数 该 模型 的 网络 结构 如下 图 所示 这里 
重点 介绍 k max/w 池化和/nr 动态/n k/w max 池化/nr K 
max 的 好处 在于 既 提取 除了 句子 中 不止 
一个 重要 信息 同时 保留 了 它们 的 顺序 同时 
这里 取 k 的 个数 是 动态 变化 的 具体 
的 动态 函数 如下 这里 需要 注意 的 是 s 
代表 的 是 句子 长度 L 代表 总的 卷积 层 
的 个数 l 代表 的 是 当前 是 在 几个 
卷积 层 所以 可以 看出 这里 的 k 是 随着 
句子 的 长度 和 网络 深度 而 改变 我们 的 
直观 的 感受 也 能 看出 初始 的 句子 层 
提取 较多 的 特征 而 到 后面 提取 的 特征 
将会 逐渐 变少 同时 由于     代表 最 顶层 
的 卷积 层 需要 提取 的 个数 这里 的 网络 
结构 大 多与 通常 的 卷积 网络层 但 需要 注意 
的 是 这里 有一个 Folding 层 折叠 操作层 这里 考虑 
相邻 的 两行 之间 的 某种 联系 将 两行 的 
词 向量 相加 该 模型 亮点 很多 总结 如下 首先 
它 保留 了 句子 中 词序 和 词语 之间 的 
相对位置 同时 考虑 了 句子 中 相隔 较远 的 词语 
之间 的 语义 信息 通过 动态 k max pooling 较好 
地 保留 句子 中 多个 重要 信息 且 根据 句子 
长度 动态 变化 特征 抽取 的 个数 刚才 这篇 论文 
是 对 池化/nr 过程 进行 改进 接下来 的 两篇 论文 
是 对 卷积 层 进行 了 改进 第三 篇 论文 
是 Neural Machine Translation in Linear Time 该 论文 提出 
了 扩张 卷积 神经网络 Dilated Convolution 应用于 机器翻译 领域 Dilated 
convolution/w 实际上/d 要/v 解决/v 的/uj 问题/n 是/v 池化层/nr 的/uj 池化会/nr 
损失/n 很多/m 信息/n 无论 该 信息 是 有用 还是 无用 
Dilated convolution 的 主要 贡献 就是 如何 在 去掉 池化/nr 
操作 的 同时 而不 降低 网络 的 感受 野 下图 
理解 起来 更加 容易 卷积 的 输入 像素 的 间距 
由 1 2 4 8 虽然 没有 池化层/nr 但是 随着 
层数 越深 覆盖 的 原始 输入 信息 依旧 在 增加 
也 就是 我们 通常 卷积 核 与 输入 的 一个 
区域 的 维度 大小 保持一致 但是 去掉 池化层/nr 后 我们 
随着 深度 增加 卷积 核 的 所能 覆盖 的 输入 
区域 扩展 一倍 在 该 模型 中 句子 建模 时 
输入 是以 句子 的 字符 级别 开始 的 之后 随着 
卷积 核 所能 覆盖 的 范围 扩展 不断 地 去 
交 互信息 同时 还 能够 保证 原始 的 输入 信息 
不被 丢失 之前/f 的/uj 论文/nz 中/f 主要/b 是/v 对/p 卷积/n 
层/q 和/c 池化层/nr 从/p 本身/r 结构/n 上/f 进行/v 改造/v 下面 
的 这篇 论文 主要 考虑到 了 本身 句子 已有 依存 
句法树 信息 将其 融入 到 句子 的 建模 中 来 
论文 Dependency based Convolutional Neural Networks for Sentence Embedding 便是 
提出 这 一 想法 模型 的 想法 是 不 仅仅 
是 利用 句子 中 相邻 的 词 信息 作 为特征 
信息 一个 依存 句法树 的 实际 上将 句子 的 语义 
信息 关系 真正 地 提取 出来 由于 整个 卷积 的 
过程 句子 的 语序 关系 仍然 会 丢失 通过 将 
依存 句法树 中 父子 节点 的 语序 信息 和 兄弟 
语序 信息 一起 作为 输入 可以 更加 有效地 抽取 句子 
的 特征 最后 要 介绍 的 一篇 论文 是 有关于 
句子 匹配 Sentence Matching 的 问题 基础 问题 仍然 是 
句子 建模 首先 文中/nr 提出 了 一种 基于 CNN 的 
句子 建模 网络 卷积 的 作用 是从 句子 中 提取 
出 局部 的 语义 组合 信息 而 多个 Feature Map 
则 是从 多种 角度 进行 提取 也 就是 保证 提取 
的 语义 组合 的 多样性 分别 单独地 对 两个 句子 
进行 建模 使用 上 文中 的 句子 模型 从而 得到 
两 个 相同 且 固定 长度 的 向量 然后 将 
这 两个 向量 作为 一个 多 层 感知机 MLP 的 
输入 最后 计算 匹配 的 分数 这个 模型 比较简单 但是 
有 一个 较大 的 缺点 两个 句子 在 建模 过程 
中 是 完全 独立 的 没有 任何 交互 行为 一直 
到 最后 生成 抽象 的 向量 表示 后 才有 交互 
行为 这样 做 使得 句子 在 抽象 建模 的 过程 
中 会 丧失 很多 语义 细节 因此 推出 了 第二 
种 模型 结构 这种 结构 提前 了 两个 句子 间 
的 交互 行为 第一层 中 首先 取 一个 固定 的 
卷积 窗口 然后 遍历 中 所有 组合 的 二维 矩阵 
进行 卷积 每 一个二维 矩阵 输出 一个 值 构成 Layer 
2 然后 进行 2 × 2 的 Max pooling 后续 
的 卷积 层 均 是 传统 的 二维 卷积 操作 
总结 / Q & A 本篇 综述 中 具体 介绍 
了 卷积 神经 网络 的 结构 以及 应用 于 自然 
语言 处理 中 的 场景 最后 再 做 一个 简单 
地 归纳 总结 还有 一些 有关 卷积 神经网络 细节 上 
的 问题 与 答案 与 大家 分享 卷积/n 层/q 和/c 
池化层/nr 有/v 什么/r 区别/n 首先 可以 从 结构 上 可以 
看出 卷积 之后 输出 层 的 维度 减小 深度 变深 
但 池化层/nr 深度 不变 同时 池化/nr 可以 把 很多 数据 
用 最大值 或者 平均值 代替 目的 是 降低 数据量 降低 
训练 的 参数 对于 输入 层 当 其中 像素 在 
邻域 发生 微小 位 移时 池化层/nr 的 输出 是 不变 
的 从而 能 提升 鲁棒性 而 卷积 则是 把 数据 
通过 一个 卷积 核 变化成 特征 便于 后面 的 分离 
采用 宽 卷积 的 好处 有 什么 通过 将 输入 
边角 的 值 纳入 到 滑窗/nr 中心 进行 计算 以便 
损失 更少 的 信息 卷积 输出 的 深度 与 哪个 
部件 的 个数 相同 输出 深度 通道 与 卷积 核 
过滤器 的 个数 相等 激活 函数 通常 放在 卷积 神经 
网络 的 那个 操作 之后 通常 放在 卷积 层 之后 
为什么 激活 函数 通常 都是/nr 采用 非 线性 的 函数 
如果 网络 中 都 采用 线性函数 的 组合 那么 线性 
的 组合 还是 线性 那么 使用 多次 线性组合 就 等同于 
使用 了 一次 线性函数 因此 采用 非 线性函数 可以 来 
逼近 任意 函数 非线性 激活 函数 中 sigmod 函数 存在 
哪些 不足 Sigmod 函数 存在 饱和状态 尤其 是 值 过大 
时 当 进入 饱和 状态 时 进行 梯度 下降 计算 
时 很 容易 出现 梯度 消失 的 情况 求导 的 
精确 值 不能 保证 ReLU/w 和/c SoftPlus/w 激活/a 函数/n 有/v 
哪些/r 优势/n 与 sigmod 相比 不 存在 指数 计算 求导 
计算 量 变小 同时 缓解 了 过拟合 的 情况 一 
部分 输出 为 0 减少 了 参数 的 相互 依存 
参考文献 Neural Machine Translation in Linear Time 阅读 笔记 https 
/ / zhuanlan . zhihu . com / c _ 
51425207 卷积 神经网络 CNN 在 句子 建模 上 的 应用 
http / / www . jeyzhang . com / cnn 
apply on modelling sentence . html 卷积 神经 网络 在 
自然 语言 处理 的 应用 https / / zhuanlan . 
zhihu . com / p / 30268946 相关 参考资料 链接 
1 . 一个 很好 的 卷积 操作 的 动画 演示 
http / / cs231n . github . io / convolutional 
networks / 2 . 宽 / 窄 卷积 的 动画 
演示 http / / deeplearning . net / software / 
theano _ versions / dev / tutorial / conv _ 
arithmetic . html3 . Udacity deep learning 课程 https / 
/ cn . udacity . com / course / deep 
learning ud7304 . Github 上 一个 有关 深度 学习 入门 
的 教程 / 代码 https / / github . com 
/ CreatCodeBuild / TensorFlow and DeepLearning Tutorial 作者简介 卞书青/nr 2017级 
研究生 目前 研究 方向 为 信息 抽取 深度 学习 来自 
中国 人民 大学 大 数据 管理 与 分析 方法 研究 
北京 市 重点 实验室 来源 https / / zhuanlan . 
zhihu . com / p / 3026894610 月 28 日 
本 周六 SDCC 2017 人工智能 技术 实战 线上 峰会 将在 
CSDN 学院 以 直播 互动 的 方式 举行 作为 SDCC 
系列 技术 峰会 的 一部分 来自 阿里 巴巴 微软 商汤 
科技 第四范式 微博 出门 问问 菱 歌 科技 的 AI 
专家 将 针对 机器学习 平台 系统 架构 对话 机器人 芯片 
推荐 系统 Keras 分布式系统 NLP 等 热点 话题 进行 分享 
先行者 们 正在 关注 哪些 关键技术 如何 从 理论 跨越 
到 企业 创新 实践 你 将从 本次 峰会 找到 答案 
每个 演讲 时段 均 设有 答疑 交流 环节 与会者 和 
讲师 可 零距离 互动 扫描 下方 二维码 入 群 交流 
实体 识别 实体 识别 – 分块 类型 名词 短语 分块 
标记 模式 分块 正则表达式 分块 分块 的 表示 方法 标记 
和 树状 图 分块 器 评估 命名 实体 识别 命名 
实体 定义 指 特定 类型 的 个体 是 一些 确切 
的 名词 短语 如 组织 人 日期 等 命名 实体 
识别 定义 指 通过 识别 文字 中 所 提及 的 
命名 实体 然后 确定 NE 的 边界 和 类型 命名 
实体 关系 提取 文 法分析 文法 定义 即 就是 文章 
的 书写 规则 一般 用来 指 以 文字 词语 短句 
句子 编排 而成 的 完整 语句 和 文章 的 合理性 
组织 文法 用途 1 性能 超越 n grams 2 确定 
句子成分 结构 形式 语法 一个 四元组 G = N ∑ 
P S 各个 符号 代表 的 意义 如下 N 非 
终结符 的 有限集合 有事 也 称为 变 量级 戒 句法 
种类 集 ∑ 终结 符号 的 有限集合 V 总 词汇表 
N ∪ ∑ P 一组 重写 规则 的 有限集合 P 
= ｛ α → β ｝ 其中 α β 是 
V 种元素 所 构成 的 串 α 种 至少 应该 
含 有 一个 非 终结 符号 S ∈ N 叫做 
句子 的 符戒/nr 初始 符 上下文 无关 文法 解析器 定义 
根据 文法 产生 式 处理 输入 的 矩阵 同时 建立 
一个 或 多个 符号 文法 的 组成 结构 分类 递归 
下降 解析器 自上而下 模式 移近 规约 解析器 自下而上 模式 左 
角落 解析器 自上而下 和 自下而上 两种 模式 相结合 递归/v 下降/v 
和左/nr 角落/n 解析/vn 都/d 存在/v 一定/d 的/uj 缺陷/v 因此 可以 
才用 动态规划 的 方法 进行 解析 依存 关系 与 依存 
文法 依存 文法 关注 词 与 其他 词 之间 的 
关系 依存关系 中心词 与 其他 从属 直接 的 二元 非对称 
关系 当前 的 一些 语法 困境 语言 数据 与 无限 
可能性 句子 构造 句子 歧义 问题 自然语言 理解 智能 问答 
系统 一阶逻辑 补充 运算 句子 语义 理解 段落 语义 理解 
图灵测试 阿兰 图灵 与 1950年 提出 测试 在 测试 者 
和被/nr 测试 者 相互 隔开 的 情况 下 通过 一些 
简单 的 装置 向被/nr 测试 者 随意 提问 通过 一些 
问题 之后 若 被 测试 者 的 答复 有 超过 
30% 的 部分 无法 让 测试者 确 认出 是 人 
还是 机器 的 回答 则 此时 这台 机器 通过 测试 
且 被 认为 具有 人工智能 命题逻辑 一阶逻辑 语法 独立 变量 
独立 常量 带 不同 参数 的 谓词 非逻辑 常量 逻辑 
常量 存在 量词 全称 量词 采取 约定 en t 是由 
n 个 e 类型 的 参数 所 组成 而 产生 
一个 类型 为 t 的 表达式 的 谓词 的 类型 
此类 情况 下 则 称 n 为 谓词 元数/nr 语句 
的 语义 组合 原则 整体 含义 是 部分 含义 与 
他们 的 句法 相结合 方式 的 函数 语料库 结构 TIMIT 
的 结构 内容 覆盖 方言 说话者 材料 TIMIT 的 设计 
特点 包含 语音 与 字形 标注 层 在 多个 维度 
的 变化 与 方言 地区 和二/nr 元音 覆盖 范围 中 
找到 一个 平衡点 将 原始 语音学 时间 作为 录音 来 
捕捉 和 标注 来 捕捉 之间 的 区别 层次结构 清晰 
结构 是 树状 结构 使用 时 目的性 TIMIT 的 基本 
数据类型 词典 文本 语料库 的 生命 周期 创建 语料库 的 
方案 研究 过程 中 逐步 形成 实验 研究 过程 中 
收集 特定 语音 的 参考 语料 质量 控制 Kappa 系数 
衡量 两个人 的 判断 类别 然后 修正 其 期望 一致性 
越大 一致性 越好 windowdiff 打分 器 衡量 两个 句子 分词 
的 一致性 维护 与 演变 数据采集 采集 方式 网上 获取 
文字 处理器 文件 获取 电子表格 和 数据库 中 获取 通过 
数据格式 转换 获取 使用 Toolbox 数据 标注 层 分词 断句 
分段 词性 句法结构 浅层 语义 对话 与 段落 无论 你 
是 成熟 的 公司 还是 想 要 推出 一个 新 
服务 都 可以 利用 文本 数据 来 验证 改进 和 
扩展 产品 的 功能 科学 的 从 文本 数据 中 
提取 语义 并 学习 是 自然 语言 处理 NLP 研究 
的 一个 课题 NLP 每天 都会 产生 新的 令人 兴奋 
的 结果 并且 它 是 一个 非常 大 的 领域 
然而 在与 数百 家 公司 合作 之后 Insight 团队 发现 
一些 关键 的 实际 应用 程序 比 其他 应用程序 出现 
得 更 频繁 例如 识别 不同 的 用户 / 客户 
群体 如 预测 客户 流失 终身 价值 产品 偏好 准确 
地 检测 和 提取 不同 类别 的 反馈 积极/ad 和/c 
消极/n 的/uj 评论/n //i 意见/n 和/c 特定/d 属性/n 如 衣服 
尺寸 / 是否 合身 根据 意图 对 文本 进行 分类 
例如 基本 请求 紧急 问题 虽然/c 有/v 许多/m 线上/i NLP/w 
文件/n 和/c 教程/n 但/c 我们/r 发现/v 很难/i 找到/v 有效/a 地/uv 
从/p 底层/n 解决/v 这些/r 问题/n 的/uj 指导/n 方针/n 和/c 技巧/n 
这篇文章 解释 了 如何 构建 机器学习 解决 方案 来 解决 
上面 提到 的 问题 我们 将 从最/nr 简单 的 方法 
开始 然后 转向 更 细致 的 解决方案 比如 特性 工程 
单词 向量 和 深度 学习 读完 这篇文章 你 会 知道 
如何 收集 准备 和 检查数据 建立 简单 的 模型 并在 
必要时 向 深度 学习 过渡 解释 和 理解 你 的 
模型 以 确保 你 是 在 获取 信息 而 不是 
噪音 我们 把 这篇文章 作为 一个 分步 指南 它 还 
可以 作为 高度 有效 的 标准 方法 的 高级 概述 
这篇文章 附带 了 一个 交互式 笔记本 演示 和 应用 所有 
这些 技术 交互式 笔记本 地址 https / / github . 
com / hundredblocks / concrete _ NLP _ tutorial / 
blob / master / NLP _ notebook . ipynbPart1 收集 
数据 示例 数据 来源 每一个 机器学习 问题 都 是从 数据 
开始 的 比如 电子邮件 帖子 或 推 文 文本 信息 
的 来源 包括 产品 评论 在 亚马逊 Yelp 和 各种 
应用 商店 用户 生成 内容 推 文 Facebook 帖子 StackOverflow 
问题 故障排除 客户 请求 支持票 聊天记录 社交 媒体 灾难 数据集 
对于 这篇文章 我们 将 使用 CrowdFlower 提供 的 称为 社交 
媒体 灾难 的 数据 集 其中 参与者 查看 了 超过 
10 000条 推 文 其中 包括 着火 隔离 和 防疫 
等 各种 关键字 的 搜索 然后 指出 这条 推 文 
是否 提到 了 灾难 事件 而 不是 带有 关键字 的 
电影 评论 或 笑话 和 一些 非 灾难性 的 事件 
我们 的 任务 是 检测 哪些 推 文是/nr 关于 灾难性 
事件 的 而 不是 像 电影 这样 无关紧要 的 话题 
这个 任务 的 特别 在于 两个 类 都 包含 相同 
搜索词 因此 我们 将 不得不 使用 更 微妙 的 差异 
来 区分 它们 在 这篇文章 的 余下 部分 中 我们 
将 把 有关 灾难 的 推 文 称为 灾难 并把 
其他 的 推 文 称为 无关 标签 我们 已经 标记 
了 数据 因此 我们 知道 哪些 推 文 属于 哪个 
类别 正如 Richard Socher 所 描述 的 那样 与 试图 
优化 复杂 的 无 监督 方法 相比 用 查找 和 
标记 足够 的 数据 来 训练 模型 更快 更简单 成本 
更低 Richard Socher 的 观点 Part2 清洗 数据 我们 遵循 
的 第一 条 规则 是 数据 的 好坏 影响 着 
你 的 模型 数据 科学家 的 关键 技能 之一 就是 
知道 下一步 应该 是 研究 模型 还是 数据 经验 告诉 
我们 应该 先 查看 数据 然后再 洗 数据集 干净 的 
数据 集 将 允许 模型 学习 有 意义 的 特性 
而 不是 过度 拟合 无关 的 噪音 以下 是 用来 
清洗 你 的 数据 的 清单 详见 代码 删除 所有 
不 相关 的 字符 例如 任 何非 字母 数字 字符 
non alphanumeric character 把 文本 分成 单独 的 单词 来 
令牌 化 文本 删除 不 相关 的 单词 比如 @ 
或 url 将 所有 字符 转换 为 小写 如 hello 
Hello 和 HELLO 考虑 将 拼错 的 单词 组合 成 
一个 单独 的 表示 如 cool kewl cooool 考虑 lemmatization 
减少 诸如 am are 和 is 这样 的 常见 形式 
例如 be 代码 地址 https / / github . com 
/ hundredblocks / concrete _ NLP _ tutorial / blob 
/ master / NLP _ notebook . ipynb 令牌 化 
地址 https / / nlp . stanford . edu / 
IR book / html / htmledition / tokenization 1 . 
html 在 遵循 这些 步骤 并 检查 其他 错误 之后 
我们 可以 开始 使用 干净 的 标记 的 数据 来 
训练 模型 Part3 找到 一个 好 的 数据 表示 机器学习 
模型 以 数值 作为 输入 例如 对 图像 进行 处理 
的 模型 利用 矩阵 表示 颜色通道 中 每个 像素 的 
强度 以 数字矩阵 表示 的 笑脸 我们 的 数据 集 
是 句子 的 列表 为了 让 我们 的 算法 从 
数据 中 提取 模式 我们 首先 需要 找到 一种 方法 
以 算法 能够 理解 的 方式 来 表示 它 也 
就是 一个 数字 列表 独 热 编码 词 袋 表示 
计算机 文本 的 一种 方法 是 将 每个 字符 单独 
编码 为 一个 数字 例如 ASCII 如果 我们 要 将 
这个 简单 的 表示 输入 到 分类器 中 那么 它 
必须 只 根据 我们 的 数据 来 学习 单词 的 
结构 这 对于 大多数 数据集 来说 是 不 可能 的 
我们 需要 使用 更 高级 的 方法 例如 我们 可以 
在 我们 的 数据 集中 建立 一个 包含 所有 单词 
的 词汇表 并为 词汇表 中的 每个 单词 创建 一个 唯一索引 
每个 句子 都被 表示 成 一个 列表 这个 列表 的 
长度 取决于 不同 单词 的 数量 在 这个 列表 中 
的 每个 索引 中 我们 标 记出 给定 词语 在 
句子 中 出现 的 次数 这 被 称为 词 袋 
模型 因为 它 是 一种 完全 无视 句子 中 词语 
顺序 的 表现 形式 以下 是 插图 说明 把 句子 
表示 为 词 袋 左边 是 句子 右边 是 数字 
表示 向量 中的 每一个 索引 都 代表 一个 特定 的 
单词 可视化 嵌入 在 社交 媒体 灾难 数据 集中 我们 
大约 有 2万 个 单词 这/r 意味着/v 每/zg 个/q 句子/n 
都/d 将被/i 表示/v 成/n 长度/ns 为/p 20000/m 的/uj 向量/n 这 
每个 句子 只 包含 了 我们 词汇量 的 一小部分 为了 
查看 嵌入 是否 捕获 了 与 我们 问题 相关 的 
信息 例如 推 文 是否 与 灾难 有关 可视化 它们 
并 查看 分类 是否 正确 是 一个 好 方法 由于 
词汇表 是 非常 大 的 并且在 20 000个 维度 中 
可视化 数据 是 不 可能 的 像 PCA 这样 的 
技术 将 有助于 将 数据 压缩 到 两个 维度 可视化 
词 袋 嵌入 这两个 类 看起来 并 没有 很好 地 
分离 这 可能 是 嵌入 的 一个 特性 或者 仅仅 
是 维度 缩减 为了 了解 这些 词 袋 的 特点 
是否 有 任何 用途 我们 可以 用 它们 来 训练 
分类器 Part4 分类 当 第一 次 尝试 时 最好 的 
做法 一般 是从 最 简单 的 工具 开始 着手 解决问题 
每当 提到 数据 分类 时 人们 最 喜欢 用 的 
是 逻辑 回归 这 是 非常 简单 的 训练 结果 
是 可以 解释 的 你 可以 很容易 地 从 模型 
中 提取 最 重要 的 系数 我们 将 数据 分解 
到 一个 训练 集中 用于 拟合 我们 的 模型 和 
测试 集 以 查看 它 对不 可见 的 数据 的 
概括 程度 经过训练 我们 的 准确率 达到 75.4% 不是 太 
糟糕 Part5 检查 混淆 矩阵 第一步 是 了解 我们 的 
模型 所 犯错误 的 类型 以及 哪些 错误 是 最不 
可取 的 在 我们 的 例子 中 误报 将 一个 
无关 的 推 文 归类 为 灾难 而 漏报 则将 
灾难 推 文 分类 为 无关 如果 首要 任务 是 
对 预测 灾难 事件 我们 就 要 降低 我们 的 
漏报率 如果 我们 在 资源 方面 受到限制 我们 可能 会 
优先 考虑 降低 误报率 以 减少 假 警报 一个 很好 
的 可视化 这个 信息 的 方法 是 使用 混淆 矩阵 
它 比较 了 我们 的 模型 预测 和 真实 标签 
理想 情况下 矩阵 将 是 一条 从左 上到 右下 的 
对角线 我们 的 预测 完全 符合 事实 混淆 矩阵 绿色 
是 高 比例 蓝色 是 低 比例 我们 的 分类器 
的 漏报率 高于 误报率 比例 换句话说 我们 的 模型 最 
常见 的 错误 是 错误 地 将 灾难 分类 为 
无关 解释 模型 为了 验证 我们 的 模型 并 解释 
它 的 预测 重要 的 是 看 一下 它 用 
哪些 单词 来做 决策 如果 我们 的 数据 有 偏差 
我们 的 分类器 会在 样本数据 中 做出 准确 的 预测 
但是 模型 在 现实 世界 中 不会 很好 地 泛化 
在 这里 我们 为 灾难 和 无关 类 找出 最 
重要 的 单词 用词 袋 和 逻辑 回 归来 绘制 
单词 的 重要 度 是 很 简单 的 因为 我们 
可以 提取 和 排列 模型 用于 预测 的 系数 词 
袋 单词 的 重要 度 我们 的 分类器 正确 地 
选择 了 一些 模式 广岛 大屠杀 但 显然 似乎 是 
过度 拟合 一些 无 意义 的 术语 heyoo x1392 现在 
我们 的 词 袋 模型 是 处理 大量 的 词汇 
并对 所有 单词 一视同仁 然而 有些 词 出现 频率 非常 
高 而且 只会 对 我们 的 预测 造成 干扰 接下来 
我们 将 尝试 用 一种 方法 来 表示 能够 解释 
单词 频率 的 句子 看看/v 是否/v 能从/nr 数据/n 中/f 获得/v 
更多/d 的/uj 信号/n Part6 词汇 结构 TF IDF 为了 帮助 
我们 的 模型 更多 地 关注 有 意义 的 单词 
我们 可以 在 我们 的 词 袋 模型 的 顶部 
使用 TF IDF 评分 术语 频率 逆 文档 频率 TF 
IDF 通过 单词 在 数据 集中 出现 的 频率 来 
衡量 单词 在 我们 的 数据 集 里 一些 词 
是 非常 罕见 的 而 有些 词 太过 频繁 只 
会 增加 噪音 这 是 我们 新 嵌入 的 PCA 
投影 可视化 TF IDF 嵌入 我们 可以 看到 这 两种 
颜色 之间 有更/nr 明显 的 区别 这将 使 我们 的 
分类器 更容易 区分 两个 组 让 我们 看看 这 会 
不会 带来 更好 的 性能 在 我们 新的 嵌入式 系统 
上 训练 另一个 逻辑 回归 我们 得到 了 76.2% 的 
精确度 一个 轻微 的 改善 我们 的 模型 是否 开始 
研究 更 重要 的 词汇 如果 我们 得到 了 更好 
的 结果 同时 防止 模型 欺骗 我们 那么 我们 就 
可以 真正 地 考虑 升级 这个 模型 TF IDF 文字 
的 重要 度 它 挑选 的 单词 看起来 更有意义 虽然 
我们 在 测试 集上 的 度量 只 稍微 增加 了 
一点 但是 我们 对 我们 的 模型 使用 的 术语 
有了 更多 的 信心 因此在 将 它 部署 到 与 
客户 交互 的 系统 中 会 更好 Part7 利用 语义 
Word2Vec 我们 的 最新 模型 设法 获得 高 信号 单词 
然而 很 有可能 的 是 如果 我们 部署 这个 模型 
我们 将 会 遇到 以前 在 我们 的 训练 中 
没有 看到 的 单词 之前 的 模型 将 无法 准确 
地 对 这些 推 文 进行 分类 即使 在 训练 
过程 中 看到 了 非常 相似 的 单词 为了 解决 
这个 问题 我们 需要 掌握 词语 的 语义 用来 帮助 
我们 捕捉 语义 的 工具 叫做 Word2Vec 使用 预先 训练 
的 单词 Word2Vec 是 一种 查找 单词 连续 嵌入 的 
技术 它 听过 阅读 大量 的 文本 来 学习 并 
记住 在 类似 的 语境 中 出现 的 单词 在对 
足够 的 数据 进行 训练 之后 它 会在 词汇表 中为 
每个 单词 生成 一个 300 维 的 向量 这些 单词 
之间 的 意思 相近 该 论文 的 作者 开源 了 
一个 在 非常 大 的 语料库 中 预先 训练 的 
模型 我们 可以 利用 它 将 一些 语义 的 知识 
包含 进 我们 的 模型 中 预先 训练 的 向量 
可以 在 相关 的 资源库 中 找到 论文 地址 https 
/ / arxiv . org / abs / 1301.3781 资源库 
地址 https / / github . com / hundredblocks / 
concrete _ NLP _ tutorial 句 子层 面上 的 表示 
让 句子 快速 嵌入 分类器 的 方法 是 平均 在 
我们 的 句子 所有 单词 的 Word2Vec 分数 这是 与 
以前 方法 类似 的 词 袋 但是 这次 我们 只 
去掉 了 句子 的 语法 同时 保留 一些 语义 信息 
Word2Vec 句子 嵌入 下面 是 我们 使用 以前 的 技术 
实现 的 新 嵌入 的 可视化 可视化 Word2Vec 嵌入 这 
两组 颜色 看 起来 更加 分离 我们 的 新 嵌入 
应该 帮助 分类器 找到 两个 类 之间 的 分离 在 
第三 次 逻辑 回归 训练 了 相同 的 模型 后 
我们 的 准确率 为 77.7% 这 是 我们 最好 的 
结果 是 时候 检查 我们 的 模型 了 复杂性 / 
可 解释性 权衡 由于 我们 的 嵌入 没有 像 我们 
以前 的 模型 那样 表示 为 每个 单词 的 一维 
向量 所以 很难 看出 哪些 单词 与 我们 的 分类 
最 相关 虽然 我们 仍然 可以 使用 逻辑 回归 的 
系数 但 它们 与 我们 的 嵌入 的 300个 维度 
有关 而 不是 单词 的 索引 对于 如此 低 的 
精确度 失去 所有 的 解释 能力 似乎 是 一种 苛刻 
的 取舍 但是 对于 更 复杂 的 模型 我们 可以 
利用 像 LIME 这样 的 黑箱 解释器 来 了解 我们 
的 分类器 是 如何 工作 的 LIMEGithub 通过 开源 软件包 
提供 LIME 黑箱 解释器 允许 用户 通过 扰动 输入 在 
我们 的 例子 中 是从 句子 中 移除 单词 和 
观察 预测 如何 改变 来 解释 任何 分类器 在 一个 
特定 示例 上 的 决定 Github 资源 包 地址 https 
/ / github . com / marcotcr / lime 让 
我们 来 看看 我们 的 数据 集中 的 几个 句子 
的 解释 真正 的 灾难 词 被 识别 为 相关 
词语 对 分类 的 贡献 似乎 不 那么 明显 但是 
我们 没有 时间 去 探索 数据 集中 的 数以千计 的 
例子 我们 要 做 的 是 在 一个 有 代表性 
的 测试 示例 样本 上 运行 LIME 看看 哪些 词 
对于 分类 贡献度 最高 使用 这种 方法 我们 可以 得到 
单词 重要 度 分数 并 验证 我们 模型 的 预测 
Word2Vec 文字 的 重要性 看起来 模型 提 取出 了 高度 
相关 的 单词 这些 单词 暗示 它 做出 可以 理解 
的 决定 这些 看起来 像 是 以前 所有 模型 中最 
相关 的 词汇 因此 我们 更 愿意 部署 到 生产 
中 Part8 使用 端 到 端 的 方法 利用 语法 
我们 已经 介绍 了 快速 有效 的 方法 来 生成 
紧凑 的 句子 嵌入 然而 通过 省略 单词 的 顺序 
我们 放弃 了 句子 的 所有 语法 信息 如果 这些 
方法 不能 提供 足够 的 结果 则 可以 使用 更 
复杂 的 模型 将 整个 句子 作为 输入 并 预测 
标签 而 不 需要 建立 中间 表示 一种 常见 的 
方法 是 使用 Word2Vec 或 其他 方法 如 GloVe 或 
CoVe 将 句子 作为 一个 单词 向量 的 序列 高效 
的 端 到 端 架构 卷积 神经 网络 的 句子 
分类 训练 非常快 并且 适用 于 作为 入门级 的 深度 
学习 架构 虽然 卷积 神经网络 CNN 主要 以其 在 图像 
数据 上 的 性能 而 著称 但 它们 在与 文本 
相关 的 任务 上 的 性能 也 非常 好 而且 
通常 比 大多数 复杂 的 NLP 方法 例如 LSTM 和 
编码器 / 解码器 架构 要 快得多 这个 模型 保存 了 
单词 的 顺序 并且 学习 了 关于 哪些 单词 序列 
可以 预测 目标 类 的 有价值 的 信息 与 以前 
的 模式 相反 它 可以 区分 Alex eats plants 和 
Plants eat Alex . 训练 这个 模型 不 需要 比 
以前 的 方法 做 更多 的 工作 详见 代码 并且 
得到 的 模型 会比 以前 的 好得多 准确率 高达 79.5% 
与 上面 的 模型 一样 下 一步 应该 使用 我们 
描述 的 方法 来 探索 和 解释 预测 以 验证 
它 确实 是 最佳 模型 代码 地址 https / / 
github . com / hundredblocks / concrete _ NLP _ 
tutorial / blob / master / NLP _ notebook . 
ipynb 最后 成功 方法 的 快速 回顾 从 一个 快速 
简单 的 模型 开始 解释 其 预测 理解 所犯 的 
错误 使用 这些 知识 来 提示 下 一步 无论 是 
处理 数据 还是 一个 更 复杂 的 模型 这些 方法 
被 应用 到 一个 特定 的 示例 案例 中 使用/v 
定制/v 的/uj 模型/n 来/v 理解/v 和/c 利用/n 诸如/nr 推/v 文/n 
之类/r 的/uj 短文/n 本/r 但是 这些 想法 广泛 适用 于 
各种 问题 ML & AI 长按 识别 二维码 加 关注 
人工智能 或许 是 人类 最 美好 的 梦想 之一 追溯 
到 公元前 仰望 星空 的 古希腊 人 当 亚里士多德 为了 
解释 人类 大脑 的 运行 规律 而 提出 了 联想主义 
心理学 的 时候 他 恐怕 不会 想到 两千 多年 后的/nr 
今天 人们 正在 利用 联想主义 心理学 衍化 而来 的 人工神经网络 
构建 的 超级 人工智能 成为 最 能 接近 梦想 的 
圣境 并 一次 又一次 地 挑战 人类 大脑 认知 的 
极限 在 以大 数据 云计算 为 背景 的 技术 框架 
支撑 下 互联网 发展 极 为 迅速 过去 一个 技术 
或者 行业 热点 从 诞生 到 消亡 需要 几年 乃至 
更长 的 时间 但是 最 近几年 其 生命 周期 在 
不断 缩短 大多数 的 热点 从 产生 到 消亡 只需要 
1 2年 有些 仅仅 是 半年 甚至 几个月 的 时间 
互联网 行业 越来越 凸显 出 快鱼 吃 慢 鱼的/nr 特点 
从/p 技术/n 本身/r 也/d 有/v 体现/v 比如 2012 2014年 是 
移动 互联网 的 热潮 安卓 和 ios APP 开发 工程师 
当时 非常 流行 随后 2015大 数据 云计算 之年 2016 年后 
大 数据 时代 2017年 被 称为 人工智能 元年 2018年 炒 
得 最 火 的 是 区块 链 和币圈/nr 在 互联网 
以 这种 迅雷不及掩耳之势 的 发展 速度 下 作为 初学者 就 
很容易 被 各种 技术 概念 迷惑 找 不到 自己 想要 
的 突破口 和 深入 的 领域 即便 是 计算机 从业者 
有时候 也 分不清 到底 如何 定位 自己 未来 的 技术 
方向 下面 我们 先 从 中国 互联网 的 发展 历程 
说起 从 1994 诞生 加入 国际 互联网 到 现在 才 
短短的 24年 就在 这 24 年里 我们 经历 了 4次 
非同凡响 一次 比 一次 更 彻底 的 发展 大 高潮 
第一 次 互联网 大 浪潮 1994年 2000年 以 四大 门户 
和 搜索 为 代表 能做 网站 的 工程师 就 可以 
被 称为 技术 牛人 第二 次 互联网 大 浪潮 2001年 
2008年 从 搜索 到 PC 端 社交 化 网络 的 
发展 我们 的 社交 形态 发生 了 根本 的 变化 
从 线下 交流 正 转变为 线上 交流 大量 的 数据 
开始 生成 第三 次 互联网 大 浪潮 2009年 2014年 PC 
端 互联网 到 移动 互联网 此时 各种 APP 如 雨后春笋 
般的 冒出来 尽管/c 后来/t 有/v 很多/m APP/w 都/d 死了/i 但是 
移动 互联网 几乎 颠覆 了 整个 中国 老百姓 个人 生活 
和 商业 形态 改变 着 我们 每 一个人 的 生活 
消费 社交 出行 方式 等 那 第四 次 是 什么 
呢 没错 第四 次 互联网 大 浪潮 2015 至今 是 
在前 3次 发展 基础 上 以大 数据 云计算 为 背景 
发展 起来 的 人工智能 技术 革命 分布式计算 让 大 数据 
处理 提速 而 昔日 陨落 的 巨星 深度 学习 此刻 
再次 被 唤醒 并 很快 在 图像 和 语音 方面 
取得 重大 突破 但在 自然 语言 方面 却 显得 有些 
暗淡 突破 并 不是 很大 尽管 有 很多 人 都去 
从事 计算机 视觉 语音 等 方面 的 工作 但随着 AI 
的 继续 发展 在 NLP 方向 显得 越来越 重要 接着 
我们/r 总结/n 一下/m 数据/n 领域/n 成就/n 和/c 挑战/vn 有/v 一个/m 
不可/v 否认/v 的/uj 事实/n 当前 从事 互联网 的 人们 已经 
制造 出了 海量 的 数据 未来 还 将 继续 持续 
其中 包括 结构化 数据 半 结构化 和非/nr 结构化 数据 笔者 
发现 对于 结构化 数据 而言 在 大 数据 云 计算技术 
上下齐心 的 大力 整合 下 其 技术 基本 趋向 成熟 
和 稳定 比如 关系型 数据库 以及 基于 Hadoop 的 HDFS 
分布式文件系统 Hive 数据仓库 和非/nr 关系型 数据库 Hbase 以及 Elasticsearch 集群 
等 数据 存储 的 关系 数据库 或者 NoSql 可以 用来 
管理 和 存储 数据 基于 MapReduce Spark 和 Storm Flink 
等 大 数据 处理 框架 可以 分别 处理 离线 和 
实时 数据 等 而 半 结构化 非 结构化 的 数据 
除了 以 ELK 为 代表 的 日志 处理 流程 过去 
在 其它 限定 领域 基于 规则 和 知识库 也 取得 
了 一定 的 成果 因其 自身 的 复杂性 未来/t 更多/d 
领域/n 应用/v 都/d 具有/v 很大/a 的/uj 困难/an 和/c 挑战/vn 最后 
我们 看看 国内外 人工智能 领域 的 工业 现状 今年 5月 
19日 有幸 在 北京 国家 会议 中心 参加 了 2018 
全球 人工智能 技术 大会 GAITC 在 大会 上 从 中国 
科学院 院士 姚期智 提出 人工智能 的 新思维 开始 其 重点 
讲述 了 人工 神经 网络 为 代表 的 深度 学习 
以及 量子 计算机 将 是 未来 发展 的 新思维 紧接着 
中国 工程院 院士 李德 毅 分享 了 路测的/nr 学问 无人 
驾驶 的 后 图灵测试 提出 未来 无人驾驶 挑战 应该是 让 
无人驾驶 具有 司机 的 认知 思维 和 情感 而 不是 
当前 以 GPS 定位 和 动力学 方面 解决 无人 驾驶 
的 问题 接下来 微软 全球 资深 副总裁 王永 东向 我们 
展示 的 微软 小冰 大家 一起 见证 了 微软 小冰 
在 社交 互动 唱歌 作诗 节目 主持 和 情感 方面 
不凡 的 表现 而 本人 也 真实 测试 了 一下 
小冰 现在 的 表现 已经 非常 优秀 了 可以 作诗 
唱歌 聊天 节目 主持 等 然而 要 达到 一个 成年 
自然人 的 水平 在 某些 方面 还 不能 完全 表现 
出人 的 特性 下面 这幅 图 是 微软 小冰 的 
个人 介绍 有兴趣 可以 在 微信 公众 号 关注 小冰 
进行 体验 人工智能 产业 的 快速 发展 资本 市场 大量 
资金 涌入 促使 中国 人工智能 领域 投融资 热度 快速 升温 
充分 表明 资本 市场 对于 人工智能 发展 前景 的 认可 
2018年 人工智能 行业 创新 企业 Top100 发布 据 榜单 显示 
进入 2018年 人工智能 行业 创新 企业 前十名 的 企业 分别 
是 百度 阿里云 美图 秀秀 华大基因 科大 讯 飞 微鲸/nr 
科技 华云/nr 数据/n 爱驰亿/nr 维 青云 七 牛云/nr 作为 人工智能 
的 一个 重要 组成部分 自然语言 处理 NLP 的 研究 对象 
是 计算机 和 人类 语言 的 交互 其 任务 是 
理解 人类 语言 并 将其 转换 为 机器语言 在 目前 
的 商业 场 中 NLP 技术 用于 分析 源自 邮件 
音频 文件 网页 论坛 社交 媒体 中 的 大量 半 
结构化 和非/nr 结构化 数据 市场前景 巨大 为什么 说 未来 数据 
领域 的 珠穆朗玛峰 是 中文 自然语言 处理 正是 基于 上 
面对 中国 互联网 发展 的 总结 对 当前 数据 领域 
所 面临 的 挑战 以及 资本 市场 对 人工智能 的 
认可 分析 未来 数据 领域 的 重点 是 自然 语言 
处理 技术 及其 在 智能 问答 情感 分析 语义 理解 
知识图谱 等 应用 方面 的 突破 对于 我们 国内 中文 
来说 如何 更好 的 把 前面 所说 的 应用 在 
中文 处理 上 显得 更为 重要 和 急迫 所以 笔者 
认为 未来 数据 领域 的 珠穆朗玛峰 是 中文 自然语言 处理 
作为 初学者 我们 目前 面 又 临 这样 的 尴尬 
网上 大 部分 自然语言 处理 内容 都是/nr 英文 为基础 大多数 
人 先是 学好 了 英语 的 处理 回头 来 再处理 
中文 却 发现 有 很大 的 不同 这样 不仅 让 
中文 自然语言 处理 学习者 走了 弯路 也 浪费 了 大量 
时间 和 精力 中文 的 处理 比 英文 复杂 的 
多 网上 中文 相关 资料 少之又少 国内 纯中文 自然语言 处理 
书籍 只有 理论 方面 的 却在 实战 方面 比较 空缺 
这 让 中文 自然语言 处理 的 研究 开发 工作 感到 
举步维艰 很难 下笔 本文 由 人工智能 观察 编译 译者 Sandy 
几年前 通过 语音 对 手机 提 问来 在 互联 网上 
找到 答案 基本 是 不 可能 的 因为 计算机 在 
理解 人类 语言 方面 并 不是 很 出色 如今 由于 
机器学习 方面 的 进步 我们 开始 逐渐 意识到 谷歌 助理 
或 苹果 Siri 在对 我们 的 问题 进行 应答 方面 
基本 已经 没有 太大 的 问题 了 不过 不可否认 这一 
进展 是 极其 艰难 的 需要 对 非常 具体 的 
自然 语言 处理 任务 进行 强化 培训 比如 将 文本 
翻译成 语音 对 感叹词 或者 理解 代词 的 引用 进行 
分析 等等 这 也是 Salesforce 的 研究 人员 正在 着手 
解决 的 问题 本 周三 他们 发布 了 一篇 论文 
概述 了 一种 可以 同时 处理 10个 独立 自然语言 处理 
NLP 任务 的 单一 模型 的 方法 从 本质 上 
说 这一 研究 是 充满 挑战 的 据 了解 这一 
模型 被称为 自然语言 十项全能 Natural Language Decathlon 简称 decaNLP 通过 
将 该 模型 与 多任务 问题 应答 网络 Multitask Question 
Answering Network MQAN 进行 结合 可以 同时 学习 10项 任务 
包括 问答 机器翻译 摘要 自然语言 推理 情感 分析 语义 角色 
标注 关系 抽取 目标 导向 对话 语义分析 常识 代词 消解 
Salesforce 的 首席 科学家 Richard Socher 在 接受 采访 时说/nr 
我们 的 模型 好比 是 NLP 领域 的 瑞士军刀 换句话说 
研究 人员 和 开发 人员 实际上 只 需 使用 一种 
工具 而 不必 为 那些 任务 中的 每一项 使用 一种 
工具 那些 任务 需要 无法 用于 其他 任何 任务 的 
超级 定制 的 模型 最终 这一 模型 可能会 带来 更 
有 能力 的 聊天 机器人 让 它们 可以 更 自然 
地 与人 进行 交谈 Socher 与 ImageNet 做 了 一个 
类比 这 是 一个 标签 图像 数据库 被 认为 开启 
了 深度 学习 的 革命 使得 图像识别 方面 取得 了 
突破 但是 对于 NLP 并 不能 通过 一项 任务 就 
能 进行 定义 对此 Socher 称 在 NLP 领域 其实 
不 存在 所有 研究 人员 都 认为 如果 你 在 
这个 方面 取得 进展 它 就会 整体 上 改善 NLP 
的 单一 任务 Salesforce 的 研究 人员 包括 Bryan McCann 
Nitish Shirish Keskar 和 Caiming Xiong 提出 的 方法 就是 
将 这 每 一项 任务 都 视作 一个 回答 问题 
的 问题 Socher 解释 说 问题 回答 其实 是 非常 
宽泛 的 你 可以 随便 问 任何问题 该 研究 相当于 
提供 了 解决 几个 任务 的 单一 模型 另外 MQAN 
允许 进行 所谓 的 零 样本 zero shot 学习 这 
意味着 该 模型 可以 处理 以前 未 见过 的 任务 
或 未 经过 培训 的 任务 把 它 应用 到 
一个 全新 的 任务 上 这是 以前 从未有过 的 McCann 
表示 遇到 换个 方式 来 表述 或 意义 上略 有变化 
大多数 模型 就 无能为力 现在 我们 的 模型 做到 了 
他 补充 说 具体 到 实际 应用 面对 并不 完全 
是 已经 学会 的 短语 聊天 机器人 可以 做出 极 
准确 的 答复 更像 是 人们 平常 交谈 那样 事实上 
Salesforce 的 这一 研究 也 代表 了 著名 人工智能 研究员 
Yoshua Bengio 的 观点 他 是 蒙特利尔 大学 计算机 科学 
与 运筹 学系 教授 与 Socher 在 机器学习 领域 展开 
过 合作 他 表示 自从 大约 二十年前 我 开始 致力于 
表示 自然 语言 的 词汇 嵌入 以来 我 的 目标 
就是 同样 的 表示法 应该 可 用于 所有 自然语言 任务 
这篇 论文 中 将 所有 这些 任务 表示 为 回答 
问题 的 想法 至关重要 但这 还不够 论文 作者 搞出 了 
自然 语言 十项全能 为 这个 目标 定义 一个 基准 并 
引入 了 最终 使 这个 梦想 有 可能 实现 的 
架构 上 的 创新 文中 图片 来自 网络 投稿 约访 
合作 联系 邮箱 aiobservation @ qq . com 添加 微信 
aixiaozhijun 更多 交流 或 进 AI 观察团 本文 由 Markdown 
语法 编辑器 编辑 完成 自然语言 处理 要 解决 的 主要 
问题 有 1 垃圾邮件 识别 2 中文 输入法 3 机器翻译 
4 自动 问答 客服 机器人 这里 简单 罗列 了 一些 
NLP 的 常见 领域 分词 词性 标注 命名 实体 识别 
句法分析 语义 识别 垃圾邮件 识别 拼写 纠错 词义 消 歧 
语音识别 音 字 转换 机器翻译 自动 问答 腾讯 文智 中文 
语义 平台 平台 链接 地址 http / / nlp . 
qq . com / index . cgi 参考 链接 1 
. 从 破译 外星人 文字 浅谈 自然语言 处理 的 基础 
http / / blog . csdn . net / han 
_ xiaoyang / article / details / 505456502 . 腾讯 
文智 中文 语义 平台 http / / nlp . qq 
. com / index . cgi 自然语言 处理 NLP 知识结构 
文|/nr 秦陇 纪 数据 简化 DataSimp 自然语言 处理 计算机 语言学 
自然语言 理解 涉及 字处理 词 处理 语句 处理 篇章 处理 
词 处理 分词 词性 标注 实体 识别 词义 消 歧 
语句 处理 句法分析 y n t a c t i 
c A n a l y s i s 语义分析 
e n m a n t i c A n 
a l y s i s 等 其中 重点 有 
1 . 句法 语义分析 分词 词性 标记 命名 实体 识别 
2 . 信息 抽取 3 . 文本 挖掘 文本 聚 
类 情感 分析 基于 统计 4 . 机器翻译 基于 规则 
基于 统计 基于 神经网络 5 . 信息检索 6 . 问答 
系统 7 . 对话 系统 建议 本文 总结 的 自然 
语言 处理 历史 模型 知识 体系 结构 内容 涉及 NLP 
的 语言 理论 算法 和 工程 实践 各 方面 内容 
繁杂 参考/v 黄志洪/nr 老师/n 自然语言/l 处理/v 课程/n 宗 成庆 老师 
统计 自然语言 处理 郑捷 2017年 电子 工业 出版社 出版 的 
图书 NLP 汉语 自然语言 处理 原理 与 实践 以及 国外 
著名 NLP 书籍 的 英文 资料 汉 译版 资料 一 
NLP 知识结构 概述 1 自然语言 处理 利用 计算机 为 工具 
对 书面 实行 或者 口头 形式 进行 各种各样 的 处理 
和 加工 的 技术 是 研究 人 与人 交际 中 
以及 人 与 计算机 交际 中的 演员 问题 的 一门 
学科 是 人工智能 的 主要 内容 2 自然语言 处理 是 
研究 语言 能力 和 语言 应用 的 模型 建立 计算机 
算法 框架 来 实现 这样 的 语言 模型 并 完善 
评测 最终 用于 设计 各种 实用 系统 3 研究 问题 
主要 信息检索 机器翻译 文档 分类 问答 系统 信息 过滤 自动 
文摘 信息 抽取 文本 挖掘 舆情 分析 机器 写作 语音识别 
研究 模式 自然语言 场景 问题 数学 算法 算法 如何 应用 
到 解决 这些 问题 预料 训练 相关 实际应用 自然 语言 
的 困难 场景 的 困难 语言 的 多样性 多变性 歧义 
性 学习 的 困难 艰难 的 数学 模型 hmm crf 
EM 深度 学习 等 语料 的 困难 什么 的 语料 
语料 的 作用 如何 获取 语料 二 NLP 知识 十大 
结构 1 形式语言 与 自动机 语言 按照 一定 规律 构成 
的 句子 或者 字符串 的 有限 或者 无限 的 集合 
描述语言 的 三种 途径 穷举法 文法 产生式系统 描述 自动机 自然 
语言 不 是 人为 设计 而 是 自然 进化 的 
形式语言 比如 运算 符号 化学 分子式 编程语言 形式语言 理论 朱 
啊哟 研究 的 是 内部 结构 模式 这类 语言 的 
纯粹 的 语法 领域 从 语言学 而来 作为 一种 理解 
自然 语言 的 句法 规律 在 计算机 科学 中 形式语言 
通常 作为 定义 编程 和 语法结构 的 基础 形式 语言 
与 自动机 基础知识 集合论 图论 自动机 的 应用 1 单词 
自动 查错 纠正 2 词性 消 歧 什么 是 词性 
什么 的 词性 标注 为什么 需要 标注 如何 标注 形式语言 
的 缺陷 1 对于 像 汉语 英语 这样 的 大型 
自然 语言 系统 难以 构造 精确 的 文法 2 不 
符合 人类 学习 语言 的 习惯 3 有些 句子 语法 
正确 但在 语义上 却 不 可能 形式语言 无法 排出 这些 
句子 4 解决 方向 基于 大量 语料 采用 统计学 手段 
建立 模型 2 语言 模型 1 语言 模型 重要 通过 
语料 计算 某个 句子 出现 的 概率 概率 表示 常用 
的 有2/nr 元 模型 3 元 模型 2 语言 模型 
应用 语音识别 歧义 消除 例如 给定 拼音串 ta shi yan 
yan jiu saun fa de 可能 的 汉字 串 踏实 
烟酒 算法 的 他 是 研究 酸 法的他/nr 是 研究 
算法 的 显然 最后 一句 才 符合 3 语言 模型 
的 启示 1 开启 自然语言 处理 的 统计 方法 2 
统计 方法 的 一般 步骤 收集 大量 语料 对 语料 
进行 统计分析 得出 知识 针对 场景 建立 算法 模型 解释 
和 应用 结果 4 语言 模型 性能评价 包括 评价 目标 
评价 的 难点 常用 指标 交叉 熵 困惑 度 5 
数据 平滑 数据 平滑 的 概念 为什么 需要 平滑 平滑 
的 方法 加 一 法 加法 平滑 法 古德 图灵 
法 J M 法 Katz 平滑 法等6/nr 语言 模型 的 
缺陷 语料 来自 不同 的 领域 而 语言 模型 对 
文本 类型 主题 等 十分 敏感 n 与 相邻 的 
n 1个 词 相关 假设 不是 很 成立 3 概率 
图 模型 生成 模型 与 判别 模型 贝叶斯 网络 马尔科夫 
链 与 隐 马尔科夫 模型 HMM 1 概率 图 模型 
概述 什么 的 概率 图 模型 参考 清华大学 教材 概率 
图 模型 2 马尔科夫 过程 定义 理解 3 隐 马尔科夫 
过程 定义 理解 HMM 的 三个 基本问题 定义 解法 应用 
注 第一 个 问题 涉及 最大 似 然 估计法 第二 
个 问题 涉及 EM 算法 第三 个 问题 涉及 维 
特比 算法 内容 很多 要 重点 理解 参考书 李航 统计 
学习 方法 网上 博客 笔者 github 4 马尔科夫 网 最大熵 
模型 条件 随 机场 CRF 1 HMM 的 三个 基本 
问题 的 参数 估计 与 计算 2 什么 是 熵 
3 EM 算法 应用 十分 广泛 好好 理解 4 HMM 
的 应用 5 层次化 马尔科夫 模型 与 马尔科夫 网络 提出 
原因 HMM 存在 两个 问题 6 最大熵 马尔科夫 模型 优点 
与 HMM 相比 允许 使用 特征 刻画 观察 序列 训练 
高效 缺点 存在 标记 偏置 问题 7 条件 随 机场 
及其 应用 概念 模型 过程 与 HMM 关系 参数估计 方法 
GIS 算法 改进 IIS 算法 CRF 基本问题 特征 选取 特征 
模板 概率 计算 参数 训练 解码 维 特比 应用 场景 
词性 标注 类 问题 现在 一般 用 RNN + CRF 
中文分词 发展过程 经典 算法 了解 开源 工具 jieba 分词 中文 
人名 地名 识别 8 CRF + + 5 命名 实体 
识别 词性 标注 内容 挖掘 语义分析 与 篇章 分析 大量 
用到 前面 的 算法 1 命名 实体 识别 问题 相关 
概率 定义 相关 任务 类型 方法 基于 规程 基于 大 
规模 语料库 2 未 登录 词 的 解决 方法 搜索引擎 
基于 语料 3 CRF 解决 命名 实体 识别 NER 流程 
总结 训练 阶段 确定 特征 模板 不同 场景 人名 地名 
等 所 使用 的 特征 模板 不同 对 现有 语料 
进行 分词 在 分词 结果 基础 上 进行 词性 标注 
可能 手工 NER 对应 的 标注 问题 是 基于 词 
的 然后 训练 CRF 模型 得到 对应 权值 参数值 识别 
过程 将 待 识别 文档 分词 然后 送入 CRF 模型 
进行 识别 计算 维 特比 算法 得到 标注 序列 然后 
根据 标注 划分 出 命名 实体 4 词性 标注 理解 
含义 意义 及其 一致性 检查 方法 位置 属性 向量 词性 
标注 序列 向量 聚 类 或者 分类 算法 6 句法分析 
1 句法分析 理解 以及 意义 1 句法结构 分析 完全 句法分析 
浅层 分析 2 依存关系 分析 2 句法 分析方法 1 基于 
规则 的 句法结构 分析 2 基于 统计 的 语法 结构 
分析 7 文本 分类 情感 分析 1 文本 分类 文本 
排 重 文本 分类 在 预定义 的 分类 体系 下 
根据 文本 的 特征 将 给定 的 文本 与 一个 
或者 多 个 类别 相关联 典型 应用 垃圾邮件 判定 网页 
自动 分类 2 文本 表示 特征 选取 与 权重 计算 
词 向量 文本 特征选择 常用 方法 1 基于 本文 频率 
的 特征 提 取法 2 信息 增 量法 3 X2 
卡方 统计量 4 互信息 法3/nr 分类器 设计 SVM 贝叶斯 决策树 
等 4 分类器 性能 评测 1 召回率 2 正确率 3 
F1 值 5 主题 模型 LDA 与 PLSALDA 模型 十分 
强大 基于 贝叶斯 改进 了 PLSA 可以 提取 出 本章 
的 主题词 和 关键词 建模 过程 复杂 难以理解 6 情感 
分析 借助 计算机 帮助 用户 快速 获取 整理 和 分析 
相关 评论 信息 对 带有 感情 色彩 的 主观 文本 
进行 分析 处理 和 归纳 例如 评论 自动 分析 水军 
识别 某种 意义 上 看 情感 分析 也 是 一种 
特殊 的 分类 问题 7 应用 案例 8 信息检索 搜索引擎 
及其 原理 1 信息检索 起源于 图书馆 资料 查询 检索 引入 
计算机 技术 后 从 单纯 的 文本 查询 扩展到 包含 
图片 音 视频 等 多媒体 信息检索 检索 对象 由 数据库 
扩展 到 互联网 1 点对点 检索 2 精确 匹配 模型 
与 相关 匹配 模型 3 检索系统 关键 技术 标引 相关度 
计算 2 常见 模型 布尔 模型 向量空间 模型 概率模型 3 
常用 技术 倒排索引 隐 语义分析 LDA 等 4 评测 指标 
9 自动 文摘 与 信息 抽取 机器翻译 问答 系统 1 
统计 机器 翻译 的 的 思路 过程 难点 以及 解决 
2 问答 系统 基本 组成 问题 分析 信息检索 答案 抽取 
类型 基于 问题 答案 基于 自由 文本 典型 的 解决 
思路 3 自动 文摘 的 意义 常用 方法 4 信息 
抽取 模型 LDA 等 10 深度 学习 在 自然 语言 
中 的 应用 1 单词 表示 比如 词 向量 的 
训练 wordvoc 2 自动 写 文本 写 新闻 等 3 
机器翻译 4 基于 CNN RNN 的 文本 分类 5 深度 
学习 与 CRF 结合 用于 词性 标注 三 中文 NLP 
知识 目录 选自 郑捷 2017年 电子 工业 出版社 出版 的 
图书 NLP 汉语 自然语言 处理 原理 与 实践 第 1 
章 中文 语言 的 机器 处理 11.1 历史 回顾 21 
. 1.1 从 科幻 到 现实 21 . 1.2 早期 
的 探索 31 . 1.3 规则 派 还是 统计 派 
31 . 1.4 从 机器 学习 到 认知 计算 51.2 
现代 自然 语言 系统 简介 61 . 2.1 NLP 流程 
与 开源 框架 61 . 2.2 哈工大 NLP 平台 及 
其 演示 环境 91 . 2.3 StanfordNLP 团队 及 其 
演示 环境 111 . 2.4 NLTK 开发环境 131.3 整合 中文分词 
模块 161 . 3.1 安装 Ltp Python 组件 171 . 
3.2 使用 Ltp 3.3 进行 中文分词 181 . 3.3 使用 
结巴 分词 模块 201.4 整合 词性 标注 模块 221 . 
4.1 Ltp 3.3 词性 标注 231 . 4.2 安装 StanfordNLP 
并 编写 Python 接口类 241 . 4.3 执行 Stanford 词性 
标注 281.5 整合 命名 实体 识别 模块 291 . 5.1 
Ltp 3.3 命名 实体 识别 291 . 5.2 Stanford 命名 
实体 识别 301.6 整合 句法 解析 模块 321 . 6.1 
Ltp 3.3 句法 依存 树 331 . 6.2 StanfordParser 类 
351 . 6.3 Stanford 短语 结 构树 361 . 6.4 
Stanford 依存 句法树 371.7 整合 语义 角色 标注 模块 381.8 
结语 40 第 2 章 汉语 语言学 研究 回顾 422.1 
文字 符号 的 起源 422 . 1.1 从 记事 谈起 
432 . 1.2 古文字 的 形成 472.2 六书 及 其他 
482 . 2.1 象形 482 . 2.2 指事 502 . 
2.3 会意 512 . 2.4 形声 532 . 2.5 转注 
542 . 2.6 假借 552.3 字形 的 流变 562 . 
3.1 笔 与 墨的/nr 形成 与 变革 562 . 3.2 
隶 变 的 方式 582 . 3.3 汉字 的 符号化 
与 结构 612.4 汉语 的 发展 672 . 4.1 完整 
语义 的 基本 形式 句子 682 . 4.2 语言 的 
初始 形态 与 文言文 712 . 4.3 白话文 与 复音词 
732 . 4.4 白话文 与 句法 研究 782.5 三个 平面 
中的 语义 研究 802 . 5.1 词汇 与 本体论 812 
. 5.2 格 语法 及其 框架 842.6 结语 86 第 
3 章 词汇 与 分词 技术 883.1 中文分词 893 . 
1.1 什么 是 词 与 分词 规范 903 . 1.2 
两种 分词 标准 933 . 1.3 歧义 机械 分词 语言 
模型 943 . 1.4 词汇 的 构成 与 未 登录 
词 973.2 系统 总体 流程 与 词典 结构 983 . 
2.1 概述 983 . 2.2 中文分词 流程 993 . 2.3 
分词 词典 结构 1033 . 2.4 命名 实体 的 词典 
结构 1053 . 2.5 词典 的 存储结构 1083.3 算法 部分 
源码 解析 1113 . 3.1 系统配置 1123 . 3.2 Main 
方法 与 例句 1133 . 3.3 句子 切分 1133 . 
3.4 分词 流程 1173 . 3.5 一元 词 网 1183 
. 3.6 二元 词 图 1253 . 3.7 NShort 算法 
原理 1303 . 3.8 后处理 规则 集 1363 . 3.9 
命名 实体 识别 1373 . 3.10 细分 阶段 与 最短 
路径 1403.4 结语 142 第 4 章 NLP 中的 概率 
图 模型 1434.1 概率论 回顾 1434 . 1.1 多元 概率论 
的 几个 基本 概念 1444 . 1.2 贝叶斯 与 朴素 
贝叶斯 算法 1464 . 1.3 文本 分类 1484 . 1.4 
文本 分类 的 实现 1514.2 信息熵 1544 . 2.1 信息量 
与 信息熵 1544 . 2.2 互信息 联合 熵 条件 熵 
1564 . 2.3 交叉 熵 和 KL 散度 1584 . 
2.4 信息熵 的 NLP 的 意义 1594.3 NLP 与 概率 
图 模型 1604 . 3.1 概率 图 模型 的 几个 
基本 问题 1614 . 3.2 产生 式 模型 和 判别式 
模型 1624 . 3.3 统计 语言 模型 与 NLP 算法 
设计 1644 . 3.4 极大 似 然 估计 1674.4 隐 
马尔科夫 模型 简介 1694 . 4.1 马尔科夫 链 1694 . 
4.2 隐 马尔科夫 模型 1704 . 4.3 HMMs 的 一个 
实例 1714 . 4.4 Viterbi 算法 的 实现 1764.5 最大熵 
模型 1794 . 5.1 从 词性 标注 谈起 1794 . 
5.2 特征 和 约束 1814 . 5.3 最大熵 原理 1834 
. 5.4 公式 推导 1854 . 5.5 对偶 问题 的 
极大 似 然 估计 1864 . 5.6 GIS 实现 1884.6 
条件 随 机场 模型 1934 . 6.1 随 机场 1934 
. 6.2 无向图 的 团 Clique 与 因子 分解 1944 
. 6.3 线性 链 条件 随 机场 1954 . 6.4 
CRF 的 概率 计算 1984 . 6.5 CRF 的 参数 
学习 1994 . 6.6 CRF 预测 标签 2004.7 结语 201 
第 5 章 词性 语 块 与 命名 实体 识别 
2025.1 汉语 词性 标注 2035 . 1.1 汉语 的 词性 
2035 . 1.2 宾州 树 库 的 词性 标注 规范 
2055.1 . 3stanfordNLP 标注 词性 2105 . 1.4 训练 模型 
文件 2135.2 语义 组块 标注 2195 . 2.1 语义 组块 
的 种类 2205 . 2.2 细说 NP 2215 . 2.3 
细说 VP 2235 . 2.4 其他 语义 块 2275 . 
2.5 语义 块 的 抽取 2295 . 2.6 CRF 的 
使用 2325.3 命名 实体 识别 2405 . 3.1 命名 实体 
2415 . 3.2 分词 架构 与 专名 词典 2435 . 
3.3 算法 的 策略 词典 与 统计 相结合 2455 . 
3.4 算法 的 策略 层叠 式 架构 2525.4 结语 259 
第 6 章 句法 理论 与 自动 分析 2606.1 转换 
生成 语法 2616 . 1.1 乔姆斯基 的 语言 观 2616 
. 1.2 短语 结构 文法 2636 . 1.3 汉 语句 
类 2696 . 1.4 谓/v 词论/i 元与空/nr 范畴/nr 2746 . 
1.5 轻 动词 分析 理论 2796 . 1.6 NLTK 操作 
句法树 2806.2 依存 句法 理论 2836 . 2.1 配价 理论 
2836 . 2.2 配价 词典 2856 . 2.3 依存 理论 
概述 2876 . 2.4 Ltp 依存 分析 介绍 2906 . 
2.5 Stanford 依存 转换 解析 2936.3 PCFG 短语 结构 句法分析 
2986 . 3.1 PCFG 短语 结构 2986 . 3.2 内向 
算法 和 外向 算法 3016 . 3.3 Viterbi 算法 3036 
. 3.4 参数估计 3046 . 3.5 Stanford 的 PCFG 算法 
训练 3056.4 结语 310 第 7 章 建设 语言 资源库 
3117.1 语料库 概述 3117 . 1.1 语料库 的 简史 3127 
. 1.2 语言 资源库 的 分类 3147 . 1.3 语料库 
的 设计 实例 国家语委 语料库 3157 . 1.4 语料库 的 
层次 加工 3217.2 语法 语料库 3237 . 2.1 中文分词 语料库 
3237 . 2.2 中文分词 的 测评 3267 . 2.3 宾州 
大学 CTB 简介 3277.3 语义 知识库 3337 . 3.1 知识库 
与 HowNet 简介 3337 . 3.2 发掘 义 原 3347 
. 3.3 语义 角色 3367 . 3.4 分类 原则 与 
事件 分类 3447 . 3.5 实体 分类 3477 . 3.6 
属性 与 分类 3527 . 3.7 相似 度 计算 与 
实例 3537.4 语义 网 与 百科 知识库 3607 . 4.1 
语义 网 理论 介绍 3607 . 4.2 维基百科 知识库 3647 
. 4.3 DBpedia 抽取 原理 3657.5 结语 368 第 8 
章 语义 与 认知 3708.1 回顾 现代 语义学 3718 . 
1.1 语义 三角 论 3718 . 1.2 语义 场论 3738 
. 1.3 基于 逻辑 的 语义学 3768.2 认知 语言学 概述 
3778 . 2.1 象 似 性 原理 3798 . 2.2 
顺序 象 似 性 3808 . 2.3 距离 象 似 
性 3808 . 2.4 重叠 象 似 性 3818.3 意象 
图式 的 构成 3838 . 3.1 主观性 与 焦点 3838 
. 3.2 范畴 化 概念 的 认知 3858 . 3.3 
主体 与 背景 3908 . 3.4 意象 图式 3928 . 
3.5 社交 中的 图式 3968 . 3.6 完形 压缩 与 
省略 3988.4 隐喻 与 转喻 4018 . 4.1 隐喻 的 
结构 4028 . 4.2 隐喻 的 认知 本质 4038 . 
4.3 隐喻 计算 的 系统 架构 4058 . 4.4 隐喻 
计算 的 实现 4088.5 构 式 语法 4128 . 5.1 
构 式 的 概念 4138 . 5.2 句法 与 构 
式 4158 . 5.3 构 式 知识库 4178.6 结语 420 
第 9 章 NLP 中的 深度 学习 4229.1 神经网络 回顾 
4229 . 1.1 神经网络 框架 4239 . 1.2 梯度 下 
降法 推导 4259 . 1.3 梯度 下 降法 的 实现 
4279 . 1.4 BP 神经网络 介绍 和 推导 4309.2 Word2Vec 
简介 4339 . 2.1 词 向量 及其 表达 4349 . 
2.2 Word2Vec 的 算法 原理 4369 . 2.3 训练 词 
向量 4399 . 2.4 大 规模 上下位 关系 的 自动 
识别 4439.3 NLP 与 RNN 4489.3 . 1Simple RNN 4499 
. 3.2 LSTM 原理 4549 . 3.3 LSTM 的 Python 
实现 4609.4 深度 学习 框架 与 应用 4679 . 4.1 
Keras 框架 介绍 4679 . 4.2 Keras 序列 标注 4719 
. 4.3 依存 句法 的 算法 原理 4789 . 4.4 
Stanford 依存 解析 的 训练 过程 4839.5 结语 488 第 
10 章 语义 计算 的 架构 49010.1 句子 的 语义 
和 语法 预处理 49010 . 1.1 长句 切分 和 融合 
49110 . 1.2 共 指 消解 49610.2 语义 角色 50210 
. 2.1 谓 词论 元与/nr 语义 角色 50210.2 . 2PropBank 
简介 50510 . 2.3 CPB 中 的 特殊 句式 50610 
. 2.4 名 词性 谓词 的 语义 角色 50910.2 . 
5PropBank 展开 51210.3 句子 的 语义 解析 51710 . 3.1 
语义 依存 51710 . 3.2 完整 架构 52410 . 3.3 
实体 关系 抽取 52710.4 结语 531 29 https / / 
blog . csdn . net / y H 0 V 
L D e 8 V G 8 e p 9 
V G e / article / details / 83747195 今天 
一个 完全 不懂 人工智能 和 计算机 的 小伙伴 问 我 
自然 语言 处理 的 工作 怎么样 我 说 这个 是 
属于 人工智能 的 一部分 他 就 搞 不 清楚 这个 
人工智能 和 计算机 是 什么 关系 我 告诉他 计算机 是 
工具 人工智能 一般 是 有 落地 产品 的 细节 上 
的 我 也 说 不好 因此 我 就 对 具体 
的 人工智能 重新 认识 了 下 人工智能 是 一门 基于 
计算机科学 生物学 心理学 神经科学 数学/n 和/c 哲学/n 等/u 学科/n 的/uj 
科学/n 和/c 技术/n 人工智能 的 一个 主要 推动力 要 开发 
与 人类 智能 相关 的 计算机 功能 例如 推理 学习 
和 解决 问题 的 能力 人工智能 的 主要 研究 方向 
有 计算机 视觉 图像识别 视频 识别 具体 应用 有 人脸识别 
步态识别 无人 驾驶 汽车 等 等 自然 语言 处理 技术 
机器翻译 语音识别 文本 挖掘 具体 siri 谷歌 翻译 数据挖掘 推荐 
和 预测 具体 电子 商务 的 商品 推荐 计算 广告 
社交 网络分析 预测 一些 趋势 股市 走向 天气 变化 人工智能 
的 特点 1 人工智能 是 人为 创造 的 只能 机械 
产品 将 人工智能 拆开 解读 可以 分为 两部分 分别 是 
人工 和 智能 即是 指人 为 创造 的 智能 机械 
产品 现阶段 的 人工智能 产品 非常 依赖 人 因为 它 
只 能够 被 人们 创造 出来 不能 自动 生成 除此以外 
在 使用 的 过程 中 还 必须 接受 人为 的 
指令 才 能够 有效 准确 地 执行 各种 动作 准确 
有效 地 运行 2 现阶段 的 人工智能 只 具备 机械 
思维 人类 拥有 情感 而 人工 智能 产品 并不 具备 
情感 人工智能 产品 之所以 能够 有效 有序 地 代替人 们 
完成 各种 工作 是 因为 人工智能 产品 具备 机械 思维 
拥有 强大 的 计算 分析 和 决策 能力 这 是 
人类 所 不具备 的 强大 能力 3 人工智能 机器 人 
能够 高效 地 代替 人们 工作 传统 的 机器人 或 
只能 产品 没有 办法 高效 地 代替 人们 工作 因为 
他们 的 智能 程度 极低 而 装载 了 人工智能 系统 
的 机器 人 能够 高效 地 代替 人们 的 工作 
只是 因为 人工智能 机器 人 具备 用户 友 好性 和 
环境 适应性 能够 更加 了解 用户 的 需求 也 能够 
根据 周围 环境 的 变化 做出 最 正确 最 恰当 
的 反应 人工智能 历史 1940 1950 一帮 来自 数学 心理学 
工程学 经济学 和 政治 学 领域 的 科学家 在 一起 
讨论 人工智能 的 可能性 当时 已经 研究 出了 人脑 的 
工作 原理 是 神经元 电脉冲 工作 1950 1956 伦 图灵 
Alan Turing 发表 了 一篇 具有 里程碑 意义 的 论文 
其中 他 预见 了 创造 思考 机器 的 可能性 重要 
事件 曼彻斯特 大学 的 Christopher Strachey 使用 Ferranti Mark 1 
机器 写 了 一个 跳棋 程序 Dietrich Prinz 写 了 
一个 国际象棋 程序 1956 达特茅斯 会议 人工智能 诞生 约翰 麦卡锡 
创造 了 人工智能 一 词 并且 演示 了 卡内基 梅隆 
大学 首个 人工智能 程序 1956 1974 推理 研究 主要 使用 
推理 算法 应用在 棋类 等 游戏 中 自然 语言 研究 
目的 是 让 计算机 能够 理解 人 的 语言 日本 
早稻田 大学 于 1967年 启动 了 WABOT 项目 并于 1972年 
完成 了 世界 上 第一个 全尺寸 智能 人形 机器人 WABOT 
1 1974 1980 由于 当时 的 计算机 技术 限制 很多 
研究 迟迟 不能 得到 预期 的 成就 这时候 AI 处于 
研究 低潮 1980 1987 在 20 世纪 80 年代 世界 
各地 的 企业 采用 了 一种 称为 专家系统 的 人工智能 
程序 知识 表达 系统 成为 主流 人工智能 研究 的 焦点 
在 同一 年 日本 政府 通过 其 第五代 计算机 项目 
积极 资助 人工智能 1982年 物理学家 John Hopfield 发明 了 一种 
神经 网络 可以 以 全新 的 方式 学习 和 处理信息 
1987 1993 第二 次 AI 研究 低潮 1993 2011 出现 
了 智能 代理 它 是 感知 周围环境 并 采取 最 
大限 度提高 成功 的 机会 的 系统 这个 时期 自然语言 
理解 和 翻译 数据挖掘 Web 爬虫 出现 了 较大 的 
发展 里程碑 的 事件 1997年 深蓝 击败 了 当时 的 
世界 象棋 冠军 Garry Kasparov 2005年 斯坦福 大学 的 机器人 
在 一条 没有 走过 的 沙漠 小路上 自动驾驶 131 英里 
2011年 至今 在 深度 学习 大/a 数据/n 和强/nr 人工智能/n 的/uj 
发展/vn 迅速/ad 人工智能 的 发展 对 人类 的 影响 1 
人工智能 给 人类 带来 新生 人工智能 需要 人类 操作 或 
下达 命令 才能 执行 相应 地 动作 完成 相应 的 
任务 如果 没有 人类 那么 人工智能 仅仅 是 一堆 破铜烂铁 
2 人工智能 将 进一步 发展 更好 地 帮助 人类 人工智能 
可能 有助于 太空 殖民 或者 地球上 的 数字 社会 3 
人工智能 将 丰富 人类 的 精神 世界 人工智能 能够 帮助 
人们 完成 更多 的 工作 全面 解放 人手 人类 的 
生活 将 变得 悠闲 而 惬意 不再 需要 进行 大量 
繁重 的 工作 同时 参考 https / / baijiahao . 
baidu . com / s id = 1 6 1 
1 0 1 4 1 3 0 4 2 5 
0 3 5 6 9 8 & wfr = spider 
& for = pc 过去 半年 以来 自然语言 处理 领域 
进化 出了 一件 神器 此 神器 乃是 深度 神经 网络 
的 一种 新模式 该 模式 分为 embed encode attend predict 
四 部分 本文 将对 这四个 部分 娓娓道来 并且 剖析 它 
在 两个 实例 中的 用法 人们 在 谈论 机器学习 带来 
的 提升 时 往往 只 想到 了 机器 在 效率 
和 准确率 方面 带给 人们 的 提升 然而 最 重要 
的 一点 却是 机器学习 算法 的 通用性 如果 你 想 
写 一段 程序 来 识别 社交 媒体 平台 上 的 
侮辱性 帖子 就把 问题 泛 化为 需要 输入 一段 文本 
预测出 文本 的 类别 ID 这种 分类 与 识别 侮辱性 
帖子 或是 标记 电子邮件 类别 之类 的 具体 任务 无关 
如果/c 两个/m 问题/n 的/uj 输入/v 和/c 输出/v 类型/n 都/d 一致/d 
那/r 我们/r 就/d 应/v 复用/v 同/p 一套/m 模型/n 的/uj 代码/n 
两者 的 区别 应该 在于 送入 的 训练 数据 不同 
就像 我们 使用 同 一个 游戏 引擎 玩 不同 的 
游戏 笔者 用 spaCy 和 Keras 实现 了 自然 语言 
推理 的 可分解 注意力 模型 代码 已经 上 传到 github 
假设 你 有 一项 强大 的 技术 可以 预测 实数 
稠密 向量 的 类别 标签 只要 输入 输出 的 格式 
相同 你 就 能用 这项 技术 解决 所有 的 问题 
与此同时 你/r 有/v 另一/i 项/n 技术/n 可以 用 一个 向量 
和 一个 矩阵 预测出 另一个 向量 那么 现在 你 手里 
就 握着 三类 问题 的 解决 方案 了 而 不是 
两类 为什么 是 三类 呢 因为 如果 第三 类 问题 
是 通过 矩阵 和 一个 向量 得到 一个 类别 标签 
显然 你 可以 组合 利用 前 两种 技术 来 解决 
大多数 NLP 问题 可以 退 化成 输入 一条 或 多条 
文本 的 机器 学习 问题 如果 我们 能将/nr 这些 文本 
转化 为 向量 我们 就 可以 复用 现有 的 深度 
学习 框架 接下来 就是 具体 的 做法 文 本类 深度 
学习 的 四部曲 嵌入式 词语 表示 也 被 称为 词 
向量 是 现在 最 广泛 使用 的 自然 语言 处理 
技术 之一 词 向量 表示 是 一种 既能 表示 词 
本身 又 可以 考虑 语义 距离 的 表示 方法 然而 
大多数 NLP 问题 面对 的 不是 单个 词语 而是 需要 
分析 更长 的 文本 内容 现在 有 一个 简单 而 
灵活 的 解决方案 它 在 许多 任务 上 都 表现 
出 了 卓越 的 性能 即 RNN 模型 将 文本 
用 一个 向量 的 序列 表示 之后 使用 双向 RNN 
模型 将 向量 编码 为 一个 句子 向量 矩阵 这个 
矩阵 的 每 一行 可以 理解 为 词 向量 它们 
对 句子 的 上下文 敏感 最后 一步 被 称为 注意力 
机制 这 可以 将 句子 矩阵 压缩 成 一个 句子 
向量 用于 预测 第一步 词/n 向/p 量词/n 向/p 量表/n 将/d 
高维/nr 的/uj 稀疏/a 二/m 值/n 向量/n 映/v 射成/v 低维/i 的/uj 
稠密/a 向量/n 举个 例子 假设 我们 收到 的 文本 是 
一串 ASCII 字符 共有 256种 可能 值 于是 我们 把 
每一种 可能 值 表示 为 一个 256 维 的 二 
值 向量 字符 a 的 向量 只有在 第 97 维 
的 值 等于 1 其它 维度 的 值 都 等于 
0 字符 b 的 向量 只有在 第 98 维 的 
值 等于 1 其它 维度 的 值 都 等于 0 
这种 表示 方法 称为 one hot 形式 不同 字符 的 
向量 表示 完全 不 一样 大部分 神经网络 模型 首先 都 
会把 输入 文本 切 分成 若干 个 词语 然后 将 
词语 都 用词 向量 表示 另 一些 模型 用 其它 
信息 扩展 了 词 向量 表示 比如 除了 词语 的 
ID 之外 还会 输入 一串 标签 然后 可以 学习 得到 
标签 向量 将 标签 向量 拼接 为 词 向量 这 
可以 让 你 将 一些 位置 敏感 的 信息 加入到 
词 向量 表示 中 然而 有 一个 更 强大 的 
方式 来使 词语 表示 呈现出 语境 相关 第二步 编码 假设 
得到 了 词 向量 的 序列 编码 这 一步 是 
将其 转化 为 句子 矩阵 矩阵 的 每 一行 表示 
每个 词 在 上下 文中 所 表达 的 意思 这一步 
用到 了 双向 RNN 模型 LSTM/w 和/c GRU/w 结构/n 的/uj 
模型/n 效果/n 都/d 不错/a 每一 行向量 通过 两 部分 计算 
得到 第一 部分 是 正向 计算 第二 部分 是 逆向 
计算 然后 拼接 两 部分 得到 完整 的 向量 计算 
过程 如 下图 代码 所示 我 个人 认为 双向 RNN 
会 是 今后 的 主流 RNN 的 主要 应用 是 
读入 文本 内容 然后 从中/nr 预测 出 一些 信息 而 
我们 是 用 它 来 计算 一个 中间 表达 状态 
最 重要 的 一点 是 得到 的 表达 能够 反映 
词语 在 文中 的 意义 理论 上 应该 学到 pick 
up 与 pick on 这 两个 词语 的 意义 有 
区别 这 一直 是 NLP 模型 的 巨大 弱点 现在 
我们 有了/nr 一个 解决方案 第三步 注意力 机制 这 一步 是 
将上 一步 的 矩阵 表示 压缩 为 一个 向量 表示 
因此 可以 被 送入 标准 的 前馈 神经 网络 进行 
预测 注意力 机制 对于 其它 压缩 方法 的 优势 在于 
它 输入 一个 辅助 的 上下文 向量 Yang 等 人在 
2016年 发表 的 论文 提出 了 一种 注意力 机制 输入 
一个 矩阵 输 出 一个 向量 区别于 从 输入 内容 
中 提取 一个 上下文 向量 该 机制 的 上下文 向量 
是 被 当做 模型 的 参数 学习 得到 这 使得 
注意 机制 变成 一个 纯粹 的 压缩 操作 可以 替换 
任何 的 池 化步骤 第四步 预测 文本 内容 被 压缩 
成 一个 向量 之后 我们 可以 学习 最终 的 目标 
表达 一种 类别 标签 一个 实数值 或 是 一个 向量 
等等 我们 也 可以 将 网络 模型 看做 是 状态机 
的 控制器 如 一个 基于 转移 的 解析器 来做 结构化 
预测 有趣 的 是 大 部分 的 NLP 模型 通常 
更 青睐 浅层 的 前馈 网络 这 意味着 近期 在 
机器 视觉 领域 取得 的 重要 技术 至今 为止 并 
没有 影响 到 NLP 领域 比如 residual connections 和 batch 
normalization 实例 1 自然语言 推测 的 可分解 注意力 模型 自然语言 
推测 是 给 一对 句子 预测 类别 标签 的 问题 
类别 标签 则 表示 它们 两者 的 逻辑 关系 斯坦福 
自然语言 预测 文 本集 使用 三种 类别 标签 1 . 
推演 Entailment 如果 第一 句话 是 真的 那么 第二 句话 
一定 为真 2 . 矛盾 Contradiction 如果 第一 句话 是 
真的 那么 第二 句话 一定 为 假 3 . 中性 
Neutral 上述 两者 都 不是 Bowman 等人 在 论文 中 
给出 了 几条 例子 文本 内容 假设 内容 标签 某人 
正在 检查 一位 来自 中亚国家 人士 的 服装 此人 正在 
睡觉 矛盾 一位 长者 和 一位 青年 在 微笑 两个人 
在笑 嘲笑 地板 上 玩耍 的 猫 中性 一辆 黑色 
赛车 在 人群 前面 启动 一个 男人 正 沿着 一条 
孤独 的 路 行驶 矛盾 一种 多个 男性 玩 的 
足球 游戏 几位 男性 正在进行 体育运动 推演 一位 微笑 盛装 
打扮 的 女性 拿着 一把伞 一位 快乐 的 女性 在 
一个 童话 服装 会上 握着 一把伞 中性 这份 语料库 的 
目的 之一 是 为 我们 提供 一个 新的 规模 合适 
的 语料库 用于 研发 将 句子 编码 为 向量 的 
模型 例如 Bowman 在 2016年 发表 的 论文 介绍 了 
一种 基于 转移 的 模型 它 依次 读入 句子 构建 
一种 树形 结构 的 内部 表达 Bowman 他们 的 准确率 
达到 了 83.2% 比 之前 的 工作 成果 提升 了 
一大 截 过了 不到 半年 Parikh 的 论文 提出 的 
模型 取得 了 86.8% 的 准确率 而 使用 的 模型 
参数 数量 只有 Bowman 模型 的 10% 不久之后 Chen 等人 
发表 的 论文 提出 了 一种 效果 更好 的 系统 
准确率 达到 88.3% 当 我 第一 次 阅读 Parikh 的 
论文 时 我 无法 理解 他们 的 模型 如何 取得 
这么 好 的 效果 原因 在于 他们 的 模型 用 
独特 的 注意力 机制 融合 了 两个 句子 矩阵 关键 
的 优势 是 他们 讲 句子 转为 向量 的 压缩 
步骤 合并 完成 而 Bowman 他们 则 是 分别 将 
两个 句子 转为 向量 请 记住 Vapnik 的 原则 当 
解决 一个 关键 问题 时 不 要 解决 一个 更 
一般 的 问题 作为 中间 步骤 VLADIMIR VAPNIKParikh 的 论文 
将 自然 语言 推测 任务 当做 是 关键 问题 他们 
想 办法 直接 解决 这个 问题 因此 比 单独 给 
句子 编码 有 巨大 的 优势 Bowman 等人 则 更 
关注 问题 的 泛化 也 是 针对 此 构建 模型 
他们 的 模型 适用 的 场景 也 就比 Parikh 的 
模型 更 广泛 比如说 利用 Bowman 的 模型 你 可以 
缓存 句子 向量 使得 计算 句子 相似 度 的 效率 
更高 实例 2 文档 分类 的 分层 注意力 网络 给 
文档 分类 是 我 接触 到 的 第一 个 NLP 
项目 澳大利亚 的 类似 证券 交易所 的 机构 资助 了 
一个 项目 爬 取 澳大利亚 的 网站 页面 并且 自动 
检测 金融 诈骗 尽管 这个 项目 已经 过去 了 一段 
时间 但是 文档 分类 的 方法 在 之后 的 十年 
中 几乎 没有 变化 这 也是 我 看到 Yang 等人 
发表 的 分层 注意力 网络 模型 之后 如此 兴奋 的 
原因 这是 继 词 袋 模型 之后 我 看到 的 
第一 篇 真正 有 通用性 改进 的 论文 下面 是 
它 的 原理 该 模型 接收 一篇 文档 作为 输入 
文档 由 句子 的 序列 组成 其中 每个 句子 是 
一个 词语 的 序列 每 句话 的 每个 词语 分别 
编码 生成 两个 词 向量 序列 每个 序列 表示 一个 
句子 这 两个 序列 分别 编码 成 两个 句子 矩阵 
然后 由 注意力 机制 将 句子 矩阵 压缩 为 句子 
向量 多个 句子 向量 又 组成 文本 矩阵 最后 一步 
注意力 操作 将 文本 矩阵 压缩 为 文本 向量 然后 
送入 最终 的 预测 网络 来 预测 类别 标签 该 
模型 使用 注意 机制 作为 一个 纯粹 的 压缩 步骤 
它 学会 了 把 矩阵 作为 输入 然后 将其 概括 
成 一个 向量 这种 学习 过程 是 通过 学习 上下文 
向量 的 两个 注意力 转换 我们 可以 将 这种 转换 
理解 为 表示 模型 认为 相关 的 词语 或者 句子 
该 模型 会 找到 理想 相关 或者 你 也 可以 
把 整个 压缩 过程 看做 是 特征提取 的 过程 按照 
这种 观点 上下文 向量 只是 另 一个 不 透明 的 
参数 作者 方法 YELP 13YELP 14YELP 15IMDBYang et al . 
2016 HN ATT68 . 270.57149 . 4Yang et al . 
2016 HN AVE6769 . 369.947 . 8Tang et al . 
2015 Paragraph Vector57 . 759 . 260.534 . 1Tang et 
al . 2015 SVM + Bigrams57 . 661 . 662.440 
. 9Tang et al . 2015 SVM + Unigrams58 . 
96061.139 . 9Tang et al . 2015 CNN word59 . 
76161 . 537.6 将 yang 等人 的 模型 与 卷积 
神经 网络 做 比较 可以 得到 有意思 的 结果 两个/m 
模型/n 都能/nr 自动/vn 提取/v 位置/v 敏感/a 特征/n 然而 CNN 模型 
既不 通用 效率 也 较低 而 双向 RNN 模型 只 
需要 对 每个 句子 读入 两次 正向 一次 反向 一次 
LSTM 编码 还能 提取 任意 长度 的 特征 因为 句子 
上下文 的 任何 信息 都 有可能 被 揉 入 词语 
的 向量 表示 将 句子 矩阵 压缩成 向量 的 步骤 
简单 并且 有效 的 要 构建 文档 向量 只 需要 
对 句子 向量 再 进行 一次 同样 的 操作 提升 
模型 准确率 的 主要 因素 是 双向 LSTM 编码器 它 
创建 了 位置 敏感 的 特点 作者 通过 将 注意力 
机制 替换 为 平均 池化/nr 证明 了 上述 观点 使用 
平均 池化的/nr 方法 该 模型 在 所有 测试数据 上 仍然 
优于 以前 的 最好 模型 然而 注意力 机制 进一步 普遍 
地 提高 了 性能 后续 内容 我 已经 用 我们 
自己 的 NLP 库 spaCy 实现 了 第一 个 例子 
我 正在 实现 文本 分类 的 系统 我们 还 计划 
开发 一个 SpaCy 版 的 通用 双向 LSTM 模型 能够 
方便 地 将 预 训练 的 词 向量 用于 实际 
问题 中 来源 AINLP 本文 约 1300字 建议 阅读 5 
分钟 本文 为 你 推荐 中文 自然语言 处理 数据集 推荐 
一个 Github 项目 ChineseNLPCorpus 该 项目 收集 了 一批 中文 
自然语言 处理 数据集 的 相关 链接 可以 用来 练手 点击 
阅读 原文 可以 直达 该 项目 链接 https / / 
github . com / InsaneLife / ChineseNLPCorpus 以下 来自 该 
项目 介绍 页 中文 自然语言 处理 数据集 平时 做 做 
实验 的 材料 欢迎 补充 提交 合并 文本 分类 新闻 
分类 今日 头条 中文 新闻 短 文本 分类 数据集 https 
/ / github . com / fateleak / toutiao text 
classfication dataset 数据 规模 共 38 万条 分布 于 15个 
分类 中 采集 时间 2018年 05月 以 0.7 0.15 0.15 
做 分割 清华 新闻 分类 语料 根据 新浪 新闻 RSS 
订阅 频道 2005 ~ 2011 年间 的 历史 数据 筛选 
过滤 生成 数据量 74万 篇 新闻 文档 2.19 GB 小 
数据 实验 可以 筛选 类别 体育 财经 房产 家居 教育 
科技 时尚 时政 游戏 娱乐 http / / thuctc . 
thunlp . org / # % E 8% 8E % 
B 7% E 5% 8F % 96% E 9% 93% 
BE % E 6% 8E % A5rnn 和 cnn 实验 
https / / github . com / gaussic / text 
classification cnn rnn 中科大 新闻 分类 语料库 http / / 
www . nlpir . org / action viewnews itemid 145 
情感 / 观点 / 评论 倾向性 分析 数据集 数据 概览 
ChnSentiCorp _ htl _ all7000 多条 酒店 评论 数据 5000 
多条 正向 评论 2000 多条 负向 评论 waimai _ 10k 
某 外卖 平台 收集 的 用户 评价 正向 4000 条 
负向 约 8000 条 online _ shopping _ 10 _ 
cats10 个 类别 共 6 万多条 评论 数据 正 负向 
评论 各 约 3 万条 包括 书籍 平板 手机 水果 
洗发水 热水器 蒙牛 衣服 计算机 酒店 weibo _ senti _ 
100k10 万多条 带 情感 标注 新浪 微博 正 负向 评论 
约 各 5 万条 simplifyweibo _ 4 _ moods36 万多条 
带 情感 标注 新浪 微博 包含 4 种 情感 其中 
喜悦 约 20 万条 愤怒 厌恶 低落 各 约 5 
万条 dmsc _ v228 部 电影 超 70 万 用户 
超 200 万条 评分 / 评论 数据 yf _ dianping24 
万 家餐馆 54 万 用户 440 万条 评论 / 评分 
数据 yf _ amazon52 万件 商品 1100 多个 类目 142 
万 用户 720 万条 评论 / 评分 数据 实体 识别 
& 词性 标注 微博 实体 识别 https / / github 
. com / hltcoe / golden horseboson 数据 包含 6种 
实体 类型 https / / github . com / InsaneLife 
/ ChineseNLPCorpus / tree / master / NER / boson1998 
年 人民日报 数据集 人名 地名 组织 名 三种 实体 类型 
https / / github . com / InsaneLife / ChineseNLPCorpus 
/ tree / master / NER / renMinRiBaoMSRA 微软 亚洲 
研究院 数据集 5 万多条 中文 命名 实体 识别 标注 数据 
包括 地点 机构 人物 https / / github . com 
/ InsaneLife / ChineseNLPCorpus / tree / master / NER 
/ MSRASIGHAN Bakeoff 2005 一 共有 四个 数据集 包含 繁体中文 
和 简体中文 下面 是 简体 中文分词 数据 MSR   http 
/ / sighan . cs . uchicago . edu / 
bakeoff2005 / PKU http / / sighan . cs . 
uchicago . edu / bakeoff2005 / 搜索 匹配 OPPO 手机 
搜索 排序 OPPO 手机 搜索 排序 query title 语义 匹配 
数据集 下载 链接 https / / pan . baidu . 
com / s / 1Obm8oRVZEIh76 cpPc0qZw 网页 搜索 结果 评价 
SogouE 用户 查询 及 相关 URL 列表 https / / 
www . sogou . com / labs / resource / 
e . php 推荐 系统 数据集 数据 概览 ez _ 
douban5 万 多部 电影 3 万多 有 电影 名称 2 
万多 没有 电影名称 2.8 万 用户 280 万条 评分 数据 
dmsc _ v228 部 电影 超 70 万 用户 超 
200 万条 评分 / 评论 数据 yf _ dianping24 万 
家餐馆 54 万 用户 440 万条 评论 / 评分 数据 
yf _ amazon52 万件 商品 1100 多个 类目 142 万 
用户 720 万条 评论 / 评分 数据 百科 数据 维基百科 
维基百科 会 定时 将 语料库 打包 发布 数据处理 博客 https 
/ / dumps . wikimedia . org / zhwiki / 
百度 百科 只能 自己 爬 爬 取得 链接 https / 
/ pan . baidu . com / share / init 
surl = i3wvfil 提 取码 neqs 指代 消 歧 CoNLL 
2012 http / / conll . cemantix . org / 
2012 / data . html 预 训练 词 向量 or 
模型 BERT 开 源代码 https / / github . com 
/ google research / bert 模型 下载 BERT Base Chinese 
Chinese Simplified and Traditional 12 layer 768 hidden 12 heads 
110M parametersELMO 开 源代码 https / / github . com 
/ allenai / bilm tf 预 训练 的 模型 https 
/ / allennlp . org / elmo 腾讯 词 向量 
腾讯 AI 实验室 公开 的 中文 词 向量 数据集 包含 
800 多万 中文 词汇 其中 每个 词 对应 一个 200 
维 的 向量 下载 地址 https / / ai . 
tencent . com / ailab / nlp / embedding . 
html 上 百种 预 训练 中 文词 向量 下载 地址 
https / / github . com / Embedding / Chinese 
Word Vectors 中文 完形填空 数据集 下载 地址 https / / 
github . com / ymcui / Chinese RC Dataset 中华 
古诗词 数据库 最全 中华 古诗词 数据集 唐/nr 宋两朝/nr 近/a 一万四千/m 
古/a 诗人/n 接近 5.5 万首 唐诗 加 26万 宋诗 . 
两宋 时期 1564位 词人 21050首 词 下载 地址 https / 
/ github . com / chinese poetry / chinese poetry 
保险行业 语料库 下载 地址 https / / github . com 
/ Samurais / insuranceqa corpus zh 汉语 拆字 字典 英文 
可以 做 char embedding 中文 不妨 可以 试试 拆字 下载 
地址 https / / github . com / kfcd / 
chaizi 中文 数据集 平台 搜狗 实验室 搜狗 实验室 提供 了 
一些 高 质量 的 中文 文本 数据集 时间 比较 早 
多为 2012年 以前 的 数据 https / / www . 
sogou . com / labs / resource / list _ 
pingce . php 中科大 自然语言 处理 与 信息检索 共享 平台 
http / / www . nlpir . org / action 
category catid 28 中文 语料 小 数据 包含 了 中文 
命名 实体 识别 中文 关系 识别 中文 阅读 理解 等 
一些 小量 数据 https / / github . com / 
crownpku / Small Chinese Corpus 维基百科 数据集 https / / 
dumps . wikimedia . org / NLP 工具 THULAC https 
/ / github . com / thunlp / THULAC   
包括 中文分词 词性 标注 功能 HanLP https / / github 
. com / hankcs / HanLP 哈工大 LTP   https 
/ / github . com / HIT SCIR / ltpNLPIR 
  https / / github . com / NLPIR team 
/ NLPIRjieba   https / / github . com / 
yanyiwu / cppjieba 编辑 于腾凯/nr 人工智能 工程师 学习 路线 / 
自然语言 处理 算法 工程师 学习 路径 人工智能 工程师 学习 路线 
自然语言 处理 算法 工程师 学习 路径 1 入门 级别 1 
数据结构 2 算法 重点 3python2 进阶 阶段 1 机器学习 算法 
2 深度 学习 算法 3 深度 学习 框架 4 大 
数据 计算 框架 3 高阶 1 强化 学习 2 迁移 
学习 3 自然语言 处理 1 入门 级别 1.1 数据结构 1.2 
算法 重点 面试 必考 参考 学习 地址 麻省理工学院 公开课 算法导论 
http / / open . 163 . com / special 
/ opencourse / algorithms . html1 . 3python 包括 python 
基础 面向对象 要 懂 2 进阶 阶段 2.1 机器学习 算法 
特征 工程 特征分析 监督 学习 算法 非 监督 学习 算法 
参考 学习 地址 1 . Coursera 斯坦福 吴恩 达 课程 
❤ ❤ ❤ 2 . 能 使用 sklearn 解决 一些 
小 的 机器 学习 任务 参考 书本 西瓜 书 2.2 
深度 学习 算法 视频 1 . Andrew Ng 吴恩 达 
深度 学习 专项 课程 by Coursera and deeplearning . ai 
❤ ❤ ❤ 2 . 或者 Hinton 大神 的 coursera 
面向 机器 学习 的 神经 网络 3 . Udacity 深度 
学习 中 / 英 by Google 你 将 通过 项目 
和 任务 接触 完整 的 机器学习 系统 TensorFlow 书 AI 
圣经 深度 学习 2.3 深度 学习 框架 kerastensorflow 掌握 好 
编程 的 利器 参考 视频 资料 1 . 斯坦福大学 深度 
学习 课程 CS 20SI Tensorflow for Deep Learning Research 准确 
的 说 这门 课程 主要 是 针对 深度 学习 工具 
Tensorflow 的 ❤ ❤ ❤ 2.4 大 数据 计算 框架 
hadoopspark 因为 深度 学习 工程师 一般 面对 的 是 大 
数据 所以 公司 的 分布式 计算 平台 要 熟悉 会用 
3 高阶 3.1 强化 学习 理论 与 实践 3.2 迁移 
学习 理论 与 实践 3.3 自然语言 处理 斯坦福 课程 深度 
学习 应用 课程 这门 课程 融合 了 两位 授课 者 
之前 在 斯坦福 大学 的 授课 课程 分别 是 自然 
语言 处理 课程 cs224n Natural Language Processing 和 面向 自然语言 
处理 的 深度 学习 课程 cs224d Deep Learning for Natural 
Language Processing . 牛津大学 Deep Learning for Natural Language Processing 
2016 2017 深度 NLP http / / study . 163 
. com / course / introduction / 1004336028 . htm 
❤ ❤ ❤ 1 . 数学 之美 吴军 这个 书写 
得 特别 生动 形象 没有 太多 公式 科普 性质 看完 
对于 nlp 的 许多 技术 原理 都会 有 初步 认识 
可以 说 是 自然 语言 处理 最好 的 入门 读物 
链接 https / / pan . baidu . com / 
s / 1eSphCSa 密码 59je . 2 . 如何 在 
NLP 领域 第一次 做 成 一件 事 by 周明 微软 
亚洲 研究院 首席 研究员 自然语言 处理 顶 会 ACL 候任 
主席 http / / www . msra . cn / 
zh cn / news / features / nlp 201611243 . 
深度 学习 基础 by 邱锡鹏/nr 复旦 大学 2017年 8月 17日 
206页 PPT 带 你 全面 梳理 深度 学习 要点 http 
/ / nlp . fudan . edu . cn / 
xpqiu / slides / 20170817 CIPS ATT DL . pdf 
https / / nndl . github . io / 4 
. Deep learning for natural language processing 自然语言 处理 中 
的 深度 学习 by 邱锡鹏/nr 主要 讨论 了 深度 学习 
在 自然 语言 处理 中 的 应用 其中 涉及 的 
模型 主要 有 卷积 神经网络 递归 神经网络 循环 神经网络 网络 
等 应用 领域 主要 包括 了 文本 生成 问答 系统 
机器翻译 以及 文本 匹配 等 http / / nlp . 
fudan . edu . cn / xpqiu / slides / 
20160618 _ DL4NLP @ CityU . pdf5 . Deep Learning 
NLP and Representations 深度 学习 自然语言 处理 及其 表达 来自 
著名 的 colah s blog 简要 概述 了 DL 应用于 
NLP 的 研究 重点 介绍 了 Word Embeddings http / 
/ colah . github . io / posts / 2014 
07 NLP RNNs Representations / 翻译 http / / blog 
. csdn . net / ycheng _ sjtu / article 
/ details / 48520293 \ 6 . 中文 信息 发展 
报告 by 中国 中文信息 学会 2016年 12月 是 一份 非常 
好 的 中文 NLP 总览 性质 的 文档 通过 这份 
报告 可以 了解 中文 和 英文 NLP 主要 的 技术 
方向 链接 http / / cips upload . bj . 
bcebos . com / cips2016 . pdf7 . Deep Learning 
in NLP 一 词 向量 和 语言 模型 by Lai 
Siwei 来 斯 惟 中科院 自动化所 2013 比较 详细 的 
介绍 了 DL 在 NLP 领域 的 研究 成果 系统 
地 梳理 了 各种 神经 网络 语言 模型 链接 http 
/ / licstar . net / archives / 3288 . 
语义分析 的 一些 方法 一 二 三 by 火光 摇曳 
腾讯 广点通/nr 链接 http / / www . flickering . 
cn / ads / 2015 / 02/9 . 我们 是 
这样 理解 语言 的 3 神经 网络 语言 模型 by 
火光 摇曳 腾讯 广点通/nr 总结 了 词 向量 和 常见 
的 几种 神经 网络 语言 模型 链接 http / / 
www . flickering . cn / nlp / 2015 / 
03/10 . 深度 学习 word2vec 笔记 之 基础 篇 by 
falao _ beiliu http / / blog . csdn . 
net / mytestmy / article / details / 2696131511 . 
Understanding Convolutional Neural Networks for NLP 卷积 神经 网络 在 
自然 语言 处理 的 应用 by WILDML 链接 http / 
/ www . wildml . com / 2015/11 / understanding 
convolutional neural networks for nlp 翻译 http / / www 
. csdn . net / article / 2015 11 11/282619212 
. The Unreasonable Effectiveness of Recurrent Neural Networks . 循环 
神经网络 惊人 的 有效性 by Andrej Karpathy 链接 http / 
/ karpathy . github . io / 2015 / 05/21 
/ rnn effectiveness / 翻译 https / / zhuanlan . 
zhihu . com / p / 2210771513 . Understanding LSTM 
Networks 理解 长短期 记忆 网络 LSTM NetWorks by colah 链接 
http / / colah . github . io / posts 
/ 2015 08 Understanding LSTMs / 翻译 http / / 
www . csdn . net / article / 2015 11 
25/2826323 ref = myread14 . 注意力 机制 Attention Mechanism 在 
自然 语言 处理 中 的 应用 by robert _ ai 
链接 http / / www . cnblogs . com / 
robert dlut / p / 5952032 . html15 . 初学者 
如何 查阅 自然语言 处理 NLP 领域 学术 资料 刘知远 链接 
http / / blog . sina . com . cn 
/ s / blog _ 574a437f01019poo . html \ 安装 
NLTK1 . 3 整合 中文分词 模块 按照 使用 的 算法 
不同 下面 介绍 两大类 中文分词 模块 基于 条件 随 机场 
CRF 的 中文分词 算法 的 开源 系统 基于 张 华平 
的 NShort 的 中文分词 算法 的 开源 系统 安装 Ltp 
Python 组件 https / / github . com / HIT 
SCIR / ltp 下载 源代码 wget   https / / 
github . com / HIT SCIR / ltp / archive 
/ v3 . 4.0 . tar . gz 下载 语言 
模型 http / / ospm9rsnd . bkt . clouddn . 
com / model / ltp _ data _ v3 . 
4.0 . zip     http / / ospm9rsnd . 
bkt . clouddn . com / model / ltp _ 
data _ v3 . 3.0 . zip 源代码 和 语言 
模型 包括 中文分词 词性 标注 未 登录 词 识别 依存 
句法 语义 角色 标注 几个 模块 将 项目 与 Python 
整合 pip install pyltp 部署 语言 模型库 解压 使用 Ltp 
进行 中文分词 1 # * coding utf 8 * import 
sys import os from pyltp import Segmentor reload sys sys 
. s e t d e f a u l 
t e n c o d i n g utf 
8 model _ path = ltp3 . 4 / cws 
. model segmentor = Segmentor segmentor . load model _ 
path words = segmentor . segment 在/p 包含/v 问题/n 的/uj 
所有/b 解的解/nr 空间/n 树/v 中/f 按照 深度 优先 搜索 的 
策略 从根/nr 节点 出发 深度 探 索解 空间 树 print 
| . join words 在 | 包含 | 问题 | 
的 | 所有 | 解 | 的 | 解 | 
空间 | 树 | 中 | | 按照 | 深度 
| 优先 | 搜索 | 的 | 策略 | | 
从 | 根 节点 | 出发 | 深度 | 探索 
| 解 | 空间 | 树 | 2 分词 结果 
的 后处理 上述 分词 粒度 过细 为了 获得 更 精确 
的 结果 可以 将 错分 的 结果 合并 为 专有名词 
这 就是 分词 结果 的 后 处理 过程 即 一般 
外部 用户 词典 的 构成 原理 postdict = { 解 
| 空间 解 空间 深度 | 优先 深度 优先 } 
seg _ sent = | . join words for key 
in postdict seg _ sent = seg _ sent . 
replace key postdict key print seg _ sent 在 | 
包含 | 问题 | 的 | 所有 | 解 | 
的 | 解 空间 | 树 | 中 | | 
按照 | 深度 优先 | 搜索 | 的 | 策略 
| | 从 | 根 节点 | 出发 | 深度 
| 探索 | 解 空间 | 树 | 3 现在 
加入 用户 词典 词典 中 登录 一些 新词 如 解 
空间 user _ dict = ltp3 . 4 / fulluserdict 
. txt # 外部 专有名词 词典 segmentor1 = Segmentor segmentor1 
. load _ with _ lexicon model _ path user 
_ dict # 加载 专有名词 词典 sent = 在/p 包含/v 
问题/n 的/uj 所有/b 解的解/nr 空间/n 树/v 中/f 按照 深度 优先 
搜索 的 策略 从根/nr 节点 出发 深度 探 索解 空间 
树 words = segmentor . segment sent print | . 
join words 使用 结巴 分词 模块 张 华平 的 NShort 
的 中文分词 算法 是 目前 大 规模 中文分词 的 主流 
算法 在 商用 领域 大多数 搜索引擎 公司 都 使用 该 
算法 作为 主要 的 分词 算法 具有 算法 原理 简单 
容易 理解 便于 训练 大 规模 分词 的 效率 高 
模型 支持 增量 扩展 模型 占用 资源 低等 优势 这里 
使用 的 结巴 分词器 是 该 算法 的 Python 实现 
结巴 分词 的 算法 核心 就是 Nshort 中文分词 算法 https 
/ / github . com / fxsjy / jieba 结巴 
分词 模块 可支持 如下 三种 分词 方式 精确 模式 试图 
将 句子 最 精确地 切开 适合 文本 分析 类似 Ltp 
的 分词 方式 全 模式 把/p 句子/n 中/f 所有/b 可以/c 
成词的/nr 词语/n 都/d 扫描/v 出来/v 速度 非常 块 但是 不能 
解决 歧义 搜索引擎 模式 在 精确 模式 的 基础上 对 
长词 再次 切分 提高 召回率 适合 用于 搜索引擎 分词 支持 
繁体 分词 支持 基于 概率 的 用户 词典 1 安装 
pip install jieba 2 使用 结巴 分词 # * coding 
utf 8 * import sys import os import jieba reload 
sys sys . s e t d e f a 
u l t e n c o d i n 
g utf 8 sent = 在/p 包含/v 问题/n 的/uj 所有/b 
解的解/nr 空间/n 树/v 中/f 按照 深度 优先 搜索 的 策略 
从根/nr 节点 出发 深度 探 索解 空间 树 wordlist = 
jieba . cut sent cut _ all = True # 
全 模式 print | . join wordlist wordlist = jieba 
. cut sent # 精确 模式 print | . join 
wordlist wordlist = jieba . cut _ for _ search 
sent # 搜索引擎 模式 print | . join wordlist 在 
| 包含 | 问题 | 的 | 所有 | 解 
| 的 | 解空 | 空间 | 树 | 中 
|   |   | 按照 | 深度 | 优先 
| 搜索 | 的 | 策略 |   |   
| 从 | 根 | 节点 | 点出 | 出发 
| 深度 | 探索 | 索解 | 解空 | 空间 
| 树 |   | 在 | 包含 | 问题 
| 的 | 所有 | 解 | 的 | 解 
| 空间 | 树 中 | | 按照 | 深度 
| 优先 | 搜索 | 的 | 策略 | | 
从根/nr | 节点 | 出发 | 深度 | 探索 | 
解 | 空间 | 树 | 在 | 包含 | 
问题 | 的 | 所有 | 解 | 的 | 
解 | 空间 | 树 中 | | 按照 | 
深度 | 优先 | 搜索 | 的 | 策略 | 
| 从根/nr | 节点 | 出发 | 深度 | 探索 
| 解 | 空间 | 树 | 3 使用 用户 
词典 ○ → cat userdict . txt 解 空间 5 
n 解 空间 树 5 n 根 结点 5 n 
深度 优先 5 njieba . load _ userdict userdict . 
txt wordlist = jieba . cut sent cut _ all 
= True # 全 模式 print | . join wordlist 
在 | 包含 | 问题 | 的 | 所有 | 
解 | 的 | 解空 | 解 空间 | 解 
空间 树 | 空间 | 树 | 中 |   
|   | 按照 | 深度 | 深度 优先 | 
优先 | 搜索 | 的 | 策略 |   | 
  | 从 | 根 | 节点 | 点出 | 
出发 | 深度 | 探索 | 索解 | 解空 | 
解 空间 | 解 空间 树 | 空间 | 树 
|   | 1.4 整合 词性 标注 模块 词性 标注 
Part of speech tagging 或者 POS Tagging 有 称为 词类 
标注 是 指 判断 出 在 一个 句子 中 每个 
词 所 扮演 的 语法 角色 例如 表 示人 事物 
地点 或 抽象 概念 的 名称 就是 名词 表示 动作 
或 状态 变化 的 词 为 动词 用来 描写 或 
修饰 名 词性 成分 或 表示 概念 的 性质 状态 
特征 或 属性 的 词 称为 形容词 等等 中文 词性 
标注 中 影响 词性 标注 精度 的 因素 主要 是 
要 正确 判断 文本 中 那些 常用词 的 词性 一般而言 
中文 的 词性 标注 算法 比较 统一 大多数 使用 HMM 
或 最大熵 算法 如 结巴 的 词性 标注 为了 获得 
更高 的 精度 也有 使用 CRF 算法 的 如 Ltp 
中的 词性 标注 在 一般 的 工程 应用 中 语料 
的 中文分词 和 词性 标注 通常 同时 完成 目前 流行 
的 中文 词性 标签 有 两个 类 北大 词性 标注 
集 和 宾州 词性 标注 集 它们 各有千秋 Ltp3 . 
3 词性 标注 词性 标注 模块 的 文件 名为 pos 
. model # * coding utf 8 * import sys 
import os from pyltp import * reload sys sys . 
s e t d e f a u l t 
e n c o d i n g utf 8 
# 已 分 好词 sent = 在 包含 问题 的 
所有 解 的 解 空间 树 中 按照 深度 优先 
搜索 的 策略 从 根 节点 出发 深度 探索 解 
空间 树 words = sent . split postagger = Postagger 
# 实例 化 词性 标注 类 postagger . load ltp3 
. 4 / pos . model postags = postagger . 
postag words for word postag in zip words postags print 
word + / + postag 在 / p 包含 / 
v 问题 / n 的 / u 所有 / b 
解/v/nr 的 / u 解 空间 树 / n 中 
/ nd / wp 按照 / p 深度 优先 / 
d 搜索 / v 的 / u 策略 / n 
/ wp 从/p/nr 根 节点 / n 出发 / v 
深度 / n 探索 / v 解 空间 树 / 
n / wp 安装 StanfordNLP 并 编写 Python 接口类 https 
/ / stanfordnlp . github . io / CoreNLP / 
http / / nlp . stanford . edu / software 
/ stanford corenlp full 2017 06 09 . zip 只 
携带 了 英文 的 语言 模型 包 中文 部分 的 
语言 模型 需要 单独 下载 http / / nlp . 
stanford . edu / software / stanford chinese corenlp 2017 
06 09 models . jarmkdir stanford corenlp   # 解压 
到此 目录 其中 stanford corenlp . jar 为主 执行 文件 
将 stanford chinese corenlp 2017 06 09 models . jar 
中的 中文 模型 全部 解 压到 models 目录 中 其中 
pos tagger 目录 下 放置 了 词性 标注 的 中文 
模型 jar xvf . . / . . / stanford 
chinese corenlp 2017 06 09 models . jarhttps / / 
nlp . stanford . edu / software / tagger . 
shtmlwget https / / nlp . stanford . edu / 
software / stanford postagger full 2017 06 09 . zip 
执行 命令 的 参考 脚本 ○ → cat stanford postagger 
. shjava mx300m cp stanford postagger . jar edu . 
stanford . nlp . tagger . maxent . MaxentTagger model 
$ 1 textFile $ 2 执行 . / stanford postagger 
. sh models / english left3words distsim . tagger sample 
input . txt ○ → cat . . / postest 
. txt 在 包含 问题 的 所有 解 的 解 
空间 树 中 按照 深度 优先 搜索 的 策略 从 
根 节点 出发 深度 探索 解 空间 树 ○ → 
. / stanford postagger . sh models / chinese distsim 
. tagger . . / postest . txtLoading default properties 
from tagger models / chinese distsim . taggerLoading POS tagger 
from models / chinese distsim . tagger . . . 
done 1.5 sec . 在 # P 包含 # VV 
问题 # NN 的 # DEC 所有 # DT 解#/nr 
VV 的 # DEC 解 空间 树 # NN 中 
# LC # PU 按照 # P 深度 优先 # 
NN 搜索 # NN 的 # DEC 策略 # NN 
# PU 从#P/nr 根 节点 # NN 出发 # VV 
深度 # JJ 探索 # NN 解 空间 树 # 
VV # PUTagged 23 words at 338.24 words per second 
. 进入 stanford corenlp ○ → java mx5g cp . 
/ * edu . stanford . nlp . tagger . 
maxent . MaxentTagger model models / edu / stanford / 
nlp / models / pos tagger / chinese distsim / 
chinese distsim . tagger textFile . . / postest . 
txt 在 # P 包含 # VV 问题 # NN 
的 # DEC 所有 # DT 解#/nr VV 的 # 
DEC 解 空间 树 # NN 中 # LC # 
PU 按照 # P 深度 优先 # NN 搜索 # 
NN 的 # DEC 策略 # NN # PU 从#P/nr 
根 节点 # NN 出发 # VV 深度 # JJ 
探索 # NN 解 空间 树 # VV # PU 
1 新建 stanford . py # * coding utf 8 
* import sys import os reload sys sys . s 
e t d e f a u l t e 
n c o d i n g utf 8 # 
CoreNLP 3.6 jar/w 包和/nr 中文/nz 模型/n 包/v # ejml 0.23 
. jar javax . json . jar jollyday . jar 
joda time . jar jollyday . jar protobuf . jar 
slf4j . api . jar # slf4j simple . jar 
stanford corenlp 3 . 6.0 . jar xom . jar 
class StanfordCoreNLP # 所有 StanfordNLP 的 父 类 def _ 
_ init _ _ self jarpath self . root = 
jarpath self . tempsrcpath = tempsrc # 输入 临时文件 路径 
self . jarlist = ejml 0.23 . jar javax . 
json . jar jollyday . jar joda time . jar 
protobuf . jar slf4j api . jar slf4j simple . 
jar stanford corenlp 3 . 8.0 . jar xom . 
jar self . jarpath = self . buildjars def buildjars 
self # 根据 root 路径 构建 所有 的 jar 包 
路径 # self . jarpath + = self . root 
+ / * for jar in self . jarlist self 
. jarpath + = self . root + jar + 
def savefile self path sent # 创建 临时文件 存储 路径 
fp = open path wb fp . write sent fp 
. close def delfile self path os . remove path 
# 词性 标注 子类 class t a n f o 
r d P O T a g g e r 
StanfordCoreNLP def _ _ init _ _ self jarpath modelpath 
StanfordCoreNLP . _ _ init _ _ self jarpath self 
. modelpath = modelpath self . classfier = edu . 
stanford . nlp . tagger . maxent . MaxentTagger self 
. delimiter = / self . _ _ buildcmd def 
_ _ buildcmd self self . cmdline = java mx1g 
cp + self . jarpath + + self . classfier 
+ model + self . modelpath + tagSeparator + self 
. delimiter print self . cmdline def tag self sent 
self . savefile self . tempsrcpath sent tagtxt = os 
. popen self . cmdline + textFile + self . 
tempsrcpath r . read self . delfile self . tempsrcpath 
return tagtxt def tagfile self inputpath outpath os . system 
self . cmdline + textFile + inputpath + + outpath 
t a n f o r d P o s 
t T a g g e r . py # 
* coding utf 8 * import sys import os reload 
sys sys . s e t d e f a 
u l t e n c o d i n 
g utf 8 from stanford import t a n f 
o r d P O T a g g e 
r root = stanford corenlp / modelpath = root + 
models / edu / stanford / nlp / models / 
pos tagger / chinese distsim / chinese distsim . tagger 
st = t a n f o r d P 
O T a g g e r root modelpath seg 
_ sent = 在 包含 问题 的 所有 解 的 
解 空间 树 中 按照 深度 优先 搜索 的 策略 
从 根 节点 出发 深度 探索 解 空间 树 taglist 
= st . tag seg _ sent print taglist 在 
/ P 包含 / VV 问题 / NN 的 / 
DEC 所有 / DT 解//nr VV 的 / DEC 解 
空间 树 / NN 中 / LC / PU 按照 
/ P 深度 优先 / NN 搜索 / NN 的 
/ DEC 策略 / NN / PU 从/P/nr 根 节点 
/ NN 出发 / VV 深度 / JJ 探索 / 
NN 解 空间 树 / VV / P U 1.5 
整合 命名 实体 识别 模块 本书 将 命名 实体 识别 
划分 在 语义 范畴 的 原因 是 命名 实体 识别 
不仅 需要 标注 词 的 语法 信息 名词 更 重要 
的 是 要 指示 词 的 语义 信息 人名 还是 
组织 机构 名 等 这里 所 需要 识别 的 命名 
实体 一般 不是 指 已知 名词 词典 中 的 登录 
词 而是 指 新词 或称 未 登录 词 更 具体 
的 命名 实体 识别 任务 还要 识别 出 文本 中 
三大类 实体类 时间 类 和数 字类 七 小 类 人名 
机构 名 地名 时间 日期 货币 和 百分比 命名 实体 
Ltp 命名 实体 识别 命名 实体 识别 模块 的 文件 
名为 ner . model # * coding utf 8 * 
import sys import os from pyltp import * reload sys 
sys . s e t d e f a u 
l t e n c o d i n g 
utf 8 sent = 欧洲 东部 的 罗马尼亚 首都 是 
布加勒斯特 也 是 一 座 世界性 的 城市 words = 
sent . split postagger = Postagger postagger . load ltp3 
. 4 / pos . model # 导入 词性 标注 
模块 postags = postagger . postag words recognizer = N 
a m e d E n t i t y 
R e c o g n i z e r 
recognizer . load ltp3 . 4 / ner . model 
# 导入 命名 实体 识别 模块 netags = recognizer . 
recognize words postags for word postag netag in zip words 
postags netags print word + / + postag + / 
+ netag 欧洲 / ns / Ns 东部 / nd 
/ O 的 / u / O 罗马尼亚 / ns 
/ Ns / wp / O 首都 / n / 
O 是 / v / O 布加勒斯特 / ns / 
Ns / wp / O 也 / d / O 
是 / v / O 一 / m / O 
座 / q / O 世界性 / n / O 
的 / u / O 城市 / n / O 
/ wp / O 第一段 是 词 欧洲 第二段 是 
词性 ns 第三段 S Ns 就是 识别 的 专名 O 
表示 非 专名 S Ns 表示 地名 Stanford 命名 实体 
识别 如果 仅用 NER 可从 http / / nlp . 
stanford . edu / software / CRF NER . shtml 
下载 # 命名 实体类 class t a n f o 
r d N E R T a g g e 
r StanfordCoreNLP def _ _ init _ _ self modelpath 
jarpath StanfordCoreNLP . _ _ init _ _ self jarpath 
self . modelpath = modelpath self . classfier = edu 
. stanford . nlp . ie . crf . CRFClassifier 
self . _ _ buildcmd def _ _ buildcmd self 
self . cmdline = java mx1g cp + self . 
jarpath + + self . classfier + loadClassifier + self 
. modelpath + print self . cmdline # 标注 句子 
def tag self sent self . savefile self . tempsrcpath 
sent tagtxt = os . popen self . cmdline + 
textFile + self . tempsrcpath r . read self . 
delfile self . tempsrcpath return tagtxt # 标注 文件 def 
tagfile self sent outpath self . savefile self . tempsrcpath 
sent os . system self . cmdline + textFile + 
self . tempsrcpath + + outpath self . delfile self 
. tempsrcpath # * coding utf 8 * import sys 
import os from stanford import t a n f o 
r d N E R T a g g e 
r reload sys sys . s e t d e 
f a u l t e n c o d 
i n g utf 8 root = stanford corenlp / 
modelpath = root + models / edu / stanford / 
nlp / models / ner / chinese . misc . 
distsim . crf . ser . gz st = t 
a n f o r d N E R T 
a g g e r modelpath root seg _ sent 
= 欧洲 东部 的 罗马尼亚 首都 是 布加勒斯特 也 是 
一 座 世界性 的 城市 taglist = st . tagfile 
seg _ sent ner _ test . txt print taglist 
欧洲 / LOCATION 东部 / O 的 / O 罗马尼亚 
/ GPE / O 首都 / O 是 / O 
布加勒斯特 / GPE / O 也 / O 是 / 
O 一 / O 座 / O 世界性 / O 
的 / O 城市 / O / O 1.6 整合 
句法 解析 模块 目前 句法分析 有 两种 不同 的 理论 
一种 是 短语 结构 语法 另一种 是 依存 语法 句法分析 
的 开源 系统 也 很多 但 迄今 为此 这些 解析 
技术 都还/nr 不够 理想 仍旧 很难 找到 高精度 处理 中文 
的 句法 解析 系统 其中 比较 突出 的 是 Ltp 
中文 句法分析 系统 使用 依存 句法 理论 还有 最 著名 
的 句法 解析器 是 Stanford 句法 解析器 截至 2015年 Stanford 
的 句法树 包含 了 如下 三 大 主要 解析器 PCFG 
概率 解析器 是 一个 高度 优化 的 词汇 化 PCFG 
依存 解析器 该 解析器 使用 A * 算法 是 一个 
随机 上下 无关 文法 解析器 除 英语 之外 该 解析器 
还 包含 一个 中文 版本 使用 滨州 中文 树 库 
训练 解析器 的 输出 格式 包含 依存关系 输出 和 短语 
结 构树 输出 Shift Reduce 解析器 为了 提高 PCFG 概率 
解析器 的 性能 Stanford 提供 了 一个 基于 移进 归约 
算法 的 高性能 解析器 其 性能 远高于 任何 PCFG 解析器 
而且 精 度上 比 其他 任何 版本 包括 RNN 的 
解析器 都更/nr 准确 神经网络 依存 解析器 神经网络 依存 解析器 是 
深度 学习 算法 在 句法 解析 中 的 一个 重要 
应用 它 通过 中心词 和 修饰词 之间 的 依存 关系 
来 构建 出 句子 的 句法树 有关 此 方面 的 
研究 是 目前 NLP 的 研究 重点 Ltp 句法 依存 
树 句法 解析 模块 的 文件 名为 parser . model 
# * coding utf 8 * import sys import os 
import nltk from nltk . tree import Tree # 导入 
nltk tree 结构 from nltk . grammar import D e 
p e n d e n c y G r 
a m m a r # 导入 依存 句法 包 
from nltk . parse import * from pyltp import * 
# 导入 ltp 应用 包 import re reload sys sys 
. s e t d e f a u l 
t e n c o d i n g utf 
8 words = 罗马尼亚 的 首都 是 布加勒斯特 . split 
# 例句 print words postagger = Postagger # 词性 标注 
postagger . load ltp3 . 4 / pos . model 
postags = postagger . postag words print len postags parser 
= Parser # 句法 解析 parser . load ltp3 . 
4 / parser . model arcs = parser . parse 
words postags arclen = len arcs print arclen conll = 
for i in xrange arclen # 构建 Conll 标准 的 
数据 结构 if arcs i . head = = 0 
arcs i . relation = ROOT conll + = \ 
t + words i + + postags i + + 
\ t + postags i + \ t + str 
arcs i . head + \ t + arcs i 
. relation + \ n print conll conlltree = DependencyGraph 
conll # 转换 为 依存 句法 图 tree = conlltree 
. tree # 构建 树结构 tree . draw 罗马尼亚 ns 
ns 3 ATT 的 u u 1 RAD 首都 n 
n 4 SBV 是 v v 0 ROOT 布加勒斯特 ns 
ns 4 VOB wp wp 4 WP 依存关系 Stanford Parser 
类 如果 仅 使用 中文 句法 解析 模块 可从 http 
/ / nlp . stanford . edu / software / 
lex parser . shtml 下载 # 句法 解析 class StanfordParser 
StanfordCoreNLP def _ _ init _ _ self modelpath jarpath 
opttype StanfordCoreNLP . _ _ init _ _ self jarpath 
self . modelpath = modelpath # 模型 文件 路径 self 
. classfier = edu . stanford . nlp . parser 
. lexparser . L e x i c a l 
i z e d P a r s e r 
self . opttype = opttype self . _ _ buildcmd 
def _ _ buildcmd self self . cmdline = java 
mx500m cp + self . jarpath + + self . 
classfier + outputFormat + self . opttype + + self 
. modelpath + print self . cmdline # 句法 解析 
def parse self sent self . savefile self . tempsrcpath 
sent tagtxt = os . popen self . cmdline + 
self . tempsrcpath r . read self . delfile self 
. tempsrcpath return tagtxt def tagfile self sent outpath self 
. savefile self tempsrcpath sent os . system self . 
cmdline + self . tempsrcpath + + outpath self . 
delfile self . tempsrcpath Stanford 短语 结 构树 # * 
coding utf 8 * import sys import os import nltk 
from nltk . tree import Tree # 导入 nltk tree 
结构 from stanford import * reload sys sys . s 
e t d e f a u l t e 
n c o d i n g utf 8 # 
配置 环境变量 # os . environ JAVA _ HOME = 
root = stanford corenlp / modelpath = root + models 
/ edu / stanford / nlp / models / lexparser 
/ chinesePCFG . ser . gz opttype = penn # 
滨州 树 库 格式 parser = StanfordParser modelpath root opttype 
result = parser . parse 罗马尼亚 的 首都 是 布加勒斯特 
print result tree = Tree . fromstring result tree . 
draw ROOT IP NP DNP NP NR 罗马尼亚 DEG 的 
NP NN 首都 VP VC 是 NP NR 布加勒斯特 PU 
stanford 依存 句法树 # * coding utf 8 * import 
sys import os import nltk from nltk . tree import 
Tree # 导入 nltk tree 结构 from stanford import * 
reload sys sys . s e t d e f 
a u l t e n c o d i 
n g utf 8 # 配置 环境变量 # os . 
environ JAVA _ HOME = root = stanford corenlp / 
modelpath = root + models / edu / stanford / 
nlp / models / lexparser / chinesePCFG . ser . 
gz opttype = t y p e d D e 
p e n d e n c i e s 
# parser = StanfordParser modelpath root opttype result = parser 
. parse 罗马尼亚 的 首都 是 布加勒斯特 print resultnmod assmod 
首都 3 罗马尼亚 1 case 罗马尼亚 1 的 2 nsubj 
布加勒斯特 5 首都 3 cop 布加勒斯特 5 是 4 root 
ROOT 0 布加勒斯特 5 punct 布加勒斯特 5 6 1.7 整合 
语义 角色 标注 模块 语义 角色 标注 SRL 来源于 20 
世纪 60 年代 美国 语言学家 菲 尔墨 提出 的 格 
语法 理论 该 理论 是 在 句子 语义 理解 上 
的 一个 重要 突破 基于 此 理论 语义 角色 标注 
就 发展 起来 了 并 成为 句子 语义分析 的 一种 
重要 方式 它 采用 谓词 论 元 角色 的 结构 
形式 标注 句子成分 相对于 给定 谓语 动词 的 语义 角色 
每个 语义 角色 被 赋予 一定 的 语义 美国 宾州 
大学 已经 开发 出 一个 具有 使用 价值 的 表示 
语义 命 题库 称为 PropBank 语义 角色 标注 系统 已经 
处于 NLP 系统 的 末端 其 精度 和 效率 都受到 
前面 几个 模块 的 影响 所以 当前 系统 的 精度 
都 不高 在 中文 领域 还 没有 投入 商业应用 的 
成功 案例 本节 介绍 的 是 Ltp 中文 语义 角色 
标注 系统 # * coding utf 8 * import sys 
import os reload sys sys . s e t d 
e f a u l t e n c o 
d i n g utf 8 from pyltp import * 
MODELDIR = ltp3 . 4 / sentence = 欧洲 东部 
的 罗马尼亚 首 都是 布加勒斯特 也是 一座 世界性 的 城市 
segmentor = Segmentor segmentor . load os . path . 
join MODELDIR cws . model words = segmentor . segment 
sentence wordlist = list words # 从 生成器 变为 列表 
元素 postagger = Postagger postagger . load os . path 
. join MODELDIR pos . model postags = postagger . 
postag words parser = Parser parser . load os . 
path . join MODELDIR parser . model arcs = parser 
. parse words postags recognizer = N a m e 
d E n t i t y R e c 
o g n i z e r recognizer . load 
os . path . join MODELDIR ner . model netags 
= recognizer . recognize words postags # 语义 角色 标注 
labeller = e m e n t i c R 
o l e L a b e l l e 
r labeller . load os . path . join MODELDIR 
srl / roles = labeller . label words postags netags 
arcs # 输出 标注 结果 for role in roles print 
rel wordlist role . index # 谓词 for arg in 
role . arguments if arg . range . start = 
arg . range . end print arg . name . 
join wordlist arg . range . start arg . range 
. end else print arg . name wordlist arg . 
range . start rel 是 A0 欧洲 东部 的 罗马尼亚 
A0 首都 A1 布加勒斯特 rel 是 ADV 也 A1 一 
座 世界性 的 rel 标签 表示 谓词 A0 指 动作 
的 实施 A1 指 动作 的 受事 3.1 中文分词 简介 
在 英文 中 单词 本身 就是 词 的 表达 一篇 
英文 文章 就是 单词 加 分隔符 空格 来 表示 的 
而在 汉语 中 词 以 字 为 基本 单位 的 
但是 一 篇 文章 的 语义 表达 却 仍然 是以 
词 来 划分 的 自 中文 自动 分词 被 提出 
以来 历经 将近 30年 的 探索 提出 了 很多 方法 
可 主要 归纳 为 规则 分词 统计 分词 和 混合 
分词 这三个 主要 流派 3.2 规则 分词 基于 规则 的 
分词 是 一种 机械 分词 方法 主要 是 通过 维护 
词典 在 切分 语句 时 将 语句 的 每个 字符串 
与 词表 中的 词 进行 逐一 匹配 找到 则 切分 
否则 不予 切分 按照 匹配 切分 的 方式 主要 有 
正向 最大 匹 配法 逆向 最大 匹配 法 以及 双向 
最大 匹 配法 三 种方法 正向 最大 匹 配法 Maximum 
Match Method MM 法 假定 分词 词典 中 的 最长 
词 有i个/nr 汉字 字符 则用 被 处理 文档 的 当前 
字串 中的 前 i 个字 作为 匹配 字段 查找 字典 
若 字典 中 存在 这样 的 一个 i 字词 则 
匹配 成功 匹配 字段 被 作为 一个 词 切 分出来 
如果 词典 中 找 不到 这样 的 一个 i 字词 
则 匹配 失败 将 匹配 字段 中 的 最后 一个 
字 去掉 对 剩下 的 字串 重新 进行 匹配 处理 
如此 进行 下去 直到 匹配 成功 即 切分 出 一个 
词 或 剩余 字串 的 长度 为零 为止 这样 就 
完成 了 一轮 匹配 然后 取 下 一个 i 字 
字串 进行 匹配 处理 直到 文档 被 扫描 完 为止 
逆向 最大 匹配 Reverse Maxinum Match Method RMM 法 的/uj 
基本/n 原理/n 和/c MM/w 法/l 相同/d 不同 的 是 分词 
切分 的 方向 与 MM 法 相反 双向 最大 匹 
配法 Bi direction Matching method 是 将 正向 最大 匹 
配法 得到 的 分词 结果 和 逆向 最大 匹 配法 
得到 的 结构 进行 比较 然后 按照 最大 匹配 原则 
选取 词数 切分 最少 的 作为 结构 据 SumM . 
. 和 Benjamin K . T . 1995 的 研究 
表明 中 文中 90.0% 的 句子 两种 切分 方法 得到 
的 结果 不 一样 但 其中 必 有 一个 是 
正确 的 歧义 检测 成功 只有 不到 1.0% 的 句子 
使用 正向 最大 匹 配法 和 逆向 最大 匹 配法 
的 切分 虽 重合 却是 错 的 或者 正向 最大 
匹 配法 和 逆向 最大 匹 配法 切分 不同 但 
两个 都不对 歧义 检测 失败 这 正是 双向 最大 匹 
配法 在 实用 中文 信息 处理 系统 中 得以 广泛 
使用 的 原因 # 逆向 最大 匹配 class IMM object 
def _ _ init _ _ self dic _ path 
self . dictionary = set self . maximum = 0 
# 读取 词典 with open dic _ path r encoding 
= utf8 as f for line in f line = 
line . strip if not line continue self . dictionary 
. add line self . maximum = max self . 
maximum len line def cut self text result = index 
= len text while index 0 word = None for 
size in range self . maximum 0 1 if index 
size 0 continue piece = text index size index if 
piece in self . dictionary word = piece result . 
append word index = size break if word is None 
index = 1 return result 1 def main text = 
南京市 长江大桥 tokenizer = IMM . / data / imm 
_ dic . utf8 print tokenizer . cut text 3.3 
统计 分词 其 主要 思想 是 把 每个 词 看做 
是由 词 的 最小 单位 的 各个 字 组成 的 
如果 相连 的 字 在 不同 的 文本 中 出现 
的 次数 越多 就 证明 这 相连 的 字 很可能 
就是 一个 词 一般 要 做 如下 两步 操作 1 
建立 统计 语言 模型 为 长度 为 m 的 字符串 
确定 其 概率分布 P w1 w2 . . . wm 
当 文本 过 长时 右 部从 第三项 起 的 每一项 
计算 难度 都 很大 为了 解决 该 问题 有人 提出 
n 元 模型 n gram model 降低 该 计算 难度 
所谓 n 元 模型 就是 在 估算 条件概率 时 忽略 
距离 大于 等于 n 的 上 文词 的 影响 因此 
P wi | w1 w2 . . . wi 1 
的 计算 可简化 为 P wi | w1 w2 . 
. . wi 1 ~ = P wi | wi 
n 1 . . . wi 1 显然 当 n 
= 2时 该 模型 是 可以 保留 一定 的 词序 
信息 的 而且 n 越大 保留 的 词序 信息 越 
丰富 但 计算 成本 也 呈 指数级 增长 一般 使用 
频率 计数 的 比例 来 计算 n 元 条件概率 P 
wi | wi n 1 . . . wi 1 
= count wi n 1 . . . wi 1 
wi / count wi n 1 . . . wi 
1 由于 会 出现 分子 分母 为零 的 情况 一般 
在 n 元 模型 中 需要 配合 相应 的 平滑 
算法 解决 如 拉普拉斯 平滑 算法 等 2 对 句子 
进行 单词 划分 然后 对 划分 结果 进行 概率 计算 
获得 概率 最大 的 分词 方式 这里 就用到 了 统计 
学习 算法 如 隐含 马尔可夫 HMM 条件 随 机场 CRF 
等 HMM 是 将 分词 作为 字 在 字串 中的 
序列 标注 任务 来 实现 的 其 基本 思路 是 
每个字 在 构造 一个 特定 的 词语 时 都 占据 
着 一个 确定 的 构词 位置 即 词位 现 规定 
每个 字 最多 只有 四个 构词 位置 即 B 词首 
M 词中 E 词尾 和 单独 成词/nr max = max 
P o1o2 . . . on | r1r2 . . 
. rn 假设 每个字 的 输出 仅仅 与 当前 字 
有关 就 能 得到 P o1o2 . . . on 
| r1r2 . . . rn = P o1 | 
r1 P o2 | r2 . . . P on 
| rn 但该 方法 完全 没有 考虑 上下文 且 会 
出现 不 合理 的 情况 HMM 就是 用来 解决 该 
问题 的 一种 方法 P o | r = P 
o r / P r = P r | o 
P o / P r 其中 P r 为 常数 
因此 求 最大化 P r | o P o 在 
HMM 中 求解 max P r | o P o 
的 常用 方法 是 Veterbi 算法 它 是 一种 动态规划 
方法 核心 思想 是 如果 最终 的 最优 路径 经过 
某个 oi 那么 从 初始 节 点到 oi 1点 的 
路径 必然 也 是 一个 最优 路径 因此 每个 节点 
oi 只 会 影响 前后 两个 P oi 1 | 
oi 和 P oi | oi + 1 # * 
coding utf 8 * class HMM object def _ _ 
init _ _ self pass def try _ load _ 
model self trained pass def train self path pass def 
viterbi self text states start _ p trans _ p 
emit _ p pass def cut self text pass class 
HMM object def _ _ init _ _ self import 
os # 主要 是 用于 存取 算法 中间 结果 不用 
每次 都 训练 模型 self . model _ file = 
. / data / hmm _ model . pkl self 
. state _ list = B M E S self 
. load _ para = False # 用于 加载 已 
计算 的 中间 结果 当 需要 重新 训练 时 需 
初始化 清空 结果 def try _ load _ model self 
trained if trained import pickle with open self . model 
_ file rb as f self . A _ dic 
= pickle . load f self . B _ dic 
= pickle . load f self . Pi _ dic 
= pickle . load f self . load _ para 
= True else # 状态 转移 概率 状态 状态 的 
条件 概率 self . A _ dic = { } 
# 发射 概率 状态 词语 的 条件 概率 self . 
B _ dic = { } # 状态 的 初始 
概率 self . Pi _ dic = { } self 
. load _ para = False # 采用 人民日报 的 
分词 语料 通过 统计 得到 HMM 所需 的 初始 概率 
转移 概率 以及 发射 概率 def train self path self 
. try _ load _ model False Count _ dic 
= { } # 求 p o # 初始化 参数 
def init _ parameters for state in self . state 
_ list self . A _ dic state = { 
s 0.0 for s in self . state _ list 
} self . Pi _ dic state = 0.0 self 
. B _ dic state = { } Count _ 
dic state = 0 def makeLabel text out _ text 
= if len text = = 1 out _ text 
. append S else out _ text + = B 
+ M * len text 2 + E return out 
_ text init _ parameters line _ num = 1 
# 观察者 集合 主要 是 字 以及 标点 等 words 
= set with open path encoding = utf 8 as 
f for line in f line _ num + = 
1 line = line . strip if not line continue 
word _ list = i for i in line if 
i = words | = set word _ list # 
更 新字 的 集合 linelist = line . split line 
_ state = for w in linelist line _ state 
. extend makeLabel w # print word _ list # 
print line _ state assert len word _ list = 
= len line _ state for k v in enumerate 
line _ state Count _ dic v + = 1 
if k = = 0 self . Pi _ dic 
v + = 1 # 每个 句子 的 第一个 字 
的 状态 用于 计算 初始状态 概率 else self . A 
_ dic line _ state k 1 v + = 
1 # 计算 转移 概率 # 计算 发射 概率 self 
. B _ dic line _ state k word _ 
list k = self . B _ dic line _ 
state k . get word _ list k 0 + 
1.0 self . Pi _ dic = { k v 
* 1.0 / line _ num for k v in 
self . Pi _ dic . items } self . 
A _ dic = { k { k1 v1 / 
Count _ dic k for k1 v1 in v . 
items } for k v in self . A _ 
dic . items } # 加 1 平滑 self . 
B _ dic = { k { k1 v1 + 
1 / Count _ dic k for k1 v1 in 
v . items } for k v in self . 
B _ dic . items } # 序列化 import pickle 
with open self . model _ file wb as f 
pickle . dump self . A _ dic f pickle 
. dump self . B _ dic f pickle . 
dump self . Pi _ dic f return self def 
viterbi self text states start _ p trans _ p 
emit _ p print start _ p # print trans 
_ p # print emit _ p V = { 
} path = { } for y in states V 
0 y = start _ p y * emit _ 
p y . get text 0 0 path y = 
y for t in range 1 len text V . 
append { } newpath = { } print text t 
# 检验 训练 的 发射 概率 矩阵 中 是否 有该字/nr 
neverSeen = text t not in emit _ p S 
. keys and \ text t not in emit _ 
p M . keys and \ text t not in 
emit _ p E . keys and \ text t 
not in emit _ p B . keys for y 
in states emitP = emit _ p y . get 
text t 0 if not neverSeen else 1.0 # 设置 
未知 字 单独 成词/nr prob state = max V t 
1 y0 * trans _ p y0 . get y 
0 * emitP y0 for y0 in states if V 
t 1 y0 0 V t y = prob newpath 
y = path state + y path = newpath if 
emit _ p M . get text 1 0 emit 
_ p S . get text 1 0 prob state 
= max V len text 1 y y for y 
in E M else prob state = max V len 
text 1 y y for y in states return prob 
path state def cut self text import os if not 
self . load _ para self . try _ load 
_ model os . path . exists self . model 
_ file prob pos _ list = self . viterbi 
text self . state _ list self . Pi _ 
dic self . A _ dic self . B _ 
dic begin next = 0 0 for i char in 
enumerate text pos = pos _ list i if pos 
= = B begin = i elif pos = = 
E yield text begin i + 1 next = i 
+ 1 elif pos = = S yield char next 
= i + 1 print next if next len text 
yield text next hmm = HMM hmm . train . 
/ data / trainCorpus . txt _ utf8 text = 
这 是 一个 非常 棒 的 方案 res = hmm 
. cut text print text print str list res 这 
是 一个 非常 棒 的 方案 { M 0.0 S 
0 . 4 1 7 9 8 8 4 4 
1 3 2 3 9 4 4 9 7 E 
0.0 B 0 . 5820149148537713 } 是 一 个 非 
常 棒 的 方 案 0 2 2 4 4 
6 7 8 8 10 11 这是 一个 非常 棒 
的 方案 3.5 中文分词 工具 jiebajieba 分词 结合 了 基于 
规则 和 基于 统计 这 两类 方法 首先 基于 前缀 
词典 进行 词 图 扫描 前缀 词典 是 指 词典 
中的 词 按照 前缀 包含 的 顺序 排列 可以 快速 
构建 包含 全部 可能 分词 结果 的 有向 无 环 
图 这个 图 中 包含 多条 分词 路径 有向 是 
指 全部 的 路径 都 始于 第一个 字 止于 最后 
一个 字 无 环 是 指节 点 之间 不 构成 
闭环 基于 标注 语料 使用 动态 规划 的 方法 可以 
找出 最大 概率 路径 并将 其 作为 最终 的 分词 
结果 对于 未 登录 词 jieba 使用 了 基于 汉字 
成词的/nr HMM 模型 采用 了 Viterbi 算法 进行 推导 实战 
之 高频词 提取 高频词 一般 是 指 文档 中 出现 
频率 较高 且 非 无用 的 词语 其 一定 程度 
上 代表 了 文档 的 焦点 所在 针对 单篇 文档 
可以 作为 一种 关键词 来看 对于 如 新闻 这样 的 
多篇 文档 可以 将 其 作为 热词 发现 舆论 焦点 
需要 去掉 标点符号 和 停用词 下 面对 搜狗 实验室 的 
新闻 数据 进行 高频词 的 提取 # read data def 
get _ content path with open path r encoding = 
gbk errors = ignore as f content = for l 
in f l = l . strip content + = 
l return content def get _ TF words topK = 
10 tf _ dic = { } for w in 
words tf _ dic w = tf _ dic . 
get w 0 + 1 return sorted tf _ dic 
. items key = lambda x x 1 reverse = 
True topK def stop _ words path with open path 
as f return l . strip for l in f 
def main import glob import random import jieba files = 
glob . glob . / data / news / C000013 
/ * . txt corpus = get _ content x 
for x in files sample _ inx = random . 
randint 0 len corpus # split _ words = list 
jieba . cut corpus sample _ inx # 停用词 split 
_ words = x for x in jieba . cut 
corpus sample _ inx if x not in stop _ 
words . / data / stop _ words . utf8 
print yangben 1 + corpus sample _ inx print / 
. join split _ words print topK 10 + str 
get _ TF split _ words topK 10 前列腺 34 
食品 7 做 7 男人 6 排尿 6 充血 5 
引起 5 前列腺癌 5 导致 5 压力 4 有时 需要 
定制 自己 的 领域 词典 用以 提升 分词 的 效果 
jieba . load _ userdict . / data / user 
_ dict . utf8 要求 格式 一般 为 词语 词频 
可省略 词性 可省略 用 空格 隔开 顺序 不可 颠倒 需 
为 utf8 编码 人工智能 的 概述 AI 指代 「 人工智能 
」 是 让 机器 能够 像 人类 一样 完成 智能 
任务 的 技术 AI 使用 智能 完成 自动化 任务 人工智能 
包含 两个 关键 点 1 . 自动化 2 . 智能 
人工智能 的 目标 推理 自动 学习 & 调度 机器学习 自然语言 
处理 计算机 视觉 机器人 通用 智能 人工智能 三大 阶段 阶段 
1 机器学习 智能系统 使用 一 系列 算法 从 经验 中 
进行 学习 阶段 2 机器 智能 机器 使用 的 一系列 
从 经验 中 进行 学习 的 高级 算法 例如 深度 
神经网络 阶段 3 机器 意识 不 需要 外部 数据 就 
能从 经验 中 自学习 目前 处于 第 3 阶段 人工智能 
的 类型 ANI 狭义 人工智能 它 包含 基础 的 角色 
型 任务 比如 由 Siri Alexa 这样 的 聊天 机器人 
个人 助手 完成 的 任务 AGI 通用 人工智能 通用 人工智能 
包含 人类 水平 的 任务 它 涉及 到 机器 的 
持续 学习 ASI 强 人工智能 强 人工智能 指代 比 人类 
更 聪明 的 机器 什么 使得 系统 智能化 自然语言 处理 
知识 表示 自动 推理 机器学习 NLP 人工智能 机器学习 深度 学习 
和 神经 网络 之间 的 区别 人工智能 建立 能 智能化 
处理 事物 的 系统 自然语言 处理 建立 能够 理解 语言 
的 系统 人工智能 的 一个 分支 机器学习 建立 能从 经验 
中 进行 学习 的 系统 也是 人工智能 的 一个 分支 
神经网络 生物学 启 发出 的 人工 神经元网络 深度 学习 在 
大型 数据 集上 建立 使用 深度 神经 网络 的 系统 
机器 学习 的 一个 分支 下面 是 我 关注 的 
重点 自然语言 处理 的 概念 自然语言 处理 NLP 是 指 
机器 理解 并 解释 人类 写作 说话 方式 的 能力 
NLP 的 目标 是 让 计算机 ／ 机器 在 理解 
语言上 像 人类 一样 智能 最终 目标 是 弥补 人类 
交流 自然语言 和 计算机 理解 机器语言 之间 的 差距 下面 
是 三个 不同 等级 的 语言学 分析 句 法学 给定 
文本 的 哪 部分 是 语法 正确 的 语义学 给定 
文本 的 含义 是 什么 语用学 文本 的 目的 是 
什么 NLP 处理 语言 的 不同 方面 音韵学 指代 语言 
中 发音 的 系统化 组织 词态 学 研究 单词 构成 
以 及 相互 之间 的 关系 NLP 中 理解 语义分析 
的 方法 分布式 它 利用 机器学习 和 深度 学习 的 
大规模 统计 策略 框架 式 句法 不同 但 语义 相同 
的 句子 在 数据结构 帧 中被 表示 为 程式化 情景 
理论 式 这种方法 基于 的 思路 是 句子 指代 的 
真正 的 词 结合 句子 的 部分 内容 可 表达 
全部 含义 交互式 学习 它 涉及 到 语用 方法 在 
交互式 学习 环境 中 用户 教 计算机 一步 一步 学习 
语言 我们 为什么 需要 NLP 有了 NLP 有 可能 完成 
自动 语音 自动 文本 编写 这样 的 任务 由于 大型 
数据 文本 的 存在 我们 为什么 不 使用 计算机 的 
能力 不知 疲倦 地 运行 算法 来 完成 这样 的 
任务 花费 的 时间 也 更少 这些 任务 包括 NLP 
的 其他 应用 比如 自动 摘要 生成 给定 文本 的 
总结 和 机器 翻译 NLP 流程 如果 要 用 语音 
产生 文本 需要 完成 文本 转 语音 任务 NLP 的 
机制 涉及 两个 流程 1 . 自然语言 理解 2 . 
自然语言 生成 自然语言 理解 NLU NLU 是 要 理解 给定 
文本 的 含义 文本 内 每个 单词 的 特性 与 
结构 需要 被 理解 在 理解 结构上 NLU 要 理解 
自然 语言 中 的 以下 几个 歧义 性 词法 歧义 
性 单词 有 多重 含义 句法 歧义 性 语句 有 
多重 解析 树 语义 歧义 性 句子 有 多重 含义 
回 指 歧义 性 Anaphoric Ambiguity 之前 提到 的 短语 
或 单词 在后面 句子 中 有 不同 的 含义 存在 
的 问题 有些 词 有 类似 的 含义 同义词 有些 
词 有 多重 含义 多义词 自然语言 生成 NLG NLG 是从 
结构化 数据 中 以 可读 地 方式 自动 生成 文本 
的 过程 自然语言 生成 的 问题 是 难以 处理 自然语言 
生成 可被 分为 三 个 阶段 1 . 文本 规划 
完成 结构化 数据 中 基础 内容 的 规划 2 . 
语句 规划 从 结构化 数据 中 组合 语句 来 表达 
信息流 3 . 实现 产生 语法 通顺 的 语句 来 
表达 文本 NLP 与 文本 挖掘 或 文本 分析 之间 
的 不同 自然 语言 处理 是 理解 给定 文本 的 
含义 与 结构 的 流程 文本 挖掘 或 文本 分析 
是 通过 模式识别 提起 文本 数据 中 隐藏 的 信息 
的 流程 自然语言 处理 被 用来 理解 给定 文本 数据 
的 含义 语义 而 文本 挖掘 被 用来 理解 给定 
文本 数据 的 结构 句法 大 数据 中的 NLP 如今 
所 有 数据 中的 80% 都 可被 用到 大 数据 
来自 于 大公司 企业所 存储 的 信息 例如 职员 信息 
公司 采购 销售 记录 经济 业务 以及 公司 社交 媒体 
的 历史 记录 等 尽管 人类 使用 的 语言 对 
计算机 而言 是 模糊 的 非 结构化 的 但 有了 
NLP 的 帮助 我们 可以 解析 这些 大型 的 非 
结构化 数据 中 的 模式 从而 更好 地 理解 里面 
包含 的 信息 NLP 可 使用 大 数据 解决 商业 
中的 难题 比如 零售 医疗 金融 领域 中 的 业务 
下面 主要 谈谈 聊天 机器人 聊天 机器人 或 自动 智能 
代理 指代 你 能 通过 聊天 app 聊天 窗口 或 
语音 唤醒 app 进行 交流 的 计算机 程序 也有 被 
用来 解决 客户 问题 的 智能 数字化 助手 成本低 高效 
且 持续 工作 聊天 机器人 的 重要性 聊天 机器人 对 
理解 数字化 客服 和 频繁 咨询 的 常规 问答 领域 
中 的 变化 至关重要 聊天 机器人 在 一些 领域 中 
的 特定 场景 中 非常 有 帮助 特别 是 会被 
频繁 问到 高度 可 预测 的 的 问题 时 聊天 
机器人 的 工作 机制 基于 知识 包含 信息库 根据 客户 
的 问题 回应 信息 数据 存储 包含 与 用户 交流 
的 历史 信息 NLP 层 它 将 用户 的 问题 
任何 形式 转译 为 信息 从而 作为 合适 的 回应 
应用层 指 用来 与 用户 交互 的 应用 接口 NLP 
中 为什么 需要 深度 学习 它 使用 基于 规则 的 
方法 将 单词 表示 为 「 one hot 」 编码 
向量 传统 的 方法 注重 句法 表征 而非 语义 表征 
词 袋 分类 模型 不能够 分别 特定 语境 深度 学习 
的 三项 能力 可 表达性 这一 能力 描述 了 机器 
如何 能 近似 通用 函数 可 训练 性 深度 学习 
系统 学习 问题 的 速度 与 能力 可 泛 化性 
在 未 训练过 的 数据 上 机器 做 预测 的 
能力 在 深度 学习 中 当然 也 要 考虑 其他 
的 能力 比如 可 解释性 模块性 可 迁移性 延迟 对抗 
稳定性 安全 等 但 以上 是 主要 的 几项 能力 
NLP 中 深度 学习 的 常见 任务 传统 NLP 和 
深度 学习 NLP 的 区别 日志 分析 与 日志 挖掘 
中的 NLP 什么 是 日志 不同 网络 设备 或硬件 的 
时序 信息 集合 表示 日志 日志 可直接 存储 在 硬盘 
文档 中 也可 作为 信息 流传 送到 日志 收集器 日志 
提供 维持 追踪 硬件 表现 参数 调整 紧急事件 系统 修复 
应用 和 架构 优化 的 过程 什么 是 日志 分析 
日志 分析 是从 日志 中 提取 信息 的 过程 分析 
信息 中的 句法 和 语义 解析 应用环境 从而 比较 分析 
不 同源 的 日志 文档 进行 异常 检测 发现 关联性 
什么 是 日志 挖掘 日志 挖掘 或 日志 知识发现 是 
提取 日志 中 模式 和 关联性 的 过程 从而 挖掘 
知识 预测 日志 中 的 异常 检测 日志 分析 和 
日志 挖掘 中 使用 到 的 技术 模式识别 将 日志 
信息 与 模式 薄中的/nr 信息 进行 对比 从而 过滤 信息 
的 技术 标准化 日志 信息 的 标准化 是 将 不同 
的 信息 转换 为 同样 的 格式 当 来自 不 
同源 的 日志 信息 有 不同 的 术语 但 含义 
相 同时 需要 进行 标准化 分类 & 标签 不同 日志 
信息 的 分类 & 标签 涉及 到 对 信息 的 
排序 并用 不同 的 关键词 进行 标注 Artificial Ignorance 使用 
机器学习 算法 抛弃 无用 日志 信息 的 技术 它 也可 
被 用来 检测 系统 异常 日志 分析 & 日志 挖掘 
中的 NLP 自然语言 处理 技术 被 普遍 用于 日志 分析 
和 日志 挖掘 词语切分 词干 提取 stemming 词形 还原 lemmatization 
解析 等 不同 技术 被 用来 将 日志 信息 转换成 
结构化 的 形式 一旦 日志 以 很好 的 形式 组织 
起来 日志/ns 分析/vn 和/c 日志/ns 挖掘/v 就能/i 提取/v 信息/n 中/f 
有用/v 的/uj 信息/n 和/c 知识/v NLP 的 一个 例子 用户 
需要 输入 一个 包含 已 写 文本 的 文件 接着 
应该 执行 以下 NLP 步骤 语句 分割 在 给定 文本 
中 辨识 语句 边界 即 一个 语句 的 结束 和 
另一个 语句 的 开始 语句 通常 以 标点符号 「 . 
」 结束 标记 化 辨识 不同 的 词 数字 及 
其他 标点符号 词干 提取 将 一个 词 还原 为 词干 
词性 标注 标出 语 句中 每 一个 词 的 词性 
比如 名词 或 副词 语法分析 将 给定 文本 的 部分 
按类 划分 命名 实体 识别 找出 给定 文本 中的人物 地点 
时间等 指代 消解 根据/p 一个/m 语句/n 的/uj 前句/i 和后句/nr 界定/n 
该/r 句中/i 给定/v 词/n 之间/f 的/uj 关系/n NLP 的 其他 
关键 应用领域 除了 在 大 数据 日志 挖掘 及 分析 
中 的 应用 NLP 还有 一些 其他 主要 应用领域 自动 
摘要 在 给定 输入 文本 的 情况 下 摈弃 次要 
信息 完成 文本 摘要 情感 分析 在 给定 文本 中 
预测 其 主题 比如 文本 中 是否 包含 判断 观点或 
评论 等 文本 分类 按照 其 领域 分 类 不同 
的 期刊 新闻报道 多 文档 分类 也是 可能 的 文本 
分类 的 一个 流行 示例 是 垃圾 电子邮件 检测 基于 
写作 风格 可 检测 作者姓名 信息提取 建议 电子邮件 程序 自动 
添加 事件 到 日历 主要 是 翻译 的 这篇 论文 
Overview of Artificial Intelligence and Role of Natural Language Processing 
in Big Data 参考文献 点击 打开 链接 目录 摘要 NER 
问题 定义 常用 数据集 CoNLL 2003OntoNotes 5.0 / CoNNLL 2 
0 1 2 N L P B A 2 0 
0 4 E n r o n EmailsBosonNLP 人民日报 1998SOTA 
算法 在 主流 数据 集上 的 表现 CoNLL 2003OntoNotes 5.0 
/ CoNNLL 2012 中文 NERAPI 接口 资源 Google NLP API 
百度 NLP 接口 BOSON NER 接口 总结 附录 推荐 论文 
资源 推荐 源代码 资源 Tensorflow Named Entity RecognitionBLOG POSTS 摘要 
命名 实体 识别 的 任务 是 识别 句子 中 的 
实体 并且 标注 实体 的 类别 如 人 PER 组织 
ORG 位置 LOC 本 博文 试图 系统 整理 与 命名 
实体 识别 技术 相关 的 数据 算法/n 及/c 各类/r 开源/n 
资源/n 以便/c 对/p 命名/n 实体/n 识别/v 技术/n 有/v 应用/v 需求/v 
的/uj 个人/n 和/c 机构/n 能够/v 各取所需/l NER 问题 定义 命名 
实体 识别 的 任务 是 识别 句子 中 的 实体 
并且 标注 实体 的 类别 如 人 PER 组织 ORG 
位置 LOC 此外 如果 一个 完整 的 实体 是由 多个 
单词 构成 可将 每个 单词 单独 标注 D a v 
i d B e c k h a m w 
a s w i t h A C M i 
l a n B PERI PEROOI ORGI ORG 以上 David 
标注 为 B PER B Beginning 代表 实体 的 起始 
部分 Beckham 标注 为 I PER I 代表 实体 的 
中间 部分 关于 单词 的 单独 标注 有 不同 的 
惯例 如 IOB Inside Outside Beginning 体系 和 IOBES Inside 
Outside Beginning End Singleton 体系 命名 实体 识别 的 标注 
实体 类别 和 标注 方式 不 存在 统一 的 标准 
可以 根据 具体 的 业务 定制 标注 体系 例如 在 
BosonNLP 网站 上 提供 的 中文 命名 实体 识别 API 
中 标注 的 类型 包括 product _ name time person 
_ name org _ name location company _ name 等 
不同 于 常见 的 NER 任务 中 只 标注 PER 
ORG LOC 用于 标准 的 字符串 格式 也 不 一样 
如 person _ name vs . PER 另外 其 标注 
的 方式 是 整个 实体 即 整个 单次 或 短句 
为 单位 进行 标注 并未 针对 每个字 去 标注 他 
在 实体 中 的 位置 常用 数据集 CoNLL 2003CoNLL 2003 
应该 算是 很 经典 的 NER 数据 集了 想 2018年 
横空出世 的 Google BERT 论文 中 也用 CoNLL 2003 的 
NER 任务 来 作为 BERT 性能 的 炼 金石 参见 
另一 篇 博文 Google BERT 概览 一 它 解决 了 
哪些 问题 以下 是 官网 上 提供 的 数据 样本 
数据 一共 包含 四列 第一 列为 单次 第二 列为 a 
part of speech POS 标签 第三 列为 syntactic chunk 标签 
第四 列为 NER 标注 关于 CoNLL 2003 数据集 的 其他 
信息 读者 可 移步 该 数据集 的 官网 介绍 或者 
在 下载 了 完整 数据集 后 自行 探索 OntoNotes 5.0 
/ CoNNLL 2012 以下 是 官网 上 提供 的 中文 
数据 样本 这里 不得不 吐槽 一下 选取 样本 的 时候 
就 不能 上 点心 挑 些 文明 得体 的 样本 
吗 可以 看到 OntoNotes 5.0 数据 中 除了 标注 NER 
也 包含 了如 POS 标签 等 其他 标签 可以 用于 
其他 类型 的 以 单词 或 短语 为 单位 的 
NLP 文字 识别 任务 关于 OntoNotes 5.0 数据集 的 其他 
信息 读者 可 移步 该 数据集 的 官网 介绍 或者 
在 下载 了 完整 数据集 后 自行 探索 NLPBA2004 关于 
NLPBA2004 数据集 的 信息 读者 可 移步 该 数据集 的 
官网 介绍 Enron EmailsEnron Emails 数据集 包含 了 Enron 公司 
内部 158名 员工 相互间 发送 的 60 多万 Email . 
Enron Emails 数据集 的 详细 信息 读者 可 移步 维基百科 
上 对 该 数据集 的 介绍 BosonNLPBosonNLP 中文 命名 实体 
识别 数据集 包含 了 2000条 句子 下载 地址 以下 展示 
了 其 原始数据 中 对 命名 实体 的 标注 方式 
{ { product _ name 浙江在线 杭州 } } { 
{ time 4月 25日 } } 讯 记者 { { 
person _ name 施 宇翔 } } 通讯员 { { 
person _ name 方英 } } 毒贩 很 时髦 用 
{ { product _ name 微信 } } 交易 毒品 
没 料想 警方 也很 潮 将计就计 一举 将 其 擒获 
记者 从{{/nr org _ name 杭州 江干区 公安 分局 } 
} 了解到 经过 一个 多 月 的 侦查 工作 { 
{ org _ name 江干区 禁毒 专案组 } } 抓获 
吸 贩毒 人员 5名 缴获 冰毒 400 余克 毒资 30000 
余元 扣押 汽车 一辆 { { location 黑龙江 } } 
籍 男子 { { person _ name 钱某 } } 
长期 落脚 于 宾馆 单身公寓 经常 变换 住址 他 有 
一辆车 经常 半夜 驾车 来往 于{{/nr location 杭州 主城区 } 
} 的 各大 宾馆 和 单身公寓 并且 常要/nr 活动 到 
{ { time 凌晨 6 7 点钟 } } { 
{ time 白天 } } 则 在 家里 呼呼大睡 人民日报 
1998 以 1998年 人民日报 语料 为 对象 由 北京大学 计算 
语言学 研究所 和 富士通 研究 开发 中心 有限公司 共同 制作 
的 标注 语料库 该 语料库 对 600 多万 字节 的 
中文 文章 进行 了 分词 及 词性 标注 以下 展示 
了 其 原始数据 的 大致 样式 19980101 01 001 007 
/ m １９９７年 / t / w 是 / v 
中国 / ns 发展 / vn 历史 / n 上 
/ f 非常 / d 重要 / a 的 / 
u 很 / d 不 / d 平凡 / a 
的 / u 一 / m 年/q/nr / w 中国 
/ ns 人民 / n 决心 / d 继承 / 
v 邓//nr nr 小平 / nr 同志 / n 的 
/ u 遗志 / n / w 继续 / v 
把 / p 建设 / v 有/v/nr 中国 / ns 
特色 / n 社会主义 / n 事业 / n 推向 
/ v 前进 / v / w 中国 / ns 
政府 / n nt 顺利 / ad 恢复 / v 
对 / p 香港 / ns 行使 / v 主权 
/ n / w 并 / c 按照 / p 
/ w 一国两制 / j / w / w / 
w 港人治港 / l / w / w 高度 / 
d 自治 / v 的 / u 方针 / n 
保持 / v 香港 / ns 的 / u 繁荣 
/ an 稳定 / an / w 中国 / ns 
共产党 / n nt 成功 / a 地 / u 
召开 / v 了 / u 第十五 / m 次 
/ q 全国 / n 代表大会 / n / w 
高举 / v 邓小平理论 / n 伟大 / a 旗帜 
/ n / w 总结 / v 百年 / m 
历史 / n / w 展望 / v 新 / 
a 的 / u 世纪 / n / w 制定 
/ v 了 / u 中国 / ns 跨 / 
v 世纪 / n 发展 / v 的 / u 
行动 / vn 纲领 / n / wSOTA 算法 在 
主流 数据 集上 的 表现 注 2019 年初 一个 专门 
收集 人工智能 领域 优秀 论文 及 开源 代码 的 网站 
https / / paperswithcode . com / 被 公之于众 读者 
可以 移步 这里 查 看在 NER 领域 的 最新 研究 
成果 本文 提供 部分 主流 数据集 的 leaderboards 截图 时间 
2019年 2月 从 以下 leaderboards 来看 在 NER 领域 表现 
比较 突出 的 深度 学习 技术 包括 BiLSTM + CRF 
Transformer 如 BERT 就是 基于 Transformer 的 等 而 BERT 
Flair 则 提供 了 非常 通用 的 NLP 框架 这些 
框架 不但 在 NER 问题 上 取得 了 不错 的 
成绩 由于 其 构建 了 非常 抽象 的 NLP 底层 
框架 他们 在 NLI QA 等 问题 上 也有 不错 
的 表现 CoNLL 2003OntoNotes 5.0 / CoNNLL 2012 中文 NER 
中文 NER 方面 统一 的 评估 平台 似乎 并不 多 
在 paperwithcode 网站 上 罗列 了 数个 leaderboard 但 其中 
每个 leaderboard 的 参与者 也 只有 一 两个 以下 为 
相关 的 截图 API 接口 资源 Google NLP API 在 
Google Cloud NLP API 官网 上 直接 进行 测试 测试 
时间 2019年 2月 测试 结果 如下 再来 一段 中文 的 
测试 结果 如下 百度 NLP 接口 在 百度 AI 开放 
平台 上 直接 进行 测试 测试 时间 2019年 2月 测试 
结果 如下 BOSON NER 接口 参考 BOSON 官方 API 文档 
说明 中的 Python 调用 示例 对 进行 命名 实体 识别 
测试 测试 时间 2019年 1月 import json import requests NER 
_ URL = http / / api . bosonnlp . 
com / ner / analysis s = 最近 一则 名叫 
啥 是 佩奇 的 短 视频 在 网上 刷屏 该 
视频 讲述 的 是 一个 生活 在 大山 里 的 
留守 老人 为 给 城里 的 孙子 准备 新年礼物 问 
遍 全村 啥 是 佩奇 的 故事 老人 广寻/nr 佩奇 
最终 亲手 打造 了 一个 硬核 佩奇 data = json 
. dumps s headers = { X Token KeyManager . 
BOSON _ API _ KEY Content Type application / json 
} resp = requests . post NER _ URL headers 
= headers data = data . encode utf 8 for 
item in resp . json for entity in item entity 
print . join item word entity 0 entity 1 entity 
2 输出 结果 为 啥 是 佩奇 product _ name 
佩奇 person _ name 佩奇 person _ name 总结 命名 
实体 识别 是 自然 语言 处理 中 的 一个 常见 
课题 主流 的 AI 公司 也 通过 API 的 形式 
提供 各类 命名 实体 识别 服务 当前 主流 的 技术 
包括 BiLSTM + CRF Transformer 等 在 BERT FLAIR 等 
NLP 开源 框架 中 也有 针对 NER 的 解决方案 国外 
的 NER 研究 主要 集中于 利用 CoNLL 2003 OntoNotes 5.0 
等 数据集 研究 成果 相对 集中 国内 的 NER 研究 
常用 的 数据集 包括 人民日报 1998 数据集 但是 作者 在 
发文 的 时候 还 没有 看到 类似 于 CoNLL 2003/m 
那样/r 比较/d 集中/v 和/c 权威/nr 的/uj 性能/n 测评/v 和/c 对比/v 
附录 推荐 论文 资源 Bidirectional LSTM CRF Models for Sequence 
Tagging by Huang Xu and YuNeural Architectures for Named Entity 
Recognition by Lample et al . End to end Sequence 
Labeling via Bi directional LSTM CNNs CRF by Ma et 
Hovy 其他 优秀论文 可以 参看 paperwithcode 网站 上 关于 NER 
相关 资源 的 总结 推荐 源代码 资源 Tensorflow Named Entity 
Recognition 项目 地址 https / / github . com / 
g u i l l a u m e g 
e n t h i a l / tf _ 
ner 其他 优秀论文 可以 参看 paperwithcode 网站 上 关于 NER 
相关 资源 的 总结 BLOG POSTShttps / / g u 
i l l a u m e g e n 
t h i a l . github . io / 
sequence tagging with tensorflow . htmlhttps / / g u 
i l l a u m e g e n 
t h i a l . github . io / 
introduction tensorflow estimator . html 声明 本文 转载 自此 链接 
维护者 Dibya C h a k r a v o 
r t y C o n t r i b 
u t i o n s F e e l 
free to send   pull requests or email me dibyachakravorty 
@ gmail . com How this list got startedOn November 
10 2016 a Hacker News HN user aarohamankad asked the 
HN community for suggestions on beginner NLP resources . This 
  Ask HN thread   became popular and stayed in 
the front page for some time . In this time 
it gathered plenty of community generated suggestions about beginner NLP 
resources . This list is an attempt to summarize this 
discussion into a coherent list of resources . I also 
wrote a   blog post   on this . Table 
of C o n t e n t s B 
o o k s M O O C s Y 
o u T u b e VideosOnline University CoursesPackages to 
Play WithAcademic PapersLearning by DoingOpen Source ProjectsFun IdeasAPIsUser GroupsOther G 
u i d e s B o o k s 
p e e c h and Language Processing   Classic 
and Standard textbook in NLP . Pre publication draft of 
3rd edition available   here . Natural Language Processing with 
Python   Application oriented book . Examples are in Python 
NLTK . Free online version   here . Taming Text 
  Application oriented book . Examples are in JAVA . 
Foundations of Statistical Natural Language Processing   Classic text on 
Statistical NLP . Goes deep into the implementation of parsers 
taggers etc . Handbook of Natural Language Processing   A 
complete treatment of NLP that starts from the historical roots 
and ends with the modern methods of NLP . Statistical 
Machine Translation   Learn how to make a service like 
Google T r a n s l a t e 
I n t r o d u c t i 
o n to Information Retrieval   Learn the nuts and 
bolts of services like Google Search and Google News search 
text classification clustering etc . Prolog and Natural Language Analysis 
  Implement NLP algortihms in Prolog . MOOCsCoursera course offered 
by University of Michigan   Introductory course that covers all 
prerequisite materials . Favored programming language is Python . Dicontinued 
Coursera course offered by Comlumbia University available on Academic torrents 
  Theory and concept oriented course . Only the course 
materials are available at this point . YouTube VideosVideo series 
by Jurafsky and Martin   Jurafsky and Martin are both 
professors at Stanford and they have written multiple classic textbooks 
on NLP . Stanford CS224D Deep Learning in NLP   
Applicatin of Deep Learning in NLPNLP with Python and   
NLTK   Application oriented video series using Python and NLTK 
. Online University CoursesMachine Translation course at the University of 
P e n n s y l v a n 
i a P a c k a g e s 
to Play WithNLTK   Most popular NLP library in Python 
. Excellent documentation in the form of a   book 
/ free online version . Powerful and extensible . Stanford 
CoreNLP   Fast and feature rich NLP library written in 
JAVA . An online demo is available   here . 
Spacy   Another emerging NLP library in Python . Fast 
and state of the art . Tries to maintain an 
uniform API while implementing state of the art algorithms . 
They have a   blog   and an   online 
demo . Apache Tika   Offers an unified interface for 
extracting text data and meta data from many different file 
formats PPT PDF etc . and analysis . Academic PapersDeep 
Learning in NLP   A GitHub repo that collects papers 
on Deep Learning in NLP . Learning by DoingOften the 
best way to learn is to contribute to an existing 
open source NLP project or implementing a fun idea . 
Open Source ProjectsBetty   Betty is a open source project 
with both real life use and practical NLP considerations and 
is looking for new maintainers . Fun IdeasInteractive Fiction / 
Parser Based Fiction   A video game where the player 
s interactions primarily involve text . Listen to this illuminating 
  FLOSS podcast   on the topic . APIsIBM Watson 
Cloud   From the makers of   IBM Watson . 
It lets you integrate NLP functionality in your app via 
an API . There s a free tier / free 
trial . User GroupsACM Special Interest Group in AI   
If you are craving for some face to face human 
contact . Other GuidesQuora question on how to get into 
NLPawesome nlp on GitHub   A GitHub repo containing a 
curated list of NLP resources . 声明 本文 转载 自此 
链接 转载 请 注明   自然语言 处理 资源 合集 | 
我 爱 计算机   我 的 人工智能 学习 之路 NLP 
方向 开篇 什么 是 机器学习 什么 是 深度 学习 机器学习 
机器学习 环境 及 所需 工具 机器学习 十大 算法 深度 学习 
深度 学习 中 的 函数 类型 深度 学习 中 的 
常见 概念 NLP 自然语言 处理 数学 基础 分词 和 统计 
分布 规律 基于 数学 统计 的 语言 模型 对于 人工智能 
的 学习 我 主要 侧重 于 自然 语言 处理 方向 
基于 这个 大 方向 将 我 的 学习 脉络 梳 
理成 一套 体系 按照 总分 原则 开篇 先从 总体 上 
介绍 机器学习 深度 学习 以及 NLP 的 相关 知识 在 
以后 的 博客 中 我会 边学边 完善 同时 我 还会 
写入 我 参加 各 算法 大赛 的 比赛 经验 做到 
理论 与 实践 相 结合 期望 我 在 这条 道路上 
越走/nr 越深 排除万难 砥砺 前行 接下来 开启 我 的 人工智能 
之旅 吧 什么 是 机器学习 什么 是 深度 学习 以 
下 关于 人工智能 机器 学习 的 定义 来自 百 面 
机器学习 进入 2018 年以来 人工智能 机器学习 深度 学习 神经 网络 
等 关键词 已经 成为 人们 茶余饭后 的 谈资 而且 更 
会 成为 软件 工程师 的 必备 技能 人工智能 泛指 机 
器具 有人 的 智力 的 技术 这项 技术 的 目的 
是 使 机器 像人 一样 感知 思考 做事 解决问题 人工智能 
是 一个 宽泛 的 技术 领域 包括 自然 语言 理解 
计算机 视觉 机器人 逻辑 和 规划 等 机器学习 机器学习 指 
计算机 通过观察 环境 与 环境 交互 在 吸取 信息 中 
学习 自我 更新 的 进步 简而言之 机器学习 可以 揭示 数据 
背后 的 真是 含义 大多数 机器学习 算法 可以 分成 训练 
和 测试 两个 步骤 这 两个 步骤 可以 重叠 进行 
训练 包括 监督 学习 和无/nr 监督 学习 两类 其中 监督 
学习 关注 对 事物 未知 表现 的 预测 一般 包括 
分 类 问题 和 回归 问题 无 监督 学习 则 
倾向 于对/nr 事物 本身 特性 的 分析 常用 的 技术 
包括 数据 降 维和 聚 类 问题 等 分类 顾名思义 
便是 对 其 所在 的 类别 进行 预测 类别 既是 
离散 的 同时 也 是 预先 知道 数量 的 回归 
同样 是 预测 问题 只是 预测 的 目标 往往 是 
连续变量 数据 降 维 是 对 事物 的 特性 进行 
压缩 和 筛选 这项 任务 相对 比较 抽象 聚 类 
是 依赖于 数据 的 相似性 把 相似 的 数据 样本 
划分 为 一个 簇 不同于 分类 问题 我们 在 大多数 
情况下 不会 预先 知道 簇 的 数量 和 每个 簇 
的 具体 含义 机器学习 环境 及 所需 工具 我 习惯 
使用 Python 进行 机器学习 任务 同时 利用 里面 强大 的 
库 资源 来 参加 算法 竞赛 为什么 使用 Python 及 
优势 Python 是 一种 兼顾 可读性 和 易用性 的 编程语言 
同时 Python 具有 免费 使用 和 跨平台 执行 的 特性 
作为 一门 解释 型 语言 也 非常 便于 调试 代码 
Python 机器 学习 的 优势 1 方便 调试 的 解释 
型 语言 2 跨平台 执行 作业 3 广泛 的 应用 
编程 接口 4 丰富 完备 的 开源 工具包 NumPy & 
SciPyNumPy 除了 提供 一些 高级 的 数学 运算 机制 以外 
还 具备 非常 高效 的 向量 和 矩阵 运算 功能 
SciPy 是 在 NumPy 的 基础 上 构建 更为 强大 
应用 领域 也 更为 广泛 的 科学 计算 包 它 
需要 依赖 NumPy 的 支持 进行 安装 和 运行 Matplotlib 
免费 使用 的 绘图 工具包 Scikit learn 封装 了 大量 
经典 以及 最新 的 机器学习 模型 Pandas 一款 针对于 数据 
处理 和 分析 的 Python 工具包 Anaconda 一个 可以 一次性 
获得 300多 种 用于 科学 和 工程 计算 相关 任务 
的 编程 库 的 平台 机器学习 十大 算法 C 4.5 
决策树 K 均值 K mean 支持 向量 机 SVM Apriori 
最大 期望 算法 EM PageRank 算法 AdaBoost 算法 k 近邻 
算法 kNN 朴素 贝叶斯 算法 NB 分裂 回归 树 算法 
CART 深度 学习 深度 学习 本身 是 传统 神经网络 算法 
的 延伸 一般来说 深度 学习 适合 解决 数据 量大 数据 
比较 规范 但是 决策函数 高度 非线性 的 问题 常见 的 
深度 学习 应用 非常 成功 的 领域 有 图像识别 语音识别 
文字 生成 自然语言 理解 等 神经网络 模型 的 发展 大致 
经历 了 四个 不同 的 阶段 基本 的 感知器 传统 
的 神经 网络 模型 历史 可以 追溯 到 20 世纪 
50 年代 现在 公认 的 鼻祖 是 Rosenblatt 在 1957年 
提出 的 感知器 算法 多层 感知器 20 世纪 70 年代 
到 80 年代 多层 感知器 被发现 其 逼近 高度 非线性 
函数 的 能力 使得 科学界 对 它 的 兴趣 大增 
甚至有 神经网络 能 解决 一切 问题 的 论调 传统 神经 
网络 比较 沉寂 的 时期 20 世纪 90 年代 到 
21 世纪 早些时候 传统 神经网络 模型 比较 沉寂 但 却是 
核 方法 大行其道 的 时候 主要 原因 是 计算 能力 
跟不上 神经网络 模型 大约在 2006年 以后 到 现在 几个 重要 
的 技术 进步 促进 了 以 深度 学习 为 代表 
的 神经 网络 的 大 规模 应用 首先 是 廉价 
的 并行计算 其次 是 深度 网络 结构 的 持续 研究 
使得 模型 训练 效率 大大 增加 最后 是 互联网 的 
出现 为 大 规模 数据 的 生成 和 获取 提供 
了 极大 的 便利 深度 学习 中 的 函数 类型 
大多数 神经 网络 中 都 包含 四类 函数 组合 函数 
激活 函数 误差函数 和 目标函数 组合 函数 激活 函数 误差函数 
目标函数 深度 学习 中 的 常见 概念 批量 在线 学习 
和 离线 学习 偏移 / 阈值 标准化 数据 深度 递减 
算法 反向 传播 算法 NLP 自然语言 处理 从 广义 上 
讲 自然语言 处理 Natural Language Processing 简称 NLP 包含 所有 
用 计算机 对 自然 语言 进行 的 操作 从最/nr 简单 
的 通过 计数 词 出现 的 频率 来 比较 不同 
的 写作 风格 到 最 复杂 的 完全 理解 人 
所说 的话 至少 要 能 达到 对人 的 话语 作出 
有效 反应 的 程度 自然语言 处理 是 用 计算机 通过 
可计算 的 方法 对 自然 语言 的 各级 语言 单位 
字 词 语句 篇章 等等 进行 转换 传输 存贮 分析 
等 加工 处理 的 科学 是 一门 与 语言学 计算机科学 
数学 心理学 信息论 声学 相 联系 的 交叉性 学科 在 
词 处理 技术 方面 词 是 自然 语言 中 最小 
的 有 意义 的 构成 单位 是 自然 语言 处理 
中 最 基本 的 研究 内容 也是 其他 研究 的 
先行 和 基础 词 处理 的 主要 内容 包括 分词 
词性 标注 词义 消 歧 三个 主要 的 内容 分词 
常用 的 方法 包括 正向 最大 匹配 和 反向 最大 
匹配 以及 基于 词 网格 的 统计 方法 困扰 分词 
的 主要 问题 就是 歧义 消解 和 新词 识别 由于 
语言 本身 的 复杂性 目前 这 两个 问题 并 没有 
得到 根本性 的 解决 词性 标注 常用 的 方法 就是 
基于 隐 马尔科夫 模型 的 词性 标注 方法 常用 的 
词性 标注 的 方法 包括 基于 词典 知识库 的 方法 
还有 一些 常用 基于 统计 的 分类 方法 包括 贝叶斯 
方法 和 最大熵 模型 分词 和 词性 标注 是 所有 
自然语言 应用 的 基础 广泛 的 应用 于 机器翻译 信息检索 
等 各个 领域 数学 基础 概率论 信息论 信息熵 联合 熵 
条件 熵 粗糙集 分词 和 统计 分布 规律 常用 的 
分词 方法 1 正向 最大 匹配 分词 2 反向 最大 
匹配 分词 3 基于 统计 的 词 网格 分词 基于 
数学 统计 的 语言 模型 现有 的 主要 统计 语言 
模型 1 上下文 无关 模型 2 N 元 文法 模型 
3 N pos 模型 4 基于 决策树 的 语言 模型 
5 动态 自适应 基于 缓存 的 语言 模型 6 隐 
马尔科夫 模型 7 最大熵 模型 1 . 数据挖掘 DM 机器学习 
ML 自然语言 处理 NPL 这 三者 是 什么 关系 首先 
要 认识 到 这三项 并 不是 独立 的 选项 机器学习 
需要 数据挖掘 和 自然 语言 处理 的 支撑 自然语言 处理 
需要 数据挖掘 的 支撑 数据挖掘 需要 大 数据 的 支撑 
最终 所有 的 根源 都要 落实 在 大 数据 上 
而 这 一切 的 顶点 就是 人工智能 从 这个 层面 
上 来看 数据挖掘 是 比较 基础 的 部分 目前 也 
有 比较 成熟 的 解决 方案 只要 你 有 数据 
不愁 找不到 工具 各种 数据库 mongodb Hive Pig HBase RedShift 
分布式系统 Hadoop Spark 编程语言 Python 和R/nr 都是 为其 开发 的 
或者 擅长 处理 大 数据 所谓 学习 数据挖掘 已经 逐渐 
变成 熟练 掌握 这些 工具 的 过程 了 当然 如果 
有 兴趣 也 可以 参与 各种 分布式 系统 的 开发 
不过 基本上 你 能 想到 的 所有 好用 的 算法 
前人 都 已经 写 好了 集成 进去 了 自然 语 
处理 在 这个 世界 上 除了 谷歌 苹果 微软 IBM 
还 没有 其他 能够 挑战 此 领域 并且 获得 受人 
瞩目 的 成就 的 公司 因为 现在 自然语言 处理 就 
是 方法 很 落后 手段 很 暴力 基本上 常用 的 
技术 在 10 几 20 年前 就 出现 了 只不过 
那 时候 没有 谁 拥有 上 万台 计算机 来 处理 
自然 语 现在 倒是 有了 可 离 实用 还有 很长 
的 路 要走 可以 看 一下 IBM 的 沃特森 基本上 
也 就 代表 现阶段 最强 的 自然 语 处理 的 
水平 了 最后 就是 机器学习 了 这 一点 除了 我 
之外 已经 有 很多 人 强调 过了 机器学习 只是 被 
过度 神话 了 说白 了 现在 的 机器 学习 技术 
就是 战五渣 谁上 谁 后悔 目前 除了 以 深度 学习 
为 代表 的 人工神经网络 之外 其他 的 大部分 常用 的 
学习 方法 都是/nr 统计 学习 不仅 要 喂 足 了 
料 还要 精心 调教 还 不一定 出货 出了 也 基本上 
不准 如果 恰 好结果 符合 预期 只能 说 运气 真好 
不过 也 正是 因为 这样 机器学习 才 作为 一项 前沿 
学科 很多 科学家 去 研究 据 我 目测 这一 波 
深度 学习热 应该 已经 过去 了吧 按 这个 节奏 不 
知道 10年 之后 又会 有 什么 技术 点燃 机器学习 的 
热情 也 说不定 2 . 数据挖掘 DM 机器学习 ML 和 
自然 语言 处理 NPL 这几个 怎么 入门 啊 1 数据挖掘 
见 上 一篇 2 机器学习 NG 的 课程 我 以前 
看过 一部分 讲 的 风格 我 觉得 在 干货 之前 
都 比较 好懂 但是 学子 接受 起来 可能 有 困难 
台湾/ns 大学/n 的/uj 林轩田/nr 老师/n 的/uj machine/w learning 至少在 本科生 
教育 上 做 的 很好 他们 有个 team 经常 去 
各种 比赛 上 刷 奖 我 目前 在 修 他 
的 机器学习 课程 觉得 质量 不错 现在 coursera 上 也有 
同步 课程 传送门 Coursera . org 个人 觉得 机器学习 的 
很多 方法 都 是从 统计学 上 借鉴 过来 的 所以 
现在 在补 统计学 的 知识 同时 作为 一个 理论 性比 
较强 的 领域 线性代数 和 高等 数学 的 知识 起码 
是 要 具备 的 至少 人家 用 矩阵 写个 公式 
再做 梯度 下降 你 要看 明白 是 在 干嘛 3 
自然语言 处理 首推 资料 以及 唯一 的 资料 Columbia University 
Micheal Collins 教授 的 自然 语言 课程 链接 Michael CollinsMichael 
Collins 绝对 的 大牛 这门 课 是 我 见过 讲 
NLP 最最 最 清楚 的 尤其 是 他 的 讲义 
Collins 的 讲义 没有 跳步 每一步 逻辑 都 无比 自然 
最 关键 的 是 Collins 的 语言 措辞 真是 超级 
顺畅 没有 长 难句 没有 装逼 句 没有 语法错误 以及 
偏 难怪 的 表示 学术 圈 大都 是 死 理工科 
宅 语文 能 这么 好 真实 太难 得了 数学 之美 
的 作者 吴军 博士 在 书中 评价 Collins 的 博士 
论文 语言 如 小说 般 流畅 其 写作 功底 可见 
一般 举 两个 例子 如果 有 时间 不妨 亲自 体验 
下 静下心来 读一读 我 相信 即使 是 零基础 的 人 
也是 能 感受到 大师 的 魅力 的 1 . 语言 
模型 Language Model http / / www . cs . 
columbia . edu / ~ mc . . . 3 
. pdf2 . 隐 马尔可夫 模型 与 序列 标注 问题 
Tagging Problems and Hidden Markov Models http / / www 
. cs . columbia . edu / ~ mc . 
. . 3 . pdf 现在 Michael Collins 在 coursera 
上 也 开了 公开课 视频 免费 看 链接 Coursera 比 
看 讲义 更 清晰 虽然 没有 字幕 但是 不妨一试 因为 
讲 的 真的 好 清楚 其 在 句法分析 与 机器 
翻译 部分 的 讲解 是 绝对 的 经典 如果 能把 
Collins 的 课 跟下来 讲义 看下来 那么 你 已经 掌握 
了 NLP 的 主要 技术 与 现状 了 应该 可以 
看懂 部分 论文 了 你 已经 入门 了 NLP 进阶 
Collins 的 NLP 课程 虽然 讲 的 清晰 不过 有些 
比较 重要 的 前沿 的 内容 没有 涉及 应该 是 
为了 突出 重点 做了 取舍 比如 语言 模型 的 KN 
平滑 算法 等 此外 Collins 的 课程 更 注重 于 
NLP 所 依赖 的 基础 算法 而 对于 这些 算法 
的 某些 重要 应用 并 没 涉及 比如 虽然 讲了 
序列 标注 的 算法 隐 马尔可夫 模型 条件 随 机场 
模型 最大熵 模型 但是 并 没有 讲 如何 用 这些 
算法 来做 命名 实体 识别 语义 标注 等 Stanford NLP 
组 在 coursera 的 这个 课程 很好 的 对 Collins 
的 课 进行 了 补充 本 课程 偏 算法 的 
应用 算法 的 实现 过 的 很快 不 过上 完 
Collins 的 课后 再上 感觉 刚刚好 ~ 这 两门课 是 
Coursera 上 仅有 的 两门 NLP 课 不得不 佩服 Coursera 
上 的 课 都是 精品 啊 进阶 前沿 上 完 
以上 两个 课后 NLP 的 主要 技术 与 实现 细节 
就 应该 都 清楚 了 离 前沿 已经 很 近了 
读 论文 已经 没 问题 了 想 要 继续 进阶 
前沿 就要 读 论文 了 NLP 比起 其它 领域 的 
一个 最大 的 好处 此时 就 显现 出来 了 NLP 
领域 的 所有 国际 会议 期刊论文 都是/nr 可以 免费 下载 
的 而且 有 专人 整理 维护 每篇 论文 的 bibtex 
也是 相当 清晰 详细 关于/p NLP/w 都有/nr 哪些/r 研究/vn 方向/n 
哪些 比较 热门 可以 参考 当前 国内外 在 自然 语言 
处理 领域 的 研究 热点 & 难点 White Pillow 的 
回答 NLP 是 会议 主导 最前沿 的 工作 都会/nr 优先 
发表 在 会议 上 关于 哪个 会议 档次 比较 高 
可以 参考 谷歌 给出 的 会议 排名 Top conference 页面 
也 可以 参考 各个 会议 的 录 稿 率 一般来说 
越低 表示 会议 档次 越高 Conference acceptance rates 基本上 大家 
公认 的 NLP 最 顶级 的 会议 为 ACL 可以 
优先 看 ACL 的 论文 最后/f 简单/a 谈/v 一下/m 这/r 
三者/n 哪个/r 更/d 有/v 发展/vn 潜力/n 作为 一个 NLP 领域 
的 研究生 当然 要说 NLP 领域 有 潜力 啦 这里 
YY 几个 未来 可能会 热门 的 NLP 的 应用 语法 
纠错 目前 文档 编辑器 比如 Word 只能 做 单词 拼写错误 
识别 语法 级别 的 错误 还 无能为力 现在 学术 领域 
最好 的 语法 纠错 系统 的 正确 率 已经 可以 
接近 50% 了 部分 细分 错误 可以 做到 80% 以上 
转化/v 成/n 产品/n 的话/u 很/zg 有/v 吸引力/n 吧/y ~/i 无论/c 
是/v 增强/v 文档/n 编辑器/n 的/uj 功能/n 还是/c 作为/v 教学软件/n 更正/d 
英语/nz 学习者/n 的/uj 写作/v 错误/n 结构化 信息 抽取 输入 一篇 
文章 输出 的 是 产品名 售价 或者 活动 名 时间 
地点 等 结构化 的 信息 NLP 相关 的 研究 很多 
不过 产品 目前 看 并不多 我 也 不是 研究 这个 
的 不知 瓶颈 在哪儿 不过 想象 未来 互联网 信息 大量 
的 结构化 语义 化 那时 的 搜索 效率 绝对 比 
现在 翻番 啊 ~ 语义 理解 这个 目前 做 的 
并不 好 但 已经 有 siri 等 一票 语音 助手 
了 也有 watson 这种 逆天 的 专家 系统 了 继续 
研究 下去 虽然 离 人工智能 还 相去甚远 但是 离 真正 
好用 的 智能 助手 估计 也不 远了 那时 生活 方式 
会 再次 改变 即使 做 不到 这么 玄乎 大大 改进 
搜索 体验 是 肯定 能 做到 的 ~ 搜索引擎 公司 
在 这 方面 的 投入 肯定会 是 巨大 的 机器翻译 
这个 不多 说 了 目前 一直 在 缓慢 进步 中 
~ 我们 已经 能 从中 获益 看 越南 网页 看 
阿拉伯 网页 猜 个 大概 意思 没问题 了 此外 口语 
级别 的 简单句 的 翻译 目前 的 效果 已经 很好 
了 潜在 的 商业 价值 也 是 巨大 的 不过 
在 可 预见 的 近几年 对于 各 大 公司 发展 
更 有 帮助 的 估计 还 是 机器 学习 与 
数据挖掘 以上 我 YY 的 那些 目前 大 都还 在 
实验室 里 目前 能给/nr 公司 带来 实际 价值 的 更多 
还是 推荐 系统 顾客 喜好 分析 股票走势 预测 等 机器 
学习 与 数据挖掘 应用 ~ 大 数据 文摘 作品 大 
数据 文摘 记者 刘涵/nr 魏 子敏 自然语言 技术 的 未来 
其 关键 点 是 自然 两个字 11月 最后 一天 思 
必 驰 联合 创始人 首席 科学家 俞凯/nr 博士 在 清华 
x lab 主办 的 人工智能 研习 社 第七 课上 如此 
评价 自然语言 处理 并与 现场 听众 一起 畅 想了 这一 
潜力 巨大 的 技术 将 走向 哪里 图 11月 30日 
思 必 驰 联合 创始人 俞凯在/nr 清华 做了 题为 认知 
型 口语 对话 智能 的 讲座 刘涵/nr 摄在 这场 题为 
认知 型 口语 对话 智能 的 讲座 上 俞凯/nr 认为 
认知 交互 面临 的 最主要 的 挑战 一定 不是 语音 
因为 从 语音 识别 的 角度 上 来说 问题 明确 
只要 专门 向 这个 领域 去做 绝大部分 都 可以 优化 
的 很好 他 认为 其 最大 的 挑战 还是 对话 
的 过程 例如 针对 抑郁症 患者 治疗 的 这类 场景 
语音 对话 更 像是 有 目的 的 聊天 如果 没有 
很强 的 数学 背景 在 后面 做 支持 是 很难 
的 只有 在 一个 垂直 领域 积累 更多 的 数据 
才能 做 得 更好 大 数据 文摘 整理 的 俞凯/nr 
博士 本次 讲座 内容 如下 在 不 改变 原意 的 
前提 下 有 删改 今天 的 题目 叫 认知 型 
口语 对话 智能 核心点 是 两个 字 对话 这 两个 
字 不 单单 包含 语音 还 包含 语言 从 人机 
变迁 讲起来 我 在 清华 待了 八年 时间 在 这 
八年 当中 我们 经历 了 人和 机器 在 不同 时代 
交互 的 几个 变迁 我们 为什么 开始 关心 口语 对话 
智能 今天 第一 个 要讲 的 问题 就是 我们 为什么 
开始 关心 口语 对话 智能 刚 开始 的 时候 我们 
使用 的 是 Windows 图形 交互 界面 通过 机器 图形 
交互 界面 使得 人 和 信息 可以 进行 交流 我们 
奇迹 般 的 看到 了 打印 出来 很 工整 的 
排版 而 到了 现在 在 2011年 开始 手机 变成 智能手机 
使用 开始 变 的 非常 广泛 这个 时代 自然 的 
语言 手动输入 语音 逐渐 形成 了 我们 现在 的 交互 
手段 再 往后 我们 发现 通过 口语沟通 是 未来 智能 
信息 获取 最 核心 的 东西 而 移动 互联网 的 
时代 最 关键 的 是 这一类 沟通 产生 了 一种 
新的 模式 那 就是 交互 讲座 现场 图 刘涵/nr 摄在 
上 世纪 出现 Google 百度 等 搜索引擎 的 时候 交互 
还是 单向 的 但 出现 智能 手机 之后 我们 的 
交互 变成 了 双向 比如 苹果 的 交互 史 在 
刚开始 做 出来 第一代 iPhone 的 时候 并 没有 语音 
交互 的 能力 但/c 经过/p 市场/n 调研/vn 之后/f 发现/v 有/v 
75%/mf 的/uj 用户/n 都/d 希望/v 有/v 语音/n 控制/v 于是 在后面 
两代 iPhone 加入 了 语音 控制 但到 后面 发现 实际 
使用 的 用户 竟然 不到 5% 苹果 经过 总结 之后 
发现 不 仅仅 是 语音 还 必须 有 自然 语言 
交互 于是 在 iPhone4S 上面 出现 了 SiRi 再次 经过 
市场 调研 之后 发现 大概有 87% 的 用户 至少 在 
一个 月 会 使用 一次 SiRi 而且 他们 还 发现 
了 一件 事情 这 87% 的 用户 使用 SiRi 的 
时候 基本上 都是 在 调戏 SiRi 并不 做 其它 的 
事情 这 导致 苹果 并不能 赚到 钱 这也 促使 了 
苹果 在 2015年 收购 了 一家 做 统计 对话 交互 
的 公司 Vocallq 这会/i 让/v 技术/n 语音/n 识别/v 和/c 语义/n 
连在/nr 一起/m 形成/v 完整/a 的/uj 闭环/vn SiRi 就 可以 为 
我们 提供 新的 功能 了 讲座 现场 图 刘涵/nr 摄 
现 如今 大家 都 说是 互联网 时代 那么 如今 的 
信息 发展 到 什么 程度 了 呢 有 一个 统计 
显示 到 2017年 年底 全世界 物联网 智能 设备 的 总数 
将 首次 超过 人类 总数 而且 这些 智能 设备 绝大部分 
是 没有 或者 拥有 很小 的 屏幕 并 没有 办法 
进行 很 复杂 的 操作 这些 设备 如果 想 要去 
访问 最 核心 抽象 复杂 的 信息 只能 是 语音 
或者 对话 的 形式 这也 是 众多 巨头 从 2014年 
的 音箱 开始 出现 一 系列 智能 音箱 的 原因 
从 技术 上 讲 这 件 事情 不 仅仅 是 
要 解决 框架 的 问题 还 包括 了 对话 管理 
识别 合成 以及 我们 的 理解 语音识别 存在 的 问题 
和 机遇 我们 会 碰到 什么样 的 问题 以及/c 在/p 
这个/r 过程/n 中/f 有/v 多少/m 和/c 我们/r 的/uj 应有/v 相关/v 
的/uj 机会/n 首先 是 语音识别 语音 识别 是 感知 技术 
这 一类 里面 前沿 的 技术 当 许多 人 看到 
语音识别 第一 个 会 想到 的 问题 就是 语音识别 似乎 
已经 被 解决 了 当 我们 使用 一个 包罗万象 的 
语音 识别 系统 的 时候 我 讲 疏影 横斜 水 
清浅 暗香 浮动 月 黄昏 这样 的 东西 都 可以 
比较 完整 的 出来 但 尽管 采用 了 深度 学习 
的 技术 仍然 避免 不 了 错误 它 也会 偶尔 
的 有一些 语音 识别 的 错误 出现 而 我们 的 
任务 就是 使得 它 像人 一样 在有 错误 的 时候 
完整 的 去 进行 人机交互 修正 错误 这 需要 感知 
技术 和 认知 技术 相互 的 帮助 来 实现 第二 
是 计算 能力 语音 识别 的 解决 是 与 计算 
能力 有关 的 举 一个 例子 刚才 我 在做 演示 
的 时候 这个 演示 的 应用 背后 早期 使用 的 
深度 神经网络 共有 7层 每层 有 2048个 节点 输入 是 
1320 输出 是 将近 1万 这 大概 有 4500万 的 
参数 在做 语音 识别 的 时候 我们 是 把 每秒钟 
的 语音 切成 100份 每 一份 提取 1320个 向量 大家 
想象 我 在 一秒钟 要让 特征向量 经过 100次 深度 神经 
网络 计算 之后 还要 在 数以亿计 节点 的 搜索 网络 
里 再去 搜 它 所以 这个 运算 是 非常 非常 
复杂 的 曾经有 过 统计 整个/b 语音识别/i 会/v 分成/v 搜索/v 
的/uj 速度/n 和做/nr 神经网络/n 前/f 向/p 传递/v 的/uj 速度/n 这 
两个 速度 的 比例 在 传统 系统 里 面前 向 
传递 的 速度 占 30% 40% 后面 在 各种各样 的 
语言 空间 搜索 的 速度 大体 占 60% 70% 所以 
在 技术 上 必须 突破 速度 的 问题 现场 听众 
提问 刘涵/nr 摄 感知 智能 另外 一件事 是 如何 把 
它 做得 更小 整个/b 信息/n 技术/n 的/uj 变化/vn 和/c 推进/v 
一定是/i 和/c 技术/n 基础/n 的/uj 推进/v 有关/vn 性能 抗 噪 
能 不能 达到 90% 能 不能 在 手机 手表 上面 
也 做到 大 词汇 等 新的 挑战 不断 应运而生 随着 
在 智能 物联网 方面 我们 做出 各种 各种 的 优化 
之后 这样 的 挑战 开始 被 一个 个 的 克服 
掉 认知 这个事情 更加 麻烦 人机对话 并 不是 大家 想象 
那样 对话 也是 分成 很多 种 形态 的 有的 可以 
很好 的 解决 有的 却 毫无 头绪 如果 以 不同 
的 轮回 次数 来 分类 大概 可以 分为 下面 几种 
第 一种 是 模式 最少 的 单轮 模式 既 我 
说一句 它 回答 一句 而且 没有 什么 特定 的 结构化 
语义 这种 情况 基本上 是 命令式 的 十分 简单 复杂 
一点 的 则是 问答 现在 的 经典 深度 学习 技术 
很多 是 用来 解决 问答 这个 问题 的 因为 问答 
基本上 是 一问一答 你 说一句 它 会给 你 一个 答案 
偶尔 会 带有 一点 上下文 这 并 不是 真正 意义 
上 多轮 的 东西 还有 一类 是 闲聊 比如 微软 
小冰 你 不停 的 说 它 就 不停 的 跟 
你 聊天 闲聊 的 准则 就是 以 聊得 时间 来 
定义 的 曾经 有 一位 用户 聊了 好几 个 小时 
依然 在 继续 但这 里面 是 没有 什么 目标 意义 
的 所以 闲聊 要 考虑 的 是 如何 把 一些 
比较 有趣 的 东西 融入 进去 但是 里面 究竟 有 
什么 意义 机器 是 不会 去 关注 的 只要 有 
用户 黏性 跟 它 一直 聊 下去 特点是 多轮 没什么 
结构化 的 东西 偶尔 会 加 一些 知识 现在 希望 
把 这个 东西 融合 起来 这是 方向 本质 上 没有 
什么 结构化 的 东西 所以 闲聊 这一类 事情 实际上 更多 
的 是 怎么样 能够 把 一些 比较 有趣 的 东西 
融 进去 实事求是 来讲 目前 还 缺乏 一 套 比较 
扎实 的 理论 体系 能够 让 真正 在 理论 上 
解决 掉 最后 一类 是 任务 型 的 多轮 对话 
这类 对话 是 有 比较 扎实 的 数学 基础 的 
把 对话 看做 是 一个 序列 决策 过程 这一 技术 
的 三个 层面 如果 从 认知 层级 的 结算 上 
来讲 我们 会 把 认知 技术 分为 三 个 层面 
第 一种 是 静态 层面 我 随便 说 一句话 自然语言 
能 不能 理解 能 不能 映射 到 正确 的 意思 
上面 去 第二类 是 交互 决策 意思 是 我 在 
说话 的 时候 如何 进行 反馈 比如 我 对 一个 
机器 说 我 要 找到 餐馆 它 要 明白 我 
想去 哪 吃什么 第三 是 进化 我 想要 便宜 的 
东西 它 却 以为 我 想要 贵 的 当 它 
发现 错了 之后 下一次 一定 要 更新 自己 的 反馈 
策略 进化 出 自己 的 认知 聊 一件 和 各位 
相关 的 事情 大 规模 可 定制 对话 智能 在 
讲 整个 对话 智能 的 时候 我们 会 发现 在 
整个 流程 里面 每 一个 环境 都 看起来 很 美好 
但 一到 专业 领域 的 环节 就 会 变得 不 
一样 了 比如 做 对话 模式 做 购物 的 场景 
与 金融 家庭 的 场景 所 理解 的 东西 完全 
不 一样 这个/r 时候/n 就要/d 看/v 做/v 出来/v 的/uj 模型/n 
是否/v 每/zg 一个/m 场景/n 都能/nr 识别/v 是否 能 很好 的 
支持 在 细节 上面 还有 很多 个性化 需求 例如 唤醒 
当 我们 喊 小乐 给 我 放 一首歌 的 时候 
这个 小 乐就 是 一种 唤醒 但 有的 时候 我们 
希望 它 有 好几 个 名字 这种 需要 多 唤醒 
词 的 需求 在 未来 会 出现 更多 当 我们 
真正 去做 的 时候 会 希望 在 我们 所 使用 
的 口语 对话 系统 上 的 支撑 可以 定制 而 
大 规模 可 定制 是 我们 提出 的 新概念 在 
2013年 我们 发布 了 一个 叫 对话 工场 的 平台 
2017年 升级到 大规模 可 定制 的 Dialogue User Interface DUI 
其 本质 上 是 把 图形界面 和 语音 界面 在 
对话 交互 的 框架 下 结合 在 一起 定制 性 
的 语音 交互 技术 可以 做 什么 这时候 我们 会 
好奇 这些 定制 技术 能做/nr 什么 呢 比如 可以 在 
做 实时 语音 识别 和大/nr 词汇 语音 识别 的 时候 
做 出来 一个 功能 当 语义 改变 的 时候 语音识别 
会 对 我们 自动 添加 的 词 做 自动识别 比如 
我们 添加 了 泷泽萝拉 四 个字 语音 识别 系统 能 
自动 把 它 加入 词表 并 具有 识别 的 能力 
继而 在 实现 理解 和 交互 我们 想 要做 一件 
事情 在 一个 车载 的 系统 里面 自动 选择 一些 
声音 添加 进去 当 想要 林志玲 甜甜 声音 的 时候 
喊一声 林志玲 出来 绝对 不会 再 出来 郭德纲 的 声音 
让 它 回去 它 就会 切换 为 原本 的 郭德纲 
声音 我们 希望 这样 的 事情 可以 很 自由 的 
来回 切换 更进一步 我们 要 支持 对 理解 和 对话 
进行 相应 的 定制 在 这个 过程 里 在 我们 
真正 背后 的 技术 上 来说 已经 不再 是 一般 
的 语音 的 和 对话 的 交互 不再 仅仅 是 
前面 我们 提到 的 感知 和 认知 的 独立 框架 
在 这里 要 解决 的 问题 是 所谓 大 规模 
可 定制 的 一些 新 技术 比如说 在 识别 里 
要 解决 所谓 的 自适应 的 问题 比如说 话人 和 
环境 的 自适应 领域 主题 的 自适应 等 这些 东西 
可以 及时 的 去 改变 它 可以 使得 对话 有 
很多 的 自适应 如果 实现 这些 自适应 规模化 的话 还 
需要 有 相应 的 系统 支持 在 这个 过程 里 
需要 有 具体 的 技术 拆借 需要 有 模型 定制 
能够 使得 它 规模化 的 扩展 并且 在 个性 的 
基础 之上 去 进行 进化 这一类 东西 里 会 有 
很多 新型 的 技术 出现 但 这些 技术 都 需要 
技术 基础 的 支撑 课程 推荐 使用 keras 快速 构造深度 
学习 模型 实战 微软 & 谷歌 数据 科学家 带 你 
每周 案例 实战 史上 最 高性价比 两位 顶尖 的 微软 
/ 谷歌 数据 科学家 直播 互动 分享 珍贵 学习 经验 
并 详细 讲解 前沿 实战 案例 GPU 云 实验 平台 
提供 便捷 的 操作环境 还有 原著 大作 免费送 七周 时间 
带 你 玩转 Keras 很多/m 即将/d 毕业/n 和/c 渴望/v 转型/n 
的/uj 小伙伴/n 都/d 加入/v 了/ul 我们/r 你 不来 吗 往 
期 精彩文章 点击 图片 阅读 沿着 地铁 买房 怎样 更 
划算 2017 上海 城市 大 数据 报告 发布 博主 github 
https / / github . com / MichaelBeechan 博主 CSDN 
https / / blog . csdn . net / u 
0 1 1 3 4 4 5 4 5 N 
a t u r a l Language Processing 自然语言 处理 
NLP 团队 1 The Berkeley NLP Group 加州 大学 伯克利 
分校 http / / nlp . cs . berkeley . 
edu / index . shtml2 The Stanford NLP Group 斯坦福大学 
https / / nlp . stanford . edu / Chris 
M a n n i n g s t a 
t i s t i c a l natural language 
processing natural language understanding and deep learningDan Jurafskynatural language understanding 
conversational speech and dialog NLP for the behavioral and social 
sciencesPercy Liangsemantic parsing probabilistic models for NLP machine learning program 
induction3 The Language Technologies Institute at Carnegie Mellon 卡内基 梅隆 
大学 https / / www . lti . cs . 
cmu . edu / 4 Natural Language Processing Group Notre 
Dame 圣母 大学 https / / nlp . nd . 
edu / 5 Harvard NLP 哈佛大学 http / / nlp 
. seas . harvard . edu / http / / 
nlp . seas . harvard . edu / code / 
Unsupervised Recurrent Neural Network GrammarsLearning Neural Templates for Text Generation6 
中科院计算所 自然语言 处理 研究组 http / / nlp . ict 
. ac . cn / 2017 / introduction . php 
自然语言 处理 研究组 隶属于 中国科学院计算技术研究所 智能 信息 处理 重点 实验室 
研究组 主要 从事 自然语言 处理 和 机器 翻译 相关 的 
研究 工作 主要 研究 方向 包括 机器翻译 人机对话 多语 言词 
法分析 句法分析 等 研究组 自 2004年 开展 统计 机器翻译 方面 
的 研究 并 取得 重大 突破 并于 2015年 起 转向 
神经 机器翻译 并 取得 很大 进展 研究组 在 自然 语言 
处理 的 顶级 国际 刊物 CL AI 和 顶级 国际 
学术 会议 ACL IJCAI AAAI EMNLP COLING 上 发表 高水平 
论文 90 余篇 并 获 ACL 颁发 的 亚洲 自然语言 
处理 优秀 论文 奖 多次 在 CWMT 评测 和 IWSLT 
评测 中 获得 第一名 并 获得 最 权威 的 国家 
机器翻译 评测 NIST 评测 国内 最 好成绩 部分 研究 成果 
获 北京市 科技 进步 奖 钱伟长 中文信息处理 科学技术 奖 研究组 
已 完成 和 正在 承担 的 国家 自然科学 基金 863 
计划 科技 支撑 计划 国际 合作 等 课题 40 余项 
在 自然 语言 处理 和 机器 翻译 领域 取得 了 
多项 创新 性 研究 成果 研究组 已经 成功 将 自主 
开发 的 统计 机器 翻译 和 神经 机器 翻译 技术 
推广 到 汉语 维吾尔语 藏语 蒙古语 英语 韩语 泰语 日语 
阿拉伯语 等 多种 语言 并与 腾讯 华为 中移动 三星 等 
公司 以及 部分 政府 机关 开展 合作 部分 语种 的 
翻译 系统 已经 在 相关 领域 得到 了 实际 应用 
获得 用户 的 好评 7 哥伦比亚大学 http / / www 
. cs . columbia . edu / people / faculty 
/ http / / www . cs . columbia . 
edu / areas / speech / 8 哈工大 社会 计算 
与 信息检索 研究中心 http / / ir . hit . 
edu . cn / teams 哈工大 社会 计算 与 信息检索 
研究中心 HIT SCIR 成立 于 2000 年 9月 隶属 于 
计算机 科学 与 技术 学院 研究 中心 主任 刘挺/nr 教授 
副 主任 秦兵 教授 教师 包括 张宇 教授 车 万翔 
教授 刘铭 副教授 / 博导 张伟 男 副教授 丁效/nr 博士 
讲师 冯骁骋/nr 博士 助理 研究员 行政主管 李冰 老师 李艺雯/nr 老师 
还有 多位 校内 其他 院系 的 老师 参与 实验室 的 
研究 工作 包括 赵 妍妍 副教授 / 博导 张紫琼/nr 教授 
/ 博士后 景东 讲师 等 研究 方向 包括 语言 分析 
信息 抽取 情感 分析 问答 系统 社会 媒体 处理 和 
用户 画像 6 个 方面 已完成 或 正在 承担 的 
国家 973 课题 国家 自然科学 基金 重点 项目 国家 863 
重点项目 国际合作 企业 合作 等 课题 60 余项 在 这些 
项目 的 支持 下 打造出 语言 技术 平台 LTP 并 
免费 共享 给 400 多家 研究 机构 百度 腾讯 华为 
金山 等 企业 付费 使用 获 2010年 钱伟长 中文信息处理 科学技术 
一等奖 9 自然语言 处理 研究小组 http / / nlpr web 
. ia . ac . cn / cip / introduction 
. htm 自然语言 处理 是 利用 计算机 技术 处理 人类 
自然 语言 的 一门 交叉 型 学科 涉及 计算机科学 数学 
逻辑学 语言学 和 认知 科学 等 多个 领域 模式识别 国家 
重点 实验室 自然语言 处理 组 主要 从事 自然语言 处理 基础 
机器翻译 信息 抽取 和 问答 系统 等 相关 研究工作 力图 
在 自然 语言 处理 的 理论 模型 和 应用 系统 
开发 方面 做出 创新 成果 目前 研究组 的 主要 方向 
包括 自然语言 处理 基础 技术 汉语 词语切分 句法分析 语义分析 和 
篇章 分析 等 多语言 机器翻译 信息 抽取 实体 识别 实体 
关系 抽取 观点 挖掘 等 和 智能 问答 系统 基于 
知识库 的 问答 系统 知识 推理 社区 问答 等 近年来 
研究组/n 注重/v 于/p 自然/d 语言/n 处理/v 基础理论/n 和/c 应用/v 基础/n 
的/uj 相关/v 研究/vn 取得 了 一批 优秀 成果 承担 了 
一系列 包括 国家 自然科学 基金 项目 973 计划 课题 863 
计划/n 项目/n 和/c 支撑/v 计划/n 项目/n 等/u 在内/u 的/uj 基础/n 
研究/vn 和/c 应用/v 基础/n 研究/vn 类/q 项目/n 以及 一批 企业 
应用 合作项目 在 自然 语言 处理 及 相关 领域 顶级 
国际 期刊 CL TASLP TKDE JMLR TACL Information Sciences Intelligent 
Systems 等 和 学术 会议 AAAI IJCAI ACL SIGIR WWW 
等 上 发表 了 一 系列 高 水平 的 研究 
论文 2009年 获得 第 23届 亚太 语言 信息 与 计算 
国际 会议 PACLIC 最佳 论文 奖 2012年 获得 第一 届 
自然语言 处理 与 中文 计算 会议 NLPCC 最佳 论文 奖 
2014年 获得 第 25届 国际 计算 语言学 大会 COLING 最佳 
论文 奖 获得 了 10余 项 国家 发明 专利 10 
Natural Language Computing 微软 自然语言 处理 计算 https / / 
www . microsoft . com / en us / research 
/ group / natural language computing / 欢迎 关注 天善智能 
我们 是 专注 于 商业智能 BI 人工智能 AI 大 数据 
分析 与 挖掘 领域 的 垂直 社区 学习 问答 求职 
一站式 搞定 对 商业智能 BI 大 数据分析 挖掘 机器学习 python 
R 等 数据 领域 感兴趣 的 同学 加 微信 tstoutiao 
邀请 你 进入 数据 爱好者 交流 群 数据 爱好者 们 
都 在这儿 作者 黄 天元 复旦 大学 博士 在读 目前 
研究 涉及 文本 挖掘 社交 网络 分析 和 机器 学习 
等 希望 与 大家 分享 学习 经验 推广 并 加深 
R 语言 在 业界 的 应用 邮箱 huang . tian 
yuan @ qq . com 前文 推送 R 语言 自然语言 
处理 中文分词 R 语言 自然语言 处理 词性 标注 与 命名 
实体 识别 R 语言 自然语言 处理 关键词 提取 TF IDF 
R 语言 自然语言 处理 关键词 提取 与 文本 摘要 TextRank 
R 语言 自然语言 处理 文本 向 量化 词 嵌入 Word 
Embedding R 语言 自然语言 处理 情感 分析 不知不觉 已经 写 
了 这么 多 但是 很多 R 语言 自然语言 处理 的 
方法 并 没有 展开 来讲 这次 希望 尝试 用 简单 
的 技术 TF IDF 和 相似 度 矩阵 做 一次 
实践 即 文档 分类 任务 定义 对于 任意 给定 的 
一个 字符串 判断 它 与 目前 哪个 文档 最为 相似 
从而 进行 归类 首先 要 对 当前 的 文档 数据 
见 github . com / hope data sc 做 词 
嵌入 就用 最 简单 的 TF IDF 模型 然后 对于 
任意 的 新 字符串 进行 向 量化 之后 与 先前 
的 标准 库 做 相似性 的 分析 看看 与 哪个 
文档 相似性 最近 就 属于 哪 一个 类别 1 读入 
文件 1 library pacman 2 p _ load tidyverse data 
. table 34 fread classification _ corpus _ raw . 
csv encoding   = UTF 8   % % 5 
as _ tibble   % % 6 mutate id = 
1 n   raw 这样 文件 就在 raw 中了 2 
计算 TF IDF 这 一部分 参考 R 语言 自然语言 处理 
关键词 提取 TF IDF 先 进行 分词 然后 对 所有 
的 词 计算 TF IDF 1 # #   快速 
分词 2 p _ load jiebaR 3 worker     
wk45 raw   % % 6 mutate words   = 
  map title segment jieba   =   wk   
% % 7 select id words     corpus89 # 
#   计算 TF IDF10 corpus   % % 11 
unnest   % % 12 count id words   % 
% 13 bind _ tf _ idf term   = 
  words document   = id n   =   
n     corpus _ tf _ idf 仔细看 这个 
文档 现在 究竟 有 多少 个 词语 呢 1 corpus 
_ tf _ idf % %   distinct words 23 
#   A   tibble   1 510   x 
  14 words5 chr 6 1 百年 7 2 办公室 
8 3 筹备工作 9 4 校庆 10 5 保卫部 11 
6处 12 7 安全 13 8 管理 14 9 生产 
15 10 保密 16 #   . . .   
with   1 500   more   rows 一共 1510个 
不多 因此 我 决定 不 进行 筛选 了 本来 常规 
套路 要把 这个 TF IDF 的 矩阵 变为 一个 文档 
词语 矩阵 Document Term Matrix DTM 但是 既然 走了 tidy 
的 路线 我 突然 认为 那是/nr 一个 多余 的 步骤 
做 了 一个 高维 稀疏 的 矩阵 效率 异常 低 
而 进行 连接 join 的 速度 可谓 异常 地 快 
下面 我 要写 一个 函数 它 要 完成 一个 这样 
的 任务 对于 任意 给定 的 字符串 求 这个 字符串 
与 当前 所有 文档 的 相似性 然后 筛选出 相似性 最高 
的 n 个 文档 显示出来 虽然 不 需要 构造 矩阵 
但是 我 还是 要 构造 一个 类似 的 数据 框 
1 corpus _ tf _ idf % % 2 select 
id tf _ idf     for _ future _ 
use3 举例 尝试 先 假设 给定 的 字符串 为 大 
数据 学院 我们 看看 是否 能够 找到 合理 的 相似 
文档 我们 首先 要 明确 什么 叫做 相似 定义 1 
字符串 中 包含 相同 的 组分 相同 的 分词 结果 
2 当 包含 组分 数量 一致 的 时候 如果 包含 
重要 表征 组分 其 得分 更高 举例说明 我们 给定 的 
字符串 是 物理学院 分词 之后 是 物理 和 学院 但是 
物理 这个词 能够 表征 的 程度 更高 因此 它 会 
得到 更高 的 得分 这个 得分 在 我们 的 模型 
中 是以 TF IDF 的 形式 存在 的 下面 我们 
给出 代码 1 string = 大 数据 学院 23 string 
% % 4 segment jiebar   =   wk   
% % 5 enframe   % % 6 transmute words 
  = value     string _ table78 for _ 
future _ use   % % 9 inner _ join 
string _ table   % % 10 group _ by 
id   % % 11 summarise score   =   
sum tf _ idf   % % 12 arrange desc 
score     sort _ table1314 sort _ table   
% % 15 slice 1 5   % % 16 
inner _ join raw by = id 1718 #   
A   tibble   5   x   319 id 
  score   title2021 1584.70大 数据 学院 22 2572.86大 数据 
研究院 23 31091.84 高级律师 学院 24 44361.84 公共卫生 学院 25 
54791.84 管理学院 我们 可以 看到 大 数据 学院 被 正确地 
筛选 出来 而 排名 第二 的 是 大 数据 研究院 
因为 大 数据 作为 一个 比 学院 拥有 更高 TF 
IDF 的 关键词 更 能够 表征 大 数据 这个 特征 
其他 3个 选项 得分 其实 是 一样 的 它们 都 
因为 有 学院 而被 筛选 出来 但是 没有 匹配 更多 
更 有价值 的 词语 了 现在 我们 就 可以 正式 
对 函数 进行 构造 1 get _ sim   = 
  function string { 2 string % % 3 segment 
jiebar   =   wk   % % 4 enframe 
  % % 5 transmute words   = value   
  string _ table67 for _ future _ use   
% % 8 inner _ join string _ table by 
= words   % % 9 group _ by id 
  % % 10 summarise score   =   sum 
tf _ idf   % % 11 arrange desc score 
    sort _ table1213 sort _ table   % 
% 14 slice 1 3   % % 15 inner 
_ join raw by = id   result1617 ifelse nrow 
result = = 0 18 NA 19 result   % 
% 20 pull title   % % 21 str _ 
c collapse   = 22 } 这个 函数 能够 对 
任意 的 字符串 进行 识别 如果 没有 任何 识别 就 
返回 NA 如果 识别 到了 最多 返回 匹配度 最高 的 
3个 分类 分类 之间 以 分隔 注意 是 英文 的 
逗号 这个 可以 根据 自己 的 洗好 更改 我们 用 
两个 例子 看看 结果 如何 1 get _ sim 稀奇古怪 
2 1 NA34 get _ sim 大 数据 5 1 
大 数据 研究院 大 数据 学院 大 数据 试验场 研究院 
筹 显然 这个 函数 是 有效 的 往 期 精彩 
今天 我 改名 了 不敢 穷 不敢 病 不 敢死 
我们 是 独生子女 qkerntool 使用 说明 R 语言 中文 社区 
2018 年终 文章 整理 作者 篇 R 语言 中文 社区 
2018 年终 文章 整理 类型 篇 整理 的 部分 网络资源 
文章 后续 会 持续 补充 1 综述 博客 我 爱 
自然 语言 处理 http / / www . 52nlp . 
cn / 码 农场 http / / www . hankcs 
. com / 2 书籍 和 入门 自然语言 相关 资源 
入门 http / / www . 52nlp . cn / 
about 自然语言 处理 书籍 推荐 http / / www . 
52nlp . cn / % E 4% B 9% A 
6% E 7% B 1% 8D 水木 论坛 自然语言 处理 
版 http / / www . newsmth . net / 
bbsdoc . php board = NLP 我 爱 自然 语言 
处理 整理 的 资源 http / / www . 52nlp 
. cn / resources 知乎 上 推荐 入门 http / 
/ www . zhihu . com / question / 19895141 
北京大学 中文系 应用 语言学 专业 课程 安排 http / / 
ccl . pku . edu . cn / all / 
info . asp item = 2 & page = 1 
& expand = 63 研究 主题 分词 Aho Corasick 算法 
的 Java 实现 与 分析 http / / www . 
hankcs . com / program / algorithm / implementation and 
analysis of aho corasick algorithm in java . htmlTrie 树 
分词 http / / www . hankcs . com / 
program / java / tire tree participle . htmlAnsj 分词 
双 数组 Trie 树 实现 与 arrays . dic 词典 
格式 http / / www . hankcs . com / 
nlp / ansj word pairs array tire tree achieved with 
arrays dic dictionary format . html 双 数组 Trie 树 
DoubleArrayTrie Java 实现 http / / www . hankcs . 
com / program / java / % E 5% 8F 
% 8C % E 6% 95% B 0% E 7% 
BB % 84trie % E 6% A 0% 9 1 
d o u b l e a r r a 
y t r i e j a v a % 
E 5% AE % 9E % E 7% 8E % 
B0 . html 基于 深度 学习 的 分词 系统 采用 
tensorflowhttps / / github . com / koth / kcws 
  训练 主题 分类 LDA 入门 与 Java 实现 http 
/ / www . hankcs . com / nlp / 
lda java introduction and implementation . htmlLDA 数学 八卦 http 
/ / www . 52nlp . cn / lda math 
% E 6% B 1% 87% E 6% 80% BB 
lda % E 6% 95% B 0% E 5% AD 
% A 6% E 5% 85% AB % E 5% 
8D % A6 深度 学习 tensorflow 相关 英文 官方 网站 
http / / tensorflow . org / 官方 GitHub 仓库 
https / / github . com / tensorflow / tensorflow 
中文版 GitHub 仓库 https / / github . com / 
jikexueyuanwiki / tensorflow zh4 语料库 汇总 语料库 资源 https / 
/ www . douban . com / note / 269081724 
/ 中文 文本 语料库 整理 不定 时 更新 2015 10 
24 . mdhttp / / www . jianshu . com 
/ p / 206caa232ded 中文 文本 分类 的 新闻 语料库 
http / / www . 52nlp . cn / opencorpus 
搜狗 实验室 语料库 http / / www . sogou . 
com / labs / resource / list _ yuliao . 
php5 论坛 语料库 语言学 在线 http / / www . 
corpus4u . org / 国内 语料库 建设 一览表 http / 
/ blog . csdn . net / yujun00 / article 
/ details / 5416336 新 增加 的 地址 自然语言 一个 
微 信号 https / / mp . weixin . qq 
. com / s _ _ biz = MzI3ODgwODA2MA = 
= & mid = 2247484260 & idx = 1 & 
sn = 3 d f 8 f 9 1 5 
c 7 a e d 6 0 3 8 d 
5 0 4 3 6 3 8 1 c c 
8 e 5 b & chksm = e b 5 
0 1 7 f 7 d c 2 7 9 
e e 1 2 a 6 f 3 2 4 
e 3 3 2 a a 5 4 d 1 
b 0 1 1 0 2 7 2 5 a 
d d 8 c f 1 b a c 2 
d 6 2 0 8 7 0 2 0 0 
d 1 d c e 0 2 8 8 a 
7 4 9 & mpshare = 1 & scene = 
23 & srcid = 0 4 1 2 D v 
q Q p 9 N j 5 O C 8 
9 2 5 q D L R s # rd 
中文信息 学报 核定 刊物 https / / item . taobao 
. com / item . htm spm = a230r . 
1 . 14.16 . b6e73fc2I1NRvB & id = 567299675084 & 
ns = 1 & abbucket = 5 # detail 中文 
信息学 报高 引用 文章 http / / jcip . cipsc 
. org . cn / CN / column / column35 
. shtmloxford cs deepnlp 2017 / lectures   https / 
/ github . com / oxford cs deepnlp 2017 / 
lectures 深度 与 自然 语言 处理 书 https / / 
zhuanlan . zhihu . com / p / 25612011NLP 进阶 
书籍 说明 http / / www . sohu . com 
/ a / 211831610 _ 473283NLP 工具包 大调查 自然语言 处理 
工具包 合集 原创 作品 转载 请 注明 出处   Mr 
. Scofield     http / / blog . csdn 
. net / scotfield _ msn / article / details 
/ 72904863   From RxNLP . 可以 想一想 如何/r 你/r 
把/p NLP/w 领域/n 的/uj 所有/b 的/uj 工具/n 都能/nr 掌握/v 的/uj 
数/n 如/v 家珍/nr 是不是 很 NB 必然 的 只 用过 
这 里面 的 一部分 这份 调查 是 基于 使用 语言 
差 别来 归纳 的 别问 我 什么 这么 分类 哈 
一 多语言 多 环境 编译 1 THULAC { 分词 词性 
标注 } c + + / python / java2 . 
NLPIR2016 { 汉语分词 系统 NLPIR 前身 ICTCLAS 分词 标注 实体 
抽取 词频 统计 关键词 提取 Word2vec 文本 分类 情感 分析 
依存 文法 繁简 编码 转换 自动 注音 摘要 提取 { 
original https / / github . com / NLPIR team 
/ NLPIRjava JNI http / / blog . csdn . 
net / u010161379 / article / details / 50813012python http 
/ / blog . csdn . net / junkichan / 
article / details / 51883160 } } c + + 
/ python / java3 . crfsuite { A fast implementation 
of Conditional Random Fields CRFs Fast training and tagging } 
c + + / python4 . 哈工大 LTP { 一整套 
中文 语言 处理 系统 以 网络 服务 Web Service 的 
形式 进行 使用 1 分词 2 词性 标注 3 依存 
句法分析 } c + + / java5 . Libsvm { 
Libsvm 和 Liblinear 都是 国立 台湾 大学 的 Chih Jen 
Lin 博士 开发 的 Libsvm 主要 是 用来 进行 非线性 
svm 分类器 的 生成 } c + + / python 
/ java / matlab6 . Liblinear { Liblinear 则是 去年 
才 创建 的 主要 是 应对 large scale 的 data 
classification 因为 linear 分类器 的 训练 比 非线性 分类器 的 
训练 计算 复杂度 要 低 很多 时间 也 少 很多 
而且在 large scale data 上 的 性能 和 非线性 的 
分类器 性能 相当 所以 Liblinear 是 针对 大 数据 而生 
的 } c + + / python / java / 
matlab 二 Java1 IKAnalyzer { 中文分词 工具包 } 2 FNLP 
{ 综合性 工具 信息检索 文本 分类 新闻 聚 类 中文 
处理 中文分词 词性 标注 实体 名 识别 关键词 抽取 依存 
句法分析 时间 短语 识别 结构化 学习 在线 学习 层次 分类 
聚 类 } 3 . hanLP { 综合性 工具 分词 
词典 命名 实体 识别 篇章 理解 简繁 拼音 转换 依存 
句法 解析 智能 推荐 } 4 . openNLP { 综合性 
工具 Apache OpenNLP 句法 检测器 Sentence Detector 分词器 Tokenizer 名字 
查找 器 Name Finder 文档 分类器 Document Categorizer 词性 标注 
器 Part of Speech Tagger 组块 分析器 Chunker 解析器 Parser 
指代 消解 Coreference Resolution } 5 . stanfod NLP { 
综合性 工具 Stanford CoreNLP 分词 词性 标注 命名 实体 识别 
语法分析 Stanford Word Segmenter 采用 CRF 条件 随 机场 算法 
进行 分词 Stanford POS Tagger 采用 Java 编写 的 面向 
英文 中文 法语 阿拉伯语 德语 的 命名 实体 识别 工具 
Stanford Named Entity Recognizer 采用 条件 随 机场 模型 的 
命名 实体 工具 Stanford Parser 进行 语法分析 的 工具 Stanford 
Classifier } 6 ansj { 基于 中科院 的 ictclas 中文分词 
算法 中文分词 . 实体 识别 用户 自定义 词典 } 7 
. lingpipe { 主题 分类 Top Classification 命名 实体 识别 
Named Entity Recognition 词性 标注 Part of Speech Tagging 句 
题 检测 Sentence Detection 查询 拼 写检查 Query Spell Checking 
兴趣 短语 检测 Interseting Phrase Detection 聚 类 Clustering 字符 
语言 建模 Character Language Modeling 医学 文献 下载 / 解析 
/ 索引 MEDLINE Download Parsing and Indexing 数据库 文本 挖掘 
Database Text Mining 中文分词 Chinese Word Segmentation 情感 分析 Sentiment 
Analysis 语言 辨别 Language Identification } 8 . GATE The 
General Architecture for Text Engineering { 针对 不同 的 用 
例 提供 了 一系列 子项目 } 9 . MALLET Machine 
Learning for Language Toolkit { 文档 分类 聚 类 主题 
建模 和 信息 提取 } 三 C / C + 
+ 1 CRF + + { crf 实现 工具 适用于 
序列 标注 问题 } 2 . HTK Hidden Markov Model 
Toolkit { 英国剑桥大学 工程学院 开发 的 隐 马尔可夫 模型 做 
语音 识别 的 } 3 . svm light { SVMlight 
is an implementation of Support Vector Machines SVMs in C 
} 四 ※ Python 调查 除 下列 包 之外 { 
s c i p y n u m p y 
s k l e a r n p a n 
d a s m a t p l t i 
P y t h o n P y B r 
a i n P y M L machine learning in 
PythonMilk Machine learning toolkit in Python . PyMVPA MultiVariate Pattern 
Analysis MVPA in PythonPyrallel Parallel Data Analytics in PythonMonte gradient 
based learning in Pythonxgboost } 1 . gensim { Corpora 
and Vector SpacesTopics and Transformations LSA LDA TF IDF Experiments 
on the English W i k i p e d 
i a D i s t r i b u 
t e d Computing word2vec } 2 . jieba { 
功能 1 分词 功能 2 添加 自定义 词典 功能 3 
关键词 提取 功能 4 词性 标注 功能 5 并行 分词 
功能 6 Tokenize 返回 词语 在 原文 的 起始 位置 
} 3 . NLTK { Tokenize and tag some text 
Identify named entities Display a parse tree 它 提供 了 
WordNet 这种 方便 处理 词汇 资源 的 借口 还有 分类 
分词 除 茎 标注 语法分析 语义 推理 等 类库 } 
4 . TextBlob { 词性 标注 名 词性 成分 提取 
情感 分析 文本 翻译 } 5 . PyNLPI { 处理 
N 元 搜索 计算 频率 表 和 分布 建立 语言 
模型 他 还 可以 处理 向 优先 队列 这种 更加 
复杂 的 数据 结构 或者 像 Beam 搜索 这种 更加 
复杂 的 算法 Segmenting TextGetting Key Words } 6 . 
spaCy { 结合 Python 和 Cython 是 具有 工业级 强度 
的 Python NLP 工具包 英文 断句 词干 化 Lemmatize 词性 
标注 POS Tagging 命名 实体 识别 NER 名词 短语 提取 
} 7 . polyglot { T o k e n 
i z a t i o n L a n 
g u a g e d e t e c 
t i o n M o r p h o 
l o g i c a l analysisNamed Entity R 
e c o g n i t i o n 
e n t i m e n t AnalysisWord Embeddings 
} 目录 文章 目录 目录 一 人工智能 学习 算法 分类 
1 . 纯 算法 类 2 . 建模 方面 二 
详细 算法 1 . 分类 算法 2 . 回归 算法 
3 . 聚 类 算法 4 . 降 维 算法 
5 . 概率 图 模型 算法 6 . 文本 挖掘 
算法 7 . 优化 算法 8 . 深度 学习 算法 
三 建模 方面 1 . 模型 优化 2 . 数据 
预处理 一 人工智能 学习 算法 分类 人工 智能算法 大体上 来说 
可以 分类 两类 基于 统计 的 机器学习 算法 Machine Learning 
和 深度 学习 算法 Deep Learning 总的来说 在 sklearn 中 
机器学习 算法 大概 的 分类 如下 1 . 纯 算法 
类 1 . 回归 算法 2 . 分类 算法 3 
. 聚 类 算法 4 降 维 算法 5 概率 
图 模型 算法 6 文本 挖掘 算法 7 优化 算法 
8 深度 学习 算法 2 . 建模 方面 1 . 
模型 优化 2 . 数据 预处理 二 详细 算法 1 
. 分类 算法 1 . LR Logistic Regression 逻辑 回归 
又叫 逻辑 分类 2 . SVM Support Vector Machine 支持 
向量 机 3 . NB Naive Bayes 朴素 贝叶斯 4 
. DT Decision Tree 决策树 1 . C 4.52 . 
ID33 . CART 5 . 集成 算法 1 . Bagging2 
. Random Forest 随机 森林 3 . GB 梯度 提升 
Gradient boosting 4 . GBDT Gradient Boosting Decision Tree 5 
. AdaBoost6 . Xgboost 6 . 最大熵 模型 2 . 
回归 算法 1 . LR Linear Regression 线性 回归 2 
. SVR 支持 向量 机 回归 3 . RR Ridge 
Regression 岭回归 3 . 聚 类 算法 1 . Knn 
2 . Kmeans 算法 3 . 层次 聚 类 4 
. 密度 聚 类 4 . 降 维 算法 1 
. SGD 随机 梯度 下降 5 . 概率 图 模型 
算法 1 . 贝叶斯 网络 2 . HMM 3 . 
CRF 条件 随 机场 6 . 文本 挖掘 算法 1 
. 模型 1 . LDA 主题 生成 模型 Latent Dirichlet 
Allocation 2 . 最大熵 模型 2 . 关键词 提取 1 
. tf idf2 . bm253 . textrank4 . pagerank5 . 
左右 熵 左右 熵 高的/nr 作为 关键词 6 . 互信息 
3 . 词 法分析 1 . 分词 – ① HMM 
因 马尔科夫 – ② CRF 条件 随 机场 2 . 
词性 标注 3 . 命名 实体 识别 4 . 句法分析 
1 . 句法结构 分析 2 . 依存 句法分析 5 . 
文本 向 量化 1 . tf idf2 . word2vec3 . 
doc2vec4 . cw2vec 6 . 距离 计算 1 . 欧氏距离 
2 . 相似 度 计算 7 . 优化 算法 1 
. 正则化 1 . L1 正则化 2 . L2 正则化 
8 . 深度 学习 算法 1 . BP 2 . 
CNN 3 . DNN 3 . RNN 4 . LSTM 
三 建模 方面 1 . 模型 优化 1 . 特征选择 
2 . 梯度 下降 3 . 交叉 验证 4 . 
参数 调 优 5 . 模型 评估 准确率 召回率 F1 
AUC ROC 损失 函数 2 . 数据 预处理 1 . 
标准化 2 . 异常值 处理 3 . 二 值 化 
4 . 缺失 值 填充 支持 均值 中位数 特 定值 
补差 多重 插补 1 Python 的 几个 自然语言 处理 工具 
NLTK NLTK 在用 Python 处理 自然 语言 的 工具 中 
处于 领先 的 地位 它 提供 了 WordNet 这种 方便 
处理 词汇 资源 的 借口 还有 分类 分词 除 茎 
标注 语法分析 语义 推理 等 类库 Pattern Pattern 的 自然 
语言 处理 工具 有 词性 标注 工具 Part Of Speech 
Tagger N 元 搜索 n gram search 情感 分析 sentiment 
analysis WordNet 支持 机器 学习 的 向量空间 模型 聚 类 
向量 机 TextBlob TextBlob 是 一个 处理 文本 数据 的 
Python 库 提供 了 一些 简单 的 api 解决 一些 
自然 语言 处理 的 任务 例如 词性 标注 名词 短语 
抽取 情感 分析 分类 翻译 等等 Gensim Gensim 提供 了 
对 大型 语料库 的 主题 建模 文件 索引 相似 度 
检索 的 功能 它 可以 处理 大于 RAM 内存 的 
数据 作者 说 它 是 实现 无 干预 从纯/nr 文本 
语义 建模 的 最强 大 最 高效 最 无障碍 的 
软件 PyNLPI 它 的 全称 是 Python 自然语言 处理 库 
Python Natural Language Processing Library 音 发作 pineapple 这 是 
一个 各 种 自然 语言 处理 任务 的 集合 PyNLPI 
可以 用来 处理 N 元 搜索 计算 频率 表 和 
分布 建立 语言 模型 他 还 可以 处理 向 优先 
队列 这种 更加 复杂 的 数据 结构 或者 像 Beam 
搜索 这种 更加 复杂 的 算法 spaCy 这 是 一个 
商业 的 开源 软件 结合 Python 和 Cython 它 的 
自然 语言 处理 能力 达到 了 工业 强度 是 速度 
最快 领域内 最 先进 的 自然 语言 处理 工具 Polyglot 
Polyglot 支持 对 海量 文本 和多/nr 语言 的 处理 它 
支持 对 165种 语言 的 分词 对 196中 语言 的 
辨识 40种 语言 的 专有名词 识别 16种 语言 的 词性 
标注 136种 语言 的 情感 分析 137种 语言 的 嵌入 
135种 语言 的 形态 分析 以及 69中 语言 的 翻译 
MontyLingua MontyLingua 是 一个 自由 的 训练有素 的 端 到 
端的 英文 处理 工具 输入 原始 英文 文本 到 MontyLingua 
就会 得到 这段 文本 的 语义解释 适合 用来 进行 信息 
检索 和 提取 问题 处理 回答 问题 等 任务 从 
英文 文本 中 它 能 提取 出 主 动宾 元组 
形容词 名词 和 动词短语 人名 地名 事件 日期 和 时间 
等 语义 信息 BLLIP Parser BLLIP Parser 也叫做 Charniak Johnson 
parser 是 一个 集成 了 产生 成分 分析 和 最大熵 
排序 的 统计 自然语言 工具 包括   命令行   和 /nr 
python 接口   Quepy Quepy 是 一个 Python 框架 提供 
将 自然 语言 转换 成为 数据库 查询语言 可以 轻松 地 
实现 不同 类型 的 自然 语言 和 数据库 查询语言 的 
转化 所以 通过 Quepy 仅仅 修改 几行 代码 就 可以 
实现 你 自己 的 自然 语言 查询 数据库系统 GitHub https 
/ / github . com / machinalis / quepyHanNLP HanLP 
是由 一系列 模型 与 算法 组成 的 Java 工具包 目标 
是 普及 自然语言 处理 在 生产 环境 中 的 应用 
不 仅仅 是 分词 而是 提 供词 法分析 句法分析 语义 
理解 等 完备 的 功能 HanLP 具备 功能完善 性能 高效 
架构 清晰 语料 时新 可自 定义 的 特点 文档 使用 
操作 说明 Python/w 调用/vn 自然语言/l 处理/v 包/v HanLP/w  /i 和 /nr 
菜鸟/n 如何/r 调用/vn HanNLP2/i  /i OpenNLP/w 进行 中文 命名 实体 
识别 OpenNLP 是 Apach 下 的 Java 自然语言 处理 API 
功能齐全 如下 给 大家 介绍 一下 使用 OpenNLP 进行 中文 
语料 命名 实体 识别 的 过程 首先 是 预 处理工作 
分词 去 听 用词 等等 的 就不 啰嗦 了 其 
实将 分词 的 结果 中间 加上 空格 隔开 就 可以 
了 OpenNLP 可以 将 这样 形式 的 的 语料 照 
处理 英文 的 方式 处理 有些 关于 字符 处理 的 
注意 点 在后面 会 提到 其次 我们 要 准备 各个 
命名 实体 类别 所 对应 的 词库 词库 被 存在 
文本文档 中 文 档名 即是 命名 实体类 别的 TypeName 下面 
两个 function 分别 是 载入 某类 命名 实体 词库 中的 
词 和 载入 命名 实体 的 类别 1 2 3 
4 5 6 7 8 9 1 0 1 1 
1 2 1 3 1 4 1 5 1 6 
1 7 1 8 1 9 2 0 2 1 
2 2 2 3 2 4 2 5 2 6 
2 7 2 8 2 9 3 0 3 1 
3 2 3 3 3 4 3 5 3 6 
3 7 3 8 / * * * 载入 词库 
中的 命名 实体 * * @ param nameListFile * @ 
return * @ throws Exception * / public   static 
  List String loadNameWords File nameListFile throws Exception { List 
String nameWords =   new   ArrayList String if   
nameListFile . exists | | nameListFile . isDirectory { System 
. err . println 不 存在 那个 文件 return   
null } BufferedReader br =   new   BufferedReader new 
  FileReader nameListFile String line =   null while   
line = br . readLine =   null { nameWords 
. add line } br . close return   nameWords 
} / * * * 获取 命名 实体 类型 * 
* @ param nameListFile * @ return * / public 
  static   String getNameType File nameListFile { String nameType 
= nameListFile . getName return   nameType . substring 0 
nameType . lastIndexOf . } 因为 OpenNLP 要求 的 训练 
语料 是 这 样子 的 1XXXXXX START Person END XXXXXXXXX 
START Action END XXXXXXX 被 标注 的 命名 实体 被 
放在 START END 范围 中 并 标出 了 实体 的 
类别 接下来 是 对 命名 实体 识别 模型 的 训练 
先上 代码 1 2 3 4 5 6 7 8 
9 1 0 1 1 1 2 1 3 1 
4 1 5 1 6 1 7 1 8 1 
9 2 0 2 1 2 2 2 3 2 
4 2 5 2 6 2 7 2 8 2 
9 3 0 3 1 3 2 3 3 3 
4 3 5 3 6 3 7 3 8 3 
9 4 0 4 1 4 2 4 3 4 
4 4 5 4 6 4 7 4 8 4 
9 5 0 5 1 5 2 5 3 5 
4 5 5 5 6 5 7 5 8 5 
9 6 0 6 1 6 2 6 3 6 
4 6 5 6 6 6 7 6 8 6 
9 7 0 7 1 7 2 7 3 7 
4 7 5 7 6 7 7 7 8 7 
9 8 0 8 1 8 2 8 3 8 
4 8 5 8 6 8 7 8 8 8 
9 9 0 9 1 9 2 9 3 9 
4 9 5 9 6 9 7 9 8 9 
9 1 0 0 1 0 1 1 0 2 
1 0 3 1 0 4 1 0 5 1 
0 6 1 0 7 1 0 8 1 0 
9 1 1 0 1 1 1 1 1 2 
1 1 3 1 1 4 1 1 5 1 
1 6 1 1 7 1 1 8 1 1 
9 1 2 0 1 2 1 1 2 2 
1 2 3 1 2 4 1 2 5 1 
2 6 1 2 7 1 2 8 1 2 
9 1 3 0 1 3 1 1 3 2 
1 3 3 1 3 4 1 3 5 1 
3 6 1 3 7 1 3 8 1 3 
9 1 4 0 1 4 1 1 4 2 
1 4 3 1 4 4 1 4 5 1 
4 6 1 4 7 1 4 8 1 4 
9 1 5 0 1 5 1 1 5 2 
1 5 3 1 5 4 i m p o 
r t java . io . File import java . 
io . FileOutputStream import java . io . IOException import 
java . io . StringReader import java . util . 
Collections import opennlp . tools . namefind . NameFinderME import 
opennlp . tools . namefind . NameSample import opennlp . 
tools . namefind . N a m e a m 
p l e D a t a t r e 
a m import opennlp . tools . namefind . T 
o k e n N a m e F i 
n d e r M o d e l import 
opennlp . tools . util . ObjectStream import opennlp . 
tools . util . P l a i n T 
e x t B y L i n e t 
r e a m import opennlp . tools . util 
. featuregen . A g g r e g a 
t e d F e a t u r e 
G e n e r a t o r import 
opennlp . tools . util . featuregen . P r 
e v i o u s M a p F 
e a t u r e G e n e 
r a t o r import opennlp . tools . 
util . featuregen . T o k e n C 
l a s s F e a t u r 
e G e n e r a t o r 
import opennlp . tools . util . featuregen . T 
o k e n F e a t u r 
e G e n e r a t o r 
import opennlp . tools . util . featuregen . W 
i n d o w F e a t u 
r e G e n e r a t o 
r / * * * 中文 命名 实体 识别 模型 
训练 组件 * * @ author ddlovehy * * / 
public   class   N a m e d E 
n t i t y M u l t i 
F i n d T r a i n e 
r { / / 默认 参数 private   int   
iterations = 80 private   int   cutoff = 5 
private   String langCode =   general private   String 
type =   default / / 待 设定 的 参数 
private   String nameWordsPath   / / 命名 实体 词库 
路径 private   String dataPath   / / 训练 集 
已 分词 语料 路径 private   String modelPath   / 
/ 模型 存储 路径 public   N a m e 
d E n t i t y M u l 
t i F i n d T r a i 
n e r { super / / TODO Auto generated 
constructor stub } public   N a m e d 
E n t i t y M u l t 
i F i n d T r a i n 
e r String nameWordsPath String dataPath String modelPath { super 
this . nameWordsPath = nameWordsPath this . dataPath = dataPath 
this . modelPath = modelPath } public   N a 
m e d E n t i t y M 
u l t i F i n d T r 
a i n e r int   iterations   int 
  cutoff String langCode String type String nameWordsPath String dataPath 
String modelPath { super this . iterations = iterations this 
. cutoff = cutoff this . langCode = langCode this 
. type = type this . nameWordsPath = nameWordsPath this 
. dataPath = dataPath this . modelPath = modelPath } 
/ * * * 生成 定制 特征 * * @ 
return * / public   A g g r e 
g a t e d F e a t u 
r e G e n e r a t o 
r p r o d F e a t u 
r e G e n e r a t o 
r s { A g g r e g a 
t e d F e a t u r e 
G e n e r a t o r f 
e a t u r e G e n e 
r a t o r s =   new   
A g g r e g a t e d 
F e a t u r e G e n 
e r a t o r new   W i 
n d o w F e a t u r 
e G e n e r a t o r 
new   T o k e n F e a 
t u r e G e n e r a 
t o r 2 2 new   W i n 
d o w F e a t u r e 
G e n e r a t o r new 
  T o k e n C l a s 
s F e a t u r e G e 
n e r a t o r 2 2   
new   P r e v i o u s 
M a p F e a t u r e 
G e n e r a t o r return 
  f e a t u r e G e 
n e r a t o r s } / 
* * * 将 模型 写入 磁盘 * * @ 
param model * @ throws Exception * / public   
void   w r i t e M o d 
e l I n t o D i s k 
T o k e n N a m e F 
i n d e r M o d e l 
model throws Exception { File outModelFile =   new   
File this . getModelPath FileOutputStream outModelStream =   new   
FileOutputStream outModelFile model . serialize outModelStream } / * * 
* 读出 标注 的 训练 语料 * * @ return 
* @ throws Exception * / public   String g 
e t T r a i n C o r 
p u s D a t a t r throws 
Exception { / / TODO 考虑 入 持久化 判断 直接 
载入 标注 数据 的 情况 以及 增量 式 训练 String 
trainDataStr =   null trainDataStr = N a m e 
E n t i t y T e x t 
F a c t o r y . p r 
o d N a m e F i n d 
T r a i n T e x t this 
. getNameWordsPath   this . getDataPath   null return   
trainDataStr } / * * * 训练 模型 * * 
@ param trainDataStr *             
          已 标注 的 训练 数据 
整体 字符串 * @ return * @ throws Exception * 
/ public   T o k e n N a 
m e F i n d e r M o 
d e l t r a i n N a 
m e E n t i t y a m 
p l e s String trainDataStr throws Exception { ObjectStream 
NameSample nameEntitySample =   new   N a m e 
a m p l e D a t a t 
r e a m new   P l a i 
n T e x t B y L i n 
e t r e a m new   StringReader trainDataStr 
System . out . println * * * * * 
* * * * * * * * * * 
* * * * * * * * * * 
* * * * * * * * * * 
* * * System . out . println trainDataStr T 
o k e n N a m e F i 
n d e r M o d e l nameFinderModel 
= NameFinderME . train this . getLangCode   this . 
getType nameEntitySample this . p r o d F e 
a t u r e G e n e r 
a t o r s Collections . String Object emptyMap 
  this . getIterations this . getCutoff return   nameFinderModel 
} / * * * 训练 组件 总 调用 方法 
* * @ return * / public   boolean e 
x e c N a m e F i n 
d T r a i n e r { try 
  { String trainDataStr =   this . g e 
t T r a i n C o r p 
u s D a t a t r T o 
k e n N a m e F i n 
d e r M o d e l nameFinderModel = 
  this . t r a i n N a 
m e E n t i t y a m 
p l e s trainDataStr / / System . out 
. println nameFinderModel this . w r i t e 
M o d e l I n t o D 
i s k nameFinderModel return   true }   catch 
  Exception e { / / TODO Auto generated catch 
blocke . printStackTrace return   false } } ｝ 注 
参数 iterations 是 训练 算法 迭代 的 次数 太少 了 
起 不到 训练 的 效果 太大 了 会 造成 过拟合 
所以 各 位 可以 自己 试试 效果 cutoff 语言 模型 
扫描 窗口 的 大小 一般 设成 5 就 可以 了 
当然 越大 效果 越好 时间 可能 会 受不了 langCode 语种 
代码 和 type 实体 类别 因为 没有 专门 针对 中文 
的 代码 设成 普通 的 即可 实体 的 类别 因为 
我们 想 训 练成 能 识别 多种 实体 的 模型 
于是 设置 为 默认 说明 p r o d F 
e a t u r e G e n e 
r a t o r s 方法 用于 生成 个人 
订制 的 特征 生成器 其 意义 在于 选择 什么样 的 
n gram 语义 模型 代码 当中 显示 的 是 选择 
窗口 大小 为 5 待 测 命名 实体词 前后 各 
扫描 两个 词 的 范围 计算 特征 加上 自己 就是 
5个 或许 有 更深 更 准确 的 意义 请 大家 
指正 t r a i n N a m e 
E n t i t y a m p l 
e s 方法 训练 模型 的 核心 首先 是 将如 
上 标注 的 训练 语料 字符串 传入 生成 字符 流 
再通过 NameFinderME 的 train 方法 传入 上面 设定 的 各个 
参数 订制 特征 生成器 等等 关于 源 实体 映射 对 
就按 默认 传入 空 Map 就 好了 源代码 开源 在 
https / / github . com / Ailab403 / ailab 
mltk4j test 包 里面 对 应有 完整 的 调用 demo 
以及 file 文件夹 里面 的 测试 语料 和 已经 训 
练好 的 模型 3 StanfordNLP Stanford NLP   Group 是 
斯坦福 大学 自然 语言 处理 的 团队 开发 了 多个 
NLP 工具 其 开发 的 工具 包括 以下 内容 Stanford 
CoreNLP   采用 Java 编写 的 面向 英文 的 处理 
工具 下载 网址 为 主要 功能 包括 分词 词性 标注 
命名 实体 识别 语法分析 等 Stanford Word Segmenter   采用 
CRF 条件 随 机场 算法 进行 分词 也是 基于 Java 
开发 的 同时 可以 支持 中文 和 Arabic 官方 要求 
Java 版本 1.6 以上 推荐 内存 至少 1G 简单 的 
示例 程序 1 2 3 4 5 6 7 8 
9 1 0 1 1 1 2 1 3 1 
4 1 5 1 6 1 7 / / 设置 
分词器 属性 Properties props =   new   Properties / 
/ 字典 文件 地址 可以 用 绝对路径 如 d / 
dataprops . setProperty s i g h a n C 
o r p o r a D i c t 
  data / / 字典 压缩包 地址 可以 用 绝对路径 
props . setProperty serDictionary data / dict chris6 . ser 
. gz / / 输入 文字 的 编码 props . 
setProperty inputEncoding   UTF 8 props . setProperty s i 
g h a n P o s t P r 
o c e s s i n g   true 
/ / 初始化 分词器 CRFClassifier classifier =   new   
CRFClassifier props / / 从 持久化 文件 中 加载 分词器 
设置 classifier . l o a d C l a 
s s i f i e r N o E 
x c e p t i o n s data 
/ ctb . gz props / / flags must be 
re set after data is loadedclassifier . flags . setProperties 
props / / 分词 List words = classifier . segmentString 
语句 内容 Stanford POS Tagger   采用 Java 编写 的 
面向 英文 中文 法语 阿拉伯语 德语 的 命名 实体 识别 
工具 Stanford Named Entity Recognizer   采用 条件 随 机场 
模型 的 命名 实体 工具 Stanford Parser   进行 语法分析 
的 工具 支持 英文 中文 阿拉伯文 和 法语 Stanford Classifier 
  采用 Java 编写 的 分类器 最后 附上 关于 中文 
分词器 性能 比较 的 一篇 文章 http / / www 
. cnblogs . com / wgp13x / p / 3748764 
. html 实现 中文 命名 实体 识别 1 分词 介绍 
斯坦福 大学 的 分词器 该 系统 需要 JDK 1.8 + 
从 上面 链接 中 下载 stanford segmenter 2014 10 26 
解压 之后 如下 图 所示 进入 data 目录 其中 有 
两个 gz 压缩文件 分别 是 ctb . gz 和 pku 
. gz 其中 CTB 宾州 大学 的 中国 树 库 
训练 资料   PKU 中国北京大学 提供 的 训练 资料 当然 
了 你 也 可以 自己 训练 一个 训练 的 例子 
可以 在 这里 面 看到 http / / nlp . 
stanford . edu / software / trainSegmenter 20080521 . tar 
. gz2 NER 介绍 斯坦福 NER 是 采用 Java 实现 
可以 识别 出 PERSON ORGANIZATION LOCATION 使用 本 软件 发表 
的 研究 成果 需 引用 下述 论文 下载 地址 在 
http / / nlp . stanford . edu / ~ 
manning / papers / gibbscrf3 . pdf 在 NER 页面 
可以 下载 到 两个 压缩文件 分别 是 stanford ner 2014 
10 26 和 stanford ner 2012 11 11 chinese 将 
两个 文件 解压 可 看到 默认 NER 可以 用来 处理 
英文 如果 需要 处理 中文 要 另外 处理 3 分词 
和 NER 使用 在 Eclipse 中 新建 一个 Java Project 
将 data 目录 拷贝到 项目 根 路径 下 再把 stanford 
ner 2012 11 11 chinese 解压 的 内容 全部 拷贝到 
classifiers 文件 夹下 将 stanford segmenter 3 . 5.0 加入到 
classpath 之中 将 classifiers 文件夹 拷贝到 项目 根目录 将 stanford 
ner 3 . 5.0 . jar 和 stanford ner . 
jar 加入到 classpath 中 最后 去 http / / nlp 
. stanford . edu / software / corenlp . shtml 
下载 stanford corenlp full 2014 10 31 将 解压 之后 
的 stanford corenlp 3 . 5.0 也 加入 到 classpath 
之中 最后 的 Eclipse 中 结构 如下 Chinese NER 这段 
说明 很 清晰 需要 将 中文分词 的 结果 作为 NER 
的 输入 然后 才能 识 别出 NER 来 同时 便于 
测试 本 Demo 使用 junit 4.10 . jar 下面 开始 
上 代码 1 2 3 4 5 6 7 8 
9 1 0 1 1 1 2 1 3 1 
4 1 5 1 6 1 7 1 8 1 
9 2 0 2 1 2 2 2 3 2 
4 2 5 2 6 2 7 2 8 2 
9 3 0 3 1 3 2 3 3 3 
4 3 5 3 6 3 7 i m p 
o r t edu . stanford . nlp . ie 
. A b s t r a c t e 
q u e n c e C l a s 
s i f i e r import edu . stanford 
. nlp . ie . crf . CRFClassifier import edu 
. stanford . nlp . ling . CoreLabel / * 
* * * p * ClassName ExtractDemo * / p 
* p * Description 加载 NER 模块 * * / 
public   class   ExtractDemo { private   static   
A b s t r a c t e q 
u e n c e C l a s s 
i f i e r CoreLabel ner public   ExtractDemo 
{ InitNer } public   void   InitNer { String 
s e r i a l i z e d 
C l a s s i f i e r 
=   classifiers / chinese . misc . distsim . 
crf . ser . gz   / / chinese . 
misc . distsim . crf . ser . gzif   
ner = =   null { ner = CRFClassifier . 
g e t C l a s s i f 
i e r N o E x c e p 
t i o n s s e r i a 
l i z e d C l a s s 
i f i e r } } public   String 
doNer String sent { return   ner . c l 
a s s i f y W i t h 
I n l i n e X M L sent 
} public   static   void   main String args 
{ String str =   我 去 吃饭 告诉 李强 
一声 ExtractDemo extractDemo =   new   ExtractDemo System . 
out . println extractDemo . doNer str System . out 
. println Complete } } 1 2 3 4 5 
6 7 8 9 1 0 1 1 1 2 
1 3 1 4 1 5 1 6 1 7 
1 8 1 9 2 0 2 1 2 2 
2 3 2 4 2 5 2 6 2 7 
2 8 2 9 3 0 3 1 3 2 
3 3 3 4 3 5 3 6 3 7 
3 8 3 9 4 0 4 1 4 2 
4 3 4 4 4 5 4 6 4 7 
4 8 4 9 5 0 5 1 5 2 
5 3 5 4 5 5 5 6 i m 
p o r t java . io . File import 
java . io . IOException import java . util . 
Properties import org . apache . commons . io . 
FileUtils import edu . stanford . nlp . ie . 
crf . CRFClassifier import edu . stanford . nlp . 
ling . CoreLabel / * * * * p * 
Description 使用 Stanford CoreNLP 进行 中文分词 * / p * 
* / public   class   ZH _ SegDemo { 
public   static   CRFClassifier CoreLabel segmenter static   { 
/ / 设置 一些 初始化 参数 Properties props =   
new   Properties props . setProperty s i g h 
a n C o r p o r a D 
i c t   data props . setProperty serDictionary   
data / dict chris6 . ser . gz props . 
setProperty inputEncoding   UTF 8 props . setProperty s i 
g h a n P o s t P r 
o c e s s i n g   true 
segmenter =   new   CRFClassifier CoreLabel props segmenter . 
l o a d C l a s s i 
f i e r N o E x c e 
p t i o n s data / ctb . 
gz props segmenter . flags . setProperties props } public 
  static   String doSegment String sent { String strs 
= String segmenter . segmentString sent . toArray StringBuffer buf 
=   new   StringBuffer for   String s strs 
{ buf . append s +   } System . 
out . println segmented res   + buf . toString 
return   buf . toString } public   static   
void   main String args { try   { String 
readFileToString = FileUtils . readFileToString new   File 澳门 141人 
食物中毒 与 进食 问题 生蚝 有关 . txt String doSegment 
= doSegment readFileToString System . out . println doSegment ExtractDemo 
extractDemo =   new   ExtractDemo System . out . 
println extractDemo . doNer doSegment System . out . println 
Complete }   catch   IOException e { e . 
printStackTrace } } } 注意 一定 是 JDK 1.8 + 
的 环境 最后 输出 结果 如下 4   IKAnalyzerIK Analyzer 
是 一个 开源 的 基于 Java 语言 开发 的 轻量级 
的 中文分词 工具包 IK 支持 细粒度 和 智能 分词 两种 
切分 模式 支持 英文字母 数字 中文 词汇 等 分词 处理 
兼容 韩文 日文 字符 可以 支持 用户 自定义 的 词典 
通过 配置 IKAnalyzer . cfg . xml 文件 来 实现 
可以 配置 自定义 的 扩展 词典 和 停用 词典 词典 
需要 采用 UTF 8 无 BOM 格式 编码 并且 每个 
词语 占 一行 配置文件 如下 所示 123456789 properties comment IK 
Analyzer 扩展 配置 / comment 用户 可以 在 这里 配置 
自己 的 扩展 字典 entry key = ext _ dict 
ext . dic / entry 用户 可以 在 这里 配置 
自己 的 扩展 停止词 字典 entry key = ext _ 
stopwords stopword . dic chinese _ stopword . dic / 
entry / properties 只 需要 把 IKAnalyzer2012 _ u6 . 
jar 部署 于 项目 的 lib 中 同时 将 IKAnalyzer 
. cfg . xml 文件 以及 词典 文件 置于 src 
中 即可 通过 API 的 方式 开发 调用 IK 简单 
易于 扩展 分词 结果 较好 并且 采用 Java 编写 因为 
我 平时 的 项目 以 Java 居多 所以 是 我 
平时 处理 分词 的 首选 工具 示例代码 1 2 3 
4 5 6 7 8 9 1 0 1 1 
1 2 1 3 1 4 1 5 1 6 
1 7 1 8 / * * * IK 分词 
功能 实现 * @ return * / public   String 
spiltWords String srcString { StringBuffer wordsBuffer =   new   
StringBuffer try { IKSegmenter ik = new   IKSegmenter new 
  StringReader srcString   true Lexeme lex = null while 
lex = ik . next = null { / / 
                    
      System . out . print lex . 
getLexemeText + wordsBuffer . append lex . getLexemeText . append 
} } catch Exception e { logger . error e 
. getMessage } return   wordsBuffer . toString } 5 
中科院 ICTCLASICTCLAS 是由 中科院计算所 历经 数 年 开发 的 分词 
工具 采用 C + + 编写 最新 版本 命名 为 
ICTCLAS2013 又名为 NLPIR 汉语分词 系统 主要 功能 包括 中文分词 词性 
标注 命名 实体 识别 用户 词典 功能 同时 支持 GBK 
编码 UTF8 编码 BIG5 编码 新增 微博 分词 新词 发现 
与 关键词 提取 可以 可视化 界面 操作 和 API 方式 
调用 6   FudanNLPFudanNLP 主要 是 为 中文 自然语言 处理 
而 开发 的 工具包 也 包含 为 实现 这些 任务 
的 机器学习 算法 和 数据集 FudanNLP 及其 包含 数据集 使用 
LGPL3 . 0 许可证 主要 功能 包括 信息检索 文本 分类 
新闻 聚 类 中文 处理 中文分词 词性 标注 实体 名 
识别 关键词 抽取 依存 句法分析 时间 短语 识别 结构化 学习 
在线 学习 层次 分类 聚 类 精确 推理 工具 采用 
Java 编写 提供 了 API 的 访问 调用 方式 下载/v 
安装包/n 后解/nr 压后/i 内容 如下 图 所示 在/p 使用/v 时将/nr 
fudannlp/w ./i jar/w 以及/c lib/w 中的/i jar/w 部署/n 于/p 项目/n 
中的/i lib/w 里面/f models 文件夹 中 存放 的 模型 文件 
主要 用于 分词 词性 标注 和 命名 实体 识别 以及 
分词 所需 的 词典 文件夹 example 中 主要 是 使用 
的 示例代码 可以 帮助 快速 入门 和 使用 java docs 
是 API 帮助 文档 src 中 存放 着 源码 PDF 
文档 中 有着 比较 详细 的 介绍 和 自然 语言 
处理 基础 知识 的 讲解 初始 运行 程序 时 初始化 
时间 有点 长 并且 加载 模型 时 占用 内存 较大 
在 进行 语法分析 时 感觉 分析 的 结果 不是 很 
准确 转载自 http / / www . cnblogs . com 
/ baiboy / p / nltk2 . html1 . 注册 
登录 系统 2 . 招聘 信息 中心 系统 用于 获得 
招聘 信息 查看 招聘 详情 查看 公司 信息 3 . 
简历 系统 用于 新建 与 完成 简历 编辑 工作 4 
. 面试 系统 采用 人工智能 自然语言 对话 开发 的 面试 
系统 包含 基于 上下文 提问 语义 理解 语音 文字 实时 
转写 基于 人脸 68 特征点 的 面相 分析 基于 人脸 
的 情绪 分析 基于 简历 的 画像 生成 人岗匹配 度 
语义 计算 算法 等 5 . 正常 招聘 流程 简历 
投递 参与 面试 等待 HR 处理 反馈 结果 浅谈 人工智能 
人工智能 是 计算机 科学 的 一个 分支 它 企图 了解 
智能 的 实质 并 生产 出 一种 新的 能以 人类 
智能 相似 的 方式 做出 反应 的 智能 机器 该 
领域 的 研究 包括 机器人 语言识别 图像识别 自然语言 处理 和 
专家 系统 等 为什么 要 学 人工智能 学习 范围 广泛 
内容 多 开发 平台 高 就业 机会 多 发展方向 多元化 
怎么 学习 人工智能 Linux 系统 C / C + + 
语言 python 语言 计算机 架构 图像处理 模式识别 人工智能 研究 内容 
人工智能 涉及 信息论 控制论 自动化 仿生学 生物学 心理学 数理逻辑 语言学 
医学 和 哲学 等 多门 学科 人工智能 学科 研究 的 
主要 内容 包括 知识 表示 自动 推理 和 搜索 方法 
机器 学习 和 知识 获取 知识处理 系统 自然语言 理解 计算机 
视觉 智能 机器人 自动 程序 设计 等 方面 整理 NLP 
Progress 上 的 东西 目录 EnglishCommon Sense 知识 推理 E 
v e n t 2 M i n d W 
A G W i n o g r a d 
Schema C h a l l e n g e 
C o n s t i t u e n 
c y parsingPenn T r e e b a n 
k E n g l i s h C o 
m m o n Sense 知识 推理 Common sense reasoning 
tasks are intended to require the model to go beyond 
pattern recognition . Instead the model should use common sense 
or world knowledge to make inferences . 常识推理 任务 旨在 
要求 模型 超越 模式识别 相反 知识 推理模型 应该 使用 常识 
或 世界 知识 来 做出 推论 E v e n 
t 2 M i n d E v e n 
t 2 M i n d is a crowdsourced corpus 
of 25 000 event phrases covering a diverse range of 
everyday events and situations . Given an event described in 
a short free form text a model should reason about 
the likely intents and reactions of the event s participants 
. Models are evaluated based on average cross entropy lower 
is better . Event2Mind 是 一个 包含 25 000个 活动 
短语 的 众包 语料库 涵盖 各 种 日常 事件 和 
情境 鉴于 在 简短 的 自由 格式 文本 中 描述 
的 事件 模型 应该 推断 事件 的 参与者 可能 的 
意图 和 反应 基于 平均 交叉 熵 评估 模型 越低 
越好 M o d e l D e v T 
e s t P a p e r / SourceCodeBiRNN 
100d Rashkin et al . 2018 4.254 . 22Event2Mind Commonsense 
Inference on Events Intents and ReactionsConvNet Rashkin et al . 
2018 4.444 . 40Event2Mind Commonsense Inference on Events Intents and 
R e a c t i o n s W 
A G i t u a t i o n 
s with Adversarial Generations SWAG is a dataset consisting of 
113k multiple choice questions about a rich spectrum of grounded 
situations . Situations with Adversarial Generations SWAG 是 一个 由 
113k 多项 选择 问题 组成 的 数据 集 这些 问题 
涉及 丰富 的 基础 情境 M o d e l 
D e v T e s t P a p 
e r / SourceCodeBERT Large Devlin et al . 2018 
86.686 . 3BERT Pre training of Deep Bidirectional Transformers for 
Language U n d e r s t a n 
d i n g B E R T Base Devlin 
et al . 2018 81.6 BERT Pre training of Deep 
Bidirectional Transformers for Language U n d e r s 
t a n d i n g E I M 
+ ELMo Zellers et al . 2018 59.159 . 2SWAG 
A Large Scale Adversarial Dataset for Grounded Commonsense InferenceESIM + 
GloVe Zellers et al . 2018 51.952 . 7SWAG A 
Large Scale Adversarial Dataset for Grounded Commonsense I n f 
e r e n c e W i n o 
g r a d Schema ChallengeThe Winograd Schema Challenge is 
a dataset for common sense reasoning . It employs Winograd 
Schema questions that require the resolution of anaphora the system 
must identify the antecedent of an ambiguous pronoun in a 
statement . Models are evaluated based on accuracy . Example 
The trophy doesn t fit in the suitcase because it 
is too big . What is too big Answer 0 
the trophy . Answer 1 the suitcaseWSC 是 常识 推理 
的 数据集 它 使用 了 需要 解决 回 指 的 
Winograd Schema 问题 系统 必须 识别 句子 中 的 模糊 
的 代词 模型 基于 准确性 进行 评估 例 奖杯 不 
适合 行李箱 因为 它 太大 了 什么 太大 了 回答 
0 奖杯 答案 1 行李箱 ModelScorePaper / SourceWord LM partial 
Trinh and Le 2018 62.6 A Simple Method for Commonsense 
ReasoningChar LM partial Trinh and Le 2018 57.9 A Simple 
Method for Commonsense ReasoningUSSM + Supervised DeepNet + KB Liu 
et al . 2017 52 . 8Combing Context and Commonsense 
Knowledge Through Neural Networks for Solving Winograd Schema P r 
o b l e m s C o n s 
t i t u e n c y parsing 句法 
解析 Consituency parsing aims to extract a constituency based parse 
tree from a sentence that represents its syntactic structure according 
to a phrase structure grammar . Example Sentence S | 
+ + + | | Noun N Verb Phrase VP 
| | John + + + | | Verb V 
Noun N | | sees BillRecent approaches convert the parse 
tree into a sequence following a depth first traversal in 
order to be able to apply sequence to sequence models 
to it . The linearized version of the above parse 
tree looks as follows S N VP V N . 
Penn TreebankThe Wall Street Journal section of the Penn Treebank 
is used for evaluating constituency parsers . Section 22 is 
used for development and Section 23 is used for evaluation 
. Models are evaluated based on F1 . Most of 
the below models incorporate external data or features . For 
a comparison of single models trained only on WSJ refer 
to Kitaev and Klein 2018 . ModelF1 scorePaper / SourceSelf 
attentive encoder + ELMo Kitaev and Klein 2018 95 . 
13Constituency Parsing with a Self Attentive EncoderModel combination Fried et 
al . 2017 94 . 66Improving Neural Parsing by Disentangling 
Model Combination and Reranking EffectsIn order Liu and Zhang 2017 
94 . 2In Order Transition based Constituent ParsingSemi supervised LSTM 
LM Choe and Charniak 2016 93 . 8Parsing as Language 
ModelingStack only RNNG Kuncoro et al . 2017 93 . 
6What Do Recurrent Neural Network Grammars Learn About Syntax RNN 
Grammar Dyer et al . 2016 ﻿ 93 . 3Recurrent 
Neural Network G r a m m a r s 
T r a n s f o r m e 
r Vaswani et al . 2017 92 . 7Attention Is 
All You NeedSemi supervised LSTM Vinyals et al . 2015 
92 . 1Grammar as a Foreign LanguageSelf trained parser McClosky 
et al . 2006 92 . 1Effective Self Training for 
ParsingDomain a d a p t a t i o 
n e n t i m e n t analysisThe 
Multi Domain Sentiment Dataset is a common evaluation dataset for 
domain adaptation for sentiment analysis . It contains product reviews 
from Amazon . com from different product categories which are 
treated as distinct domains . Reviews contain star ratings 1 
to 5 stars that are generally converted into binary labels 
. Models are typically evaluated on a target domain that 
is different from the source domain they were trained on 
while only having access to unlabeled examples of the target 
domain unsupervised domain adaptation . The evaluation metric is accuracy 
and scores are averaged across each domain . 多 域 
情感 数据集 是 用于 情绪 分析 的 域 适应 的 
通用 评估 数据集 它 包含 来自 Amazon . com 的 
不同 产品 类别 的 产品 评论 这些 评论 被 视为 
不同 的 域 评论 包含 星级 1 至 5 星 
通常 转换 为 二进制 标签 模型 通常 在 目标 域 
上 进行 评估 该 目标 域 与 它们 所 训练 
的 源 域 不同 而 只能 访问 目标 域 的 
未 标记 示例 无 监督 域 适应 评估 指标 是 
准确性 并且 每个 域 的 平均 得分 M o d 
e l D V D B o o k s 
E l e c t r o n i c 
s K i t c h e n A v 
e r a g e P a p e r 
/ SourceMulti task tri training Ruder and Plank 2018 78 
. 1474.8681 . 4582.1479 . 15Strong Baselines for Neural Semi 
supervised Learning under Domain ShiftAsymmetric tri training Saito et al 
. 2017 76 . 1772.9780 . 4783.9778 . 39Asymmetric Tri 
training for Unsupervised Domain AdaptationVFAE Louizos et al . 2015 
76 . 5773.4080 . 5382.9378 . 36The Variational Fair AutoencoderDANN 
Ganin et al . 2016 75 . 4071.4377 . 6780.5376 
. 26Domain Adversarial Training of Neural NetworksMulti task learningMulti task 
learning aims to learn multiple different tasks simultaneously while maximizing 
performance on one or all of the tasks . DecaNLPThe 
Natural Language Decathlon decaNLP is a benchmark for studying general 
NLP models that can perform a variety of complex natural 
language tasks . It evaluates performance on ten disparate natural 
language tasks . Results can be seen on the public 
leaderboard . GLUEThe General Language Understanding Evaluation benchmark GLUE is 
a tool for evaluating and analyzing the performance of models 
across a diverse range of existing natural language understanding tasks 
. Models are evaluated based on their average accuracy across 
all tasks . The state of the art results can 
be seen on the public GLUE leaderboard . 作者 Melanie 
Tosik 翻译 闵黎/nr 校对/v 丁楠 雅 Melanie Tosik 目前 就职 
于 旅游 搜索 公司 WayBlazer 她 的 工作 内容 是 
通过 自然 语言 请求 来 生产 个性化 旅游 推荐 路线 
回顾 她 的 学习 历程 她 为 期望 入门 自然语言 
处理 的 初学者 列出 了 一份 学习 资源 清单 displaCy 
网站 上 的 可视化 依赖 解析 树 https / / 
demos . explosion . ai / displacy / text = 
Great % 2C % 20this % 20is % 20just % 
20what % 20I % 20needed & model = en & 
cpu = 1 & cph = 0 记得 我 曾经 
读到 过 这样 一段 话 如果 你 觉得 有 必要 
回答 两次 同样 的 问题 那 就把 答案 发到 博客 
上 这 可能 是 一个 好 主意 根据 这 一 
原则 也 为了 节省 回答 问题 的 时间 我 在 
这里 给出 该 问题 的 标准 问法 我 的 背景 
是 研究 * * 科学 我 对 学习 NLP 很 
有兴趣 应该 从哪/nr 说起 呢 在 您 一头 扎 进去 
阅读 本文 之前 请注意 下面 列表 只是 提供 了 非常 
通用 的 入门 清单 有 可能 不 完整   为了 
帮助 读者 更好 地 阅读 我 在 括号 内 添加 
了 简短 的 描述 并对 难度 做了 估计 最好 具备 
基本 的 编程 技能 例如 Python 在线 课程 •   
Dan Jurafsky 和 /nr Chris Manning 自然语言 处理 非常 棒 的 
视频 介绍 系列 https / / www . youtube . 
com / watch v = nfoudtpBV68 & list = P 
L 6 3 9 7 E 4 B 2 6 
D 0 0 A 2 6 9 •   斯坦福 
CS224d 自然语言 处理 的 深度 学习 更 高级 的 机器学习 
算法 深度 学习 和 NLP 的 神经 网络 架构 http 
/ / cs224d . stanford . edu / syllabus . 
html •   Coursera 自然语言 处理 简介 由 密西根 大学 
提供 的 NLP 课程 https / / www . coursera 
. org / learn / natural language processing 图书馆 和 
开放 资源 •   spaCy 网站 博客 Python 新兴 的 
开放 源码库 并 自带 炫 酷 的 用法 示例 API 
文档 和 演示 应用程序 网站 网址 https / / spacy 
. io / 博客 网址 https / / explosion . 
ai / blog / 演示 应用 网址   https / 
/ spacy . io / docs / usage / showcase 
•   自然语言 工具包 NLTK 网站 图书 Python NLP 实用 
编程 介绍 主要 用于 教学 目的 网站 网址 http / 
/ www . nltk . org 图书 网址   http 
/ / www . nltk . org / book / 
•   斯坦福 CoreNLP 网站 由 Java 开发 的 高质量 
的 自然 语言 分析 工具包 网站 网址   https / 
/ stanfordnlp . github . io / CoreNLP / 活跃 
的 博客 •   自然语言 处理 博客 HalDaum é 博客 
网址 https / / nlpers . blogspot . com / 
•   Google 研究 博客 博客 网址 https / / 
research . googleblog . com / •   语言 日志 
博客 Mark Liberman 博客 网址 http / / languagelog . 
ldc . upenn . edu / nll / 书籍 • 
  言语 和 语言 处理 Daniel Jurafsky 和 James H 
. Martin 经典 的 NLP 教科书 涵盖 了 所有 NLP 
的 基础 知识 第 3版 即将 出版 https / / 
web . stanford . edu / ~ jurafsky / slp3 
/ •   统计 自然语言 处理 的 基础 Chris Manning 
和 HinrichSch ü tze 更 高级 的 统计 NLP 方法 
https / / nlp . stanford . edu / fsnlp 
/ •   信息检索 简介 Chris Manning Prabhakar Raghavan 和 
HinrichSch ü tze 关于 排名 / 搜索 的 优秀 参考书 
https / / nlp . stanford . edu / IR 
book / •   自然语言 处理 中 的 神经 网络 
方法 Yoav Goldberg 深入 介绍 NLP 的 NN 方法 和相/nr 
对应/vn 的/uj 入门/ns 书籍/n https / / www . amazon 
. com / Network Methods Natural Language Processing / dp 
/ 1627052984 入门 书籍   http / / u . 
cs . biu . ac . il / ~ yogo 
/ nnlp . pdf 其它 杂项 •   如何 在 
TensorFlow 中 构建 word2vec 模型 学习指南 https / / www 
. tensorflow . org / versions / master / tutorials 
/ word2vec / index . html •   NLP 深度 
学习 的 资源 按 主题 分类 的 关于 深度 学习 
的 顶尖 资源 的 概述 https / / github . 
com / andrewt3000 / dl4nlp •   最后 一句话 计算 
语言学 和 深度 学习 论 自然语言 处理 的 重要性 Chris 
Manning 文章 http / / mitp . nautil . us 
/ article / 170 / last words computational linguistics and 
deep learning •   对 分布式 表征 的 自然 语言 
的 理解 Kyunghyun Cho 关于 NLU 的 ML / NN 
方法 的 独立 讲义 https / / github . com 
/ nyu dl / NLP _ DL _ Lecture _ 
Note / blob / master / lecture _ note . 
pdf •   带 泪水 的 贝叶斯 推论 Kevin Knight 
教程 工作簿 http / / www . isi . edu 
/ natural language / people / bayes with tears . 
pdf •   国际 计算 语言学 协会 ACL 期刊 选集 
http / / aclanthology . info / •   果壳 
问答 网站 Quora 我 是 如何 学习 自然语言 处理 的 
https / / www . quora . com / How 
do I learn Natural Language ProcessingDIY 项目 和 数据集 资料 
来源 http / / gunshowcomic . com / •   
Nicolas Iderhoff 已经 创建 了 一份 公开 的 详尽 的 
NLP 数据集 的 列表 除了 这些 这里 还有 一些 项目 
可以 推荐 给 那些 想 要 亲自 动手 实践 的 
NLP 新手 们 数据集 https / / github . com 
/ niderhoff / nlp datasets •   基于 隐 马尔可夫 
模型 HMM 实现 词性 标注 POS tagging . https / 
/ en . wikipedia . org / wiki / Part 
of speech _ tagginghttps / / en . wikipedia . 
org / wiki / Hidden _ Markov _ model • 
  使用 CYK 算法 执行 上下文 无关 的 语法 解析 
https / / en . wikipedia . org / wiki 
/ CYK _ algorithmhttps / / en . wikipedia . 
org / wiki / Context free _ grammar •   
在 文本 集合 中 计算 给定 两个 单词 之间 的 
语义 相似 度 例如 点 互信息 PMI Pointwise Mutual Information 
https / / en . wikipedia . org / wiki 
/ Semantic _ similarityhttps / / en . wikipedia . 
org / wiki / Pointwise _ mutual _ information • 
  使用 朴素 贝叶斯 分类器 来 过滤 垃圾邮件 https / 
/ en . wikipedia . org / wiki / Naive 
_ Bayes _ classifierhttps / / en . wikipedia . 
org / wiki / Naive _ Bayes _ spam _ 
filtering •   根据 单词 之间 的 编辑 距离 执行 
拼 写检查 https / / en . wikipedia . org 
/ wiki / Spell _ checkerhttps / / en . 
wikipedia . org / wiki / Edit _ distance • 
  实现 一个 马尔科夫 链 文本 生成器 https / / 
en . wikipedia . org / wiki / Markov _ 
chain •   使用 LDA 实现 主题 模型 https / 
/ en . wikipedia . org / wiki / Topic 
_ modelhttps / / en . wikipedia . org / 
wiki / Latent _ Dirichlet _ allocation •   使用 
word2vec 从 大型 文本 语料库 例如 维基百科 生成 单词 嵌入 
https / / code . google . com / archive 
/ p / word2vec / https / / en . 
wikipedia . org / wiki / Wikipedia Database _ downloadNLP 
在 社交 媒体 上 •   Twitter # nlproc NLPers 
上 的 文章 列表 由 Jason Baldrige 提供 https / 
/ twitter . com / hashtag / nlprochttps / / 
twitter . com / jasonbaldridge / lists / nlpers • 
  Reddit   社交 新闻 站点 / r / L 
a n g u a g e T e c 
h n o l o g y h t t 
p s / / www . reddit . com / 
r / L a n g u a g e 
T e c h n o l o g y 
•   Medium 发布 平台 Nlphttps / / medium . 
com / tag / nlp 原文 链接 https / / 
medium . com / towards data science / how to 
get started in nlp 6a62aa4eaeff 记者 | CSDN 苏靖芝/nr 7 
月 22 23 日 由 中国 人工智能 学会 阿里 巴巴 
集团 & 蚂蚁 金服/nr 主办 CSDN 中国科学院自动化研究所 承办 的 第三 
届 中国 人工智能 大会 CCAI 2017 在 杭州 国际会议中心 盛大 
开幕 本次 大会 的 第一 场 分 论坛 讨论 是 
关于 语言 智能 领域 的 八大 问题 讨论 期间 哈尔滨工业大学 
刘挺/nr 教授 对 自然 语言 处理 的 发展 趋势 做了 
一次 精彩 的 归纳 他 把 这里 的 趋势 分成 
了 十 个 方面 趋势 1 语义 表示 从/p 符号/n 
表示/v 到/v 分布/v 表示/v 自然语言/l 处理/v 一直/d 以来/f 都是/nr 比较/d 
抽象/v 的/uj 都是 直接 用 词汇 和 符号 来 表达 
概念 但是 使用 符号 存在 一个 问题 比如 两个 词 
它们 的 词 性相近 但 词形 不匹配 计算机 内部 就 
会 认为 它们 是 两个 词 举个 例子 荷兰 和 
苏格兰 这 两个 国家 名 如果 我们 在 一个 语义 
的 空间 里 用 词汇 与 词汇 组合 的 方法 
把 它 表示 为 连续 低维 稠密 的 向量 的话 
就 可以 计算 不同 层次 的 语言 单元 之间 的 
相似 度 这种方法 同时 也 可以 被 神经 网络 直接 
使用 是 这个 领域 的 一个 重要 的 变化 从 
词汇 间 的 组合 到 短语 句子 一直 到 篇章 
现在 有 很多 人 在做 这个 事 这 和 以前 
的 思路 是 完全 不 一样 的 有了/nr 这种 方法 
之后 再用 深度 学习 就 带来 了 一个 很大 的 
转变 原来 我们 认为 自然语言 处理 要 分成 几个 层次 
但是 就 句法分析 来说 它 是 人为 定义 的 层次 
那它 是不是 一定 必要 的 这里 应该 打 一个 问号 
实际 工作 中 我们 面临 着 一个 课题 信息 抽取 
我 之前 和 一个 单位 合作 初衷 是 我 做 
句法分析 然后 他们 在 我 的 基础 上 做 信息 
抽取 相互配合 后来 他们 发表 了 一篇 论文 与 初衷 
是 相悖 的 它 证明 了 没有 句法分析 也 可以 
直接 做 端 到 端的 直接 的 实体 关系 抽取 
这 很 震撼 不是 说 现在 句法分析 没用 了 而是 
我们 认为 句法分析 是 人为 定义 的 层次 在 端 
到 端 的 数据 量 非常 充分 可以 直接 进行 
信息 抽取 的 时候 那么 不用 句法分析 也 能 达到 
类似 的 效果 当 端 到 端 的 数据 不 
充分 时 才 需要 人为 划分 层次 趋势 2 学习 
模式 从 浅层 学习 到 深度 学习 浅层 到 深层 
的 学习 模式 中 浅层 是 分步骤 走 可能 每一步 
都 用了 深度 学习 的 方法 实际上 各个 步骤 是 
串接 起来 的 直接 的 深度 学习 是 一步到位 的 
端 到 端 在 这个 过程 中 我们 确实 可以 
看到 一些 人 为 贡献 的 知识 包括 该 分 
几层 每层 的 表示 形式 一些 规则 等 但 我们 
所谓 的 知识 在 深度 学习 里 所占 的 比重 
确实 减小 了 主要 体现 在 对 深度 学习 网络结构 
的 调整 趋势 3 NLP 平台 化 从 封闭 走向 
开放 以前 我们 搞 研究 的 都 不是 很 愿意 
分享 自己 的 成果 像 程序 或是 数据 现在 这些 
资料 彻底 开放 了 无论 是 学校 还是 大 企业 
都 更多 地 提供 平台 NLP 领域 提供 的 开放 
平台 越来越 多 它 的 门槛 也 越来越 降低 语音/n 
和/c 语言/n 其实/d 有/v 很大/a 的/uj 差别/d 我 认识 的 
好几 位 国内外 的 进入 NLP 的 学者 他们 发现 
NLP 很复杂 因为 像 语音 识别 和 语音 合成 等 
只有 有限 的 问题 而且 这些 问题 定义 非常 清晰 
但到 了 自然 语言 要 处理 的 问题 变得 纷繁复杂 
尤其 是 NLP 和 其他 的 领域 还 会 有所 
结合 所以 问题 非常 琐碎 趋势 4 语言 知识 从 
人工 构建 到 自动 构建 AlphaGo 告诉 我们 没有 围棋 
高手 介入 他 的 开发 过程 到 AlphaGo 最后 的 
版本 它 已经 不怎么 需要 看 棋谱 了 所以/c AlphaGo/w 
在/p 学习/v 和/c 使用/v 过程/n 中/f 都/d 有可能/i 会/v 超出/v 
人/n 的/uj 想像/v 因为 它 并 不是 简单 地 跟人 
学习 美国 有 一家 文艺复兴 公司 它 做 金融 领域 
的 预测 但是 这个 公司 不 招 金融 领域 的 
人 只是 招 计算机 物理 数学 领域 的 人 这就 
给 了 我们 一个 启发 计算机 不是 跟人 的 顶级 
高手 学 而是 用 自己 已有 的 算法 去 直接 
解决 问题 但是 在 自然 语言 处理 领域 还是 要 
有 大量 的 显性 知识 的 但是 构造 知识 的 
方式 也 在 产生 变化 比如 现在 我们 开始 用 
自动 的 方法 自动 地 去 发现 词汇 与 词汇 
之间 的 关系 像 毛细血管 一样 渗透 到 各个 方面 
趋势 5 对话 机器人 从 通用 到 场景 化 最近 
出现 了 各种 图灵测试 的 翻版 就是 做 知识 抢答 
赛 来 验证 人工智能 从 产学研 应用 上 来讲 就是 
对话 机器人 非常/d 有/v 趣味性/n 和/c 实用/v 价值/n 这块 的 
趋势 在 哪里 我们 知道 从 Siri 刚出来 国内 就 
开始 做 语音 助手 了 后来 语音 助手 很快 下了 
马 因为 它 可以 听得到 但是 听 不懂 导致 后面 
的 服务 跟不上 后来 国内 把 难度 降低 成了 聊天 
你 不是 调戏 Siri 吗 我 就做 小冰 就 跟 
你 聊 但是 难度 降低 了 实用性 却 跟 不上来 
所以 在 用户 的 留存率 上 还是 要 打个 问号 
现在 更多 的 做法 和 场景 结合 降低 难度 然后 
做 任务 执行 即 希望 做 特定 场景 时的/nr 有用 
的 人机对话 在 做人 机 对话 的 过程 中 大家 
热情 一轮 比 一轮 高涨 但是 随后 大家 发现 很多 
问题 是 由于 自然 语言 的 理解 没有 到位 才 
难以 产生 真正 的 突破 趋势 6 文本 理解 与 
推理 从/p 浅层/n 分析/vn 向/p 深度/ns 理解/v 迈进/v Google/w 等/u 
都/d 已经/d 推出/v 了/ul 这样/r 的/uj 测试机/nz 以 阅读 理解 
作为 一个 深入 探索 自然语言 理解 的 平台 就是说 给 
计算机 一篇 文章 让 它 去 理解 然 后人 问 
计算机 各种 问题 看 计算机 是否 能 回答 这样 做 
是 很有 难度 的 因为 答案 就在 这 文章 里面 
人 会很 刁 钻地 问 计算机 所以 说 阅读 理解 
是 现在 竞争 的 一个 很 重要 的 点 趋势 
7 文本 情感 分析 从 事实性 文本 到 情感 文本 
多年 以前 很多 人 都 在做 新闻 领域 的 事实 
性 文本 而 如今 搞 情感 文本 分析 的 似乎 
更 受 群众 欢迎 这/r 一块/m 这在/i 商业/n 和/c 政府/n 
舆情/n 上/f 也都/i 有/v 很好/i 地/uv 应用/v 趋势 8 社会 
媒体 处理 从 传统 媒体 到 社交 媒体 相应 的 
在 社会 媒体 处理 上 从 传统 媒体 到 社交 
媒体 的 过渡 情感 的 影响 是 一方面 大家 还 
会用 社交 媒体 做 电影 票房 的 预测 做股票 的 
预测 等等 但是 从 长远 的 角度 看 社会 人文 
等 的 学科 与 计算机 学科 的 结合 是 历史性 
的 比如 在 文学 历史学 等 学科 中 有 相当 
一 部分 新锐 学者 对本 门 学科 的 计算机 的 
大 数据 非常 关心 这 两者 在 碰撞 未来 的 
前景 是 无限 的 而 自然 语言 处理 是 其中 
重要 的 基础性 的 技术 趋势 9 文本 生成 从 
规范 文本 到 自由 文本 文本 生成 这两年 很 火 
从 生成 古诗词 到 生成 新闻报道 到 再到 写作文 这 
方面 的 研究 价值 是 很大 的 它 的 趋势 
是从 生成 规范性 的 文本 到 生成 自由 文本 比如 
我们 可以 从 数据库 里面 生成 一个 可以 模板 化 
的 体育 报道 这个 模板 是 很 规范 的 然后 
我们 可以 再 向 自由 文本 过渡 比如 写作文 趋势 
10 NLP + 行业 与 领域 深度 结合 为 行业 
创造 价值 最后 是 谈与/nr 企业 的 合作 现在 像 
银行 电器 医药 司法 教育 金融 等 的 各个 领域 
对 NLP 的 需求 都 非常 多 我 预测 NLP 
首先是 会在 信息 准备 的 充分 的 并且 服务 方式 
本身 就是 知识 和 信息 的 领域 产生 突破 还 
比如 司法 领域 它 的 服务 本身 也 有 信息 
它 就会 首先 使用 NLP NLP 最 主要 将 会用 
在 以下 四 个 领域 医疗 金融 教育 和 司法 
CSDN AI 热衷 分享 欢迎 扫 码 关注 以下内容 转载自 
百度 百科 如果 没 时间 仔细 看 可 只看 加粗 
部分 即可 自然语言 处理 是 计算机 科学 领域 与 人工智能 
领域 中 的 一个 重要 方向 它/r 研究/vn 能/v 实现/v 
人/n 与/p 计算机/n 之间/f 用/p 自然/d 语言/n 进行/v 有效/a 通信/l 
的/uj 各种/r 理论/n 和/c 方法/n 自然语言 处理 是 一门 融 
语言学 计算机科学 数学 于 一体 的 科学 因此 这一 领域 
的 研究 将 涉及 自然语言 即 人们 日常 使用 的 
语言 所以 它 与 语言学 的 研究 有着 密切 的 
联系 但又 有 重要 的 区别 自然语言 处理 并 不是 
一般 地 研究 自然语言 而在于 研制 能 有效 地 实现 
自然 语言 通信 的 计算机 系统 特别 是 其中 的 
软件 系统 因而 它 是 计算机 科学 的 一部分 自然语言 
处理 NLP 是 计算机 科学 人工智能 语言学 关注 计算机 和 
人类 自然 语言 之间 的 相互 作用 的 领域 1 
详细 介绍 2 发展 历史 3 相关内容 4 相关 技术 
5 概述 ▪ 基础理论 ▪ 语言 资源 ▪ 关键技术 ▪ 
应用 系统 6 争论 7 处理 数据 8 处理 工具 
▪ OpenNLP ▪ FudanNLP ▪ 语言 技术 平台 LTP 9 
自然语言 处理 技术 难点 ▪ 单词 的 边界 界定 ▪ 
词义 的 消 歧 ▪ 句法 的 模糊性 ▪ 有 
瑕疵 的 或不 规范 的 输入 ▪ 语言 行为 与 
计划 1 详细 介绍 语言 是 人类 区别 其他 动物 
的 本质 特性 在所 有生 物中 只有 人类 才 具有 
语言 能力 人类 的 多种 智能 都与 语言 有着 密切 
的 关系 人类 的 逻辑思维 以 语言 为 形式 人类 
的 绝大部分 知识 也 是以 语言 文字 的 形式 记载 
和 流传 下来 的 因而 它 也是 人工智能 的 一个 
重要 甚至 核心 部分 用 自然 语言 与 计算机 进行通信 
这 是 人们 长期以来 所 追求 的 因为 它 既有 
明显 的 实际 意义 同时 也 有 重要 的 理论 
意义 人们 可以 用 自己 最 习惯 的 语言 来 
使用 计算机 而/c 无需/v 再/d 花/v 大量/n 的/uj 时间/n 和/c 
精力/n 去/v 学习/v 不/d 很/zg 自然/d 和/c 习惯/n 的/uj 各种/r 
计算机/n 语言/n 人们 也 可 通过 它 进一步 了解 人类 
的 语言 能力 和 智能 的 机制 实现 人机 间 
自然语言 通信 意味着 要使 计算机 既 能理解 自然语言 文本 的 
意义 也能 以 自然 语言 文本 来 表达 给定 的 
意图 思想 等 前者 称为 自然语言 理解 后者 称为 自然语言 
生成 因此 自然语言 处理 大体 包括 了 自然 语言 理解 
和 自然 语言 生成 两个 部分 历史 上 对 自然 
语言 理解 研究 得 较多 而对 自然语言 生成 研究 得 
较少 但 这种 状况 已 有所 改变 无论 实现 自然 
语言 理解 还是 自然 语言 生成 都 远不如 人们 原来 
想象 的 那么 简单 而 是 十分 困难 的 从/p 
现有/b 的/uj 理论/n 和/c 技术/n 现状/n 看/v 通用 的 高 
质量 的 自然 语言 处理 系统 仍然 是 较 长期 
的 努力 目标 但是 针对 一定 应用 具有 相当 自然语言 
处理 能力 的 实用 系统 已经 出现 有些 已 商品化 
甚至 开始 产业化 典型 的 例子 有 多语种 数据库 和 
专家 系统 的 自然 语言 接口 各种 机器 翻译 系统 
全文 信息 检索系统 自动 文摘 系统 等 自然语言 处理 即 
实现 人机 间 自然语言 通信 或 实现 自然 语言 理解 
和 自然 语言 生成 是 十分 困难 的 造成 困难 
的 根本 原因 是 自然 语言 文本 和 对话 的 
各个 层次 上 广泛 存在 的 各种各样 的 歧义 性 
或 多义性 ambiguity 一个 中文 文本 从 形式 上看 是由 
汉字 包括 标点符号 等 组成 的 一个 字符串 由 字 
可 组成 词 由 词 可 组成 词组 由 词组 
可 组成 句子 进而 由 一些 句子 组 成段 节 
章 篇 无论 在 上述 的 各种 层次 字 符 
词 词组 句子 段 还是/c 在/p 下一/i 层次/m 向/p 上一/i 
层次/m 转变/v 中/f 都/d 存在/v 着/uz 歧义/n 和/c 多义/n 现象/n 
即 形式 上 一样 的 一段 字符串 在 不同 的 
场景 或 不同 的 语境 下 可以 理解 成 不同 
的 词串 词组 串 等 并 有 不同 的 意义 
一般 情况 下 它们/r 中/f 的/uj 大多数/m 都是/nr 可以/c 根据/p 
相应/v 的/uj 语境/n 和/c 场景/n 的/uj 规定/n 而/c 得到/v 解决/v 
的/uj 也 就是说 从总体上 说 并不 存在 歧义 这 也 
就是 我们 平时 并不 感到 自然语言 歧义 和 能用 自然 
语言 进行 正确 交流 的 原因 但是 一 方面 我们 
也 看到 为了 消解 歧义 是 需要 极 其 大量 
的 知识 和 进行 推理 的 如何 将 这些 知识 
较 完整 地 加以 收集 和 整理 出来 又 如何 
找到 合适 的 形式 将 它们 存入 计算机系统 中去 以及 
如何 有效 地 利用 它们 来 消除歧义 都是 工作量 极大 
且 十分 困难 的 工作 这 不是 少数人 短时期 内 
可以 完成 的 还 有待 长期 的 系统 的 工作 
以上 说 的 是 一个 中文 文本 或 一个 汉字 
含 标点符号 等 串 可能 有 多个 含义 它 是 
自然 语言 理解 中 的 主要 困难 和 障碍 反过来 
一个 相同 或 相近 的 意义 同样 可以 用 多个 
中文 文本 或 多个 汉 字串 来 表示 因此 自然 
语言 的 形式 字符串 与其 意义 之间 是 一种 多对 
多 的 关系 其实 这 也 正是 自然 语言 的 
魅力 所在 但从 计算机 处理 的 角度 看 我们 必须 
消除歧义 而且 有人 认为 它 正是 自然语言 理解 中 的 
中心 问题 即 要把 带有 潜在 歧义 的 自然 语言 
输入 转换成 某种 无 歧义 的 计算机 内部 表示 歧义 
现象 的 广泛 存在 使得 消除 它们 需要 大量 的 
知识 和 推理 这就 给 基于 语言学 的 方法 基于 
知识 的 方法 带来 了 巨大 的 困难 因而 以 
这些 方法 为 主流 的 自然 语言 处理 研究 几十年 
来 一方面 在 理论 和 方法 方面 取得 了 很多 
成就 但在 能 处理 大规模 真实 文本 的 系统 研制 
方面 成绩 并 不显著 研制 的 一些 系统 大多数 是 
小 规模 的 研究性 的 演示 系统 目前 存在 的 
问题 有 两个 方面 一方面 迄今为止 的 语法 都 限于 
分析 一个 孤立 的 句子 上下文/l 关系/n 和/c 谈话/vn 环境/n 
对/p 本句/i 的/uj 约束/vn 和/c 影响/vn 还/d 缺乏/v 系统/n 的/uj 
研究/vn 因此 分析 歧义 词语 省略 代词 所指 同一 句话 
在 不同 场合 或由 不同 的 人 说 出来 所 
具有 的 不同 含义 等 问题 尚无 明确 规律 可循 
需要 加强 语用学 的 研究 才能 逐步 解决 另一方面 人 
理解 一个 句子 不是 单凭 语法 还 运用 了 大量 
的 有关 知识 包括 生活 知识 和 专门 知识 这些 
知识 无法 全部 贮存 在 计算机 里 因此 一个 书面 
理解 系统 只能 建立 在 有限 的 词汇 句型 和 
特定 的 主题 范围 内 计算机 的 贮存量 和 运转 
速度 大大 提高 之后 才 有可能 适当 扩大 范围 . 
以上 存在 的 问题 成为 自然 语言 理解 在 机器 
翻译 应用 中 的 主要 难题 这 也 就是 当今 
机器翻译 系统 的 译文 质量 离 理想 目标 仍 相差 
甚远 的 原因 之一 而 译文 质量 是 机译 系统 
成败 的 关键 中国 数学家 语言学家 周海中 教授 曾在/nr 经典 
论文 机器翻译 五十年 中 指出 要 提高 机译 的 质量 
首先 要 解决 的 是 语言 本身 问题 而 不是 
程序设计 问题 单靠 若干 程序 来做 机译 系统 肯定 是 
无法 提高 机译 质量 的 另外 在 人类 尚未 明了 
大脑 是 如何 进行 语言 的 模糊 识别 和 逻辑 
判断 的 情况 下 机译 要想 达到 信 达 雅 
的 程度 是 不 可能 的 2 发展 历史 最早 
的 自然 语言 理解 方面 的 研究 工作 是 机器 
翻译 1949年 美国人 威 弗 首先 提出 了 机器 翻译 
设计方案 20 世纪 60 年代 国外 对 机器 翻译 曾有 
大 规模 的 研究 工作 耗费 了 巨额 费用 但 
人们 当时 显然是 低估 了 自然 语言 的 复杂性 语言 
处理 的 理论 和 技术 均 不成 热 所以 进展 
不大 主要 的 做法 是 存储 两种 语言 的 单词 
短语 对应 译法 的 大辞典 翻译 时一/nr 一 对应 技术 
上 只是 调整 语言 的 同条 顺序 但 日常 生活 
中 语言 的 翻译 远 不是 如此 简单 很多 时候 
还要 参考 某 句话 前后 的 意思 大约 90 年代 
开始 自然语言 处理 领域 发生 了 巨大 的 变化 这种 
变化 的 两个 明显 的 特征 是 1 对系统 输入 
要求 研制 的 自然 语言 处理 系统 能 处理 大规模 
的 真实 文本 而 不是 如 以前 的 研究 性 
系统 那样 只能 处理 很少 的 词条 和 典型 句子 
只有 这样 研制 的 系统 才有 真正 的 实用 价值 
2 对 系统 的 输出 鉴于 真实 地 理解 自然 
语言 是 十分 困难 的 对 系统 并 不 要求 
能对/nr 自然 语言 文本 进行 深层 的 理解 但要 能 
从中 抽取 有用 的 信息 例如 对 自然 语言 文本 
进行 自动 地 提取 索引 词 过滤 检索 自动 提取 
重要 信息 进行 自动 摘要 等等 同时 由于 强调 了 
大 规模 强调 了 真实 文本 下面 两 方面 的 
基础性 工作 也 得到 了 重视 和 加强 1 大 
规模 真实 语料库 的 研制 大 规模 的 经过 不同 
深度 加工 的 真实 文本 的 语料库 是 研究 自然语言 
统计 性质 的 基础 没有 它们 统计 方法 只能 是 
无源之水 2 大 规模 信息 丰富 的 词典 的 编制 
工作 规模 为 几万 十几万 甚至 几十 万词/nr 含有 丰富 
的 信息 如 包含 词 的 搭配 信息 的 计算机 
可用 词典 对 自然 语言 处理 的 重要性 是 很 
明显 的 3 相关内容 自然语言 处理 NLP 是 计算机 科学 
人工智能 语言学 关注 计算机 和 人类 自然 语言 之间 的 
相互 作用 的 领域 因此 自然语言 处理 是 与 人机交互 
的 领域 有关 的 在 自然 语言 处理 面临 很多 
挑战 包括 自然 语言 理解 因此 自然语言 处理 涉及 人 
机 交互 的 面积 在 NLP 诸多 挑战 涉及 自然语言 
理解 即 计算机 源于 人为 或 自然 语言 输入 的 
意思 和 其他 涉及 到 自然 语言 生成 现代 NLP 
算法 是 基于 机器学习 特别 是 统计 机器学习 机器学习 范式 
是 不同于 一般 之前 的 尝试 语言 处理 语言 处理 
任务 的 实现 通常 涉及 直接 用手 的 大套 规则 
编码 许多 不 同类 的 机器学习 算法 已 应用于 自然语言 
处理 任务 这些 算法 的 输入 是 一大 组 从 
输入 数据 生成 的 特征 一些 最早 使用 的 算法 
如 决策树 产 生硬 的 if then 规则 类似于 手写 
的 规则 是 再 普通 的 系统 体系 然而 越来越 
多 的 研究 集中于 统计模型 这 使得 基于 附加 实数值 
的 权重 每个 输入 要素 柔软 概率 的 决策 此类 
模型 具有 能够 表达 许多 不同 的 可能 的 答案 
而 不是 只有 一个 相对 的 确定性 产生 更 可靠 
的 结果 时 这种 模型 被 包括 作为 较大 系统 
的 一个 组成部分 的 优点 自然语言 处理 研究 逐渐 从 
词汇 语义 成分 的 语义 转移 进一步 的 叙事 的 
理解 然而 人类 水平 的 自然 语言 处理 是 一个 
人工智能 完全问题 它/r 是/v 相当/d 于/p 解决/v 中央/n 的/uj 人工智能/n 
问题/n 使/v 计算机/n 和人/nr 一样/r 聪明/a 或 强大 的 AI 
自然语言 处理 的 未来 一般 也 因此 密切 结合 人工智能 
发展 1 4 相关 技术 略 5 概述 基础理论 自动机 
形式逻辑 统计 机器学习 汉语 语言学 形式 语法 理论 语言 资源 
语料库 词典 关键 技术 汉字编码 词 法分析 句法分析 语义分析 文本 
生成 语音识别 应用 系统 文本 分类 和聚类/nr 信息检索 和 过滤 
信息 抽取 问答 系统 拼音 汉字 转换 系统 机器翻译 新 
信息 检测 6 争论 自然语言 处理 的 基础 是 各类 
自然语言 处理 数据集 如 tc corpus train 语料库 训练 集 
面向 文本 分类 研究 的 中英文 新闻 分类 语料 以 
IG 卡方 等 特征词 选择 方法 生成 的 多维度 ARFF 
格式 中文 VSM 模型 万篇 随机 抽取 论文 中文 DBLP 
资源 用于 非 监督 中文分词 算法 的 中文分词 词库 UCI 
评价 排序 数据 带有 初始化 说明 的 情感 分析 数据集 
等 7 处理 数据 8 处理 工具 ▪ OpenNLP ▪ 
FudanNLP ▪ 语言 技术 平台 LTP 9 自然语言 处理 技术 
难点 ▪ 单词 的 边界 界定 ▪ 词义 的 消 
歧 ▪ 句法 的 模糊性 ▪ 有 瑕疵 的 或不 
规范 的 输入 ▪ 语言 行为 与 计划 单词 的 
边界 界 定在 口语 中 词 与 词 之间 通常 
是 连贯 的 而 界定 字词 边界 通常 使用 的 
办法 是 取用 能让 给定 的 上下文 最为 通顺 且 
在 文法上 无误 的 一种 最佳 组合 在 书写 上 
汉语 也 没有 词 与 词 之间 的 边界 词义 
的 消 歧 许 多字词 不单 只有 一个 意思 因而 
我们 必须 选 出使 句 意 最为 通顺 的 解释 
句法 的 模糊性 自然 语言 的 文法 通常 是 模棱两可 
的 针对 一个 句子 通常 可能会 剖析 Parse 出 多棵 
剖析 树 Parse Tree 而 我们 必须 要 仰赖 语意 
及 前后文 的 信息 才能 在 其中 选择 一棵 最为 
适合 的 剖析 树 有/v 瑕疵/n 的/uj 或不/i 规范/n 的/uj 
输入/v 例如/v 语音/n 处理/v 时/n 遇到/v 外国/ns 口音/n 或/c 地方/n 
口音/n 或者 在 文本 的 处理 中 处理 拼写 语法 
或者 光学 字符识别 OCR 的 错误 语言 行为 与 计划 
句子 常常 并不 只是 字面 上 的 意思 例如 你 
能把 盐 递 过来 吗 一个 好 的 回答 应当 
是 把 盐 递过去 在 大多数 上下文 环境 中 能 
将 是 糟糕 的 回答 虽说 回答 不 或者 太远 
了 我 拿不到 也 是 可以 接受 的 再者 如果 
一门 课程 上 一年 没 开设 对于 提问 这门 课程 
去年 有 多少 学生 没 通过 回答 去年 没开 这门 
课 要比 回答 没人 没 通过 好 一般 认为 计算 
语言学 CL 是 语言学 的 一个 分支 自然语言 处理 NLP 
是 计算机 科学 的 一个 子 学科 但是 现在 由于 
CL 和 NLP 之间 的 界限 越来越 模糊 甚至 两个 
领域 的 人 常常 去 参加 同样 的 会议 交流 
起 工作 来 也 完全 没有 障碍 于是/nr 一个 问题 
出现 了 NLP 是 跨 语言学 和 计算机 科学 的 
交叉 学科 吗 近日 在 NLP 学术 圈里 因为 Twitter 
上 的 一个 推 文 引发 了 对 这个 问题 
的 一场 小争论 一 The Beginning 过程 大概 是 这样 
的 华盛顿 大学 著名 的 语言 学 教授 Emily M 
. Bender 在 审核 一篇 跨语言 应用 的 论文 时 
为 作者 数据集 的 混乱 不堪 发愁 于是 就 发了 
个 twetter 直接 的 评论 到 没有 转 推 也就 
两个 但 两个 转 推却 引来 两场 争论 一场 主要 
关于 怎么 用 数据集 的 因为 和 本文 无关 这里 
我们 就 按下不表 了 另一 场 争论 起点 是 这样 
的 推 文 转了 四天 到了 纽约大学 计算 科学 与 
数据 科学 的 助理 教授 Kyunghyun Cho 这里 他 抱怨 
说 为什么 我 不用 更多 的 语言 因为 投稿 ACL 
反馈 回来 的 评审 意见 是 奇怪 作者 竟然 选择 
用 土耳其语 英语 数据集 这条 推 文 本身 也是 对 
数据集 问题 的 响应 Emily Bender 教授 是 这么 回复 
的 会 不会 是 因为 由于 ACL 是 跨学科 的 
人手 不够 不能 相互 审查 我们 知道 推 文 就像 
聊天 不一定 看到 的 人 联想 到 什么 呢 所以 
推着 推着 话题 就 变了 接着 i n t e 
r d i s c i p l i n 
a r i t y 这个词 约翰 霍普金斯大学 Jason Eisner 
教授 nlp 几个 神 牛 之一 的 博士生 Ryan Cotterell 
加入 了 讨论 一个 是 著名 的 语言 学 教授 
一个 只 是 一个 博士生 在 国内 可能 Ryan 早早 
缴械投降 说 我 one million percent endorse Emily 教授 的 
观点 了 但 Ryan 毕竟 也是 大牛 的 学生 并不/i 
怯/zg 于和/nr 教授/n 辩论/n 二 ACL 是 跨学科 的 吗 
随后 的 内容 就 变得 越来越 复杂 也有 越来越 多 
的 人 加入 争论 加上 twitter 140 个字符 的 限制 
道理 更是 说不清 于是 Ryan Cotterell 决定 做 两件 事情 
1 在 medium . com 上 写篇 博文 来 澄清 
他 的 观点 2 通过 定量 的 方法 来 考察 
语言学 和 NLP 之间 的 关系 博文 内容 大致 如下 
我 最近 推 了 一些 东西 没想到 引起 了 一些 
争论 由于 Twitter 上 140 个字符 长度 的 限制 可能 
会 引起 误解 我 的 观点 是 在 NLP 上 
发表 的 工作 并 没有 吸取 语言学 方面 最 新的 
进展 因此 也 没有 被 公 认为 是 跨学科 的 
也许 更 坦率 的 观点 例如 这样 当然 我 不认为 
语言学 最近 三十年 没什么 有意思 的 成果 但 很明显 语言学 
和 NLP 已经 分 离开了 举 一个 例子 在 NLP 
阅读 群 例如 Stanford CLSP Stony Brook 和 Arizona 等 
从 他们 的 日程 安排 中 我 找不到 一篇 最近 
的 语言学 论文 如果 两个 学科 有 交叉 的话 这是 
不 应该 的 所以 我 的 观点 的 弱化 版本 
是 NLP 在 过去 10 到 20年 的 发展 与 
近期 语言学 的 研究 无关 在 深入 这个 观点 之前 
我 想 先 说 清楚 两个 概念 什么 是 计算 
语言学 CL 以及 什么 是 自然 语言 处理 NLP 内容 
来 自我 导师 Jason Eisner 在 Quora 上 的 回答 
1 什么 是 计算 语言学 计算 语言学 CL 类似 于 
计算 生物学 或者 任何 计算 XXX 它 主要 致力于 用 
计算 的 方法 来 回答 语言学 的 科学 问题 在 
语言学 中 的 核心 问题 包括 语言表征 和 语言 知识 
的 性质 如何 在 语言 的 产生 理解 中 获得 
和 运用 语言 学 知识 对这 类 问题 的 回答 
有助于 描述 人类 的 语言 能力 也 有助于 解释 我们 
实际 记录 的 语言 数据 和 行为 的 分布 在 
计算 语言学 中 我们 用 更 形式化 的 答案 来 
回答 这些 问题 语言学家 关心 人 类 计算 了 什么 
以及 如何 计算 的 所以 我们 将 语言表征 和 语法 
通过 数学 的 形式 来 定义 我们 研究 它们 的 
数学 属性 并 设计 有效 的 算法 来 学习 生成 
和 理解 只要 这些 算法 可以 实际 运行 我们 就 
可以 测试 我们 的 模型 看 它们 是否 能 作出 
合理 的 预测 语言学 也 考虑 一些 非 核心 的 
问题 例如 社会 语言学 历史 语言学 生理 语言学 或者 神经 
语言学 等等 这些 学科 问题 本质 上 和 计算 语言学 
是 平等 的 都是 在用 一套 模型 和 算法 来 
让 数据 看 起来 合理 从 这个 角度 来说 计算 
语言学 并不 试图 去 对 日常用语 进行 建模 而是 将 
语言学家 所作 的 推论 自动化 这 潜在地 就使 我们 能够 
处理 更大 的 数据 集 甚至 新的 数据 并 得出 
更 准确 的 结论 同样 的 计算 语言学家 可能会 设计 
软件工具 来 帮助 记录 濒危 语言 2 什么 是 自然 
语言 处理 NLP 自然语言 处理 NLP 是 解决 分析 或 
生成 自然语言 文本 的 工程 问题 的 艺术 在 这里 
成功 的 标准 不 在于 你 设计 了 一个 更好 
的 科学 理论 或者 是 证明 了 语言 X 和Y在/nr 
历史 上 是 相关 的 它 的 标准 是 你 
是否 在 工程 问题 上 得到 了 好 的 解决方案 
例如 你 不会 去 考虑 谷歌 翻译 有 没有 解释 
翻译 的 本质 是 什么 或者 翻译 人员 如何 工作 
的 你 在意 的 是 它 能否 给 你 产生 
出 一个 合理 精确 流畅 的 翻译 结果 机器翻译 社群 
有 他们 自己 的 衡量 方法 他们 致力 于 提高 
这些 分数 而 不是 理解 翻译 的 本质 NLP 主要 
是 用来 帮助 人们 去 理解 和 消化 那些 以 
文本 形式 存在 的 大量 信息 当然 也 会被 用来 
生成 更好 的 用户 接口 以 便于 人类 更好 的 
与 机器 或人 进行 交流 我 说 NLP 是 工程 
性质 的 并不 意味着 它 只 用来 开发 商业 价值 
NLP 也会 被 拿来 研究 学术 问题 例如 政治 科学 
博客 文章 经济学 金融 新闻 和 报道 医学 医生 的 
笔记 数字 人文 文学作品 历史 资料 等 这些 都是被/nr 作为 
计算 XX 学 的 工具 来 回答 XX 学家 的 
科学 问题 而 不属于 语言学家 的 科学 问题 3 跟 
交叉 学科 有 什么 关系 呢 计算 语言学 已经 被 
定义 为 一个 交叉 学科 但 NLP 还 没有 可能 
是 也 可能 不是 正如 航空工程 不 需要 从 鸟类 
获得 灵感 一样 NLP 也 不必 从 人类 如何 处理 
语言 中 获得 灵感 所以 我 认为 应当 认真 考虑 
的 一个 问题 是 目前 还 没有 关于 NLP 是否 
是 交叉 学科 的 判断 标准 相关 的 人员 只是 
从 他们 的 工具箱 里 选择 一些 工具 解决 他们 
的 工程 问题 在 ACL 会议 中 很多 绝大多数 工作认真 
来看 都 不能 算是 交叉 学科 的 4 交叉学科 应该 
是 什么 样子 Wilson and Hayes 2008 曾经 做 的 
一份 工作 可以 很好 的 回答 这个 问题 首先 他们 
借鉴 NLP 和 ML 的 技术 提出 了 一些 提高 
语言 理解 的 方法 由此 他们 得出 一些 关于 语言 
的 科学 结论 并 通过 实验 验证 了 这些 结论 
学科 的 定义 一直 是 在 变化 的 我 认为 
所谓 跨学科 的 工作 其实 就是 两个 多个 群体 兴趣 
的 交集 一些 人 认为 因为 NLP 里 有 word 
和 punctuation 这些 语言学 的 概念 就是 跨学科 了 这是 
很 荒谬 的 我们 也 使用 对数 的 概念 能说 
NLP 与 数学 也有 交叉 吗 我们 所做 的 工作 
与 数学 期刊 上 的 完全 不同 争论 中 出现 
有 两个 容易 混淆 的 论断 Claim 1 没有 语言学 
的 理论 NLP 就做 不下去 这是 Emily Bender 教授 的 
说法 我 倾向 于 认同 但 我 没有 做过 面向 
人类 的 NLP 任务 所以 我 实在 不好 下判断 Claim2 
计算 语言学 的 工作 没有 真正 地 呈现 在 * 
ACL 会议 上 我 认为 这 基本上 是 对 的 
但也 有 一些 例外 只不过 很 少见 原因 有 两面 
当 我 对 NLPer 谈论 问题 时 他们 会问 这 
有 什么 用 而 当 我 和 语言学家 讨论 时 
他们 又 完全 听 不懂 因为 他们 最后 一 节 
数学课 还是 在 高中 上 的 基于 这样 经验 我 
觉得 ACL 并 不是 一个 真正 的 跨学科 的 地方 
而且 是 越来越 如此 举 三个 例子 1 许多 以 
计算 为 导向 的 语言学家 和 面向 语言 的 ACLers 
希望 在 2018年 初 成立 一个 新的 会议 如果 * 
ACL 真的 能够 体现 计算机科学 和 语言学 之间 的 跨 
学科 合作 为什么 会 有 很多 人 需要 另 一个 
会议 呢 我 认为 这 主要 是 因为 这 两个 
学科 之间 基本上 没有 交叉 2 我 在 EMNLP 2017 
上 有一个 海报 展示 是 关于 多 语言 形态 标记 
的 Cotterell and Heigold 2017 我 得到 的 第一 个 
问题 是 来自 工业界 的 一个 NLP 研究 人员 他 
很 真诚 地 问 现在 已经 可以 通过 端对端 训练 
一切 东西 了 为什么 还要 进行 词性 标注 呢 在 
一定 程度 上 这也 是 许多 有 建树 的 研究 
人员 的 观点 例如 Kyunghyun Cho 当然 在 模型 中选 
不 选用 词性 标注 应该 根据 你 的 问题 而定 
我 想 强调 的 是 我们 正 处于 这样 一个 
时期 之前 NLP 的 一些 旧 的 辅助 应该 用 
新一代 重新 判断 了 如果 认真 去 考察 的话 就 
会 发现 词性 标注 是 句法 理论 中 相对 肤浅 
的 部分 Fred Jelinek 著名 的 讽刺 是 每次 他 
炒 掉 一个 语言学家 性能 就 会 提升 一些 而且 
这个 咒语 现在 还 在 很多 NLP 领域 有效 3 
另外 一点 就是 很多 NLPer 并 没有 学过 语言学 如 
Emily 所说 交叉 学科 研究 的 本质 上 需要 两个 
领域 的 专业 知识 我 认为 这些 专业 知识 应该 
以 某种 形式 来源于 领域内 的 专家 而 据我所知 这 
似乎 并 没有 发生 5 定量化 研究 我 现在 正在 
尝试 研究 发表 在 语言 学会 议 / 期刊 的 
论文 与 发表 在 NLP 会议 / 期刊 上 的 
论文 之间 的 引用 情况 来 定量化 研究 语言学 与 
NLP 之间 关系 初步 的 结果 表明 两者 的 重合 
率 非常 小 我 非常 欢迎 任何 能 促进 这份 
研究 的 建议 三 语言学 NLP 和 跨学科 看到 Ryan 
Cotterell 的 博文 还 有些 别的 原因 Emily Bender 教授 
也 就此 在 medium 上 写了 一篇 博文 来 反驳 
Ryan 博文 有 一部分 是 针对 另一 场 争论 的 
反驳 首先 Emily 教授 回顾 了 一下 事件 的 起因 
前面 已经 说 过了 然后 针对 第一波 争论 给 出了 
一针见血 的 评论 随后 话锋一转 说 第一 波 争论 周日 
晚上 已经 圆满 结束 但 又来了 波 新的 部分 博文 
内容 Ryan Cotterell 花了 很大 力气 试图 说服 每一个 人 
相信 NLP 不是 一个 交叉 学科 理由 是 他 认为 
交叉学科 必须 建立 在 两个 学科 共同 的 工作 基础 
上 而 目前 NLP 的 工作 大 部分 不 符合 
不 符合 这个 定义 对此 我 想 做出 以下 回答 
如果 问题 要求 多个 领域 的 专业 知识 有效 地 
接近 一个 研究 领域 原则 上 就是 跨学科 的 根据 
我 的 定义 NLP 原则 上 就是 跨学科 的 我 
同意 Ryan 的 观点 说 NLP 在 实践 中 大多 
不 是 学科 交叉 的 但 我 觉得 没有 必要 
非要 达到 这么 高的/nr 标准 同样 的 我/r 也/d 不认为/i 
语言学/n 的/uj 所有/b 子/ng 领域/n 都和/nr NLP/w 相关/v 我 的 
观点 是 学习 语言 如何 工作 以及 或者 与 有 
相关 经验 的 人 合作 会 让 NLP 发展 地 
更好 对 交叉学科 如此 高 标准 的 定义 我 觉得 
是 无益 的 我 不 希望 人们 认为 如果 我 
不能 拿 一个 语言学 学位 我 就 没法 做 交叉 
学科 的 工作 同样 我 也不 希望 人们 留下 语言学 
无关紧要 的 印象 也许 这个 争论 中 最 令人 沮丧 
的 部分 是 它 抹 除了 我 在 语言学 领域 
和 CL / NLP 领域 的 工作 他们 似乎 还将 
语言学 等同于 现代 乔姆斯基 语法 另一方面 那些 通常 NLP 不 
使用 语言学 的 工作 的 论断 等同于 把 包括 我 
在内 的 一些 人 的 工作 都给 排除 在外 了 
所以 Hey world 语言学 已经 不是 乔姆斯基 时代 的 了 
四 结语 事情 大致 就是 如此 说 大不大 说 小 
也 确实 小 它 反映 出 几个 问题 1 要不 
要 认真 地 考察 一下 NLP 是否 是 交叉 学科 
Ryan Cotterell 在 尝试 用 论文 之间 引 用量 的 
数据 来 定量化 分析 这个 问题 也许会 很 有意思 2 
另一方面 不管 讨论 的 结果 是 什么 它 都是 有益 
的 因为 讨论 促使 人们 去 反复 地 思考 自己 
的 观点 国外 学术 圈子 的 讨论 风气 很 旺盛 
第一步 获取 语料 语料 即 语言 材料 语料 是 语言 
学 研究 的 内容 语料 是 构成 语料库 的 基本 
单元 所以 人们 简单 地 用 文本 作为 替代 并把 
文本 中的 上下文 关系 作为 现实 世界 中 语言 的 
上下文 关系 的 替代品 我们 把 一个 文本 集合 称为 
语料库 Corpus 当 有几个 这样 的 文本 集合 的 时候 
我们 称之为 语料库 集合 Corpora 定义 来源 百度 百科 按 
语料 来源 我们 将 语料 分为 以下 两种 1 . 
已有 语料 很多 业务 部门 公司/n 等/u 组织/v 随着/p 业务/n 
发展/vn 都会/nr 积累/v 有/v 大量/n 的/uj 纸质/n 或者/c 电子文本/n 资料/n 
那么 对于 这些 资料 在 允许 的 条件 下 我们 
稍加 整合 把 纸质 的 文本 全部 电子化 就 可以 
作为 我们 的 语料库 2 . 网上 下载 抓取 语料 
如果 现在 个人 手里 没有 数据 怎么办 呢 这个 时候 
我们 可以 选择 获取 国内外 标准 开放 数据集 比如 国内 
的 中文 汉语 有 搜狗 语料 人民日报 语料 国外 的 
因为 大都 是 英文 或者 外文 这里 暂时 用不到 也 
可以 选择 通过 爬虫 自己 去 抓取 一些 数据 然后 
来 进行 后续 内容 第二步 语料 预处理 这里 重点 介绍 
一下 语料 的 预处理 在 一个 完整 的 中文 自然语言 
处理 工程 应用 中 语料 预处理 大概会 占到 整个 50% 
70% 的 工作量 所以 开发 人员 大 部分 时间 就 
在 进行 语料 预处理 下面 通过 数据 洗清 分词 词性 
标注 去 停用词 四 个大 的 方面 来 完成 语料 
的 预 处理工作 1 . 语料 清洗 数据 清洗 顾名思义 
就是 在 语料 中 找到 我们 感兴趣 的 东西 把 
不 感兴趣 的 视为 噪音 的 内容 清洗 删除 包括 
对于 原始 文本 提取 标题 摘要 正文 等 信息 对于 
爬 取 的 网页 内容 去除 广告 标签 HTML JS 
等 代码 和 注释 等 常见 的 数据 清洗 方式 
有 人工 去 重 对齐 删除 和 标注 等 或者 
规则 提取 内容 正则表达式 匹配 根据 词性 和 命名 实体 
提取 编写 脚本 或者 代码 批处理 等 2 . 分词 
中文 语料 数据 为 一批 短 文本 或者 长 文本 
比如 句子 文章 摘要 段落 或者 整篇文章 组成 的 一个 
集合 一般 句子 段落 之间 的 字 词语 是 连续 
的 有 一定 含义 而 进行 文本 挖掘 分析 时 
我们 希望 文本处理 的 最小 单位 粒度 是 词 或者 
词语 所以 这个 时候 就 需要 分词 来 将 文本 
全部 进行 分词 常见 的 分词 算法 有 基于 字符串 
匹配 的 分词 方法 基于 理解 的 分词 方法 基于 
统计 的 分词 方法 和 基于 规则 的 分词 方法 
每 种方法 下面 对应 许多 具体 的 算法 当前/t 中文分词/i 
算法/n 的/uj 主要/b 难点/d 有/v 歧义/n 识别/v 和/c 新词/n 识别/v 
比如 羽毛球拍 卖完 了 这个 可以 切 分成 羽毛 球拍 
卖 完 了 也可 切 分成 羽毛球 拍卖 完 了 
如果 不 依赖 上下文 其他 的 句子 恐怕 很难 知道 
如何 去 理解 3 . 词性 标注 词性 标注 就是 
给 每个 词 或者 词语 打 词类 标签 如 形容词 
动词 名词 等 这样 做 可以 让 文本 在 后面 
的 处理 中 融入 更多 有用 的 语言 信息 词性 
标注 是 一个 经典 的 序列 标注 问题 不过 对于 
有些 中文 自然语言 处理 来说 词性 标注 不 是非 必需 
的 比如 常见 的 文本 分类 就 不用 关心 词 
性问题 但是 类似 情感 分析 知识 推理 却是 需要 的 
下图 是 常见 的 中文 词性 整理 常见 的 词性 
标注 方法 可以 分为 基于 规则 和 基于 统计 的 
方法 其中 基于 统计 的 方法 如 基于 最大熵 的 
词性 标注 基于 统计 最大 概率 输出 词性 和 基于 
HMM 的 词性 标注 4 . 去 停用词 停用词 一般指 
对 文本 特征 没有 任何 贡献 作用 的 字词 比如 
标点符号 语气 人称 等 一些 词 所以在 一般性 的 文本 
处理 中 分词 之后 接下来 一步 就是 去 停用词 但是 
对于 中文 来说 去 停用词 操作 不是 一成不变 的 停用词 
词典 是 根据 具体 场景 来 决定 的 比如 在 
情感 分析 中 语气词 感叹号 是 应该 保留 的 因为 
他们 对 表示 语气 程度 感情/n 色彩/n 有/v 一定/d 的/uj 
贡献/n 和/c 意义/n 第三步 特征 工程 做完 语料 预处理 之后 
接下来/l 需要/v 考虑/v 如何/r 把/p 分词/n 之后/f 的/uj 字/n 和/c 
词语/n 表示/v 成/n 计算机/n 能够/v 计算/v 的/uj 类型/n 显然 如果 
要 计算 我们 至少 需要 把 中文分词 的 字符串 转换成 
数字 确切的说 应该 是 数学 中的 向量 有/v 两种/m 常用/b 
的/uj 表示/v 模型/n 分别/d 是/v 词/n 袋/q 模型/n 和词/nr 向量/n 
词 袋 模型 Bag of Word BOW 即 不考虑 词语 
原本在 句子 中 的 顺序 直接 将 每 一个 词语 
或者 符号 统一 放置 在 一个 集合 如 list 然后 
按照 计数 的 方式 对 出现 的 次数 进行 统计 
统计 词频 这 只是 最 基本 的 方式 TF IDF 
是 词 袋 模型 的 一个 经典 用法 词 向量 
是 将 字 词语 转换成 向量 矩阵 的 计算 模型 
目前 为止 最 常用 的 词 表示 方法 是 One 
hot 这种 方法 把 每个 词 表示 为 一个 很长 
的 向量 这个 向量 的 维度 是 词表 大小 其中 
绝大多数 元素 为 0 只有 一个 维度 的 值 为 
1 这个 维度 就 代表 了 当前 的 词 还有 
Google 团队 的 Word2Vec 其 主要 包含 两 个 模型 
跳 字 模型 Skip Gram 和 连续 词 袋 模型 
Continuous Bag of Words 简称 CBOW 以及 两 种 高效 
训练 的 方法 负 采样 Negative Sampling 和 层序 Softmax 
Hierarchical Softmax 值得一提的是 Word2Vec 词 向量 可以 较好 地 表达 
不同 词 之间 的 相似 和 类比 关系 除此之外 还有 
一些 词 向量 的 表示 方式 如 Doc2Vec WordRank 和 
FastText 等 第四步 特征选择 同 数据挖掘 一样 在 文本 挖掘 
相关 问题 中 特征 工程 也 是 必不可少 的 在 
一个 实际 问题 中 构造 好 的 特征向量 是 要 
选择 合适 的 表达 能力 强的/nr 特征 文本 特征 一般 
都是 词语 具有 语义 信息 使用 特征选择 能够 找出 一个 
特征 子集 其 仍然 可以 保留 语义 信息 但 通过 
特征提取 找到 的 特征 子空间 将会 丢失 部分 语义 信息 
所以 特征选择 是 一个 很 有 挑战 的 过程 更多/d 
的/uj 依赖/v 于/p 经验/n 和/c 专业/n 知识/v 并且 有 很多 
现成 的 算法 来 进行 特征 的 选择 目前 常见 
的 特征 选择 方法 主要 有 DF MI IG CHI 
WLLR WFO 六种 第五步 模型 训练 在 特征向量 选择 好 
之后 接下来 要做 的 事情 当然 就是 训练 模型 对于 
不同 的 应用 需求 我们 使用 不同 的 模型 传统/n 
的/uj 有/v 监督/vn 和无/nr 监督/vn 等/u 机器学习/i 模型/n 如 KNN 
SVM Naive Bayes 决策树 GBDT K means 等 模型 深度 
学习 模型 比如 CNN RNN LSTM Seq2Seq FastText TextCNN 等 
这些 模型 在 后续 的 分类 聚 类 神经 序列 
情感 分析 等 示例 中 都会 用到 这里 不再 赘述 
下面 是 在 模型 训练 时 需要 注意 的 几个 
点 1 . 注意 过拟合 欠 拟合 问题 不断 提高 
模型 的 泛化 能力 过拟合 模型 学习 能力 太强 以至于 
把 噪声 数据 的 特征 也 学习 到了 导致 模型 
泛化 能力 下降 在 训练 集上 表现 很好 但是 在 
测试 集上 表现 很差 常见 的 解决 方法 有 增大 
数据 的 训练 量 增加 正则化 项 如 L1 正则 
和 L2 正则 特征 选取 不合理 人工 筛选 特征 和 
使用 特征选择 算法 采用 Dropout 方法 等 欠 拟合 就是 
模型 不 能够 很好 地 拟合 数据 表现 在 模型 
过于 简单 常见 的 解决 方法 有 添加 其他 特征 
项 增加 模型 复杂度 比如 神经网络 加 更多 的 层 
线性 模型 通过 添加 多项式 使 模型 泛化 能力 更强 
减少 正则化 参数 正则化 的 目的 是 用来 防止 过拟合 
的 但是 现在 模型 出现 了 欠 拟合 则 需要 
减少 正则化 参数 2 . 对于 神经网络 注意 梯度 消 
失和 梯度 爆炸 问题 115 入门 问题 TensorFlow 深度 学习 
好玩儿 的 算法 应用 实例 聊天 机器人 神经网络 机器学习 机器学习 
算法 应用 实例 自然语言 处理 数据 科学 Python Java 机器学习 
－ － 初期 的 笔记 路线 软件 安装 面试 入门 
问题 简单 粗暴 地 入门 机器学习 机器学习 的 技术 栈 
及 应用 实例 脑 洞 深度 学习 相关 最新图书 推荐 
T e n s o r F l o w 
T e n s o r F l o w 
11 策略 网络 TensorFlow 10 基于 LSTM 建立 一个 语言 
模型 TensorFlow 9 词 的 向量 表示 TensorFlow 8 详解 
TensorBoard 如何 调 参 TensorFlow 7 TensorBoard Embedding 可视化 TensorFlow 
6 TensorBoard 可视化 学习 TensorFlow － 5 用 tf . 
contrib . learn 来 构建 输入 函数 TensorFlow 4 tf 
. contrib . learn 快速 入门 TensorFlow － 3 用 
feed forward neural network 识别 数字 TensorFlow 2 用 CNN 
识别 数字 TensorFlow － 1 如何 识别 数字 TensorFlow 入门 
一 文学 会用 Tensorflow 搭建 神经网络 用 Tensorflow 建立 CNN 
深度 学习 深度 学习 的 主要 应用 举例 Keras 对比 
学习 用 Keras 搭建 CNN RNN 等 常用 神经网络 强化 
学习 强化 学习 是 什么 一文 了解 强化 学习 好 
玩儿 的 算法 应用 实例 5 分钟 构建 一个 自己 
的 无人 驾驶 车 自己动手 写个 聊天 机器人 吧 自己 
写个 Prisma 用 TensorFlow 创建 自己 的 Speech Recognizer 用 
TensorFlow 让 你 的 机器人 唱首 原创 给 你 听 
如何 自动 生成 文章 摘要 聊天 机器人 开启 聊天 机器人 
模式 用 TensorFlow 做个 聊天 机器人 神经网络 神经网络 神经 网络 
的 前世 神经网络 之 感知器 的 概念 和 实现 神经网络 
之 线性 单元 什么 是 神经 网络 手写 纯 享 
版 反向 传播 算法 公式 推导 常用 激活 函数 比较 
什么 是 DropoutCNN 图解 何为 CNN 用 Tensorflow 建立 CNN 
按 时间轴 简述 九大 卷积 神经网络 RNN 详解 循环 神经网络 
Recurrent Neural Network 图解 RNNCS224d － Day 5 RNN 快速 
入门 用 深度 神经网络 处理 NER 命名 实体 识别 问题 
用 RNN 训练 语言 模型 生 成文 本用 Recursive Neural 
Networks 得到 分析树 RNN 的 高级 应用 LSTM 详解 LSTM 
用 LSTM 来 做 一个 分类 小 问题 用 LSTM 
做 时间 序列 预测 的 一个 小 例子 seq2seqseq2seq 入门 
seq2seq 的 keras 实现 机器学习 Kaggle － － 由此来看 实战 
是 什么样 的 一个 框架 解决 几乎 所有 机器 学习 
问题 通过 一个 kaggle 实例 学习 解决 机器学习 问题 从 
0 到 1 走进 KaggleKaggle 神器 xgboost 基础 － － 
一些 基本 概念 和小/nr 技巧 轻松 看懂 机器学习 十大 常用 
算法 特征 工程 怎么做 机器学习 算法 应用 中 常用 技巧 
1 机器学习 算法 应用 中 常用 技巧 2 关于 凸 
优化 如何 选 择优 化器 optimizer 为什么 要用 交叉 验证 
用 学习曲线 learning curve 来 判别 过拟合 问题 用 验证 
曲线 validation curve 选择 超 参 数用 Grid Search 对 
SVM 进行 调 参用 Pipeline 将 训练 集 参数 重复 
应用到 测试 集 PCA 的 数学 原理 和 可视化 效果 
机器学习 中 常用 评估 指标 汇总 什么 是 ROC AUC 
算法 － － 通俗易懂 讲 算法 决策树 的 python 实现 
CART 分类 与 回归 树 Bagging 简述 Adaboost 算法 浅谈 
GBDT 用 ARIMA 模型 做 需求预测 推荐 系统 Sklearn Sklearn 
快速 入门 了解 Sklearn 的 数据 集 自然语言 处理 cs224d 
Day 1 . 深度 学习 与 自然 语言 处理 主要 
概念 一览 Day 2 . TensorFlow 入门 Day 3 . 
word2vec 模型 思想 和 代码 实现 Day 4 . 怎样 
做 情感 分析 Day 5 . CS224d － Day 5 
RNN 快速 入门 Day 6 . 一 文学 会用 Tensorflow 
搭建 神经网络 Day 7 . 用 深度 神经网络 处理 NER 
命名 实体 识别 问题 Day 8 . 用 RNN 训练 
语言 模型 生成 文本 Day 9 . RNN 与 机器 
翻译 Day 10 . 用 Recursive Neural Networks 得到 分析树 
Day 11 . RNN 的 高级 应用 一个 隐 马尔科夫 
模型 的 应用 实例 中文分词 数据 科学 1 . ［ 
图解 DS 基础 概念 ］ AB Testing Type 1 / 
2 Error2 . ［ 图解 DS 基础 概念 ］ Critical 
value Alpha Z － score P － value 关系 PythonPandas 
常用命令 － 1Pandas 常用命令 － 2Pandas QQ 聊天记录 分析 Python 
爬虫 1 快速 入门 Python 爬虫 2 爬 取 多页 
网页 Java 入门 Java 系列 汇总 2 天 入门 Java 
－ Day 1Day 1 Java imooc － 2 . 变量 
常量 Day 1 Java imooc － 3 . 运算符 Day 
1 Java imooc － 4 . 流程 控制 语句 Day 
1 Java imooc － 5 . 数组 Day 1 Java 
imooc － 6 . 方法 2 天 入门 Java － 
Day 2Day 2 Java － imooc － 8 封装 Day 
2 Java － imooc － 9 继承 Day 2 Java 
－ imooc － 10 多态 机器学习 － － 初期 的 
笔记 很 粗糙 机器学习 － 多元 线性 回归 Udacity Machine 
Learning 纳米 学位 － 学习 笔记 1Machine Learning Notes Decision 
Trees UdacityMachine Learning Notes Linear Regression Udacity 支持 向量 机 
神经网络 Instance Based LearningEnsemble Learners 路线 数据 科学家 养成 路线 
纯粹 的 数学 之美 Python 很 强大 一张 图 带 
你 看懂 何为 数据 分析 如何 成为 一名 数据 科学家 
并 得到 一份 工作 软件 安装 ［ MySQL ］ 5 
分钟 入门 MySQL Workbench 图解 Mac 下 如何 安装 管理 
MySQL ［ Virtualenv ］ 详解 Mac 配置 虚拟环境 Virtualenv 安装 
Python 科学计算 包 面试 面试官 是 怎么 看 你 的 
Github profile ［ Leetcode ］ LEETCODE Linked List 题目 思路 
汇总 欢迎 关注 公众 号 极客 X 养成 计划 人工智能 
时代 学点 机器学习 一起 持续 迭代 Run With AI 自然语言 
处理 入门 读物 本文 目前 研 二 已经 接触 自然语言 
处理 有 一年 的 时间 半路出家 下面 写 一点 自己 
关于 自然 语言 处理 的 心得 纯属 个人 见解 先从 
入门 学习 开始 写 吧 书籍 理论 篇 书籍 是 
人类 进步 的 阶梯 这个 一点 不假 自己 刚 开始 
接触 自然语言 处理 是从 吴军 老师 的 的 数学 之美 
开始 的 这里 再次 感谢 吴军 老师 这门 书写 的 
通俗 易懂 内容 非常 的 吸引 人 读 起来 不会 
感觉 枯燥 每次 读 都会 有 新的 体会 本书 可以 
作为 自然语言 处理 入门 的 第一 本书 书中 的 知识 
一定 要 查阅 其他 文献 和 博客 因为 本书 并 
没有 对 内容 讲 的 特别 的 细 所以 每个 
知识 都 包含 了 大量 的 拓展 内容 例如 隐 
马尔可夫 的 内容 其实 远 比书 中将 的 多 所以 
拓展 是 配合 本书 的 不二法门 第二本 书 推荐 统计 
自然语言 处理 第 2版 宗 成庆 蓝皮 版 这 本书 
是 宗 老师 的 心血 之作 内容 较 数学 之美 
的 内容 更加 的 偏 特定 领域 对/p 自然/d 语言/n 
处理/v 领域/n 有宗/nr 老师/n 自己/r 独特/a 的/uj 见解/v 书本 内容 
涉及 自然语言 处理 的 大部分 方向 且 对 重要 的 
知识 点 给出 了 较为 详细 的 理论 推导 语言 
通俗易懂 第三本 书 推荐 统计 学习 方法 李航 这 本书 
的 内容 就 更加 偏 数学化 主要 对 自然 语言 
处理 特别 是 统计 自然语言 处理 中 应用 的 模型 
给出 了 非常 详尽 的 数学 公式 推导 建议 有 
一定 数学 功底 的 同学 阅读 因为 我 读 起来 
很 吃力 此外 还有 很多 业界 推荐 的 好书 自然语言 
处理 简明 教程 冯志伟 自然语言 处理 综论 Daniel Jurafsky 自然语言 
处理 的 形式 模型 冯志伟 但是 这些 书 因为 时间 
和 个人 精力有限 尚 未曾 阅读 此处 仅 列出 书籍 
实践篇 自然语言 处理 领域 使用 较多 的 语言 是 python 
所以 建议 使用 python 来 处理 自然语言 处理 领域 的 
相关 内容 另外 自然语言 处理 领域 特别 是 基于 统计 
的 自然 语言 处理 以及 当前 大热 的 深度 学习 
下 的 自然 语言 处理 往往 使用 大量 的 机器 
学习 知识 和 深度 学习知识 书籍 推荐 1 python 基础教程 
翻 译版 本书 的 内容 已经 足够 入门 了 书本 
的 内容 不必 大而全 的 全部 阅读 抓住 主要 的 
想要 精通 以后 慢慢来 python 入门 博客 推荐 2 廖 
雪峰 的 python 教程 非常 的 不错 也是 抓住 主要 
的 以上 两个 已经 足够 我们 python 入门 啦 python 
练习 环境 推荐 3 强烈推荐 ipython 和 ipython notebook 不 
知道 的 百度 哦 谁 用 谁知道 机器学习 书籍 推荐 
4 机器学习 实战 这 本书 中 的 内容 既 有 
理论 说明 也有 代码 讲解 并且 代码 可以 在 书本 
提供 的 网站 上 下载 但是 不 推荐 一 上来 
就看 这本书 因为 如果 原理 没 搞懂 直接 上 代码 
感觉 不 理解 此外 书本 中用 到了 numpy 等 python 
库 若 之前 对 其 没有 了解 直接 学习 比较 
痛苦 机器学习 书籍 推荐 5 西瓜 书 机器学习 周志华 本人 
有 周老师 亲笔 签名 的 书籍 想想 都 开心 本书 
的 内容 介绍 非常 全面 知识 讲解 也 非常 的 
到位 理论知识 较多 代码 不多 非常适合 阅读 主要 是 国内 
的 国内 的 国内 的 没错 这 本书 是 国内 
的 不是 翻译 的 支持 必须 支持 既然 学习 了 
python 和 机器学习 那 总不能 python 停留 在 练习 上 
机器学习 停留 在 理论上 吧 所以 书籍 推荐 6 集体 
智慧 编程 python 自然语言 处理 前 一本 是 使用 python 
语言 编写 代码 实现 一些 现实 的 问题 通过 学习 
可以 切实 的 感受 到 原来 他 一直 都在 只是 
我 不 知道 后者 介绍 了 python 的 一个 自然 
语言 处理 库 NLTK 使用 该 库 解决 自然语言 处理 
中 的 任务 至此 理论 也 有了 实践 也 有了 
可以 说 非常 完美 了 神马 还不够 你 觉得 读书 
太累 一读 就 困 那你就/nr 看点 视频 吧 视频 推荐 
1 自然语言 处理 宗庆成/nr 不要/df 问/n 我/r 怎么样/r 因为 我 
没 看过 因为 我 不 可能 什么 都 看过 但是 
宗 老师 的 课 那是 没 问题 的 推荐 2 
自然语言 处理 关毅 这个 课 我 看过 感觉 不错 推荐 
3 计算 语言学 概论 _ 侯敏 本人 没 看过 推荐 
4 哥伦比亚 大学 的 自然 语言 处理 英文 的 具体 
的 课程 地址 已经 变动 可以 百度一下 也 可以 看 
一下 他人 的 博客 自然语言 处理 大 菜鸟 自己 英语 
不好 就 看看 别人 的 心得 喽 当然 coursera 上 
还有 一些 其他 的 视频 资源 如果 你 的 英语 
还 可以 可以 去 平台 上 搜索 一下 自然语言 处理 
已经 有 视频 来了 那么 机器学习 有什么 视频 看 吗 
当然有 推荐 5 mooc 学院 机器学习 这个 视频 是 大牛 
Andrew Ng 讲 的 非常 的 不错 毕竟 是 大牛 
嘛 比 我等 凡人 理解 的 深 太多 了 推荐 
6 这个 推荐 就是 众多 的 mooc 网站 了 因为 
推荐 5 是 我 自己 看 的 视频 但是/c 很多/m 
慕课/nr 网站/n 都/d 存在/v 机器学习/i 课程/n 如 网易 公开课 机器学习 
也是 Andrew Ng 讲 的 慕课网/nr 初识 机器学习 台湾 大 
学林 轩 田 机器学习 林 老师 的 机器学习 讲解 非常 
的 不错 现在 好啦 我们 可以 拿 本书 一边 学习 
书本 知识 一边 看 视频 讲解 我 想 这会 让 
自己 很快 的 入门 想想 都 开心 机器学习 领域 重要 
的 会议 国际 机器学习 会议 ICML 国际 神经 信息处理系统 会议 
NIPS 国际 学习理论 会议 COLT 欧洲 机器学习 会议 ECML 亚洲 
机器学习 会议 ACML 重要 的 国际 学术 期刊 Journal of 
Machine Learning ResearchMachine L e a r n i n 
g I J C A I A A A I 
A r t i f i c i a l 
I n t e l l i g e n 
c e J o u r n a l of 
Artificial Intelligence Research 参考 1 自然语言 处理 怎么 最快 入门 
知乎 上 的 大神 作者 微软 亚洲 研究院 链接 https 
/ / www . zhihu . com / question / 
19895141 / answer / 149475410 来源 知乎 著作权 归 作者 
所有 商业 转载 请 联系 作者 获得 授权 非商业 转载 
请 注明 出处 自然语言 处理 简称 NLP 是 研究 计算机 
处理 人类 语言 的 一门 技术 包括 1 . 句法 
语义分析 对于 给定 的 句子 进行 分词 词性 标记 命名 
实体 识别 和 链接 句法分析 语义 角色 识别 和 多义词 
消 歧 2 . 信息 抽取 从 给定 文本 中 
抽取 重要 的 信息 比如 时间 地点 人物 事件 原因 
结果 数字 日期 货币 专有名词 等等 通俗 说来 就是 要 
了解 谁在 什么 时候 什么原因 对谁 做 了 什么 事 
有 什么 结果 涉及 到 实体 识别 时间 抽取 因果关系 
抽取 等 关键 技术 3 . 文本 挖掘 或者 文本 
数据挖掘 包括 文本 聚 类 分类 信息 抽取 摘要 情感 
分析 以及 对 挖掘 的 信息 和 知识 的 可视化 
交互式 的 表达 界面 目前 主流 的 技术 都是/nr 基于 
统计 机器 学习 的 4 . 机器翻译 把 输入 的 
源语言 文本 通过 自动 翻译 获得 另外 一种 语言 的 
文本 根据 输入 媒介 不同 可以 细分 为 文本 翻译 
语音 翻译 手语 翻译 图形 翻译 等 机器翻译 从 最早 
的 基于 规则 的 方法 到 二十年前 的 基于 统计 
的 方法 再到 今天 的 基于 神经网络 编码 解码 的 
方法 逐渐 形成 了 一套 比较 严谨 的 方法 体系 
5 . 信息检索 对 大 规模 的 文档 进行 索引 
可 简单 对 文档 中 的 词汇 赋 之以 不同 
的 权重 来 建立 索引 也 可利用 1 2 3 
的 技术 来 建立 更加 深层 的 索引 在 查询 
的 时候 对 输入 的 查询 表达式 比如 一个 检索 
词 或者 一个 句子 进行 分析 然后 在 索引 里面 
查找 匹配 的 候选 文档 再 根据 一个 排序 机制 
把 候选 文档 排序 最后 输出 排序 得分 最高 的 
文档 6 . 问答 系统 对 一个 自然 语言 表达 
的 问题 由 问答 系统 给 出 一个 精准 的 
答案 需要 对 自然 语言 查询 语句 进行 某种 程度 
的 语义分析 包括 实体 链接 关系 识别 形成 逻辑 表达式 
然后 到 知识库 中 查找 可能 的 候选 答案 并 
通过 一个 排序 机制 找出 最佳 的 答案 7 . 
对话 系统 系统 通过 一 系列 的 对话 跟 用户 
进行 聊天 回答 完成 某一 项 任务 涉及 到 用户 
意图 理解 通用 聊天 引擎 问答 引擎 对话 管理 等 
技术 此外 为了 体现 上下文 相关 要 具备 多轮 对话 
能力 同时 为了 体现 个性化 要 开发 用户 画像 以及 
基于 用户 画像 的 个性化 回复 随着 深度 学习 在 
图像 识别 语音识别 领域 的 大放异彩 人们 对 深度 学习 
在 NLP 的 价值 也 寄予厚望 再 加上 AlphaGo 的 
成功 人工智能 的 研究 和 应用 变得 炙手可热 自然语言 处理 
作为 人工智能 领域 的 认知 智能 成为 目前 大家 关注 
的 焦点 很多 研究生 都在/nr 进入 自然语言 领域 寄望 未来 
在 人工智能 方向 大展身手 但是 大家 常常 遇到 一些 问题 
俗话说 万事开头难 如果 第一 件 事情 成功 了 学生 就 
能 建立 信心 找到 窍门 今后 越做/nr 越好 否则 也 
可能 就 灰心丧气 甚至 离开 这个 领域 这里 针对 给出 
我 个人 的 建议 希望 我 的 这些 粗浅 观点 
能够 引起 大家 更 深层次 的 讨论 建议 1 如何 
在 NLP 领域 快速 学会 第一个 技能 我 的 建议 
是 找到 一个 开 源 项目 比如 机器翻译 或者 深度 
学习 的 项目 理解 开源 项目 的 任务 编译 通过 
该 项目 发布 的 示范 程序 得到 与 项目 示范 
程序 一致 的 结果 然后 再 深入 理解 开源 项目 
示范 程序 的 算法 自己 编程 实现 一下 这个 示范 
程序 的 算法 再 按照 项目 提供 的 标准 测试 
集 测试 自己 实现 的 程序 如果 输出 的 结果 
与 项目 中 出现 的 结果 不 一致 就要 仔细 
查验 自己 的 程序 反复 修改 直到 结果 与 示范 
程序 基本一致 如果 还是 不行 就 大胆 给 项目 的 
作者 写信 请教 在此 基础 上 再 看看 自己 能否 
进一步 完善 算法 或者 实现 取 得比 示范 程序 更好 
的 结果 建议 2 如何 选择 第一 个 好 题目 
工程型 研究生 选题 很多 都是/nr 老师 给定 的 需要 采取 
比较 实用 的 方法 扎扎实实 地 动手 实现 可能 不 
需要 多少 理论 创新 但是 需要 较强 的 实现 能力 
和 综合 创新 能力 而 学术 型 研究生 需要 取得 
一流 的 研究 成果 因此 选题 需要 有 一定 的 
创新 我 这里 给 出 如下 的 几点 建议 先 
找到 自己 喜欢 的 研究 领域 你 找到 一本 最近 
的 ACL 会议 论文集 从中 找到 一个 你 比较 喜欢 
的 领域 在 选题 的 时候 多 注意 选择 蓝海 
的 领域 这 是因为 蓝海 的 领域 相对 比较 新 
容易 出 成果 充分 调研 这个 领域 目前 的 发展 
状况 包括 如下 几 个 方面 的 调研 方法 方面 
是否/v 有/v 一套/m 比较/d 清晰/a 的/uj 数学/n 体系/n 和/c 机器学习/i 
体系/n 数据 方面 有/v 没有/v 一个/m 大家/n 公认/v 的/uj 标准/n 
训练/vn 集/q 和/c 测试/vn 集/q 研究 团队 是否/v 有/v 著名/a 
团队/n 和/c 人士/n 参加/v 如果 以 上 几个 方面 的 
调研 结论 不是 太 清晰 作为 初学者 可能 不要 轻易 
进入 在 确认 进入 一个 领域 之后 按照 建议 一 
所述 需要 找到 本 领域 的 开源 项目 或者 工具 
仔细 研究 一遍 现有 的 主要 流派 和 方法 先 
入门 反复 阅读 本 领域 最新 发表 的 文章 多 
阅读 本 领域 牛人 发表 的 文章 在 深入 了解 
已有 工作 的 基础 上 探讨 还有 没有 一些 地方 
可以 推翻 改进 综合 迁移 注意 做 实验 的 时候 
不要 贪多 每次 实验 只需要 验证 一个 想法 每次 实验 
之后 必须 要 进行 分析 存在 的 错误 找出 原因 
对 成功 的 实验 进一步 探讨 如何 改进 算法 注意 
实验 数据 必须 是 业界 公认 的 数据 与 已有 
的 算法 进行 比较 体会 能够 得出 比较 一般性 的 
结论 如果 有 则 去 写 一篇 文章 否则 应该 
换 一个 新 的 选题 建议 3 如何写 出 第一 
篇 论文 接 上 一个 问题 如果 想法 不错 且 
被 实验 所 证明 就可 开始 写 第一 篇 论文 
了 确定 论文 的 题目 在 定 题目 的 时候 
一般 不要 系统 研究 与 实践 要 避免 太长 的 
题目 因为 不好 体现 要点 题目 要 具体 有 深度 
突出 算法 写 论文 摘要 要 突出 本文 针对 什么 
重要 问题 提出 了 什么 方法 跟 已有 工作 相比 
具 有 什么 优势 实验 结果 表明 达到 了 什么 
水准 解决 了 什么 问题 写 引言 首先 讲出 本 
项 工作 的 背景 这个 问题 的 定义 它 具有 
什么 重要性 然后 介绍 对 这个 问题 现有 的 方法 
是 什么 有 什么 优点 但是 注意 但是 现有 的 
方法 仍然 有 很多 缺陷 或者 挑战 比如 注意 比如 
有 什么 问题 本文 针对 这个 问题 受 什么 方法 
谁 的 工作 之 启发 提出 了 什么 新的 方法 
并 做了 如下 几 个 方面 的 研究 然后 对 
每个 方面 分门别类 加以 叙述 最后 说明 实验 的 结论 
再说 本文 有 几条 贡献 一般 写 三条 足矣 然后 
说说 文章 的 章节 组织 以及 本文 的 重点 有的 
时候 东西 太多 篇幅 有限 只能 介绍 最 重要 的 
部分 不 需要 面面俱到 相关 工作 对 相关 工作 做 
一个 梳理 按照 流派 划分 对 主要 的 最多 三个 
流派 做 一个 简单 介绍 介绍 其 原理 然后 说明 
其 局限性 然后 可 设立 两 个 章节 介绍 自己 
的 工作 第一 个 章节 是 算法 描述 包括 问题 
定义 数学 符号 算法 描述 文章 的 主要 公式 基本 
都在/nr 这里 有时候 要 给出 简明 的 推导 过程 如果 
借鉴 了 别人 的 理论 和 算法 要给 出 清晰 
的 引文 信息 在此 基础 上 由于 一般 是 基于 
机器学习 或者 深度 学习 的 方法 要 介绍 你 的 
模型 训练 方法 和 解码 方法 第二章 就是 实验 环节 
一般 要 给出 实验 的 目的 要 检验 什么 实验 
的 方法 数据 从 哪里 来 多 大 规模 最好 
数据 是 用 公开 评 测数据 便于 别人 重复 你 
的 工作 然后 对 每个 实验 给出 所需 的 技术 
参数 并 报告 实验 结果 同时 为了 与 已有 工作 
比较 需要 引用 已有 工作 的 结果 必要 的 时候 
需要 重现 重要 的 工作 并 报告 结果 用 实验 
数据 说话 说明 你 比 人家 的 方法 要好 要对 
实验 结果 好好 分析 你 的 工作 与 别人 的 
工作 的 不同 及 各自 利弊 并 说明 其 原因 
对于 目前 尚 不太好 的 地方 要 分析 问题 之 
所在 并将 其 列为 未来 的 工作 结论 对 本文 
的 贡献 再一次 总结 既要 从 理论 方法 上 加以 
总结 和 提炼 也要 说明 在 实验上 的 贡献 和 
结论 所做 的 结论 要让 读者 感到 信服 同时 指出 
未来 的 研究 方向 参考文献 给出 所有 重要 相关 工作 
的 论文 记住 漏掉 了 一篇 重要 的 参考 文献 
或者 牛人 的 工作 基本上 就 没有 被 录取 的 
希望 了 写完 第一稿 然后 就是 再改 三遍 把 文章 
交给 同一个 项目组 的 人士 请 他们 从 算法 新颖 
度 创新性/n 和/c 实验/vn 规模/n 和/c 结论/n 方面/n 以 挑剔 
的 眼光 审核 你 的 文章 自己 针对 薄弱环节 进一步 
改进 重点 加强 算法 深度 和 工作 创新 性 然后 
请 不同 项目组 的 人士 审阅 如果 他们 看 不明白 
说明 文章 的 可读性 不够 你 需要 修改 篇章 结构 
进行 文字 润色 增加 文章 可读性 如 投 ACL 等 
国际 会议 最好 再请 英文 专业 或者 母语 人士 提炼 
文字 1 . 自然语言 处理 要 解决 哪些 任务 1 
解剖 类 分词 词性 标注 命名 实体 识别 word2vec 2 
生成 类 文本 分类 主题 识别 关键词 提取 自动 摘要 
情感 分析 文本 生成 3 情感 分析 智能 问答 系统 
和 知识图谱 为了 直观 理解 这些 任务 推荐 这个 网站 
http / / ictclas . nlpir . org / nlpir 
/ 比较 商业 成熟 一些 的 网站 http / / 
www . datagrand . com / 2 . 对应 不同 
的 任务 所 需要 掌握 的 技能 树 有 哪些 
数据 清洗 正则表达式 匹配 基础 处理 one hot bag of 
words 文本 数字化 tf idf 分词 英文 nltk spacy 中文 
jieba 词性 标注 英文 nltk spacy 中文 jieba CRF 条件 
随 机场 HMM 隐 马 命名 实体 识别 英文 nltk 
spacy 中文 CRF Stanford CoreNLP 主题 识别 plsa 和 LDA 
文本 分类 Word2vec + CNN 文本 生成 RNN LSTM 情感 
分析 关键词 打分 机制 比如 AFINN 1113 . 正则表达式 匹配 
https / / blog . csdn . net / qq 
_ 28633249 / article / details / 77686976 这篇 博客 
讲得 挺 全的/nr 另外 附上 规则 表 用 Python 进行 
自然语言 处理 第一章 搜索 文本 text1 . concordance monstrous # 
搜索 文章 中 的 词语 text3 . concordance lived text1 
. similar monstrous # 近义词 text2 . common _ contexts 
monstrous very # 两个 词 共同 的 上下文 text4 . 
dispersion _ plot citizens democracy freedom duties America # 该 
函数 需要 依赖 numpy 和 matplotlib 库 计数 词汇 set 
text3 # text3 中 所有 标点 单词 的 集合 去 
重 sorted set text3 # text3 中 的 所有 标点 
单词 排序 之后 去 重 len set text3 # text3 
的 独一无二 的 标点 单词 类型 个数 称为 唯一 项目 
类型 print len text3 / len set text3 # 每个字 
平均 被 使用 的 次数 print text3 . count smote 
# 统计 一个 词语 在 一个 文本 中 出现 的 
次数 print 100 * text4 . count a / len 
text4 # 统计 一个 词语 占 全部 词语 的 百分比 
是 多少 函数 def 关键字 定义 lexical _ diversity 为 
函 数名 text 为 参数 def lexical _ diversity text 
    return len text / len set text print 
lexical _ diversity text3 def percentage count total     
return   100 * count / total 将 文 本当 
作词 链表 a = Call me Ishmael . print a 
1 # 索引 是从 0 开始 的 print text4 173 
# 找到 索引 处 的 元素 print text4 . index 
awaken # 找到 元素 第一 次 出现 的 索引 print 
text5 16715 16735 # 获取 链表 中 任意 片段 中 
的 元素 sent = word1 word2 word3 word4 word5 word6 
word7 word8 word9 word10 print sent 5 8 # sent 
m n m n 1 m represents index print sent 
3 # from the first to index 3 exclude index3 
print sent 3 # from index3 to the end sent 
0 = First # replace word1 to First sent 1 
9 = First Last # replace index 1 ~ index 
9 to the designated two words 变量 # assignment 赋值 
variation = expression words ahead numbers and _ is permitted 
my _ sent = a wwe eee noun = my 
_ sent 0 3 print sorted noun # capital is 
ahead of lowercase 字符串 # operation on character string a 
= Monty print a 0 # M print a 4 
# Mont # connect character string print . join Monty 
Python asd # split two character string print Monty Pytho 
n . split 统计 分布 找出 文本 中 最 常见 
的 50个 词 fdist = FreqDist text1 vocabulary = fdist 
. keys voc = list vocabulary # 必须 得 转换成 
list 才能 用 print fdist # print the number of 
words print voc 50 # 分片 前 50个 print fdist 
whale # the number of whale fdist . plot 50 
cumulative = True # 画出 图案 需要 安装 pyqt 找出 
text1 中长 度 超过 15 个字符 的 词 并 排序 
V = set text1 long _ words = w for 
w in V if len w 15 print sorted long 
_ words 找出 长度 超过 7 个字符 并且 出现 次数 
超过 7次 的 词 fdist5 = FreqDist text5 print sorted 
w for w in set text5 if len w 7 
and fdist5 w 7 搭配 经常 出现 的 词 的 
序列 词 对 双 连词 找到 在 一起 出现 的 
两个 词 print bigrams more is said than done # 
找词 对 有 问题 text4 . collocations # 找到 出现 
频繁 的 双 连词 输出 text1 中 每个 词 的 
长度 print len w for w in text1 文本 中的 
词 的 长度 的 性质 统计 fdist = FreqDist len 
w for w in text1 # FreqDist 计数 链表 中 
每个 长 度 出现 的 次数 print fdist . keys 
# 输出 的 是 包含 的 元素 的 不同 词长 
print fdist . items # 以 a b 输出 a 
长度 的 词 出现 了 b 次 print fdist . 
max # 输出 出现 次数 最多 的 长度 print fdist 
3 # 输出 长度 为 3 的 词 出现 的 
次数 print fdist . freq 3 # 输出 长度 为 
3 的 词 出现 的 次数 占 总 词数 的 
比例 print fdist . N # 输出 样本 总数 fdist 
. plot # 绘制 频率 分布图 fdist . plot cumulative 
= True # 绘制 累积 频率 分布图 决策 w for 
w in sent7 if len w 4 # 输出 sent7 
中长 度 小于 4 的 元素 sorted w for w 
in set text1 if w . endswith ableness # 输出 
以 ableness 结尾 的 单词 并 排序 s . startswith 
t # 测试 s 是否 以 t 开头 s . 
endswith t # 测试 s 是否 以 t 结尾 t 
in s # 测试 s 是否 包含 t s . 
islower # 测试 s 中 所有 字符 是否 都是 小写字母 
s . isupper # 测试 s 中 所有 字符 是否 
都是 大写字母 s . isalpha # 测试 s 中 所有 
字符 是否 都是/nr 字母 s . isalnum # 测试 s 
中 所有 字符 是否 都是/nr 字母 或 数字 s . 
isdigit # 测试 s 中 所有 字符 是否 都是/nr 数字 
s . istitle # 测试 s 是否 首字母 大写 s 
中 所有 的 词 都 首字母 大写 控制 对 每个 
元素 进行 操作 len w for w in text1 w 
. upper for w in text1 # 这些 表达式 形式 
为 f w for . . . 或 w . 
f for . . . 其中 f 是 一个 函数 
用来 计算 词长 或 把 字母 转换 为 大写 len 
set word . lower for word in text1 # 由于 
我们 不 重复 计算 像 This 和 this 这样 仅仅 
大小写 不同 的 词 就 已经 从 词汇表 计数 中 
抹 去了 2 000个 len set word . lower for 
word in text1 if word . isalpha # 通过 过滤 
掉 所有 非 字 母元素 从/p 词汇表/n 中/f 消除/v 数字/n 
和/c 标点符号/n 嵌套/nz 代码/n 块/zg 控制结构/n if len word 5 
# 注意 冒号     print word length is less 
than 5 #/i 注意/v 段首空/nr tab/w . . . # 
该行 空出来 再下 一行 输出 # 如果 不 满足 if 
成立 条件 没有 输出 for word in Call me Ishmael 
.     print word . . . # 空 
一行 下 一行 输出 sent1 = Call me Ishmael . 
for word in sent1     if word . endswith 
l             print word . 
. . Call Ishmael # 在 if 和 for 语句 
所 在行 末尾 缩进 开始 之前 有 一个 冒号 # 
所有 的 Python 控制结构 都以 冒号 结尾 冒号 表示 当前 
语句 与 后面 的 缩进 块 有 关联 for token 
in sent1 . . .     if token . 
islower . . .             
print token is a lowercase word . . .   
  elif token . istitle . . .     
        print token is a titlecase word 
. . .     else . . .   
          print token is punctuation . 
. . Call is a titlecase word me is a 
lowercase word Ishmael is a titlecase word . is punctuation 
tricky = sorted w for w in set text2 if 
cei in w or cie in w for word in 
tricky . . .     print word . . 
. 关于 NLP 自动 理解 自然语言 词意 消 歧 我们 
要 算出 特定 上下文 中的 词 被 赋予 的 是 
哪个 意思 自动 消除歧义 需要 使用 上下文 利用 相邻 词汇 
有 相近 含义 这样 一个 简单 的 事实 指代 消解 
一种 更 深刻 的 语言 理解 是 解决 谁 对谁 
做 了 什么 即 检测 主语 和 动词 的 宾语 
a . The thieves stole the paintings . They were 
subsequently sold . b . The thieves stole the paintings 
. They were subsequently caught . c . The thieves 
stole the paintings . They were subsequently found . 要 
回答 这个 问题 涉及 到 寻找 代词 they 的 先行词 
thieves 或者 paintings 处理 这个 问题 的 计算 技术 包括 
指代 消解 anaphora resolution 确定 代词 或 名词 短语 指 
的 是 什么 和 语义 角色 标注 semantic role labeling 
确定 名词 短语 如何 与 动词 相关联 如 施事 受事 
工具 等 自动 生成 语言 如果 我们 能够 解决 自动 
语言 理解 等 问题 我们 将 能够 继续 那些 包含 
自动 生成 语言 的 任务 如 自动 问答 和 机器 
翻译 在 自动 问 答中 一台 机器 要 能够 回答 
用户 关于 特定 文 本集 的 问题 a . Text 
. . . The thieves stole the paintings . They 
were subsequently sold . . . . b . Human 
Who or what was sold c . Machine The paintings 
. 机器 的 回答 表明 它 已经 正确 的 计算 
出 they 是 指 paintings 而 不是 thieves 在 机器 
翻译 中 机器 要 能够 把 文本 翻译成 另一种 语言文字 
并 准确 传达 原文 的 意思 正确 的 翻译 实际上 
取决于 对 代词 的 正确 理解 所有 这些 例子 中 
弄清楚 词 的 含义 动作 的 主语 以及 代词 的 
先行词 是 理解 句子 含义 的 步骤 也 是 我们 
希望 语言 理解 系统 能够 做到 的 事情 机器翻译 长久以来 
机器翻译 MT 都是 语言 理解 的 圣杯 人们/n 希望/v 能/v 
找到/v 从/p 根本/a 上/f 提供/v 高品质/n 的/uj 符合/v 语言/n 习惯/n 
的/uj 任意/v 两种/m 语言/n 之间/f 的/uj 翻译/v 其 历史 可以 
追溯 到 冷战 初期 当时 自动 翻译 的 许诺 带来 
大量 的 政府 赞助 它 也是 NLP 本身 的 起源 
今天 特定 语言 之间 实用 的 翻译 系统 已经 存在 
有些 已经 集成 到 搜索引擎 中了 但是 这些 系统 有 
一些 严重 的 缺点 例如 babelize _ shell 该 函数 
在 nltk3 . 0 中 已经 不再 可用 机器 翻译 
是 困难 的 因为 一个 给定 的 词 可能 有 
几种 不同 的 解释 取决于 它 的 意思 也 因为 
必须 改变 词序 才能 与 目标 语言 的 语法 结构 
保持 一致 今天 这些 困难 遇到 新 情况 从 闻和/nr 
政府/n 网站/n 发布/v 的/uj 两种/m 或/c 两种/m 以上/f 的/uj 语言/n 
文档/n 中/f 可以/c 收集/v 到/v 大量/n 的/uj 相似/v 文本/n 给 
出 一个 德文 和 英文 双语 的 文档 或者 一个 
双语 词典 我们 就 可以 自动 配对 组成 句子 这个 
过程 叫做 文本 对齐 一旦 我们 有 一百万 或 更多 
的 句子 对 就 可以 检测 出 相应 的 词 
和 短语 并 建立 一个 能 用来 翻译 新 文本 
的 模型 人机对话 系统 在 人工智能 的 历史 主要 的 
智能 测试 是 一个 语言学 测试 叫做 图灵测试 一个 响应 
用户 文本 输入 的 对话 系统 能否 表现 的 自然 
到 我们 无法 区分 它 是 人工 生成 的 响应 
相比之下 今天 的 商业 对话 系统 能力 是 非常 有限 
的 但在 较小 的 给定 领域 仍然 有些 作用 How 
may I help you U When is Saving Private Ryan 
playing S For what theater U The Paramount theater . 
S Saving Private Ryan is not playing at the Paramount 
theater but it s playing at the Madison theater at 
3 00 5 30 8 00 and 10 30 . 
你 不能 要求 这个 系统 提供 驾驶 指示 或 附近 
餐馆 的 细节 除非 所需 的 信息 已经 被 保存 
并且 合适 的 问题 答案 对 已经 被 纳入 语言 
处理 系统 请看 这个 系统 似乎 了解 用户 的 目标 
用户 询问 电影 上映 的 时间 系统 正确 的 判断 
出 用户 是 想要 看电影 这一 推断 看起来 如此 明显 
你 可能 都 没有 注意到 它 一个 自然 语言 系统 
需要 被 赋予 这 种 自然 的 交互 能力 没有 
它 当 问到 你 知道 拯救 大兵 瑞恩 什么 时候 
上映 时 系统 可能 只 会 回答 一个 冷冷的 毫无用处 
的 是的 然而 商业/n 对话/n 系统/n 的/uj 开发/v 者/k 使用/v 
上下文/l 语境/n 假设/vn 和/c 业务/n 逻辑/n 确保/v 在/p 用户/n 以/p 
不同/a 方式/n 表达/v 需求/v 或/c 提供/v 信息/n 时对/nr 特定/d 应用/v 
都能/nr 有效/a 处理/v 因此 如果 你 输入 When is . 
. . 或者 I want to know when . . 
. 或者 Can you tell me whe n . . 
. 时 这些 简单 的 规则 总是 对应 着 放映 
时间 这就 足够 系统 提供 有益 的 服务 了 举例 
原始 的 对话 系统 import nltknltk . chat . chatbots 
Which chatbot would you like to talk to 1 Eliza 
psycho babble 2 Iesha teen anime junky 3 Rude abusive 
bot 4 Suntsu Chinese sayings 5 Zen gems of wisdom 
Enter a number in the range 1 5 Error bad 
chatbot numberEnter a number in the range 1 5 1 
文本 的 含义 近年来 一个 叫做 文本 含义 识别 Recognizing 
Textual Entailment 简称 RTE 的 公开 的 共享 任务 使 
语言 理解 所 面临 的 挑战 成为 关注 焦点 基本 
情形 很 简单 假设 你 想 找到 证据 来 支持 
一个 假设 Sandra Goudie 被 Max Purnell 击败 了 而 
你 有 一段 简短 的 文字 似乎 是 有关 的 
例如 Sandra Goudie 在 2002 年 国会 选举 首次 当选 
通过 击败 工党 候选人 Max Purnell 将 现 任 绿党 
下院 议员 Jeanette Fitzsimons 推到 第三位 以 微弱 优势 赢得 
了 Coromandel 席位 文本 是否 为 你 接受 假说 提供 
了 足够 的 证据 呢 在 这种 特殊 情况 下 
答案 是 否 你 可以 很 容易 得 出 这样 
的 结论 但 使用 自动 方法 做出 正确 决策 是 
困难 的 RTE 挑战 为 竞赛者 开发 他们 的 系统 
提供 数据 但 这些 数据 对 蛮力 机器学习 技术 我们 
将 在 第 6 章 讲述 这 一 主题 来说 
是 不 够 的 因此 一些 语言学 分析 是 至关重要 
的 在 前面 的 例子 中 很 重要 的 一点 
是 让 系统 知道 Sandra Goudie 是 假设 中被 击败 
的 人 而 不是 文本 中 击败 别人 的 人 
思考 下面 的 文本 假设 对 这是 任务 困难性 的 
另一个 例证 a . Text David Golinkin is the editor 
or author of 18 books and over 150 responsa articles 
sermons and books b . Hypothesis Golinkin has written 18 
books 为了 确定 假说 是否 得到 文本 的 支持 该 
系统 需要 以下 背景 知识 一 如果 有人 是 一 
本书 的 作者 那么 他 / 她 写了 这本书 二 
如果 有人 是 一 本书 的 编辑 那么 他 / 
她 完全 没有 写 这本书 三 如果 有人 是 18 
本书 的 编辑 或 作者 则 无法 断定 他 / 
她 是 18 本书 的 作者 NLP 的 局限性 尽管 
在 很多 如 RTE 这样 的 任务 中 研究 取得 
了 进展 但在 现实 世界 的 应用 中 已经 部署 
的 语言 理解 系统 仍 不能 进行 常识推理 或以 一种 
一般 的 可靠 的 方式 描绘 这个 世界 的 知识 
我们 在 等待 这些 困难 的 人工智能 问题 得到 解决 
的 同时 接受/v 一些/m 在/p 推理/v 和/c 知识/v 能力/n 上/f 
存在/v 严重/a 限制/v 的/uj 自然/d 语言/n 系统/n 是/v 有/v 必要/d 
的/uj 因此 从一/nr 开始 自然语言 处理 研究 的 一个 重要 
目标 一直 是 使用 浅显 但 强大 的 技术 代替 
无边无际 的 知识 和 推理 能力 促进 构建 语言 理解 
技术 的 艰巨 任务 的 不断 取得 进展 事实上 这是 
本书 的 目标 之一 我们/r 希望/v 你/r 能/v 掌握/v 这些/r 
知识/v 和/c 技能/n 构建 有效 的 自然 语言 处理 系统 
并为 构建 智能 机器 这一 长期 的 理想 做出 贡献 
marginwidth = 0 marginheight = 0 src = http / 
/ www . zealware . com / csdnblog01 . html 
frameborder = 0 width = 728 scrolling = no height 
= 90 自然语言 处理 能够 把 全网 内容 组织 到 
什么 程度 Zhengyun 发表于 创业 + 社区 2007 03 27 
23 23 40 我 的 要求 是 不 需要 任何 
推动力 用户 不 需要 做 任何 输入 或 搜索 社区 
内 就 已经 围绕着 细粒度 的 话题 展开 了 结果 
我们 做到 的 自然 语言 处理 后的/nr 主题 收敛性 很强 
哈哈 随手 举个 例子 推荐 转载 如此 令人 恶心 的 
三亚 今年 春节 我们 在 三亚 的 惊魂 遭遇 作者 
倾城 2007 03 25 16 20 04 XXX 自动 计算 
相关 博主 论点 三亚 制订 旅游 整治 方案 欲 让 
99% 游客 满意 市长/n 向/p 游客/n 道歉/v 是/v 网络/n 媒体/n 
和/c 草根/n 的/uj 胜利/vn 三亚 是否 真的 如此 令人恶心 又一 
中国 高官 道歉 事件 如此 让 人 恶心 的 三亚 
一文 作者 其实 三亚 政府 应该 追究 你 的 责任 
谁要 保护 游客 的 安全 市长/n 向/p 游客/n 道歉/v 显示/v 
新/a 媒体/n 和/c 草根/n 的/uj 力量/n 阅读 这个 话题 讨论 
例子 二 推荐 娱乐场所 实名制 管理 不止 一 石三鸟 作者 
诗情 碧霄 2007 03 25 17 23 57 XXX 自动 
计算 相关 博主 论点 时评 实名制 不是 万金油 欢场 实名制 
是个 好东西 娱乐场所 实行 实名制 还有 谁 再去 消费 小姐 
不是 小姐 翠花 也 不叫 翠花 阅读 这个 话题 讨论 
这 两个 例子 是 社会 民生 自动 分类 的 我们 
再 来看 明星 演艺 分类 的 例子 推荐 外国人 才艺 
大赛 出 意外 变脸 失误 选手 下跪 作者 王伟 的 
BLOG 2007 03 25 01 34 14 XXX 自动 计算 
相关 博主 论点 黑人 小伙 表演 变脸 失误 泪流满面 下跪 
全球 博客 文摘 精典 周刊 和谐 世界 老外 参加 央视 
节目 变脸 失败 下跪 痛哭 道歉 我 为此 感动 和 
鼓掌 不断 而 我们 的 优秀 的 传统 文化 礼仪 
和 精神 又上 哪里 去了 瞬间 的 感动 这样 的 
道歉 真是 精品 阅读 这个 话题 讨论 这些 都是/nr 机器 
自己 没有 第一 推动力 情况 下 自行 计算 的 结果 
社会上 有 一个 热点 我们 就 自动 计算 出来 了 
并 灌输 到 社区 里 所以能 整合 全网 内容 百度 
贴吧 毕竟 还是 有人 输入 了 搜索 关键词 从而 形成 
第一 推动力 的 我们 不 需要 就像 我 经常 说 
的 一句话 百度 Google 是 通过 用户 搜索 输入 的 
关键词 来 判断 中文 世界 的 热点 而 我们 通过 
分析 全网 写作 的 文章 来 寻找 热点 的 通过 
这种 主题 自动 发现 技术 可以 很容易 知道 最近 中文 
世界 人们 在 讨论 什么 在 关注 什么 下面 举 
几个 长 一点 的 例子 推荐 博文 港 选 特首 
一场 只动 眼 不 动手 的 选举 转 自 BBC 
中文网 作者   TheTwoDogs 2007 03 25 15 58 47 
XXXX 自动 计算 相关 博主 论点 举 选     
为什么 内地人 对 香港 特首 选举 漠不关心     新华网 
曾荫权 当选 香港 第三 任 行政 长官     我 
期待 的 晚年 生活     香港 特首 选举 结束 
了 . . . . . .     曾荫权 
的 高票当选 显示 一 国二 制 强大 的 生命力   
  更多 要闻 曾荫权 高票 连任 香港 行政 长官   
  民主 需要 秩序 一个 合理 平稳 的 选举 和 
权力交接 是 政治 民主 的 保证     分析 曾 
荫 权和梁/nr 家 杰 以后 的 路 转 自 BBC 
中文网     为 防泄密 港 特 首 选 举 
拆 闭路电视 转 自 BBC 中文网     年轻 没有 
失败     换届     曾荫权 在 香港 第三 
任 行政 长官 选举 中 以 高票 胜出     
香港 特别 行政区 第三 任 行政 长官 选举 揭晓 现任 
行政 长官 曾荫权 获得 649票 以 超过 八成 的 得票率 
胜出     曾荫权 当选 香港特区 第三 届 行政 长官 
    曾荫权 当选 新 特首     曾荫权   
            阅读 这个 话题 讨论 
推荐 博文 重庆 钉子户 给 政府 出难题 作者   蔡律/nr 
http / / cailv . bokee . com / 2007 
03 25 09 45 08 XX 自动 计算 相关 博主 
论点 根除 野蛮 拆迁 的 契机 已经 降临     
随笔     搬迁 最后 期限 已过 钉子户 仍 钉在 
孤岛 上     长平 最 牛 钉子户 的 举动 
真牛     2007 3 24 一种 拆迁 两种 命运 
钉子户 是 谁 眼里 的 钉子 var stattitle = 一种 
拆迁 两种 命运 钉子户 是 谁 眼里 的 钉子 今天 
重庆 将 成为 众 媒体 记者 网友 和 百姓 聚焦 
之地 07 全国 两会 刚 结束 在 笔者 地 脑海 
里 一直 有 一组 强烈 的 声音 在 回荡 权力 
过于 集中 造成     贪婪 无耻 的 最 牛 
钉子户     国旗 你 不 应该 成为 做秀 的 
工具     钉子 何以 成 钉     不 
接受 法庭 判决 挂起 国旗 和 标语 重庆 钉子户 给 
政府 出难题           阅读 这个 话题 
讨论 Trackback http / / tb . blog . csdn 
. net / TrackBack . aspx PostId = 1543390 自然语言 
处理 Natural Language Processing NLP 是 人工 智能 和 语言学 
领域 的 学科 分支 它 研究 实现 人 与 计算机 
之间 使用 自然 语言 进行 有效 通信 的 各种 理论 
和 方法 词 嵌入 前面 介绍 过 处理 文本 序列 
时 通常用 建立 字典 后以 one hot 的 形式 表示 
某个 词 进而 表示 某个 句子 的 方法 这种 表示 
方法 孤立 了 每个 词 无法 表现 各个 词 之间 
的 相关性 满足 不了 NLP 的 要求 词 嵌入 Word 
Embedding 是 NLP 中 语言 模型 与 表征 学习 技术 
的 统称 概念 上 而言 它 是 指 把 一个 
维数 为 所有 词 的 数量 的 高维空间 one hot 
形式 表示 的 词 嵌入 到 一个 维数 低 得多 
的 连续 向量空间 中 每个 单词 或 词组 被 映射 
为 实 数域 上 的 向量 如上 图中 各 列 
分别 组成 的 向量 是 词 嵌入 后 获得 的 
第一 行 中 几个 词 的 词 向量 的 一部分 
这些 向量 中的 值 可 代表 该词 与 第一 列 
中 几个 词 的 相关 程度 使用 2008年 van der 
Maaten 和 Hinton 在 论文 Visualizing Data using t SNE 
中 提出 的 t SNE 数据 可视化 算法 将 词 
嵌入 后 获得 的 一些 词 向量 进行 非线性 降 
维 可到 下面 的 映射 结果 其中 可 发现 各 
词 根据 它们 的 语义 及 相关 程度 分别 汇聚 
在 了 一起 对 大量 词汇 进行 词 嵌入 后 
获得 的 词 向量 可 用来 完成 命名 实体 识别 
Named Entity Recognition 等 任务 其中 可 充分 结合 迁移 
学习 以 降低 学习 成本 提高效率 好比 前面 讲过 的 
用 Siamese 网络 进行 人脸识别 过程 使 用词 嵌入 方法 
获得 的 词 向量 可实现 词汇 的 类比 及 相似 
度 度量 例如 给定 对应 关系 男性 Man 对 女性 
Woman 要求 机 器类 比出 国王 King 对应 的 词汇 
通过 上面 的 表格 可 发现 词 向量 存在 数学 
关系 Man Woman ≈ ≈ \ approx King Queen 也 
可以 从 可视化 结果 中 看出 男性 Man 到 女性 
女性 的 向量 与 国王 King 到 王后 Queen 的 
向量 相似 词 嵌入 具有 的 这种 特性 在 2013年 
Mikolov 等 发表 的 论文 Linguistic Regularities in Continuous Space 
Word Representations 中 提出 成为 词 嵌入 领域 具有 显著 
影响力 的 研究 成果 上述 思想 可写 成 一个 余弦 
cos 相似 度 函数 sim u v = uTv ∣ 
∣ u ∣ ∣ 2 ∣ ∣ v ∣ ∣ 
2sim u v = uTv ∣ ∣ u ∣ ∣ 
2 ∣ ∣ v ∣ ∣ 2sim u v = 
\ frac { u ^ T v } { \ 
mid \ mid u \ mid \ mid _ 2 
\ mid \ mid v \ mid \ mid _ 
2 } 以此 度 量词 向量 的 相似 度 词 
嵌入 方法 词 嵌入 的 方法 包括 人工神经网络 对 词语 
同现 矩阵 降 维 概率模型 以及 单词 所在 上下文 的 
显 式 表示 等 以 词汇 的 one hot 形式 
作为 输入 不同 的 词 嵌入 方法 能以/nr 不同 的 
方式 学习 到 一个 嵌入 矩阵 Embedding Matrix 最后 输出 
某个 词 的 词 向量 将 字典 中 位置 为 
iii 的 词 以 one hot 形式 表示 为 oioio 
_ i 嵌入 矩阵 用 EEE 表示 词 嵌入 后 
生成 的 词 向量 用 eieie _ i 表示 则 
三者 存在 数学 关系 E ⋅ oi = eiE ⋅ 
oi = eiE \ cdot o _ i = e 
_ i 例如 字典 中 包含 10000个 词 每个 词 
的 one hot 形式 就是 个 大小 为 10000 × 
110000 × 110000 \ times 1 的 列 向量 采用 
某 种方法 学习 到 的 嵌入 矩阵 大小 为 300 
× 10000300 × 10000300 \ times 10000 的话 将 生成 
大小 为 300 × 1300 × 1300 \ times 1 
的 词 向量 神经 概率 语言 模型 采用 神经网络 建立 
语言 模型 是 学习 词 嵌入 的 有效 方法 之一 
2003年 Bengio 等人 的 经典之作 A Neural Probabilistic Language Model 
中 提出 的 神经 概率 语言 模型 是 早期 最 
成功 的 词 嵌入 方法 之一 模型 中 构建 了 
了 一个 能够 通过 上下文 来 预测 未知 词 的 
神经 网络 在 训练 这个 语言 模型 的 同时 学习 
词 嵌入 例如 将 下 图中 上面 的 句子 作为 
下面 的 神经 网络 的 输入 经过 隐藏 层 后 
最后 经 Softmax 将 输出 预测 结果 其中 的 嵌入 
矩阵 EEE 与 www bbb 一样 是 该 网络 中 
的 参数 需 通过 训练 得到 训练 过程 中 取 
语料库 中的 某些 词 作为 目标 词 以 目标 词 
的 部分 上下文 作为 输入 训练 网络 输出 的 预测 
结果 为 目标 词 得到 了 嵌入 矩阵 就能 通过 
前面 所述 的 数学 关系式 求得 词 嵌入 后的词/nr 向量 
Word2VecWord2Vec Word To Vectors 是 现在 最 常用 最 流行 
的 词 嵌入 算法 它 由 2013年 由 Mikolov 等人 
在 论文 Efficient Estimation of Word Representations in Vector Space 
中 提出 Word2Vec 中的 Skip Gram 模型 所做 的 是 
在 语料库 中 选定 某个 词 Context 随后 在 该词 
的 正负 10个 词 距 内 取 一些 目标 词 
Target 与之 配对 构造 一个 用 Context 预测 输出 为 
Target 的 监督 学习 问题 训练 一个 如 下图 结构 
的 网络 该 网络 仅有 一个 Softmax 单元 输出 Context 
下 Target 出现 的 条件概率 p t ∣ c = 
exp θ Ttec ∑ mj = 1exp θ Tjec p 
t ∣ c = exp θ tTec ∑ j = 
1mexp θ jTec p t \ mid c = \ 
frac { exp \ theta _ t ^ T e 
_ c } { \ sum _ { j = 
1 } ^ m exp \ theta _ j ^ 
T e _ c } 上式 中 θ t θ 
t \ theta _ t 是 一个 与 输出 的 
Target 有关 的 参数 其中 省略 了 用以 纠正 偏差 
的 参数 训练 过程 中 还是 用 交叉 熵 损失 
函数 选定 的 Context 是 常见 或不 常见 的 词 
将 影响 到 训练 结果 在 实际 中 Context 并 
不是 单纯 地 通过 在 语料库 均匀 随机 采样 得到 
而是 采用 了 一些 策略 来 平衡选择 Word2Vec 中 还有 
一种 CBOW Continuous Bag of Words Model 模型 它 的 
工作 方式 是 采样 上下文 中的 词 来 预测 中间 
的 词 与 Skip Gram 相反 以上 方法 的 Softmax 
单元 中 产生 的 计算 量 往往 过大 改进 方法 
之一 是 使用 分级 Softmax 分类器 Hierarchical Softmax Classifier 采用 
霍夫曼 树 Huffman Tree 来 代替 隐藏 层 到 输出 
Softmax 层 的 映射 此外 Word2Vec 的 作者 在 后续 
论文 Distributed Representations of Words and Phrases and their Compositionality 
中 提出 了 负 采样 Negative Sampling 模型 进一步 改进 
和 简化 了 词 嵌入 方法 负 采样 模型 中 
构造 了 一个 预测 给定 的 单词 是否 为 一对 
Context Target 的 新 监督 学习 问题 采用 的 网络 
结构 和 前面 类似 训练 过程 中 从 语料库 中 
选定 Context 输入 的 词 为 一对 Context Target 则 
标签 设置 为 1 另外 任取 kkk 对 非 Context 
Target 作为 负 样本 标签 设置 为 0 只有 较少 
的 训练 数据 kkk 的 值 取 5 ~ 20 
的话 能达到 比较 好 的 效果 拥有 大量 训练 数据 
kkk 的 取值 取 2 ~ 5 较为 合适 原 
网络 中的 Softmax 变成 多个 Sigmoid 单元 输出 Context Target 
c t 对 为 正 样本 y = 1y = 
1y = 1 的 概率 p y = 1 ∣ 
c t = σ θ Ttec p y = 1 
∣ c t = σ θ tTec p y = 
1 \ mid c t = \ sigma \ theta 
_ t ^ T e _ c 其中 的 θ 
t θ t \ theta _ t ecece _ c 
分别 代表 Target 及 Context 的 词 向量 通过 这种 
方法 将 之前 的 一个 复杂 的 多分 类 问题 
变成 了 多个 简单 的 二分 类 问题 而 降低 
计算成本 模型 中 还 包含 了 对 负 样本 的 
采样 算法 从 本质 上 来说 选择 某个 单词 来 
作为 负 样本 的 概率 取决于 它 出现 频率 对于 
更 经常 出现 的 单词 将 更 倾向 于 选择 
它 为 负 样本 但 这样 会 导致 一些 极端 
的 情况 模型 中 采用 一下 公式 来 计算 选择 
某个 词 作为 负 样本 的 概率 p wi = 
f wi 34 ∑ mj = 0f wj 34p wi 
= f wi 34 ∑ j = 0mf wj 34p 
w _ i = \ frac { f w _ 
i ^ { \ frac { 3 } { 4 
} } } { \ sum _ { j = 
0 } ^ m f w _ j ^ { 
\ frac { 3 } { 4 } } } 
其中 f wi f wi f w _ i 代表 
语料库 中 单词 wiwiw _ i 出现 的 频率 GloVeGloVe 
Global Vectors 是 另一种 现在 流行 的 词 嵌入 算法 
它 在 2014年 由 Pennington 等人 在 论文 GloVe Global 
Vectors for Word Representation 中 提出 Glove 模型 中 首先 
基于 语料库 统计 了 词 的 共 现 矩阵 XXX 
XXX 中的 元素 为 Xi jXi jX _ { i 
j } 表示 整个 语料库 中 单词 iii 和 单词 
jjj 彼此 接近 的 频率 也 就是 它 们 共同 
出现 在 一个 窗口 中的 次数 之后 要做 的 就是 
优化 以下 代价 函数 J = ∑ i jNf Xi 
j θ Tiej + bi + bj − log Xi 
j 2J = ∑ i jNf Xi j θ iTej 
+ bi + bj − log Xi j 2J = 
\ sum _ { i j } ^ N f 
X _ { i j } \ theta _ i 
^ T e _ j + b _ i + 
b _ j log X _ { i j } 
^ 2 其中 θ i θ i \ theta _ 
i ejeje _ j 分 是 单词 iii 和 单词 
jjj 的 词 向量 bibib _ i bjbjb _ j 
是 两个 偏差 项 f f f 是 一个 用 
以 防止 Xi j = 0Xi j = 0X _ 
{ i j } = 0时 log Xi j log 
Xi j log X _ { i j } 无解 
的 权重 函数 词汇表 的 大小 为 NNN 以上 优化 
函数 的 推导 过程 见 参考资料 中的 理解 GloVe 模型 
最后 要 说明 的 是 使用 各种 词 嵌入 方法 
学习 到 的 词 向量 并不 像 最开始 介绍 词 
嵌入 时 展示 的 表格 中 Man Woman King Queen 
的 词 向量 那样 其中 的 值 能够 代表 着 
与 Gender Royal 等 词 的 的 相关 程度 实际上 
它 们 大都 超出 了 人们 的 能够 理解 范围 
词 嵌入 应用 情感 分类器 NLP 中的 情感 分类 是 
对 某段 文字 中 所 表达 的 情感 做出 分类 
它 能在 很多 个 方面 得到 应用 训练 情感 分类 
模型 时 面临 的 挑战 之一 可能 是 标 记好 
的 训练 数据 不 够多 然而 有了 词 嵌入 得到 
的 词 向量 只需要 中等 数量 的 标记 好 的 
训练 数据 就能 构建 出 一个 表现 出色 的 情感 
分类器 如上图 要 训练 一个 将 左边 的 餐厅 评价 
转换 为 右边 评价 所属 星级 的 情感 分类器 也 
就是 实现 xxx 到 yyy 的 映射 有了 用词 嵌入 
方法 获得 的 嵌入 矩阵 EEE 一种 简单 的 实现 
方法 如下 方法 中 计算出 句中 每个 单词 的 词 
向量 后 取 这些 词 向量 的 平均值 输入 一个 
Softmax 单元 输出 预测 结果 这种 简单 的 方法 适用 
于 任何 长度 的 评价 但 忽略 了 词 的 
顺序 对于 某些 包含 多个 正面 评价 词 的 负面 
评价 很容易 预测 到 错误 结果 采用 RNN 能 实现 
一个 表现 更加 出色 的 情感 分类器 此时 构建 的 
模型 如下 这 是 一个 多对一 结构 的 循环 神经网络 
每个 词 的 词 向量 作为 网络 的 输入 由 
Softmax 输出 结果 由于 词 向量 是从 一个 大型 的 
语料库 中 获得 的 这种 方法 将 保证 了 词 
的 顺序 的 同时 能够 对 一些 词 作出 泛化 
词 嵌入 除 偏在 词 嵌入 过程 中 所 使用 
的 语料库 中 往往 会 存在 一些 性别 种族 年龄 
性取向 等 方面 的 偏见 从而 导致 获得 的 词 
向量 中 也 包含 这些 偏见 比如 使用 未 除 
偏 的 词 嵌入 结果 进行 词汇 类比 时 男性 
Man 对 程序员 Computer Programmer 将 得到 类似 女性 Woman 
对 家务 料 理人 Homemaker 的 性别 偏见 结果 2016年 
Bolukbasi 等人 在 论文 Man is to Computer Programmer as 
Woman is to Homemaker Debiasing Word Embeddings 中 提出 了 
一些 消除 词 嵌入 中的 偏见 的 方法 这里 列举 
消除 词 向量 存在 的 性别 偏见 的 过程 来 
说明 这些 方法 摘自 第二周 课后 作业 1 . 中和 
本身 与 性别 无关 词汇 中和 Neutralize 医生 doctor 老师 
teacher 接待员 receptionist 等 本身 与 性别 无关 词汇 中的 
偏见 首先 计算 g = ewoman − emang = ewoman 
− emang = e _ { woman } e _ 
{ man } 用 女性 woman 的 词 向量 减去 
男性 man 的 词 向量 得到 的 向量 ggg 就 
代表 了 性别 gender 假设 现有 的 词 向量 维数 
为 50 那么 对 某个 词 向量 将 50 维空间 
分成 两个 部分 与 性别 相关 的 方向 ggg 和与/nr 
ggg 正交 的 其他 49个 维度 g ⊥ g ⊥ 
g _ { \ perp } 如 下左图 除 偏 
的 步骤 是 将要 除 偏 的 词 向量 左图 
中的 e r e c e p t i o 
n i s t e r e c e p 
t i o n i s t e _ { 
receptionist } 在 向量 ggg 方 向上 的 值 置 
为 000 变成 右图 所示 的 e d e b 
i a s e d r e c e p 
t i o n i s t e r e 
c e p t i o n i s t 
d e b i a s e d e _ 
{ receptionist } ^ { debiased } 所用 的 公式 
如下 ebiascomponent = e ⋅ g | | g | 
| 22 × gecomponentbias = e ⋅ g | | 
g | | 22 × ge ^ { bias } 
_ { component } = \ frac { e \ 
cdot g } { | | g | | _ 
2 ^ 2 } \ times g e d e 
b i a s e d r e c e 
p t i o n i s t = e 
− ebias _ c o m p o n e 
n t e r e c e p t i 
o n i s t d e b i a 
s e d = e − ebias _ componente _ 
{ receptionist } ^ { debiased } = e e 
^ { bias } \ _ { component } 2 
. 均衡 本身 与 性别 有关 词汇 对 男演员 actor 
女演员 actress 爷爷 grandfather 等 本身 与 性别 有关 词汇 
如 下左图 假设 女演员 actress 的 词 向 量比 男演员 
actor 更 靠近 于 婴儿 看护人 babysit 中和 婴儿 看护人 
babysit 中 存在 的 性别 偏见 后 还是 无法 保证 
它 到 女演员 actress 与 到 男演员 actor 的 距离 
相等 对一对 这样 的 词 除 偏 的 过程 是 
均衡 Equalization 它们 的 性别 属性 均衡 过程 的 核心 
思想 是 确保 一 对词 actor 和 actress 到 g 
⊥ g ⊥ g _ { \ perp } 的 
距离 相等 的 同时 也 确保 了 它们 到 除 
偏后 的 某个 词 babysit 的 距离 相等 如 上右图 
对 需要 除 偏 的 一对 词 w1w1w1 w2w2w2 选定 
与 它们 相关 的 某个 未 中和 偏见 的 单词 
BBB 之后 均衡 偏见 的 过程 如下 公式 μ = 
ew1 + ew22 μ = ew1 + ew22 \ mu 
= \ frac { e _ { w1 } + 
e _ { w2 } } { 2 } μ 
B = μ ⋅ bias _ axis | | bias 
_ axis | | 22 × bias _ axis μ 
B = μ ⋅ bias _ axis | | bias 
_ axis | | 22 × bias _ axis \ 
mu _ { B } = \ frac { \ 
mu \ cdot \ text { bias _ axis } 
} { | | \ text { bias _ axis 
} | | _ 2 ^ 2 } \ times 
\ text { bias _ axis } μ ⊥ = 
μ − μ B μ ⊥ = μ − μ 
B \ mu _ { \ perp } = \ 
mu \ mu _ { B } ew1B = ew1 
⋅ bias _ axis | | bias _ axis | 
| 22 × bias _ axisew1B = ew1 ⋅ bias 
_ axis | | bias _ axis | | 22 
× bias _ axise _ { w1B } = \ 
frac { e _ { w1 } \ cdot \ 
text { bias _ axis } } { | | 
\ text { bias _ axis } | | _ 
2 ^ 2 } \ times \ text { bias 
_ axis } ew2B = ew2 ⋅ bias _ axis 
| | bias _ axis | | 22 × bias 
_ axisew2B = ew2 ⋅ bias _ axis | | 
bias _ axis | | 22 × bias _ axise 
_ { w2B } = \ frac { e _ 
{ w2 } \ cdot \ text { bias _ 
axis } } { | | \ text { bias 
_ axis } | | _ 2 ^ 2 } 
\ times \ text { bias _ axis } ecorrectedw1B 
= | 1 − | | μ ⊥ | | 
22 | − − − − − − − − 
− √ × ew1B − μ B | | ew1 
− μ ⊥ − μ B | | 2ew1Bcorrected = 
| 1 − | | μ ⊥ | | 22 
| × ew1B − μ B | | ew1 − 
μ ⊥ − μ B | | 2e _ { 
w1B } ^ { corrected } = \ sqrt { 
| { 1 | | \ mu _ { \ 
perp } | | ^ 2 _ 2 } | 
} \ times \ frac { e _ { \ 
text { w1B } } \ mu _ B } 
{ | | e _ { w1 } \ mu 
_ { \ perp } \ mu _ B | 
| _ 2 } ecorrectedw2B = | 1 − | 
| μ ⊥ | | 22 | − − − 
− − − − − − √ × ew2B − 
μ B | | ew1 − μ ⊥ − μ 
B | | 2ew2Bcorrected = | 1 − | | 
μ ⊥ | | 22 | × ew2B − μ 
B | | ew1 − μ ⊥ − μ B 
| | 2e _ { w2B } ^ { corrected 
} = \ sqrt { | { 1 | | 
\ mu _ { \ perp } | | ^ 
2 _ 2 } | } \ times \ frac 
{ e _ { \ text { w2B } } 
\ mu _ B } { | | e _ 
{ w1 } \ mu _ { \ perp } 
\ mu _ B | | _ 2 } e1 
= ecorrectedw1B + μ ⊥ e1 = ew1Bcorrected + μ 
⊥ e _ 1 = e _ { w1B } 
^ { corrected } + \ mu _ { \ 
perp } e2 = ecorrectedw2B + μ ⊥ e2 = 
ew2Bcorrected + μ ⊥ e _ 2 = e _ 
{ w2B } ^ { corrected } + \ mu 
_ { \ perp } 参考资料 吴恩 达 序列 模型 
网易 云 课堂 Andrew Ng Sequence Model C o u 
r s e r a d e e p l 
e a r n i n g . aiDeep Learning 
in NLP 一 词/n 向量/n 和/c 语言/n 模型/n 从/p SNE/w 
到/v t/w SNE 再到 LargeVisword2vec 前世 今生 Word2Vec 导学 第二 
部分 负 采样 csdn 理解 GloVe 模型 csdn 课程 代码 
与 资料 GitHub 注 本文 涉及 的 图片 及 资料 
均 整理 翻 译自 Andrew Ng 的 Deep Learning 系列 
课程 版权 归其/nr 所有 翻译 整理 水平 有限 如有 不妥 
的 地方 欢迎 指出 更新 历史 * 2018 . 03.08 
完成 初稿 原文 链接 从 建模 的 角度 看 为了 
方便 计算机 处理 自然语言 可以 被 定义 为 一组 规则 
或 符号 的 集合 我们 组合 集合 中的 符号 来 
传递 各种 信息 自然语言 处理 研究 表示 语言 能力 语言 
应用 的 模型 通过 建立 计算机 框架 来 实现 这样 
的 语言 模型 并且 不断 完善 这样 的 语言 模型 
还 需要 根据 语言 模型 来 设计 各种 实用 的 
系统 并且 探讨 这些 实用 技术 的 评测 技术 从 
自然 语言 的 角度 出发 NLP 基本 可以 分为 两个 
部分 自然语言 处理 以及 自然 语言 的 生成 演化 为 
理解 和 生成 文本 的 任务 自然 语言 的 理解 
是 个 综合 的 系统工程 它 又 包含 了 很多 
细分 学科 有 代表 声音 的 音 系 学 代表 
构词法 的 词态 学 代表 语句 结构 的 句 法学 
代表 理解 的 语义 句 法学 和 语用学 音 系 
学 指代 语言 中 发音 的 系统化 组织 词态 学 
研究 单词 构成 以 及 相互 之间 的 关系 句 
法学 给定 文本 的 哪 部分 是 语法 正确 的 
语义学 给定 文本 的 含义 是 什么 语用学 文本 的 
目的 是 什么 自然语言 生成 恰恰相反 从 结构化 数据 中 
以 读取 的 方式 自动 生成 文本 该 过程 主要 
包含 三 个 阶段 文本 规划 完成 结构化 数据 中 
的 基础 内容 规划 语句 规划 从 结构化 数据 中 
组合 语句 来 表达 信息流 实现 产生 语法 通顺 的 
语句 来 表达 文本 1.2 NLP 的 研究 任务 机器翻译 
计算机 具备 将 一种 语言 翻译 成 另一种 语言 的 
能力 情感 分析 计算机 能够 判断 用户 评论 是否 积极 
智能 问答 计算机 能够 正确 回答 输入 的 问题 文摘 
生成 能够 准确 归纳 总结 并 产生 文本 摘要 文本 
分类 能够 采集 各种 文章 进行 主题 分析 从而 进行 
自动 分类 舆论 分析 能够 判断 目前 舆论 的 导向 
知识图谱 知识点 相互连接 而成 的 语义 网络 1.3 NLP 相关 
知识 的 构成 分词 segment 词 是 最小 的 能够 
独立 活动 的 有 意义 的 语言 成分 英文单词 之间 
是以 空格 作为 自然 分界符 的 而 汉语 是以 字位 
基本 的 书写 单位 词语 之间 没有 明显 的 区分 
标记 因此 中文 词语 分析 是 中文分词 的 基础 和 
关键 中文分词 常用 的 手段 是 基于 字典 的 最长 
串 匹配 据说 可以 解决 85% 的 问题 但是 歧义 
分词 很难 词性 标注 part of speech tagging 标注 的 
目的 是 表征 词 的 一种 隐藏 状态 隐藏 状态 
构成 的 转移 就 构成 了 状态 转移 序列 命名 
实体 识别 NER Named Entity Recognition 从 文本 中 识别 
具有 特定 类别 的 实体 通常 是 名词 句法分析 syntax 
parsing 往往 是 一种 基于 规则 的 专家 系统 目的 
是 解析 句子 中 各个 成分 的 依赖 关系 可以 
解决 传统 词 袋 模型 不考虑 上下文 的 问题 指代 
消解 anaphora resolution 中文 中代 词 出现 的 频率 很高 
情感 识别 emotion recognition 本质上 是 分类 问题 通常 可以 
基于 词 袋 模型 + 分类器 或者 现在 流行 的 
词 向量 模型 + RNN 经过 测试 发现 后者 比 
前者 准确率 略有 提升 纠错 correction 具体 做法 有 很多 
可以 基于 N Gram 进行 纠错 也 可以 通过 字典 
树 有限 状态机 等 方法 进行 纠错 问答 系统 QA 
system 往往 需要 语音识别 合成 自然语言 理解 知识图谱 等 多项 
技术 的 配合 才 会 实现 得 比较 好 知识结构 
NLP/w 是/v 研究/vn 人/n 和/c 机器/n 之间/f 用/p 自然/d 语言/n 
进行/v 有效/a 通信/l 的/uj 理解/v 和/c 方法/n 这 需要 很多 
跨学科 的 知识 需要 语言学 统计学 最优化 理论 机器学习 深度 
学习 以及 自然 语言 处理 相关 理论 模型 知识 做 
基础 句法 语义分析 针对 目标 句子 进行 各种 句法分析 如 
分词 词性 标记 命名 实体 识别 及 链接 句法分析 语义 
角色 识别 和 多义词 消 歧 等 关键词 抽取 抽取 
目标 文本 中 的 主要 信息 比如/v 从/p 一条/m 新闻/n 
中/f 抽取/v 关/v 机信息/n 主要 是 了解 是 谁 于 
何时 为何 对谁 做了 何事 产生 了 有 什么 结果 
涉及 实体 识别 时间 抽取 因果关系 抽取 等 多项 关键技术 
文本 挖掘 主要 包含 了 对 文本 的 聚 类 
分类 信息 抽取 摘要 情感 分析 以及 对 挖掘 的 
信息 和 知识 的 可视化 交互式 的 呈现 界面 机器翻译 
将 输入 的 源语言 文本 通过 自动 翻译 转化 为 
另一种 语言 的 文本 根据 输入 数据类型 的 不同 可 
细 分位 文本 翻译 语音 翻译 手语 翻译 图形 翻译 
等 机器翻译 从 最早 的 基于 规则 到 二十年前 的 
基于 统计 的 方法 再到 今天 的 基于 深度 学习 
编解码 的 方法 逐渐 形成 了 一套 比较 严谨 的 
方法 体系 信息检索 对 大 规模 的 文档 进行 索引 
可 简单 对 文档 中 的 词汇 赋以 不同 的 
权重 来 建立 索引 也 可使用 算法 模型 来 建立 
更加 深层 的 索引 查询 时 首先 对 输入 比 
进行 分析 然后 在 索引 里面 查找 匹配 的 候选 
文档 再 根据 一个 排序 机制 把 候选 文档 排序 
最后 输出 排序 得分 最高 的 文档 问答 系统 针对 
某个 自然 语言 表达 的 问题 由 问答 系统 给 
出 一个 精准 的 答案 需要 对 自然 语言 查询 
语句 进行 语义分析 包括 实体 链接 关系 识别 形成 逻辑 
表达式 然后 到 知识库 中 查找 可能 的 候选 答案 
并 通过 一个 排序 机制 找出 最佳 的 答案 对话 
系统 系统 通过 多 回合 对话 跟 用户 进行 聊天 
回答 完成 某项 任务 主要 涉及 用户 意图 理解 通用 
聊天 引擎 问答 引擎 对话 管理 等 技术 此外 为了 
体现 上下文 相关 要 具备 多轮 对话 能力 同时 为了 
体现 个性化 对话 系统 还 需要 基于 用户 画像 做 
个性化 回复 note C + + Boost 库 可以 使用 
编译 好 的 c + + 代码 替换 python 代码 
块 提升 代码 性能 自然语言 处理 概述 自然语言 包括 口语 
  语音 文本 是 人工智能 和 语言学 的 交叉 学科 
基于 机器学习 深度 学习 文本 实际上 包含 了 十分 丰富 
的 信息 语义 理解 推理 和 推断       
最后 的 语义 推断 是 重点 后面 的 三个 部分 
目前 就业 市场 中 NLP 的 比重 是 十分 重要 
的 想要 人工智能 发挥 作用 就 要让 计算机 理解 语言 
之上 的 内容 打开 手机 app 之后 会 发现 新闻 
媒体 资讯 类 的 app 中 呈现 的 信息 大多数 
是 文字 信息 的 大多数 信息 是由 文字 去 传递 
的 自然 语言 处理 不是 编程语言 自 言 语言 具有 
很强 的 逻辑性 比喻 性强 的 语言 双击 666 打 
call NLP 的 核心 的 问题 在 海量 数据 中 
如何 及时 提取 出 核心 的 信息 简单 的 自然 
语言 分词 在 垂直 领域 中有 简单 的 分词 操 
作文本 主体 和 标签 分类 摘要 生成 舆情 监控 搜索引擎 
语音识别 对话 机器人 学术论文 新闻 的 标注 方便 索引 翻遍 
做 推荐 自动 对 对联 的 系统     神经网络 
机器翻译 语言 映射 情感 分析 与 舆情 监控 民众 情绪 
的 变化 语音识别 应用 基于 深度 学习 聊天 机器人 客服 
机器人 深度 学习 的 流程 数据 准备 与 清洗 原始数据 
输入 与 表征 分布 学习 网络设计 与 非线性 表达 模型 
拟合 与 预测 具有 很强 的 非线性 表达能力 深度 学习 
的 使用 率 逐年 增加 基于 评论 情感 分析 的 
酒店 挑选 传统 机器学习 的 文本 表述 词 袋 模型 
1 基础 知识 以及 项目 背景 词 嵌入 算法 稠密 
表示 每个 词 映射 为 一个 超高 维度 的 向量 
2 机器 学习 的 解决方案 3 深度 学习 的 解决方案 
项目经理   完整 的 项目 经历 的 描述 1 . 
项目 背景 描述 项目 的 具体 背景   酒店 2 
. 项目数 据 项目 的 数据 是 哪里 来 的 
    飞猪 携程 3 . 数据 处理 方法 脏 
的 数据 怎么 定义 的 脏数据 是 怎么 处理 的 
类别 不 均衡 的 情况 怎么做 的 处理   以及 
一些 在 数据 预处理 怎么 表述 数据 独 热 向量 
    聚集 表示 为什么 要 使用 这些 东西 如何 
调 优 性能 如何 识别 判断 是否 是 有 问题 
的 如何 去 优化 项目 模型 选择 与 建模 过程 
评估 推测 与 效果 模型 优化 与 提升 进行 统计 
自然语言 处理 系统 梳理 学习 资料 统计 自然语言 处理 . 
宗 成庆 篇章 分析 的 最终 目标 是 从 整体 
上 理解 篇章 最 重要 的 任务 是 分析 篇章 
结构 篇章 结构 包括 语义 结构 话题 结构 指代 结构 
等 一 基本 理论 概念 依存 理论 Beaugrandeand Dressler 1981 
认为 篇章 有 7个 基本特征 衔接 性 连续性 意图 性 
信息性 可接受性 情景 性 和跨/nr 篇章 性 其中 衔接 性 
连续性 意图 性 和 信息性 对 自然 语言 产生 了 
很多 影响 言语 行为 理论 中心 理论 修辞 结构理论 脉络 
理论 篇章 表示 理论 二 篇章 衔接 性 衔接 又 
称为 外部 联结 主要 表现 为 整个 篇章 范围 内 
词汇 或 短语 之间 的 关联 当 语 篇 中 
一个 成分 的 含义 依赖于 另一个 成分 的 解释 时 
便 产生 衔接 关系 Hallidayand Hasan 1980 将 衔接 分为 
五 种 情况 一般 指代 替换 省略 连接 和 词汇 
衔接 目前 的 研究 主要 集中 在 指代 消解 方面 
和 词汇 衔接 方面 的 衔接 性 相关 研究 指代 
一般 包括 两种 情况 回 指 和共指/nr 二者 之间 有 
很大 的 交集 但 并不 严格 地 彼此 地 包含 
词汇 衔接 方面 整体 上 篇章 衔接 性 研究 还 
处于 初步 或 停留 于 问题 本身 三 篇章 连贯性 
连贯性 又称 内部 联结 主要 通过 句子 之间 的 语义 
关联 来 表示 篇章 不同 部分 之间 的 关联关系 这 
方面 主要 针对 信息性 和 意图 性 进行 展开 文本 
分类 是 机器 学习 在 自然 语言 处理 中的 最 
常用 也是 最 基础 的 应用 机器学习 相关 内容 可以 
直接 看 我 的 有关 scikit learn 相关 教程 本节 
直接 涉及 nltk 中的 机器学习 相关内容 预备 机器学习 的 过程 
是 训练 模型 和 使用 模型 的 过程 训 练就 
是 基于 已知 数据 做 统计 学习 使用 就是 用 
统计学 习好 的 模型 来 计算 未知 的 数据 机器学习/i 
分为/v 有/v 监督/vn 学习/v 和无/nr 监督/vn 学习/v 文本/n 分类/n 也/d 
分为/v 有/v 监督/vn 的/uj 分类/n 和无/nr 监督/vn 的/uj 分类/n 有 
监督 就 是 训练 的 样本 数 据有 了 确定 
的 判断 基于 这些 已 有的 判断 来 断定 新的 
数据 无 监督 就 是 训练 的 样本数据 没有什么 判断 
完全 自发 的 生成 结论 无论 监督 学习 还是 无 
监督 学习 都是/nr 通过 某种 算法 来 实现 而 这种 
算法 可以 有 多重选择 贝叶斯 就是 其中 一种 在 多种 
算法 中 如何 选择 最 适合 的 这 才是 机器学习 
最难 的 事情 也是 最高 境界 nltk 中的 贝叶斯 分类器 
贝叶斯 是 概率论 的 鼻祖 贝叶 斯定理 是 关于 随机 
事件 的 条件 概率 的 一则 定理 贝叶斯 公式 是 
P B | A = P A | B P 
B / P A 即 已知 P A | B 
P A 和P/nr B 可以 计算出 P B | A 
贝叶斯 分类器 就是 基于 贝叶斯 概率 理论 设计 的 分类器 
算法 nltk 库 中 已经 实现 具体 用法 如下 # 
encoding utf 8 import nltk my _ train _ set 
= { feature1 u a } 1 { feature1 u 
a } 2 { feature1 u a } 3 { 
feature1 u a } 3 { feature1 u b } 
2 { feature1 u b } 2 { feature1 u 
b } 2 { feature1 u b } 2 { 
feature1 u b } 2 { feature1 u b } 
2 classifier = nltk . N a i v e 
B a y e s C l a s s 
i f i e r . train my _ train 
_ set print classifier . classify { feature1 u a 
} print classifier . classify { feature1 u b } 
文档 分类 不管 是 什么 分类 最 重要 的 是 
要 知道 哪些 特征 最能 反映 这个 分类 的 特点 
也 就是 特征 选取 文档 分类 使用 的 特征 就 
最能 代表 这个 分类 的 词 因为 对 文档 分类 
要 经过 训练 和 预测 两个 过程 而 特征 的 
提取 是 这两个 过程 都 需要 的 所以 习惯 上 
我们 会 把 特征提取 单独 抽象 出来 作为 一个 公共 
方法 比如 from nltk . corpus import movie _ reviews 
all _ words = nltk . FreeDist w . lower 
for w in movie _ reviews . words word _ 
features = all _ words . keys 2000 def document 
_ features document for word in word features features contains 
% s % word = word in document _ words 
return features 这 是 一个 简单 的 特征提取 过程 前 
两行 找到 movie _ reviews 语料库 中 出现 词频 最高 
的 2000个 词作 为特征 下面 定义 的 函数 就是 特征提取 
函数 每个 特征 都是 形如 contains * 的 key value 
就是 True 或 False 表示 这个 词 是否 在 文档 
中 出现 那么 我们 训练 的 过程 就是 featuresets = 
document _ features d c for d c in documents 
classifier = nltk . N a i v e B 
a y e s C l a s s i 
f i e r . train featuresets 要 预测 一个 
新 的 文档 时 classifier . classify document _ features 
d 通过 classifier . show _ most _ informative _ 
features 5 可以 找到 最优 信息量 的 特征 这 对 
我们 选取 特征 是 非常 有 帮助 的 其他 文本 
分类 文本 分类 除了 文档 分类 外 还有 许多 其他 
类型 的 分类 比如 词性 标注 属于 一种 文本 分类 
一般 是 基于 上下文 语境 的 文本 分类 句子 分割 
属于 标点符号 的 分类 任务 它 的 特征 一般 选取 
为 单独 句子 标识符 的 合并 链表 数据 特征 下 
一个 词 是否 大写 前 一个 词 是 什么 前 
一个 词 长度 识别 对话 行为 类型 对话 行为 类型 
是 指 问候 问题 回答 断言 说明 等 识别 文字 
蕴含 即 一个 句子 是否 能 得出 另外 一个 句子 
的 结论 这 可以 认为 是 真假 标签 的 分类 
任务 这 是 一个 有 挑战 的 事情 参考 资料 
来源 http / / www . shareditor . com / 
自然语言 概念 自然语言 即 我们 人类 日常 所 使用 的 
语言 是 人类 交际 的 重要 方式 也 是 人类 
区别于 其他 动物 的 本质 特征 我们 只能 使用 自然 
语言 与 人 进行 交流 而 无法 与 计算机 进行 
交流 自然语言 处理 自然语言 处理 NLP Natural Language Processing 是 
人工智能 AI Artificial Intelligence 的 一部分 实现 人 与 计算机 
之间 的 有效 通信 自然语言 处理 属于 计算机 科学 领域 
与 人工智能 领域 其 研究 使用 计算机 编程 来 处理 
与 理解 人类 的 语言 应用 场景 自然语言 处理 具有 
非常 广泛 的 应用 场景 例如 情感 分析 机器翻译 文本 
相似 度 匹配 智能 客服 通用 技术 分词 停用词 过滤 
词干 提取 词形 还 原词 袋 模型 TF IDFWord2Vec 说明 
scikit learn 库 中 实现 的 tf idf 转换 与 
标准 的 公式 略有不同 并且 tf idf 结果 会 使用 
L2 范数 进行 规范化 处理 import numpy as np # 
对 语料库 中 出现 的 词汇 进行 词频 统计 相当于 
词 袋 模型 # 操作 方式 将 语料库 当中 出现 
的 词汇 作 为特征 将 词汇 在 当前 文档 中 
出现 的 频率 次数 # 作为 特征值 from sklearn . 
feature _ extraction . text import CountVectorizer count = CountVectorizer 
# 语料库 docs = np . array Where there is 
a will there is a way . There is no 
royal road to learning . # bag 是 一个 稀疏 
的 矩阵 因为 词 袋 模型 就 是 一种 稀疏 
的 表示 bag = count . fit _ transform docs 
# 输出 单词 与 编号 的 映射 关系 print count 
. vocabulary _ # 调用 稀疏 矩阵 的 toarray 方法 
将 稀疏 矩阵 转换 为 ndarray 对象 print bag . 
toarray { where 8 there 5 is 0 will 9 
way 7 no 2 royal 4 road 3 to 6 
learning 1 } 2 0 0 0 0 2 0 
1 1 1 1 1 1 1 1 1 1 
0 0 0 from sklearn . feature _ extraction . 
text import TfidfTransformer tfidf = TfidfTransformer print tfidf . fit 
_ transform count . fit _ transform docs . toarray 
0.53594084 0 . 0 . 0 . 0 . 0.53594084 
0 . 0.37662308 0.37662308 0.37662308 0.29017021 0.4078241 0.4078241 0.4078241 0.4078241 
0.29017021 0.4078241 0 . 0 . 0 . 评论 情感 
分析 项目 背景 ¶ 公司 活动 新闻 微博 影评 商品 
评价 等 加载 数据集 import pandas as pd import numpy 
as np data = pd . read _ csv r 
movie . csv data . head label comment 0 pos 
此 英雄 完全 自给自足 没有 任何 超能力 自己 造就 自己 
酷 科技 以人为本 发展 才 是 硬道理 1 pos 如果 
一个 男人 嫌 女人 太聪明 了 那 一定 是 因为 
他 自己 还 不够 牛逼 2 pos 这 是 一个 
会 搞笑 爱 臭屁 又 喋喋不休 的 英雄 噢 结尾 
那句 I am Iron Man 太帅了 ~ ~ 3 pos 
没想到/l 会比/i 这/r 侠/nr 那侠的/nr 都/d 要好看/a 2个 多 小时 
的 片子 并不 觉 长 紧凑 爽快 意犹未尽 4 pos 
想起 一个 人 铁臂 阿木 童./nr 数据 预处理 数据 清洗 
# 缺失 值 探索 data . isnull . sum axis 
= 0 # 异常值 探索 data label . value _ 
counts # 重复 值 # data . duplicated . sum 
# data data . duplicated data . drop _ duplicates 
inplace = True 数据 转换 将 label 与 comment 列 
转换 为数 值类型 data label = data label . map 
{ pos 1 neg 0 } data label . value 
_ counts 1 11555 0 3928 Name label dtype int64 
# 用于 进行 中文分词 的 库 安装 # pip install 
jieba import jieba import re # 获取 停用词 列表 def 
get _ stopword # 默认 情况 下 在 读取 文件 
时 双引号 会被 解析 为 特殊 的 引用 符号 双引号 
中 的 内容 会 正确 解析 但是 双引号 不会 解析 
为 文本 内容 # 在 这种 情况 下 如果 文本 
中 仅 含 有一个 双引号 会 产生 解析 错误 如果 
需要 将 双引号 作为 普通 的 字符 解析 将 quoting 
参数设置 为 3 stopword = pd . read _ csv 
r stopword . txt header = None quoting = 3 
sep = a # 转换 为 set 这样 可以 比 
list 具有 更快 的 查询 速度 return set stopword 0 
. tolist # 清洗 文本 数据 def clear text return 
re . sub \ s + \ . \ \ 
/ _ $ % ^ * + \ \ + 
| + ~ @ # ￥ % & * + 
text # 进行 分词 的 函数 def cut _ word 
text return jieba . cut text # 去掉 停用词 函数 
def remove _ stopword words # 获取 停用词 列表 stopword 
= get _ stopword return word for word in words 
if word not in stopword def preprocess text # 文本 
清洗 text = clear text # 分词 word _ iter 
= cut _ word text # 去除 停用词 word _ 
list = remove _ stopword word _ iter return . 
join word _ list # 对 文本 数据 评论 数据 
的 处理 步骤 # 1 文本 清洗 去掉 一些 特殊 
无用 的 符号 例如 @ # # 2 分词 将 
文本 分解 为 若干 单词 # 3 去除 停用词 # 
以上 步骤 通过 调用 preprocess 方法 来 实现 data comment 
= data comment . apply lambda text preprocess text # 
调用 cut 方法 可以 对 文本 进行 分词 返回 结果 
cut 方法 返回 的 是 生成器 对象 # jieba . 
cut 今天 我们 学习 自然语言 处理 # lcut 方法 返回 
的 是 列表 # jieba . lcut 今天 我们 学习 
自然语言 处理 data comment . head 0 英雄 自给自足 超能力 
造就 酷 科技 以人为本 发展 硬道理 1 男人 嫌 女人 
太 聪明 是 因为 牛 逼 2 这是 搞笑 爱 
臭屁 喋喋不休 英雄 噢 结尾 那句 IamIronMan 太帅 3 没想到 
这 侠 那侠/nr 要好看 小时 片子 不觉 长 紧凑 爽快 
意犹未尽 4 想起 铁臂 阿木 童 Name comment dtype object 
# 使用 TfidfVectorizer 来 进行 文本 向 量化 具有 一个 
局限 不足 就是 语料库 中 存在 多少 个 单词 就会 
具 有 多少 个 特征 # 这样 会 造成 特征 
矩阵 非常 庞大 矩阵 非常 稀疏 from sklearn . feature 
_ extraction . text import TfidfVectorizer tfidf = TfidfVectorizer tfidf 
. fit _ transform data comment . tolist data comment 
0 英雄 自给自足 超能力 造就 酷 科技 以人为本 发展 硬道理 
1 男人 嫌 女人 太 聪明 是 因为 牛 逼 
2 这是 搞笑 爱 臭屁 喋喋不休 英雄 噢 结尾 那句 
IamIronMan 太帅 3 没想到 这 侠 那侠/nr 要好看 小时 片子 
不觉 长 紧凑 爽快 意犹未尽 4 想起 铁臂 阿木 童 
5 个 会 飞 锅炉 6 美国 人 真 幼稚 
喜欢 大力士 7 超级 英雄 电影 差 很远 RobertDowneyJr 天才 
演员 电影 娱乐性 十足 话 . . . 8 腐朽 
堕落 资本主义 垃圾 9 爱看 商业片 动作片 科幻片 动画片 喜剧片 
10 太 拉风 装备 蝙蝠侠 摩托 拉风 11 资本主义 钢铁 
炮弹 纸老虎 12 蜘蛛侠 哈里波特 功夫 骇客帝国 攻 壳 II 
吐 死 13 真的 男生 喜欢 看吧 14 高科技 份 
15 中规中矩 超级 英雄 片 16 侠 中 算是 17 
期望值 高 不免有些 失望 当作 minitransformer 斯坦 李 作品 名单 
蜘蛛 . . . 18 唐尼 goodluck 19 复仇者 连门/nr 
回来 补 真的 片子 复仇者 联盟 啊啊啊 20 Jarvis 21 
主角 高 大帅 情节 老套 经不起 推敲 起伏跌宕 噱头 幽默 
烂 装备 代表 票房 烂片 22 完 复仇者 联盟 倒 
回来 第一部 发现 罗伯特 童孩/nr 演了 福尔摩斯 特别 喜欢 咔 
咔 . . . 23 RobertDowneyJr 24 SEXYIRONMAN 25 电影院 
看一看 图个 爽 片子 26 想 一套 钢铁 侠 铠甲 
27 想起 铁臂 阿木 童 28 视效 噱头 情节 弱 
得 要死 女一号 不错 29 不错 人造 强 . . 
. 15541 听 搞笑 研究 阶段 蛮牛 15542 影片 没什么 
兴趣 一部 超过 预期 值得 收藏 15543 迅雷 抢先 版画 
质 差 总 找个 高清 版 弥补 情节 空洞 带给 
失望 15544 昨天 钢铁 侠 感觉 不错 15545 灰 常想 
一套 钢铁 侠 衣服 15546 APPLESMOM 15547 构思 错 动作 
错 造型 很酷 故事 单调 15548 c o m i 
c c h a r a c t e r 
s u p e r h e r o fetish 
15549 爱 marvel superhero 题材 电影 ironman 心理变态 小萝卜头 糖 
泥 . . . 15550 赞 年 极力推荐 这部 15551 
喜欢 美国 漫画 接触 机会 非常少 铁人 MARVEL 二线 英雄 
电影 发现 错 仅次于 . . . 15552 好看 我会 
想起 奥特曼 15553 特效 不错 剧情 15554 美国式 英雄 场面 
不错 15555 特效 不错 男人 心中 想 飞 冲动 男人 
想 英雄 记住 喜欢 IRONMANSAY IA . . . 15556 
TONYSTARK 诙 诣 语言 不错 那句 事实上 钢铁 侠 帅 
15557 电影 里 道具 科幻 构思新颖 15558 一部 好看 superheromovie 
15559 好看 电影 特地去 电影院 花 80 块钱 15560 08 
年度 最 棒 英雄 科幻片 15561 a t y p 
i c a l H o l l e y 
w o o d b l o c k b 
u s t e r N o s u r 
p r i s e 15562 钢铁 侠 生活 完美 
男人 生活 15563 不错 想象力 主角 牛 牛 15564 每次 
美国 超级 英雄 电影 影 虫 们 失望 钢铁 侠 
剧情 不谈 确实 场 视觉 盛宴 15565 不错 特别 组装 
装甲 帅呆了 15566 诺 极力推荐 真的 失望 近期 好看 15567 
目前 为止 喜欢 超级 英雄 电影 15568 西 老公 喜欢 
15569 功夫 之王 烂片 强奸 强档 好片 回到 人间 15570 
2008 04 30cathyAMK Name comment Length 15483 dtype objectfrom sklearn 
. feature _ extraction . text import TfidfVectorizer from sklearn 
. model _ selection import train _ test _ split 
from sklearn . linear _ model import L o g 
i s t i c R e g r e 
s s i o n from sklearn . pipeline import 
Pipeline from sklearn . metrics import classification _ report X 
_ train X _ test y _ train y _ 
test = train _ test _ split data comment data 
label test _ size = 0.25 random _ state = 
0 # TfidfVectorizer 可以 看做 是 CountVectorizer 与 TfidfTransformer 两个 
类型 的 合体 tfidf = TfidfVectorizer lr = L o 
g i s t i c R e g r 
e s s i o n class _ weight = 
balanced # lr = L o g i s t 
i c R e g r e s s i 
o n steps = tfidf tfidf model lr pipe = 
Pipeline steps = steps pipe . fit X _ train 
y _ train y _ hat = pipe . predict 
X _ test print pipe . score X _ train 
y _ train print pipe . score X _ test 
y _ test print classification _ report y _ test 
y _ hat 0 . 8405959352394075 0 . 672436063032808 precision 
recall f1 score support 0 0.40 0.51 0.45 1004 1 
0.81 0.73 0.77 2867 micro avg 0.67 0.67 0.67 3871 
macro avg 0.60 0.62 0.61 3871 weighted avg 0.70 0.67 
0.68 3871print pd . Series y _ test . value 
_ counts print pd . Series y _ hat . 
value _ counts 1 2867 0 1004 Name label dtype 
int64 1 2585 0 1286 dtype int64from sklearn . ensemble 
import R a n d o m F o r 
e s t C l a s s i f 
i e r rf = R a n d o 
m F o r e s t C l a 
s s i f i e r n _ estimators 
= 100 n _ jobs = 1 class _ weight 
= balanced # rf = R a n d o 
m F o r e s t C l a 
s s i f i e r n _ estimators 
= 100 n _ jobs = 1 pipe . set 
_ params model = rf pipe . fit X _ 
train y _ train y _ hat = pipe . 
predict X _ test print pipe . score X _ 
train y _ train print pipe . score X _ 
test y _ test print classification _ report y _ 
test y _ hat 0 . 9889769204271444 0 . 706277447687936 
precision recall f1 score support 0 0.38 0.21 0.27 1004 
1 0.76 0.88 0.82 2867 micro avg 0.71 0.71 0.71 
3871 macro avg 0.57 0.55 0.54 3871 weighted avg 0.66 
0.71 0.68 3871from sklearn . ensemble import B a g 
g i n g C l a s s i 
f i e r from sklearn . tree import D 
e c i s i o n T r e 
e C l a s s i f i e 
r b = B a g g i n g 
C l a s s i f i e r 
base _ estimator = D e c i s i 
o n T r e e C l a s 
s i f i e r n _ estimators = 
10 n _ jobs = 1 pipe . set _ 
params model = b pipe . fit X _ train 
y _ train y _ hat = pipe . predict 
X _ test print pipe . score X _ train 
y _ train print pipe . score X _ test 
y _ test print classification _ report y _ test 
y _ hat 0 . 9770065449534964 0 . 7091190906742444 precision 
recall f1 score support 0 0.41 0.26 0.32 1004 1 
0.77 0.86 0.81 2867 micro avg 0.71 0.71 0.71 3871 
macro avg 0.59 0.56 0.57 3871 weighted avg 0.68 0.71 
0.69 3871from sklearn . ensemble import A d a B 
o o s t C l a s s i 
f i e r ada = A d a B 
o o s t C l a s s i 
f i e r base _ estimator = D e 
c i s i o n T r e e 
C l a s s i f i e r 
max _ depth = 1 n _ estimators = 100 
pipe . set _ params model = ada pipe . 
fit X _ train y _ train y _ hat 
= pipe . predict X _ test print pipe . 
score X _ train y _ train print pipe . 
score X _ test y _ test print classification _ 
report y _ test y _ hat 0 . 7695487426799862 
0 . 7377938517179023 precision recall f1 score support 0 0.48 
0.13 0.21 1004 1 0.76 0.95 0.84 2867 micro avg 
0.74 0.74 0.74 3871 macro avg 0.62 0.54 0.52 3871 
weighted avg 0.69 0.74 0.68 3871 人工智能 简介 人工智能 Artificial 
Intelligence 英文 简称 AI 它 是 计算机 科学 的 一个 
分支 了解 和 探索 智能 的 实质 并以 人类 智能 
相似 的 方式 做出 反应 的 智能 机器 人工智能 主要 
研究 包括 机器人 语言识别 图像识别 自然语言 处理 和 专家 系统 
等 这是 一门 极具 挑战 的 科学 从事 这项 工作 
的 人 必须 懂得 计算机 知识 它 是 研究 开发 
用于 模拟 延伸 和 扩展 人 的 智能 的 理论 
方法 技术 及 应用 系统 的 一门 新的 技术 科学 
人工智能 在 计算机 学科 领域 可以 细分 为 五大 学科 
一 机器学习 机器学习 Machine Learning ML 是 一门 多 领域 
交叉 学科 涉及 概率论 统计学 逼近 论 凸 分析 算法 
复杂度 理论 等 多门 学科 专门研究 计算机 怎样 模拟 或 
实现 人类 的 学习 行为 以 获取 新 的 知识 
或 技能 重新组织 已有 的 知识 结构 使 之 不断 
改善 自身 的 性能 它 是 人工智能 的 核心 是 
使 计算机 具有 智能 的 根本 途径 其 应用 遍及 
人工智能 的 各个 领域 它 主要 使用 归纳 综合 而 
不是 演绎 二 人机对话 人机对话 是 计算机 的 一种 工作 
方式 即 计算机 操作员 或 用户 与 计算机 之间 通过 
控制台 或 终端 显示 屏幕 以 对话 方式 进行 工作 
操作员 可用 命令 或 命令 过程 告诉 计算机 执行 某 
一 任务 三 深度 学习 深度 学习 是 机器 学习 
研究 中 的 一个 新的 领域 其 动机 在于 建立 
模拟 人脑 进行 分析 学习 的 神经 网络 它 模仿 
人脑 的 机制 来 解释 数据 例如 图像 声音 和 
文本 四 图像处理 用 计算机 对 图像 进行 分析 以 
达到 所需 结果 的 技术 五 网络爬虫 网络爬虫 是 一个 
自动 提取 网页 的 程序 它 为 搜索引擎 从 万维 
网上 下载 网页 是 搜索引擎 的 重要 组成 1 自然语言 
处理 是 一门 集 语言学 数学 计算机科学 认知科学 于 一体 
的 综合性 交叉学科 2 自然语言 处理 主要 有 两种 方法 
基于 规则 的 方法 和 基于 统计 的 方法 基于 
统计 的 方法 属于 哲学 中 的 经验 主义 主要 
采用 归纳法 同样 假设 大脑 中 存在 某些 认知 的 
能力 该 方法 和 理性主义 方法 的 区别 是 程度 
上 的 区别 而非 本质上 的 区别 基于 规则 的 
方法 属于 哲学 中 的 理性主义 主要 采用 演绎法 人们 
相信 人类 大脑 中 的 重要 知识 不 是由 感官 
得到 的 而是 提前 固定 在 头脑 中 由 遗传 
基因 决定 3 培根 主张 理性主义 与 经验主义 相结合 4 
语言 障碍 是 制约 21 世纪 社会 全球化 的 一个 
重要 因素 5 自然语言 处理 NLP 也 称为 自然语言 理解 
NLU 6 图灵测试 计算机 被 误认为 是 人 的 几率 
就 是 智能 程度 7 部分 研究 方向 机器翻译 Machine 
Translation MT 一种 语言 到 另一种 语言 自动 文摘 automatic 
summarizing / abstracting 提炼出 原 文档 的 主要 内容 信息检索 
information retrieval IR 从 海量 文档 中 找到 符合 用户 
要求 的 相关 文档 面向 多 语言 的 信息 检索 
叫做 跨语言 信息检索 cross language IR 文档 分类 document categorization 
/ classification 也称 文本 分类 信息 分类 把 大量 的 
文档 按照 一定 的 标准 进行 自动 归类 问答 系统 
question answering system 计算机 对 人 提出 的 问题 进行 
理解 推理 分析 在 有关 的 知识 资源 中 自动 
求解 答案 并 回答 与 其他 技术 构成 人机对话 系统 
human computer dialogue system 文字 编辑 和 自动 校对 对 
文字 的 拼写 用词 语法 格式 等 检查 校对 编排 
信息 过滤 自动 过滤 那些 满足 特定 要求 条件 的 
文档 主要 用于 信息 安全 和 防护 文字 识别 对 
印刷体 或 手写体 文字 进行 自动 识别 将其 转换 成 
计算机 可以 处理 的 电子 文本 其 主要 内容 属于 
字符 图像识别 语音识别 把 语音信号 转换成 书面形式 文语/nr 转换 将 
文本 转换成 语音 又叫 语音合成 说 话人 识别 / 验证 
对 一个 人 说话 的 言语 样本 进行 声学 分析 
推断 说 话人 的 身份 8 自然语言 形态学 语法学 语义学 
语用学 形态学 研究 词 的 内部 结构 包括 屈折 变化 
和 构词法 语法学 研究 句 子结构 成分 之间 的 关系 
中心 就是 为什么 一句话 可以 这么 说 也 可以 那么 
说 语义学 研究 语言 各级 单位 的 意义 以及 语义 
与 语音 语法 修辞 文字 语境 哲学思想 社会 环境 个人 
修养 的 关系 等等 中心 就是 这个 语言 单位 到底 
说 了 什么 语用学 涉及 方面 较多 中心 就是 为什么 
在 特定 的 上下 文中 要说 这 句话 9 面临 
的 问题 1 歧义 消解 问题 无论 在 词法 层次 
句法 层次 语义 层次 语用 层次 无论 哪 类 语言 
单位 其 歧义 性 始终 都是/nr 困扰 人们 实现 应用 
目标 的 根本 问题 例句 1 put the block in 
the box on the table . 可以 是 理解 为 
1             put the block 
in the box on the table 2       
      put the block in the box on 
the table 实际上 英 文中 歧义 结构 分析 结果 的 
数量 是 随着 借此 短语 数目 的 增加 呈 指数 
上升 的 其 歧义 组合 的 复杂 程度 随着 借此 
短语 个数 的 增加 不断 加深 这个 歧义 结构 的 
组合 数 成为 开 塔兰 数 Catalan numbers 记作 Cn 
如果 句子 中 存在 n 个 介词短语 那么 Cn 可以 
表示 为 汉语 尽管 不像 英语 那样 由于 多个 介词 
结构 成分 而 导致 大量 歧义 但是 汉语 中 也 
普遍 存在 有 歧义 现象 例句 2 喜欢 乡下 的 
孩子 例句 3 关于 鲁迅 的 著作 这都是 句法 歧义 
而 词汇 的 词类 歧义 词义 歧义 句子 的 语义 
歧义 也 同样 是 NPL 中 普遍 存在 的 现象 
例句 4 我 的 头像 牛逼 吗 例句 5 今天 
中午 吃 食堂 例句 6 火烧 圆明园 与 驴肉火烧 例句 
7 打鼓 打架 打球 打电话 打毛衣 打伞 一打 铅笔 自打 
今天 起 例句 8 他 说 她 这个 人 真 
有意思 funny 她说 他 这个 人 也怪 有意思 的 funny 
人们 以为 他俩 有了 意思 wish 就让 他 向她 意思 
意思 express 他 急了 我 根本 没 那个 意思 thought 
她 也 生气 了 你们 这么 说 是 什么 意思 
intention 有人 觉得 这个 段子 很 有意思 funny 但是 也 
有人 觉得 这个 段子 并 没有 意思 sense 当然 像 
这个 段子 中 这么 复杂 的 用词 方法 在 实际 
生活 中 几乎 没有 人 使用 这个 段子 的 目的 
呢 只是 说明 自然 语言 中 的 歧义 是 普遍 
存在 的 现象 并 不是 说 一个 自然 语言 处理 
系统 必须 具备 如此 复杂 的 歧义 消解 能力 才能 
算得 上 是 真正 实用 的 系统 2 未知 语言 
现象 处理 随着 社会 的 发展 新的 词汇 新的 词义 
新的 用法 新的 句子 用法 都在 不断出现 例如 9 元芳 
你 怎么 看 例如 10 灌水 盖楼 沙发 童鞋 盆 
友 驴友 10 实践证明 除了 语音 识别 和 机器 翻译 
以外 很多 自然语言 处理 的 研究 任务 包括 汉语 自动 
分词 和 词性 标注 文字 识别 拼音 法 汉字输入 等 
都/d 可以/c 用/p 噪声/n 信道/n 模型/n 来/v 描述/v 和/c 实现/v 
11 研究 现状 1 很多 技术 已经 达到 或者 基本 
达到 实用 程度 例如 文字 输入 编辑 排版 文字 识别 
电子词典 语音合成 2 许多 新的 研究 方向 不断 出现 受 
实际应用 驱动 将 NLP 技术 与 其他 相关 技术 融合 
用于 研究 和 开发 更多 实用 的 技术 例如 网络 
内容管理 网络 信息 监控 有害信息 过滤 等 这些 技术 不仅 
与 NLP 技术 密切相关 还 设计 图像处理 情感 计算 网络 
技术 等 多种 技术 此外 还有 语音 自动 翻译 语音 
自动 文摘 语音 检索 基于 图像 内容 及 文字 说明 
的 图像 理解 技术 研究 3 许多 理论 问题 尚未 
得到 根本性 解决 整个 NLP 领域 也 尚未 建立 起 
一套 完整 系统 的 理论 框架 体系 很多 方法 已经 
得到 实际 应用 比如 上下文 无关 问法 HMM 噪声 信道 
模型 等 很多 重要 问题 也 尚未 彻底 有效 的 
解决 例如 语义 的 形式化 与 计算 问题 句法分析 问题 
指代 歧义 消解 问题 汉语 自动 分词 中的 未 登录 
词 识别 问题 等 新建 AipNlp AipNlp 是 自然 语言 
处理 的 Python SDK 客户端 为 使用 自然 语言 处理 
的 开发 人员 提供 了 一系列 的 交互 方法 参考 
如下 代码 新建 一个 AipNlp from aip import AipNlp 你 
的 APPID AK SK APP _ ID = # # 
# # # # # # # # # 你 
的 APP ID API _ KEY = # # # 
# # # # # # # # 你 的 
Api key SECRET _ KEY = # # # # 
# # # # # # # 你 的 Secret 
key client = AipNlp APP _ ID API _ KEY 
SECRET _ KEY 配置 AipNlp 如果 用户 需要 配置 AipNlp 
的 网络 请求 参数 一般 不 需要 配置 可以 在 
构造 AipNlp 之后 调用 接口 设置 参数 目前 只 支持 
以下 参数 接口 说明 s e t C o n 
n e c t i o n T i m 
e o u t I n M i l l 
i s 建立 连接 的 超时 时间 单位 毫秒 s 
e t o c k e t T i m 
e o u t I n M i l l 
i s 通过 打开 的 连接 传输 数据 的 超时 
时间 单位 毫秒 接口 说明 词 法分析 词 法分析 接口 
向 用户 提供 分词 词性 标注 专名 识别 三 大 
功能 能够 识别 出 文本 串 中的 基本词汇 分词 对 
这些 词汇 进行 重组 标注 组合 后 词汇 的 词性 
并 进一步 识别 出 命名 实体 text = 百度 是 
一家 高科技 公司 调 用词 法分析 client . lexer text 
{ log _ id 3 1 7 4 1 7 
9 6 8 3 1 0 2 5 6 1 
6 2 2 text 百度 是 一家 高科技 公司 items 
{ loc _ details byte _ offset 0 uri pos 
ne ORG item 百度 basic _ words 百度 byte _ 
length 4 formal } { loc _ details byte _ 
offset 4 uri pos v ne item 是 basic _ 
words 是 byte _ length 2 formal } { loc 
_ details byte _ offset 6 uri pos m ne 
item 一家 basic _ words 一 家 byte _ length 
4 formal } { loc _ details byte _ offset 
10 uri pos n ne item 高科技 basic _ words 
高 科技 byte _ length 6 formal } { loc 
_ details byte _ offset 16 uri pos n ne 
item 公司 basic _ words 公司 byte _ length 4 
formal } } 词 法分析 定制 版 text = 百度 
是 一家 高科技 公司 调 用词 法分析 定制 版 client 
. lexerCustom text { log _ id 1 0 3 
0 6 8 7 2 7 3 1 4 6 
3 8 4 7 5 8 items { loc _ 
details byte _ offset 0 uri ne ORG basic _ 
words 百度 item 百度 pos byte _ length 4 formal 
} { loc _ details byte _ offset 4 uri 
ne basic _ words 是 item 是 pos v byte 
_ length 2 formal } { loc _ details byte 
_ offset 6 uri ne basic _ words 一 家 
item 一家 pos m byte _ length 4 formal } 
{ loc _ details byte _ offset 10 uri ne 
basic _ words 高 科技 item 高科技 pos n byte 
_ length 6 formal } { loc _ details byte 
_ offset 16 uri ne basic _ words 公司 item 
公司 pos n byte _ length 4 formal } text 
百度 是 一家 高科技 公司 } 依存 句法分析 依存 句法分析 
接口 可 自动 分析 文本 中 的 依存 句法结构 信息 
哦 拥 句子 中词 与 词 之间 的 依存 关系 
来 表示 词语 的 句法结构 信息 如 主谓 动宾 定 
中 等 结构 关系 并用 树状 结构 来 表示 整句 
的 结构 如 主谓宾 定 状 补 等 text = 
今天天气 怎么样 调用 依存 句法分析 client . depParser text 如果 
有 可选 参数 options = { } options mode = 
1 带 参数 调用 依存 句法分析 client . depParser text 
options { log _ id 6 7 3 8 9 
4 7 3 7 6 0 1 1 8 3 
9 6 7 0 text 今天天气 怎么样 items { postag 
t head 2 word 今天 id 1 deprel ATT } 
{ postag n head 3 word 天气 id 2 deprel 
SBV } { postag r head 0 word 怎么样 id 
3 deprel HED } } 词 向量 表示 词 向量 
表示 接口 提供 中文 词 向量 的 查询 功能 word 
= 张飞 调 用词 向量 表示 client . wordEmbedding word 
{ log _ id 1 6 9 6 6 5 
6 2 4 8 5 1 4 3 3 8 
9 0 2 word 张飞 vec 0.290384 0.276273 0.302719 0.7209 
0.108958 0.553115 0.0877021 0.359806 0.0880146 0.189588 0.244222 0.0651301 0.0638421 0.533272 
0.00821664 0.0375696 0.327892 0.46532 0.865607 0.623493 0.178252 0.0400714 0.25975 0.11109 
0.0953429 0.101911 0.535927 0.0933478 0.601825 0.321298 0.631975 0.0875886 0.870735 0.269735 
0.585102 0.319081 0.184684 0.720537 0.383718 0.0765072 0.31901 0.270633 0.795086 0.203823 
0.125412 0.45416 0.172919 0.295541 0.216173 0.430564 0.0180166 0.138979 0.277238 0.741072 
0.190484 0.030923 0.0943274 0.591492 0.418138 0.523783 0.227849 0.366404 0.443689 0.125983 
0.0810465 0.40937 0.1809 0.391663 0.184682 0.176599 0.296323 0.263794 0.148703 0.121896 
0.267335 0.20897 0.000618858 0.258487 0.284275 0.115589 0.28355 0.150706 0.220889 0.591039 
0.0290777 0.201643 0.0797944 0.488941 0.831331 0.379756 0.139497 0.2703 0.504657 0.440968 
0.1447 0.110457 0.0163559 0.767792 0.491371 0.549788 0.205589 0.362547 0.445447 0.114256 
0.390303 0.355757 0.35865 0.309228 0.0702368 0.0218542 0.20673 0.18002 0.0739457 0.230891 
0.014336 0.18294 0.660368 0.771709 0.210481 0.366585 0.487737 0.392698 0.165913 0.0634584 
0.327222 0.170312 0.16333 0.0126046 0.139614 0.41918 0.151494 0.317118 0.391317 0.673394 
0.430471 0.0830508 0.270076 0.336409 0.218263 0.417467 0.595822 0.114509 0.323514 0.405187 
0.144482 0.179517 0.185674 0.161061 0.0338107 0.290429 0.187511 0.131024 0.0655593 0.0429835 
0.249348 0.470223 0.439866 0.191249 0.551478 0.0530808 0.220113 0.21264 0.4053 0.000986318 
0.431895 0.266691 0.387755 0.176948 0.790972 0.186954 0.311339 0.847612 0.0591855 0.217022 
0.40963 0.0388994 0.258638 0.0700524 0.517052 0.0738539 0.0278234 0.0207165 0.64623 0.397078 
0.512611 0.240432 0.631851 0.266089 0.23193 0.335795 0.48978 0.101472 0.112899 0.0119656 
0.205143 0.59687 0.139228 0.2366 0.0448019 0.463323 0.136911 0.245667 0.531107 0.203959 
0.437006 0.0385832 0.475222 0.152122 0.183256 0.147781 0.976636 0.268798 0.0467436 0.398612 
0.726595 0.0641848 0.442981 0.392992 0.277279 0.191023 0.540712 0.041807 0.521223 0.494714 
0.114315 0.623037 0.503307 0.16223 0.0109138 0.0030869 0.0127418 0.0324629 0.257331 0.724175 
0.071035 0.293041 0.142676 0.216268 0.217721 0.150594 0.524261 0.136377 0.26703 0.143736 
0.377088 0.0852308 0.248864 0.2864 0.336949 0.0106289 0.142447 0.0830073 0.00827009 0.170654 
0.0537858 0.66666 0.167388 0.00478372 0.370992 0.420722 0.0163072 0.224316 0.900274 0.0618271 
0.0933983 0.138376 0.0352047 0.133874 0.274968 0.1037 0.056145 0.283046 0.222181 0.0843009 
0.201509 0.0759472 0.430465 0.279714 0.0762712 0.0291045 0.0666021 0.389999 0.0268815 0.35655 
0.167335 0.555981 0.277015 0.370779 0.249201 0.153099 0.15063 0.59068 0.144961 0.36857 
0.38433 0.627967 0.460143 0.207135 0.270095 0.175896 0.132773 0.260412 0.0316362 0.511945 
0.014644 0.338383 0.513172 0.273772 0.245957 0.484812 0.479638 0.781593 0.692486 0.269043 
0.48944 0.151724 0.109521 0.0716606 0.454819 0.641453 0.28264 0.0844294 0.0127063 0.0473483 
0.0599927 0.0715608 0.562256 0.215818 0.207625 0.0960898 0.0344254 0.0852497 0.119984 0.296039 
0.595229 0.253829 0.111723 0.411277 0.101737 0.0322796 0.345638 0.0965107 0.083087 0.291633 
0.091778 0.0279783 0.108174 0.300271 0.541914 0.197143 0.631338 0.479441 0.0369768 0.451288 
0.127012 0.639879 0.0512995 0.273387 0.418342 0.45064 0.1239 0.595654 0.31378 0.35008 
0.0134738 0.476063 0.0309964 0.0264222 0.4704 0.201462 0.967353 0.0587739 0.221851 0.221493 
0.319194 0.321394 0.176416 0.0173751 0.0174415 0.339173 0.0516278 0.255842 0.283161 0.017094 
0.138473 0.271638 0.496162 0.519359 0.00602108 0.459303 0.295921 0.27062 0.753482 0.0583323 
0.181312 0.106313 0.646242 0.00311025 0.163957 0.182659 0.0996339 0.272461 0.301206 0.35085 
0.37463 0.155242 0.281236 0.294234 0.00533482 0.00310824 0.0731524 0.394956 0.452704 0.000153456 
0.0800992 0.0785606 0.439399 0.575366 0.216206 0.212303 0.624662 0.0487097 0.15867 0.278319 
0.21006 0.786678 0.23844 0.189342 0.108299 0.511393 0.405482 0.161949 0.212671 0.379168 
0.0637337 0.13583 0.0522022 0.072762 0.11513 0.647886 0.112957 0.147099 0.156163 0.127035 
0.145647 0.182698 0.482085 0.0702394 0.0172681 0.24563 0.0392392 0.491031 0.19934 0.132408 
0.285179 0.40498 0.134263 0.262012 0.142867 0.147229 0.268257 0.1726 0.476211 0.836967 
0.568796 0.077607 0.510508 0.0675741 0.681589 0.100888 0.326709 0.266345 0.397411 0.644215 
0.13274 0.354817 0.558334 0.114178 0.0940336 0.235152 0.554642 0.382976 0.274543 0.105513 
0.409024 0.0281389 0.350335 0.773656 0.602614 0.0406916 0.566817 0.100671 0.0793555 0.176259 
0.218086 0.654524 0.109966 0.157835 0.214399 0.166806 0.297687 0.526347 0.330715 0.223834 
0.354683 0.164879 0.060529 0.208646 0.347635 0.386788 0.434064 0.448538 0.106584 0.137211 
0.821776 0.448596 0.55277 0.486275 0.0597583 0.108438 0.0167387 0.205475 0.367478 0.0528088 
0.191489 0.308181 0.124091 0.0241138 0.332369 0.418433 0.609042 0.564987 0.0275926 0.190715 
0.114899 0.0137452 0.00163973 0.0747787 0.219737 0.0336625 0.0256406 0.14083 0.0510848 0.280421 
0.0751052 0.195839 0.217633 0.110681 0.692188 0.516287 0.0406127 0.514706 0.461349 0.31112 
0.505281 0.209302 0.478191 0.159178 0.262902 0.215158 0.0384547 0.0301001 0.68696 0.333097 
0.387189 0.397549 0.389793 0.326927 0.426165 0.249444 0.287807 0.358692 0.344935 0.22274 
0.12828 0.0673532 0.0972766 0.227617 0.248091 0.0705791 0.63178 0.759731 0.368149 0.578806 
0.280523 0.0312885 0.516321 0.308148 0.463663 1.11399 0.299133 0.324969 0.0922515 0.223782 
0.0757393 0.0956187 0.307651 0.274788 0.495276 0.305883 0.0228269 0.437532 0.260021 0.36529 
0.122708 0.175827 0.146148 0.143242 0.142164 0.0918094 0.415535 0.0301366 0.295545 0.618801 
0.175826 0.756559 0.128965 0.0491931 0.733814 0.0347257 0.460981 0.540235 0.138612 0.353038 
0.0671316 0.0149887 0.503586 0.0874566 0.441919 0.0776407 0.272449 0.0997288 0.44766 0.216144 
0.00963199 0.0527866 0.0218697 0.180018 0.164696 0.724876 0.136289 0.225619 0.161481 0.165889 
0.857903 0.15784 0.186857 0.662843 0.558884 0.0192077 0.00818205 0.0243429 0.217057 0.455544 
0.00163086 0.466992 0.113344 0.174208 0.251834 0.0775733 0.102453 0.258227 0.145805 0.00610516 
0.173767 0.129026 0.132582 0.148301 0.458603 0.367434 0.382593 0.116882 0.0928457 0.276499 
0.180621 0.351536 0.353009 0.31789 0.0245226 0.189822 0.705618 0.0623819 0.68237 0.027945 
0.0396841 0.081132 0.414828 0.251657 0.193545 0.0149343 0.0925272 0.12489 0.458534 0.55974 
0.277349 0.113657 0.574713 0.198563 0.905217 0.101096 0.0367823 0.120045 0.278173 0.191525 
0.0414615 0.105125 0.78052 0.448668 0.30789 0.497319 0.398035 0.55494 0.272399 0.102899 
0.281833 0.262621 0.138731 0.444618 0.497306 0.275449 0.0123345 0.120426 0.491484 0.402516 
0.288962 0.387392 0.144125 0.838843 0.236083 0.227957 0.418015 0.510442 0.0841282 0.544343 
0.0525509 0.0398014 0.381329 0.281488 0.403923 0.210186 0.53414 0.0852807 0.345891 0.294183 
1.17415 0.023307 0.828112 0.0523113 0.0824572 0.317031 0.543952 0.699134 0.278506 0.576854 
0.434733 0.267847 0.570456 0.017377 0.645807 0.917205 0.441665 0.393248 0.0631595 0.386241 
0.0413631 0.0191933 0.474338 0.113288 0.400757 0.0247571 0.348845 0.0123555 0.25809 0.427283 
0.245173 0.294317 0.159206 0.118759 0.273828 0.643573 0.0927131 0.265129 0.233232 0.138332 
0.136015 0.673727 0.684253 0.0585586 0.327816 0.716404 0.58116 0.0275417 0.0388521 0.0237589 
0.277684 0.0602299 0.209622 0.0348703 0.327143 0.24981 0.251077 0.455329 0.396863 0.0570048 
0.265072 0.0683558 0.0132361 0.273579 0.366049 0.615134 0.103124 0.481334 0.746339 0.0640788 
0.484396 0.00114065 0.366753 0.0240541 0.439156 0.159546 0.0506753 0.0468946 0.43076 0.602602 
0.0107401 1.19797 0.44314 0.698443 0.336827 0.0258312 0.172399 0.305746 0.150144 0.0203008 
0.326867 0.644517 0.0156665 0.13351 0.23441 0.293748 0.0695886 0.477291 0.281291 0.755484 
0.74025 0.552702 0.381103 0.164566 0.15145 0.728736 0.448275 0.0725737 0.116212 0.210402 
0.691626 0.0265872 0.448584 0.244172 0.245309 0.139035 0.0288716 0.364476 0.0426868 0.21928 
0.742586 0.0932949 0.193005 0.0303013 0.76493 0.0455655 0.608174 0.255099 0.0151615 0.0139608 
0.0158675 0.3893 0.373225 0.250462 0.0276716 0.0752877 0.0127418 0.435184 0.0627005 0.400453 
0.147969 0.235518 0.181853 0.339577 0.553451 0.00837407 0.248918 0.136399 0.354747 0.350052 
0.220699 0.183795 0.784734 0.395384 0.315588 0.0276707 0.0840118 0.254402 0.0226935 0.483695 
0.075312 0.402732 0.0151023 0.166692 0.65539 0.467999 0.192916 0.429285 0.349553 0.626268 
0.153931 0.0643198 0.292859 0.156136 0.064216 0.0490229 0.147063 0.151404 0.701247 0.0486219 
0.0359798 0.307433 0.254073 0.0960998 0.386864 0.100606 0.0278402 0.27646 0.373706 0.244237 
0.445031 0.0736471 0.681565 0.361913 0.107957 0.0310045 0.0797901 0.0512583 0.560119 0.0451696 
0.112058 0.010503 0.456464 0.180504 0.187385 0.492449 0.0517042 0.269497 0.0741519 0.134895 
0.102614 0.0668148 0.498746 0.386095 0.131642 0.208304 0.0341324 0.151889 0.341949 0.0420371 
0.116241 0.440811 0.108852 0.134327 0.0777457 0.488344 0.0472591 0.697291 0.580174 0.101828 
0.131381 0.192425 0.317998 0.122801 0.0694366 0.21801 0.0429734 0.136425 0.437184 0.11753 
0.344893 0.24043 0.0306901 0.422333 0.146097 0.520181 0.0972754 0.186103 0.0766742 0.745162 
0.364611 0.186148 0.250859 0.243429 0.251991 0.424686 . . . } 
DNN 语言 模型 中文 DNN 语言 模型 接口 用于 输出 
切 词 结果 并 给出 每个 词 在 句子 中的 
概率值 判断 一句话 是否 符合 语言表达 习惯 text = 床 
前 明月光 调用 DNN 语言 模型 client . dnnlm text 
{ log _ id 8 4 6 1 8 9 
3 4 9 8 4 1 0 1 6 2 
9 0 2 text 床 前 明月光 items { word 
床 prob 3.85273 e 05 } { word 前 prob 
0.0289018 } { word 明月 prob 0.0284406 } { word 
光 prob 0.808029 } ppl 79.0651 } 词意 相似 度 
输入 两个 词 得到 两个 词 的 相似 度 结果 
word1 = 北京 word2 = 上海 调用 词义 相似 度 
client . wordSimEmbedding word1 word2 如果 有 可选 参数 options 
= { } options mode = 0 带 参数 调用 
词义 相似 度 client . wordSimEmbedding word1 word2 options { 
log _ id 1 8 4 1 0 6 2 
0 6 3 0 6 9 4 9 0 9 
3 4 error _ code 282004 error _ msg invalid 
parameter s } 短 文本 相似 度 text1 = 浙 
富 股份 text2 = 万事通 自考网 调用 短 文本 相似 
度 client . simnet text1 text2 如果 有 可选 参数 
options = { } options model = CNN 带 参数 
调用 短 文本 相似 度 client . simnet text1 text2 
options { log _ id 8 7 5 9 6 
1 3 9 6 1 9 6 6 5 8 
5 0 4 6 texts { text _ 2 万事通 
自考网 text _ 1 浙 富 股份 } score 0.0549339 
} 评论 观点 抽取 评论 观点 抽取 接口 用来 提取 
一 条 评论 句子 的 关注点 和 评论 观点 并 
输出 评论 观点 标签 以及 评论 观点 极性 text = 
三星 电脑 电池 不给力 调用 评论 观点 抽取 client . 
commentTag text 如果 有 可选 参数 options = { } 
options type = 13 带 参数 调用 评论 观点 抽取 
client . commentTag text options { log _ id 8 
4 2 6 9 2 3 8 2 6 3 
7 8 1 6 4 6 3 0 items { 
sentiment 0 abstract 三星 电脑 span 电池 不给力 / span 
prop 电池 begin _ pos 8 end _ pos 18 
adj 不给力 } } 情感 倾向 分析 对 包含 主观 
观点 信息 的 文本 进行 情感 极性 类别 积极 消极 
中性 的 判断 并给 出 相应 的 置信度 text = 
苹果 是 一家 伟大 公司 调用 情感 倾向 分析 client 
. s e n t i m e n t 
C l a s s i f y text { 
log _ id 7 4 1 5 4 8 7 
4 6 2 1 2 5 0 7 8 5 
8 2 text 苹果 是 一家 伟大 公司 items { 
positive _ prob 0.691839 confidence 0.315198 negative _ prob 0.308161 
sentiment 2 } } 文章 标签 文章 标签 服务 能够 
针对 网络 各类 媒体 文章 进行 快速 的 内容 理解 
根据 输入 含有 标题 的 文章 输出 多个 内容 标签 
以及 对应 的 置信度 用于 个性化 推荐 相似 文章 聚合 
文本 内容 分析 等 场景 title = iphone 手机 出现 
白苹果 原因 及 解决 办法 用 苹果 手机 的 可以 
看下 content = 如果 下面 的 方法 还是 没有 解决 
你 的 问题 建议 来 我们 门店 看下 成都市 锦江区 
红星路 三段 99号 银石 广场 24层 01室 调用 文章 标签 
client . keyword title content { log _ id 4 
3 1 3 9 0 9 1 3 2 9 
9 6 8 8 8 0 2 2 items { 
score 0.99775 tag iphone } { score 0.862602 tag 手机 
} { score 0.845657 tag 苹果 } { score 0.837886 
tag 苹果公司 } { score 0.811601 tag 白苹果 } { 
score 0.797911 tag 数码 } } 文章 分类 对 文章 
按照 内容 类型 进行 自动 分类 首批 支持 娱乐 体育 
科技 等 26个 主流 内容 类型 文本 内容 分析 等 
应用 提供 基础 技术 支持 title = 欧洲 冠军杯 足球赛 
content = 欧洲 冠军联赛 是 欧洲 足球 协会 联盟 主办 
的 年度 足球比赛 代表 欧洲 俱乐部 足球 最高 荣誉 和 
水平 被 认为 是 全世界 最 高素质 最具 影响力 以及 
最高 水平 的 俱乐部 赛事 亦 是 世界 上 奖金 
最高 的 足球 赛事 和 体育 赛事 之一 调用 文章 
分类 client . topic title content { log _ id 
2 2 0 7 1 8 7 7 2 9 
1 9 6 3 8 0 1 1 8 item 
{ lv2 _ tag _ list { score 0.915631 tag 
足球 } { score 0.803507 tag 国际足球 } { score 
0.77813 tag 英超 } lv1 _ tag _ list { 
score 0.830915 tag 体育 } } } 文本 纠错 识别 
输入 文本 中 有 错误 的 片段 提示 错误 并 
给出 正确 的 文本 结果 支持 短 文本 长 文本 
语音 等 内容 的 错误 识别 纠错 是 搜索引擎 语音识别 
内容 审查 等 功能 更好 运行 的 基础 模块 之一 
text = 百度 是 一家 仁 工 智能 公司 调用 
文本 纠错 client . ecnet text { log _ id 
4 8 1 9 2 6 8 2 7 1 
3 6 0 2 7 1 5 7 4 item 
{ vec _ fragment { ori _ frag 仁 工 
begin _ pos 10 correct _ frag 人工 end _ 
pos 14 } score 0.529867 correct _ query 百度 是 
一家 人工智能 公司 } text 百度 是 一家 仁 工 
智能 公司 } 对话 情绪 识别 接口 针对 用户 日常 
沟通 文本 背后 所 蕴含 情绪 的 一种 直观 检测 
可 自动 识别 出 当前 会 话者 所 表现 出 
的 情绪 类别 及其 置信度 可以 帮助 企业 更 全面 
地 把握 产品 服务 质量 监控 客户 服务 质量 text 
= 本来 今天 高高兴兴 调用 对话 情绪 识别 接口 client 
. emotion text 如果 有 可选 参数 options = { 
} options scene = talk 带 参数 调用 对话 情绪 
识别 接口 client . emotion text options { log _ 
id 9 0 1 8 5 6 6 0 0 
5 2 1 5 1 2 6 9 4 text 
本来 今天 高高兴兴 items { subitems { prob 0.501008 label 
happy } replies 你 的 笑声 真 欢乐 prob 0.501008 
label optimistic } { subitems replies prob 0.49872 label neutral 
} { subitems replies prob 0.000272128 label pessimistic } } 
新闻 摘要 接口 自动 抽取 新闻 文本 中 的 关键 
信息 进而 生成 指定 长度 的 新闻 摘要 content = 
麻省 理工学院 的 研究 团队 为 无人机 在 仓库 中 
使用 RFID 技术 进行 库存 查找 等 工作 创造 了 
一种 . . . maxSummaryLen = 300 调用 新闻 摘要 
接口 client . newsSummary content maxSummaryLen 如果 有 可选 参数 
options = { } options title = 标题 带 参数 
调用 新闻 摘要 接口 client . newsSummary content maxSummaryLen options 
{ error _ code 6 error _ msg No permission 
to access data } 原文 来源   机器人 圈 概要 
在 自然 语言 处理 方面 的 研究 已经 延续 了 
五十 多年 而 随着 计算机 的 兴起 它 的 发展 
也 早已 超出 了 语言学 的 范畴 提起 AI 你 
可能会 不假思索 的 想到 自然语言 处理 人脸识别 无人 驾驶 等 
那么 你 对 这些 真的 了解 吗 接下来 我们 就 
以 自然 语言 处理 为例 来 仔细 说一说 自然语言 处理 
Natural Language Processing 简称 NLP 广义 上 定义 为 通过 
软件 对 诸如 语音 和 文本 这样 的 自然 语言 
进行 自动 操作 在 自然 语言 处理 方面 的 研究 
已经 延续 了 五十 多年 而 随着 计算机 的 兴起 
它 的 发展 也 早已 超出 了 语言学 的 范畴 
而 当 你 读完 本篇 文章 之后 你 就会 明白 
什么 是 自然 语言 处理 以及 它 为什么 那么 重要 
• 什么 是 自然 语言 它 与 其他 类型 的 
数据 有 什么 不同 • 什么 使得 处理 自然 语言 
的 工作 变得 如此 具有 挑战性 • NLP 的 领域 
来自 哪里 现代 从业者 是 如何 对 其 进行 界定 
的 现在 我们 就 来 深入 探索 一下 自然语言 自然 
语言 是 指 我们 作为 人类 来说 人与 人 之间 
相互 沟通 的 方式 即 语音 和 文本 而 我们 
无时不刻 地 处在 文本 的 包围 之中 想象 一下 你 
每天 将 会 看到 多少 文本 • 符号 • 菜单 
• 电子邮件 • 短信 • 网页 • 还有 更多 甚至 
可以 说 这个 名单 是 无休无止 的 现在 我们 先 
来 考虑 一下 语音 人类 作为 一个 物种 我们 可以 
相互 交流 而这 要 远远 超过 我们 所 需要 进行 
的 写作 我们 不得不 承认 相对 写作 来说 学会 说话 
要 简单 得多 语音 和 文字 是 我们 进行 相互 
交流 的 方式 鉴于 这种 类型 的 数据 的 重要性 
我们/r 必须/d 有/v 方法/n 来/v 了解/v 和/c 理解/v 自然语言/l 就像 
我们 对 其他 类型 的 数据 一样 自然 语言 的 
挑战 使用 自然 语言 数据 所 存在 的 问题 还 
没有 得到 解决 该 领域 的 专家 已经 对 其 
进行 了 半个 多 世纪 的 研究 但 必须 要 
承认 它 真的 很难 对于 那些 必须 花 费好 多年 
来 学习 获得 一门 语言 的 孩子 来说 这是 很难 
的 对于 一个 学习 语言 的 成年 人 来说 这是 
很难 的 对于 那些 尝试 建模 的 科学家 来说 这是 
很难 的 同样 对于 那些 尝试 构建 处理 自然语言 输入 
或 输出 系统 的 工程师 来说 这也 是 很难 的 
这些 任务 是 如此 的 困难 以至于 图灵 将 可以 
用 自然 语言 进行 流畅 的 交流 作为 他 对 
智能 测试 的 核心 2010年 数学 语言学 第 248页 自然语言 
之所以 难 主要 是 因为 它 很 混乱 几乎 没有 
规则 可循 不过 大 部分 的 时间 里 我们 而已 
很容易 地 彼此 相互理解 人类 语言 是 非常 模糊 的 
它 也 在 不断 变化 和 演变 从古至今 人类 在 
创造 语言 理解 语言 方面 是 非常 擅长 的 并且 
能够 表达 感知/v 和/c 解读/v 非常/d 精细/a 和/c 细微/a 的/uj 
含义/n 同时 虽然 我们 人类 是 语言 使用 的 极大 
群体 者 但 我们 在 形式 理解 和 描述 用以 
管理 语言 的 规则 方面 还 存在 一定 的 欠缺 
2017年 自然语言 处理 中 的 神经 网络 方法 第 1页 
从 语言 学到 自然语言 处理 语言学 语言学 是 对 语言 
进行 的 科学研究 包括 语法 语义学 和 语音学 古典 语言学 
涉及 到 语言 规则 的 设计 和 评估 在 语法 
和 语义学 的 形式 方法 上 取得 了 很大 的 
进步 但在 大多数 情况 下 自然语言 理解 中 存在 的 
很多 有趣 的 问题 遏制 了 清晰 的 数学 形式 
广义 上 来说 语言学家 可以 是 学习 语言 的 任何 
人 但 更 通俗 地 说 一个 以 语言学家 自居 
的 人 可能 更 侧重于 此 领域 之外 的 领域 
数学 是 科学 的 工具 从事 自然语言 工作 的 数学家 
可能 将 他们 的 研究 称 为 数学 语言学 仅/d 
专注/v 于/p 离散数学/n 形式学/i 的/uj 使用/v 和/c 自然/d 语言/n 理论/n 
例如 形式语言 和 自动机 理论 计算 语言学 计算 语言学 是 
使用 计算机 科学 工具 对 语言学 进行 的 现代 研究 
昨天 的 语言 学 可能 是 今天 的 计算 语言学家 
因为 计算 工具 的 使用 和 思考 方式 的 改变 
已然 跨越 了 研究 的 大多数 领域 计算 语言学 是 
对 理解 和 产生 自然 语言 的 计算机 系统 的 
研究 计算 语言学 的 一个 本质 上 的 功能 将 
是 对 理论 语言学家 提出 的 语法 进行 测试 1986年 
计算 语言学 导言 第 4 5页 大 数据 和 计算机 
的 发展 意味着 通过 编写 和 运行 软件 可以 从 
大量 文本 数据集 中 发现 新的 不同 的 事物 在 
20 世纪 90 年代 统计 方法 和 统计 机器学习 开始 
盛行 并 最终 取代 了 经典 的 自上而下 的 基于 
规则 的 语言 方法 而这 主要 得益于 它们 结果 的 
优良性 速度 的 快捷 性 以及 鲁棒性 现在 研究 自然 
语言 的 统计 方法 主导 了 这一 领域 它 可以 
定义 这个 领域 现如今 数据 驱动 的 自然 语言 处理 
方法 受到 了 广泛 的 欢迎 以至于 被 认定 是 
计算 语言学 的 主流 方法 导致 这 一 发展 的 
一个 强有力 的 因素 无疑 是 可用 的 电子 存储 
数据 的 增加 量 从而 为 这些 处理 方法 的 
应用 提供 了 充足 的 数据 量 由于 其 观察 
到 的 脆性 另 一个 因素 可能 是 在 看到 
现存 方法 的 脆弱性 之后 对 过分 依赖 手工 制动 
规则 的 觉醒 2005年 牛津 计算 语言学 手册 第 358页 
自然 语言 的 统计 方法 不仅 限于 统计 本身 而且 
还 包括 用于 应用/nr 机器 学习 中 的 高级 推理方法 
理解 自然语言 并 不是 一件 简单 的 事情 这 需要 
大量 的 关于 形态学 语法 语义 和 语用学 知识 以及 
对 世界 的 普遍 认识 获取/v 和对/nr 所有/b 这些/r 知识/v 
进行/v 编码/n 是/v 开发/v 具有/v 良好/a 有效性/n 和/c 鲁棒性/nr 的/uj 
语言/n 系统/n 的/uj 根本/a 障碍/n 之一/r 就像 统计 方法 一样 
机器学习 方法 并 没有 做到 这 一点 即从 带有 注释 
或 未 注释 的 语言 语料库 中 自动 获取 这种 
知识 2005年 牛津 计算 语言学 手册 第 377页 统计 自然语言 
处理 计算 语言学 也 被 称为 自然语言 处理 或 NLP 
以 反映 统计 方法 的 更为 基于 工程师 或 经验 
的 方法 性 一面 该 领域 的 统计 优势 还 
常常 导致 NLP 被 描述 为 统计 自然语言 处理 也许 
是 为了 将其 与 经典 计算 语言学 方法 区别 开来 
我 认为 计算 语言学 既 具有 科学 的 一面 又 
具有 工程学 的 一面 称为 工程学 的 这 一面 通常 
称为 自然语言 处理 NLP 主要 涉及 构建 计算 工具 以便 
使用 语言 做 有用 的 事情 例如 机器翻译 总结 问答 
等 与 任何 工程 学科 一样 自然语言 处理 也 涵盖 
了 各种 不同 的 科学 学科 2009年 统计 变革 是 
如何 改变 计算 语言学 的 语言学 是 一个 很大 的 
研究 课题 虽然 NLP 的 统计学 方法 在 某些 领域 
取得 了 巨大 的 成功 但从 传统 的 自上而下 的 
方法 来看 仍然/d 有/v 很大/a 的/uj 空间/n 和/c 巨大/a 的/uj 
收益/n 粗略地 说 统计 NLP 将 概率 与 在 分析 
话语 或 文本 过程 中 遇到 的 替代 方案 相关联 
并将 最 可能 的 结果 接受 为 正确 的 结果 
. . . . . . 毫不 奇怪的是 词语 的 
名称 现象 在 世界 上 都是 密切 相关 的 或者 
我们 对 它 的 认知 在 关于 世界 的 事实 
反映 在 文本 的 一些 模糊 事实上 经常 彼此 接近 
这个 观点 有 很大 的 争论 空间 2005年 牛津 计算 
语言学 手册 第 19页 自然语言 处理 作为 对 处理 文本 
数据 感兴趣 的 机器学习 从业者 我们 关注 自然语言 处理 领域 
中 的 工具 和 方法 在 前面 的 内容 中 
我们 已经 看到 了 从 语言 学到 NLP 的 路径 
现在 我们 来 看看 现代 研究 人员 和 从业 人员 
如何 定义 NLP 的 所有 内容 在 这一 领域 最 
顶尖 研究 人员 撰写 的 教科书 中 他们 将 这个 
学科 称为 语言 科学 允许 讨论 古典 语言学 和 现代 
统计学 方法 语言 科学 的 目的 是 能够 描述 和 
解释 围绕 在 我们 周围 的 大量 语言 观察 在 
对话 写作 和 其他 媒体 中 其中 一 部分 与 
人类 获取 产生 和 理解 语言 的 认知 范围 有关 
一 部分 与 理解 语言 话语 与 世界 的 关系 
有关 一 部分 与 了解 用 哪种 语言 沟通 的 
语言 结构 有关 1999年 统计 自然语言 处理 基础 第 3页 
他们 通过 在 自然 语言 处理 中 使用 统计 方法 
继续 关注 推理 过程 统计 NLP 旨在 对 自然 语言 
领域 进行 统计 推理 统计 推理 通常 包括 采取 一些 
数据 根据 一些 未知 概率分布 生成 然后 对 该 分布 
进行 一些 推断   1999年 统计 自然语言 处理 基础 第 
191页 在 应用 自然语言 处理 的 文本 中 作者 NLP 
的 知名 NLPK Python 库 的 贡献者 将其 广泛 描述 
为 使用 计算机 来 处理 自然语言 数据 我们 将 采用 
自然 语言 处理 简称 NLP 涵盖 了 对 自然 语言 
任何 类型 的 计算机 操作 一方面 它 可以 简单 地 
计算 单词 频率 来 比较 不同 的 写作 风格 另一方面 
NLP 涉及 理解 完整 的 人类 言语 至少 在 能够 
给予 有效 回应 的 程度 上 2009 用 Python 进行 
自然语言 处理 第 9页 统计 NLP 已经 转向 另一个 角度 
现在 强调 使用 深度 学习 神经 网络 来 对 特定 
任务 进行 推理 并 开发 强大 的 端对端 系统 在 
第一 本 专门 针对 这 一 新兴 主题 的 教科书 
中 Yoav Goldberg 简洁 地 将 NLP 定义 为 将 
自然 语言 作为 输入 或 生成 自然语言 作为 输出 的 
自动 方法 自然语言 处理 NLP 是 指 人类 语言 的 
自动 计算 处理 的 总称 这 包括 将 人类 生成 
的 文本 作为 输入 的 算法 以及/c 生成/nr 自然/d 文本/n 
作为/v 输出/v 的/uj 算法/n 2017年 自然语言 处理 中 的 神经 
网络 方法 第 17页 进一步 阅读 如果 你 想 更 
深入 了解 本 部分 将 提供 有关 该 主题 的 
更多 资源 图书 数学 语言学 2010 http / / amzn 
. to / 2tO1cOO 自然语言 处理 中 的 神经 网络 
方法 2017 http / / amzn . to / 2u0JtPl 
计算 语言学 导论 1986 http / / amzn . to 
/ 2h6U4qY 牛津 计算 语言学 手册 2005年 http / / 
amzn . to / 2uHeERE 统计 自然语言 处理 基础 1999 
http / / amzn . to / 2uzwxDE 用 Python 
进行 自然语言 处理 2009 http / / amzn . to 
/ 2uZMF27 维基百科 维基百科 上 的 语言学 https / / 
en . wikipedia . org / wiki / Linguistics 维基百科 
上 的 计算 语言学 https / / en . wikipedia 
. org / wiki / Computational _ linguistics 维基百科 上 
的 自然 语言 处理 https / / en . wikipedia 
. org / wiki / Natural _ language _ processing 
维基百科 上 的 自然 语言 处理 史 https / / 
en . wikipedia . org / wiki / History _ 
of _ natural _ language _ processing 维基百科 上 的 
自然 语言 处理 概要 https / / en . wikipedia 
. org / wiki / Outline _ of _ natural 
_ language _ processing 目录 自然语言 处理 NLP 应用 程序 
的 示例 技术 资产 支持 自然语言 处理 NLP 端 到 
端 自然语言 处理 NLP 解决 方案 在 当今 的 企业 
世界 中 单独 分析 结构化 数据 已经 不足以 进行 复杂 
的 业务 分析 预测 和 决策 非 结构化 内容 自然语言 
通信 例如 电子邮件 社交 媒体 视频 客户 评论 等 可以 
帮助 发现 巨大 的 洞察力 通过 自然 语言 处理 NLP 
解决方案 您 的 组织 可以 更 深入 地 了解 非 
结构化 或 半 结构化 内容 从而 提供 增强 的 BI 
和 分析 自然语言 处理 NLP 应用 程序 的 示例 问答 
系统   增强 企业 中的 语义搜索 并将 员工 连接 到 
业务 数据 图表 信息/n 和/c 资源/n 商务/n 聊天/nz 机器人/n 和/c 
客户/n 支持/v 应用程序/n  /i 回答 问题 指导 用户 使用 手册 
或 产品 以及 自动 回复 / 重新路 由 请求 电子商务 
  提高 搜索 相关性 提供 有 针对性 的 响应 并 
根据 查询 意图 提供 个性化 结果 Phamacovigilance   从 ADR 
中 提取 实体 药物 不良 反应 报告 以 发现 见解 
并 减少 手动 操作 客户 支持   自动 处理 大量 
支持 请求 降低 成本 并 增加 追加 销售 机会 供应商 
/ 法律合同 分析   识别 差距 或 违约 留置权 / 
贷款 合同 事实 提取   取代 手动 流程 并 获得 
更好 的 贷款 信息 以 实现 更好 的 目标 定位 
招聘   自动 将 工作 与 候选人 匹配 以 提高 
填充 率 并 缩短 填写 时间 石油 和 天然气   
从 高度 非 结构化 的 日常 钻井 报告 DDRs 中 
提取 信息 技术 资产 支持 自然语言 处理 NLP 洞察力 驱动 
的 企业 越来越 多 地 寻求 利用 庞大 的 非 
结构化 数据 来 加速 和 改善 业务 成果 但是 现有 
的 自然 语言 处理 技术 并 不能 满足 企业 的 
需求 它们 太 狭隘 聊天 机器人 太 浅薄 和 通用 
基于 云的/nr 自然 语言 处理 解决 方案 或者 开发 部署 
和 维护 成本 太高 作为 技术 资产 收集 的 一部分 
Saga Natural Language Understanding NLU 是 一个 可扩展 经济 高效 
且 易于 使用 的 框架 填补 了 现有 NLP / 
NLU 技术 的 空白 了解 有关 Saga 的 更多 信息 
并 请求 演示 端 到 端 自然语言 处理 NLP 解决方案 
我们 的 自然 语言 处理 解决 方案 涵盖 了 一系列 
需求 无论 您 是 要 解决 非 结构化 内容 处理 
挑战 还是 开发 自定义 NLP 解决方案 我们 都 可以 与 
您 合作 完整 的 解决 方案 项目 规划 架构设计 实施 
以及 对 NLP 驱动 的 应用 程序 的 支持 数据采集 
使用/v 我们/r 的/uj 预/v 构建/v 和/c 定制/v 连接器/l 获取/v 非/h 
结构化/n 或/c 半/m 结构化/n 数据/n 原始/v 语言/n 处理/v 和/c 统计/v 
语言/n 处理/v 为 搜索 和 分析 提供 最高 质量 的 
结果 文本 分析 解决 方案 文本 挖掘 文本 提取 实体 
提取 内容 分类 内容 聚 类 事实 提取 和 关系 
提取 查询 理解 用于 搜索 相关性 和 个性化 参考 中文 
信息 处理 发展 报告 2016 什么 是 语音识别 语音识别 Automatic 
Speech Recognition ASR 利用计算机 实现 从 语音 到 文字 自动 
转换 的 任务 语音 识别 的 技术 有 哪些 语音 
识别 技术 = 早期 基于 信号 处理 和 模式识别 + 
机器学习 + 深度 学习 + 数值分析 + 高性能 计算 + 
自然语言 处理 语音 识别 技术 的 发展 可以 说 是 
有 一定 的 历史 背景 上世纪 80 年代 语音识别 研究 
的 重点 已经 开始 逐渐 转向 大 词汇量 非 特定 
人 连续 语音识别 到了 90 年代 以后 语音 识别 并 
没有 什么 重大突破 直到 大 数据 与 深度 神经 网络 
时代 的 到来 语音 识别 技术 才 取得 了 突飞猛进 
的 进展 语音 识别 的 相关 领域 有 哪些 语音识别 
关联 领域 = 自然语言 理解 + 自然语言 生成 + 语音合成 
语音 识别 的 社会 价值 在 哪里 语音信号 是 典型 
的 局部 稳态 时间 序列 而 日常 所见 的 大量 
信号 都 属于 这种 局部 稳态 时间 序列 信号 如 
视频 雷达 信号 金融 资产 价格 经济 数据 等 这些 
信号 的 共同 特点 是 在 抽象 的 时间 序列 
中 包含 大量 不同 层次 的 信息 可以 用 相似 
的 模型 进行 分析 历史 上 语音信号 的 研究 成果 
在 若干 领域 起到 启发 作用 如 语音 信号处理 中的 
隐 马尔科夫 模型 在 金融 分析 机械 控制 等 领域 
都 得到 广泛 的 应用 近年来 深度 神经 网络 在 
语音 识别 领域 的 巨大 成功 直接 促进 了 各种 
深度 学习 模型 在 自然 语言 处理 图形 图像处理 知识 
推理 等 众多 领域 的 发展 应用 取得 了 一个 
有 一个 令人 惊叹 的 成果 怎么 构建 语音 识别 
系统 语音 识别 系统 构建 总体 包括 两个 部分 训练 
和 识别 训练 通常 来讲 都是 离线 完成 的 将 
海量 的 未知 语音 通过 话筒 变成 信号 之后 加在 
识别 系统 的 输入端 经过 处理 后再/nr 根据 语音 特点 
建立 模型 对 输入 的 信号 进行 分析 并 提取 
信号 中 的 特征 在此 基础 上 建立 语音识别 所需 
的 模板 识别 则 通常 是 在线 完成 的 对 
用户 实时 语音 进行 自动 识别 这个 过程 又 基本 
可以 分为 前端 和 后端 两个 模块 前端 主要 的 
作用 就是 进行 端点 检测 降噪 特征提取 等 后端 的 
主要 作用 是 利用 训 练好 的 声音 模型 和 
语音 模型 对 用户 的 语音 特征向量 进行 统计 模式识别 
得到 其中 包含 的 文字 信息 语音 识别 技术 中 
的 关键 问题 是 什么 语音 特征 抽取 语音 识别 
的 一个 主要 困难在于 语音信号 的 复杂性 和 多变 性 
一段 看似 简单 的 语音信号 其中 包含 了 说 话人 
发音 内容 信道 特征 口音 方言 等 大量 信息 不仅如此 
这些 底层 信息 互相 组合 在 一起 又 表达 了如 
情绪 变化 语法 语义 暗示 内涵 等 丰富 的 高层 
信息 如此 众多 的 信息 中 仅有 少量 是 和 
语音 识别 相关 的 这些 信息 被 淹没 在 大量 
其它 信息 中 因此 充满 了 变动性 语音 特征 抽取 
即是 在 原始 语音信号 中 提取 出 与 语音识别 最 
相关 的 信息 滤除 其它 无关 信息 语音 特征 抽取 
的 原则 是 尽量 保留 对 发音 内容 的 区分 
性 同时 提高 对 其它 信息 变量 的 鲁棒性 历史 
上 研究 者 通过 各种 物理学 生理学 心理学 等 模型 
构造 出 各种 精巧 的 语音 特征 抽 取 方法 
近年来 的 研究 倾向 于 通过 数据 驱动 学习 适合 
某一 应用 场景 的 语音 特征 模型 构建 语音识别 中的 
建模 包括 声学 建模 和 语言 建模 声学 建模 是 
对 声音 信号 语音 特征 的 特性 进行 抽象化 自 
上世纪 70 年代 中期 以来 声学 模型 基本上 以 统计模型 
为主 特别 是 隐 马尔科夫 模型 / 高斯 混合模型 HMM 
/ GMM 结构 最 近几年 深度 神经网络 DNN 和 各种 
异构 神经 网络 成为 声学 模型 的 主流 结构 声学 
模型 需要 解决 如下 几个 基本 问题 如何 描述 语音信号 
的 短时 平稳性  如何 描述 语音信号 在 某一 平稳 
瞬态 的 静态 特性 即 特征 分布 规律  如何 
应用 语法 语义 等 高层 信息  如何 对 模型 
进行 优化 即 模型 训练 同时 在 实际 应用 中 
还 需要 解决 众多 应用 问题 例如  如何 从 
一个 领域 快速 自适应 到 另一个 领域  如何 对 
噪音 信道 等 非 语音 内容 进行 补偿  如何 
利用 少量 数据 建模  如何 提高 对 语音 内容 
的 区分 性  如何 利用 半 标注 或 无 
标注 数据 等等 语言 建模 是 对 语言 中 的 
词语 搭配 关系 进行 归纳 抽象 成 概率模型 这一 模型 
在 解码 过程 中 对 解码 空间 形成 约束 不仅 
减小 计算 量 而且 可以 提高 解码 精度 传统 语言 
模型 多 基于   N 元 文法 n gram 近年来 
基于 递归 神经网络 RNN 的 语言 模型 发展 很快 在 
某些 识别 任务 中 取得 了 比 n gram 模型 
更好 的 结果 语言 模型 要 解决 的 主要 问题 
是 如何 对 低频词 进行 平滑 不论是 n gram 模型 
还是 RNN 模型 低频词 很难 积累 足够 的 统计量 因而 
无法 得到 较好 的 概率 估计 平滑 方法 借用 高频词 
或 相似 词 的 统计量 提高 对 低频词 概率 估计 
的 准确性 除此之外 语言 建模 研究 还 包括  如何 
对 字母 字 词 短语 主题 等 多层次 语言 单元 
进行 多 层次 建模  如何 对 应用 领域 进行 
快速 自适应  如何 提高 训练 效率 特别 是 对 
神经 网络 模型 来说 提高效率 尤为重要  如何 有效 利用 
大量 噪声 数据 等等 解码 解码 是 利用 语音 模型 
和 语言 模型 中 积累 的 知识 对 语音信号 序列 
进行 推理 从而 得到 相应 语音 内容 的 过程 早期 
的 解码器 一般 为 动态 解码 即在 开始 解码 前 
将 各种 知识源 以 独立 模块 形式 加载 到 内存 
中 动态 构造 解码 图 现代 语音 识别 系统 多 
采用 静态 解码 即将 各种 知 识 源 统一 表 
达成 有限 状态 转 移机 FST 并将 各 层次 的 
FST 嵌套 组合 在 一起 形成 解码 图 解码 时 
一般 采用 Viterbi 算法 在 解码 图中 进行 路径 搜索 
为 加快 搜索 速度 一般 对 搜索 路 径 进行 
剪枝 保留 最 有 希望 的 路径 即 束 搜索 
beam search 对 解码器 的 研究 包括 但 不限 于 
如下 内容  如何 加快 解码 速度 特别 是 在 
应用 神经 网络 语言 模型 进行 一遍 解码 时  
如何 实现 静态 解码 图 的 动态 更新 如 加入 
新词  如何 利用 高层 语义 信息  如何 估计 
解码 结果 的 信任度  如何 实现 多 语言 和 
混合 语言 解码  如何 对 多个 解码器 的 解码 
结果 进行 融合 在 自然 语言 处理 中 尽管 文本 
清理 受 所做 的 任务 影响 比较 大 但是 有 
一些 通用 的 清理 流程 标准 是 通用 的 比如 
是否 有 必要 替换 URLS 时间 货币 姓名 地名 数字 
等 我们 以 英文 文本 为例 大致 将 文本处理 流程 
分为 以下 几个 步骤 N o r m a l 
i z a t i o n T o k 
e n i z a t i o n t 
o p wordsPart of speech TaggingNamed Entity R e c 
o g n i t i o n t e 
m m i n g and Lemmatization 下面 是 各个 
流程 的 具体 介绍 Normalization 得到 纯 文本 后 第一步 
通常 要 做 就是 Normalization 在 英文 中 所有 句子 
第一 个 单词 的 首字母 一般 是 大写 有的/i 单词/n 
也/d 会/v 全部/n 字母/n 都/d 大写/n 用于/v 表示/v 强调/v 和/c 
区分/n 风格/n 这样 更 易于 人类 理解 表达 的 意思 
但是 从 计算机 的 角度 来说 是 没法 区别 Car 
car CAR 是否 是 一个 意思 的 因此 我们 一般 
把 文本 中 所有 字母 都 转换 为 小写 或 
大写 通常 意义 上 是 小写 没 歌词 用 一个 
唯一 的 词 来 表示 例如 在 下面 的 代码 
中 字符串 文本 调用 lower 函数 就 可以 将 所有 
字母 转换 为 小写 形式 pre _ str = I 
Love My Family after _ str = pre _ str 
. lower print after _ str 输出 结果 为 i 
love my family 你 可能 还 想 清楚 文本 中 
的 句号 问号 感叹号 等 特殊字符 并且 保留 字母表 中的 
字母 和 数字 文档 分类 和聚类/nr 等 应用 中 若要 
将 所有 文本文档 作为 一个 整体 那么 正则表达式 这个 方法 
特别 有效 用 正则 匹配 小写 a 到 z 以及 
大写 A 到 Z 以及 数字 0 到 9 的 
范围 之外 的 所有 字符 并用 空格 代替 这个 方法 
无需 指定 所有 标点符号 当然 也 可以 采用 其他 正则表达式 
在 下面 的 代码 中 使用 re 模块 的 sub 
正则 匹配 所有 非 a z A Z 0 9 
的 字母 并 将其 替换 为 空格 import re text 
= the first time you see the second renaissance it 
may look boring . look at it at least and 
definitely watch part 2 . it will text = re 
. sub r ^ a zA Z0 9 text print 
text 输出 结果 the first time you see the second 
renaissance it may look boring look at it at least 
and definitely watchpart 2 it will 小写 转换 和 标点 
移除 是 两个 最 常见 的 文本 Normalization 步骤 是否 
需要 以及 在 哪个 阶段 使用 这 两个 步骤 取决于 
你 的 最终 目标 T o k e n i 
z a t i o n T o k e 
n 是 符号 的 高级 表达 一般 值 具有 某种 
意义 无法 再 拆分 的 符号 在 英文 自然语言 处理 
中 Tokens 通常 是 单独 的 词 因此 Tokenization 就是 
将 每个 句子 拆 分为 一 系列 的 词 通常 
情况下 最 简单 的 方法 是 使用 split 方法 返回 
词 列表 text = the first time you see the 
second renaissance it may look boring . look at it 
at least and definitely watch part 2 . it will 
words = text . split print words 输出 单词 组成 
的 列表 the first time you see the second renaissance 
it may look boring . look at it at least 
and definitely watch part 2 . it will 这里 默认 
情况下 是 将 一段话 在 空格 字符 处 拆分 除了 
空格 也 包括 其他 标签 新行 等 这种方法 还很 智能 
可以忽略 一个 序 列中 的 两个 或 多个 空格 字符 
因此 不会 返 回空 字符串 同样 也 可以 使用 可选 
参数 对 其 进行 控制 目前 为止 我们 只 使用 
了 Python 内置 的 处理 工具 当然 我们 也 可以 
使用 其他 工具 来 完成 相同 的 事情 比如 NLTK 
一种 处理 英文 最 常见 的 自然 语言 处理 工具箱 
某些 运算 会 简单 很多 在 NLTK 中 拆分 文本 
最 常用 的 方法 是 使用 nltk tokenize 中的 word 
_ tokenize 函数 这与 split 函数 的 效果 差不多 但是 
更加 聪明 一些 在 尝试 传入 未 标准化 的 原始 
文本 时 会 发现 根据 标点符号 位置 的 不同 对 
它们 的 处理 也 不同 例如 头衔 Dr 后面 的 
句号 . 与 Dr 保留 在 一起 作为 一个 Token 
可想而知 NLTK 使用 某种 规则 或 模式 决定 如何 处理 
每个 标点符号 有时 我们 可能 需要 将 一段话 分解成 句子 
而不是 单词 比如 如果 你 想 翻译 文本 可能 需要 
将 文 本分 拆成 句子 这时 我们 可以 通过 NLTK 
使用 sent _ tokenize 实现 这 一点 然后 可以 根据 
需要 将 每个 句子 分 拆成 词 NLTK 提供 多种 
Token 解析器 包括 基于 正则表达式 的 令牌 解析器 可以 用于 
一步 清除 标点符号 并 将其 Tokenize Stop WordStop Word 是 
无 含义 的 词 例如 is / our / the 
/ in / at 等 它们 不 会给 句子 增加 
太多 含义 单 停止词 是 频率 非常 多 的 词 
为了 减少 我们 要 处理 的 词汇 量 从而 降低 
后续 程序 的 复杂度 需要 清除 停止词 在 上述 句子 
中 即使 没有 are 和 the 我们 仍然 能 推断 
出人 对 狗 的 正面 感情 你 可以 自己 思考 
一下 NLTK 将 英语 中的 哪些 词 作为 停止词 这里 
NLTK 是 基于 特定 的 文本 语料库 或 文 本集 
不同 的 语料库 可能 有 不同 的 停止词 在 一个 
应用 中 某个 词 可能 是 停止词 而在 另一个 应用 
这个 词 就是 有 意义 的 词 要从 文本 中 
清除 停止词 可以 使 用带 过滤 条件 的 Python 列表 
理解 这里 我们 将 影评 Normalization 和 Tokenization 之后 清除 
其中 的 停止词 结果 有点 难懂 但现在 输入量 缩小 了 
很多 并 保留 了 比较 重要 的 词汇 Part of 
Speech Tagging 还 记得 在 学校 学过 的 词性 吗 
名词 代词 动词 副词 等等 识 别词 在 句子 中 
的 用途 有助于 我们 更好 理解 句子 内容 并且 标注 
词性 还 可以 明确 词 之间 的 关系 并 识别 
出 交叉 引用 同样地 NLTK 给 我们 带来 了 很多 
便利 你 可以 将 词 传入 PoS tag 函数 然后 
对 每个 词 返回 一个 标签 并 注明 不同 的 
词性 这里 函数 正确地 将 出现 的 第一 个 lie 
标注 为 动词 将 第二 个 标注 为 名词 关于 
标签 含义 的 更多 详细信息 请参阅 NLTK 文档 词性 标注 
的 一个 典型 应用 是 句子 解析 上面 的 示例 
是 NLTK 手册 中 使用 自定义 语法 解析 歧义 句 
的 一个 示例 实例 中 解析器 返回 了 两种 有效 
解释 我们 也 可以 使用 代码 画出 解析 树 以便 
可以 轻易 地 看出 两者 的 区别 I / shot 
an elephant / in my pajamas 我 穿着 睡衣 杀了 
一头 象 以及 I / shot / an elephant in 
my pajamas 我 杀了 一头 穿着 我 睡衣 的 象 
另外 还有 其他 很多 方法 可以 进行 PoS 比如 Hidden 
Markov Models HMM 以及 Recurrent Neural Networks RNNs Named EntityNamed 
Entity 一般 是 名词 短语 又来 指代 某些 特定 对象 
人 或 地点 可以 使用 ne _ chunk 方法 标注 
文本 中 的 命名 实体 在 进行 这 一步 前 
必须 先 进行 Tokenization 并 进行 PoS Tagging 如图 这 
是 一个 非常 简单 的 示例 NLTK 还 可以 识别 
出 不同 的 实体 类型 分辨 出人 组织 和 GPE 
地缘 政治 实体 另外 它 还将 Udaticy 和 Inc 这 
两个 词 识别 成 一个 实体 效果 不错 Named Entity 
并 不是 所有 的 情况 都 识别 的 很好 但 
如果 是 对 大型 语料库 进行 训练 却 非常 有效 
命名 实体 识别 通常 用于 对 新闻 文章 建立 索引 
和 进行 搜索 我们 可以 搜索 自己 感兴趣 的 公司 
的 相关 新闻 Stemming and Lemmatization 为了 进一步 简化 文本 
数据 我们 可以 将 词 的 不同 变化 和 变形 
标准化 Stemming 提取 是 将 词 还原成 词干 或 词根 
的 过程 例如 brancing / branched / branches 等 都 
可以 还原 成 branch 总而言之 它们 都 表达 了 分成 
多个 路线 或 分支 的 含义 这 有助于 降低 复杂度 
并 同时 保留 词 所含 的 基本 含义 Stemming 是 
利用 非常 简单 的 搜索 和 替换 样式 规则 进行 
的 例如 后缀 ing 和 ed 可以 丢弃 ies 可以 
用 y 替换 等等 这样 可能 会 变成 不 是 
完整 词 的 词干 但是 只要 这个 词 的 所有 
形式 都 还原成 同一个 词干 即可 因此 它们 都 含有 
共同 的 根本 含义 NLTK 有 几个 不同 的 词干 
提取 器 可供 选择 例如 PorterStemmer 方法 上图 例子 中 
我们 已经 清除 了 Stop Words 所以 部分 转换 效果 
非常 好 例如 started 还原 成了 start 但是 像 其它 
词 例如 people 末尾 的 e 被 删除 出现 这样 
的 原因 是 因为 规则 过于 简单 Lemmatization 是 将 
词 还原成 标准化 形式 的 另一种 技术 在 这种 情况 
下 转换 过程 实际上 是 利用 词典 将 一个 词 
的 不同 变形 映 射到 它 的 词根 通过 这种 
方法 我们 能将/nr 较大 的 词形变化 如 is / was 
/ were 还原成 词根 be NLTK 中的 默认 词形 还 
原器 使用 nltk . stem . wordnet 数据库 将 词 
还原成 词根 这里 我们 试一下 像 词干 提取 一样 将 
W o r d N e t L e m 
m a t i z e r 的 实例 初始化 
并将 各个 词 传入 lemmatize 方法 结果 中 只有 词 
ones 被 还原 成了 one 其它 词 并无 任何 变化 
仔细 读 各个 词 你 会 发现 ones 是 这里 
唯一 的 复数名词 实际上 这 就是 它 被 转换 的 
原因 Lemmatization 需要 知道 每个 词 的 词性 在 这个 
例子 中 W o r d N e t L 
e m m a t i z e r 默认 
词性 是 名词 但是 我们 可以 指定 PoS 参数 修改 
这个 默认设置 我们 传入 v 代表 动词 现在 两个 动词 
形式 boring 和 started 都被 转换 了 小结 一下 在 
前面 的 示例 中 可以 看出 Stemming 有时 会 生成 
不 是 完整 英 语词 的 词干 Lemmatization 与 Stemming 
类似 差别 在于 最终 形式 也是 有 含义 的 词 
这就是说 Lemmatization 需要 字典 而 Stemming 不 需要 字典 因此 
根据 你 施加 约束 的 不同 Stemming 是 对 内存 
要求 较低 的 方案 一个 基本 的 搜索引擎 的 工作 
基本上 可以 分成 以下 三 个 部分 利用 网络爬虫 下载 
网页 分析 网页 关键词 制成 索引 备用 理解 用户 输入 
确定 检索 关键词 根据 关键词 和 网页 索引 按照 相关性 
排序 列出 搜索 结果 第一 个 部分 主要 涉及 网络爬虫 
技术 图论 自然语言 处理 等 技术 第二 个 部分 主要 
涉及 自然语言 处理 第三 个 部分 同样 涉及 自然语言 处理 
自然语言 即 是 人类 用来 交流 的 语言 由此可见 自然语言 
处理 NLP Natural Language Processing 是 现代 搜索引擎 很 重要 
的 内容 其 终极 目的 是 将 自然 语言 转化 
为 计算机 容易 处理 的 形式 从/p 分词/n 的/uj 角度/n 
来看/u 文/n 法分析/n 与/p 统计模型/i 分词/n 是/v NLP 需要 解决 
的 基础 问题 分词 算法 的 好坏 直接影响 NLP 的 
结果 这里 我们 先 从 一个 简单 的 例子 说起 
逐步 探讨 合理 的 分词 算法 从 一个 简单 的 
句子 说起 现在 有一个 句子 比如 我 去 电脑城 买了 
一台 电脑 如果 要让 计算机 对 这个 句子 做 分词 
处理 进而 理解 这个 句子 你 会有 怎样 的 思路 
呢 大多数 人 首先 会 思考 一下 自己 是 怎么 
理解 这个 句子 的 对于 中国 人 来说 这样 一个 
简单 的 句子 可能 不 需要 什么 特殊 的 思维过程 
句子 的 文字 形式 和 句子 背后 的 含义 可以 
在 瞬间 反映 出来 稍有 汉语 文法 知识 的 读者 
可能 会想 句子 可以 分成 几个 部分 我 主语 去 
电脑城 买了 一台 电脑 谓语 去 电脑城 状语 买了 谓语 
动词 一台 电脑 动词 宾语 名词 短语 句子 结束 的 
标识 分别 理解 每个 部分 的 意思 将 意思 拼 
合起来 变成 完整 的 句 意 它 先 通过 文 
法分析 将 句子 拆分 成 一个二维 的 语法树 然后再 理解 
各个 部分 的 含义 最后 做 拼接 这样 的 方案 
或者 说是 算法 是 基于 文法 规则 的 清晰 明了 
也 易于 实现 在 计算机 里 就是 几个 循环 判断 
对于 程序员 来说 这样 的 算法 也 特别 亲切 因为 
程序员 使用 的 高级 编程语言 比如 C + + 的 
语法 规则 和 这样 的 方案 非常 相似 由于 这样 
的 算法 直观 易于 实现 所以 人们 相信 在 有了 
愈加 全面的 文法 概括 和 愈加 强大 的 计算 能力 
时 人们 就 能 彻底 解决 自然语言 处理 的 问题 
了 文法 分析 的 困境 然而 如果 你 仔细 观察 
文法 分析 的 过程 就 会 发现 这么 一个 简单 
的 句子 被 分成 了 一个 这样 复杂 的 二维 
树状 结构 耗费 了 六条 注释 用 计算机 来 处理 
这样 一个 过程 当然 不难 但是 要 处理 现实 生活 
中 遇到 的 真实 句子 往往 就 不 那么 容易 
了 由于 理解 understanding 自然语言 需要 关 于外 在 世界 
的 广泛 知识 以及 运用 操作 这些 知识 的 能力 
自然语言 认知 同时 也 被 视为 一个 人工智能 完备 AI 
complete 的 问题 这个 句子 依然 可以 用 上述 方法 
来 处理 先 分成 主谓 部分 再 仔细 拆分 谓语 
部分 比如 自然语言 认知 主语 偏正短语 自然语言 名 词作 定语 
修饰 认知 名词 由于 理解 understanding 自然语言 需要 关 于外 
在 世界 的 广泛 知识 以及 运用 操作 这些 知识 
的 能力 同时 也 被 视为 一个 人工智能 完备 AI 
complete 的 问题 谓语 由于 理解 understanding 自然语言 需要 关 
于外 在 世界 的 广泛 知识 以及 运用 操作 这些 
知识 的 能力 原因 状语 同时 也 被 视为 谓语 
动词短语 同时 状语 也 被 视为 谓语 动词 一个 人工智能 
完备 AI complete 的 问题 动词 宾语 一个 定语 人工智能 
完备 的 定语 问题 名词 句子 结束 的 标志 这个 
句子 的 语法 分析树 我 没有 写完 因为 实在 太 
复杂 了 显而易见 单纯 基于 文法 分析 的 分析器 是 
很难 处理 生活 中 的 真实 句子 的 那么 问题 
出 在哪里 我 认为 至少 有 两个 问题 文法 规则 
数量 巨大 上万条 语法 规则 才 只能 覆盖 约 20% 
的 真实 句子 且 有些 为了 处理 特殊 情况 的 
语法 规则 和 其他 规则 相互 矛盾 自然 语言 与 
程序 设计 语言 不同 自然 语言 中 词汇 的 具体 
含义 与 上下文 相关 而 程序 设计 语言 则 没有 
这样 的 歧义 性 从 算法 复杂度 的 角度 来说 
单纯 基于 文法 分析 的 分析器 用于 分析 自然语言 其 
复杂度 比 分析程序 设计 语言 要 高出 四个 量级 从 
直观 的 印象 来说 上述 句子 在 一台 现代 计算机 
上 用文 法分析 的 方式 处理 也 需要 至少 一 
分钟 的 时间 这种 低效 是 无法 接受 的 查字典 
分 词法 在 之前 的 文法 分析方法 里 分词 依赖于 
文法 分析 的 结果 程序 要 先 输出 语法树 然后 
才能 得到 分词 结果 而 这样 的 方法 已经 被 
证明 是 低效 的 这样 的 低效 来源于 复杂 的 
文 法分析 过程 为了 提高 效率 人们 很 自然 地 
想到 是否/v 有/v 办法/n 绕开/v 文/n 法分析/n 直接 尝试 分词 
呢 对于 中文分词 北京/ns 航空/n 航天/n 大学/n 的/uj 梁南元/nr 教授/n 
提出/v 了/ul 查字典/nr 分/v 词法/n 做法 相当 简单 比如 对于 
下列 句子 山东 大学 数学 学院 是 中国 最好 的 
数学 基础 教育 基地 之一 我们 让 计算机 从左到右 扫描 
整个 句子 每 扫到 一个 字 就 往 字典 里 
查询 遇到 字典 里 有的 词 就 标注 出来 于是 
整个 句子 就被 分割 成了/nr 这样 山东 | 大学 | 
数学 | 学院 | 是 | 中国 | 最好 的 
| 数学 | 基础 | 教育 | 基地 | 之一 
看起来 结果 不错 不过 细心 的 读者 很快 就 会 
发现 山东/ns 大学/n 和/c 基础/n 教育/vn 都是/nr 完整/a 的/uj 词/n 
在 它们 之间 不 应该 再做 划分 会 出现 这种 
情况 也 不 意外 我们 要求 计算机 从左到右 扫描 当 
计算机 遇到 「 山东 」 二字 的 时候 就 认为 
这 是 一个 词 了 自然 不会 再 去 寻找 
下 一个 字 去 寻求 匹配 同理 基础教育 梁 教授 
提出 了 一个 方案 即 总是 搜寻 尽可能 长 的 
分词 这在 计算机 科学 领域 叫做 「 贪婪 」 运用 
贪婪 的 办法 上述 句子 的 分词 就会 变成 山东大学 
| 数学 | 学院 | 是 | 中国 | 最好 
的 | 数学 | 基础教育 | 基地 | 之一 看起来 
就 没什么 问题 了 不过 汉语 博大精深 这种 办法 也 
不能 一劳永逸 比如 大学 生活区 正确 的 分词 应该是 大学 
| 生活区 但是 按照 贪婪 的 办法 会被 分词 成 
大学生 | 活 | 区 这 就不 对了 又 比如 
发展 中 国家 正确 的 分词 应该是 发展 中 | 
国家 而 不是 发展 | 中国 | 家 可见 查字典 
的 办法 虽然 效率 很高 但是 时有 出错 并不 牢靠 
查字典 的 办法 遇到 的 困境 来自 于 自然 语言 
的 歧义 性 人类 在 阅读 自然语言 时 会 结合 
上下文 判断 有 多个 意向 的 词汇 在 文中 的 
具体 含义 但是 计算机 却 没有 这个 能力 实际上 中国 
传统 文学 里 说 的 「 句读 」 其 目的 
就是 通过 分词 断句 来 消除歧义 那么 怎么 让 计算机 
具备 这样 的 能力 呢 千呼万唤 始 出来 的 统计模型 
行文 至此 数学 终于 要 第一 次 展现 其 威力 
和 美丽 我们 之前 提到 对 一个 句子 做 分词 
其 正确 与否 和 词汇 的 二义性 紧密 相关 由于 
计算机 无力 综合 上下文 判断 词汇 含义 解决 二义性 所以 
查字典 的 办法 陷入 了 困境 数学 中 有 所谓 
的 「 反证法 」 在 这里 我们 不讲 反证法 但是 
要 讲讲 反证法 的 思想 反证法 的 核心 思想 就是 
「 正 难 则 反 」 正面 突破 很 困难 
那就 不走 大路 开个 后门 照样 进城 在 这里 既然 
计算机 没有 能力 综合 上下文 解决 词汇 的 二义性 那么 
我们 就 不依赖 计算机 智能 去 解决 转而 借助 人工 
的 力量 解决 当然 我 说 的 不是 找 一个 
工人 实时 干预 程序 的 运行 帮助 程序 作出 正确 
的 判断 而是 说 让 计算机 经过 大量 的 文本 
训练 吸取 人类 的 「 分词 经验 」 而 这个 
方法 就是 统计模型 假定 一个 句子 SS 可以 有 几种 
分词 的 方案 比如 有 以下 三种 A1 A2 A3 
Aj 1 B1 B2 B3 Bk 2 C1 C2 C3 
Cl 3 其中 A1 A2 B1 B2 C1 C2 等 
都是 汉语 的 词汇 这样一来 如果 1 1 是 最好 
的 分词 那么 1 1 出现 的 概率 应该 最大 
也 就是说 分词 方案 1 1 应该 满足 4 4 
P A1 A2 A3 Aj P B1 B2 B3 Bk 
4 P A1 A2 A3 Aj P C1 C2 C3 
Cl 答案 就是 这么 简单 当然 如何 处理 4 4 
需要 一点 统计 知识 和 技巧 得到 这些 分词 方案 
也 需要 依靠 动态规划 算法 不然 计算 量 太大 还有 
诸如 分词 颗粒 大小 之类 的 细节 问题 需要 处理 
这些 内容 我 们 放在 后续 的 小节 里 讨论 
在 这里 读者 只 需要 知道 这种 利用 统计 的 
方法 处理 分词 效果 好 效率高 就 可以 了 小结 
对于 分词 来说 统计模型 的 方法 效率 比 文法 分析 
的 方法 高 同时 效果 也 要好 这里 效率 的 
提升 是 十分 显著 的 此外 我们 发现 一个 优秀 
算法 背后 的 数学 模型 是 十分 简洁 优美 的 
统计模型 只 需要 一个 概率 不等式 组 就 可以 描述 
而 文法 分析模型 几乎 无法 构建 一个 可读 的 数学 
模型 我们 在 设计 算法 的 时候 要 尽可能 追求 
简洁 优美 的 数学 模型 从 简单 粗暴 做起 逐步完善 
完美 正如 牛顿 爵士 所言 「 真理 在 形式 上 
总是 简单 的 而 不是 复杂 含混 的 」 最后 
文法 分析 方法 是 十分 容易 想到 的 十分 自然 
的 处理 方法 然而 这种 「 自然 」 也 使人 
误入歧途 这 提醒 我们 不可 固执 不可 迷信 经验 文章 
目录 一 Jieba 二 NLPIR 三 nltk 四 SnowNLP 五 
StandFordNLP 六 thulac 七 StandfordNLP 八 结论 微信 公众 号 
本次 依然 使用 上篇 博客 自然语言 处理 一 中英文 分词 
中 我们 使用 过 的 工具 来 对 中英文 文本 
进行 词性 标注 与 命名 实体 识别 一 Jieba 词性 
标注 与 命名 实体 识别 二 NLPIRNLPIR 词性 标注 与 
命名 实体 识别 三 nltknltk 词性 标注 与 命名 实体 
识别 四 SnowNLPSnowNLP 词性 标注 与 命名 实体 识别 五 
t a n d F o r d N L 
P t a n d f o r d N 
L P 词性 标注 与 命名 实体 识别 六 thulacthulac 
词性 标注 与 命名 实体 识别 七 StandfordNLP 中文 词性 
标注 与 命名 实体 识别 八 结论 词性 标注 与 
命名 实体 识别 和上/nr 一个 实验 的 分词 相比 难度 
又 有所 增加 在 给出 的 实验 文件 中 已经 
无法 找到 现成 的 代码 有些 需要 去 百度 找 
有些 需要 自己 写 不过 好在 都 完成 了 在 
词性 标注 中 斯坦福大学 nlp 库 的 效果 最好 但是 
占用 内存 较大 而且 需要 下载 较大 的 预装 包 
在对 中文 进行 处理 时 需要 中文版 此外 结巴 的 
中文分词 是 轻量级 中 效果 最好 的 代码 和 文本 
太多 上传 太麻烦 如果 需要 请 点击 这里 下载 微信 
公众 号 同时 也 欢迎 各位 关注 我 的 微信 
公众 号 南木 的 下午茶 自然语言 处理 包含 4个 部分 
1 语言识别 语音合成 2 自然语言 理解 对话 理解 知识 获取 
和 问答 任务 理解 3 底层 的 机器学习 TennsorFlow 和 
other 4 个性化 信息 的 获取 和 利用 其中 两个 
重要 的 竞 品 分析 google 的 产品 各种 语言 
场景 Alexa 亚马逊 产品 基于 云 计算 的 对话 机器人 
产品 NLP 定制 化 场景 google NLP 开源 项目 BERT 
Bidirectional Encoder Representations from Transformers 预 训练 语言 表示 的 
方法 可以 在 大型 文本 语料库 如 维基百科 上 训练 
通用 的 语言 理解 模型 然后 将 该 模型 用于 
下游 NLP 任务 比如 机器翻译 问答 第一 个 无 监督 
的 用于 预 训练 NLP 的 深度 双向 系统 无 
监督 意味着 BERT 仅 使用 文本 语料库 进行 训练 也 
就是说 网络 上 有 大量 多种语言 文本 数据 可供使用 NLP 
中的 3个 关键 概念 1 文本 嵌入 字符串 的 矢量 
表示 2 机器翻译 使用 神经网络 翻译 语言 3 以及 Dialogue 
和 Conversations 可以 实时 与人 进行 对话 的 技术 还 
涉及 到 的 技术 技术 1 情绪 分析 情绪 分析 
是 通过 较小 元素 的 语义 组成 来 解释 较大 
文本 单元 实体 描述性 术语 事实 论据 故事 的 含义 
的 过程 用于 情感 分析 的 现代 深度 学习 方法 
可 用于 形态学 语法 和 逻辑 语义 其中 最 有效 
的 是 递归 神经网络 迄今为止 用于 情感 分析 的 最 
强大 的 RNN 模型 是 递归 神经 张量 网络 其 
在 每个 节点 处 具有 神经 网络 的 树结构 技术 
2 问答 系统 问答 QA 系统 的 想法 是 直接 
从 文档 对话 在线 搜索 和 其他 地方 提取 信息 
以 满足 用户 的 信息 需求 QA 系统 不是 让 
用户 阅读 整个 文档 而是 更 喜欢 简短 而 简洁 
的 答案 QA 系统 可以 非常 容易 地 与 其他 
NLP 系统 结合 使用 并且 一些 QA 系统 甚至 超越 
了 对 文本文档 的 搜索 并且 可以 从 图片 集合 
中 提取 信息 强大 的 深度 学习 架构 称为 动态内存 
网络 DMN 已 针对 QA 问题 进行 了 专门 开发 
和 优化 给定 输入 序列 知识 和 问题 的 训练 
集 它 可以 形成 情节 记忆 并 使用 它们 来 
产生 相关 答案 该 体系 结构 具有 以下 组件 语义 
内存模块 类似于 知识库 被 用来 创建 从 输入 句子 的 
嵌入 字 序列 预先 训练 手套 载体 输入 模块 处理 
与 问题 有关 的 输入 矢量 称为 事实 问题 模块 
逐 字处理 疑问词 并且 使用 输出 相同 权重 的 GRU 
输入 模块 的 向量 情景记忆/n 模块/n 接收/v 从/p 输入/v 中/f 
提取/v 和/c 编码/n 的/uj 嵌入/v 事实/n 和/c 问题/n 载体/n 答案 
生成 模块 通过 适当 的 响应 情景记忆 应该 包含 回答 
问题 所需 的 所有 信息 DMN 不仅 在 质量 保证 
方面 做 得 非常 好 而且 在 情感 分析 和 
词性 标注 方面 也 优于 其他 架构 技术 3 文本 
摘要 人类 很难 手动 汇总 大型 文本文档 文本 摘 要是 
NLP 为 源 文档 创建 简短 准确 和 流畅 的 
摘要 问题 随着 推送 通知 和 文章 摘要 获得 越来越 
多 的 注意力 为 长 文本 生成 智能 且 准确 
摘要 的 任务 每天 都在/nr 增长 技术 4 注意力 机制 
神经 网络 中 的 注意力 机制 是 基于 人类 的 
视觉 注意 机制 研究 人员 不得不 处理 各种 障碍 算法 
的 局限性 模型 的 可扩展性 对 人类 语言 的 模糊 
理解 好消息 是 大量 的 开源 存在 NLP 中 已经 
解决 的 主要 障碍 没有 单一 的 模型 架构 跨 
任务 具有 一致 的 最新 结果 机器学习 中 一种 强大 
的 方法 是 多任务 学习 它 共享 相关 任务 之间 
的 表示 以使 模型 能够 更好 地 概括 原始 任务 
另 一个 挑战 是 重复 字 表示 的 问题 其中 
模型 中 编码器 和 解码器 的 不同 编码 导致 重复 
的 参数 / 含义 另 一个 障碍 是 与 诸如 
卷积 神经 网络 或 前馈 神经网络 相比 任何 Deep NLP 
技术 的 基本 构建 块 Recurrent Neural Networks 相当 慢 
准 递归 神经 网络 采用 RNN 和 CNN 的 最佳 
部分 来 提高 训练 速度 使用 卷积 跨越 时间 的 
并行性 和 跨越 信道 的 并行性 的 元素 级 门控 
递归 在 NLP 中 架构 搜索 使用 机器学习 自动化 人工神经网络 
设计 的 过程 非常 缓慢 使用 Google Brain 进行 强化 
学习 的 神经 架构 搜索 是 迄今为止 开发 的 最 
可行 的 解决方案 对话 系统 场景 Task bot 任务 型 
对话 系统 建立 IR BOT 检索 型 对话 系统 Chitchat 
bot 闲聊 系统 NLU 中 设计 概念 域 确认 用户 
意图 甄别 填充 槽 点 第一个 是 语法分析 可以 通过 
语法 规则 去 分析 一句话 得到 这句 活 是 疑问句 
还是 肯定句 继而 分析 出 用户 意图 第二 种 方法 
是 生成 模式 主要 两个 代表性 的 HMM CRF 这样 
就 需要 标注 数据 第三 种 方法 是 分类 思想 
先 对 一句话 提取 特征 再 根据 有 多少 个 
槽 值 或 意图 训练 多少 个 分类器 输入 一句话 
分别 给 不同 的 分类器 最终 得到 包含 槽 值 
的 概率 有 多大 最终 得到 这个 槽 值 还有 
一种 采用 深度 学习 方式 使用 LSTM + CRF 两种 
组合 的 方式 进行 实体 识别 现在 也 是 首选 
的 方法 一般 轻量 型 的 对话 系统 还是 通过 
语法分析 或 分类 方式 或 序列 标注 来做 自然语言 生成 
也 有 多种 方法 这里 举 三个 方法 基于 模板 
基于 语法 规则 和 基于 生成 模型 方法 DM 涉及 
的 概念 对话 状态 追踪 DST 对话 策略 意图 识别 
的 准确度 跟 两 方面 有关 1 关键字 在 当前 
意图 中 出现 的 频率 2 关键字 在 整个 文件 
中 出现 的 频率 自然语言 理解 后 需要 有 状态 
追踪 策略 优化 等 对话 管理 模块 一般 用 传统 
的 三元组 方式 即 action slot value action 就是 意图 
slot 是 需要 填充 的 槽 值 value 是 对应 
的 值 语音识别 孤立 词 语音识别 连续 词 语音识别 大词 
连续 语音识别 语音合成 语言 处理 声学 处理 韵律 处理 情感 
处理 语义 理解 中文分词 序列 标注 实体 识别 意图 识别 
语言 生成 预定义 模板 问答 语料库 知识图谱 深度 学习 对话 
状态 模型 对话 表示 模型 对话 推理模型 对话 学习 模型 
对话 策略 模型 通用 对话 策略 领域 对话 策略 语料库 
资源 预制 模板 问答 语料 知识图谱 生成 模型 百科 问答 
搜索引擎 作者 刘才权/nr 编辑/n 田 /nr   旭 前   言在/nr 
这个 日新月异 的 信息 时代 海量 数据 的 积累 计算 
能力 的 不断 提升 机器学习 尤其 是 深度 学习 的 
蓬勃 发展 使得 人工智能 技术 在 不同 领域 焕发 出 
蓬勃 的 活力 自己 经历 了 嵌入式 开发 移动 互联网 
开发 目前 从事 自然语言 处理 算法 开发 工作 从 工程 
软件 开发 到 自然 语言 处理 算法 开发 希望 通过 
这个 系列 的 文章 能够 由浅入深 通俗 易懂 的 介绍 
自然语言 处理 的 领域 知识 分享 自己 的 成长 同 
大家 一起 进步 01 问题 描述 在上 一篇 从零开始 学习 
自然语言 处理 NLP NLP Framework 开源 方案 梳理 3 中 
梳理 了 目前 流行 的 NLP 开源 框架 这里 重点 
介绍 下 DeepPavlov 框架 DeepPavlov 框架 的 模型 实用性 很强 
对 实际 的 生产 开发 有 很大 的 借鉴 意义 
02 框架 组成 DeepPavlov 是 一个 基于 TensorFlow 和 Keras 
的 专门 针对 对话 系统 研究 和 实验 部署 的 
自然 语言 处理 框架 项目 地址 http / / docs 
. deeppavlov . ai / en / master / # 
框架 主要 包括 常用 的 NLP 模型 包括 Pre train 
模型 如 词 向量 训练 分类 命名 实体 识别 NER 
相似 度 计算 等 针对 对话 系统 实现 和 评测 
的 实验 框架 Framework 基于 Json 文件 进行 开发 流程 
和 数据流 pipeline 配置 提供 同 第三方 应用 进行 集成 
的 工具 如与 Amazon Alexa 和 Microsoft Bot Framework 的 
集成 为 对话 模型 的 评测 提供 Benchmark 环境 DeepPavlov 
的 默认 Pre train/w 模型/n 和/c 测评/v 数据集/i 主要/b 基于/p 
英文/nz 和/c 俄文/nz 对于 中文 场景 需要 做 适当 的 
调整 03 框架 使用 对象 新 模型 开发者 方便 同 
已有 Benchmark 模型 进行 对比 评测 普通 NLP 任务 处理者 
如 针对 内容 审核 任务 敏感 信息 增加 掩码 等 
任务 可以 直接 使用 框架 提供 的 分类 和 序列 
标注 模型 完成 业务 服务 的 快速 开发 和 测评 
对话 系统 开发 者 DeepPavlov 是 为 对话 系统 场景 
量身 定制 的 对话 系统 开发 者 可以 直接 参考 
使用 对话 系统 应用 开发 者 DeepPavlov 框架 为 应用 
集成 提供 了 专门 的 工具 可以 直接 与 Amazon 
Alexa Microsoft Bot Framework 等 平台 进行 对接 04 框架 
使用 规范 开发 流程 框架 将 数据模型 服务 的 开发 
和 验证 流程 如 数据 清洗 模型 设计 模型 训练 
模型 选优 模型 评测 使用 Json 配置文件 串联 成 pipeline 
能够 很好 的 规范 开发 流程 新 模型 对比 评测 
为 对话 模型 的 评测 提供 Benchmark 环境 方便 新 
模型 的 对比 评测 但 环境 主要 基于 英文 和 
俄文 对于 中文 任务 需要 重新 训练 对比 常用 NLP 
模型 使用 框架 内置 了 很多 常用 模型 以 分类 
为例 就 包含 了 cnn _ model bilstm _ model 
bilstm _ attention _ model transformer _ model 等 12种 
模型 实现 在 项目 中 可以 直接 使用 同时 框架 
提供 了 Pre train 模型 但 主要 是 基于 英文 
和 俄文 的 对于 中文 场景 需要 自己 进行 重新 
训练 对话 系统开发 DeepPavlov 是 为 对话 系统 场景 量身 
定制 的 对话 系统 开发 者 可以 直接 参考 使用 
05DeepPavlov 框架 层次 DeepPavlov 从 整体 到 局部 可分为 如下 
三 个 层次 下面 我们 从 外层 到 内层 逐层 
介绍 DeepPavlov 的 框架 设计 06DeepPavlov 顶层 框架 Agent 同 
用户 直接 交互 的 代理 直接 接收 用户 输入 的 
纯 文本 信息 raw text Skill 领域 技能 如 基于 
意图 词 槽 的 任务 型 技能 基于 Seq2Seq 的 
闲聊 技能 基于 知识图谱 的 知识 问答 技能 Skill Manager 
确定 用户 query 选择 使用 哪些 skill 并 确定 将 
哪一个 skill 的 召回 结果 作为 最终 的 回复 ComponentSkill 
实现 的 组成部分 如 针对 任务 型 技能 包括 数据 
预处理 component 意图 识别 component slotfilling component 等 ChainerChainer 以 
Json 配置文件 的 形式 将 某个 skill 相关 的 所有 
component 串联 起来 Data Storage 框架 本身 包含 的 Pre 
train 模型 和 Benchmark 评测 数据集 07Skill 框架 DeepPavlov 内置 
的 skill 主要 包括 任务 型 skill Goal Oriented Dialogue 
Bot 基于 意图 / 词 槽 / 对话 管理 等 
component 实现 的 问答 skill 阅读 理解 skill Open Domain 
Question Answering 基于 阅读 理解 实现 问答 skill 相对于 阅读 
理解 component Context Question Answering skill 还 包含 在 多个 
召回 结果 中 进行 排序 的 能力 规则 型 skill 
Pattern Matching 基于 自定义 规则 实现 问答 skill Seq2Seq skill 
Sequence To Sequence Dialogue Bot 基于 Seq2Seq 实现 问答 skill 
常见问题 问答 skill Frequently Asked Questions Answering 先将 句子 嵌入 
为 向量 使 用词 向量 叠加 然后 做 分类 处理 
给 每一个 answer 一个 打分 选取 打分 最高 的 answer 
作为 最终 回复 的 skill 商品 查询 skill eCommerce Bot 
商品 查询 回复 skill 支持 多轮 添加 过滤 条件 下面 
是 场景 示例 08 基本 能力 框架 DeepPavlov 内置 的 
基本 能力 主要 包括 数据 预处理 component Data processors 主要 
提供 包括 分词 嵌入 向 量化 等 预处理 能力 主要 
基于 俄文 和 英文 阅读 理解 component Context Question Answering 
相对于 阅读 理解 skill Open Domain Question Answering component 不 
包含 对 多个 召回 结果 进行 排序 rank 的 能力 
具体 的 处理 场景 示例 如下 分类 component Classification 分类 
组件 可以 用来 做 场景 和 意图 的 分类 Morphological 
Tagger component 一种 特殊 的 POS 命名 实体 识别 component 
Named Entity Recognition NER 能力 组件 相似 度 计算 component 
Neural Ranking 通过 基于 孪生 网络 完成 相似 度 计算 
实现 在 标准 问答 库 中 标准 答复 的 查找 
词 槽 填充 component Slot filling 在 NER 的 基础 
上 增加 了 词表 限制 * 拼写 纠错 component Spelling 
Correction 提供 了 两种 纠错 方法 * levenshtein _ corrector 
基于 编辑 距离 * brillmoore 基于 统计模型 TF IDF 排序 
component TF IDF Ranking 基于 TF IDF 的 文档 召回 
排序 流 行度 排序 component Popularity Ranking 将 TF IDF 
打分 和 流行 度 打 分作 为特征 通过 逻辑 回归 
计算 流 行度 最终 实现 排序 09Json 配置文件 解析 DeepPavlov 
通过 Json 配置文件 实现 开发 流程 控制 和 数据流 pipeline 
的 控制 上面 提到 DeepPavlov 主要 分为 Agent Skill 和 
Component 三个 层次 而 Json 配置文件 主要 应用在 Skill 和 
Component 这 两个 层面 而对 Agent 的 控制 框架 通过 
直接 的 代码 来 实现 例如 其中 skillsAgent 支持 的 
所有 skill 列表 skills _ filter 针对 用户 query 确定 
Agent 使用 哪些 skill skills _ filter 通过 召回 的 
不同 skill 结果 确定 最后 的 回复 内容 给用户 下面 
我们 具体 的 介绍 下 Json 配置文件 的 具体 结构 
09Json 配置文件 结构 如上 图 所示 Json 配置文件 主要 由 
如下 五个 部分 组成 参考 分类 component dataset _ reader 
主要 负责 数据 的 读取 dataset _ iterator 数据 迭代 
器 从 dataset _ reader 中 获得 数据 然后按 batch 
抽取 数据 供 后面 的 模型 训练 使用 chainer 配置文件 
的 核心 将 数据 预处理 模型 选优 和 模型 预测 
输出 通过 pipeline pipe 字段 内 进行 约束 的 形式 
串联 起来 chainer { in x in _ y y 
pipe { id classes _ vocab class _ name simple 
_ vocab fit _ on y save _ path { 
MODELS _ PATH } / classifiers / { PROJECT _ 
NAME } _ { MODEL _ NAME } / classes 
. dict load _ path { MODELS _ PATH } 
/ classifiers / { PROJECT _ NAME } _ { 
MODEL _ NAME } / classes . dict in y 
out y _ ids } { in x out x 
_ tok id my _ tokenizer class _ name char 
_ tokenizer tokenizer char _ tokenizer } { in x 
_ tok out x _ ids id seq _ to 
_ emb _ ids class _ name seq _ to 
_ emb _ ids emb _ file _ path { 
EMBED _ PATH } text _ size 250 } { 
in y _ ids out y _ onehot class _ 
name one _ hotter id my _ one _ hotter 
single _ vector true depth # classes _ vocab . 
len } { in x _ ids fit _ on 
_ batch _ preprocess x _ ids y _ onehot 
out y _ pred _ probas main true class _ 
name keras _ classifier _ model graph _ metrics multilabel 
_ f1 save _ path { MODELS _ PATH } 
/ classifiers / { PROJECT _ NAME } _ { 
MODEL _ NAME } / model load _ path { 
MODELS _ PATH } / classifiers / { PROJECT _ 
NAME } _ { MODEL _ NAME } / model 
n _ classes # classes _ vocab . len train 
_ emb false kernel _ sizes _ cnn 1 2 
3 4 5 filters _ cnn 512 optimizer Adam learning 
_ rate 0.001 learning _ rate _ decay 0.9 loss 
categorical _ crossentropy embedding _ matrix # seq _ to 
_ emb _ ids . matrix text _ size 250 
last _ layer _ activation softmax coef _ reg _ 
cnn 0.0 coef _ reg _ den 0.0 dropout _ 
rate 0.5 dense _ size 30 model _ name cnn 
_ model } { in y _ pred _ probas 
out y _ pred _ ids class _ name proba2labels 
max _ proba true } { in y _ pred 
_ ids out y _ pred _ onehot ref my 
_ one _ hotter } { in y _ pred 
_ ids out y _ pred _ labels ref classes 
_ vocab } out y _ pred _ ids y 
_ pred _ onehot y _ pred _ labels y 
_ pred _ probas } 官方 文档 中 将 pipe 
字段 内 的 每 一对 花括号 { } 中 的 
内容 成为 一个 component 注意 这里 的 component 和 上面 
提到 的 框架 component 是 不同 的 为了 方便 区分 
我们 将 pipe 中的 component 标识 为 pipe component train 
模型 训练 模型 选优 和 评测 配置 train { epochs 
10 batch _ size 256 metrics { name cal _ 
confusion _ matrix inputs y y _ pred _ labels 
} { name f1 _ micro inputs y y _ 
pred _ labels } { name recall _ micro inputs 
y _ ids y _ pred _ ids } { 
name precision _ micro inputs y _ onehot y _ 
pred _ onehot } { name f1 _ macro inputs 
y _ onehot y _ pred _ onehot } { 
name precision _ macro inputs y y _ pred _ 
labels } { name recall _ macro inputs y _ 
ids y _ pred _ ids } { name recall 
_ group inputs y _ onehot y _ pred _ 
onehot } { name precision _ group inputs y y 
_ pred _ labels } { name f1 _ group 
inputs y _ ids y _ pred _ ids } 
validation _ patience 5 val _ every _ n _ 
epochs 1 log _ every _ n _ epochs 1 
show _ examples true validate _ best true test _ 
best true report _ path { MODELS _ PATH } 
/ classifiers / { PROJECT _ NAME } _ { 
MODEL _ NAME } / report . xlsx } 其中 
metric 字段 中排 在 最 前面 的 指标 作为 模型 
选优 的 标准 metadata 相关 相关 的 常量 配置 其中 
imports 是 DeepPavlov 框架 之外 自定义 实现 10DeepPavlov 存在 的 
问题 环境 依赖 DeepPavlov 是 基于 TensorFlow 和 Keras 实现 
的 不能 继承 其他 计算 框架 的 模型 实现 如 
PyTorch 语言 支持 Pre train/w 模型/n 和/c 评测/vn 数据集/i 主要/b 
基于/p 英文/nz 和/c 俄文/nz 不支持 中文 生产 环境 部署 DeepPavlov 
在 运行 时 需要 依赖 整个 框架 源码 开发 环境 
对 框架 修改后 生产 环境 需要 更新 整个 框架 同时 
也 不能 直接 将 功能 Component 作为 服务 独立 导出 
不 适合 在 生产 环境 的 部署 和 发布 END 
往 期 回顾 之 作者 刘才权/nr 1 机器学习 笔记 神经网络 
5 2 从零开始 学习 自然语言 处理 NLP 基础 准备 0 
3 机器学习 笔记 降 维 与 度量 学习 10 4 
机器学习 笔记 聚 类 9 5 机器学习 笔记 集成 学习 
8 6 机器学习 笔记 贝叶斯 分类器 7 机器学习 算法 工程师 
一个 用心 的 公众 号 长按 识别 加 关注 进 
群 学习 得 帮助 你 的 关注 我们 的 热度 
我们 一定 给 你 学习 最大 的 帮助 你 点 
的 每个 赞 我 都 认真 当成 了 喜欢 基于 
规则 的 自然 语言 处理 在 自然 语言 处理 刚 
开始 发展 的 时候 是 处在 领先 的 地位 因为/c 
人们/n 觉得/v 自然语言/l 处理/v 的/uj 过程/n 和/c 人类/n 学习/v 和/c 
认知/v 一门/m 语言/n 的/uj 过程/n 应该/v 相似/v 的/uj 所以 大量 
的 研究员 也是 从 这个 点 出发 来 研究 自然语言 
处理 但是 后来 发现 根据 规则 开发 出来 的 系统 
不是 特别 好 有的 研究员 就 换 一个 方向 觉得 
应该 从 基于 统计 的 角度 来 研究 自然语言 处理 
其实 基于 统计 的 方法 有点像 通信 的 编码 与 
解码 原始 语句 就是 一个 已经 编码 的 信息 然后 
我们 根据 统计 的 概率 解码 处 原始 信息 这个 
方法 是 自然 语言 处理 的 效果 有了/nr 很大 的 
提升 慢慢地 该 方法 也 占据 了 该 方向 的 
主导 地位 基于 统计 的 自然 语言 处理 为什么 能 
发展 这么 好 有它的/nr 历史 原因 互联网 的 高速 发展 
使得 人们 可以 得到 丰富 的 语料 而 完善 的 
语料库 是 统计 自然语言 处理 成功 地 一个 先决条件 还有 
就是 硬件 的 发展 也给 该 方向 提供 了 强大 
的 硬件 支撑 虽然 目前 基于 统计 的 自然 语言 
处理 比 基于 规则 的 自然 语言 处理 有 很大 
优势 但 是 历史 是 发展 的 我们 谁 也 
不能 保障 在 将来 的 某一天 基于 规则 的 自然 
语言 处理 会 不会 因为 具备 了 某种 条件 而 
再一次 重 获得 历史 的 主角 自然语言 处理 是 计算机 
科学 领域 与 人工智能 领域 中 的 一个 重要 方向 
自然语言 工具箱 NLTK Natural Language Toolkit 是 一个 基于 Python 
语言 的 类库 它 也是 当前 最为 流行 的 自然 
语言 编程 与 开发工具 在 进行 自然语言 处理 研究 和 
应用 时 恰当 利用 NLTK 中 提供 的 函数 可以 
大 幅度 地 提高 效率 本文 就 将 通过 一些 
实例 来向 读者 介绍 NLTK 的 使用 开发环境 我 所 
使用 的 Python 版本 是 最新 的 3 . 5.1 
NLTK 版本 是 3.2 Python 的 安装 不在 本文 的 
讨论 范围内 我们 略去 不表 你 可以 从 NLTK 的 
官网 上 http / / www . nltk . org 
/ 获得 最新 版本 的 NLTK Anyway 使用/v pip/w 指令/n 
来/v 完成/v NLTK/w 包的/nr 下载/v 和/c 安装/v 无疑/d 是/v 最/d 
简便/a 的/uj 方法/n 当然 当 你 完成 这 一步 时 
其实 还 不够 因为 NLTK 是由 许多 许多 的 包来/nr 
构成 的 此时 运行 Python 并 输入 下面 的 指令 
当然 第一条 指令 还是 要 导入 NLTK 包 import nltk 
nltk . download 然后 Python Launcher 会 弹出 下面 这个 
界面 建议 你 选择 安装 所有 的 Packages 以 免去 
日后 一而再 再而三 的 进行 安装 也为 你 的 后续 
开发 提供 一个 稳定 的 环境 某些 包的/nr Status 显示 
out of date 你 可以 不必 理会 它 基本 不 
影响 你 的 使用 与 开发 既然 你 已经 安装 
成功 我们 来 小试牛刀 一下 当然 本文 涉及 的 主要 
任务 都是/nr 自然 语言 处理 中 最 常用 最 基础 
的 pre processing 过程 结合 机器 学习 的 高级 应用 
我们 会 在 后续 文章 中 再 进行 介绍 1 
Sentences Segment 分句 也 就是说 我们 手头 有 一段 文本 
我们 希望 把 它 分成 一个 一个 的 句子 此时 
可以 使用 NLTK 中的 punkt sentence segmenter 来看 示例代码 sent 
_ tokenizer = nltk . data . load tokenizers / 
punkt / english . pickle paragraph = The first time 
I heard that song was in Hawaii on radio . 
. . . I was just a kid and loved 
it very much What a fantastic song sentences = sent 
_ tokenizer . tokenize paragraph sentences The first time I 
heard that song was in Hawaii on radio . I 
was just a kid and loved it very much What 
a fantastic song 由此 我们 便把 一段话 成功 分句 了 
2 Tokenize sentences 分词 接下来 我们 要 把 每个 句话 
再 切割成 逐个 单词 最 简单 的 方法 是 使用 
NLTK 包 中的 WordPunct tokenizer 来看 示例代码 from nltk . 
tokenize import W o r d P u n c 
t T o k e n i z e r 
sentence = Are you old enough to remember Michael Jackson 
attending . . . the Grammys with Brooke Shields and 
Webster sat on his lap during the show words = 
W o r d P u n c t T 
o k e n i z e r . tokenize 
sentence words Are you old enough to remember Michael Jackson 
attending the Grammys with Brooke Shields and Webster sat on 
his lap during the show 我们 的 分词 任务 仍然 
完成 的 很好 除了 WordPunct tokenizer 之外 NLTK 中 还 
提供 有 另外 三个 分词 方法 T r e e 
b a n k W o r d T o 
k e n i z e r P u n 
k t W o r d T o k e 
n i z e r 和Wh/nr i t e s 
p a c e T o k e n i 
z e r 而且 他们 的 用法 与 WordPunct tokenizer 
也 类似 然而 显然 我们 并不 满足 于此 对于 比较 
复杂 的 词 型 WordPunct tokenizer 往往 并 不胜任 此时 
我们 需要 借助 正则表达式 的 强大 能力 来 完成 分词 
任务 此时 我 所 使用 的 函数 是 regexp _ 
tokenize 来看 下面 这段话 text = That U . . 
A . poster print costs $ 12.40 . . . 
目前 市面 上 可以 参考 的 在 Python 下 进行 
自然语言 处理 的 书籍 是由 Steven Bird Ewan Klein Edward 
Loper 编写 的 Python 自然语言 处理 但是 该书 的 编写 
时间 距今 已有 近 十年 的 时间 由于 软件包 更新 
等 语言 在 新 环境 下 进行 开发 时 书中 
的 某些 代码 并不能 很 正常 的 运行 最后 我们 
举 一个 书中 代码 out of date 的 例子 对 
上面 这就 话 进行 分词 并给 出 相应 的 解决 
办法 首先 来 看书 中的 一段 节录 text = That 
U . . A . poster print costs $ 12.40 
. . . pattern = r x # set flag 
to allow verbose regexps . . . A Z \ 
. + # abbreviations e . g . U . 
. A . . . . | \ w + 
\ w + * # words with optional internal hyphens 
. . . | \ $ \ d + \ 
. \ d + % # currency and percentages e 
. g . $ 12.40 82% . . . | 
\ . \ . \ . # ellipsis . . 
. | . _ ` # these are separate tokens 
includes . . . nltk . regexp _ tokenize text 
pattern 我们 预期 得到 输出 应该 是 这样 的 That 
U . . A . poster print costs $ 12.40 
. . . 但是 我们 实际 得到 的 输出 却 
是 这样 的 注意 我们 所 使用 的 NLTK 版本 
A . print . 40 会 出现 这样 的 问题 
是 由于 nltk . internals . compile _ regexp _ 
to _ noncapturing 在 V 3.1 版本 的 NLTK 中 
已经 被 抛弃 尽管 在 更早 的 版本 中 它 
仍然 可以 运行 为此 我们 把 之前 定义 的 pattern 
稍作 修改 pattern = r x # set flag to 
allow verbose regexps A Z \ . + # abbreviations 
e . g . U . . A . | 
\ d + \ . \ d + % # 
numbers incl . currency and percentages | \ w + 
\ w + * # words w / optional internal 
hyphens / apostrophe | \ . \ . \ . 
# ellipsis | . _ ` # special characters with 
meanings 再次 执行 前面 的 语句 便会 得到 nltk . 
regexp _ tokenize text pattern That U . . A 
. poster print costs 12.40 . . . 以上 便是 
我们 对 NLTK 这个 自然语言 处理 工具包 的 初步 探索 
日后 主页 君 将 结合 机器学习 中 的 方法 再 
来 探讨 一些 更为 深入 的 应用 最后 我 想 
说 Python 自然语言 处理 仍然 是 当前 非常 值得 推荐 
的 一本 讲述 利用 NLTK 和 Python 进行 自然语言 处理 
技术 的 非常 值得 推荐 的 书籍 自然语言 处理 实际 
开发 一个 可以 识别 自然 语言 的 翻译 应用 必不可少 
的 开发 环境 Eclipse4 . 5 + JDK1 . 7 
+ WindowBuilder 插件 其他 资源 语义 平台 OLAMI 源代码 https 
/ / github . com / volcanoliu / TranslateDemo 可执行文件 
http / / download . csdn . net / detail 
/ u011211290 / 9888544 百度 云 地址 http / / 
pan . baidu . com / s / 1bQhH4U1 . 
界面 及 使用 这里 介绍 一下 页面 整体 分为 三 
个 部分 最 上面 的 是 对话框 中间 的 是 
回答 框 最 下面 的 比较 大 的 显示 的 
是从 语义 平台 取得 的 语义 数据 使用 方式 把 
需要 理解 的 语句 输入 到 对话框 中 点击 发送 
就 可以 得到 结果 返回 结果 2 . 代码 简介 
这里 先 整体 简单 介绍 一下 NLPJSON . java 里面 
是 拿到 语义 返回 JSON 数据 的 关键字 APIJSON . 
java 里面 是 拿到 翻译 返回 JSON 数据 的 关键字 
ApiLanguage . java 里面 是 翻译 API 接口 需要 的 
各国 语言 的 缩写 Encrypt . java 功能 是 加密 
字符串 里面 只有 MD5 加密 的 方法 Format . java 
功能 是 整理 JSON 内容 用于 输出 GetModifier . java 
功能 是从 OLAMI 提供 的 API 接口 拿到 语义 HttpRequestUtils 
. java 功能 是 发送 HTTP 请求 获得 HTTP 返回 
的 数据 MainWindow . java 是 主程序 做 的 是 
窗口 的 建立 和 主流程 的 控制 ModifierProcess . java 
功能 是 处理 语义 TranslateByAPI . java 功能 是从 翻译 
API 接口 拿到 翻译 的 结果 3 . 核心 代码 
3.1 MainWindows . javaButton btnNewButton = new Button translateShell SWT 
. NONE translateShell . setDefaultButton btnNewButton btnNewButton . setLocation 319 
91 btnNewButton . setSize 80 27 btnNewButton . a d 
d e l e c t i o n L 
i s t e n e r new SelectionAdapter { 
@ Override public void widgetSelected SelectionEvent e { NLPText . 
setText String src = inputText . getText if src = 
= null | | src . length = = 0 
{ answerText . setText 你 还 没有 输入 内容 return 
} / / 把 string 用 接口 拿到 语义 JSONObject 
nlp = GetModifier . GetNLI src NLPText . setText Format 
. formatJson nlp . toString / / 处理 语义 String 
answer = ModifierProcess . NLPProcess nlp answerText . setText answer 
if errorFlag = = 1 { answerText . setText errorMessage 
} else if errorFlag = = 2 { answerText . 
setText 遇到 了 错误 但 这 不是 我 的 锅 
} resetError } } btnNewButton . setText 发送 这里 是 
主程序 的 处理 流程 发送 按钮 做了 监听 先 处理 
输入 内容 然后 把 内容 发送到 语义 平台 拿到 语义 
然后 去 处理 语义 输出 结果 3.2 GetModifier . javaprotected 
static JSONObject GetNLI String src { JSONObject data = new 
JSONObject data . put input _ type 1 data . 
put text src JSONObject send = new JSONObject send . 
put data _ type stt send . put data data 
/ / 时间戳 long timestamp = new Timestamp System . 
c u r r e n t T i m 
e M i l l i s . getTime / 
/ 签名 String sign = appSecret + api = + 
api + appkey = + appKey + timestamp = + 
timestamp + appSecret sign = Encrypt . MD5 sign / 
/ 参数 Map String String post _ data = new 
HashMap String String post _ data . put appkey appKey 
post _ data . put api api post _ data 
. put timestamp String . valueOf timestamp post _ data 
. put sign sign post _ data . put rq 
send . toString post _ data . put cusid cusid 
return HttpRequestUtils . httpPost apiUrl post _ data } 3.3 
ModifierProcess . javaprotected static String NLPProcess JSONObject nlpJson { String 
status = nlpJson . getString NLPJSON . JSON _ STATUS 
if status = = null | | NLPJSON . STATUS 
_ OK . equalsIgnoreCase status { MainWindow . setErrorFlag 2 
return } / / Get info from JSON String result 
= String modifier = null String src _ language = 
null String dst _ language = null String src _ 
code = null String dst _ code = null String 
content = null String resultLanguage = null try { JSONObject 
nli = nlpJson . getJSONObject NLPJSON . JSON _ DATA 
. getJSONArray NLPJSON . DATA _ NLI . getJSONObject 0 
JSONObject descobj = nli . getJSONObject NLPJSON . NLI _ 
DESCOBJ if 0 . equals descobj . getString NLPJSON . 
DESCOBJ _ STATUS { MainWindow . setErrorFlag 1 MainWindow . 
setErrorMessage descobj . getString NLPJSON . DESCOBJ _ RESULT return 
} JSONObject semantic = nli . getJSONArray NLPJSON . NLI 
_ SEMANTIC . getJSONObject 0 modifier = semantic . getJSONArray 
NLPJSON . SEMANTIC _ MODIFIER . getString 0 JSONArray slotsArray 
= semantic . getJSONArray NLPJSON . SEMANTIC _ SLOTS if 
slotsArray = null & & slotsArray . size 0 { 
Map String String slotsMap = new HashMap String String for 
int i = 0 i slotsArray . size i + 
+ { JSONObject slots = slotsArray . getJSONObject i String 
name = slots . getString NLPJSON . SLOTS _ NAME 
String value = slots . getString NLPJSON . SLOTS _ 
VALUE slotsMap . put name value } src _ language 
= slotsMap . get NLPJSON . _ SRCLANGUAGE dst _ 
language = slotsMap . get NLPJSON . _ DSTLANGUAGE content 
= slotsMap . get NLPJSON . _ CONTENT } } 
catch Exception e { MainWindow . setErrorFlag 2 return } 
/ / Process the info if modifier = = null 
| | modifier . length 1 { MainWindow . setErrorFlag 
2 return } / / 问 能力 if modifier . 
equalsIgnoreCase NLPJSON . M _ CAN { if dst _ 
language = null & & dst _ language . length 
0 { String language = ApiLanguage . language . get 
dst _ language if language = null & & language 
. length 0 { MainWindow . r e s e 
t L a s t L a n g u 
a g e MainWindow . s e t L a 
s t D s t L a n g u 
a g e dst _ language result = 没问题 return 
result } result = 我 还 不懂 + dst _ 
language + 但 我会 慢慢 学习 的 return result } 
result = 我 可以 翻译 问 我 吧 但 最好 
不要 太难 哟 return result } else if modifier . 
equalsIgnoreCase NLPJSON . M _ CANDOWHICH { / / 问 
能 翻译 多少 种 语言 / / map . keyset 
get 5 languages Set String set = ApiLanguage . language 
. keySet set . remove ApiLanguage . AUTO String language 
= set . toArray new String set . size int 
start = 0 for int i = 0 i o 
u t p u t L a n g u 
a g e N u m b e r i 
+ + { Random random = new Random start + 
= random . nextInt language . length o u t 
p u t L a n g u a g 
e N u m b e r + i start 
+ 1 result + = language start + } MainWindow 
. r e s e t L a s t 
L a n g u a g e result = 
我会 翻译 + result . substring 0 result . length 
1 + . . . 还有 好 多种语言 return result 
} else if modifier . equalsIgnoreCase NLPJSON . M _ 
TRANSLATE { / / 翻译 内容 / / If content 
exist get languages translate it by translateApi if content = 
null & & content . length 0 { if src 
_ language = = null | | src _ language 
. length = = 0 { String lastSrcLanguage = MainWindow 
. g e t L a s t r c 
L a n g u a g e if lastSrcLanguage 
= = null | | lastSrcLanguage . length = = 
0 { src _ code = ApiLanguage . language . 
get ApiLanguage . AUTO } else { src _ code 
= ApiLanguage . language . get lastSrcLanguage } } else 
{ src _ code = ApiLanguage . language . get 
src _ language if src _ code = = null 
| | src _ code . length = = 0 
{ result = 我 还 没有 学会 + src _ 
language + 等 我 学会 之后 你 再问 我 吧 
return result } } if dst _ language = = 
null | | dst _ language . length = = 
0 { String lastDstLanguage = MainWindow . g e t 
L a s t D s t L a n 
g u a g e if lastDstLanguage = = null 
| | lastDstLanguage . length = = 0 { dst 
_ code = ApiLanguage . language . get ApiLanguage . 
ENGLISH resultLanguage = ApiLanguage . ENGLISH } else { dst 
_ code = ApiLanguage . language . get lastDstLanguage resultLanguage 
= lastDstLanguage } } else { dst _ code = 
ApiLanguage . language . get dst _ language resultLanguage = 
dst _ language if dst _ code = = null 
| | dst _ code . length = = 0 
{ result = 我 还 没有 学会 + dst _ 
language + 等 我 学会 之后 你 再问 我 吧 
return result } } MainWindow . s e t L 
a s t r c L a n g u 
a g e src _ language MainWindow . s e 
t L a s t D s t L a 
n g u a g e dst _ language String 
transResult = TranslateByAPI . translateProcess content src _ code dst 
_ code if transResult . equals content { dst _ 
code = ApiLanguage . language . get ApiLanguage . CHINESE 
resultLanguage = ApiLanguage . CHINESE transResult = TranslateByAPI . translateProcess 
content src _ code dst _ code } if . 
equals transResult { return } result = + content + 
翻译成 + resultLanguage + 的 结果 是 + transResult + 
return result } else { if src _ language = 
null & & src _ language . length 0 & 
& dst _ language = null & & dst _ 
language . length 0 { MainWindow . s e t 
L a s t r c L a n g 
u a g e src _ language MainWindow . s 
e t L a s t D s t L 
a n g u a g e dst _ language 
} result = 你 说 我 来 翻译 return result 
} } / / 前面 没有 处理 设置 错误 标志 
MainWindow . setErrorFlag 2 return result } 从 语义 数据 
拿到 语义 信息 然后 判断 返回 的 modifier 和 slot 
内容 作出 处理 3.4 TranslateByAPI . javaprotected static String translateProcess 
String src String from String to { if src = 
= null | | src . length = = 0 
{ MainWindow . setErrorFlag 2 return } else if src 
. length 1500 { MainWindow . setErrorFlag 1 MainWindow . 
setErrorMessage 翻译 的 文字 太多 了 return } if from 
= = null { MainWindow . setErrorFlag 1 MainWindow . 
setErrorMessage 输入 的 文字 看不懂 啊 return } if to 
= = null { MainWindow . setErrorFlag 1 MainWindow . 
setErrorMessage 我 还不会 你 想要 的 语言 return } int 
salt = new Random . nextInt 10000 / / 签名 
String sign = Encrypt . MD5 APPID + src + 
salt + key if sign = = { return } 
Map String String params = new HashMap String String params 
. put appid APPID params . put salt String . 
valueOf salt params . put sign sign params . put 
from from params . put to to params . put 
q src JSONObject translate = HttpRequestUtils . httpPost apiUrl params 
String errorCode = translate . getString APIJSON . ERROR if 
errorCode = null & & errorCode . length 0 { 
MainWindow . setErrorFlag 2 return } JSONObject transResult = null 
try { transResult = translate . getJSONArray APIJSON . TRANS 
_ RESULT . getJSONObject 0 } catch Exception e { 
MainWindow . setErrorFlag 2 return } if transResult = = 
null { MainWindow . setErrorFlag 2 return } String result 
= transResult . getString APIJSON . RESULT _ DST if 
result = = null { MainWindow . setErrorFlag 2 return 
} return result } 通过 翻译 API 来 翻译 内容 
API 的 调用 方法 大同小异 到此为止 翻译 工作 就 完成 
了 4 . 总结 总体 来说 功能 简单 代码 简单 
但是 能 做到 的 事情 就 很 强大 总体 还是 
归功于 语义 开放 平台 的 语义 解析 其他 说明 GetModifier 
. java   里面 所 使用 到 的 调用 语义 
API 接口 的 关键 信息 开发者 需 自行 在 OLAMI 
官网 注册 生成 并 更换 这样 开发 者 可以 自己 
定义 所需 的 语法 TranslateByAPI . java   里 所 
使用 到 的 调用 翻译 API 接口 的 关键 信息 
开发者 也需 自行 更换 目前 某 度 的 翻译 API 
处于 量小 免费 量大 收费 的 模式 开发 者 使用 
的话 还是 不用 担心 超出 免费 额度 的 情况 但是 
如果 发现 有被/nr 乱用 的 现象 作者 将 关闭 翻译 
API 的 接口 语义 开放平台 1 . 注册 登陆 OLAMI 
官网 2 . 点击 账号 出现 下拉菜单 点击 NLI 系统 
进入 语法 编辑 系统 可以 选择 导入 已 有的 模块 
也 可以 创建 模块 自定义 语法 3 . 导入 translate 
语义 模块 4 . 进入 语义 模块 点击 发布 进入 
发布页面 点击 发布 按钮 启用 刚才 导入 的 语法 5 
. 回到 官网 点击 账号 出现 菜单 点击 应用 管理 
进入 应用 管理 页面 点击 创建 新 应用 新建 一个 
应用 6 . 配置 应用 点击 配置 模块 勾选/nr 模块 
选择 这个 应用 需要 支持 的 模块 第一 个 选项卡 
是 自定义 的 模块 第二个 选项卡 是 系统 自带 模块 
7 . 查看 Key 可以 查询 使用 API 调用 应用 
所 需要 的 key 测试 可以 测试 应用 的 语法 
更多 的 文档 可以 参照 OLAMI 文档 中心 最后 这 
里 只是 做 了 一个 可以 运行 的 demo 如果说 
能把 这些 功能 整合 到 个人 网站 或者 公众 号 
微信 小 程序 之中 就能 极大 的 提高 逼 格 
闲聊 智能 对话 微信 小 程序 详解 欢快 的 小 
程序开发 之路 微信 小 程序 IOS 端 showLoading 之后 showToast 
不 显示 * * 优秀 自然语言 理解 博客 文章 推荐 
* * 根据 OLAMI 平台 开发 的 日历 Demo 用 
olami 开放 语义 平台 做 汇率 换算 应用 自然语言 处理 
实际 开发 用 语义 开放平台 olami 写 一个 翻译 的 
应用 自定义 java . awt . Canvas 趣味 聊天 微信 
小 程序 + OLAMI 自然语言 API 接口 制作 智能 查询 
工具 快递 聊天 日历 等 热门 自然语言 理解 和 语音 
API 开发平台 对比 使用 OLAMI SDK 和讯 飞 语音合成 制作 
一个 语音 回复 的 短信 小助手 告诉 你 如何 使用 
OLAMI 自然语言 理解 开放平台 API 制作 自己 的 智能 对话 
助手 斯坦福 自然语言 处理 工具 python 环境 配置 1 . 
简介 Stanford nlp group 是 世界 知名 的 自然 语言 
处理 研究组 该组 的 研究 内容 涵盖 了 从 基本 
的 计算 语言 原理 研究 到 NLP 的 关键 应用 
技术 其中 该组 所 开发 的 coreNLP 工具 被 广泛 
应用 该 工具 提供 了 分词 词性 标注 语法分析 共 
指 消解 命名 实体 识别 等 操作 Stanford coreNLP 源码 
使用 Java 编写 而成 但 一些 程序员 将 coreNLP 进行 
了 封装 从而 可以 便于 在 其他 语言 环境 下 
使用 该 工具 本文 对 自己 配置 coreNLP 的 python 
环境 的 过程 进行 总结 2 . 过程 首先 需要 
下载 Stanford coreNLP 的 Java 源码 该 代码 可以 在 
斯坦福 NLP 组 的 下载 页面 进行 下载 见 此处 
标准 的 coreNLP 为 jar 格式 可以 通过 Java 程序 
引入 命令行 等 方式 进行 调用 若是 需要 处理 中文 
则 还需 在 该 页面 上 下载 对应 的 中文 
处理 jar 文件 之后 由于 我们 要 使用 python 调用 
coreNLP 该 主页 上 还 提供 了 其他 语言 使用 
Stanford coreNLP 的 库 见 此处 如 我们 在 python 
环境 下 使用 coreNLP 则 需要 安装 一个 可以 调用 
coreNLP 源码 的 库 可选 的 库 有 很多 包括 
pycorenlp stanfordcorenlp corenlp pywrap 等等 每个 库 在 GitHub 上都 
有 相应 的 说明 参考 其 介绍 即可 我 本次 
使用 的 是 stanfordcorenlp 库 见 此处 使用 pip 安装 
好 后 按照 参考 文档 测试运行 即 完成 了 配置 
# Simple usage from stanfordcorenlp import StanfordCoreNLP nlp = StanfordCoreNLP 
r G / JavaLibraries / stanford corenlp full 2016 10 
31 / sentence = Guangdong University of Foreign Studies is 
located in Guangzhou . print Tokenize nlp . word _ 
tokenize sentence print Part of Speech nlp . pos _ 
tag sentence print Named Entities nlp . ner sentence print 
Constituency Parsing nlp . parse sentence print Dependency Parsing nlp 
. dependency _ parse sentence ` 内容简介 自然语言 处理 是 
人工智能 的 一个 重要 应用 领域 也是 新一代 计算机 必须 
研究 的 课题 它 的 主要 目的 是 克服 人机对话 
中 的 各种 限制 使 用户 能用/nr 自己 的 语言 
与 计算机 对话 因此 本 研究 报告 对 自然 语言 
进行 了 简单 梳理 包括 以下 内容 自然语言 处理 概念 
自然语言 处理 研究 情况 自然语言 处理 领域 专家 介绍 自然语言 
处理 的 应用 及 趋势 预测 作者简介 AMiner 平台 由 
清华大学 计算机系 研发 拥有 我国 完全 自主 知识产权 平台 包含 
了 超过 2.3亿 学术论文 / 专利 和 1.36亿 学者 的 
科技 图谱 提供 学者 评价 专家 发现 智能 指派 学术 
地图 等 科技情报 专业化 服务 系统 2006年 上线 吸引 了 
全球 220 个 国家 / 地区 1000 多万 独立 IP 
访问 数据 下载量 230 万次 年度 访问量 超过 1100万 成为/v 
学术/n 搜索/v 和/c 社会/n 网络/n 挖掘/v 研究/vn 的/uj 重要/a 数据/n 
和/c 实验/vn 平台/n 系统 相关 核心技术 申请专利 50 余项 发表 
论文 500 余篇 其中 SCI 论文 110篇 编著 英文 论著 
两部 Google 引用 超过 11000次 SCI 他 引 超过 2200次 
项目 成果 及 核心 技术 应用于 工程院 科技部 自然 基金委 
华为 腾讯 阿里巴巴 百度 等 国内外 20多 家 企事业 单位 
为 各 单位 的 系统 建设 及 产品 升级 提供 
了 重要 数据 及 技术 支撑 AMiner 唯一 官方 微信 
公众 号 学术 头条 ID SciTouTiao AMiner 官方 网站 https 
/ / www . aminer . cn / 版权 声明 
AMiner 研究 报告 提供 给 订阅 用户 使用 仅限于 用户 
内部 使用 未获得 AMiner 团队 授权 任何人 和 单位 不 
得以 任何 方式 在 任何 媒体 上 包括 互联网 公开 
发布 复制 且 不得 以 任何 方式 将 研究 报告 
的 内容 提供 给 其他 单位 或 个人 使用 如 
引用 刊发 需 注明 出处 为 AMiner . org 且 
不得 对本 报告 进行 有悖 原意 的 删节 与 修改 
AMiner 研究 报告 是 基于 AMiner 团队 及其 研究员 认可 
的 研究 资料 所有 资料 源自 AMiner 后台程序 对 大 
数据 的 自动 分析 得到 本 研究 报告 仅 作为 
参考 AMiner 团队 不 保证 所 分析 得到 的 准确性 
和 完整性 也不 承担 任何 投资 者 因 使用 本 
产品 与 服务 而 产生 的 任何 责任 本书 内容摘要 
自然语言 处理 是 人工智能 的 一个 重要 应用 领域 也是 
新一代 计算机 必须 研究 的 课题 它 的 主要 目的 
是 克服 人机对话 中 的 各种 限制 使 用户 能用/nr 
自己 的 语言 与 计算机 对话 因此 本 研究 报告 
对 自然 语言 进行 了 简单 梳理 包括 以下 内容 
自然语言 处理 概念 首先 对 自然 语言 处理 进行 定义 
接着 对 自然 语言 的 发展 历程 进行 了 梳理 
对 我国 自然语言 处理 现状 进行 了 简单 介绍 对 
自然 语言 处理 业界 情况 进行 介绍 自然语言 处理 研究 
情况 依据 2016年 中文信息 学会 发布 的 中文 信息 处理 
发展 报告 对 自然 语言 处理 研究 中 的 重要 
技术 进行 介绍 自然语言 处理 领域 专家 介绍 利用 AMiner 
大 数据 对 自然 语言 处理 领域 专家 进行 深入 
挖掘 对 国内外 自然语言 处理 知名 实验室 及其 主要 负责人 
进行 介绍 自然语言 处理 的 应用 及 趋势 预测 自然语言 
处理 在 现实 生活 中 应用 广泛 目前 的 应用 
集中 在 语言学 数据处理 认知科学 以及 语言工程 等 领域 在 
介绍 相关 应用 的 基础 上 对 机器 翻译 未来 
的 发展 趋势 做出 了 相应 的 预测 第一章 概述 
篇 第二章 技术篇 第三章 技术篇 第四章 应用 篇 第五章 趋势 
篇 参考文献 阅读 全文 http / / gitbook . cn 
/ gitchat / geekbook / 5 b 9 8 8 
b 4 e c a 9 9 1 0 6 
5 4 c 0 8 2 3 f 5 博主 
github https / / github . com / MichaelBeechan 博主 
CSDN https / / blog . csdn . net / 
u011344545 = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = 概念 篇 https / / 
blog . csdn . net / u011344545 / article / 
details / 89525801 技术篇 https / / blog . csdn 
. net / u011344545 / article / details / 89526149 
人才篇 https / / blog . csdn . net / 
u011344545 / article / details / 89556941 应用 篇 https 
/ / blog . csdn . net / u011344545 / 
article / details / 89574915 下载 链接 https / / 
download . csdn . net / download / u011344545 / 
11147085 = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = 清华 AMiner 团队 AMiner . 
org 从 知识 产业 角度 来看 自然语言 处理软件 占有 重要 
的 地位 专家系统 数据库 知识库 计算机 辅助 设计 系统 CAD 
计算机 辅助 教学 系统 Cal 计算机 辅助 决策 系统 办公室 
自动化 管理 系统 智能 机器人 等 全都/nr 需要/v 自然语言/l 做/v 
人机界面/n 长远 看来 具有 篇章 理解 能力 的 自然 语言 
理解 系统 可 用于 机器 自动 翻译 情报检索 自动 标引 
及 自动 文摘 等 领域 有着 广阔 的 应用 前景 
随着 自然语言 处理 研究 的 不断 深入 和 发展 应用 
领域 越来越 广 文本 方面 的 应用 主要 有 基于 
自然语言 理解 的 智能 搜索 引擎 和 智能 检索 智能 
机器 翻译 自动 摘 要与 文本 综合 文本 分类 与 
文件 整理 智能 自动 作文 系统 自动 判卷 系统 信息 
过滤 与 垃圾 邮件 处理 文学 研究 与 古文 研究 
语法 校对 文本 数据挖掘 与 智能 决策 以及 基于 自然 
语言 的 计算机 程序 设计 等 语音 方面 的 应用 
主要 有 机器 同声 传译 智能 远程教学 与 答疑 语音 
控制 智能 客户服务 机器 聊天 与 智能 参谋 智能 交通 
信息 服务 ATIS 智能 解说 与 体育 新闻 实时 解说 
语音 挖掘 与 多媒体 挖掘 多媒体 信息 提取 与 文本 
转化 以及 对 残疾人 智能 帮助 系统 等 此外 建立 
在 自然 语言 处理 技术 基础 之上 的 心理学 认知 
学 哲学 混沌 学说 的 共同 发展 将使 人们 对 
智能 的 起源 问题 有 新的 认识 如果把 计算机 网络 
和 未来 的 网格 看作 是 由 机器 组成 的 
机器 社会 那么 一种 属于 机器 的 智能 可能会 因为 
人类 的 参与 以及 机器 社会中 各 元素 的 相互 
作用 而 自然 诞生 这样 机器 必将 能够 通过 图灵测试 
达到 会 思考 的 层次 而 有关 智能 机器 的 
研究 也 会 诞生 一系列 新的 领域 比如 机器 心理学 
和 机器 认知 学 等 其中 机器 心理学 主要 研究 
机器 的 心理 反应 和 意图 美国 圣迭戈 神经科学 研究所 
研制 的 机器人 DarwinV II 能够 根据 其 感知 对外部 
事物 进行 分类 并 根据 经验 和 知识 采取 相应 
的 对策 然而 机器 心理学 的 研究 不 能 局限 
于此 人们 还 需要 对 机器 的 意识 知觉 思想 
情感 情绪 创造力 机器 社会 机器 交流 等 方面 进行 
研究 而 这 一切 还 需要 计算机科学 心理学 神经 科学 
的 同步 发展 我们 选取 一些 自然 语言 处理 应用 
较为 频繁 的 场景 进行 介绍 1 知识图谱 知识图谱 能够 
描述 复杂 的 关联关系 它 的 应用 极为 广泛 最 
为人所知 的 就是 被 用在 搜索引擎 中 丰富 搜索 结果 
并为 搜索 结果 提供 结构化 结果 体现 关联 这 也是 
google 提出 知识图谱 的 初衷 同时 微软 小冰 苹果 siri 
等 聊天 机器 人中 也 加入 了 知识 图谱 的 
应用 IBM Watson 是 问答 系统 中 应用 知识图谱 较为 
典型 的 例子 按照 应用 方式 可以 将 知识 图谱 
的 应用 分为 语义搜索 知识 问答 以及 基于 知识 的 
大 数据分析 和 决策 等 语义搜索/i 利用/n 建立/v 大队/n 莫/d 
知识库/n 对/p 搜索/v 关键词/n 和/c 文档/n 内容/n 进行/v 语义/n 标注/v 
改善 搜索 结果 如 谷歌 百度 等 在 搜索 结果 
中 嵌入 知识图谱 知识 问答 是 基于 知识库 的 问答 
通过 对 提问 句子 的 语义分析 再 将其 解析 为 
结构化 的 询问 在 已有 的 知识 库 中 获取 
答案 在 大 数据 的 分析 和 决策 方面 知识图谱 
起到 了 辅助 作用 典型 应用 是 美国 Netflix 公司 
利用 其 订阅 用户 的 注册 信息 以及 观看 行为 
构建 的 知识图谱 反映出 英剧 版 纸牌屋 很 受欢迎 于是 
拍摄 了 美剧 纸牌屋 大受 追捧 2 机器翻译 机器 翻译 
是 自然 语言 处理 最为 人知 的 应用 场景 一般 
是 将 机器翻译 作为 某 个 应用 的 组成部分 例如 
跨语言 的 搜索 引流 等 目前 以 IBM 谷歌 微软 
为 代表 的 国外 科研 机构 和 企业 均 相继 
成立 机器翻译 团队 专门从事 智能 翻译 研究 如 IBM 于 
2009 年 9 月 推出 V i a V o 
i c e T r a n s l a 
t o r 机器 翻译 软件 为 自动化 翻译 奠定 
了 基础 2011 年 开始 伴随 着 语音识别 机器 翻译 
技术 DNN 深度 神经网络 技术 的 快速 发展 和 经济 
全球化 的 需求 口语 自动 翻译 研究 成为 当今 信息 
处理 领域 新 的 研究 热点 Google 于 2011 年 
1 月 正式 在其 Android 系统 上 推出 了 升级版 
的 机器 翻译 服务 微软 的 Skype 于 2014 年 
12 月 宣布 推出 实时 机器 翻译 的 预览版 支持 
英语 和 西班牙语 的 实时 翻译 并 宣布 支持 40 
多种 语言 的 文本 实时 翻译 功能 尤其 值得 之 
注意 的 是 在 一带 一路 这一 发展 背景 下 
合作 沟通 会 涉及 60 多个 国家 53 种 语言 
此时 机器 翻译 的 技术 应用 显得 尤为 重要 语言 
的 畅通 是 一带 一路 战略 得以 实施 的 重要 
基础 而 机器翻译 涉及到 语义分析 上下文 环境 等 诸多 挑战 
其 发展 道路 还有 很长 一段 路 要走 3 聊天 
机器 人 聊天 机器人 是 指 能 通过 聊天 app 
聊天 窗口 或 语音 唤醒 app 进行 交流 的 计算机 
程序 是 被 用来 解决 客户 问题 的 智能 数字化 
助手 其 特点 是 成本 低 高效 且 持续 工作 
例如 siri 小娜 等 对话 机器人 是 一个 应用 场景 
除此之外 聊天 机器人 在 一些 电商 网站 有着 很 实用 
的 价值 可以 充当 客服 角色 例如 京东 客服 jimi 
有 很多 基本 的 问题 其实 并不 需要 真 的 
联系 人工 客服 来 解决 通过 应用 智能 问答 系统 
可以 排除 掉 大量 的 用户 问题 比如 商品 的 
质量 投诉 商品 的 基本 信息 查询 等 程式 化 
问题 在 这些 特定 的 场景 中 特别 是 会被 
问到 高度 可 预测 的 问题 中 利用 聊天 机器人 
可以 节省 大量 的 人工 成本 4 文本 分类 文本 
分类 是 指 根据 文档 的 内容 或者 属性 将 
大量 的 文档 归到 一个 或 多个 类别 的 过程 
这一 技术 的 关键 问题 是 如何 构建 一个 分类 
函数 或 分类 模型 并 利用 这一 分类 模型 将 
未知 文档 映 射到 给定 的 类别 空间 按照 其 
领域 分 类 不同 的 期刊 新闻报道 甚至 多 文档 
分类 也是 可能 的 文本 分类 的 一个 重要 应用 
之处 是 垃圾 电子邮件 检测 除此之外 腾讯 新浪 搜狐 之类 
的 门户 网站 每天 产生 的 信息 分 繁 杂多 
依靠 人工整理 分类 是 一项 耗时 巨大 的 工作 且 
很 不现实 此时 文本 分类 技术 的 应用 就 显得 
极为 重要 5 搜索引擎 自然语言 处理 技术 例如 词义 消 
歧 句法分析 指代 消解 等 技术 在 搜索引擎 中 常常 
被 使用 搜索引擎 的 职责 不 单单 是 帮助 用户 
找到 答案 还能 帮助 用户 找到 所求 连接 人与 实体 
世界 的 服务 搜索引擎 最 基本 的 模式 是 自动化 
地 聚合 足够 多 的 内容 对之 进行 解析 处理 
和 组织 响应 用户 的 搜索 请求 找到 对应 结果 
返回 每 一个 环节 都 需要 用 到 自然 语言 
处理 用 百度 举例 比如 用户 可以 搜 天气 日历 
机票 及 汇率 这样 的 模糊 需求 会 直接 在 
搜索 结果 呈现 结果 用户 还 可以 搜索 范冰冰 演过 
的 电视剧 这样 的 复杂 问题 百度 都 可以 准确 
地 回答 一方面 有了 自然语言 处理 技术 才 使得 搜索引擎 
能够 快速 精准 的 返回 用户 的 搜索 结果 几乎 
所有 的 自然 语言 处理 技术 都在/nr 搜索引擎 中有 应用 
的 影子 另一方面 搜索引擎 例如 谷歌 商业 帝国 和 百度 
巨头 在 商业 上 的 成功 也 促进 了 自然 
语言 处理 技术 的 进步 6 推荐 系统 第一 个 
推荐 系统 是 1992 年 Goldberg 提出 的 Tapestry 这 
是 一个 个性化 邮件 推荐 系统 第一 次 提出 了 
协同 过滤 的 思想 利用 用户 的 标注 和 行为 
信息 对 邮件 进行 重 排序 推荐 系统 依赖 数据 
算法 人机交互 等 环节 的 相互 配合 应用 了 数据挖掘 
技术 信息检索 技术 以及 计算 统计学 等 技术 使用 推荐 
系统 的 目的 是 联系 用户 和 信息 帮助 用户 
发现 对 自己 有 价值 的 信息 同时 让 信息 
能够 展示 在对 它 感兴趣 的 用户 面前 精准 推荐 
用来 解决 信息 过载 和 用户 无 明确 需求 的 
问题 推荐 系统 在 音乐 电影 的 推荐 电子商务 产品 
推荐 个性化 阅读 社交 网络 好友 推荐 等 场景 发挥 
着 重要 的 作用 美国 Netflix 2/3 的 电影 是 
因为 被 推荐 而 观看 Google news 利用 推荐 系统 
提升 了 38% 的 点击率 Amazon 的 销售 中 推荐 
占 比 高达 35% 7 发展趋势 随着 深度 学习 时代 
的 来临 神经 网络 成为 一种 强大 的 机器学习 工具 
自然语言 处理 取得 了 许多 突破性 发展 情绪 分析 自动 
问答 机器 翻译 等 领域 都 飞速发展 下图 分别 是 
AMiner 计算出 的 自然 语言 处理 近期 热点 和 全球 
热点 通过 对 1994 2017 年间 自然语言 处理 领域 有关 
论文 的 挖掘 总结 出 二十 多年来 自然语言 处理 的 
领域 关键词 主要 集中 在 计算机 语言 神经网络 情感 分析 
机器翻译 词义 消 歧 信息提取 知识库 和 文本 分析 等 
领域 旨在 基于 历史 的 科研 成果 数据 的 基础 
上 对 自然 语言 处理 热度 甚至 发展 趋势 进行 
研究 图中 每个 彩色 分支 表示 一个 关键词 领域 其 
宽度 表示 该 关键词 的 研究 热度 各 关键词 在 
每一 年份 纵轴 的 位置 是 按照 这 一 时间 
点 上 所有 关键词 的 热度 高低 进行 排序 图 
14 显示 情绪 分析 词义 消 歧 知识库 和 计算机 
语言学 将 是 最近 的 热点 发展趋势 图 15 显示 
词义 消 歧 词义 理解 计算机 语言学 信息检索 和 信息 
提取 将 是 自然 语言 处理 全球 热点 参考文献 ［ 
1 ］ 中文 信息 处理 发展 报告 2016 ［ 2 
］ 李 涓子 侯磊 知识图谱 研究 综述 . J 山西大学 
学报 2017 ［ 3 ］ 冯志伟 . 机器翻译 研究 . 
M . 北京 中国 对外 翻译 出版社 . 2004 ［ 
4 ］ 冯志伟 . 自然语言 处理 的 形式 模型 M 
. 北京 中国 科学技术 大学 出版社 2010 ［ 5 ］ 
吴军 数学 之美 M . 北京 人民邮电出版社 2012 ［ 6 
］ 2006 2020 年 国家 信息化 发展 战略 Z 中共中央办公厅 
国务院办公厅 2006 ［ 7 ］ 刘奕群/nr 马少平/nr 洪涛 等 搜索 
引擎 技术 基础 M 北京 清华大学出版社 2010 ［ 8 ］ 
韩家 炜 等 数据挖掘 概念 与 技术 M 北京 机械 
工业 出版社 2012 一 定义 实现 人机 间 自然语言 通信 
意味着 要使 计算机 既 能理解 自然语言 文本 的 意义 也能 
以 自然 语言 文本 表达 给定 的 意图 思想 等 
前者 称为 自然语言 理解 后者 称为 自然语言 生成 自然语言 处理 
是 计算机 科学 领域 与 人工智能 领域 中 的 一个 
重要 方向 它/r 研究/vn 能/v 实现/v 人/n 与/p 计算机/n 之间/f 
用/p 自然/d 语言/n 进行/v 有效/a 通信/l 的/uj 各种/r 理论/n 和/c 
方法/n 自然语言 处理 是 一门 融 语言学 计算机科学 数学 于 
一体 的 科学 百度 百科 定义 二 自然语言 处理 的 
测试 要求 判断 计算机 是否 理解 了 某种 自然语言 具体 
的 判别 标准 至少 有 如下 四条 1 回答 问题 
机器 能 正确 回答 输入 文本 中 的 有关 问题 
2 文摘 生成 机器 有 能力 产生 输入 文本 的 
摘要 3 释义 机器 能用 不同 的 词语 和 句型 
来 复述 其 输入 文本 4 翻译 机 器具 有把 
一种 语言 源 语 翻译成 另一种 语言 目 标语 的 
能力 三 自然语言 处理 需要 解决 的 问题 自然 语言 
处理 的 首要 任务 是 将 语言 学 知识 在 
计算机 中 表示 出来 在此 基础 上 才能 完成 文本 
意义 的 计算 也 就是 文本 意义 的 解释 理解 
另外 歧义 消解 是 自然 语言 理解 的 一个 基本 
问题 因为/c 在/p 词/n 的/uj 层面/n 有一词/nr 多义/n 和/c 多词/i 
同义/n 的/uj 问题/n 一个 句子 在 不同 语言 环境 中 
也 有 不同 的 含义 对 篇章 的 理解 更是 
仁者见仁 智者见智 四 发展 历程 20 世纪 60 年代 以 
关键词 匹配 为 主流 的 早期 70 年代 以 语法 
语义分析 为 主流 的 中期 80 年代 开始 走向 实用化 
和 工程化 的 近期 自然语言 处理 主要 分为 两 大 
派别 1 基于 规则 的 语法 语义分析 2 基于 统计学 
方法 的 语料库 语言学 五 中文 语言 处理 存在 的 
障碍 1 输入 问题 汉字 不是 拼音文字 而是 象形文字 或 
音形 结合 的 文字 2 分词 问题 多数 中文 句子 
是 一长串 连续 的 汉字 而 不是 以 空格 或 
其他 分隔 标记 分开 的 单词 而且 词汇 缺少 明显 
的 形态 变化 3 句法分析 问题 六 关于 自动 分词 
自动 分词 是 汉语 特有 的 研究 课题 也是 中文 
信息 处理 技术 中 最 基础 最 重要 的 一个 
问题 就是 把 一个 句子 按照 其 中词 的 含义 
进行 切分 分词 单位 指 汉语 信息处理 使用 的 具有 
确定 的 语义 或 语法 功能 的 基本 单位 包括 
词 和 少量 词组 词 最小 的 能 独立 运用 
的 语言 单位 词组 由 两个 或 两个 以上 的 
词 按 一定 的 语法 规则 组成 以 表达 一定 
意义 的 语言 单位 注 为了 实现 机器 自动 分词 
首先 需要 建立 高效 准确 的 分词 词典 需要 有 
快速 的 字符串 匹配 算法 由于 汉语 的 广泛 的 
歧义 性 消 歧 算法 的 研究 显得 尤为 重要 
最后 还要 解决 未 登录 词 的 识别 问题 参考文献 
1 苗夺谦/nr 卫志 华./nr 中文 文本 信息 处理 的 原理 
与 应用 . 2007.2 百度 百科 自然语言 处理 https / 
/ baike . baidu . com / item / % 
E 8% 87% AA % E 7% 84% B 6% 
E 8% AF % AD % E 8% A 8% 
80% E 5% A 4% 84% E 7% 90% 86/365730 
fr = aladdin3 自然语言 处理 学习 相关 书籍 推荐 http 
/ / www . 52nlp . cn / % e 
4% b 9% a 6% e 7% b 1% 8d 
写 在前面 整理 一些 质量 不错 的 自然 语言 处理 
资源 备忘录 性质 持续 更新 ~ Awesome GithubAwesome Artificial Intelligence 
AI Awesome Deep LearningAwesome Machine Learningawesome nlpNLP progressYSDA Natural Language 
Processing coursenlp datasetsawesome chinese nlpAwesome Deep Learning for Natural Language 
Processing NLP awesome sentence embeddingAwesome Artificial Intelligence use casesBooks & 
Videos 数学 之美 吴军 博士 自然语言 处理 综论 统计 自然语言 
处理 宗 成庆 神经 网络 与 深度 学习 复旦 大学 
邱锡鹏/nr NLTK with Python 3 for Natural Language ProcessingCS224n Natural 
Language Processing with Deep Learning 这个 不用说 了 斯坦福 c 
s 2 2 4 n I n t r o 
d u c t i o n to Natural Language 
Processing 密歇根 大学 自然语言 处理 导论 Natural Language Processing 哥伦比亚大学 
NLPNatural Language Understanding and Computational Semantics NYUInteresting blogsgeneralized language model 
系列 博客 Google AI Blogcolah s blogSebastian RuderMostafa DehghaniAndrej Karpathy 
blogJay AlammarNLP Highlights hosted by Matt Gardner and Waleed Ammarnatural 
language processing blogDetermined22/i 我/r 爱/v 自然/d 语言/n 处理/v 徐/nr 阿衡/nr 
立委/l NLP/w 频道/n 羊肉泡馍/i 与/p 糖蒜/n 2016年/tdq 全球/n 瞩目/v 的/uj 
围棋/v 大战/nz 中/f 人类 以 失败 告终 更是 激起 了 
各种 机器 超越 控制 人类 的 讨论 然而 机器 真的 
懂 人类 吗 机器 能 感受 到 人类 的 情绪 
吗 机器 能理解 人类 的 语言 吗 如果能 那它 又是 
如何 做到 呢 带着 这样 好奇心 本文 将 带领 大家 
熟悉 和 回顾 一个 完整 的 自然 语言 处理过程 后续 
所有 章节 所有 示例 开发 都将 遵从 这个 处理 过程 
首先 我们 通过 一张 图 来 了解 NLP 所 包含 
的 技术 知识 点 这张/i 图/n 从/p 分析/vn 对象/n 和/c 
分析/vn 内容/n 两个/m 不同/a 的/uj 维度/ns 来/v 进行/v 表达/v 个人 
觉得 内容 只能 作为 参考 对于 整个 AI 背景 下 
的 自然 语言 处理 来说 还 不够 完整 enter image 
description here 有 机器学习 相关 经验 的 人都 知道 中文 
自然语言 处理 的 过程 和 机器 学习 过程 大体一致 但又 
存在 很多 细节 上 的 不同 点 下面/f 我们/r 就/d 
来看/u 看中/v 文/n 自然语言/l 处理/v 的/uj 基本/n 过程/n 有/v 哪些/r 
呢/y 获取 语料 语料 即 语言 材料 语料 是 语言 
学 研究 的 内容 语料 是 构成 语料库 的 基本 
单元 所以 人们 简单 地 用 文本 作为 替代 并把 
文本 中的 上下文 关系 作为 现实 世界 中 语言 的 
上下文 关系 的 替代品 我们 把 一个 文本 集合 称为 
语料库 Corpus 当 有几个 这样 的 文本 集合 的 时候 
我们 称之为 语料库 集合 Corpora 定义 来源 百度 百科 按 
语料 来源 我们 将 语料 分为 以下 两种 1 . 
已有 语料 很多 业务 部门 公司/n 等/u 组织/v 随着/p 业务/n 
发展/vn 都会/nr 积累/v 有/v 大量/n 的/uj 纸质/n 或者/c 电子文本/n 资料/n 
那么 对于 这些 资料 在 允许 的 条件 下 我们 
稍加 整合 把 纸质 的 文本 全部 电子化 就 可以 
作为 我们 的 语料库 2 . 网上 下载 抓取 语料 
如果 现在 个人 手里 没有 数据 怎么办 呢 这个 时候 
我们 可以 选择 获取 国内外 标准 开放 数据集 比如 国内 
的 中文 汉语 有 搜狗 语料 人民日报 语料 国外 的 
因为 大都 是 英文 或者 外文 这里 暂时 用不到 也 
可以 选择 通过 爬虫 自己 去 抓取 一些 数据 然后 
来 进行 后续 内容 语料 预处理 这里 重点 介绍 一下 
语料 的 预处理 在 一个 完整 的 中文 自然语言 处理 
工程 应用 中 语料 预处理 大概会 占到 整个 50% 70% 
的 工作量 所以 开发 人员 大 部分 时间 就 在 
进行 语料 预处理 下面 通过 数据 洗清 分词 词性 标注 
去 停用词 四 个大 的 方面 来 完成 语料 的 
预 处理工作 1 . 语料 清洗 数据 清洗 顾名思义 就是 
在 语料 中 找到 我们 感兴趣 的 东西 把 不 
感兴趣 的 视为 噪音 的 内容 清洗 删除 包括 对于 
原始 文本 提取 标题 摘要 正文 等 信息 对于 爬 
取 的 网页 内容 去除 广告 标签 HTML JS 等 
代码 和 注释 等 常见 的 数据 清洗 方式 有 
人工 去 重 对齐 删除 和 标注 等 或者 规则 
提取 内容 正则表达式 匹配 根据 词性 和 命名 实体 提取 
编写 脚本 或者 代码 批处理 等 2 . 分词 中文 
语料 数据 为 一批 短 文本 或者 长 文本 比如 
句子 文章 摘要 段落 或者 整篇文章 组成 的 一个 集合 
一般 句子 段落 之间 的 字 词语 是 连续 的 
有 一定 含义 而 进行 文本 挖掘 分析 时 我们 
希望 文本处理 的 最小 单位 粒度 是 词 或者 词语 
所以 这个 时候 就 需要 分词 来 将 文本 全部 
进行 分词 常见 的 分词 算法 有 基于 字符串 匹配 
的 分词 方法 基于 理解 的 分词 方法 基于 统计 
的 分词 方法 和 基于 规则 的 分词 方法 每 
种方法 下面 对应 许多 具体 的 算法 当前/t 中文分词/i 算法/n 
的/uj 主要/b 难点/d 有/v 歧义/n 识别/v 和/c 新词/n 识别/v 比如 
羽毛球拍 卖完 了 这个 可以 切 分成 羽毛 球拍 卖 
完 了 也可 切 分成 羽毛球 拍卖 完 了 如果 
不 依赖 上下文 其他 的 句子 恐怕 很难 知道 如何 
去 理解 3 . 词性 标注 词性 标注 就是 给 
每个 词 或者 词语 打 词类 标签 如 形容词 动词 
名词 等 这样 做 可以 让 文本 在 后面 的 
处理 中 融入 更多 有用 的 语言 信息 词性 标注 
是 一个 经典 的 序列 标注 问题 不过 对于 有些 
中文 自然语言 处理 来说 词性 标注 不 是非 必需 的 
比如 常见 的 文本 分类 就 不用 关心 词 性问题 
但是 类似 情感 分析 知识 推理 却是 需要 的 下图 
是 常见 的 中文 词性 整理 enter image description here 
常见 的 词性 标注 方法 可以 分为 基于 规则 和 
基于 统计 的 方法 其中 基于 统计 的 方法 如 
基于 最大熵 的 词性 标注 基于 统计 最大 概率 输出 
词性 和 基于 HMM 的 词性 标注 4 . 去 
停用词 停用词 一般指 对 文本 特征 没有 任何 贡献 作用 
的 字词 比如 标点符号 语气 人称 等 一些 词 所以在 
一般性 的 文本 处理 中 分词 之后 接下来 一步 就是 
去 停用词 但是 对于 中文 来说 去 停用词 操作 不是 
一成不变 的 停用词 词典 是 根据 具体 场景 来 决定 
的 比如 在 情感 分析 中 语气词 感叹号 是 应该 
保留 的 因为 他们 对 表示 语气 程度 感情/n 色彩/n 
有/v 一定/d 的/uj 贡献/n 和/c 意义/n 特征 工程 做完 语料 
预处理 之后 接下来/l 需要/v 考虑/v 如何/r 把/p 分词/n 之后/f 的/uj 
字/n 和/c 词语/n 表示/v 成/n 计算机/n 能够/v 计算/v 的/uj 类型/n 
显然 如果 要 计算 我们 至少 需要 把 中文分词 的 
字符串 转换成 数字 确切的说 应该 是 数学 中的 向量 有/v 
两种/m 常用/b 的/uj 表示/v 模型/n 分别/d 是/v 词/n 袋/q 模型/n 
和词/nr 向量/n 词 袋 模型 Bag of Word BOW 即 
不考虑 词语 原本在 句子 中 的 顺序 直接 将 每 
一个 词语 或者 符号 统一 放置 在 一个 集合 如 
list 然后 按照 计数 的 方式 对 出现 的 次数 
进行 统计 统计 词频 这 只是 最 基本 的 方式 
TF IDF 是 词 袋 模型 的 一个 经典 用法 
词 向量 是 将 字 词语 转换成 向量 矩阵 的 
计算 模型 目前 为止 最 常用 的 词 表示 方法 
是 One hot 这种 方法 把 每个 词 表示 为 
一个 很长 的 向量 这个 向量 的 维度 是 词表 
大小 其中 绝大多数 元素 为 0 只有 一个 维度 的 
值 为 1 这个 维度 就 代表 了 当前 的 
词 还有 Google 团队 的 Word2Vec 其 主要 包含 两 
个 模型 跳 字 模型 Skip Gram 和 连续 词 
袋 模型 Continuous Bag of Words 简称 CBOW 以及 两 
种 高效 训练 的 方法 负 采样 Negative Sampling 和 
层序 Softmax Hierarchical Softmax 值得一提的是 Word2Vec 词 向量 可以 较好 
地 表达 不同 词 之间 的 相似 和 类比 关系 
除此之外 还有 一些 词 向量 的 表示 方式 如 Doc2Vec 
WordRank 和 FastText 等 特征选择 同 数据挖掘 一样 在 文本 
挖掘 相关 问题 中 特征 工程 也 是 必不可少 的 
在 一个 实际 问题 中 构造 好 的 特征向量 是 
要 选择 合适 的 表达 能力 强的/nr 特征 文本 特征 
一般 都是 词语 具有 语义 信息 使用 特征选择 能够 找出 
一个 特征 子集 其 仍然 可以 保留 语义 信息 但 
通过 特征提取 找到 的 特征 子空间 将会 丢失 部分 语义 
信息 所以 特征选择 是 一个 很 有 挑战 的 过程 
更多/d 的/uj 依赖/v 于/p 经验/n 和/c 专业/n 知识/v 并且 有 
很多 现成 的 算法 来 进行 特征 的 选择 目前 
常见 的 特征 选择 方法 主要 有 DF MI IG 
CHI WLLR WFO 六种 模型 训练 在 特征向量 选择 好 
之后 接下来 要做 的 事情 当然 就是 训练 模型 对于 
不同 的 应用 需求 我们 使用 不同 的 模型 传统/n 
的/uj 有/v 监督/vn 和无/nr 监督/vn 等/u 机器学习/i 模型/n 如 KNN 
SVM Naive Bayes 决策树 GBDT K means 等 模型 深度 
学习 模型 比如 CNN RNN LSTM Seq2Seq FastText TextCNN 等 
这些 模型 在 后续 的 分类 聚 类 神经 序列 
情感 分析 等 示例 中 都会 用到 这里 不再 赘述 
下面 是 在 模型 训练 时 需要 注意 的 几个 
点 1 . 注意 过拟合 欠 拟合 问题 不断 提高 
模型 的 泛化 能力 过拟合 模型 学习 能力 太强 以至于 
把 噪声 数据 的 特征 也 学习 到了 导致 模型 
泛化 能力 下降 在 训练 集上 表现 很好 但是 在 
测试 集上 表现 很差 常见 的 解决 方法 有 增大 
数据 的 训练 量 增加 正则化 项 如 L1 正则 
和 L2 正则 特征 选取 不合理 人工 筛选 特征 和 
使用 特征选择 算法 采用 Dropout 方法 等 欠 拟合 就是 
模型 不 能够 很好 地 拟合 数据 表现 在 模型 
过于 简单 常见 的 解决 方法 有 添加 其他 特征 
项 增加 模型 复杂度 比如 神经网络 加 更多 的 层 
线性 模型 通过 添加 多项式 使 模型 泛化 能力 更强 
减少 正则化 参数 正则化 的 目的 是 用来 防止 过拟合 
的 但是 现在 模型 出现 了 欠 拟合 则 需要 
减少 正则化 参数 2 . 对于 神经网络 注意 梯度 消 
失和 梯度 爆炸 问题 评价 指标 训 练好 的 模型 
上线 之前 要 对模型 进行 必要 的 评估 目的 让 
模型 对 语料 具备 较好 的 泛化 能力 具体 有 
以下 这些 指标 可以 参考 1 . 错误率 精度 准确率 
精确度 召回率 F1 衡量 错误率 是 分类 错误 的 样本 
数 占 样本 总数 的 比例 对 样例 集 D 
分类 错误率 计算 公式 如下 enter image description here 精度 
是 分类 正确 的 样本 数 占 样本 总数 的 
比例 这里 的 分类 正确 的 样本 数 指 的 
不仅 是 正 例 分类 正确 的 个数 还有 反例 
分类 正确 的 个数 对 样例 集 D 精度 计算 
公式 如下 enter image description here 对于 二分 类 问题 
可将 样例 根据 其 真实 类别 与 学习 器 预测 
类别 的 组合 划分 为 真正 例 True Positive 假 
正 例 False Positive 真 反例 True Negative 假 反例 
False Negative 四种 情形 令 TP FP TN FN 分别 
表示 其 对应 的 样 例数 则 显然有 TP + 
FP + + TN + FN = 样例 总数 分类 
结果 的 混淆 矩阵 Confusion Matrix 如下 enter image description 
here 准确率 缩写 表示 用 P 准确率 是 针对 我们 
预测 结果 而言 的 它 表示 的 是 预测 为 
正 的 样例 中有 多少 是 真正 的 正 样例 
定义 公式 如下 enter image description here 精确度 缩写 表示 
用 A 精确度 则是 分类 正确 的 样本 数 占 
样本 总数 的 比例 Accuracy 反应 了 分类器 对 整个 
样本 的 判定 能力 即 能将 正 的 判定 为 
正 的 负 的 判定 为 负 的 定义 公式 
如下 enter image description here 召回率 缩写 表示 用 R 
召回率 是 针对 我们 原来 的 样本 而言 的 它 
表示 的 是 样本 中的 正 例 有 多少 被 
预测 正确 定义 公式 如下 enter image description hereF1 衡量 
表达出 对 查准率 / 查全率 的 不同 偏好 定义 公式 
如下 enter image description here2 . ROC 曲线 AUC 曲线 
ROC 全称 是 受试者 工作 特征 Receiver Operating Characteristic 曲线 
我们 根据 模型 的 预测 结果 把 阈值 从0/nr 变到 
最大 即 刚开始 是 把 每个 样本 作为 正 例 
进行 预测 随着 阈值 的 增大 学习 器 预测 正 
样 例数 越来越少 直到 最后 没有 一个 样本 是 正 
样例 在 这一 过程 中 每次 计算 出 两个 重要 
量 的 值 分别 以 它们 为 横 纵坐标 作图 
就 得到 了 ROC 曲线 ROC 曲线 的 纵轴 是 
真正 例 率 True Positive Rate 简称 TPR 横轴 是 
假 正 例 率 False Positive Rate 简称 FPR 两者 
分别 定义 为 enter image description hereenter image description hereROC 
曲线 的 意义 有 以下 几点 ROC 曲线 能很/nr 容易 
的 查出 任意 阈值 对模型 的 泛化 性能 影响 有助于 
选择 最佳 的 阈值 可以 对 不同 的 模型 比较 
性能 在 同一 坐标 中 靠近 左上角 的 ROC 曲所/nr 
代表 的 学习 器 准确性 最高 如果 两条 ROC 曲线 
没有 相交 我们 可以 根据 哪条 曲线 最靠近 左上角 哪条 
曲线 代表 的 学习 器 性能 就 最好 但是 实际 
任务 中 情况 很复杂 若 两个 模型 的 ROC 曲线 
发生 交叉 则 难以 一般性 的 断言 两者 孰 优 
孰 劣 此时 如果 一定 要 进行 比较 则 比较 
合理 的 判断 依据 是 比较 ROC 曲 线下 的 
面积 即 AUC Area Under ROC Curve AUC 就是 ROC 
曲 线下 的 面积 衡量 学习 器 优劣 的 一种 
性能指标 AUC 是 衡量 二 分类 模型 优劣 的 一种 
评价 指标 表示 预测 的 正 例 排在 负 例 
前面 的 概率 前面 我们 所讲 的 都是 针对 二分 
类 问题 那么 如果 实际 需要 在 多分 类 问题 
中用 ROC 曲线 的话 一般性 的 转化 为 多个 一对多 
的 问题 即把 其中 一个 当作 正 例 其余 当作 
负 例 来 看待 画出 多个 ROC 曲线 模型 上线 
应用 模型 线上 应用 目前 主流 的 应用 方式 就是 
提供 服务 或者 将 模型 持久化 第一 就是 线下 训练 
模型 然后 将 模型 做 线上 部署 发布 成 接口 
服务 以供 业务 系统 使用 第二 种 就是 在 线 
训练 在线 训练 完成 之后 把 模型 pickle 持久化 然后 
在线 服务 接口 模板 通过 读取 pickle 而 改变 接口 
服务 模型 重构 非 必须 随着 时间 和 变化 可能 
需要 对模型 做 一定 的 重构 包括 根据 业务 不 
同侧 重点 对 上面 提到 的 一至七 步骤 也 进行 
调整 重新 训练 模型 进行 上线 作者 米饭 超人 链接 
https / / www . jianshu . com / p 
/ b87e01374a65 来源 简书 简书 著作权 归 作者 所有 任何 
形式 的 转载 都请/nr 联系 作者 获得 授权 并 注明 
出处 如果 你 对 自然 语言 处理 方面 的 资源 
感兴趣 请 仔细 阅读 本 篇文章 运行 数据 科学 POC 
的 7个 步骤 网上 有 很多 依靠 深度 学习 方法 
的 NLP 资源 有 一些 资源 理论 深厚 十分 经典 
特别 是 斯坦福 大学 和 牛津 大学 的 NLP 其 
深度 学习 课程 为 l 自然语言 处理 与 深度 学习 
斯坦福大学 l 自然语言 处理 的 深度 学习 牛津大学 但是 如果 
你 已经 完成 了 这些 或 已经 在 NLP 中 
获得 了 基础 并 想要 转向 一些 实用 资源 或者 
只是 对 其他 方法 感兴趣 希望 这 篇 文章 能对 
你 有所 帮助 1 . 用 Python 进行 自然语言 处理 
用/p 自然/d 语言/n 工具包/n 分析/vn 文本/n 这是/i 一本/m 至少/d 从/p 
实用性/n 和/c Python/w 生态/n 系统/n 的/uj 双重/n 视角/n 介绍/v 自然语言/l 
处理/v 的/uj 书/n 它 可以 用于 个人 学习 或 作为 
自然语言 处理 或 计算 语言学 课程 的 教科书 或 作为 
人工智能 文本 挖掘 或 语料库 语言学 课程 的 补充 本书 
通过 使用 自然 语言 工具包 NLTK 来 接近 NLP NLTK 
包含 丰富 的 软件 数据 和 文档 可从 http / 
/ nltk . org / 免费 下载 发行 版本 是由 
Windows Macintosh 和 Unix 平台 提供 的 我们 强烈 建议 
您 下载 Python 和 NLTP 并 尝试 一下 示 例和 
练习 2 .   深度 学习 自然语言 处理 Jupyter 笔记本 
课程 这 是 一个 Jupyter 笔记本 并 附随 Jon Krohn 
的 关于 NLP 深度 学习 的 一系列 精彩 视频 的 
回购 协议 如果 你 有兴趣 观看 他 的 视频 它 
是 通过 O Reilly 的 Safari 平台 提供 的 请 
注册 一个 免费 的 10天 试用版 Jon 在 这些 笔记本 
和 随附 视频 中 的 主要 内容 包括 1 . 
预处理 用于 机器学习 应用 的 自然 语言 数据 2 . 
将 自然 语言 转换 为 数字 表示 使用 word2vec 3 
. 通过训练 自然 语言 的 深层 学习 模型 进行 预测 
4 . 在 高级 TensorFlow API Keras 中 应用 先进 
的 NLP 方法 5 . 通过 调整 超 参数 来 
提高 深度 学习 模型 的 性能 3 . 如何 解决 
90% 的 NLP 问题 一步 一步 的 指导 这 是以 
笔记本 的 形式 出现 的 另一 套 非常 棒 的 
教程 它 遵循 类似 于 上述 Krohn 的 轨迹 Insight 
AI 的 Emmanuel Ameisen 分解 了 完成 哪些 任务 需要 
执行 哪些 步骤 阅 读完 本文 后 您 将 知道 
如何 1 . 收集 准备 和 检查数据 2 . 从 
建立 简单 的 模型 开始 并在 必要时 过渡 到 深度 
学习 3 . 解释 并 理解 你 的 模型 确保您 
实际上 获取 的 是 信息 而 不是 噪音 4 . 
Keras LSTM 教程 如何 轻松 构建 强大 的 深度 学习 
语言 模型 本 教程 比 之前 的 资源 重点 更多 
因为 它 涵盖 了 在 Keras 中 实施 用于 语言 
建模 的 LSTM 它 通过 附有 解释 代码 和 视觉 
效果 对此 进行 了 详细 的 介绍 说明 在 本 
教程 中 我 将 专注 于在/nr Keras 中 创建 LSTM 
网络 简要 回顾 或 概述 LSTM 的 工作 原理 在 
Keras LSTM 教程 中 我们 将 利用 称为 PTB 语料库 
的 大型 文本 数据集 来 实现 序 列到 序列 的 
文本 预测模型 5 . 使用 组合 LSTM CNN 模型 的 
Twitter 情感 分析 这 是 一篇 较短 的 教程 它 
是 一个 文章 的 概述 其中 有 使用 LSTM / 
CNN 的 组合 方法 的 代码 来 分析 情感 该 
项目 颠覆 了 体系结构 并 报告 了 不同 的 结果 
我们 的 CNN LSTM 模型 的 准确度 比 CNN 模型 
高 3% 但 比 LSTM 模型 差 3.2% 同时 我们 
的 LSTM CNN 模型 比 CNN 模型 的 性能 好 
8.5% 比 LSTM 模型 好 2.7% 关于 该 项目 的 
结果 的 可靠性 我 暂时 无法 保证 但是 其 创新 
的 情感 分析 方式 与 混合 在 不同 的 神经 
网络 体系 结构 中 搭配 使用 使 我 将其 纳入 
该 列表 中 希望 能对/nr 读者 有所 启发 相关 l 
免费资源 用于 深入 学习 自然语言 处理 入门 l 自然语言 处理 
键 术语 解释 l 处理 文本 数据 科学 任务 的 
框架 本文 由 阿里云 云栖 社区 组织 翻译 文章 原 
标题 5 Fantastic Practical Natural Language Processing Resources 作者 Matthew 
Mayo 译者 乌拉 乌拉 审校 袁虎/nr 文章 为 简译/nr 更为 
详细 的 内容 请 查看 原文 文章 2018年 马上 就要 
过去 回顾 深度 学习 在 今年 的 进展 让人 印象 
最 深刻 的 就是 谷歌 提出 的 应用 于 自然 
语言 处理 领域 的 BERT 解决方案 BERT Pre training of 
Deep Bidirectional Transformers for Language Understanding https / / arxiv 
. org / abs / 1810.04805 BERT 解决方案 刷新 了 
各大 NLP 任务 的 榜单 在 各种 NLP 任务 上 
都 做到 state of the art 这里 我 把 BERT 
说 成是/nr 解决 方案 而 不是 一个 算法 因为 这 
篇 文章 并 没有 提出 新的 算法 模型 还是 沿用 
了 之前 已 有的 算法 模型 BERT 最大 的 创新 
点 在于 提出 了 一套 完整 的 方案 利用 之前 
最新 的 算法 模型 去 解决 各种各样 的 NLP 任务 
因此 BERT 这篇 论文 对于 算法 模型 完全 不做 介绍 
以至于 在 我 直接 看 这篇文章 的 时候 感觉 云里雾里 
但是 本 文中 我会 从 算法 模型 到 解决 方案 
进行 完整 的 诠释 本文 中 我 会分 3个 部分 
进行 介绍 第一 部分 我会 大概 介绍 一下 NLP 的 
发展 第二 部分 主要 讲 BERT 用到 的 算法 最后 
一 部分 讲 BERT 具体 是 怎么 操作 的 一 
NLP 的 发展 要 处理 NLP 问题 首先 要 解决 
文本 的 表示 问题 虽然 我们 人 去看 文本 能够 
清楚 明白 文本 中 的 符号 表达 什么 含义 但是 
计算机 只能 做 数学 计算 需要 将 文本 表示 成 
计算机 可以 处理 的 形式 最 开始 的 方法 是 
采用 one hot 比如 我们 假设 英文 中 常用 的 
单词 有3/nr 万个 那么 我们 就 用 一个 3 万维 
的 向量 表示 这个 词 所有 位置 都置0/nr 当 我们 
想 表示 apple 这个词 时 就在 对应 位置 设置 1 
如 . 1 所示 这种 表示 方式 存在 的 问题 
就是 高维 稀疏 高维 是 指 有 多少 个 词 
就 需要 多少 个 维度 的 向量 稀疏 是 指 
每个 向量 中大 部分值 都是 0 另外 一个 不足 是 
这个 向量 没有 任何 含义 . 1 后来 出现 了 
词 向量 word embedding 用 一个 低维 稠密 的 向量 
去 表示 一个词 如 . 2 所示 通常 这个 向量 
的 维度 在 几百 到 上千 之间 相比 one hot 
几千几万 的 维度 就 低 了 很多 词 与 词 
之间 可以 通过 相似 度 或者 距离 来 表示 关系 
相关 的 词 向量 相似 度 比较 高 或者 距离 
比较 近 不 相关 的 词 向量 相似 度 低 
或者 距离 比较 远 这样 词 向量 本身 就 有了 
含义 文本 的 表示 问题 就 得到 了 解决 词 
向量 可以 通过 一些 无 监督 的 方法 学习 得到 
比如 CBOW 或者 Skip Gram 等 可以 预先 在 语料库 
上 训练 出 词 向量 以供 后续 的 使用 顺便 
提 一句 在 图像 中就 不存在 表示 方法 的 困扰 
因为 图像 本身 就是 数值 矩阵 计算机 可以 直接 处理 
. 2NLP 中有 各种各样 的 任务 比如 分类 Classification 问答 
QA 实体 命名 识别 NER 等 对于 这些 不同 的 
任务 最早 的 做法 是 根据 每类 任务 定制 不同 
的 模型 输入 预 训 练好 的 embedding 然后 利用 
特定 任务 的 数据集 对模型 进行 训练 如 . 3 
所示 这里 存在 的 问题 就是 不是/c 每个/r 特定/d 任务/n 
都/d 有/v 大量/n 的/uj 标签/n 数据/n 可供/v 训练/vn 对于 那些 
数据集 非常 小 的 任务 恐怕 就 难以 得到 一个 
理想 的 模型 . 3 我们 看 一下 图像 领域 
是 如何 解决 这个 问题 的 图像 分类 是 计算机 
视觉 中最 基本 的 任务 当 我 要 解决 一个 
小 数据集 的 图像 分类 任务 时 该 怎么做 CV 
领域 已经 有了/nr 一套 成熟 的 解决方案 我会 用 一个 
通用 的 网络 模型 比如 Vgg ResNet 或者 GoogleNet 在 
ImageNet 上 做 预 训练 pre training ImageNet 有 1400万 
张有 标注 的 图片 包含 1000个 类别 这样 的 数据 
规模 足以 训练 出 一个 规模 庞大 的 模型 在 
训练 过程 中 模型 会 不断 的 学习 如何 提取 
特征 底层 的 CNN 网络结构 会 提取 边缘 角 点 
等 通用 特征 模型 越 往上走 提取 的 特征 也 
越 抽象 与 特定 的 任务 更加 相关 当 完成 
预 训练 之后 根据 我 自己 的 分类 任务 调整 
最 上层 的 网络结构 然后 在 小 数据集 里 对模型 
进行 训练 在 训练 时 可以 固定 住 底层 的 
模型 参 数只 训练 顶层 的 参数 也 可以 对 
整个 模型 进行 训练 这个 过程 叫做 微调 fine tuning 
最终 得到 一个 可用 的 模型 总结 一下 整个 过程 
包括 两步 拿 一个 通用 模型 在 ImageNet 上 做 
预 训练 pre training 然后 针对 特定 任务 进行 微调 
fine tuning 完美 解决 了 特定 任务 数据 不足 的 
问题 还有 一个 好处 是 对于 各种各样 的 任务 都 
不再 需要 从头 开始 训练 网络 可以 直接 拿 预 
训 练好 的 结果 进行 微调 既 减少 了 训练 
计算 量 的 负担 也 减少 了 人工 标注 数据 
的 负担 NLP 领域 也 引入 了 这种 做法 用 
一个 通用 模型 在 非常 大 的 语料库 上 进行 
预 训练 然后 在 特定 任务 上 进行 微调 BERT 
就是 这 套 方案 的 集大成 者 BERT 不是 第一 
个 但 目前 为止 是 效果 最好 的 方案 BERT 
用 了 一个 已有 的 模型 结构 提出 了 一整套 
的 预 训练 方法 和 微调 方法 我们 在 后 
文中 再 进行 详细 的 描述 二 算法 BERT 所 
采用 的 算法 来自于 2017年 12 月份 的 这篇文章 Attenion 
Is All You Need https / / arxiv . org 
/ abs / 1706.03762 同样 来自 于 谷歌 这 篇 
文章 要 解决 的 是 翻译 问题 比如/v 从/p 中文/nz 
翻译/v 成/n 英文/nz 这篇文章 完全 放弃 了 以往 经常 采用 
的 RNN 和 CNN 提出 了 一种 新的 网络结构 即 
Transformer 其中 包括 encoder 和 decoder 我们 只 关注 encoder 
这篇 英文 博客 https / / jalammar . github . 
io / illustrated transformer / 对 Transformer 介绍 得 非常 
详细 有兴趣 的 读者 可以 看 一下 如果 不想 看 
英文 博客 也 可以 看 本文 本 文中 的 部分 
图片 也 截 取自 这篇 博客 . 1.1 是 Transformer 
encoder 的 结构 后文 中 我们 都 简称为 Transformer 首先 
是 输入 word embedding 这里 是 直接 输入 一整 句话 
的 所有 embedding 如 . 1 所示 假设 我们 的 
输入 是 Thinking Machines 每个 词 对应 一个 embedding 就有 
2个 embedding 输入 embedding 需要 加 上 位置 编码 Positional 
Encoding 为什么 要 加 位置 编码 后文 会做 详细 介绍 
然后 经过 一个 Multi Head Attention 结构 这个 结构 是 
算法 单元 中 最重要 的 部分 我们 会 在 后边 
详细 介绍 之后 是 做 了 一个 shortcut 的 处理 
就是 把 输入 和 输出 按照 对应 位置 加起来 如果 
了解 残差 网络 ResNet 的 同学 会 对 这个 结构 
比较 熟悉 这个 操作 有 利于 加速 训练 然后 经过 
一个 归一化 normalization 的 操作 接着 经过 一个 两层 的 
全 连接 网络 最后 同样 是 shortcut 和 normalization 的 
操作 可以 看到 除了 Multi Head Attention 都是 常规 操作 
没有 什么 难 理解 的 这里 需要 注意 的 是 
每个 小 模块 的 输入 和 输出 向量 维度 都是 
相等 的 比如 Multi Head Attention 的 输入 和 输出 
向量 维度 是 相等 的 否则 无法 进行 shortcut 的 
操作 Feed Forward 的 输入 和 输出 向量 维度 也是 
相等 的 最终 的 输出 和 输入 向量 维度 也是 
相等 的 但是 Multi Head Attention 和 Feed Forward 内部 
向量 维度 会 发生 变化 . 2 我们 来 详细 
看 一下 Multi Head Attention 的 结构 这个 Multi Head 
表示 多头 的 意思 先从 最 简单 的 看起 看看 
单头 Attention 是 如何 操作 的 从.1/nr 的 橙色 方块 
可以 看到 embedding 在 进入 到 Attention 之前 有 3个 
分叉 那/r 表示/v 说/v 从/p 1个/mq 向量/n 变成 了 3个 
向量 具体 是 怎么 算 的 呢 我们 看 . 
3 定义 一个 WQ 矩阵 这个 矩阵 随机 初始化 通过训练 
得到 将 embedding 和 WQ 矩阵 做 乘法 得到 查询 
向量 q 假设 输入 embedding 是 512 维 在 中 
我们 用 4个 小 方格 表示 输出 的 查询 向量 
是 64 维 中用 3个 小 方格 以示 不同 然 
后类 似地 定义 WK 和 WV 矩阵 将 embedding 和 
WK 做 矩阵 乘法 得到 键 向量 k 将 embeding 
和 WV 做 矩阵 乘法 得到 值 向量 v 对 
每一个 embedding 做 同样 的 操作 那么 每个 输入 就 
得到 了 3个 向量 查询 向量 键 向量 和值/nr 向量 
需要 注意 的 是 查询 向量 和键/nr 向量 要有 相同 
的 维度 值 向量 的 维度 可以 相同 也 可以 
不同 但 一般 也 是 相同 的 . 3 接下来 
我们 计算 每一个 embedding 的 输出 以 第一 个 词 
Thinking 为例 参看 . 4 用 查询 向量 q1 跟 
键 向量 k1 和 k2 分别 做 点积 得到 112 
和 96 两个 数值 这也 是 为什么 前文 提到 查询 
向量 和键/nr 向量 的 维度 必须 要 一致 否则 无法 
做 点积 然后 除以 常数 8 得到 14 和 12 
两个 数值 这个 常数 8 是 键 向量 的 维度 
的 开方 键 向量 和 查询 向量 的 维度 都是 
64 开方 后是8/nr 做 这个 尺度 上 的 调整 目的 
是 为了 易于 训练 然后 把 14 和 12 丢到 
softmax 函数 中 得到 一组 加 和为1/nr 的 系数 权重 
算 出来 是 大约 是 0.88 和 0.12 将/d 0.88/mx 
和/c 0.12对/mq 两个/m 值/n 向量/n v1/i 和/c v2/i 做/v 加权/v 
求和/v 就 得到 了 Thinking 的 输出 向量 z1 类似 
的 可以 算出 Machines 的 输出 z2 如果 一 句话 
中 包含 更多 的 词 也是 相同 的 计算 方法 
. 4 通过 这样 一 系列 的 计算 可以 看到 
现在 每个 词 的 输出 向量 z 都 包含 了 
其他 词 的 信息 每个 词 都 不再 是 孤立 
的 了 而且 每个 位置 中 词 与 词 的 
相关 程度 可以 通过 softmax 输出 的 权重 进行 分析 
如 . 5 所示 这是 某 一次 计算 的 权重 
其中 线条 颜色 的 深浅 反映 了 权重 的 大小 
可以 看到 it 中 权重 最大 的 两个 词 是 
The 和 animal 表示 it 跟 这 两个 词 关联 
最大 这 就是 attention 的 含义 输出 跟 哪个 词 
关联 比较 强 就 放 比 较多 的 注意力 在上面 
上面 我们 把 每一步 计算 都 拆开 了 看 实际 
计算 的 时候 可以 通过 矩阵 来 计算 如 . 
6 所示 . 5.6 讲完 了 attention 再 来讲 Multi 
Head 对于 同 一组 输入 embedding 我们 可以 并行 做 
若干组 上面 的 操作 例如 我们 可以 进行 8组 这样 
的 运算 每 一组 都有 WQ WK WV 矩阵 并且 
不 同组 的 矩阵 也 不相同 这样 最终 会 计算 
出 8组 输出 我们 把 8组 的 输出 连接起来 并且 
乘以 矩阵 WO 做 一次 线性变换 得到 输出 WO 也是 
随机 初始化 通过训练 得到 计算 过程 如 . 7 所示 
这样 的 好处 一是 多个 组 可以 并行计算 二 是 
不同 的 组 可以 捕获 不同 的 子空间 的 信息 
. 7 到 这里 就把 Transformer 的 结构 讲 完了 
同样 都是 做 NLP 任务 我们 来 和 RNN 做个 
对比 . 8 是个 最 基本 的 RNN 结构 还有 
计算公式 当 计算 隐 向量 h 4时 用 到了 输入 
x4 和上/nr 一步 算 出来 的 隐 向量 h3 h3 
包含 了 前面 所有 节点 的 信息 h4 中 包含 
最多 的 信息 是 当前 的 输入 x4 越 往前 
的 输入 随着 距离 的 增加 信息 衰减 得 越多 
对于 每一个 输出 隐 向量 h 都是 如此 包含 信息 
最 多得是 当前 的 输入 随着 距离 拉 远 包含 
前面 输入 的 信息 越来越少 但是 Transformer 这个 结构 就 
不 存在 这个 问题 不管/c 当前/t 词/n 和/c 其他/r 词/n 
的/uj 空间/n 距离/n 有/v 多远/i 包含 其他 词 的 信息 
不 取决于 距离 而是 取决于 两者 的 相关性 这是 Transformer 
的 第一 个 优势 第二 个 优势 在于 对于 Transformer 
来说 在对 当前 词 进行 计算 的 时候 不仅 可以 
用 到 前面 的 词 也 可以 用 到 后面 
的 词 而 RNN 只能 用 到 前面 的 词 
这 并 不是 个 严重 的 问题 因为 这 可以 
通过 双向 RNN 来 解决 第三点 RNN 是 一个 顺序 
的 结构 必须 要 一步 一步 地 计算 只有 计算出 
h1 才能 计算 h2 再 计算 h3 隐 向量 无法 
同时 并行计算 导致 RNN 的 计算 效率 不高 这是 RNN 
的 固有 结构 所 造成 的 之前 有 一些 工作 
就是 在 研究 如何 对 RNN 的 计算 并行 化 
通过 前文 的 介绍 可以 看到 Transformer 不 存在 这个 
问题 通过 这里 的 比较 可以 看到 Transformer 相对于 RNN 
有 巨大 的 优势 因此 我 看到 有人 说 RNN 
以后 会 被 取代 . 8 关于 上面 的 第三 
点 优势 可能 有人 会 不 认可 RNN 的 结构 
包含 了 序列 的 时序 信息 而 Transformer 却 完全 
把 时序 信息 给 丢掉 了 为了 解决 时序 的 
问题 Transformer 的 作者 用 了 一个 绝妙 的 办法 
这 就是 我 在前 文 提到 的 位置 编码 Positional 
Encoding 位置 编码 是 和 word embedding 同样 维度 的 
向量 将 位置 embedding 和词/nr embedding 加在一起 作为 输入 embedding 
如 . 9 所示 位置 编码 可以 通过 学习 得到 
也 可以 通过 设置 一个 跟 位置 或者 时序 相关 
的 函数 得到 比如 设置 一个 正弦 或者 余弦函数 这里 
不 再多 说 . 9 我们 把 . 1 的 
结构 作为 一个 基本 单元 把 N 个 这样 的 
基本 单元 顺序 连起来 就是 BERT 的 算法 模型 如 
. 10 所示 从 前面 的 描述 中 可以 看到 
当 输入 有 多少 个 embedding 那么 输出 也就 有 
相同 数量 的 embedding 可以 采用 和 RNN 采用 相同 
的 叫法 把 输出 叫做 隐 向量 在做 具体 NLP 
任务 的 时候 只需要 从中 取 对应 的 隐 向量 
作为 输出 即可 . 10 三 BERT 在 介绍 BERT 
之前 我们 先 看看 另外 一套 方案 我 在 第一 
部分 说过 BERT 并 不是 第一 个 提出 预 训练 
加 微调 的 方案 此前 还 有一套 方案 叫 GPT 
这 也是 BERT 重点 对比 的 方案 文章 在 这 
Improving Language Understanding by Generative Pre Training https / / 
s3 us west 2 . amazonaws . com / openai 
assets / research covers / language unsupervised / language _ 
understanding _ paper . pdf GPT 的 模型 结构 和 
BERT 是 相同 的 都是 . 10 的 结构 只是 
BERT 的 模型 规模 更加 庞大 GPT 是 这么 预 
训练 的 在 一个 8亿 单词 的 语料库 上 做 
训练 给出 前文 不断 地 预测 下 一个 单词 比如 
这 句话 Winter is coming 当 给出 第 一个词 Winter 
之后 预测 下 一个 词 is 之后 再 预测 下 
一个 词 coming 不 需要 标注 数据 通过 这种 无 
监督 训练 的 方式 得到 一个 预 训练 模型 我们 
再 来 看看 BERT 有 什么 不同 BERT 来自 于 
Bidirectional Encoder Representations from Transformers 首字母 缩写 这里 提到 了 
一个 双向 Bidirectional 的 概念 BERT 在 一个 33亿 单词 
的 语料库 上 做 预 训练 语料库 就要 比 GPT 
大了 几倍 预 训练 包括 了 两个 任务 第一 个 
任务 是 随机 地 扣掉 15% 的 单词 用 一个 
掩码 MASK 代替 让 模型 去 猜测 这个 单词 第二 
个 任务 是 每个 训练样本 是 一个 上下句 有 50% 
的 样本 下句 和 上句 是 真实 的 另外 50% 
的 样本 下句 和 上句 是 无关 的 模型 需要 
判断 两句 的 关系 这 两个 任务 各 有一个 loss 
将 这 两个 loss 加起来 作为 总的 loss 进行 优化 
下面 两行 是 一个 小 栗子 用 括号 标注 的 
是 扣掉 的 词 用 MASK 来 代替 正 样本 
我 MASK 是 个 算法 工程师 我 服务 于 WiFi 
万能钥匙 这家 MASK 公司 负 样本 我 MASK 是 个 
算法 工程师 今天 MASK 股票 又 跌了 我们 来 对 
比下 GPT 和 BERT 两种 预 训练 方式 的 优劣 
GPT 在 预测 词 的 时候 只 预测 下 一个 
词 因此 只能 用到 上文 的 信息 无法 利用 到 
下文 的 信息 而 BERT 是 预测 文中 扣掉 的 
词 可以 充分 利用 到 上下文 的 信息 这 使得 
模型 有 更强 的 表达 能力 这 也是 BERT 中 
Bidirectional 的 含义 在 一些 NLP 任务 中 需要 判断 
句子 关系 比如 判断 两句话 是否 有 相同 的 含义 
BERT 有了 第二 个 任务 就 能够 很好 的 捕捉 
句子 之间 的 关系 . 1 是 BERT 原文中 对 
另外 两种 方法 的 预 训练 对比 包括 GPT 和 
ELMo ELMo 采用 的 还是 LSTM 这里 我们 不多 讲 
ELMo 这里会 有 读者 困惑 这里 的 结构图 怎么 跟 
. 10 不 一样 如果 熟悉 LSTM 的 同学 看到 
最 右边 的 ELMo 就会 知道 那些 水平 相连 的 
LSTM 其实 只是 一个 LSTM 单元 左边 的 BERT 和 
GPT 也是 一样 水平 方向 的 Trm 表示 的 是 
同一 个 单元 图中 那些 复杂 的 连线 表示 的 
是 词 与 词 之间 的 依赖 关系 BERT 中的 
依赖 关系 既有 前文 又有 后文 而 GPT 的 依赖 
关系 只有 前文 . 1 讲完 了 这两个 任务 我们 
再 来 看看 如何 表达 这么 复杂 的 一个 训练样本 
让 计算机 能够 明白 . 2 表示 my dog is 
cute he likes playing . 的 输入 形式 每个 符号 
的 输入 由 3 部分 构成 一个 是 词 本身 
的 embedding 第二个 是 表示 上下句 的 embedding 如果 是 
上句 就用 A embedding 如果 是 下句 就用 B embedding 
最后 根据 Transformer 模型 的 特点 还要 加上 位置 embedding 
这里 的 位置 embedding 是 通过 学习 的 方式 得到 
的 BERT 设计 一个 样本 最多 支持 512个 位置 将 
3个 embedding 相加 作为 输入 需要 注意 的 是 在 
每个 句子 的 开头 需要 加 一个 Classification CLS 符号 
后文 中会 进行 介绍 其他 的 一些 小 细节 就 
不说 了 . 2 完成 预 训练 之后 就要 针对 
特定 任务 就行 微 调了 这里 描述 一下 论文 中的 
4个 例子 看 . 4 首先 说 下 分类 任务 
分类 任务 包括 对 单 句子 的 分类 任务 比如 
判断 电影 评论 是 喜欢 还是 讨厌 多 句子 分类 
比如 判断 两句话 是否 表示 相同 的 含义 . 4 
a b 是 对 这类 任务 的 一个 示例 左边 
表示 两 个 句子 的 分类 右边 是 单句 子 
分类 在 输出 的 隐 向量 中 取出 CLS 对应 
的 向量 C 加 一层 网络 W 并 丢给 softmax 
进行 分类 得到 预测 结果 P 计算 过程 如 . 
3 中的 计算公式 在 特定 任务 数据 集中 对 Transformer 
模型 的 所有 参数 和 网络 W 共同 训练 直到 
收敛 新 增加 的 网络 W 是 HxK 维 H 
表示 隐 向量 的 维度 K 表示 分类 数量 W 
的 参数 数量 相比 预 训练 模型 的 参数 少得 
可怜 . 3.4 我们 再 来看 问答 任务 如 . 
4 c 以 SQuAD v 1.1 为例 给 出 一个 
问题 Question 并且 给 出 一个 段落 Paragraph 然后 从 
段落 中 标出 答案 的 具体 位置 需要 学习 一个 
开始 向量 维度 和 输出 隐 向量 维度 相同 然后 
和 所有 的 隐 向量 做 点积 取值 最大 的 
词 作为 开始 位置 另外 再 学 一个 结束 向量 
E 做 同样 的 运算 得到 结束 位置 附加 一个 
条件 结束 位置 一定 要 大于 开始 位置 最后 再看 
NER 任务 实体 命名 识别 比如 给出 一句话 对 每个 
词 进行 标注 判断 属于 人名 地名 机构 名 还是 
其他 如 . 4 d 所示 加 一层 分 类网络 
对 每个 输出 隐 向量 都做 一次 判断 可以 看到 
这些 任务 都只/nr 需要 新增 少量 的 参数 然后 在 
特定 数据集 上 进行 训练 即可 从 实验 结果 来看 
即便 是 很小 的 数据 集 也能 取得 不错 的 
效果 到此 本文 对 BERT 做了 完整 的 介绍 如 
有疑问 欢迎 留言 ~ 腾讯 人工智能 AI 开放 平台 上 
提供 了 很多 免费 的 人工智能 API 开发 人员 只 
需要 一个 QQ 号 就 可以 登录 进去 使用 腾讯 
人工智能 AI 开放 平台 的 地址 https / / ai 
. qq . com / 里面 的 好 东西 很多 
以 自然 语言 处理 的 人工智能 API 为例 假设 我们 
有 一个 句子 腾讯 AI 人工智能 开放平台 我们 希望 用 
腾讯 的 人工智能 开放平台 里 提供 的 自然 语言 处理 
API 对 这个 句子 进行 智能 分词 用 您 的 
QQ 号 登录 腾讯 人工智能 开放平台 创建 一个 新 的 
应用 https / / ai . qq . com / 
根据 您 的 实际 需要 选择 自然 语言 处理 的 
具体 类别 文本 朗读 Text to speech / 语音合成 Speech 
synthesis 语音识别 Speech recognition 中文 自动 分词 Chinese word segmentation 
词性 标注 Part of speech tagging 句法分析 Parsing 自然语言 生成 
Natural language generation 文本 分类 Text categorization 信息检索 Information retrieval 
信息 抽取 Information extraction 文字 校对 Text proofing 问答 系统 
Question answering 机器翻译 Machine translation 自动 摘要 Automatic summarization 文字 
蕴涵 Textual entailment 创建 应用 之后 生成 的 app id 
和 app key 要 记下来 在 代码 里 要 使用 
新建 一个 js 文件 输入 如下 代码 var md5 = 
require md5 var app _ id = 2107823355 var time 
_ stamp = Date . now / 1000 var nonce 
_ str = Date . now var text = 腾讯 
AI 人工智能 开放平台 var app _ key = LHGNH0usjUTRRRSA var 
input = app _ id = + app _ id 
+ & nonce _ str = + nonce _ str 
+ & text = + encodeURI text + & time 
_ stamp = + time _ stamp + & app 
_ key = + app _ key var upper = 
md5 input . toUpperCase console . log upper input = 
input + & sign = + upper var request = 
require request var oOptions = { url https / / 
api . ai . qq . com / fcgi bin 
/ nlp / nlp _ wordseg method POST headers { 
content type application / x www form urlencoded } body 
input } console . log request sent + oOptions . 
body var action = new Promise function resolve reject { 
request oOptions function error response body { console . log 
response + body } / / end of request } 
通过 nodejs 里 的 request 组件 使用 HTTP POST 调用 
https / / api . ai . qq . com 
/ fcgi bin / nlp / nlp _ wordseg 去 
消费 腾讯 人工智能 开放 平台 的 自然 语言 处理 的 
分词 API 这些 代码 的 详细 解释 我 已经 在 
我 之前 的 NLP 版本 里 介绍 过了 https / 
/ www . toutiao . com / i 6 5 
8 8 3 1 1 1 6 7 0 8 
7 6 7 3 8 6 9 / group _ 
id = 6 5 8 8 3 1 1 1 
6 7 0 8 7 6 7 3 8 6 
9 使用 命令行 node nlp . js 即可 消费 该 
API 并 查看 结果 要 获取 更多 Jerry 的 原创 
技术文章 请 关注 公众 号 汪子 熙 或者 扫描 下面 
二维码 对于 技术 人员 来说 如果 要问 当前 最 热门 
的 技术 是 什么 我 想 大家 一定 会 回答 
是 人工智能 技术 而在 人工智能 技术 中 哪个 技术 方向 
最 火 呢 大家 肯定 会 回答 是 深度 学习 
技术 如果 我们 要 问 在 深度 学习 技术 中 
哪些 应用 方向 最 火 呢 我 想 大家 可能会 
不约而同 地 说是 机器 视觉 和 自然 语言 处理 了 
机器 视觉 自然 就 不必 说 了 比如说 像 商汤 
旷 视 Face + + 等 独角兽 级 企业 是 
史上 成长 最快 的 独角兽 级 企业 了 融资 规模 
在 几十 亿 以上 同时 机器 视觉 工程师 的 平均 
年薪 已经 达到 50 万起 由此可见 这个 领域 有多/nr 火爆 
但是 我们 觉得 机器 视觉 已经 发展 接近 顶峰 了 
未来 几年 很难 再 现像 前 几年 的 飞速 发展 
期 了 未来 的 发展 的 会 比较 平稳 但是 
自然语言 处理 领域 则 不同 目前 处于 刚刚 开始 起飞 
阶段 未来三 到 五年 将 迎来 进 喷 式 发展 
发展 速度 甚至 比 机器 视觉 还要 快 因为 自然语言 
处理 应用领域 比 机器 视觉 更 广泛 因为 未来 的 
人工智能 要求 更 自然 的 人机交互 人机 协 同和 人机 
融合 这就 要求 机器 具有 听 说 读 写 能力 
而这 正是 自然语言 处理 的 研究 领域 综上所述 目前 是 
学习 自然语言 处理 技术 的 黄金 时期 一 方面 是 
目前 自然语言 处理 技术 工程师 的 年薪 已经 接近 甚至 
超过 机器 视觉 工程师 的 年薪 另一方面 我们 认为 未来 
2 3年 或者 3 至 5年 将 是 自然 语言 
处理 飞速 发展 期 会对 自然语言 处理 方面 的 人才 
产生 巨大 的 需求 现在 开始 学习 才 能够 抓 
这一 趋势 成为 市场 上 抢手 的 人才 那么 我们 
怎样 来 学习 自然语言 处理 技术 呢 我 想 大部分 
人 可能会 选择 学习 网络 上 可以 公开 获取 到 
的 斯坦福 大学 自然 语言 处理 课程 cs224n 了 这门 
课 出生 名门 不仅 讲述 基础 原 论 而且 讲述 
业界 最新进展 同时 还 沿袭 了 斯坦福 大学 理论 联系 
实际 的 传统 这点 从 课程 作业 的 选题 中 
可以 看 出来 所以 说 这门 课 绝对 是 学习 
自然语言 处理 技术 不可多得 的 资料 但是 我们 从 公开 
渠道 只能 看到 老师 上课 的 视频 课程 讲义 PPT 
和 作业 资料 而 这门 课 中 更加 重要 的 
由 斯坦福大学 这门 课 助教 讲解 作业 实现 技术 的 
习题课 我们 就 看不到 了 而 对于 初学者 来说 这些 
助教 讲 的 习题课 才是 这门 课 的 干货 虽然 
老师 讲 的 理论 和 最新 进展 也 很重要 但是 
我们 只有 通过 动手 实践 才能 真正 掌握 和 理解 
这些 内容 但是 非常 遗憾 习题课 是 不 公开 的 
基于 这种 情况 我们 在 网易 云 课堂 上 开设 
了 斯坦福 自然语言 处理 习题课 这样 一门 课程 我们 在 
这门 课 中 将 像 斯坦福 的 助教 一样 向 
大家 详细 讲解 作业 的 实现 技术 使 大家 真正 
做到 理论 联系 实际 讲 到 这里 可能 有 同学 
会问 对于 CS224n 这门 课 的 作业 网上/s 也/d 有/v 
很多/m 作业/n 实现/v 和/c 博客/nr 文章/n 来/v 讲解/v 实现/v 细节/n 
我们 直接 学习 这些 内容 不 就 可以 了吗 还用 
学习 这门 课 吗 当然 直接 看 这些 是 可以 
的 但是 由于 同学们 是 初学者 对于 网络 上 的 
这些 资料 到底 质量 怎么样 很难 区分 另外 这些 资料 
通常 都 是由 在校生 完成 并 分享 出来 的 而 
在校生 由于 没有 大型 工程 项目 的 经验 和 教训 
他们 的 实现 容易 陷入 就事论事 的 状态 比如 我们 
来看 一个 例子 这个 还是 一个 比较 著名 的 实现 
实现 的 质量 还是 蛮高 的 但是 就是 这样 一个 
实现 也是 有 很大 的 问题 的 先来 说 一下 
这个 实现好 的 地方 在 这个 实现 里面 使用 了 
lambda 表达式 numpy apply _ along _ axis 这样 的 
高级 函数 但是 这个 实现 有 一个 小 的 问题 
大家 看 老师 给 的 作业 要求 里面 对于 X 
的 说明 里 写 了 这样 一句 You are allowed 
to modify x in place . 这句话 只是 一个 建议 
所以 很多 同学 都把 这句 给 忽略 了 绝大多数/m 同学/n 
在/p 实现/v 时都/nr 采用/v x/w = np . exp x 
的 形式 但是 这样 写 的 会 复制 一个 x 
的 数组 并 返回 这样 既 浪费 了 内存空间 同时 
会 浪费 运行时间 虽然 softmax 一般 只 用于 输出 层 
一般 对 MNIST 手写 数字 识别 只有 10个 类别 李 
飞飞 开发 的 ImageNet 竞赛 也 不过 是 1000类 计算 
量 并不大 但是 在 训练 阶段 训练样本 集 通常 有 
数百万 的 训练样本 而且 会 学习 多轮 这样 就 浪费 
太大 了 所以 老 师才会 提醒 你 你 可以 在 
x 数组 里 进行 修改 不必 返回 一个 全新 的 
数组 但是 可惜 被 同学们 忽略 掉了 当然 可能 也 
有 同学 尝试 过 老师 提 的 in place 方式 
但是 在 老师 给 的 代码 情况 下 直接 使用 
是 通 不过 的 需要 进行 一些 小 的 修改 
这点 在 我们 课程 中间 会 详细 讲解 这就 让 
我们 想起 了 西游记 里 孙悟空 学艺 菩提 老祖 在 
孙悟空 头上 敲 三下 孙悟空 就能 悟到 是 老师 让 
他 三更 去找 老师 而 其他 弟子 就 悟 不到 
了 所以在 这门 课 里 我们 会 代领 大家 掌握 
自然语言 处理 技术 的 精髓 达到 业界标准 甚至 更高 的 
水平 所以 说 我们 这 门 课 的 价值 不 
仅仅 告诉 你 怎么 把 CS224n 这门 课 里面 的 
作业 做出来 而是 要 给 大家 展示 业界 的 最佳 
实践 使 同学 们 真正 达到 业界 自然语言 处理 工程师 
的 水平 这 里面 还有 一点 就是 即使 像 斯坦福 
自然语言 处理 CS224n 这样 的 精品 课程 也 存在 由于 
录 课 时间 较 久远 课上 所用 技术 存在 过时 
的 情况 例如 在 CS224n 的 作业 中 还是 基于 
python2 的 代码 而 我们 知道 Python2 已经 停止 更新 
了 业界 已经 基本 完成 从 python2 到 python3 的 
转化 了 大家 到 实际 工作 中 基本 都是/nr 使用 
python3 了 这 不得 不说 是 一个 遗憾 所以在 这门 
课 里面 我们 将 带领 大家 一起 把 课程 作业 
中 的 代码 移植 到 python3 还有 例如 在 作业 
1 里面 老师 给 我们 的 神经元 激活 函数 还是 
sigmoid 而 我们 知道 自从 06年 深度 学习 崛起 之后 
神经元 特别 是 隐藏 层 神经元 的 缺省 激活 函数 
已经 变为 ReLU 了 如果 是 近 一两 年 还会 
看到 很多 采用 SeLU 激活 函数 的 这 一点 也 
比较 遗憾 关于 这 一点 我们 在 课程 中 也会 
向 大家 作 一个 简单 的 介绍 使 同学 们 
能够 掌握 最新 的 技术 还有 一点 也是 在 作业 
1 里面 在 讲解 随机 梯度 下降 算法 的 时候 
还 采用 的 是 传统 的 BP 算法 的 方式 
而 我们 知道 当前 流行 的 深度 学习 框架 如 
TensorFlow PyTorch 和 Theano 等 都是 基于 计 算图 的 
实现 方式 了 这些 内容 也 略微 有些 过时 我们 
在 课程 中 也会 以 TensorFlow 普遍 采用 的 计算 
图 方式 向 大家 讲解 递 度 下降 算法 当然 
了 这门 课 毕竟 是 CS224n 配套 的 习题课 不/d 
可能/v 向/p 大家/n 系统/n 的/uj 讲解/v 计/n 算图/i 模型/n 如果 
同学们 对 这 方面 内容 感 举 趣 可以 关注 
我们 即将 推出 的 自己 动手 写 TensorFlow 课程 在 
这门 课 中 我们 将 带领 大家 模仿 TensorFlow 框架 
使用 C 语言 来 实现 深度 学习 的 核心 算法 
使用 Python 语言 来 进行 数据 可视化 模型 可视化 和 
训练 过程 可视化 工作 回到 我们 这 门 课程 我们 
会 带领 大家 使用 python + numpy 的 方式 来 
实现 所有 的 核心 算法 虽然 实际 工作 中 大家 
可能 会 使用 TensorFlow PyTorch 这样 的 深度 学习 框架 
来 实现 算法 甚至 很多 人 用 更 高层 的 
Keras 来写 算法 但是 这些 框架 和库会/nr 给 我们 屏蔽 
了 底层 的 细节 虽然 使 我们 更 容易 入门 
可是 也 使 我们 很难 搞懂 底层 的 数学 原理 
和 代码 实现 技术 而 我们 通过 python + numpy 
的 方式 自己动手 完全 从头来 实现 这些 核心 算法 恰恰 
可以 弥补 大家 在 这 方面 的 知识 缺陷 如果 
大家 对 自然 语言 处理 技术 感兴趣 想 掌握 这些 
算法 的 底层 实现 原理 请 关注 我们 的 课程 
斯坦福 自然语言 处理 习题课 https / / study . 163 
. com / course / introduction / 1006361019 . htm 
share = 2 & shareId = 400000000383016 课程 目前 处于 
连载 状态 早 学 的 同学 有 优惠 呦 第一句 
AI 顾名思义 就是 英文单词 Artificial intelligenc 即 人工智能 其实 人工智能 
并 不是 什么 触 不可 及 的 东西 包括 苹果 
Siri 百 度度 秘 微软 小冰 等 智能 助理 和 
智能 聊天 类 应用 以及 美图 秀秀 的 自动 美化 
功能 都 属于 人工智能 甚至 一些 简单 的 套路 固定 
的 资讯 类 新闻 也 是由 人工智能 来 完成 的 
当然 现在 的 主流 搜索引擎 以及 翻译 技术 也 都在 
尝试 利用 人工智能 来 为 广大 网友 提供 更为 精准 
的 搜索 服务 至于 以 实物 存在 的 人工智能 当属 
现在 物流 仓库 的 小黄 机器人 了 他们 正 代替 
人类 完成 繁重 的 商品 摆放 整理 快速 出库 入库 
等 操作 第二句 现在 人工智能 并 没有 发展 到 像 
电影 中 的 机器 人 一样 那么 高 智能化 的 
程度 今天 的 家庭 机器人 还 远 无法 像 大家 
奢望 的 那样 以 人形 外貌 出现 在 主人 面前 
现在 的 人工智能 分 三个 级别 弱 人工智能 强 人工智能 
超 人工智能 1 弱 人工智能 也称 限制 领域 人工智能 或 
应用型 人工智能 指 的 是 专注 于且/nr 只能 解决 特定 
领域 问题 的 人工智能 毫无疑问 今天/t 我们/r 看到/v 的/uj 所有/b 
人工智能/n 算法/n 和/c 应用/v 都/d 属于/v 弱/a 人工智能/n Alpha Go 
其实 也 是 一个 弱 人工智能 2 强/a 人工智能/n 强/a 
人工智能/n 又称/n 通用/v 人工智能/n 或/c 完全/ad 人工智能/n 指 的 是 
可以 胜任 人类 所有 工作 的 人工智能 一个 可以 称得上 
强 人工智能 的 程序 大概 需要 具备 以下 几 方面 
的 能力 存在 不 确定 因素 时 进行 推理 使用 
策略 解决问题 制定 决策 的 能力 知识 表示 的 能力 
包括 常识性 知识 的 表示 能力 规划 能力 学习 能力 
使用 自然 语言 进行 交流 沟通 的 能力 将 上述 
能力 整合 起来 实现 既定 目标 的 能力 3 超 
人工智能 假设 计算机程序 通过 不断 发展 可以 比 世界 上 
最聪明 最有 天赋 的 人类 还 聪明 那么 由此 产生 
的 人工智能 系统 就 可以 被 称为 超 人工智能 超 
人工智能 的 定义 最为 模糊 因为 没 人 知道 超越 
人类 最高 水平 的 智慧 到底 会 表现 为 何种 
能力 如果 说 对于 强 人工智能 我们 还 存在 从 
技术 角度 进行 探讨 的 可能性 的话 那么 对 于超 
人工智能 今天 的 人类 大多 就 只能 从 哲学 或 
科幻 的 角度 加以 解析 了 第三句 人工智能 的 主要 
技术 深度 学习 + 大 数据 简单 地 说 深度 
学习 就是 把 计算机 要 学习 的 东西 看成 一大堆 
数据 把 这些 数据 丢 进 一个 复杂 的 包含 
多个 层级 的 数据 处理 网络 然后 检查 经过 这个 
网络 处理 得到 的 结果 数据 是不是 符合要求 如果 符合 
就 保留 这个 网络 作为 目标 模型 如果 不 符合 
就 一次次 地 锲而不舍 地 调整 网络 的 参数设置 直到 
输出 满足 要求 为止 这就 好比 输入 一股 水流 计算机 
只要 调节 中间 层层 阀门 如果 可以 在 预期 的 
管道 出口 看到 水流 那么 就 说明 这个 管道 符合要求 
而 我们 要 做 的 只是 告诉 计算机 输入 和 
预期 的 结果 让 他 自己 找 规律 当然 新的 
输入 进入 时 我们 也 要 保证 已经 调节 好 
的 管道 不变化 也 就是说 深度 学习 算法 是 有 
计算机 自己 凑 出来 的 模型 这样 反倒 更加 实用 
更 能够 从 本质 上 解决 问题 当然 搭建 好 
的 管道 只有 通过 各种 类型 水流 的 检验 才能 
变得 越来越 接近 真实 的 世界 值得一提的是 大 数据 正是 
为 这些 管道 提供 了 源源不断 的 水流 我们 知道 
深度 学习 大 规模 计算 大 数据 都是 在 2010年 
前后 逐渐 步入 成熟 的 第四句 人工智能 主要 用 在 
以下 领域 自动驾驶 智慧 生活 智慧医疗 1 自动驾驶 最大 的 
应用 场景 自动驾驶 是 现在 逐渐 发展 成熟 的 一项 
智能 应用 可以想象 自动驾驶 一旦 实现 可以 带来 如下 改变 
1 完全 意义 上 的 共享 汽车 成为 可能 大多数 
汽车 可以 用 共享 经济 的 模式 随叫随到 因为 不 
需要 司机 这些 车辆 可以 保证 24 小时 待命 可以 
在 任何 时间 任何 地点 提供 高 质量 的 租用 
服务 2 汽车 本身 的 形态 也 会 发生 根本性 
的 变化 一辆 不 需要 方向盘 不 需要 司机 的 
汽车 可以 被 设计 成 前所未有 的 样子 3 未来 
的 道路 发生 变化 它们 也 会 按照 自动 驾驶 
汽车 的 要求 来 重新 设计 专 用于 自动 驾驶 
的 车道 可以 变得 更 窄 交通信号 可以 更容易 被 
自动 驾驶 汽车 识别 2 智慧 生活 目前 的 机器 
翻译 水平 大概 相当于 一个 刚 学 某种 外语 两三年 
的 中学生 做出 的 翻译 作业 对于 多数 非专业 类 
的 普通 文本 内容 机器 翻译 的 结果 已经 可以 
做到 基本 表达 原文 语意 不影响 理解 与 沟通 但 
假以时日 不断 提高 翻译 准确度 的 人工 智能系统 极 有 
可能 像 下 围棋 的 Alpha Go 那样 悄然 越过 
了 业余 译员 和 职业 译员 之间 的 技术 鸿沟 
一跃 而 成为 翻译 大师 那时候 不 只是 手 机会 
和人/nr 智能 对话 我们 每个 家庭 里 的 每一件 家用电器 
都会 拥有 足够 强大 的 对话 功能 为 我们 提供 
方便 的 服务 3 智慧医疗 AI 将 成为 医生 的 
好帮手 大 数据 和 基于 大 数据 的 人工智能 为 
医生 辅助 诊断 疾病 提供 了 最好 的 支持 事实证明 
就 在 今年 2月 经过 深度 学习 的 神经 网络 
在 诊断 某些 皮肤病 方面 的 表现 比 大部分 医生 
还要 好 在 AI 的 帮助 下 我们 看到 的 
不会 是 医生 失业 而是 同样 数量 的 医生 可以 
服务 几倍 数十 倍 甚至 更多 的 人群 医疗 资源 
分布 不 均衡 的 地区 会 因为 AI 的 引入 
让 绝大多数 病人 享受到 一流 的 医疗 服务 人工智能 建立 
能 智能化 处理 事物 的 系统 自然语言 处理 建立 能够 
理解 语言 的 系统 人工智能 的 一个 分支 机器学习 建立 
能从 经验 中 进行 学习 的 系统 也是 人工智能 的 
一个 分支 神经网络 生物学 启 发出 的 人工 神经元网络 深度 
学习 在 大型 数据 集上 建立 使用 深度 神经 网络 
的 系统 机器 学习 的 一个 分支 1 . 如何 
快速 入门 NLP 自然语言 处理 概述 http / / www 
. duozhishidai . com / article 11742 1 . html2 
. 人工智能 机器 学习 和 深度 学习 之间 主要 有 
什么 差异 http / / www . duozhishidai . com 
/ article 15858 1 . html3 . 人工智能 机器学习 数据挖掘 
以及 数据 分析 有 什么 联系 http / / www 
. duozhishidai . com / article 13135 1 . html 
十分钟 学习 自然语言 处理 概述 摘要 近来 自然语言 处理 行业 
发展 朝气蓬勃 市场 应用 广泛 笔者 学习 以 来写 了 
不少 文章 文章 深度 层次 不一 今天 因为 某 种 
需要 将 文章 全部 看了 一遍 做个 整理 也 可以 
称之为 概述 关于 这些 问题 博客/nr 里面/f 都有/nr 详细/ad 的/uj 
文章/n 去/v 介绍/v 本文 只是 对其 各个 部分 高度 概括 
梳理 本文 原创 转载 注明 出处 十分钟 学习 自然语言 处理 
概述   1 什么 是 文本 挖掘 文本 挖掘 是 
信息 挖掘 的 一个 研究 分支 用于 基于 文本 信息 
的 知识 发现 文本 挖掘 的 准备 工作 由 文本 
收集 文本 分析 和 特征 修剪 三个 步骤 组成 目前 
研究 和 应用 最多 的 几种 文本 挖掘 技术 有 
文档 聚 类 文档 分类 和 摘要 抽取 2 什么 
是 自然 语言 处理 自然语言 处理 是 计算机 科学 领域 
与 人工智能 领域 中 的 一个 重要 方向 它 研究人 
与 计算机 之间 用 自然 语言 进行 有效 通信 的 
理论 和 方法 融 语言学 计算机科学 数学 等 于 一体 
的 科学 自然语言 处理 原理 形式化 描述 数学模型 算法 化 
程序化 实用化 语音 的 自动合成 与 识别 机器翻译 自然语言 理解 
人机对话 信息检索 文本 分类 自动 文摘 等 3   常用 
中文分词 中文 文本 词 与 词 之间 没有 像 英文 
那样 有 空格 分隔 因此 很多 时候 中文 文本 操作 
都 涉及 切 词 这里 整理 了 一些 中文分词 工具 
Stanford 直接 使用 CRF 的 方法 特征 窗口 为 5 
汉语分词 工具 个人 推荐 哈工大 语言 云 庖丁解牛 分词 盘古 
分词   ICTCLAS 中科院 汉语 词法 分析 系统 IKAnalyzer Luence 
项目 下 基于 java 的 FudanNLP 复旦 大学 4 词性 
标注 方法 句法 分析方法 原理 描述 标注 一篇 文章 中 
的 句子 即 语句 标注 使用 标注 方法 BIO 标注 
则 观察 序列 X 就是 一个 语料库 此处 假设 一篇 
文章 x 代表 文章 中的 每 一句 X 是 x 
的 集合 标识 序列 Y 是 BIO 即 对应 X 
序列 的 识别 从而 可以 根据 条件概率 P 标注 | 
句子 推测 出 正确 的 句子 标注 显然 这里 针对 
的 是 序列 状态 即 CRF 是 用来 标注 或 
划分 序列 结构 数据 的 概率 化 结构 模型 CRF 
可以 看作 无向图 模型 或者 马尔科夫 随 机场   用过 
CRF 的 都 知道 CRF 是 一个 序列 标注 模型 
指 的 是 把 一个 词 序列 的 每个 词 
打 上一个 标记 一般 通过 在 词 的 左右 开 
一个 小 窗口 根据 窗口 里 面的 词 和待/nr 标注 
词语 来 实现 特征 模板 的 提取 最后 通过 特征 
的 组合 决定 需要 打的 tag 是 什么 5 命名 
实体 识别 三种 主流 算法 CRF 字 典法 和 混合 
方法 1 CRF 在 CRF for Chinese NER 这个 任务 
中 提取 的 特征 大多 是 该词 是否 为 中国 
人名 姓氏 用字 该词 是否 为 中国 人名 名字 用字 
之类 的 True or false 的 特征 所以 一个 可靠 
的 百家姓 的 表 就 十分 重要 啦 ~ 在 
国内 学者 做 的 诸多 实验 中 效果 最好 的 
人名 可以 F1 测度 达到 90% 最差 的 机构 名 
达到 85% 2 字 典法 在 NER 中 就是 把 
每个 字 都当/nr 开头 的 字 放到 trie tree 中 
查 一遍 查到 了 就是 NE 中文 的 trie tree 
需要 进行 哈希 因为 中文 字符 太多 了 不像 英文 
就 26个 3 对 六类 不同 的 命名 实体 采取 
不 一样 的 手段 进行 处理 例如 对于 人名 进行 
字 级别 的 条件概率 计算   中文 哈工大 语言 云 
上海交大       英文 stanfordNER 等 7 基于 主动 
学习 的 中医 文献 句法 识别 研究 7.1 语料库 知识 
语料库 作为 一个 或者 多 个 应用 目标 而 专门 
收集 的 有 一定 结构 的 有 代表 的 可被 
计算机程序 检索 的 具有 一定 规模 的 语料 的 集合 
语料库 划分 ① 时间 划分 ② 加工 深度 划分 标注 
语料库 和非/nr 标注 语料库 ③ 结构 划分 ⑤ 语种 划分 
⑥ 动态更新 程度 划分 参考 语料库 和 监控 语料库 语料库 
构建 原则 ①     代表性 ②     结构性 
③     平衡性 ④     规模性 ⑤   
  元数据 元数据 对 语料 标注 的 优缺点 ①   
  优点 研究 方便 可 重用 功能 多样性 分析 清晰 
②     缺点 语料 不 客观 手工 标注 准确率 
高而/nr 一致性 差 自动 或者 半自动 标注 一致性 高而/nr 准确率 
差 标注 不 一致 准确率 低 7.2 条件 随 机场 
解决 标注 问题 条件 随 机场 用于 序列 标注 中文分词 
中文 人名 识别 和 歧义 消解 等 自然 语言 处理 
中 表现 出 很好 的 效果 原理 是 对 给定 
的 观察 序列 和 标注 序列 建立 条件 概率模型 条件 
随 机场 可 用于 不同 预测 问题 其 学习 方法 
通常 是 极大 似 然 估计 我 爱 中国 进行 
序列 标注 案例 讲解 条件 随 机场 规则 模型 和 
统计模型 问题 条件 随 机场 模型 也 需要 解决 三 
个 基本 问题 特征 的 选择 表示 第 i 个 
观察 值 为 爱 时 相对 yi yi 1 的 
标记 分别 是 B I 参数 训练 和 解码 7.3 
隐 马尔可夫 模型 应用 词类 标注 语音识别 局部 句法 剖析 
语 块 分析 命名 实体 识别 信息 抽取 等 应用于 
自然科学 工程技术 生物科技 公用事业 信道编码 等 多个 领域 马尔可夫 链 
在 随机 过程 中 每个 语言 符号 的 出现 概率 
不 相互 独立 每个 随机 试验 的 当前 状态 依赖 
于 此前 状态 这种 链 就是 马尔可夫 链 多元 马尔科夫 
链 考虑 前 一个 语言 符号 对 后 一个 语言 
符号 出现 概率 的 影响 这样 得出 的 语言 成分 
的 链 叫做 一重 马尔可夫 链 也是 二元 语法 二重 
马尔可夫 链 也是 三元 语法 三重 马尔可夫 链 也是 四元 
语法 隐 马尔可夫 模型 思想 的 三个 问题 问题 1 
似 然 度 问题 给 一个 HMM λ = A 
B 和 一个 观察 序列 O 确定 观察 序列 的 
似 然 度 问题 P O | λ 向前 算法 
解决 问题 2 解码 问题 给定 一个 观察 序列 O 
和 一个 HMM λ = A B 找出 最好 的 
隐藏 状态 序列 Q 维 特比 算法 解决 问题 3 
学习 问题 给定 一个 观察 序列 O 和 一个 HMM 
中的 状态 集合 自动 学习 HMM 的 参数 A 和B/nr 
向前 向后 算法 解决 7.4 Viterbi 算法 解码 思路 1 
计算 时间 步1的/nr 维 特比 概率 2 计算 时间 步2的/nr 
维 特比 概率 在 1 基础 计算 3 计算 时间 
步3的/nr 维 特比 概率 在 2 基础 计算 4 维 
特比 反向 追踪 路径 维 特比 算法 与 向前 算法 
的 区别 1 维 特比 算法 要 在前面 路径 的 
概率 中 选择 最大值 而 向前 算 法则 计算 其 
总和 除此之外 维 特比 算法 和 向前 算法 一样 2 
维 特比 算法 有 反向 指针 寻找 隐藏 状态 路径 
而 向前 算法 没有 反向 指针 HMM 和维/nr 特比 算法 
解决 随机 词类 标注 问题 利用 Viterbi 算法 的 中文 
句法 标注 7.5 序列 标注 方法         
    参照 上面 词性 标注 7.6 模型 评价 方法 
模型 方法 = 模型 + 策略 + 算法 模型 问题 
涉及 训练 误差 测试 误差 过拟合 等 问题 通常 将 
学习 方法 对 未知 数据 的 预测 能力 称为 泛化 
能力 模型 评价 参数 准确率 P = 识别 正确 的 
数量 / 全部 识别 出 的 数量 错误率 = 识别 
错误 的 数量 / 全部 识别 出 的 数量 精度 
= 识别 正确 正 的 数量 / 识别 正确 的 
数量 召回率 R = 识别 正确 的 数量 / 全部 
正确 的 总量 识别 出 + 识别 不出 的 F 
度量 = 2PR / P + R 数据 正负 均衡 
适合 准确率     数据 不均 适合 召回率 精度 F 
度量 几种 模型 评估 的 方法 K 折 交叉 验证 
随机 二次 抽样 评估 等     ROC 曲线 评价 
两 个 模型 好坏 8 基于 文本处理 技术 的 研究生 
英语 等级 考试 词汇表 构建 系统 完成 对 2002 2010年 
17套 GET 真题 的 核心 单词 抽取 其中 包括 数据 
清洗 停用词 处理 分词 词频 统计 排序 等 常用 方法 
真题 算是 结构化 数据 有 一定 规则 比较 容易 处理 
此 过程 其实 就是 数据 清洗 过程 最后 把 所有 
单词 集中 汇总 再 去除 如 a / an / 
of / on / frist 等 停用词 中文 文本处理 也 
需要 对 停用词 处理 诸如 的 地 是 等 处理 
好 的 单词 进行 去 重和 词频 统计 最后 再 
利用 网络 工具 对 英语 翻译 然后 根据 词频 排序 
8.1 Apache Tika Apache Tika 内容 抽取 工具 其 强大 
之处 在于 可以 处理 各种 文件 另外 节约 您 更多 
的 时间 用来 做 重要 的 事情 Tika 是 一个 
内容 分析 工具 自带 全面的 parser 工具 类 能 解析 
基本 所有 常见 格式 的 文件 Tika 的 功能 • 
文档 类型 检测     • 内容 提取   • 
元数据 提取   • 语言 检测 8.2 文本 词频 统计 
词频 排序 方法 算法 思想 1 历年 2002 2010年 GET 
考试 真题 文档 格式 不一 网上 收集 2 对 所有 
格式 不一 的 文档 进行 统计 处理 成 txt 文档 
格式化 去除 汉字 / 标点 / 空格 等 非 英文单词 
和 去除 停用词 去除 891个 停用词 处理 3 对 清洗 
后的/nr 单词 进行 去 重和 词频 统计 通过 Map 统计 
词频 实体 存储 单词 词频 数组 也 可以 只是 面对 
特别 大 的 数据 数组 存在 越界 问题 排序 根据 
词频 或者 字母 4 提取 核心 词汇 大于 5 的 
和 小于 25次 的 数据 可以 自己 制定 阈值 遍历 
list 实体 列表 时候 通过 获取 实体 的 词频 属性 
控制 选取 词汇表 尺寸 5 最后 一步 中 英文 翻译 
9 朴素 贝叶斯 模型 的 文本 分类器 的 设计 与 
实现 9.1 朴素 贝叶斯 公式 0 喜悦   1 愤怒 
2 厌恶 3 低落 9.2 朴素 贝叶斯 原理 训练 文本 
预处理 构造 分类器 即对 贝叶斯 公式 实现 文本 分类 参数值 
的 求解 暂时 不 理解 没关系 下文 详解 构造 预测 
分类 函数 对 测试数据 预处理 使用 分类器 分类 对于 一个 
新的 训练 文档 d 究竟 属于 如上 四个 类别 的 
哪个 类别 我们 可以 根据 贝叶斯 公式 只是 此刻 变化成 
具体 的 对象 P Category | Document 测试 文档 属于 
某类 的 概率 P Category 从 文档 空间 中 随机 
抽取 一个 文档 d 它 属于 类别 c 的 概率 
某类 文档 数目 / 总 文档 数目 P Document | 
Category 文档 d 对于 给定 类 c 的 概率 某类/r 
下/f 文档/n 中/f 单/n 词数/n //i 某类/r 中/f 总的/b 单/n 
词数/n P Document 从 文档 空间 中 随机 抽取 一个 
文档 d 的 概率 对于 每个 类别 都 一样 可以 
忽略 不 计算 此时 为求 最大 似 然 概率   
C d = argmax { P C _ i * 
P d | c _ i } 求出 近似 的 
贝叶斯 每个 类别 的 概率 比较 获取 最大 的 概率 
此时 文档 归为 最大 概率 的 一类 分类 成功 综述 
1 .   事先 收集 处理 数据集 涉及 网络爬虫 和 
中文 切 词 特征 选取 2 .   预处理 去掉 
停用词 移除 频数 过小 的 词汇 根据 具体 情况 3 
.   实验 过程 数据集 分 两部分 3 7 30% 
作为 测试 集 70% 作为 训练 集 增加 置信度 10 
折 交叉 验证 整个 数据集 分为 10 等份 9份 合并 
为 训练 集 余下 1份 作为 测试 集 一共 运行 
10遍 取 平均值 作为 分类 结果 优缺点 对比 分析 4 
. 评价 标准 宏 评价 & 微 评价 平滑 因子 
9.3 生产 模型 与 判别 模型 区别 1 生产 式 
模型 直接 对 联合 分布 进行 建模 如 隐 马尔科夫 
模型 马尔科夫 随 机场 等 2 判别式 模型 对 条件分布 
进行 建模 如 条件 随 机场 支持 向量 机 逻辑 
回归 等 生成 模型 优点 1 由 联合 分布 2 
收敛 速度 比较 快 3 能够 应付 隐 变量 缺点 
为了 估算 准确 样本量 和 计算 量大 样本 数目 较多 
时候 不 建议 使用 判别 模型 优点 1 计算 和 
样本 数量 少 2 准确率 高 缺点 收敛 慢 不能 
针对 隐 变量 9.4 ROC 曲线 ROC 曲线 又叫 接受者 
操作 特征 曲线 比较 学习 器 模型 好坏 可视化 工具 
横坐标 参数 假 正 例 率 纵坐标 参数 是 真正 
例 率 曲线 越 靠近 对角线 随机 猜测 线 模型 
越 不好 好 的 模型 真正 比例 比较 多 曲线 
应是 陡峭 的 从0/nr 开始 上升 后来 遇到 真正 比例 
越来越 少 假 正比例 元组 越来越 多 曲线 平缓 变 
的 更加 水平 完全 正确 的 模型 面积 为 110 
统计学 知识 信息 图形化 饼图 线 形图 等 集中趋势 度量 
平均值 中位数 众数 方 差等 概率 排列组合 分布 几何 二项 
泊松 正 态 卡方 统计 抽样 样本 估计 假设检验 回归 
11 stanfordNLP 句子 理解 自动 问答 系统 机器翻译 句法分析 标注 
情感 分析 文本/n 和/c 视觉/n 场景/n 和/c 模型/n 以及 自然 
语言 处理 数字 人文 社会 科学 中 的 应用 和 
计算 12   APache OpenNLPApache 的 OpenNLP 库 是 自然 
语言 文本 的 处理 基于 机器学习 的 工具包 它 支持 
最 常见 的 NLP 任务 如 断 词 句子 切分 
部分 词性 标注 命名 实体 提取 分块 解析 和 指代 
消解 句子 探测器 句子 检测器 是 用于 检测 句子 边界 
标记 生成器 该 OpenNLP 断 词 段 输入 字符 序列 
为 标记 常是这/nr 是由 空格 分隔 的 单词 但也 有 
例外 名称 搜索 名称 查找 器 可 检测 文本 命名 
实体 和 数字 POS 标注 器 该 OpenNLP POS 标注 
器 使用 的 概率模型 来 预测 正确 的 POS 标记 
出了 标签 组 细节 化 文本 分块 由 除以 单词 
句法 相关 部分 如 名词 基 动词 基 的 文字 
但 没有 指定 其 内部 结构 也 没有 其 在 
主句 作用 分析器 尝试 解析器 最 简单 的 方法 是 
在 命令行 工具 该 工具 仅 用于 演示 和 测试 
请 从 我们 网站 上 的 英文 分块 13 LuceneLucene 
是 一个 基于 Java 的 全文 信息检索 工具包 它 不是 
一个 完整 的 搜索 应用程序 而是 为 你 的 应用 
程序 提供 索引 和 搜索 功能 Lucene 目前 是 Apache 
Jakarta 雅加达 家族 中 的 一个 开源 项目 也 是 
目前 最为 流行 的 基于 Java 开源 全文检索 工具包 目前 
已经 有 很多 应用 程序 的 搜索 功能 是 基于 
Lucene 比如 Eclipse 帮助 系统 的 搜索 功能 Lucene 能够 
为 文本 类型 的 数 据 建立 索引 所以 你 
只要 把 你 要 索引 的 数据格式 转化 的 文本格式 
Lucene 就 能对 你 的 文档 进行 索引 和 搜索 
14 Apache SolrSolr 它 是 一种 开放源码 的 基于 Lucene 
Java 的 搜索 服务器 Solr 提供 了 层面 搜索 就是 
统计 命中 醒目 显示 并且 支持 多种 输出 格式 它 
易于 安装 和 配置 而且 附带 了 一个 基于 HTTP 
的 管理 界面 可以 使用 Solr 的 表现 优异 的 
基本 搜索 功能 也 可以 对 它 进行 扩展 从而 
满足 企业 的 需要 Solr 的 特性 包括 • 高级 
的 全文 搜索 功能 • 专为 高通量 的 网络 流量 
进行 的 优化 • 基于 开放 接口 XML 和 HTTP 
的 标准 • 综合 的 HTML 管理 界面 • 可伸缩性 
－ 能够 有效 地 复制 到 另外 一个 Solr 搜索 
服务器 • 使用 XML 配置 达到 灵活性 和 适配 性 
• 可扩展 的 插件 体系 solr 中文分词 15 机器学习 降 
维 主要 特征 选取 随机 森林 主 成分 分析 线性 
降 维 16 领域 本体 构建 方法 1 确定 领域 
本体 的 专业 领域 和 范畴 2 考虑 复用 现有 
的 本体 3 列出 本体 涉及 领域 中 的 重要 
术语 4 定义 分类 概念 和 概念 分类 层次 5 
定义 概念 之间 的 关系 17 构建 领域 本体 的 
知识 工程 方法 主要 特点 本体 更 强调 共享 重用 
可以 为 不同 系统 提供 一种 统一 的 语言 因此 
本体 构建 的 工程性 更为 明显 方法 目前 为止 本体 
工程 中 比较 有名 的 几种 方法 包括 TOVE 法 
Methontology 方法 骨架 法 IDEF 5/m 法和七/nr 步法/n 等/u 大多 
是 手工 构建 领域 本体 现状 由于 本体 工程 到 
目前 为止 仍 处于 相对 不 成熟 的 阶段 领域 
本体 的 建设 还 处于 探索期 因此 构建 过程 中 
还 存在 着 很多 问题 方法 成熟度 以上 常用 方法 
的 依次 为 七 步法 Methontology 方法 IDEF 5 法 
TOVE 法 骨架 法 Atitit 人工智能 体系 树 培训 列表 
应用 较为 广泛 的 技术 . docxAtitit 人工智能 体系 培训 
列表 目录 1 . 1 . NLP 自然语言 处理 文本处理 
21.1 . 语言 理解   分词 21.2 . 抽取 压缩 
文档 的 读取   格式 转换 21.3 . 索引 处理 
    摘要 提取 21.4 . 搜索 按照 标题 内容 
21.5 . 热词 检索 排行 词 云 可视化 展示 21.6 
. 专家系统   问答 系统 21.7 . 智能 搜索 引擎 
  数据挖掘 和 知识发现 22 . 知识图谱   知识处理 系统 
23 . 2 . 机器 视觉 图像处理 33.1 . ocr 
文字 识别 43.2 . 条码 二维码 识别 43.3 . 人脸识别 
43.4 . 目标 识别   验证码 识别 43.5 . 证件 
识别   银行卡 识别 43.6 . 指纹识别 43.7 . 图像 
视频 内容 分析 43.8 . 图像搜索 相似 图片 小 图 
搜 大图 人脸 搜 类似 43.9 . 视频 编解码 五大类 
44 . 3 . 机器人 在线 机器人 自动化 44.1 . 
web 自动化 webdriver 44.2 . 爬虫 信息采集 与 信息 发布 
机器人 44.3 . gui 自动化 45 . 生物 特征 识别 
none 56 . 人机交互 56.1 . 键盘 鼠标 操纵杆 56.2 
. 打印机   显示器   音 箱等 输出设备 57 . 
4 . 语言识别 语言 tts 等 58 . Ar vr 
none 59 . 5 . 机器学习 决策树   贝叶斯 knn 
  等 510 . Other 511 . ref 6NLP 自然语言 
处理 文本处理 语言 理解   分词 抽取 压缩 文档 的 
读取   格式 转换 索引 处理     摘要 提取 
搜索 按照 标题 内容 热词 检索 排行 词/n 云/ns 可视化/n 
展示/v 专家系统/l  /i 问答/v 系统/n 智能/n 搜索/v 引擎/n 数据挖掘/n 和/c 
知识发现/i 知识图谱/i  /i 知识处理/i 系统/n 知识图谱/i 本质上/i 是/v 结构化/n 的/uj 
语义/n 知识库/n 是/v 一种/m 由/p 节点/n 和边/nr 组成/v 的/uj 图/n 
数据结构/n 以 符号 形式 描述 物理 世界 中 的 概念 
及 其 相互 关系 其 基本 组成 单位 是 实体 
关系 实体 三元组 以及 实体 及其 相关 属性 值 对 
不同 实体 之间 通过 关系 相互 联结 构成 网状 的 
知识 结构 在 知识图谱 中 每个 节点 表示 现实 世界 
的 实体 每条 边为/nr 实体 与 实体 之间 的 关系 
通俗 地 讲 知识图谱 就是 把 所有 不同 种类 的 
信息 连接 在 一起 而 得到 的 一个 关系 网络 
提供 了 从 关系 的 角度 去 分析 问题 的 
能力 知识图谱 可 用于 反 欺诈 不一致性 验证 组团 欺诈 
等 公共 安全 保障 领域 需要 用到 异常 分析 静态分析 
动态 分析 等 数据挖掘 方法 特别地 知识图谱 在 搜索引擎 可视化/n 
展示/v 和/c 精准/n 营销/vn 方面/n 有/v 很大/a 的/uj 优势/n 已 
成为 业界 的 热门 工具 但是 知识图谱 的 发展 还有 
很大 的 挑战 如 数据 的 噪声 问题 即 数据 
本身 有 错误 或者 数据 存在 冗余 随着 知识图谱 应用 
的 不断 深入 还有 一 系列 关键 技术 需要 突破 
2 . 机器 视觉 图像处理 计算机 视觉 是 使用 计算机 
模仿 人类 视觉 系统 的 科学 让 计算机 拥有 类似 
人类 提取 处理 理解 和 分析 图像 以及 图像 序列 
的 能力 根据 解决 的 问题 计算机 视觉 可分为 计算 
成像 学 图像 理解 三维 视觉 动态 视觉 和 视频 
编解码 五大类 ocr 文字 识别 条码 二维码 识别 人脸识别 目标 
识别   验证码 识别 证件 识别   银行卡 识别 指纹识别 
图像 视频 内容 分析 图像搜索 相似 图片 小 图 搜 
大图 人脸 搜 类似 视频 编解码 五大类 3 . 机器人 
在线 机器人 自动化 web 自动化 webdriver 爬虫 信息采集 与 信息 
发布 机器人 gui 自动化 生物 特征 识别 none 人机交互 人机交互 
主要 研究 人 和 计算机 之间 的 信息 交换 主要 
包括 人 到 计算机 和 计算机 到人 的 两 部分 
信息 交换 是 人工智能 领域 的 重要 的 外围 技术 
人机交互 是 与 认知 心理学 人机 工程学 多媒体技术 虚拟 现实 
技术 等 密切 相关 的 综合 学科 传统 的 人与 
计算机 之间 的 信息 交换 主要 依靠 交互 设备 进行 
主要 包括 键盘 鼠标 操纵杆 数据 服装 眼动 跟踪器 位置 
跟踪器 数据 手套 压力 笔 等 输入设备 以及 打印机 绘图仪 
显示器 头盔式 显示器 音 箱等 输出设备 人机 交互技术 除了 传统 
的 基本 交互 和 图形 交互 外 还包括 语音 交互 
情感 交互 体感 交互 及 脑 机 交互 等 技术 
键盘 鼠标 操纵杆 打印机   显示器   音 箱等 输出设备 
4 . 语言识别 语言 tts 等 Ar vr none5 . 
机器学习 决策树   贝叶斯 knn   等 Other 人工智能 技术 
包含 了 机器学习 知识图谱 自然语言 处理 人机交互 计算机 视觉 生物 
特征 识别 AR / VR 七个 关键技术 ref 什么 是 
自然 语言 处理 自然语言 处理 是 研究 在 人 与人 
交际 中 以及 人 与 计算机 交际 中的 语言 问题 
的 一门 学科 自然语言 处理 要 研制 表示 语言 能力 
linguistic competence 和 语言 应用 linguistic performance 的 模型 建立 
计算 框架 来 实现 这样 的 语言 模型 提出 相应 
的 方法 来 不断 完善 这样 的 语言 模型 根据 
这样 的 语言 模型 设计 各种 实用 系统 并 探讨 
这些 实用 系统 的 评测 技术 根据 这个 定义 自然语言 
处理 要 研究 在 人 与人 交际 中 以及 人 
与 计算机 交际 中的 语言 问题 既要 研究 语言 又要 
研究 计算机 因此 它 是 一门 交叉学科 它 涉及 语言学 
计算机科学 数学 自动化 等 不同 学科 以 宗 成庆 所著 
统计 自然语言 处理 为例 其 在 统计 自然语言 处理 的 
理论 方面 首先 介绍 相关 的 基础 知识 例如 概率论 
和 信息论 的 基本 概念 形式语言 和 自动机 的 基本 
概念 由于 统计 自然语言 处理 是以 语料库 和 词汇 知识库 
为 语言 资源 的 因此 接下来 本书 讲解 了 语料库 
和 词汇 知识库 的 基本 原理 语言 模型 和隐/nr 马尔可夫 
模型 是 统计 自然语言 处理 的 基础理论 在 统计 自然语言 
处理 中 具有 重要 地位 因此 本书 介绍 了 语言 
模型 的 基本 概念 并 讨论 了 各种 平滑 方法 
和 自适应 方法 又 介绍 了 隐 马尔可夫 模型 和 
参数估计 的 方法 接着 本书 分别 论述 了 在 词法 
分析 与 词性 标注 中的 统计 方法 在 句法分析 中的 
统计 方法 在 词汇 语义 中的 统计 方法 基于 统计 
的 自然 语言 处理 的 理论 基础 是 哲学 中 
的 经验 主义 基于 规则 的 自然 原因 处理 的 
理论 基础 是 哲学 中 的 理性主义 说到底 这个 问题 
是 关于 如何 处理 经验主义 和 理论 主义 关系 的 
问题 自然语言 处理 研究 的 内容 机器翻译 machine translation MT 
实现 一种 语言 到 另一种 语言 的 自动 翻译 自动 
文摘 automatic abstracting 将 文档 的 主要 内容 和 含义 
自动 归纳 提炼 形成 摘要 信息检索 information retrieval 从 海量 
文档 中 找到 符合 用户 需要 的 相关 文档 文档 
分类 document categorization / classification 对 大量 的 文档 按照 
一定 的 分类 标准 例如 根据 主题 或 内容 划 
分等 实现 自动 归类 文档 分类 也称 文本 分类 text 
categorization / classification 或 信息 分类 information categorization / classification 
近年来 情感 分类 sentiment classification 或称 文本 倾向性 识别 text 
orientation identification 成为 本 领域 研究 的 热点 问答 系统 
question answering system 对 用户 提出 的 问题 的 理解 
利用 自动 推理 等 手段 在 有关 知识 资源 中 
自动 求解 答案 并 做出 相应 的 回答 信息 过滤 
information filtering 自动 识别 和 过滤 那些 满足 特定 条件 
的 文档 信息 信息 抽取 information extraction 指 从 文本 
中 抽取 出 特定 的 事件 event 或 事实 信息 
有时候 又称 事件 抽取 event extraction 信息 抽取 与 信息检索 
不同 信息 抽取 直接 从 自然语言 文本 中 抽取 信息 
框架 一般 是 用户 感兴趣 的 事实 信息 而 信息检索 
主要 是 从 海量 文档 集合 中 找到 与 用户 
需求 一般 通过 关键词 表达 相关 的 文档 列表 而 
信息 抽取 则是 希望 直接 从 文本 中 获得 用户 
感兴趣 的 事实 信息 当然 信息 抽取 与 信息检索 也有 
密切 的 关系 信息 抽取 系统 通常 以 信息 检索系统 
如 文本 过滤 的 输出 作为 输入 而 信息 抽取 
技术 又 可以 用来 提高 信息检索 系统 的 性能 信息 
抽取 与 问答 系统 也 有 密切 的 联系 一般而言 
信息 抽取 系统 要 抽取 的 信息 是 明定 的 
事先 规 定好 的 系统 只是 将 抽取 出来 的 
事实 信息 填充 在 给定 的 框架 槽 里 而 
问答 系统 面对 的 用户 问题 往往 是 随机 的 
不 确定 的 而且 系统 需要 将 问题 的 答案 
生成 自然语言 句子 通过 自然 规范 的 语句 准确 地 
表达 出来 使 系统 与 用户 之间 形成 一问一答 的 
交互 过程 文本 挖掘 text mining 从 文本 多指 网络 
文本 中 获取 高 质量 信息 的 过程 文本 挖掘 
技术 一般 涉及 文本 分类 文本 聚 类 text clustering 
概念 或 实体 抽取 concept / entity extraction 粒度 分类 
情感 分析 sentiment analysis 自动 文摘 和 实体 关系 建模 
entity relation modeling 等 多种 技术 舆情 分析 public opinion 
analysis 舆情 是 较多 群众 关于 社会 中 各种 现象 
问题 所 表达 的 信念 态度 意见 和 情绪 等 
等 表现 的 总和 显然 舆情 分析 是 一项 十分 
复杂 涉及 问题 众多 的 综合性 技术 它 涉及 网络 
文本 挖掘 观点 意见 挖掘 opinion mining 等 各 方面 
的 问题 隐喻 计算 metaphorical computation 研究 自然语言 语句 或 
篇章 中 隐喻 修辞 的 理解 方法 文字 编辑 和 
自动 校对 automatic proofreading 对 文字 拼写 用词 甚至 语法 
文档 格式 等 进行 自动 检查 校对 和 编排 作文 
自动 评分 对/p 作文/n 质量/n 和/c 写作/v 水平/n 进行/v 自动/vn 
评价/n 和/c 打分/v 语音识别/i speech recognition 将 输入 的 语音信号 
识别 转换成 书面语 表示 文语/nr 转换 text to speech conversion 
将 书面 文本 自动 转换 成 对应 的 语音 表征 
又称 语音合成 speech synthesis 说 话人 识别 ／ 认证 ／ 
验证 speaker recognition / identification / verification 对 说 话人 
的 言语 样本 做 声学 分析 依此 推断 确定 或 
验证 说 话人 的 身份 自然语言 处理 涉及 的 几个 
层次 如果 撇开 语音学 研究 的 层面 自然语言 处理 研究 
的 问题 一般 会 涉及 自然 语言 的 形态学 语法学 
语义学 和 语用学 等 几个 层次 形态学 morphology 形态学 又称 
词汇 形态学 或 词法 是 语言学 的 一个 分支 研究 
词 的 内部 结构 包括 屈折 变化 和 构词法 两个 
部分 由于 词 具有 语音 特征 句法 特征 和 语义 
特征 形态学 处于 音位学 句 法学 和 语义学 的 结合部位 
所以 形态学 是 每个 语言学家 都要 关注 的 一门 学科 
语法学 syntax 研究 句 子结构 成分 之间 的 相互 关系 
和 组成 句子 序列 的 规则 其 关注 的 中心 
是 为什么 一句话 可以 这么 说 也 可以 那么 说 
语义学 semantics 语义学 的 研究 对象 是 语言 的 各级 
单位 词素 词 词组 句子 句 子群 整段 整篇 的 
话语 和 文章 乃至 整个 著作 的 意义 以及 语义 
与 语音 语法 修辞 文字 语境 哲学思想 社会 环境 个人 
修养 的 关系 等等 其 重点 在 探明 符号 与 
符号 所指 的 对象 之间 的 关系 从而 指导 人们 
的 言语 活动 它 所 关注 的 重点 是 这个 
语言 单位 到底 说 了 什么 语用学 pragmatics 是 现代 
语言学 用来 指 从 使用者 的 角度 研究 语言 特别 
是 使用者 所作 的 选择 他们 在 社会 互动 中 
所受 的 制约 他们 的 语言 使用 对 信 递 
活动 中 其他 参与者 的 影响 目前 还 缺乏 一种 
连贯 的 语用 学理论 主要 是 因为 它 必须 说明 
的 问题 是 多方面 的 包括 直指 会话 隐含 预设 
言语 行为 话语 结构 等 部分 原因 是 由于 这 
一 学科 的 范围 太 宽泛 因此 出现 多种 不 
一致 的 定义 从 狭隘 的 语言 学 观点 看 
语用学 处理 的 是 语言 结构 中有 形式 体现 的 
那些 语境 相反 语用学 最 宽泛 的 定义 是 研究 
语义学 未能 涵盖 的 那些 意义 因此 语用学 可以 是 
集中 在 句子 层次 上 的 语用 研究 也 可以 
是 超出 句子 对 语言 的 实际 使用 情况 的 
调查 研究 甚至 与 会话 分析 语 篇 分析 相结合 
研究 在 不同 上下 文中 的 语句 应用 以及 上下文 
对 语句 理解 所 产生 的 影响 其 关注 的 
重点 在于 为什么 在 特定 的 上下 文中 要说 这 
句话 在 实际 问题 的 研究 中 上述 几 方面 
的 问题 尤其 是 语义学 和 语用学 的 问题 往往 
是 相互 交织 在 一起 的 语法 结构 的 研究 
离不开 对 词汇 形态 的 分析 句子 语义 的 分析 
也 离不开 对 词汇 语义 的 分析 语法 结构 和 
语用 的 分析 它们 之间 往往 互为 前提 自然语言 处理 
面临 的 困难 根据 上面 的 介绍 自然语言 处理 涉及 
形态学 语法学 语义学 和 语用学 等 几个 层面 的 问题 
其 最终 应用 目标 包括 机器翻译 信息检索 问答 系统 等 
非常 广泛 的 应用 领域 其实 如果 进一步 归结 实现 
所有 这些 应用 目标 最终 需要 解决 的 关键 问题 
就是 歧义 消解 disambiguation 问题 和 未知 语言 现象 的 
处理 问题 一方面 自然 语言 中 大量 存在 的 歧义 
现象 无论 在 词法 层次 句法 层次 还是 在 语义 
层次 和 语用 层次 无论 哪 类 语言 单位 其 
歧义 性 始终 都是/nr 困扰 人们 实现 应用 目标 的 
一个 根本 问题 因此 如何 面向 不同 的 应用 目标 
针对 不同 语言 单位 的 特点 研究 歧义 消解 和 
未知 语言 现象 的 处理 策略 及 实现 方法 就成 
了 自然 语言 处理 面临 的 核心 问题 另一方面 对于 
一个 特定 系统 来说 总是 有 可能 遇到 未知 词汇 
未知 结构 等 各种 意想不到 的 情况 而且 每 一种 
语言 又都 随着 社会 的 发展 而 动态 变化 着 
新的 词汇 尤其 是 一些 新 的 人名 地名 组织 
机构 名 和 专用 词汇 新的 词义 新的 词汇 用法 
新 词类 甚至 新的 句子 结构 都在/nr 不断 出现 尤其在 
口语 对话 或 计算机网络 对话 微博 博客 等中 稀奇古怪 的 
词语 和 话语 结构 更 是 司空见惯 因此 一个 实用 
的 自然 语言 处理 系统 必须 具有 较好 的 未知 
语言 现象 的 处理 能力 而且 有 足够 的 对 
各种 可能 输入 形式 的 容错 能力 即 我们 通常 
所说 的 系统 的 鲁棒性 robustness 问题 当然 对于 机器翻译 
信息检索 文本 分类 等 特定 的 自然 语言 处理 任务 
来说 还 存在 若干 与 任务 相关 的 其他 问题 
诸如 如何 处理 不同 语言 的 差异 如何 提取 文本 
特征 等 总而言之 目前 的 自然 语言 处理 研究 面临着 
若干 问题 的 困扰 既有 数学模型 不够 奏效 有些 算法 
的 复杂度 过高 鲁棒性 太差 等 理论 问题 也有 数据 
资源 匮乏 覆盖率 低 知识 表示 困难 等 知识 资源 
方面 的 问题 当然 还有 实现 技术 和 系统 集成 
方法 不够 先进 等 方面 的 问题 接触 自然语言 已有 
两年 下面 谈一谈 自己 的 一些 理解 文本 基本 处理 
过程 1 . 获取数据 可以 是 任何 文本 类型 的 
数据 自有 的 或者 爬虫 爬 取 的 数据 曾经 
用 几行 代码 写 了 一个 爬虫 爬 取了 几 
万条 商品 评论 还是 很好 用 的 2 . 数据 
预处理 这 一部分 很 重要 很 重要 很 重要 有 
可能 会 决定 着 你 文本处理 任务 的 最终 质量 
1 观察 数据 尤其 是 网上 的 数据 质量 参差不齐 
一定 要 先 观察 数据 有 没有 异常 符号 有的 
时候 有 很多 空格 或者 会有 换行 这些 符号 都要/nr 
首先 去掉 我 观察 的 方式 也 很简单 把 数据 
统一 存储 到 一个 csv 文件 里 正常 情况 下 
文本 非常 整齐 如果/c 有/v 特殊/a 符号/n 可能会/i 让/v 一段/m 
文本/n 被/p 分开/v 成好/nr 多列/m 看到/v 有/v 分成/v 好/a 多列/m 
的/uj 情况/n 那/r 可能/v 有/v 特殊符号/n 处理 了 就行 2 
分词 因为 中文 词语 之间 没有 空格 所以 一定 要 
分词 分词 的 工具 很多 做 数据 分析 我 一直 
用 Python 推荐 使用 jieba 分词 个人 感觉 很好 用 
3 去除 停用词 停用词 就是 那些 几乎 出现 在 每 
一篇 文本 数量 很多 但是 没有 区分度 的 词 比如 
的 一 等等 如果 做 分类 的话 这些 词 对 
处理 结果 有 一定 影响 所以 可以 考虑 先 去掉 
停用词 词表 一般 网上 都有 可以 找 一个 适合 自己 
的 用 3 . 特征 工程 做 过 比赛 的 
人都 知道 特征 工程 有 多重 要了 文本 特征 需要 
自己 构建 最 简单 的 就是 词 袋 模型 还 
可以 用 n gram 模型 ti idf 模型 但是 这些 
模型 共同 的 特点 就是 太 稀 疏了 一般 情况 
下 需要 降 维 比如 SVD 其实 很多 模型 也 
可以 用来 进行 特征选择 比如 决策树 L1 正则 也 可以 
用来 进行 特征选择 具体 原理 这里 就不 讲了 是不是 很 
复杂 其实 已有 一个 强大 的 工具 帮 我们 做好 
了 sklearn 超级 好用 4 . 正式 干活 至此 你 
的 数据 应该 还 算是 比较 干净 了 可以 开始 
做 下 一步 工作 比如 分类 聚 类 命名 实体 
识别 事件 抽取 机器翻译 太多 了 还是 很 有意思 的 
欢迎 交流 指导 一 想到 每次 被 同学 问到 你 
在 做什么 我 回答 自然语言 处理 同学 就会 说 这么 
难 的 东西 你 也 做 我 就 觉得 很多 
人 觉得 自然语言 处理 很难 其实 是 对 自然 语言 
处理 的 误解 至少 入门 没 大家 想象 的 那么 
难 其实 自然语言 处理 不是 难 在 算法 复杂 而是 
语句 的 结构 因为 不管 是 在 商品 评论 或是 
社交 平台 上 人 的 发言 最近 闲来无事 正好 自己 
也 在做 一个 微博 话题 情感 分析 的 工具 所以 
准备 将 整个 过程 记录 下来 以供 想 入门 的 
同学 参考 所有 的 处理 都 要在 词性 标注 的 
文件 上 进行 所以 要 进行 文本 分析 首先 要 
进行 分词 现有 的 分词 工具 有 不少 我 自己 
用 的 是 中科院 开发 的 ICTCLAS 目前 更新 到 
2016版 可以 在 http / / ictclas . nlpir . 
org / 下载 下面 讲 一下 如何 来 应用 这个 
工具 来 进行 分词 我 平时 写 程序 是 用 
C / C + + 所以 在 这里 以 C 
/ C + + 为例 下载 好 解压 之后 如 
下边 的 Data 文件夹 复制 到 自己 工程 下 该 
Data 文件夹 下 存放 了 词典 配置文件 和 许可证 信息 
同时 在 lib 文件 夹下 选择 与 自己 系统 相关 
的 NLPIR . dll 文件 和 NLPIR . lib 文件 
复制 到 自己 工程 下 然后 将 include 文件 夹下 
的 NLPIR . h 复制 到 工程 下 现在 就 
可以 开始 在 自己 的 工程 中 编码 进行 分词 
了 开发环境 我 选择 的 是 VS2012 如何 建 工程 
这里 就 不说 了 如何 建 工程 这里 就 不说 
了 要想 使用 NLPIR 动态链接库 还有 引入 头文件 也 就是 
刚才 我们 复制 到 工程 下边 的 NLPIR . h 
此外 用 # pargma comment 指定 要 连接 的 库 
# include NLPIR . h # pragma comment lib NLPIR 
. lib 现在 我们 可以 调用 它 的 分词 函数 
进行 分词 我们 写 一个 函 数来 调用 该 库 
中的 函数 int participle char * src _ file char 
* result _ file char * userdict { if NLPIR 
_ Init return 1 NLPIR _ SetPOSmap ICT _ POS 
_ MAP _ FIRST NLPIR _ ImportUserDict userdict NLPIR _ 
FileProcess src _ file result _ file 1 NLPIR _ 
Exit return 0 } 其中 NLPIR _ Init 为 初始化 
函数 NLPIR _ SetPOSmap 设置 标注 集 ICT _ POS 
_ MAP _ FIRST 计算所 一级 标注 集 ICT _ 
POS _ MAP _ SECOND 计算所 二级 标注 集 PKU 
_ POS _ MAP _ SECOND 北大 二级 标注 集 
PKU _ POS _ MAP _ FIRST 北大 一级 标注 
集 NLPIR _ ImportUserDict 导入 用户 词典 来 提高 分词 
精度 NLPIR _ FileProcess src _ file result _ file 
1 进行 分词 其中 src _ file 为 需要 分词 
的 文件 result _ file 为 分词 结果 文件 现在 
文本 经过 分词 已经 是 标注 好 的 文本 如 
所示 可以 看到 在 分词 结果 中 每个 词 后面 
都是 跟 / 在跟 词性 为了 方便 后面 处理 我们 
将 / 过滤掉 具体 程序 如下 int resultFilter char * 
src _ file char * result _ file { int 
i = 0 ifstream ifs _ tfilter _ file ofstream 
ofs _ tfilter _ result _ file char cixing ifs 
_ tfilter _ file . open src _ file ios 
in ofs _ tfilter _ result _ file . open 
result _ file ios ate while ifs _ tfilter _ 
file . eof { ifs _ tfilter _ file cixing 
if cixing = = \ / { ofs _ tfilter 
_ result _ file i = 1 } else { 
ofs _ tfilter _ result _ file cixing if i 
= = 1 { ofs _ tfilter _ result _ 
file i = 0 } } } return 0 } 
过滤 之后 的 结果 如 所示 过滤 / 后的/nr 文本 
就 比较 方便 我们 后续 的 其他 处理 了 至此 
自然语言 处理 的 第一 步 分词 就 结束 了 尽管 
ICTCLAS 分词 系统 精度 已经 很高 但在 某一 领域 中 
其 分词 也有 不 准确 的 时候 所以 在 分词 
完成后 应该/v 人工/n 检验/vn 一下/m 是否/v 有和/nr 自己/r 比较/d 相关/v 
的/uj 词/n 被/p 切分/ad 错误/n 的/uj 情况/n 然后 筛选出 来 
作为 添加 词典 这样 便 可以 保证 其 准确 了 
在 自然 语言 处理 中 经常 要 计算 单词 序列 
句子 出现 的 概率 估计 我们 知道 算法 在 训练 
时 语料库 不 可能 包含 所有 可能 出现 的 序列 
因此 为了 防止 对 训练 样本 中 未 出现 的 
新 序列 概率 估计值 为零 人们 发明 了 好多 改善 
估 计新 序列 出现 概率 的 算法 即 数据 平滑 
算法 Laplace 法则 最 简单 的 算法 是 Laplace 法则 
思路 很 简单 统计 测试数据 集中 的 元素 在 训练 
数据 集中 出现 的 次数 时 计数器 的 初始值 不要 
设成 零 而是 设成 １ 这样 即 使该 元素 没有 
在 训练 集中 出现 其 出现 次数 统计 值 至少 
也是 １ 因此 其 出现 的 概率 估计值 就 不会 
是 零 了 假设 测试 集 VV 中 某个 元素 
在 训练 集 TT 中 出现 rr 次 经过 Laplace 
法则 调整 后的/nr 统计 次数 为 r ∗ = r 
+ 1r ^ * = r + 1 当然 这样做 
纯粹 是 为了 不 出现 零 概率 并 没有 解决 
对 未 见过 的 实例 进行 有效 预测 的 问题 
因此 Laplace 法则 仅仅 是 一种 非常 初级 的 技术 
有点 太 小儿科 了 Good Turing 估计 Laplace 方法 一个 
很 明显 的 问题 是 ∑ r ∗ ≠ ∑ 
r \ sum { r ^ * } \ ne 
\ sum r Good Turning 方法 认为 这 是 一个 
重大 缺陷 需要 给予 改进 其实 我 觉得 这 真 
不算 重要 只要 能 合理 估计 未 见过 的 新 
实例 的 概率 总 的 统计 次数 发生 变化 又 
怎样 呢 Good Turing 修正 后的/nr 计算 公式 还 真的 
很 巧妙 它 在 Laplace 法则 后面 乘 了 一个 
修正 系数 就 可以 保证 总次数 不变 这个 拿出 来 
炫 一 炫 还是 没 问题 的 r ∗ = 
r + 1 nr + 1nrr ^ * = r 
+ 1 \ frac { n _ { r + 
1 } } { n _ r } 其中 nrn 
_ r 表示 测试 集 VV 中 一 共有 nrn 
_ r 个 元素 在 训练 集 TT 中 出现 
过 nrn _ r 次 虽然 我 觉得 这个 方法 
没啥 用 但是 它 的确 保证 了 测试 集中 元素 
在 训练 集中 出现 的 总次数 不变 即 N1 = 
∑ r = 0 ∞ rnr = 0 × n0 
+ 1 × n1 + 2 × n2 + . 
. . N2 = ∑ r = 0 ∞ r 
∗ nr = 1 × n1n0 × n0 + 2 
× n2n1 × n1 + . . . = 1 
× n1 + 2 × n2 + . . . 
\ begin { matrix } N _ 1 = \ 
sum _ { r = 0 } ^ { \ 
infty } rn _ r = 0 \ times n 
_ 0 + 1 \ times n _ 1 + 
2 \ times n _ 2 + . . . 
\ \ N _ 2 = \ sum _ { 
r = 0 } ^ { \ infty } r 
^ * n _ r = 1 \ times \ 
frac { n _ 1 } { n _ 0 
} \ times { n _ 0 } + 2 
\ times \ frac { n _ 2 } { 
n _ 1 } \ times { n _ 1 
} + . . . \ \ = 1 \ 
times n _ 1 + 2 \ times n _ 
2 + . . . \ end { matrix } 
显然 N1 = N2N _ 1 = N _ 2 
或许 这个 方法 解决 不 了 自然 语言 处理 问题 
而且 nr = 0n _ r = 0 时 公式 
也会 失效 但 其 思路 应该 还是 很 有价值 的 
或许 解决 其他 问题 能 用得上 绝对 折扣 和 线性 
折扣 估计 发明 的 作者 受到 Good Turing 的 刺激 
了 认为 这个 方法 就是 劫富济贫 把 数量 较大 的 
统计 次数 拿出 一 部分 均 给了 较小 的 统计 
次数 减少 贫富差距 只不过 这个 方法 用 了 一个 很 
有 技巧 的 公式 掩盖 的 其本质 与其 羞羞答答 劫富济贫 
不如 来个 赤裸裸 的 方法 于是乎 就 出现 了 绝对 
折扣 和 线性 折扣 方法 问题 是 劫富济贫 并 不是 
我们 的 目的 我们 需要 的 是 能够 对 语料库 
中 从未 出现 过 的 句子 做出 概率 判断 要 
得到 正确 的 判断 需要 劫 多少 济 多少 这个/r 
问题/n 绝对/d 折扣/v 和/c 线性/n 折扣/v 都/d 回答/v 不了/v 所以 
无论 Good Turing 方法 还是 这 两种 折扣 方法 本质上 
都没 跳出 Laplace 法则 的 思路 Witten Bell 算法 Witten 
Bell 算法 终于 从 Laplace 算法 跳 了 出来 有了 
质 的 突破 这个 方法 的 基本 思想 是 如果 
测试 过程 中 一个 实例 在 训练 语料库 中 未 
出现 过 那么 他 就是 一个 新 事物 也 就是说 
他 是 第一 次 出现 那么 可以 用 在 语料库 
中 看到 新 实例 即 第一 次 出现 的 实例 
的 概率 来 代替 未 出现 实例 的 概率 假设 
词汇 在 语料库 出现 的 次数 参见 下表 rr12345nrn _ 
r5040302010 则 N = 1 × 50 + 2 × 
40 + 3 × 30 + 4 × 20 + 
5 × 10 = 350T = 50 + 40 + 
30 + 20 + 10 = 150 \ begin { 
matrix } N = 1 \ times 50 + 2 
\ times 40 + 3 \ times 30 + 4 
\ times 20 + 5 \ times 10 = 350 
\ \ T = 50 + 40 + 30 + 
20 + 10 = 150 \ \ \ end { 
matrix } 那么 我们 可以 用 TN + T = 
150350 + 150 = 0.3 \ frac { T } 
{ N + T } = \ frac { 150 
} { 350 + 150 } = 0.3 近似 表示 
在 语料库 看到 新 词汇 的 概率 我 不能 说 
这个 方法 有 多少 道理 但 与 那些 劫富济贫 的 
方法 相比 它 至少 提供 了 一个 说 得 过去 
的 理由 扣留/v 估计/v 和/c 交叉/n 检验/vn 扣留/v 估计/v 和/c 
交叉/n 检验/vn 这/r 两种/m 方法/n 估计/v 是/v 受到/v Witten/w Bell 
算法 启发 了 但是 思路 没 跳出 该 方法 套路 
而且 手法 比较 卑劣 和 Witten Bell 算法 一样 对于 
所有 遇到 的 新事物 都给 出 完全 相同 的 概率 
预测 插值 算法 前面 的 平滑 算法 对于 从来 没 
出现 的 n gram 都 给与 相同 的 概率 估计 
有些 情况 下 这 并不 合适 事实上 我们 可以 考虑 
根据 n gram 中的 n 1 gram 的 频率 产生 
一个 更好 的 概率 估计 如果 n 1 gram 很少 
出现 就 给 n gram 一个 较小 的 估计 反之 
给出 一个 较大 的 估计 例如 假定 要在 一批 语料库 
上 构建 二元 语法 模型 其中 有两/nr 对词 的 同现 
次数 为 0 C send   the = 0 C 
send   thou = 0 \ begin { matrix } 
C send \ the = 0 \ \ C send 
\ thou = 0 \ \ \ end { matrix 
} 那么 按照 前面 提到 的 任何 一种 平滑 方法 
都 可以 得到 p the | send = p thou 
| send p the | send = p thou | 
send 但是 直觉 上 我们 认为 应该 有p/nr the | 
send p thou | send p the | send p 
thou | send 因为 冠词 the 要比 单词 thou 出现 
的 频率 要高 得多 因此 可以 通过 组合 不同 信息 
资源 的 方法 来 产生 一个 更好 的 模型 基本思路 
一般来讲 使用/v 低阶/n 的/uj ｎ/w 元/m 模型/n 向/p 高阶/nr ｎ/w 
元/m 模型/n 插值/n 是/v 有效/a 的/uj 因为 当 没有 足够 
的 语料 估计 高阶 模型 时 低阶 模型 往往 可以 
提供 有用 的 信息 例如 bigram 模型 中 的 删除 
插值法 最 基本 的 做法 是 Pinterp wi | wi 
− 1 = λ PML wi | wi − 1 
+ 1 − λ PML wi     0 ≤ 
λ ≤ 1P _ { interp } w _ i 
| w _ { i 1 } = \ lambda 
P _ { ML } w _ i | w 
_ { i 1 } + 1 \ lambda P 
_ { ML } w _ i \ \ 0 
\ le \ lambda \ le1 由于 PML the | 
send = PML thou | send = 0P _ { 
ML } the | send = P _ { ML 
} thou | send = 0 而且 PML the ≫ 
PML thou P _ { ML } the \ gg 
P _ { ML } thou 所以 Pinterp the | 
send Pinterp thou | send P _ { interp } 
the | send P _ { interp } thou | 
send 定义 在 统计 自然语言 处理 中 这种方法 通常 被 
称为 线性 插值法 Linear Interpolation 在 其他 的 地方 常常 
被 称为 混合模型 Mixture Model 插值 模型 的 递归 定义 
如下 Pinterp wi | wi − n − 1 . 
. . wi − 1 = λ PML wi | 
wi − n − 1 . . . wi − 
1 + 1 − λ Pinterp wi | wi − 
n − 2 . . . wi − 1 P 
_ { interp } w _ i | w _ 
{ i n 1 } . . . w _ 
{ i 1 } = \ lambda P _ { 
ML } w _ i | w _ { i 
n 1 } . . . w _ { i 
1 } + 1 \ lambda P _ { interp 
} w _ i | w _ { i n 
2 } . . . w _ { i 1 
} 例子 对于 三元 模型 有 Pinterp wi | wi 
− n − 1 . . . wi − 1 
= λ PML wi | wi − n − 1 
. . . wi − 1 + 1 − λ 
Pinterp wi | wi − 1 P _ { interp 
} w _ i | w _ { i n 
1 } . . . w _ { i 1 
} = \ lambda P _ { ML } w 
_ i | w _ { i n 1 } 
. . . w _ { i 1 } + 
1 \ lambda P { interp } w _ i 
| w _ { i 1 } 对于 二元 模型 
有 Pinterp wi | wi − 1 = λ PML 
wi | wi − 1 + 1 − λ PML 
wi P _ { interp } w _ i | 
w _ { i 1 } = \ lambda P 
_ { ML } w _ i | w _ 
{ i 1 } + 1 \ lambda P _ 
{ ML } w _ i 于是 Pinterp = . 
. . = λ 3PML wi | wi − 2wi 
− 1 + λ 2PML wi | wi − 1 
+ λ 1PML wi     λ 1 + λ 
2 + λ 3 = 1P _ { interp } 
= . . . = \ lambda _ 3P _ 
{ ML } w _ i | w _ { 
i 2 } w _ { i 1 } + 
\ lambda _ 2P _ { ML } w _ 
i | w _ { i 1 } + \ 
lambda _ 1P _ { ML } w _ i 
\ \ \ lambda _ 1 + \ lambda _ 
2 + \ lambda _ 3 = 1Katz 回退 算法 
katz 方法 也 是 一种 插值 方法 不过 它 仅对 
语料库 中 未 出现 的 数据 进行 预测 总起来说 这类 
插值 方法 与 传统 数学 中用 已知 数据 插值 预测 
未知 数据 的 方法 思路 相同 属于 比较 靠谱 的 
方法 参考文献 1 陈鄞/nr 自然语言 处理 基本 理论 和 方法 
哈尔滨 工业 大学 出版社 第 1版 2013年 8月 1日 出版社 
机械 工业 出版社 ISBN 9787111597674 出版 时间 2018 06 01 
作者 涂铭 刘祥 刘树春/nr Python 自然语言 处理 实战 今天 注册 
了 IBM Bluemix 的 30天 免费 账号 大概 了解 了 
下 其 提供 的 自然 语言 处理 功能 这些 自然 
语言 处理 还是 浅层 的 处理 包括 了 识别 概念 
实体 关键字 类别 观点 情绪 关系 语义 角色 并不 包含 
推理 等 深层 处理 一个 典型 的 自然 语言 处理 
pipline 包括 意图 识别 实体 识别 语气 识别 上下文 分析 
和 知识 扩展 基于 Bluemix 在云端 实现 一个 领域 内 
的 chatbot 不再 是 难事 Bluemix 这些 开放 NLP 平台 
的 出现 对于 一些 小 公司 的 NLP 团队 来说 
不是 好事 了 一般 的 互联网 开发 人员 掌握 这些 
工具 就 可以 开发 一些 NLP 功能 了 在云端 大 
公司 有着 各种 优势 在 嵌入式 等 端上 语义 理解 
会 是 什么样 的 情景 呢 转载 原文 地址 https 
/ / gitbook . cn / gitchat / geekbook / 
5 b 9 8 8 b 4 e c a 
9 9 1 0 6 5 4 c 0 8 
2 3 f 5 / topic / 5 b 9 
9 3 d 6 6 c a 9 9 1 
0 6 5 4 c 0 8 4 8 5 
3 第五章 趋势 篇 随着 深度 学习 时代 的 来临 
神经 网络 成为 一种 强大 的 机器学习 工具 自然语言 处理 
取得 了 许多 突破性 发展 情绪 分析 自动 问答 机器 
翻译 等 领域 都 飞速发展 下图 分别 是 AMiner 计算出 
的 自然 语言 处理 近期 热点 和 全球 热点 通过 
对 1994 2017 年间 自然语言 处理 领域 论文 的 挖掘 
总结 出 二十 多年来 自然语言 处理 的 领域 关键词 主要 
集中 在 计算机 语言 神经网络 情感 分析 机器翻译 词义 消 
歧 信息提取 知识库 和 文本 分析 等 领域 旨在 基于 
历史 的 科研 成果 数据 的 基础 上 对 自然 
语言 处理 热度 甚至 发展 趋势 进行 研究 图中 每个 
彩色 分支 表示 一个 关键词 领域 其 宽度 表示 该 
关键词 的 研究 热度 各 关键词 在 每一 年份 纵轴 
的 位置 是 按照 这 一 时间 点 上 所有 
关键词 的 热度 高低 进行 排序 图 14 自然语言 处理 
近期 热点 图图 15 自然语言 处理 全球 热点 图 显示 
情绪 分析 词义 消 歧 知识库 和 计算机 语言学 将 
是 最近 的 热点 发展趋势 显示 词义 消 歧 词义 
理解 计算机 语言学 信息检索 和 信息 提取 将 是 自然 
语言 处理 全球 热点 我们 同时 在 微博 @ ArnetMiner 
中 发起 了 关于 自然 语言 处理 未来 发展 趋势 
的 投票 得到 了 如下 结果 文本 理解 与 推理 
浅层 分析 到 深度 理解 135 28.1% 对话 机器人 实用化 
场景 化 83 17.3% NLP 行业 与 专业 领域 结合 
74 15.4% 学习 模式 先验 语言 知识 与 深度 学习 
结合 45 9.4% 文本 情感 分析 事实性 文本 到 情感 
性 文本 43 9% 语言 知识 人工 构建 到 自动 
构建 25 5.2% 信息检索 跨语言 多媒体 23 4.8% 文本 生成 
规范 文本 到 自由 文本 15 3.1% NLP 平台 化 
封闭 到 开放 13 2.7% 对抗 训练 思想 的 应用 
9 1.9% 共有 465 人次 参与 了 投票 文本 理解 
与 推 理由 浅层 分析 到 深度 理解 有 135 
人次 支持 占 比 28.1% 对话 机器人 实用化 场景 化 
NLP 行业 与 专业 领域 结合 学习 模式 由 先验 
语言 知识 与 深度 学习 结合 以及 文本 情感 分析 
由 传统 媒体 到 社交 媒体 依次 排列 分别 占 
比 17.3% 15.4% 9.4% 和 9% 我们 依据 排列 由 
高到低/nr 选取 其中 几项 展开 介绍 文本 理解 与 推理 
浅层 分析 向 深度 理解 迈进 Google 等 公司 已经 
推出 了 以 阅读 理解 作为 深入 探索 自然语言 理解 
的 平台 文本 理解 和 推理 是 自然 语言 处理 
的 重要 部分 现在 的 机器 软件 已经 可以 根据 
文本 的 语境 上下文 分辨 代词 等 指示 词 这是/i 
文本/n 理解/v 与/p 推理/v 从/p 浅层/n 分析/vn 向/p 深度/ns 理解/v 
迈进/v 的/uj 重要/a 一步/m 对话 机器人 实用化 场景 化 从 
最初 2012年 到 2014年 的 语音 助手 到 2014年 起 
逐渐 出现 的 聊天 机器人 微软 小冰 百度 小 度 
再到 2016年 哈工/nr SCIR 笨笨 对话 机器 人 越来越 智能 
最初 的 语音 助手 可以 听得到 但是 听 不懂 之后 
的 对话 机器人 可以 听得懂 但是 实用性 却 不强 现在 
对话 机器 人 更多 的 是 和 场景 结合 即 
做 特定 场景 时 有用 的 人机对话 NLP + 行业 
与 专业 领域 深度 结合 银行 电器 医药 司法 教育 
等 领域 对 自然 语言 处理 的 需求 都 非常 
多 自然语言 处理 与 各行各业 的 结合 越来越 紧密 专业化 
的 服务 趋势 逐渐 增强 刘挺/nr 教授 预测 自然语言 处理 
首先 会 在 信息 准备充分 并且 服务 方式 本身 就是 
知识 和 信息 的 领域 产生 突破 例如 医疗 金融 
教育 和 司法 领域 学习 模式 先验 语言 知识 与 
深度 学习 结合 自然语言 处理 中 学习 模式 有 一个 
较为 明显 的 变化 在 浅层 到 深层 的 学习 
模式 中 浅层 学习 是 分 步骤 的 深度 学习 
的 方法 贯穿 在 浅层 分析 的 每个 步骤 中 
由 各个 步骤 连接 而成 而 直接 的 深度 学习 
则是 直接 的 端 到 端 人为 贡献 的 知识 
在 深度 学习 中 所占 的 比重 大 幅度 减小 
但/c 如何/r 将/d 深度/ns 学习/v 应用/v 于/p 自然/d 语言/n 处理/v 
需要/v 进行/v 更多/d 的/uj 研究/vn 和/c 探索/v 针对 不同 任务 
的 不同 字词 表示 将/d 先验/n 知识/v 和/c 深度/ns 学习/v 
相/v 结合/v 是/v 未来/t 的/uj 一个/m 发展/vn 趋势/n 文本 情感 
分析 事实性 文本 到 情感 文本 之前 的 研究 主要 
是 新闻 领域 的 事实 性 文本 现在 情感 文本 
分析 更 受重视 并且 在 商业 和 政府 舆情 上 
可以 得到 很好 地 应用 如/v 2017年/tdq 新浪/nz 微/n 舆情/n 
和/c 哈工大/nt 推出/v 情绪 地图 网民 可以 登录 新浪 舆情 
官方 网站 查询 任何 关键词 的 情绪 地图 这是 语义 
情绪 分析 在 舆情 分析 产业 的 首次 正式 应用 
目录 文章 目录 目录 前言 Markov 模型 1Markov 模型 2Markov 
模型 3Markov 模型 4Markov 模型 5 前言 硕士 生涯 结束 
开始 专心 做 一件 自己 觉得 有用 的 工具 先 
做 工程 后搞/nr 理论 自然语言 处理 是 一个 非常 难 
的 问题 同时 是 人工智能 皇冠 上 的 明珠 接下来 
会 记录 一 系列 自然语言 处理 的 笔记 来自 于 
哈工大 老师 关毅 Markov 模型 1 设 X = X1 
X2 . . . Xt 是 随机变量 序列 其中 每个 
随机变量 的 取值 在 有限集 = s1 S2 称为 状态 
空间 时间 不变性 假设 X = X _ 1 X 
_ 2 . . . X _ t 是 随机变量 
序列 其中 每个 随机变量 的 取值 在 有限集 = { 
s _ 1 _ 2 } 称为 状态 空间 时间 
不变性 假设 X = X1 X2 . . . Xt 
是 随机变量 序列 其中 每个 随机变量 的 取值 在 有限集 
= s1 S2 称为 状态 空间 时间 不变性 假设 N 
阶 Markov 模型 只需 修改 状态 空间 的 定义 = 
{ X } 定义 新的 变量 Xibelongtos ′ X _ 
i belong to s & # x27 Xi belongtos ′ 
使得 Xt = Si − 1 Si X _ t 
= _ { i 1 } _ i Xt = 
Si − 1 Si 并且 约定 P Xi ∣ Xi 
− 1 = P Si − 1 Si ∣ Si 
− 2 Si − 3 P X _ i | 
X _ { i 1 } = P _ { 
i 1 } _ i | _ { i 2 
} _ { i 3 } P Xi ∣ Xi 
− 1 = P Si − 1 Si ∣ Si 
− 2 Si − 3 Markov 模型 的 形式 化 
表示 一个 马尔可夫 模型 是 一个 三元组 S π A 
S \ pi A S π A 其中 是 状态 
的 集合 π \ pi π 是 初始状态 的 概率 
A 是 状态 间 的 转移 概率 发射 字符 依赖 
于 当前 状态 不同 状态 有 不同 输出 HMM 不同 
状态 可以 有 相同 输出 输出 在 状态 转移 中 
进行 Markov 模型 2HMM 模型 最大 的 灵活性 在 状态 
转移 中 以 特定 概率 输出 # # HMM 模型 
HMM 是 一个 五 元组 S k pi a b 
其中 s 是 状态 的 集合 k 是 输出 字符 
的 集合 pi 是 初始状态 的 概率 a 是 状态 
转移 的 概率 b 是 状态 转移 时 输出 字符 
的 概率 t = 1 以 概率 pip _ ipi 
在 状态 SiS _ iSi 开始 ie X1 = i 
forever domove from state Si to state Sj w i 
t h p r o b a b i l 
i t y A i j i e . . 
Xt + 1 = j A _ { ij } 
i e . . { X _ { t + 
1 } = j } Aij i e . . 
Xt + 1 = j Emit observation symbol Ot = 
kwith probability bt = t + 1end # # HMM 
的 基本 问题 给 定 一个 输出 的 字符 序列 
如何 调整 模型 的 参数 使得 产生 这 一序列 的 
概率 最大 IBM Watson 医生 隐 马 模型 的 基本 
问题 给定 一个 模型 M = S k pi a 
b 如何 高效 地 计算 某一 输出 字符 序列 的 
概率 P O | u 给定 一个 输出 字符 序列 
O 和 一个 模型 u 如何 确定 产生 这 一序列 
概率 最大 的 状态 序列 X1 x2 词 网格 分类 
音 字 转换 网格 cell states 问题 1 评价 evaluation 
给定 一个 模型 u = s k pi a b 
如何 高效 地 计算 某一 输出 字符 序列 的 概率 
P O | u O = o1 o2 or u 
= a b pi 计算 P O | u 给定 
词 网格 最优 路径 方案 一 直观 方法 X1 – 
o1P o | x u = bx1oz = ∑ P 
O ∣ X U ∗ P X ∣ u \ 
sum P O | X U * P X | 
u ∑ P O ∣ X U ∗ P X 
∣ u 动态规划 递推 求解 α i t = P 
O1 . . Oi ∣ Xt \ alpha _ i 
t = P O1 . . Oi | X _ 
t α i t = P O1 . . Oi 
∣ Xt 方案 2 向前 过程 = ∑ i = 
1 α i t ∗ bj α ijbj ∗ α 
t + 1 \ sum _ { i = 1 
} \ alpha _ i t * b _ j 
\ alpha _ { ij } b _ j * 
\ alpha _ { t + 1 } i = 
1 ∑ α i t ∗ bj α ij bj 
∗ α t + 1Markov 模型 3 向前 过程 RRGB 
动态 规划法 向后 过程 概述 KaTeX parse error Expected EOF 
got \ lmd at position 5 P O | \ 
̲ l ̲ m ̲ d ̲ = \ sum 
_ { 1 j N } p 算法 效率 与 
前 算法 相同 用途 参数 训练 问题 的 一个 重要 
组成部分 # # 解码 确定 产生 概率 最大 的 状态 
delta 为 在 t 时刻 到达 状态 j 输出 字符 
Ot 时 输出 前面 t 1 个字符 的 最 可能 
路径 的 概率 delta _ j t = max _ 
{ xi xt + 1 } P x1 xt + 
1 O1 Ot 1 Xt = 1 Ot delta + 
{ t + 1 } j = max _ deltat 
j aijbij ot + 1 viterbi algorithm 初始化 delta i 
= piibi Oi phi i = 0 递归 最优 路径 
qt = phi _ t = 1 Qt + 1 
把 连乘 变成 加 参数 统计 argmax _ uP O 
| u Markov 模型 4 设计 更新 计算 更新 值 
basic 思想 设定 模型 的 初始值 U old 基于 U 
_ old 计算 输出 U _ new 和O的/nr 概率 如果 
P o | u _ new P O | u 
_ old 某个 阈值 停止 否则 U _ old U 
_ new 返回 step2 . Baum Welch 算法 向前 向后 
算法 基于 HMM 的 词性 标注 词性 标注 作用 句法分析 
的 前期 步骤 难点 兼类 词 词性 标准 应用 Tbest 
= argmaxPr T | s = argmaxP | t P 
T 如何 计算 P | t 和P/nr T 简化 词 
wi 的 出现 仅仅 依赖 于它的/nr 词性 标记 标记 ti 
的 出现 仅仅 条件 依赖于 它 前面 的 标记 t 
_ i 1 公式 转化 计算 P | T 和P/nr 
T Pr | t Pr t = \ timr P 
Wi | ti P Ti | ti 1 使用 最大 
相似 度 估计 P Ti | ti 1 = c 
ti tj / c ti 音 字 转换 发射 字符 
状态 是 什么 发射 字 是 什么 不是 什么 转化 
为 生产力 的 学习 Markov 模型 5 HMM 评价 解码 
编码 问题 ch6 尾声 音 字 转换 T = argmax 
v | s 语言 单位 间 的 远距离 约束 递归 
模型 规则 与 统计 相结合 采用 规则 的 方法 短语 
结合 规则 A + NP NPA + 的 + NP 
NPM + 枝 + NP NP 短语 匹配 算法 从词/nr 
网格 到 元素 网格 颗粒度 疏 工作量 太大 规则 匹配 
强度 不够 做了 几个 宣传词 要 有 自己 的 优势 
项 还 做了 系统 挂接 问题 最近 在 学习 和 
实践 自然语言 处理 相关 的 知识 在 这个 文档 从头到尾 
做个 总结 防止 自己 忘记 也 提供 给 新人 来 
参考 本 教程 英文 处理 使用 的 是 NLTK 这个 
Python 库 中文 处理 使用 的 是 jieba 这个 Python 
库 主要 是 看 July7 月 学习 NLP 视频 学习 
而来 如有 侵权 立即 删除 Natural Language Processing NLP 自然语言 
处理 主要 是 处理 以及 理解 自然 语言 的 计算 
过程 整个 自然语言 处理 的 大致 流程 入 下图 所示 
自然语言 处理 流程 一 自然语言 处理 流程 第一步 Tokenize 分词 
分词 是 将 一个 句子 分成 很多 个 单词 用 
一个 word list 存 起来 如 英文 How are you 
today 会 分成 How are you today 中文 今天 心情 
很好 会 分成 今天 心情 很 好 第二步 Stemming / 
Lemma 提取 词干 是 将 英文 的 过去 式 名词 
形式 复数形式 全部 转 换为 最 原始 单词 如 apples 
= apple went = go watched = watch watching = 
watch 第三步 stopwords 去除 停止词 去掉 单词 列表 中的 停止词 
the a 等 单词 如 英文 The school is beautiful 
. = school beautiful 去掉 了 the is 等 单词 
有时会 用到 POS Tag 标注 词性 即 标注 出 单词 
是 动词 / 名词 / 形容词 / 副词 等 第四步 
Get feature 提取 特征 这个 步骤 的 意思 是 用 
一个 什么样 的 向量 来 表示 这 单词 或者 句子 
如 使用 TF IDF 来 表示 一个 单词 TF Term 
Frequecy 衡量 一个 单词 在 文档 中 出现 的 次数 
TF term = term 出现 在 文档 中 的 次数 
/ 文档 中 单词 的 总数 IDF Inverse Document Frequecy 
衡量 一个 单词 的 重要性 IDF term = loge 文档 
总数 / 含有 term 的 文档 总数 如果 一个 单词 
在 所有 文档 中 都 出现 了 则 IDF term 
= 0 表明 这个 单词 不重要 TF IDF = TF 
* IDF 对 每个 单词 进行 统计 和 计算 就 
可以 得到 每个 单词 的 TF IDF 的 值 用 
这个 值 来 代替 这个 单词 整个 句子 就 变成 
了 一个 浮点数 的 List 当然 这个 是 最简单 的 
模型 这个 模型 有 很多 缺陷 现在 流行 的 word2vect 
和 fasttext 都 是由 google 实习生 写 出来 的 这 
两个 模型 生成 的 分布式 向量 可以 有效 的 表达 
出 两个 单词 之间 的 关系 这个 在 后续 再做 
介绍 第五步 Machine Learning 机器学习 机器学习 是 表示 得到 特征向量 
之后 能 根据 训练 集合 来 预测 需要 测试 集合 
这个 部分 也 在 后续 再做 专门 的 介绍 二 
自然语言 处理 入门 软件 安装 以及 常见问题 整个 实验 环境 
是 在 VMWare + Ubuntu 16.04 LTS 下 完成 的 
最好 是 能 翻墙 我 用 的 翻墙 软件 是 
LoCo 加速器 NLTK 安装  安装 pip 安装 pip 一个 
python 第三方 软件 的 库 apt get 是 获得 软件 
或者 库 sudo apt get install python pip python dev 
build essential  问题 1 可能 会 碰到 的 问题 
install 的 时候 碰到 Could not get lock / var 
/ lib / dpkg / lock 无法 Install 解决方案 找到 
哪个 线程 锁住 了 这个 资源 然后 Kill 掉 指令 
sudo lsof / var / lib / dpkg / locksudo 
kill 9 get from lsof output  更新 pip 这个 
库 sudo pip install – upgrade pip  安装 nltk 
库 用于 自然语言 处理 sudo pip install U nltk  
安装 numpy 库 sudo pip install U numpy  下载 
nltk 所有 相关 东西 语料库 模型 等 pythonimport nltknltk . 
download all  Python Debug 使用 import pdb 在 需要 
断点 的 地方 pdb . set _ trace h Helpq 
Quitp PrintPp Prettyprintw Where + stack trace 执行 到 了 
什么 地方 l 断点 前后 的 代码 n 执行 下 
一句 b 35 在 第 35行 断点 变量 名称 打印 
出 变量 的 值 是 多少 c continue until breaks 
step inside  使用 VIM 做 IDE 可能 需要 配置 
和 Python 相关 的 信息 在 . vrmrc 中 进行 
配置 第一 次 需要 新建 这个 文件 如果 需要 添加 
插件 则 先在 . vimrc 中 进行 配置 然后 使用 
vim 输入 PluginInstall 来 在 Vim 上 安装 指定 插件 
 问题 2 运行 pandas . test 报错 运行 numpy 
. test 或者 pandas . test 出现 如下 错误 ImportError 
Need nose = 1 . 0.0 for tests see http 
/ / s o m e t h i n 
g a b o u t o r a n 
g e . com / mrl / projects / nose 
需要 先 安装 nose sudo pip install nose  问题 
3 pandas 从0./nr 19.0 开始 不再 支持 pandas . io 
. wb 改用 pandas _ datareader 这个 python 库 需要 
先行 下载 sudo pip install pandas _ d a t 
a r e a d e r t a r 
t i n g in 0 . 19.0 pandas will 
no longer support pandas . io . data or pandas 
. io . wb so you must replace your imports 
from pandas . io with those from pandas _ datareader 
from pandas . io import data wb # becomesfrom pandas 
_ datareader import data wbMany functions from the data module 
have been included in the top level API . import 
pandas _ datareader as pdrpdr . get _ data _ 
yahoo AAPL  问题 4 使用 matplotlibc 出现 错误 ImportError 
No module named _ tkinter please install the python tk 
package 需要 安装 python tk 安装包 sudo apt get install 
python tk  问题 5 vim 添加 Python 支持 1 
Ctrl + Alt + T 打开 命令 终端 输入 vim 
– version | grep python 查看 vim 是否 支持 python 
我 这个 vim 只 支持 python3 不支持 python 2 安装 
py2 包 在 命令 终 端下 输入 sudo apt get 
install vim nox py2 3 可以 再次 用 vim – 
version | grep python 查看 此时 vim 是否 支持 python 
若 支持 到此为止 若 不支持 请 执行 第四步 4 在 
命令 终端 输入 sudo update alternatives – config vim 我 
这里 是 第三 项 属于 python 第二项 属于 python3 想 
打开 哪 一项 支持 就 输入 它 的 编号 就 
可以 了 0 1 2 3 其他 资料 的 使用 
 matplotlin 画图 软件 画 二维图 可以 使用 这个 工具 
功能 和 matlab 画图 类似 sudo pip install matplotlib 结合 
pandas 使用 API 使用 文档 http / / matplotlib . 
org / 1 . 5.3 / users / beginner . 
html  pandas datareader 从 yahoo Finance google API 上 
下载 相关 信息 具体 使用 API 如下 https / / 
pandas datareader . readthedocs . io / en / latest 
/  python 一些 常见 库 的 入门 指导 材料 
可以 加快 学习 的 速度 https / / p y 
t h o n p r o g r a 
m m i n g . net /  美国 
金融 方面 的 数据 房价 / 股票 等 信息 已经 
格式化 好了 容易 处理 的 数据 https / / www 
. quandl . com /  pandas 入门 资料 http 
/ / pandas . pydata . org / pandas docs 
/ stable / 10min . html 三 自然语言 处理 实践 
实践 的 题目 是 Kaggle 上 的 一道 竞赛 题目 
链接 https / / www . kaggle . com / 
c / home depot product search relevance Home Depot 是 
美国 一家 网上 卖 五金 的 公司 在 用户 输入 
修 洗脸盆 的 时候 希望/v 能/v 提供/v 给/p 用户/n 所有/b 
洗脸盆/l 需要/v 的/uj 五金/n 和/c 工具/n 给 出了 五组 数据 
产品 属性 product _ uid name value 100001 Bullet01 Versatile 
connector for various 90 ° connections and home repair projects 
产品描述 product _ uid product _ description 100001 Not only 
do angles make joints stronger they also provide more consistent 
straight corners . Simpson Strong Tie offers a wide variety 
of angles in various sizes and thicknesses to handle light 
duty jobs or projects where a structural connection is needed 
. Some can be bent skewed to match the project 
. For outdoor projects or those where moisture is present 
use our ZMAX zinc coated connectors which provide extra resistance 
against corrosion look for a Z at the end of 
the model number . Versatile connector for various 90 connections 
and home repair projectsStronger than angled nailing or screw fastening 
aloneHelp ensure joints are consistently straight and strongDimensions 3 in 
. x 3 in . x 1 1/2 in . 
Made from 12 Gauge steelGalvanized for extra corrosion r e 
s i s t a n c e I n 
s t a l l with 10d common nails or 
# 9 x 1 1/2 in . Strong Drive SD 
screws 测试 集 id product _ uid product _ title 
search _ term 1 100001 Simpson Strong Tie 12 Gauge 
Angle 90 degree bracket 训练 集合 id product _ uid 
product _ title search _ term relevance 2 100001 Simpson 
Strong Tie 12 Gauge Angle angle bracket 3 提交 的 
样本 id relevance 1 1Relevance 是 表示 选出 工具 和 
输入 搜索 关键词 之间 的 相关性 相关性 = 3 表示 
非常 相关 相关性 = 1 表示 不太 相关 第一步 数据 
清洗  用 pandas 读取 csv 中 的 数据 因为 
数据 过大 没法 一次 读取 出来 进行 处理 每次 处理 
10000条 处理 完 1次 就 放到 另外 一个 csv 文件 
中 存储 起来  处理 的 过程 是 将 英文 
用 nltk 的 stemming 方法 对 每个 单词 进行 提取 
词干  只有 第一 次 写入 csv 文件 的 时候 
需要 写入 header 且 不 需要 index Header 表示 表头 
处理 完成 之后 产品描述 变成 如下 的 样子 训练 数据 
和 测试数据 类似 处理 product _ uid product _ d 
e s c r i p t i o n 
1 0 0 0 0 1 not onli do angl 
make joint stronger they also provid more consi stent straight 
corners . simpson strong ti offer a wide varieti of 
angl i n various size and thick to handl light 
duti job or project where a struc tur connect is 
needed . some can be bent skewed to match the 
project . fo r outdoor project or those where moistur 
is present use our zmax zinc co at connectors which 
provid extra resist against corros look for a z at 
the end of the model number . versatil connector for 
various 90 connec t and home repair projectsstrong than angl 
nail or screw fasten alonehelp ensur joint are consist straight 
and strongdimensions 3 in . x 3 in . x 
1 1/2 in . mad from 12 gaug steelgalvan for 
extra corros resistanceinstal wi th 10d common nail or # 
9 x 1 1/2 in . strong driv sd screw 
注意事项  使用 chunk _ size 进行 分块 读入  
使用 iterrows 一行 一行 读入 数据  使用 final _ 
df column = A 新 增加 一列 A 可以 是 
一个 list  to _ csv 的 header 设置 来 
表示 是否 需要 表格 头  注意 编码 是 ISO 
8859 1 第二步 提取 特征 假设 使用 搜索 关键词 在产品 
名字 和 产品 描述 中 出现 的 最大 次数 来 
表示 这个 搜索 关键词 的 两个 主要 特征 搜索 关键词 
的 长度 表示 另外 一个 特征 注意事项  数据 连接 
之后 可能 会 出现 NaN 的 字符 Python 会 默认 
为 float 类型 的 无穷大 需要 通过 pd . isnull 
a 来 判断 下 是否 为 空 第三步 使用 机器学习 
来 预测 加 博士 是 用 的 随机 森林 来 
进行 relevance 的 预测 后续 章节 会 继续 深入 讨论 
未完 待续 自然语言 处理 与 DuReader 概述 自然语言 处理 是 
人工智能 桂冠 上 的 明珠 这句话 反应 了 NLP 发展 
之 艰巨 事实上 语言 理解 被 我们 认为 是 AI 
的 终极 任务 要 解决 这 一 难题 前提 是 
要能 解决 全部/nr 人类 水平 人工智能 的 问题 https / 
/ www . yangfenzi . com / sousuo / 61444 
. html 百度 在 自然 语言 处理 上 研究 和 
推进 正式 实际 的 应用 出发 的 内部 的 迫切 
需求 请见 下面 链接 文章 https / / www . 
leiphone . com / news / 201805 / 0UtBTaxsxpOqEU3h . 
html . 插播 一句 建议 某些 公司 研究 技术 要 
应用 的 实际 促进 产品 中去 搜索 结果 不能 让 
人 怀疑 技术 都是/nr 临时 抱佛脚 吧 目前 机器 阅读 
理解 国外 做 的 比较 多 例如 在 2018年 伊始 
阿里 巴巴 和 微软 亚洲 研究院 相继 刷新 了 斯坦福 
大学 发起 的 SQuAD Stanford Question Answering Dataset 文本 理解 
挑战赛 成绩 机器 阅读 理解 评分 超过 人类 这 意味着 
机器 阅读 理解 的 能力 已经 开始 在 指标 上 
超越 人类 又 是否 能够 引领 自然语言 处理 NLP 领域 
的 下一场 革命 https / / www . huxiu . 
com / article / 233577 . html f = member 
_ article SQuAD 是 斯坦福 大学 于 2016年 推出 的 
阅读 理解 数据集 也是 行 业内 公认 的 机器 阅读 
理解 标准 水平 测试 该 数据集 包含 来自 维基 百科 
的 536篇 文章 及 共计 十万 多个 问题 在 阅读 
数据集 内 的 文章 后 机器 需要 回答 若干 与 
文章 内容 相关 的 问题 通过 与 标准 答案 对比 
来 获取 得分 目前 整体 排名 第一 的 是 科大 
讯 飞 与 哈工大 联合 实验室 得分 为 82.482 或者 
89.281 标准 不同 微软 MARCO 也 应用 在 机器 阅读 
理解 领域 是由 10 万个 问答 和 20 万篇 不 
重复 的 文档 组成 的 数据 集 相比 SQuAD 其 
最大 不同 在于 数据 集中 的 问题 来自 微软 自家 
必应 搜索引擎 MARCO 的 挑战 难度 更大 它 需要 测试 
者 提交 的 模型 具备 理解 复杂 文档 回答 复杂 
问题 的 能力 目前 最高 得分 为 百度 NLP 团队 
获得 为 46.72 50.45 70.96 不同 标准 DuReader 数据集 目前 
最好 的 模型 与人 的 准确率 接近 但是 还是 很低 
约 是 60% 的 准确率 相比 英文 阅读 还是 有 
很大 差距 机器 阅读 理解 取得 的 成绩 确实 是 
一个 突破性 的 进展 其 可能 是 继 机器翻译 之后 
又 一个 取得 重要 进展 的 NLP 领域 但 机器 
阅读 理解 仍然 是 一种 限定 边界 的 任务 从 
某种 意义 上 来讲 目前 机器 的 阅读 理解 是 
把 问题 词 作为 查找 向量 从 文章 中 搜寻 
答案 的 词语 位置 几乎 没有 变换 及 推断 题 
机器 阅读 理解 是 一种 边界 限定 的 场景 式 
机器 理解 问题/n 的/uj 前提/n 条件/n 和/c 场景/n 边界/n 都/d 
比较/d 清楚/a 所以 机器 阅读 理解 超过 人类 是以 设定 
文章 集合 有限 问题 为 前提 条件 的 远远 达 
不到 真正 的 归纳 和 推理 因此 对于 人类 的 
胜利 更 应该 说是 指标 上 的 胜利 以 机器 
阅读 理解 任务 来说 机器 应该 很快 会 从 指标 
上 超过 人类 的 现有 水平 但 真正 的 阅读 
理解 过程 需要 深层 的 推理 和 归纳 这 恰恰 
是 目前 机器 所 欠缺 的 还 需要 通过 底层 
算法 的 突破 才 有可能 实现 机器 在 NLP 领域 
的 真正 突破 王士进/nr 谈到 有人 认为 下 一步 推动 
NLP 发展 可能 在 知识图谱 层面 通过 知识图谱 构建 机器 
对 任务 的 认知 能力 再 加以 语义 交互 等 
处理 工具 通过 应用 才能 更好 推动 一个 行业 的 
发展 其实 知识图谱 是 源 不是 机器 阅读 理解 的 
钥匙 现在 的 核心 是 算法 以及 数据 的 自然 
性 需要 改进 目前 机器 阅读 理解 的 技术 现状 
1956 年 乔姆斯基 借鉴 香农 的 工作 把 有限 状态机 
用作 刻画 语法 的 工具 建立 了 自然 语言 的 
有限 状态 模型 具体 来说 就是 用 代数 和 集合 
将 语言 转化 为 符号 序列 建立 了 一大堆 有关 
语法 的 数学 模型 https / / www . sohu 
. com / a / 204243606 _ 500659 90 年代 
以来 基于 统计 的 自然 语言 处理 就 开始 大放异彩 
了 大家 的 重心 开始 转向 大 规模 真实 文本 
了 传统 的 仅仅 基于 规则 的 自然 语言 处理 
显然 力不从心 了 如 句法 剖析 词类 标注 参照 消解 
话语 处理 的 算法 几乎 把 概率 与 数据 作为 
标准 方法 成为 了 自然 语言 处理 的 主流 其实 
语言 词汇 用法 上下 下文 环境 都会 导致 自然语言 处理 
的 难度 当前 中文 自然语言 处理 发展 还 不甚 成熟 
的 时期 私 以为 基于 统计 的 方法 在 很多 
方面 并不 完美 理性主义 的 作用 空间 还 很大 总 
成庆 深度 学习 掀开 了 机器 学习 的 新篇章 目前 
深度 学习 应用于 图像 和 语音 已经 产生 了 突破性 
的 研究 进展 https / / www . cnblogs . 
com / maybe2030 / p / 5427148 . html 引用 
三年前 一位 网友 的话 来讲 Steve Renals 算 了 一下 
icassp 录取 文章 题目 中 包含 deep learning 的 数量 
发现 有 44篇 而 naacl 则有 0篇 有 一种 说法 
是 语言 词 句子 篇章 等 属于 人类 认知 过程 
中 产生 的 高层 认知 抽象 实体 而 语音 和 
图像 属于 较为 底层 的 原始 输入 信号 所以 后两者 
更 适合 做 deep learning 来 学习 特征 研究 难点 
https / / baike . baidu . com / item 
/ NLP / 25220 单词 的 边界 界定 在 英语 
等 语言 中 使用 空格 等 自然 分词 这在 有别于 
汉语 等 需要 使用 者 自己 根据 语义 来自 我 
划分 词语 边界 在 汉语 口语 中 词 与 词 
之间 通常 是 连贯 的 而 界定 字词 边界 通常 
使用 的 办法 是 取用 能让 给定 的 上下文 最为 
通顺 且 在 文法上 无误 的 一种 最佳 组合 这 
其中 往往 也有 断句 导致 的 沟通 问题 在 书写 
上 汉语 也 没有 词 与 词 之间 的 边界 
在 古汉语 研究 中 断句 也是 需要 仔细 的 分析 
词义 的 消 歧 许 多字词 不单 只有 一个 意思 
因而 我们 必须 选 出使 句 意 最为 通顺 的 
解释 句法 的 模糊性 自然 语言 的 文法 通常 是 
模棱两可 的 针对 一个 句子 通常 可能会 剖析 Parse 出 
多棵 剖析 树 Parse Tree 而 我们 必须 要 仰赖 
语意 及 前后文 的 资讯 才能 在 其中 选择 一棵 
最为 适合 的 剖析 树 语言 行为 与 机动 响应 
句子 常常 并不 只是 字面 上 的 意思 例如 你 
能把 盐 递 过来 吗 一个 好 的 回答 应当 
是 把 盐 递过去 在 大多数 上下文 环境 中 能 
将 是 糟糕 的 回答 虽说 回答 不 或者 太远 
了 我 拿不到 也 是 可以 接受 的 再者 如果 
一门 课程 去年 没 开设 对于 提问 这门 课程 去年 
有 多少 学生 没 通过 回答 去年 没开 这门 课 
要比 回答 没人 没 通过 好 阅读 库 的 完备性 
自然语言 内容 多样 语法 语义 使用 场景 都 不能 很好 
的 界定 数据库 的 特点 现在 Squid 微软 Marco 百度 
的 dureader 缺乏 统一 有效 的 标准 都是/nr 一种 收集 
或者 固定 来源 整理 https / / blog . csdn 
. net / weixin _ 38440272 / article / details 
/ 80239298 机器 阅读 理解 其实 和人/nr 阅读 理解 面临 
的 问题 是 类似 的 不过 为了 降低 任务 难度 
很多 目前 研究 的 机器 阅读 理解 都将 世界知识 排除 
在外 采用 人工 构造 的 比较 简单 的 数据集 以及 
回答 一些 相对 简单 的 问题 https / / zhuanlan 
. zhihu . com / p / 22671467 机器 对 
语言 的 理解 过程 可以 分为 几个 步骤 其中 很多 
的 不确定性 是 逐渐 明晰 的 语音 识别 的 不确定性 
更多 因为 还要 解决 从 声音 到 词 的 转换 
第一步 是 要把 词 分开 放到 依存 树上 看 哪 
一个 词 是 动词 对 名词 有 哪些 影响 等等 
随后 要 理解 每 一个 名字 的 含义 再次 再加入 
许多 先验 知识 即 对 这个 世界 的 理解 因为 
很多 句子 只有 使用 了 这些 信息 才能 真正 理解 
如果 足够 幸运 的话 到这 就能 得到 清晰 的 理解 
了 https / / www . yangfenzi . com / 
sousuo / 61444 . html 自然语言 处理 领域 进化 出了 
深度 神经 网络 的 一种 新模式 该 模式 分为 embed 
encode attend predict 四 部分 https / / blog . 
csdn . net / jdbc / article / details / 
53292414     然而 大多数 NLP 问题 面对 的 不是 
单个 词语 而是 需要 分析 更长 的 文本 内容 现在 
有 一个 简单 而 灵活 的 解决方案 它 在 许多 
任务 上 都 表现 出 了 卓越 的 性能 即 
RNN 模型 高级 词 向量 三部曲 https / / blog 
. csdn . net / sinat _ 26917383 / article 
/ details / 548509331 NLP ︱ 高级 词 向量 表达 
一 GloVe 理论 相关 测评 结果 R & python 实现 
相关 应用 2 NLP ︱ 高级 词 向量 表达 二 
FastText 简述 学习 笔记 3 NLP ︱ 高级 词 向量 
表达 三 WordRank 简述 4 其他 NLP 词 表示 方法 
paper 从 符号 到 分布式 表示 NLP 中词 各种 表示 
方法 综述 如果 用 分类 问题 用 CNN 对于 顺序 
建模 需要 联系 上 下文 用 RNN 深度 学习 是 
一个 年轻 的 领域 理论 建立 并 不完备 观点 也 
会 快速 变化 目前 机器 阅读 理解 研究 领域 出现 
了 非常 多 的 具体 模型 如果 对 这些 模型 
进行 技术 思路 梳理 的话 会 发现 本质 上 大多数 
模型 都是 论文 Teaching Machines to Read and Comprehend 提出 
的 两个 基础 模型 Attentive Reader 和 Impatient Reader 的 
变体 当然 很多 后续 模型 在 结构 上 看上去 有了/nr 
很大 的 变化 但是 如果 仔细 推敲 的话 会 发现 
根源 和 基础 思路 并未 发生 颠覆性 的 改变 https 
/ / zhuanlan . zhihu . com / p / 
22671467 深度 学习 解决 机器 阅读 理解 任务 的 研究 
进展 DuReader 一个 新的 大型 开放 中文 机器 阅读 理解 
数据集 其 在 中文 应用 中 还是 很 有 开创 
意义 实际上 是 百度知道 和 以及 百度 搜索 数据 的 
全面性 和 权威性 令人 存疑 Dureader 的 基本 内容 介绍 
可以 参见 https / / blog . csdn . net 
/ LiJiancheng0614 / article / details / 80866088 由于 规模 
大 且 问题 类型 复杂 基于 DuReader 数据集 的 分析 
工作 相比 以往 数据集 都要 难得 多 百度 通过 计算 
人工 答案 和 文档 的 最小 编辑 距离 来 判断 
回答 问题 的 困难 度 编辑 距离 越大 对 文档 
的 编辑 修改 就 更多 回答 问题 的 复杂度 也就 
越高 对于 答案 直接 来源于 原文 的 数据集 如 SQuAD 
它们 的 编辑 距离 应该 是 0 下图 展示 了 
MS MARCO 和 DuReader 两个 数据集 答案 与 文档 编辑 
距离 分布 情况 https / / zhuanlan . zhihu . 
com / p / 36415104 图 MS MARCO/w 和/c DuReader/w 
两个/m 数据集/i 答案/n 与/p 文档/n 编辑/n 距离/n 分布/v 情况/n 从图/nr 
可以/c 看出/v 在 同为 人工 标注 的 数据集 MS MARCO 
中 77.1% 的 样本 的 编辑 距离 低于 3 而在 
DuReader 中 51.3% 的 样本 的 编辑 距离 高于 10 
这说明 DuReader 更为 复杂 百度 基于 DuReader 构建 了 两个 
基线 模型 Match LSTM 和 BiDAF Match LSTM 是 广泛 
应用 的 MRC 模型 Match LSTM 为了 在 文章 中 
找到 答案 依次 遍历 文章 动态地 将 注意力 权重 与 
文章 的 每个 标记 进行 匹配 最后 使用 一个 应答 
指针 层 来 查找 文章 中 的 答案 跨度 BiDAF 
既使 用了 语境 对 问题 的 注意 又 使用 了 
问题 对 上下文 的 注意 从而 突出 了 问题 和 
上下 文中 的 重要 部分 然后 利用 注意 流层 融合 
所有 有用 的 信息 从而 得到 每个 位置 的 向量 
表示 下 面对 Match LSTM 进行 分享 https / / 
www . imooc . com / article / 25027 模型 
都 分为 三 部分 Embed 层 使 用词 向量 表示 
原文 和 问题 Encode 层 使用 单向 LSTM 编码 原文 
和 问题 embedding Interaction 层 对 原文 中 每个 词 
计算 其 关于 问题 的 注意力 分布 并 使用 该 
注意力 分布 汇总 问题 表示 将 原文 该词 表示 和 
对应 问题 表示 输入 另一个 LSTM 编码 得到 该词 的 
query aware 表示 在 反方向 重复 步骤 2 获得 双向 
query aware 表示 Answer 层 基于 双向 query aware 表示 
使用 Sequence Model 或 Boundary Model 预测 答案 范围 BiDAF 
5 相比 于 之前 工作 BiDAF Bi Directional Attention Flow 
最大 的 改进 在于 Interaction 层 中 引入 了 双向 
注意力 机制 即 首先 计算 一个 原文 和 问题 的 
Alignment matrix 然后 基于 该 矩阵 计算 Query2Context 和 Context2Query 
两种 注意力 并 基于 注意力 计算 query aware 的 原文 
表示 接着 使用 双向 LSTM 进行 语义 信息 的 聚合 
另外 其 Embed 层 中 混合 了 词 级 embedding 
和 字符 级 embedding 词 级 embedding 使用 预 训练 
的 词 向量 进行 初始化 而 字符 级 embedding 使用 
CNN 进一步 编码 两种 embedding 共同 经过 2 层 Highway 
Network 作为 Encode 层 输入 最后 BiDAF 同样 使用 Boundary 
Model 来 预测 答案 开始 和 结束 位置 目前 有 
很多 的 框架 实现 版本 例如 Tensorflow Pytorch Paddlepadlle QAnet 
等 尽管 目前 结果 提升 比较 明显 但是 离 英文 
机器 阅读 理解 取得 的 成绩 尚有 很大 距离 目前 
笔者 认为 现在 实现 彻底 的 机器 阅读 理解 不妨/v 
从/p 人类/n 阅读/v 理解/v 的/uj 本质/n 以及/c 能从/i 算法/n 上/f 
实现/v 渐进/v 的/uj 路标/n 和/c 更加/d 丰富/a 完备/v 的/uj 阅读/v 
理解/v 数据集/i 转载 请 注明 出处 https / / blog 
. csdn . net / HHTNAN 简介 CoreNLP 项目 是 
Stanford 开发 的 一套 开源 的 NLP 系统 包括 tokenize 
pos parse 等功能 与 SpaCy 类似 SpaCy 号称 是 目前 
最快 的 NLP 系统 并且 提供 现成 的 python 接口 
但 不足之处 就是 目前 还 不支持 中文 处理 CoreNLP 则 
包含 了 中文 模型 可以 直接 用于 处理 中文 但 
CoreNLP 使用 Java 开发 python 调用 稍微 麻烦 一点 Stanford 
CoreNLP 是 一个 比较 厉害 的 自然 语言 处理 工具 
很多 模型 都是/nr 基于 深度 学习 方法 训练 得到 的 
先 附上 其 官网 链接 https / / stanfordnlp . 
github . io / CoreNLP / index . htmlhttps / 
/ nlp . stanford . edu / nlp / javadoc 
/ javanlp / https / / github . com / 
stanfordnlp / CoreNLP 安装 I n s t a l 
l a t i o n w i n d 
o w s 10 环境 安装 依赖 1 . 首先 
需要 配置 JDK 安装 JDK 1.8 及 以上 版本 2 
. 之后 到 https / / stanfordnlp . github . 
io / CoreNLP / history . html 下载 对应 的 
jar 包 将 压缩包 解压 得到 目录 再将 语言 的 
jar 包放 到 这个 目录 下 即可 3 . 下载 
Stanford CoreNLP 文件 http / / stanfordnlp . github . 
io / CoreNLP / download . html4 . 下载 中文 
模型 jar 包 注意 一定 要 下载 这个 文件 否则 
它 默认 是 按 英文 来 处理 的 5 . 
接下来 py 安装 stanfordcorenlp6 . 解压/v 配置/v 下载/v 完成/v 后/f 
两个/m 文件/n 加起来/i 1G/i +/i 下载/v 完成/v 后/f 两个/m 文件/n 
加起来/i 1G/i +/i 把/p 解/v 压后/i 的/uj Stanford/w CoreNLP 文件夹 
下载 的 Stanford chinese corenlp 2018 models . jar 放在 
同 一目 录下 注意 一定 要 在 同一 目 录下 
否则 执行 会 报错 7 . 在 Python 中 引用 
模型 执行 下面 语句 from stanfordcorenlp import t a n 
f o r d C o r e N L 
P n l p = StanfordCoreNLP r D \ D 
\ stanford _ nlp \ stanford corenlp full 2018 10 
05 lang = zh 应用 # encoding = utf 8 
from stanfordcorenlp import StanfordCoreNLP import os if os . path 
. exists D \ \ stanford _ nlp \ \ 
stanford corenlp full 2018 10 05 print corenlp exists else 
print corenlp not exists nlp = StanfordCoreNLP D \ \ 
stanford _ nlp \ \ stanford corenlp full 2018 10 
05 lang = zh sentence = 王明 是 清华大学 的 
一个 研究生 print nlp . ner sentence 输出 corenlp exists 
王明 PERSON 是 O 清华 ORGANIZATION 大学 ORGANIZATION 的 O 
一 NUMBER 个 O 研究生 O 三 查看 词性 标注 
在 浏览器 中 访问 http / / localhost 9000 / 
转载 请 注明 出处 https / / blog . csdn 
. net / HHTNAN 转 自 https / / blog 
. csdn . net / diye2008 / article / details 
/ 53105652 所周知 卷积 神经网络 CNN 在 计算机 视觉 领域 
取得 了 极大 的 进展 但是 除此之外 CNN 也 逐渐 
在 自然 语言 处理 NLP 领域 攻城略地 本文 主要 以 
文本 分类 为例 介绍 卷积 神经 网络 在 NLP 领域 
的 一个 基本 使用 方法 由于 本人 是 初学者 而且 
为了 避免 东施效颦 所以 下面 的 理论 介绍 更多 采用 
非 数学化 且 较为 通俗 的 方式 解释 0 . 
文本 分类 所谓 文本 分类 就是 使用 计算机 将 一篇 
文本 分为 a 类 或者 b 类 属于 分类 问题 
的 一种 同时 也 是 NLP 中 较为 常见 的 
任务 一 . 词 向量 提到 深度 学习 在 NLP 
中的 应用 就 不得不 提到 词 向量 词 向量 Distributed 
Representation 在 国内 也 经常 被 翻译 为 词 嵌入 
等等 关于 词 向量 的 介绍 的 文章 已经 有 
很多 比如 这位 大神 的 博客 http / / blog 
. csdn . net / zhoubl668 / article / details 
/ 23271225 本文 则用 较为 通俗 的 语言 帮助 大家 
了解 词 向量 所 谓词 向量 就是 通过 神经 网络 
来 训练 语言 模型 并在 训练 过程 钟 生成 一组 
向量 这组 向量 将 每个 词 表示 为 一个 n 
维 向量 举个 例子 假如 我们 要 将 北京 表示 
为 一个 2 维 向量 可能 的 一种 结果 如 
北京 = 1.1 2.2 在 这里 北京 这个 词 就被 
表示 为 一个 2 维 的 向量 但是 除了 将 
词 表示 为 向量 以外 词 向量 还要 保证 语义 
相近 的 词 在 词 向量 表示 方法 中 的 
空间 距离 应当 是 相近 的 比如 中国   北京 
  ≈ 英国 伦敦 上述 条件 可 在下列 词 向量 
分布 时 满足 北京 = 1.1 2.2 中国 = 1.2 
2.3 伦敦 = 1.5 2.4 英国 = 1.6 2.5 一般 
训练 词 向量 可以 使用 google 开源 word2vec 程序 二 
. 卷积 神经 网络 与 词 向量 的 结合 有关 
CNN 的 博客 非常 之多 如果 不 了解 CNN 的 
基本 概念 可以 参 见 这位 大神 的 博客 如下 
http / / blog . csdn . net / zhoubl668 
/ article / details / 23271225 这里 就 不在 赘述 
通常/d 卷积/n 神经/n 网络/n 都是/nr 用来/v 处理/v 类似/v 于/p 图像/n 
这样/r 的/uj 二维/m 不考虑 rgb 矩阵 比如 一张 图片 通常 
都 可以 表示 为 一个 2 维 数组 比如 255 
* 255 这就 表示 该 图片 是 一张 255 像素 
宽 255 像素 高的/nr 图片 那么 如何 将 CNN 应用到 
文本 中 呢 答案 就是 词 向量 我们 刚刚 介绍 
了 词 向量 的 概念 下面 介绍 下 如何 将 
文本 通过 词 向量 的 方式 转换 成 一个 类 
图像 类型 的 格式 一般来说 一篇 文本 可以 被 视为 
一个 词汇 序列 的 组合 比如 有篇 文本 内容 是 
书写 代码 改变 世界 可以 将其 转换 为 书写 代码 
改变 世界 这样 一个 文本 序列 显然 这个 序列 是 
一个 一维 的 向量 不能 直接 使用 cnn 进行 处理 
但是 如果 使用 词 向量 的 方式 将 其 展开 
假设 在某 词 向量 钟 书写 = 1.1 2.1 代码 
= 1.5 2.9 改变 = 2.7 3.1 世界 = 2.9 
3.5 那么 书写 代码 改变 世界 这个 序列 就 可以 
改 写成 1.1 2.1 1.5 2.9 2.7 3.1 2.9 3.5 
显然 原先 的 文本 序列 是 4 * 1 的 
向量 改写 之后 的 文本 可以 表示 为 一个 4 
* 2 的 矩阵 推而广之 任何 以 文本 序列 都 
可以 表示 为 m * d 的 数组 m 维 
文本 序列 的 词数 d 维 词 向量 的 维数 
三 . 用于 文本 分类 的 神经 网络 结构设计 本文 
前面 介绍 了 词 向量 卷积 神经 网络 等 概念 
并/c 提出/v 可以/c 将/d 文本/n 转换/v 成/n 一个/m 由/p 词/n 
序列/n 和词/nr 向量/n 嵌套/nz 而成/i 的/uj 二维/m 矩阵/n 并 通过 
CNN 对其 进行 处理 下面 以 文本 分类 任务 为例 
举例 说明 如何 设计 该 神经 网络 的 样式 3.1 
文本 预处理 部分 的 流程 这 部分 主要 是 分 
3步 共 4种 状态 1 . 将 原始 文本 分词 
并 转换成 以 词 的 序列 2 . 将 词 
序列 转换成 以 词 编号 每个 词表 中的 词 都有 
唯一 编号 为 元素 的 序列 3 . 将 词 
的 编号 序列 中 的 每个 元素 某个 词 展开 
为 词 向量 的 形式 下面 通过 一张 图 本 
人手 画 简图 囧 来 表示 这个 过程 如下 图 
所示 上述 图片 以 书写 代码 改变 世界 这一 文本 
为例 介绍 了 将其 转换成 词 向量 为 元素 的 
序列 表示 最后 得到 了 一个 2 维 矩阵 该 
矩阵 可 用于 后续 神经 网络 的 训练 等 操作 
3.2 神经网络 模块 的 设计 本文 关于 神经网络 设计 的 
思想 来自于 以下 博文 http / / www . wildml 
. com / 2015/12 / implementing a cnn for text 
classification in tensorflow /   由于 该 文章 是 纯 
英文 的 某些 读者 可能 还 不习惯 阅读 这类 文献 
我 下面 结合 一张 神经网络 设计图 来 说明 本 文中 
所 使用 的 神经 网络 具体 设计 图 又是 手 
画图 囧 如下 简要 介绍 下 上面 的 图 第一层 
数据 输入 层 将 文本 序列 展 开成 词 向量 
的 序列 之后 连接 卷积 层 激活 层 池化层/nr 这里 
的 卷积 层 因为 卷积 窗口 大小 不同 平行 放置 
了 三个 卷积 层 垂直 方向 则 放置 了 三重 
卷积 层 激活 层 池化层/nr 的 组合 之后/f 连接/v 全脸/nr 
阶层/n 和/c 激活/a 层/q 激活 层 采用 softmax 并 输出 
该文 本属于 某类 的 概率 3.3 编程 实现 所 需要 
的 框架 和 数据集 等 3 . 3.1 框架 本文 
采用 keras 框架 来 编写 神经网络 关于 keras 的 介绍 
请 参见 莫言 大神 翻译 的 keras 中文 文档 http 
/ / keras cn . readthedocs . io / en 
/ latest / 3 . 3.2 数据集 文本 训练 集 
来自 20 _ newsgroup 该 数据集 包括 20种 新闻 文本 
下载 地址 如下 http / / www . qwone . 
com / ~ jason / 20Newsgroups / 3 . 3.3 
词 向量 虽然 keras 框架 已经 有 embedding 层 但是 
本文 采用 glove 词 向量 作为 预 训练 的 词 
向量 glove 的 介绍 和 下载 地址 如下 打开 会 
比较 慢 http / / nlp . stanford . edu 
/ projects / glove / 3.4 代码 和 相应 的 
注释 在 3.2 部分 已经 通过 一张 图 介绍 了 
神经 网络 的 设计 部分 但是 考虑到 不够 直观 这里 
还是 把 所 使用 的 代码 罗列 如下 采用 keras 
编程 关键 部分 都 已经 罗列 注释 代码 有 部分 
是 来 源自 keras 文档 中的 example 目 录下 的 
pretrained _ word _ embeddings . py 但是 该 程序 
我 实际 运行 时 出现 了 无法 训练 的 bug 
所以 做 了 诸多 改变 最 主要 的 是 我 
把 原文中 的 激活 层 从 relu 改成 了 tanh 
整体 的 设计 结构 也 有了 根本性 的 改变 对 
keras 原始 demo 有兴趣 的 可以 参见 http / / 
keras cn . readthedocs . io / en / latest 
/ blog / word _ embedding / 下面 就是 本 
文中 所 使用 的 文本 分类 代码 本 程序 将 
训练 得到 一个 20类 的 文本 分类器 数据 来源 是 
20 Newsgroup datasetGloVe 词 向量 的 下载 地址 如下 http 
/ / nlp . stanford . edu / data / 
glove . 6B . zip20 Newsgroup 数据集 来自于 http / 
/ www . cs . cmu . edu / afs 
/ cs . cmu . edu / project / theo 
20 / www / data / news20 . html from 
_ _ future _ _ import print _ functionimport osimport 
numpy as npnp . random . seed 1337 from keras 
. preprocessing . text import Tokenizerfrom keras . preprocessing . 
sequence import pad _ sequencesfrom keras . utils . np 
_ utils import to _ categoricalfrom keras . layers import 
Dense Input Flattenfrom keras . layers import Conv1D MaxPooling1D Embeddingfrom 
keras . models import Modelfrom keras . optimizers import * 
from keras . models import Sequentialfrom keras . layers import 
Mergeimport sysBASE _ DIR = . # 这里 是 指 
当前目录 GLOVE _ DIR = BASE _ DIR + / 
glove . 6B / # 根据 实际 目录名 更改 TEXT 
_ DATA _ DIR = BASE _ DIR + / 
20 _ newsgroup / # 根据 实际 目录名 更改 MAX 
_ SEQUENCE _ LENGTH = 1000 # 每个 文本 的 
最长 选取 长度 较短 的 文本 可以 设 短 些 
MAX _ NB _ WORDS = 20000 # 整体 词库 
字典 中 词 的 多少 可以 略 微调 大 或 
调 小 EMBEDDING _ DIM = 50 # 词 向量 
的 维度 可以 根据 实际 情况 使用 如果 不 了解 
暂时 不要 改 VALIDATION _ SPLIT = 0.4 # 这里 
用作 是 测试 集 的 比例 单词 本身 的 意思 
是 验证 集 # first build index mapping words in 
the embeddings set # to their embedding vector 这段话 是 
指 建立 一个 词 到 词 向量 之间 的 索引 
比如 peking 对应 的 词 向量 可能 是 0.1 0 
32 . . . 0.35 0.5 等等 print Indexing word 
vectors . embeddings _ index = { } f = 
open os . path . join GLOVE _ DIR glove 
. 6B . 50d . txt # 读入 50 维 
的 词 向量 文件 可以 改成 100 维 或者 其他 
for line in f values = line . split word 
= values 0 coefs = np . asarray values 1 
dtype = float32 embeddings _ index word = coefsf . 
close print Found % s word vectors . % len 
embeddings _ index # second prepare text samples and their 
labelsprint Processing text dataset # 下面 这段 代码 主要 作用 
是 读入 训练样本 并 读入 相应 的 标签 并给 每个 
出现 过 的 单词 赋 一个 编号 比如 单词 peking 
对应 编号 100texts = # 存储 训练样本 的 listlabels _ 
index = { } # 词 到 词 编号 的 
字典 比如 peking 对应 100labels = # 存储 训练样本 类别 
编号 的 文本 比如 文章 a 属于 第 1类 文本 
for name in sorted os . listdir TEXT _ DATA 
_ DIR path = os . path . join TEXT 
_ DATA _ DIR name if os . path . 
isdir path label _ id = len labels _ index 
labels _ index name = label _ idfor fname in 
sorted os . listdir path if fname . isdigit fpath 
= os . path . join path fname if sys 
. version _ info 3 f = open fpath else 
f = open fpath encoding = latin 1 texts . 
append f . read f . close labels . append 
label _ id print Found % s texts . % 
len texts # 输出 训练样本 的 数量 # finally vectorize 
the text samples into a 2D integer tensor 下面 这段 
代码 主要 是 将 文本 转换成 文本 序列 比如 文本 
我 爱 中华 转化 为 我 爱 中华 然后再 将其 
转化 为 101 231 最后 将 这些 编号 展 开成 
词 向量 这样 每个 文本 就是 一个 2 维 矩阵 
这块 可以 参加 本文 二 . 卷积 神经 网络 与 
词 向量 的 结合 这一 章节 的 讲述 tokenizer . 
fit _ on _ texts texts sequences = tokenizer . 
texts _ to _ sequences texts word _ index = 
tokenizer . word _ indexprint Found % s unique tokens 
. % len word _ index data = pad _ 
sequences sequences maxlen = MAX _ SEQUENCE _ LENGTH labels 
= to _ categorical np . asarray labels print Shape 
of data tensor data . shape print Shape of label 
tensor labels . shape # split the data into a 
training set and a validation set 下面 这段 代码 主要 
是 将 数据集 分为 训练 集 和 测试 集 英文 
原意 是 验证 集 但是 我 略有 改动 代码 indices 
= np . arange data . shape 0 np . 
random . shuffle indices data = data indices labels = 
labels indices nb _ validation _ samples = int VALIDATION 
_ SPLIT * data . shape 0 x _ train 
= data nb _ validation _ samples # 训练 集 
y _ train = labels nb _ validation _ samples 
# 训练 集 的 标签 x _ val = data 
nb _ validation _ samples # 测试 集 英文 原意 
是 验证 集 y _ val = labels nb _ 
validation _ samples # 测试 集 的 标签 print Preparing 
embedding matrix . # prepare embedding matrix 这 部分 主要 
是 创建 一个 词 向量 矩阵 使 每个 词 都有 
其 对应 的 词 向量 相 对应 nb _ words 
= min MAX _ NB _ WORDS len word _ 
index embedding _ matrix = np . zeros nb _ 
words + 1 EMBEDDING _ DIM for word i in 
word _ index . items if i MAX _ NB 
_ WORDS c o n t i n u e 
e m b e d d i n g _ 
vector = embeddings _ index . get word if embedding 
_ vector is not None # words not found in 
embedding index will be all zeros . embedding _ matrix 
i = embedding _ vector # word _ index to 
word _ embedding _ vector 20000 nb _ words # 
load pre trained word embeddings into an Embedding layer # 
神经网路 的 第一 层 词 向量 层 本文 使用 了 
预 训练 glove 词 向量 可以 把 trainable 那里 设为 
Falseembedding _ layer = Embedding nb _ words + 1 
EMBEDDING _ DIM input _ length = MAX _ SEQUENCE 
_ LENGTH weights = embedding _ matrix trainable = True 
print Training model . # train a 1D convnet with 
global maxpoolinnb _ wordsg # left model 第一块 神经网络 卷积 
窗口 是 5 * 50 50 是 词 向量 维度 
model _ left = Sequential # model . add Input 
shape = MAX _ SEQUENCE _ LENGTH dtype = int32 
model _ left . add embedding _ layer model _ 
left . add Conv1D 128 5 activation = tanh model 
_ left . add MaxPooling1D 5 model _ left . 
add Conv1D 128 5 activation = tanh model _ left 
. add MaxPooling1D 5 model _ left . add Conv1D 
128 5 activation = tanh model _ left . add 
MaxPooling1D 35 model _ left . add Flatten # right 
model 第二块 神经网络 卷积 窗口 是 4 * 50model _ 
right = Sequential model _ right . add embedding _ 
layer model _ right . add Conv1D 128 4 activation 
= tanh model _ right . add MaxPooling1D 4 model 
_ right . add Conv1D 128 4 activation = tanh 
model _ right . add MaxPooling1D 4 model _ right 
. add Conv1D 128 4 activation = tanh model _ 
right . add MaxPooling1D 28 model _ right . add 
Flatten # third model 第三块 神经网络 卷积 窗口 是 6 
* 50model _ 3 = Sequential model _ 3 . 
add embedding _ layer model _ 3 . add Conv1D 
128 6 activation = tanh model _ 3 . add 
MaxPooling1D 3 model _ 3 . add Conv1D 128 6 
activation = tanh model _ 3 . add MaxPooling1D 3 
model _ 3 . add Conv1D 128 6 activation = 
tanh model _ 3 . add MaxPooling1D 30 model _ 
3 . add Flatten merged = Merge model _ left 
model _ right model _ 3 mode = concat # 
将 三种 不同 卷积 窗口 的 卷积 层 组合 连接 
在 一起 当然 也 可以 只是 用 三个 model 中 
的 一个 一样 可以 得到 不错 的 效果 只是 本文 
采用 论 文中 的 结构 设计 model = Sequential model 
. add merged # add mergemodel . add Dense 128 
activation = tanh # 全 连接 层 model . add 
Dense len labels _ index activation = softmax # softmax 
输 出文 本属于 20种 类别 中 每个 类别 的 概率 
# 优 化器 我 这里 用了 adadelta 也 可以 使用 
其他 方法 model . compile loss = categorical _ crossentropy 
optimizer = Adadelta metrics = accuracy # = 下面 开始 
训练 nb _ epoch 是 迭代 次数 可以 高 一些 
训练 效果 会 更好 但是 训练 会 变慢 model . 
fit x _ train y _ train nb _ epoch 
= 3 score = model . evaluate x _ train 
y _ train verbose = 0 # 评估 模型 在 
训练 集中 的 效果 准确率 约 99% print train score 
score 0 print train accuracy score 1 score = model 
. evaluate x _ val y _ val verbose = 
0 # 评估 模型 在 测试 集中 的 效果 准确率 
约为 97% 迭代 次数 多了 会 进一步 提升 print Test 
score score 0 print Test accuracy score 1 上述 代码 
和 注释 较为 详细 的 描述 了 该 神经 网络 
的 结构 但是 实际 使用 代码 时 最好 去 除 
中文 注释 部分 否则 可能会 有 一些 编码 问题 四 
. 总结 本文 描述 了 如何 使用 深度 学习 和 
keras 框架 构建 一个 文本 分类器 的 全过程 并给 出了 
相应 的 代码 实现 为了 方便 大家 使用 下面 给出 
本文 代码 的 下载 地址 一 简单 版 https / 
/ github . com / 894939677 / deeplearning _ by 
_ diye / blob / master / pretrain _ text 
_ class _ by _ diye . py 下面 给出 
本文 代码 的 下载 地址 二 完整版 https / / 
github . com / 894939677 / deeplearning _ by _ 
diye / blob / master / pre _ merge _ 
3 . py 五 . 后记 本文 描述 的 是 
使用 类似 于 googlenet 的 网络结构 实际上 也 可以 使用 
类似 与 resnet 的 网络 结构 来 做 这个 事情 
简单 共有 词 判断 模型 TFIDF 向量 表示 T F 
I D F W o r d 2 v e 
c L M e n t e n c e 
Embedding/w 表示/v 简单/a 共有/v 词/n 判断/v 模型/n 假设/vn 现在/t 有/v 
文本/n A/w 和B/nr Num A ∩ B Num A \ 
cap B 表示 A 和B中/nr 相同 词 的 数量 Num 
A ∪ B Num A \ cup B 表示 A 
和B中/nr 所有 词 的 数量 那么 定义 A 和B的/nr 相似 
程度 为 Similarity A B = Num A ∩ B 
Num A ∪ B \ begin { equation } Similarity 
A B = \ frac { Num A \ cap 
B } { Num A \ cup B } \ 
end { equation } TFIDF 向量 表示 上述 共有 词 
方式 只 利用 了 词语 的 信息 却 忽略 了 
词频 信息 引入 TFIDF 将 词语 向 量化 既 考虑 
了 Term Frequency 词频 又 考虑 了 词语 在整个 文档 
中 的 分布 情况 文本 A 和 文本 B 可以 
分别 表示 为 A = a1 a2 . . . 
aN B = b1 b2 . . . bN \ 
begin { equation } A = a _ 1 a 
_ 2 . . . a _ N \ \ 
B = b _ 1 b _ 2 . . 
. b _ N \ end { equation } 其中 
N 表示 词语 的 总数 或 词典 大小 ai = 
TF i ∗ IDF i a _ i = TF 
i * IDF i TF i TF i 表示 词语 
i 在 文档 aia _ i 表示 为 A 文档 
中 出现 的 频率 IDF i IDF i 表示 在 
所有 文档 中 出现 的 频率 同理 可以 得到 bib 
_ i 得到 A B 文档 的 TFIDF 向量 表示 
后 可以 根据 相似 度 函数 f a b f 
a b 来 计算 A 和B/nr 文档 的 相似 度 
f a b f a b 可以 选用 一 阶 
范数 | a − b | | a b | 
也 可以 选用 余弦 相似 度 cosine a b cosine 
a b 来 表示 TFIDF + Word2vecTFIDF 未 给出 词语 
与 词语 之间 的 关系 认为 每个 词语 都是/nr 相互 
独立 的 个体 但 有些 词语 是 同义词 有些 词语 
是 反义词 需要 表征 词语 之间 意思 相距 程度 此处 
选用 word2vec 利用 额外 的 大 预料 为 每个 词语 
训练 一个 word2vec 向量 表示 该 向量 可以 表示 矩阵 
为 Mword2vecM _ { word2vec } 维度 为 N * 
K 其中 N 表示 词典 大小 K 表示 向量 维度 
Mword2vec = mij N ∗ K \ begin { equation 
} M _ { word2vec } = m _ { 
ij } _ { N * K } \ end 
{ equation } 由上 一段 知道 TFIDF 是 一个 M 
* N 的 向量 其中 M 表示 文档 的 总数 
N 表示 词典 的 大小 因此 可以 使用 向量 M 
表示 文档 A 或 B 如下 所示 M = MTFIDF 
∗ Mword2vec \ begin { equation } M = M 
_ { TFIDF } * M _ { word2vec } 
\ end { equation } M 是 一个 M * 
K 的 矩阵 即 每个 文档 可以 表示 为 M 
的 一个 行向量 K 维 再 使用 该 向量 用于 
计算 文本 之间 的 相似 度 LM + Sentence Embedding 
表示 使用 Deep Learning LSTM 的 方法 对 一个 大 
语料 训练 一个 Language Model 然后 使用 BiRNN 模型 训练 
得到 句子 的 表达 ff fb f _ f f 
_ b fff _ f 表示 前 向 RNN 的 
的 表达 fbf _ b 表示 反向 RNN 的 表达 
模型 的 输出 得到 句子 表达 然后 再 利用 余弦 
相似 度 进行 文本 相似 度 比较 摘要 在 信息 
网络 高速 发展 的 时代 神经 网络 已经 成为 人工智能 
领域 不可缺少 的 部分 神经 网络 的 推广 使得 更多 
的 人 了解到 人工智能 推动 了 控制 理论 的 不断 
前进 以 深度 神经网络 算法 为例 推动 识别 技术 新 
业务 的 功能 突破 人脸 识别 率 达到 99.5% 助力 
人工智能 的 应用 和 发展 与此同时 在 自然 语言 的 
处理 研究 过程 中 多义词 的 辨识 以及 短 文本 
的 情感 分析 等 方面 的 研究 使得 人工智能 具有 
更 强大 脑 随着 深度 神经 网络 和 自然 语言 
处理 等 基础 技术 的 进步 人工智能 具有 了 更广 
的 应用 领域 将 逐步 进入 到 各行各业 中 最终 
革新 人们 的 生产 和 生活 方式 关键字 神经网络 自然语言 
处理 人脸识别 多义词 辨识 短 文本 情感 分析 人工智能 一 
神经 网络 在 人工智能 中的 应用 1 典型 的 神经 
网络 1.1 感知机 感知机 可以 称为 第一代 的 神经 网络 
主要 包括 输入 的 多个 特征 单元 人工 定义 或 
程序 寻找 中间 由 学习 权重 连接 最后 由 决策 
单元 输出 典型 的 感知机 单元 遵循 前馈 模型 输入/v 
通过/p 权重/nr 处理/v 后/f 直接/ad 连接/v 到/v 输出/v 单元/n 上/f 
图表 1 感知机 感知机 可以 使用 的 前提 是 感知机 
已经 训练 完毕 训练 是 为了 调整 它 的 权值 
1.2 卷积 神经 网络 在 感知机 和 多层 感知机 的 
基础 上 人们 提出 了 一种 新的 网络结构 卷积 神经网络 
利用 卷积 神经 网络 可以 对 一些 特征 的 检测 
进行 共享 并在/i 尺度/n 和/c 位置/v 和/c 方向/n 上/f 具有/v 
一定/d 的/uj 不变性/l 1998年 Yann LeCun 提出 的 一个 称为 
LeNet 的 网络 进行 手写 字符识别 获得 了 巨大 的 
成功 下图 是 LeNet 的 主要 结构 一个 包括 卷积 
池/nr 化/n 和全/nr 连接/v 的/uj 六层/m 网络结构/n 图表 2LeNet 网络 
它 利用 反向 传播 算法 来 对 隐藏 层 的 
单元 权重 进行 训练 并在 每个 卷积 层 中 实现 
了 卷积 操作 的 卷积 核 权值 共享 并 引入 
池化层/nr 实现 了 特征 的 缩聚 最后 通过 全 连接 
层 来 实现 输出 2012年 ILSVRC 比赛 由 ImageNet 提供 
了 120 万张 的 高清 训练 数据 目的 是 训练 
一个 可以 分类 出 图像 属于 一千类 中 每 一类 
的 概率 的 模型 并 以此 来 进行 图像 的 
识别 Hinton 的 学生 Alex Krizhevsky 在 LeNet 的 基础上 
改进 了 神经 网络 训 练出 了 一个 具有 7个 
隐藏 层 深度 网络 更深 更 强大 的 AlexNet 并 
引入 了 GPU 进行 并行 训练 极大 的 提高 了 
深度 学习 模型 的 训练 效率 图表 3AlexNet 网络 1.3 
循环 神经网络 循环 神经网络 主要 用于 处理 序列 数据 在 
机器学习 领域 序列 模型 一般 利用 序列 数据 作为 输入 
来 训练 序列 模型 用于 预测 序列 数据 的 下 
一项 在 循环 神经网络 之前 主要 使用 无 记忆 模型 
处理 这类 任务 图表 4 循环 神经网络 循环 神经网络 包含 
了 两个 重要 的 特点 首先 拥有 一 系列 隐含 
状态 的 分布 可以 高效 的 存储 过去 的 信息 
其次 它 具有 非线性 动力学 可以允许 它 以 复杂 的 
方式 更新 隐藏 状态 在 足够 的 时间 和 神经元 
数量 下 RNN 甚至 可以 计算出 计算机 能 计算 的 
任何 东西 它们 甚至 会 表现 出 振动 牵引 和 
混沌 的 行为 目前 主要 有 四种 有效 的 方式 
实现 循环 神经网络 主要 包括 长 短时记忆 Long Short Term 
Memory 海森 自由 优化 方法 Hessian Free Optimization 回声 状态 
网络 Echo State Networks 以及 利用 动量 的 初始化 Good 
initialization with momentum 2 深度 神经 网络 在 人工智能 应用 
中 的 识别 过程 深度 神经网络 广泛 应用 与 语音识别 
大 数据分析 图像识别 行为 识别 等 领域 在 各自 的 
领域 中 如果 一个 深度 神经网络 识别 过程 示意图 如下 
那么 我们 称之为 一个 较好 的 深度 神经网络 图表 5 
一个 好 的 深度 神经网络 识别 过程 在 识别 领域 
深度 神经网络 发挥 这 极其 重要 的 作用 它 可以 
解决 某些 我们 人类 很难 解决 的 识别 问题 比如/v 
对于/p 狗/n 和狼大/nr 多数人/i 都/d 无法/n 将/d 他们/r 正确/ad 辨认/v 
但是 深度 神经 网络 可以 通过 训练 机器 用 标 
记好 的 图片 训练 它 让 它 学会 分类 . 
对于 给定 的 一张 图 神经网络 做出 基本型 反应 进一步 
计算 神经 网络 做 复杂 结构 反应 近 而 神经 
网络 做 抽象概念 反应 最终 输出 预测 结果 识别 狗 
和狼的/nr 过程 如下 图 所示 图表 6 神经网络 识别 狗 
3 神经网络 算法 基于 多层 神经 网络 的 新的 深度 
学习 算法 是 一种 新的 算法 和 结构 新的 网络 
结构 中 最 著名 的 就是 CNN 它 解决 了 
传统 较深 的 网络 参数 太多 很难 训练 的 问题 
使用 了 局部 感受 野 和 权植/nr 共享 的 概念 
大大 减少 了 网络 参数 的 数量 这种 结构 很 
符合 视觉 类 任务 在 人脑 上 的 工作 原理 
为了 解决 传统 的 多层 神经 网络 的 一些 不足 
梯度 消失 过拟合 等 产生 了 一些 新的 方法 新的 
激活 函数 ReLU 新的 权重 初始化 方法 逐层 初始化 XAVIER 
等 新 的 损失 函数 新的 防止 过拟合 方法 Dropout 
BN 等 神经 网络 解决 了 早期 人工智能 的 一些 
遗留 问题 在 大 数据 和大算/nr 力 的 加持 下 
使得 人工智能 重新 进入 到 大众 的 视野 广泛 应用 
与 视觉 识别 图像识别 语音识别 棋类 AI 中 提升 了 
人工智能 的 深度 和 广度 二 自然语言 处理 中 的 
人工智能 1 自然语言 处理 的 多义词 辨识 造成 语言 模糊性 
的 一个 重要 原因 是 词 的 多义性 词义 辨识 
不准确 使 语法 语义 的 分析 存在 偏差 运用 神经网络 
模型 根据 词 的 搭配 关系 句 内 语法 语义 
约束 前后文 的 语义 逻辑关系 确定 词义 实现 机器 翻译 
中 多义词 的 正确 辨识 例如 我们 通过 网络 模型 
根据 文中 关键词 创建 语义 场 然后 根据 当前 语义 
场 对 多义词 进行 解释 实现 汉英 翻译 中 多义词 
语义 的 正确 辨识 正确 辨识 汉英 翻译 中的 多义词 
语义 的 处理 方法 我们 只有 词形 信息 的 书面 
汉语 文本 提供给 机器 这些 词形 信息 隐含 了 语法 
关系 逻辑关系 修辞 关系 以及 文本 的 主题 和 作者 
的 写作 风格 等 算法 的 核心 是 如何 利用 
这些 信息 在 机器 翻译 系统 中 记录 词 到 
词 义项 映射 关系 的 是 双语 语义 词典 知识库 
在 双语 语义 词典 中一 词形 X 有m个/nr 语 义项 
具体 选择 哪 一个 语 义项 由 所处 的 语言 
环境 决定 根据 上下文 很 容易 获取 多义词 的 具体 
意义 从 我们 人脑 处理 语言 的 过程 中 可以 
总结 出 机器 翻译 的 多义词 辨识 分为 两个 阶段 
第一 阶段 是 当前 语义 场 的 建立 假设 语义 
场 是由 相继 出现 的 一系列 关键词 共同 创建 第二 
阶段 是 利用 当前 语义 场 决定 词 的 词义 
由此可见 机器翻译 过程 是 一个 约束 满足 问题 在此 基础 
上 可 建立 约束 满足 的 神经 网络 模型 模型 
学习 的 例句 为 1 汤挺/nr 热乎 2 他 待人 
总是 很 热乎 在 双语 词典 中 热乎 有 两个 
词义 一个 词义 在 温度 场 中 解释 为 warm 
另 一个 在 情感 场 中 解释 为 warm and 
friendly 在 例子 1中 创建 温度 场 的 关键字 是 
具有 温度 属性 的 汤 在 例子 2中 创建 情感 
场 的 是 与 情感 有关 的 他 待人 关键字 
多义词 识别 不仅 应用于 机器翻译 还 广泛 应用 与 人机交互 
的 语音 识别 系统 以及 智能 机器人 在 一定 成 
都上 多义词 识别 问题 的 解决 推动 的 人工智能 在 
智能 机器人 的 发展 2 自然语言 处理 的 短文 本 
情感 分析 在 自然 语言 处理 中的 又一 重要 难题 
是 能否 正确 表达出 句子 的 情感 是 自然 语言 
处理 关键性 任务 的 前提 基础 2.1 情感 分析 情感 
分析 SA 又 称为 倾向性 分析 和 意见 挖掘 它 
是 对 带有 情感 色彩 的 主观性 文本 进行 分析 
处理 归纳 和 推理 的 过程 其中 情感 分析 还 
可以 细分 为 情感 极性 倾向 分析 情感 程度 分析 
主 客观 分析 等 情感 极性 分析 的 目的 是 
对 文本 进行 褒义 贬义 中性 的 进行 判断 情感 
程度 分析 主要 是 对 同一 情感 极性 中 再 
进行 划分 或者 细分 以 描述 该 极性 的 强度 
例如 喜爱 和 敬爱 都是 褒义词 但是 敬爱 相对来说 褒义 
的 程度 更加 强烈 一些 主 客观 分析 主要 目的 
是 识别 文本 中 哪些 部分 是 客观 称 述而 
不带 情感 色彩 哪些 是 带有 情感 的 主管 描述 
在 对于 微博 或者 商品 评论 时 这个 分析 步骤 
一般 都 忽略 因为 微博 和 商品 评论 本身 就 
一定 存在 强烈 的 情感 色彩 而且 客观 描述 的 
情感 值 理论上 是 为零 不影响 最终 的 情感 分析 
结果 2.2 情感 分析 的 主要 方法 现阶段 主要 的 
情感 分析 方法 主要 有 两类 基于/p 词典/n 的/uj 方法/n 
和/c 基于/p 机器/n 学习/v 的/uj 方法/n 基于/p 词典/n 的/uj 方法/n 
主要/b 通过/p 制定/v 一/m 系列/q 的/uj 情感/n 词典/n 和/c 规则/n 
对 文本 进行 段落 拆借 句法分析 计算 情感 值 最后 
通过 情感 值 来 作为 文本 的 情感 倾向 依据 
基于 机器 学习 的 方法 大多 将 这个 问题 转化 
为 一个 分 类 问题 来 看待 对于 情感 极性 
的 判断 将 目标 情感 分类 2类 正 负 对 
情感 程度 的 分析 则 转化 为 回归 问题 看待 
对 训练 文本 进行 人工 标 标注 然后 进行 有 
监督 的 机器 学习 过程 在 还 没有 获得 大量 
文本 的 情况 下 使用 基于 词典 的 方法 或者 
简单 的 机器 学习 方法 是 一个 不错 的 选择 
获得 大量 文本 后 可以 尝试 使用 一些 复杂 的 
机器 学习 方法 甚至 使用 深度 学习 来 进一步 提升 
分析 效果 2 . 2.1 基于 词典 的 情感 分析 
情感 分析 对象 的 粒度 最小 是 词汇 但是 表达 
一 个 情感 的 最 基本 的 单位 则是 句子 
词汇 虽然能 描述 情感 的 基本 信息 但是 单一 的 
词汇 缺少 对象 缺少 关联 程度 并且 不同 的 词汇 
组合 在 一起 所 得到 的 情感 程度不同 甚至 情感 
倾向 都 相反 所以 以 句子 为最 基本 的 情感 
分析 粒度 是 较为 合理 的 篇章 或者 段落 的 
情感 可以 通过 句子 的 情感 来 计算 基于 词典 
的 情感 分析 大致 步骤 如下 分解 文章 段落 分解 
段落 中 的 句子 分解 句子 中 的 词汇 探索 
情感 并 标注 和 计数 搜索 情感 词 前 的 
程度 词 根据 程度 大小 赋予 不同 权值 搜索 情感 
词 前 的 否定词 赋予 反转 权值 1 计算 句子 
情感 得分 计算 段落 情感 得分 计算 文章 情感 得分 
考虑到 语句 中的 褒贬 并非 稳定 分布 以上 步骤 对于 
积极 和 消极 的 情感 词 分开 执行 最终 的 
到 两个 分值 分别 表示 文本 的 正向 情感 值 
和 负向 情感 值 进过 以上 的 步骤 每 篇 
文章 的 每个 段落 的 每个 句子 都会 有 相应 
的 情感 分值 之后 针对 需求 可以 针对 句子 的 
分值 作 统计 计算 也 可以 针对 段落 的 分值 
作 统计 计算 得到 最后 的 文本 的 正向 情感 
值 和 负向 情感 值 2.3 基于 情感 分析 的 
系统 和 应用 总体 来看 基于 情感 分析 的 系统 
和 应用 在 商品 / 服务 评论 分析 社交 网络分析 
情感 机器人 这 三 方面 被 广泛 应用 传统/n 的/uj 
情感/n 分析/vn 应用/v 聚焦/v 于/p 来自/v 消费/vn 产品/n 和/c 服务/vn 
的/uj 评论/n 基于 产品 评论 的 代表性 平台 有 Google 
Shopping 它 还 可以 为 用户 提供 在线 购物 平台 
的 商品 检索 和 比价 服务 OpinionEQ 允许 商业 组织 
和 个人 按需 定制 产品 分析 服务 微博 Twitter 等 
社交 网络 服务 的 爆炸 式 发展 也 为 研究 
人员 带来 了 极大 的 机遇 研究 人员 能够 通过 
分析 大量 富 情感 的 数据 来 分析 公众 的 
情绪 变化 并 对 政府 管理 经济 娱乐 领域 产生 
影响 从/p 政府/n 和/c 管理者/n 角度/n 出发/v 联合国 开发 了 
针对 全球 情感 波动 监测 的 应用 Global Pulse 北航 
的 研究 小组 推出 了 第一 个 针对 中文 微博 
的 在线 情感 系统 MoodLens 2012年/tdq 美国/ns 大选/v 时/n 罗姆尼/nr 
和/c 奥巴马/ns 在/p Twitter/w 上/f 展开/v 了/ul 激烈/a 宣传/vn 借此 
影响 普通 民众 及 新闻 从业者 成为 互联网 参与 总统 
竞选 典型 案例 在 金融 应用 方面 许多 研究 机构 
将 情感 分析 技术 应用 于 股票 分析 及 预测 
系统 例如 Stock Sonar 在 每只 股票 的 价格 旁边 
展示 了 每天 针对 该股 的 积极 和 消极 的 
情感 信息 为 投资者 提供 即时 的 参考 UIC 开发 
的 Twitter 情感 分析 进行 为 股市 的 涨跌 进行 
预测 和 追踪 在 娱乐 领域 阿里云 的 人工 智能系统 
小 Ai 在 我 是 歌手 节目 中 成功 预测 
李玟 夺冠 也是 依靠 现场 数据 以及 社交 网络 上 
的 点评 数据 进行 分析 预测 这 其中 都 运用 
了 对 海量 文本 情感 分析 技术 可以 看到 对 
社交 媒体 的 情感 大 数据 的 监测 和 分析 
预测 不断 影响 着 政府 决策 和 大众 选择 除了 
在 电商 平台 和 社交 网络 得到 广泛 应用 情感 
分析 技术 还 被 引入 到 对话 机器人 领域 例如 
微软 的 小冰 机器人 可以 通过 分析 用户 的 文本 
输入 和 表情 贴图 理解 用户 当前 的 情绪 状况 
并 据此 回复 文本 或者 语音 等 情感 回应 部分 
研究 机构 还将 情感 分析 技术 融入 实体 机器 人中 
日本 软银 公司 的 Pepper 机器人 依据 常见 的 情感 
认知 喜怒 哀 惊 及 对 用户 的 面部 表情 
肢体 语言 和 措辞 的 分析 了解 用户 的 情绪 
并 选择 恰当 的 方式 与 用户 交流 而 香港 
Hanson Robotics 公司 开发 的 Han 机器人 不仅 可以 理解 
用户 的 情感 它 还 可以 将 情感 反馈 以 
模拟 的 面部 表情 展现 出来 国内/s 的/uj Gowild/w 公司/n 
也/d 推出/v 了/ul 可以/c 提供/v 生活/vn 助理/vn 和/c 年轻人/n 强/a 
社交/n 情感/n 交流/n 服务/vn 的/uj 公子小白 机器人 这些 工作 实际上 
并 不是 从 认知 机理 出发 而是 通过 外在 的 
形式 词语 表情 肢体 判断 人类情感 3 中国 的 自然 
语言 处理 领域 的 人工智能 公司 近年来 自然语言 处理 在 
工业界 与 学术界 被 广泛 应用 与 人工智能 领域 从 
学术界 来说 中国 大陆 地区 除 了 微软 亚洲 研究院 
越来越 多 的 研究 机构 设立 了 自然 语言 处理 
实验室 据 互联网 周刊 了解 清华大学 自然语言 处理 与 社会 
人文 计算 实验室 北京大学 计算机 科学 研究所 语言 计算 与 
互联网 挖掘 研究室 哈工大 机器 智能 技术 与 自然 语言 
处理 实验室 中科院 自然语言 处理 研究组 复旦 大学 自然 语言 
处理 研究组 等 都对 自然语言 处理 有 深入 的 研究 
与此同时 随着 自然语言 处理 领域 的 兴起 越来越 多 的 
自然 语言 处理 领域 的 公司 相继 出现 互联网 周刊 
整理 了 自然 语言 处理 领域 的 代表性 公司 图表 
7 自然语言 处理 领域 的 代表性 公司 三 总结 随着 
神经网络 算法 的 不断 优化 将 加速 人工智能 产业 的 
发展 尤其 是 推动 人机交互 万物 相连 的 发展 物联网 
服务 更加 智能化 基础 电信 企业 能 快速 的 拓展 
新 业务 领域 搜寻 到 其他 有 价值 的 业务 
增长点 与此同时 各行业 运营商 积极 人工智能 领域 的 应用 通过 
神经网络 建立 合作 开发 的 智能 应用平台 联合 创新 推动 
人工智能 在 居民 的 日常 生活 交通 物流 家居 教育 
医疗 等 行业 的 不断 成长 将来 随着 情感 分析研究 
不断 突破 自然语言 处理 技术 越来越 成熟 其 应用 前途 
广大 尤其 是 和 实体 机器 人 结合 将 多媒体 
技术 融为一体 结合 语音 图像 处理 技术 可以 从 语言 
表情 和 行为 方面 理解 人类情感 并给 出 相应 的 
情感 回复 打造 一个 具有 情感 的 机器人 时代 已经 
不 远了 参考文献 1 神经 网络 在 人工智能 中的 应用 
刘 肖楠 信息 技术 与 信息化 2 神经网络 算法 在 
人工 智能 识别 中的 应用 研究 张庆 刘中儒/nr 郭华 江苏 
通信 3 密集 连接 卷积 神经网络 让 人工智能 拥有 更 
强大 脑 黄高 上海 信息化 4 基于 卷积 神经 网络 
的 自然 语言 处理 相关 技术 研究 于彦秋/nr 国防科学技术大学 5 
神经网络 理论 应用 于 自然 语言 处理 贺前华/nr 徐秉铮/nr 中文信息 
6 中国 的 自然 语言 处理 领域 的 人工智能 公司 
轩中 互联网 周刊 7 人工智能 在 自然 语言 处理 中 
的 应用 李彦峰 襄阳 职业 技术 学院 学报 8 面向 
自然语言 处理 的 人工智能 框架 蔡艳婧/nr 程显毅/nr 潘燕 微电子学 与 
计算机 9 文本 情感 分析 让 机器 读懂 人类情感 黄高 
tableI translated it myself . It may not be authoritative 
. i n d e x E n g l 
i s h C h i n e s e 
1 A u t o m a t i c 
speech recogniton 自动 语音 识别 2CCG supertaggingCCG 超级 标记 3Common 
sense 常识 4Constituency parsing 选区 分析 5Coreference resolution 共 指 
消解 6Dependency parsing 依存关系 句法分析 7Dialogue 对话 8Domain adaptation 领域 
自适应 9Entity linking 实体 链接 10Grammatical error corrrection 语法错误 更正 
11Information extraction 信息提取 12Language modeling 语言 模型 13Lexical normalization 词汇 
规范化 14Machine translation 机器翻译 15Multi task learning 多任务 学习 16Multi 
model 多 模态 17Named entity recognition 命名 实体 识别 18Natural 
language inference 自然语言 推理 19Part of speech tagging 词性 标注 
20Question answering 问答 21Relation prediction 关系 预测 22Relationship extraction 关系 
提取 23Semantic textual similarity 语义 文本 相似性 24Semantic parsing 语义分析 
25Semantic role labeling 语义 角色 标记 26Sentiment analysis 情感 分析 
27Shallow syntax 浅 句法 28Simplification 简化 29Stance detection 姿态 检测 
30Summarization 摘要 31Taxonomy learning 分类 学习 32Temporal processing 时间 处理 
33Text classification 文本 分类 34Word sense disambiguation 词义 消 歧 
前言 最近 在做 的 一个 项目 非 结构化 数据处理 然后 
从 自然语言 处理 入手 开始 学习 一下 如果 使用 python 
进行 自然 语言 的 处理 参考资料 https / / github 
. com / wnma3mz / Nltk _ Studyhttps / / 
wnma3mz . github . io / hexo _ blog / 
2018 / 05/13 / Python 自然语言 处理 阅读 笔记 一 
/ 首先 下载 了 anaconda 然后 按照 说明 下载 了 
数据文件 这个 数据文件 一 开始 并不 知道 要 拷 到 
哪里 去 但是 尝试 了 输入 了 import nltk from 
nltk . book import * 之后 发现 了 报错 然后 
我 在 download 之后 顺 其 根源 发现 了 需要 
拷贝 的 地址 这样 以后 找到 了 需要 拷贝 的 
目录 将 数据文件 复制 进去 这样 就 可以 运行 我 
需要 的 操作 了 ~ 然后 再运行 以上 代码 # 
导入 nltk 模块 import nltk # 导入 基本 语料 集 
不 需要 额外 下载 包含 text1 到 text9 变量 可以 
直接 输出 这些 变量 from nltk . book import * 
# 搜索 文本 这里 表示 找到 monstrous 所 包含 的 
句子 并且 输出 上下文 text1 . concordance monstrous # 搜索 
文本 出现 在 相似 的 上下 文中 text1 . similar 
monstrous # 搜索 两个 及 两个 以上 共同 词 的 
上下文 text2 . common _ contexts monstrous very # 画 
一张 离散 图 表示 这些 词 出现 在 文本 中 
的 位置 输出 见 下图 text4 . dispersion _ plot 
citizens democracy freedom duties America # 基于 文本 随机 生成 
一些 文本 text3 . generate 最后 一句话 又 遇到 了 
报错 看上去 是 缺少 参数 于是 我 在 括号 里面 
添加 参数 没有 任何 变化 并不 知道 发生 了 什么 
愣住 接下来 继续 分析 text # 有序 字典 按 词频 
从高 到 低 排序 fdist1 = FreqDist text1 # 选出 
词频 最高 的 50个 词 fdist1 . keys 50 # 
某个 词 出现 的 频数 fdist1 whale # text1 中 
词频 最高 的 50个 单词 进行 绘图 输出 见 下图 
fdist1 . plot 50 cumulative = True # text1 中 
只 出现 过 一次 的 单词 fdist1 . hapaxes 然后 
其实 只要 修改 添加 一个 list 就行了 list fdist1 . 
keys 50 可能 是 因为 版本 的 缘故 又一次 遇到 
需要 list 的 地方 修改 如下 对于 机器 人 对话 
的 命令 发现 根本 停不下来 崩溃 好吧 今天 的 学习 
到此结束 每日 一吹 咖啡 鸡 天下第一 文章/n 目录/n 自然语言/l 处理/v 
的/uj 概述/v 人工智能/n 自然语言/l 处理/v 的/uj 应用/v 简单/a 的/uj 自然/d 
语言/n 处理/v 模型/n 感想/v 自然语言/l 处理/v 的/uj 概述/v 自然语言/l 处理/v 
就是/d 利用/n 电子/n 计算机/n 为/p 工具/n 对/p 人类/n 特有/b 的/uj 
书面/n 和/c 口头/n 形式/n 的/uj 自然/d 语言/n 的/uj 信息/n 进行/v 
各种/r 类型/n 处理/v 和/c 加工/vn 的/uj 技术/n 这种 技术 因为 
运用 环境 和 发展 潜力 巨大 已经 成为 一 门 
专门 的 边缘性 交叉学科 涉及 到 语言学 计算机科学 以及 数学 
等 自然 处理 的 目的 在于 建立 各种 自然语言 处理 
系统 包括 机器翻译 系统 自然语言 理解 系统 自动 检索系统 文字 
自动 识别 系统 数据库系统 等 自然 语言 是 人脑 的 
高级 功能 之一 也 是 人类 区别于 其他 动物 的 
重要 标志 人类 可以 借助 于 自然 语言 交流 思想 
在 人际 交往 及 社会 组成 中 发挥 了 巨大 
的 作用 另外 人类 还 借助 于 自然 语言 进行 
思维 活动 认识 事物 的 本质 和 规律 所谓 脑海中 
的 声音 意味着 人类 无法 脱离 一个 表达 性质 的 
工具 来 进行 思考 每次 人类 进行 思考 的 时候 
其实/d 是/v 相当/d 于/p 自己/r 和/c 自己/r 进行/v 着/uz 对话/n 
可以 说 离开 了 自然 语言 人类 社会 和 科学 
技术 的 发展 不 可能 如此 井然有序 的 进行 自 
计算机 诞生 以来 人类 与 计算机 交互 只能 通过 编程语言 
编写 的 代码 来 实现 如 使用 basic pasical c 
lisp 等 计算机程序 设计 语言 对于 计算机 来说 他 只能 
根据 二进制 的 指令 来 作出 不同 的 行为 反应 
而 计算机程序 设计者 则 往往 在 这个 过程 中 起到 
了 翻译 的 功能 即将 在 自然 语言 表述 下 
的 功能 需求 用 程序 设计 语言 表述 再由 特定 
的 编程 转化 为 机器 可以 理解 的 二进制 指令 
计算机 能够 做 我们 想 要 完成 的 事情 但是 
他 并不 真正 理解 我们 的 语言 从 这个 意义 
上 来说 如果 想 要让 计算机 足够 智能 让 计算机 
能够 直接 理解 我们 的 指令 显然 是 非常 重要 
的 而在 这 其中 自然 语言 的 处理 及 是 
人与 计算机 沟通 的 桥梁 人工智能 自然语言 处理 的 应用 
现在 人工智能 的 飞速 发展 主要 得益于 统计学 方法 即 
以 从此 诞生 的 机器 学习 的 思想 而 之前 
的 的 其 学习 的 思想 机器 学习 的 积极 
学习 的 发展 现在 人们 通过 获取 并 输入 海量 
的 数据 让 计算机 能够 在 这些 数据 集中 找到 
自己 其中 的 规律 不同 于 以往 使用 逻辑 严密 
的 语言 告诉 计算机 应当 怎样 做 现在 我们 所 
做 的 只是 把 基础 的 数据 和 正确 的 
答案 给 计算机 在 中间 的 过程 中 为什么 计算机 
做出 这样 的 判断 其实 并不 重要 那么 这样 的 
思想 对 自然 语言 处理 会有 怎样 的 启发 呢 
接下来 我 就 通过 一个 实例 来 讲述 一下 人工智能 
自然 语言 在 机器 学习上 的 运用 围棋 AI 的 
机器学习 AlphaGo 在 社会 上 引起 巨大 的 反响 而且 
围棋 和 自然 语言 处理 的 复杂度 都是 极高 的 
AlphaGo 项目 的 研究 人员 中 并 没有 特别 高 
水平 的 围棋 选手 如果 按照 以往 的 人工智能 的 
思路 研究 人员 必须 要 把 如何 更好 地 获胜 
采用 逻辑 较为 严密 的 语言 编写 为 程序 使得 
AlphaGo 在 对局 的 时候 能够 利用 自己 超高 的 
运算 能力 在 获胜 技巧 中 不断 穷举 运算 从而 
做出 最优 解 这也 是 以往 在 象棋 等 棋牌 
类 游戏 中 人工智能 的 思路 然而 围棋 之所以 有 
挑战性 正是 因为 围棋 的 可能性 有 太多 种 根本无法 
依靠 穷举 来 计算 下 几步 可能 的 结果 无法 
进行 比较 也 就是 无从谈起 所谓 的 最优 解 了 
而 之所以 AlphaGo 能够 在 之后 的 棋局 中 战胜 
世界 围棋 第一 人 正是 人工智能 采取 新的 机器学习 思路 
的 胜利 简单 的 自然 语言 处理 模型 在 具体 
涉及 到 自然 语言 处理 时 学习 数据 从 棋局 
变成 了 语料库 下面 将 以 一个 简单 的 自然 
语言 问答 系统 为例 简要介绍 其中 的 步骤 数据 加载 
分为 两部分 加载 语料 和 预处理 加载 语料 可以 认为 
是 简单 的 数据 存储 在 系统 能够 访问 在 
数据库 里 之后 数据 会 从 数据库 中 流向 设计 
好 的 神经 网络 模型 预 处理 的 目的 是 
为了 让 数据 更 规范 一般而言 是 把 语料 组成 
组合成 输入 需要 的 格式 从 人类 的 角度 把 
数据 变得 更 易于 机器 理解 从而 增加 机器 学习 
的 效率 训练 过程 机器学习 模型 一般 使用 神经网络 输入 
是 序列 从 左侧 进入 输出 序 列为 包含 多个 
结果 数值 序列 在 这个 过程 中 研究 人员 需要 
设置 超 参数 模型 的 各项 参数 如 问题 的 
最大 程度 神经元 个数 等 在 设置 神经 网络 的 
激活 函数 损失 函数 损失 大 自然 等 思想 使得 
该 神经网络 能够 在 一定 程度 上 进行 忽略 模拟 
至此 一个 简单 的 神经 网络 已经 建成 再 往后 
就是 进行 迭代 训练 使 数据 不断 的 流向 神经网络 
模型 观察 效果 因为 超 参数 及 模型 的 设置 
等 可能会 有不/nr 合理 之处 在 训练 过程 中 可以 
观察 损失 函数 和 准确度 的 变化 根据 这些 变化 
可以 帮助 优化 模型 辅助 超 参数 的 设置 感想 
自然语言 处理 是 计算机 科学 中 的 重要 课题 但 
因为 自然语言 处理 巨大 的 复杂性 使得 以 逻辑 为基础 
的 符号 模型 化 研究 方法 男 音 取得 较 
为基础 的 进展 然而 随着 人工智能 技术 的 发展 特别 
是 机器 学习 的 应用 对于 类似 规则 非逻辑 严谨 
的 问题 有了 较 理想 的 解决方案 相信 随着 人工智能 
研究 的 不断 发展 实现 人类 与 机器 的 无障碍 
交流 也许 就在 不远 的 将来 自然语言 处理 基础 技术 
自然语言 处理 基础 技术 自然 语言 的 基础 技术 包括 
词汇 短语 句子 和 篇章 级别 的 表示 以及 分词 
句法分析 和 语义分析 等 词法 分析 的 主要 任务 是 
词性 标注 和 词义 标注 词性 标注 就是 在 给定 
句子 中 判断 每个 词 的 语法 范畴 确定 其 
词性 并 进行 标注 解决 兼类 词 和 确定 未 
登录 词 的 词性 问题是 标注 的 重点 词义 标注 
的 重点 就是 解决 如何 确定 多义词 在 具体 语境 
中的 义项 问题 标注 过程 中 通常 是 先 确定 
语境 再 明确 词义 方法 和 词性 标注 类似 判断 
句子 的 句法结构 和 组成 句子 的 各 成分 明确 
它们 之间 的 相互 关系 是 句法分析 的 主要 任务 
句法分析/l 通常/d 有/v 完全/ad 句法分析/l 和/c 浅层/n 句法分析/l 两种/m 完全 
句法分析 是 通过 一 系列 的 句法分析 过程 最终 得到 
一个 句子 的 完整 的 句法树 存在 两个 难点 一是 
词性 歧义 二 是 搜索 空间 太大 通常 是 句子 
中词 的 个数 n 的 指数级 浅层 句法分析 又叫 部分 
句法分析 或 语 块 分析 只 要求 识别 出 句子 
中 某些 结构 相对 简单 的 成分 如 动词短语 非 
递归 的 名词 短语 等 这些 结构 被 称为 语 
块 一般来说 浅层 语法分析 会 完成 语 块 的 识别 
和 分析 语 块 之间 依存 关系 的 分析 两个 
任务 其中 语 块 的 识别 和 分析 是 浅层 
语法分析 的 主要 任务 语义分析 是 指 根据 句子 的 
句法结构 和 句子 中 每个 实词 的 词义 推 导出来 
能够 反映 这个 句子 意义 的 某种 形式 化 表示 
即将 人类 能够 理解 的 自然 语言 转化 为 计算机 
能够 理解 的 形式语言 自然语言 处理 的 基础 研究 还 
包括 语用 语境 和 篇章 分析 语用 是 指人 对 
语言 的 具体 运用 研究 和 分析 语言 使用者 的 
真正 用意 它 与 语境 语言 使用者 的 知识 涵养 
言语 行为 想法 和 意图 是 分不开 的 是 对 
自然 语言 的 深层 理解 情景 语境 和 文化 语境 
是 语境 分析 主要 涉及 的 方面 篇章 分析 则是 
将 研究 扩展到 句子 的 界限 之外 对/p 段落/n 和/c 
整篇文章/n 进行/v 理解/v 和/c 分析/vn 之外 自然 语言 的 基础 
研究 还 涉及 词义 消 歧 指代 消解 命名 实体 
识别 等 方面 的 研究 自然语言 处理 核心技术 自然语言 处理 
核心技术 包括 知识图谱 文本 分类 与 聚 类 自动 文摘 
自动 问答 信息 抽取 文字 识别 语音 技术 信息 推荐 
与 过滤 多 模态 信息 处理 等 知识图谱 知识图谱 是 
为了 表示 知识 描述 客观 世界 的 概念 实体 事件 
等 之间 关系 的 一种 表示 形式 起源 可以 追溯 
至 语义 网络 提 出于 20 世纪 五六十 年代 的 
一种 知识 表示 形式 语义/n 网络/n 由/p 许多/m 个/q 节点/n 
和边/nr 组成/v 这些/r 节点/n 和边/nr 相互连接/l 节点 表示 的 是 
概念 或 对象 边 表示 各个 节点 之间 的 关系 
如 下图 语义 网络 示意图 知识图谱 在 表现 形式 上 
与 语义 网络 比较 类似 不同 的 是 语义 网络 
侧重于 表示 概念 与 概念 之间 的 关系 而 知识图谱 
更 侧重于 表述 实体 之间 的 关系 现在 的 知识 
网络 被 用来 泛指 大 规模 的 知识库 知识图谱 中 
包含 的 节点 有 以下 几种 实体 指 独立 存在 
且 具有 某种 区别性 的 事物 如 一个 人 一种 
动物 一个 国家 一种 植物 等 语义 类 具有 同种 
特性 的 实体 构成 的 集合 如 人类 动物 国家 
植物 等 内容 通常 是 实体 和 语义 类 的 
名字 描述 解释 等 变现 形式 一般 有 文本 图像 
音 视频 等 属性 值 主要指 对象 指定 属性 的 
值 不同 的 属性 类型 对应 于 不同 类型 属性 
的 边 关系 在 知识 图 谱上 表现 形式 是 
一个 将 节点 实体 语义 类 属性值 映 射到 布尔值 
的 函数 文本 分类 与 聚 类 自动 文摘 自动 
文摘 是 运用 计算机技术 依据 用户 需求 从源/nr 文本 中 
提取 最 重要 的 信息 内容 进行 精简 提炼 和 
总结 最后 生成 一个 精简 版本 的 过程 生成 的 
文摘 具有 压缩性 内容 完整性 和 可读性 自动 问答 自动 
问答 是 指 利用 计算机 自动 回答 用户 所 提出 
的 问题 以 满足 用户 知识 需求 的 任务 系统 
反馈 给 用户 的 不再 是 基于 关键词 匹配 排序 
的 文档 列表 而是 精准 的 自然 语言 答案 在 
自然 语言 理解 领域 自动 问答 和 机器 翻译 复述 
和 文本 摘要 一起 被 认为 是 验证 机器 是否 
具备 自然 理解 能力 的 四个 任务 根据 目标 数据源 
的 不同 问答 技术 大致 可以 分为 检索 式 问答 
社区 问答 以及 知识库 问答 三种 检索 式 问答 通过 
检索 和 匹配 回答 问题 推理 能力 较弱 社区 问答 
是 web2 . 0 的 产物 用户 生成 内容 是 
其 基础 Yahoo Answer 百度知道 等 是 典型 代表 这些 
社区 问答 数据 覆盖 了 大量 的 用户 知识 和 
用户 需求 检索/vn 式/k 问答/v 和/c 社区/n 问答/v 的/uj 核心/n 
是/v 浅层/n 语义分析/i 和/c 关键词/n 匹配/v 而 知识库 问答 则 
正在 逐步 实现 知识 的 深层 逻辑推理 信息 抽取 信息 
抽取 主要 是 指 从 文本 中 抽取 出 特定 
的 事实 信息 例如 从 经济 新闻 中 抽取 新 
发布 产品 情况 如 公司 新 产品名 发布 时间 发布 
地点 产品 情况 等 这些 被 抽取 出来 的 信息 
通常 以 结构化 的 形式 直接 存入 数据库 可以 供 
用户 查询 及 进一步 分析 使用 为 之后 构建 知识库 
智能 问答 等 提供 数据 支撑 信息检索 与 信息 信息检索 
信息 抽取 目的 从 大量 的 文档 中 找到 用户 
所 需要 的 文档 在 文本 中 获取 用户 感 
兴趣 或 所 需要 的 事实 信息 技术 以 关键 
字词 匹配 以及 统计 等 技术 不 需要 对 文本 
进行 理解 和 分析 需要 利用 自然 语言 处理 的 
技术 包括 命名 实体 识别 句法分析 篇章 分析 与 推理 
以及 知识库 等 对/p 文本/n 进行/v 深入/v 理解/v 和/c 分析/vn 
后/f 才能/v 完成/v 信息/n 抽取/v 工作/vn 联系/n 信息/n 检索/vn 的/uj 
结果/n 可以/c 作为/v 信息/n 抽取/v 的/uj 范围/n 提高效率 信息 抽取 
用于 信息检索 可以 提高 检索 质量 更好 地 满足 用户 
的 需求 社会 计算 社会 计算 也称 计算 社会学 指 
在 互联网 的 环境 下 以 现代 信息 技术 为 
手段 以 社会 科学 理论 为 指导 帮助 人们 分析 
社会关系 挖掘 社会 知识 协助 社会 沟通 研究 社会 规律 
破解 社会 难题 的 学科 社会 媒体 是 社会 计算 
的 主要 工具 和 手段 它 是 一种 在线 交互 
媒体 有着 广泛 的 用户 参与 性 允许 用户 在线 
交流 协作 发布 分享 传递信息 组成 虚拟 的 网络 社区 
等等 文字 识别 语音 技术 信息 推荐 与 过滤 多 
模态 信息处理 自然语言 处理 应用 技术 自然语言 处理 应用 技术 
包括 机器翻译 信息检索 情感 分析 社会 媒体 处理 等 机器翻译 
运用 机器 通过 特定 的 计算机 程序 将 一种 书写 
形式 或 声音 形式 的 自然 语言 翻译成 另一种 书写 
形式 或 声音 形式 的 自然 语言 机器 翻译 的 
方法 总体 上 可以 分为 基于 理性 的 研究 方法 
和 基于 经验 的 研究 方法 两种 理性主义 的 翻译 
方法 经验主义 的 翻译 方法 特点 由人 类 专家 通过 
编撰 规则 的 方式 将 不同 自然 语言 之间 的 
转换 规律 生成 算法 计算机 通过 这种 规则 进行 翻译 
以 数据 驱动 为基础 主张 计算机 自动 从大/nr 规模 数据 
中 学习 自然语言 之间 的 转换 规律 优势 理论 上 
能够 把握 语言 间 深层次 的 转换 规律 互联网 文本 
数据 不断增长 计算机 运算 能力 也 不断 加强 以 数据 
驱动 为基础 的 统计 翻译 方法 逐渐 成为 机器 翻译 
的 主流 技术 局限性 对 专家 的 要求 极高 不仅 
要求 其 了解 源语言 和 目标语言 还要 具备 一定 的 
语言 学 知识 和 翻译 知识 更要 熟练掌握 计算机 的 
相关 操作技能 统计 机器翻译 也 面临 诸如 数据 稀疏 难以 
设计 特征 等 问题 现实 研制 系统 的 成本 高 
周期长 面向 小语种 的 翻译 更是 人才 匮乏 非常 困难 
因此 翻译 知识 和 语言 学 知识 的 获取 成为 
基于 理性 的 机器 翻译 方法 所 面临 的 主要 
问题 深度 学习 能够 较好 的 缓解 统计 机器翻译 所 
面临 的 挑战 基于 深度 学习 的 机器 翻译 现在 
正 获得 迅速 发展 成为 当前 机器翻译 领域 的 热点 
文本 翻译 最为 主流 的 工作 方式 依然 是 以 
传统 的 统计 机器 翻译 和 神经 网络 翻译 为主 
语音 翻译 可能 是 目前 机器翻译 中 比较 富有 创新 
意思 的 领域 吸引 了 众多 资金 和 公众 的 
注意力 图像 翻译 也有 不小 的 进展 除此之外 还有 视频 
翻译 和 VR 翻译 也在 逐渐 应用 中 但是 目前 
的 应用 还 不太 成熟 信息检索 信息检索 是从 相关 文档 
集合 中 查找 用户 所需 信息 的 过程 信息检索 包括 
存 与 取 两个方面 对 信息 进行 收集 标引 描述 
组织 进行 有序 的 存放 是 存 按照 某种 查询 
机制 从 有序 存放 的 信息 集合 数据库 中 找出 
用户 所需 信息 或 获取 其 线索 的 过程 是 
取 信息检索 的 基本 原理 是 将 用户 输入 的 
检索 关键词 与 数据库 中的 标引词 进行 对比 当 二者 
匹配 成功 时 检索 成功 检索 标识 是 为 沟通 
文献标引 和 检索 关键词 而 编制 的 人工 语言 通过 
检索 标识 可以 实现 存 取 的 联系 一致 检索 
结果 按照 与 提问 词 的 关联度 输出 供 用户 
选择 用户 则 采用 关键词 查询 + 选择性 浏览 的 
交互 方式 获取 信息 情感 分析 情感 分析 又称 意见 
挖掘 是 指 通过 计算 技术 对 文本 的 主客观 
性 观点 情绪 极性 的 挖掘 和 分析 对 文本 
的 情感 倾向 做出 分类 判断 前言 跳过 废话 直接 
看 正文 循环 神经网络 Recurrent Neural Networks RNNs 目前 在 
自然 语言 处理 领域 中的 格外 受 欢迎 很多 简单 
的 自然 语言 处理 任务 可以 直接 由 RNN 来 
完成 这里 列出 几种 RNN 在 自然 语言 处理 领域 
的 应用 算法 以 供参考 目前 只 列出 了 参考 
代码 后续 会 补上 相关 说明 正文 中文分词 算法 具体 
代码 参考 github 命名 实体 识别 算法 具体 代码 参考 
github 文本 生成 算法 这里 内容 比较 多 详见 后 
一篇 博客 后记 分词 算法 的 关键 有 两个 算法 
和 词典 缺 了 其中 一个 效果 都 不会 太好 
可惜 现在 互联网 上 公布 的 标注 语料库 太少 了 
希望 将来 能做/nr 一份 贡献 吧 语言 处理 领域 中 
的 很多 问题 分词 命名 实体 识别 等 都 可以 
转 换为 序列 标注 问题 而 序列 标注 这样 的 
上下文 关系 较 紧密 的 问题 由 RNN 来 处理 
再 适合 不过 了 视频 地址     https / 
/ www . bilibili . com / video / av55504443 
/ 相关 资料 待 审核 Kaggle 竞赛题 https / / 
www . kaggle . com / c / home depot 
product search relevance 这里 将 使用 多种 处理 库 对比 
一下 Python NLP 领域 各个 库 的 优缺点 Step1 导入 
所需 所有 要 用到 的 库 读入 训练 / 测试 
集 及 介绍 import numpy as np import pandas as 
pd from sklearn . ensemble import R a n d 
o m F o r e s t R e 
g r e s s o r BaggingRegressor from nltk 
. stem . snowball import SnowballStemmer df _ train = 
pd . read _ csv . . / input / 
train . csv encoding = ISO 8859 1 df _ 
test = pd . read _ csv . . / 
input / test . csv encoding = ISO 8859 1 
df _ desc = pd . read _ csv . 
. / input / product _ descriptions . csv 查看 
数据 df _ train . head df _ desc . 
head 看来 不 要做 太多 的 复杂 处理 我们 于是 
直接 合并 测试 / 训练 集 以 便于 统一 做 
进一步 的 文本 预处理 df _ all = pd . 
concat df _ train df _ test axis = 0 
ignore _ index = True df _ all . head 
df _ all = pd . merge df _ all 
df _ desc how = left on = product _ 
uid df _ all . head Step 2 文本 预处理 
我们 这里 遇到 的 文本 预处理 比较简单 因为 最 主要 
的 就是 看 关键词 是否 会被 包含 所以 我们 统一化 
我们 的 文本 内容 以 达到 任何 term 在 我们 
的 数据 集中 只有 一种 表达式 的 效果 我们 这里 
用 简单 的 Stem 做个 例子 有兴趣 再选 用 其他 
预 处理方式 去掉 停止词 纠正 拼写 去掉 数字 去掉 各种 
emoji 等等 stemmer = SnowballStemmer english def str _ stemmer 
s return . join stemmer . stem word for word 
in s . lower . split def str _ common 
_ word str1 str2 return sum int str2 . find 
word = 0 for word in str1 . split df 
_ all search _ term = df _ all search 
_ term . map lambda x str _ stemmer x 
df _ all product _ title = df _ all 
product _ title . map lambda x str _ stemmer 
x df _ all product _ description = df _ 
all product _ description . map lambda x str _ 
stemmer x Step 3 自制 文本 特征 一般 属于 一种 
脑 洞 大开 的 过程 想到 什么 可以 加 什么 
当然 特征 也 不是 越 丰富 越好 稍微 靠谱 点 
是 肯定 的 关键词 的 长度 df _ all len 
_ of _ query = df _ all search _ 
term . map lambda x len x . split . 
astype np . int64 描述 中 有 多少 关键词 重合 
df _ all commons _ in _ desc = df 
_ all . apply lambda x str _ common _ 
word x search _ term x product _ description axis 
= 1 等等等等 变着 法子 想出 些 数字 能 代表 
的 features 一股脑 放进来 ~ 搞完 之后 我们 把 不能 
被 机器学习 模型 处理 的 column 给 drop 掉 df 
_ all = df _ all . drop search _ 
term product _ title product _ description axis = 1 
Step 4 重塑 训练 / 测试 集 舒淇 说得好 要把 
之前 脱下 的 衣服 再 一件件 穿 回来 数据 处理 
也 是 如此 搞完 一圈 预处理 之后 我们 让 数据 
重回 原本 的 样貌 分开 训练 和 测试 集 df 
_ train = df _ all . loc df _ 
train . index df _ test = df _ all 
. loc df _ test . index 记录 下 测试 
集 的 id     留着 上传 的 时候 能对 
的 上 号 test _ ids = df _ test 
id 分离出 y _ trainy _ train = df _ 
train relevance . values 把 原 集中 的 label 给 
删去   否则 就是 cheating 了 X _ train = 
df _ train . drop id relevance axis = 1 
. values X _ test = df _ test . 
drop id relevance axis = 1 . valuesStep 5 建立 
模型 我们 用 个 最简单 的 模型 Ridge 回归模型 from 
sklearn . ensemble import R a n d o m 
F o r e s t R e g r 
e s s o r from sklearn . model _ 
selection import cross _ val _ score 用 CV 结果 
保证 公正 客观性 并 调试 不同 的 alpha 值 params 
= 1 3 5 6 7 8 9 10 test 
_ scores = for param in params clf = R 
a n d o m F o r e s 
t R e g r e s s o r 
n _ estimators = 30 max _ depth = param 
test _ score = np . sqrt cross _ val 
_ score clf X _ train y _ train cv 
= 5 scoring = neg _ mean _ squared _ 
error test _ scores . append np . mean test 
_ score 画 个 图 来 看看 import matplotlib . 
pyplot as plt % matplotlib inline plt . plot params 
test _ scores plt . title Param vs CV Error 
大概 6 ~ 7 的 时候 达到 了 最优 解 
Step 6 上传 结果 用 我们 测试 出 的 最优 
解 建立 模型 并 跑跑 测试 集 rf = R 
a n d o m F o r e s 
t R e g r e s s o r 
n _ estimators = 30 max _ depth = 6 
rf . fit X _ train y _ train R 
a n d o m F o r e s 
t R e g r e s s o r 
bootstrap = True criterion = mse max _ depth = 
6 max _ features = auto max _ leaf _ 
nodes = None min _ impurity _ split = 1e 
07 min _ samples _ leaf = 1 min _ 
samples _ split = 2 min _ weight _ fraction 
_ leaf = 0.0 n _ estimators = 30 n 
_ jobs = 1 oob _ score = False random 
_ state = None verbose = 0 warm _ start 
= False y _ pred = rf . predict X 
_ test pd . DataFrame { id test _ ids 
relevance y _ pred } . to _ csv submission 
. csv index = False 总结 虽然 都是 用 的 
最简单 的 方法 但是 基本 框架 是 很 完整 的 
后续 可以 尝试 修改 / 调试 / 升级 的 部分 
是 文本 预处理 步骤 你 可以 使 用 很多 不同 
的 方法 来 使得 文本 数据 变得 更加 清洁 自制 
的 特征 相处 更多 的 特征值 表达方法 关键词 全段 重合 
数量 重合 比率 等等 更好 的 回归模型 根据 之前 的 
课 讲 的 Ensemble 方法 把 分类器 提升 到 极致 
论 人工智能 背后 的 伦理 问题 人工智能 Artificial Intelligence 英文 
缩写 为 AI 它 是 研究 开发 用于 模拟 延伸 
和 扩展 人 的 智能 的 理论 方法 技术 及 
应用 系统 的 一门 新的 技术 科学 人工智能 是 计算机 
科学 的 一个 分支 它 企图 了解 智能 的 实质 
并 生产 出 一种 新的 能以 人类 智能 相似 的 
方式 做出 反应 的 智能 机器 该 领域 的 研究 
包括 机器人 语言识别 图像识别 自然语言 处理 和 专家 系统 等 
人工智能 从 诞生 以来 理论 和 技术 日益 成熟 应用 
领域 也 不断 扩大 可以 设想 未来 人工智能 带来 的 
科技 产品 将会 是 人类 智慧 的 容器 人工智能 可以 
对人 的 意识 思维 的 信息 过程 的 模拟 人工智能 
不是人 的 智能 但能 像人 那样 思考 也 可能 超 
过人 的 智能 人工智能 会有 感情 吗 是的 kismet 还是 
麻省理工学院 研制出 的 世界 上 第一 个 有 感情 的 
机器人 情感 和 社交 技能 对于 一个 智能 AGENT 是 
很 重要 的 首先 通过/p 了/ul 解/v 他们/r 的/uj 动机/n 
和/c 情感/n 状态/n 代理人 能够 预测 别人 的 行动 这 
涉及 要素 博弈论 决策 理论 以及 能够 塑造 人 的 
情感 和 情绪 感知 能力 检测 此外 为 了 良好 
的 人机 互动 智慧 代理人 也 需要 表现 出 情绪 
来 至少 它 必须 出现 礼貌 地 和 人类 打交道 
至少 它 本身 应该 有 正常 的 情绪 如果 人工智能 
拥有 了 情感 它 是否 会 吸引 人类 呢 人类 
与 人工智能 之间 的 爱情 又 符合 伦理道德 吗 很多 
爱恋 都是 从 朋友 发展 而来 的 试想 一下 如果/c 
你/r 每天/r 都能/nr 接触/v 到/v 你/r 的/uj 好朋友/i 他 懂 
你 他 关心 你 他 幽默 风趣 他 带给 你 
无限 的 正能量 和 快乐 他 让 你 变得 积极 
这样 的 关系 发展 下去 真的 就 很容易 发展 成 
爱恋 吧 特别 是 当 你 有 需要 的 时候 
又 抑或 是 闺蜜 兄弟 死党 即使 对方 不是 传统 
意义 上 的 生物 人 这样的话 又 牵涉 到 伦理 
问题 传统 意义 上 的 爱情 是 指 是 人与 
人 之间 的 强列的/nr 依恋 亲近 向往 以及 无私 专一 
并且 无所 不尽 的 情感 还有 向往 未来 的 生活 
现代科学 解析 爱情 是 通过 激素 作用 的 生物 程序 
有关 爱情 的 行为 都 是由 进化 的 力量 主导 
通过 激素 起作用 所有 疯狂 的 行为 只 为了 把 
基因 传递 给 后代 以此 为 标准 人机 恋爱 是 
违背 常理 的 影视 作品 中 也有 类似 的 桥段 
人机 相恋 的 结局 有好有坏 我们 当然 应该 思考 如何 
规避 风险 应对 未来 这 一天 的 到来 西部 世界 
里 机器人 向 人类 开枪 她 讲述 了 作家 西奥多 
在 结束 了 一段 令 他 心碎 的 爱情 长跑 
之后 他 爱上 了 电脑 操作 系统 里 的 女声 
这个 叫 萨曼莎 的 姑娘 不仅 有着/nr 一把 略微 沙哑 
的 性感 嗓音 并且 风趣 幽默 善解人意 让 孤独 的 
男 主 泥 足 深陷 机械 姬 讲述 了 老板 
邀请 员工 到 别墅 进行 对 智能 机器人 进行 图灵测试 
的 故事 最终 机械 姬 逃出 实验室 混入 人类 社会 
人工智能 伦理 和 监管 基金 MIT 媒体 实验室 MIT Media 
Lab 和 哈佛 大学 伯克曼 克莱恩 互联网 及 社会 研究 
中心 Berkman Klein Center 共同 成为 了 人工智能 伦理 和 
监管 基金 的 管理 机构 该 基金 将 用于 解决 
人工智能 所 带来 的 人文 及 道德 问题 我们 了解到 
除了 LinkedIn 创始人 Reid Hoffman eBay 创始人 Pierre Omidya 的 
非盈利 组织 Omidyar Network 各 捐出 的 1000 万美金 之外 
Knight 基金会 也 捐助 了 500 万美元 此外 还有 William 
and Flora Hewlett 基金会 和 Raptor 集团 创始人 Jim Pallotta 
各 捐出 了 100 万美元 这笔/r 总计/n 2700/m 万/m 美元/q 
的/uj 基金/n 将由/i MIT/w 媒体/n 实验室/n 和/c 哈佛/l 大学/n Berkman/w 
Klein 中心 共同 管理 将来 也会 有 更多 的 捐款者 
加入 基金 的 规模 还 将 扩大 人工智能 将 影响 
我们 每 一个 地球人 对 社会 的 各个 领域 意义 
重大 Knight 基金会 CEO Alberto Ibarg ü en 表示 尽管 
人工智能 帮助 人类 干了/nr 不少 事儿 比如 用 自动 驾驶 
技术 来 减少 大量 的 交通 事故 扫描 医学影像 来 
发现 癌症 等等 但是 有 不少 业界 人士 也 在 
发出 警告 称 AI 很 有可能 在 给 人类 带来 
帮助 的 同时 也 带给 人 不小 的 威胁 人工智能 
系统 能 分析 大量 数据 但 其中 也 可能 存在 
偏见 AI 的 飞速 发展 带来 了 很多 严峻 的 
挑战 麻省理工学院 媒体 实验室 主任 伊藤 穰 一 Joi Ito 
解释 道 人工智能 给 社会 以及 人类 带来 了 不可 
忽视 的 影响 机器 变得 越来越 智能 以至于 有 时候 
我们 会 担心 它们 是否 会 脱离 我们 的 控制 
其中 最 关键 的 挑战 之一 就是 我们 如何 确保 
自己 培训 的 机器 永远 不会 产生 并且 放大 人类 
的 偏见 以此 来 困扰 社会 我们 该 如何 针对 
这项 技术 展开 更 广泛 深入 的 讨论 社会 该 
如何 与 AI 共同 演变 人们 如何 将 计算机 科学 
和 社会 科学 连接 在 一起 开发 出 不仅 聪明 
更 重要 的 是 对 社会 负责 的 智能 机器 
人工智能 伦理 和 监管 基金 将 用于 研究 AI 应当 
如何 承担 社会 责任 例如 教育 交通 运输 和 司法 
等 领域 的 计算机 程序 怎样 确保 公平 性 问题 
该 基金 还 希望 探索 出 AI 是以 何种 方式 
与 公众 展开 对话 的 帮助 公众 理解 AI 的 
复杂性 和 多样性 此外 建立 这项 基金 的 必要性 在于 
它 将会 跨越 学科 之间 的 障碍 打破 不同 领域 
的 孤岛 MIT 媒体 实验室 哈佛 Berkman Klein 中心 以及 
其他 潜在 合作者 将 作为 一种 共同 机制 加强 基金 
组织 的 跨学科 工作 并 鼓励 交叉 学科 的 并行 
对话 和 协作 据 了解 该 组织 预计 在 未来 
数年 以 阶 段式 来 达成 目标 比如 在 MIT 
媒体 实验室 原 定于 7月 10日 举办 的 AI 研讨会 
上将 进行 一定 的 补充 此外 该 基金 还 将 
监督 AI 奖学金 计划 对 一些 相关 的 合作 项目 
进行 支持 引导 AI 向 利于 社会 的 方向 发展 
协作型 网络 MIT 媒体 实验室 和 哈佛 Berkman Klein 中心 
在 网络 社区 以及 多 学科 人才 的 帮助 下 
使用/v 机器学习/i 从/p 数据/n 中/f 学习/v 伦理/ns 和/c 法律/n 规范/n 
并且 使用 数据 驱动 技术 来 量化 人工智能 对 劳动力 
市场 的 潜在 影响 据悉 这项 工作 已经 在 两个 
机构 中 开始 进行 其中 一个 是 自动 驾驶 汽车 
的 合作 小组 由 Iyad Rahwan 领导 旨在 讨论 汽车 
中 相关 的 AI 道德 复杂性 另 一个 是 机器 
人 小组 由 Cynthia Breazeal 领导 旨在 研究 人类 与 
机器人 互动 过程 中 所 涉及 的 道德 问题 正如 
18 19 世纪 工业革命 对 整个 世界 产生 的 影响 
那样 AI 的 影响力 也 是 类似 的 Rahwan 表示 
他/r 将/d 运输/vn 系统/n 和/c 就业/v 列为/v 最/d 可能/v 受到/v 
自动化/l 和/c 人工智能/n 影响/vn 的/uj 领域/n 之一/r 我们 需要 的 
是 让 整个 社会 处在 这些 系统 的 控制 环路 
中 包括 技术 专家 工程师 伦理学家 认知 科学家 经济学家 法律 
学者 人类学家 信仰 领袖 政府 监管者 等等 以 达到 公众 
效益 Breazeal 表示 人工智能 为 所有 年龄 和 阶段 的 
人 提供 了 深度 个性化 的 学习 体验 她 强调 
人工智能 需要 接触 发展 中 国家 和 少数派 人群 同时 
她 补充 到 人工智能 也是 一把 双刃剑 这种 技术 该 
如何 惠及 大众 又 该 如何 保护 人类 的 隐私 
和 安全 这些 问题 都 需要 慢慢 思考 共同 的 
目标 哈佛大学 Berkman Klein 中心 一直 致力于 研究 以 公共 
利益 导向 的 解决方案 比如 知识 共享 和 美国 数字 
公共 图书馆 此外 目前 它 还和 MIT 媒体 实验室 合作 
将/d 一/m 部分/n 高/a 水平/n 的/uj 开发/v 人员/n 和/c 高科技/nz 
行业/n 的/uj 专业/n 人士/n 汇集/v 在/p 哈佛/l 大学/n 进行/v 为期/r 
三周/m 的/uj 严格/ad 培训/vn 随后 还 将 展开 12周 的 
协同 研究 让 各路 人才 共同 探索 网络安全 问题 哈佛大学 
Berkman Klein 中心 的 联合 创始人 兼 计算机 科学 教授 
Jonathan Zittrain 表示 这些 研究 会 帮助 我们 判断 AI 
事业 更深 层次 的 目标 虽然 有时 人工智能 让人 担忧 
但/c 是/v 人类/n 应该/v 做/v 的/uj 工作/vn 是/v 确定/v 和/c 
培养/v 自身/r 在/p 技术/n 面前/f 的/uj 自主性/n 和/c 尊严/a 而 
不是 去 削弱 技术 据 了解 人工智能 伦理 和 监管 
基金 将由 一个 小型 董事会 进行 管理 此外 董事会 在 
选举 出 一组 专家 顾问 其中 包括 Daniela Rus Andrew 
以及 Max Tegmark 等 MIT CSAIL 实验室 的 大牛 麻省理工学院 
媒体 实验室 主任 伊藤 穰 一 表示 现在 对 各个 
领域 进行 覆盖 是 至关重要 的 我们 决定 创建 一个 
动态 网络 而 不是 建立 一个 机构 解决 问题 最好 
的 方法 就是 展开 跨学科 交叉 领域 的 研究 我们 
这个 项目 只是 一个 开始 前言 关毅 老师 现为 哈工大 
计算机 学院 语言 技术 中心 教授 博士生 导师 通过 认真 
学习 了 自然语言 处理 哈工大 关毅 64集 视频 1 来自 
互联网 的 课程 受益 良多 在此 感谢 关毅 老师 的 
辛勤 工作 为 进一步 深入 理解 课程内容 对 部分 内容 
进行 了 延伸 学习 2 3 在此 分享 期待 对 
大家 有所 帮助 欢迎 加 我 微信 验证 NLP 一起 
学习 讨论 不足之处 欢迎 指正 视频 列表 01 自然语言 处理 
绪论 一 02 自然语言 处理 绪论 二 03 自然语言 处理 
绪论 三 04 自然语言 处理 绪论 四 05 自然语言 处理 
绪论 五 06 自然语言 处理 概论 六 07 自然语言 处理 
概论 七 08 自然语言 处理 概论 八 09 自然语言 处理 
概论 九 哈工大 LTP 开始使用 01 自然语言 处理 绪论 一 
第一 章 自然语言 处理 概论 自然语言 处理 团队 自然语言 处理 
的 重要 应用 机器翻译 数据库 技术 语音识别 主要 研究室 语言 
技术 研究中心 语音 处理 实验室 关毅 老师 主要 工程 项目 
微软拼音 输入法 BOPOMOFO 汉字 输入 系统 关毅 老师 主要 科学 
贡献 关于 相似 的 研究 提出 系统 相似 度 测度 
的 理论模型 02 自然语言 处理 绪论 二 简史 崛起 于 
80 年代初 的 统计 自然语言 处理 技术 已经 成为 自然 
语言 处理 的 主流 技术 本 课程 重点 介绍 统计 
语言 处理 技术 特别 是 基于 统计 的 汉语 词法 
分析 技术 自然语言 处理 技术 起源 于 人们 对 机器 
翻译 技术 的 研究 从 1946年 算起 至今 已有 60 
多年 的 历史 了 目前 一些 试 用过 的 用户 
表示 改进 后的/nr 翻译 服务 在 质量 方面 令人 惊讶 
对于 那些 从 未使用 机器 翻译 的 用户 来说 他们 
完全 可以 通过 翻译 后的/nr 文本 理解 原文 的 意思 
一些 细微 的 错误 并 不会 引起 太大 的 麻烦 
Franz Josef Och 董 振东 JSCL 2005 拿 统计 机器翻译 
说 事 还 需要 多少 年 才能 实现 计算机 与 
人类 无障碍 地 沟通 1968年 的 美国 影片 2001 太空 
奥德赛 机器人 HAL 和 Dave 进行 了 如下 对话 Dave 
Bownman Open the pod bay doors HAL . Dave Bownman 
HAL 请 你 打开 太空舱 的 分离 门 HAL I 
m sorry Dave I am afraid I can t do 
that . HAL 对不起 Dave 我 恐怕 不能 这样 做 
自然语言 处理 是 一个 AI complete 问题 自然 语言 理解 
与 计算 分子生物学 有着 深刻 的 渊源 生物学 中 有着 
至少 500年 也 解决 不完 的 有趣 问题 Donald E 
. Knuth 自然 语言 与 人工 语言 的 最大 区别 
在于 歧义 问题 03 自然语言 处理 绪论 三 歧义 问 
题词 法分析 歧义 分词 严守一 把 手机 关了 严守 / 
一把手 / 机关 / 了 严守一 / 把 / 手机 
/ 关//nr 了 词性 标注 我 / pro 计划 / 
v 考 / v 研 / n 我 / pro 
完成 / v 了 / aux 计划 / n 语法分析 
歧义 咬死 了 猎人 的 狗 那只 狼 咬死 了 
猎人 的 狗 咬死 了 猎人 的 狗 失踪 了 
语义分析 歧义 At last a computer that understands you like 
your mother . – 1985 McDonnell Douglas ad 含义 1 
计算机 会象 你 的 母亲 那样 很好 地 理解 你 
的 语言 含义 2 计算机 理解 你 喜欢 你 的 
母亲 含义 3 计算机 会象 很好 地 理解 你 的 
母亲 那样 理解 你 语义 用 分析 歧义 你 真坏 
音 字 转换 例 ji qi fan yi ji qi 
ying yong ji qi le ren men ji qi nong 
hou de xing qu 几点 感性认识 有点 繁琐 枯燥 充满 
乐趣 团队 合作 独创 精神 一只 美丽 的 小花猫 04 
自然语言 处理 绪论 四 名言 取法 其上 仅 得 其中 
取法 其中 仅 得 其下 取法 其 众 得 其上 
中国 古代 思想家 Every important idea is simple 列夫 . 
托尔斯泰 The grand aim of all science is to cover 
the greatest number of empirical facts by logical deduction from 
the smallest number of hypotheses or axioms 爱因斯坦 工具 Mindjet 
MindManager ver 8.0 Biblioscape ver 7.0 教材 Christopher Manning and 
Hinrich Schutze Foundations of Statistical Language Processing MIT press 1999 
有 中译本 译者 苑 春 法 等 自然语言 处理 综论 
Daniel Jurafsky & James H . Martin 著 冯志伟 孙乐/nr 
译 王晓龙 关毅 计算机 自然语言 处理 清华大学出版社 2005年 什么 是 
自然 语言 处理 定义 1 研究 在 人 与人 交际 
中 以及 在 人与 计算机 交际 中的 语言 问题 的 
一门 学科 自然语言 处理 要 研制 表示 语言 能力 和 
语言 应用 的 模型 建立 计算 框架 来 实现 这样 
的 语言 模型 提出 相应 的 方法 来 不断 地 
完善 这样 的 语言 模型 根据 这样 的 语言 模型 
设计 各种 实用 系统 并 探讨 这些 实用 系统 的 
评测 技术 Bill Manaris 本 学科 的 主题 与 背景 
自然语言 处理 可以 定义 为 研究 在 人 与人 交际 
中 以及 在 人与 计算机 交际 中的 语言 问题 的 
一门 学科 人人 交际 中 的 语言 问题 例如 语言 
不通 的 问题 促进 了 机器 翻译 这一 语言 处理 
中 最重要 的 应用 之一 的 发展 人机 交际 中的 
语言 问题 例如 语言 文字 的 输入 输 出问题 促进 
了 智能化 人机接口 技术 的 研究 研究 自然语言 处理 的 
意义 从 科学 研究 的 角度 探寻 人类 通过 语言 
来 交互 信息 的 奥秘 更好 地 理解 语言 本身 
的 内在 规律 从 实际 应用 的 角度 构建 更加 
有效 的 人机 交互方式 05 自然语言 处理 绪论 五 两类 
不同 的 语言 处理 模型 能力 模型 基于 语言学 规则 
的 模型 建模 步骤 语言学 知识 形式化 形式化 规则 算法 
化 算法 实现 应用 模型 根据 不同 的 语言 处理 
应用 而 建立 的 特定 语言 模型 通常 是 基于 
统计 的 模型 又称 经验主义 的 语言 模型 上世纪 80 
年代 崛起 的 统计 自然语言 模型 可以 归入 建模 步骤 
大规模 真实 语料库 中 获得 语言 各级 语言 单位 上 
的 统计 信息 依据 较 低级语言 单位 上 的 统计 
信息 运用 相关 的 统计 推理 技术 计算 较 高级语言 
单位 上 的 统计 信息 用 人工 智能 等 相应 
的 方法 来 不断 地 完善 这样 的 语言 模型 
规则 与 统计 相结合 评测 技术 自然语言 处理 的 重要 
研究 专题 之一 国际 公认 的 自然 语言 研究 竞技场 
SighanConllTREC 什么 是 自然 语言 处理 定义 2 是 人工 
智能 和 语言学 的 交叉 学科 研究 自然 语言 的 
自动 生成 与 理解 图灵 实验 让 机器 模 仿人 
来 回答 某些 问题 通过 实验 和 观察 来 判断 
机器 是否 具备 智能 为 人工智能 确定 了 奋斗 的 
目标 并 指明 了 前进 的 方向 人工智能 自 诞生 
之日起 就和 自然语言 理解 结下 了 不解之缘 06 自然语言 处理 
概论 六 交叉性 学科 自然语言 处理 是 人工智能 的 重要 
分支 也是 应用 语言学 的 分支 语言学 计算机科学 数学 心理学 
信息论 中文信息处理 中文 语言 处理 计算 语言学 自然语言 理解 07 
自然语言 处理 概论 七 知识 内容 基础 应用 资源 评测 
基础 内容 音位学 描述 音位 的 结合 规律 说明 音位 
怎样 形成 语素 举例 delete file x dilet # fail 
# eks 形态学 描述 语素 的 结合 规律 说明 语素 
怎样 形成 单词 举例 dilet # fail # eks delete 
file x 词汇学 描述 词汇 系统 的 规律 说明 单词 
本身 固有 的 语义 特性 和 语法 特性 举例 句 
法学 描述 单词 或 词组 之间 的 结构 规则 说明 
单词 或 词组 怎样 构成 句子 举例 语义学 描述 句子 
中 各个 成分 之间 的 语义 关系 怎样 从 构成 
句子 的 各个 成分 推导 出 整个 句子 的 语义 
举例 语用学 描述 与 情景 有关 的 情景 语义 说明 
怎样 推导 出 句子 具有 的 与 周围 话语 有关 
的 各种 涵义 举例 delete file x rm i x 
应用 系统 常用 的 中文 资源 北京大学 人民日报 语料库 现代 
汉语语法 信息 词典 概念 层次 网络 知网 评测 内容 评测 
方法 评测 量度 08 自然语言 处理 概论 八/m 中文/nz 语言/n 
处理/v 的/uj 发展/vn 概况/n 从/p 汉字信息处理/n 到/v 汉语/nz 信息处理/n 汉字信息处理/n 
已经/d 基本/n 解决/v 汉语/nz 信息处理/n 遭遇/n 瓶颈/n 从/p 单机/n 信息/n 
处理/v 到/v 网络/n 信息/n 处理/v 单机/n 信息/n 处理/v 系统/n 网络/n 
信息/n 处理/v 系统/n 汉字/nz 排版系统/n 词/n 处理/v 词/n 是/v 自然/d 
语言/n 中/f 最小/a 的/uj 有/v 意义/n 的/uj 构成/v 单位/n 研究 
内容 分词 词性 标注 名 实体 识别 词义 消 歧 
等等 语句 处理 应用 音 字 转换 文本校对 语音合成 机器翻译 
篇章 处理 09 自然语言 处理 概论 九 中文 的 主要 
特点 汉语 是 大字符集 的 意 音 文字 汉 语词 
与 词 之间 没有 空格 汉语 的 同音词 较多 汉语 
没有 形态 变化 中文 的 主要 困难 汉语 的 语法 
研究 尚未 规范化 汉语 的 语言 学 知识 的 量化 
与 形式 化 工作 滞后 中文 语言 处理 研究 力量 
分散 中文 语言 处理 缺乏 规范 科学 的 评测 机制 
尚未 建立 自然语言 处理 的 主要 课题 基础理论 概率 与 
统计理论 统计 机器学习 理论 人工智能 基本 理论 认知科学 理论 词 
法分析 分词 词性 标注 命名 实体 识别 新词 发现 句法分析 
上下文 无关 文法 概率 语义分析/i 语义/n 表示/v 概念/n 语义/n 网络/n 
词义/n 消/v 歧/v 语用分析/n 自然语言/l 生/vn 成语/n 段/q 分析/vn 对话 
机器翻译 自然语言 处理 的 主要 应用 哈工大 LTP 开始使用 开始使用 
LTP 本文 实验 环境 为 64位 win7 系统 64位 python3 
. 5.2 哈工大 LTP 官方主页 http / / ltp . 
ai / 哈工大 LTP 使用 文档 https / / ltp 
. readthedocs . io / zh _ CN / latest 
/ install . htmlLTP 下载 1 LTP 项目 文件 ltp 
3 . 3.1 win x86 . ziphttps / / github 
. com / HIT SCIR / ltp / releases 2 
LTP 模型 文件 ltp _ data _ v 3.3 . 
1http / / pan . baidu . com / share 
/ link shareid = 1988562907 & uk = 2738088569 文件夹 
放置 1 新建 一个 项目 文件夹 C \ projects \ 
ltp 2 将 模型 文件 解 压后 的 ltp _ 
data 文件夹 放入 项目 文件夹 3 将 ltp 3 . 
3.1 win x86 . zip 解 压后 的 dll exe 
文件 全部 拷入 项目 文件夹 将 路径 C \ projects 
\ ltp 添加到 Windows 系统 环境变量 Path 中 Python 使用 
ltp _ testLTP 提供 的 模型 包括 在 ltp _ 
data 文件夹 cws . model 分句 模型 单 文件 pos 
. model 词性 标注 模型 单 文件 ner . model 
命名 实体 识别 模型 单 文件 parser . model 依存 
句法 分析模型 单 文件 srl _ data / 语义 角色 
标注 模型 多 文件 文件夹 srl ltp _ test 是 
一个 整合 LTP 中 各 模块 的 命令行 工具 它 
完成 加载 模型 依照 指定 方法 执行 分析 的 功能 
主要参数 线程数 最终 步骤 输入 文件 路径 模型 路径 词典 
路径 等 具体 可 通过 CMD 运行 ltp _ test 
. exe 查看 python 程序 简单 调用 test . txt 
# coding = utf 8 txtName = C \ \ 
projects \ \ ltp \ \ file \ \ test 
. txt f = open txtName w encoding = utf 
8 f . write 我 爱 北京 天安门 f . 
close 调用 ltp _ test # * coding utf 8 
* import os project _ path = C \ \ 
projects \ \ ltp \ \ # 项目 文件夹 目录 
# 可 设置 cws pos par ner _ cmdline 但是 
注意 各自 能用 的 参数 没有 的 参数 请 置 
空 model _ exe = ltp _ test # 又如 
cws _ cmdline threads _ num = threads + str 
2 # 更改 线程数 input _ path = input + 
C \ \ projects \ \ ltp \ \ file 
\ \ test . txt # 输入 文件 seg _ 
lexicon = # 分词 用户 词典 pos _ lexicon = 
# 词性 标注 用户 词典 output _ path = C 
\ \ projects \ \ ltp \ \ result \ 
\ out . txt # 输出 文件 command = cd 
+ project _ path + & + model _ exe 
+ threads _ num + input _ path + seg 
_ lexicon + + output _ path os . system 
command 运行 过程 out . txt xml version = 1.0 
encoding = utf 8 xml4nlp note sent = y word 
= y pos = y ne = y parser = 
y wsd = n srl = y / doc para 
id = 0 sent id = 0 cont = ﻿ 
我 爱 北京 天安门 word id = 0 cont = 
﻿ pos = v ne = O parent = 1 
relate = HED / word id = 1 cont = 
我 pos = r ne = O parent = 2 
relate = SBV / word id = 2 cont = 
爱 pos = v ne = O parent = 0 
relate = COO arg id = 0 type = 7 
& # x07 beg = 1 end = 1 / 
arg id = 1 type = beg = 3 end 
= 4 / / word word id = 3 cont 
= 北京 pos = ns ne = B Ns parent 
= 4 relate = ATT / word id = 4 
cont = 天安门 pos = ns ne = E Ns 
parent = 2 relate = VOB / word id = 
5 cont = pos = wp ne = O parent 
= 0 relate = WP / / sent / para 
/ doc / xml4nlp Python 使用 xxx _ cmdline 1 
cws _ cmdline 分词 命令行 2 pos _ cmdline 词性 
标注 命令行 3 par _ cmdline 句法分析 命令行 4 ner 
_ cmdline 命名 实体 识别 命令行 pyltp 使用 安装 pyltphttps 
/ / pyltp . readthedocs . io / zh _ 
CN / latest / 使用 pip 安装 pip install pyltp 
从 源代码 编译 安装 $ git clone https / / 
github . com / HIT SCIR / pyltp $ git 
submodule init $ git submodule update $ python setup . 
py install 注意 有时候 装 python 库 的 时候 会 
出现 Microsoft visual c + + 14.0 is required 的 
问题 欢迎 加 我 微信 验证 NLP 一起 研究 解决 
下载 LTP 模型 文件 当前 模型 版本 3 . 4.0 
使用 pyltp 进行 分句 示例 # * coding utf 8 
* from pyltp import SentenceSplitter sents = SentenceSplitter . split 
我 爱 北京 天安门 天安门 上 太阳升 # 分句 print 
\ n . join sents 结果 如下 我 爱 北京 
天安门 天安门 上 太阳升 参考文献 自然语言 处理 哈工大 关毅 64集 
视频 来自 互联网 ↩ ︎ 王晓龙 关毅 计算机 自然语言 处理 
清华大学出版社 2005年 ↩ ︎ 哈工大 语言 技术 平台 云 官网 
http / / ltp . ai / ↩ ︎ 目录 
文章 目录 目录 前言 课堂 总结 一 课堂 总结 二 
课堂 总结 三 课堂 总结 四 前言 硕士 生涯 结束 
开始 专心 做 一件 自己 觉得 有用 的 工具 先 
做 工程 后搞/nr 理论 自然语言 处理 是 一个 非常 难 
的 问题 同时 是 人工智能 皇冠 上 的 明珠 接下来 
会 记录 一 系列 自然语言 处理 的 笔记 来自 于 
哈工大 老师 关毅 课堂 总结 一 问答 系统 总结 数据 
层 搜索引擎 控制 信息采集 文本 分类 信息 索引 课堂 总结 
二 文本 分类 系统 新 的 结构 国家 863 项目 
处理 精度 鲁棒性 达到 相应 的 目标 新一代 学习 检索 
机制 持续 学习 的 能力 统计 词 法分析 外界 反馈 
来 学习 机制 研究 实体 信息 抽取 电子 病历 信息 
抽取 电子 健康 基于 最大熵 的 识别 系统 CRF 模型 
transfer learning 模型 句法分析 补偿 学习 增量 学习 主动 式 
学习 在线 学习 强化 学习 文本 聚 类 自组织 映射 
文 本系统 做 一个 有 人用 的 东西 课堂 总结 
三 把 自己 做 的 东西 的 应用 率 作为 
自己 的 追求 目标 忠实 的 fans 领域 知识 的 
自动 构建 单词 变体 缩略语 的 研究 难度 极大 非常 
有用 的 应用 mindmanager 推荐 思维导图 构成 结构 清晰 思路 
连贯 开始 回顾 语言 多类 文档 字处理 编码 输入输出 分词 
的 难点 every great idea is simple 频度 统计 很多 
工作 科学 的 定量 方法 语料库 的 多级 加工 n 
grams 语言 模型 课堂 总结 四 n gram 噪声 信道 
模型 平滑 平滑 的 原则 隐 码 句法 浅层 句法分析 
问题 总结 的话 规则 + 统计 结合 的 思想 一般性 
问题 和 特殊 性问题 语义 不能 这么 弄 分个 类 
1 抢占 高地 的 研究 先 做 式 2 解决 
问题 的 研究 Hownet 3 填补 空白 的 研究 成熟 
方法 + 新事物 处理 语义 信息 在 神经 中的 机制 
Tensorflow 自然语言 处理 331人 已 学习 课程 介绍 课程 以 
Tensorflow 作为 核心 武器 基于 自然语言 处理 热点 话题 进行 
案例 实战 选择 当下 热门 模型 使用 真实 数据 集 
进行 实战 演示 通俗 讲解 整个 算法 模型 并 使用 
tensorflow 进行 实战 详解 其中 的 原理 与 代码 实现 
课程 收益 掌握 如何 使用 Tensorflow 进行 自然语言 处理 任务 
实战 讲师 介绍 唐宇迪/nr 更多 讲师 课程 计算机 博士 专注 
于 机器学习 与 计算机 视觉 领域 深度 学习 领域 一线 
实战 讲师 在 图像 识别 领域 有着 丰富 经验 实现 
过 包括 人脸识别 物体 识别 关键点 检测 等 多种 应用 
的 新 算法 参与 多个 国家 级计算机 视觉 项目 多年 
数据 领域 培训 经验 丰富 的 教学 讲解 经验 出品 
多套 机器学习 与 深度 学习 系列 课程 课程 生动 形象 
风格 通俗易懂 课程 大纲 第 1 章 词 向量 模型 
1 . 词 向量 任务 简介     10 362 
. 数据源 制作     6 343 . 输入 数据 
制作     12 244 . 模型 整体 架构   
  12 365 . 模型 结果     6 19 
第 2 章 相似 度 判别 1 . 相似 度 
分析模型     5 402 . 数据源 分析     
5 533 . 数据 输入 制作     8 414 
. 数据 前期 整理     7 495 . 整体 
模型 架构     11 246 . 单词 训练方法   
  9 197 . 整体 相似 度 判别     
5 52 第 3 章 聊天 机器人 构造 1 . 
对话 展示     8 272 . 数据 制作 与 
配置     12 553 . 输入 数据处理     
8 524 . 向量 层 与 输出 层     
9 185 . 编码 解码 网络     8 236 
. 整体 模型 实现     6 557 . 整体 
模型 概述     6 518 . 数据处理 与 架构 
模型     13 179 . 模型 实现     
8 47 大家 可以 点击 查看 详情 查看 我 的 
课程 人工智能 的 目标 推理 自动 学习 & 调度 机器学习 
自然语言 处理 计算机 视觉 机器人 通用 智能 人工智能 三大 阶段 
阶段 1 机器学习 智能系统 使用 一 系列 算法 从 经验 
中 进行 学习 阶段 2 机器 智能 机器 使用 的 
一系列 从 经验 中 进行 学习 的 高级 算法 例如 
深度 神经网络 人工智能 目前 处于 此 阶段 阶段 3 机器 
意识 不 需要 外部 数据 就 能从 经验 中 自学习 
image . png 人工智能 的 类型 ANI 狭义 人工智能 它 
包含 基础 的 角色 型 任务 比如 由 Siri Alexa 
这样 的 聊天 机器人 个人 助手 完成 的 任务 AGI 
通用 人工智能 通用 人工智能 包含 人类 水平 的 任务 它 
涉及 到 机器 的 持续 学习 ASI 强 人工智能 强 
人工智能 指代 比 人类 更 聪明 的 机器 什么 使得 
系统 智能化 image . png 自然语言 处理 | 知识 表示 
| 自动 推理 | 机器学习 什么 是 自然 语言 处理 
自然语言 处理 NLP 是 指 机器 理解 并 解释 人类 
paralyzes 写作 说话 方式 的 能力 NLP 的 目标 是 
让 计算机 ／ 机器 在 理解 语言上 像 人类 一样 
智能 最终 目标 是 弥补 人类 交流 自然语言 和 计算机 
理解 机器语言 之间 的 差距 image . png 下面 是 
三个 不同 等级 的 语言学 分析 句 法学 给定 文本 
的 哪 部分 是 语法 正确 的 语义学 给定 文本 
的 含义 是 什么 语用学 文本 的 目的 是 什么 
NLP 处理 语言 的 不同 方面 例如 音韵学 指代 语言 
中 发音 的 系统化 组织 词态 学 研究 单词 构成 
以 及 相互 之间 的 关系 NLP 中 理解 语义分析 
的 方法 分布式 它 利用 机器学习 和 深度 学习 的 
大规模 统计 策略 框架 式 句法 不同 但 语义 相同 
的 句子 在 数据结构 帧 中被 表示 为 程式化 情景 
理论 式 这种方法 基于 的 思路 是 句子 指代 的 
真正 的 词 结合 句子 的 部分 内容 可 表达 
全部 含义 交互式 学习 它 涉及 到 语用 方法 在 
交互式 学习 环境 中 用户 教 计算机 一步 一步 学习 
语言 为什么 需要 NLP 有了 NLP 有 可能 完成 自动 
语音 自动 文本 编写 这样 的 任务 由于 大型 数据 
文本 的 存在 我们 为什么 不 使用 计算机 的 能力 
不知 疲倦 地 运行 算法 来 完成 这样 的 任务 
花费 的 时间 也 更少 这些 任务 包括 NLP 的 
其他 应用 比如 自动 摘要 生成 给定 文本 的 总结 
和 机器 翻译 NLP 流程 如果 要 用 语音 产生 
文本 需要 完成 ASR 任务 NLP 的 机制 涉及 两个 
流程 自然语言 理解 自然语言 生成 自然语言 理解 NLU NLU 是 
要 理解 给定 文本 的 含义 本 内 每个 单词 
的 特性 与 结构 需要 被 理解 在 理解 结构上 
NLU 要 理解 自然 语言 中 的 以下 几个 歧义 
性 词法 歧义 性 单词 有 多重 含义 句法 歧义 
性 语句 有 多重 解析 树 语义 歧义 性 句子 
有 多重 含义 回 指 歧义 性 Anaphoric Ambiguity 之前 
提到 的 短语 或 单词 在后面 句子 中 有 不同 
的 含义 接下来 通过 使用 词汇 和 语法 规则 理解 
每个 单词 的 含义 然而 有些 词 有 类似 的 
含义 同义词 有些 词 有 多重 含义 多义词 自然语言 生成 
NLG NLG 是从 结构化 数据 中 以 可读 地 方式 
自动 生成 文本 的 过程 难以 处理 是 自然 语言 
生成 的 主要 问题 自然语言 生成 可被 分为 三 个 
阶段 文本 规划 完成 结构化 数据 中 基础 内容 的 
规划 语句 规划 从 结构化 数据 中 组合 语句 来 
表达 信息流 实现 产生 语法 通顺 的 语句 来 表达 
文本 NLP 与 文本 挖掘 或 文本 分析 之间 的 
不同 自然 语言 处理 是 理解 给定 文本 的 含义 
与 结构 的 流程 文本 挖掘 或 文本 分析 是 
通过 模式识别 提起 文本 数据 中 隐藏 的 信息 的 
流程 自然语言 处理 被 用来 理解 给定 文本 数据 的 
含义 语义 而 文本 挖掘 被 用来 理解 给定 文本 
数据 的 结构 句法 image . png 例如 在 I 
found my wallet near the bank 一 句中 NLP 的 
任务 是 理解 句 尾 「 bank 」 一 词 
指代 的 是 银行 还是 河边 大 数据 中的 NLP 
The next Big Thing 如今 所 有 数据 中的 80% 
都 可被 用到 大 数据 来自 于 大公司 企业所 存储 
的 信息 例如 职员 信息 公司 采购 销售 记录 经济 
业务 以及 公司 社交 媒体 的 历史 记录 等 尽管 
人类 使用 的 语言 对 计算机 而言 是 模糊 的 
非 结构化 的 但 有了 NLP 的 帮助 我们 可以 
解析 这些 大型 的 非 结构化 数据 中 的 模式 
从而 更好 地 理解 里面 包含 的 信息 NLP 可 
使用 大 数据 解决 商业 中的 难题 比如 零售 医疗 
金融 领域 中 的 业务 聊天 机器 人 聊天 机器人 
或 自动 智能 代理 指代 你 能 通过 聊天 app 
聊天 窗口 或 语音 唤醒 app 进行 交流 的 计算机 
程序 也有 被 用来 解决 客户 问题 的 智能 数字化 
助手 成本低 高效 且 持续 工作 聊天 机器人 的 重要性 
聊天 机器人 对 理解 数字化 客服 和 频繁 咨询 的 
常规 问答 领域 中 的 变化 至关重要 聊天 机器人 在 
一些 领域 中 的 特定 场景 中 非常 有 帮助 
特别 是 会被 频繁 问到 高度 可 预测 的 的 
问题 时 聊天 机器人 的 工作 机制 image . png 
基于 知识 包含 信息库 根据 客户 的 问题 回应 信息 
数据 存储 包含 与 用户 交流 的 历史 信息 NLP 
层 它 将 用户 的 问题 任何 形式 转译 为 
信息 从而 作为 合适 的 回应 应用层 指 用来 与 
用户 交互 的 应用 接口 聊天 机器 人 每次 与 
用户 交流 时 都能 进行 学习 使用 机器学习 回应 信息库 
中 的 信息 NLP 中 为什么 需要 深度 学习 它 
使用 基于 规则 的 方法 将 单词 表示 为 「 
one hot 」 编码 向量 传统 的 方法 注重 句法 
表征 而非 语义 表征 词 袋 分类 模型 不能够 分别 
特定 语境 image . png 深度 学习 的 三项 能力 
可 表达性 这一 能力 描述 了 机器 如何 能 近似 
通用 函数 可 训练 性 深度 学习 系统 学习 问题 
的 速度 与 能力 可 泛 化性 在 未 训练过 
的 数据 上 机器 做 预测 的 能力 在 深度 
学习 中 当然 也 要 考虑 其他 的 能力 比如 
可 解释性 模块性 可 迁移性 延迟 对抗 稳定性 安全 等 
但 以上 是 主要 的 几项 能力 NLP 中 深度 
学习 的 常见 任务 image . png 传统 NLP 和 
深度 学习 NLP 的 区别 image . png 日志 分析 
与 日志 挖掘 中的 NLP 什么 是 日志 不同 网络 
设备 或硬件 的 时序 信息 集合 表示 日志 日志 可直接 
存储 在 硬盘 文档 中 也可 作为 信息 流传 送到 
日志 收集器 日志 提供 维持 追踪 硬件 表现 参数 调整 
紧急事件 系统 修复 应用 和 架构 优化 的 过程 什么 
是 日志 分析 日志 分析 是从 日志 中 提取 信息 
的 过程 分析 信息 中的 句法 和 语义 解析 应用环境 
从而 比较 分析 不 同源 的 日志 文档 进行 异常 
检测 发现 关联性 什么 是 日志 挖掘 日志 挖掘 或 
日志 知识发现 是 提取 日志 中 模式 和 关联性 的 
过程 从而 挖掘 知识 预测 日志 中 的 异常 检测 
日志 分析 和 日志 挖掘 中 使用 到 的 技术 
下面 介绍 了 完成 日志 分析 的 不同 技术 模式识别 
将 日志 信息 与 模式 薄中的/nr 信息 进行 对比 从而 
过滤 信息 的 技术 标准化 日志 信息 的 标准化 是 
将 不同 的 信息 转换 为 同样 的 格式 当 
来自 不 同源 的 日志 信息 有 不同 的 术语 
但 含义 相 同时 需要 进行 标准化 分类 & 标签 
不同 日志 信息 的 分类 & 标签 涉及 到 对 
信息 的 排序 并用 不同 的 关键词 进行 标注 Artificial 
Ignorance 使用 机器学习 算法 抛弃 无用 日志 信息 的 技术 
它 也可 被 用来 检测 系统 异常 日志 分析 & 
日志 挖掘 中的 NLP 自然语言 处理 技术 被 普遍 用于 
日志 分析 和 日志 挖掘 词语切分 词干 提取 stemming 词形 
还原 lemmatization 解析 等 不同 技术 被 用来 将 日志 
信息 转换成 结构化 的 形式 一旦 日志 以 很好 的 
形式 组织 起来 日志/ns 分析/vn 和/c 日志/ns 挖掘/v 就能/i 提取/v 
信息/n 中/f 有用/v 的/uj 信息/n 和/c 知识/v 深度 自然语言 处理 
自然语言 处理 是 一个 复杂 的 领域 处于 人工智能 计算 
语言学 和 计算机 科学 的 交叉 领域 从 NLP 开始 
用户 需要 输入 一个 包含 已 写 文本 的 文件 
接着 应该 执行 以下 NLP 步骤 image . pngimage . 
png 语句 分割 在 给定 文本 中 辨识 语句 边界 
即 一个 语句 的 结束 和 另一个 语句 的 开始 
语句 通常 以 标点符号 「 . 」 结束 标记 化 
辨识 不同 的 词 数字 及 其他 标点符号 词干 提取 
将 一个 词 还原 为 词干 词性 标注 标出 语 
句中 每 一个 词 的 词性 比如 名词 或 副词 
语法分析 将 给定 文本 的 部分 按类 划分 命名 实体 
识别 找出 给定 文本 中的人物 地点 时间等 指代 消解 根据/p 
一个/m 语句/n 的/uj 前句/i 和后句/nr 界定/n 该/r 句中/i 给定/v 词/n 
之间/f 的/uj 关系/n NLP 的 其他 关键 应用领域 除了 在 
大 数据 日志 挖掘 及 分析 中 的 应用 NLP 
还有 一些 其他 主要 应用领域 尽管 NLP 不如 大 数据 
机器学习 听 起来 那么 火 但 我们 每天 都在/nr 使用 
它 自动 摘要 在 给定 输入 文本 的 情况 下 
摈弃 次要 信息 完成 文本 摘要 情感 分析 在 给定 
文本 中 预测 其 主题 比如 文本 中 是否 包含 
判断 观点或 评论 等 文本 分类 按照 其 领域 分 
类 不同 的 期刊 新闻报道 多 文档 分类 也是 可能 
的 文本 分类 的 一个 流行 示例 是 垃圾 电子邮件 
检测 基于 写作 风格 可 检测 作者姓名 信息提取 建议 电子邮件 
程序 自动 添加 事件 到 日历 image . png 参考 
https / / www . jiqizhixin . com / articles 
/ 2017 05 07 3 https / / www . 
xenonstack . com / blog / overview of artificial intelligence 
and role of natural language processing in big data 作者 
郭少悲/nr 链接 https / / www . jianshu . com 
/ p / b627cb31aab7 來 源 简书 著作权 归 作者 
所有 商业 转载 请 联系 作者 获得 授权 非商业 转载 
请 注明 出处 哈工大 语言 云 语言 技术 平台 云 
是以 哈工大 社会 计算 与 信息检索 研究 中心 研发 的 
语言 技术 平台 LTP 为基础 提供 高效 精准 的 中文 
自然语言 处理 云 服务 官网 http / / www . 
ltp cloud . com / 使用 python 调用 API 实验 
参考 文档 http / / www . ltp cloud . 
com / document / 1 . 注册 免费 注册 一个 
帐号 注册 网址 http / / www . ltp cloud/w 
./i com/w //i accounts/w //i register/w //i 注册/v 后/f 获取/v 
调用/vn 语言/n 云/ns 服务/vn 的/uj token/w 以及/c api/w _/i key/w 
新版 API 的 调用 认证 方式 目前 新 注册 用户 
将 获得 每月 20G 的 免费 流量 2 . Python 
程序 注 32位 python 2 . 7.11 64位 win7 系统 
1 简单 测试 句子 # * coding utf 8 * 
功能 哈工大 语言 云 使用 测试 时间 2016年 4月 9日 
13 45 24 import urllib2 url _ get _ base 
= http / / api . ltp cloud . com 
/ analysis / api _ key = * * * 
* * * * * 替换 为 自己 的 API 
_ KEY * * * * * * * * 
# 输入 注册 API _ KEY # 待 分析 的 
文本 text = 这 是 一个 测试 文本 format0 = 
xml # 结果 格式 有 xml json conll plain 不可 
改成 大写 pattern = ws # 指定 分析 模式 有 
ws pos ner dp sdp srl 和 all result = 
urllib2 . urlopen % sapi _ key = % s 
& text = % s & format = % s 
& pattern = % s % url _ get _ 
base api _ key text format0 pattern content = result 
. read . strip print content 2 本地 文本处理 # 
* coding utf 8 * 功能 哈工大 语言 云 使用 
测试 时间 2016年 4月 12日 19 56 11 import urllib2 
import codecs def ltp _ cloud par1 url _ get 
_ base = http / / api . ltp cloud 
. com / analysis / api _ key = * 
* * * * * * * * * * 
替换 为 自己 的 API _ KEY * * * 
* * * * * * * * # 用户注册/n 
语言/n 云/ns 服务/vn 后/f 获得/v 的/uj 认证/v 标识/n format0 = 
plain # 结果 格式 有 xml json conll plain 不可 
改成 大写 pattern = ws # 指定 分析 模式 有 
ws pos ner dp sdp srl 和 all result1 = 
urllib2 . urlopen % sapi _ key = % s 
& text = % s & format = % s 
& pattern = % s % url _ get _ 
base api _ key par1 format0 pattern return result1 . 
read . strip f = open r C \ Users 
\ lenovo \ Desktop \ test . txt r # 
待 分析 文本 已 分句 每行 一句 savef = codecs 
. open u C \ \ Users \ \ lenovo 
\ \ Desktop \ \ out1 . txt a utf 
8 # 结果 存储 linenum = 0 newline = for 
line in f linenum + = 1 # 记录 处理 
行数 newline + = line . strip . replace # 
# 删除 行 末 空白符 干扰 符号 以免 影响 URI 
if line 1 = \ n # 如果 处理 到 
文本 最后 一行 if and and in in newline print 
u 需要 更改 单词 in newline = newline . replace 
in i . n print u 已 处理 到 文本 
最后 一行 linenum savef . write ltp _ cloud newline 
. decode utf 8 + \ n if len newline 
6000 # 让 文本 足够 长时 再 提交 处理 最大值 
在 8000 左右 if and and in in newline # 
不能 同时 含有 and 和 in 两个 词 print u 
需要 更改 单词 in newline = newline . replace in 
i . n print u 处理 到 第 + str 
linenum + u 行 savef . write ltp _ cloud 
newline . decode utf 8 + \ n newline = 
savef . close f . close 说明 1 如果 是 
本地 文本 尽量 一次 提交 尽可能 多 的 文本 而 
不是 一句 一句 提交 以 提高 请求 效率 一次 提交 
的 文本 有 最大 长度 限制 在 UTF 8 编码 
下 单次 解析 的 文本 长度 大约 为 2700个 汉字 
8100 长度 2 提交 的 文本 中 不能 有 影响 
URI 构造 的 特殊 符号 目前 已知 的 干扰 符号 
有 # & + 四种 另外/c 不/d 知道/v 为什么/r 英文单词/nr 
and/w 和/c in/w 不能/v 同时/c 存在/v 于/p 提交/v 的/uj 文本/n 
中/f 3 上述 程序 中 读取 的 文本 是 已经 
分 好句 的 每行 一句 不过 语言 云 本身 提供 
分句 功能 因此 可以 直接 提交 没有 分句 的 文本 
其 分句 是 根据 中文 标点符号 五种 快速 了解 什么 
是 自然 语言 处理 摘要 自然语言 处理 是 计算机 科学 
领域 与 人工智能 领域 中 的 一个 重要 方向 它/r 
研究/vn 能/v 实现/v 人/n 与/p 计算机/n 之间/f 用/p 自然/d 语言/n 
进行/v 有效/a 通信/l 的/uj 各种/r 理论/n 和/c 方法/n 自然语言 处理 
是 一门 融 语言学 计算机科学 数学 等 于 一体 的 
科学 因此 这一 领域 的 研究 将 涉及 自然语言 即 
人们 日常 使用 的 语言 所以 它 与 语言学 的 
研究 有着 密切 的 联系 但又 有 重要 的 区别 
自然语言 处理 并 不是 一般 地 研究 自然语言 而在于 研制 
能 有效 地 实现 自然 语言 通信 的 计算机 系统 
特别 是 其中 的 软件 系统 因而 它 是 计算机 
科学 的 一部分 本文 原创 分享 供 于 学习 转载 
标明 出处 快速 了解 什么 是 自然 语言 处理 相关 
文章 文本处理 自然语言 处理 在 现实 生活 中 运用 文本处理 
多种 贝叶斯 模型 构建 及 文本 分类 的 实现 文本处理 
快速 了解 什么 是 自然 语言 处理 文本处理 领域 本体 
构建 方法 概述 文本 挖掘 1 OpenNLP 驾驭 文本 分词 
那些 事 文本 挖掘 2 NLP Tika 文本 预处理 抽取 
各种 格式文件 内容 文本 挖掘 3 自己动手 搭建 搜索 工具 
1 计算机 对 自然 语言 处理 的 过程 1.1把 需要 
研究 是 问题 在 语言 上 建立 形式化 模型 使其 
可以 数学 形式 表示 出来 这个 过程 称之为 形式化 1.2把 
数学模型 表示 为 算法 的 过程 称之为 算法 化 1.3 
根据 算法 计算机 进行 实现 建立 各种 自然语言 处理 系统 
这个 过程 是 程序化 1.4对 系统 进行 评测 和 改进 
最终 满足 现实 需求 这个 过程 是 实用化 2 自然语言 
处理 涉及 的 知识 领域 语言学 计算机科学 提供 模型表示 算法 
设计 计算机 实现 数学 数学模型 心理学 人类 言语 心理 模型 
和 理论 哲学 提供 人类 思维 和 语言 的 更深 
层次 理论 统计学 提供 样本数据 的 预测 统计技术 电子工程 信息论 
基础 和 语言 信号 处理 技术 生物学 人类 言语 行为 
机制 理论 故 其 为多 边缘 的 交叉 学科 3 
自然语言 处理 涉及 的 范围 3.1 语音 的 自动合成 与 
识别 机器翻译 自然语言 理解 人机对话 信息检索 文本 分类 自动 文摘 
等等 总之 分为 四 大方向 语言学 方向 数据处理 方向 人工智能 
和 认知科学 方向 语言工程 方向 3.2 也可 细分 为 13个 
方面 口语 输入 语音识别 信号 表示 鲁棒 的 语音 识别 
语音识别 中的 隐 马尔科夫 模型 方法 语言 模型 说 话人 
识别 口语 理解 书面语 输入 文献 格式 识别 光学 字符识别 
OCR 印刷体 识别 / 手写体 识别 手写 界面 手写 文字 
分析 语言 分析 理解 小于 句子 单位 的 处理 语法 
的 形式 化 针对 基于 约束 的 语法 编写 的 
词表 计算 语义学 句子 建模 和 剖析 技术 鲁棒 的 
剖析 技术 语言 生成 句法 生成 深层 生成 口语 输入 
技术 合成语音 技术 语音 合成 的 文本 解释 口语 生成 
话语分析 与 对话 对话 建模 话语 建模 口语 对话 系统 
文献 自动 处理 文献检索 文本 解释 信息 抽取 文本 内容 
自动 归纳 文本 写作 和 编辑 的 计算机 支持 工业 
和 企业 中 使用 的 受限 语言 多语 问题 的 
计算机 处理 机器翻译 人 助 机译 机 助人 译 多语言 
信息检索 多语言 语音识别 自动 语种 验证 多 模态 的 计算机 
处理 空间 和 时间 表示 方法 文本 与 图像 处理 
口语 与 手势 的 模态 结合 口语 与 面部 信息 
的 模态 结合 面部/f 运动/vn 和/c 语音/n 识别/v 信息/n 传输/vn 
和/c 信息/n 存储/l 语音 压缩 语音 品质 的 提升 自然语言 
处理 中 的 数学 方法 统计 建模 和 分类 的 
数学 理论 数字 信号 处理 技术 剖析 算法 的 数学 
基础 研究 神经网络 有限 状态 分析 技术 语音/n 和/c 语言/n 
处理/v 中的/i 最优化/v 技术/n 和/c 搜索/v 技术/n 语言/n 资源/n 书面 
语料库 口语 语料库 机器 词典 与 词 网 的 建设 
术语 编撰 和 术语 数据库 网络 数据 挖掘 和 信息 
提取 自然语言 处理 系统 的 评测 面向 任务 的 文本 
分析 评测 机器翻译 系统 和 翻译 工具 的 评测 大 
覆盖面 的 自然 语言 剖析器 的 评测 语音识别 评估 和 
评测 语音合成 评测 系统 的 可用性 和 界面 的 评测 
语音 通信 质量 的 评测 文字 识别 系统 的 评测 
4   自然语言 处理 的 发展 的 几个 特点 基于 
句法 语义 规则 的 理性主义 方法 受到 质疑 随着 语料库 
建设 和 语料库 语言学 的 崛起 大 规模 真实 文本 
的 处理 成为 自然 语言 处理 的 主要 战略 目标 
自然语言 处理 中 越来越 多 地 使用 机器 自动 学习 
的 方法 来 获取 语言 知识 统计 数学方法 越来越 受到 
重视 自然语言 处理 中 越来越 重视 词汇 的 作用 出现 
了 强烈 的 词汇 主义 的 倾向 自然语言 处理 中 
的 符号 表征 0 . Preface 自然语言 处理 中 的 
符号 表征 Author Cao ShengmingEmail caoshengming @ trio . aiCompany 
Trio 北京 三角 兽 科技 有限公司 0 . Preface 这 
部分 将 探讨 一下 自然语言 处理 中 的 符号 表征 
问题 自然语言 处理 五 传统 机器学习 1 . 朴素 贝叶斯 
的 原理 1.1 朴素 贝叶斯 相关 的 统计 学 知识 
1.2 基本 定义 2 . 利用 朴素 贝叶斯 模型 进行 
文本 分类 2.1 模型 原理 与 训练 3 . SVM 
的 原理 3.1 快速 理解 SVM 原理 4 . 利用 
SVM 模型 进行 文本 分类 5 . pLSA 共轭 先验 
分布 LDA 主题 模型 原理 6 . 使用 LDA 生成 
主题 特征 在 之前 特征 的 基础 上 加入 主题 
特征 进行 文本 分类 传统 机器学习 1 . 朴素 贝叶斯 
的 原理 1.1 朴素 贝叶斯 相关 的 统计 学 知识 
贝叶斯 学派 很 古老 但是/c 从/p 诞生/v 到/v 一百/m 年前/nr 
一直/d 不是/c 主流/b 主流 是 频率 学派 频率 学派 的 
权威 皮尔逊 和 费歇尔 都对 贝叶斯 学派 不屑一顾 但是 贝叶斯 
学派 硬是 凭借 在 现代 特定 领域 的 出色 应用 
表现 为 自己 赢得 了 半壁江山 贝叶斯 学派 的 思想 
可以 概括 为 先验概率 + 数据 = 后验/nr 概率 也 
就是说 我们 在 实际 问题 中 需要 得到 的 后验/nr 
概率 可以 通过 先验概率 和 数据 一起 综合 得到 数据 
大家 好 理解 被 频率 学派 攻击 的 是 先验概率 
一般来说 先验概率 就是 我们 对于 数据 所在 领域 的 历史 
经验 但是 这个 经验 常常 难以 量化 或者 模型 化 
于是 贝叶斯 学派 大胆 的 假设 先验 分布 的 模型 
比如 正态分布 beta 分布 等 这个 假设 一般 没有 特定 
的 依据 因此 一直 被 频率 学派 认为 很 荒谬 
虽然 难以 从 严密 的 数学逻辑 里 推出 贝叶斯 学派 
的 逻辑 但是 在 很多 实际 应用 中 贝叶斯 理论 
很好用 比如 垃圾邮件 分类 文本 分类 我们 先 看看 条件 
独立 公式 如果 X 和Y/nr 相互 独立 则有 P X 
Y = P X P Y P X Y = 
P X P Y 我们 接 着 看看 条件概率 公式 
P Y | X = P X Y / P 
X P Y | X = P X Y / 
P X P X | Y = P X Y 
/ P Y P X | Y = P X 
Y / P Y 或者说 P Y | X = 
P X | Y P Y / P X P 
Y | X = P X | Y P Y 
/ P X 接着 看看 全 概率 公式 P X 
= ∑ kP X | Y = Yk P Yk 
其中 ∑ kP Yk = 1P X = ∑ kP 
X | Y = Yk P Yk 其中 ∑ kP 
Yk = 1 从 上面 的 公式 很容易 得出 贝叶斯 
公式 P Yk | X = P X | Yk 
P Yk ∑ kP X | Y = Yk P 
Yk 基于 朴素 贝叶斯 公式 比较 出 后验/nr 概率 的 
最大值 来 进行 分类 后验/nr 概率 的 计算 是由 先验概率 
与 类 条件概率 的 乘积 得出 先验概率 和类/nr 条件概率 要 
通过 训练 数据集 得出 即为 朴素 贝叶斯 分类 模型 将其 
保存为 中间 结果 测试 文档 进行 分类 时调 用 这个 
中间 结果 得出 后验/nr 概率 1.2 基本 定义 朴素 贝叶斯 
分类 是 一种 十分 简单 的 分类 算法 叫 它 
朴素 贝叶斯 分类 是因为 这种 方法 的 思想 真的 很 
朴素 朴素 贝叶斯 的 思想 基础 是 这样 的 对于 
给出 的 待 分 类项 求解 在 此项 出现 的 
条件 下 各个 类别 出现 的 概率 哪个 最大 就 
认为 此 待 分 类项 属于 哪个 类别 朴素 贝叶斯 
分类 的 正式 定义 如下 1 设 为 一个 待 
分 类项 而 每个 a 为 x 的 一个 特征 
属性 2 有 类别 集合 3 计算 4 如果 则 
那么 现在 的 关键 就是 如何 计算 第 3步 中的 
各个 条件概率 我们 可以 这么 做 1 找到 一个 已知 
分类 的 待 分 类项 集合 这个 集合 叫做 训练样本 
集 2 统计 得到 在 各类 别下 各个 特征 属性 
的 条件 概率 估计 即 3 如果 各 个 特征 
属性 是 条件 独立 的 则 根据 贝叶 斯定理 有 
如下 推导 因为 分母 对于 所有 类别 为 常数 因为 
我们 只要 将 分子 最大化 皆可 又 因为 各 特征 
属性 是 条件 独立 的 所以有 2 . 利用 朴素 
贝叶斯 模型 进行 文本 分类 2.1 模型 原理 与 训练 
朴素 贝叶斯 分类器 是 一种 有 监督 学习 常见 有 
两种 模型 多项式 模型 multinomial model 即为 词频 型 和 
伯努利 模型 Bernoulli model 即 文档 型 还有 一种 高斯 
模型 前 二者 的 计算 粒度 不一样 多项式 模型 以 
单词 为 粒度 伯努利 模型 以 文件 为 粒度 因此/c 
二者/n 的/uj 先验概率/l 和类/nr 条件概率/i 的/uj 计算/v 方法/n 都/d 不同/a 
计算 后验/nr 概率 时 对于 一个 文档 d 多项式 模型 
中 只有在 d 中 出现 过 的 单词 才会 参与 
后验/nr 概率 计算 伯努利 模型 中 没有 在 d 中 
出现 但是 在 全局 单词表 中 出现 的 单词 也会 
参与 计算 不过 是 作为 反方 参与 的 这里 暂 
不考虑 特征 抽取 为避免 消除 测试 文档 时类/nr 条件概率 中 
有为 0 现象 而 做 的 取 对数 等 问题 
# * coding UTF 8 * from sklearn . naive 
_ bayes import MultinomialNB import matplotlib . pyplot as plt 
import os import random import jieba 函数 说明 中文 文本处理 
Parameters folder _ path 文本 存放 的 路径 test _ 
size 测试 集 占 比 默认 占 所有 数据集 的 
百分之 20 Returns all _ words _ list 按 词频 
降序 排序 的 训练 集 列表 train _ data _ 
list 训练 集 列表 test _ data _ list 测试 
集 列表 train _ class _ list 训练 集 标签 
列表 test _ class _ list 测试 集 标签 列表 
Author Jack Cui Blog http / / blog . csdn 
. net / c406495762 Modify 2017 08 22 def TextProcessing 
folder _ path test _ size = 0.2 folder _ 
list = os . listdir folder _ path # 查看 
folder _ path 下 的 文件 data _ list = 
# 数据集 数据 class _ list = # 数据集 类别 
# 遍历 每 个子 文件夹 for folder in folder _ 
list new _ folder _ path = os . path 
. join folder _ path folder # 根据 子 文件夹 
生成 新的 路径 files = os . listdir new _ 
folder _ path # 存放 子 文件 夹下 的 txt 
文件 的 列表 j = 1 # 遍历 每个 txt 
文件 for file in files if j 100 # 每类 
txt 样本 数 最多 100个 break with open os . 
path . join new _ folder _ path file r 
encoding = utf 8 as f # 打开 txt 文件 
raw = f . read word _ cut = jieba 
. cut raw cut _ all = False # 精简 
模式 返回 一个 可 迭代 的 generator word _ list 
= list word _ cut # generator 转换 为 list 
data _ list . append word _ list # 添加 
数据集 数据 class _ list . append folder # 添加 
数据集 类别 j + = 1 data _ class _ 
list = list zip data _ list class _ list 
# zip 压缩 合并 将 数据 与 标签 对应 压缩 
random . shuffle data _ class _ list # 将 
data _ class _ list 乱序 index = int len 
data _ class _ list * test _ size + 
1 # 训练 集 和 测试 集 切分 的 索引 
值 train _ list = data _ class _ list 
index # 训练 集 test _ list = data _ 
class _ list index # 测试 集 train _ data 
_ list train _ class _ list = zip * 
train _ list # 训练 集 解压缩 test _ data 
_ list test _ class _ list = zip * 
test _ list # 测试 集 解压缩 all _ words 
_ dict = { } # 统计 训练 集 词频 
for word _ list in train _ data _ list 
for word in word _ list if word in all 
_ words _ dict . keys all _ words _ 
dict word + = 1 else all _ words _ 
dict word = 1 # 根据 键 的 值 倒序 
排序 all _ words _ tuple _ list = sorted 
all _ words _ dict . items key = lambda 
f f 1 reverse = True all _ words _ 
list all _ words _ nums = zip * all 
_ words _ tuple _ list # 解压缩 all _ 
words _ list = list all _ words _ list 
# 转换成 列表 return all _ words _ list train 
_ data _ list test _ data _ list train 
_ class _ list test _ class _ list 函数 
说明 读取 文件 里 的 内容 并 去 重 Parameters 
words _ file 文件 路径 Returns words _ set 读取 
的 内容 的 set 集合 Author Jack Cui Blog http 
/ / blog . csdn . net / c406495762 Modify 
2017 08 22 def MakeWordsSet words _ file words _ 
set = set # 创建 set 集合 with open words 
_ file r encoding = utf 8 as f # 
打开 文件 for line in f . readlines # 一行 
一行 读取 word = line . strip # 去 回车 
if len word 0 # 有 文本 则 添加到 words 
_ set 中 words _ set . add word return 
words _ set # 返回 处理结果 函数 说明 根据 feature 
_ words 将 文本 向 量化 Parameters train _ data 
_ list 训练 集 test _ data _ list 测试 
集 feature _ words 特 征集 Returns train _ feature 
_ list 训练 集 向 量化 列表 test _ feature 
_ list 测试 集 向 量化 列表 Author Jack Cui 
Blog http / / blog . csdn . net / 
c406495762 Modify 2017 08 22 def TextFeatures train _ data 
_ list test _ data _ list feature _ words 
def text _ features text feature _ words # 出现 
在 特征 集中 则 置 1 text _ words = 
set text features = 1 if word in text _ 
words else 0 for word in feature _ words return 
features train _ feature _ list = text _ features 
text feature _ words for text in train _ data 
_ list test _ feature _ list = text _ 
features text feature _ words for text in test _ 
data _ list return train _ feature _ list test 
_ feature _ list # 返回 结果 函数 说明 文本 
特征 选取 Parameters all _ words _ list 训练 集 
所有 文本 列表 deleteN 删除 词频 最高 的 deleteN 个 
词 stopwords _ set 指定 的 结束语 Returns feature _ 
words 特 征集 Author Jack Cui Blog http / / 
blog . csdn . net / c406495762 Modify 2017 08 
22 def words _ dict all _ words _ list 
deleteN stopwords _ set = set feature _ words = 
# 特征 列表 n = 1 for t in range 
deleteN len all _ words _ list 1 if n 
1000 # feature _ words 的 维度 为 1000 break 
# 如果 这个 词 不是 数字 并且 不 是 指定 
的 结束语 并且 单词 长度 大于 1 小于 5 那么 
这个 词 就 可以 作为 特征词 if not all _ 
words _ list t . isdigit and all _ words 
_ list t not in stopwords _ set and 1 
len all _ words _ list t 5 feature _ 
words . append all _ words _ list t n 
+ = 1 return feature _ words 函数 说明 新闻 
分类器 Parameters train _ feature _ list 训练 集 向 
量化 的 特征 文本 test _ feature _ list 测试 
集 向 量化 的 特征 文本 train _ class _ 
list 训练 集 分类 标签 test _ class _ list 
测试 集 分类 标签 Returns test _ accuracy 分类器 精度 
Author Jack Cui Blog http / / blog . csdn 
. net / c406495762 Modify 2017 08 22 def TextClassifier 
train _ feature _ list test _ feature _ list 
train _ class _ list test _ class _ list 
classifier = MultinomialNB . fit train _ feature _ list 
train _ class _ list test _ accuracy = classifier 
. score test _ feature _ list test _ class 
_ list return test _ accuracy if _ _ name 
_ _ = = _ _ main _ _ # 
文本 预处理 folder _ path = . / SogouC / 
Sample # 训练 集 存放 地址 all _ words _ 
list train _ data _ list test _ data _ 
list train _ class _ list test _ class _ 
list = TextProcessing folder _ path test _ size = 
0.2 # 生成 stopwords _ set stopwords _ file = 
. / stopwords _ cn . txt stopwords _ set 
= MakeWordsSet stopwords _ file test _ accuracy _ list 
= deleteNs = range 0 1000 20 # 0 20 
40 60 . . . 980 for deleteN in deleteNs 
feature _ words = words _ dict all _ words 
_ list deleteN stopwords _ set train _ feature _ 
list test _ feature _ list = TextFeatures train _ 
data _ list test _ data _ list feature _ 
words test _ accuracy = TextClassifier train _ feature _ 
list test _ feature _ list train _ class _ 
list test _ class _ list test _ accuracy _ 
list . append test _ accuracy # ave = lambda 
c sum c / len c # print ave test 
_ accuracy _ list plt . figure plt . plot 
deleteNs test _ accuracy _ list plt . title Relationship 
of deleteNs and test _ accuracy plt . xlabel deleteNs 
plt . ylabel test _ accuracy plt . show 3 
. SVM 的 原理 3.1 快速 理解 SVM 原理 很多 
讲解 SVM 的 书籍 都 是从 原理 开始 讲解 如果 
没有 相关 知识 的 铺垫 理解 起来 还是 比较 吃力 
的 以下 的 一个 例子 可以 让 我们 对 SVM 
快速 建立 一个 认知 给定 训练样本 支持 向量 机 建立 
一个 超平面 作为 决策 曲面 使得 正 例和 反例 的 
隔离 边界 最大化 决策 曲面 的 初步 理解 可以 参考 
如下 过程 1 如 下图 想象 红色 和 蓝色 的 
球 为 球 台上 的 桌球 我们 首先 目的 是 
找到 一条 曲线 将 蓝色 和 红色 的 球 分开 
于是 我们 得到 一条 黑色 的 曲线 2 为了 使 
黑色 的 曲线 离 任意 的 蓝球 和 红球 距离 
也 就是 我们 后面 要 提到 的 margin 最大化 我们 
需要 找到 一条 最优 的 曲线 如 下图 3 想象 
一下 如果 这些 球 不是 在 球 桌上 而是 被 
抛向 了 空中 我们 仍然 需要 将 红色 球 和 
蓝色 球 分开 这时 就 需要 一个 曲面 而且 我们 
需要 这个 曲面 仍然 满足 跟 所有 任意 红球 和 
蓝球 的 间距 的 最大化 需要 找到 的 这个 曲面 
就是 我们 后面 详细 了解 的 最优 超平面 4 离 
这个 曲面 最近 的 红色 球 和 蓝色 球 就是 
Support Vector 4 . 利用 SVM 模型 进行 文本 分类 
具体 参考 https / / blog . csdn . net 
/ Kaiyuan _ sjtu / article / details / 800641455 
. pLSA 共轭 先验 分布 LDA 主题 模型 原理 pLSA 
参考 https / / www . cnblogs . com / 
Determined22 / p / 7237111 . htmlLDA 主题 模型 原理 
参考 http / / www . cnblogs . com / 
pinard / p / 6831308 . html6 . 使用 LDA 
生成 主题 特征 在 之前 特征 的 基础 上 加入 
主题 特征 进行 文本 分类 参考 https / / blog 
. csdn . net / Kaiyuan _ sjtu / article 
/ details / 83572927 参考 1 . https / / 
blog . csdn . net / u013710265 / article / 
details / 727805202 . https / / blog . csdn 
. net / u013710265 / article / details / 727805203 
. https / / blog . csdn . net / 
yyy430 / article / details / 88346920 原文 题目 NLP 
in Python 翻译 陈之炎/nr 校对 和 中华 本文 共 2700字 
建议 阅读 6 分钟 自然语言 处理 是 数据 科学 中 
的 一大 难题 在 这篇文章 中 我们 会 介绍 一个 
工业级 的 python 库 自然语言 处理 NLP 是 数据 科学 
中 最 有趣 的 子 领域 之一 越来越 多 的 
数据 科学家 希望 能够 开发 出 涉及 非 结构化 文本 
数据 的 解决方案 尽管如此 许多 应用 数据 科学家 均 具有 
STEM 和 社会 科学 背景 依然 缺乏 NLP 自然语言 处理 
经验 在 这篇文章 中 我 将 探讨 一些 基本 的 
NLP 概念 并 展示 如何 使用 日益 流行 的 Python 
  spaCy 包来/nr 实现 这些 概念 这篇文章 适合 NLP 初学者 
阅读 但 前提 是 假设 读者 具备 Python 的 知识 
你 是 在 说 spaCy 吗 spaCy 是 一个 相对 
较 新的 包 工业级 的 Python 自然语言 工具包 由 Matt 
Honnibal 在 Explosion AI . 开发 它 在 设计 时 
目标 用户 以 应用 数据 科学家 为主 这也 意味着 它 
不 需要 用户 来 决定 使用 哪个 算法 来 处理 
常见 任务 而且 它 非常 地 快 快得 难以置信 它 
用 Cython 来 实现 如果 你 熟悉 Python 数据 科学 
栈 spaCy 就是 NLP 的 numpy 它 虽然 理所当然 地 
位于 底层 但是 却很 直观 性能 也 相当 地 高 
那么 它 能做 什么 呢 spaCy 为 任何 NLP 项目 
中 常用 的 任务 提供 一站式 服务 . 包括 符号化 
Tokenizatioin 词干 提取 Lemmatization 词性 标注 Part of speech tagging 
实体 识别 Entity recognition 依存 句法分析 Dependency parsing 句子 的 
识别 Sentence recognition 字 向量 变换 Word to vector transformation 
许多 方便 的 清除 文本 和 标准化 文本 的 方法 
cleaning and normalizing text 我会 对 这些 功能 做 一个 
高 层次 的 概述 并 说明 如何 利用 spaCy 访问 
它们 那 我们 就 开始 吧 首先 我们 加载 spaCy 
的 管线 按照 约定 它 存储 在 一个 名为 nlp 
的 变量 中 需要 花几/nr 秒钟 时间 声明 该 变量 
因为 spaCy 预 先将 模型 和 数据 加载 到 前端 
以 节省 时间 实际上 这样 做 可以 提前 完成 一些 
繁重 的 工作 使得 nlp 解析 数据 时 开销 不至于 
过大   请注意 在 这里 我们 使用 的 语言 模型 
是 英语 同时 也 有 一个 功能 齐全 的 德语 
模型 在 多种 语言 中 均可 实现 标记 化 将 
在下面 讨论 我们 在 示例 文本 中 调用 NLP 来 
创建 Doc 对象 Doc   对象 是 文本 本身 NLP 
任务 容器 将 文本 切 分成 文字 Span   对象 
和 元素 Token   对象 这些 对象 实际上 不 包含 
数据 值得 注意 的 是 Token   和 /nr Span 对象 
实际上 没有 数据 相反 它们 包含 Doc 对象 中 的 
数据 的 指针 并且 被 惰性 求值 即 根据 请求 
绝大多数 spaCy 的 核心 功能 是 通过 对 Doc   
n = 33   Span   n = 29 和 /nr 
Token   n = 78 对象 的 方法 来 实现 
的 In 1 import spacy . . . nlp = 
spacy . load en . . . doc = nlp 
The big grey dog ate all of the chocolate but 
fortunately he wasn t sick 分词 tokenization 分词 是 许多 
自然语言 处理 任务 中 的 一个 基本 步骤 分词 就是 
将 一段 文本 拆 分为 单词 符号 标点符号 空格 和 
其他 元素 的 过程 从而 创建 token 这样 做 的 
一个 简单 方法 是 在 空格 上 拆分 字符串 In 
2 doc . text . split . . . Out 
2 The big grey dog ate all of the chocolate 
but fortunately he wasn t sick 从 表面 上 直接 
以 空格 进行 分词 效果 还 不错 但是 请注意 它 
忽略 了 标点符号 且 没有 将 动词 和 副词 分开 
was n t 换句话说 它 太天真 了 它 无法 识别 
出 帮助 我们 和 机器 理解 其 结构 和 含义 
的 文本 元素 让 我们 来 看看 spaCy 如何 处理 
这个 问题 In 3 token . orth _ for token 
in doc . . . Out 3 The big grey 
dog ate all of the chocolate but fortunately he was 
n t sick 这里 我们 访问 的 每个 token 的 
. orth _ 方法 它 返回 一个 代表 token 的 
字符串 而 不是 一个 SpaCytoken 对象 这 可能 并不 总是 
可取 的 但 值得 注意 SpaCy 能够 识别 标点符号 并 
能够 将 这些 标点符号 与 单词 的 token 分开 许多 
SpaCy 的 token 方法 为 待处理 的 文字 同时 提供 
了 字符串 和 整数 的 返回值 带有 下划线 后缀 的 
方法 返回 字符串 而 没有 下划线 后缀 的 方法 返回 
的 是 整数 例如 In 4 token token . orth 
_ token . orth for token in doc . . 
. Out 4 The The 517 big big 742 grey 
grey 4623 dog dog 1175 ate ate 3469 all all 
516 of of 471 the the 466 chocolate chocolate 3593 
416 but but 494 fortunately fortunately 15520 he he 514 
was was 491 n t n t 479 483 sick 
sick 1698 495 In 5 token . orth _ for 
token in doc if not token . is _ punct 
| token . is _ space . . . Out 
5 The big grey dog ate all of the chocolate 
but fortunately he was n t sick 很酷 对吧 词干 
提取 和 分词 相关 的 任务 是 词干 提取 词干 
提取 是 将 一个 单词 还原成 它 的 基本 形式 
母词的/nr 过程 不同 用法 的 单词 往往 具有 相同 意义 
的 词根 例如 practice 练习   practiced 熟练 的 和 /nr 
practising 实习 这三个 单词 实质上 指 的 是 同 一件 
事情 通常 需要 将 相似 意义 的 单词 进行 标准化 
标准化 到 其 基本 的 形式 使用 SpaCy 我们 利用 
标记 的 . lemma _   方法 访问 到 每个 
单词 的 基本 形式 In 6 practice = practice practiced 
practicing . . . nlp _ practice = nlp practice 
. . . word . lemma _ for word in 
nlp _ practice . . . Out 6 practice practice 
practice 为什么 这个 会 有用 一个 即时 用 例 便是 
机器学习 特别 是 文本 分类 例如 在 创建 单词 袋 
之前 需 对 文本 进行 词干 提取 避免了 单词 的 
重复 因此 该 模型 可以 更 清晰 地 描述 跨 
多个 文档 的 单词 使用 模式 词性 标注 POS Tagging 
词性 标注 是 将 语法 属性 如 名词 动词 副词 
形容词 等 赋值 给 词 的 过程 共享 相同 词性 
标记 的 单词 往往 遵循 类似 的 句法结构 在 基于 
规则 的 处理 过程 中 非常 有用 例如 在 给定 
的 事件 描述 中 我们 可能 希望 确定 谁 拥有 
什么 通过 利用 所有格 我们 可以 做到 这 一点 提供 
文本 的 语法 SpaCy 采用 流行 的 Penn Treebank POS 
标记 参见 这里 利用 SpaCy 可以/c 分别/d 使用/v ./i pos/w 
_/i  /i 和 ./nr tag/w _/i 方法/n 访问/v 粗粒度/n POS/w 标记/n 
和/c 细粒度/n POS/w 标记/n 在 这里 我 访问 细粒度 的 
POS 标记 In 7 doc2 = nlp Conor s dog 
s toy was hidden under the man s sofa in 
the woman s house . . . pos _ tags 
= i i . tag _ fori indoc2 . . 
. pos _ tags . . . Out 7 Conor 
NNP s POS dog NN s POS toy NN was 
VBD hidden VBN under IN the DT man NN s 
POS sofa NN in IN the DT woman NN s 
POS house NN 我们 可以 看到 s   的 标签 
被 标记 为   POS . 我们 可以 利用 这个 
标记 提取 所有者 和 他们 拥有 的 东西 In 8 
owners _ possessions = . . . for i in 
pos _ tags . . . if i 1 = 
= POS . . . owner = i 0 . 
nbor 1 . . . possession = i 0 . 
nbor 1 . . . owners _ possessions . append 
owner possession . . . . . . owners _ 
possessions . . . Out 8 Conor dog dog toy 
man sofa woman house 这将 返回 所有者 拥有 元组 的 
列表 如果 你 想在 这件事 上 表现 成为 超级 Python 
能手 的话 你 可以 把 它 写成 一个 完整 的 
列表 我 认为 这 是 最好 的 In 9 i 
0 . nbor 1 i 0 . nbor + 1 
for i in pos _ tags if i 1 = 
= POS . . . Out 9 Conor dog dog 
toy man sofa woman house 在 这里 我们 使用 的 
是 每个 标记 的 . nbor   方法 它 返回 
一个 和 这个 标记 相邻 的 标记 实体 识别 实体 
识别 是 将 文本 中 的 指定 实体 分类 为 
预先 定义 的 类别 的 过程 如 个人 地点 组织 
日期 等 spaCy 使用 统计模型 对 各种 模型 进行 分类 
包括 个人 事件 艺术 作品 和 国籍 / 宗教 参见 
完整 列表 文件 例如 让 我们 从 贝拉克 奥巴马 的 
维基百科 条目 中 选出 前 两句话 我们 将 解析 此 
文本 然后 使用 Doc   对象 的   . ents 
方法 访问 标识 的 实体 通过 调用 Doc   的 
这个 方法 我们 可以 访问 其他 的 标记 方法 特别 
是   . label _   和 ./nr label 两个 方法 
In 10 wiki _ obama = Barack Obama is an 
American politician who served as . . . the 44th 
President of the United States from 2009 to 2017 . 
He is the first . . . African American to 
have served as president . . . as well as 
the first born outside the contiguous United States . nlp 
_ obama = NLP wiki _ obama i i . 
label _ i . label for i in nlp _ 
obama . ents . . . Out 10 Barack Obama 
PERSON 346 American NORP 347 the United States GPE 350 
2009 to 2017 DATE 356 first ORDINAL 361 African NORP 
347 American NORP 347 first ORDINAL 361 United States GPE 
350 您 可以 看到 在 本例 中 模型 所 识别 
的 实体 以及 它们 的 精确 程度 PERSON   是 
不言自明 的 NORP 是 国籍 或 宗教团体 GGPE 标识 位置 
城市 国家 等等 DATE   标识 特定 的 日期 或 
日期 范围   ORDINAL 标识 一个 表示 某种 类型 的 
顺序 的 单词 或 数字 在 我们 讨论 Doc 方法 
的 主题 时 值得一提的是 spaCy 的 句子 标识符 NLP 任务 
希望 将 文档 拆分 成 句子 的 情况 并不 少见 
利用 SpaCy 访问 Doc s . sents   方法 并 
不难 做到 In 11 for ix sent in enumerate nlp 
_ obama . sents 1 . . . print Sentence 
number { } { } . format ix sent . 
. . Sentence number 1 Barack Obama is an American 
politician who served as the 44th President of the United 
States from 2009 to 2017 . Sentence number 2 He 
is the first African American to have served as president 
as well as the first born outside the contiguous United 
States . 目前 就是 这样 在 以后 的 文章 中 
我 将 展示 如何 在 复杂 的 数据 挖掘 和 
ML 的 任务 中 使用 spaCy TrueSight 是 一个 AIOps 
平台 由 机器 学习 和 分析 提供 动力 支持 它 
解决 了 多个 云的/nr 复杂性 并且 提高 了 数字 转化 
的 速度 从而 提升 了 IT 运 ­ 作 的 
效率 原文 链接 https / / dzone . com / 
articles / nlp in python 译者 简介 陈炎 之 北京 
交通 大学 通信 与 控制 工程 专业 毕业 获得 工学 
硕士 学位 历任 长城 计算机 软件 与 系统 公司 工程师 
大唐微电子公司 工程师 现任 北京 吾 译 超群 科技 有限公司 技术支持 
目前 从事 智能化 翻译 教学 系统 的 运营 和 维护 
在 人工智能 深度 学习 和 自然 语言 处理 NLP 方面 
积累 有 一定 的 经验 业余时间 喜爱 翻译 创作 翻译 
作品 主要 有 IEC ISO 7816 伊拉克 石油 工程 项目 
新 财税 主义 宣言 等等 其中 中译英 作品 新 财税 
主义 宣言 在 GLOBAL TIMES 正式 发表 能够 利用 业余 
时间 加入到 THU 数据 派 平台 的 翻译 志愿者 小组 
希望/v 能和/nr 大家/n 一起/m 交流/n 分享/v 共同进步 翻译 组 招募 
信息 工作 内容 需要 一颗 细致 的 心 将 选取 
好 的 外文 文章 翻译成 流畅 的 中文 如果 你 
是 数据 科学 / 统计学 / 计算机 类 的 留学生 
或 在 海外 从事 相关 工作 或 对 自己 外语 
水平 有 信心 的 朋友 欢迎 加入 翻译 小组 你 
能 得到 定期 的 翻译 培训 提高 志愿者 的 翻译 
水平 提高 对于 数据 科学 前沿 的 认知 海外 的 
朋友 可以 和 国内 技术 应用 发展 保持 联系 THU 
数据 派 产学研 的 背景 为 志愿者 带来 好 的 
发展 机遇 其他 福利 来自 于 名企 的 数据 科学 
工作者 北大清华 以及 海外 等 名校 学生 他们 都将/nr 成为 
你 在 翻译 小组 的 伙伴 点击 文末 阅读 原文 
加入 数据 派 团队 ~ 点击 阅读 原文 拥抱 组织 
自然 语言 处理 是 Natural Language Processing 简称 NLP 自然语言 
理解 是 Natural Language Understanding 简称 NLU 1 概括 来说 
NLP 除了 NLU 图中 红框 部分 还 包含 理解 之前 
的 处理 阶段 和 理解 之后 的 应用 阶段 也 
就是说 NLU 是 NLP 的 子集 他们 不是 并 集 
更 不是 等价 的 概念 这里 是 很多 AI 从业 
人员 都 容易 混淆 的 大家 可以 先 记住 这个 
概念 关系 NLU 是 NLP 的 子集 2 其他 1 
左边 最底部 是 最 基础 的 大 数据 机器 学习 
和 语言学 Linguistics 2 往 上看 是 知识图谱 Knowledge Graph 
其中 包含 了 实体 图谱 注意力 图谱 和 意图 图谱 
3 再 往上 左侧 是 语言 理解 Language Understanding 右侧 
是 语言 生成 Language Generation 语言 理解 包含 了 Query 
理解 文本 理解 情感 分析 Sentiment Analysis 等 还有 词法 
Lexical 句法 Syntax 和 语义 Semantic 等 不同 层次 的 
分析 语言 生成 包含 了 写作 阅读 理解 等等 4 
最 上方 是 系统 层面 包含 了 问答 系统 机器 
翻译 和 对话 系统 5 最 右侧 是 各种 应用 
场景 包含 搜索 feeds 流 O2O 广告 等等 NLP 的 
难点 ~ 6 NLP 的 解决 方法 规则 统计 深度 
学习 ~ 9 NLP 是 AI 的 最大 瓶颈 语言 
生成 是 NLP 的 最前沿 这 2点 大家 知道 就 
可以 了 以上 内容 来自 饭团 AI 产品 经理 大本营 
点击 这里 可 关注 http / / fantuan . guokr 
. net / groups / 219 / 如果 遇到 支付 
问题 请先 关注 饭团 的 官方 微信 服务 号 fantuan 
app 作者 黄钊/nr hanniman 图灵 机器人 人才 战略 官 前 
腾讯 产品 经理 5年 AI 实战经验 8年 互联网 背景 微信 
公众 号 / 知乎 / 在行 ID hanniman 饭团 AI 
产品 经理 大本营 分享 人工智能 相关 原创 干货 200页 PPT 
人工智能 产品 经理 的 新起点 被 业内 广泛 好评 下载量 
1万 + 自然语言 处理 大体 包括 了 自然 语言 理解 
和 自然 语言 生成 两个 部分 实现 人机 间 自然语言 
通信 意味着 要使 计算机 既 能理解 自然语言 文本 的 意义 
也能 以 自然 语言 文本 来 表达 给定 的 意图 
思想 等 前者 称为 自然语言 理解 后者 称为 自然语言 生成 
自然语言 处理 是 计算机 科学 领域 与 人工智能 领域 中 
的 一个 重要 方向 自然语言 处理 的 终极 目标 是 
用 自然 语言 与 计算机 进行通信 使 人们 可以 用 
自己 最 习惯 的 语言 来 使用 计算机 而/c 无需/v 
再/d 花/v 大量/n 的/uj 时间/n 和/c 精力/n 去/v 学习/v 不/d 
很/zg 自然/d 和/c 习惯/n 的/uj 各种/r 计算机/n 语言/n 针对 一定 
应用 具有 相当 自然语言 处理 能力 的 实用 系统 已经 
出现 典型 的 例子 有 多语种 数据库 和 专家 系统 
的 自然 语言 接口 各种 机器 翻译 系统 全文 信息 
检索系统 自动 文摘 系统 等 国内 BAT 京东 科大 讯 
飞 都有 涉及 自然语言 处理 的 业务 另外 还 出现 
了 爱 特曼 出门 问问 思 必 驰 蓦然 认知 
三角 兽 科技 森 亿 智能 乂 学 教育 智齿 
客服 等 新兴 企业 人工智能 大 数据 云计算 和 物联网 
的 未来 发展 值得 重视 均为 前沿 产业 多/m 智/ng 
时代/n 专注/v 于/p 人工智能/n 和大/nr 数据/n 的/uj 入门/ns 和科谱/nr 在 
此为 你 推荐 几篇 优质 好文 如何 快速 入门 NLP 
自然语言 处理 概述 http / / www . duozhishidai . 
com / article 11742 1 . html 自然语言 处理 NLP 
知识结构 总结 http / / www . duozhishidai . com 
/ article 10036 1 . htmlNLP 自然语言 处理 技术 在 
人工智能 法官 中的 应用 是 什么 http / / www 
. duozhishidai . com / article 2325 1 . html 
多 智 时代 人工智能 和大/nr 数据 学习 入门 网站 | 
人工智能 大 数据 物联网 云计算 的 学习 交流 网站 摘要 
深度 学习 方法 使用 多个 处理 层 来 学习 数据 
的 层次 表示 并在 许多 领域 产生 了 最 先进 
的 结果 近年来 在 自然 语言 处理 NLP 的 背景 
下 各种 模型 设计 和 方法 得到 了 蓬勃 发展 
在 这篇 综述 中 我们 回顾 了 许多 NLP 任务 
中 所 使用 的 重要 的 深度 学习 相关 模型 
和 方法 在 综述 中 我们 提到 了 序列 生成 
方法 神经 机器翻译 对话 系统 的 模型 并 叙述 了 
它们 的 发展 过程 与此 我们 还 对 各种 模型 
进行 了 总结 比较 和 对比 并对 NLP 中 深度 
学习 的 过去 现在 和 未来 进行 了 详细 的 
了解 同时 我们 也 深入 探讨 了 现在 主流 的 
几种 算法 关键词 深度 学习 自然语言 处理 序列 生成 技术 
机器翻译 对话/n 系统/n 1/m ．/i 研究/vn 背景/n 及/c 意义/n 深度/ns 
学习/v 体系/n 结构/n 和/c 算法/n 在/p 计算机/n 视觉/n 和/c 模式/n 
识别/v 等/u 领域/n 已经/d 取得/v 了/ul 令人瞩目/l 的/uj 进展/vn 随着 
这 一 趋势 近年来 的 关于 NLP 研究 越来越 多 
地 使用 新 的 深度 学习 方法 针对 NLP 问题 
的 机器 学习 方法 是 基于 浅层 模型 的 例 
SVM 和 逻辑 回归 大多数 的 语言 信息 用 稀疏 
表示 高维 特征 表示 这 导致 诸如 维数 灾难 之类 
的 问题 近年来 基于 密集 向量 表示 的 神经 网络 
在 各种 NLP 任务 中 取得 了 较好 的 效果 
这一 趋势 是由 词 嵌入 和 深度 学习 方法 的 
成功 引发 的 深度 学习 可以 实现 多 层次 的 
自动 特征 表示 学习 相比之下 传统 的 基于 机器 学习 
的 NLP 系统 在 很大 程度 上 依赖于 手工 构建 
的 特征 这种/r 手工/n 构建/v 的/uj 特征/n 常耗/nr 大量/n 的/uj 
时间/n 和/c 成本/n 并且 不同 任务 所 需要 的 特征 
又是 不同 的 自然语言 处理 NLP 是 一种 基于 理论 
的 计算 技术 用于 人类 语言 的 自动 分析 和 
表达 NLP/w 的/uj 研究/vn 已经/d 从/p 打卡/v 和/c 批处理/vn 的/uj 
时代/n 一个 句子 的 分析 时间 可以 长达 7 分钟 
发展 过来 到 现在 可以 在 不到 一秒 的 时间 
内 可以 处理 数百万个 网页 NLP 使 计算机 能够 在 
所有 级别 上 执行 广泛 的 自然 语言 相关 任务 
从/p 解析/vn 和/c 词性/n 标记/n 到/v 机器/n 翻译/v 和/c 对话/n 
系统/n 自然语言 处理 NLP 通过 计算 技术 学习 理解 然后 
产生 人类 语言 NLP/w 中/f 有/v 许多/m 有/v 意义/n 的/uj 
研究/vn 方向/n 其中 包括 序列 生成 机器翻译 对话 系统 NLP 
还有 一些 其他 的 主题 计算机 视觉 与 NLP 的 
集成 如 视觉 字幕 视觉 对话 视觉 关系 和 属性 
检测 在 过去 的 几年 中 深度 学习 DL 架构/n 
和/c 算法/n 在/p 图像/n 识别/v 和/c 语音/n 处理/v 等/u 领域/n 
取得/v 了/ul 令人瞩目/l 的/uj 进步/d 它们 在 自然 语言 处理 
NLP 中 的 应用 起初 并不 那么 令人 印象 深刻 
但 现在 已经 证明 可以 做出 重大 贡献 为 一些 
常见 的 NLP 任务 提供 最 先进 的 结果 命名 
实体 识别 NER 词性 POS 标记 或 情感 分析 是 
神经 网络 模型 优于 传统 方法 的 一些 问题 机器 
翻译 的 进步 可能 是 最 引人注目 的 R . 
Collobert 等人 1 证实 了 一个 简单 的 深度 学习 
框架 在 几个 NLP 任务 如 命名 实体 识别 NER 
语义 角色 标注 SRL 和 词性 标注 POS 具体分析 见 
附录 方面 的 性能 优于 最 先进 的 方法 此后 
针对 复杂 的 NLP 任务 提出 了 大量 基于 深度 
学习 的 复杂 算法 这/r 其中/r 包括/v 应用/v 于/p 自然/d 
语言/n 任务/n 的/uj 主要/b 深度/ns 学习/v 相关/v 模型/n 和/c 方法/n 
如 卷积 神经网络 CNNs 循环 神经网络 RNNs 和 递归 神经网络 
还包括 增强 记忆 策略 注意力 机制 以及 无 监督 模型 
强化 学习 方法 以及 最近 的 深度 生成 模型 是 
如何 应用于 与 语言 相关 任务 的 2 ． 当前 
研究 现状 在 过去 几年 中 深度 学习 改变 了 
整个 景观 每天 有/v 更多/d 的/uj 应用/v 程序/n 依赖/v 于/p 
医疗/n 保健/nr 金融 人力 资源 零售 地震 检测 和 自动 
驾驶 汽车 等 领域 的 深度 学习 技术 至于 现有 
的 应用 结果 一直 在 稳步 提高 在 学术 层面 
机器学习 领域 变得 如此 重要 以至于 每 20 分钟 就 
会 出现 一篇 新的 科学 文章 而且 深度 学习 已经 
渗透 到 NLP 的 许多 子 领域 并 帮助 取得 
重大 进展 对于 深度 学习 算法 和非/nr 深度 学习 算法 
以及 基于 无 领域 知识 的 方法 和 基于 语言学 
知识 的 方法 NLP 似乎 仍然 是 一个 关于 协同 
而不是 竞争 的 领域 一些 非 深度 学习 算法 是 
有效 且 表现 良好 的 如 word2vec 和 fastText 等 
在 2018年 深度 学习 也 取得 了 一些 主要 进展 
但 最受 关注 的 是 Google AI 语言 团队 发表 
了 一篇 BERT 语言 模型 论文 1 在 自然 语言 
处理 NLP 中 语言 模型 是 可以估计 一组 语言 单元 
通常 是 单词 序列 的 概率 分布 的 模型 这些 
是 有趣 的 模型 因为 它们 可以 以 很低 的 
成本 构建 并且 显著 改进 了 几个 NLP 任务 例如 
机器翻译 语音 识别 和 解析 之前 最 著名 的 方法 
是 马尔可夫 模型 和n/nr gram 模型 随着 深度 学习 的 
出现 出现 了 一些 基于 长短期 记忆 网络 LSTM 的 
更 强大 的 模型 虽然 高效 但 现有 模型 通常 
是 单向 的 这 意味着 只有 单词 的 左 或 
右 的 有序 序列 才会 被 考虑 但在 18年 10 
月份 Google AI 语言 团队 提出 的 深度 双向 变换器 
模型 不仅 解决 了 单向 模型 的 问题 而且 该 
模型 可实现 11种 NLP 任务 的 最 先进 性能 包括 
斯坦福 问答 SQUAD 数据集 强化 学习 也 在 自然 语言 
处理 上 取得 了 巨大 的 进步 因为 目前 的 
自然 语言 处理 大多 都是/nr 一个 离散 空间 的 自然 
语言 处理 生成 或者 是 序列 决策 这时 我们 很 
天然 地 可以 利用 到 强化 学习 去 拟合 和 
运作 强化 学习 下 的 几种 分类 在 自然 语言 
处理 的 任务 中 也都 取得 了 非常 优异 的 
成果 例如 value based RL 基于 价值 函数 policy based 
RL 基于 策略 的 函数 model based RL 基于 模型 
的 函数 2018年 在 各种 领域 被 搜索 的 最多 
的 词条 是 自然 语言 处理 NLP 和 生成 对抗 
网络 在 Github 上 最 流行 的 生成 对抗 网络 
方法 包括 vid2vid DeOldify CycleGAN and faceswaps 最 流行 的 
NLP 方法 包括 BERT HanLP jieba AllenNLP and fastText 3 
. 基本 理论 分析 3.1 序列 生成 技术 序列 可以 
采用 文本 音乐 分子 等 形式 序列 生成 技术 可以 
应用 于 多个 领域 包括 与 音乐 旋律 有关 的 
实验 和 计算 分子 生成 4 本节 主要 研究 文 
本生 成问题 因为 文本 生成 是 会话 响应 生成 机器翻译 
抽象 摘要 等 许多 NLP 问题 的 基础 最开始 文本 
生成 模型 通常 基于 n gram 前馈 神经 网络 或 
递归 神经网络 训练 它们 根据 前面 的 基础 真 词 
作为 输入 预测 下 一个 单词 生成 的 模型 用于 
整个 序列 然后 在 测试 中 使用 训练 的 模型 
逐字 生成 序列 并将 生成 的 单词 作为 输入 再 
对 这些 模型 进行 词 级 损失 的 训练 如 
交叉 熵 使 下 一个 词 的 概率 最大化 不过 
这些 模型 在 用于 生成 文本 时 存在 两个 主要 
缺陷 首先 他们 被 训练 预测 下 一个 单词 时 
给定 之前 的 基础 真 词 作为 输入 但是 在 
测试 时 通过 每次 预测 一个 单词 并在 下一次 步骤 
中将 生成 的 单词 作为 输入 返回 生成 的 模型 
用于 生成 整个 序列 这个 过程 非常 脆弱 因为 模型 
是 在 不同 的 输入 分布 上 进行 训练 的 
即从 数据分布 中 提取 的 单词 而 不是 从 模型 
分布 中 提取 的 单词 因此 在此 过程 中 所犯 
的 错误 将 迅速 累积 我们 把 这种 差异 称为 
暴露 偏差 即 一个 模型 只 暴露 于 训练 数据 
的 分配 而 不是 它 自己 的 预测 其次 用于 
训练 这些 模型 的 损失 函数 是 字 级 的 
一个 流行 的 选择 是 交叉 熵 损失 用来 最大化 
下 一个 正确 单词 的 概率 然而 这些 模型 的 
性能 通常 使用 离散 的 度量 来 评估 针对 这 
一 问题 现在 有 一种 新的 序列 级 训练 算法 
5 为 混合 增量 交叉 熵 增强 MIXER 因为 它 
结合 了 XENT 和 增强 还有 增量 学习 第一 个 
关键 思想 是 改变 增强 的 初始 策略 以 确保 
模型 能够 有效 地 处理 文本 生成 的 巨大 操作 
空间 而 不是 从 一个 糟糕 的 随机 策略 开始 
训练 模型 收敛 到 最优 策略 作者 从 最优 策略 
开始 然后 慢慢 偏离 它 让 模型 去 探索 和 
利用 它 自己 的 预测 第二 个 关键 思想 为 
在 退火 过程 中 引入 模型 预测 以 逐步 教会 
模型 生成 稳定 的 序列 这种 方法 可以 直接 优化 
测试 时 使用 的 度量 如 BLEU 或 ROUGE 在三 
个 不同 的 任务 上 方法 优于 贪婪 生成 的 
几个 强大 基线 当 这些 基线 使用 波束 搜索 时 
这种 方法 也 很有 竞争力 而且 速度 要快 几倍 Bahdanau 
et al . 2017 提出 了 一种 序列 预测 的 
actor critic 算法 6 以 改进 序列 级 训练 算法 
5 作者 利用 一个 评估 网络 来 预测 一个 token 
的 值 即 序列 预测 策略 下 的 期望 得分 
由 行动者 网络 定义 通过训练 来 预测 tokens 的 值 
采用 了 一些 技术 来 提高 性能 用 SARSA 而不是 
蒙特卡罗 方法 来 减少 估计值 函数 的 方差 稳定 的 
目标 网络 这种 训练 神经 网络 的 方法 是 使用 
来自 强化 学习 的 actor critic 方法来 生成 序列 结果 
显示 这种 方法 提升 了 在 合成 任务 synthetic task 
以及 德 英 机器翻译 任务 上 的 表现 作者 的 
分析 为 这样 的 方法 在 自然 语言 生成 任务 
上 的 应用 铺平 了 道路 比如 机器翻译 图片 描述 
生成 对话 模型 Bahdanau 提出 的 论文 有 两个 重要 
的 贡献 首先 它 描述 了 强化 学习 中 像 
actor critic 方法 这样 的 方法 能被/nr 应用于 带有 结构化 
输出 的 监督 学习 问题 上 然后 调查 了 新 
方法 在 合成 任务 以及 机器翻译 这样 的 真实世界 任务 
上 的 表现 与 行为 展示 了 由 actor critic 
带来 的 在 最大 似 然 方法 以及 REINFORCE 方法 
上 的 改进 生成 对抗 网 GAN 是 一种 新的 
生成 模型 训练方法 它 利用 判别 模型 来 指导 生成 
模型 的 训练 在 生成 实际 数据 方面 取得 了 
很大 的 成功 GAN 网络 在 计算机 视觉 上 已经 
得到 了 很好 的 应用 然而 其 在 自然 语言 
处理 上 并 不是 很 有效 最初 的 GAN 仅仅 
定义 在 实数 领域 GAN 通过训练 出 的 生成器 来 
产生 合成 数据 然后 在 合成 数据 上 运行 判别 
器 判别 器 的 输出 梯度 将会 告诉 你 如何 
通过 略微 改变 合成 数据 而 使其 更加 现实 然而 
当 目标 是 生成 离散 tokens 序列 时 它 有 
局限性 主要 原因 是 生成 模型 的 离散 输出 使得 
判别 模型 的 梯度 更新 难以 传递 到 生成 模型 
另外 判别 模型 只能 对 一个 完整 的 序列 进行 
评估 而 对于 一个 部分 生成 的 序列 一旦 生成 
了 整个 序列 要 平衡 其 当前 的 分数 和 
未来 的 分数 是 很 重要 的 例如 如果 你 
输出 了 一张 图片 其 像素 值 是 1.0 那么 
接下来 你 可以 将 这个 值 改为 1.0001 如果 输出 
了 一个 单词 penguin 那么 接下来 就 不能 将 其 
改变 为 penguin + . 001 因为 没有 penguin + 
. 001 这个 单词 因为 所有 的 自然 语言 处理 
NLP 的 基础 都是 离散 值 如 单词 字母 或者 
音节 NLP 中 应用 GANs 是 非常 困难 的 随后 
Yu 等人 也 提出 了 SeqGAN 解决 了 上述 问题 
即 具 有策略 梯度 的 序列 生成式 对抗 网络 7 
来 解决 这些 问题 将 数据 生成器 建模 为 强化 
学习 RL 中的 随机 策略 SeqGAN 通过 直接 执行 梯度 
策略 更 新来 绕过 生成器 的 微分 问题 RL 激励 
信号 来自 基于 完整 序列 判断 的 GAN 鉴别器 并 
通过 蒙特卡罗 搜索 返回 到 中间 状态 动作 步骤 在 
合成 数据 和 真实 任务 上 的 大量 实验 表明 
与 强大 的 基线 相比 它们 有了/nr 显著 的 改进 
SeqGAN 的 结构图 如 . 1 所示 右 D 是 
训练 完毕 真实 数据 和G是/nr 生成 的 数据 左 G 
是 受过 训练 根据 提供 最终 激励 信号 的 政策 
梯度 并 通过 它 传 递回 中间 操作 值 蒙特卡罗 
搜索 参考文献 1 . R . Collobert J . Weston 
L . Bottou M . Karlen K . Kavukcuoglu and 
P . Kuksa Natural language processing almost from scratch Journal 
of Machine Learning Research vol . 12 no . Aug 
pp . 2493 – 2537 2011 . 2 . Devlin 
J Chang M W Lee K et al . Bert 
Pre training of deep bidirectional transformers for language understanding J 
. arXiv preprint arXiv 1810.04805 2018 . 3 Young T 
Hazarika D Poria S et al . Recent trends in 
deep learning based natural language processing J . ieee Computational 
intelligenCe magazine 2018 13 3 55 75 . 4 Jaques 
N Gu S Bahdanau D et al . Sequence tutor 
Conservative fine tuning of sequence generation models with kl control 
J . arXiv preprint arXiv 1611.02796 2016 . 5 Ranzato 
M A Chopra S Auli M et al . Sequence 
level training with recurrent neural networks J . arXiv preprint 
arXiv 1511.06732 2015 . 6 Bahdanau D Brakel P Xu 
K et al . An actor critic algorithm for sequence 
prediction J . arXiv preprint arXiv 1607.07086 2016 7 Yu 
L Zhang W Wang J et al . SeqGAN Sequence 
Generative Adversarial Nets with Policy Gradient C / / AAAI 
. 2017 2852 2858 . 自然语言 处理 的 加持 与 
工业 设计 学习 了 大 数据 与 人工智能 的 课程 
知道 了 自然 语言 处理 是 研究 计算机 处理 人类 
语言 的 一门 技术 身为 一名 工业 设计 专业 的 
学生 我/r 觉得/v 自然语言/l 处理/v 能和/nr 工业/n 设计/vn 达到/v 一个/m 
质/ng 的/uj 结合/v 随着 科技 水平 的 提高 各个 领域 
都会 往 科技 方面 有所 偏向 而 工业 设计 旨在 
创新 为 人们 提升 生活 的 质量 因此 便利 和 
创新 是 在 工业 设计 领域 的 每 一个人 的 
追求 工业 设计 涉及 到 产品 系统 服务 体验 等等 
各个 方面 而 这些 方面 都 可以 加入 自然语言 处理 
这 门 技术 工业 设计 里 如果 融入 了 自然 
语言 处理 设计 出来 的 产品 也 会 更加 智能化 
更加 符合 人们 的 需求 在 设计 出 的 产品 
中 可能 不同 情景 下 有着 不同 的 表现 因此 
自然语言 处理 往往 可以 加入 到 产品 的 设计 中 
去 帮助 用户 理解 产品 的 语言 人群 的 多样性 
使得 产品 的 受众 人群 变得 单一 但 如果 加入 
自然语言 处理 产品 的 贴 合度 就会 得到 扩展 自然语言 
处理 指在 人与 计算机 之间 通过 自然 语言 进行 有效 
通信 的 各种 技术 和 方法 但 由于 语言 的 
复杂性 处理 过程 中 常常 会 涉及 理解 因此 被 
认为 是 距离 人工智能 最近 的 任务 其 理解 在 
工业 设计 中 也能 得到 充分 应用 如 产品 可以 
加入 语音 智能 通过 对 用户 语言 的 识别 通过 
自然 语言 的 处理 转换 为 另一种 自然语言 反馈 给 
用户 根据 每 一个 用户 的 不同 点 给予 每 
一个 用户 的 最优 的 反馈 让 产品 的 优势 
得以 最大化 的 展示 这 便是 自然语言 处理 的 在 
产品 设计 中 的 应用 优势 与其 相似 的 还有 
翻译 就是 把 输入 的 源语言 文本 通过 自动 翻译 
获得 另外 一种 语言 的 文本 如 文本 翻译 语音 
翻译 手语 翻译 图形 翻译 等 如果 设计 一台 翻译机 
加入 自然语言 处理 就能 让其 发挥 最大 优势 再者 还有 
问答 系统 就是 对 一个 自然 语言 表达 的 问题 
由 问答 系统 给予 一个 精准 的 答案 这 其中 
就 是 通过 自然 语言 的 处理 转化 的 还有 
就是 对话 系统 系统 通过 一 系列 的 对话 跟 
用户 进行 聊天 回答 完成 某项 任务 这 涉及 到 
用户 意图 理解 通用 聊天 引擎 问答 引擎 对话 管理 
等 技术 的 加持 这 就 可以 与 现在 火热 
的 私人 订制 相 结合 了 设计 出 一台 私人 
订制 机器 这些 运用 自然语言 处理 的 方面 无 不可以 
加载 到 工业 产品 上 与其 产生 完美 结合 科学技术 
日新月异 每一天 都 有着 巨大 变化 技术 与 技术 中 
有着 紧密联系 而 领域 与 领域 间 也 可以 相互 
加持 结合 让 世界 多元化 让 明天 更 美好 自然语言 
处理 是 计算机 科学 领域 与 人工智能 领域 中 的 
一个 重要 方向 它/r 研究/vn 能/v 实现/v 人/n 与/p 计算机/n 
之间/f 用/p 自然/d 语言/n 进行/v 有效/a 通信/l 的/uj 各种/r 理论/n 
和/c 方法/n 无论 实现 自然 语言 理解 还是 自然 语言 
生成 都 远不如 人们 原来 想象 的 那么 简单 而 
是 十分 困难 的 从/p 现有/b 的/uj 理论/n 和/c 技术/n 
现状/n 看/v 通用 的 高 质量 的 自然 语言 处理 
系统 仍然 是 较 长期 的 努力 目标 本人 的 
专业 是 电气工程 及其 自动化 其中 电气 工程 的 自动化 
就 涉及 到 自然 语言 处理 如果 在 不久 的 
将来 能够 将 自然 语言 处理 做 的 更加 成熟 
计算机 能够 通过 自然 语言 处理 实现 真正 意义 上 
的 人机交互 那么 便 可以 将 这一 成果 投入 到 
电气 工程 的 智能化 中 提高 人们 的 工作 效率 
和 生产 效率 概念 Natural Language Processing / Understanding 自然语言 
处理 / 理解 日常 对话 办公 写作 上网 浏览 希望 
机器 能 像人 一样 去 理解 以 人类 自然 语言 
为 载体 的 文本 所 包含 的 信息 并 完成 
一些 特定 任务 内容 中文分词 词性 标注 命名 实体 识别 
关系 抽取 关键词 提取 信息 抽取 依存 分析 词 嵌入 
应用 篇章 理解 文本 摘要 情感 分析 知识图谱 文本 翻译 
问答 系统 聊天 机器人 自然语言 处理 是 计算机 科学 领域 
与 人工智能 领域 中 的 一个 重要 方向 它 研究 
能 实现 人 与 计算机 之间 用 自然 语言 进行 
有效 通信 的 方法 在 如今 互联网 大 数据 与 
人工智能 迅速 发展 的 时代 背景 下 自然语言 处理 能够 
应用 的 场所 极大 地 扩大 除开 计算机领域 自然语言 处理 
与 大 数据 早已 逐渐 渗透 到 其他 各个 领域 
医学 方面 也 囊括 在内 近年来 医疗 数据挖掘 发展迅速 然而 
医疗 数据 结构化 处于 起步 阶段 更多 的 医疗 数据 
仍然 以 自然 语言 形式 出现 我们 可以 通过 自然 
语言 处理 技术 研制 出 一套 完整 的 系统 辅助 
完成 汇总 医学 领域 知识 的 过程 将 知识 提炼 
出来 提取 其中 有用 的 诊疗 信息 最终 形成 知识 
网络 比如说 人体 解剖 学生 可以 通过 完整 的 系统 
了解 人体 的 组织 器官 以及 各种 骨骼 同时 通过 
自然 语言 处理 学生 还 能 了解 他们 之间 的 
联系 然后 包括 各种 病症 与 病理 通过 这项 技术 
学生 也 能 系统 地 学习 自然语言 处理 为 医学 
知识 的 线上 学习 提供 了 可能 当然 这 是 
建立 在 技术 成熟 的 基础 之上 自然语言 处理 在 
临床 信息 系统 中 起着 作用 临床 信息 系统 能将 
文本 形式 的 病历 报告 转换成 编码数据 以供 使用 医生/n 
与/p 病人/n 方面/n 都能/nr 受益/v 通过 完善 医疗 系统 病人 
能 较早 地 知晓 自己 的 病情 为 救治 提供 
了 帮助 医生 能够 通过 这项 技术 调动 病人 的 
病历 以便 查看 此外 还 能 实时 观察 病人 的 
病情 为 治疗 带来 了 便利 自然语言 处理 复习 汇总 
南京大学 标签 空格 分隔 自然语言 处理 参考 书籍 统计 自然语言 
处理 – 宗 成庆 该 文档 用 markdown 编写 github 
地址 为 https / / github . com / lyfadvance 
/ nlp / blob / master / % E 8% 
87% AA % E 7% 84% B 6% E 8% 
AF % AD % E 8% A 8% 80% E 
5% A 4% 84% E 7% 90% 86% E 5% 
A 4% 8D % E 4% B 9% A 0% 
E 6% 96% 87% E 6% A 1% A3 . 
md 如果 想 继续 编写 可以 fork 自然语言 处理 复习 
汇总 南京 大学 统计 语言 模型 N GramNeural language modelword2vector 
文本 分类 朴素 贝叶斯 模型 Bernoulli document model 伯努利 文档 
模型 Multinomial document model 训练 句 向量 文本 或 句子 
向 量化 词 袋 模型 N Gram Bag of WordsTF 
IDFTF 词频 IDF 逆 文档 频率 特征 过滤 Distributional similarity 
based representations 词性 标注 与 隐 马尔科夫 模型 隐 马尔科夫 
模型 的 三个 基本问题 问题 1 问题 2 问题 3 
统计 语义分析 PCFG 概率 上下文 无关 文法 问题 1TreebankChomsky Normal 
Form 统计 机器翻译 统计 语言 模型 N GramN 1 阶 
马尔可夫 链 我们 称之为 N 元语言 模型 P wi | 
wi − 1 = P wi − 1wi P wi 
− 1 = Count wi − 1wi ∑ wCount wi 
− 1w P w _ i | w _ { 
i 1 } = \ dfrac { P w _ 
{ i 1 } w _ i } { P 
w _ { i 1 } } = \ dfrac 
{ Count w _ { i 1 } w _ 
i } { \ sum _ wCount w _ { 
i 1 } w } Count wi − 1wi Count 
w _ { i 1 } w _ i 由于 
稀疏 性 值 可能 等于 0 ． 从而 导致 整个 
句子 的 概率 都 等于 0 进行 平滑 处理 线性 
平滑 P wi | wi − 1 = P wi 
− 1wi P wi − 1 = Count wi − 
1wi + α ∑ w Count wi − 1w + 
α P w _ i | w _ { i 
1 } = \ dfrac { P w _ { 
i 1 } w _ i } { P w 
_ { i 1 } } = \ dfrac { 
Count w _ { i 1 } w _ i 
+ \ alpha } { \ sum _ w Count 
w _ { i 1 } w + \ alpha 
} laplace 平滑 P wi | wi − 1 = 
P wi − 1wi P wi − 1 = Count 
wi − 1wi + kP w ∑ wCount wi − 
1w + kP w _ i | w _ { 
i 1 } = \ dfrac { P w _ 
{ i 1 } w _ i } { P 
w _ { i 1 } } = \ dfrac 
{ Count w _ { i 1 } w _ 
i + kP w } { \ sum _ wCount 
w _ { i 1 } w + k } 
简单 线性插值 平滑 Neural language modelword2vector 文本 分类 朴素 贝叶斯 
模型 D 为 待 分类 的 文档 ckc _ k 
指 第 k 个 类别 argmaxckP ck | D = 
argmaxckP D | ck P ck P D = argmaxckP 
D | ck P ck argmax _ { c _ 
k } P c _ k | D = argmax 
_ { c _ k } \ dfrac { P 
D | c _ k P c _ k } 
{ P D } = argmax _ { c _ 
k } P D | c _ k P c 
_ k 1 . Bernoulli document model 伯努利 文档 模型 
一个 文档 被 表示 成 01 向量 . 向量 中 
每 一个 元素 表示 相应 的 单词 是否 在 文档 
中 出现 了 令 DiD _ i 表示 第 i 
个 文档 的 01 向量 令 DitD _ it 表示 
第 i 个 文档 的 01 向量 中 第 t 
个 元素 的 值 即 单词 wtw _ t 是否 
在 文档 i 中 出现 了 P wt | ck 
P w _ t | c _ k 表示 单词 
wtw _ t 在 类别 ckc _ k 中 出现 
的 文档 数 的 占 比 . 则 P wt 
| ck = ck 中 wt 出现 的 文档 个数 
ck 中 所有 文档 的 个数 P w _ t 
| c _ k = \ dfrac { c _ 
k 中 w _ t 出现 的 文档 个数 } 
{ c _ k 中 所有 文档 的 个数 } 
P Dit | ck = DitP wt | ck + 
1 − Dit 1 − P wt | ck P 
D _ { it } | c _ k = 
D _ { it } P w _ t | 
c _ k + 1 D _ { it } 
1 P w _ t | c _ k P 
Di | Ck = ∏ | V | t = 
1P Dit | ck P D _ i | C 
_ k = \ prod _ { t = 1 
} ^ { | V | } P D _ 
{ it } | c _ k 2 . Multinomial 
document model 一个 文档 被 表示 成 整数 向量 . 
向量 中 每 一个 元素 表示 相应 的 单词 在 
文档 中 出现 了 多少 次 令 DiD _ i 
表示 第 i 个 文档 的 向量 令 DitD _ 
{ it } 表示 第 i 个 文档 的 向量 
中 第 t 个 元素 的 值 P wt | 
ck P w _ t | c _ k 表示 
单词 wtw _ t 在 类别 ckc _ k 中 
出现 的 文档 数 的 占 比 . 训练 句 
向量 一般来讲 每 一个 类别 ckc _ k 也 可以 
看成 一个 向量 记为 f ck f c _ k 
文本 DiD _ i 也表示 成 向量 ww 训练 句 
向量 也就是 训练 打分 模型 score w f ck score 
w f c _ k 可以 根据 这个 设计 各种 
loss 函数 用 SVM 的 loss 函数 训练 文本 或 
句子 向 量化 词 袋 模型 0 1 向量 N 
Gram Bag of WordsVocab = set of all n grams 
in corpusDocument = n grams in document w . r 
. t vocab with multiplicityFor bigram Sentence 1 The cat 
sat on the hat Sentence 2 The dog ate the 
cat and the hat Vocab = { the cat cat 
sat sat on on the the hat the dog dog 
ate ate the cat and and the } Sentence 1 
{ 1 1 1 1 1 0 0 0 0 
0 } Sentence 2 { 1 0 0 0 0 
1 1 1 1 1 } TF IDFTF 词频 词频 
TF = 某个 词 在 文章 中 的 出现 次数 
文章 中 出现 最 多词 的 个数 词频 TF = 
\ dfrac { 某个 词 在 文章 中 的 出现 
次数 } { 文章 中 出现 最 多词 的 个数 
} IDF 逆 文档 频率 逆 文档 频率 IDF = 
log 语料库 的 文档 总数 包含 该词 的 文档 数 
+ 1 逆 文档 频率 IDF = \ log \ 
dfrac { 语料库 的 文档 总数 } { 包含 该词 
的 文档 数 + 1 } 特征 过滤 停用词 基于 
文档 频率 DF 的 特征 提 取法 从 训练 预料 
中 统计 出 包含 某个 特征 的 文档 的 频率 
个数 然后 根据 设定 的 阈值 当 该 特征 项的/nr 
DF 值 小于 某个 阈值 时 从 特征 空间 中 
去掉 该 特征 项 因为 该 特征 项使/nr 文档 出现 
的 频率 太低 没有 代表性 当 该 特征 项的/nr DF 
值 大于 另外 一个 阈值 时 从 特征 空间 中 
也 去掉 该 特征 项 因为 该 特征 项使/nr 文档 
出现 的 频率 太高 没有 区分度 信息 增 益法 信息 
增益 IG 法/l 依据/p 某/r 特征/n 项/n tit/w _/i i/w 
为/p 整个/b 分类/n 所能/v 提供/v 的/uj 信息/n 量/n 多少/m 来/v 
衡量/v 该/r 特征/n 项的/nr 重要/a 程度/n 从而 决定 对 该 
特征 项的/nr 取舍 某个/r 特征/n 项/n tit/w _/i i/w 的/uj 
信息/n 增益/n 是/v 指/n 有该/nr 特征/n 或/c 没有/v 该/r 特征/n 
时/n 为 整个 分类 所能 提供 的 信息量 的 差别 
其中 信息量 的 多少 由 熵 来 衡量 因此 信息/n 
增益/n 即/v 不/d 考虑/v 任何/r 特征/n 时/n 文档/n 的/uj 熵/g 
和/c 考虑/v 该/r 特征/n 后/f 文档/n 的/uj 熵/g 的/uj 差值/n 
Gain ti = Entropy S − Expected   Entropy Sti 
= { − ∑ j = 1MP Cj ⋅ logP 
Cj } − { P ti ⋅ − ∑ j 
= 1MP Cj | ti ⋅ logP Cj | ti 
      + P ti ¯ ⋅ − ∑ 
j = 1MP Cj | ti ¯ ⋅ logP Cj 
| ti ¯ } \ begin { aligned } Gain 
t _ i & = Entropy S Expected \ Entropy 
_ { t _ i } \ \ & = 
\ { \ sum _ { j = 1 } 
^ MP C _ j \ cdot \ log P 
C _ j \ } \ { P t _ 
i \ cdot \ sum _ { j = 1 
} ^ { M } P C _ j | 
t _ i \ cdot \ log P C _ 
j | t _ i \ \ & \ \ 
\ + P \ bar { t _ i } 
\ cdot \ sum _ { j = 1 } 
^ { M } P C _ j | \ 
bar { t _ i } \ cdot \ log 
P C _ j | \ bar { t _ 
i } \ } \ end { aligned } 其中 
P Cj P C _ j 表示 CjC _ j 
类 文档 在 预料 中 出现 的 概率 P ti 
P t _ i 表示 语料 中 包含 特征 项 
tit _ i 的 文档 的 概率 P Cj | 
ti P C _ j | t _ i 表示/v 
文档/n 包含/v 特征/n 项/n tit/w _/i i/w 时/n 属于/v CjC/w 
_/i j/w 类/q 的/uj 条件概率/i P ti ¯ P \ 
bar { t _ i } 表示 语 料中 不 
包含 特征 项 tit _ i 的 文档 的 概率 
P Cj | ti ¯ P C _ j | 
\ bar { t _ i } 表示/v 文档/n 不/d 
包含/v 特征/n 项/n tit/w _/i i/w 时/n 属于/v CjC/w _/i 
j/w 的/uj 条件概率/i MM 表示 类 别数 mutual information 互信息 
法 χ 2 \ chi ^ 2 统计量 Distributional similarity 
based r e p r e s e n t 
a t i o n s L I F i 
r s t P r o p o s e 
W o r d 2 v e c D o 
c 2 V e c 词性 标注 与 隐 马尔科夫 
模型 维 特比 算法 和 算法 AA 是 状态 转移 
概率 矩阵 BB 是 观测 概率 矩阵 π \ pi 
是 初始状态 概率 向量 隐 马尔科夫 模型 的 三个 基本问题 
概率 计算 问题 给定 模型 λ = A B π 
\ lambda = A B \ pi 和 观测 序列 
O = o1 o2 . . . oT O = 
o _ 1 o _ 2 . . . o 
_ T 计算 在 模型 λ \ lambda 下 观测 
序列 OO 出现 的 概率 P O | λ P 
O | \ lambda 学习 问题 . 已知 观测 序列 
O = o1 o2 . . . oT O = 
o _ 1 o _ 2 . . . o 
_ T . 估计 模型 λ = A B π 
\ lambda = A B \ pi 参数 使得 在 
该 模型 下 观测 序列 概率 P O | λ 
P O | \ lambda 最大 . 即用 极大 似 
然 估计 的 方法 估计 参数 . 预测 问题 也 
称为 解码 decoding 问题 已知 模型 λ = A B 
π \ lambda = A B \ pi 和 观测 
序列 O = o1 o2 . . . oT O 
= o _ 1 o _ 2 . . . 
o _ T 求 对 给定 观测 序列 条件概率 P 
I | O P I | O 最大 的 状态 
序列 I = i1 i2 . . . iT I 
= i _ 1 i _ 2 . . . 
i _ T . 即 给定 观测 序列 求 最 
有可能 的 对应 的 状态 序列 . 问题 1 前/f 
向/p 算法/n ./i 定义/n 前/f 向/p 概率/n 给定 隐 马尔科夫 
模型 λ \ lambda 定义 到 时刻 tt 部分 观 
测序 列为 o1 o2 . . . oto _ 1 
o _ 2 . . . o _ t 且 
状态 为 qiq _ i 的 概率 为 前 向 
概率 记作 α t i = P o1 o2 . 
. . ot it = qi | λ \ alpha 
_ t i = P o _ 1 o _ 
2 . . . o _ t i _ t 
= q _ i | \ lambda 输入 隐 马尔科夫 
模型 λ \ lambda 观测 序列 OO 输出 观测 序列 
概率 P O | λ P O | \ lambda 
1 初值 α 1 i = π ibi o1 i 
= 1 2 . . . N \ alpha _ 
1 i = \ pi _ ib _ i o 
_ 1 i = 1 2 . . . N 
2 递推 对 t = 1 2 T 1 α 
t + 1 i = ⎡ ⎣ ∑ j = 
1N α t j aji ⎤ ⎦ bi ot + 
1 i = 1 2 . . . N \ 
alpha _ { t + 1 } i = \ 
left \ sum _ { j = 1 } ^ 
N \ alpha _ t j a _ { ji 
} \ right b _ i o _ { t 
+ 1 } i = 1 2 . . . 
N 3 终止 P O | λ = ∑ i 
= 1N α T i P O | \ lambda 
= \ sum _ { i = 1 } ^ 
{ N } \ alpha _ T i 4 最优 
路径 回溯 后向 算法 定义 后向 概率 给定 隐 马尔科夫 
模型 λ \ lambda 定义 在 时刻 tt 状态 为 
qiq _ i 的 条件 下 从t+/nr 1t + 1 
到 TT 的 部分 观 测序 列为 ot + 1 
ot + 2 . . . oTo _ { t 
+ 1 } o _ { t + 2 } 
. . . o _ T 的 概率 为 后向 
概率 记作 β t i = P ot + 1 
ot + 2 . . . oT | it = 
qi λ \ beta _ t i = P o 
_ { t + 1 } o _ { t 
+ 2 } . . . o _ T | 
i _ t = q _ i \ lambda 输入 
隐 马尔可夫 模型 λ \ lambda 观测 序列 OO 输出 
观测 序列 概率 P O | λ P O | 
\ lambda 1 β T i = 1 i = 
1 2 . . . N \ beta _ T 
i = 1 i = 1 2 . . . 
N 2 对 t = T − 1 T − 
2 . . . 1t = T 1 T 2 
. . . 1 β t i = ∑ j 
= 1Naijbj ot + 1 β t + 1 j 
i = 1 2 . . . N \ beta 
_ t i = \ sum _ { j = 
1 } ^ { N } a _ { ij 
} b _ j o _ { t + 1 
} \ beta _ { t + 1 } j 
i = 1 2 . . . N 3 P 
O | λ = ∑ i = 1N π ibi 
o1 β 1 i P O | \ lambda = 
\ sum _ { i = 1 } ^ N 
\ pi _ ib _ i o _ 1 \ 
beta _ 1 i 问题 2Baum Welch 算法 无 监督 
学习 方法 假设 给定 训练 数据 只 包含 SS 个 
长度 为 TT 的 观测 序列 O1 O2 . . 
. OS { O _ 1 O _ 2 . 
. . O _ } 而 没有 对应 的 状态 
序列 目标 是 学习 隐 马尔科夫 模型 λ = A 
B π \ lambda = A B \ pi 的 
参数 我们 将 观测 序列 数据 看做 观测 数据 OO 
状态 序列 数据 看 做 不可 观测 的 隐 数据 
II 那么 隐 马尔科夫 模型 事实上 是 一个 含有 隐 
变量 的 概率模型 P O | λ = ∑ IP 
O | I λ P I | λ P O 
| \ lambda = \ sum _ IP O | 
I \ lambda P I | \ lambda 它 的 
参数 学习 可以 由 EMEM 算法 实现 参数估计 问题是 HMM 
面临 的 第三 个 问题 即 给定 一个 观察 序列 
O = O1O2 . . . OTO = O _ 
1O _ 2 . . . O _ T 如何 
调节 模型 u = A B π u = A 
B \ pi 的 参数 使得 P O | u 
P O | u 最大化 argmaxuP Otraining | u arg 
\ underset { u } { max } P O 
_ { training } | u 模型 的 参数 是 
指 构成 uu 的 π i aij bj k \ 
pi _ i a _ { ij } b _ 
j k . 最大 似 然 估计 方法 可以 作为 
HMM 参数估计 的 一种 选择 如果 产生 观察 序列 OO 
的 状态 序列 Q = q1q2 . . . qTQ 
= q _ 1q _ 2 . . . q 
_ T 已知 根据 最大 似 然 估计 HMM 的 
参数 可以 通过 如下 公式 计算 π ¯ i = 
δ q1 si \ bar { \ pi } _ 
i = \ delta q _ 1 s _ i 
a/w ¯/i ij/w =/i Q/w 中/f 从/p 状态/n qi/w 转移到/i 
qj/w 的/uj 次数/n Q/w 中/f 所有/b 从/p 状态/n qi/w 转移/v 
到/v 另一/i 状态/n 包括 qi 自身 的 次数 = ∑ 
T − 1t = 1 δ qt si ∗ δ 
qt + 1 sj ∑ T − 1t = 1 
δ qt si \ begin { aligned } \/i bar/w 
{/i a/w }/i _/i {/i ij/w }/i &/i =/i \/i 
dfrac/w {/i Q/w 中/f 从/p 状态/n q/w _/i i/w 转移到/i 
q/w _/i j/w 的/uj 次数/n }/i {/i Q/w 中/f 所有/b 
从/p 状态/n q/w _/i i/w 转移/v 到/v 另一/i 状态/n 包括 
q _ i 自身 的 次数 } \ \ & 
= \ dfrac { \ sum _ { t = 
1 } ^ { T 1 } \ delta q 
_ t s _ i * \ delta q _ 
{ t + 1 } s _ j } { 
\ sum _ { t = 1 } ^ { 
T 1 } \ delta q _ t s _ 
i } \ end { aligned } b ¯ j 
k = Q 中 从 状态 qj 输出 符号 vk 
的 次数 Q 到达 qj 的 次数 \ bar { 
b } _ j k = \ dfrac { Q 
中 从 状态 q _ j 输出 符号 v _ 
k 的 次数 } { Q 到达 q _ j 
的 次数 } 但 实际上 由于 HMM 中 的 状态 
序列 Q 是 观察 不到 的 隐 变量 因此 这种 
最大 似 然 估计 的 方法 不 可行 所幸 的 
是 期望 最大化 expectation maximization EM 算法 可以 用于 含有 
隐 变量 的 统计 模型 的 参数 最大 似 然 
估计 其 基本 思想 是 初始 时 随机 地 给 
模型 的 参数 赋值 该 复制 遵循 模型 对 参数 
的 限制 例如 从/p 某一/i 状态/n 出发/v 的/uj 所有/b 转移/v 
概率/n 的/uj 和为1/nr 给 模型 参数 赋 初值 以后 得到 
模型 u0u _ 0 然后 根据 u0u _ 0 可以 
得到 模型 中 隐 变量 的 期望值 例如 从u_/nr 0/m 
得到/v 从/p 某一/i 状态/n 转移/v 到/v 另一/i 状态/n 的/uj 期望/v 
次数/n 用 期望 次数 来 替代 上式 中 的 实际 
次数 这样 可以 得到 模型 参数 的 新 估计值 由此 
得到 新的 模型 u1u _ 1 . 从 u1u _ 
1 又 可以 得到 模型 中 隐 变量 的 期望值 
然后 重新 估计 模型 的 参数 执行 这个 迭代 过程 
知道 参数 收敛 于 最大 似 然 估计值 . 问题 
3 维 特比 算法 其实 就是 前 向 算法 的 
变种 形式 输入 隐 马尔科夫 模型 λ \ lambda 观测 
序列 OO 输出 最优 路径 I ∗ = i ∗ 
1 i ∗ 2 . . . i ∗ T 
I ^ * = i _ 1 ^ * i 
_ 2 ^ * . . . i _ T 
^ * 1 初值 α 1 i = π ibi 
o1 i = 1 2 . . . N \ 
alpha _ 1 i = \ pi _ ib _ 
i o _ 1 i = 1 2 . . 
. N ψ 1 i = 0 \ psi _ 
1 i = 0 2 递推 对 t = 1 
2 T 1 α t + 1 i = max1 
≤ j ≤ N ⎡ ⎣ ∑ j = 1N 
α t j aji ⎤ ⎦ bi ot + 1 
i = 1 2 . . . N \ alpha 
_ { t + 1 } i = \ underset 
{ 1 \ le j \ le N } { 
max } \ left \ sum _ { j = 
1 } ^ N \ alpha _ t j a 
_ { ji } \ right b _ i o 
_ { t + 1 } i = 1 2 
. . . N ψ t + 1 i = 
argmax1 ≤ j ≤ N ⎡ ⎣ ∑ j = 
1N α t j aji ⎤ ⎦ i = 1 
2 . . . N \ psi _ { t 
+ 1 } i = arg \ underset { 1 
\ le j \ le N } { max } 
\ left \ sum _ { j = 1 } 
^ N \ alpha _ t j a _ { 
ji } \ right i = 1 2 . . 
. N 3 终止 P ∗ = max1 ≤ i 
≤ N α T i P ^ * = \ 
underset { 1 \ le i \ le N } 
{ max } \ alpha _ T i i ∗ 
T = argmaxi ≤ i ≤ N α T i 
i _ T ^ * = arg \ underset { 
i \ le i \ le N } { max 
} \ alpha _ T i 统计 语义分析 PCFG 概率 
上下文 无关 文法 三个 基本问题 给定 一个 句子 W = 
w1w2 . . . wnW = w _ 1w _ 
2 . . . w _ n 和 文法 GG 
如何 快速 计算 概率 P W | G P W 
| G 给定 一个 句子 W = w1w2 . . 
. wnW = w _ 1w _ 2 . . 
. w _ n 和 文法 GG 如何 选择 该 
句子 的 最佳 结构 即 选择 句法结构 树 tt 使其 
具有 最 大 概率 argmaxtP t | W G argmax 
_ tP t | W G 给定 PCFG G 和 
句子 W = w1w2 . . . wnW = w 
_ 1w _ 2 . . . w _ n 
如何 调节 G 的 概率 参数 使 句子 的 概率 
最大 即 求解 argmaxGP W | G argmax _ GP 
W | G 问题 1 内向 算法 和 外向 算法 
内向 算法 的 基本 思想 是 利用 动态规划 算法 计算 
非 终结符 AA 推导 出 WW 中 子串 wiwi + 
1 . . . wjw _ iw _ { i 
+ 1 } . . . w _ j 的 
概率 aij A a _ { ij } A 有 
递推公式 如下 aii A = P A − wi a 
_ { ii } A = P A w _ 
i aij A = ∑ B C ∑ i ≤ 
k ≤ j − 1P A − BC ⋅ aik 
B ⋅ a k + 1 j C a _ 
{ ij } A = \ sum _ { B 
C } \ sum _ { i \ le k 
\ le j 1 } P A BC \ cdot 
a _ { ik } B \ cdot a _ 
{ k + 1 j } C 算法 如下 输入 
PCFG G S 和 句子 W = w1w2 . . 
. wnW = w _ 1w _ 2 . . 
. w _ n 输出 aij A 1 ≤ i 
≤ j ≤ na _ { ij } A 1 
\ le i \ le j \ le n 步1/nr 
初始化 aii A = P A → wi 1 ≤ 
i ≤ na _ { ii } A = P 
A \ rightarrow w _ i 1 \ le i 
\ le n 步2/nr 归纳 计算 j = 1 . 
. . n i = 1 . . . n 
− jj = 1 . . . n i = 
1 . . . n j 重复 下列 计算 ai 
i + j A = ∑ B C ∑ i 
≤ k ≤ i + j − 1P A → 
BC ∗ aik B ∗ a k + 1 i 
+ j C a _ { i i + j 
} A = \ sum _ { B C } 
\ sum _ { i \ le k \ le 
i + j 1 } P A \ rightarrow BC 
* a _ { ik } B * a _ 
{ k + 1 i + j } C 步3/nr 
终结 P → w1w2 . . . wn = a1n 
S P \ rightarrow w _ 1w _ 2 . 
. . w _ n = a _ { 1n 
} S 外向 算法 的 基本 思想 是 定义 外向 
变量 β ij A \ beta _ { ij } 
A 为 初始 非 终结符 SS 在 推导 出 语句 
W = w1w2 . . . wnW = w _ 
1w _ 2 . . . w _ n 的 
过程 中 产生 符号串 w1 . . . wi − 
1Awj + 1 . . . wnw _ 1 . 
. . w _ { i 1 } Aw _ 
{ j + 1 } . . . w _ 
n 的 概率 有 如下 递推公式 β 1n A = 
{ 10A = SA ≠ \ beta _ { 1n 
} A = \ left \ { \ begin { 
array } { rcl } 1 & & { A 
= } \ \ 0 & & { A \ 
neq } \ \ \ end { array } \ 
right . β ij A = ∑ B C ∑ 
k jP B → AC α j + 1 k 
C β ik B       + ∑ B 
C ∑ k iP B → CA α k i 
− 1 C β kj B \ begin { aligned 
} % requires amsmath align * for no eq . 
number \ beta _ { ij } A & = 
\ sum _ { B C } \ sum _ 
{ k \ gt j } P B \ rightarrow 
AC \ alpha _ { j + 1 k } 
C \ beta _ { ik } B \ \ 
& \ \ \ + \ sum _ { B 
C } \ sum _ { k \ lt i 
} P B \ rightarrow CA \ alpha _ { 
k i 1 } C \ beta _ { kj 
} B \ \ \ end { aligned } 问题 
2 就是 将 内向 算法 的 递推 式 取 最大 
aii A = P A → wi a _ { 
ii } A = P A \ rightarrow w _ 
i aij A = argmaxB C ∈ N i ≤ 
k ≤ i + jP A → BC ⋅ aik 
B ⋅ a k + 1 j C a _ 
{ ij } A = arg \ underset { B 
C \ in N i \ le k \ le 
i + j } { max } P A \ 
rightarrow BC \ cdot a _ { ik } B 
\ cdot a _ { k + 1 j } 
C 然后 用 变量 β ij \ beta _ { 
ij } 记忆 子串 wi . . . wjw _ 
i . . . w _ j 的 维 特比 
句法 分析树 β ij A = argmaxB C ∈ N 
i ≤ k ≤ i + jP A → BC 
⋅ aik B ⋅ a k + 1 j C 
\ beta _ { ij } A = arg \ 
underset { B C \ in N i \ le 
k \ le i + j } { max } 
P A \ rightarrow BC \ cdot a _ { 
ik } B \ cdot a _ { k + 
1 j } C TreebankChomsky Normal Form 统计 机器翻译 要点 
该 教程 为 基于 python + opencv 的 图像 目标 
区域 自动 提取 实现 自动 提取 一 张照 片中 的 
纸张 内容 环境 配置 Wn10 + CPU i7 6 7 
0 0 P y c h a r m 2 
0 1 8 o p e n c v python 
3 . 4.2 . 17numpy 1 . 14.5 笔者 信息 
Next _ Legend QQ 1219154092 人工智能 自然语言 处理 图像处理 神经网络 
2018 . 8.12 于 天津 大学 该 项目 的 代码 
在 笔者 的 资源 仓库 中 代码 地址 基于 python 
+ opencv 的 图像 目标 区域 自动 提取 一 项目 
背景 一张 照片 中的 感兴趣 区域 总 是 沿着 x 
y z/w 三个/m 轴/n 都有/nr 一定/d 倾斜/v 如 下图 要想 
把 照片 翻 转到 平行 位置 需要 进行 透视 变换 
而 透视 变换 需要 同一 像素点 变换 前后 的 坐标 
由此 可以 想到 提取 矩形 区域 四个 角 的 坐标 
作为 变换 前 的 坐标 变换 后的/nr 坐标 可以 设为 
照片 的 四个 角落 经过 投影变换 矩形 区域 将会 翻转 
并 充满 图像 因此 我们 要 解决 的 问题 变为 
提取 矩形 的 四个 角落 进行 透视 变换 二 提取 
矩形 角落 坐标 矩形 的 检测 主要 是 提取 边缘 
图片 显示 部分 的 亮度 通常 高于 周围环境 我们 可以 
将 图片 阈值 化 将 图片 部分 与 周围 环境 
明显 的 分别 开来 这对 后边 的 边缘 检测 非常 
有 帮助 检测 矩形 并 提取 坐标 需要 对 图像 
进行 预处理 边缘 检测 提取 轮廓 检测 凸包 角 点 
检测 1 预处理 转为 灰度 图 由于 手机 拍摄 的 
照片 像素 可能 会 很高 为了 加快 处理速度 我们 首先 
将 图像 转化 为 灰度 图 image = cv2 . 
imread Config . src gray = cv2 . cvtColor image 
cv2 . COLOR _ BGR2GRAY srcWidth srcHeight channels = image 
. shape print srcWidth srcHeight 2 中值 滤波 binary = 
cv2 . medianBlur gray 7 3 转化 为 二 值 
图像 ret binary = cv2 . threshold binary Config . 
threshold _ thresh 255 cv2 . THRESH _ BINARY cv2 
. imwrite 1 threshold . png binary int cv2 . 
IMWRITE _ PNG _ COMPRESSION 9 此时 图片 已经 变成 
了 这个 样子 可见 纸张 页面 部分 已经 与 背景 
环境 分离 开来 4 边缘 检测 与 轮廓 处理 我们 
用 Canny 算子 边缘 检测 提取 轮廓 # canny 提取 
轮廓 binary = cv2 . Canny binary 0 60 apertureSize 
= 3 cv2 . imwrite 3 canny . png binary 
int cv2 . IMWRITE _ PNG _ COMPRESSION 9 提取 
轮廓 后 拟合 外接 多边形 矩形 # 提取 轮廓 后 
拟合 外接 多边形 矩形 _ contours _ = cv2 . 
findContours binary cv2 . RETR _ EXTERNAL cv2 . CHAIN 
_ APPROX _ SIMPLE print len contours = % d 
% len contours 5 提取 面积 最大 的 轮廓 并用 
多边形 将 轮廓 包围 for idx c in enumerate contours 
if len c Config . min _ contours continue epsilon 
= Config . epsilon _ start while True approx = 
cv2 . approxPolyDP c epsilon True print idx epsilon len 
approx len c = % d % d % d 
% d % idx epsilon len approx len c if 
len approx 4 break if math . fabs cv2 . 
contourArea approx Config . min _ area if len approx 
4 epsilon + = Config . epsilon _ step print 
epsilon = % d count = % d % epsilon 
len approx continue else # for p in approx # 
cv2 . circle binary p 0 0 p 0 1 
8 255 255 0 thickness = 1 approx = approx 
. reshape 4 2 # 点 重 排序 top left 
top right bottom right bottom left src _ rect = 
order _ points approx cv2 . drawContours image c 1 
0 255 255 1 cv2 . line image src _ 
rect 0 0 src _ rect 0 1 src _ 
rect 1 0 src _ rect 1 1 color = 
100 255 100 cv2 . line image src _ rect 
2 0 src _ rect 2 1 src _ rect 
1 0 src _ rect 1 1 color = 100 
255 100 cv2 . line image src _ rect 2 
0 src _ rect 2 1 src _ rect 3 
0 src _ rect 3 1 color = 100 255 
100 cv2 . line image src _ rect 0 0 
src _ rect 0 1 src _ rect 3 0 
src _ rect 3 1 color = 100 255 100 
# 获取 最小 矩形 包络 rect = cv2 . minAreaRect 
approx # rect = cv2 . maxAreaRect approx box = 
cv2 . boxPoints rect box = np . int0 box 
box = box . reshape 4 2 box = order 
_ points box print approx box print approx print src 
_ rect print box w h = point _ distance 
box 0 box 1 \ point _ distance box 1 
box 2 print w h = % d % d 
% w h 6 透视 变换 dst _ rect = 
np . array 0 0 w 1 0 w 1 
h 1 0 h 1 dtype = float32 M = 
cv2 . g e t P e r s p 
e c t i v e T r a n 
s f o r m src _ rect dst _ 
rect warped = cv2 . warpPerspective image M w h 
cv2 . imwrite transfer % d . png % idx 
warped int cv2 . IMWRITE _ PNG _ COMPRESSION 9 
break 总结 本 项目 利用 了 照片 背景 亮度 较高 
的 特点 通过 二 值 化 突出 轮廓 提取 坐标 
进行 透视 变换 但是 局限性 在于 如果 矩形 的 亮度 
与 背景 相差 不大 就 很难 用 这种 方法 检测 
到 轮廓 还需要 算法 优化 该 项目 的 代码 在 
笔者 的 资源 仓库 中 代码 地址 基于 python + 
opencv 的 图像 目标 区域 自动 提取 清华大学 自然语言 处理 
与 社会 人文 计算 实验室 清华大学 智能 技术 与 系统 
国家 重点 实验室 信息检索 组 北京大学 计算 语言学 教育部 重点 
实验室 北京大学 计算机 科学 技术 研究所 语言 计算 与 互联网 
挖掘 研究室 哈工大 社会 计算 与 信息检索 研究中心 哈工大 机器 
智能 与 翻译 研究室 哈尔滨工业大学 智能 技术 与 自然 语言 
处理 实验室 中科院计算所 自然语言 处理 研究组 中科院 自动化 研究所 语音 
语言 技术 研究组 南京 大学 自然 语言 处理 研究组 东北 
大学 自然 语言 处理 实验室 厦门大学 智能科学 与 技术系 自然语言 
处理 实验室 苏州 大学 自然 语言 处理 实验室 郑州 大学 
自然 语言 处理 实验室 中科院 自动化所 模式识别 实验室 NLPR Huawei 
Noah s Ark LabHuman Language Technology Center at Hong Kong 
University of Science & TechnologyNUS Natural Language Processing GroupThe Stanford 
Natural Language Processing GroupThe Berkeley NLP GroupNatural Language Processing research 
at Columbia U n i v e r s i 
t y N a t u r a l Language 
and Information Processing Research Group at University of CambridgeSpeech Research 
Group at University of CambridgeThe Language Technologies Institute LTI at 
Carnegie Mellon UniversityThe Computational Linguistics Group at Oxford UniversityHuman Language 
Technology and Pattern Recognition Group at the RWTH AachenAlgorithms for 
Computational Linguistics at City University of New YorkRPI Blender LabThe 
Natural Language Group at USC / ISINatural Language Processing Group 
at University of Notre DameArtificial Intelligence Research Group at HarvardNatural 
Language Processing Research at GoogleThe Redmond based Natural Language Processing 
g r o u p C o m p u 
t a t i o n a l Linguistics and 
Information Processing at MarylandLanguage and Speech Processing at Johns Hopkins 
UniversityHuman Language Technology Center of Excellence at Johns Hopkins U 
n i v e r s i t y t 
a t i s t i c a l Machine 
Translation Group at the University of E d i n 
b u r g h U n i v e 
r s i t y of Sheffield NLP GroupThe CNGL 
Centre for Global Intelligent ContentCornell NLP groupNatural Language Processing NLP 
group at University Of WashingtonNLP @ Illinois 搜狗 公司 百度 
公司 现任 副 总裁 王海峰 先生 是 自然 语言 处理 
领域 世界 上 影响力 最大 也 最具 活力 的 国际 
学术 组织 ACL Association for Computational Linguistics 50 多年 历史 
上 唯一 的 华人 主席 科大 讯 飞 国内 专业 
做 中文 语音 文字 产品 研发 的 企业 是 目前 
国内 最大 的 智能 语音 技术 提供 商 参考 知乎 
warrioR _ wx CSDN 博客 观点 情感 以及 与 之 
相关 的 许多 概念 如 评价 评估 态度 感情 情绪 
和 心情 与 我们 主观 的 感觉 和 感受 密切相关 
这些 是 人类 心理 活动 的 核心 要素 也是 影响 
人们 日常 行为 的 关键 因素 情感 分析 也 称为 
观点 挖掘 是/v 一个/m 旨在/v 利用/n 可计算/i 的/uj 方法/n 从/p 
自然语言/l 文本/n 中/f 提取/v 观点/n 和/c 情感/n 信息/n 的/uj 研究/vn 
课题/n 一 . 情感 分析 其 伴随 着 网络 社会 
媒体 如 评论 论坛 博客 和 微博 的 兴起 而 
快速 发展 情感 分析 研究 的 目标 是从 文本 中 
分析 出 人们 对于 实体 及其 属性 所 表达 的 
观点 情感 评价 态度 和 情绪 这些 实体 可以 是 
各种 产品 服务 机构 个人 事件 问题 或 主题 等 
包含 很多 相关 研究 任务 例如 情感 分析 观点 挖掘 
观点 分析 观点 信息 抽取 情感 挖掘 主观性 分析 倾向性 
分析 情绪 分析 以及 评论 挖掘 从 自然语言 处理 的 
角度 看 情感 分析 的 人物 就是 识别 人们 谈论 
的 主题 以及 针对 主题 所 表达 出来 的 观点 
倾向 因此 它 常被 看成 一个 语义分析 任务 的 子 
问题 情感 分析研究 可 划分 三个 级别 篇章 级 句子 
级 和 属性 级 情感 词典 承载 情感 信息 最 
重要 的 基本 单元 是 情感 词 也 称为 观点 
词 但是 仅仅 依靠 它们 对 构建 精准 的 情感 
分析 系统 远远不够 辩论 和 评论 分析 意图 挖掘 意图 
就是 一个 人 或者 一群人 试图 遵循 的 行动 步骤 
尽管 意图 与 情感 是 两个 不同 的 概念 但是 
它们 有 许多 相关 点 第一 在 一个 含 有意图 
倾向 的 句子 中 作者 通常 会 表达 对于 某一 
事物 或 实体 的 情感 或 情绪 第二 当 一个 
人 非常 想得到 某一 东西 的 时候 他 通常 会 
对 这个 东西 表达 褒义 的 情感 第三 有些 观点 
是 通过 描述 意图 的 方式 表达 出来 的 垃圾 
观点 检测 与 评论 质量 社会 媒体 的 一个 关键 
特点 就是 允许 每个人 在 任何 时间 任何 地点 以 
匿名 的 方式 自由 地 表达 自己 的 想法 和 
观点 而 不必 害怕 自己 的 真正 身份 被 泄露 
也 不必 担心 这些 言论 会 让 自己 招致 麻烦 
尽管 这些 观点 和 想法 对于 很多 应用 来说 十分 
有价值 但是 这种 匿名 的 方式 是 有 代价 的 
这种 代价 就是 使得 那些 存有 不良 目的 或 隐藏 
企图 的 人 可以 通过 发表 虚假 评论 的 方式 
欺骗 情感 分析 系统 对 某种 产品 服务 机构 和 
个人 进行 蓄意 的 夸奖 或 贬低 而 不必 暴露 
其 真正 的 目的 这种 发表 虚假 评论 的 个体 
被称为 垃圾 观点 发布者 这种 行为 被 称为 垃圾 观点 
发布 二 . 什么 是 情感 分析 情感 分析 主要 
研究 那些 表达 或 暗示 褒义 或 贬义 情感 的 
观点 信息 这里 观点 是 一个 广义 的 概念 包括 
了 情感 评估 评价 态度 以及 其他 相关 信息 包括 
观点 持有者 和 观点 评价 对象 观点 情感 与 目标 
一个 观点 有 两个 重要 组成部分 一个 是 观点 评价 
的 对象 或 目标 g 另 一个 是 针对 该 
目标 所 表达 的 情感 s g s 中的 g 
可以 是 一个 实体 也 可以 是 所 评价 实体 
的 某个 属性 或 一个 侧面 s 是 一个 正面 
褒义 负面 贬义 或 中立 的 情感 倾向 或 打分 
正面 褒义 负面 贬义 中立 则 称为 情感 或 观点 
倾向 极性 可以 把 观点 定义 为 一个 四元组 g 
s h t h 是 观点 持有者 t 是 时间 
情感 对象 观点 所 评价 的 实体 实体 的 一部分 
或 实体 的 一个 属性 观点 中的 情感 情感 是 
观点 中所 蕴含 的 感受 态度 评价 或 情绪 通常 
情感 由 一个 三元组 表示 y o i 其中 y 
是 情感 类型 分 理性 和 感性 o 是 情感 
的 倾向 正面 负面 或 中立 i 是 情感 的 
强度 情感 评分 简化 的 观点 定义 上述 观点 的 
定义 虽然 简练 但 很难 应用 于 实际 操作 从/p 
文本/n 中/f 识别/v 出/v 实体/n 不同/a 层次/m 上/f 的/uj 组件/n 
和/c 属性/n 是/v 很/zg 困难/an 的/uj 任务/n 其实 大多数 应用 
并 不 需要 如此 复杂 的 分析 因此 我们 可以 
简化 之前 对于 观点 评价 对象 的 定义 其 层次结构 
只有 2层 同时 我们 使用 属性 或 方面 这个词 来 
指代 目标 实体 的 组件 和 参数 在 这颗 简化 
的 树 中 根 节点 依然 是 实体 本身 第二层 
叶 子层 的 节点 是 该 实体 的 不同 属性 
重新 定义 观点 的 概念 五 元组 e a s 
h t 其中 e 是 观点 评价 的 目标 实体 
a 是 实体 e 中 一个 观点 评价 的 实体 
属性 s 是 对 实体 e 的 a 属性 的 
观点 中所 包含 的 情感 h 是 观点 持有者 t 
是 观点 发布 时间 基于 此 定义 的 情感 分析 
常 称为 基于 属性 的 情感 分析 情感 分析 的 
目标 给定 一个 包含 观点 信息 的 文档 d 找出 
d 中 所有 的 观点 五 元组 对于 更 高级 
的 分析 需求 还要 找 出 每个 观点 五 元组 
中 情感 的 原因 和 限定 条件 情感 分析 的 
关键 任务 实体 消解 或者 实体 聚 类 观点 的 
不同 类型 常规 型 观点 和 比较 型 观点 三 
. 文 档级 情感 分类 任务 的 目标 是 将 
一篇 给定 观点 的 文档 如 产品 评论 根据 所持 
观点 为 正面 或 负面 进行 分类 定义 是 给定 
针对 一个 实体 的 观点 文档 d 判断 观点 持有者 
对 实体 的 整体 的 观点 倾向性 s 大多数 现有 
的 技术 都是/nr 基于 监督 学习 的 也 有 一些 
无 监督 学习 的 方法 现有 大多数 技术 都是 特征 
工程 加 机器学习 算法 在 实际 中 的 直接 应用 
但 目前 还 没有 工作 对于 这些 既有 方法 的 
有效性 和 准确性 进行 全面的 独立 的 评测 和 比较 
3.1 基于 监督 的 情感 分类 本节 提到 两类 分类 
方法 1 使用 一个 标准 的 有 监督 机器学习 算法 
进行 情感 分类 2 使用 一个 专为 情感 分类 设计 
的 分类 方法 基于 机器学习 算法 的 情感 分类 情感 
分类 的 关键 还是 抽取 有效 的 特征 一些 特征 
样例 词 和 词频 带有 词频 信息 的 单独 的 
词 袋 及 与其 相关 的 n gram 词性 研究 
表明 形容词 是 观点 和 情感 的 主要 承载 词 
http / / www . cis . upenn . edu 
~ treebank / home . html 情感 词 和 情感 
短语 大多 情感 词 都是 形容词 或 副词 观点 的 
规划 文本 结构 或 语音 成分 可以 表示 或 隐含 
情感 和 观点 情感 转置 词 有的 表达 可以 反转 
文本 中 的 情感 倾向 句法 依存关系 3.2 基于 无 
监督 的 情感 分类 使用 句法 模板 和 网页 检索 
的 情感 分类 使用 情感 词典 的 情感 分类 四 
. 句子 级 主 客观 和 情感 分类 文档 级别 
的 情感 分类 对 实际 应用 来说 还是 太 粗糙 
句子 级 其 目标 是 识别 每个 观点 文档 中 
的 句子 中 所 包含 的 情感 倾向 判断 每个 
句子 包含 的 正面 负面 还是 中性 的 情感 这 
离 实际 应用 的 情感 分类 系统 的 需求 更进一步 
即 提取 针对 每个 评论 对象 的 观点 信息 因为 
句子 太短 从而 包含 的 信息 也 少 得多 因此 
句子 级别 的 情感 分类 要 更加 困难 大多数 文档 
级别 的 情感 分类 论文 都 忽略 中性 类 主要 
是 做 准确 的 三类 分类 太难 了 但是 对于 
句子 级别 的 情感 分类 中性 类 就不 可以 忽略 
了 句子 级别 分类 有个 潜在 的 假设 是 一个 
句子 只 表达 了 一个 观点 即 只 包含 一个 
中 情感 句子 级 情感 分类 处理 条件句 处理 讽刺 
句 跨语言 主客观 分类 和 情感 分类 在 情感 分类 
中使 用语 篇 信息 句子 级 情绪 分类 文章 目录 
词 处理 语句 处理 篇章 处理 当前 热点 统计 语言 
模型 分词 语料库 词性 标注 句法分析 语料库 多 机加工 系统 
词语 搭配 识别 技术 N Gram 统计模型 平滑 方法 动态 
自适应 基于 缓存 的 语言 模型 马尔科夫 模型 隐 马尔科夫 
模型 基于 HMM 的 词性 标注 句法分析 自然语言 处理 计算机 
语言学 自然语言 理解 涉及 字处理 词 处理 语句 处理 篇章 
处理 词 处理 分词 词性 标注 实体 识别 词义 消 
歧 语句 处理 句法分析 Syntactic Analysis 语义分析 Senmantic Analysis 机器翻译 
语音合成 篇章 处理 自动 文摘 当前 热点 信息 抽取 文本 
分类 问答 系统 统计 语言 模型 分词 字串 均 分为 
词串 难点 未 登录 词 最大 匹 配法 ／ 逆向 
最大 匹 配法 ／ 双向 匹 配法 ／ 最佳 匹 
配法 ／ 最 少分 词法 ／ 词 网格 算法 语料库 
生 语料 自动 分词 语法 标注 句法分析 语义 / 语法分析 
语言 知识库 词性 标注 基于 规则 的 词性 标注 基于 
隐 马尔科夫 模型 HMM 的 词性 标注 基于 转移 的 
词性 标注 基于 转移 与 隐 马尔科夫 模型 相结合 的 
词性 标注 句法分析 总体 结构 输入 句子 短语 界定 自动 
预测 括号 匹配 区间 限制 句法分析 人工 校队 分析树 表示 
自动 短语 定界 确定 短语 左 边界 右 边界 根据 
上下文 信息 把 开 括号 与其 相应 的 比 括号 
对应 起来 根据 歧义 消解 规则 和 统计 信息 消解/v 
短语/nz 定界/n 的/uj 歧义/n 生成/v 表示/v 句子/n 结构/n 的/uj 成分/n 
结/n 构树/n 语料库/n 多/m 机加工/n 系统/n 自动/vn 切/v 词/n 和/c 
词性/n 标注/v 子系统/n 自动/vn 短语/nz 定界/n 和/c 句法/n 标注/v 子系统/n 
自动/vn 语义/n 标注/v 子系统/n 词语/n 搭配/v 识别/v 技术/n 基于/p 频率/n 
方法/n 基于/p 均值/n 和/c 方差/n 的/uj 方法/n 基于/p 假设检验/n 的/uj 
方法/n 基于/p 互信息/n 方法/n N/w Gram 统计模型 N Pos 考虑 
词类 当 整个 模型 只有 一个 词类 时 那么 前 
N 1个 词类 没有 提供 任何 上下文 信息 退 化为 
Unigram 模型 如果 每 一个 词 都 有一个 各不相同 的 
词类 N pos 模型 等价 于N/nr gram 模型 语言 模型 
的 评价 KL 距离 某一 语言 的 真实 概率分布 与 
构造 的 概率 模型 的 KL 距离 平滑 方法 拉普拉斯 
定律 加 1 平滑 降低 已 出现 的 N gram 
条件 概率分布 以使 未 出现 N gram 条件 概率分布 非 
0Good Turing 平滑 简单 线性插值 平滑 其他 常用 平滑 方法 
Heldout Back off witten Bell 动态 自适应 基于 缓存 的 
语言 模型 将 N 个 最近 出现 过 的 词 
存于 一个 缓存 中 作为 独立 的 训练 数据 通过 
这些 数据 计算 动态 频度 分布 数据 将 动态 频度 
分布 数据 与 静态 分布 数据 由 大 规模性 预料 
训练 得到 通过 线性 差值 的 方法 相结合 马尔科夫 模型 
有限 历史 假设 时间 不变性 假设 不随 时间 改变 而 
改变 稳定性 假设 S pai A S 状态 集合 pai 
初始状态 概率 A 转移 概率 隐 马尔科夫 模型 S K 
pai A B S 状态 集合 K 输出 字符 集合 
pai 初始状态 概率 A 状态 转移 概率 B 状态 转移 
时 输出 字符 的 概率 三个 基本问题 评价 给定 一个 
模型 如果 搞笑 计算 某一 输出 字符 序列 的 概率 
给定 一个 输出 字符 序列 o 和 模型 u 如何 
确定 产生 这 一序列 概率 最大 的 状态 序列 给定 
一个 输出 字符 的 序列 o 如何 调整 模型 的 
参数 使得 产生 这 一序列 的 概率 最大 基于 HMM 
的 词性 标注 句法 分析判断 输入 的 次序 列 嫩 
否 构成 一个 合乎 语法 的 句子 运用 句法 规则 
和 其他 知识 将 输入 句子 中词 之间 的 线性 
词库 变成 一个 非 线性 的 结构 概率 上下文 无关 
文法 Probabilistic Stochastic Context Free Grammar 位置 无关 上下文 无关 
祖先 无关 完全 句法分析 浅层 句法分析 部分 分析 / 组块 
分析 Day1 初识 自然语言 处理 在 哲学 层面 上 构建 
智能 机器人 是 人工智能 长久 以来 的 挑战 语言 理解 
是 智能 行为 的 重要 组成部分 这一 目标 多年来 一直 
是 被 认为 是 太 困难 了 然而 随着 NLP 
技术 日趋 成熟 用 稳定 的 方法 来 分析 非 
结构化 文本 越来越 广泛 对 自然 语言 理解 的 期望 
变成 一个 合理 的 目标 再次 浮现 今天 介绍 一些 
语言 理解 技术 词意 消 歧 在 词意 消 歧 
中 要 分析 出 特定 的 上下 文中 的 词 
被 赋予 的 是 哪个 意思 思考 存在 歧义 的 
词 serve 和 dish a . serve help with food 
or drink hold an office put ball into playb . 
dish plate course of a meal communications devices 在 包含 
短话 he served the dish 的 句子 中 你 知道 
serve 和 dish 用 的 都是 它们 与 食物 相关 
的 含义 仅仅 三个 词 讨论 的 话题 不太可能 从 
体育 转向 陶器 这 也许 会 迫使 你 眼前 产生 
一幅 怪异 的 画面 一个 职业 网球手 正 把 他 
的 郁闷 发泄 到 放在 网球 场边 的 陶瓷 茶具 
上 换句话说 自动 消除歧义 需要 使用 上下文 利用 相邻 词汇 
的 相近 含义 指代 消解 更 深刻 的 语言 理解 
是 解决 谁 对谁 做 了 什么 即 检测 动词 
的 主语 和 宾语 虽然 你 也在 小学 已经 学会 
了 这些 但 它 比 你 想象 的 更难 在 
句子 the thieves stole the paintings 中 很容易 分辨出 谁 
做了 偷窃 的 行为 考虑 下面 句子 中的 3种 可能 
尝试 确定 是 什么 被 出售 被 抓 和 被发现 
其中 一种 情况 是 有 歧义 的 a . The 
thieves stole the paintings . They were subsequently sold . 
b . The thieves stole the paintings . They were 
subsequently caught . c . The thieves stole the paintings 
. They were subsequently found . 要 回答 这个 问题 
涉及 到 寻找 代词 they 的 先行词 或者 paintings 处理 
这个 问题 的 计算 技术 包括 指代 消解 确定 代词 
或 名词 短语 指 的 是 什么 和 语义 角色 
标注 确定 名词 短语 如何 与 动词 相关联 如 代理 
受事 工具 等 自动 生成 语言 如果 能够 自动 的 
解决 语言 理解 的 问题 我们 就 能够 继续 进行 
那些 包含 自动 生成 语言 的 任务 如 自动 问答 
和 机器 翻译 在 自动 问 答中 一台 机器 应该 
能够 回答 用户 关于 特定 文 本集 的 问题 a 
. Text The thieves stole the paintings . They were 
subsequently sold b . Human Who or what was sold 
c . Machine The paintings . 机械 的 回答 表明 
它 已经 正确 的 计算 出 they 是 指 paintings 
而 不是 thieves 在 机器 翻译 中 机器 应该 能够 
把 文本 翻译成 另一种 语言文字 并 准确 传达 原文 的 
意思 所有 这些 例子 中 弄清楚 词 的 含义 动作 
的 主语 及 代词 的 先行词 是 确定 句子 含义 
的 步骤 也是 希望 语言 理解 系统 能够 做到 的 
事情 机器翻译 长久以来 机器翻译 MT 都是 语言 理解 的 圣杯 
人们/n 希望/v 能/v 找到/v 从/p 根本/a 上/f 提供/v 高品质/n 且/zg 
符合/v 语言/n 习惯/n 的/uj 任意/v 两种/m 语言/n 之间/f 的/uj 翻译/v 
其 历史 可以 追溯 到 冷战 初期 当时 自动 翻译 
带来 大量 的 政府 赞助 它 也是 NLP 本身 的 
起源 今天 特定 语言 之间 实用 的 翻译 系统 已经 
形成 而且 有些 已经 集成 到 搜索引擎 中了 但是 这些 
系统 有 一些 严重 的 缺点 机器 翻译 是 困难 
的 一 方面 原因 是 给定 的 单词 可能 有 
几种 不同 的 解释 另一方面 原因 是 必须 改变 词序 
才能 与 目标 语言 的 语法 结构 保持 一致 如今 
遇到 的 难题 是 从/p 新闻/n 和/c 政府/n 网站/n 发布/v 
的/uj 两种/m 或/c 两种/m 以上/f 的/uj 语言/n 文档/n 中/f 可以/c 
搜集/v 到/v 大量/n 的/uj 相似/v 文本/n 如果 给 出 一个 
德文 和 英文 双语 的 文档 或者 一个 双语 词典 
就 可以 自动 配对 组成 句子 这个 过程 叫做 文本 
对齐 一旦 有 100 万个 或 更多 的 句子 对 
就 可以 检测 出 相应 的 单词 和 短语 并 
建立 能 用来 翻译 新 文本 的 模型 人机对话 系统 
在 人工智能 的 历史 上 主要 的 智能 测试 是 
一种 语言学 测试 叫做 图灵测试 一个 响应 用户 文本 输入 
的 对话 系统 能否 表现 得 如此 自然 以至于 我们 
无法 区分 它 是 人工 生成 的 响应 相比之下 今天 
的 商业 对话 系统 能力 是 非常 有限 的 下面 
这个 图 展示 了 一个 简单 的 语音 对话 系统 
的 流程 架构 沿途 的 顶部 从 左向右 是 一些 
语言 理解 组件 的 管道 这些 图 是从 语音输入 经过 
文法 分析 到 某种 意义 的 重现 图 的 中间 
从右 向左 是 这些 组件 将 概念 转换 为 语音 
的 逆向 流程 这些 组件 构成 了 系统 的 动态 
方面 在 图 的 底部 是 一些 有 代表性 的 
静态 信息 处理 组件 在 其上 运作 的 语言 相关 
数据 的 仓库 NLP 的 局限性 尽管 在 很多 如 
RTE Recognizing Textual Entaliment 这样 的 任务 研究 中 取得 
了 进展 但在 现实 世界 已经 部署 的 语言 理解 
系统 仍然 不能 进行 常识推理 或以 一般 的 可靠 的 
方式 描述 这个 世界 的 知识 在 等待 这些 困难 
的 人工智能 问题 得到 解决 的 同时 接受/v 一些/m 在/p 
推理/v 和/c 知识/v 能力/n 上/f 存在/v 严重/a 限制/v 的/uj 智能/n 
语音/n 系统/n 是/v 有/v 必要/d 的/uj 因此 从一/nr 开始 自然语言 
处理 研究 的 重要 目标 一直 是 使用 浅显 但 
强大 的 技术 代替 无边无际 的 知识 和 推理 能力 
维 特比 算法 Viterbi algorithm 是 机器 学习 中 应用 
非常 广泛 的 动态规划 算法 在 求解 隐 马尔科夫 条件 
随 机场 的 预测 以及 seq2seq 模型 概率 计算 等 
问题 中 均用 到了 该 算法 实际上 维 特比 算法 
不仅 是 很多 自然语言 处理 的 解码 算法 也是 现代 
数字通信 中 使用 最 频繁 的 算法 在 介绍 维 
特比 算法 之前 先 回顾 一下 隐 马尔科夫 模型 进而 
介绍 维 特比 算法 的 计算 步骤 以下 为 一个 
简单 的 隐 马尔科夫 模型 如下 图 所示 其中 x 
  = x1 x2 . . . xN 为 隐 
状态 序列 y   = y1 y2 . . . 
yN   为 观测 序列 要求 的 预测 问题 为 
依据 马尔科夫 假设 上式 等价 于 在 隐 马尔科夫 链 
中 任意 时刻 t 下 状态 的 值 有 多个 
以 拼音 转 汉字 为例 输入 拼音 为 yike 可能 
有的 值 为 一棵 一刻 或者 是 一颗 等待 用 
符号 xij 表示 状态 xi 的 第 j 个 可能 
值 将 状态 序列 按 值 展开 就 得到 了 
一个 篱笆 网 了 这 也 就是 维 特比 算法 
求解 最优 路径 的 图 结构 隐 马尔科夫 的 预测 
问题 就是 要求 图中 的 一条 路径 使得 该 路径 
对应 的 概率 值 最大 对应 上图 来讲 假设 每个 
时刻 x 可 能取 的 值 为 3 如果 直接 
求 的话 有3^/nr N 的 组合 数 底数 3 为 
篱笆 网络 宽度 指数 N 为 篱笆 网络 的 长度 
计算 量 非常 大 维 特比 利用 动态规划 的 思想 
来 求解 概率 最大 路径 可理解 为求 图 最短 路径 
使得 复杂度 正比 于 序列 长度 复杂度 为 O N 
⋅ D ⋅ D N 为 长度 D 为 宽度 
从而 很好 地 解决 了 问题 的 求解 维 特比 
算法 的 基础 可以 概括 为 下面 三点 来源于 吴军 
数学 之美 1 如果 概率 最大 的 路径 经过 篱笆 
网络 的 某 点 则从 开始 点到 该点 的 子 
路径 也 一定 是 从 开始 到 该点 路径 中 
概率 最大 的 2 假定 第 i 时 刻有 k 
个 状态 从/p 开始/v 到/v i/w 时刻/n 的/uj k/w 个/q 
状态/n 有k条/nr 最短/i 路径/n 而 最终 的 最短 路径 必然 
经过 其中 的 一条 3 根据上述 性质 在 计算 第 
i + 1 状态 的 最短 路径 时 只/d 需要/v 
考虑/v 从/p 开始/v 到/v 当前/t 的/uj k/w 个/q 状态值/n 的/uj 
最短/i 路径/n 和/c 当前/t 状态值/n 到/v 第/m i/w +/i 1/m 
状态值/n 的/uj 最短/i 路径/n 即可/d 如 求 t = 3时 
的 最短 路径 等于 求 t = 2时 的 所有 
状态 结点 x2i 的 最短 路径 加上 t = 2 
到 t = 3 的 各 节点 的 最短 路径 
为了 纪录 中间 变量 引入 两个 变量 sigma 和 phi 
定义 t 时刻 状态 为 i 的 所有 单个 路径 
i1 i2 . . . it 中 最大 概率值 最短 
路径 为 前文 小修 已经 有 介绍 隐 马尔科夫 相关 
的 概念 如果 不 清楚 可以 看 一下 前面 的 
详解 隐 马尔可夫 模型 HMM   其中 it 表示 最短 
路径 Ot 表示 观测 符号 lamda 表示 模型 参数 根据 
上式 可以 得出 变量 sigma 的 递推公式 其中 i = 
1 2 . . . N t = 1 2 
. . . T 1 定义 在 时刻 t 状态 
为 i 的 所有 单个 路径   i1 i2 . 
. . it   i   中 概率 最大 的 
路径 的 第 t － 1个 结点 为 根据 上面 
的 两个 定义 下面 给出 维 特比 算法 具体内容 输入 
为 模型 和 观测 状态 分别为 输出 为 求出 最优 
路径 步骤 为 1 初始化 各 参数 2 根据 上式 
进行 递推 对 t ＝ 2 3 . . . 
T 3 最后 计算 终止 状态 最优 路径 的 回溯 
对 t ＝ T 1 T － 2 . . 
. 1 最后 求得 最优 路径 以上 就是 维 特比 
算法 的 主要 过程 和 内容 下面 介绍 一个 个 
例子 在 自然 语言 处理 技术 中的 seq2seq 模型 中 
如下 图 所示 其实 seq2seq 模型 的 核心 就是 其中/r 
e/w 和f/nr 就是/d 相应/v 的/uj 输出/v 和/c 输入/v 序列/n 在 
进行 解码 的 时候 如果 词 袋中 的 个数 为 
V 个 那么 那么 输出 长度 为 N 的 序列 
则 需要 的 总共 搜索 V ^ N 次 如果 
N 的 个数 非常 之大 这样 的 搜索 非常 耗时间 
那么 这个 时候 使用 维 特比 算法 就会 大大 的 
降低 搜索 的 时间 这里 假 设词 袋中 只有 a 
和b/nr 而且 它们 之间 的 转变 概率 为 在 这时 
使用 维 特比 算法 其 主要 的 思想 为 其中 
s v n 表示 的 是以 v 结尾 的 最大 
概率 的 序列 的 概率 t i j n 为/p 
第/m n/w －/i 1步/mq 从i/nr 跳到/v 第/m n/w 步的j/nr 的/uj 
概率/n 根据 算法 可以 得到 因此 最终 输出 的 序列 
为 bba 在 这里 最后 说 一点 就是 其实 对于 
seq2seq 有 相应 的 算法 对 整个 序列 的 输出 
进行 搜索 计算 beam search 算法 其 思想 和维/nr 特比 
算法 非常 相似 这里 不做 介绍 下次 有 机会 给 
大家 介绍 参考书目 1 统计 学习 方法 李航 文章 来源 
于 微信 公众 号 言 处理 技术 更多 内容 请 
访问 该 公众 号 欢迎 关注 公众 号 学习 一 
． 什么 是 NLPNLP 中文 叫 自然语言 处理 简单 来说 
是 一门 让 计算机 理解 分析/vn 以及/c 生/vn 成/n 自然/d 
语言/n 的/uj 学科/n 大概 的 研究 过程 是 研制 出 
可以 表示 语言 能力 的 模型 提出 各种 方法 来 
不断 提高 语言 模型 的 能力 根据 语言 模型 来 
设计 各种 应用 系统 不断 地 完善 语言 模型 NLP 
理解 自然语言 目前 有 两种 处理方式 1 . 基于 规则 
来 理解 自然语言 即 通过 制定 一些 系列 的 规则 
来 设计 一个 程序 然后 通过 这个 程序 来 解决 
自然 语言 问题 输入 是 规则 输出 是 程序 2 
. 基于 统计 机器学习 来 理解 自然语言 即用 大量 的 
数据 通过 机器学习 算法 来 训练 一个 模型 然后 通过 
这个 模型 来 解决 自然 语言 问题 输入 是 数据 
和 想要 的 结果 输出 是 模型 接下来 简单 介绍 
NLP 常见 的 任务 或 应用 二 ． NLP 能 
做什么 1 . 分词 中文 可以 分为 字 词 短语 
句子 段落 文档 这几个 层面 如果 要 表达 一个 意思 
很多 时候 通过 一个 字 是 无法 表达 的 一个 
含义 的 至少 一个 词 才能 更好 表达 一 个 
含义 所以 一般 情况 是以 词 为 基本 单位 用 
词 组合 来 表示 短语 句子 段落 文档 至于 计算机 
的 输入 是 短语 或 句子 或 段落 还是 文档 
就要 看 具体 的 场景 由于 中文 不像 英文 那样 
词 与 词 之间 用 空格 隔开 计算机无法 用 区分 
一个 文本 有 哪些 词 所以 要 进行 分词 目前 
分词 常用 的 方法 有 两种 1 基于 规则 Heuristic 
启发式 关键 字表 2 基于 机器学习 / 统计 方法 HMM 
隐 马尔科夫 模型 CRF 条件 随 机场 注 在 这里 
就 不 具体 介绍 方法 的 原理 和 实现 过程 
了 大家 感兴趣 可以 自行 百度 了解 现状 分词 这项 
技术 非常 成熟 了 分词 的 准确率 已经 达到 了 
可用 的 程度 也/d 有/v 很多/m 第三/m 方的库/nr 供/v 我们/r 
使用/v 比如 jieba 所以 一般 在 实际 运用 中 我们 
会 采用 jieba + 自定义 词典 的 方式 进行 分词 
2 . 词 编码 现在 把 我 喜欢 你 这个 
文本 通过 分词 分成 我 喜欢 你 三个 词 此时 
把这 三 词 作为 计算机 的 输入 计算机 是 无法 
理解 的 所以/c 我们/r 把/p 这些/r 词/n 转换/v 成/n 计算机/n 
能/v 理解/v 的/uj 方式/n 即 词 编码 现在 普遍 是 
将 词 表示 为 词 向量 来 作为 机器 学习 
的 输入 和 表示 空间 目前 有 两种 表示 空间 
1 离散 表示 A . One hot 表示 假设 我们 
的 语料库 是 我 喜欢 你 你 对 我 有 
感觉 吗 词典 { 我 1 喜欢 2 你 3 
对 4 有 5 感觉 6 吗 7 } 一 
共有 七 个 维度 所以 用 One hot 表示 我 
1 0 0 0 0 0 0 喜欢 0 1 
0 0 0 0 0 吗 0 0 0 0 
0 0 1 即 一个 词 用 一个 维度 表示 
B . bagofword 即将 所有 词 的 向量 直接 加 
和 作为 一个 文档 的 向量 所以 我 喜欢 你 
就 表示 为 1 1 1 0 0 0 0 
C . Bi gram 和N/nr gram 语言 模型 考虑 了 
词 的 顺序 用词 组合 表示 一个词 向量 这三种 方式 
背后 的 思想 是 不同 的 词 都 代表 着 
不同 的 维度 即 一个 单位 词 或 词 组合 
等 为 一个 维度 2 分布式 表示 word2vec 表示 一个 
共 现 矩阵 向量 其 背后 的 思想 是 一个词 
可以 用 其 附近 的 词 来 表示 离散/v 式/k 
或/c 分布式/n 的/uj 表示/v 空间/n 都有/nr 它们/r 各自/r 的/uj 优缺点/n 
感 兴趣 的 读者 可以 自行 查资料 了解 在 这里 
不 阐述 了 这里 有 一个 问题 当 语料库 越大 
时 包含 的 词 就 越多 那词/nr 向量 的 维度 
就 越大 这样 在 空间 储存 和 计算 量 都会 
指数 增大 所以 工程师 在 处理 词 向量 时 一般 
都会 进行 降 维 降 维 就 意味着 部分 信息 
会 丢失 从而 影响 最终 的 效果 所以 作为 产品 
经理 跟进 项目 开发 时 也 需要 了解 工程师 降 
维 的 合理性 3 . 自动 文摘 自动 文摘 是 
指在 原始 文本 中 自动 摘要 出 关键 的 文本 
或 知识 为什么 需要 自动 文摘 有 两个 主要 的 
原因 1 信息 过载 我们 需要 在 大量 的 文本 
中 抽出 最 有用 最 有价值 的 文本 2 人工 
摘要 的 成本 非常 高 目前 自动 文摘 有 两种 
解决 思路 第 一种 是 extractive 抽取式 从 原始 文本 
中 找到 一些 关键 的 句子 组成 一篇 摘要 另一种 
方式 是 abstractive 摘要 式 计算 机先 理解 原始 文本 
的 内容 再 用 自己 的 意思 将其 表达出来 自动 
文摘 技术 目前 在 新闻 领域 运用 的 最广 在 
信息 过载 的 时代 用 该 技术 帮助 用户 用 
最短 的 时间 了解 最多 最 有价值 的 新闻 此外 
如何 在 非 结构 的 数据 中 提取 结构化 的 
知识 也将 是 问答 机器人 的 一大 方向 4 . 
实体 识别 实体 识别 是 指 在 一个 文本 中 
识别 出 具体 特定 类别 的 实体 例如 人名 地名 
数值 专有名词 等 它 在 信息检索 自动 问答 知识图谱 等 
领域 运用 的 比较 多 实体 识别 的 目的 就是 
告诉 计算机 这个词 是 属于 某类 实体 有助于 识别 出 
用户 意图 比如 百度 的 知识图谱 周星驰 多 大了 识别 
出 的 实体 是 周星驰 明星 实体 关系 是 年龄 
搜索 系统 可以 知道 用户 提问 的 是 某个 明星 
的 年龄 然后 结合 数据 周星驰 出生 时间 1962年 6月 
22日 以及 当前 日期 来 推算 出 周星驰 的 年龄 
并把 结果 直接 把 这个 结果 显示 给用户 而 不是 
显示 候选 答案 的 链接 此外 NLP 常见 的 任务 
还有 主题 识别 机器翻译 文本 分类 文本 生成 情感 分析 
关键字 提取 文本 相似 度 等 以后 有 时间 再 
为 大家 做 简单 介绍 人工智能 大 数据 云计算 和 
物联网 的 未来 发展 值得 重视 均为 前沿 产业 有兴趣 
的 朋友 可以 查阅 多 智 时代 在 此为 你 
推荐 几篇 优质 好文 自然语言 理解 过程 主要 有 哪些 
层次 各 层次 的 功能 是 怎么样 http / / 
www . duozhishidai . com / article 1726 1 . 
html 如何 快速 入门 NLP 自然语言 处理 概述 http / 
/ www . duozhishidai . com / article 11742 1 
. html 什么 是 人脸识别 主要 的 应用 于 哪些 
场景 http / / www . duozhishidai . com / 
article 1246 1 . html 多 智 时代 人工智能 和大/nr 
数据 学习 入门 网站 | 人工智能 大 数据 物联网 云计算 
的 学习 交流 网站 定义 书中 定义 的 统计 自然语言 
处理 由 所有 的 自动 语言 处理 的 定量 方法 
组成 包括 概率模型 信息论 线性代数 代表 自然语言 处理 中非 符号化 
和 非逻辑 的 工作 语言 的 非 绝对性 需要 利用 
统计 观察 来 考察 问题 个人 思考 因为 生活 中 
充满 了 不 确定 和不/nr 完整 的 信息 为了/p 能和/nr 
世界/n 有效/a 的/uj 相互作用/l 我们 需要 处理 这 类 信息 
所以/c 概率论/n 和/c 随机/d 过程/n 给/p 我/r 么/y 一个/m 可以/c 
处理/v 不/d 确定/v 和/c 不完整/i 信息架构/i 的/uj 量化/v 框架/n 这里/r 
只/d 是因为/c 想到/v 认知/v 是/v 随机/d 的/uj 所以 推广 到 
语言 但是 我 认为 问题 是 需要 针对 特定 问题 
的 在 这里 我 认为 语言 处理 的 第一 步 
就是 需要 让 机器 知道 我们 的 某些 想法 并且 
完成 某些 事情 如果 阶段性 的 去 划分 这样 一个 
过程 我 觉得 应该 是 这样 下命令 执行命令 1 . 
给出 具体 某 一条 命令 电脑 执行 某 一条 命令 
开机 / 关机 2 . 给出 命令 电脑 反馈 所有 
能 执行 的 命令 明天 天气真好 各个 游玩 地点 信息 
日程 安排 等 3 . 给出 任意 命令 准确 知道 
我 要 干什么 那么 以上 的 问题 首先 就是 机器 
要 能做 某些 事情 语言 就是 信息 信息 就 是 
一定 要 传达 某种 内容 目标 就是 解析 内容 嘛 
关于 歧义 自己 的 想法 其实 我 还 没有 觉得 
有 什么 歧义 的 问题 首先 你 必须 知道 这 
个 句子 中 的 每个 词语 和字/nr 如果 这个 都不/nr 
知道 肯定 分析 不 出来 嘛 我 还是 那个 观点 
每个 人 心中 对词 都有 一个词 网 南京市 长江大桥 如果 
我 知道 这个 地方 OK 那么 这个 词 就是 一个 
固定 词 如果 不 知道 那么 我 就要 用 已知 
的 方式 去 猜测 长江大桥 我 有 概念 南京市 我 
也 知道 如果 能 精确 的 让 我 去 划分 
这个 短语 我 肯定 不 知道 江 大桥 是 个 
什么 玩意 所以 我 能 很好 的 判断 出来 也 
就是说 我 需要 建立 这样 一套 系统 我 要 给出 
我 对 每个 词语 了解 程度 的 多少 关键 就 
成了 如何 去 联系 词语 和 现实 世界 语法 规则 
问题 关于 语法 这 一块 的话 我 现在 是 这么 
考虑 的 所有 的 内容 都是 词语 不 需要 必要 
一定 是 必要 的 以 我 语文 语法 水平 为 
基准 我 知道 的 基本 我 认为 是 必要 的 
名词 动词 形容词 词语 中 人物 时间 地点 等 基本 
少许 的 概念 一定是 要 有的 吧 既然 是 我 
知道 的 那么 规则 也 肯定 不多 肯定 是 够用 
的 一些 法则 由于 语料库 使用中 绝大部分 词语 出现 极少 
而 常用词 出现 频率 极高 这样 很难 预测 行为 最初 
认为 使用 更大 的 语料库 就 可以 解决 这个 问题 
但是 愿望 是 无法 证实 的 下面 提出 语料库 语言学 
中最 著名 的 早期 结论 Zipf 法则 这个 法则/nr 针对 
的 问题 就是 这些 稀有 词汇 Zipf 我们 能够 统计 
一种 语言 中 所有 的 词 在 一个 大型 语料库 
中 出现 的 次数 并且 按照 其 出现 次数 排列 
发现 f x r = k 概述 本书 本章 描述 
自然语言 处理 中 消除歧义 的 问题 并 介绍 几种 重要 
的 语义 消 歧 算法 描述 他们 的 资源 需求 
和 算法 性能 消/v 歧/v 我们/r 应该/v 能/v 直观/n 的/uj 
想象/n 到/v 就是/d 一句话/i 可能/v 有/v 几个/m 意思/n 但是 落实 
到 具体 细节 中 我 认为 主要 分 以下 几种 
１ ． 分词 的 消 歧 这是 很 常见 的 
一个 例子 南京 市 长 江 大桥 ２ ． 多义词 
的 具体 词义 ３ ． 词性 的 判断 对于 词性 
的 判断 可以 看做 一个 词性 标注 的 问题 词性 
标注 的话 我们 通常 考虑 邻近 上下文 相反 如果 是 
词义 判决 的话 可能会 有 相隔 很远 的 词语 来 
决定 他 的 词性 因此 大 部分 的 词性 标注 
模型 简单 地 使用 当前 上下文 而 语义 消 歧 
模型 通常 使用 规模 广泛 一些 的 上下 文中 的 
实词 本章 将会 介绍 ３个 方法 基于 标注 训练 集 
的 有 监督 消 歧 基于 词典 的 消 歧 
无 监督 消 歧 性能 上 下界 性能 上界 相同 
情况 下 人工 标注 的 性能 这里 一定 要 强调 
相同 判断 情况 实际 判决 中 人 往往 会 将该 
系统 没有 利用 的 特性 加入 进来 这里 有 几个 
思考 １ ． 系统 的 性能 是 有 上界 的 
我们 判决 时 应该 使用 系统 的 视角 来 看待 
问题 不要 全局 来看 这样 能 意识 到 系统 本身 
的 缺陷 ２ ． 正是 因为 系统 有了/nr 明显 的 
上界 缺陷 才 使得 我们 有 改进 的 方向 比如 
我们 知道 利用 前后 一个词 不能 判断 语义 那么 我们 
考虑 的 方向 就 变成 了 看 一句话 我们 能否 
进行 这样 的 判断 呢 我们/r 做/v 的/uj 是/v 需要/v 
不断/d 挖掘/v 我们/r 本/r 能/v 考虑/v 但是/c 系统/n 没有/v 考虑/v 
的/uj 内容/n 和/c 信息/n 讲 其 模型 化 基于 贝叶斯 
分类 的 语义 消 歧 原理 是 考虑 一个 上下文 
窗 口中 歧义 词 周围 词 的 信息 通常 我们 
这里 使用 一个 特殊 的 分类器 即 朴素 贝叶斯 分类器 
这里 使用 朴素 贝叶斯 有 两个 假设 １/m ．/i 第一个/m 
是/v 上下/f 文中/s 的/uj 所有/b 结构/n 和/c 词语/n 顺序/n 都/d 
可以/c 被/p 忽略/d ２ ． 可有 重复 的 单词 集中 
出现 的 词 独立 于 其他 词 虽然 这 两个 
假设 不太 成立 但是 朴素 贝叶斯 能在/nr 这个 任务 中 
取得 一定 的 效果 以下 给出 书中 该 方法 得出 
的 靠前 的 两个 结果 一种 基于 信息论 的 方法 
贝叶斯 采取 的 是 利用 上下 文中 所有 的 词 
来 帮助 消 歧 决策 而且 事先 还 做 了 
一个 不太 真是 的 独立 假设 现在 利用 信息论 的 
方法 可以 得到 一个 比较 不同 的 策略 试图 寻找 
一个 单一 的 上下文 特征 用来 指示 哪一种 语义 应该 
被 使用 方法 如下 该 方法 适用 于２/nr 语义 词 
多 语义 请看 书中 文献 扩展 我们 划分 出 语义 
集 Ｐ１ Ｐ２ 然后 划分 出 指示 集 Ｑ1 Ｑ２ 
书中 利用 了 一种 叫 flip flop 的 方法 来 
计算 最大化 的 互 信息量 I P Q 该 方法 
是 一个 有效 的 线性 时间 算法 比 暴力 搜索 
效果 要好 这里 重点 在于 提出 了 指示器 的 集合 
然后 我们 可以 根据 这个 集合 来 判断 我们 想要 
的 语义 如何 划分 下面 给出 书中 集合 结果 基于 
词典 的 消 歧 书中 提供 了 一系列 词典 消 
歧 的 方法 准确率 都不 太高 而且 思路 也 比较 
通俗 比如说 利用 语义 不同 定义 里面 的 一些 统计 
特性 来 区分 还有 基于 主题 的 分类 没有 太多 
想 细致 去看 的 大家 有 兴趣 自己 去 看吧 
无 监督 消 歧 书中 仅仅 提了 一下 无 监督 
做法 思路 模型 与 贝叶斯 相似 随机 初始化 参数 P 
vj | sk 然后 ＥＭ 算法 重新 估计 确认 参数 
后 利用 贝叶斯 判别 规则 决策 这里 聚 类 我 
暂时 不 太 清楚 以后 补上 自己 的 体会 这里 
插 一下 自己 的 想法 本章 主要 介绍 了 英文 
的 语义 消 歧 这里 语义 更多 的 是 在 
说 词义 对于 中文 而言 词语 不是 硬性 划分 的 
那么 我们 首先 会 存在 的 一个 问题 就是 如何 
去 划分 词语 南京市 长江大桥 南京 市长 江 大桥 在 
英文 中 是 存在 这个 歧义 的 所以 以下 介绍 
的 内容 对于 中文 处理者 来说 仅仅 提供 一些 基础 
方案 吧 估计 不 太 适用 综述 的 大体 部分 
自然语言 处理 的 基础 研究 主要 包括 词 法分析 句法分析 
语义分析 语用 语境 与 篇章 分析 等 的 研究 词 
向量 Word embedding 或 Word representation 方法 可以 将 词 
映射 转换 到 一个 独立 的 向量空间 自然语言 处理 技术 
中 采用 深度 学习 知识 的 原因 可以 总结 为 
以下 几点 1 自然语言 处理 任务 中 首先 要 解决 
的 问题 是 处理 对象 的 表示 形式 为了 表示 
对象 通常 必须 抽取 一些 特征 如 文本 的 处理 
中 常 常用词 集合 来 表示 一个 文档 传统 依赖 
手工 的 方式 抽取 特征 费时费力 不仅 获取 过程 比较 
随意 且 完备性 较差 同时 根据 处理 任务 或 领域 
的 不同 特征提取 工作 要 重复 进行 无法 实现 表示 
共享 深度 学习 中的 特征提取 即指 可以 自动 从 数据 
中 学习 获取 特征 这也 是 考虑 在 自然 语言 
处理 技术 中 采用 深度 学习 知识 的 主要 原因 
2 目前/t 大多数/m 效果/n 较好/i 的/uj 自然/d 语言/n 处理/v 任务/n 
和/c 机器/n 学习/v 方法/n 都/d 依赖/v 于/p 标注/v 数据/n 实际应用 
而言 自然 语言 中 大量 存在 的 是 未 标注 
数据 深度 神经 网络 采用 无 监督 方式 完成 预 
训练 过程 恰恰 提供 了 合适 的 训练 模型 3 
深度 学习 结构 一般 由 多层 神经网络 结点 组成 其 
预 训练 过程 通常 需要 高 性能 计算 的 支持 
硬件 及 软件 技术 的 发展 都为 当前 采用 深度 
学习 结构 的 自然 语言 处理 提供 了 良好 支撑 
环境 如何将 深度 学习 与 现有 自然语言 处理 具体 任务 
结合 比较 直接 简单 的 做法 是 以 词 或 
短语 作为 原始 输入 构建 向量 类型 的 表达 方式 
经过 深度 学习 分层 学习 后 得到 的 特征 可以 
添加 进 现有 基于 特征 的 半 监督 学习 系统 
中 进行 处理 中文 自然语言 处理 的 难点 首先/d 由/p 
于/p 每个/r 汉字/nz 都/d 包含/v 不同/a 的/uj 含义/n 需要 为 
每个 含义 获取 相应 的 表示 另外 使用 同音词 或者 
多义词 来 为 词语 学习 单一 表征 反而 可能会 影响 
最终 的 表征 结果 由于 多个 含义 之间 的 相互 
影响 不能 准确 表示 任何 一个 含义 其次 需要 进一步 
考虑 训练 语料 问题 如何 保证 系统 的 鲁棒性 通用性 
保证 能够 在 不同 领域 都 得到 较好 的 效果 
另外 需要 考虑 新 生词 网络 用语 等 的 识别 
问题 最后 需要 考虑 语料 是否是 越多越好 在 训练 学习 
的 过程 中 需要 能够 检测 训练 情况 避免 过大 
的 数据 训练 破坏 汉字 的 分布式 表示 并且 英文 
分词 可以 根据 空格 分词 而 中文 则 不能 简单 
的 按照 标点符号 划分 需要 联系 上 下文 考虑 词性 
词意 参考文献 1 竺 宝宝 张娜 . 基于 深度 学习 
的 自然 语言 处理 J . 无线 互联 科技 2017 
10 25 26 . 2 吴轲 . 基于 深度 学习 
的 中文 自然语言 处理 D . 东南大学 2014 . 3 
朱国 进 沈盼宇/nr . 基于 深度 学习 的 算法 知识 
实体 识别 与 发现 J . 智能 计算机 与 应用 
2017 7 1 17 21 . 4 冯志伟 . 自然语言 
处理 的 历史 与 现状 J . 中国 外语 2008 
5 1 14 22 . 5 姜倩盼/nr . 自然语言 处理 
的 挑战 与 未来 J . 信息 与 电脑 理论 
版 2013 7 219 221 . 6 翟 剑锋 . 
深度 学习 在 自然 语言 处理 中 的 应用 J 
. 电脑编程 技巧 与 维护 2013 18 74 76 . 
理论动态 | FAIR 最新 论文 一种 不 需要 训练 就 
能 探索 句子 分类 的 随机 编码器 NLP 中/f 评价/n 
文本/n 输出/v 都有/nr 哪些/r 方法/n 为什么 要 小心 使用 BLEU 
AAAI 2019 教程 361页 PPT 带 你 回顾 最新 词句 
Embedding 技术 和 应用 嘘 这里有 千 展 价值 提高 
95% 的 秘密 句法 敏感 的 实体 表示 用于 神经网络 
关系 抽取 命名 实体 识别 中 众包 标注 能否 优于 
专家 标注 针对 商品 标题 冗长 问题 阿里 工程师 怎么 
解决 如何 生成 你 的 专属 推荐 文案 智能 文案 
在 1688 平台 的 应用 Facebook 最新 论文 跨语言 模型 
预 训练 三大 任务 刷新 最 高性能 跨 语言版 BERT 
Facebook 提出 跨语言 预 训练 模型 XLM 命名 实体 识别 
NER 综述 用 可视化 解构 BERT 我们 从 上亿 参数 
中 提取 出了 6种 直观 模式 CMU 谷歌 提出 Transformer 
XL 学习 超长 上下文 关系 知识图谱 最新 论文 清单 高阶 
炼丹 师为你/nr 逐一 解读 BERT 大火 却 不懂 Transformer 读 
这 一篇 就 够了 NLP 的 巨人 肩膀 下 从 
CoVe 到 BERT 深度 长文 NLP 的 巨人 肩膀 上 
博客 | 谷歌 最强 NLP 模型 BERT 解读 深度 学习 
文本 分类 实战 报告 CNN RNN & HAN 微软 小冰 
首席 科学家 武威 解读 EMNLP 论文 聊天 机器人 的 深度 
学习 模型 CCKS 2018 | 最佳 论文 南京大学 提出 DSKG 
将 多层 RNN 用于 知识图谱 补全 从 经典 到 端 
到 端 详解 问答 系统 和 机器 阅读 理解 消除 
NLP 中的 刻板 印象 程序员 之于 男性 = 家政 人员 
之 于 女性 让 AI 触类旁通 93种 语言 Facebook 最新 
多语种 句 嵌入 来了 在 大规模 数据 集上 应用 潜在 
语义分析 的 三种 方式 从 语言学 角度看 词 嵌入 模型 
谷歌 Quoc Le 这篇 NLP 预 训练 模型 论文 值得一看 
博客 | 总结 + paper 分享 | 任务 型 对话 
中的 跨 领域 & 个性化 & 迁移 学习 博客 | 
任务 型 对话 系统 公式 建模 & & 实例 说明 
项目 推荐 系统 Github 项目 推荐 | RecQ Python 推荐 
系统 框架 ICME 2019 短 视频 内容 理解 与 推荐 
竞赛 正式 启动 附 baseline 方法 从 KDD 2018 最佳 
论文 看 Airbnb 实时 搜索 排序 中的 Embedding 技巧 如何 
增加 用户 的 参与感 交互式 推荐 来了 强化 学习 与 
推荐 系统 的 强强联合 蚂蚁 金服/nr 核心技术 百亿 特征 实时 
推荐算法 揭秘 大众 点评 搜索 基于 知识图谱 的 深度 学习 
排序 实践 双 11 商品 怎样 凑 于 品类 关系 
虚拟 类目 如何 建设 如何 解决 移动 电商 平台 中的 
伪 曝光 凑 单 这个 技术 活 阿里 工程师 怎么搞 
可视化 理解 深度 神经网络 CTR 预估 模型 人群 优选 算法 
模型 如何 挖掘 品牌 潜 客 JUMP 一种 点击 和 
停留 时长 的 协同 预估 器 数十亿 商品 中 长尾 
和 新品 怎么 找到 新 主人 为 电商 而生 的 
知识图谱 如何 感应 用户 需求 一种 端 到 端的 模型 
基于 异构 内容 流 的 动态 排序 火箭 发射 点击率 
预估 界 的 神算子 是 如 何炼成 的 为什么 短 
视频 会 让人 刷 不停 背后 也许 用了 这套 技术 
机器 如何 猜 你 喜欢 深度 学习 模型 在 1688 
的 应用 实践 让 机器 帮 你 做 决策 强化 
学习 在 智能 交互 搜索 的 应用 分享 基于 改进 
注意力 循 环控制 门 品牌 个性化 排序 升级 系统 来 
了 基于 快速 GeoHash 如何 实现 海量 商品 与 商圈 
的 高效 匹配 打破 传统 搜索 排序 阿里 首次 提出 
商品 间 相互 影响 的 全局 排序 法 Github 项目 
推荐 | Sentence Classification 神经网络 句子 分类 陈述 / 疑问 
/ 感叹 / 祈使 Github 项目 推荐 | Chatito 使用 
简单 的 DSL 为 AI 聊天 机器人 NLP 任务 命名 
实体 识别 或 文本 分类 模型 生成 数据集 一文 看懂 
虚假 新闻 检测 附 数据集 & 论文 推荐 基于 CNN 
和 序列 标注 的 对联 机器人 | 附 数据集 & 
开 源代码 搞定 NLP 领域 的 变形金刚 手把手 教你用 BERT 
进行 多 标签 文本 分类 为了 写 春联 我 用 
Transformer 训练 了 一个 对穿 肠 Github 项目 推荐 | 
PAAG 电子商务 问 答中 的 产品 感知 应答 生成 谷歌 
升级版 Transformer 官方 解读 更大 更强 解决 长 文本 问题 
开源 贼 好 理解 这个 项目 教 你 如何 用 
百行 代码 搞定 各类 NLP 模型 谷歌 的 机器 翻译 
模型 Transformer 现在 可以 用来 做 任何 事 了 CMU 
和 谷歌 联手 放出 XL 号 Transformer 提速 1800倍 | 
代码 + 预 训练 模型 + 超 参数 谷歌 开源 
BERT 不费吹灰之力 轻松 训练 自然语言 模型 谷歌 最强 NLP 模型 
BERT 如约 开源 12 小时 GitHub 标 星 破 1500 
即将 支持 中文 自然语言 处理 是 如何 工作 的 一步步 
教 你 构建 NLP 流水线 基于 Tensorflow eager 的 文本 
生成 注意力 图像 注释 的 完整 代码 自动 文本 摘要 
博客 | 一次 LDA 的 项目 实战 附 GibbsLDA + 
+ 代码 解读 斯坦福 发布 重磅 NLP 工具包 StanfordNLP 支持 
中文 等 53种 语言 Facebook 开源 增强版 LASER 库 包含 
93种 语言 工具包 40个 中文 NLP 词库 北大 开源 了 
中文分词 工具包 准确度 远超 Jieba 提供 三个 预 训练 模型 
Facebook 开源 新 NLP 框架 简化 部署 流程 大 规模 
应用 也 OK 深度 学习 的 NLP 工具 资讯 AI 
人 必看 89页 全网 最全 清华 知识图谱 报告 对话 清华 
NLP 实验室 刘知远 NLP 搞 事情 少不了 知识库 与 图 
神经网络 谷歌 CMU 重磅 论文 Transformer 升级版 评估 速度 提升 
超 1800倍 请 收好 这份 NLP 热门 词汇 解读 预 
训练 Transformer 无 监督 机器翻译 BERT 霸 榜 问答 任务 
谷歌 新 基准 模型 缩小 AI 与 人类 差距 50% 
横扫 13项 中文 NLP 任务 香侬 科技 提出 汉语 字形 
表征 向量 Glyce + 田字格 CNNTransformer 在 进化 谷歌 大脑 
用 架构 搜索 方法 找到 Evolved Transformer 现有 模型 还 
「 不懂 」 自然语言 20多 位 研究 者 谈 NLP 
四大 开放 性问题 放弃 幻想 全面 拥抱 Transformer 自然语言 处理 
三大 特征 抽取 器 CNN / RNN / TF 比较 
史上 最强 NLP 知识 集合 知识结构 发展 历程 导师 名单 
不 只有 BERT 盘点 2018年 NLP 令人 激动 的 10大 
想法 清华大学 发布 10大 机器翻译 学习 必读 论文 清单 | 
资源 一文 概述 2018 年 深度 学习 NLP 十大 创新 
思路 博客 | 斯坦福大学 自然语言 处理 中 的 深度 学习 
CS 224D notes 1 CMU 课程 上 新 Neural Networks 
for NLP 18年 视频 课件 放出 斯坦福大学 2019年 NLP 课程 
上线 下 周二 开课 | 附 PPT + 视频 时隔 
两年 斯坦福 NLP 标准 公开课 CS224N 将 再次 开放 视频 
这套 GitHub 1300 星 的 NLP 课程 即将 完结 视频 
授课 在线 答疑 丨 课程 word2vec 本来 就是 用来 解决 
自然语言 处理 问题 的 它 在 NLP 中 的 应用 
是 显然 的 比如 你 可以 直接 用 它 来 
寻找 相关 词 发现 新词 命名 实体 识别 信息 索引 
情感 分析 等 你 也 可以 将 词 向量 作为 
其他 模型 的 输入 用于 诸如 文本 分类 聚 类 
等 各种 自然语言 处理 问题 事实上 word2vec 的 思想 和 
工具 还 可以 应用 于 自然 语言 处理 之外 的 
其他 领域 一个词 无非 就是 个 符号 句子 是 词 
的 序列 无非 也 就是 个 符号 序列 如果 我们 
能够 在 其他 的 应用 场景 中 构造 出 一些 
符号 还有 这些 符号 形成 的 序列 那 我们 就 
可以 试 一把 word2vec 下面 是 根据 网络 上 的 
资料 整理 的 word2vec 在 自然 语言 处理 领域 之外 
的 一些 应用 社交 网络 应用 场景 在 社交 网络 
中 给 当前 用户 推荐 他 / 她 可能 关注 
的 大 V 映射 关系 每 一个 大 V 就是 
一个 词 将 每个 用户 关注 的 大 V 按照 
关注 的 顺序 排列 形成 文章 App 商店 应用 场景 
App 商店 中 向 用户 推荐 感兴趣 的 App 映射 
关系 每个 App 就是 一个 词 将 每个 用户 下载 
的 App 按照 下载 的 顺序 排列 形成 文章 广告 
系统 应用 场景 广告主 在 媒体 网站 上 打广告 媒体 
网站 提供 一个 后台 管理系统 可以 让 广告主 自行 决定 
要 将 广告 推荐 给 哪些 目标 人群 映射 关系 
每一个 页面 就 是 一个 词 将 每个 用户 浏览 
的 页面 按照 浏览 的 顺序 排列 形成 文章 这样 
根据 训练 后的词/nr 向量 就 可以 计算出 页面 之间 的 
相关 程度 那 目标 用户 怎么 计算 呢 浏览 与 
广告主 的 广告 页 相关 的 页面 的 用户 就是 
广告主 潜在 的 目标 用户 把 这些 用户 推荐 给 
广告主 就 可以 了 应用 场景 广告 系统 中 广告主 
上线 了 一支 新 广告 如何 估算 用户 对 新 
广告 的 CTR Click Through Rate 即 点击 通过率 映射 
关系 和 上面 给 广告主 推荐 目标 用户 一样 的 
做法 可以 计算 出 每个 广告 页 对应 的 向量 
然后 对 这些 广告 页 做一个 聚 类 把 相似 
的 广告 页 聚 在 一个 簇 中 用 新 
广告 所在 簇 的 CTR 来 近似 新 广告 的 
CTR 向量 快速 检索 综合 以上 各种 应用 将 各种 
文档 转换成 向量 之后 常见 一个 基本 操作 就是 输入 
一个 文档 对应 的 向量 寻找 和它最/nr 相关 的 top 
k 个 文档 对应 的 向量 如果 要 所有 文档 
都 比对 一遍 的话 那 时间 复杂度 就是 O n 
这在 实际 的 工程 应用 中就 太慢 了 因此 需要 
借助 redis 或者 引入 kd tree simhash 聚 类 等 
算法 来 加速 检索 参考 word2vec 在 工业界 的 应用 
场景 深度 学习 word2vec 笔记 之 应用 篇 word2vec 有什么 
应用 A non NLP application of Word2VecWord2Vec with Non Textual 
Data 原文 链接 http / / www . ipaomi . 
com / 2017 / 09/22 / word2vec 在 非 自然语言 
处理 nlp 领域 的 应用 / 下载 链接 精通 Python 
自然语言 处理 带 完整 书签 微信 公众 号 关键字 全网 
搜索 最新 排名 机器学习 算法 排名 第一 机器学习 排名 第一 
Python 排名 第三 算法 排名 第四 如果 一 台 计算机 
能够 欺骗 人类 让人 相信 它 是 人类 那么 该 
计算机 就 应当 被 认为 是 智能 的 阿兰 图灵 
机器 能跟/nr 我们 人类 交流 吗 能像/nr 我们 人类 一样 
理解 文本 吗 这是 大家 对 人工智能 最初 的 幻想 
如今 它 已 成为 人工智能 的 核心 领域 自然语言 处理 
简称 NLP 自然语言 处理 是 一门 融 语言学 计算机科学 人工智能 
于 一体 的 科学 解决 的 是 让 机器 可以 
理解 自然语言 这一 到 目前 为止 都还/nr 只是 人类 独有 
的 特权 因此 被 誉为 人工智能 皇冠 上 的 明珠 
如今 这门 学科 受到 了 国家 政府 各 大 企业 
的 普遍 关注 国务院 新一代 人工智能 发展规划 明确指出 建立 新一代 
人工智能 关键 共性 技术 体系 自然语言 处理 技术 作为 八大 
共性 技术 之一 被 重点 强调 和 扶持 无处不在 的 
自然 语言 处理 我们 每天 都在/nr 使用 或 受益 于 
自然语言 处理 的 技术 举个 例子 微软 小冰 是 中国 
微博 上 的 一款 将 对话 带入 我们 日常 生活 
的 聊天 机器人 百万 年轻 中国 用户 通过 小冰 交换 
信息 与 他人 分手 丢 了 工作 或 感觉 沮丧 
时 人们 经常 会 和 小冰 聊天 到目前 小冰 已经 
累积 了 上亿 用户 平均 聊天 的 回数 23轮 平时 
聊天 时长 大概 是 25 分钟 左右 自然语言 处理 技术 
更 广泛 使用 可见 下面 的 案例 机器翻译 去年 秋天 
谷歌 翻译 推出 了 一个 全新 升级 的 人工智能 翻译 
引擎 这样一来 曾以 产出 语言 生硬 但又 可用 的 翻译 
而 闻名 的 谷歌 翻译 已 开始 产出 语言 流畅 
精确度 高的/nr 翻译 文本 对 未经 专业翻译 训练 的 人 
来说 这种 文本 输出 几乎 与 人工 翻译 并未 有 
区别 我们 将 上面 这 段 文字 输入 到 谷歌 
翻译 中 中译英 输出 的 英文 句子 让人 惊叹 图一 
谷歌 翻译 示意图 垃圾邮件 检测 在 自动 垃圾邮件 检测 等 
一些 应用 中 分类 只有 两个 垃圾 邮件 和非/nr 垃圾邮件 
在 其它 情况 下 分类器 可以 有 多个 分类 比如 
按 主题 组织 新闻 报道 或 按 领域 组织 学术论文 
而要 是 一篇 博客 文章 谈论 的 是 体育 和 
娱乐 又会 怎样 一个 分类器 如何 在 多个 选项 之间 
选择 正确 的 分类 那/r 依赖/v 于/p 具体/a 应用/v 它 
可以 简单 地 选择 最 有可能 的 选项 但 有时候 
为 一个 文本 分配 多个 分类 是 有 意义 的 
图二 邮件 自动 分类 问答 系统 从 2011年 Siri 诞生 
到 Google Now 再到 Cortana 和 Alexa 作为 语音 助手 
其实 它们 本质上 都是 问答 系统 这 几个 都是/nr 面向 
公开 领域 的 问答 系统 在 我们 的 日常 生活 
中 帮忙 定 闹钟 打电话 导航 搜索 问题 偶尔 还 
能讲 讲 笑话 也正 让 我们 的 生活 越来越 方便 
图三 苹果 Siri 示意图 尤其 是 2010 年后 深度 学习 
应用 于 自然 语言 处理 领域 一 系列 的 产品 
功能 逐渐 走进 我们 的 生活 各 大 企业 也 
在 纷纷 布局 相关 产业 重金 招揽 相关 领域 人才 
我国 在 语言 文字 信息 处理 方面 就 诞生 了 
三家 上市公司 从 上市 的 顺序 来说 最早 是 汉王 
做 模式识别 后来 科大 讯 飞 做 语音识别 然后 是 
拓 而 思 的 信息 检索 和 文本 挖掘 图 
四 知名 招聘 网站 岗 位图 作为 人工智能 的 一大 
热门 研究领域 如何 从 基础 开始 入门 并 学习 到 
最新 的 技术 呢 自然语言 处理 领域 知名 青年 学者 
国际 顶级 会议 作者 周教授 推出 自然语言 处理 基础 与 
算法 实践 基于 深度 学习 的 自然 语言 处理 两门 
在线 直播 课程 基础课 + 提高 课 课程 优秀 学员 
可 直接 推荐 至 百度 搜狗 今日 头条 等 知名 
企业 实习 就业 课程 讲师 周老师 教授 硕士生 导师 中科院 
自动化所 博士 主要 从事 自然语言 处理 以及 深度 学习 等 
方面 的 研究 工作 在 相关 领域 国际 期刊 以及 
国际 顶级 学术会议 ACL 等 发表 论文 20 余篇 先后 
两次 获得 国际 会议 最佳 论文 奖 目前 承担 国家 
自然科学 基金 973 子课题 等 10 余项 课程 特色 1 
.   顶级 会议 作者 主讲 洞悉 技术 前沿 2 
.   理论 结合 实践 基础 搭配 强化 课程 3 
.   课上 在线 直播 答疑 课下 微信 群 答疑 
4 .   优秀 学员 推荐 名企 实习 就业 5 
.   课程 PPT 数据集 和 源程序 均 向 学员 
公开 课程 目录 Part   I 基础 课程 10 学时 
1 . 句法分析 与 语义分析 2 学时 1.1   依存 
句法分析 1.2   语义 角色 标注 1.3     相关 
数据集 工具 介绍 2 . 观点 挖掘 与 情感 分析 
2 学时 2.1     句子 级 情感 分析 2.2 
    文 档级 情感 分析 2.3     跨语言 
情感 分析 2.4     跨 领域 情感 分析 2.5 
    相关 数据集 工具 介绍 3 . 信息 抽取 
part 1 2 学时 3.1     命名 实体 识别 
与 抽取 3.2     实体 消 歧 3.3   
  相关 数据集 工具 介绍 4 . 信息 抽取 part 
2 2 学时 4.1     实体 关系 抽取 4.2 
    事件 抽取 4.3     相关 数据集 工具 
介绍 5 . 问答 系统 2 学时 5.1     
检索 式 问答 5.2     社区 问答 5.3   
  知识库 问答 5.4     相关 数据集 工具 介绍 
Part   II 基于 深度 学习 的 NLP 实战 24 
学时 6 . 基于 深度 学习 的 词 法分析 4 
学时 6.1 基于 深度 学习 的 中文分词 6.2   基于 
深度 学习 的 词性 标注 6.3   基于 深度 学习 
的 命名 实体 识别 6.4   代码 模块 演示 常用 
工具 和 公共 数据集 7 . 基于 深度 学习 的 
句法 与 语义分析 4 学时 7.1   基于 图 的 
依存 句法分析 7.2   基于 转移 的 依存 句法分析 7.3 
  浅层 语义 角色 标注 7.4   代码 模块 演示 
常用 工具 和 公共 数据集 8 . 基于 深度 学习 
的 情感 分析 4 学时 8.1   基于 深度 学习 
的 情感 词典 构建 8.2   基于 深度 学习 的 
句子 级 情感 分析 8.3   基于 深度 学习 的 
文档 级 情感 分析 8.4   基于 深度 学习 的 
跨语言 情感 分析 8.5   代码 模块 演示 常用 工具 
和 公共 数据集 9 . 基于 深度 学习 的 信息 
抽取 Part 1 4 学时 9.1   基于 深度 学习 
的 实体 关系 抽取 9.2   基于 深度 学习 的 
实体 消 歧 9.3   代表性 系统 模块 演示 常用 
工具 和 公共 数据集 10 . 基于 深度 学习 的 
信息 抽取 Part 2 4 学时 10.1   基于 深度 
学习 的 事件 抽取 10.2   基于 深度 学习 的 
知识库 表示 10.3   基于 深度 学习 的 知识库 补全 
10.4   代码 模块 演示 常用 工具 和 公共 数据集 
11 . 基于 深度 学习 的 问答 系统 4 学时 
11.1   基于 深度 学习 的 社区 问答 11.2   
基于 深度 学习 的 复杂 问句 解析 11.3   基于 
深度 学习 的 知识库 问答 11.4   代码 模块 演示 
常用 工具 和 公共 数据集 报名 前 100名 可 领取 
  150元 优惠券   1月 6日 正式 开课 每周六 周日 
晚 19 点到 21 点 在线 直播 授课 一年 内 
可以 无限 次 在线 回放 注 基础 课程 1月 6号 
开课 适合 小白 用户 入门 实践 环节 采用 开源 工具 
无需 编程 提高 课程 3月 中旬 开课 主要 实践 领域内 
最新 最 前沿 的 技术 需要 深度 学习 基础 采用 
Python 2.8 与 深度 学习 TensorFlow 框架 请 添加 工作 
人员 「 深蓝 学院 」 助教 报名 人工智能 Python 的 
自然 语言 处理 视频 课程 共 33 课时 共 6 
小时 25 分钟 更新 时间 2018 08 02 本套 课程 
是 针对 人工智能 领域 – 自然语言 理解 的 视频 讲解 
介绍 了 python 语言 对 自然 语言 处理 的 工具包 
以及 自然 语言 处理 的 方法 使用 本套 课程 真对 
具有 python 编程 基础 的 同学 在有 python 编程 的 
基础 上 学习 本套 视频 课程 会 比较 轻松 的 
掌握 python 对 自然 语言 处理 的 过程 以及 使用 
的 方法 成为 NLP 工程 适用 人群 有 英文 基础 
有 python 编程 基础 课程 简介 python 版本 2.7 课程目标 
处理 语言 英语 English NLP 自然语言 工程师 初级 课程 所谓 
自然语言 是 指 人们 日常 交流 使用 的 语言 本套 
课程 是 针对 人工智能 领域 – 自然语言 理解 的 入门 
视频 讲解 介绍 了 python 语言 对 自然 语言 处理 
的 工具包 以及 自然 语言 处理 的 方法 使用 本套 
课程 真对 具有 python 编程 基础 的 同学 在有 python 
编程 的 基础 上 学习 本套 视频 课程 会 比较 
轻松 的 掌握 python 对 自然 语言 处理 的 过程 
以及 使用 的 方法 该 套 课程 能够 帮助 想要 
进入 人工智能 自然语言 处理 NLP 行业 的 程序员 对 自然 
语言 处理 有 初步 的 掌握 与 理解 成为 一名 
出色 的 自然 语言 工程师 我 相信 这套 课程 会 
是 你 的 开始 加入 我们 人工智能 的 大家庭 吧 
祝 同学们 学有所成 课程 介绍 课时 1 1 人工智能 基本概念 
介绍 课时 1 2 自然语言 处理 导论 课时 2 1 
NLTK 函数 使用 一 课时 2 2 NLTK 函数 使用 
二 课时 2 3 计算机 的 简单 统计 课时 2 
4 细粒度 划分 基础 课时 2 5 细粒度 划分 条件 
详解 课时 2 6 自动 理解 自然语言 概念 课时 3 
1 nltk 操作 语料库 的 函数 介绍 课时 3 2 
古腾堡/nr 语料库 课时 3 3 网络 和 聊天 文本库 课时 
3 4 布朗 语料库 课时 3 5 路透社 语料库 课时 
3 6 就职演说 语料库 课时 3 7 标注 文本 语料库 
课时 3 8 文本 语料库 的 结构 与 获取 自己 
的 语料库 课时 3 9 条件 频率分布 于 图表 绘制 
课时 4 1 网络 读取 文件 处理 HTML 课时 4 
2 对 搜索引擎 与 RSS 订阅 的 处理 课时 4 
3 读取 本地 文件 与 NLP 流程 介绍 课时 4 
4 使用 Unicode 进行 文本处理 课时 4 5 使用 正则表达式 
进行 词干 提取 课时 4 6 NLTK 词干 提取 器 
课时 4 7 使用 正则表达式 为 文本 分词 课时 5 
1 使用 词性 标注 器 课时 5 2 对 语料库 
进行 词性 标注 课时 5 3 自动 标注 默认 标注 
课时 5 4 自动 标注 正则表达式 标注 课时 5 5 
N gram 标注 课时 6 1 信息提取 介绍 课时 6 
2 名词 短语 分块 课时 6 3 标记 模式 课时 
6 4 理解 自然语言 全套 视频 更新 完毕 百度 云下载 
地址 http / / itxuexiweb . com / thread 104 
1 1 . html nodejs 做 自然语言 处理 是 非常 
可行 的 这次 我 做 了 一些 小小的 尝试 一起 
来 体验 一下 吧 因为 还 保持 着 对 自然 
语言 处理 的 那份 热爱 最近 没事 的 时候 会把 
毕业论文 翻 出来 看 毕业 论文 的 课题 就是 关于 
自然 语言 处理 的 然后 在 我 的 新 博客 
中 加入 了 一些 相关 的 处理 主要 做 了 
以下 几个 方面 对 每 一篇 文章 进行 快速 的 
内容 理解 根据 标题 和 内容 输出 多个 内容 标签 
对 文章 按照 内容 进行 自动 分类 为 文章 聚 
类 文本 内容 分析 等 提供 基础 根据 文章 标题 
用户 自定义 标签 以及 人工智能 获 得到 的 标签 进行 
相似 度 计算 在 阅读 一篇 文章 的 时候 通过 
相似 度 计算 的 结果 推荐 相关 的 文章 給 
用户 下面 给出 自动 输出 内容 标签 的 结果 图 
博客 系统 运行 环境 centos9 + docker 开发 语言 nodejs 
数据库 MariaDB 开发 框架 eggjs + nunjucks 模板 引擎 这次 
也是 我 第一次 做 后端 渲染 的 博客 ajax 的 
网站 做 seo 是 真的 不 好做 然后 这次 也是 
我 第一次 正儿八经 的 用了 下 阿里 大佬 们 的 
eggjs 这种 洋葱 模型 的 框架 我 真的 是 超级 
喜欢 不管 是 用 es7 优雅 地 处理 js 异步 
还是 经典 的 MVC 还是 框架 的 插件 机制 等等 
确实 是 超级 赞 的 如果 有 喜欢 nodejs 的 
同志 强力 推荐 此 框架 推荐 系统 推荐 系统 是 
我们 平时 在 用 软件 或者 网站 中 经常 会 
遇见 的 比如 资讯 类 的 百度 feed 头条 qq 
看点 等 电商 类 的 阿里 京东 等等 还有 抖 
音 什么 的 很多很多 一个 好 的 推荐 系统 可以 
带来 更多 的 收益 but 一个 不好 的 推荐 系统 
往往 会 得到 别人 的 吐槽 之前 在 脉脉 看到 
某公司 CTO 收到 脉脉 推荐 的 安卓 工程师 的 推荐 
职位 遭到 吐槽 百度 李彦宏 某天 因为 没有 在 feed 
收到 一条 重要 的 科技 资讯 信息 而 吐槽 自家 
员工 这样 的 事情 通常 会 很多 我 觉得 一个 
好 的 推荐 系统 应该 更 懂 人 假如 我 
最近 一个 月 前 买了 一部 手机 我 希望 能给我/nr 
推送 一些 手机 配件 而 不是 在给 我 推送 一部 
手机 这个 时候 我 买 手机 配件 的 概率 是 
远远 大于 在 买 一部 手机 的 现在 很多 推荐 
系统 都是/nr 通过 用户 画像 加上 各种 埋 点 用户 
操作 数据 从而 进行 分析 推送 的 我 觉得 未必 
不 可以 在 此 基础上 加上 情感 分析 多 一个 
维度 或许 能够 得到 更 准确 的 数据 说 了 
这么 多 我 觉得 还是 有 很多 瓶颈 存在 的 
? ? 现在 的 AI 就像 很多 年前 的 移动 
互联网 正处于 上升期 我们 还有 很多 事情 可以 做 下面 
进入 今天 的 真题 这次 做 的 文章 推荐 系统 
分享 一些 细节 给 大家 图中 右侧 部分 就是 我们 
这个 文章 推送 系统 的 推送 结果 我们 用 不同 
的 颜色 标注 了 这篇文章 和 当前 正在 浏览 的 
文章 的 关联度 颜色 越深 表示 关联度 越高 置信度 越高 
权重 越大 这个 推荐 系统 中 主要 使用 了 上面 
所说 的 第三 点 相似 度 计算 使用 的 数学 
模型 为 空间 向量 模型 空间 向量 模型 能够 将 
非 结构化 的 文本 数据 转换成 向量 形式 表示 成 
向量 形式 之后 能为 之后 的 处理 过程 打下 良好 
的 数学 基础 空间 向量 模型 帮助 我们 把 每篇 
文档 转化 为 一个 多 维 的 空间 向量 形式 
https / / wx4 . sinaimg . cn / large 
/ 8 f 2 9 f 1 0 b g 
y 1 f w i 8 f l 6 l 
a x j 2 0 d u 0 1 p 
t 8 l . jpg 其中 向量 W1i 表示 第 
一个词 占 文档 Ci 的 比重 向量 W2i 表示 第二个 
词 占 文档 Ci 的 比重 依次 类推 向量 Wti 
表示 第 t 个 词 占 文档 Ci 的 比重 
那么 两篇 文章 的 相似 度 我们 就 可以 计算 
他们 对应 向量 的 夹角 余弦 值 来 进行 计算 
两个 文档 的 余弦 值 越 接近 1 这 两个 
文档 则 越 相似 下面 给出 计算 相似 度 的 
关键 代码 目录 一 自然语言 处理 介绍 概念 子 领域 
数据集 工具包 二 APIJieba 分词 Pyltp 分词 词性 标注 命名 
实体 识别 句法 依存 树 语义 角色 标注 NLTK 词性 
词性 标注 提取 词频 提取 词根 词形 还原 编辑 距离 
Pre trained BERT 特征提取 GloVe 词 嵌入 向量 Spacy 词 
嵌入 向量 Gensim 词 嵌入 训练 其他 三 相关 算法 
一 自然语言 处理 介绍 概念 通俗 而言 自然语言 处理 Natural 
Language Processing 即为 处理 与 人类 语言 相关 的 各项 
任务 与 计算机 视觉 类似 是 一个 由来已久 却在 近几年 
被 神经网络 颠覆 的 传统 领域 在 人机对话 搜索引擎 后台 
广告 推荐 机器翻译 语音 识别 等 领域 有 广泛 应用 
传统 的 自然 语言 处理 以 统计学 为 根基 发 
展出 了 各具特色 的 优异 模型 其中 最为 著名 的 
包括 朴素 贝叶斯 隐 马尔科夫 模型 HMM 条件 随 机场 
CRF 神经 网络 的 出现 使得 NLP 领域 得到 空前 
的 发展 从 Word2Vec 2013 到 Attention 机制 2014 Transformer 
2017 号称 开启 NLP 新纪元 的 集大成 者 BERT 2018 
再到 近期 卡内基 梅隆 大学 的 团队 研发 的 XLNet 
2019 NLP 在 不断 树立 新的 里程碑 走在 人工智能 的 
前沿 子 领域 语音 文本 文本 朗读 Text to Speech 
语音合成 Speech Synthesis 语音识别 Speech Recognition 自然语言 理解 中文分词 Chinese 
Word Segmentation 词性 标注 Part of Speech Tagging 句法分析 Parsing 
情绪 分析 Sentiment Analysis 文字 蕴涵 Textual Entailment 自然语言 生成 
Natural Language Generation 问答 系统 Question Answering 人机对话 Man Machine 
Interaction 文字 校对 Text Proofing 机器翻译 Machine Translation 大型 文本 
分析 信息 抽取 Information Extraction 自动 摘要 Automatic Summarization 文本 
分类 Text Categorization 信息检索 Information Retrieval 字符 串处理 模式匹配 Pattern 
Matching 文本 相似 度 Text Similarity 文本 压缩 Text Compression 
数据集 数据集 内容 领域 语言 数量 IWSLTTED 演讲 多国 语言 
字幕 机器翻译 中英 不限 SQuAD 维基百科 词条 文档 问答 英文 
150 000 + DuReader 用户 日志 文档 问答 中文 CoQA 
人为 对话 对话 问答 英文 127 000 + LOB 历史文献 
词性 标注 英文 1 000 000 这里 只 呈现 笔者 
自己 熟悉 的 数据 集 网上 有 很多 关于 开源 
数据集 的 总结 博文 这里 推荐 几篇 https / / 
www . jiqizhixin . com / articles / 2018 09 
05 2https / / blog . csdn . net / 
e n o h t z v q i j 
x o 0 0 a t z 3 y 8 
/ article / details / 80163069 工具包 中文 NLP 领域 
著名 的 Python 工具包 列示 如下 Python Language Technology Platform 
Pyltp Pyltp 是 LTP 的 Python 封装 提供 了 分词 
词性 标注 命名 实体 识别 依存 句法分析 语义 角色 标注 
的 功能 Jieba 专业 提供 分词 功能 的 工具包 Standford 
NLP 除 可以 实现 Pyltp 的 功能 以外 还能 进行 
情绪 分析 但 安装 较为 复杂 需要 通过 Java 安装 
并 设置 Python 接口 英文 NLP 领域 有 Natural Language 
Toolkit NLTK A Python library that provides modules for processing 
text classifying frequency analyzing tokenizing stemming part of speech tagging 
parsing and more . Apache OpenNLPA machine learning toolkit that 
provides tokenizers sentence segmentation part of speech tagging named entity 
extraction chunking parsing coreference resolution and more . Standford NLPA 
suite of NLP tools that provide part of speech tagging 
the named entity recognizer coreference resolution system sentiment analysis and 
more . GlounNLPProvides implementations of the state of the art 
SOTA deep learning models in NLP and build blocks for 
text data pipelines and models . It is designed for 
engineers researchers and students to fast prototype research ideas and 
products based on these models . 二 API 笔者 将 
分词 词性 标注 命名 实体 识别 句法分析 语义 角色 标注 
等 应用 领域 底层 的 NLP 任务 定义 为 基础 
NLP 任务 在 实际 的 应用 研究 与 开发 时 
由于 语料库 的 准备 成本 较高 预 训练 通常 也 
耗时 过长 在 研究 时 这 一部分 任务 通常 可以 
通过 调用 第三方 专业 机构 预 训 练好 的 模型 
实现 将 更多 的 注意力 集中 到 上层 模型 的 
设计 和 搭建 特别地 当 预 训练 的 模型 无法 
满足 实际 的 业务 需求 时 可以 通过 定义 用户 
词典 修正 模型 结果 本章 详细 列示 基础 NLP 任务 
的 API 实现 由于 中文 的 特殊性 中文 NLP 与 
英文 NLP 的 一大 不同 在于 中文 文本处理 需要 借助 
语料库 预先 对 语句 进行 分词 而 英文 只 需要 
通过 空格 即可 完成 在 Python 语言 环境 下 运用 
Nshort 中文分词 算法 的 Jieba 出于 杰出 的 分词 效果 
以及 安装 和 使用 方便 成为 最为 著名 的 中文分词 
工具 在 其他 的 基础 NLP 任务 上 哈尔滨工业大学 开发 
的 Pyltp 库 更为 专业 和 全面 词库 储备 也 
更为 丰富 在 学术界 和 工业界 得到 广泛 应用 Jieba 
分词 关于 Jieba 以下 仅 列示 分词 相关 代码 import 
jieba sentence = 里约热内卢 的 奶牛 拿 榴莲 牛奶 以 
折 足 之 姿 跑到 委内瑞拉 拿了 蜂花 护发素 送给 
红鲤鱼 与 绿 鲤鱼 与 驴 wordlist = jieba . 
cut sentence # 精确 模式 wordlist = jieba . cut 
sentence cut _ all = True # 全 模式 wordlist 
= jieba . cut _ for _ search sentence # 
搜索引擎 模式 jieba . load _ userdict open r D 
\ NLP resources \ jieba userdict . txt encoding = 
gbk # 导入 用户 词典 精确 模式 试图 将 句子 
最 精确地 切开 适合 文本 分析 全 模式 把/p 句子/n 
中/f 所有/b 的/uj 可以/c 成词的/nr 词语/n 都/d 扫描/v 出来/v 速度 
非常 快 但是 不能 解决 歧义 搜索引擎 模式 在 精确 
模式 的 基础 上 对 长词 再次 切分 提高 召回率 
适合 用于 搜索引擎 分词 使用 用户 词典 由 用户 自行 
选择 地址 新建 词典 txt 文档 需要 满足 每行 词语 
词频 词性 的 编写 要求 例 榴莲 牛奶 5 n 
Pyltp 分词 词性 标注 命名 实体 识别 句法 依存 树 
语义 角色 标注 from pyltp import * sentence = 里约热内卢 
的 奶牛 拿 榴莲 牛奶 以 折 足 之 姿 
跑到 委内瑞拉 拿了 蜂花 护发素 送给 红鲤鱼 与 绿 鲤鱼 
与 驴 # 分词 segmentor = Segmentor segmentor . load 
r D \ NLP resources \ cws . model words 
= segmentor . segment sentence print | . join words 
# 词性 标注 pos _ tagger = Postagger pos _ 
tagger . load r D \ NLP resources \ pos 
. model pos _ tags = pos _ tagger . 
postag words for word pos _ tag in zip words 
pos _ tags print word + / + pos _ 
tag # 命名 实体 识别 recognizer = N a m 
e d E n t i t y R e 
c o g n i z e r recognizer . 
load r D \ NLP resources \ ner . model 
ne _ tags = recognizer . recognize words pos _ 
tags for word pos _ tag ne _ tag in 
zip words pos _ tags ne _ tags print word 
+ / + pos _ tag + / + ne 
_ tag # 句法 依存 树 import nltk from nltk 
. tree import Tree from nltk . grammar import D 
e p e n d e n c y G 
r a m m a r from nltk . parse 
import * import re parser = Parser parser . load 
r D \ NLP resources \ parser . model arcs 
= parser . parse words pos _ tags conll = 
for i in range len arcs if arcs i . 
head = = 0 arcs i . relation = ROOT 
conll + = \ n + words i + + 
pos _ tags i + \ t + pos _ 
tags i + \ t + str arcs i . 
head + \ t + arcs i . relation print 
conll conlltree = DependencyGraph conll tree = conlltree . tree 
tree . draw # 语义 角色 标注 labeller = e 
m e n t i c R o l e 
L a b e l l e r labeller . 
load r D \ NLP resources \ pisrl _ win 
. model roles = labeller . label words pos _ 
tags arcs for role in roles print words role . 
index . join % s % s % arg . 
name . join words arg . range . start arg 
. range . end + 1 for arg in role 
. arguments NLTK 词性 词性 标注 提取 词频 提取 词根 
词形 还原 编辑 距离 import nltk article = Beyonc é 
Giselle Knowles Carter / bi ː ˈ j ɒ nse 
ɪ / bee YON say born September 4 1981 is 
an American singer songwriter record producer and actress . Born 
and raised in Houston Texas she performed in various singing 
and dancing competitions as a child and rose to fame 
in the late 1990s as lead singer of R & 
B girl group Destiny s Child . # 常用 功能 
tokens = nltk . word _ tokenize article # 分词 
nltk . pos _ tag tokens # 词性 标注 nltk 
. FreqDist w . lower for w in tokens # 
提取 词频 nltk . PorterStemmer . stem lying # 提取 
词根 nltk . stem . W o r d N 
e t L e m m a t i z 
e r . lemmatize dancing v # 词形 还原 nltk 
. edit _ distance humble dumpy # 编辑 距离 # 
查看 nltk 词性 标注 分类 nltk . help . upenn 
_ tagset # 下载 功能 包 nltk . download Pre 
trained BERT 特征提取 当前 最 受欢迎 的 预 训练 BERT 
库 需要 预先 下载 参数 文件 Github 地址 名为 BERT 
实则 同时 包含 了 GPT Transformer XL GPT 2 的 
预 训练 参数 提取/v 隐藏/v 状态/n 后/f 可直接/i 嫁接/v 于/p 
任何/r 下游/f 任务/n import torch from pytorch _ pretrained _ 
bert import BertTokenizer BertModel BertForMaskedLM # tokens text = CLS 
Who was Henson SEP Jim MASK was a puppeteer SEP 
tokenizer = BertTokenizer . from _ pretrained r D \ 
NLP \ BERT \ pytorch pretrained BERT \ bert tokenization 
vocabulary . txt # 这里 文本 改为 bert base uncased 
将 自动 下载 参数 文件 下同 tokens = tokenizer . 
tokenize text indicies = tokenizer . convert _ tokens _ 
to _ ids tokens tokens _ tensor = torch . 
tensor indicies tokens _ tensor = tokens _ tensor . 
to cuda # 迁移 至 GPU 运行 # segments segments 
_ ids = 0 0 0 0 0 0 1 
1 1 1 1 1 1 segments _ tensor = 
torch . tensor segments _ ids segments _ tensor = 
segments _ tensor . to cuda # model model = 
BertModel . from _ pretrained r D \ NLP \ 
BERT \ pytorch pretrained BERT \ bert base uncased . 
tar . gz model . to cuda # forward with 
torch . no _ grad encoded _ layers _ = 
model tokens _ tensor segments _ tensor GloVe 词 嵌入 
向量 GloVe 是 斯坦福 大学 提供 的 专业 词 嵌入 
算法 在 官网 同时 开放源码 和预/nr 训练 词 向量 供 
免费 下载 # 使用 预 训练 词 向量 模型 from 
tqdm import tqdm import numpy as np X = np 
. empty 400000 300 word _ to _ id id 
_ to _ word idx = { } { } 
0 with open r D \ NLP \ glove . 
6B \ glove . 6B . 300d . txt r 
encoding = utf 8 as f # 载入 前 需 
提前 下载 for line in tqdm f total = 400000 
line = line . strip . split vector = list 
map float line 1 X idx = vector word _ 
to _ id line 0 = idx id _ to 
_ word idx = line 0 idx + = 1 
# 查看 词 向量空间 分布 import pandas as pd import 
matplotlib . pyplot as plt def pca X # PCA 
将 词 向量 降至 二维 X = pd . DataFrame 
X X = X X . mean / X . 
std X = np . matrix X cov = X 
. T * X / X . shape 0 U 
S V = np . linalg . svd cov return 
U U = pca X # 提取 正交矩阵 Y = 
np . dot X U 2 # 获取 降 维 
数据 ax = plt . subplot 111 for word in 
list word _ to _ id . keys 20 coordinate 
= Y word _ to _ id word ax . 
scatter coordinate 0 0 coordinate 0 1 ax . annotate 
word xy = coordinate 0 0 coordinate 0 1 # 
坐标 点 xycoords = data # 坐标 点 类型 xytext 
= + 5 + 5 # 标注 文字 相对位置 textcoords 
= offset points # 标注 文字 类型 fontsize = 16 
# 标注 文字大小 plt . show Spacy 词 嵌入 向量 
Spacy 是 一个 相较 于 NLTK 执行 效率 更高 各 
基础 任务 准确度 也 更高 的 专业 NLP 工具包 在 
这里 仅 列示 词 嵌入 向量 的 代码 感 兴趣 
的 读者 可 自行 检索 # 使用 预 训练 词 
向量 模型 import spacy import numpy as np library = 
spacy . load en _ core _ web _ lg 
# 载入 前 需 提前 下载 article = let coward 
father mother brother sister juice milk is are be to 
2013 2014 2015 2016 2017 2018 tokens = library article 
X = np . empty 0 300 word _ to 
_ id id _ to _ word idx = { 
} { } 0 for token in tokens X = 
np . vstack X token . vector word _ to 
_ id str token = idx id _ to _ 
word idx = str token idx + = 1 # 
查看 词 向量空间 分布 import pandas as pd import matplotlib 
. pyplot as plt def pca X # PCA 将 
词 向量 降至 二维 X = pd . DataFrame X 
X = X X . mean / X . std 
X = np . matrix X cov = X . 
T * X / X . shape 0 U S 
V = np . linalg . svd cov return U 
U = pca X # 提取 正交矩阵 Y = np 
. dot X U 2 # 获取 降 维 数据 
ax = plt . subplot 111 for word in word 
_ to _ id . keys coordinate = Y word 
_ to _ id word ax . scatter coordinate 0 
0 coordinate 0 1 ax . annotate word xy = 
coordinate 0 0 coordinate 0 1 # 坐标 点 xycoords 
= data # 坐标 点 类型 xytext = + 5 
+ 5 # 标注 文字 相对位置 textcoords = offset points 
# 标注 文字 类型 fontsize = 16 # 标注 文字大小 
plt . show Gensim 词 嵌入 训练 # 自行 训练 
词 向量 from gensim . models import Word2Vec sentences = 
cat say meow dog say woof model = Word2Vec min 
_ count = 1 model . build _ vocab sentences 
# 搭建 语料库 model . train sentences total _ examples 
= model . corpus _ count epochs = model . 
epochs # 训练 model . wv cat # 查看 词 
向量 model . save / word2vec # 保存 模型 model 
. vocabulary . load / word2vec # 读取 模型 其他 
# 拼写 相似 度 import difflib difflib . SequenceMatcher None 
sequence sequential . ratio 三 相关/v 算法/n 每一种/i 应用/v 都有/nr 
经过/p 长期/d 考验/vn 效果 最佳 的 算法 列示 如下 部分 
算法 笔者 提供 代码 实现 其余 请 读者 自行 搜索 
开放源码 算法 应用 链接 朴素 贝叶斯 Naive Bayes 文本 分类 
代码 隐 马尔科夫 模型 HMM 语音识别 代码 最大熵 模型 MEM 
词性 标注 代码 条件 随 机场 CRF 中文分词 语义 组块 
TF IDF + BM25 搜索引擎 广告 推荐 LDA Latent Dirichelt 
Allocation 文本 相似 度 ARC 1 / ARC 2 文本 
相似 度 bi LSTM + CRF 命名 实体 识别 IOB 
/ BIE 序列 标注 Seq2Seq 机器翻译 文本 会话 图像 字幕 
自然语言 生成 DCN 文档 问答 系统 BERT 特征提取 GPT 2 
特征提取 XLNet 特征提取 日前 杭州 一 知 智能 科技 有限公司 
宣布 在 2018 年 7 月 完成 A 轮 融资 
融资 金额 7000 万元 人民币 本轮 融资 由 启 赋 
资 本领 投 金沙江 联合 资本 等 机构 跟 投 
资金 主要 用于 进一步 加强 人工智能 NLP 人才 引进 和 
核心 技术 科研 投入 并 推出 基于 NLP 技术 的 
智能 外呼 机器人 此次 融资 是 继 2017 年 9 
月 获 金沙江 天使 投资 2000 万元 后 再获 投资 
公司 累计 已获 近 1 亿元 投资 成为 近两年 国内 
NLP 领域 获 最多 融资 的 初创 企业 之一 据悉 
杭州 一 知 智能 科技 成立 于 2017 年 8 
月 是 一家 专注 于 NLP Natural Language Processing / 
自然语言 处理 的 技术 型 人工智能 公司 主攻 中文 自然 
语义 理解 与 人机交互 的 前沿 底层 技术 研发 包括 
阅读 理解 语义 识别 多轮 对话 精准 问答 知识图谱 等 
综合 NLP 技术 在 业界 处于 领先 水平 并被 机器 
之心 评选为 「 2017 十大 最 具 潜力 早期 AI 
公司 」 公司 核心 创始 团队 及 研发 人员 来自 
香港 科技 大学 浙江大学 清华大学 卡耐基 梅隆 大学 南洋 理工大学 
中科院 等 顶尖 院校 在 AAAI IJCAI TKDE ACL KDD 
等 国际 顶级 会议 及 期刊 发表 数十 篇 论文 
横跨 机器学习 深度 学习 自然语言 处理 语义 识别 知识图谱 语义 
纠错 语音合成 等 团队 在 2017 世界 顶级 机器 阅读 
理解 大赛 SQuAD 获 世界 排名 第二 的 优异 成绩 
SQuAD 被 誉为 机器 阅读 理解 界 的 ImageNet 该 
挑战赛 不仅 有 微软 Google Facebook 腾讯 阿里巴巴 IBM 科大 
讯 飞 等 科技 公司 参与 还有 斯坦福 哈佛 卡内基 
梅隆 新加坡 国立 清华大学 北京大学 哈工大 等 国内外 知名 大学 
和 科研 院所 参与 其中 在 一 知 智能 创始人 
首席 科学家 赵洲/nr 博士 看来 NLP 技术 是 人工 智 
能从 计算 智能 感知 智能 走向 认知 智能 的 关键 
领域 也是 中美 等 大国 在 AI 前沿 科技 竞争 
的 焦点 关于 公司 的 发展 定位 赵洲/nr 博士 希望 
一 知 智能 作为 NLP 领域 前沿 科研 高地 持续 
引进 NLP 尖端 人才 科研成果 走在 业界 领先 同时 加快 
NLP 技术 应用 落地 在 金融 政务 教育 等 领域 
打造 通用 型 产品 当前 赵洲/nr 及其 带领 的 博士 
团队 与 微软 腾讯 阿里 网易 等 均有 项目 合作 
过去 五年 人工智能 企业 大量 涌现 出现 了 讯 飞 
商汤 旷 视 寒武纪 等 独角兽 科技 企业 然而 在 
认知 智能 领域 由于 理论 和 底层 技术 局限 仍 
处于 探索 初期 NLP 是 人工智能 「 皇冠 上 的 
明珠 」 是 一个 极其 复杂 的 研究 领域 全球 
来看 美国 从 2016 年起在/nr NLP 领域 投资 案例 大幅 
增多 国内 仍 少见 据 Tractica 报告 预估 到 2025 
年 全球 NLP 领域 的 软硬件 市场 规模 将 达 
223 亿美金 服务 市场 规模 有望 突破 1000 亿美金 启 
赋 资本 投资 总监 刘永 佳 认为 「 NLP 实际 
应用 的 最大 困难 来自 语义 识别 的 复杂性 中文 
语义 尤为 困难 但 NLP 技术 价值 巨大 长期 来看 
NLP 会 推动 人 机 交互 和非/nr 结构化 信息 应用 
的 巨量 蓝海 市场 在 金融 商业 教育 等 行业 
有 很大 应用 空间 机器 阅读 语义分析 多轮 对话 知识图谱 
认知 推理 等 多项 NLP 技术 综合 突破 应用 产品 
才能 成熟 」 同时 刘永 佳 表示 本轮 投资 看好 
一 知 智能 主要 基于 三 方面 考虑 一是 NLP 
当前 最好 投 顶尖 科技 人才 驱动 的 团队 赵洲/nr 
博士 团队 拥有 极强 的 创新 基因 能从/nr 根本/a 上/f 
实现/v NLP 底层 理论 和 前沿 技术 创造 二 是 
公司 虽 在 初创 期 但 其 团队 综合 实力 
强 由 一群 年轻 的 技术 天才 和 工程 产品 
团队 构成 平均 年龄 不到 30 岁 企业 创新 文化 
类似 Google 能/v 夜以继日/l 推进/v 底层/n 技术/n 和/c 产品/n 研发/l 
三 是 他们 以 市场 和 客户 为 驱动 高度 
重视 产品化 和 商业 落地 非常 务实 公司 人才 引进 
和 产品 研发 速度 很快 展望 人工智能 未来 发展 金沙江 
联合 资本 董事 总经理 王国成 表示 「 AI 科技 内涵 
和 外延 很大 未来 长达 几十 年 发展 期 AI 
在 认知 领域 才刚 起步 一 知 智能 拥有 一支 
年轻 的 科研 团队 工程 能力 持续 加强 公司 会 
高速成长 」 在 启 赋 资本 董事长 傅哲宽/nr 看来 「 
NLP 是 人工智能 领域 继 语音 计算机 视觉 芯片 之后 
最 有 机会 诞生 一批 优秀 科技 企业 的 领域 
希望 一 知 智能 把握 先机 加大 人才 引进 力度 
成为 人工智能 NLP 领域 国际 前沿 的 科研 阵地 和 
人才 集聚 地 早日 成长 为 NLP 领域 独角兽 企业 
」 产业 融资 NLP 创业 公司 1 数据 发展 到 
今天 已 不再 是 一个 新 的 概念 基于 大 
数据 技术 的 应用 也 层出不穷 但 作为 一项 发展 
前景 广阔 的 技术 其 很多 作用 还 有待 挖掘 
比如 为 人们 的 生活 带来 方便 为 企业 带来 
更多 利益 等 现今 互联 网上 每日 产生 的 数据 
已 由 曾经 的 TB 级 发展到 了 今天 的 
PB 级 EB 级 甚至 ZB 级 如此 爆炸性 的 
数据 怎样 去 使用 它 又 怎样 使 它 拥有 
不可估量 的 价值 呢 这 就 需要 不断 去 研究 
开发 让 每天 的 数据 砂砾 变为 黄金 那么 如何 
才能 将 大量 的 数据 存储 起来 并 加以 分析 
利用 呢 大 数据 技术 应运而生 在 大 数据 时代 
数据挖掘 是 最 关键 的 工作 大 数据 的 挖掘 
是从 海量 不 完全 的 有噪声 的 模糊 的 随机 
的 大型 数据库 中 发现 隐含 在 其中 有 价值 
的 潜在 有用 的 信息 和 知识 的 过程 也 
是 一种 决策 支持 过程 其 主要 基于 人工智能 机器学习 
模式 学习 统计学 等 通过 对 大 数据 高度 自动化 
地 分析 做出 归纳 性 的 推理 从中 挖掘出 潜在 
的 模式 可以 帮助 企业 商家 用户 调整 市场 政策 
减少 风险 理性 面对 市场 并 做出 正确 的 决策 
数据 管理 理念 不断 变革 大 数据 成为 信息 技术 
发展 的 必然 选择 随着 现代 信息 传播 技术 手段 
和 方式 不断 丰富 信息 获取 信息 传递 信息处理 信息 
再生 信息 利用 等 功能 应用 日益 多样化 智能化 信息系统 
逐渐 形成 一个 信息 网络 体系 人类 社会 的 生产方式 
工作方式 学习 方式 交往 方式 生活 方式 思维 方式 等 
发生 了 极其 深刻 的 变革 互动 化 即时性 全媒体 
等 成为 常态 性 的 信息 生态 环境 传统 的 
数据库 组织 架构 和 信息 服务 模式 已经 难以 适应 
信息 社会 现实 需要 整个 信息 技术 架构 的 革命性 
重构 势在必行 大 数据 成为 信息 技术 发展 的 必由之路 
灵 玖 软件 NLPIR 大 数据 语义 智能 分析 平台 
针对 中文 数据挖掘 的 综合 需求 融合 了 网络 精准 
采集 自然语言 理解 文本 挖掘 和 语义搜索 的 研究 成果 
先后 历时 十八年 服务 了 全球 四十万家 机构 用户 是 
大时代 语义 智能 分析 的 一大 利器 NLPIR 大 数据 
语义 智能 分析 平台 平台 针对 互联网内容 处理 的 需要 
融合 了 自然 语言 理解 网络 搜索 和 文本 挖掘 
的 技术 提供 了 用于 技术 二次 开发 的 基础 
工具集 NLPIR 能够 全方位 多角度 满足 应 用者 对 大 
数据 文本 的 处理 需求 包括 大 数据 完整 的 
技术 链条 网络 采集 正文 提取 中英文 分词 词性 标注 
实体 抽取 词频 统计 关键词 提取 语义 信息 抽取 文本 
分类 情感 分析 语义 深度 扩展 繁简 编码 转换 自动 
注音 文本 聚 类 等 大 数据 的 本质 实际上 
是 数据 生产 的 社会化 其 对 统计 尤其 是 
政府 统计 的 冲击 是 重大 的 不仅 涉及 到 
整个 统计 流程 更加 对 当前 的 政府 统计 管理 
体制 机构 设置 数据 价值 等 方面 形成 了 挑战 
可以 大胆 预测 未来 政府 统计 的 政府 角色 会 
被 统计 专业性 取代 经济 分析 的 职能 会被 更为 
专业 的 经济 分析 部门 取代 宏观 数据 的 重要性 
会 让位于 更有 信息 价值 的 微观 数据 概念 Natural 
Language Processing 简称 NLP 就是/d 利用/n 电子/n 计算机/n 为/p 工具/n 
对/p 人类/n 特有/b 的/uj 书面/n 形式/n 和/c 口头/n 形式/n 的/uj 
自然/d 语言/n 的/uj 信息/n 进行/v 各种/r 类型/n 处理/v 和/c 加工/vn 
的/uj 技术/n 这种 技术 现在 已经 形成 一 门 专门 
的 边缘性 交叉性 学科 它 涉及 语言学 数学 和 计算机 
科学 横跨 文科 理科 和 工科 三大 知识领域 自然语言 处理 
的 目的 在于 建立 各种 自然语言 处理 系统 如 机器翻译 
系统 自然语言 理解 系统 信息 自动 检索系统 信息 自动 抽取 
系统 文本 信息 挖掘 系统 术语 数据库系统 计算机 辅助 教学 
系统 语音 自动 识别 系统 语音 自动 合成 系统 文字 
自动 识别 系统 等 自然 语言 是 人类 区别于 其它 
动物 的 重要 标志 之一 人 借助于 自然 语言 交流 
思想 达到 互相 了解 组成 人类 社会 生活 人 还 
借助 于 自然 语言 进行 思维 活动 认识 事物 的 
本质 和 规律 创造 了 人类 的 物质文明 和 精神文明 
自然语言 起码 在 下面 四 个 方面 与 人工 语言 
大相径庭 1 自然 语言 中 充满 着 歧义 而 人工 
语言 中 的 歧义 则 是 可以 控制 的 2 
自然 语言 的 结构 复杂 多样 而 人工 语言 的 
结构 则 相对 简单 3 自然 语言 的 语义 表达 
千变万化 迄今 还 没有 一种 简单 而 通用 的 途径 
来 描述 它 而 人工 语言 的 语义 则 可以 
由人 来 直接 定义 4 自然 语言 的 结构 和 
语义 之间 有着 千丝万缕 的 错综复杂 的 联系 一般 不 
存在 一一对应 的 同构 关系 而 人工 语言 则 常常 
可以 把 结构 和 语义 分别 进行 处理 人工 语言 
的 结构 和 语义 之间 有着 整齐 的 一一对应 的 
同构 关系 自然语言 处理 的 发展 基于 句法 语义 规则 
的 理性主义 方法 受到 质疑 随着 语料库 建设 和 语料库 
语言学 的 崛起 大 规模 真实 文本 的 处理 成为 
自然 语言 处理 的 主要 战略 目标 概率 和 数据 
驱动 的 方法 几乎 成为 了 自然 语言 处理 的 
标准 方法 自然语言 处理 越来越 多 的 使用 机器 自动 
学习 的 方法 来 获取 语言 知识 统计 数学 越来越 
受到 重视 自然 语言 处理 中 越来越 重视 词汇 的 
作用 出现 了 强烈 的 词汇 主义 的 倾向 多语言 
在线 自然语言 处理 技术 迅猛发展 随着 网络 技术 的 发展 
互联网 Web 逐渐 变成 一个 多 语言 的 网络 世界 
互联 网上 的 机器 翻译 信息检索 和 信息 抽取 等 
自然 语言 处理 的 需要 变得 更加 紧迫 自然语言 处理 
的 常见 运用 信息 提取 如 下面 的 这段 话 
Hi Dan we ve now scheduled the c u r 
r i c u l u m m e e 
t i n g . It will be in Gates 
159 tomorrow from10 00 11 30 . Chris 我们 通过 
自然 语言 处理 能够 得出 如下 信息 Event Curriculum mtg 
Date Dec 18 2017 Start 10 00am End 11 30am 
where Gates 159 语义分析 比如 淘宝 某个 商品 的 评价 
我们 能够 提取 信息 并且 根据 语义 来 进行 测评 
. 比如 对于 一个 照相机 提取 出 如下 的 主要 
特征 zoom affordability size and weight flash ease of use 
我们 根据 语义分析 搜集 到 关于 大小 和 重量 的 
如下 三 个 评价 1 . 拿 起来 很好 很 
舒适 2 . 好轻 的 照相机 我 再也 不用 拿着 
又大 又 笨 的 机器 到处 跑了 . 3 . 
这个 照相机 太 娇嫩 了 拿在手上 必须 非常 小心 . 
再 进一步 的 通过 NLP 我们 可以 得出 前 两个 
是 好 的 评价 最后 一个 是 不好 的 . 
通过 这些 我们 就 可以 对 这款 相机 做 一些 
基于 NLP 的 测评 指标 自动 翻译 . 谷歌 翻译 
百度 翻译 网易 云 翻译 等 等 就是 实例 . 
工欲善其事 必先利其器 自己 最近 爱上 了 自然 语言 处理 机器翻译 
人工智能 看了 老师 推荐 的 计算机 自然语言 处理 真的 是 
云里雾里 不能 说 都 看不懂 但是 没有 get 到有 层次 
的 东西 所以 自己 在 网站 各个 博客 社区 知乎 
github 上看 了 一些 关于 自然 语言 处理 入门 的 
讲解 或者 简述 了解 了 一些 之后 我 其实 觉得 
自然语言 处理 就是 机器学习 自己 根据 了解 的 情况 写了 
这篇 杂记 也 安排 了 自己 的 一个 学习 计划 
计算机 自然语言 处理 上面 分词 规范 特别 学习 跟着 看 
一些 python 在 自然 语言 处理 上 的 应用 源码 
多看 源码 在 github 上 多在 练习 中 更加 深刻 
理解 自然语言 处理 的 思想 编程语言 我 用 的 是 
python 现阶段 先 学习 这些 写 吧 眼看 就要 考 
试了 期间 看看 数学 之美 应该会 很不错 感觉 还是 云里雾里 
~ ~ ~ ~ 视频 列表 31 n gram 语言 
模型 一 32 n gram 语言 模型 二 33 n 
gram 语言 模型 三 34 n gram 语言 模型 四 
35 n gram 语言 模型 五 36 n gram 语言 
模型 六 37 n gram 语言 模型 七 31 n 
gram 语言 模型 一 第五章 n gram 语言 模型 噪声 
信道 模型 I = argmaxIP I ∣ O = argmaxIP 
O ∣ I P I P O = argmaxIP O 
∣ I P I I = \ underset { I 
} { argmax } P I | O = \ 
underset { I } { argmax } \ frac { 
P O | I P I } { P O 
} = \ underset { I } { argmax } 
P O | I P I I = Iargmax P 
I ∣ O = Iargmax P O P O ∣ 
I P I = Iargmax P O ∣ I P 
I 目标 通过 有 噪声 的 输出 信号 试图 恢复 
输入 信号 噪声 信道 模型 的 应用 噪声 信道 模型 
是 一种 普适性 的 模型 通过 修改 噪声 信道 的 
定义 可以 将 如下 应用 纳入 到 这一 模型 的 
框架 之中 语音识别 一个 声学 信号 对应 于 一个 语句 
一个 语音 识别 器 需 找到 其 对应 的 可能性 
最大 的 语言 文本 T = argmaxTP T ∣ A 
T = \ underset { T } { argmax } 
P T | A T = Targmax P T ∣ 
A 根据 贝叶斯 公式 T = argmaxTP A ∣ T 
P T P A = argmaxIP A ∣ T P 
T T = \ underset { T } { argmax 
} \ frac { P A | T P T 
} { P A } = \ underset { I 
} { argmax } P A | T P T 
T = Targmax P A P A ∣ T P 
T = Iargmax P A ∣ T P T 信息源 
对应 于以/nr 概率 P T P T P T 生成 
语句 文本 噪声 信道 对应 于以/nr 概率分布 P A ∣ 
T P A | T P A ∣ T 将 
语句 文本 转换成 声音 信号 语音 识别 的 目的 就是 
由 通过 噪声 信道 而 输出 的 声音 信号 恢复 
其 原始 的 语句 文本 其他 应用 信源 以 概率 
P T P T P T 生成 语句 文本 信道 
为 P O ∣ T P O | T P 
O ∣ T 语音 / 图像 / 翻译 文本 / 
字音 转换 模型 手写体 汉字 识别 文本 － 书写 或者 
打印 扫描 － 图像 文 本校 错 文本 － 输入 
编辑 － 带有 错误 的 文本 机器翻译 目标语言 的 文本 
－ 翻译 － 源语言 文本 音 字 转换 文本 － 
字音 转换 － 汉字 拼音 编码 词性 标注 词性 标注 
序列 － 词性 词串 转换 － 词串 香农 游戏 1 
给定 前 n 1个 词 或者 字母 预测 下 一个 
词 字母 32 n gram 语言 模型 二 语言 模型 
P T P T P T 语言 模型 如何 计算 
P T 根据 链 规则 P T = P S 
= P w1w2 . . . w3 = P w1 
P w2 ∣ w1 P w3 ∣ w1w2 . . 
. P wn ∣ w1w2 . . . wn − 
1 P T = P S = P w _ 
{ 1 } w _ { 2 } . . 
. w _ { 3 } = P w _ 
{ 1 } P w _ { 2 } | 
w _ { 1 } P w _ { 3 
} | w _ { 1 } w _ { 
2 } . . . P w _ { n 
} | w _ { 1 } w _ { 
2 } . . . w _ { n 1 
} P T = P S = P w1 w2 
. . . w3 = P w1 P w2 ∣ 
w1 P w3 ∣ w1 w2 . . . P 
wn ∣ w1 w2 . . . wn − 1 
问题 1 参数 空间 过大 无法 实用 2 数据 稀疏 
问题 马尔科夫 假设 P T = P S = P 
w1w2 . . . w3 = P w1 P w2 
∣ w1 P w3 ∣ w1w2 . . . P 
wn ∣ w1w2 . . . wn − 1 P 
T = P S = P w _ { 1 
} w _ { 2 } . . . w 
_ { 3 } = P w _ { 1 
} P w _ { 2 } | w _ 
{ 1 } P w _ { 3 } | 
w _ { 1 } w _ { 2 } 
. . . P w _ { n } | 
w _ { 1 } w _ { 2 } 
. . . w _ { n 1 } P 
T = P S = P w1 w2 . . 
. w3 = P w1 P w2 ∣ w1 P 
w3 ∣ w1 w2 . . . P wn ∣ 
w1 w2 . . . wn − 1 biigram 假设 
下 一个 词 的 出现 依赖于 它 前面 的 一个 
词 ≈ P w1 P w2 ∣ w1 P w3 
∣ w2 . . . P wn ∣ wn − 
1 \ approx P w _ { 1 } P 
w _ { 2 } | w _ { 1 
} P w _ { 3 } | w _ 
{ 2 } . . . P w _ { 
n } | w _ { n 1 } ≈ 
P w1 P w2 ∣ w1 P w3 ∣ w2 
. . . P wn ∣ wn − 1 trigram 
假设 下 一下 一个词 的 出现 依赖于 它 前面 的 
两个 词 ≈ P w1 P w2 ∣ w1 P 
w3 ∣ w1w2 . . . P wn ∣ wn 
− 2wn − 1 \ approx P w _ { 
1 } P w _ { 2 } | w 
_ { 1 } P w _ { 3 } 
| w _ { 1 } w _ { 2 
} . . . P w _ { n } 
| w _ { n 2 } w _ { 
n 1 } ≈ P w1 P w2 ∣ w1 
P w3 ∣ w1 w2 . . . P wn 
∣ wn − 2 wn − 1 N gram 语言 
模型 最大 相似 度 估计 Maximum Likelihood Estimate P wn 
∣ w1w2 . . . wn − 1 = C 
w1w2 . . . wn C w1w2 . . . 
wn − 1 P w _ { n } | 
w _ { 1 } w _ { 2 } 
. . . w _ { n 1 } = 
\ frac { C w _ { 1 } w 
_ { 2 } . . . w _ { 
n } } { C w _ { 1 } 
w _ { 2 } . . . w _ 
{ n 1 } } P wn ∣ w1 w2 
. . . wn − 1 = C w1 w2 
. . . wn − 1 C w1 w2 . 
. . wn n gram = n 个 词 构成 
的 序列 u n i g r a m b 
i g r a m t r i g r 
a m f o u r gram quadgram 4 gram 
N 元 文法 对 下一个 单词 的 条件 概率 逼近 
的 通用 等式 是 P wn ∣ w1n − 1 
≈ P wn ∣ wn − N + 1n − 
1 P w _ { n } | w _ 
{ 1 } ^ { n 1 } \ approx 
P w _ { n } | w _ { 
n N + 1 } ^ { n 1 } 
P wn ∣ w1n − 1 ≈ P wn ∣ 
wn − N + 1n − 1 构造 训练 N 
gram 语言 模型 在 训练 语料库 中 统计 获得 n 
gram 的 频度 信息 举例 假设 语料库 总 词数 为 
13 748 词 I 3 4 3 7 w a 
n t 1 2 1 5 t o 3 2 
5 6 e a t 9 3 8 C h 
i n e s e 2 1 3 f o 
o d 1 5 0 6 l u n c 
h 4 5 9 \ I w a n t 
t o e a t C h i n e 
s e f o o d l u n c 
h I 8 1 0 8 7 0 1 3 
0 0 0 w a n t 3 0 7 
8 6 0 6 8 6 t o 3 0 
1 0 8 6 0 3 0 1 2 e 
a t 0 0 2 0 1 9 2 5 
2 C h i n e s e 2 0 
0 0 0 1 2 0 1 f o o 
d 1 9 0 1 7 0 0 0 0 
l u n c h 4 0 0 0 0 
1 0 P I want to eat Chinese food = 
P I P want | I P to | want 
P eat | to P Chinese | eat P food 
| Chinese = 0 . 251087/3437786 / 1215860/325619 / 938120/213 
= 0.000154171 N 的 选择 可靠性 vs . 辨别力 我 
正在 _ _ _ _ _ _ _ _ 讲课 
图书馆 听课 学习 借书 我 正在 图书馆 _ _ _ 
_ _ _ _ _ 学习 借书 更大 的 n 
对 下 一个 词 出现 的 约束性 信息 更多 更大 
的 辨别力 更小 的 n 在 训练 语料库 中 出现 
的 次数 更多 更 可靠 的 统计 结果 更高 的 
可靠性 N 的 选择 方法 词表 中词 的 个数 | 
V | = 20 000 词 n 所有 可能 的 
n gram 的 个数 2 bigrams 400 000 0003 trigrams 
8 000 000 000 0004 4 grams 1.6 x 10171.6 
x 10 ^ { 17 } 1 . 6x1017 数据 
稀疏 问题 假设 我们 使用 trigram 模型 P S = 
P w1 P w2 ∣ w1 P w3 ∣ w1w2 
. . . P wn ∣ wn − 2wn − 
1 P S = P w _ { 1 } 
P w _ { 2 } | w _ { 
1 } P w _ { 3 } | w 
_ { 1 } w _ { 2 } . 
. . P w _ { n } | w 
_ { n 2 } w _ { n 1 
} P S = P w1 P w2 ∣ w1 
P w3 ∣ w1 w2 . . . P wn 
∣ wn − 2 wn − 1 如果 某个 P 
wi ∣ wi − 2wi − 1 = C wi 
− 2wi − 1wi C wi − 2wi − 1 
= 0P w _ { i } | w _ 
{ i 2 } w _ { i 1 } 
= \ frac { C w _ { i 2 
} w _ { i 1 } w _ { 
i } } { C w _ { i 2 
} w _ { i 1 } } = 0P 
wi ∣ wi − 2 wi − 1 = C 
wi − 2 wi − 1 C wi − 2 
wi − 1 wi = 0 那么 P S = 
0P S = 0P S = 0 数据 稀疏 问题 
必须 保证 C ≠ 0C \ neq 0C ̸ = 
0 从而 使 P ≠ 0P \ neq 0P ̸ 
= 0 假设 某 语料库 词汇 分布 如下 最大 相似 
度 估计 期望 概率分布 33 n gram 语言 模型 三 
数据 平滑 技术 降低 已 出现 的 n gram 条件 
概率分布 以使 未 出现 n gram 条件 概率分布 非 0 
又可 称为 折扣 方法 Discounting methods 确认 Validation – 特指 
使用 两个 不同 的 训练 语料库 的 平滑 方法 拉普拉斯 
定律 加 一 平滑 法 Jeffreys Perks LawGood Turing 估计 
Good Turing 估计 示例 建立 频度 bigram 个 数表 词 
表中 词数 14585 语料库 中 出现 的 各不相同 的 bigram 
总数 199252个 bigram 总数 为 617091个 对于 未 出现 的 
bigram 假设 语料库 中 某 bigram 出现 了 1次 P 
= 0 . 3663/617091 = 5.94 E 7 简单 Good 
Turing 对于 比 较大 的 r Nr = arb b 
1 用 线性 回归 的 方法 估算 a 和 b 
log Nr = log a + b log r 对于 
比 较小 的 r 直接 使用 Nr 关于 Good Turing 
平滑 的 两个 问题 21 Good Turing 估计 的 理论 
依据 是 什么 2 Good Turing 估计 是 完备 的 
吗 其他 常用 平滑 方法 Back off 平滑 线性插值 平滑 
Witten Bell 平滑 平滑 的 效果 数据 平滑 的 效果 
与 训练 语料库 的 规模 有关 数据 平滑 技术 是 
构造 高 鲁棒性 语言 模型 的 重要 手段 训练 语料库 
规模 越小 数据 平滑 的 效果 越 显著 训练 语料库 
规模 越大 数据 平滑 的 效果 越 不显著 甚至 可以 
忽略不计 34 n gram 语言 模型 四 一元 模型 N 
gram 模型 与 N pos 模型 之间 的 关系 考察 
N pos 模型 的 极端 情况 即 当 整个 模型 
只有 一个 词类 与 每 一个 词 都 有一个 各自 
不同 的 词类 的 情况 如果 N pos 模型 只有 
一个 词类 那么 前 N 1个 词类 没有 提供 任何 
上下文 信息 于是 N pos 模型 退 化为 Unigram 模型 
如果 每 一个 词 都 有一个 各不相同 的 词类 那么 
这样 的 N pos 模型 等价 于N/nr gram 模型 统计 
语言 模型 的 评价 实用 方法 通过 查看 该 模型 
在 实际 应用 中 的 表现 来 评价 统计 语言 
模型 优点 直观 实用 缺点 缺乏 针对性 不够 客观 理论 
方法 交叉 熵 与 迷惑 度 Kullback Leibler KL 距离 
Kullback Leibler KL 距离 相关 熵 两个 概率密度函数 p x 
与 q x 它们 的 相关 熵 由 下式 给出 
描述 了 两个 概率分布 之间 的 差异 D p | 
| q = = 0 iff p = q 非 
量度 不满足 交换 率 和 三角 不等式 语言 与其 模型 
的 交叉 熵 我们 构造 的 语言 模型 为 q 
x 如何 评价 它 的 好坏 Idea 如果 q x 
与 正确 的 语言 模型 p x 的 相关 熵 
越小 模型 越好 问题 是 我们 并不 知道 p x 
可以 借助 交叉 熵 某 语言 L 其 真实 的 
概率 分布 为 p x 我们 构造 的 该 语言 
的 概率模型 为 q x 那么 该 语言 与其 模型 
的 交叉 熵 为 如果 我们 将 语言 视为 平稳 
各 态 可 遍历 的 随机 过程 那么 迷惑 度 
举例 150万 词 WSJ 语料库 得到 的 不同 n gram 
语言 模型 的 迷惑 度 Unigram 962Bigram 170Trigram 10935 n 
gram 语言 模型 五 音 字 转换 系统 附录 1 
语言 模型 构造 实例 N gram 语言 模型 构造 举例 
36 n gram 语言 模型 六 附录 2 最大熵 模型 
基础 最大熵 模型 一种 基于 最大熵 原理 的 统计 预测模型 
最大熵 原理 在 一定 的 限制 条件 下 尽可能 地 
选择 熵 最大 的 概率分布 均匀分布 作为 预测 结果 对 
不 知道 限制 条件 以外 的 情形 不做 任何 假设 
举例 1 抛 一枚 硬币 p H = p1 p 
T = p2 . 限制 条件 p1 + p2 = 
1 问题 如何 估计 概率分布 p = p1 p2 基于 
最大熵 原理 的 答案 选择 使 H § 最大 的 
那个 p 举例 2 最大熵 模型 目的 估计 在 限定 
条件 下 的 概率 p 选择 满足 限定 条件 的 
p 使 H § 为 最大 H x = − 
∑ x ∈ Xp x log ⁡ p x H 
x = { \ sum _ { } ^ { 
x \ in X } } p x \ log 
p x H x = − ∑ x ∈ X 
p x logp x x = a b a ∈ 
A ∧ b ∈ Bx = a b a \ 
in A \ wedge b \ in Bx = a 
b a ∈ A ∧ b ∈ BAAA 为 上下文 
特征 集合 为 待 预测 标记 的 集合 37 n 
gram 语言 模型 七 如何 获得 这样 的 模型 从 
训练 数据 中 统计 a b 对 a 上下文 b 
预测 标记 观察 值 a b 称为 一个 事件 举例 
词性 标注 a = 在 某个 文本 窗口 中的 词 
b = NN 学习 得 到 每个 a b 的 
概率值 p a b 问题 如何 表示 限制 条件 特征 
在 最大熵 模型 中 特征 是 一个 关于 事件 二 
值 函数 fj X → { 0 1 } X 
= A × Bf _ { j } X \ 
rightarrow \ left \ { 0 1 \ right \ 
} X = A \ times Bfj X → { 
0 1 } X = A × B 举例 特征 
事件 举例 title caps NNP Citibank Mr . sufix ing 
VBG running cooking POS tag DT I NP the bank 
a thief current word from I PP from the bank 
next word Inc . I ORG Lotus Inc . previous 
word said I PER said Mr . Vinken 复杂 特 
征文 档级 特征 document category = 篮球 & current word 
= 火箭 I ORG 可能 将 火箭 标 为 I 
ORG 而非 普通名词 原子级 词 特征 current word = 李宁 
& next word = 公司 I ORG 可能 将 李宁 
标 为 I ORG 而非 I PER 限制 条件 最大熵 
模型 的 使用 根据 局部 概率值 直接 标注 计算 全局 
最优 解 Viterbi searchBeam search 最大熵 模型 总结 原理 找到 
满足 所有 限制 的 熵 最大 的 概率分布 训练 通过 
GIS 或者 IIS 收敛/v 速度/n 可能/v 较慢/a 对/p 交叉性/n 的/uj 
特征/n 也/d 能/v 很好/i 的/uj 处理/v 对/p 自然/d 语言/n 处理/v 
的/uj 许多/m 问题/n 都能/nr 提供/v 很好/i 的/uj 解决方案/n 致谢/v 关毅/nr 
老师/n 现为 哈工大 计算机 学院 语言 技术 中心 教授 博士生 
导师 通过 认真 学习 了 自然语言 处理 哈工大 关毅 64集 
视频 3 来自 互联网 的 课程 受益 良多 在此 感谢 
关毅 老师 的 辛勤 工作 为 进一步 深入 理解 课程内容 
对 部分 内容 进行 了 延伸 学习 4 5 612 
在此 分享 期待 对 大家 有所 帮助 欢迎 加 我 
微信 验证 NLP 一起 学习 讨论 不足之处 欢迎 指正 参考文献 
Claude E . Shannon . Prediction and Entropy of Printed 
English Bell System Technical Journal 30 50 64 . 195 
↩ ︎ ↩ ︎ An Empirical Study of Smoothing Techniques 
for Language Modeling Stanley F . Chen ↩ ︎ ↩ 
︎ 自然语言 处理 哈工大 关毅 64集 视频 来自 互联网 ↩ 
︎ 王晓龙 关毅 计算机 自然语言 处理 清华大学出版社 2005年 ↩ ︎ 
哈工大 语言 技术 平台 云 官网 http / / ltp 
. ai / ↩ ︎ Steven Bird Natural Language Processing 
with Python 2015 ↩ ︎ 自然语言 处理 NLP 历史悠久 从上 
个 世纪 初 便 有人 开始 提出 自然语言 相关 的 
规律 和 假设 但 本人 阅读 了 若干 自然语言 相关 
的 书籍 后 发现 自然语言 处理 的 方法论 在 长达 
近 1个 世纪 的 时间 内 并无 半点 实质上 的 
进展 自然语言 处理 的 方法 体系 目前 大致 可 分为 
两个 方向 1 . 形式化 语言 处理 方向 这个 方向 
吸引 了 众多 学者 体系 非常 庞杂 其中 诞生 了 
很多 处理 主义 但/c 都/d 并未/d 有/v 革命性/n 的/uj 变化/vn 
基本上 属于 盲人摸象 其中 就 包括 了 如下 理论 范畴 
语法 语言 串 分析 语言 集合论 有限 状态 语法 短语 
结构 语法 线图 分析 汉字结构 左 结合 合一 运算 依存 
配价 格 语法 词汇 模型 不 一一 列举 了 防止 
被 绕 晕 其实 都是/nr 文字 概念上 的 变化 很多/m 
概念/n 都有/nr 重复/d 的/uj 嫌疑/v 总结 一下 就是 基于 语言 
规则 的 形式化 模型 各有 各 的 细微 变化 但/c 
都/d 没有/v 从/p 根本/a 上/f 解决问题/n 用 人力 可以 实现 
有限 状态机 的 有 限度 的 智能 2 . 数字化 
语言 处理 方向 这个 方向 似乎 才 是 沿着 科学 
的 道路 在 前进 但 发展 似乎 也 很慢 主要 
包括 如下 理论 概率 语法 Bayes 动态规划 HMM CRF LSTM 
CNN 这些 方法 将 语言 看作 数字信号 使用 概率论 的 
方法 对 其 处理 但 也 没有 真正 实现 语义 
理解 针对 以上 两个 方向 我 个人 认为 数字化 语言 
处理 才 是 正确 的 但对 形式化 语言 处理 的 
认识 越深 才能 更好 的 设计 自然语言 处理 模型 现阶段 
想要 做出 自动 学习 语言 并 生成 语言 认知 模型 
还 比较 困难 但 形式化 方向 上 很多 先驱 提出 
的 算法 语言 规律 和 语言 现象 有助于 网络结构 和 
参数 的 设计 Witmart . com 帮 你 担保 做 
网络 兼职 也能 轻轻松松 赚 美元 提现 无忧 可靠 安全 
深度 学习 10 自然语言 处理 2018 03 01 自然语言 处理 
Natural Language Processing NLP 是 人工 智能 和 语言学 领域 
的 学科 分支 它 研究 实现 人 与 计算机 之间 
使用 自然 语言 进行 有效 通信 的 各种 理论 和 
方法 词 嵌入 前面 介绍 过 处理 文本 序列 时 
通常用 建立 字典 后以 one hot 的 形式 表示 某个 
词 进而 表示 某个 句子 的 方法 这种 表示 方法 
孤立 了 每个 词 无法 表现 各个 词 之间 的 
相关性 满足 不了 NLP 的 要求 词 嵌入 Word Embedding 
是 NLP 中 语言 模型 与 表征 学习 技术 的 
统称 概念 上 而言 它 是 指 把 一个 维数 
为 所有 词 的 数量 的 高维空间 one hot 形式 
表示 的 词 嵌入 到 一个 维数 低 得多 的 
连续 向量空间 中 每个 单词 或 词组 被 映射 为 
实 数域 上 的 向量 Word Embedding 如上 图中 各 
列 分别 组成 的 向量 是 词 嵌入 后 获得 
的 第一 行 中 几个 词 的 词 向量 的 
一部分 这些 向量 中的 值 可 代表 该词 与 第一 
列 中 几个 词 的 相关 程度 使用 2008年 van 
der Maaten 和 Hinton 在 论文 Visualizing Data using t 
SNE 中 提出 的 t SNE 数据 可视化 算法 将 
词 嵌入 后 获得 的 一些 词 向量 进行 非线性 
降 维 可到 下面 的 映射 结果 t SNE 映射 
其中 可 发现 各 词 根据 它们 的 语义 及 
相关 程度 分别 汇聚 在 了 一起 对 大量 词汇 
进行 词 嵌入 后 获得 的 词 向量 可 用来 
完成 命名 实体 识别 Named Entity Recognition 等 任务 其中 
可 充分 结合 迁移 学习 以 降低 学习 成本 提高效率 
好比 前面 讲过 的 用 Siamese 网络 进行 人脸识别 过程 
使 用词 嵌入 方法 获得 的 词 向量 可实现 词汇 
的 类比 及 相似 度 度量 例如 给定 对应 关系 
男性 Man 对 女性 Woman 要求 机 器类 比出 国王 
King 对应 的 词汇 通过 上面 的 表格 可 发现 
词 向量 存在 数学 关系 Man Woman ≈ ≈ King 
Queen 也 可以 从 可视化 结果 中 看出 男性 Man 
到 女性 女性 的 向量 与 国王 King 到 王后 
Queen 的 向量 相似 词 嵌入 具有 的 这种 特性 
在 2013年 Mikolov 等 发表 的 论文 Linguistic Regularities in 
Continuous Space Word Representations 中 提出 成为 词 嵌入 领域 
具有 显著 影响力 的 研究 成果 上述 思想 可写 成 
一个 余弦 cos 相似 度 函数 sim u v = 
uTv ∣ ∣ u ∣ ∣ 2 ∣ ∣ v 
∣ ∣ 2sim u v = uTv ∣ ∣ u 
∣ ∣ 2 ∣ ∣ v ∣ ∣ 2 以此 
度 量词 向量 的 相似 度 词 嵌入 方法 词 
嵌入 的 方法 包括 人工神经网络 对 词语 同现 矩阵 降 
维 概率模型 以及 单词 所在 上下文 的 显 式 表示 
等 以 词汇 的 one hot 形式 作为 输入 不同 
的 词 嵌入 方法 能以/nr 不同 的 方式 学习 到 
一个 嵌入 矩阵 Embedding Matrix 最后 输出 某个 词 的 
词 向量 将 字典 中 位置 为 ii 的 词 
以 one hot 形式 表示 为 oioi 嵌入 矩阵 用 
EE 表示 词 嵌入 后 生成 的 词 向量 用 
eiei 表示 则 三者 存在 数学 关系 E ⋅ oi 
= eiE ⋅ oi = ei 例如 字典 中 包含 
10000个 词 每个 词 的 one hot 形式 就是 个 
大小 为 10000 × 110000 × 1 的 列 向量 
采用 某 种方法 学习 到 的 嵌入 矩阵 大小 为 
300 × 10000300 × 10000 的话 将 生成 大小 为 
300 × 1300 × 1 的 词 向量 神经 概率 
语言 模型 采用 神经网络 建立 语言 模型 是 学习 词 
嵌入 的 有效 方法 之一 2003年 Bengio 等人 的 经典之作 
A Neural Probabilistic Language Model 中 提出 的 神经 概率 
语言 模型 是 早期 最 成功 的 词 嵌入 方法 
之一 模型 中 构建 了 了 一个 能够 通过 上下文 
来 预测 未知 词 的 神经 网络 在 训练 这个 
语言 模型 的 同时 学习 词 嵌入 例如 将 下 
图中 上面 的 句子 作为 下面 的 神经 网络 的 
输入 语言 模型 经过 隐藏 层 后 最后 经 Softmax 
将 输出 预测 结果 其中 的 嵌入 矩阵 EE 与 
ww bb 一样 是 该 网络 中 的 参数 需 
通过 训练 得到 训练 过程 中 取 语料库 中的 某些 
词 作为 目标 词 以 目标 词 的 部分 上下文 
作为 输入 训练 网络 输出 的 预测 结果 为 目标 
词 得到 了 嵌入 矩阵 就能 通过 前面 所述 的 
数学 关系式 求得 词 嵌入 后的词/nr 向量 WORD2VECWord2Vec Word To 
Vectors 是 现在 最 常用 最 流行 的 词 嵌入 
算法 它 由 2013年 由 Mikolov 等人 在 论文 Efficient 
Estimation of Word Representations in Vector Space 中 提出 Word2Vec 
中的 Skip Gram 模型 所做 的 是 在 语料库 中 
选定 某个 词 Context 随后 在 该词 的 正负 10个 
词 距 内 取 一些 目标 词 Target 与之 配对 
构造 一个 用 Context 预测 输出 为 Target 的 监督 
学习 问题 训练 一个 如 下图 结构 的 网络 Skip 
Gram 网络 该 网络 仅有 一个 Softmax 单元 输出 Context 
下 Target 出现 的 条件概率 p t ∣ c = 
exp θ Ttec ∑ mj = 1exp θ Tjec p 
t ∣ c = exp θ tTec ∑ j = 
1mexp θ jTec 上式 中 θ t θ t 是 
一个 与 输出 的 Target 有关 的 参数 其中 省略 
了 用以 纠正 偏差 的 参数 训练 过程 中 还是 
用 交叉 熵 损失 函数 选定 的 Context 是 常见 
或不 常见 的 词 将 影响 到 训练 结果 在 
实际 中 Context 并 不是 单纯 地 通过 在 语料库 
均匀 随机 采样 得到 而是 采用 了 一些 策略 来 
平衡选择 Word2Vec 中 还有 一种 CBOW Continuous Bag of Words 
Model 模型 它 的 工作 方式 是 采样 上下文 中的 
词 来 预测 中间 的 词 与 Skip Gram 相反 
以上 方法 的 Softmax 单元 中 产生 的 计算 量 
往往 过大 改进 方法 之一 是 使用 分级 Softmax 分类器 
Hierarchical Softmax Classifier 采用 霍夫曼 树 Huffman Tree 来 代替 
隐藏 层 到 输出 Softmax 层 的 映射 此外 Word2Vec 
的 作者 在 后续 论文 Distributed Representations of Words and 
Phrases and their Compositionality 中 提出 了 负 采样 Negative 
Sampling 模型 进一步 改进 和 简化 了 词 嵌入 方法 
负 采样 模型 中 构造 了 一个 预测 给定 的 
单词 是否 为 一对 Context Target 的 新 监督 学习 
问题 采用 的 网络 结构 和 前面 类似 负 采样 
训练 过程 中 从 语料库 中 选定 Context 输入 的 
词 为 一对 Context Target 则 标签 设置 为 1 
另外 任取 kk 对 非 Context Target 作为 负 样本 
标签 设置 为 0 只有 较少 的 训练 数据 kk 
的 值 取 5 ~ 20 的话 能达到 比较 好 
的 效果 拥有 大量 训练 数据 kk 的 取值 取 
2 ~ 5 较为 合适 原 网络 中的 Softmax 变成 
多个 Sigmoid 单元 输出 Context Target c t 对 为 
正 样本 y = 1y = 1 的 概率 p 
y = 1 ∣ c t = σ θ Ttec 
p y = 1 ∣ c t = σ θ 
tTec 其中 的 θ t θ t ecec 分别 代表 
Target 及 Context 的 词 向量 通过 这种 方法 将 
之前 的 一个 复杂 的 多分 类 问题 变成 了 
多个 简单 的 二分 类 问题 而 降低 计算成本 模型 
中 还 包含 了 对 负 样本 的 采样 算法 
从 本质 上 来说 选择 某个 单词 来 作为 负 
样本 的 概率 取决于 它 出现 频率 对于 更 经常 
出现 的 单词 将 更 倾向 于 选择 它 为 
负 样本 但 这样 会 导致 一些 极端 的 情况 
模型 中 采用 一下 公式 来 计算 选择 某个 词 
作为 负 样本 的 概率 p wi = f wi 
34 ∑ mj = 0f wj 34p wi = f 
wi 34 ∑ j = 0mf wj 34 其中 f 
wi f wi 代表 语料库 中 单词 wiwi 出现 的 
频率 GLOVEGloVe Global Vectors 是 另一种 现在 流行 的 词 
嵌入 算法 它 在 2014年 由 Pennington 等人 在 论文 
GloVe Global Vectors for Word Representation 中 提出 Glove 模型 
中 首先 基于 语料库 统计 了 词 的 共 现 
矩阵 XX XX 中的 元素 为 Xi jXi j 表示 
整个 语料库 中 单词 ii 和 单词 jj 彼此 接近 
的 频率 也 就是 它 们 共同 出现 在 一个 
窗口 中的 次数 之后 要做 的 就是 优化 以下 代价 
函数 J = ∑ i jNf Xi j θ Tiej 
+ bi + bj − log Xi j 2J = 
∑ i jNf Xi j θ iTej + bi + 
bj − log Xi j 2 其中 θ i θ 
i ejej 分 是 单词 ii 和 单词 jj 的 
词 向量 bibi bjbj 是 两个 偏差 项 f f 
是 一个 用 以 防止 Xi j = 0Xi j 
= 0时 log Xi j log Xi j 无解 的 
权重 函数 词汇表 的 大小 为 NN 以上 优化 函数 
的 推导 过程 见 参考资料 中的 理解 GloVe 模型 最后 
要 说明 的 是 使用 各种 词 嵌入 方法 学习 
到 的 词 向量 并不 像 最开始 介绍 词 嵌入 
时 展示 的 表格 中 Man Woman King Queen 的 
词 向量 那样 其中 的 值 能够 代表 着 与 
Gender Royal 等 词 的 的 相关 程度 实际上 它 
们 大都 超出 了 人们 的 能够 理解 范围 词 
嵌入 应用 情感 分类器 NLP 中的 情感 分类 是 对 
某段 文字 中 所 表达 的 情感 做出 分类 它 
能在 很多 个 方面 得到 应用 训练 情感 分类 模型 
时 面临 的 挑战 之一 可能 是 标 记好 的 
训练 数据 不 够多 然而 有了 词 嵌入 得到 的 
词 向量 只需要 中等 数量 的 标记 好 的 训练 
数据 就能 构建 出 一个 表现 出色 的 情感 分类器 
情感 分类 如上图 要 训练 一个 将 左边 的 餐厅 
评价 转换 为 右边 评价 所属 星级 的 情感 分类器 
也 就是 实现 xx 到 yy 的 映射 有了 用词 
嵌入 方法 获得 的 嵌入 矩阵 EE 一种 简单 的 
实现 方法 如下 简单方法 方法 中 计算出 句中 每个 单词 
的 词 向量 后 取 这些 词 向量 的 平均值 
输入 一个 Softmax 单元 输出 预测 结果 这种 简单 的 
方法 适用 于 任何 长度 的 评价 但 忽略 了 
词 的 顺序 对于 某些 包含 多个 正面 评价 词 
的 负面 评价 很容易 预测 到 错误 结果 采用 RNN 
能 实现 一个 表现 更加 出色 的 情感 分类器 此时 
构建 的 模型 如下 RNN 情感 分类 这 是 一个 
多对一 结构 的 循环 神经网络 每个 词 的 词 向量 
作为 网络 的 输入 由 Softmax 输出 结果 由于 词 
向量 是从 一个 大型 的 语料库 中 获得 的 这种 
方法 将 保证 了 词 的 顺序 的 同时 能够 
对 一些 词 作出 泛化 词 嵌入 除 偏在 词 
嵌入 过程 中 所 使用 的 语料库 中 往往 会 
存在 一些 性别 种族 年龄 性取向 等 方面 的 偏见 
从而 导致 获得 的 词 向量 中 也 包含 这些 
偏见 比如 使用 未 除 偏 的 词 嵌入 结果 
进行 词汇 类比 时 男性 Man 对 程序员 Computer Programmer 
将 得到 类似 女性 Woman 对 家务 料 理人 Homemaker 
的 性别 偏见 结果 2016年 Bolukbasi 等人 在 论文 Man 
is to Computer Programmer as Woman is to Homemaker Debiasing 
Word Embeddings 中 提出 了 一些 消除 词 嵌入 中的 
偏见 的 方法 这里 列举 消除 词 向量 存在 的 
性别 偏见 的 过程 来 说明 这些 方法 摘自 第二周 
课后 作业 1 . 中和 本身 与 性别 无关 词汇 
中和 Neutralize 医生 doctor 老师 teacher 接待员 receptionist 等 本身 
与 性别 无关 词汇 中的 偏见 首先 计算 g = 
ewoman − emang = ewoman − eman 用 女性 woman 
的 词 向量 减去 男性 man 的 词 向量 得到 
的 向量 gg 就 代表 了 性别 gender 假设 现有 
的 词 向量 维数 为 50 那么 对 某个 词 
向量 将 50 维空间 分成 两个 部分 与 性别 相关 
的 方向 gg 和与/nr gg 正交 的 其他 49个 维度 
g ⊥ g ⊥ 如 下左图 本身 与 性别 无关 
除 偏 的 步骤 是 将要 除 偏 的 词 
向量 左图 中的 e r e c e p t 
i o n i s t e r e c 
e p t i o n i s t 在 
向量 gg 方 向上 的 值 置 为 00 变成 
右图 所示 的 e d e b i a s 
e d r e c e p t i o 
n i s t e r e c e p 
t i o n i s t d e b 
i a s e d 所用 的 公式 如下 ebiascomponent 
= e ⋅ g | | g | | 22 
× gecomponentbias = e ⋅ g | | g | 
| 22 × g e d e b i a 
s e d r e c e p t i 
o n i s t = e − e b 
i a s c o m p o n e 
n t e r e c e p t i 
o n i s t d e b i a 
s e d = e − ecomponentbias2 . 均衡 本身 
与 性别 有关 词汇 对 男演员 actor 女演员 actress 爷爷 
grandfather 等 本身 与 性别 有关 词汇 如 下左图 假设 
女演员 actress 的 词 向 量比 男演员 actor 更 靠近 
于 婴儿 看护人 babysit 中和 婴儿 看护人 babysit 中 存在 
的 性别 偏见 后 还是 无法 保证 它 到 女演员 
actress 与 到 男演员 actor 的 距离 相等 对一对 这样 
的 词 除 偏 的 过程 是 均衡 Equalization 它们 
的 性别 属性 本身 与 性别 有关 均衡 过程 的 
核心 思想 是 确保 一 对词 actor 和 actress 到 
g ⊥ g ⊥ 的 距离 相等 的 同时 也 
确保 了 它们 到 除 偏后 的 某个 词 babysit 
的 距离 相等 如 上右图 对 需要 除 偏 的 
一对 词 w1w1 w2w2 选定 与 它们 相关 的 某个 
未 中和 偏见 的 单词 BB 之后 均衡 偏见 的 
过程 如下 公式 μ = ew1 + ew22 μ = 
ew1 + ew22 μ B = μ ⋅ bias _ 
axis | | bias _ axis | | 22 × 
bias _ axis μ B = μ ⋅ bias _ 
axis | | bias _ axis | | 22 × 
bias _ axis μ ⊥ = μ − μ B 
μ ⊥ = μ − μ Bew1B = ew1 ⋅ 
bias _ axis | | bias _ axis | | 
22 × bias _ axisew1B = ew1 ⋅ bias _ 
axis | | bias _ axis | | 22 × 
bias _ axisew2B = ew2 ⋅ bias _ axis | 
| bias _ axis | | 22 × bias _ 
axisew2B = ew2 ⋅ bias _ axis | | bias 
_ axis | | 22 × bias _ a x 
i s e c o r r e c t 
e d w 1 B = | 1 − | 
| μ ⊥ | | 22 | − − − 
− − − − − − √ × ew1B − 
μ B | | ew1 − μ ⊥ − μ 
B | | 2ew1Bcorrected = | 1 − | | 
μ ⊥ | | 22 | × ew1B − μ 
B | | ew1 − μ ⊥ − μ B 
| | 2ecorrectedw2B = | 1 − | | μ 
⊥ | | 22 | − − − − − 
− − − − √ × ew2B − μ B 
| | ew1 − μ ⊥ − μ B | 
| 2ew2Bcorrected = | 1 − | | μ ⊥ 
| | 22 | × ew2B − μ B | 
| ew1 − μ ⊥ − μ B | | 
2e1 = ecorrectedw1B + μ ⊥ e1 = ew1Bcorrected + 
μ ⊥ e2 = ecorrectedw2B + μ ⊥ e2 = 
ew2Bcorrected + μ ⊥ 参考资料 吴恩 达 序列 模型 网易 
云 课堂 Andrew Ng Sequence Model C o u r 
s e r a d e e p l e 
a r n i n g . aiDeep Learning in 
NLP 一 词/n 向量/n 和/c 语言/n 模型/n 从/p SNE/w 到/v 
t/w SNE 再到 LargeVisword2vec 前世 今生 Word2Vec 导学 第二 部分 
负 采样 csdn 理解 GloVe 模型 csdn 课程 代码 与 
资料 GitHub 注 本文 涉及 的 图片 及 资料 均 
整理 翻 译自 Andrew Ng 的 Deep Learning 系列 课程 
版权 归其/nr 所有 翻译 整理 水平 有限 如有 不妥 的 
地方 欢迎 指出 上周 小 编   惊喜 地   
收到 网络 安全 十余年 老兵 飞絮 老师 的 投稿 大佬 
终于 重出江湖 了 一 人 工 智 能 人工智能 是 
一门 以 数学 为基础 涉及 到 计算机 科学 生物学 心理学 
语言学 和 哲学 等 的 交叉 类 学科 维 基 
百 科 人工智能 就是 机器 展现出 的 智能 即 只要 
是 某种 机器 具有 某种 或 某些 智能 的 特征 
或 表现 都应该 算作 人工智能 人工智能 三大 核心 要素 算法 
算 力 数据 根据 中国 电子 技术 标准化 研究院 的 
人工智能 标准化 白皮书 2018版 下载 点击   阅读 原文 阐述 
人工智能 领域 关键 技术 包括 机器学习 知识图谱 自然语言 处理 计算机 
视觉 人机交互 生物 特征 识别 虚拟现实 / 增强 现实 等 
其实 人工智能 已 发展 了 半个 多 世纪 如今 随着 
计算能力 飞速 发展 及 硬件 成本 的 不断 降低 促 
使用 人工智能 发展 到了 第三波 浪潮 2017年 3月 5日 人工智能 
正式 写入 2017 政府 工作 报告 无人驾驶 个人 助理 金融 
电商 医疗 教育 等 各大 领域 大量 应用 了 人工智能 
预计 2030年 全球 将 达 7万 亿 美元 规模 的 
市场 人工智能 和 机器学习 有望 彻底 改变 很多 行业 但 
它们 也 带来 了 重大 安全 风险 比如 算法 黑箱 
或 算法 不透明性 将 引发 算法 安全 管理 困境 可能 
成为 隐形 恶意 武器 操控 决策 致使 算法 权力 诱导 
个人行为 影响 政府 决策 和 司法 判决 剑桥 分析 助力 
特朗普 总统 竞选 携程 差异化 定价 杀熟 等 一个 个 
案例 呈 现在 我们 面前 除了 算法 算 力 外 
另 一个 核心 因素 是 数据 实现 人工智能 有 两个 
阶段 即 准备 数据 与 训练 模型 数据 准备 工作量 
占 比达   70% 以上 但 更 重要 的 数据 
背后 的 人工 即 数据 预处理 模型 选择 与 参数 
调整 二 数据安全 治理 目前 数据 已 成为 资产 能源 
和 基础 设施 的 关键 要素 数据安全 市场 呈 井喷 
之势 据 中商 产业 研究院 分析 2016 2020年 中国 数据安全 
市场 规模 年 增速 30% 以上 预计 2020年 市场 规模 
将 超 70 亿元 理解 数据安全 保护 的 内涵 一般 
可以 分为 3个 阶梯式 层次 数据安全 个人 数据 保护 国家 
层面 的 数据 保护 数据安全 可以 理解 为 信息 或 
信息 系统 免受 未经 授权 的 访问 使用 披露 破坏 
修改 销毁 等 数据安全 = 保密性 + 完整性 + 可用性 
个人 数据 保护 = 数据安全 + 个人 数据 自决 权利 
+ 数据 控制者 等 相关 方 满足 个人 数据 自决 
权利 的 义务 国家 层面 的 数据 保护   = 
数据安全 + 数据 支配权 + 防止 敏感数据 遭 恶意 使用 
对 国家 安全 的 威胁 目前 数据 安全 面临 的 
挑战 如下 新的 数据 和 隐私 保护 的 合规 要求 
网络攻击 造成 的 数据 泄露 破坏 了 组织 声誉 和 
客户 信任 混合 IT 架构 下 缺乏 数据 安全策略 数据 
安全 和 身份 管理 产品 不会 整合 甚至不 共享 通用 
策略 著名 的 咨询 与 研究 机构 Gartner 在 2018年 
5月 发布 了 数据安全 治理 Data Security Governance 框架 提供 
了 一个 如何 通过 数据 保护 和 隐私 声明 的 
平衡 方法 来 实现 实际 的 安全 性 数据安全 治理 
不 仅仅 是 工具 或 产品 的 解决方案 而是 基于 
战略 业务 应用 人员 的 安全 和 风险 管理 的 
有机 整体 从 管理 制度 到 工具 支撑 从 上层 
管理 架构 到 下层 技术 实现 采取 的 一系列 适合 
组织 数据 生命 周期 的 措施 Gartner 指出 了 从 
数据 的 加密 监控 审计 防 泄露 用户 身份证 用户 
行为 等 环节 入手 是 一个 错误 的 实践 数据安全 
治理 的 最佳 实践 是从 考虑 组织 的 经营 战略 
与 策略 面临 的 内外 合规 要求 整体 的 IT 
策略 以及 组织 的 安全 风险 容忍度 开始 然后 是 
对 数据 进行 分级 分类 再者 是 对 不同 级别 
的 数据 实行 合理 的 安全 手段 我们 可以 设计 
从 安全监控 评估 安全 技术 加固 安全 治理 服务 三位一体 
的 数据 安全 保障 体系 同时 依据 Gartner DSG 的 
理念 在 数据 的 全 生命 周期 中 采用 先从 
数据安全 治理 咨询 为 入口 的 阶梯式 数据安全 治理 思路 
详细 步骤 如下 图 所示 注 红色 字体 可 大量 
应用 人工智能 技术 三 人工智能 应用于 数据安全 治理 人工智能 在 
机器 学习 和 自然 语言 处理 方面 的 应用 一直 
受到 业界 的 关注 依托 人工智能 引擎 通过 对 业务 
数据 的 获取 清洗 语义 计算 数据挖掘 机器学习 知识图谱 认知 
计算 等 技术 将 快速 促进 数据安全 保障 体系 完善 
应用 机器学习 自然语言 处理 和 文本 聚 类 分类 技术 
能对 数据 进行 基于 内容 的 实时 精准 分类 分级 
而 数据 的 分类 分级 是 数据安全 治理 的 核心 
环节 数据 分类 引擎 已 成功 应用 在 邮件 内容过滤 
保密 文件管理 知识 挖掘 情报分析 反 欺诈 电子 发现 和 
归档 数据 防 泄露 等 领域 利用 人工智能 可 实现 
对 数据 的 智 准 深 的 识别 控制 和 
价值 挖掘 然而 人工智能 需要 海量 的 数据 人工智能 技术 
的 进步 取决于 各种 来源 数据 的 可用性 如何 确保 
这些 数据 的 安全性 与 保证 用户 数据 的 隐私 
性 又是 一个 相生 相伴 的 问题 想 看到 更多 
像 飞絮 老师 这样 的 大佬 们 带来 的 干货 
请 长按 下文 二维码 关注 世 平 信息 杭州 世 
平 信息 有限公司 简称 世 平 信息 致力于 智能化 数据 
管理 与 应用 的 深入 开拓 和 持续 创新 为 
用户 提供 数据安全 数据 治理 数据共享 和 数据 利用 解决 
方案 帮助 用户 切实 把握 大 数据 价值 与 信息 
安全 近期 热点 Black Hat 2018 | 会议 概览 + 
10个 热门 议题 央 采 中标 世 平 数据 防 
泄漏 数据 脱敏 系统 C 位 出道 信息安全 自主 可控 
可信计算 助飞 等级 保护 2.0 互利 共赢 | 世 平 
信息 与 盘 州市 区域 政府 开展 友好 交流 活动 
助推 数字 经济 世/n 平/n 信息/n 上榜/v 2018大/mq 数据/n 产业/n 
生态/n 地图/n 在/p 老板/n 曾是/nr 军人/n 的/uj 公司/n 工作/vn 是/v 
一种/m 怎样/r 的/uj 体验/n 网络 安全 执法 检查 大 数据 
安全 整治 我 该 怎么办 程序员 如何 自信 的 回答 
普通人 问 你 什么 是 人工智能 因为 我 被 问过 
答 不 出来 感觉 很 丢脸 当然 只是 个人 觉得 
不是 说 程序员 就要 懂 人工智能 试图 理解 智能 的 
本质 并 产生 一种 新的 智能 机器 它 可以 以 
类似 于 人类 智能 的 方式 作出 反应 该 领域 
的 研究 包括 机器人 技术 语音识别 图像识别 自然语言 处理 和 
专家 系统 自 人工智能 诞生 以来 理论 和 技术 日趋 
成熟 应用 领域 不断 扩大 可以 预见 在 未来 几年 
内 它 将 进入 人工智能 时代 人工智能 现在 遍布 全世界 
并在 日常 生活 中 经历 了 巨大 的 变化 这些 
AI 不是 科幻 电影 中 的 机器人 它们 具有 自我 
意识 并 计划 摧毁 世界 的 邪恶 相反 我们 的 
智能 手机 智能家居 银行/n 信用卡/n 管理员/n 和/c 智能/n 汽车/n 等/u 
产品/n 在/p 我们/r 每天/r 生活/vn 的/uj 产品/n 和/c 服务/vn 中/f 
使用/v AI/w 人工智能 将 通过 促进 自动 驾驶 汽车 的 
发展 改进 医学 图像 分析 促进 更好 的 医疗 诊断 
和 个性化 医疗 带来 重大 的 社会 转型 人工智能 也 
将 成为 支持 未来技术 发展 的 基本 资源 就像 电力 
和 网络 一样 但 对于 大多数 人 来说 人工智能 仍然 
非常 奇怪 充满 了 神秘 色彩 那么 让 我们 来 
谈谈 今天 人工智能 最 重要 的 功能 模式识别 有效 我 
希望 通过 简短 而 简洁 的 介绍 帮助 您 理解 
这 一 领域 人工智能 是 一门 严谨 的 科学 而 
不是 一种 无所不能 的 神话 媒体 过度 夸大 报道 人工智能 
的 功能 提倡 威胁论 是 不 负责任 的 人工智能 的 
目标 是 设计 一个 具有 智能 的 机器 其中 算法 
和 技术 基于 人脑 的 当前 研究结果 今天 许多 流行 
的 AI 系统 使用 人工 神经 网络 来 模拟 非常 
简单 的 互连 元素 的 网络 有点像 大脑 中的 神经元 
这些 网络 可以 通过 调整 单元 之间 的 连接 来 
学习 经验 这个 过程 类似 于 通过 修改 神经元 之间 
的 连接 而 学习 的 人和 动物 大脑 神经 网络 
可以 学习 模式识别 翻译 语言 学习 简单 的 逻辑推理 甚至 
创建 图像 或 形成 新的 设计 其中 模式识别 是 一项 
特别 重要 的 功能 因为 AI 非常 擅长 识别 海量 
数据 中 隐藏 的 模式 这 对于 依赖 经验 和 
知识 的 人 来说 并不 容易 这些 程序 运行 在 
具有 数百万 单位 和 数十 亿 连接 的 神经 网络 
上 我们 现在 可以 创建 的 智能 由 这些 电子 
神经元网络 组成 机器 没有 人体 器官 和 大脑 它们 可以 
很好 地 协同工作 例如 当 我们 看到 一只 狗 时 
我们 会 很快 判断 它 是 什么 动物 以及 它 
是 什么 类型 的 动物 这个 看似 简单 的 过程 
对于 机器 来说 非常 困难 人类 获得 这种 力量 的 
能力 也 来源于 数亿 年来 生物学 的 进化 过程 机器 
了解 世界 的 方式 是 通过 模型 需要 通过 复杂 
的 算法 和 数据 建立 模型 这样 机器 就 可以 
获得 简单 的 感知 和 判断 能力 以下 描述 了 
深度 学习 系统 中 最重要 的 算法 之一 即 卷积 
神经网络 如果 你 以前 对 AI 有 一些 了解 那么 
你 一定 听说 过 这个 概念 该 算法 涉及 人类 
和 其他 动物 大脑 视觉 皮层 结构 的 研究 简要 
介绍 这种 特殊 类型 的 人工神经网络 它 使用 感知器 机器学习 
单元 算法 来 监督 数据 的 学习 和 分析 适用于 
图像处理 自然语言 处理 和 其他 类型 的 认知 任务 与 
其他 类型 的 人工神经网络 一样 卷积 神经网络 具有 输入 层 
输出 层 和 各种 隐藏 层 其中 一些 层 是 
复卷 的 并 使用 数学模型 将 结果 传递 给 连续 
的 层 该 过程 模仿 人类 视觉 皮层 中 的 
一些 动作 因此 称为 卷积 神经 网络 或 CNN 例如 
当 我们 人类 看到 猫 和狗时/nr 虽然 它们 的 大小 
相似 但 我们 可以 立即 将 它们 与 猫 和狗/nr 
区分 开来 对于 计算机 图像 只是 一堆 数据 通过 神经 
网络 的 第一 层 中的 特征 来 检测 对象 的 
轮廓 神经 网络 的 下 一层 将 检测 由 这些 
简单 模式 组合 形成 的 简单 形状 例如 动物 眼睛 
和 耳朵 下 一层 将 检测 由 这些 形状 组合 
形成 的 物体 的 某些 部分 例如 猫 和狗的/nr 头部 
或 腿部 神经 网络 的 最后 一层 将 检测 这些 
部分 的 组合 完整 的 猫 完整 的 狗 等等 
神经 网络 的 每 一层 将 执行 图像 组合分析 和 
特征 检测 以 判断 和 组合 并 将 结果 传递 
给 下一层 神经网络 所 使用 的 神经 网络 的 实际 
深度 将比 该 示例 深 得多 因此 神经 网络 可以 
以 这种 分层 方式 执行 复杂 的 模式识别 只要 有 
大量 标记 的 样本 数据库 就 可以 在 神经 网络 
上 进行 特征 训练 它 对于 识别 图像 视频 语音 
音乐 甚至 文本 特别 有用 为了 很好 地 训练 AI 
的 机器 视觉 我们 需要 提供 这些 神经 网络 所 
标记 的 大量 图像 数据 神经 网络 学习 将 每个 
图像 与其 对应 的 标签 相关联 您 还 可以 将 
之前 从未 见过 的 图像 与 相应 的 标签 配对 
这样 的 系统 可以 对 各种 图像 进行 分类 并 
识别 照 片中 的 元素 同时 神经 网络 在 语音 
识别 和 文本 识别 中 也 非常 有用 自动 驾驶 
汽车 和 最新 的 医学 图像 分析 系统 也 是 
关键 组件 因此 您 可以 看到 神经 网络 的 使用 
非常 广泛 和 有效 最初 有 必要 依靠 手工 标记 
大量 有效 数据 来 完成 知识 输入 现在 通过 运行 
海量 数据 神经 网络 可以 自学 大大 增强 的 人工智能 
应用 范围 降低 了 使用 门槛 人类 的 大脑 与 
动物 大不相同 在 进化 过程 中 具有 高度 的 专业性 
和 适应性 目前 的 人工 智能系统 远 没有 人类 拥有 
的 看似 普遍 的 智能 人工智能 的 更 先进 的 
发展 将 在后面 讨论 我们 仍然 关注 现在 实施 的 
人工智能 的 基本 原则 人工智能 相关 阅读 游戏 的 人工智能 
时代 悄悄 来临 AI 已经 发展 61年 人工智能 已经 到 
了 春天 人工智能 NLP 需要 了解 这些 东西 nlg 自然语言 
生成 有写/nr 什么 项目 写作 神器 人工智能 写作 软件 有 
哪些 2018年 NLP 技术 学习 总结 伪 原创 文章 的 
软件 自然语言 处理 主要 步骤 包括 1 . 分词 只 
针对 中文 英文 等 西方 字母 语言 已 经用 空格 
做好 分词 了 将 文章 按 词组 分开 2 . 
词 法分析 对于 英文 有 词头 词根 词尾 的 拆分 
名词 动词 形容词 副词 介词 的 定性 多种 词意 的 
选择 比如 DIAMOND 有 菱形 棒球场 钻石 3个 含义 要根据 
应用 选择 正确 的 意思 3 . 语法分析 通过 语法树 
或 其他 算法 分析 主语 谓语 宾语 定语 状语 补语 
等 句子 元素 4 . 语义分析 通过 选择 词 的 
正确 含义 在 正确 句法 的 指导 下 将 句子 
的 正确 含义 表达出来 方法 主要 有 语义 文法 格 
文法 但是 以上 的 分析 仅 适用 于小/nr 规模 的 
实验室 研究 远 不能 应用 到 实际 语言 环境 中 
比如说 语法 我们 能 总结 出 的 语法 是 有限 
的 可是 日常 应用 的 句子 绝大部分 是 不遵守 语法 
的 如果 让 语法 包罗 所有 可能 的 应用 会 
出现 爆炸 的 景象 自然语言 处理 的 应用 方向 主要 
有 1 . 文本 分类 和聚类/nr 主要 是 将 文本 
按照 关键 字词 做出 统计 建造 一个 索引库 这样 当 
有关 键 字词 查询 时 可以 根据 索引库 快速 地 
找到 需要 的 内容 此 方向 是 搜索引擎 的 基础 
在 早期 的 搜索引擎 比如 北大 开发 的 天问 系统 
采用 这种 先 搜集 资料 在 后台 做 索引 在前 
台 提供 搜索 查询 服务 目前 GOOGLE 百度 的 搜索引擎 
仍旧 类似 但是 采用 了 自动 蜘蛛 去 采集 网络 
上 的 信息 自动 分类 并 做 索引 然后再 提供 
给 用户 我 曾经 在 我 的 文章 中 做过 
测试 当 文章 中有 十八禁 这样 的 字眼 时 点击 
次数 是 我 其他 文章 点击 次数 的 几十 倍 
说明 搜索 引擎 将 十八禁 这个词 列为 热门 索引 一旦 
有 一个 蜘蛛 发现 这个 词 其他 蜘蛛 也会 爬 
过来 2 . 信息检索 和 过滤 这是 网络 瞬时 检查 
的 应用 范畴 主要 为 网警 服务 在 大 流量 
的 信息 中 寻找 关键词 找到 了 就要 做 一些 
其他 的 判断 比如 报警 3 . 信息 抽取 抄书 
信息 抽取 研究 旨在 为 人们 提供 更 有力 的 
信息 获取 工具 以 应对 信息 爆炸 带来 的 严重 
挑战 与 信息检索 不同 信息 抽取 直接 从 自然语言 文本 
中 抽取 事实 信息 过去 十 多年来 信息 抽取 逐步 
发展 成为 自然 语言 处理 领域 的 一个 重要 分支 
其 独特 的 发展 轨迹 通过 系统化 大 规模 地 
定量 评测 推动 研究 向前 发展 以及 某些 成功 启示 
如 部分 分析 技术 的 有效性 快速 自然语言 处理 系统 
开发 的 必要性 都 极大 地 推动 了 自然 语言 
处理 研究 的 发展 促进 了 自然 语言 处理 研究 
与 应用 的 紧密 结合 回顾 信息 抽取 研究 的 
历史 总结 信息 抽取 研究 的 现状 将 有助于 这 
方面 研究 工作 向前 发展 4 . 问答 系统 目前 
仍 局限于 80 年代 的 专家 系统 就是 按照 LISP 
语言 的 天然 特性 做 逻辑 递归 LISP 语言 是 
括号 式 的 语言 比如 A = B C D 
A = B E F 提问 已知 B C 能 
得到 什么样 的 结论 结论 是 A D 若 提问 
改为 已知 B 结论 则是 C D A 或 E 
F A 比如 一个 医疗 用 的 专家 系统 你 
若 询问 感冒 的 治疗 方法 系统 可能 给 出 
多种 原因 带来 的 感冒 极其 治疗 方法 你 若 
询问 病毒性 感冒 的 治疗 方法 则 系统 会 给出 
比较 单一 的 明确 的 治疗 方法 你 有 没有 
用过 AUTOCAD 系统 这个 就是 建立 在 LISP 语言上 的 
括号 系统 在用 的 时候 会 出现 上述 情况 5 
. 拼音 汉字 转换 系统 这 应该 是 中文 输入法 
应用 范畴 的 东西 再多 的 东西 我 就 没 
想过 6 . 机器翻译 当前 最 热门 的 应用 方向 
这 方面 的 文章 最多 国际 上 已经 有 比较 
好 的 应用 系统 美国 有个 AIC 公司 推出 过 
著名 的 实时 翻译 系统 欧共体 的 SYSTRAN 系统 可以 
将 英 法 德 西 意 葡 六种 语言 实时 
对 译 美 日 德 联合 开发 的 自动 语音 
翻译 系统 成功 进行 了 10 多分钟 对话 我国 军事 
科学院 中科院 也 开发 过 此类 系统 但是 这 里边 
的 问题 也 很多 最 主要 的 是 满篇 洋文 
难不住 满篇 译文 看不懂 就是 脱离 了 人类 智慧 的 
机器 翻译 总 会搞 出让 人 无法 理解 的 翻译 
比如 多意 词 选择 哪个 意思 合适 怎么 组织 出 
通顺 的 语句 等等 所以 目前 微软 GOOGLE 的 新趋势 
是 翻译 + 记忆 类似 机器学习 将 大量 以往 正确 
的 翻译 存储 下来 通过 检索 如果 碰到 类似 的 
翻译 要求 将 以往 正确 的 翻译 结果 拿出 来用 
GOOGLE 宣称 今后 几年 就 可以 推出 商业化 的 网页 
翻译 系统 7 . 新 信息 检测 这个 我 不 
知道 没 思路 以上 已经 回答 了 自然 语言 发展 
方向 的 问题 我 认为 机器 翻译 是 最有 前途 
的 方向 其 难点 在于 机器翻译 还 不具备 人类 智能 
虽然 翻译 已经 达到 90% 以上 的 正确 程度 然而 
还是 不能 象 人类 翻译 那样 可以 准确 表达 为什么 
存在 这样 的 难点 关键 是 自然 语言 处理 做不到 
人类 对 自然 语言 的 理解 处理 和 理解 是 
天差地别 的 两个 概念 处理 好比 控制 眼睛 耳朵 舌头 
的 神经 他们 将 接收 的 信息 转化成 大脑 可以 
理解 的 内部 信息 或者 反过来 他们 的 功能 就是 
这么 多 而 理解 则是 大脑皮层 负责 语言 理解 那 
部分 多少 亿 的 脑细胞 共同 完成 的 功能 一个 
人 因为 其 自身 家庭 背景 受 教育 程度 接触 
现实 中 长期 形成 的 条件反射 刺激 特殊 的 强列/nr 
刺激 当时 的 心理 状况 这么 多 的 因素 都会 
影响 和 改变 理解 的 功能 比如 我 说 一个 
靓女 开着 BMW 跑车 有人 心里 会想 这是 二奶 吧 
有人 心里 会 仇视 她 联想 到她 会 撞了人 白撞 
做 汽车 买卖 的 人 则 会去 估量 这部 车的/nr 
价值 爱 攀比 的 人 也许 会想 我 什么 时候 
才能 开上 BWM 所以 理解 是 更加 深奥 的 东西 
涉及 更多 神经学 心理学 逻辑学 领域 还有 上下文 理解 问题 
比如 这句 我们 90 平方米 以后 会占 的 分量 越来越 
大 那么 这样 他 的 价格 本身 比 高档 低 
很多 所以 对于 整体 把 这个 价格 水平 给 压 
下来 了 这个 确实 非常 好 的 你 能理解 么 
估计 很难 或者 理解 出 多种 意思 但是 我 把 
前文 写出来 去年 国家 九 部委 联合 发布 了 建设部 
等 部门 关于 调整 住房 供应 结构 稳定 住房价格 意见 
的 通知 对 90 平方米 以下 住房 须占/nr 总面积 的 
70% 以上 作出 了 硬性 规定 深圳市 经过 一年 的 
调控 目前 已 做到 每个 项目 的 75% 都是 90 
平方米 以内 深圳市 国土 资源 和 房产 管理 局 官员 
说 看了 后面 的 你 才能 知道 是 根据 国家 
的 通知 深圳 做了 相应 的 调整 自然语言 理解 1 
. 语义 表示 自然语言 理解 的 结果 就是 要 获得 
一个 语义 表示 semantic representation 语义 表示 主要 有三种 方式 
1 . 分布 语义 Distributional semantics2 . 框架 语义 Frame 
semantics3 . 模型论 语义 Model theoretic semantics1 . 1 分布 
语义 表示 Distributional semantics 说 distributional semantics 大家 比较 陌生 
但 如果 说 word2vec 估计 大家 都 很熟悉 word2vec 的 
vector 就是 一种 distributional semantics distributional semantics 就是 把 语义 
表示 成 一个 向量 它 的 理论 基础 来自于 Harris 
的 分布 假设 语义 相似 的 词 出现 在 相似 
的 语境 中 Semantically similar words occur in similar contexts 
具体 的 计算 方法 有 多种 比如 LSA Latent Semantic 
Analysis LDA Latent Dirichlet Allocation 及 各种 神经网络 模型 如 
LSTM 等 这种 方法 的 优点 在于 它 完全 是 
数据 驱动 的 方法 并且 能够 很好 的 表示 语义 
但 一个 很大 的 缺点 在于 它 的 表示 结果 
是 一个 整体 没有 进一步 的 子结构 1.2 框架 语义 
表示 Frame semantics 顾名思义 这种 方法 把 语义 用 一个 
frame 表示出来 比如 我们 一 开始 举 得 例子 订 
一张 明天 北京 去 杭州 的 机票 国航 头等舱 表示 
如下 在 计算 方法 上 典型 的 比如 语义 角色 
标注 Semantic Role Labeling 具体 可以 分为 两个 步骤 frame 
identification 和 argument identification frame identification 用于 确定 frame 的 
类型 argument identification 用于 计算 各个 属性 的 具体 值 
这种 方法 和 distributional semantics 相比 能够 表达 丰富 的 
结构 1.3 模型论 语义 表示 Model theoretic semantics 模型 轮 
语义 表示 的 典型 框架 是 把 自然 语言 映 
射成 逻辑 表达式 logic form 比如 对于 下图 中的 中国 
面积 最大 的 省份 是 哪个 将其 表示 成 逻辑 
表达式 就是 图 中 红色 字体 部分 进一步 那 这个 
逻辑 表达式 去 知识库 中 查询 就 得到 了 答案 
在 计算 方法 上 典型 的 就是 构建 一个 semantic 
parser 模型论 语义 表示 是 对 世界 知识 的 完整 
表示 比 前 两种 方法 表达 的 语义 更加 完整 
但是 缺点 是 semantic parser 的 构建 比较 困难 这 
大大 限制 了 该 方法 的 应用 1.4 目前 采用 
的 语义 表示 目前 常用 的 是 frame semantics 表示 
的 一种 变形 采用 领域 domain 意图 intent 和 属性 
槽 slots 来 表示 语义 结果 其中 领域 是 指 
同一 类型 的 数据 或者 资源 以及 围绕 这些 数据 
或 资源 提供 的 服务 比如 餐厅 酒店 飞机票 火车票 
电话 黄页 等 意图 是 指 对于 领域 数据 的 
操作 一般以 动宾短语 来 命名 比如 飞机票 领域 中 有 
购票 退票 等 意图 属性 槽 用来 存放 领域 的 
属性 比如 飞机票 领域 有 时间 出发地 目的地 等 对于 
飞机票 领域 我们 的 语义 表示 结构 如下 图 所示 
进一步 我们 对于 世界 的 语义 描述 又 称为 domain 
ontology 如下 2 . 自然语言 理解 技术 难点 在 确定 
了 自然 语言 理解 的 语义 表示 方法 后 我们 
把 技术 方案 抽象 为 如下 两步 这 和 前文 
提到 的 语义 角色 标注 把 过程 分为 frame identification 
和 argument identification 类似 领域 分类 和 意图 分类 对应 
frame identification 属性 抽取 对应 argument identification 无论 对于 分类 
还是 对于 抽取 来说 都/d 需要/v 有/v 外部/f 知识/v 的/uj 
支持/v 在 实现 的 过程 中 我们 面临 着 如下 
的 困难 1 如何 构建 知识库 总参 除了 表示 总参谋部 
外 还是 南京 一家 很 火 的 火锅店 中华 冷面 
除了 是 一种 面条 还是 一首 歌名 王菲 的 红豆 
是 指 王菲 唱 的 红豆 这 首歌 但 如果 
说 韩红 的 红豆 就不 对了 因为 韩红 没有 唱过 
红豆 这 首歌 要想 把 这些 知识 都 理解 对 
就 需要 一个 庞大 的 知识库 这个 知识库 中的 实体词 
数以千万计 怎么 挖掘 怎么 清洗 噪音 都是 很大 的 挑战 
2 如何 理解 用户 语句 的 意图 东三 环堵 吗 
这句话 意图 是 查询 路况 下水道 堵吗/nr 就 不是 查 
路况 了 今天 的 天气 是 想问 天气状况 今天 的 
天气 不错 则无 此意 附近 哪儿 可以 喝 咖啡 是 
想找 咖啡馆 但 牛皮癣 能 喝咖啡 吗 就是 一个 知识 
问答 了 类似 上述 的 例子 举不胜举 更 别说 语言 
理解 还 受 时间 位置 设备 语境 等等 问题 的 
影响 3 如何 构建 可扩展 的 算法 框架 现实 世界 
包含 众多 的 领域 而 我们 不 可能 一次性 的 
把 所有 领域 都 定义 清楚 并且 实现 之 那 
我们 就 需要 一个 可 扩展 的 算法 框架 每当 
修改 或者 新增 某个 领域 的 时候 不会 对 其他 
领域 造成 干扰 4 如何 构建 数据 驱动 的 计算 
流程 大 数据 时代 如果 一个 算法 或者 流程 不 
是 数据 驱动 的 不是 随着 数据 的 增加 而 
自动 提升 效果 那 这个 算法 框架 就 没有 持续 
的 生命力 5 如何 融入 上下文 知识 在 对话 场景 
中 每/zg 句话/i 都有/nr 对话/n 上下文/l 同样 的 句子 在 
不同 的 上下 文中 理解 结果 是 不 一样 的 
比如 如下 的 例子 同样 的 一句话 今天天气 好吗 在 
左侧 图中 属于 天气 领域 而在 右侧 图中 则 属于 
音乐 领域 C h i n e s e N 
L P c o r p u s A n 
collection of Chinese nlp corpus including basic Chinese syntactic wordset 
semantic wordset historic corpus and evaluate corpus . 中文 自然语言 
处理 的 语料 集合 包括 语义 词 领域 共时 历时 
语料库 评测 语料库 等 本 项目 简单 谈谈 自己 对 
语言 资源 的 感想 以及 目前 自己 进行 语言 资源 
构建 的 现状 项目 地址 https / / github . 
com / liuhuanyong / ChineseNLPCorpus 介绍 语言 资源 本身 是 
一个 宽泛 的 概念 即 语言 + 资源 语言 指 
的 是 资源 的 限定 域 资源 = 资 + 
源 是 资料 的 来源 或者 汇总 加在一起 也就 形成 
了 这样 一种 界定 任何 语言 单位 形成 的 集合 
都 可以 称为 语言 资源 语言 资源 是 自然 语言 
处理 任务 中 的 一个 必不可少 的 组成部分 一方面 语言 
资源 是 相关 语言 处理 任务 的 支撑 为 语言 
处理 任务 提供 先验 知识 进行 辅助 另一方面 语言 处理 
任务 也为 语言 资源 提出 了 需求 并 能够 对 
语言 资源 的 搭建 扩充 起到 技术性 的 支持 作用 
因此 随着 自然语言 处理 技术 的 不断 发展 自然语言 处理 
需求 在 各个 领域 的 不断 扩张 应用 相关 语言 
资源 的 构建 占据 了 越来越 为 重要 的 地位 
作者 硕士 期间 所在 的 研究 机构 为 国家 语言 
资源 监测 与 研究 平面 媒体 中心 深受 导师 所 
传授 的 语言 资源 观 熏陶 并在 实际 的 学习 
工作 过程 中 动手 实践 形成 了 自己 的 一些 
浅薄 的 语言 资源 认识 现在 写出来 供 大家 一起 
讨论 主要 介绍 一些 自己 对 语言 资源 的 搜索 
搭建 过程 中 的 一些 心得 以及 自己 目前 在 
语言 资源 建设 上 的 一些 工作 语言 资源 的 
分类 介绍 中 说到 任何 语言 单位 的 集合 都 
可以 称为 语言 资源 比如 我 有 一个 个人 的 
口头禅 集合 这个 就 可以 称为 一个 语言 资源库 在 
你 实际 生活 中 进行 言语 活动 时 你 其实 
就 在 使用 这个 语言 资源库 再 比如说 一个 班级 
中 的 学生 名单 其实 也 可以 当作 是 一种 
语言 资源 这个 语言 资源 在 进行 班级 学生 点名 
考核 的 时候 也 大 有帮助 当然 此处 所 讨论 
的 语言 资源 是从 自然语言 处理 应用 的 角度 上 
出发 的 总的来说 我 把 它 归为 以下 两种 类型 
1 领域 语料库 领域 语料库 是从 语料 的 这个 角度 
来讲 的 这里 的 语料 界定 成 文本 级别 以 
自然 语句 为基础 级别 形成 的 文本 集合 即 可以 
是 句子 段落 篇章 等 领域 语料库 可以 根据 不同 
的 划分 规则 而 形成 不同 的 语料 类别 1 
根据 所属 领域 可以 进一步 细化 成 不同 领域 的 
语料库 包括 金融 领域 语料 医药 领域 语料 教育领域 语料 
文学 领域 语料 等等 2 根据 所属 目的 可以 进一步 
细化 为 评测 语料 为 自然语言 处理 技术 pk 而 
人工 构造 的 一些 评测 语料 如 ACE MUC 等 
国际 评测 中 所 出现 的 如 semeval2014 snli 等 
工具 语料 指供 自然语言 处理 技术 提供 资源 支撑 的 
语料 3 根据 语料 加工 程度 的 不同 可 进一步 
分为 熟 语料 指在 自然语言 单位 上 添加 人工 的 
标签 标注 如 经过 分词 词性 标注 命名 实体 识别 
依存 句法 标注 形成 的 语料 生 语料 指 直接 
收集 而 未经 加工 形成 的 语言 资源 集 如 
常见 的 微博 语料 新闻 语料 等 4 根据 语料 
语种 的 不同 可 进一步 分为 单语/nr 语料/n 和/c 多语/i 
语料/n 多语 语料 指 的 是 平行 语料 常见于 机器翻译 
任务 中 的 双语 对齐 语料 汉 阿 平行 语料库 
汉 英 平行 语料库 等 5 根据 语料 规模 的 
不同 可以 进一步 分为 小型 语料库 中型 语料库 大型 语料库 
至于 小型 中型 大型 的 界定 可 根据 实际 领域 
语料 的 规模 而 动态 调整 2 领域 词库 领域 
词库 指 以 句 级 以下 语言 单位 形成 的 
语言 资源库 这个 层级 的 语言 单位 可以 是 笔画 
偏旁部首 字 词 短语 等 同样 的 领域 词库 也 
可以 进一步 细分 1 领 域特征 词库 这里 所说 的 
领域 特征 词库 指 的 是 与 领域 强 相关 
具有 领域 区别 能力 形成 的 词语 集合 如 体育 
领域 中 常见 的 篮球 足球 等 词 文学 领域 
常见 的 令狐冲 鲁迅 等 词 又如 敏感 词库 等 
这些 词 常常 可 作为 分类 特征 而 存在 2 
语法 语义 词库 语义 词库 的 侧重点 在与 语言 的 
语法 层面 和 语义 层面 a 语法 词库 北大 的 
语法 信息 词典 北大 的 实体 概念 词典 Hownet 语义 
词典 这三类 词典 这几个 语法 词库 在 对词 的 语法 
功能上 都做 了 不同 的 工作 对词 的 内部 结构 
信息 进行 了 详细 的 标注 如 北大 的 语法 
信息 词典 以 词类 为 划分 标准 讲 汉语 的 
常用词 进行 了 划分 并对 词性 搭配 前 接 成分 
和 后接 成分 进行 了 详细 的 标注 Hownet/w 语义/n 
词典/n 从/p 义项/n 的/uj 角度/n 对词/v 的/uj 义/ng 元/m 进行/v 
了/ul 分解/v 和/c 注释/v b 语义 词库 这类 语义 词 
侧重点 不在 词语 的 内部 语法结构 而在 词语 的 整体 
语义上 这类 词库 常见/a 的/uj 词库/n 有/v 哈工大/nt 发布/v 的/uj 
同义词/n 词/n 林/ng 扩展/v 版/n 这个 词库 将 同义词 按照 
语义 的 相近 程度 进行 了 不同 层次 的 聚 
类 可以 作为 同义词 扩展 提供 帮助 另 一个 是 
情感 分析 任务 中 常用 的 情感 词典 这类 词典 
主要 公开 的 词典 包括 大连理工大学 信息检索 实验室 公开 的 
情感 本体 词库 hownet 香港中文大学 台湾 清华大学 公开 的 情感 
词库 具体 包括 情感 词库 否定 词库 强度 词库 等 
另外 工业界 有 boson 公开 的 微博 情感 词库 词 
的 规模 比较 大 但 标注 信息 不是 很 精准 
还有 的 则是 中文 的 反义 词库 等 这个 可以 
参考 我 的 github 项目 里面 对 这些 词库 也 
有 一些 涉及 语言 资源 的 问题 语言 资源 的 
搭建 指 的 是 语言 资源 的 整个 搭建 过程 
其实 是 要 解决 四 个 问题 一个 是 语言 
资源 的 收集 问题 二 是 语言 资源 的 融合 
标准化 问题 三 是 语言 资源 的 动态 更新 问题 
四 是 语言 资源 的 共享 与 联盟 问题 下面 
就这 四点 展开 阐述 1 语言 资源 收集 的 问题 
语言 资源 搜索 过程 中 有 三步走 策略 在 这个 
步骤 完成 之后 会 得到 一 系列 的 词库 这些 
词库 可能 初期 不会 特别 完善 往往 还 需要 人工 
使用 启发式 规则 进行 人工 去 噪 的 工作 2 
语言 资源 的 融合 标准化 问题 通过 不同 方式 收集 
起来 的 语言 资源 往往 会 存在 一个 格式 不 
对称 的 问题 这 有点 像 知识图谱 中 的 知识 
融合 问题 因此 为了 解决 这个 问题 我们 通常 需要 
制定 一个 标准化 的 语言 资源 格式 例如 在 构建 
情感 词表 的 过程 当中 有的/nr 情感 词表 没有 强度 
标记 有的 强度 值 范围 不 一样 有的/nr 情感 词表 
的 标记 不一 这个 时候 往往 需要 标准化 给定 一个 
标准化 的 样式 再将 不同 来源 的 情感 词 按照 
这个 标记 做 相应 的 调整 我 在 实际 的 
工作 过程 中 常常 把 这种 问题 类 别成 知识图谱 
构建 过程 中 的 schema 搭建 问题 信息 抽取 过程 
中 的 slot definition 问题 先把 规范 和 标准 搭好 
再去 统一 标准化 3 语言 资源 的 动态 更新 问题 
知识 和 信息 的 价值 在 很大 程度 上 都 
在于 它 的 一种 实时性 语言 资源 作为 一种 常识性 
知识库 能够 保证 自身 的 一种 与时俱进 将 能够 最大限度 
地 发挥 自身 的 价值 而从 实践 的 角度 上 
来说 语言 资源 的 动态 更新 可以 靠 人工 去 
维持 去 动态 及时 更新 也 可以 建立 一种 动态 
监测 和 更新 机制 让 机器 自动 地 去 更新 
这类 其实 可以 参考 知识图谱 更新 的 相关 工作 4 
语言 资源 的 共享 与 联盟 问题 语言 资源 是否 
共享 其实 是 一个 与 业务 敏感 以及 开源 意识 
想 结合 的 一种 决策 有的/nr 资源 因为 某 种 
业务 敏感 或者 开源 意识 不够 open 而 无法 共享 
当然 还有 其他 因素 成分 在 不过 语言 资源 最好 
是 需要 共享 的 这样 能够 最大 力度 的 发挥 
语言 资源 在 各个 领域 的 应用 语言 资源 的 
联盟 问题 更 像是 对开 源语言 资源 的 一种 链接 
与 互联 这类 问题 是 对 当前 的 资源 零散 
碎片化 问题 的 一个 思考 前面 也 说到 目前 情感 
分析 的 词表 有 很多 个 语法/n 和/c 语义/n 词库/n 
也/d 有/v 很多/m 个/q 但 每个人 在 构建 时的/nr 出发点 
不同 构建 者 也 分布 在 不同 的 高校 或 
机构 当中 这些 资源 虽然 在 个数 上会 有 增长 
但 随着 时间 的 推移 这种 零散化 的 现象 将 
会 越来越 严重 语言 资源 的 实践 作者 在 学习 
和 工作 之余 根据 语言 资源 搭建 策略 构建 起了 
语义 词库 领域 词库 领域 语料库 评测 语料库 种类 约 
50种 具体 如下 语义 知识库 类型 名称 介绍 语义 词库 
语法 信息 词典 汉语 词语 的 语法 功能 分类 词语 
的 语法 属性 描述 语义 词库 Hownet 义 原 词典 
董 振东 老师 研制 汉 语词 语义 原 分类 语义 
词库 程度 副词 词典 表示 程度 的 词语 义 词库 
现代 汉语 词典 现代 汉语 词典 txt 版本 语义 词库 
否定词 词典 对 意义 进行 反转 的 词典 语义 词库 
同义词 词 林 词典 哈工大 同义词 词典 语义 词库 反义词 
词典 反义词 词表 1.5 W 对 语义 词库 同义词 词典 
同义词 词典 5.5 W 对 语义 词库 schema 概念 词典 
互动 百科 概念 体系 百度 百科 概念 体系 语义 词库 
停用词 自然语言 处理 用 停用词 词表 领域 词库 类型 名称 
介绍 领域 词库 搜狗 输入法 领域 词库 超过 1W 个 
领域 的 搜狗 输入法 词库 txt 版本 领域 词库 职位 
词典 基于 百万级 拉钩 JD 网 抽取 形成 的 职位 
词典 领域 词库 敏感词 词 词库 敏感词 词库 包括 政治 
反动 等 词 领域 词库 情感 词 词库 大连理工 知网 
港 中大 台大 boson 等 公开 情感 词典 领域 语料库 
类型 名称 介绍 领域 语料库 人民日报 标注 语料 1998年 人民日报 
分词 语料库 领域 语料库 20类 小说 文本 集合 20个 领域 
武侠 恐怖 等 小说 集合 7K + 小说 文本 领域 
语料库 字幕 网 70W 字幕 文本 语料 字幕 网 字幕 
文件 解析 70W 字幕 文本 语料 领域 语料库 内涵 段子 
50W 等 语料 基于 内涵 段子 采集 50W 短文 本 
领域 语料库 歌词 14W 语料 基于 公开 歌词网 采集 14W 
首 歌曲 歌词 领域 语料库 职位 JD 语料 基于 公开 
职位 采集 213W 职位 jd 领域 语料库 古诗词 语料 唐诗宋词 
语料 集合 10W 篇 领域 语料库 相声 剧本 语料 基于 
公开 相声剧 本网站 采集 6K 篇 领域 语料库 中文 维基百科 
语料 中文 简体版 98W 篇 领域 语料库 法务 问答 语料 
法务 咨 询问 答对 22W 领域 语料库 股票 问答 语料 
股票 相关 咨询 问 答对 10W 领域 语料库 携程 攻略 
50W 携程 攻略 文 本集 50W 篇 领域 语料库 法律 
案例 语料 17W17W 法律 案例 语料 带 案例 标签 领域 
语料库 人民日报 历时 语料库 1946 20031946 2003 133W 篇 领域 
语料库 参考消息 历时 语料库 1957 20021957 2002 57W 篇 领域 
语料库 腾讯 滚动 新闻 历时 语料库 2009 2016 腾讯 历时 
滚动 新闻 13 板块 领域 语料库 酒店 评论 语料 酒店 
评论 数据 7K 条 领域 语料库 外卖 点评 语料 外卖 
评论 数据 1.2 W 条 领域 语料库 京东 商品 评论 
语料 10类 商品 6W 条 领域 语料库 新浪 微博 情感 
语料 正文 及 评论 10W 条 领域 语料库 细粒度 微博 
情感 语料 喜悦 愤怒 厌恶 低落 等 标签 共 36W 
条 领域 语料库 电影 评论 语料 电影 评分 评论 语料 
200W + 条 领域 语料库 餐馆 点评 语料 餐馆 点评 
语料 440W 条 领域 语料库 亚马逊 商品 评论 语料 亚马逊 
商品 评论 语料 720W 条 评测 语料库 类型 名称 介绍 
评测 语料库 问句 匹配 英文 question 相似 问句 6.5 W 
对 中文 微众/nr 银行 问句 集 1000对 评测 语料库 命名 
实体 识别 中文 电子 病历 命名 实体 识别 微软 MSR 
命名 实体 识别 5W 条 评测 语料库 情感 分析 斯坦福 
sentibank 评测 语料库 实体 关系 抽取 中文 人物 关系 数据集 
英文 SEMEVAL2008 评测 数据集 NYT NYTfilter 评测 语料库 文本 蕴含 
英文 snli multinli 数据集 116W 中文 文本 蕴含 数据集 100W 
评测 语料库 音乐 问句 解析 音乐 问句 解析 数据集 1.2 
W 评测 语料库 幽默 计算 中文 幽默 计算 数据集 幽默 
类型 幽默 等级 隐喻 类型 隐喻 等级分类 等 评测 语料库 
阅读 理解 squad 数据集 评测 语料库 知识图谱 补全 知识图谱 链接 
数据集 FB15K FB40K Freebase WN18 WordNet 评测 语料库 中文 实体 
链接 基于 中文 百科 知识 的 实体 链接 数据集 1.3 
K 评测 语料库 中文 自动 问答 中文 智能 问答 数据集 
两个 任务 问句 意图 分类 航空 酒店 火车 客服 问答 
评测 语料库 中文 罪行 分类 法律 智能 评测 数据集 288W 
总结 1 本 项目 阐述 了 语言 资源 的 相关 
感想 并给 出了 目前 语言 资源 的 构建 现状 目前 
为止 收集 了 四个 大类 共 50 小 类 的 
语言 资源 数据集 2 本 项目 中 所 涉及 到 
的 报告 内容 均 来源 于 网上 公开 资源 对此 
免责 声明 3 如果 有 需要 用到 以上 作者 收集到 
的 这些 语料库 可以 联系 作者 获取 4 自然语言 处理 
是 人工智能 皇冠 上 的 一颗 明珠 懂 语言者 得 
天下 语言 资源 在 自然 语言 处理 中 扮演 着 
举足轻重 的 作用 懂 语言 资源 者 分得 天下 目前 
开放 的 网络 环境 对 语言 资源 的 大 繁荣 
提供 了 很大 的 契机 语言 资源 构建 是 一门 
学问 也 是 一种 手段 现在 自然语言 处理 技术 也 
对 语言 资源 的 构建 提供 了 技术 上 的 
支持 如何 把握 语言 资源 搜索 策略 搭建 策略 重点 
解决 语言 资源 的 动态 更新 共享 与 联盟 问题 
将 是 语言 资源 建设 未来 需要 解决 的 问题 
本 项目 地址 https / / github . com / 
liuhuanyong / ChineseNLPCorpus 如有 自然语言 处理 知识图谱 事理 图谱 社会 
计算 语言 资源 建设 等 问题 或 合作 可 联系 
我 1 我 的 github 项目 介绍 https / / 
liuhuanyong . github . io2 我 的 csdn 博客 https 
/ / blog . csdn . net / lhy20143 刘焕勇/nr 
中国 科学院 软件 研究所 lhy _ in _ blcu @ 
126 . com 关注 网易 智能 聚焦 AI 大事件 读懂 
下 一个 大 时代 选自 | 学术 头条 SciTouTiao 作者 
| AMiner 自然语言 处理 是 包括 了 计算机 科学 语言学 
心理 认知 学 等 一系列 学科 的 一门 交叉学科 这些 
学科 性质 不同 但 又 彼此 相互 交叉 1950年 图灵 
提出 了 著名 的 图灵测试 这 一般 被 认为 是 
自然 语言 处理 思想 的 开端 20 世纪 50 年代 
到 70 年代 自然语言 处理 主要 采用 基于 规则 的 
方法 70 年代 以后 随着 互联网 的 高速 发展 自然语言 
处理 思潮 由 理性主义 向 经验主义 过渡 基于 统计 的 
方法 逐渐 代替 了 基于 规则 的 方法 从 2008年 
到现在 在 图像 识别 和 语音 识别 领域 的 成果 
激励 下 人们 也 逐渐 开始 引入 深度 学习 来做 
自然语言 处理 研究 由 最初 的 词 向 量到 2013年 
word2vec 将 深度 学习 与 自然 语言 处理 的 结合 
推向 了 高潮 并在 机器翻译 问答 系统 阅读 理解 等 
领域 取得 了 一定 成功 接下来 AMiner 将为 大家 介绍 
自然语言 处理 的 业界 发展 涵盖 了 以下 企业 微软 
亚洲 研究院 微软 亚洲 研究院 1998年 成立 自然语言 计算 组 
研究 内容 包括 多 国 语言 文本 分析 机器翻译 跨语言 
信息检索 和 自动 问答 系统 等 这些 研究 项目 研发 
了 一系列 实用 成果 如 IME Input Method Editors 输入法 
编辑器 它 是 一种 专门 的 应用 程序 用来 输入 
代表 东亚地区 书面 语言 文字 的 不同 字符 对联 游戏 
Bing 词典 Bing 翻译器 语音 翻译 搜索引擎 等 为 微软 
产品 做出 了 重大 的 贡献 微软 IME 微软 对联 
游戏 微软 必应 词典 并且 在 自然 语言 处理 顶级 
会议 例如 ACL COLING 等 会议 上 发表 了 许多 
论文 语音 翻译 2017年 微软 在 语音 翻译 上 全面 
采用 了 神经 网络 机器翻译 并 新 扩展 了 Microsoft 
Translator Live Feature 可以 在 演讲 和 开会 时 实时 
同步 在 手机 端 和 桌面 端 同时 把 讲话者 
的话 翻译 成 多种 语言 其中 最 重要 的 技术 
是 对于 源语言 的 编码 以及 引进 的 语言 知识 
同时 微软 还 表示 将来 要将 知识图谱 纳入 神经网络 机器翻译 
中 规划 语言 理解 的 过程 中 人机对话 小娜 现在 
已经 拥有 超过 1.4亿 用户 在 数以十亿计 的 设备 上 
与 人们 进行 交流 并且 覆盖 了 十几 种 语言 
有 聊天 机器人 小冰 正在 试图 把 各国 语言 的 
知识 融合 在 一起 实现 一个 开放 语言 自由 聊天 
的 过程 目前 小冰 实现 了 中文 日文 和 英文 
的 覆盖 有上 亿 用户 GoogleGoogle 是 最早 开始 研究 
自然语言 处理 技术 的 团队 之一 作为 一个 以 搜索 
为 核心 的 公司 Google 对 自然 语言 处理 更为 
重视 Google 拥有 着 海量 数据 可以 搭建 丰富 庞大 
的 数据库 可以为 其 研究 提供 强大 的 数据 支撑 
Google 对 自然 语言 处理 的 研究 侧重于 应用 规模 
跨语言 和跨/nr 领域 的 算法 机器翻译 知识图谱 Google 的 知识图谱 
更是 遥遥领先 例如 自动 挖掘 新 知识 的 准确 程度 
文本 中 命名 实体 的 识别 纯 文本 搜索 词条 
到 在 知识 图 谱上 的 结构化 搜索 词条 的 
转换 等 效果/n 都/d 领先/n 于/p 其他/r 公司/n 而且 很多 
技术 都 实现 了 产品化 语音识别/i Google/w 一直/d 致力/n 于/p 
投资/vn 语音/n 搜索/v 技术/n 和/c 苹果/n 公司/n 的/uj siri/w 竞争/vn 
自 2012年 以来 将 神经 网络 应用于 这一 领域 使 
语音识别 错误率 极大 降低 2011年 收购 语言 信息 平台 SayNow 
把 语音 通信 点对点 对话 以及 群组 通话 和 社交 
应用 融合 在 一起 2014年 收购 了 SR Tech Group 
的 多项 语音识别 相关 专利 FacebookFacebook 涉猎 自然语言 处理 较晚 
2013年 开始 发展 语音 翻译 2015年 开始 语音 识别 的 
研发 之路 语音 翻译 发展 道路 如下 图 所示 语音识别 
2015年 Facebook 相继 建立 语音 识别 和 对话 理解 工具 
开始 了 语音 识别 的 研发 之路 2016年 Facebook 开发 
了 一个 响应 Hey Oculus 的 语音 识别 系统 并在 
2018年 初开 发了 wav2letter 这 是 一个 简单 高效 的 
端 到 端 自动 语音 识别 ASR 系统 百度 百度 
自然语言 处理 部 是 百度 最早 成立 的 部门 之一 
研究 涉及 以下 方面 百度 在 深度 问答 方向 经过 
多年 打磨 积累 了 问句 理解 答案 抽取 观点 分析 
与 聚合 等 方面 的 一整套 技术 方案 目前 已经 
在 搜索 度 秘 等 多个 产品 中 实现 应用 
百度 翻译 目前 支持 全球 28种 语言 覆盖 756个 翻译 
方向 支持 文本 语音 图像 等 翻译 功能 并 提供 
精准 人工 翻译 服务 满足 不同 场景 下 的 翻译 
需求 发布 了 世界 上 首个 线上 神经网络 翻译 系统 
并 获得 2015 年度 国家 科技 进步奖 阿里巴巴 阿里 自然语言 
处理 为 其 产品 服务 在 电商 平台 中 构建 
知识图谱 实现 智能 导购 同时 进行 全网 用户 兴趣 挖掘 
在 客服 场景 中 也 运用 自然语言 处理 技术 打造 
机器人 客服 例如 蚂蚁 金融 智能 小宝 淘宝 卖家 的 
辅助 工具 千牛 插件 等 同时 进行 语音 识别 以及 
后续 分析 阿里 的 机器 翻译 主要 与其 国家化 电商 
的 规划 相 联系 2017 年初 阿里 正式 上线 了 
自主 开发 的 神经 网络 翻译 系统 进一步 提升 了 
其 翻译 质量 腾讯 AI Lab 是 腾讯 的 人工智能 
实验室 研究 领域 包括 计算机 视觉 语音识别 自然语言 处理 机器学习 
等 其 研发 的 腾讯 文智 自然语言 处理 基于 并行计算 
分布式 爬虫 系统 结合 独特 的 语义分析 技术 可满足 自然语言 
处理 转码 抽取 数据 抓取 等 需求 在 机器 翻译 
方面 2017年 腾讯 宣布 翻译 君 上线 同声 传译 新功能 
用户/n 边/d 说/v 边翻的/nr 需求/v 得到/v 满足/v 语音识别/i +/i NMT/w 
等/u 技术/n 的/uj 应用/v 保证/v 了/ul 边说边/nr 翻/v 的/uj 速度/n 
与/p 精准性/i 京东 京东 在 人工智能 的 浪潮 中 也 
不甘落后 京东 AI 开放平台 基本上 由 模型 定制 化 平台 
和 在线 服务 模块 构成 其中 在线 服务 模块 包括 
计算机 视觉 语音 交互 自然语言 处理 和 机器 学习 等 
按照 京东 的 规划 NeuHub 平台 将 作为 普惠 性 
开放平台 不同 角色 均可 找到 适合 自己 的 场景 例如 
用 简单 代码 即可 实现 对 图像 质量 的 分析 
评估 从 业务 上 说 平台 可以 支撑 科研 人员 
算法 工程师 不断 设计 新的 AI 能力 以 满足 用户 
需求 并 深耕 电商 供应链 物流 金融 广告 等 多个 
领域 应用 探索 试验 医疗 扶贫 政务 养老 教育 文化 
体育 等 多 领域 应用 聚焦/v 于新/nr 技术/n 和/c 行业/n 
趋势/n 研究/vn 孵化 行业 最新 落地 项目 科大 讯 飞 
科大 讯 飞 股份 有限公司 成立 于 1999年 是 一家 
专业 从事 智能 语音 及 语言 技术 人工智能 技术 研究 
软件 及 芯片 产品开发 语音 信息 服务 及 电子 政务 
系统 集成 的 国家级 骨干 软件 企业 科大 讯 飞 
作为 中国 智能 语音 与 人工智能 产业 领导者 在 语音 
合成 语音识别 口语 评测 自然语言 处理 等 多项 技术 上 
拥有 国际 领先 的 成果 科大 讯 飞 成立 之时 
就 开始 在 语言 和 翻译 领域 布局 项目 基于 
深度 神经网络 算 法上 的 创新 和 突破 在 翻译 
方面 的 发展 如下 图 所示 关于 AMiner 以 科研 
人员 为 中心 提供 在线 实时 的 人才 科技 评估 
报告 的 情报 追踪 关注 人工智能 + 20 领域 的 
发展 动态 点击 阅读 原文 下载 自然语言 处理 研究报告 完整版 
加入 社群 吧 网易 智能 AI 社群 AI 专家 群 
AI 黑板报 火热 招募 中 对 AI 感兴趣 的 小伙伴 
添加 智能 菌 微信 kaiwu _ club 说明 身份 即可 
加入 第 34 集 机器学习 与 人工智能 01 23 分类 
Classification01 25 分类器 Classifier01 34 特征 Feature02 03 标记 数据 
Labeled data02 38 决策 边界 Decision boundaries03 00 混淆 矩阵 
Confusion matrix03 39 未 标签 数据 Unlabeled data03 49 决策树 
Decision tree04 25 支持 向量 机 Support Vector Machines05 52 
人工神经网络 Artificial Neural Network08 34 深度 学习 Deep learning09 21 
弱 AI 窄 AI Weak AI Narrow AI09 43 强 
AI Strong AI10 42 强化 学习 Reinforcement Learning 分类器 做 
分类 的 算法 叫做 分类器 特征 是 用来 帮助 分类 
的 值 标记 数据 不仅 要 记录 特征值 还要 记录 
种类 机器学习 算法 的 目的 是 最大化 正确 分类 + 
最小化 错误 分类 决策树 生成 决策树 的 机器学习 算法 多个 
决策树 组成 的 算法 叫 决策 森林 支持 向量 机 
本质上 是 用 任意 线段 来 切分 决策空间 而且 线段 
不一定 是 直线 可以 是 多项式 或 其他 数学 函数 
人工神经网络 决策树 和 支持 向量 机 这样 的 技术 发 
源自 统计学 但 也有 不用 统计学 的 算法 比如 人工神经网络 
神经元 常见 处理 流程 加权 求和 偏置 加 或 减 
一个 固定值 激活 函数 激活 函数 也叫 传递函数 应用 与 
输出 对 结果 执行 最后 一 次数 学修 改做 神经 
网络 时 这些 偏差 和 权重 一 开始 会 设计 
成 随机 值 然后 将 最后 算出 的 结果 跟 
样本数据 进行 对比 不断 调整 和 训练 直到 获得 让 
结果 最 接近 真实 数据 的   偏 差值 和 
权重 值 输入 层 主要 用于 样本数据 输入 隐藏 层 
可以 有 很多 层 用于 对 数据 进行 加权 求和 
等 各种 处理 输出 层 输出 最后 处理 的 结果 
弱 AI 只能 做 特定 任务 强 AI 像人 一样 
聪明 的 AI 强化 学习 通过 反复 试错 自己 发现 
成功 的 策略 人工智能 的 处理 逻辑 个人 总结 的 
认识 仅供参考 1 . 有 庞大 的 样本数据 正确性 真实性 
2 . 根据 推测 的 关联 因素 作为 数据 特征 
3 . 然后 利用 神经网络 算法 得出 的 结果 与 
样本数据 进行 对比 得到 最优 算法 的 各种 值 如 
某个 特征 的 权重 4 . 然后 输入 真实 的 
数据 根据 最优 算法 进行 事实 的 提前 预测 第 
35 集 计算机 视觉 02 41 检测 垂直 边缘 的 
算法 03 26 核 / 过滤器 kernel or filter03 56 
卷积 convolution04 23 Prewitt 算子 Prewitt Operators05 34 维奥拉 琼斯 
人脸 检测 Viola Jones Face Detection05 35 卷积 神经网络 Convolutional 
Neural Networks07 33 识别 出 脸 之后 可以 进一步 用 
其他 算法 定位 面部 标志 如 眼睛 和 眉毛 具体位置 
从而 判断 心情 等 信息 08 52 跟踪 全身 的 
标记 点 如 肩部 手臂 等 RGB 三原色 颜色 跟踪 
算法 最 简单 的 计算机 视觉 算法 跟踪 一个 颜色 
物体 比如 一个 白色 的 球 1 . 颜色 跟踪 
算法 是 一个 个 像素 搜索 因为 颜色 是 在 
一个 像 素里 首先 记下 球 的 颜色 保存 最 
中心 像素 的 RGB 值 然后 让 程序 在 图像 
中 找 最 接近 这个 颜色 的 像素 可以 在 
视频 的 每 一帧 图片 跑 这个 算法 跟踪 球 
的 位置 卷积 算法 核     2个 边缘增强 的 
核 卷积 神经网络 第 36 集 自然语言 处理 01 50 
词性 Parts of speech02 15 短语 结构 规则 Phrase structure 
rules02 32 分析树 Parse tree05 30 语音识别 Speech recognition07 26 
谱图 Spectrogram07 44 快速 傅立叶 变换 Fast Fourier Transform08 42 
音素 Phonemes09 29 语音合成 Speech SynthesisNLP   自然语言 处理 Natural 
  Language   Processing 快速 傅利叶 变换 FFT 一种 将 
声音 的 波形 转换成 频率 图形 的 算法 语音识别 声音 
频率 的 共振 峰 特征 音素 识别 组成 单词 识别 
句首 和句尾/nr 语言 模型 修正 口 音和 发音 错误 转换成 
文字 语音合成 技术 人机交互 正向 反馈 我 之前 一直 觉得 
自然语言 处理 应该 是 一个 很 高深 很难 去 学习 
的 东西 如果 我们 不 去 学习 人工智能 我们 是 
绝对 用不上 它 的 然而 我 上 了 我们 的 
选修课 了解 了 它 之后 我 才 知道 我 错了 
我 真的 错了 学习 之前 我 并不 怎么 了解 什么 
是 自然 语言 处理 之后 我 才 懂 原来 自然语言 
处理 Natural Language Processing 简称 NLP 就是 用 计算机 来 
处理 理解 以及 运用 人类 语言 如 中文 英文 等 
它 属于 人工智能 的 一个 分支 是 计算机 科学 与 
语言学 的 交叉 学科 又 常 被称为 计算 语言学 由于 
自然 语言 是 人类 区别于 其他 动物 的 根本 标志 
没有 语言 人类 的 思维 也 就 无从谈起 所以 自然 
语言 处理 体现 了 人工智能 的 最高 任务 与 境界 
也 就是说 只有 当 计算机 具备 了 处理 自然 语言 
的 能力 时 机器 才算 实现 了 真正 的 智能 
我 个人 认为 对于 我们 信息 管理 与 信息 系统 
专业 来说 在 庞大 的 数据 流 里面 来 找到 
我们 需要 的 数据 是 我们 专业 的 一大 难点 
但是 如果 我们 结合 大 数据 和 这个 自然语言 这个 
过程 将 会 变得 无比 简单 和 精确 我们 的 
学习 也 会 变得 十分 容易 我们 信息 管理 与 
信息 系统 专业 本来 就要 与 计算机 经常 打交道 所以 
自然 语言 处理 也 就 似乎 成为 了 我们 所 
需要 掌管 的 知识 但是 我们 也 面临 着 一些 
严峻 的 事实 自然语言 处理 这项 技术 虽然 在 进行 
着 突飞猛进 的 发展 但是 却 并不 是 一帆风顺 它 
有过 低谷 也 有过 高潮 现在 我们 正 面临 着 
新的 挑战 和 机遇 不论是 探究 自然 本质 还是 付诸 
实际 应用 自然语言/l 处理/v 在/p 将来/t 必定会/l 有/v 令人/l 期待/v 
的/uj 惊喜/a 和/c 异常/d 快速/d 的/uj 发展/vn 现在 的 我们 
正处 黄金 年龄 风华正茂 唯有 努力学习 方能 不付 我们 璀璨 
的 年华 自然语言 处理 NLP 中 一个 很 重要 的 
研究 方向 就是 语义 的 情感 分析 Sentiment Analysis 例如 
IMDB 上 有 很多 关于 电影 的 评论 那么 我们 
就 可以 通过 Sentiment Analysis 来 评估 某部 电影 的 
口碑 如果 它 才 刚刚 上映 的话 甚至 还 可以 
据此 预测 它 是否 能够 卖座 与此 相 类似 国内 
的 豆瓣 上 也 有 很多 对 影视 作品 或者 
书籍 的 评论 内容 亦 可以 作为 情感 分析 的 
语料库 对于 那些 电子 商务 网站 而言 针对 某 一件 
商品 我们 也 可以 看到 留言 区里 为数众多 的 评价 
内容 那么 同类 商品 中 哪个 产品 最受 消费者 喜爱 
呢 或许 对 商品 评论 的 情感 分析 可以 告诉 
我们 答案 在 之前 的 文章 中 笔者 已经 向 
读者 介绍 了 在 Python 中 利用 NLTK 进行 自然语言 
处理 的 一些 基本 方法 利用 NLTK 在 Python 下 
进行 自然语言 处理 Python 自然语言 处理 词干 词形 与 MaxMatch 
算法 同时 我 也 介绍 了 在 Python 中 利用 
Scikit Learn 进行 机器学习 尤其 是 是 利用 L o 
g i s t i c R e g r 
e s s i o n 进行 分类 的 基本 
方法 Python 机器学习 之 Logistic 回归 下面 本文 将 在 
这些 文章 的 基础 之上 尝试 将 机器学习 和 自然 
语言 处理 结合起来 以 Tweet 文 为例 演示 进行 Sentiment 
Analysis 的 基本 方法 首先 需要 说明 的 是 内容 
有 三点 1 下面 的 例子 仍然 主要 使用 Python 
中 NLTK 和 Scikit Learn 两个 函数库 2 SemEval 是 
NLP 领域 的 带有 竞赛 性质 的 年度 盛会 类似 
KDD Cup SemEval 创始 于 1998年 今年 2016 的 活动 
主页 为 http / / alt . qcri . org 
/ semeval2016 /   下面 程序 中 所 使用 的 
数据 即 来自   SemEval 2016 的 Task 当然 在 
使用 时 我们 已经 完成 了 基本 的 预 处理过程 
而这 并非 本文 的 重点 我们 略去 不表 3 我们 
所 演示 的 方法 主要 目的 在于 帮助 大家 熟悉 
Sentiment Analysis 的 基本 内容 深化 Scikit Learn 函数库 的 
使用 而且 我们 所 分析 的 数据 来自 于 实际 
数据集 而非 模拟 数据集 所以 最终 的 分析 结果 并不 
保证 得 到 非常 高的/nr 准确率 要 得到 更高 的 
准确率 需要 在 模型 构建 和 特征 选择 上 做 
更深 层次 的 思考 而 这些 思考 已经 超出 本 
博文 所 讨论 的 范围 我们 原始 的 数据 是 
一条 一条 的 Tweet 例如 Top 5 most searched for 
Back to School topics the list may surprise you http 
/ / t . co / Xj21uMVo0p   @ bing 
@ MSFTnews # backtoschool @ Microsoft @ taehongmin1 We have 
an IOT workshop by @ Microsoft at 11PM on the 
Friday definitely worth going for inspiration # HackThePlanet 当然 我们 
同时 还 拥有 一个 list of labels 即对 每条 Tweet 
的 Polarity 进行 评定 的 标签 其中 + 1 表示 
positive 1 表示 negative 0 表示 neutral 在 预处理 阶段 
我 对 每条 Tweet 进行 了 分句 和 分词 然后 
1 剔除 了 @ * * * 这样 的 内容 
2 对于 # 引导 的 Topic 我们 将 其 视为 
一个 独立 的 句子 进行 处理 3 删 除了 由 
http 引导 的 网络地址 4 统一 了 大小写 所以 上述 
两个 Tweet 处理 之后 将 得到 下面 两个 结果 top 
5 most searched for back to school topics the list 
may surprise you . back to school . we have 
an iot workshop by at 11pm on the friday definitely 
worth going for inspiration . hack the planet . 然后 
我们 根据 训练 数据集 创建 一个 词 袋 BOW bag 
of word 这个词 袋 是 一个 字典 里面 存储 着 
所有 训练 数据 集中 出现 过 的 词汇 以及 它们 
在 全文 中 出现 的 频数 这样 做 的 目的 
在于 我们 期望 剔除 那些 在 全部 训练 数据 集中 
极少 出现 的 词汇 生僻 词 以及 那些 频繁 出现 
但 毫无 意义 的 词汇 通常 我们 称之为 停 词 
stop words 例如 the of a 等 在 BOW 基础 
之上 接下来 就 可以 为 每条 Tweet 创建 创建 feature 
dictionaries 了 特征 字典 是 指 每条 Tweet 中 出现 
在 BOW 中的 词 即 剔除 了 罕见 的 生僻 
词 和停词/nr 以及 它们 在 该条 Tweet 中 出现 的 
频数 构成 的 字典 { 2 1 . 2 5 
1 back 2 list 1 may 1 school 2 searched 
1 surprise 1 top 1 topics 1 } { 1 
1   . 2 11pm 1 definitely 1 friday 1 
going 1 hack 1 inspiration 1 iot 1 planet 1 
workshop 1 worth 1 } 到此为止 所有 的 预处理 工作 
都 已经 完成 了 我们 得到 了 一个 list of 
dicts 形式 的 训练 数据集 以及 它 对应 的 list 
of labels 和 一个 list of dicts 形式 的 测试 
数据集 以及 它 对应 的 list of labels 但是 现在 
问题 来了 这种 形式 的 数据 显然 不能 被 直接 
使用 回忆 一下 我们 在 前 篇 介绍 Logistic Regression 
的 文章 中 所 使用 的 鸢尾花 数据集 的 样子 
便 不难 发现 与 当前 我们 所 拥有 的 数据 
形式 大相径庭 这时 就要 借助于 Scikit Learn 中 提供 的 
特征提取 Feature Extraction 模块 The sklearn . feature _ extraction 
module can be used to extract features in a format 
supported by machine learning algorithms from datasets consisting of formats 
such as text and image . 更 直接 的 说 
我们 将有 借助 的 函数 是 DictVectorizer The class DictVectorizer 
can be used to convert feature arrays represented as lists 
of standard Python dict objects to the NumPy / SciPy 
representation used by scikit learn estimators . 如果 你 对 
Scikit Learn 文档 中 的 这些 描述 感到 困惑 那么 
下面 的 例子 将 让 你 很容易 理解 其 作用 
首先 我们 给出 它 的 定义 原型 class sklearn . 
feature _ extraction . DictVectorizer dtype = class numpy . 
float64 separator = = sparse = True sort = True 
其中 sparse 是 一个 布尔 类型 的 参数 用于 指示 
是否 将 结果 转换成 scipy . sparse matrices 即 稀疏 
矩阵 缺省 情况 下 其 赋值 为 True 来看 一个 
例子 measurements 是 一个 list of dicts 我们 把 它 
转化 成 矩阵 表示 当 对应 位置 出现 某 个 
城市 名 时 其 对应 行 的 那 一列 就被 
置 为 1 否则 就是 0 from sklearn . feature 
_ extraction import DictVectorizer measurements = { city Dubai temperature 
33 . } { city London temperature 12 . } 
{ city San Fransisco temperature 18 . } vec = 
DictVectorizer vec . fit _ transform measurements . toarray array 
1 . 0 . 0 . 33 . 0 . 
1 . 0 . 12 . 0 . 0 . 
1 . 18 . vec . get _ feature _ 
names city = Dubai city = London city = San 
Fransisco temperature 再来一个 补充 例子 measurements = { city = 
Dubai True city = London True temperature 33 . } 
{ city = London True city = San Fransisco True 
temperature 12 . } { city San Fransisco temperature 18 
. } vec . fit _ transform measurements . toarray 
array 1 . 1 . 0 . 33 . 0 
. 1 . 1 . 12 . 0 . 0 
. 1 . 18 . 另外 的 一个 常见 问题 
是 训练 数据集 和 测试 数据集 的 字典 大小 不 
一致 此时 我们 希望 短 的 那个 能够 通过 补零 
的 方式 来 追 平长的/nr 那个 这时 就 需要 使用 
transform 还是 来看 例子 D = { foo 1 bar 
2 } { foo 3 baz 1 } v = 
DictVectorizer sparse = False X = v . fit _ 
transform D X array 2 . 0 . 1 . 
0 . 1 . 3 . v . transform { 
foo 4 unseen _ feature 3 } array 0 . 
0 . 4 . v . transform { foo 4 
} array 0 . 0 . 4 . 可见 当 
使用 transform 之后 后面 的 那个 总是 可以 实现 同 
前面 的 一个 相同 的 维度 当然 这种 追 平 
可以 是 补齐 也 可以 是 删减 所以 通常 我们 
都是用/nr 补齐 短 的 这样 的 方式 来 实现 维度 
一致 如果 你 不 使用 transform 而是 继续 fit _ 
transform 则会 得到 下面 的 结果 这 显然 不能 满足 
我们 的 要求 v . fit _ transform { foo 
4 unseen _ feature 3 } array 4 . 3 
. 有了/nr 这样 的 认识 下面 就 可以 为 我们 
后续 的 Logistic Regression 建立 稀疏 矩阵 了 代码 如下 
vec = DictVectorizer sparse _ matrix _ tra = vec 
. fit _ transform feature _ dicts _ tra sparse 
_ matrix _ dev = vec . transform feature _ 
dicts _ dev 当然 这里 你 还 可以 用 下面 
的 代码 来 测试 一下 他们 的 维度 是否 按 
我们 预想 的 那样 print sparse _ matrix _ dev 
. shape print sparse _ matrix _ tra . shape 
然后 我们 就 可以 利用 之前 用过 的 Logistic Regression 
来 建立 分类 模型 了 from sklearn import linear _ 
model logreg = linear _ model . L o g 
i s t i c R e g r e 
s s i o n C = 1 logreg . 
fit sparse _ matrix _ tra labels _ t prediction 
= logreg . predict sparse _ matrix _ dev print 
logreg print accuracy score print accuracy _ score labels _ 
d prediction print classification _ report labels _ d prediction 
一同 来看 一下 该 模型 对 测试 集 的 预测 
结果 L o g i s t i c R 
e g r e s s i o n C 
= 1 class _ weight = None dual = False 
fit _ intercept = True intercept _ scaling = 1 
max _ iter = 100 multi _ class = ovr 
n _ jobs = 1 penalty = l2 random _ 
state = None solver = liblinear tol = 0.0001 verbose 
= 0 warm _ start = False accuracy score 0.512848551121 
precision recall f1 score support 1 0.41 0.28 0.33 360 
0 0.46 0.69 0.55 700 1 0.68 0.46 0.55 769 
avg / total 0.54 0.51 0.51 1829 该 Sentiment 分类 
模型 的 准确率 为 51.28% 当然 正如 我们 前面 所说 
这个 模型 显然 还 有 很大 的 改进 空间 你 
完全 可以 通过 引入 新的 feature 或者 使用 其他 机器学习 
模型 或者 调整 模型 参数 等 多种 途径 来 提升 
模型 的 准确率 但是 本文 旨在 演示 NLP 中的 Sentiment 
Analysis 的 基本 步骤 和 策略 以及 进一步 演示 利用 
Scikit Learn 进行 机器 学习 的 更 广泛 的 方法 
例如 基于 字典 的 特征 提取 和 引入 稀疏 矩阵 
等 方面 的 初衷 已经 完成 了 有兴趣 的 读者 
完全 可以 在 此 基础 上 继续 进行 模型 优化 
以期 实现 更 准确 的 分类 能力 目录 文章 目录 
目录 前言 问答 系统 基础 一 问答 系统 基础 二 
问答 系统 术语 问答 系统 基础 三 问答 系统 基础 
四 问答 系统 基础 五 前言 硕士 生涯 结束 开始 
专心 做 一件 自己 觉得 有用 的 工具 先 做 
工程 后搞/nr 理论 自然语言 处理 是 一个 非常 难 的 
问题 同时 是 人工智能 皇冠 上 的 明珠 接下来 会 
记录 一 系列 自然语言 处理 的 笔记 来自 于 哈工大 
老师 关毅 问答 系统 基础 一 IBM 这个人 蓝色 巨人 
硅谷 海盗 Watson 对话 系统 doing 老师 自我 吐槽 扯淡 
时间 多于 工作 的 时间 问答 系统 基本 概念 问答 
的 简史 问答 的 所有 问题 1 机器翻译 2 语音识别 
3 数据库 自然语言 控制 机器人 动作 积木 世界 LUNAR 系统 
扩充 知识 转移 网络 进行 句法分析 LIFER 系统 CHAT 80 
系统 START 系统 MURAX 系统 AskFeeres 系统 人肉 高科技 人工 
恢复 问题 十万篇 相关 文档 分析 相关 问题 答案 未 
实现 问答 系统 基础 二 在 各个 步骤 建立 统计 
分类 模型 问答式 信息检索 会议 检索 评测 技术 平台 check 
1 问句 处理 2 海量 答案 对应 3 事实性 陈述 
即 可解决 问答 系统 术语 问题 类型 question type 答案 
类型 answer type 问句 焦点 question focus 问句 主题 question 
topic 候选 段落 candidate passage 候选 答案 candidate answer 答案 
所属 类别 最 型 问题 观点 问题 因果 类 问题 
事实 类 问题 问句 对应 的 目标 类型 问答 系统 
基础 三 2008年 认识/v 到/v 自己/r 目前/t 做/v 的/uj 和/c 
想做/i 的/uj 距离/n 有/v 多大/i 焦点 实体 的 属性 主题 
讨论 的 实体 候选 段落 由 搜索引擎 响应 用户 问句 
而 检索 得到 的 文本 片段 候选 答案 可能 的 
答案 { 1 找到 候选 段落 2 与 问句 匹配 
并 检查 段落 的 语义 3 抽取 答案 } { 
1 相似 段落 2 语义 匹 配法 3 语法 匹 
配法 } 智能化 信息检索 结构图 语法 语义 专业 知识库 元 
搜索 满足 确切 的 未知 的 点 问答 系统 基础 
四 网页 重复 太多 一些 搜索引擎 死了 百度 活 下来 
了 工程 问题 细节 超 多 这 才是 你 的 
生命线 数据库 索引 技术 PAT 树 B + 树 哈希 
树 My SQL 智能化 信息 检索 模型 { 布尔 向量空间 
概率模型 } 基于 结构 映射 理论 的 新型 信息 检索 
模型 系统 相似 模型 向量空间 模型 的 本源 理论模型 通用性 
理论模型 自然语言 处理 技术 各个 技术 的 综合 完善 自己 
的 理论 做 应用 课题 原创 理论 时代 复杂性 精度 
是 冲突 的 问答式 基础理论 问答 系统 基础 五 项目 
研究 进展 报告 提纲 1 开放 域 问答 系统 概要 
设计 2 工作进展 与 展望 3 主要 阶段性 成果 4 
总结 用户 层 用户 交互 语言 分析 识别 个性化 信息 
最终 肯定 要 个性 化发展 机器学习 ranking 结果 NLP + 
NLG 三层 体系 四层 系统 语义层 用户 层 强化 学习 
技术 转载 请 标明 出处 http / / blog . 
csdn . net / DJY1992 / article / details / 
74979436 本文 出自 奥特曼 超人 的 博客 AI Artificial Intelligence 
即 人工智能 人工智能 领域 的 研究 包括 机器人 语音识别 图像识别 
自然语言 处理 和 专家 系统 等 人工智能 从 诞生 以来 
理论 和 技术 日益 成熟 应用 领域 也 不断 扩大 
可以 设想 未来 人工智能 带来 的 科技 产品 将会 是 
人类 智慧 的 容器 可观 看上 篇文章 CSDN 观点 人工智能 
会 不会 取代 开发 它 的 人 所谓 的 人工智能 
是 可以 对人 的 意识 思维 的 信息 过程 的 
模拟 Q 你 会为 AI 转型 学习 代价 有 多大 
我 想 大部分 不会 All In 毕竟/d 每个人/i 都/d 有/v 
家庭/n 有 消费 当然 也 有 自己 的 职业 生涯 
顾虑 职业 发展 稍有不慎 将 掉进 坑里 不说 其它 的 
光是 入行 所 学习 的 代价 就是 巨大 的 如果 
你 不是 在校 研究生 硕士生 我 想 学习 上 的 
优势 并 不是 很大 而且 不同 的 行业 所 涉及 
的 学科 都是/nr 不同 的 如 认知科学 数学 神经 生理学 
心理学 计算机科学 信息论 控制论 不 定性 论 仿生学 社会 结构学 
与 科学 发展观 也 就是说 如果 真的 从事 这 行业 
一个人 也 只能 单 方面 的 针对 某个 行业 的 
某个 领域 Q AI 人工智能 领域 的 状况 微软 小冰 
人机 围棋 大战 亦或 是 昨天 的 百度 AI 开发者 
大会 无一 不 代表 着 人工智能 的 火热 光是/n 百度/n 
养的/nr 那几/i 支/n 团队/n 耗费 都是 惊人 的 两个字 太贵 
Q AI 人工智能 的 实现 方法 用 什么 工具 或 
语言 我们 大部分 都是 用 传统 的 编程语言 如 JAVA 
\ C # \ C \ C + + \ 
ANDROID \ IOS 等 那 很多 人 不是 太 理解 
人工智能 要 怎么 操作 用 什么 语言 先 来 看看 
实现 方法 百度 上 已 给出 很好 的 解释 这里 
精简 一下 AI 在 计算机 上 实现 时有 2种 不同 
的 方式 一种 是 采用 传统 的 编程技术 使 系统 
呈现 智能 的 效果 而 不考虑 所用 方法 是否 与人 
或 动物 机体 所用 的 方法 相同 这种方法 叫 工程学 
方法 ENGINEERING APPROACH 它 已 在 一些 领域 内 作出 
了 成果 如 文字 识别 电脑 下棋 等 另一种 是 
模拟法 MODELING APPROACH 它 不仅 要 看 效果 还/d 要求/v 
实现/v 方法/n 也/d 和/c 人类/n 或/c 生物/n 机体/n 所用/b 的/uj 
方法/n 相同/d 或/c 相/v 类似/v 遗传算法 GENERIC ALGORITHM 简称 GA 
和 人工神经网络 ARTIFICIAL NEURAL NETWORK 简称 ANN 均属 后一/nr 类型 
为了 得到 相同 智能 效果 两种 方式 通常 都 可使用 
采用 前 一种 方法 需要 人工 详细 规定 程序逻辑 如果 
游戏 简单 还是 方便 的 如果 游戏 复杂 角色 数量 
和 活动 空间 增加 相应 的 逻辑 就会 很复杂 按指 
数式 增长 人工 编程 就 非常 繁琐 容易 出错 而 
一旦 出错 就必须 修改 原程序 重新 编译 调试 最后 为 
用户 提供 一个 新的 版本 或 提供 一个 新 补丁 
非常 麻烦 采用 后 一种 方法 时 编程 者 要为 
每一 角色 设计 一个 智能系统 一个 模块 来 进行 控制 
这个 智能系统 模块 开始 什么 也 不懂 就像 初 生婴儿 
那样 但 它 能够 学习 能 渐渐 地 适应 环境 
应付 各种 复杂 情况 这种 系统 开始 也 常 犯错误 
但 它 能 吸取 教训 下一次 运行时 就可能 改正 至少 
不会 永远 错 下去 用不到 发布 新 版本 或 打补丁 
利用 这种 方法 来 实现 人工智能 要求 编程 者 具有 
生物学 的 思考 方法 入门 难度 大 一点 但 一旦 
入了 门 就可 得到 广泛 应用 由于 这种 方法 编程 
时 无须 对 角色 的 活动 规律 做 详细 规定 
应用于 复杂问题 通常 会比 前一种 方法 更 省力 至于 用 
什么 工具 和 语言 就看 个人 了 学习 的 前期 
是 你 数学 Very good 反正 博主 我 是 不行 
了 Q 个人 对 AI 远景 的 看法 人工智能 是 
一门 边沿 学科 也是 一门 交叉学科 虽然 说 冲击 着 
某些 行业 如 无人 驾驶 汽车 等 但/c 大部分/m 大多数/m 
的/uj 实现/v 方式/n 都是/nr 基于/p 大/a 数据/n 和/c 模拟/v 预测/vn 
的/uj 方式/n 至 目前 而言 研究 院内 还 无法 模拟出 
机器 的 专属 情感 与 自我 意识 当然 这 就是 
为何 未来 还是 需要 编程人员 因为 无法 自主 实现 还有 
一点 就是 基因 遗传学 这里 所谓 的 基因 遗传学 并 
不是 类似 人类 的 DNA 而是 RB 的 自主 创造性 
思维 能力 到 目前 为止 计算机 或者 人工智能 的 智能 
水平 在 不断 提高 但是 人工智能 的 意识 还是 零 
换句话说 我们 现在 没有 任何 指标 证明 或者 任何 迹象 
表明 计算机 和 人工智能 在 未来 能 获得 这种 意识 
其实在 这 一块 的 技术 发展 上 我们 还是 属于 
慢动作 的 期待 未来 的 科技 能更/nr 智能化 Q 建议 
大家 去 学习 这 一块 要看 个人 兴趣 了 上面 
也 提及 了 这块 的 学习 成本 如果 有 兴趣 
的 去 了解 下 也 无妨 如果 基础 不是 太 
扎实 的 想 要 深入 的 建议 先去 补下 数学 
方面 还有 相关 的 算法 知识 如果 基础 不是太好 又想 
往 这 方面 的 职业 生涯 走 恐怕 要吃 不少 
苦头 还 不如 在 自己 的 专属 下 深入 如果 
基础 底子 不错 的 也不 建议 All In 可以 抽 
80% 的 精力 去 学习 保留 20% 作为 后路 | 
| 版权 声明 本文 为 博主 杜 锦阳 原创 文章 
转载 请 注明 出处 人生 第一 次 面 BAT 记录 
一下 为 明年 找 工作 赞 经验 毕竟 网上 很少 
自然语言 处理 相关 的 面经 先 来点 干货 面试 流程 
1 . 自我介绍 2 . 谈谈 项目 3 . 上 
黑板 写 代码 按照 上述 流程 分为 三块 如下 简单 
的 自我 介绍 了 一下 我 我 做 的 项目 
是 关于 自然 语言 生成 面试官 1 为什么 不用 生成式 
的 方法 来做 呢 我 我 尝试 了 char rnn 
和 seq2seq 的 方法 面试官 1 rnn 是 怎么 运行 
的 你 能说 一下 吗 我 巴拉巴拉 面试官 1 什么 
是 char rnn 你 能画/nr 一下 rnn 的 结构图 吗 
我 OK 开始 画 画画 画 完 解释 了 一通 
面试官 2 那/r 如果/c 是/v seq2seq/i 的/uj 结构/n 是/v 什么样/r 
的/uj 你/r 能画/nr 一下/m 吗/y 我 OK 继续 画 此时 
我 给 自己 挖 了 一个 坑 这是 不加 attention 
的 seq2seq 加上 attention encoder 后的/nr 编码 向量 就 不是 
固定 的 了 此刻 的 我 觉得 自己 回答 的 
还 蛮好 面试官 1 那你能/nr 画/n 一下/m attention/w 的/uj 结构/n 
吗/y 是 怎么做 的 我 我 有点 忘记了 能 翻下 
笔记本 吗 思考 了 一下 大脑 一片 空白 想想 前 
两周 每天 都在/nr 研究 attention 啊 年纪 大了 记性 不好 
出来 翻了 下 笔记 这么 熟悉 的 attention 啊 那一刻 
怎么 就 不 记得 了 面试官 1 没事 那来 道 
代码 题 吧 面试官 2 出题 一个 struct 含有 start 
end 属性 现在 有 一个 这样 结构 的 数据 集 
统计/v 这个/r 数据/n 集中/v 具有/v 相同/d start/w 和/c end/w 的/uj 
数据/n 有/v 多少/m 个/q 简化 一下 就是 统计 { 0 
2 0 2 2 4 } 中 0 2 2 
4 分别 有 多少 个 解题 思路 输入 格式 为 
一个 存放 string 的 listdata = 0 2 2 4 
3 6 0 2 2 4 2 4 ret = 
dict for d in data if d in ret ret 
d = ret d + 1 else ret d = 
1 for d in ret print d + + str 
ret d 当时 跟 面试官 应该 没 沟通 彻底 隐约 
感觉 他 想要 的 输入 是 struct 类型 然后 就 
结束 了 基于 本次 面试 未来 需要 做 的 准备 
1 准备 一个 完美 的 自我 介绍 2 刷 LeetCode3 
熟记 主流 框架 公式 4 项目 深入 了解 摘要 NLP 
概述 主要 参考 自然语言 处理 NLP 知识结构 总结 和 知乎 
上 的 一些 问答 目录 NLP 界 神 级 人物 
NLP 知识结构 1 . 概述 2 . 形式语言 与 自动机 
3 . 语言 模型 4 . 概率 图 模型 生成 
模型 与 判别 模型 贝叶斯 网络 马尔科夫 链 与 隐 
马尔科夫 模型 HMM 5 . 马尔科夫 网 最大熵 模型 条件 
随 机场 CRF 6 . 命名 实体 识别 词性 标注 
内容 挖掘 语义分析 与 篇章 分析 大量 用到 前面 的 
算法 7 . 句法分析 8 . 文本 分类 情感 分析 
9 . 信息检索 搜索引擎 及其 原理 10 . 自动 文摘 
与 信息 抽取 机器翻译 问答 系统 11 . 深度 学习 
在 自然 语言 中 的 应用 NLP 用于 征信 参考资料 
NLP 界 神 级 人物 Michael Colins CU Jason Eisner 
JHU David Yarowsky JHU Chris Manning Dekang Lin 奇点 机智 
Michael Colins 英国人 哥伦比亚 大学 教授 研究 领域 包括 parse 
re ranking tree kernels semi supervised learning machine translation 和 
exponentiated gradient algorithms collins / eisner 对 NLP 结构 学习 
领域 贡献 极大 研究 parsing 并 一战 成名 http / 
/ www . cs . columbia . edu / ~ 
mcollins / 非常 喜欢 Michael Collins 认为 他 写 的 
paper 看得 最 舒服 最爽 犹如 沐浴 于 樱花 之中 
Jason Eisner 确实 是 厉害 不过 看 他 paper 实在 
太难 看懂 写 的 语言 非常 抽象 我等 屌丝 实在 
难以 深入 理解 经过 Collins 大侠 的 一些 paper 才能 
对 Eisner 的 paper 妙语 进行 理解 https / / 
www . zhihu . com / question / 32318281Jason Eisner 
JHU 约翰 霍普金斯大学 http / / www . cs . 
jhu . edu / ~ jason / David Yarowsky JHU 
yarowsky 早年 研究 词义 消 歧 是 著名 的 yarowsky 
algorithm 的 作者 后来 做 了 很多 跨语言 学习 的 
开创性 工作 http / / www . cs . jhu 
. edu / ~ yarowsky / Stanford NLP 掌门 Chris 
Manning 以 统计 自然语言 处理 基础 一 书 以及 Stanford 
NLP toolkit 而 闻名 Dan Jurafsky 著有 语音 与 语言 
处理 一 书 具有 深厚 的 语言 学 背景 稍微 
提 一下 Manning 的 学生 Richard Socher 近几年 声名鹊起 在 
dl4nlp 领域 风头 一时无两 属 年轻一代 翘楚 Dekang Lin 林 
德康 老师 前 Google 高级 管理 科学家 senior staff research 
scientist 在 加入 Google 之前 是 加拿大 Alberta 大学 计算机 
教授 发表 过逾 90篇 论文 被 引用 超过 12000次 目前 
做 了 一家 NLP 相关 的 创业 公司 奇点 机智 
NLP 知识结构 1 . 概述 1 自然语言 处理 利用 计算机 
为 工具 对 书面 实行 或者 口头 形式 进行 各种各样 
的 处理 和 加工 的 技术 是 研究 人 与人 
交际 中 以及 人 与 计算机 交际 中的 演员 问题 
的 一门 学科 是 人工智能 的 主要 内容 2 自然语言 
处理 是 研究 语言 能力 和 语言 应用 的 模型 
建立 计算机 算法 框架 来 实现 这样 的 语言 模型 
并 完善 评测 最终 用于 设计 各种 实用 系统 3 
研究 问题 主要 信息检索 机器翻译 文档 分类 问答 系统 信息 
过滤 自动 文摘 信息 抽取 文本 挖掘 舆情 分析 机器 
写作 语音识别 4 困难 所 在 场景 的 困难 语言 
的 多样性 多变性 歧义 性 学习 的 困难 艰难 的 
数学 模型 hmm crf EM 深度 学习 等 语料 的 
困难 什么 的 语料 语料 的 作用 如何 获取 语料 
2 . 形式语言 与 自动机 1 语言 按照 一定 规律 
构成 的 句子 或者 字符串 的 有限 或者 无限 的 
集合 2 描述语言 的 三种 途径 穷举法 文法 产生式系统 描述 
自动机 3 自然 语言 不 是 人为 设计 而 是 
自然 进化 的 形式语言 比如 运算 符号 化学 分子式 编程语言 
主要 研究 内部 结构 模式 这类 语言 的 纯粹 的 
语法 领域 从 语言学 而来 作为 一种 理解 自然 语言 
的 句法 规律 在 计算机 科学 中 形式语言 通常 作为 
定义 编程 和 语法结构 的 基础 4 形式语言 与 自动机 
基础知识 集合论 图论 5 自动机 的 应用 单词 自动 查错 
纠正 词性 消 歧 什么 是 词性 什么 的 词性 
标注 为什么 需要 标注 如何 标注 6 形式语言 的 缺陷 
对于 像 汉语 英语 这样 的 大型 自然 语言 系统 
难以 构造 精确 的 文法 不符合 人类 学习 语言 的 
习惯 有些 句子 语法 正确 但在 语义上 却 不 可能 
形式语言 无法 排出 这些 句子 解决 方向 基于 大量 语料 
采用 统计学 手段 建立 模型 3 . 语言 模型 1 
语言 模型 重要 通过 语料 计算 某个 句子 出现 的 
概率 概率 表示 常用 的 有2/nr 元 模型 3 元 
模型 2 语言 模型 应用 语音识别 歧义 消除 例如 给定 
拼音串 ta shi yan yan jiu saun fa de 可能 
的 汉字 串 踏实 烟酒 算法 的 他 是 研究 
酸 法的/nr 他 是 研究 算法 的 显然 最后 一句 
才 符合 3 语言 模型 的 启示 开启 自然语言 处理 
的 统计 方法 统计 方法 的 一般 步骤 收集 大量 
语料 对 语料 进行 统计分析 得出 知识 针对 场景 建立 
算法 模型 解释 和 应用 结果 4 语言 模型 性能评价 
包括 评价 目标 评价 的 难点 常用 指标 交叉 熵 
困惑 度 5 数据 平滑 数据 平滑 的 概念 为什么 
需要 平滑 平滑 的 方法 加 一 法 加法 平滑 
法 古德 图灵 法 J M 法 Katz 平滑 法等6/nr 
语言 模型 的 缺陷 语料 来自 不同 的 领域 而 
语言 模型 对 文本 类型 主题 等 十分 敏感 n 
与 相邻 的 n 1个 词 相关 假设 不是 很 
成立 4 . 概率 图 模型 生成 模型 与 判别 
模型 贝叶斯 网络 马尔科夫 链 与 隐 马尔科夫 模型 HMM 
1 概率 图 模型 概述 什么 的 概率 图 模型 
参考 清华大学 教材 概率 图 模型 2 马尔科夫 过程 定义 
理解 3 隐 马尔科夫 过程 定义 理解 HMM 的 三个 
基本问题 定义 解法 应用 注 第一 个 问题 涉及 最大 
似 然 估计法 第二 个 问题 涉及 EM 算法 第三 
个 问题 涉及 维 特比 算法 内容 很多 要 重点 
理解 5 . 马尔科夫 网 最大熵 模型 条件 随 机场 
CRF 1 HMM 的 三个 基本 问题 的 参数 估计 
与 计算 2 什么 是 熵 3 EM 算法 应用 
十分 广泛 好好 理解 4 HMM 的 应用 5 层次化 
马尔科夫 模型 与 马尔科夫 网络 提出 原因 HMM 存在 两个 
问题 6 最大熵 马尔科夫 模型 优点 与 HMM 相比 允许 
使用 特征 刻画 观察 序列 训练 高效 缺点 存在 标记 
偏置 问题 7 条件 随 机场 及其 应用 概念 模型 
过程 与 HMM 关系 参数估计 方法 GIS 算法 改进 IIS 
算法 CRF 基本问题 特征 选取 特征 模板 概率 计算 参数 
训练 解码 维 特比 应用 场景 + 词性 标注 类 
问题 现在 一般 用 RNN + CRF + 中文分词 发展过程 
经典 算法 了解 开源 工具 jieba 分词 + 中文 人名 
地名 识别 8 CRF + + 6 . 命名 实体 
识别 词性 标注 内容 挖掘 语义分析 与 篇章 分析 大量 
用到 前面 的 算法 1 命名 实体 识别 问题 2 
未 登录 词 的 解决 方法 搜索引擎 基于 语料 3 
CRF 解决 命名 实体 识别 NER 流程 总结 训练 阶段 
确定 特征 模板 不同 场景 人名 地名 等 所 使用 
的 特征 模板 不同 对 现有 语料 进行 分词 在 
分词 结果 基础 上 进行 词性 标注 可能 手工 NER 
对应 的 标注 问题 是 基于 词 的 然后 训练 
CRF 模型 得到 对应 权值 参数值 识别 过程 将 待 
识别 文档 分词 然后 送入 CRF 模型 进行 识别 计算 
维 特比 算法 得到 标注 序列 然后 根据 标 注 
划分 出 命名 实体 4 词性 标注 理解 含义 意义 
及其 一致性 检查 方法 位置 属性 向量 词性 标注 序列 
向量 聚 类 或者 分类 算法 7 . 句法分析 1 
句法分析 理解 以及 意义 句法结构 分析 完全 句法分析 浅层 分析 
这里 有 很多 方法 依存关系 分析 2 句法 分析方法 1 
. 基于 规则 的 句法结构 分析 2 . 基于 统计 
的 语法 结构 分析 8 . 文本 分类 情感 分析 
1 文本 分类 文本 排 重 文本 分类 在 预定义 
的 分类 体系 下 根据 文本 的 特征 将 给定 
的 文本 与 一个 或者 多 个 类别 相关联 典型 
应用 垃圾邮件 判定 网页 自动 分类 2 文本 表示 特征 
选取 与 权重 计算 词 向量 文本 特征选择 常用 方法 
1 基于 本文 频率 的 特征 提 取法 2 信息 
增 量法 3 X2 卡方 统计量 4 互信息 法3/nr 分类器 
设计 SVM 贝叶斯 决策树 等 4 分类器 性能 评测 1 
召回率 2 正确率 3 F1 值 5 主题 模型 LDA 
与 PLSALDA 模型 十分 强大 基于 贝叶斯 改进 了 PLSA 
可以 提取 出 本章 的 主题词 和 关键词 建模 过程 
复杂 难以理解 6 情感 分析 借助 计算机 帮助 用户 快速 
获取 整理 和 分析 相关 评论 信息 对 带有 感情 
色彩 的 主观 文本 进行 分析 处理 和 归纳 例如 
评论 自动 分析 水军 识别 某种 意义 上 看 情感 
分析 也 是 一种 特殊 的 分类 问题 9 . 
信息检索 搜索引擎 及其 原理 1 信息检索 起源于 图书馆 资料 查询 
检索 引入 计算机 技术 后 从 单纯 的 文本 查询 
扩展到 包含 图片 音 视频 等 多媒体 信息检索 检索 对象 
由 数据库 扩展 到 互联网 1 . 点对点 检索 2 
. 精确 匹配 模型 与 相关 匹配 模型 3 . 
检索系统 关键技术 标引 相关度 计算 2 常见 模型 布尔 模型 
向量空间 模型 概率模型 3 常用 技术 倒排索引 隐 语义分析 LDA 
等 4 评测 指标 10 . 自动 文摘 与 信息 
抽取 机器翻译 问答 系统 1 统计 机器 翻译 的 的 
思路 过程 难点 以及 解决 2 问答 系统 基本 组成 
问题 分析 信息检索 答案 抽取 类型 基于 问题 答案 基于 
自由 文本 典型 的 解决 思路 3 自动 文摘 的 
意义 常用 方法 4 信息 抽取 模型 LDA 等 11 
. 深度 学习 在 自然 语言 中 的 应用 1 
单词 表示 比如 词 向量 的 训练 wordvoc 2 自动 
写 文本 写 新闻 等 3 机器翻译 4 基于 CNN 
RNN 的 文本 分类 5 深度 学习 与 CRF 结合 
用于 词性 标注 NLP 用于 征信 近几年 国内 P2P 和 
现金 贷 的 大量 涌现 说明 了 个人 小额 信贷 
的 市场 需求 巨大 在 过去 针对 该类 小 贷 
用户 一般 单纯 地 依靠 地 推 人员 挨家挨户 进行 
实地 征信 如今 基于 大 数据 和 人工智能 技术 可以 
实现 智能 征信 和 审批 极大 地 提高 工作 效率 
通过 多 渠道 获取 用户 多 维度 的 数据 如 
通话记录 短信 信息 购买 历史 以及 社交 网络 上 的 
相关 留存 信息 等 然后 从 信息 中 提取 各种 
特征 建立 模型 对 用户 进行 多 维度 画像 最后 
根据 模型 评分 对 用户 的 个人 信用 进行 评估 
同样 对于 市场 上 中小 微 企业 融资 难 的 
问题 也 可以 通过 大 数据 征信 得以 解决 信用 
评分 模型 案例 业务 目标 建立 信用 评估 系统 当 
把 信用卡 用户 的 信息 导入到 该 系统 时 系统 
会 自动 输出 这批 用户 的 违约 风险 及 信用 
得分 为 信用卡 用户 的 管理 提供 决策 支持 数据挖掘 
目标 建立 信用 卡 用户 的 信用 评估 模型 该 
模型 以 用户 的 信息 指标 为 输入 以 违约 
为 目标 建立 预测模型 该 模型 可以 根据 输入 指标 
的 值 计算 预测值 违约 建立 信用 评分 模型 分类 
预测 算法 并不 局限于 神经网络 算法 只要/c 是/v 适用/v 于/p 
目标/n 变量/vn 为/p 字符/n 型/k 的/uj 分类/n 预测/vn 算法/n 都/d 
可以/c 如 决策树 支持 向量 机 贝叶斯 网络 KNN Logistic 
回归 等 http / / bbs . pinggu . org 
/ thread 3823928 1 1/m ./i html/w 参考资料/n 黄志洪/nr 老师/n 
的/uj 自然/d 语言/n 处理/v 课程/n 参考书/n 宗 成庆 老师 的 
统计 自然语言 处理 自然语言 处理 NLP 知识结构 总结 目录 文章 
目录 目录 前言 绪论 课程 课程 定性 课程 应用 个人 
经验 统计 自然语言 处理 自然语言 处理 绪论 自然语言 处理 绪论 
二 自然语言 处理 绪论 三 自然语言 处理 绪论 四 前言 
硕士 生涯 结束 开始 专心 做 一件 自己 觉得 有用 
的 工具 先 做 工程 后搞/nr 理论 自然语言 处理 是 
一个 非常 难 的 问题 同时 是 人工智能 皇冠 上 
的 明珠 接下来 会 记录 一 系列 自然语言 处理 的 
笔记 来自/v 于/p 哈工大/nt 老师/n 关毅/nr 绪论/n 课程/n 课程/n 定性/n 
本/r 门/n 课程/n 关注/v 于最/nr 基础/n 的/uj 部分/n 只会 讲述 
不容置疑 的 基本点 以及 一些 使用 的 技术 所以 本 
门 课程 由 两部分 组成 1 知识 内容 2 动手 
实践 自然语言 处理 概论 一些 段子 哈工大 自然语言 处理 人 
超 多 同时 工作 原创性 不够 其次 20 年前 的 
问题 依旧 是 问题 课程 应用 机器翻译 数据库 技术 语音识别 
个人 经验 老师 先 做了 一段 时间 的 工程 后来 
发现 做 的 东西 都被/nr 时间 所 淘汰 于是 转而 
做 持久 的 理论 统计 自然语言 处理 本 门 课程 
主要 专注 于词/nr 法分析 句法分析 语义分析 自然语言 处理 的 历史 
1946 自然语言 处理 的 诞生 目前 人工智能 陷入 低谷 大家 
开始 反思 究竟 什么 是 智能 什么 是 理解 1 
高德纳 的 书 人和 机器 对于 歧义 的 理解 是 
不同 的 人工 语言 不 允许 语法错误 自然语言 则 各种 
语法错误 自然语言 处理 绪论 1 分词 歧义 2 词性 标准 
3 语法分析 歧义 4 语义分析 歧义 5 语用分析 歧义 歧义 
很多 不好 处理 老师 靠 一个 关键 问题 拿到 了 
微软 的 offer 希望 我们 能 掌握 自己 的 核心 
技术 自然语言 处理 绪论 二 取法 其上 得 乎 其中 
取法 其 众 得 乎 其上 博采 众家 之长 优于 
众人 的 模型 参考文献 建议 两个 工具 1 思维导图 2endnote 
文献 管理 推荐 书籍 自然语言 处理 综论 计算机 自然语言 处理 
自然语言 定义 研究人 人 人 机 交互 的 语言 的 
模型 问题 提出 一个 语言 处理 的 框架 理解 语言 
本身 的 规律 研制 出 更好 人机 的 交互 系统 
自然语言 处理 绪论 三 语义学 模型 1 能力 模型 1 
语言学 规则 2 算法 优化 3 将 模型 数学 形式化 
2 应用 模型 1 语言 库 2 统计技术 3 建立 
计算 框架 处理 语言 自然语言 处理 的 其他 定义 用 
人工 方法 来 处理 语言学 的 相应 问题 目前 推荐 
的 处理 办法 是 规则 与 统计 相 结合 提高 
处理 效果 自然语言 处理 绪论 四 资源 介绍 1 基础 
资源 2 应用 资源 3 使用 资源 4 评测 标准 
自然语言 设计 的 学科 1 音位学 2 形态学 3 词汇学 
4 语用学 5 语义学 6 句 法学 自然语言 处理 的 
主要 应用 1 发现 句子 与 句子 间 的 关系 
2 句子 中 的 关系 3 单词 结构 与 规则 
自然语言 处理 的 资源 与 内容 1 北大 人民日报 语料库 
2 现代 汉语语法 信息 词典 3 概念 层次 网络 4 
知网 评判 一个 技术 有 没有 使用 前景 的 一个 
方法 1 让 工具 为 人 服务 解放 人类 个人 
学习 nlp 笔记 学习材料 CS124 COSC572 和 Speech and Language 
Processing 第三版 自然语言 处理 学习 笔记 五 1 . 矢量 
语义 Vector Semantics 1.1 词汇 语义 1.2 语义 的 矢量 
表达 2 . 信息 抽取 和 命名 实体 抽取 2.1 
关系 抽取 2 . 1.1 手写 规则 2 . 1.2 
监督 式 关系 抽取 2 . 1.3 非 监督 式 
和半/nr 监督 式 关系 抽取 3 . 问答 系统 Q 
A 3.1 信息检索 型 问答 3 . 1.1 答案 类型 
检测 3 . 1.2 构建 query3 . 1.3 章节 检索 
3 . 1.4 答案 提取 3.2 模型 评估 3.4 使用 
知识 knowledge 的 QA 系统 3.5 更 复杂 的 问题 
1 . 矢量 语义 Vector Semantics 分布式 假说 distributional hypothesis 
若 词 的 分布 相似 则 词 的 意思 也 
相似 1.1 词汇 语义 有 两个 重要 的 概念 词根 
和 词义 词根 lemma 也叫 citation form 词 sung sang 
的 词根 就是 sing 而 sung 这些 词 也称 wordform 
词义 sense 就 更好 理解 了 比如/v mouse/w 能/v 表示/v 
老鼠/n 和/c 鼠标/n 这就 说明 mouse 有 多个 词义 多个 
词义 也 带来 了 诸如 同义词 synonyms 的 问题 couch 
/ sofavomit / throw upfilbert / hazelnutcar / automobile 当然 
我们 也 有 反义词 Antonymslong / shortbig / littlefast / 
slowcold / hot dark / light 同样 我们 也 有 
很多 相似 的 词 word similarity 比如 cat 和 dog 
显然 他们 不是 同义词 但是 他们 都很/nr 相似 指代 着 
名词 和 一种 动物 我们 很多 种 方法 来 测量 
这种 相似 度 比如 很多 手工 标注 的 数据库 同时 
还有 词 相关度 Word Relatedness 比如 词 coffee 和 cup 
一个 指 一种 饮料 或 植物 吗 而 另一种 指 
一个 餐具 或者 形状 但是 毫无疑问 他们 在 现实 世界 
中 会 经常 在 同一个 语义 场景 下 出现 诸如 
hospitals surgeon scalpel nurse anaesthetic hospital restaurants waiter menu plate 
food chef or houses door roof kitchen family bed 语义 
场景 又和 主题 模型 topic models 有联系 语义 框架 比如 
在 交易 的 场景 下 词 代表 着 不同 的 
事件 比如 买 是由 买家 发出 而 支付 是 着重于 
货币 方面 的 比如 我 卖了 本书 给 B 则 
B 就为 买家 我 为 卖家 若 机器 能 理解 
这些 会在 QA 系统 或 翻译 上有 很大 帮助 分类 
关系 Taxonomic Relations 比如 把 车 归到 交通工具 芒果 归到 
水果 给 每个 详细 的 词 一个 上位 词 此外 
词 还有 很多 情感 的 含义 比如/v sad/w 和/c terrible/w 
有/v 不同/a 程度/n 的/uj 感情/n 色彩/n 1.2/mx 语义 的 矢量 
表达 人类 可以 从 一句话 中的 其他 信息 获得 对 
不 认识 的 词 的 信息 矢量 语义 的 直觉 
是 来自 分布式 假 说和 前文 的 connotation 中用 向量 
表达 一个词 结合 矢量 语义 就算 把 一个 词 映 
射到 多维 的 语义空间 中 我们 能 看见 褒义 的 
和 贬义 的 词 是 明显 分布 在 不同 的 
地方 的 同样 我们 在 矢量化 的 单词 中能 执行 
很多 操作 比如 计算 相似 度 和 语义分析 在 前文 
的 语义分析 中 贝叶斯 只能 在 足够 多 的 关键词 
同时 出现 在 训练 和 测试 集中 才行 而在 矢量化 
后 我们 可以 用 相似 度 高的词/nr 替代 在 训练 
集中 未 出现 的 词 总而言之 是 一个 很 实用 
的 非 监督 的 方法 比如 前文 的 tf idf 
和 word2vec2 . 信息 抽取 和 命名 实体 抽取 信息 
抽取 的 目的 主要 有 抽取 有用 的 信息 把 
信息 整合 为 数据 框 等 易处理 的 形式 也 
可以 用来 抽取 重要 信息 比如 一句话 的 发生 地点 
和 主要 人物 这些 信息 可以 用来 做 一些 应用 
比如 自动 建立 日程 等等 命名 实体 识别 也 就是 
把 一些 词 分为 人名 地点 时间 组织 等 要 
先 find 才能 classify2 . 1 关系 抽取 抽取 文中 
的 关系 能让 QA 系统 更加 智能 抽取 的 关系 
不同 的 任务 可能 由 不同 的 需要 抽取 的 
信息 比如 在 医疗 场景 原理 维基百科 上 的 这个 
条目 是由 关系 抽取 得来 的 啊 2 . 1.1 
手写 规则 准确率 高 召回率 低 漏掉 的 多 2 
. 1.2 监督 式 关系 抽取 第一步 找到 命名 实体 
第二步 判断 2个 命名 实体 是否 存在 关系 如果 binary 
分类器 返回 存在 开始 对 关系 进行 分类 因为有 二 
分类 这一步 很多 不 必要 的 显然 没 关系 的 
命名 实体 关系 就 不用 进行 分类 同时 我们 也 
可以 采用 两套 特征 来 分别 解决问题 监督 学习 索要 
用到 的 特征 命名 实体 的 中心词 以及 把 两者 
结合 M1/i 和/c M2/i 的/uj unigram/w 和/c bigram/w 词/n 袋/q 
模型/n M1/i 和/c M2/i 左边/f 和/c 右边/f 的/uj 单词/n M1/i 
和/c M2/i 之间/f 的/uj 词/n 的/uj unigram/w 和/c bigram/w 词/n 
袋/q 模型/n 命名/n 实体/n 的/uj 类型/n 类型 组合 实体 层面 
entity level 名字 名词 还是 代词 语法 特征 亲属 词 
地名 索引 和 上级 关系 比如 海南 在 中国 准确度 
高 但是 泛化 能力 一般 2 . 1.3 非/h 监督/vn 
式/k 和半/nr 监督/vn 式/k 关系/n 抽取/v 使用/v 手头/n 已有/v 的/uj 
少数/m 数据/n 和/c 准确度/n 高的/nr 模式/n 自动 找 其他 模式 
用 找到 的 pair 周围 的 内容 再 生成 新的 
模式 来 找 新的 pair 键入 已 有的 seed tuple 
找到 符合 的 内容 学习 模式 再用 学习 来 的 
模式 去 找 新的 tuplesnowball 的 方法 加上 了 判断 
是否 为 命名 实体 一种 结合 了 半 监督 和非/nr 
监督 的 的 方法 就是 用 半 监督 的 方法 
把 标记 的 数据 命名 实体 放着 更大 的 语料库 
google 找到 相应 语句 抽 取其 的 关系 再 训练 
非 监督 使用 一个 小 的 有 语法 信息 的 
数据 训练 一个 分类器 看 得到 的 关系 或者 tuple 
是否 值得 相信 放入 大 语 料中 抽取 关系 若 
这是 个 值得 相信 的 关系 或 tuple 保留 最后 
把 得到 的 关系 根据 出现 频率 排序 评估 方式 
抽取 前 1000个 新 关系 手动 评价 3 . 问答 
系统 QA 一种 更 智能 的 检索 或者 知识管理 方法 
一般 问题 有 两种 第一种 factoid question 易 回答 商业系统 
已 使用 另一种 是 复杂 的 问题 在 research system 
中 常见 常见 的 方案 第一个 是 信息 检索 的 
方法 直接 在 网上 找 答案 第二种 是 通过 对 
问题 解析 并用 混合 的 方法 构建 回答 比如 用 
部分 数据库 再结合 信息检索 的 方法 也 是 比较 modern 
的 方法 3.1 信息检索 型 问答 3 . 1.1 答案 
类型 检测 第一步 理解 问题 问 了 什么 x 表示 
unknown 以 答案 类型 检测 为 例子 展开 Jeopardy 知识 
竞赛 中 的 答案 类型 然而 最 频繁 的 200 
各 类型 包含 了 50% 的 数据 答案 类型 检测 
的 方法 有三种 手写 规则 在 一些 场景 很有 用 
机器 学习 方法 3 . 1.2 构建 query 第二步 构成 
query 决定 把 什么 词 传 输给 信息检索 系统 一个 
关键词 选择 的 算法 引号 中 是非 停顿 词 则 
排 第一 而 名词 应排六/nr ppt 应该 有误 剩下 的 
动词 排 七 我们 可以 输入 前 4个 也 可以 
只 输入 rank1 的 作为 关键词 3 . 1.3 章节 
检索 前 两步 等同 前文 的 检索系统 不 过把 文档 
换成 按 段落 来 检索 了 而 章节 的 重 
排序 需要 其他 方法 比如 按 规则 或者 监督 学习 
3 . 1.4 答案 提取 对 排序 得到 的 passage 
按 答案 类型 做 提取 但也 有 问题 比如 人 
名 问题 中 存在 多个 名字 我们 需要 对 其 
进行 再次 排序 比如 IBM 的 Waston 用了 50 多个 
变量 3.2 模型 评估 可以 对 前 M 个 得分 
高的/nr 答案 进行 对比 若有 回 答对 的 返回 1ranki 
\ frac { 1 } { rank _ { i 
} } ranki 1 则 最高 得分 的 candidate 若 
正确 就是 1 若 M 个 答案 都错/nr 就是 0 
3.4 使用 知识 knowledge 的 QA 系统 比如 whose granddaughter 
starred in E . T . 分解为 谁在 et 中 
出演 和她 又是 谁 的 孙女 通过 人物 时间 判断 
回答 准确性 通过 地理 知识 3.5 更 复杂 的 问题 
整合 答案 抽取 专业 文献 的 答案 两种 主要 的 
办法 主要 谈了/nr 第二 种 根据 不同 问题 比如 在 
回答 定义 的 类型 中 设定 20个 文档 并 返回 
8 句话 的 答案 先 将其 检索 发现 1000 余个 
与 Hajj 有关 的 句子 再用 一个 分类器 判断 是否 
这 是 一个 下 定义 的 句子 再将 两种 定义 
语句 和非/nr 定义 语句 句子 聚 类 和 重要性 排序 
本文 简要介绍 Python 自然语言 处理 NLP 使用 Python 的 NLTK 
库 NLTK 是 Python 的 自然 语言 处理 工具包 在 
NLP 领域 中 最常 使用 的 一个 Python 库 什么 
是 NLP 简单 来说 自然语言 处理 NLP 就是 开发 能够 
理解 人类 语言 的 应用 程序 或 服务 这里 讨论 
一些 自然 语言 处理 NLP 的 实际 应用 例子 如 
语音识别 语音 翻译 理解 完整 的 句子 理解 匹配 词 
的 同义词 以及 生成 语法 正确 完整 句子 和 段落 
这 并 不是 NLP 能做 的 所有 事情 NLP 实现 
搜索引擎 比如 谷歌 Yahoo 等 谷歌 搜索引擎 知道 你 是 
一个 技术 人员 所以 它 显示 与 技术 相关 的 
结果 社交 网站 推送 比如 Facebook News Feed 如果 News 
Feed 算法 知道 你 的 兴趣 是 自然 语言 处理 
就会 显示 相关 的 广告 和 帖子 语音 引擎 比如 
Apple 的 Siri 垃圾邮件 过滤 如 谷歌 垃圾邮件 过滤器 和 
普通 垃圾邮件 过滤 不同 它 通过 了 解 邮件 内容 
里面 的 的 深层 意义 来 判断 是 不是 垃圾 
邮件 NLP 库 下面 是 一些 开源 的 自然 语言 
处理 库 NLP Natural language toolkit NLTK Apache OpenNLP Stanford 
NLP suite Gate NLP library 其中 自然 语言 工具包 NLTK 
是 最 受欢迎 的 自然 语言 处理 库 NLP 它 
是 用 Python 编写 的 而且 背后 有 非常 强大 
的 社区 支持 NLTK 也 很容易 上手 实际上 它 是 
最简单 的 自然 语言 处理 NLP 库 在 这个 NLP 
教程 中 我们 将 使用 Python NLTK 库 安装 NLTK 
如果 您 使用 的 是 Windows / Linux / Mac 
您 可以 使用 pip 安装 NLTK pip install nltk1 打开 
python 终端 导入 NLTK 检查 NLTK 是否 正确 安装 import 
nltk1 如果 一切 顺利 这 意味着 您 已经 成功 地 
安装 了 NLTK 库 首次 安装 了 NLTK 需要 通过 
运行 以下 代码 来 安装 NLTK 扩展 包 import nltk 
nltk . download 123 这将 弹出 NLTK 下载 窗口 来 
选择 需要 安装 哪些 包 您 可以 安装 所有 的 
包 因为 它们 的 大小 都 很小 所以 没有 什么 
问题 使用 Python Tokenize 文本 首先 我们 将 抓取 一个 
web 页面 内容 然后 分析 文本 了解 页面 的 内容 
我们 将 使用 urllib 模块 来 抓取 web 页面 import 
urllib . request response = urllib . request . urlopen 
http / / php . net / html = response 
. read print html 12345 从 打印 结果 中 可以 
看到 结果 包含 许多 需要 清理 的 HTML 标签 然后 
BeautifulSoup 模块 来 清洗 这样 的 文字 from bs4 import 
BeautifulSoup import urllib . request response = urllib . request 
. urlopen http / / php . net / html 
= response . read soup = BeautifulSoup html html5lib 123456 
这 需要 安装 html5lib 模块 text = soup . get 
_ text strip = True print text 12 现在 我们 
从 抓取 的 网页 中 得到 了 一个 干净 的 
文本 下 一步 将 文本 转换 为 tokens 像这样 from 
bs4 import BeautifulSoup import urllib . request response = urllib 
. request . urlopen http / / php . net 
/ html = response . read soup = BeautifulSoup html 
html5lib text = soup . get _ text strip = 
True tokens = text . split print tokens 123456789 统计 
词频 text 已经 处理 完毕 了 现在 使用 Python NLTK 
统计 token 的 频率分布 可以 通过 调用 NLTK 中的 FreqDist 
方法 实现 from bs4 import BeautifulSoup import urllib . request 
import nltk response = urllib . request . urlopen http 
/ / php . net / html = response . 
read soup = BeautifulSoup html html5lib text = soup . 
get _ text strip = True tokens = text . 
split freq = nltk . FreqDist tokens for key val 
in freq . items print str key + + str 
val 1 2 3 4 5 6 7 8 9 
1 0 1 1 1 2 1 3 如果 搜索 
输出 结果 可以 发现 最 常见 的 token 是 PHP 
您 可以 调用 plot 函数 做出 频率 分布图 freq . 
plot 20 cumulative = False # 需要 安装 matplotlib 库 
12 这 上面 这些 单词 比如 of a an 等等 
这些 词 都 属于 停用词 一般来说 停用词 应该 删除 防止 
它们 影响 分析 结果 处理 停用词 NLTK 自带 了 许多 
种 语言 的 停用词 列表 如果 你 获取 英文 停用词 
from nltk . corpus import stopwords stopwords . words english 
123 现在 修 改下 代码 在 绘图 之前 清除 一些 
无效 的 token clean _ tokens = list sr = 
stopwords . words english for token in tokens if token 
not in sr clean _ tokens . append token 12345 
最终 的 代码 应该 是 这样 的 下面 代码 应该是 
if token not in sr from bs4 import BeautifulSoup import 
urllib . request import nltk from nltk . corpus import 
stopwords response = urllib . request . urlopen http / 
/ php . net / html = response . read 
soup = BeautifulSoup html html5lib text = soup . get 
_ text strip = True tokens = text . split 
clean _ tokens = list sr = stopwords . words 
english for token in tokens if not token in sr 
clean _ tokens . append token freq = nltk . 
FreqDist clean _ tokens for key val in freq . 
items print str key + + str val 1 2 
3 4 5 6 7 8 9 1 0 1 
1 1 2 1 3 1 4 1 5 1 
6 1 7 1 8 现在 再 做 一次 词频 
统计图 效果 会 比 之前 好些 因为 剔除 了 停用词 
freq . plot 20 cumulative = False 1 使用 NLTK 
Tokenize 文本 在 之前 我们 用 split 方法 将 文本 
分割 成 tokens 现在 我们 使用 NLTK 来 Tokenize 文本 
文本 没有 Tokenize 之前 是 无法 处理 的 所以 对 
文本 进行 Tokenize 非常 重要 的 token 化 过程 意味着 
将 大 的 部件 分割为 小 部件 你 可以 将 
段落 tokenize 成 句子 将 句子 tokenize 成 单个 词 
NLTK 分别 提供 了 句子 tokenizer 和 单词 tokenizer 假如 
有 这样 这段 文本 Hello Adam how are you I 
hope everything is going well . Today is a good 
day see you dude1 使用 句子 tokenizer 将 文本 tokenize 
成 句子 from nltk . tokenize import sent _ tokenize 
mytext = Hello Adam how are you I hope everything 
is going well . Today is a good day see 
you dude . print sent _ tokenize mytext 1234 输出 
如下 Hello Adam how are you I hope everything is 
going well . Today is a good day see you 
dude . 1 这是 你 可能 会想 这也 太 简单 
了 不 需要 使用 NLTK 的 tokenizer 都可以 直接 使用 
正则表达式 来 拆分 句子 就行 因为 每个 句子 都有 标点 
和 空格 那么 再 来看 下面 的 文本 Hello Mr 
. Adam how are you I hope everything is going 
well . Today is a good day see you dude 
. 1 这样 如果 使用 标点符号 拆分 Hello Mr 将会 
被 认为 是 一个 句子 如果 使用 NLTK from nltk 
. tokenize import sent _ tokenize mytext = Hello Mr 
. Adam how are you I hope everything is going 
well . Today is a good day see you dude 
. print sent _ tokenize mytext 1234 输出 如下 Hello 
Mr . Adam how are you I hope everything is 
going well . Today is a good day see you 
dude . 1 这才 是 正确 的 拆分 接下来 试试 
单词 tokenizer from nltk . tokenize import word _ tokenize 
mytext = Hello Mr . Adam how are you I 
hope everything is going well . Today is a good 
day see you dude . print word _ tokenize mytext 
1234 输出 如下 Hello Mr . Adam how are you 
I hope everything is going well . Today is a 
good day see you dude . 1Mr . 这个词 也 
没有 被 分开 NLTK 使用 的 是 punkt 模块 的 
P u n k t e n t e n 
c e T o k e n i z e 
r 它 是 NLTK . tokenize 的 一部分 而且 这个 
tokenizer 经过训练 可以 适用 于 多种语言 非 英文 TokenizeTokenize 时 
可以 指定 语言 from nltk . tokenize import sent _ 
tokenize mytext = Bonjour M . Adam comment allez vous 
J esp è re que tout va bien . Aujourd 
hui est un bon jour . print sent _ tokenize 
mytext french 1234 输出 结果 如下 Bonjour M . Adam 
comment allez vous J esp è re que tout va 
bien . Aujourd hui est un bon jour . 1 
同义词 处理 使用 nltk . download 安装 界面 其中 一个 
包是/nr WordNet WordNet 是 一个 为 自然语言 处理 而 建立 
的 数据库 它 包括 一些 同义 词组 和 一些 简短 
的 定义 您 可以 这样 获取 某个 给定 单词 的 
定义 和 示例 from nltk . corpus import wordnet syn 
= wordnet . synsets pain print syn 0 . definition 
print syn 0 . examples 12345 输出 结果 是 a 
symptom of some physical hurt or disorder the patient developed 
severe pain and distension 12WordNet 包含 了 很多 定义 from 
nltk . corpus import wordnet syn = wordnet . synsets 
NLP print syn 0 . definition syn = wordnet . 
synsets Python print syn 0 . definition 123456 结果 如下 
the branch of information science that deals with natural language 
information large Old World boas123 可以 像 这样 使用 WordNet 
来 获取 同义词 from nltk . corpus import wordnet synonyms 
= for syn in wordnet . synsets Computer for lemma 
in syn . lemmas synonyms . append lemma . name 
print synonyms 1234567 输出 computer computing _ machine computing _ 
device data _ processor electronic _ computer information _ processing 
_ system calculator reckoner figurer estimator computer 1 反义词 处理 
也 可以 用 同样 的 方法 得到 反义词 from nltk 
. corpus import wordnet antonyms = for syn in wordnet 
. synsets small for l in syn . lemmas if 
l . antonyms antonyms . append l . antonyms 0 
. name print antonyms 123456789 输出 large big big 1 
词干 提取 语言 形态学 和 信息检索 里 词干 提取 是 
去除 词缀 得到 词根 的 过程 例如 working 的 词干 
为 work 搜索引擎 在 索引 页面 时 就会 使用 这种 
技术 所以 很多 人 为 相同 的 单词 写出 不同 
的 版本 有 很多 种 算法 可以 避免 这种 情况 
最 常见 的 是 波特 词干 算法 NLTK 有 一个 
名为 PorterStemmer 的 类 就是 这个 算法 的 实现 from 
nltk . stem import PorterStemmer stemmer = PorterStemmer print stemmer 
. stem working print stemmer . stem worked 12345 输出 
结果 是 work work12 还有 其他 的 一些 词干 提取 
算法 比如 Lancaster 词干 算法 非 英文 词干 提取 除了 
英文 之外 SnowballStemmer 还 支持 13种 语言 支持 的 语言 
from nltk . stem import SnowballStemmer print SnowballStemmer . languages 
danish dutch english finnish french german hungarian italian norwegian porter 
portuguese romanian russian spanish swedish 12345 你 可以 使用 SnowballStemmer 
类 的 stem 函数 来 提取 像 这样 的 非 
英文单词 from nltk . stem import SnowballStemmer french _ stemmer 
= SnowballStemmer french print french _ stemmer . stem French 
word 12345 单词 变体 还原 单词 变体 还原 类似于 词干 
但 不同 的 是 变体 还原 的 结果 是 一个 
真实 的 单词 不同于 词干 当 你 试图 提取 某些 
词 时 它 会 产生 类似 的 词 from nltk 
. stem import PorterStemmer stemmer = PorterStemmer print stemmer . 
stem increases 12345 结果 increas1 现在 如果 用 NLTK 的 
WordNet 来 对 同一 个 单词 进行 变体 还原 才是 
正确 的 结果 from nltk . stem import W o 
r d N e t L e m m a 
t i z e r lemmatizer = W o r 
d N e t L e m m a t 
i z e r print lemmatizer . lemmatize increases 12345 
结果 increase1 结果 可能会 是 一个 同义词 或 同一 个 
意思 的 不同 单词 有时候 将 一个 单词 做 变体 
还 原时 总是 得到 相同 的 词 这 是因为 语言 
的 默认 部分 是 名词 要 得到 动词 可以 这样 
指定 from nltk . stem import W o r d 
N e t L e m m a t i 
z e r lemmatizer = W o r d N 
e t L e m m a t i z 
e r print lemmatizer . lemmatize playing pos = v 
12345 结果 play1 实际上 这也 是 一种 很好 的 文本 
压缩 方式 最终 得到 文本 只有 原先 的 50% 到 
60% 结果 还 可以 是 动词 v 名词 n 形容词 
a 或 副词 r from nltk . stem import W 
o r d N e t L e m m 
a t i z e r lemmatizer = W o 
r d N e t L e m m a 
t i z e r print lemmatizer . lemmatize playing 
pos = v print lemmatizer . lemmatize playing pos = 
n print lemmatizer . lemmatize playing pos = a print 
lemmatizer . lemmatize playing pos = r 12345678 输出 play 
playing playing playing12345 词干 和 变体 的 区别 通过 下面 
例子 来 观察 from nltk . stem import W o 
r d N e t L e m m a 
t i z e r from nltk . stem import 
PorterStemmer stemmer = PorterStemmer lemmatizer = W o r d 
N e t L e m m a t i 
z e r print stemmer . stem stones print stemmer 
. stem speaking print stemmer . stem bedroom print stemmer 
. stem jokes print stemmer . stem lisa print stemmer 
. stem purple print print lemmatizer . lemmatize stones print 
lemmatizer . lemmatize speaking print lemmatizer . lemmatize bedroom print 
lemmatizer . lemmatize jokes print lemmatizer . lemmatize lisa print 
lemmatizer . lemmatize purple 1 2 3 4 5 6 
7 8 9 1 0 1 1 1 2 1 
3 1 4 1 5 1 6 1 7 1 
8 1 9 输出 stone speak bedroom joke l i 
s a 1 2 3 4 5 p u r 
p l s t o n e speaking bedroom joke 
lisa purple123456 词干 提取 不会 考虑 语境 这也 是 为什么 
词干 提取 比 变体 还原 快 且 准确度 低 的 
原因 个人认为 变体 还原 比 词干 提取 更好 单词 变体 
还原 返回 一个 真实 的 单词 即使 它 不是 同一 
个 单词 也是 同义词 但 至少 它 是 一个 真实 
存在 的 单词 如果 你 只 关心 速度 不在意 准确度 
这时 你 可以 选用 词干 提取 在此 NLP 教程 中 
讨论 的 所有 步骤 都 只是 文本 预处理 在 以后 
的 文章 中 将会 使用 Python NLTK 来 实现 文本 
分析 我 已经 尽量 使 文章 通俗易懂 希望 能对你/nr 有所 
帮助 script function { function setArticleH btnReadmore posi { var 
winH = $ window . height var articleBox = $ 
div . article _ content var artH = articleBox . 
height if artH winH * posi { articleBox . css 
{ height winH * posi + px overflow hidden } 
btnReadmore . click function { if typeof window . localStorage 
= = = object & & typeof window . csdn 
. a n o n y m o u s 
U s e r L i m i t = 
= = object { if window . csdn . a 
n o n y m o u s U s 
e r L i m i t . judgment { 
window . csdn . a n o n y m 
o u s U s e r L i m 
i t . Jumplogin return false } else if currentUserName 
{ window . csdn . a n o n y 
m o u s U s e r L i 
m i t . updata } } articleBox . removeAttr 
style $ this . parent . remove } } else 
{ btnReadmore . parent . remove } } var btnReadmore 
= $ # btn readmore if btnReadmore . length 0 
{ if currentUserName { setArticleH btnReadmore 3 } else { 
setArticleH btnReadmore 1.2 } } } / script / article 
@ TOC 这里 写 自定义 目录 标题 后台 回复 关键词 
NLP 下载 研究报告 含 人才 分布图 目录 第 1 章 /nr 
自然语言 处理 概念 篇 第 2 章 自/nr 言/vg 语言/n 处理/v 
技术篇/n 第/m 3 章 /nr 自然语言 处理 人才篇 第 4 章 /nr 
自然语言 处理 应用 篇 第 5 章 /nr 自然语言 处理 趋势 
篇 自然语言 处理 是 包括 了 计算机 科学 语言学 心理 
认知 学 等 一系列 学科 的 一门 交叉学科 这些 学科 
性质 不同 但 又 彼此 相互 交叉 1950年 图灵 提出 
了 著名 的 图灵测试 这 一般 被 认为 是 自然 
语言 处理 思想 的 开端 20 世纪 50 年代 到 
70 年代 自然语言 处理 主要 采用 基于 规则 的 方法 
70 年代 以后 随着 互联网 的 高速 发展 自然语言 处理 
思潮 由 理性主义 向 经验主义 过渡 基于 统计 的 方法 
逐渐 代替 了 基于 规则 的 方法 从 2008年 到现在 
在 图像 识别 和 语音 识别 领域 的 成果 激励 
下 人们 也 逐渐 开始 引入 深度 学习 来做 自然语言 
处理 研究 由 最初 的 词 向 量到 2013年 word2vec 
将 深度 学习 与 自然 语言 处理 的 结合 推向 
了 高潮 并在 机器翻译 问答 系统 阅读 理解 等 领域 
取得 了 一定 成功 接下来 我们 将 为 大家 介绍 
自然语言 处理 的 业界 发展 涵盖 了 以下 企业 微软 
亚洲 研究院 微软 亚洲 研究院 1998年 成立 自然语言 计算 组 
研究 内容 包括 多 国 语言 文本 分析 机器翻译 跨语言 
信息检索 和 自动 问答 系统 等 这些 研究 项目 研发 
了 一系列 实用 成果 如 IME Input Method Editors 输入法 
编辑器 它 是 一种 专门 的 应用 程序 用来 输入 
代表 东亚地区 书面 语言 文字 的 不同 字符 对联 游戏 
Bing 词典 Bing 翻译器 语音 翻译 搜索引擎 等 为 微软 
产品 做出 了 重大 的 贡献 微软 IME 微软 对联 
游戏 微软 必应 词典 并且 在 自然 语言 处理 顶级 
会议 例如 ACL COLING 等 会议 上 发表 了 许多 
论文 语音 翻译 2017年 微软 在 语音 翻译 上 全面 
采用 了 神经 网络 机器翻译 并 新 扩展 了 Microsoft 
Translator Live Feature 可以 在 演讲 和 开会 时 实时 
同步 在 手机 端 和 桌面 端 同时 把 讲话者 
的话 翻译 成 多种 语言 其中 最 重要 的 技术 
是 对于 源语言 的 编码 以及 引进 的 语言 知识 
同时 微软 还 表示 将来 要将 知识图谱 纳入 神经网络 机器翻译 
中 规划 语言 理解 的 过程 中 人机对话 小娜 现在 
已经 拥有 超过 1.4亿 用户 在 数以十亿计 的 设备 上 
与 人们 进行 交流 并且 覆盖 了 十几 种 语言 
有 聊天 机器人 小冰 正在 试图 把 各国 语言 的 
知识 融合 在 一起 实现 一个 开放 语言 自由 聊天 
的 过程 目前 小冰 实现 了 中文 日文 和 英文 
的 覆盖 有上 亿 用户 GoogleGoogle 是 最早 开始 研究 
自然语言 处理 技术 的 团队 之一 作为 一个 以 搜索 
为 核心 的 公司 Google 对 自然 语言 处理 更为 
重视 Google 拥有 着 海量 数据 可以 搭建 丰富 庞大 
的 数据库 可以为 其 研究 提供 强大 的 数据 支撑 
Google 对 自然 语言 处理 的 研究 侧重于 应用 规模 
跨语言 和跨/nr 领域 的 算法 机器翻译 知识图谱 Google 的 知识图谱 
更是 遥遥领先 例如 自动 挖掘 新 知识 的 准确 程度 
文本 中 命名 实体 的 识别 纯 文本 搜索 词条 
到 在 知识 图 谱上 的 结构化 搜索 词条 的 
转换 等 效果/n 都/d 领先/n 于/p 其他/r 公司/n 而且 很多 
技术 都 实现 了 产品化 语音识别/i Google/w 一直/d 致力/n 于/p 
投资/vn 语音/n 搜索/v 技术/n 和/c 苹果/n 公司/n 的/uj siri/w 竞争/vn 
自 2012年 以来 将 神经 网络 应用于 这一 领域 使 
语音识别 错误率 极大 降低 2011年 收购 语言 信息 平台 SayNow 
把 语音 通信 点对点 对话 以及 群组 通话 和 社交 
应用 融合 在 一起 2014年 收购 了 SR Tech Group 
的 多项 语音识别 相关 专利 FacebookFacebook 涉猎 自然语言 处理 较晚 
2013年 开始 发展 语音 翻译 2015年 开始 语音 识别 的 
研发 之路 语音 翻译 发展 道路 如下 图 所示 语音识别 
2015年 Facebook 相继 建立 语音 识别 和 对话 理解 工具 
开始 了 语音 识别 的 研发 之路 2016年 Facebook 开发 
了 一个 响应 Hey Oculus 的 语音 识别 系统 并在 
2018年 初开 发了 wav2letter 这 是 一个 简单 高效 的 
端 到 端 自动 语音 识别 ASR 系统 百度 百度 
自然语言 处理 部 是 百度 最早 成立 的 部门 之一 
研究 涉及 以下 方面 百度 在 深度 问答 方向 经过 
多年 打磨 积累 了 问句 理解 答案 抽取 观点 分析 
与 聚合 等 方面 的 一整套 技术 方案 目前 已经 
在 搜索 度 秘 等 多个 产品 中 实现 应用 
百度 翻译 目前 支持 全球 28种 语言 覆盖 756个 翻译 
方向 支持 文本 语音 图像 等 翻译 功能 并 提供 
精准 人工 翻译 服务 满足 不同 场景 下 的 翻译 
需求 发布 了 世界 上 首个 线上 神经网络 翻译 系统 
并 获得 2015 年度 国家 科技 进步奖 阿里巴巴 阿里 自然语言 
处理 为 其 产品 服务 在 电商 平台 中 构建 
知识图谱 实现 智能 导购 同时 进行 全网 用户 兴趣 挖掘 
在 客服 场景 中 也 运用 自然语言 处理 技术 打造 
机器人 客服 例如 蚂蚁 金融 智能 小宝 淘宝 卖家 的 
辅助 工具 千牛 插件 等 同时 进行 语音 识别 以及 
后续 分析 阿里 的 机器 翻译 主要 与其 国家化 电商 
的 规划 相 联系 2017 年初 阿里 正式 上线 了 
自主 开发 的 神经 网络 翻译 系统 进一步 提升 了 
其 翻译 质量 腾讯 AI Lab 是 腾讯 的 人工智能 
实验室 研究 领域 包括 计算机 视觉 语音识别 自然语言 处理 机器学习 
等 其 研发 的 腾讯 文智 自然语言 处理 基于 并行计算 
分布式 爬虫 系统 结合 独特 的 语义分析 技术 可满足 自然语言 
处理 转码 抽取 数据 抓取 等 需求 在 机器 翻译 
方面 2017年 腾讯 宣布 翻译 君 上线 同声 传译 新功能 
用户/n 边/d 说/v 边翻的/nr 需求/v 得到/v 满足/v 语音识别/i +/i NMT/w 
等/u 技术/n 的/uj 应用/v 保证/v 了/ul 边说边/nr 翻/v 的/uj 速度/n 
与/p 精准性/i 京东 京东 在 人工智能 的 浪潮 中 也 
不甘落后 京东 AI 开放平台 基本上 由 模型 定制 化 平台 
和 在线 服务 模块 构成 其中 在线 服务 模块 包括 
计算机 视觉 语音 交互 自然语言 处理 和 机器 学习 等 
按照 京东 的 规划 NeuHub 平台 将 作为 普惠 性 
开放平台 不同 角色 均可 找到 适合 自己 的 场景 例如 
用 简单 代码 即可 实现 对 图像 质量 的 分析 
评估 从 业务 上 说 平台 可以 支撑 科研 人员 
算法 工程师 不断 设计 新的 AI 能力 以 满足 用户 
需求 并 深耕 电商 供应链 物流 金融 广告 等 多个 
领域 应用 探索 试验 医疗 扶贫 政务 养老 教育 文化 
体育 等 多 领域 应用 聚焦/v 于新/nr 技术/n 和/c 行业/n 
趋势/n 研究/vn 孵化 行业 最新 落地 项目 科大 讯 飞 
科大 讯 飞 股份 有限公司 成立 于 1999年 是 一家 
专业 从事 智能 语音 及 语言 技术 人工智能 技术 研究 
软件 及 芯片 产品开发 语音 信息 服务 及 电子 政务 
系统 集成 的 国家级 骨干 软件 企业 科大 讯 飞 
作为 中国 智能 语音 与 人工智能 产业 领导者 在 语音 
合成 语音识别 口语 评测 自然语言 处理 等 多项 技术 上 
拥有 国际 领先 的 成果 科大 讯 飞 成立 之时 
就 开始 在 语言 和 翻译 领域 布局 项目 基于 
深度 神经网络 算 法上 的 创新 和 突破 在 翻译 
方面 的 发展 如下 图 所示 后台 回复 关键词 NLP 
下载 研究报告 含 人才 分布图 版权 声明 AMiner 属 于清华 
青岛 数据 科学 研究院 科技 大 数据 研究中心 www . 
ids . tsinghua . edu . cn AMiner 咨询 产品 
版 权为 AMiner 团队 独家 所有 拥有 唯一 著作权 AMiner 
咨询 产品 是 AMiner 团队 的 研究 与 统计 成果 
其 性质 是 供 客户 内部 参考 的 商业 资料 
AMiner 咨询 产品 为 有偿 提供给 购买 该 产品 的 
客户 使用 并 仅限于 该 客户 内部 使用 未获得 AMiner 
团队 书面 授权 任何人 不得 以 任何 方式 在 任何 
媒体 上 包括 互联网 公开 发布 复制 且 不得 以 
任何 方式 将 本 产品 的 内容 提供 给 其他 
单位 或 个人 使用 如 引用 刊发 需 注明 出处 
为 AMiner . org 且 不得 对本 报告 进行 有悖 
原意 的 删节 与 修改 否则 引起 的 一切 法律 
后果 由该 客户 自行 承担 同时 AMiner 团队 亦 认为 
其 行为 侵犯 了 AMiner 团队 著作权 AMiner 团队 有权 
依法 追究 其 法律 责任 AMiner 咨询 产品 是 基于 
AMiner 团队 及其 研究员 认为 可信 的 公开 资料 但 
AMiner 团队 及其 研究员 均不 保证 所 使用 的 公开 
资料 的 准确性 和 完整性 也不 承担 任何 投资 者 
因 使用 本 产品 与 服务 而 产生 的 任何 
责任 行业 研究 报告 是 AMiner 团队 智能 服务 体系 
的 重要 组成部分 如对 有关 信息 或 问题 有 深入 
需求 的 客户 欢迎 使用 AMiner 团队 专项 研究 智能 
服务 NLTK 自然语言 处理 库 自带 语料库 词性 分 类库 
要 记得 安装 语料库 import nltk nltk . download Tokenize 
拆 句子 拆 小 英文 分词 import nltk sentence = 
hello world tokens = nltk . word _ tokenize sentence 
tokens 社交语言 的 分词 表情符号 需要用 正则表达式 去 匹配 中文分词 
启发式 Heuristic 或者 机器学习 统计 方法 可以 用 Jiebaimport jieba 
seg _ list = jieba . cut 啊啊啊 cut _ 
all = Ture # 全 模式 False 是 精准 模式 
分词 分词 之后 是 一个 数组 比如 word = i 
am a fool 英文单词 比较复杂 为了 降低 复杂度 一般 要 
经过 词干 提取 stemming 和 词形 归一 lemma walking 变成 
walk 去掉 了 ing 的 后缀 因为 它 不 影响 
词性 went 变成 go 去掉 了 过去 式 模式 不影响 
词性 from nlk . stem . porter import PorterStemmer porter 
_ stemmer = PorterStemmer porter _ stemmer . stem running 
from nlk . stem . lancaster import LancasterStemmer lancaster _ 
stemmer = LancasterStemmer lancaster _ stemmer . stem probalility from 
nltk . stem import W o r d N e 
t L e m m a t i z e 
r 为了 更好 的 进行 词性 划分 因为 一个 单词 
有 不同 的 意思 所以 必须 加入 词性 让 它 
更加 容易 分离 NLTK 有 词性 标注 器 标注 POS 
Tagimport nltk text = nltk . word _ tokenize i 
am a fool text . pos _ tag text Stopwords 
停止词 以上 就是 对 文本 预处理 的 流程 文本 预处理 
有 什么 用 得到 一个 list 包含 了 句子 中 
有意思 的 词 去掉 了 不 需要 的 然后 进行 
feature 化 进入 竞赛 内容 在 人工智能 异常 火爆 的 
当下 自然语言 处理 技术 因其 具有 广泛 的 应用 领域 
良好 的 计算 性能 等 因素 备受 科研 人员 的 
青睐 而 序列 标注 是 自然 语言 处理 领域 的 
一个 非常 常见 的 问题 从 分词 词性 标注 到 
较 深层 的 组块 分析 以至 更为 深层 的 完全 
句法分析 语义 角色 标注 等 任务 都 可以 看作 是 
典型 的 序列 标注 问题 序列 标注 问题 指 对 
序列 中 每个 元素 进行 标记 输出 标记 序列 y 
= y1 . . . yn n 是 序列 的 
长度 若 yi 的 取值 范围 定义 为 = { 
si } 输出 序列 的 可能 组 合数 为 Ci 
变量 yi 的 不同 取值 也叫 不同 的 状态 yi 
所有 可能 取值 的 集合 也 被 称为 状态 空间 
因为 一个 序列 状态 的 组合 数 非常多 也 不能 
直接 用 传统 的 学习 方法 通过 枚举 y 来 
得到 最佳 的 标记 需要用 动态 优化 的 方法 来 
求解 y 传统 的 单点 分类器 方法 难以获得 整个 序列 
的 最优 标记 下图 是 两种 线性 链 序列 标注 
结构 每个 元素 标记 只 与 相邻 的 元素 相关 
构成 了 线性 链式 结构 其中 图 a 是 有向图 
结构 每个 元素 标记 只 与 前 一个 元素 标记 
相关 图 b 是 无向图 结构 每个 元素 标记 与 
左右两个 相邻 元素 标记 相关 序列 标注 问题 需要 解决 
四 个 问题 1 . 如何 选择 合适 的 序列 
标注 模型 确定 标记 之间 的 关联关系 2 . 怎样 
从序/nr 列上 抽取 特征 3 . 如何 进行 求解 也 
就是 解码 问题 4 . 如何 进行 参数 学习 常用 
的 序列 标注 模型 有 线性 模型 隐 马尔可夫 模型 
最大熵 马尔可夫 模型 条件 随 机场 等 马尔可夫 链 简称 
马氏 链 是由 随机变量 组成 的 一个 序列 x1 x2 
x3 xt 的 值 是 在 时间 t 时的/nr 状态 
如果 xt + 1 对于 过去 状态 的 条件 概率分布 
仅 是 xt 的 一个 函数 即 P xt + 
1 | x1 x2 xt = P xt + 1 
| xt 序列 标注 模型 可以 分为 两大类 一种 是非 
统计 方法 另一种 是 统计 的 方法 在 非 统计 
方法 中 最有 代表性 的 是 线性 分类器 y = 
arg max w O x y 在 基于 统计 方法 
比较 主流 的 方法 是 用 无向图 来 表示 模型 
对于 中文分词 的 序列 标注 问题 可以 定义 y 属于 
{ B O } 这里 B 表示 把 当前 字 
作为 一个 新词 的 开始 O 表示 当前 字 与 
前面 的 字 构成 一个 词 例如 句子 他 ／ 
说 ／ 的 ／ 确实 ／ 在理 转化 为 下面 
以 字 为 基本 元素 构成 的 序列 x = 
他   说   的   确   实   
在   理 y =   B     B 
    B     B     O   
  B     O 假设 状态 空间 大小 为 
C 对于 长度 为 n 的 y 其 可能 的 
组合 数 为 Cn 因此 穷举 不同 的 y 已 
获得 最佳 序列 是 不 可行 的 通过观察 公式 我们 
可以 用 动态 优化 方法来 快速 的 求解 我们 首先 
定义 As i 是 输入 序列 x0 xi 且 yi 
= s 的 最佳 标记 序列 As i 可以 通过 
下面 两个 递归 公式 来 计算 As 0 = 0 
s 属于 As i = maxAs ′ i − 1 
+ w ϕ x s ′ s 这个 方法 也叫 
Viterbi 算法 可以 保证 找到 得分 最高 的 标记 序列 
有点 动态规划 的 意思 在 里面 学习 参数 的 方法 
也 不同 一般 为 最大 似 然 估计 最大 边际 
距离 或 最小 均方 误 差等 可以 用 传统 的 
分类器 训练 算法 比如 感知器 SVM kNN 等 隐 马尔可夫 
模型 HMM 是 常见 的 序列 标注 模型 HMM 有 
三个 典型 问题 1 已知 模型 参数 计算 某一 特定 
输出 序列 的 概率 . 通常 使用 forward 算法 解决 
. 2 已知 模型 参数 寻找 最 可能 的 能 
产生 某一 特定 输出 序列 的 隐含 状态 的 序列 
. 通常 使用 Viterbi 算法 解决 . 3 已知 输出 
序列 寻找 最 可能 的 状态 转移 以及 输出 概率 
. 通常 使用 Baum Welch 算法 以及 Reversed Viterbi 算法 
解决 隐 马尔可夫 模型 是 一个 生成式 模型 要对 p 
x y 进行 建模 并且 样本 序列 的 观测值 只 
与 当前 状态 标记 有关 这就 限制 了 模型 的 
能力 最大熵 马尔可夫 模型 是 一个 判别式 模型 直接 对 
p y | x 进行 建模 这样 可以 利用 大量 
的 冗余 特征 提高 模型 性能 最大熵 马尔可夫 模型 是 
用 局部 信息 去 优化 全局 会有 标注 偏置 Label 
Bias 的 问题 条件 随 机场 Conditional Random Fields CRF 
图 模型 它 是 在 给定 需要 标记 的 观察 
序列 x 的 条件 下 计算 整个 标记 序列 y 
的 联合 概率分布 而 不是 在 给定 当前 状态 条件 
下定义 下一个 状态 的 分布 即 P y | x 
= exp w O x y / Zx 隐 马尔可夫 
模型 中 存在 两个 假设 输出 独立性 假设 和 马尔可夫 
性 假设 其中 输出 独立性 假设 要求 序列 数据 严格 
相互 独立 才能 保证 推导 的 正确性 而 事实上 大多数 
序列 数据 不 能被 表示 成一/nr 系列 独立 事件 而 
条件 随 机场 则 使用 一种 概率 图 模型 条件 
随 机场 没有 隐 马尔可夫 模型 那样 严格 的 独立性 
假设 条件 因而 可以 容纳 任意 的 上下文 信息 可以 
灵活 地 设计 特征 同时 条件 随 机场 具有 表达 
长距离 依赖性 和 交叠 性 特征 的 能力 而且 所有 
特征 可以 进行 全局 归一化 能够 求得 全局 的 最优 
解 还 克服 了 最大熵 马尔可夫 模型 标记 偏置 的 
缺点 条件 随 机场 模型 作为 一个 整句 联合 标定 
的 判别式 概率模型 同时 具有 很强 的 特征 融入 能力 
是 目前 解决 自然语言 序列 标注 问题 最好 的 统计模型 
之一 条件 随 机场 的 缺点 是 训练 的 时间 
比较 长       上面 简单 介绍 了 基本 
的 序列 标注 的 模型 方法 以后 会 详细 分析 
前言 本节 主要 针对 斯坦福大学 CS224N 的 自然 语言 处理 
与 深度 学习 课程 所 做 笔记 将 学习 过程 
中 的 一些 重 难点 进行 记录 方便 后续 复习 
什么 是 自然 语言 处理 自然语言 处理 是 计算机 科学 
人工智能 和 语言学 的 集合 该 技术 的 目的 是 
为了 使 计算机 能够 理解 语言 . 自然语言 处理 的 
一些 应用 拼 写检查 关键词 查询 语法 检查 文本 分类 
对话 系统 什么 是 深度 学习 深度 学习 是 机器 
学习 的 一个 分之 和 传统 方法 的 区别 主要 
在于 其端 到 端 的 形式 从 raw input 中 
自动 提取 特征 最后 输出 想要 的 结果 分类 或者 
回归 . 一些 先 修 知识 Python 基础 线性代数 概率论 
统计学 基本 的 机器 学习理论 希望 教授 的 知识 使用 
有效 深度 学习 模型 的 能力 比如 NLP 中 的 
一些 重要 技术 RNN attention 机制 有 NLP 有个 宏观 
的 认知 了解 该 领域 的 一些 难点 有 能力 
构建 一些 NLP 中的 系统 来 解决 一些 主要 问题 
比如 单词 的 相似 度 命名 体 识别 翻译 系统 
对话 系统 为什么 NLP 难 语言 表示 和 学习 的 
复杂性 语言 的 歧义 性 人们 语言 的 解读 依赖 
于 实际 环境 比如 场景 和 上下文 . Deep NLP 
= Deeplearning + NLP 词义 的 表示 向 量化 单词 
且 可 进行 可视化 词义 向量 Morphology 依存 句法分析 Parsing 
for sentence structure 句子 的 含义 Semantic 情感 分析 Sentiment 
Analysis 作业 第 4 部分 作业 Assignment 1.1 1.2 地址 
https / / github . com / learning511 / cs224n 
learning camp / blob / master / Assignmnet . md 
1.1 Softmax 算法 1.2 Neural Network Basics 神经 网络 基础 
实现 Word Vector 本节 主要 记录 述词 向量 相关 的 
原理 和 内容 如何 表示 单词 含义 词义 进行 表示 
离散 表示 Onehot 基于 单词 上下文 的 分布式 表示 将 
单词 表示 为 向量 形式 Word2vec 的 主要 思想 两个 
方法 Skip gram 和 CBOWSkip gram 根据 中心词 预测 上下文 
CBOW 根据 上下文 预测 中心词 两种 相对效率 的 训练 方法 
霍夫曼 树 负 采样法 训练 中 同时 更新 迭代 所有 
向量 每个 词 有 两个 向量 的 表示 一个 是 
作为 中心词 时候 的 词 向量 另 一个 是 作为 
上下文 的 词 向量 更新 迭代 的 过程 更新 计算所 
有 梯度 在 划 窗 过程 中 更新 计算 的 
不止 是 中心词 的 词 向量 在 每一个 划 窗 
过程 对 窗口 中的 中心词 词 向量 上下 文词 向量 
都 进行 更新 . 自然语言 处理 是 计算机 科学 领域 
与 人工智能 领域 中 的 一个 重要 方向 它/r 研究/vn 
能/v 实现/v 人/n 与/p 计算机/n 之间/f 用/p 自然/d 语言/n 进行/v 
有效/a 通信/l 的/uj 各种/r 理论/n 和/c 方法/n 自然语言 处理 是 
一门 融 语言学 计算机科学 数学 于 一体 的 科学 语言 
是 人类 区别于 其他 动物 的 本质 特征 人类 的 
多种 智能 都与 语言 有关 语言 是 人工智能 的 一个 
重要 部分 甚至 是 核心 部分 但是 迄今为止 语法 都 
限于 分析 一个 独立 的 句子 上下文 关系 存在 约束 
作用 不能 完全 考虑 文章 存在 歧义 的 一系列 问题 
得不到 系统 的 解决 另 一个 方面 就是 人们 理解 
一个 句子 不是 单凭 语法 就行 还有 和 生活 阅历 
相关 的 大量 知识 这些 知识 肯定 不能 全部 储存 
在 计算机 里面 且 对于 不同 的 人 也许 会 
有 不同 的 解释 因此 只有 当 计算机 的 储存 
量 和 运转 速度 大大 提高 才能 广泛 使用 但这 
也是 如今 有关 自然语言 处理 的 一个 巨大 难题 我 
的 专业 是 信息 与 计算 科学 一个 结合 数学 
和 计算机 的 专业 看似 数学 和 计算机 并 没有 
什么 关系 其实不然 数学 是 一切 人类 文明 的 基础 
数学 的 逻辑 性 很强 一步步 的 算法 必须 通过 
严谨 的 认证 后 往下 算 一 开始 的 编程 
也许 很少 用到 数学 但是 到 写 程序 时 就要 
用 到 高中 数学 大学 高等数学 计算机 学习 离不开 数学 
很多 计算机 学科 的 建立 在 数学 模型 的 基础 
之上 的 编程 需要 逻辑 思维能力 而 恰好 逻辑思维 能力 
是 数学 所 培养 所以/c 编程/n 和/c 数学/n 有/v 很大/a 
的/uj 关系/n 我们 专业 也会 学到 很多 算法 并且 也 
要有 自己 能设/nr 计算法 的 能力 当然 本科 学习 并 
不会 上升 到 这种 高度 但随着 学习 的 进一步 加深 
对于 算法 设计 以 达到 优化 模型 优化 算法 的 
目的 现在 本 专业 本科 学习 既是 为了 有 一定 
的 数学 基础 是 为了 锻炼 思维能力 能为 今后 的 
学习 和 发展 添砖加瓦 我们 专业 结合 了 计算机 和 
数学 虽 暂时 还 没到 机器学习 和 数据挖掘 那种 高度 
但却 为 以后 学习 打下 坚实 基础 其实 也 可以 
就 机器语言 数据挖掘/n 和/c 自然/d 语言/n 处理/v 的/uj 关系/n 来说/u 
 /i 更加/d 明显/a 的/uj 对比/v 出/v 本/r 专业/n 和/c 自然/d 
语言/n 处理/v 的/uj 关系/n 因为 基本 的 机器 学习 和 
数据挖掘 我们 专业 也 会 涉及 其中 一部分 机器学习 就像 
内力 一样 是 一个 武者 的 基础 而 自然 语言 
和 数据挖掘 就是 如果 你 内功 深厚 招式 什么 的 
其实 就 很 简单 了 机器学习 数据挖掘 自然语言 处理 这三项 
并 不是 独立 的 选项 机器学习 需要 数据挖掘 和 自然 
语言 处理 的 支撑 自然语言 处理 需要 数据挖掘 的 支撑 
数据挖掘 需要 大 数据 的 支撑 最终 所有 的 根源 
都要 落实 在 大 数据 上 而 这 一切 的 
顶点 就是 人工智能 数据挖掘 与 机器 学习 是 两个 不同 
的 概念 数据挖掘 中 使用 到 机器 学习 的 各种 
工具 而 自然 语言 处理 也是 是 一种 机器学习 的 
方式 属于 数据挖掘 的 范畴 数据挖掘 英语 Data mining 又 
译为 资料 探勘 数据 采矿 它 是 数据库 知识发现 英语 
Knowledge Discovery in Databases 简称 KDD 中 的 一个 步骤 
数据挖掘/n 一般/a 是/v 指/n 从/p 大量/n 的/uj 数据/n 中/f 自动/vn 
搜索/v 隐藏/v 于/p 其中/r 的/uj 有着/v 特殊/a 关系/n 性/n 属于 
Association rule learning 的 信息 的 过程 数据挖掘 通常 与 
计算机 科学 有关 并 通过 统计 在线 分析处理 情报检索 机器学习 
专家系统 依靠 过去 的 经验法则 和 模式 识别 等 诸多 
方法 来 实现 上述 目标 机器学习 Machine Learning ML 是 
一门 多 领域 交叉 学科 涉及 概率论 统计学 逼近 论 
凸 分析 算法 复杂度 理论 等 多门 学科 专门研究 计算机 
怎样 模拟 或 实现 人类 的 学习 行为 以 获取 
新 的 知识 或 技能 重新组织 已有 的 知识 结构 
使之 不断 改善 自身 的 性能 它 是 人工智能 的 
核心 是 使 计算机 具有 智能 的 根本 途径 其 
应用 遍及 人工智能 的 各个 领域 它 主要 使用 归纳 
综合 而 不是 演绎 自然语言 处理 是 计算机 科学 领域 
与 人工智能 领域 中 的 一个 重要 方向 它 研究 
能 实现 人 与 计算机 之间 用 自然 语言 进行 
有效 通信 的 各种 理论 和 方法 自然语言 处理 是 
一门 融 语言学 计算机科学 数学 于 一体 的 科学 因此 
这一 领域 的 研究 将 涉及 自然语言 即 人们 日常 
使用 的 语言 所以 它 与 语言学 的 研究 有着 
密切 的 联系 但又 有 重要 的 区别 自然语言 处理 
并 不是 一般 地 研究 自然语言 而在于 研制 能 有效 
地 实现 自然 语言 通信 的 计算机系统 特别 是 其中 
的 软件 系统 因而 它 是 计算机 科学 的 一部分 
自然语言 处理 NLP 是 计算机 科学 人工智能 语言学 关注 计算机 
和 人类 自然 语言 之间 的 相互 作用 的 领域 
自然语言 处理 知识 太 庞大 了 网上 也 都是 一些 
零零散散 的 知识 比如 单独 讲 某些 模型 也 没有 
来龙去脉 学习 起来 较为 困难 于是 我 自己 总结 了 
一份 知识 体系 结构 不足之处 欢迎 指正 内容/n 来源/n 主要/b 
参考/v 黄志洪/nr 老师/n 的/uj 自然/d 语言/n 处理/v 课程/n 主要 参考书 
为 宗 成庆 老师 的 统计 自然语言 处理 虽然 很多 
内容 写 的 不清楚 但 好像 中文 NLP 书籍 就 
这 一本 全 一些 如果 想 看好 的 英文 资料 
可以 到 我 的 GitHub 上 下载 http / / 
github . com / lovesoft5 / ml 下面 直接 开始 
正文 一 自然语言 处理 概述 1 自然语言 处理 利用 计算机 
为 工具 对 书面 实行 或者 口头 形式 进行 各种各样 
的 处理 和 加工 的 技术 是 研究 人 与人 
交际 中 以及 人 与 计算机 交际 中的 演员 问题 
的 一门 学科 是 人工智能 的 主要 内容 2 自然语言 
处理 是 研究 语言 能力 和 语言 应用 的 模型 
建立 计算机 算法 框架 来 实现 这样 的 语言 模型 
并 完善 评测 最终 用于 设计 各种 实用 系统 3 
研究 问题 主要 信息检索 机器翻译 文档 分类 问答 系统 信息 
过滤 自动 文摘 信息 抽取 文本 挖掘 舆情 分析 机器 
写作 语音识别 研究 模式 自然语言 场景 问题 数学 算法 算法 
如何 应用 到 解决 这些 问题 预料 训练 相关 实际应用 
自然 语言 的 困难 场景 的 困难 语言 的 多样性 
多变性 歧义 性 学习 的 困难 艰难 的 数学 模型 
hmm crf EM 深度 学习 等 语料 的 困难 什么 
的 语料 语料 的 作用 如何 获取 语料 二 形式语言 
与 自动机 语言 按照 一定 规律 构成 的 句子 或者 
字符串 的 有限 或者 无限 的 集合 描述语言 的 三种 
途径 穷举法 文法 产生式系统 描述 自动机 自然 语言 不 是 
人为 设计 而 是 自然 进化 的 形式语言 比如 运算 
符号 化学 分子式 编程语言 形式语言 理论 朱 啊哟 研究 的 
是 内部 结构 模式 这类 语言 的 纯粹 的 语法 
领域 从 语言学 而来 作为 一种 理解 自然 语言 的 
句法 规律 在 计算机 科学 中 形式语言 通常 作为 定义 
编程 和 语法结构 的 基础 形式 语言 与 自动机 基础知识 
集合论 图论 自动机 的 应用 1 单词 自动 查错 纠正 
2 词性 消 歧 什么 是 词性 什么 的 词性 
标注 为什么 需要 标注 如何 标注 形式语言 的 缺陷 1 
对于 像 汉语 英语 这样 的 大型 自然 语言 系统 
难以 构造 精确 的 文法 2 不 符合 人类 学习 
语言 的 习惯 3 有些 句子 语法 正确 但在 语义上 
却 不 可能 形式语言 无法 排出 这些 句子 4 解决 
方向 基于 大量 语料 采用 统计学 手段 建立 模型 三 
语言 模型 1 语言 模型 重要 通过 语料 计算 某个 
句子 出现 的 概率 概率 表示 常用 的 有2/nr 元 
模型 3 元 模型 2 语言 模型 应用 语音识别 歧义 
消除 例如 给定 拼音串 ta shi yan yan jiu saun 
fa de 可能 的 汉字 串 踏实 烟酒 算法 的 
  他 是 研究 酸 法的/nr       他 
是 研究 算法 的 显然 最后 一句 才 符合 3 
语言 模型 的 启示 1 开启 自然语言 处理 的 统计 
方法 2 统计 方法 的 一般 步骤 收集 大量 语料 
对 语料 进行 统计分析 得出 知识 针对 场景 建立 算法 
模型 解释 和 应用 结果 4 语言 模型 性能评价 包括 
评价 目标 评价 的 难点 常用 指标 交叉 熵 困惑 
度 5 数据 平滑 数据 平滑 的 概念 为什么 需要 
平滑 平滑 的 方法 加 一 法 加法 平滑 法 
古德 图灵 法 J M 法 Katz 平滑 法等6/nr 语言 
模型 的 缺陷 语料 来自 不同 的 领域 而 语言 
模型 对 文本 类型 主题 等 十分 敏感 n 与 
相邻 的 n 1个 词 相关 假设 不是 很 成立 
四 概率 图 模型 生成 模型 与 判别 模型 贝叶斯 
网络 马尔科夫 链 与 隐 马尔科夫 模型 HMM 1 概率 
图 模型 概述 什么 的 概率 图 模型 参考 清华大学 
教材 概率 图 模型 2 马尔科夫 过程 定义 理解 3 
隐 马尔科夫 过程 定义 理解 HMM 的 三个 基本问题 定义 
解法 应用 注 第一 个 问题 涉及 最大 似 然 
估计法 第二 个 问题 涉及 EM 算法 第三 个 问题 
涉及 维 特比 算法 内容 很多 要 重点 理解 参考书 
李航 统计 学习 方法 网上 博客 笔者 github 五 马尔科夫 
网 最大熵 模型 条件 随 机场 CRF 1 HMM 的 
三个 基本 问题 的 参数 估计 与 计算 2 什么 
是 熵 3 EM 算法 应用 十分 广泛 好好 理解 
4 HMM 的 应用 5 层次化 马尔科夫 模型 与 马尔科夫 
网络 提出 原因 HMM 存在 两个 问题 6 最大熵 马尔科夫 
模型 优点 与 HMM 相比 允许 使用 特征 刻画 观察 
序列 训练 高效 缺点 存在 标记 偏置 问题 7 条件 
随 机场 及其 应用 概念 模型 过程 与 HMM 关系 
参数估计 方法 GIS 算法 改进 IIS 算法 CRF 基本问题 特征 
选取 特征 模板 概率 计算 参数 训练 解码 维 特比 
应用 场景 词性 标注 类 问题 现在 一般 用 RNN 
+ CRF 中文分词 发展过程 经典 算法 了解 开源 工具 jieba 
分词 中文 人名 地名 识别 8   CRF + + 
六 命名 实体 识别 词性 标注 内容 挖掘 语义分析 与 
篇章 分析 大量 用到 前面 的 算法 1 命名 实体 
识别 问题 相关 概率 定义 相关 任务 类型 方法 基于 
规程 基于 大 规模 语料库 2 未 登录 词 的 
解决 方法 搜索引擎 基于 语料 3 CRF 解决 命名 实体 
识别 NER 流程 总结 训练 阶段 确定 特征 模板 不同 
场景 人名 地名 等 所 使用 的 特征 模板 不同 
对 现有 语料 进行 分词 在 分词 结     
                  果 
基础 上 进行 词性 标注 可能 手工 NER 对应 的 
标注 问题 是 基于 词 的 然后 训练 CRF 模型 
得到 对应 权值 参数值 识别 过程 将 待 识别 文档 
分词 然后 送入 CRF 模型 进行 识别 计算 维 特比 
算法 得到 标注 序列 然后 根据 标       
                    
  注 划分 出 命名 实体 4 词性 标注 理解 
含义 意义 及其 一致性 检查 方法 位置 属性 向量 词性 
标注 序列 向量 聚 类 或者 分类 算法 七 句法分析 
1 句法分析 理解 以及 意义 1 句法结构 分析 完全 句法分析 
浅层 分析 这里 有 很多 方法 2 依存关系 分析 2 
句法 分析方法 1 基于 规则 的 句法结构 分析 2 基于 
统计 的 语法 结构 分析 八 文本 分类 情感 分析 
1 文本 分类 文本 排 重 文本 分类 在 预定义 
的 分类 体系 下 根据 文本 的 特征 将 给定 
的 文本 与 一个 或者 多 个 类别 相关联 典型 
应用 垃圾邮件 判定 网页 自动 分类 2 文本 表示 特征 
选取 与 权重 计算 词 向量 文本 特征选择 常用 方法 
1 基于 本文 频率 的 特征 提 取法 2 信息 
增 量法 3 X2 卡方 统计量 4 互信息 法3/nr 分类器 
设计 SVM 贝叶斯 决策树 等 4 分类器 性能 评测 1 
召回率 2 正确率 3 F1 值 5 主题 模型 LDA 
与 PLSALDA 模型 十分 强大 基于 贝叶斯 改进 了 PLSA 
可以 提取 出 本章 的 主题词 和 关键词 建模 过程 
复杂 难以理解 6 情感 分析 借助 计算机 帮助 用户 快速 
获取 整理 和 分析 相关 评论 信息 对 带有 感情 
色彩 的 主观 文本 进行 分析 处理 和 归纳 例如 
评论 自动 分析 水军 识别 某种 意义 上 看 情感 
分析 也 是 一种 特殊 的 分类 问题 7 应用 
案例 九 信息检索 搜索引擎 及其 原理 1 信息检索 起源于 图书馆 
资料 查询 检索 引入 计算机 技术 后 从 单纯 的 
文本 查询 扩展到 包含 图片 音 视频 等 多媒体 信息检索 
检索 对象 由 数据库 扩展 到 互联网 1 点对点 检索 
2 精确 匹配 模型 与 相关 匹配 模型 3 检索系统 
关键 技术 标引 相关度 计算 2 常见 模型 布尔 模型 
向量空间 模型 概率模型 3 常用 技术 倒排索引 隐 语义分析 LDA 
等 4 评测 指标 十 自动 文摘 与 信息 抽取 
机器翻译 问答 系统 1 统计 机器 翻译 的 的 思路 
过程 难点 以及 解决 2 问答 系统 基本 组成 问题 
分析 信息检索 答案 抽取 类型 基于 问题 答案 基于 自由 
文本 典型 的 解决 思路 3 自动 文摘 的 意义 
常用 方法 4 信息 抽取 模型 LDA 等 十一 深度 
学习 在 自然 语言 中 的 应用 1 单词 表示 
比如 词 向量 的 训练 wordvoc 2 自动 写 文本 
写 新闻 等 3 机器翻译 4 基于 CNN RNN 的 
文本 分类 5 深度 学习 与 CRF 结合 用于 词性 
标注 . . . . . . . . . 
. . . . . . 更多 深度 学习 内容 
可 参考 我 之前 的 文章 Attention 机制 是 最近 
深度 学习 的 一个 趋势 在 一次 采访 中 OpenAI 
的 研究 总监 Ilya Sutskever 说 attention 机制 是 最 
令人 兴奋 的 进步 之一 而且 已经 广为 使用 听起来 
激动人心 吧 但 attention 机制 究竟 是 什么 呢 神经 
网络 里 的 attention 机制 是 非常 松散地 基于 人类 
的 视觉 注意 机制 人类 的 视觉 注意 机制 已经 
被 充分 地 研究 过了 而且 提出 了 多个 不同 
的 模型 所有 的 模型 归根结底 都是/nr 按照 高分辨率 聚焦 
在 图片 的 某个 特定 区域 并以 低分辨率 感知 图像 
的 周边 区域 的 模式 然后 不断 地 调整 聚焦点 
Attention 在 神经网络 领域 有着 很长 的 历史 尤其 是 
在 图像 识别 领域 相关 的 论文 有 Learning to 
combine foveal glimpses with a third order Boltzmann machine 和 
Learning where to Attend with Deep Architectures for Image Tracking 
但 直到 最近 attention 机制 才 被 引入 NLP 界 
常用 的 视觉 领域 也 逐步 使用 的 递归 神经网络 
结构 中 这 正是 我们 这 篇 文章 的 主要 
关注 点 attention 解决 了 什么 问题 我们 以 神经 
机器翻译 Neural Machine Translation NMT 为例 来 理解 attention 能为/nr 
我们 做 什么 传统 的 机器 翻译 系统 通常 依赖于 
基于 文本 统计 特性 的 复杂 特征 工程 简而言之 这些 
系统 非常复杂 需要 投入 大量 工程 来 搭建 它们 神经 
机器翻译 系统 则 有所 区别 在 NMT 系统 里 我们 
把 一句话 的 意思 映射 为 一个 固定 长度 的 
表征 向量 然后 基于 此 向量 生成 翻译 文本 由于 
不 依赖 于 类似 n gram 计数 而是 捕捉 文本 
更高 层次 的 含义 NMT 系统 生成 的 翻译 语句 
比 大多数 其它 方法 都 要好 更 重要 的 是 
NMT 系统 的 搭建 和 训练 过程 更 方便 它们 
不 需要 任何 手工 的 特征 工程 事实上 TensorFlow 只 
需要 几 百行 代码 就能 实现 一个 简单 版本 大多数 
NMT 系统 使用 递归 神经网络 RNN 将 源 语句 比如 
一句 德语 编码 为 一个 向量 然后 同样 用 RNN 
将其 解码 为 英语 句子 如上 图 所示 Echt Dicke 
和 Kiste 依次 输入 到 编码器 中 一个 特殊字符 标志 
输入 结束 图中 未 显示 然后 解码器 开始 生成 翻译 
的 语句 解码器 持续 逐词 地 生成 直到 生成 句子 
的 终止符 这里 的 h 向量 表示 了 编码器 的 
内部 状态 如果 你 仔细 观察 你 会 发现 解码器 
在 翻译 时仅/nr 依赖 编码器 最后 的 隐藏 状态 上图 
的 h3 h3 向量 必须 对 源 句子 的 所有 
内容 都 进行 编码 它 必须 充分 地 捕捉 含义 
用 专业 术语 来说 这个 向量 就是 一个 sentence embedding 
事实上 如果 你 用 PCA 或者 t SNE 降 维 
之后 将 不同 句子 的 embedding 绘制 出来 你 将 
看到 语义 相近 的 句子 彼此 很 接近 真是 令人 
觉得 神奇 然而 我们 似乎 无法 把 一个 很长 的 
句子 所 包含 的 所有 信息 编码 成 一个 向量 
然后 解码器 仅 根据 这个 向量 生成 完美 的 翻译 
这种 假设 显得 不可理喻 我们 假设 原文 句子 长度 有 
50个 单词 英文 译文 的 第一 个 单词 可能 与 
原文 的 第一 个 单词 高度 相关 但 这 意味着 
解码器 必须 考虑 50步 之前 的 信息 而且 那段 信息 
需要 以 某种 形式 已经 被 编入 向量 中 众所周知 
RNN 在 处理 这类 长距离 依赖 关系 时会/nr 出现 问题 
理论上 LSTM 这类 结构 能够 处理 这个 问题 但在 实践 
中 长距离 依赖 关系 仍旧 是 个 问题 例如 研究 
人员 发现 将 原文 倒序 将其 倒序 输入 编码器 产生 
了 显著 改善 的 结果 因为 从 解码器 到 编码器 
对应 部分 的 路径 被 缩短 了 同样 两次 输入 
同一个 序列 似乎 也 有助于 网络 更好 地 记忆 我 
认为 倒序 句子 这种方法 属于 hack 手段 它 属于 被 
实践 证明 有效 的 方法 而 不是 有 理论 依据 
的 解决 方法 大多数 翻译 的 基准 都是 用 法语 
德语 等 语种 它们 和 英语 非常 相似 即使 汉语 
的 词序 与 英语 也 极其 相似 但是 有些 语种 
像 日语 句子 的 最后 一个 词语 在 英语 译 
文中 对 第一 个 词语 有 高度 预言性 那么 倒序 
输入 将 使得 结果 更 糟糕 还有 其它 办法 吗 
那 就是 Attention 机制 有了 Attention 机制 我们 不再 需要 
将 完整 的 原文 句子 编码 为 固定 长度 的 
向量 相反 我们 允许 解码器 在 每一步 输出 时 参与 
attend 到 原文 的 不同 部分 尤为 重要 的 是 
我们 让 模型 根据 输入 的 句子 以及 已经 产生 
的 内容 来 决定 参与 什么 因此 在 形式 非常 
相似 的 语种 之间 如 英语 与 德语 解码器 可能会 
选择 顺序 地 参与 事情 生成 第一 个 英语 词语 
时 参与 原文 的 第一 个 词语 以此类推 这 正是 
论文 Neural Machine Translation by Jointly Learning to Align and 
Translate 的 成果 如下 图 所示 y 是 编码器 生成 
的 译文 词语 x 是 原文 的 词语 上图 使用 
了 双向 递归 网络 但这 并 不是 重点 你 先 
忽略 反向 的 路径 吧 重点 在于 现在 每个 解码器 
输出 的 词语 yt 取决于 所有 输入 状态 的 一个 
权重 组合 而 不只 是 最后 一个 状态 a 是 
决定 每个 输入 状态 对 输出 状态 的 权重 贡献 
因此 如果 a3 2 的 值 很大 这 意味着 解码器 
在 生成 译文 的 第三 个 词语 时 会 更 
关注 与 原文 句子 的 第二个 状态 a 求和 的 
结果 通常 归一化 到 1 因此 它 是 输入 状态 
的 一个 分布 Attention 机制 的 一个 主要 优势 是 
它 让 我们 能够 解释 并 可视化 整个 模型 举个 
例子 通过 对 attention 权重 矩阵 a 的 可视化 我们 
能够 理解 模型 翻译 的 过程 我们 注意到 当 从 
法语 译为 英语 时 网络 模型 顺序 地 关注 每个 
输入 状态 但 有时 输出 一个 词语 时会 关注 两个 
原文 的 词语 比如 将 la Syrie 翻译 为 Syria 
Attention 的 成本 如果 再 仔细 观察 attention 的 等式 
我们 会 发现 attention 机制 有 一定 的 成本 我们 
需要 为 每个 输入输出 组合 分别 计算 attention 值 50个 
单词 的 输入 序列 和 50个 单词 的 输出 序列 
需要 计算 2500个 attention 值 这还 不算 太 糟糕 但 
如果 你 做 字符 级别 的 计算 而且 字符 序列 
长达 几百 个 字符 那么 attention 机制 将 会 变得 
代价 昂贵 其实 它 和 我们 的 直觉 恰恰相反 人类 
的 注意力 是 节省 计算 资源 的 当 专注 于 
一件事 时 我们 能 忽略 其它 事情 但这 并 不是 
我们 上 一个 模型 的 作法 我们 在 决定 专注 
于 某个 方面 之前 先 仔细 观察 每件事 直观 地 
说 这 相当于 输出 一个 翻译 后的/nr 词语 然后 遍历 
记忆 里 所有 文本 再 决定 下 一个 输出 什么 
这 似乎 是 一种 浪费 而且 没人 会 这么 干 
事实上 它 更 类似于 内存 访问 不是 attention 在我看来 有点儿 
用词不当 下文 会 继续 讨论 不过 这 并 没有 阻碍 
attention 机制 的 流行 传播 attention 的 另一 种 替代 
方法 是 用 强化 学习 Reinforcement Learning 来 预测 关注点 
的 大概 位置 这 听 起来 更 像是 人 的 
注意力 这 也是 Recurrent Models of Visual Attention 文中 的 
作法 然而 强化 学习 模型 不能 用 反向 传播 算 
法端 到 端 训练 因此 它 在 NLP 的 应用 
不是 很 广泛 机器翻译 之外 领域 的 Attention 机制 到 
目前 为止 我们 已经 见识 了 attention 在 机器 翻译 
领域 的 应用 但 上述 的 attention 机制 同样 也能 
应用于 递归 模型 让 我们 再 来看 几个 例子 在 
Show Attend and Tell 一 文中 作者 将 attention 机制 
应用于 生成 图片 的 描述 他们 用 卷积 神经 网络 
来 编码 图片 并用 一个 递归 神经网络 模型 和 attention 
机制 来 生成 描述 通过 对 attention 权重 值 的 
可视化 就如 之前 机器 翻译 的 例子 一样 在 生成 
词语 的 同时 我们 能 解释 模型 正在 关注 哪个 
部分 在 Grammar as a Foreign Language 论文 中 作者 
用 递归 神经网络 模型 和 attention 机制 的 来 生成 
语法 分析树 可视化 的 attention 矩阵 让人 深入 地 了解 
网络 模型 如何 生成 这些 树 在 Teaching Machines to 
Read and Comprehend 论文 里 作者 利用 RNN 模型 读入 
文本 先读 入 一个 合成 的 问题 然后 产生 一个 
答案 通过 将 attention 可视化 我们 可以 看到 网络 模型 
在 试图 寻找 问题 答案 的 时候 关注 哪些方面 ATTENTION 
= FUZZY MEMORY attention 机制 解决 的 根本 问题 是 
允许 网络 返回 到 输入 序列 而 不是 把 所有 
信息 编码 成 固定 长度 的 向量 正如 我 在 
上面 提到 我 认为 使用 attention 有点儿 用词不当 换句话说 attention 
机制 只是 简单 地 让 网络 模型 访问 它 的 
内部 存储器 也 就是 编码器 的 隐藏 状态 在 这种 
解释 中 网络 选择 从 记忆 中 检索 东西 而 
不是 选择 注意 什么 不同于 典型 的 内存 这里 的 
内存 访问 机制 是 弹性 的 也 就是说 模型 检索 
到 的 是 所有 内存 位置 的 加权 组合 而 
不是 某个 独立 离散 位置 的 值 弹性 的 内存 
访问 机制 好 处 在于 我们 可以 很容易 地 用 
反向 传播 算 法端 到 端 地 训练 网络 模型 
虽然有 non fuzzy 的 方法 其中 的 梯度 使用 抽样 
方法 计算 而 不是 反向 传播 记忆 机制 本身 的 
历史 更 久远 标准 递归 网络 模型 的 隐藏 状态 
本身 就是 一种 内部 记忆 RNN 由于 存在 梯度 消失 
问题 而 无法 从 长距离 依赖 学习 LSTM 通过 门 
控 机制 对此 做 了 改善 它 允许 显 式 
的 记忆 删除 和 更新 更 复杂 的 内存 结构 
的 趋势 还 在 延续 End To End Memory Networks 
一 文中 的 方法 允许 网络 在 输出 内容 前 
多次 读入 相同 的 序列 每一步 都 更新 记忆 内容 
举个 例子 输入 一个 故事 在 经过 多步 推理 之后 
回答 一个 问题 然而 当 网络 参数 的 权重 以 
某种 特定 方式 被 绑定 端 到 端 记忆 网络 
的 记忆 机制 就 和 这里 所 介绍 的 attention 
机制 一样 了 只是 它 是 多跳 的 记忆 因为 
它 试图 整合 多个 句子 信息 神经 图灵 机器 使用 
类似 的 记忆 机制 但 有一个 更 复杂 的 解决方案 
它 同时 基于 内容 如 在 这里 和 位置 使 
网络 模型 通过学习 模式 来 执行 简单 的 计算机程序 比如 
排序算法 在将来 我们/r 很/zg 可能/v 看到/v 记忆/n 机制/n 和/c attention/w 
机制/n 之间/f 有更/nr 清晰/a 的/uj 区别/n 也许 是 沿着 Reinforcement 
Learning Neural Turing Machines 它 尝试 学习 访问 模式 来 
处理 外部 接口 原文 地址 ATTENTION AND MEMORY IN DEEP 
LEARNING AND NLP 译者 / 赵屹华/nr 审校 / 刘翔宇 责编 
/ 仲浩/nr 译者 简介 赵屹华/nr 计算 广告 工程师 @ 搜狗 
前 生物医学 工程师 关注 推荐算法 机器学习 领域 https / / 
mp . weixin . qq . com / s / 
rmANEGi _ a9k19Bxeidxf Q 作者简介 小郭 计算机 专业 在读 硕士 
研究生 AI 学习 与 爱好者 欢迎 交流 留言 或者 邮箱 
guo _ jc5 @ 163 . com 本文 选自 CSDN 
博客 自然语言 处理 知识 太 庞大 了 网上 也 都是 
一些 零零散散 的 知识 比如 单独 讲 某些 模型 也 
没有 来龙去脉 学习 起来 较为 困难 于是 我 自己 总结 
了 一份 知识 体系 结构 内容/n 来源/n 主要/b 参考/v 黄志洪/nr 
老师/n 的/uj 自然/d 语言/n 处理/v 课程/n 主要 参考书 为 宗 
成庆 老师 的 统计 自然语言 处理 可能 很多 内容 写 
的 不清楚 但 好像 中文 NLP 书籍 就 这 一本 
全 一些 如果 想 看好 的 英文 资料 可以 到 
我 的 GitHub 上 下载 http / / github . 
com / lovesoft5 / ml 下面 直接 开始 正文 ▌ 
一 自然语言 处理 概述 1 自然语言 处理 利用 计算机 为 
工具 对 书面 实行 或者 口头 形式 进行 各种各样 的 
处理 和 加工 的 技术 是 研究 人 与人 交际 
中 以及 人 与 计算机 交际 中的 演员 问题 的 
一门 学科 是 人工智能 的 主要 内容 2 自然语言 处理 
是 研究 语言 能力 和 语言 应用 的 模型 建立 
计算机 算法 框架 来 实现 这样 的 语言 模型 并 
完善 评测 最终 用于 设计 各种 实用 系统 3 研究 
问题 主要 信息检索 机器翻译 文档 分类 问答 系统 信息 过滤 
自动 文摘 信息 抽取 文本 挖掘 舆情 分析 机器 写作 
语音识别 研究 模式 自然语言 场景 问题 数学 算法 算法 如何 
应用 到 解决 这些 问题 预料 训练 相关 实际应用 自然 
语言 的 困难 场景 的 困难 语言 的 多样性 多变性 
歧义 性 学习 的 困难 艰难 的 数学 模型 hmm 
crf EM 深度 学习 等 语料 的 困难 什么 的 
语料 语料 的 作用 如何 获取 语料 ▌ 二 形式语言 
与 自动机 语言 按照 一定 规律 构成 的 句子 或者 
字符串 的 有限 或者 无限 的 集合 描述语言 的 三种 
途径 穷举法 文法 产生式系统 描述 自动机 自然 语言 不 是 
人为 设计 而 是 自然 进化 的 形式语言 比如 运算 
符号 化学 分子式 编程语言 形式语言 理论 朱 啊哟 研究 的 
是 内部 结构 模式 这类 语言 的 纯粹 的 语法 
领域 从 语言学 而来 作为 一种 理解 自然 语言 的 
句法 规律 在 计算机 科学 中 形式语言 通常 作为 定义 
编程 和 语法结构 的 基础 形式 语言 与 自动机 基础知识 
集合论 图论 自动机 的 应用 单词 自动 查错 纠正 词性 
消 歧 什么 是 词性 什么 的 词性 标注 为什么 
需要 标注 如何 标注 形式语言 的 缺陷 对于 像 汉语 
英语 这样 的 大型 自然 语言 系统 难以 构造 精确 
的 文法 不符合 人类 学习 语言 的 习惯 有些 句子 
语法 正确 但在 语义上 却 不 可能 形式语言 无法 排出 
这些 句子 解决 方向 基于 大量 语料 采用 统计学 手段 
建立 模型 ▌ 三 语言 模型 1 语言 模型 重要 
通过 语料 计算 某个 句子 出现 的 概率 概率 表示 
常用 的 有2/nr 元 模型 3 元 模型 2 语言 
模型 应用 语音识别 歧义 消除 例如 给定 拼音串 t a 
s h i y a n y a n j 
i u s a u n f a d e 
可能 的 汉字 串 踏实 烟酒 算法 的 他 是 
研究 酸 法的他/nr 是 研究 算法 的 显然 最后 一句 
才 符合 3 语言 模型 的 启示 开启 自然语言 处理 
的 统计 方法 统计 方法 的 一般 步骤 收集 大量 
语料 对 语料 进行 统计分析 得出 知识 针对 场景 建立 
算法 模型 解释 和 应用 结果 4 语言 模型 性能评价 
包括 评价 目标 评价 的 难点 常用 指标 交叉 熵 
困惑 度 5 数据 平滑 数据 平滑 的 概念 为什么 
需要 平滑 平滑 的 方法 加 一 法 加法 平滑 
法 古德 图灵 法 J M 法 Katz 平滑 法等/nr 
6 语言 模型 的 缺陷 语料 来自 不同 的 领域 
而 语言 模型 对 文本 类型 主题 等 十分 敏感 
n 与 相邻 的 n 1个 词 相关 假设 不是 
很 成立 ▌ 四 概率 图 模型 生成 模型 与 
判别 模型 贝叶斯 网络 马尔科夫 链 与 隐 马尔科夫 模型 
HMM 1 概率 图 模型 概述 什么 的 概率 图 
模型 参考 清华大学 教材 概率 图 模型 2 马尔科夫 过程 
定义 理解 3 隐 马尔科夫 过程 定义 理解 HMM 的 
三个 基本问题 定义 解法 应用 注 第一 个 问题 涉及 
最大 似 然 估计法 第二 个 问题 涉及 EM 算法 
第三 个 问题 涉及 维 特比 算法 内容 很多 要 
重点 理解 参考书 李航 统计 学习 方法 网上 博客 笔者 
github ▌ 五 马尔科夫 网 最大熵 模型 条件 随 机场 
CRF 1 HMM 的 三个 基本 问题 的 参数 估计 
与 计算 2 什么 是 熵 3 EM 算法 应用 
十分 广泛 好好 理解 4 HMM 的 应用 5 层次化 
马尔科夫 模型 与 马尔科夫 网络 提出 原因 HMM 存在 两个 
问题 6 最大熵 马尔科夫 模型 优点 与 HMM 相比 允许 
使用 特征 刻画 观察 序列 训练 高效 缺点 存在 标记 
偏置 问题 7 条件 随 机场 及其 应用 概念 模型 
过程 与 HMM 关系 参数估计 方法 GIS 算法 改进 IIS 
算法 CRF 基本问题 特征 选取 特征 模板 概率 计算 参数 
训练 解码 维 特比 应用 场景 词性 标注 类 问题 
现在 一般 用 RNN + CRF 中文分词 发展过程 经典 算法 
了解 开源 工具 jieba 分词 中文 人名 地名 识别 8 
CRF + + ▌ 六 命名 实体 识别 词性 标注 
内容 挖掘 语义分析 与 篇章 分析 大量 用到 前面 的 
算法 1 命名 实体 识别 问题 相关 概率 定义 相关 
任务 类型 方法 基于 规程 基于 大 规模 语料库 2 
未 登录 词 的 解决 方法 搜索引擎 基于 语料 3 
CRF 解决 命名 实体 识别 NER 流程 总结 训练 阶段 
确定 特征 模板 不同 场景 人名 地名 等 所 使用 
的 特征 模板 不同 对 现有 语料 进行 分词 在 
分词 结果 基础 上 进行 词性 标注 可能 手工 NER 
对应 的 标注 问题 是 基于 词 的 然后 训练 
CRF 模型 得到 对应 权值 参数值 识别 过程 将 待 
识别 文档 分词 然后 送入 CRF 模型 进行 识别 计算 
维 特比 算法 得到 标注 序列 然后 根据 标注 划分 
出 命名 实体 4 词性 标注 理解 含义 意义 及其 
一致性 检查 方法 位置 属性 向量 词性 标注 序列 向量 
聚 类 或者 分类 算法 ▌ 七 句法分析 1 句法分析 
理解 以及 意义 1 句法结构 分析 完全 句法分析 浅层 分析 
这里 有 很多 方法 2 依存关系 分析 2 句法 分析方法 
基于 规则 的 句法结构 分析 基于 统计 的 语法 结构 
分析 ▌ 八 文本 分类 情感 分析 1 文本 分类 
文本 排 重 文本 分类 在 预定义 的 分类 体系 
下 根据 文本 的 特征 将 给定 的 文本 与 
一个 或者 多 个 类别 相关联 典型 应用 垃圾邮件 判定 
网页 自动 分类 2 文本 表示 特征 选取 与 权重 
计算 词 向量 文本 特征选择 常用 方法 基于 本文 频率 
的 特征 提 取法 信息 增 量法 X2 卡方 统计量 
互信息 法3/nr 分类器 设计 SVM 贝叶斯 决策树 等 4 分类器 
性能 评测 召回率 正确率 F1 值 5 主题 模型 LDA 
与 PLSALDA 模型 十分 强大 基于 贝叶斯 改进 了 PLSA 
可以 提取 出 本章 的 主题词 和 关键词 建模 过程 
复杂 难以理解 6 情感 分析 借助 计算机 帮助 用户 快速 
获取 整理 和 分析 相关 评论 信息 对 带有 感情 
色彩 的 主观 文本 进行 分析 处理 和 归纳 例如 
评论 自动 分析 水军 识别 某种 意义 上 看 情感 
分析 也 是 一种 特殊 的 分类 问题 7 应用 
案例 ▌ 九 信息检索 搜索引擎 及其 原理 1 信息检索 起源于 
图书馆 资料 查询 检索 引入 计算机 技术 后 从 单纯 
的 文本 查询 扩展到 包含 图片 音 视频 等 多媒体 
信息检索 检索 对象 由 数据库 扩展 到 互联网 点对点 检索 
精确 匹配 模型 与 相关 匹配 模型 检索系统 关键技术 标引 
相关度 计算 2 常见 模型 布尔 模型 向量空间 模型 概率模型 
3 常用 技术 倒排索引 隐 语义分析 LDA 等 4 评测 
指标 ▌ 十 自动 文摘 与 信息 抽取 机器翻译 问答 
系统 1 统计 机器 翻译 的 的 思路 过程 难点 
以及 解决 2 问答 系统 基本 组成 问题 分析 信息检索 
答案 抽取 类型 基于 问题 答案 基于 自由 文本 典型 
的 解决 思路 3 自动 文摘 的 意义 常用 方法 
4 信息 抽取 模型 LDA 等 ▌ 十一 深度 学习 
在 自然 语言 中 的 应用 1 单词 表示 比如 
词 向量 的 训练 wordvoc 2 自动 写 文本 写 
新闻 等 3 机器翻译 4 基于 CNN RNN 的 文本 
分类 5 深度 学习 与 CRF 结合 用于 词性 标注 
. . . . . . . . . . 
. . . . . 原文 地址 https / / 
blog . csdn . net / meihao5 / article / 
details / 79592667 完 中文 自然语言 处理 的 完整 机器 
处理 流程 1 . 获取 语料 读取 原始数据 语言 材料 
文本 集合 2 . 语料 预处理 数据 清洗 1 . 
数据 清洗 整理 出 感兴趣 的 内容 2 . 分词 
将 文本 全部 进行 分词 基于 字符串 匹配 统计 的 
分词 方法 规则 的 分词 方法 3 . 词性 标注 
形容词 动词 名词 等 4 . 去 停用词 标点符号 人称 
语气词 等 由 具体 场 景定 3 . 特征 工程 
1 . 词 袋 模型 不考虑 出现 的 顺序 直接 
放 一个 集合 统计 出现 的 次数 频率 2 . 
词 向量 将 字 词语 转换成 向量 矩阵 的 计算 
模型 4 . 特征选择 特征选择 方法 DF MI IG CHI 
WLLR WFO 六种 5 . 模型 训练 1 . 注意 
过拟合 欠 拟合 问题 不断 提高 模型 的 泛化 能力 
常见 的 解决 方法 有 增大 数据 的 训练 量 
增加 正则化 项 如 L1 正则 和 L2 正则 特征 
选取 不合理 人工 筛选 特征 和 使用 特征选择 算法 采用 
Dropout 方法 等 欠 拟合 就是 模型 不 能够 很好 
地 拟合 数据 表现 在 模型 过于 简单 常见 的 
解决 方法 有 添加 其他 特征 项 增加 模型 复杂度 
比如 神经网络 加 更多 的 层 线性 模型 通过 添加 
多项式 使 模型 泛化 能力 更强 减少 正则化 参数 正则化 
的 目的 是 用来 防止 过拟合 的 但是 现在 模型 
出现 了 欠 拟合 则 需要 减少 正则化 参数 2 
. 对于 神经网络 注意 梯度 消 失和 梯度 爆炸 问题 
6 . 评价 指标 7 . 模型 上线 应用 8 
. 模型 重构 参考文献 中文 自然语言 处理 入门 百度 词条 
自然语言 处理 是 计算机 科学 领域 与 人工智能 领域 中 
的 一个 重要 方向 它/r 研究/vn 能/v 实现/v 人/n 与/p 
计算机/n 之间/f 用/p 自然/d 语言/n 进行/v 有效/a 通信/l 的/uj 各种/r 
理论/n 和/c 方法/n 自然语言 处理 是 一门 融 语言学 计算机科学 
数学 于 一体 的 科学 因此 这一 领域 的 研究 
将 涉及 自然语言 即 人们 日常 使用 的 语言 所以 
它 与 语言学 的 研究 有着 密切 的 联系 但又 
有 重要 的 区别 自然语言 处理 并 不是 一般 地 
研究 自然语言 而在于 研制 能 有效 地 实现 自然 语言 
通信 的 计算机 系统 特别 是 其中 的 软件 系统 
因而 它 是 计算机 科学 的 一部分 自然语言 处理 NLP 
是 计算机 科学 人工智能 语言学 关注 计算机 和 人类 自然 
语言 之间 的 相互 作用 的 领域 Natural language processing 
NLP is a field of computer science artificial intelligence and 
computational linguistics concerned with the interactions between computers and human 
natural languages and in particular concerned with programming computers to 
fruitfully process large natural language corpora . Challenges in natural 
language processing frequently involve natural language understanding NLU natural language 
generation frequently from formal machine readable logical forms connecting language 
and machine perception dialog systems or some combination thereof . 
随着 深度 学习 的 发展 LSTM 的 应用 取得 的 
突破 极大 地 促进 了 NLP 的 发展 自然语言 处理 
的 主要 范畴 有 以下 文本 朗读 Text to speech 
/ 语音合成 Speech synthesis 语音识别 Speech recognition 中文 自动 分词 
Chinese word segmentation 词性 标注 Part of speech tagging 句法分析 
Parsing 自然语言 生成 Natural language generation 文本 分类 Text categorization 
信息检索 Information retrieval 信息 抽取 Information extraction 文字 校对 Text 
proofing 问答 系统 Question answering 给 一句 人类 语言 的 
问 定 决定 其 答案 典型 问题 有 特定 答案 
像是 加拿大 的 首都 叫什么 但也 考虑 些 开放式 问句 
像是 人生 的 意义 是 是 什么 机器翻译 Machine translation 
将 某种 人类 语言 自动 翻译 至 另一 种 语言 
自动 摘要 Automatic summarization 产生 一段 文字 的 大意 通常用于 
提供 已知 领域 的 文章 摘要 例如 产生 报纸 上 
某 篇文章 之 摘要 文字 蕴涵 Textual entailment 自然语言 处理 
目前 研究 的 难点 单词 的 边界 界 定在 口语 
中 词 与 词 之间 通常 是 连贯 的 而 
界定 字词 边界 通常 使用 的 办法 是 取用 能让 
给定 的 上下文 最为 通顺 且 在 文法上 无误 的 
一种 最佳 组合 在 书写 上 汉语 也 没有 词 
与 词 之间 的 边界 词义 的 消 歧 许 
多字词 不单 只有 一个 意思 因而 我们 必须 选 出使 
句 意 最为 通顺 的 解释 句法 的 模糊性 自然 
语言 的 文法 通常 是 模棱两可 的 针对 一个 句子 
通常 可能会 剖析 Parse 出 多棵 剖析 树 Parse Tree 
而 我们 必须 要 仰赖 语意 及 前后文 的 资讯 
才能 在 其中 选择 一棵 最为 适合 的 剖析 树 
有/v 瑕疵/n 的/uj 或不/i 规范/n 的/uj 输入/v 例如/v 语音/n 处理/v 
时/n 遇到/v 外国/ns 口音/n 或/c 地方/n 口音/n 或者 在 文本 
的 处理 中 处理 拼写 语法 或者 光学 字元 识别 
OCR 的 错误 语言 行为 与 计划 句子 常常 并不 
只是 字面 的 意思 例如 你 能把 盐 递 过来 
吗 一个 好 的 回答 应当 是 动手 把 盐 
递过去 在 大多数 上下文 环境 中 能 将 是 糟糕 
的 回答 虽说 回答 不 或者 太远 了 我 拿不到 
也 是 可以 接受 的 再者 如果 一门 课程 去年 
没 开设 对于 提问 这门 课程 去年 有 多少 学生 
没 通过 回答 去年 没开 这门 课 要比 回答 没人 
没 通过 好 当前 自然语言 处理 研究 的 发展 趋势 
第一 传统 的 基于 句法 语义 规则 的 理性主义 方法 
受到 质疑 随着 语料库 建设 和 语料库 语言学 的 崛起 
大 规模 真实 文本 的 处理 成为 自然 语言 处理 
的 主要 战略 目标 第二 统计 数学方法 越来越 受到 重视 
自然语言 处理 中 越来越 多 地 使用 机器 自动 学习 
的 方法 来 获取 语言 知识 第三 浅层 处理 与 
深层 处理 并重 统计 与 规则 方法 并重 形成 混合式 
的 系统 第四 自然语言 处理 中 越来越 重视 词汇 的 
作用 出现 了 强烈 的 词汇 主义 的 倾向 词汇 
知识库 的 建造 成为 了 普遍 关注 的 问题 第五 
统计 自然语言 处理 统计 自然语言 处理 运用 了 推测学 机率 
统计 的 方法 来 解决 上述 尤其 是 针对 容易 
高度 模糊 的 长串 句子 当 套用 实际 文法 进行 
分析 产生 出 成千上万 笔 可能性 时所/nr 引发 之 难题 
处理 这些 高度 模糊 句子 所 采用 消 歧 的 
方法 通常 运用 到 语料库 以及 马 可夫 模型 Markov 
models 统计 自然语言 处理 的 技术 主要 由 同样 自 
人工智能 下 与 学习 行为 相关 的 子 领域 机器学习 
及 资料 采掘 所 演进 而成 转 自 维基百科 NLP 
自然语言 处理 Natural Language Processing 是 人工 智能 和 语言学 
领域 的 分支 学科 主要 包括 自然 语言 理解 和 
生成 自然语言 理解 系统 把 自然 语言 转化 为 计算机程序 
更 易于 处理 的 形式 即 让 电脑 懂 人类 
的 语言 自然语言 生成 系统 把 计算机 数据 转化 自然语言 
处理过程 形式化 描述 数学模型 算法 化 程序化 实用化 使用 Python 
语言 首先 需要 安装 numpy matplotlib 库 也 可以 安装 
Anaconda 实现 KNN 算法 实现 预测 功能 KNN K nearest 
Neighbor 邻近 算法 或者说 K 最 近邻 kNN k NearestNeighbor 
分类 算法 是 数据挖掘 分类 技术 中 最简单 的 方法 
之一 所谓 K 最 近邻 就是 k 个 最近 的 
邻居 的 意思 说 的 是 每个 样本 都 可以 
用 它 最 接近 的 k 个 邻居 来 代表 
KNN 算法 思想 计算 一直 类别 中 数据集 的 点 
与 当前 点 的 距离 计算 样本 距离 并 排序 
选取 距离 样本 最近 的 K 个 点 确定 K 
个 点 所在 类别 的 出现 频率 返回 K 个 
点 出现 频率 最高 的 类别 作为 预测 结果 KNN 
算法 模型 流程 与 实现 1 . 搜集 数据 数据采集 
过程 分 为非 结构化 数据 和 结构化 数据 如 网络爬虫 
数据库 文件 等 2 . 准备 数据 格式化 处理 对 
不同 类别 的 数据 进行 处理 如 转为 统一 csv 
格式 3 . 分析 数据 主要 看 数据 特点 有 
没有 缺失 数据 离散性 还是 连续性 进而 选择 不同 模型 
跟着 网上 视频 敲 的 代码 完整 如下 # coding 
utf 8 NLP 自然 语言 学习 import numpy as np 
import matplotlib . pyplot as plt import matplotlib . font 
_ manager as fm import math operator # 中文 乱码 
myfont = fm . FontProperties fname = C \ Windows 
\ Fonts \ simsunb . ttf # 只 支持 后缀 
ttc plt . rcParams font . sans serif = SimHei 
数据 保存 到 文件 中 def create _ dataset datasets 
= np . array 8 4 2 7 1 1 
1 4 4 3 0 5 # 数据集 labels = 
非常 热 非常 热 一般 热 一般 热 # 类 
标签 return datasets labels def create _ datasets datasets = 
np . array 8 4 2 7 1 1 1 
4 4 3 0 5 3 0 4 5 2 
1 5 3 2 # 数据集 labels = 0 0 
1 1 0 0 1 # 非常 热 非常 热 
一般 热 一般 热 一般 热 # 类 标签 return 
datasets labels 可视化 分析 数据 def analyze _ data _ 
plot x y fig = plt . figure ax = 
fig . add _ subplot 111 ax . scatter x 
y # plt . scatter x y # 设置 散点图 
标题 和 横坐标 # plt . title 冷热 感知 图 
fontsize = 25 fontproperties = myfont plt . title 冷热 
感知 图 fontsize = 25 # plt . xlabel 冰淇淋 
fontsize = 15 fontproperties = myfont plt . xlabel 冰淇淋 
fontsize = 15 # plt . ylabel 喝水 fontsize = 
15 fontproperties = myfont plt . ylabel 喝水 fontsize = 
15 # 自动 保存 plt . savefig result . png 
bbox _ inches = tight plt . show 构造 KNN 
分类器 def knn _ classifier newV datasets labels k # 
1 . 计算 样本数据 和 样本 库 数据 的 距离 
sqrtDist = EuclideanDis3 newV datasets # 2 . 根据 距离 
排序 按照 列 向量 排序 sortedDistIndexs = sqrtDist . argsort 
axis = 0 # 3 . 针对 k 个 值 
统计 各个 类别 的 数量 classCount = { } for 
i in range k # 根据 距离 排序 索引 值 
找到 类 标签 votelabel = labels sortedDistIndexs i # 统计 
类 标签 的 键值 对 classCount votelabel = classCount . 
get votelabel 0 + 1 # 4 . 投票 机制 
少数 服从 多数 原则 # 对 各个 分类 字典 进行 
排序 降序 按照 值 sortedClassCount = sorted classCount . items 
key = operator . itemgetter 1 reverse = True # 
print 结果 预测 sortedClassCount 0 0 return sortedClassCount 0 0 
欧式 距离 计算 d2 = x1 x2 2 + y1 
y2 2 def c o m p u t e 
E u c l i d e a n D 
i s x1 x2 y1 y2 d = math . 
sqrt math . pow x1 x2 2 + math . 
pow y1 y2 2 return d 欧式 距离 计算 优化 
公式 def EuclideanDis instance1 instance2 d = 0 length = 
len instance1 for x in range length d + = 
math . pow instance1 x instance2 x 2 return math 
. sqrt d 欧式 距离 计算 3 大量 数据 计算 
def EuclideanDis3 newV datasets # 获取 向量 维度 rowsize colsize 
= datasets . shape # 各 特征向量 间做 差值 diffMat 
= np . tile newV rowsize 1 datasets # 差值 
平方 sqDiffMat = diffMat * * 2 # 差值 开方 
求和 sqrtDist = sqDiffMat . sum axis = 1 * 
* 0.5 return sqrtDist 利用 KNN 随机 预测 访客 天气 
感知 度 def predict _ temperature # 创建 数据集 和类/nr 
标签 datasets labels = create _ dataset newV = 2 
4 4 iceCream = float input Q 请问 你 今天 
吃 了 几个 冰淇淋 \ n drinkWater = float input 
Q 请问 你 今天 喝了 几瓶 水 \ n playHours 
= float input Q 请问 你 今天 在 户外 玩了 
几个 小时 \ n newV = np . array iceCream 
drinkWater playHours # vecs = np . array 2 4 
4 3 0 0 5 7 2 # for i 
in vecs res = knn _ classifier newV datasets labels 
3 print KNN 天气 预测 结果 res 使用 机器学习 库 
sklearn 实现 预测 from sklearn import neighbors def knn _ 
sklearn _ predict # 调用 机器学习 库 knn 分类器 算法 
knn = neighbors . K N e i g h 
b o r s C l a s s i 
f i e r datasets labels = create _ datasets 
# 传入 参数 特征 数据 和 分类 标签 print datasets 
knn . fit datasets labels # knn 预测 predictRes = 
knn . predict 2 4 0 print 天气 \ t 
非常 热 if predictRes 0 = = 0 else 一般 
热 return predictRes if _ _ name _ _ = 
= _ _ main _ _ # predict _ temperature 
knn _ sklearn _ predict 关于 分词 目前 有 三大 
主流 分词 方法 基于 字符串 匹配 的 分词 方法 基于 
理解 的 分词 方法 和 基于 统计 的 分词 方法 
1 基于 字符串 匹配 的 分词 方法 基于 字符串 匹配 
的 分词 方法 又 称为 机械 分词 方法 它 需要 
有 一个 初始 的 充分 大 的 词典 然后 将 
待 分词 的 字符串 与 词典 中 的 元素 进行 
匹配 若能 成功 匹配 则将 该词 切 分出来 按 扫描 
方向 的 不同 字符串 匹配 分词 方法 可以 分为 正相 
匹配 和 逆向 匹配 按照 不同 长度 的 匹配 优先 
度 可以 划分 为 最大 匹配 和 最小 匹配 1.1 
正向 最大 匹配 1 . 从左到右 将 待 切分 句子 
的 m 个字符 作为 匹配 字符 m 为 初始 词典 
中 最长 词条 的 长度 2 . 将 字符 与 
字典 中 元素 进行 匹配 2.1 . 若 匹配 成功 
则将 这 个字符 作为 一个 词 切 分出来 2.2 . 
若 匹配 不成功 则将 这 个字符 的 最后 一个 字 
去掉 再 进行 匹配 重复 上述 过程 知道 切分 完整 
个 文本 为止 举个 例子 吧 假设 我们 要 切分 
的 句子 为 南京市 长江大桥 字典 中 最长 的 元素 
长度 为 5 则 先取 待 切分 句子 的 前 
5 个字符 南京市 长江 字典 中 没有 元素 与 之 
匹配 长度 减 一 则 变成 南京 市长 匹配 成功 
对 剩余 三个字 江 大桥 再次 进行 正向 最大 匹配 
会 切成 江 大桥 整个 句子 切分 完 成为 南京 
市长 江 大桥 1.2 逆向 最大 匹配 逆向 最大 匹配 
思想 与 正向 最大 匹配 基本相同 不同 的 是 将 
扫描 方向 变成 了 从右 往左 匹配 不 成功 时 
去掉 最 左边 的 字符 实验 表明 逆向 最大 匹配 
算法 效果 要 优于 正向 最大 匹配 算法 南京市 长江大桥 
的 逆向 最大 匹配 1 . 取出 南京市 长江大桥 的 
后5/nr 个字 市 长江大桥 字典 中 无匹配 元素 将 字符 
市 去掉 发现 词典 中有 匹配 切 割下来 2 . 
对 剩余 的 南京市 进行 分词 整体 结果 为 南京市 
长江大桥 1.3 双向 最大 匹配 双向 最大 匹 配法 是 
将 正向 最大 匹 配法 得到 的 分词 结果 和 
逆向 最大 匹 配法 得到 的 结果 进行 比较 从而 
决定 正确 的 分词 方法 还是 上面 的 例子 双向 
最大 匹配 的 划分 结果 为 南京 市长 南京市 长江大桥 
江 大桥 这类 算法 的 优点 是 速度 快 时间 
复杂度 为 O n 实现 简单 但是 对于 歧义 和未/nr 
登录 词 表现 不佳 2 基于 理解 的 分词 方法 
其 基本 思想 就是 在 分词 的 同时 进行 句法 
语义分析 利用 句法 信息 和 语义 信息 来 处理 歧义 
现象 它 通常 包括 三 个 部分 分词 子系统 句法 
语义 子系统 总控 部分 由于 汉语 语言 知识 的 笼统 
复杂性 难以 将 各种 语言 信息 组织 成 机器 可 
直接 读取 的 形式 因此 目前 基于 理解 的 分词 
系统 还 处在 试验 阶段 3 基于 统计 的 分词 
方法 主要 思想 每个字 都是 词 的 最小 单元 如果 
相连 的 字 在 不同 的 文本 中 出现 的 
频率 越多 这就 越有 可能 是 一个 词 因此 我们 
可以 用 相邻 字 出现 的 频率 来 衡量 组词 
的 可能性 当/t 频率/n 高于/nr 某个/r 阈值/n 时/n 我们 可以 
认为 这些 字 可能 会 构成 一个 词 主要 统计模型 
N 元 文法 模型 N gram 隐 马尔可夫 模型 Hidden 
Markov Model HMM 最大熵 模型 ME 条件 随 机场 Conditional 
Random Fields CRF 等 优势 在 实际 运用 中 常常 
将 字符串 匹配 分词 和 统计 分词 结合 使用 这样 
既 体现 了 匹配 分词 速度快 效率高 的 优点 同时 
又 能 运用 统计 分词 识别 生词 自动 消除歧义 等 
方面 的 特点 3.1 N gram 模型 思想 该 模型 
基于 这样 一种 假设 第 n 个 词 出现 只 
与 前面 n 1个 词 相关 而 与 其他 词 
都 不相关 整句话 的 概率 就 是 各个 词 出现 
概率 的 乘积 对于 一个 句子 T 假设 它 由 
n 个 词 w1 w2 w3 ⋯ wnw1 w2 w3 
⋯ wn { w _ 1 } { w _ 
2 } { w _ 3 } \ cdots { 
w _ n } 组成 的 则 p T = 
p w1w2w3 ⋯ wn = p w1 p w2 | 
w1 p w3 | w1w2 ⋯ p wn | w1w2 
⋯ wn − 1 p T = p w1w2w3 ⋯ 
wn = p w1 p w2 | w1 p w3 
| w1w2 ⋯ p wn | w1w2 ⋯ wn − 
1 p \ left T \ right = p \ 
left { { w _ 1 } { w _ 
2 } { w _ 3 } \ cdots { 
w _ n } } \ right = p \ 
left { { w _ 1 } } \ right 
p \ left { { w _ 2 } \ 
left | { { w _ 1 } } \ 
right . } \ right p \ left { { 
w _ 3 } \ left | { { w 
_ 1 } } \ right . { w _ 
2 } } \ right \ cdots p \ left 
{ { w _ n } \ left | { 
{ w _ 1 } } \ right . { 
w _ 2 } \ cdots { w _ { 
n 1 } } } \ right 计算 这个 式子 
很麻烦 我们 引入 马尔科夫 假设 一个词 的 出现 仅 依赖于 
它 前面 有限 的 几个 词 如果 一个 词 的 
出现 仅 依赖于 它 前面 出现 的 一个 词 我们 
就 称之为 bigram 则 上式 变为 p T = p 
w1 p w2 | w1 p w3w2 ⋯ p wnwn 
− 1 p T = p w1 p w2 | 
w1 p w3w2 ⋯ p wnwn − 1 p \ 
left T \ right = p \ left { { 
w _ 1 } } \ right p \ left 
{ { w _ 2 } \ left | { 
{ w _ 1 } } \ right . } 
\ right p \ left { { w _ 3 
} { w _ 2 } } \ right \ 
cdots p \ left { { w _ n } 
{ w _ { n 1 } } } \ 
right 如果 一个 词 的 出现 仅 依赖于 它 前面 
出现 的 两个 词 那么 我们 就 称之为 trigram 以此类推 
N 元 模型 就是 假设 当前 词 的 出现 概率 
只 同 它 前面 的 N 1个 词 有关 实际 
中 通常 只用到 二元 模型 3.2 隐 马尔可夫 模型 HMM 
3 . 2.1 隐 马尔可夫 模型 简介 隐 马尔可夫 模型 
中的 变量 有 两组 一组 为 状态变量 { y1 y2 
yn } 其中 yi 表示 第 i 时刻 所处 的 
状态 这些 状态 是 隐藏 的 不可 观测 的 因此 
又 称为 隐 变量 隐 变量 的 取值 通常 是 
离散 的 第二组 是 观测 变量 { x1 x2 xn 
} 其中 xi 表示 第 i 时刻 的 观测值 在任 
一 时刻 观测 变量 的 取值 只 与 该 时刻 
的 状态变量 有关 即 xi 由 yi 决定 而 当前 
状态 只 与 前一 时刻 的 状态 有关 与 其他 
状态 无关 3 . 2.2 隐 马尔可夫 模型 的 三大 
问题 一般 的 一个 HMM 可以 表示 为 u = 
S K A B π 其中 是 状态 集合 K 
是 输出 符号 也就是 观察 集合 A 是 状态 转移 
概率 B 是 符号 发射 概率 π 是 初始状态 的 
概率分布 HMM 主要 解决 三 个 基本 问题 估计 问题 
给定 一个 观察 序列 O = O1 O2 O3 Ot 
和 模型 u = A B π 计算 观察 序列 
的 概率 序列 问题 给定 一个 观察 序列 O = 
O1 O2 O3 Ot 和 模型 μ = A B 
π 计算 最优 的 状态 序列 Q = q1 q2 
q3 qt 参数估计 问题 给定 一个 观察 序列 O = 
O1 O2 O3 Ot 如何 调节 模型 μ = A 
B π 的 参数 使得 P O | μ 最大 
三类 问题 的 求解 在 这里 略去 3 . 2.3 
隐 马尔可夫 模型 分词 方法 隐 马尔可夫 的 三大 问题 
分别 对应 了 分词 中的 几个 步骤 参数估计 问题 就是 
分词 的 学习 阶段 通过 海量 的 预料 数据 来 
学习 归纳 出 分词 模型 的 各个 参数 状态 序列 
问题是 分词 的 执行 阶段 通过 观测 变量 待 分词 
句子 的 序列 来 预测 出 最优 的 状态 序列 
分词 结构 设 状态 集合 = B . M E 
S 每个 状态 代表 的 是 这个 字 在 词语 
中 的 位置 B 代表 该 字 是 词语 中 
的 起始 字 M 代表 是 词语 中 的 中间 
字 E 代表 是 词语 中 的 结束 字 则 
代表 是 单字 成词/nr 观察 值 集合 K = 所有 
的 汉字 则 中文分词 的 问题 就是 通过 观察 序列 
来 预测 出 最优 的 状态 序列 比如 观察 序 
列为 O = 南京市 长江大桥 预测 的 状态 序 列为 
Q = BMEBMME 根据 这个 状态 序列 我们 可以 进行 
切 词 BME / BMME / 所以 切 词 结果 
如下 南京市 / 长江大桥 / 因为 HMM 分词 算法 是 
基于 字 的 状态 BEMS 来 进行 分词 的 因此 
很 适合 用于 新词 发现 某 一个 新词 只要 标记 
为 如 BMME 就算 它 没有 在 历史 词典 中 
出现 过 HMM 分词 算法 也 能将 它 识别 出来 
中文分词/i 工具/n 介绍/v python/w 常用/b 的/uj 分词/n 包有/nr jieba/w 分词/n 
SnowNLP THULAC NLPIR 等 1 jieba 分词 jieba 分词 是 
国内 使用 人数 最多 的 中文分词 工具 1.1 jieba 分词 
的 三种 模式 1 精确 模式 试图 将 句子 最 
精确地 切分 适合 文本 分析 2 全 模式 把/p 句子/n 
中/f 所有/b 可以/c 成词的/nr 词语/n 都/d 扫描/v 出来/v 速度 非常 
快 但是 不能 解决 歧义 3 搜索引擎 模式 在 精确 
模式 的 基础 上 对 长词 再次 切分 提高 召回率 
适合 用于 搜索引擎 分词 1.2 jieba 分词 涉及 的 算法 
jieba 分词 过程 中 主要 涉及 如下 几种 算法 1 
基于 前缀 词典 实现 高效 的 词 图 扫描 生成 
句子 中 汉字 所有 可能 成词/nr 情况 所 构成 的 
有向 无 环 图 DAG 2 采用 了 动态规划 查找 
最大 概率 路径 找出 基于 词频 的 最大 切分 组合 
3 对于 未 登录 词 采用 了 基于 汉字 成词/nr 
能力 的 HMM 模型 采用 Viterbi 算法 进行 计算 4 
基于 Viterbi 算法 做 词性 标注 5 基于 tf idf 
和 textrank 模型 抽取 关键词 jieba 分词 测试 如下 import 
jieba u = 我 来到 北京 清华大学 # 全 模式 
test1 = jieba . cut u cut _ all = 
True print 全 模式 + | . join test1 # 
精确 模式 test2 = jieba . cut u cut _ 
all = False print 精确 模式 + | . join 
test2 # 搜索引擎 模式 test3 = jieba . cut _ 
for _ search u print 搜索引擎 模式 + | . 
join test3 全 模式 我 | 来到 | 北京 | 
清华 | 清华大学 | 华大 | 大学 精确 模式 我 
| 来到 | 北京 | 清华大学 搜索引擎 模式 我 | 
来到 | 北京 | 清华 | 华大 | 大学 | 
清华大学 2 SnowNLPSnowNLP 可以 方便 的 处理 中文 文本 内容 
是 受到 了 TextBlob 的 启发 而 写 的 SnowNLP 
主要 包括 如下 几 个 功能 1 中文分词 Character Based 
Generative Model 2 词性 标注 3 gram HMM 3 情感 
分析 简单 分析 如 评价 信息 4 文本 分类 Naive 
Bayes 5 转换 成 拼音 Trie 树 实现 的 最大 
匹配 6 繁简 转换 Trie 树 实现 的 最大 匹配 
7 文本 关键词 和 文本 摘要 提取 TextRank 算法 8 
计算 文档 词频 TF Term Frequency 和 逆向 文档 频率 
IDF Inverse Document Frequency 9 Tokenization 分割 成 句子 10 
文本 相似 度 计算 BM25 SnowNLP 的 最大 特点 是 
特别 容易 上手 用 其 处理 中文 文本 时 能够 
得到 不少 有意思 的 结果 但 不少 功能 比较 简单 
还 有待 进一步 完善 3 THULACTHULAC 由 清华大学 自然语言 处理 
与 社会 人文 计算 实验室 研制 推出 的 一套 中文 
词 法分析 工具包 具有 中文分词 和 词性 标注 功能 THULAC 
具有 如下 几个 特点 1 能力强 利用 我们 集成 的 
目前 世界 上 规模 最大 的 人工 分词 和 词性 
标注 中文 语料库 约 含 5800 万字 训练 而成 模型 
标注 能力 强大 2 准确率 高 该 工具包 在 标准 
数据集 Chinese Treebank CTB5 上 分词 的 F1 值 可达 
97.3％ 词性 标注 的 F1 值 可达到 92.9％ 与 该 
数据 集上 最好 方法 效果 相当 3 速度 较快 同时 
进行 分词 和 词性 标注 速度 为 300KB / s 
每秒 可 处理 约 15 万字 只 进行 分词 速度 
可 达到 1 . 3MB / s 4 NLPIRNLPIR 分词 
系统 是 由 北京 理工大学 张 华平 博士 研发 的 
中文分词 系统 经过 十 余年 的 不断 完善 拥有 丰富 
的 功能 和 强大 的 性能 NLPIR 是 一整套 对 
原始 文本 集 进行 处理 和 加工 的 软件 提供 
了 中间件 处理 效果 的 可视化 展示 也 可以 作为 
小 规模 数据 的 处理 加工 工具 主要 功能 包括 
中文分词 词性 标注 命名 实体 识别 用户 词典 新词 发现 
与 关键词 提取 等 功能 人工智能 Artificial Intelligence AI 机器学习 
Machine Leaining ML 模式识别 Pattern Recognition PR 数据挖掘 Data Mining 
DM 他们 要 解决 的 核心 问题 不同 但是 运用 
的 数学 模型 如出 一 撤 主要 是 统计学 方法 
人工智能 就是 计算机 自动 做 决策 包含 其他 几个 机器学习 
研究 重点 是 算法 的 学习 过程 强调 的 是 
一个 反馈 的 自我 完善 的 框架 是 人工智能 的 
一个 分支 模式识别 就是 分类 问题 是 机器 学习 的 
一个 方面 包括/v 监督/vn 法和非/nr 监督法/n 分类/n 数据挖掘 就是 在 
大型 数据库 上 的 机器学习 应用 偏重于 从 大型 数据库 
中 找 规律 的 应用 方面 自然语言 处理 NLP 主要 
的 统计 学 方法 回归分析 决策树 贝叶斯 学习 支持 向量 
机 SVM PageRank K Means CRF 条件 随 机场 隐 
马尔可夫 模型 开源 软件包 WekaCRF + + 欢迎 加入 学习 
交流 QQ 群 657341423Gensim 是 一款 开源 的 第三 方 
Python 工具包 用于 从 原始 的 非 结构化 的 文本 
中 无 监督 地 学习 到 文本 隐 层 的 
主题 向量 表达 它 支持 包括 TF IDF LSA LDA 
和 word2vec 在内 的 多种 主题 模型 算法 支持 流式 
训练 并 提供 了 诸如 相似 度 计算 信息检索 等 
一些 常用 任务 的 API 接口 简单 地 说 Gensim 
主要 处理 文本 数据 对 文本 数据 进行 建模 挖掘 
语料 Corpus 一组 原始 文本 的 集合 用于 无 监督 
地 训练 文本 主题 的 隐 层 结构 语 料中 
不 需要 人工 标注 的 附加 信息 在 Gensim 中 
Corpus 通常 是 一个 可 迭代 的 对象 比如 列表 
每一次 迭代 返回 一个 可 用于 表达 文本 对象 的 
稀疏 向量 向量 Vector 由 一组 文本 特征 构成 的 
列表 是 一段 文本 在 Gensim 中的 内部 表达 稀疏 
向量 Sparse Vector 通常 我们 可以 略去 向量 中 多余 
的 0 元素 此时 向量 中的 每 一个 元素 是 
一个 key value 的 tuple 模型 Model 是 一个 抽象 
的 术语 定义 了 两个 向量空间 的 变换 即从 文本 
的 一种 向量 表达 变换 为 另一种 向量 表达 文件 
准备 aa . txt 文件 内容 为 某 新闻 报道 
from gensim import corpora models similarities import jieba import re 
from snownlp import SnowNLP # 读取 文本 内容 f = 
open aa . txt r encoding = utf 8 text 
= f . read f . close # 分句 s 
= SnowNLP text text _ list = s . sentences 
seg _ list = # 循环 句子 列表 对 每个 
句子 做 分词 处理 for i in text _ list 
temp _ list = jieba . cut i cut _ 
all = False results = re . sub . \ 
d + . join temp _ list seg _ list 
. append results # 将 分词 写入 文件 f = 
open data . txt w encoding = utf 8 f 
. write . join seg _ list f . close 
# 我 是 分割线 # # * * * * 
* * * * * * 字典 的 使用 * 
* * * * * * * * * # 
gensim 的 字典 是 将 分词 好 的 数据 转换成 
gensim 能 处理 的 数据格式 seg _ dict = x 
. split for x in seg _ list dict1 = 
corpora . Dictionary seg _ dict prune _ at = 
2000000 print dict1 . token2id # 手动 添加 字典 dict2 
= corpora . Dictionary dict2 . token2id = { computer 
0 human 1 response 2 survey 3 } print dict2 
. token2id # 合并 字典 dict2 = corpora . Dictionary 
seg _ dict prune _ at = 2000000 dict2 _ 
to _ dict1 = dict1 . merge _ with dict2 
# 获取 字典 中 某 词语 的 词 袋 向量 
new _ doc = 生态 环境 政府 非法 new _ 
vec = dict1 . doc2bow new _ doc . split 
print new _ vec # 14 1 22 1 66 
1 14 代表 生态 环境 在 字典 dict1 的 ID 
1 代表 出现 次数 # 获取 整个 dict1 的 词 
袋 向量 bow _ corpus = dict1 . doc2bow text 
for text in seg _ dict print bow _ corpus 
# * * * * * * * * * 
* 字典 的 使用 * * * * * * 
* * * * # 我 是 分割线 # # 
我 是 分割线 # # * * * * * 
* * * * * 模型 的 使用 * * 
* * * * * * * * # 模型 
对象 的 初始化 实现 词 向 量化 tfidf = models 
. TfidfModel bow _ corpus # 计算 new _ vec 
的 权重 string _ tfidf = tfidf new _ vec 
print string _ tfidf # 基于 Tf Idf 计算 相似 
度 参考 https / / radimrehurek . com / gensim 
/ tutorial . html index = similarities . p a 
r s e M a t r i x i 
m i l a r i t y bow _ 
corpus num _ features = 10 sims = index string 
_ tfidf print sims # 输出 14 0 . 5862218816946012 
22 0 . 4809979876921243 66 0 . 6519086141926397 # 14 
代表 生态 环境 在 字典 dict1 的 ID 0 . 
5862218816946012 代表 相似性 分数 # * * * * 建模 
* * * * # 参考 https / / radimrehurek 
. com / gensim / tut2 . html # LSI 
建模 models . LsiModel corpus = tfidf bow _ corpus 
id2word = dict1 num _ topics = 50 chunksize = 
10000 # HDP 建模 models . HdpModel corpus = tfidf 
bow _ corpus id2word = dict1 chunksize = 10000 # 
RP 建模 models . RpModel corpus = tfidf bow _ 
corpus id2word = dict1 num _ topics = 50 lda 
= models . LdaModel corpus = tfidf bow _ corpus 
id2word = dict1 num _ topics = 50 update _ 
every = 1 chunksize = 10000 passes = 1 for 
i in range 0 3 print lda . print _ 
topics i 0 # 利用 模型 获取 文档 的 主题 
概率分布 doc _ lda = lda new _ vec print 
doc _ lda # 根据 模型 计算 相似 度 # 
参考 https / / radimrehurek . com / gensim / 
tut3 . html index = similarities . MatrixSimilarity bow _ 
corpus sims = index new _ vec print list enumerate 
sims # * * * * 建模 * * * 
* # * * * * * * * * 
* * 模型 的 使用 * * * * * 
* * * * * # 我 是 分割线 # 
# 我 是 分割线 # # * * * * 
* * * * * * word2vec 的 使用 * 
* * * * * * * * * # 
通过 word2vec 的 skip gram 和 CBOW 模型 生成 深度 
学习 的 单词 向量 # 读取 已 分词 的 文件 
sentences = models . word2vec . LineSentence data . txt 
# 建立 模型 实现 词 向 量化 第一 个 参数 
是 训练 语料 min _ count 是 小于 该数 的 
单词 会被 踢出 默认值 为 5 size 是 神经 网络 
的 隐藏 层 单元 数 在 保存 的 model . 
txt 中会 显示 size 维 的 向 量值 默认 是 
100 默认 window = 5 model = models . word2vec 
. Word2Vec sentences size = 100 window = 25 min 
_ count = 5 workers = 4 # 根据 语料 
计算 某个 词 的 相关 词 列表 sim = model 
. wv . most _ similar 生态 环境 topn = 
10 # 计算 一个词 d 或者 词表 使得 该词 的 
向量 v d 与 v a = 政府 v c 
= 生态 环境 + v b = 街道 最近 # 
sim = model . most _ similar positive = 政府 
街道 negative = 生态 环境 topn = 10 for s 
in sim print word % s similar % s % 
s 0 s 1 # 根据 语料 计算 两个 词 
的 相似 度 / 相关 程度 print str model . 
similarity 政府 生态 环境 # 计算 文本 的 相似 度 
similarity _ matrix = model . wv . similarity _ 
matrix dict1 # MatrixSimilarity 指数 相似性 密集 与 余弦 距离 
# p a r s e M a t r 
i x i m i l a r i t 
y 索引 相似 度 带 余弦 距离 的 稀疏 # 
o f t C o s i n e i 
m i l a r i t y 指数 相似性 
具有 软 余弦 距离 # WmdSimilarity 索引 相似 度 与 
字 移动 距离 index = similarities . o f t 
C o s i n e i m i l 
a r i t y bow _ corpus similarity _ 
matrix num _ best = 10 sims = index dict1 
. doc2bow new _ doc . split print sims # 
保存 模型 方法 一 model . save test _ 01 
. model # 保存 模型 方法 二 # model . 
wv . save _ word2vec _ format test _ 01 
. model . bin binary = True # model = 
models . KeyedVectors . load _ word2vec _ format test 
_ 01 . model . bin binary = True # 
* * * * * * * * * * 
word2vec 的 使用 * * * * * * * 
* * * # 我 是 分割线 # 参考资料 官方 
文档 自然语言 人类 语言 无 强加 规则 自然 进化 形式语言 
特别 设计 规则 认为 设计 计算机 代码 数学 符号 等等 
一 选择 第三 方 NLP 开放平台 NLP 技术 沉淀 周期 
过长 投入 会 很大 选择 第三 方 开放平台 想必 是 
小公司 最好 的 选择 推荐 三个 AI 语音 开放平台 科大 
讯 飞 开放平台 百度 AI 开 放平 搜狗 云 知音 
二 明确 技术 分工 没有 NLP 技术 背景 如何 造 
一款 AI 产品 上图 是 引入 单个 NLP 的 对接 
方案 通过 任务 分解 可以 很 清楚 知道 哪些 是 
第三 方 平台 做 的 哪些 是 我们 要 做 
的 NLP 底层 识别 交给 第三方 开放平台 ASR A u 
t o m a t i c p e e 
c h R e c o g n i t 
i o n 自动 语音 识别 作用 是 将 语音输入 
转化 为 文本 文字 NLU 后台 N a t u 
r a l L a n g u a g 
e U n d e r s t a n 
d i n g 自然语言 理解 开放 给 使用者 的 
一套 自定义 语义 系统 TTS TextToSpeech 文本 转 语音 用于 
文本 转 语音 唤醒 模型 预置 唤醒 词 当 用户 
发出 该 语音指令 时 设备 便从 休眠状态 中被 唤醒 并 
作出 指定 响应 唤醒 词 需要 反复 训练 提升 唤醒 
率 降低 误 唤醒 OS OperatingSystem OS 在 执行 层面 
发挥 的 巨大 作用 比如 正在 执行 播放 音乐 你 
想 关闭 切换 歌曲 这时候 OS 就 显示 出 他 
的 作用 了 系统 垂 类 开放平台 所带 的 系统 
技能 NLU 补充 执行 干预 运营 系统 是 我们 需要 
做 的 三 谈谈 我们 要 做 的 内容 底层 
工作 交给 开放 平台 之后 我们 需要 搭建 自己 的 
运营 管理 系统 开发 自己 想要 的 技能 技能 相当于 
垂 类 简单 的 说 就是 某 个 应用 程序 
语音 作为 入口 打开 应用 像 音乐 新闻 天气 笑话 
等 都 属于 技能 比如 讲个 笑话 语音 产品 执行 
打开 了 笑话 应用 给 你 返回 一条 笑话 内容 
技能 决定 了 产品 内容 的 广度 技能 可以 是 
自制 比如 闹钟 也/d 可以/c 从/p 第三/m 方/n 合作/vn 引进/v 
像 抖 音 微信 这样 自带 流量 的 第三 方 
估计 想必 都想 接入 吧 对于 一个 智能 产品 来说 
技能 自然 多多益善 至于 需要 多少 看 公司 的 产品 
定位 业务 成本 等 因素 综合 考虑 自定义 NLU 给 
你 的 技能 配置 语义 基于 开放 平台 下 建立 
自己 产品 的 自定义 NLU 语义 内容 NLU 主要 由 
三 个 方面 构成 语义 文本 意图 参数 语义 文本 
Text 语义 文本 设计 目的 是 为了 能 听得 懂 
用户 声音 同一个 请求 每个 用户 说法 都 不一样 举个 
简单 的 例子 比如 帮 我 放 首 周杰伦 的 
歌 来点 周杰伦 音乐 周杰伦 的 音乐 有 没有 设计 
语义 文本 时 既要 使用 正规 的 主谓宾 结构 又要 
考虑 到 特殊 的 说法 语义 要 尽量 覆盖 全 
意图 Intent 意图 指 用户 的 具体 请求 或 目的 
一个 意图 可以 包含 多个 语义 文本 举例 明天 早上 
8点 叫 我 起床 定 明天 早上 8 点钟 的 
闹钟 都 属于 新增 闹钟 意图 通常 意图 依赖于 技能 
举例 的 意图 就 属于 闹钟 技能 详细 参数 Detail 
读懂 用户 说 什么 后 需要 根据 用户 的 意图 
作出 相应 的 反馈 参数 设计 就 显得 特别 重要 
了 NLP 平台 做法 是 当 语义 文本 输入 命中 
意图 后 通过 接口 将 自定义 NLU 的 参数 传达 
给 后台 参数 存在 的 目的 是 要 告诉 后台 
接下来 你 要 做什么 还是 用 歌曲 的 例子 来 
说明 没有 NLP 技术 背景 如何 造 一款 AI 产品 
语义 告诉 后台 命中 MUSIC 意图 执行 音乐 技能 播放 
作者 为 周杰伦 的 歌曲 产品 交互 规则 拿到 了 
NLP 传达 的 参数 指令 接下来 系统 要做 的 是 
给 用户 反馈 结果 命中 到 NLP 系统 自带 的 
技能 如果 你 不做 干预 的话 系统 可以 直接 给 
出 结果 命中 不是 系统 技能 意图 或 干预 系统 
自带 技能 需要 根据 参数 开发 相应 的 功能 最后 
没有 语音 识别 技术 同样 可以 打造 一 款 智能 
语音 产品 它 可以 成为 你 的 产品 体系 里 
的 一部分 因为 出身 决定了 它 的 造价 成本 会 
很高 如果 脱离 产品 体系 将该 语音 产品 单独 为 
投入市场 至少 在 价格 上 缺乏 竞争力 人工智能 大 数据 
云计算 和 物联网 的 未来 发展 值得 重视 均为 前沿 
产业 多/m 智/ng 时代/n 专注/v 于/p 人工智能/n 和大/nr 数据/n 的/uj 
入门/ns 和科谱/nr 在 此为 你 推荐 几篇 优质 好文 改变 
世界 的 七大 NLP 技术 你 了解 多少 http / 
/ www . duozhishidai . com / article 8918 1 
. htmlNLP 自然语言 处理 技术 在 人工智能 法官 中的 应用 
是 什么 http / / www . duozhishidai . com 
/ article 2325 1 . html 如何 快速 入门 NLP 
自然语言 处理 概述 http / / www . duozhishidai . 
com / article 11742 1 . html 多 智 时代 
人工智能 和大/nr 数据 学习 入门 网站 | 人工智能 大 数据 
物联网 云计算 的 学习 交流 网站 http / / www 
. duozhishidai . com NLTK 是 python 上 著名 的 
自然 语言 处理 库 自带 语料库 词性 分 类库 自带 
分类 分词 等 等功能 安装 语料库 import nlknlk . download 
文本处理 流程 最后 从 文本 转换 为 一组 数字 这些 
数字 就 隐含 了 文本 的 意义 stopwords 对于 注重 
理解 文本 意思 的 应用 场景 来说 歧义 太多 所以 
要 去掉 NLTK 在 NLP 上 的 经典 应用 有 
情感 分析 文本 相似 度 文本 分类 哈尔滨工程大学 537 自然语言 
处理 BM25 相关度 打分 注 文中 大写 Query Document 等 
代表 集合 小写 query document 等 代表 集合 中的 个体 
一 优缺点 适用 于 在 文档 包含 查询 词 的 
情况 下 或者说 查询 词 精确 命中 文档 的 前提 
下 如何 计算 相似 度 如何 对 内容 进行 排序 
不适 用于 基于 传统 检索 模型 的 方法 会 存在 
一个 固 有缺陷 就是 检索 模型 只能 处理 Query 与 
Document 有 重合 词 的 情况 传统 检索 模型 无法 
处理 词语 的 语义 相关性 白话 举例 提出 一个 query 
当下 最 火 的 女网 红是谁/nr 在 Document 集合 中 
document1 的 内容 为 当下 最 火 的 男明星 为 
鹿晗 document2 的 内容 为 女网/n 红能火/nr 的/uj 只是/c 一小部分/m 
显然 document1 和 document2 中都 包含 火 当下 网 红 
等 词语 但是 document3 的 内容 可能 是 如今 最 
众所周知 的 网络 女主播 是 周二 柯 很 显然 与 
当前 Query 能 最好 匹配 的 应该 是 document3 可是 
document3 中 却 没有 一个 词 是 与 query 中的 
词 相同 的 即 上文 所说 的 没有 精确 命中 
此时 就 无法 应用 BM25 检索 模型 二 算法 核心 
BM25 算法 是 一种 常见 用来 做 相关度 打分 的 
公式 思路 比较 简单 主要 就是 计算 一个 query 里面 
所有 词 q1 q2 . . . qnq1 q2 . 
. . qnq _ 1 q _ 2 . . 
. q _ n 和 文档 的 相关度 然后再 把 
分数 做 累加 操作 公式 如下 Score Q d = 
∑ inWi ⋅ R qi d 8 8 Score Q 
d = ∑ inWi ⋅ R qi d Score Q 
d = \ sum _ i ^ n { W 
_ i } \ cdot { R q _ i 
d } 其中 R qi d R qi d R 
q _ i d 是 查询 语句 query 中 每个 
词 qiqiq _ i 和 文档 d 的 相关度 值 
WiWiW _ i 是 该词 的 权重 最后 将 所有 
词 的 Wi ∗ R qi d Wi ∗ R 
qi d W _ i * R q _ i 
d 相加 WiWiW _ i 一般 情况下 为 IDF I 
n v e r s e D o c u 
m e n t F r e q u e 
n c y IDF I n v e r s 
e D o c u m e n t F 
r e q u e n c y IDF Inverse 
Document Frequency 值 即 逆向 文档 频率 公式 如下 IDF 
qi = logN + 0.5 n qi + 0.5 9 
9 IDF qi = logN + 0.5 n qi + 
0 . 5IDF q _ i = log \ frac 
{ N + 0.5 } { n q _ i 
+ 0.5 } 其中 NNN 是 文档 总数 n qi 
n qi n q _ i 是 包含 该词 的 
文档 数 0.5 是 调教 系数 避免 n qi = 
0n qi = 0n q _ i = 0 的 
情况 logloglog 函数 是 为了 让 IDF 的 值 受 
NNN 和n/nr qi n qi n q _ i 的 
影响 更加 平滑 从/p 公式/n 中/f 显然/ad 能/v 看出/v IDF/w 
值/n 的/uj 含义/n 即 总 文档 数 越大 包含 词 
qiqiq _ i 的 文档 数 越小 则 qiqiq _ 
i 的 IDF 值 越大 白话 举例 就是 比如 我们 
有1/nr 万篇 文档 而 单词 basketball Kobe Bryant 几乎 只在 
和 体育运动 有关 的 文档 中 出现 说明 这 两个 
词 的 IDF 值 比较 大 而 单词 is are 
what 几乎 在 所有 文档 中 都有 出现 那么 这 
几个 单词 的 IDF 值 就 非常 小 解决 了 
WiWiW _ i 现在 再 来 解决 R qi d 
R qi d R q _ i d R qi 
d R qi d R q _ i d 公式 
如下 R qi d = fi ⋅ k1 + 1 
fi + K ⋅ qfi ⋅ k2 + 1 qfi 
+ k2 10 10 R qi d = fi ⋅ 
k1 + 1 fi + K ⋅ qfi ⋅ k2 
+ 1 qfi + k2R q _ i d = 
\ frac { { f _ i } \ cdot 
{ k _ 1 + 1 } } { f 
_ i + K } \ cdot { \ frac 
{ { qf _ i } \ cdot { k 
_ 2 + 1 } } { qf _ i 
+ k _ 2 } } 其中 k1 k2 bk1 
k2 bk _ 1 k _ 2 b 都是 调节 
因子 一般 k1 = 1 k2 = 1 b = 
0 . 75k1 = 1 k2 = 1 b = 
0.75 k _ 1 = 1 k _ 2 = 
1 b = 0.75 式 中 qfiqfiqf _ i 为 
词 qiqiq _ i 在 查询 语句 queryqueryquery 中的 出现 
频率 fifif _ i 为 qiqiq _ i 在 文档 
ddd 中的 出现 频率 由于 绝大多数 情况 下 一条 简短 
的 查询 语句 queryqueryquery 中 词 qiqiq _ i 只 
会 出现 一次 即 qfi = 1qfi = 1qf _ 
i = 1 因此 公式 可 化简 为 R qi 
d = fi ⋅ k1 + 1 fi + K 
11 11 R qi d = fi ⋅ k1 + 
1 fi + KR q _ i d = \ 
frac { { f _ i } \ cdot { 
k _ 1 + 1 } } { f _ 
i + K } 其中 K = k1 ⋅ 1 
− b + b ⋅ dlavgdl 12 12 K = 
k1 ⋅ 1 − b + b ⋅ dlavgdl K 
= { k _ 1 } \ cdot { 1 
b + b \ cdot { \ frac { dl 
} { avgdl } } } dldldl 为 文档 ddd 
的 长度 avgdlavgdlavgdl 为 所有 文档 的 平均 长度 意即 
该 文档 ddd 的 长度 和 平均 长度 比 越大 
则 KKK 越大 则 相关度 R qi d R qi 
d R q _ i d 越小 bbb 为 调节 
因子 bbb 越大 则 文档 长度 所占 的 影响 因素 
越大 反之 则 越小 白话 举例 就是 一个 queryqueryquery 为 
诸葛亮 在 哪里 去世 的 document1 的 内容 为 诸葛亮 
在 五丈原 积劳成疾 最终 去世 document2 的 内容 为 司马懿 
与 诸葛亮 多次 在 五丈原 交锋 而 document3 为 一整 
本 中国 历史 书 的 内容 显然 document3 中 肯定 
包含 了 大量 诸葛亮 哪里 去世 这些 词语 可是 由于 
document3 文档 长度 太大 所以 KKK 非常大 所以 和 queryqueryquery 
中 每个 词 qiqiq _ i 的 相关度 R qi 
d R qi d R q _ i d 非常 
小 综上所述 可将 BM25 相关度 打分 算法 的 公式 整理 
为 Score Q d = ∑ inIDF qi ⋅ fi 
⋅ k1 + 1 fi + k1 ⋅ 1 − 
b + b ⋅ dlavgdl 13 13 Score Q d 
= ∑ inIDF qi ⋅ fi ⋅ k1 + 1 
fi + k1 ⋅ 1 − b + b ⋅ 
dlavgdl Score Q d = \ sum _ i ^ 
nIDF q _ i \ cdot \ frac { { 
f _ i } \ cdot { k _ 1 
+ 1 } } { f _ i + k 
_ 1 \ cdot { 1 b + b \ 
cdot { \ frac { dl } { avgdl } 
} } } 人工智能 大 背景 和 历史 关于 人工智能 
的 知识 网络 拓扑图 及 学习 路线 阿尔法 狗 原理 
算法 深入 解析 包含 阿尔法 狗 各 模块 详解 价值 
判断 专家 网络 反向 更新 强化 学习 快速 响应 卷积 
神经网络 分层 拆分 计算 求 无限 接近 值 + 蒙特卡洛 
树 搜索 选 重要 节点 向后 推断 得到 最优 值 
阿尔法 狗 适用于 哪些 应用 场景 以及 如何 拿 来用 
个人 如何 开发 一款 人工智能 应用 个人 如何 利用 免费 
的 人工智能 工具 与 平台 赚钱 实录 提要 人工智能 发展 
到 什么 程度 会 取代 程序猿 或者 其他 行业 现在 
自然语言 处理 达到 什么 地步 了 虽然 接触 了 3 
个月 AI 但是 学 的 比较 散 可以 推荐 一套 
系统 的 学习 路线 吗 前端开发 转 人工智能 开发 需要 
从哪/nr 方面 入手 需要 掌握 什么 技能 人工智能 怎么 检验 
自己 的 能力 到 哪种 等级 了 适合 个人 开发者 
的 具体 的 盈利 模式 及 发展 方向 有 哪些 
学习 人工智能 开发 要求 数学 掌握 到 什么 程度 没学 
过高 数 的 同学 应该 怎样 学习 移动 开发 与 
人工智能 结合 的 场景 因为 移动设备 计算 能力 有限 有 
哪些 应用 点 前端 开发 和 AI 有 什么 结合点 
吗 人工智能 是否 会 带来 新的 商业 模式 人工智能 与 
硬件 无线电 APP 或 小 程序 有 哪些 结合 应用 
前景 关于 自然 语言 处理 目前 世界 上 有 什么 
前沿 的 论文 或 书籍 推荐 吗 盈利模式 的话 数据 
能否 成为 盈利 点 成功 的 AI 项目 中 是 
如何 获取 训练 数据 的 除了 经典 的 书籍 AI 
有/v 哪些/r 高质量/n 论文/nz 和/c 论坛/n 可供/v 持续/vd 学习/v 阅读 
全文 http / / gitbook . cn / gitchat / 
activity / 5 9 e d 5 e 0 7 
9 9 1 d f 7 0 e c d 
5 a 0 1 e 2 您 还 可以 下载 
CSDN 旗下 精品 原创 内容社区 GitChat App 阅读 更多 GitChat 
专 享 技术 内容 哦 ACL 文章 链接 http / 
/ www . aclweb . org / anthology / 2016 
年 ACL WMT 机器翻译 数据集 PaperWeeklyhttp / / rsarxiv . 
github . io / 中国 NLP 联盟 墙 裂 推荐 
https / / github . com / NLPchina 中文 NLP 
工具 大全 https / / github . com / NLPchina 
/ Awesome Chinese NLP 中文 Embeddinghttps / / github . 
com / Embedding / Chinese Word Vectors 更多 优秀 文章 
http / / blog . csdn . net / leyounger 
/ article / details / 78085905NLP 工具 1 NLTK http 
/ / www . nltk . org 提供 了 常用 
的 文本 预 处理函数 和 算法 2 HIT LTP 汉语分词 
系统 模型库 百度 Pan https / / pan . baidu 
. com / share / link shareid = 1988562907 & 
uk = 2738088569 # list / path = % 2Fltp 
models 3 Sklearn 用于 分类 相关 的 还是 很好 用 
的 不仅 提供 小 数据集 搜索 对于 大 数据 环境 
下 也 提供 了 支持 partial _ fit 方法 的 
模型 来 获得 相对 好 的 结果 http / / 
scikit learn . org / stable / modules / scaling 
_ strategies . htmlNLP 工作 http / / www . 
nlpjob . com / Stanford cs224dhttp / / blog . 
csdn . net / neighborhoodguo / article / details / 
46868143 可爱 的 数据集 https / / zhuanlan . zhihu 
. com / p / 25138563https / / blog . 
csdn . net / t M b 8 Z 9 
V d m 6 6 w H 6 8 V 
X 1 / article / details / 78153519 优秀 NLP 
系统 1 腾讯 Peacock 推荐 系统 http / / www 
. flickering . cn / nlp / 2015/03 / peacock 
大 规模 主题 模型 及其 在 腾讯 业务 中 的 
应用 / 神经网络 相关 1 深度 学习 与 计算机 视觉 
系列 http / / blog . csdn . net / 
x i a o p i h a i e 
r l e t i a n / article / 
details / 74688643 2 用 深度 学习 CNN RNN Attention 
解决 大 规模 文本 分类 问题 综述 和 实践 https 
/ / zhuanlan . zhihu . com / p / 
25928551 深度 学习 相关 1 深度 学习 中文版 书籍 https 
/ / github . com / exacity / deeplearningbook chinese 
2 深度 学习 CrossEntropy 导数 推导 http / / www 
. cnblogs . com / python27 / p / M 
a c h i n e L e a r 
n i n g W e e k 0 5 
. htmlNLP 比赛 经验 1 知乎 看山 杯 夺冠 记 
https / / zhuanlan . zhihu . com / p 
/ 28923961 还有 一个 用 TensorFlow 做 的 版本 http 
/ / blog . csdn . net / jerr _ 
_ y / article / details / 77751885 百度 上线 
的 智能 写作 平台 集合 了 百度 领先 的 自然 
语言 处理 技术 NLP 和 知识图谱 技术 KG 内置 百度 
丰富 的 数据 和 素材 给 您 提供 自动 写作 
和 辅助 写作 的 能力 帮 您 全面 提升 内容 
创作 效率 旨在 成为 最 懂 你 的 智能 写作 
助手 自动 写作 自动 写作 技术 能够 让 机器 自主 
的 完成 文章 写作 当前 计算机 已经 能够 自动 的 
撰写 新闻 快讯 热点 组稿 春联 等 类型 的 文章 
百度 自动 写作 的 财经 新闻 这类 自动 写作 通常 
以 结构化 数据 为 输入 智能 写作 算法 按照 人类 
习惯 的 方式 描述 数据 中 蕴含 的 主要 信息 
非常 擅长 完成 时效性 新闻 的 报道 任务 这种 自动 
写作 的 典型 例子 包括 地震 快讯 财经 快讯 体育 
战报 等 热点 组稿 写作 这类 自动 写作 通常 以 
海量 素材 为基础 按照 应用 需求 线索 筛选 合适 的 
内容 并 基于 对 内容 的 分析 抽取 关注 的 
信息 最后 按照 写作 逻辑 组织 为 篇章 结果 非常 
擅长 挖掘 大 数据 中 蕴含 的 分布 关联 等 
信息 这种 自动 写作 的 典型 例子 包括 热点 组稿 
事件 脉络 排行 盘点 等 百度 NLP 的 智能 春联 
在 这类 自动 写作 任务 中 机器 基于 充分 的 
训练 数据 训练 模型 并 得到 创作 能力 可以 根据 
人类 的 指令 产出 符合 特定 格式 要求 的 创作 
结果 这种 自动 写作 的 典型 例子 包括 智能 写诗 
智能 对联 等 辅助 写作 提供 领域 热点 事件 发现 
热点 事件 脉络 文本 纠错 和 自动 摘要 能力 从 
素材 收集 文章 撰写 文章 检查 三 个 角度 辅 
助您 的 创作 提升 写作 效率 辅助 写作 的 目标 
是 为 人类 的 写作 过程 提供 辅助 按照 人 
的 写作 步骤 辅助 写作 主要 从 四个 角度 提供 
帮助 写 什么 如何写 如何 写好 如何 更好 地 分发 
写作 之前 算法 可以 通过 分析 当前 热点 事件 和 
话题 推荐 适合 创作 的 热门 话题 写作 过程 中 
算法 可以 提供 写作 素材 写作 风格 写作 内容 建议 
等 多角度 的 辅助 写作 完成 后 算法 可以 从 
纠错 配图 排版 等 多个 角度 提供 改进 建议 帮助 
人类 作者 完善 写作 结果 智能 写作 的 核心 技术 
1 . 经典 自然语言 生成 算法 从 篇章 规划 写 
什么 到 微观 规划 如何写 再到 表层 实现 转换 为 
自然语言 来 逐步 按照 流水线 进行 生成 算法 2 . 
神经网络 序列 生成 算法 深度 神经 网络 技术 为 人工智能 
带来 的 技术 变革 在 智能 写作 技术 中 的 
集中 体现 是 神经 网络 序列 生成 算法 这种 算法 
能够 有效 利用 语料 中 包含 的 统计 规律 按 
特定 要求 产出 符合 人类 语言 特性 的 文本 结果 
智能 写诗 是 机器 创作 的 常用 例子 也是 序列 
生成 算法 的 一个 典型 例子 在 生成 每 一句 
诗歌 时 关键词 和上/nr 一句 的 信息 会 经过 循环 
神经 网络结构 计算 作为 生成 诗歌 中 每一个 字 的 
依据 模型 在 学习 过 大量 诗歌 语料 之后 能够 
具备 概率 统计 意义上 输出 像 诗歌 的 字 序列 
的 能力 这种 能力 即 对应 机器 创作 型 智能 
写作 能够 根据 需求 生成 诗歌 虽然 机器 的 创作 
思路 和/c 人类/n 有/v 本质/n 的/uj 不同/a 但是 机器 生成 
的 诗歌 与人 写 的 诗歌 效果 相当 因此 能够 
帮助 人类 分担 相应 的 工作量 标题 生成 是 在 
辅助 写作 中 有 广泛 的 应用 完成 写作 之后 
如果 能够 快速 确定 一个 优质 的 标题 不仅 节省 
作者 的 人力 投入 也 有利于 写作 结果 的 分发 
让/v 写作/v 结果/n 更好/d 地/uv 触及/v 相/v 对应/vn 需求/v 和/c 
兴趣/n 的/uj 读者/n 3 . 文本 分析 技术 文本 分析 
技术 主要 是 关注 作为 智能 写作 素材 的 输入 
对于 各类 素材 需要 利用 文本 分析 技术 抽取 关键词 
标签 情感 倾向 摘要 等 用于 智能 写作 的 特征 
文章 来 至 百度 AI1 请 列出 几种 文本 特征提取 
算法 答 文档 频率 信息 增益 互信息 X ^ 2 
统计 TF IDF2 简述 几种 自然语言 处理 开源 工具包 答 
LingPipe FudanNLP OpenNLP CRF + + Standord CoreNLP IKAnalyzer3 简述/v 
无/v 监督/vn 和有/nr 监督/vn 算法/n 的/uj 区别/n 答/v 1 有 
监督 学习 对 具有 概念 标记 分类 的 训练 样本 
进行 学习 以 尽可能 对 训练样本 集外 的 数据 进行 
标记 分类 预测 这里 所有 的 标记 分类 是 已知 
的 因此 训练样本 的 岐义 性 低 无 监督 学习 
对 没有 概念 标记 分类 的 训练 样本 进行 学习 
以 发现 训练样本 集中 的 结构 性 知识 这里 所有 
的 标记 分类 是 未知 的 因此 训练样本 的 岐义 
性 高 聚 类 就是 典型 的 无 监督 学习 
2 有 监督 学习 的 样本 全部 带 标记 无 
监督 学习 的 样本 全部 不带 标记 PS 部 分带 
标记 的 是 半 监督 学习 3 训练/vn 集/q 有/v 
输入/v 有/v 输出/v 是/v 有/v 监督/vn 包括 所有 的 回归 
算法 分类 算法 比如 线性 回归 决策树 神经网络 KNN SVM 
等 训练 集 只有 输入 没有 输出 是 无 监督 
包括 所有 的 聚 类 算法 比如 k means PCA 
GMM 等 4 请 简述 几种 熟悉 的 分类 算法 
答 kNN kMeans 决策树 随机 森林 等 5 以下 代码 
是 Java 实现 中文分词 请 简述 分词 过程 public class 
p l i t C h i n e s 
e C h a r a c t e r 
{ public static void main String args { String input 
= 太好了 今天 是 星期六 啊 new Split input . 
start } } class Split { private String dictionary = 
{ 今天 是 星期 星期六 } private String input = 
null public Split String input { this . input = 
input } public void start { String temp = null 
System . out . println this . input . length 
for int i = 0 i this . input . 
length i + + { temp = this . input 
. substring i if this . isInDictionay temp { System 
. out . println temp this . input = this 
. input . replace temp i = 1 } } 
if null = this . input & & . equals 
this . input { this . input = this . 
input . substring 0 this . input . length 1 
this . start } } public boolean isInDictionay String temp 
{ for int i = 0 i this . dictionary 
. length i + + { if temp . equals 
this . dictionary i { return true } } return 
false } } 运行 结果 星期六 是 今天 更多 内容 
请 关注 微信 公众 号 本文 简要介绍 Python 自然语言 处理 
NLP 使用 Python 的 NLTK 库 NLTK 是 Python 的 
自然 语言 处理 工具包 在 NLP 领域 中 最常 使用 
的 一个 Python 库 什么 是 NLP 简单 来说 自然语言 
处理 NLP 就是 开发 能够 理解 人类 语言 的 应用 
程序 或 服务 这里 讨论 一些 自然 语言 处理 NLP 
的 实际 应用 例子 如 语音识别 语音 翻译 理解 完整 
的 句子 理解 匹配 词 的 同义词 以及 生成 语法 
正确 完整 句子 和 段落 这 并 不是 NLP 能做 
的 所有 事情 NLP 实现 搜索引擎 比如 谷歌 Yahoo 等 
谷歌 搜索引擎 知道 你 是 一个 技术 人员 所以 它 
显示 与 技术 相关 的 结果 社交 网站 推送 比如 
Facebook News Feed 如果 News Feed 算法 知道 你 的 
兴趣 是 自然 语言 处理 就会 显示 相关 的 广告 
和 帖子 语音 引擎 比如 Apple 的 Siri 垃圾邮件 过滤 
如 谷歌 垃圾邮件 过滤器 和 普通 垃圾邮件 过滤 不同 它 
通过 了 解 邮件 内容 里面 的 的 深层 意义 
来 判断 是 不是 垃圾 邮件 NLP 库 下面 是 
一些 开源 的 自然 语言 处理 库 NLP Natural language 
toolkit NLTK Apache OpenNLP Stanford NLP suite Gate NLP library 
其中 自然 语言 工具包 NLTK 是 最 受欢迎 的 自然 
语言 处理 库 NLP 它 是 用 Python 编写 的 
而且 背后 有 非常 强大 的 社区 支持 NLTK 也 
很容易 上手 实际上 它 是 最简单 的 自然 语言 处理 
NLP 库 在 这个 NLP 教程 中 我们 将 使用 
Python NLTK 库 安装 NLTK 如果 您 使用 的 是 
Windows / Linux / Mac 您 可以 使用 pip 安装 
NLTK pip install nltk 打开 python 终端 导入 NLTK 检查 
NLTK 是否 正确 安装 import nltk 如果 一切 顺利 这 
意味着 您 已经 成功 地 安装 了 NLTK 库 首次 
安装 了 NLTK 需要 通过 运行 以下 代码 来 安装 
NLTK 扩展 包 import nltk nltk . download 这将 弹出 
NLTK 下载 窗口 来 选择 需要 安装 哪些 包 您 
可以 安装 所有 的 包 因为 它们 的 大小 都 
很小 所以 没有 什么 问题 使用 Python Tokenize 文本 首先 
我们 将 抓取 一个 web 页面 内容 然后 分析 文本 
了解 页面 的 内容 我们 将 使用 urllib 模块 来 
抓取 web 页面 import urllib . request response = urllib 
. request . urlopen http / / php . net 
/ html = response . read print html 从 打印 
结果 中 可以 看到 结果 包含 许多 需要 清理 的 
HTML 标签 然后 BeautifulSoup 模块 来 清洗 这样 的 文字 
from bs4 import BeautifulSoup import urllib . request response = 
urllib . request . urlopen http / / php . 
net / html = response . read soup = BeautifulSoup 
html html5lib 这 需要 安装 html5lib 模块 text = soup 
. get _ text strip = True print text 现在 
我们 从 抓取 的 网页 中 得到 了 一个 干净 
的 文本 下 一步 将 文本 转换 为 tokens 像这样 
from bs4 import BeautifulSoup import urllib . request response = 
urllib . request . urlopen http / / php . 
net / html = response . read soup = BeautifulSoup 
html html5lib text = soup . get _ text strip 
= True tokens = text . split print tokens 统计 
词频 text 已经 处理 完毕 了 现在 使用 Python NLTK 
统计 token 的 频率分布 可以 通过 调用 NLTK 中的 FreqDist 
方法 实现 from bs4 import BeautifulSoup import urllib . request 
import nltk response = urllib . request . urlopen http 
/ / php . net / html = response . 
read soup = BeautifulSoup html html5lib text = soup . 
get _ text strip = True tokens = text . 
split freq = nltk . FreqDist tokens for key val 
in freq . items print str key + + str 
val 如果 搜索 输出 结果 可以 发现 最 常见 的 
token 是 PHP 您 可以 调用 plot 函数 做出 频率 
分布图 freq . plot 20 cumulative = False # 需要 
安装 matplotlib 库 这 上面 这些 单词 比如 of a 
an 等等 这些 词 都 属于 停用词 一般来说 停用词 应该 
删除 防止 它们 影响 分析 结果 处理 停用词 NLTK 自带 
了 许多 种 语言 的 停用词 列表 如果 你 获取 
英文 停用词 from nltk . corpus import stopwords stopwords . 
words english 现在 修 改下 代码 在 绘图 之前 清除 
一些 无效 的 token clean _ tokens = list sr 
= stopwords . words english for token in tokens if 
token not in sr clean _ tokens . append token 
最终 的 代码 应该 是 这样 的 from bs4 import 
BeautifulSoup import urllib . request import nltk from nltk . 
corpus import stopwords response = urllib . request . urlopen 
http / / php . net / html = response 
. read soup = BeautifulSoup html html5lib text = soup 
. get _ text strip = True tokens = text 
. split clean _ tokens = list sr = stopwords 
. words english for token in tokens if not token 
in sr clean _ tokens . append token freq = 
nltk . FreqDist clean _ tokens for key val in 
freq . items print str key + + str val 
现在 再 做 一次 词频 统计图 效果 会 比 之前 
好些 因为 剔除 了 停用词 freq . plot 20 cumulative 
= False 使用 NLTK Tokenize 文本 在 之前 我们 用 
split 方法 将 文本 分割 成 tokens 现在 我们 使用 
NLTK 来 Tokenize 文本 文本 没有 Tokenize 之前 是 无法 
处理 的 所以 对 文本 进行 Tokenize 非常 重要 的 
token 化 过程 意味着 将 大 的 部件 分割为 小 
部件 你 可以 将 段落 tokenize 成 句子 将 句子 
tokenize 成 单个 词 NLTK 分别 提供 了 句子 tokenizer 
和 单词 tokenizer 假如 有 这样 这段 文本 Hello Adam 
how are you I hope everything is going well . 
Today is a good day see you dude 使用 句子 
tokenizer 将 文本 tokenize 成 句子 from nltk . tokenize 
import sent _ tokenize mytext = Hello Adam how are 
you I hope everything is going well . Today is 
a good day see you dude . print sent _ 
tokenize mytext 输出 如下 Hello Adam how are you I 
hope everything is going well . Today is a good 
day see you dude . 这是 你 可能 会想 这也 
太 简单 了 不 需要 使用 NLTK 的 tokenizer 都可以 
直接 使用 正则表达式 来 拆分 句子 就行 因为 每个 句子 
都有 标点 和 空格 那么 再 来看 下面 的 文本 
Hello Mr . Adam how are you I hope everything 
is going well . Today is a good day see 
you dude . 这样 如果 使用 标点符号 拆分 Hello Mr 
将会 被 认为 是 一个 句子 如果 使用 NLTK from 
nltk . tokenize import sent _ tokenize mytext = Hello 
Mr . Adam how are you I hope everything is 
going well . Today is a good day see you 
dude . print sent _ tokenize mytext 输出 如下 Hello 
Mr . Adam how are you I hope everything is 
going well . Today is a good day see you 
dude . 这才 是 正确 的 拆分 接下来 试试 单词 
tokenizer from nltk . tokenize import word _ tokenize mytext 
= Hello Mr . Adam how are you I hope 
everything is going well . Today is a good day 
see you dude . print word _ tokenize mytext 输出 
如下 Hello Mr . Adam how are you I hope 
everything is going well . Today is a good day 
see you dude . Mr . 这个词 也 没有 被 
分开 NLTK 使用 的 是 punkt 模块 的 P u 
n k t e n t e n c e 
T o k e n i z e r 它 
是 NLTK . tokenize 的 一部分 而且 这个 tokenizer 经过训练 
可以 适用 于 多种语言 非 英文 TokenizeTokenize 时 可以 指定 
语言 from nltk . tokenize import sent _ tokenize mytext 
= Bonjour M . Adam comment allez vous J esp 
è re que tout va bien . Aujourd hui est 
un bon jour . print sent _ tokenize mytext french 
输出 结果 如下 Bonjour M . Adam comment allez vous 
J esp è re que tout va bien . Aujourd 
hui est un bon jour . 同义词 处理 使用 nltk 
. download 安装 界面 其中 一个 包是/nr WordNet WordNet 是 
一个 为 自然语言 处理 而 建立 的 数据库 它 包括 
一些 同义 词组 和 一些 简短 的 定义 您 可以 
这样 获取 某个 给定 单词 的 定义 和 示例 from 
nltk . corpus import wordnet syn = wordnet . synsets 
pain print syn 0 . definition print syn 0 . 
examples 输出 结果 是 a symptom of some physical hurt 
or disorder the patient developed severe pain and distension WordNet 
包含 了 很多 定义 from nltk . corpus import wordnet 
syn = wordnet . synsets NLP print syn 0 . 
definition syn = wordnet . synsets Python print syn 0 
. definition 结果 如下 the branch of information science that 
deals with natural language information large Old World boas 可以 
像 这样 使用 WordNet 来 获取 同义词 from nltk . 
corpus import wordnet synonyms = for syn in wordnet . 
synsets Computer for lemma in syn . lemmas synonyms . 
append lemma . name print synonyms 输出 computer computing _ 
machine computing _ device data _ processor electronic _ computer 
information _ processing _ system calculator reckoner figurer estimator computer 
反义词 处理 也 可以 用 同样 的 方法 得到 反义词 
from nltk . corpus import wordnet antonyms = for syn 
in wordnet . synsets small for l in syn . 
lemmas if l . antonyms antonyms . append l . 
antonyms 0 . name print antonyms 输出 large big big 
词干 提取 语言 形态学 和 信息检索 里 词干 提取 是 
去除 词缀 得到 词根 的 过程 例如 working 的 词干 
为 work 搜索引擎 在 索引 页面 时 就会 使用 这种 
技术 所以 很多 人 为 相同 的 单词 写出 不同 
的 版本 有 很多 种 算法 可以 避免 这种 情况 
最 常见 的 是 波特 词干 算法 NLTK 有 一个 
名为 PorterStemmer 的 类 就是 这个 算法 的 实现 from 
nltk . stem import PorterStemmer stemmer = PorterStemmer print stemmer 
. stem working print stemmer . stem worked 输出 结果 
是 work work 还有 其他 的 一些 词干 提取 算法 
比如 Lancaster 词干 算法 非 英文 词干 提取 除了 英文 
之外 SnowballStemmer 还 支持 13种 语言 支持 的 语言 from 
nltk . stem import SnowballStemmer print SnowballStemmer . languages danish 
dutch english finnish french german hungarian italian norwegian porter portuguese 
romanian russian spanish swedish 你 可以 使用 SnowballStemmer 类 的 
stem 函数 来 提取 像 这样 的 非 英文单词 from 
nltk . stem import SnowballStemmer french _ stemmer = SnowballStemmer 
french print french _ stemmer . stem French word 单词 
变体 还原 单词 变体 还原 类似于 词干 但 不同 的 
是 变体 还原 的 结果 是 一个 真实 的 单词 
不同于 词干 当 你 试图 提取 某些 词 时 它 
会 产生 类似 的 词 from nltk . stem import 
PorterStemmer stemmer = PorterStemmer print stemmer . stem increases 结果 
increas 现在 如果 用 NLTK 的 WordNet 来 对 同一 
个 单词 进行 变体 还原 才是 正确 的 结果 from 
nltk . stem import W o r d N e 
t L e m m a t i z e 
r lemmatizer = W o r d N e t 
L e m m a t i z e r 
print lemmatizer . lemmatize increases 结果 increase 结果 可能会 是 
一个 同义词 或 同一 个 意思 的 不同 单词 有时候 
将 一个 单词 做 变体 还 原时 总是 得到 相同 
的 词 这 是因为 语言 的 默认 部分 是 名词 
要 得到 动词 可以 这样 指定 from nltk . stem 
import W o r d N e t L e 
m m a t i z e r lemmatizer = 
W o r d N e t L e m 
m a t i z e r print lemmatizer . 
lemmatize playing pos = v 结果 play 实际上 这也 是 
一种 很好 的 文本 压缩 方式 最终 得到 文本 只有 
原先 的 50% 到 60% 结果 还 可以 是 动词 
v 名词 n 形容词 a 或 副词 r from nltk 
. stem import W o r d N e t 
L e m m a t i z e r 
lemmatizer = W o r d N e t L 
e m m a t i z e r print 
lemmatizer . lemmatize playing pos = v print lemmatizer . 
lemmatize playing pos = n print lemmatizer . lemmatize playing 
pos = a print lemmatizer . lemmatize playing pos = 
r 输出 play playing playing playing 词干 和 变体 的 
区别 通过 下面 例子 来 观察 from nltk . stem 
import W o r d N e t L e 
m m a t i z e r from nltk 
. stem import PorterStemmer stemmer = PorterStemmer lemmatizer = W 
o r d N e t L e m m 
a t i z e r print stemmer . stem 
stones print stemmer . stem speaking print stemmer . stem 
bedroom print stemmer . stem jokes print stemmer . stem 
lisa print stemmer . stem purple print print lemmatizer . 
lemmatize stones print lemmatizer . lemmatize speaking print lemmatizer . 
lemmatize bedroom print lemmatizer . lemmatize jokes print lemmatizer . 
lemmatize lisa print lemmatizer . lemmatize purple 输出 stone speak 
bedroom joke lisapurplstone speaking bedroom joke lisa purple 词干 提取 
不会 考虑 语境 这也 是 为什么 词干 提取 比 变体 
还原 快 且 准确度 低 的 原因 个人认为 变体 还原 
比 词干 提取 更好 单词 变体 还原 返回 一个 真实 
的 单词 即使 它 不是 同一 个 单词 也是 同义词 
但 至少 它 是 一个 真实 存在 的 单词 如果 
你 只 关心 速度 不在意 准确度 这时 你 可以 选用 
词干 提取 在此 NLP 教程 中 讨论 的 所有 步骤 
都 只是 文本 预处理 在 以后 的 文章 中 将会 
使用 Python NLTK 来 实现 文本 分析 我 已经 尽量 
使 文章 通俗易懂 希望 能对你/nr 有所 帮助 欢迎 加入 学习 
交流 QQ 群 657341423SnowNLP 是 一个 python 写 的 类库 
可以 方便 的 处理 中文 文本 内容 如 中文分词 词性 
标注 情感 分析 文本 分类 提取 文本 关键词 文本 相似 
度 计算 安装 pip install snownlp 完成 snownlp 安装 后 
查看 模块 的 目录 结构 如图所示 normal 文字 转换 成 
拼音 seg 中文分词 sentiment 情感 分析 sim 文本 相似 度 
summary 提取 文本 摘要 tag 词性 标注 _ _ init 
_ _ . py 整个 模块 的 函数 方法 想 
了解 snownlp 可以 打开 _ _ init _ _ . 
py 查看 snownlp 提供 的 方法 函数 # * coding 
utf 8 * from _ _ future _ _ import 
unicode _ literals from . import normal from . import 
seg from . import tag from . import sentiment from 
. sim import bm25 from . summary import textrank from 
. summary import words _ merge class SnowNLP object def 
_ _ init _ _ self doc self . doc 
= doc self . bm25 = bm25 . BM25 doc 
@ property def words self return seg . seg self 
. doc @ property def sentences self return normal . 
get _ sentences self . doc @ property def han 
self return normal . zh2hans self . doc @ property 
def pinyin self return normal . get _ pinyin self 
. doc @ property def sentiments self return sentiment . 
classify self . doc @ property def tags self words 
= self . words tags = tag . tag words 
return zip words tags @ property def tf self return 
self . bm25 . f @ property def idf self 
return self . bm25 . idf def sim self doc 
return self . bm25 . simall doc def summary self 
limit = 5 doc = sents = self . sentences 
for sent in sents words = seg . seg sent 
words = normal . filter _ stop words doc . 
append words rank = textrank . TextRank doc rank . 
solve ret = for index in rank . top _ 
index limit ret . append sents index return ret def 
keywords self limit = 5 merge = False doc = 
sents = self . sentences for sent in sents words 
= seg . seg sent words = normal . filter 
_ stop words doc . append words rank = textrank 
. KeywordTextRank doc rank . solve ret = for w 
in rank . top _ index limit ret . append 
w if merge wm = words _ merge . SimpleMerge 
self . doc ret return wm . merge return ret 
整个 snownlp 模块 就 提供 这些 方法 函数 给 我们 
使用 具体 的 使用 方式 以 官方 文档 为例 from 
snownlp import SnowNLP s = SnowNLP u 这个东西 真心 很 
赞 # 分词 s . words # u 这个 u 
东西 u 真心 # u 很 u 赞 # 词语 
标注 s . tags # u 这个 u r u 
东西 u n # u 真心 u d u 很 
u d # u 赞 u Vg # 情感 分析 
s . sentiments # 0 . 9769663402895832 positive 的 概率 
# 转换 拼音 s . pinyin # u zhe u 
ge u dong u xi # u zhen u xin 
u hen u zan s = SnowNLP u 「 繁 
體 字 」 「 繁 體 中文 」 的 叫法 
在 臺 灣 亦 很 常見/nr # 转换 简体 s 
. han # u 「 繁体字 」 「 繁体中文 」 
的 叫法 # 在 台湾 亦 很 常见 text = 
u 自然语言 处理 是 计算机 科学 领域 与 人工智能 领域 
中 的 一个 重要 方向 它/r 研究/vn 能/v 实现/v 人/n 
与/p 计算机/n 之间/f 用/p 自然/d 语言/n 进行/v 有效/a 通信/l 的/uj 
各种/r 理论/n 和/c 方法/n 自然语言 处理 是 一门 融 语言学 
计算机科学 数学 于 一体 的 科学 因此 这一 领域 的 
研究 将 涉及 自然语言 即 人们 日常 使用 的 语言 
所以 它 与 语言学 的 研究 有着 密切 的 联系 
但又 有 重要 的 区别 自然语言 处理 并 不是 一般 
地 研究 自然语言 而在于 研制 能 有效 地 实现 自然 
语言 通信 的 计算机 系统 特别 是 其中 的 软件 
系统 因而 它 是 计算机 科学 的 一部分 s = 
SnowNLP text # 提取 关键字 s . keywords 3 # 
u 语言 u 自然 u 计算机 # 提取 摘要 s 
. summary 3 # u 因而 它 是 计算机 科学 
的 一部分 # u 自然语言 处理 是 一门 融 语言学 
计算机科学 # 数学 于 一体 的 科学 # u 自然语言 
处理 是 计算机 科学 领域 与 人工智能 # 领域 中 
的 一个 重要 方向 # 文本 分句 处理 temp _ 
list = s . sentences s = SnowNLP 这篇 文章 
那篇 论文 这个 # TF IDF 算法 s . tf 
s . idf # 文本 相似 度 从s/nr 对象 中 
找出 与 sim 文章 相似 的 文本 s . sim 
文章 # 0 . 3756070762985226 0 0 关于 训练 训练 
是 更好 地 完善 现有 的 语料库 现在 提供 训练 
的 包括 分词 词性 标注 情感 分析 以 分词 为例 
分词 在 snownlp / seg 目 录下 # 分词 训练 
from snownlp import seg seg . train data . txt 
seg . save seg . marshal # 词性 标注 训练 
# from snownlp import tag # tag . train 199801 
. txt # tag . save tag . marshal # 
情感 分析 训练 # from snownlp import sentiment # sentiment 
. train neg . txt pos . txt # sentiment 
. save sentiment . marshal 这样 训 练好 的 文件 
就 存储 为 seg . marshal 了 之后 修改 snownlp 
/ seg / init . py 里 的 data _ 
path 指向 刚 训 练好 的 文件 即可 声明 转载 
请 注明 出处 谢谢 https / / blog . csdn 
. net / m0 _ 37306360 / article / details 
/ 84502176 另外 更多 实时 更新 的 个人 学习 笔记 
分享 请 关注 知乎 https / / www . zhihu 
. com / people / yuquanle / columns 公众 号 
StudyForAI 今天 总结 一下 自然语言 处理 之 词性 标注 后附 
现有 比较好 的 开源 实现 工具 基于 python 实现 包 
~ ~ ~ 词性 定义 百度 百科 定义 词性 指 
以 词 的 特点 作为 划分 词类 的 根据 词类 
是 一个 语言学 术语 是 一种 语言 中词 的 语法 
分类 是以 语法 特征 包括 句法 功能 和 形态 变化 
为 主要 依据 兼顾 词汇 意义 对词 进行 划分 的 
结果 维基百科 定义 In traditional grammar a part of speech 
abbreviated form PoS or POS is a category of words 
or more generally of lexical items which have similar grammatical 
properties/w ./i 从/p 组合/v 和/c 聚合/v 关系/n 来说/u 一个 词类 
是 指 在 一个 语言 中 众多 具有 相同 句法 
功能 能在 同样 的 组合 位置 中 出现 的 词 
聚合 在 一起 形成 的 范畴 词类 是 最 普遍 
的 语法 的 聚合 词类 划分 具有 层次性 如 汉语 
中 词 可以 分成 实词 和 虚词 实 词中 又 
包括 体词 谓词 等 体 词中 又 可以 分出 名词 
和 代词 等 词性 标注 就是 在 给定 句子 中 
判定 每个 词 的 语法 范畴 确定 其 词性 并 
加以 标注 的 过程 这也 是 自然 语言 处理 中 
一项 非常 重要 的 基础性 工作 所有 对于 词性 标注 
的 研究 已经 有 较长 的 时间 在 研究 者 
长期 的 研究 总结 中 发现 汉语 词性 标注 中 
面临 了 许多 棘手 的 问题 中文 词性 标注 的 
难点 汉语 是 一种 缺乏 词 形态 变化 的 语言 
词 的 类别 不能 像 印欧语 那样 直接 从词的/nr 形态 
变化 上 来 判别 常用词 兼类 现象 严重 现代汉语 八百 
词 收取 的 常用 词中 兼类 词 所占 的 比例 
高达 22.5% 而且 发现 越是 常用 的 词 不同 的 
用法 越多 由于 兼类 使用 程度 高 兼类 现象 涉及 
汉语 中 大部分 词类 因而 造成 在 汉语 文本 中 
词类 歧义 排除 的 任务 量大 研究者 主观 原因 造成 
的 困难 语言学界 在 词性 划分 的 目的 标准 等 
问题 上 还 存在 分歧 目前 还 没有 一个 统 
的 被 广泛 认可 汉语 词类 划分 标准 词类/n 划分/v 
的/uj 粒度/n 和/c 标记符号/n 都/d 不统一/i 词类 划分 标准 和 
标记符号 集 的 差异 以及 分词 规范 的 含 混性 
给 中文信息处理 带来 了 极大 的 困难 词性 标注 常见 
方法 基于 规则 的 词性 标注 方法 基于 规则 的 
词性 标注 方法 是 人们 提出 较早 的 一种 词性 
标注 方法 其 基本 思想 是 按 兼类 词 搭配 
关系 和 上下文 语境 建造 词类 消 歧 规则 早期 
的 词类 标注 规则 一般 由 人工 构建 随着 标注 
语料库 规模 的 增大 可 利用 的 资源 也 变得 
越来越 多 这时候 以 人工 提取 规则 的 方法 显然 
变得 不 现实 于是乎 人们 提出 了 基于 机器 学习 
的 规则 自动 提出 方法 基于 统计模型 的 词性 标注 
方法 统计 方法 将 词性 标注 看作 是 一个 序列 
标注 问题 其 基本 思想 是 给定 带有 各自 标注 
的 词 的 序列 我们 可以 确定 下 一个 词 
最 可能 的 词性 现在 已经 有隐/nr 马尔可夫 模型 HMM 
或 条件 随机 域 CRF 等 统计模型 了 这些 模型 
可以 使用 有 标记 数据 的 大型 语料库 进行 训练 
而有 标记 的 数据 则是 指 其中 每 一个 词 
都 分配 了 正确 的 词性 标注 的 文本 基于/p 
统计/v 方法/n 与/p 规则/n 方法/n 相/v 结合/v 的/uj 词性/n 标注/v 
方法/n 理性主义/n 方法/n 与/p 经验主义/n 相结合/v 的/uj 处理/v 策略/n 一直/d 
是/v 自然/d 语言/n 处理/v 领域/n 的/uj 专家/n 们/k 不断/d 研究/vn 
和/c 探索/v 的/uj 问题/n 对于 词性 标注 问题 当然 也不例外 
这类 方法 的 主要 特点 在于 对 统计 标注 结果 
的 筛选 只对 那些 被 认为 可疑 的 标注 结果 
才 采用 规则 方法 进行 歧义 消解 而 不是 对 
所有 情况 都既/nr 使用 统计 方法 又 使用 规则 方法 
基于 深度 学习 的 词性 标注 方法 可以 当作 序列 
标注 的 任务 来做 目前 深度 学习 解决 序列 标注 
任务 常用 方法 包括 LSTM + CRF BiLSTM + CRF 
等 * * 词性 标注 任务 数据集 * * 人民日报 
1998 词性 标注 数据集 https / / pan . baidu 
. com / s / 1 f W 9 0 
8 E Q m y M v 0 X B 
5 i 0 D h V y Q 词性 标注 
工具 推荐 Jieba 结巴 中文分词 做 最好 的 Python 中文分词 
组件 可以 进行 词性 标注 Github 地址 https / / 
github . com / fxsjy / jiebaSnowNLP SnowNLP 是 一个 
python 写 的 类库 可以 方便 的 处理 中文 文本 
内容 Github 地址 https / / github . com / 
isnowfy / snownlpTHULAC THULAC THU Lexical Analyzer for Chinese 由 
清华大学 自然语言 处理 与 社会 人文 计算 实验室 研制 推出 
的 一套 中文 词 法分析 工具包 具有 中文分词 和 词性 
标注 功能 Github 地址 https / / github . com 
/ thunlp / THULAC 官网 http / / thulac . 
thunlp . org / StanfordCoreNLP 斯坦福 的 具备 各种 nlp 
功能 包括 词性 标注 Github 地址 https / / github 
. com / Lynten / stanford corenlp 官网 https / 
/ stanfordnlp . github . io / CoreNLP / Hanlp 
HanLP 是 一系列 模型 与 算法 组成 的 NLP 工具包 
由 大 快 搜索 主导 并 完全 开源 目标 是 
普及 自然语言 处理 在 生产 环境 中 的 应用 Github 
地址 https / / github . com / hankcs / 
pyhanlp 官网 http / / hanlp . linrunsoft . com 
/ NLTK NLTK 是 一个 高效 的 Python 构建 的 
平台 用来 处理 人类 自然语言 数据 Github 地址 https / 
/ github . com / nltk / nltk 官网 http 
/ / www . nltk . org / SpaCy 工业级 
的 自然 语言 处理 工具 遗憾 的 是 不支持 中文 
Gihub 地址 https / / github . com / explosion 
/ spaCy 官网 https / / spacy . io / 
最新 研究 进展 看 这里 https / / github . 
com / sebastianruder / NLP progress / blob / master 
/ english / part of speech _ tagging . md 
参考 1 . 统计 自然语言 处理 01 自然语言 处理 与 
文本 挖掘 概述 02 自动机 及其 应用 文稿 自动 校正 
歧义 消除 03 语言 模型 平滑 方法 应用 案例 语音识别 
分词 消 岐 04 概率 图 模型 生成式 模型 与 
判别式 模型 贝叶斯 网 马尔科夫 链 隐 马尔科夫 模型 HMM 
应用 案例 语音 识别 与 分词 05 马尔科夫 网 最大熵 
模型 条件 随 机场 CRF 实现 HMM 和 CRF 的 
软件 应用 案例 使用 最大熵 消除歧义 使用 CRF 进行 标注 
06 汉语分词 专题 世界 上 最难 的 语言 名不虚传 07 
命名 实体 识别 词性 标注 从文 本里 挖出 最 重要 
的 内容 08 句法分析 找出 句子 的 重点 09 语义分析 
与 篇章 分析 让 机器 象 语言学家 那样 思考 10 
文本 分类 情感 分析 应用 案例 互联网 自动 门户 评论 
倾向性 分析 11 信息 检索系统 搜索引擎 原理 问答 系统 应用 
案例 客服 机器人 是 怎么 造 出来 的 12 文本 
深度 挖掘 自动 文摘 与 信息 抽取 13 机器 翻译 
与 语音 识别 技术 介绍 IBM Watson 系统 的 认知 
智慧 分词 算法 下载 地址 百度 网盘 下载 人工智能 人工智能 
英文 缩写 为 AI 它 是 研究 开发 用于 模拟 
延伸 和 扩展 人 的 智能 的 理论 方法 技术 
及 应用 系统 的 一门 新的 技术 科学 人工智能 是 
计算机 科学 研究 领域 的 一个 重要 分支 又是 众多 
学科 的 一个 交叉 学科 它 企图 了解 智能 的 
实质 并 生产 出 一种 新的 能以 人类 智能 相似 
的 方式 做出 反应 的 智能 机器 该 领域 的 
研究 包括 语音识别 图像识别 机器人 自然语言 处理 智能 搜索 和 
专家 系统 等等 人工智能 可以 对人 的 意识 思维 的 
信息 过程 的 模拟 人工智能 包括 众多 的 分支 领域 
比如 大家 熟悉 的 机器学习 自然语言 理解 和 模式 识别 
等 机器学习 机器学习 属于 人工智能 研究 与 应用 的 一个 
分支 领域 机器 学习 的 研究 更加 偏向 理论性 其/r 
目的/n 更/d 偏向/n 于是/nr 研究/vn 一种/m 为了/p 让/v 计算机/n 不断/d 
从/p 数据/n 中/f 学习/v 知识/v 而使 机器学习 得到 的 结果 
不断 接近 目标 函数 的 理论 机器学习 引用 卡内基 梅隆 
大学 机器学习 研究 领域 的 着 名教授 TomMitchell 的 经典 
定义 如果 一个 程序 在 使用 既有 的 经验 E 
Experience 来 执行 某类 任务 T Task 的 过程 中 
被 认为 是 具备 学习 能力 的 那么 它 一定 
要 展现 出 利用 现有 的 经验 E 不断 改善 
其 完成 既定 任务 T 的 性能 Performance 的 特质 
机器学习 已经 有了/nr 十分 广泛 的 应用 例如 数据挖掘 计算机 
视觉 自然语言 处理 生物 特征 识别 搜索引擎 医学 诊断 检测 
信用卡 欺诈 证券 市场 分析 DNA 序列 测序 语音 和 
手写识别 战略游戏 和 机器 人 运用 在 我们 当下 的 
生活 中 语音输入 识别 手写输入 识别 等 技术 识别率 相比 
之前 若干年 的 技术 识别率 提升 非常 巨大 达到 了 
将近 97% 以上 大家 可以 在 各自 的 手机 上 
体验 这些 功能 这些 技术 来自 于 机器学习 技术 的 
应用 人工智能 大 数据 云计算 和 物联网 的 未来 发展 
值得 重视 均为 前沿 产业 多/m 智/ng 时代/n 专注/v 于/p 
人工智能/n 和大/nr 数据/n 的/uj 入门/ns 和科谱/nr 在 此为 你 推荐 
几篇 优质 好文 AI vs 深度 学习 vs 机器学习 人工智能 
的 12 大 应用 场景 http / / www . 
duozhishidai . com / article 15385 1 . html 人工智能 
全景图 与 发展 趋势 分析 http / / www . 
duozhishidai . com / article 15301 1 . html 在 
网络 大 时代 背景 下 人工智能 技术 是 如何 应用 
的 http / / www . duozhishidai . com / 
article 15277 1 . html 多 智 时代 人工智能 和大/nr 
数据 学习 入门 网站 | 人工智能 大 数据 物联网 云计算 
的 学习 交流 网站 更多 内容 请 至 南木 博客 
主页 查看 哦 中文分词 工具 J i e b a 
n o w N L P T H U L 
A C N L P I R N L P 
I R t a n f o r d C 
o r e N L P H a n L 
P 英文 分词 工具 n l t k n l 
t k n l t k p a c y 
p a c y t a n f o r 
d C o r e N L P 更多 关于 
自然 语言 处理 的 内容 请 转至 自然语言 处理 一 
中英文 分词 自然语言 处理 二 词性 标注 与 命名 实体 
识别 自然语言 处理 三 句法分析 与 依存 句法分析 哈工大 依存 
句法分析 工具 LTP 的 使用 与 安装 进行 查看 同时 
也 欢迎 各位 关注 我 的 微信 公众 号 南木 
的 下午茶 我 是 一名 计算机 学院 的 大二 学生 
本学期 选 修了 大 数据 与 人工智能 这门 课 通过 
一 学期 的 学习 让 我 对 大 数据 与 
人工智能 有了 更多 的 了解 也 让 我 深深 地 
感受到 人工智能 在 当今 社会 的 广泛 应用 技术 正在 
快速 崛起 不断 延伸 到 各个 行业 当中 人工智能 发展 
到 现在 以及 拥有 很多 的 分支 包括 机器学习 神经网络 
自然语言 处理 深度 学习 等等 所以 接下来 我 想 谈谈 
人工智能 自然语言 处理 在 计算机 领域 的 应用 人工智能 技术 
主要 研究 的 目的 是 使 一台 计算机 或者 是 
一台 机器 完成 一些 需要 我们 人类 亲自动手 或者 动脑 
来 完成 的 工作 因此 人工智能 的 发展 历史 和 
计算机 科学 与 技术 的 发展 历史 密不可分 自然语言 处理 
NLP 是 人工智能 的 一个 分支 用于 分析 理解 和 
生成 自然语言 以 方便 人和 计算机 设备 进行 交流 以及 
人 与人 之间 的 交流 NLP 是 计算机 科学 人工智能 
语言学 关注 计算机 和 人类 自然 语言 之间 相互 作用 
的 领域 因此 自然语言 处理 是 与 人机交互 的 领域 
有关 的 那么 NLP 和我的/nr 专业 又有 哪些 联系 呢 
1 . 帮助 我 完成 学习 任务 前段/n 时间/n 和/c 
同学/n 要/v 完成/v 一个/m 班的小/nr 论文/nz 相似/v 度/zg 的/uj 分析/vn 
这 里面 就 需要 运用 NLP 的 相关 知识 我们 
首先 把 每篇 文章 进行 分词 等 一系列 操作 最后 
再 进行 文本 聚 类 得到 结果 分词 就是 nlp 
的 一个 应用 除此之外 只要 我 一直 在 计算机 领域 
学习 下去 那么 用 到 自然 语言 处理 技术 的 
地方 也 会 越来越 多 像 数据挖掘 深度 学习 等等 
NLP 是 一种 处理 数据 的 手段 它 让 我们 
看到 得到 更 直观 的 结果 2 . 找工作 随着 
大 数据 的 发展 人工智能 得到 了 全新 的 发展 
机遇 人工智能 技术 越来越 多 的 应用 到 我们 的 
工作 和 日常 生活 中 人工智能 相关 职业 的 发展 
前景 还是 非常 值得 期待 的 相关 的 产业 也 
会 得到 进一步 的 发展 相关 的 人才 需求 会 
得到 进一步 的 释放 所以 从事 人工智能 的 相关 工作 
是 一个 不错 的 选择 但是 不管 从事 什么 行业 
打铁 还 需 自身 硬 现在 我 的 主要 任务 
就是 学习 不断 充实 自己 尽管 NLP 不如 大 数据 
机器学习 听 起来 那么 火 但 实际上 我们 每天 都在/nr 
使用 它 文本 语音 翻译 信息检索 家里 的 聊天 机器人 
等等 现在 人工智能 领域 应用 广泛 我们 也 要 把握 
时代 不断前进 欢迎 加入 学习 交流 QQ 群 657341423 自然语言 
处理 是 人工智能 的 类别 之一 自然语言 处理 主要 有 
那些 功能 我们 以 百度 AI 为例 从 上述 的 
例子 可以 看到 自然语言 处理 最 基本 的 功能 是 
词 法分析 词法 分析 的 功能 主要 有 分词 分句 
词语 标注 词法 时态 适用于 英文 词语 关键词 提前 词干 
提取 由于 英文 和 中文 在 文化 上 存在 巨大 
的 差异 因此 Python 处理 英文 和 中文 需要 使用 
不同 的 模块 中文 处理 推荐 使用 jieba 模块 英文 
处理 推荐 使用 nltk 模块 模块 安装 方法 可 自行 
搜索 相关 资料 英文 处理 import nltk f = open 
aa . txt r encoding = utf 8 text = 
f . read f . close # sent _ tokenize 
文本 分句 处理 text 是 一个 英文 句子 或 文章 
value = nltk . sent _ tokenize text print value 
# word _ tokenize 分词 处理 分词 不支持 中文 for 
i in value words = nltk . word _ tokenize 
text = i print words # pos _ tag 词性 
标注 pos _ tag 以 一 组词 为 单位 words 
是 列表 组成 的 词语 列表 words = My name 
is Lucy tags = nltk . pos _ tag words 
print tags # 时态 过去 词 进行时 等 # 词语 
列表 的 时态 复原 如果 单词 是 全 变形 的 
无法 识别 from nltk . stem import PorterStemmer data = 
nltk . word _ tokenize text = worked presumably goes 
play playing played language = english ps = PorterStemmer for 
w in data print w ps . stem word = 
w # 单个 词语 的 时态 复原 如果 单词 是 
全 变形 的 无法 识别 from nltk . stem import 
SnowballStemmer snowball _ stemmer = SnowballStemmer english a = snowball 
_ stemmer . stem plays print a # 复数 复原 
如果 单词 是 全 变形 的 无法 识别 from nltk 
. stem import W o r d N e t 
L e m m a t i z e r 
wordnet _ lemmatizer = W o r d N e 
t L e m m a t i z e 
r a = wordnet _ lemmatizer . lemmatize leaves print 
a # 词干 提取 提前 每个 单词 的 关键词 然后 
可 进行 统计 得出 词频 from nltk . stem . 
porter import PorterStemmer porter = PorterStemmer a = porter . 
stem pets insurance print a from nltk . corpus import 
wordnet word = good # 返回 一个 单词 的 同义词 
和 反义词 列表 def Word _ synonyms _ and _ 
antonyms word synonyms = antonyms = list _ good = 
wordnet . synsets word for syn in list _ good 
# 获取 同义词 for l in syn . lemmas synonyms 
. append l . name # 获取 反义词 if l 
. antonyms antonyms . append l . antonyms 0 . 
name return set synonyms set antonyms # 返回 一个 单词 
的 同义词 列表 def Word _ synonyms word list _ 
synonyms _ and _ antonyms = Word _ synonyms _ 
and _ antonyms word return list _ synonyms _ and 
_ antonyms 0 # 返回 一个 单词 的 反义词 列表 
def Word _ antonyms word list _ synonyms _ and 
_ antonyms = Word _ synonyms _ and _ antonyms 
word return list _ synonyms _ and _ antonyms 1 
print Word _ synonyms word print Word _ antonyms word 
# 造句 print wordnet . synset name . n . 
01 . examples # 词义 解释 print wordnet . synset 
name . n . 01 . definition from nltk . 
corpus import wordnet # 词义 相似 度 . go . 
v . 01 的 go 为 词语 v 为 动词 
# w1 = wordnet . synset fulfil . v . 
01 # w2 = wordnet . synset finish . v 
. 01 # hello . n . 01 的 n 
为 名词 w1 = wordnet . synset hello . n 
. 01 w2 = wordnet . synset hi . n 
. 01 # 基于 路径 的 方法 print w1 . 
wup _ similarity w2 # Wu Palmer 提出 的 最短 
路径 print w1 . path _ similarity w2 # 词 
在 词典 层次结构 中的 最短 路径 print w1 . lch 
_ similarity w2 # Leacock Chodorow 最短 路径 加上 类别 
信息 # 基于 互 信息 的 方法 from nltk . 
corpus import genesis # 从 语料库 加载 信息内容 # brown 
_ ic = wordnet _ ic . ic ic brown 
. dat # nltk 自带 的 语料库 创建 信息 内容 
词典 genesis _ ic = wordnet . ic genesis False 
0.0 print w1 . res _ similarity w2 genesis _ 
ic print w1 . jcn _ similarity w2 genesis _ 
ic print w1 . lin _ similarity w2 genesis _ 
ic 由于 上述 的 方法 是 建立 在 语料库 中 
有时候 一些 不 被 记录 的 单词 可能 无法 识别 
或 标注 这时候 需要 自定义 词性 标注 器 词性 标注 
器 的 类型 有 几种 具体 教程 可以 看 自定义 
词性 标注 器 中文 处理 import jieba import jieba . 
analyse f = open aa . txt r encoding = 
utf 8 text = f . read f . close 
# 分词 seg _ list = jieba . cut text 
cut _ all = True print Full Mode + / 
. join seg _ list # 全 模式 seg _ 
list = jieba . cut text cut _ all = 
False print Default Mode + / . join seg _ 
list # 精确 模式 seg _ list = jieba . 
cut _ for _ search text # 搜索引擎 模式 print 
. join seg _ list # 关键字 提取 # 基于 
TF IDF 算法 的 关键词 抽取 # sentence 为 待 
提取 的 文本 # topK 为 返回 几个 TF / 
IDF 权重 最大 的 关键词 默认值 为 20 # withWeight 
为 是否 一并 返回 关键词 权重 值 默认值 为 False 
# allowPOS 仅 包括 指定 词性 的 词 默认值 为 
空 即 不 筛选 keywords = jieba . analyse . 
extract _ tags sentence = text topK = 20 withWeight 
= True allowPOS = n nr ns # 基于 TextRank 
算法 的 关键词 抽取 # keywords = jieba . analyse 
. textrank text topK = 20 withWeight = True allowPOS 
= n nr ns for item in keywords print item 
0 item 1 # 词语 标注 import jieba . posseg 
# 新建 自定义 分词器 tokenizer 参数 可 指定 内部 使用 
的 jieba . Tokenizer 分词器 jieba . posseg . dt 
为 默认 词性 标注 分词器 posseg = jieba . posseg 
. POSTokenizer tokenizer = None words = posseg . cut 
text for word flag in words print % s % 
s % word flag jieba 分词 也是 基于 语料库 我们 
可以 对 原有 的 语料库 添加 词语 或者 导入 自定义 
的 语料 文件 如下 所示 # 对 原有 的 语料库 
添加 词语 jieba . add _ word word freq = 
None tag = None # 导入 语料 文件 jieba . 
load _ userdict disney . txt 语料 文件格式 如下 每行 
分 三个 部分 用 空格 隔开 词语 词频 可 省 
词性 可 省 ns 是 词语 标记 词语 和 标注 
之间 用 空格 隔开 txt 文件格式 为 uft 8jieba 更多 
教程 jieba 教程 最近 在 研究 自然语言 处理 最 基础 
的 内容 之一 是 分词 处理 但是 分词 的 结果 
并非 均 是 有效 的 信息 按照 普遍 说法 存在 
停用词 这样 的 尴尬 信息 所谓 停用词 即是 在 自然 
语言 处理 时 与 文章 包含 的 情感 信息 或 
文章主题 信息 关系 性 不强 的 词语 所以 如果 进行 
筛选 过滤 之后 更 便于 主题 分析 或者 情感 分析 
这里 我 在 网上 找到 了 结合 哈工大 停用 词表 
四川大学 机器 智能 实验室 停用 词库 百度 停用 词表 以及 
网络 上 较大 的 一份 无 名称 停用 词表 并 
整理 了 一下 做了 去 重 处理 最终 得到 了 
一份 较 全的/nr 停用 词表 在此 分享 出来 给 大家 
希望 对 各位 有用 整合 的 停用 词表 下载 后续 
可能 即 需 更新 其他 相关 文章 逐步 积累 哈哈 
持续 更 新中 使用 的 是 python 自然语言 处理 这本书 
只给 部分 笔者 做 的 答案 不敢 保证 都对 仅供参考 
我 的 目录 持续 更 新中 . . . 第一章 
1 . 41.61 . 131.141 . 181.211 . 222.232 . 
242.252 . 262.28 第一章 1 . 4len text2 # 先 
弄 成都 小写 去掉 大小写 区别 在 求 个数 len 
set w . lower for w in set a 1 
. 6text2 . dispersion _ plot Elinor Marianne Edward Willoughby 
1 . 131.141 . 18sorted set sent1 + sent2 + 
sent3 . . . 1.211 . 22words = sorted w 
. lower for w in text5 if len w = 
= 4 fdist = FreqDist words fdist . most _ 
common # or plot the first 10 words fdist . 
plot 10 2.23 列表 w for w in text6 if 
w . isupper 每行 一个 2.24 a w for w 
in text6 if w . endswith ize b 可以 看到 
上面 有 重复 的 单词 故 将上 面的 text6 改为 
set text6 © d w for w in set text6 
if w . istitle 2 . 252.26 用来 求 全篇 
的 字符 / 字母 长度 如果 要求 字 的 平均 
长度 可以 如下 2 . 28def percent word text return 
FreqDist text word / len text 自然语言 处理 技术 的 
一些 应用 转载 https / / zhuanlan . zhihu . 
com / p / 31388720 自然语言 处理 NLP 是 现代 
计算机 科学 和 人工智能 领域 的 一个 重要 分支 是 
一门 融合 了 语言学 数学 计算机 科学 的 科学 这一 
领域 的 研究 将 涉及 自然语言 即 人们 日常 使用 
的 语言 所以 它 与 语言学 的 研究 有着 密切 
的 联系 但又 有 重要 的 区别 自然语言 处理 并 
不是 一般 地 研究 自然语言 而在于 研制 能 有效 地 
实现 自然 语言 通信 的 计算机 系统 特别 是 其中 
的 软件 系统 因而 它 是 计算机 科学 的 一部分 
近 段 时间 笔者 由于 做 了 一些 信息流 内容 
平台 相关 的 工作 对 NLP 的 一些 应用 有了/nr 
一定 了解 所以 和 大家 分享 一下 1 . 词 
法分析 基于 大 数据 和 用户 行为 对 自然 语言 
进行 中文分词 词性 标注 命名 识体 识别 定位 基本 语言 
元素 消除歧义 支撑 自然 语言 的 准确 理解 中文分词 将 
连续 的 自然 语言 文本 切分/ad 成/n 具有/v 语义/n 合理性/n 
和/c 完整性/n 的/uj 词汇/n 序列/n 词性/n 标注/v 将 自然 语言 
中 的 每个 词 赋予 一个 词性 如 动词 名词 
副词 命名 实体 识别 即 专有名词 识别 识别 自然语言 文本 
中 具有 特殊 意义 的 实体 如 人名 机构 名 
地名 举例 2 . 依存 句法分析 利用 句子 中词 与 
词 之间 的 依存 关系 来 表示 词语 的 句法结构 
信息 并用 树状 结构 来 表示 整句 的 结构 依存 
句法分析 主要有 几大 作用 精准 理解 用户 意图 当 用户 
搜索 时 输入 一个 query 通过 依存 句法分析 抽取 语义 
主干 及 相关 语义 成分 实现 对 用户 意图 的 
精准 理解 知识 挖掘 对 大量 的 非 结构化 文本 
进行 依存 句法分析 从中 抽取 实体 概念 语义 关系 等 
信息 构建 领域 知识 语言 结构 匹配 基于 句法结构 信息 
进行 语言 的 匹配 计算 提升 语言 匹配 计算 的 
准确率 举例 3 . 词 向量 表示 词 向量 计算 
是 通过 训练 的 方法 将 语言 词表 中的 词 
映射 成 一个 长度 固定 的 向量 词 表中 的 
所有 词 向量 构成 了 一个 向量空间 每 一个 词 
都是 这个 向量空间 中 的 一个 点 利用 这种 方法 
实现 文本 的 可计算 主要 应用在 快速 召回 结果 不同 
于 传统 的 倒排索引 结构 构建 基于 词 向量 的 
快速 索引 技术 直接 从 语义 相关性 的 角度 召回 
结果 个性化 推荐 基于 用户 的 过去 行为 通过 词 
向 量计算 学习 用户 的 兴趣 实现 个性化 推荐 举例 
4 . DNN 语言 模型 语言 模型 是 通过 计算 
给定 词 组成 的 句子 的 概率 从而 判断 所 
组成 的 句子 是否 符合 客观 语言表达 习惯 通常用于 机器翻译 
拼写 纠错 语音识别 问答 系统 词性 标注 句法分析 和 信息检索 
等 举例 5 . 词义 相似 度 用于 计算 两个 
给定 词语 的 语义 相似 度 基于 自然 语言 中 
的 分布 假设 即 越是 经常 共同 出现 的 词 
之间 的 相似 度 越高 词义 相似 度 是 自然 
语言 处理 中 的 重要 基础 技术 是 专名 挖掘 
query 改写 词性 标注 等 常用 技术 的 基础 之一 
主要 应用 专名 挖掘 通过 词语 间 语义 相关性 计算 
寻找 人名 地名 机构 名 等 词 的 相关 词 
扩大 专有 名词 的 词典 更好 的 辅助 应用 query 
改写 通过 寻找 搜索 query 中 词语 的 相似 词 
进行 合理 的 替换 从而 达到 改写 query 的 目的 
提高 搜索 结果 的 多样性 举例 6 . 短 文本 
相似 度 短 文本 相似 度 计算 服务 能够 提供 
不同 短 文本 之间 相似 度 的 计算 输出 的 
相似 度 是 一个 介于 1 到 1 之间 的 
实数值 越大 则 相似 度 越高 这个 相似 度 值 
可以 直接 用于 结果 排序 也 可以 作为 一维 基础 
特征 作用于 更 复杂 的 系统 举例 7 . 评论 
观点 抽取 自动 分析 评论 关注点 和 评论 观点 并 
输出 评论 观点 标签 及 评论 观点 极性 包括 美食 
酒店 汽车 景点 等 可 帮助 商家 进行 产品 分析 
辅助 用户 进行 消费 决策 举例 8 . 情感 倾向 
分析 针对 带有 主观 描述 的 中文 文本 可 自动 
判断 该 文本 的 情感 极性 类别 并给 出 相应 
的 置信度 情感 极性 分为 积极 消极 中性 情感 倾向 
分析 能 帮助 企业 理解 用户 消费 习惯 分析 热点 
话题 和 危机 舆情 监控 为 企业 提供 有力 的 
决策 支持 举例 目录 文章 目录 目录 前言 句法分析 技术 
1 句法分析 技术 2 句法分析 技术 3 句法分析 技术 4 
前言 硕士 生涯 结束 开始 专心 做 一件 自己 觉得 
有用 的 工具 先 做 工程 后搞/nr 理论 自然语言 处理 
是 一个 非常 难 的 问题 同时 是 人工智能 皇冠 
上 的 明珠 接下来 会 记录 一 系列 自然语言 处理 
的 笔记 来自 于 哈工大 老师 关毅 句法分析 技术 1 
基于 规则 + 统计 结合 的 句法分析 判定 输入 的 
词 序列 是否 合法 短语 结 构树 有向 无 环 
图 句子 { 主 定语 中心 } { 谓语 状 
谓 { 动宾 动 宾语 定语 中心词 补语 } } 
状语 修饰 核心 动作 提高 语法分析 结果 计算机 的 语法分析 
里面 不 明确 词性 层级 两种 句法分析 的 区别 因子 
进入 短语 结合 规则 句法分析 和 短语 结合 分析 进入 
区别 语法 歧义 示例 汉语 句法分析 句法分析 细语 形式 语法 
体系 匹配 模式 方法 正则文法 短语 结构 文法 信息处理系统 机器翻译 
运用 留下 此路不通 的 牌子 科研 有 风险 不是 一帆风顺 
需要 有 挑战 精神 的 人 去做 扩充 转移 网络 
状态 转 移机 树 邻接 语法 句法分析 技术 2 基于 
合一 运算 的 语法 复杂 描述 集 的 语法 合一 
运算 实现 该 方法 依存 语法 上下文 颗粒度 太大 短语 
限定 在 词汇 上 K + 语法 依存 文法 形式 
语法 体系 模式 正则 匹配 短语 结构 语法分析 很多 方法 
扩充 转移 网络 回顾 Chomsky 文法 体系 G = N 
∑ P S G = N \ sum P S 
G = N ∑ P S 是 一个 文法 α 
− & gt β ∈ P \ alpha & gt 
\ beta \ in P α − β ∈ P0 
型文法 对 α − & gt β 不作 任何 限制 
\ alpha & gt \ beta 不作 任何 限制 α 
− β 不作 任何 限制 I 型文法 ∣ α ∣ 
≤ ∣ β ∣ | \ alpha | \ leq 
| \ beta | ∣ α ∣ ≤ ∣ β 
∣ II 型文法 上下文 无关 文法 α ∈ N 上下文 
无关 文法 \ alpha \ in N 上下文 无关 文法 
α ∈ NIII 型文法 正则文法 一个 字串 的 推导 是 
一系列 文化 规则 的 应用 起始 符 推导 到 最好 
强 范式 基于 词 的 语法 格里 巴克 形式语言 自动 
化机 一种 语言 LgL _ gLg 是由 某 上下文 无关 
文法 推导 出来 的 所有 终结 符号串 的 集合 其中 
的 每个 终结 符串 称为 合乎 文法 G 否则 称之为 
不合乎 文法 上下文 文法 扩充 概率 无关 文法 一个 随机 
上下文 无关 语法 PCFG 的 三个 假设 1 位置 无关 
2 上下文 无关 3 祖先 无关 推出 非 总结 串 
隐 码 模型 推 出问题 PCFG 的 三个 基本问题 一个 
语句 W = WiWi − 1Wi − 2WnW = W 
_ iW _ { i 1 } W _ { 
i 2 } W _ nW = Wi Wi − 
1 Wi − 2 Wn 的 P W | G 
也 就是 产生 语句 W 的 概率 在 语句 W 
的 句法结构 有 歧义 的 情况 下 如何 快速 选择 
最佳 的 语法分析 parse 如何 从 语料库 中 训练 G 
的 概率 参数 使得 P W | G 最大 类比 
之前 的 问题 评价 解码 编码 问题 节点 间 的 
递推关系 叶节/nr 点到 根 节点 的 句法树 向内 算法 句法分析 
技术 3 随机 上下文 无关 文法 任何 一个 语句 都 
可以 视为 一种 语言 模型 一个 句法树 中的 结点 词 
句法树 开始 推导 自顶向下 自下 向上 某 一部 推导 对应 
于 几个 规则 开始 推导 做出 结果 登上 算法 尝试 
去做 EM 算法 优化 前进 无 指导 学习 算法 PCFG 
的 优点 可以 对 句法分析 的 歧义 结果 进行 概率 
排序 提高 文法 的 容错 能力 词 对 结构 分析 
上下文 对 结构 分析 随机 上下文 无关 文法 向前 算法 
节点 值 增加 提前 α ij A = P Wi 
Wj ∣ A i & lt j \ alpha _ 
{ ij } A = P W _ i W 
_ j | A i & lt j α ij 
A = P Wi Wj ∣ A i j = 
∑ B C ∈ RP Wi Wj B Wr + 
1 . . Wj C ∣ A = \ sum 
_ { B C \ in R } P W 
_ i W _ j B W _ { r 
+ 1 } . . W _ j C | 
A = B C ∈ R ∑ P Wi Wj 
B Wr + 1 . . Wj C ∣ A 
α i j = P A − & gt Wi 
i = j \ alpha _ { i j } 
= P A & gt W _ i i = 
j α i j = P A − Wi i 
= j 句法分析 技术 4 浅层 句法分析 形式 合规 分析 
结构 分析 就行 部分 分析 组块 分析 例句 这 一切 
已经 引起 世界 各国 的 普遍 关注 S k r 
c p . 浅层 专项 研究 基于 HMM 的 浅层 
分析 技术 ACL 会议 他 识别 的 目标 是 非 
递归 的 NLP 浅层 句法分析 隐 码 是 五 元组 
浅层 分析 状态 空间 如何 定义 输出 一对 词性 标记 
一个 组块 开始 照 着看 任何 阶段 都 可以 用 
任何 一个 模型 不同 的 是 标记 的 内容 级联 
式 有限 状态 分析 句法 # 句法分析 技术 5 基于 
规则 的 方法 需要 大量 人力 不好 迁移 总结 概率 
上下文 无关 文法 句法分析 是 目前 语言 处理 技术 瓶颈 
之一 发现 问题 比 解决 问题 更 重要 句法分析 是 
必由之路 ACL 每年 关注 语法分析 强化 学习 技术 免疫 机制 
分析 合适 吗 句法 是 形式 语义 是 内容 完整 
合法性 没有 公认 的 内容 句法 的 强制性 和 语义 
的 决定性 句法 系统 和 语义 系统 是 两个 不同 
的 系统 它们 各自 独立 而 又 相互 依存 彼此 
的 对应 关系 十分 复杂 统计 规则 之后 讲 应用 
奇虎 360 面试 主要 考察 的 知识点 1 . 机器学习 
常用 的 分类 算法 Logistic 回归 SVM Decision Tree 随机 
森林 等 相关 分类 算法 的 原理 公式 推导 模型 
评价 模型 调 参 模型 使用 场景 2 . 机器学习 
常用 的 聚 类 算法 Kmeans BDSCAN SOM 个人 论文 
中 使用 的 算法 LDA 等 算法 的 原理 算法 
模型 中 参数 的 确定 具体 到 确定 的 方法 
模型 的 评价 例如 LDA 应该 确定 几个 主题 Kmeans 
的 k 如何 确定 DBSCAN 密度 可达 与 密度 直达 
模型 使用 场景 3 . 特征 工程 特征选择 特征提取 PCA 
降 维 方法 中 参数 主 成分 的 确定 方法 
如何 进行 特征选择 4 . Boosting 和 bagging 的 区别 
5 . 数据 如何 去除 噪声 如何 找到 离群 点 
异常值 现有 机器学习 算法 哪些 可以 去除 噪声 6 . 
HMM 与 N gram 模型 之间 的 区别 7 . 
梯度 消失 与 梯度 爆炸 8 . 奥卡姆 剃须刀 原理 
9 . TCP 三次 握手 的 原理 为什么 是 三次 
而 不是 其他 次 10 . 进行 数据 处理 时 
如何 过滤 无用 的 信息 例如 利用 正则表达式 提取 或者 
其他 方法 数据 乱码 的 处理 11 . 交叉 熵 
与 信息熵 信息 增益 与 信息 增益 率 gini 系数 
具体 如何 计算 12 . BIC 准则 贝叶斯 信息 准则 
与 AIC 赤 池 信息 准则 13 . 需要 手写 
代码 此次 面试 字符串 的 操作 14 . 前 向 
传播 与 反向 传播 15 . 常见 的 损失 函数 
最后 面试官 给 的 建议 1 . 多看 看 文献 
看 别人 的 成果 2 . 视野 要 打开 需要 
进一步 扩宽 知识面 在校 期间 更多 关注 理论 工作 时 
没有 太多 的 时间 关注 理论 而 更多 是 偏向 
业务 理论 数学 统计学 同时 需要 多 读 多 练 
向 面试官 提问 时 尤其 是 技术 类 面试 尽量 
不要 用 短 时间 内 这个词 毕竟 技术 是 一个 
积累 的 过程 原创 申明 此 博文 为 博主 原创 
未经 允许 不得 转载 NLP 概述 NLP 是 利用 计算机 
为 工具 对/p 人类/n 特有/b 的/uj 书面/n 形式/n 和/c 口头/n 
形式/n 的/uj 自然/d 语言/n 的/uj 信息/n 进行/v 各种/r 类型/n 处理/v 
和/c 加工/vn 的/uj 技术/n ./i NLP/w 内容/n 结构/n NLP/w 基础/n 
技术/n 词/n 法分析/n 词/n 法分析/n 目的/n 是从/v 句子/n 中/f 分出/v 
单词/n 找出 词汇 的 各个 词素 从中 获得 单词 的 
语言学 信息 并 确定 单词 的 词性 . 词法 分析 
是 很多 中文信息处理 任务 的 必要 步骤 . 自动 分词 
命名 实体 识别 词性 标注 句法分析 句法分析 是 对 句子 
和 短语 结构 进行 分析 如 句子 的 形式 结构 
主语 谓语 宾语 等 . 句法分析 是 语言 学 理论 
和 实际 的 自然 语言 应用 的 一个 重要 桥梁 
. 一个 实用 的 完备 的 准确 的 句法分析 将 
是 计算机 真正 理解 自然 语言 的 基础 . 短语 
结构 分析 宾州 树 库 依存 分析 语义分析 解释 自然语言 
句子 或 篇章 各部分 词 词组 句子 段落 篇章 的 
意义 . 目前 语义 计算 的 理论 方法 模型 尚 
不成熟 . 词义 消 歧 词 语义 归纳 推理 词 
语义 角色 标注 句子 篇章 分析 指 超越 单个 句子 
范围 的 各种 可能 分析 包括 句子 语 段 之间 
的 关系 以及 关系 类型 的 划分 段落 之间 的 
关系 的 判断 跨越 单个 句子 的 词 与 词 
之间 的 关系 分析 话题 的 继承 与 变迁 等 
. NLP 核心 应用 机器翻译 Machine translation MT 信息检索 Information 
Retrieval 信息 抽取 Information Extraction 自动 文摘 Automatic summarization / 
abstracting 问答 系统 Question Answering system 阅读 理解 Machine Reading 
文档 分类 Document categorization 情感 分类 Sentimental classification 信息 推荐 
与 过滤 Formation Recommendation and Filtering NLP 技术 及 应用 
架构 NLP 领域 的 学术 会议 ACL Association of Computational 
Linguistics Coling International Conference on Computational Linguistics EMNLP Conference on 
Empirical Methods in Natural language Processing EACL European Chapter of 
ACL IJCNLP International Joint Conference on Natural language Processing SIGIR 
SIG Information Retrieval TREC Text R E t r i 
e v a l C o n f e r 
e n c e JSCL 全国 计算 语言学 联合 学术会议 
国内 NLP 研究组 Tencent AI Lab 苏州大学 NLP 实验室 微软 
亚洲 研究院 自然语言 计算 组 N a t u r 
a l L a n g u a g e 
C o m p u t i n g NLC 
Group 头条 人工智能 实验室 清华大学 自然语言 处理 与 社会 人文 
计算 实验室 清华大学 智能 技术 与 系统 国家 重点 实验室 
信息检索 组 北京大学 计算 语言学 教育部 重点 实验室 北京大学 计算机 
科学 技术 研究所 语言 计算 与 互联网 挖掘 研究室 哈工大 
社会 计算 与 信息检索 研究中心 哈工大 机器 智能 与 翻译 
研究室 哈尔滨工业大学 智能 技术 与 自然 语言 处理 实验室 中科院计算所 
自然语言 处理 研究组 中科院 自动化 研究所 语音 语言 技术 研究组 
南京 大学 自然 语言 处理 研究组 复旦 大学 自然 语言 
处理 研究组 东北 大学 自然 语言 处理 实验室 厦门大学 智能科学 
与 技术系 自然语言 处理 实验室 参考资料 中国科学院 大学 NLP 课程 
课件 IIE 胡玥/nr 老师 主讲 中文 语言 的 机器 处 
理直 观上 一个 自然 语言 处理 系统 最少 三个 模块 
语言 的 解析 语义 的 理解 及 语言 的 生成 
计算机 处理 自然语言 最早 应用 在 机器 翻译 上 此后 
在 信息检索 信息 抽取 数据挖掘 舆情 分析 文本 摘要 自动 
问答 系统 等 方面 都 获得 了 很 广泛 的 
应用 虽然 已经 产生 了 许多 专业 技术 作用域 语言 
理解 的 不同 层面 和 不同 任务 例如 这些 技术 
包括 完全 句法分析 浅层 句法分析 信息 抽取 词义 消 歧 
潜在 语义分析 文本 蕴含 和 指代 消解 但是 还 不能 
完美 或 完全 地 译 解出 语言 的 本义 命名 
实体 识别 主要/b 用来/v 识别/v 语/ng 料中/i 专有/b 名词/n 和未/nr 
登录/v 词/n 的/uj 成词/nr 情况/n 如 人名 地名 组织 机构 
名称 等 也 包括 一些 特别 的 专名 准确 的 
命名 实体 识别 一 准确 的 分词 和 词性 标注 
为 前提 语义 组块 用来 确定 一个 以上 的 词汇 
构成 的 短语 结构 即 短语 级别 的 标注 主要 
识 别名 词性 短语 动 词性 短语 介 词性 短语 
等 以及 其他 类型 的 短语 结构 语义 组块 的 
自动 识别 来源于 中文分词 词性 标注 和 命名 实体 识别 
的 共同 信息 语义 组块 的 识别 特征 必须 包含 
中文分词 和 词性 标注 两部分 语义 角色 标注 以 句子 
中的 谓语 动词 为 中心 预测 出 句子 中 各个 
语法 成分 的 语义 特征 是 句子 解析 的 最后 
一个 环节 也是 句子 级别 研究 的 重要 里程碑 语义 
角色 标注 直接 受到 句法 解析 和 语义 组块 的 
影响 词性 标注 Part of Speech Tagging 或 POS Tagging 
又 称为 词类 标注 是 指 判断 出 在 一个 
句子 中 每个 词 所 扮演 的 语法 角色 例如 
表 示人 事物 地点 等 的 名称 为 名词 表示 
动作 或 状态 变化 的 词 为 动词 等 一个 
词 可能 具有 多个 词性 一般而言 中文 的 词性 标注 
算法 比较 同意 大多数 使用 隐 马尔科夫 模型 HMM 或 
最大熵 算法 如 结巴 分词 的 词性 标注 为了 获得 
更高 的 精度 也有 使用 条件 随 机场 CRF 算法 
的 如 LTP3 . 3 中的 词性 标注 中文 词性 
标 签有 两大类 北大 词性 标注 集 和 宾州 词性 
标注 集 句法分析 是 根据 给定 的 语法 体系 自动 
推导 出 句子 的 语法结构 分析 句子 所 包含 的 
语法 单元 和 这些 语法 单元 之间 的 关系 将 
句子 转化 为 一棵 结构化 的 语法树 目前 句法分析 有 
两种 不同 的 理论 一种 是 短语 结构 语法 另一种 
是 依存 语法 哈工大 NLP 平台 哈工大 语言 技术 平台 
Language Technology Platform LTP 是 哈工大 社会 计算 与 信息检索 
研究 中心 研发 的 一整套 中文 语言 处理 系统 语言 
技术 平台 包括 中文分词 词性 标注 命名 实体 识别 依存 
句法分析 语义 角色 标准 等 丰富 高效 精准 的 自然 
语言 处理 技术 还 可以 通过 可视化 的 图形 输出 
使 用户 一目了然 P 10 Stanford NLP 团队 斯坦福 自然语言 
处理 团队 http / / nlp . stanford . edu 
/ 是 一个 由 斯坦福 大学 的 教师 科研人员 博士后 
程序员 组成 的 团队 该 团队 致力于 研究 计算机 理解 
人类 语言 的 工作 涵盖 诸如 句子 的 理解 机器翻译 
概率 解析 和 标注 生物 医学 信息 抽取 语法 归纳 
词义 消 歧 自动 问答 及 文本 区域 到 3D 
场景 的 生成 等 在 某些 中文 NLP 应用 中 
局域 卓越 的 性能 一些 主要 的 中文 NLP 应用 
如下 1 斯坦福 句法 解析器 概率 自然语言 句法 解析器 包括 
PCFG 与 概率 的 上下文 无关 的 短语 和 依存 
句法 解析器 一个 词汇 的 PCFG 解析器 以及 一个 超 
快速 的 神经 网络 的 依存 句法 解析器 和 深度 
学习 重 排序 器 在线 句法 分析器 演示 http / 
/ nlp . stanford . edu 8080 / parser / 
index . jsp 2 斯坦福 命名 实体 识别器 该 识别器 
基于 条件 随 机场 序列 模型 用于 英文 中文 德文 
西班牙文 的 连同 命名 实体 识别 以及 一个 在线 NER 
演示 3 斯坦福 词性 标注 器 基于 最大熵 CMM 算法 
词性 标注 POS 系统 包括 英语 阿拉伯语 汉语 法语 德语 
和 西班牙语 4 斯坦福 分析器 基于 CRF 算法 的 分词器 
支持 阿拉伯语 和 汉语 Python + NLTK 自然语言 处理 学习 
一 环境 搭建 Python + NLTK 自然语言 处理 学习 二 
常用 方法 similar common _ contexts generate Python + NLTK 
自然语言 处理 学习 三 计算机 自动 学习 机制 统计 语言 
模型 自然语言 从它/nr 产生 开始 逐渐/d 演变/v 成/n 一种/m 上下文/l 
相关/v 的/uj 信息/n 表达/v 和/c 传递/v 方式/n 因此 让 计算机 
处理 自然语言 一个 基本 问题 就是 为 自然语言 这种 上下文 
相关 的 特性 建立 数学模型 这个 数学模型 就是 在 自然 
语言 处理 中 常说 的 统计 语言 模型 Statistical Language 
Model 它 是 今天 所有 自然语言 处理 的 基础 并且 
广泛 应用于 机器翻译 语音识别 印刷体 或 手写体 识别 拼写 纠错 
汉字输入 和 文献 查询 目录 1 . 用 数学 的 
方法 描述语言 规律 2 . 高阶语言 模型 3 . 模型 
的 训练 零 概率 问题 和 平滑 方法 1 . 
用 数学 的 方法 描述语言 规律 统计 语言 模型 产生 
的 初衷 是 为了 解决 语音识别 问题 在 语音 识别 
中 计算机/n 需要/v 知道/v 一个/m 文字/n 序列/n 是否/v 能/v 构成/v 
一个/m 大家/n 理解/v 并且/c 有/v 意义/n 的/uj 句子/n 然后 显示 
或 打印 给 使用者 比如 这句话 就很 通顺 意义 也 
很 明白 如果 改变 一些 词 的 顺序 或者 替换 
掉 一些 词 将 这 句话 变成 意思 就 含 
混了 虽然 多少 还能 猜到 一点 但 如果 再 换成 
基本上 读者 就 不知所云 了 第一 个 句子 合乎 语法 
词义 清晰 第二 个 句子 虽 不合乎 语法 但是 词义 
还算 清晰 而 第三 个 句子 则 连 词义 都不 
清晰 了 上世纪 70 年代 以前 科学家 们 也 是 
这样 想 的 他们 试图 判断 这个 文字 序列 是否 
合乎 文法 含义 是否 正确 等 但是 语言 的 结构 
千变万化 要 通过 制定 规则 来 覆盖 所有 的 文法 
根本 是 不 可能 的 而 弗里德里克 贾里 尼克 Frederick 
Jelinek 换 了 一个 角度 用 一个 简单 的 统计 
模型 就 很 漂亮 地 搞定 了 这个 问题 贾里 
尼克 想法 贾里 尼克 的 出发点 很简单 一个 句子 是否 
合理 就看 它 的 可能性 大小 如何 上面 的 例子 
中 第一 个 句子 出现 的 概率 大致 是 第二 
个 句子 出现 的 概率 是   第三 个 句子 
出现 的 概率 是   因此 第一 个 句子 出现 
的 可能性 最大 是 第二 个 句子 的 10 万倍 
是 第三 个 句子 的 一百亿亿亿亿亿亿倍 用 更 普遍 而 
严格 的 描述 是 假定   是 一个 有 意义 
的 句子 由 一连串 特定 顺序 排列 的 词   
? ? 1 ? ? 2 ⋯ ? ? ? 
? 组成 n 为 句子 的 长度 那么 S 在 
文本 中 出现 的 可能性 就是 S 的 概率 P 
S 于是/nr 可以 把 P S 展开 表示 为 利用 
条件概率 公式   这个 序列 出现 的 概率 等于 每 
一个 词 出现 的 条件 概率 相乘 于是 其中 表示 
句子 第 一个词 为 的 概率 是 在 已知 第 
一个词 的 前提 下 第二个 词 出现 的 概率 以此类推 
不难看出 词 的 出现 概率 取决于 他 前面 的 所有 
词 从 计算 上 来看 第一 个 词 的 条件概率 
  ? ? ? ? 1   很容易 算 第二个 
词 的 条件概率   ? ? ? ? 2 ∣ 
? ? 1 也 还不 太麻烦 但是 从 第三 个 
词 的 条件概率   ? ? ? ? 3 ∣ 
? ? 1 ? ? 2 开始 就 非常 难 
算了 因为 它 涉及 到 三个 变量   ? ? 
1 ? ? 2 ? ? 3 而 每个 变量 
的 可能性 / 可能 取值 都是 语言 字典 的 大小 
到了 最后 一个 词   ? ? ? ? 条件概率 
  ? ? ? ? ? ? ∣ ? ? 
1 ? ? 2 ⋯ ? ? ? ? − 
1   的 可能性 太多 根本无法 估算 二元 模型 与 
N 元 模型 从 19 世纪 到 20 世纪初 俄国 
有个 数学家 叫 马尔可夫 Andrey Markov 他 提出 了 一种 
偷懒 但还 颇为 有效 的 方法 假设 任意 一个 词语 
出现 的 概率 只 同 它 前面 的 词 有关 
于是 问题 就 变得 很 简单 了 这种 假设 在 
数学 上 称为 马尔可夫 假设 现在 句子 出现 的 概率 
就 变得 简单 了 上面 的 公式 对应 的 统计 
语言 模型 是 二元 模型 Bigram Model 当然 也 可以 
假设 一个词 由 前面 的 N − 1 个 词 
决定 对应 的 模型 稍微 复杂 些 被 称为 N 
元 模型 接下来 的 问题 就是 如何 估计 条件概率   
根据 它 的 定义 而 估计 联合 概率 和 边缘 
概率 很简单 根据 大数定理 只要 统计量 足够 相对 频率 就 
等于 概率 因而 只需 在 语料库 corpus 的 文本 中 
统计 一下 这 两个 词 前后 相邻 出现 的 次数 
以及 出现 了 多少 次 然后 把 这 两个 数 
分别 处以 语料库 大小 N 即可 得到 这些 词 或 
2 元组 的 概率 于是 更 一般 的 对于 n 
gram 这 似乎 有点 难以置信 用 这么 简单 的 数学 
模型 就 能 解决 复杂 的 语音 识别 机器 翻译 
等 问题 而用 很 复杂 的 文法 规则 和 人工智能 
却 做不到 其实 很多 语言学家 都曾 质疑 过 这种 方法 
的 有效性 但 事实 证明 统计 语言 模型 比 任何 
已知 的 借助 某种 规则 的 解决 方法 更 有效 
2 . 高阶语言 模型 在 基于 一 阶 马尔可夫 假设 
的 二元 模型 bi gram / 2 gram 中 句子 
中 每个 词 只和 前面 一个 词 有关 这 似乎 
过于 简化 了 或者说 近 似地 过头 了 比如说 在 
句子 美丽 的 花朵 中 花朵 其实 是 和 美丽 
有关 也 就是说 是 与 前面 的 第二个 词 有关 
因此 更 普遍 的 假设 是 某个 词 和 前面 
的 若干 个 词 有关 正如 之前 介绍 的 那样 
N 元 模型 n gram 假设 每个 词   和 
前面 的 N − 1 个 词 有关 与 更 
前面 的 词 无关 不是 与 前面 所 有的 词 
相关 这样 词     的 概率 只 取决于 前面 
的 N − 1 个 词 因此 这种 假设 被 
称为 N − 1 阶 马尔可夫 假设 对应 的 语言 
模型 称为 N 元 模型 N Gram Model N = 
2时 就是 之前 介绍 的 二元 模型 而 N = 
1 的 一元 模型 实际上 是 一个 上下文 无关 模型 
即 假定 当前 词 的 出现 概率 与 前面 的 
词 无关 在 实际 中 应用 最多 的 就是 N 
= 3 的 三元 模型 trigram / 3 gram 更 
高阶 的 模型 就 很少 使用 了 为什么 N 取值 
那么 小 我们 之前 在上 一篇 博客 中 曾经 探讨 
过 这个 问题 1 . 首先 N 元 模型 的 
大小 空间 复杂度 几乎 是 N 的 指数函数 即   
这里 | V | 是 一种 语言 词典 的 词汇 
量 一般 在 几万 到 几十万 个 其次 使用 N 
元 模型 的 速度 时间 复杂度 也 几乎 是 一个 
指数函数 即   因此 N 不能 很大 2 . 当 
N 从 1 到 2 再从 2 到 3 时 
模型 的 效果 上升 显著 而 当 模型 从 3 
到 4 时 效果 的 提升 就 不是 很 显著 
了 而 资源 的 耗费 却 增加 地 非常 快 
所以 除非 是 为了 做到 极致 不惜 资源 很少 有人 
会 使用 四 元 以上 的 模型 还有 一个 问题 
三元 四元 或 更高 阶 的 模型 也 并不 能 
覆盖 所有 的 语言 现象 在 自然 语言 处理 中 
上下文 之间 的 相关 性 可能 跨度 非常大 甚至 可以 
从 一个 段落 跨到 另 一个 段落 因此 即便 再 
怎么 提高 模型 的 阶数 对 这种 情况 也 无可奈何 
这 就是 马尔可夫 模型 的 局限性 这时 就 需要 采用 
其他 一些 长程 的 依赖性 Long Distance Dependency 来 解决 
这个 问题 了 如之 后 学习 的 神经 语言 模型 
LSTM / GRU 等 可以 很好 的 解决 这个 问题 
3 . 模型 的 训练 零 概率 问题 和 平滑 
方法 语言 模型 中 所有 的 条件 概率 称为 模型 
的 参数 通过 对 语料 的 统计 得到 这些 参数 
的 过程 计算 这些 条件概率 称为 模型 的 训练 前面 
提到 的 二元 模型 训练方法 似乎 非常 简单 只需 计算 
一下 前后 相邻 出现 的 次数   和 /nr 单独 出现 
的 次数   的 比值 即可 而 的 取值 可能 
是 词典 中 的 任意 一个 单词 即 考虑 所有 
可能 的 组合 基于 语料库 计算 频数 频率 及 条件概率 
对于 N 元 模型 也是 同理 这样 做 的话 的 
很多 组合 可能 没有 意义 在 语料库 中 没有 出现 
过 即 = 0 那么 是否 意味着 条件概率 = 0 
反之 如果 都在 语料库 中 只 出现 一次 那么 能否 
得到 = 1 这样 非常 绝对 的 结论 注意 词典 
和 语料库 不是 一个 概念 词典 基于 语料库 构建 对 
语料库 分词 去 重 调整 顺序 来 构建 词典 n 
gram 模型 可以 理解 为 考虑 词典 中 的 所有 
可能 组合 然后 基于 语料库 进行 统计 计算 条件概率 存 
储起来 应用 时 直接 查询 计算 即可 这样 考虑 所有 
可能 组合 很多 组合 会 没有 意义 在 语料库 中 
也 不会 出现 就会 存在 0 概率 / 数据 稀疏 
的 问题 此时 需要 使用 平滑 方法 对 没有 见过 
的 gram 赋于 一个 非 0 的 概率值 还会 面临 
统计 可靠性 或 统计量 不足 的 问题 在 数理统计 中 
我们 之所以 敢用 对 采样 数据 进行 观察 的 结果 
来 预测 概率 是 因为 有 大数定理 Law of Large 
Number 在 背后 做 支持 它 的 要求 是 有 
足够 的 观察 值 但是 在 估计 语言 模型 的 
概率 时 很多 人 恰恰 忘了 这个 道理 因此 训练 
出来 的 语言 模型 不管用 然后 回过头来 怀疑 这个 方法 
是否 有效 那么 如何 正确 地 训练 一个 语言 模型 
呢 一个 直接 的 办法 就是 增加 数据量 但是 即使 
如此 仍会 遇到 零 概率 或者 统计量 不足 的 问题 
假 定要 训练 一个 汉语 的 语言 模型 汉语 的 
词汇 量 大致 是 20 万 这个 数量级 训练 一个 
三元 模型 就有   个 不同 参数 假设 抓取 100 
亿个 有 意义 的 中文 网页 每个 网页 平均 1000 
词 全部 用作 训练 也 依然 只有   因此 如果 
用 直接 的 比值 计算 概率 大部分 条件概率 依然 是 
零 这种 模型 我们 称之为 不 平滑 训练 统计 语言 
模型 的 艺术 就 在于 解决 好 统计 样本 不足 
时的/nr 概率 估计 问题 关于 平滑 技术 的 详细 介绍 
可以 阅读 这篇 博客 自然语言 处理 中 N Gram 模型 
的 Smoothing 算法 当然 如果 对 这些 平滑 算法 不是 
很懂 也 不必 太 担心 平滑 技术 在 统计 自然语言 
处理 时代 用得 比较 多 现代 的 神经 网络 对 
语言 模型 建模 的 方式 由于 本身 结构 的 原因 
自动 解决 了 这个 问题 我们 之后 还 会 学习 
自然语言 处理 简介 自然语言 处理 Natural Language Processing 简称 NLP 
就是 用 计算机 来 处理 理解 以及 运用 人类 语言 
如 中文 英文 等 它 属于 人工智能 的 一个 分支 
是 计算机 科学 与 语言学 的 交叉 学科 又 常 
被称为 计算 语言学 自然语言 处理 任务 在 得到 字 句子 
表示 之后 自然语言 处理 任务 类型 划分为 类别 对象 到 
序列 例如 文本 生成 图像 描述 生成 序 列到 类别 
文本 分类 情感 分析 同步 的 序列 到 序列 中文分词 
词性 标注 语义 角色 标注 异步 的 序列 到 序列 
机器翻译 自动 摘要 参考资料 Awesome Chinese NLP 兜 哥 出品 
一本 开源 的 NLP 入门 书籍 NLP 研究 入门 之道 
YSDA course in Natural Language Processingnlp tutorial Natural Language Processing 
Tutorial for Deep Learning Researchers 利用 空余 时间 将 用 
Python 进行 自然语言 处理 的 前面 几章 内容 都 敲了 
一遍 其中 遇到 与 书中 示例 不太 一致 的 地方 
也 进行 了 修改 第一章 示例 如下 # / usr 
/ bin / env python # * coding utf 8 
* # @ Author Peidong # @ Site # @ 
File eg1 . py # @ Software PyCharm the first 
example for nltk book from nltk . book import * 
# 查找 特定 词语 上下文 text1 . concordance monstrous # 
相关 词 查找 text1 . similar monstrous # 查找 多个 
词语 的 共同 上下文 text2 . common _ contexts monstrous 
very # 画出 词语 的 离散 图 text4 . dispersion 
_ plot citizens democracy freedom duties America # 产生 随机 
文本 text3 . generate Traceback most recent call last File 
E / nlp / eg1 . py line 25 in 
module text3 . generate TypeError generate missing 1 required positional 
argument words # 单词 数量 标识符 总数 print len text3 
# 词汇 的 种类 及 数量 用 集合 set 显示 
print sorted set text3 print len set text3 # 测量 
平均 每类 词语 被 使用 的 次数 from _ _ 
future _ _ import division # 本 命令 必须 放 
在 文件 的 开始 之初 print len text3 / len 
set text3 # 统计 特定 单词 在 文本 中 出现 
的 次数 并 计算 其 占 比 print text3 . 
count smote print 100 * text4 . count a / 
len text4 # # 词 的 频率分布 fdist1 = FreqDist 
text1 # # 输出 总的 词数 print fdist1 # In 
Python 3 dict . keys returns an iteratable but not 
indexable object . vac1 = list fdist1 . keys # 
# 输出 词数 最多 的 前 五十 个 词 print 
vac1 50 # # 输出 whale 的 次数 print fdist1 
whale # # 输出 前 五十 个 词 的 累积 
频率 图 fdist1 . plot 50 # 查找 长度 超过 
15 个字符 的 词 V = set text1 long _ 
words = w for w in V if len w 
15 print sorted long _ words # 查找 长度 超过 
7 的 词 且 频率 超过 7 fdist5 = FreqDist 
text5 print sorted w for w in set text5 if 
len w 7 and fdist5 w 7 # 双 连词 
的 使用 from nltk import bigrams # # 查了/nr 一下 
nltk 官 网上 的 函数 说明 要 加 list 函数 
结果 才是 书上 的 情况 print list bigrams more is 
said than done # 文本 中 常用 的 连接词 print 
text4 . collocations print len w for w in text1 
fdist = FreqDist len w for w in text1 print 
fdist print fdist . keys print fdist . items print 
fdist . max print fdist 3 print fdist . freq 
3 print sorted w for w in set text1 if 
w . endswith ableness print babelize _ shell 主题 提取 
参考 知乎 的 回答 关键词 提取/v 都有/nr 哪些/r 方案/n word2vec 
词 向量 训练 及 中文 文本 相似 度 计算 简单 
的 LDA 实现 NLP 主题 抽取 Topic LDA 代码 实践 
gensim 包 代码 命名 实体 识别 参考 http / / 
spaces . ac . cn / archives / 3942 / 
IIS 推导 http / / blog . csdn . net 
/ xueyingxue001 / article / details / 50773917Bi LSTM + 
CRF 进行 NER https / / www . cnblogs . 
com / Determined22 / p / 7238342 . html 使用 
CRF + + 进行 命名 实体 识别 https / / 
zhuanlan . zhihu . com / p / 27597790 如何 
学习 自然语言 处理 52nlp 1 . 几篇 文章 介绍 相关 
技术 nlper http / / nlpers . blogspot . my 
/ 2 . 标准 书籍 统计 自然语言 处理 基础 自然语言 
处理 综论 涉猎 广 有 门槛 自然语言 理解 nltk 工具包 
用 python 进行 自然语言 处理 e － book http / 
/ code . google . com / p / brishen 
/ downloads / list3 . ACL anthology 或者 到 作者 
主页 去 进一步 follow 或者 去 其 导师 主页 看看 
是否 有 进一步 相关 文献 4 . 多 参加 会议 
大体 流程 就是 博客 入门 － 书籍 系统 了解 － 
论文 跟进 － 会议 感受 目录 1.1 自然语言 处理 的 
挑战 1.2 神经 网络 和 深度 学习 1.3 自然语言 处理 
中 的 深度 学习 1.1 自然语言 处理 的 挑战 自然语言 
处理 是 一个 设计 输入 与 输出 为非 结构化 自然语言 
数据 的 方法 和 算法 的 研究 领域 人类 语言 
有 很强 的 歧义 性 如 句子 I ate pizza 
with friends 我 和 朋友 一起 吃 披萨 和 I 
ate pizza with olives 我 吃了 有 橄榄 的 披萨 
和 多样性 如 I ate pizza with friends 也 可以 
说 成 Friends and I shared some pizza 语言 也 
一直 在 进化 中 人 善于 产生 和 理解 语言 
并 具有 表达 感知 理解 复杂 且 微妙 信息 的 
能力 与此同时 虽然 人类 是 语言 的 伟大 使用者 但是 
我们 并不 善于 形式化 地 理解 和 描述 支配 语言 
的 规则 使用 计算机 理解 和 产生 语言 因此 极具 
挑战性 事实上 最 为人所知 的 处理 语言 数据 的 方法 
是 使用 有 监督 机器学习 算法 其/r 试图/v 从/p 事先/d 
标注/v 好/a 的/uj 输入/v //i 输出/v 集合/v 中/f 推导/v 出/v 
使用/v 的/uj 模式/n 和/c 规则/n 例如 一个 将 文本 分为 
四类 的 任务 类别 为 体育 政治 八卦 经济 显然 
文本 中 的 单词 提供 了 很强 的 线索 但是 
到底 哪些 单词 提供 了 什么 线索 呢 为 该 
任务 书写 规则 极具 挑战性 然而 读者 可以 轻松 地 
将 一篇 文档 分到 一个 主题 中 然后 基于 几百篇 
认为 分类 的 样例 可以 让 有 监督 机器学习 产生 
用词 的 模式 从而 帮助 文本 分类 机器学习 方法 擅长 
那些 很 难获得 规则 集 但是 相对 容易 获得 给定 
输入 及 相应 输出 样本 的 领域 除了 使用 不 
明确 规则 集 处理 歧义 和 多样 输入 的 挑战 
外 自然语言 展现 了 另外 一些 特性 其 使得 用 
包括 机器学习 在内 的 计算 方法 更 具 挑战性 即 
离散性 discrete 组 合性 compositional 和 稀疏 性 sparse 语言 
是 符号化 和 离散 的 书面 语义 的 基本 单位 
是 字符 字符 构成 了 单词 单词 再 表示 对象 
概念 事件 动作 和 思想 字符 和 单词 都是 离散 
符号 如 hamburger 或 pizza 会 唤起 我们 头 脑中 
的 某种 表示 但是 它们 也 是 不同 的 符号 
其 含义 是 不相关的 待 我们 的 大脑 去 理解 
从 符号 自身 看 hamburger 和 pizza 之间 没有 内在 
的 关系 从 构成 它们 的 字母 看 也 一样 
与 机器 视觉 中 普遍 使用 的 如 颜色 的 
概念 或 声学 信号 相对比 这些 概念 都是/nr 连续 的 
如 可以 使用 简单 的 数学 运算 从 一幅 彩色图像 
变为 灰度 图像 或者 从 色调 光强 等 内在 性质 
比较 两 幅 图像 对于 单词 这些 都不/nr 容易 做到 
如果 不 使用 一个 大 的 查找表 或者 词典 没有 
什么 简单 的 运算 可以 从 单词 red 变为 单词 
pink . 语言 还 具有 组 合性 即 字母 形成 
单词 单词 形成 短语 和 句子 短语 的 含义 可以 
比 包含 的 单词 更大 并 遵循 复杂 的 规则 
集 为了 理解 一个 文本 我们 需要 超越 字母 和 
单词 看到 更长 的 单词 序列 如 句子 甚至 整篇 
文本 以上 性质 的 组合 导致 了 数据 稀疏 性 
data sparseness 单词 离散 符号 组合 并 形成 意义 的 
方式 实际上 是 无限 的 可能 合法 的 句子 数 
是 巨大 的 我们/r 从/p 没指望/l 能/v 全部/n 枚/m 举出来/i 
随便 翻 一 本书 其中 绝大部分 句子 是 你 之前 
从没 看过 和 听过 的 甚至 很 有可能 很多 四个 
单词 构成 的 序列 对 你 都是 新鲜 的 如果 
你 看 一下 过去 10年 的 报纸 或者 想像 一下 
未来 10年 的 报纸 许多 单词 特别 是 人名 品牌/n 
和/c 公司/n 以及/c 俚语/n 和/c 术语/n 都将/i 是/v 新的/i 我们 
也 不 清楚 如何 从 一个 句子 生成 另一个 句子 
或者 定义 句子 之间 的 相似性 也不 依赖于 它们 的 
意思 对 我们 是 不可 观测 的 当/t 我们/r 要/v 
从/p 实例/n 中/f 学习/v 时/n 也是/i 挑战/vn 重重/a 即使 有 
非常 大 的 实例 集合 我们 仍然 很 容易 观测 
到 实例 集合 中 从 没有 出现 过 的 事件 
其/r 与/p 曾/d 出现/v 过/ug 的/uj 所有/b 实例/n 都/d 非常/d 
不同/a 1.2 神经 网络 和 深度 学习 深度 学习 是 
机器 学习 的 一个 分支 是 神经 网络 的 重命名 
神经 网络 是 一 系列 学习 技术 历史 上 曾受/nr 
模拟 脑 计算 工作 的 启发 可被 看作 学习 参数 
可微 的 数学 函数 深度 学习 的 名字 源于 许多 
曾被 连接 在 一起 的 可微 函数 虽然 全部 机器学习 
技术 都 可以 被 认为 是 基于 过去 的 观测 
学习 如何 做出 预测 但是 深度 学习 方法 不仅 学习 
预测 而且 学习 正确 地 表示 数据 以 使其 更 
有助于 预测 给 出 一个 巨大 的 输入 输出 映射 
集合 深度 学习 方法 将 数据 喂 给 一个 网络 
其 产生 输入 的 后继 转换 直到 用 最终 的 
转换 来 预测 输出 网络 产生 的 转换 都 学习 
自 给定 的 输入 输出 映射 以便/c 每个/r 转换/v 都/d 
使得/v 更/d 易于/v 将/d 数据/n 和/c 期望/v 的/uj 标签/n 之间/f 
建立/v 联系/n 开发 者 负责 设计 网络结构 和 训练 方式 
提供 给 网络 合适 的 输入 输出 实例 集合 将 
输入 数据 恰当 地 编码 大量 学习 正确 表示 的 
工作 则 由 网络 自动 执行 同时 受到 网络 结构 
的 支持 1.3 自然语言 处理 中 的 深度 学习 神经 
网络 提供 了 强大 的 学习 机制 对 自然 语言 
处理 问题 极具 吸引力 将 神经 网络 用于 语言 的 
一个 主要 组件 是 使用 嵌入 曾 embedding layer 即将 
离散 的 符号 映射 为 相对 低维 的 连续 向量 
当 嵌入 单词 的 时候 从 不同 的 独立 符号 
转换 为 可以 运算 的 数学 对象 特别地 向量 之间 
的 距离 可以 等价 于 单词 之间 的 距离 这 
使得 更容易 从 一个 单词 泛化 到 另一个 单词 学习 
单词 的 向量 表示 成为 训练 过程 的 一部分 再往 
上层 网络 学习 单词 向量 的 组合 方式 以 更 
有利于 观测 该 能力 减轻 了 离散 和 数据 稀疏 
问题 有 两种 主要 的 神经 网络结构 即 前馈 网络 
feed forward network 和 循环 / 递归 网络 recurrent / 
recursive network 它们 可以 以 各种 方式 组合 前馈 网络 
也叫 多层 感知器 Multi Layer Perceptron 其 输入 大小 固定 
对于 变化 的 输入 长度 我们 可以 忽略 元素 的 
顺序 当 将 输入 集合 喂给 网络 时 网络 学习 
用 有 意义 的 方式 组合 它们 之前 线性 模型 
所能 应用 的 地方 多层 感知器 都能 使用 网络 的 
非线性 以及 易于 整合 预 训练 词 嵌入 的 能力 
经常 导致 更高 的 分类 精度 卷积 前馈 网络 是 
一类 特殊 的 结构 其 善于 抽取 数据 中 有 
特殊 意义 的 局部 模式 将 任意 长度 的 输入 
喂 给 网络 网络/n 能/v 抽取/v 有/v 意义/n 的/uj 局部/n 
模式/n 这些 模式 对 单词 顺序 敏感 而 忽略 它们 
在 输入 中 出现 的 位置 这些/r 工作/vn 适合/v 于/p 
识别/v 长/a 句子/n 或者/c 文本/n 中/f 有指/i 示性/n 的/uj 短语/nz 
和/c 惯用语/n 循环 神经 网络 是 适于 序列 数据 的 
特殊 模型 网络 接收 输入 序列 作为 输入 产生 固定 
大小 的 向量 作为 序列 的 摘要 对于 不同 的 
任务 一个 序列 的 摘要 意味着 不同 的 东西 也 
就是说 用于 回答 一个 句子 情感 所需 的 信息 与 
回答 其 语法 的 信息 并不 相同 循环 网络 很少 
被 当做 独立 组件 使用 其 能力 在于 可能 当做 
可 训练 的 组件 喂 给 其他 网络 组件 然后 
串联 地 训练 它们 例如 循环 网络 的 输出 可以 
喂 给 前馈 网络 用于 预测 一些 值 循环 网络 
被 用作 一个 输入 转换器 其 被 训练 用于 产生 
富含 信息 的 表示 前馈 网络 将 在其 上 进行 
运算 对于 序列 循环 网络 是 非常 引人注目 的 模型 
可能 也 是 神经 网络 用于 自然语言 最 令人 激动 
的 成果 它们 允许 打破 自然语言 处理 中 存在 几十年 
的 马尔科夫 假设 设计 能 依赖 整个 句子 的 模型 
并在 需要 的 情况 下 考虑 词 的 顺序 同时 
不太 受 由于 数据 稀疏 造成 的 统计 估计 问题 
之苦 该 能力 是 语言 模型 产生 了 令人 印象 
深刻 的 收益 其中 语言 模型 指 的 是 预测 
序列 中 下一个 单词 的 概率 等价 于 预测 一个 
序列 的 概率 是 许多 自然语言 处理 应用 的 核心 
递归 神经 网络 将 循环 网络 从 序列 扩展到 树 
自然语言 处理 的 许多 问题 是 结构化 的 需要 产生 
复杂 的 输出 结构 如 序列 和树/nr 神经网络 模型 能适应 
该 需求 一 方面 可以 改进 已知 的 面向 线性 
模型 的 结构化 预测 算法 另一方面 可以 使用 新 的 
结构 如 序 列到 序列 编码器 解码器 模型 指 的 
是 条件 生成 模型 此类 模型 是 目前 公认 的 
最好 的 机器 翻译 模型 的 核心 最后 许多 自然语言 
预测 任务 互 相关联 在 某种 意义 上 知道 一种 
任务 是 如何 执行 的 将对 另 一些 任务 有所 
帮助 另外 我们 可能 没有 足够 的 有 监督 带 
标签 训练 数据 而 只有 足够 的 原始 文本 无 
标签 数据 那/r 我们/r 能从/nr 相关/v 的/uj 任务/n 或者/c 未/d 
标注/v 数据/n 中/f 学习/v 吗/y 对于 多 任务 学习 Multi 
Task Learning 即从 相关 问题 中 学习 和半/nr 监督 semi 
supervised 学习 从 额外 的 未 标注 的 数据 中 
学习 神经网络 方法 提供 了 令人 激动 的 机会 注 
文章内容 摘自 Yoav Goldberg 所著 Neural Network Methods for Natural 
Language Processing 的 中文版 基于 深度 学习 的 自然 语言 
处理 chapter 1 Introduction # 希拉里 右键 门 文档 主题 
分类 LDA 模型 数据 读取 还 有点 问题 # 数据 
来源 请 联系 公众 号 湾区 人工智能 import numpy as 
np import pandas as pd import re import codecs # 
U n i c o d e E n c 
o d e E r r o r mbcs codec 
can t encode characters in position 0 1 invalid character 
df = pd . read _ csv D / 自然语言 
处理 / Lecture _ 3 LDA 主题 模型 课件 与 
资料 / Lecture _ 3 LDA 主题 模型 课件 与 
资料 / 主题 模型 课件 与 资料 / input / 
HillaryEmails . csv encoding = utf 8 # 原 邮件 
数据 中 有 很多 Nan 的 值 直接 扔了 df 
= df Id E x t r a c t 
e d B o d y T e x t 
. dropna def clean _ email _ text text text 
= text . replace \ n # 新行 我们 是 
不 需要 的 text = re . sub r text 
# 把 的 两个 单词 分开 比如 july edu = 
= july edu text = re . sub r \ 
d + / \ d + / \ d + 
text # 日期 对 主体 模型 没什么 意义 text = 
re . sub r 0 2 0 9 0 6 
0 9 text # 时间 没 意义 text = re 
. sub r \ w + @ \ . \ 
w + text # 邮件地址 没 意义 text = re 
. sub r / a zA Z * \ / 
/ \ * A Za z0 9 \ _ + 
\ . + A Za z0 9 \ . \ 
/ % & = \ \ _ + / i 
text # 网址 没 意义 pure _ text = # 
以防 还有 其他 特殊字符 数字 等等 我们 直接 把 他们 
loop 一遍 过滤掉 for letter in text # 只留下 字母 
和 空格 if letter . isalpha or letter = = 
pure _ text + = letter # 再把 那些 去除 
特殊字符 后 落单 的 单词 直接 排除 # 我们 就 
只剩 下 有 意义 的 单词 了 text = . 
join word for word in pure _ text . split 
if len word 1 return text # 新建 一个 colum 
docs = df E x t r a c t 
e d B o d y T e x t 
docs = docs . apply lambda s clean _ email 
_ text s # docs . head 1 . values 
doclist = docs . values from gensim import corpora models 
similarities import gensim stoplist = very ourselves am doesn through 
me against up just her ours couldn because is isn 
it only in such too mustn under their if to 
my himself after why while can each itself his all 
once herself more our they hasn on ma them its 
where did ll you didn nor as now before those 
yours from who was m been will into same how 
some of out with s being t mightn she again 
be by shan have yourselves needn and are o these 
further most yourself having aren here he were but this 
myself own we so i does both when between d 
had the y has down off than haven whom wouldn 
should ve over themselves few then hadn what until won 
no about any that for shouldn don do there doing 
an or ain hers wasn weren above a at your 
theirs below other not re him during which texts = 
word for word in doc . lower . split if 
word not in stoplist for doc in doclist # 用词 
袋 的 方法 把 每个 单词 用 一个 数字 index 
指代 并 把 我们 的 原文 本 变成 一条 长长的 
数组 dictionary = corpora . Dictionary texts corpus = dictionary 
. doc2bow text for text in texts # 建立 模型 
lda = gensim . models . ldamodel . LdaModel corpus 
= corpus id2word = dictionary num _ topics = 20 
# 第 10号 分类 其中 最 常 出现 的 单词 
是 # lda . print _ topic 10 topn = 
5 # 所有 的 主题 打印 出来 看看 lda . 
print _ topics num _ topics = 20 num _ 
words = 5 # 可以 把 新鲜 的 文本 / 
单词 分类 成 20个 主 题中 的 一个 文本 和 
单词 都 必须 得 经过 同样 步骤 的 文本 预处理 
+ 词 袋 化 也 就是说 变成 数字 表示 每个 
单词 的 形式 lda . get _ document _ topics 
bow 自然语言 处理 自然语言 处理 是 什么 自然语言 处理 Natural 
Language Process 就是 利用 计算机 来 处理 人类 语言 的 
学科 属于 计算机 与 语言学 的 交叉 学科 自然语言 处理 
有 哪些 技术 大致 包括 如下 技术 1 . 分词 
Word Segmentation 或 Word Breaker WB 在 英文 文本 当中 
每个 词 之间 都 有间 隔好 分 但在 中文 文本 
当中 一句话 之间 每个 词 是 没有 间隔 的 所以 
需要 对 一个 句子 当中 每个字 进行 切分 句子 的 
基本 语义 单元 就 变成 了 词 这 就是 分词 
任务 2 . 句法分析 Parsing 句法分析 指 的 是 将 
句子 中 每个 部分 的 组块 也 就是 每个 词 
字 的 归属 类 标注 出来 组块 分析 标出 句子 
的 短语 块 如 This is a dog NP 超级 
标签 分析 给 每个 句子 加上 超级 标签 超级 标签 
是 一个 树形 结构图 成分 句法分析 分析 句子成分 给出 一颗 
由 终结符 和非/nr 终结符 构成 的 成分 句法树 依存 句法分析 
分析 句 中词 的 依存 关系 给出 一颗 由 词语 
依存 关系 构成 的 依存 句法树 3 . 信息 抽取 
Information Extraction IE 命名 实体 识别 和 关系 抽取 Named 
Entity Recognition & Relation Extraction NER 我们 从 一段 文本 
中 抽取 关键 信息 即 从无 结构 的 文本 中 
抽取 结构化 的 信息 4 . 词性 标注 Part Of 
Speech Tagging POS 对 词语 的 词性 进行 标注 5 
. 指代 消解 Coreference Resolution 消除 一些 对 文本处理 没有 
意义 的 指 代名词 减轻 程序 对 语言 的 处理 
6 . 词义 消 歧 Word Sense Disambiguation WSD 一个词 
他 可能会 有 歧义 该 任务 是 用来 消除歧义 的 
7 . 机器翻译 Machine Translation MT 要 实现 文本 的 
自动 翻译 8 . 自动 文摘 Automatic Summarization 摘要 是 
一大 段 文字 我们 需要 将 里面 的 梗 提取 
出来 然后 缩短 方便 阅读 或 方便 提取 信息 9 
. 问答 系统 Question Answering 你 提出 一个 问题 机器 
给予 你 准确 的 答案 10 . OCR 也 属于 
视觉 模块 内容 将 图片 当中 的 文字 通过 机器 
识别 图像 翻译成 文本 形式 11 . 信息检索 Information Retrieval 
IR 用户 进行 信息 查询 和 获取 的 主要 方式 
是 查找 信息 的 方法 和 手段 自然语言 处理 核心 
问题 是 什么 文本 分类 关键词 提取 情感 分析 语义 
消 歧 主题 模型 机器翻译 问题 问答 汉语分词 垂直 领域 
的 对话 机器 人 自然 语言 处理 有 哪些 应用 
方向 搜索引擎 文本 主题 / 标签 分类 文本 创作 与 
生成 机器翻译 情感 分析 舆情 监控 语音 识别 系统 对话 
机器 人 自然 语言 处理 的 难点 是 什么 歧义 
问题 很多 话 的 意思 说 的 模棱两可 具有 歧义 
知识 问题 知识 稀疏 或者 词汇 稀疏 词汇 稀疏 导致 
了 搭配 稀疏 然后 导致 了 语义 稀疏 它 有一个 
递进 关系 一个 比较 出名 的 定律 叫 齐夫/nr 定律 
Zipf Law 这个 定律 是 说 在 自然 语言 语料 
当中 一个/m 单词/n 出现/v 的/uj 频率/n 和它在/nr 频率/n 表/v 当中/s 
的/uj 排名/v 基本/n 成/n 一个/m 反/zg 比/p 关系/n 离散 符号计算 
问题 我们 看到 的 文本 其实 都是/nr 一些 符号 对 
计算机 来说 它 看 的 其实 也 是 一些 离散 
的 符号 但 我们 知道 计算机 其实 最 擅长 的 
是 数值 型 的 运算 而 不是 符号 的 推理 
并且 符号 之间 的 逻辑推理 会 非常 复杂 语义 本质 
的 问题 到底 什么 是 语义 什么 是 语义 语言 
里面 到底 是 什么 东西 符号 背后 真正 的 语义 
怎么 来 表示 语言学家 他 走 的 路子 就是 我 
构建 好多 形式化 的 结构化 的 图 之类 的 这种 
结构 去做 语义 或者 是 一些 符号 推导 系统 认为 
它 可以 接近 语义 本质 但是 这些 其实 走 得越 
远离 计算机 就 越远 因为 它 越 符号 语义 的 
可 解释性 就会 很差 拿 数字 来 表示 语义 我们 
也 不 知道 这个 数字 到底 它 是 什么 东西 
所以 目前 为止 现在 研究 领域 对 这个 问题 解决 
得 比较 差 自然语言 处理 学习 路线 熟悉 基本知识 基本操作 
如 文本 操作 正则 掌握 一些 基本 文本处理 框架 英文 
有 NLTK spaCy 中文 有 中科院计算所 NLPIR 哈工大 LTP 清华大学 
THULAC Hanlp 分词器 Python jieba 工具 库 知道 什么 是 
语言 模型 利用 语言 模型 来 完成 一些 项目 文本 
表示 将 文本 中的 字符串 转化 为 计算机 当中 的 
向量 文本 分类 分类 模型 传统 的 一个 解决 方法 
就是 标 带 标注 的 语料 再 特征提取 然后 训 
分类器 进行 分类 这个 分类器 就 会用 比如说 逻辑 回归 
贝叶斯 支持 向量 机 决策树 等等 主题 模型 使用 无 
监督 学习 的 方式 对 文本 中的 隐含 语义 进行 
聚 类 的 统计模型 seq2seq 模型 通过 深度 神经 网络 
将 一个 序列 作为 映射 为 另外 一个 输出 的 
序列 文本 生成 GAN 文本 生成 也叫 机器人 写作 原文 
7 Applications of Deep Learning for Natural Language Processing 作者 
Jason Brownlee 翻译 无阻 我 飞扬 摘要 在 这篇文章 中 
作者 详细 介绍 了 自然 语言 处理 深度 学习 的 
7种 应用 以下 是 译文 自然语言 处理 领域 正在 从 
统计 方法 转变为 神经网络 方法 自然 语言 中 仍 有 
许多 具有 挑战性 的 问题 需要 解决 然而 深度 学习 
方法 在 一些 特定 的 语言 问题 上 取得 了 
最新 的 成果 这 不仅仅 是 深度 学习 模型 在 
基准 问题 上 的 表现 基准 问题 也是 最 有趣 
的 事实上 一个 单一 的 模型 可以 学习 词义 和 
执行 语言 任务 从而 消 除了 对 专业 手工 制作 
方法 渠道 的 需要 在 这篇文章 中 你 会 发现 
7个 有趣 的 自然 语言 处理 任务 也会 了解 深度 
学习 方法 取得 的 一些 进展 1 文本 分类 2 
语言 建模 3 语音识别 4 字幕 生成 5 机器翻译 6 
文档 摘要 7 问答 Q & A 我 试图 专注 
于你/nr 可能 感兴趣 的 各种 类型 的 终端用户 问题 而 
不是 更多 的 学术 或 语言 的 子 问题 在 
有些 方面 深度 学习 已经 做 的 很好 如 词性 
标注 程序 分块 命名 实体 识别 等等 每个 示例 提供 
了 一个 问题 描述 示例 对 演示 方法 和 结果 
的 文档 引用 大多数 参考 来自 2015年 的 Goldberg s 
的 优秀 的 NLP 研究 人员 深度 学习 入门 文献 
你 有 没有 一个 深度 学习 中 最 受欢迎 的 
NLP 应用 没有 被 列出 请在 下面 的 评论 中 
告诉 我 1 文本 分类 给出 一个 文本 实例 预测 
一个 预定义 的 类 标签 文本 分类 的 目的 是 
对 文档 的 标题 或 主题 进行 分类 575页 自然语言 
处理 的 基础 统计 1999 一个 流行 的 分类 示例 
是 情感 分析 类 标签 代表 源 文本 的 情感 
基调 比如 积极 的 或 消极 的 下面 是 另外 
三 个 例子 垃圾邮件 过滤 将 电子 邮件 文本 分类 
为 垃圾 邮件 或 正常 邮件 语言识别 对 源 文本 
的 语言 进行 分类 体裁 分类 对 小说 故事 体裁 
进行 分类 此外 这个 问题 可以 用 某种 方式 加以 
解决 将 多个 类 分配 给 一个 文本 即 所谓 
的 多 标签 分类 如 给 一个 源 tweet 预测 
多个 # 标签 更多 相关 主题 的 内容 请 参见 
Scholarpedia 的 文本 分类 维基 百科 的 文档 分类 下面 
是 3个 文本 分类 深度 学习 的 论文 例子 烂片 
评论 的 情感 分析 文本 分类 的 DUCR 结构 方法 
2015 亚马逊 产品 评价 的 情感 分析 IMDB 电影 评论 
和 新闻 文章 的 主题 分类 有效 使用 词序 进行 
基于 卷积 神经 网络 的 文本 分类 2015 影评 的 
情感 分析 将 句子 分类 为 主观 的 和 客观 
的 分类 问题 类型 产品 评论 的 情感 及 更多 
基于 卷积 神经 网络 的 句子 分类 20142 语言 建模语言 
建模 真的 是 更 有趣 的 自然 语言 问题 的 
一个 子 任务 特别 是 那些 在 其它 输入 条件 
下 的 语言 模型 问题 是 根据 给出 的 前 
一个 词 来 预测 下 一个 词 这项 任务 是 
语音 或 光学 字符识别 的 基础 也 用于 拼写 矫正 
手写识别 和 统计 机器翻译 191页 统计 自然语言 处理 基础 1999 
. 除了 对 语言 建模 的 学术 兴趣 外 它 
也是 许多 自然语言 处理 体系 结构 深度 学习 的 一个 
重要 组成部分 一个 语言 模型 学习 词 与 词 之间 
的 概率 关系 这样 以来 新的 词 的 序列 可以 
生成 与 源 文本 统计学 上 一致 的 文本 内容 
单独地 语言 模型 可 用于 文本 或 语音 生成 例如 
生成 新的 文章 标题 生成 新的 句子 段落 或 文件 
生成 一个 句子 的 建议 延续 的 句子 有关 语言 
建模 的 更多 信息 请 参见 维基百科 上 的 语言 
模型 循环 神经 网络 的 不可思议 的 效用 2015 生成 
基于 模型 的 合成 文本 语音 第十 讲 牛津 2017 
下面 是 深度 学习 语言 建模 仅有 的 一个 例子 
英语 课文 书籍 和 新闻 文章 的 的 语言 模型 
一种 神经 概率 语言 模型 20033 语音识别 语音 识别 是 
理解 说 了 什么 的 问题 语音 识别 的 任务 
是 将 包含 自然语言 话语 的 语音 映射 成说 话人 
想要 表达 的 对应 的 词 传统 的 语音 识别 
模型 是 通过 人工 建立 一张 语音 词表 将 相似 
发音 的 字母 划分 为 一类 并 借助 一个 分类 
模型 实现 语音 到 字母 的 转译 458页 深度 学习 
2016 . 给定 作为 音频 数据 的 文本 的 发声 
该 模型 必须 生成 可读 的 文本 自动 给出 自然 
语言 的 处理 这个 问题 也可 被称为 自动 语音 识别 
ASR . 语言 模型 用于 创建 以 音频 数据 为 
条件 的 文本 输出 包含 的 一些 例子 录制 语音 
为 电影 或 电视 节目 创建 文本 字幕 开车 的 
时候 向 无线电 发出 指令 有关 语音 识别 的 更多 
信息 请 参见 维基百科 上 的 语音 识别 以下 是 
用于 语音识别 深度 学习 的 3个 例子 英语 语音 到 
文字 连接时间 分类 循环 神经 网络 的 不 分段 标签 
序列 数据 2006 英语 语音 到 文字 深度 循环 神经 
网络 的 语音 识别 2013 英语 语音 到 文字 用于 
语音 识别 的 卷积 神经网络 结构 的 研究 和 优化 
技术 2014 4 字幕 生成 字幕 生成 是 描述 图像 
内容 的 问题 给定 一个 数字 图像 如 一张 图片 
生成 关于 这个 图像 内容 的 文本 描述 语言 模型 
用于 创建 符合 图像 内容 的 字幕 包含 的 一些 
例子 描述 一个 场景 的 内容 为 照片 创建 标题 
描述 一个 视频 这 不仅仅 是 对 听障 者 的 
一个 应用程序 还 可以 为 图像 和 视频 数据 生成 
可读 的 文本 将来 可以 搜索 比如 在 网上 以下 
是 字幕 生成 深度 学习 的 3个 例子 为 照片 
生成 字幕 展示 出席 和 讲述 视觉 注意力 的 神经 
图像 字幕 生成 2016 . 为 照片 生成 字幕 展示 
和 讲述 神经 图像 字幕 生成器 2015 . 为 视频 
生成 字幕 片段 到 片段 视频 到 文本 2015.5 机器翻译/l 
机器/n 翻译/v 是/v 把/p 源/ng 文本/n 从/p 一种/m 语言/n 转换/v 
成/n 另外/c 一种/m 语言/n 的/uj 问题/n 机器翻译 文本 或 语音 
从 一种 语言 到 另外 一种 语言 的 自动 翻译 
它 是 NLP 最 重要 的 应用 463页 统计 自然语言 
处理 基础 1999 . 考虑 到 深度 神经 网络 的 
使用 该 领域 被 称为 神经 机器翻译 在 一个 机器翻译 
任务 中 输入 由 一些 语言 中 的 一系列 符号 
组成 计算机程序 必须 把 它 转换成 另一种 语言 中 的 
符号 序列 这 通常 用于 自然语言 比如 从 英语 到 
法语 的 翻译 深度 学习 最近 开始 对 这种 任务 
产生 重要 影响 98页 深度 学习 2016 . 语言 模型 
用于 输出 翻译 以后 语言 的 目标 文本 以 源 
文本 为 基础 包含 的 一些 例子 将/d 一个/m 文本文件/n 
从/p 法语/nz 翻译/v 成/n 英语/nz 将 西班牙 音频 翻译成 德语 
文本 将 英语 文本 翻译成 意大利 音频 更多 关于 神经 
机器翻译 请 参见 维基百科 上 的 神经 机器翻译 下面 是 
机器 翻译 深度 学习 的 3个 例子 从 英语 到 
法语 的 文本 翻译 基于 神经 网络 的 片段 到 
片段 的 学习 2014 从 英语 到 法语 的 文本 
翻译 联合 学习 对齐 和 翻译 的 神经 机器翻译 2014 
从 英语 到 法语 的 文本 翻译 基于 循环 神经网络 
组合 语言 和 翻译 模型 20136 文档 摘要 文档 摘 
要是 对 创建 的 文本文档 进行 简短 描述 的 任务 
如上所述 语言 模型 用于 基于 完整 文档 的 摘要 输出 
一些 文档 摘要 的 例子 为 一篇 文档 创建 一个 
标题 为 一篇 文档 创建 一个 摘要 更多 关于 这个 
话题 的 信息 请 参见 维基百科 上 的 自动 摘要 
深度 学习 已经 被 应用于 自动 文本 摘要 成功 了吗 
下面 是 文档 摘要 深度 学习 的 3个 例子 新闻 
文章 中 的 句子 摘要 一个 抽象 概括 的 神经 
注意力 模型 2015 新闻 文章 中 的 句子 摘要 使用 
片段 到 片段 RNN 循环 神经网络 的 抽象 总结 及 
更多 2015 新闻 文章 中 的 句子 摘要 通过 提取 
句子 和 单词 的 神经 摘要 20167 问答 回答 问题 
就是 给 定 一个 主题 如 文本文件 回答 关于 这个 
主题 的 一个 特定 问题 问答 系统 尝试 回答 用户 
以 问题 形式 表述 的 疑问 它 返回 适当 的 
短语 如 位置 人员 或者 日期 例如 问题 是 总统 
肯尼迪 为什么 被 刺杀 可能 回答 的 短语 是 Oswald 
凶手 奥 司 华德 377页 统计 自然语言 处理 基础 1999 
包含 的 一些 例子 维基百科 上 的 问答 更多 关于 
问答 的 信息 请 参见 关于 维基百科 文章 的 问答 
关于 新闻 文章 的 问答 关于 医疗 记录 的 问答 
下面 是 问答 深度 学习 的 3个 例子 新闻 文章 
中 的 问答 阅读 和 理解 的 机器 教学 2015 
回答 关于 Freebase 文章 的 一般 知识性 问题 用 多列 
卷积 神经网络 回答 关于 Freebase 的 问题 2015 回答 给定 
文件 的 事实型 问题 深度 学习 回答 选择 句 2015 
扩展阅读 如果 你 需要 更 深入 的 了解 本节 提供 
更多 用于 NLP 深度 学习 应用 程序 的 资源 自然语言 
处理 的 优先 神经网络 模型 2015 从零/nr 几乎 开始 自然语言 
处理 2011 自然语言 处理 深度 学习 实践 概述 牛津 2017 
深度 学习 或 神经 网络 的 NLP 问题 已 成功 
应用 深度/ns 学习/v 能像/nr 自然语言/l 处理/v 在/p 视觉/n 和/c 语音/n 
处理/v 领域/n 一样/r 取得/v 类似/v 的/uj 突破/vn 吗/y 2017年 10月 
14日 SDCC 2017 之大 数据 技术 实战 线上 峰会 即将 
召开 邀请 圈内 顶尖 的 布道 师 技术 专家 和 
技术 引领者 共话 大 数据 平台 构建 优化 提升 大 
数据 平台 的 各项 性能 Spark 部署 实践 企业 流 
平台 实践 以及 实现 应用 大 数据 支持 业务 创新 
发展 等 核心 话题 七位 大牛 与 你 相聚 狂欢 
详情 查看 所有 嘉宾 和 议题 以及 注册 参会 陌陌 
科技 职位 描述 岗位职责 1 参与 陌陌 平台 文本 spam 
识别 的 开发 参与 优化 文本 分类 聚 类 文本 
相似性 语言 模型 情感 分析 用户 行为 分析 等 工作 
持续 改进 和 升级 现有 产品 2 跟进 文本 挖掘 
NLP 和 机器学习 领域 的 前沿 技术 将 前沿 技术 
应用 于 实际 业务 岗位 要求 1 在 以下 至少 
一个 领域 有 一定 了解 1 统计 机器学习 相关 方法 
如 深度 神经网络 概率 图 模型 最优化 方法 等 2 
语义 理解 技术 如 知识图谱 语义 解析 知识 挖掘 等 
2 良好 的 分析 问题 与 发现 问题 的 能力 
善于 归纳 技术 方案 的 特性 并 找出 其 不足 
与 改进 方法 3 有 一定 编程 能力 熟悉 Hadoop 
Spark 等 分布式计算 框架 者 更佳 4 具有 良好 的 
沟通 能力 和 良好 的 团队 合作 精神 5 应聘 
实习生 职位 的 要求 能 每周 实习 3天 以上 实习 
半年 以上 = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = 京东 集团 职位 描述 职位 自然语言 处理 
算法 工程师 主要 研究 方向 为 自然语言 处理 文本 分析 
或 相关 机器学习 方向 非常 欢迎 从事 深度 学习 机器学习 
方向 研究 且有 兴趣 在 文本处理 方向 做 落地 实践 
的 同学 也 欢迎 投递 简历 基本要求 1 .   
有 自然 语言 处理 文本 分析 或 文本 理解 等 
相关 项目 经验 2 .   熟练 掌握 一门 脚本语言 
如 python 或者 perl 3 .   对 中文分词 词性 
标注 命名 实体 识别 的 某一 研究 领域 有 较深 
的 研究 4 .   对 文本 分类 语义 理解 
文本/n 摘要/v 等/u 技术/n 方向/n 有/v 一定/d 的/uj 了解/v 和/c 
研究/vn 5 .   对于 AI 技术 新产品 新/a 技术/n 
有/v 关注/v 和/c 热情/n 有 以下 经验 之一 者 优先 
考虑       1 .   能够 编写 高质量 
的 线上 服务 代码 2 . NLP 之外 的 其它 
机器学习 方向 的 相关 项目 经验 3 .   对 
基于 DNN 的 NLP 前沿 方法 有较/nr 深入 了解 4 
.   对于 对话 系统 聊天 机器人 的 原理 有较/nr 
深入 了解 = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = 爱奇艺 职位 介绍 
1       feed 流 标签 分类 2   
    关键词 提取 命名 实体 提取 文本 纠错 等 
3     对话 系统 职位 要求 1     
    计算机 电子 数学 物理 等 相关 专业 硕 
/ 博 研究生 均可 2         熟悉 
Linux 基本操作 熟悉 C + + 和 python 对/p 数据/n 
结构/n 和/c 算法/n 设计/vn 有/v 较为/d 深刻/d 的/uj 理解/v 3/m 
 /i  /i  /i  /i 有/v 自然/d 语言/n 处理/v 相关/v 知识/v 
包括 但 不限 于 分词 词性 标注 命名 实体 识别 
等 4         熟悉 基本 的 机器学习 
算法 和 深度 学习 如 RNN CNN 等 算法 5 
        熟悉 一种 深度 学习 框架 如 
tensorFlow caffe6         优秀 的 分析 问题 
和 解决 问题 的 能力 对 解决 具有 挑战性 问题 
充满 激情 7         每周 至少 4个 
工作日 6个 月 以上 8         对 
2019年 毕业 的 同学 提供 转正 机会 = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = 点 智 互动 职位 描述 1 负责 NLP 
基础 算法 包括 分词 词性 标注 命名 实体 识别 新词 
发现 句法 和 语义分析 等 算法 优化 2 负责 实现 
智能 问答 从 海量 对话 数据 中 挖掘 数据 构建 
行业 知识图谱 回答 常见问题 提升 用户 体验 3 负责 优化 
对话 系统 研发 具有 学习 能力 的 智能 机器 4 
负责 数据 分类 / 聚 类 情感 分析 和 质量 
识别 工作 任职 要求 1 211/985 计算机 模式识别 数学 相关 
专业 本科 及 以上 学历 2 有 数据挖掘 策略 和 
算法 研发 工作 实践 经验 优先 3 有 文本 挖掘 
相关 经验 有较/nr 丰富 Python linux 环境 开发 经验 4 
有 海量 数据挖掘 知识图谱 构建 深度 学习 研发 实践经验 优先 
5 有 搜索引擎 风 控 项目 推荐 系统 行为 分析 
系统 用户 画像 等 研发 经验 优先 = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = 最右 APP 工作 
职责 研究 数据挖掘 或 机器学习 领域 的 前沿 技术 并 
用于 实际 问题 的 解决 和 优化 大 规模 机器学习 
算法 研究 及 并行 化 实现 为 各种 大 规模 
机器学习 应用 研发 核心技术 通过 对 数据 的 敏锐 洞察 
深入 挖掘 产品 潜在 价值 和 需求 进而 提供 更 
有价值 的 产品 和 服务 通过 技术 创新 推动 产品 
成长 职责 要求 热爱 互联网 对 技术 研究 和 应用 
抱有 浓厚 的 兴趣 有/v 强烈/a 的/uj 上进心/nt 和/c 求知欲/l 
善于 学习 和 运用 新 知识 具有 以下 一个 或 
多个 领域 的 理论 背景 和 实践 经验 机器学习 / 
数据挖掘 / 计算机 视觉 / 信息检索 / 推荐 系统 至少 
精通 一门 编程语言 熟悉 网络 编程 多线程 分布式 编程技术 对/p 
数据/n 结构/n 和/c 算法/n 设计/vn 有/v 较为/d 深刻/d 的/uj 理解/v 
良好 的 逻辑 思维能力 对 数据 敏感 能够 发现 关键 
数据 抓住 核心问题 较强 的 沟通 能力 和 逻辑 表达能力 
具备 良好 的 团队 合作 精神 和 主动 沟通 意识 
具有 以下 条件 者 优先 熟悉 文本 分类 聚 类 
计算机 视觉 有 相关 项目 经验 熟悉 海量 数据 处理 
最优化 算法 分布式计算 或 高性能 并行计算 有 相关 项目 经验 
有 应用 tensorflow 等 深度 学习 框架 解决 过 实际应用 
问题 的 经验 = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = 字节 跳动 职位 
描述 1 . 文本 分类 包括 训练 语料 收集 清理 
标注 特征选择 特征值 优化 类别 体系 修改 训练 算法 改进 
等 2 . 话题 聚 类 分析 新闻 语料 学习 
出 语料 中 涵盖 的 相关 话题 以及 在线 预测出 
文章 中 包含 的 话题 3 . 分词 词性 预测 
命名 实体 识别 结合/v 业界/n 最新/d 进展/vn 和/c 语料/n 改进/v 
和/c 开发/v 相关/v 算法/n 职位 要求 1 . 对 职位 
描述 中 的 一项 或 多项 工作 感兴趣 且 熟悉 
有 具体 相关 经验 者 优先 2 . 具备 强悍 
的 编码 能力 熟悉 Linux 开发环境 熟悉 Python / C 
+ + / Java / Scala 语言 3 . 优秀 
的 分析 问题 和 解决 问题 的 能力 对 解决 
具有 挑战性 问题 充满 激情 实习 时间 ≥ 4天 / 
周   ≥ 3 个月 需 提交 的 材料 个人简历 
+ 做过 的 项目 代码 岗位 对 编程 能力 有 
要求 希望 通过 代码 进行 二 次 筛选 = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= 搜狐 工作 内容 结合 产品 需求 应用 自然语言 处理 
技术 进行 文本 分析 分类 关键词 提取 相关性 计算 等 
任务 职位 要求 1 . 自然语言 处理 等 相关 专业 
2 . 熟练掌握 c + + 或者 java 熟悉 python 
编程 3 . 熟悉 linux 和 shell 环境 4 . 
有 积极向上 的 工作 热情 有 独当一面 的 工作 能力 
思维 清晰 表达 准确 5 . 有大/nr 数据/n 和短/nr 视频/n 
推荐/v 系统/n 方面/n 的/uj 相关/v 经验/n 加分/v =/i =/i =/i 
=/i =/i =/i =/i =/i =/i =/i =/i =/i =/i 
=/i =/i =/i =/i =/i =/i =/i =/i =/i =/i 
=/i =/i =/i =/i =/i =/i =/i =/i =/i =/i 
=/i =/i =/i =/i =/i =/i =/i =/i =/i =/i 
=/i =/i =/i =/i =/i =/i =/i =/i 网易/n 工作/vn 
职责/n 1 理解 业务 产品 运营 的 工作 需求 将 
业务 环节 提取 抽象 为 数学 问题 设计 实现 基于 
数据 挖掘 的 解决方案 进行/v 数据/n 特征/n 工程/n 和/c 机器学习/i 
模型/n 的/uj 选取/v 和调优/nr 对接 产品 开发 数据 人员 实现 
模型 的 测试 和 落地 2 . 分析 和 研究 
数据 与 实际 业务 的 关联关系 针对 具体 业务 需求 
场景 设计 用户价值 用户 行为 预测 用户 分类 用户 画像 
用户 生命周期 用户 流失 交易 盈利 风险 控制 等 模型 
并 搭建 职能 投资 顾问 系统 任职 资格 1 . 
硕士 或 博士 应届生 或 工作 经验 1 3年 2 
. 计算机 统计 电子 数学 数理统计 等 相关 专业 机器学习 
/ 数据挖掘 / 信息检索 / 自然语言 处理 / 统计分析 相关 
背景 3 . 掌握 常用 的 分类 回归 聚 类 
预测 关联 规则 序列 模式 等 挖掘 算法 了解 数据挖掘 
前沿技术 4 . 熟练 使用 一种 或 多种 数据挖掘 工具 
使用 python / shell / scala 等 脚本语言 5 . 
具有 很强 的 学习 和 研究 能力 英语 熟练 能够 
熟练 阅读 英文 技术 资料 积极 创新 乐于 协作 善于 
沟通 = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = 创新 工场 工作 职责 
1 对 中文 文本 进行 基础 NLP 处理 包括 但 
不限 于 关键词 提取 命名 实体 识别 分词 词性 标注 
文本 分析 新词 发现 词义 消 歧 等 对 海量 
文本 进行 深度 分析 情感 分析 文本 分类 文本 生成 
等 2 调研 和 设计 策略 算法 参与 NLP 深度 
学习 相关 算法 的 研究 与 开发 4 . 对 
对话 系统 智能 问答 知识图谱 等 有了 解者 加分 5 
. 学习 NLP 领域 的 先进 技术 并 开展 相关 
研发 工作 职位 要求 1 计算机科学 自然语言 处理 数学 等 
相关 学科 的 在读 学士 硕士 或 博士 具备 良好 
的 数学 功底 包括 线性代数 数理统计 数值 优化 优化 理论 
等 方面 2 熟悉 NLP 和 机器学习 深度 学习 增强 
学习 迁移 学习 等 理论 基础 熟悉 版本控制 工具 3 
能/v 快速/d 阅读/v 并/c 理解/v 顶/v 会/v 论文/nz 并/c 能够/v 
参与/v 复现/v 和/c 改进/v 4/m 熟悉 c / c + 
+ java python R 及 GPU 开发 调试 性能 优化 
者 优先 有 深度 学习 经验 使用 过 Caffe TensorFlow 
Theano 等 工具 的 优先 熟悉 Hadoop / Hive / 
HBase / Spark / Storm 等 系统 的 优先 参与 
过 NLP 领域 顶 会 论文 发表 如 ACL AAAI 
者 优先 5 在 文本 分类 文本 聚 类 新词 
发现 深度 学习 等 领域 有 丰富 经验 优先 6 
责任心 强 积极主动 有/v 良好/a 的/uj 沟通/v 能力/n 和/c 团队/n 
合作/vn 能力/n =/i =/i =/i =/i =/i =/i =/i =/i 
=/i =/i =/i =/i =/i =/i =/i =/i =/i =/i 
=/i =/i =/i =/i =/i =/i =/i =/i =/i =/i 
=/i =/i =/i =/i =/i =/i =/i =/i =/i =/i 
=/i =/i =/i =/i =/i =/i =/i =/i =/i =/i 
=/i =/i =/i 注意/v 这些/r 都是/i 实习生/n 的/uj 要求/v 啊/zg 
最后 自己 默默 地 总结 一下 自然语言 应用 的 领域 
方面 文本 分类 相似性         4 语言 
模型 情感 分析         3 文本 分析 
        5 关键词 提取       
  4 文本 纠错 对话 系统 智能 问答     
    4 分词 词性 标注         
3 知识图谱         3 技能 要求 hadoop 
/ spark     2 脚本语言 python     7linux 
    5C + + java     4RNN CNNtensorflow 
caffe     2 人工智能 语言 是 一类 适 应于 
人工智能 和 知识 工程 领域 的 具有 符号 处理 和 
逻辑 推理 能力 的 计算机 程序 设计 语言 其中 Prolog 
是 当代 最 有 影响 的 人工智能 语言 之一 一 
什么 是 人工智能 语言 人工智能 AI 语言 是 一类 适 
应于 人工智能 和 知识 工程 领域 的 具有 符号 处理 
和 逻辑 推理 能力 的 计算机 程序 设计 语言 能够 
用 它 来 编写程序 求解 非 数值 计算 知识处理 推理 
规划 决策 等 具有 智能 的 各种 复杂 问题 典型 
的 人工智能 语言 主要有 LISP Prolog Smaltalk C + + 
等 一般来说 人工智能 语言 应 具备 如下 特点 • 具有 
符号 处理 能力 即 非 数值 处理 能力 • 适合于 
结构化 程序设计 编程 容易 • 具有 递归 功能 和 回溯 
功能 • 具有 人机交互 能力 • 适合于 推理 • 既有 
把 过程 与 说 明式 数据结构 混合 起来 的 能力 
又有 辨别 数据 确定 控制 的 模式 匹配 机制 人们 
可能会 问 用 人工智能 语言 解决 问题 与 传统 的 
方法 有 什么 区别 呢 传统 方法 通常 把 问题 
的 全部 知识 以 各种 的 模型 表达 在 固定 
程序 中 问题 的 求解 完全 在 程序 制导 下 
按着 预先安排 好 的 步骤 一步 一步 逐条 执行 解决/v 
问题/n 的/uj 思路/n 与/p 冯./nr 诺依曼/i 式/k 计算机/n 结构/n 相/v 
吻合/v 当前 大型 数据库 法 数学模型 法 统计 方法 等 
都是 严格 结构化 的 方法 对于 人工智能 技术 要 解决 
的 问题 往往 无法 把 全部 知识 都 体现 在 
固定 的 程序 中 通常 需要 建立 一个 知识库 包含 
事实 和 推理 规则 程序 根据 环境 和所给/nr 的 输入 
信息 以及 所 要 解决 的 问题 来 决定 自己 
的 行动 所以 它 是 在 环境 模式 的 制导 
下 的 推理 过程 这种 方法 有 极大 的 灵活性 
对话 能力 有/v 自我/r 解释/v 能力/n 和/c 学习/v 能力/n 这种 
方法 对 解决 一些 条件 和 目标 不 大 明确 
或 不完备 即 不能 很好 地 形式化 不好 描述 的 
非 结构化 问题 比 传统 方法 好 它 通常 采用 
启发式 试探法 策略 来 解决问题 二 Prolog 语言 及 其 
基本 结构 Prolog 是 当代 最 有 影响 的 人工智能 
语言 之一 由于 该 语言 很 适合 表 达人 的 
思维 和 推理 规则 在 自然 语言 理解 机器 定理证明 
专家 系统 等 方面 得到 了 广泛 的 应用 已经 
成为 人工智能 应用 领域 的 强有力 的 开发 语言 尽管 
Prolog 语言 有 许多 版本 但 它们 的 核心 部分 
都是/nr 一样 的 Prolog 的 基本 语句 仅 有三种 即 
事实 规则 和 目标 三种 类型 的 语句 且 都用 
谓词 表示 因而 程序逻辑 性强 文法 简捷 清晰 易懂 另一方面 
Prolog 是 陈述 性 语言 一旦 给 它 提交 必要 
的 事实 和 规则 之后 Prolog 就 使用 内部 的 
演绎推理 机制 自动 求解 程序 给定 的 目标 而 不 
需要 在 程序 中 列出 详细 的 求解 步骤 １ 
事实 事 实用 来 说明 一个 问题 中 已知 的 
对象 和 它们 之间 的 关系 在 Prolog 程序 中 
事实 由 谓词 名 及 用 括号 括 起来 的 
一个 或 几个 对象 组成 谓词 和 对象 可由 用户 
自己 定义 例如 谓词 likes bill book . 是 一个 
名为 like 的 关系 表示/v 对象/n bill/w 和/c book/w 之间/f 
有/v 喜欢/v 的/uj 关系/n ２ 规则 规则 由 几个 互相 
有 依赖性 的 简单句 谓词 组成 用来 描述 事实 之间 
的 依赖 关系 从 形式 上看 规 则由 左边 表示 
结论 的 后件 谓词 和 右边 表示 条件 的 前提 
谓词 组成 例如 规则 bird X animal X has X 
feather . 表示 凡是 动物 并且有 羽毛 那么 它 就是 
鸟 ３ 目标 问题 把 事实 和 规则 写进 Prolog 
程序 中 后 就 可以 向 Prolog 询问 有关 问题 
的 答案 询问 的 问题 就是 程序运行 的 目标 目标 
的 结构 与 事实 或 规则 相同 可以 是 一个 
简单 的 谓词 也 可以 是 多个 谓词 的 组合 
目标 分内 外 两种 内部 目标 写在 程序 中 外部 
目标 在 程序 运行时 由 用户 手工 键入 例如 问题 
student john . 表示 john 是 学生 吗 三 Prolog 
程序 的 简单 例子 以下 两个 例子 在 Turbo Prolog 
2.0 环境 下 运行 通过 注 一个 Turbo Prolog 程序 
至少 包括 谓词 段 子/ng 句段/i 和/c 目标/n 段/q 三项/m 
目标 可以 包含 在 程序 中 也 可以 在 程序 
运行时 给出 例 1 谁 是 john 的 朋友 predicates 
/ * 谓词 段 对 要用 的 谓词 名 和 
参数 进行 说明 * / likes symbol symbol friend symbol 
symbol clauses / * 子 句段 存放 所有 的 事实 
和 规则 * / likes bell sports . / * 
前 4行 是 事实 * / likes mary music . 
likes mary sports . likes jane smith . friend john 
X likes X sports likes X music . / * 
本行 是 规则 * / 当 上述 事实 与 规则 
输入 计算机 后 运行 该 程序 用户 就 可以 进行 
询问 如 输入 目标 friend john X 即 询问 john 
的 朋友 是 谁 这时 计算机 的 运行 结果 为 
X = mary mary 是 john 的 朋友 1 Solution 
得到 了 一个 结果 程序运行 界面 如下 图 所示 例 
2 汉诺塔 问题 有N个/nr 有孔 的 盘子 最初 这些 盘子 
都叠/nr 放在 柱 a 上 如 要求 将 这 N 
个 盘子 借助 柱 b 从柱a/nr 移到 柱 c 如 
移动/vn 时有/nr 以下/f 限制/v 每次 只能 移动 一个 盘子 大盘 
不能 放在 小 盘上 问 如何 移动 该 问题 可以 
采用 递 归法 思想 来 求解 其源 程序 为 predicates 
/ * 谓词 段*//nr hanoi integer move integer symbol symbol 
symbol inform symbol symbol . clauses / * 子 句段 
* / hanoi N move N a b c . 
move 1 A _ C inform A C . move 
N A B C N1 = N 1 move N1 
A C B inform A C move N1 B A 
C . inform Loc1 Loc2 nl write 移动 1个 盘子 
从柱/nr Loc1 到 柱 Loc2 . goal / * 目 
标段 问 移动 3个 盘子 的 方法 * / hanoi 
3 . 这个 例子 的 目标 包含 在 程序 里面 
因此 运行时 程序 将 直接 输出 所 有结果 程序运行 界面 
如下 图 所示 四 Prolog/w 语言/n 的/uj 常用/b 版本/n Prolog/w 
语言/n 最早/d 是/v 由/p 法国/ns 马赛/nr 大学/n 的/uj Colmerauer/w 和他的/nr 
研究/vn 小组/n 于/p 1972年/tdq 研制成功/n 早期 的 Prolog 版本 都是/nr 
解释 型 的 自 1986年 美国 Borland 公司 推出 编译 
型 Prolog 即 Turbo Prolog 以后 Prolog 便 很快 在 
PC 机上 流行起来 后来 又 经历 了 PDC PROLOG Visual 
Prolog 不同 版本 的 发展 并行 的 逻辑 语言 也于 
80 年代 初 开始 研制 其中 比较 著名 的 有 
PARLOG Concurrent PROLOG 等 1 Turbo Prolog 由 美国 Prolog 
开发中心 Prolog Development Center PDC 1986年 开发 成功 Borland 公司 
对外 发行 其 1.0 2.0 2.1 版本 取名为 Turbo Prolog 
主要 在 IBM PC 系列 计算机 MS DOS 环境 下 
运行 2 PDC Prolog1990 年后 PDC 推出 新的 版本 更名 
为 PDC Prolog 3.0 3.2 它 把 运行 环境 扩展到 
OS / 2 操作系统 并且 向 全世界 发行 它 的 
主要 特点 是 • 速度快 编译 及 运行 速度 都 
很快 产生 的 代码 非常 紧凑 • 用户界面 友好 提供 
了 图形化 的 集成 开发 环境 • 提供 了 强有力 
的 外部 数据库系统 • 提供 了 一个 用 PDC Prolog 
编写 的 Prolog 解释 起 源代码 用户 可以 用 它 
研究 Prolog 的 内部 机制 并 创建 自己 的 专用 
编程语言 推理机 专家系统 外壳 或 程序接口 • 提供 了 与 
其他 语言 如 C Pascal Fortran 等 的 接口 Prolog 
和 其他 语言 可以 相互 调用 对方 的 子程序 • 
具有 强大 的 图形 功能 支持 Turbo C Turbo Pascal 
同样 的 功能 3 Visual PrologVisual Prolog 是 基于 Prolog 
语言 的 可视化 集成 开发 环境 是 PDC 推出 的 
基于 Windows 环境 的 智能化 编程 工具 目前 Visual Prolog 
在 美国 西欧 日本 加拿大 澳大利亚 等 国家 和 地区 
十分 流行 是 国际 上 研究 和 开发 智能化 应用 
的 主流 工具 之一 Visual Prolog 具有 模式匹配 递归 回溯 
对象 机制 事实 数据库 和谓/nr 词库 等 强大 功能 它 
包含 构建 大型 应用程序 所 需要 的 一切 特性 图形 
开发环境 编译器 连接器 和 调试器 支持 模块化 和 面向对象 程序设计 
支持系统 级 编程 文件 操作 字符 串处理 位 级 运算 
算术 与 逻辑运算 以及 与 其它 编程语言 的 接口 Visual 
Prolog 包含 一个 全部 使用 Visual Prolog 语言 写成 的 
有效 的 开发 环境 包含 对话框 菜单 工具栏 等 编辑 
功能 Visual Prolog 与 SQL 数据库系统 C + + 开发 
系统 以及 Visual Basic Delphi 或 Visual Age 等 编程语言 
一样 也 可以 用来 轻松 地 开发 各种 应用 Visual 
Prolog 软件 的 下载 地址 为 http / / www 
. visual prolog . com 更多 学习 笔记 关注 公众 
号 StudyForAI 知乎 专栏 https / / www . zhihu 
. com / people / yuquanle / columns 自然语言 处理 
NLP 是 人工智能 的 一个 重要 应用 领域 由于 本 
人 主要 研究 方向 为 NLP 也 由于 最近 学习 
的 需要 特意 搜罗 资料 整理 了 一份 简要 的 
NLP 的 基本 任务 和 研究 方向 希望 对 大家 
有 帮助 自然 语言 的 发展 一般 认为 1950 年 
图灵 提出 著名 的 图灵测试 是 自然 语言 处理 思想 
的 开端 20 世纪 50 年代 到 70 年代 自然语言 
处理 主要 采用 基于 规则 的 方法 基于 规则 的 
方法 不 可能 覆盖 所有 语句 且 对 开发者 的 
要求 极高 这时 的 自然 语言 处理 停留在 理性主义 思潮 
阶段 70 年代 以后 随着 互联网 的 高速 发展 语料库 
越来越 丰富 以及 硬件 更新 完善 自然语言 处理 思潮 由 
理性主义 向 经验主义 过渡 基于 统计 的 方法 逐渐 代替 
了 基于 规则 的 方法 从 2008 年到/nr 现在 由于 
深度 学习 在 图像 识别 语音 识别 等 领域 不断 
取得 突破 人们 也 逐渐 开始 引入 深度 学习 来做 
自然语言 处理 研究 由 最初 的 词 向 量到 2013 
年 word2vec 将 深度 学习 与 自然 语言 处理 的 
结合 推向 了 高潮 并且 在 机器 翻译 问答 系统 
阅读 理解 等 领域 取得 了 一定 成功 分割线 先 
来 看看 自然 语言 处理 的 定义 自然 语言 是 
指 汉语 英语 等 人们 日常 使用 的 语言 是 
随着 人类 社会 发展 自然而然 的 演变 而来 的 语言 
不是 人造 的 语言 自然 语言 是 人类 学习 生活 
的 重要 工具 或者说 自然 语言 是 指 人类 社会 
约定俗成 的 区别于 人工 语言 如 程序 设计 的 语言 
处理 包含 理解 转化 生成 等 过程 自然语言 处理 是 
指用 计算机 对 自然 语言 的 形 音 义 等 
信息 进行 处理 即对 字 如果 是 英文 即为 字符 
词 句 段落 篇章 的 输入 输出 识别 分析 理解 
生成 等 的 操作 和 加工 实现 人机 间 的 
信息 交流 是 人工智能 界 计算机科学 和 语言 学界 所 
共同 关注 的 重要 问题 所以 自然 语言 处理 也 
被誉为 人工智能 的 掌上明珠 可以 说 自然语言 处理 就是 要 
计算机 理解 自然语言 自然语言 处理 机制 涉及 两个 流程 包括 
自然 语言 理解 和 自然 语言 生成 自然语言 理解 是 
指 计算机 能够 理解 自然语言 文本 的 意义 自然语言 生成 
则是 指 能以 自然语言 文本 来 表达 给定 的 意图 
自然 语言 的 理解 和 分析 是 一个 层次化 的 
过程 许多 语言学家 把这 一 过程 分为 五 个 层次 
可以 更好 地 体现 语言 本身 的 构成 五个 层次分 
别是 语音 分析 词 法分析 句法分析 语义分析 和 语用分析 语音 
分析 是 要根据 音位 规则 从语/nr 音流 中 区分 出 
一个 个 独立 的 音素 再 根据 音位 形态 规则 
找出 音节 及其 对应 的 词素 或 词 词法 分析 
是 找出 词汇 的 各个 词素 从中 获得 语言学 的 
信息 句法分析 是 对 句子 和 短语 的 结构 进行 
分析 目的 是 要 找出 词 短语 等 的 相互 
关系 以及 各 自在 句中 的 作用 语义分析 是 指 
运用 各种 机器 学习 方法 学习 与 理解 一段 文本 
所 表示 的 语义 内容 语义分析 是 一个 非常 广的/nr 
概念 语用分析 是 研究 语言所 存在 的 外界 环境 对 
语言 使用者 所 产生 的 影响 这里 根据 自己 的 
学习 以及 查阅 相关 资料 的 理解 简要 的 介绍 
一下 自然语言 处理 nlp 一些 相关 技术 以及 相关 任务 
nlp 技术 包括 基础 技术 和 应用 技术 基础 技术 
包括 词 法分析 句法分析 语义分析 等 词 法分析 lexical analysis 
包括 汉语分词 word segmentation 或 tokenization 和 词性 标注 part 
of speech tag 等 汉语分词 处理 汉语 英文 自带 分词 
首要 工作 就是 要 将 输入 的 字串 切分 为 
单独 的 词语 这一 步骤 称为 分词 词性 标注 词性 
标注 的 目的 是 为 每一个 词 赋予 一个 类别 
这个 类别 称为 词性 标记 比如 名词 noun 动词 verb 
等 句法分析 syntactic parsing 是 对 输入 的 文本 句子 
进行 分析 得到 句子 的 句法结构 的 处理 过程 最 
常见 的 句法分析 任务 有 下列 几种 短语 结构 句法分析 
phrase structure syntactic parsing 该 任务 也 被称作 成分 句法分析 
constituent syntactic parsing 作用 是 识别 出 句子 中 的 
短语 结构 以及 短语 之间 的 层次 句法关系 依存 句法分析 
dependency syntactic parsing 作用 是 识别 句子 中 词汇 与 
词汇 之间 的 相互 依存 关系 深层 文法 句法分析 即 
利用 深层 文法 例如 词汇 化 树 邻接 文法 Lexicalized 
Tree Adjoining Grammar LTAG 词汇 功能 文法 Lexical Functional Grammar 
LFG 组合 范畴 文法 Combinatory Categorial Grammar CCG 等 对 
句子 进行 深层 的 句法 以及 语义分析 语义分析 Semantic Analysis 
语义分析 的 最终 目的 是 理解 句子 表达 的 真实 
语义 但是 语义 应该 采用 什么 表示 形式 一直 困扰 
着 研究者 们 至今 这个 问题 也 没有 一个 统一 
的 答案 语义 角色 标注 semantic role labeling 是 目前 
比较 成熟 的 浅层 语义分析 技术 总而言之 自然语言 处理 系统 
通常 采用 级联 的 方式 即 分词 词性 标注 句法分析 
语义分析 分别 训练 模型 在 使用 过程 中 给定 输入 
句子 逐一 使用 各个 模块 进行 分析 最终 得到 所 
有结果 近年来 研究者 们 提出 了 很多 有效 的 联合 
模型 将 多个 任务 联合 学习 和 解码 如 分词 
词性 联合 词性 句法 联合 分词 词性 句法 联合 句法 
语义 联合 等 取得 了 不错 的 效果 特别 值得一提的是 
今年 EMNLP 上 有 一个 联合 模型 的 教程 大家 
可以 从 这里 下载 https / / pan . baidu 
. com / s / 1DxOqXxlK 1BCHqMCwr5 _ ZA 另一方面 
是 自然 语言 处理 的 应用 技术 这些 任务 往往 
会 依赖 基础 技术 包括 文本 聚 类 Text Clustering 
文本 分类 Text Classification 文本 摘要 Text abstract 情感 分析 
sentiment analysis 自动 问答 Question Answering QA 机器翻译 machine translation 
MT 信息 抽取 Information Extraction 信息 推荐 Information Recommendation 信息检索 
Information Retrieval IR 等 因为 每 一个 任务 都 涉及 
的 东西 很多 因此 在 这里 我 知识 简单 总结 
介绍 一下 这些 任务 等 以后 有 时间 随着 我 
的 学习 深入 再分 专题 详细 总结 各种 技术 文本 
分类 文本 分类 任务 是 根据 给定 文档 的 内容 
或 主题 自动 分配 预先 定义 的 类别 标签 文本 
聚 类 任务 则 是 根据 文档 之间 的 内容 
或 主题 相似 度 将 文档 集合 划分 成 若干 
个 子集 每个 子集 内部 的 文档 相似 度 较高 
而 子集 之间 的 相似 度 较低 文本 摘要 文本 
摘要 任务 是 指 通过 对 原 文本 进行 压缩 
提炼 为 用户 提供 简明扼要 的 文字描述 情感 分析 情感 
分析 任务 是 指 利用 计算机 实现 对 文本 数据 
的 观点 情感 态度 情绪 等 的 分析 挖掘 自动 
问答 自动 问答 是 指 利用 计算机 自动 回答 用户 
所 提出 的 问题 以 满足 用户 知识 需求 的 
任务 机器翻译 机器 翻译 是 指 利用 计算机 实现 从 
一种 自然 语言 到 另外 一种 自然 语言 的 自动 
翻译 被 翻译 的 语言 称为 源语言 source language 翻译 
到 的 语言 称作 目标语言 target language 信息 抽取 信息 
抽取 是 指 从非/nr 结构化 / 半 结构化 文本 如 
网页 新闻 论文 文献 微博 等 中 提取 指定 类型 
的 信息 如 实体 属性 关系 事件 商品 记录 等 
并 通过 信息 归并 冗余 消除 和 冲突消解 等 手段 
将 非 结构化 文本 转换 为 结构化 信息 的 一项 
综合 技术 信息 推荐 信息 推荐 据 用户 的 习惯 
偏好 或 兴趣 从 不断 到来 的 大规模 信息 中 
识别 满足用户 兴趣 的 信息 的 过程 信息检索 信息检索 是 
指 将 信息 按 一定 的 方式 加以 组织 并 
通过 信息 查找 满足 用户 的 信息 需求 的 过程 
和 技术 参考 1 . 中文 信息 处理 发展 报告 
2016 图像 处理 是 将 输入 图像 转换 为 输出 
图像 的 过程 人 是 图像 处理 的 效果 的 
最终 解释者 在 计算机 视觉 中 计算机 是 图像 的 
解释者 图像处理 仅仅 是 计算机 视觉 系统 中 的 一个 
模块 计算机 图形学 的 主要 工作 是从 三维 描述 到 
二维 图像 显示 的 过程 计算机 视觉 则 是从 二维 
图像 数据 到 三维 描述 的 过程 计算机 视觉 是 
计算机 图形学 的 逆 问题 模式识别 主要 解决 分类 的 
问题 是 计算机 视觉 中 的 一个 模块 总体 来说 
他们 有 如下 的 关系 不要 把 几个 相关 的 
概念 混为一谈 自然语言 处理 中 主题 模型 的 发展 强烈 
建议 直接 看 论文 看 一些 博客 对于 入门 并 
没有 什么 太大 帮助 1 徐戈/nr 王厚 峰 . 自然语言 
处理 中 主题 模型 的 发展 J . 计算机 学报 
2011 08 1423 1436 . 摘要 主题 词 项的/nr 概率分布 
主题 模型 文档/n 从词项/nr 空间/n 转换/v 到/v 主题/n 空间/n 降 
维 表达 主要 内容 1 . 对 LSI PLSI LDA/w 
等/u 主题/n 模型/n 进行/v 介绍/v 比较/d 2/m ./i LDA/w 派生/v 
模型/n 介绍/v 3/m ./i 对/p EM/w 算法/n 生成/v 主题/n 的/uj 
词/n 项/n 概率分布/n 和/c 文档/n 的/uj 主题/n 概率分布/n 进行/v 分析/vn 
1/m ./i 引言/v 主题/n 可以/c 看作/v 是/v 词/n 项的/nr 概率分布/n 
一篇 文章 使用 bag of words 进行 表示 长度 较长 
映 射到 主题 空间 之后 由于 通常 主题 数 K 
远远 小于 词 项的/nr 数目 因此 可以 通过 主题 模型 
进行 降 维 隐性 语义 索引 LSI latent semantic indexing 
不是 一个 概率模型 Deerwester Scott et al . Indexing by 
latent semantic analysis . Journal of the American society for 
information science 41.6 1990 391 . 概率 隐性 语义 索引 
pLSI 真正 意义 上 的 主题 模型 Hofmann Thomas . 
Probabilistic latent semantic indexing . Proceedings of the 22nd annual 
international ACM SIGIR conference on Research and development in information 
retrieval . ACM 1999 . LDA latent Dirichlet Allocation BleiBlei 
David M . Andrew Y . Ng and Michael I 
. Jordan . Latent dirichlet allocation . Journal of machine 
Learning research 3 . Jan 2003 993 1022 . LSI 
PLSI LDA 各种 LDA2 . 主题 模型 的 主要 内容 
五大 组成部分 输入 模型 假设 表示 参数估计 新 样本 推测 
2.1 主题 模型 的 输入 主题 模型 的 输入 是 
文档 集合 由于 交换 性 的 假设 所以 等价 于 
term document 矩阵 term \ documentdocument 1document 2document 3term 1102term 
2031 另一个 输入 是 主题 数目 K 通常 K 是 
经验 决定 最 简单 的 方法 是 使用 不同 的 
K 重复 实验 评价 指标 困惑 度 语料 似 然 
值 分类 正确率 等 估计 KBlei David M . Andrew 
Y . Ng and Michael I . Jordan . Latent 
dirichlet allocation . Journal of machine Learning research 3 . 
Jan 2003 993 1022 . Griffiths Thomas L . and 
Mark Steyvers . Finding scientific topics . Proceedings of the 
National academy of Sciences 101 . suppl 1 2004 5228 
5235 . Blei David M . Probabilistic topic models . 
Communications of the ACM 55.4 2012 77 84 . CAO 
Juan et al . A method of adaptively selecting best 
LDA model based on density . Chinese Journal of Computer 
31 2008 1780 1787 . 非 参数 贝叶斯 估计 KTeh 
Yee Whye et al . Hierarchical dirichlet processes . Journal 
of the american statistical association 2012 . Shi Jin et 
al . Text segmentation based on model LDA . Chinese 
Journal of Computers 31.10 2008 1865 1873 . 2.2 主题 
模型 中 的 基本 假设 bag of words 假设 即 
文档 内 词 的 顺序 与 模型 结果 无关 但是 
在 LDA 的 派生 模型 中 有的 交换 性 会被 
打破 2.3 主题 模型 的 表示 分别 是 图 模型 
和 生成 模型 注意 其中 有 两个 超 参数 α 
\ alpha 和β\/nr beta 2.4 参数估计 过程 参数估计 的 结果 
是 训练 的 最终 结果 首先 要 选择 优化 的 
目标 函数 通常 就是 整个 语料 的 概率值 以 LDA 
模型 为例 根据 图 模型 可以 比较 容易 得到 概率值 
的 大小 参看 我 的 EM 算法 的 文章 其实 
计算 语料 的 概率 就 是 计算 整体 的 可能 
期望 2.5 新 样本 的 推断 其实 新 样本 的 
推断 就是 将其 映 射到 低 维度 的 主题 空间 
即可 可以 用于 信息检索 中 3 . 期望 最大化 算法 
和 参数估计 参看 我 的 EM 算法 文章 期望 最大化 
算法 对于 隐 变量 通过 概率模型 寻找 极大 似 然 
估计 的 一般 方法 能够 不断 迭代 从而 修改 现有 
模型 的 参数 使用 现有 模型 推断 隐 变量 的 
后验/nr 概率分布 然后 对于 参数 重新 估计 不能 保证 全剧 
最优 解 不过 可以 通过 多次 试验 取得 最好 的 
结果 概率模型 包括 1 . 隐 变量 集合 Z 2 
. 观测 集合 X 3 . 参数 集合 θ \ 
theta 目标 得到 P X | θ \ theta 最大化 
时候 的 θ \ thetaEM 算法 过程 初始化 θ \ 
thetaE 步骤 使用 当前 θ \ theta 对于 P Z 
| X θ \ theta 进行 估计 M 步骤 利用 
前 一步 的 结果 最大化 期望 4 . 隐性 语义 
索引 隐性 语义 索引 主要 包含 奇异 值 分解 SVD 
和主/nr 成分 分析 PCA 4.1 主 成分 分析 4.2 隐性 
语义 索引 隐性 语义 索引 是 通过 奇异 值 分解 
构造 新的 隐性 语义空间 即 SVD 分解 对于 过于 大 
的 矩阵 来说 可以 通过 EM 算法 来 求得 SVD 
分解 的 结果 其实/d SVD/w 的/uj U/w 和V/nr 矩阵/n 都/d 
可以/c 看作/v 是/v 对于/p 两个/m 矩阵/n 做了/i 主/b 成分/n 分析/vn 
这/r 两个/m 矩阵/n 的/uj 特征值/n 和/c 特征向量/n 都/d 可以/c 通过/p 
EM/w 算法/n 计算出来/l 对于 EM 计算 PCA 参见 PRML 缺项 
矩阵 Roweis Sam . EM Algorithms for PCA and SPCA 
. 5 . 概率 隐性 语义 索引 概率 隐性 语义 
索引 也 是从 词 项 空间 到 主题 空间 的 
变换 但是 pLSI 是 一个 概率 生成 模型 而且 选择 
了 不同 的 优化 目标函数 就是 两张 二维 参数表 分别 
是 p w | z 和p/nr z | d 可以 
理解 为 和 LSI 中的 类似 U 对应 p w 
| z V 对应 p d | z 而 中间 
矩阵 对应 着 z 的 概率分布 对应 于 EM 算法 
可以 对应 于本/nr 模型 即 w d 为 观测值 z 
是 隐 变量 p w | z p z | 
d 为 待 估计 的 参数 pLSI 模型 和 LSI 
的 效果 比较 Hofmann Thomas . Probabilistic latent semantic indexing 
. Proceedings of the 22nd annual international ACM SIGIR conference 
on Research and development in information retrieval . ACM 1999 
. Hofmann Thomas . Unsupervised learning by probabilistic latent semantic 
analysis . Machine learning 42.1 2 2001 177 196 . 
和 pLSI 等价 的 NMF 非 负 矩阵 分解 Lee 
Daniel D . and H . Sebastian Seung . Learning 
the parts of objects by non negative matrix factorization . 
Nature 401.6755 1999 788 791 . Lee Daniel D . 
and H . Sebastian Seung . Algorithms for Non negative 
Matrix Factorization . 6 . 从 这里 开始 看 不懂 
了 待 我 补完 prml 的 概率 部分 再 来 
看看 提出 起因 Chomsky 短语 结构 语法 生成 能力 太强 
产生 许多 不 符合 语法 或有 歧义 的 句子 标记 
十分 简单 分析 能力 有限 难以 反映 自然 语言 的 
复杂 特性 FUG 对 短语 结构 语法 的 改进 采用 
复杂 特 征集 来 描述 词 句法 规则 语义 信息 
以及 句子 的 结构 功能 试图 以 单一 形式 的 
结构 模式 来 描述 特征 组合 功能 分配 词条 和 
组成 成分 的 顺序 以 达到 对 句子 的 完全 
功能 描述 采用 合一 运算 对 复杂 特征 集 进行 
运算 复杂 特 征集 1 . 复杂 特 征集 功能 
描述 的 定义 设为 一个 功能 描述 Functional Description 当且仅当 
可以 表示 为 其中 表示 特 征名 表示 特征值 且 
满足 以下 两个 条件 1 特征 名为 原子 特征值 为 
原子 或 另 一个 功能 描述 2 读作 复杂 特征 
集中 特征 的 值 等于 2 . 可以 用 复杂 
特 征集 描述 词汇 在 词典 中 单词 的 特征 
可以 包括 词类 形态 句法 和 语义 等 多方面 的 
信息 如 3 . 可以 用 复杂 特 征集 描述 
规则 4 . 可以 用 复杂 特 征集 描述 句子 
句子 We helped her . 5 . 复杂 特 征集 
的 特点 1 允许 利用 多个 特征描述 同 一个 语言 
单位 2 从 结构 上看 复杂 特 征集 是 一种 
嵌套 结构 可以 有效 地 表示 复杂 词组 或 句 
子结构 3 特 征名 的 定义 及其 相互 关系 具有 
明显 的 层次性 而 所有 自然 语言 的 结构 都是 
层次性 的 复杂 特 征集 的 这一 特点 显然 对 
语言 的 层次 分析 有益 4 复杂 特 征集 便于 
运算 两个 复杂 特 征集 通过 合一 运算 可以 产生 
另 一个 复杂 特 征集 这与 句法分析 中 词组 和 
句子 的 产生 是 一致 的 合一 运算 1 . 
复杂 特 征集 相容 的 定义 若 均为 复杂 特 
征集 则是 相容 的 当且仅当 1 如果 且 都是 原子 
那么 是 相容 的 当且仅当 2 如果 均为 复杂 特 
征集 是 相容 的 当且仅当 相容 2 . 合一 运算 
的 递归 定义 1 在 都是 原子 的 情况 下 
如果 则 否则 2 如果 均为 复杂 特 征集 则 
a 若 但 的 值 未经 定义 则 属于 b 
若 但 的 值 未经 定义 则 属于 c 若 
但 且 与 相容 不相 抵触 则 属于 否则 合一 
运算 的 作用 1 合并 原有 的 特征 信息 构造 
新的 特征 结构 2 检查 特征 的 相容性 和 规则 
执行 的 前提 条件 是否 满足 如果 参与 合一 的 
特征 相 冲突 就 立即 宣布 合一 失败 自然语言 处理 
要 代替 RNN LSTM 的 T r a n s 
f o r m e r T r a n 
s f o r m e r 结构 计算 过程 
Seq2Seq 模型 通常 来讲 里面 是由 RNN GRU LSTM 的 
cell 来 组建 的 但 最近 Google 推出 了 一个 
新 的 架构 Transformer . 这个 模型 解决 了 Seq2Seq 
模型 依赖 之前 结果 无法 并行 的 问题 而且 最终 
的 效果 也 是 非常 棒 原文 图解 Transformer 已经 
这么 详细 的 翻译 了 我 这里 为 自己 总结 
一下 关键点 Transformer 结构 Seq2Seq 模型 是 这个 样子 的 
Transformer 的 宏观 结构 也 是 这样 的 不同 的 
是 内部 的 微观 结构 它 里面 又 多个 Encoder 
或 多个 Decoder 组成 而 每个 Coder 内部 又 拥有 
不同 的 结构 计算 过程 最 关键 的 是 对 
每个 词 向量 每次 在 Self Attention 时 初始化 后续 
反向 传播 更新 三个 矩阵 query / key / value 
利用 query * key 计算 得到 score 并 通过 softmax 
计算 得到 概率 权重 乘以 value 得到 激 活词 向量 
每个 词 向量 分别 进入 独立 的 FFNN 计算 当 
Encoder 计算 完毕 后 再 转化 为 两个 矩阵 key 
/ value 代入 到 Decoder 中 Encoder Decoder Attention 的 
计算 中 最好 的 入门 自然语言 处理 NLP 的 资源 
清单 Melanie Tosik 目前 就职 于 旅游 搜索 公司 WayBlazer 
她 的 工作 内容 是 通过 自然 语言 请求 来 
生产 个性化 旅游 推荐 路线 回顾 她 的 学习 历程 
她 为 期望 入门 自然语言 处理 的 初学者 列出 了 
一份 学习 资源 清单 目录   在线 课程   图书馆 
和 开放 资源   活跃 的 博客     书籍 
    数据集   NLP 之 社交 媒体   其它 
displaCy 网站 上 的 可视化 依赖 解析 树 displaCy 记得 
我 曾经 读到 过 这样 一段 话 如果 你 觉得 
有 必要 回答 两次 同样 的 问题 那 就把 答案 
发到 博客 上 这 可能 是 一个 好 主意 根据 
这 一 原则 也 为了 节省 回答 问题 的 时间 
我 在 这里 给出 该 问题 的 标准 问法 我 
的 背景 是 研究 * * 科学 我 对 学习 
NLP 很 有兴趣 应该 从哪/nr 说起 呢 在 您 一头 
扎 进去 阅读 本文 之前 请注意 下面 列表 只是 提供 
了 非常 通用 的 入门 清单 有 可能 不 完整 
  为了 帮助 读者 更好 地 阅读 我 在 括号 
内 添加 了 简短 的 描述 并对 难度 做了 估计 
最好 具备 基本 的 编程 技能 例如 Python 在线 课程 
1 . Dan Jurafsky 和 /nr Chris Manning 自然语言 处理 非常 
棒 的 视频 介绍 系列 YouTube Dan Jurafsky / Chris 
Manning 自然语言 处理 2 . 斯坦福 CS224d 自然语言 处理 的 
深度 学习 更 高级 的 机器学习 算法 深度 学习 和 
NLP 的 神经 网络 架构 斯坦福 CS224d3 . Coursera 自然语言 
处理 简介 由 密西根 大学 提供 的 NLP 课程 Coursera 
自然语言 处理 简介 图书馆 和 开放 资源 1 . spaCy 
网站 博客 Python 新兴 的 开放 源码库 并 自带 炫 
酷 的 用法 示例 API 文档 和 演示 应用程序 https 
/ / spacy . io / https / / explosion 
. ai / blog / https / / spacy . 
io / docs / usage / showcase2 . 自然语言 工具包 
NLTK 网站 图书 Python NLP 实用 编程 介绍 主要 用于 
教学 目的 http / / www . nltk . orghttp 
/ / www . nltk . org / book / 
3 . 斯坦福 CoreNLP 网站 由 Java 开发 的 高质量 
的 自然 语言 分析 工具包 https / / stanfordnlp . 
github . io / CoreNLP / 活跃 的 博客 1 
. 自然语言 处理 博客 HalDaum é https / / nlpers 
. blogspot . com / 2 . Google 研究 博客 
https / / research . googleblog . com / 3 
. 语言 日志 博客 Mark Liberman http / / languagelog 
. ldc . upenn . edu / nll / 书籍 
1 . 言语 和 语言 处理 Daniel Jurafsky 和 James 
H . Martin 经典 的 NLP 教科书 涵盖 了 所有 
NLP 的 基础 知识 第 3版 即将 出版 https / 
/ web . stanford . edu / ~ jurafsky / 
slp3 / 2 . 统计 自然语言 处理 的 基础 Chris 
Manning 和 HinrichSch ü tze 更 高级 的 统计 NLP 
方法 https / / nlp . stanford . edu / 
fsnlp / 3 . 信息检索 简介 Chris Manning Prabhakar Raghavan 
和 HinrichSch ü tze 关于 排名 / 搜索 的 优秀 
参考书 https / / nlp . stanford . edu / 
IR book / 4 . 自然语言 处理 中 的 神经 
网络 方法 Yoav Goldberg 深入 介绍 NLP 的 NN 方法 
和相/nr 对应/vn 的/uj 入门/ns 书籍/n https / / www . 
amazon . com / Network Methods Natural Language Processing / 
dp / 1627052984http / / u . cs . biu 
. ac . il / ~ yogo / nnlp . 
pdf 数据集 1 . Nicolas Iderhoff 已经 创建 了 一份 
公开 的 详尽 的 NLP 数据集 的 列表 除了 这些 
这里 还有 一些 项目 可以 推荐 给 那些 想 要 
亲自 动手 实践 的 NLP 新手 们 https / / 
github . com / niderhoff / nlp datasets2 . 基于 
隐 马尔可夫 模型 HMM 实现 词性 标注 POS tagging . 
https / / en . wikipedia . org / wiki 
/ Part of speech _ tagginghttps / / en . 
wikipedia . org / wiki / Hidden _ Markov _ 
model3 . 使用 CYK 算法 执行 上下文 无关 的 语法 
解析 https / / en . wikipedia . org / 
wiki / CYK _ algorithmhttps / / en . wikipedia 
. org / wiki / Context free _ grammar4 . 
在 文本 集合 中 计算 给定 两个 单词 之间 的 
语义 相似 度 例如 点 互信息 PMI Pointwise Mutual Information 
https / / en . wikipedia . org / wiki 
/ Semantic _ similarityhttps / / en . wikipedia . 
org / wiki / Pointwise _ mutual _ information5 . 
使用 朴素 贝叶斯 分类器 来 过滤 垃圾邮件 https / / 
en . wikipedia . org / wiki / Naive _ 
Bayes _ classifierhttps / / en . wikipedia . org 
/ wiki / Naive _ Bayes _ spam _ filtering6 
. 根据 单词 之间 的 编辑 距离 执行 拼 写检查 
https / / en . wikipedia . org / wiki 
/ Spell _ checkerhttps / / en . wikipedia . 
org / wiki / Edit _ distance7 . 实现 一个 
马尔科夫 链 文本 生成器 https / / en . wikipedia 
. org / wiki / Markov _ chain8 . 使用 
LDA 实现 主题 模型 https / / en . wikipedia 
. org / wiki / Topic _ modelhttps / / 
en . wikipedia . org / wiki / Latent _ 
Dirichlet _ allocation9 . 使用 word2vec 从 大型 文本 语料库 
例如 维基百科 生成 单词 嵌入 https / / code . 
google . com / archive / p / word2vec / 
https / / en . wikipedia . org / wiki 
/ Wikipedia Database _ downloadNLP 之 社交 媒体 1 . 
Twitter # nlproc NLPers 上 的 文章 列表 由 Jason 
Baldrige 提供 https / / twitter . com / hashtag 
/ nlprochttps / / twitter . com / jasonbaldridge / 
lists / nlpers2 . Reddit   社交 新闻 站点 / 
r / L a n g u a g e 
T e c h n o l o g y 
h t t p s / / www . reddit 
. com / r / L a n g u 
a g e T e c h n o l 
o g y 3 . Medium 发布 平台 Nlphttps / 
/ medium . com / tag / nlp 其它 1 
. 如何 在 TensorFlow 中 构建 word2vec 模型 学习指南 https 
/ / www . tensorflow . org / versions / 
master / tutorials / word2vec / index . html2 . 
NLP 深度 学习 的 资源 按 主题 分类 的 关于 
深度 学习 的 顶尖 资源 的 概述 https / / 
github . com / andrewt3000 / dl4nlp3 . 最后 一句话 
计算 语言学 和 深度 学习 论 自然语言 处理 的 重要性 
Chris Manning 文章 http / / mitp . nautil . 
us / article / 170 / last words computational linguistics 
and deep learning4 . 对 分布式 表征 的 自然 语言 
的 理解 Kyunghyun Cho 关于 NLU 的 ML / NN 
方法 的 独立 讲义 https / / github . com 
/ nyu dl / NLP _ DL _ Lecture _ 
Note / blob / master / lecture _ note . 
pdf5 . 带 泪水 的 贝叶斯 推论 Kevin Knight 教程 
工作簿 http / / www . isi . edu / 
natural language / people / bayes with tears . pdf6 
. 国际 计算 语言学 协会 ACL 期刊 选集 http / 
/ aclanthology . info / 7 . 果壳 问答 网站 
Quora 我 是 如何 学习 自然语言 处理 的 https / 
/ www . quora . com / How do I 
learn Natural Language Processing 人工智能 人工智能 Artificial Intelligence 即 AI 
指 的 是 研究 开发 用于 模拟 延伸 和 扩展 
人 的 智能 的 理论 方法 技术 及 应用 系统 
的 一门 新的 技术 科学 这是 计算机 科学 的 一个 
分支 它 企图 了解 智能 的 实质 并 生产 出 
一种 新的 能以 人类 智能 相似 的 方式 做出 反应 
的 智能 机器 该 领域 的 研究 包括 机器人 语言识别 
图像识别 自然语言 处理 和 专家 系统 等 人工智能 一 词 
最初 是 在 1956 年 Dartmouth 学会 上 提出 的 
从那以后 研究者 们 发展 了 众多 理论 和 原理 人工智能 
的 概念 也 随之 扩展 AI 是否 会 取代 人类 
关于 这个 问题 许多 学者 都 表达 出 了 他们 
的 忧虑 早在 1993年 计算机 科学家 Vernon Vinge 推广 了 
技术 新 概念 AI 驱动 的 计算机 或 机器人 能 
重新 设计 并 改进 自身 或者 能 设计 比 自身 
更 先进 的 AI 这个 概念 让 很多 人 认为 
这将 导致 AI 超出 人类 智慧 理解 和 控制 从而 
导致 人类 时代 的 终结 近来 许多 权威 科学家 包括 
Stuart Russell Max Tegmark 和 Frank Wilczek 都曾 警告 过 
AI 太过 聪明 的 潜在 后果 就是 机器 将 摆脱 
人类 的 控制 起身 反抗 同治 甚至 是 消灭 人类 
而 权威 物理学家 与 宇宙学家 史蒂芬 霍金 也曾 说过 人工智能 
的 崛起 可能 是 人类 文明 的 终结 出自 霍金 
的 主题 演讲 让 人工智能 造福人类 及其 赖以生存 的 家园 
霍金 表示 人工智能 的 威胁 分 短期 和 长期 两种 
短期 威胁 包括 自动 驾驶 智能性 自主 武器 以及 隐私 
问题 长期 担忧 主要 是 人工 智能系统 失控 带来 的 
风险 如 人工智能 系统 可能 不听 人类 指挥 终结者 就是 
这种 危险 的 放大 不过 对于 这个 话题 也 有 
一些 人 持 不同 的 意见 微软 Azure 公司 副总裁 
茱莉亚 怀特 就 曾在 悉尼 举办 的 微软 峰会 上 
说过 人工智能 将 取代 人类 的 说法 忽略 了 一个 
前提 那 就是 人类 是 不断学习 成长 改变 自己 以 
适应 环境 的 怀特 认为 人 是 智慧 的 生物 
能够 在 不断 学习 的 过程 中 利用 人工智能 控制 
人工智能 而 不是 被 机器 取代 而 英国 物理学家 罗杰 
彭罗斯 甚至 认为 作为 一种 算法 确定性 的 系统 当前 
的 电子 计算机 无法 产生 智能 对于 这个 问题 不同 
的 人 有 不同 的 观点 但 唯一 不变 的 
一点 就是 Vernon Vinge 提出 的 观点 在 未来 某天 
终会 到来 人工智能 终会 实现 真正 意义 上 的 智能 
国内 第一 起 机器人 伤人 事件 2016年 11月 18日 网络 
盛传 深圳 高交会 上 发生 全国 首例 机器人 伤人 事件 
名叫 小胖 的 机器 人 突发 故障 杀伤力 爆棚 在 
没有 指令 的 情况 下 自行 打砸 展台 玻璃 砸伤 
路人 一位 路人 全身 多处 划伤 后被 担架 抬走 这 
让 许多 人 认为 机器人 自我 意识 觉醒 对抗 人类 
的 危险 越来越 可能 了 但是 18日 下午 高交会 组委会 
在 其 官网 发布 公告 称 事故 是 由于 工作 
人员 操作 反了 方向键 所致 公告 声明 11月 17日 13时 
30分 左右 在 1号 展馆 D 32号 展位 小胖 的 
投影机 供应商 旁通道 内 参展 人员 试图 将 一台 面罩 
打开 的 小胖 机器人 移动 到 展位 内 时 误将 
后退 键 按 成 前进 键 并未 及时 按 停 
导致 另一侧 展台 的 玻璃 被 部分 碰倒 摔碎 玻璃 
划伤 了 另一 侧 展台 内 的 一名 观众 全 
过程 持续 约 10余 秒 虽然 最终 并非 机器人 自我 
意识 觉醒 但 许多 人 依然 强调 人工智能 不能 脱离 
伦理道德 这 一点 并非 仅仅 针对 AI 还 针对 AI 
工作者 AI 工作者 应该 富有 责任感 用 伦理 道德 约束 
自己 在 可能 的 情况 下 用 伦理 道德 约束 
AI . △ 今日 头条 创始人 CEO 张一鸣 作为 人工智能 
的 企业 应该 永远 恪守 一条 原则 必须 对 整个 
人类 的 未来 充满 责任感 充满 善意 资料 皆 来自 
网络 检索 这段 时间 一直 在 接触 学习 hadoop 方面 
的 知识 所以 说 对 自然 语言 处理 技术 也是 
做 了 一些 了解 网络 上 关于 自然 语言 处理 
技术 的 分享 文章 很多 今天 就 给 大家 分享 
一下 HanLP 方面 的 内容 自然语言 处理 技术 其实 是 
所有 与 自然 语言 的 计算机 处理 相 关联 的 
技术 的 统称 自然语言 处理 技术 应用 的 目的 是 
为了 能够 让 计算机 理解 和 接收 我们 用 自然 
语言 输入 的 指令 实现/v 从将/nr 我们/r 人类/n 的/uj 语言/n 
翻译/v 成/n 计算机/n 能够/v 理解/v 的/uj 并且/c 不/d 会/v 产生/n 
歧义/n 的/uj 一种/m 语言/n 接合 目前 的 大 数据 以及 
人工智能 自然语言 处理 技术 的 快速 发展 能够 很好 的 
助力 人工智能 的 发展 大 快 DKhadoop 技术 架构图 这里 
要 分享 的 HanLP 是 我 在 学习 使用 大 
快 DKhadoop 大 数据 一体化 平台 时 使用 到 的 
自然 语言 处理 技术 使用 这个 组建 可以 很 高效 
的 进行 自然 语言 的 处理 工作 比如 进行 文章 
摘要 语义 判别 以及 提高 内容 检索 的 精确度 和 
有效性 等 本想 找个 通俗 的 案例 来 介绍 一下 
HanLP 一 时间 也 没想到 什么 好 的 案例 索性 
就从 HanLp 数据结构 HE 分词 简单 介绍 下 吧 首先 
我们 来看 了 解下 HanLP 的 数据结构 二分 tire 树 
Tire 树 是 一种 前缀 压缩 结构 可以 压缩 存 
大量 字符串 并 提供 速度 高于 Map 的 get 操作 
HanLP 中的 trie 树 采用 有序 数组 储存 子 节点 
通过 二分 搜索算法 检索 可以 提供 比 TreeMap 更快 的 
查询 速度 不同于 父 节点 储存 子 节点 引用 的 
普通 trie 树 双/n 数组/n trie/w 树/v 将/d 节点/n 的/uj 
从属关系/l 转化/v 为/p 字符/n 内码/n 的/uj 加法/v 与/p 校验/n 操作/v 
对于/p 一个/m 接收/v 字符/n c/w 从/p 状态/n s/w 移动/vn 到/v 
t/w 的/uj 转移/v 需 满足 条件 是 base s + 
c = tcheck t = s 比如 base 一号 + 
店 = 一号店 check 一号店 = 一号 相较 于 trie 
树 的 前缀 压缩 success 表 AC 自动机 还 实现 
了 后缀 压缩 output 表 在 匹配 失败 时 AC 
自动机 会跳 转到 最 可能 成功 的 状态 fail 指针 
关于 HanLP 分词 1 词典 分词 基于 双 数组 trie 
树 或 ACDAT 的 词典 最长 分词 即从 词典 中 
找出 所有 可能 的 词 顺序 选择 最长 的 词语 
输出 HanLP / 名词 是不是 / null 特别 / 副词 
方便 / 形容词 / null 2 NGram 分词 统计 语料库 
中的 BiGram 根据 转移 概率 选出 最 可能 的 句子 
达到 排除 歧义 的 目的 3 HMM2 分词 这 是 
一种 由 字 构词 的 生成式 模型 由 二阶 隐 
马 模型 提供 序列 标注 被 称为 TnT Tagger 特点 
是 利用 低阶 事件 平滑 高阶 事件 弥补 高阶 模型 
的 数据 稀疏 问题 4 CRF 分词 这 是 一种 
由 字 构词 的 生成式 模型 由 CRF 提供 序列 
标注 相较 于 HMM CRF 的 优点 是 能够 利用 
更多 特征 对 OOV 分词 效果 好 缺点 是 占 
内存 大 解码 慢 之前 看到 的 一篇 文章 不 
知道 原文 在哪 如有 侵权 请 告知 1.11对 人工智能 常见 
的 误解 有 哪些 ADA 人工智能 就是 机器学习 B 机器学习 
只是 人工智能 中 的 一个 方向 C 人工智能 最近 十年 
受到 深度 学习 的 驱动 较多 D 人工智能 就是 深度 
学习 2 哲学 思维 对于 人工智能 的 重要 性 表现 
在 哲学 所 强调 的 批判性 思维 有助于 认清 人工智能 
发展中 的 问题 对 3 深度 学习 在 人工智能 领域 
的 表现 并不 突出 X 1.21 计算机 之父 是 CA 
约翰 麦卡锡 B 艾伦 图灵 C 冯▪/nr 诺依曼 D 马文 
明斯基 2 人工智能 与 计算机 学科 的 关系 是 CA 
计算机 学科 的 主要 驱动力 是 人工智能 研究 B 计算机 
是 人工智能 研究 的 一个 领域 C 人工智能 是 计算机 
学科 的 一个 分支 D 人工智能 与 计算机 学科 没有 
联系 3 人工智能 作为 一门 学科 的 建立 时间 是 
AA 1956年 B 1930年 C 1960年 D 1952年 4 下列 
哪些 选项 是 符号 AI 的 技术 路线 ADA 通用 
问题 求解 器 B 深度 学习 C 机器学习 D 贝叶斯 
网络 5 符号 AI 是 将 人 的 思维 通过 
逻辑 语言 制成 流 形图 让 计算机 去 执行 对 
6 通用 问题 求解 器 需要 寻找 全局 最优 解 
X7 符号 AI 无法 面对 人类 经验 的 变动性 对 
1.31 是 现在 新 出现 的 人工智能 的 研究 方向 
DA 深度 学习 B 人工 神经元网络 C 贝叶斯 网络 D 
类 脑 人工智能 2 深度 学习 中的 深度 是 指 
BA 计算机 理解 的 深度 B 中间 神经元网络 的 层次 
很多 C 计算机 的 求解 更加 精准 D 计算机 对 
问题 的 处理 更加 灵活 3 人工 神经元网络 与 深度 
学习 的 关系 是 ACA 人工 神经元网络 是 深度 学习 
的 前身 B 深度 学习 是 人工 神经元网络 的 一个 
分支 C 深度 学习 是 人工 神经元网络 的 一个 发展 
D 深度 学习 与 人工 神经元网络 无关 4 人工 神经元网络 
的 运作 可以 粗略 分为 三个 层面 ACDA 输入 层 
B 映射 机制 C 中间 处理 层 D 输出 层 
5 符号 AI 不是 人工智能 的 正统 X6 人工 神经元网络 
是 对 人类 的 神经元 运作 进行 一种 非常 粗糙 
的 数学模拟 对/p 7/m 相比/v 于/p 人工/n 神经元网络/n 和/c 深度/ns 
学习/v 类 脑 人工智能 对 人类 大脑 的 神经 回路 
具有 更 深入 的 了解 对 1.41 深度 学习 的 
实质 是 BA 推理 机制 B 映射 机制 C 识别 
机制 D 模拟 机制 2 符号 AI 的 问题 在于 
BCDA 缺少 推理 必要 的 信息 B 把 推理 所 
依赖 的 公理系统 全部 锁死 C 缺少 推理 的 灵活性 
D 会 遭遇 框架 问题 3 推理 的 本质 是 
在 信息 不足 的 情况 下 能够 最大 程度 的 
得到 最 靠谱 的 结论 对 4 计算机 具有 触类旁通 
的 能力 可以 根据 具体 语境 对 事件 进行 分类 
X5 人工 神经元网络 会 遭遇 框架 问题 X 1.51 日本 
五代 计算机 泡沫 关注 的 核心 问题 是 DA 人工 
神经元网络 B 符号 AIC 贝叶斯 网络 D 自然语言 处理 2 
制造 人工智能 的 规划 计划/n 和/c 方案/n 本身/r 应该/v 能/v 
根据/p 情况/n 的/uj 变化/vn 进行/v 自我/r 调整/vn 正确 2.11 目前 
对 人工智能 的 发展 所 持有 的 观点 有 ACDA 
乌托邦 论 B 模块 论 C 末世论 D 泡沫 论 
2 现在 的 人工智能 系统 都是/nr 专用 人工智能 而非 通用 
人工智能 正确 2.21 一个 真正 的 通用 人工智能 系统 应 
具备 处理 问题 的 能力 AA 全局性 B 局部性 C 
专业性 D 统一性 2 目前 的 人工智能 研发 的 动力 
主要 来源于 BA 科学 B 商业 C 学术 D 军事 
3 现有 的 人工 神经元网络 或 深度 学习 无法 处理 
全局性 问题 正确 4 人工 神经元网络 只需要 很少 的 数据 
便可 掌握 处理 特定 问题 的 能力 错误 2.31 能够 
推进 人工智能 智能 的 研究 最好 方法 是 CA 继续 
完善 深度 学习 B 提升 计算机 处理 数据 的 能力 
C 研究 人类 自己 的 智能 D 研发 通用 人工智能 
2 下列 哪些 选项 属于 通用 智力因素 ABCDA 短期 记忆 
B 流体 智力 C 晶体 智力 D 反应速度 3类 脑 
人工智能 是 指 模拟 人类 大脑 的 人工智能 错误 4 
人类 自己 的 智能 体现 了 通用性 正确 2.41 以下 
哪些 选项 属于 自然 智能 ABCA 植物 B 动物 C 
细菌 D 机器 2 智能 的 特点 是 ACA 能对 
环境 进行 灵活 的 应对 B 能够 不断 创新 C 
具有 十分 牢固 的 记忆力 D 经济 高效 3 智能 
与 神经元网络 的 存在 具有 必然 关系 错误 4类 脑 
人工智能 及 人工 神经元网络 只是 智能 展现 的 一种 形式 
正确 2.51 提出 强 人工智能 与 弱 人工智能 的 人 
是 AA 约翰 塞尔 B 彼得 卡鲁 瑟斯 C 杰瑞 
佛 多 D 埃隆 马斯克 2 通用 人工智能 就是 强 
人工智能 错误 3.11 深度 学习 的 数据 材料 来源于 DA 
人工 搜集 B 已有 数据库 C 抽样调查 D 互联网 2大 
数据 技术 的 样本 空间 是 CA 针对 所有 相关 
数据 B 需要 确立 样本 范围 C 不做 样本 控制 
D 以上 都不对 3 统计学 研究 首先 要 确立 样本空间 
进行 合理 抽样 然后 估测 出 相关 的 情况 正确 
4 当前 的 主流 人工智能 是 通向 真正 的 通用 
人工智能 的 康庄大道 错误 ~ 2 微软 亚洲 研究院 副院长 
周明 老师 关于 NLP 的 7个 重要 领域 的 详细 
解释 自然语言 处理 的 重要 应用 领域 之一 机器翻译 图中 
对比 了 3种 翻译 方式 的 效果 红色 是 基于 
短语 天蓝色 是 基于 神经网络 黄色 是 人类 水平 可以 
看到 英文 和 西班牙语 / 法语 的 双向 翻译 通过 
应用 神经网络 效果 已经 是 比较 完美 了 perfect 接近 
于 人类 水平 但 英文 和 中文 的 翻译 效果 
还 比较 普通 介绍 了 机器 翻译 的 2 种方法 
SMT NMT 并 引出 了 注意力 机制 用 图像 领域 
的 一张 非常 经典 的 图 解释 注意力 机制 左侧 
问题 是 外套 的 颜色 是 什么 传统 的 VQA 
方法 会给 出 错误 答案 棕色 因为 图中 大 部分 
区域 是 砖块 而 基于 注意力 机制 的 VQA 方法 
会 先 找到 外套 然后再 给出 回答 黄色 类似 的 
右侧 问题 是 雨伞 的 颜色 是 什么 基于 注意力 
的 VQA 方法 也会 先 找到 雨伞 然后 回答 红色 
而 不是 之前 方法 的 错误 判断 绿色 因为 雨伞 
背后 更大 区域 是 绿色 语言 生成 的 6个 应用 
方向 语言 生成 对话 的 4个 难题 知乎 上 的 
一个 问题 百度 NLP 部门 怎么样 有 匿名 用户 回答 
NLP 是 现在 人工智能 的 瓶颈 有志于 从事 NLP 工作 
的 同学 要 明白 这 一点 下面 还有 同学 问 
能 不能 多 解释一下 其实 匿名 用户 的 意思 是 
如果 想在 NLP 领域 做出 真正 有 突破性 的 成就 
是 非常 难 的 如果 不能 忍受 长时间 的 煎熬 
还 不如 去 其他 相对 更 容易 做 出 成果 
的 AI 领域 以上 内容 来自 饭团 AI 产品 经理 
大本营 点击 这里 可 关注 http / / fantuan . 
guokr . net / groups / 219 / 如果 遇到 
支付 问题 请先 关注 饭团 的 官方 微信 服务 号 
fantuan app 作者 黄钊/nr hanniman 图灵 机器人 人才 战略 官 
前 腾讯 产品 经理 5年 AI 实战经验 8年 互联网 背景 
微信 公众 号 / 知乎 / 在行 ID hanniman 饭团 
AI 产品 经理 大本营 分享 人工智能 相关 原创 干货 200页 
PPT 人工智能 产品 经理 的 新起点 被 业内 广泛 好评 
下载量 1万 + 自然语言 处理 是 计算机 科学 领域 与 
人工智能 领域 中 的 一个 重要 方向 它/r 研究/vn 能/v 
实现/v 人/n 与/p 计算机/n 之间/f 用/p 自然/d 语言/n 进行/v 有效/a 
通信/l 的/uj 各种/r 理论/n 和/c 方法/n 自然语言 处理 是 一门 
融 语言学 计算机科学 数学 于 一体 的 科学 因此 这一 
领域 的 研究 将 涉及 自然语言 即 人们 日常 使用 
的 语言 所以 它 与 语言学 的 研究 有着 密切 
的 联系 但又 有 重要 的 区别 自然语言 处理 并 
不是 一般 地 研究 自然语言 而在于 研制 能 有效 地 
实现 自然 语言 通信 的 计算机 系统 特别 是 其中 
的 软件 系统 因而 它 是 计算机 科学 的 一部分 
用 自然 语言 与 计算机 进行通信 这 是 人们 长期以来 
所 追求 的 因为 它 既有 明显 的 实际 意义 
同时 也 有 重要 的 理论 意义 人们 可以 用 
自己 最 习惯 的 语言 来 使用 计算机 而/c 无需/v 
再/d 花/v 大量/n 的/uj 时间/n 和/c 精力/n 去/v 学习/v 不/d 
很/zg 自然/d 和/c 习惯/n 的/uj 各种/r 计算机/n 语言/n 人们 也 
可 通过 它 进一步 了解 人类 的 语言 能力 和 
智能 的 机制 实现 人机 间 自然语言 通信 意味着 要使 
计算机 既 能理解 自然语言 文本 的 意义 也能 以 自然 
语言 文本 来 表达 给定 的 意图 思想 等 前者 
称为 自然语言 理解 后者 称为 自然语言 生成 因此 自然语言 处理 
大体 包括 了 自然 语言 理解 和 自然 语言 生成 
两个 部分 历史 上 对 自然 语言 理解 研究 得 
较多 而对 自然语言 生成 研究 得 较少 但 这种 状况 
已 有所 改变 通信 工程 的 学生 主要 学习 通信 
系统 和 通信网 方面 的 基础理论 组成 原理 和 设计 
方法 受到 通信 工程 实践 的 基本 训练 具备 从事 
现代 通信 系统 和 网络 的 设计 开发 调测 和 
工程 应用 的 基本 能力 自然语言 处理 研究 的 内容 
包括 但 不限 于 如下 分支 领域 文本 分类 信息 
抽取 自动 摘要 智能 问答 话题 推荐 机器翻译 主题词 识别 
知识库 构建 深度 文本 表示 命名 实体 识别 文本 生成 
文本 分析 词法 句法 语法 语音 识别 与 合成 等 
随着 科技 的 发展 自然 语言 会 在 通信 工程 
得到 更多 的 应用 面向 自然语言 处理 的 深度 学习 
作者 印 帕 拉什 戈 雅尔 Palash Goyal 苏 米特 
潘迪 出版 时间 2019 02 18 出版社 机械 工业 出版社 
最近 几个月 小 编 遨游 在 税务 行业 的 智能 
问答 调研 和 开发 中 里面 涉及 到 了 很多 
的 自然 语言 处理 NLP 的 功能 点 虽然 接触 
NLP 也有 近两年 的 时间 了 现在 真正 要 应用到 
问 答中 避 免不了 还是 需要 再 重新 熟识 并 
深入 研究 理解 下面 是 与 NLP 相关 的 一些 
书籍 推荐 课件 推荐 和 开源 工具 推荐 主要 是 
记录 下 入门 的 资料 由于 资料 的 存储 位置 
没有 做 规整 所以 本文 没有 附带 资源 下载 链接 
如果 有 同学 需要 其中 的 资源 可以 在 公众 
号 上 给 我 留言 回头 我 把 资源 链接 
反馈 给 您 部分 开源 工具 和 语料 资源 1 
NLTK 官方 提供 的 语料库 资源 列表 2 OpenNLP 上 
的 开源 自然语言 处理 工具 列表 3 斯坦福 大学 自然 
语言 处理 组 维护 的 统计 自然语言 处理 及 基于 
语料库 的 计算 语言学 资源 列表 4 LDC 上 免费 
的 中文 信息 处理 资源 课件 1 哈工大 刘挺/nr 老师 
的 统计 自然语言 处理 课件 2 哈工大 刘秉 权 老师 
的 自然语言 处理 课件 3 中科院计算所 刘群 老师 的 计算 
语言学 讲义 课件 4 中科院 自动化所 宗 成庆 老师 的 
自然语言 理解 课件 5 北大 常 宝宝 老师 的 计算 
语言学 课件 6 北大 詹 卫东 老师 的 中文信息处理 基础 
的 课件 及 相关 代码 7 MIT 大牛 Michael Collins 
的 Machine Learning Approaches for Natural Language Processing 面向 自然语言 
处理 的 机器 学习 方法 课件 8 Michael Collins 的 
Machine Learning 机器学习 课件 9 SMT 牛人 Philipp Koehn Advanced 
Natural Language Processing 高级 自然语言 处理 课件 10 Philipp Koehn 
Empirical Methods in Natural Language Processing 课件 11 Philipp Koehn 
Machine Translation 机器翻译 课件 书籍 1 自然语言 处理 综论 英文版 
第二 版 2 统计 自然语言 处理 基础 英文版 3 用 
Python 进行 自然语言 处理 NLTK 配 套书 4 Learning Python 
第三版 Python 入门 经典 书籍 详细 而 不厌其烦 5 自然语言 
处理 中的 模式识别 6 EM 算法 及其 扩展 7 统计 
学习 基础 8 自然语言 理解 英文版 似乎 只有 前 9 
章 9 Fundamentals of Speech Recognition 质量 不太好 不过 第 
6 章 关于 HMM 的 部分 比较 详细 作者 之一 
便是 Lawrence Rabiner 10 概率 统计 经典 入门书 概率论 及其 
应用 英文版 威廉 * 费勒 著 第一卷 第二卷 DjVuLibre 阅读器 
阅读 前 两卷 书 需要 11 一本 利用 Perl 和 
Prolog 进行 自然语言 处理 的 介绍 书籍 An Introduction to 
Language Processing with Perl and Prolog 12 国外 机器学习 书籍 
之 1 Programming Collective Intelligence 中文 译名 集体 智慧 编程 
机器学习 & 数据挖掘 领域 近年 出 的 入门 好书 培养 
兴趣 是 最重要 的 一环 一上 来看 大部头 很容易 被 
吓走 的 2 Machine Learning 机器学习 领域 无可争议 的 经典 
书籍 下载 完毕 将 后缀 改为 pdf 即可 豆瓣 评论 
by 王宁 老 书 牛人 现在看来 内容 并 不算 深 
很多 章节 有 点到为止 的 感觉 但是 很 适合 新手 
当然 不能 新 到/v 连/nr 算法/n 和/c 概率/n 都不/nr 知道/v 
入门 比如 决策树 部分 就 很 精彩 并且 这 几年 
没有 特别 大 的 进展 所以 并不 过时 另外 这本书 
算是 对 97 年前 数十年 机器学习 工作 的 大 综述 
参考文献 列表 极 有价值 国内/s 有/v 翻译/v 和/c 影印版/n 不 
知道 绝版 否 3 Introduction to Machine Learning 13 国外 
数据挖掘 书籍 之 1 Data . Mining . Concepts . 
and . Techniques . 2nd 数据挖掘 经典 书籍 华裔 科学家 
写 的 书 相当 深入浅出 2 Data Mining Practical Machine 
Learning Tools and Techniques3 Beautiful Data The Stories Behind Elegant 
Data Solutions Toby Segaran Jeff Hammerbacher 14 国外 模式识别 书籍 
之 1 Pattern Recognition 2 Pattern Recongnition Technologies and Applications 
3 An Introduction to Pattern Recognition 4 Introduction to Statistical 
Pattern Recognition 5 Statistical Pattern Recognition 2nd Edition 6 Supervised 
and Unsupervised Pattern Recognition 7 Support Vector Machines for Pattern 
Classification 15 国外 人工智能 书籍 之 1 Artificial Intelligence A 
Modern Approach 2nd Edition 人工智能 领域 无 争议 的 经典 
2 Paradigms of Artificial Intelligence Programming Case Studies in Common 
LISP 16 其他 相关 书籍 1 Programming the Semantic Web 
Toby Segaran Colin Evans Jamie Taylor2 Learning . Python 第四版 
英文 任何 梦想家 都 不足以 成事 因为 所有 的 成功者 
都是 实干家 浪潮 之 巅 欢迎 转发 到 朋友圈 或 
分享 给 好友 NLP 自然语言 处理 入门 1 . 书籍 
理论 篇 吴军 老师 的 的 数学 之美 统计 自然语言 
处理 第 2版 宗 成庆 蓝皮 版 统计 学习 方法 
李航 自然语言 处理 简明 教程 冯志伟 自然语言 处理 综论 Daniel 
Jurafsky 自然语言 处理 的 形式 模型 冯志伟 2 . 书籍 
实践篇 python 基础教程 翻 译版 + python 入门 博客 推荐 
廖 雪峰 的 python 教程 机器学习 实战 哈林顿 Peter Harrington 
西瓜 书 机器学习 周志华 集体 智慧 编程 美   西 
格兰   著 莫映/nr 王 开福   译 python 自然语言 
处理 伯德 Steven Bird 主要 讲 NLTK 这个 包的/nr 使用 
3 . 视频 辅助 篇 自然语言 处理 宗庆成/nr 自然/d 语言/n 
处理/v 关毅 计算 语言学 概论 _ 侯敏 计算 语言学 _ 
冯志伟 语法分析 _ 陆俭明 哥伦比亚大学 https / / class . 
coursera . org / nlan + 他人 的 博客 自然语言 
处理 大 菜鸟 mooc 学院 机器学习 大牛 Andrew Ng 网易 
公开课 机器学习 Andrew Ng 慕课网/nr 初识 机器学习 台湾 大 学林 
轩 田 机器学习 斯坦福 的 nlp 课程 Video Listing4 . 
优秀 参考 博客 我 爱 自然 语言 处理 专门 记录 
nlp 的 北京大学 中文系 应用 语言学 专业 5 . 国际 
学术 组织 学术 会议 与 学术 论文 国际 机器学习 会议 
ICML ACL URL http / / aclweb . org / 
国际 神经 信息处理系统 会议 NIPS 国际 学习理论 会议 COLT 欧洲 
机器学习 会议 ECML 亚洲 机器学习 会议 ACML EMNLP http / 
/ emnlp2017 . net /   丹麦 哥本哈根 9.7 9 
. 11CCKS   http / / www . ccks2017 . 
com / index . php / att /   成都 
8月 26 8月 29SMP   http / / www . 
cips smp . org / smp2017 /   北京 9.14 
9 . 17CCL   http / / www . cips 
cl . org 8080 / CCL2017 / home . html 
  南京 10.13 10 . 15NLPCC   http / / 
tcci . ccf . org . cn / conference / 
2017 /   大连 11.8 11 . 12NCMMSC   http 
/ / www . ncmmsc2017 . org / index . 
html   连云港 11.11 － 11.136 . 知名 国际 学术 
期刊 Journal of Machine Learning R e s e a 
r c h C o m p u t a 
t i o n a l Linguistics URL http / 
/ www . mitpressjournals . org / loi / coli 
TACL URL http / / www . transacl . org 
/ Machine L e a r n i n g 
I J C A I A A A I A 
r t i f i c i a l I 
n t e l l i g e n c 
e J o u r n a l of Artificial 
Intelligence Research7 . 工具包 推荐 中文 的 显然 是 哈工大 
开源 的 那个 工具包 LTP Language Technology Platform developed by 
HIT SCIR 哈尔滨 工业 大学 社会 计算 与 信息检索 研究中心 
. 英文 的 python pattern   simpler to get started 
than NLTKchardet   character encoding d e t e c 
t i o n p y e n c h 
a n t   easy access to d i c 
t i o n a r i e s s 
c i k i t learn   has support for 
text c l a s s i f i c 
a t i o n u n i d e 
c o d e   because ascii is much easier 
to deal with8 . Quora 上 推荐 的 NLP 的 
论文 Parsing 句法结构 分析 ~ 语言学 知识 多 会 比较 
枯燥 Klein & Manning Accurate Unlexicalized Parsing 克莱因 与 曼宁 
精 确非 词汇 化 句法分析 Klein & Manning Corpus Based 
Induction of Syntactic Structure Models of Dependency and Constituency 革命性 
的 用 非 监督 学习 的 方法 做了 parser Nivre 
Deterministic Dependency Parsing of English Text shows that deterministic parsing 
actually works quite well McDonald et al . Non Projective 
Dependency Parsing using Spanning Tree Algorithms the other main method 
of dependency parsing MST parsing Machine Translation 机器翻译 如果 不 
做 机器翻译 就 可以 跳过 了 不过 翻译 模型 在 
其他 领域 也 有 应用 Knight A statistical MT tutorial 
workbook easy to understand use instead of the original Brown 
paper Och The Alignment Template Approach to Statistical Machine Translation 
foundations of phrase based systems Wu Inversion Transduction Grammars and 
the Bilingual Parsing of Parallel Corpora arguably the first realistic 
method for biparsing which is used in many systems Chiang 
Hierarchical Phrase Based Translation significantly improves accuracy by allowing for 
gappy phrases Language Modeling 语言 模型 Goodman A bit of 
progress in language modeling describes just about everything related to 
n gram language models 这 是 一个 survey 这个 survey 
写了 几乎 所有 和n/nr gram 有关 的 东西 包括 平滑 
聚 类 Teh A Bayesian interpretation of Interpolated Kneser Ney 
shows how to get state of the art accuracy in 
a Bayesian framework opening the path for other applications Machine 
Learning for NLPSutton & McCallum An introduction to conditional random 
fields for relational learning CRF 实在 是 在 NLP 中 
太好 用了 而且/c 我们/r 大家/n 都/d 知道/v 有/v 很多/m 现成/v 
的/uj tool/w 实现/v 这个/r 而 这个 就是 一个 很 简单 
的 论文 讲述 CRF 的 不过 其实 还是 蛮 数学 
= = Knight Bayesian Inference with Tears explains the general 
idea of bayesian techniques quite well Berg Kirkpatrick et al 
. Painless Unsupervised Learning with Features this is from this 
year and thus a bit of a gamble but this 
has the potential to bring the power of discriminative methods 
to unsupervised learning Information ExtractionHearst . Automatic Acquisition of Hyponyms 
from Large Text Corpora . COLING 1992 . The very 
first paper for all the bootstrapping methods for NLP . 
It is a hypothetical work in a sense that it 
doesn t give experimental results but it influenced it s 
followers a lot . Collins and Singer . Unsupervised Models 
for Named Entity Classification . EMNLP 1999 . It applies 
several variants of co training like IE methods to NER 
task and gives the motivation why they did so . 
Students can learn the logic from this work for writing 
a good research paper in NLP . Computational SemanticsGildea and 
Jurafsky . Automatic Labeling of Semantic Roles . Computational Linguistics 
2002 . It opened up the trends in NLP for 
semantic role labeling followed by several CoNLL shared tasks dedicated 
for SRL . It shows how linguistics and engineering can 
collaborate with each other . It has a shorter version 
in ACL 2000 . Pantel and Lin . Discovering Word 
Senses from Text . KDD 2002 . Supervised WSD has 
been explored a lot in the early 00 s thanks 
to the senseval workshop but a few system actually benefits 
from WSD because manually crafted sense mappings are hard to 
obtain . These days we see a lot of evidence 
that unsupervised clustering improves NLP tasks such as NER parsing 
SRL etc 文章 目录 人工智能 的 分类 计算机 视觉 人工智能 
的 分类 计算机 视觉 CV Computer Vision 语音识别 自然语言 处理 
推荐 系统 专家系统 计算机 视觉 一 Word Embedding 概述 简单 
来说 词 嵌入 Word Embedding 或者 分布式 向量 Distributional Vectors 
是 将 自然 语言 表示 的 单词 转换 为 计算机 
能够 理解 的 向量 或 矩阵 形式 的 技术 由于 
要 考虑 多种 因素 比如 词 的 语义 同义词 近义词 
语料 中词 之间 的 关系 上下文 和 向量 的 维度 
处理 复杂度 等等 我们 希望 近义词 或者 表示 同类 事物 
的 单词 之间 的 距离 可以 理想地 近 只有 拿到 
很 理想 的 单词 表示 形式 我们 才 更容易 地 
去做 翻译 问答 信息 抽取 等 进一步 的 工作 在 
Word Embedding 之前 常用 的 方法 有 one hot n 
gram 但是/c 他们/r 都有/nr 各自/r 的/uj 缺点/n 下面 会 说明 
之后 Bengio 提出 了 NLM 是 为 Word Embedding 的 
想法 的 雏形 再 后来 Mikolov 对其 进行 了 优化 
即 Word2vec 包含 了 两种 类型 Continuous Bag of Words 
Model 和 skip gram model 二 Word2vec 之前 2.1 one 
hotone hot 是 最简单 的 一种 处理 方式 通俗 地 
去 讲 把 语料 中的 词汇 去 重 取出 按照 
一定 的 顺序 字典 序 出现 顺序 等 排 列为 
词汇表 则 每 一个 单词 都 可以 表示 为 一个 
长度 为 N 的 向量 N 为 词汇表 长度 即 
单词 总数 该 向量 中 除了 该词 所在 的 分量 
为 1 其余 均 置 为 0 2.2 n gramn 
gram 可以 表示 单词 间 的 位置 关系 所 反映 
的 语义 关联 在 说明 n gram 之前 我们 从 
最初 的 句子 概率 进行 推导 假设 一个 句子 为 
n 个 单词 有序 排列 记为 我们 将 其 简 
记为 则 这个 句子 的 概率 为 对于 单个 概率 
意思为 该 单词 在前面 单词 给定 的 情况 下 出现 
的 概率 我们 利用 贝叶斯 公式 可以 得到 其中 最后 
一项 为 在 语料 中 出现 的 频数 但是/c 长/a 
句子/n 或者/c 经/n 过去/t 标点/n 处理/v 后的/nr 文本/n 可能/v 很长/i 
而且 太 靠前 的 词 对于 词 的 预测 影响 
不是 很大 于是 我们 利用 马尔可夫 假设 取 该词 出现 
的 概率 仅 依赖于 该词 前面 的 n 1个 词 
这 就是 n gram 模型 的 思想 所以 上面 的 
公式 变为 在 这里 我们 不对 n 的 确定 做 
算法 复杂 度上 的 讨论 详细 请 参考文献 1 一般来说 
n 取 3 比较 合适 此外 对于 一些 概率 为 
0 的 情况 所 出现 的 稀疏 数据 采用 平滑 
化 处理 此类 算法 很多 以后 有 时间 再 具体 
展开 学习 2.4 神经 语言 模型 NLM 神经 语言 模型 
Neural Language Model 是 Word Embeddings 的 基本 思想 NLM 
的 输入 是 词 向量 词 向量 和 模型 参数 
最终 的 语言 模型 可以 通过 神经网络 训练 一同 得到 
相比 于n/nr gram 通过 联合 概率 考虑 词 之间 的 
位置 关系 NLM 则是 利 用词 向量 进一步 表示 词语 
之间 的 相似性 比如 近义词 在 相似 的 上下文 里 
可以 替代 或者 同类 事物 的 词 可以 在 语 
料中 频数 不同 的 情况 下 获得 相近 的 概率 
结合 参考文献 1 举 一个 简单 例子 在 一个 语料 
C 中 S1 = A dog is sitting in the 
room . 共 出现 了 10000次 S2 = A cat 
is sitting in the room 出现 了 1次 按照 n 
gram 的 模型 当 我们 输入 A _ _ _ 
_ _ is sitting in the room 来 预测 下划线 
上 应该 填入 的 词 时 dog 的 概率 会 
远大于 cat 这 是 针对 于 语料 C 得到 的 
概率 但是 我们 希望 相似 含义 的 词 在 目标 
向量空间 中的 距离 比 不相关 词 的 距离 更近 比如 
v man v woman 约等于 v gentleman v madam 用 
这样 生成 的 词 向量 或者 已 经训 练好 的 
模型 在 去做 翻译 问答 等 后续 工作 时 就 
会很 有效果 而 NLM 利 用词 向量 表示 就能 达到 
这样 的 效果 NLM 的 神经 网络 训练样本 同 n 
gram 的 取法 取 语 料中 任一 词 w 的 
前 n 1个 词 作为 Context w 则 Context w 
w 就是 一个 训练样本 了 这里 的 每 一个 词 
都被 表示 为 一个 长度 为 L 的 词 向量 
然后 将 Context w 的 n 1个 词 向量 首位 
连接 拼成 n 1 L 的 长 向量 下面 为 
NLM 图解 包括 四层 输入 层 投影 层 隐藏 层 
输出 层 注意 这 只是 取 一个 词 w 后 
输出 的 向量 y 我们 需要 的 就是 通过 训练 
集 所有 的 词 都做 一遍 这个 过程 来 优化 
得到 理想 的 W q 和U/nr b 上图 中 所有 
参数 W q U b 以及 词 向量 都是 通过训练 
得到 的 三 Word2vec 目前 学习 了解到 的 Word2vec 有 
基于 Hierarchical Softmax 和 基于 Negative Sampling 两种 方式 由于 
这 两个 模型 是 相反 的 过程 即 CBOW 是 
在 给定 上下文 基础 上 预测 中心词 Skip gram 在有 
中心词 后 预测 上下文 两个 模型 都 包含 三层 输入 
层 投影 层 输出 层 3.1   CBOW 不同于 NLM 
的 是 Context w 的 向量 不再 是 前后 连接 
而是 求和 我们 记为 3 . 1.1 基于 Hierarchical Softmax 
基于 Hierarchical Softmax 的 CBOW 所要 构建 的 霍夫曼 树 
所需 参数 如下 从 根结 点到 w 对应 结点 的 
路径 路径 上 包含 结点 个数 到 w 路径 上 
的 的 结点 结点 编码 根 结点 不 编码 非 
叶子 结点 包括 根 结点 对应 的 向量 霍夫曼 树 
构建 按照 频数 大 小有 左右 两种 其实 都是/nr 自己 
约定 的 在 这里 就 不 麻烦 了 构建/v 后左/nr 
结点/n 编码/n 为/p 0/m 为 正 类 右 结点 为 
1 为 负 类 根据 逻辑 回归 一个 结点 被 
分为 正 类 的 概率 为 所以 之前 我们 要 
构造 的 目标 函数 就 可以 写 为 以下 形式 
对 以上 式子 进行 最 优化 得到 Θ 和词/nr 向量 
V 我们 发现 词 向量 在 这里 是 累加 的 
我们 省略 求 各个 词 的 V 3 . 1.2 
基于 Negative Sampling 对于 大规模 语料 构建 霍夫曼 树 的 
工作量 是 巨大 的 而且 叶子 节点 为 N 的 
霍夫曼 数 需要 新添 N 1 个 结点 而 随着 
树 的 深度 增加 参数 计算 的 量 也会 增加 
很多 很多 得到 的 词 向量 也会 不够好 为此 Mikolov 
作出 了 优化 将 构建 霍夫曼 树 改为 随机 负 
采 样方法 对于 给定 的 上下文 Context w 去 预测 
w 如果 从 语料 中 就是 存在 Context w w 
那么 w 就是 正 样本 其他 词 就是 负 样本 
我们 设 负 样本 集为 词 的 标签 训练 目标 
为 增 大正 样本 的 概率 减 小负 样本 的 
概率 可见 对于 单词 w 基于 Hierarchical Softmax 将其 频数 
用来 构建 霍夫曼 树 正负 样本 标签 取自 结点 左右 
编码 而 基于 Negative Sampling 将其 频数 作为 随机 采样 
线段 的 子 长度 正负 样本 标签 取 自从 语料 
中 随机 取出 的 词 是否 为 目标 词 构造 
复杂度 小于 前者 3.2 Skip gram 由于 Skip gram 是 
CBOW 的 相反 操作 输入输出 稍 有 不同 推导 大同小异 
. . . . . . . . . . 
. . . . . . . . 经过 几 
天对 nlp 的 理解 接下来 我们 说说 语言 模型 下面 
还 是以 PPT 方式 给出 一 统计 语言 模型 1 
什么 是 统计 语言 模型 一个 语言 模型 通常 构建 
为 字符串 s 的 概率分布 p s 这里 的 p 
s 实际上 反映 的 是 s 作为 一个 句子 出现 
的 概率 这里 的 概率 指 的 是 组成 字符串 
的 这个 组合 在 训练 语料 中 出现 的 似 
然 与 句子 是否 合乎 语法 无关 假设 训练 语料 
来自 于 人类 的 语言 那么 可以 认为 这个 概率 
是的 是 一句话 是否是 人话 的 概率 2 怎么 建立 
统计 语言 模型 对于 一个 由 T 个 词 按顺序 
构成 的 句子 p s 实际上 求解 的 是 字符串 
的 联合 概率 利用 贝叶斯 公式 链式 分解 如下 从 
上面 可以 看到 一个 统计 语言 模型 可以 表示 成 
给定 前面 的 的 词 求 后面 一个 词 出现 
的 条件概率 我们 在 求 p s 时 实际上 就 
已经 建立 了 一个 模型 这里 的 p * 就是 
模型 的 参数 如果 这些 参数 已经 求解 得到 那么 
很容易 就 能够 得到 字符串 s 的 概率 3 求解 
的 问题 假定 字符串 s 为 i want to drink 
some water 那么 根据 上面 所 建立 的 模型 问题 
归结 为 如何 求解 上面 的 每一个 概率 比如 一种 
比较 直观 的 方法 就是 分别 计算 出 I want 
to 和 I want to drink 在 语料 中 出现 
的 频数 然后 再用 除法 看起来 好像 很 美好 实际上 
这里 存在 两个 问题 1 自由 参数 数目 假定 字符串 
中 字符 全部 来自 与 大小 为 V 的 词典 
上述 例子 中 我们 需要 计算 所有 的 条件概率 对于 
所有 的 条件 概率 这里 的 w 都有 V 种 
取值 那么 实际上 这个 模型 的 自由 参数 数目 量级 
是 V ^ 6 6 为 字符串 的 长度 从 
上面 可以 看出 模型 的 自由 参数 是 随着 字符串 
长度 的 增加 而 指数级 暴增 的 这 使 我们 
几乎 不 可能 正确 的 估计 出 这些 参数 2 
数据 稀疏 性 从 上面 可以 看到 每一个 w 都 
具有 V 种 取值 这样 构 造出 了 非常 多 
的 词 对 但 实际 中 训练 语料 是 不会 
出现 这么 多种 组合 的 那么 依据 最大 似 然 
估计 最终 得到 的 概率 实际 是 很 可能 是 
0 4 怎么 解决 上面 提出 了 传统 统计 语言 
模型 的 两个 问题 后面 分别 介绍 两种 方法 进行 
求解 N gram 语言 模型 神经 概率 语言 模型 二 
N gram 语言 模型 1 什么 是 N gram 语言 
模型 为了 解决 自由 参数 数目 过多 的 问题 引入 
了 马尔科夫 假设 随意 一个 词 出现 的 概率 只 
与 它 前面 出现 的 有限 的 n 个 词 
有关 基于 上述 假设 的 统计 语言 模型 被称为 N 
gram 语言 模型 2 如何 确定 N 的 取值 通常 
情况下 n 的 取值 不能够 太大 否则 自由 参数 过多 
的 问题 依旧 存在 1 当 n = 1时 即 
一个 词 的 出现 与 它 周围 的 词 是 
独立 这种 我们 称为 unigram 也 就是 一 元语言 模型 
此时 自由 参 数量级 是 词典 大小 V 2 当 
n = 2时 即 一个 词 的 出现 仅 与 
它 前面 的 一个 词 有关 时 这种 我们 称为 
bigram 叫 二 元语言 模型 也叫 一 阶 马尔科夫 链 
此时 自由 参数 数量级 是 V ^ 2 3 当 
n = 3时 即 一个 词 的 出现 仅 与 
它 前面 的 两个 词 有关 称为 trigram 叫 三 
元语言 模型 也叫 二阶 马尔科夫 链 此时 自由 参数 数量级 
是 V ^ 3 一般 情况 下 只 使用 上述 
取值 因为 从 上面 可以 看出 自由 参数 的 数量级 
是 n 取值 的 指数 倍 从 模型 的 效果 
来看 理论上 n 的 取值 越大 效果 越好 但随着 n 
取值 的 增加 效果 提升 的 幅度 是 在 下降 
的 同时 还 涉及 到 一个 可靠性 和可/nr 区别性 的 
问题 参数 越多 可 区别性 越好 但 同时 单个 参数 
的 实例 变少 从而 降低 了 可靠性 3 建模 与 
求解 N gram 语言 模型 的 求解 跟 传统 统计 
语言 模型 一致 都是 求解 每 一个 条件 概率 的 
值 简单 计算 N 元 语法 在 语料 中 出现 
的 频率 然后 归一化 4 平滑 化 我们 在 传统 
统计 语言 模型 提出 了 两个 问题 自由 参数 数目 
和 数据 稀疏 上述 N gram 只是 解决 了 第一 
个 问题 而 平滑 化 就是 为了 解决 第二个 问题 
假设 有 一个 词组 在 训练 语料 中 没有 出现 
过 那么 它 的 频次 就为 0 但 实际上 能 
不能 认为 它 出现 的 概率 为 0 呢 显然 
不 可以 我们 无法 保证 训练 语料 的 完备性 那么 
解决 的 方法 是 什么 如果 我们 默认 每一个 词组 
都 出现 1次 呢 无论 词组 出现 的 频次 是 
多少 都往/nr 上加 1 这就 能够 解决 概率 为 0 
的 问题 了 上述 的 方法 就是 加 1 平滑 
也 称为 拉普拉斯 平滑 平滑 化 还有 许多 方法 这里 
就不 展开 介绍 了 1 加法 平滑 2 古德 图灵 
平滑 3 K 平滑 三 神经 概率 语言 模型 1 
前置 知识 在 N gram 语言 模型 中 计算 条件概率 
的 方法 是 简单 的 用 词频 做 除法 然后 
归一化 在 机器 学习 的 领域 中 通用 的 做法 
是 对 所 考虑 的 问题 建模 后先/nr 为其 构造 
一个 目标函数 然后 对 这个 目标 函数 进行 优化 从而 
求得 一组 最优 的 参数 最后 再 利用 这组 参数 
对应 的 模型 来 进行 预测 那么 在 上述 的 
语言 模型 中 利用 最大化 对数 似 然 将 目标函数 
设为 Context 代表 词 w 的 上下文 对应 N gram 
就是 词 w 的 前 N 1个 词 之后 对 
目标函数 进行 最大化 由上 可见 概率 实际上 是 w 和的/nr 
函数 其中 θ 为 待定 参 数集 这样 将 计算 
所有 的 条件概率 转化 为了 最优化 目标函数 求解 得到 θ 
的 过程 通过 选取 合适 模型 可以 使得 θ 参数 
的 个数 远 小于 N gram 模型 中 参数 的 
个数 2 什么 是 神经 概率 语言 模型 Begio 等 
人在 2003年 发表 的 A Neural Probabilistic Language Model 里面 
详解 了 这个 方法 基本 的 思想 其实 与 上述 
的 前置 知识 有所/nr 联系 既然 是 神经 概率 语言 
模型 那么 实现 的 时候 自然 有 一个 神经 网络 
结构图 如下 它 包括 了 四个 层 输入 层 投影 
层 隐藏 层 和 输出 层 2 计算 流程 1 
输入 层 这里 就是 词 w 的 上下文 如果 用 
N gram 的 方法 就是 词 w 的 前 n 
1个 词 了 每 一个 词 都 作为 一个 长度 
为 V 的 one hot 向量 传入神经 网络 中 2 
投影 层 在 投影 层 中 存在 一个 look up 
表 C C 被 表示 成 一个 V * m 
的 自由 参数 矩阵 其中 V 是 词典 的 大小 
而 m 作为 自定义 的 参数 一般 是 10 ^ 
2 的 倍数 表 C 中 每 一行 都 作为 
一个 词 向量 存在 这个词 向量 可以 理解 为 每一个 
词 的 另一种 分布式 表示 每一个 one hot 向量 都 
经过 表 C 的 转化 变成 一个 词 向量 n 
1个 词 向量 首尾 相接 的 拼起来 转化 为 n 
1 m 的 列 向量 输入 到 下一层 3 隐藏 
层 输出 层 之后 再 对列 向量 进行 计算 大致 
如下 其中 tanh 是 激活 函数 是 为 归一化 的 
log 概率 之后 再用 softmax 进行 归一化 就 得到 最终 
的 概率 输出 了 在 前置 知识 中 我们 提到 
了 参数 θ 那么 在 神经 网络 中 实际 的 
参数 如下 词 向量 v w w 以及 填充 向量 
神经网络 参数 W p U q3 最后 在 传统 统计 
语言 模型 中 我们 提出 两个 问题 自由 参数 数目 
和 数据 稀疏 这里在 实际上 使用 参数 θ 代替 了 
自由 参数 指数级 的 求解 而 数据 稀疏 问题 我们 
在 最后 使用 softmax 进行 归一化 求解 出来 的 概率 
是 平滑 的 所以 也 解决 了 这个 问题 参考 
PPT 来源 小象 学院 史兴 老师 更多 实时 更新 的 
个人 学习 笔记 分享 请 关注 知乎 https / / 
www . zhihu . com / people / yuquanle / 
columns 微信 订阅 号 人工智能 小白 入门 学习 ID t 
u d y F o r A I t a 
n f o r d c o r e n 
l p 简介 Stanford CoreNLP 提供 了 一套 人类 语言 
技术 工具 支持 多种 自然语言 处理 基本功能 Stanfordcorenlp 是 它 
的 一个 python 接口 官网 地址 https / / stanfordnlp 
. github . io / CoreNLP / Github 地址 https 
/ / github . com / stanfordnlp / C o 
r e N L P t a n f o 
r d c o r e n l p 主要 
功能 包括 分词 词性 标注 命名 实体 识别 句法结构 分析 
和 依存 分析 等等 Stanfordcorenlp 工具 Demo 安装 pip install 
stanfordcorenlp 先 下载 模型 下载 地址 https / / nlp 
. stanford . edu / software / corenlp backup download 
. html 支持 多种 语言 这里 记录 一下 中 英文 
使用 方法 from stanfordcorenlp import StanfordCoreNLP zh _ model = 
StanfordCoreNLP r stanford corenlp full 2018 02 27 lang = 
zh en _ model = StanfordCoreNLP r stanford corenlp full 
2018 02 27 lang = en zh _ sentence = 
我 爱 自然 语言 处理 技术 en _ sentence = 
I love natural language processing technology 1 . 分词 Tokenize 
print Tokenize zh _ model . word _ tokenize zh 
_ sentence print Tokenize en _ model . word _ 
tokenize en _ sentence Tokenize 我 爱 自然 语言 处理 
技术 Tokenize I love natural language processing technology 2 . 
词性 标注 Part of Speech print Part of Speech zh 
_ model . pos _ tag zh _ sentence print 
Part of Speech en _ model . pos _ tag 
en _ sentence Part of Speech 我 爱 NN 自然 
AD 语言 NN 处理 VV 技术 NN PU Part of 
Speech I PRP love VBP natural JJ language NN processing 
NN technology NN . 3 . 命名 实体 识别 Named 
Entity print Named Entities zh _ model . ner zh 
_ sentence print Named Entities en _ model . ner 
en _ sentence Named Entities 我 爱 O 自然 O 
语言 O 处理 O 技术 O O Named Entities I 
O love O natural O language O processing O technology 
O O 4 . 句法 成分 分析 Constituency Parse print 
Constituency Parsing zh _ model . parse zh _ sentence 
+ \ n print Constituency Parsing en _ model . 
parse en _ sentence Constituency Parsing ROOT IP IP NP 
NN 我 爱 ADVP AD 自然 NP NN 语言 VP 
VV 处理 NP NN 技术 PU Constituency Parsing ROOT S 
NP PRP I VP VBP love NP JJ natural NN 
language NN processing NN technology . 5 . 依存 句法分析 
Dependency Parse print Dependency zh _ model . dependency _ 
parse zh _ sentence print Dependency en _ model . 
dependency _ parse en _ sentence Dependency ROOT 0 4 
nsubj 4 1 advmod 4 2 nsubj 4 3 dobj 
4 5 punct 4 6 Dependency ROOT 0 2 nsubj 
2 1 amod 6 3 compound 6 4 compound 6 
5 dobj 2 6 punct 2 7 国内 自然语言 处理 
期刊 现代 语言学 汉斯 出版社 汉斯 出版社 Hans Publishers www 
. hanspub . org 聚焦 于 国际 开源 Open Access 
中文 期刊 的 出版 发行 覆盖 以下 领域 数学物理 生命科学 
化学材料 地球 环境 医药卫生 工程技术 信息 通讯 人文 社科 经济 
管理 等 秉承 着 传播 文化 促进 交流 的 理念 
本社 将 积极探索 中文 学术期刊 国际化 道路 并 积极 推进 
中国 学术 思想 走向世界 目前 汉斯 出版社 的 所有 期刊 
均被 知网 CNKI Scholar 等 数据库 收录 其中 23本 被 
美国 化学 文摘 Chemical Abstracts 收录 30本 被 EBSCO 收录 
计算机 学报 计算机 学报 刊登 的 内容 覆盖 计算机 领域 
的 各个 学科 以 论文 技术 报告 短文 研究 简报 
综论 等 形式 报道 以下 方面 的 科研 成果 计算机 
科学 理论 计算机硬件 体系结构 计算机软件 人工智能 数据库 计算机 网络 与 
多媒体 计算机 辅助 设计 与 图形学 以及 新 技术 应用 
等 计算机 研究 与 发展 刊登 内容 计算机 科学 技术 
领域 高 水平 的 学术 论文 最新 科研 成果 和 
重大 应用 成果 刊登 内容 综述 软件技术 信息安全 计算机网络 体系结构 
人工智能 计算机 应用 技术 图形 图象 自然语言 处理 信息检索 数据库 
技术 存储 技术 及 计算机 计算机 基础 理论 等 相关 
领域 软件 学报 软件 学报 注重 刊登 反映 计算机科学 和 
计算机 软件 新理论 新 方法 和新/nr 技术 以及 学科 发展 
趋势 的 文章 主要 涉及 理论 计算机科学 算法 设计 与 
分析 系统 软件 与 软件 工程 模式识别 与 人工智能 数据库 
技术 计算机网络 信息安全 计算机 图形学 与 计算机 辅助 设计 多媒体 
技术 及 其他 相关 的 内容 . 中国 中文信息 学会 
学会 的 学术 研究 内容 是 利用 计算机 对 汉语 
的 音 形 义 等 语言 文字 信息 进行 的 
加工 和 操作 包括 对 字 词 短语 句 篇章 
的 输入 输出 识别 转换 压缩 存储 检索 分析 理解 
和 生成 等 各 方面 的 处理 技术 中文信息处理 学科 
是 在 语言 文字学 计算机 应用 技术 人工智能 认知 心理学 
和 数学 等 相关 学科 的 基础 上 形成 的 
一门 新兴 的 边缘 学科 中国 中文信息 学会 2018年 学术 
活动 计划 国际 自然 语言 处理 及 中文 计算 会议 
中文信息 学报 中文信息 学报 刊登 内容 有 计算 语言学 包括 
音位学 词法 句法 语义 知识 本体 和 语用学 语言 资源 
包括 计算 词汇学 术语 电子词典 和 语料库 机器翻译 MT 或 
机器 辅助 翻译 MAT 汉语/nz 和/c 少数民族/n 语言/n 文字/n 输入输出/n 
和/c 处理/v 中文 手写 和 印刷体 识别 OCR 中文 语音 
识别 与 合成 以及 文语/nr 转换 TTS 信息检索 IR 信息 
抽取 IE 及 相关 的 语言 技术 网上 搜索 引擎 
数据挖掘 知识 获取 神经网络 机器学习 专家系统 知识 工程 和 其他 
人工智能 AI 技术 国外 自然语言 处理 期刊 2018年 自然语言 处理 
及 相关 国际 会议 重要 日期 整理 NLP 会议 会议 
名称 截稿 日期 通知 日期 会议 日期 举办 地点 ACL 
20182.224 . 207.15 7.20 墨尔本 澳大利亚 MNLP 2018 ‍ 5.228 
. 610.31 11.04 布鲁塞尔 比利时 NAACL HLT 2018 已过 已过 
6.01 6.06 新奥尔良 美国 COLING 20183.165 . 178.20 8.25 圣达菲 
美国 CICLING 2018 已过 已过 3.18 3.24 河内 越南 相关 
会议 会议 名称 截稿 日期 通知 日期 会议 日期 举办 
地点 IJCAI ECAI 2018 已过 4 . 167.13 7.19 斯德哥尔摩 
瑞典 AAAI 2018 已过 已过 2.02 2.07 新奥尔良 美国 NIPS 
2018 待定 待定 12.03 12.08 蒙特利尔 加拿大 ICML 2018 已过 
5 . 117.10 7.15 斯德哥尔摩 瑞典 SIGIR 2018 已过 4 
. 117.08 7.12 安娜堡 美国 KDD 2018 已过 5 . 
068.19 8.23 伦敦 英国 WSDM 2018 已过 已过 2.06 2.08 
洛杉矶 美国 CIKM 20185.158 . 0610.22 10.26 灵 格 托 
意大利 WWW 2018 已过 已过 4.23 4.27 里昂 法国 编者注 
  更多 人工智能 业务 方面 重要 的 发展 请 关注 
2018年 4月 10 13日 人工智能 北京 大会 人工智能 的 奇妙 
之处 在于 它 能让 机器 像 人类 一样 拥有 理解能力 
完成 智能 任务 而 它 的 难解 之 处 在于 
如何 让 人工智能 拥 有理解力 甚至 让 机器 可以 像 
人 一样 思考 让 人工智能 听懂 人话 是 近几年 数据 
科学家 们 一直 在 做 的 努力 也 收获 了 
很多 欣喜 为 用户 提供 实时 应答 服务 为 用户 
提供 精确 的 搜索 推荐 等 个性化 服务 辅助 医生 
对 患者 进行 综合 诊疗 在 无人 驾驶 新闻 传媒 
在线 娱乐 金融 教育 等 领域 还将 有 许多 令人 
期待 的 美好 应用 前景 可 这些 离 人工智能 像人 
的 小 目标 还 远 着呢 尽管 机器学习 深度 学习 
神经 网络 的 发展 推动 自然语言 技术 的 进步 但 
想要 让 机器 学会 理解 首先 需要 攻克 人工智能 的 
核心 领域 自然语言 处理 技术 NLP 关于 它 的 技术 
痛点 应用 难题 你 都会 在 AI Conference 2018 北京站 
  自然语言 处理 与 语音 技术 板块 中 茅塞顿开 这里 
汇集 了 传媒 / 新闻 机器翻译 电信 和 教育 行业 
的 应用 案例 先进 的 模型 与 算法 这里 有 
我们 耳熟能详 的 小冰 还有 百度 微软 Intel 谷歌 这些 
人工智能 巨头 分享 他们 这些 年在/nr 人工智能 上 翻过 的 
山 趟 过 的 河 在 进入 NLP 的 世界 
探 究其 奥秘 之前 我 迫不及待 的 要 分享 给 
你 一些 很 有价值 的 内部 消息 1 . 你 
将会 成为 少数 深入 了解 微软 小冰 的 一员 方向 
与 人工智能 交互 主题 小冰 从 人类 与 AI 之间 
的 对话 中 学到 的 经验 教训 主讲人 周力 微软中国 
语音 识别 是 人机交互 的 入口 经过 四年 的 探索 
小冰 已经 成为 科技 史上 最大 规模 的 人工智能 情感 
计算 框架 系统 她 当过 歌手 诗人 主持人 评论员 客服 
与 中国 日本 和 美国 超过 1亿 用户 进行 互动 
从中 学习 人类 特有 的 情感 小冰 的 每一次 演进 
都让/nr 我们 对 机器 能做 什么 产生 了 非常 多 
的 联想 这次 又 是 什么 呢 4月 12日 微软 
小冰 首席 架构师 周力 博士 将 亲自 带着 小冰 来到 
AI Conference 2018 北京站 与 大家 分享 在 过去 四年 
中 研究 微软 小冰 的 感悟 希望 在 不久 的 
将来 人类 的 生活 会 因为 与 人工智能 的 直接 
交流 变得 更加 美好 ❖ 2 . 使用 Intel AI 
技术 的 NLP 企业 案例 让 你 醍醐灌顶 方向 模型 
与 方法 主题 深度 学习 时代 的 数据 科学 和 
自然 语言 处理 主讲人 Yinyin Liu Intel Nervana 2016年 起 
Intel 逐渐 将 自己 的 战略 重心 转移 到 了 
数据 科学 和 人工智能 领域 向业界 提供 AI 解决方案 最 
近几年 主要 的 AI 推动力 是由 深度 学习 产生 的 
NLP 利用 深度 学习 最新 算法 发展 例如 文档 理解 
之类 的 应用 使 公司 能够 筛查 海量 文本 分类 
并 找到 相关 信息 Intel 人工智能 产品 事业部 数据 科学 
主任 Yinyin Liu 将 会与 你 讨论 深度 学习 最新 
发展 如何 影响 处理 文本 语言 及 基于 对话 应用 
并 启发 了 利用 数据 的 新方向 另外 还 毫不 
吝啬 的 为 大家 分享 一些 使用 Intel ® AI 
技术 的 NLP 企业 案例 ❖ 3 . NLP 落地 
金融 了 方向 模型 与 方法 主题 使用 AI 来 
分析 财务 新闻 的 影响 主讲人 ZhefuShi 密苏里 大学 AI 
的 领域 在 不断 地 进展 之中 越来越 多 的 
公司 认识到 NLP 对于 金融 行业 分析 的 重要 作用 
在 金融 领域 AI 技术 对 分析 金融 新闻 的 
影响 是 有 帮助 的 将 非 结构化 数据结构 化 
处理 从中 探寻 影响 市场 变动 的 线索 比如 通过 
历史 金融 新闻 预测 价格 趋势 评估 市场 风险 为 
监管 人员 提供 企业 监管 市场监管 舆情 监控 应用 知识图谱 
和 图谱 计算 技术 来 进行 风险 管理 供应链 金融 
管理 和 投融资 管理 等 但 自然语言 处理 技术 目前 
是 人工智能 进行 场景 落地 时的/nr 一大 难点 重点 密苏里 
大学 的 Zhefu Shi 博士 带着 自己 的 知识 宝库 
与 你 分享 如何 使用 AI 来 分析 财务 新闻 
的 影响 如何 提取 金融 实体 信息 并 将其 用于 
分析 业务 影响 敬请 关注 ❖ 4 . 提升 深度 
学习 的 表现 有 技巧 方向 企业 人工智能 实施 人工智能 
模型 与 方法 主题 深度 学习 在 文本 挖掘 中 
的 应用 主讲人 Emmanuel Ameisen Insight DataScience Jeremy Karnowski Insight 
Data Science 深度 学习 在 自然 语言 处理 中 的 
应用 非常 广泛 可以 说 横扫 自然语言 处理 的 各个 
应用 从 底层 的 分词 语言 模型 句法分析 等到 高层 
的 语义 理解 对话 管理 知识/v 问答/v 等/u 方面/n 都/d 
几乎/d 都/d 有/v 深度/ns 学习/v 的/uj 模型/n 并且 取得 了 
不错 的 效果 多数 公司 已经 开始 利用 文本 数据 
支持 部分 业务 运营 但也 遇到 了 一系列 挑战 其中 
包括 如何 验证 和 解释 模型 性能 以及 模型 复杂性 
如何 影响 部署 它们 的 简便性 Emmanuel Ameisen 和 Jeremy 
Karnowski 通过 对 google Facebook Amazon Twitter Salesforce Airbnb 等 
超过 75个 团队 的 对话 进行 分析 得到 了 很 
有价值 的 研究 成果 分享 他们 如何 从 传统 的 
机器学习 算法 转变成 更有 表现力 的 深度 学习 模型 如 
卷积 神经 网络 和 回归 神经网络 这些 新 技术 使 
公司 能够 改进 许多 关键 业务 操作 您 将 学习 
不同 模型 在 不同 项目 中 的 应用 并 了解 
如何 选择 最 适合 您 项目 的 模型 Gartner 认为 
未来 10年 人工智能 将 成为 最 具 破坏性 级别 的 
技术 主要 是 因为 卓越 的 计算 能力 漫无边际 的 
数据 集 深度 神经网络 领域 的 超乎 寻常 的 进步 
与其 停留 观望 不如 赶快 行动 跟着 大咖 学习 人工智能 
领域 NLP 的 新知识 借鉴 他们 在 人工智能 布局 的 
新思路 自然语言 处理 技术 与 语音 技术 15个 议题   
  6 大方向 企业 人工智能 模型 与 方法 实施 人工智能 
人工智能 交互 4月 10 13日 AI Conference 2018 北京站 已经 
准备好 了 你 呢 什么 是 人工智能 人工智能 是 计算机 
科学 的 一个 分支 她 企图 了解 智能 的 实质 
并 产生 一种 新的 能以 人类 智能 相似 的 方式 
做出 反应 的 智能 机器 该 领域 的 研究 包括 
机器人 语言识别 图像识别 自然语言 处理 和 专家 系统 机器学习 机器 
主要 通过 大量 的 训练 数据 进行 训练 程序 不断 
地 进行 自我 学习 和 修正 来 训练 出 一个 
模型 而 模型 的 本质 就是 一堆 参数 用 成千上万 
的 参数 来 描述 业务 特点 从而 接近 人类 的 
智力 深度 学习 深度 学习 是 机器 学习 的 一个 
子集 深度 学习 的 前身 是 人工神经网络 ANN 它 的 
基本 特点 就是 模仿 人脑 神经元 传递 和 处理 信息 
的 模式 有 监督 学习 输入 的 训练 数据 有 
特征 有 标记 在 学习 中 就是 找到 特征 与 
标记 之间 的 映射 关系 通过 标记 不断 纠正 学习 
中 的 偏差 使 预测 率 不断 提高 这种/r 训练/vn 
数据/n 有/v 标记/n 的/uj 学习/v 称为/v 有/v 监督/vn 学习/v 无 
监督 学习 让 计算机 自己 去 学习 怎样 做 一些 
事情 所有 训练 数据 没有 标记 只有 特征 无 监督 
学习 有 两种 思路 第一种 训练 时 不为 其 指定 
明确 分类 但 数据 会 呈现 聚 群 的 结构 
彼此 相似 的 类型 会 聚集 在 一起 计算机 把 
这些 没有 标记 的 数据 分成 一个 个 组合 就是 
聚 类 第二种 在 成功 时 采用 某种 激励 制度 
即 强化 学习 . 半 监督 学习 训练/vn 数据/n 中/f 
有/v 一部分/m 有/v 标记/n 有/v 一部分/m 无/v 标记/n 没有 标记 
的 数量 远远 大于 有 标记 的 数量 这也 符合 
现实 它 的 基本 规律 是 数据 的 分布 必然 
不完全 随机 通过 结合 有 标记 的 局部 特征 以及 
大量 没 标记 的 数据 的 整体 分布 可以 得到 
比较 好 的 分类 结果 文章 目录 1 . THUCNews 
中文 数据集 1.1 数据 下载 1.2 数据 探索 2 . 
IMDB 英文 数据集 2.1 数据 下载 2.2 数据 探索 3 
. 常用 评估 方式 1 . THUCNews 中文 数据集 THUCNews 
是 根据 新浪 新闻 RSS 订阅 频道 2005 ~ 2011 
年间 的 历史 数据 筛选 过滤 生成 包含 74万 篇 
新闻 文档 2.19 GB 均为 UTF 8 纯 文本格式 在 
原始 新浪 新闻 分类 体系 的 基础 上 重新 整合 
划分 出 14个 候选 分类 类别 财经 彩票 房产 股票 
家居 教育 科技 社会 时尚 时政 体育 星座 游戏 娱乐 
1.1 数据 下载 官网 链接 http / / thuctc . 
thunlp . org / # 获取 链接 提供 个人 信息 
后可/nr 下载 1.2 数据 探索 数据 集中 包含 四个 文本文件 
cnews . test . txt cnews . train . txt 
cnews . val . txt cnews . vocab . txt 
cnews . train . txt 为 训练 数据集 cnews . 
test . txt 为 测试 数据集 cnews . val . 
txt 为 验证 数据集 cnews . vocab . txt 是 
所有 数据 集中 出现 的 汉字 字母 与 标点符号 汇 
集成 的 词典 其中 是 词汇表 中 添加 的 辅助 
Token 用来 补齐 句子 长度 简单 建立 一个 数据字典 观察 
一下 可以 看到 包含 的 中文 汉字 还是 挺多 的 
基本上 常用字 都 包含 了 附 部分 截图 2 . 
IMDB 英文 数据集 数据集 包含 电影 评论 及其 关联 的 
二进制 标签 旨在 作为 情感 分类 的 基准 核心 数据集 
包含 50 000个 评论 均匀 分为 25k 训练 集 和 
25k 测试 集 标签 的 整体 分布 是 平衡 的 
25k pos 和 25k neg 还 包括 另外 50 000个 
未 标记 文档 用于 无 监督 学习 2.1 数据 下载 
http / / ai . stanford . edu / ~ 
amaas / data / sentiment / 进入 后 直接 点击 
Large Movie Review Dataset v 1.0 开始 下载 即可 2.2 
数据 探索 下载 后会/nr 得到 一个 aclImdb _ v1 . 
tar . gz 压缩包 解压 之后 可以 看到 文件夹 中 
包含 train 训练 数据集 的 文件夹 和 test 测试 数据集 
文件夹 在/p 训练/vn 数据/n 集中/v 主要/b 包括/v 两个/m 已/d 标记/n 
情感/n 类/q 别的/r 影评/vn 数据集/i pos/w 和/c neg/w 和/c 一个/m 
未/d 标记/n 的/uj 用于/v 无/v 监督/vn 学习/v 的/uj 数据集/i unsup/w 
还有 一个 imdb 的 词汇表 字典 包含 了 训练 集中 
出现 的 所有 单词 测试 集中 主要 包括 两个 已 
标记 情感 类 别的 影评 数据集 pos 和 neg 同样 
简单 建立 一个 数据字典 观察 一下 这个 英文单词 果然 是 
庞然大物 太多 了 密集 恐惧 附 部分 截图 3 . 
常用 评估 方式 首先 要 提出 混淆 矩阵 混淆 矩阵 
P o s i t i v e N e 
g a t i v e P o s i 
t i v e T P F P N e 
g a t i v e F N T N 
T r u e Positive 真正 TP 将 正 类 
预测 为 正 类 数 True Negative 真 负 TN 
将 负 类 预测 为 负 类 数 False Positive 
假 正 FP 将 负 类 预测 为 正 类 
数 → 误报 Type I error False Negative 假 负 
FN 将 正 类 预测 为 负 类 数 → 
漏报 Type II error 准确率 accuracy 预测 准确 的 在 
所有 样本 中 的 比例 accuracy = TP + TN 
TP + FN + FP + TN \ frac { 
TP + TN } { TP + FN + FP 
+ TN } TP + FN + FP + TN 
TP + TN 精确 率 precision precision = TPTP + 
FP \ frac { TP } { TP + FP 
} TP + FPTP 对于 给定 的 测试 数据集 分类器 
正确 分类 的 样本 数 与 正 样本数 之比 简单 
点 给出 的 结果 有 多少 是 正确 的 精确 
率 是 针对 预测 结果 而言 的 它 表示 的 
是 预测 为 正 的 样本 中 有 多少 是 
对 的 那么 预测 为 正 就有 两种 可能 了 
一种 就是 把 正 类 预测 为 正 类 TP 
另一种 就是 把 负 类 预测 为 正 类 FP 
召回率 recall recall = TPTP + FN \ frac { 
TP } { TP + FN } TP + FNTP 
正确 的 结果 有 多少 被 给 出了 召回率 是 
针对 我们 原来 的 样本 而言 的 它 表示 的 
是 样本 中的 正 例 有 多少 被 预测 正确 
了 那也 有 两种 可能 一种 是 把 原来 的 
正 类 预测 成正类/nr TP 另一种 就是 把 原来 的 
正 类 预测 为 负 类 FN ROC 关注 两个 
指标 一个 是 TPR 也 就是 召回率 另 一个 是 
将 负 例 错 分为 正 例 的 概率 FPR 
= FPTP + TR \ frac { FP } { 
TP + TR } TP + TRFP 直 观上 TPR 
代表 能将 正 例 分对 的 概率 FPR 代表 将 
负 例 错 分为 正 例 的 概率 在 ROC 
空间 中 每个 点 的 横坐标 是 FPR 纵坐标 是 
TPR AUC Area Under Curve 被 定义 为 ROC 曲 
线下 的 面积 可以 综合 衡量 一个 预测模型 的 好坏 
这 一个 指标 综合 了 precision 和 recall 两个 指标 
简单 说 AUC 值 越大 的 分类器 正确率 越高 AUC 
= 1 完美 分类器 采用 这个 预测模型 时 不管 设定 
什么 阈值 都能 得出 完美 预测 绝大多数 预测 的 场合 
不 存在 完美 分类器 0.5 AUC 1 优于 随机 猜测 
这个 分类器 模型 妥善 设定 阈值 的话 能有 预测 价值 
AUC = 0.5 跟 随机 猜测 一样 例 丢 铜板 
模型 没有 预测 价值 AUC 0.5 比 随机 猜测 还差 
但 只要 总是 反 预测 而行 就 优于 随机 猜测 
因此 不 存在 AUC 0.5 的 情况 自然语言 处理 的 
方法 分词 分词 的 任务 定义 为 输入 一个 句子 
输 出 一个 词语 序列 的 过程 如 将 「 
严守一 把 手机 关了 」 输出 为 「 严守一 / 
把 / 手机 / 关/了/nr 」 目前 的 两种 主流 
方法 包括 基于 离散 特征 的 CRF 和 BILSTM CRF 
挑战 包括 交叉 歧义 新词 识别 领域 移植 多源 异构 
数据 融合 及 多 粒度 分词 等 命名 实体 现在 
的 主流 方法 包括 1 . 规则 系统 2 . 
基于 机器 学习 的 学习 系统 目前 的 挑战 包括 
新 领域 旧 实体 类别 识别 新 实体 类别 识别 
等 解决办法 包括 利用 构词 知识 领域 知识 使用 强化 
学习 跨 领域 学习 半 监督 学习 众包 远程 监督 
等 机器 学习 方法 句法分析 句法分析 的 任务 定义 为 
输入 一个 句子 的 词语 序列 输出 为 句 子结构 
表示 的 过程 依存 句法分析 输出 的 是 依存 句法树 
下面 以 依存 句法分析 为例 目前 采用 的 方法 包括 
基于 图 的 方法 即从 图中 搜索 得到 句法树 主要 
的 任务 在于 确定 每个 依存 弧 的 分值 基于 
转移 的 方法 即 通过 一 系列 移进 规约 的 
动作 得到 句法树 主要 任务 在于 基于 当前 状态 确定 
每个 动作 的 分值 现在 的 主流 做法 是 在 
上述 两者 的 基础 上 加入 深度 学习 的 方法 
语义分析 定义 是 将 文本 转换 为 可计算 的 知识 
表示 目前 学术界 语义 表达方法 包括 1 浅层 语义分析 2 
逻辑 语义分析 3 抽象 语义 表示 分析 篇章 分析 篇章 
的 定义 指 的 是 一 系列 连续 的 语 
段或/nr 句子 构成 的 语言 整体 单位 核心 问题 是 
篇章 结构 和 篇章 特征 其所 基于 的 语言学 基本 
理论 包括 中心 理论 脉络 理论 RST 等 多种 语言学 
基本 理论 基本 结构 分析 篇章 结构 指 的 是 
篇章 内部 关系 的 不同 结构化 表达形式 主要 包括 逻辑 
语言 结构 指代 结构 话题 结构 功能 结构 事件 结构 
等 范畴 基本 特征 的 研究 包括 连接性 连贯性 意图 
可接受性 信息性 情景 性 和跨/nr 篇章 等 七个 基本特征 自然语言 
生成 张民 教授 总结 了 在 基于 规则 基于 知识 
的 检索 及 基于 深度 学习 等 三种 自然语言 生成 
方法 的 优缺点 对比 及 适用 场景 基于 规则 它 
的 一大 优势 在于 具体 领域 的 能 做到 精准 
回答 但 相应 地 在 可移植性 及 可扩展性 上 则 
存在 不足 适用 的 场景 以 个人 助理 为主 和 
任务 驱动 型 的 对话 基于 知识 的 检索 它 
的 优点 在于 知识库 易于 扩充 答案 没有 语法错误 但 
对话 连续性 差 容易 出现 答非所问 的 情况 适用 场景 
以 问答 系统 娱乐 聊天 为主 基于 深度 学习 基于 
数据 驱动 的 方法 能够 省去 显示 语言 理解 等 
过程 但 需要 大量 语料 支持 适用 场景 以 虚拟 
影像 智能 聊天 机器 人 为主 的 有 丰富 领域 
语料 的 场景 自然语言 处理 的 应用 1 . 情感/n 
和/c 情绪/n 分析/vn 在/p 业界/n 研究/vn 和/c 应用/v 情感 一般 
包括 正面 负面 和 中性 而 情绪 一般 表现 为 
喜 怒 哀 乐 惊 恐 思 等 情绪 和 
情感 都是 人 对 客观 事物 所持 的 态度 体验 
只是 情绪 更 倾向 于 个体 基本 需求 欲望 上 
的 态度 体验 而 情感 则 更 倾向 于 社会 
需求 欲望 上 的 态度 体验 情感/n 和/c 情绪/n 分析/vn 
包括/v 问题/n 驱动/vn 和/c 模型/n 驱动/vn 两个方面/i 在/p 工业界/n 和/c 
学术界/n 都/d 已经/d 有着/v 广泛/a 的/uj 应用/v 和/c 研究/vn 2 
. 问答 智能 问答 主要有 三 方面 的 要求 一是 
理解 人类 语言 的 内涵 二 是 推敲 知识 获取 
的 意图 三 是 挖掘 精确 贴切 的 知识 相应 
地 问答 系统 需要 解决 三 个 问题 1 . 
问题 分类 分析 和 理解 一阶逻辑 二阶逻辑 2 . 答案 
的 匹配 检索 3 . 答案 生成 问答 的 四个 
难点 及 解决 方法 1 多源 异构 大 数据 背景 
下 开放 域 问答 的 瓶颈 在 效率 与 覆盖率 
的 权衡 下 数据 大小 与 知识 占 比 的 
关系 是 每个 研究 者 需要 考虑 的 问题 而 
结构化 数据 与非 结构化 数据 的 混杂 导致 知识 挖掘 
与 存 储存 在 相应 的 难点 此外 数据 时效性 
的 变化 也给 新旧 知识 的 应用 带来 了 挑战 
以往 是 用 IR 或 RC 的 方法 但 目前 
流行 采用 对 检索 所得 的 多个 段落 排序 也 
就是 在 IR 和 RC 中 加入 了 排序 的 
操作 进而 进行 面向 多 段落 的 提取 / 生成 
答案 2 深度 语义 理解 的 问答 技术 以 Watson 
为 代表 的 系统 采用 的 是 抽取 与 置信度 
计算 的 方法 目前 则 是 阅读 理解 抽取 / 
生成式 方法 推动 了 技术 发展 3 知识库 与 知识图谱 
以往 的 知识 库 存在 可靠性 包容性 低 存在 通用性 
不高 的 问题 目前 研究者 们 更多 考虑 用 当下 
热门 问题 自动 生成 来 实现 知识图谱 的 自动 更新 
和 扩展 4 多 模态 场景 下 的 问答 问题 
的 对象 往往 潜藏 于 多媒体 且 答案 的 判断 
需要 参考 其它 媒体 的 数据 资源 目前 出现 了 
以 语言 处理 RNN 与 图像 处理 的 CNN 的 
有机 结合 方法 实现 跨 媒体 的 特征 共享 独立 
和抗/nr 依赖 对话 根据 应用 场景 的 不同 可分为 开放 
域 及 封 闭域 对话 系统 高 准确率 的 上下文 
篇章 建模 对话 状态 转移 模型 和 领域 知识 建模 
是 目前 对话 亟待解决 的 问题 知识图谱 包括 知识 建模 
知识图谱 构建 知识 融合 知识 推理 计算 以及 知识 赋 
能等/nr 主要 任务 知识图谱 构建 是 目前 学术界 和 产业界 
研究 热点 包括 实体 及其 属性 识别 事件 抽取 实体 
事件 关系 抽取 概念 实例 化 和 规则 学习 等 
机器翻译 机器翻译 目前 已经 取得 较大 进展 未来 机器翻译 可以 
从 如下 领域 做 发展 知识 建模 和 翻译 引擎 
从 词序 列到 语义 到 知识 利用 知识图谱 和 各类 
知识 语言学 知识 领域 知识 常识 知识 等 进一步 延伸 
机器 翻译 的 边界 研究 新的 翻译 模型 从 广度 
篇章 和 深度 深度 理解 进一步 推进 机器 翻译 的 
理解 能力 此外 还 需要 适应 产业化 的 需求 和 
国家 战略 需求 转 自 2018 中国 人工智能 大会 专题 
论坛 自然语言 处理 是 人工智能 领域 中 的 一个 重要 
方向 它 研究 能 人机 之间 通讯 的 方式 并/c 
涉及/v 机器/n 对/p 人类/n 知识/v 体系/n 的/uj 学习/v 和/c 应用/v 
．/i 从/p 分词/n 相似 度 计算 情感 分析 文章 摘要 
到 学习 文献 知识 推理 都 涉及 自然语言 分析 ． 
下面 介绍 一些 中文 语言 语义分析 的 资源 ． 以下 
只 讨论 能 嵌入 到 我们 程序 里 的 资源 
1 .             同义词 词 
林 同义词 词 林 是 80 年代 出版 的 一本 
词典 这 提供 了 词 的 归类 相关性 信息 起始 
主要 用于 翻译 哈工大 对 它 进行 了 细化 和 
扩充 出了 词 林 扩展 版 其中 含有 7万 多词 
17000 多种 语义 五层 编码 ． 12 大类 94中 类 
1428 小 类 形如 Aa01A01 = 人 士 人物 人士 
人氏 人选 每 一个 条目 对应 一种 语义 根据 分类 
编号 第一位 大写 表示 大类 第二位 小写 表示 中 类 
其中 涉及 了 一 词 多义 和 一义 多词 ． 
词 林 扩展 版 网上 的 下载 很多 大小 不到 
1M 可以 直接 load 到 程序 中 用于 简单 的 
分词 文章 分类 模糊 查找 统计 情感 分析 不同 感情 
色彩 对应 不同 类别 号 等等 ． 2 .   
          哈工大 语言 云 LTP 中文 
的 语义分析 工具 大多数 都像 LTP 这样 提供 一个 在线 
的 分析器 一组 API 比较简单 稳定 的 功能 ． LTP 
是 其中 做得 比较好 的 ． 它 提供 了 中文分词 
词性 标注 命名 实体 识别 依存 句法分析 语义 角色 标注 
等 等功能 ． 但 对于 进一步 语义 方面 的 深入 
的 开发 用处 不大 而且 需要 连网 使用 速度/n 和/c 
处理/v 数量/n 上都/i 有/v 一些/m 限制/v ．/i 详见/a http / 
/ www . ltp cloud . com / demo3 . 
            结巴 分词 结巴 是 
一个 Python 的 中文分词 组件 ． 它 提供 了 分词 
和 词性 标注 功能 ． 能在 本地 自由 使用 是 
Python 实现 的 可以 很好 的 和 其它 Python 工具 
相结合 使用 方法 如下 # encoding = utf 8 import 
jieba . posseg as pseg import jieba seg _ list 
= jieba . cut 我 爱 北京 天安门 cut _ 
all = True print Full Mode / . join seg 
_ list words = pseg . cut 我 爱 北京 
天安门 for w in words print w . word w 
. flag 执行 结果 是 Full Mode 我 / 爱//nr 
北京 / 天安 / 天安门 我 r 爱 v 北京 
ns 天安门 ns 详见 http / / www . oschina 
. net / p / jieba / 4 .   
          知网 HowNet 对于 语言 的 
理解 人们 更 关注 语义 即 研究 文字 真正 的 
含义 是 什么 并/c 希望/v 机器/n 能像/nr 人脑/n 一样/r 把/p 
知识/v 组织/v 成/n 体系/n ．/i 中文/nz 语义/n 库/n 开放/v 的/uj 
资源/n 非常/d 少/a 现代汉语 语义 词典 中文 概念 辞书 这些 
都是/nr 听说 过 没 见过 总之 人家 是 不 开放 
. 就算 能去 书店 买 一本 也 用不到 程序 里 
. 我 在 网上 只 找到 了 HowNet 可以 在 
csdn 下载 压缩包 1.5 M 左右 . 形如 NO . 
= 069980 W _ C = 群众 G _ C 
= N E _ C = W _ E = 
the masses G _ E = N E _ E 
= DEF = human | 人 mass | 众 可以 
看到 它 包含 编号 中文 词 对应 英 文词 词性 
约 12万 多项 . HowNet 在 2013 年后 就不 更新 
了 以上/f 版本/n 差不多/l 是/v 能在/nr 网上/s 找到/v 的/uj 比较/d 
全的/nr 数据/n 了/ul ./i 它 还 提供 了 一些 库 
可 用于 判断 相似 度 等 ． 详见 http / 
/ www . keenage . com / html / c 
_ index . html5 .           
  NLTK 与 WordNet sentiwordnet WordNet 是 一个 语义 词典 
NLTK 是 Python 的 一个 自然 语言 处理 工具 它 
提供 了 访问 WordNet 各种 功能 的 函数 WordNet 形如 
n 03790512 0 0 motorcycle # 1 bike # 1 
a motor vehicle with two wheels and a strong frame 
其中 含有 词性 编号 语义 词汇 间 的 关系 同义 
/ 反义 上行 / 下行 整体 / 部分 大家 都 
觉得 ＂ 它 很棒 只 可惜 没有 中文 支持 ＂ 
. 其实 也 不是 没 中文 支持 . WordNet 有 
中文 以及 其它 更多 语言 的 支持 可以 从 以下 
网址 下载 http / / globalwordnet . org / wordnets 
in the world / 其中 的 数据 文件 形如 03790512 
n cmn lemma 摩托车 可以 看到 它 与 sentiwordnet 的 
词条 编号 一致 尽管 对应 可能 不是 特别 完美 但 
理论上 是 对 英文 能做 的 处理 对 中文 也 
能做 ． NLTK + WordNet 功能 非常 丰富 强烈推荐 PYTHON 
自然语言 处理 NLTK Natural L a n g u a 
g e P r o c e s s i 
n g with Python 这本书 它 已由 爱好者 译成 中文版 
可从 网上 下载 ． 里面 不但 讨论 了 具体 的 
实现 方法 还 讨论 了 一些 研究 方向 比如 ＂ 
从 自然语言 到 一阶逻辑 ＂ 6 .       
      随想 对 语言 的 处理 首先 是 
分词 然后 是 消 歧 判断 词 在 句中 的 
成份 识别 语义 ． 形成 知识 网络 ． ． ． 
希望 最终 机器 能像/nr 人类 一样 学习 思考/v 和/c 创造/v 
．/i 语言/n 处理/v 在/p 不同/a 的/uj 层次/m 有/v 不同/a 的/uj 
应用/v 从 文章 分类 内容 提取 到 自动 诊断 病情 
IBM Watson 或者 存在 更 通用 的 逻辑 使 机器 
成为 比 搜索引擎 更 智能 的 各个 行业 的 专家 
系统 ． 自然 语言 和 语义 看似 多对 多 的 
关系 我/r 觉得/v 本质上/i 语义/n 转换成/l 语言/n 是从/v 高维/nr 到/v 
低/a 的/uj 投影/n ．/i 从词林/nr 的/uj 分类/n 看/v 真正 核心 
的 概念 并不 太多 但是 语义 的 关系 和 组合 
很复杂 再 深 层次 还 涉及 知识 线 等等 ． 
而 语言 只是 它 的 表象 ． 在 分析 过程 
中 越/d 拟合/v 那/r 表象/n 差得 越多 ． 另外 这一 
领域 已经 有 几十年 的 历史 了 学习 时 尽可能 
利用 现有 工具 把 精力 集中 在 目标 而 非 
具体 过程 ． 多 参考 人家 都 实现 了 什么 
功能 人家 的 数据 是 怎么 组织 的 ． 记录 
文字 处理 的 各种 简介 的 代码 表示 1 . 
快速 去除 中文标点 read 的 时候 要以 utf8 格式 def 
clean _ str string string = re . sub ^ 
\ u4e00 \ u9fff string string = re . sub 
r \ s { 2 } string # 合并 多个 
空格 为 一个 return string . strip 2 . 快速 
分词 默认 一 行为 一 样本 def seperate _ line 
string return . join word + for word in jieba 
. cut string f = open xxx r encoding = 
utf8 lines = list f . readlines lines = clean 
_ str seperate _ line line for line in lines 
3 . 分行 使得 一 行为 一句 for line in 
lines line . replace \ n . replace \ n 
. replace \ n . replace \ n . replace 
\ n 重新 写入 4 . 语料 训练 集 生成 
def load _ positive _ negative _ data _ files 
positive _ data _ file _ path negative _ data 
_ file _ path positive _ example _ lists = 
read _ and _ clean _ zh _ file positive 
_ data _ file _ path # positive _ example 
_ lists 0 维度 上为 样本 有 多少 句 句子 
1 维度 上为 每句 的 string 单词 间 空格 隔开 
negative _ example _ lists = read _ and _ 
clean _ zh _ file negative _ data _ file 
_ path # positive _ example _ lists 形式 同上 
# Combine data x _ text = positive _ example 
_ lists + negative _ example _ lists # Generate 
labels positive _ labels = 1 for _ in positive 
_ example _ lists negative _ labels = 0 for 
_ in negative _ example _ lists y = np 
. concatenate positive _ labels negative _ labels 0 return 
x _ text y 5 . 句子 填充 def padding 
_ sentences input _ sentences padding _ token padding _ 
sentence _ length = None sentences = sentence . split 
for sentence in input _ sentences if padding _ sentence 
_ length = None max _ sentence _ length = 
padding _ sentence _ length else max _ sentence _ 
length = max len sentence for sentence in sentences for 
i sentence in generate sentences if len sentence max _ 
sentence _ length sentences i = sentence max _ sentence 
_ length else sentence . extend padding _ token * 
max _ sentence _ length len sentence return sentences max 
_ sentence _ length 6 . 从 gensim 训练 模型 
拿 词 向量 model 加载 all _ vectors = embeddingDim 
= w2vModel . vector _ size embeddingUnknown = 0 for 
i in range embeddingDim for sentence in sentences this _ 
vector = for word in sentence if word in w2vModel 
. wv . vocab this _ vector . append w2vModel 
word else this _ vector . append embeddingUnknown all _ 
vectors . append this _ vector return all _ vectors7 
. 打乱 np 矩阵 的 方法 x = 0 1 
2 3 4 5 6 x = np . array 
x np . random . seed 10 shuffle _ indices 
= np . random . permutation np . arange len 
x print shuffle _ indices x _ shuffled = x 
shuffle _ indices print x _ shuffled 输出 2 6 
0 3 4 5 1 2 6 0 3 4 
5 1 8 . 分离 部分 样本 为 训练 集 
和 验证 集 1 . 打乱 样本 顺序 参考 上面 
代码 2 . 按比例 截断 本文 为 http / / 
blog . sina . com . cn / s / 
blog _ 1 3 3 4 c a e 8 
1 0 1 0 2 w o v b . 
html 笔记 自然语言 处理 常用 术语 文本 主要 分为 三种 
文本 自由 文本 结构化 文本 半 结构化 文本 自然语言 处理 
一般 是 对 自由 文本 进行 的 处理 常见 的 
基本 操作 如下 分词 通常 我们 处理 的 自由 文本 
分为 中文 英文 等 词 为 文本 最 基本 的 
单位 分词 是 进行 自然语言 处理 中 最 基本 的 
步骤 分词 算法 分为 词典 方法 和 统计 方法 其中 
基于 词典 和 人工 规则 的 方法 是 按照 一定 
的 策略 将 待 分析 词 与 词典 中的 词条 
进行 匹配 正向 匹配 逆向 匹配 最大 匹配 统计 方法 
是 基本 字符 串在 语料库 中 出现 的 统计 频率 
典型 的 算法 有 HMM \ CRF 等 其中 CRF 
相比 HMM 有 更弱 的 上下文 无 相关性 假设 理论 
上 效果 更好 一点 英文 以 空格 为 分割 符 
因此 不 需要 进行 分词 的 操作 这是 片面 的 
对于 一些 特殊 情况 依旧 需要 分词 的 操作 例如 
it s 等 另外 对于 英 文中 复合词 的 情况 
也 需要 进行 一定 的 识别 因此 在 进行 关键词 
识别 的 时候 会 运营 到 分词 的 一些 技术 
中文 的 分词 工具 有 很多 近年来 常用 的 是 
jieba 和 stanford corenlp 等 词性 标注 在 进行 词性 
标注 时 需 先 定义出 词性 的 类别 名词 动词 
形容词 连词 副词 标点符号 等 词性 标注 是 语音识别 句法分析 
信息 抽取 技术 的 基础 技术 之一 词性 标注 是 
标注 问题 可以 采用 最大熵 HMM 或 CRF 等 具体 
算法 进行 模型 的 训练 自动 问答 系统 中 为了 
提高 用户 问题 匹配 后端 知识库 的 召回率 对 一些 
关键词 进行 了 过滤 包括 连词 副词 对于 全文 检索系统 
理论 上 可以 通过 对 用户 输入 的 查询 条件 
进行 词性 过滤 但 由于 全文检索 是 基于 词 袋 
模型 的 机械 匹配 并且 采用 IDF 作为 特征值 之一 
因此 词性 标注 的 效果 不大 句法分析 句法分析 的 目的 
是 确定 句子 的 句法结构 主谓宾 动宾 定 中 动 
补 等 在/p 问答/v 系统/n 和/c 信息检索/n 领域/n 有/v 重要/a 
的/uj 作用/v 命名 实体 识别 命名 实体 识别 是 定位 
句子 中 出现 的 人名 地名 机构 名 专有名词 等 
命名 实体 属于 标注 问题 因此 可以 采用 HMM \ 
CRF 等 进行 模型 的 训练 基于 统计 的 命名 
实体 识别 需要 基于 分词 词性 标注 等 技术 命名 
实体 定义 了 五大类 类型 设施 FAC \ 地理 政治 
实体 GPE \ 位置 LOC \ 人物 PER 在 实际 
应用 中 可以 根据 自己 的 业务 需求 定义 实体 
类别 并 进行 模型 训练 实体 关系 抽取 实体 关系 
抽取 是 自动 识别 非 结构化 文档 中 两个 实体 
之间 的 关联关系 属于 信息 抽取 领域 的 基础 知识 
之一 近年来 搜索 领域 流行 的 知识图谱 技术 是 构建 
实体 关系 实体 关系 抽取 有 多种 方式 包括 规则 
匹配 有 监督 学习 无 监督 学习 其中 有 监督 
学习 需要 预先 定义 实体 关系 类别 并 通常 将 
问题 建模 为 分类 问题 有 监督 学习 需要 预先 
人工 标注 语料库 作者 Virginia5 来源 CSDN 原文 https / 
/ blog . csdn . net / Virginia5 / article 
/ details / 68060563 版权 声明 本文 为 博主 原创 
文章 转载 请 附上 博文 链接 参考 书籍 Python 自然语言 
处理 书籍 中 的 版本 是 Python2 和 NLTK2 我 
使用 的 版本 是 Python3 和 NLTK3 实验 环境 Windows8 
. 1 已有 Python3 . 4 并 安装 了 NumPy 
Matplotlib 参考 http / / blog . csdn . net 
/ monkey131499 / article / details / 50734183 安装 NLTK3 
Natural Language Toolkit 自然语言 工具包 地址 http / / www 
. nltk . org / 安装 命令 pip install nltk 
安装 完成 后 测试 import nltk 没有 报错 即 表明 
安装 成功 NLTK 包含 大量 的 软件 数据 和 文档 
可以 进行 文本 分析 和 语言 结构 分析 等 数据 
资源 可以 自行 下载 使用 地址 http / / www 
. nltk . org / data . html 数据 列表 
http / / www . nltk . org / nltk 
_ data / 下载 NLTK Data 在 Python 中 输入 
命令 import nltk nltk . download 弹出 新的 窗口 用于 
选择 下载 的 资源 点击 File 可以 更改 下载 安装 
的 路径 all 表示 全部 数据 集合 all corpora 表示 
只有 语料库 和 没有 语法 或 训练 的 模型 book 
表示 只有 书籍 中 例子 或 练习 的 数据 需要 
注意 一点 就是 数据 的 保存 路径 要么 在 C 
盘中 要么 在 Python 的 根目录 下 否则 后面 程序 
调用 数据 的 时候 会 因为 找 不到 而 报错 
注意 软件 安装 需求 Python NLTK NLTK Data 必须 安装 
NumPy 和 Matplotlin 推荐 安装 NetworkX 和 Prover9 可选 安装 
简单 测试 NLTK 分词 功能 但是 在 词性 标 注上 
就 出现 问题 了 百度 也 没有 明确 的 解决 
办法 若有 大神 知道 是 什么 原因 请 不吝赐教 词性 
标注 功能 就 先 暂且 放 一放 下面 看一下 NLTK 
数据 的 几种 方法 1 . 加载 数据 from nltk 
. book import * 2 . 搜索 文本 print text1 
. concordance monstrous 3 . 相似 文本 print text1 . 
similar monstrous 4 . 共用 词汇 的 上下文 print text2 
. common _ contexts monstrous very 5 . 词汇 分布图 
text4 . dispersion _ plot citizens democracy freedom duties America 
6 . 词汇 统计 # encoding = utf 8 import 
nltk from nltk . book import * print ~ ~ 
~ ~ ~ ~ ~ ~ ~ ~ ~ ~ 
~ ~ ~ ~ ~ ~ ~ ~ ~ ~ 
~ ~ ~ print 文档 text3 的 长度 len text3 
print 文档 text3 词汇 和 标识符 排序 sorted set text3 
print 文档 text3 词汇 和 标识符 总数 len set text3 
print 单个 词汇 平均 使用 次数 len text3 * 1.0 
/ len set text3 print 单词 Abram 在 text3 中 
使用 次数 text3 . count Abram print 单词 Abram 在 
text3 中 使用 百分率 text3 . count Abram * 100 
/ len text3 暂时 先 练习 到 这里 基本上 对 
NLTK Data 有了 一定 的 了解 以及 学会 了 其 
基本 使用 方法 本 博客 主要 是 对 网络 上 
的 一些 关于 英文 自然语言 处理 开源 工具 的 博客 
进行 整理 汇总 如果 有 涉及 到 您 的 知识 
产权 等 请 联系 本人 已 进行 修改 也 欢迎 
广大 读者 进行 指正 以及 补充 本 博客 将 尽量 
从 工具 的 使用 语言 功能 等 方面 进行 汇总 
介绍 1 斯坦福大学 语言 Java 功能 分词 词性 标注 命名 
实体 识别 语法 解析 分类 介绍 Stanford NLP Group 是 
斯坦福 大学 自然 语言 处理 的 团队 开发 了 多个 
NLP 工具 官网 网址 由于 该 团队 将该 开源 分为 
多 个子 模块 以下 将 逐一 进行 介绍 1.1   
Stanford Word Segmenter 介绍 采用 CRF 条件 随 机场 算法 
进行 分词 也是 基于 Java 开发 的 同时 可以 支持 
中文 和 Arabic 官方 要求 Java 版本 1.6 以上 推荐 
内存 至少 1G 下载 地址 示例代码 java   view plain 
  copy / / 设置 分词器 属性 Properties   props 
  =   new   Properties / / 字典 文件 
地址 可以 用 绝对路径 如 d / dataprops . setProperty 
s i g h a n C o r p 
o r a D i c t   data / 
/ 字典 压缩包 地址 可以 用 绝对路径 props . setProperty 
serDictionary data / dict chris6 . ser . gz / 
/ 输入 文字 的 编码 props . setProperty inputEncoding   
UTF 8 props . setProperty s i g h a 
n P o s t P r o c e 
s s i n g   true / / 初始化 
分词器 CRFClassifier   classifier   =   new   CRFClassifier 
props / / 从 持久化 文件 中 加载 分词器 设置 
classifier . l o a d C l a s 
s i f i e r N o E x 
c e p t i o n s data / 
ctb . gz   props / /   flags   
must   be   re set   after   data 
  is   loadedclassifier . flags . setProperties props / 
/ 分词 List   words   =   classifier . 
segmentString 语句 内容 1.2   Stanford POS Tagger 介绍 采用 
Java 编写 的 面向 英文 中文 法语 阿拉伯语 德语 的 
命名 实体 识别 工具 下载 地址 1.3   Stanford Named 
Entity Recognizer 介绍 采用 条件 随 机场 模型 的 命名 
实体 工具 下载 地址 1.4 Stanford Parser 介绍 进行 语法分析 
的 工具 支持 英文 中文 阿拉伯文 和 法语 下载 地址 
1.5   Stanford Classifier 介绍 采用 Java 编写 的 分类器 
下载 地址 最后 附上 关于 中文 分词器 性能 比较 的 
一篇 文章 2014 . 05.27 更新 1.6 Stanford CoreNLP 功能 
分词 词性 标注 命名 实体 识别 语法分析 介绍 采用 Java 
编写 的 面向 英文 的 处理 工具 下载 网址 用户 
评价 采用 它 进行 英语 单词 的 词性 还原 具体 
应用 详见 文章 采用 Stanford CoreNLP 实现 英文单词 词形 还原 
一 会计 文本 分析 随着 人工智能 自然语言 处理 的 发展 
近年 文本 信息 逐渐 成为 国外 会计 实证 研究 的 
热点 许多 学者 开始 致力于 运用 文本 分析 方法 来 
解决 会计 与 财务 问题 并 取得 了 众多 有 
价值 的 研究 成果 这里 鄙人 浅谈 一下 二者 的 
联系 以及 如何 运用 会计 文本 一般指 由 公司 发布 
的 具有 会计 相关性 的 文本 信息 关注 对象 还包括 
分析师 研究报告 媒体 新闻 报道 互联网 论坛 上 的 帖子 
具体来说 狭义 会计 文本 还包括 公司 披露 的 年报 季报 
招股 说明书 季度 盈余 公告 管理层 盈余 预告 以及 电话 
会议纪要 文本 广义 的 会计 文本 还包括 分析师 的 研究 
报告 媒体 的 新闻 报道 以及 投资者 通过 各种 渠道 
发表 的 观点 与 评论 总结 的 现有 的 文献 
我们 可以 发现 已被 量化 的 会计 文本 特征 有 
九个 按照 是否 与 内容 相关 可以 分为 两类 显然 
语调 可读性 重复性 管理 者 特征 与 文本 内容 无关 
而 风险 竞争 虚假性 融资 约束则 属于 文本 内容 的 
一部分 语调 语调 是 会计 文本 分析 最 基本 的 
特征 有 乐观 或者 悲观 正面 或 负面 积极 或 
消极 两种 对立 的 感觉 构成 中性 语调 可 视为 
第三种 语调 因为 大部分 词句 既不 乐观 也不 悲观 字 
典法 是 度量 语调 的 基本 方法 研究者/n 通过/p 对/p 
乐观/a 和/c 悲观/a 两类/m 单词/n 进行/v 词频/n 统计/v 和/c 比较/d 
得到/v 文本/n 整体/n 语调/n 另一种 度量 方法 是 朴素 贝叶斯 
算法 文本 分析 方法 一 字 典法 字 典法 实质上 
是 一种 词频 统计法 它 基于 预设 的 字典 和 
规则 将 目标 文档 中 的 单词 逐一 映射 到 
各个 集合 中 经过 统计 计算 得到 文本 的 量化 
特征 字典 可分为 通用 专用 和 自编 三种 类型 通用性 
字典 广泛 应用于 众多 研究领域 而 不限于 会计 研究 由于 
某些 词汇 在 会计 用语 有其 特殊 的 含义 因此 
通用 类 字典 的 识别 能力 不强 二 机器学习 算法 
机器学习 方法 的 本质 是 一种 统计算法 具有 类似 于 
人工智能 的 自动 学习 能力 学习 过程 是 利用 培训 
样本 进行 反复 训练 从而 获得 有 文本处理 功能 的 
数学 模型 研究者 将 目标 文本 输入 该 数学模型 即 
可输入 文本 的 量化 特征 自然 语言 与 会计 的 
运用 利用 NLP 技术 来 探索 一种 新型 的 会计 
信息 系统 核算 和 审计 信息 系统 的 新模式 企业 
会计 信息 主要 如实 反映 企业 的 真实 的 经济 
活动 状况 客观 的 核算 以及 预测 未来 的 经济 
发展 趋势 利用 NLP 技术 来 拓展 审计 电算化 的 
新思路 自然语言 会计核算 原则 和 自然 处理 系统 的 基本 
框架 建立 经济 事项 和 会计 语言 对照 信息 库 
对 会计 经济 事项 自动 生成 会计分录 自动 生成 会计凭证 
以及 相关 重要 事项 语言 文字 是 人类 社会 信息 
的 主要 载体 自然语言 处理 系统 包括 自然 语言 入 
机 接口 机器翻译 文献检索 自动 文摘 自动 校对 语音识别 也 
合成 字符识别 等等 计算机 审计 业务 主要 关注 对 审计 
单位 电子 数据 的 取得 和 分析 计算 等 数据 
处理 的 业务 还 称不上 信息系统 审计 从 财务 报表 
审计 的 角度 来看 这一 阶段 的 主要 业务 内容 
是 对 交易 金额 和 账户 报表 余额 进行 检查 
属于 审计 程序 的 实质性 测试 环节 人工智能 一直 是 
个 很 火 的 词 被 称为 新的 风口 未来 
的 趋势 总之 就是 很 有 前瞻性 很 未来 的 
概念 但 其实 它 并不 那么 未来 我们 生活 中 
其实 每天 都 在用 人工智能 我们 先 明确 下 人工智能 
的 定义 人工智能 是 计算机 科学 的 一个 分支 它 
企图 了解 智能 的 实质 并 生产 出 一种 新的 
能以 人类 智能 相似 的 方式 做出 反应 的 智能 
机器 该 领域 的 研究 包括 机器人 语言识别 图像识别 自然语言 
处理 和 专家 系统 等 所以 人工智能 其实 就是 计算机 
科学 的 一个 分支 将来 也是 会 成为 人类 社会 
基础 设施 的 一部分 现在 让 我们 从头开始 学习 人工智能 
这里有 800G 的 人工智能 学习 资料 如果 你 想 站在 
时代 的 转折点 上 成为 历史 的 见证 者 请 
认真 学习 这份 学习 资料 以下 是 资料 概览 40G 
人工智能 入门 课 Python 语言 入门 课 25G 机器学习 教程 
资料 还 包括 谷歌 人工智能 学习 系统 TensorFlow 教程 华盛顿大学 
规模 系统 和 算法 的 数据 操作 课 1.8 G 
斯坦福 NLP 课程 . . . . . . . 
. . . . 学习 人工智能 资料 基本 都在/nr 这儿 
了 下载 链接 https / / pan . baidu . 
com / s / 1 J q i t Y 
J l s Y 1 h 8 J t 6 
z J v W P V Q   密码 o2bu 
更多 资料 欢迎 关注 公众 号 OFweek 机器 人网 ofweekrobot 
百度 词汇 自然语言 处理 是 计算机 科学 领域 与 人工智能 
领域 中 的 一个 重要 方向 它/r 研究/vn 能/v 实现/v 
人/n 与/p 计算机/n 之间/f 用/p 自然/d 语言/n 进行/v 有效/a 通信/l 
的/uj 各种/r 理论/n 和/c 方法/n 自然语言 处理 是 一门 融 
语言学 计算机科学 数学 于 一体 的 科学 因此 这一 领域 
的 研究 将 涉及 自然语言 即 人们 日常 使用 的 
语言 所以 它 与 语言学 的 研究 有着 密切 的 
联系 但又 有 重要 的 区别 自然语言 处理 并 不是 
一般 地 研究 自然语言 而在于 研制 能 有效 地 实现 
自然 语言 通信 的 计算机 系统 特别 是 其中 的 
软件 系统 因而 它 是 计算机 科学 的 一部分 自然语言 
处理 NLP 是 计算机 科学 人工智能 语言学 关注 计算机 和 
人类 自然 语言 之间 的 相互 作用 的 领域 随着 
深度 学习 的 发展 LSTM 的 应用 取得 的 突破 
极大 地 促进 了 NLP 的 发展 自然语言 处理 的 
主要 范畴 有 以下 文本 朗读 Text to speech / 
语音合成 Speech synthesis 语音识别 Speech recognition 中文 自动 分词 Chinese 
word segmentation 词性 标注 Part of speech tagging 句法分析 Parsing 
自然语言 生成 Natural language generation 文本 分类 Text categorization 信息检索 
Information retrieval 信息 抽取 Information extraction 文字 校对 Text proofing 
问答 系统 Question answering 给 一句 人类 语言 的 问 
定 决定 其 答案 典型 问题 有 特定 答案 像是 
加拿大 的 首都 叫什么 但也 考虑 些 开放式 问句 像是 
人生 的 意义 是 是 什么 机器翻译 Machine translation 将 
某种 人类 语言 自动 翻译 至 另一 种 语言 自动 
摘要 Automatic summarization 产生 一段 文字 的 大意 通常用于 提供 
已知 领域 的 文章 摘要 例如 产生 报纸 上 某 
篇文章 之 摘要 文字 蕴含 Textual entailment 自然语言 处理 目前 
研究 的 难点 单词 的 边界 界 定在 口语 中 
词 与 词 之间 通常 是 连贯 的 而 界定 
字词 边界 通常 使用 的 办法 是 取用 能让 给定 
的 上下文 最为 通顺 且 在 文法上 无误 的 一种 
最佳 组合 在 书写 上 汉语 也 没有 词 与 
词 之间 的 边界 词义 的 消 岐 许 多字词 
不单 只有 一个 意思 因而 我们 必须 选 出使 句 
意 最为 通顺 的 解释 句法 的 模糊性 自然 语言 
的 文法 通常 是 模棱两可 的 针对 一个 句子 通常 
可能会 剖析 Parse 出 多棵 剖析 树 Parse Tree 而 
我们 必须 要 仰赖 语意 及 前后文 的 资讯 才能 
在 其中 选择 一棵 最为 适合 的 剖析 树 有/v 
瑕疵/n 的/uj 或不/i 规范/n 的/uj 输入/v 例如/v 语音/n 处理/v 时/n 
遇到/v 外国/ns 口音/n 或/c 地方/n 口音/n 或者 在 文本 的 
处理 中 处理 拼写 语法 或者 光学 字元 识别 OCR 
的 错误 语言 行为 与 计划 句子 常常 并不 只是 
字面 的 意思 例如 你 能把 盐 递 过来 吗 
一个 好 的 回答 应当 是 动手 把 盐 递过去 
在 大多数 上下文 环境 中 能 将 是 糟糕 的 
回答 虽说 回答 不 或者 太远 了 我 拿不到 也 
是 可以 接受 的 再者 如果 一门 课程 去年 没 
开设 对于 提问 这门 课程 去年 有 多少 学生 没 
通过 回答 去年 没开 这门 课 要比 回答 没人 没 
通过 好 当前 自然语言 处理 研究 的 发展 趋势 第一 
传统 的 基于 句法 语义 规则 的 理性主义 方法 受到 
质疑 随着 语料库 建设 和 语料库 语言学 的 崛起 大 
规模 真实 文本 的 处理 成为 自然 语言 处理 的 
主要 战略 目标 第二 统计 数学方法 越来越 受到 重视 自然语言 
处理 中 越来越 多 地 使用 机器 自动 学习 的 
方法 来 获取 语言 知识 第三 浅层 处理 与 深层 
处理 并重 统计 与 规则 方法 并重 形成 混合式 的 
系统 第四 自然语言 处理 中 越来越 重视 词汇 的 作用 
出现 了 强烈 的 词汇 主义 的 倾向 词汇 知识库 
的 建造 成为 了 普遍 关注 的 问题 第五 统计 
自然语言 处理 统计 自然语言 处理 运用 了 推测学 机率 统计 
的 方法 来 解决 上述 尤其 是 针对 容易 高度 
模糊 的 长串 句子 当 套用 实际 文法 进行 分析 
产生 出 成千上万 笔 可能性 时所/nr 引发 之 难题 处理 
这些 高度 模糊 句子 所 采用 消 歧 的 方法 
通常 运用 到 语料库 以及 马 可夫 模型 Markov models 
统计 自然语言 处理 的 技术 主要 由 同样 自 人工智能 
下 与 学习 行为 相关 的 子 领域 机器学习 及 
资料 采掘 所 演进 而成 转 自 维基百科 RNN 语言 
模型 RNN 语言 模型 语言 模型 RNN 语言 模型 模型 
扩展 语言 模型 语言 模型 就是指 语言产生 的 规律 一般 
用来 预测 所 使用 语言 语序 的 概率 或者 是 
当前 上下文 使用 某 个 词语 的 概率 换句话说 就是 
用来 表示 语言产生 顺序 的 建模 用 某个 词 是否 
恰当 这样 的 语序 构造 句子 是否 妥当 这样 的 
于是 训练 出 一个 语言 模型 就 需要 相当大 的 
样本数据 语言 模型 可以 分为 文法 型 的 语言 模型 
就是 定义 相关 的 文法 结构 例如 主语 + 谓语 
+ 宾语 构成 陈述句 这样 的 统计模型 神经 网络 语言 
模型 其中 统计 类 的 语言 模型 包括 N gram 
N pos 隐 马尔科夫 链 模型 最大熵 模型 等 就是 
给出 前边 的 词 判断 后面 出现 词 的 概率 
p w3 | w1w2 p w _ 3 | w 
_ 1w _ 2 表示 w3w _ 3 在 词语 
w1w2w _ 1w _ 2 之后 出现 的 概率 具体 
计算公式 为 p w3 | w1w2 = p w1w2w3 p 
w1w2 = Count w1w2w3 Count w1w2 p w _ 3 
| w _ 1w _ 2 = \ frac { 
p w _ 1w _ 2w _ 3 } { 
p w _ 1w _ 2 } = \ frac 
{ Count w _ 1w _ 2w _ 3 } 
{ Count w _ 1w _ 2 } Count x 
表示 x 在 语料库 中 出现 的 频率 这种 模型 
能给 出 后面 单词 发生 的 概率 但是 会 出现 
Count x = 0 的 情况 为 避免 这种 问题 
出现 了 很多 平滑 技术 例如 Laplace 平滑 等 但是 
这种 统计模型 的 计算 非常 消耗 内存 RNN 语言 模型 
RNN 语言 模型 就是 利用 RNN 神经 网络 对 语言 
建模 用于 描述语言 序列 的 产生 过程 RNN 神经 网络 
就是 循环 神经网络 能 很好 地 拟合 序列 数据 假设 
当前 你 有 大量 文本 语料库 C 根据 这个 预料 
你 构建 了 词典 V 然后 你 做 分句 把 
每句话 通过 扩展 变成 等长 的 句子 句子 开始 以 
START 标志 结 束以 EOS 结束 使用 PAD 来 进行 
短 句子 的 填充 现在 得到 长度 为 L 的 
sequence 序列 每个 词 使用 vector 进行 表示 1 of 
N model 序 列为 x1 x2 . . . xLx 
_ 1 x _ 2 . . . x _ 
L 假设 x1x _ 1 是 词典 V 中 的 
第一 个 词 V 的 大小 为 N 则 x1 
= 1 0 0 . . . 0 x _ 
1 = 1 0 0 . . . 0 . 
对于 RNN 输出 数据 对应 的 True Value 这里 选择 
使用 x2 x3 . . . xL − 1 EOSx 
_ 2 x _ 3 . . . x _ 
{ L 1 } EOS 使用 符号 表示 为 y1 
y2 . . . yLy _ 1 y _ 2 
. . . y _ L 对于 RNN 预测 数据 
表示 为 y ′ 1 y ′ 2 . . 
. y ′ Ly _ 1 y _ 2 . 
. . y _ L ht = f whh ∗ 
ht − 1 + wxh ∗ xt \ begin { 
equation } h _ t = f w _ { 
hh } * h _ { t 1 } + 
w _ { xh } * x _ t \ 
end { equation } y ′ t = g ht 
\ begin { equation } y _ t = g 
h _ t \ end { equation } y ′ 
ty _ t 是 一个 N 维 词典 的 大小 
向量 表示 一个 概率分布 即 下 一个 词语 出现 的 
概率 在 词典 中的 概率分布 y ′ t n y 
_ t n 表示 下 一个 词 是 词典 中 
第 n 个 词 的 概率 大小 损失 函数 定义 
维 Loss = − 1L ∑ t = 1L ∑ 
j = 1Nyt j log y ′ t j \ 
begin { equation } Loss = \ frac { 1 
} { L } \ sum _ { t = 
1 } ^ { L } \ sum _ { 
j = 1 } ^ { N } y _ 
t j log y _ t j \ end { 
equation } 求导 根据 BackPropogation + SGD 进行 训练 最小化 
损失 函数 模型 扩展 一般 对于 RNN 的 训练 采用 
BPTT 的 算法 当 L 较大 时 模型 的 训练 
会 出现 梯度 消 失和 梯度 爆炸 的 问题 对于 
梯度 爆炸 可以 采取 Clipping 的 方法 解决 具体 就是 
设置 门限 超过 这个 门 限时 进行 该 梯度 方 
向上 的 归一化 对于 梯度 消失 可以 采用 LSTM 或 
GRU 来 替代 SRNN 或者 使用 ReLU 来 替代 Sigmoid 
激励函数 Python 自然语言 处理 一 NLTK 及 语料库 下载 参考 
网站 http / / www . nltk . org / 
NLTK 是 用来 进行 自然语言 处理 很 强大 的 包 
本文 介绍 Python 下 安装 NLTK 及 语料 下载 1 
. 安装 NLTKpip install nltk 如果 已经 安装 了 Anaconda 
则 默认 安装 了 nltk 但是 没有 安装 语料库 2 
. 自动 安装 语料库 如果 在 引入 nltk 包后/nr 发现 
没有 安装 语料库 则 可以 自动 下载安装 命令 import nltk 
nltk . download showing info https / / raw . 
g i t h u b u s e r 
c o n t e n t . com / 
nltk / nltk _ data / gh pages / index 
. xmlTrue3 . 手动 导入 语料库 由于 自动 安装 语料库 
会 耗费 很大 时间 可以 手动 导入 语料库 语料库 下载 
地址 百度 云盘 http / / pan . baidu . 
com / s / 1hswoU5u 下载 后的/nr 语料库 可以 导入到 
以下 目录 / home / zhanghc / nltk _ data 
/ usr / share / nltk _ data / usr 
/ local / share / nltk _ data / usr 
/ lib / nltk _ data / usr / local 
/ lib / nltk _ data 4 . NLTK 安装包 
及 语料库 安装 完成 import nltk # NLTK 自带 的 
语料库 展示 from nltk . corpus import brownbrown . categories 
u adventure u belles _ lettres u editorial u fiction 
u government u hobbies u humor u learned u lore 
u mystery u news u religion u reviews u romance 
u science _ fiction len brown . sents 57340len brown 
. words 11611925 . NLTK 常用 函数 NLP 自然语言 处理 
五 不可思议 的 机器 翻译 除了 震惊 还是 震惊 的 
机器 翻译 . 如果 你 认为 机器翻译 就是 英译汉 汉译英 
那么 你 落伍 了 以下 是 基于 神经网络 机器 翻译 
技术 的 机器人 写 的 一首 诗 转载 请 注明 
出处 谢谢 MOOCsMIT 的 Natural Language P r o c 
e s s i n g t a n f 
o r d 的 cs224n Natural Language P r o 
c e s s i n g t a n 
f o r d 的 CS224d Deep Learning for Natural 
Language Processing 讲述 深度 学习 在 自然 语言 处理 方面 
比较 成功 的 应用 BOOKs 入门 首选 吴军 博士 的 
数学 之美 深入浅出 的 讲述 了 数学 在 计算机 领域 
的 应用 体现 了 数学 的 简单 美 书中 主要 
涉及 了 自然 语言 处理 的 一些 内容 宗 成庆 
的 统计 自然语言 处理 全面 介绍 了 统计 自然语言 处理 
的 基本 概念 理论 方法 和 最新 研究 进展 尤其 
是 讲述 了 中文 的 自然 语言 处理 统计 自然语言 
处理 基础 统计 自然语言 处理 的 一本 著作 Python 自然语言 
处理 主要 讲 NLTK 这个 包的/nr 使用 LibsNLTK Python 的 
自然 语言 处理 包 自然语言 处理 相关 学习 资料 转 
book 宗 成庆 . 统计 自然语言 处理 . 清华大学出版社 . 
2008 . 此书 为 统计 观点 适合 CS 背景 做 
NLP 的 人 读 2 . Manning C . D 
Foundations of Statistical Natural Language Processing . MIT Press . 
1999 . 冯志伟 . 自然语言 处理 的 形式 模型 . 
中国 科技 大学 出版社 . 2010 . 此书 讲 涵盖 
句法 语义 各个 层面 ps 作者 是从 Linguistic 角度 去 
分析 自然语言 处理 Model Yoshua Bengio . A Neural Probabilistic 
Language Model . JMLR 2003 . 2003 . 神经 网络 
语言 模型 的 开山 之作 MileStone 论文 引 用率 634 
Google Scholar Frederic Morin Yoshua Bengio . Hierarchical Probabilistic Neural 
Network Language Model . Innovations in Machine Learning 2006 . 
2006 . 提出 了 Hierarchical NPLMAndriy Mnih Geoffrey Hinton . 
Three New Graphical Models for Statistical Language Modelling . ICML 
2007 . 2007 . 提出 了 三个 Model 其 中提 
的 较多 的 是 A Log Bilinear Language Model 后续 
论文 多 引 用此 模型 Andriy Mnih Geoffrey Hinton . 
A Scalable Hierarchical Distributed Language Model . NIPS 2008 . 
2008 . 提出 HLBLRonan Collobert Jason Weston . A Unified 
Architecture for Natural Language Processing Deep Neural Networks with Multitask 
Learning . ICML 2008 . 2008 . 旧瓶新酒 TDNN Multitask 
LearningRonan Collobert Jason Weston et al . Natural Language Processing 
Almost from Scratch . JMLR 2011 . 2011 . 对 
SENNA 进行 解释 的 论文 注意 SENNA 要 区别 5 
中的 C & W embedding . Eric H . Huang 
Richard Socher etc . ImprovingWord Representations via Global Context and 
MultipleWord Prototypes . ACL 2012 . 2012 . 此 篇 
paper 把 全局 信息 加入 模型 模型 求解 用了 5 
中 的 方法 word2vec 系列 paper Distributed Representations ofWords and 
Phrases and their C o m p o s i 
t i o n a l i t y E 
f f i c i e n t Estimation of 
Word Representations in Vector Spaceword2vec Explained Deriving Mikolov et al 
. s NegativeSampling Word Embedding Method 解释性 的 paper 发布 
arxiv 上 的 和 有道 那个 可以 一起 看 Nitish 
Srivastava Ruslan Salakhutdinov Geoffrey Hinton . Modeling Documents with a 
Deep Boltzmann Machine . UAI 2013 . 类似于 LDA 的 
一种 topic modelRNN 系列 Recurrent NN 能 model long term 
dependency 训 练出 的 结果 比 Feed Forward NN 结果 
更好 但 训练 复杂度 更大 这个 系列 word2vec 作者 Mikolov 
研究 较多 比如 其 博士 论文 Linguistic Regularities in Continuous 
SpaceWord R e p r e s e n t 
a t i o n s R e c u 
r r e n t neural network based language modelRecursive 
NN 这个 主要 用在 句法分析 上 model 自然语言 存在 的 
递归结构 这个 主要 是 Richard Socher 的 paperRecursive Deep Models 
for Semantic Compositionality Over a Sentiment TreebankParsing Natural Scenes and 
Natural Language with Recursive Neural NetworksJoseph Turian Lev Ratinov Yoshua 
Bengio . Word representations A simple and general method for 
semi supervised learning . ACL 2010 对 现有 的 word 
Representation 做了 对比 提供 一个 新的 word embedding 读者 可以 
自行 复现 见 Section 13 Jeffrey Pennington Richard Socher Chris 
Manning . GloVe Global Vectors for Word Representation . EMNLP 
2014 GloVe 与 word2vec 对比 的 效果 曾经 被 质疑 
过 其实 word2vec 效果 差不多 Omer Levy Yoav Goldberg . 
Neural Word Embedding as Implicit Matrix Factorization . NIPS . 
2014 . 将 SGNS Skip Gram with Negative Sampling 和 
矩阵 分解 等价 分析 SGNS 等价 于 分解 PMI 矩阵 
文中 作者 基于 谱 方法 SVD 分解 shifted PPMI 的 
矩阵 得到 了 不错 的 效果 word sim 上 和 
word2vec 类似 作者 还在 arxiv 提交 了 一个 分析 SGNS 
的 note 结合 看 更加 Q . V . Le 
T . Mikolov . Distributed Representations of Sentences and Documents 
. ICML 2014 . 2014 . 文中 各个 实验 都 
体现 了 好 的 效果 但是 可 复现 性 一直 
遭到 质疑 最近 在 word2vec 的 google group 上 公布 
了 复现 方法 已经 有人 复 现出 92.6% 的 结果 
Tutorial Tomas Mikolov . Statistical Language Models Based on Neural 
NetworksRichard Socher . Recursive Deep Learning for Modeling Semantic C 
o m p o s i t i o n 
a l i t y R u c h a 
r d Socher Christpher Manning . Deep Learning for Natural 
Language Processing without Magic Evaluation Yanqing Chen etc . The 
Expressive Power of Word Embeddings . ICML 2013 . 实验 
评价 了 四个 model – HLBL 4 SENNA 11 Turian 
s 12 Huang s 6 . 理解 人类 语言 在 
人工智能 领域 称为 自然语言 处理 所谓 的 自然 语言 处理 
就是 用 计算机 处理 人类 在 日常 生活 串 所 
使用 的 自然 语言 的 能力 让 机 算机 理解 
自然 语言 是 十分 艰难 的 任务 无法 理解 计算机 
语言 的 原因 主要 存在 语义 语法 语音 问题 归纳 
起来 主要 有 6条 原因 1 . 句子 的 正确 
词序 规则 和 概念 难以理解 不含 规则 的 句子 2 
. 词语 的 确切 含义 形式 词类 及 构词法 3 
. 词 的 语义分类 以及 词 的 多义性 和 岐义 
性 4 . 指 定和 不定 特性 及 所有 隶属 
特性 5 . 问题 领域 的 结构 知识 和 时间 
概念 6 . 有关 语言 表达 形式 的 文学 知识 
语言 的 理解 与 交流 需要 一个 相当 庞大 和 
复杂 的 知识 体系 自然语言 理解 最大 的 困难 就 
在于 对 知识 不 完整性 不确定性 模糊性 的 处理 了解 
了 以下 学习 难点 才 可以 说 是 真正 的 
入门 如何 快速 入门 NLP 自然语言 处理 概述 http / 
/ www . duozhishidai . com / article 11742 1 
. html 从 语言 学到 深度 学习 NLP 一文 概述 
自然语言 处理 http / / www . duozhishidai . com 
/ article 1120 1 . html 改变 世界 的 七大 
NLP 技术 http / / www . duozhishidai . com 
/ article 8918 1 . htmlpython 下 NLP 工具 有 
很多 jieba nltk ltp 等 虽然 他们 很 强大 但是 
提供 的 功能 比较 分散 而且 通常 模型 比较 大 
为了 方便 平时 的 处理 工作 我 尝试 找 了 
一些 集成 工具包 发现 snownlp 还 可以 它 的 分词 
是 基于 TnT 的 总得来说 分词 效果 逊色于 基于 词典 
的 分词 比如 jieba 所以 决定 自己 写 一个 包 
xmnlp 主打 轻量 快捷 功能 中文分词 & 词性 标注 基于 
词典 构建 DAG 图 然后 采用 动态规划 的 思想 求得 
最大 概率 路径 jieba 分词 采用 了 反向 输出 我 
采用 了 正向 加权 反向 输出 的 方式 使得 正 
反向 共同 影响 分词 效果 对于 未 登录 词 采用 
HMM + Viterbi 处理 文本 纠错 采用 了 bi gram 
+   levenshtein 实现 文本 摘要 & 关键词 提取 textrank 
情感 分析 naive bayes 文本 转 拼音 Trie 树 检索 
以下 展示 xmnlp 的 功能 效果 不同 模块 的 原理 
之后 的 文章 会 补上 分词 & 词性 标注 In 
自然语言 处理 是 人工 智能 和 语言学 领域 的 分支 
学科 在 这 此 领域 中 探讨 如何 处理 及 
运用 自然语言 自然语言 认知 则是 指 让 电脑 懂 人类 
的 语言 自然语言 生成 系统 把 计算机 数据 转化 为 
自然语言 自然语言 理解 系统 把 自然 语言 转化 为 计算机程序 
更 易于 处理 的 形式 分词 自然语言 处理 / / 
是 / 人工智能 / 和 / 语言学 / 领域 / 
的 / 分支 / 学科 / / 在 / 这 
此 / 领域 / 中 / 探讨 / 如何 / 
处理 / 及 / 运用 / 自然 / 语言 / 
/ 自然 / 语言 / 认知 / 则 / 是 
/ 指 让 / 电脑 / / 懂 / / 
人类 / 的 / 语言 / / 自然 / 语言 
/ 生成 / 系统 / 把 / 计算机 / 数据 
/ 转化 / 为 / 自然 / 语言 / / 
自然 / 语言 / 理解 / 系统 / 把 / 
自然 / 语言 / 转化 / 为 / 计算机程序 / 
更 / 易于 / 处理 / 的 / 形式 / 
标注 自然语言 处理 un / un / 是 v / 
人工智能 nw / 和 c / 语言学 n / 领域 
n / 的 uj / 分支 n / 学科 n 
/ un / 在 p / 这 此 un / 
领域 n / 中 f / 探讨 v / 如何 
r / 处理 v / 及 c / 运用 vn 
/ 自然 d / 语言 n / un / 自然 
d / 语言 n / 认知 v / 则 d 
/ 是 v / 指 让 un / 电脑 n 
/ un / 懂 v / un / 人类 n 
/ 的 uj / 语言 n / un / 自然 
d / 语言 n / 生成 v / 系统 n 
/ 把 p / 计算机 n / 数据 n / 
转化 v / 为 p / 自然 d / 语言 
n / un / 自然 d / 语言 n / 
理解 v / 系统 n / 把 p / 自然 
d / 语言 n / 转化 v / 为 p 
/ 计算机程序 n / 更 d / 易于 v / 
处理 v / 的 uj / 形式 n / un 
文本 纠错 In 这 理 风景 绣 丽 而且 天汽 
不错 我 的 心情 各 外 舒畅 Out 这里 风景秀丽 
而且 天气 不错 我 的 心情 格外 舒畅 文本 摘要 
& 关键词 In 自然语言 处理 是 人工 智能 和 语言学 
领域 的 分支 学科 在 这 此 领域 中 探讨 
如何 处理 及 运用 自然语言 自然语言 认知 则是 指 让 
电脑 懂 人类 的 语言 自然语言 生成 系统 把 计算机 
数据 转化 为 自然语言 自然语言 理解 系统 把 自然 语言 
转化 为 计算机程序 更 易于 处理 的 形式 关键词 自然语言 
2 . 2069266136741321 处理 1 . 5572478858429686 是 1 . 
4182222157079281 系统 1 . 2431338210535401 转化 1 . 1532093387566391 摘要 
自然语言 理解 系统 把 自然 语言 转化 为 计算机程序 更 
易于 处理 的 形式 情感 分析 In 这件 衣服 的 
质量 也 太差 了吧 一 穿 就 烂 Out 0 
. 0 0 9 9 5 9 6 9 4 
6 2 1 6 4 5 6 9 8 文本 
转 拼音 In 面朝 大海 春暖花开 Out mian zhao da 
hai chun nuan hua kai 前往 项目 github 地址 1 
. 先 解释 何为 CFG 及 PCFG 一个 栗子 2 
. CKY 算法 或称 CYK 算法 在 计算机 科学 领域 
CYK 算法 也 称为 Cocke – Younger – Kasami 算法 
是 一种 用来 对 上下文 无关 文法 CFG Context Free 
Grammar 进行 语法分析 parsing 的 算法 该 算法 最早 由 
John Cocke Daniel Younger and Tadao Kasami 分别独立 提出 其中 
John Cocke 还是 1987 年度 的 图灵 奖得主 CYK 算法 
是 基于 动态规划 思想 设计 的 一种 自底向上 语法分析 算法 
CYK 算法 可以 在 O n3 的 时间 内 得出 
结果 CKY 算法 CYK 处理 的 CFG 必须 是 CNF 
形式 的 所以 算法 首先 要把 非 CNF 形式 的 
CFG 转化 到 弱 等价 CNF 形式 CYK 是 一种 
自底向上 的 算法 乔姆斯基 范式 乔姆斯基 范式 CNF 或者 ABC 
都 为非 终结符 为 终结符 那个 这个 CFG 就是 采用 
CNF 形式 的 可见 CNF 语法 都是 二 分叉 的 
任何/r 语法/n 都/d 可以/c 转化/v 成/n 一个/m 弱/a 等价/n 的/uj 
CNF/w 形式/n 具体 方法 如下 之后 会 有 拓展 版 
的 不只 二元 了 还 有空 的 与 一元 的 
方法 CKY 算法 用于 PCFG 下 的 句法分析 实现 句子 
fish people fish tanks 的 句法树 分析 实现 最 可能 
的 统计 句法树 基于 概率 的 上下文 无关 语法 PCFG 
是 一个 五 元组 其 定义 为 T   N 
S R P . 可以 看到 这 基本上 与 CFG 
类似 只是 多 出来 一个 元素 p 表示 在 语 
料中 规则 出现 的 概率 . 使用 p 可以 定义 
一棵 语法树 出现 的 概率 为 树 中所 有规则 出现 
概率 之 积 . 这样 当 一个 句子 在 可选 
的 范围 内有 多棵 可能 的 语法树 时 我们 选择 
先验概率 大 的 那 棵树 这样 能 最大 程度 避免 
解析 错误 其中 N 代表 非 终结符 集合 T 代表 
终结符 集合 代表 初始 非 终结符 R 代表 产生 规则 
集 P 代表 每个 产生 规则 的 统计 概率 栗子 
拓展 版 加入 了 一元 CKY 动态规划 具体 算法 类似 
填表 的 方法 贴 一个 维基 百科 的 CYK 算法 
用于 CFG https / / en . wikipedia . org 
/ wiki / CYK _ algorithm # / media / 
File CYK _ algorithm _ animation _ showing _ every 
_ step _ of _ a _ sentence _ parsing 
. gif 第一 部分 下载 stanford parser full 2018 10 
17 . zip 解压 打开 eclipse 创建 一个 项目 导入 
在 build path 中 引入 stanford parser 3 . 9.2 
models . jar stanford parser . jar slf4j api . 
jar 等 相关 库 . 调 参 主要 代码 结果 
句法树 GUI 界面 相关 教程 连接 http / / www 
. cnblogs . com / Denise hzf / p / 
6612574 . html 第二 部分 Python3 . 5 pycharm . 
动态规划 PCFG + CKY 程序 链接 http / / f 
. dataguru . cn / thread 693052 1 1 . 
htmlPCFG 的 训练 对于 PCFG 中的 CFG 部分 一般 是由 
领域 相关 的 专家 给出 的 例如 英语 专家 规定 
英语 的 CFG . 而 PCFG 中的 p 是从 语料 
中 统计 而来 . 运用 最大 似 然 估计 可以 
有 P X Y = count X Y / count 
X 注意到 规则 中 包括 终端 词 与非 终端 词 
两 种元素 . 在 一个 适当 规模 的 语料 中 
我们 可以 认为 所有 的 非 终端 词 都会 出现 
但是 认为 所有 的 终端 词 都会 出现 却是 不 
现实 的 想 一下 我们 常 听到 的 那个 美国 
农民 日常 使用 的 英语 单词 只有 数千 个 而 
所有 的 英语 单词 却 有数 万个 的 情况 . 
当 语料 中 没有 出现 而 在 我们 的 测试 
样本 中 却 出现 了 少见 的 单词 时 PCFG 
会对 所有 的 语法树 都 给出 概率 为 0 的 
估计 这对 PCFG 的 模型 是 一个 致命 的 问题 
. 通常 的 补救 措施 是 对语 料中 所有 单词 
出现 次数 进行 统计 然后 将 出现 频率 少于 t 
的 所有 单词 都 换成 同一个 symbol . 在 进行 
测试 时 先 查找 测试 句子 中 的 所有 单词 
是否 在 句子 中 出现 若 没有 出现 则 使用 
symbol 代替 . 通过 这种 方法 可以 避免 PCFG 模型 
给出 概率 为 0 的 估计 同时 也 不会 损失 
太多 的 信息 . 开源 NLP 自然语言 处理 工具 集锦 
现状 首先 看看 目前 常用 的 分词 系统 N o 
N a m e F e a t u r 
e 1 B o s o n N L P 
h t t p / / bosonnlp . com / 
2IKAnalyzerhttp / / git . oschina . net / wltea 
/ IK Analyzer 2012FF3NLPIRhttp / / ictclas . nlpir . 
org / 4SCWShttp / / www . xunsearch . com 
/ scws / 5 结巴 分词 http / / www 
. oschina . net / p / jieba6 盘古 分词 
http / / pangusegment . codeplex . com / 7 
庖丁解牛 http / / zengzhaoshuai . iteye . com / 
blog / 9863148 搜狗 分词 http / / www . 
sogou . com / labs / webservice / 9 腾讯 
文智 http / / nlp . qq . com / 
10 新浪 云 http / / www . sinacloud . 
com / doc / sae / php / storage . 
html11 语言 云 http / / www . ltp cloud 
. com / demo / 博主 也是 刚 开始 接触 
分词 使用 的 不多 目前 看来 市场 上 用 的 
比较 多 的 是 中科院 的 NLPIR 分词 系统 大家 
可以 在 官网 上 下载 试用 貌似 是 一个 月 
￣ ▽ ￣ 然后就 被 无情 的 提示 license 过期 
这时 只 需要 在 git 上 下载 新的 license 替换 
旧 license 就 好啦 ~ ps . 每次 更新 license 
有效期 一个月 所以 大家 勤 动手吧 BosonNLP 和 大多数 的 
NLP 工具 一样 玻 森 的 处理 能力 大概 就 
以上 几种 分词 与 词性 标注 大家 可以 点击 链接 
浏览 词性 分析 的 文档 博主 摘取 部分 关键 信息 
如下 1 分词 和 词性 标注 联合 枚举 的 方法 
2 开放 API 接口 3 基于 序列 标注 实现 的 
以 词 为 单位 对 句子 进行 词 边界 和 
词性 的 标注 即 基于 字符串 匹配 的 方法 4 
结合 上下文 识别 生词 5 加入 了 对 url email 
等 特殊 词 的 识别 6 对 词性 标签 进行 
调整 和 优化 实现 了 更 细 的 标签 划分 
22个 大类 69个 标签 7 对 训练 语料 进行 修正 
8 加入 繁简 转化 可以 处理 繁体中文 或者 繁简 混合 
的 中文 句子 9 多种 分词 选项 空格 保留 选项 
新词 枚举 强度 选项 繁简 转换 选项 特殊字符 转换 选项 
下面 看一下 玻 森 的 免费 使用 次数 可见 除了 
词性 分析 比较 多 以外 其他 的 均为 500次 ′ 
⌒ ` 我们 这里 额外 讲解 一下 rest api 表述 
性 状态 转移 Representational State Transfer 它 是 一种 设计 
风格 而 非标准 通常 基于 使用 http uri xml 以及 
html 这些 现有 的 广泛 流行 的 协议 和 标准 
想 深入 了解 的 童鞋 可以 查看 下面 的 链接 
Rest API 开发 学习 笔记 by spring yangRest 维基百科 情感 
分析 这 是 情感 分析 返回 的 结果 可见 我们 
查询 了 两句话 每 句话 的 前面 是 正面 概率 
后面 是 消极 概率 这里 提供 一个 curl 的 下载 
链接 CURL 官方 网址 新闻 分类 时间 转换 这个 在 
博主 看来 还是 很 有意思 的 它 可以 将 中文 
描述 的 时间 短语 转换 为 三种 标准 的 时间 
格式 字符串 1 时间 点 timestamp 表示 某一 具体 时间 
时间 描述 2 时间 量 timedelta 表示 时间 的 增量 
的 时间 描述 3 时间区间 timespan 大于/d 一天/m 的/uj 有/v 
具体/a 起始/v 和/c 结束/v 时间/n 点/m 的/uj 时间/n 描述/v 新闻 
摘要 摘要 系统 提供 4个 输入 选项 新闻标题 新闻 正文 
字数 限制 是否 为 严格 字数 限制 文本 聚 类 
引擎 看到 可以 文本 聚 类 的 时候 博主 是 
很 激动 的 因为 毕设 就 一直 在 折腾 这个 
文档 中说 该 引擎 能够 对 给定 的 文本 进行 
话题 聚 类 将 语义上 相似 的 文章 归为 一类 
IKAnalyzer 点击 IKAnalyzer 链接 可以 看到 最新 的 版本 也是 
2012年 的 实现 的 功能 比较 单一 感 兴趣 的 
童鞋 可以 看看 总之 玻 森 使用 比较 方便 个人认为 
界面 简介 明了 易于 初学者 使用 目录 文章 目录 目录 
前言 汉语 的 分词 与 频度 统计 1 汉语 词汇 
的 特点 汉语 的 分词 与 频度 统计 2 汉语 
的 分词 与 频度 统计 3 汉语 的 分词 与 
频度 统计 4 汉语 的 分词 与 频度 统计 5 
汉语 的 分词 与 频度 统计 6 汉语 的 分词 
与 频度 统计 7 前言 硕士 生涯 结束 开始 专心 
做 一件 自己 觉得 有用 的 工具 先 做 工程 
后搞/nr 理论 自然语言 处理 是 一个 非常 难 的 问题 
同时 是 人工智能 皇冠 上 的 明珠 接下来 会 记录 
一 系列 自然语言 处理 的 笔记 来自 于 哈工大 老师 
关毅 汉语 的 分词 与 频度 统计 1 语言 分类 
{ 孤立语 if 没有 附 加词 如 汉语 黏着语 if 
有附/nr 加词 如 日语 曲折 语 if 形态 变化 如 
英语 语言 分类 \ begin { cases } 孤立语 & 
amp \ text { if } 没有 附 加词 如 
汉语 \ \ 黏着语 & amp \ text { if 
} 有附/nr 加词 如 日语 \ \ 曲折 语 & 
amp \ text { if } 形态 变化 如 英语 
\ end { cases } 语言 分类 ⎩ ⎪ ⎨ 
⎪ ⎧ 孤立语 黏着语 曲折 语 if   没有 附 
加词 如 汉语 if   有附/nr 加词 如 日语 if 
  形态 变化 如 英 语词 是 自然 语言 处理 
中的 最小 单位 语速 词 短语 句子 语群 汉语 词汇 
的 特点 结合 紧密 使用 频繁 汉语 的 词 可以 
拆开 调换 位置 有 限度 地 展开 字串 可以 切 
分为 词串 提出 规划 汉语 的 自动 分词 是 他 
的 重要 组成部分 对 他 分词 很 困难 新领域 老 
方法 新 瓶装 旧 酒 汉语 的 分词 与 频度 
统计 2 GB 分词 规划 提出 了 汉字 的 分词 
规则 四字 词语 一律 是 词 切分 歧义 未 登录 
词 比较 困难 比如 提高 中国 人民 生活 水平 比较 
困难 覆盖 型 切分 容易 出 问题 真 歧义 同属 
切 分型 如何 排除 歧义 呢 蛋鸡 问题 先有 蛋 
分词 做 词 切分 前驱/n 字串/n 和后驱/nr 字串/n 词法 信息 
实例 歧义 字串 单切/nr 句法 规则 调整 利用 语义 信息 
实例 进行 切分 新 出现 的 词 最 困难 挂一漏万 
上下文 出现 的 条件 以及 分词 系统 互信息 极大 方差 
极大 熵 模型 汉语 的 分词 与 频度 统计 3 
主要 分词 方法 正向 最大 匹 配方法 几 个字符 在 
一块儿 去掉 一个词 再试 逆向 匹 配方法 双向 匹 配法 
最小分词 方法 做 的 东西 是 给 人看 的 创造力 
最 丰富 20 40岁 的 时候 不 存在 切分 歧义 
的 点 分段 计算 最短 路径 图 的 方法 去 
理解 这些 东西 词 网格 方法 生成 所有 可能 切分 
的 方式 计算 词 的 概率 汉语 的 分词 与 
频度 统计 4 哈工大 2005年 第一名 做到 95% 语料库 平衡 
语料库 生 语料库 半生不熟 语料库 句法分析 所困 语法分析 十万级 的 
词汇 基本 没用 共时 语料库 历时 语料库 发展 时间 一段 
时间 以内 各种 模型 的 正确率 统计 机器翻译 统计 翻译 
模型 中文信息 语料库 英语 Brown corpus Penn Treebank 句法树 数学化 
双语 语料库 法律 文档 语料库 词频 统计 构建 词汇 模型 
的 核心 词典 收词 的 规律 汉语 的 分词 与 
频度 统计 5 现代汉语 频率 词典 LJVAC 华语 共时 语料库 
建立 了 各地 词典 双音节 词 最多 定量分析 用词 相同 
率 和 地域 相关 词频 反映 国家 政策 的 变化 
汉语 的 分词 与 频度 统计 6 词频 一个 数表 
高频 虚 低频 实词 定量分析 占 90% 的 词 低于 
10次 zipf 定律 f 正比 于1//nr r y = kxcy 
= kx ^ cy = kxc 指数 定理 同 取 
对数 除/p 特高频/b 和特/nr 低频/b 以外/c 都/d 符合/v 语料库 规律 
可以 推测 句式 规律 1 构 语 语言 模型 模型 
多少 词 enough2heap s law 反映 了 词表 长度 与 
语料库 的 关系 平滑 算法 更好 的 保障 汉语 的 
分词 与 频度 统计 7 其他 的 统计 分布 规律 
频度 和 频度 词 个数 推荐 大家 看 创 世纪 
的 第八 天 真正 的 科学 需要 枯燥 的 处理 
一件事 需要 把 一件 事情 做到 极致 Python 的 几个 
自然语言 处理 工具 自然语言 处理 Natural Language Processing 简称 NLP 
是 人工智能 的 一个 子域 自然语言 处理 的 应用 包括 
机器翻译 情感 分析 智能 问答 信息提取 语言 输入 舆论 分析 
知识图谱 等 方面 也是 深度 学习 的 一个 分支 首先 
介绍 一下 Python 的 自然 语言 处理 工具包 1 . 
NLTK 工具包 NLTK 在用 Python 处理 自然 语言 的 工具 
中 处于 领先 的 地位 它 提供 了 WordNet 这种 
方便 处理 词汇 资源 的 接口 还有 分类 分词 除 
茎 标注 语法分析 语义 推理 等 类库 2 . Jieba 
工具包 3 . Pattern 工具包 Pattern 工具包 包括 词性 标注 
工具 Part Of Speech Tagger N 元 搜索 n gram 
search 情感 分析 sentiment analysis WordNet 同时 也 支持 机器 
学习 的 向量空间 模型 聚 类 和 支持 向量 机 
4 . TextBlobTextBlob 是 一个 处理 文本 数据 的 Python 
库 提供 了 一些 简单 的 api 解决 一些 自然 
语言 处理 的 任务 例如 词性 标注 名词 短语 抽取 
情感 分析 分类 翻译 等等 5 . GensimGensim 提供 了 
对 大型 语料库 的 主题 建模 文件 索引 相似 度 
检索 的 功能 它 可以 处理 大于 RAM 内存 的 
数据 作者 说 它 是 实现 无 干预 从纯/nr 文本 
语义 建模 的 最强 大 最 高效 最 无障碍 的 
软件 6 . PyNLPIPython 自然语言 处理 库 Python Natural Language 
Processing Library 音 发作 pineapple 这 是 一个 各 种 
自然 语言 处理 任务 的 集合 PyNLPI 可以 用来 处理 
N 元 搜索 计算 频率 表 和 分布 建立 语言 
模型 他 还 可以 处理 向 优先 队列 这种 更加 
复杂 的 数据 结构 或者 像 Beam 搜索 这种 更加 
复杂 的 算法 7 . spaCyspaCy 是 一个 商业 的 
开源 软件 结合 Python 和 Cython 自然语言 处理 能力 达到 
了 工业 强度 是 领域内 速度 最快 最 先进 的 
自然 语言 处理 工具 8 . PolyglotPolyglot 支持 对 海量 
文本 和多/nr 语言 的 处理 它 支持 对 165种 语言 
的 分词 对 196种 语言 的 辨识 40种 语言 的 
专有名词 识别 16种 语言 的 词性 标注 136种 语言 的 
情感 分析 137种 语言 的 嵌入 135种 语言 的 形态 
分析 以及 69中 语言 的 翻译 9 . MontyLingua 英文 
MontyLingua 是 一个 自由 的 训练有素 的 端 到 端的 
英文 处理 工具 输入 原始 英文 文本 到 MontyLingua 就会 
得到 这段 文本 的 语义解释 适合 用来 进行 信息 检索 
和 提取 问题 处理 回答 问题 等 任务 从 英文 
文本 中 它 能 提取 出 主 动宾 元组 形容词 
名词 和 动词短语 人名 地名 事件 日期 和 时间 等 
语义 信息 10 . BLLIP ParserBLLIP Parser 也叫做 Charniak Johnson 
parser 是 一个 集成 了 产生 成分 分析 和 最大熵 
排序 的 统计 自然语言 工具 包括 命令行 和 python 接口 
11 . QuepyQuepy 是 一个 Python 框架 提供 将 自然 
语言 转换 成为 数据库 查询语言 可以 轻松 地 实现 不同 
类型 的 自然 语言 和 数据库 查询语言 的 转化 所以 
通过 Quepy 仅仅 修改 几行 代码 就 可以 实现 你 
自己 的 自然 语言 查询 数据库系统 GitHub https / / 
github . com / machinalis / quepy12 . HanNLPHanLP 是 
一个 致力于 向 生产 环境 普及 NLP 技术 的 开源 
Java 工具包 支持 中文分词 N 最 短路 分词 CRF 分词 
索引 分词 用户 自定义 词典 词性 标注 命名 实体 识别 
中国 人 名 音译 人名 日本 人名 地名 实体 机构 
名 识别 关键词 提取 自动 摘要 短语 提取 拼音 转换 
简繁转换 文本 推荐 依存 句法分析 MaxEnt 依存 句法分析 神经网络 依存 
句法分析 文档 使用 操作 说明 Python 调用 自然语言 处理 包 
HanLP 和 菜鸟 如何 调用 HanNLP 参考文献 1 . Python 
自然语言 处理 工具 小结 先 介绍 一下 我 自己 我 
有过 5年 以上 机器学习 的 工作 经验 主要 工作 内容 
有 图像 分析 自然语言 模式识别 我 认为 该 领域 最 
稀缺 的 人才 是 NLP 专业 然后 是 图像 分析 
CV 我 准备 做 一个 系列 的 文章 把 我 
在 面试 过程 中 遇到 的 各种 技术 性问题 每个 
问题 分别 讲解 1 . 我 常常 会 遇到 问 
LSTM 的 问题 现在 详细 讲解 下 理解 LSTM 前 
要 先 理解 RNN Recurrent Neural Networks 这种 神经网络 带有 
环 可以 将 信息 持久化 在上 图 所示 的 神经 
网络 AA 中 输入 为 XtXt 输出 为 htht AA 
上 的 环 允许 将 每一步 产生 的 信息 传递 
到 下 一步 中 环 的 加入 使得 RNN 变得 
神秘 不过 如果 你 多 思考 一下 的话 其实 RNN 
跟 普通 的 神经 网络 也 没有 那么 不同 一个 
RNN 可以 看作 是 同一 个 网络 的 多份 副本 
每 一份 都将 信息 传递 到 下一个 副本 RNN 在 
一系列 的 任务 中 都 取得 了 令人 惊叹 的 
成就 比如 语音识别 图片 标题 等等 LSTM 是 这 一系列 
成功 中 的 必要 组成部分 LSTM Long Short Term Memory 
是 一种 特殊 的 循环 神经网络 在 许多 任务 中 
LSTM 表现 得 比 标准 的 RNN 要 出色 得多 
几乎 所有 基于 RNN 的 令人 赞叹 的 结果 都是 
LSTM 取得 的 接下来 将 着重 介绍 LSTM 长期 依赖 
Long Term Dependencies 的 问题 RNN 的 一个 核心 思想 
是 将 以前 的 信息 连接 到 当前 的 任务 
中 来 例如 通过 前面 的 视频 帧 来 帮助 
理解 当前 帧 如果 RNN 真的 能够 这样 做 的话 
那么 它们 将 会 极其 有用 但是 事实 真是 如此 
吗 未必 有时候 我们 只 需要 看 最近 的 信息 
就 可以 完成 当前 的 任务 比如 考虑 一个 语言 
模型 通过 前面 的 单词 来 预测 接下来 的 单词 
如果 我们 想 预测 句子 the clouds are in the 
  sky 中 的 最后 一个 单词 我们 不 需要 
更多 的 上下文 信息 很明显 下 一个 单词 应该是 sky 
在 这种 情况 下 当前 位置 与 相关 信息 所在位置 
之间 的 距离 相对 较小 RNN 可以 被 训练 来 
使用 这样 的 信息 然而 有时候 我们 需要 更多 的 
上下文 信息 比如 我们 想 预测 句子 I am a 
tall man . . . . . i can play 
basketball 中 的 最后 一个 单词 最近 的 信息 告诉 
我们 最后 一 个 单词 可能 是 某种 语言 的 
名字 然而 如果 我们 想 确定 到底 是 哪种 语言 
的话 我们 需要 basket 这个 更远 的 上下文 信息 实际上 
相关 信息 和 需要 该 信息 的 位置 之间 的 
距离 可能 非常 的 远 随着 距离 的 增大 RNN 
对于 如何 将 这样 的 信息 连接起来 无能为力 LSTM 全称 
为 长短期 记忆 网络 Long Short Term Memory networks 是 
一种 特殊 的 RNN 能够 学习 到 长期 依赖 关系 
LSTM 由 Hochreiter & Schmidhuber 1997 提出 许多 研究 者 
进行 了 一系列 的 工作 对其 改进 并 使之 发扬光大 
LSTM 在 许多 问题 上 效果 非常 好 现在 被 
广泛 使用 LSTM 在 设计 上 明确 地 避免 了 
长期 依赖 的 问题 记住 长期 信息 是 小菜 一碟 
所有 的 循环 神经 网络 都 有着 重复 的 神经 
网络 模块 形 成链 的 形式 在 普通 的 RNN 
中 重复 模块 结构 非常 简单 例如 只有 一个 tanh 
层 我会 在 专栏 和 视频 中 免费 给 大家 
具体 讲解 细节 的 技术 最近 跑了 3D CNN 实现 
MRI 的 分割 及 分类 训练 过程 中将 准确率 等 
自动 保存 成 一个 txt 文件 该 文件 有 64M 
多 想要 从中 提取 有用 信息 比如 Mean accuracy sensitivity 
specificity 和 mean dsc 等 人工 筛选 太 繁琐 所以 
想到 用 python 编程 来 实现 去 请教 了 一位 
研究 自然语言 处理 的 计算机系 的 师兄 实现 代码 如下 
# * coding utf 8 * # @ Time 18 
4 18 # @ Author sadscv # @ File textExtract 
. pyimport jsonimport redef readfile save = Nonecount = 0with 
open data / t r a i n e s 
s i o n D e e p M e 
d i c . txt r as f tmpfile = 
open data / Epoch . txt wb # split filefor 
line in f print save head = re . search 
Starting new Epoch Epoch line flags = 0 if head 
pos = re . search # line . span 1 
num = line pos pos + 2 . rstrip / 
save = numopen data / Epoch _ { } . 
txt . format save wb count + = 1tmpfile = 
open data / Epoch _ { } . txt . 
format save a elif save tmpfile . write line return 
countdef subfile _ extract epoch with open data / Epoch 
_ { } . txt . format epoch r as 
f class _ json = { } for line in 
f # 两行 whole epochif re . search finished . 
Reporting Accuracy over whole epoch . line tmp = tmp 
_ flag = Falsefor i in range 4 current _ 
line = next f if re . search Finished sampling 
segment current _ line tmp _ flag = Trueif re 
. search VALIDATION current _ line tmp . append current 
_ line if tmp _ flag tmp . append next 
f class _ json . update json _ generater tmp 
whole # 8行 subepochif re . search Reporting Accuracy over 
whole epoch for Class line tmpval = tmptrain = for 
i in range 8 current _ line = next f 
if re . search VALIDATION current _ line tmpval . 
append current _ line if re . search TRAINING current 
_ line tmptrain . append current _ line if tmpval 
class _ json . update json _ generater tmpval validation 
if tmptrain class _ json . update json _ generater 
tmptrain training return json . dumps class _ json indent 
= 4 def json _ generater lines key = None 
# 送进来 8行 输 出 一个 json . 代表 epoch 
* validation or training class * 包含 的 数据 # 
如果 长度 为 2 则是 overall validationif len lines = 
= 2 p0 = re . search mean accuracy of 
epoch lines 0 . span 1 p1 = re . 
search mean accuracy of each subepoch lines 1 . span 
1 mean _ accuracy = lines 0 p0 . split 
= 0 . strip subepoch _ accuracy = lines 1 
p1 . strip output _ dict = { mean accuracy 
of epoch mean _ accuracy mean accuracy of each subepoch 
subepoch _ accuracy } return { overall output _ dict 
} if len lines = = 8 accuracy = { 
} sensitivity = { } specificity = { } dice 
= { } # Todo 添加 函数 重构 重复 代码 
for i in range len lines if re . search 
accuracy lines i accuracyidx = re . search mean accuracy 
of epoch lines i accuracysubidx = re . search mean 
accuracy of each subepoch lines i if accuracyidx accuracy . 
update { epoch lines i accuracyidx . span 1 . 
split = 0 . strip } if accuracysubidx accuracy . 
update { subepoch lines i accuracysubidx . span 1 . 
strip } elif re . search sensitivity lines i sen 
_ idx = re . search mean sensitivity of epoch 
lines i sen _ subidx = re . search mean 
sensitivity of each subepoch lines i if sen _ idx 
sensitivity . update { epoch lines i sen _ idx 
. span 1 . split = 0 . strip } 
if sen _ subidx sensitivity . update { subepoch lines 
i sen _ subidx . span 1 . strip } 
elif re . search specificity lines i spc _ idx 
= re . search mean specificity of epoch lines i 
spc _ subidx = re . search mean specificity of 
each subepoch lines i if spc _ idx specificity . 
update { epoch lines i spc _ idx . span 
1 . split = 0 . strip } if spc 
_ subidx specificity . update { subepoch lines i spc 
_ subidx . span 1 . strip } elif re 
. search dice lines i dice _ idx = re 
. search mean dice of epoch lines i dice _ 
subidx = re . search mean dice of each subepoch 
lines i if dice _ idx dice . update { 
epoch lines i dice _ idx . span 1 . 
strip } if dice _ subidx dice . update { 
subepoch lines i dice _ subidx . span 1 . 
strip } output = { accuracy accuracy sensitivity sensitivity specificity 
specificity dice dice } l = lines 0 re . 
search Class lines 0 . span 1 re . search 
Class lines 0 . span 1 + 1 return { 
key + _ class _ + l output } def 
main count = readfile for i in range count with 
open data / Epoch _ { } . json . 
format i w + as f f . write subfile 
_ extract i if _ _ name _ _ = 
= _ _ main _ _ main github 地址 https 
/ / github . com / sadscv / gadgets / 
tree / master / ZJ _ text _ extract 这是 
师兄 的 github 大家 可以 关注 一下 呀 最后 感谢 
师兄 本文 是 从 自然语言 处理 到 机器学习 入门 系列 
课程 的 第二 次 作业 由于 我 的 作业 环境 
没有 配好 配了 n 次 了 还是 不行 T _ 
T 但是 为了 保证 这 一系列 作业 的 完整性 于是/c 
经/n 罗曜强/nr 律师/n 同意/d 人工智能 A7 论坛 授权 转载 他 
的 作业 笔记 1 基本 要求 通过 自己 训练 的 
语言 模型 编程 判断 每 句话 中 是否 存在 a 
an 用错 的 问题 所谓 用错 指 a an 用 
反了 比如 i have a apple 是 错误 的 i 
have an apple 是 正确 的 2 准备 工作 1 
实验 的 环境 Ubuntu16 . 04 Python 版本 2.7 2 
使用 kenlm 训练 一个 语言 模型 首先 要 准备 kenlm 
所 需要 的 语料 按照 http / / kheafield . 
com / code / kenlm / 官方 文档 上 使用 
说明 训练 的 文件 会被 训 练成 . arpa 的 
格式 3 训练 模型 例如 我 有 名为 test . 
txt 的 文件 需要 训 练成 kenlm 指定 的 . 
arpa 格式文件 训练 后的/nr 文件 为 text . arpa 我 
需要 在 Ubuntu 的 Teminal 终端 使用 如 下 命令 
进行 训练 bin / lmplz o 5 test . txt 
text . arpa o Required . Order of the language 
model to estimate o 5 代表 使用 5ngram 将 arpa 
文件 转换 为 binary 文件 这样 可以 对 arpa 文件 
进行 压缩 提高 后续 在 python 中 加载 的 速度 
bin / build _ binary s text . arpa text 
. bin3 具体 实验 做好 上述 前置 准备 工作 后 
接着 就是 在 Python 下 运行 text . arpa 主要 
分为 以下 几个 步骤 # 导入 训练 所 需要 的 
包 import kenlm import nltk from itertools import combinations permutations 
# 将 文件 导入 kenlm 语言 模型 model = kenlm 
. LanuageModel text . bin #/i 判断/v a/w 或者/c an/w 
在/p 互换/v 前/f 的/uj 得分/v 和/c 互换/v 后的/nr 得分/v 如果 
互换 前 的 得分 高于 互换 后的/nr 得分 则 说明 
a 或 an 没有 错误 如果 互换 后的/nr 得分 高于 
互换 前 的 得分 则 说明 a 或者 an 语法错误 
def judge _ a _ or _ an sentence # 
创建 一个 空 list 用于 存放 sentence s = # 
将 句子 进行 分词 Model . score 函数 输出 的 
是 对数 概率 bos = False eos = False 意思 
是 不 自动 添加 句首 和 句末 标记 符 pre 
_ score = model . score . join sentence bos 
= True eos = True # 通过 循环 的 方式 
替换 a 或 an 然后 进行 评分 对比 for word 
in sentence # 如果 word 里面 有a/nr 则 把 a 
换成 an If word = = a s . append 
an # 如果 word 里面 有 an 则 把 an 
换成 a elif word = = an s . append 
a # 如果 word 里面 没有 a 或者 an 按照 
原 句 输出 else s . append word after _ 
score = model . score . join s bos = 
True eos = True # 对话 置换 前 置换 后的/nr 
得分 如果 置换 前 得分 高于 置换 后 则 返回 
0 否则 返回 1 if pre _ score after _ 
score return 0 else return 1 # 打开 文件 inputs 
= open text . arpa r outputs = open text 
_ after . txt r for line in inputs data 
= nltk . tokenize . word _ tokenize line # 
调用 judge _ a _ or _ an 函数 label 
= judge _ a _ or _ an data # 
格式化 输出 0 或 1 print line + \ t 
% d % label # 关闭 IO 流 outputs . 
close inputs . close 4 常见 错误 1 M o 
d u l e N o t F o u 
n d E r r o r No module named 
kenlm Kenlm 安装 错误 导致 无法 正常 调用 kenlm 2 
打开 文件 后未/nr 关闭 IO 流 致使 文件 无法 正常 
输出 3 其他 语法错误 智能 语音 助手 的 工作 原理 
是 先 了解 自然语言 处理 NLP 与 自然 语言 生成 
NLG 语音 助手 越来越 像 人类 了 与 人类 之间 
的 交流 不再 是 简单 的 你问 我 答 不少 
语音 助手 甚至能 和 人类 进行 深度 交谈 在 交流 
的 背后 离不开 自然语言 处理 NLP 和 自然 语言 生成 
NLG 这两种 基础 技术 机器 学习 的 这 两个 分支 
使得 语音 助手 能够 将 人类 语言 转换 为 计算机 
命令 反之亦然 这两种 技术 有 什么 差异 工作 原理 是 
什么 NLP vs NLG 了解 基本 差异 什么 是 NLP 
NLP 指在 计算机 读取 语言 时将/nr 文本 转换 为 结构化 
数据 的 过程 简而言之 NLP 是 计算机 的 阅读 语言 
可以 粗略地 说 在 NLP 中 系统 摄取 人语 将其 
分解 分析 确定 适当 的 操作 并以 人类 理解 的 
语言 进行 响应 NLP 结合 了 计算机 科学 人工智能 和 
计算 语言学 涵盖 了 以 人类 理解 的 方式 解释 
和 生成 人类 语言 的 所有 机制 语言 过滤 情感 
分析 主题 分类 位置 检测 等 什么 是 NLG 自然语言 
处理 由 自然 语言 理解 NLU 和 自然 语言 生成 
NLG 构成 NLG 是 计算机 的 编写 语言 它 将 
结构化 数据 转换 为 文本 以 人类 语言表达 即 能够 
根据 一些 关键 信息 及其 在 机器 内部 的 表达 
形式 经过 一个 规划 过程 来 自动 生成 一段 高 
质量 的 自然 语言 文本 NLP vs NLG 聊天 机器人 
的 工作 方式 人类 谈话 涉及 双向 沟通 的 方式 
聊天 机器人 也 一样 只是 沟通 渠道 略有不同 您 是 
与 机器 交谈 当 给 机器人 发送 消息 时 它 
会 将其 拾取 并 使用 NLP 机器 将 文本 转换 
为 自身 的 编码 命令 然后 将 该 数据 发送 
到 决策 引擎 在 整个 过程 中 计算机 将 自然 
语言 转换 为 计算机 理解 的 语言 处理 识别 语音 
语音 识别 系统 常用 的 是 Hidden Markov 模型 HMM 
它 将 语音 转换 为 文本 以 确定 用户 所说 
的 内容 通过 倾听 您 所说 的 内容 将其 分解为 
小 单元 并对 其 进行 分析 以 生成 文本 形式 
的 输出 或 信息 此后 的 关键 步骤 是 自然 
语言 理解 NLU 如 上文 所说 它 是 NLP 的 
另一个 子集 试图 理解 文本 形式 的 含义 重要 的 
是 计算机 要 理解 每个 单词 是 什么 这 是由 
NLU 执行 的 部分 在对 词汇 语法 和 其他 信息 
进行 筛选 时 NLP 算法 使用 统计 机器学习 应用 自然 
语言 的 语法 规则 并 确定 所说 的 最 可能 
的 含义 另一方面 NLG/w 是/v 一种/m 利用/n 人工智能/n 和/c 计算/v 
语言/n 学生/n 成/n 自然/d 语言/n 的/uj 系统/n 它 还 可以 
将该 文本 翻译成 语音 NLP 系统 首先 确定 要 翻译成 
文本 的 信息 然后 组织 表达 结构 再使用 一组 语法 
规则 NLG 就能 系统 形成 完整 的 句子 并 读出来 
应用 语音 助手 只是 NLP 众多 应用 程序 之一 它 
还可 用于 网络 安全 文章 白皮书 科研 等 领域 例如 
NLP 对 在线 内容 进行 情绪 分析 以 改进 服务 
并 为 客户 提供 更好 的 产品 而 NLG 通常用于 
Gmail 它 可以 为 您 自动 创建 答复 创建 公司 
数据 图表 的 描述 说 明时 NLG 也是 很好 的 
工具 说 NLP 和 NLG 完全 不 相关 也 不正确 
因为 NLP 和 NLG 相当于 学习 中 的 阅读 写作 
过程 还是 有 内在 关联 的 文本 标注 tagging 是 
一个 监督 学习 问题 可以 认为 标注 问题是 分类 问题 
的 一个 推广 标注 问题 又是 更 复杂 的 结构 
预测 structure prediction 问题 的 简单 形式 标注 问题 的 
输入 是 一个 观测 序列 输出 是 一个 标记 序列 
护着 状态 序列 标注 问题 的 目标 在于 学习 一个 
模型 使 它 能够 对 观测 序列 给出 标记 序列 
作为 预测 注意 的 是 可能 的 标记 个数 是 
有限 的 但 其 组合 所成 的 标记 序列 的 
个数 是 依 序列 长度 呈 指数级 增长 的 标注 
问题 氛围 学习 和 标注 两个 过程 如上 图 所示 
首先 给 定 一个 训练 数据集 在 这里 xi 为 
输入 观测 序列 一维 向量 yi 为 相应 的 输出 
观测 序列   一维 向量 每个 输入 观测 序列 向量 
的 长度 为 n 对 不同 样本 具有 不 一样 
的 值 学习 系统 基于 训练 数据集 构建 一个 模型 
表示 为 条件 概率分布 这里 的 每个 xi i = 
1 2 . . . n 取值 为 所有 可能 
的 观测 每个 Yi i = 1 2 . . 
. n 取值 为 所有 可能 的 标记 一般 n 
远 小于 N 标注 系统 按照 学习 得到 的 条件 
概率分布 模型 对 新 输入 观测 序列 找到 相应 的 
输出 标记 序列 具体 的 对 每一个 观测 序列 找到 
上式 中 概率 最大 的 标记 序列 评价 标注 模型 
的 指标 与 评价分类 模型 的 指标 一样 常用 的 
有 标注 准确率 精确 率 和 召回率 标注 问题 常用 
的 统计学 方法 有 详解 隐 马尔可夫 模型 HMM 和 
自然 语言 模型 之 条件 随 机场 理论 CRF 这 
两个 模型 之前 的 文章 有 介绍 过 标注 问题 
在 信息 抽取 自然语言 处理 等 领域 被 广泛 应用 
是 这些 领域 的 基本 问题 例如 自然语言 处理 中的 
词性 标注 就是 一个 典型 的 标注 问题 给定 一个 
由 单词 组成 的 句子 对 这个 句子 中 的 
每一个 单词 进行 词性 标注 即对 一个 单词 序列 预测 
其 对应 的 词性 标记 序列 举 一个 信息 抽取 
的 例子 从 英文 文章 中 抽取 基本 名词 短语 
为此 要对 文章 进行 标注 英文单词 是 一个 观测 英文 
句子 是 一个 观测 序列 标记 表示 名词 短语 的 
开始 结束 或 其它 标记 序列 表示 英文 句子 中 
基本 名词 短语 的 所在 位置 信息 抽取 时 将 
标记 开始 到 标记 结束 的 单词 作为 名词 短语 
标注 模型 的 评价 指标 标注 问题 常用 的 评价 
指标 是 精确 率 precision 召回率 recall 和 F1 值 
它 和 分类 问题 的 评价 指标 相同 为了 简便 
这里 使用 分类 来 进行 说 通常 标注 模型 在 
测试数据 集上 的 预测 和或 正确 或 不正确 4中 情况 
出现 的 总数 分别 记作 TP 将 正确 类 预测 
为 正 类 数 FP 将 正 类 预测 为 
负 类 数 FP 将 负 类 预测 为 正 
类 数 TN 将 负 类 预测 为 负 类 
数 那么 精确 率 定义 为 P = TP / 
TP + FP 召回率 定义 为 R = TP / 
TP + FN F1 值 是 根据 精确 率 和 
召回率 来 进行 计算 的 表达式 为 2 / F1 
= 1 / P + 1 / R 即 F1 
= 2TP / 2TP + FP + FN 一般/a 精确/a 
率/v 和/c 召回率/i 都高时/nr F1 值 也会 很高 参考 学习 
资料 1 统计 学习 方法 李航 文章 来源 于 微信 
公众 号 言 处理 技术 更多 内容 请 访问 该 
公众 号 欢迎 关注 公众 号 学习 目录 文章 目录 
目录 前言 一篇 论文 的 诞生 1 一篇 论文 的 
诞生 2 一片 论文 的 诞生 3 前言 硕士 生涯 
结束 开始 专心 做 一件 自己 觉得 有用 的 工具 
先 做 工程 后搞/nr 理论 自然语言 处理 是 一个 非常 
难 的 问题 同时 是 人工智能 皇冠 上 的 明珠 
接下来 会 记录 一 系列 自然语言 处理 的 笔记 来自 
于 哈工大 老师 关毅 一篇 论文 的 诞生 1 这是 
课堂 额外 福利 不同 的 研究 专题 不 一样 论文 
的 研究 过程 8年 整合 现有 相似 模型 语义 相似 
计算 论文 创作 阶段 草创 2000.1 2001.5 布局 2001.5 2002.12 
奋争 2003.1 2004.12 沉沦 与 转折 2005 发表 之路 2006 
2008 . 72000.1 博士 毕业 去 挣钱 问一问 智能 搜索 
引擎 语义 相似 度 好好 计算 争取 出 结果 做了 
4个 月出 结果 了 写 了 一个 专利 因为/c 第一/m 
作者/n 和/c 老板/n 谈/v 崩了/i 后来 想 明白 了 自己 
是 去 挣钱 的 还想 着名 有点儿 想 太多 了 
2001.5月 出局 回国 哈工大 任教 2001 2002 按照 老师 的 
方法 做 的 idea 是 老师 的 你 只是 负责 
了 实现 工作 不在 你 一篇 论文 的 诞生 2 
反思 小 公司 不 要去 争 这些 名利 大 公司 
还 可以 争 一 争 新阶段 新目标 一个 阶段 一个 
目标 错过 了 进入 微软 亚洲 研究院 的 机会 专心 
处理 后面 的 遗留 问题 一直 在 处理 争取 加速 
解决 进行 调研 发现 相似 系统 工程 和 这个 问题 
一样 没做 实验 就 开始 投 文章 被 大会 举办方 
使劲 怼 了 国内 的 优势 大量 高 素质 低 
成本 的 人才 写作 比 一切 都 重要 擅长于 调动 
资源 把 idea 实验 做出来 然后 去 落实 在做 的 
这个 过程 中 产生 了 新的 想法 句法结构 新的 结构 
认知 系统 的 深层 原理 基础性 研究 解决 了 问题 
一片 论文 的 诞生 3 两个 压力 项目 和 职称 
逼迫 犯了 一件 蠢事 中英文 文章 投稿 算是 沾边 一稿多投 
这一 学术 问题 幸好 没 接受 什么 时候 适合 写 
论文 呢 如鲠在喉 不吐 不快 时 适合 写 论文 再写 
其他 论文 应付 工作 问答 系统 应为 参加 了 比赛 
结果 也 不错 所以 写出来 期刊 也 比较 接受 写 
文章 之前 要 有 明确 投 论文 的 目标 要有 
对 科技 问题 的 认识 要 知道 一些 问题 适用 
于 一些 问题 创新 不难 难 在 让人接受 绝对 不要 
违规 最后 给 大家 一点儿 建议 1 心怀 大志 2 
始终如一 3 相信 自己 4 不懈努力 5 坚持 自信 自然语言 
处理 1 什么 是 自然 语言 处理 NLP 自然语言 处理 
是 一门 交叉学科 包括 计算机 科学 人工智能 和 语言学 目标 
让 计算机 去 处理 或 理解 自然语言 完成 一些 有用 
的 任务 例如 问答 系统 机器翻译 完全 理解 或者 表示 
语言 的 意义 甚至 去 定义 它 都是/nr 一个 虚幻 
的 目标 完美 的 理解 语言 是 一个 AI complete 
的 问题 2 自然语言 处理 的 应用 应用 范围 从 
简单 到 复杂 拼 写检查 关键词 提取 & 搜索 同义词 
查找 & 替换 从 网页 中 提取 有用 的 信息 
例如 产品价格 日期 地址 人名 或 公司名 等 分类 例如 
对 教科书 的 文本 进行 分级 对 长 文本 进行 
正负 情绪 判断 机器翻译 口语 对话 系统 复杂 的 问答 
系统 3 工业 届 里 的 NLP 应用 搜索引擎 在线广告 
自动 的 或 辅助 的 翻译 技术 市场 营销 或者 
金融交易 领域 的 情感 分析 语音识别 4 NLP 为什么 这么 
难 语言 在 表达 上 就 很复杂 使用 的 时候 
要 综合 考虑 使用 情境 Jane hit June and then 
she fell / ran . 歧义 问题 I made her 
duck 5 现有 的 中文 自然语言 处理 工具 N L 
P I R f u d a n N L 
P L T P p y t h o n 
NLTK 直 接有 纠错 功能 的 腾讯 文智     
大汉 科技 N gram 语言 模型 目的 在 给定 语料库 
的 情况 下 计算 一个 字符串 出现 的 概率 N 
gram 是 自然 语言 处理 NLP 中 一个 非常 重要 
的 概念 通常在 NLP 中 人们 基于 一定 的 语料库 
可以 利用 N gram 来做 以下 几类 事情 预计 或者 
评估 一个 句子 是否 合理 评估 两个 字符串 之间 的 
差异 程度 这 也是 模糊 匹配 中 常用 的 一种 
手段 语音识别 机器翻译 文本 分类 资料 链接 n gram _ 
1 包含 开源 n gram 数据集 n _ gram _ 
2n gram _ 3n gram 数据格式 那么 我们 如何 用 
N gram 来做 篇章 单元 的 分类器 呢 其实 很 
简单 了 只要 根据 每个 类别 的 语料库 训练 各自 
的 语言 模型 也 就是 上面 的 频率 分布 表 
实质上 就是 每 一个 篇章 单元 的 类别 都 有一个 
概率分布 当 新来 一个 篇章 单元 的 时候 只要 根据 
各自 的 语言 模型 计算 出 每个 语言 模型 下 
这个 篇章 单元 的 发生 概率 篇章 单元 在 哪个 
模型 的 概率 大 这篇 文本 就 属于 哪个 类 
别了 数据 稀疏 和 平滑 技术 平滑 中文 纠错 小孙 
纠错 算法 github 上 中文 纠错 项目 Cn _ Checkerxmnlpjcjc 
在线 纠错 结巴 分词 的 词典 结巴 分词 词典 高效 
的 分词 工具 结巴 THULAC 自然语言 处理 概述 概述 1 
. 基本概念 2 . 人类 语言 技术 HLT 发展 简史 
3 . HLT 研究/vn 内容/n 4/m ./i 基本/n 问题/n 和/c 
主要/b 困难/an 5/m ./i 基本/n 研究/vn 方法/n 概述/v 本/r 系列/q 
文章/n 计划/n 总结/n 整理/n 中国科学院/nt 大学/n 宗/nr 成庆/i 老师/n 自然语言 
处理 课程 相关 知识 参考 数目 统计 自然语言 处理 第二 
版 宗 成庆 1 . 基本概念 语言学 Linguistics 研究 语言 
本质 结构 和 发展 规律 的 科学 商务印书馆 现代 汉语 
词典 1996年 自然语言 人类 特有 的 书面 和 口头 形式 
的 语言 自然语言 理解 Natural Language Understanding NLU 研究 模仿 
人类 语言 认知过程 的 自然 语言 处理 方法 和 实现 
技术 的 一门 学科 计算机 科学 技术 百科全书 第三版 P1223 
宗 成庆 黄 昌宁 计算 语言学 Computation Linguistics CL 通过 
建立 形式化 的 计算 模型 来 分析 理解 和 生成 
自然 语言 的 学科 是 人工 智能 和 语言学 的 
分支 学科 计算 语言学 更加 侧重 基础理论 和 方法 的 
研究 计算机 科学 技术 百科全书 第三版 2018 5 P476 常 
宝宝 自然语言 处理 Natural Language Processing NLP 自然语言 处理 是 
研究 如何 利用 计算机 技术 对 语言 文本 句子 篇章 
或 话语 等 进行 处理 和 加工 的 一门 学科 
计算机 科学 技术 百科全书 第三版 P1223 宗 成庆 黄 昌宁 
人类 语言 技术 Human Language Technology HLT 就 字面 意思 
理解 研究 人类 语言 的 技术 上 个 世纪 五十年代 
学术界 对 机器 翻译 产生 了 浓厚 的 兴趣 并 
得到 了 实业界 的 支持 因此 国际 上 出现 了 
研究 机器 翻译 的 热潮 随着 机器 翻译 的 发展 
各种 自然语言 处理 技术 应运而生 并 逐渐 发展 壮大 形成 
了 这一 语言学 与 计算机 技术 相 结合 的 新兴 
学科 2 . 人类 语言 技术 HLT 发展 简史 1950s 
基于 模板 的 NLP 方法 1960 1980s 基于 规则 的 
方法 1990 2013 统计 NLP 方法 2013 ~ 深度 学习 
的 方法 3 . HLT 研究 内容 机器翻译 信息检索 自动 
文摘 问答 系统 信息 过滤 信息 抽取 文档 文类 语音识别 
说 话人 识别 有/v 很多/m 研究/vn 方向/n 都/d 密切相关/l 4 
. 基本 问题 和 主要 困难 基本问题 形态学 问题 句法 
问题 语义 问题 语用学 问题 语音学 问题 主要 困难 大量 
歧义 现象 词法 歧义 词性 歧义 结构 歧义 语义 歧义 
语音 歧义 多音字 歧义 大量 未知 语言 现象 随着 社会 
生活 的 发展 每时每刻 都会/nr 产生 大量 的 具有 新 
意义 的 词汇 5 . 基本 研究 方法 1 . 
理性主义 会 基于 规则 的 分析 方法 建立 符号 处理 
系统 2 . 经验主义 会 基于 大 规模 真实 语料 
语言 真实 数据 建立 计算方法 自然语言 处理 python 环境 配置 
NLTK 的 安装 1 . 自然语言 处理 的 介绍 NLP 
Natural Language Processing 是 人工智能 AI 的 一个 子 领域 
是 机器 真正 能够 理解 人类 说话 的 重要 一环 
自然语言 处理 也 不是 新的 研究 领域 早在 上个世纪 就 
开始 研究 但是 给予 计算机 环境 等等 因素 导致 这 
方面 的 发展 一直 停滞不前 再 机器学习 统计学 计算机 科学 
的 快速 发展 下 NLP 又 迎来 了 新的 春天 
在 将来 的 发展 中 也 是 非常 重要 的 
一环 具体 介绍 可以 参见 百度 百科 等 https / 
/ baike . baidu . com / item / nlp 
/ 25220 2 . 自然语言 处理 语言 工具 在 自然 
语言 处理 中 python 也 成了 当仁不让 的 语言 了 
这种 包的/nr 继承 有如 站在 巨人 的 肩膀 上 前进 
但是 这 仅 是 对 当前 已 技术 的 使用 
对于 深 层次 的 研究 确实 是 需要 花费 功夫 
的 特别是在 硕士 研究生 博士 研究生 等等 都是 是 需要 
真正 地 去 思考 语言 的 形成 这里 又 乔姆斯基 
的 形式语言 学说 中国/ns 的/uj 自然/d 语言/n 处理/v 大家/n 冯远/i 
炜/nr 教授/n 的/uj 著作/n 都是/nr 我们/r 值得/v 去/v 思考/v 和/c 
借鉴/v 的/uj 在 结合 当前 的 统计 学 机器学习 计算机 
科学 的 发展 自然语言 处理 在 python 这种 好用 的 
编程 工具 的 基础 上 会 发展 的 更好 3 
. 自然语言 处理 的 第一 步 当然 博主 是 想 
从事 这 方面 研究 的 小白 才 刚刚 起步 希望 
这 是 一个 记录 自己 成长 的 平台 也 希望 
把 自己 知道 的 学习 中 遇到 的 问题 分享 
出来 这是 开始 学习 使用 的 是 比较 出名 的 
nltk 包 当然 对于 汉字 的 分词 处理 等 据 
博主 知道 的 还有 jieba 分词 等 环境 准备 System 
window 10IDE anaconda spyder 环境 配置 似乎 anaconda 中 已经 
把 nltk 集成 了 当然 自己 也 可以 在 命令 
行中 输入 pip install nltk 前提 是 读者 已经 把 
环境 都配/nr 好了 之后 就是 打开 IDE 创建 一个 py 
文件 import nltk nltk . download 即可 下载 nltk 的 
语料 等 资源 如图 由于 国内 访问 比较慢 所以 需要 
下载 的 博主 已经 下载 好了 链接 https / / 
pan . baidu . com / s / 1WbNb h 
9 U 8 V K Y Q X Y Z 
o n b v Q 密码 dq4s 更多 信息 也 
可 查看 官网 http / / www . nltk . 
org / 路线图 请 戳 这里 