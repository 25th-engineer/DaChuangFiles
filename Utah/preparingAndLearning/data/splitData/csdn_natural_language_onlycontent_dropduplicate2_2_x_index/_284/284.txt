概述
统计自然语言处理的目的就是针对自然语言领域进行统计推理。作为一个常用的统计估计的例子，我们将考察经典建模问题，即当前词预测下一个词。词汇预测任务是一项技术可以解决的简单明了的问题。
Bins：构造等价类
利用历史词汇预测词汇，我们构造这样一个模型。模型中所有历史都是前n-1个已经出现的词，那么我们就有一个（n-1）阶马尔可夫模型，或者称N元语法模型。随着n的增加和词表数量的增加，我们把数据划分到太多的类别中，有大量的参数要去估计。所以有一些方法如“词干化”来减小词语表数量，使用2-3元语言模型来预测等。另外还有很多模型能比较好的进行预测，比如我们可以想象如果我们知道句子的主谓宾等一节结构，我们可以基于谓语来识别下一个词。但这里只介绍n元语法模型。
构建n元语法模型
对于n元语法模型的例子，我们感兴趣的是概率P（w1…wn） 和预测任务P(wn | w1….wn-1)
我们可以利用MLE，最大似然估计去预测。
这个里面存在的问题就是，对于没有出现的n元组，我们统统给了0概率，这个问题是普遍存在的，没有如此大的一个数据集能让我们满足不出现稀疏的情况。虽然有些办法试图去解决这个问题，比如我们动态调整n的大小，然后在超大的数据集上去跑。但是终究这些方法是不完备的。我们需要尝试平滑的去处理那些没有在历史中出现的情况，并且给这些情况赋予一定的概率。
Laplace法则 Lidstone法则 和 Jeffreys-Perks 法则
这种处理方式常常被非正式的称为加1法，它把一小部分概率有效地转移到了未知事件上。这里的假设是有统一的先验证概率（每个n元组都有相同的可能性），事实上就是一个贝叶斯估计。（个人觉得这个假设不太成立，但是这是一种平滑的方法吧）
这些方法有一些缺点，如在预测句子中概率都被打了折扣（我认为相对比较而言，这个折扣关系不是那么大）。并且结果证明“差的上下文不如没有”。这里可能是平滑时，将0概率比低出现的更大，导致了这种情况。这种建模是可以对概率估计进行排序的。下面放一张图解释一下相对性问题。证明该建模概率折扣与相对有效性。
留存估计
这里的留存估计与机器学习中讨论的比较接近，是一种自我验证的方法，一定程度上防止训练或者决策的过拟合。书中介绍了讲训练集划分成两部分的方法，也介绍了交叉验证法。Leaving-one-Out的方法应该就是我们机器学习中常用的N折叠法。这里就不再描述。
Good-Turing
Good根据图灵机原理提出了一种确定时间频率或者概率估计的方法，假设事件是二项分布的。这种方法适用于从大此表得来的大量数据观察，而且，尽管词汇和n-gram不服从二项式分布。（因为概率论没学太好，不太能直观体会到不同分布的感觉-_-!）,该方法利用了一个调整后的频率，参见如下。
这里具体的算法本人理解不是很透彻，但是大致思路是这样，这些方法都重新归一化了所有的概率估计，以确保得到合理的概率分布结果。比如我们调整转移到未知事件上的概率大小，或者很好的方法似乎是保持转移到未知事件上的概率N1/N不变，并且重新归一化所有的已知事件的概率估计。（Gale and Sampson 1995提出）
简单总结
为了处理空类或者说是数据集中不存在的元组，我们采用折扣的的方法将频率增益均分到未知事件上，然后有几种均分方案
绝对折扣
线性折扣
等等吧，各有各的特性。
组合估计法
对于ｎ-gram模型来说，找到合适的组合不同阶模型的方法是成功的关键。一种合并不同ｎ值的ＭＬＥ ｎ-gram估计（对于未知词有一定的概率转移）的方法使用了简单的线性插值技术，得出了一个非常好的语言模型。
简单的线性模型
解决trigram模型中稀疏问题的一种方法是，把bigram模型和unigram模型组合到trigram模型中，这两个模型容忍稀疏数据问题的能力比较强。
回退法
当没有ｎ-gram时，回退到低阶的模型。同时这里也是需要注意，需要讲概率转移到未知词语上。
贝叶斯估计是什么？