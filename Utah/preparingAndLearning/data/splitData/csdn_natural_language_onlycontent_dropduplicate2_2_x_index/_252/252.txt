暑期学习自然语言处理笔记
一、 自然语言处理的应用
自然语言处理（natural language processing，即 NLP）
（1）拼写检查纠错，关键词搜索，垃圾邮件识别
（2）文本挖掘，文本分类
（3）机器翻译
（4）自动问答、客服机器人
（5）复杂对话系统：微软小冰
二、 自然语言处理的模型
深度学习网络应用于NLP，在于特征提取的优势。深度学习中的强化学习是无监督的模型。
关于语言模型
机器翻译 （语言搭配的概率）
拼写纠错 （出现一句话的概率）
智能问答
什么是语言模型
用来计算 一句话概率的 模型。
几个词都出现的联合概率密度。在前 i-1 个词出现的条件下第 i 个词出现的概率称之为与之相关性。当词非常多的时候就会造成数据过于稀疏，参数空间太大。 如果 i 很大，参数空间过大容易过拟合，也无法实用。
如何简化问题？近似上面的公式，效果又要求比独立性假设好。
n-gram 模型
n-gram公式如下图所示。下图公式②叫做三元语法（trigram，3-gram）：
马尔科夫假设（Markov Assumption）：下一个词的出现仅依赖于它前面的一个或几个词。这对于联合概率链规则来说其实是相对粗糙的简化，位置离得较远而且关系比较弱的词语就简化省略掉了。
概率的计算过程（P=0.33 = 927/2533）：
参考网址：http://blog.csdn.net/yaoweijiao/article/details/52945186
大概了解语言模型的计算处理过程。
三、 词向量
计算机理解文本的方式——word vector，就是指将单词向量化，将某个单词用特定的向量来表示。
注意：转化的是一个词，而非一个字。如：
假设/下/一个/词/没有/出现。=>正确转化
假/设/下/一/个/词/没/有/出/现。=>错误转化
如何构造有意义的向量？
希望构造的词向量，对于意义相近的词对应的向量相关性大些。
构造是基于上下文的语境构造，与语言拼写规则本身无关。如下图，虽然单词是不同国家的语言，但是转化为的向量分布想非常相似。
关于NLP的神经网络模型。
假设一个文本（“神经网络模型。”）交给这个模型，根据上下文的前 i-1 个词（【1】【2】）输入到input layer，经过网络，让模型自己找到后面的第 i 个词（【3】）是什么。这就是模型的任务。
模型的架构：输入层=>投影层=>隐层=>输出层。
输入层：每个词的向量维数必须一致。
投影层：输入层的多个向量连成一串变成一个大向量。
输出层：类似于softmax，输出的是一组概率值。
如何用一个向量更好地表示词？ 参考网址：
http://blog.csdn.net/u013362975/article/details/53319002
四、 Hierarchical Softmax 模型
更新每个输出词向量在训练集上每个词的分布的问题是非常耗时耗力的。为了解决这个问题。两个方法：hierarchical softmax 和 negative sampling。
Hierarchical softmax：用一个二叉树代表词表中的所有词，这个词是叶子节点。对于每一个叶子节点，存在着从根到叶子的唯一路径，这个路径用来估计这个词的概率。
Negative Sampling：每次只更新一个输出词，目标输出词应该一直在样本中得到更新，并且添加一些negative samples进去。
1. CBOW模型
拿一个词语的上下文作为输入，来预测这个词语本身。（基于上下文预测某词）（ContinuousBag Of Words Model）
公式参考：
http://blog.csdn.net/dream_catcher_10/article/details/51361328
当结果分类较多（比如50分类）时，如何解决输出问题。
哈夫曼树，最优二叉树（Huffman Tree）。
路径长度是指一个结点到另外一个结点之间分支数。
带权路径长度是指每个分支上有权值，一个结点到另外一个结点所有路径权值总和。树的带权路径长度是从根结点出发到每一个叶节点的带权路径长度总和。
哈夫曼树的建立步骤：在给定的权值中选择两棵根结点权值最小的作为左右子树构造一棵新的二叉树，并将新二叉树的根结点的新权值再替代原来两个小权值放入原权值中重新挑选两棵根结点权值最小，不断迭代创建左右子树。
哈夫曼编码 参考网址：
http://blog.csdn.net/qq_19762007/article/details/50708573
逻辑回归
不属于回归分析，而是属于分类，差异主要在于变量不同。逻辑回归是无监督学习的一个重要算法，对某些数据与（事物的归属类别）及（分到某一类别的概率）进行评估。
logistic（即sigmoid）具体针对的是二分类问题，而softmax解决的是多分类问题。sigmoid函数在这里将得分值转化为概率。
到输出层则利用上下文词向量的拼接和做为输入，输出的是窗口中心位置所有词出现的概率。利用softmax求中心词概率，当语料较大时，计算变的非常耗时。于是为了解决这个问题，利用哈夫曼树对词表进行分类，用一连串的二分类来近似多分类 。
哈夫曼编码，一句话就是 频率越高编码越短。
哈夫曼编码怎么用的，先将词表的词频统计好，词频高的放在接近于跟根节点的位置，词频低的放在叶子。
训练不仅仅针对 θ 的更新，还有输入的词向量 x 要更新。
损失函数中有2个待求参数：θ、x。在训练CBOW模型时，词向量只是个副产品。确切来说，是CBOW模型的一个参数。
参考网址：
http://blog.csdn.net/qwe11002698_ling/article/details/53888284
http://blog.csdn.net/dream_catcher_10/article/details/51361328
2. skip-gram模型
用一个词语作为输入，来预测它周围的上下文。（基于当前词预测上下文）（Continuous Skip-gram Model）
该模型与CBOW类似。参考资料：
http://blog.csdn.net/qwe11002698_ling/article/details/53888284
http://www.cnblogs.com/tina-smile/p/5204619.html
五、 Negative Sampling 模型
Negative Sampling：负采样
已知一个词w，它的上下文是context(w)，那么词w就是一个正例，其他词就是一个负例。但是负例样本太多了，我们怎么去选取呢？
在语料库C中，各个词出现的频率不一样，采样的时候要求高频词选中的概率较大，而低频词选中的概率较小。这就是一个带权采样的问题。 随机抽取负样本，随机数生成满足均匀分布，而取词概率可不是均匀分布，其概率应当随着词频大小变化。
将词频转换为线段长度。选取负例样本的时候，取线段上的一个随机数，对应到相应词频区间上就可以了。
优化求偏导过程类似于 Hierarchical Softmax。
参考网址：
http://blog.csdn.net/chunyun0716/article/details/51722230
http://blog.csdn.net/suibianti/article/details/68483231#基于negative-sampling的模型