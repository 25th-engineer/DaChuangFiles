目录
1.1 自然语言处理的挑战
1.2 神经网络和深度学习
1.3 自然语言处理中的深度学习
1.1 自然语言处理的挑战
自然语言处理是一个设计输入与输出为非结构化自然语言数据的方法和算法的研究领域。人类语言有很强的歧义性（如句子“I ate pizza with friends”（我和朋友一起吃披萨）和“I ate pizza with olives”(我吃了有橄榄的披萨)）和多样性（如“I ate pizza with friends”也可以说成“Friends and I shared some pizza”）。语言也一直在进化中。人善于产生和理解语言，并具有表达、感知、理解复杂且微妙信息的能力。与此同时，虽然人类是语言的伟大使用者，但是我们并不善于形式化地理解和描述支配语言的规则。
使用计算机理解和产生语言因此极具挑战性。事实上，最为人所知的处理语言数据的方法是使用有监督机器学习算法，其试图从事先标注好的输入/输出集合中推导出使用的模式和规则。例如，一个将文本分为四类的任务，类别为：体育、政治、八卦、经济。显然，文本中的单词提供了很强的线索，但是到底哪些单词提供了什么线索呢？为该任务书写规则极具挑战性。然而，读者可以轻松地将一篇文档分到一个主题中，然后，基于几百篇认为分类的样例，可以让有监督机器学习产生用词的模式，从而帮助文本分类。机器学习方法擅长那些很难获得规则集，但是相对容易获得给定输入及相应输出样本的领域。
除了使用不明确规则集处理歧义和多样输入的挑战外，自然语言展现了另外一些特性，其使得用包括机器学习在内的计算方法更具挑战性，即离散性（discrete）、组合性（compositional）和稀疏性（sparse）。
语言是符号化和离散的。书面语义的基本单位是字符，字符构成了单词，单词再表示对象、概念、事件、动作和思想。字符和单词都是离散符号：如“hamburger”或“pizza”会唤起我们头脑中的某种表示，但是它们也是不同的符号，其含义是不相关的，待我们的大脑去理解。从符号自身看，“hamburger”和“pizza”之间没有内在的关系，从构成它们的字母看也一样。与机器视觉中普遍使用的如颜色的概念或声学信号相对比，这些概念都是连续的，如可以使用简单的数学运算从一幅彩色图像变为灰度图像，或者从色调、光强等内在性质比较两幅图像。对于单词，这些都不容易做到，如果不使用一个大的查找表或者词典，没有什么简单的运算可以从单词“red”变为单词“pink”.
语言还具有组合性，即字母形成单词，单词形成短语和句子。短语的含义可以比包含的单词更大，并遵循复杂的规则集。为了理解一个文本，我们需要超越字母和单词，看到更长的单词序列，如句子甚至整篇文本。
以上性质的组合导致了数据稀疏性（data sparseness）。单词（离散符号）组合并形成意义的方式实际上是无限的。可能合法的句子数是巨大的，我们从没指望能全部枚举出来。随便翻一本书，其中绝大部分句子是你之前从没看过和听过的。甚至，很有可能很多四个单词构成的序列对你都是新鲜的。如果你看一下过去10年的报纸或者想像一下未来10年的报纸，许多单词，特别是人名、品牌和公司以及俚语和术语都将是新的。我们也不清楚如何从一个句子生成另一个句子或者定义句子之间的相似性，也不依赖于它们的意思——对我们是不可观测的。当我们要从实例中学习时也是挑战重重，即使有非常大的实例集合，我们仍然很容易观测到实例集合中从没有出现过的事件，其与曾出现过的所有实例都非常不同。
1.2 神经网络和深度学习
深度学习是机器学习的一个分支，是神经网络的重命名。神经网络是一系列学习技术，历史上曾受模拟脑计算工作的启发，可被看作学习参数可微的数学函数。深度学习的名字源于许多曾被连接在一起的可微函数。
虽然全部机器学习技术都可以被认为是基于过去的观测学习如何做出预测，但是深度学习方法不仅学习预测，而且学习正确地表示数据，以使其更有助于预测。给出一个巨大的输入-输出映射集合，深度学习方法将数据“喂”给一个网络，其产生输入的后继转换，直到用最终的转换来预测输出。网络产生的转换都学习自给定的输入-输出映射，以便每个转换都使得更易于将数据和期望的标签之间建立联系。
开发者负责设计网络结构和训练方式，提供给网络合适的输入-输出实例集合，将输入数据恰当地编码，大量学习正确表示的工作则由网络自动执行，同时受到网络结构的支持。
1.3 自然语言处理中的深度学习
神经网络提供了强大的学习机制，对自然语言处理问题极具吸引力。将神经网络用于语言的一个主要组件是使用嵌入曾（embedding layer），即将离散的符号映射为相对低维的连续向量。当嵌入单词的时候，从不同的独立符号转换为可以运算的数学对象。特别地，向量之间的距离可以等价于单词之间的距离，这使得更容易从一个单词泛化到另一个单词。学习单词的向量表示成为训练过程的一部分。再往上层，网络学习单词向量的组合方式以更有利于观测，该能力减轻了离散和数据稀疏问题。
有两种主要的神经网络结构，即前馈网络（feed-forward network）和循环/递归网络（recurrent/recursive network），它们可以以各种方式组合。
前馈网络，也叫多层感知器（Multi-Layer Perceptron）,其输入大小固定，对于变化的输入长度，我们可以忽略元素的顺序。当将输入集合喂给网络时，网络学习用有意义的方式组合它们。之前线性模型所能应用的地方，多层感知器都能使用。网络的非线性以及易于整合预训练词嵌入的能力经常导致更高的分类精度。
卷积前馈网络是一类特殊的结构，其善于抽取数据中有特殊意义的局部模式；将任意长度的输入“喂”给网络，网络能抽取有意义的局部模式，这些模式对单词顺序敏感，而忽略它们在输入中出现的位置。这些工作适合于识别长句子或者文本中有指示性的短语和惯用语。
循环神经网络是适于序列数据的特殊模型，网络接收输入序列作为输入，产生固定大小的向量作为序列的摘要。对于不同的任务，“一个序列的摘要”意味着不同的东西（也就是说，用于回答一个句子情感所需的信息与回答其语法的信息并不相同）。循环网络很少被当做独立组件使用，其能力在于可能当做可训练的组件“喂”给其他网络组件，然后串联地训练它们。例如，循环网络的输出可以“喂”给前馈网络，用于预测一些值。循环网络被用作一个输入转换器，其被训练用于产生富含信息的表示，前馈网络将在其上进行运算。对于序列循环网络是非常引人注目的模型，可能也是神经网络用于自然语言最令人激动的成果。它们允许：打破自然语言处理中存在几十年的马尔科夫假设，设计能依赖整个句子的模型，并在需要的情况下考虑词的顺序，同时不太受由于数据稀疏造成的统计估计问题之苦。该能力是语言模型产生了令人印象深刻的收益，其中语言模型指的是预测序列中下一个单词的概率（等价于预测一个序列的概率），是许多自然语言处理应用的核心。递归神经网络将循环网络从序列扩展到树。
自然语言处理的许多问题是结构化的，需要产生复杂的输出结构，如序列和树。神经网络模型能适应该需求，一方面可以改进已知的面向线性模型的结构化预测算法，另一方面可以使用新的结构，如序列到序列（编码器-解码器）模型，指的是条件生成模型。此类模型是目前公认的最好的机器翻译模型的核心。
最后，许多自然语言预测任务互相关联，在某种意义上知道一种任务是如何执行的将对另一些任务有所帮助。另外，我们可能没有足够的有监督（带标签）训练数据，而只有足够的原始文本（无标签数据）。那我们能从相关的任务或者未标注数据中学习吗？对于多任务学习（Multi-Task Learning，即从相关问题中学习）和半监督（semi-supervised）学习（从额外的、未标注的数据中学习），神经网络方法提供了令人激动的机会。
注:文章内容摘自Yoav Goldberg所著《Neural Network Methods for Natural Language Processing》的中文版《基于深度学习的自然语言处理》chapter 1 Introduction