目录
文章目录
目录
前言
Markov模型1
Markov模型2
Markov 模型3
Markov模型4
Markov模型（5）
前言
硕士生涯结束，开始专心做一件自己觉得有用的工具，先做工程，后搞理论。
自然语言处理是一个非常难的问题，同时是人工智能皇冠上的明珠。
接下来会记录一系列自然语言处理的笔记，来自于哈工大老师关毅。
Markov模型1
设
X
=
（
X
1
,
X
2
,
.
.
.
X
t
）
是
随
机
变
量
序
列
，
其
中
每
个
随
机
变
量
的
取
值
在
有
限
集
S
=
s
1
,
S
2
,
称
为
状
态
空
间
，
时
间
不
变
性
假
设
X=（X_1,X_2,...X_t）是随机变量序列，其中每个随机变量的取值在有限集S={s_1,S_2},称为状态空间，时间不变性假设
X=（X1 ,X2 ,...Xt ）是随机变量序列，其中每个随机变量的取值在有限集S=s1 ,S2 ,称为状态空间，时间不变性假设
N阶Markov模型，只需修改状态空间的定义S’={X}。定义新的变量
X
i
b
e
l
o
n
g
t
o
s
′
X_i belong to s&#x27;
Xi belongtos′
使得
X
t
=
(
S
i
−
1
,
S
i
)
X_t=(S_{i-1},S_i)
Xt =(Si−1 ,Si )并且约定：
P
(
X
i
∣
X
i
−
1
)
=
P
(
(
S
i
−
1
,
S
i
)
∣
(
S
i
−
2
，
S
i
−
3
)
)
P(X_i|X_{i-1})=P((S_{i-1},S_i)|(S_{i-2}，S_{i-3}))
P(Xi ∣Xi−1 )=P((Si−1 ,Si )∣(Si−2 ，Si−3 ))
Markov模型的形式化表示，一个马尔可夫模型是一个三元组
（
S
，
π
,
A
）
（S，\pi,A）
（S，π,A）,其中S是状态的集合，
π
\pi
π是初始状态的概率，A是状态间的转移概率。
发射字符依赖于当前状态，不同状态，有不同输出。
HMM：不同状态可以有相同输出，输出在状态转移中进行。
Markov模型2
HMM模型：
最大的灵活性在状态转移中以特定概率输出。
##HMM模型：
HMM是一个五元组（S,k,pi,a,b），其中s是状态的集合，k是输出字符的集合，pi是初始状态的概率，a是状态转移的概率。b是状态转移时输出字符的概率。
t:=1
以概率
p
i
p_i
pi 在状态
S
i
S_i
Si 开始（ie，X1=i）
forever do
move from state Si to state Sj with
probability
A
i
j
(
i
,
e
,
.
.
X
t
+
1
=
j
)
A_{ij}(i,e,..{X_{t+1}=j})
Aij (i,e,..Xt+1 =j)
Emit observation symbol Ot=k
with probability b
t:=t+1
end
##HMM的基本问题
给定一个输出的字符序列。如何调整模型的参数使得产生这一序列的概率最大，IBM Watson医生。 隐马模型的基本问题：给定一个模型M=（S,k,pi,a,b），如何高效地计算某一输出字符序列的概率P（O|u）。
给定一个输出字符序列O和一个模型u，如何确定产生这一序列概率最大的状态序列
（X1，x2）
词网格分类，音字转换。网格cell states。
问题1：评价（evaluation）
给定一个模型u=（s,k,pi,a,b）如何高效地计算某一输出字符序列的概率P（O|u）。
O=（o1,o2,…,or）,u=(a,b,pi)
计算P（O|u）。
给定词网格最优路径
方案一：直观方法。
X1–>o1
P(o|x,u)=bx1oz=
∑
P
(
O
∣
X
,
U
)
∗
P
(
X
∣
u
)
\sum P(O|X,U)*P(X|u)
∑P(O∣X,U)∗P(X∣u)
动态规划，递推求解。
α
i
(
t
)
=
P
(
O
1
,
.
.
O
i
∣
X
t
)
\alpha_i(t)=P(O1,..Oi|X_t)
αi (t)=P(O1,..Oi∣Xt )
方案2：向前过程
=
∑
i
=
1
α
i
(
t
)
∗
b
j
α
i
j
b
j
∗
α
t
+
1
\sum_{i=1}\alpha_i(t)*b_j\alpha_{ij}b_j*\alpha_{t+1}
i=1∑ αi (t)∗bj αij bj ∗αt+1
Markov 模型3
向前过程
RRGB
动态规划法
向后过程概述：
KaTeX parse error: Expected 'EOF', got '\lmd' at position 5: P(O|\̲l̲m̲d̲)=\sum_{1<j<N}p…
算法效率与前算法相同。
用途：参数训练问题的一个重要组成部分。
##解码
确定产生概率最大的状态
delta为在t时刻到达状态j，输出字符Ot时，输出前面t-1
个字符的最可能路径的概率。
delta_j(t)=max_{xi…xt+1}P(x1…xt+1,O1…Ot-1,Xt=1,Ot)
delta+{t+1}(j)=max_deltat(j)aijbij(ot+1)
viterbi algorithm:
初始化：
delta（i）=piibi（Oi）
phi（i）=0
递归：
最优路径 qt=phi_t=1（Qt+1）
把连乘变成加。
参数统计
argmax_uP(O|u)
Markov模型4
设计更新计算更新值。basic思想。
设定模型的初始值，U-old。
基于U_old计算输出U_new和O的概率。
如果P（o|u_new）-P(O|u_old)<某个阈值
停止
否则，U_old<-U_new返回step2.
Baum-Welch算法。
向前向后算法。
基于HMM的词性标注。
词性标注：
作用句法分析的前期步骤
难点：兼类词。
词性标准应用：
Tbest=argmaxPr(T|s)=argmaxP(S|t)P(T)
如何计算P(S|t)和P(T)
简化：
词wi的出现，仅仅依赖于它的词性标记，标记ti的出现仅仅条件依赖于它前面的标记t_i-1
公式转化 计算P(S|T)和P(T)
Pr(S|t)Pr(t)=\timr P(Wi|ti)P(Ti|ti-1)
使用最大相似度估计：
P(Ti|ti-1)=c(ti,tj)/c(ti)
音字转换
发射字符：状态是什么？
发射字是什么？
不是什么？转化为生产力的学习。
Markov模型（5）
HMM评价，解码编码问题
ch6尾声，音字转换
T=argmax（v|s）
语言单位间的远距离约束
递归模型
规则与统计相结合
采用规则的方法：
短语结合规则：
A+NP->NP
A+‘的’+NP->NP
M+‘枝’+NP->NP
短语匹配算法。
从词网格到元素网格
颗粒度疏，工作量太大。
规则匹配强度不够。
做了几个宣传词，要有自己的优势项。
还做了系统挂接问题。