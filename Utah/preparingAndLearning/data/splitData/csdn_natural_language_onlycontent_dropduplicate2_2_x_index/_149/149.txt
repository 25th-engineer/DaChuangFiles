先介绍一下我自己，我有过5年以上机器学习的工作经验，主要工作内容有图像分析,自然语言，模式识别。我认为该领域最稀缺的人才是NLP专业，然后是图像分析（CV），我准备做一个系列的文章，把我在面试过程中遇到的各种技术性问题，每个问题分别讲解。
1.我常常会遇到问LSTM的问题： 现在详细讲解下
理解LSTM前要先理解： RNN
（Recurrent Neural Networks)这种神经网络带有环，可以将信息持久化。
在上图所示的神经网络AA中，输入为XtXt，输出为htht。AA上的环允许将每一步产生的信息传递到下一步中。环的加入使得RNN变得神秘。不过，如果你多思考一下的话，其实RNN跟普通的神经网络也没有那么不同。一个RNN可以看作是同一个网络的多份副本，每一份都将信息传递到下一个副本。RNN在一系列的任务中都取得了令人惊叹的成就，比如语音识别，图片标题等等。
LSTM是这一系列成功中的必要组成部分。LSTM(Long Short Term Memory)是一种特殊的循环神经网络，在许多任务中，LSTM表现得比标准的RNN要出色得多。几乎所有基于RNN的令人赞叹的结果都是LSTM取得的，接下来将着重介绍LSTM。
长期依赖(Long Term Dependencies)的问题
RNN的一个核心思想是将以前的信息连接到当前的任务中来，例如，通过前面的视频帧来帮助理解当前帧。如果RNN真的能够这样做的话，那么它们将会极其有用。但是事实真是如此吗？未必。
有时候，我们只需要看最近的信息，就可以完成当前的任务。比如，考虑一个语言模型，通过前面的单词来预测接下来的单词。如果我们想预测句子“the clouds are in the sky”中的最后一个单词，我们不需要更多的上下文信息——很明显下一个单词应该是sky。在这种情况下，当前位置与相关信息所在位置之间的距离相对较小，RNN可以被训练来使用这样的信息。
然而，有时候我们需要更多的上下文信息。比如，我们想预测句子“I am a tall man, .....i can play basketball”中的最后一个单词。最近的信息告诉我们，最后一个单词可能是某种语言的名字，然而如果我们想确定到底是哪种语言的话，我们需要basket这个更远的上下文信息。实际上，相关信息和需要该信息的位置之间的距离可能非常的远。
随着距离的增大，RNN对于如何将这样的信息连接起来无能为力。
LSTM，全称为长短期记忆网络(Long Short Term Memory networks)，是一种特殊的RNN，能够学习到长期依赖关系。LSTM由Hochreiter & Schmidhuber (1997)提出，许多研究者进行了一系列的工作对其改进并使之发扬光大。LSTM在许多问题上效果非常好，现在被广泛使用。
LSTM在设计上明确地避免了长期依赖的问题。记住长期信息是小菜一碟！所有的循环神经网络都有着重复的神经网络模块形成链的形式。在普通的RNN中，重复模块结构非常简单，例如只有一个tanh层。
我会在专栏和视频中免费给大家具体讲解细节的技术。