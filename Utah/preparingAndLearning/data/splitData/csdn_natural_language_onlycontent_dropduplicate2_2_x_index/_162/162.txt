前言
TensorFlow官网教程 Vector Representation of words主要是介绍了谷歌2013开源的 word2vec 工具包中的两个模型 CBOW(Continuous Bag-of-words Model)和 Skip-gram 模型（Continuous Skip-gram Model）。
在讲具体的模型之前，先要介绍一点自然语言处理的基础知识
什么是语言模型？
通俗的讲，语言模型就是某种自然语言的模型，它用来判断给定的语句是否属于该自然语言范畴，即判断语句是否符合该自然语言所属的语义、语法和逻辑。
那放到机器学习里该怎样做呢？
首先进行一些定义操作。
假设某自然语言为Ω，给定一个属于该自然语言的文本数据集，记为T，T就是自然语言Ω的样本集。然后我们把文本按单词拆解，拆解后所有单词组成的集合叫做语料库，记为C，再把语料库中的单词去重复，称为词典，记为D。
那我们来简单算一下，我们事先要统计多少个值？
但是这类算法有个问题，就是避免不了还是要提前反复扫描文本，并且存储统计值，而且对于样本集没有遇见的情况，统计的概率为0，因此对没有遇到的情况泛化能力很差。
什么是神经概率语言模型？
神经概率语言模型（Neural Probabilistic Languange Model）的想法很简单，就是使用神经网络来拟合函数F(*)。
该神经网络包含四层结构，输入层，映射层、隐层和输出层。
输入层神经元的个数是 context(w)的单词数量，一般定义的context(w)是固定长度，对于context(w)不足固定长度的情况，使用填充向量进行填充，此处按下不表。
映射层负责将输入层的输入映射为词向量，这里插播一下什么是词向量。
什么是词向量？
词向量是 vector representation of words 又叫做 word embeddings，就是把语料库中的单词映射为固定长度的向量的技术。
由于机器只能处理二进制数据，不能直接识别和处理文本字符，因此我们首先要将文本数据进行编码。
传统的编码方式叫做one-hot-reprensentation。实现方式很简单，假设你的语料库中的词典数量为n，则将这n个词按一定的顺序排列。每次词对应一个序号i，然后将词表示为长度为n的向量，该向量的第i个元素为1，其余元素全部为0。
这个方式虽然简单，但存在很多问题：
1、向量的长度n等于语料库的词汇量，而一般的语料库的词汇量成千上万的，都非常大，所以这样构造的数据集维度很大，过于稀疏，容易造成维数灾难。
2、这种做法无法表示出单词和单词之间的关联性。比如dog和cat应该是非常类似的两个单词，但被映射为与其他单词没有区别差异的0,1组成的向量
因此 word embeddings 的目的在于将所有的词汇映射到m维空间中，将语法或者语义相近的词表示为空间中邻近的点。m的大小一般可以人为指定。word embedding一般是所有自然语言处理过程的第一步，也是最重要的一步。
我们再把话题转回到神经概率语言模型中来。
映射层负责将文本字符映射成词向量，至于怎样实现映射的，事先是不知道的，我们需要把这种映射关系也放到神经网络中进行训练（将映射矩阵定义为未知参数）。因此词向量变为了神经网络语言模型的附属产物。
隐层是全连接结构，神经元的数量可以人为指定，一般还会对隐层的输出再作用一个tanh激活函数。
假设训练集的词典数目为N，则定义输出层的神经元个数为N，输出层的神经元与词典D中的单词一一对应。输出层也是全连接结构。因此整个神经网络的计算过程如下
输入层：Input
映射层：P = Prpject(Input)
隐层：H = tanh(W*P +bias_p)
输出层: Output = U*H + bias_o
W、U是网络权重，bias_p、bias_o是偏置项
CBOW和Skip-gram模型就是在此基础上衍生出来的。
CBOW是用上下文context(w) 预测目标 w，而skip-gram则是逆向操作，用目标 w预测 context(w)。但二者的原理相同，因此这里以CBOW为例进行介绍。
CBOW模型
神经网络的结构保持不变，让我们换个角度来看待问题。
在神经概率语言模型中，我们把问题看作是一个N分类问题，使用softmax regression进行分类，而现在我们现在把问题看成是N个二分类问题，每个单词 w_i∈D 对应一个二分类问题
y = 1：某context input对应的目标单词是该单词
y = 0：某context input对应的目标单词不是该单词
因此在每一个样本对应的似然函数中计算项由原先的N降低为k+1，k的大小一般由人为指定，k<< N，从而大大减少了计算量
Tensorflow中Skip-gram模型的代码实现
我们以word2vec_basic.py为例简单讲解一下语言模型的代码实现。
word2vec_basic.py使用的模型是Skip-gram，具体的实现和之前的神经网络教程区别不大，我们这边只讲讲不同的地方。
不同的地方在于：1、输入数据的构造；2、映射层的构造 3、目标函数的定义
1、输入数据的构造
输入数据的构造负责将自然语言文本转换成计算机能识别的数据。这是初级翻译操作，还不是word embedding。
我们将字典D中的N个单词进行编号，编号范围0~(N-1)，编号方式可以任意选择。给定一段自然语言文本作为输入，假设文本固定长度为n，将其拆解为单词w_1,w_2,…,w_n，然后将每个单词对应的编号feed给tensor placeholder: train_inputs（shape = [batch_size, n]）
2、映射层的构造
创建映射矩阵tensor variable: embeddings, 其shape = [N, M]。然后对该矩阵进行随机初始化。
M为人为规定的词向量的长度，则映射矩阵的embeddings[i, :]对应的是第i个单词的词向量。embeddings是variable类型的对象，属于神经网络的待优化参数。
调用embed = tf.nn.embedding_lookup(embeddings, train_inputs)实现词向量映射。
返回的embed是映射层的输出：输入数据的词向量映射结果，shape = [batch_size, n, M]
embed[i, j, :] = embeddings[ train_inputs[i, j ], : ]
可以理解为embeddings是词向量字典，train_inputs是索引，将train_inputs中每个元素替换为该元素值对应的embeddings中相应序号的向量。
3、目标函数的定义
loss = tf.reduce_mean(tf.nn.nce_loss(nce_weights, nce_biases, embed, train_labels, num_sampled, vocabulary_size))
这里使用的是NEC loss是Negative Sampling的近似替代。
nce_weights、 nce_biases：输出层的权重和偏置项
embed：映射层的输出，PS：在这个例子中，神经网络没有隐层。
train_labels ：训练集的真实标注
num_sampled 指定随机负采样 negative samplings的数量
vocabulary_size ：词典数量，也是输出层神经元的数量。
然后剩下的工作就交给tf.nn.nce_loss来完成了。
由此可见，虽然CBOW和skip-gram模型稍微有点复杂，但在实际的代码实现中tensorflow已经把大部分的工作都提前封装好了，我们只需要一行简单的代码调用即可以实现。但理解一下它内部的原理肯定还是益处多多的。
注：自然语言处理入门级小白，上述如有不妥之处，欢迎批评指正