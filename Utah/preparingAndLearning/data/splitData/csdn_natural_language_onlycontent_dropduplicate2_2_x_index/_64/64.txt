RNN语言模型
RNN语言模型
语言模型
RNN语言模型
模型扩展
语言模型
语言模型就是指语言产生的规律，一般用来预测所使用语言语序的概率，或者是当前上下文使用某个词语的概率。换句话说，就是用来表示语言产生顺序的建模，用某个词是否恰当，这样的语序构造句子是否妥当这样的。于是，训练出一个语言模型就需要相当大的样本数据。语言模型可以分为：文法型的语言模型（就是定义相关的文法结构，例如主语+谓语+宾语构成陈述句这样的），统计模型，神经网络语言模型。
其中统计类的语言模型包括N-gram，N-pos，隐马尔科夫链模型、最大熵模型等。就是给出前边的词，判断后面出现词的概率。
p(w3|w1w2)
p(w_3|w_1w_2)表示
w3
w_3在词语
w1w2
w_1w_2之后出现的概率。具体计算公式为
p(w3|w1w2)=p(w1w2w3)p(w1w2)=Count(w1w2w3)Count(w1w2)
p(w_3|w_1w_2)=\frac{p(w_1w_2w_3)}{p(w_1w_2)}=\frac{Count(w_1w_2w_3)}{Count(w_1w_2)}, Count(x)表示x在语料库中出现的频率。这种模型能给出后面单词发生的概率。但是会出现Count(x)=0的情况，为避免这种问题出现了很多平滑技术，例如Laplace平滑等。
但是这种统计模型的计算非常消耗内存。
RNN语言模型
RNN语言模型就是利用RNN神经网络对语言建模，用于描述语言序列的产生过程。RNN神经网络就是循环神经网络，能很好地拟合序列数据。
假设当前你有大量文本语料库C，根据这个预料你构建了词典V，然后你做分句，把每句话通过扩展变成等长的句子。句子开始以START标志，结束以EOS结束，使用PAD来进行短句子的填充。现在得到长度为L的sequence序列。每个词使用vector进行表示（1-of-N model）序列为
x1,x2,...,xL
x_1,x_2,...,x_L，假设
x1
x_1是词典V中的第一个词，V的大小为N，则
x1=[1,0,0,...,0]
x_1=[1,0,0,...,0].对于RNN输出数据对应的True Value这里选择使用
x2,x3,...,xL−1,EOS
x_2,x_3,...,x_{L-1},EOS, 使用符号表示为
y1,y2,...,yL
y_1,y_2,...,y_L 对于RNN预测数据表示为
y′1,y′2,...,y′L
y'_1,y'_2,...,y'_L。
ht=f(whh∗ht−1+wxh∗xt)
\begin{equation} h_t = f(w_{hh}*h_{t-1}+w_{xh}*x_t) \end{equation}
y′t=g(ht)
\begin{equation} y'_t = g(h_t) \end{equation}
y′t
y'_t是一个N维（词典的大小）向量，表示一个概率分布，即下一个词语出现的概率在词典中的概率分布。
y′t(n)
y'_t(n)表示下一个词是词典中第n个词的概率大小。
损失函数定义维：
Loss=−1L∑t=1L∑j=1Nyt(j)log(y′t(j))
\begin{equation} Loss = -\frac{1}{L}\sum_{t=1}^{L}\sum_{j=1}^{N}y_t(j)log(y'_t(j)) \end{equation}
求导根据BackPropogation+SGD进行训练。最小化损失函数。
模型扩展
一般对于RNN的训练采用BPTT的算法。
当L较大时，模型的训练会出现梯度消失和梯度爆炸的问题。
对于梯度爆炸可以采取Clipping的方法解决，具体就是设置门限，超过这个门限时，进行该梯度方向上的归一化。
对于梯度消失，可以采用LSTM或GRU来替代SRNN；或者使用ReLU来替代Sigmoid激励函数。