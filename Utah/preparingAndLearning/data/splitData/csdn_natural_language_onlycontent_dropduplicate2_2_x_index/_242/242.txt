文章目录
自然语言处理基本概念
语言的数学本质
统计语言模型
N-Gram Model
分词
信息度量
信息熵
信息的作用
互信息
相对熵
信息熵的应用 -- 决策树
Feature Extraction and Preprocessing
One-hot encoding
Bag Of Words Model
Sparse Vectors
Stop-word filter
Lemmatization vs Stemming
TF-IDF
TF-IDF的信息论依据
文本分类样例
Summary
自然语言处理基本概念
语言的数学本质
语言的出现是为了通信，通信的本质是为了传递信息。字母，文字，数字都是信息编码的不同单元。任何一种语言都是一种编解码算法。
我们通过语言把要表达的意思传递出来，实际上就是用语言将大脑中的信息进行了一次编码，形成了一串文字。懂得这种语言的接收方就能够使用这种语言进行解码，然后获取到里面的信息。这就是语言的数学本质。
统计语言模型
机器是不懂得任何一种语言的，早期的自然语言处理方式是让计算机学习理解语言的语义，语法，然后据此判断一个句子是否合理，含义是什么。但最终证明这种研究方向和学习方式是行不通的。
现在的自然语言处理是基于统计语言模型，它根本不需要计算机理解人类的语言，它要做的就是判断一个句子是否合理，就看这个句子在语料库中出现的概率如何。
假定S表示某一个有意义的句子，由一连串的词
w
1
,
w
2
,
⋯
&ThinSpace;
,
w
n
w_1, w_2, \cdots, w_n
w1 ,w2 ,⋯,wn 组成，
n
n
n是句子的长度。如果想知道S在文本中出现的概率
P
(
S
)
P(S)
P(S)，那就需要把有史以来人类讲过的话统计一下，然后计算出出现的概率。这种方法很显然是行不通的。因此，需要一个模型来估算。由于
S
=
w
1
,
w
2
,
⋯
&ThinSpace;
,
w
n
S = w_1, w_2, \cdots, w_n
S=w1 ,w2 ,⋯,wn ，那么
P
(
S
)
=
P
(
w
1
,
w
2
,
⋯
&ThinSpace;
,
w
n
)
P(S) = P(w_1, w_2, \cdots, w_n)
P(S)=P(w1 ,w2 ,⋯,wn )，利用条件概率公式，S出现的概率等于每一个词出现的条件概率的乘积
P
(
w
1
,
w
2
,
⋯
&ThinSpace;
,
w
n
)
=
P
(
w
1
)
⋅
P
(
w
2
∣
w
1
)
⋅
P
(
w
3
∣
w
1
,
w
2
)
⋯
P
(
w
n
∣
w
1
,
w
2
,
⋯
&ThinSpace;
,
w
n
−
1
)
P(w_1, w_2, \cdots, w_n) = P(w_1) \cdot P(w_2|w_1) \cdot P(w_3|w_1, w_2) \cdots P(w_n|w_1, w_2, \cdots, w_{n-1})
P(w1 ,w2 ,⋯,wn )=P(w1 )⋅P(w2 ∣w1 )⋅P(w3 ∣w1 ,w2 )⋯P(wn ∣w1 ,w2 ,⋯,wn−1 )
其中
P
(
w
2
∣
w
1
)
P(w_2|w_1)
P(w2 ∣w1 )表示在已知以一个词出现的前提下，第二个词出现的概率，以此类推，
w
n
w_n
wn 的出现概率取决于它前面所有的词。但这种条件概率的可能性太多，非常难以计算。俄国数学家马尔科夫提出了一个偷懒但是有效的做法，即马尔科夫假设模型来简化这种计算：任意一个词
w
i
w_i
wi 出现的概率只同它前面的词
w
i
−
1
w_{i-1}
wi−1 有关，简化后S出现的概率为：
P
(
S
)
=
P
(
w
1
)
⋅
P
(
w
2
∣
w
1
)
⋅
P
(
w
3
∣
w
2
)
⋯
P
(
w
n
∣
w
n
−
1
)
P(S) = P(w_1) \cdot P(w_2|w_1) \cdot P(w_3|w_2) \cdots P(w_n|w_{n-1})
P(S)=P(w1 )⋅P(w2 ∣w1 )⋅P(w3 ∣w2 )⋯P(wn ∣wn−1 )
该公式对应的统计语言模型为二元模型(Bigram Model)。
以上是理论，那么在实际的机器学习中是如何操作的呢？
首先计算
P
(
w
i
∣
w
i
−
1
)
P(w_i|w_{i-1})
P(wi ∣wi−1 )，根据条件概率的定义
P
(
w
i
∣
w
i
−
1
)
=
P
(
w
i
,
w
i
−
1
)
P
(
W
i
−
1
)
P(w_i|w_{i-1}) = \frac{P(w_i, w_{i-1})}{P(W_{i-1})}
P(wi ∣wi−1 )=P(Wi−1 )P(wi ,wi−1 ) ，只需估计联合概率
P
(
w
i
,
w
i
−
1
)
P(w_i, w_{i-1})
P(wi ,wi−1 )和边缘概率
P
(
w
i
−
1
)
P(w_{i-1})
P(wi−1 )，就变得很简单。基于大量的语料库(Corpus)，只需要统计
w
i
−
1
,
w
i
w_{i-1}, w_i
wi−1 ,wi 这对词在统计的文本中出现的次数
#
(
w
i
−
1
,
w
i
)
\#(w_{i-1}, w_i)
#(wi−1 ,wi )以及
w
i
−
1
w_{i-1}
wi−1 本身在同样的文本中出现的次数
#
(
w
i
)
\#(w_i)
#(wi )，然后用这两个数分别除以语料库的大小
#
\#
#，即可得到这些词的相对频度：
f
(
w
i
−
1
,
w
i
)
=
#
(
w
i
−
1
,
w
i
)
#
f(w_{i-1}, w_i) = \frac{\#(w_{i-1}, w_i)}{\#}
f(wi−1 ,wi )=##(wi−1 ,wi )
f
(
w
i
−
1
)
=
#
(
w
i
−
1
)
#
f(w_{i-1}) = \frac{\#(w_{i-1})}{\#}
f(wi−1 )=##(wi−1 )
然后根据大数原理，只要统计量足够，相对频度就等于概率，即
P
(
w
i
,
w
i
−
1
)
≈
#
(
w
i
−
1
,
w
i
)
#
P(w_i, w_{i-1}) \approx \frac{\#(w_{i-1}, w_i)}{\#}
P(wi ,wi−1 )≈##(wi−1 ,wi )
P
(
w
i
−
1
)
≈
#
(
w
i
−
1
)
#
P(w_{i-1}) \approx \frac{\#(w_{i-1})}{\#}
P(wi−1 )≈##(wi−1 )
最终简化后，
#
\#
#约掉，因此
P
(
w
i
∣
w
i
−
1
)
≈
#
(
w
i
−
1
,
w
i
)
#
(
w
i
−
1
)
P(w_i|w_{i-1}) \approx \frac{\#(w_{i-1}, w_i)}{\#(w_{i-1})}
P(wi ∣wi−1 )≈#(wi−1 )#(wi−1 ,wi )
N-Gram Model
马尔科夫假设中只定义和前面一个词有关，称之为二元模型。当和其前面N个词有关的情况，则成为N元模型，这就是文本处理中经常见到的N-Gram Model。实际应用最多的是N=3的三元模型，之所以不用更高阶的原因主要是：
空间复杂度。N元模型的大小是N的指数，即
O
(
∣
V
∣
N
)
O(|V|^N)
O(∣V∣N)，V为一种语言字典的词汇量
时间复杂度。N元模型的速度也是N的指数，即
O
(
∣
V
∣
N
−
1
)
O(|V|^{N-1})
O(∣V∣N−1)
因此，N不能太大，而且N从1-2,2-3的效果提升显著，但是3-4时效果就不明显了。而且N即使更高阶，也无法覆盖所有的语言，因为语言的上下文的相关性跨度可能非常大，比如跨段落，这是马尔科夫假设无法解决的。
分词
统计语言模型是建立在词的基础上的，词是表达语义的最小单位。对于西方拼音语言来说，词之间是有分界符，因此分词很简单。但是对于东方语言，词之间没有分界符，因此，进行自然语言处理前，首先要对句子进行分词。
查字典法
把句子从左到右扫描，遇到字典里面有的词就标识出来，遇到复合词就找最长匹配，遇到不认识的字串就分割成单字词。比如“上海大学”，“上”是单字词，遇到“海”时，发现可以和前面的“上”组成更长的词，分割点就放在“上海”后面。后面它还能发现“上海大学”其实是个复合词，那么最后把分割点再移到“大学”后面。
统计语言模型分词法
虽然查字典法可以解决70-80%的分词问题，但是中文中有很多二义性的词语，比如“发展中国家”，按照查字典的方法，得到的分词结果是“发展-中国-家”，而正确的分词结果应该是“发展-中-国家”。又比如长匹配带来的问题，“北京大学生”，正确的应该是“北京-大学生”，而不是“北京大学-生”。
最终解决这个问题的方法还是依赖统计语言模型，原理如下。
假设一个句子S可以有以下几种分词方法：
A
1
,
A
2
,
⋯
&ThinSpace;
,
A
x
A_1, A_2, \cdots, A_x
A1 ,A2 ,⋯,Ax
B
1
,
B
2
,
⋯
&ThinSpace;
,
B
y
B_1, B_2, \cdots, B_y
B1 ,B2 ,⋯,By
C
1
,
C
2
,
⋯
&ThinSpace;
,
C
z
C_1, C_2, \cdots, C_z
C1 ,C2 ,⋯,Cz
那么最好的一种分词方法，应该保证分词后的句子出现的概率最大。如果
A
1
,
A
2
,
⋯
&ThinSpace;
,
A
x
A_1, A_2, \cdots, A_x
A1 ,A2 ,⋯,Ax 最好，那么需要满足
P
(
A
1
,
A
2
,
⋯
&ThinSpace;
,
A
x
)
&gt;
P
(
B
1
,
B
2
,
⋯
&ThinSpace;
,
B
y
)
P(A_1, A_2, \cdots, A_x) &gt; P(B_1, B_2, \cdots, B_y)
P(A1 ,A2 ,⋯,Ax )>P(B1 ,B2 ,⋯,By )，且
P
(
A
1
,
A
2
,
⋯
&ThinSpace;
,
A
x
)
&gt;
P
(
C
1
,
C
2
,
⋯
&ThinSpace;
,
C
z
)
P(A_1, A_2, \cdots, A_x) &gt; P(C_1, C_2, \cdots, C_z)
P(A1 ,A2 ,⋯,Ax )>P(C1 ,C2 ,⋯,Cz )
分词粒度，对于不同的应用场景，可以有不同的分词粒度。比如机器翻译中，粒度大效果好。而在网页搜索中，粒度小的效果好。
以统计预言模型为基础的中文分词基本可以看做是一个已经解决了的问题，提升空间微乎其微。分词器好坏的差别在于数据的使用工程实现的精度。
信息度量
信息是一个比较抽象的概念，比如50万字的《史记》信息量是多少？直到香农1948年提出“信息熵”的概念，才解决了信息的度量问题。
信息熵
一条信息的信息量与其不确定性有着直接的关系。比如2018年世界杯冠军是谁，不确定性就大，因此需要了解大量的信息才能推断。又比如，中国队能否进入世界杯，不确定性就很小，基本不需要什么信息量就能确定。前面的信息量大，后面的信息量小。因此，可以认为，信息量就等于不确定性的多少。
香农使用bit来度量信息量。比如32只球队比赛，每个球队夺冠的概率相等，那么谁是冠军的信息量是5bit。它的算法如下：
H
=
−
(
p
1
⋅
log
⁡
p
1
+
p
2
⋅
log
⁡
p
2
+
⋯
+
p
32
⋅
log
⁡
p
32
)
H = -(p_1 \cdot \log p_1 + p_2 \cdot \log p_2 + \cdots + p_{32} \cdot \log p_{32})
H=−(p1 ⋅logp1 +p2 ⋅logp2 +⋯+p32 ⋅logp32 )
其中，
p
1
,
⋯
&ThinSpace;
,
p
32
p_1, \cdots, p_{32}
p1 ,⋯,p32 分别是这32支球队夺冠的概率，H为信息熵（Entropy），单位是bit。当32支球队的夺冠概率相等时，H为5bit。
对于任意一个随机变量X（比如得冠的球队），它的信息熵定义如下：
H
(
x
)
=
−
∑
x
∈
X
P
(
x
)
log
⁡
P
(
x
)
H(x) = -\sum_{x \in X}P(x) \log P(x)
H(x)=−x∈X∑ P(x)logP(x)
变量的不确定性越大，熵就越大。比如P(x)越小，熵就越大。
案例：一本50万字的中文书平均信息量为多少？
中文常用汉字7000左右，假如每个汉字概率相等，那么大约每个汉字的信息熵需要13bit。但是汉字的使用频率是不等的，基本10%左右的常用字占据整个文本的95%，那么每个汉字的信息量10bit就够了。如果再考虑上下文，每个汉字的信息熵5-6bit就够了。所以一本50万字的书的信息量大约是250万-300万bit。
但这只是一个平均数，同样长度的书所含的信息量是不同的。如果一本书重复的内容很多，它的信息量就会很少，冗余度就很大。而且不同语言的冗余度差别也很大，汉语的冗余度是比较小的，一般认为汉语是最简洁的语言。
信息的作用
信息是消除系统不确定性的唯一方法。假如一个系统的不确定性为
U
U
U，从外部消除这个不确定性的唯一方法是引入信息
I
I
I，如果
I
&gt;
U
I &gt; U
I>U，那么就消除了不确定性，如果
I
&lt;
U
I &lt; U
I<U，只是部分消除了，但仍遗留了新的不确定性：
U
′
=
U
−
I
U^{\prime} = U - I
U′=U−I。
自然语言处理的过程就是一个消除不确定性的过程。比如在一元模型就是通过单个词的概率分布消除不确定性因素，二元模型使用了上下文信息，就能消除更多的不确定性，提高准确率。通过上下文信息可以消除不确定性可以用数学的方法证明。这里使用了条件熵。
假定X和Y是两个随机变量，如果知道了X的随机分布
P
(
x
)
P(x)
P(x)，那么也就知道了X的熵：
H
(
x
)
=
−
∑
x
∈
X
P
(
x
)
log
⁡
P
(
x
)
H(x) = -\sum_{x \in X}P(x) \log P(x)
H(x)=−∑x∈X P(x)logP(x)
假定还知道Y的一些情况，包括Y和X一起出现的概率（联合概率）以及Y在取不同值的前提下X的概率分布（条件概率）。则在Y的条件下的条件熵为：
H
(
X
∣
Y
)
=
−
∑
x
∈
X
,
y
∈
Y
P
(
x
,
y
)
log
⁡
P
(
x
,
y
)
H(X|Y) = - \sum_{x \in X, y \in Y}P(x, y) \log P(x, y)
H(X∣Y)=−∑x∈X,y∈Y P(x,y)logP(x,y)
数学上可以证明
H
(
x
)
≥
H
(
X
∣
Y
)
H(x) \ge H(X|Y)
H(x)≥H(X∣Y)，也就是在知道了Y的信息后，关于X的不确定性降低了。那么由此可以得出二元模型的不确定性小于一元模型。同理，三元模型的不确定性小于二元模型。
总之，信息的作用就是消除不确定性。
互信息
上节讲到的有上下文关系的随机变量能够帮忙消除不确定性，但是这个有关系是个模糊的说法，能不能把这种关系也量化呢？香农提出的互信息就是对两个随机变量的相关性做的度量量化。
假定有两个随机事件X和Y，它们的互信息定义：
I
(
X
;
Y
)
=
∑
x
∈
X
,
y
∈
y
P
(
x
,
y
)
log
⁡
P
(
x
,
y
)
P
(
x
)
P
(
y
)
I(X;Y) = \sum_{x \in X, y \in y}P(x, y) \log{\frac { P(x, y)}{P(x)P(y)}}
I(X;Y)=x∈X,y∈y∑ P(x,y)logP(x)P(y)P(x,y)
实际上，
I
(
X
;
Y
)
=
H
(
X
)
−
H
(
X
∣
Y
)
I(X;Y) = H(X) - H(X|Y)
I(X;Y)=H(X)−H(X∣Y)，就是上节里面提到的X的熵与条件熵的差。所谓两个事件相关性的量化度量，就是在了解其中一个Y的前提下，对消除另一个X不确定性所提供的信息量。互信息的范围是0到min(H(X), H(Y))，当X和Y完全相关时，取值为1，完全无关时，取值为0
相对熵
也称为交叉熵（Kullback-Leibler Divergence），也用来衡量相关性，但和变量的互信息不同，它用来衡量两个取值为正数的函数的相似性，它的定义如下：
K
L
(
f
(
x
)
∣
∣
g
(
x
)
)
=
∑
x
i
n
X
f
(
x
)
⋅
log
⁡
f
(
x
)
g
(
x
)
KL(f(x)||g(x)) = \sum_{x in X}f(x) \cdot \log {\frac{f(x)}{g(x)}}
KL(f(x)∣∣g(x))=xinX∑ f(x)⋅logg(x)f(x)
公式不重要，结论记住就好：
对于两个完全相同的函数，它们的相对熵等于零
相对熵越大，两个函数差异越大，反之越小
对于概率分布或者概率密度函数，如果取值均大于零，相对熵可以度量两个随机分布的差异性
相对熵之前用于信号处理，两个随机信号，相对熵越小，说明两个信号越接近。后来也把它用来衡量两端信息的相似度，比如一篇文章照抄或者改写另一篇，那么这两篇文章中的词频分布的相对熵就非常小。
信息熵的应用 – 决策树
决策树由结点（node）和有向边（directed edge）组成。结点有两种类型：内部结点（internal node）和叶结点（leaf node）。内部结点表示一个特征，叶结点表示一个类。
用决策树分类，从根结点开始，对实例的每一个特征进行测试，根据测试结果，将实例分配到其子结点，每个子结点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直到达到叶结点，最后将实例分配到叶结点。
下图为决策树示意图，圆和方框分别表示内部结点和叶结点：
如下14个训练样本，其中8只猫，6只狗。采用决策树算法，如何高效地分类。
我们的目标是希望每一次的特征测试分出来的子类要么包括所有的猫或者所有的狗，而不是两者都有。每一次的测试都能最大的降低不确定性（这样就能提高决策树的效率），而不确定性的度量就是用信息熵。
H
(
x
)
=
−
∑
x
=
1
N
P
(
x
i
)
log
⁡
P
(
x
i
)
H(x) = -\sum_{x=1}^{N}P(x_i) \log P(x_i)
H(x)=−∑x=1N P(xi )logP(xi )
对于训练样本，除了知道里面有6只狗和8只猫，其他信息一无所知，那么对于猫狗分类这件事的信息熵为：
H
(
x
)
=
−
(
6
14
l
o
g
(
6
14
)
+
8
14
l
o
g
(
8
14
)
)
=
0.98523
H(x) = -(\frac {6}{14}log(\frac{6}{14}) + \frac {8}{14}log(\frac{8}{14})) = 0.98523
H(x)=−(146 log(146 )+148 log(148 ))=0.98523
现在有3个特征：play fetch，is grumpy，favorite food，我们希望选择一个用来测试，能够最大的降低信息熵（不确定性）。比如，选择play fetch，分类后结果如下：
决策树经常使用上图可视化的方式来查看分类逻辑和效果：在根节点中的信息熵是0.985，然后我们使用“Play fetch”这个特征分成2类后，一类中有9个样本，其中7只猫2只狗；另一类5个样本，其中1只猫4只狗。结果不是很理想，每个子类里面都同时包括了猫和狗。那么对于这两个子类，其信息熵为：
H
(
x
)
=
−
(
7
9
l
o
g
(
7
9
)
+
2
9
l
o
g
(
2
9
)
)
=
0.7642
H(x) = -(\frac {7}{9}log(\frac{7}{9}) + \frac {2}{9}log(\frac{2}{9})) = 0.7642
H(x)=−(97 log(97 )+92 log(92 ))=0.7642
H
(
x
)
=
−
(
1
5
l
o
g
(
1
5
)
+
4
5
l
o
g
(
4
5
)
)
=
0.7219
H(x) = -(\frac {1}{5}log(\frac{1}{5}) + \frac {4}{5}log(\frac{4}{5})) = 0.7219
H(x)=−(51 log(51 )+54 log(54 ))=0.7219
如果我们使用is grumy作为特征测试，结果为：
使用cat food：
那么对于这3种结果，哪一种性能最好呢？
实际上，这是一个最优特征选择的问题。这里，我们引入information gain(信息增益)来解决这个问题。
Information Gain：特征A对训练集D的信息增益g(D, A)，定义为集合D的信息熵H(D)与在特征A给定的条件下D的条件熵H(D|A)之差，即：
g
(
D
,
A
)
=
H
(
D
)
−
H
(
D
∣
A
)
g(D, A) = H(D) - H(D|A)
g(D,A)=H(D)−H(D∣A)
信息熵H(D)表示对于数据集D进行分类的不确定性。条件熵H(D | A)表示在特征A给定的条件下对数据集D进行分类的不确定性，它们的差即为信息增益。它表示：由于特征A而使得对数据集D的分类不确定性减少的程度。显然，对于数据集D而言，信息增益依赖特征，不同的特征具有不同的信息增益。信息增益大的特征具有更强的分类能力。
根据信息增益的特征选择方法是：对训练集（或者子集）D，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。
之前已经讲过，信息熵H(X)与条件熵H(X|Y)之差为互信息：
I
(
X
;
Y
)
=
H
(
X
)
−
H
(
X
∣
Y
)
I(X;Y) = H(X) - H(X|Y)
I(X;Y)=H(X)−H(X∣Y)。
实际上，决策树中的信息增益等价于训练数据集中类与特征的互信息。决策树算法其实就是使用互信息来选择最优特征。
下图即为各个特征条件下的信息增益：
通过计算IG，我们发现cat food这个特征IG最大，所以它是最有的特征。
注意：在这里，在计算IG时，
I
G
=
P
a
r
e
n
t
′
s
E
n
t
r
o
p
y
−
W
e
i
g
h
t
e
d
A
v
e
r
a
g
e
IG = Parent&#x27;s Entropy - Weighted Average
IG=Parent′sEntropy−WeightedAverage
以上只是选择除了第一级的最优特征，后面第二级的最右特征选择，方法一样，需要在剩余的特征中，递归找到IG最大的那个特征
但是这里IG最大值有两个，在ID3算法中，它是随机选择一种的。后面第三级，第四级等等都是采用这种决策方法。最终我们会得到如下一张决策树的图：
决策树算法C4.5是基于ID3的变种，它可以修建分支，也是最流行的决策树方法。
决策树使用了信息论中的信息熵，互信息，在接下里的文本处理中还将看到对于交叉熵的使用。
代码实例：internet ads
Feature Extraction and Preprocessing
One-hot encoding
计算机是读不懂人类的文字的，它本质上只能做快速计算。为了让计算机能够处理文字，就要求我们先把文字变成一组可计算的数字，然后设计一个算法来算出这些文字的关系。
One-Hot编码，又称为一位有效编码，主要是采用N位状态寄存器来对N个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候只有一位有效。该编码用在文字处理上面，举例如下：
from sklearn.feature_extraction import DictVectorizer onehot_encoder = DictVectorizer() instances = [ {'city': 'Shanghai'}, {'city': 'Beijing'}, {'city': 'Shenzhen'} ] print(onehot_encoder.fit_transform(instances).toarray()) [[0. 1. 0.] [1. 0. 0.] [0. 0. 1.]]
从上面输出的feature vector可以看出：
通过DictVectorizer，将字符串转换成了一个字典向量
3个城市的名字作为了3个元素，但是城市名字的顺序不是按照定义排序的，而是按照字母顺序排序的。分别为：‘Beijing’ ‘Shanghai’ ‘Shenzhen’
表示‘Shanghai‘时，其’对应位置的元素为1，[0, 1, 0]
通过这个简单的例子说明，文本在用来机器学习前，一定要先将其向量化，将人类可读的文本，转换为机器可算的数字。
这里只是处理几个简单的变量，那么计算机如何处理大量的文本呢？这就需要通过下面的模型来实现。
Bag Of Words Model
我们已经知道，文本在机器学习前，一定要将其向量化，而且需要尽可能多得保证文本的原意。其中最为通用的一个模型是Bag of words，中文为词袋模型。
该模型可以看做是one-hot编码的扩展，特点如下：
忽略词序
忽略语法
创建一个特征向量，里面包含了文本中的每个单词
词袋模型的动机是为了说明包含相似单词的文本应该有着相似的意思，它可以高效地处理文本分类并能从编码后的向量恢复对应的文本。
Corpus：包含所有文本的集合称为语料库
Vocabulary：由语料库中所有不重复的单词组成，称为词表
Dimension：组成feature vector的元素数量称为维度
from sklearn.feature_extraction.text import CountVectorizer corpus = [ 'UNC played Duke in basketball', 'Duke lost the basketball game' ] vectorizer = CountVectorizer() print(vectorizer.fit_transform(corpus).todense()) print(vectorizer.vocabulary_) [[1 1 0 1 0 1 0 1] [1 1 1 0 1 0 1 0]] {'unc': 7, 'played': 5, 'duke': 1, 'in': 3, 'basketball': 0, 'lost': 4, 'the': 6, 'game': 2}
上例中：
corpus中包含了两个文本
每个文本的feature vector采用的one-hot编码，该feature vector的维度是8
Vocabulary为{‘unc’: 7, ‘played’: 5, ‘duke’: 1, ‘in’: 3, ‘basketball’: 0, ‘lost’: 4, ‘the’: 6, ‘game’: 2}，单词后面的数字表示该单词在feature vector中的位置。由此词表，它可以快速地将feature vector恢复为文本。
使用CountVectorizer，它默认会自动最小化字母，自动去重，自动分词，自动去空格等符号，改过程称为Tokenization，即将string分段为tokens。Tokens大多数为单词，当然也可以分段为短语(可以包括标点符号)，也支持自定义正规表达式分段
Sparse Vectors
Sparse vectors：稀疏矩阵，这个在使用词袋模型时很常见。比如
corpus = [ 'UNC played Duke in basketball', 'Duke lost the basketball game', 'I ate a sandwich' ] [0 1 1 0 1 0 1 0 0 1] [0 1 1 1 0 1 0 0 1 0] [1 0 0 0 0 0 0 1 0 0]
新加了一条文本，在意思上面与前两条没有关系，它的向量与上面两条也无交集。这也体现了词袋模型的特点：能够发现相似意思的文本。但同时这也带来一个问题，向量中出现了大量的零元素。
如果语料库有几千万条文本，它的feature vector维度可能也有几百万，那么语义无关的文本肯定也会出现大量的零元素。这种含有大量零元素的高维矩阵称为稀疏矩阵。它带来两个严重的问题：
占用大量的memory
维数灾难，随着维度地不断增加，这就要求样本数据需要更多有效的feature保持文本的意思，否则会被稀释掉；而且高维度的情况下，距离计算也会困难。
维数灾难是机器学习中常见的问题，也有一些降维的方案，接下来介绍几个文本降维的方法。
代码实例：20NewsGroups_Classification
Stop-word filter
Stop word：停用词。这个过滤原理很简单，比如针对英文和中文，各自维护了一个停用词列表，里面收集了一些不能表示语义的词语，比如，a, the, I, you, do, be, is, will, …。中文也类似，比如，我，你，她，啊，呢 …通过这种方法，可以达到降维的目标。比如：
corpus = [ 'UNC played Duke in basketball', 'Duke lost the basketball game', 'I ate a sandwich' ] vectorizer = CountVectorizer(stop_words='english') data = vectorizer.fit_transform(corpus) print(data.todense()) print(data.shape) print(vectorizer.vocabulary_) {'unc': 7, 'played': 5, 'duke': 2, 'basketball': 1, 'lost': 4, 'game': 3, 'ate': 0, 'sandwich': 6}
你会发现，它去掉了’I’, ‘a’, ‘in’, ‘the’。现在也能支持自定义停用词，用来应对不同的应用场景。
Lemmatization vs Stemming
Stop word是一个很简单的降维策略，但是stop word list里面只有几百个单词。对于一个很大的语料库来讲仍然是杯水车薪。下面针对英文文本，还有2个相似的降维方法：
Lemmatization：词性还原
Stemming：词根化
这是两个相似的用来降维的策略。一篇高维的文档向量里面，可能有很多同一个词的各种时态，但是它们也是作为feature vector里面一个独立的元素。Stemming和Lemmatization的作用就是将这些词简化成为一个向量元素。
from sklearn.feature_extraction.text import CountVectorizer corpus = [ 'He ate the sandwiches', 'Every sandwich was eaten by him' ] vectorizer = CountVectorizer(stop_words='english') data = vectorizer.fit_transform(corpus) print(data.todense()) print(vectorizer.vocabulary_) [[1 0 0 1] [0 1 1 0]] {'ate': 0, 'sandwiches': 3, 'sandwich': 2, 'eaten': 1}
这两条文本有着相似的意思，但是他们的feature vector却没有任何交集。理想情况下，有着相似意思的文本，应该有相似的feature vector。而造成这个问题的原因就是eat和sandwich的不同形态被认为是不同的feature。解决这种问题就可以使用Lemmatization和Stemming。
Lemmatization的过程就是将单词还原为其原型的一个过程，它需要依赖词典资源，比如WordNet，来恢复出一个正确的单词原型
Stemming的目标和Lemmatization一样，它就是直接地去掉所有看起来是词缀样式的东西，而不在意恢复后的原型是不是一个有效的单词，它不依赖词典，只是依赖规则。
对于上面的文本，处理后分别为：
Stemmed: [['He', 'ate', 'the', 'sandwich'], ['Everi', 'sandwich', 'wa', 'eaten', 'by', 'him']] Lemmatized: [['He', 'eat', 'the', 'sandwich'], ['Every', 'sandwich', 'be', 'eat', 'by', 'him']] [1 1] [1 1]
由上述结果可知，通过Stemming和Lemmatization，除了可以降低feature vector的维度，还能够提高句子之间相似度。
TF-IDF
通过词袋模型向量化后的feature vector，忽略了语法，词序，词频。但是从直觉上来分析，如果一篇文档中一些词多次出现，那么它和该文档主题的关联性大于那些只出现过一次的词。为此，我们要在feature vector中，把词频信息考虑进来。通过TF-IDF来扩展词袋模型。
TF-IDF：Term Frequency / Inverse Document Frequency
首先介绍Term Frequency，这就是词频的意思。那么怎么把词频的信息添加到feature vector中呢？方法如下：
from sklearn.feature_extraction.text import CountVectorizer corpus = ['The dog ate a sandwich, the wizard transfigured a sandwich, and I ate a sandwich'] vectorizer = CountVectorizer(stop_words='english') print(vectorizer.fit_transform(corpus).todense()) [[2 1 3 1 1]] {'dog': 1, 'ate': 0, 'sandwich': 2, 'wizard': 4, 'transfigured': 3}
CountVectorizer就是一个能够记录词频的方法，feature vector中的元素值由之前的0或者1，变成了单词出现的频次。
接下来举一个形象的例子进一步说明：
短语“原子能的应用”可以分为3个词：原子能，的，应用。如果一篇文档中，这些词出现的次数多于其他文档的时候，那么可以说明该篇文档的主题和这些词的相关性高，这些关键词就是该文档主题的重要feature。但是这里也有个漏洞，那就是篇幅长的文档比短的占便宜。所以，根据文档的长度，对关键词的次数进行归一化。即，用关键词的数量除以文档总的词数，这才是真正的词频（Term Frequency）。
比如一篇文档一共有1000个词，其中“原子”，‘的’，“应用”分别出现了2次，35次，5次，那么它们的词频分别是0.002，0.035，0.005。这里面“的”的词频数最高，但是看起来它对于主题没有什么价值，所以可以直接将其去掉，其实“的”即为中文的停止词。至此，这些关键词对于主题的贡献，“原子”为0.002，“应用”为0.005。
这里还有一个小问题。在中文中，“应用”是个很普通的词，而“原子能”是个很专业的词，它和主题的相关性比前者重要。因此，我们有必要给这些关键词一个权重，来体现出它的重要性。这个权重的设定必须满足下面2个条件：
一个词预测主题的能力越强，权重越大，反之，权重越小
停止词的权重为零
那么如何判断哪些次预测主题的能力强呢？
如果一个关键词在语料库中少量的文档中出现，说明该词和这些文档关系密切，它的权重也就应该大。反之，如果一个词在大量的文档中出现，比如“应用”，它的权重就应该小。
接下来的问题就是：如何量化权重？
这里就需要IDF。假定一个关键词w在
D
w
D_w
Dw 个文档中出现，那么
D
w
D_w
Dw 越大，w的权重就越小。IDF的公式为：
log
⁡
(
D
D
w
)
\log (\frac {D}{D_w})
log(Dw D )，其中D为全部文档数。比如，语料库总文档数D = 10亿，停止词“的”在所有文档中出现，它的
D
w
=
10
D_w = 10
Dw =10亿，那么它的IDF = log (10亿 / 10 亿) = log(1) = 0。假如“原子能”在200万个文档中出现，
D
w
=
200
D_w = 200
Dw =200万，则它的权重IDF = log(500) = 8.96。“应用”在5亿个网页中出现，IDF = log(2) = 1。
利用IDF，关键词和文档主题的相关性公式变为：
T
F
1
⋅
I
D
F
1
+
T
F
2
⋅
I
D
F
2
+
⋯
+
T
F
n
⋅
I
D
F
n
TF_1 \cdot IDF_1 + TF_2 \cdot IDF_2 + \cdots + TF_n \cdot IDF_n
TF1 ⋅IDF1 +TF2 ⋅IDF2 +⋯+TFn ⋅IDFn
上例中，该文档和“原子能的应用”的相关性为0.002 x 8.96 + 0.035 x 0 + 0.005 x 1 = 0.02292。
TF-IDF是对搜索关键词的重要性的度量，并且具备很强的理论依据。因此，即使是对搜索不是很精通的人，直接采用TF-IDF，效果也会太差。
TF-IDF的信息论依据
衡量一个词的权重，一个简单的办法就是用这个词的信息量作为它的权重，即：
I
(
w
)
=
−
P
(
w
)
l
o
g
P
(
w
)
=
−
T
F
(
w
)
N
l
o
g
(
T
F
(
w
)
N
)
=
T
F
(
w
)
N
l
o
g
(
N
T
F
(
w
)
)
I(w) = -P(w)logP(w) = - \frac {TF(w)}{N}log( \frac {TF(w)}{N}) = \frac {TF(w)}{N}log(\frac{N}{TF(w)})
I(w)=−P(w)logP(w)=−NTF(w) log(NTF(w) )=NTF(w) log(TF(w)N ) ，
其中，N是整个语料库的大小，是个可以省略的常数，公式可以简化为：
I
(
w
)
=
T
F
(
w
)
l
o
g
(
N
T
F
(
w
)
)
I(w) = TF(w)log(\frac{N}{TF(w)})
I(w)=TF(w)log(TF(w)N )
这里存在一个缺陷：两个词出现的TF相同，比如一个是一篇文章的常见词，另一个是分散在多篇文章中，那么显然第一个词应该贡献更大，应该有更大的权重。为此，我们做一些理想的假设：
每个文档的大小基本相同，均为M个词，即
M
=
N
D
=
∑
w
T
F
(
w
)
D
M = \frac {N}{D} = \frac {\sum_wTF(w)}{D}
M=DN =D∑w TF(w)
一个关键词在文档中一旦出现，不论次数多少，贡献都相同。这样一个词要么在一个文献中出现
c
(
w
)
=
T
F
(
w
)
D
(
w
)
c(w)=\frac{TF(w)}{D(w)}
c(w)=D(w)TF(w) 次，要么就是零。注意，
c
(
w
)
&lt;
M
c(w) &lt; M
c(w)<M
把这两个条件带入到上面的信息量公式后得出：
T
F
(
w
)
l
o
g
N
T
F
(
w
)
=
T
F
(
w
)
l
o
g
M
D
c
(
w
)
D
(
w
)
=
T
F
(
w
)
l
o
g
(
D
D
(
w
)
M
c
(
w
)
)
=
T
F
(
w
)
l
o
g
(
D
D
(
w
)
)
+
T
F
(
w
)
l
o
g
(
M
c
(
w
)
)
TF(w)log\frac{N}{TF(w)}=TF(w)log\frac{MD}{c(w)D(w)}=TF(w)log(\frac{D}{D(w)}\frac{M}{c(w)}) = TF(w)log(\frac{D}{D(w)}) + TF(w)log(\frac{M}{c(w)})
TF(w)logTF(w)N =TF(w)logc(w)D(w)MD =TF(w)log(D(w)D c(w)M )=TF(w)log(D(w)D )+TF(w)log(c(w)M )
其中
T
F
−
I
D
F
(
w
)
=
T
F
(
w
)
l
o
g
(
D
D
(
w
)
)
TF-IDF(w) = TF(w)log(\frac{D}{D(w)})
TF−IDF(w)=TF(w)log(D(w)D )，最终：
T
F
−
I
D
F
(
w
)
=
I
(
w
)
−
T
F
(
w
)
l
o
g
(
M
c
(
w
)
)
TF-IDF(w) = I(w) - TF(w)log(\frac{M}{c(w)})
TF−IDF(w)=I(w)−TF(w)log(c(w)M )
可以看出，当一个词的信息量
I
(
w
)
越
多
，
T
F
−
I
D
F
值
越
大
，
第
二
项
值
越
小
，
T
F
−
I
D
F
也
越
大
I(w)越多，TF-IDF值越大，第二项值越小，TF-IDF也越大
I(w)越多，TF−IDF值越大，第二项值越小，TF−IDF也越大
代码实现：
from sklearn.feature_extraction.text import TfidfVectorizer corpus = [ 'The dog ate a sandwich and I ate a sandwich', 'The wizard transfigured a sandwich' ] vectorizer = TfidfVectorizer(stop_words='english') print(vectorizer.fit_transform(corpus).todense()) [[0.75458397 0.37729199 0.53689271 0. 0. ] [0. 0. 0.44943642 0.6316672 0.6316672 ]] {'dog': 1, 'ate': 0, 'sandwich': 2, 'wizard': 4, 'transfigured': 3}
文本分类样例
垃圾短消息分类
新闻类别分类
Summary
理论原理：
自然语言处理的数学本质
统计语言模型
中文分词
信息论：信息熵，互信息，交叉熵
文本feature抽取和预处理
词袋模型
降维方法：停止词，词性还原/词根化
TF-IDF模型