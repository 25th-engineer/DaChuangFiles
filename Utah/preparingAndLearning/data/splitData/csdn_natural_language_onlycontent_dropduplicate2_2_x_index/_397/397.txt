自然语言处理（五）
传统机器学习
1. 朴素贝叶斯的原理
1.1 朴素贝叶斯相关的统计学知识
1.2基本定义
2. 利用朴素贝叶斯模型进行文本分类
2.1模型原理与训练
3. SVM的原理
3.1快速理解SVM原理
4. 利用SVM模型进行文本分类
5. pLSA、共轭先验分布；LDA主题模型原理
6. 使用LDA生成主题特征，在之前特征的基础上加入主题特征进行文本分类
传统机器学习
1. 朴素贝叶斯的原理
1.1 朴素贝叶斯相关的统计学知识
贝叶斯学派很古老，但是从诞生到一百年前一直不是主流。主流是频率学派。频率学派的权威皮尔逊和费歇尔都对贝叶斯学派不屑一顾，但是贝叶斯学派硬是凭借在现代特定领域的出色应用表现为自己赢得了半壁江山。
贝叶斯学派的思想可以概括为先验概率+数据=后验概率。也就是说我们在实际问题中需要得到的后验概率，可以通过先验概率和数据一起综合得到。数据大家好理解，被频率学派攻击的是先验概率，一般来说先验概率就是我们对于数据所在领域的历史经验，但是这个经验常常难以量化或者模型化，于是贝叶斯学派大胆的假设先验分布的模型，比如正态分布，beta分布等。这个假设一般没有特定的依据，因此一直被频率学派认为很荒谬。虽然难以从严密的数学逻辑里推出贝叶斯学派的逻辑，但是在很多实际应用中，贝叶斯理论很好用，比如垃圾邮件分类，文本分类。
我们先看看条件独立公式，如果X和Y相互独立，则有：
P(X,Y)=P(X)P(Y)
P(X,Y)=P(X)P(Y)
我们接着看看条件概率公式：
P(Y|X)=P(X,Y)/P(X)
P(Y|X)=P(X,Y)/P(X)
P(X|Y)=P(X,Y)/P(Y)
P(X|Y)=P(X,Y)/P(Y)
或者说:
P(Y|X)=P(X|Y)P(Y)/P(X)
P(Y|X)=P(X|Y)P(Y)/P(X)
接着看看全概率公式
P(X)=∑kP(X|Y=Yk)P(Yk)其中∑kP(Yk)=1
P(X)=∑kP(X|Y=Yk)P(Yk)其中∑kP(Yk)=1
从上面的公式很容易得出贝叶斯公式：
P(Yk|X)=P(X|Yk)P(Yk)∑kP(X|Y=Yk)P(Yk)
基于朴素贝叶斯公式，比较出后验概率的最大值来进行分类，后验概率的计算是由先验概率与类条件概率的乘积得出，先验概率和类条件概率要通过训练数据集得出，即为朴素贝叶斯分类模型，将其保存为中间结果，测试文档进行分类时调用这个中间结果得出后验概率。
1.2基本定义
朴素贝叶斯分类是一种十分简单的分类算法，叫它朴素贝叶斯分类是因为这种方法的思想真的很朴素，朴素贝叶斯的思想基础是这样的：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。
朴素贝叶斯分类的正式定义如下：
1、设     为一个待分类项，而每个a为x的一个特征属性。
2、有类别集合。
3、计算。
4、如果 ，则。
那么现在的关键就是如何计算第3步中的各个条件概率。我们可以这么做：
1、找到一个已知分类的待分类项集合，这个集合叫做训练样本集。
2、统计得到在各类别下各个特征属性的条件概率估计。即
。
3、如果各个特征属性是条件独立的，则根据贝叶斯定理有如下推导：
因为分母对于所有类别为常数，因为我们只要将分子最大化皆可。又因为各特征属性是条件独立的，所以有：
2. 利用朴素贝叶斯模型进行文本分类
2.1模型原理与训练
朴素贝叶斯分类器是一种有监督学习，常见有两种模型，多项式模型(multinomial model)即为词频型和伯努利模型(Bernoulli model)即文档型，还有一种高斯模型。
前二者的计算粒度不一样，多项式模型以单词为粒度，伯努利模型以文件为粒度，因此二者的先验概率和类条件概率的计算方法都不同。计算后验概率时，对于一个文档d，多项式模型中，只有在d中出现过的单词，才会参与后验概率计算，伯努利模型中，没有在d中出现，但是在全局单词表中出现的单词，也会参与计算，不过是作为“反方”参与的。
这里暂不考虑特征抽取、为避免消除测试文档时类条件概率中有为0现象而做的取对数等问题。
3. SVM的原理
3.1快速理解SVM原理
很多讲解SVM的书籍都是从原理开始讲解，如果没有相关知识的铺垫，理解起来还是比较吃力的，以下的一个例子可以让我们对SVM快速建立一个认知。
给定训练样本，支持向量机建立一个超平面作为决策曲面，使得正例和反例的隔离边界最大化。
决策曲面的初步理解可以参考如下过程，
1）如下图想象红色和蓝色的球为球台上的桌球，我们首先目的是找到一条曲线将蓝色和红色的球分开，于是我们得到一条黑色的曲线。
2） 为了使黑色的曲线离任意的蓝球和红球距离（也就是我们后面要提到的margin）最大化，我们需要找到一条最优的曲线。如下图，
3） 想象一下如果这些球不是在球桌上，而是被抛向了空中，我们仍然需要将红色球和蓝色球分开，这时就需要一个曲面，而且我们需要这个曲面仍然满足跟所有任意红球和蓝球的间距的最大化。需要找到的这个曲面，就是我们后面详细了解的最优超平面。
4) 离这个曲面最近的红色球和蓝色球就是Support Vector。