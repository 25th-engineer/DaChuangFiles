文章目录
1. THUCNews中文数据集
1.1 数据下载
1.2 数据探索
2. IMDB英文数据集
2.1 数据下载
2.2 数据探索
3. 常用评估方式
1. THUCNews中文数据集
THUCNews是根据新浪新闻RSS订阅频道2005~2011年间的历史数据筛选过滤生成，包含74万篇新闻文档（2.19 GB），均为UTF-8纯文本格式。在原始新浪新闻分类体系的基础上，重新整合划分出14个候选分类类别：财经、彩票、房产、股票、家居、教育、科技、社会、时尚、时政、体育、星座、游戏、娱乐。
1.1 数据下载
官网链接 http://thuctc.thunlp.org/#获取链接， 提供个人信息后可下载。
1.2 数据探索
数据集中包含四个文本文件：cnews.test.txt，cnews.train.txt，cnews.val.txt，cnews.vocab.txt。
cnews.train.txt为训练数据集，cnews.test.txt为测试数据集，cnews.val.txt为验证数据集，cnews.vocab.txt是所有数据集中出现的汉字、字母与标点符号汇集成的词典，其中是词汇表中添加的辅助Token，用来补齐句子长度。
简单建立一个数据字典观察一下，可以看到包含的中文汉字还是挺多的，基本上常用字都包含了，附部分截图：
2. IMDB英文数据集
数据集包含电影评论及其关联的二进制标签，旨在作为情感分类的基准。核心数据集包含50,000个评论，均匀分为25k训练集和25k测试集。
标签的整体分布是平衡的（25k pos和25k neg），还包括另外50,000个未标记文档，用于无监督学习。
2.1 数据下载
http://ai.stanford.edu/~amaas/data/sentiment/ 进入后直接点击Large Movie Review Dataset v1.0开始下载即可。
2.2 数据探索
下载后会得到一个aclImdb_v1.tar.gz压缩包，解压之后可以看到，文件夹中包含train训练数据集的文件夹和test测试数据集文件夹。
在训练数据集中主要包括两个已标记情感类别的影评数据集pos和neg和一个未标记的用于无监督学习的数据集unsup，还有一个imdb的词汇表字典，包含了训练集中出现的所有单词。
测试集中主要包括两个已标记情感类别的影评数据集pos和neg。
同样简单建立一个数据字典观察一下，这个…英文单词果然是庞然大物，太多了，密集恐惧…附部分截图：
3. 常用评估方式
首先要提出混淆矩阵：
混淆矩阵
Positive
Negative
Positive
TP
FP
Negative
FN
TN
True Positive(真正, TP)：将正类预测为正类数
True Negative(真负 , TN)：将负类预测为负类数
False Positive(假正, FP)：将负类预测为正类数 → 误报 (Type I error)
False Negative(假负 , FN)：将正类预测为负类数 → 漏报 (Type II error)
准确率(accuracy) 预测准确的在所有样本中的比例， accuracy=
(
T
P
+
T
N
)
T
P
+
F
N
+
F
P
+
T
N
\frac{(TP+TN)}{TP+FN+FP+TN}
TP+FN+FP+TN(TP+TN)
精确率（precision）：precision=
T
P
T
P
+
F
P
\frac{TP}{TP+FP}
TP+FPTP
对于给定的测试数据集，分类器正确分类的样本数与正样本数之比。（简单点：给出的结果有多少是正确的）；精确率是针对预测结果而言的，它表示的是预测为正的样本中有多少是对的。那么预测为正就有两种可能了，一种就是把正类预测为正类(TP)，另一种就是把负类预测为正类(FP)。
召回率（recall）： recall =
T
P
T
P
+
F
N
\frac{TP}{TP+FN}
TP+FNTP （正确的结果有多少被给出了）
召回率是针对我们原来的样本而言的，它表示的是样本中的正例有多少被预测正确了。那也有两种可能，一种是把原来的正类预测成正类(TP)，另一种就是把原来的正类预测为负类(FN)。
ROC 关注两个指标:一个是TPR（也就是召回率），另一个是将负例错分为正例的概率（FPR=
F
P
T
P
+
T
R
\frac{FP}{TP+TR}
TP+TRFP ）。直观上，TPR 代表能将正例分对的概率，FPR 代表将负例错分为正例的概率。在 ROC 空间中，每个点的横坐标是 FPR，纵坐标是 TPR，
AUC（Area Under Curve）被定义为ROC曲线下的面积。可以综合衡量一个预测模型的好坏，这一个指标综合了precision和recall两个指标。简单说：AUC值越大的分类器，正确率越高。
AUC=1，完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。
0.5<AUC<1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。
AUC=0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。
AUC<0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测，因此不存在 AUC<0.5 的情况。