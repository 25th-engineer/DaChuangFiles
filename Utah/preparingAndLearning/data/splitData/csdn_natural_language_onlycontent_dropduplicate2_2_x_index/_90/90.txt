一、前言
1、前人研究
图灵的图灵机
关于算法计算模型的研究；图灵机是一种抽象的数学模型；
香农的信息论
噪声声道，解码；把熵作为测量信道的信息能力或者语言的信息量的一种方法，用概率测定；噪声信道与解码模型；
信息：文字和语言/数字和信息；信息冗余是信息安全的保障/语料对翻译至关重要。
信息的度量：信息熵是对一个信息系统不确定性的度量；熵；冗余度；条件熵；互信息；相对熵；相对熵，利用它可以得到词频率-逆向文档频率TF-IDF；香农第一定理：对于一个信息，任何编码的长度都不小于它的信息熵；信息的作用就是消除不确定性，自然语言处理的大量问题就是找相关的信息。
2、发展历史
<90年代：规则系统：专家系统和知识工程；
1990-2014:概率系统：规则从数据中抽取/规则是有概率的；流程设计-手机训练数据-预处理-抽取特征-分类器-预测-评估；特征和流程都是专家设计的，存在大量独立的子任务
2014之后：深度学习。
3、形式模型
1、基于短语结构语法的形式模型、基于合一运算的形式模型、基于依存和配价的形式模型、基于格语法的形式模型、基于词汇主义的形式模型；
2、基于概率和统计的形式模型：n-gram，隐马尔科夫模型、最大熵模型，条件随机场，查理亚克的概率上下文无关语法和词汇化的概率上下文无关语法、贝叶斯公式、动态规划算法、噪声信道模型、最小编辑距离算法、决策树模型、加权自动机、维特比算法、向内向外算法、向前向后算法。
3、语义自动处理的形式模型
4、语用自动处理的形式模型
4、基本介绍
自然语言处理：是机器理解人类语言和表达方式并作出回应；
句法分析和语义消歧（依赖上下文，消除歧义性）；
语言信息、世界信息和视觉信息；
主要任务：文本处理、文本生成、文本翻译；
层次分类：语音学，形态学，语法学，语义学，语用学；
研究方法：理性主义，经验主义；
统计方法：隐马尔可夫模型，上下文无关文法，噪声信道模型；
关键问题：歧义消解问题和未知语言现象；
挑战：一词多义、新词、不规范用语；领域隔离，只有封闭环境可用；数据获取难；效果评估难；
过去25年来，自动问答的需求被网页搜索和数据挖掘替代，新的应用越来越依靠数据的作用和浅层的自然语言处理。研究者们也从单纯的句法分析和语义理解，转变到了对机器翻译、语音识别、文本生成、数据挖掘和知识获取等方向。
二、形式语言与自动机
语言：按照一定规律构成的句子或者字符串的有限或者无限的集合。
描述语言的三种途径：穷举法、文法描述和自动机。
基础知识：集合论/图论
1、基本概念
图、树和字符串
2、形式语言
缺陷：对于像汉语英语这样的大型自然语言系统，难以构造精确的文法/不符合人类学习语言的习惯/有些句子语法正确，但在语义上却不可能，形式语言无法排出这些句子；
解决方向：基于大量语料，采用统计学手段建立模型。
形式语法：正则文法，上下文无关文法，上下文相关文法和无约束文法；
3、自动机
有限自动机，下推自动机，线性带限自动机和图灵机
应用：单词自动查错纠正/词性消歧。
三、语料库与词汇知识库
1、语料库
语料库，基于语料的统计方法；
2、词汇知识库
3、本体论
4、知网
定义了各种关系/动态演化认知架构系统：概念对象和动作对象。
概念之间定义了两种关系：扩展和属性。
动作接受一些概念对象，然后产出一些新的概念对象，动作接受的概念对象有两类：一类是必须要有的，没有动作就没法执行；另一类是可选的，可有可无，类似提供了默认参数。
四、统计语言模型
语言模型就是给某句语言打分，给某个话题打分；狗叫模型；球星模型；电影模型；
概率系统：基本分类器；经典序列模型（HMM/CRF/EM、自动机、语言模型）；
概率语言模型：核心就是通过分数告诉机器怎么说话；
概率模型：语言模型、翻译模型、文本对齐、seq2seq模型；
语言模型：文法语言模型、统计语言模型。统计语言模型：n-gram模型；深度学习：神经序列模型LSTM
相似度计算：篇章表示、编辑距离；
computing device：自动机、规则系统、分类器；
搜索技术：关键词匹配、Beam Search、Local Sensitive Hashing、倒排索引；
语言相关技术：stemming、同义词识别替换、中文分词、语法分析、语义意图理解；
seq2seq模型：基本分类器、诗歌生成、情感分析、机器翻译、序列标注 ；
人能否解决，如果能就自己解决，然后考虑机器能否模仿；人如果不能解决，就尝试从计算机的角度思考；
Bash Script：wc/sed/awk/grep/sort/uniq/paste/cat/head/tail(学会linux下的基本命令)；python：处理稍微复杂的问题；
Stanford Core NLP（语义分析）；NLTK（句子划分、读取语义树）；Tensorflow；
有向网络图：首先将问句进行词法分析，得到语义组块序列，然后对其进行意图识别，意图分为两部分：目标概念对象和条件概念对象。Viv的核心技术就是利用DECAS找到从条件概念对象到目标概念对象的联通路径，称之为计划。
1、n-gram模型
统计语言模型：根据前面的所有词测算当前词出现的概率，最后累积相乘，得到整句话出现的概率。/马尔可夫假设当前词出现的概率只与前一个词有关，二元模型，也可以假设由前面n-1个词决定，为n-gram模型；结合语料库计算相对频度，根据大数定理估算最终条件概率。
n-gram模型：利用链式法则计算每句话的概率P（w1.w2.,,,wn）；引入马尔可夫假设：无记忆性：未来的事件，只取决于有限的历史；unigram\bigram\trigram分别对应1，2，3个参考事件；
模型评估
外在评估：能不能抓到老鼠；语音识别的准确性；特点：接近业务场景但比较慢和复杂；
内在评估：颜色速度和力量；预测测试集的能力；特点：与真正的目标有偏差但快和简单；
perplexity（ppx）指标。
新词
新词：未登录词，ovv;
p(wi|wi-1,wi-2)=count(wi-2,wi-1,wi)/count(wi-2,wi-1);
最大似然估计方法:log p(Td);拉格朗日法工具：KenLM(Modified Kneser-Ney Smoothing)
应用案例：完形填空，加入使用3 gram LM
数据平滑
模型的参数：模型中所有的条件概率。
模型的训练：通过对语料的统计，得到这些参数的过程。由于大数定理的局限，需要增加数据量，但不可避免，因此出现了概率估计：古德-图灵估计；
Zipf定律：出现次数少的词总比出现次数多的词要多；
数据平滑，解决零概率，下调出现频率很低的词的概率,卡茨退避法。
n-gram平滑：
本质上是贫富分化的问题；
+1平滑方法：政府给大家每人发一点钱，没用；
Back-off回退法：自己有钱自己出，自己没钱爸爸出，爸爸没钱爷爷出；
interpolate插值法：自己，爸爸，爷爷各出一笔钱，Development Set,EM最大期望值算法解决；
Absolute Discounting"绝对折扣"：有钱的，每个人叫固定的税D，建立一个基金；没钱的，根据自己爸爸有多少钱来分了这个基金；
Kneser-Ney:有钱人缴固定税，按爸爸人脉分配；词的适配度；
Modified KN:有钱人缴阶梯税，按爸爸人脉分配；阶梯税率，最好的方法！
自适应
训练用户特定的语言模型的步骤如下：将训练语言模型的文本按照主题分成很多不同的类别/对于每个类，找到它们的特征向量/统计某个人输入的文本，得到他输入词的特征向量/余弦定理测相似度/选择距离最近的类对应的文本，作为这个特定用户语言模型的训练数据/训练处一个用户特定的语言模型。特定模型在特定领域内效果比通用模型好，但相对偏僻的内容，就比不上通用模型。因此需要采用最大熵模型来综合两个模型的特征，简化来做的话，采用线性插值的模型。
2、其他语言模型（指数）
概率图模型
马尔可夫模型
隐马尔可夫模型
通信模型：信息，上下文-编码-信道-解码-接收；
独立输出假设；
三个基本问题：
1.给定一个模型，如何计算某个特定的输出序列的概率（前向-后向算法）；
2.给定一个模型和某个特定的输出序列，如何找到最可能产生这个输出的状态序列（维特比算法）；
3.给定足够量的观测数据，如何估计隐含马尔可夫模型的参数（模型训练）。
模型训练：计算转移概率和生成概率。
有监督训练：人工标注；
无监督训练：鲍姆-韦尔奇算法；
EM过程，期望值最大化，保证算法迭代到最优。
条件随机场
条件随机场是隐马尔可夫模型的扩展，是一种特殊的概率图模型，变量之间要遵循马尔可夫假设，即每个状态的转移概率只取决于相邻的概率。与贝叶斯网络不同的是，条件随机场是无向图。根据最大熵原则，希望找到一个符合所有边缘分布，同时使得熵最大的模型，就是指数函数。浅层句法分析：看到的东西是词、词性，要推导的东西是语法成分。条件随机场是一个非常灵活的用于预测的统计模型。
对于给定的句子，进行分词、词性标记、命名实体识别和链接、句法分析、语义角色识别和多义词消歧。
前后向算法及参数估计
维特比算法：解码算法，动态规划算法，可以解决最短路径的问题，凡是使用隐马尔可夫模型描述的问题都可以用它来解码，可以概括成以下三点：
1、如果最短路径经过某个点，那么这一条路径上的子路径，一定也是最短路径，否则用另一段最短路径来代替它，便构成更短的路径，这明显是矛盾的。
2、路径上必定经过某个时刻的某个状态，嘉假定在某个时刻有很多状态，那么如果记录了从起始到这个状态所有节点的最短路径，最终最短路径一定是这其中的一条。
3、假定状态变更，最短路径已经找到，并且记录在这些节点上，那么考虑最短路径时，只需要考虑前一个状态的最短路径，以及这个节点的距离即可。维特比算法是和长度成正比的。
五、神经网络语言模型
5种神经网络语言模型：
a) Neural Network Language Model ，NNLM
b) Log-Bilinear Language Model， LBL
c) Recurrent Neural Network based Language Model，RNNLM
d) Collobert 和 Weston 在2008 年提出的 C&W 模型
e) Mikolov 等人提出了 CBOW（ Continuous Bagof-Words）和 Skip-gram 模型
词向量
word2vec的神经网络是浅层的，GloVe实现了一种计数方法，借助两者进行训练的模型通常用作深度学习NLP方法的输入数据。fastText避免了OOV问题，在小数据集上效果更优。主要好处是不用自己积累语料库，只需要爬取网络数据即可。
为什么要用向量表示？因为单词编码是任意的，很难表示出之间的任意关系，还会带来数据系数问题，使用向量进行词的表示可以克服一些障碍。上下文相似的词，其语义也相似；词的语义由其上下文决定。选择一种方式描述上下文-选择一种模型刻画目标词与上下文之间的关系。
GloVe：Global Vector模型，是一种基于矩阵的分布式表示模型
词向量：基于神经网络的分布式表示，word embedding
word2vec模型通常用于预处理阶段的词向量表示。
一种方法是构建共现矩阵，其中包含着语料库中每一个单词同出现在它后一个单词的统计信息。
相同语境中的词语具有相似的语义，基于这一原则，有两类方法：计数的方法（隐性语义分析）和预测方法（神经概率语言模型）。两者的区别在于前者计算出共同出现的概率然后映射到每个词的小而密集的向量。预测模型直接尝试根据学习到的近邻单词的小密集嵌入向量（模型参数）来预测单词。一种用于从原始文本中学习词嵌入的模型，具有很高的计算效率。两种实现方式：连续词袋模型（CBOW），skip-gram模型。两者相似，唯一的区别在于前者从源上下文单词中预测目标单词，而后者根据目标单词预测源上下文单词。
简单的窗口分类器：softmax，将元素变成概率值
最重要的3个环节是分词、锁定关键词、文本相似度计算。
1:词义， 相似度计算，词义向量，微积分，lamanda，为向量的短语赋予意义，情绪分析，邮件建议回复，机器翻译
2:skip-gram模型，给定概率，向量表示，最大化概率分布，损失函数，目标函数，成本函数，单词序列，theta模型，中心向量和上下文向量，更改参数，期望向量，加权，梯度下降，SGD
3：编码会话，职业展览，项目建议，哈希表，语料库，损失函数【0，1】，随机抽样，超参数，unigram，hacky，最大化概率，最小化成本，连续词汇模型，PCA主成分分析
语义组合
简单的加权组合；
卷积神经网络；
循环神经网络；
递归神经网络。
六、基本方法
1、词法分析
中文：字/词/短语/句子/段落/文档。
相关概率，定义/相关任务模型/方法；
词性标注及其一致性检查方法。
分词方法/未登录词处理/词性标注/自动校对/命名实体识别；
中文分词：
词是表达语义的最小单位：中国/航天/历史/已经/有/100/年。
查字典/动态规划，利用维特比算法快速地找到最佳分词（分词器）。
分词的一致性/词的颗粒度和层次。
矩阵运算和文本处理中的两个分类问题：
SVD奇异值分解
就是把一个大矩阵，分解成三个小矩阵相乘，第一个矩阵X是对词进行分类的一个结果；最后一个矩阵Y是对文本的分类结果；中间矩阵B表示词的类和问这个的类之间的相关性。使用矩阵的特征值和数值分析中的各种算法就可以进行奇异值分解。奇异值分解不可迭代，适合处理超大规模文本的粗分类。信息指纹及其应用/相似哈希。
SPAM反作弊：
1、从信息源出发，加强通信（编码）自身的抗干扰能力；
2、从传输来看，过滤掉噪音，还原信息。最大熵模型/原理：保留全部的不确定性，将风险降到最小。
贝叶斯网络
马尔可夫链的扩展，很多事物的相互关系显然不能用一条链来表示，他们之间的关系可能是一个有向网络。状态和关系，可信度用概率来描述，可以有附加的权重。它虽然也是依赖前一个的状态，但不受链状结构的限制，可以更准确滴描述事件之间的相关性。
贝叶斯网络在文本分类-主题模型中的应用：
把文本和关键词的关联矩阵扭转90度，进行奇异值分解，或者对每一个词以文本为维度，建立一个向量，再进行向量的聚类，那么得到的是对词的分类而不是文本的分类，分出来的每一类我们成为一个概念。贝叶斯网络是一个加权的有向图，是马尔可夫链的扩展。它克服了马尔可夫链那种机械的线性约束，把任何有关联的事件统一到了它的框架下面。
期望最大化算法
文本自动分类算法，不需要预定义类别，也不需要合并聚类。只要随机的挑出一些类的中心，然后优化这个中心，是他们和真实的聚类中心尽可能一致。
分类的步骤如下：
1、随机挑选一些点，作为起始的中心。
2、计算所有点到这些聚类中心的距离，将这些点归到最近的一类中。
3、重新计算每一类的中心，新的聚类中心和原先的相比会有一个位移。
4、重复上述过程，直到新的中心和旧的中心之间偏移非常非常小，即过程收敛。
1、根据现有的聚类结果，对所有数据进行重新划分。如果把最终的分类结果看作是一个数学的模型，那么这些聚类的中心，以及每一个点和聚类的隶属关系，可以看作是这个模型的参数。
2、根据重新划分的结果，得到新的聚类。最大化目标函数。EM算法：E过程：期望值计算过程；M过程：最大化过程。如果我们优化的目标函数是一个凸函数，那么一定保证能得到全局最优解。
2、句法分析
句法结构分析：完全句法分析、浅层分析；依存关系分析。
分析方法：基于规则；基于统计。
3、词义消歧
七、处理流程
1、处理流程
根据要求，将自然语言处理成query，再加以形式化，建立语言模型，称之为算法和计算模型；对计算模型的研究，是一个强不适定问题，因为难以满足存在性、唯一性和稳定性的要求，所以应当加入约束条件，使在一定范围内编程适定问题。
1、形式化表示为数学形式
2、形式化表示为算法，表现为模型
3、编写程序，在计算机上加以实现
4、评测，不断改进和优化，以满足需求。
数据基本处理流程：获取数据-数据预处理（观察数据-分词-去除停用词）-特征工程-机器学习工程。
2、指标类别
TP:true positives 真正：判断为真的正确率；
TN:True negatives 真负：判断为假的正确率；
FP:false positives 假正：判断为正的误报率；
FN:false negatives 假负：判断为负的漏报率；
accuracy准确率：反映了分类器对整个样本的判定能力，也就是说能将正的判定为正，负的判定为负；
A=(TP+TN)/(TP+FN+FP+TN)。
precision精准度：被分类器判定正例中的正样本的比重；
P=TP/(TP+FP)。
recall召回率：被预测为正例的占总的正例的比重；
R=TP/(TP+FN)。
F-measure:precision和recall调和均值的2倍；
F=(a2+1)P*R/a2(P+R),取参数a=1。当F1较高时，说明实验结果比较理想。 以具体场景为例：假定某个班级有男生80人，女生20人，共计100人。目标是找出所有女生。现在某人挑选出了50人，其中20人是女生，把其余30人错认为是女生。请你来评估一下他的工作。
假定目标女生为正类P，男生为负类N，则TP=20，FP=30，TN=50，FN=0，A=70/100，P=20/50，R=20/20。
八、技术应用
1、应用方向
自然语言生成/文本分类/信息检索/信息抽取/文字校对/问答系统/机器翻译/自动摘要/文字蕴涵/对话系统/文本挖掘。
信息抽取
从给定文本中抽取重要信息，如时间、地点、人物、事件、原因、结果、数字、日期、货币、专有名词等，涉及到实体识别、时间抽取、因果关系抽取等。
文本挖掘
包括文本聚类、分类、信息抽取、摘要、情感分析以及对所挖掘信息、知识的可视化和交互式的表达界面，基于统计机器学习。
机器翻译
输入一种语言，输出另外一种语言。根据输入媒介不同，可以分为文本翻译、语音翻译、手语翻译、图形翻译等。机器翻译最早基于规则，后来基于统计，到近年基于神经网络，发展至今。
信息检索
对大规模的文档进行检索。可简单对文档中的词汇，赋之以不同的权重来建立索引，也可利用123的技术来建立更深层的索引。在查询的时候，对输入的查询表达式比如一个检索词或者句子进行分析，然后在索引里面查找匹配的候选文档，再根据一个排序机制把候选文档排序，最后输出排序得分最高的文档。 4.3.6问答系统 对一个自然语言表达的问题，由问答系统给出一个精准的答案。需要对自然语言查询语句进行某种程度的语义分析，包括实体链接、关系识别，形成逻辑表达式，然后道知识库中查找可能的候选答案并通过一个排序机制找出最佳答案。
对话系统
系统通过一系列的对话，跟用户聊天、回答、完成某一项任务。涉及到用户意图理解、通用聊天引擎、问答引擎、对话管理等技术。为了体现上下文关联，需要具备多伦对话能力。同时为了体现个性化，要开发用户画像以及基于用户画像的个性化回复。
2、相关项目
自动生成天气预报；自动翻译和自动问答；饭馆咨询服务；图像到语音的转换；残疾人增强交际；旅行咨询服务；语音地理导航；语音资料搜索；跨语言信息检索和翻译；作文自动评分；自动阅读家庭教师；个性化市场服务。
输入法的应用：语言模型和自动机；
自动拼写更正：语言模型、自动机和编辑距离；
机器翻译：中文分词、文本对齐、翻译模型、语言模型、Beam Search
Query意图理解：模板匹配、分类器
Evernote推荐系统：篇章表示、相似度计算、Local Sensitive Hashing、文本分类、倒排索引
小黄鸡：关键词匹配、倒排索引
英文写作助手：语法分析、倒排索引、stem（找词根）
重大事件监测：模板匹配、分类器、
医疗诊断书自动生成：规则系统、深度学习
体育报道自动生成：模板填充、同义词替换、文本对齐
法律专利生成：模板匹配、分类器
聊天互动：seq2seq
邮件自动回复：seq2seq模型、语义意图理解
行业：办公自动化、文体娱乐行业、财经、法律、医疗
3、相关会议
ACL、EMNLP、EACL、NAACL
4、其他
自然语言处理与知识图谱的区别
自然语言处理的研究对象是计算机和人类语言的交互，其任务是理解人类语言并将其转换为机器语言。在目前的商业场景中，NLP技术用于分析源自邮件、音频、文件、网页、论坛、社交媒体中的大量数据。知识图谱是通过将应用数学、图形学、信息可视化技术、信息科学等学科的理论与方法与计量学引文分析、共现分析等方法结合，并利用可视化的图谱形象地展示学科的核心结构、发展历史、前沿领域以及整体知识架构达到多学科融合目的的现代理论。它把复杂的知识领域通过数据挖掘、信息处理、知识计量和图形绘制而显示出来，揭示知识领域的动态发展规律，为学科研究提供切实的、有价值的参考。
自然语言处理与机器学习的区别
自然语言处理都需要依赖统计学知识，而且它和机器学习不同，机器学习依靠的更多是严谨的数学知识以及推导，去创造一个又一个机器学习算法，而自然语言处理是把那些机器学习大牛们创造出来的东西当工具使用。所以入门也只是需要涉猎而已，把每个模型原理看看就行了。
数学模型的重要性
1、一个正确的数学模型应当在形式上是简单的；
2、一个正确的模型一开始可能还不如一个精细雕琢过的错误模型来的准确，但是，如果我们认定大方向是对的，就应该坚持下去。
3、大量准确的数据对研发很重要。
4、正确的模型也可能受噪音干扰，而显得不准确；这时不应该用一种凑合的方法来弥补它，而要找到噪音的根源，这也许能通往重大的发现。