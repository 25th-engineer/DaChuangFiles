递归神经网络是当今最常见的人工智能应用程序的核心，但我们很快就发现，它们并不适合用来解决广义时间序列问题。现在已经有几个在使用中的替代解决方案，其中有一个是刚刚出现的——ODE网络，它与我们思考解决方案的方式截然不同。
\n
递归神经网络及其近亲LSTM是人工智能自然语言处理应用程序的核心。与其他形式的人工智能相比，RNN-NLP在现实世界中的应用要多得多，包括使用卷积神经网络识别和处理图像。
\n
从某种意义上说，数据科学家的队伍已经分成了两组，每一组都在追求使用这两种技术开发独立的应用。从应用角度来看，这两种技术基本上不会发生重叠，因为图像处理处理的是静态数据，而RNN-NLP是将语音和文本解释为时间序列数据。
\n
虽然RNN/LSTM仍然是大多数NLP的首选技术，但我们越是试图扩展时间序列应用，遇到的麻烦就越多。即将出现的技术可能不只是RNN的修改版本，而是对其他几种创新人工智能方法的硬分支。
\n
第一个分支：将CNN与RNN组合使用
\n
第一个分支是我们去年提出的将CNN和RNN结合在一个神经网络中（详见《将CNN与RNN组合使用，天才还是错乱？》）。需要解决的问题与时间序列上的图像有关，即视频，而最常见的任务是视频场景标记。事实证明，这种技术对于识别和标记视频中的情感以及根据之前在视频中见过的人来识别某些类型的人也很有用。
\n
第二个分支：时间卷积神经网络TCN
\n
去年，谷歌和Facebook都解决了RNN的第二类问题。因为要分析的数据扩展到DNN中的多个层，所以，在开始计算之前必须等待所有这些层都完成。这也意味着MPP实际上并不可行。虽然这个过程仍然很快，但不足以快到可以让实时语言翻译应用程序避免明显的延迟。
\n
第二个分支导致这两家公司放弃了RNN，转而采用一种他们称之为时间卷积神经网络（TCN）的CNN变体来进行实时翻译。这看起来很像添加了“Attention”功能的CNN。因为它们的结构与CNN类似，所以可以应用MPP，于是延迟就消失了。
\n
第三个分支：不规则时间序列
\n
还有一些其他类型的时间序列问题是RNN无法完美解决的。它们的主要是具有连续值或者希望将具有不同频率、持续时间和起始点的时间序列数据组合在一起系统。
\n
最后这一个分支看起来并没有那么神秘。它描述的是这样的一种情况，在你去看不同的医生时，你会看到自己的医疗记录，你有不同的预约时间间隔，有不同剂量和时间间隔的用药情况，对这些药品等有不同的身体反应，并且你的身体在以某种可测量的方式变老、变强、变好或变坏。
\n
这就是为什么人工智能的绝大多数医疗应用都只与图像识别有关。我们在使用不规则时序AI能力方面确实存在不足，无法很好地基于不规则时间序列数据得出预测结果。
\n
一种解决方案是将并行的医疗记录分为几星期、几天甚至是几小时的离散步骤。理论上，这样可以满足RNN所要求的离散化。但问题是，为了获得最大的收益，你必须使用非常合适的时间桶，这样会增加计算成本和复杂性。还有一个问题，那就是很多时间桶可能不包含任何数据。
\n
因此，预测社区和医疗社区都需要一个人工智能解决方案，其性能要优于目前的RNN。
\n
ODE网络
\n
去年12月在蒙特利尔举行的神经信息处理系统（NIPS）大会上，来自加拿大向量研究所的研究人员提出了人工智能时间序列建模的全新概念，并被评为大会四篇最佳论文之一。
\n
他们的系统的名字叫作“ODE网络”，是Ordinary Differential Equation Net（常微分方程网络）的缩写。但不要被误导了，ODE网络看起来一点也不像DNN，它没有节点、层或互连。这是一种使用带有反向传播的黑盒微分方程解算器的方法，在连续和离散时间序列问题上都优于RNN。换句话说，它更像是一个坚实的计算板，而不是可以被可视化为神经网络的东西。
\n
这种方法带来了思维方式上的几个有趣的变化。例如，在使用RNN时，你可以指定层和其他超参数，然后运行实验，并查看所获得的准确性。
\n
而在使用ODE网络时，在准确性和训练时间之间存在一个权衡。你指定了准确性级别，ODE网络将会找到实现这一目标的最佳方法，但训练时间是变化的。如果训练时间长得让人无法接受，可以指定一个较低的准确性，以便加快训练过程。一个有趣的结果可能是在训练时指定高准确性，但在测试时可以指定较低的准确性。
\n
这篇论文（https://arxiv.org/abs/1806.07366）的内容非常全面，并提供了几个实验的结果，其中的结果明显优于RNN。但它仍处于研究阶段，但与数据科学中的大多数东西一样，这并不需要很长时间就能走向应用。
\n
英文原文：
\n
https://www.datasciencecentral.com/profiles/blogs/the-coming-revolution-in-recurrent-neural-nets-rnns
\n
\n