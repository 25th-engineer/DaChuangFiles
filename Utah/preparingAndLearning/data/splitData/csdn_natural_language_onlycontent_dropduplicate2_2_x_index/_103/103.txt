个人学习nlp笔记：学习材料CS124、COSC572和《Speech and Language Processing》第三版
自然语言处理 学习笔记（四）
1. 信息检索
2. 词汇-文本关联矩阵
3.倒排索引(Inverted Index)
2.1 倒排索引的结构
2.2 用倒排索引的查询处理（Query processing）
2.3 布尔检索模型
4.短语查询
4.1 双词索引（biwords indexes）
4.2 带位置的索引
5.排序检索
5.1 Jaccard系数
5.2 加权词频
5.3 逆文档频率加权（Invrse document frequency weighting/IDF）
6.TF-IDF
7. 向量空间模型
8.TF-IDF的cosine得分
1. 信息检索
从文档中提取需要的信息
info need步骤里，把我们想要的信息翻译为搜索框能够理解的形式
query里翻译为搜索引擎能理解的形式。这个过程中主要会出现两种错误，本课主要关注第二种，即怎样才能正确组织文字来送到搜索引擎。我们选择 how trap mice alive; how trap mice without killing或者是加上引号，都有不同效果。
如何评价是否很好地检索到文件
2. 词汇-文本关联矩阵
比如在所有文档种，我们想检索A and B, but NOT C，我们可以用正则的方法，但是这对大的语料库很慢，而且很多复杂操作不能用或不灵活，而且我们还要能对文档进行排序。
我们可以引入词汇-文本关联矩阵来解决上诉的那个要求，因为从矩阵种我们知道这些词汇是否在某个文档种存在。
用其的二进制形式表示，若为NOT，则取反，即101111就表示Calpurnia是否在这6个文档中出现，因为前面是NOT，所以010000取反。
在大文档中，100w个1000字的文档，而我们的term，有500k个，
得到一个巨大的文档，其中绝大多数都是0，因此需要更好的，比如只记录1的数据结构。
3.倒排索引(Inverted Index)
若只是最普通的数据结构，因为文字出现的频率不同，每个列表包含的内容长度不同。同时因为列表是有序的，所以插入数据时候也会很麻烦。
2.1 倒排索引的结构
第一步：
得到一个token和其的documentID
由词汇进行排序（字母表中顺序）
把同个文档中重复的token只考虑一个，映射到dictionary和postings中，同时在dictionary中记录出现频率（也就是这个词在几个文档中出现了）
2.2 用倒排索引的查询处理（Query processing）
利用倒排索引完成查询操作 AND，抽取Brutus和Caesar的postings，并合并
用指针进行元素间的对比，如Brutus出现第一个在2，而caesar在1，不同，则两者小的指针向前一格。此时brutus在2，caesar也在2，两者都前进一格。
伪代码
2.3 布尔检索模型
一个法律领域的检索模型例子，所以说这个方法过时，但还是在一些地方适用：
在a and b and c的 query中，从短的开始，把全部都遍历了
若存在多个or操作，先估计or的尺寸，先对小的（预测的，也就是直接or两端的频数相加）进行操作
如果是not呢
4.短语查询
我们经常把stanford university或者san francisco当成一个短语，也就是一个不被分割的整体。这样倒排索引就解决不了了。
4.1 双词索引（biwords indexes）
第一步的尝试，就算建立一个两个词的逆序索引：
在大于2的短语中，比如stanford university palo alto，可以分为stanford university AND university palo AND palo alto，这样就等同于前面单个词的逆序索引了。但是也有一个问题，上文这4个词若只找到同时连续出现才有意义（长短语），那文档不同地方分别出现就没意义了，这也就是会导致positive falsely。不过其实问题不大
一种扩展的双词方法：
对编入索引的文本进行词性标注，若词（term）为名词(N) 或冠词/介词(X)，称具有NX*N形式的词项序列为扩展双词(extended biword)，将这样扩展词对作为一个词项放入词典中。
比如索引catcher in the rye (麦田守望者)时，N X X N，符合NX*N，将查询分析成N和X序列，将查询切分成扩展双词，在索引中查找catcher rye
字典太大，存在false positive问题，但是可以作为综合的索引策略第一部分。
4.2 带位置的索引
第二种方法，带位置的索引
先对比是否同时出现在一个文档，然后进行对比，此处to要为be的位置-1。
这个方法同样可以用到模糊搜索上
带位置信息的索引特点，储存要求大，不过很灵活而且可压缩。
大小和文档长度有关，若文档不长，那和普通的posting差不多大小，若很大，比如书等大约100000词，那就是其100倍。
总的来说，比无位置信息大2~4倍，不过仍是原文本的35%~50%，无位置信息的大致是10%
一种将biword和位置信息的索引结合的办法，省时，但费内存。
5.排序检索
普通的布尔检索没能满足用户的需要，尤其是同时返回上千个结果时候，这是我们需要对索引进行排序，给用户最早看到最重要的内容。
5.1 Jaccard系数
例子：
但也存在问题，比如没用单词的出现次数，而且标准化的方式不大对
5.2 加权词频
但是这样的模型没有位置信息，这样 A is better than B和B is better than A的词袋模型其实是一样的：
intuition是词频和相关性是相关的，但不是线性的
评分就
5.3 逆文档频率加权（Invrse document frequency weighting/IDF）
intuition: 检索中，不常见的词应当有更多信息，应当赋予更高的权重。像it, and 这类停用词，基本没有什么信息。
所以使用词在多少文档中出现，来表示其是否常见。
d
f
t
df_{t}
dft 表示词在多少文档中出现，N表示语料库中的文档个数，则
i
d
f
t
∈
[
0
,
l
o
g
10
N
]
idf_{t}\in[0,log_{10}N]
idft ∈[0,log10 N]，若词在每个文档中都出现，那其的逆文档频率/
i
d
f
idf
idf为0。
且若语料库是不变的，那么我们得到的idf也是不变的，是一个对应每个词的值
单词检索时候，idf只是一个点值（scalar），对检索没影响。但是在多个单词的索引时，idf可以给诸如capricous person两个词赋权，少见的前者赋予更高的权重。
try和insurance虽然出现的频率都差不多，但是try出现地很广，而insurance出现的文章较少，因此虽然总频数差不多，根据idf的原理，因赋予insurance更高权重
6.TF-IDF
TF-IDF是信息检索领域中最重要的加权方法之一。第一个系数
1
+
l
o
g
t
f
t
,
d
1+logtf_{t,d}
1+logtft,d 表示的是前文对词出现频率的加权方法log-frequency，其大小与词出现次数成正相关，而后方的
l
o
g
10
(
N
/
d
f
t
)
log_{10}(N/df_{t})
log10 (N/dft )指的是单词在多少文档出现的反比，也就是前文的idf
总得分，所有单词的tf-idf加合
7. 向量空间模型
前文我们把文档当作了一个向量，这是很稀疏的向量，同时占据了很大的内存空间。
在查询处，我们也将问题，转换为向量，并于文档进行相似度对比，排序。相似度，约等于距离的倒数（越近越相似）
但通常使用的欧式距离有很多问题，比如向量的长度对距离大小影响很大，如下图，尽管q和d2看似最相近，但是因此向量长度，导致查询向量q和d1或d3最相似。
取而代之，我们可以使用cosines来计算，其在
[
0
,
π
]
[0,\pi]
[0,π]中为减函数（值从1到-1），能满足我们的需求
L2 正则，使文档文档大小的影响变小。
‘若文档和查询向量都已标准化，直接使用下列式子
emmm，好像也可以用log词频来替代词频
8.TF-IDF的cosine得分
综上所述，我们有很多种方法来做加权，这就像是一种组合。若我们使用log词频，逆文档词频加上L2 norm，则我们用的为ltc（smart notation），log词频适应于长文档
计算实例，注意，标准化时候，若前方使用的是加权词频，则标准化时候使用的就是加权后的词频。