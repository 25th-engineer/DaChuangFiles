自然语言处理是人工智能领域中的一个重要方向。它研究能人机之间通讯的方式，并涉及机器对人类知识体系的学习和应用．从分词，相似度计算，情感分析，文章摘要，到学习文献，知识推理，都涉及自然语言分析．下面介绍一些中文语言语义分析的资源．（以下只讨论能嵌入到我们程序里的资源）
1.      同义词词林
《同义词词林》是80年代出版的一本词典，这提供了词的归类，相关性信息，起始主要用于翻译，哈工大对它进行了细化和扩充，出了《词林扩展版》，其中含有7万多词，17000多种语义，五层编码．12大类，94中类，1428小类，形如：
Aa01A01= 人 士 人物 人士 人氏 人选
每一个条目对应一种语义，根据分类编号：第一位大写表示大类，第二位小写表示中类…其中涉及了一词多义和一义多词．
《词林扩展版》网上的下载很多，大小不到1M，可以直接load到程序中，用于简单的分词，文章分类，模糊查找，统计，情感分析（不同感情色彩对应不同类别号）等等．
2.      哈工大语言云(LTP)
中文的语义分析工具，大多数都像LTP这样，提供一个在线的分析器，一组API，比较简单稳定的功能．LTP是其中做得比较好的．
它提供了中文分词、词性标注、命名实体识别、依存句法分析、语义角色标注等等功能．但对于进一步语义方面的深入的开发，用处不大，而且需要连网使用，速度和处理数量上都有一些限制．
详见：http://www.ltp-cloud.com/demo
3.      结巴分词
结巴是一个Python的中文分词组件．它提供了分词和词性标注功能．能在本地自由使用, 是Python实现的, 可以很好的和其它Python工具相结合，使用方法如下：
#encoding=utf-8 import jieba.posseg as pseg import jieba seg_list = jieba.cut("我爱北京天安门", cut_all=True) print "Full Mode:", "/ ".join(seg_list) words = pseg.cut("我爱北京天安门") for w in words: print w.word,w.flag
执行结果是:
Full Mode: 我/ 爱/ 北京/ 天安/ 天安门 我 r 爱 v 北京 ns 天安门 ns
详见: http://www.oschina.net/p/jieba/
4.      知网 HowNet
对于语言的理解, 人们更关注语义，即研究文字真正的含义是什么，并希望机器能像人脑一样把知识组织成体系．
中文语义库开放的资源非常少，《现代汉语语义词典》，《中文概念辞书》这些都是听说过没见过，总之人家是不开放. 就算能去书店买一本, 也用不到程序里. 我在网上只找到了HowNet (可以在csdn下载, 压缩包1.5M左右). 形如:
NO.=069980 W_C=群众 G_C=N E_C= W_E=the masses G_E=N E_E= DEF=human|人,mass|众
可以看到它包含：编号, 中文词, 对应英文词, 词性, 约12万多项.
HowNet在2013年后就不更新了, 以上版本差不多是能在网上找到的比较全的数据了. 它还提供了一些库, 可用于判断相似度等．
详见：http://www.keenage.com/html/c_index.html
5.      NLTK与WordNet (sentiwordnet)
WordNet是一个语义词典, NLTK是Python的一个自然语言处理工具，它提供了访问WordNet各种功能的函数。WordNet形如:
n 03790512 0 0 motorcycle#1 bike#1 a motor vehicle with two wheels and a strong frame
其中含有词性, 编号, 语义, 词汇间的关系(同义/反义,上行/下行,整体/部分…), 大家都觉得＂它很棒, 只可惜没有中文支持＂. 其实也不是没中文支持. WordNet有中文以及其它更多语言的支持, 可以从以下网址下载:
http://globalwordnet.org/wordnets-in-the-world/
其中的数据文件形如：
03790512-n cmn:lemma 摩托车
可以看到，它与sentiwordnet的词条编号一致，尽管对应可能不是特别完美，但理论上是：对英文能做的处理，对中文也能做．
NLTK+WordNet功能非常丰富，强烈推荐《PYTHON自然语言处理NLTK Natural LanguageProcessing with Python》这本书，它已由爱好者译成中文版，可从网上下载．里面不但讨论了具体的实现方法，还讨论了一些研究方向，比如＂从自然语言到一阶逻辑＂…
6.      随想
对语言的处理，首先是分词，然后是消歧, 判断词在句中的成份, 识别语义．形成知识网络．．．希望最终机器能像人类一样，学习，思考和创造．
语言处理在不同的层次有不同的应用：从文章分类，内容提取，到自动诊断病情（IBM Watson），或者存在更通用的逻辑，使机器成为比搜索引擎更智能的各个行业的专家系统．
自然语言和语义看似多对多的关系，我觉得本质上语义转换成语言是从高维到低的投影．从词林的分类看，真正核心的概念并不太多，但是语义的关系和组合很复杂，再深层次还涉及知识线等等．而语言只是它的表象．在分析过程中，越拟合那表象，差得越多．
另外，这一领域已经有几十年的历史了，学习时尽可能利用现有工具，把精力集中在目标而非具体过程．多参考人家都实现了什么功能，人家的数据是怎么组织的．