大部分内容摘抄自知乎相关问题
作者：微软亚洲研究院
作者：陈见耸
作者：刘知远
背景知识
自然语言处理是一门交叉的学科
概率论：需要了解概率、条件概率、贝叶斯法则；二项分布、期望、方差；最大似然估计、梯度下降等等
统计学：建模、数据稀疏问题、回退方法等
机器学习：分类、感知器、支持向量机
语言学：构词、词类、句法、语义；语料库和知识库等等
建议1：如何在NLP领域快速学会第一个技能？
我的建议是：找到一个开源项目，比如机器翻译或者深度学习的项目。理解开源项目的任务，编译通过该项目发布的示范程序，得到与项目示范程序一致的结果。然后再深入理解开源项目示范程序的算法。自己编程实现一下这个示范程序的算法。再按照项目提供的标准测试集测试自己实现的程序。如果输出的结果与项目中出现的结果不一致，就要仔细查验自己的程序，反复修改，直到结果与示范程序基本一致。如果还是不行，就大胆给项目的作者写信请教。在此基础上，再看看自己能否进一步完善算法或者实现，取得比示范程序更好的结果。
项目不要太大，以小型的算法模块为佳，这样便于独立实现。像文本领域的文本分类、分词等项目就是比较合适的项目。 运行程序得到项目所声称的结果。然后看懂程序，这期间一般需要阅读程序实现所参考的文献。最后，自己尝试独立实现该算法，得到与示例程序相同的结果。再进一步的，可以调试参数，了解各参数对效果的影响，看是否能得到性能更好的参数组合。
这一阶段主要是学习快速上手一个项目，从而对自然语言处理的项目有比较感性的认识——大体了解自然语言处理算法的原理、实现流程等。
当我们对自然语言处理项目有了一定的认识之后，接下来就要深入进去。任何自然语言处理应用都包含算法和所要解决的问题两方面，要想深入进去就需要从这两方面进行着手。
建议2：如何选择第一个好题目？
工程型研究生，选题很多都是老师给定的。需要采取比较实用的方法，扎扎实实地动手实现。可能不需要多少理论创新，但是需要较强的实现能力和综合创新能力。而学术型研究生需要取得一流的研究成果，因此选题需要有一定的创新。我这里给出如下的几点建议。
先找到自己喜欢的研究领域。你找到一本最近的ACL会议论文集, 从中找到一个你比较喜欢的领域。在选题的时候，多注意选择蓝海的领域。这是因为蓝海的领域，相对比较新，容易出成果。
充分调研这个领域目前的发展状况。包括如下几个方面的调研：方法方面，是否有一套比较清晰的数学体系和机器学习体系；数据方面，有没有一个大家公认的标准训练集和测试集；研究团队，是否有著名团队和人士参加。如果以上几个方面的调研结论不是太清晰，作为初学者可能不要轻易进入。
在确认进入一个领域之后，按照建议一所述，需要找到本领域的开源项目或者工具，仔细研究一遍现有的主要流派和方法，先入门。
反复阅读本领域最新发表的文章，多阅读本领域牛人发表的文章。在深入了解已有工作的基础上，探讨还有没有一些地方可以推翻、改进、综合、迁移。注意做实验的时候，不要贪多，每次实验只需要验证一个想法。每次实验之后，必须要进行分析存在的错误，找出原因。
对成功的实验，进一步探讨如何改进算法。注意实验数据必须是业界公认的数据。
与已有的算法进行比较，体会能够得出比较一般性的结论。如果有，则去写一篇文章，否则，应该换一个新的选题。
建议3：如何写出第一篇论文？
接上一个问题，如果想法不错，且被实验所证明，就可开始写第一篇论文了。
确定论文的题目。在定题目的时候，一般不要“…系统”、“…研究与实践”，要避免太长的题目，因为不好体现要点。题目要具体，有深度，突出算法。
写论文摘要。要突出本文针对什么重要问题，提出了什么方法，跟已有工作相比，具有什么优势。实验结果表明，达到了什么水准，解决了什么问题。
写引言。首先讲出本项工作的背景，这个问题的定义，它具有什么重要性。然后介绍对这个问题，现有的方法是什么，有什么优点。但是（注意但是）现有的方法仍然有很多缺陷或者挑战。比如（注意比如），有什么问题。本文针对这个问题，受什么方法（谁的工作）之启发，提出了什么新的方法并做了如下几个方面的研究。然后对每个方面分门别类加以叙述，最后说明实验的结论。再说本文有几条贡献，一般写三条足矣。然后说说文章的章节组织，以及本文的重点。有的时候东西太多，篇幅有限，只能介绍最重要的部分，不需要面面俱到。
相关工作。对相关工作做一个梳理，按照流派划分，对主要的最多三个流派做一个简单介绍。介绍其原理，然后说明其局限性。
然后可设立两个章节介绍自己的工作。第一个章节是算法描述。包括问题定义，数学符号，算法描述。文章的主要公式基本都在这里。有时候要给出简明的推导过程。如果借鉴了别人的理论和算法，要给出清晰的引文信息。在此基础上，由于一般是基于机器学习或者深度学习的方法，要介绍你的模型训练方法和解码方法。第二章就是实验环节。一般要给出实验的目的，要检验什么，实验的方法，数据从哪里来，多大规模。最好数据是用公开评测数据，便于别人重复你的工作。然后对每个实验给出所需的技术参数，并报告实验结果。同时为了与已有工作比较，需要引用已有工作的结果，必要的时候需要重现重要的工作并报告结果。用实验数据说话，说明你比人家的方法要好。要对实验结果好好分析你的工作与别人的工作的不同及各自利弊，并说明其原因。对于目前尚不太好的地方，要分析问题之所在，并将其列为未来的工作。
结论。对本文的贡献再一次总结。既要从理论、方法上加以总结和提炼，也要说明在实验上的贡献和结论。所做的结论，要让读者感到信服，同时指出未来的研究方向。
参考文献。给出所有重要相关工作的论文。记住，漏掉了一篇重要的参考文献（或者牛人的工作），基本上就没有被录取的希望了。
写完第一稿，然后就是再改三遍。
把文章交给同一个项目组的人士，请他们从算法新颖度、创新性和实验规模和结论方面，以挑剔的眼光，审核你的文章。自己针对薄弱环节，进一步改进，重点加强算法深度和工作创新性。
然后请不同项目组的人士审阅。如果他们看不明白，说明文章的可读性不够。你需要修改篇章结构、进行文字润色，增加文章可读性。
如投ACL等国际会议，最好再请英文专业或者母语人士提炼文字。
建议4：对问题进行深入认识
对问题的深入认识通常来源于两个方面，一是阅读当前领域的文献，尤其是综述性的文献，理解当前领域所面临的主要问题、已有的解决方案有哪些、有待解决的问题有哪些。这里值得一提的是，博士生论文的相关文献介绍部分通常会对本问题做比较详细的介绍，也是比较好的综述类材料。
除了从文献中获取对问题的认识外，另一种对问题进行深入认识的直观方法就是对算法得出的结果进行bad case分析，总结提炼出一些共性的问题。对bad case进行分析还有一个好处，可以帮助我们了解哪些问题是主要问题，哪些问题是次要问题，从而可以帮助我们建立问题优先级。如果有具体任务的真实数据，一定要在真实数据上进行测试。这是因为，即使是相同的算法，在不同的数据集上，所得到的结果也可能相差很大。
建议5：对算法进行深入理解
除了具体的问题分析，对算法的理解是学习人工智能必须要过的关。经过这么多年的发展，机器学习、模式识别的算法已经多如牛毛。幸运的是，这方面已经有不少好的书籍可供参考。这里推荐华为李航的蓝宝书《统计学习方法》和周志华的西瓜书《机器学习》，这两本都是国内顶级的机器学习专家撰写的书籍，思路清晰，行文流畅，样例丰富。
如果觉得教科书稍感乏味，那我推荐吴军的《数学之美》，这是一本入门级的科普读物，作者以生动有趣的方式，深入浅出的讲解了很多人工智能领域的算法，相信你一定会有兴趣。
国外的书籍《Pattern Recognition and Machine Learning》主要从概率的角度解释机器学习的各种算法，也是不可多得的入门教材。如果要了解最新的深度学习的相关算法，可以阅读被誉为深度学习三架马车之一Bengio所著的《Deep Learning》。 在学习教材时，对于应用工程师来说，重要的是理解算法的原理，从而掌握什么数据情况下适合什么样的数据，以及参数的意义是什么。
建议6：深入到领域前沿
自然语言处理领域一直处在快速的发展变化当中，不管是综述类文章还是书籍，都不能反映当前领域的最新进展。如果要进一步的了解领域前沿，那就需要关注国际顶级会议上的最新论文了。下面是各个领域的一些顶级会议。这里值得一提的是，和其他人工智能领域类似，自然语言处理领域最主要的学术交流方式就会议论文，这和其他领域比如数学、化学、物理等传统领域都不太一样，这些领域通常都以期刊论文作为最主要的交流方式。 但是期刊论文审稿周期太长，好的期刊，通常都要两三年的时间才能发表，这完全满足不了日新月异的人工智能领域的发展需求，因此，大家都会倾向于在审稿周期更短的会议上尽快发表自己的论文。
这里列举了国际和国内文本领域的一些会议，以及官网，大家可以自行查看。
国际上的文本领域会议：
ACL：http://acl2017.org/ 加拿大温哥华 7.30-8.4
EMNLP：http://emnlp2017.net/ 丹麦哥本哈根 9.7-9.11
COLING：没找到2017年的
国内会议：
CCKS http://www.ccks2017.com/index.php/att/ 成都 8月26-8月29
SMP http://www.cips-smp.org/smp2017/ 北京 9.14-9.17
CCL http://www.cips-cl.org:8080/CCL2017/home.html 南京 10.13-10.15
NLPCC http://tcci.ccf.org.cn/conference/2017/ 大连 11.8-11.12
NCMMSC http://www.ncmmsc2017.org/index.html 连云港 11.11 － 11.13
像paperweekly，机器学习研究会，深度学习大讲堂等微信公众号，也经常会探讨一些自然语言处理的最新论文，是不错的中文资料。
建议7：当然，工欲善其事，必先利其器。我们要做好自然语言处理的项目，还需要熟练掌握至少一门工具。当前，深度学习相关的工具已经比较多了，比如：tensorflow、mxnet、caffe、theano、cntk等。这里向大家推荐tensorflow，自从google推出之后，tensorflow几乎成为最流行的深度学习工具。究其原因，除了google的大力宣传之外，tensorflow秉承了google开源项目的一贯风格，社区力量比较活跃，目前github上有相当多数量的以tensorflow为工具的项目，这对于开发者来说是相当大的资源。
以上就是对于没有自然语言处理项目经验的人来说，如何学习自然语言处理的一些经验，希望对大家能有所帮助。
其中文献部分：
1. 国际学术组织、学术会议与学术论文
自然语言处理（natural language processing，NLP）在很大程度上与计算语言学（computational linguistics，CL）重合。与其他计算机学科类似，NLP/CL有一个属于自己的最权威的国际专业学会，叫做The Association for Computational Linguistics（ACL，URL：ACL Home Page），这个协会主办了NLP/CL领域最权威的国际会议，即ACL年会，ACL学会还会在北美和欧洲召开分年会，分别称为NAACL和EACL。除此之外，ACL学会下设多个特殊兴趣小组（special interest groups，SIGs），聚集了NLP/CL不同子领域的学者，性质类似一个大学校园的兴趣社团。其中比较有名的诸如SIGDAT（Linguistic data and corpus-based approaches to NLP）、SIGNLL（Natural Language Learning）等。这些SIGs也会召开一些国际学术会议，其中比较有名的就是SIGDAT组织的EMNLP（Conference on Empirical Methods on Natural Language Processing）和SIGNLL组织的CoNLL（Conference on Natural Language Learning）。此外还有一个International Committee on Computational Linguistics的老牌NLP/CL学术组织，它每两年组织一个称为International Conference on Computational Linguistics (COLING)的国际会议，也是NLP/CL的重要学术会议。NLP/CL的主要学术论文就分布在这些会议上。
作为NLP/CL领域的学者最大的幸福在于，ACL学会网站建立了称作ACL Anthology的页面（URL：ACL Anthology），支持该领域绝大部分国际学术会议论文的免费下载，甚至包含了其他组织主办的学术会议，例如COLING、IJCNLP等，并支持基于Google的全文检索功能，可谓一站在手，NLP论文我有。由于这个论文集合非常庞大，并且可以开放获取，很多学者也基于它开展研究，提供了更丰富的检索支持，具体入口可以参考ACL Anthology页面上方搜索框右侧的不同检索按钮。
与大部分计算机学科类似，由于技术发展迅速，NLP/CL领域更重视发表学术会议论文，原因是发表周期短，并可以通过会议进行交流。当然NLP/CL也有自己的旗舰学术期刊，发表过很多经典学术论文，那就是Computational Linguistics（URL：MIT Press Journals）。该期刊每期只有几篇文章，平均质量高于会议论文，时间允许的话值得及时追踪。此外，ACL学会为了提高学术影响力，也刚刚创办了Transactions of ACL（TACL，URL：Transactions of the Association for Computational Linguistics (ISSN: 2307-387X)），值得关注。值得一提的是这两份期刊也都是开放获取的。此外也有一些与NLP/CL有关的期刊，如ACM Transactions on Speech and Language Processing，ACM Transactions on Asian Language Information Processing，Journal of Quantitative Linguistics等等。
根据Google Scholar Metrics 2013年对NLP/CL学术期刊和会议的评价，ACL、EMNLP、NAACL、COLING、LREC、Computational Linguistics位于前5位，基本反映了本领域学者的关注程度。
NLP/CL作为交叉学科，其相关领域也值得关注。主要包括以下几个方面：（1）信息检索和数据挖掘领域。相关学术会议主要由美国计算机学会（ACM）主办，包括SIGIR、WWW、WSDM等；（2）人工智能领域。相关学术会议主要包括AAAI和IJCAI等，相关学术期刊主要包括Artificial Intelligence和Journal of AI Research；（3）机器学习领域，相关学术会议主要包括ICML，NIPS，AISTATS，UAI等，相关学术期刊主要包括Journal of Machine Learning Research（JMLR）和Machine Learning（ML）等。例如最近兴起的knowledge graph研究论文，就有相当一部分发表在人工智能和信息检索领域的会议和期刊上。实际上国内计算机学会（CCF）制定了“中国计算机学会推荐国际学术会议和期刊目录”（CCF推荐排名），通过这个列表，可以迅速了解每个领域的主要期刊与学术会议。
最后，值得一提的是，美国Hal Daumé III维护了一个natural language processing的博客（natural language processing blog），经常评论最新学术动态，值得关注。我经常看他关于ACL、NAACL等学术会议的参会感想和对论文的点评，很有启发。另外，ACL学会维护了一个Wiki页面（ACL Wiki），包含了大量NLP/CL的相关信息，如著名研究机构、历届会议录用率，等等，都是居家必备之良品，值得深挖。
2. 国内学术组织、学术会议与学术论文
与国际上相似，国内也有一个与NLP/CL相关的学会，叫做中国中文信息学会（URL：中国中文信息学会）。通过学会的理事名单（中国中文信息学会）基本可以了解国内从事NLP/CL的主要单位和学者。学会每年组织很多学术会议，例如全国计算语言学学术会议（CCL）、全国青年计算语言学研讨会（YCCL）、全国信息检索学术会议（CCIR）、全国机器翻译研讨会（CWMT），等等，是国内NLP/CL学者进行学术交流的重要平台。尤其值得一提的是，全国青年计算语言学研讨会是专门面向国内NLP/CL研究生的学术会议，从组织到审稿都由该领域研究生担任，非常有特色，也是NLP/CL同学们学术交流、快速成长的好去处。值得一提的是，2010年在北京召开的COLING以及2015年即将在北京召开的ACL，学会都是主要承办者，这也一定程度上反映了学会在国内NLP/CL领域的重要地位。此外，计算机学会中文信息技术专委会组织的自然语言处理与中文计算会议（NLP&CC）也是最近崛起的重要学术会议。中文信息学会主编了一份历史悠久的《中文信息学报》，是国内该领域的重要学术期刊，发表过很多篇重量级论文。此外，国内著名的《计算机学报》、《软件学报》等期刊上也经常有NLP/CL论文发表，值得关注。
过去几年，在水木社区BBS上开设的AI、NLP版面曾经是国内NLP/CL领域在线交流讨论的重要平台。这几年随着社会媒体的发展，越来越多学者转战新浪微博，有浓厚的交流氛围。如何找到这些学者呢，一个简单的方法就是在新浪微博搜索的“找人”功能中检索“自然语言处理”、 “计算语言学”、“信息检索”、“机器学习”等字样，马上就能跟过去只在论文中看到名字的老师同学们近距离交流了。还有一种办法，清华大学梁斌开发的“微博寻人”系统（清华大学信息检索组）可以检索每个领域的有影响力人士，因此也可以用来寻找NLP/CL领域的重要学者。值得一提的是，很多在国外任教的老师和求学的同学也活跃在新浪微博上，例如王威廉（Sina Visitor System）、李沐（Sina Visitor System）等，经常爆料业内新闻，值得关注。还有，国内NLP/CL的著名博客是52nlp（我爱自然语言处理），影响力比较大。总之，学术研究既需要苦练内功，也需要与人交流。所谓言者无意、听者有心，也许其他人的一句话就能点醒你苦思良久的问题。无疑，博客微博等提供了很好的交流平台，当然也注意不要沉迷哦。
3. 如何快速了解某个领域研究进展
最后简单说一下快速了解某领域研究进展的经验。你会发现，搜索引擎是查阅文献的重要工具，尤其是谷歌提供的Google Scholar，由于其庞大的索引量，将是我们披荆斩棘的利器。
当需要了解某个领域，如果能找到一篇该领域的最新研究综述，就省劲多了。最方便的方法还是在Google Scholar中搜索“领域名称 + survey / review / tutorial / 综述”来查找。也有一些出版社专门出版各领域的综述文章，例如NOW Publisher出版的Foundations and Trends系列，Morgan & Claypool Publisher出版的Synthesis Lectures on Human Language Technologies系列等。它们发表了很多热门方向的综述，如文档摘要、情感分析和意见挖掘、学习排序、语言模型等。
如果方向太新还没有相关综述，一般还可以查找该方向发表的最新论文，阅读它们的“相关工作”章节，顺着列出的参考文献，就基本能够了解相关研究脉络了。当然，还有很多其他办法，例如去http://videolectures.net上看著名学者在各大学术会议或暑期学校上做的tutorial报告，去直接咨询这个领域的研究者，等等。