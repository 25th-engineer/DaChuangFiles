对于从事统计自然语言处理来说，了解概率论、信息论以及语言学知识都是很有必要的。
下面内容主要介绍了在统计自然语言处理中需要了解的概率论基础。
概率
如果P(A)作为事件A的概率，Ω是试验的样本空间，则概率函数满足下面三条公理：
非负性 P(A) >= 0
规范性 P(Ω) = 1
可列可加性：对于不相交的集合Aj ∈F
条件概率和独立性
假设事件B的概率已知，那么事件A发生的条件概率为（P(B) > 0）:
在统计自然语言处理中，上面那个链式法则很有用处，比如推导马尔可夫模型的性质。
贝叶斯定理
由条件概率和链式规则推得：
右边的分母P(A)可以看作是归一化常数，以保证其满足概率函数的性质。
如果我们感兴趣的仅仅是事件发生的相对可能性，这时可以忽略分母：
随机变量
设X为一离散型随机变量，其全部可能的值为{a1,a2,···}。那么：
pi = P(X = ai), i = 1, 2, ····
称为X的概率函数。
P(X <= x) = F(x), x∈R
称为X的分布函数。
期望和方差
联合分布和条件分布
设两个离散随机变量X和Y，它们的联合密度函数可以写为：
描述其中单个随机变量的概率密度函数称为边缘密度函数：
标准分布
离散分布函数：二项分布
重复一个只有两种输出的实验，并且每次实验之间相互独立时，我们认为实验结果服从二项分布（例如抛硬币实验）。
在自然语言处理中，语料库中的句子间肯定不是完全相互独立的。但是为了简化问题的复杂性，我们通常可能会做独立性假设，假设一个句子的出现独立于它前面的其他句子，近似认为它们服从二项分布。
当实验有两个以上结果时，二项分布问题就转化为多项式分布（multi-nomial distribution）。
连续分布函数：正态分布