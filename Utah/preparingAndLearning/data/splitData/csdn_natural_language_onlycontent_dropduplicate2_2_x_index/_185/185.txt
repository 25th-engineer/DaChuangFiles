哈尔滨工程大学-537
自然语言处理-BM25相关度打分
(注：文中大写Query、Document等代表集合，小写query、document等代表集合中的个体)
一、优缺点
适用于：在文档包含查询词的情况下，或者说查询词精确命中文档的前提下，如何计算相似度，如何对内容进行排序。
不适用于：基于传统检索模型的方法会存在一个固有缺陷，就是检索模型只能处理 Query 与 Document 有重合词的情况，传统检索模型无法处理词语的语义相关性。
白话举例：提出一个query：当下最火的女网红是谁？
在Document集合中document1的内容为：[当下最火的男明星为鹿晗]；
document2的内容为：[女网红能火的只是一小部分]。
显然document1和document2中都包含[火]、[当下]、[网红]等词语。但是document3的内容可能是：[如今最众所周知的网络女主播是周二柯]。很显然与当前Query能最好匹配的应该是document3，可是document3中却没有一个词是与query中的词相同的（即上文所说的没有“精确命中”），此时就无法应用BM25检索模型。
二、算法核心
BM25算法是一种常见用来做相关度打分的公式，思路比较简单，主要就是计算一个query里面所有词
q1,q2...qn
q
1
,
q
2
.
.
.
q
n
q_1,q_2...q_n和文档的相关度，然后再把分数做累加操作。公式如下：
Score(Q,d)=∑inWi⋅R(qi,d)(8)
(8)
S
c
o
r
e
(
Q
,
d
)
=
∑
i
n
W
i
⋅
R
(
q
i
,
d
)
Score(Q,d)=\sum_i^n{W_i}\cdot{R(q_i,d)}
其中
R(qi,d)
R
(
q
i
,
d
)
R(q_i,d)是查询语句query中每个词
qi
q
i
q_i和文档d的相关度值，
Wi
W
i
W_i是该词的权重，最后将所有词的
Wi∗R(qi,d)
W
i
∗
R
(
q
i
,
d
)
W_i*R(q_i,d)相加。
Wi
W
i
W_i一般情况下为
IDF(InverseDocumentFrequency)
I
D
F
(
I
n
v
e
r
s
e
D
o
c
u
m
e
n
t
F
r
e
q
u
e
n
c
y
)
IDF(Inverse Document Frequency)值，即逆向文档频率，公式如下：
IDF(qi)=logN+0.5n(qi)+0.5(9)
(9)
I
D
F
(
q
i
)
=
l
o
g
N
+
0.5
n
(
q
i
)
+
0.5
IDF(q_i)=log\frac{N+0.5}{n(q_i)+0.5}
其中
N
N
N是文档总数，
n(qi)
n
(
q
i
)
n(q_i)是包含该词的文档数，0.5是调教系数，避免
n(qi)=0
n
(
q
i
)
=
0
n(q_i)=0的情况。
log
l
o
g
log函数是为了让IDF的值受
N
N
N和
n(qi)
n
(
q
i
)
n(q_i)的影响更加平滑。
从公式中显然能看出IDF值的含义：即总文档数越大，包含词
qi
q
i
q_i的文档数越小，则
qi
q
i
q_i的IDF值越大。
白话举例就是：比如我们有1万篇文档，而单词basketball,Kobe Bryant几乎只在和体育运动有关的文档中出现，说明这两个词的IDF值比较大，而单词is, are, what几乎在所有文档中都有出现，那么这几个单词的IDF值就非常小。
解决了
Wi
W
i
W_i，现在再来解决
R(qi,d)
R
(
q
i
,
d
)
R(q_i,d)。
R(qi,d)
R
(
q
i
,
d
)
R(q_i,d)公式如下：
R(qi,d)=fi⋅(k1+1)fi+K⋅qfi⋅(k2+1)qfi+k2(10)
(10)
R
(
q
i
,
d
)
=
f
i
⋅
(
k
1
+
1
)
f
i
+
K
⋅
q
f
i
⋅
(
k
2
+
1
)
q
f
i
+
k
2
R(q_i,d)=\frac{{f_i}\cdot{(k_1+1)}}{f_i+K}\cdot{\frac{{qf_i}\cdot{(k_2+1)}}{qf_i+k_2}}
其中
k1,k2,b
k
1
,
k
2
,
b
k_1,k_2,b都是调节因子，一般
k1=1,k2=1,b=0.75
k
1
=
1
,
k
2
=
1
,
b
=
0.75
k_1=1,k_2=1,b=0.75。
式中
qfi
q
f
i
qf_i为词
qi
q
i
q_i在查询语句
query
q
u
e
r
y
query中的出现频率，
fi
f
i
f_i为
qi
q
i
q_i在文档
d
d
d中的出现频率。由于绝大多数情况下一条简短的查询语句
query
q
u
e
r
y
query中，词
qi
q
i
q_i只会出现一次，即
qfi=1
q
f
i
=
1
qf_i=1，因此公式可化简为：
R(qi,d)=fi⋅(k1+1)fi+K(11)
(11)
R
(
q
i
,
d
)
=
f
i
⋅
(
k
1
+
1
)
f
i
+
K
R(q_i,d)=\frac{{f_i}\cdot{(k_1+1)}}{f_i+K}
其中
K=k1⋅(1−b+b⋅dlavgdl)(12)
(12)
K
=
k
1
⋅
(
1
−
b
+
b
⋅
d
l
a
v
g
d
l
)
K={k_1}\cdot{(1-b+b\cdot{\frac{dl}{avgdl}})}
dl
d
l
dl为文档
d
d
d的长度，
avgdl
a
v
g
d
l
avgdl为所有文档的平均长度。意即该文档
d
d
d的长度和平均长度比越大，则
K
K
K越大，则相关度
R(qi,d)
R
(
q
i
,
d
)
R(q_i,d)越小,
b
b
b为调节因子，
b
b
b越大，则文档长度所占的影响因素越大，反之则越小。
白话举例就是：一个
query
q
u
e
r
y
query为：诸葛亮在哪里去世的？
document1的内容为：诸葛亮在五丈原积劳成疾，最终去世；
document2的内容为：司马懿与诸葛亮多次在五丈原交锋；
而document3为一整本中国历史书的内容。
显然document3中肯定包含了大量[诸葛亮]、[哪里]、[去世]这些词语，可是由于document3文档长度太大，所以
K
K
K非常大，所以和
query
q
u
e
r
y
query中每个词
qi
q
i
q_i的相关度
R(qi,d)
R
(
q
i
,
d
)
R(q_i,d)非常小。
综上所述，可将BM25相关度打分算法的公式整理为：
Score(Q,d)=∑inIDF(qi)⋅fi⋅(k1+1)fi+k1⋅(1−b+b⋅dlavgdl)(13)
(13)
S
c
o
r
e
(
Q
,
d
)
=
∑
i
n
I
D
F
(
q
i
)
⋅
f
i
⋅
(
k
1
+
1
)
f
i
+
k
1
⋅
(
1
−
b
+
b
⋅
d
l
a
v
g
d
l
)
Score(Q,d)=\sum_i^nIDF(q_i)\cdot\frac{{f_i}\cdot{(k_1+1)}}{f_i+k_1\cdot{(1-b+b\cdot{\frac{dl}{avgdl}})}}