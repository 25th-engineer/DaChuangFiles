目录
文章目录
目录
前言
n-gram语言模型（一）
n-gram语言模型（二）
n-gram语言模型（三）
n-gram语言模型（四）
n-gram语言模型（五）
n-gram语言模型（六）
n-gram语言模型（七）
前言
硕士生涯结束，开始专心做一件自己觉得有用的工具，先做工程，后搞理论。
自然语言处理是一个非常难的问题，同时是人工智能皇冠上的明珠。
接下来会记录一系列自然语言处理的笔记，来自于哈工大老师关毅。
n-gram语言模型（一）
n元词序列，通分词一元频度，语料库加工，最大熵模型基本信息，噪声信道模型，信源s发出信息，一系列01序列。输入和输出完全匹配一致，信息转变。
in->process->out 贝叶斯公式是统计的核心地位，一个声学信号对应于一个语句。
T=argmax（p（T/A））
求的是使其概率最大的T。
语音识别的应用，信源的应用：手写体汉字识别，文字作用信源。
以概率p输出字符串。
目标，翻译，输出。
一段语音文字出现的概率（P（T）），语言模型，完成特定功能的数据结构。实现字符串结构的模型概率，信源字符序列。
香农游戏给定前n个词，求下一个词。
n-gram语言模型（二）
全概率模型0-1规则，力量较强。
参数统计模型，空间大，稀疏。
马尔可夫假设：
下一个词依赖于前一个词：
P（s|t）=P(S|t-1)
Trigram模型：
P
(
I
)
P
(
W
1
)
P
(
W
2
∣
W
1
)
P(I)~P(W_1)P(W_2|W_1)
P(I) P(W1 )P(W2 ∣W1 )
还可以无限延伸变成ngram模型。
约减参数空间，可靠辨别，一个参数。
数据平滑技术，只统计他的一元频度
某语料库词汇分布图，最大相似度估计分布图，期望概率分布图。
n-gram语言模型（三）
数据平滑技术，discounting技术，分给小的validation，拉普拉斯定律，加一平滑法。
大家同加一，解决数据稀疏问题。
P
l
a
p
=
W
1
N
+
B
B
=
∣
U
∣
n
P_{lap}=\frac{W_1}{N+B} B=|U|^n
Plap =N+BW1 B=∣U∣n
Good-Turing 估计 ：如果
C
(
W
1
,
W
2
,
.
.
.
W
N
)
=
r
&gt;
0
C(W_1,W_2,...W_N)=r&gt;0
C(W1 ,W2 ,...WN )=r>0
P
a
t
(
W
1
,
.
.
W
n
)
=
r
∗
/
N
Pat(W_1,..W_n)=r*/N
Pat(W1 ,..Wn )=r∗/N
此处R*=（r+1）S(r+1)/s®
这里S®是Nr的期望平滑估计。Nr=arb，估计整体分布参数估计的一种。
图灵估计，线性插值平滑。构造高鲁棒性语言模型2规模小，效果显著3规模大效果不显著。
技术上实行。
n-gram语言模型（四）
只依赖前n-1个词的词性，n-pose模型。
动态估计和静态估计合力解决词汇问题
统计语言模型的评价方法：实用方法。
基于交叉熵与迷惑度的方法。
H
(
x
)
=
−
∑
q
(
x
)
l
o
g
p
(
x
)
q
(
x
)
H(x)=-\sum q(x) log \frac{p(x)}{q(x)}
H(x)=−∑q(x)logq(x)p(x)
n-gram语言模型（五）
argmax（P（T|s））语言模型的实例
考虑数据的加载与注入，高压缩比数据。
N-gram 语言模型的构造
n-gram语言模型（六）
理解骨架，基本模型，隐马，极大熵。
生成/条件判别模型。
最大熵原理是指在一定的限制条件下，尽可能地选择熵最大的概率分布（均匀分布），作为预测结果，而对不知道（限制条件以外）的情形不做任何假设。
如何设计正负的概率。
假设在语料库中，有如下词性标记及次数，估计在限定条件下的概率，选择满足限定条件的P。
使H（p）为最大
H
(
x
)
=
−
∑
P
(
x
)
l
o
g
p
(
x
)
H(x)=-\sum P(x)logp(x)
H(x)=−∑P(x)logp(x)a<A且b<B.
在最大熵模型中，特征是一个关于事件的二值函数。
f
j
:
x
−
》
0
,
1
,
x
=
A
∗
B
f_j:x-》{0,1},x=A*B
fj :x−》0,1,x=A∗B,原子级特征。
n-gram语言模型（七）
限制条件，模型特征的期望值等于训练语料库中观察到的特征的期望值。
E
p
f
j
=
E
p
f
j
E_pf_j=Ep^~f_j
Ep fj =Ep fj
训练语料库非常关键，从训练数据到可观测事件，解的存在且唯一，拉格朗日解。
最大熵模型的使用方法（rf条件随机域）
文本数据，数据缺失，HMM/EM。