经过几天对nlp的理解，接下来我们说说语言模型，下面还是以PPT方式给出。
一、统计语言模型
1、什么是统计语言模型？
一个语言模型通常构建为字符串s的概率分布p(s)，这里的p(s)实际上反映的是s作为一个句子出现的概率。
这里的概率指的是组成字符串的这个组合，在训练语料中出现的似然，与句子是否合乎语法无关。假设训练语料来自于人类的语言，那么可以认为这个概率是的是一句话是否是人话的概率。
2、怎么建立统计语言模型？
对于一个由T个词按顺序构成的句子，p(s)实际上求解的是字符串的联合概率，利用贝叶斯公式，链式分解如下：
从上面可以看到，一个统计语言模型可以表示成，给定前面的的词，求后面一个词出现的条件概率。
我们在求p(s)时实际上就已经建立了一个模型，这里的p(*)就是模型的参数，如果这些参数已经求解得到，那么很容易就能够得到字符串s的概率。
3、求解的问题
假定字符串s为“i want to drink some water”，那么根据上面所建立的模型：
问题归结为如何求解上面的每一个概率，比如，一种比较直观的方法就是分别计算出“I want to”和“I want to drink”在语料中出现的频数，然后再用除法：
看起来好像很美好，实际上这里存在两个问题：
（1）自由参数数目：
假定字符串中字符全部来自与大小为V的词典，上述例子中我们需要计算所有的条件概率，对于所有的条件概率，这里的w都有V种取值，那么实际上这个模型的自由参数数目量级是V^6，6为字符串的长度。
从上面可以看出，模型的自由参数是随着字符串长度的增加而指数级暴增的，这使我们几乎不可能正确的估计出这些参数。
（2）数据稀疏性：
从上面可以看到，每一个w都具有V种取值，这样构造出了非常多的词对，但实际中训练语料是不会出现这么多种组合的，那么依据最大似然估计，最终得到的概率实际是很可能是0。
4、怎么解决？
上面提出了传统统计语言模型的两个问题，后面分别介绍两种方法进行求解：N-gram语言模型，神经概率语言模型
二、N-gram语言模型
1、什么是N-gram语言模型？
为了解决自由参数数目过多的问题，引入了马尔科夫假设：随意一个词出现的概率只与它前面出现的有限的n个词有关。基于上述假设的统计语言模型被称为N-gram语言模型。
2、如何确定N的取值？
通常情况下，n的取值不能够太大，否则自由参数过多的问题依旧存在：
（1）当n=1时，即一个词的出现与它周围的词是独立，这种我们称为unigram，也就是一元语言模型，此时自由参数量级是词典大小V。
（2）当n=2时，即一个词的出现仅与它前面的一个词有关时，这种我们称为bigram，叫二元语言模型，也叫一阶马尔科夫链，此时自由参数数量级是V^2。
（3）当n=3时，即一个词的出现仅与它前面的两个词有关，称为trigram，叫三元语言模型，也叫二阶马尔科夫链，此时自由参数数量级是V^3。
一般情况下只使用上述取值，因为从上面可以看出，自由参数的数量级是n取值的指数倍。
从模型的效果来看，理论上n的取值越大，效果越好。但随着n取值的增加，效果提升的幅度是在下降的。同时还涉及到一个可靠性和可区别性的问题，参数越多，可区别性越好，但同时单个参数的实例变少从而降低了可靠性。
3、建模与求解
N-gram语言模型的求解跟传统统计语言模型一致，都是求解每一个条件概率的值，简单计算N元语法在语料中出现的频率，然后归一化。
4、平滑化
我们在传统统计语言模型提出了两个问题：自由参数数目和数据稀疏，上述N-gram只是解决了第一个问题，而平滑化就是为了解决第二个问题。
假设有一个词组在训练语料中没有出现过，那么它的频次就为0，但实际上能不能认为它出现的概率为0呢？显然不可以，我们无法保证训练语料的完备性。那么，解决的方法是什么？如果我们默认每一个词组都出现1次呢，无论词组出现的频次是多少，都往上加1，这就能够解决概率为0的问题了。
上述的方法就是加1平滑，也称为拉普拉斯平滑。平滑化还有许多方法，这里就不展开介绍了：
（1）加法平滑
（2）古德-图灵平滑
（3）K平滑
三、神经概率语言模型
1、前置知识
在N-gram语言模型中，计算条件概率的方法是简单的用词频做除法然后归一化。
在机器学习的领域中，通用的做法是：对所考虑的问题建模后先为其构造一个目标函数，然后对这个目标函数进行优化，从而求得一组最优的参数，最后再利用这组参数对应的模型来进行预测。
那么在上述的语言模型中，利用最大化对数似然，将目标函数设为：
Context代表词w的上下文，对应N-gram就是词w的前N-1个词。之后对目标函数进行最大化，由上可见，概率实际上是w和的函数：
其中θ为待定参数集，这样将计算所有的条件概率转化为了最优化目标函数，求解得到θ的过程。通过选取合适模型可以使得θ参数的个数远小于N-gram模型中参数的个数。
2、什么是神经概率语言模型？
Begio等人在2003年发表的A Neural Probabilistic Language Model，里面详解了这个方法。
基本的思想其实与上述的前置知识有所联系，既然是神经概率语言模型，那么实现的时候自然有一个神经网络，结构图如下：
它包括了四个层：输入层、投影层、隐藏层和输出层。
2、计算流程
（1）输入层
这里就是词w的上下文，如果用N-gram的方法就是词w的前n-1个词了。每一个词都作为一个长度为V的one-hot向量传入神经网络中
（2）投影层
在投影层中，存在一个look-up表C，C被表示成一个V*m的自由参数矩阵，其中V是词典的大小，而m作为自定义的参数，一般是10^2的倍数。
表C中每一行都作为一个词向量存在，这个词向量可以理解为每一个词的另一种分布式表示。每一个one-hot向量都经过表C的转化变成一个词向量。
n-1个词向量首尾相接的拼起来，转化为(n-1)m的列向量输入到下一层。
（3）隐藏层、输出层
之后再对列向量进行计算，大致如下：
其中tanh是激活函数，是为归一化的log概率，之后再用softmax进行归一化，就得到最终的概率输出了。
在前置知识中我们提到了参数θ，那么在神经网络中，实际的参数如下：
词向量：v(w)，w以及填充向量
神经网络参数：W，p，U，q
3、最后
在传统统计语言模型中，我们提出两个问题：自由参数数目和数据稀疏。
这里在实际上使用参数θ代替了自由参数指数级的求解，而数据稀疏问题，我们在最后使用softmax进行归一化，求解出来的概率是平滑的，所以也解决了这个问题。






参考：（PPT来源小象学院史兴老师）