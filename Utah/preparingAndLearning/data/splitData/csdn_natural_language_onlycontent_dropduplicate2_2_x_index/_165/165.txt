前言
本节主要针对斯坦福大学CS224N的自然语言处理与深度学习课程所做笔记,将学习过程中的一些重难点进行记录,方便后续复习
什么是自然语言处理
自然语言处理是计算机科学,人工智能和语言学的集合,该技术的目的是为了使计算机能够理解语言.
自然语言处理的一些应用
拼写检查,关键词查询,语法检查,文本分类,对话系统…
什么是深度学习
深度学习是机器学习的一个分之,和传统方法的区别主要在于其端到端的形式,从raw input中自动提取特征,最后输出想要的结果分类或者回归.
一些先修知识
Python基础,线性代数,概率论,统计学,基本的机器学习理论
希望教授的知识
使用有效深度学习模型的能力,比如NLP中的一些重要技术,RNN,attention机制
有NLP有个宏观的认知,了解该领域的一些难点
有能力构建一些NLP中的系统来解决一些主要问题,比如单词的相似度,命名体识别,翻译系统,对话系统…
为什么NLP难
语言表示和学习的复杂性,语言的歧义性,人们语言的解读依赖于实际环境,比如场景和上下文.
Deep NLP=Deeplearning + NLP
词义的表示,向量化单词,且可进行可视化
词义向量(Morphology)
依存句法分析(Parsing for sentence structure)
句子的含义(Semantic)
情感分析(Sentiment Analysis)
作业
第4部分作业：Assignment 1.1-1.2（地址： https://github.com/learning511/cs224n-learning-camp/blob/master/Assignmnet.md ）
1.1 Softmax 算法
1.2 Neural Network Basics 神经网络基础实现
Word Vector
本节主要记录述词向量相关的原理和内容
如何表示单词含义
词义进行表示
离散表示,Onehot
基于单词上下文的分布式表示
将单词表示为向量形式
Word2vec 的主要思想
两个方法:Skip-gram和CBOW
Skip-gram:根据中心词,预测上下文
CBOW:根据上下文预测中心词
两种相对效率的训练方法:霍夫曼树,负采样法
训练中同时更新迭代所有向量,每个词有两个向量的表示,一个是作为中心词时候的词向量,另一个是作为上下文的词向量
更新迭代的过程
更新计算所有梯度,在划窗过程中,更新计算的不止是中心词的词向量,在每一个划窗过程,对窗口中的中心词词向量,上下文词向量都进行更新.