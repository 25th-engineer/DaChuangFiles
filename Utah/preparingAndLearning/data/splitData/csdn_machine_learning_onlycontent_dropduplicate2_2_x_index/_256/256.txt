在说贝叶斯规则（Bayes rule）和将贝叶斯规则用于图模型之前，先让大家了解下机器学习的四个范式（paradigms），也可以理解为四个流派；
连接主义（connectionist）：用现在比较流行的说法就是神经网络，现在用到的工具有Tensorflow、PyTorch、Theano、caffe等~
符号主义：逻辑的学习，思想严密但计算复杂度很高，从18年开始，在nlp领域上有人开始尝试连接主义和符号主义的联姻~
统计学习：包括大家很熟悉的SVM，统计学习是基于统计学的原理，通过理论出发实现可靠的模型~目前的困难在于大数据量训练的时候需要一个更为有效的训练方法
图模型：这个包括我们今天需要学习的贝叶斯网络，还有下一篇文章我将会讲到的HMM（隐形马尔可夫模型）
下图为贝叶斯公式
在Monty Hall Problem上，贝叶斯公式有着很直接的应用，大家可以查看链接并了解下是如何通过贝叶斯公式来解决这个难题的~如果需要更直观的理解这个问题和解法，可以看看这篇博客，问题中每个概率的值怎么来博主都有相应的解释。
维度的诅咒（The curse of dimensionality）
当一个模型中有多个参数（设有d个），若要知道这个模型参数的联合分布，如果用直接存储的方式，则需要2^d的存储空间，当d的数量上升到成千上万个以后，存储空间的开销将会变得异常的大，而概率图模型可以以高效的方式将联合分布从指数级别的复杂度转变成多项式级别的复杂度，从而减少存储空间，这是概率图模型的motivation~
介绍概率图模型前先说下两个词：
Directed probabilistic graphical models：有向概率图模型
Directed Acyclic Graph（DAG）：有向无环图
上面的图片可能有点太乱了，但顺着看下来还是可以看懂的。首先，将图片内中间的有向无环图看成为一颗树，这棵树中每个节点（node）的概率为：P（Node|Parent(Node)），例如图中的Alarm，它的父节点为Earthquake和Burglary，这Alarm节点的概率为P(A|E,B)。所以该概率图的全概率P(B,E,A,R,C)=P(B)P(E)P(A|B,E)P(R|E)P(C|A)，所以DAG的概率公式为图片下方所示。
条件概率、联合分布、VC维