来源：AI 研习社
本文作者紫杉，本文整理自作者在知乎《有没有必要把机器学习算法自己实现一遍？》问题下的回答。AI 研习社已获得转载授权。
哈哈哈哈，我觉得很多人都有这个疑问吧。机器学习好高大上，多么牛逼的东西，光是看公式就已经眼花缭乱了，总觉得自己该全部去实现一遍，有的时候太懒，有的时候觉得能力不够。道理虽然明白——任何事情自己亲手做一做还是更好的，但机器学习已经有了大量的库了，SVM-Light，R里面的glm()方程，自己实现一遍，最后又不敢用（因为不知道算法究竟是否正确），或者不能用（一是速度赶不上大神写的库那么快，二是精度没有专业库那么高），耗时耗力的写了一堆后究竟有什么用？
这里很多答案都提供了一些解释，但我想从另一个角度来聊聊这个问题。
我在1年半前（本科阶段）就开始接触计算心理学和机器学习方面的研究，在NAACL（自然语言处理排名第三的论坛）上发表了一篇文章，用的计算机教授写的算法库，跑的是经过AdaGrad优化的向量支持机（SVM）算法。在这种论坛发文章，你是必须去做海报展示的，站在自己的大幅海报面前傻傻的待4个小时，我的两位教授（一位是认知语言学教授，一位是计算机教授）都在那里。我的位置不太好，在最边缘的角落里，本来以为就可以赢得一份清净，Philip Resnik走了过来。直到那一刹那之前，我一直不知道他是谁。但经过教授介绍后，他是马里兰大学的机器学习和自然语言处理教授，在这个领域混了多年，在Google Schoar上的论文引用数高达12,853。
他走过来的第一句话是：“假设我一点也不懂数学，告诉我你这篇论文做的是什么。”我解释后，看到我的计算机教授走了过来和Resnik聊天，Resnik问我的教授：“你用的是不是hinge loss（辛基损失函数）？”我的教授说：“是。但不是全局优化，所以我没有叫这玩意SVM……”（我凭回忆打出来的，可能不完全精确）。当时我站在一旁觉得这他们能这样大聊特聊数学，甚至是向量支持机（我当时认为这是最厉害的算法——除神经网络以外），简直是太厉害了，我一点也听不懂他们在讲什么。
直到现在，我才明白所谓的“辛基损失函数（Hinge loss）”其实就是Max(a,b)函数，就是比较 a 和 b 谁大谁小，然后选大的那个。这玩意究竟有什么难理解的？为什么要那么高大上？你让一个五岁的小孩，问他：“有一堆红球，一堆绿球，哪一堆的球更多啊？”这个小孩都能告诉你正确答案。
当然这说的有点偏题了。后来我非常幸运的考上了研究生，才终于开始了对“高档”算法的学习。第一个学期被Christopher Manning（克里斯多夫·曼宁）的CS224N自然语言处理虐了一番，这个学期开始上Andrej Karpathy（安杰·卡帕西）的神经网络（CS231N），该君是李菲菲教授（音译，Fei-Fei Li）的爱徒，在推特上有14.9K关注者，我越看他那张方块脸，越觉得他长得像贾斯丁·汀布莱克（Justin Timberlake）。
我其实也是自控能力很差的人，在上安杰·卡帕西的课之前，也从没有萌生过自己去写机器学习算法的想法。原因在文章开头有提过：1. 我的代码运行速度肯定赶不上经过多次迭代的专业库的运行速度；2. 我咋知道我的代码写的就是对的呢？
我直到现在都这样认为：不考虑对方的环境和条件，知识与技能，就一味要求对方把机器学习算法都实现一遍，估计是最无理取闹的行为了吧。前天晚上，我跟另一个研究生Jason Freeman（杰森·弗里曼）聊天，他在微软的西雅图总部工作了4年，在目前越来越有名的TypeScript团队工作了3年（TypeScript是静态的JavaScript语言，正在国内和国外开始流行）——他告诉我他和安德斯·海尔斯伯格（Anders Hejlsberg）一起工作，他还经常顶撞安德斯。我的第一反应是：“他是谁……”（安德斯·海尔斯伯格是Delphi和C#之父，但我不熟悉这两门语言，所以不会崇拜他——小广告：Scala是我目前最喜欢的语言）。
我和杰森讨论的是3月份开始究竟要不要选吴恩达（Andrew Ng）的机器学习课（CS229）。我持的立场是我可能不打算上那门课，因为我已经看过大部分他的视频了，还读了他讲义的一部分（这里是讲义链接： CS 229: Machine Learning (Course handouts) http://t.cn/R009lCm）。因为我已经确定以后要做神经网络方面的研究，所以觉得他课上的一些其他内容比如特征降维（PCA），对我而言用处不大，我只需要会用就行了。我不仅用过特征降维，还用过更好的降维可视化（tSNE算法）。这玩意和我的领域不搭，为什么我要浪费时间去学？
杰森的论点是，如果我学了它们的理论（甚至把它们实现一遍），就能更好的应用它们。我说：你把直觉（intuition）当什么了？在我看来，对算法进行“直观”上的了解，就已经很足够了。什么是向量支持机？就是拿一个平面去分隔一堆点。更术语一点的解释不外乎是拿一个超平面（Hyperplane）在高维空间里去分割。什么是特征降维？就是看如何把高维度的点阵降到两三个维度。什么是alpha值？就是看这个算法学得有多快。什么是正则化（regularization）？就是别让你的算法过度拟合数据（当然L1，L2等等都有区别，但这些区别都很简单，L1让你关注某个值，L2让你利用所有的值）。
为什么我谈这么多关于理论的问题？在我看来，学习机器学习的算法的进度是这样的：应用 -》理论 -》实现。就跟教小孩折射一样，你先让他看看筷子在水中如何弯折（应用），再告诉他光的折射原因（理论），再让他自己用其他物体来试试（实现）。实现，是这个漫长学习过程的最后一步。一开始就来谈实现，实在是很神奇的事情。
让我准确论述一下我的观点：如果你是学界精英，那么去学习那些你将要使用的算法的理论，最后再自己尝试着实现他们，是很有必要的，除非你是只做应用（比如社会科学，心理学，商学等等）。如果是普通的程序员/工程师，不需要强迫自己去实现这些算法。没人会给你一个小奖章，大公司招这类员工的时候，也是更看重学历，而不是看“哦，我把‘所有’的机器学习算法都实现了一遍”。
最后送上一点我觉得实现机器学习算法最好的路径：
最好用Python和Numpy库。这两样宝具会让你非常轻松。安杰·卡帕西（Andrej）推荐用ipython notebook（现在改名叫Jupyter了），来可视化数据，以及实验算法。昨天有一个下午茶会，我们系举办的，也邀请了安杰，我跑去凑热闹，跟安杰谈到了这个问题，他说就算是大公司做研究，也是这种路径，先从ipython notebook开始（这点让我很惊讶）。
机器学习算法最难的部分其实不是写出来，而是高效率的实现，让你的算法跑快一点。其中一个技巧叫做“矢量化”（Vectorization）。矢量化就是说，能做矩阵操作就矩阵操作，最好连一个外循环都不写。
这是我写的Softmax算法的测评：（在500个样本上跑的）
naive loss: 2.384533e+00 computed in 0.255952s vectorized loss: 2.384533e+00 computed in 0.004148s
第一个是用普通的Python和循环写出来的，第二个是用矢量化操作写出来的，可以看到64倍速度的提升——侧面也可以看到Python有多垃圾（慢）。
这个是SVM（支持向量机）算法的测评：（同样500个样本）
Naive loss: 9.102322e+00 computed in 0.136226s Vectorized loss: 9.102322e+00 computed in 0.005909s
这次的速度提升没有那么明显，但也是26倍的提速。
但我只想说：矢量化真是很难的事情。数学家随便就写公式，也不考虑考虑可怜的计算机科学孩子们。原初的公式几十分钟就搞定，矢量化要一两个小时的冥思苦想。
最后，对于那些读懂了理论，实在是闲得无聊，或者想要进军更高级的学术界的同志们，这里是安杰·卡帕西课代码的链接：CS231n Convolutional Neural Networks for Visual Recognition（http://t.cn/RZ0FlxD）。如果你不属于这个类别，就不要瞎掺合啦，用用别人的库又怎么了？骇客精神(Hacker Code)中一条就是：“不要重复劳动，有库就要用库，不然就是对库写作者的不尊重。”
（如果你还是不知道究竟该不该实现，欢迎阅读下面我增加的内容）
------------------
最近这篇文章被学姐前辈Danqi Chen看到了。。所以我稍微补充几句，免得被大牛们看到后笑话。。- ___ - || Danqi前辈是清华姚班的高材生，Chris Manning的博士，在224N课上是首席助教，然后被我缠着问了好多次问题……
这篇文章有点接近“反智”文章的边缘，大意是实用主义至上，自己实现的必要性不大。这个观点还是有很多争议的，比如目前有一个答案就“实名”反对这个答案。机器学习是一个交叉学科，作为学生而言，从不同的部门学到的机器学习，必然是不一样的。在统计学部门学到的机器学习，和在计算机部门学的机器学习，肯定是两个样。我秋天的时候跟一位概率教授上了一节课，当我告诉他斯坦福计算机入门概率课要介绍MLE（最大拟然估值）和蒙特拉罗模拟（Monte Carlo Simulation）的时候，他沉重的摇摇头，说这么早就介绍这样深刻的概念，是很不应该的，在他的部门，要第三年的学生才接触这样的知识，因为直到那时，学生才有足够的知识框架去理解这些知识。
我写这篇文章是有一定的原因的。我认识一些国内的大学同学，都异常优秀，他们努力的程度是我一辈子都比不上的，他们中一部分人因为运气不好（高考），不幸去了一些相对不是那么优异的大学，但是他们用努力弥补这个缺陷，对数学和各种学科展开攻克，很多人的阅读量和数学解题技巧也是我不能企及的。还有一部分人，是处于业界转型边缘，本来已是成熟的程序员，或者数据分析师，但是想要进一步提升自己，亦或是转型。我把这两类人定做这篇回答的目标受众。我希望为他们写一篇回答，不让他们走我走过的弯路，不受太多的误导。
开复先生（李开复）最近说深度学习急缺人才。我非常的不赞同。深度学习领域是处于半饱和状态的，实际上就业情况就是一堆熠熠生辉（Scintillating）的博士们在学术界待腻了，想要去赚点钱玩玩，就跑去业界晃一圈的状况。这和大部分人的就业状况根本是不搭边的。深度学习，以及理论机器学习，除非是平台很高，起点很高，是很难得到广泛认可的。
我最近刚买了一本书：
这本书很详细的在讲Lasso Loss（L1），写SVM的部分也非常不错，很神奇的是，三位作者，两位是斯坦福统计学系的，一位是伯克利的。如果我能读完这本书，会上来改进一下这个答案的。
最近我想提一提答案末尾写的，关于“实现”的问题。我过去几周一直在写我自己的Theano库（是的，放着牛逼的Lasagne不用，非要自己手写），终于把CNN写完后，现在在写RNN的部分。当我已经花费这么多的时间，然后意识到，我的代码暂时还只能在CPU上跑，因为我暂时还没有用Theano的CUDA库，又意识到，仅仅几周后，我的两门春季课已经开始教TensorFlow了，于是觉得自己是个傻子。
所以我自己都陷入了我回答中所写的那个陷阱：实现之后却不能使用，但又不愿意放弃自己的代码，于是只有投入更多的时间去改代码，而不是去理解数学。愿与各位共勉。
点击下方“阅读原文”下载“科技头条”↓↓↓