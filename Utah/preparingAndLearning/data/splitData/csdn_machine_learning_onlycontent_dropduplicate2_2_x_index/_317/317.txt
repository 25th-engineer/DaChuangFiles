无论数据科学解决方案的最终目标如何，最终用户总是更喜欢可解释且易于理解的解决方案。此外，作为数据科学家，您将始终受益于模型的可解释性，以验证和改进您的工作。在这篇博客文章中，我试图解释机器学习中可解释性的重要性，并讨论一些您可以自己试验的简单操作和框架。
关于机器学习的xkcd
为什么机器学习中的可解释性很重要？
在传统统计中，我们通过调查整个数据来构建和验证假设。我们构建模型来构建规则，我们可以将这些规则合并到流程的心理模型中。例如，营销公司可以构建一个模型，该模型将营销活动数据与财务数据相关联，以确定什么构成有效的营销活动。
这是一种自上而下的数据科学方法，可解释性是关键，因为它是所定义的规则和流程的基石。由于相关性通常不等于因果关系，因此在制定决策和解释时需要一个可靠的模型理解。
在自下而上的数据科学方法中，我们将业务流程的各个部分委托给机器学习模型。此外，通过机器学习实现了全新的商业创意。自下而上的数据科学通常对应于手动和繁重任务的自动化。制造公司可以例如将传感器放在他们的机器上并执行预测性维护。因此，维护工程师可以更高效地工作，而无需执行昂贵的定期检查。模型可解释性是必要的，以验证模型正在做什么符合您的期望，并允许与用户建立信任并简化从手动过程到自动过程的过渡。
在自上而下的过程中，您可以迭代地构造和验证一组假设。在自下而上的方法中，您尝试通过自下而上解决问题来自动化流程。
作为数据科学家，您经常关注微调模型以获得最佳性能。数据科学通常被定义为：'给定带有标签y的数据X，找到具有最小误差的模型'。虽然培训高性能模型的能力对于数据科学家来说是一项关键技能，但重要的是能够从更大的角度看待。数据和机器学习模型的可解释性是数据科学管道实际“有用”中关键的一个方面，它确保模型与您要解决的问题保持一致。虽然在构建模型时尝试使用最先进的技术很容易迷失自己，但能够正确解释您的发现是数据科学过程的重要组成部分。
需要解释模型来验证模型预测的有用性。
为什么对模型进行深入分析至关重要？
作为数据科学家，有几个理由关注模型可解释性。虽然这些之间存在重叠，但它们捕获了可解释性的不同动机：
识别并减轻偏见。
偏差可能存在于任何数据集中，由数据科学家来识别并尝试修复它。数据集的大小可能有限，并且可能无法代表整个数据集，或者数据捕获过程可能没有考虑潜在的偏差。在彻底的数据分析之后或当模型预测与模型输入之间的关系被分析时，偏差通常才会变得明显。如果您想了解更多有关不同类型偏见的信息，我强烈推荐以下视频。请注意，解决偏见没有单一的解决方案，但是在解释性方面意识到潜在偏见的关键步骤。
其他偏见的例子如下：
例如，word2vec向量包含性别偏见，因为他们已经训练过的语料库中存在固有的偏见。当您使用这些单词嵌入来训练模型时，搜索“技术配置文件”的招聘人员会将女性简历留在堆的底部。
例如，当您在小型手动创建的数据集上训练对象检测模型时，通常情况下图像的宽度太有限。需要在不同环境，不同闪电条件和不同角度的物体的各种图像，以避免仅适合数据中的噪声和不重要元素的模型。
考虑问题的背景。
在大多数问题中，您正在使用的数据集只是您要解决的问题的粗略表示，而机器学习模型通常无法捕获现实任务的完整复杂性。可解释的模型可帮助您理解和解释模型中包含的（未）因素，并在根据模型预测执行操作时考虑问题的上下文。
改善泛化和绩效。
高可解释性通常会导致模型更好地推广。可解释性不是要理解所有数据点的模型的每个细节。固体数据，模型和问题理解的结合对于拥有更好的解决方案是必要的。
道德和法律原因。
在金融和医疗保健等行业中，审核决策流程并确保其不歧视或违反任何法律至关重要。随着GDPR等数据和隐私保护法规的兴起，可解释性变得更加重要。此外，在医疗应用或自动驾驶汽车中，单个不正确的预测可能会产生重大影响，并且能够“验证”模型是至关重要的。因此，系统应该能够解释它是如何达到给定的建议的。
解释你的模型
关于模型可解释性的一个共同点是，随着模型复杂性的增加，模型可解释性至少下降得如此之快。特征重要性是解释模型的基本（通常是免费）方法。即使对于诸如深度学习的黑盒模型，也存在提高可解释性的技术。最后，将讨论LIME框架，作为模型分析的工具箱。
特征重要性
广义线性模型
广义线性模型（GLM）都基于以下原则：
如果您将特征x与模型权重w进行线性组合，并通过壁球函数f提供结果，则可以使用它来预测各种各样的响应变量。GLM的最常见应用是回归（线性回归），分类（逻辑回归）或建模泊松过程（泊松回归）。训练后获得的权重是特征重要性的直接代表，它们为模型内部提供了非常具体的解释。
例如，在构建文本分类器时，您可以绘制最重要的特征并验证模型是否过度拟合噪声。如果最重要的单词与您的直觉（例如名称或停用词）不对应，则可能意味着该模型适合数据集中的噪声，并且在新数据上表现不佳。
TidyTextMining中用于文本可解释性的整洁可视化示例。
随机森林和SVM
甚至诸如基于树的模型（例如，随机森林）之类的非线性模型也允许获得关于特征重要性的信息。在随机森林中，在训练模型时，特征重要性是免费的，因此它是验证初始假设并确定模型正在学习什么的好方法。基于内核的方法（如SVM）中的权重通常不是特征重要性的非常好的代表。内核方法的优点是，您可以通过将特征投影到内核空间来捕获变量之间的非线性关系。另一方面，仅仅将权重视为特征重要性并不能完成特征交互。
通过查看特征重要性，您可以确定模型正在学习什么。由于这个模型中的许多重要性都被放到了一天中，因此可能值得加入其他基于时间的功能。（Kaggle）
深度学习
由于参数的剪切数量和提取和组合特征的复杂方法，深度学习模型因其不可解释性而臭名昭着。由于这类模型能够在许多任务上获得最先进的性能，因此许多研究都集中在将模型预测与输入联系起来。
可解释机器学习的研究数量正在快速增长（麻省理工学院）。
特别是当转向处理文本和图像数据的更复杂系统时，很难解释模型实际学习的内容。研究的主要焦点目前主要是将输出或预测与输入数据联系起来并将其关联起来。虽然这在线性模型的背景下相当容易，但对于深度学习网络来说仍然是一个未解决的问题。两种主要方法是基于梯度或基于注意力。
- 在基于梯度的方法中，在向后传递中计算的目标概念的梯度用于产生突出显示输入中用于预测目标概念的重要区域的地图。这通常应用于计算机视觉环境中。
Grad-CAM是一种基于梯度的方法，用于生成视觉字幕。基于输出标题，该方法确定输入图像中的哪些区域是重要的。
- 基于注意的方法通常与顺序数据（例如文本数据）一起使用。除了网络的正常权重之外，还训练注意力量作为“输入门”。这些注意力量决定了最终网络输出中每个不同元素的数量。除了可解释性之外，在例如基于文本的问答的背景下的关注也导致更好的结果，因为网络能够“集中”其注意力。
在回答问题时，可以指出文本中哪些词最重要，以确定问题的答案。
Lime
Lime是一个更通用的框架，旨在使“任何”机器学习模型的预测更易于理解。
为了保持与模型无关，LIME通过在本地修改模型的输入来工作。因此，不是试图同时理解整个模型，而是修改特定的输入实例，并监控对预测的影响。在文本分类的上下文中，这意味着例如替换了一些单词，以确定输入的哪些元素影响预测。
原文：https://towardsdatascience.com/interpretability-in-machine-learning-70c30694a05f