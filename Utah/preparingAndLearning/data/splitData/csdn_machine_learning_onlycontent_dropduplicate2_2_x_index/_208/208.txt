保研之后，时间整个人都放松了，节奏很慢，懒散了几天。还是决定要学点东西，之前学过机器学习课程，但是没有认真听，这段时间刚好可以看看，做做笔记。教材是周志华老师的《机器学习》。
一、奥卡姆剃刀（Occam’s razor）
奥卡姆剃刀原则主张选择与经验观察一致的最简单假设，是一种常用的、自然科学研究中最基本的原则，即“若有多个假设与观察一直，则选最简单的那个”。
举个例子。假如有一些连续点，可以用二次或更复杂的函数拟合，那么就用二次函数来拟合。
问题是，怎么判断，哪一个假设更“简单”？ 这就要用其他机制来来解决了，这个问题也一直困扰者研究者们，因此，对奥卡姆剃刀在机器学习领域的作用，一直存在争议。
二、没有免费的午餐（No Free Lunch Theorem - 简称NLF定理）
通过奥卡姆剃刀，我们确定了选择更简单的假设a作为学习算法，但是由于训练集外的数据样本并不一定符合a，所以a不一定比另一个算法b更好。
从而引伸出，如果简单的学习算法a，它在某些问题上比算法b好，则必然存在另一些问题，b比a的性能要好。
有趣的是，经过数学证明（有兴趣可自行查阅），这个结论对任何算法都成立。
也就是说，无论学习算法a有多聪明，b有多笨拙，他们的期望性能是相同的。这就是NLF定理。
幸运的是，这有一个前提，就是所有问题出现的机会相同，或者所有问题同样重要，才会性能相同。
然而实际情形并不是这样。我们一般要解决的问题都是某个具体任务，不管这个解决方案在其他问题上的性能。
所以NLF定理，让我们清楚认识到，脱离具体问题谈论什么“学习算法更好”是毫无意义的，一个算法无法在所有问题上都表现良好。