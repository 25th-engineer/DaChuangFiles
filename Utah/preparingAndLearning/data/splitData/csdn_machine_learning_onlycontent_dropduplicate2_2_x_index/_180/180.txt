今天上B站的时候突然间看到居然有阿婆主上传了台大李弘毅教授的机器学习课程的视频，点开进去看了一下，发现这个老师真的是挺逗的，对于我这个初学者来说，课讲的也很棒（课程主页）。所以打算在这里做一个课程的学习记录，以激励自己坚持学习下去。这里先做一下introduction的记录。
0.开篇
其实我觉得这里老师对机器学习的比喻特别形象，机器学习简单来说就是一个三部曲：首先用一个初始化的函数去拟合数据、接下来不断地优化这个函数、最后得到一个最优的函数。
其实就跟把大象放进冰箱一样，说起来简单，做起来就难啦，2333~
1.Learning Map
老师在这里归纳了一个机器学习的导图，其实这张图已经把大部分的机器学习涉及到的东西概括进去了。
首先，蓝色的框框表示不同的机器学习技术场景，分为：Supervised Learning（监督学习）、Semi-supervised Learing（半监督学习）、Transfer Learning（迁移学习）、Unsupervised Learning（无监督学习）和Reinforcement Learning（增强学习）。我们从图中可以看见，Supervised Learning几乎占据了机器学习场景的半壁江山。这可以说明其实在机器学习中，Supervised Learning是最常用的手段之一，但是由于Supervised Learning需要话费大量的时间和人力成本在数据label的标注上（用label来告诉机器这一条数据的是什么，也就是说像老师一样监督着机器），在数据量很大的情况下，通常是不现实的，所以，衍生了一些其他的学习方法。
红色的框框表示一些任务场景，基本可以分为以下三类：Regression（回归）、Classification（分类）以及Structured Learing（结构学习？）。对于这个Structured Learing，其实可以看做它既不是Regression问题也不是Classification问题，我们等会会详细说明。
绿色的框框表示一些常用的方法（模型），这就有很多了，并且还在不断地更新换代，比较经典的有：Linear Model（线性模型）、Deep Learning（深度学习模型）、SVM（支持向量机）、decision tree（决策树）、KNN（K近邻）等等等等，这些在以后的课程中老师都会介绍。
我们只要知道，我们要用机器学习去解决一个问题的时候，首先是要决定用哪个技术场景去解决（蓝色框框）、其次是我们的问题适用于什么任务场景（红色框框）、最后才是我们模型的选择（绿色框框）。
为了能更直观的理解这个过程，我自己画了一幅思维导图，这个图里边，所有的红色和绿色的分支都适用于所有蓝色的分支，在这里为了方便我只画了其中一个分支：
2.Regression回归
首先我们来看回归问题：
Regression（回归）问题是机器学习中的一大类问题，它的主要特点是函数的输出结果是一个连续的值（scalar）。
这里以预测PM2.5的值为例，我们都知道，PM2.5的值是一个介于0-500之间的连续的数，所以对PM2.5的预测是一个典型的回归问题。
总之，回归问题就是用来解决一些输出是连续的数的机器学习问题的，它和我们接下来要讲的分类问题有着很明显的区分。
3.Classification分类
Classification（分类）问题是机器学习中的另一大类问题，与回归不同的是，它是用于解决输出结果为离散值的问题的。
分类问题又可以分为Binary Classification（二分类问题）和Multi-class Classification（多分类问题），二分类问题的输出只有两个结果：yes or no(1或者0)，多分类则可以输出任意多的分类结果。有时候，同一个问题，我们可以看做二分类问题，也可以看做多分类问题，比如：
我们的期末考试，如果我们将成绩分为：
及格/不及格
那很明显，这是一个二分类问题；如果我们将成绩分为：
优、良、中、差
那这就是一个多分类问题了。所以，我们在解决实际问题的时候，可以多动脑经想想，我这个问题是用二分类来实现比较好，还是多分类来实现比较好。
这里老师也针对二分类和多分类问题举了两个典型的例子：
垃圾邮件的判断是典型的二分类问题，如果系统判断这封邮件是一封垃圾邮件，则可以把它放入垃圾邮件箱，以增加用户的使用体验。但是我们也可以拓展一下思维，如果把邮件按照紧急程度分为：非常紧急、紧急、普通，或者把邮件按照类型分为：工作类邮件、普通社交类邮件、家庭类邮件，那么这就变成了一个多分类问题了。
新闻报道的分类是一个典型的多分类问题，我们在浏览新闻客户端的时候可以发现，它已经帮我们给新闻分好种类了，这样可以方便用户根据自己的兴趣爱好进行浏览。但是，我们也可以根据地域，将新闻报道分为国内新闻和国外新闻，那这就成为了一个二分类问题了。
所以，还是那句话，在面对分类问题的时候，我们可以多思考，用什么方式去解决我们的问题是最优的。
4 .Deep Learning（深度学习）
深度学习是最近很火的一个概念，它其实是一个机器学习模型，可以用来解决我们的回归问题、分类问题以及结构学习问题。与其他模型不同的是，深度学习拥有比其它模型更为复杂的结构（神经网络），这能让它更接近人脑思考问题的方式。具体的深度学习的内容老师在之后的课程中会介绍，这里老师只是稍微的介绍了一下深度学习。
这里老师给出的例子是根据输入的图片对图片中的动物进行分类，是一个将深度学习应用在多分类问题的例子。
5. Semi-supervised Learning和Transfer Learning半监督学习和迁移学习
半监督学习就是我们有一个数据集，这个数据集包括了猫和够的照片。但是，这个数据集中只有很少一部分的照片被打上了猫和狗的标签（label），而要手动对其余没有标签的图片打上标签对成本要求太高了，这时候我们就可以用到Semi-supervised Learning（半监督学习），就是用一些机器学习的方法（比如KNN），为没有标签的图片打上标签。
迁移学习就是在上面的情景中，我有一部分其他的已经打上标签的动物或者人物的图片，而我可以用对这些其他的分类问题上面所学习到的分类规则，应用于我对猫和狗的分类问题中来。
6.Unsupervised Learning无监督学习
无监督学习就是我们给机器的数据都是没有标签的，机器通过自己的方法来对我们输入的数据进行分类，典型的有聚类模型。
比如，我输入了一堆猫和狗的照片给机器，而并没有告诉它哪些是猫哪些是够。但是聪明的机器能根据猫和狗之间的差异，把属于猫的图片放在一堆，狗的图片放在一堆。也就是说，机器虽然不知道这些是猫和狗，但是它知道这些两堆图片是不一样的，而这对于我们这个任务来说，也就足够了。
7. Structured Learning
其实我不知道应该怎么翻译这个Structured Learning ，Structured Learning 指的应该是既不是分类也不是回归的一类问题。
典型的Structured Learning就是语音识别和机器翻译问题。因为这些问题的输出既不是一个连续的数值，也不是一些离散的分类点可以概括的。对于Structured Learning老师在后续的课程也会介绍，等到那时我再学习一下。
8.Reinforcement Learning增强学习
增强学习也是一个很厉害的家伙，它与监督学习的区别在于，它不是通过标签，而是通过评分系统来确定自己的好坏的。
比如，一个人机对话机器，如果用户突然发怒了（一个非常低的评分），那么它就会反思自己什么地方说错话了？
我们熟知的alphaGO，使用的是监督学习结合增强学习的模型，首先给它输入大量的棋谱，告诉它通常情况下应该怎么下棋。然而，再多的棋谱也不能覆盖棋盘上的所有情况，所以，alpha使用了增强学习，通过自己和自己下棋的方式，来进一步学习一些棋谱上学不到的东西，而它的评分当然就是这一盘棋是否下赢了，如果输了，那么它就会反思是不是哪一手棋下错了，如果赢了，那么它就会告诉自己，恩刚才下的不错，以后还要这样下才能赢。所以我觉得这也就能解释为什么alphaGO在与人类对弈的时候会下出一些从来没有见过的“神来之手”了，那是因为它就是这样下赢它自己的呀。
第一节introduction的内容大概就是那么多了，老师主要介绍了一下机器学习领域常见的任务分类以及一些模型。作者也是刚刚开始学习这方面的内容，所以有什么地方说的不对的，还希望各位多多给我指出来，谢谢。