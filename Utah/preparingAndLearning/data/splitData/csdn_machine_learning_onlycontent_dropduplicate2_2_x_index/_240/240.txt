回归分析研究的是客观事物变量间的统计关系，通过在对客观事物大量实验和观测的基础上，找到隐藏在不确定现象中的规律性的统计方法。这一方法从高斯提出的最小二乘法算起已经有200年的历史，一直在被各种学科广泛的应用中。
机器学习的回归思想也是通过丢给机器学习和观察数据来找到杂乱数据间隐藏的规律，通过建模和算法使得规律得到的结果不仅与真实的结果越逼近越好，而且在新的数据上也有很好的预测准确性，也就是好的泛化能力。首先从误差理论说起：
一、机器学习中的误差理论
模型和学习算法的泛化性能好坏的评价标准是泛化误差（泛化错误率），首先对范化误差进行计算和拆解：
几个参量描述如下：
测试样本：x
x在数据集上的标记：yD
x的真实标记：y
训练集D上的模型f在x上的预测输出：f（x；D）
学习算法的期望预测：ED[f（x；D）]
样本数相同的不同训练集产生的方差：var（x）= ED[(f（x；D）-ED[f（x；D）])^2]，表征数据扰动造成的影响
噪声：epsilon=ED[(yD-Y）^2]，表征学习本身的难度
期望输出和真实标记的差别，即方差：Bias^2（x）=(ED[f（x；D）]-y)^2，表征学习本身的拟合能力
则泛化误差表示为E（f，D）=var（x）+Bias^2（x）+epsilon^2
由上述的理论推导可以看出机器学习的泛化能力是有数据，算法和学习本身的难度决定的。数据的充分性，学习算法好的拟合能力以及学习本身的难易程度都会影响最终的输出结果的准确性。
一般偏差和方差是冲突的，在不考虑epsilon的情况下（即epsilon=0），偏差是训练不足，学习器的拟合能力较弱，数据的扰动能力弱；而方差是训练过度，学习器过拟合，数据的扰动都被学习器学到，如下图所示，真正的训练是在偏差和方差之间的tradeoff.
二、机器学习线性回归违背基本假设的情况
2.1线性 回归模型的假设：
1、y=wx+error，y和error都是相互独立的随机变量，x是确定性的变量，x的值是可以精确控制和测量的
2、随机误差error满足：
则对于n的样本，有
则对于所有的样本，回归模型的基本假设假定随机误差项具有相同的方差，独立或者不相关，即：
2.2 基本假设中经常出现违背的情况有以下三种：
1.随机误差项（noise项）的异方差性，数学表达为：
原因：某一或某些因素随解释变量观测值变化而对解释变量产生不同的影响，即由于数据不同，数据的扰动而造成随机误差项产生的方差不一样
2.随机误差项（noise项）的自相关性，数学表达为：
原因：随机误差项由于关键变量、使用错误的回归函数或者对数据加工整理而使其产生序列的自相关性，自相关是指的变量前后期数值之间的相互关系。
3.自变量之间不是完全独立的，存在多重共线性，数学表达为：
则称x1，x2,…,xp之间存在多重共线性.
原因：因变量之间完全不相关，独立的情况在现实生活中是非常少见，多重共线性使得行列式|X'X|~0，,也就是说验证多重共线性只要看X'X至少有一个特征根近似为0.
二、机器学习中解决违背线性回归基本假设的方法
在机器学习中，随机误差项（noise）是没有做任何处理的，真实数据中的误差随机可能违背了基于回归模型的假设，这也是机器学习受分歧的原因。但是对于消除多重共线性，在机器学习中有如下几种方法：
1.PCA主成分分析法，思想是将高维数据投射到低维正交基上，使得数据是正交不相关的
2.lasso回归法（L2惩罚法），思想是当X之间存在多重共线性时，行列式|X'X|~0，那么在X'X上加上一个正常数矩阵kI，（k>0），则X'X+kI接近奇异(奇异是指|A|=0)的程度会比X'X小得多.即机器学习中的L2 penalty.
总结：机器学习中基于线性回归以及在线性回归基础上发展的算法都和回归模型的基本假设不一致，这也是为什么每一种机器学习算法在使用前都会存在一定的数学假设，如线性回归的数据线性相关，朴素贝叶斯的样本间完全独立的前提假设等，在此前提下机器学习才是work的，但是基于假设的前提，这也是的机器学习在预测上是不准确有偏差的，这也是为什么收集更多更好的数据是机器学习关键和重要的第一步，如误差理论中泛化误差是数据，算法和学习本身的难度综合影响的结果。