机器学习之监督学习-回归
一、机器学习算法分类
有监督学习：
分类
回归
半监督学习：
分类
回归
无监督学习：
聚类
降维
强化学习：
马尔可夫决策过程
动态规划
参考网址：http://qing0991.blog.51cto.com/1640542/1851981
二、线性回归
一个案例：对连续型数据做出的预测属于回归问题。例如人们买房的时候，在知道房屋面积
X
1
X_1
X1 和卧室的数量
X
2
X_2
X2 的情况下，怎么推测得知房屋的价格
Y
Y
Y 呢。通过一组
X
1
X_1
X1 、
X
2
X_2
X2 、
Y
Y
Y 的实际数据，我们可以得到一个这样的关系：
Y
=
θ
0
+
θ
1
X
1
+
θ
2
X
2
Y=θ_0+θ_1X_1+θ_2X_2
Y=θ0 +θ1 X1 +θ2 X2
类似这种问题很多，比如已知一个人的年龄
X
1
X_1
X1 和体重
X
2
X_2
X2 ，推测人的身高
Y
Y
Y 。这都是线性回归问题，本质是拟合多组数据到一个函数上。
参考网址：http://lib.csdn.net/article/machinelearning/2975
线性回归（linear regression）
输入特征（input features）：
x
(
i
)
x^{(i)}
x(i)
输出（output）：
y
(
i
)
y^{(i)}
y(i) （取值连续）
模型参数（model parameters）：
θ
θ
θ
假设函数（hypothesis function）：
h
θ
(
x
)
=
x
T
θ
=
∑
i
=
1
n
x
i
θ
i
h_θ(x)=x^{T}θ=\sum_{i=1}^{n}x_iθ_i
hθ (x)=xTθ=∑i=1n xi θi
损失函数（squared loss function to be minimized）：
l
(
h
θ
(
x
)
,
y
)
=
(
h
θ
(
x
)
−
y
)
2
l(h_θ(x),y)=(h_θ(x)-y)^{2}
l(hθ (x),y)=(hθ (x)−y)2
注：输入的
y
y
y 和
h
(
x
)
h(x)
h(x) 之间满足方程
y
=
h
(
x
)
+
e
y=h(x)+e
y=h(x)+e。
e
e
e 是误差项（噪音项），假设
e
e
e 是独立同分布 iid（independent and identity distribution）和均值为0，方差为某一定数的高斯分布。
线性回归的目标是求出线性回归方程，即求出线性回归方程中的回归系数
θ
θ
θ。
参考网址：http://blog.csdn.net/tangyudi/article/details/77711981
二维空间内的线性回归非常简单。它就是寻找一条最优直线来对数据进行拟合。根据最小二乘原理，确定的准则：寻找一条直线，使得函数值与模型预测值之差的平方和最小。
多维空间内的线性回归就是寻找一条最优超平面来对数据进行拟合。根据最小二乘原理，确定的准则：超平面与分布数据的误差最小。
求解方法：
最大似然函数+最小二乘法
梯度下降
参考网址：http://blog.csdn.net/tangyudi/article/details/77769045
参考网址： http://blog.csdn.net/titan0427/article/details/50365480
##三、非线性回归
非线性回归（non-linear regression）：拟合曲线、非直线。有部分非线性回归可以转化为线性求解，这些模型称为广义线性模型，例如 logistic 回归。（非线性回归又称为逻辑回归，LR）
实际问题中，变量之间常常不是直线。解决方法通常是选择一条比较接近的曲线，通过变量替换把非线性方程加以线性化，然后按照线性回归的方法进行拟合。