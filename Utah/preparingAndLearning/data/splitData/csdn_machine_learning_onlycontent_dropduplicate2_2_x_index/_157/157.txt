人工智能就是机器学习和大数据；
机器学习是什么：就是算法模型；
算法模型是什么：
俗地说，模型就是机器学习采用的算法。“模型文件”一般说的是这个算法用到的各种输入、输出数据的值。
因为机器学习往往需要大量的运算，所以有必要将中间变量也存入文件中，以便可以多次地运算。
机器学习是一类算法的通称，具体到某个算法，那模型的差异就很大了，就算某一类算法（比如神经元网络），它的不同形态（CNN、RNN、DNN、LSTM）构成的模型也有很大的差异。
《统计学习方法》上来说，一个机器学习的三要素为模型、策略、算法，模型就是我们简单的理解，策略主要是是损失函数和正则化项，算法是指优化策略的算法
如果你数学很好，可以这么理解。模型就是一个函数y = f(x)，x是数据的各个特征，y是类别标签，模型就是把特征映射到类别的一个函数。
当然，这只是一个简单的模型，如线性模型的函数为y = ax+b，这里面只有一个特征x，多个类别y。
大多数模型的原理都是这简单的模型建立起来的；
模型文件就是写好的算法下次可以不断地复用，如同jar包 python中的模块差不多；
模型就是通过当前数据集得到一个复杂的多维函数，可以理解为 y = w1.x1+w2.x2+...+b 只是这个函数极为复杂，他的参数是要随之改变，而y就是我们的目标值，这个整体我们可以理解为一个策略或者一个函数，我们要做的就是优化w和b，使其每次y都跟真实的y无限接近。
运用机器学习算法进行研究，其实就是寻找目标函数的过程。通过构建机器学习模型（形成函数集），用训练数据做驱动，寻找与训练数据匹配，并且在测试数据中表现优异的函数。所以构建机器学习模型就显得十分的重要了。以线性回归为例子，大家可以看一下下面的图。
在寻找目标函数时，假如函数集范围太小，正如图左所示只是一次式项，那么很有可能目标函数不在函数集里面，也就说bias(偏差)比较大，远离了目标函数（也就是图中的靶心），这是我们经常说的欠拟合。而如果我们尽量把函数集设大一点（函数集尽可能的大就有希望能够包含目标函数），这样bias会变小，但是也带来了坏处，模型对噪音数据会特别敏感，一不小心就会出现过拟合的情况。因为我们本身并不知道目标函数到底长什么样，所以我们就要不断测试修改模型，希望能达到好的效果。下面是一点tips,大家可以参考一下。
欠拟合：1 增加特征 2 调整为更加复杂的模型
过拟合：1 增加数据量 2 正则化
上面说的是传统机器学习中模型的设计，那么在深度学习中，我们又该如何设计？首先大家要理解一点，为什么要“deep”？下面大家继续看图。
大家仔细看图就会发现，1.随着隐藏层的增加，错误率在减低。2. 7X2K 对应的错误率是17.1% ，而与之参数量相当的1X16K 对应的错误率却是22.1% 。 理论上说，只要一层隐藏层里面神经元够多，那么这个模型足以接近任何函数。也就是说，我们没必要把神经网络弄deep，但是大家看图就会发现，deep 要比 一层效果好。其实这就包含了拆分与共享的思想，看图。
原来我也觉得深度学习应该会需要大量的数据，事实上并不是，我使用mnist数做过实验，在数据量较少时，多层神经网络的效果要比单层神经网络要好，意不意外？惊不惊喜？所以当你数据较少又想用深度学习来处理数据时，不妨多搭几层。
最后还要谈一下 no free lunch理论，no free lunch理论指的是没有最好的算法，只有最适合的算法。深度学习的模型有许多种，大家在选择是一定要根据自己问题来选择模型，比如说CNN对处理图像信息就非常的有用，而RNN对处理序列非常在行。所以大家还是要对每种算法都要了解清楚，选择合适自己的算法。
---------------------
作者：ml_lsc
来源：CSDN
原文：https://blog.csdn.net/u011421866/article/details/73292141
版权声明：本文为博主原创文章，转载请附上博文链接！
一、机器学习的发展背景：人工智能
人工智能（Artificial Intelligence，缩写为AI）是对人的意识、思维过程进行模拟的一门新学科。如今，人工智能从虚无缥缈的科学幻想变成了现实。计算机科学家们在人工智能的技术核心－－机器学习（Machine Learning）和深度学习（Deep Learning）领域上已经取得重大的突破，机器被赋予强大的认知和预测能力。回顾历史，在1997年，IBM“深蓝”战胜国际象棋冠军卡斯帕罗夫；在2011年，具备机器学习能力的IBM Waston参加综艺节目赢得100万美金；在2016年，利用深度学习训练的Aplphago成功击败人类世界冠军。种种事件表明机器也可以像人类一样思考，甚至比人类做得更好。
目前，人工智能在金融、医疗、制造等行业得到了广泛应用，全球投资从2012年的5．89亿美元猛增至2016年50多亿美元。麦肯锡预计，到2025年人工智能应用市场的总值将达到1270亿美元。与此同时，麦肯锡通过对2016年人工智能市场的投资进行深入分析，发现有将近60％的资金并购围绕机器学习来布局。其中，基于软件的机器学习初创公司比基于机器的机器人公司更受投资欢迎。从2013 年到2016 年，这一领域的投资复合年均增长率达到约80％。由此可见，机器学习已经成为目前人工智能技术发展的主要方向。
二、机器学习与人工智能、深度学习的关系
在介绍机器学习之前，先需要对人工智能、机器学习和深度学习三者之间的关系进行梳理。目前业界最常见的划分是：
人工智能是使用与传统计算机系统完全不同的工作模式，它可以依据通用的学习策略，读取海量的“大数据”，并从中发现规律、联系和洞见，因此人工智能能够根据新数据自动调整，而无需重设程序。
机器学习是人工智能研究的核心技术，在大数据的支撑下，通过各种算法让机器对数据进行深层次的统计分析以进行“自学”；利用机器学习，人工智能系统获得了归纳推理和决策能力；而深度学习更将这一能力推向了更高的层次。
深度学习则是机器学习算法的一种，隶属于人工神经网络体系，现在很多应用领域中性能最佳的机器学习都是基于模仿人类大脑结构的神经网络设计而来的，这些计算机系统能够完全自主地学习、发现并应用规则。相比较其他方法，在解决更复杂的问题上表现更优异，深度学习是可以帮助机器实现“独立思考”的一种方式。
总而言之，人工智能是社会发展的重要推动力，而机器学习，尤其是深度学习技术就是人工智能发展的核心，它们三者之间是包含与被包含的关系。如所示。

三、机器学习：实现人工智能的高效方法
从广义上来说，机器学习是一种能够赋予机器学习的能力以此让它完成直接编程无法完成的功能的方法。但从实践的意义上来说，机器学习是一种通过利用数据，训练出模型，然后使用模型预测的一种方法。国外有些学者对机器学习进行了定义大同小异，有学者认为，机器学习是对能通过经验自动改进的计算机算法的研究；也有学者认为，机器学习是指利用数据或以往的经验，以此优化计算机程序的性能标准。由此可知，机器学习是通过经验或数据来改进算法的研究，通过算法让机器从大量历史数据中学习规律，得到某种模式并利用此模型预测未来，机器在学习的过程中，处理的数据越多，预测结果就越精准。
机器学习在人工智能的研究中具有十分重要的地位。它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域。从20世纪50年代人们就开始了对机器学习的研究，从最初的基于神经元模型以及函数逼近论的方法研究，到以符号演算为基础的规则学习和决策树学习的产生，以及之后的认知心理学中归纳、解释、类比等概念的引入，至最新的计算学习理论和统计学习的兴起，机器学习一直都在相关学科的实践应用中起着主导作用。现在已取得了不少成就，并分化出许多研究方向，主要有符号学习、连接学习和统计学习等。
（一）机器学习的发展历程
机器学习发展过程大体可分为以下四个阶段：
1．50年代中叶到60年代中叶
在这个时期，所研究的是“没有知识”的学习，即“无知”学习；其研究目标是各类自组织系统和自适应系统；其主要研究方法是不断修改系统的控制参数以改进系统的执行能力，不涉及与具体任务有关的知识。指导本阶段研究的理论基础是早在40年代就开始研究的神经网络模型。随着电子计算机的产生和发展，机器学习的实现才成为可能。这个阶段的研究导致了模式识别这门新科学的诞生，同时形成了机器学习的二种重要方法，即判别函数法和进化学习。塞缪尔的下棋程序就是使用判别函数法的典型例子。不过，这种脱离知识的感知型学习系统具有很大的局限性。无论是神经模型、进化学习或是判别函数法，所取得的学习结果都很有限，远不能满足人们对机器学习系统的期望。在这个时期，我国研制了数字识别学习机。
2．60年代中叶至70年代中叶
本阶段的研究目标是模拟人类的概念学习过程，并采用逻辑结构或图结构作为机器内部描述。机器能够采用符号来描述概念（符号概念获取），并提出关于学习概念的各种假设。本阶段的代表性工作有温斯顿（Winston）的结构学习系统和海斯·罗思（Hayes Roth）等的基于逻辑的归纳学习系统。虽然这类学习系统取得较大的成功，但只能学习单一概念，而且未能投入实际应用。此外，神经网络学习机因理论缺陷未能达到预期效果而转入低潮。因此，使那些对机器学习的进展抱过大希望的人们感到失望。他们称这个时期为“黑暗时期”。
3．70年代中叶至80年代中叶
在这个时期，人们从学习单个概念扩展到学习多个概念，探索不同的学习策略和各种学习方法。机器的学习过程一般都建立在大规模的知识库上，实现知识强化学习。尤其令人鼓舞的是，本阶段已开始把学习系统与各种应用结合起来，并取得很大的成功，促进机器学习的发展。在出现第一个专家学习系统之后，示例归约学习系统成为研究主流，自动知识获取成为机器学习的应用研究目标。1980年，在美国的卡内基梅隆大学（CMU）召开了第一届机器学习国际研讨会，标志着机器学习研究已在全世界兴起。此后，机器归纳学习进入应用。1986年，国际杂志《机器学习》（Machine Learning）创刊，迎来了机器学习蓬勃发展的新时期。70年代末，中国科学院自动化研究所进行质谱分析和模式文法推断研究，表明我国的机器学习研究得到恢复。1980年西蒙来华传播机器学习的火种后，我国的机器学习研究出现了新局面。
4．机器学习最新阶段始于1986年
一方面，由于神经网络研究的重新兴起，对连接机制学习方法的研究方兴未艾，机器学习的研究已经在全世界范围内出现新的高潮，机器学习的基本理论和综合系统的研究得到加强和发展。另一方面，对实验研究和应用研究得到前所未有的重视，机器学习有了更强的研究手段和环境。从而出现了符号学习、神经网络学习、进化学习和基于行为主义（actionism）的强化学习等百家争鸣的局面。
 机器学习的发展历程
（二）机器学习的结构模型
机器学习的本质就是算法。算法是用于解决问题的一系列指令。程序员开发的用于指导计算机进行新任务的算法是我们今天看到的先进数字世界的基础。计算机算法根据某些指令和规则，将大量数据组织到信息和服务中。机器学习向计算机发出指令，允许计算机从数据中学习，而不需要程序员做出新的分步指令。
机器学习的基本过程是给学习算法提供训练数据。然后，学习算法基于数据的推论生成一组新的规则。这本质上就是生成一种新的算法，称之为机器学习模型。通过使用不同的训练数据，相同的学习算法可以生成不同的模型。从数据中推理出新的指令是机器学习的核心优势。它还突出了数据的关键作用：用于训练算法的可用数据越多，算法学习到的就越多。事实上，AI 的许多最新进展并不是由于学习算法的激进创新，而是现在积累了大量的可用数据。
 机器学习的结构模型
（三）机器学习的工作方式
1．选择数据：首先将原始数据分成三组：训练数据、验证数据和测试数据；
2．数据建模：再使用训练数据来构建使用相关特征的模型；
3．验证模型：使用验证数据输入到已经构建的数据模型中；
4．测试模型：使用测试数据检查被验证的模型的性能表现；
5．使用模型：使用完全训练好的模型在新数据上做预测；
6．调优模型：使用更多数据、不同的特征或调整过的参数来提升算法的性能表现。

（四）机器学习发展的关键基石：
（1）海量数据：人工智能的能量来源是稳定的数据流。机器学习可以通过海量数据来“训练” 自己，才能开发新规则来完成日益复杂的任务。目前全球有超过30亿人在线，约170 亿个连接的设备或传感器，产生了大量数据，而数据存储成本的降低，使得这些数据易于被使用。
（2）超强计算：强大的计算机和通过互联网连接远程处理能力使可以处理海量数据的机器学习技术成为可能，具某媒体称，ALPHGO之所以能在与对李世石的对决中取得历史性的胜利，这与它硬件配置的1920个CPU和280个GPU超强运算系统密不可分，可见计算能力对于机器学习是至关重要的。
（3）优秀算法：在机器学习中，学习算法（learning algorithms）创建了规则，允许计算机从数据中学习，从而推论出新的指令（算法模型），这也是机器学习的核心优势。新的机器学习技术，特别是分层神经网络，也被称为“深度学习”，启发了新的服务，刺激了对人工智能这一领域其他方面的投资和研究。
 机器学习的关键基石
（五）机器学习的算法分类
机器学习基于学习形式的不同通常可分为三类：
1．监督学习（Supervised Learning）    给学习算法提供标记的数据和所需的输出，对于每一个输入，学习者都被提供了一个回应的目标。监督学习主要被应用于快速高效地教熟AI现有的知识，被用于解决分类和回归的问题。常见的算法有：
（1）决策树（Decision Trees）：决策树可看作一个树状预测模型，它通过把实例从根节点排列到某个叶子节点来分类实例，叶子节点即为实例所属的分类。决策树的核心问题是选择分裂属性和决策树的剪枝。决策树是一个决策支持工具，它用树形的图或者模型表示决策及其可能的后果，包括随机事件的影响、资源消耗、以及用途。用于分析判断有无贷款意向的决策树示如图 所示，从商业角度看，常用于基于规则的信用评估、赛马结果预测等。

（2）Adaboost算法：这是一种迭代算法，其核心思想是针对同一个训练集训练不同的分类器（弱分类器），然后把这些弱分类器集合起来，构成一个更强的最终分类器（强分类器）。算法本身是改变数据分布实现的，它根据每次训练集之中的每个样本的分类是否正确，以及上次的总体分类的准确率，来确定每个样本的权值。将修改权值的新数据送给下层分类器进行训练，然后将每次训练得到的分类器融合起来，作为最后的决策分类器。AdaBoost算法主要解决了：两类问题、多类单标签问题、多类多标签问题、大类单标签问题和回归问题； 优点：学习精度明显增加，并且不会出现过拟合问题，AdaBoost算法技术常用于人脸识别和目标跟踪领域。
Adaboost
（3）人工神经网络（Artificial Neural Network －ANN）算法：人工神经网络是由大量处理单元互联组成的非线性、自适应信息处理系统。它是在现代神经科学研究成果的基础上提出的，试图通过模拟大脑神经网络处理、记忆信息的方式进行信息处理。人工神经网络是并行分布式系统，采用了与传统人工智能和信息处理技术完全不同的机理，克服了传统的基于逻辑符号的人工智能在处理直觉、非结构化信息方面的缺陷，具有自适应、自组织和实时学习的特点。

（4）SVM（Support Vector Machine）：SVM 法即支持向量机算法，由Vapnik等人于1995年提出，具有相对优良的性能指标。该方法是建立在统计学习理论基础上的机器学习方法。　SVM是一种二分算法。假设在N维空间，有一组点，包含两种类型，SVM生成a（N－1） 维的超平面，把这些点分成两组。比如你有一些点在纸上面，这些点是线性分离的。SVM会找到一个直线，把这些点分成两类，并且会尽可能远离这些点。从规模看来，SVM（包括适当调整过的）解决的一些特大的问题有：广告、人类基因剪接位点识别、基于图片的性别检测、大规模图片分类，适用于新闻分类、手写识别等应用。

（5）朴素贝叶斯（Naive Bayesian）：贝叶斯法是一种在已知先验概率与类条件概率的情况下的模式分类方法，待分样本的分类结果取决于各类域中样本的全体。朴素贝叶斯分类器基于把贝叶斯定理运用在特征之间关系的强独立性假设上。优点：在数据较少的情况下仍然有效，可以处理多类别问题。缺点：对于输入数据的准备方式较为敏感。适用数据类型：标称型数据。现实生活中的应用例子：电子邮件垃圾副过滤、判定文章属性分类、分析文字表达的内容含义和人脸识别、情感分析、消费者分类。
 朴素贝叶斯算法
（6）K－近邻（k－Nearest Neighbors，KNN）：这是一种分类算法，其核心思想是如果一个样本在特征空间中的k个最相邻的样本中的大多数属于某一个类别，则该样本也属于这个类别，并具有这个类别上样本的特性。该方法在确定分类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。 kNN方法在类别决策时，只与极少量的相邻样本有关。由于kNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，kNN方法较其他方法更为适合。
 K－近邻算法
（7）逻辑回归（Logistic Regression）：这是一种分类算法，主要用于二分类问题。逻辑回归是一种非常强大的统计方法，可以把有一个或者多个解释变量的数据，建立为二项式类型的模型，通过用累积逻辑分布的逻辑函数估计概率，测量分类因变量和一个或多个独立变量之间的关系。逻辑回归是一种非线性回归模型，相比于线性回归，它多了一个sigmoid函数（或称为Logistic函数）。通常，回归在现实生活中的用途如下：信用评估、测量市场营销的成功度、预测某个产品的收益、特定的某天是否会发生地震，路面交通流量分析、邮件过滤。
 逻辑回归算法
（8）随机森林算法（Random Forest）：随机森林算法可以用于处理回归、分类、聚类以及生存分析等问题，当用于分类或回归问题时，它的主要思想是通过自助法重采样，生成很多个树回归器或分类器。在机器学习中，随机森林是一个包含多个决策树的分类器， 并且其输出的类别是由个别树输出的类别的众数而定，常用于用户流失分析、风险评估。
 随机森林算法
（9）线形回归（ Linear Regression）：这是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法，运用十分广泛。线性回归是回归分析中第一种经过严格研究并在实际应用中广泛使用的类型。这是因为线性依赖于其未知参数的模型比非线性依赖于其位置参数的模型更容易拟合，而且产生的估计的统计特性也更容易确定。
 线形回归算法
2．无监督学习（Unsupervised Learning）
给学习算法提供的数据是未标记的，并且要求算法识别输入数据中的模式，主要是建立一个模型，用其试着对输入的数据进行解释，并用于下次输入。现实情况下往往很多数据集都有大量的未标记样本，有标记的样本反而比较少。如果直接弃用，很大程度上会导致模型精度低。这种情况解决的思路往往是结合有标记的样本，通过估计的方法把未标记样本变为伪的有标记样本，所以无监督学习比监督学习更难掌握。主要用于解决聚类和降维问题，常见的算法有：
（1）聚类算法：把一组对象分组化的任务，使得在同一组的对象比起其它组的对象，它们彼此更加相似。常用的聚类算法包括：
①K－means算法：这是典型的基于原型的目标函数聚类方法的代表，它是数据点到原型的某种距离作为优化的目标函数，利用函数求极值的方法得到迭代运算的调整规则。其优点是算法足够快速、简单，并且如果预处理数据和特征工程十分有效，那么该聚类算法将拥有极高的灵活性。缺点是该算法需要指定集群的数量，而K值的选择通常都不是那么容易确定的。另外，如果训练数据中的真实集群并不是类球状的，那么K均值聚类会得出一些比较差的集群。
 K－means算法
②Expectation Maximisation （EM）：这是一种迭代算法，用于含有隐变量（latent variable）的概率参数模型的最大似然估计或极大后验概率估计。EM算法的主要目的是提供一个简单的迭代算法计算后验密度函数，它的最大优点是简单和稳定，但容易陷入局部最优。
 EM算法
③Affinity Propagation 聚类：AP 聚类算法是一种相对较新的聚类算法，该聚类算法基于两个样本点之间的图形距离（graph distances）确定集群。采用该聚类方法的集群拥有更小和不相等的大小。优点：该算法不需要指出明确的集群数量。缺点：AP 聚类算法主要的缺点就是训练速度比较慢，并需要大量内存，因此也就很难扩展到大数据集中。另外，该算法同样假定潜在的集群是类球状的。
④层次聚类（Hierarchical Clustering）：层次聚类是一系列基于以下概念的聚类算法：是通过对数据集按照某种方法进行层次分解，直到满足某种条件为止。按照分类原理的不同，可以分为凝聚和分裂两种方法。优点：层次聚类最主要的优点是集群不再需要假设为类球形。另外其也可以扩展到大数据集。缺点：有点像 K 均值聚类，该算法需要设定集群的数量。
 层次聚类算法
⑤DBSCAN：这是一个比较有代表性的基于密度的聚类算法。与划分和层次聚类方法不同，它将簇定义为密度相连的点的最大集合，能够把具有足够高密度的区域划分为簇，并可在噪声的空间数据库中发现任意形状的聚类。它将样本点的密集区域组成一个集群。优点：DBSCAN 不需要假设集群为球状，并且它的性能是可扩展的。此外，它不需要每个点都被分配到一个集群中，这降低了集群的异常数据。缺点：用户必须要调整「epsilon」和「min＿sample」这两个定义了集群密度的超参数。DBSCAN 对这些超参数非常敏感。
DBSCAN算法
（2）降维算法：其主要特征是将数据从高维降低到低维层次，最大程度的保留了数据的信息。代表算法是：
①主要代表是主成分分析算法（PCA算法）：主成分分析也称主分量分析，旨在利用降维的思想，把多指标转化为少数几个综合指标（即主成分），其中每个主成分都能够反映原始变量的大部分信息，且所含信息互不重复。这种方法在引进多方面变量的同时将复杂因素归结为几个主成分，使问题简单化，同时得到的结果更加科学有效的数据信息。
 PCA算法
②局部线性嵌入（Locally linear embeddingLLE）LLE降维算法：一种非线性降维算法，它能够使降维后的数据较好地保持原有 流形结构 。该算法是针对非线性信号特征矢量维数的优化方法，这种维数优化并不是仅仅在数量上简单的约简，而是在保持原始数据性质不变的情况下，将高维空间的信号映射到低维空间上，即特征值的二次提取。
 LLE降维算法
3．强化学习（Reinforcement Learning）     该算法与动态环境相互作用，把环境的反馈作为输入，通过学习选择能达到其目标的最优动作。强化学习这一方法背后的数学原理与监督／非监督学习略有差异。监督／非监督学习更多地应用了统计学，而强化学习更多地结合了离散数学、随机过程这些数学方法。常见的算法有：
①TD（λ）算法：TD（temporal differenee）学习是强化学习技术中最主要的学习技术之一．TD学习是蒙特卡罗思想和动态规划思想的结合，即一方面TD算法在不需要系统模型情况下可以直接从agent经验中学习；另一方面TD算法和动态规划一样，利用估计的值函数进行迭代。
 TD（λ）算法
②Q＿learning算法：Q＿learning学习是一种模型无关的强化学习算法 ，又称为离策略TD学习（off－policy TD）．不同于TD算法，Q＿learning迭代时采用状态＿动作对的奖赏和Q （s，a）作为估计函数，在Agent每一次学习迭代时都需要考察每一个行为，可确保学习过程收敛。
  Q＿learning算法
（六）机器学习过程举例说明
所谓机器学习过程，是指观察有n个样本数据组成的集合，并根据这些数据来预测未知数据的性质，那么在给定数据集（所谓大数据）和具体问题的前提下，一般解决问题的步骤可以概括如下：
1．数据抽象
将数据集和具体问题抽象成数学语言，以恰当的数学符号表示。这样做自然是为了方便表述和求解问题，而且也更加直观。
2．设定性能度量指标
机器学习是产生模型的算法，一般来说模型都有误差。如果模型学的太好，把训练样本自身的一些特点当成所有潜在样本具有的一般性质，这种情况称为过拟合，这样的模型在面对新样本时就会出现较大误差，专业表述就是导致模型的泛化性能下降。与之相对的是欠拟合，模型对样本的一般性质都没学好，这种情况一般比较好解决，扩充数据集或者调整模型皆可。
3．数据预处理
之所以要做数据预处理，是因为提供的数据集往往很少是可以直接拿来用的。例如：如果样本的属性太多，一般有两种方法： 降维和特征选择。特征选择比较好理解，就是选择有用相关的属性，或者用另外一种表达方式：选择样本中有用、跟问题相关的特征。
4．选定模型
在数据集完美的情况下，接下来就是根据具体问题选定恰当的模型了。一种方式是根据有没有标记样本考虑。如果是有标记样本，可以考虑有监督学习，反之则是无监督学习，兼而有之就看半监督学习是否派的上用场。
5．训练及优化
选定了模型，如何训练和优化也是一个重要问题。如果要评估训练集和验证集的划分效果，常用的有留出法、交叉验证法、自助法、模型调参等如果模型计算时间太长，可以考虑剪枝如果是过拟合，则可通过引入正则化项来抑制（补偿原理）如果单个模型效果不佳，可以集成多个学习器通过一定策略结合，取长补短（集成学习）
6．机器学习举例分析
在机器学习领域特征比模型（学习算法）更重要。举个例子，如果我们的特征选得很好，可能我们用简单的规则就能判断出最终的结果，甚至不需要模型。比如，要判断一个人是男还是女，假设由计算机来完成这个任务，首先采集到各种数据（特征：姓名、身高、头发长度，籍贯、是否吸烟等等。因为根据统计我们知道男人一般比女人高，头发比女人短，并且会吸烟；所以这些特征都有一定的区分度，但是总有反例存在。我们用最好的算法可能准确率也达不到100％。然后再进行特征提取，提出对目标有意义的特征，删除无关的（籍贯），然后进行预处理，对特征提取结果的再加工，目的是增强特征的表示能力，防止模型（分类器）过于复杂和学习困难。接下来就是训练数据，这里我们通过监督学习或无监督的方法来拟合分类器模型。学习器通过分析数据的规律尝试拟合出这些数据和学习目标间的函数，使得定义在训练集上的总体误差尽可能的小，从而利用学得的函数来预测未知数据的学习方法预测出结果，最后对结果进行评价和改进。
（七）机器学习覆盖的范围
从范围上来说，机器学习跟模式识别，统计学习，数据挖掘是类似的，同时，机器学习与其他领域的处理技术的结合，形成了计算机视觉、语音识别、自然语言处理等交叉学科。因此，一般说数据挖掘时，可以等同于说机器学习。同时，我们平常所说的机器学习应用，应该是通用的，不仅仅局限在结构化数据，还有图像，音频等应用。
（1）模式识别
模式识别＝机器学习。两者的主要区别在于前者是从工业界发展起来的概念，后者则主要源自计算机学科。在著名的《Pattern Recognition And Machine Learning》这本书中，Christopher M． Bishop在开头是这样说的：“模式识别源自工业界，而机器学习来自于计算机学科。不过，它们中的活动可以被视为同一个领域的两个方面，同时在过去的十年间，它们都有了长足的发展”。
（2）数据挖掘
数据挖掘＝机器学习＋数据库。数据挖掘仅仅是一种方式，不是所有的数据都具有价值，数据挖掘思维方式才是关键，加上对数据深刻的认识，这样才可能从数据中导出模式指引业务的改善。大部分数据挖掘中的算法是机器学习的算法在数据库中的优化。
（3）统计学习
统计学习近似等于机器学习。统计学习是个与机器学习高度重叠的学科。因为机器学中的大多数方法来自统计学，甚至可以认为，统计学的发展促进机器学习的繁荣昌盛。例如著名的支持向量机算法，就是源自统计学科。两者的区别在于：统计学习者重点关注的是统计模型的发展与优化，偏数学，而机器学习者更关注的是能够解决问题，偏实践，因此机器学习研究者会重点研究学习算法在计算机上执行的效率与准确性的提升。
（4）计算机视觉
计算机视觉＝图像处理＋机器学习。图像处理技术用于将图像处理为适合进入机器学模型中的输入，机器学习则负责从图像中识别出相关的模式。计算机视觉相关的应用非常的多，例如百度识图、手写字符识别、车牌识别等等应用。这个领域将是未来研究的热门方向。随着机器学习的新领域深度学习的发展，大大促进了计算机图像识别的效果，因此未来计算机视觉界的发展前景不可估量。
（5）语音识别
语音识别＝语音处理＋机器学习。语音识别就是音频处理技术与机器学习的结合。语音识别技术一般不会单独使用，一般会结合自然语言处理的相关技术。目前的相关应用有苹果的语音助手siri等。
（6）自然语言处理
自然语言处理＝文本处理＋机器学习。自然语言处理技术主要是让机器理解人类的语言的一门领域。在自然语言处理技术中，大量使用了编译原理相关的技术，例如词法分析，语法分析等等，除此之外，在理解这个层面，则使用了语义理解，机器学习等技术。作为唯一由人类自身创造的符号，自然语言处理一直是机器学习界不断研究的方向。
  机器学习覆盖的范围
（八）机器学习在工业生产中的主要应用场景
机器学习作为人工智能的最有效的实现方法，已经在工业制造等众多场景中得到了广泛应用，以下是机器学习在工业生产中的五个应用场景。
1．代替肉眼检查作业，实现制造检查智能化和无人化
例如工程岩体的分类，目前主要是通过有经验的工程师通过仔细鉴别来判断，效率比较低，并且因人而异会产生不同的判断偏差。通过采用人工智能，把工程师的经验转化为深度学习算法，判断的准确率和人工判断相当，得到对应的权值后开发出APP，这样工程人员在使用平板拍照后，就可以通过APP自动得到工程岩体分类的结果，高效而且准确率高。
2．大幅改善工业机器人的作业性能，提升制造流程的自动化和无人化
工业上有许多需要分捡的作业，如果采用人工的作业，速度缓慢且成本高，而且还需要提供适宜的工作温度环境。如果采用工业机器人的话，可以大幅减低成本，提高速度。例如所示的Bin Picking机器人。
Bin Picking（零件分检）机器人
但是，一般需要分捡的零件并没有被整齐摆放，机器人虽然有摄像机看到零件，但却不知道如何把零件成功的捡起来。在这种情况下，利用机器学习，先让工业机器人随机的进行一次分捡动作，然后告诉它这次动作是成功分捡到零件还是抓空了，经过多次训练之后，机器人就会知道按照怎样的顺序来分捡才有更高的成功率，如所示。
如所示，经过机器学习后，机器人知道了分捡时夹圆柱的哪个位置会有更高的捡起成功率。
如表明通过机器学习后，机器人知道按照怎样的顺序分捡，成功率会更高，图中数字是分捡的先后次序。
如所示，经过8个小时的学习后，机器人的分捡成功率可以达到90％，和熟练工人的水平相当。
3．工业机器人异常的提前检知，从而有效避免机器故障带来的损失和影响
在制造流水线上，有大量的工业机器人。如果其中一个机器人出现了故障，当人感知到这个故障时，可能已经造成大量的不合格品，从而带来不小的损失。如果能在故障发生以前就检知的话，就可以有效得做出预防，减少损失。如中的工业机器人减速机，如果给它们配上传感器，并提前提取它们正常／不正常工作时的波形，电流等信息，用于训练机器学习系统，那么训练出来的模型就可以用来提前预警，实际数据表明，机器人会比人更早地预知到故障，从而降低损失。
  工业机器人故障预测
如所示，经过机器学习后，模型通过观测到的波形，可以检知到人很难感知到的细微变化，并在机器人彻底故障之前的数星期，就提出有效的预警。是利用机器学习来提前预警主轴的故障，一般情况下都是主轴出现问题后才被发现。
4．PCB电路板的辅助设计
任何一块印制板，都存在与其他结构件配合装配的问题，所以印制板的外形和尺寸必须以产品整机结构为依据，另外还需要考虑到生产工艺，层数方面也需要根据电路性能要求、板型尺寸和线路的密集程度而定。如果不是经验丰富的技术人员，很难设计出合适的多层板。利用机器学习，系统可以将技术人员的经验转化为模型，从而提升PCB设计的效率与成功率，如所示。
PCB板辅助设计
5．快速高效地找出符合3D模型的现实零件
例如工业上的3D模型设计完成后，需要根据3D模型中参数，寻找可对应的现实中的零件，用于制造实际的产品。利用机器学习来完成这个任务的话，可以快速，高匹配率地找出符合3D模型参数的那些现实零件。
是根据3D模型设计的参数，机器学习模型计算各个现实零件与这些参数的类似度，从而筛选出匹配的现实零件。没有使用机器学习时，筛选的匹配率大概是68％，也就是说，找出的现实零件中有1／3不能满足3D模型设计的参数，而使用机器学习后，匹配率高达96％。
（九）机器学习中的日常生活场景
1． 市民出行选乘公交预测
基于海量公交数据记录，希望挖掘市民在公共交通中的行为模式。以市民出行公交线路选乘预测为方向，期望通过分析公交线路的历史公交卡交易数据，挖掘固定人群在公共交通中的行为模式，分析推测乘客的出行习惯和偏好，从而建立模型预测人们在未来一周内将会搭乘哪些公交线路，为广大乘客提供信息对称、安全舒适的出行环境，用数据引领未来城市智慧出行。
2． 商品图片分类
电商网站含有数以百万计的商品图片，“拍照购”“找同款”等应用必须对用户提供的商品图片进行分类。同时，提取商品图像特征，可以提供给推荐、广告等系统，提高推荐／广告的效果。希望通过对图像数据进行学习，以达到对图像进行分类划分的目的。
3． 基于文本内容的垃圾短信识别
垃圾短信已日益成为困扰运营商和手机用户的难题，严重影响到人们正常生活、侵害到运营商的社会形象以及危害着社会稳定。而不法分子运用科技手段不断更新垃圾短信形式且传播途径非常广泛，传统的基于策略、关键词等过滤的效果有限，很多垃圾短信“逃脱”过滤，继续到达手机终端。希望基于短信文本内容，结合机器学习算法、大数据分析挖掘来智能地识别垃圾短信及其变种。
4． 国家电网客户用电异常行为分析
随着电力系统升级，智能电力设备的普及，国家电网公司可以实时收集海量的用户用电行为数据、电力设备监测数据，因此，国家电网公司希望通过大数据分析技术，科学的开展防窃电监测分析，以提高反窃电工作效率，降低窃电行为分析的时间及成本。
5．自动驾驶场景中的交通标志检测
在自动驾驶场景中，交通标志的检测和识别对行车周围环境的理解起着至关重要的作用。例如通过检测识别限速标志来控制当前车辆的速度等；另一方面，将交通标志嵌入到高精度地图中，对定位导航也起到关键的辅助作用。希望机遇完全真实场景下的图片数据用于训练和测试，训练能够实际应用在自动驾驶中的识别模型。
6．大数据精准营销中用户画像挖掘
在现代广告投放系统中，多层级成体系的用户画像构建算法是实现精准广告投放的基础技术之一。期望基于用户历史一个月的查询词与用户的人口属性标签（包括性别、年龄、学历）做为训练数据，通过机器学习、数据挖掘技术构建分类算法来对新增用户的人口属性进行判定。
7． 监控场景下的行人精细化识别
随着平安中国、平安城市的提出，视频监控被广泛应用于各种领域，这给维护社会治安带来了便捷；但同时也带来了一个问题，即海量的视频监控流使得发生突发事故后，需要耗费大量的人力物力去搜索有效信息。希望基于监控场景下多张带有标注信息的行人图像，在定位（头部、上身、下身、脚、帽子、包）的基础上研究行人精细化识别算法，自动识别出行人图像中行人的属性特征。
8．需求预测与仓储规划方案
拥有海量的买家和卖家交易数据的情况下，利用数据挖掘技术，我们能对未来的商品需求量进行准确地预测，从而帮助商家自动化很多供应链过程中的决策。这些以大数据驱动的供应链能够帮助商家大幅降低运营成本，更精确的需求预测，能够大大地优化运营成本，降低收货时效，提升整个社会的供应链物流效率，朝智能化的供应链平台方向更加迈进一步。高质量的商品需求预测是供应链管理的基础和核心功能。
9．股价走势预测
随着经济社会的发展，以及人们投资意识的增强，人们越来越多的参与到股票市场的经济活动中，股票投资也已经成为人们生活的一个重要组成部分。然而在股票市场中，众多的指标、众多的信息，很难找出对股价更为关键的因素；其次股市结构极为复杂，影响因素具有多样性、相关性。这导致了很难找出股市内在的模式。希望在尽可能全面的收集股市信息的基础上，建立股价预测模。
10．地震预报
根据历史全球大地震的时空图，找出与中国大陆大地震有关的14个相关区，对这些相关区逐一鉴别，选取较优的9个，再根据这9个相关区发生的大震来预测中国大陆在未来一年内会不会有大震发生。
11．穿衣搭配推荐
穿衣搭配是服饰鞋包导购中非常重要的课题，基于搭配专家和达人生成的搭配组合数据，百万级别的商品的文本和图像数据，以及用户的行为数据。期待能从以上行为、文本和图像数据中挖掘穿衣搭配模型，为用户提供个性化、优质的、专业的穿衣搭配方案，预测给定商品的搭配商品集合。
12．依据用户轨迹的商户精准营销
随着用户访问移动互联网的与日俱增，如何根据用户的画像对用户进行精准营销成为了很多互联网和非互联网企业的新发展方向。希望根据商户位置及分类数据、用户标签画像数据提取用户标签和商户分类的关联关系，然后根据用户在某一段时间内的位置数据，判断用户进入该商户地位范围300米内，则对用户推送符合该用户画像的商户位置和其他优惠信息。
13．气象关联分析
在社会经济生活中，不少行业，如农业、交通业、建筑业、旅游业、销售业、保险业等，无一例外与天气的变化息息相关。为了更深入地挖掘气象资源的价值，希望基于共计60年的中国地面历史气象数据，推动气象数据与其他各行各业数据的有效结合，寻求气象要素之间、以及气象与其它事物之间的相互关系，让气象数据发挥更多元化的价值。
14．交通事故成因分析
随着时代发展，便捷交通对社会产生巨大贡献的同时，各类交通事故也严重地影响了人们生命财产安全和社会经济发展。希望通过对事故类型、事故人员、事故车辆、事故天气、驾照信息、驾驶人员犯罪记录数据以及其他和交通事故有关的数据进行深度挖掘，形成交通事故成因分析方案。
15．基于兴趣的实时新闻推荐
随着近年来互联网的飞速发展，个性化推荐已成为各大主流网站的一项必不可少服务。提供各类新闻的门户网站是互联网上的传统服务，但是与当今蓬勃发展的电子商务网站相比，新闻的个性化推荐服务水平仍存在较大差距。希望通过对带有时间标记的用户浏览行为和新闻文本内容进行分析，挖掘用户的新闻浏览模式和变化规律，设计及时准确的推荐系统预测用户未来可能感兴趣的新闻。
四、深度学习：机器学习的更高智能进阶
1．深度学习的背景
2006年，加拿大多伦多大学教授、机器学习领域的泰斗Geoffrey Hinton和学生Salakhutdinov在Science上发表文章 《Reducing the Dimensionalitg of Data with Neural Neworks》，这篇文章有两个主要观点：1）多隐层神经网络有更厉害的学习能力，可以表达更多特征来描述对象；2）训练深度神经网络时，可通过降维（pre－training）来实现，老教授设计出来的Autoencoder网络能够快速找到好的全局最优点，采用无监督的方法先分开对每层网络进行训练，然后再来微调。该文章的发表翻开了深度学习的新篇章。2013年4月，深度学习技术被《麻省理工学院技术评论》（MIT TechnologyReview）杂志列为2013年十大突破性技术（Breakthrough Technology） 之首。与浅层学习模型依赖人工经验不同，深层学习模型通过构建机器学习模型和海量的训练数据，来学习更有用的特征，从而最终提升分类或预测的准确性。
2．深度学习的定义
深度学习是机器学习研究领域的分支，隶属于神经网络体系。深度学习通过建立、模拟人脑的信息处理神经结构来实现对外部输入的数据进行从低级到高级的特征提取，从而能够使机器理解学习数据，获得信息，因具有多个隐藏层的神经网络又被称为深度神经网络。深度学习将数据输入系统后，通过建模及模拟人脑的神经网从而进行学习的技术，像生物神经元一样，神经网络系统中有系列分层排列的模拟神经元（信息传递的连接点），且经过每个神经元的响应函数都会分配一个相应的“权值”，表示彼此之间的连接强度。通过每层神经元相互“连接”，计算机就可以由达到最佳方案时所有神经元的加权和，从而可以实现这一决策方案。
3．深度学习的基础和实现
①深度学习的思想基础一误差逆传播算法（BP算法）
BP神经网络（如） 是1986年Rumelhart和McClelland等人提出的，是一种按误差逆传播算法训练的多层前馈神经网络，它存储大量映射模式关系，无需揭示其映射方程。BP算法的核心思想是采用最速下降法（梯度下降法），通过反向传播调试网络的权值和阈值，使得其误差平方和最小。
BP神经网络
②图像处理领域的里程碑一卷积神经网络（CNN）
20世纪60年代，Hubel和Wiesel在研究猫脑皮层中用于局部敏感和方向选择的神经元时发现网络结构可以降低反馈神经网络的复杂性，进而提出了卷积神经网络的概念。由于其避免了对图像的前期预处理，可以直接输入原始图像，CNN已经成为神经网络的标志性代表之一。
 卷积神经网络（CNN）
③深度神经网络的实现基础一玻尔兹曼机和受限玻尔兹曼机
玻尔兹曼机 是Hinton和Sejnowski提出的随机递归神经网络，也可以看做是随机的Hopfield网络，因样本分布遵循玻尔兹曼分布而命名为BM。
 玻尔兹曼机
4．深度学习的重大成就
利用机器学习，人工智能系统获得了归纳推理和决策能力；而深度学习更将这一能力推向了更高的层次。目前，在深度学习中，卷积神经网络（Convolutional Neural Network，简称CNN）作为最有效的深层神经网络，现在已经被越来越多地应用到许多智能领域之中，并且它们越来越像人类了，例如AlphaGo、SIRI和FACEBOOK等都应用了卷积神经网络。在中国目前非常关注的智能制造领域中，制造机器人是深度学习的经典案例，深度学习的机器人能够自动适应外部环境的变化，面对新型任务时可以自动重新调整算法和技术。
5．深度学习的发展展望
深度学习必将成为人工智能发展的核心驱动力。虽然深度学习在实际应用中取得了许多成就，但是仍有局限性：理论研究缺乏、无监督学习能力弱、缺少逻辑推理和记忆能力等。深度学习的研究多是基于实验训练进行的，但是对其内部原理，学习本质研究很少。现在的研究多是在网络架构、参数选择等方面，而且深度学习的还有进一步提升空间，也需要更加完备深入的理论支撑其发展。
目前主流应用还是以监督学习为主的，但在实际生活中，无标签未知的数据占主体，所以更应该应用可以发现事物内在关系的无监督学习，未来还有更广阔的发展空间。深度学习是人工智能发展的巨大推力，目前阶段中深度学习更侧重于处理数据，在面对更复杂的任务时，则需要更多记忆能力和逻辑推理能力。
五：机器学习的未来：挑战与机遇并存
机器学习是人工智能应用的又一重要研究领域。当今，尽管在机器学习领域已经取得重大技术进展，但就目前机器学习发展现状而言，自主学习能力还十分有限，还不具备类似人那样的学习能力，同时机器学习的发展也面临着巨大的挑战，诸如泛化能力、速度、可理解性以及数据利用能力等技术性难关必须克服。但令人可喜的是，在某些复杂的类人神经分析算法的开发领域，计算机专家已经取得了很大进展，人们已经可以开发出许多自主性的算法和模型让机器展现出高效的学习能力。对机器学习的进一步深入研究，势必推动人工智能技术的深化应用与发展。