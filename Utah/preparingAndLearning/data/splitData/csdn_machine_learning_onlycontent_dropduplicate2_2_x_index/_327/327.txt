最近很是火热的人工智能、机器学习，作为一个从事软件开发的人员来说，不能不关注一下。
信息是通过语言文字来传递的，文字作为信息的载体，文字组合又是变化无穷的。再加上表达者表达时的各种语气，同一句话被不同的人说出来的意思也是不一样的。
语言文字的传递就是信息的编码和解码的过程，陈述者首先把信息通过编码变成语音或者文字，再通过媒介（空气、书籍等）传递出去，接受者再需要解码才能理解其中的意思。
那么作为机器有没有可能理解人类的语言呢？
科学家在很久以前就已经关注研究这方面的技术。计算机之父图灵提供了一种判断机器是否只能的方法：人通过和机器交流无法判断出交流的对象是人还是机器，那么机器就已经具有智能了。
想让机器完成翻译或者语音识别这些只有人类才能完成的情，必须让计算机理解人类语言。一个翻译者能很好的翻译中英文，那么这个翻译者必能很好的理解中文和英文。
只不过刚开始的时候科学家研究的方向是词法和语法语义的分析。
这也和人们的认知比较相近，就像刚开始学习英语的时候，老师会告诉学生们什么是动词什么是名词，以及动词名词如何组合才能准确的表达出想要表达的意思。
但是慢慢到了后来，人们发现语义词法分析已经不能很好的区分理解二义性问题，必须结合上下文才可以准确的理解。
比如：“能穿多少就穿多少”。这句话如果是夏天说的话，重音在“少”字上面，意思是尽量少穿凉快；如果是在冬天说的话，重音在“穿”字上面，意思是尽量多穿保暖。
后来人们采用统计的方法将语音识别率提高了很多，同是在文字识别方面也有很大的提升。
基于规则和基于统计的自然语言的处理，又有很长时间的争论，到了90年代以后，越来越多的人开始研究基于统计的自然语言的处理。
基于统计的模型是加隐含马尔科夫模型。
一个有意义句子，是由一串有特定顺序的连续排列的词组成的，把这些词使用w1,w2,w3…wn 由N个词组成，每个词出现的顺序是P(wx)。
基于以上的规则，俄国数学家马尔可夫提出了一种方法，就是假设任意一个词w(x)出现的概率只与它前面的w(x-1)有关，也就是说任何一个词只与它前面紧挨着它的这个词有关，与其他的词无关，这就是统计语言模型的二元模型。
当然也可以假设一个词有前面的2个词有关，这样的模型叫做三元模型；一个词与前面的N-1个词有关，就被成为N元模型。
条件概率：P(w(i)|w(i-1)) = P(w(i-1),w(i)) / p(w(i-1))
P(w(i-1),w(i)):前后相邻的两个词出现的频度；
p(w(i-1))：这个词出现的频度；
只要是大量的文本中，只要是统计量足够大，相对频度就等于概率。
这个方法的出现就连语言学家都质疑过它的有效性，但事实证明，统计语言模型比任何借助规则的语言模型更加有效。
其实事实也是这样的，当我们读一篇文章的时候，通常和当前词关联最紧密的往往是它前面的那个词，助词（的地得）除外。