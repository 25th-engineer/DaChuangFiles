1. 什么是机器学习？
权威定义：
Arthur samuel: 在不直接针对问题进行编程的情况下，赋予计算机学习能力的一个研究领域。
Tom Mitchell: 对于某类任务T和性能度量P，如果计算机程序在T上以P衡量的性能随着经验E而自我完善，那么就称这个计算机程序从经验E学习。
其实随着学习的深入，慢慢会发现机器学习越来越难定义，因为涉及到的领域很广，应用也很广，现在基本成为计算机相关专业的标配，但是在实际的操作过程中，又慢慢会发现其实机器学习也是很简单的一件事，我们最的大部分事情其实就是两件事情，一个是分类，一个是回归。比如房价的预测、股价的预测等是回归问题，情感判别、信用卡是否发放等则是属于分类。现实的情况 一般是给我们一堆数据，我们根据专业知识和一些经验提取最能表达数据的特征，然后我们再用算法去建模，等有未知数据过来的时候我们就能够预测到这个是属于哪个类别或者说预测到是一个什么值以便作出下一步的决策。比如说人脸识别系统，目的是作为一个验证系统，可能是一个权限管理，如果是系统中的人则有权限否则没有权限，首先给到我们的数据是一堆人脸的照片，第一步要做的事情是对数据进行预处理，然后是提取人脸特征，最后选择算法比如说SVM或者RF等等，算法的最终选择设计到评价标准，这个后面具体讲，这样我们就建立了一个人脸识别的模型，当系统输入一张人脸，我们就能够知道他是不是在系统之中。机器学习的整个流程不过就这几步，最后不过就是参数寻优，包括现在如火如荼的机器学习。
当我们判断是否要使机器学习时，可以看看是不是以下的场景
１）人类不能手动编程；
２）人类不能很好的定义这个问题的解决方案是什么；
３）人类不能做i到的需要极度快速决策的系统；
４）大规模个性化服务系统；
２.机器学习分类
监督学习：数据集是有标签的，就是说对于给出的样本我们是知道答案的，我们大部分学到的模型都是属于这一类的，包括线性分类器、支持向量机等等；
无监督学习：跟监督学习相反，数据集市完全没有标签的，主要的依据是相似的样本在数据空间中一般距离是相近的，这样就能通过距离的计算把样本分类，这样就完全不需要ｌａｂｅｌ，比如著名的ｋｍｅａｎｓ算法就是无监督学习应用最广泛的算法；
半监督学习：半监督学习一般针对的问题是数据量超级大但是有标签数据很少或者说标签数据的获取很难很贵的情况，训练的时候有一部分是有标签的而有一部分是没有的；
强化学习：一直激励学习的方式，通过激励函数来让模型不断根据遇到的情况做出调整；
３.感知机
感知机是最简单的机器学习算法，一般作为机器学习的入门级算法，也很好理解，但是麻雀虽小，五脏俱全，机器学习大致的思想和过程都涉及到了。
感知机可以认为是线性二元分类器，我们有一些特征数据，根据这些特征数据我们线性回归出一个值，如果超过了某个阈值，我们就说ＹＥＳ，否则ｓａｙ　ＮＯ．这其实就是简单的判断题了，一个简单的现实例子就是信用卡的发放问题，银行得到用户的一些个人信息，比如年龄，收入，信用记录等，针对这些信息我们赋予一些权重，这样我们就能够得到一个具体的数值，以此来判断是否发信用卡。具体算法执行如下
当然这是针对线性可分的情况，线性可分的意思是针对一个如果现行不可分这就要无限循环下去了，在现实生活中我们往往不能事先判断是否线性可分，一般的做法是限制迭代次数，保存当前最好结果。感知机的证明很简单，有兴趣可以参考林轩田老师的机器学习基石课程。
４.机器学习算法的评价
在生活当中，我们有一个直观的感受，针对相同的问题，不同的人可能会有不同的看法，不同的答案，这是因为我们从不同的角度出发，也就是说我们的评价体系是不一样的。而在机器学习中，这个影响尤其明显，不同的评价标准会造成完全不一样的预测。
这就需要从统计学的一个问题出发，对于一个样本大小为N的抽样数据集，如果N足够大，满足Ｈｏｅｆｆｄｉｎｇ不等式，
这个不等式的意思是说当N足够大时，我们能够大约认为样本的属性能够代表整体的属性。
这个不等式很重要！这个不等式很重要！这个不等式很重要！！！这也使我们谈机器学习的基础。
在机器学习中，当样本Ｎ足够i大并且是独立同分布的条件下，我们能够认为看到的样本能够代表未知样本的情况，也就是说我们通过已知数据找出的规律能够用来预测未知数据。
在机器学习的过程中，我们一般会把数据集分成两部分，一部分作为训练集，用来训练模型用，一部分用来做测试，当作我们的未知数据，训练测错误作为我们的评价标准，因为我们最终应用机器学习模型时，面临的是未知的数据，如果用训练错误来作为评判标准，可能引起的问题是过拟合，也就是我们训练效果很好而实际预测情况很糟糕，这是我们不想看到的。
总的来说，在机器学习中，我们要做两件事，做好这两件事其实就是表示我们学习到了：
１）测试错误接近于０，越小越好；
２）训练错误能够大致认为是预测错误，并且尽量使训练错误为零；
5 . VC维
k 代表 最小的break point
VC维具体的作用是证明误差有上限的，这很重要！如果VC维是有限的，表明训练误差和测试误差可以大致等同。
训练误差和测试误差中间差的其实就是模型复杂度的惩罚，这跟我们的样本大小N、VC维和H空间有关，上图描述了VC维、训练误差和测试误差之间的关系。
随着模型复杂度的上升，训练误差会逐渐减小，但是测试误差会先减小，但是后面会增加，上升的那段其实就是过拟合了，我们并不需要这么复杂的模型，因此并不代表我模型越复杂，泛化的效果就会越好，我们往往要做的是平衡，从简单的模型尝试，慢慢找到最好的分类器。

在实际运用中，N=10VC就够了！因为在证明的时候上限放的是很大的，在实际情况下我们并不会一直碰到坏情况。
6.  线性回归和逻辑回归
线性回归其实对于每个属性都配一个权重，然后最后得到一个值，和感知机的区别在于感知机预测到值之后是取正负号的而线性回归直接拿到值就好了，比如说一个人的信用分评测或则房价的预测等等。
线性回归评价采用平方误差，问题就转化为一个优化问题，求解平方误差的最小值，求偏导然后等于零解出权重。
线性回归取个符号就变成了线性分类，并且线性回归的错误取得是比0/1错误取得大的，能够用线性回归来做分类。
逻辑回归和现行回归不一样的地方在于sigmoid函数的介入，
逻辑回归的误差通过最大似然估计来评价，最终转化成如下优化问题
通过梯度下降来求解，
学习速率可以动态调整，梯度大时学习率大些，因为可以更快接近最优值而梯度小时慢些。
7. 非线性变换
非线性变换主要解决的问题是一些线性不可分的情况，其实做的工作就是对特征进行变换，以上图为例，很明显数据集是线性不可分的，但是我们能够i通过曲线进行分离，比如说圆，圆是二次的，因此我们把特征变化到另一个空间维度，在新的空间维度下，数据是线性可分的，我们就能用线性分类器来处理了。
更高次的变化也是一样的，但是要注意的是在特征变换的过程中，参数变多了，计算的复杂度上升了，模型的复杂度也上升了。
8. 过拟合
过拟合的特征一般是具有很小的训练错误，但是泛化能力很差，具有很高的泛化误差。
过拟合在我们实际的操作中经常出现，主要的原因在于模型的复杂度太高。
造成过拟合的原因主要是下面四个方面：
解决过拟合的方法：
1）从简单的模型开始尝试；
2）数据预处理，数据清洗；
3）额外的数据；
4）正则化，regularization；
5）验证，validation；
9. 正则化
正则化相当于给训练误差加了一个惩罚项，以防止过拟合的发生。
用的比较多的正则项是L1和L2，
正则化通过牺牲一定的准确率而增加一定的泛化能力。
10. 验证
验证的目的是选择最优的模型，而依据就是泛化误差，因为我们最终把模型应用的是未知数据。
目前用的最广泛的是V-fold cross validation，把数据集分成V份，每次拿出V-1作为训练集，而剩下的一份作为验证，通过V次的训练，把最后训练错误的平均值作为该模型的评价，然后选出最佳。
条件允许的条件下，尽量采用交叉验证，一般用5折或者10折。
致谢：
感谢台湾大学的林轩田老师，《机器学习基石》课程让我更进一步理解机器学习的基本概念，层层深入的教学方式让我受益颇多！文中内容也是参考此课程。
Andrewseu 2016/12/7