BAT机器学习面试1000题系列
我的Leetcode账号
七月在线 购买的课程
强调两点:
1.虽然本系列主要都是机器学习、深度学习相关的考题，并无其他类型的题，但不代表应聘机器学习或深度学习时面试官就只问这两项，虽是做数据或AI相关，基本的语言（比如Python）、编码coding能力、数据结构、算法、计算机体系结构、操作系统、概率统计基本都是搞IT必备，也必须掌握。对于数据结构和算法，一者，重点推荐前面说的微软面试100题系列；二者，多刷leetcode，看1000道题不如实际动手刷100道。
—BAT机器学习面试1000题系列（第1~10题）—
1.请简要介绍下SVM
SVM，全称是support vector machine，中文名叫支持向量机。SVM是一个面向数据的分类算法，它的目标是为确定一个分类超平面，从而将不同的数据分隔开。
扩展：这里有篇文章详尽介绍了SVM的原理、推导，《支持向量机通俗导论（理解SVM的三层境界）》。此外，这里有个视频也是关于SVM的推导：《纯白板手推SVM》
2.请简要介绍下tensorflow的计算图
Tensorflow是一个通过计算图的形式来表述计算的编程系统，计算图也叫数据流图，可以把计算图看做是一种有向图，Tensorflow中的每一个计算都是计算图上的一个节点，而节点之间的边描述了计算之间的依赖关系。
3.请问GBDT和XGBoost的区别是什么？
xgboost类似于gbdt的优化版，不论是精度还是效率上都有了提升。与gbdt相比，具体的优点有：
1.损失函数是用泰勒展式二项逼近，而不是像gbdt里的就是一阶导数
2.对树的结构进行了正则化约束，防止模型过度复杂，降低了过拟合的可能性
3.节点分裂的方式不同，gbdt是用的gini系数，xgboost是经过优化推导后的
更多详见
4.在k-means或kNN，我们是用欧氏距离来计算最近的邻居之间的距离。为什么不用曼哈顿距离？
曼哈顿距离只计算水平或垂直距离，有维度的限制。另一方面，欧氏距离可用于任何空间的距离计算问题。因为，数据点可以存在于任何空间，欧氏距离是更可行的选择。例如：想象一下国际象棋棋盘，象或车所做的移动是由曼哈顿距离计算的，因为它们是在各自的水平和垂直方向做的运动。
5.百度2015校招机器学习笔试题
百度2015校招机器学习笔试题
6.简单说说特征工程
7.关于LR
把LR从头到脚都给讲一遍。建模，现场数学推导，每种解法的原理，正则化，LR和maxent模型啥关系，lr为啥比线性回归好。有不少会背答案的人，问逻辑细节就糊涂了。原理都会? 那就问工程，并行化怎么做，有几种并行化方式，读过哪些开源的实现。还会，那就准备收了吧，顺便逼问LR模型发展历史。
另外，关于答案这篇文章可以做参考：
http://blog.csdn.net/cyh_24/article/details/50359055.html
http://blog.csdn.net/zouxy09/article/details/20319673
8.overfitting怎么解决？
dropout、regularization、batch normalizatin
9.LR和SVM的联系与区别
1、LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题（在改进的情况下可以处理多分类问题）
2、两个方法都可以增加不同的正则化项，如l1、l2等等。所以在很多实验中，两种算法的结果是很接近的。
区别：
1、LR是参数模型，SVM是非参数模型。
2、从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss.这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。
3、SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。
4、逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。
5、logic 能做的 svm能做，但可能在准确率上有问题，svm能做的logic有的做不了。
来源：http://blog.csdn.net/timcompp/article/details/62237986
10.LR与线性回归的区别与联系
个人感觉逻辑回归和线性回归首先都是广义的线性回归，
其次经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数，
另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在[0,1]。逻辑回归就是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。
@乖乖癞皮狗：逻辑回归的模型本质上是一个线性回归模型，逻辑回归都是以线性回归为理论支持的。但线性回归模型无法做到sigmoid的非线性形式，sigmoid可以轻松处理0/1分类问题。
本来来自七月在线实验室 仅供自己学习使用