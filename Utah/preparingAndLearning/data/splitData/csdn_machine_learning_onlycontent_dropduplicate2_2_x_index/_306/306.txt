支持向量机(SVM)算法推导
1.1  SVM要解决的问题
SVM和逻辑回归类似，也是非常经典的二分类问题，也可以作多分类问题。现以二分类问题为例进行算法推导。
支持向量机（SVM）要解决的问题：
什么样的决策边界是最好的？
特征数据本身如果很难分，怎么处理？
计算复杂度怎么样？能实际应用么
目标：基于上述问题对SVM进行推导
假设有一个问题：如下-a所示，将两种不同颜色的点分开，图b、图c中的黑色实现均可作决策边界，术语称“决策面”。到底哪个决策边界会是最好的呢？现在需要解决的就是找最好的决策边界将不同的颜色的点分开。

很明显，-b决策边界相对于-c，边界区域比较宽，越宽越好。越宽，边界的泛化能力也越强(容忍度比较高)。我们的目标是在训练集上训练出一个最佳的边界，在测试集上表现最佳。
实际问题问题--转化数学问题（目标函数）
决策边界：离样本点距离最远的直线
1.2 距离与数据标签的定义
1) 距离：
决策边界---空间中的一条直线：WX+b =0
X1W1+X2W2+......+b=0
根据点到直线的距离公式可得，样本点到直线的距离为：
Distance(x,b,w) =
假设支持向量的点到决策曲面的最近的距离是1个单位，则样本点到直线的距离1，即
数据标签定义
数据集：(X1,Y1),(X2,Y2)... (Xn,Yn)  X:数据    Y：样本类别
决策方程：
与逻辑回归类似，SVM作二分类 要么正例 要么负例。逻辑回归正例为1，负例为0；SVM正例为1，负例为-1。
预测值 > 0 则y为正例对应y值为+1
预测值 < 0  则为负例y值为-1
y(Xi)为预测值   Yi为数据原有的标签值
无论是正例还是负例，下面的表达式始终成立：
y(Xi)* Yi> 0
1.3目标函数
优化目标：找到一条直线(合适的w和b)，使得离该线最近的点和这条直线（决策面）的距离最远。
点到平面的距离-----点到直线的距离