本文试图从分类学习来分析现有几种机器学习模型的基本思路。
首先强调，机器学习所使用的训练数据和测试数据被看作是依联合分布概率
P(X,Y)
P(X,Y)独立同分布产生，这是监督学习关于数据的基本假设。
对于分类问题，设输入空间
X⊆Rn
\mathcal{X} \subseteq R^n为
n
n维向量的集合，输出空间为类标记集合
Y={c1,c2,...,cK}
\mathcal{Y} = \{ c_1, c_2, ..., c_K \}。输入为特征向量
x∈X
x \in \mathcal{X}，输出为类标记
y∈Y
y \in \mathcal{Y}。
X
X是定义在输入空间
X
\mathcal{X}上的随机变量，
Y
Y是定义在输出空间
Y
\mathcal{Y}上的随机变量。
P(X,Y)
P(X,Y)是
X
X和
Y
Y的联合概率分布。训练数据集
T={(x1,y1),(x2,y2),...,(xN,yN)}
T = \{ (x_1, y_1), (x_2, y_2), ..., (x_N, y_N) \}
由
P(X,Y)
P(X,Y)独立同分布产生。我们的目的是通过训练集学习到联合概率分布
P(X,Y)
P(X,Y)或条件概率分布
P(Y|X)
P(Y|X)。
如果训练集足够大，我们由训练集所获得的经验条件概率分布
P^(Y|X)
\hat{P} (Y|X)可以无限逼近于实际的条件概率分布
P(Y|X)
P(Y|X)。但问题在于，我们很难获得足够多的样本。对于分类问题，假设
x(j)
x^{(j)}可取值有
Sj
S_j个，
j=1,2,...,n
j = 1, 2, ..., n，那么输入空间与输出空间所可能的组合数
M=K∏nj=1Sj
M = K \prod_{j=1}^{n} S_j。
事实上，样本数量
|T|
|T|相对“可能性”数量
M
M不够大，是个机器学习各分类模型要解决的主要问题。如前所述，如果
|T|M
\frac{|T|}{M}足够大，我们实际上可以以经验条件概率分布
P^(Y|X)
\hat{P} (Y|X)来作为实际条件概率分布
P(Y|X)
P(Y|X)（注意关于数据的基本假设：训练数据和测试数据被看作是依联合分布概率
P(X,Y)
P(X,Y)独立同分布产生）。
所谓过拟合，就是无视
|T|M
\frac{|T|}{M}必须保持较高的比例这一要求，所求得的结果其实没有统计意义。在训练集之外的现实世界，正是因为数据的巨型数量，才呈现出分布规律。对于分类问题，现实世界中数据数量与“可能性”数量
M
M的比趋向于无穷。
好，既然
|T|
|T|不够大，只有想办法减少
M
M，保持
|T|M
\frac{|T|}{M}处于较高的比例，才有可能获得接近
P(Y|X)
P(Y|X)的
P^(Y|X)
\hat{P} (Y|X)。现有的机器学习模型，基本都是遵循这一基本思路设计，考虑到
M=K∏Nj=1Sj
M = K \prod_{j=1}^{N} S_j，其中
K
K是不变的，具体的减少
M
M的办法有
2
2类。
减少条件特征数
n
n
决策树
决策树模型通过计算信息增益来识别对结果（种类）
Y
Y影响最大的那些特征，决策树的剪枝实际上就是去掉那些对结果影响小的特征的识别，实际上就是减少了特征数量
n
n，从而
M
M也得到减小。
最大熵
最大熵模型通过指定“特征”及“特征函数”，来过滤出那些更有价值的样本点。实际上，这些“特征”往往是某个条件特征或是某些条件特征组合的一种反映，本质上还是条件特征的筛选，设计思路也是减少条件特征数
n
n，从而减小
M
M。
减少条件取值数
S
S
k
k临近法
k
k临近法模型，对于样本集中没有出现过的特征向量
x
x，直接以样本中“靠近
x
x”的特征向量来表征它。比如极端情况
k=1
k = 1，则相当于直接用离
x
x最近的样本特征向量来代替
x
x。所以
k
k临近法模型的思路其实是：使用少部分（样本集中出现过的）特征值来取代其他的特征值，也就是减少取值范围
Sj(j=1,2,...,n)
S_{j(j = 1, 2, ..., n)}，从而减小
M
M。
朴素贝叶斯
朴素贝叶斯模型通过条件独立性假设，来推测样本集中没有出现过的特征向量的分布概率。举个例子，我们知道
x(1)i
x_i^{(1)}取值范围是
{Si1,Si2}
\{S_{i1}, S_{i2}\}，
x(2)i
x_i^{(2)}取值范围是
{Si3,Si4,Si5}
\{S_{i3}, S_{i4}, S_{i5}\}，我们不需要样本集中出现过
xj={Si1,Si3,...}
x_j = \{S_{i1}, S_{i3}, ...\}就可以推测
xj
x_j的分布概率。实际上朴素贝叶斯模型同样是使用少部分（样本集中出现过的）特征值来取代（计算）其他的特征值，本质上也是减少取值范围
S
S，从而减小
M
M。
这里多聊聊
k
k临近法和朴素贝叶斯，它们虽然基本思路一样，但考虑问题的角度却截然相反。朴素贝叶斯基于条件独立性假设，各条件特征相关性越小越好。
k
k临近法认为接近的点更相似，但判断“接近”时只考虑数学上的距离而不考虑各坐标轴（条件特征）是否具有可比性。比如衣服的长度和颜色是两个无关的特征，但在
k
k临近法模型中它们变得相互关联，根据颜色到数值的映射你会发现某种颜色跟长度
180
180更接近（？）。所以其实
k
k临近法模型想取得较好的效果，反而要求条件特征具有较高的关联性，才能使得“特征向量
x
x在某个特征维度上移动会影响到其它特征维度上的向量
x′
x’与
x
x的相关性的判断（即距离）”这件事情显得合理。
（未完）