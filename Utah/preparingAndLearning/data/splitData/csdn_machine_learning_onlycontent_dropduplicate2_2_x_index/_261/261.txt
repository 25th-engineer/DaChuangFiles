声明：本文翻译自Vishal Maini在Medium平台上发布的《Machine Learning for Humans》的教程的《Part 5: Reinforcement Learning》的英文原文（原文链接）。该翻译都是本人（tomqianmaple@outlook.com）本着分享知识的目的自愿进行的，欢迎大家交流！
纯属自愿翻译，只为学习与分享知识。所以如果本系列教程对你有帮助，麻烦不吝在github的项目上点个star吧！非常感谢！
关键词：探索和利用、马尔科夫决策过程、Q-Learning、策略学习、深度增强学习。
[Update 9/2/17] 现在本系列教程已经出了电子书了，可以在这里下载！
有问题请咨询ml4humans@gmail.com。
“我刚因为完成上个部分的任务而吃了些巧克力。”
在监督学习中，训练数据伴随着来自像神一般的“监督者”给定的相应答案，如果生活也能像这样一切有确定的答案就好了！
在**强化学习（Reinforcement Learning）**中，却没有答案这一说，但是你的强化学习agent仍然不得不决定如何行动以完成指定任务。在没有训练数据存在的情况下，agent从经验当中学习。 它会在尝试完成任务的过程中通过是错收集训练样例（“这个行为是好的，那个行为很糟糕”），而其目标则始终是最大化长期回报。
在《Machine Learning for Humans》系列的这最后一部分，我们将探索：
探索/利用之间的权衡
马尔科夫决策过程（MDPs），强化学习任务的经典设定
Q-Learning，策略学习，深度学习
还有最后的价值学习问题
最后，我们也如往常一样提供一些好的继续可供深入探索的相关学习资源。
我们首先来把一个机器人老鼠放在迷宫里
最简单的理解强化学习的环境，就是一个有清晰目标和记分系统的游戏。
假设我们在玩一个游戏，我们的老鼠