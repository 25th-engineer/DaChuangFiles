目录
一、简介
二、监督学习
2.1、决策树（Decision Tree，DT）
2.2、朴素贝叶斯分类器（Naive Bayesian Model，NBM）
2.3、最小二乘法（Least squares）
2.4、逻辑回归（Logistic Regression）
2.5、支持向量机（Support Vector Machine）
2.6、K最近邻算法（KNN，K-NearestNeighbor）
2.7、集成学习（Ensemble Learning）
三、无监督学习
3.1、聚类算法
3.2、K-均值算法(K-Means)
3.3、主成分分析（Principal Component Analysis，PCA）
3.4、SVD矩阵分解（Singular Value Decomposition）
3.5、独立成分分析(ICA)
四、强化学习
4.1、Q-Learning算法
五、机器学习常用Python包
5.1、sklearn
5.2、numpy
5.3、scipy
5.4、pandas
5.5、statsmodels
5.6、matplotlib、pyplot、pylab
5.7、jieba
5.8、Pattern
六、各个算法精确率对比
6.1、支持向量机（SupportVectorMachine）
6.1.1、升级版sklearn
6.1.2、Liblinear
6.1.3、sklearn
6.2、随机森林（Random Forest）
6.3、朴素贝叶斯（Naive Bayesian Model）
6.4、K近邻（K-NearestNeighbor）
6.5、 逻辑回归（LogisticRegression）
6.6、决策树（Decision Tree）
一、简介
本文讲解了机器学习常用算法总结和各个常用分类算法精确率对比。收集了现在比较热门的TensorFlow、Sklearn，借鉴了Github和一些国内外的文章。
机器学习的知识树，这个图片是Github上的，有兴趣的可以自己去看一下：
地址：https://github.com/trekhleb/homemade-machine-learning
简单的翻译一下这个树：
英文
中文
Machine Learning
机器学习
Supervised Learning
监督学习
Unsupervised Learning
非监督学习
Reinforcement Learning
强化学习
Neural Networks and Deep Learning
神经网络与深度学习
Ensemble Learning
集成学习
以下是一部分算法的概念和应用，仅供大家参考
二、监督学习
监督学习可以看作是原先的预测模型，有基础的训练数据，再将需要预测的数据进行输入，得到预测的结果（不管是连续的还是离散的）
2.1、决策树（Decision Tree，DT）
决策树是一种树形结构，为人们提供决策依据，决策树可以用来回答yes和no问题，它通过树形结构将各种情况组合都表示出来，每个分支表示一次选择（选择yes还是no），直到所有选择都进行完毕，最终给出正确答案。
决策树（decision tree）是一个树结构（可以是二叉树或非二叉树）。在实际构造决策树时，通常要进行剪枝，这时为了处理由于数据中的噪声和离群点导致的过分拟合问题。剪枝有两种：
先剪枝——在构造过程中，当某个节点满足剪枝条件，则直接停止此分支的构造。 后剪枝——先构造完成完整的决策树，再通过某些条件遍历树进行剪枝。
2.2、朴素贝叶斯分类器（Naive Bayesian Model，NBM）
朴素贝叶斯分类器基于贝叶斯定理及其假设（即特征之间是独立的，是不相互影响的），主要用来解决分类和回归问题。
具体应用有： 标记一个电子邮件为垃圾邮件或非垃圾邮件； 将新闻文章分为技术类、政治类或体育类； 检查一段文字表达积极的情绪，或消极的情绪； 用于人脸识别软件。
学过概率的同学一定都知道贝叶斯定理，这个在250多年前发明的算法，在信息领域内有着无与伦比的地位。贝叶斯分类是一系列分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类。朴素贝叶斯算法（Naive Bayesian) 是其中应用最为广泛的分类算法之一。朴素贝叶斯分类器基于一个简单的假定：给定目标值时属性之间相互条件独立。
2.3、最小二乘法（Least squares）
你可能听说过线性回归。最小均方就是用来求线性回归的。如下图所示，平面内会有一系列点，然后我们求取一条线，使得这条线尽可能拟合这些点分布，这就是线性回归。这条线有多种找法，最小二乘法就是其中一种。最小二乘法其原理如下，找到一条线使得平面内的所有点到这条线的欧式距离和最小。这条线就是我们要求取得线。
2.4、逻辑回归（Logistic Regression）
逻辑回归模型是一个二分类模型，它选取不同的特征与权重来对样本进行概率分类，用一个log函数计算样本属于某一类的概率。即一个样本会有一定的概率属于一个类，会有一定的概率属于另一类，概率大的类即为样本所属类。用于估计某种事物的可能性。
2.5、支持向量机（Support Vector Machine）
支持向量机（support vector machine）是一个二分类算法，它可以在N维空间找到一个(N-1)维的超平面，这个超平面可以将这些点分为两类。也就是说，平面内如果存在线性可分的两类点，SVM可以找到一条最优的直线将这些点分开。SVM应用范围很广。
要将两类分开，想要得到一个超平面，最优的超平面是到两类的margin达到最大，margin就是超平面与离它最近一点的距离，如下图，Z2>Z1，所以绿色的超平面比较好。
2.6、K最近邻算法（KNN，K-NearestNeighbor）
邻近算法，或者说K最近邻(KNN，K-NearestNeighbor)分类算法是数据挖掘分类技术中最简单的方法之一。KNN算法的核心思想是如果一个样本在特征空间中的k个最相邻的样本中的大多数属于某一个类别，则该样本也属于这个类别，并具有这个类别上样本的特性。该方法在确定分类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。 KNN方法在类别决策时，只与极少量的相邻样本有关。由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法更为适合。
主要应用领域是对未知事物的识别，即判断未知事物属于哪一类，判断思想是，基于欧几里得定理，判断未知事物的特征和哪一类已知事物的的特征最接近。如上图，绿色圆要被决定赋予哪个类，是红色三角形还是蓝色四方形？如果K=3，由于红色三角形所占比例为2/3，绿色圆将被赋予红色三角形那个类，如果K=5，由于蓝色四方形比例为3/5，因此绿色圆被赋予蓝色四方形类。由此也说明了KNN算法的结果很大程度取决于K的选择。
2.7、集成学习（Ensemble Learning）
集成学习就是将很多分类器集成在一起，每个分类器有不同的权重，将这些分类器的分类结果合并在一起，作为最终的分类结果。最初集成方法为贝叶斯决策。
集成算法用一些相对较弱的学习模型独立地就同样的样本进行训练，然后把结果整合起来进行整体预测。集成算法的主要难点在于究竟集成哪些独立的较弱的学习模型以及如何把学习结果整合起来。这是一类非常强大的算法，同时也非常流行。
常见的算法包括： Boosting， Bootstrapped Aggregation（Bagging）， AdaBoost，堆叠泛化（Stacked Generalization， Blending）， 梯度推进机（Gradient Boosting Machine, GBM），随机森林（Random Forest）。
那么集成方法是怎样工作的，为什么他们会优于单个的模型？
他们拉平了输出偏差：如果你将具有民主党倾向的民意调查和具有共和党倾向的民意调查取平均，你将得到一个中和的没有倾向一方的结果。
它们减小了方差：一堆模型的聚合结果和单一模型的结果相比具有更少的噪声。在金融领域，这被称为多元化——多只股票的混合投资要比一只股票变化更小。这就是为什么数据点越多你的模型会越好，而不是数据点越少越好。
它们不太可能产生过拟合：如果你有一个单独的没有过拟合的模型，你是用一种简单的方式（平均，加权平均，逻辑回归）将这些预测结果结合起来，然后就没有产生过拟合的空间了。
三、无监督学习
3.1、聚类算法
聚类算法就是将一堆数据进行处理，根据它们的相似性对数据进行聚类。
聚类，就像回归一样，有时候人们描述的是一类问题，有时候描述的是一类算法。聚类算法通常按照中心点或者分层的方式对输入数据进行归并。所以的聚类算法都试图找到数据的内在结构，以便按照最大的共同点将数据进行归类。常见的聚类算法包括 k-Means算法以及期望最大化算法（Expectation Maximization， EM）。
聚类算法有很多种，具体如下：中心聚类、关联聚类、密度聚类、概率聚类、降维、神经网络/深度学习。
3.2、K-均值算法(K-Means)
K-means算法是硬聚类算法，是典型的基于原型的目标函数聚类方法的代表，它是数据点到原型的某种距离作为优化的目标函数，利用函数求极值的方法得到迭代运算的调整规则。K-means算法以欧式距离作为相似度测度，它是求对应某一初始聚类中心向量V最优分类，使得评价指标J最小。算法采用误差平方和准则函数作为聚类准则函数。K-means算法是很典型的基于距离的聚类算法，采用距离作为相似性的评价指标，即认为两个对象的距离越近，其相似度就越大。该算法认为簇是由距离靠近的对象组成的，因此把得到紧凑且独立的簇作为最终目标。
通常，人们根据样本间的某种距离或者相似性来定义聚类，即把相似的（或距离近的）样本聚为同一类，而把不相似的（或距离远的）样本归在其他类。
3.3、主成分分析（Principal Component Analysis，PCA）
主成分分析是利用正交变换将一些列可能相关数据转换为线性无关数据，从而找到主成分。PCA方法最著名的应用应该是在人脸识别中特征提取及数据降维。
PCA主要用于简单学习与可视化中数据压缩、简化。但是PCA有一定的局限性，它需要你拥有特定领域的相关知识。对噪音比较多的数据并不适用。
3.4、SVD矩阵分解（Singular Value Decomposition）
也叫奇异值分解（Singular Value Decomposition），是线性代数中一种重要的矩阵分解，是矩阵分析中正规矩阵酉对角化的推广。在信号处理、统计学等领域有重要应用。SVD矩阵是一个复杂的实复负数矩阵，给定一个m行、n列的矩阵M,那么M矩阵可以分解为M = UΣV。U和V是酉矩阵，Σ为对角阵。
PCA实际上就是一个简化版本的SVD分解。在计算机视觉领域，第一个脸部识别算法就是基于PCA与SVD的，用特征对脸部进行特征表示，然后降维、最后进行面部匹配。尽管现在面部识别方法复杂，但是基本原理还是类似的。
3.5、独立成分分析(ICA)
独立成分分析（Independent Component Analysis，ICA）是一门统计技术，用于发现存在于随机变量下的隐性因素。ICA为给观测数据定义了一个生成模型。在这个模型中，其认为数据变量是由隐性变量，经一个混合系统线性混合而成，这个混合系统未知。并且假设潜在因素属于非高斯分布、并且相互独立，称之为可观测数据的独立成分。
ICA与PCA相关，但它在发现潜在因素方面效果良好。它可以应用在数字图像、档文数据库、经济指标、心里测量等。
上图为基于ICA的人脸识别模型。实际上这些机器学习算法并不是全都像想象中一样复杂，有些还和高中数学紧密相关。
四、强化学习
4.1、Q-Learning算法
Q-learning要解决的是这样的问题：一个能感知环境的自治agent，怎样通过学习选择能达到其目标的最优动作。
强化学习目的是构造一个控制策略，使得Agent行为性能达到最大。Agent从复杂的环境中感知信息，对信息进行处理。Agent通过学习改进自身的性能并选择行为，从而产生群体行为的选择，个体行为选择和群体行为选择使得Agent作出决策选择某一动作，进而影响环境。增强学习是指从动物学习、随机逼近和优化控制等理论发展而来，是一种无导师在线学习技术，从环境状态到动作映射学习，使得Agent根据最大奖励值采取最优的策略；Agent感知环境中的状态信息，搜索策略（哪种策略可以产生最有效的学习）选择最优的动作，从而引起状态的改变并得到一个延迟回报值，更新评估函数，完成一次学习过程后，进入下一轮的学习训练，重复循环迭代，直到满足整个学习的条件，终止学习。
Q-Learning是一种无模型的强化学习技术。具体来说，可以使用Q学习来为任何给定的（有限的）马尔可夫决策过程（MDP）找到最优的动作选择策略。它通过学习一个动作价值函数，最终给出在给定状态下采取给定动作的预期效用，然后遵循最优策略。一个策略是代理在选择动作后遵循的规则。当这种动作值函数被学习时，可以通过简单地选择每个状态中具有最高值的动作来构建最优策略。 Q-learning的优点之一是能够比较可用操作的预期效用，而不需要环境模型。此外，Q学习可以处理随机过渡和奖励的问题，而不需要任何适应。已经证明，对于任何有限的MDP，Q学习最终找到一个最优策略，从总体奖励的预期值返回到从当前状态开始的所有连续步骤是最大可实现的意义。
五、机器学习常用Python包
5.1、sklearn
开源机器学习模块，包括分类、回归、聚类系列算法，主要算法有SVM、逻辑回归、朴素贝叶斯、Kmeans、DBSCAN等；也提供了一些语料库。
学习地址：https://scikit-learn.org/stable/modules/classes.html
5.2、numpy
Python的语言扩展，定义了数字的数组和矩阵。提供了存储单一数据类型的多维数组(ndarray)和矩阵（matrix）。
学习地址：http://www.numpy.org/
5.3、scipy
其在numpy的基础上增加了众多的数学、科学以及工程计算中常用的模块，例如线性代数、常微分方程数值求解、信号处理、图像处理、稀疏矩阵等等。
学习地址：https://www.scipy.org/
5.4、pandas
直接处理和操作数据的主要package，提供了dataframe等方便处理表格数据的数据结构
学习地址：http://pandas.pydata.org/
5.5、statsmodels
统计和计量经济学的package，包含了用于参数评估和统计测试的实用工具
学习地址：https://pypi.org/project/statsmodels/
5.6、matplotlib、pyplot、pylab
用于生成统计图。pyplot 和 pylab属于matplotlib的子模块，所以只需安装matplotlib，就会有pyplot和pylab的了。
学习地址：https://matplotlib.org/
5.7、jieba
中文分词工具。
学习地址：https://github.com/fxsjy/jieba
5.8、Pattern
此库更像是一个“全套”库，因为它不仅提供了一些机器学习算法，而且还提供了工具来帮助你收集和分析数据。数据挖掘部分可以帮助你收集来自谷歌、推特和维基百科等网络服务的数据。它也有一个Web爬虫和HTML DOM解析器。“引入这些工具的优点就是：在同一个程序中收集和训练数据显得更加容易。
学习地址：https://github.com/clips/pattern
六、各个算法精确率对比
此次算精确率对比，总语料样本21282条，分类标签911个，语料是企业的语料集，不对外公开。精准率是把整体样本按照8：2的比例，分为80%的训练集，20%的测试集来算的，实验流程在每篇文章中都有详细记载。数据量低于21282的是取了总样本的部分数据做的实验，效果统计如下：
6.1、支持向量机（SupportVectorMachine）
6.1.1、升级版sklearn
机器学习 之 支持向量机（SupportVectorMachine）文本算法的精确率——升级版sklearn
6.1.2、Liblinear
机器学习 之 Liblinear中的支持向量机（SupportVectorMachine）文本算法的精确率
6.1.3、sklearn
机器学习 之 sklearn中的支持向量机（SupportVectorMachine）文本算法的精确率
6.2、随机森林（Random Forest）
机器学习 之 随机森林（Random Forest）文本算法的精确率
6.3、朴素贝叶斯（Naive Bayesian Model）
机器学习 之 朴素贝叶斯（Naive Bayesian Model）文本算法的精确率
6.4、K近邻（K-NearestNeighbor）
机器学习 之 K近邻（K-NearestNeighbor）文本算法的精确率
6.5、 逻辑回归（LogisticRegression）
机器学习 之 逻辑回归（LogisticRegression）文本算法的精确率
6.6、决策树（Decision Tree）
机器学习 之 决策树（Decision Tree）文本算法的精确率
看完本文实属不易，写本文也耗费了我很多时间和精力，希望大家有钱场捧个钱场，有人场捧个人场，谢谢~