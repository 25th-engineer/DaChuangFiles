机器学习中建模过程
数据处理
特征工程
模型选择
寻找最佳超参数
模型分析与模型融合
1.数据处理
1.1机器学习中使用的数据的原则
属性的值和属性的名称应该具有实际的意义。
去除属性中相关度高的属性
去除对结果影响不大的属性
合理选择关联字段
1.2常见的数据预处理方法
数据清洗：数据清洗的目的不仅仅是清除错误点，冗余点和数据的噪声，还要将数据按照一定的规则进行统一处理。
数据集成：将多个数据源中的数据进行合并，形成一个统一的表格。如果数据量比较多，则存储于数据仓库中；若数据亮不大，则存储于文件中，常用的数据文件存储格式为：csv,json等等。
数据变化：找到数据的特征表示，用维度变换来减少有效的数据，包括：规格化，规约和投影等操作
数据规约：是在对发现任务和数据本身内容理解的基础上，寻找依赖于发现目标的表达数据的有用特征，以缩减数据模型，从而在尽可能保持数据原貌的前提下最大限度的精简数据量，主要有两个途径：属性选择和数据抽样，分别针对数据库中的属性和记录
1.3数据清洗
1.3.1缺失值的处理
删除法：根据数据处理的不同角度，删除法可分为以下4种
删除观测样本
删除变量：当某个变量缺失值较多且对研究目标影响不大时，可以将整个变量整体删除。
使用完整原始数据分析：当数据存在较多缺失而其原始数据完整时，可以使用原始数据替代现有数据进行分析；
改变权重：当删除缺失数据会改变数据结构时，通过对完整数据按照不同的权重进行加工，可以降低删除数据带来的偏差
插补法：在条件允许的情况下，找到缺失值的替代值进行插补，尽可能还原真实数据是更好的方法。常见的方法有均值插补、回归插补、二阶插补、热平台、冷平台等单一变量插补。
均值法是通过计算缺失值所在变量所有非缺失观测值的均值，使用均值来代替缺失值的插补方法。
均值法不能利用相关变量信息，因此会存在一定偏差，而回归模型是将需要插补变量作为因变量，其他相关变量作为自变量，通过建立回归模型预测出因变量的值对缺失变量进行插补
热平台插补是指在非缺失数据集中找到一个与缺失值所在样本相似的样本（匹配样本），利用其中的观测值对缺失值进行插补。
在实际操作中，尤其当变量数量很多时，通常很难找到与需要插补样本完全相同的样本，此时可以按照某些变量将数据分层，在层中对缺失值使用均值插补，即采取冷平台插补法。
1.3.2噪声数据的处理： 噪声是一个测量变量中的随机错误和偏差，包括错误值或偏离期望的孤立点值
噪声检查中比较常见的方法：
通过寻找数据集中与其他观测值及均值差距最大的点作为异常
聚类方法检测，将类似的取值组织成“群”或“簇”，落在“簇”集合之外的值被视为离群点。
在进行噪声检查后，通常采用分箱、聚类、回归、计算机检查和人工检查结合等方法“光滑”数据，去掉数据中的噪声
分箱：分箱方法是一种简单常用的预处理方法，通过考察相邻数据来确定最终值。所谓“分箱”，实际上就是按照属性值划分的子区间，如果一个属性值处于某个子区间范围内，就称把该属性值放进这个子区间所代表的“箱子”内。把待处理的数据（某列属性值）按照一定的规则放进一些箱子中，考察每一个箱子中的数据，采用某种方法分别对各个箱子中的数据进行处理。在采用分箱技术时，需要确定的两个主要问题就是：如何分箱以及如何对每个箱子中的数据进行平滑处理。
1.3.3噪声数据的处理： 分箱的方法：有4种：等深分箱法、等宽分箱法、最小熵法和用户自定义区间法。
等深分箱法（统一权重）：将数据集按记录行数分箱，每箱具有相同的记录数，每箱记录数称为箱子的深度。这是最简单的一种分箱方法。
箱1：800 1000 1200 1500 箱2：1500 1800 2000 2300 箱3：2500 2800 3000 3500 箱4：4000 4500 4800 5000
等宽分箱法（统一区间）：使数据集在整个属性值的区间上平均分布，即每个箱的区间范围是一个常量，称为箱子宽度。
箱1：800 1000 1200 1500 1500 1800 箱2：2000 2300 2500 2800 3000 箱3：3500 4000 4500 箱4：4800 5000
用户自定义区间：用户可以根据需要自定义区间，当用户明确希望观察某些区间范围内的数据分布时，使用这种方法可以方便地帮助用户达到目的。
箱1：800 箱2：1000 1200 1500 1500 1800 2000 箱3：2300 2500 2800 3000 箱4：3500 4000 箱5：4500 4800 5000
1.3.4数据平滑方法。
按平均值平滑 ：对同一箱值中的数据求平均值，用平均值替代该箱子中的所有数据。
按边界值平滑：用距离较小的边界值替代箱中每一数据。
按中值平滑：取箱子的中值，用来替代箱子中的所有数据。
1.4数据集成
数据集成中的两个主要问题是：
如何对多个数据集进行匹配，当一个数据库的属性与另一个数据库的属性匹配时，必须注意数据的结构；
数据冗余。两个数据集有两个命名不同但实际数据相同的属性，那么其中一个属性就是冗余的。
1.5数据变换
数据变换策略主要包括以下几种：
光滑：去掉噪声；
属性构造：由给定的属性构造出新属性并添加到数据集中。例如，通过“销售额”和“成本”构造出“利润”，只需要对相应属性数据进行简单变换即可
聚集：对数据进行汇总。比如通过日销售数据，计算月和年的销售数据；
规范化：把数据单按比例缩放，比如数据标准化处理；针对数值型的数据
离散化：将定量数据向定性数据转化。比如一系列连续数据，可用标签进行替换（0,1）；
对于文本型数据，若包含的是类别的信息，可以采用one-hot的编码方式，对其进行处理；对于分类别信息，则可以视具体的属性的含义，将其转换为数值型，如:姓名属性可以转换为词频的统计。
1.6数据规约
数据归约通常用维归约、数值归约方法实现。维归约指通过减少属性的方式压缩数据量，通过移除不相关的属性，可以提高模型效率。常见的维归约方法有：分类树、随机森林通过对分类效果的影响大小筛选属性；小波变换、主成分分析通过把原数据变换或投影到较小的空间来降低维数。
1.7数据可视化
我们将绘制图表，找出特征信号（比如，模式，可分性，特征和目标之间的关系，不同特征间的关系等等）和波动（比如，噪声量，数据分布等）。
数据可视化常用方法
箱线图（Boxplot／violinplot）；
直方图（histogram）；
散点图矩阵（scatter plot matrice／splom）；
径向坐标可视化（radviz）；
平行坐标图（parallel coordinate）；
双标图（jointplot）
箱线图
箱线图可以看出数据的集中趋势、分布和异常点。
Violinplot提供了的传统箱线图，除了提供前面的信息，也反映相对密度估计，这对判断特征的可分性很有用。violin的两边显示了分类变量的分布，这对二分类特有用。使用sns.violinplot代替sns.boxplot即可绘制Violinplot图。
直方图
直方图显示根据每个特征的组距值放入不同的直条，并根据每个直条的频率来计算直条值。下面绘制信用卡默认支付数据集的年龄特征的直方图
散点图矩阵
散点图矩阵是非常值得推荐的特征分析工具。我们在散点图中把所有特征配对（每两两特征组合画在一个矩阵中）绘制，对角一般留白或者用来显示核密度估计、直方图或者特征标注。散点图矩阵可以检查两两不同特征之间的关系。我们从散点图矩阵中找出协方差，线性关系、二次关系或者指数关系，同方差或者异方差（代表特征之间分散的程度）。
径向坐标可视化（radviz）
径向坐标可视化是基于弹簧张力最小化算法。它把数据集的特征映射成二维目标空间单位圆中的一个点，点的位置由系在点上的特征决定。把实例投入圆的中心，特征会朝圆中此实例位置（实例对应的归一化数值）“拉”实例。
平行坐标图
平行坐标图，类似于radviz图，是可视化数据集聚类的方法。数据点表示为可连接的线段，x轴的单位没有实际意义，每个竖直线代表一个属性。连接线段的一个集合代表一个实例。紧挨着的点聚成一类，相同颜色的线意味着好的分散性。
双标图（jointplot）
一般来讲，维度数目必须通过技术（比如，层次聚合，降维（例如，PCA和LDA）和维度裁剪）来减少。对于维度裁剪，可以使用散点图矩阵生成小倍数的特征。另外一种可行的办法是用双标图来检测每两两特征间的相关性。
2.特征工程
2.1 特征工程定义
特征工程是将原始数据转化为特征，更好表示预测模型处理的实际问题，提升对于未知数据的准确性。它是用目标问题所在的特定领域知识或者自动化的方法来生成、提取、删减或者组合变化得到特征。
2.2特征工程的重要意义
数据特征会直接影响你使用的预测模型和实现的预测结果。准备和选择的特征越好，则实现的结果越好。
影响预测结果好坏的因素：模型的选择、可用的数据、特征的提取
优质的特征往往描述了数据的固有结构。
大多数模型都可以通过数据中良好的结构很好的学习，即使不是最优的模型，优质的特征也可以得到不错的效果。优质特征的灵活性可以让你使用简单的模型运算的更快，更容易理解，更容易维护。
优质的特征可以在使用不是最优的模型参数的情况下得到不错的预测结果，这样你就不必费力去选择最适合的模型和最优的参数了。
2.3特征工程的内容
特征选择
不同的特征对模型的准确度的影响不同，有些特征与要解决的问题不相关，有些特征是冗余信息，这些特征都应该被移除掉。
特征选择是自动地选择出对于问题最重要的那些特征子集的过程。
特征选择算法可以使用评分的方法来进行排序；还有些方法通过反复试验来搜索出特征子集，自动地创建并评估模型以得到客观的、预测效果最好的特征子集；还有一些方法，将特征选择作为模型的附加功能，像逐步回归法(Stepwise regression)
就是一个在模型构建过程中自动进行特征选择的算法。
特征提取
一些观测数据如果直接建模，其原始状态的数据太多。像图像、音频和文本数据，如果将其看做是表格数据，那么其中包含了数以千计的属性。
特征提取是自动地对原始观测降维，使其特征集合小到可以进行建模的过程。
对于表格式数据，可以使用主元素分析(Principal Component Analysis)、聚类等映射方法；对于图像数据，可以进行线(line)或边缘(edge)的提取；根据相应的领域，图像、视频和音频数据可以有很多数字信号处理的方法对其进行处理。
特征构建
特征重要性和选择是告诉使用者特征的客观特性，但这些工作之后，需要你人工进行特征的构建。
特征构建需要花费大量的时间对实际样本数据进行处理，思考数据的结构，和如何将特征数据输入给预测算法。
对于表格数据，特征构建意味着将特征进行混合或组合以得到新的特征，或通过对特征进行分解或切分来构造新的特征；对于文本数据，特征够自己按意味着设计出针对特定问题的文本指标；对于图像数据，这意味着自动过滤，得到相关的结构。
特征学习
特征学习是在原始数据中自动识别和使用特征。
现代深度学习方法在特征学习领域有很多成功案例，比如自编码器和受限玻尔兹曼机。它们以无监督或半监督的方式实现自动的学习抽象的特征表示（压缩形式），其结果用于支撑像语音识别、图像分类、物体识别和其他领域的先进成果。
抽象的特征表达可以自动得到，但是你无法理解和利用这些学习得到的结果，只有黑盒的方式才可以使用这些特征。你不可能轻易懂得如何创造和那些效果很好的特征相似或相异的特征。这个技能是很难的，但同时它也是很有魅力的，很重要的。
2.4特征工程的流程
数据的转换流程
选择数据：收集整合数据，将数据规划化为一个数据集
预处理数据：对数据进行清洗、格式化、采样
转换数据：特征工程所在
对数据建模：构建模型、评估模型、调整模型
特征工程迭代过程
对特征进行头脑风暴：深入分析问题，观察数据特点，参考其他问题的有关特征工程的方法并应用到自己问题中
特征的设计：你可以自动提取特征，手动构造特征，或将两者相结合
特征选择：使用不同的特征重要性评分方法或特征选择方法
评估模型：利用所选择的特征对测试数据进行预测，评估模型准确性
3.模型选择
问题导读
1.机器学习常见的分类有哪些？
2.如何确定自己当前的问题需要哪类算法来解决？
3.不同类别的机器学习算法下分别包括哪些算法？
4.为什么要对统一数据采用多种可用的算法？
5.特征工程是用来干嘛的？
6.如何进行超参数的优化？
3.1机器学习分类
在我们深入之前，我们要明确我们了解了基础知识。具体来说，我们应该知道有三个主要的机器学习分类：监督学习（supervised learning）、无监督学习（unsupervised learning），以及强化学习（reinforcement learning)
监督学习：每个数据点被标记或者与一个类别或者感兴趣值相关联。分类标签的一个例子是将图像指定为“猫”或者“狗”。价值标签的一个例子是销售价格与二手车相关联。监督学习的目标是研究许多这样的标记示例，进而能够堆未来的数据点进行预测，例如，确定新的照片与正确的动物（分类（classification））或者指定其他二手车的准确销售价格（回归（regression））。
无监督学习：数据点没有标签对应。相反，一个无监督学习算法的目标是以一些方式组织数据或者表述它的结构。这意味着将其分组到集群内部，或者寻找不同的方式查看复杂数据，使其看起来更简单。
强化学习：对应于每一个数据点，算法需要去选择一个动作。这是一种常见的机器人方法，在一个时间点的传感器读数集合是一个数据点，算法必须选择机器人的下一个动作。这也是很普通的物联网应用模式，学习算法接收一个回报信号后不久，反馈这个决定到底好不好。基于此，算法修改其策略为了达到更高的回报.
3.2对问题进行分类
下一步，我们要对手头上的问题进行分类。这是一个两步步骤：
- 通过输入分类：如果我们有标签数据，这是一个监督学习问题。如果我们有无标签数据并且想要去发现结构，这是一个无监督学习问题。如果我们想要通过与环境交互优化目标函数，这是一个强化学习问题。
- 通过输出分类：如果一个模型的输出是一个数字，这是一个回归问题。如果模型的输出是一个类（或者分类），这是一个分类问题。如果模型的输出是输入组的集合，这是一个分类问题。
就是那么简单。总而言之，我们可以通过问自己算法需要解决什么问题，进而发现算法的正确分类。
上面这张图包含了一些我们还没有讨论的技术术语：
分类（Classification）：当数据被用来预测一个分类，监督学习也被称为分类。这是一个例子当指定一张相作为“猫”或“狗”的图片。当只有两种选择时，称为二类（two-class）或二项式分类（binomialclassification）。当有更多类别的时候，当我们预测下一个诺贝尔物理学奖得住，这个问题被称为多项式分类（multi-classclassification）。
回归（Regression）：当一个值被预测时，与股票价格一样，监督学习也被称为回归。
聚类（Clustering）：非监督学习最常用的方法是聚类分析或者聚类。聚类是一组对象组的任务，在这样的一种方式下，在同一组中的对象（称为集群）是更加相似的（在某一种意义上），相比其他组（集群）里的对象。
异常检测（Anomalydetection）：需要在目标里找到不寻常的数据点。在欺诈检测里，例如，任何非常不寻常的信用卡消费模式都是可以的。可能的变化很多，而训练示例很少，这看起来不是一种可行方式了解欺诈活动。异常检测需要的方法是简单地了解什么是正常的活动（使用非欺诈交易历史记录），并且确定明显不同的内容。
3.3找到可用算法
现在我们有分类问题，我们可以使用工具去调研和验证算法是可行的和可实践的。
一些值得关注的算法是：
分类（Classification）：
支持向量机（SVM）：通过尽可能宽的边缘方式发现分离类的边界。当二分式不能清晰的切分时，算法找到最好的边界。这个算法真正的亮点是强烈的数据特征，好像文本或者染色体组（>100特性）。在这些情况下，SVMs比其许多其他算法更快递切分二项，也更少地过度拟合，除了需要少量的内存。
人工神经网络（Artificial neural networks）：是大脑启发学习算法，覆盖多项式分类、二项式分类，以及回归问题。它们带来了无限的多样性，包括感知和深度学习。它们花费很长时间进行训练，但是带来各种应用领域的先进性能。
逻辑回归（Logistic regression）：虽然包含‘回归’这个词看上去有点令人费解，逻辑回归事实上是一个对于二项式和多项式分类来说强大的功能。它很快和简单。事实是它使用了‘S’形曲线代替直线让它对于切分数据进入组变得很自然。逻辑回归给出线性分类边界（linear class boundaries），所以当你使用它来确保一个线性近似的时候，类似于你生活中可以使用的一些东西。
决策树和随机树（Decision trees、random forests）：决策森林（回归、二项式，以及多项式），决策丛林（二项式、多项式），以及提高决策树（回归和二项式）所有被称为决策树，一种机器学习的基本概念。决策树的变种有很多，但是它们都做了相同的事情，使用相同的标签细分特征空间到各个区域。这些可以是一致类别或者恒定值的区域，依赖于是否你正在做分类或者回归。
回归（Regression）：
线性回归（Linearregression）：线性回归拟合直接（或者平台，或者超平面）数据集。这是一个工具，简单而快速，但是对于一些问题可能过于简单。
贝叶斯线性回归（Bayesian linearregression）：它有非常可取的品质，避免了过度拟合。贝叶斯方式实现它通过对可能分布的答案作出一些假设。这种方式的其他副产品是它们有很少的参数。
提高决策树回归：如上所述，提高决策树（回归或二项式）是基于决策树的，并通过细分大多数相同标签的特征空间到区域完成。提高决策树通过限制它们可以细分的次数和每一个区域的最小数据点数量避免过度拟合。算法构造一颗序列树，每一颗树学习补偿树前留下的错误。结果是非常准确的学习者，该算法倾向于使用大量内存。
聚合（Clustering）：
层次聚类（Hierarchicalclustering）：层次聚类的试图简历一个层次结构的聚类，它有两种格式。聚集聚类（Agglomerativeclustering）是一个“自下而上”的过程，其中每个观察从自己的聚类开始，随着其在层次中向上移动，成对的聚类会进行融合。分裂聚类（Divisiveclustering）则是一种“自顶向下”的方式，所有的观察开始于一个聚类，并且会随着向下的层次移动而递归式地分裂。整体而言，这里进行的融合和分裂是以一种激进的方式确定。层次聚类的结果通常表示成树状图（dendrogram）形式。
k-均值聚类（k-meansclustering）的目标是将n组观测值分为k个聚类，其中每个观测值都属于其接近的那个均值的聚类，这些均值被用作这些聚类的原型。这会将数据空间分割成Voronoidan单元。
异常检测（Anomaly detection）：
K最近邻（k-nearestneighbors/k-NN）是用于分类和回归的非参数方法。在这两种情况下，输入都是由特征空间中与k最接近的训练样本组成的。在k-NN分类中，输出是一个类成员。对象通过其k最近邻的多数投票来分类，其中对象被分配给k最近邻并且最常见的类（k是一个正整数，通常较小）。在k-NN回归中，输出为对象的属性值。该值为其k最近邻值的平均值。
单类支持向量机（One-classSVM）：使用了非线性支持向量机的一个巧妙的扩展，单类支持向量机可以描绘一个严格概述整个数据集的边界。远在边界之外的任何新数据点都是足够非正常的，也是值得特别关注的。
3.4尝试所有可用算法
对于给定的问题，通常会有一些候选算法可以适用。所以我们如何知道哪一个可以挑选？通常，这个问题的答案不是那么直截了当的，所以我们必须反复试验。
原型开发最好分两步完成。第一步，我们希望通过最小化特征工程快速而简单地完成几种算法的实现。在这个阶段，我们主要兴趣在粗略来看那个算法表现更好。这个步骤有点类似招聘：我们会尽可能地寻找可以缩短我们候选算法列表的理由。
一旦我们将列表缩减为几个候选算法，真正的原型开发开始了。理想地，我们想建立一个机器学习流程，使用一组经过精心挑选的评估标准比较每个算法在数据集上的表现。在这个阶段，我们只处理一小部分的算法，所以我们可以把注意力转到真正神奇的地方：特征工程
3.5 特征工程
或许比选择算法更重要的是正确选择表示数据的特征。从上面的列表中选择合适的算法是相对简单直接的，然而特征工程却更像是一门艺术。
主要问题在于我们试图分类的数据在特征空间的描述极少。利如，用像素的灰度值来预测图片通常是不佳的选择；相反，我们需要找到能提高信噪比的数据变换。如果没有这些数据转换，我们的任务可能无法解决。利如，在方向梯度直方图（HOG）出现之前，复杂的视觉任务（像行人检测或面部检测）都是很难做到的。
虽然大多数特征的有效性需要靠实验来评估，但是了解常见的选取数据特征的方法是很有帮助的。这里有几个较好的方法：
主成分分析（Principal componentanalysis，PCA）：一种线性降维方法，可以找出包含信息量较高的特征主成分，可以解释数据中的大多数方差。
尺度不变特征变换（Scale-invariant featuretransform，SIFT）：计算机视觉领域中的算法，用以检测和描述图片的局部特征。它有一个开源的替代方法ORB（Oriented FAST and rotated BRIEF）。
加速稳健特征（Speeded up robust features，SURF）：SIFT 的更稳健版本。
方向梯度直方图（Histogram of orientedgradients，HOG）：一种特征描述方法，在计算机视觉中用于计数一张图像中局部部分的梯度方向的发生。
更多算法请参考：[url=]https://en.wikipedia.org/wiki/Visual_deor[/url]
当然，你也可以想出你自己的特征描述方法。如果你有几个候选方法，你可以使用封装好的方法进行智能的特征选择。
前向搜索：
最开始不选取任何特征。
然后选择最相关的特征，将这个特征加入到已有特征；计算模型的交叉验证误差，重复选取其它所有候选特征；最后，选取能使你交叉验证误差最小特征，并放入已选择的特征之中。
重复，直到达到期望数量的特征为止！
反向搜索：
从所有特征开始。
先移除最不相关的特征，然后计算模型的交叉验证误差；对其它所有候选特征，重复这一过程；最后，移除使交叉验证误差最大的候选特征。
重复，直到达到期望数量的特征为止！
使用交叉验证的准则来移除和增加特征！
4.寻找最佳超参数
最后，你可能想优化算法的超参数。例如，主成分分析中的主成分个数，k 近邻算法的参数 k，或者是神经网络中的层数和学习速率。最好的方法是使用交叉验证来选择。
一旦你运用了上述所有方法，你将有很好的机会创造出强大的机器学习系统。但是，你可能也猜到了，成败在于细节，你可能不得不反复实验，最后才能走向成功。
相关知识
二项式分类（binomial classification）：
适用环境：
各观察单位只能具有相互对立的一种结果，如阳性或阴性，生存或死亡等，属于两分类资料。
已知发生某一结果（阳性）的概率为p，其对立结果的概率为1−p，实际工作中要求p是从大量观察中获得比较稳定的数值。
n次试验在相同条件下进行，且各个观察单位的观察结果相互独立，即每个观察单位的观察结果不会影响到其他观察单位的结果。如要求疾病无传染性、无家族性等。
符号：b(x,n,p)概率函数*：Cxnpxqn−x，其中x=0,1,⋯,n为正整数即发生的次数，Cxn=n!x!(n−x)!
例题：掷硬币试验。有10个硬币掷一次，或1个硬币掷十次。问五次正面向上的概率是多少?
解：根据题意n＝10，p＝q＝12，x＝5　　b(5,l0,12)=C510p5q10=10!(5!(10−5)!)×(12)5×(12)5=252×(132)×(132)=0.2469所以五次正面向上的概率为0．24609
Support vector machines
是一种监督式学习的方法，可广泛地应用于统计分类以及回归分析。支持向量机属于一般化线性分类器，也可以被认为是提克洛夫规范化（Tikhonov Regularization）方法的一个特例。这族分类器的特点是他们能够同时最小化经验误差与最大化几何边缘区，因此支持向量机也被称为最大边缘区分类器。
5.模型分析和模型融合