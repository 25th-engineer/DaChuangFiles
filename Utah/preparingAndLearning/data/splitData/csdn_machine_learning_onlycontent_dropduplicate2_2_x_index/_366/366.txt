机器学习可以概括如下几个层次
学术研究者
他们的工作是从理论上诠释机器学习的各个方面，试图找出“这样设计模型/参数为什么效果更好”，并且为其他从业者提供更优秀的模型，甚至将理论研究向前推进一步。
能够做到这一步的人，可以说凤毛麟角，天赋是绕不过去的大山，机遇和努力也缺一不可。
算法改进者
他们也许无法回答出“我的方法为什么work”，也许没有Hinton，LeCun那样足以载入史册的重大成果，但是却能根据经验和一些奇思妙想，将现有的模型玩出更好的效果，或者提出一些改进的模型。
这些人通常都是各个机器学习巨头公司的中坚力量或者成长中的独角兽，使用什么模型对他们来讲也不是问题，根据所处的环境，通常都有固定的几个选择。
在这个层面，insight和idea才是重要的东西，各种工具的区别，影响真的没那么大。可能会让一个结果早得到或者晚得到几天或者几周，却不可能影响“有没有成果”。
工业实现者
这些人基本上不会在算法领域涉入太深，也就是了解一下各个算法的实现，各个模型的结构。他们更多地是根据论文去复现优秀的成果，或者使用其他人复现出来的成果，并且试图去在工业上应用它。
勿在浮沙筑高台
基础知识
入门，了解概念，算法基础
Coursera 机器学习课程 - by 吴恩达
Machine Learning Coursera
https://www.coursera.org/learn/machine-learning
进阶，多层神经网络，卷积和softmax回归
斯坦福机器学习课程 UFLDL
UFLDL Tutorial
http://ufldl.stanford.edu/wiki/index.php/UFLDL_Tutorial
重点关注其中的softmax回归，卷积和池化这几个章节
进阶，计算机视觉，卷积神经网络的新进展，实现和应用
斯坦福计算机视觉课程 CS231n
斯坦福CS231n -深度学习与计算机视觉 - 网易云课堂
http://study.163.com/course/introduction/1003223001.htm
上面的课程大概会消耗你1到2个月的所有业余时间，网络上的课程非常多，也有很多优秀的免费课程，但是作为入门，我没有找到过比这三个更适合的。
会算矩阵乘法
在这个框架高度封装的年代，梯度不需要自己算，损失不需要自己求，反向传导更是被处理得妥妥的，在不求甚解的情况下，你甚至只需要知道这么几个概念就可以开始着手写第一个程序了：
它就是通过一系列矩阵运算（或者类似的一些其他运算）将输入空间映射到输出空间而已。参与运算的矩阵的值称为权重，是需要通过不断迭代来寻找到最优值。
当前的权重值离最优值还差多远，用一个数值来表示，这个值就叫损失，计算这个值的函数叫损失函数。
当前的权重值应该调大还是调小，这个值通过对损失函数求导来判断，这个求导得到的函数叫做梯度。
通过损失和梯度来更新权重的方法叫做反向传导。
迭代的方法称为梯度下降。
选择框架
目前机器学习的框架非常的多，从面向的使用者这个维度去划分，大体上分成这么两个阵营：
学术友好型：Theano，Torch，Caffe
学术研究时，弄出来一个新模型，新算法，新函数是常有的事，做出新的突破也是学术研究的最基本要求。所以，这些框架通常都可以方便地定制模型，也可以深入修改内部实现。很多新成果都会在发表论文的同时，提供这些框架上的实现代码供参考。性能方面也是比较出色。
而代价就是，要么使用了困难（Caffe：C++）或者小众（Torch：Lua）的开发语言界面，要么具有一些古怪的缺点（Theano：编译超级慢）。
而且，这些框架似乎都没怎么考虑过“怎么提供服务”的问题。想要部署到服务器上？Caffe已经是最简单的了，然而仍然要经历漫长而痛苦的摸索历程。
工业友好型：Tensorflow，MXnet，Caffe
工业上往往更注重“把一个东西做出来，并且让它运行得良好”。所以这些框架首先就需要支持并行训练。其中Tensorflow和MXnet支持多机多卡、单机多卡，多机单卡并行，Caffe支持单机多卡。虽然这些性能都不是特别理想。
在我们的测试中，Tensorflow的双卡并行只能达到单卡的1.5倍左右性能，卡越多，这个比例越低。
Caffe要好一些，但是参数同步和梯度计算无论如何也都需要时间，所以没有哪个框架能没有性能损失地实现扩展。
而多机的情况下，性能损失更大，很多时候都让人感到无法接受（这方面的优化只能以后有机会再提，如果有这方面的同好，欢迎讨论）。
相对来说，Tensorflow提供了比较好的部署机制（Serving），并且有直接部署到移动端的方案。而MXnet和Caffe则是直接编译的方式，虽然也可以实现，但是说实话，还是很麻烦。
至于缺点，就是除了Caffe之外，其他两种框架对于学术界的动态跟踪的都不太紧，Tensorflow到现在都没有pRelu的官方实现，在前阵子也才刚刚推出了一系列检测（Detection）的模型。
MXnet这一点要积极些，可是受限于较小的开发者社区，很多成果都只能等待大神们的contribution或者自行实现。
这样看来，难道最好的框架是Caffe？兼顾学术和实现，灵活性和性能兼备……
我得说，我的确是这么认为的。当然有一个前提，得懂C++……
如果不是C++开发人员出身，这门语言也不比机器学习容易多少了。
对于大多数有志于投身于机器学习开发（而不是研究）的同学们来说，我推荐首选Tensorflow作为你的第一个开发框架。
除了上述的优点之外，最主要的因素是，它人气高。在遇到任何问题的时候，你都会找到一群志同道合的伙伴们去咨询或者一起研究。这对于初学者而言，重要程度不言而喻。
好吧，选择就是这么的简单粗暴。
此外，还有一个良心建议，不论你选择哪个框架，千万不要试图在windows上运行它。哪怕是号称支持windows的MXnet或者新版Tensorflow，不要问我怎么知道……还是去装个Linux系统吧。建议使用ubuntu14.04或16.04。
学习机器配置选择
如果仅是入门和学习的话，CPU或者GPU完全不影响对代码和框架的学习。运行起类似Mnist或者Cifar之类的玩具数据集，其实差距也并不大。比如在我的机器上，运行自带的Cifar demo，i7 CPU和GTX 1080 Ti的速度大概 770 pics/s VS. 2200 pics/s。GPU大概是不到三倍的性能优势。
不过这里有一个小窍门，如果想用CPU版本的Tensorflow，最好不要使用pip下载的方式，而是用自行编译的方法。因为在开发机上编译时它会自动打开所有支持的加速指令集(SSE4.1/SSE4.2/AVX/AVX2/FMA)，从而使CPU的运算大大加快。
根据我们的测试，在打开全部加速指令集的情况下，训练速度大概会有30%的提升，而预测的速度大概会提升一倍。
当然，如果真的想要使用一个复杂模型去处理实际的生产问题，模型的复杂度和数据量都不是Cifar这样的玩具数据集可以相比的。如果用我们的一个模型同样去运行Cifar数据集，其他参数和条件完全相同，它在i5/i7/960/GTX1080/GTX1080Ti的速度分别是（单位还是pics/s，越大越好）：
19/25/140/460/620
这个时候大概就能看出差距了，1080Ti大概是i7 CPU的25倍。而在模型上线使用（inference）时，GPU也会有10-20倍的性能优势。模型越复杂，GPU的优势越明显。
综合这些来看，如果仅仅是入门时期的学习，我建议先不用专门购买带GPU的机器，先用你现有的机器，使用CPU版本，去学习框架和一些基础。等到你对基础已经掌握得比较扎实，那么自然就会有跑一些更复杂的模型和更“真实”的数据的想法，这个时候，可以考虑买一块GPU，来缩短训练的时间。
在选择GPU时，我听过一些朋友们推荐GTX1070 x 2这种选择。从理论上讲，1070的性能大概能达到1080的75%，而价格只是1080的一半，从各个方面来看，似乎都是双1070更有优势。
然而不要忘记，双卡的性能是不可能达到单卡的2倍的，在目前的Tensorflow上，大概只能达到1.5倍上下，算下来其实和1080单卡差不多。而双显卡的主板和电源以及机箱散热都需要更多的考虑，从性价比上来看未必真的划算。
不过，如果显卡预算刚好卡在5000-6000这个档位，双1070也有它的优势。比如可以学习使用多显卡并行计算的用法，在不着急的时候可以同时用两块显卡跑两个不同的任务，合并起来就相当于有了16G的显存等等。考虑到这些因素，双1070的确是最适合入门学习的选择——如果买不起双1080/双TITAN的话（哈哈哈）。
如果你打算用笔记本来作为主力学习用机的话，我的建议是：最好不要，除非你使用Linux经验很丰富或者不打算用GPU加速。很多笔记本在安装Liunx之后出现驱动方面的问题，而且使用GPU加速时的高热量也会非常影响系统的稳定性。如果没有很丰富的经验，经常会在一个问题上卡掉几个小时宝贵的学习时间。
安装Tensorflow时的那些坑
一般来说，严格按照官网说明，在干净的系统上通过PIP安装CPU版，都不会遇到什么问题。
新手常犯的错误是忘记了执行：导致在安装tensorflow时找不到。
sudo pip install –upgrade pip
而GPU版本最常见的坑是：
忘记关闭lightdm就去装驱动
这个不要紧，执行sudo stop lightdm
就好了。ubuntu 16.04用sudo systemctl stop lightdm
安装CUDA时的第二个询问
Install NVIDIA Accelerated Graphics Driver for Linux-x86_64 xxx.xx？
这里输入yes是错的！
记住输入no！一定不要安装CUDA自带驱动。这一点格外重要，装上去以后往往就卡在GUI输入密码界面死循环了。
入门数据集选择
mnist？cifar？ImageNet？COCO？这些都是啥?
MNIST
不论选择哪本教材，哪个框架，在刚刚接触机器学习的时候，一定会接触到Mnist（读作M- nist）这个名字。
这是个由Yann LeCun（读成杨乐坤，不是严乐村）建立的手写数字库，每条数据是固定的784个字节，由28x28个灰度像素组成，大概长这样：
目标是对输入进行10-分类，从而输出每个手写数字所表达的真实数字。因为它体积小（10M左右），数据多（6万张训练图片），适用范围广（NN/CNN/SVM/KNN都可以拿来跑跑）而天下闻名，其地位相当于机器学习界的Hello World。在LeCun的Mnist官方网站上，贴有各种模型跑这个数据集的最好成绩，当前的最好得分是CNN的大概99.7%。
MNIST handwritten digit database, Yann LeCun, Corinna Cortes and Chris Burgeshttp://yann.lecun.com/exdb/mnist/
因为这个数据集实在非常的小，所以即使在CPU上，也可以用几秒钟时间跑完NN的训练，或者几分钟跑完一个简单的CNN的模型。
CIFAR
而对于打算从图像方面入手的同学，Cifar（读做see far）数据库则是更好的入门选项。
官网：CIFAR-10 and CIFAR-100 datasets
http://www.cs.toronto.edu/~kriz/cifar.html
这个数据库分为2个版本，CIFAR-10和CIFAR-100，顾名思义，CIFAR-10有10个分类，每个分类有5000张训练图片和1000张测试图片，每张图片是32x32像素的3通道位图。大概长这样：
而CIFAR-100有100个分类，每个分类变成了500张训练图片+100张测试图片，而图片的大小并没什么变化。
之所以说它比Mnist更适合作为图片处理的入门，是因为它虽然分辨率低了些，但是却是三通道，真实拍摄的照片。其中有些图片的背景还略微复杂，更贴近我们真实的图片处理场景。相对而言，Mnist的灰度输入和干净背景就显得有些过于简单，而且99.7%的准确率也确实很难有提升的空间。
Tensorflow给出了Cifar的例程：
https://www.tensorflow.org/tutorials/deep_cnn
并附有代码 ：tensorflow/models
https://github.com/tensorflow/models/tree/fb96b71aec356e054678978875d6007ccc068e7a/tutorials/image/cifar10
ImageNet 和 MS COCO
至于ImageNet（ImageNet）和COCO（http://mscoco.org/），则是两个工业级别的图像数据集。通常我们提到他们时，ImageNet指的是ILSVRC2012的训练集，而COCO则是COCO-2014训练集。
ImageNet具有大量的图片（一百多万张，分成1000个分类）和标注，大部分都是这样的：
COCO虽然图片数量少一些（8万多张，80个分类），但是每张图片都有轮廓标记，并且附带分类标注和5句描述话语（英文的）。大概是这样的：
所以当我们进入实际的工作时，可以根据具体的需要从中选择适合自己的数据集作为benchmark或者pretrain数据集。
运行一个CIFAR demo与显卡、显存的分配
在Tenforflow 安装完成后，我们可以用这种方式最快地跑起来第一个Cifar demo：
git clone https://github.com/tensorflow/models.git my_models
cd my_models/tutorials/image/cifar10/
python cifar10_train.py
OK，只需几分钟下载数据，我们就可以看到我们的第一个“图像识别模型”正在训练了。
训练过程中我们可以看到log中在不断地输出loss信息，但是我们除了想要跟踪loss之外，还希望能够看到当前训练的模型到底识别的准确率如何，这个就不是cifar10_train.py这个脚本能提供的了。我们还需要执行
python cifar10_eval.py
这个脚本会不断地验证最近的检查点的识别准确率。
如果使用GPU的话，会发现在运行起来训练脚本之后，所有的显存都已经被这个进程占满，再启动验证脚本的话会报错一大堆的内存不足（OOM)，这是Tensorflow的机制决定的，它会默认占据所有显卡的所有显存，而不管自己是否真的用到那么多。
解决这个问题的办法也很简单。
首先，我们可以指定Tensorflow使用哪几块显卡进行训练。要做到这一点，可以在执行较本前，用命令行指定环境变量：
export CUDA_VISIBLE_DEVICES=”0,2”
其中的“0，2”就是希望使用的GPU编号，从0开始，用逗号分隔开。
或者在代码中创建一个GPUOption，设置visible_device_list=‘0,2’，也能起到同样的效果。
然后，我们可以限制Tensorflow使用的显存，使其动态增长而不是启动就占满。方法和上面的类似，代码中创建一个GPUOption，并设置allow_growth=True即可。
官方的Cifar例程大概能达到86%的准确率，这个成绩在现在可以说算是比较差的了，最新的模型通常都有97%左右的准确率，即使是不仔细调参随便训训，也能轻松达到93%左右，大家可以尝试着修改cifar10.py中定义的模型，得到更好的效果。
那么，在运行完这个例子之后，其实你已经可以算是机器学习工程师中的一员了。
接下来就可以收集一些自己的数据，并且训练一些自己的识别引擎；或者尝试着优化这个模型，感受一下所谓调参党的痛苦；又或者直接尝试实现ResNet、Inception这些更为先进的网络来刷刷Cifar；再不然可以尝试着向NLP或者强化学习方向去学习一下。
总之，这些事情远没有看起来那么难。当然，不论那条路，学习，进步和自我鞭策都是逃避不掉的必修课。
对于坚强地完成了所有课程的同学们，我得说声恭喜，从我们掌握的数据来看，只有10%左右的现役程序员决定向AI转身，并付诸了行动；而能够完成基础课程的人，只占了其中的30%不到。也就是说，当你完成CS231n的最后一个作业开始，你就已经站在了开发者的Top 5%之列。
在这个层面上，通常就会面临两件更加困难的任务：追论文和补数学。
现在的机器学习界，“ArXiv醒”已经成了症候群，几乎每周都会有更先进的内容冒出来，隔一两个月就会发现很多公认的知识被刷新了，隔一两年？简直无法想象。所以跟踪学术界的动态就成了从业者的必修课。哪怕作为工业实现者，如果不能第一时间掌握学术界的动态，就一定会面临对手突然实力碾压自己的窘境。
好在机器学习界的风气和传统领域的学术界有一个很大的区别，得益于钢铁侠Elon Musk的OpenAI组织，学术界形成了ArXiv上预印论文和开源的风气。只是要想看懂这些论文和代码，前面提到的那些知识，却远远不够。看到这种论文（[1706.02515] Self-Normalizing Neural Networks)的时候，你才会真正理解什么叫“没个博士学位都搞不了这东西”：
[1706.02515] Self-Normalizing Neural Networks
https://arxiv.org/abs/1706.02515