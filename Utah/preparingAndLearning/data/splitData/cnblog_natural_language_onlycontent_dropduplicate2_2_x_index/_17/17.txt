1.
>>> s[:4]+'u'+s[-5:]
2.
>>> word1='dish-es' >>> word1 'dish-es' >>> word2='running' >>> 'undo'[:2] 'un' >>> 'pre-heat'[:3] 'pre' 3. >>> word1[-2] 'e' >>> word1[-8]
Traceback (most recent call last): File "<pyshell#100>", line 1, in <module> word1[-8] IndexError: string index out of range >>> word1[7] Traceback (most recent call last): File "<pyshell#101>", line 1, in <module> word1[7] IndexError: string index out of range >>>
4.
>>> monty='Monty Python' >>> monty[6:11:2] 'Pto' >>> monty[10:5:-2] 'otP'
5.
逆序输出
6.
a. [a-zA-Z]+           字母字符串
b. [A-Z][a-z]*         开头大写后小字母不限
c. p[aeiou]{,2}t        p开头t结尾中间有<=2个元音字幕
d. \d+(\.\d+)?          .数字可有可没有但是一定要有一个或多个数字。整数或者带小数的整数
e. ([^aeiou][aeiou][^aeiou])*   pot类似的
f. \w+|[^\w\s]+          字母一个多个或者不是字母空格的一个多个
>>> nltk.re_show('([^aeiou][aeiou][^aeiou])* ','poat',left='{',right='}') >>> nltk.re_show('([^aeiou][aeiou][^aeiou])*',out,left='{',right='}')
{}.{}T{hef}a{milyofDas}h{}w{}o{}o{}d{hadlon}g{}b{}e{}e{}n{set}t{led}i{}n{Sussex}.{}
>>> nltk.re_show('\w+|[^\w\s]+',out,left='{',right='}')
{.}{ThefamilyofDashwoodhadlongbeensettledinSussex}{.}
7.
1）.
>>> nltk.re_show('an?|the','thesisiaishihsthean',left='{',right='}')
{the}sisi{a}ishihs{the}{an}
2）.
>>> nltk.re_show('\d+\*\d+\+\d+','2*3+8',left='{',right='}')
{2*3+8}
8.
>>> from urllib import urlopen >>> def dealhtml(url): html=urlopen(url).read() raw=nltk.clean_html(html) tokens=nltk.word_tokenize(raw) text=nltk.Text(tokens) return text >>>url="http://news.baidu.com/z/2012europe/zhuanti.html"
9.
>>> def load(name): f=open(name) raw=f.read() return raw >>> print load('corpus.txt')
a.
>>> pattern = r'''(?x) # set flag to allow verbose regexps [][.,;"'?():-_`] # these are separate tokens ''' >>> nltk.regexp_tokenize(text, pattern)
b.
>>> pattern =r'''(?x) # set flag to allow verbose regexps ([A-Z]\.)+ # abbreviations, e.g. U.S.A. | [A-Z][a-z]*\s[A-Z][a-z]* # words with optional internal | \$?\d+(\.\d+)?%? # currency and percentages, e.g. $12.40, 82% | \d+-\d+-\d+ ''' >>> nltk.regexp_tokenize(text, pattern)
10.
>>> sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper'] >>> result=[] >>> for word in sent: result.append(format%(word,len(word))) >>> result
['(The,3),', '(dog,3),', '(gave,4),', '(John,4),', '(the,3),', '(newspaper,9),']
11.
>>> raw='Good muffins cost $3.88\\nin New York.Please buy me\\ntwo of them.\\n\\nThankes.' >>> raw.split('s')
['Good muffin', ' co', 't $3.88\\nin New York.Plea', 'e buy me\\ntwo of them.\\n\\nThanke', '.']
12.
>>> len(raw)
76
>>> for i in range(len(raw)): print raw[i]+"\n",
13.
>>> text.split() ['Good', 'muffi', 'ns', 'cost', '$3.88\\nin', 'New', 'York.Please', 'buy', 'me\\ntwo', 'of', 'them.\\n\\nThankes.'] >>> text.split(' ') ['Good', 'muffi', '', '', 'ns', 'cost', '$3.88\\nin', 'New', 'York.Please', 'buy', 'me\\ntwo', 'of', 'them.\\n\\nThankes.'] >>> >>> text='Good muffi \t ns co\ts' >>> text.split(' ') ['Good', 'muffi', '', '\t', 'ns', 'co\ts'] >>> text.split() ['Good', 'muffi', 'ns', 'co', 's'] >>>
14.
汗一个，没发现有什么区别
15.
>>> "3"*7 '3333333' >>> 3*7 21 >>> '3'*7 '3333333' >>> >>> int("3") 3 >>> str(3) '3' >>>
相当于强制转换类型
16.
我的不成功
17.
%6s与%-6s的区别在于-是左对齐
>>> '%6s' %'dog' ' dog' >>> '%6s'%'sdasdasdsds' 'sdasdasdsds' >>> '%-6s'%'sdasdasdsds' 'sdasdasdsds' >>> '%-6s' %'dog'' 'dog ' >>>
18.
>>> def load(name): f=open(name) raw=f.read() return raw >>> text=load('corpus.txt') >>> pattern =r'''(?x) # set flag to allow verbose regexps [Ww][Hh]\S+ # abbreviations, e.g. U.S.A. ''' >>> nltk.regexp_tokenize(text, pattern) ['What', 'while', 'who', 'who', 'who', 'who', 'who', 'when', 'when', 'who', 'which']
19.
>>> f=open('freq.txt').readlines() >>> f
['fuzzy 53\n', 'absent 46\n', 'lost 33\n', 'dead 17\n', 'over 32']
>>> words=[] >>> for str in f: words.append(str.split()) >>> result=[] >>> for [str,num] in words: word=str; intnum=int(num); result.append([word,intnum]) >>> print result
20.
>>> from urllib import urlopen >>> def dealhtml(url): html=urlopen(url).read() raw=nltk.clean_html(html) tokens=nltk.word_tokenize(raw) text=nltk.Text(tokens) return text >>> url="http://www.weather.com.cn/weather/101020100.shtml" >>> text=dealhtml(url) >>> print text >>> output_file = open('output.txt', 'w') >>> for word in text: output_file.write(word+'\s')
有个中文编码的问题。这个是个问题暂时不会等一会没事再研究一下
21.
>>> from urllib import urlopen >>> import nltk,re >>> url="http://www.bbc.co.uk/news/world-middle-east-18650775" >>> wordsres=[] >>> def unknown(url): html=urlopen(url).read() raw=nltk.clean_html(html) words=re.findall(r'[a-z]+',raw) wordlist=[w for w in nltk.corpus.words.words('en') if w.islower()] for word in words: if word not in wordlist: wordsres.append(word) return wordsres >>> wordsres=[] >>> wordsres=unknown(url)
23.
因为这个有问题n't|\w因为你正则表达式是r'  '，这样就结束了r｛'n'｝t|\w‘
>>> text="who is don't you know" >>> re.split('',text) ["who is don't you know"] >>> re.split(' ',text) ['who', 'is', "don't", 'you', 'know'] >>> re.findall(r'n\'t|\w+',text) ['who', 'is', 'don', 't', 'you', 'know'] >>> re.findall('n\'t'|r'\w+',text) Traceback (most recent call last): File "<pyshell#17>", line 1, in <module> re.findall('n\'t'|r'\w+',text) TypeError: unsupported operand type(s) for |: 'str' and 'str' >>>
24.
>>>result=[] >>>text='say what your classment' >>> words=text.split() >>> result=[] >>> for word in words: if 'e' in word: result.append(word.replace('e','3')) if 'ate' in word: result.append(word.replace('ate','8')) if '.' in word: result.append(word.replace('.','5w33t!')) if '1' in word: result.append(word.replace('1','|')) if 'o' in word: result.append(word.replace('o','0')) if word in re.findall(r'[^s\s]\w+s+\w+',text): result.append(word.replace('s','5')) if word.startswith('s'): result.append(word.replace(word[0],'$')) >>>result
但是这种方法会得到重复的值因此不合适要改
>>> for i in range(len(words)): if words[i] in re.findall(r'[^s\s]+s+\w+',text): words[i]=words[i].replace('s','5') if words[i].startswith('s'): words[i]=words[i].replace(words[i][0],'$') if 'e' in words[i]: words[i]=words[i].replace('e','3') if 'ate' in words[i]: words[i]=words[i].replace('ate','8') if '.' in words[i]: words[i]=words[i].replace('.','5w33t!') if '1' in words[i]: words[i]=words[i].replace('1','|') if 'o' in words[i]: words[i]=words[i].replace('o','0')
修改以后这个就可以了！终于对了
25.
a.
>>> def pig_word(word): for i in range(len(word)): if word[i] in '[AEIOUaeiou]': pigword=[word[i:],word[:i],'ay'] result=''.join(pigword) return result >>> >>> pig_word(word) 'ingstr'
b.
>>> re=[] >>> for str in text: re.append(pig_word(str)) >>> print re ['eTh', 'amilyf', 'of', 'ashwoodD', 'adh', 'ongl', 'eenb', 'ettleds', 'in', 'ussexS', None] >>>
C.
>>> def qupig_word(word): for i in range(len(word)): if word[i] in '[AEIOUaeiou]': pigword=[word[i+1:],word[:i+1],'ay'] result=''.join(pigword) return result >>> words=["yellow","happy","quiet"] >>> for word in words: if word.startswith('y'): re.append(pig_word(word)) if 'qu' in word.lower(): re.append(qupig_word(word)) else: re.append(pig_word(word)) >>> re
26.这个我不会。。真心不会，求解答。哪位好心人告诉我怎么办
27.
>>> str=[] >>> for i in range(500): str.append(random.choice("aehh ")) >>> str >>> ''.join(str) >>> word=str.split() >>> str=''.join(word) >>> str
28.
>>>import nltk >>>from nltk.corpus import brown >>>words1=[len(word) for word in nltk.corpus.brown.words(categories='lore')] >>>sents1=[len(sent) for sent in nltk.corpus.brown.sents(categories='lore')] >>>words2=[len(word) for word in nltk.corpus.brown.words(categories='learned')] >>>sents2=[len(sent) for sent in nltk.corpus.brown.sents(categories='learned')] >>> wordsum=0; >>> for wlength in words1: wordsum+=int(wlength) >>> for slength in sents1: sentsum+=slength >>> from __future__ import division >>> def ARI(uw,us): return 4.71*uw+0.5*us-21.43 >>> uw=wordsum/len(words1) >>> us=sentsum/len(sents1) >>> us 22.59762343782012 >>> ARI(uw,us) 10.254756197101155
30.
>>> saying=['After', 'all', 'is', 'said','and', 'done', ',', 'more','is', 'said', 'than', 'done', '.'] >>> porter=nltk.PorterStemmer() >>> lancaster=nltk.LancasterStemmer() >>> [porter.stem(word) for word in saying] ['After', 'all', 'is', 'said', 'and', 'done', ',', 'more', 'is', 'said', 'than', 'done', '.'] >>> [lancaster.stem(word) for word in saying] ['aft', 'al', 'is', 'said', 'and', 'don', ',', 'mor', 'is', 'said', 'than', 'don', '.'] >>>
31.
>>> saying=['After', 'all', 'is', 'said','and', 'done', ',', 'more', 'is', 'said', 'than', 'done', '.'] >>> length=[] >>> for say in saying: length.append(len(say)) >>> length
32.
a.
>>> silly='newly formed bland ideas are inexpressible in an infuriating way' >>> bland=silly.split() >>> bland
['newly', 'formed', 'bland', 'ideas', 'are', 'inexpressible', 'in', 'an', 'infuriating', 'way']
B.
>>> silly2=[] >>> for word in bland: silly2.append(word[1]) >>> silly2 ['e', 'o', 'l', 'd', 'r', 'n', 'n', 'n', 'n', 'a'] >>> ''.join(silly2)
'eoldrnnnna'
C.
>>> newsilly=' '.join(bland) >>> newsilly
'newly formed bland ideas are inexpressible in an infuriating way'
>>>
D.
>>> for word in bland: print word,'\n',
33.
a.
>>> 'inexpressible'.index('re')
5
b.
>>> bland.index('way')
9
C.
>>> silly.index('in ')
43
>>> phrase=[] >>> phrase=silly[:silly.index('in ')].split() >>> phrase
['newly', 'formed', 'bland', 'ideas', 'are', 'inexpressible']
34.
我把维基百科上面的表存到了country.txt这个文件里，这样就很好办了（某些有乱码）
>>> import nltk,re >>> f=open("countryname.txt") >>> f <open file 'countryname.txt', mode 'r' at 0x0000000005687930> >>> n=raw.split('\n') >>> def CTadjton(countryadj): for country in n: if re.findall(countryadj,country): countrypt=country.split() return countrypt[0] >>> countryadj='Canadian' >>> CTadjton(countryadj)
'Canada'
35.
>>>from nltk.corpus import brown >>>text=''.join(brown.words()) >>> re.findall(r'as best \w+ can',text) >>> re.findall(r'as best as \w+ can',text)
36.
喵星语搞不了、
37.
在Python根目录下见了一个test.html内容为
“
<html> this is a test </html>
”
>>> import re >>> f=open("test.html") >>> raw=f.read() >>> pattern=r'''(?x) <html> |</html> ''' >>> re.sub(pattern,'',raw)
'this is a test'