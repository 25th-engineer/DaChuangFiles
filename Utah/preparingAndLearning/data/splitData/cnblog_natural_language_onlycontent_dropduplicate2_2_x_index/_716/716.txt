自然语言处理的流程：
获得数据
预处理
去除不需要的内容，如HTML标签。
将文档分割成句子。将句子分割成词（tokenize）。
其他一些操作。如转换为小写，纠正拼写错误。
去掉长度过短的词。去掉停用词。
提取词干。
训练。
绘制结果。
中文分词的常见算法：
基于字符串匹配的方式：速度快，实现简单；对歧义和未登录词处理不好。
最简单的，最大匹配法：每次从词典中找最长的词匹配。
反例：“北京大学生前来应聘”本应是“北京／大学生／前来／应聘”，却会被误分成“北京大学／生前／来 ／应聘”。
可以维护特殊的规则表，如某个固定的组合应当如何划分；再如维护不单独成词的字表，如果发现字表中的字单独成字，需要修改分词结果。
稍好一点的，最少词数法：认为分出的词数量更少的方案更好。
可能会出现多种方案词数相同。
可以利用不单独成词字表：每个词罚一分，出现字表中的字加罚一分。选择罚分最少的方案。
基于统计和机器学习的方式：能很好处理歧义和未登录词问题，效果比前一类效果好，但是需要大量的人工标注数据，以及较慢的分词速度。
基于隐马尔可夫模型（HMM）。隐马尔可夫的内容可参考[6]。
条件随机场（CRF）：目前公认的效果最好的分词算法。
新词（未登录词）的发现：
常见的有人名、地名、机构名、专有名词等。
人名：有规律：姓氏、头衔、动作等。
地名：利用收录地名的资料。
还可以使用基于统计的方法：
互信息
互信息之统计模型中衡量2个随机变量X,Y之间的关联程度，而在新词的识别中则特指相邻2个词之间的关联程度。
MI(X，Y)=log2(p(X，Y)/p(X)*P(Y))
那么当『XY』未在已经训练完的词库中出现，且该互信息高于某一阀值值，那么我们就假定该词为新词。
频率
当某一组连续相邻的字在新的语料库或网络日志中大量出现而未在词库中登记时，那么我们也可假定该词为新词。
这种方法思路很简单，实现的代码也相当容易，但是该方法必须基于大规模的语料库才能准确识别。
停用词：
为节省存储空间和提高搜索效率，在处理自然语言数据（或文本）之前或之后会自动过滤掉某些字或词，这些字或词即被称为Stop Words(停用词)。停用词大致分为两类。一类是人类语言中包含的功能词，这些功能词极其普遍，与其他词相比，功能词没有什么实际含义，比如'the'、'is'、'at'、'which'、'on'等。；另一类词包括词汇词，比如'want'等，这些词应用十分广泛，但是对这样的词搜索引擎无法保证能够给出真正相关的搜索结果，难以帮助缩小搜索范围，同时还会降低搜索的效率，所以通常会把这些词从问题中移去，从而提高搜索性能。
将停用词分为三类：
首先自动识别：
如果是分类问题：通过互信息（MI）或者信息增益（IG）等算法来自动识别停用词。
如果不是：采用平均(tf-)idf 等算法来进行。
通过领域知识排除。
使用手工维护的列表。
自动识别停用词的方案主要有：
文档频数（ DF）：当一个词在大量文本中出现时,这个词通常被认为是噪声词。
词频（WF）：当一个词在大量出现时，通常被认为是噪声词。
熵计算：一个基于单词出现的平均信息量，对词的有效性进行计算。
联合熵：基于词在句子中出现的频率与该词包含个icid句子频率的联合熵分别计算词条在语料库中各个句子内发生的概率，以及包含该词条的句子在文本中发生的概率，计算他们的熵，并根据他们的联合熵选取停用词。
在唐诗自动分类中遇到的一些问题：
训练语料库：全唐诗。
停用词表的选择：古汉语停用词和现代汉语停用词都不太合适。
词的长度：一个字、两个字或三个字。
分词：使用结巴分词。它采用HMM。
新词的发现：使用互信息判断是否成词。
降维：去除停用词和低频词。
参考资料：
[1] http://zh.wikipedia.org/wiki/%E5%81%9C%E7%94%A8%E8%AF%8D
[2] https://github.com/chao787/RNote/blob/master/Blog/IR/stopword.org
[3] http://zhuanlan.zhihu.com/textmining-experience/19630762
[4] http://www.matrix67.com/blog/archives/4212
[5] http://www.zhihu.com/question/19578687
[6] http://blog.csdn.net/stdcoutzyx/article/details/8522078