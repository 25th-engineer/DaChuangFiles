1、NLP自然语言处理
2、NLTK功能：分类/分词/标签/提取/解析/语义推理
NLP的功能：
》分词
》标记
》提取
应用：情感分析-舆情分析
>>> s = SnowNLP('NLTK is a leading platform for building Python programs to work') >>> s.words ['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work'] >>> s.tags [('NLTK', u'o'), ('is', u'o'), ('a', u'o'), ('leading', u'o'), ('platform', u'o'), ('for', u'o'), ('building', u'o'), ('Python', u'o'), ('programs', u'o'), ('to', u'e'), ('work', u'e')] >>>
>>> s.pinyin ['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work'] >>> s.sentiments----情感指数 0.05432463924126918 >>>
eg:统计评论的情感值
from snownlp import SnowNLP l = ['want to die','i am happy','you are stupid'] result = [SnowNLP(s).sentiments for s in l] print result
每条评论的情感值[0.5, 0.18369201291839055, 0.5]
情感平均值
from snownlp import SnowNLP l = ['want to die','i am happy','you are stupid'] result = sum([SnowNLP(s).sentiments for s in l])/len(l) print result
0.394564004306
3、分析语法树
import nltk s = """The official home of the Python Programming Language... Looking for Python with a different OS? Python for Windows, Linux/UNIX, Mac OS X, Other...1""" #分词 words = nltk.word_tokenize(s) print(words) #打标签 tags = nltk.pos_tag(words) print(tags) ''' ['The', 'official', 'home', 'of', 'the', 'Python', 'Programming', 'Language', '...', 'Looking', 'for', 'Python', 'with', 'a', 'different', 'OS', '?', 'Python', 'for', 'Windows', ',', 'Linux/UNIX', ',', 'Mac', 'OS', 'X', ',', 'Other', '...', '1'] [('The', 'DT'), ('official', 'JJ'), ('home', 'NN'), ('of', 'IN'), ('the', 'DT'), ('Python', 'NNP'), ('Programming', 'NNP'), ('Language', 'NNP'), ('...', ':'), ('Looking', 'VBG'), ('for', 'IN'), ('Python', 'NNP'), ('with', 'IN'), ('a', 'DT'), ('different', 'JJ'), ('OS', 'NNP'), ('?', '.'), ('Python', 'NNP'), ('for', 'IN'), ('Windows', 'NNP'), (',', ','), ('Linux/UNIX', 'NNP'), (',', ','), ('Mac', 'NNP'), ('OS', 'NNP'), ('X', 'NNP'), (',', ','), ('Other', 'JJ'), ('...', ':'), ('1', 'CD')] '''
chunk = nltk.chunk.ne_chunk(tags) chunk.draw()
2、近似词处理
#词库 l = nltk.corpus.brown.words() #遍历 l2 = [w.lower() for w in l] #文本处理 text = nltk.Text(l2) #寻找和‘the’相似的词 print(text.similar('the'))
打印：
a his this their its her an that our any all one these my in your no
some other and
None
3、名字分析
import random import nltk import math from nltk.corpus import names #原始数据+特征提取 l = [({"last":name[-1]},'男') for name in names.words('male.txt')]+[({"last":name[-1]},'女') for name in names.words('female.txt')] #顺序随机 random.shuffle(l) #分成两半,测试集合和训练集 train_set =l[:math.floor(len(l)/2)] test_set = l[math.ceil(len(l)/2):] #通过训练集得到分类器 classfier = nltk.NaiveBayesClassifier.train(train_set) #测试分类器 print(classfier.classify({"last":'yewr'[-1]})) #测试 res = sum([1 if classfier.classify(features)==gender else 0 for(features,gender) in test_set])/len(test_set) print(res)
男
0.751007049345418
4、给定一个字符串，获取每个字母出现的次数
def features(s): #创建一个空字典，存放26个字母的次数 res = {} for w in "abcdefghijklmnopqrstovwxyz": #初始化每个字母出现的次数为0 res[w] = 0 #遍历字符串 for w in s: w = w.lower() #如果字符串中的字符存在于字典中 if w in res: #将对应的字母的次数加1 res[w] += 1 return res print(features('belueal')) # {'a': 1, 'b': 1, 'c': 0, 'd': 0, 'e': 2, 'f': 0, 'g': 0, 'h': 0, 'i': 0, 'j': 0, 'k': 0, 'l': 2, 'm': 0, 'n': 0, 'o': 0, 'p': 0, 'q': 0, 'r': 0, 's': 0, 't': 0, 'v': 0, 'w': 0, 'x': 0, 'y': 0, 'z': 0} #
改进：选取两个特征值进行分类
#原始数据+特征提取 l = [({"last":name[-1],"first":name[0]},'男') for name in names.words('male.txt')]+[({"last":name[-1],"first":name[0]},'女') for name in names.words('female.txt')] #顺序随机 random.shuffle(l) #分成两半,测试集合和训练集 train_set =l[:math.floor(len(l)/2)] test_set = l[math.ceil(len(l)/2):] #通过训练集得到分类器 classfier = nltk.NaiveBayesClassifier.train(train_set) #测试分类器 # print(classfier.classify(features('dsfhih'))) #测试 res = sum([1 if classfier.classify(features)==gender else 0 for(features,gender) in test_set])/len(test_set) print(res)
0.7663645518630413
5、利用多个特征值进行分类的优化（选取多个特征值）
#原始数据+特征提取 l = [(features(name),'男') for name in names.words('male.txt')]+[(features(name),'女') for name in names.words('female.txt')] #顺序随机 random.shuffle(l) #分成两半,测试集合和训练集 train_set =l[:math.floor(len(l)/2)] test_set = l[math.ceil(len(l)/2):] #通过训练集得到分类器 classfier = nltk.NaiveBayesClassifier.train(train_set) #测试分类器 print(classfier.classify(features('dsfhih'))) #测试 res = sum([1 if classfier.classify(features)==gender else 0 for(features,gender) in test_set])/len(test_set) print(res)
男
0.6865558912386707
6、中文分词jieba
#全模式 cut_all=True res = list(jieba.cut("新华社北京12月9日消息，中共中央政治局12月8日下午就实施国家大数据战略进行第二次集体学习。中共中央总书记***在主持学习时强调，大数据发展日新月异，我们应该审时度势",cut_all=True)) # ['新华', '新华社', '北京', '12', '月', '9', '日', '消息', '', '', '中共', '中共中央', '中共中央政治局', '中央', '中央政治局', '政治', '政治局', '12', '月', '8', '日', '下午', '就', '实施', '国家', '大数', '数据', '战略', res = list(jieba.cut("新华社北京12月9日消息，中共中央政治局12月8日下午就实施国家大数据战略进行第二次集体学习。中共中央总书记***在主持学习时强调，大数据发展日新月异，我们应该审时度势")) print(res)
7、中文分词-带词性
import jieba.posseg as posseg #分词 词性 print(list(posseg.cut("新华社北京12月9日消息，中共中央政治局12月8日下"))) # [pair('新华社', 'nt'), pair('北京', 'ns'), pair('12', 'm'), pair('月', 'm'), pair('9', 'm'), pair('日', 'm'), pair('消息', 'n'), pair('，', '
8、构建中文语法树
import nltk import jieba import jieba.posseg as posseg s=""" 新华社北京12月9日消息，中共中央政治局12月8日下午就实施国家大数据战略进行第二次集体学习 """ #[pair('新华社', 'nt'), pair('北京', 'ns'), pair('12', 'm') #这里的tags为pair类型 tags = [(item.word,item.flag) for item in list(posseg.cut(s))] #ne_chunk()函数里面的参数的类型需要是元组类型tuple chunk = nltk.chunk.ne_chunk(tags) chunk.draw()
语法树
9、新闻分类实战演练
#!/usr/bin/env python # -*- coding: utf-8 -*- # @Time : 2017/12/9 16:29 # @Author : Aries # @Site : # @File : NLPK1.py # @Software: PyCharm import os import math import nltk import jieba import random import jieba.posseg as posseg #1:读取文件 path = '/newstest/' #os.getcwd()获取当前路径 #os.listdir获取路径下的文件目录 types = os.listdir(os.getcwd()+path) # print(types) # ['hangkong', 'shehui', 'war'] #创建一个字典 datas = {} #循环遍历文件列表 for name in types: #将文件名作为字典的键 datas[name]=[] print(name) #打开文件 for filename in os.listdir(os.getcwd()+path+name): fp = open(os.getcwd()+path+name+'/'+filename,'r',encoding='utf-8') content =fp.read() fp.close() #将读取到的每个文件的内容添加到字典的对应的类型里面 datas[name].append(content) #对于字典的value里面又是一个由不同文件组成的列表 print(datas) # hangkong # {'hangkong': [ # '12月9日下午，南航A380客机，注册号B-6137，在执行CZ3104北京至广州航班时，偏出跑道边线。据初步了解，B-6137从W2滑行道进入36R跑道过程中，飞机未能进入跑道，停留在跑道口处，飞机前轮偏出跑道边线1米左右，需由拖车推离跑道。', # '据美国侨报纽约网综合报道，美国达美航空一架波音757客机，本月2日从美国东岸纽约市飞往西岸西雅图，旅程约6小时，不料当时飞机上的厕所无预警故障，许多乘客向机组人员表示需要上厕所，最后机长在蒙大拿州比灵斯城贴心转降，让旅客下机“解放”']} # shehui # {'hangkong': [ # '12月9日下午，南航A380客机，注册号B-6137，在执行CZ3104北京至广州航班时，偏出跑道边线。据初步了解，B-6137从W2滑行道进入36R跑道过程中，飞机未能进入跑道，停留在跑道口处，飞机前轮偏出跑道边线1米左右，需由拖车推离跑道。', # '据美国侨报纽约网综合报道，美国达美航空一架波音757客机，本月2日从美国东岸纽约市飞往西岸西雅图，旅程约6小时，不料当时飞机上的厕所无预警故障，许多乘客向机组人员表示需要上厕所，最后机长在蒙大拿州比灵斯城贴心转降，让旅客下机“解放”'], # 'shehui': [ # '【网友爆料称国内高空挑战第一人失手坠楼 其女友证实死亡消息】近日，有网友爆料称，自称国内极限高空挑战第一人的网络直播博主“极限-咏宁”于11月8日后再未更新所有社交网络账号，疑似因失手坠楼导致死亡。12月7日，其女友通过咏宁美拍账号发布内容为“我只是想安静的睡上一觉。以后都不会不再更新了”的视频。8日又通过个人微博发布消息称“今天是12月8号！让我想到11月8号你离开我们！离开这个世界。”证实咏宁去世消息。\n据了解，咏宁全名吴咏宁，此前曾在众多的视频网站上使用“极限-咏宁”账号发布小视频作品，内容多为攀登高层建筑及塔吊，其个人账号介绍信息显示：国内无任何保护极限挑战第一人，挑战全世界高楼大厦。有消息称，咏宁之所以选择拍如此危险的视频，是为了多赚钱给母亲治病。截至目前，该消息尚未得到证实。', # '据了解，莎普爱思的通用名称是苄***氨酸滴眼液，生产商声称其适应症是早期老年性白内障，属于非处方药品（OTC），2016年该滴眼液营业收入7.5亿元，占生产公司营业收入的77％，毛利率94.59％。\n“上市之初为处方药，一开始在医院推不开，药物本身没能达到预期效果，得不到医生认可，从2004年起换了个OTC的‘马甲’，这几年开始铺天盖地做广告宣传。”同济大学附属东方医院眼科主任崔红平表示，在莎普爱思的广告里，从白内障到眼睛眩光、黑影、重影、模糊……好像所有的眼科问题，这款眼药水都能“治”。']} # war # {'hangkong': [ # '12月9日下午，南航A380客机，注册号B-6137，在执行CZ3104北京至广州航班时，偏出跑道边线。据初步了解，B-6137从W2滑行道进入36R跑道过程中，飞机未能进入跑道，停留在跑道口处，飞机前轮偏出跑道边线1米左右，需由拖车推离跑道。', # '据美国侨报纽约网综合报道，美国达美航空一架波音757客机，本月2日从美国东岸纽约市飞往西岸西雅图，旅程约6小时，不料当时飞机上的厕所无预警故障，许多乘客向机组人员表示需要上厕所，最后机长在蒙大拿州比灵斯城贴心转降，让旅客下机“解放”'], # 'shehui': [ # '【网友爆料称国内高空挑战第一人失手坠楼 其女友证实死亡消息】近日，有网友爆料称，自称国内极限高空挑战第一人的网络直播博主“极限-咏宁”于11月8日后再未更新所有社交网络账号，疑似因失手坠楼导致死亡。12月7日，其女友通过咏宁美拍账号发布内容为“我只是想安静的睡上一觉。以后都不会不再更新了”的视频。8日又通过个人微博发布消息称“今天是12月8号！让我想到11月8号你离开我们！离开这个世界。”证实咏宁去世消息。\n据了解，咏宁全名吴咏宁，此前曾在众多的视频网站上使用“极限-咏宁”账号发布小视频作品，内容多为攀登高层建筑及塔吊，其个人账号介绍信息显示：国内无任何保护极限挑战第一人，挑战全世界高楼大厦。有消息称，咏宁之所以选择拍如此危险的视频，是为了多赚钱给母亲治病。截至目前，该消息尚未得到证实。', # '据了解，莎普爱思的通用名称是苄***氨酸滴眼液，生产商声称其适应症是早期老年性白内障，属于非处方药品（OTC），2016年该滴眼液营业收入7.5亿元，占生产公司营业收入的77％，毛利率94.59％。\n“上市之初为处方药，一开始在医院推不开，药物本身没能达到预期效果，得不到医生认可，从2004年起换了个OTC的‘马甲’，这几年开始铺天盖地做广告宣传。”同济大学附属东方医院眼科主任崔红平表示，在莎普爱思的广告里，从白内障到眼睛眩光、黑影、重影、模糊……好像所有的眼科问题，这款眼药水都能“治”。'], # 'war': ['其舰员缓缓地进入横须贺的母港。“里根”号航母是在2017年9月8日从横须贺出港的，整个航程的时间不足3个月。但由于长期在外海作业，舰体四处生锈，显得非常狼狈，犹如一艘准备拖往拆船厂给拆掉的废旧船。', # '然不是历史上建造的最大直升机，但它是迄今为止投入使用的最大直升机。在直升机界，米-26直升机是十足的巨无霸，我们可以通过一些简单的对比管窥一二']} #2找出出现最多的N个词 #遍历字典的每个类别的文章 #创建一个set集合装N个词 words = set() for name in datas: #创建一个空列表，存储同一类型的所有文章的内容 l_join = [] #遍历同一个类别下的所有文章的内容 for content in datas[name]: #先将同一类别的文章放在一个列表里面 l_join.append(content) #对文章进行分词 #对同一类型的文章进行分词 l = jieba.cut("".join(l_join)) # print(list(jieba.cut(content))) #统计每个分词出现的次数 d = nltk.probability.FreqDist(l) # l2 = list(d.items()) # print(l2) '''[('12', 1), ('月', 1), ('9', 1), ('日', 1), ('下午', 1), ('，', 9), ('南航', 1), ('A380', 1), ('客机', 1), ('注册号', 1), ('B', 2), ('-', 2), ('6137', 2), ('在', 2), ('执行', 1), ('CZ3104', 1), ('北京', 1), ('至', 1), ('广州', 1), ('航班', 1), ('时', 1), ('偏出', 2), ('跑道', 6), ('边线', 2), ('。', 2), ('据', 1), ('初步', 1), ('了解', 1), ('从', 1), ('W2', 1), ('滑行道', 1), ('进入', 2), ('36R', 1), ('过程', 1), ('中', 1), ('飞机', 2), ('未能', 1), ('停留', 1), ('口处', 1), ('前轮', 1), ('1', 1), ('米左右', 1), ('需由', 1), ('拖车', 1), ('推离', 1)] [('据', 1), ('美国', 3), ('侨报', 1), ('纽约', 1), ('网', 1), ('综合', 1), ('报道', 1), ('，', 7), ('达美', 1), ('航空', 1), ('一架', 1), ('波音', 1), ('757', 1), ('客机', 1), ('本月', 1), ('2', 1), ('日', 1), ('从', 1), ('东岸', 1), ('纽约市', 1), ('飞往', 1), ('西岸', 1), ('西雅图', 1), ('旅程', 1), ('约', 1), ('6', 1), ('小时', 1), ('不料', 1), ('当时', 1), ('飞机', 1), ('上', 2), ('的', 1), ('厕所', 2), ('无', 1), ('预警', 1), ('故障', 1), ('许多', 1), ('乘客', 1), ('向', 1), ('机组人员', 1), ('表示', 1), ('需要', 1), ('最后', 1), ('机长', 1), ('在', 1), ('蒙大拿州', 1), ('比灵', 1), ('斯城', 1), ('贴心', 1), ('转降', 1), ('让', 1), ('旅客', 1), ('下', 1), ('机', 1), ('“', 1), ('解放', 1), ('”', 1)] [('【', 1), ('网友', 2), ('爆料', 2), ('称', 4), ('国''' # l2 = sorted(list(d.items()),key=lambda x:x[1],reverse=True) # print(l2) '''[('，', 9), ('跑道', 6), ('B', 2), ('-', 2), ('6137', 2), ('在', 2), ('偏出', 2), ('边线', 2), ('。', 2), ('进入', 2), ('飞机', 2), ('12', 1), ('月', 1), ('9', 1), ('日', 1), ('下午', 1), ('南航', 1), ('A380', 1), ('客机', 1), ('注册号', 1), ('执行', 1), ('CZ3104', 1), ('北京', 1), ('至', 1), ('广州', 1), ('航班', 1), ('时', 1), ('据', 1), ('初步', 1), ('了解', 1), ('从', 1), ('W2', 1), ('滑行道', 1), ('36R', 1), ('过程', 1), ('中', 1), ('未能', 1), ('停留', 1), ('口处', 1), ('前轮', 1), ('1', 1), ('米左右', 1), ('需由', 1), ('拖车', 1), ('推离', 1)] [('，', 7), ('美国', 3), ('上', 2), ('厕所', 2), ('据', 1), ('侨报', 1), ('纽约', 1), ('网', 1), ('综合', 1), ('报道', 1), ('达美', 1), ('航空', 1), ('一架', 1), ('波音', 1), ('757', 1), ('客机', 1), ('本月', 1), ('2', 1), ('日', 1), ('从', 1), ('东岸', 1), ('纽约市', 1), ('飞往', 1), ('西岸', 1), ('西雅图', 1), ('旅程', 1), ('约', 1), ('6', 1), ('小时', 1), ('不料', 1), ('当时', 1), ('飞机', 1), ('的', 1), ('无', 1), ('预警', 1), ('故障', 1), ('许多', 1), ('乘客', 1), ('向', 1), ('机组人员', 1), ('表示', 1), ('需要', 1), ('最后', 1), ('机长', 1), ('在', 1), ('蒙大拿州', 1), ('比灵', 1), ('斯城', 1), ('贴心', 1), ('转降', 1), ('让', 1), ('旅客', 1), ('下', 1), ('机', 1), ('“', 1), ('解放', 1), ('”', 1)] [('，', 12), ('。', 8), ('消息', 5), ('的', 5), ('咏宁', 5), ('称', 4), ('挑战', 4), ('极限', 4), ('“', 4), ('”', 4), ('月', 4), ('8', 4), ('账号', 4), ('国内', 3), ('第一', 3), ('人', 3), ('其', 3), ('证实', 3), ('发布', 3), ('视频', 3), ('网友', 2), ('爆料', 2), ('高空', 2), ('失手', 2), ('坠楼', 2), ('女友', 2), ('死亡', 2), ('有', 2), ('网络', 2), ('-', 2), ('11', 2), ('更新', 2), ('12', 2), ('日', 2), ('通过', 2), ('拍', 2), ('内容', 2), ('我', 2), ('上', 2), ('个人', 2), ('是', 2), ('号', 2), ('！', 2), ('离开', 2), ('【', 1), (' ', 1), ('】', 1), ('近日', 1), ('自称', 1), ('直播', 1), ('博主', 1), ('于', 1), ('日后', 1), ('再', 1), ('未', 1), ('所有', 1), ('社交', 1), ('疑似', 1), ('因', 1), ('导致', 1), ('7', 1), ('咏宁美', 1), ('为', 1), ('只是', 1), ('想', 1), ('安静', 1), ('睡', 1), ('一觉', 1), ('以后', 1), ('都', 1), ('不会', 1), ('不再', 1), ('了', 1), ('又', 1), ('微博', 1), ('今天', 1), ('让', 1), ('想到', 1), ('你', 1), ('我们', 1), ('这个', 1), ('世界', 1), ('去世', 1), ('\n', 1), ('据', 1), ('了解', 1), ('全名', 1), ('吴咏宁', 1), ('此前', 1), ('曾', 1), ('在', 1), ('众多', 1), ('网站', 1), ('使用', 1), ('小视频', 1), ('作品', 1), ('多为', 1), ('攀登', 1), ('高层建筑', 1), ('及', 1), ('塔吊', 1), ('介绍', 1), ('信息', 1), ('显示', 1), ('：', 1), ('无', 1), ('任何', 1), ('保护', 1), ('全世界', 1), ('高楼大厦', 1), ('之所以', 1), ('选择', 1), ('如此', 1), ('危险', 1), ('为了', 1), ('多', 1), ('赚钱', 1), ('给', 1), ('母亲', 1), ('治病', 1), ('截至', 1), ('目前', 1), ('该', 1), ('尚未', 1), ('得到', 1)] [('，', 14), ('的', 5), ('。', 3), ('、', 3), ('莎', 2), ('普爱思', 2), ('是', 2), ('滴眼液', 2), ('白内障', 2), ('OTC', 2), ('年', 2), ('营业', 2), ('收入', 2), ('％', 2), ('“', 2), ('开始', 2), ('在', 2), ('医院', 2), ('能', 2), ('从', 2), ('这', 2), ('”', 2), ('眼科', 2), ('…', 2), ('据', 1), ('了解', 1), ('通用', 1), ('名称', 1), ('苄', 1), ('达', 1), ('赖氨酸', 1), ('生产商', 1), ('声称', 1), ('其', 1), ('适应症', 1), ('早期', 1), ('老年性', 1), ('属于', 1), ('非处方药品', 1), ('（', 1), ('）', 1), ('2016', 1), ('该', 1), ('7.5', 1), ('亿元', 1), ('占', 1), ('生产', 1), ('公司', 1), ('77', 1), ('毛利率', 1), ('94.59', 1), ('\n', 1), ('上市', 1), ('之初', 1), ('为', 1), ('处方药', 1), ('一', 1), ('推不开', 1), ('药物', 1), ('本身', 1), ('没', 1), ('达到', 1), ('预期', 1), ('效果', 1), ('得不到', 1), ('医生', 1), ('认可', 1), ('2004', 1), ('起换', 1), ('了', 1), ('个', 1), ('‘', 1), ('马甲', 1), ('’', 1), ('几年', 1), ('铺天盖地', 1), ('做', 1), ('广告宣传', 1), ('同济大学', 1), ('附属', 1), ('东方', 1), ('主任', 1), ('崔红平', 1), ('表示', 1), ('广告', 1), ('里', 1), ('到', 1), ('眼睛', 1), ('眩光', 1), ('黑影', 1), ('重影', 1), ('模糊', 1), ('好像', 1), ('所有', 1), ('问题', 1), ('款', 1), ('眼药水', 1), ('都', 1), ('治', 1)] [('的', 4), ('，', 4), ('。', 3), ''' l2 = sorted(list(d.items()),key=lambda x:x[1],reverse=True)[:10] #遍历选出的前10个词语 for(w,c) in l2: #取出每个词语的名字 words.add(w) print (words) #打印出了频率最高的词语 '''Loading model cost 0.822 seconds. {'6137', '进入', '跑道', '据', '-', '。', '边线', '偏出', '，', '报道', '在', 'B', '厕所', '纽约', '美国', '上', '综合', '网', '侨报'} Prefix dict has been built succesfully. {'是', '”', '消息', '“', '，', '普爱思', '咏宁', '莎', '据', '-', '边线', 'B', '的', '侨报', '进入', '偏出', '在', '极限', '、', '上', '6137', '跑道', '白内障', '。', '称', 'OTC', '报道', '厕所', '纽约', '美国', '综合', '滴眼液', '挑战', '网'} {'是', '”', '消息', '“', '地', '，', '建造', '普爱思', '咏宁', '莎', '据', '-', '横须贺', '边线', '月', 'B', '直升机', '的', '侨报', '进入', '偏出', '不是', '在', '极限', '、', '上', '其舰员', '缓缓', '6137', '历史', '跑道', '白内障', '然', '最大', '。', '称', 'OTC', '报道', '厕所', '纽约', '美国', '综合', '滴眼液', '挑战', '网'} ''' #3.特征生成函数 def getFeatures(content): result = {} #遍历频率最高的词语 for w in words: #将这些词语存入字典，并初始化出现的次数为0 result[w] = 0 #对输入的文本内容进行分词 l = list(jieba.cut(content)) #遍历分词后的列表 for w in l: #如果存在于关键词words中 if w in words: #就将对应的次数加1 result[w]+=1 return result #4训练集 测试集 train_set=[] test_set = [] l = [] #遍历所有类型 for name in datas: #遍历同一个类型的文件 for conten in datas[name]: l.append((getFeatures(content), name)) random.shuffle(l) train_set = l[:math.floor(len(l)/2)] test_set = l[math.ceil(len(l)/2):] #训练 classfier = nltk.NaiveBayesClassifier.train(train_set) #测试 res = sum([1 if classfier.classify(features)==name else 0 for(features,name) in test_set])/len(test_set) print(res)
hangkong
{'hangkong': ['12月9日下午，南航A380客机，注册号B-6137，在执行CZ3104北京至广州航班时，偏出跑道边线。据初步了解，B-6137从W2滑行道进入36R跑道过程中，飞机未能进入跑道，停留在跑道口处，飞机前轮偏出跑道边线1米左右，需由拖车推离跑道。', '据美国侨报纽约网综合报道，美国达美航空一架波音757客机，本月2日从美国东岸纽约市飞往西岸西雅图，旅程约6小时，不料当时飞机上的厕所无预警故障，许多乘客向机组人员表示需要上厕所，最后机长在蒙大拿州比灵斯城贴心转降，让旅客下机“解放”']}
shehui
{'hangkong': ['12月9日下午，南航A380客机，注册号B-6137，在执行CZ3104北京至广州航班时，偏出跑道边线。据初步了解，B-6137从W2滑行道进入36R跑道过程中，飞机未能进入跑道，停留在跑道口处，飞机前轮偏出跑道边线1米左右，需由拖车推离跑道。', '据美国侨报纽约网综合报道，美国达美航空一架波音757客机，本月2日从美国东岸纽约市飞往西岸西雅图，旅程约6小时，不料当时飞机上的厕所无预警故障，许多乘客向机组人员表示需要上厕所，最后机长在蒙大拿州比灵斯城贴心转降，让旅客下机“解放”'], 'shehui': ['【网友爆料称国内高空挑战第一人失手坠楼 其女友证实死亡消息】近日，有网友爆料称，自称国内极限高空挑战第一人的网络直播博主“极限-咏宁”于11月8日后再未更新所有社交网络账号，疑似因失手坠楼导致死亡。12月7日，其女友通过咏宁美拍账号发布内容为“我只是想安静的睡上一觉。以后都不会不再更新了”的视频。8日又通过个人微博发布消息称“今天是12月8号！让我想到11月8号你离开我们！离开这个世界。”证实咏宁去世消息。\n据了解，咏宁全名吴咏宁，此前曾在众多的视频网站上使用“极限-咏宁”账号发布小视频作品，内容多为攀登高层建筑及塔吊，其个人账号介绍信息显示：国内无任何保护极限挑战第一人，挑战全世界高楼大厦。有消息称，咏宁之所以选择拍如此危险的视频，是为了多赚钱给母亲治病。截至目前，该消息尚未得到证实。', '据了解，莎普爱思的通用名称是苄***氨酸滴眼液，生产商声称其适应症是早期老年性白内障，属于非处方药品（OTC），2016年该滴眼液营业收入7.5亿元，占生产公司营业收入的77％，毛利率94.59％。\n“上市之初为处方药，一开始在医院推不开，药物本身没能达到预期效果，得不到医生认可，从2004年起换了个OTC的‘马甲’，这几年开始铺天盖地做广告宣传。”同济大学附属东方医院眼科主任崔红平表示，在莎普爱思的广告里，从白内障到眼睛眩光、黑影、重影、模糊……好像所有的眼科问题，这款眼药水都能“治”。']}
war
{'hangkong': ['12月9日下午，南航A380客机，注册号B-6137，在执行CZ3104北京至广州航班时，偏出跑道边线。据初步了解，B-6137从W2滑行道进入36R跑道过程中，飞机未能进入跑道，停留在跑道口处，飞机前轮偏出跑道边线1米左右，需由拖车推离跑道。', '据美国侨报纽约网综合报道，美国达美航空一架波音757客机，本月2日从美国东岸纽约市飞往西岸西雅图，旅程约6小时，不料当时飞机上的厕所无预警故障，许多乘客向机组人员表示需要上厕所，最后机长在蒙大拿州比灵斯城贴心转降，让旅客下机“解放”'], 'shehui': ['【网友爆料称国内高空挑战第一人失手坠楼 其女友证实死亡消息】近日，有网友爆料称，自称国内极限高空挑战第一人的网络直播博主“极限-咏宁”于11月8日后再未更新所有社交网络账号，疑似因失手坠楼导致死亡。12月7日，其女友通过咏宁美拍账号发布内容为“我只是想安静的睡上一觉。以后都不会不再更新了”的视频。8日又通过个人微博发布消息称“今天是12月8号！让我想到11月8号你离开我们！离开这个世界。”证实咏宁去世消息。\n据了解，咏宁全名吴咏宁，此前曾在众多的视频网站上使用“极限-咏宁”账号发布小视频作品，内容多为攀登高层建筑及塔吊，其个人账号介绍信息显示：国内无任何保护极限挑战第一人，挑战全世界高楼大厦。有消息称，咏宁之所以选择拍如此危险的视频，是为了多赚钱给母亲治病。截至目前，该消息尚未得到证实。', '据了解，莎普爱思的通用名称是苄***氨酸滴眼液，生产商声称其适应症是早期老年性白内障，属于非处方药品（OTC），2016年该滴眼液营业收入7.5亿元，占生产公司营业收入的77％，毛利率94.59％。\n“上市之初为处方药，一开始在医院推不开，药物本身没能达到预期效果，得不到医生认可，从2004年起换了个OTC的‘马甲’，这几年开始铺天盖地做广告宣传。”同济大学附属东方医院眼科主任崔红平表示，在莎普爱思的广告里，从白内障到眼睛眩光、黑影、重影、模糊……好像所有的眼科问题，这款眼药水都能“治”。'], 'war': ['其舰员缓缓地进入横须贺的母港。“里根”号航母是在2017年9月8日从横须贺出港的，整个航程的时间不足3个月。但由于长期在外海作业，舰体四处生锈，显得非常狼狈，犹如一艘准备拖往拆船厂给拆掉的废旧船。', '然不是历史上建造的最大直升机，但它是迄今为止投入使用的最大直升机。在直升机界，米-26直升机是十足的巨无霸，我们可以通过一些简单的对比管窥一二']}
Building prefix dict from the default dictionary ...
Loading model from cache C:\Users\小秋\AppData\Local\Temp\jieba.cache
Loading model cost 0.843 seconds.
Prefix dict has been built succesfully.
{'偏出', '客机', '美国', '进入', '跑道', '。', '边线', 'B', '飞机', '日', '6137', '在', '-', '，'}
{'客机', '跑道', '。', 'B', '日', '“', '”', '挑战', '6137', '咏宁', '，', '偏出', '消息', '的', '-', '称', '极限', '美国', '进入', '边线', '飞机', '在', '其'}
{'客机', '跑道', '。', '其舰员', 'B', '日', '“', '缓缓', '”', '直升机', '挑战', '地', '6137', '咏宁', '是', '，', '横须贺', '偏出', '消息', '但', '的', '-', '称', '最大', '极限', '美国', '进入', '边线', '飞机', '月', '在', '其'}
0.0