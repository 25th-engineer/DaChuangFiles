ä¸€ã€å‰è¿°
Pythonä¸Šè‘—åçš„â¾ƒç„¶è¯­â¾”å¤„ç†åº“â¾ƒå¸¦è¯­æ–™åº“ï¼Œè¯æ€§åˆ†ç±»åº“â¾ƒå¸¦åˆ†ç±»ï¼Œåˆ†è¯ï¼Œç­‰ç­‰åŠŸèƒ½å¼ºâ¼¤çš„ç¤¾åŒºâ½€æŒï¼Œè¿˜æœ‰Nå¤šçš„ç®€å•ç‰ˆwrapperã€‚
äºŒã€æ–‡æœ¬é¢„å¤„ç†
1ã€å®‰è£…nltk
pip install -U nltk
å®‰è£…è¯­æ–™åº“ (ä¸€å †å¯¹è¯ï¼Œä¸€å¯¹æ¨¡å‹)
import nltk nltk.download()
2ã€åŠŸèƒ½ä¸€è§ˆè¡¨ï¼š
3ã€æ–‡æœ¬å¤„ç†æµç¨‹
4ã€Tokenize æŠŠé•¿å¥â¼¦æ‹†æˆæœ‰â€œæ„ä¹‰â€çš„â¼©éƒ¨ä»¶
import jieba seg_list = jieba.cut("æˆ‘æ¥åˆ°åŒ—ï¥£äº¬æ¸…åâ¼¤å¤§å­¦", cut_all=True) print "Full Mode:", "/ ".join(seg_list) # å…¨æ¨¡å¼ seg_list = jieba.cut("æˆ‘æ¥åˆ°åŒ—ï¥£äº¬æ¸…åâ¼¤å¤§å­¦", cut_all=False) print "Default Mode:", "/ ".join(seg_list) # ç²¾ç¡®æ¨¡å¼ seg_list = jieba.cut("ä»–æ¥åˆ°äº†ï¦ºâ½¹ç½‘æ˜“ï§ æ­ç ”â¼¤å¤§å¦") # é»˜è®¤æ˜¯ç²¾ç¡®æ¨¡å¼ print ", ".join(seg_list) seg_list = jieba.cut_for_search("â¼©å°æ˜ç¡•â¼ å£«æ¯•ä¸šäºä¸­å›½ç§‘å­¦é™¢è®¡ç®—æ‰€ï¼Œååœ¨â½‡æ—¥æœ¬äº¬éƒ½â¼¤å¤§å­¦æ·±é€ ") # æœç´¢å¼•æ“æ¨¡å¼ print ", ".join(seg_list)
ç»“æœï¼š
ã€å…¨æ¨¡å¼ã€‘: æˆ‘/ æ¥åˆ°/ åŒ—ï¥£äº¬/ æ¸…å/ æ¸…åâ¼¤å¤§å­¦/ åâ¼¤å¤§/ â¼¤å¤§å­¦ ã€ç²¾ç¡®æ¨¡å¼ã€‘: æˆ‘/ æ¥åˆ°/ åŒ—ï¥£äº¬/ æ¸…åâ¼¤å¤§å­¦ ã€æ–°è¯è¯†åˆ«ã€‘ï¼šä»–, æ¥åˆ°, äº†ï¦º, â½¹ç½‘æ˜“ï§ , æ­ç ”, â¼¤å¤§å¦ (æ­¤å¤„ï¼Œâ€œæ­ç ”â€å¹¶æ²¡æœ‰åœ¨è¯å…¸ä¸­ï¼Œä½†æ˜¯ä¹Ÿè¢«Viterbiç®—æ³•è¯†åˆ«å‡ºæ¥äº†ï¦º) ã€æœç´¢å¼•æ“æ¨¡å¼ã€‘ï¼š â¼©å°æ˜, ç¡•â¼ å£«, æ¯•ä¸š, äº, ä¸­å›½, ç§‘å­¦, å­¦é™¢, ç§‘å­¦é™¢, ä¸­å›½ç§‘å­¦é™¢, è®¡ç®—, è®¡ç®—æ‰€, å, åœ¨, â½‡æ—¥æœ¬, äº¬éƒ½, â¼¤å¤§å­¦, â½‡æ—¥æœ¬äº¬éƒ½â¼¤å¤§å­¦, æ·±é€ 
ç¤¾äº¤â½¹ç»œè¯­â¾”çš„tokenize:
import re emoticons_str = r""" (?: [:=;] # çœ¼ç› [oO\-]? # â¿é¼»â¼¦å­ [D\)\]\(\]/\\OpP] # å˜´ )""" regex_str = [ emoticons_str, r'<[^>]+>', # HTML tags r'(?:@[\w_]+)', # @æŸâ¼ˆäºº r"(?:\#+[\w_]+[\w\'_\-]*[\w_]+)", # è¯é¢˜æ ‡ç­¾ r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\(\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs r'(?:(?:\d+,?)+(?:\.?\d+)?)', # æ•°å­— r"(?:[a-z][a-z'\-_]+[a-z])", # å«æœ‰ - å’Œ â€˜ çš„å•è¯ r'(?:[\w_]+)', # å…¶ä»– r'(?:\S)' # å…¶ä»– ]
æ­£åˆ™è¡¨è¾¾å¼å¯¹ç…§è¡¨
http://www.regexlab.com/zh/regref.htm
è¿™æ ·èƒ½å¤„ç†ç¤¾äº¤è¯­è¨€ä¸­çš„è¡¨æƒ…ç­‰ç¬¦å·ï¼š
tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE) emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE) def tokenize(s): return tokens_re.findall(s) def preprocess(s, lowercase=False): tokens = tokenize(s) if lowercase: tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens] return tokens tweet = 'RT @angelababy: love you baby! :D http://ah.love #168cm' print(preprocess(tweet)) # ['RT', '@angelababy', ':', 'love', 'you', 'baby', # â€™!', ':D', 'http://ah.love', '#168cm']
5ã€è¯å½¢å½’â¼€åŒ–
Stemming è¯â¼²æå–ï¼šâ¼€èˆ¬æ¥è¯´ï¼Œå°±æ˜¯æŠŠä¸å½±å“è¯æ€§çš„inflectionçš„â¼©å°¾å·´ç æ‰
walking ç ing = walk
walked ç ed = walk
Lemmatization è¯å½¢å½’â¼€ï¼šæŠŠå„ç§ç±»å‹çš„è¯çš„å˜å½¢ï¼Œéƒ½å½’ä¸ºâ¼€ä¸ªå½¢å¼
went å½’â¼€ = go
are å½’â¼€ = be
>>> from nltk.stem.porter import PorterStemmer >>> porter_stemmer = PorterStemmer() >>> porter_stemmer.stem(â€˜maximumâ€™) uâ€™maximumâ€™ >>> porter_stemmer.stem(â€˜presumablyâ€™) uâ€™presumâ€™ >>> porter_stemmer.stem(â€˜multiplyâ€™) uâ€™multipliâ€™ >>> porter_stemmer.stem(â€˜provisionâ€™) uâ€™provisâ€™ >>> from nltk.stem import SnowballStemmer >>> snowball_stemmer = SnowballStemmer(â€œenglishâ€) >>> snowball_stemmer.stem(â€˜maximumâ€™) uâ€™maximumâ€™ >>> snowball_stemmer.stem(â€˜presumablyâ€™) uâ€™presumâ€™ >>> from nltk.stem.lancaster import LancasterStemmer >>> lancaster_stemmer = LancasterStemmer() >>> lancaster_stemmer.stem(â€˜maximumâ€™) â€˜maximâ€™ >>> lancaster_stemmer.stem(â€˜presumablyâ€™) â€˜presumâ€™ >>> lancaster_stemmer.stem(â€˜presumablyâ€™) â€˜presumâ€™ >>> from nltk.stem.porter import PorterStemmer >>> p = PorterStemmer() >>> p.stem('went') 'went' >>> p.stem('wenting') 'went'
6ã€è¯æ€§Part-Of-Speech
>>> import nltk >>> text = nltk.word_tokenize('what does the fox say') >>> text ['what', 'does', 'the', 'fox', 'say'] >>> nltk.pos_tag(text) [('what', 'WDT'), ('does', 'VBZ'), ('the', 'DT'), ('fox', 'NNS'), ('say', 'VBP')]
7ã€Stopwords
â¾¸å…ˆè®°å¾—åœ¨consoleâ¾¥â¾¯ä¸‹è½½â¼€ä¸‹è¯åº“ æˆ–è€… nltk.download(â€˜stopwordsâ€™)
from nltk.corpus import stopwords # å…ˆtokenâ¼€ä¸€æŠŠï¼Œå¾—åˆ°â¼€ä¸€ä¸ªword_list # ... # ç„¶åfilterâ¼€ä¸€æŠŠ filtered_words = [word for word in word_list if word not in stopwords.words('english')]
8ã€â¼€æ¡â½‚æœ¬é¢„å¤„ç†æµâ½”çº¿
ä¸‰ã€è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨ã€‚
å®é™…ä¸Šé¢„å¤„ç†å°±æ˜¯å°†æ–‡æœ¬è½¬æ¢ä¸ºWord_Listï¼Œè‡ªç„¶è¯­è¨€å¤„ç†å†è½¬å˜æˆè®¡ç®—æœºèƒ½è¯†åˆ«çš„è¯­è¨€ã€‚
è‡ªç„¶è¯­è¨€å¤„ç†æœ‰ä»¥ä¸‹å‡ ä¸ªåº”ç”¨ï¼šæƒ…æ„Ÿåˆ†æï¼Œâ½‚æœ¬ç›¸ä¼¼åº¦ï¼Œ â½‚æœ¬åˆ†ç±»
1ã€æƒ…æ„Ÿåˆ†æ
æœ€ç®€å•çš„ sentiment dictionary,ç±»ä¼¼äºå…³é”®è¯æ‰“åˆ†æœºåˆ¶.
like 1
good 2
bad -2
terrible -3
sentiment_dictionary = {} for line in open('data/AFINN-111.txt') word, score = line.split('\t') sentiment_dictionary[word] = int(score) # æŠŠè¿™ä¸ªæ‰“åˆ†è¡¨è®°å½•åœ¨â¼€ä¸€ä¸ªDictä¸Šä»¥å # è·‘â¼€ä¸€éæ•´ä¸ªå¥ï¤†â¼¦å­ï¼ŒæŠŠå¯¹åº”çš„å€¼ç›¸åŠ  total_score = sum(sentiment_dictionary.get(word, 0) for word in words) # æœ‰å€¼å°±æ˜¯Dictä¸­çš„å€¼ï¼Œæ²¡æœ‰å°±æ˜¯0 # äºæ˜¯ä½ å°±å¾—åˆ°äº†ï¦ºâ¼€ä¸€ä¸ª sentiment score
æ˜¾ç„¶è¿™ä¸ªâ½…æ³•å¤ªNaive,æ–°è¯æ€ä¹ˆåŠï¼Ÿç‰¹æ®Šè¯æ±‡æ€ä¹ˆåŠï¼Ÿæ›´æ·±å±‚æ¬¡çš„ç©æ„â¼‰æ€ä¹ˆåŠï¼Ÿ
åŠ ä¸ŠMLæƒ…æ„Ÿåˆ†æ
from nltk.classify import NaiveBayesClassifier # éšâ¼¿æ‰‹é€ ç‚¹è®­ç»ƒé›† s1 = 'this is a good book' s2 = 'this is a awesome book' s3 = 'this is a bad book' s4 = 'this is a terrible book' def preprocess(s): # Func: å¥ï¤†â¼¦å­å¤„ç†ï§¤ # è¿™â¾¥é‡Œï§©ç®€å•çš„â½¤ç”¨äº†ï¦ºsplit(), æŠŠå¥ï¤†â¼¦å­ä¸­æ¯ä¸ªå•è¯åˆ†å¼€ # æ˜¾ç„¶ è¿˜æœ‰æ›´ï¤å¤šçš„processing methodå¯ä»¥â½¤ç”¨ return {word: True for word in s.lower().split()} # returnâ»“é•¿è¿™æ ·: # {'this': True, 'is':True, 'a':True, 'good':True, 'book':True} # å…¶ä¸­, å‰â¼€ä¸€ä¸ªå«fname, å¯¹åº”æ¯ä¸ªå‡ºç°çš„â½‚æ–‡æœ¬å•è¯; # åâ¼€ä¸€ä¸ªå«fval, æŒ‡çš„æ˜¯æ¯ä¸ªâ½‚æ–‡æœ¬å•è¯å¯¹åº”çš„å€¼ã€‚ # è¿™â¾¥é‡Œï§©æˆ‘ä»¬â½¤ç”¨æœ€ç®€å•çš„True,æ¥è¡¨ç¤º,è¿™ä¸ªè¯ã€å‡ºç°åœ¨å½“å‰çš„å¥ï¤†â¼¦å­ä¸­ã€çš„æ„ä¹‰ã€‚ # å½“ç„¶å•¦, æˆ‘ä»¬ä»¥åå¯ä»¥å‡çº§è¿™ä¸ªâ½…æ–¹ç¨‹, è®©å®ƒå¸¦æœ‰æ›´ï¤åŠ â½œç‰›é€¼çš„fval, â½æ¯”å¦‚ word2vec
# æŠŠè®­ç»ƒé›†ç»™åšæˆæ ‡å‡†å½¢å¼ training_data = [[preprocess(s1), 'pos'], [preprocess(s2), 'pos'], [preprocess(s3), 'neg'], [preprocess(s4), 'neg']] # å–‚ç»™modelåƒ model = NaiveBayesClassifier.train(training_data) # æ‰“å‡ºç»“æœ print(model.classify(preprocess('this is a good book')))
2ã€æ–‡æœ¬ç›¸ä¼¼åº¦
â½¤å…ƒç´ é¢‘ç‡è¡¨â½°â½‚æœ¬ç‰¹å¾ï¼Œå¸¸è§çš„åšæ³•
ç„¶åç”¨ä½™å¼¦å®šç†æ¥è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦ï¼š
Frequency é¢‘ç‡ç»Ÿè®¡ï¼š
import nltk from nltk import FreqDist # åšä¸ªè¯åº“å…ˆ corpus = 'this is my sentence ' \ 'this is my life ' \ 'this is the day' # éšä¾¿ï¥¥tokenizeâ¼€ä¸€ä¸‹ # æ˜¾ç„¶, æ­£å¦‚ä¸Šâ½‚æ–‡æåˆ°, # è¿™â¾¥é‡Œï§©å¯ä»¥æ ¹æ®éœ€è¦åšä»»ä½•çš„preprocessing: # stopwords, lemma, stemming, etc. tokens = nltk.word_tokenize(corpus) print(tokens) # å¾—åˆ°tokenå¥½çš„word list # ['this', 'is', 'my', 'sentence', # 'this', 'is', 'my', 'life', 'this', # 'is', 'the', 'day'] # å€Ÿâ½¤ç”¨NLTKçš„FreqDistç»Ÿè®¡â¼€ä¸€ä¸‹â½‚æ–‡å­—å‡ºç°çš„é¢‘ç‡ fdist = FreqDist(tokens) # å®ƒå°±ç±»ä¼¼äºâ¼€ä¸€ä¸ªDict # å¸¦ä¸ŠæŸä¸ªå•è¯, å¯ä»¥çœ‹åˆ°å®ƒåœ¨æ•´ä¸ªâ½‚æ–‡ç« ä¸­å‡ºç°çš„æ¬¡æ•° print(fdist['is']) # 3
# å¥½, æ­¤åˆ», æˆ‘ä»¬å¯ä»¥æŠŠæœ€å¸¸â½¤ç”¨çš„50ä¸ªå•è¯æ‹¿å‡ºæ¥ standard_freq_vector = fdist.most_common(50) size = len(standard_freq_vector) print(standard_freq_vector) # [('is', 3), ('this', 3), ('my', 2), # ('the', 1), ('d
3ã€æ–‡æœ¬åˆ†ç±»
TF: Term Frequency, è¡¡é‡â¼€ä¸ªtermåœ¨â½‚æ¡£ä¸­å‡ºç°å¾—æœ‰å¤šé¢‘ç¹ã€‚
TF(t) = (tå‡ºç°åœ¨â½‚æ¡£ä¸­çš„æ¬¡æ•°) / (â½‚æ¡£ä¸­çš„termæ€»æ•°).
IDF: Inverse Document Frequency, è¡¡é‡â¼€ä¸ªtermæœ‰å¤šé‡è¦ã€‚
æœ‰äº›è¯å‡ºç°çš„å¾ˆå¤šï¼Œä½†æ˜¯æ˜æ˜¾ä¸æ˜¯å¾ˆæœ‰åµâ½¤ã€‚â½å¦‚â€™is'ï¼Œâ€™theâ€˜ï¼Œâ€™andâ€˜ä¹‹ç±»
çš„ã€‚
ä¸ºäº†å¹³è¡¡ï¼Œæˆ‘ä»¬æŠŠç½•è§çš„è¯çš„é‡è¦æ€§ï¼ˆweightï¼‰æâ¾¼ï¼Œ
æŠŠå¸¸è§è¯çš„é‡è¦æ€§æä½ã€‚
IDF(t) = log_e(â½‚æ¡£æ€»æ•° / å«æœ‰tçš„â½‚æ¡£æ€»æ•°).
TF-IDF = TF * IDF
ä¸¾ä¸ªæ —â¼¦ğŸŒ° :
â¼€ä¸ªâ½‚æ¡£æœ‰100ä¸ªå•è¯ï¼Œå…¶ä¸­å•è¯babyå‡ºç°äº†3æ¬¡ã€‚
é‚£ä¹ˆï¼ŒTF(baby) = (3/100) = 0.03.
å¥½ï¼Œç°åœ¨æˆ‘ä»¬å¦‚æœæœ‰10Mçš„â½‚æ¡£ï¼Œ babyå‡ºç°åœ¨å…¶ä¸­çš„1000ä¸ªâ½‚æ¡£ä¸­ã€‚
é‚£ä¹ˆï¼ŒIDF(baby) = log(10,000,000 / 1,000) = 4
æ‰€ä»¥ï¼Œ TF-IDF(baby) = TF(baby) * IDF(baby) = 0.03 * 4 = 0.12
from nltk.text import TextCollection # â¾¸é¦–å…ˆ, æŠŠæ‰€æœ‰çš„â½‚æ–‡æ¡£æ”¾åˆ°TextCollectionç±»ä¸­ã€‚ # è¿™ä¸ªç±»ä¼šâ¾ƒè‡ªåŠ¨å¸®ä½ æ–­å¥ï¤†, åšç»Ÿè®¡, åšè®¡ç®— corpus = TextCollection(['this is sentence one', 'this is sentence two', 'this is sentence three']) # ç›´æ¥å°±èƒ½ç®—å‡ºtfidf # (term: â¼€ä¸€å¥ï¤†è¯ä¸­çš„æŸä¸ªterm, text: è¿™å¥ï¤†è¯) print(corpus.tf_idf('this', 'this is sentence four')) # 0.444342 # åŒç†ï§¤, æ€ä¹ˆå¾—åˆ°â¼€ä¸€ä¸ªæ ‡å‡†â¼¤å¤§â¼©å°çš„vectoræ¥è¡¨ç¤ºæ‰€æœ‰çš„å¥ï¤†â¼¦å­? # å¯¹äºæ¯ä¸ªæ–°å¥ï¤†â¼¦å­ new_sentence = 'this is sentence five' # éå†â¼€ä¸€éæ‰€æœ‰çš„vocabularyä¸­çš„è¯: for word in standard_vocab: print(corpus.tf_idf(word, new_sentence)) # æˆ‘ä»¬ä¼šå¾—åˆ°â¼€ä¸€ä¸ªå·¨â»“é•¿(=æ‰€æœ‰vocabâ»“é•¿åº¦)çš„å‘é‡ï¥¾
ç›®å‰å‡ ç§è¡¨è¾¾å¥å­çš„æ–¹å¼ï¼šè¯é¢‘ï¼ŒTF-IDFã€‚