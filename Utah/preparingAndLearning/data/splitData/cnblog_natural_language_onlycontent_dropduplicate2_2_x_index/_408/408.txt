前言：
实习需要用到自然语言处理方面的知识。关于自然语言处理，说实话，打心眼里有抵触。
我很喜欢AI的，但是我觉得语言的理解，特别是中文的不确定性，使得我个人认为语言理解与处理
是一件非常棘手的问题，而且不会取得太好的效果。再加上编译原理留下的阴影。使得我虽然
知道自然语言处理 以后必然 有用，但是还是没有选这门课。这不，还是要自学。
这本《Python自然语言处理》是自己找到的，电子版的，虽然不知道好坏，但老外的书，我还是
比较放心的。可惜老外的书就不会以汉语为例子了。我不打算全篇通读，我打算先读些，我认为最重要或最感兴趣的。
喜欢感兴趣的朋友可以和我一起讨论，有讨论才有进步吗。
目标：
1.切词
2.词性，组词
3.同义词，反义词，词意理解
4.数据组织与存放
大致扫了一下，决定主要看第5章和第7章，其它的以后再说。
第5章 分类和标注词汇
词性标注，先看例子。
import nltk text1 = ['I','love','you'] text2 = ['Love','is','good'] print nltk.pos_tag(text1) print nltk.pos_tag(text2)
运行结果如下：
[('I', 'PRP'), ('love', 'VBP'), ('you', 'PRP')]
[('Love', 'NNP'), ('is', 'VBZ'), ('good', 'JJ')]
同一个Love不同的结果。我很像知道这是怎么做到的。
显然Love一般只可能是动词或名词。那么如何识别什么时候是动词，什么时候是名词呢。
再看一个例子：
1 import nltk 2 from nltk.corpus import brown 3 4 #使pos成为某个特殊的dict数据结构，不用深究 5 pos = nltk.defaultdict(lambda: nltk.defaultdict(int)) 6 #获得一个已经标注词性的语料库，每个词以元组 （词，词性）的形式出现 7 brown_news_tagged = brown.tagged_words(categories = 'news', simplify_tags = True) 8 for((w1,t1),(w2,t2)) in nltk.ibigrams(brown_news_tagged): 9 pos[(t1,w2)][t2] += 1 10 #查看当单词right前面单词词性是DET时，right的可能词性 11 print pos[('DET','right')]
结果如下：
defaultdict(<type 'int'>, {'ADV': 3, 'ADJ': 9, 'N': 4})
难道是基于统计加概率的方法来计算一个词在具体语境中的词性，不会吧。
终于到重点了，各位是不是一样都很激动呢。
先从一元标注器1-gram说起，其意思应该是不考虑上下文，只考虑单词本身。
1.默认标注器
文中接着使用nltk.DefaultTagger('NN')定义了一个默认标注器，将所有词都标注成NN 即名词。
并且指出大多数新词都是名词，所以默认标注器可以帮助我们提高语言处理系统的稳定性。
2. 正则表达式标注器
例如，以ed结尾的可能是动词过去分词。不过这些好像在中文里没用。
3.查询标注器
查询标注器就是事先将一些频繁词出现最多的词性记录下来。文中还指出对于那些没记录的词，可以
结合默认标注器。随着频繁词的增加，这个标注器的准确率上升很快，但是达到一定量后趋于平衡。
n-gram标注器：考虑待标记词前面n-1个词的词性。
同样是记录在已知上下文中，当前词最有可能出现的词性。
需要使用语料库进行训练。在实践中，当n越大，越容易出现数据稀疏问题。所以n的选取需要对精度和覆盖进行权衡。
n-gram说白了就是对词性组合规律的一种模型学习，遗憾的是文中对于学习的方法没有细谈。
或许可以训练得到一颗决策树。