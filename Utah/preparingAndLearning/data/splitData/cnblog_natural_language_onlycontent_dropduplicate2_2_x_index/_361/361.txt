从今天起开始写自然语言处理的实践用法，今天学了文本分类，并没用什么创新的东西，只是把学到的知识点复习一下
性别识别（根据给定的名字确定性别）
第一步是创建一个特征提取函数（feature extractor）：该函数建立了一个字典，包含给定姓名的有关特征信息。
>>> def gender_features(word): ... return {'last_letter': word[-1]} >>> gender_features('Shrek') {'last_letter': 'k'}
第二步是准备数据集，该步通过导入现成的NLTK语料库实现
>>> from nltk.corpus import names >>> labeled_names = ([(name, 'male') for name in names.words('male.txt')] + ... [(name, 'female') for name in names.words('female.txt')]) >>> import random >>> random.shuffle(labeled_names)
其中random.shuffle（）的功能是将给定的列表顺序打乱，如：
>>> test=[1,2,3,4,5,6,7,8,9] >>> random.shuffle(test) >>> test [5, 7, 8, 1, 4, 2, 6, 3, 9]
第三步利用特征提取函数对数据集进行处理，生成产生分类器所需要的数据集featuresets，并将数据集featuresets分成训练集和测试集，最后利用NLTK工具包自带的方法
nltk.NaiveBayesClassifier.train()生成一个朴素贝叶斯分类器。
>>> featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names] >>> train_set, test_set = featuresets[500:], featuresets[:500] >>> classifier = nltk.NaiveBayesClassifier.train(train_set)
我们还可以使用方法nltk.classify.accuracy(classifier,test_set)来测试分类器的准确率，使用方法classifier.show_most_informative_features(n)来观察对哪些特征该分类器的准确率最高。
>>> classifier.show_most_informative_features(5) Most Informative Features last_letter = 'a' female : male = 33.2 : 1.0 last_letter = 'k' male : female = 32.6 : 1.0 last_letter = 'p' male : female = 19.7 : 1.0 last_letter = 'v' male : female = 18.6 : 1.0 last_letter = 'f' male : female = 17.3 : 1.0
上面的哪些比率被称为似然比likelihood ratios，例如33.2:1.0表示当名字以字母a结尾时，那这个人事女性的概率时男性的33.2倍。
当数据集比较大时，建立包含所有案例特称的列表会占用大量的内存，这时可以用方法nltk.classify.apply_features(),该方法会返回一个类似列表的对象，对不会把所有的特征都放到内存中。
>>> from nltk.classify import apply_features >>> train_set = apply_features(gender_features, labeled_names[500:]) >>> test_set = apply_features(gender_features, labeled_names[:500])