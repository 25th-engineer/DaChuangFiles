首先申明本人的英语很搓，看英文非常吃力，只能用这种笨办法来方便下次阅读。有理解错误的地方，请别喷我。
什么是卷积和什么是卷积神经网络就不讲了，自行google。从在自然语言处理的应用开始(SO, HOW DOES ANY OF THIS APPLY TO NLP?)。
和图像像素不同的是，在自然语言处理中用矩阵来代表一句话或者一段话作为输入，矩阵的每一行代表一个token，可以是词，也可以是字符。这样每一行是一个向量，这个向量可以是词向量像word2vec或者GloVe。也可以是one-hot向量。如果一句话有10个词，每个词是100维的词向量，那么得到10*100的矩阵，这就相当于图像识别中的图像（input）。
在图像中，过滤器是在图像的部分滑动，而在NLP中过滤器在整行上滑动。意思是过滤器的宽度和输入矩阵的宽度是一致地。（就是说过滤器的宽度等于词向量的维度。）在高度上常常是开2-5个词的滑动窗口。总结起来，一个在NLP上的CNN长这样：
这里写图片描述
这里有3种过滤器，滑动窗口为2、3、4，每种有2个。后面阐述了CNN在NLP上的不足（没看明白）。表示RNN更符合语言的理解习惯。后面又说模型跟实现的理解有偏差，但是CNN在NLP上的表现是不错的。同时也吐槽了词袋模型也一样。（原因鬼知道）
CNN的另一个优势是快，这里用N-Gram模型做对比。我们都知道在VSM模型中采用3-gram的维度就很恐怖了，文中说google也处理不了超过5-gram的模型。这是CNN模型的优势，同时在CNN的输入层采用n-size的滑动窗口和n-gram处理是相似的。（不能同意再多，个人认为部分功劳在word embeddings上。当然不全是，因为即使采用one-hot，维度也不会随着窗口的size变化。而在n-gram中是随着n的变化爆发性增加的。）
（干货，对理解模型和代码都非常必要。）
对于窄卷积来说，是从第一个点开始做卷积，每次窗口滑动固定步幅。比如下图左部分为窄卷积。那么注意到越在边缘的位置被卷积的次数越少。于是有了宽卷积的方法，可以看作在卷积之前在边缘用0补充，常见有两种情况，一个是全补充，入下图右部分，这样输出大于输入的维度。另一种常用的方法是补充一部0值，使得输出和输入的维度一致。这里文中给了一个公式 。这里npadding在全补充里是filter-1，在输入输出相等时，就要主要奇偶性了，注意到卷积核常为奇数，这里应该有原因之一。（思考下为什么）
这里写图片描述
这个参数很简单，就是卷积核移动的步长。下面两幅图左边的步长为1，右边的步长为2。（看出卷积核是啥了吗）
这里写图片描述
这里说步幅常设置为1，在一些更接近于RNN的模型中会设置更大的stride。
一般在卷积层后会有汇聚层。最常用的是max-pooling（就是取最大的那个）。stride的大小一般和max-pooling的窗口大小一致。（在NLP中代表性的操作是在整个输出上作汇聚，每个过滤器只输出一个值。） 为啥要做汇聚？讲了两个原因：一是可以提供确定的输出，对于后面做全连接有用。二是可以在保存大部分信息的前提下降维（希望是这样）。这里说这样的做法相当于某个词是否在句子中出现，而不关心这个词在句子的哪个位置出现。这和词袋模型的思想相同。不同的是在局部信息中，“not amazing”和”amazing not“在模型中会有很大的不同。（这里得好好想想，mark下）
这没啥好说的，就是输入有几层。在图像中一般有1、3层（分别灰度图和RGB图）。在NLP中也可以有多个通道，比如说使用不同词向量化方式，甚至不同的语言等
这里说CNN在NLP中常应用到文本分类中，比如情感分析、垃圾信息识别、主题分类中。由于卷积的汇聚操作会遗失一些词的位置信息，所以较难应用到词性标注和实体抽取中。但是也不是不可以做，你需要把位置信息加入到特征里。下面是作者看的CNN在NLP方面的论文。
这里举了论文[1]中的例子，模型很简单。输入层是由word2vec词向量表示的句子，后面跟着是卷基础，然后是max-pooling层，最后是全连接的softmax分类器。同时论文中还实验了使用两个通道，一个静态一个动态，一个会在训练中变化（词向量变化？参数谁不会变化，mark）。在论文[2][6]还有多加入一层来实现“情感聚类”。
这里写图片描述
[4]中就没有像word2vec这样还要先训练，直接简单粗暴的使用one-hot向量。[5]的作者表示他的模型在长文本中表现非常好。总结了下，词向量这种在短文本中比长文本表现更好。
构建CNN模型要做些啥：1、输入的向量化表示。2、卷积核的大小和数量的设置。3、汇聚层类型的选择。4、激活函数的选择。一个好的模型的建立需要多次的实验，这里作者表示如果没能力建立更好的模型，效仿他就足够了。另外有几点经验：1、max-pooling好于average-pooling。2、过滤器的大小很重要。3、正则并没有卵用。4、警告最好文本的长度都差不多。
剩下的论文就不说了。
[1] Kim, Y. (2014). Convolutional Neural Networks for Sentence Classification. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014), 1746–1751. [2] Kalchbrenner, N., Grefenstette, E., & Blunsom, P. (2014). A Convolutional Neural Network for Modelling Sentences. Acl, 655–665. [3] Santos, C. N. dos, & Gatti, M. (2014). Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts. In COLING-2014 (pp. 69–78). [4] Johnson, R., & Zhang, T. (2015). Effective Use of Word Order for Text Categorization with Convolutional Neural Networks. To Appear: NAACL-2015, (2011). [5] Johnson, R., & Zhang, T. (2015). Semi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding. [6] Wang, P., Xu, J., Xu, B., Liu, C., Zhang, H., Wang, F., & Hao, H. (2015). Semantic Clustering and Convolutional Neural Network for Short Text Categorization. Proceedings ACL 2015, 352–357. [7] Zhang, Y., & Wallace, B. (2015). A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification, [8] Nguyen, T. H., & Grishman, R. (2015). Relation Extraction: Perspective from Convolutional Neural Networks. Workshop on Vector Modeling for NLP, 39–48. [9] Sun, Y., Lin, L., Tang, D., Yang, N., Ji, Z., & Wang, X. (2015). Modeling Mention , Context and Entity with Neural Networks for Entity Disambiguation, (Ijcai), 1333–1339. [10] Zeng, D., Liu, K., Lai, S., Zhou, G., & Zhao, J. (2014). Relation Classification via Convolutional Deep Neural Network. Coling, (2011), 2335–2344. [11] Gao, J., Pantel, P., Gamon, M., He, X., & Deng, L. (2014). Modeling Interestingness with Deep Neural Networks. [12] Shen, Y., He, X., Gao, J., Deng, L., & Mesnil, G. (2014). A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval. Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management – CIKM ’14, 101–110. [13] Weston, J., & Adams, K. (2014). # T AG S PACE : Semantic Embeddings from Hashtags, 1822–1827. [14] Santos, C., & Zadrozny, B. (2014). Learning Character-level Representations for Part-of-Speech Tagging. Proceedings of the 31st International Conference on Machine Learning, ICML-14(2011), 1818–1826. [15] Zhang, X., Zhao, J., & LeCun, Y. (2015). Character-level Convolutional Networks for Text Classification, 1–9. [16] Zhang, X., & LeCun, Y. (2015). Text Understanding from Scratch. arXiv E-Prints, 3, 011102. [17] Kim, Y., Jernite, Y., Sontag, D., & Rush, A. M. (2015). Character-Aware Neural Language Models.