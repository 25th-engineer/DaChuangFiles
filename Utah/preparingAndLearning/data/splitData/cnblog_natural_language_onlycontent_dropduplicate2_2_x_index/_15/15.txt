1 — 语言处理综合工具包（暂未完善）
工具包名
支持语言
受欢迎程度
简介
个人使用评价
HanLP
pyhanlp
中文
1.3W star
HanLP是一系列模型与算法组成的NLP工具包，由大快搜索主导并完全开源，目标是普及自然语言处理在生产环境中的应用。HanLP具备功能完善、性能高效、架构清晰、语料时新、可自定义的特点。
NLTK
多语言
7.8K star
NLTK是一个被广泛使用的高效的Python构建的平台，用来处理人类自然语言数据。它提供了易于使用的接口，通过这些接口可以访问超过50个语料库和词汇资源（如WordNet）。
LTP语言技术平台
中文
2.2K star
LTP提供了一系列中文自然语言处理工具，用户可以使用这些工具对于中文文本进行分词、词性标注、句法分析等等工作。
Stanford CoreNLP
多语言
6.3K star
斯坦福CoreNLP是一个Java自然语言分析库，它集成了所有的自然语言处理工具，包括词性的终端（POS）标注器，命名实体识别（NER），分析器，对指代消解系统，以及情感分析工具，并提供英语分析的模型文件。
spaCy
多语言，中文支持有限
1.34W star
spaCy 是一个Python自然语言处理工具包，诞生于2014年年中，号称“Industrial-Strength Natural Language Processing in Python”，是具有工业级强度的Python NLP工具包。spaCy里大量使用了 Cython 来提高相关模块的性能，这个区别于学术性质更浓的Python NLTK，因此具有了业界应用的实际价值。
gensim
多语言
9.3K star
Gensim是一款开源的第三方Python工具包，用于从原始的非结构化的文本中，无监督地学习到文本隐层的主题向量表达。
它支持包括TF-IDF，LSA，LDA，和word2vec在内的多种主题模型算法，
支持流式训练，并提供了诸如相似度计算，信息检索等一些常用任务的API接口
2 — 分词
工具包
官方简介
个人评测
jieba结巴中文分词
“结巴”中文分词：做最好的 Python 中文分词组件
1、使用人数多（github star1.8万），速度快;
2、虽然准确率和召回率较其他新出分词工具有差距，但是各方面的综合效果还是不错的
pkuseg北京大学开源分词工具
pkuseg简单易用，支持细分领域分词，有效提升了分词准确度。
1、会自动去除空格等空字符；
2、分词粒度较细（例如“北京  工业   大学”）；
3、速度较jieba慢很多（粗略评测速度是jieba的约1/5）
HanLP
HanLP是一系列模型与算法组成的NLP工具包，由大快搜索主导并完全开源，目标是普及自然语言处理在生产环境中的应用。HanLP具备功能完善、性能高效、架构清晰、语料时新、可自定义的特点。
1、使用人数多（github star1.2万），速度约jieba的1/3;
2、综合处理工具包
3 —词向量
资源名称
简介
使用评估
Chinese Word Vectors 中文词向量
100+ Chinese Word Vectors 上百种预训练中文词向量（个人只使用了mixed-large）
1、词汇量128万, 300维度，大小3.4G；
2、词长度分布情况（1:2:3:4:其他）：1.4:27:32:11:28.6；
腾讯高质量词向量
腾讯AI Lab此次公开的中文词向量数据包含800多万中文词汇，其中每个词对应一个200维的向量
1、词汇量882万，200维度，大小15.5G；
2、词长度分布情况（1:2:3:4:其他）：0.25:23:23:22.4:31.35；
3、缺少中文标点逗号等部分标点字符的支持。
5 — 语料库
4.1 实体数据集
数据集
数据集描述
MSRA数据集
30个实体属性，4.6万行，126.5万词，11.8万个实体