词典或者词典资源是一个词和/或者短语及其相关信息的集合，例如：词性和词意定义等相关信息。词典资源隶属于文本，并且通过在文本的基础上创建和丰富。例如定义了一个文本my_text，然后通过vocab=sorted（set（my_text））建立my_text的词汇表，再利用word_Freq=FreqDist(my_text)计数文本中每个词的频率。vocab和word_Freq都是简单的词汇资源。
【词项】包括词目（词条）及其他附加信息。例如：词性和词意
1、词汇列表语料库
1.1NLTK中包括一些仅仅包含词汇列表的语料库。我们可以使用它来检查文本预料中不常见的或者拼写错误的词汇。
例子：过滤文本：本程序能计算文本的词汇表，然后删除所有在现在的词汇列表中出现的元素，值留下罕见的或者拼写错误的词汇。
def unusual_words(text): text_vocab=set(w.lower() for w in text in w.isalpha()) english_vocab=set(w.lower() for w in nltk.corpus.words.words()) unusual=text_vocab.difference(english_vocab) return sorted(unusual) >>>unusual_words(nltk.corpus.gutenberg.words('austen-sense.txt'))
输出结果：['abbeyland', 'abhorred', 'abilities', 'abounded', 'abridgement', 'abused', 'abuses', 'accents', 'accepting', 'accommodations', 'accompanied', 'accounted', 'accounts', ..... 大约有1600个。
1.2还有一个停用词语料库，所谓停用词就是指高频词汇，如the,a,and等等。有时候在进一步处理之前需要将他们过滤出去。
>>>from nltk.corpus import stopwords >>>stopwords=stopwords.words('english')
['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']
可以定义一个函数来计算文本中不包含在停用词列表中的词所占的比例：
from nltk.corpus import stopwords def content_fraction(text): spwords=stopwords.words('english') content=[w for w in text if w.lower() not in spwords] return len(content)/len(text) >>>print(content_fraction(nltk.corpus.reuters.words())) 0.735240435097661
可以看出停用词占了将近1/3的词。
词谜问题：如下图：
词汇列表对于解决上面所示的字谜问题很有用。运行程序去遍历每一个词，检查每一个词是否符合条件。检查必须出现的字母和长度限制很简单，但是指定某些字母出现的两次（V），这样的检查很棘手。利用FreqDist比较法可以检查候选词中的每个字母出现的频率关系。
def puzzle(text): puzzle_letter=nltk.FreqDist(text) obligatory='r' wordlist=nltk.corpus.words.words() res=[w for w in wordlist if len(w)>=6 and obligatory in w and nltk.FreqDist(w)<=puzzle_letter] print(res) puzzle('egivrvonl')
结果为：['glover', 'gorlin', 'govern', 'grovel', 'ignore', 'involver', 'lienor', 'linger', 'longer', 'lovering', 'noiler', 'overling', 'region', 'renvoi', 'revolving', 'ringle', 'roving', 'violer', 'virole']
1.3 还有一个词汇列表时名字语料库，包括8000个按性别分类的名字。男性和女性的名字存储在单独的文件中。
>>>names=nltk.corpus.names >>>names.fileids() ['female.txt','male.txt'] ####寻找男女生都使用的名字： >>>male_names=names.words('male.txt') >>>female_names=names.words('female.txt') >>>[w for w in male_names if w in female_name] ['Abbey', 'Abbie', 'Abby', 'Addie', 'Adrian', 'Adrien', 'Ajay', 'Alex', 'Alexis', 'Alfie', 'Ali', 'Alix', 'Allie', 'Allyn',
'Andie', 'Andrea', 'Andy', 'Angel', 'Angie', 'Ariel', 'Ashley', 'Aubrey', 'Augustine', ...
我们来看看名字最后一位在男女性之间有什么差别
>>>from nltk.corpus import names >>>cfd=nltk.ConditionalFreqDist((fileid,name[-1]) for fileid in names.fileids() for name in names.words(fileid)) >>>cfd.plot
可以看出大多数以a,e,i结尾的名字为女性，以k,o,r,s,t结尾的为男性。
2.发音的词典
2.1NLTK中包括了美国英语的CMU发音词典，他是为语音合成器而设计的。
>>>entries=nltk.corpus.cumdict.entries() >>>print(len(entries)) 133737 >>>for entry in entries[39943:39948]: print(entry) ('explorer', ['IH0', 'K', 'S', 'P', 'L', 'AO1', 'R', 'ER0']) ('explorers', ['IH0', 'K', 'S', 'P', 'L', 'AO1', 'R', 'ER0', 'Z']) ('explores', ['IH0', 'K', 'S', 'P', 'L', 'AO1', 'R', 'Z']) ('exploring', ['IH0', 'K', 'S', 'P', 'L', 'AO1', 'R', 'IH0', 'NG']) ('explosion', ['IH0', 'K', 'S', 'P', 'L', 'OW1', 'ZH', 'AH0', 'N'])
例子：寻找发音包含三个音素的条目，并且第一个发音为'P'，第三个发音为'T',打印满足条件的词和该词的第二个音素
>>>entries=nltk.corpus.cmudict.entries() >>>for word,pron in entries: if len(pron)==3: ph1,ph2,ph3=pron if ph1=='P' and ph3=='T': print(word,ph2)
结果：pait EY1 pat AE1 pate EY1 patt AE1 peart ER1 peat IY1 peet IY1 peete IY1 pert ER1 pet EH1 pete IY1 pett EH1 piet IY1 piette IY1 pit IH1 pitt IH1
pot AA1 pote OW1 pott AA1 pout AW1 puett UW1 purt ER1 put UH1 putt AH1
音素包含数字表示主重音（1）、次重音（2）和无重音（0）。定义一个函数来提取重音数字，然后寻找具有特定重音模式的词汇。
>>>entries=nltk.corpus.cmudict.entries() >>>def stress(pron): return [char for phone in pron for char in phone if char.isdigit()] ####寻找音素为01020的词汇 >>>res=[w for w,pron in entries if stress(pron)==['0','1','0','2','0']] >>>print(res) ['abbreviated', 'abbreviated', 'abbreviating', 'accelerated', 'accelerating', 'accelerator', 'accelerators', 'accentuated', 'accentuating', 'accommodated', 'accommodating', 'accommodative', 'accumulated', 'accumulating', 'accumulative', 'accumulator', 'accumulators'...
3.比较词表
NLTK中包含了所谓的斯瓦迪士核心词列表，包括几种语言的约200个常见词的列表。
>>>from nltk.corpus import swadesh >>>swadesh.fileids() ['be', 'bg', 'bs', 'ca', 'cs', 'cu', 'de', 'en', 'es', 'fr', 'hr', 'it', 'la', 'mk', 'nl', 'pl', 'pt', 'ro', 'ru', 'sk', 'sl', 'sr', 'sw', 'uk']
可以使用entries()方法来制定一个语言链表来访问多语言的同源词。而且，可以把它转换成一个简单的词典、
>>>fr2en=swadesh.entries(['fr','en']) ###法语和英语 >>>translate=dict(fr2en) >>>translate['chien'] ###进行翻译 'dog' >>>translate['jeter'] 'throw'