最近在看《Python自然语言处理》中文版这本书，可能由于是从py2.x到py3.x，加上nltk的更新的原因，或者作者的一些笔误，在书中很多代码都运行不能通过，下面我就整理一下一点有问题的代码。
第一章：
p3.该处为小建议，书中没有错误：关于nltk.book的下载，最好下载到'/nltk_data'文件夹下，如'D:/nltk_data'
p7.text3.generate(). generate()函数用法已经过时，正在查找最新的方法。
p18.关于FreqDist()函数发生了更新，如果按照书上的代码键入，并不会得到预期的结果，可以用下面的方法进行改进来得到相同的结果：
>>>fdist1=FreqDist(text1) >>>len(fdist1) 19317 >>>vocabulary1=sorted(fdist1.items(),key=lambda jj:jj[1],reverse=True) >>>s=[] >>>for i in range(len(vocabulary1)): s.append(vocabulary1[i][0]) >>>print(s)
p22.FreqDist函数，和18页的问题是一样的，可以仿照上面的解决方法进行改进。
p32.babelize_shell() 该函数在nltk3.0中已经不再可用了，跳过该函数讲解部分。
第二章：
p48页：cfd=nltk.ConditionalFreqDist((target,file[:4]) for fileid in inaugural.fileids() for w in inaugural.words(fileid) for target in ['america','citizen'] if w.lower().startswith(target))  会显示出错
改正：将第一个括号内的file[:4]改为fileid[:4]即可。即：cfd=nltk.ConditionalFreqDist((target,fileid[:4]) for fileid in inaugural.fileids() for w in inaugural.words(fileid) for target in ['america','citizen'] if w.lower().startswith(target))
p51:代码最后一行cfd.plot(cumulative=True少了闭括号。
p56:>>>cfd 书上写的是不显示cfd里面的内容，而在Python3.X中，输入这句话会自动输出cfd里面的内容。
p58:使用双连词生成随机文本。输入nltk.bigrams(sent)并不会生成列表，需要写成：list(nltk.bigrams(sent))才能生成书上的形式。
p72：倒数第二行，>>>wn.synset('car.n.01').lemma_names忘记加括号，改为：>>>wn.synset('car.n.01').lemma_names（）
p73:一开始的代码.definition和.examples和上面问题一样，需要加括号才能显示结果。本页上的其他函数也需要同样处理、
P85:如果使用py3，在使用urlopen时需要：from urllib.request import urlopen
P87：NLTK提供了辅助函数nltk.clean_html()这个函数现在不在支持，可以使用beautifulsoup库。
P116：在concordanc函数中，wc=width/4,在py3中会报错，应该改为wc=width//4.
P121：关于nltk.regexp_tokenize(text,pattern)并不会得到预期的效果，需要对pattern进行重写，具体重写代码如下：
pattern = r"""(?x) # set flag to allow verbose regexps (?:[A-Z]\.)+ # abbreviations, e.g. U.S.A. |\d+(?:\.\d+)?%? # numbers, incl. currency and percentages |\w+(?:[-']\w+)* # words w/ optional internal hyphens/apostrophe |\.\.\. # ellipsis |(?:[.,;"'?():-_`]) # special characters with meanings """
重写以后在执行就会出现预期的结果。
由于是刚开始看，所以后面的还没看到，本文也会持续更新新遇到的错误，也欢迎大家补充。