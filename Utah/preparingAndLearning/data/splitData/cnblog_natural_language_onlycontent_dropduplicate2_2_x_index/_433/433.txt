原文转载：http://www.hankcs.com/nlp/cs224n-introduction-to-nlp-and-deep-learning.html
什么是自然语言处理
这是一门计算机科学、人工智能以及语言学的交叉学科。虽然语言只是人工智能的一部分（人工智能还包括计算机视觉等），但它是非常独特的一部分。这个星球上有许多生物拥有超过人类的视觉系统，但只有人类才拥有这么高级的语言。
自然语言处理的目标是让计算机处理或说“理解”自然语言，以完成有意义的任务，比如订机票购物或QA等。完全理解和表达语言是极其困难的，完美的语言理解等效于实现人工智能。（曾经有人问我，是不是光自然语言处理的知识都够一本书的量，无知可笑。另一方面，市面上似乎又有点野鸡书/教程/培训班泛滥的趋势）
自然语言处理涉及的几个层次
作为输入一共有两个来源，语音与文本。所以第一级是语音识别和OCR或分词（事实上，跳过分词虽然理所当然地不能做句法分析，但字符级也可以直接做不少应用）。接下来是形态学，援引《统计自然语言处理》中的定义：
形态学（morphology）：形态学（又称“词汇形态学”或“词法”）是语言学的一个分支，研究词的内部结构，包括屈折变化和构词法两个部分。由于词具有语音特征、句
法特征和语义特征，形态学处于音位学、句法学和语义学的结合部位，所以形态学是每个语言学家都要关注的一门学科［Matthews,2000］。
下面的是句法分析和语义分析，最后面的在中文中似乎翻译做“对话分析”，需要根据上文语境理解下文。
这门课主要关注画圈的三个部分，其中中间的两个是重中之重，虽然深度学习在语音识别上的发力最大。
自然语言处理应用
一个小子集，从简单到复杂有：
拼写检查、关键词检索……
文本挖掘（产品价格、日期、时间、地点、人名、公司名）
文本分类
机器翻译
客服系统
复杂对话系统
在工业界从搜索到广告投放、自动\辅助翻译、情感舆情分析、语音识别、聊天机器人\管家等等五花八门。
人类语言的特殊之处
与信号处理、数据挖掘不同，自然语言的随机性小而目的性强；语言是用来传输有意义的信息的，这种传输连小孩子都能很快学会。人类语言是离散的、明确的符号系统。但又允许出现各种变种，比如颜文字，随意的错误拼写“I loooove it”。这种自由性可能是因为语言的可靠性（赘余性）。所以说语言文字绝对不是形式逻辑或传统AI的产物。
语言符号有多种形式（声音、手势、书写），在这些不同的形式中，其意义保持不变：
虽然人类语言是明确的符号系统，但符号传输到大脑的过程是通过连续的声学光学信号，大脑编码似乎是连续的激活值上的模式。另外巨大的词表也导致数据稀疏，不利于机器学习。这构成一种动机，是不是应该用连续的信号而不是离散的符号去处理语言。
什么是深度学习
这是机器学习的一个子集。传统机器学习中，人类需要对专业问题理解非常透彻，才能手工设计特征。比如地名和机构名识别的特征模板：
然后把特征交给某个机器学习算法，比如线性分类器。机器为这些特征调整找到合适的权值，将误差优化到最小。
在这个过程中一直在学习的其实是人类，而不是机器。机器仅仅做了一道数值优化的题目而已。
下面这张图很好地展示了这个过程中的比例：
而深度学习是表示学习的一部分，用来学习原始输入的多层特征表示：
“深度学习”的历史
虽然这个术语大部分时候指代利用各种各样多层的神经网络进行表示学习，有时候也有一些概率图模型参与。统计学家会说，哦，不过是一些逻辑斯谛回归单元的堆砌而已。也许的确如此，但这还是以偏概全的说法（电子计算机还是一堆半导体的堆砌呢，大脑还是一堆神经元的堆砌呢）。这门课不会回顾历史（像Hinton老爷子那样博古通今），而只会专注当前在NLP领域大放异彩的方法。
为什么需要研究深度学习
手工特征耗时耗力，还不易拓展
自动特征学习快，方便拓展
深度学习提供了一种通用的学习框架，可用来表示世界、视觉和语言学信息
深度学习既可以无监督学习，也可以监督学习
深度学习可追溯到八九十年代，但在2010年左右才崛起（最先是语音与图像，后来才是NLP），那之前为什么没有呢？
与Hinton介绍的一样，无非是以前数据量不够，计算力太弱。当然，最近也的确有许多新模型，新算法。
语音识别中的深度学习
突破性研究来自Hinton老爷子的学生，具体参考：http://www.hankcs.com/ml/hinton-deep-neural-nets-with-generative-pre-training.html#h3-11
计算机视觉中的深度学习
还是来自Hinton的学生。
课程相关
有4次编程练习，会用到TensorFlow。
为什么NLP难
人类语言是充满歧义的，不像编程语言那样明确。编程语言中有各种变量名，但人类语言中只有少数几个代词可以用，你得思考到底指代的是谁……
人类语言的解读依赖于现实世界、常识以及上下文。由于说话速度书写速度阅读速度的限制，人类语言非常简练，省略了大量背景知识。
接下来是几个英文的歧义例子，对native speaker而言很有趣。为了完整性只看一个：
The Pope’s baby steps on gays
主要歧义发生在baby上面，可以理解为“教皇的孩子踩了基佬”，也可以理解为“教皇在同性恋问题上裹足不前”。
旧版CS224d里面还有个更直观的例子，推特上关于电影明星“海瑟薇”的评论影响了保险公司哈撒韦的股价，因为两者拼写是一样的。
说明某些“舆情系统”没做好命名实体识别。
Deep NLP = Deep Learning + NLP
将自然语言处理的思想与表示学习结合起来，用深度学习的手法解决NLP目标。这提高了许多方面的效果：
层次：语音、词汇、语法、语义
工具：词性标注、命名实体识别、句法\语义分析
应用：机器翻译、情感分析、客服系统、问答系统
深度学习的一个魅力之处是，它提供了一套“宇宙通用”的框架解决了各种问题。虽然工具就那么几个，但在各行各业都适用。
word vector
老生常谈了。略过。听说接下来两课都在讲这个，希望有些更深入的收获。
NLP表示层次：形态级别
传统方法在形态级别的表示是词素：
深度学习中把词素也作为向量：
多个词素向量构成相同纬度语义更丰富的词向量。
NLP工具：句法分析
我在《基于神经网络的高性能依存句法分析器》中分析并移植的LTP句法分析器，参考的就是这里介绍的Danqi Chen的A Fast and Accurate Dependency Parser using Neural Networks.pdf。原来她是这门课的TA：
NLP语义层面的表示
传统方法是手写大量的规则函数，叫做Lambda calculus：
在深度学习中，每个句子、短语和逻辑表述都是向量。神经网络负责它们的合并。
情感分析
传统方法是请一两百个工人，手工搜集“情感极性词典”在词袋模型上做分类器。
深度学习复用了RNN来解决这个问题，它可以识别“反话”的情感极性：
注意这只是为了方便理解的示意图，并不是RNN的工作流程。私以为这张图放在这里不合适，可能会误导一部分人，以为神经网络就是这样的基于规则的“决策树”模型。
QA
传统方法是手工编写大量的逻辑规则，比如正则表达式之类：
我见过这类QA系统的实体，挺没意思的。
深度学习依然使用了类似的学习框架，把事实储存在向量里：
客服系统
最著名的例子得数GMail的自动回复：
图源
这是Neural Language Models的又一次成功应用，Neural Language Models是基于RNN的：
机器翻译
传统方法在许多层级上做了尝试，词语、语法、语义之类。这类方法试图找到一种世界通用的“国际语”（Interlingua）来作为原文和译文的桥梁。
而Neural Machine Translation将原文映射为向量，由向量构建译文。也许可以说Neural Machine Translation的“国际语”是向量。
结论：所有层级的表示都是向量
这可能是因为向量是最灵活的形式，它的维度是自由的，它可以组合成矩阵，或者更高维的Tensor。事实上，在实践的时候向量和矩阵没什么本质区别，经常看到为了效率或单纯的美观而pack成矩阵unroll成向量的操作。
旧版视频中Socher还顺便广告了一下他的创业公司MetaMind（已被收购，人生赢家）：
这个demo让我非常惊讶，因为普通NLP演示页面都是让人手工选择要执行的任务的。而这个demo竟然支持用一句话表示自己要执行的意图。不光可以执行情感分析、句法分析之类的常规任务，还可以输入一段话做推理任务。更让我惊讶的是，据说后台所有任务用的都是同一种模型，真乃神机也。据说这种模型是Dynamic Memory Network。另外，他们又发了篇A Joint Many-Task Model:Growing a Neural Network for Multiple NLP Tasks，不知道两者有什么联系没有。
下面两次课会详细地讲解向量表示，希望能带来新的体会。