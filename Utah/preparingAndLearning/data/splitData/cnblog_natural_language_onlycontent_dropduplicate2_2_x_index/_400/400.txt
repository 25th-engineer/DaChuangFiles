前言
NTLK是著名的Python自然语言处理工具包,记录一下学习NTLK的总结。
安装nltk
pip install nltk # 测试 import nltk
安装相关的包
import nltk nltk.download() # 在弹出的界面选择想要安装的包 # 也可以指定安装 nltk.download('brown')
自然语言处理
第一步：获取语料库
语料库又称为词典，涉及多个分类，nltk自带了大量的语料库，意料之中大部分都是英文语料库，随便选一个装上。
import nltk nltk.download('brown') # 布朗大学的语料库
但是我需要的是中文的语料库，发现有一个繁体中文的语料库，装上；
nltk.download('sinica_treebank')
另外还可以安装自己的语料库，将语料库文件放在LTK_DATA/corpora/的目录下
from nltk.corpus import PlaintextCorpusReader corpus_root = r"xxxx" # 获取语料库目录 file_pattern = r"xxx\.txt" # 获取下面的文件 wordlists = PlaintextCorpusReader(corpus_root, file_pattern) wordlists.fileids() # 获取文件列表 wordlists.words("1001.txt") # 获取单词
很多的分词工具自带了标注语料库，可以参考使用。
第二步：分词
将一个句子分解成不同词性的词语，有众多不同的分词工具，英文分词简单，中文如jieba，foolnltk等。
英文分词
# 安装库 import nltk nltk.download('punkt')
import nltk text1 = 'I like the movie so much!' raw_words = nltk.word_tokenize(text1) print(raw_words)
中文分词
# 安装 pip install jieba
import jieba seg_list = jieba.cut("我来到北京清华大学", cut_all=True) print("Full Mode:" + "/ ".join(seg_list)) # 全模式 seg_list = jieba.cut("我来到北京清华大学", cut_all=False) print("Default Mode: " + "/ ".join(seg_list)) # 精确模式 seg_list = jieba.cut("他来到了网易杭研大厦") # 默认是精确模式 print(", ".join(seg_list)) seg_list = jieba.cut_for_search("小明硕士毕业于中国科学院计算所，后在日本京都大学深造") # 搜索引擎模式 print(", ".join(seg_list))
英文文本的词形归一化
词干提取
# 三个常用的类PorterStemmer, SnowballStemmer, LancasterStemmer from nltk.stem.porter import PorterStemmer, SnowballStemmer, LancasterStemmer porter_stemmer = PorterStemmer() snowball_stemmer = SnowballStemmer('english') # 选择一门语言 lancaster_stemmer = LancasterStemmer() print(lancaster_stemmer.stem('looked')) print(porter_stemmer.stem('red')) print(porter_stemmer.stem('looking')) # 除去ing等保留词干 snowball_stemmer.stem('looking')
词形归并
from nltk.stem import WordNetLemmatizer wordnet_lematizer = WordNetLemmatizer() print(wordnet_lematizer.lemmatize('are')) print(wordnet_lematizer.lemmatize('went'))
词性标注
import nltk words = nltk.word_tokenize('Python is a good language.') print(nltk.pos_tag(words))
去除停用词
停用词都是人工输入、非自动化生成的，形成停用词表,分词后判断是否是停用词。
from nltk.corpus import stopwords filtered_words = [word for word in words if word not in stopwords.words('english')]
英文语言处理实例
安装需要的库
import nltk nltk.download('wordnet') nltk.download('punkt') nltk.download('stopwords')
实例
import nltk from nltk.stem import WordNetLemmatizer from nltk.corpus import stopwords def proc_text(text): """ 处理文本 """ # 分词 raw_words = nltk.word_tokenize(text) new_words = raw_words # 词形归并 wordnet_lematizer = WordNetLemmatizer() words = [wordnet_lematizer.lemmatize(new_word) for new_word in new_words] # 去除停用词 filtered_words = [word for word in words if word not in stopwords.words('english')] # 去除标点 res_words = [word for word in filtered_words if word not in ',.!' ] return res_words if __name__ == "__main__": with open('./english.txt') as f: text = f.read() print(proc_text(text))
中文语言处理实例
import jieba def chinese_text(text): """ 处理中文文本 :param text: :return: """ # 分词 word_list = jieba.cut(text, cut_all=True) # 除去空字符 words = [word for word in word_list if word] return words
总结
自然语言处理按步骤来分别为获取语料库、加载语句进行分词、词型归一化、最后输出处理结果，后续在此基础上可以进一步统计、过滤等。
参考
https://github.com/fxsjy/jieba/tree/master
http://www.nltk.org/py-modindex.html