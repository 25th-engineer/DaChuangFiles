原文链接：http://www.xianguo.com/go.php?fi=4540220
自然语言处理领域的两种创新观念
作者：张俊林
2006年11月26日
自然语言处理作为一个研究领域，曾经是一个颇为冷门的方向，但是现在随着互联网搜索概念股的疯狂被投资人追捧，搜索和自然语言处理逐渐成为学术领域的显学。借着感恩节的当口，让我们这些靠自然语言处理技术混饭吃的兄弟们也表达一下感激之情：感谢CCTV,感谢CHANNEL V….不对，排错马屁了。应该是：国际主义阵营感谢GOOGLE，民族主义阵营感谢百度，是你们在纳斯达克上市给了我们这些人混口饭吃的机会，使得我们从吃不饱饭的非洲难民阵营进化到勉强能吃饱的社会主义初级阶段那群人的阵营，顺便还带给我们跨入吃得更好的资本主义阵营的梦想。
其实，真正应该感谢的是互联网，现在互联网的数据实在是太多了，所以现在大家上网面临的问题不是没有信息的问题，而是信息太多找不到自己所需要的信息，这个时候搜索和语言处理就体现出用武之地了。我们需要采取技术手段把过多的嘈杂的信息整理的头头是道，这样网民才能便捷地找到自己想要的东西。所以我个人乐观的认为，随着互联网的发展，搜索和自然语言处理会成为越来越重要的工具。
自然语言处理作为一个研究领域，其成长历程应该说是比较坎坷的。很早的时候，也曾风光过，通俗的说就是：咱也阔过。那时候研究人员都采用规则的方法，就是大家想一些处理规则，然后计算机按照人想的规则去处理文本。开始大家都还是很乐观的，期望自然语言处理能够大师拳脚，很快应用到各行各业。但是现实的残酷很快打碎了人们的美梦，发现现实世界的复杂不是人想出一些规则就能搞定，而且规则多了还会出现规则之间打架的问题。总而言之，自然语言处理(NLP)成为了一个鸡肋方向，食指无味，弃之可惜。直到统计方法破石而出，NLP才见到了一丝曙光，并且有渐渐光大门楣，光宗耀祖的趋势。现在统计方法基本上占了所有 NLP子领域的山头，漫山遍插统计大王旗，统计方法应用效果也确实不错。基本上可以进入实用阶段了。
但是，目前NLP学术研究基本上处于发展平台期，就是说大局已定，能做的就是在一些细枝末节的方向上做些修修补补的工作，你去看ACL/COLING这些最高级别的国际会议的论文就知道所言非虚，一个研究领域进入平台期的标志是：假设你几年不看论文，等想起来去看最新的论文，发现大家还是在一个圈子里面绕来绕去的。现在的研究圈子模式已经变成了：各种数学模型是一个万能工具箱，研究人员从这个工具箱里面取出不同的工具，然后用这些工具来进行修修补补的工作。场景基本上如下：A博士说了：你用隐马尔科夫分词？那我用隐马尔科夫标注词性；此时又跳出来一位B博士：你们太落后了，居然还在玩隐马尔科夫？我都玩到最大熵了。话音未落，C博士飞起一脚把B博士踢下台去：瞧你那熊样，还最大熵呢?你以为现在才是二十一世纪初啊(B博士敬佩而又无辜的眼光望着台上的C博士，挠着头想：难道现在不是二十一世纪初么），听说过CRF么？我不仅CRF了，我都CRF好几年了。
总而言之，现在NLP研究基本上和补鞋匠的工作有的一拼。就好像用不同型号的胶水来补不同牌子的鞋子一样，看着挺热闹，其实没啥意思在补也不能把一双布鞋补成一双运动鞋，顶多是把一双破布鞋补成看上去不那么破的布鞋而已。有时候，补完一个小洞后又露出一个大洞，只是布鞋匠不说而已。
说说我理解的NLP的两种创新。其实，其他领域估计也差不多，而且，我的看法看起来相当象废话，其实基本上就是废话，世上废话本来就很多，在多两句也无妨。
一种创新是研究模式的颠覆，这需要大智慧，是所谓的大创新。就像刚开始的规则方法的出现，后来统计方法的一枝独秀，再到最近的大家都嚷嚷要把统计和规则结合起来搞。当然，我个人对两者结合的效果持怀疑态度，因为以我愚钝的智力看不出两者到底有多大的互补性，至于是否真有效那就走着瞧吧。现在需要的是一种完全不同的处理思路，至于是什么，估计谁也不知道，NLP呼唤爱因斯坦。
另外一种创新是应用创新，就是说大家采用的核心技术其实差不多，都那么点货，其实你也不用藏着掖着，你怎么做的外人不知道，内人还不知道么？这个时候最好的方法是用同样的核心技术做不一样的应用。应用创新可能是目前更加值得关注的创新方法。
至于搜索研究领域，跟NLP处境差不多，基本上是难兄难弟的关系。从最初的内容匹配到后一阶段的链接分析，在之后基本上停留在链接分析上没怎么动过窝，大家一样在从事补鞋的工作。
说道搜索，就顺便谈谈国内的搜索公司，其实百度也好，雅虎也好，包括后起之秀搜狗，奇虎也好。大家用的什么技术估计自己心理都有数，哪个敢跳出来说我有独门秘笈？如果真跳出来了，只能问候一声：骗子你好。除此之外，无话可谈。大家技术上其实都差不多，可能闻道有先后，但是道就是那些道。
注：文中提到的CRF指的是条件随机场模型。