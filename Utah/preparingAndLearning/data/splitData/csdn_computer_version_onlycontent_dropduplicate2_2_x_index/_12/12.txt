1. 计算机视觉在机器人上的应用：
该部分内容源自一篇中文文献，由于是在大约一年前读的，现在只是把当时的笔记复制过来，具体是哪篇文章会随后去找，如果有知情的也可以告诉我，谢谢大家。
1.1 传统工业机器人（机械臂）的工作原理是“示教-再现”的模式，这种模式缺乏对非设计情况的适应性。这篇文章是将计算机视觉技术利用在原有机械臂控制系统上，从而提高机械臂对不同环境条件的适用性。
1.2 具体构成图如下：
1.3 视觉系统算法构成：
1.4 软件实现基本流程：
1.5 这个项目中用的是定标算法，在实际拍摄场景中标定几个定位点，然后根据定位点确定摄像机和物体的3D位置。
1.6 本项目实现流程图：
2. 人机交互中计算机视觉应用：
2.1 人机交互的组成：人机交互可以大概分为两个组成部分，语音和视觉，语音对应于语音自动识别，其试图构造能够感知人们交流的文字方面的机器；视觉对应于计算机视觉技术。其致力于构造能够“观察人”并自动感知相关视觉信息的机器。
2.2 计算机视觉在的发展定义：计算机视觉是一门试图通过图像处理或视频处理而使计算机具备“看”的能力的学科。通过理解图像形成的几何和辐射线测定，接受器（相机）的属性和物理世界的属性，就有可能（至少在某些情况下）从图像中推断出关于事物的有用信息。
2.3 计算机视觉发展的关联性：传统意义上，计算机视觉由诸如生物视觉建模、机器人导航和操作、监控安防、医疗图像及各种检查、检测和识别推动的。近年来，计算机视觉呈现多模态感知交互的态势。
2.4 计算机视觉中的人机交互：着重于建模、识别和解释人的行为。
1）人脸检测和定位：场景中有多少人，他们在哪里？
2）人脸识别：他是谁？
3）头和脸部的跟踪：用户的头部在哪里，脸部的位置和方向是什么？
4）脸部表情分析：用户在微笑，大笑，皱眉，说话还是困乏？
5）视听语音识别：使用语音识别以及伴随视话（lip-reading）和face-reading，判断用户说什么？
6）眼睛注视跟踪：用户的眼睛朝哪里看？
7）身体跟踪：用户的身体在何处？关节处（articulation）是什么？
8）手跟踪：用户的手在哪里？是2维的还是3维的？特别地，手的结构是怎样的？
9）步态识别：这是谁的走路/跑步风格？
10）姿势、手势和活动识别：这个人在做什么？
※ 人机交互的难点：
这些任务都非常困难，从一个摄像机拍得图像（有时或者是多相机从不同的视角）开始，这项 工作典型情况下至少包括每秒30次的240*320个像素（每像素24比特）。我们试图很快地使这些数据变得有意义。与语音识别问题相比较，语音识别是从一个一维的，时间序列信号开始，然后尝试将其分段并分类成相对少数目的已知类别（音素或词）。计算机视觉事实上是一堆子问题的集合，这些子问题彼此间很少有共同点，且都非常复杂。
2.5 基于计算机视觉的前沿成果：
虽然计算机视觉在局部取得了进展，但是依然没有被真正的商业应用，不过有一些征兆显示商业应用即将到来。
1）摩尔定律（英特尔的创始人Gordon Moore：当价格不变时，集成电路上可容纳的元器件的数目，约每隔18-24个月便会增加一倍，性能也将提升一倍。）；相机技术的进步；相机的进步；数码视频的普及；软件的推广（Inter的OpenCV库）。
2）美国政府资助的人脸识别项目：FERET项目（1993-1997）和FRVT项目（2000-2002）。
3）DARPA资助的远距离识别人和视频监防的大型工程。
4）Geometrix,A4Vision和3Dbiometrics。
5）MIT媒体实验室的幼儿室工程。
2.6 技术挑战：
虽然有很多这一类研究项目，但是为了从实验室走向商业化，几个问题需要说明：
1）鲁棒性：大多数视觉技术是脆弱，缺乏鲁棒性的，照明和相机位置的微小变化可能会导致系统出错。系统需要在各种条件下工作，且能适度地、快速地从错误中恢复。
2）速度：对于大多数计算机视觉技术，在全面和快速交互两者间都采取了实际折衷。视频数据太多了，以至于无法实时地做复杂处理。我们需要更好的算法、更快的硬件设备和更灵巧的方法来决策需要计算什么，可以忽略什么。（提供了已处理的图像流的数码相机能够有很大的帮助）
3）初始化：许多技术在得到了初始模型后，跟踪效果都很好，但是初始化步骤往往很慢且需要用户参与。系统必须能快速和透明地进行初始化。
前三个问题已经在日常的研究实验室和全球的产品研发组织得到了关注，使用性和上下文集成很少被考虑，但是随着更多的应用开发，这两个问题将会提到研究日程的前面。
4）使用性：对于开发系统的人来说（花费了许多时间研究复杂难点）视觉技术的示范使用能工作地很好，但对于那些没有经过“系统训练”的新手却很困难。这些系统需 要适应用户，处理无法预期的用户行为。此外，它们需要提供简单的纠错和处理错误解释机制以及能提供反馈给用户，以避免预料之外的灾难性后果。
5）上下文集成：一个基于视觉的交互技术本身不是最终的结果，而是一个更庞大的系统中的组成部分。手势和活动需要放在合适的应用过程中加以理解，而不是孤立的行为。从长期来看，这需要在各种应用的上下文关系中深刻地理解人类行为。
3. 基于计算机视觉的智能机器人设计：
3.1 这个项目的设计内容包括传感器模块，图像处理模块和执行模块，目的是用于煤矿井下发生突发事件时井下环境的探测，完成安全检查和监控等功能。
3.2 原始输入图像是连续的数字视频图像，系统工作时，需调用图像处理模块对原始输入图像进行缩小、边缘检测、二值化、哈夫变换等处理，从而获得有用的路径信息，运动控制模块根据此信息作出决策，通过无线串口将控制命令发给机器人。机器人接收指令在电机控制模块下作出相应的移动。
3.3 具体来说，这个项目是在视频中加有一个引导线，引导线在图像中的方位将会控制机器人的走向。此项目的局限性在于，此项目的机器人需要时刻保证引导线在视野内，这对于环境的适用性下降了。
3.4 本项目的设计模块统称为“上位机”。下位机是具体的执行平台。下位机的控制芯片采用NXP公司的ARM7中的PC2132微处理器，用来完成命令接收、电机控制和机器人状态信息的上传等功能。
3.5 电机控制电路利用L298芯片构成差动方式驱动电机运行，通过处理器驱动PWM信号控制电机转速，并且采用了PID闭环控制。