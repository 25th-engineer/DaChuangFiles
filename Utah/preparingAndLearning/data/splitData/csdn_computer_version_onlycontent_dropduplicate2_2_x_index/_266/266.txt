本文系网易新闻-智能工作室出品
聚焦AI，读懂下一个大时代！
近日，中国科学院深圳先进技术研究院副院长、香港中文大学教授汤晓鸥教授在杭州云栖大会发表题目为《人工智能的云中漫步》的演讲。
他表示，总结起来做人工智能的跟阿里的理念其实相似，阿里讲“让天下没有难做的生意”，做人工智能是讲“让天下没有难吹的牛”。
他还分享了商汤科技在计算机视觉方面的研究成果，比如如何分辨云和雪和地面的物体，用人工智能、图像识别技术，可以做的比人更精准，他举例到，十一的时候很多人去旅游、去登山，山上可以看到半山腰有些云，登到山上以后发现进到云里就变成雾了，拍照就不太清晰，我们有个算法可以帮你把雾去掉。
汤晓鸥还介绍了目前有关城市大脑的应用实践。
1
人脸识别已经可以做到没有任何人工配合的情况下实时的识别人物、抓捕犯人，在广州、深圳、重庆等几十个城市都已经开始帮助公安解决了大量的案件，抓了很多犯人。
2
人群，现在可以在上海外滩这样的公共场合，实时判断每一个点的人群的密度、人数，进来多少人，出去多少人，还有人流有没有逆行等特殊情况，这样可以防止踩踏事件。
3
视频结构化，可以把视频里面所有的人、车、非机动车、自行车检测、追踪，识别出来属性，比如这个人穿什么衣服，男的女的，多大年龄，车什么牌子，哪年生产的，这些东西都从视频处理成文本文件，你可以进行对应物体的快速搜索。
以下是汤晓鸥教授演讲实录：
汤晓鸥：我先帮大会发一个通知，今天午饭取消了，改下午茶了，大家不着急， 慢慢听吧。
非常感谢阿里的邀请，尤其是做压轴演讲，我跟阿里说太客气了，压轴这么重要的演讲应该马总做，我做个简单的开场演讲就差不多了，后来他们坚持我在午饭时间做压轴。
另外他们还告诉我说今天有大概一千万人在网上看直播，所以我非常紧张，我就做了个一百页的PPT，我想十万人一页也对得起观众了，但是组委会就非常紧张，一直问我说40分钟讲一百页，会不会超时，我就跟他们保证说，放心吧，一定会的。昨天奥委会的客人讲到奥运要更快更高更强，那云栖大会的特点就是要更长。
前些时候我跟马化腾还有一些学者在清华做了一次对话，我当时当着马化腾先生的面提了一些意见。今天我到了阿里这里，我想我也不会客气的，作为学者，我们就是要敢于提意见。所以当时我敢于当面给Pony（马化腾）提意见，今天（到了阿里这）我就准备在背后再给Pony提一些意见（此处玩笑，请勿当真）。
现在言归正传，今天我讲的题目是《人工智能的云中漫步》。人工智能其实我也听了很多人讲，我自己也讲了很多，我觉得总结起来做人工智能的跟阿里的理念其实非常相似。阿里是讲“让天下没有难做的生意”，做人工智能的是讲“让天下没有难吹的牛”：）。
这两天大会听大家讲了半天人工智能，讲了半天的云，一直到今天为止，一直到现在为止，我们其实一直没有看到真正的云，现在我给大家看一下。
这是高分一号卫星拍的云图。其实我们发射卫星拍摄这些图像，是为了分析地面上的情况。高分一号拍出来的图，有云有雪，遮盖了地貌，如何分辨云和雪和地面的物体，我们用人工智能、图像识别技术，可以做的比人更精准。大家可以看到褐色的是雪，白色的云，绿色的是物体。我们识别的这些云以后，还能用算法把这些云去掉了，这样卫星就可以识别云下面的东西。
十一的时候我想很多人去旅游、去登山，山上可以看到半山腰有些云，登到山上以后发现进到云里就变成雾了，拍照就不太清晰，我们有个算法可以帮你把雾去掉。还有你航拍的时候，有一些云、雾，我们也可以用算法实时的在视频里把它去掉。
大家想我们杭州好像很少有雾霾，这个跟杭州有什么关系？确实也没什么关系，当年做的时候，是专门给北京做的，给北京量身定制的，奥运会时直接把雾去掉了，蓝天白云的，我们把这个叫商汤蓝。
这个算法，这个应用我们已经把它做到微博相机上成为产品了，去年就已经上线了。
如果这么一直讲下去，阿里的人可能急了，我们是阿里云，不是阿里气象局。当然，我们讲的是虚拟云，云计算，我们其实在不知不觉间已经生活在云中间了，我们生活在物理云下面，实际上我们也生活在虚拟云上面。今天就给大家讲你是如何在云上生活一天的，大概要讲8个小时。
一开始，早上起来要化妆。就是拿着手机可以当镜子，可以做美颜，换衣服，用各种特效效果看一整天该穿什么。这个化妆下来大概的时间从8点开始的，最后结束了以后，就到9点了，一般女孩的话大概也确实需要一个小时化妆，最后通过美颜、增强现实（AR）这些特效，不知道为什么最后这张图成兔女郎了。
然后接着这些AR技术还可以应用在其它的场景上，比如社交场景应用，你们现在看到的这些拍照APP，直播APP，有很多AR特效，其实绝大部分都是基于我们提供的人工智能技术，比如人脸的106点和最新的240点的追踪分析，是我们定义的行业标准。
我们不但做人脸、手势识别，现在已经做到三维的SLAM特效了，大家可能看过这种特效，游戏里面可以把虚拟物体加到这个现实世界里面，但是以前看的都是在一台很强的计算机上算出来的，现在我们这个是在手机端，手机上实时算出来，这是非常难的事情。
还有你刚才为什么花了一个小时换衣服呢？要一件一件换，不合适换另一件，很麻烦。我们实际上可以用计算机帮你换衣服，计算机生成衣服。这个用什么做的呢？用基于自然语言处理的图像图像生成技术，比如说我要一只小鸟，有白色的胸脯，灰色的头部，就生成这样的小鸟；再要一只红色的小鸟，黑色的翅膀，就再对应生成出来图像。这都是计算机自动的根据你的语言描述生成的，或者是花也一样，可以生成一些不同的花。
更实用的应用是什么呢？是衣服。我可以说我想穿一件浅蓝色的连衣裙就换成浅蓝色的连衣裙，或者黑色无袖外套就给你换上了，这样换衣服的速度非常快，几分钟就完事了。
十点钟要出门了。出门走路的时候，可能没有什么感觉，但实际上每个城市里，刚才讲都有几十万台甚至百万台相机，这些相机做的事情是把人、车，物体都检测、识别、分析出来。
今天讲了很多关于城市大脑的问题。
我们要解决这些问题还是需要核心技术，来一样一样完成这些任务。首先我们人脸识别，已经可以做到没有任何人工配合的情况下实时的识别人物、抓捕犯人，在广州、深圳、重庆等几十个城市都已经开始帮助公安解决了大量的案件，抓了很多犯人。
人群，我们可以在上海外滩这样的公共场合，实时判断每一个点的人群的密度、人数，进来多少人，出去多少人，还有人流有没有逆行等特殊情况，这样可以防止踩踏事件。
再就是视频结构化，可以把视频里面所有的人、车、非机动车、自行车检测、追踪，识别出来属性，比如这个人穿什么衣服，男的女的，多大年龄，车什么牌子，哪年生产的，这些东西都从视频处理成文本文件，你可以进行对应物体的快速搜索。
所以其实你在走在路上的时候，所有的这些信息都是可以记录下来的。所以以后如果做坏事会越来越难。大家如果现在还有什么事没做赶紧做，以后再做相对会困难很多了。
12点钟大家可能出去跟朋友玩了。拍一些自拍照，其实拍的时候，就是用了我们的一些视频处理的技术，比如把一个手机拍照拍成单反的效果，这也是我们做的技术，先拍照后聚焦，拍完点什么地方就聚焦到什么地方。另外在拍之前，我就想看看单反预览效果是什么样子的，所以这时候你在动的时候，效果就要显示出来，这就是要实时视频级的处理。视频上能够实时把深度信息算出来，预览做出来。这些技术已经在OPPO R9S和R11用了很长时间了，包括里面的人脸技术都是使用我们的技术支持。
还有手机上可以做一些智能相册的特效、处理。计算机识别你的照片内容，然后根据内容打标签、分类管理。
这些特效，大家现在手机上可能节日期间也会用到一些这些应用，比如把卡通图片里的脸换成自己小孩的脸。但是我给你演示这些是我们十年前做的，我们十年前已经做到这个效果了，当然那个时候是在计算机上做出来的，现在把这些技术可以做到手机上了。
我们跟小米合作做了小米智能相册，跟华为合作做了华为智能相册，跟微博合作，把大V的照片管理做起来。
两点多钟，你照完相了，吃完饭回来，对照片想处理，做一些新的艺术化的滤镜。
感觉我们公司的人基本不干活，整天在玩手机。
处理出来这些特效，这是在图像上做成的特效，其实这是我们两年前做的工作，现在满大街都是。我们现在又做了新的工作，是视频上实时也可以做出特效，而且可以做出各种特效。
4点钟，大家可以出去玩一玩，可以做一些体育运动了，大家可以想像一下，我们公司4点钟就下班了，开始去玩了。
这个就是我们在实时的把人体的整个结构都能跟踪出来，大家可能觉得这个不是什么新鲜事，因为几年前Kinect体感摄像头就能做的，但是原来是一个昂贵的特殊设备做的，设备有两个摄像头还有激光投影，我们是用一个几块钱的单个webcam，可以实时做这件事情，所以这个应用可以在各种的智能家居、自动驾驶，各种地方做到实用。
再往下用这些技术还可以做体育运动的分析。昨天讲到奥委会跟阿里合作，我们也在跟国家体育总局做合作。这个大家可以看到我们用智能分析的方法跟踪运动员的动作。然后也可以帮助运动员做康复的训练。所以昨天奥委会朋友讲，奥运会要做到更高、更快、更强、更聪明，那其实我现在给你讲的，就是如何做到更聪明。
同时我们可以用跟踪的算法，然后把整个画面分析清楚，用自然语言描述视频里运动员到底在干什么。
然后大家下班的时候要坐车回家了。这时候可以乘坐由我们自动驾驶技术支持的汽车。自动驾驶里面我们做了六个大的方向，三十几项技术，目前跟全球前五大车企其中一个顶级的厂商进行合作。
下面看一下刷脸支付场景，因为你下班了，总是要买东西的。可以用刷脸支付，阿里无人店可以用这些技术。还有一些门禁系统，酒店，机场等等应用，所有这些地方其实现都在用我们做的人脸识别技术，现在的准确率从当年第一次超过人眼睛极限的时候，从97.5%，到99.15%，到99.55%，一直做到万分之一，十万分之一，百万分之一，今天我们早就做到亿分之一，实际上已经达到了八位数密码的精度，可以做各种应用了。
到了晚上，这个视频里，我们分析人的运动方向。这些对整个分析视频的结构也是非常重要的技术。
我们综合前面这些技术，可以把整个这个视频场景分析全部做出来。可以看到左下角会讲你在什么地方，什么样的活动，每个人是哪一个人，哪一个演员，穿的什么衣服，后面有什么物体，骑的什么摩托车，所有这些结构化都可以做出来，大家网上看到很多公司用这两段视频结果演示做宣传，这个原创是我们做的，视频分析演示也是我们做的。这是《欢乐颂》，本来想做一个更新的，想用《我的前半生》，后来一想我的前半生也快过去了，还是做《欢乐颂》了。
刚才很多是我们已经落地的产品，是由我们的400多家合作厂商真正落地来用了的。下面还有一些新的技术突破，明天就可以马上用出去，就是因为这些新的技术突破，才继续推动做出来新的应用。
首先讲运动监测。还是回到奥运会这个应用，实际上我们可以在体育的视频里面把这些射门的镜头提取出来，两个小时的比赛可以很快缩到几分钟，可以完全自动做的。
或者田径比赛，真的很漫长的，但是精彩的镜头，百米、跳高那几个镜头，就是那几块，我们可以自动的识别提取出来，同时你也可以进行描述，要求怎么样提取出来，你感兴趣的部分。
然后还可以进行搜索。比如你要搜索音乐表演的视频，战争场面的视频，都可以自动搜索出来。
或者你要想做电影自动理解。比如可以明白这个镜头到底是灾难的镜头还是浪漫的镜头，用我们前面说的技术来分析整个场景到底是什么样的，红线代表浪漫的，蓝色是灾难的，实时分析镜头。或者说他们在吵架还是浪漫的镜头。都可以实时分析出来。
可以用自然语言来描述来搜索电影的场景。就是你可以说一段话，它就把那一段镜头的场景把它给搜出来，同时把所有人，物体和各种东西都检测出来。
还有对体育场景进行分析，就是说可以直接对运动视频进行描述，自动用自然语言描述到底发生了什么事情，这个时候其实我们就不需要播音员了，机器自动分析运动场景做什么，直接给大家讲解，就像一个专业播音员一样。
还有图像的分割，以前大家讲图像分割都是前景和背景分开，现在做的分割是不但把前景和背景分开，而且还可以像素级地把前景的每一个物体分开，前面有很多跳舞的，每一个人都标注出来，每一个物体，和背景都分割开来，就可以做很多很多各种各样的特效。
还有就是判断两个人的关系，如果你在网上放了照片，我们根据你这两个人的姿势和两个人的表情，分析出来你们两个人的关系。这个有什么用呢？比如说你跟一个很有钱的人照相，分析的结果是很友好，说明你认识有钱人这样可能你的可信度就增高了，我就可以把钱借给你了，可以做征信的一个维度。
还有我小孩的照片，他女朋友比较多，想知道哪个是他真正的女朋友，可以分析识别一下，后来发现每一个都是，他跟我一样对每一个都很专一。
我给学生发了一些比较难处理的关系的照片，比如铭铭6个月的时候跟他第一个女朋友的照片，第一个关系分析的还可以，第二个也分析出来了。后面两张照片就难多了，最后基本上搞不清楚他在干什么了，当然最后这张的这种探索精神还是值得敬佩的。
我们以前在微软的时候，出去玩的时候照了照片，我也拿过来让机器分析，这是我的两个同事照的我们在九寨沟的照片，机器分析出来的结果不明白真正的含义是什么？（这两个男同事的背影合影）实际的含义是我们在演绎《断背山》这个电影。下面这几张就更难的让机器分析了。一个人的背影还好，两个人也可以理解，出来三个人的背影，机器就糊涂了，到四个人的时候可更糊涂了，五个人就更接受不了了。所以这种对机器来说很难理解，对我们来说，我们是很开心的可以笑出来。我想在这里，提出一个新研究课题，提出一个挑战吧，就叫XO Challenge吧，就是我们怎么能让机器笑？就是你怎么能让机器识别一张图像是搞笑的，我们人可以分辨，机器能不能做到？我希望我们研究人员以后可以试试，看看我们是不是能够让机器看到这些图像，也会会心一笑。
最后我用一个我们研究的例子来讲一下原创的难度，我刚才讲的每一个技术其实都不是那么简单的，都不是说一拍脑袋一下就做出来的，有非常多的事情要做的，这个例子是图像超分辨率增强，就是我们怎么把一张图放的很大，能够恢复的很清晰。这是美国的一个电影，FBI在抓人。最后他抓到一张很模糊的图像图像放大做成清晰的图像，当时觉得FBI很厉害，非常棒。我们用传统的技术也试图把这个图像恢复一下。当时希望把小图恢复成这样，用传统算法做了最大的努力，最后的结果是这样，所以我们很不满意。
这个应用有什么用处呢？实际上是把可以进行图像、视频放大，可以把普通的电视信号变成4K的高清信号，8K的高清信号，这是我们最新做出来的结果，可以看到如果直接放大是很模糊的，现在用新的结果基本上达到高清的效果，已经达到实用的阶段。
在日本有个工作叫WAIFU2X，他们用我们的技术做了演示，就是把太太（二次元妹子）放大两倍，然后用这个图，最后的效果非常清晰。
超分辨率这个工作是很重要的，因为有很多场合有应用。所以谷歌、推特也对这个非常重视，他们在2016年连着发四篇文章做这个工作。按照以往，大家可能都是跟着谷歌后面做，而我们不是，我们发表了全球第一篇用深度学习超分辨率文章，那是2014年，早于谷歌两年，2015年又发了一篇，2016年两篇，2017发了三篇，我们不但是做的最早的，第一个做的，而且也是目前做的最好的。所以是谷歌在跟着我们做！
做这一项工作要想做成功，牵扯的工作是非常多的，有各种各样的技术，涉及到十几篇几十篇的文章才能做到现在的效果。
所以现在我们已经可以做到实用，在街头上拍的照片，模糊照片可以真正看到罪犯的样子。
而且已经给深圳的公安用了，公安用手机可以拍人的照片，很模糊的图像可以在库里搜索，实时抓捕罪犯。
经过我们的努力，所有这些加一起，从原来这个效果现在可以做到这个效果了。
所以每一项工作后面都有大量的工作需要做的，都有大量的顶级文章。我们不是刚刚这几年人工智能热了才开始做的，而是十五六年的积累，04年到08年我们统计了一下在两个顶级的会议上，我们一个实验室发了57篇论文，而MIT全校是51篇，伯克利大学是33篇，牛津大学是45篇；我们十几年在顶级会议文章数量上一直是在全球领先的。在过去两年，三个顶级会议上我们统计了数据，微软最多是发了124篇，CMU是86篇，我们排第三是76篇，是亚洲唯一的进入前十名的。所以我们是有这种强大的人才和经验的积累，才做出刚才这些真正落地的产品。
在2011年到2013年深度学习刚刚开始的时候，这两个顶级会议上，29篇文章我们占了14篇，全球的一半，这里面16项技术，都是我们第一个真正成功的把深度学习应用到这些技术领域。
所以我们是深度学习的原创技术公司，是真正做平台的，和脸书的Torch、谷歌的TensorFlow一样，我们做了自己的原创平台Parrots，来在这上面开发我们深度学习相关的技术。
7月份的时候，我很荣幸作为国际期刊IJCV主编，召集了夏威夷IJCV Night晚宴会议，计算机视觉领域很多顶级学者都参加了我们的这个晚宴。我们在马上10月份，在威尼斯的ICCV大会上会再开一次这样的国际顶级学者的·聚会，欢迎大家过来参加。
最后，大家看一下这一页上的这些图像的一个共同的点是什么？米开朗基罗、贝多芬、梵高、乔布斯、兰博基尼的设计首席设计师，这些人有一个共同特点，其实就是两个字：原创。中国最缺的就是原创，我们现在做的就是原创，做原创是非常难的一件事情，但是不做原创一个国家是永远也发展不起来的。
我们在做电影分析的时候，看到这些老的电影，《上甘岭》《英雄儿女》《小兵张嘎》，我们团队的120个博士很像当年《上甘岭》上最后一个加强连，一个博士的加强连。但是以我们这一个加强连的兵力看起来很强大，但是对手是谷歌、微软、IBM这样强大的对手，我们是需要援军的，需要炮火支援，用《英雄儿女》里面王成的一句话，就是向我开炮，我们这代人好好努力，我相信我们下一代人，小兵汤嘎们就会比上一代的小兵张嘎的生活过的更好。谢谢大家！
点击阅读原文，观看更多精彩内容