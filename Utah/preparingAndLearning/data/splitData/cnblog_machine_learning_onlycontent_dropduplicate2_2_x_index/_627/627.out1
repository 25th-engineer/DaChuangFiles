机器学习 中 常常 要 用到 分类 算法 在 诸多 的 
分类 算法 中 有一种 算法 名为 k 近邻 算法 也 
称为 kNN 算法 一 kNN 算法 的 工作 原理 二 
适用 情况 三 算法 实例 及 讲解 1 . 收集 
数据 2 . 准备 数据 3 . 设 计算法 分析 
数据 4 . 测试 算法 一 kNN 算法 的 工作 
原理 官方 解释 存在 一个 样本 数据集 也 称作 训练样本 
集 并且 样本 中 每个 数据 都 存在 标签 即 
我们 知道 样本 集中 每一 数据 与 所属 分类 的 
对应 关系 输入 没有 标签 的 新 数据 后 将 
新 数据 的 每个 特征 与 样本 集中 的 数据 
对应 的 特征 进行 比较 然后 算法 提取 样本 集中 
特征 最 相似 的 数据 最 近邻 的 分类 标签 
一般来说 我们 只 选择 样 本集 中前 k 个 最 
相似 的 数据 这 就是 k 近邻 算法 中 k 
的 出处 通常 k 是 不大 于 20 的 整数 
最后 选择 k 个 最 相似 的 数据 中 出现 
次数 最多 的 分类 作为 新 数据 的 分类 我 
的 理解 k 近邻 算法 就是 根据 新 数据 的 
分类 取决于 它 的 邻居 进行 的 比如 邻居 中 
大多数 都是 退伍军人 那么 这个 人也 极 有可能 是 退伍军人 
而 算法 的 目的 就是 先 找出 它 的 邻居 
然后 分析 这 几位 邻居 大多数 的 分类 极 有可能 
就是 它 本省 的 分类 二 适用 情况 优点 精度高 
对 异常 数据 不 敏感 你 的 类别 是由 邻 
居中 的 大多数 决定 的 一个 异常 邻居 并 不能 
影响 太大 无 数据 输入 假定 缺点 计算 发 杂 
度 高 需要 计算 新的 数据 点 与 样本 集中 
每个 数据 的 距离 以 判断 是否 是 前 k 
个 邻居 空间 复杂度 高 巨大 的 矩阵 适用 数据 
范围 数值 型 目标 变量 可以 从 无限 的 数值 
集合 中 取值 和 标称 型 目标 变 量 只有 
在 有限 目标 集中 取值 三 算法 实例 及 讲解 
例子 中 的 案例 摘 机器学习 实战 一 书 中的 
代码 例子 是 用 python 编写 的 需要 matplotlib 和 
numpy 库 不过 重在 算法 只要 算法 明白 了 用 
其他 语言 都是/nr 可以 写 出来 的 海伦 一直 使用 
在线 约会 网站 寻找 合适 自己 的 约会对象 尽管 约会 
网站 会 推荐 不同 的 人选 但她 没有 从 中找到 
喜欢 的 人 经过 一番 总结 她 发现 曾 交往 
过 三种 类型 的 人 1 . 不 喜欢 的 
人 以下 简称 1 2 . 魅力 一般 的 人 
以下 简称 2 3 . 极具 魅力 的 人 以下 
简称 3 尽管 发现 了 上述 规律 但 海伦 依然 
无法 将 约会 网站 推荐 的 匹配 对象 归入 恰当 
的 分类 她 觉得 可以 在 周一 到 周五 约会 
哪些 魅力 一般 的 人 而 周末 则 更 喜欢 
与 那些 极具 魅力 的 人为 伴 海伦 希望 我们 
的 分类 软件 可以 更好 的 帮助 她 将 匹配 
对象 划分 到 确切 的 分类 中 此外 海伦 还 
收集 了 一些 约会 网站 未曾 记录 的 数据 信息 
她 认为 这些 数据 更 有助于 匹配 对象 的 归类 
我们 先 提取 一下 这个 案例 的 目标 根据 一些 
数据 信息 对 指定 人选 进行 分类 1 或 2 
或 3 为了 使用 kNN 算法 达到 这个 目标 我们 
需要 哪些 信息 前面 提到 过 就是 需要 样本数据 仔细阅读 
我们 发现 这些 样本数据 就是 海伦 还 收集 了 一些 
约会 网站 未曾 记录 的 数据 信息 好 的 下面 
我们 就 开始 吧 1 . 收集 数据 海伦 收集 
的 数据 是 记录 一个人 的 三个 特征 每年 获得 
的 飞行 常客 里程数 玩 视频 游戏 所 消耗 的 
时间 百分比 每周 消费 的 冰淇淋 公升 数 数据 是 
txt 格式文件 如 下图 前 三列 依次 是 三个 特征 
第 四列 是 分类 1 不 喜欢 的 人 2 
魅力 一般 的 人 3 极具 魅力 的 人 每 
一行 代表 一个 人 数据 文档 的 下载 链接 是 
http / / pan . baidu . com / s 
/ 1jG7n4hS 2 . 准备 数据 何为 准备 数据 之前 
收集到 了 数据 放到 了 txt 格式 的 文档 中了 
看 起来 也 比较 规整 但是 计算机 并不 认识 啊 
计算机 需要 从 txt 文档 中 读取数据 并把 数据 进行 
格式化 也 就是说 存到 矩阵 中 用 矩阵 来 承 
装 这些 数据 这样 才能 使用 计算机 处理 需要 两个 
矩阵 一个 承 装 三个 特征 数据 一个 承 装 
对应 的 分类 于是 我们 定义 一个 函数 函数 的 
输入 时数 据 文档 txt 格式 输出 为 两个 矩阵 
代码 如下 1 def file2matrix filename 2 fr = open 
filename 3 numberOfLines = len fr . readlines 4 returnMat 
= zeros numberOfLines 3 5 classLabelVector = 6 fr = 
open filename 7 index = 0 8 for line in 
fr . readlines 9 line = line . strip 10 
listFromLine = line . split \ t 11 returnMat index 
= listFromLine 0 3 12 classLabelVector . append int listFromLine 
1 13 index + = 1 14 return returnMat classLabelVector 
简要 解读 代码 首先 打开 文件 读取 文件 的 行数 
然后 初始化 之后 要 返回 的 两个 矩阵 returnMat c 
l a s s L a b e l s 
V e c t o r 然后 进入 循环 将 
每行 的 数据 各就各位 分配给 returnMat 和cl/nr a s s 
L a b e l s V e c t 
o r 3 . 设 计算法 分析 数据 k 近邻 
算法 的 目的 就是 找到 新 数据 的 前 k 
个 邻居 然后 根据 邻居 的 分类 来 确定 该 
数据 的 分类 首先 要 解决 的 问题 就是 什么 
是 邻居 当然 就是 距离 近 的 了 不同 人 
的 距离 怎么 确定 这个 有点 抽象 不过 我们 有 
每个 人 的 3个 特征 数据 每个 人 可以 使用 
这三个 特征 数据 来 代替 这个人 三维 点 比如 样本 
的 第一 个人 就 可以 用 40920 8.326976 0.953952 来 
代替 并且 他 的 分类 是 3 那么 此时 的 
距离 就是 点 的 距离 A 点 x1 x2 x3 
B 点 y1 y2 y3 这 两个 点 的 距离 
就是 x1 y1 ^ 2 + x2 y2 ^ 2 
+ x3 y3 ^ 2 的 平方根 求出 新 数据 
与 样本 中 每个 点 的 距离 然后 进行 从小到大 
排序 前 k 位 的 就是 k 近邻 然后 看看 
这 k 位 近邻 中 占得 最多 的 分类 是 
什么 也就 获得 了 最终 的 答案 这个 处理 过程 
也 是 放到 一个 函 数里 的 代码 如下 1 
def classify0 inX dataSet labels k 2 dataSetSize = dataSet 
. shape 0 3 diffMat = tile inX dataSetSize 1 
dataSet 4 sqDiffMat = diffMat * * 2 5 sqDistances 
= sqDiffMat . sum axis = 1 6 distances = 
sqDistances * * 0.5 7 s o r t e 
d D i s t I n d i c 
i e s = distances . argsort 8 classCount = 
{ } 9 for i in range k 10 voteIlabel 
= labels s o r t e d D i 
s t I n d i c i e s 
i 11 classCount voteIlabel = classCount . get voteIlabel 0 
+ 1 12 sortedClassCount = sorted classCount . iteritems key 
= operator . itemgetter 1 reverse = True 13 return 
sortedClassCount 0 0 简要 解读 代码 该 函数 的 4个 
参数 分别为 新 数据 的 三个 特征 inX 样本数据 特 
征集 上 一个 函数 的 返回值 样本数据 分类 上 一个 
函数 的 返回值 k 函数 返 回位 新 数据 的 
分类 第二行 dataSetSize 获取 特 征集 矩阵 的 行数 第三 
行为 新 数据 与 样本 各个 数据 的 差值 第四行 
取 差值 去 平方 之后 就是 再取 和 然后 平方根 
代码 中 使用 的 排序 函数 都是 python 自带 的 
好了 现在 我们 可以 分析 数据 了 不过 有一点 不 
知道 大家 有 没有 注意 我们 回到 那个 数据集 第一列 
代表 的 特征 数值 远远 大于 其他 两 项 特征 
这样在 求 距离 的 公式 中就 会占 很大 的 比重 
致使 两点 的 距离 很大 程度 上 取决于 这个 特征 
这 当然 是 不 公平 的 我们 需要 的 三个 
特征 都 均平 地 决定 距离 所以 我们 要 对 
数据 进行 处理 希望 处理 之后 既不 影响 相对 大小 
又 可以 不 失 公平 这种方法 叫做 归一化 数值 通过 
这种 方法 可以 把 每 一列 的 取值 范围 划到 
0 ~ 1 或 1 ~ 1 处理 的 公式 
为 newValue = oldValue min / max min 归一化 数值 
的 函数 代码 为 1 def autoNorm dataSet 2 minVals 
= dataSet . min 0 3 maxVals = dataSet . 
max 0 4 ranges = maxVals minVals 5 normDataSet = 
zeros shape dataSet 6 m = dataSet . shape 0 
7 normDataSet = dataSet tile minVals m 1 8 normDataSet 
= normDataSet / tile ranges m 1 9 return normDataSet 
ranges minVals 4 . 测试 算法 经过 了 格式化 数据 
归一化 数值 同时 我们 也 已经 完成 kNN 核心 算法 
的 函数 现在 可以 测试 了 测试代码 为 1 def 
datingClassTest 2 hoRatio = 0.10 3 datingDataMat datingLabels = file2matrix 
datingTestSet . txt 4 normMat ranges minVals = autoNorm datingDataMat 
5 m = normMat . shape 0 6 numTestVecs = 
int m * hoRatio 7 errorCount = 0.0 8 for 
i in range numTestVecs 9 classifierResult = classify0 normMat i 
normMat numTestVecs m datingLabels numTestVecs m 3 10 print the 
classifier came back with % d the real answer is 
% d % classifierResult datingLabels i 11 if classifierResult = 
datingLabels i errorCount + = 1.0 12 print the total 
error rate is % f % errorCount / float numTestVecs 
通过 测试代码 我们 可以 在 回忆 一下 这个 例子 的 
整体 过程 读取 txt 文件 提取 里面 的 数据 到 
datingDataMat datingLabels 归一化 数据 得到 归一化 的 数据 矩阵 测试数据 
不止 一个 这里 需要 一个 循环 依次 对 每个 测试数据 
进行 分类 代码 中 大家 可能 不 太 明白 hoRatio 
是 什么 注意 这里 的 测试数据 并 不是 另外 一批 
数据 而 是 之前 的 数据 集 里 的 一部分 
这样 我们 可以 把 算法 得到 的 结果 和 原本 
的 分类 进行 对比 查看 算法 的 准确度 在 这里 
海伦 提供 的 数据 集 又 1000行 我们 把 前 
100行 作为 测试数据 后 900行 作为 样本 数据集 现在 大家 
应该 可以 明白 hoRatio 是 什么 了吧 整体 的 代码 
1 from numpy import * 2 import operator 3 4 
def classify0 inX dataSet labels k 5 dataSetSize = dataSet 
. shape 0 6 diffMat = tile inX dataSetSize 1 
dataSet 7 sqDiffMat = diffMat * * 2 8 sqDistances 
= sqDiffMat . sum axis = 1 9 distances = 
sqDistances * * 0.5 10 s o r t e 
d D i s t I n d i c 
i e s = distances . argsort 11 classCount = 
{ } 12 for i in range k 13 voteIlabel 
= labels s o r t e d D i 
s t I n d i c i e s 
i 14 classCount voteIlabel = classCount . get voteIlabel 0 
+ 1 15 sortedClassCount = sorted classCount . iteritems key 
= operator . itemgetter 1 reverse = True 16 return 
sortedClassCount 0 0 17 18 def file2matrix filename 19 fr 
= open filename 20 numberOfLines = len fr . readlines 
21 returnMat = zeros numberOfLines 3 22 classLabelVector = 23 
fr = open filename 24 index = 0 25 for 
line in fr . readlines 26 line = line . 
strip 27 listFromLine = line . split \ t 28 
returnMat index = listFromLine 0 3 29 classLabelVector . append 
int listFromLine 1 30 index + = 1 31 return 
returnMat classLabelVector 32 33 def autoNorm dataSet 34 minVals = 
dataSet . min 0 35 maxVals = dataSet . max 
0 36 ranges = maxVals minVals 37 normDataSet = zeros 
shape dataSet 38 m = dataSet . shape 0 39 
normDataSet = dataSet tile minVals m 1 40 normDataSet = 
normDataSet / tile ranges m 1 41 return normDataSet ranges 
minVals 42 43 def datingClassTest 44 hoRatio = 0.10 45 
datingDataMat datingLabels = file2matrix datingTestSet . txt 46 normMat ranges 
minVals = autoNorm datingDataMat 47 m = normMat . shape 
0 48 numTestVecs = int m * hoRatio 49 errorCount 
= 0.0 50 for i in range numTestVecs 51 classifierResult 
= classify0 normMat i normMat numTestVecs m datingLabels numTestVecs m 
3 52 print the classifier came back with % d 
the real answer is % d % classifierResult datingLabels i 
53 if classifierResult = datingLabels i errorCount + = 1.0 
54 print the total error rate is % f % 
errorCount / float numTestVecs 运行 一下 代码 这里 我 使用 
的 是 ipython 最后 的 错误率 为 0.05 