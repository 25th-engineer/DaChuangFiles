这 一章 可能 是 Andrew Ng 讲得 最 不 清楚 
的 一章 为什么 这么 说 呢 这 一章 主要 讲 
后向 传播 Backpropagration BP 算法 Ng 花了 一大半 的 时间 
在 讲 如何 计算误差 项$\/nr delta $ 如何 计算 $ 
\ Delta $ 的 矩阵 以及 如何 用 Matlab 去 
实现 后向 传播 然而 最 关键 的 问题 为什么 要 
这么 计算 前面 计算 的 这些 量 到底 代表 着 
什么 Ng 基本 没有 讲解 也 没有 给 出 数学 
的 推导 的 例子 所以 这次 内容 我 不 打算 
照着 公开课 的 内容 去 写 在 查阅 了 许多 
资料 后 我 想 先从 一个 简单 的 神经 网络 
的 梯度 推导 入手 理解 后向 传播 算法 的 基本 
工作 原理 以及 每个 符号 代表 的 实际 意义 然后 
再 按照 课程 的 给出 BP 计算 的 具体 步骤 
这样 更 有助于 理解 简单 神经 网络 的 后向 传播 
Backpropagration BP 算法 1 . 回顾 之前 的 前 向 
传播 F o r w a r d P r 
o p a g r a t i o n 
FP 算法 FP 算法 还是 很 简单 的 说白 了 
就是 根据 前 一层 神经元 的 值 先 加权 然后 
取 sigmoid 函数 得到 后 一层 神经元 的 值 写成 
数学 的 形式 就是 $ $ a ^ { 1 
} = X $ $ $ $ z ^ { 
2 } = \ Theta ^ { 1 } a 
^ { 1 } $ $ $ $ a ^ 
{ 2 } = g z ^ { 2 } 
$ $ $ $ z ^ { 3 } = 
\ Theta ^ { 2 } a ^ { 2 
} $ $ $ $ a ^ { 3 } 
= g z ^ { 3 } $ $ $ 
$ z ^ { 4 } = \ Theta ^ 
{ 3 } a ^ { 3 } $ $ 
$ $ a ^ { 4 } = g z 
^ { 4 } $ $ 2 . 回顾 神经 
网络 的 代价 函数 不含 regularization 项 $ J \ 
Theta = \ frac { 1 } { m } 
\ left \ sum \ limits _ { i = 
1 } ^ { m } \ sum \ limits 
_ { k = 1 } ^ { K } 
y ^ { i } _ { k } log 
h _ \ theta x ^ { i } _ 
k + 1 y ^ { i } _ k 
log 1 h _ \ theta x ^ { i 
} _ k \ right $ 3 . 一个 简单 
神经 网络 的 BP 推导 过程 BP 算法 解决 了 
什么 问题 我们 已经 有了/nr 代价 函数 $ J \ 
Theta $ 接下来 我们 需要 利用 梯度 下降 算法 或者 
其他 高级 优化 算法 对 $ J \ Theta $ 
进行 优化 从而 得到 训练 参数 $ \ Theta $ 
然而 关键 问题 是 优化 算法 需要 传递 两个 重要 
的 参数 一个 代价 函数 $ J \ Theta $ 
另 一个 是 代价 函数 的 梯度 $ \ frac 
{ \ partial J \ Theta } { \ partial 
\ Theta } $ BP 算法 其实 就是 解决 如何 
计算 梯度 的 问题 下面/f 我们/r 从/p 一个/m 简单/a 的/uj 
例子/n 入手/v 考虑/v 如何/r 从/p 数学/n 上/f 计算/v 代价/n 函数/n 
的/uj 梯度/n 考虑 如下 简单 的 神经 网络 为 方便 
起见 途中 已经 给 出了 前 向 传播 FP 的 
计算 过程 该 神经 网络 有 三层 神经元 对应 的 
有 两个 权重 矩阵 $ \ Theta ^ { 1 
} $ 和$\/nr Theta ^ { 2 } $ 为 
计算 梯度 我们 只 需要 计算 两个 偏 导数 即可 
$ \ frac { \ partial J \ Theta } 
{ \ partial \ Theta ^ { 1 } } 
$ 和$\/nr frac { \ partial J \ Theta } 
{ \ partial \ Theta ^ { 2 } } 
$ 首先 我们 先 计算 第 2个 权重 矩阵 的 
偏 导数 即 $ \ frac { \ partial } 
{ \ partial \ Theta ^ { 2 } } 
J \ Theta $ 首先 我们 需要 在 $ J 
\ Theta $ 和$\/nr Theta ^ { 2 } $ 
之间 建立 联系 很容易 可以 看到 $ J \ Theta 
$ 的 值 取决于 $ h _ \ theta x 
$ 而 $ h _ \ theta x = a 
^ { 3 } $ $ a ^ { 3 
} $ 又 是由 $ z ^ { 3 } 
$ 取 sigmoid 得到 最后 $ z ^ { 3 
} = \ Theta ^ { 2 } \ times 
a ^ { 2 } $ 所以 他们 之间 的 
联系 可以 如下 表示 按照 求导 的 链式法则 我们 可以 
先 求 $ J \ Theta $ 对 $ z 
^ { 3 } $ 的 导数 然后 乘以 $ 
z ^ { 3 } $ 对 $ \ Theta 
^ { 2 } $ 的 导数 即 $ $ 
\ frac { \ partial } { \ partial \ 
Theta ^ { 2 } } J \ Theta = 
\ frac { \ partial } { \ partial z 
^ { 3 } } J \ Theta   \ 
times \ frac { \ partial z ^ { 3 
} } { \ partial \ Theta ^ { 2 
} } $ $ 由 $ z ^ { 3 
} = \ Theta ^ { 2 } a ^ 
{ 2 } $ 不难 计算 $ \ frac { 
\ partial z ^ { 3 } } { \ 
partial \ Theta ^ { 2 } } = a 
^ { 2 } ^ T $ 令 $ \ 
frac { \ partial } { \ partial z ^ 
{ 3 } } J \ Theta = \ delta 
^ { 3 } $ 上式 可以 重写 为 $ 
$ \ frac { \ partial } { \ partial 
\ Theta ^ { 2 } } J \ Theta 
= \ delta ^ { 3 } a ^ { 
2 } ^ T $ $ 接下来 仅 需要 计算 
$ \ delta ^ { 3 } $ 即可 由上 
一章 的 内容 我们 已经 知道 $ g z = 
g z 1 g z $ $ h _ \ 
theta x = a ^ { 3 } = g 
z ^ { 3 } $ 忽略 前面 的 $ 
1 / m \ sum \ limits _ { i 
= 1 } ^ { m } $ 这里 我们 
只 对 一个 example 推导 最后 累加 即可 $ $ 
\ begin { aligned } \ delta ^ { 3 
} & = \ frac { \ partial J \ 
Theta } { z ^ { 3 } } \ 
\ & = y \ frac { 1 } { 
g z ^ { 3 } } g ^ { 
} z ^ { 3 } 1 y \ frac 
{ 1 } { 1 g z ^ { 3 
} } 1 g z ^ { 3 } \ 
\ & = y 1 g z ^ { 3 
} + 1 y g z ^ { 3 } 
\ \ & = y + g z ^ { 
3 } \ \ & = y + a ^ 
{ 3 } \ end { aligned } $ $ 
至此 我们 已经 得到 $ J \ Theta $ 对 
$ \ Theta ^ { 2 } $ 的 偏 
导数 即 $ $ \ frac { \ partial J 
\ Theta } { \ partial \ Theta ^ { 
2 } } = a ^ { 2 } ^ 
T \ delta ^ { 3 } $ $ $ 
$ \ delta ^ { 3 } = a ^ 
{ 3 } y $ $ 接下来 我们 需要 求 
$ J \ Theta $ 对 $ \ Theta ^ 
{ 1 } $ 的 偏 导数 $ J \ 
Theta $ 对 $ \ Theta ^ { 1 } 
$ 的 依赖 关系 如下 根据 链式 求导 法 则有 
$ $ \ begin { aligned } \ frac { 
\ partial J \ Theta } { \ partial \ 
Theta ^ { 1 } } & = \ frac 
{ \ partial J \ Theta } { \ partial 
z ^ { 3 } } \ frac { \ 
partial z ^ { 3 } } { \ partial 
a ^ { 2 } } \ frac { \ 
partial a ^ { 2 } } { \ partial 
\ Theta ^ { 1 } }   \ end 
{ aligned } $ $ 我们 分别 计算 等式 右边 
的 三项 可得 $ $   \ frac { \ 
partial J \ Theta } { \ partial z ^ 
{ 3 } } = \ delta ^ { 3 
} $ $ $ $ \ frac { \ partial 
z ^ { 3 } } { \ partial a 
^ { 2 } } = \ Theta ^ { 
2 } ^ T $ $ $ $ \ frac 
{ \ partial a ^ { 2 } } { 
\ partial \ Theta ^ { 1 } } = 
\ frac { \ partial a ^ { 2 } 
} { \ partial z ^ { 2 } } 
\ frac { \ partial z ^ { 2 } 
} { \ partial \ Theta ^ { 1 } 
} = g z ^ { 2 } a ^ 
{ 1 } $ $ 带入 后得$/nr $ \ frac 
{ \ partial J \ Theta } { \ partial 
\ Theta ^ { 1 } } = a ^ 
{ 1 } ^ T \ delta ^ { 3 
} \ Theta ^ { 2 } ^ T g 
z ^ { 2 } $ $ 令 $ \ 
delta ^ { 2 } = \ delta ^ { 
3 } \ Theta ^ { 2 } ^ Tg 
z ^ { 2 } $ 上式 可以 重写 为 
$ $ \ frac { \ partial J \ Theta 
} { \ partial \ Theta ^ { 1 } 
} = a ^ { 1 } ^ T \ 
delta ^ { 2 } $ $ $ $ \ 
delta ^ { 2 } = \ delta ^ { 
3 } \ Theta ^ { 2 } ^ T 
g z ^ { 2 } $ $ 把 上面 
的 结果 放在 一起 我们 得到 $ J \ Theta 
$ 对 两个 权重 矩阵 的 偏 导数 为 $ 
$ \ delta ^ { 3 } = a ^ 
{ 3 } y $ $ $ $ \ frac 
{ \ partial J \ Theta } { \ partial 
\ Theta ^ { 2 } } = a ^ 
{ 2 } ^ T \ delta ^ { 3 
} $ $ $ $ \ delta ^ { 2 
} = \ delta ^ { 3 } \ Theta 
^ { 2 } ^ T g z ^ { 
2 } $ $ $ $ \ frac { \ 
partial J \ Theta } { \ partial \ Theta 
^ { 1 } } = a ^ { 1 
} ^ T \ delta ^ { 2 } $ 
$ 观察 上面 的 四个 等式 我们 发现 偏 导数 
可以 由 当前 层 神经元 向量 $ a ^ { 
l } $ 与 下 一层 的 误差 向量 $ 
\ delta ^ { l + 1 } $ 相乘 
得到 当 前层 的 误差 向量 $ \ delta ^ 
{ l } $ 可以 由 下 一层 的 误差 
向量 $ \ delta ^ { l + 1 }/i 
$/i 与/p 权重/n 矩阵/n $/i \/i Delta/w ^/i {/i l/w 
}/i $/i 的/uj 乘积/n 得到/v 所以/c 可以/c 从后/nr 往前/t 逐层/d 
计算误差/n 向量/n 这 就是 后向 传播 的 来源 然后 通过 
简单 的 乘法 运算 得到 代价 函数 对 每 一层 
权重 矩阵 的 偏 导数 到 这里 算是 终于 明白 
为什么 要 计算误差 向量 以及 为什么 误差 向量 之间 有 
递归 关系 了 尽管 这里 的 神经 网络 十分 简单 
推导 过程 也 不是 十分 严谨 但是 通过 这个 简单 
的 例子 基本 能够 理解 后向 传播 算法 的 工作 
原理 了 严谨 的 后向 传播 算法 计算 梯度 假设 
我们 有$m/nr $ 个 训练 example $ L $ 层 
神经网络 并且 此处 考虑 正则 项 即 $ J \ 
Theta = \ frac { 1 } { m } 
\ left \ sum \ limits _ { i = 
1 } ^ { m } \ sum \ limits 
_ { k = 1 } ^ { K } 
y ^ { i } _ { k } log 
h _ \ theta x ^ { i } _ 
k + 1 y ^ { i } _ k 
log 1 h _ \ theta x ^ { i 
} _ k \ right + \ frac { \ 
lambda } { 2m } \ sum \ limits _ 
{ l = 1 } ^ { L 1 } 
\ sum \ limits _ { i = 1 } 
^ { s _ l } \ sum \ limits 
_ { j = 1 } ^ { s _ 
{ l + 1 } } \ Theta _ { 
ji } ^ { l } ^ 2 $ 初始化 
设置 $ \ Delta ^ { l } _ { 
ij } = 0 $ 理解 为对 第 $ l 
$ 层 的 权重 矩阵 的 偏 导 累 加值 
For i = 1 m 设置 $ a ^ { 
1 } = X $ 通过 前 向 传播 算法 
FP 计算 对 各层 的 预测 值 $ a ^ 
{ l } $ 其中 $ l = 2 3 
4 \ ldots L $ 计算 最后 一层 的 误差 
向量 $ \ delta ^ { L } = a 
^ { L } y $ 利用 后向 传播 算法 
BP 从后至/nr 前/f 逐层/d 计算误差/n 向量/n $ \ delta ^ 
{ L 1 } \ delta ^ { L 1 
} \ ldots   \ delta ^ { 2 } 
$ 计算公式 为 $ \ delta ^ { l } 
= \ Theta ^ { l } ^ T \ 
delta ^ { l + 1 } . * g 
z ^ { l } $ 更新 $ \ Delta 
^ { l } = \ Delta ^ { l 
} + \ delta ^ { l + 1 } 
a ^ { l } ^ T $ end / 
/ for 计算 梯度 $ $ D _ { ij 
} ^ { l } = \ frac { 1 
} { m } \ Delta ^ { l } 
_ { ij }   j = 0 $ $ 
$ $ D _ { ij } ^ { l 
} = \ frac { 1 } { m } 
\ Delta ^ { l } _ { ij } 
+ \ frac { \ lambda } { m } 
\ Theta _ { ij } ^ { l } 
  j \ neq 0 $ $ $ $ \ 
frac { \ partial J \ Theta } { \ 
partial \ Theta ^ { l } } = D 
^ { l } $ $ BP 实际 运用 中的 
技巧 1 . 将 参数 展 开成 向量 对于 四层 
三个 权重 矩阵 参数 $ \ Theta ^ { 1 
} \ Theta ^ { 2 }   \ Theta 
^ { 3 } $ 将其 展开 成 一个 参数 
向量 Matlab code 如下 thetaVec = Theta1 Theta2 Theta3 2 
. 梯度 检查 为了 保证 梯度 计算 的 正确性 可以 
用 数值解 进行 检查 根据 导数 的 定义 $ $ 
\ frac { dJ \ theta } { d \ 
theta } \ approx \ frac { J \ theta 
+ \ epsilon J \ theta \ epsilon } { 
2 \ epsilon } $ $ Matlab Code 如下 for 
i = 1 n thetaPlus = theta thetaPlus i = 
thetaPlus i + EPS thetaMinus = theta thetaMinus i = 
thetaMinus i EPS gradApprox i = J thetaPlus J thetaMinus 
/ 2 * EPS end 最后 检查 gradApprox 是否 约等于 
之前 计算 的 梯度 值 即可 需要 注意 的 是 
因为 近似 的 梯度 计算 代价 很大 在 梯度 检查 
后 记得 关闭 梯度 检查 的 代码 3 . 随机 
初始化 初始 权重 矩阵 的 初始化 应该 打破 对称性 symmetry 
breaking 避免 使用 全零/nr 矩阵 进行 初始化 可以 采用 随机数 
进行 初始化 即 $ \ Theta ^ { l } 
_ { ij } \ in \ epsilon + \ 
epsilon $ 如何 训练 一个 神经 网络 随机 初始化 权重 
矩阵 利用 前 向 传播 算法 FP 计算 模型 预测值 
$ h _ \ theta x $ 计算 代价 函数 
$ J \ Theta $ 利用 后向 传播 算法 BP 
计算 代价 函数 的 梯度 $ \ frac { \ 
partial J \ Theta } { \ partial \ Theta 
^ { l } } $ 利用 数值 算法 进行 
梯度 检查 gradient checking 确保 正确 后 关闭 梯度 检查 
利用 梯度 下降 或者 其他 优化 算法 求得 最优 参数 
$ \ Theta $ 附 一个 简短 的 后向 传播 
教学 视频 参考文献 1   Andrew Ng Coursera 公开课 第 
五周 2   Derivation of Backpropagation .   http / 
/ web . cs . swarthmore . edu / ~ 
meeden / cs81 / s10 / BackPropDeriv . pdf 3 
Wikipedia Backpropagation .   https / / en . wikipedia 
. org / wiki / Backpropagation 4   How the 
backpropagation algorithm works .   http / / n e 
u r a l n e t w o r 
k s a n d d e e p l 
e a r n i n g . com / 
chap2 . html 5 神经 网络 和 反向 传播 算法 
推导 .   http / / www . mamicode . 
com / info detail 671452 . html 