注 该文 是 上了 开 智 学堂 数据 科学 入门 
班的/nr 课后 做 的 笔记 主讲人 是 肖凯 老师 机器学习 
初步 机器学习 基本概念 机器学习 统计模型 和 数据挖掘 有什么 异同 机器 
学习 和 统计模型 区别 不是 很大 机器/n 学习/v 和/c 统计/v 
模型/n 中/f 的/uj 回归/v 都/d 一样/r 底层 算法 都是/nr 差不多 
的 只是 侧重点 不一样 在 统计学 的 角度 回归 主要 
解决 的 问题 侧重点 在于 模型 的 解释 能力 关注 
的 是 x 和 y 之间 的 关系 关注 的 
更多 是 系数 从 机器 学习 的 角度 看 关注 
的 重点 是 预测 的 准确性 机器 学习 和 数据挖掘 
也 没什么 不 一样 两者 的 算法 基本上 是 一样 
的 只是 在 一些 流程 步骤 上 数据挖掘 会 有一些 
特征 工程 的 工作 以及 对 具体 应用 问题 的 
解释 有/v 监督/vn 学习/v 和无/nr 监督/vn 学习/v 有/v 什么/r 区别/n 
有 监督 学习 就是指 有 y 作为 数据 的 一部分 
被 称为 目标 变量 或 被 解释 变量 无 监督 
学习 是 指 一堆 数据 没有 特定 的 y 要从 
一堆 x 里 找到 模式 或者 规律 出来 有 监督 
学习 可以 分为 两个 子类 分类 和 回归 分类 问题 
中 要 预测 的 y 偏 离散 比如 性别 血型 
回归 问题 y 都是 连续 的 实 数域 中的 比如 
收入 天气 分类/n 问题/n 和聚类/nr 问题/n 有/v 什么/r 区别/n 分类 
问题 是 预测 一个 未知 类别 的 对象 属于 哪个 
类别 而 聚 类 是 根据 选定 的 指标 对 
一群 对象 进行 划分 它 不属于 预测 问题 交叉 验证 
是 什么 交叉 验证 是 指用 来 建立 模型 的 
数据 和 最后 用来 模型 验证 的 数据 是 不 
一样 的 实践 中 拿到 数据 后 应该 分 为 
几个 部分 最 简单 的 分为 两部分 一 部分 用于 
训练 模型 另外 一 部分 用于 检验 模型 这 就是 
交叉 验证 何时 用到 特征 工程 特征 工程 是 指 
要把 原始数据 做些 整理 做些 转换 主要 目的 是 暴露 
出 预测 y 的 信息 如何 加载 sklearn 的 内置 
数据集 from sklearn import datasets from sklearn import cross _ 
validation from sklearn import linear _ model from sklearn import 
metrics from sklearn import tree from sklearn import neighbors from 
sklearn import svm from sklearn import ensemble from sklearn import 
cluster % matplotlib inline import matplotlib . pyplot as pltimport 
numpy as np import seaborn as snsskearn 有 很多 内置 
的 数据 集 上面 已经 加载 了 sklearn 的 datasets 
datasets 有 一些 可以 用 的 数据 比如说 加载 boston 
数据集 加载 后 返回 的 就是 个 数据字典 boston = 
datasets . load _ boston print boston . keys data 
feature _ names DESCR target 数据集 的 大小 / 格式 
/ 类型 等 信息 如何 得知 print boston . DESCRBoston 
House Prices dataset Notes Data Set Characteristics Number of Instances 
506 Number of Attributes 13 numeric / categorical predictive Median 
Value attribute 14 is usually the target Attribute Information in 
order CRIM per capita crime rate by town ZN proportion 
of residential land zoned for lots over 25 000 sq 
. ft . INDUS proportion of non retail business acres 
per town CHAS Charles River dummy variable = 1 if 
tract bounds river 0 otherwise NOX nitric oxides concentration parts 
per 10 million RM average number of rooms per dwelling 
AGE proportion of owner occupied units built prior to 1940 
DIS weighted distances to five Boston employment centres RAD index 
of accessibility to radial highways TAX full value property tax 
rate per $ 10 000 PTRATIO pupil teacher ratio by 
town B 1000 Bk 0.63 ^ 2 where Bk is 
the proportion of blacks by town LSTAT % lower status 
of the population MEDV Median value of owner occupied homes 
in $ 1000 s Missing Attribute Values None Creator Harrison 
D . and Rubinfeld D . L . This is 
a copy of UCI ML housing dataset . http / 
/ archive . ics . uci . edu / ml 
/ datasets / Housing This dataset was taken from the 
StatLib library which is maintained at Carnegie Mellon University . 
The Boston house price data of Harrison D . and 
Rubinfeld D . L . Hedonic prices and the demand 
for clean air J . Environ . Economics & Management 
vol . 5 81 102 1978 . Used in Belsley 
Kuh & Welsch Regression diagnostics . . . Wiley 1980 
. N . B . Various transformations are used in 
the table on pages 244 261 of the latter . 
The Boston house price data has been used in many 
machine learning papers that address regression problems . * * 
References * * Belsley Kuh & Welsch Regression diagnostics Identifying 
Influential Data and Sources of Collinearity Wiley 1980 . 244 
261 . Quinlan R . 1993 . Combining Instance Based 
and Model Based Learning . In Proceedings on the Tenth 
International Conference of Machine Learning 236 243 University of Massachusetts 
Amherst . Morgan Kaufmann . many more see http / 
/ archive . ics . uci . edu / ml 
/ datasets / Housing boston . data # 自变量 矩阵 
面积 家具 等 特征 array 6.32000000 e 03 1.80000000 e 
+ 01 2.31000000 e + 00 . . . 1.53000000 
e + 01 3.96900000 e + 02 4.98000000 e + 
00 2.73100000 e 02 0.00000000 e + 00 7.07000000 e 
+ 00 . . . 1.78000000 e + 01 3.96900000 
e + 02 9.14000000 e + 00 2.72900000 e 02 
0.00000000 e + 00 7.07000000 e + 00 . . 
. 1.78000000 e + 01 3.92830000 e + 02 4.03000000 
e + 00 . . . 6.07600000 e 02 0.00000000 
e + 00 1.19300000 e + 01 . . . 
2.10000000 e + 01 3.96900000 e + 02 5.64000000 e 
+ 00 1.09590000 e 01 0.00000000 e + 00 1.19300000 
e + 01 . . . 2.10000000 e + 01 
3.93450000 e + 02 6.48000000 e + 00 4.74100000 e 
02 0.00000000 e + 00 1.19300000 e + 01 . 
. . 2.10000000 e + 01 3.96900000 e + 02 
7.88000000 e + 00 boston . data . shape 506 
13 boston . target . shape # 目标 变量 房价 
506 datasets 除了 现有 的 数据 还 可以 造 一些 
数据 通过 人造 数据 来 研究 不同 算法 的 特点 
datasets . make _ regression function sklearn . datasets . 
samples _ generator . make _ regression 回归 问题 回归 
问题 中 y 是个 连续 的 数值 不仅 可以 采取 
线性 回归 还 可以 使用 决策树 等 做 回归 只要 
输出 是 连续 值 都 可以 用 回归模型 sklearn 的 
回归 和 statsmodel 中的 回归 有什么 异同 如果 使用 同样 
的 模型 两者 的 回归 的 解 都是 一样 的 
只是 statsmodel 输出 更多 些 比较 偏向 对 参数 做 
更多 的 解释 而 sklearn 更 注重 预测 准确性 如何 
使用 交叉 验证 它/r 和/c 过拟合/i 有/v 什么/r 关系/n 来个 
完整 的 例子 np . random . seed 123 X 
_ all y _ all = datasets . make _ 
regression n _ samples = 50 n _ features = 
50 n _ informative = 10 # 真正 有用 的 
变量 只有 10 个 另外 40 个 都是 噪音 print 
X _ all . shape y _ all . shape 
50 50 50 这个 数据集 比较 棘手 样本数 比较 少 
变 量多 容易 过拟合 而且 有用 的 变量 不多 有 
很多 噪音 在里面 在做 回归 时 很容易 把 噪音 放到 
方程 里 用 传统 的 方法 比较 难于 处理 这里 
用 机器学习 处理 为了 防止 过拟合 要用 交叉 验证 把 
数据 分为 两部分 一 部分 用于 训练 一 部分 用于 
验证 X _ train X _ test y _ train 
y _ test = cross _ validation . train _ 
test _ split X _ all y _ all train 
_ size = 0.5 print X _ train . shape 
y _ train . shape print X _ test . 
shape y _ test . shape print type X _ 
train 25 50 25 25 50 25 type numpy . 
ndarray 如何 以 线性 模型 拟合 数据集 model = linear 
_ model . LinearRegression # 实例 化 model . fit 
X _ train y _ train # 做 线性 回归 
拟合 数据 LinearRegression copy _ X = True fit _ 
intercept = True n _ jobs = 1 normalize = 
False 残差 residual 是 什么 如何 评估 模型 是个 好 
模型 残差 是 真实 的 y 和 预测 的 y 
的 差 残差 越小 拟合 越好 def sse resid return 
sum resid * * 2 # 残差 平方和 是 回归 
效果 的 一个 指标 残差 平方和 是 回归 效果 的 
一个 指标 值 越小 说明 模型 越好 resid _ train 
= y _ train model . predict X _ train 
sse _ train = sse resid _ train print sse 
_ train5 . 87164948974e 25resid _ test = y _ 
test model . predict X _ test sse _ test 
= sse resid _ test sse _ test194948 . 84691187815model 
. score X _ train y _ train # 计算 
判定 系数 R squared1 . 0model . score X _ 
test y _ test 0 . 2 6 2 7 
5 0 8 8 5 4 9 0 6 0 
6 4 3 模型 在 测试 集上 的 分数 只有 
0.26 效果 并 不好 画出 各个 样本 对应 的 残差 
和 各个 变量 的 系数 def plot _ residuals _ 
and _ coeff resid _ train resid _ test coeff 
fig axes = plt . subplots 1 3 figsize = 
12 3 axes 0 . bar np . arange len 
resid _ train resid _ train # 各个 样本 对应 
的 残差 axes 0 . set _ xlabel sample number 
axes 0 . set _ ylabel residual axes 0 . 
set _ title training data axes 1 . bar np 
. arange len resid _ test resid _ test # 
各个 样本 对应 的 残差 axes 1 . set _ 
xlabel sample number axes 1 . set _ ylabel residual 
axes 1 . set _ title testing data axes 2 
. bar np . arange len coeff coeff # 各个 
变量 的 系数 axes 2 . set _ xlabel coefficient 
number axes 2 . set _ ylabel coefficient fig . 
tight _ layout return fig axesfig ax = plot _ 
residuals _ and _ coeff resid _ train resid _ 
test model . coef _ # 训练 集 的 残差 
测试 集 的 残差 各个 系数 的 大小 可以 看到 
第 一幅 图 训练 集中 的 残差 不算 大 范围 
在 3 到 5 之间 测试 集 的 残差 就 
过大 范围 在 150 到 250 之间 可见 模型 过拟合 
了 在 训练 集上 还行 测试 集上 就很 糟糕 真实 
变量 里 只有 10 个 是 有用 的 而上/i 面的/i 
变量/vn 系数/n 图/n 有/v 很多/m 都/d 不是/c 0 有 很多 
冗余 变 量多 样本 少 怎么 解决 呢 变 量比 
样本 多时 如何 处理 一种 方法 是 做个 主 成分 
分析 降 维 对 变量 做个 筛选 再 放到 模型 
里来 但 这种 方法 比较 麻烦 还 有种 是 正则化 
的 方法 正则化 是 什么 有哪/nr 两种 方法 正则化 在 
统计学 里 有 两种 思路 一种 叫 岭回归 把 系数 
放到 loss function 中 经典 的 loss function 是 残差 
平方和 这里 把 50 个 系数 平方 求和 放到 loss 
function 中 所以 最终 既 要使 残差 平方和 小 又 
要使 权重 小 可以 压制 一些 过于 冗余 的 权重 
model = linear _ model . Ridge alpha = 5 
# 参数 alpha 表示 对于 权重 系数 的 决定 因子 
model . fit X _ train y _ train Ridge 
alpha = 5 copy _ X = True fit _ 
intercept = True max _ iter = None normalize = 
False random _ state = None solver = auto tol 
= 0.001 resid _ train = y _ train model 
. predict X _ train sse _ train = sum 
resid _ train * * 2 print sse _ train2963 
. 35374445resid _ test = y _ test model . 
predict X _ test sse _ test = sum resid 
_ test * * 2 print sse _ test187177 . 
590437 残差 平方和 还 比较 高 model . score X 
_ train y _ train model . score X _ 
test y _ test 0 . 9 9 1 9 
7 1 3 2 1 5 2 0 1 1 
4 1 4 0 . 2 9 2 1 3 
9 8 8 6 9 9 1 6 8 5 
0 3 之前 测试 集 的 R 方是/nr 0.26 这里 
是 0.29 略有 改善 fig ax = plot _ residuals 
_ and _ coeff resid _ train resid _ test 
model . coef _ 从图/nr 上看 training 的 残差 增加 
了 testing 的 有所 减少 系数 大小 没有 太多 改善 
所以 在 这里 使用 岭回归 有 一点点 效果 并不 明显 
下面 使用 正则化 的 另一种 方法 称为 Lasso 思路 跟 
岭回归 是 一样 的 都是 把 残差 平方和 以及 系数 
放到 loss function 中 既 要使 残差 平方和 小 又 
要使 系数 小 但 Losso 公式 有点 不 一样 Losso 
是 把 权重 的 绝对值 加起来 而 岭回归 是 把 
权重 的 方法 加起来 有 这么 一点 不 一样 就 
可以 使得 很多 权 重回 为 0 看 小 效果 
model = linear _ model . Lasso alpha = 1.0 
model . fit X _ train y _ train Lasso 
alpha = 1.0 copy _ X = True fit _ 
intercept = True max _ iter = 1000 normalize = 
False positive = False precompute = False random _ state 
= None selection = cyclic tol = 0.0001 warm _ 
start = False resid _ train = y _ train 
model . predict X _ train sse _ train = 
sse resid _ train print sse _ train256 . 539066413resid 
_ test = y _ test model . predict X 
_ test sse _ test = sse resid _ test 
print sse _ test691 . 523154567fig ax = plot _ 
residuals _ and _ coeff resid _ train resid _ 
test model . coef _ testing 的 残差 有 明显 
的 减小 范围 减小 到 10 到 15 之间 很多 
系数 也 变为 0 真正 的 起 作用 的 只有 
少数 几个 这是 lasso 的 优点 它 可以 应付 有 
很多 噪音 的 情况 对于 维度 比较 高 噪音 比 
较多 的 情况 lasso 可以 在 建模 的 同时 做 
降 维 alphas = np . logspace 4 2 100 
# 尝试 100 个 不同 的 alphacoeffs = np . 
zeros len alphas X _ train . shape 1 sse 
_ train = np . zeros _ like alphas sse 
_ test = np . zeros _ like alphas for 
n alpha in enumerate alphas model = linear _ model 
. Lasso alpha = alpha model . fit X _ 
train y _ train coeffs n = model . coef 
_ resid = y _ train model . predict X 
_ train sse _ train n = sum resid * 
* 2 resid = y _ test model . predict 
X _ test sse _ test n = sum resid 
* * 2 fig axes = plt . subplots 1 
2 figsize = 12 4 sharex = True for n 
in range coeffs . shape 1 axes 0 . plot 
np . log10 alphas coeffs n color = k lw 
= 0.5 axes 1 . semilogy np . log10 alphas 
sse _ train label = train axes 1 . semilogy 
np . log10 alphas sse _ test label = test 
axes 1 . legend loc = 0 axes 0 . 
set _ xlabel r $ { \ log _ { 
10 } } \ alpha $ fontsize = 18 axes 
0 . set _ ylabel r coefficients fontsize = 18 
axes 1 . set _ xlabel r $ { \ 
log _ { 10 } } \ alpha $ fontsize 
= 18 axes 1 . set _ ylabel r sse 
fontsize = 18 fig . tight _ layout alpha 为 
0 时 表示 没有 在 loss function 中放 权重 项 
即 没有 惩罚 这时 做 回归 跟 前面 结果 是 
一样 的 alpha 增大 很多 噪音 的 因子 就会 降为 
0 即对 变量 做 筛选 我们 的 目标 是 要使 
模型 的 预测 效果 最佳 自然 要 选择 测试 集上 
残差 平方和 最小 的 地方 所 对应 的 alpha 怎么 
求 这个 点 实际 用时 用 LassoCV 自动 找出 最好 
的 alpha model = linear _ model . LassoCV # 
可以 去 尝试 不同 的 参数值 model . fit X 
_ all y _ all LassoCV alphas = None copy 
_ X = True cv = None eps = 0.001 
fit _ intercept = True max _ iter = 1000 
n _ alphas = 100 n _ jobs = 1 
normalize = False positive = False precompute = auto random 
_ state = None selection = cyclic tol = 0.0001 
verbose = False model . alpha _ # 自动 找出 
最好 的 alpha0 . 0 6 5 5 9 2 
3 8 7 4 7 5 3 4 7 1 
8 r e s i d _ train = y 
_ train model . predict X _ train sse _ 
train = sse resid _ train print sse _ train1 
. 76481994041resid _ test = y _ test model . 
predict X _ test sse _ test = sse resid 
_ test print sse _ test1 . 31238073253 效果 非常 
不错 model . score X _ train y _ train 
model . score X _ test y _ test 0 
. 9999952185351132 0 . 9 9 9 9 9 5 
0 3 6 8 9 5 3 2 7 8 
7 fig ax = plot _ residuals _ and _ 
coeff resid _ train resid _ test model . coef 
_ training 和 testing 的 残差 都 比较 小 无关 
变量 的 系数 都被 压到 0 效果 非常 好 分类/n 
问题/n 分类/n 问题/n 和/c 回归/v 问题/n 有/v 什么/r 区别/n 分类 
的 目标 和 回归 不一样 虽然 都是 做 预测 回归 
的 y 是 连续 的 数值 分类 预测 的 y 
是 离散 的 数值 比如 预测 明天 会 不会 下雨 
就有 会 下雨 和 不会 下雨 两种 情况 这是 二元 
分类 问题 编码 时可/nr 编为 0 和 1 两种 情况 
还有 是 判断 一个 图形 是 什么 阿拉伯数字 可能 是 
0 1 . . . 9 有 10 个 可能 
的 分类 是 多元 分类 问题 之前 用 的 statsmodels 
的 logistic 回归 就是 分类 模型 这里 用 sklearn 中 
更多 的 分类 模型 sklearn 内 有 哪些 分类 模型 
广义 线性 模型 岭回归 Logistic 回归 贝叶斯 回归 支持 向量 
机 最 近邻 朴素 贝叶斯 决策树 iris = datasets . 
load _ iris # 载入 鸢尾花 数据集 print iris . 
target _ names print iris . feature _ names setosa 
versicolor virginica sepal length cm sepal width cm petal length 
cm petal width cm print iris . data . shape 
print iris . target . shape 150 4 150 X 
_ train X _ test y _ train y _ 
test = cross _ validation . train _ test _ 
split iris . data iris . target train _ size 
= 0.7 # 70% 用于 训练 30% 用于 检验 classifier 
= linear _ model . L o g i s 
t i c R e g r e s s 
i o n classifier . fit X _ train y 
_ train L o g i s t i c 
R e g r e s s i o n 
C = 1.0 class _ weight = None dual = 
False fit _ intercept = True intercept _ scaling = 
1 max _ iter = 100 multi _ class = 
ovr n _ jobs = 1 penalty = l2 random 
_ state = None solver = liblinear tol = 0.0001 
verbose = 0 warm _ start = False y _ 
test _ pred = classifier . predict X _ test 
如何 评估 分类 效果 confusion matrix 是 什么 用 metrics 
模块 来 检查 模型 效果 其中 的 classification _ report 
是 分类 报告 显示 各种 指标 来 衡量 模型 的 
效果 print metrics . classification _ report y _ test 
y _ test _ pred # 真实 的 y 和 
预测 的 yprecision recall f1 score support 0 1.00 1.00 
1.00 15 1 1.00 0.75 0.86 16 2 0.78 1.00 
0.88 14 avg / total 0.93 0.91 0.91 45precision 是 
精准度 recall 是 召回率 fs score 是 F1 值 从 
这几个 值 可以 看到 模型 很 完美 还 可以 用 
混淆 矩阵 confusion matrix 来 评估 分类器 混淆 矩阵 的 
每 一列 代表 了 预测 类别 每 一列 的 总数 
表示 预测 为 该 类别 的 数据 的 数目 每 
一行 代表 了 数据 的 真实 归属 类别 每 一行 
的 数据 总数 表示 该 类别 的 数据 实例 的 
数目 每 一列 中的 数值 表示 真实 数据 被 预测 
为 该类 的 数 如果 混淆 矩阵 的 所有 数据 
都在 对角 线上 就 说明 预测 是 完全 正确 的 
metrics . confusion _ matrix y _ test y _ 
test _ pred array 15 0 0 0 12 4 
0 0 14 y _ test . shape 45 classifier 
= tree . D e c i s i o 
n T r e e C l a s s 
i f i e r # 决策树 classifier . fit 
X _ train y _ train y _ test _ 
pred = classifier . predict X _ test metrics . 
confusion _ matrix y _ test y _ test _ 
pred array 12 0 0 0 13 2 0 2 
16 classifier = neighbors . K N e i g 
h b o r s C l a s s 
i f i e r # K 近邻 classifier . 
fit X _ train y _ train y _ test 
_ pred = classifier . predict X _ test metrics 
. confusion _ matrix y _ test y _ test 
_ pred array 12 0 0 0 14 1 0 
2 16 classifier = svm . SVC # 支持 向量 
机 classifier . fit X _ train y _ train 
y _ test _ pred = classifier . predict X 
_ test metrics . confusion _ matrix y _ test 
y _ test _ pred array 12 0 0 0 
15 0 0 3 15 classifier = ensemble . R 
a n d o m F o r e s 
t C l a s s i f i e 
r classifier . fit X _ train y _ train 
y _ test _ pred = classifier . predict X 
_ test metrics . confusion _ matrix y _ test 
y _ test _ pred array 12 0 0 0 
14 1 0 2 16 train _ size _ vec 
= np . linspace 0.1 0.9 30 # 尝试 不同 
的 样本 大小 classifiers = tree . D e c 
i s i o n T r e e C 
l a s s i f i e r neighbors 
. K N e i g h b o r 
s C l a s s i f i e 
r svm . SVC ensemble . R a n d 
o m F o r e s t C l 
a s s i f i e r cm _ 
diags = np . zeros 3 len train _ size 
_ vec len classifiers dtype = float # 用来 放 
结果 for n train _ size in enumerate train _ 
size _ vec X _ train X _ test y 
_ train y _ test = \ cross _ validation 
. train _ test _ split iris . data iris 
. target train _ size = train _ size for 
m Classifier in enumerate classifiers classifier = Classifier classifier . 
fit X _ train y _ train y _ test 
_ pred = classifier . predict X _ test cm 
_ diags n m = metrics . confusion _ matrix 
y _ test y _ test _ pred . diagonal 
cm _ diags n m / = np . bincount 
y _ test fig axes = plt . subplots 1 
len classifiers figsize = 12 3 for m Classifier in 
enumerate classifiers axes m . plot train _ size _ 
vec cm _ diags 2 m label = iris . 
target _ names 2 axes m . plot train _ 
size _ vec cm _ diags 1 m label = 
iris . target _ names 1 axes m . plot 
train _ size _ vec cm _ diags 0 m 
label = iris . target _ names 0 axes m 
. set _ title type Classifier . _ _ name 
_ _ axes m . set _ ylim 0 1.1 
axes m . set _ xlim 0.1 0.9 axes m 
. set _ ylabel classification accuracy axes m . set 
_ xlabel training size ratio axes m . legend loc 
= 4 fig . tight _ layout 样本 大小 对 
预测 分类 结果 有 影响 吗 从 上图 可 看出 
当 样本 太 小时 有些 模型 会 表现 很差 比如 
决策树 K 最 近邻 和 SVC KNN 和 SVM 是 
比较 依赖 数据 规模 的 当 train size 变大 后 
分类 准确率 就 比较 高了 聚 类 问题 聚 类 
是 一种 无 监督 学习 方法 聚 类 问题 的 
应用 场景 是 什么 主要 解决 把 一群 对象 划分 
为 若干 个 组 的 问题 例如 用户 细分 选择 
若干 指标 把 用户 群聚 为 若干 个 组 组 
内 特征 类似 组件 特征 差异 明显 应用 最 广泛 
的 聚 类 方法 是 什么 K means 聚 类 
X y = iris . data iris . target np 
. random . seed 123 n _ clusters = 3 
# 可以 尝试 其它 值 c = cluster . KMeans 
n _ clusters = n _ clusters # 实例 化 
c . fit X # 这里 的 fit 没有 yKMeans 
copy _ x = True init = k means + 
+ max _ iter = 300 n _ clusters = 
3 n _ init = 10 n _ jobs = 
1 precompute _ distances = auto random _ state = 
None tol = 0.0001 verbose = 0 y _ pred 
= c . predict X print y _ pred 8 
print y 8 1 1 1 1 1 1 1 
2 2 2 2 2 2 0 0 0 0 
0 0 0 0 0 0 0 0 0 1 
1 1 1 1 1 2 2 2 2 2 
2 聚 类 结果 是 1 2 0 真实 结果 
是 0 1 2 为了 跟 真实 值 做 比对 
需要 做个 转换 idx _ 0 idx _ 1 idx 
_ 2 = np . where y _ pred = 
= n for n in range 3 # 做 转换 
y _ pred idx _ 0 y _ pred idx 
_ 1 y _ pred idx _ 2 = 2 
0 1 print y _ pred 8 print y 8 
0 0 0 0 0 0 0 1 1 1 
1 1 1 2 2 2 2 2 2 0 
0 0 0 0 0 0 1 1 1 1 
1 1 2 2 2 2 2 2 print metrics 
. confusion _ matrix y y _ pred # 当然 
在 实际 场景 中 是 不 可能 有 混淆 矩阵 
的 因为 根本 就 没有 真实 的 y 50 0 
0 0 48 2 0 14 36 补充 阅读 scikit 
learn 官网 Scipy Lectures 第 20 章 Numerical Python 第 
15 章 数据挖掘 导论 