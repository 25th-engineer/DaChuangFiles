在 前段 时间 做 本科 毕业 设计 的 时候 遇到 
了 各个 类别 的 样本 量 分布 不均 的 问题 
某些 类别 的 样本 数量 极多 而 有些 类别 的 
样本 数量 极少 也 就是 所谓 的 类 不平衡 class 
imbalance 问题 本篇 简述 了 以下 内容 什么 是 类 
不 平衡 问题 为什么 类 不 平衡 是 不好 的 
几种 解决方案 SMOTE 过 采样 算法 进一步 阅读 什么 是 
类 不 平衡 问题 类 不平衡 class imbalance 是 指在 
训练 分类器 中所 使用 的 训练 集 的 类别 分布 
不均 比如说 一个 二 分类 问题 1000个 训练样本 比较 理想 
的 情况 是 正 类 负 类 样本 的 数量 
相 差不多 而 如果 正 类 样本 有 995个 负 
类 样本 仅 5个 就 意味着 存在 类 不平衡 在后 
文中 把 样本 数量 过少 的 类别 称为 少数类 但 
实际上 数据 集上 的 类 不平衡 到底 有 没有 达到 
需要 特殊 处理 的 程度 还要 看 不 处理 时 
训练 出来 的 模型 在 验证 集上 的 效果 有些 
时候 是 没必要 处理 的 为什么/r 类/q 不/d 平衡/a 是/v 
不好/d 的/uj 从/p 模型/n 的/uj 训练/vn 过程/n 来看/u 从/p 训练/vn 
模型/n 的/uj 角度/n 来说/u 如果 某类 的 样本 数量 很少 
那么 这个 类别 所 提供 的 信息 就 太少 使用 
经验 风险 模型 在 训练 集上 的 平均 损失 最小化 
作为 模型 的 学习 准则 设 损失 函数 为 0 
1 loss 这 是 一种 典型 的 均等 代价 的 
损失 函数 那么 优化 目标 就 等价 于 错误率 最小化 
也 就是 accuracy 最大化 考虑 极端 情况 1000个 训练样本 中 
正 类 样本 999个 负 类 样本 1个 训练 过程 
中 在 某次 迭代 结束 后 模型 把 所有 的 
样本 都 分为 正 类 虽然 分 错 了 这个 
负 类 但是 所 带来 的 损失 实在 微不足道 accuracy 
已经 是 99.9% 于是 满足 停机 条件 或者 达到 最大 
迭代 次数 之 后 自然 没 必要 再 优化 下去 
ok 到此为止 训练 结束 于是/nr 这个 模型 模型 没有 学习 
到 如何 去 判 别出 少数类 从 模型 的 预测 
过程 来看 考虑 二项 Logistic 回归模型 输入 一个 样本 $ 
\ textbf x $ 模型 输出 的 是 其 属 
于正 类 的 概率 $ \ hat y $ 当 
$ \ hat y 0.5 $ 时 模型 判定 该 
样 本属 于正 类 否则 就是 属于 反 类 为什么 
是 0.5 呢 可以 认为 模型 是 出于 最大 后验/nr 
概率 决策 的 角度 考虑 的 选择/v 了/ul 0.5/mx 意味着/v 
当/t 模型/n 估计/v 的/uj 样本/n 属/v 于正/i 类/q 的/uj 后/f 
验/v 概率/n 要/v 大于/d 样/n 本属于/i 负/v 类/q 的/uj 后验/nr 
概率/n 时/n 就将/i 样本/n 判为/v 正/d 类/q 但 实际上 这个 
后验/nr 概率 的 估计值 是否 准确 呢 从 几率 odds 
的 角度 考虑 几率 表达 的 是 样本 属 于正 
类 的 可能性 与 属于 负 类 的 可能性 的 
比值 模型 对于 样本 的 预测 几率 为 $ \ 
dfrac { \ hat y } { 1 \ hat 
y } $ 模型 在 做出 决策 时 当然 希望 
能够 遵循 真实 样本 总体 的 正负 类 样本分布 设 
$ \ theta $ 等 于正 类 样本数 除以 全部 
样本数 那么 样本 的 真实 几率 为   $ \ 
dfrac { \ theta } { 1 \ theta } 
$   当 观测 几率 大 于 真实 几率 时 
也 就是 $ \ hat y \ theta $ 时 
那么 就 判定 这 个样 本属 于正 类 虽然 我们 
无法 获悉 真实 样本 总体 但 之于 训练 集 存在 
这样 一个 假设 训练 集 是 真实 样本 总体 的 
无偏 采样 正是 因为 这个 假设 所以 认为 训练 集 
的 观测 几率 $ \ dfrac { \ hat \ 
theta } { 1 \ hat \ theta } $ 
就 代表 了 真实 几率 $ \ dfrac { \ 
theta } { 1 \ theta } $   所以 
在 这个 假设 下 当 一个 样本 的 预测 几率 
大于 观测 几率 时 就 应该 将 样本 判断 为 
正 类 几种 解决 方案 目前 主要 有三种 办法 1 
. 调整 $ \ theta $ 值 根据 训练 集 
的 正负 样本 比例 调整 $ \ theta $ 值 
这样 做 的 依据 是 上面 所述 的 对 训练 
集 的 假设 但在 给定 任务 中 这个 假设 是否 
成立 还 有待 讨论 2 . 过 采样 对 训练 
集 里面 样本 数量 较少 的 类别 少数类 进行 过 
采样 合成 新的 样本 来 缓解 类 不平衡 下面 将 
介绍 一种 经典 的 过 采样 算法 SMOTE 3 . 
欠 采样 对 训练 集 里面 样本 数量 较多 的 
类别 多数 类 进行 欠 采样 抛弃 一些 样本 来 
缓解 类 不平衡 SMOTE 过 采样 算法 JAIR 2002 的 
文章 SMOTE Synthetic Minority Over sampling Technique 提出 了 一种 
过 采样 算法 SMOTE 概括 来说 本 算法 基于 插值 
来 为 少数 类 合成 新的 样本 下面 介绍 如何 
合成 新的 样本 设 训练 集 的 一个 少数类 的 
样本 数 为 $ T $ 那么 SMOTE 算法 将为 
这个 少数类 合成 $ NT $ 个 新 样本 这里 
要求 $ N $ 必须 是 正整数 如果 给定 的 
$ N 1 $ 那么 算法 将 认为 少数类 的 
样本 数 $ T = NT $ 并将 强制 $ 
N = 1 $ 考虑 该 少数类 的 一个 样本 
$ i $ 其 特征向量 为 $ \ boldsymbol x 
_ i i \ in \ { 1 . . 
. T \ } $ 1 . 首先 从该/nr 少数类 
的 全部 $ T $ 个 样本 中 找到 样本 
$ \ boldsymbol   x _ i $ 的 $ 
k $ 个 近邻 例 如用 欧氏距离 记为 $ \ 
boldsymbol   x _ { i near } near \ 
in \ { 1 . . . k \ } 
$ 2 . 然后 从这 $ k $ 个 近邻 
中 随机 选择 一个 样本 $ \ boldsymbol   x 
_ { i nn } $ 再 生成 一个 $ 
0 $ 到 $ 1 $ 之间 的 随机数 $ 
\ zeta _ 1 $ 从而 合成 一个 新 样本 
$ \ boldsymbol   x _ { i1 } $ 
$ $ \ boldsymbol   x _ { i1 } 
= \ boldsymbol   x _ i + \ zeta 
_ 1 \ cdot \ boldsymbol   x _ { 
i nn } − \ boldsymbol   x _ i 
$ $ 3 . 将 步骤 2 重复 进行 $ 
N $ 次 从而 可以 合成 $ N $ 个 
新 样本 $ \ boldsymbol   x _ { inew 
} new \ in { 1 . . . N 
} $ 那么 对 全部 的 $ T $ 个 
少数类 样本 进行 上述 操作 便 可为 该 少数类 合成 
$ NT $ 个 新 样本 如果 样本 的 特征 
维数 是 $ 2 $ 维 那么/r 每个/r 样本/n 都/d 
可以/c 用/p 二维/m 平/n 面上/f 的/uj 一个/m 点/m 来/v 表示/v 
SMOTE 算法 所 合成 出 的 一个 新 样本 $ 
\ boldsymbol   x _ { i1 } $ 相当 
于是 表示 样本 $ \ boldsymbol   x _ i 
$ 的 点 和 表示 样本 $ \ boldsymbol   
x _ { i nn } $ 的 点 之间 
所 连 线段 上 的 一个 点 所以 说 该 
算法 是 基于 插值 来 合成 新 样本 进一步 阅读 
有 两篇 翻 译自 国外 博客 的 文章 解决 真实世界 
问题 如何 在 不 平衡 类 上 使用 机器学习 从重 
采样 到 数据 合成 如何 处理 机器学习 中的 不平衡 分类 
问题 可以 先读 中文 的 了解 一下 说 了 哪些 
事情 如果 感 兴趣 的话 就 去看 英文 原文 来 
深入 学习 参考 机器学习 周志华 SMOTE Synthetic Minority Over sampling 
Technique JAIR 2002 