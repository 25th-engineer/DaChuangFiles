本来 这篇 是 准备 5.15 更 的 但是 上周 一直 
在 忙 签证 和 工作 的 事 没 时间 就 
推迟 了 现在 终于 有 时间 来 写写 Learning Spark 
最后 一 部分 内容 了 第 10 11 章 主要 
讲 的 是 Spark Streaming 和 MLlib 方面 的 内容 
我们 知道 Spark 在 离线 处理 数据 上 的 性能 
很好 那么 它 在 实时 数据 上 的 表现 怎么样 
呢 在 实际 生产 中 我们 经常 需要 即使 处理 
收到 的 数据 比如 实时 机器学习 模型 的 应用 自动 
异常 的 检测 实时 追踪 页面 访问 统计 的 应用 
等 Spark Streaming 可以 很好 的 解决 上述 类似 的 
问题 了解 Spark Streaming 只 需要 掌握 以下几点 即可 DStream 
概念 离散化 流 discretized stream 是 随 时间 推移 的 
数据 由 每个 时间区间 的 RDD 组成 的 序列 DStream 
可以 从 Flume Kafka 或者 HDFS 等 多个 输入 源 
创建 操作 转换 和 输出 支持 RDD 相关 的 操作 
增加 了 滑动 窗口 等于 时间 相关 的 操作 下面 
以 一张 图 来 说明 Spark Streaming 的 工作 流程 
从上 图中 也 可以 看到 Spark Streaming 把 流式 计算 
当做 一 系列 连续 的 小规模 批处理 来 对待 它 
从 各种 输入 源 读取数据 并把 数据分组 为 小 的 
批次 新的 批次 按 均匀 的 时间 间隔 创建 出来 
在 每个 时间区间 开始 的 时候 一个 新 的 批次 
就 创建 出来 在 该 区间 内 收到 的 数据 
都会 被 添加 到 这个 批次 中去 在 时间区间 结束时 
批次 停止 增长 转化 操作 无状态 转化 操作 把 简单 
的 R D D t r a n s f 
o r m a t i o n 分别 应用 
到 每个 批次 上 每个 批次 的 处理 不 依赖 
于 之前 的 批次 的 数据 包括 map filter reduceBykey 
等 有 状态 转化 操作 需要 使用 之前 批次 的 
数据 或者 中间 结果 来 计算 当前 批次 的 数据 
包括 基于 滑动 窗口 的 转化 操作 和 追踪 状态 
变化 的 转化 操作 updateStateByKey 无状态 转化 操作 有 状态 
转化 操作 Windows 机制 一 图 盛 千言 上图 应该 
很 容易 看懂 下面 举个 实例 JAVA 写 的 UpdateStateByKey 
转化 操作 主要 用于 访问 状态变量 用于 键值 对 形式 
的 DStream 首先 会 给定 一个 由 键 事件 对 
构成 的 DStream 并 传递 一个 指定 如何 个人 剧 
新的 事件 更新 每个 键 对应状态 的 函数 它 可以 
构建 出 一个 新的 DStream 为 键 状态 通俗 点 
说 加入 我们 想 知道 一个 用户 最近 访问 的 
10个 页面 是 什么 可以 把 键 设置 为 用户 
ID 然后 UpdateStateByKey 就 可以 跟踪 每个 用户 最近 访问 
的 10个 页面 这个 列表 就是 状态 对象 具体 的 
要 怎么 操作 呢 UpdateStateByKey 提供 了 一个 update events 
oldState 函数 用于 接收 与 某 键 相关 的 时间 
以及 该 键 之前 对应 的 状态 然后 返回 这个 
键 对应 的 新 状态 events 是 在 当前 批次 
中 收到 的 时间 列表 可能 为 空 oldState 是 
一个 可选 的 状态 对象 存放在 Option 内 如果 一个 
键 没有 之前 的 状态 可以为 空 newState 由 函数 
返回 也 以 Option 形式 存在 如果 返回 一个 空的/nr 
Option 表示 想要 删除 该 状态 UpdateStateByKey 的 结果 是 
一个 新的 DStream 内部 的 RDD 序列 由 每个 时间区间 
对应 的 键 状态 对 组成 接下来 讲 一下 输入 
源 核心 数据源 文件 流 包括 文本格式 和 任意 hadoop 
的 输入 格式 附加 数据源 kafka 和 flume 比较 常用 
下面 会 讲 一下 kafka 的 输入 多 数据源 与 
集群 规模 Kafka 的 具体 操作 如下 基于 MLlib 的 
机器学习 一般 我们 常用 的 算法 都是 单机 跑 的 
但是 想 要在 集群 上 运行 不能 把 这些 算法 
直接 拿 过来 用 一是 数据格式 不同 单机 上 我们 
一般 是 离散 型 或者 连续型 的 数据 数据类型 一般 
为 array list dataframe 比较 多 以 txt csv 等 
格式 存储 但是 在 spark 上 数据 是以 RDD 的 
形式 存在 的 如何 把 ndarray 等 转化 为 RDD 
是 一个 问题 此外 就算 我们 把 数据 转化成 RDD 
格式 算法 也会 不一样 举个 例子 你 现在 有 一堆 
数据 存储 为 RDD 格式 然后 设置 了 分区 每个 
分区 存储 一些 数据 准备 来 跑 算法 可以 把 
每个 分区 看做 是 一个 单机 跑 的 程序 但是 
所有 分区 跑完 以后 呢 怎么 把 结果 综合 起来 
直接 求 平均值 还是 别的/nr 方式 所以 说 在 集群 
上 跑 的 算法 必须 是 专门 写 的 分布式 
算法 而且 有些 算法 是 不能 分布式 的 跑 Mllib 
中 也只 包含 能够 在 集群 上 运行 良好 的 
并行算法 MLlib 的 数据 类型 Vector 向量 mllib . linalg 
. Vectors 支持 dense 和 sparse 稠密 向量 和 稀疏 
向量 区别 在 与 前者 的 没 一个 数值 都会 
存储 下来 后者 只 存储 非零 数值 以 节约 空间 
LabeledPoint mllib . regression 表示 带 标签 的 数据 点 
包含 一个 特征向量 与 一个 标签 注意 标签 要 转化 
成 浮点 型 的 通过 StringIndexer 转化 Rating mllib . 
recommendation 用户 对 一个 产品 的 评分 用于 产品 推荐 
各种 Model 类 每个 Model 都是 训练 算法 的 结果 
一般 都 有一个 predict 方法 可以 用来 对 新的 数据 
点 或者 数 据点 组成 的 RDD 应用 该 模型 
进行 预测 一般来说 大多数 算法 直接 操作 由 Vector LabledPoint 
或 Rating 组成 的 RDD 通常/d 我们/r 从/p 外部/f 数据/n 
读取数据/n 后/f 需要/v 进行/v 转化/v 操作/v 构建/v RDD/w 具体 的 
聚 类 和 分类 算法 原理 不多 讲了 可以 自己 
去看 MLlib 的 在线 文档 里 去看 下面 举个 实例 
垃圾邮件 分类 的 运行 过程 步骤 1 . 将 数据 
转化 为 字符串 RDD2 . 特征提取 把 文本 数据 转化 
为 数值 特征 返回 一个 向量 RDD3 . 在 训练 
集上 跑 模型 用 分类 算法 4 . 在 测试 
系上 评估 效果 具体 代码 1 from pyspark . mllib 
. regression import LabeledPoint 2 from pyspark . mllib . 
feature import HashingTF 3 from pyspark . mllib . calssification 
import L o g i s t i c R 
e g r e s s i o n W 
i t h G D 4 5 spam = sc 
. textFile spam . txt 6 normal = sc . 
textFile normal . txt 7 8 # 创建 一个 HashingTF 
实例 来 把 邮件 文本 映射 为 包含 10000个 特征 
的 向量 9 tf = HashingTF numFeatures = 10000 10 
# 各 邮件 都被 切 分为 单词 每个 单词 背 
映射 为 一个 特征 11 spamFeatures = spam . map 
lambda email tf . transform email . split 12 normalFeatures 
= normal . map lambda email tf . transform email 
. split 13 14 # 创建 LabeledPoint 数据集 分别 存放 
阳性 垃圾邮件 和 阴性 正常 邮件 的 例子 15 positiveExamples 
= spamFeatures . map lambda features LabeledPoint 1 features 16 
negativeExamples = normalFeatures . map lambda features LabeledPoint 0 features 
17 trainingData = positiveExamples . union negativeExamples 18 trainingData . 
cache # 因为 逻辑 回归 是 迭代 算法 所以 缓存数据 
RDD 19 20 # 使用 SGD 算法 运行 逻辑 回归 
21 model = L o g i s t i 
c R e g r e s s i o 
n W i t h G D . train trainingData 
22 23 # 以 阳性 垃圾邮件 和 阴性 正常 邮件 
的 例子 分别 进行 测试 24 posTest = tf . 
transform O M G GET cheap stuff by sending money 
to . . . . split 25 negTest = tf 
. transform Hi Dad I stared studying Spark the other 
. . . . split 26 print Prediction for positive 
test examples % g % model . predict posTest 27 
print Prediction for negative test examples % g % model 
. predict negTest 这个 例子 很 简单 讲 的 也很 
有限 建议 大家 根据 自己 的 需求 直接 看 MLlib 
的 官方 文档 关于 聚 类 分类 讲 的 都 
很详细 注 图片 参考 同事 的 PPT 讲义 ^ _ 
^ 已 授权 哈哈 