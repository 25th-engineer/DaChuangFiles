10 . Dimensionality R e d u c t i 
o n C o n t e n t 1 
0 . Dimensionality Reduction10 . 1 Motivation10 . 1.1 Motivation 
one Data Compression10 . 2.2 Motivation two Visualization10 . 2 
Principal Component Analysis10 . 2.1 Problem formulation10 . 2.2 Principal 
Component Analysis Algorithm10 . 2.3 Choosing the Number of Principal 
Components10 . 2.4 Advice for Applying PCA10 . 1 Motivation10 
. 1.1 Motivation one Data Compression 如果 我们 有 许多 
冗余 的 数据 我们 可能 需要 对 特征 量 进行 
降 维 Dimensionality Reduction 我们 可以 找到 两 个 非常 
相关 的 特征 量 可视化 然后 用 一条 新的 直线 
来 准确 的 描述 这 两个 特征 量 例如 1 
所示 x1 和 x2 是 两个 单位 不同 本质 相同 
的 特征 量 我们 可以 对 其 降 维 1 
一个 2 维 到 1 维 的 例子 又如 2 
所示 的 3 维 到 2 维 的 例子 通过 
对 x1 x2 x3 的 可视化 发现 虽然 样本 处于 
3 维空间 但是 他们 大多数 都 分布 在 同一 个 
平面 中 所以 我们 可以 通过 投影 将 3 维 
降为 2 维 2 一个 3 维 到 2 维 
的 例子 降 维 的 好处 很明显 它 不仅 可以 
数据 减少 对 内存 的 占用 而且 还 可以 加快 
学习 算法 的 执行 注意 降 维 只是 减小 特征 
量 的 个数 即 n 而 不是 减小 训练 集 
的 个数 即 m 10 . 2.2 Motivation two Visualization 
我们 可以 知道 但 特征 量 维数 大于 3时 我们 
几乎 不能 对 数据 进行 可视化 所以 有时 为了 对 
数据 进行 可视化 我们 需要 对 其 进行 降 维 
我们 可以 找到 2个 或 3个 具有 代表性 的 特征 
量 他们 大致 可以 概括 其他 的 特征 量 例如 
描述 一个 国家 有 很多 特征 量 比如 GDP 人均 
GDP 人均寿命 平均 家庭收入 等等 想要 研究 国家 的 经济 
情况 并 进行 可视化 我们 可以 选 出 两个 具有 
代表性 的 特征 量 如 GDP 和 人均 GDP 然后 
对 数据 进行 可视化 如 3 所示 3 一个 可视化 
的 例子 10.2 Principal Component Analysis 主 成分 分析 Principal 
Component Analysis PCA 是 最 常用 的 降 维 算法 
10 . 2.1 Problem formulation 首先 我们 思考 如下 问题 
对于 正交 属性 空间 对 2 维空间 即为 直角 坐标系 
中的 样本点 如何 用 一个 超平面 直线 / 平面 的 
高维 推广 对 所有 样本 进行 恰当 的 表达 事实上 
若 存在 这样 的 超平面 那么 它 大概 应 具有 
这样 的 性质 最近 重构 性 样本点 到 这个 超平面 
的 距离 都 足够 近 最大 可分性 样本点/i 在/p 这个/r 
超/v 平/n 面上/f 的/uj 投影/n 能/v 尽可能/d 分开/v 下面 我们 
以 3 维 降到 2 维 为例 来 试着 理解 
为什么 需要 这 两种 性质 4 给 出了 样本 在 
3 维空间 的 分布 情况 其 中图 2 是 图 
1 旋转 调整 后的/nr 结果 在 10.1节 我们 默认 以 
红色 线 所画 平面 不妨 称之为 平面 s1 为 2 
维 平面 进行 投影 降 维 投影 结果 为 5 
的 1 所示 这样 似乎 还 不错 那/r 为什么/r 不用/v 
蓝/nr 色线/n 所画/n 平面/n 不妨 称之为 平面 s2 进行 投影 
呢 可以想象 用 s2 投影 的 结果 将如 5 的 
2 所示 4 样本 在 3 维 正交 空间 的 
分布 5 样本 投影 在 2 维 平面 后的/nr 结果 
由 4 可以 很 明显 的 看出 对 当前 样本 
而言 s1 平面 比 s2 平面 的 最近 重构 性 
要好 样本 离 平面 的 距离 更近 由 5 可以 
很 明显 的 看出 对 当前 样本 而言 s1 平面 
比 s2 平面 的 最大 可分性 要好 样本点 更 分散 
不难理解 如果 选择 s2 平面 进行 投影 降 维 我们 
会 丢失 更多 相当多 的 特征 量 信息 因为 它 
的 投影 结果 甚至 可以 在 转化 为 1 维 
而在 s1 平 面上 的 投影 包含 更多 的 信息 
丢失 的 更少 这样 是否 就是说 我们 从3维/nr 降到 1 
维 一定会 丢失 相当多 的 信息 呢 其实 也 不一定 
试想 如果 平面 s1 投影 结果 和 平面 s2 的 
类似 那么 我们 可以 推断 这 3个 特征 量 本质上 
的 含义 大致相同 所以 即使 直接 从3维/nr 到 1 维 
也不会 丢失 较多 的 信息 这里 也 反映 了 我们 
需要 知道 如何 选择 到底 降到 几维 会 比较 好 
在 10 . 2.3 节 中 讨论 让 我们 高兴 
的 是 上面 的 例子 也 说明 了 最近 重构 
性 和 最大 可分性 可以 同时 满足 更让人 兴奋 的 
是 分别 以 最近 重构 性 和 最大 可分性 为 
目标 能够 得到 PCA 的 两种 等价 推导 一般 的 
将 特征 量 从n维/nr 降到 k 维 以 最近 重构 
性 为 目标 PCA 的 目标 是 找到 k 个 
向量 将 所有 样本 投影 到这 k 个 向量 构成 
的 超平面 使得 投影 的 距离 最小 或者说 投影 误差 
projection error 最小 以 最大 可分性 为 目标 PCA 的 
目标 是 找到 k 个 向量 将 所有 样本 投影 
到这 k 个 向量 构成 的 超平面 使得 样本点 的 
投影 能够 尽可能 的 分开 也 就是 使 投 影后 
的 样本点 方差 最大化 注意 PCA 和 线性 回归 是 
不同 的 如 6 所示 线性 回归 是以 平方 误差 
和 SSE 最小 为 目标 参见 1 . 2.4 节 
而 PCA 是 使 投影 二维 即 垂直 距离 最小 
PCA 与 标记 或者 预测值 完全 无关 而 线性 回归 
是 为了 预测 y 的 值 6 PCA 不是 线性 
回归 分别 基于 上述 两种 目标 的 具体 推导 过程 
参见 周志华 老师 的 机器学习 P230 从 方差 的 角度 
推导 参见 李宏 毅 老师 机器学习 课程 Unsupervised Learning Principle 
Component Analysis 两种 等价 的 推导 结论 是 对 协方差 
矩阵 进行 特征值 分解 将 求得 的 特征 值 进行 
降序 排序 再取 前 k 个 特征值 对应 的 特征向量 
构成 其中 10 . 2.2 Principal Component Analysis Algorithm 基于 
上 一节 给出 的 结论 下面 给出 PCA 算法 输入 
训练 集 低 维空间 维数 k 过程 数据 预处理 对 
所有 样本 进行 中心化 即 使得 样本 和为0/nr 计算 样本 
的 协方差 矩阵 Sigma 其中 是 n * 1 的 
向量 在 matlab 中 具体 实现 如下 其中 X 为 
m * n 的 矩阵 Sigma = 1 / m 
* X * X 对 2中 求得 的 协方差 矩阵 
Sigma 进行 特征值 分解 在 实践中 通常 对 协方差 矩阵 
进行 奇异 值 分解 代替 特征值 分解 在 matlab 中 
实现 如下 U S V = svd Sigma svd 即为 
matlab 中 奇异 值 分解 的 内置 函数 取 最大 
的 k 个 特征值 所 对应 的 特征向量 在 matlab 
具体 实现 时 Ureduce = 认为 是 第 3步 求得 
的 U 的 前 k 个 即有 Ureduce = U 
1 k 其中 Ureduce 为 n * k 的 矩阵 
经过 了 上述 4步 得到 了 投影 矩阵 Ureduce 利用 
Ureduce 就 可以 得到 投 影后 的 样本 值 为 
k * 1 的 向量 下面 总结 在 matlab 中 
实现 PCA 的 全部 算法 假设 数据 已被 中心化 Sigma 
= 1 / m * X * X     
% compute the covariance matrix U S V = svd 
Sigma       % compute our projected d i 
r e c t i o n s U r 
e d u c e = U 1 k   
      % take the first k directionsZ = 
Ureduce * X           % compute 
the projected data points10 . 2.3 Choosing the Number of 
Principal Components 如何 选择 k 又称 为主 成分 的 个数 
的 值 首先 试想 我们 可以 使用 PCA 来 压缩 
数据 我们 应该 如何 解压 或者说 如何 回到 原本 的 
样本 值 事实上 我们 可以 利用 下列 等式 计算出 原始数据 
的 近似值 Xapprox Xapprox = Z * Ureduce m * 
n = m * k * k * n 自然 
的 还原 的 数据 Xapprox 越 接近 原始数据 X 说明 
PCA 误差 越小 基于 这点 下面 给出 选择 k 的 
一种 方法 结合 PCA 算法 选择 K 的 算法 总结 
如下 这个 算法 效率 特别 低 在 实际 应用 中 
我们 只需 利用 svd 函数 如下 10 . 2.4 Advice 
for Applying PCAPCA 通常用 来 加快 监督 学习 算法 PCA 
应该 只是 通过 训练 集 的 特征 量 来 获取 
投影 矩阵 Ureduce 而 不是 交叉 检验 集 或 测试 
集 但是 获取 到 Ureduce 之后 可以 应用 在 交叉 
检验 集 和 测试 集 避免 使用 PCA 来 防止 
过拟合 PCA 只是 对 特征 量 X 进行 降 维 
并 没有 考虑 Y 的 值 正则化 是 防止 过拟合 
的 有效 方法 不 应该 在 项目 一 开始 就 
使用 PCA 花 大量 时间 来 选择 k 值 很可能 
当前 项目 并不 需要 使用 PCA 来 降 维 同时 
PCA 将 特征 量 从n维/nr 降到 k 维 一定会 丢失 
一些 信息 仅仅 在 我们 需要 用 PCA 的 时候 
使用 PCA 降 维 丢失 的 信息 可能 在 一定 
程度 上 是 噪声 使用 PCA 可以 起到 一定 的 
去 噪 效果 PCA 通常 用来 压缩 数据 以 加快 
算法 减少 内存 使用 或 磁盘 占用 或者 用于 可视化 
k = 2 3 参考 机器学习   周志华 