转载 请 注明 出处 http / / www . cnblogs 
. com / ymingjingr / p / 4271742 . html 
目录 机器学习 基石 笔记 1 在 何时 可以 使用 机器学习 
1 机器学习 基石 笔记 2 在 何时 可以 使用 机器学习 
2 机器学习 基石 笔记 3 在 何时 可以 使用 机器学习 
3 修改版 机器学习 基石 笔记 4 在 何时 可以 使用 
机器学习 4 机器学习 基石 笔记 5 为什么 机器 可以 学习 
1 机器学习 基石 笔记 6 为什么 机器 可以 学习 2 
机器学习 基石 笔记 7 为什么 机器 可以 学习 3 机器学习 
基石 笔记 8 为什么 机器 可以 学习 4 机器学习 基石 
笔记 9 机器 可以 怎样 学习 1 机器学习 基石 笔记 
10 机器 可以 怎样 学习 2 机器学习 基石 笔记 11 
机器 可以 怎样 学习 3 机器学习 基石 笔记 12 机器 
可以 怎样 学习 4 机器学习 基石 笔记 13 机器 可以 
怎样 学 得 更好 1 机器学习 基石 笔记 14 机器 
可以 怎样 学 得 更好 2 机器学习 基石 笔记 15 
机器 可以 怎样 学 得 更好 3 机器学习 基石 笔记 
16 机器 可以 怎样 学 得 更好 4 十 Logistic 
Regression 罗 杰斯特 回归 最常 见到 的 翻译 Logistic 回归 
10.1 Logistic Regression ProblemLogistic 回归 问题 使用 二元 分类 分析 
心脏病 复发 问题 其 输出 空间 只含 有 两项 { 
+ 1 1 } 分别 表示 复发 和 不发 复发 
在 含有 噪音 的 情况 下 目标函数 f 可以 使用 
目标 分布 P 来 表示 如 公式 10 1 所示 
此 情形 的 机器学习 流程图 如 1 所示 公式 10 
1 1 心脏病 复发 二元 分类 流程图 但是 通常 情况下 
不会 确定 的 告知 患者 心脏病 一定 会 复发 或者 
一定 不会 而是 以 概率 的 方式 告知 患者 复发 
的 可能性 如 2 所示 一位 患者 心脏病 复发 的 
可能性 为 80% 2 以 概率 的 形式 表示 复发 
可能性 此种 情况 被 称为 软 二元 分类 soft binary 
classification 目标函数 f 的 表达 如 公式 10 2 所示 
其 输出 以 概率 的 形式 因此在 0 ~ 1 
之间 公式 10 2 面对 如 公式 10 2 的 
目标 函数 理想 的 数据 集 D 输入 加 输出 
空间 应如/nr 3 所示 3 理想 的 数据 集 D 
所有 的 输出 都以 概率 的 形式 存在 如 用 
心脏病 复发 的 例子 来 说明 一般 病人 只有 心脏病发 
与 没 复发 两种 情况 而 不 可能 在 病历 
中 记录 他 曾经 的 病发 概率 现实 中 的 
训练 数据 应如/nr 4 所示 4 实际 训练 数据 可以 
将 实际 训练 数据 看做 含有 噪音 的 理想 训练 
数据 问题 是 如何 使用 这些 实际 的 训练 数据 
以 解决 软 二元 分类 的 问题 即 假设 函数 
如何 设计 首先 回忆 在 之前 的 几章 内容 中 
提到 的 两种 假设 函数 二元 分类 和 线性 回归 
中都 具 有的是 哪 部分 答案 是 求 输入 各 
属性 的 加权 总 分数 score 还记得 第二章 中用 成绩 
分数 来 说明 加权 求和 的 意义 吗 可以 使用 
公式 10 3 表示 公式 10 3 如何 把 该 
得分 从 在整个 实数 范围内 转换 成为 一个 0 ~ 
1 之间 的 值 呢 此处 就 引出 了 本章 
的 主题 logistic 函数 logistic function 用 表示 分数 s 
越大 风险 越高 分数 s 越小 风险 越低 假设 函数 
h 如 公式 10 4 所示 函数 曲线 的 示意图 
如 5 所示 公式 10 4 5 logistic 函数 的 
示意图 具体 的 logistic 函数 的 数学 表达式 如 公式 
10 5 所示 公式 10 5 代入 几个 特殊 的 
数值 检验 是否 能将/nr 整个 实数 集上 的 得分 映 
射到 0 ~ 1 之间 代入 负无穷 得 代入 0 
得 代入 正无穷 得 logistic 函数 完美 的 将 整个 
实数 集上 的 值 映射 到了 0 ~ 1 区间 
上 观察 函数 的 图形 该 函数 是 一个 平滑 
处处 可 微分 单调 monotonic 的 形 sigmoid 函数 因此 
又 被 称为 sigmoid 函数 通过 logistic 函数 的 数学 
表达式 重写 软 二元 分类 的 假设 函数 表达 如 
公式 10 6 所示 公式 10 6 10.2 Logistic Regression 
ErrorLogistic 回归 错误 将 logisitic 回归 与 之前 学习 的 
二元 分类 和 线性 回归 做 一 对比 如 7 
所示 7 二元 分类 线性 回归 与 logistic 回归 的 
对比 其中 分数 s 是 在 每个 假设 函数 中 
都会 出现 的 前 两个 学习 模型 的 错误 衡量 
分别 对应 着 0/1 错误 和 平方 错误 而 logistic 
回归 所 使用 的 err 函数 应 如何 表示 则是 
本节 要 介绍 的 内容 从 logistic 回归 的 目标 
函数 可以 推 导出 公式 10 7 成立 公式 10 
7 其中 花括号 上半部分 不难理解 是 将 目标函数 等式 左右 
对调 的 结果 而下 半 部分 的 推导 也 很简单 
因为 + 1 与 1 的 几率 相加 需要 等于 
1 假设 存在 一个 数据集 则 通过 目标函数 产生 此种 
数据集 样本 的 概率 可以 用 公式 10 8 表示 
公式 10 8 就是 各 输入 样本 产生 对应 输出 
标记 概率 的 连乘 而从 公式 10 7 可知 公式 
10 8 可以 写成 公式 10 9 的 形式 公式 
10 9 但是 函数 f 是 未知 的 已知 的 
只有 假设 函数 h 可不 可以 将 假设 函数 h 
取代 公式 10 9 中的 f 呢 如果 这样 做 
意味着 什么 意味着 假设 函数 h 产生 同样 数据集 样本 
D 的 可能性 多大 在 数学 上 又 翻译成 似 
然 likelihood 替代 之后 的 公式 如 公式 10 10 
所示 公式 10 10 假设 假设 函数 h 和 未知 
函数 f 很 接近 即 err 很小 那么 h 产生 
数据 样本 D 的 可能性 或 叫 似 然 likelihood 
和f/nr 产生 同样 数据 D 的 可能性 probability 也很 接近 
函数 f 既然 产生 了 数据 样本 D 那么 可以 
认为 函数 f 产生 该 数据 样本 D 的 可能性 
很大 因此 可以 推断 出 最好 的 假设 函数 g 
应该 是 似 然 最大 的 假设 函数 h 用 
公式 10 11 表示 公式 10 11 在当 假设 函数 
h 使用 公式 10 6 的 logistic 函数 可以 得到 
如 公式 10 12 的 特殊 性质 公式 10 12 
因此 公式 10 10 可以 写成 公式 10 13 此处 
注意 计算 最大 的 时 所有 的 对 大小 没有 
影响 因为 所有 的 假设 函数 都会 乘以 同样 的 
即 h 的 似 然 只 与 函数 h 对 
每个 样本 的 连乘 有关 如 公式 10 14 公式 
10 14 其中 表示 标记 将 标记 代替 正负号 放进 
假设 函数 中 使得 整个 式子 更加 简洁 寻找 的 
是 似 然 最大 的 假设 函数 h 因此 可以 
将 公式 10 14 代入 寻找 最大 似 然 的 
公式 中 并 通过 一连串 的 转换 得到 公式 10 
15 假设 函数 h 与 加 权向量 w 一一对应 连乘 
公式 不容易 求解 最大 问题 因此 求其 对数 此 处以 
自然对数 e 为 底 之前 都是 在 求 最 小问题 
因此 将 最大 问题 加上 一个 负号 转 成了 最 
小问题 为了 与 以前 的 错误 衡量 类似 多 成了/nr 
一个 将 代入 表达式 得出 上述 结果 公式 10 15 
公式 10 15中 这个 错误 函数 称作 交叉 熵 错误 
cross entropy error 10.3 Gradient of Logistic Regression ErrorLogistic 回归 
错误 的 梯度 推导 出 logistic 回归 的 下 一步 
的 工作 是 寻找 使得 最小 的 权值 向量 w 
的 表达 如 公式 10 16 所示 公式 10 16 
仔细 的 观察 该 公式 可以 得出 该 函数 为 
连续 continuous 可微 differentiable 的 凸函数 因此 其 最小值 在 
梯度 为零 时 取得 即 那 如何 求解 呢 即 
为对 权值 向量 w 的 各个 分量 求 偏微分 对 
这种 复杂 公式 求解 偏微分 可以 使用 微分 中的 连锁 
律 将 公式 10 16中 复杂 的 表示 方式 用 
临时 符号 表示 为了 强调 符号 的 临时性 不 使用 
字母 表示 而是 使用 和 具体 如 公式 10 17 
公式 10 17 对 权值 向量 w 的 单个 分量 
求 偏微分 过程 如 公式 10 18 所示 公式 10 
18 其中 函数 为 10.1节 中 介绍 的 logistic 函数 
而 求 梯度 的 公式 可以 写成 公式 10 19 
所示 公式 10 19 求出 的 梯度 后 由于 为 
凸函数 令 为零 求出 的 权值 向量 w 即使 函数 
取得 最小 的 w 观察 发现该 函数 是 一个 函数 
作为 权值 关于 的 加权 求和 函数 假设 一种 特殊 
情况 函数 的 所有 权值 为零 即 所有 都 为零 
可以 得出 趋于 负无穷 即 也 意味着 所有 的 都与 
对应 的 同号 即 线性 可分 排除 这种 特殊 情况 
当 加权 求和 为 零时 求/v 该/r 问题/n 的/uj 解/v 
不能/v 使用/v 类似/v 求解/v 线性/n 回归/v 时/n 使用/v 的/uj 闭式/n 
解的/nr 求解/v 方式/n 此 最小值 又该 如何 计算 还记得 最早 
使用 的 PLA 的 求解 方式 吗 迭代 求解 可以 
将 PLA 的 求解 步骤 合并 成如/nr 公式 10 20 
的 形式 公式 10 20 时 向量 不变 时 加上 
将 使用 一些 符号 将该 公式 更 一般化 的 表示 
如 公式 10 21 所示 公式 10 21 其中 多 
乘以 一个 1 用 表示 表示 更新 的 步长 PLA 
中 更新 的 部分 用 v 来 代表 表示 更新 
的 方向 而 这类 算法 被称为 迭代 优化 方法 iterative 
optimization approach 10.4 Gradient Descent 梯度 下降 Logistic 回归 求解 
最小 也 使用 上节 中 提到 的 迭代 优化 方法 
通过 一步 一步 改变 权值 向量 寻找 使得 最小 的 
变 权值 向量 迭代 优化 方法 的 更新 公式 如 
公式 10 22 所示 公式 10 22 针对 logistic 回归 
个 问题 如何 设计 该 公式 中的 参数 和是/nr 本节 
主要 解决 的 问题 回忆 PLA 其中 参数 来自 于 
修正 错误 观察 logistic 回归 的 针对 其 特性 设计 
一种 能够 快速 寻找 最佳 权值 向量 的 方法 如 
8 为 logistic 回归 的 关于 权值 向量 w 的 
示意图 为 一个 平滑 可微 的 凸函数 其中 图像 谷底 
的 点 对应 着 最佳 w 使得 最小 如何 选择 
参数 和 可以 使得 更新 公式 快速 到达 该点 8 
logistic 回归 的 示意图 为了 分工 明确 设 作为 单位向量 
仅代表 方向 代表 步长 表示 每次 更新 改变 的 大小 
在 固定 的 情况 下 如何 选择 的 方向 保证 
更新 速度 最快 是 按照 最 陡峭 的 方向 更改 
即在 固定 的 情况 下 最快 的 速度 有 指导 
方向 找出 使得 最小 的 w 如 公式 10 23 
所示 公式 10 23 以上 是 非线性 带 约束 的 
公式 寻找 最小 w 仍然 非常 困难 考虑 将其 转换 
成 一个 近似 的 公式 通过 寻找 近似 公式 中 
最小 w 达到 寻找 原 公式 最小 w 的 目的 
此处 使用 到 泰勒 展开 Taylor expansion 回忆 一维空间 下 
的 泰勒公式 如 公式 10 24 所示 公式 10 24 
同理 在 很 小时 将 公式 10 23 写成 多维 
泰勒 展开 的 形式 如 公式 10 25 所示 公式 
10 25 其中 相当于 公式 10 24 中的 相当于 通俗 
点 解释 将 原 的 曲线 的 形式 看做 一 
小段 一 小段 的 线段 的 形式 即 的 曲线 
可以 看做 周围 一段 很小 的 线段 因此 求解 公式 
10 26 最小 情况下 的 w 可以 认为 是 近似 
的 求解 公式 10 23 最小 状况 下 的 w 
公式 10 26 该 公式 中 是 已知 值 而为 
给定 的 大于 零 的 值 因此 求 公式 10 
26 最小 的 问题 又 可 转换 为求 公式 10 
27 最小 的 问题 公式 10 27 两个 向量 最小 
的 情况 为其 方向 相反 即 乘积 为 负值 又因 
是 单位向量 因此 方向 如 公式 10 28 所示 公式 
10 27 在 很小 的 情况 下 将 公式 10 
27 代入 公式 10 22 得 公式 10 28 具体 
的 更新 公式 公式 10 27 该 更新 公式 表示 
权值 向量 w 每次 向着 梯度 的 反方向 移动 一 
小步 按照 此 种 方式 更新 可以 尽快 速度 找到 
使得 最小 的 w 此种 方式 称作 梯度 下降 gradient 
descent 简写 为 GD 该 方法 是 一种 常用 且 
简单 的 方法 讲完 了 参数 v 的 选择 再 
回头 观察 事先 给定 的 参数 的 取值 对 梯度 
下降 的 影响 如 9 所示 9 参数 的 大小 
对 梯度 下降 的 影响 如 9 最左 太小 时 
下降 速度 很慢 因此 寻找 最优 w 的 速度 很慢 
9 中间 当 太大 时 下降 不 稳定 甚至 可能 
出现 越 下降 越高 的 情况 合适 的 应为 随着 
梯度 的 减 小而 减小 如图 最右 所示 即 参数 
是 可变 的 且 与 梯度 大小 成正比 根据 与 
梯度 大小 成正比 的 条件 可以 将 重新 给定 新的 
如 公式 10 28 所示 公式 10 28 最终 公式 
10 27 可 写成 公式 10 29 公式 10 29 
此时 的 被称作 固定 的 学习 速率 fixed learning rate 
公式 10 29 即 固定 学习 速率 下 的 梯度 
下降 Logistic 回归 算法 的 步骤 如下 设置 权值 向量 
w 初始值 为 设 迭代 次数 为 t 计算 梯度 
对 权值 向量 w 进行 更新 直到 或者 迭代 次数 
足够 多 