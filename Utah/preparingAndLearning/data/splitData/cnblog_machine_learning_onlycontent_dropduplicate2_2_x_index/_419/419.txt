在之前写的上百篇机器学习博客中，不时会使用矩阵向量求导的方法来简化公式推演，但是并没有系统性的进行过讲解，因此让很多朋友迷惑矩阵向量求导的具体过程为什么会是这样的。这里准备用几篇博文来讨论下机器学习中的矩阵向量求导，今天是第一篇。
本系列主要参考文献为维基百科的Matrix Caculas和张贤达的《矩阵分析与应用》。
1. 矩阵向量求导引入
在高等数学里面，我们已经学过了标量对标量的求导，比如标量$y$对标量$x$的求导，可以表示为$\frac{\partial y}{\partial x}$。
有些时候，我们会有一组标量$y_i,i=1,2,...,m$来对一个标量$x$的求导,那么我们会得到一组标量求导的结果：$$\frac{\partial y_i}{\partial x}, i=1,2.,,,m$$
如果我们把这组标量写成向量的形式，即得到维度为m的一个向量$\mathbf{y}$对一个标量$x$的求导，那么结果也是一个m维的向量：$\frac{\partial \mathbf{y}}{\partial x}$
可见，所谓向量对标量的求导，其实就是向量里的每个分量分别对标量求导，最后把求导的结果排列在一起，按一个向量表示而已。类似的结论也存在于标量对向量的求导，向量对向量的求导，向量对矩阵的求导，矩阵对向量的求导，以及矩阵对矩阵的求导等。
总而言之，所谓的向量矩阵求导本质上就是多元函数求导，仅仅是把把函数的自变量，因变量以及标量求导的结果排列成了向量矩阵的形式，方便表达与计算，更加简洁而已。
为了便于描述，后面如果没有指明，则求导的自变量用$x$表示标量，$\mathbf{x}$表示n维向量，$\mathbf{X}$表示$m \times n$维度的矩阵，求导的因变量用$y$表示标量，$\mathbf{y}$表示m维向量，$\mathbf{Y}$表示$p \times q$维度的矩阵。
2. 矩阵向量求导定义
根据求导的自变量和因变量是标量，向量还是矩阵，我们有9种可能的矩阵求导定义，如下：
自变量\因变量
标量$y$
向量$\mathbf{y}$
矩阵$\mathbf{Y}$
标量$x$
$\frac{\partial y}{\partial x}$
$\frac{\partial  \mathbf{y}}{\partial x}$
$\frac{\partial  \mathbf{Y}}{\partial x}$
向量$\mathbf{x}$
$\frac{\partial y}{\partial \mathbf{x}}$
$\frac{\partial  \mathbf{y}}{\partial \mathbf{x}}$
$\frac{\partial  \mathbf{Y}}{\partial \mathbf{x}}$
矩阵$\mathbf{X}$
$\frac{\partial y}{\partial \mathbf{X}}$
$\frac{\partial  \mathbf{y}}{\partial \mathbf{X}}$
$\frac{\partial  \mathbf{Y}}{\partial \mathbf{X}}$
这9种里面，标量对标量的求导高数里面就有，不需要我们单独讨论，在剩下的8种情况里面，我们先讨论上图中标量对向量或矩阵求导，向量或矩阵对标量求导，以及向量对向量求导这5种情况。另外三种向量对矩阵的求导，矩阵对向量的求导，以及矩阵对矩阵的求导我们在后面再讲。
现在我们回看第一节讲到的例子，维度为m的一个向量$\mathbf{y}$对一个标量$x$的求导，那么结果也是一个m维的向量：$\frac{\partial \mathbf{y}}{\partial x}$。这是我们表格里面向量对标量求导的情况。这里有一个问题没有讲到，就是这个m维的求导结果排列成的m维向量到底应该是列向量还是行向量？
这个问题的答案是：行向量或者列向量皆可！毕竟我们求导的本质只是把标量求导的结果排列起来，至于是按行排列还是按列排列都是可以的。但是这样也有问题，在我们机器学习算法法优化过程中，如果行向量或者列向量随便写，那么结果就不唯一，乱套了。
为了解决这个问题，我们引入求导布局的概念。
3. 矩阵向量求导布局
为了解决矩阵向量求导的结果不唯一，我们引入求导布局。最基本的求导布局有两个：分子布局(numerator layout)和分母布局(denominator layout )。
对于分子布局来说，我们求导结果的维度以分子为主，比如对于我们上面对标量求导的例子，结果的维度和分子的维度是一致的。也就是说，如果向量$\mathbf{y}$是一个m维的列向量，那么求导结果$\frac{\partial \mathbf{y}}{\partial x}$也是一个m维列向量。如果如果向量$\mathbf{y}$是一个m维的行向量，那么求导结果$\frac{\partial \mathbf{y}}{\partial x}$也是一个m维行向量。
对于分母布局来说，我们求导结果的维度以分母为主，比如对于我们上面对标量求导的例子，如果向量$\mathbf{y}$是一个m维的列向量，那么求导结果$\frac{\partial \mathbf{y}}{\partial x}$是一个m维行向量。如果如果向量$\mathbf{y}$是一个m维的行向量，那么求导结果$\frac{\partial \mathbf{y}}{\partial x}$是一个m维的列向量向量。
可见，对于分子布局和分母布局的结果来说，两者相差一个转置。
再举一个例子，标量$y$对矩阵$ \mathbf{X}$求导，那么如果按分母布局，则求导结果的维度和矩阵$X$的维度$m \times n$是一致的。如果是分子布局，则求导结果的维度为$n \times m$。
这样，对于标量对向量或者矩阵求导，向量或者矩阵对标量求导这4种情况，对应的分子布局和分母布局的排列方式已经确定了。
稍微麻烦点的是向量对向量的求导，本文只讨论列向量对列向量的求导，其他的行向量求导只是差一个转置而已。比如m维列向量$\mathbf{y}$对n维列向量$\mathbf{x}$求导。它的求导结果在分子布局和分母布局各是什么呢？对于这2个向量求导，那么一共有$mn$个标量对标量的求导。求导的结果一般是排列为一个矩阵。如果是分子布局，则矩阵的第一个维度以分子为准，即结果是一个$m \times n$的矩阵，如下：$$ \frac{\partial  \mathbf{y}}{\partial \mathbf{x}} = \left( \begin{array}{ccc} \frac{\partial y_1}{\partial x_1}& \frac{\partial y_1}{\partial x_2}& \ldots & \frac{\partial y_1}{\partial x_n}\\  \frac{\partial y_2}{\partial x_1}& \frac{\partial y_2}{\partial x_2} & \ldots & \frac{\partial y_2}{\partial x_n}\\   \vdots&  \vdots &  \ddots & \vdots \\ \frac{\partial y_m}{\partial x_1}& \frac{\partial y_m}{\partial x_2} & \ldots & \frac{\partial y_m}{\partial x_n}  \end{array} \right)$$
上边这个按分子布局的向量对向量求导的结果矩阵，我们一般叫做雅克比 (Jacobian)矩阵。有的资料上会使用$ \frac{\partial  \mathbf{y}}{\partial \mathbf{x^T}}$来定义雅克比矩阵，意义是一样的。
如果是按分母布局，则求导的结果矩阵的第一维度会以分母为准，即结果是一个$n \times m$的矩阵，如下：$$ \frac{\partial  \mathbf{y}}{\partial \mathbf{x}} = \left( \begin{array}{ccc} \frac{\partial y_1}{\partial x_1}& \frac{\partial y_2}{\partial x_1}& \ldots & \frac{\partial y_m}{\partial x_1}\\  \frac{\partial y_1}{\partial x_2}& \frac{\partial y_2}{\partial x_2} & \ldots & \frac{\partial y_m}{\partial x_2}\\   \vdots&  \vdots &  \ddots & \vdots \\ \frac{\partial y_1}{\partial x_n}& \frac{\partial y_2}{\partial x_n} & \ldots & \frac{\partial y_m}{\partial x_n}  \end{array} \right)$$
上边这个按分母布局的向量对向量求导的结果矩阵，我们一般叫做梯度矩阵。有的资料上会使用$ \frac{\partial  \mathbf{y^T}}{\partial \mathbf{x}}$来定义梯度矩阵，意义是一样的。
有了布局的概念，我们对于上面5种求导类型，可以各选择一种布局来求导。但是对于某一种求导类型，不能同时使用分子布局和分母布局求导。
但是在机器学习算法原理的资料推导里，我们并没有看到说正在使用什么布局，也就是说布局被隐含了，这就需要自己去推演，比较麻烦。但是一般来说我们会使用一种叫混合布局的思路，即如果是向量或者矩阵对标量求导，则使用分子布局为准，如果是标量对向量或者矩阵求导，则以分母布局为准。对于向量对对向量求导，有些分歧，我的所有文章中会以分子布局的雅克比矩阵为主。
具体总结如下：
自变量\因变量
标量$y$
列向量$\mathbf{y}$
矩阵$\mathbf{Y}$
标量$x$
/
$\frac{\partial  \mathbf{y}}{\partial x}$
分子布局：m维列向量（默认布局）
分母布局：m维行向量
$\frac{\partial  \mathbf{Y}}{\partial x}$
分子布局：$p \times q$矩阵（默认布局）
分母布局：$q \times p$矩阵
列向量$\mathbf{x}$
$\frac{\partial y}{\partial \mathbf{x}}$
分子布局：n维行向量
分母布局：n维列向量（默认布局）
$\frac{\partial  \mathbf{y}}{\partial \mathbf{x}}$
分子布局：$m \times n$雅克比矩阵（默认布局）
分母布局：$n \times m$梯度矩阵
/
矩阵$\mathbf{X}$
$\frac{\partial y}{\partial \mathbf{X}}$
分子布局：$n \times m$矩阵
分母布局：$m \times n$矩阵（默认布局）
/
/
4. 矩阵向量求导基础总结
有了矩阵向量求导的定义和默认布局，我们后续就可以对上表中的5种矩阵向量求导过程进行一些常见的求导推导总结求导方法，并讨论向量求导的链式法则。
（欢迎转载，转载请注明出处。欢迎沟通交流： liujianping-ok@163.com）