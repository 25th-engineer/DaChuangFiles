目录 导数 偏/a 导数/n 和/c 方向/n 导数/n 方向/n 导数/n 的/uj 
推导/v 过程/n 方向/n 导数/n 和/c 梯度/n References/w 相关/v 博客/nr 最近/f 
学习/v 最优化 导论 遇到 了 方向 导数 这一 概念 故 
对其 及 相关 概念 进行 一遍 梳理 并 给出 方向 
导数 的 推导 过程 导数 偏 导数 和 方向 导数 
在 一元 可 导函数 \ y = f x \ 
中 导数 \ f x _ 0 \ 即 是曲 
线上 \ x = x _ 0 \ 处 的 
斜率 按照 定义 求 导数 \ f x = \ 
lim _ { \ Delta x \ to 0 } 
\ frac { f x + \ Delta x f 
x } { \ Delta x } \ tag { 
1 } \ 当然 我们 也 可以 通过 各种 求导 
法则 来 计算 导数 对 一个 \ R ^ m 
\ to R \ 的 多元 可 导函数 \ y 
= f \ bm x \ bm x = x 
_ 1 x _ 2 . . . x _ 
m ^ \ top \ 我们 能够 求 的 导数 
就多 如 偏 导数 方向 导数 但 归根到底 这些 导数 
都 可以 认为 是 曲面 上 一点 在 某个 方向 
的 斜率 对于 \ m \ le 2 \ 的 
情况 我们 还 能够 通过 坐标系 很 直观 地 了解 
当 \ m 2 \ 时 我们 可以 从 向量空间 
的 角度 理解 偏 导数 是 指 \ y = 
f \ bm x \ 对 \ \ bm x 
= x _ 1 x _ 2 . . . 
x _ m ^ \ top \ 中的 某 一维 
进行 求导 如 下式 2 所示 对 第 \ i 
\ 维 求 偏 导数 \ \ begin { split 
} \ frac { \ partial f \ bm x 
} { \ partial x _ i } & = 
\ frac { \ partial f x _ 1 x 
_ 2 . . . x _ i . . 
. x _ m } { \ partial x _ 
i } \ \ & = \ lim _ { 
\ Delta x _ i \ to 0 } \ 
frac { f x _ 1 x _ 2 . 
. . x _ i + \ Delta x _ 
i . . . x _ m f x _ 
1 x _ 2 . . . x _ i 
. . . x _ m } { \ Delta 
x _ i } \ end { split } \ 
tag { 2 } \ 方向 导数 就 更好 理解 
了 \ y = f \ bm x \ 对 
\ \ bm x = x _ 1 x _ 
2 . . . x _ m ^ \ top 
\ 构成 的 向量空间 \ R ^ m \ 中 
某一 方向 \ \ bm d = \ Delta x 
_ 1 \ Delta x _ 2 . . . 
\ Delta x _ m ^ \ top \ 求 
导数 即 得到 该 方向 上 的 方向 导数 \ 
\ frac { \ partial f \ bm x } 
{ \ partial \ bm d } \ 如 式 
3 所示 \ \ begin { split } \ frac 
{ \ partial f \ bm x } { \ 
partial \ bm d } & = \ frac { 
\ partial f x _ 1 x _ 2 . 
. . x _ m } { \ partial x 
_ i } \ \ & = \ lim _ 
{ \ rho \ to 0 } \ frac { 
f x _ 1 + \ Delta x _ 1 
x _ 2 + \ Delta x _ 2 . 
. . x _ m + \ Delta x _ 
m f x _ 1 x _ 2 . . 
. x _ m } { \ rho } \ 
\ & \ rho = \ sqrt { \ Delta 
x _ 1 ^ 2 + \ Delta x _ 
2 ^ 2 + \ cdots + \ Delta x 
_ m ^ 2 } \ end { split } 
\ tag { 3 } \ 方向 导数 和偏/nr 导数 
是 什么 关系 对于 多元 可 导函数 \ y = 
f \ bm x \ bm x = x _ 
1 x _ 2 . . . x _ m 
^ \ top \ 在其 上任 一点 \ \ bm 
x _ i \ 我们 都 可以 在 向量空间 \ 
R ^ m \ 中的 每 一个 方向 都 可以 
计算 一个 方向 导数 也 就是 超平面 上点 \ \ 
bm x _ i \ 在 每 一个 方向 切线 
的 斜率 这里 每 一个 方向 自然 包括 各个 偏 
导数 的 方向 即 偏 导数 构成 的 集合 A 
是 方向 导数 构成 集合 B 的 子集 方向 导数 
的 推导 过程 \ f \ boldsymbol x \ 是 
一个 \ R ^ m \ to R \ 的 
函数 如果 我们 要求 \ f \ boldsymbol x \ 
在任 一点 \ \ boldsymbol x _ 0 = x 
_ 1 ^ { 0 } x _ 2 ^ 
{ 0 } . . . x _ m ^ 
{ 0 } ^ \ top \ 点 方向 为 
\ \ boldsymbol d \ 的 方向 导数 那么 按照 
定义 我们 得到 如下 公式 \ \ frac { \ 
partial f \ boldsymbol x } { \ partial \ 
boldsymbol d } \ mid _ { \ boldsymbol x 
= \ boldsymbol x _ 0 } = \ lim 
_ { \ alpha \ to 0 } \ frac 
{ f \ boldsymbol x _ 0 + \ alpha 
\ boldsymbol d f \ boldsymbol x _ 0 } 
{ \ alpha } \ tag { 4 } \ 
式 4 中 \ \ boldsymbol d \ 为 单位向量 
公式 4 其实 是 公式 3 的 向量 形式 plus 
公式 3 中 \ d \ 不是 单位向量 故 加上 
\ \ 来 区分 设 \ g \ alpha = 
f x _ 0 + \ alpha \ boldsymbol d 
\ 我们 注意到 \ g 0 = f x _ 
0 \ 所以 式 4 又 可以 写 为 \ 
\ begin { split } \ frac { \ partial 
f \ boldsymbol x } { \ partial \ boldsymbol 
d } \ mid _ { \ boldsymbol x = 
\ boldsymbol x _ 0 } & = \ lim 
_ { \ alpha \ to 0 } \ frac 
{ g \ alpha g 0 } { \ alpha 
} \ \ & = \ frac { d g 
\ alpha } { d \ alpha } \ mid 
_ { \ alpha = 0 } \ \ & 
= \ frac { d f \ boldsymbol x _ 
0 + \ alpha \ boldsymbol d } { d 
\ alpha } | _ { \ alpha = 0 
} \ \ & = \ nabla f \ boldsymbol 
x _ 0 ^ \ top \ boldsymbol d \ 
\ & = \ nabla f \ boldsymbol x _ 
0 \ boldsymbol d \ \ & = \ boldsymbol 
d ^ \ top \ nabla f \ boldsymbol x 
_ 0 \ end { split } \ tag { 
5 } \ 所以 \ \ frac { \ partial 
f \ boldsymbol x } { \ partial \ boldsymbol 
d } = \ boldsymbol d ^ \ top \ 
nabla f \ boldsymbol x \ tag { 6 } 
\ 方向 导数 和 梯度 首先 明确 导数 是 一个 
值 代表 切线 的 斜率 而 梯度 是 一个 向量 
最 大方向 导数 的 方向 就是 梯度 代表 的 方向 
梯度 是 \ f \ bm x \ 对 各个 
自变量 \ \ bm x = x _ 1 x 
_ 2 . . . x _ m ^ \ 
top \ 每 一维 分别 求 偏 导数 得到 的 
向量 从式/nr 5 和 6 中 我们 也 可以 知道 
当 \ \ bm d = \ frac { \ 
nabla f \ bm x } { \ | \ 
nabla f \ bm x \ | } \ 方向 
导数 最大 最 大方向 导数 的 方向 就是 梯度 最大 
的 方向 导数 就是 梯度 的 欧几里德 范数 References 如何 
直观 形象 的 理解 方向 导数 与 梯度 以及 它们 
之间 的 关系 马 同学 方向 导数 与 梯度 学习 
笔记 Reclusiveman 机器学习 ML 重要 概念 梯度 Gradient 与 梯度 
下 降法 Gradient Descent WangBo _ NLPREdwin K . P 
. Chong Stanislaw H . Zak An Introduction to Optimization 
4th Edition 相关 博客 机器学习 之 数学 01 导数 偏 
导数 方向 导数 梯度 机器学习 之 数学 02 梯度 下 
降法 最速 下 降法 牛顿 法 共轭 方向 法 拟 
牛顿 法 机器学习 之 数学 03 有 约束 的 非线性 
优化 问题 拉格朗日 乘子 法 KKT 条件 投影 法 