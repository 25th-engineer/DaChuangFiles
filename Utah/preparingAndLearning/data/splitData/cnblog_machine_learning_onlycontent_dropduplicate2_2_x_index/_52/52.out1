1 介绍 决策树 decision tree 是 一种 有 监督 的 
机器学习 算法 是 一个 分类 算法 在 给定 训练 集 
的 条件 下 生成 一个 自 顶 而下 的 决策树 
树 的 根 为 起点 树 的 叶子 为 样本 
的 分类 从根到/nr 叶子 的 路径 就是 一个 样本 进行 
分类 的 过程 下图 为 一个 决策树 的 例子 见 
http / / zh . wikipedia . org / wiki 
/ % E 5% 86% B 3% E 7% AD 
% 96% E 6% A 0% 91 可见 决策 树上 
的 判断 节点 是 对 某一个 属性 进行 判断 生成 
的 路径 数量 为 该 属性 可能 的 取值 最终 
到 叶子 节点 时 就 完成 一个 分类 或 预测 
决策树 具有 直观 易于 解释 的 特性 2 决策树 生成 
算法 本文 主要 讨论 如何 由 一个 给定 的 训练 
集 生成 一个 决策树 如果 都 一个 数据 集合 $ 
D $ 其 特征 集 合为 $ A $ 那么 
以 何种 顺序 对 A 中的 特征 进行 判断 就 
成为 决策树 生成 过程 中 的 关键 首先 给 出 
一个 决策树 生成 算法 ID3 算法 参考 统计 学习 方法 
李航 著 我 是 算法 开始 分割线 ID3 算法 输入 
训练 数据集 D 特 征集 A 阈值 e 输出 决策树 
T 1 若 D 中 所有 样本 属于 同 一类 
Ck 则 T 为 单 节点 树 并将 类 Ck 
作为 该 节点 的 类 标记 返回 T 2 A 
为 空集 T 为 单 节点 树 将 D 中实 
例数 最大 的 类 Ck 作为 该 节点 的 类 
标记 返回 T 3 否则 计算 A 中 各 特征 
对 D 的 信息 增益 选择 信息 增益 最大 的 
特征值 Ag 4 如果 Ag e 则 置 T 为 
单 节点 树 将 D 中实 例数 最大 的 类 
Ck 作为 该 节点 的 类 标记 返回 T 5 
否则 对 Ag 的 每一个 可能 的 取值 ai 依 
Ag = ai 将 D 分割 为 若干 非空 子集 
Di 将 Di 中实 例数 最大 的 类 作为 标记 
构建 子 节点 由 节点 及其 子 节点 构 成树 
T 返回 T 6 对 第 i 个子 节点 以 
Di 为 训练 集 以 A { Ag } 为特征 
集 递归调用 1 ~ 5 步 得到 子树 Ti 返回 
Ti 我 是 算法 结束 分割线 算法 第 3 步中/nr 
信息 增益 是 评估 每一个 特征值 对 D 的 划分 
效果 划分 的 原则 为 将 无序 的 数据 变得 
尽量 有序 评价 随机变量 不确定性 的 一个 概念 是 熵 
熵 越大 不确定性 越大 如果 确定 一个 特征 Ag 在 
确定 该 特征 前后 D 的 熵 的 变化 值 
就是 特征 Ag 的 信息 增益 3 熵 及 信息 
增益 熵 设 X 是 一个 取 有限 个 值 
n 的 离散 随机变量 其 概率分布 为 \ P X 
= x _ { i } = P _ { 
i } i = 1 2 . . . n 
\ 则 随机变量 X 的 熵 定义 为 \ H 
x =   \ sum \ limits _ { i 
= 1 } ^ n { { P _ i 
} \ log { P _ i } } \ 
信息 增益 训练 集为 \ D \ \ | D 
| \ 为 样本容量 设有 k 个 类 \ { 
C _ k } \ k = 1 . . 
. k   \ { | C _ k | 
} \ 为 类 \ { C _ k } 
\ 的 样本 个数 且有 \ \ sum \ limits 
_ { i = 1 } ^ k { | 
{ C _ k } | }   = | 
D | \ 设 特征 A 有n个/nr 不同 取值 \ 
\ { { a _ { 1 } } { 
a _ 2 } \ cdots { a _ n 
} \ } \   根据 A 的 值 将 
D 划分为 n 个 子集 \ { D _ 1 
} { D _ 2 } \ cdots { D 
_ n } \   \ { | D _ 
i | } \ 为 \ { D _ i 
} \   的 样本 数 \ \ sum \ 
limits _ { i = 1 } ^ n { 
| { D _ i } | }   = 
| D | \ 记 子集 \ { D _ 
i } \ 中 属于 类 \ { C _ 
k } \ 的 样本 集 合为 \ { D 
_ { ik } } \ 即 \ { D 
_ { ik } } = { D _ i 
} \ cap { C _ k } \ \ 
{ | D _ { ik } | } \ 
为 \ { D _ { ik } } \ 
的 样本 个数 1 数据集 D 的 经验 熵 H 
D \ H D =   \ sum \ limits 
_ { k = 1 } ^ K { \ 
frac { { | { C _ k } | 
} } { { | D | } } { 
{ \ log } _ 2 } } \ frac 
{ { | { C _ k } | } 
} { { | D | } } \ 2 
特征 A 对 数据集 D 的 经验 条件 熵 H 
D | A \ H D | A = \ 
sum \ limits _ { i = 1 } ^ 
n { \ frac { { | { D _ 
i } | } } { { | D | 
} } H { D _ i } =   
} \ sum \ limits _ { i = 1 
} ^ n { \ frac { { | { 
D _ i } | } } { { | 
D | } } \ sum \ limits _ { 
k = 1 } ^ K { \ frac { 
{ | { D _ { ik } } | 
} } { { | { D _ i } 
| } } } } { \ log _ 2 
} \ frac { { | { D _ { 
ik } } | } } { { | { 
D _ i } | } } \ 3 计算 
信息 增益 \ g D A = H D H 
D | A \ 信息 增益 越大 表示 A 对 
D 趋于 有序 的 贡献 越大 分割线 决策树 的 R 
语言 实现 如下 library plyr # 测试 数据集 http / 
/ archive . ics . uci . edu / ml 
/ datasets / Car + Evaluation # # 计算 训练 
集合 D 的 熵 H D # # 输入 trainData 
训练 集 类型 为 数据 框 # #     
  nClass 指明 训练 集中 第 nClass 列为 分类 结果 
# # 输出 训练 集 的 熵 cal _ HD 
function trainData nClass { if is . data . frame 
trainData & is . numeric nClass input error if length 
trainData nClass nClass is larger than the length of trainData 
rownum nrow trainData # 对 第 nClass 列 的 值 
统计 频数 calss . freq count trainData nClass # 计算 
每个 取值 的   概率 * log2 概率 calss . 
freq mutate calss . freq freq2 = freq / rownum 
* log2 freq / rownum sum calss . freq freq2 
# 使用 arrange 代替 order 方便 的 按照 多列 对 
数据 框 进行 排序 # mtcars . new2 arrange mtcars 
cyl vs gear } # cal _ HD mtcars 11 
# # 计算 训练 集合 D 对 特征值 A 的 
条件 熵 H D | A # # 输入 trainData 
训练 集 类型 为 数据 框 # #     
  nClass 指明 训练 集中 第 nClass 列为 分类 结果 
# #       nA 指明 trainData 中 条件 
A 的 列 号 # # 输出 训练 集 trainData 
对 特征 A 的 条件 熵 cal _ HDA function 
trainData nClass nA { rownum nrow trainData # 对 第 
nA 列 的 特征 A 计算 频数 nA . freq 
count trainData nA i 1sub . hd c for nA 
. value in nA . freq 1 { # 取 
特征值 A 取值 为 na . value 的 子集 sub 
. trainData trainData which trainData nA = = nA . 
value sub . hd i cal _ HD sub . 
trainData nClass i i + 1 } nA . freq 
mutate nA . freq freq2 = freq / rownum * 
sub . hd sum nA . freq freq2 } # 
# 计算 训练 集合 D 对 特征值 A 的 信息 
增益 g D A # # 输入 trainData 训练 集 
类型 为 数据 框 # #       nClass 
指明 训练 集中 第 nClass 列为 分类 结果 # # 
      nA 指明 trainData 中 特征 A 的 
列 号 # # 输出 训练 集 trainData 对 特征 
A 的 信息 增益 g _ DA function trainData nClass 
nA { cal _ HD trainData nClass cal _ HDA 
trainData nClass nA } # # 根据 训练 集合 生成 
决策树 # # 输入 trainData 训练 集 类型 为 数据 
框 # #       strRoot 指明 根 节点 
的 属性 名称 # #       strRootAttri 指明 
根 节点 的 属性 取值 # #       
nClass 指明 训练 集中 第 nClass 列为 分类 结果 # 
#       cAttri 向量 表示 当前 可用 的 
特征 集合 用 列 号 表示 # #     
  e 如果 特征 的 最大 信息 增益 小于 e 
则 剩余 作为 一个 分类 类频数 最高 的 最为 分类 
结果 # # 输出 决策树 Tgen _ decision _ tree 
function trainData strRoot strRootAttri nClass cAttri e { # 树 
的 描述 上级 节点 名称 上级 节点 属性值 自己 节点 
名称 自己 节点 的 取值 decision _ tree data . 
frame nClass . freq count trainData nClass   # # 
类别 出现 的 频数 nClass . freq arrange nClass . 
freq desc freq  /i #/i #/i 按/p 频数/n 从/p 低到/i 
高/a 排列/v col/w ./i name/w names trainData # # trainData 
的 列名 # # 1 如果 D 中 所有 属于 
同 一类 Ck 则 T 为 单 节点 树 if 
nrow nClass . freq = = 1 { rbind decision 
_ tree c strRoot strRootAttri nClass . freq 1 1 
return decision _ tree } # # 2 如果 属性 
cAttri 为 空 将 D 中频 数 最高 的 类别 
返回 if length cAttri = = 0 { rbind decision 
_ tree c strRoot strRootAttri nClass . freq 1 1 
return decision _ tree } # # 3 计算 cAttri 
中 各 特征值 对 D 的 信息 增益 选择 信息 
增益 最大 的 特征值 Ag 及其 信息 增益 maxDA 0 
    # 记录 最大 的 信息 增益 maxAttriName   
# 记录 最大 信息 增益 对应 的 属性 名称 maxAttriIndex 
  # 记录 最大 信息 增益 对应 的 属性 列 
号 for i in cAttri { curDA g _ DA 
trainData nClass i if maxDA = curDA { maxDA c 
u r D A m a x A t t 
r i N a m e col . name i 
} } # # 4 如果 最大 信息 增益 小于 
阈值 e 将 D 中频 数 最高 的 类别 返回 
if maxDA e { rbind decision _ tree c strRoot 
strRootAttri nClass . freq 1 1 return decision _ tree 
} # # 5 否则 对 Ag 的 每一 可能 
值 ai 依 Ag = ai 将 D 分割 为 
若干 非空 子集 Di # #   将 Di 中实 
例数 最大 的 类 作为 标记 构建 子 节点 # 
#   由 节点 及其 子 节点 构 成树 T 
返回 Tfor oneValue in unique trainData maxAttriName { sub . 
train trainData which trainData maxAttriName = = oneValue   # 
Di # sub . trian . freq count sub . 
train nClass   # # 类别 出现 的 频数 # 
sub . trian . freq arrange sub . trian . 
freq desc freq  /i #/i #/i 按/p 频数/n 从/p 低到/i 
高/a 排列/v rbind/w decision _ tree c strRoot strRootAttri maxAttriName 
oneValue # # 6 递归 构建 下一步 # 剔除 已经 
使用 的 属性 next . cAttri cAttri which cAttri = 
maxAttriIndex # 递归调用 next . dt gen _ decision _ 
tree sub . train maxAttriName oneValue nClass next . cAttri 
e rbind decision _ tree next . dt } names 
decision _ tree c preName preValue curName curValue decision _ 
tree } 决策树 总结 1 R 中有 实现 决策树 算法 
的 包 rpart 和/c 画出/i 决策树/n 的/uj 包/v rpart/w ./i 
plot/w 本例 自己 实现 决策树 算法 是 为了 更好 的 
理解 2 由于 决策树 只能 处理 离散 属性 因此 连续 
属性 应 首先 进行 离散化 3 决策树 易于 理解 对 
业务 的 解释 性 较强 4 ID3 算法 容易 引起 
过拟合 需 考虑 树 的 剪枝 