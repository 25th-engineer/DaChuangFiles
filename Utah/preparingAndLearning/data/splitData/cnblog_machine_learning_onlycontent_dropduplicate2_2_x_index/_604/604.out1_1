本文 结构 是 什么 有 什么 算法 数学原理 编码 实现 
算法 1 . 是 什么 简单 地 理解 就是 根据 
一些 feature 进行 分类 每个 节点 提 一个 问题 通过 
判断 将 数据 分为 几类 再继续 提问 这些 问题 是 
根据 已 有 数据 学习 出来 的 再 投入 新 
数据 的 时候 就 可以 根据 这棵 树上 的 问题 
将 数据 划分 到 合适 的 叶子 上 2 . 
有 什么 算法 常用 的 几种 决策树 算法 有 ID3 
C 4.5 CART ID3 选择 信息熵 增益 最大 的 feature 
作为 node 实现 对 数据 的 归纳 分类 C 4.5 
是 ID3 的 一个 改进 比 ID3 准确率 高且快/nr 可以 
处理 连续 值 和有 缺失 值 的 feature CART 使用 
基尼指数 的 划分 准则 通过 在 每个 步骤 最大限度 降低 
不纯 洁度 CART 能够 处理 孤立点 以及 能够 对 空缺 
值 进行 处理 3 . 数学原理 ID3 Iterative Dichotomiser 3 
参考 下面 这个 数据集 可以 同时 被 上面 两颗 树 
表示 结果 是 一样 的 而 我们 更 倾向 于 
选择 简单 的 树 那么 怎样 做 才能 使得 学习 
到 的 树 是 最简单 的 呢 下面 是 ID3 
Iterative Dichotomiser 3 的 算法 例如 下面 数据集 哪个 是 
最好 的 Attribute 用 熵 Entropy 来 衡量 E S 
是 数据集 的 熵 i 指 每个 结果 即 No 
Yes 的 概率 E 越大 意味着 信息 越 混乱 我们 
的 目标 是 要让 E 最小 E 在 0 1 
之间 如果 P ＋ 的 概率 在 0.5 此时 E 
最大 这时候 说明 信息 对 我们 没有 明确 的 意义 
对分 类 没有 帮助 但是 我们 不 仅仅 想要 变量 
的 E 最小 还 想要 这 棵树 是 well organized 
所以 用到 Gain 信息 增益 意思 是 如果 我 后面 
要用 这个 变量 的话 它 的 E 会 减少 多少 
例如 下面 的 数据 集 先 计算 四个 feature 的 
熵 E 及其 分支 的 熵 然后 用 Gain 的 
公式 计算 信息 增益 再 选择 Gain 最大 的 特征 
是 outlook 第一层 选择 出来 后 各个 分支 再 继续 
选择 下 一层 计算 Gain 最大 的 例如 分支 sunny 
的 下一层 节点 是 humidity 详细 的 计算 步骤 可以 
参考 这 篇 博文 C 4.5 参考 ID3 有个 局限 
是 对于 有 大量 数据 的 feature 过于 敏感 C 
4.5 是 它 的 一个 改进 通过 选择 最大 的 
信息 增益 率 gain ratio 来 选择 节点 而且 它 
可以 处理 连续 的 和有 缺失 值 的 数据 P 
j / p is the proportion of elements present at 
the position p taking the value of j th test 
. 例如 outlook 作为 第一 层 节 点后 它 有 
3 个 分支 分别 有 5 4 5 条 数据 
则 SplitInfo 5 4 5 = 5 / 14log 5 
14 4 / 14log 4 14 5/14 5 14 其中 
log 5 14 即为 log2 5/14 下面/f 是/v 一个/m 有/v 
连续/a 值/n 和/c 缺失/v 值/n 的/uj 例子/n 连续 值 第一 
步 计算 Gain 除了 连续 值 的 humudity 其他 步骤 
和 前文 一样 要 计算 humudity 的 Gain 的话 先把 
所有 值 升序 排列 { 65 70 70 70 75 
78 80 80 80 85 90 90 95 96 } 
然后 把 重复 的 去掉 { 65 70 75 78 
80 85 90 95 96 } 如下 图 所示 按 
区间 计算 Gain 然后 选择 最大 的 Gain S Humidity 
= 0.102 因为 Gain S Outlook = 0 . 246 
所以 root 还是 outlook 缺失 值 处理 有 缺失 值 
的 数据 时候 用 下图 的 公式 例如 D12 是 
不 知道 的 计算 全集 和 outlook 的 info 其中 
几 个 分支 的 熵 如下 再 计算出 outlook 的 
Gain 比较 一下 ID3 和 C 4.5 的 准确率 和 
时间 accuracy execution time 4 . 编码 实现 算法 代码 
可以 看 机器学习 实战 这本书 和 这篇 博客 完整 代码 
可以 在 github 上 查看 接下来 以 C 4.5 的 
代码 为例 1 . 定义数据 1 def createDataSet 2 dataSet 
= 0 0 0 0 N 3 0 0 0 
1 N 4 1 0 0 0 Y 5 2 
1 0 0 Y 6 2 2 1 0 Y 
7 2 2 1 1 N 8 1 2 1 
1 Y 9 labels = outlook temperature humidity windy 10 
return dataSet labels2 . 计算 熵 1 def calcShannonEnt dataSet 
2 numEntries = len dataSet 3 labelCounts = { } 
4 for featVec in dataSet 5 currentLabel = featVec 1 
6 if currentLabel not in labelCounts . keys 7 labelCounts 
currentLabel = 0 8 labelCounts currentLabel + = 1 # 
数 每 一类 各 多少 个 { Y 4 N 
3 } 9 shannonEnt = 0.0 10 for key in 
labelCounts 11 prob = float labelCounts key / numEntries 12 
shannonEnt = prob * log prob 2 13 return shannonEnt3 
. 选择 最大 的 gain ratio 对应 的 feature 1 
def c h o o s e B e s 
t F e a t u r e T o 
p l i t dataSet 2 numFeatures = len dataSet 
0 1 # feature 个数 3 baseEntropy = calcShannonEnt dataSet 
# 整个 dataset 的 熵 4 b e s t 
I n f o G a i n R a 
t i o = 0.0 5 bestFeature = 1 6 
for i in range numFeatures 7 featList = example i 
for example in dataSet # 每个 feature 的 list 8 
uniqueVals = set featList # 每个 list 的 唯一 值 
集合 9 newEntropy = 0.0 10 splitInfo = 0.0 11 
for value in uniqueVals 12 subDataSet = splitDataSet dataSet i 
value # 每个 唯一 值 对应 的 剩余 feature 的 
组成 子集 13 prob = len subDataSet / float len 
dataSet 14 newEntropy + = prob * calcShannonEnt subDataSet 15 
splitInfo + = prob * log prob 2 16 infoGain 
= baseEntropy newEntropy # 这个 feature 的 infoGain 17 if 
splitInfo = = 0 # fix the overflow bug 18 
continue 19 infoGainRatio = infoGain / splitInfo # 这个 feature 
的 infoGainRatio 20 if infoGainRatio b e s t I 
n f o G a i n R a t 
i o # 选择 最大 的 gain ratio 21 b 
e s t I n f o G a i 
n R a t i o = infoGainRatio 22 bestFeature 
= i # 选择 最大 的 gain ratio 对应 的 
feature 23 return bestFeature4 . 划分 数据 为 下一层 计算 
准备 1 def splitDataSet dataSet axis value 2 retDataSet = 
3 for featVec in dataSet 4 if featVec axis = 
= value # 只看 当 第 i 列 的 值 
＝ value 时的/nr item 5 reduceFeatVec = featVec axis # 
featVec 的 第 i 列 给 除去 6 reduceFeatVec . 
extend featVec axis + 1 7 retDataSet . append reduceFeatVec 
8 return retDataSet5 . 多重 字典 构 建树 1 def 
createTree dataSet labels 2 classList = example 1 for example 
in dataSet # N N Y Y Y N Y 
3 if classList . count classList 0 = = len 
classList 4 # classList 所有 元素 都 相等 即 类别 
完全相同 停止 划分 5 return classList 0 # splitDataSet dataSet 
0 0 此时 全是 N 返回 N 6 if len 
dataSet 0 = = 1 # 0 0 0 0 
N 7 # 遍历 完 所有 特征 时 返回 出现 
次数 最多 的 8 return majorityCnt classList 9 bestFeat = 
c h o o s e B e s t 
F e a t u r e T o p 
l i t dataSet # 0 － 2 10 # 
选择 最大 的 gain ratio 对应 的 feature 11 bestFeatLabel 
= labels bestFeat # outlook windy 12 myTree = { 
bestFeatLabel { } } 13 # 多重 字典 构 建树 
{ outlook { 0 N 14 del labels bestFeat # 
temperature humidity windy temperature humidity 15 featValues = example bestFeat 
for example in dataSet # 0 0 1 2 2 
2 1 16 uniqueVals = set featValues 17 for value 
in uniqueVals 18 subLabels = labels # temperature humidity windy 
19 myTree bestFeatLabel value = createTree splitDataSet dataSet bestFeat value 
subLabels 20 # 划分 数据 为 下一层 计算 准备 21 
return myTree6 . 可视化 决策树 的 结果 dataSet labels = 
createDataSet labels _ tmp = labels desicionTree = createTree dataSet 
labels _ tmp treePlotter . createPlot desicionTree 