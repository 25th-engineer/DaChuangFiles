5   Neural Networks part two content 5   Neural 
Networks part two 5.1 cost function5 . 2 Back Propagation5 
. 3 神经网络 总结 接上 一篇 4 . Neural Networks 
part one . 本文 将 先 定义 神经 网络 的 
代价 函数 然后 介绍 逆向 传播 Back Propagation BP 算法 
它 能 有效 求解 代价 函数 对 连接 权重 的 
偏 导 最后 对 训练 神经 网络 的 过程 进行 
总结 5.1 cost function 注 正则化 相关内容 参见 3 . 
Bayesian statistics and Regularization 5.2 Back Propagation 详细 推导 过程 
参见 反向 传播 算法 以及 李宏 毅 的 机器学习 课程 
youtube B 站 1 BP 算法 步骤 在 实现 反向 
传播 算法 时 有 如下 几 个 需要 注意 的 
地方 需要 对 所有 的 连接 权重 包括 偏移 单元 
初始 化为 接近 0 但 不全 等于 0 的 随机数 
如果 所有 参数 都用/nr 相同 的 值 作为 初始值 那么 
所有 隐藏 层 单元 最终 会 得到 与 输入 值 
有关 的 相同 的 函数 也 就是说 所有 神经元 的 
激活 值 都会 取 相同 的 值 对于 任何 输入 
x 都会 有   随机 初始化 的 目的 是 使 
对称 失效 具体 地 我们 可以 如 2 一样 随机 
初始化 matlab 实现 见 后文 代码 1 如果 实现 的 
BP 算法 计算出 的 梯度 偏 导数 是 错误 的 
那么 用 该 模型 来 预测 新的 值 肯定 是 
不 科学 的 所以 我们 应该 在 应用 之前 就 
判断 BP 算法 是否 正确 具体 的 可以 通过 数值 
的 方法 如 3 所示 的 计算出 较 精确 的 
偏 导 然后再 和 BP 算法 计算 出来 的 进行 
比较 若 两者 相差 在 正常 的 误差 范围内 则 
BP 算法 计算出 的 应该 是 比较 正确 的 否则 
说明 算法 实现 有误 注意 在 检查 完 后 在 
真正 训练 模型 时 不应该 再运行 数值 计算 偏 导 
的 方法 否则 将 会 运行 很慢 matlab 实现 见 
后文 代码 2 用 matlab 实 现时 要注意 matlab 的 
函数参数 不能 为 矩阵 而 连接 权 重为 矩阵 所以 
在 传递 初始化 连接 权重 前先 将其 向 量化 再用 
reshape 函数 恢复 见 后文 代码 3 2 随机 初始化 
连接 权重 3 数值 方法 求 代价 函数 偏 导 
的 近似值 5.3 神经网络 总结 第一步 设计 神经 网络结构 隐藏 
层 单元 个数 通常 都是 不 确定 的 一般 选取 
神经网络 隐藏 层 单元 个数 的 几个 经验公式 如下 参考 
https / / www . zhihu . com / question 
/ 46530834 此外 MNIST 手写 数字 识别 中 给 出了 
以 不同 的 神经 网络结构 训练 的 结果 供参考 第二步 
实现 正向 传播 FP 和 反向 传播 算法 这一步 包括 
如下 的 子 步骤 第三步 用 数值 方法 检查 求 
偏 导 的 正确性 第四步 用 梯度 下 降法 或 
更 先进 的 优化 算法 求 使得 代价 函数 最小 
的 连接 权重 在 第四 步中/nr 由于 代价 函数 是非 
凸 non convex 函数 所以 在 优化 过程 中 可能 
陷入 局部 最优 值 但 不一定 比 全局 最优 差 
很多 如 4 在 实际 应用 中 通常 不是 大 
问题 也会 有 一些 启发式 的 算法 如 模拟 退火算法 
遗传算法 等 来 帮助 跳出 局部 最优 4 陷入 局部 
最优 不一定 比 全局 最优 差 很多 代码 1 随机 
初始化 连接 权重 function W = r a n d 
I n i t i a l i z e 
W e i g h t s L _ in 
L _ out % R A N D I N 
I T I A L I Z E W E 
I G H T Randomly initialize the weights of a 
layer with L _ in % incoming connections and L 
_ out outgoing connections % W = R A N 
D I N I T I A L I Z 
E W E I G H T L _ in 
L _ out randomly initializes the weights % of a 
layer with L _ in incoming connections and L _ 
out outgoing % connections . % % Note that W 
should be set to a matrix of size L _ 
out 1 + L _ in as % the column 
row of W handles the bias terms % W = 
zeros L _ out 1 + L _ in % 
Instructions Initialize W randomly so that we break the symmetry 
while % training the neural network . % % Note 
The first row of W corresponds to the parameters for 
the bias units % epsilon _ init = sqrt 6 
/ sqrt L _ out + L _ in W 
= rand L _ out 1 + L _ in 
* 2 * epsilon _ init epsilon _ init endView 
Code 代码 2 用 数值 方法 求 代价 函数 对 
连接 权重 偏 导 的 近似值 function numgrad = c 
o m p u t e N u m e 
r i c a l G r a d i 
e n t J theta % C O M P 
U T E N U M E R I C 
A L G R A D I E N T 
Computes the gradient using finite differences % and gives us 
a numerical estimate of the gradient . % numgrad = 
C O M P U T E N U M 
E R I C A L G R A D 
I E N T J theta computes the numerical % 
gradient of the function J around theta . Calling y 
= J theta should % return the function value at 
theta . % Notes The following code implements numerical gradient 
checking and % returns the numerical gradient . It sets 
numgrad i to a numerical % approximation of the partial 
derivative of J with respect to the % i th 
input argument evaluated at theta . i . e . 
numgrad i should % be the approximately the partial derivative 
of J with respect % to theta i . % 
numgrad = zeros size theta perturb = zeros size theta 
e = 1e 4 for p = 1 numel theta 
% Set perturbation vector perturb p = e % Compute 
Numerical Gradient numgrad p = J theta + perturb J 
theta perturb / 2 * e perturb p = 0 
end endView Code 代码 3 应用 FP 和 BP 算法 
实现 计算 隐藏 层 为 1层 的 神经 网络 的 
代价 函数 以 及其 对 连接 权重 的 偏 导数 
function J grad = nnCostFunction nn _ params . . 
. input _ layer _ size . . . hidden 
_ layer _ size . . . num _ labels 
. . . X y lambda % NNCOSTFUNCTION Implements the 
neural network cost function for a two layer % neural 
network which performs classification % J grad = NNCOSTFUNCTON nn 
_ params hidden _ layer _ size num _ labels 
. . . % X y lambda computes the cost 
and gradient of the neural network . The % parameters 
for the neural network are unrolled into the vector % 
nn _ params and need to be converted back into 
the weight matrices . % % The returned parameter grad 
should be a unrolled vector of the % partial derivatives 
of the neural network . % % Reshape nn _ 
params back into the parameters Theta1 and Theta2 the weight 
matrices % for our 2 layer neural network Theta1 1 
2 Theta2 2 3 Theta1 = reshape nn _ params 
1 hidden _ layer _ size * input _ layer 
_ size + 1 . . . hidden _ layer 
_ size input _ layer _ size + 1 Theta2 
= reshape nn _ params 1 + hidden _ layer 
_ size * input _ layer _ size + 1 
end . . . num _ labels hidden _ layer 
_ size + 1 % Setup some useful variables m 
= size X 1 J = 0 Theta1 _ grad 
= zeros size Theta1 Theta2 _ grad = zeros size 
Theta2 % Note The vector y passed into the function 
is a vector of labels % containing values from 1 
. . K . You need to map this vector 
into a % binary vector of 1 s and 0 
s to be used with the neural network % cost 
function . for i = 1 m % compute activation 
by Forward Propagation a1 = 1 X i z2 = 
Theta1 * a1 a2 = 1 sigmoid z2 z3 = 
Theta2 * a2 h = sigmoid z3 yy = zeros 
num _ labels 1 yy y i = 1 % 
训练 集 的 真实 值 yy J = J + 
sum yy . * log h 1 yy . * 
log 1 h % Back Propagation delta3 = h yy 
delta2 = Theta2 2 end * delta3 . * sigmoidGradient 
z2 % 注意 要 除去 偏移 单元 的 连接 权重 
Theta2 _ grad = Theta2 _ grad + delta3 * 
a2 Theta1 _ grad = Theta1 _ grad + delta2 
* a1 end J = J / m + lambda 
* sum sum Theta1 2 end . ^ 2 + 
sum sum Theta2 2 end . ^ 2 / 2 
* m Theta2 _ grad = Theta2 _ grad / 
m Theta2 _ grad 2 end = Theta2 _ grad 
2 end + lambda * Theta2 2 end / m 
% regularized nn Theta1 _ grad = Theta1 _ grad 
/ m Theta1 _ grad 2 end = Theta1 _ 
grad 2 end + lambda * Theta1 2 end / 
m % regularized nn % Unroll gradients grad = Theta1 
_ grad Theta2 _ grad endView Code 