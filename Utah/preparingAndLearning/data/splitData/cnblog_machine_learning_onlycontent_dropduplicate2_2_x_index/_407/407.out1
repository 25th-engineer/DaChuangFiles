各种 机器 学习 的 应用 场景 分别 是 什么 例如 
k 近邻 贝叶斯 决策树 svm 逻辑 斯蒂 回归 和 最大熵 
模型 k 近邻 贝叶斯 决策树 svm 逻辑 斯蒂 回归 和 
最大熵 模型 隐 马尔科夫 条件 随 机场 adaboost em 这些 
在 一般 工作 中 分别 用到 的 频率 多大 一般用 
关于 这个 问题 我 今天 正好 看到 了 这个 文章 
讲 的 正是 各个 算法 的 优劣 分析 很 中肯 
https / / zhuanlan . zhihu . com / p 
/ 25327755 正好 14年 的 时候 有 人做 过 一个 
实验 1 比较 在 不同 数据 集上 121个 不同 的 
分类器 179个 的 实际 效果 论文 题为 Do we Need 
Hundreds of Classifiers to Solve Real World Classification Problems 实验 
时间 有点 早 我 尝试 着 结合 我 自己 的 
理解 一些 最近 的 实验 来 谈一谈 吧 主要 针对 
分类器 Classifier 写给 懒得 看 的 人 没有 最好 的 
分类器 只有 最 合适 的 分类器 随机 森林 平均 来说 
最强 但也 只在 9.9% 的 数据 集上 拿到 了 第一 
优点 是 鲜有 短板 SVM 的 平均 水平 紧随其后 在 
10.7% 的 数据 集上 拿到 第一 神经网络 13.2% 和 boosting 
~ 9% 表现 不错 数据 维度 越高 随机 森林 就比 
AdaBoost 强 越多 但是 整体 不及 SVM 2 数据 量 
越大 神经 网络 就 越强 近邻 Nearest Neighbor 典型 的 
例子 是 KNN 它 的 思路 就是 对于 待 判断 
的 点 找到 离 它 最近 的 几个 数据 点 
根据 它们 的 类型 决定 待 判 断点 的 类型 
它 的 特点 是 完全 跟着 数据 走 没有 数学模型 
可言 适用 情景 需要 一个 特别 容易 解释 的 模型 
的 时候 比如 需要 向 用户 解释 原因 的 推荐算法 
贝叶斯 Bayesian 典型 的 例子 是 Naive Bayes 核心 思路 
是 根据 条件概率 计算 待 判 断点 的 类型 是 
相对 容易 理解 的 一个 模型 至今 依然 被 垃圾邮件 
过滤器 使用 适用 情景 需要 一个 比较 容易 解释 而且 
不同 维度 之间 相关性 较小 的 模型 的 时候 可以 
高效 处理 高维 数据 虽然 结果 可能 不 尽如人意 决策树 
Decision tree 决策树 的 特点 是 它 总是 在 沿着 
特征 做 切分 随着 层层 递进 这个 划分 会 越来越 
细 虽然 生成 的 树 不 容易 给 用户 看 
但是 数据分析 的 时候 通过观察 树 的 上层 结构 能够 
对 分类器 的 核心 思路 有 一个 直观 的 感受 
举个 简单 的 例子 当 我们 预测 一个 孩子 的 
身高 的 时候 决策树 的 第一 层 可能 是 这个 
孩子 的 性别 男生 走 左边 的 树 进行 进一步 
预测 女生 则 走 右边 的 树 这就 说明性 别对 
身高 有 很强 的 影响 适用 情景 因为 它 能够 
生成 清晰 的 基于 特征 feature 选择 不同 预测 结果 
的 树状 结构 数据 分析 师 希望 更好 的 理解 
手上 的 数据 的 时候 往往 可以 使用 决策树 同时 
它 也是 相对 容易 被 攻击 的 分类器 3 这里 
的 攻击 是 指 人为 的 改变 一些 特征 使得 
分类器 判断 错误 常见于 垃圾邮件 躲避 检测 中 因为 决策树 
最终 在 底层 判断 是 基于 单个 条件 的 攻击者 
往往 只 需要 改变 很少 的 特征 就 可以 逃过 
监测 受限于 它 的 简单 性 决策树 更大 的 用处 
是 作为 一些 更 有用 的 算法 的 基石 随机 
森林 Random forest 提到 决策树 就 不得不 提 随机 森林 
顾名思义 森林 就是 很 多树 严格来说 随机 森林 其实 算是 
一种 集成 算法 它 首先 随机 选取 不同 的 特征 
feature 和 训练样本 training sample 生成 大量 的 决策树 然后 
综合 这些 决策树 的 结果 来 进行 最终 的 分类 
随机 森林 在 现实 分析 中 被 大量 使用 它 
相对于 决策树 在 准确性 上有 了 很大 的 提升 同时 
一定 程度 上 改善 了 决策树 容易 被 攻击 的 
特点 适用 情景 数据 维度 相对 低 几十 维 同时 
对 准确性 有 较高 要求 时 因为 不 需要 很多 
参数 调整 就 可以 达到 不错 的 效果 基本上 不 
知道 用 什么 方法 的 时候 都 可以 先 试一下 
随机 森林 SVM Support vector machine SVM 的 核心 思想 
就是 找到 不同 类别 之间 的 分 界面 使得 两类 
样本 尽量 落在 面的 两边 而且 离 分 界面 尽量 
远 最早 的 SVM 是 平面 的 局限 很大 但是 
利用 核 函数 kernel function 我们 可以 把 平面 投射 
mapping 成 曲面 进而 大大提高 SVM 的 适用 范围 提高 
之后 的 SVM 同样 被 大量 使用 在 实际 分类 
中 展现 了 很 优秀 的 正确 率 适用 情景 
SVM 在 很多 数据 集上 都有 优秀 的 表现 相对来说 
SVM 尽量 保持 与 样本 间 距离 的 性质 导致 
它 抗 攻击 的 能力 更强 和 随机 森林 一样 
这也 是 一个 拿到 数据 就 可以 先 尝试 一下 
的 算法 逻辑 斯蒂 回归 Logistic regression 逻辑 斯蒂 回归 
这个 名字 太 诡异 了 我 就叫 它 LR 吧 
反正 讨论 的 是 分类器 也 没有 别的 方法 叫 
LR 顾名思义 它 其实 是 回归 类 方法 的 一个 
变体 回归 方法 的 核心 就是 为 函数 找到 最 
合适 的 参数 使得 函数 的 值 和 样本 的 
值 最接近 例如 线性 回归 Linear regression 就是 对于 函数 
f x = ax + b 找到 最 合适 的 
a b LR 拟合 的 就 不是 线性函数 了 它 
拟合 的 是 一个 概率 学 中的 函数 f x 
的 值 这时候 就 反映 了 样本 属于 这个 类 
的 概率 适用 情景 LR 同样 是 很多 分类 算法 
的 基础 组件 它 的 好处 是 输出 值 自然地 
落在 0 到 1 之间 并且有 概率 意义 因为 它 
本质上 是 一个 线性 的 分类器 所以 处理 不好 特征 
之间 相关 的 情况 虽然 效果 一般 却 胜在 模型 
清晰 背后 的 概率 学 经得住 推敲 它 拟合 出来 
的 参数 就 代表 了 每一个 特征 feature 对 结果 
的 影响 也 是 一个 理解 数据 的 好 工具 
判别分析 Discriminant analysis 判别分析 主要 是 统计 那边 在用 所以 
我 也 不是 很 熟悉 临时 找 统计 系 的 
闺蜜 补了 补课 这里 就 现学现卖 了 判别分析 的 典型 
例子 是 线性 判别分析 Linear discriminant analysis 简称 LDA 这里 
注意 不要 和 隐含 狄利克雷 分布 Latent Dirichlet allocation 弄混 
虽然 都叫/nr LDA 但 说 的 不是 一件 事 LDA 
的 核心 思想 是 把 高维 的 样本 投射 project 
到 低维 上 如果 要 分成 两类 就 投射 到 
一维 要 分 三类 就 投射 到 二维 平面 上 
这样 的 投射 当然 有 很多 种 不同 的 方式 
LDA 投射 的 标准 就是 让 同类 的 样本 尽量 
靠近 而不 同类 的 尽量 分开 对于 未来 要 预测 
的 样本 用 同样 的 方式 投射 之后 就 可以 
轻易 地 分辨 类 别了 使用 情景 判别分析 适用 于 
高维 数据 需要 降 维 的 情况 自带 降 维 
功能 使得 我们 能 方便 地 观察 样本分布 它 的 
正确性 有 数学 公式 可以 证明 所以 同样 是 很 
经得住 推敲 的 方式 但是 它 的 分类 准确率 往往 
不是 很高 所以 不是 统计 系 的 人 就把 它 
作为 降 维 工具 用吧 同时 注意 它 是 假定 
样本 成 正态分布 的 所以 那种 同心 圆形 的 数据 
就 不要 尝试 了 神经网络 Neural network 神经网络 现在 是 
火 得 不行 啊 它 的 核心 思路 是 利用 
训练样本 training sample 来 逐渐 地 完善 参数 还是 举个 
例子 预测 身高 的 例子 如果 输入 的 特征 中 
有一个 是 性别 1 男 0 女 而 输出 的 
特征 是 身高 1 高 0 矮 那么 当 训练样本 
是 一个 个子 高的/nr 男生 的 时候 在 神经 网络 
中 从 男 到 高 的 路线 就 会被 强化 
同理 如果 来 了 一个 个子 高的/nr 女生 那从/nr 女 
到 高 的 路线 就 会被 强化 最终 神经 网络 
的 哪些 路线 比较 强 就由 我们 的 样本 所 
决定 神经 网络 的 优势 在于 它 可以 有 很多 
很 多层 如果 输入输出 是 直接 连接 的 那它 和 
LR 就 没有 什么 区别 但是 通过 大量 中间层 的 
引入 它 就 能够 捕捉 很多 输入 特征 之间 的 
关系 卷积 神经网络 有很 经典 的 不同 层 的 可视化 
展示 visulization 我 这里 就不 赘述 了 神经 网络 的 
提出 其实 很 早了 但是 它 的 准确率 依赖于 庞大 
的 训练 集 原本 受限 于 计算机 的 速度 分类 
效果 一直 不如 随机 森林 和 SVM 这种 经典 算法 
使用 情景 数据量 庞大 参数 之间 存在 内在 联系 的 
时候 当然 现在 神经 网络 不 只是 一个 分类器 它 
还 可以 用来 生成 数据 用来 做 降 维 这些 
就不 在 这里 讨论 了 Rule based methods 这个 我 
是 真 不熟 都不/nr 知道 中文 翻译 是 什么 它 
里面 典型 的 算法 是 C 5.0 Rules 一个 基于 
决策树 的 变体 因为 决策树 毕竟 是 树状 结构 理解 
上 还是 有 一定 难度 所以 它 把 决策树 的 
结果 提取 出来 形成 一个 一个 两三 个 条件 组成 
的 小 规则 使用 情景 它 的 准确度 比 决策树 
稍低 很 少见 人用 大概 需要 提供 明确 小 规则 
来 解释 决定 的 时候 才会 用吧 提升 算法 Boosting 
接下 来讲 的 一系列 模型 都 属于 集成 学习 算法 
Ensemble Learning 基于 一个 核心 理念 三个臭皮匠 顶个 诸葛亮 翻译 
过来 就是 当 我们 把 多个 较弱 的 分类器 结合 
起来 的 时候 它 的 结果 会 比 一个 强的/nr 
分类器 更 典型 的 例子 是 AdaBoost AdaBoost 的 实现 
是 一个 渐进 的 过程 从 一个 最 基础 的 
分类器 开始 每次 寻找 一个 最 能 解决 当前 错误 
样本 的 分类器 用 加权 取 和 weighted sum 的 
方式 把 这个 新 分类器 结合 进 已 有的 分类器 
中 它 的 好处 是 自 带了 特征选择 feature selection 
只 使用 在 训练 集 中 发现 有效 的 特征 
feature 这样 就 降低 了 分类 时 需要 计算 的 
特征 数量 也 在 一定 程度 上 解决 了 高维 
数据 难以 理解 的 问题 最 经典 的 AdaBoost 实现 
中 它 的 每一个 弱 分类器 其实 就是 一个 决策树 
这 就是 之前 为什么 说 决策树 是 各种 算法 的 
基石 使用 情景 好 的 Boosting 算法 它 的 准确性 
不逊于 随机 森林 虽然在 1 的 实验 中 只有 一个 
挤进 前 十 但是 实际 使用 中 它 还是 很强 
的 因为 自带 特征选择 feature selection 所以 对 新手 很 
友好 是 一个 不 知道 用 什么 就 试 一下 
它 吧 的 算法 装袋 算法 Bagging 同样 是 弱 
分类器 组合 的 思路 相对于 Boosting 其实 Bagging 更好 理解 
它 首先 随机 地 抽取 训练 集 training set 以之 
为 基础 训练 多个 弱 分类器 然后 通过 取 平均 
或者 投票 voting 的 方式 决定 最终 的 分类 结果 
因为 它 随机 选取 训练 集 的 特点 Bagging 可以 
一定 程度 上 避免 过渡 拟合 overfit 在 1 中 
最强 的 Bagging 算法 是 基于 SVM 的 如果 用 
定义 不 那么 严格 的 话 随机 森林 也 算是 
Bagging 的 一种 使用 情景 相较 于 经典 的 必 
使 算法 Bagging 使用 的 人 更少 一些 一 部分 
的 原因 是 Bagging 的 效果 和 参数 的 选择 
关系 比较 大 用 默认 参数 往往 没有 很好 的 
效果 虽然 调 对 参数 结果 会比 决策树 和 LR 
好 但是 模型 也 变得 复杂 了 没事 有 特别 
的 原因 就 别用 它 了 Stacking 这个 我 是 
真不知道 中文 怎么说 了 它 所做 的 是 在 多个 
分类器 的 结果 上 再 套 一个 新的 分类器 这个 
新的 分类器 就 基于 弱 分类器 的 分析 结果 加上 
训练 标签 training label 进行 训练 一般 这 最后 一层 
用 的 是 LR Stacking 在 1 里面 的 表现 
不好 可能 是 因为 增加 的 一层 分类器 引入 了 
更多 的 参数 也 可能 是 因为 有 过渡 拟合 
overfit 的 现象 使用 情景 没事 就 别 用了 修订 
@ 庄岩/nr 提醒 说 stacking 在 数据挖掘 竞赛 的 网站 
kaggle 上 很 火 相信/v 参数/n 调得/v 好/a 的话/u 还是/c 
对/p 结果/n 能有/nr 帮助/v 的/uj http / / blog . 
kaggle . com / 2016 / 12/27 / a kagglers 
guide to model stacking in practice / 这篇文章 很好 地 
介绍 了 stacking 的 好处 在 kaggle 这种 一点点 提升 
就 意味着 名次 不同 的 场合 下 stacking 还是 很 
有效 的 但是 对于 一般 商用 它 所 带来 的 
提升 就 很难 值 回 额外 的 复杂 度了 多 
专家 模型 Mixture of Experts 最近 这个 模型 还 挺 
流行 的 主要 是 用来 合并 神经 网络 的 分类 
结果 我 也 不是 很 熟 对 神经 网络 感兴趣 
而且 训练 集 异质性 heterogeneity 比较 强 的话 可以 研究 
一下 这个 讲 到 这里 分类器 其实 基本 说完 了 
讲 一下 问题 里面 其他 一些 名词 吧 最大熵 模型 
Maximum entropy model 最大熵 模型 本身 不是 分类器 它 一般 
是 用来 判断 模型 预测 结果 的 好坏 的 对于 
它 来说 分类器 预测 是 相当 于是 针对 样本 给 
每个 类 一个 出现 概率 比如说 样本 的 特征 是 
性别 男 我 的 分类器 可能 就 给出 了 下面 
这样 一个 概率 高 60% 矮 40% 而 如果 这个 
样 本 真的 是 高的/nr 那 我们 就 得了 一个 
分数 60% 最大熵 模型 的 目标 就是 让 这些 分数 
的 乘积 尽 量大 LR 其实 就是 使用 最大熵 模型 
作为 优化 目标 的 一个 算法 4 EM 就像 最大熵 
模型 一样 EM 不是 分类器 而是 一个 思路 很多 算法 
都是/nr 基于 这个 思路 实现 的 @ 刘奕驰/nr 已经 讲 
得很 清楚 了 我 就 不多 说 了 隐 马尔科夫 
Hidden Markov model 这 是 一个 基于 序列 的 预测 
方法 核心 思想 就是 通过 上 一个 或 几个 状态 
预测 下 一个 状态 之所以 叫 隐 马尔科夫 是 因为 
它 的 设定 是 状态 本身 我们 是 看不到 的 
我们 只能 根据 状态 生成 的 结果 序列 来 学习 
可能 的 状态 适用 场景 可以 用于 序列 的 预测 
可以 用来 生成 序列 条件 随 机场 Conditional random field 
典型 的 例子 是 linear chain CRF 具体 的 使用 
@ Aron 有讲/nr 我 就不 献丑 了 因为 我 从来 
没 用过 这个 就是 这些 啦 相关 的 文章 1 
Do we need hundreds of classifiers to solve real world 
classification problems . Fern á ndez Delgado Manuel et al 
. J . Mach . Learn . Res 15.1 2014 
2 An empirical evaluation of supervised learning in high dimensions 
. Rich Caruana Nikos Karampatziakis and Ainur Yessenalina . ICML 
08 3 Man vs . Machine Practical Adversarial Detection of 
Malicious Crowdsourcing WorkersWang G . Wang T . Zheng H 
. & Zhao B . Y . Usenix Security 14 
4 http / / www . win vector . com 
/ dfiles / L o g i s t i 
c R e g r e s s i o 
n M a x E n t . pdf 编辑 
于 2017 05 18Hu Haitanghttps / / hthu . github 
. io / 一般来说 Machine Learning 主要 有 3种 Supervised 
LearningSemi supervised L e a r n i n g 
U n s u p e r v i s 
e d L e a r n i n g 
u p e r v i s e d Learning 
适用 与 已知 LABEL 的 情况 . Semi supervised / 
Unsupervised Learning/w 适用/v 于有/nr Latent/w Variable 的 情况 . 题 
主 提到 的 这些 算法 虽然 常见 但是 想 真正 
的 搞懂 需要 深入 了解 Calculus Linear Algebra Probabilistic . 
比如 SVM 为什么 叫 Support Vector Machine 为什么 SVM 有 
Sparsity 的 特性 L1 / L2 Regularizar 起到 了 什么 
作用 等等 . . . 单单 一个 SVM 就有 太多 
的 内容 在 里面 了 . 接下来 回答 问题 k 
近邻 贝叶斯 决策树 svm 逻辑 斯蒂 回归 和 最大熵 模型 
隐 马尔科夫 条件 随 机场 adaboost em 首先 Maximum Entropy 
和 EM 是 理念 . Maximum Entropy 的 背后 是 
信息论 Information Theory 和 概率模型 . EM Model 事实上 对 
所有 含有 Latent Variable 的 模型 都 可以 用 . 
kNN 可以 用于 图像压缩 . 但是 如何 选取 K Similarity 
用 什么 都 不是 随便 说 应用 就 应用 的 
. . . Naive Bayes 知名 的 http / / 
arxiv . org 背后 就 有用 简单 粗暴 强大 有效 
. . . 但是 NB 算法 如何 Smooth 如何 应对 
Online Setting 也 不是 那么 简单 . . . SVM 
/ Logistic Regression 都 可以 用于 各种 分类 . 但是 
Hinge Loss 和 Sigmoid Function 有 什么 区别 什么 时候 
用 哪个 HMM / CRF NLP 常用 Ada boost 可以 
参考 Netflix Prize 貌似 大奖 用 了 这个 . . 
. 具体 不 清楚 但是 Weak Learner 是 什么 Why 
boost works 总结 前几天 Yann Lecun 来 我们 学校 做 
了 演讲 然后 我们 的 老师 回头 总结 我 觉得 
很 有道理 就是 现在 Deep Learning 处于 一个 Scaling 就是 
Accuracy 的 阶段 很多 背后 的 原因 我们 其实 都 
没有 搞清楚 . 例如 Deep Learning 里 用 Max 做 
Feature selection 效果 很好 但是 Yann Lecun 也 只能 猜测 
原因 . 所以 真正 的 应用 取决于 你 动手 去做 
去 尝试 . 最后 推荐 Andrew 的 公开课 你 值得 
拥有 . https / / class . coursera . org 
/ ml 006 编辑 于 2015 03 09 唐学伟/nr 机器学习 
金融 招聘 机器学习 数据分析 tangxuewei @ wecash . net 先上 
一张 图 吧 有 需要 再 详细 写 编辑 于 
2014 12 08 知乎 用户 crf 分词 语法 依存 分析 
命名 实体 识别 但是 现在 正在 越来越 多 的 应用 
场景 里 被 RNN 已经 它 的 变种 们 所 
替代 ~ LSTM + CRF 的 解决 方案 取得 了 
state of art 的 效果 ~ lr ctr 预估 商品 
推荐 转换 为 点击率 预估 也 可以 用 该 模型 
之前 天猫 大 数据 比赛 很多 同学 都是 用 它 
做 的 session first 可以 着重 了解 推导 正则化 及 
并行 但 现在 越来越 多 的 依赖 于 深度 学习 
包括 DNN DRL . svm 可以 做 文本 分类 人脸 
识别 等 了解 下 原始 问题 如何 转 换为 对偶 
问题 然后 使用 smo 或者 其他 问题 求解 还有 了 
解下 核 函数 ~ adaboost 本身 是 exp loss 在 
boosting 方法 下 的 Model . EM 是 一种 优化 
算法 本质 个人认为 有点 类似于 离散 空间 的 梯度 上升 
算法 一般 是 结合 具体 的 算法 来用 比如 混合 
高斯 模型 最大熵 无 监督 HMM 等 但 比较 经典 
的 个人 感觉 还是 pLSA 其实 k means 背后 也有 
em 的 思想 了解 em 再看 k means 就有 感觉 
了 决策树 可以 认为 是 将 空间 进行 划分 ID3 
和C/nr 4.5 算是 比较 经典 的 决策树 算法 可以 用 
来 分类 也 可以 用来 回归 但 业界 很少 直接 
使用 一棵树 一般 使用 多 棵树 组成 committee 较为 经典 
有 GBDT 和 RF 两者 都是 ensemble learning 的 典范 
只不过 前者 使用 boosting 降低 bias 后者 使用 bagging 降低 
variance 从而 提升 模型 的 performance 在 ESL 中 有个 
对比 使 树形 模型 几乎 完爆 其他 算法 泛化 能力 
和 学习 能力 都很 牛逼 业界 的话 一般 用 来做 
搜索 排序 和 相关性 HMM 在 基础 的 一 阶 
马儿 科夫 的 基础 之上 加入 隐含 状态 有 两者 
假设 解决 三 种 问题 一般 时间 序列 预测 都 
可以 用 该 模型 当然 了 NLP 中的 分词 语音 
识别 等 效果 也 还不错 先写 到这 求 大 神来 
喷 ~ 编辑 于 2017 09 20 刘奕驰/nr 分别 用到 
的 频率 多大 It depends . 看 你 需要 处理 
的 问题 是 什么 不同 数据 规模 不同 特征 都会/nr 
影响 算法 的 选择 一般 用途 是 什么 这个 问题 
太 大了 简单 来说 KNN 朴素 贝叶斯 决策树 SVM logistic 
回归 adaboost 用来 分类 EM 算法 用于 寻找 隐藏 参数 
的 最大 似 然 估计 该 算法 首先在 E step 
中 计算 隐藏 参数 的 似 然 估计 然后再 M 
step 中 进行 最大化 然后 进行 EM step 的 迭代 
直至 收敛 应用 场景 之一 是 聚 类 问题 但 
EM 算法 本身 并 不是 一个 聚 类 算法 举个 
例子 GMM 高斯 混合模型 和/c Kmeans/w 在/p 聚/v 类/q 时都/nr 
使用/v 了/ul EM/w 算法/n 最大熵 模型 是 一个 概率模型 决策树 
的 数学 基础 就是 它 HMM 也 是 一个 统计模型 
我们 不能 用 它 来 做什么 但是 可以 利用 这个 
模型 对 现实 生活 中 的 问题 建模 需要 注意 
什么 实践 出 真知 编辑 于 2014 12 03 刘莉莉 
能 宽容 必定 心怀 珍宝 DL & RL/w 谢霄哥/nr 邀请/v 
@/i 孙/zg 凌霄/nr 这个/r 问题/n 确实/ad 很/zg 有意思/l 作为 新 
入门 的 小白 只能 提供 一点点 粗略 的 认识 K 
近邻 算法 采用 测量 不同 特征值 之间 的 距离 的 
方法 进行 分类 优点 1 . 简单 好用 容易 理解 
精度高 理论 成熟 既 可以 用来 做 分类 也 可以 
用来 做 回归 2 . 可 用于 数值 型 数据 
和 离散 型 数据 3 . 训练 时间 复杂度 为 
O n 无 数据 输入 假定 4 . 对 异常值 
不 敏感 缺点 1 . 计算 复杂性 高 空间 复杂性 
高 2 . 样本 不 平衡 问题 即 有些 类别 
的 样本 数量 很多 而 其它 样本 的 数量 很少 
3 . 一般 数值 很大 的 时候 不 用 这个 
计算 量 太大 但是 单个/nr 样本 又不能 太少 否则 容易 
发生 误 分 4 . 最大 的 缺点 是 无法 
给出 数据 的 内在 含义 朴素 贝叶斯 优点 1 . 
生成式 模型 通过 计算 概率 来 进行 分类 可以 用来 
处理 多分 类 问题 2 . 对 小 规模 的 
数据 表现 很好 适合 多 分类 任务 适合 增量 式 
训练 算法 也 比较 简单 缺点 1 . 对 输入 
数据 的 表达 形式 很 敏感 2 . 由于 朴素 
贝叶斯 的 朴素 特点 所以 会 带来 一些 准确率 上 
的 损失 3 . 需要 计算 先验概率 分类 决策 存在 
错误率 决策树 优点 1 . 概念 简单 计算 复杂度 不高 
可解释 性强 输出 结果 易于 理解 2 . 数据 的 
准备 工作 简单 能够 同时 处理 数据 型 和 常规 
型 属性 其他 的 技术 往往 要求 数据 属性 的 
单一 3 . 对 中间 值得 确实 不 敏感 比较 
适合 处理 有 缺失 属性值 的 样本 能够 处理 不 
相关 的 特征 4 . 应用 范围 广 可以 对 
很多 属性 的 数据 集 构造 决策树 可扩展 性强 决策树 
可以 用于 不 熟悉 的 数据 集合 并从 中 提取 
出 一些 列 规则 这 一点 强于 KNN 缺点 1 
. 容易 出现 过拟合 2 . 对于 那些 各 类别 
样本 数量 不 一致 的 数据 在 决策树 当中 信息 
增益 的 结果 偏向 于 那些 具有 更多 数值 的 
特征 3 . 信息 缺 失时 处理 起来 比较 困难 
忽略 数据 集中 属性 之间 的 相关性 Svm 优点 1 
. 可 用于 线性 / 非线性 分类 也 可以 用于 
回归 泛化 错误率 低 计算 开销 不大 结果 容易 解释 
2 . 可以 解决 小 样本 情况下 的 机器 学习 
问题 可以 解决 高维 问题 可以 避免 神经 网络结构 选择 
和 局部 极小 点 问题 3 . SVM 是 最好 
的 现成 的 分类器 现成 是 指 不加 修改 可 
直接 使用 并且 能够 得到 较低 的 错误率 SVM 可以 
对 训练 集 之外 的 数据 点 做 很好 的 
分类 决策 缺点 对 参数 调节 和和 函数 的 选择 
敏感 原始 分类器 不加 修改 仅 适用 于 处理 二分 
类 问题 Logistic 回归 根据 现有 数据 对 分类 边界线 
建立 回归 公式 依次 进行 分类 优点 实现 简单 易于 
理解 和 实现 计算 代价 不高 速度 很快 存储资源 低 
缺点 容易 欠 拟合 分类 精度 可能 不高 EM 期望 
最大化 算法 上帝 算法 只要 有 一些 训练 数据 再 
定义 一个 最大化 函数 采用 EM 算法 利用计算机 经过 若干 
次 迭代 就 可以 得到 所需 的 模型 EM 算法 
是 自 收敛 的 分类 算法 既 不 需要 事先 
设定 类别 也 不需要 数据 见 的 两两 比较 合并 
等 操作 缺点 是 当 所要 优化 的 函数 不是 
凸函数 时 EM 算法 容易 给出 局部 最佳解 而 不是 
最优 解 参考文献 机器学习 判别式 模型 与 生成式 模型 数据挖掘 
十大 算法 EM 算法 最大 期望 算法 各种 分类 算法 
的 优缺点 机器学习 & 数据挖掘 笔记 _ 16 常 见面 
试之 机器学习 算法 思想 简单 梳理 吴军 ． 数学 之美 
M ． 北京 人民邮电出版社 2014 . Peter Harrington 李锐 李鹏 
曲 亚东 王斌 ． 机器学习 实战 M ． 北京 人民邮电出版社 
2013 ． 李航 ． 统计 学习 方法 M ． 北京 
清华大学出版社 2012 ． 杉山 将 许永伟/nr . 图解 机器学习 M 
． 北京 人民邮电出版社 2015 ． 斯坦福大学 公开课 机器学习 课程 编辑 
于 2016 06 12 张戎 数学 话题/n 的/uj 优秀/a 回答者/n 
说/v 一些/m 和/c 题目/n 里面/f 的/uj 模型/n 不太/i 有/v 关系/n 
的话/u 介绍 一个 KL 散度 的 运用 小 场景 写过 
一篇 关于 KL 散度 的 理论 ＋ 运用 的 文章 
KL 散度 从 动力 系统 到 推荐 系统 在 信息论 
和 动力 系统 里面 Kullback Leibler 散度 简称 KL 散度 
KL divergence 是 两个 概率分布 P 和 Q 的 一个 
非 对称 的 度量 公式 这个 概念 是由 Solomon Kullback 
和 Richard Leibler 在 1951 年 引入 的 从 概率分布 
Q 到 概率分布 P 的 KL 散度 用 D _ 
{ KL } P | | Q 来 表示 尽管 
从 直觉 上看 KL 散度 是 一个 度 量 或者 
是 一个 距离 但是 它 却不 满足 度 量 或者 
距离 的 定义 例如 从 Q 到 P 的 KL 
散度 就 不一定 等于 从 P 到 Q 的 KL 
散度 本文 即将 介绍 如何 将 动力 系统 的 概念 
运用 到 实际 推荐 系统 的 工作 中 从而 达到 
更佳 的 推荐 效果 详细 请见 KL 散度 从 动力 
系统 到 推荐 系统 发布 于 2016 03 27 知乎 
用户 现代 玄学 很多 需要 依靠 经验 发布 于 2014 
11 23iamsile 算法 工程师 具体 问题 具体 分析 发布 于 
2014 11 23 章鱼 小丸子 不懂 算法 的 产品 经理 
不是 好 的 程序员 算法 是 解决 方法 的 数学 
抽象 一个 算法 诞生 于 某个 应用 场景 下 但 
也 可以 用 在 其他 应用 场景 按 场景 来 
分 不太 合理 比如 pagerank 是 用来 做 网页 排序 
的 有人 把 它 用在 文本处理 上 发现 效果 奇好 
于是 发 明了 textrank 再 比如 word2vec 是 自然 语言 
处理 的 方法 但 有人 用 它 来 处理 交互 
数据 给 微博 用户 做 推荐 发布 于 2017 03 
15 勤 哥哥 迷惘 少年 随便 强答/nr 一发 吧 说 
几个 自己 用过 的 lr 万金油 中的 万金油 解释性 很强 
每个 特征 可以 通过 权 重来 分析 重要性 这 就是 
所谓 的 可 解释性 应用 场合 很多 例如 CTR 预估 
因为 在 这个 场合 下 特征 纬度 很高 并且 稀疏 
很 适合 用 lr 当 特征 维度 不高 而且 不会 
稀疏 一般 是 没 onehot 也 就是 比较 稠密 的 
时候 用 gbdt 或者 xgboost 比较 合适 