先从 一 本书 说起 吧 机器学习 实战 作者 在 书中 
讲到 逻辑 回归 的 时候 用 简短 的 语言 介绍 
了 一下 理论 之后 就 给出 了 一段 代码 然而 
就是 这段 代码 把 我 带进 了 误区 也许 不能 
叫 误区 而是 因为 我 自己 的 水平 不够 后来 
在 查阅 资料 的 时候 发现 有人 也 因为 这个 
问题 纠结 了 好久 也许 这 本书 是 写给 一些 
有 经验 的 人员 看 的 不是 特别 适合 作为 
入门 的 书 在 查找 关于 逻辑 回归 相关 资料 
的 时候 发现 大多数 都是/nr 介绍 了 好多 数学公式 所以 
我 一直 都在/nr 理解 数学公式 的 基础 上 同时 试图 
在 脑海 中 演练 该 如何 编程 实现 它 然后再 
对照 上面 提到 的 书中 的 代码 然后 悲哀 的 
发现 了 解 不了 并且 查到 的 大多数 资料 上 
并 没有 详细 的 代码 实现 如果 有 也是 跟 
书上 的 代码 是 一样 的 最后 从 网上 找到 
书中 使用 的 测试数据 跟踪 打印 代码 中的 每个 变量 
才 理解 了 书中 第一段 代码 的 求解 原理 进而 
理解 了 后面 一些 代码 的 原理 现在 回过 头 
想想 确实 比较 简单 但是 这个 简单 是 有一个 前提 
的 书上 的 代码 或者 资料 中 推导 出 的 
的 公式 或者 我 自己 的 理解 这 三者 之间 
必须 是 有 一个 错误 的 并且 在 这个 过程 
中 我 一直 试图 绕过 那么多 数学公式 和 一堆 概念 
但是 发现 很难 所以 我 按照 自己 的 理解 剔除 
掉 一些 无用 的 数学 公式 和 概念 力求 用 
最少 的 理论 解释 清楚 什么 是 逻辑 回归 1 
. 逻辑 回归 的 定义 1 有 一种 定义 是 
这样 的 逻辑 回归 其实 是 一个 线性 分类器 只是 
在 外面 嵌套 了 一个 逻辑 函数 主要 用于 二分 
类 问题 这个 定义 明确 的 指明 了 逻辑 回归 
的 特点 a 一个 线性 分类器 b 外层 有 一个 
逻辑 函数 2 假设 有 一个 线性函数 z 其 一般 
公式 为 公式 一 转换成 求和 公式 公式 二 转化成 
向量 的 形式 公式 三 3 逻辑 函数 也叫 Sigmoid 
函数 基本上 采用 的 都是 下面 这个 函数 公式 四 
这个 函数 的 作用 就是 把 无限大 或者 无限小 的 
数据 压缩 到 0 1 之间 用来 估计 概率 图像 
大致 为 基本上 是以 0.5 分界 0.5 以上 为 1 
0.5 以下 为 0 但是 这个 分 界值 可以 自己 
设定 4 逻辑/n 回归/v 函数/n 综合/vn 公式/n 四/m 和/c 公式/n 
一/m 或者/c 公式/n 四/m 和/c 公式/n 三/m 即可 得到 逻辑 
回归 函数 公式 五 或者 公式 六 其实 如果 编程 
求解 的话 到 这里 基本 就 可以 了 但是 既然 
都 提到 了 最大 似 然 估计 那 我们 也 
说 下 2 . 最大 似 然 估计 最大 似 
然 估计 是 建立 在 这样 的 思想 上 已知 
某个 参数 能使 这个 样本 出现 的 概率 最大 我们 
当然 不会 再 去 选择 其他 小 概率 的 样本 
所以 干脆 就 把 这个 参数 作为 估计 的 真实 
值 换 句话 说 就是 既然 我们 无法 知道 真实 
值 那么 就 把 这个 当作 真 实值 吧 其 
唯一 的 作用 就是 给 这个 算法 找 一个 说 
的 过去 的 理论 基础 然后 在 这个 基础 上 
推导 出 最大 似 然 函数 接着 构建 损失 函数 
这 对于 非 数学 专业 的 人 来说 用途 并不大 
有时候 甚至 会 造成 理解 上 的 困难 进而 变成 
学习 的 阻碍 其实 我们 完全 可以 绕过 这个 阻碍 
只去 关注 最后 的 结果 补充 说明 如果 想 要 
理解 推导 过程 可以 先 看看 最大 似 然 估计 
思想 然后 也 要 理解 联合 概率 因为 有些 讲 
逻辑 回归 的 文章 会 直接 跳出 最大 似 然 
估计 的 函数 如果 不 了解 这 两点 内容 容易 
抓瞎 3 . 求解 方式 1 使用 梯度 下 降法 
求解 经过 一 系列 推导 之后 得出 梯度 下 降法 
求解 的 核心 公式 即 权重 的 更新 方式 公式 
七 需要 说明 的 是 α 表示 下降 的 步长 
可以 自己 指定 h θ   表示 损失 函数 或者 
惩罚 系数 如果 h θ 表示 惩罚 系数 那么 如何 
求 的 这组 系数 才是 整个 逻辑 回归 算法 的 
重点 在 开始 写 代码 前 再 介绍 另外 一个 
求解 方式 向 量化 2 向/p 量化/v 向/p 量化/v 是/v 
使用/v 矩阵/n 计算/v 来/v 代替/v for/w 循环/vn 以 简化 计算 
过程 提高效率 下面 引用 下 其他 文章 的 讲解 出现 
的 地方 太多 不 知道 哪个 是 原作者 见谅 向量 
化 过程 约定 训练 数据 的 矩阵 形式 如下 x 
的 每 一行 为 一条 训练样本 而每 一列 为 不同 
的 特称 取值 g A 的 参数 A 为 一列 
向量 所以 实现 g 函数 时要/nr 支持 列 向量 作为 
参数 并 返回 列 向量 θ 更新过程 可以 改为 综上所述 
向/p 量化/v 后θ/nr 更新/d 的/uj 步骤/n 如下/t a 求 A 
= x * θ b 求 E = g A 
yc 求 4 . 实现 过程 下面 的 代码 实现 
的 是 梯度 上升 法 其 跟 梯度 下 降法 
的 唯一 区别 就是 和 之间 的 减号 变成 了 
加号 前者 求 最大值 后者 求 最小值 代码 基本 脱胎 
于 机器学习 实战 这本书 但是 有 改动 1 普通 的 
梯度 上升 法 下面 这段 代码 也 就是 开头 提到 
的 那个 造成 误解 的 代码 其 实现 依据 是 
向 量化 求解 并 不是 根据 公式 七 来 的 
所以 如果 对照 公式 七 理解 这段 代码 会 完全 
摸不着头脑 如果/c 对照/p 向/p 量化/v 后θ/nr 也 就是 权重 的 
更新 步骤 会 很容易 理解 普通 的 梯度 上升 法 
import numpy as np import os import pandas as pd 
def loadDataSet # # 运行 脚本 所在 目录 base _ 
dir = os . getcwd # # 记得 添加 header 
= None 否则 会 把 第一 行 当作 头 data 
= pd . read _ table base _ dir + 
r \ lr . txt header = None # # 
dataLen 行 dataWid 列 返回值 是 dataLen = 100 dataWid 
= 3 dataLen dataWid = data . shape # # 
训练 数据集 xList = # # 标签 数据集 lables = 
# # 读取数据 for i in range dataLen row = 
data . values i xList . append row 0 dataWid 
1 lables . append row 1 return xList lables # 
# 逻辑 函数 def sigmoid inX return 1.0 / 1 
+ np . exp inX # # 梯度 上升 函数 
def gradAscent datamatIn classLables # # 把 datamatIn 从 列表 
转换成 矩阵 dataMatrix = np . mat datamatIn # # 
把 列表 转换成 100行 1列 的 矩阵 而 np . 
mat classLables 是 转换 成 1行 100列 的 矩阵 labelMat 
= np . mat classLables . transpose # # 求 
矩阵 的 长宽 m n = np . shape dataMatrix 
# # 步长 可以 自己 设置 alpha = 0.001 # 
# 最 大循环 次数 maxTry = 500 # # 初始化 
向量 2行 1列 的 矩阵 weights = np . ones 
n 1 # # 循环 一定 次数 求 权重 for 
k in range maxTry # # dataMatrix 100行 2列 weights 
是 2行 1列 # # h 是 100行 1列 h 
= sigmoid dataMatrix * weights # # 向量 的 偏差 
error = labelMat h # # dataMatrix . transpose 转换成 
2行 100列 的 矩阵 # # error 是 100行 1列 
# # weights 是 2行 1列 的 值 weights = 
weights + alpha * dataMatrix . transpose * error return 
weights 结果 大于 0.3 的 设置 为 1 正确率 基本 
100% def GetResult dataMat labelMat = loadDataSet weights = gradAscent 
dataMat labelMat dataMatrix = np . mat dataMat # # 
求 的 最后 的 结果 h = sigmoid dataMatrix * 
weights # # 打印 结果 观察 数据 for i in 
range len h print str h i + + str 
labelMat i # print h # print weights # # 
0.08108752 0.1233496 if _ _ name _ _ = = 
_ _ main _ _ GetResult 2 随机 梯度 上升 
发 这个 算法 才是 符合 公式 七 的 算法 但是 
代码 中 并 没有 求和 这步 只有 括号 中的 那 
部分 这 也是 我 开头 说 的 三者 之间 必有 
一个 错误 的 地方 只 包括 核心 部分 其他 部分 
见 上段 代码 结果 大于 0.29 或者 0.26 都 可以 
也 只有 1 2个 分类 错误 weights 0.0868611 0.13086297 # 
# 随机 梯度 上升 算法 def gradAscent datamatIn classLables m 
n = np . shape datamatIn # # 步长 可以 
自己 指定 决定 收敛 速度 alpha = 0.001 # # 
最 大循环 次数 maxTry = 200 # # 初始化 权重 
列表 而 不是 矩阵 weights = np . ones n 
# # 循环 求解 在整个 数据 集上 循环 for k 
in range maxTry # # 对 每行 进行 处理 for 
i in range m # # 每行 向 量化 h 
= sigmoid sum datamatIn i * weights # # 每 
行向量 偏差 error = classLables i h # # 更新 
权重 weights = weights + alpha * error * datamatIn 
i return weights # # 打印 结果 def GetResult dataMat 
labelMat = loadDataSet weights = gradAscent dataMat labelMat m n 
= np . shape dataMat for i in range m 
h = sigmoid sum dataMat i * weights print str 
h + + str labelMat i # print weights 3 
改进 的 随机 梯度 上升 算法 书中 还 讲到 了 
一个 改进 的 随机 梯度 上升 算法 # # 随机 
梯度 上升 函数 def gradAscent datamatIn classLables m n = 
np . shape datamatIn # # 循环 次数 maxTry = 
150 # # 初始化 权重 列表 weights = np . 
ones n # # 循环 求解 for j in range 
maxTry # # 在整个 数据 集上 循环 for i in 
range m # # 跟 新 alpha 即 跟 新 
步长 值 alpha = 4 / 1.0 + j + 
i + 0.01 # # 随机 抽取 一个 下标 randIndex 
= int np . random . uniform 0 m # 
# 对 抽到 下标 的 数据 行 进行 求值 h 
= sigmoid sum datamatIn randIndex * weights # # 求得 
误差值 error = classLables randIndex h # # 更新 权重 
weights = weights + alpha * error * datamatIn randIndex 
return weights 该 算法 每次 都会 调整 步长 值 即 
缓解 了 随着 循环 次数 的 增加 造成 的 特征值 
的 波动 也 保证 了当 j max i 时 步长 
值 的 下降 不是 严格 下降 的 而 避免 参数 
的 严格 下降 在 优化 退火算法 中 常常 用到 5 
. 使用 sklearn 包 中的 逻辑 回归 算法 非 完整 
代码 缺少 部分 在 第一 个 代码段 sklearn 包 中的 
L o g i s t i c R e 
g r e s s i o n 函数 默认 
使用 L2 正则化 防止 过度 拟合 from sklearn . linear 
_ model import L o g i s t i 
c R e g r e s s i o 
n def sk _ lr X _ train y _ 
train model = L o g i s t i 
c R e g r e s s i o 
n model . fit X _ train y _ train 
model . score X _ train y _ train # 
print 权重 model . coef _ return model . predict 
X _ train # # 分类 错了 2个 def GetResult 
dataMat labelMat = loadDataSet pred = sk _ lr dataMat 
labelMat for i in range len pred print str pred 
i + + str labelMat i if _ _ name 
_ _ = = _ _ main _ _ GetResult 
最后 得出 的 预测 结果 就是 0 1 值 跟 
标签 对比 有 两个 分类 错了 6 . 逻辑 回归 
优缺点 优点 计算 代价 不高 易于 理解 和 实现 缺点 
容易 欠 拟合 分类 精度 可能 不高 适用 数据类型 数值 
型 和 标称 型 数据 附录 测试数据 0.017612 14.053064 0 
1.395634 4.662541 1 0.752157 6.538620 0 1.322371 7.152853 0 0.423363 
11.054677 0 0.406704 7.067335 1 0.667394 12.741452 0 2.460150 6.866805 
1 0.569411 9.548755 0 0.026632 10.427743 0 0.850433 6.920334 1 
1.347183 13.175500 0 1.176813 3.167020 1 1.781871 9.097953 0 0.566606 
5.749003 1 0.931635 1.589505 1 0.024205 6.151823 1 0.036453 2.690988 
1 0.196949 0.444165 1 1.014459 5.754399 1 1.985298 3.230619 1 
1.693453 0.557540 1 0.576525 11.778922 0 0.346811 1.678730 1 2.124484 
2.672471 1 1.217916 9.597015 0 0.733928 9.098687 0 3.642001 1.618087 
1 0.315985 3.523953 1 1.416614 9.619232 0 0.386323 3.989286 1 
0.556921 8.294984 1 1.224863 11.587360 0 1.347803 2.406051 1 1.196604 
4.951851 1 0.275221 9.543647 0 0.470575 9.332488 0 1.889567 9.542662 
0 1.527893 12.150579 0 1.185247 11.309318 0 0.445678 3.297303 1 
1.042222 6.105155 1 0.618787 10.320986 0 1.152083 0.548467 1 0.828534 
2.676045 1 1.237728 10.549033 0 0.683565 2.166125 1 0.229456 5.921938 
1 0.959885 11.555336 0 0.492911 10.993324 0 0.184992 8.721488 0 
0.355715 10.325976 0 0.397822 8.058397 0 0.824839 13.730343 0 1.507278 
5.027866 1 0.099671 6.835839 1 0.344008 10.717485 0 1.785928 7.718645 
1 0.918801 11.560217 0 0.364009 4.747300 1 0.841722 4.119083 1 
0.490426 1.960539 1 0.007194 9.075792 0 0.356107 12.447863 0 0.342578 
12.281162 0 0.810823 1.466018 1 2.530777 6.476801 1 1.296683 11.607559 
0 0.475487 12.040035 0 0.783277 11.009725 0 0.074798 11.023650 0 
1.337472 0.468339 1 0.102781 13.763651 0 0.147324 2.874846 1 0.518389 
9.887035 0 1.015399 7.571882 0 1.658086 0.027255 1 1.319944 2.171228 
1 2.056216 5.019981 1 0.851633 4.375691 1 1.510047 6.061992 0 
1.076637 3.181888 1 1.821096 10.283990 0 3.010150 8.401766 1 1.099458 
1.688274 1 0.834872 1.733869 1 0.846637 3.849075 1 1.400102 12.628781 
0 1.752842 5.468166 1 0.078557 0.059736 1 0.089392 0.715300 1 
1.825662 12.693808 0 0.197445 9.744638 0 0.126117 0.922311 1 0.679797 
1.220530 1 0.677983 2.556666 1 0.761349 10.693862 0 2.168791 0.143632 
1 1.388610 9.341997 0 0.317029 14.739025 0View Code 