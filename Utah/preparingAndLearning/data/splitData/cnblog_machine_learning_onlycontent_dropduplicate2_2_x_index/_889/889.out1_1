高斯 判别分析 GDA 简介 首先 高斯 判别分析 的 作用 也 
是 用于 分类 对于 两类 样本 其 服从 伯努利 分布 
而对 每个 类 中的 样本 假 定都 服从 高斯分布 则有 
\ y \ \ sim \ Bernouli \ phi \ 
\ x | y = 0 \ \ sim \ 
N \ mu _ 0 \ Sigma \ \ x 
| y = 1 \ \ sim \ N \ 
mu _ 1 \ Sigma \ 这样 根据 训练样本 估计 
出 先验概率 以及 高斯分布 的 均值 和 协方差 矩阵 注意 
这里 两类 内部 高斯分布 的 协方差 矩阵 相同 即可 通过 
如下 贝叶斯 公式 求出 一个 新 样本 分别 属于 两类 
的 概率 进而 可 实现 对 该 样本 的 分类 
\ \ begin { aligned } p y | x 
= \ frac { p x | y p y 
} { p x } \ end { aligned } 
\ \ \ begin { aligned } y = \ 
underset { y } { argmax } \ { p 
y | x } =   \ underset { y 
} { argmax } { \ { \ frac { 
p x | y p y } { p x 
} } } = \ underset { y } { 
argmax } { \ p x | y p y 
} \ end { aligned } \ GDA 详细 推导 
那么 高斯 判别分析 的 核心 工作 就是 估计 上述 未知量 
\ \ phi \ mu _ 0 \ mu _ 
1 \ Sigma \ 如何 来 估计 这些 参数 又该 
最大 似 然 估计 上场 了 其 对数 似 然 
函数 为 \ \ begin { aligned } l \ 
phi \ mu _ 0 \ mu _ 1 \ 
Sigma & = log { \ prod _ { i 
= 1 } ^ m { p x ^ { 
i } y ^ { i } } } = 
log { \ prod _ { i = 1 } 
^ m { p x ^ { i } | 
y ^ { i } p y ^ { i 
} } } \ \   & = \ sum 
_ { i = 1 } ^ m { log 
\ p x ^ { i } | y ^ 
{ i } } + \ sum _ { i 
= 1 } ^ m { log \ p y 
^ { i } } \ \ & = \ 
sum _ { i = 1 } ^ m { 
log \ \ p x ^ { i } | 
y ^ { i } = 0 ^ { 1 
y ^ { i } } * p x ^ 
{ i } | y ^ { i } = 
1 ^ { y ^ { i } } \ 
} + \ sum _ { i = 1 } 
^ m { log \ p y ^ { i 
} } \ \ & = \ sum _ { 
i = 1 } ^ m { 1 y ^ 
{ i } log \ p x ^ { i 
} | y ^ { i } = 0 } 
+ \ sum _ { i = 1 } ^ 
m { { y ^ { i } } log 
\ p x ^ { i } | y ^ 
{ i } = 1 } + \ sum _ 
{ i = 1 } ^ m { log \ 
p y ^ { i } } \ end { 
aligned } \ 注意 此 函数 第一 部分 只和 \ 
\ mu _ 0 \ Sigma \ 有关 第二 部分 
只和 \ \ mu _ 1 \ Sigma \ 有关 
最后 一 部分 只和 \ \ phi \ 有关 最大化 
该 函数 首先 求 \ \ phi \ 先 对其 
求 偏 导数 \ \ begin { aligned } \ 
frac { \ partial \ l \ phi \ mu 
_ 0 \ mu _ 1 \ Sigma } { 
\ partial \ phi } & = \ frac { 
\ sum _ { i = 1 } ^ m 
{ log \ p y ^ { i } } 
} { \ partial \ phi } \ \ & 
=   \ frac { \ partial \ sum _ 
{ i = 1 } ^ m { log \ 
\ phi ^ { y ^ { i } } 
1 \ phi ^ { 1 y ^ { i 
} } } } { \ partial \ phi } 
\ \ & = \ frac { \ partial \ 
sum _ { i = 1 } ^ m { 
y ^ { i } \ log \ \ phi 
+ 1 y ^ { i } log 1 \ 
phi } } { \ partial \ phi } \ 
\ & = \ sum _ { i = 1 
} ^ m { y ^ { i } \ 
frac { 1 } { \ phi } 1 y 
^ { i } \ frac { 1 } { 
1 \ phi } } \ \ & = \ 
sum _ { i = 1 } ^ m { 
I y ^ { i } = 1 \ frac 
{ 1 } { \ phi } I y ^ 
{ i } = 0 \ frac { 1 } 
{ 1 \ phi } } \ end { aligned 
} \ 此处 \ I \ 为 指示 函数 令 
其为 0 可求 解出 \ \ begin { aligned } 
\ phi = \ frac { I y ^ { 
i } = 1 } { I y ^ { 
i } = 0 + I y ^ { i 
} = 1 } = \ frac { I y 
^ { i } = 1 } { m } 
\ end { aligned } \ 同样地 对 \ \ 
mu _ 0 \ 求 偏 导数 \ \ begin 
{ aligned } \ frac { \ partial \ l 
\ phi \ mu _ 0 \ mu _ 1 
\ Sigma } { \ partial \ mu _ 0 
} & = \ frac { \ partial \ sum 
_ { i = 1 } ^ m { 1 
y ^ { i } log \ p x ^ 
{ i } | y ^ { i } = 
0 } } { \ partial \ mu _ 0 
} \ \ & = \ frac { \ partial 
\ sum _ { i = 1 } ^ m 
{ 1 y ^ { i } log \ frac 
{ 1 } { \ sqrt { 2 \ pi 
^ n | \ Sigma | } } \ frac 
{ 1 } { 2 } x ^ { i 
} \ mu _ 0 ^ T \ Sigma ^ 
{ 1 } x ^ { i } \ mu 
_ 0 } } { \ partial \ mu _ 
0 } \ \ & = \ sum _ { 
i = 1 } ^ m { 1 y ^ 
{ i } \ Sigma ^ { 1 } x 
^ { i } \ mu _ 0 } \ 
\ & = \ sum _ { i = 1 
} ^ m { I y ^ { i } 
= 0 \ Sigma ^ { 1 } x ^ 
{ i } \ mu _ 0 } \ end 
{ aligned } \ 令 其为 0 可求 解得 \ 
\ begin { aligned } \ mu _ 0 = 
\ frac { \ sum _ { i = 1 
} ^ m { I y ^ { i } 
= 0 x ^ { i } } } { 
\ sum _ { i = 1 } ^ m 
{ I y ^ { i } = 0 } 
} \ end { aligned } \ 根据 对称性 可直接 
得出 \ \ begin { aligned } \ mu _ 
1 = \ frac { \ sum _ { i 
= 1 } ^ m { I y ^ { 
i } = 1 x ^ { i } } 
} { \ sum _ { i = 1 } 
^ m { I y ^ { i } = 
1 } } \ end { aligned } \ 下 
面对 \ \ Sigma \ 求 偏 导数 由于 似 
然 函数 只有 前面 两 部分 与 \ \ Sigma 
\ 有关 则将 前 两部分 改写 如下 \ \ begin 
{ aligned } & \ sum _ { i = 
1 } ^ m { 1 y ^ { i 
} log \ p x ^ { i } | 
y ^ { i } = 0 } + \ 
sum _ { i = 1 } ^ m { 
{ y ^ { i } } log \ p 
x ^ { i } | y ^ { i 
} = 1 } \ \ & = \ sum 
_ { i = 1 } ^ m { 1 
y ^ { i } log \ frac { 1 
} { \ sqrt { 2 \ pi ^ n 
| \ Sigma | } } \ frac { 1 
} { 2 } x ^ { i } \ 
mu _ 0 ^ T \ Sigma ^ { 1 
} x ^ { i } \ mu _ 0 
} + \ sum _ { i = 1 } 
^ m { { y ^ { i } } 
log \ frac { 1 } { \ sqrt { 
2 \ pi ^ n | \ Sigma | } 
} \ frac { 1 } { 2 } x 
^ { i } \ mu _ 1 ^ T 
\ Sigma ^ { 1 } x ^ { i 
} \ mu _ 1 } \ \ & = 
\ sum _ { i = 1 } ^ m 
{ log \ frac { 1 } { \ sqrt 
{ 2 \ pi ^ n | \ Sigma | 
} } } \ frac { 1 } { 2 
} \ sum _ { i = 1 } ^ 
m { x ^ { i } \ mu _ 
{ y ^ { i } } ^ T \ 
Sigma ^ { 1 } x ^ { i } 
\ mu _ { y ^ { i } } 
} \ \ & = \ sum _ { i 
= 1 } ^ m { \ frac { n 
} { 2 } log 2 \ pi \ frac 
{ 1 } { 2 } log | \ Sigma 
| } \ frac { 1 } { 2 } 
\ sum _ { i = 1 } ^ m 
{ x ^ { i } \ mu _ { 
y ^ { i } } ^ T \ Sigma 
^ { 1 } x ^ { i } \ 
mu _ { y ^ { i } } } 
  \ end { aligned } \ 进 而有 \ 
\ begin { aligned } \ frac { \ partial 
\ l \ phi \ mu _ 0 \ mu 
_ 1 \ Sigma } { \ partial \ Sigma 
} & = \ frac { 1 } { 2 
} \ sum _ { i = 1 } ^ 
m \ frac { 1 } { | \ Sigma 
| } | \ Sigma | \ Sigma ^ { 
1 } \ frac { 1 } { 2 } 
\ sum _ { i = 1 } ^ m 
x ^ { i } \ mu _ { y 
^ { i } } x ^ { i } 
\ mu _ { y ^ { i } } 
^ T \ frac { \ partial \ Sigma ^ 
{ 1 } } { \ partial \ Sigma } 
\ \ & = \ frac { m } { 
2 } \ Sigma ^ { 1 } \ frac 
{ 1 } { 2 } \ sum _ { 
i = 1 } ^ m x ^ { i 
} \ mu _ { y ^ { i } 
} x ^ { i } \ mu _ { 
y ^ { i } } ^ T \ Sigma 
^ { 2 }   \ end { aligned } 
\ 这里 推导 用 到了 \ \ begin { aligned 
} \ frac { \ partial | \ Sigma | 
} { \ partial \ Sigma } = | \ 
Sigma | \ Sigma ^ { 1 } \ end 
{ aligned } \ \ \ begin { aligned } 
\ frac { \ partial \ Sigma ^ { 1 
} } { \ partial \ Sigma } = \ 
Sigma ^ { 2 } \ end { aligned } 
\ 令 其为 0 从而 求得 \ \ begin { 
aligned } \ Sigma = \ frac { 1 } 
{ m } \ sum _ { i = 1 
} ^ m x ^ { i } \ mu 
_ { y ^ { i } } x ^ 
{ i } \ mu _ { y ^ { 
i } } ^ T \ end { aligned } 
\ 上面 的 推导 似乎 很 复杂 但 其 结果 
却 是 非常 简洁 通过 上述 公式 所有 的 参数 
都 已经 估计 出来 需要 判断 一个 新 样本 x 
时 可 分别 使用 贝叶斯 求出 p y = 0 
| x 和p/nr y = 1 | x 取 概率 
更大 的 那个 类 实际 计算 时 我们 只 需要 
比 大小 那么 贝叶斯 公式 中 分母 项 可以 不 
计算 由于 2个 高斯 函数 协方差 矩阵 相同 则 高斯分布 
前面 那 相同 部分 也 可以 忽略 实际上 GDA 算法 
也 是 一个 线性 分类器 根据 上面 推导 可以 知道 
GDA 的 分界线 面 的 方程 为 \ \ begin 
{ aligned } 1 \ phi exp x \ mu 
_ 0 ^ T \ Sigma ^ { 1 } 
x \ mu _ 0 = { \ phi } 
exp x \ mu _ 1 ^ T \ Sigma 
^ { 1 } x \ mu _ 1 \ 
end { aligned } \ 取 对数 展开 后 化解 
可得 \ \ begin { aligned } 2x ^ T 
\ Sigma ^ { 1 } \ mu _ 1 
\ mu _ 0 = \ mu _ 1 ^ 
T \ Sigma ^ { 1 } \ mu _ 
1 \ mu _ 0 ^ T \ Sigma ^ 
{ 1 } \ mu _ 0 + log \ 
\ phi log 1 \ phi \ end { aligned 
} \ 若 \ \ begin { aligned } A 
= 2 \ Sigma ^ { 1 } \ mu 
_ 1 \ mu _ 0 = a _ 1 
a _ 2 . . . a _ n \ 
quad b = \ mu _ 1 ^ T \ 
Sigma ^ { 1 } \ mu _ 1 \ 
mu _ 0 ^ T \ Sigma ^ { 1 
} \ mu _ 0 + log \ \ phi 
log 1 \ phi \ end { aligned } \ 
则 \ \ begin { aligned } a _ 1x 
_ 1 + a _ 2x _ 2 + . 
. . + a _ nx _ n = b 
\ end { aligned } \ 这 就是 GDA 算法 
的 线性 分 界面 GDA 实现 这里 也 采用 前面 
讲 逻辑 回归 生成 的 数据 来 进行 实验 直接 
load 进来 进行 处理 详见 逻辑 回归 GDA 训练 代码 
如下 1 % mu = mu0 mu1 2 function mu 
sigma phi = GDA _ train Sample 3 m n 
= size Sample % m 个 样本 每个 n 维 
4 Y = Sample end 5 X = Sample 1 
end 1 6 7 idx = find Y = = 
0 8 mu 1 = mean X idx 9 10 
idx2 = find Y = = 1 11 mu 2 
= mean X idx2 12 13 phi = size idx2 
1 / m 14 15 sigma = zeros n 1 
16 for i = 1 m 17 x = X 
i 18 muc = mu Y i + 1 19 
sigma = sigma + x muc * x muc 20 
end 21 sigma = sigma / m 22 endView Code 
测试代码 1 function GDA _ test 2 clear 3 close 
all 4 clc 5 load log _ data . mat 
6 mu sigma phi = GDA _ train Sample 7 
8 % 显示 结果 以下 代码 不 通用 样本 维数 
增加 时 显示 不 可用 9 figure 10 idx = 
find Sample 3 = = 1 11 plot Sample idx 
1 Sample idx 2 g * hold on 12 idx 
= find Sample 3 = = 0 13 plot Sample 
idx 1 Sample idx 2 ro hold on 14 15 
t1 t2 = meshgrid min Sample 1 . 1 max 
Sample 1 min Sample 2 . 1 max Sample 2 
16 G1 = Gaussian mu 1 sigma t1 t2 1 
phi 17 G2 = Gaussian mu 2 sigma t1 t2 
phi 18 contour t1 t2 G1 hold on 19 contour 
t1 t2 G2 hold on 20 21 A = 2 
* inv sigma * mu 2 mu 1 22 b 
= mu 2 * inv sigma * mu 2 mu 
1 * inv sigma * mu 1 + log phi 
log 1 phi 23 x1 = min Sample 1 . 
1 max Sample 1 24 x2 = b A 1 
* x1 / A 2 25 plot x1 x2 m 
26 end 27 28 function G = Gaussian mu sigma 
t1 t2 phi 29 for i = 1 size t1 
1 30 for j = 1 size t1 2 31 
x = t1 i j t2 i j 32 z 
= x mu * inv sigma * x mu 33 
G i j = phi * exp z / 0.001 
34 end 35 end 36 endView Code 训练 结果 如下 
训练样本 中 正负 样本 均为 100个 故 \ \ phi 
= 0.5 \ 改变 正负 样本 数量 即 相当于 改变 
先验概率 则 实验 结果 如下 相应 的 \ \ phi 
\ 的 值 显示 在 图像 标题 算 法分析 1 
. 与 逻辑 回归 的 关系 根据 上面 的 结果 
以及 贝叶斯 公式 可有 \ \ begin { aligned } 
p y = 1 | x & = \ frac 
{ p x | y = 1 p y = 
1 } { p x } \ \ & = 
\ frac { N \ mu _ 1 \ Sigma 
\ phi } { N \ mu _ 0 \ 
Sigma 1 \ phi + N \ mu _ 1 
\ Sigma \ phi } \ \ & = 1 
/ { 1 + \ frac { N \ mu 
_ 0 \ Sigma } { N \ mu _ 
1 \ Sigma } \ frac { 1 \ phi 
} { \ phi } } \ end { aligned 
} \ 而 \ \ begin { aligned } \ 
frac { N \ mu _ 0 \ Sigma } 
{ N \ mu _ 1 \ Sigma } & 
= exp \ { x \ mu _ 0 ^ 
T \ Sigma ^ { 1 } x \ mu 
_ 0 x \ mu _ 1 ^ T \ 
Sigma ^ { 1 } x \ mu _ 1 
\ } \ \ & = exp \ { 2 
\ mu _ 1 \ mu _ 0 ^ T 
\ Sigma ^ { 1 } x + \ mu 
_ 0 ^ T \ Sigma \ mu _ 0 
\ mu _ 1 ^ T \ Sigma \ mu 
_ 1 \ } \ end { aligned } \ 
那么 令 \ \ begin { aligned } 2 \ 
Sigma ^ { 1 } \ mu _ 1 \ 
mu _ 0 = \ theta _ 1 \ theta 
_ 2 . . . \ theta _ n ^ 
T \ \ \ theta _ 0 = \ mu 
_ 0 ^ T \ Sigma \ mu _ 0 
\ mu _ 1 ^ T \ Sigma \ mu 
_ 1 + log \ frac { 1 \ phi 
} { \ phi } \ \ \ end { 
aligned } \ 则 \ \ begin { aligned } 
p y = 1 | x = \ frac { 
1 } { 1 + exp \ theta _ 0 
+ \ theta _ 1x _ 1 + \ theta 
_ 2x _ 2 + . . . + \ 
theta _ nx _ n } \ end { aligned 
} \ 这不 就是 逻辑 回归 的 形式 么 在 
推导 逻辑 回归 的 时候 我们 并 没有 假设 类 
内 样本 是 服从 高斯分布 的 因而 GDA 只是 逻辑 
回归 的 一个 特例 其 建立 在 更强 的 假设 
条 故 两者 效果 比较 a . 逻辑 回归 是 
基于 弱 假设 推导 的 则 其 效果 更 稳定 
适用范围 更广 b . 数据 服从 高斯分布 时 GDA 效果 
更好 c . 当 训练 样本数 很大 时 根据 中心 
极限 定理 数据 将 无限 逼近 于 高斯分布 则 此时 
GDA 的 表现 效果 会 非常 好 2 . 为何 
要 假设 两类 内部 高斯分布 的 协方差 矩阵 相同 从直/nr 
观上 讲 假设 两个 类 的 高斯分布 协方差 矩阵 不同 
会 更加 合理 在 混合 高斯 模型 中 就是 如此 
假设 的 而且 可 推导 出 类似 上面 简洁 的 
结果 假定 两个 类有 相同 协方差 矩阵 分析 具有 以下 
几点 影响 A ． 当 样本 不充分 时 使用 不同 
协方差 矩阵 会 导致 算法 稳定性 不够 过少 的 样本 
甚至 导致 协方差 矩阵 不可逆 那么 GDA 算法 就 没法 
进行 B ． 使用 不同 协方差 矩阵 最终 GDA 的 
分 界面 不是 线性 的 同样 也 推导 不出 GDA 
的 逻辑 回归 形式 3 . 使用 GDA 时对/nr 训练样本 
有何 要求 首先 正负 样本数 的 比例 需要 符合 其 
先验概率 若是 预先 明确 知道 两类 的 先验概率 那么 可 
使用 此 概率 来 代替 GDA 计算 的 先验概率 若 
是 完全 不 知道 则 可以 公平 地 认为 先验概率 
为 50% 其次 样本数 必须 不 小于 样本 特征 维数 
否则 会 导致 协方差 矩阵 不可逆 按照 前面 分析 应该 
是 多多益善 