注 其实 自 认为 还是 非常 喜欢 数学 的 但是 
对于 复杂 的 公式 还是 有种 恐惧感 就像 最 开始 
学 英语 时 对 英语 的 感觉 一样 但是 数学 
与 英语 不同 的 地方 在于 你 可以 尽情 的 
刨根问底 从最/nr 基础 的 知识 开始 了解 直到 最终 把 
一个 符号 或者 公式 的 含义 弄明白 在 机器 学习 
的 过程 中 也会 碰到 各种各样 的 符号 尤其 是 
遇到 多 参数 多 样本 的 情况 时 更是 让 
人 眼花缭乱 最近 学习 完 coursera 上 吴恩 达 老师 
的 机器学习 前 两周 的 课程 有种 豁然开朗 的 感觉 
在此 做 一个 小 结 1 . 一些 基本 概念 
展示 了 机器 学习 的 基本 过程 对于 学 过 
高中 数学 的 人 来说 解方程 是 我们 再 熟悉 
不过 的 事情 了 例如 一个二元 一次 方程组 其解 如果 
存在 的话 就是 平面 上 两条 直线 的 交点 此时 
方程 以及 参数 方程 的 系数 都是 确定 的 我们 
通常 意义 上 的 算法 相当于 一个 定义 好 的 
函数 中的 $ h $ 应用 该 算法 的 过程 
就是 带入 不同 的 自变量 求 函数值 的 过程 然而 
在 机器学习 算法 中 最大 的 不同 在于 没有 一个 
定义 好 的 函数 而是 需要 通过 收集 到 的 
数据 训练 出 一个 函数 中 从 Training Set 到 
$ h $ 的 过程 本质上 是 对 训练 集中 
数据 的 一种 概括 和 总结 例如 只有 两个 参数 
的 线性 回归 就是 在 二维 平面 上 找 一条 
适合 描述 训练 集中 样本点 变化 规律 的 直线 的 
过程 传统 的 确定性 算法 与 机器学习 算法 的 区别 
可以用 下图 表示 传统 编程 与 机器学习 之间 的 差别 
source 如果 将 一个 程序 大致 分为 三 个 部分 
输入 输出 和 算法 那么 传统 编程 中 已知 的 
是 输入 和 算法 需 要求 输出 机器学习 中 则是 
已知 输入 和 输出 需要 通过 训练 学习 来 得到 
有 泛化 能力 的 算法 开普勒 通过 分析 第谷 留下 
的 大约 20年 的 天文 观测 数据 建立 的 开普勒 
三大 定律 孟德尔 通过 分析 他 自己 在 豌豆 实验 
中 获得 的 不同 性状 数据 建立 了 孟德尔 遗传 
定律 这 两位 科学家 所做 的 事情 其本质 也是 对 
实验 数据 的 概括 总结 最终 提炼出 了 具有 高度 
概括 和 普适 价值 的 基本 定律 这样 想想 机器学习 
算法 确实有 了 几分 智能 的 味道 训练 集 Training 
Set 为了 研究 一个 变量 x 与 另一个 变量 y 
的 关系 而 通过 观察 测量 等 方式 获得 的 
一组 数据 这组 数据 中 收集 了 x 和 与之 
对应 的 y 一个 数据 对 x y 例如 我们 
要 研究 房屋 面积 x 和 售价 y 之间 的 
关系 每 观察 一套 已 出售 的 房屋 就 得到 
一个 数据 对 x y 观察 10套 已 出售 的 
房屋 就 可以 得到 10个 这样 的 数据 对 这时 
就 得到 了 一个 用来 研究 房屋 面积 和 售价 
之间 的 关系 的 训练 集了 虽然 样本 量 比较 
小 这些 数据集 一般 采集 自 现实 环境 中 属于 
现象 我们 的 目的 是 透过 现象 看 本质 样本 
Sample 训练 集中 采集 数据 的 对象 就是 一个 样本 
例如 一套 已 出售 的 房屋 模型 Model 由于 某些 
历史 原因 机器学习 中的 模型 也 被 叫做 假设 hypothesis 
h 这个 h 就是 我们 透过 现象 想要 寻找 的 
本质 建立 模型 的 过程 通常 就 是 确定 一个 
函数 表达式 的 过程 是否 还 记得 寒假作业 中的 这类 
题目 观察 一组 数 写出 下 一个 数 是 什么 
最 常见 的 模型 是 回归模型 线性 回归 或 逻辑 
回归 等 例如 我们 假设 房屋 面积 与 售价 之间 
的 关系 是 一个 线性 回归模型 则 可以 写成 $ 
$ h \ theta = \ theta _ 0   
+ \ theta _ 1 x \ qquad \ ldots 
  1 $ $ 其中 h 是 函数 可能 更 
习惯 叫做 y 但在 机器学习 中 y 一般 表示 已知 
的 函数值 即 后面 的 因变量 这里 的 h 相当于 
预测 得到 的 y θ 是 函数 的 参数 也 
可以 看做 是 每个 自变量 的 权重 权重 越大 对 
y 的 影响 也 越大 x 是 自变量 训练 模型 
Training Model 选定 模型 选择 合适 的 模型 需要 丰富 
的 经验 后 函数 的 一般 形式 就 确定 了 
通常 所说 的 训练 模型 是 指 利用 训练 集 
求解 函数 的 待定 参数 的 过程 上面 的 1 
式 与 直线 方程 的 一般 形式 y = ax 
+ b 是 相同 的 这里 不过 换了 一种 写法 
此时 我们 知道 模型 是 一条 直线 为了 确定 这 
条 直线 的 确定 方程 我们 需要 求出 两个 未知 
的 参数 θ 0 截距 和θ1/nr 斜率 如果 训练 集中 
只有 两个 样本 那就 只是 求 一个二元 二次 方程组 就 
解决 问题 了 特征 Feature 特征 就是 在 一个 模型 
中 所有 想 研究 的 自变量 x 的 集合 例如 
我们 在 研究 房屋 售价 的 模型 中 所有 可能 
影响 售价 的 因素 都 可以 看成 是 一个 特征 
房屋 面积 所在城市 房间 个数 等 在 建立 模型 的 
过程 中 特征 的 选择 是 一个 大 学问 甚至 
有 专门 的 分支 来 研究 特征选择 或 特征 表示 
2 . 训练 集 的 表示 上面 提到 过 训练 
集 就是 许多 的 x y 数据 对 的 集合 
其中 x 是 因变量 y 是 自变量 通常 认为 x 
的 变化 引起 了 y 的 改变 即 x 的 
值 决定了 y 的 值 在 预测 房屋 价格 的 
模型 中 假如 我们 能 找到 所 有影响 房屋 价格 
的 因素 所有 的 x 并且 确定 各 个 因素 
准确 的 参数 θ 那么 理论上 可以 准确 的 预测 
出 任何 房屋 的 价格 y 2.1 单/n 因素/n 训练/vn 
集中/v 自变量/l 的/uj 表示/v 方法/n 单/n 因素/n 相当于/v 方程/n 中/f 
只有/c 一个/m 自变量/l 这个 自变量 可以 用 一个 小写字母 x 
来 表示 如果 收集 了 多个 样本 则 通过 在 
右上角 添 加带 括号 的 角标 的 方式 区分 表示 
为 x 1 x 2 . . . x m 
其中 m 表示 样本 的 个数 矩阵 的 表示 向量 
一般 用 小写 字母 表示 矩阵 用 大写字母 表示 所有 
单 因素 样本 中的 x 可以 用 一个 m x 
1 m 行 1列 的 列 向量 x 小写字母 只有 
一列 的 矩阵 就是 一个 列 向量 来 表示 $ 
$ x = \ begin { pmatrix } x ^ 
{ 1 } \ \ x ^ { 2 } 
\ \ \ vdots \ \ x ^ { m 
} \ end { pmatrix } $ $ 2.2 多 
因素 训练 集中 自变量 的 表示 方法 多 因素 相当于 
方程 中有 多个 自变量 多个 feature 不同 的 自变量 之间 
使用 右下角 添加 不带 括号 的 角标 来 区分 表示 
为 x1 x2 . . . xn 其中 n 表示 
feature 的 个数 当 存在 多 个 样本 时 可以 
用 一个 m x n m 行 n 列 的 
矩阵 X 大写字母 来 表示 $ $ X = \ 
begin { bmatrix } x _ { 1 } ^ 
{ 1 } & x _ { 2 } ^ 
{ 1 } & \ ldots &   x _ 
{ n } ^ { 1 } \ \ x 
_ { 1 } ^ { 2 } &   
x _ { 2 } ^ { 2 } & 
  \ ldots   &   x _ { n 
} ^ { 2 } \ \   \ vdots 
& \ vdots & \ ddots & \ vdots   
\ \ x _ { 1 } ^ { m 
} &   x _ { 2 } ^ { 
m } &   \ ldots   &   x 
_ { n } ^ { m }   \ 
end { bmatrix } $ $ 2.3 训练 集中 因变量 
的 表示 方法 无论是 单 因素 还是 多 因素 每一个 
样本 中都 只 包含 一个 因变量 y 因此 只 需要 
区分 不同 样 本间 的 y y 1 y 2 
. . . y m 其中 m 表示 样本 的 
个数 用 列 向量 y 表示 为 $ $ y 
= \ begin { pmatrix } y ^ { 1 
} \ \ y ^ { 2 } \ \ 
\ vdots \ \ y ^ { m } \ 
end { pmatrix } $ $ 3 . 参数 的 
表示 也许 是 某种 约定 在 机器 学习 中 一般 
都是用/nr θ 来 表示 参数 参数 是 自变量 X 的 
参数 也 可以 看做 是 每个 自变量 的 权重 权重 
越大 的 自变量 对 y 的 影响 也 越大 理论上 
有/v 多少/m 个/q 自变量/l 就/d 有/v 多少/m 个/q 参数/n 但 
就 像在 直线 方程 y = ax + b 中 
表现 出来 的 那样 除了 x 的 参数 a 还有 
一个 常数项 b 因 此参数 一般比 自变量 的 个数 多 
一个 当 有n个/nr 自变量 的 时候 会有 n + 1个 
参数 最终 的 模型 是 由 一个 特定 的 方程 
来 表示 的 方程 中的 未知 参数 是 在 训练 
模型 的 过程 中 确定 的 这些 参数 对于 所有 
的 样本 都是/nr 相同 的 例如 第一 个 样本 x 
1 中 的 第一 个 自变量 x1 的 参数 与 
任意 其他 样本 x i 中 第一 个 自变量 x1 
的 参数 是 相同 的 因此 不用 区分 样 本间 
的 参数 只用 区分 不同 自变量 之间 的 参数 可以 
使用 一个 n + 1 维 的 列 向量 θ 
来 表示 所有 的 参数 $ $ \ theta = 
\ begin { pmatrix } \ theta _ 0 \ 
\ \ theta _ 1   \ \ \ vdots 
\ \ \ theta _ n   \ end { 
pmatrix } $ $ 4 . 模型 的 表示 这里 
说 的 模型 就 是 一个 特定 的 函数 上面 
已经 提过 模型 一般 使用 h 来 表示 下面 用 
线性 回归模型 来 举例说明 模型 的 符号 表示 4.1 直接 
表示 直接 表示 方法 是 我们 在 没有 学习 线性代数 
之前 的 代数 表示 方式 单 变量 线性 回归方程 $ 
$ h _ \ theta x = \ theta _ 
0 + \ theta _ 1 x $ $ 多 
变量 线性 回归方程 $ $ h _ \ theta x 
= \ theta _ 0 + \ theta _ 1 
x _ 1 + \ theta _ 2 x _ 
2 +   \ theta _ 3 x _ 3 
+ \ ldots +   \ theta _ n x 
_ n $ $ 4.2 矩阵 表示 学习 了 线性代数 
后 可以 使用 矩阵 来 表示 上面 的 方程 不仅 
表示 起来 方便 直接 进 行矩阵 运算 效率 也 更 
高效 在 这里 需要 特别 说明 的 一点 是 为了 
配合 矩阵 的 表示 在 上面 的 方程 中 添加 
了 x0 并且 x0 = 1 且 将 θ 0 
作为 x0 的 参数 单 变量 / 多 变量 线性 
回归方程 $ $ h _ \ theta x = X 
\ theta = \ begin { bmatrix } x _ 
{ 0 } ^ { 1 } & x _ 
{ 1 } ^ { 1 } & \ ldots 
&   x _ { n } ^ { 1 
} \ \ x _ { 0 } ^ { 
2 } &   x _ { 1 } ^ 
{ 2 } & \ ldots   &   x 
_ { n } ^ { 2 } \ \ 
\ vdots & \ vdots & \ ddots & \ 
vdots   \ \ x _ { 0 } ^ 
{ m } &   x _ { 1 } 
^ { m } & \ ldots   &   
x _ { n } ^ { m }   
\ end { bmatrix } \ begin { bmatrix } 
\ theta _ 0 \ \ \ theta _ 1 
  \ \ \ vdots   \ \ \ theta 
_ n   \ end { bmatrix } $ $ 
此时 X 是 一个 m x n + 1 的 
矩阵 每 一行 表示 一个 样本 每 一列 表示 一个 
特征 结果 是 一个 m x 1 的 列 向量 
其中 m 表示 样本 的 个数 n 表示 变量 的 
个数 X 中的 每 一列 具有 同样 的 参数 一列 
表示 在 不同 的 样本 中 同 一个 特征 的 
取值 当 只有 一个 样本 多个 变量 时 还 可以 
表示 为 $ $ h _ \ theta x = 
\ theta ^ T   x = \ begin { 
bmatrix } \ theta _ 0 & \ theta _ 
1 & \ ldots & \ theta _ n   
\ end { bmatrix } \ begin { bmatrix } 
x _ 0 \ \ x _ 1   \ 
\ \ vdots \ \ x _ n   \ 
end { bmatrix } $ $ 此时 x 是 一个 
n + 1 维 的 列 向量 每 一行 表示 
一个 变量 的 值 Referencehttps / / www . coursera 
. org / learn / machine learninghttp / / blog 
. sciencenet . cn / blog 100379 1037923 . htmlhttps 
/ / www . sumologic . com / blog / 
devops / machine learning deep learning / 