AS WE ALL KNOW 学 机器 学习 的 一般 都 
是从 python + sklearn 开始 学 适用于 数据 量 不大 
的 场景 这里 就 别 计较 不大 具体 指标 是 
啥 了 哈哈 数据量 大了 就 需要 用 到 其他 
技术 了 如 spark tensorflow 当然 也 有 其他 技术 
此处 略过 一坨 字 . . . 先来 看看 如何 
让 这 3个 集成 起来 吧 WINDOWS 环境 pycharm python 
开发环境   pyspark . cmd REPL 命令行 接口 spark spark 
驱动 MASTER 等 download Anaconda latest version which 64bit support 
for windows 这里 必须 安装 64位 版本 的 Anaconda 因为 
后面 tensorflow 只 支持 64位 的 https / / www 
. continuum . io / downloads / 安装 Anaconda 都是 
默认 选项 就行 dowload pycharm from jetbrain site and install 
please do it by yourself 这个 很 简单 直接 略过 
接下来 是 下载 spark 我 下 的 是 最新版 2 
. 1.0 的   http / / spark . apache 
. org / downloads . html 解压缩 后把它/nr 复制 到 
一个 容易 找 的 目录 我 这是 C \ spark 
2 . 1.0 bin hadoop2 . 7 这个 时候 如果 
直接 双击 bin 下 的 spark shell . cmd 文件 
的话 是 会 报错 的 主要 原因 是 没有 winutils 
. exe 这 东西 用来 在 windows 环境 下 模拟 
文件 操作 的 因此 还 需要 做 几个 小 步骤 
才能 正常 启动 1 . 设置 一个 假 的 hadoop 
目录 在 这个 目录 的 bin 下放 刚才 说 的 
那个 winutils . exe 文件 需要 自己 创建 bin 目录 
2 . 设置 环境变量 HADOOP _ HOME 值 为 这个 
假 的 hadoop 目录 3 . 拷贝 winutils . exe 
到 这个 bin 里 下载 OK 这时 可以 双击 spark 
shell . cmd 了 如下 HOHO = = = = 
我们 不是 要 搞 PYTHON 环境 嘛 怎么搞 scala 了 
别急 先搞 scala 是因为 先要 把 基本 的 给 走通 
再去 搞 python 环境 的 接口 python 接口 的 REPL 
是 这个 文件 pyspark . cmd 双击 也 报错 . 
. . 别急 这里 是 因为 python 版本 问题 anaconda 
最新版 的 python 解释器 版本 是 3 . 6.1 这个 
版本 的 spark 不支持 需要 降低 版本 到 3.5 卸载 
python 不用 用 anaconda 的 环境 切换 就行了 1 . 
先 创建 一个 新的 开发 环境 conda create n my 
_ new _ env _ python352 . 激活 这个 新的 
开发 环境 activate   my _ new _ env _ 
python353 . 在 这个 新的 开发 环境 中 安装 python 
3.5 conda install python = 3.5 这时 python3 . 5 
版本 的 解释器 就算 是 安装 完成 了 默认 目录 
在 C \ ProgramData \ Anaconda3 \ envs \ my 
_ new _ env _ python35 \ python . exe 
然后 就是 需要 把 spark 的 python 支持 包 复制 
到 相应 的 路径 中了 从下 复制到 my _ new 
_ env _ python35 环境 的 Lib \ site packages 
目 录下 接下来 需要 把 python 默认 版本 改成 python3 
. 5 需要 修改 PATH 路径 把 python3 . 5 
的 路径 放在 第一 个 查找 路径 下 就行了 然后 
就 开始 整 pycharm 开发环境 了 首先 肯定 是 新建 
一个 python 项目 了 然后 改 设置 用来 指定 python 
解释器 的 路径 菜单 File Settings 接着 设置 运行 时候 
的 配置 参数 漏了 python 调用 pyspark 的 代码 了 
代码 如下 import sys from operator import add from pyspark 
import SparkContext if _ _ name _ _ = = 
_ _ main _ _ sc = SparkContext appName = 
PythonWordCount lines = sc . textFile words . txt count 
= lines . count print count counts = lines . 
flatMap lambda x x . split \ . map lambda 
x x 1 \ . reduceByKey add output = counts 
. collect for word count in output print % s 
% i % word count sc . stop 至此 python 
环境 算是 搞定 了 