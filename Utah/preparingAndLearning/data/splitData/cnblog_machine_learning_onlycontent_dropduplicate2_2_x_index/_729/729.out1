瓶颈 任何 事物 的 发展 都会 遇到 瓶颈 半导体 业界 
的 摩尔定律 在 很长 的 一段 时间 里面 一直 是 
有效 的 但是 在 近几年 也快 走到 尽头 了 机器学习 
在 AlphaGo 战胜 人类 棋手 之后 名声大噪 我 也是 在那 
次 比赛 之后 开始 研究 机器学习 的 机器学习 这项 技术 
是不是 有 一个 天花板 这个 天花板 在哪里 我们 现在 的 
技术 发展 离开 这个 天花板 到底有 多远 我们 是 在 
地板 上 呢 还是 快 触碰 到 天花板 了 呢 
在 五年前 Intel 公司 的 CEO 就 抛出 了 无法 
继续 摩尔定律 的 危机 说 摩尔定律 由 英特尔 联合 创始人 
戈登 摩尔 Gordon Moore 提出 意思 是 说 当 价格 
不变 时 集 成电 路上 可 容纳 的 晶体管 数目 
约 每隔 18 个月 便会 增加一倍 性能 也 将 提升 
一倍 换言之 每 一美元 所能 买到 的 电脑 性能 将 
每隔 18 个月 翻 两倍 以上 这个 定律 虽然 奏效 
了 数十年 但是 从 2018年 开始 这个 定律 就 已经 
失效 黑盒 白盒/nr 之争 在 知乎 上有 这样 一篇 文章 
https / / zhuanlan . zhihu . com / p 
/ 21362413 fc = 1 & group _ id = 
8 2 1 4 0 0 6 3 8 1 
5 0 8 2 8 0 3 2 # comment 
145854724 大概 的 意思 是 用 一个 神经 网络 来 
调控 另 一个 神经 网络 以前 虽然 我们 不 知道 
AlphaGo 是 怎么 想 的 但是 我们 知道 它 是 
怎么 学 的 以后 我们 不但 不 知道 AlphaGo 是 
怎么 想 的 我们 还 不 知道 它 是 怎么 
学 的 人工智能 到底 是 黑盒 还是 白盒/nr 在 评论 
里面 关于 这个 话题 大家 产生 了 很大 的 分歧 
作为 传统 的 程序员 我 的 观点 如下 当然 如果 
您 有时间 可以 看 一下 评论 非常 精彩 1 . 
ML 归根到底 是 程序 如果 LOG 足够 多 的话 如果 
你 足够 耐心 的话 你 肯定 可以 知道 结果 是 
如何 产生 的 2 . 用 神经网络 去 优化 神经网络 
其 本质 是 一样 的 就想 加法 变成 乘法 但是 
还 没有 脱离 实数 的 范围 到达 一个 更高 的 
维度 3 . ML 的 程序 包括 无 监督 的 
程序 都是 人 写 的 都是 按照 人 的 想法 
在 执行 的 所以 为什么 人 不 知道 机器 是 
怎么 想 的 即使 这 个 程序 表现 得 再 
不可思议 但 结果 应该 都在 人 的 预料 之中 AlphaGo 
为什么 会 做 决定 背后 是 程序 程序 的 背后 
是 写 程序 的 人 的 想法 除非 是 真正 
的 随机 函数 不然 写 程序 的 人 肯定 知道 
程序 是 如何 运行 和 预想 结果 的 总结 不 
知道 程序 是 怎么 想 的 只是 因为 你 不 
愿意 去 阅读 程序 的 日志 和 不愿意 调试程序 如果 
有 无穷 的 时间 你 单步 调试 所有 的 代码 
你 肯定 知道 这个 结果 是 怎么 来 的 如果 
整个 机器学习 慢慢 进入 黑盒 的 时代 则 可以 预测 
瓶颈 快 到了 我们 不 知道 机器 到底 是 怎么 
学习 的 我们 就 无法 进行 改进 就像 我们 不 
知道 雨水 的 形成 机理 我们 光 在地上 求雨 是 
徒劳 的 随机 森林 和 Dropout 很多 算法 中 都 
可以 看到 随机 的 影子 RF 的话 也 就是 多次 
随机 抽取 样本 训练 模型 这些 模型 再 进行 平均 
操作 当然 这 是 根据 中心 极限 理论 得出 的 
好 方法 神经 网络 的 Dropout 也是 如此 随机 的 
将 一些 神经 节点 进行 屏蔽 但是 随机 就 意味着 
失控 意味着 人工 很难 干预 结果 包括 梯度 下降 是否 
能 收敛 到 全局 最优 解 很大 程度 上 也是 
有 运气 成分 在 里面 的 初始值 学习率 都是 影响 
结果 的 因素 调 参数 和 巨大 模型 现在 很多 
机器学习 的 比赛 已经 从 技术 比拼 转向 资源 比 
拼了 神经 网络 的 层数 越来越 长 越来越 深 微软 
的 神经 网络 是 152层 阿里 巴巴 的 机器学习 模型 
已经 是 3GB 的 庞然大物 了 整个 业界 都从 硬件 
和 物理 层面 去 获得 精度 的 收益 了 同时 
超 参数 的 选取 现在 也 都是 经验论 神经 网络 
的 层数 我们 首先 需要 确定 网络 的 层数 和 
每层 的 节点 数 关于 第一个 问题 实际上 并 没有 
什么 理论化 的 方法 大家 都是/nr 根据 经验 来 拍 
如果 没有 经验 的话 就 随便 拍 一个 然后 你 
可以 多试 几个 值 训练 不同 层数 的 神经 网络 
看看 哪个 效果 最好 就 用 哪个 嗯 现在 你 
可能 明白 为什么 说 深度 学习 是个 手艺 活了 有些 
手艺 很 让人 无语 而 有些 手艺 还是 很 有 
技术 含量 的 K 聚 类 的 K 取 多少 
自然语言 处理 的 主题 模型 主题 数 选择 多少 比较 
合适 等等 都还/nr 没有 或者 难以 找到 理论依据 机器学习 还是 
数理统计 机器 学习 的 本质 就是 数理统计 答案 可能 没 
这么 简单 http / / tech . sina . com 
. cn / roll / 2017 03 27 / doc 
ifycspxp0038858/i ./i shtml/w 如果/c 从/p 传统/n 意义/n 上/f 的/uj 数据/n 
分析/vn 师的/nr 观点/n 来说/u 这个 问题 的 答案 很简单 无非是 
下面 这 两点 机器学习 本质上 是 一种 算法 这种 算法 
由 数据分析 习得 而且 不 依赖 于 规则 导向 的 
程序 设计 统计 建模 则是 以 数据 为基础 利用 数学 
方程式 来 探究 变量 变化 规律 的 一套 规范化 流程 
有 一种 观点 就 是 机器 学习 只是 数理统计 的 
一个 华丽 包装 而已 在 自然 语言 处理 里面 原本 
是 语言学家 占 主导 的 然后 慢慢 的 统计 学家 
开始 占上风 特别 是 在 翻译 领域 基本上 都是 靠 
强大 的 计算 能力 和 巨大 的 模型 在 处理 
问题 也 就是说 从 规则 到 统计 的 转变 如果说 
机器 学习 的 本质 还是 统计学 的话 统计学 概率 学 
这些东西 其实 已经 发展 到 尽头 很难 再 有 什么 
革命性 的 突破 了 是不是 也 意味着 机器学习 也 走到 
尽头 了 呢 脑 科学 研究 机器学习 在 很大 程度 
上 是 对于 大脑 工作 原理 的 仿生学 我 觉得 
机器 学习 的 发展 肯定 和 人类 对于 大脑 研究 
的 发展 密不可分 神经 网络 就是 一个 例子 也 有可能 
在 多年 之后 我们 会 发现 大脑 的 工作 原理 
和 我们 现在 的 认知 完全 不同 这样的话 当前 的 
机器 学习 很 有可能 会 被 完全 推翻 走向 一条 
新的 道路 