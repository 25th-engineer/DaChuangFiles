按照 前面 文章 的 方法 进行 数据 预测 完全 不 
使用 POI 天气 交通 情况 的 数据 可以 达到 0.43 
的 成绩 不过 如果 想 要 获得 更好 的 成绩 
简单 的 预测 方法 显然 无法 满足 要求 了 GBDT 
网友 说 可以 使用 GBDT 的 方法 来 进行 数据 
预测 所以 我们 先 来 聊聊 GBDT 算法 的 一些 
基础 知识 熵 凡是 说到 算法 人工智能 机器 学习 的 
文章 多半 一定 要说 到 熵 这个 概念 的 什么 
是 熵 百度一下 熵 entropy 指 的 是 体系 的 
混乱 的 程度 它 在 控制论 概率论 数论 天体物理 生命/vn 
科学/n 等/u 领域/n 都有/nr 重要/a 应用/v 在 不同 的 学科 
中 也有 引申 出 的 更为 具体 的 定义 是 
各 领域 十分 重要 的 参量 熵 由 鲁道夫 克劳修斯 
Rudolf Clausius 提出 并 应用在 热力学 中 后来 在 克劳德 
艾尔 伍德 香农 Claude Elwood Shannon 第一次 将 熵 的 
概念 引入 到 信息论 中 来 一个 体系 越是 单调 
则 熵 越低 反之亦然 这里 我们 引用 数据挖掘 大神 的 
文章 来 接单 说 一下 熵 如果 有 一个 字符串 
里面 包含 了 4种 字符 每种 出现 的 概率 都是 
P = 1/4 P X = A = 1/4 P 
X = B = 1/4 P X = C = 
1/4 P X = D = 1/4 这样 的 字符串 
可能 是 BAACBADCDADDDA 传送 这样 的 字符串 每一 个字符 需要用 
几个 bit 答案 是 2个 bitA = 00 B = 
01 C = 10 D = 11 如果 有 一个 
字符串 里面 包含 了 4种 字符 但是 每个 字符串 出现 
的 概率 不同 P X = A = 1/2 P 
X = B = 1/4 P X = C = 
1/8 P X = D = 1/8 传送 这样 的 
字符串 每一 个字符 平均 需要 用 几个 bit 注意 这里 
说 平均 答案 是 1.75个 bitA = 0 B = 
10 C = 110 D = 111 如果 使用 等 
概率 的 方法 A = 00 B = 01 C 
= 10 D = 11 则 无法 节省 编码 量 
还是 2个 bit 这里 巧妙 的 做到 了 出现 概率 
高的/nr 字符 使用 的 bit 位 少 同时 做到 了 
编码 上 的 问题 AB = 010 和 C 110 
D 111 不 重复 AA = 00 和 B 10 
不 重复 等 有/v 如果/c 有/v 一个/m 字符串/n 里面 3种 
字符串 每种 出现 概率 都是 1/3 呢 最 简单 的 
编码 方式 是 A = 00 B = 01 C 
= 10 这样 是 2个 bit 但是 如果 好好 计算 
一下 可以 做到 1.6个 bit A = 10 B = 
11 C = 0 理论上 是 1.58496 个 bit 有/v 
如果/c 有/v 一个/m 字符串/n 里面 N 种 字符串 每种 出现 
概率 是 PN 呢 如果 有 一个 字符串 里面 包含 
了 4种 字符 每种 出现 的 概率 都是 P = 
1/4 = 0.25 log 0.25 2 = 2H X = 
1/4 * log 0.25 2 1/4 * log 0.25 2 
1/4 * log 0.25 2 1/4 * log 0.25 2 
= 2 如果 要 表示 下图 的 H X 和H/nr 
Y 呢 这个 很 容易 计算 这个 很 容易 计算 
H X = 1.5 P Math = 1/2 P History 
= 1/4 P CS = 1 / 4log 0.25 2 
= 2 log 0.5 2 = 1H X = 1/2 
* log 0.5 2 1/4 * log 0.25 2 1/4 
* log 0.25 2 = 0.5 + 0.5 + 0.5 
= 1.5 H Y = 1P Yes = 1/2 P 
No = 1/2 H Y = 1/2 * log 0.5 
2 1/2 * log 0.5 2 = 0.5 + 0.5 
= 1 如果说 我们 的 计算 范围 只是 X = 
Math 的 数据 那么 这个 时候 H Y | X 
= Math 是 多少 呢 是 多少 呢 答案 是 
1 一共 4条 记录 但是 Y 有 两种 可能性 如果说 
我们 的 计算 范围 只是 X = Histroy 的 数据 
那么 这个 时候 H Y | X = Histroy 是 
多少 呢 答案 也是 0 一共 2条 记录 但是 Y 
只是 一种 可能性 如果说 我们 的 计算 范围 只是 X 
= CS 的 数据 那么 这个 时候 H Y | 
X = CS 是 多少 呢 答案 也是 0 一共 
2条 记录 但是 Y 只是 一种 可能性 H Y | 
X 条件 熵 Conditional Entropy 现在 我们 考虑 一个 问题 
如果 我们 需要 将 Y 传 输出去 当然 如果 直接 
传输 的话 H Y = 1 如果 我们 在 传输 
的 时候 双方 都 知道 X 的 值 则 需要 
熵 定义 为 H Y | X 例如 大家 都 
知道 X = History 则 Y 必然 是 NO H 
Y = 0 Histroy 的 可能性 是 1/4 需要 的 
传输 量 是 0 CS 同理 大家 都 知道 X 
= Math 则 Y 可能 是 Yes 或者 No H 
Y = 1 Math 的 可能性 是 1/2 需要 的 
平均 传输率 是 1/2 * 1 = 0 . 5Math 
的 概率 P Math = 1/2 History 的 概率 P 
Histroy = 1/4 History 的 概率 P CS = 1/4 
则 我们 定义 H Y | X = H Y 
| X = Math * P Math + H Y 
| X = Histroy * P Histroy + H Y 
| X = CS * P CS = 0 . 
5Information Gain 信息 增益 和 Relative Information Gain 从 上文 
可知 比起 直接 传输 Y 条件 熵 则 更加 划算 
了 这些 划算 的 部分 我们 称为 信息 增益 IG 
IG Y | X = H Y H Y | 
X 上面 的 例子 IG Y | X = H 
Y H Y | X = 1 0.5 = 0.5 
进一步 这样 划算 的 部分 占 原来 所需 部分 的 
比重 是 多少 呢 RIG = IG Y | X 
/ H Y = 0.5 / 1 = 0.5 节省 
的 部分 占了 50% 信息 增益 是 什么 我们 先从 
它 的 用处 来 了解 它 信息 增益 是 特征选择 
中 的 一个 重要 指标 它 定义 为 一个 特征 
能够 为 分类 系统 带来 多少 信息 带来 的 信息 
越多 该 特征 越 重要 指标 选择 回到 滴滴 算法 
的 问题 我们 应该 挑选 哪些 指标 作为 GBDT 的 
参考 呢 滴滴 算法 大赛 算法 解决 过程 数据分析 滴滴 
算法 大赛 算法 解决 过程 拟合 算法 滴滴 算法 大赛 
算法 解决 过程 方案设计 滴滴 算法 大赛 算法 解决 过程 
机器学习 