摘要 本文 分别 介绍 了 线性 回归 局部 加权回归 和 
岭回归 并 使用 python 进行 了 简单 实现 在这之前 已经 
学习 过了 Logistic 回归 今天 继续 看 回归 首先 说 
一下 回归 的 由来 回归 是由 达尔文 的 表兄弟 Francis 
Galton 发明 的 Galton 于 1877年 完成 了 第一次 回归 
预测 目的 是 根据 上 一代 豌豆 的 种子 双亲 
的 尺寸 来 预测 下一代 豌豆 种子 孩子 的 尺寸 
身高 Galton 在 大量 对象 上 应用 了 回归分析 甚至 
包括 人 的 身高 他 得到 的 结论 是 如果 
双亲 的 高度 比 平均 高度 高 他们/r 的/uj 子女/n 
也/d 倾向/v 于/p 平均/a 身高/v 但/c 尚/d 不及/c 双亲/n 这里 
就 可以 表述 为 孩子 的 身高 向着 平均 身高 
回归 Galton 在 多项 研究 上 都 注意到 了 这一点 
并将 此 研究 方法 称为 回归 接下来 就 上文 提到 
的 三种 回归 一一 介绍 一 线性 回归 Linear Regression 
1 . 线性 回归 概述 回归 的 目的 是 预测 
数值 型 数据 的 目标 值 最 直接 的 方法 
就是 根据 输入 写出 一个 求出 目标值 的 计算 公式 
也 就是 所谓 的 回归方程 例如 y = ax1 + 
bx2 其中 求 回归系数 的 过程 就是 回归 那么 回归 
是 如何 预测 的 呢 当 有了 这些 回归系数 给定 
输入 具体 的 做法 就是 将 回归 系数 与 输入 
相乘 再将 结果 加 起来 就是 最终 的 预测 值 
说到 回归 一般指 的 都是 线性 回归 当然 也 存在 
非线性 回归 在此 不做 讨论 假定 输入 数据 存在 矩阵 
x 中 而 回归系数 存放在 向量 w 中 那么 对于 
给定 的 数据 x1 预测 结果 可以 通过 y1   
= x1Tw 给出 那么 问题 就是 来 寻找 回归系数 一个 
最 常用 的 方法 就是 寻找 误差 最小 的 w 
误差 可以 用 预测 的 y 值 和 真实 的 
y 值 的 差值 表示 由于 正负 差值 的 差异 
可以 选用 平方 误差 也 就是 对 预测 的 y 
值 和 真实 的 y 值 的 平方 求和 用 
矩阵 可表示 为 y xw T y xw 现在 问题 
就 转换 为 寻找 使得 上述 矩阵 值 最小 的 
w 对 w 求导 为 xT y xw 令 其为 
0 解得 w = xTx 1xTy 这 就是 采用 此 
方法 估计 出来 的 2 . python 实现 结合 上述 
的 分析 采用 python 实现 首先 导入 数据 # 导入 
数据 def loadData fileName numFeat = len open fileName . 
readline . split \ t 1 dataMat = labelMat = 
fr = open fileName for line in fr . readlines 
linArr = curline = line . strip . split \ 
t # 得到 每行 并以 tab 作为 间隔 for i 
in range numFeat linArr . append float curline i dataMat 
. append linArr labelMat . append float curline 1 return 
dataMat labelMat 之后 求解 回归系数 # 计算 回归系数 from numpy 
import * def standRegres x y xMat = mat x 
yMat = mat y . T xTx = xMat . 
T * xMat # 采用 numpy 中的 线性代数 库 linalg 
其中 linalg . det 直接 可以 计算 行列式 if linalg 
. det xTx = = 0.0 print 这个 行列式 是 
错误 的 的 不能 求 逆 return # 求 回归系数 
w = xTx . I * xMat . T * 
yMat return w 求 得了 回归系数 结合 输入 就 可以 
得到 回归方程 为了 直观 的 表示 采用 Matplotlib 绘图 x 
y = loadData ex0 . txt w = standRegres x 
y xMat = mat x yMat = mat y # 
绘制 原 数 据点 fig = plt . figure ax 
= fig . add _ subplot 111 ax . scatter 
xMat 1 . flatten . A 0 yMat . T 
0 . flatten . A 0 # matplotlib . collections 
. CircleCollectin object at 0x04ED9D30 # plt . show # 
在 之前 的 图像 上 绘制 出 拟合 直线 xCopy 
= xMat . copy xCopy . sort 0 yHat = 
xCopy * w ax . plot xCopy 1 yHat plt 
. show 结果 如 下图 至此 拟合 就 结束 了 
那么 如何 评判 拟合 的 好坏 numpy 库 提供 了 
相关 系数 的 计算 方法 通过 命令 corrcoef 可以 来 
计算 预测值 和 真实 值 的 相关性 yHat = xMat 
* w print corrcoef yHat . T yMat 分析 结果 
我们 可以 看出 线性 回归 得到 的 相关性 还是 挺 
理想 的 但是 从 图像 中 明显 可以 看出 线性 
回归 未能 捕获 到 一些 数据 点 没能 很好 的 
表示 数据 的 变化 趋势 在 某种 情况 下 存在 
欠 拟合 的 情况 这是 线性 回归 的 一个 缺点 
在此 想要 说明 的 一点 是 要 只是 简单 的 
实现 拟合 的话 不妨 采用 MATLAB 中的 cftool 的 工具 
简单 高效 直观 二 局部 加权 线性 回归 Locally Weighted 
Linear Regression LWLR 1 . 概述 针对于 线性 回归 存在 
的 欠 拟合 现象 可以 引入 一些 偏差 得到 局部 
加权 线性 回归 对 算法 进行 优化 在 该 算法 
中 给 待 测点 附近 的 每个 点 赋予 一定 
的 权重 进而 在 所 建立 的 子集 上 进行 
给予 最小 均方差 来 进行 普通 的 回归 分析 可得 
回归系数 w 可表示 为 w = xTWx 1xTWy 其中 W 
为 每个 数 据点 赋予 的 权重 那么 怎样 求 
权重 呢 核 函数 可以 看成 是 求解 点 与 
点 之间 的 相似 度 在此 可以 采用 核 函数 
相应 的 根据 预测 点 与 附近 点 之间 的 
相似 程度 赋予 一定 的 权重 在此 选用 最 常用 
的 高斯 核 则 权重 可以 表示 为 w i 
i = exp | x i x | / 2k2 
其中 K 为 宽度 参数 至于 此参数 的 取值 目前 
仍 没有 一个 确切 的 标准 只有 一个 范围 的 
描述 所以在 算法 的 应用 中 可以 采用 不同 的 
取值 分别 调试 进而 选取 最好 的 结果 2 . 
python 实现 结合 上述 的 分析 采用 python 编程 实现 
代码 如下 def lwlr testPoint xArr yArr k xMat = 
mat xArr yMat = mat yArr . T m = 
shape xMat 0 weights = mat eye m for i 
in range m weights i i = exp testPoint xMat 
i * testPoint xMat i . T / 2.0 * 
k * * 2 xTx = xMat . T * 
weights * xMat if linalg . det xTx = = 
0 print 输入 有误 return ws = xTx . I 
* xMat . T * weights * yMat return testPoint 
* ws # 为 数据 点中 的 每个 数据 调用 
lwlr def lwlrTest testArr xArr yArr k m = shape 
testArr 0 yHat = zeros m for i in range 
m yHat i = lwlr testArr i xArr yArr k 
return yHat 结合 上述 分析 我们 可以 选取 不同 的 
k 值 分别 求得 结果 进而 采用 Matplotlib 绘图 直观 
的 表示 在此 分别 选取 k = 1 0.01 0.002 
代码 如下 # test x y = loadData ex0 . 
txt a = lwlr x 0 x y 0.002 b 
= lwlrTest x x y 0.002 # 采用 matplotlib 绘制 
图像 xMat = mat x srtInd = xMat 1 . 
argsort 0 # 按 升序 排序 返回 下标 xSort = 
xMat srtInd 0 # 将 xMat 按照 升序 排列 fig 
= plt . figure ax = fig . add _ 
subplot 111 ax . plot xSort 1 b srtInd ax 
. scatter xMat 1 . flatten . A 0 mat 
y . T . flatten . A 0 s = 
2 c = red plt . show 最终 结果 分别 
如下 依次为 k = 1 0.01 0.002 对应 的 结果 
可以 看出 当 k = 1时 结果 和 线性 回归 
使用 最 小二 乘法 的 结果 相似 而 k = 
0.001时 噪声 太多 属于 过拟合 的 情况 相比之下 k = 
0.01 得到 的 结果 更 理想 虽然 LWLR 得到 了 
较为 理想 的 结果 但是 此种 方法 的 缺点 是 
在对 每个 点 进行 预 测时 都必须 遍历 整个 数据集 
这样 无疑 是 增加 了 工作 量 并且 该 方法 
中的 的 宽度 参数 的 取值 对于 结果 的 影响 
也是 蛮 大 的 同时 当 数据 的 特征 比 
样本点 还多 当然 是 用 线性 回归 和 之前 的 
方法 是 不能 实现 的 当 特征 比 样本点 还 
多时 表明 输入 的 矩阵 X 不是 一个 满秩/nr 矩阵 
在 计算 XTX 1时 会 出错 三 岭回归 1 . 
概述 为了 解决 上述 问题 统计学家 引入 了 岭回归 的 
概念 简单 说来 岭回归 就是 在 矩阵 XTX 上加 上一个 
λ r 从而 使得 矩阵 非 奇异 从而 能对 XTX 
+ λ x 求 逆 其中 矩阵 r 为 一个 
m * m 的 单位矩阵 对角 线上 的 元素 全为 
1 其他 元素 全为 0 而 λ 是 一个 用户 
定义 的 数值 这种 情况 下 回归系数 的 计算 公式 
将 变为 w = xTx + λ I 1xTy 其中 
I 是 一个 单位矩阵 岭回归 就是 用了 单位矩阵 乘以 常量 
λ 因为 只 I 贯穿 了 整个 对角线 其余 元素 
为 0 形象 的 就是 在 0 构成 的 平面 
上 有 一条 1 组成 的 岭 这 就是 岭回归 
中 岭 的 由来 岭回归 最先 是 用来 处理 特征 
数 多与 样本数 的 情况 现在 也 用于 在 估计 
中 加入 偏差 从而 得到 更好 的 估计 这里 引入 
λ 限制 了 所有 w 的 和 通过 引入 该 
惩罚 项 能够 减少 不 重要 的 参数 这个 技术 
在 统计 学上 也叫做 缩减 缩减 方法 可以 去掉 不 
重要 的 参数 因此 能 更好 的 理解 数据 在此 
选取 不同 的 λ 进行 测试 最后 得到 一个 使得 
误差 最小 的 λ 2 . python 实现 结合 上述 
的 分析 同样 采用 python 实现 代码 如下 def ridgeRegress 
xMat yMat lam = 0.2 # 在 没 给定 lam 
的 时候 默认 为 0.2 xTx = xMat . T 
* xMat denom = xTx + eye shape xMat 1 
* lam if linalg . det denom = = 0.0 
print 这个 矩阵 是 错误 的 不能 求 逆 return 
ws = denom . I * xMat . T * 
yMat return ws # 对 数据 进行 标准化 之后 调用 
30个 不同 的 lam 进行 计算 def ridgeTest xArr yArr 
xMat = mat xArr yMat = mat yArr . T 
yMean = mean yMat 0 yMat = yMat yMean xMeans 
= mean xMat 0 xVar = var xMat 0 xMat 
= xMat xMeans / xVar numTestPts = 30 wMat = 
zeros numTestPts shape xMat 1 for i in range numTestPts 
ws = ridgeRegress xMat yMat exp i 10 wMat i 
= ws . T return wMat 进而 采用 matplotlib 绘图 
得到 在 30个 不同 的 lam 下 回归 系数 的 
变化 情况 如 下图 由 该图 可以 看出 lam 很 
小时 系数 和 普通 回归 一样 而 当 lam 非常大 
时 所有 的 回归系数 缩减 为 0 可以 看出 在中间 
某处 可以 找到 使得 预测 结果 最好 的 lam 值 
为了 选取 最优 的 lam 值 可以 采取 交叉 验证法 
总结 与 分类 一样 回归 是 预测 目标值 的 过程 
回归 与 分类 的 不同 在于 回归 预测 的 是 
连续型 变量 而 分类 预测 的 是 离散 型 的 
变量 回归 是 统计学 中最 有力 的 工具 之一 如果 
给定 输入 矩阵 x xTx 的 逆 如果 存在 的话 
回归 法 可以 直接 使用 回归方程 中的 求得 特征 对应 
的 最佳 回归系数 的 方法 是 最小化 误差 的 平方和 
判断 回归 结果 的 好坏 可以 利用 预测值 和 真实 
值 之间 的 相关性 判断 当 数据 样本 总 个数 
少于 特征 总数 时 矩阵 x xTx 的 逆 不能 
直接 计算 这时 可以 考虑 岭回归 