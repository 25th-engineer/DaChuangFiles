特征选择 是 一个 重要 的 数据 预 处理过程 在 现实 
机器学习 任务 中 获得 数据 之后 通常 先 进行 特征选择 
此后 在 训练 学习 器 如下 图 所示 进行 特征选择 
有 两个 很 重要 的 原因 避免 维数 灾难 能 
剔除 不相关 irrelevant 或 冗余 redundant 的 特征 从而 达到 
减少 特征 个数 提高 模型 精确度 减少 运行 时间 的 
目的 降低 学习 任务 的 难度 选取 出 真正 相关 
的 特征 简化 模型 协助 理解 数据 产生 的 过程 
如 流程图 所示 特征选择 包括 两个 环节 子集 搜索 subset 
search 子集 评价 subset evaluation 机器学习 将 特征选择 分 为了 
三 种方法 分别 是 过滤 式 filter 包裹 式 wrapper 
和 嵌入式 embedded 下面 依据 sklearn 中的 特征选择 文档 来 
叙述 特征选择 的 几个 方法 过滤 式 filter 这类 方法 
先 对 数据 机 进行 特征选择 然后再 训练 学习 器 
特征 选择 的 过程 与 后续 学习 器 无关 移除 
低 方差 的 特征 V a r i a n 
c e T h r e s h o l 
d 是 特征选择 的 一个 简单 基本 方法 它 会 
移除 所有 那些 方差 不满足 阈值 的 特征 默认 情况 
下 它 将会 移除 所有 的 零 方差 特征 即 
那些 在 所有 的 样本 上 的 取值 均 不变 
的 特征 例如 假设 我们 有 一个 特征 是 布尔值 
的 数据集 我们 想要 移除 那些 在 整个 数据 集中 
特征值 为 0 或者 为 1 的 比例 超过 80% 
的 特征 布尔 特征 是 伯努利 Bernoulli 随机变量 变量 的 
方差 为 \ Var X = p 1 p \ 
因此 我们 可以 使用 阈值 . 8 * 1 . 
8 进行 选择 from sklearn . feature _ selection import 
V a r i a n c e T h 
r e s h o l d X = 0 
0 1 0 1 0 1 0 0 0 1 
1 0 1 0 0 1 1 sel = V 
a r i a n c e T h r 
e s h o l d threshold = . 8 
* 1 . 8 sel . fit _ transform X 
array 0 1 1 0 0 0 1 1 1 
0 1 1 正如 预期 一样 V a r i 
a n c e T h r e s h 
o l d 移除 了 第一 列 单/n 变量/vn 特征选择/nr 
单/n 变量/vn 的/uj 特征/n 选择/v 是/v 通过/p 基/a 于单/i 变量/vn 
的/uj 统计/v 测试/vn 来/v 选择/v 最好/a 的/uj 特征/n 它 可以 
当做 是 评估器 的 预处理 步骤 Scikit learn 将 特征选择 
的 内容 作为 实现 了 transform 方法 的 对象 SelectKBest 
移除 那些 除了 评分 最高 的 K 个 特征 之外 
的 所有 特征 SelectPercentile 移除 除了 用户 指定 的 最高 
得分 百分比 之外 的 所有 特征 G e n e 
r i c U n i v a r i 
a t e e l e c t 允许 使用 
可 配置 方法 来 进行 单 变量 特征选择 它 允许 
超 参数 搜索 评估器 来 选择 最好 的 单 变量 
特征 例如 下面 的 实例 我们 可以 使用 \ \ 
chi ^ { 2 } \ 卡方检验 检验 样 本集 
来 选择 最好 的 两个 特征 from sklearn . datasets 
import load _ iris from sklearn . feature _ selection 
import SelectKBest from sklearn . feature _ selection import chi2 
iris = load _ iris X y = iris . 
data iris . target X . shape 150 4 X 
_ new = SelectKBest chi2 k = 2 . fit 
_ transform X y X _ new . shape 150 
2 很明显 上述 两种 方法 都是 过滤 式 特征 选择 
的 方法 所以 与 模型 无关 的 特征 权重 显得 
尤为 重要 这种 权重 主要 分析 特征 与 target 的 
相关性 这样 的 分析 是 与 这次 学习 所 使用 
的 模型 无关 的 与 模型 无关 特征 权重 分析 
方法 包括 1 交叉 熵 2 Information Gain 3 Odds 
ratio 4 互信息 5 KL 散度 相对 熵 等 上述 
代码 用到 的 是 经典 的 卡方检验 这里 简单 叙述 
原理 经典 的 卡方检验 是 检验 定性 自变量 对 定性 
因变量 的 相关性 假设 自变量 有N种/nr 取值 因变量 有M种/nr 取值 
考虑 自变量 等于 i 且 因变量 等于 j 的 样本 
频数 的 观察 值 与 期望 的 差距 构建 统计量 
\ \ chi ^ { 2 } = \ sum 
\ frac { A E ^ { 2 } } 
{ E } \ A 为 实际 值 E 为 
理论值 求和 值 为 理论值 与 实际 值 的 差异 
程度 基本 思想 是 根据 样本数据 推断 总体 的 分布 
与 期望 分布 是否 有 显著 性 差异 或者 推断 
两个 分类 变量 是否 相关 或者 独立 卡方检验 具体 可 
参考 这篇 博客 卡方 分布 与 卡方检验 包裹 式 wrapper 
这类 方法 选择 直接 把 最终 将 要 使用 学习 
期 的 性能 作 为特征 子集 的 评价 准则 递归 
式 特征 消除 RFE 给定 一个 外部 的 估计 器 
该 估计 起 对 特征 赋予 一定 的 权重 比如 
线性 模型 的 系数 recursive feature elimination RFE 通过 处理 
越来越少 的 特征 集合 来 递归 的 选择 特征 首先 
评估器 在 初始 的 特征 集合 上面 进行 训练 并且 
每 一个 特征 的 重要 程度 是 通过 一个 诸如 
sklearn 里 的 coef _ 属性 或者 feature _ importances 
_ 属性 来 获得 然后 从 当前 的 特征 集合 
中 移除 最不 重要 的 特征 在 特征 集合 上 
不断 的 重复 递归 这个 步骤 直到 最终 达到 所 
需要 的 特征 数量 为止 下列 代码 使用 RFE 抽取 
5个 最 informative 的 特征 from sklearn . datasets import 
make _ friedman1 from sklearn . feature _ selection import 
RFE from sklearn . svm import SVR X y = 
make _ friedman1 n _ samples = 50 n _ 
features = 10 random _ state = 0 estimator = 
SVR kernel = linear selector = RFE estimator 5 step 
= 1 selector = selector . fit X y selector 
. support _ array True True True True True False 
False False False False dtype = bool selector . ranking 
_ array 1 1 1 1 1 6 4 3 
2 5 从 最终 的 学习 器 性能 来看 包裹 
式 特征选择 比 过滤 式 特征选择 更好 但是 另一方面 由于 
在 特征选择 过程 中 需 多次 训练 学习 期 因此 
包裹 式 特征 选择 的 计算 开销 通常 要 大得多 
嵌入式 embedded SelectFromModel 选取 特征 sklearn . feature _ selection 
. SelectFromModel estimator threshold = None prefit = False norm 
_ order = 1 SelectFromModel 是 一个 meta transformer 元 
转换器 它 可以 用来 处理 任何 带有 coef _ 或者 
feature _ importances _ 属性 的 训练 之后 的 评估器 
如果 相关 的 coef _ 或者 feature _ importances _ 
属性值 低于 预先 设置 的 阈值 这些 特征 将会 被 
认为 不 重要 并且 移 除掉 除了 指定 数值 上 
的 阈值 之外 还 可以 通过 给定 字符串 参数 来 
使用 内置 的 启发式 方法 找到 一个 合适 的 阈值 
可以 使用 的 启发式 方法 有 mean median 以及 使用 
浮点数 乘以 这些 例如 0.1 * mean Linear models 使用 
L1 正则化 的 线性 模型 会 得到 稀 疏解 他们 
的 许多 系数 为 0 当 目标 是 降低 使用 
另一个 分类器 的 数据集 的 维度 它们 可以 与 feature 
_ selection . SelectFromModel 一起 使用 来 选择 非零 系数 
from sklearn . svm import LinearSVC from sklearn . datasets 
import load _ iris from sklearn . feature _ selection 
import SelectFromModel iris = load _ iris X y = 
iris . data iris . target X . shape 150 
4 lsvc = LinearSVC C = 0.01 penalty = l1 
dual = False . fit X y model = SelectFromModel 
lsvc prefit = True X _ new = model . 
transform X X _ new . shape 150 3 其实 
用 包装 好 的 库 看不出 嵌入式 的 两者 兼顾 
实际上 在 fit 后 得到 coef 的 过程 中 相当于 
已经 做出 了 特征选择 另外 基于 树 的 estimators 也 
可以 用来 计算 特征 的 重要性 然后 可以 消除 不 
相关 的 特征 当 与 sklearn . feature _ selection 
. SelectFromModel 等 元 转换器 一同 使用 时 以下 是 
一个 使用 随机 森林 进行 特征选择 的 例子 from sklearn 
. ensemble import R a n d o m F 
o r e s t C l a s s 
i f i e r feat _ labels = df 
_ wine . columns 1 forest = R a n 
d o m F o r e s t C 
l a s s i f i e r n 
_ estimators = 500 random _ state = 1 forest 
. fit X _ train y _ train importances = 
forest . feature _ importances _ indices = np . 
argsort importances 1 for f in range X _ train 
. shape 1 print % 2d % * s % 
f % f + 1 30 feat _ labels indices 
f importances indices f Reference http / / scikit learn 
. org / stable / modules / feature _ selection 
. htmlhttp / / sklearn . apachecn . org / 
cn / 0 . 19.0 / modules / feature _ 
selection . htmlhttps / / www . kaggle . com 
/ bertcarremans / data preparation explorationhttps / / github . 
com / rasbt / python machine learning book 2nd editionhttps 
/ / www . zhihu . com / question / 
28641663 机器学习 . 周志华 