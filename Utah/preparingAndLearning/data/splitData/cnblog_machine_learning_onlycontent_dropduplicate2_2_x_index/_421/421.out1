在做 分类 时 常常 需要 估算 不同 样本 之间 的 
相似性 度量 Similarity Measurement 这时 通常 采用 的 方法 就是 
计算 样 本间 的 距离 Distance 采用 什么样 的 方法 
计算 距离 是 很 讲究 甚至 关系 到 分类 的 
正确 与否 本文 的 目的 就是 对 常用 的 相似性 
度量 作 一个 总结 本文 目录 1 . 欧氏距离 2 
. 曼哈顿 距离 3 . 切比雪夫 距离 4 . 闵可夫 
斯基 距离 5 . 标准化 欧氏距离 6 . 马氏 距离 
7 . 夹角 余弦 8 . 汉明 距离 9 . 
杰 卡德 距离 & 杰 卡德 相似系数 10 . 相关系数 
& 相关 距离 11 . 信息熵 1 . 欧氏距离 Euclidean 
Distance 欧氏距离 是 最 易于 理解 的 一种 距离 计算方法 
源自 欧氏 空间 中 两点 间 的 距离 公式 1 
二维 平面 上 两点 a x1 y1 与 b x2 
y2 间 的 欧氏距离 2 三维空间 两点 a x1 y1 
z1 与 b x2 y2 z2 间 的 欧氏距离 3 
两个 n 维 向量 a x11 x12 x1n 与 b 
x21 x22 x2n 间 的 欧氏距离 也 可以 用 表示 
成 向量 运算 的 形式 4 Matlab 计算 欧氏距离 Matlab 
计算 距离 主要 使用 pdist 函数 若 X 是 一个 
M × N 的 矩阵 则 pdist X 将 X 
矩阵 M 行 的 每 一行 作为 一个 N 维 
向量 然后 计算 这 M 个 向量 两 两间 的 
距离 例子 计算 向量 0 0 1 0 0 2 
两 两间 的 欧式 距离 X = 0 0 1 
0 0 2 D = pdist X euclidean 结果 D 
= 1.0000       2.0000       2.23612 
. 曼哈顿 距离 Manhattan Distance 从 名字 就 可以 猜 
出 这种 距离 的 计算 方法 了 想象 你 在 
曼哈顿 要从 一个 十字路口 开车 到 另外 一个 十字路口 驾驶 
距离 是 两点 间 的 直线 距离 吗 显然 不是 
除非 你 能 穿越 大楼 实际 驾驶 距离 就是 这个 
曼哈顿 距离 而这 也是 曼哈顿 距离 名称 的 来源 曼哈顿 
距离 也 称为 城市 街区 距离 City Block distance 1 
二维 平面 两点 a x1 y1 与 b x2 y2 
间 的 曼哈顿 距离 2 两个 n 维 向量 a 
x11 x12 x1n 与 b x21 x22 x2n 间 的 
曼哈顿 距离 3 Matlab 计算 曼哈顿 距离 例子 计算 向量 
0 0 1 0 0 2 两 两间 的 曼哈顿 
距离 X = 0 0 1 0 0 2 D 
= pdist X cityblock 结果 D = 1     
    2         33 . 切比雪夫 
距离 Chebyshev Distance 国际象棋 玩过 么 国王 走 一步 能够 
移动 到 相邻 的 8个 方格 中的 任意 一个 那么 
国王 从 格子 x1 y1 走到 格子 x2 y2 最少 
需要 多少 步 自己 走走 试试 你 会 发现 最少 
步数 总是 max | x2 x1 | | y2 y1 
| 步 有 一种 类似 的 一种 距离 度量 方法 
叫 切比雪夫 距离 1 二维 平面 两点 a x1 y1 
与 b x2 y2 间 的 切比雪夫 距离 2 两个 
n 维 向量 a x11 x12 x1n 与 b x21 
x22 x2n 间 的 切比雪夫 距离 这个 公式 的 另一种 
等价 形式 是 看不出 两个 公式 是 等价 的 提示 
一下 试试 用 放缩法 和夹逼/nr 法则 来 证明 3 Matlab 
计算 切比雪夫 距离 例子 计算 向量 0 0 1 0 
0 2 两 两间 的 切比雪夫 距离 X = 0 
0 1 0 0 2 D = pdist X chebychev 
结果 D = 1         2   
      24 . 闵可夫 斯基 距离 Minkowski Distance 
闵氏 距离 不是 一种 距离 而是 一组 距离 的 定义 
1 闵氏 距离 的 定义 两个 n 维 变量 a 
x11 x12 x1n 与 b x21 x22 x2n 间 的 
闵可夫 斯基 距离 定义 为 其中 p 是 一个 变 
参数 当 p = 1时 就是 曼哈顿 距离 当 p 
= 2时 就是 欧氏距离 当 p → ∞ 时 就是 
切比雪夫 距离 根据 变 参数 的 不同 闵氏 距离 可以 
表示 一类 的 距离 2 闵氏 距离 的 缺点 闵氏 
距离 包括 曼哈顿 距离 欧氏距离/i 和/c 切比雪夫/i 距离/n 都/d 存在/v 
明显/a 的/uj 缺点/n 举个 例子 二维 样本 身高 体重 其中 
身高 范围 是 150 ~ 190 体重 范围 是 50 
~ 60 有 三个 样本 a 180 50 b 190 
50 c 180 60 那么 a 与 b 之间 的 
闵氏 距离 无论是 曼哈顿 距离 欧氏距离 或 切比雪夫 距离 等于 
a 与 c 之间 的 闵氏 距离 但是 身高 的 
10cm 真的 等价 于 体重 的 10kg 么 因此 用 
闵氏 距离 来 衡量 这些 样本 间 的 相似 度 
很 有问题 简单 说来 闵氏 距离 的 缺点 主要 有 
两个 1 将 各个 分量 的 量纲 scale 也 就是 
单位 当作 相同 的 看待 了 2 没有 考虑 各个 
分量 的 分布 期望 方 差等 可能 是 不同 的 
3 Matlab 计算 闵氏 距离 例子 计算 向量 0 0 
1 0 0 2 两 两间 的 闵氏 距离 以 
变 参数 为 2 的 欧氏距离 为例 X = 0 
0 1 0 0 2 D = pdist X minkowski 
2 结果 D = 1.0000       2.0000   
    2.23615 . 标准化 欧氏距离 Standardized Euclidean distance 1 
标准 欧氏距离 的 定义 标准化 欧氏距离 是 针对 简单 欧氏距离 
的 缺点 而作 的 一种 改进 方案 标准 欧氏距离 的 
思路 既然 数据 各 维 分量 的 分布 不 一样 
好吧 那我/nr 先将 各个 分量 都 标准化 到 均值 方差 
相等 吧 均值 和 方差 标准化 到 多少 呢 这里 
先 复习 点 统计学 知识 吧 假设 样 本集 X 
的 均值 mean 为 m 标准差 standard deviation 为 s 
那么 X 的 标准化 变量 表示 为 而且 标准化 变量 
的 数学期望 为 0 方差 为 1 因此 样 本集 
的 标准化 过程 standardization 用 公式 描述 就是 标准化 后的值/nr 
=   标准化 前 的 值   － 分量 的 
均值 / 分量 的 标准 差 经过 简单 的 推导 
就 可以 得到 两个 n 维 向量 a x11 x12 
x1n 与 b x21 x22 x2n 间 的 标准化 欧氏距离 
的 公式 如果 将 方差 的 倒数 看成 是 一个 
权重 这个 公式 可以 看成 是 一种 加权 欧氏距离 Weighted 
Euclidean distance 2 Matlab 计算 标准化 欧氏距离 例子 计算 向量 
0 0 1 0 0 2 两 两间 的 标准化 
欧氏距离 假设 两个 分量 的 标准差 分别为 0.5 和1/nr X 
= 0 0 1 0 0 2 D = pdist 
X seuclidean 0.5 1 结果 D = 2.0000     
  2.0000       2.82846 . 马氏 距离 Mahalanobis 
Distance 1 马氏 距离 定义 有M个/nr 样本 向量 X1 ~ 
Xm 协方差 矩阵 记为 均值 记为 向量 μ 则 其中 
样本 向量 X 到 u 的 马氏 距离 表示 为 
而 其中 向量 Xi 与 Xj 之间 的 马氏 距离 
定义 为 若 协方差 矩阵 是 单位矩阵 各个 样本 向量 
之间 独立 同 分布 则 公式 就 成了 也 就是 
欧氏距离 了 若 协方差 矩阵 是 对角 矩阵 公式 变成 
了 标准化 欧氏距离 2 马氏 距离 的 优缺点 量纲 无关 
排除 变量 之间 的 相关性 的 干扰 3 Matlab 计算 
1 2 1 3 2 2 3 1 两两 之间 
的 马氏 距离 X = 1 2 1 3 2 
2 3 1 Y = pdist X mahalanobis 结果 Y 
= 2.3452       2.0000       2.3452 
      1.2247       2.4495     
  1.22477 . 夹角 余弦 Cosine 有 没有 搞错 又 
不是 学 几何 怎么 扯到 夹角 余弦 了 各位 看官 
稍安 勿 躁 几何 中 夹角 余弦 可 用来 衡量 
两个 向量 方向 的 差异 机器学习 中 借用 这一 概念 
来 衡量 样本 向量 之间 的 差异 1 在 二维 
空间 中 向量 A x1 y1 与 向量 B x2 
y2 的 夹角 余弦公式 2 两个 n 维 样本点 a 
x11 x12 x1n 和b/nr x21 x22 x2n 的 夹角 余弦 
类似 的 对于 两个 n 维 样本点 a x11 x12 
x1n 和b/nr x21 x22 x2n 可以 使用 类似 于 夹角 
余弦 的 概念 来 衡量 它们 间 的 相似 程度 
即 夹角 余弦 取值 范围 为 1 1 夹角 余弦 
越大 表示 两个 向量 的 夹角 越小 夹角 余弦 越小 
表示 两 向量 的 夹角 越大 当 两个 向量 的 
方向 重 合时 夹角 余弦 取 最大值 1 当 两个 
向量 的 方向 完全 相反 夹角 余弦 取 最小值 1 
夹角 余弦 的 具体 应用 可以 参阅 参考文献 1 3 
Matlab 计算 夹角 余弦 例子 计算 1 0 1 1.732 
1 0 两 两间 的 夹角 余弦 X = 1 
0 1 1.732 1 0 D = 1 pdist X 
cosine   % Matlab 中的 pdist X cosine 得到 的 
是 1 减 夹角 余弦 的 值 结果 D = 
0.5000     1.0000     0.50008 . 汉明 距离 
Hamming distance 1 汉明 距离 的 定义 两个 等长 字符串 
s1 与 s2 之间 的 汉明 距离 定义 为 将 
其中 一个 变为 另外 一个 所 需要 作 的 最小 
替换 次数 例如 字符串 1111 与 1001 之间 的 汉明 
距离 为 2 应用 信息 编码 为了 增强 容错性 应 
使得 编码 间 的 最小 汉明 距离 尽可能 大 2 
Matlab 计算 汉明 距离 Matlab 中 2个 向量 之间 的 
汉明 距离 的 定义 为 2个 向量 不同 的 分量 
所占 的 百分比 例子 计算 向量 0 0 1 0 
0 2 两 两间 的 汉明 距离 X = 0 
0 1 0 0 2 D = PDIST X hamming 
结果 D = 0.5000       0.5000     
  1.00009 . 杰 卡德 相似系数 Jaccard similarity coefficient 1 
杰 卡德 相似系数 两个 集合 A 和B的/nr 交集 元素 在 
A B 的 并 集中 所占 的 比例 称为 两个 
集合 的 杰 卡德 相似系数 用 符号 J A B 
表示 杰 卡德 相似系数 是 衡量 两个 集合 的 相似 
度 一种 指标 2 杰 卡德 距离 与 杰 卡德 
相似系数 相反 的 概念 是 杰 卡德 距离 Jaccard distance 
杰 卡德 距离 可用 如下 公式 表示 杰 卡德 距离 
用 两个 集合 中 不同 元素 占 所有 元素 的 
比例 来 衡量 两个 集合 的 区分度 3 杰 卡德 
相似系数 与 杰 卡德 距离 的 应用 可将 杰 卡德 
相似系数 用在 衡量 样本 的 相似 度上 样本 A 与 
样本 B 是 两个 n 维 向量 而且 所有 维度 
的 取值 都是 0 或 1 例如 A 0111 和B/nr 
1011 我们 将 样本 看成 是 一个 集合 1 表示 
集合 包含 该 元素 0 表示 集合 不 包含 该 
元素 p 样本 A 与 B 都是 1 的 维度 
的 个数 q 样本 A 是 1 样本 B 是 
0 的 维度 的 个数 r 样本 A 是 0 
样本 B 是 1 的 维度 的 个数 s 样本 
A 与 B 都是 0 的 维度 的 个数 那么 
样本 A 与 B 的 杰 卡德 相似系数 可以 表示 
为 这里 p + q + r 可 理解 为 
A 与 B 的 并 集 的 元素 个数 而 
p 是 A 与 B 的 交集 的 元素 个数 
而 样本 A 与 B 的 杰 卡德 距离 表示 
为 4 Matlab 计算 杰 卡德 距离 Matlab 的 pdist 
函数 定义 的 杰 卡德 距离 跟 我 这里 的 
定义 有 一些 差别 Matlab 中将 其 定义 为 不同 
的 维度 的 个数 占 非 全零/nr 维度 的 比例 
例子 计算 1 1 0 1 1 0 1 1 
0 两两 之间 的 杰 卡德 距离 X = 1 
1 0 1 1 0 1 1 0 D = 
pdist X jaccard 结果 D = 0.5000       
0.5000       1.000010 . 相关系数 Correlation coefficient 与 
相关 距离 Correlation distance 1 相关 系数 的 定义 相关 
系数 是 衡量 随机变量 X 与 Y 相关 程度 的 
一种 方法 相关 系数 的 取值 范围 是 1 1 
相关 系数 的 绝对值 越大 则 表明 X 与 Y 
相关度 越高 当 X 与 Y 线性相关 时 相关系数 取值 
为 1 正 线性相关 或 1 负 线性相关 2 相关 
距离 的 定义 3 Matlab 计算 1 2 3 4 
与 3 8 7 6 之间 的 相关 系数 与 
相关 距离 X = 1 2 3 4 3 8 
7 6 C = corrcoef X     % 将 
返回 相关系数 矩阵 D = pdist X correlation 结果 C 
= 1.0000       0 . 47810.4781     
  1.0000 D = 0.5219 其中 0.4781 就是 相关系数 0.5219 
是 相关 距离 11 . 信息熵 Information Entropy 信息熵 并不 
属于 一种 相似性 度量 那 为什么 放在 这 篇 文章 
中 啊 这个 我 也 不 知道 ╯ ▽ ╰ 
信息熵 是 衡量 分布 的 混乱 程度 或 分散 程度 
的 一种 度量 分布 越 分散 或者说 分布 越 平均 
信息熵 就 越大 分布 越 有序 或者说 分布 越 集中 
信息熵 就 越小 计算 给定 的 样本 集 X 的 
信息 熵 的 公式 参数 的 含义 n 样 本集 
X 的 分类 数 pi X 中 第 i 类 
元素 出现 的 概率 信息熵 越大 表明 样 本集 分类 
越 分散 信息熵 越小 则 表明 样 本集 X 分类 
越 集中 当 中 n 个 分类 出现 的 概率 
一样大 时 都是 1 / n 信息熵 取 最大值 log2 
n 当 X 只有 一个 分类 时 信息熵 取 最小值 
0 参考资料 1 吴军 . 数学 之美 系列 12 余弦定理 
和 新闻 的 分类 . http / / www . 
google . com . hk / ggblog / googlechinablog / 
2006 / 07/12 _ 4010 . html 2 Wikipedia . 
Jaccard index . http / / en . wikipedia . 
org / wiki / Jaccard _ index 3 Wikipedia . 
Hamming distancehttp / / en . wikipedia . org / 
wiki / Hamming _ distance 4 求 马氏 距离 Mahalanobis 
distance matlab 版 http / / junjun0595 . blog . 
163 . com / blog / static / 9 6 
9 5 6 1 4 2 0 1 0 0 
6 3 3 3 5 1 2 1 0 / 
5 Pearson product moment correlation coefficienthttp / / en . 
wikipedia . org / wiki / Pearson _ product moment 
_ correlation _ coefficient 