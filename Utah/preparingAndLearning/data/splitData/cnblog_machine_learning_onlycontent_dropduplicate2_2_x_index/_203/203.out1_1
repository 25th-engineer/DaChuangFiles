一 集成 学习法 在 机器 学习 的 有 监督 学习 
算法 中 我们 的 目标 是 学习 出 一个 稳定 
的 且 在 各个 方面 表现 都 较好 的 模型 
但 实际 情况 往往 不 这么 理想 有时 我们 只能 
得到 多个 有 偏好 的 模型 弱 监督 模型 在 
某些 方面 表现 的 比较 好 集成 学习 就是 组合 
这里 的 多个 弱 监督 模型 以期 得到 一个 更好 
更 全面的 强 监督 模型 集成 学习 潜在 的 思想 
是 即便 某一个 弱 分类器 得到 了 错误 的 预测 
其他 的 弱 分类器 也 可以 将 错误 纠正 回来 
集成 方法 是 将 几种 机器学习 技术 组 合成 一个 
预测模型 的 元 算法 以 达到 减小 方差 bagging 偏差 
boosting 或 改进 预测 stacking 的 效果 集成 学习 在 
各个 规模 的 数据 集上 都有 很好 的 策略 数据集 
大 划分 成 多个 小 数据集 学习 多 个 模型 
进行 组合 数据集 小 利用 Bootstrap 方法 进行 抽样 得到 
多个 数据集 分别 训练 多个 模型 再 进行 组合 集合 
方法 可 分为 两类 序列 集成 方法 其中 参与 训练 
的 基础 学习 器 按照 顺序 生成 例如 AdaBoost 序列 
方法 的 原理 是 利用 基础 学习 器 之间 的 
依赖 关系 通过 对 之前 训练 中 错误 标记 的 
样本 赋值 较高 的 权重 可以 提高 整体 的 预测 
效果 并行 集成 方法 其中 参与 训练 的 基础 学习 
器 并行 生成 例如 Random Forest 并行 方法 的 原理 
是 利用 基础 学习 器 之间 的 独立性 通过 平均 
可 以 显著 降低 错误 总结 一下 集成 学习法 的 
特点 ①   将 多个 分类 方法 聚集 在 一起 
以 提高 分类 的 准确率 这些 算法 可以 是 不同 
的 算法 也 可以 是 相同 的 算法 ②   
集成 学习法 由 训练 数据 构建 一组 基 分类器 然后 
通过 对 每个 基 分类器 的 预测 进行 投票 来 
进行 分类 ③   严格来说 集成 学习 并 不算 是 
一种 分类器 而是 一种 分类器 结合 的 方法 ④   
通常 一个 集成 分类器 的 分类 性能 会 好于 单个 
分类器 ⑤   如果把 单个 分类器 比作 一个 决策 者 
的话 集成 学习 的 方法 就 相当于 多个 决策者 共同 
进行 一项 决策 自然地 就 产生 两 个 问题 1 
怎么 训练 每个 算法 2 怎么 融合 每个 算法 这篇 
博客 介绍 一下 集成 学习 的 几个 方法 Bagging Boosting 
以及 Stacking 1 Bagging bootstrap aggregating 装袋 Bagging 即 套袋 
法 先 说 一下 bootstrap bootstrap 也 称为 自助 法 
它 是 一种 有 放回 的 抽样 方法 目的 为了 
得到 统计量 的 分布 以及 置信区间 其 算法 过程 如下 
A 从 原始 样本 集中 抽取 训练 集 每 轮 
从 原始 样本 集中 使用 Bootstraping 的 方法 抽取 n 
个 训练样本 在 训练 集中 有些 样本 可能 被 多次 
抽 取到 而 有些 样本 可能 一次 都 没有 被 
抽中 共 进行 k 轮 抽取 得到 k 个 训练 
集 k 个 训练 集 之间 是 相互 独立 的 
B 每次 使用 一个 训练 集 得到 一个 模型 k 
个 训练 集 共 得到 k 个 模型 注 这里 
并 没有 具体 的 分类 算法 或 回归 方法 我们 
可以 根据 具体 问题 采用 不同 的 分类 或 回归 
方法 如 决策树 感知器 等 C 对分 类 问题 将 
上步 得到 的 k 个 模型 采用 投票 的 方式 
得到 分类 结果 对 回归 问题 计算 上述 模型 的 
均值 作为 最后 的 结果 所有 模型 的 重要性 相同 
为了 让 更好 地 理解 bagging 方法 这里 提供 一个 
例子 X 表示 一维 属性 Y 表示 类 标号 1 
或 1 测试 条件 当 x = k 时 y 
= 当 x k 时 y = k 为 最佳 
分裂 点下 表为 属性 x 对应 的 唯一 正确 的 
y 类别 现在 进行 5轮 随机抽样 结果 如下 每 一轮 
随机抽样 后 都/d 生成/v 一个/m 分类器/n 然后/c 再将/i 五轮/m 分类/n 
融合/vn 对比/v 符号/n 和/c 实际/n 类/q 我们 可以 发现 在 
该 例子 中 Bagging 使得 准确率 可达 90% 由此 总结 
一下 bagging 方法 ①   Bagging 通过 降低 基 分类器 
的 方差 改善 了 泛化 误差 ②   其 性能 
依赖于 基 分类器 的 稳定性 如果 基 分类器 不稳定 bagging 
有助于 降低 训练 数据 的 随机 波动 导致 的 误差 
如果 稳定 则 集成 分类器 的 误差 主要 由 基 
分类器 的 偏倚 引起 ③   由于 每 个 样本 
被 选中 的 概率 相同 因此 bagging 并不 侧重于 训练 
数据 集中 的 任何 特定 实例 常用 的 集成 算法 
类 是 随机 森林 在 随机 森林 中 集成/v 中的/i 
每/zg 棵树/i 都/d 是由/i 从/p 训练/vn 集中/v 抽取/v 的/uj 样本/n 
即 bootstrap 样本 构建 的 另外 与 使用 所有 特征 
不同 这里 随机 选择 特征 子集 从而 进一步 达到 对 
树 的 随机化 目的 因此 随机 森林 产生 的 偏差 
略有 增加 但是 由于 对 相关 性 较小 的 树 
计算 平均值 估计 方差 减小 了 导致 模型 的 整体 
效果 更好 2 Boosting/w 其/r 主要/b 思想/n 是/v 将/d 弱/a 
分类器/n 组装/v 成/n 一个/m 强/a 分类器/n 在 PAC probably approximately 
correct 概率 近似 正确 学习 框架 下 则/d 一定/d 可以/c 
将/d 弱/a 分类器/n 组装/v 成/n 一个/m 强/a 分类器/n 关于 Boosting 
的 两个 核心 问题 1 在 每 一轮 如何 改变 
训练 数据 的 权值 或 概率分布 通过 提高 那些 在 
前 一轮 被 弱 分类器 分 错 样例 的 权值 
减小 前 一轮 分对 样例 的 权值 来 使得 分类器 
对 误 分 的 数据 有 较好 的 效果 2 
通过 什么 方式 来 组合 弱 分类器 通过 加法 模型 
将 弱 分类器 进行 线性组合 比如 AdaBoost Adaptive boosting 算法 
刚 开始 训练 时对/nr 每一个 训练 例 赋 相等 的 
权重 然后 用 该 算法 对 训练 集 训练 t 
轮 每次 训练 后 对 训练 失败 的 训练 例 
赋以 较大 的 权重 也 就是 让 学习 算法 在 
每次 学习 以后 更 注意 学 错 的 样本 从而 
得到 多 个 预测 函数 通 过拟合 残差 的 方式 
逐步 减小 残差 将 每一步 生成 的 模型 叠加 得到 
最 终模型 GBDT Gradient Boost Decision Tree 每一次 的 计算 
是 为了 减少 上 一次 的 残差 GBDT 在 残差 
减少 负 梯度 的 方向 上 建立 一个 新 的 
模型 3 StackingStacking 方法 是 指 训练 一个 模型 用于 
组合 其他 各个 模型 首先 我们 先 训练 多个 不同 
的 模型 然后 把 之前 训练 的 各个 模型 的 
输出 为 输入 来 训练 一个 模型 以 得到 一个 
最终 的 输出 理论上 Stacking 可以 表示 上面 提到 的 
两种 Ensemble 方法 只要 我们 采用 合适 的 模型 组合 
策略 即可 但在 实际 中 我们 通常 使用 logistic 回归 
作为 组合 策略 如 下图 先 在整个 训练 数据集 上 
通过 bootstrap 抽样 得到 各个 训练 集合 得到 一 系列 
分类 模型 然后 将 输出 用于 训练 第二层 分类器 二 
Bagging Boosting 二者 之间 的 区别 1 Bagging 和 Boosting 
的 区别 1 样本 选择 上 Bagging 训练 集 是 
在 原始 集 中有 放回 选取 的 从 原始 集中 
选出 的 各 轮 训练 集 之间 是 独立 的 
Boosting 每 一轮 的 训练 集 不变 只是 训练 集中 
每个 样例 在 分类器 中的 权重 发生变化 而 权值 是 
根据 上 一轮 的 分类 结果 进行 调整 2 样例 
权重 Bagging 使用 均匀 取样 每个 样例 的 权重 相等 
Boosting 根据 错误率 不断 调整 样例 的 权值 错误率 越大 
则 权重 越大 3 预测 函数 Bagging 所有 预测 函数 
的 权重 相等 Boosting 每个 弱 分类器 都有 相应 的 
权重 对于 分类 误差 小 的 分类器 会有 更大 的 
权重 4 并行计算 Bagging 各个 预测 函数 可以 并行 生成 
Boosting 各个 预测 函数 只能 顺序 生成 因为 后 一个 
模型 参数 需要 前 一轮 模型 的 结果 2 决策树 
与 这些 算法 框架 进行 结合 所 得到 的 新的 
算法 1 Bagging + 决策树 = 随机 森林 2 AdaBoost 
+ 决策树 = 提升 树 3 Gradient Boosting + 决策树 
= GBDT 参考 博文 1 集成 学习 总结 & Stacking 
方法 详解     https / / blog . csdn 
. net / willduan1 / article / details / 73618677 
2 Bagging 和 Boosting 概念 及 区别     https 
/ / www . cnblogs . com / liuwu265 / 
p / 4690486 . html 3 集成 学习法 之 bagging 
方法 和 boosting 方法   https / / blog . 
csdn . net / qq _ 30189255 / article / 
details / 51532442 4 机器学习 中的 集成 学习 Ensemble Learning 
    http / / baijiahao . baidu . com 
/ s id = 1 5 9 0 2 6 
6 9 5 5 4 9 9 9 4 2 
4 1 9 & wfr = spider & for = 
pc 5 简单 易学 的 机器学习 算法 集成 方法 Ensemble 
Method   https / / blog . csdn . net 
/ google19890102 / article / details / 46507387 