机器学习 编年史 这几年 机器学习 火热 很大 程度 上 是 由于 
深度 学习 分 支给 机器学习 送入 了 大量 新鲜 活力 
统计学家 说 我 概率 统计理论 我 来讲 代表 Leonard E 
. Baum 隐 马尔科夫 模型 数学家 说 我 有 严谨 
数学 证明 我 来讲 代表 Vapnik SVM 支持 向量 机 
神经计算 & 计算机 科学家 说 我 有 自然法则 我 来讲 
代表 Geoffrey Hinton 神经网络 这三家 的 理论 争鸣 促成 了 
今天 机器学习 学派 三分 天下 局面 按 历史 来说 对 
概率 统计 了解 有限 编年史 多有 遗漏 1965年 Rosenblatt 感知器 
开启 了 神经 网络 学派 的 先河 神经网络 学派 1965年 
HMM 隐 马尔科夫 模型 掀起 了 统计 预测学 的 热潮 
概率 统计 学派 1974年 基于 信息熵 的 ID3 决策树 掀起 
了 统计 机器学习 热潮 统计 机器学习 学派 1980年 马尔科夫 随 
机场 提出 概率 统计 学派 1982年 Hopfield 神经 网络 引起 
神经 模拟 热潮 神经网络 学派 1984年 CART 分类 回归 树 
提出 统计 机器 学派 1985年 贝叶斯 网络 提出 概率 统计 
学派 1986年 反向 传播 训练 算法 提出 前馈 神经网络 登上 
舞台 神经网络 学派 1986年 RBM 限制 玻尔兹曼 神经网络 因为 无 
有效 训练 算法 无 影响 神经网络 学派 1990年 多层 前馈 
网络 实现 无果 神经网络 走向 低谷 神经网络 学派 1990年 循环 
递归 神经网络 RNN Elman Jordan 提出 神经网络 学派 1992年 SVM 
支持 向量 机 提出 因为 训练 算法 需要 二 次 
规划 关注 不高 统计 机器学习 学派 1993年 C 4.5 决策树 
提出 统计 机器学习 学派 1997年 RNN 变种 LSTM 长短期 记忆 
网络 提出 神经网络 学派 1998年 CNN 卷积 神经网络 提出 神经网络 
学派 1998年 SVM 支持 向量 机 的 SMO 训练 算法 
提出 SVM 开始 普及 统计 机器学习 学派 2000年 Adaboost 决策树 
提出 统计 机器学习 学派 2001年 随机 森林 决策树 提出 统计 
机器学习 学派 2005年 对比 散度 训练 算法 提出 RBM 限制 
玻尔兹曼 登上 舞台 神经网络 学派 2006年 深 信度 网络 DBN 
提出 深度 学习 概念 提出 神经网络 学派 2007年 AutoEncoder 自动 
编码器 提出 神经网络 学派 理性 与 感性 之争 数理 证明 
与 假设 实验 细分 为 三派 其实 归根结底 就是 两派 
① 拥有 严谨 推理 证明 的 理性 派 统计 机器学习 
派 + 概率 统计 学派 ② 崇尚 先 假设 后 
验证 的 感性 派 神经网络 学派 有趣 的 是 这 
两股 力量 分别 有 学术界 两位 泰斗 坐镇 分别 是 
Michael I . Jordan 和 Geoffrey Hinton 摘自 http / 
/ blog . sina . com . cn / s 
/ blog _ 5980285201015311 . html 的 人物 列传 Yann 
LeCun 是 Geoffrey Hinton 的 博士 Yoshua Bengio 是 Michael 
Jordan 的 博士后 Yann LeCun 与 Yoshua Bengio 在 AT 
& T Bell Labs 是 同事 1992 1993 ★ Hinton 
今年 70岁 是 逻辑学家 乔治 布尔 bool 类型 19 世纪 
著名 数学家 的 玄孙 他 父亲 是 昆虫学家 他 祖父 
是 数学 家和 科幻 小说家 他 曾祖父 是 外科 医生 
和 自然 生理 小说家 ★ Jordan 曾经 想 申请 Hinton 
的 博士生 被 拒 了 理由 论文 太少 ★ Jordan 
在 神经 网络 中 出名 之作 的 就是 在 1990年 
提出 的 Jordan s simple RNN 此时 神经 网络 已经 
进入 衰落期 ★ Jordan 被 拒 了 之后 从 神经网络 
转向 了 概率 统计 概率 图 蹲点 Stanford Andrew Ng 
是 他 的 学生 ★ Andrew Ng 的 同事 Coursera 
联合 创始人 Daphne Koller 是 概率 图 模型 大牛 但 
Ng 本人 却 去 搞了 深度 学习 ★ 机器学习 两 
大 顶级 会议 之一 的 NIPS 神经 信息 促进 大会 
基本 由 下图 这些人 以及 各自 的 研究组 控制 着 
理性 派 发言人 Michael I . Jordan 机器学习 不能 没有 
数学 证明 引用 自 http / / www . infoq 
. com / cn / news / 2014/10 / reddit 
首先 Jordan 提到 他 个人 并 没有 把 统计学 和 
机器 学习 区 分开 他 认为 把 理论 和 实际 
明确 分开 是 没有 很 大用处 的 它们 的 相互 
作用 已经 被 证明 了 是 很 有用 的 当 
越来越 复杂 的 系统 诞生 时 它们 也 会 互相 
促进 并且 提升 设想 一下 建 一座 桥 的 工程 
这是/i 从/p 物理/n 界/n 到/v 人文/n 界/n 工程师/n 都/d 需要/v 
一起/m 参与/v 的/uj 工程/n 链/n 建筑师 需要 设计 桥梁 土木 
工程师 需要 保证 桥梁 不会 在 某些 极端 条件 下 
塌陷 等等 在 这条 链 中 几乎 没 有人 不 
知道 把 理论 概念 和 工程 实际 结合 起来 这 
一点 已经 在 几十年 里 都被 证明 过了 类似 的 
Maxwell 方程 为 电子学 提供 了 理论 依据 但是/c 感应/v 
匹配/v 之类/r 的/uj 想法/v 也是/i 在/p 工程/n 上/f 开始/v 建造/v 
电路/n 和和/nr 流水/n 线/n 之后/f 才/d 进入/v 人们/n 的/uj 视线/n 
的/uj 这些/r 想法/v 都是/nr 结合/v 了/ul 理论/n 和/c 实际/n 的/uj 
感性 派 发言人 Yoshua Bengio 只需 实验 假设 和 验证 
引用 自 http / / www . infoq . com 
/ cn / articles / ask yoshua bengio / 问 
Bengio 教授 您好 我 是 McGill 大学 的 本科生 从事 
类型论 译者 注 与 集合论 差不多 的 学科 方面 的 
研究 我 的 问题 是 我 所在 的 领域 很 
注重 形式化 证明 机器学习 领域 有 没有 关注 形式化 证明 
的 课题 呢 如果 没有 的话 怎么 保证 学科 的 
严谨 性 有 没有 人 研究 用 深度 学习 产生 
程序 呢 答 有 一种 非常 简单 的 方法 让 
我们 无需 证明 就 能够 得到 学科 的 严谨 性 
大家 都 经常 用到 叫做 科学 方法论 它 依赖 实验 
假设 和 验证 另外 深度 学习 的 论文 里 逐渐 
出现 越来越 多 的 数学 人工智能 与 先验 知识 一个 
机器学习 问题 如果 有 严谨 的 数学 证明 那么 就 
没有 什么 讨论 的 必要 了 但 人工智能 属于 超级 
超级 超级 的 民 科 问题 打开 百度 贴吧 人工智能 
满眼 神经网络 学派 作为 机器学习 派系 之一 主要 任务 就是 
把 人工智能 这个 民 科 问题 给 专科化 武器 叫 
Prior Knowledge 在 Bengio 的 论文 Learning Deep Architectures for 
AI 提到 较多 所谓 的 先验 知识 就是 凭感觉 无 
严谨 数学 证明 的 黑 科技 AI 那一套 鬼 Agent 
理论 本质 就是 依赖 大量 的 Prior 植入 模型 系统 
搞出 伪 人工智能 Prior If else 和 机器人 足球 Robocup 
基本 就是 If . . . else . . . 
撑 起来 的 本质 上 可以 算是 一种 低级 的 
Prior 当然 数学 证明 就 免了 If . . . 
else 相当于 直接 向 模型 里 注入 人工 设定 的 
响应 内容 这种 方法 是 冯 诺依曼 结构 的 典型 
例子 众所周知 限于 条件 冯 诺依曼 造 不出 终极 人工智能 
机器 图灵机 于是 用 了 这种 暴力 的 方法 填充 
出 低级 智能 和 下面 的 具有 学习 能力 的 
Prior 相比 实在 是 太 Low 了 而 下面 这些 
具有 Adaptive Learning 的 Prior 则是 对 神经 响应 机制 
建模 利用 这些 机制 生成 响应 内容 按照 Hinton 的 
说法 叫做 Neural Computation & Adaptive Perception 神经计算 与 自适应 
感知 Prior Smooth 神经 网络 与 KNNBengio 在 Learning Deep 
Architectures for AI 指出 SVM 本质 就是 依赖 一个 叫 
Smooth 的 Prior 这个 Prior 的 作用 如下 给定 $ 
X $ 学习 $ F X $ 给定 $ X 
$ 预测 $ F X $ 用 通俗 语言 讲 
就是 A 哎 那个 小孩 像 猪 咧 B 噢 
这个 小孩 挺 像 你 说 的 那个 小孩 所以 
也像 猪 咧 考虑到 SVM 的 拓扑结构 几乎 和 2层 
神经网络 一样 支持 向量 等效于 隐 层 神经元 所以 SVM 
与 NN 都是 Smooth 的 产物 SVM 在 Optimization 优于 
2 NN 但是 在 Generalization 没有 突破 2 NN 若 
从 Optimization 来讲 早在 1993年 就有 数学 方面 的 大神 
证明了 论文 神经网络 只需 1个 隐藏 层 和n个/nr 隐藏 节点 
即 能把 任意 光滑 函数 拟 合到 1 / n 
的 精度 这几年 随着 Dropout AdaDelta Pre Training 等 有效 
方法 提出 SVM 在 Optimization 上 的 优势 已经 不大 
了 使用 Smooth 的 还有 数据挖掘 十大 算法 之一 的 
KNN 毫无 难度 的 KNN 位列 十大 算法 的 原因 
只有 一个 大 部分 情况 下 KNN 效果 太好了 但是 
没人 能从 数学 角度 说明 KNN 为什么 效果 这么 好 
一般 情况 下 2层 神经网络 NN or SVM = KNN 
Logistic Softmax 回归 当然 偏要 KNN 解释 起来 就 一句话 
根据 经验 可证 距离 近 就 very good 以 距离 
来 评价 本质上 是 对比 $ X $ 和$X/nr $ 
的 差异 所以 仍然 使用 了 Smooth Prior 针对 图像 
空间 相对 平滑 的 特殊 处理 手段 CNN 卷积 神经 
网络 中 除了 Smooth 之外 还有 注入 了 一些 针对 
图像 处理 的 Prior 局部 连接 Locality 权值 共享 Weight 
Sharing 降 采样 Pooling 这些 特殊 手段 都 有利于 提取 
出 图像 数据 中的 Data Distribution Prior 针对 时序 记忆 
的 特殊 处理 手段 RNN 中 除了 注入 Smooth 之外 
还有 针对 自然语言 上下文 关联 的 Prior 即 时序 记忆 
在 当前 状态 T 输入 记忆 着 1 2 . 
. . . . T 1 的 所有 状态 协助 
理解 自然 语言 的 上下文 含义 Prior 重构 学习 与 
降噪 众所周知 Hinton 组 的 RBM 是 有 严格 数学 
证明 的 很 明显 是 一个 $ P X $ 
生成 模型 但是 Bengio 组 在 一年 后 提出 的 
AutoEncoder 则 没有 取而代之 则 是以 Reconstruction 重构 描述 对比 
重构 实际上 是 根据 RBM 的 数学 证明 演化 的 
Prior 从 感觉 上 来看 重构 可以 迫使 参数 学习 
到 数据 特征 Bengio 组 在 一年 后 又 提出 
Denoising AutoEncoder 作者 是 这么 描述 的 DA 机 有效 
的 原因 可以 看作 是 把 一块 物体 挡住 一部分 
可以 更好 的 认知 尽管 DA 机 完全 是 Prior 
概念 产物 但是 在 实验 中 N 个 DA 机 
连接起来 效果 N 个 由 概率 推导 RBM 自然法则 莫名其妙 
又把 数学 打 趴 了 Prior 逐层 贪心 Pre Training 
与 Fine Tuninghttp / / t . cn / RLDeu7e 
的 结尾 作者 留了 一句 吐槽 Unlike Hinton We don 
t know how the brain works . 在 深度 学习 
概念 提出 的 11 年前 的 1995年 认知 心理学 专业 
出身 的 Hinton 就在 论文 里 提出 了 雏形 Wake 
Sleep 算法 摘自 http / / blog . csdn . 
net / zouxy09 / article / details / 8775518 的 
解释 1 wake 阶段 认知过程 通过 外界 的 特征 和 
向上 的 权重 认知 权重 产生 每 一层 的 抽象 
表示 结点 状态 并且 使用 梯度 下降 修改 层间 的 
下行 权重 生成 权重 也 就是 如果 现实 跟 我 
想象 的 不一样 改变 我 的 权重 使得 我 想象 
的 东西 就是 这样 的 2 sleep 阶段 生成 过程 
通过 顶层 表示 醒 时 学得 的 概念 和 向下 
权重 生成 底层 的 状态 同时 修改 层间 向上 的 
权重 也 就是 如果 梦中 的 景象 不是 我 脑中 
的 相应 概念 改变 我 的 认知 权重 使得 这种 
景象 在 我 看来 就是 这个 概念 Wake 阶段 就是 
Pre Training Sleep 阶段 就是 Fine Tuning 逐层 贪心 Pre 
Training 与 Fine Tuning 仍然 是 从 数学 方面 说不清 
的 详见 我 的 深度 神经网络 结构 以及 Pre Training 
的 理解 Prior DropoutHinton 的 最新 成果 Dropout 可以 有效 
让 神经 网络 避免 过拟合 Dropout 本质 就是 每次 前 
向 传播 以 一定 概率 如 50% 丢掉 当 前层 
的 输出 然后 继续 学习 理解 起来 就是 多次 看 
一个 物体 从 不同 角度 来看 假设 数据 是 全的/nr 
丢掉 之后 模拟 出 不同 角度 这个 按照 概率 丢弃 
的 方法 仍然 无法 证明 但是 在 实验 中 对于 
任何 类型 的 神经 网络 都能 有效 避免 过拟合 Prior 
还有 更多 . . . . . . 从 某种 
意义 上 来看 深度 学习 分支 就是 在 寻找 更多 
的 Prior 然后 用 实验 验证 这些 Prior 但 不是 
用 数学 证明 这些 Prior 你 也 无法 用 数学 
证明 这些 莫名其妙 的 自然 规则 Smooth + 图像 Prior 
就 使得 CNN 在 图像 数据 上 甩开 FullyConnected 网络 
几条 街 FC 负 似 然 函数 1 . 691.551 
. 491.441 . 321.251 . 161.071 . 051.00 验证 集 
错误率 55% 53% 52% 51% 49% 48% 49% 49% 49% 
49% CNN 负 似 然 函数 1 . 871.451 . 
251.151 . 050.980 . 940.890 . 70.63 验证 集 错误率 
55% 50% 44% 43% 38% 37% 35% 34% 32% 31% 
所以 如果 模型 中 注入 越多 的 Prior 那么 应该 
就越 强大 按照 大一统 机器学习 理论 的 想法 XX 年后 
会 出现 一个 Model 能够 解决 一切 机器学习 问题 概率 
与 人工 智 能从 任务 来看 知乎 http / / 
t . cn / RLDeu7e 对 两派 做了 分析 我 
的 观点 是 Prior 把 AI 和 统计 拉近 了 
DeepLearningNet Tutorial 在 三大 模式识别 问题 CV NLP Speech 中 
唯一 没 涉及 到 的 就是 Speech 因为 Speech Recognization 
任务 没法用 Softmax 完成 依赖于 HMM 隐 马尔科夫 模型 HMM 
属于 概率 图 阵营 和 DeepLearing 没有 关系 传统 的 
Speech Recognization 是 GMM HMM 即 高斯 混合模型 提 特征 
隐 马尔科夫 做 判别 2006年 Hinton 提出 DBN 之后 首先 
应用 的 不是 CV 而是 Speech 于是 出现 了 奇葩 
DNN HMM 逐层 贪心 预 训练 过后 的 DNN 提取 
特征 能力 远超 GMM 所以 堪称 一大 突破 我 首次 
看到 DNN HMM 是 在 自动化所 的 模式识别 与 人工智能 
2015.1 月刊 上 科大 讯 飞 的 研究员 发表 的 
当然 这个 模型 奇葩 在于 DNN 和 HMM 分 属于 
两个 对立 的 学派 居然在 Speech Recognization 任务 上 融合 
了 按照 这个 趋势 今后 三大 学派 的 模型 可能 
有 更多 的 混合 Prior 从 贝叶斯 概率 体系 来看 
按照 PRML 中 说法 一个 机器学习 任务 可以 分为 两 
大 Stage Stage 1 Inference 推理 对应 ML 里 的 
Learning 阶段 Stage 2 Decision 决策 对应 ML 里 的 
Prediction 阶段 Learning 的 本质 任务 就是 给定 Data 建立 
学习 系统 训练 参数 即 $ \ max \ p 
W _ { param } | Data $ Prediction 的 
本质 任务 就是 给定 参数 预测 结果 即 $ \ 
max \ p Data | W _ { param } 
$ 显然 需要 准备 两套 概率 优化 方案 并且 Learning 
的 概率 是 隐性 的 非直接 观测 所以 比较 麻烦 
幸运 的 是 最大 似 然 参数估计 方法 可以 合二为一 
即 Learning 阶段 用 Prediction 阶段 的 优化 方案 所以 
设定 $ log \ \ p Data | W _ 
{ param } $ 为 Likelihood 似 然 函数 这是 
频率 统计 体系 的 观点 当然 尽管 $ maximum \ 
\ likelihood $ 可以 逆向 求解 $ W _ { 
param } $ 但也 带来 过拟合 问题 因为 这样 优化 
本质 是 $ \ max \ P Data $ 顺带 
作为 条件 求出 $ W _ { param } $ 
一旦 Data 分布 有 问题 比如 数据 过少 而 导致 
分布 不 均匀 那么 $ W _ { param } 
$ 也会 跟着 大幅度 偏离 导致 过拟合 贝叶斯 体系 与 
频率 统计 体系 最大 的 区别 在于 贝叶斯 通常在 $ 
likelihood $ 之后 引入 一个 $ prior $ $ posterior 
= likelihood \ times   prior $ 处于 隐性 状态 
的 Learning 阶段 概率 可以 通过 一个 条件 概率分析 $ 
p W _ { param } | Data = \ 
frac { p Data | W _ { param } 
\ times p W _ { param } } { 
p Data } $ 这部分 任务 可以 通过 条件 分解为 
两部分 ① $ \ max   \ p Data | 
W _ { param } $ 这是 Prediction 阶段 的 
概率 目标 也是 判别 模型 的 基础 这是 频率 统计 
学派 的 一贯 坚持 的 所见即 所得 数据 反应 一切 
如果 没有 这 部分 数据 那么 结果 就 判 别为 
0 这是 不合 常理 的 ② $ \ max   
\ p W _ { param } $ 相对 的 
这 可不 是 $ P X $ 生成 模型 在 
RRML 的 1 . 2.5 节 的 高斯分布 曲线拟合 中 
引入 了 一个 M 项 元 的 prior 假定 高斯分布 
是 0 均值 方差 倒数 是 $ \ alpha $ 
$ prior vector w = w _ { 0 } 
w _ { 1 } w _ { 2 } 
. . . w _ { M } $ 则 
Prior 的 概率分布 $ P W _ { param } 
| \ alpha = N W _ { param } 
| 0 \ alpha ^ { 1 } I = 
\ frac { \ alpha } { 2 \ pi 
} ^ { M + 1 / 2 } \ 
cdot exp \ frac { \ alpha } { 2 
} W ^ { T } W $ 将此 Prior 
应用到 高斯 判别 模型 的 似 然 函 数上 得到 
转化 后的/nr 概率 形式 $ p W _ { param 
} | x t \ alpha \ beta \ propto 
p t | x w \ beta \ cdot p 
W _ { param } | \ alpha $ 取其 
负 似 然 对数 有 目标函数 $ \ frac { 
\ beta } { 2 } \ sum _ { 
n = 1 } ^ { N } y x 
_ { n } W t _ { n } 
^ { 2 } + \ frac { \ alpha 
} { 2 } W ^ { T } W 
$ 可以 看到 第一 部分 为 不加 Prior 的 目标 
函数 第二 部分 为 Prior 的 作用 结果 如果 你 
了解 L2 Regularier 就 会 发现 用于 修正 的 Prior 
变成 了 一个 L2 Regularier 的 平方 等效 L2 即 
所谓 的 贝叶斯 线性 回归 就是 线性 回归 + L2 
Regularier 概率 延伸 Local Represention VS Distributed R e p 
r e s e n t i o n D 
i s t r i b u t e d 
Represention 摘自 Bengio 教授 在 2015年 暑期 组织 的 蒙特利尔 
暑期 深度 学习 & NLP 学习会 Link 整体 来说 Distributed 
Represention 就是指 数据 中 的 各个 维度 特征 之间 相互 
缠绕 互相 依赖 关联 这些 在 各个 维度 的 之间 
连接 着 的 隐 信息 通过 常规 label + 判别 
模型 是 无法 挖掘 的 已经 发现 的 存在 大量 
信息 相互 缠绕 的 三大 领域 就是 Image Speech Text 
这些 数据 的 第二 特点 就是 维度 大 维数 灾难 
因为 源 数据 的 维度 很大 所以 需要 进行 对应 
维度 的 内积 计算 这样 参数 维度 就 需要 很大 
考虑 某个 神经 网络 的 隐 层 有N维/nr 的 参数 
神经元 每个 维 度上 的 参数 变化 值 范围 大概 
是 1 1 近似 长度 算 为 2 当 N 
= 2时 在 一个 xy 轴 平面坐标 系内 以 1x1 
方形 为 一个 单位 则 共有 2x2 个 区域 当 
N = 3时 在 一个 xyz 轴 空间坐标 系内 以 
1x1x1 方体为/nr 一个 单位 则 共有 2x2x2 个 区域 当 
N = M 时 在 一个 M 维 空间坐标 系内 
以 1 ^ M 维 体 为 一个 单位 则 
共有 2 ^ M 个 区域 这 使得 在 拟合 
这些 高 维度 数据 时 参数 的 可 搜索 空间 
非常 大 如果 数据 过少 则 容易 造成 过拟合 解决方案 
PCA or Pre Training PCA 是 一种 聪明 的 方法 
它 通过 以下 两 方面 对 抗 上面 的 问题 
① 针对 信息 缠绕 问题 提出 线性变换 原则 即 $ 
Input = Decompose Output * LinearBase $ 它 假设 模型 
中 存在 某种 Linear 关系 通过 计算 一个 线性 基 
将 缠绕 的 信息 以 变换 的 形式 协同 起来 
② 针对 维度 过高 问题 对 变换 后的/nr 数据 进行 
压缩 降 维 减小 搜索 空间 Deep Learning 中的 Pre 
Training 就 比较 特殊 它 是 一个 参数 学习 方法 
① 针对 信息 缠绕 问题 进行 非 线性变换 即 $ 
Input = Decompose Output * Non LinearBase $ ② 针对 
搜索 空间 过 大问题 Pre Training 预先 利用 非 线性变换 
关系 引导 了 一个 搜索 方向 这个 搜索 方向 可以 
有效 避免 过拟合 Local RepresentionSVM 一个 经典 的 工作 就是 
避免 了 维度 灾难 它 的 最优 间隔 化 目标函数 
迫使 Input Layer 和 Hidden Layer 之间 点积 运算 变成 
了 两个 数据 点 之间 的 点积 而 不是 数 
据点 和 参数 的 点积 这 意味着 尽管 数据 有 
10000 维 但是 参数 无须 膨胀 到 10000 维 整个 
SVM 的 全部 参数 就是 支持 向量 的 $ \ 
alpha $ 这 等效于 隐 层 神经元 但是 本身 却 
不是 超 参数 可以 通过 优化 自行 确定 但是 SVM 
仅仅 是 神经 网络 在 Optimization 上 修改 版 其 
注入 的 Prior 仍然 是 Smooth 这 意味着 SVM 超越 
不了 神经 网络 的 瓶颈 因为 Prior Smooth 本质 就是 
一个 可供 Optimization 的 likelihood 完全 就是 个 根据 频率 
吃 数据 的 方法 朴素 贝叶斯 朴素 贝叶斯 堪称 Local 
Represention 的 典范 朴素 贝叶斯 理论 假设 输入 向量 各个 
维度 间 是 独立 的 当然 今天 有了 Distributed Represention 
之后 我们 知道 各个 维度 之间 不仅 不 独立 而且 
还 高度 缠绕 当然 朴素 贝叶斯 假设 不无 它 的 
道理 因为 想 要 计算 不 独立 的 概率 简直 
比 登天 还难 这 方面 的 工作 后来 被 2006年 
的 RBM 完成 RBM 通过 Gibbs Sampling 近似 让 参数 
训 出了 不 独立 的 概率 判别 模型 与 生成 
模型 模型 只能 拿到 Local Represention 罪魁祸首 还是 Prior 问题 
整个 Smooth 基本 就是 依赖 label 在 分割 面上 进行 
拟 合调 优 Label 中 隐含 的 信息 毕竟 是 
有限 的 CNN 是 一个 例外 尽管 它 仅仅 依赖 
Label 的 判别 模型 但是 它 在 图像 方面 却 
有着 逆天 的 Distributed Represention 抽取 能力 当然 这 归功于 
其 独有 的 一些 高效 转 化出 Represention 的 Prior 
而 生成 模型 $ P X $ 指 的 是 
RBM 能 拿到 Distributed Represention 原因 是 它 让 参数 
预 先去 拟合 了 数据 本身 部分 缠绕 的 信息 
被 Encode 锁在 了 参 数里 当 Fine Tuning 的 
时候 这些 缠绕 的 信息 就 可以 被 直接 Decode 
利用 Bengio 组 的 AutoEncoder 观点 再看 贝叶斯 概率 体系 
与 Pre Training 贝叶斯 概率 体系 饱受 批评 的 就是 
其 单纯 的 从 数学 计算 角度 选取 Prior 然后 
$ \ max   \ p W _ { param 
} $ 这 部分 其实 有 Adaptive Method 就是 Deep 
Learning 中的 Pre Training 与 PCA 之类 无 参数 方法 
的 生成 模型 不同 有 参数 的 生成 模型 最后 
实际上 已经 完成 了 $ \ max   \ p 
W _ { param } $ Pre Training 的 工作 
恰恰 是 利用 自适应 感知 的 方法 确定 一个 引导 
参数 的 Prior 这与 Erhan10 提出 的 设想 不谋而合 数学 
方法 的 M 阶 多项式 Prior 最后 变成 了 Regularizer 
而 Pre Training 最后 也 变成 了 Regularizer 其中 应该 
有 一些 联系 数据挖掘 VS 模式识别 这 两个 概念 常常 
被 搞混 掉 模式识别 当前 模式识别 三大 领域 CV NLP 
Speech 其实 都 在对 数据 进行 挖掘 挖掘 的 数据 
特点 数量 庞大 近几年 信息 维度 高 1000 ~ 65536 
维 特征 隐蔽 一句话 概括 数据 噪声 小 也 就是 
垃圾 维度 信息 很少 且 各个 维度 之间 局部性 联系 
非常 紧密 常用 算法 神经网络 概率 统计 最终 目的 感知 
世界 人工智能 数据挖掘 当然 数据挖掘/n 和/c 模式识别/n 独立/v 开来/v 应该/v 
是/v 有/v 原因/n 的/uj 数据挖掘 常见 任务 包含 欺诈 检测 
信用 评分 癌症 检测 金融 预测 挖掘 的 数据 特点 
数量 庞大 信息 维度 低 10 ~ 20 维 冗余 
量大 一句话 概括 数据 噪声 大 垃圾 维度 信息 特别 
多 如 欺诈 检测 任务 中 往往 原始数据 含 有时间 
标 码 之类 的 垃圾 信息 这些 垃圾 信息 是 
不可 直接 带入 模型 的 需要 人工 剔除 常用 算法 
统计 机器学习 以 决策树 系 为主 概率 统计 最终 目的 
服务 社会 简化 劳力 对比 ★ 从 针对 的 任务 
来看 数据挖掘 更适合 概率 统计 统计 机器学习 学派 而 模式识别 
则 更适合 神经网络 学派 ★ 数据挖掘 任务 不 需要 各种 
Prior 来 挖掘 内在 的 Distrubuted Represention 而 模式识别 则 
需要 各种 黑 科技 Prior 挖掘 数据 中的 Distrubuted Represention 
★ 数据挖掘 通常 包含 数据仓库 之类 的 额外 数据管理 技巧 
★ 这几年 模式识别 进入 大 数据 阶段 通常 额外 包含 
GPU 并行计算 设计 远景 未来 的 数据 科学 必然 是 
模式识别 和 数据 挖掘 的 二分 天下 尽管 早期 大 
部分 研究 者 都 在做 数据挖掘 任务 但 这几年 随着 
深度 学习 在 Represention 上 大 强大 挖掘 能力 以及 
GPU 并行计算 的 异军突起 感知 人工智能 与 模式识别 已经 开始 
推动 图像 文本 语音 这类 感知 数据 的 发展 大 
数据 不再 只 来自 银行 政府 公司 的 私密 统计 
而是 来自 于 互联网 用户 你 与 我 对 世界 
的 感知 这 是 一个 更为 艰难 的 挑战 因为 
感知 数据 实在 庞大 而 我们 对 生物 神经 机制 
又 了解 太少 机器学习 到底 要 做什么 当 实践性 地做 
了 创新 项目 之后 个人 对 机器 学习 的 流程 
任务 又有 了 新的 理解 个人认为 机器学习 流程 大致 分为 
三步 I 机器学习 理论 与 建模 这是 整个 机器学习 技术 
进步 的 核心 机器学习 两 大 顶级 会议 ICML 夏 
NIPS 冬 相隔 半年 世界 上 的 研究组 在 这 
半年 中 主要 任务 就是 互相 撕 逼 在 模型 
结构 算法 精度 上 提出 研究方案 你 有了 state of 
art 我 下次 就 秒 了 你 的 state of 
art 然后 我 的 state of art 又被 你 秒 
了 II 机器学习 实现 与 平台 建模 和 实现 是 
两回事 ① 典型 例子 就是 SVM 1993年 模型 提出 但是 
直到 1997年 SMO 的 高效 实现 出来 才算 大 范围 
普及 另 一个 例子 就是 Restricted Boltzmann machines 限制 玻尔兹曼 
机 1986年 提出 但是 2005年 有了 CDK K 步 对比 
分歧 算法 后 才算 大 范围 普及 这是 算法 实现 
上 的 困扰 ② Elman & Jordan RNN LSTM 分别 
在 1990年 1997年 提出 但是 在 2006年 之后 在 大 
范围 普及 因为 循环 递归 网络 本质 是 深度 神经网络 
受限 于 当时 的 计算力 难以实现 这是 机器 太 落后 
的 困扰 实现 和 平台 也 是 两回事 CNN 正式 
普及 是 在 1997年 以后 但是/c 一直/d 都是/nr 应用/v 于小/nr 
数据/n LeCun 教授 在 上个 世纪 的 MNIST 用 CPU 
计算 才 6W 的 数据 量 用 今天 的 Core 
i7 都要 跑 几个 小时 更 别提 大 数据 了 
2009年 ImageNet 推出 后 在 NVIDIA 的 配合 下 Theano 
Torch Caffe 等 GPU 计算 框架 的 推出 才 算是 
把 深度 学习 大 数据 并行计算 推向 了 民间 推向 
了 普及 机器学习 作为 计算机 研究 重点 的 最大 原因 
无外乎 就是   高效 的 实现 与 平台 涉及 到 
算法 优化 计算 优化 大 规模 计算 平台 部署 这是 
一门 手艺活 III 机器学习 应用 与 系统 你 可能 已经 
在在 MNIST Cifar10 ImageNet IMDB 等 数据 集中 游刃有余 但 
你 的 任务 和 这些 数据 没有 任何 关系 我 
接到 的 第一 个 正式 项目 是 人脸 情感 分析 
与 识别 这 不是 一个 热点 研究 问题 起码 从 
数据 上 就有 了 限制 首先 它 派生 与 人脸 
检测 你 需要 从 一般 图片 中 分离 人脸 其次 
它 需要 指定 的 人脸 中性 高兴 悲伤 愤怒 恐惧 
厌恶 惊讶 收集 或 采集 一张 人脸 是 简单 的 
但是 这张 脸 还 有有 指定 的 表情 这 就是 
为 难了 我们 小组 尝试 通过 各大 搜索引擎 去 寻找 
Wild 数据 尽全力 之下 数据 仍然 不多 表情 高兴 悲伤 
愤怒 惊讶 厌恶 数量 398张 1073张 204张 214张 44张 这 
数字 不是 很 意外 因为 某些 表情 在 互联网 出现 
的 概率 确实 不大 除了 数 量 严重 不足 外 
最 致命 的 问题 就是 分类 不均衡 这是 机器学习 理论 
与 实际 最 容易 出现 的 脱节 实验 数据集 往往 
都是/nr 经过 多 少年 的 沉淀 修改 的 而 我们 
实际 能 搞到 的 Wild 数据 则是 千疮百孔 惨不忍睹 当年 
关于 不 均衡 数据 的 学习 问题 也是 当前 机器 
学习 的 研究 重点 起码 在 当下 我们 还是 得 
活受罪 为了 解决 数据 不足 的 问题 我 的 导师 
推荐 参考 香港中文大学 汤晓鸥/nr 教授 小组 DeepID 的 做法 对 
一份 数据 进行 PATCH 修改 一份 当成 多份 使 在 
之后 我 自己 捣鼓 出了 9倍 化 的 PATCH 方案 
并 通过 投票 机制 对 CNN 检测 结果 加上 这个 
投票 的 Prior 修正 最后 得到 了 5% 的 精度 
提升 能过/nr 仿真 出 结果 后 你 还得 设计 一个 
系统 我 接手 的 这个 项目 的 本 尊 是 
云 机器人 这 只是 计算机 视觉 设计 部分 为了 配合 
网络 你 得 为 你 的 Model 设计 一个 远程 
系统 由于 Theano 工作 在 Python 下 为了 效率 我 
用 C + + / Qt 设计 了 两个 Server 
Python 设计 的 一个 Server 在 三个 Sever 的 协同 
工作 下 才 勉强 完成 远程 计算 任务 机器学习 系统 
很 重要 一味 地 跑 仿真 跑 人家 做好 的 
数据 只会 蒙蔽 你 的 双眼 毕竟 我们 得 使用 
机器学习 技术 来 解决 实际 问题 