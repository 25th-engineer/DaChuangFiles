机器学习编年史
这几年机器学习火热，很大程度上是由于深度学习分支给机器学习送入了大量新鲜活力。
统计学家说：“我概率统计理论，我来讲！" （代表 Leonard E. Baum [隐马尔科夫模型])
数学家说：“我有严谨数学证明，我来讲！” （代表 Vapnik [SVM支持向量机])
神经计算&计算机科学家说：“我有自然法则，我来讲！” (代表 Geoffrey Hinton [神经网络])
这三家的理论争鸣，促成了今天机器学习学派三分天下局面。
按历史来说（对概率统计了解有限，编年史多有遗漏)：
1965年 Rosenblatt感知器开启了神经网络学派的先河。[神经网络学派]
1965年 HMM隐马尔科夫模型掀起了统计预测学的热潮。[概率统计学派]
1974年 基于信息熵的ID3决策树掀起了统计机器学习热潮。[统计机器学习学派]
1980年 马尔科夫随机场提出。[概率统计学派]
1982年 Hopfield神经网络引起神经模拟热潮。[神经网络学派]
1984年 CART分类回归树提出。[统计机器学派]
1985年 贝叶斯网络提出。[概率统计学派]
1986年 反向传播训练算法提出，前馈神经网络登上舞台。[神经网络学派]
1986年 RBM限制玻尔兹曼神经网络，因为无有效训练算法，无影响。[神经网络学派]
1990年 多层前馈网络实现无果，神经网络走向低谷。[神经网络学派]
1990年 循环递归神经网络RNN(Elman、Jordan)提出。[神经网络学派]
1992年 SVM支持向量机提出，因为训练算法需要二次规划，关注不高。[统计机器学习学派]
1993年 C4.5决策树提出。[统计机器学习学派]
1997年 RNN变种，LSTM长短期记忆网络提出。[神经网络学派]
1998年 CNN卷积神经网络提出。[神经网络学派]
1998年 SVM支持向量机的SMO训练算法提出，SVM开始普及。[统计机器学习学派]
2000年 Adaboost决策树提出。[统计机器学习学派]
2001年 随机森林决策树提出。[统计机器学习学派]
2005年 对比散度训练算法提出，RBM限制玻尔兹曼登上舞台。[神经网络学派]
2006年 深信度网络(DBN)提出，深度学习概念提出。[神经网络学派]
2007年 AutoEncoder自动编码器提出。[神经网络学派]
理性与感性之争：数理证明与假设实验
细分为三派，其实归根结底就是两派：
①拥有严谨推理证明的理性派。[统计机器学习派+概率统计学派]
②崇尚先假设，后验证的感性派。[神经网络学派]
有趣的是，这两股力量分别有学术界两位泰斗坐镇，分别是Michael I. Jordan和Geoffrey Hinton
摘自 http://blog.sina.com.cn/s/blog_5980285201015311.html 的人物列传：
Yann LeCun是Geoffrey Hinton的博士
Yoshua Bengio是Michael Jordan的博士后
Yann LeCun与Yoshua Bengio在AT&T Bell Labs是同事(1992-1993)
★Hinton今年70岁，是逻辑学家乔治·布尔(bool类型，19世纪著名数学家)的玄孙。
他父亲是昆虫学家，他祖父是数学家和科幻小说家，他曾祖父是外科医生和自然生理小说家。
★Jordan曾经想申请Hinton的博士生，被拒了，理由：论文太少。
★Jordan在神经网络中出名之作的就是在1990年提出的Jordan‘s simple RNN，此时神经网络已经进入衰落期。
★Jordan被拒了之后，从神经网络转向了概率统计(概率图)，蹲点Stanford，Andrew Ng是他的学生。
★Andrew Ng的同事，Coursera联合创始人Daphne Koller是概率图模型大牛，但Ng本人却去搞了深度学习。
★机器学习两大顶级会议之一的NIPS(神经信息促进大会)，基本由下图这些人、以及各自的研究组控制着。
理性派发言人Michael I. Jordan：机器学习不能没有数学证明
引用自：http://www.infoq.com/cn/news/2014/10/reddit
首先Jordan提到，他个人并没有把统计学和机器学习区分开。
他认为把理论和实际明确分开是没有很大用处的，它们的相互作用已经被证明了是很有用的，当越来越复杂的系统诞生时，它们也会互相促进并且提升。
设想一下建一座桥的工程，这是从物理界到人文界工程师都需要一起参与的工程链，建筑师需要设计桥梁，土木工程师需要保证桥梁不会在某些极端条件下塌陷等等。
在这条链中几乎没有人不知道把“理论概念”和“工程实际”结合起来，这一点已经在几十年里都被证明过了。
类似的，Maxwell方程为电子学提供了理论依据，但是感应匹配之类的想法也是在工程上开始建造电路和和流水线之后才进入人们的视线的，这些想法都是结合了理论和实际的。
感性派发言人Yoshua Bengio：只需实验、假设和验证
引用自 http://www.infoq.com/cn/articles/ask-yoshua-bengio/
问：Bengio教授您好，我是McGill大学的本科生，从事类型论（译者注：与集合论差不多的学科）方面的研究，我的问题是：
我所在的领域很注重形式化证明，机器学习领域有没有关注形式化证明的课题呢？
如果没有的话，怎么保证学科的严谨性？有没有人研究用深度学习产生程序 呢？
答：有一种非常简单的方法，让我们无需证明，就能够得到学科的严谨性，大家都经常用到：叫做科学方法论，
它依赖 实验、假设和验证。另外，深度学习的论文里逐渐出现越来越多的数学。
人工智能与先验知识
一个机器学习问题如果有严谨的数学证明，那么就没有什么讨论的必要了。
但人工智能属于超级超级超级的民科问题，打开百度贴吧：人工智能，满眼：
神经网络学派作为机器学习派系之一，主要任务就是把人工智能这个民科问题给专科化。
武器叫 "Prior-Knowledge”，在Bengio的论文Learning Deep Architectures for AI 提到较多。
所谓的先验知识，就是凭感觉、无严谨数学证明的黑科技。
AI那一套鬼Agent理论，本质就是依赖大量的Prior植入模型系统，搞出伪人工智能。
Prior：If、else和机器人足球
Robocup基本就是If...else...撑起来的，本质上可以算是一种低级的Prior。当然数学证明就免了。
If...else相当于直接向模型里注入人工设定的响应内容。
这种方法是冯·诺依曼结构的典型例子。众所周知，限于条件，冯·诺依曼造不出终极人工智能机器——图灵机。
于是用了这种暴力的方法填充出低级智能，和下面的具有学习能力的Prior相比，实在是太Low了。
而下面这些具有Adaptive Learning的Prior则是对神经响应机制建模，利用这些机制生成响应内容。
按照Hinton的说法，叫做“Neural Computation&Adaptive Perception”（神经计算与自适应感知）
Prior：Smooth、神经网络与KNN
Bengio在Learning Deep Architectures for AI 指出SVM本质就是依赖一个叫Smooth的Prior。
这个Prior的作用如下：
给定$X$，学习$F(X)$
给定$X'$，预测$F(X')$
用通俗语言讲就是：
A：哎，那个小孩像猪咧！
B：噢，这个小孩挺像你说的那个小孩，所以也像猪咧！
考虑到SVM的拓扑结构几乎和2层神经网络一样，支持向量等效于隐层神经元，所以，SVM与NN都是Smooth的产物。
SVM在Optimization优于2-NN，但是在Generalization没有突破2-NN。
若从Optimization来讲，早在1993年，就有数学方面的大神证明了：[论文]
神经网络只需1个隐藏层和n个隐藏节点，即能把任意光滑函数拟合到1/n的精度。
这几年随着Dropout、AdaDelta、Pre-Training等有效方法提出，SVM在Optimization上的优势已经不大了。
————————————————————————————————————————————————
使用Smooth的还有数据挖掘十大算法之一的KNN。
毫无难度的KNN位列十大算法的原因只有一个：大部分情况下，KNN效果太好了，
但是没人能从数学角度说明KNN为什么效果这么好。
一般情况下：2层神经网络(NN or SVM) >= KNN > Logistic(Softmax)回归
当然偏要KNN解释起来就一句话：根据经验，可证：距离近就very good。
以距离来评价，本质上是对比$X$和$X'$的差异，所以仍然使用了Smooth。
Prior：针对图像空间相对平滑的特殊处理手段
CNN卷积神经网络中，除了Smooth之外。还有注入了一些针对图像处理的Prior：
局部连接：Locality
权值共享：Weight Sharing
降采样：Pooling
这些特殊手段，都有利于提取出图像数据中的Data Distribution。
Prior：针对时序记忆的特殊处理手段
RNN中，除了注入Smooth之外。还有针对自然语言上下文关联的Prior，即时序记忆。
在当前状态T，输入记忆着1、2、.....T-1的所有状态，协助理解自然语言的上下文含义。
Prior：重构学习与降噪
众所周知，Hinton组的RBM是有严格数学证明的，很明显是一个$P(X)$生成模型。
但是Bengio组在一年后提出的AutoEncoder则没有，取而代之则是以Reconstruction(重构)描述。
对比重构实际上是根据RBM的数学证明演化的Prior，从感觉上来看，重构可以迫使参数学习到数据特征。
Bengio组在一年后，又提出Denoising AutoEncoder，作者是这么描述的：
DA机有效的原因可以看作是，把一块物体挡住一部分，可以更好的认知。
尽管DA机完全是Prior概念产物，但是在实验中，N个DA机连接起来效果>N个由概率推导RBM，
自然法则莫名其妙又把数学打趴了。
Prior：逐层贪心Pre-Training与Fine-Tuning
http://t.cn/RLDeu7e 的结尾，作者留了一句吐槽：Unlike Hinton, We don't know how the brain works.
在深度学习概念提出的11年前的1995年，认知心理学专业出身的Hinton就在论文里提出了雏形“Wake-Sleep”算法。
摘自 http://blog.csdn.net/zouxy09/article/details/8775518 的解释：
1）wake阶段：认知过程，通过外界的特征和向上的权重（认知权重）产生每一层的抽象表示（结点状态），
并且使用梯度下降修改层间的下行权重（生成权重）。也就是“如果现实跟我想象的不一样，改变我的权重使得我想象的东西就是这样的”。
2）sleep阶段：生成过程，通过顶层表示（醒时学得的概念）和向下权重，生成底层的状态，同时修改层间向上的权重。
也就是“如果梦中的景象不是我脑中的相应概念，改变我的认知权重使得这种景象在我看来就是这个概念”。
Wake阶段就是Pre-Training，Sleep阶段就是Fine-Tuning。
逐层贪心Pre-Training与Fine-Tuning仍然是从数学方面说不清的，详见我的 深度神经网络结构以及Pre-Training的理解
Prior：Dropout
Hinton的最新成果Dropout，可以有效让神经网络避免过拟合。
Dropout本质就是：每次前向传播，以一定概率(如50%)丢掉当前层的输出，然后继续学习。
理解起来就是：多次看一个物体，从不同角度来看(假设数据是全的，丢掉之后模拟出不同角度)。
这个按照概率丢弃的方法仍然无法证明，但是在实验中，对于任何类型的神经网络，都能有效避免过拟合。
Prior：还有更多......
从某种意义上来看，深度学习分支就是在寻找更多的Prior，然后用实验验证这些Prior。
但不是用数学证明这些Prior，你也无法用数学证明这些莫名其妙的自然规则。
Smooth+图像Prior就使得CNN在图像数据上甩开FullyConnected网络几条街。
[FC]
负似然函数
1.69
1.55
1.49
1.44
1.32
1.25
1.16
1.07
1.05
1.00
验证集错误率
55%
53%
52%
51%
49%
48%
49%
49%
49%
49%
[CNN]
负似然函数
1.87
1.45
1.25
1.15
1.05
0.98
0.94
0.89
0.7
0.63
验证集错误率
55%
50%
44%
43%
38%
37%
35%
34%
32%
31%
所以，如果模型中注入越多的Prior，那么应该就越强大。
按照大一统机器学习理论的想法：XX年后，会出现一个Model，能够解决一切机器学习问题。
概率与人工智能
从任务来看
知乎 http://t.cn/RLDeu7e对两派做了分析，我的观点是：Prior把AI和统计拉近了。
DeepLearningNet Tutorial在三大模式识别问题：CV、NLP、Speech中唯一没涉及到的，就是Speech
因为Speech Recognization任务没法用Softmax完成，依赖于HMM隐马尔科夫模型。
HMM属于概率图阵营，和DeepLearing没有关系。
传统的Speech Recognization是GMM-HMM，即高斯混合模型提特征，隐马尔科夫做判别。
2006年，Hinton提出DBN之后，首先应用的不是CV，而是Speech。
于是出现了奇葩：DNN-HMM。逐层贪心预训练过后的DNN，提取特征能力远超GMM，所以堪称一大突破。
我首次看到DNN-HMM是在自动化所的《模式识别与人工智能》2015.1月刊上，科大讯飞的研究员发表的。
当然，这个模型奇葩在于，DNN和HMM分属于两个对立的学派，居然在Speech Recognization任务上融合了。
按照这个趋势，今后三大学派的模型可能有更多的混合。
Prior：从贝叶斯概率体系来看
按照PRML中说法，一个机器学习任务可以分为两大Stage：
Stage 1：
Inference(推理)———对应ML里的Learning阶段
Stage 2:
Decision(决策)———对应ML里的Prediction阶段
Learning的本质任务就是给定Data，建立学习系统，训练参数，即$\max \, p(W_{param}|Data)$。
Prediction的本质任务就是给定参数，预测结果，即$\max \, p(Data|W_{param})$。
显然，需要准备两套概率优化方案，并且Learning的概率是隐性的(非直接观测)，所以比较麻烦。
幸运的是，最大似然参数估计方法可以合二为一。即Learning阶段用Prediction阶段的优化方案。
所以设定$log\,\, p(Data|W_{param})$为Likelihood似然函数，这是频率统计体系的观点。
当然，尽管$maximum \,\, likelihood$可以逆向求解$W_{param}$，但也带来过拟合问题。
因为这样优化，本质是$\max \, P(Data)$，顺带作为条件，求出$W_{param}$。
一旦Data分布有问题，比如数据过少，而导致分布不均匀，那么$W_{param}$也会跟着大幅度偏离，导致过拟合。
——————————————————————————————————————————————
贝叶斯体系与频率统计体系最大的区别在于，贝叶斯通常在$likelihood$之后引入一个$prior$：
$posterior=likelihood \times  prior$
处于隐性状态的Learning阶段概率可以通过一个条件概率分析：
$p(W_{param}|Data)=\frac{p(Data|W_{param}) \times p(W_{param})}{p(Data)}$
这部分任务可以通过条件分解为两部分：
①$\max \, p(Data|W_{param})$，这是Prediction阶段的概率目标，也是判别模型的基础。
这是频率统计学派的一贯坚持的，所见即所得，数据反应一切。如果没有这部分数据，那么结果就判别为0，这是不合常理的。
②$\max \, p(W_{param})$，相对的，这可不是$P(X)$生成模型。
在RRML的1.2.5节的高斯分布曲线拟合中，引入了一个M项(元)的prior，假定高斯分布是0均值，方差倒数是$\alpha$
$prior-vector(w)=(w_{0},w_{1},w_{2},...w_{M})$
则Prior的概率分布：
$P(W_{param}|\alpha)=N(W_{param}|0,\alpha^{-1}I)=(\frac{\alpha}{2\pi})^{(M+1)/2}\cdot exp(-\frac{\alpha}{2}W^{T}W)$
将此Prior应用到高斯判别模型的似然函数上，得到转化后的概率形式：
$p(W_{param}|x,t,\alpha,\beta) \propto p(t|x,w,\beta) \cdot p(W_{param}|\alpha)$
取其负似然对数，有目标函数：
$\frac{\beta}{2}\sum_{n=1}^{N}(y(x_{n},W)-t_{n})^{2}+\frac{\alpha}{2}W^{T}W$
可以看到，第一部分为不加Prior的目标函数，第二部分为Prior的作用结果。
如果你了解L2 Regularier，就会发现，用于修正的Prior变成了一个L2 Regularier的平方，等效L2。
即所谓的贝叶斯线性回归，就是线性回归+L2 Regularier。
概率延伸：Local Represention VS Distributed Represention
Distributed Represention
摘自Bengio教授在2015年暑期组织的蒙特利尔暑期深度学习&NLP学习会：[Link]
整体来说，Distributed Represention就是指数据中的各个维度特征之间相互缠绕，互相依赖关联。
这些在各个维度的之间连接着的隐信息，通过常规label+判别模型是无法挖掘的。
已经发现的存在大量信息相互缠绕的三大领域就是：Image、Speech、Text，这些数据的第二特点就是维度大。
维数灾难
因为源数据的维度很大，所以需要进行对应维度的内积计算。这样参数维度就需要很大。
考虑某个神经网络的隐层有N维的参数(神经元)。每个维度上的参数变化值范围大概是[-1，1]，近似长度算为2。
当N=2时，在一个xy轴平面坐标系内，以1x1方形为一个单位，则共有2x2个区域。
当N=3时，在一个xyz轴空间坐标系内，以1x1x1方体为一个单位，则共有2x2x2个区域。
当N=M时，在一个M维空间坐标系内，以1^M维体为一个单位，则共有2^M个区域。
这使得在拟合这些高维度数据时，参数的可搜索空间非常大，如果数据过少，则容易造成过拟合。
解决方案：PCA or Pre-Training？
PCA是一种聪明的方法，它通过以下两方面对抗上面的问题:
①针对信息缠绕问题，提出线性变换原则。即：
$[Input]=>(Decompose)[Output]*[LinearBase]$。
它假设模型中存在某种Linear关系，通过计算一个线性基，将缠绕的信息以变换的形式协同起来。
②针对维度过高问题，对变换后的数据进行压缩降维。减小搜索空间。
-------------------------------------------------------------------------------------
Deep Learning中的Pre-Training就比较特殊，它是一个参数学习方法。
①针对信息缠绕问题，进行非线性变换。即：
$[Input]=>(Decompose)[Output]*[Non-LinearBase]$
②针对搜索空间过大问题，Pre-Training预先利用非线性变换关系引导了一个搜索方向，
这个搜索方向可以有效避免过拟合。
Local Represention
SVM一个经典的工作就是避免了维度灾难。
它的最优间隔化目标函数，迫使Input Layer和Hidden Layer之间点积运算变成了两个数据点之间的点积，
而不是数据点和参数的点积。
这意味着，尽管数据有10000维，但是参数无须膨胀到10000维。
整个SVM的全部参数就是支持向量的$\alpha$，这等效于隐层神经元，但是本身却不是超参数，可以通过优化自行确定。
但是SVM仅仅是神经网络在Optimization上修改版，其注入的Prior仍然是Smooth，这意味着SVM超越不了神经网络的瓶颈。
因为Prior：Smooth本质就是一个可供Optimization的likelihood，完全就是个根据频率吃数据的方法。
朴素贝叶斯
朴素贝叶斯堪称Local Represention的典范。
朴素贝叶斯理论假设，输入向量各个维度间是独立的。
当然，今天有了Distributed Represention之后，我们知道，各个维度之间不仅不独立，而且还高度缠绕。
当然朴素贝叶斯假设不无它的道理，因为想要计算不独立的概率，简直比登天还难。
这方面的工作后来被2006年的RBM完成，RBM通过Gibbs Sampling，近似让参数训出了不独立的概率。
判别模型与生成模型
模型只能拿到Local Represention，罪魁祸首还是Prior问题。
整个Smooth基本就是依赖label在分割面上进行拟合调优，Label中隐含的信息毕竟是有限的。
CNN是一个例外，尽管它仅仅依赖Label的判别模型，但是它在图像方面却有着逆天的Distributed Represention抽取能力。
当然，这归功于其独有的一些高效转化出Represention的Prior。
而生成模型$P(X)$(指的是RBM)能拿到Distributed Represention，原因是它让参数预先去拟合了数据本身。
部分缠绕的信息被Encode锁在了参数里。当Fine-Tuning的时候，这些缠绕的信息就可以被直接Decode利用。
(Bengio组的AutoEncoder观点)
再看贝叶斯概率体系与Pre-Training
贝叶斯概率体系饱受批评的就是其单纯的从数学计算角度选取Prior，然后：
$\max \, p(W_{param})$
这部分其实有Adaptive Method，就是Deep Learning中的Pre-Training。
与PCA之类无参数方法的生成模型不同，有参数的生成模型最后实际上已经完成了$\max \, p(W_{param})$
Pre-Training的工作恰恰是利用自适应感知的方法，确定一个引导参数的Prior。
这与[Erhan10]提出的设想不谋而合，数学方法的M阶多项式Prior最后变成了Regularizer
而Pre-Training最后也变成了Regularizer，其中应该有一些联系。
数据挖掘 VS 模式识别
这两个概念常常被搞混掉。
模式识别
当前模式识别三大领域：CV、NLP、Speech，其实都在对数据进行挖掘。
挖掘的数据特点：数量庞大(近几年)、信息维度高(1000~65536维)、特征隐蔽。
一句话概括：数据噪声小，也就是垃圾维度信息很少，且各个维度之间局部性联系非常紧密。
常用算法：神经网络、概率统计
最终目的：感知世界，人工智能。
数据挖掘
当然，数据挖掘和模式识别独立开来应该是有原因的。
数据挖掘常见任务包含：欺诈检测、信用评分、癌症检测、金融预测。
挖掘的数据特点：数量庞大、信息维度低（10~20维)、冗余量大。
一句话概括：数据噪声大，垃圾维度信息特别多。如欺诈检测任务中，往往原始数据含有时间、标码之类的垃圾信息。
这些垃圾信息是不可直接带入模型的，需要人工剔除。
常用算法：统计机器学习(以决策树系为主)、概率统计
最终目的：服务社会，简化劳力。
对比
★从针对的任务来看，数据挖掘更适合概率统计、统计机器学习学派。
而模式识别则更适合神经网络学派。
★数据挖掘任务不需要各种Prior来挖掘内在的Distrubuted Represention
而模式识别则需要各种黑科技Prior挖掘数据中的Distrubuted Represention
★数据挖掘通常包含数据仓库之类的额外数据管理技巧。
★这几年，模式识别进入大数据阶段，通常额外包含GPU并行计算设计。
远景
未来的数据科学必然是模式识别和数据挖掘的二分天下。尽管早期大部分研究者都在做数据挖掘任务。
但这几年，随着深度学习在Represention上大强大挖掘能力，以及GPU并行计算的异军突起，
感知人工智能与模式识别，已经开始推动图像、文本、语音这类感知数据的发展。
大数据不再只来自银行、政府、公司的私密统计，而是来自于互联网、用户、你与我对世界的感知，
这是一个更为艰难的挑战。因为感知数据实在庞大，而我们对生物神经机制又了解太少。
机器学习到底要做什么?
当实践性地做了创新项目之后，个人对机器学习的流程任务又有了新的理解。
个人认为机器学习流程大致分为三步：
I、机器学习理论与建模
这是整个机器学习技术进步的核心。
机器学习两大顶级会议，ICML(夏)、NIPS(冬)，相隔半年。
世界上的研究组在这半年中主要任务就是互相撕逼，在模型结构、算法、精度上提出研究方案。
你有了state of art，我下次就秒了你的state of art，然后我的state of art又被你秒了。
II、机器学习实现与平台
建模和实现是两回事。
①典型例子就是SVM，1993年模型提出。但是直到1997年SMO的高效实现出来，才算大范围普及。
另一个例子就是Restricted Boltzmann machines限制玻尔兹曼机，1986年提出。
但是2005年有了CDK——K步对比分歧算法后，才算大范围普及。
这是算法实现上的困扰。
②Elman&Jordan RNN、LSTM分别在1990年、1997年提出。
但是在2006年之后，在大范围普及。因为循环递归网络本质是深度神经网络，受限于当时的计算力，难以实现。
这是机器太落后的困扰。
————————————————————————————————————————————————
实现和平台也是两回事。
CNN正式普及是在1997年以后，但是一直都是应用于小数据。LeCun教授在上个世纪的MNIST用CPU计算，
才6W的数据量，用今天的Core i7都要跑几个小时。更别提大数据了。
2009年ImageNet推出后，在NVIDIA的配合下，Theano、Torch、Caffe等GPU计算框架的推出，
才算是把深度学习、大数据、并行计算推向了民间，推向了普及。
————————————————————————————————————————————————
机器学习作为计算机研究重点的最大原因无外乎就是  高效的实现与平台。
涉及到算法优化、计算优化，大规模计算平台部署，这是一门手艺活。
III、机器学习应用与系统
你可能已经在在MNIST、Cifar10、ImageNet、IMDB等数据集中游刃有余，但你的任务和这些数据没有任何关系。
我接到的第一个正式项目是 人脸情感分析与识别
这不是一个热点研究问题，起码从数据上就有了限制。
首先，它派生与人脸检测，你需要从一般图片中分离人脸。
其次，它需要指定的人脸：中性、高兴、悲伤、愤怒、恐惧、厌恶、惊讶。
收集或采集一张人脸是简单的，但是这张脸还有有指定的表情，这就是为难了。
我们小组尝试通过各大搜索引擎，去寻找Wild数据，尽全力之下，数据仍然不多。
表情
高兴
悲伤
愤怒
惊讶
厌恶
数量
398张
1073张
204张
214张
44张
这数字不是很意外，因为某些表情在互联网出现的概率确实不大。
除了数量严重不足外，最致命的问题就是分类不均衡。这是机器学习理论与实际最容易出现的脱节。
实验数据集往往都是经过多少年的沉淀、修改的，而我们实际能搞到的Wild数据则是千疮百孔，惨不忍睹。
当年，关于不均衡数据的学习问题，也是当前机器学习的研究重点。起码在当下，我们还是得活受罪。
——————————————————————————————————————————————
为了解决数据不足的问题，我的导师推荐参考香港中文大学汤晓鸥教授小组DeepID的做法，
对一份数据进行PATCH修改，一份当成多份使。在之后，我自己捣鼓出了9倍化的PATCH方案：
并通过投票机制，对CNN检测结果加上这个投票的Prior修正，最后得到了5%的精度提升。
——————————————————————————————————————————————
能过仿真出结果后，你还得设计一个系统。
我接手的这个项目的本尊是云机器人，这只是计算机视觉设计部分。
为了配合网络，你得为你的Model设计一个远程系统。
由于Theano工作在Python下，为了效率，我用C++/Qt设计了两个Server，
Python设计的一个Server，在三个Sever的协同工作下，才勉强完成远程计算任务。
——————————————————————————————————————————————
机器学习系统很重要，一味地跑仿真、跑人家做好的数据，只会蒙蔽你的双眼。
毕竟我们得使用机器学习技术来解决实际问题。