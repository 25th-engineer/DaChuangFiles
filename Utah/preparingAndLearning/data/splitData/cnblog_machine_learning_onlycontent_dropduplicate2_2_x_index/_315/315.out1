介绍 目前 对于 全球 科学家 而言 如何 去 学习 一种 
新 技能 成为 了 一个 最 基本 的 研究 问题 
为什么 要 解决 这个 问题 的 初衷 是 显而易见 的 
如果 我们 理解 了 这个 问题 那么 我们 可以 使 
人类 做 一些 我们 以前 可能 没有 想到 的 事 
或者 我们 可以 训练 去做 更多 的 人类 工作 常遭/nr 
一个 真正 的 人工智能 时代 虽然 对于 上述 问题 我们 
目前 还 没有 一个 完整 的 答案 去 解释 但是 
有 一些 事情 是 可以 理解 的 先 不考虑 技能 
的 学习 我们 首先 需要 与 环境 进行 交互 无论 
我们 是 学习 驾驶 汽车 还是 婴儿 学习 走路 学习/v 
都是/nr 基于/p 和/c 环境/n 的/uj 相互/d 交互/v 从/p 互动/d 中/f 
学习/v 是/v 所有/b 智力/n 发展/vn 和/c 学习/v 理论/n 的/uj 基础/n 
概念/n 强化 学习 今天 我们 将 探讨 强化 学习 这 
是 一种 基于 环境 相互 交互 的 学习 算法 有些 
人 认为 强化 学习 是 实现 强 人工智能 的 真正 
希望 这种 说法 也 是 正确 的 因为 强化 学习 
所 拥有 的 潜力 确实 是 巨大 的 目前 有关 
强化 学习 的 研究 正在 快速 增长 人们 为 不同 
的 应用 程序 生成 各种各样 的 学习 算法 因此 熟悉 
强化 学习 的 技术 就 变得 尤其 重要 了 如果 
你 还 不是 很 熟悉 强化 学习 那么 我 建议 
你 可以 去 看看 我 以前 有关 强化 学习 文章 
和 一些 开源 的 强化 学习 平台 一旦 你 已经 
掌握 和 理解 了 强化 学习 的 基础 知识 那么 
请 继续 阅读 这篇文章 读完 本文 之后 你 会对 强化 
学习 有 一个 透彻 的 了解 并且 会 进行 实际 
代码 实现 注 在 代码 实现 部分 我们 假设 你 
已经 有了/nr Python 的 基本 知识 如果 你 还 不 
知道 Python 那么 你 应该 先 看看 这篇 教程 1 
. 确定 一个 强化 学习 问题 强化 学习 是 学习 
如何 去做 如何 根据 与 环境 的 交互 采取 相应 
的 行动 最终 的 结果 就是 使得 系统 的 回报 
信号 数值 最大化 学习者 不会 被 告知 去 执行 哪个 
行动 而是 要 他 自己 去 发现 哪种 行动 将 
产生 最大 的 回报 让 我们 通过 一个 简单 的 
例子 来 解释 一下 我们 将 一个 正在 学习 走路 
的 孩子 作为 一个 例子 以下 是 孩子 在 学习 
走路 时 所要 采取 的 步骤 1 . 孩子 会 
观察 的 第一 件事 就是 注意 你 是 如何 走路 
的 你 使用 两条腿 一次 走 一步 一步 一步 往前走 
孩子 会 抓住 这个 概念 然后 试图 去 模仿 你 
2 . 但 很快 他 / 她 又会 明白 在 
走路 之前 孩子 必须先 站起来 在 学习 走路 的 时候 
这 对于 孩子 来说 是 一个 挑战 因此 孩子 试图 
自己 站 起来 他 / 她 不断 跌倒 但是 任然/nr 
不断地 站起来 3 . 然而 还有 另外 一个 挑战 需要 
应付 站 起来 是 相对 容易 的 但是 要 保持 
站立 状态 就 是 另一个 挑战 了 在 一个 狭小 
的 空气 中 找到 支撑 孩子 设法 保持 站立 4 
. 现在 孩子 的 真正 任务 就是 开始 学习 走路 
了 但是 学习 走路 说 起来 很容易 而 实际 做 
起来 就 不是 那么 容易 了 在 孩子 的 大脑 
中 需要 处理 很多 事情 比如 平衡 身体 决定 哪个 
脚 是 下一次 需要 放下 的 放在 哪里 这 听 
起来 像 是 一个 很 困难 的 任务 对吗 它 
实际上 确实 是 一个 挑战 先要 学习 站立 然后 才能 
学习 行走 但是 现在 我们 不 都 学会 了 走路 
嘛 再也 不会 被 这个 问题 所 困扰 了 现在 
你 可以 明白 为什么 这 对于 孩子 是 多么 困难 
的 原因 了 让 我们 形式化 上面 的 例子 例子 
所要 陈述 的 问题 是 走路 问题 其中 孩子 是 
一个 试图 通过 采取 行动 走路 来 操纵 环境 在 
地上 走路 的 智能体 他 / 她 试图 从 一个 
状态 即 他 / 她 走 的 每一步 转移 到 
另一个 状态 当 他 / 她 完成 任务 的 一个 
子 模块 即 孩子 走 了 几步 时 孩子 会 
获得 奖励 比如 一些 巧克力 但是 当 他 / 她 
不会 走路 时 他 / 她 不会 收到 任何 巧克力 
这 是 一个 负反馈 过程 这 就是 像 话 学习 
问题 的 简单 描述 这 是 一个 有关 强化 学习 
很好 的 介绍 视频 2 . 与 其他 机器学习 方法 
的 比较 强化 学习 属于 更 打雷 的 机器学习 算法 
以下 是 有关 机器学习 算法 类型 的 描述 让 我们 
比较 一下 强化 学习 算法 和 别的 类型 算法 之间 
的 区别 监督 学习 与 强化 学习 在 监督 学习 
中 在 外部 有 一个 监督 主管 它 拥有 所有 
环境 的 知识 并且 与 智能体 一起 共享 这个 知识 
从而 帮助 智能体 完成 任务 但是 这样 存在 一些 问题 
因为 在 一个 任务 中 其中 存在 如此 多 的 
子 任务 之间 的 组合 智能体 应该 执行 并且 实现目标 
所以 创建 一个 监督 主管 几乎 是 不切实际 的 例如 
在 象棋 游戏 中 存在 数万 个 可以 移动 的 
玩法 因此 去 创建 一个 可以 获胜 的 玩法 知识库 
是 一个 单调 乏味 的 任务 在 这些 问题 中 
从 自己 的 经验 中 学习 并且 获得 知识 是 
更加 合理 可行 的 这 就是 强化 学习 和 监督 
学习 的 主要 区别 在 监督 学习 和 强化 学习 
中 在/p 输入/v 和/c 输出/v 之间/f 都/d 存在/v 映射/v 但是 
在 强化 学习 中 存在 的 是 对 智能体 的 
奖励 反馈 函数 而 不是 像 监督 学习 直接 告诉 
智能体 最终 的 答案 无 监督 学习 与 强化 学习 
在 强化 学习 中 有/v 一个/m 从/p 输入/v 到/v 输出/v 
的/uj 映射过程/i 但是 这个 过程 在 无 监督 学习 中 
是 不 存在 的 在 无 监督 学习 中 主要 
任务 是 找到 一个 最 基础 的 模式 而 不是 
一种 映射 关系 例如 如果 任务 是 向 用户 推荐 
新闻 文章 则无 监督 学习 算法 是 先 查看 该 
人 以前 读过 的 类似 文章 并把 它们 推荐 给 
其他人 而 强化 学习 算法 则是 通过 用户 的 一些 
文章 并且 获得 用户 的 不断 反馈 从而 构建 一个 
知识图谱 从而 得知 用户 与 文章 之间 的 喜爱 关系 
还有 第四 种 类型 的 机器学习 成为 半 监督 学习 
其 本质 上 是 监督 学习 和无/nr 监督 学习 的 
组合 它 不同 于 强化 学习 类似/v 于/p 监督/vn 学习/v 
和半/nr 监督/vn 学习/v 具有/v 直接/ad 的/uj 参照/v 答案/n 而 强化 
学习 不 具有 3 . 解决 强化 学习 问题 的 
框架 为 了 理解 如何 去 解决 一个 强化 学习 
问题 让 我们 通过 一个 经典 的 例子 来 说明 
一下 强化 学习 问题 多 臂 赌博机 首先 我们 需要 
了解 探索 与 开发 的 基本 问题 然后 去 定义 
解决 强化 学习 问题 的 框架 Tiger Machine 如上图 假设 
你 已经 在 Tiger Machine 上面 玩 了 很多 次 
了 现在 你 想做 的 是从 Tiger Machine 上面 获得 
最大 的 回报 并且 尽可能 的 快 你 会 怎么做 
呢 一个 比较 天真 的 想法 是 只 选择 一个 
Tiger Machine 然后 一 整天 都在 玩 它 这 听 
起来 非常 无聊 但 Tiger Machine 可能会 给 你 一些 
报酬 即 让 你 赢钱 使用 这种 方法 你 可能 
中奖 的 概率 大约 是 0.00000 . . . . 
. 1 也 就是说 大多数 时间 你 可能 知识 坐在 
Tiger Machine 面前 亏钱 正式 说明 一下 这 可以 被 
定义 为 一种 纯粹 的 开发方法 但是 这 是 最佳 
选择 吗 答案 当然 是 否定 的 让 我们 看看 
另外 一种 方法 我们 可以 拉 每个 Tiger Machine 的 
拉杆 并且 向 上帝 祈祷 让 我们 至少 打 中 
一个 当然 这是 另一种 天真 的 想法 你 只会 一天 
都在 拉动 拉杆 但 只是 给 你 一点点 报酬 正式 
说明 一下 这种 方法 只 是 一种 纯粹 的 探索 
方法 这 两种 方法 都 不是 最优 的 我们 必须 
在 它们 之间 找到 适当 的 平衡点 已 获得 最大 
的 回报 这 被 称为 强化 学习 的 探索 和 
开发 困境 首先 我们 正式 的 定义 解决 强化 学习 
问题 的 框架 然后 列出 可能 的 方法 来 解决 
这个 问题 马尔科夫 决策 过程 在 强化 学习 场景 中 
我们 定义 问题 的 数学 框架 被 称之为 马尔科夫 决策 
过程 这 可以 被 设计 为 状态 集合 动作 集合 
A 奖励 函数 R 策略 π 价值 V 我们 必须 
采取 一定 的 行动 A 让 我们 从 开始 状态 
移动 到 结束 状态 S 每当 我们 采取 一个 行动 
之后 我们 都会 得到 一定 的 回报 作为 奖励 当然 
所 获得 的 奖励 的 性质 正面 奖励 还 是 
负面 奖励 是由 我们 的 行动 决定 的 我们 的 
策略 集合 π 是由 我们 的 动作 集合 来 确定 
的 而 我们 得到 的 回报 确定 了 我们 的 
价值 V 在 这里 我们 的 任务 就是 通过 选择 
正确 的 策略 来 最大化 我们 的 价值 所以 我们 
必须 最大化 下面 的 方程 对于 时间 t 所有 可能 
的 旅行 推销员 问题 让 我们 通过 另外 一个 例子 
来 说明 一下 这个 问题 是 一系列 旅行 商 TSP 
问题 的 代表 任务 是 以 尽可能 低 的 成本 
完成 从 地点 A 到 地点 F 两个 字母 之间 
的 每条 边上 的 数字 表示 两地 之间 的 距离 
花费 如果 这个 值 是 负数 那么 表示 经过 这条路 
你 会 得到 一定 的 报酬 我们 定义 价值 是 
当 你 用 选择 的 策略 走完 整个 路程 时 
所 获得 的 总价值 这里 说明 一下 符号 状态 节 
点集合 { A B C D E F } 动作 
集合 是从 一个 地点 到 另一个 地点 { A B 
C D etc } 奖励 函数 是 边上 的 值 
策略 函数 指 的 是 完整 的 路径规划 比如 { 
A C F } 现在 假设 你 在 地点 A 
唯一/b 你/r 能/v 看见/v 的/uj 路/n 就是/d 你/r 下/f 一个/m 
目的地/n 也 就是说 你 只能 看见 B D C E 
而别 的 地点 你 是 不 知道 的 你 可以 
采取 贪心 算法 去 获取 当前 状态下 最 有利 的 
步骤 也 就是说 你 从{A/nr B C D E } 
中 选择 采取 { A D } 这种方法 同样 现在 
你 所在 的 地点 是 D 想要 到达 地点 F 
你 可以 从{D/nr B C F } 中 采取 { 
D F } 这个 方法 可以 让 你 得到 最大 
的 报酬 因此 我们 采取 这 一条路 至此 我们 的 
策略 就是 采取 { A D F } 我们 获得 
的 回报 是 120 恭喜 你 刚刚 就 实现 了 
强化 学习 算法 这种 算法 被 称之为 epsilon 贪婪 算法 
这 是 一种 逐步 测试 从而 解决 问题 的 贪婪 
算法 现在 如果 见 你 推销员 想 再次 从 地点 
A 到 地点 F 你 总是 会 选择 这 一条路 
了 其他 旅游 方式 你 能 猜出 我们 的 策略 
是 属于 哪个 类别 纯粹 的 探索 还是 纯粹 的 
开发 吗 请注意 我们 采取 的 策略 并 不是 一个 
最佳 策略 我们 必须 探索 一点 然后 去 寻找 最佳 
的 策略 在 这里 我们 采取 的 方法 是 局域 
策略 的 而 学习 我们 的 任务 是 在 所有 
可能 的 策略 中找到 最佳 的 策略 有/v 很多/m 的/uj 
方法/n 都/d 可以/c 解决/v 这个/r 问题/n 在 这里 我们 简要 
的 列出 一些 主要 类别 策略 优先 我们 的 重点 
是 找到 最佳 的 策略 回报 优先 我们 的 重点 
是 找到 最佳 的 回报 价值 即 累计 奖励 行动 
优先 我们 的 重点 是 在 每个 步骤 上 采取 
最佳 行动 在 以后 的 文章 中 我会 深入 讨论 
强化 学习 算法 到那时 你 可以 参考 这 篇 关于 
强化 学习 算法 调研 的 论文 4 . 强化 学习 
的 实现 接下来 我们 将 使用 深度 Q 学习 算法 
Q 学习 是 一种 基于 策略 的 学习 算法 它 
具有 和 神经 网络 近似 的 函数 表示 这个 算法 
被 Google 使用 并且 打败 了 Atari 游戏 让 我们 
看看 Q 学习 的 伪代码 1 . 初始化 价 值表 
Q s a . 2 . 观察 当前 的 状态值 
s . 3 . 基于 动作 选择 一个 策略 例如 
epsilon 贪婪 作为 该 状态 选择 的 动作 . 4 
. 根据 这个 动作 观察 回报 价值 r 和下/nr 一个 
新 的 状态 s . 5 . 使用 观察到 的 
奖励 和 可能 的 下一个 状态 所 获得 的 最大 
奖励 来 更新 状态 的 值 根据上述 公式 和 参数 
进行 更新 6 . 将 状态 设置 为 新的 状态 
并且 重复 上述 过程 直到 达到 最终 状态 Q 学习 
的 简单 描述 可以 总结 如下 我们 首先 来 了解 
一下 Cartpole 问题 然后 继续 编写 我们 的 解决方案 当 
我 还是 一个 孩子 的 时候 我 记得 我会 选择 
一根 木棍 并 试图 用 一只 手指 去使 它 保持平衡 
我/r 和我的/nr 朋友/n 过去/t 有/v 这样/r 一个/m 比赛/vn 看谁 能让 
木棍 保持 平衡 的 时间 更多 谁 就能 得到 一块 
巧克力 作为 奖励 这里 有 一个 简单 的 视频 来 
描述 一个 真正 的 Cart Pole 系统 让 我们 开始 
编写 代码 吧 在 开始 编写 之前 我们 需要 先 
安装 几个 软件 步骤 一 安装 keras rl 包在 终端 
中 你 可以 运行 以 下命令 git clone https / 
/ github . com / matthiasplappert / keras rl . 
git cd keras rl python setup . py install 步骤 
二 安装 CartPole 环境 的 依赖 我们 假设 你 已经 
安装 好了 pip 那么 你 只 需要 使用 以 下 
命令 进行 安装 pip install h5py pip install gym 步骤 
三 开始 编写 代码 首先 我们 需要 导入 一些 我们 
需要 的 模块 import numpy as np import gym from 
keras . models import Sequential from keras . layers import 
Dense Activation Flatten from keras . optimizers import Adam from 
rl . agents . dqn import DQNAgent from rl . 
policy import EpsGreedyQPolicy from rl . memory import SequentialMemory 然后 
设置 相关 变量 ENV _ NAME = CartPole v0 # 
Get the environment and extract the number of actions available 
in the Cartpole problem env = gym . make ENV 
_ NAME np . random . seed 123 env . 
seed 123 nb _ actions = env . action _ 
space . n 之后 我们 来 构建 一个 非常 简单 
的 单层 神经网络 模型 model = Sequential model . add 
Flatten input _ shape = 1 + env . observation 
_ space . shape model . add Dense 16 model 
. add Activation relu model . add Dense nb _ 
actions model . add Activation linear print model . summary 
接下来 我们 配置 和 编译 我们 的 智能体 我们 将 
策略 设置 为 Epsilon 贪婪 我们 还 将 我们 的 
存储空间 设置 为 序列 存储 因为 我们 要 需要 存储 
我们 执行 操作 的 结果 和 每一个 操作 所 获得 
的 奖励 policy = EpsGreedyQPolicy memory = SequentialMemory limit = 
50000 window _ length = 1 dqn = DQNAgent model 
= model nb _ actions = nb _ actions memory 
= memory nb _ steps _ warmup = 10 target 
_ model _ update = 1e 2 policy = policy 
dqn . compile Adam lr = 1e 3 metrics = 
mae # Okay now it s time to learn something 
We visualize the training here for show but this slows 
down training quite a lot . dqn . fit env 
nb _ steps = 5000 visualize = True verbose = 
2 现在 让 我们 来 测试 一下 我们 的 强化 
学习 模型 dqn . test env nb _ episodes = 
5 visualize = True 下图 是 模型 的 输出 结果 
瞧 你 刚刚 就 建立 了 一个 强化 学习 机器人 
5 . 增加 复杂性 现在 你 已经 看到 了 强化 
学习 的 一个 基本 实现 让 我们 开始 学习 更多 
的 问题 吧 每次 增加 一点点 复杂性 汉诺塔 问题 对于 
那些 不 知道 比赛 的 人 来说 汉诺塔 问题 是 
在 1883年 发明 的 它 是由 3根 木棍 和 一系列 
大小 不一 的 圆盘 组成 的 比如 上图 中的 3个 
从最/nr 左侧 的 木棍 开始 目的 是 以 最少 的 
移动 次数 把 最 左边 的 圆盘 移动 到 最 
右边 的 圆 盘上 如果 我们 要 处理 这个 问题 
那么 我们 先从 处理 状态 开始 初始状态 三个 圆盘 都 
在最 左边 的 木棍 上 从上到下 依次 编号 为 1 
2 3 结束 状态 三个 圆盘 都在 最 右边 的 
木棍 上 从上到下 依次 编号 为 1 2 3 所有 
可能 的 状态 这里 是 我们 可能 得到 的 27种 
状态 其中 12 3 * 表示 圆盘 1 和 圆盘 
2 在最 左边 的 木棍 上面 从上往下 编号 圆盘 3 
在中间 那个 木棍 上面 最 右边 的 木棍 没有 圆盘 
数值 奖励 由于 我们 想 要以 最少 的 移动 步数 
解决 这个 问题 所以 我们 可以 给 每个 移动 赋予 
1 的 奖励 策略 现在 如果 我们 不 考虑 任何 
的 技术 细节 那么 前 一个 状态 可能会 存在 几种 
下一个 状态 比如 当 数值 奖励 为 1时 状态 123 
* * 会 转移 到 状态 23 1 或者 状态 
23 1 如果 你 现在 看到 了 一个 并发 进行 
的 状态 那么/r 上面/f 提到/v 的/uj 这/r 27个/mq 状态/n 的/uj 
每一个/i 都/d 可以/c 表示/v 成/n 一个/m 类似于/i 旅行/vn 商/n 问题/n 
的/uj 图/n 我们 可以 通过 通过 实验 各种 状态 和 
路径 来 找到 最优 的 解决方案 3 x 3 魔方 
问题 虽然 我 可以 为 你 解决 这个 问题 但是 
我 想 让 你 自己 去 解决 这个 问题 你 
可以 按照 我 上述 同样 的 思路 你 应该 就 
可以 解决 了 从/p 定义/n 开始/v 状态/n 和/c 结束/v 状态/n 
开始/v 接下来 定义 所有 可能 的 状态 及其 转换 以及 
奖励 和 策略 最后 你 应该 就 可以 使用 相同 
的 方法 来 构建 自己 的 解决 方案 了 6 
. 深入 了解 强化 学习 的 最新 进展 正如 你 
所 认识 到 的 一个 魔方 的 复杂 性比 汉诺塔 
问题 要 高 很多 倍 现在 让 我们 来 想象 
一下 棋类 游戏 中 的 状态 和 选择 的 策略 
数量 吧 比如 围棋 最近 Google DeepMind 公司 创建 了 
一个 深度 强化 学习 算法 并且 打败 了 李世石 最近 
随着 在 深度 学习 方面 的 成功 现在 的 重点 
是 在 慢慢 转 向应 用 深度 学习 来 解决 
强化 学习 问题 最近 洪水 一般 的 消息 就是 由 
Google DeepMind 创建 的 深度 强化 学习 算法 打败 了 
李世石 在 视频 游戏 中 也 出现 了 类似 的 
情况 开发 的 深度 强化 学习 算法 实现 了 人类 
的 准确性 并且 在 某些 游戏 上 超越 了 人类 
研究 和 实践 仍然 需要 一同 前进 工业界 和 学术界 
共同 策划 努力 以 实现 建立 更好 地 自适应 学习 
机器人 以下 是 几个 已经 应用 强化 学习 的 主要 
领域 博弈论 和 多个 智能体 交互 机器人 计算机网络 车载 导航 
医学 工业 物流 还有 这么 多 的 领域 没有 被 
开发 结合 目前 的 深度 学习 应用于 强化 学习 额 
热潮 我 相信 以后 肯定 会 有 突破 这里 是 
最近 的 消息 之一 7 . 其他 资源 我 希望 
你 现在 能够 深入 了解 强化 学习 的 工作 原理 
这里 还是 一些 额外 的 资源 以 帮助 你 学习 
更多 有关 强化 学习 的 内容 后期 文章 我 将会 
带 大家 进入 强化 学习 的 学习 旅程 中 敬请期待 
~ ~ ~ 有关 强化 学习 的 视频 有关 强化 
学习 的 书籍 GitHub 上 有关 强化 学习 的 资料库 
David Silver 的 强化 学习 课程 