机器学习 岗位 的 面试 中 通常会 对 一些 常见 的 
机器学习 算法 和 思想 进行 提问 在 平时 的 学习 
过程 中 可能 对 算法 的 理论 注意 点 区别 
会 有 一定 的 认识 但是 这些 知识 可能 不 
系统 在 回答 的 时候 未必 能在/nr 短时间 内 答出 
自己 的 认识 因此 将 机器学习 中 常见 的 原理 
性 问题 记录下来 保持 对 各个 机器学习 算法 原理 和 
特点 的 熟练度               
    无/v 监督/vn 和有/nr 监督/vn 算法/n 的/uj 区别/n 有 
监督 学习 对 具有 概念 标记 分类 的 训练 样本 
进行 学习 以 尽可能 对 训练样本 集外 的 数据 进行 
标记 分类 预测 这里 所有 的 标记 分类 是 已知 
的 因此 训练样本 的 岐义 性 低 无 监督 学习 
对 没有 概念 标记 分类 的 训练 样本 进行 学习 
以 发现 训练样本 集中 的 结构 性 知识 这里 所有 
的 标记 分类 是 未知 的 因此 训练样本 的 岐义 
性 高 聚 类 就是 典型 的 无 监督 学习 
                  SVM 
的 推导 特性 多 分类 怎么 处理 SVM 是 最大 
间隔 分类器 几何 间隔 和 样本 的 误 分 次数 
之间 存在 关系 其中 从 线性 可分 情况下 原 问题 
特征 转换 后的/nr dual 问题 引入 kernel 线性 kernel 多项式 
高斯 最后 是 soft margin 线性 简单 速度快 但是 需要 
线性 可分 多项式 比 线性 核 拟合 程度 更强 知道 
具体 的 维度 但是 高次 容易 出现 数值 不 稳定 
参数 选择 比较 多 高斯 拟合 能力 最强 但是 要 
注意 过拟合 问题 不过 只有 一个 参数 需要 调整 多分 
类 问题 一般 将 二 分类 推广 到 多 分类 
的 方式 有三种 一对一 一对多 多对 多 一对一 将 N 
个 类别 两两 配对 产生 N N 1 / 2 
个 二 分类 任务 测试阶段 新 样本 同时 交给 所有 
的 分类器 最终 结果 通过 投票 产生 一对多 每一次 将 
一个 例 作为 正 例 其他 的 作为 反例 训练 
N 个 分类器 测试 时 如果 只有 一个 分类器 预测 
为 正 类 则 对应 类别 为 最终 结果 如果 
有 多个 则 一般 选择 置信度 最大 的 从 分类器 
角度 一对一 更多 但是 每一次 都 只用了 2个 类别 因此当 
类别 数很多 的 时候 一对一 开销 通常 更小 只要 训练 
复杂度 高于 O N 即可 得到 此 结果 多对 多 
若干 各类 作为 正 类 若干个 类 作为 反 类 
注意 正反 类 必须 特殊 的 设计       
            LR 的 推导 特性 
LR 的 优点 在于 实现 简单 并且 计算 量 非常 
小 速度 很快 存储资源 低 缺点 就是 因为 模型 简单 
对于 复杂 的 情况下 会 出现 欠 拟合 并且 只 
能 处理 2分 类 问题 可以 通过 一般 的 二元 
转换 为 多元 或者 用 softmax 回归       
            决策树 的 特性 决策树 
基于 树结构 进行 决策 与 人类 在 面临 问题 的 
时候 处理 机制 十分 类似 其 特点 在于 需要 选择 
一个 属性 进行 分支 在 分支 的 过程 中 选择 
信息 增益 最大 的 属性 定义 如下 在 划分 中 
我们 希望 决策树 的 分支 节点 所 包含 的 样本 
属于 同一 类别 即 节点 的 纯度 越来越高 决策树 计算 
量 简单 可解释 性强 比较 适合 处理 有 缺失 属性值 
的 样本 能够 处理 不 相关 的 特征 但是 容易 
过拟合 需要 使用 剪枝 或者 随机 森林 信息 增益 是 
熵 减去 条件 熵 代表 信息 不确定性 较少 的 程度 
信息 增益 越大 说明 不确定性 降低 的 越大 因此 说明 
该 特征 对 分类 来说 很 重要 由于 信息 增益 
准则 会对 数目 较多 的 属性 有所 偏好 因此 一般 
用 信息 增益 率 c 4.5 其中 分母 可以 看 
作为 属性 自身 的 熵 取值 可能性 越多 属性 的 
熵 越大 Cart 决策树 使用 基尼指数 来 选择 划分 属性 
直观 的 来说 Gini D 反映 了 从 数据集 D 
中 随机 抽取 两个 样本 其 类别 标记 不 一致 
的 概率 因此 基尼指数 越小 数据集 D 的 纯度 越高 
一般 为了 防止 过拟合 要 进行 剪枝 有预/nr 剪枝/n 和后/nr 
剪枝/n 一般用 cross validation 集 进行 剪枝 连续 值 和 
缺失 值 的 处理 对于 连续 属性 a 将 a 
在 D 上 出现 的 不同 的 取值 进行 排序 
基于 划 分点 t 将 D 分为 两个 子集 一般 
对 每 一个 连续 的 两个 取值 的 中点 作为 
划 分点 然后 根据 信息 增益 选择 最大 的 与 
离散 属性 不同 若 当前 节点 划分 属性 为 连续 
属性 该 属性 还 可以 作为 其 后代 的 划分 
属性                   
SVM LR 决策树 的 对比 SVM 既 可以 用于 分类 
问题 也 可以 用于 回归 问题 并且 可以 通过 核 
函数 快速 的 计算 LR 实现 简单 训练 速度 非常 
快 但是 模型 较为简单 决策树 容易 过拟合 需要 进行 剪枝 
等 从 优化 函数 上看 soft margin 的 SVM 用 
的 是 hinge loss 而带 L2 正则化 的 LR 对应 
的 是 cross entropy loss 另外 adaboost 对应 的 是 
exponential loss 所以 LR 对 远点 敏感 但是 SVM 对 
outlier 不太 敏感 因为 只 关心 support vector SVM 可以 
将 特征 映 射到 无穷 维空间 但是 LR 不可以 一般 
小 数据 中 SVM 比 LR 更优 一点 但是 LR 
可以 预测 概率 而 SVM 不可以 SVM 依赖于 数据 测度 
需要 先 做 归一化 LR 一般 不 需要 对于 大量 
的 数据 LR 使用 更加 广泛 LR 向多/nr 分类 的 
扩展 更加 直接 对于 类别 不平衡 SVM 一般用 权重 解决 
即 目标函数 中 对 正负 样本 代价 函数 不同 LR 
可以 用 一般 的 方法 也 可以 直接 对 最后 
结果 调整 通过 阈值 一般 小 数据 下 样本 维度 
比 较高 的 时候 SVM 效果 要 更优 一些   
                GBDT 和 
随机 森林 的 区别 随机 森林 采用 的 是 bagging 
的 思想 bagging 又 称为 bootstrap aggreagation 通过 在 训练样本 
集中 进行 有 放回 的 采样 得到 多个 采样 集 
基于 每个 采样 集训 练出 一个 基 学习 器 再将 
基 学习 器 结合 随机 森林 在对 决策树 进行 bagging 
的 基础 上 在 决策树 的 训练 过程 中 引入 
了 随机 属 性选择 传统 决策树 在 选择 划分 属性 
的 时候 是 在 当前 节点 属性 集合 中 选择 
最优 属性 而 随机 森林 则是 对 结点 先 随机 
选择 包含 k 个 属性 的 子集 再 选择 最有 
属性 k 作为 一个 参数 控制 了 随机性 的 引入 
程度 另外 GBDT 训练 是 基于 Boosting 思想 每一 迭代 
中 根据 错误 更新 样本 权重 因此 是 串行 生成 
的 序列化 方法 而 随机 森林 是 bagging 的 思想 
因此 是 并行 化 方法           
        如何 判断 函数 凸 或非 凸 
什么 是 凸 优化 首先 定义 凸 集 如果 x 
y 属于 某个 集合 C 并且 所 有的 也 属于 
c 那么 c 为 一个 凸 集 进一步 如果 一个 
函数 其 定义域 是 凸 集 并 且则 该 函数 
为 凸函数 上述 条件 还 能 推出 更 一般 的 
结果 如果 函数 有 二阶 导数 那么 如果 函数 二阶 
导数 为 正 或者 对于 多元 函数 Hessian 矩阵 半 
正定 则为 凸函数 也 可能 引到 SVM 或者 凸函数 局部 
最优 也是 全局 最优 的 证明 或者 上述 公式 期望 
情况下 的 Jessen 不等式             
      如何 解决 类别 不 平衡 问题 有些 
情况 下 训练 集中 的 样本分布 很 不平衡 例如 在 
肿瘤 检测 等 问题 中 正 样本 的 个数 往往 
非常 的 少 从 线性 分类器 的 角度 在用 对 
新 样本 进行 分类 的 时候 事实上 在用 预测出 的 
y 值 和 一个 y 值 进行 比较 例如 常常 
在 y 0.5 的 时候 判为 正 例 否则 判为 
反例 几率 反映 了 正 例 可能性 和 反例 可能性 
的 比值 阈值 0.5 恰好 表明 分类器 认为 正反 的 
可能性 相同 在 样本 不 均衡 的 情况 下 应该 
是 分类器 的 预测 几率 高于 观测 几率 就 判断 
为 正 例 因此 应该 是 时 预测 为 正 
例 这种 策略 称为 rebalancing 但是 训练 集并 不一定 是 
真实 样本 总体 的 无偏 采样 通常 有三种 做法 一种 
是 对 训练 集 的 负 样本 进行 欠 采样 
第二种 是 对正 例 进行 升 采样 第三种 是 直接 
基于 原始 训练 集 进行 学习 在 预测 的 时候 
再 改变 阈值 称为 阈值 移动 注意 过 采样 一般 
通过 对 训练 集 的 正 例 进行 插值 产生 
额外 的 正 例 而 欠 采样 将 反例 划分 
为 不同 的 集合 供 不同 的 学习 器 使用 
                  解释 
对偶 的 概念 一个 优化 问题 可以 从 两个 角度 
进行 考察 一个 是 primal 问题 一个 是 dual 问题 
就是 对偶 问题 一般 情况 下 对偶 问题 给出 主 
问题 最优 值 的 下界 在 强 对偶性 成立 的 
情况 下 由 对偶 问题 可以 得到 主 问题 的 
最优 下界 对偶 问题 是 凸 优化 问题 可以 进行 
较好 的 求解 SVM 中 就是 将 primal 问题 转换 
为 dual 问题 进行 求解 从而 进一步 引入 核 函数 
的 思想                 
  如何 进行 特征选择 特征选择 是 一个 重要 的 数据 
预 处理过程 主要 有 两个 原因 首先 在 现实 任务 
中 我们 会 遇到 维数 灾难 的 问题 样本 密度 
非常 稀疏 若能 从中 选择 一部分 特征 那么 这个 问题 
能 大大 缓解 另外 就是 去除 不相关 特征 会 降低 
学习 任务 的 难度 增加 模型 的 泛化 能力 冗余 
特征 指 该 特征 包含 的 信息 可以 从 其他 
特征 中 推演出来 但是 这 并不 代表 该 冗余 特征 
一定 没有 作用 例如 在 欠 拟合 的 情况下 也 
可以 用过 加入 冗余 特征 增加 简单 模型 的 复杂度 
在 理论 上 如果 没有 任何 领域 知识 作为 先验 
假设 那么 只能 遍历 所有 可能 的 子集 但是 这 
显然 是 不 可能 的 因为 需要 遍历 的 数量 
是 组合 爆炸 的 一般 我们 分为 子集 搜索 和 
子集 评价 两 个 过程 子集 搜索 一般 采用 贪心 
算法 每 一轮 从 候选 特征 中 添加 或者 删除 
分别/d 成为/v 前/f 向/p 和后先/nr 搜索/v 或者 两者 结合 的 
双向 搜索 子集 评价 一般 采用 信息 增益 对于 连续 
数据 往往 排序 之后 选择 中 点 作为 分割 点 
常见 的 特征 选择 方式 有 过滤 式 包裹 式 
和 嵌入式 filter wrapper 和 embedding Filter 类型 先 对 
数据 集 进行 特征选择 再 训练 学习 器 Wrapper 直接 
把 最终 学习 器 的 性能 作 为特征 子集 的 
评价 准则 一般 通过 不断 候选 子集 然后 利用 cross 
validation 过程 更新 候选 特征 通常 计算 量 比较 大 
嵌入式/nz 特征选择/nr 将/d 特征选择/nr 过程/n 和/c 训练/vn 过程/n 融/vn 为了/p 
一体/n 在 训练 过程 中 自动 进行 了 特征选择 例如 
L1 正则化 更 易于 获得 稀 疏解 而 L2 正则化 
更 不容易 过拟合 L1 正则化 可以 通过 PGD 近端 梯度 
下降 进行 求解               
    为什么 会 产生 过拟合 有 哪些 方法 可以 
预防 或 克服 过拟合 一般 在 机器 学习 中 将 
学习 器 在 训练 集上 的 误差 称为 训练 误差 
或者 经验 误差 在 新 样 本上 的 误差 称为 
泛化 误差 显然 我们 希望 得到 泛化 误差 小 的 
学习 器 但是 我们 事先 并不 知道 新 样本 因此 
实际上 往往 努力 使 经验 误差 最小化 然而 当 学习 
器 将 训练样本 学 的 太好 的 时候 往往 可能 
把 训练样本 自身 的 特点 当做 了 潜在 样本 具有 
的 一般 性质 这样 就 会 导致 泛化 性能 下降 
称之为 过拟合 相反 欠 拟合 一般指 对 训练 样本 的 
一般 性质 尚未 学习 好 在 训练 集上 仍然 有 
较大 的 误差 欠 拟合 一般来说 欠 拟合 更容易 解决 
一些 例如 增加 模型 的 复杂度 增加 决策树 中的 分支 
增加 神经 网络 中 的 训练 次数 等等 过拟合 一般 
认为 过拟合 是 无法 彻底 避免 的 因为 机器学习 面临 
的 问题 一般 是 np hard 但是 一个 有效 的 
解 一定 要 在 多项式 内 可以 工作 所以 会 
牺牲 一些 泛化 能力 过拟合 的 解决 方案 一般 有 
增加 样本 数量 对 样本 进行 降 维 降低 模型 
复杂度 利用 先验 知识 L1 L2 正则化 利用 cross validation 
early stopping 等等               
    什么 是 偏差 与 方差 泛化 误差 可以 
分解 成 偏差 的 平方 加上 方差 加上 噪声 偏差 
度量 了 学习 算法 的 期望 预测 和 真实 结果 
的 偏离 程度 刻画 了 学习 算法 本身 的 拟合 
能力 方差 度量 了 同样 大小 的 训练 集 的 
变动 所 导致 的 学习 性能 的 变化 刻画 了 
数据 扰动 所 造成 的 影响 噪声 表达 了 当前 
任务 上 任何 学习 算法 所能 达到 的 期望 泛化 
误差 下界 刻画 了 问题 本身 的 难度 偏差/n 和/c 
方差/n 一般/a 称为/v bias/w 和/c variance/w 一般 训练 程度 越强 
偏差 越小 方差 越大 泛化 误差 一般 在 中间 有一个 
最小值 如果 偏差 较大 方差 较小 此时 一般 称为 欠 
拟合 而 偏差 较小 方差 较大 称为 过拟合 偏差 方差 
                  神经 
网络 的 原理 如何 进行 训练 神经 网络 自 发展 
以来 已经 是 一个 非常 庞大 的 学科 一般而言 认为 
神经网络 是由 单个 的 神经 元和 不同 神经元 之间 的 
连接 构成 不够 的 结构 构成 不同 的 神经 网络 
最 常见 的 神经 网络 一般 称为 多层 前馈 神经网络 
除了 输入 和 输出 层 中间 隐藏 层 的 个数 
被称为 神经 网络 的 层数 BP 算法 是 训练 神经 
网络 中 最 著名 的 算法 其 本质 是 梯度 
下降 和 链式法则               
    介绍 卷积 神经网络 和 DBN 有 什么 区别 
卷积 神经 网络 的 特点 是 卷积 核 CNN 中 
使用 了 权 共享 通过 不断 的 上 采用 和 
卷积 得到 不同 的 特征 表示 采样 层 又 称为 
pooling 层 基于 局部 相关性 原理 进行 亚 采样 在 
减少 数据量 的 同时 保持 有用 的 信息 DBN 是 
深度 信念 网络 每 一层 是 一个 RBM 整个 网络 
可以 视为 RBM 堆叠 得到 通常 使用 无 监督 逐层 
训练 从 第一 层 开始 每 一层 利用 上 一层 
的 输入 进行 训练 等 各层 训练 结束 之后 再 
利用 BP 算法 对 整个 网络 进行 训练     
              采用 EM 算法 
求解 的 模型 有 哪些 为什么 不用 牛顿 法或/nr 梯度 
下 降法 用 EM 算法 求解 的 模型 一般 有 
GMM 或者 协同 过滤 k means 其实 也 属于 EM 
EM 算法 一定 会 收敛 但是 可能 收敛 到 局部 
最优 由于 求和 的 项数 将 随着 隐 变量 的 
数目 指数 上升 会给 梯度 计算 带来 麻烦     
              用 EM 算法 
推导 解释 Kmeans k means 算法 是 高斯 混合 聚 
类 在 混合 成分 方差 相等 且 每个 样本 仅指 
派 一个 混合 成分 时候 的 特例 注意 k means 
在 运行 之前 需要 进行 归一化 处理 不然 可能会 因为 
样本 在 某些 维度 上过 大 导致 距离 计算 失效 
k means 中 每个 样本 所属 的 类 就 可以 
看成 是 一个 隐 变量 在 E 步中/nr 我们 固定 
每个 类 的 中心 通过 对 每一个 样本 选择 最近 
的 类 优化 目标函数 在 M 步 重新 更新 每个 
类 的 中心点 该 步骤 可以 通过 对 目标函数 求导 
实现 最终 可得 新的 类 中心 就是 类 中 样本 
的 均值                 
  用过 哪些 聚 类 算法 解释 密度 聚 类 
算法 k means 算法 聚 类 性能 的 度量 一般 
分为 两类 一类 是 聚 类 结果 与 某个 参考模型 
比较 外部 指标 另外 是 直接 考察 聚 类 结果 
内部 指标 后者 通常 有 DB 指数和 DI DB 指数 
是 对 每个 类 找出 类 内 平均 距离 / 
类 间 中心 距离 最大 的 类 然后 计算 上述 
值 并对 所有 的 类 求和 越小 越好 类似 k 
means 的 算法 仅在 类 中 数据 构成 簇 的 
情况 下 表现 较好 密度 聚 类 算法 从 样本 
密度 的 角度 考察 样本 之间 的 可 连接 性 
并 基于 可连接 样本 不断 扩展 聚 类 蔟 得到 
最终 结果 DBSCAN density based spatial clustering of applications with 
noise 是 一种 著名 的 密度 聚 类 算法 基于 
一组 邻域 参数 进行 刻画 包括 邻域 核心 对象 邻域 
内 至少 包含 个 对象 密度 直达 j 由 i 
密度 直达 表示 j 在 i 的 邻 域内 且 
i 是 一个 核心 对象 密度 可达 j 由 i 
密度 可达 存在 样本 序列 使得 每 一对 都 密度 
直达 密度 相连 xi xj 存在 k i j 均有 
k 可达 先 找出 样本 中 所有 的 核心 对象 
然后 以 任一 核心 对象 作为 出发点 找出 由其 密度 
可达 的 样本 生成 聚 类 蔟 直到 所有 核心 
对象 被 访问 过 为止           
        聚 类 算法 中 的 距离 
度量 有 哪些 聚 类 算法 中 的 距离 度量 
一般用 闽 科夫斯基 距离 在 p 取 不同 的 值 
下 对应 不同 的 距离 例如 p = 1 的 
时候 对应 曼哈顿 距离 p = 2 的 情况 下 
对应 欧式 距离 p = inf 的 情况 下 变为 
切比雪夫 距离 还有 jaccard 距离 幂 距离 闽 科夫斯基 的 
更 一般 形式 余弦 相似 度 加权 的 距离 马氏 
距离 类似 加权 作为 距离 度量 需要 满足 非 负性 
同一性 对称性 和直递/nr 性 闽 科夫斯基 在 p = 1 
的 时候 满足 读来 那个 性质 对于 一些 离散 属性 
例如 { 飞机 火车 轮船 } 则 不能 直接 在 
属性值 上 计算 距离 这些 称为 无序 属性 可以 用 
VDM Value Diffrence Metrix 属性 u 上 两个 离散 值 
a b 之间 的 VDM 距离 定义 为 其中 表示 
在 第 i 个 簇 中 属性 u 上 a 
的 样本 数 样本空间 中 不同 属性 的 重要性 不同 
的 时候 可以 采用 加权 距离 一般 如果 认为 所有 
属性 重要性 相同 则 要对 特征 进行 归一化 一般来说 距离 
需要 的 是 相似性 度量 距离 越大 相似 度 越小 
用于 相似性 度量 的 距离 未必 一定 要 满足 距离 
度量 的 所有 性质 例如 直 递 性 比如 人马 
和人/nr 人马/n 和马的/nr 距离/n 较近/i 然后 人和 马的/nr 距离 可能 
就 很远                 
  解释 贝叶斯 公式 和 朴素 贝叶斯 分类 贝叶斯 公式 
最小化/l 分类/n 错误/n 的/uj 贝叶斯/nr 最优/d 分类器/n 等价/n 于/p 最大化/nt 
后验/nr 概率/n 基于 贝叶斯 公式 来 估计 后验/nr 概率 的 
主要 困难在于 条件概率 是 所有 属性 上 的 联合 概率 
难以 从 有限 的 训练 样本 直接 估计 得到 朴素 
贝叶斯 分类器 采用 了 属性 条件 独立性 假设 对于 已知 
的 类别 假设 所有 属性 相互 独立 这样 朴素 贝叶斯 
分类 则 定义 为 如果 有 足够 多 的 独立 
同 分布 样本 那么 可以 根据 每个 类 中的 样本 
数量 直接 估计 出来 在 离散 情况下 先验概率 可以 利用 
样本 数量 估计 或者 离散 情况 下 根据 假设 的 
概率密度函数 进行 最大 似 然 估计 朴素 贝叶斯 可以 用于 
同时 包含 连续变量 和 离散 变量 的 情况 如果 直接 
基于 出现 的 次数 进行 估计 会 出现 一项 为 
0 而 乘 积为 0 的 情况 所以 一般 会 
用 一些 平滑 的 方法 例如 拉普拉斯 修正 这样 既 
可以 保证 概率 的 归一化 同时 还 能避免 上述 出现 
的 现象                 
  L1 和 L2 正则化 的 作用 解释 L1 正则化 
是 在 代价 函数 后面 加上 L2 正则化 是 在 
代价 函数 后面 增加 了 两者 都 起到 一定 的 
过拟合 作用 两者 都 对应 一定 的 先验 知识 L1 
对应 拉普拉斯 分布 L2 对应 高斯分布 L1 偏向 于 参数 
稀疏 性 L2 偏向 于 参数 分布 较为 稠密 TF 
IDF 是 什么 TF 指 Term frequecy 代表 词频 IDF 
代表 inverse document frequency 叫做 逆 文档 频率 这个 算 
法 可以 用 来 提取 文档 的 关键词 首先 一般 
认为 在 文章 中 出现 次数 较多 的 词 是 
关键词 词频 就 代表 了 这 一项 然而 有些 词 
是 停用词 例如 的 是 有 这种 大量 出现 的 
词 首先 需要 进行 过滤 比如 过滤 之后 再 统计 
词频 出现 了 中国 蜜蜂 养殖 且 三个 词 的 
词频 几乎 一致 但是 中国 这个 词 出现 在 其他 
文章 的 概率 比 其他 两个 词 要高 不少 因此 
我们 应该 认为 后 两个 词 更能 表现 文章 的 
主题 IDF 就 代表 了 这样 的 信息 计算 该 
值 需要 一个 语料库 如果 一个 词 在 语料库 中 
出现 的 概率 越小 那么 该词 的 IDF 应该 越大 
一般来说 TF 计算公式 为 某个 词 在 文章 中 出现 
次数 / 文章 的 总 词数 这样 消除 长文 章中 
词 出现 次数 多 的 影响 IDF 计算公式 为 log 
语料库 文章 总数 / 包含 该词 的 文章 数 + 
1 将 两者 乘 乘起来 就 得到 了 词 的 
TF IDF 传统 的 TF IDF 对词 出现 的 位置 
没有 进行 考虑 可以 针对 不同 位置 赋予 不同 的 
权重 进行 修正 注意 这些 修正 之所以 是 有效 的 
正是 因为 人 观测 过 了 大量 的 信息 因此 
建议 了 一个 先验 估计 人 将 这个 先验 估计 
融合 到了 算法 里面 所 以使 算法 更加 的 有效 
文本 中的 余弦 距离 是 什么 有 哪些 作用 余弦 
距离 是 两个 向量 的 距离 的 一种 度量 方式 
其 值 在 1 ~ 1 之间 如果 为 1 
表示 两个 向量 同相 0 表示 两个 向量 正交 1 
表示 两个 向量 反向 使用 TF IDF 和 余弦 距离 
可以 寻找 内容 相似 的 文章 例如 首先 用 TF 
IDF 找出 两 篇 文章 的 关键词 然后 每个 文章 
分别 取出 k 个 关键词 10 20个 统计 这些 关键词 
的 词频 生成 两篇 文章 的 词频 向量 然后 用 
余弦 距离 计算 其 相似 度 参考资料 1 . 面试 
经验 分享 之 机器学习 大 数据 问题 本文 中 大 
部分 问题 就是 这 篇 文章 中 的 基础 部分 
的 问题 2 . 统计 学习 方法 李航 3 . 
机器学习 周志华 4 . TF IDF 与与 余弦 相似性 的 
应用 阮 一峰 的 网络 日志 http / / www 
. ruanyifeng . com / blog / 2013/03 / tf 
idf . html 