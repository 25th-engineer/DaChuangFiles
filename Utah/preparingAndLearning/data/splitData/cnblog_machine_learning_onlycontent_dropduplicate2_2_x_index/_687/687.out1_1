欢迎 大家 前往 腾讯 云 技术 社区 获取 更多 腾讯 
海量 技术 实践 干货 哦 ~ 作者 汪毅雄/nr 导语/n  /i 
本文/r 用/p 容易/a 理解/v 的/uj 语言/n 和/c 例子/n 来/v 解释/v 
了/ul 决策树/n 三种/m 常见/a 的/uj 算法/n 及其/c 优劣/a 随机 森林 
的 含义 相信 能 帮助 初学者 真正 地 理解 相关 
知识 决策树 引言 决策树 是 机器 学习 中 一种 非常 
常见 的 分类 方法 也 可以 说 是 所有 算法 
中最 直观 也 最好 理解 的 算法 先 举个 最 
简单 的 例子 A 你 去不去 吃饭 B 你 去 
我 就去 你 去 我 就去 这是 典型 的 决策 
树 思想 再举 个 例子 有人 找 我 借钱 当然 
不太可能 借 还是 不借 我会 结合 根据 我 自己 有 
没有 钱 我 自己 用 不用 钱 对方 信用 好 
不好 这三个 特征 来 决定 我 的 答案 我们 把 
转到 更 普遍 一点 的 视角 对于 一些 有 特征 
的 数据 如果 我们 能够 有 这么 一颗 决策树 我们 
也 就能 非常 容易 地 预测 样本 的 结论 所以 
问题 就 转换 成 怎么 求 一颗 合适 的 决策 
树 也 就是 怎么 对 这些 特征 进行 排序 在对 
特征 排序 前先 设想 一下 对 某 一个 特征 进行 
决策 时 我们 肯定 希望 分类 后 样本 的 纯度 
越高 越好 也 就是说 分支 结点 的 样本 尽可能 属于 
同一 类别 所以 在 选择 根 节点 的 时候 我们 
应该 选择 能够 使得 分支 结点 纯度 最高 的 那个 
特征 在 处理 完 根 节 点后 对于 其 分支 
节点 继续 套用 根 节点 的 思想 不断 递归 这样 
就 能 形成 一颗 树 这 其实 也是 贪心 算法 
的 基本 思想 那 怎么 量化 纯度 最高 呢 熵 
就 当仁不让 了 它 是 我们 最 常用 的 度量 
纯度 的 指标 其 数学 表达式 如下 其中 N 表示 
结论 有 多少 种 可能 取值 p 表示 在 取 
第 k 个 值 的 时候 发生 的 概率 对于 
样本 而言 就是 发生 的 频率 / 总 个数 熵 
越小 说明 样本 越纯/nr 以 一个 两 点 分布 样本 
X x = 0 或 1 的 熵 的 函数 
图像 来 说明 吧 横坐标 表示 样 本值 为 1 
的 概率 纵坐标 表示 熵 可以 看到 到 当 p 
x = 1 = 0时 也 就是说 所有 的 样本 
都为 0 此时 熵 为 0 . 当 p x 
= 1 = 1时 也 就是说 所有 的 样本 都为 
1 熵 也为 0 . 当 p x = 1 
= 0.5时 也 就是 样本 中 0 1 各占 一半 
此时 熵 能 取得 最大值 扩展 一下 样本 X 可能 
取值 为 n 种 x1 xn 可以 证明 当 p 
xi 都 等于 1 / n 时 也 就是 样本 
绝对 均匀 熵 能 达到 最大 当 p xi 有 
一个 为 1 其他 都为 0时 也 就是 样本 取值 
都是 xi 熵 最小 决策树 算法 ID3 假设在 样 本集 
X 中 对于 一个 特征 a 它 可能 有 a1 
a2 an 这些 取值 如果 用 特征 a 对 样本 
集 X 进行 划分 把 它 当 根 节点 肯定 
会 有n个/nr 分支 结点 刚才 提了 我们 希望 划分 后 
分支 结点 的 样本 越纯/nr 越好 也 就是 分支 结点 
的 总 熵 越小 越好 因为 每个 分支 结点 的 
个数 不一样 因此 我们 计算 总 熵 时 应该 做 
一个 加权 假设 第 i 个 结点 样本 个数 为 
W ai 其 在 所有 样本 中的 权值 为 W 
ai / W X 所以 我们 可以 得到 一个 总 
熵 这个 公式 代表 含义 一句话 加权 后 各个 结点 
的 熵 的 总和 这个 值 应该 越小 纯度 越高 
这时候 我们 引入 一个 名词 叫 信息 增益 G X 
a 意思 就是 a 这个 特征 给 样本 带来 的 
信息 的 提升 公式 就是 由于 H X 对 一个 
样本 而言 是 一个 固定值 因 此信息 增益 G 应该 
越大 越好 寻找 使得 信息 增益 最大 的 特征 作为 
目标 结点 并 逐步 递归 构 建树 这 就是 ID3 
算法 的 思想 好了 以 一个 简单 的 例子 来 
说明 信息 增益 的 计算 上面 的 例子 我 计算 
一下 特征 1 的 信息 增益 首先 计算 样本 的 
熵 H X 再 计算 总 熵 可以 看到 特征 
1 有 3个 结点 A B C 其 分别 为 
6个 6个 5个 所以 A 的 权值 为 6 / 
6 + 6 + 5 B 的 权值 为 6 
/ 6 + 6 + 5 C 的 为 5 
/ 6 + 6 + 5 因为 我们 希望 划分 
后 结点 的 纯度 越高 越好 因此 还 需要 再 
分别 计算 结点 A B C 的 熵 特征 1 
= A 3个 是 3个 否 其 熵 为特征 1 
= B 2个 是 4个 否 其 熵 为特征 1 
= C 4个 是 1个 否 其 熵 为 这样 
分支 结点 的 总 熵 就 等于 特征 1 的 
信息 增益 就 等于 0.998 0.889 = 0.109类 似地 我们 
也 能 算出 其他 的 特征 的 信息 增益 最终 
取 信息 增益 最大 的 特征 作为 根 节点 以上 
计算 也 可以 有 经验 条件 熵 来 推导 G 
X A = H X H X | A 这 
部分 有 兴趣 的 同学 可以 了解 一下 C 4.5 
在 ID3 算法 中 其实 有个 很 明显 的 问题 
如果 有 一个 样 本集 它 有一个 叫 id 或者 
姓名 之类 的 唯一 的 的 特征 那就 完蛋了 设想 
一下 如果 有n个/nr 样本 id 这个 特征 肯定会 把 这个 
样本 也 分成 n 份 也 就是 有n个/nr 结点 每个 
结点 只有 一个 值 那 每个 结点 的 熵 就为 
0 就是说 所有 分支 结点 的 总 熵 为 0 
那么 这个 特征 的 信息 增益 一定会 达到 最大值 因此 
如果 此时 用 ID3 作为 决策树 算法 根 节点 必然 
是 id 这个 特征 但是 显然 这 是 不合理 的 
当然 上面 说 的 是 极限 情况 一般 情况 下 
如果 一个 特征 对 样本 划分 的 过于 稀疏 这个 
也 是 不合理 的 换 句话 就是 偏向 更多 取值 
的 特征 为了 解决 这个 问题 C 4.5 算法 采用 
了 信息 增益 率 来作 为特征 选取 标准 所谓 信息 
增益 率 是 在 信息 增益 基础 上 除了 一项 
split information 来 惩罚 值 更多 的 属性 而 这个 
split information 其实 就是 特征 个数 的 熵 H A 
为什么 这样 可以 减少 呢 以 上面 id 的 例子 
来 理解 一下 如果 id 把 n 个 样本 分成 
了 n 份 那 id 这个 特征 的 取值 的 
概率 都是 1 / n 文章 引言 已经 说 了 
样本 绝对 均匀 的 时候 熵 最大 因此 这种 情况 
以 id 为特征 虽然 信息 增益 最大 但是 惩罚 因子 
split information 也 最大 以此来 拉 低 其 增益 率 
这 就是 C 4.5 的 思想 CART 决策树 的 目的 
最终 还是 寻找 到 区分 样本 的 纯度 的 量化 
标准 在 CART 决策树 中 采用 的 是 基尼指数 来 
作为 其 衡量标准 基尼系数 直观 的 理解 是 从 集合 
中 随机 抽取 两个 样本 如果 样本 集合 越纯/nr 取到 
不同 样本 的 概率 越小 这个 概率 反应 的 就是 
基尼系数 因此 如果 一个 样本 有K个/nr 分类 假设 样本 的 
某一个 特征 a 有n个/nr 取值 的话 其 某一个 结点 取到 
不同 样本 的 概率 为 因此 k 个 分类 的 
概率 总和 我们 称之为 基尼系数 而 基尼指数 则是 对 所有 
结点 的 基尼 系数 进行 加权 处理 计算 出来 后 
我们 会 选择 基尼系数 最小 的 那个 特征 作为 最优 
划分 特征 剪枝 剪枝 的 目的 其实 就是 防止 过拟合 
它 是 决策树 防止 过拟合 的 最主要 手段 决策树 中 
为了 尽可能 争取 的 分类 训练样本 所以 我们 的 决策树 
也会 一直 生长 但是 呢 有时候 训练样本 可能会 学 的 
太好 以至于 把 某些 样本 的 特有 属性 当成 一般 
属性 这时候 就 我们 就 需要 主动 去 除 一些 
分支 来 降低 过拟合 的 风险 剪枝 一般 有 两种 
方式 预/v 剪枝/n 和后/nr 剪枝/n 预 剪枝 一般 情况下 只要 
结点 样本 已经 100% 纯 了 树 才会 停止 生长 
但 这个 可能 会 产生 过拟合 因此 我们 没有 必要 
让 它 100% 生长 所以 在这之前 设定 一些 终止 条件 
来 提前 终止 它 这 就叫 预 剪枝 这个 过程 
发生 在 决策树 生成 之前 一般 我们 预 剪枝 的 
手段 有 1 限定 树 的 深度 2 节点 的 
子 节点 数目 小于 阈值 3 设定 结点 熵 的 
阈值 等等 后 剪枝 顾名思义 这个 剪枝 是 在 决策树 
建立 过程 后 后 剪枝 算法 的 算法 很多 有些 
也挺 深奥 这里 提 一个 简单 的 算法 的 思想 
就不 深究 啦 Reduced Error Pruning REP 该 剪枝 方法 
考虑 将 树上 的 每个 节点 都 作为 修剪 的 
候选 对象 但是 有 一些 条件 决定 是否 修剪 通常 
有这 几步 1 删除 其 所有 的 子树 使其 成为 
叶 节点 2 赋予 该 节点 最 关联 的 分类 
3 用 验证 数据验证 其 准确度 与 处理 前 比较 
如果不 比 原来 差 则 真正 删除 其 子树 然后 
反复 从下 往上 对 结点 处理 这个 处理 方式 其实 
是 处理 掉 那些 有害 的 节点 随机 森林 随机 
森林 的 理论 其实 和 决策树 本身 不 应该 牵扯 
在 一起 决策树 只能 作为 其 思想 的 一种 算法 
为什么 要 引入 随机 森林 呢 我们 知道 同 一批 
数据 我们 只能 产生 一颗 决策树 这个 变化 就 比较 
单一 了 还有 要用 多个 算法 的 结合 呢 这就 
有了 集成 学习 的 概念 图中 可以 看到 每个 个体 
学习 器 弱 学习 器 都可 包含 一种 算法 算法 
可以 相同 也 可以 不同 如果 相同 我们 把 它 
叫做 同质 集成 反之 则为 异质 随机 森林 则是 集成 
学习 采用 基于 bagging 策略 的 一个 特例 从 上图 
可以 看出 bagging 的 个体 学习 器 的 训练 集 
是 通过 随机 采样 得到 的 通过 n 次 的 
随机 采样 我们 就 可以 得到 n 个样 本集 对于 
这 n 个样 本集 我们 可以 分别 独立 的 训练 
出 n 个 个体 学习 器 再 对这 n 个 
个体 学习 器 通过 集合 策略 来 得到 最终 的 
输出 这 n 个 个体 学习 器 之间 是 相互 
独立 的 可以 并行 注 集成 学习 还有 另一种 方式 
叫 boosting 这种 方式 学习 器 之间 存在 强 关联 
有兴趣 的 可以 了解 下 随机 森林 采用 的 采样 
方法 一般 是 是 Bootstap sampling 对于 原始 样 本集 
我们 每次 先 随机 采集 一个 样本 放入 采样 集 
然后 放回 也 就是说 下次 采样 时该/nr 样本 仍 有可能 
被 采集 到 经过 一定 数量 的 采样 后 得到 
一个样 本集 由于 是 随机 采样 这样 每次 的 采样 
集 是 和 原始 样 本集 不同 的 和 其他 
采样 集 也是 不同 的 这样 得到 的 个体 学习 
器 也是 不同 的 随机 森林 最主要 的 问题 是 
有了 n 个 结果 怎么 设定 结合 策略 主要 方式 
也 有 这么 几种 加权 平均法 平均法 常 用于 回归 
做法 就是 先 对 每个 学习 器 都 有一个 事先 
设定 的 权值 wi 然后 最终 的 输出 就是 当 
学习 器 的 权值 都为 1 / n 时 这个 
平均法 叫 简单 平均法 投票 法 投票 法 类似 我们 
生活 中 的 投票 如果 每个 学习 器 的 权值 
都是 一样 的 那么 有 绝对 投票 法 也 就是 
票数 过半 相对 投票 法 少数 服从 多数 如果 有 
加权 依然 是 少数 服从 多数 只不过 这 里面 的 
数 是 加权 后的/nr 例子 以 一个 简单 的 二次函数 
的 代码 来 看看 决策树 怎么 用吧 训练 数据 是 
100个 随机 的 真实 的 平方 数据 不同 的 深度 
将会 得到 不同 的 曲线 测试数据 也是 随机 数据 但是 
不同 深度 的 树 的 模型 产生 的 预测 值 
也不 太 一样 如图 这幅 图 的 代码 如下 我 
的 是 python 3.6 环境 需要 安装 numpy matplotlib sklearn 
这三个 库 需要的话 直接 pip install 大家 可以 跑跑 看看 
虽然 简单 但 挺 有趣 # / usr / bin 
/ python # * coding utf 8 * import numpy 
as np import matplotlib as mpl import matplotlib . pyplot 
as plt from sklearn . tree import D e c 
i s i o n T r e e R 
e g r e s s o r if _ 
_ name _ _ = = _ _ main _ 
_ # 准备 训练 数据 N = 100 x = 
np . random . rand N * 6 3 x 
. sort y = x * x x = x 
. reshape 1 1 mpl . rcParams font . sans 
serif = SimHei mpl . rcParams axes . unicode _ 
minus = False # 决策树 深度 及其 曲线 颜色 depth 
= 2 4 6 8 10 clr = rgbmy # 
实际 值 plt . figure facecolor = w plt . 
plot x y ro ms = 5 mec = k 
label = 实际 值 # 准备 测试数据 x _ test 
= np . linspace 3 3 50 . reshape 1 
1 # 构建 决策树 dtr = D e c i 
s i o n T r e e R e 
g r e s s o r # 循环 不同 
深度 情况下 决策树 的 模型 并用 之 测试数据 的 输出 
for d c in zip depth clr # 设置 最大 
深度 预 剪枝 dtr . set _ params max _ 
depth = d # 训练 决策树 dtr . fit x 
y # 用 训练 数据 得到 的 模型 来 验证 
测试数据 y _ hat = dtr . predict x _ 
test # 画出 模型 得到 的 曲线 plt . plot 
x _ test y _ hat color = c linewidth 
= 2 markeredgecolor = k label = Depth = % 
d % d # 一些 画图 的 基本 参数 plt 
. legend loc = upper center fontsize = 12 plt 
. xlabel X plt . ylabel Y plt . grid 
b = True ls = color = # 606060 plt 
. title 二次函数 决策树 fontsize = 15 plt . tight 
_ layout 2 plt . show 参考资料 机器学习 周志华 机器学习 
课程 邹博/nr 相关 阅读 机器学习 从 入门 到 第一 个 
模型 GBDT 算法 原理篇 道器 相融 由 Angel 谈 一个 
优秀 机器学习 平台 的 自我 修养 下 此文 已由 作者 
授权 腾讯 云 技术 社区 发布 转载 请 注明 文章 
出处 原文 链接 https / / www . qcloud . 
com / community / article / 160232 