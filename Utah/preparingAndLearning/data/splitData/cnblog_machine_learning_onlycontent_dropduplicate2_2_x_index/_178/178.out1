一 决策树 模型 组合 单 决策树 C 4.5 由于 功能 
太 简单 并且 非常 容易 出现 过 拟合 的 现象 
于是 引申 出 了 许多 变种 决策树 就是 将 单 
决策树 进行 模型 组合 形成 多 决策树 比较 典型 的 
就是 迭代 决策树 GBRT 和 随机 森林 RF 在 最近 
几年 的 paper 上 如 iccv 这种 重量级 会议 iccv 
09年/tdq 的/uj 里面/f 有/v 不少/d 文章/n 都是/i 与/p Boosting/w 和/c 
随机/d 森林/n 相关/v 的/uj 模型 组合 + 决策树 相关 算法 
有 两种 比较 基本 的 形式 随机 森林 RF 与 
GBDT 其他 比较 新的 模型 组合 + 决策树 算法 都是/nr 
来自 这 两种 算法 的 延伸 核心思想 其实 很多 渐进 
梯度 Gradient Boost 都 只是 一个 框架 里面 可以 套 
用 很多 不同 的 算法 首先 说明 一下 GBRT 这个 
算法 有 很多 名字 但 都是 同一个 算法 GBRT Gradient 
BoostRegression Tree 渐进 梯度 回归 树 GBDT Gradient BoostDecision Tree 
渐进 梯度 决策树 MART MultipleAdditive Regression Tree 多 决策 回归 
树 Tree Net 决策树 网络 二 GBRT 迭代 决策树 算法 
在 阿里 内部 用得 比较 多 所以 阿里 算法 岗位 
面试 时 可能会 问到 由 多棵 决策树 组成 所有 树 
的 输出 结果 累加 起来 就是 最终 答案 它 在被 
提出 之初 就和 SVM 一起 被 认为 是 泛化 能力 
generalization 较强 的 算法 近些年 更 因为 被 用于 搜索 
排序 的 机器学习 模型 而 引起 大家 关注 GBRT 是 
回归 树 不是 分类 树 其 核心 就 在于 每 
一棵树 是从 之前 所有 树 的 残差 中 来 学习 
的 为了 防止 过拟合 和 Adaboosting 一样 也 加入 了 
boosting 这 一项 关于 GBRT 的 介绍 可以 可以 参考 
GBDT MART 迭代 决策树 入门教程 | 简介 提起 决策树 DT 
DecisionTree 不要 只 想到 C 4.5 单 分类 决策树 GBRT 
不是 分类 树 而是 回归 树 决策树 分为 回归 树 
和 分类 树 回归 树 用于 预测 实数值 如 明天 
温度 用户 年龄 分类 树 用于 分类 标签 值 如 
晴天 / 阴天 / 雾 / 雨 用户 性别 注意 
前者 结果 加减 是 有 意义 的 如 10岁 + 
5岁 3岁 = 12岁 后者 结果 加减 无意义 如 男 
+ 女 = 到底 是 男 还是 女 GBRT 的 
核心 在于 累加 所有 树 的 结果 作为 最终 结果 
而 分类 树 是 没有 办法 累加 的 所以 GBDT 
中的 树 都是 回归 树 而非 分类 树 第 一棵树 
是 正常 的 之后 所有 的 树 的 决策 全 
是由 残差 此次 的 值 与 上次 的 值 之差 
来作 决策 三 算法 原理 0 . 给定 一个 初始值 
1 . 建立 M 棵 决策树 迭代 M 次 2 
. 对 函数 估计值 F x 进行 Logistic 变换 3 
. 对于 K 各 分类 进行 下面 的 操作 其实 
这个 for 循环 也 可以 理解 为 向量 的 操作 
每个 样本点 xi 都 对应 了 K 种 可能 的 
分类 yi 所以 yi F xi p xi 都是/nr 一个 
K 维 向量 4 . 求得 残差 减少 的 梯度方向 
5 . 根据 每个 样本点 x 与其 残差 减少 的 
梯度方向 得到 一棵 由 J 个 叶子 节点 组成 的 
决策树 6 . 当 决策树 建立 完成后 通过 这个 公式 
可以 得到 每个 叶子 节点 的 增益 这个 增益 在 
预测 时候 用 的 每个 增益 的 组成 其实 也 
是 一个 K 维 向量 表示 如果 在 决策树 预测 
的 过程 中 如果 某个 样本点 掉入 了 这个 叶子 
节点 则 其 对应 的 K 个 分类 的 值 
是 多少 比如 GBDT 得到 了 三棵 决策树 一个 样本点 
在 预测 的 时候 也会 掉入 3个 叶子 节点 上 
其 增益 分别为 假设 为 3分 类 问题 0.5 0.8 
0.1 0.2 0.6 0.3 0.4 . 0.3 0.3 那么 这样 
最终 得到 的 分类 为 第二个 因为 选择 分类 2 
的 决策树 是 最多 的 7 . 将 当前 得到 
的 决策树 与 之前 的 那些 决策树 合并 起来 作为 
一个 新的 模型 跟 6中 的 例子 差不多 还是 年龄 
预测 简单 起见 训练 集 只有 4 个人 A B 
C D 他们 的 年龄 分别 是 14 16 24 
26 其中 A B 分别 是 高一 和 高三 学生 
C D 分别 是 应届 毕业生 和 工作 两年 的 
员工 如果 是 用 一棵 传统 的 回归 决策树 来 
训练 会 得到 如下 所示 结果 现在 我们 使用 GBDT 
来做 这件事 由于 数据 太少 我们 限定 叶子 节点 做多 
有 两个 即 每 棵树 只有 一个 分枝 并且 限定 
只学 两棵树 我们 会 得到 如下 所示 结果 在 第一 
棵树 分枝 和 一样 由于 A B 年龄 较为 相近 
C D 年龄 较为 相近 他们 被 分为 两拨 每 
拨用 平均 年龄 作为 预测值 此时 计算 残差 残差 的 
意思 就是   A 的 预测 值   +   
A 的 残差   =   A 的 实际 值 
所以 A 的 残差 就是 16 15 = 1 注意 
A 的 预测 值 是 指 前面 所有 树 累加 
的 和 这里 前面 只有 一棵树 所以 直接 是 15 
如果 还有 树 则 需要 都累/nr 加起来 作为 A 的 
预测 值 进而 得到 A B C D 的 残差 
分别为 1 1 1 1 然后 我们 拿 残差 替代 
A B C D 的 原值 到 第二 棵树 去 
学习 如果 我们 的 预测 值 和 它们 的 残差 
相等 则 只需 把 第二 棵树 的 结论 累加 到 
第一 棵 树上 就能 得到 真实 年龄 了 这里 的 
数据 显然是 我 可以 做 的 第二 棵树 只有 两个 
值 1 和 1 直接 分成 两个 节点 此时 所有 
人 的 残差 都是 0 即 每个人 都 得到 了 
真实 的 预测 值 换句话说 现在 A B C D/w 
的/uj 预测/vn 值/n 都和/nr 真实/d 年龄/n 一致/d 了/ul A   
14岁 高一 学生 购物 较少 经常 问 学长 问题 预测 
年龄 A   =   15   –   1 
  =   14B   16岁 高三 学生 购物 较少 
经常 被 学弟 问问题 预测 年龄 B   =   
15   +   1   =   16C   
24岁 应届 毕业生 购物 较多 经常 问 师兄 问题 预测 
年龄 C   =   25   –   1 
  =   24D   26岁 工作 两年 员工 购物 
较多 经常 被 师弟 问问题 预测 年龄 D   = 
  25   +   1   =   26 
那么 哪里 体现 了 Gradient 呢 其实 回到 第 一棵树 
结束时 想一想 无论 此时 的 cost   function 是 什么 
是 均方差 还是 均差 只要 它 以 误差 作为 衡量 
标准 残差 向量 1   1   1   1 
都是 它 的 全局 最优 方向 这 就是 Gradient 四 
GBRT 适用范围 该 版本 的 GBRT 几乎 可 用于 所有 
的 回归 问题 线性 / 非线性 相对 logistic   regression 
仅能 用于 线性 回归 GBRT 的 适用 面 非常 广 
亦可 用于 二分 类 问题 设定 阈值 大于 阈值 为 
正 例 反 之为 负 例 五 搜索引擎 排序 应用 
RankNet 搜索 排序 关注 各个 doc 的 顺序 而不是 绝对值 
所以 需要 一个 新的 cost   function 而 RankNet 基本 
就是 在 定义 这个 cost   function 它 可以 兼容 
不同 的 算法 GBDT 神经网络 . . . 实际 的 
搜索 排序 使用 的 是 Lambda MART 算法 必须 指出 
的 是 由于 这里 要 使用 排序 需要 的 cost 
  function LambdaMART 迭 代用 的 并 不是 残差 Lambda 
在 这里 充当 替代 残差 的 计算 方法 它 使用 
了 一种 类似 Gradient * 步长 模拟 残差 的 方法 
这里 的 MART 在 求解 方法 上 和 之前 说 
的 残差 略有不同 其 区别 描述 见 这里 搜索 排序 
也 需要 训练 集 但 多数 用 人工 标注 实现 
即对 每个 query doc pair 给定 一个 分值 如 1 
2 3 4 分值 越高 越 相关 越 应该 排到 
前面 RankNet 就是 基于 此 制定 了 一个 学习 误差 
衡量 方法 即 cost   function RankNet 对 任意 两个 
文档 A B 通过 它们 的 人工 标注 分 差 
用 sigmoid 函数 估计 两者 顺序 和 逆序 的 概率 
P1 然后 同理 用 机器 学习 到 的 分 差 
计算 概率 P2 sigmoid 的 好处 在于 它 允许 机器学习 
得到 的 分值 是 任意 实数值 只要 它们 的 分 
差 和 标准 分 的 分 差 一致 P2 就 
趋 近于 P1 这时 利用 P1 和 P2 求 的 
两者 的 交叉 熵 该 交叉 熵 就是 cost   
function 有了 cost   function 可以 求导 求 Gradient Gradient 
即 每个 文档 得分 的 一个 下降 方向 组成 的 
N 维 向量 N 为 文档 个数 应该 说是 query 
doc   pair 个数 这里 仅仅 是 把 求 残差 
的 逻辑 替换 为 求 梯度 每个 样本 通过 Shrinkage 
累加 都会 得到 一个 最终 得分 直接 按 分数 从大到/nr 
小 排序 就 可以 了 