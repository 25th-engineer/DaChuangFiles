一 概述 前 两章 我们 要求 分类器 做出 艰难 决策 
给出 该 数据 实例 属于 哪 一类 这类 问题 的 
明确 答案 不过 分类器 有时 会 产生 错误 结果 这时 
可以 要求 分类器 给出 一个 最优 的 类别 猜测 结果 
同时 给 出 这个 猜测 的 概率 估计值 概率论 是 
许多 机器学习 算法 的 基础 所以 深刻 理解 这 一 
主题 就 显得 十分 重要 第/m 3/m 章在/nr 计算/v 特征值/n 
取/v 某个/r 值/n 的/uj 概率/n 时/n 涉及/v 了/ul 一些/m 概率/n 
知识/v 在 那里 我们 先 统计 特征 在 数据 集中 
取 某个 特定 值 的 次数 然后 除以 数据集 的 
实例 总数 就 得到 了 特征 取 该 值 的 
概率 我们 将 在此 基础 上 深 人 讨论 本章 
会给 出 一些 使用 概率论 进行 分类 的 方法 首先 
从 一个 最 简单 的 概率 分类器 开始 然后 给 
出 一些 假 设来 学习 朴素 贝叶斯 分类器 我们 称之为 
朴素 是 因为 整个 形式化 过程 只做 最 原始 最 
简单 的 假设 不必 担心 你 会 详细 了解 到 
这些 假设 我们 将 充分 利用 Python 的 文本 处理 
能力 将 文档 切 分成 词 向量 然后 利用 词 
向量 对 文档 进行 分类 我们 还 将 构建 另一个 
分类器 观察 其 在 真实 的 垃圾 邮件 数据 集中 
的 过滤 效果 必要时 还会 回顾 一下 条件概率 最后 我们 
将 介绍 如何 从 个人 发布 的 大量 广告 中 
学习 分类器 并将 学习 结果 转换成 人类 可 理解 的 
信息 假设 现在 我们 有 一个 数据集 它 由 两类 
数据 组成 数据分布 如图所示 我们 现在 用 p1 x y 
表示 数 据点 x y 属于 类别 1 以图 中用 
圆点 表示 的 类别 的 概率 用 p2 x y 
表示 数 据点 x y 属于 类别 2 图 中用 
三角形 表示 的 类别 的 概率 那么 对于 一个 新 
数据 点 x y 可以 用 下面 的 规则 来 
判断 它 的 类别 也 就是说 我们 会 选择 高 
概率 对应 的 类别 这 就是 贝叶斯 决策 理论 的 
核心 思想 即 选择 具有 最高 概率 的 决策 回到 
图 如果 该 图中 的 整个 数据 使用 6个 浮点数 
来 表示 并且 计算 类别 概率 的 python 代码 只有 
两行 那么 你 会 更 倾向 于 使用 下面 哪种 
方法 来 对 该 数据 点 进行 分类 1 使用 
第 1 章的/nr knn 进行 1000次 距离 计算 2 使用 
第 2 章的/nr 决策树 分别 沿 x 轴 y 轴 
划分 数据 3 计 算数 据点 属于 每个 类别 的 
概率 并 进行 比较 使用 决策树 不会 非常 成功 而 
和 简单 的 概率 计算 相比 knn 的 计算 量 
太大 因此 对于 上述 问题 最佳 选择 是 使用 刚才 
提到 的 概率 比较 方法 二 优缺点 优点 在 数据 
较少 的 情况 下 仍然 有效 可以 处理 多 类别 
问题 缺点 对于 输入 数据 的 准备 方式 较为 敏感 
适用 数据类型 标称 型 数据 三 数学公式 贝叶 斯定理 了解 
贝叶 斯定理 之前 需要 先 了解 下 条件概率 P A 
| B 表示 在 事件 B 已经 发生 的 条件 
下 事件 A 发生 的 概率 假如 我们 已经 知道 
了 P A | B 但是 现在 我们 想 要求 
P B | A 也 就是 在 事件 A 发生 
的 条件 下 事件 B 发生 的 概率 这时 贝叶 
斯定理 就 派上用场 了 前面 提到 贝叶斯 决策理论 要求 计算 
两个 概率 p1 x y 和 p2 x y 但 
这 两个 准则 并 不是 贝叶斯 决策 理论 的 所有 
内容 使用 p1 p2 只是 为了 简化 描述 而 真正 
需要 计算 和 比较 的 是 p c1 | x 
y 和p/nr c2 | x y 这些 符号 的 意思 
是 给定 某个 x y 表示 的 数据 点 那么 
该 数据 点 来自 类别 c1 的 概率 是 多少 
来自 c2 的 概率 又是 多少 现 分别 有 A 
B 两个 容器 在 容器 A 里 分别 有 7 
个 红球 和 3 个 白球 在 容器 B 里 
有 1 个 红球 和 9 个 白球 现 已知 
从 这两个 容器 里 任意 抽出 了 一个 球 问 
这个 球 是 红球 且 来自 容器 A 的 概率 
是 多少 假设 已经 抽出 红球 为 事件 B 选中 
容器 A 为 事件 A 则有 P B = 8/20 
P A = 1/2 P B | A = 7/10 
按照 公式 则有 P A | B = 7/10 * 
1/2 / 8/20 = 0.875 四 使用 朴素 贝叶斯 进行 
文档 分类 机器学习 的 一个 重要 应用 就是 文档 的 
自动 分类 在 文档 分类 中 整个 文档 如一 封电子邮件 
是 实例 而 电子 邮件 中 的 某些 元素 则 
构成 特征 虽然 电子 邮件 是 一种 会 不断 增加 
的 文本 但 我们 同样 也 可以 对 新闻 报道 
用户 留言 政府 公文 等 其他 任意 类型 的 文本 
进行 分类 我们 可以 观察 文档 中 出现 的 词 
并把 每个 词 的 出现 或者 不出 现 作为 一个 
特征 这样 得到 的 特征 数目 就 会跟 词汇表 中的 
词目 一样 多 朴素 贝叶斯 是 上节 介绍 的 贝叶斯 
分类器 的 一个 扩展 是 用于 文档 分类 的 常用 
算法 使用 每个 词作 为特征 并 观察 它们 是否 出现 
这样 得到 的 特征 数目 会 有 多少 呢 针对 
的 是 哪一种 人类 语言 呢 当然 不止 一种 语言 
据估计 仅在 英语 中 单词 的 总数 就有 500000 之多 
为了 能 进行 英文 阅读 估计 需要 掌握 数千 单词 
所谓 独立 指 的 是 统计 意义上 的 独立 即/v 
一个/m 特征/n 或者/c 单词/nr 出现/v 的/uj 可能性/n 与/p 它/r 和/c 
其他/r 单词/n 相邻/v 没有/v 关系/n 这个 假设 正是 朴素 贝叶斯 
分类器 中 朴素 一 词 的 含义 朴素 贝叶斯 分类器 
中的 另一个 假设 是 每个 特征 同等 重要 . 算法 
一般 流程 1 . 数据 的 收集 2 . 数据 
的 准备 数值 型 或 布尔 型 3 . 分析 
数据 4 . 训练 算法 计算 不同 的 独立 特征 
的 条件 概率 5 . 测试 算法 计算 错误率 6 
. 使用 算法 以 实际 应用 为 驱动 朴素 贝叶斯 
伪代码 1 . 计算 各 个 独立 特征 在 各个 
分类 中的 条件概率 2 . 计算 各 类别 出现 的 
概率 3 . 对于 特定 的 特征 输入 计算 其 
相应 属于 特定 分类 的 条件概率 4 . 选择 条件 
概率 最大 的 类别 作为 该 输入 类别 进行 返回 
五 准备 数据 从/p 文本/n 中/f 构建/v 词/n 向量/n 我们/r 
将把/i 文本/n 看成/v 单词/n 向量/nr 或者/c 词条/n 向量/n 也 就是说 
将 句子 转换 为 向量 考虑 出现 在 所有 文档 
中 的 所有 单词 再 决定 将 哪些 词 纳人 
词汇表 或者说 所要 的 词汇 集合 然后 必须 要将 每 
一篇 文档 转换 为 词汇 表上 的 向量 词表 到 
向量 的 转换 函数 1 def loadDataSet 2 postingList = 
my dog has flea problems help please 3 maybe not 
take him to dog park stupid 4 my dalmation is 
so cute I love him 5 stop posting stupid worthless 
garbage 6 mr licks ate my steak how to stop 
him 7 quit buying worthless dog food stupid 8 classVec 
= 0 1 0 1 0 1 # 1 代表 
侮辱性 词 0 代表 正常 言论 9 return postingList classVec 
10 11 def createVocabList dataSet # 根据 数据集 返回 关键 
词汇 向量 12 vocabSet = set # 创建 空的/nr 集合 
13 for document in dataSet 14 vocabSet = vocabSet | 
set document # 操作符 | 用来 求 两个 集合 的 
并 集 15 return list vocabSet # 返回 集合 中 
所有 不 重复 的 关键词 16 17 def setOfWords2Vec vocabList 
inputSet # vocabList = 词汇表 inputSet = 输入 的 文档 
# 文档 词汇 转换 成 文档 向量 18 returnVec = 
0 * len vocabList # 生成 一个 值 为 0 
长度 和 vocabList 一样 的 集合 19 for word in 
inputSet 20 if word in vocabList 21 returnVec vocabList . 
index word = 1 22 else print the word % 
s is not in my Vocabulary % word 23 return 
returnVec # 返回 输入 文档 inputSet 的 向量 第一个 函数 
loadDataset 创建 了 一些 实验 样本 该 函数 返回 的 
第一 个 变量 是 进行 词条 切分 后的/nr 文档 集合 
这些 文档 来自 斑点 犬 爱好者 留言板 这些 留言 文本 
被 切 分成 一 系列 的 词条 集合 标点符号 从 
文本 中 去掉 后 面会 探讨 文本处理 的 细节 loadDataSet 
函数 返回 的 第二 个 变量 是 一个 类别 标签 
的 集合 这里 有 两类 侮辱性 和非/nr 侮辱性 这些 文本 
的 类别 由 人工 标注 这些 标注 信息 用于 训练 
程序 以便 自动检测 侮辱性 留言 下一个 函数 createVocabList 会 创建 
一个 包含 在 所有 文档 中 出现 的 不重 复词 
的 列表 为此 使用 了 Python 的 set 数据类型 将 
词条 列表 输给 set 构造 数 set 就会 返回 一个 
不 重复 词表 首先 创建 一个 空集合 然后 将 每篇 
文档 返回 的 新词 集合 添加到 该 集合 中 操作符 
丨 用于 求 两个 集合 的 并 集 这也 是 
一个 按 位或 or 操作符 参见 附录 0 在 数学 
符号 表示 上 按 位或 操作 与 集合 求 并 
操作 使用 相同 记号 获得 词汇表 后 便 可以 使用 
函数 setofWords2Vec 该 函数 的 输人 参数 为 词汇表 及 
某个 文档 输出 的 是 文档 向量 向量 的 每一 
元素 为 1 或 0 分别 表示 词汇表 中的 单词 
在 输人 文档 中 是否 出现 函数 首先 创建 一个 
和 词汇表 等长 的 向量 并 将其 元素 都 设置 
为 0 接着 遍历 文档 中 的 所有 单词 如果 
出现 了 词汇表 中的 单词 则将 输出 的 文档 向量 
中的 对应 值 设为 1 一切 都 顺利 的话 就 
不 需要 检查 某个 词 是否 还 vobalist 中 后边 
可能会 用 到这 一 操作 测试代码 1 listOPost listClasses = 
bayes . loadDataSet 2 listOPost 3 my dog has flea 
problems help please maybe not take him to dog park 
stupid my dalmation is so cute I love him stop 
posting stupid worthless garbage mr licks ate my steak how 
to stop him quit buying worthless dog food stupid 4 
listClasses 5 0 1 0 1 0 1 6 myVocabList 
= bayes . createVocabList listOPost 7 myVocabList 8 garbage love 
my dog park buying help is so to ate steak 
please him not stupid take maybe posting problems worthless I 
food quit mr dalmation stop has licks how flea cute 
9 # 检查 上述 的 词表 发现 这里 不 会 
出现 重复 的 单词 10 bayes . setOfWords2Vec myVocabList listOPost 
0 # 把 文档 转换成 向理/nr 11 0 0 1 
1 0 0 1 0 0 0 0 0 1 
0 0 0 0 0 0 1 0 0 0 
0 0 0 0 1 0 0 1 0 12 
bayes . setOfWords2Vec myVocabList listOPost 3 # 把 文档 转换成 
向理/nr 13 1 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 1 0 0 
1 0 1 0 0 0 0 0 1 0 
0 0 0 0 六 训练 算法 从词向/nr 量计算/l 概率/n 
前面/f 介绍/v 了/ul 如何/r 将/d 一组/m 单词/n 转换/v 为/p 一组/m 
数字/n 接下 来 看看 如何 使用 这些 数字 计算 概率 
现在 已经 知道 一个 词 是否 出现 在 一篇 文档 
中 也 知道 该 文档 所属 的 类别 还记得 前面 
提到 的 贝叶斯 准则 我们 重写 贝叶斯 准则 将 之前 
的 x y 替换 为 w 粗体 w 表示 这 
是 一个 向量 即 它 由 多个 数值 组成 在 
这个 例子 中 数值 个数 与 词汇表 中的 词 个数 
相同 我们 将 使用 上述 公式 对 每个 类 计算 
该 值 然后 比较 这 两个 概率值 的 大小 如何 
计算 呢 首先 可以 通过 类别 i 侮辱性 留言 或非 
侮辱性 留言 中 文档 数 除以 总 的 文档 数来 
计算 概率 p ci 接下来 计算 p w | ci 
这里 就要 用到 朴素 贝叶斯 假设 如果 将 w 展开 
为 一个 个 独立 特征 那么 就 可以 将 上述 
概率 写作 p w0 w1 w2 . . wN | 
ci 这里 假设 所有 词 都 互相 独立 该 假设 
也 称作 条件 独立性 假设 它 意味着 可以 使用 p 
w0 | ci p w1 | ci p w2 | 
ci . . . p wN | ci 来 计算 
上述 概率 这就 极大 地 简化 了 计算 的 过程 
该 函数 的 伪代码 如下 计算 每个 类别 中 的 
文档 数目 对 每篇 训练 文档 对 每个 类别 如果 
词条 出现 文档 中 ― 增加 该 词条 的 计数 
值 增加 所有 词条 的 计 数值 对 每个 类别 
对 每个 词条 将该 词条 的 数目 除以 总 词条 
数目 得到 条件概率 还回 每个 类别 的 条件 概率 朴素 
贝叶斯 分类器 训练 函数 1 def trainNB0 trainMatrix trainCategory # 
trainMatrix 所有 文档 的 向量 形式 trainCategory 文档 的 分类 
类别 向量 # 计算 不同 分类 的 文档 概率 即 
P W | C1 P W | C0 2 numTrainDocs 
= len trainMatrix # numTrainDocs = 总 文档 数 3 
numWords = len trainMatrix 0 # numWords = 词汇 长度 
4 pAbusive = sum trainCategory / float numTrainDocs # 计算 
分类 = 1 的 文档 比例 p 1 的 概率 
这 是 一个 二 分类 的 问题 p 0 = 
1 p 1 5 p0Num = zeros numWords p1Num = 
zeros numWords # 初始化 概率 分子 6 p0Denom = 0.0 
p1Denom = 0.0 # 定义 分母 7 for i in 
range numTrainDocs 8 if trainCategory i = = 1 # 
类别 为 1 9 p1Num + = trainMatrix i # 
分子 向量 累计 相加 10 p1Denom + = sum trainMatrix 
i # 分母 向量 之和 11 else 12 p0Num + 
= trainMatrix i 13 p0Denom + = sum trainMatrix i 
14 p1Vect = p1Num / p1Denom # 当 类别 为 
1 时 计算 每个 文档 中 出现 词汇 的 概率 
15 p0Vect = p0Num / p0Denom # 16 return p0Vect 
p1Vect pAbusive 代码 函数 中的 输入 参数 为 文档 矩阵 
trainMa trix 以及 由 每篇 文档 类别 标签 所 构成 
的 向量 train Category 首先 计算 文档 属于 侮辱性 文档 
class = 1 的 概率 即 P 1 因为 这 
是 一个 二类 分类 问题 所以 可以 通过 1 P 
1 得到 P 0 对于 多于 两类 的 分类 问题 
则 需要 对 代码 稍加 修改 计算 p wi | 
c1 和p/nr wi | c0 需要 初始化 程序 中 的 
分子 变量 和 分母 变量 ① 由于 w 中 元素 
如此 众多 因此 可以 使用 NumPy 数组 快速 计算 这些 
值 上述 程序 中的 分母 变量 是 一个 元素 个数 
等于 词汇表 大小 的 NumPy 数组 在 for 循环 中 
要 遍历 训练 集 trainMatrix 中的 所有 文档 一旦 某个 
词语 侮辱性 或 正常 词语 在 某一 文档 中 出现 
则 该词 对应 的 个数 p1Num 或者 p0Num 就 加 
1 而且 在 所有 的 文档 中 该 文档 的 
总 词数 也 相应 加 1 ② 对于 两个 类别 
都要/nr 进行 同样 的 计算 处理 最后 对 每个 元素 
除 以该 类别 中的 总 词数 ③ 利用 NumPy 可以 
很好 实现 用 一个 数组 除以 浮点数 即可 若 使用 
常规 的 Python 列表 则 难以 完成 这种 任务 读者 
可以 自己 尝试 一下 最后 函数 会 返回 两个 向量 
和 一个 概率 接下来 试验 一下 在 Python 提示符 下 
输入 1 listOPosts listClasses = bayes . loadDataSet 2 3 
listOPosts 4 my dog has flea problems help please 5 
maybe not take him to dog park stupid 6 my 
dalmation is so cute I love him 7 stop posting 
stupid worthless garbage 8 mr licks ate my steak how 
to stop him 9 quit buying worthless dog food stupid 
10 listClasses 11 0 1 0 1 0 1 12 
13 myVocabList = bayes . createVocabList listOPosts 14 # 至此 
我们 构建 了 一个 包含 所有 词 的 列表 myVocabList 
15 myVocabList 16 garbage love my dog park buying help 
is so to ate steak please him not stupid take 
maybe posting problems worthless I food quit mr dalmation stop 
has licks how flea cute 17 18 trainMat = 19 
for postinDoc in listOPosts 20 trainMat . append bayes . 
setOfWords2Vec myVocabList postinDoc 21 22 trainMat 23 0 0 1 
1 0 0 1 0 0 0 0 0 1 
0 0 0 0 0 0 1 0 0 0 
0 0 0 0 1 0 0 1 0 24 
0 0 0 1 1 0 0 0 0 1 
0 0 0 1 1 1 1 1 0 0 
0 0 0 0 0 0 0 0 0 0 
0 0 25 0 1 1 0 0 0 0 
1 1 0 0 0 0 1 0 0 0 
0 0 0 0 1 0 0 0 1 0 
0 0 0 0 1 26 1 0 0 0 
0 0 0 0 0 0 0 0 0 0 
0 1 0 0 1 0 1 0 0 0 
0 0 1 0 0 0 0 0 27 0 
0 1 0 0 0 0 0 0 1 1 
1 0 1 0 0 0 0 0 0 0 
0 0 0 1 0 1 0 1 1 0 
0 28 0 0 0 1 0 1 0 0 
0 0 0 0 0 0 0 1 0 0 
0 0 1 0 1 1 0 0 0 0 
0 0 0 0 29 # 该 for 循环使用 词 
向量 来 填充 trainMat 列表 下面 给 出 属于 侮辱性 
文档 的 概率 以及 两个 类别 的 概率 向量 30 
p0V p1V pAb = bayes . trainNB0 trainMat listClasses 31 
pAb # 这 就是 任意 文档 属于 侮辱性 文档 的 
概率 32 0.5 33 p0V 34 array 0 . 0.04166667 
0.125 0.04166667 0 . 35 0 . 0.04166667 0.04166667 0.04166667 
0.04166667 36 0.04166667 0.04166667 0.04166667 0.08333333 0 . 37 0 
. 0 . 0 . 0 . 0.04166667 38 0 
. 0.04166667 0 . 0 . 0.04166667 39 0.04166667 0.04166667 
0.04166667 0.04166667 0.04166667 40 0.04166667 0.04166667 41 p1V 42 array 
0.05263158 0 . 0 . 0.10526316 0.05263158 43 0.05263158 0 
. 0 . 0 . 0.05263158 44 0 . 0 
. 0 . 0.05263158 0.05263158 45 0.15789474 0.05263158 0.05263158 0.05263158 
0 . 46 0.10526316 0 . 0.05263158 0.05263158 0 . 
47 0 . 0.05263158 0 . 0 . 0 . 
48 0 . 0 . 首先 我们 发现 文档 属于 
侮辱 类 的 概率 pAb 为 0.5 该 值 是 
正确 的 接下来 看一看 在 给定 文档 类别 条件 下 
词汇 表中 单词 的 出现 概率 看看 是否 正确 词汇 
表中 的 第一 个 词 是 cute 其 在 类别 
0 中 出现 1次 而在 类别 1中 从未 出现 对应 
的 条件 概率 分别为 0.041 666 67 与 0.0 该 
计算 是 正确 的 我们 找找 所有 概率 中的 最大值 
该 值 出现 在 P 1 数组 第 26个 下标 
位置 大小 为 0.157 89474 在 myVocabList 的 第 26个 
下标 位置 上 可以 查到 该 单词 是 stupid 这 
意味着 stupid 是 最能 表征 类别 1 侮辱性 文档 类 
的 单词 七 测试 算法 根据 现实 情况 修改 分类器 
利用 贝叶斯 分类器 对 文档 进行 分类 时 要 计算 
多个 概率 的 乘积 以 获得 文档 属于 某个 类别 
的 概率 即 计算 p w0 | 1 p w1 
| 1 p w2 | 1 如果 其中 一个 概率值 
为 0 那么 最后 的 乘积 也为 0 为 降低 
这种 影响 可以 将 所有 词 的 出现 数 初始 
化为 1 并将 分母 初始 化为 2 在 文本 编辑器 
中 打开 bayes . py 文件 并将 trainNB0 的 第 
4行 和第/nr 5行 修改 为 p0Num = ones numWords p1Num 
= ones numWords p0Denom = 2.0 p1Denom = 2.0 另一个 
遇到 的 问题 是 下 溢出 这 是 由于 太多 
很小 的 数 相乘 造成 的 当 计算 乘积 p 
w0 | ci p w1 | ci p w2 | 
ci . . . p wn | ci 时 由于 
大部分 因子 都 非常 小 所以 程序 会下 溢出 或者 
得到 不 正确 的 答案 读者 可以 用 Python 尝试 
相乘 许多 很小 的 数 最后 四舍五入 后会 得到 0 
一种 解决 办法 是 对 乘积 取 自然对数 在 代数 
中有 ln a * b = ln a + ln 
b 于是 通过 求 对数 可以 避免 下 溢出 或者 
浮点数 舍入 导致 的 错误 同时 采用 自然对数 进行 处理 
不会 有 任何 损失 图 给出 函数 f x 与 
ln f x 的 曲线 检查 这两条 曲线 就 会 
发现 它们 在 相同 区域 内 同时 增加 或者 减少 
并且 在 相同 点上 取到 极值 它们 的 取值 虽然 
不同 但 不 影响 最终 结果 函数 f x 与 
ln f x 会 一块 增大 这表明 想 求函数 的 
最大值 时 可以 使用 该 函数 的 自然 对 数来 
替换 原函数 进行 求解 通过 修改 return 前 的 两行 
代码 将 上述 做法 用到 分类器 中 p1Vect = log 
p1Num / p1Denom p0Vect = log p0Num / p0Denom 修改后 
的 trainNB0 代码 1 def trainNB0 trainMatrix trainCategory 2 numTrainDocs 
= len trainMatrix 3 numWords = len trainMatrix 0 4 
pAbusive = sum trainCategory / float numTrainDocs 5 p0Num = 
ones numWords p1Num = ones numWords # change to ones 
6 p0Denom = 2.0 p1Denom = 2.0 # change to 
2.0 7 for i in range numTrainDocs 8 if trainCategory 
i = = 1 9 p1Num + = trainMatrix i 
10 p1Denom + = sum trainMatrix i 11 else 12 
p0Num + = trainMatrix i 13 p0Denom + = sum 
trainMatrix i 14 p1Vect = log p1Num / p1Denom # 
change to log 15 p0Vect = log p0Num / p0Denom 
# change to log 16 return p0Vect p1Vect pAbusive 现在 
已经 准备好 构建 完整 的 分类器 了 当 使用 NumPy 
向量 处理 功能 时 这 一切 变得 十分 简单 打开 
文本 编辑器 将 下面 的 代码 添加到 bayes . py 
中 朴素 贝叶斯 分类 函数 1 def classifyNB vec2Classify p0Vec 
p1Vec pClass1 # 根据 输入 的 文档 对 文档 进行 
分类 预测 P W | C0 = p0Vec P W 
| C1 = p0Vec 2 p1 = sum vec2Classify * 
p1Vec + log pClass1 # element wise mult 3 p0 
= sum vec2Classify * p0Vec + log 1.0 pClass1 4 
if p1 p0 5 return 1 6 else 7 return 
0 8 9 def testingNB 10 listOPosts listClasses = loadDataSet 
11 myVocabList = createVocabList listOPosts 12 trainMat = 13 for 
postinDoc in listOPosts 14 trainMat . append setOfWords2Vec myVocabList postinDoc 
15 p0V p1V pAb = trainNB0 array trainMat array listClasses 
16 testEntry = love my dalmation 17 thisDoc = array 
setOfWords2Vec myVocabList testEntry 18 print testEntry classified as classifyNB thisDoc 
p0V p1V pAb 19 testEntry = stupid garbage 20 thisDoc 
= array setOfWords2Vec myVocabList testEntry 21 print testEntry classified as 
classifyNB thisDoc p0V p1V pAb classifyNB 代码 有 4个 输入 
要 分类 的 向量 vec2Clas sify 以及 使用 函数 trainNB0 
计算 得到 的 三个 概率 使用 NumPy 的 数组 来 
计算 两个 向量 相乘 的 结果 ① 这里 的 相乘 
是 指 对应 元素 相乘 即 先将 两个 向量 中的 
第 1个 元素 相乘 然后 将 第 2个 元素 相乘 
以此类推 接下来 将 词汇表 中 所有 词 的 对应 值 
相加 然后 将该 值 加到 类别 的 对数 概率 上 
最后 比较 类别 的 概率 返回 大 概率 对应 的 
类别 标签 下面 来 看看 实际 结果 将 程序清单 4 
3中 的 代码 添加 之后 在 Python 提示符 下 输入 
1 imp . reload bayes 2 module bayes from F 
\ \ 99999 _ 算法 \ \ 机器学习 实战 源代码 
\ \ m a c h i n e l 
e a r n i n g i n a 
c t i o n \ \ Ch04 \ \ 
bayes . py 3 bayes . testingNB 4 love my 
dalmation classified as 0 5 stupid garbage classified as 1 
八 文档 词 袋 模型 目前为止 我们 将 每个 词 
的 出现 与否 作为 一个 特征 这 可以 被 描述 
为 词集 模型 set of words model 如果 一个 词 
在 文档 中 出现 不止 一次 这 可能 意味着 包含 
该词 是否 出现 在 文档 中 所 不能 表达 的 
某种 信息 这种 方法 被 称为 词 袋 模型 bag 
of wordsmodel 在 词 袋中 每个 单词 可以 出现 多次 
而在 词 集中 每个 词 只能 出现 一次 为 适应 
词 袋 模型 需要 对 函数 setOf Words2Vec 稍加 修改 
修改后 的 函数 称为 bagOfWords2Vec 下面 的 程序 清单 给出 
了 基于 词 袋 模型 的 朴素 贝叶斯 代码 它 
与 函数 setOfWords2Vec 几乎 完全 相同 唯一 不同 的 是 
每当 遇到 一个 单词 时 它 会 增加 词 向量 
中的 对应 值 而 不 只是 将 对应 的 数值 
设为 1 朴素 贝叶斯 词 袋 模型 def bagOfWords2VecMN vocabList 
inputSet returnVec = 0 * len vocabList for word in 
inputSet if word in vocabList returnVec vocabList . index word 
+ = 1 return returnVec 九 示例 使用 朴素 贝叶斯 
过滤 垃圾邮件 