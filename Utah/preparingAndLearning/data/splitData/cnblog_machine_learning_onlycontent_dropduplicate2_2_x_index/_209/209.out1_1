一 引言 前面 讲到 的 基本 都是 分类 问题 分类 
问题 的 目标 变量 是 标称 型 数据 或者 离散 
型 数据 而 回归 的 目标 变量 为 连续型 也即 
是 回归 对 连续型 变量 做出 预测 最 直接 的 
办法 是 依据 输入 写出 一个 目标值 的 计算 公式 
这样 对于 给定 的 输入 利用 该 公式 可以 计算 
出 相应 的 预测 输出 这个 公式 称为 回归方程 而 
求 回归方程 显然 就是 求 该 方程 的 回归系数 而 
一旦 有了/nr 这些 回归系数 再 给定 输入 就 可以 将 
这些 回归系数 乘以 输入 值 就 得到 了 预测 值 
二 线性 回归 线性 回归 简单 而言 就是 将 输入项 
分别 乘以 一些 常量 再将 结果 加起来 得到 输出 假设 
输入 数据 存放在 矩阵 x 中 而 回归系数 存放在 向量 
w 中 那么 对于 给定 的 数据 x1 预测 结果 
将 会 通过 y = xTw 给出 那么 如何 才 
能够 找出 最佳 的 回归系数 向量 w 呢 很容易 想到 
使用 最小化 误差 的 w 但是 这里 的 误差 为 
预测 y 值 和 真实 y 值 的 差值 使用 
该 误差 的 简单 累加 将 会 出现 正 差值 
和负/nr 差值 的 相互 抵消 所以 我们 可以 采用 平方 
误差 来 进行 度量 即 Σ yi xiTW 2 i 
= 1 2 3 . . . . . . 
N N 为 样本 总数 这样 用 矩阵 表示 可以 
写成 y Xw T y Xw . 因为 要求 函数 
的 极小值 再 对 w 求导 得 xT y Xw 
则 令其 等于 0 即可 得到 w 的 最优 解 
w * = XTX 1XTy 需要 注意 的 是 公式 
中 出现 的 求 逆运算 而 对于 任意 一个 矩阵 
而言 不一定 可逆 所以 我们 在 实际 写 代码 过程 
中 需要 事先 确定 矩阵 是否 可逆 否则 很可能 会 
造成 程序 出现 严重 的 错误 有了 回归系数 的 求解 
公式 我们 就 可以 写 代码 来 根据 样本数据 拟合 
出 最佳 的 回归系数 向量 w 了 form numpy import 
* # 解析 文件 中 的 数据 为 适合 机器 
处理 的 形式 def loadDataSet filename numFeat = len open 
filename . readline . split \ t 1 dataMat = 
labelMat = fr = open filename for line in fr 
. readlines lineArr = curLine = line . strip . 
split \ t for i in range numFeat lineArr . 
extend float curLine i dataMat . append lineArr labelMat . 
append float curLine 1 return dataMat labelMat # 标准 线性 
回归 算法 # ws = X . T * X 
. I * X . T * Y def standRegres 
xArr yArr # 将 列表 形式 的 数据 转为 numpy 
矩阵 形式 xMat = mat xArr yMat = mat yArr 
. T # 求 矩阵 的 内积 xTx = xMat 
. T * xMat # numpy 线性代数 库 linalg # 
调用 linalg . det 计算 矩阵 行列式 # 计算 矩阵 
行列式 是否 为 0 if linalg . det xTx = 
= 0.0 print This matrix is singular cannot do inverse 
return # 如果 可逆 根据 公式 计算 回归系数 ws = 
xTx . I * xMat . T * yMat # 
可以 用 yHat = xMat * ws 计算 实际 值 
y 的 预测 值 # 返 回归系数 return ws 这里 
需要 说明 的 是 Numpy 提供 了 一个 线性代数 的 
库 linalg 其中 包含 有 很多 有用 的 函数 其中 
就有 代码 中用 到 的 计算 矩阵 行列式 值得 函数 
linalg . det 如果 行列式 值 为 0 则 矩阵 
不可逆 如果 不 为 0 那么 就 可以 顺利 求出 
回归系数 此外 我们 知道 线性 回归 的 方程 的 一般 
形式 为 y = wx + b 即 存在 一定 
的 偏移量 b 于是 我们 可以 将 回归系数 和 特征向量 
均 增加 一个 维度 将 函数 的 偏移 值 b 
也 算作 回归系数 的 一部分 占据 回归系数 向量 中 的 
一个 维度 比如 w = w1 w2 . . . 
wN b 相应 地 将 每条 数据 特征向量 的 第一 
个 维度 设置 为 1.0 即 所有 特征向量 的 第一 
个 维度 值 均为 1.0 这样 最后 得到 的 回归 
函数 形式 为 y = w 0 + w 1 
* x1 + . . . + w N * 
xN . 通过 上面 的 数据 我们 可以 得到 一个 
拟合 的 线性 回归模型 但是 我们 还 需要 验证 该 
模型 的 好坏 如果 将 所有 样本 的 真实 值 
y 保存 于 一个 数列 将 所有 的 预测 值 
yHat 保存 在 另外 一个 数列 那么 我们 可以 通过 
计算 这 两个 序列 的 相关 系数 来 度量 预测值 
与 真实 值得 匹配 程度 我们 可以 通过 NumPy 库 
中 提供 的 corrcoef 函数 计算 两个 序列 的 相关性 
比如 如下 代码 可以 得到 真 实值 和 预测 值 
序列 的 相关性 相关性 最好 为 1 最差 为 0 
表示 不 相关 # 计算 预测值 序列 yHat = xMat 
* ws corrcoef yHat . T yMat 三 局部 加权 
线性 回归 线性 回归 一个 比较 容易 出现 的 问题 
是 有可能 出现 欠 拟合 现象 欠 拟合 显然 不能 
取得 最好 的 预测 效果 因为 我们 求 的 是 
均方 误差 最小 的 模型 所以 可以 在 估计 中 
引入 一些 偏差 从而 降低 预测 的 均方 误差 达到 
偏差 和 方差 的 折中 从而 找到 最佳 的 模型 
参数 即 回归系数 解决 上述 问题 的 一个 方法 即是 
局部 加权 线性 回归 LWLR 即在 算法 中 为 每 
一个 待 预测 的 数据 点 附近 的 赋予 一定 
的 权重 越 靠近 预 测点 的 数据 点 分配 
的 权重 越高 这里 我们 采用 高斯 核 函数 为 
预测 点 附近 的 数据 点 分配 权重 即 w 
i i = exp | x i x | / 
2 * k2 其中 参数 k 可以 由 用户 自己 
定义 显然 有 上面 高斯 核 函数 可知 矩阵 W 
是 一个 只含 对角 元素 的 矩阵 这样 回归系数 的 
解的/nr 形式 变为 w * = xTWx 1 xTWy 对 
上述 代码 稍作 修改 之后 得到 如下 局部 加权 线性 
回归 函数 # 局部 加权 线性 回归 # 每个 测试点 
赋予 权重 系数 # @ testPoint 测试点 # @ xArr 
样本数据 矩阵 # @ yArr 样本 对应 的 原始 值 
# @ k 用户定义 的 参数 决定 权重 的 大小 
默认 1.0 def lwlr testPoint xArr yArr k = 1.0 
# 转为 矩阵 形式 xMat = mat xArr yMat = 
mat yArr . T # 样本数 m = shape xMat 
0 # 初始化 权重 矩阵 为 m * m 的 
单位阵 weights = mat eye m # 循环 遍历 各个 
样本 for j in range m # 计算 预 测点 
与 该 样本 的 偏差 diffMat = testPoint xMat j 
# 根据 偏差 利用 高斯 核 函数 赋予 该 样本 
相应 的 权重 weights j j = exp diffMat * 
diffMat . T / 2.0 * k * * 2 
# 将 权重 矩阵 应用到 公式 中 xTx = xMat 
. T * weights * xMat # 计算 行列式 值 
是否 为 0 即 确定 是否 可逆 if linalg . 
det xTx = = 0.0 print This matrix is singular 
cannot do inverse return # 根据 公式 计算 回归系数 ws 
= xTx . I * xMat . T * weights 
* yMat # 计算 测试点 的 预测 值 return testPoint 
* ws # 测试 集 进行 预测 def lwlrTest testArr 
xArr yArr k = 1.0 # 测试 集 样本数 m 
= shape testArr 0 # 测试 集 预测 结果 保存 
在 yHat 列表 中 yHat = zeros m # 遍历 
每 一个 测试 样本 for i in range m # 
计算 预测值 yHat i = lwlr testArr i xArr yArr 
k return yHat 需要 说明 的 是 当 为 某一 
预 测点 附件 的 数据 点 分配 权重 时 由 
高斯 核 函数 公式 可知 与 预测 点 约 相近 
的 点 分配 得到 的 权重 越大 否则 权重 将以 
指数级 衰减 与 预测 点 足够 远 的 数据 点 
权重 接近 0 那么 这些 数据 点将 不会在 该 次 
预测 中 起作用 当然 用户 可以 通过 自己 设定 参数 
k 来 控制 衰减 的 速度 如果 k 值 较大 
衰减 速度 较慢 这样 就 会有 更多 的 数据 点 
共同 参与 决策 否则 如果 参数 k 非常 小 那么 
衰减 速度 极快 参与 预 测点 决策 的 数据 点 
就 很少 所以 我们 在 实验 中 应该 多 选择 
几组 不同 的 k 值 得到 不同 的 回归模型 从而 
找到 最优 的 回归模型 对应 的 k 值 下图 从上到下 
依次 是 k 取 1.0 0.01 及 0.003 情况下 对应 
的 拟合 模型 图中 当 k = 1.0 上图 时 
意味着 所有 的 数据 等 权重 这样 拟合 结果 与 
普通 的 线性 回归 拟合 结果 一致 显然 很 可能 
出现 欠 拟合 情况 当 k = 0.01时 中图 显然 
得到 了 最好 的 拟合 效果 因为 此时 掌握 了 
数据 的 潜在 模式 在 预测 时 剔除 了 部分 
不 重要 数据 的 影响 增大 了 重要 数据 的 
权重 而 k = 0.003 下图 可以 看出 预 测时 
纳入 了 太多 的 噪声 点 拟合 的 直线 与 
数据 点 过于 贴近 出现 了 过拟合 的 现象 此外 
局部 加权 线性 回归 也 存在 一定 的 问题 相对 
于 普通 的 线性 回归 由于 引入 了 权重 大大 
增加 了 计算 量 虽然 取得 了 不错 的 拟合 
效果 但也 相应 地 付出 了 计算 量 的 代价 
我们 发现 在 k = 0.01时 大多 的 数据 点 
的 权重 都 接近 0 所以 如果 我们 能 避免 
这些 计算 将 一定 程度 上 减少 程序运行 的 时间 
从而 缓解 计算 量 增加 带来 的 问题 四 示例 
预测 鲍鱼 的 年龄 接下来 就 利用 上述 算法 对 
真实 的 数据 进行 测试 并 计算 预测 的 误差 
鲍鱼 的 年龄 可以 通过 鲍鱼 壳 的 层数 推算 
得到 在 运行 代码 前 我们 还 需要 添加 计算 
预测误差 的 函数 代码 # 计算 平方 误差 的 和 
def rssError yArr yHatArr # 返回 平方 误差 和 return 
yArr yHatArr * * 2 . sum 当然 为了 得到 
更好 的 效果 我们 有 必要 采取 交叉 验证 的 
方法 选取 多 个样 本集 来 进行 测试 找出 预测误差 
最小 的 回归模型 五 缩减 系数 理解 数据 的 方法 
试想 如果 此时 数据集 样本 的 特征 维度 大于 样本 
的 数量 此时 我们 还 能 采取 上面 的 线性 
回归 方法 求出 最佳 拟合 参数 么 显然 不 可能 
因为 当 样本 特征 维度 大于 样本数 时 数据 矩阵 
显然 是非 满秩/nr 矩阵 那么 对 非 满秩/nr 矩阵 求 
逆运算 会 出现 错误 为了 解决 这个 问题 科学家 提出 
了 岭回归 的 概念 此外 还有 一种 称为 前 向 
逐步回归 的 算法 该算 法 可以 取得 很好 的 效果 
且 计算 相对 容易 1 岭回归 简单 而言 岭回归 即是 
在 矩阵 xTx 上 加入 一个 λ I 从而 使得 
矩阵 非 奇异 进而 能对 矩阵 xTx + λ I 
求 逆 其中 矩阵 I 是 一个 单位矩阵 即 对角 
线上 元素 皆为 1 其他 均为 0 这样 回归系数 的 
计算 公式 变为 w * = xTx + λ I 
1 xTy 公式 中 通过 引入 该 惩罚 项 从而 
减少 不 重要 的 参数 更好 的 理解 和 利用 
数据 此外 增加 了 相关 约束 Σ wi2 = λ 
即 回归系数 向量 中 每个 参数 的 平方 和 不能 
大于 λ 这就 避免了 当 两个 或 多个 特征 相关 
时 可能 出现 很大 的 正 系数 和 很大 的 
负 系数 上面 的 岭回归 就是 一种 缩减 方法 通过 
此 方法 可以 去掉 不 重要 的 参数 更好 的 
理解 数据 的 重要性 和非/nr 重要性 从而 更好 的 预测 
数据 在 岭回归 算法 中 通过 预测误差 最小化 来 得到 
最优 的 λ 值 数据 获取 之后 将 数据 随机 
分成 两部分 一 部分 用于 训练 模型 另一 部分 则 
用于 测试 预测误差 为了 找到 最优 的 λ 可以 选择 
多个 不同 λ 值 重复 上述 测试过程 最终 得到 一个 
使 预测误差 最小 的 λ # 岭回归 # @ xMat 
样本数据 # @ yMat 样本 对应 的 原始 值 # 
@ lam 惩罚 项 系数 lamda 默认值 为 0.2 def 
ridgeRegres xMat yMat lam = 0.2 # 计算 矩阵 内积 
xTx = xMat . T * xMat # 添加 惩罚 
项 使 矩阵 xTx 变换 后 可逆 denom = xTx 
+ eye shape xMat 1 * lam # 判断 行列式 
值 是否 为 0 确定 是否 可逆 if linalg . 
det denom = = 0.0 print This matrix is singular 
cannot do inverse return # 计算 回归系数 ws = denom 
. I * xMat . T * yMat return ws 
# 特征 需要 标准化 处理 使 所有 特征 具有 相同 
重要性 def ridgeTest xArr yArr xMat = mat xArr yMat 
= mat yArr . T # 计算 均值 yMean = 
mean yMat 0 yMat = yMat yMean xMeans = mean 
xMat 0 # 计算 各 个 特征 的 方差 xVar 
= var xMat 0 # 特征 均值 / 方差 xMat 
= xMat xMeans / xVar # 在 30个 不同 的 
lamda 下 进行 测试 numTestpts = 30 # 30次 的 
结果 保存 在 wMat 中 wMat = zeros numTestpts shape 
xMat 1 for i in range numTestpts # 计算 对应 
lamda 回归系数 lamda 以 指数 形式 变换 ws = ridgeRegres 
xMat yMat exp i 10 wMat i = ws . 
T return wMat 需要 说明 的 几点 如下 1 代码 
中用 到 NumPy 库 中的 eye 方法来 生成 单位矩阵 2 
代码 中 仍 保留 了 判断 行列式 是否 为 0 
的 代码 原因 是 当 λ 取值 为 0时 又 
回到 了 普通 的 线性 回归 那么 矩阵 很可能 出现 
不 可逆 的 情况 3 岭回归 中 数据 需要 进行 
标准化 处理 即 数据 的 每一 维度 特征 减去 相应 
的 特征 均值 之后 再 除以 特征 的 方差 4 
这里 选择 了 30个 不同 的 λ 进行 测试 并且 
这里 的 λ 是 按照 指数级 进行 变化 从而 可以 
看出 当 λ 非常 小和 非常大 的 情况下 对 结果 
造成 的 影响 下 图示 出了 回归系数 与 log λ 
之间 的 关系 可以 看出 当 λ 非常 小时 系数 
与 普通 回归 一样 而 λ 非常大 时 所有 回归系数 
缩减 为 0 这样 可以 在 中间 的 某处 找到 
使得 预测 结果 最好 的 λ 2 逐步/d 前/f 向/p 
回归/v 逐步/d 前/f 向/p 回归/v 是/v 一种/m 贪心/v 算法/n 即 
每一步 都 尽可能 的 减小 误差 从一/nr 开始 所有 的 
权重 都 设为 1 然后 每一步 所做 的 决策 是 
对 某个 权重 增加 或 减少 一个 较小 的 数值 
算法 的 伪代码 为 数据 标准化 使其 分布 满足 均值 
为 0 和 方差 为 1 在 每 轮 的 
迭代 中 设置 当前 最小 的 误差 为 正无穷 对 
每个 特征 增大 或 减小 改变 一个 系数 得到 一个 
新的 w 计算 新 w 下 的 误差 如果 误差 
小于 当前 最小 的 误差 设置 最小 误差 等于 当前 
误差 将 当前 的 w 设置 为 最优 的 w 
将 本次 迭代 得到 的 预测误差 最小 的 w 存入 
矩阵 中 返回 多次 迭代 下 的 回归系数 组成 的 
矩阵 # 前 向 逐步回归 # @ eps 每次 迭代 
需要 调整 的 步长 def stageWise xArr yArr eps = 
0.01 numIt = 100 xMat = mat xArr yMat = 
mat yArr yMean = mean yMat 0 yMat = yMat 
yMean # 将 特征 标准化 处理 为 均值 为 0 
方差 为 1 xMat = regularize xMat m n = 
shape xMat # 将 每次 迭代 中 得到 的 回归系数 
存入 矩阵 returnMat = zeros numIt m ws = zeros 
n 1 wsTest = ws . copy wsMat = ws 
. copy for i in range numIt print ws . 
T # 初始化 最小 误差 为 正无穷 lowestError = inf 
for j in range n # 对 每个 特征 的 
系数 执行 增加 和 减少 eps * sign 操作 for 
sign in 1 1 wsTest = ws . copy wsTest 
j + = eps * sign # 变化 后 计算 
相应 预测值 yTest = xMat * wsTest # 保存 最小 
的 误差 以及 对应 的 回归系数 rssE = rssError yMat 
. A yTest . A if rssE lowestError lowestError = 
rssE wsMat = wsTest ws = wsMat . copy returnMat 
i = ws return returnMat 下面 看 一次 迭代 的 
算法 执行 过程 此时 学习 步长 eps = 0.01 可以 
看到 上述 结果 中 w1 和 w6 都是 0 这 
表明 了 这 两个 维度 的 特征 对 预测 结果 
不 构成 影响 即 所谓 的 不 重要 特征 此外 
在 eps = 0.01 情况下 一段 时间 后 系数 就 
已经 饱和 并在 特 定值 之间 来回 震荡 这 显然 
是 步长 太大 的 缘故 我们 可以 据此 调整 较小 
的 步长 前 向 逐步回归 算法 也 属于 缩减 算法 
它 主要 的 优点 是 可以 帮助 我们 更好 地 
理解 现有 的 模型 并 作出 改进 当 构建 出 
一个 模型 时 可以 运行 该 算法 找出 重要 的 
特征 这样 就 可能 及时 停止 对 不 重要 特征 
的 收集 同样 在 算法 测试 的 过程 中 可以 
使用 s 折 交叉 验证 的 方法 选择 误差 最小 
的 模型 此外 当 我们 不管 是 应用 岭回归 还是 
前 向 逐步回归 等 缩减 算法 时 就 相应 的 
为 模型 增加 了 偏差 与此同时 也就 减小 了 模型 
的 方差 而/c 最优/d 的/uj 模型/n 往往/t 是/v 在/p 模型/n 
偏差/n 和/c 方差/n 的/uj 折中/v 时/n 获得/v 否则 当 模型 
方差 很 大偏差 很 小时 模型 复杂度 很大 而 出现 
过拟合 现象 而 方差 很小 偏差 很大 时而 容易 出现 
欠 拟合 现象 因此 权衡 模型 的 偏差 和 方差 
可以 做出 最好 的 预测 六 岭回归 应用 示例 预测 
乐高 玩具 套装 的 价格 我们 知道 乐高 玩具 是 
一种 拼装 类 玩具 由 很多 大小 不同 的 塑料 
插件 组成 一种 乐高 玩具 套装 基本上 在 几年 后 
就会 停产 但 乐高 收藏者 之间 仍 会在 停产 后 
彼此 交易 这样 我们 可以 拟合 一个 回归模型 从而 对 
乐高 套装 进行 估价 显然 这样 做 十分 有 意义 
算法 流程 1 收集 数据 用 google shopping 的 api 
收集 数据 2 准备 数据 从 返回 的 json 数据 
中 抽取 价格 3 分析 数据 可视化 并 观察 数据 
4 训练 算法 构建 不同 的 模型 采用 岭回归 和 
普通 线性 回归 训练 模型 5 测试 算法 使用 交叉 
验证 来 测试 不同 的 模型 选择 效果 最好 的 
模型 1 收集 数据 使用 Google 购物 的 API 来 
获取 玩具 套装 的 相关 信息 和 价格 可以 通过 
urllib2 发送 http 请求 API 将以 JSON 格式 返回 需要 
的 产品 信息 python 的 JSON 解析 模块 可以 帮助 
我们 从 JSON 格式 中 解析 出 所 需要 的 
数据 收集 数据 的 代码 如下 # 收集 数据 # 
添加 时间 函数库 from time import sleep # 添加 json 
库 import json # 添加 urllib2 库 import urllib2 # 
@ retX 样本 玩具 特征 矩阵 # @ retY 样本 
玩具 的 真实 价格 # @ setNum 获取 样本 的 
数量 # @ yr 样本 玩具 的 年份 # @ 
numPce 玩具 套装 的 零件 数 # @ origPce 原始 
价格 def searchForSet retX retY setNum yr numPce origPrc # 
睡眠 十秒 sleep 10 # 拼接 查询 的 url 字符串 
myAPIstr = get from code . google . com searchURL 
= https / / www . googleapis . com / 
shopping / search / v1 / public / products \ 
key = % s & country = US & q 
= lego + % d & alt = json % 
myAPIstr setNum # 利用 urllib2 访问 url 地址 pg = 
urllib2 . urlopen searchURL # 利用 json 打开 和 解析 
url 获得 的 数据 数据 信息 存入 字典 中 retDict 
= json . load pg . read # 遍历 数据 
的 每一个 条目 for i in range len retDict items 
try # 获得 当前 条目 currItem = retDict items i 
# 当前 条目 对应 的 产品 为 新产品 if currItem 
product condition = = new # 标记 为 新 newFlag 
= 1 else newFlag = 0 # 得到 当前目录 产品 
的 库存 列表 listOfInv = currItem product inventories # 遍历 
库存 中的 每一个 条目 for item in listOfInv # 得到 
该 条目 玩具 商品 的 价格 sellingPrice = item price 
# 价格 低于 原价 的 50% 视为 不 完整 套装 
if sellingPrice origPrc * 0.5 print % d \ t 
% d \ t % d \ t % f 
\ t % f % yr numPce newFlag \ origPrc 
sellingPrice # 将 符合 条件 套装 信息 作 为特征 存入 
数据 矩阵 retX . append yr numPce newFlag origPce # 
将 对应 套装 的 出售 价格 存入 矩阵 retY . 
append sellingPrice except print problem with item % d % 
i # 多次 调用 收集 数据 函数 获取 多组 不同 
年份 不同 价格 的 数据 def setDataCollect retX retY searchForSet 
retX retY 8288 2006 800 49.99 searchForSet retX retY 10030 
2002 3096 49.99 searchForSet retX retY 10179 2007 5195 499.99 
searchForSet retX retY 10181 2007 3428 199.99 searchForSet retX retY 
10189 2008 5922 299.99 searchForSet retX retY 10196 2009 3263 
249.99 需要 指出 的 是 我们 在 代码 中 发送 
http 请求 需要 从 NumPy 导入 urllib2 模块 使用 json 
解析 获得 的 数据 时 需要 导入 json 模块 同时 
为 避免 多个 函数 同时 访问 网站 在 程序 开始 
时先/nr 睡眠 一定 的 时间 用于 缓冲 即 需要 调用 
time 模块 的 sleep 此外 由于 套装 是由 多个 小 
插件 组成 所以 存在 插件 损失 的 情况 所以 需要 
过滤 掉 这样 的 玩具 套装 我们 可以 通过 一定 
的 过滤 条件 一般 套 装有 缺失 可能 有 标识 
可 通过 关键词 筛选 或者 通过 贝叶斯 估计 估计 套装 
完整性 这里 用 的 是 价格 来 判断 如果 当前 
价格 不到 原始 价格 的 一半 那么 这样 的 套装 
必然 有 缺陷 损坏 和 缺失 等 因为 一旦 具有 
收藏 价值 的 玩具 套装 停产 必然 价格 相对 上涨 
所以 出现 这种 价格 反常 的 情况 表明 该 产品 
套装 存在 一定 的 缺陷 可以 将其 过滤掉 2 训练 
算法 建立 模型 上 面有 了 数据 那么 我们 就 
可以 开始 完成 代码 进行 模型 训练 了 这里 采用 
岭回归 来 训练 模型 并且 采用 交叉 验证 的 方法 
来 求出 每个 λ 对应 的 测试 误差 的 均值 
最后 分析 选出 预测误差 最小 的 回归模型 # 训练 算法 
建立 模型 # 交叉 验证 测试 岭回归 # @ xArr 
从 网站 中 获得 的 玩具 套装 样本数据 # @ 
yArr 样本 对应 的 出售 价格 # @ numVal 交叉 
验证 次数 def crossValidation xArr yArr numVal = 10 # 
m n = shape xArr # xArr1 = mat ones 
m n + 1 # xArr1 1 n + 1 
= mat xArr # 获取 样本数 m = len yArr 
indexList = range m # 将 每个 回归系数 对应 的 
误差 存入 矩阵 errorMat = zeros numVal 30 # 进行 
10 折 交叉 验证 for i in range numVal trainX 
= trainY = testX = testY = # 混洗 索引 
列表 random . shuffle indexList # 遍历 每个 样本 for 
j in range m # 数据集 90% 作为 训练 集 
if j m * 0.9 trainX . append xArr1 indexList 
j trainY . append yArr indexList j # 剩余 10% 
作为 测试 集 else testX . append xArr1 indexList j 
testY . append yArr indexList j # 利用 训练 集 
计算 岭 回归系数 wMat = ridgeRegres trainX trainY # 对于 
每 一个 验证 模型 的 30组 回归系数 for k in 
range 30 # 转为 矩阵 形式 matTestX = mat testX 
matTrainX = mat trainX # 求 训练 集 特征 的 
均值 meanTrain = mean matTrainX 0 # 计算 训练 集 
特征 的 方差 varTrain = val matTrainX 0 # 岭回归 
需要 对 数据 特征 进行 标准化 处理 # 测试 集 
用 与 训练 集 相同 的 参数 进行 标准化 matTestX 
= matTestX meanTrain / varTrain # 对 每组 回归系数 计算 
测试 集 的 预测 值 yEst = matTestX * mat 
wMat k . T + mean trainY # 将 原始 
值 和 预测 值 的 误差 保存 errorMat i k 
= rssError yEst . T . A array testY # 
对 误差 矩阵 中 每个 lamda 对应 的 10次 交叉 
验证 的 误差 结果 求 均值 meanErrors = mean errorMat 
0 # 找到 最小 的 均值 误差 minMean = float 
min meanErrors # 将 均值 误差 最小 的 lamda 对应 
的 回归系数 作为 最佳 回归系数 bestWeigths = wMat nonzero meanErrors 
= = minMean xMat = mat xArr yMat = mat 
yArr . T meanX = mean xMat 0 valX = 
val xMat 0 # 数据 标准化 还原 操作 unReg = 
bestWeigths / valX print the best model from Ridge Regression 
is \ n unReg print with constant term 1 * 
sum multiply meanX unReg + mean yMat 同样 这里 需要 
说明 的 有 以下 几点 1 这里 对于 数据集 采用 
随机 的 方式 random . shffle 选取 训练 集 和 
测试 集 训练 集 占 数据 总数 的 90% 测试 
集 剩余 的 10% 采取 这种 方式 的 原因 是 
便于 我们 进行 多次 交叉 验证 得到 不同 的 训练 
集 和 测试 集 . 2 我们 知道 岭回归 中会 
选取 多个 不同 的 λ 值 来 找到 预测误差 最小 
的 模型 此外 算法 中 采用 交叉 验证 的 方法 
所以 对于 每一个 λ 对应 着 多个 测试 误差值 所以 
在 分析 预测 效果 最好 的 λ 之前 需要 先 
对 每个 λ 对应 的 多个 误差 求取 均值 3 
我们 呢 知道 岭回归 算法 需要 对 训练 集 数据 
的 每 一维 特征 进行 标准化 处理 那么 为 保证 
结果 的 准确性 也 需要 对 测试 集 进行 和 
训练 集 相同 的 标准化 操作 即 测试 集 数据 
特征 减去 训练 集 该 维度 特征 均值 再 除以 
训练 集 该 维度 特征 方差 4 因为 采用 岭回归 
算法 时 对 数据 进行 了 标准化 处理 而 标准 
的 回归 算法 则 没有 所以在 代码 最后 我们 还是 
需要 将 数据 进行 还原 这样 便于 分析 比较 二者 
的 真实 数据 的 预测误差 由 实验 结果 得到 回归方程 
上面 模型 可能 还是 不 易理解 再 看一下 具体 的 
缩减 过程 中 系数 的 变化 情况 最后 得到 的 
回归 系数 是 经过 不同 程度 的 衰减 得到 的 
比 如上图 中 第一 行 第四项 比 第二项 系数 大 
5倍 比 第一 项大/nr 57倍 依次 来看 第四 特征 可以 
看做 是 最重要 特征 在 预测 时起最/nr 主要 作用 其次 
就是 第二 特征 也 即是 特征 对应 的 系数 值 
越大 那么 其 对 预测 的 决定 作用 也 就 
越大 如果 某一 维度 系 数值 为 0 则 表明 
该 特征 在 预测 结果 中 不起作用 可以 被 视为 
不 重要 特征 所以 这种 缩减 的 分析 方法 还是 
比较 有用 的 因为 运算 这些 算法 可以 帮助 我们 
充分 理解 和 挖掘 大量 数据 中 的 内在 规律 
当 特征 数 较少 时 可能 效果 不够 明显 而 
当 特征 数 相当大 时 我们 就 可以 据此 了解 
特征 中 哪些 特征 是 关键 的 哪些 是 不 
重要 的 这就 为 我们 节省 不少 成本 和 损耗 
7 总结 1 回归 与 分类 的 区别 前者 预测 
连续型 变量 后者 预测 离散 型 变量 回 归中 求 
最佳 系数 的 方法 常用 的 是 最小化 误差 的 
平方和 如果 xTx 可逆 那么 回归 算法 可以 使用 可以 
通过 预测 值 和 原始 值 的 相关 系数 来 
度量 回归方程 的 好坏 2 当 特征 数 大于 样本 
总数 时 为 解决 xTx 不可逆 的 问题 我们 可以 
通过 引入 岭回归 来 保证 能够 求得 回归系数 3 另外 
一种 缩减 算法 是 前 向 逐步回归 算法 它 是 
一种 贪心 算法 每一步 通过 修改 某一 维度 特征 方法来 
减小 预测误差 最后 通过 多次 迭代 的 方法 找到 最小 
误差 对应 的 模型 4 缩减 法 可以 看做 是 
对 一个 模型 增加 偏差 的 同时 减少 方差 通过 
偏差 方差 折中 的 方法 可以 帮助 我们 理解 模型 
并 进行 改进 从而 得到 更好 的 预测 结果 