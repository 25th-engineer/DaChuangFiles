1 . 决策树 算法 决策树 是 一种 树形 分类 结构 
一棵 决策树 由 内部 结点 和 叶子 结点 构成 内部 
结点 代表 一个 属性 或者 一组 属性 该 结点 的 
孩子 代表 这个 属性 的 不同 取值 叶子 结点 表示 
一个 类 标 决策树 保证 每 一个 实例 都能 被 
一条 从 根结 点到 叶子 结点 的 路径 覆盖 叶子 
结点 就是 这条 实例 对应 的 类别 遍历 这条 路径 
的 过程 就是 对 这条 实例 分类 的 过程 关于 
决策树 的 详细 介绍 可以 参考 这 篇 文章 损失 
函数 假设 决策树 T 的 叶 结点 个数 为 | 
T | t 是 树 T 的 某个 叶 结点 
该 结点 有$N/nr _ t $ 个 样本点 其中 k 
类 的 样本点 有$N/nr _ { tk } $ 个 
k = 1 2 . . . K K 为 
类别 个数 $ H _ t T $ 为 叶 
结点 t 上 的 经验 熵 即从 训练 数据 算出 
的 熵 决策树 模型 对 训练 数据 的 预测误差 $ 
C T $ 定义 为 $ $ C T = 
\ sum _ { t = 1 } ^ { 
| T | } N _ tH _ t T 
= \ sum _ { t = 1 } ^ 
{ | T | } \ sum _ { k 
= 1 } ^ { K } N _ { 
tk } log \ frac { N _ { tk 
} } { N _ t } $ $ 损失 
函数 $ C _ \ alpha T $ $ C 
_ \ alpha T = C T + \ alpha 
| T | $ 参数 $ \ alpha = 0 
$ 控制 预测误差 与 模型 复杂度 的 影响 优化 目标 
在 决策树 的 构造 阶段 其 优化 目标 是 寻找 
最优 的 分裂 属性 具体 实现 是 最大化 属 性选择 
指标 包括 信息 增益 信息 增益 比率 基尼指数 等 构造 
阶段 用 贪心 策略 得到 局部 最优 的 模型 在 
剪枝 阶段 其 优化 目标 是 最小化 损失 函数 $ 
C _ \ alpha T $ 即 $ $ min 
_ TC _ a T $ $ 剪枝 阶段 是 
一个 全局 优化 的 过程 如果把 $ \ alpha $ 
设置 得 较大 则 倾向 于 选择 简单 的 树 
此时 预测误差 较大 而 泛化 能力 较好 而 如果 $ 
\ alpha $ 设置 得 较小 则 倾向 于 选择 
更 复杂 的 模型 此时 预测误差 较小 而 泛化 能力 
偏差 设置 为 0 那就 只 考虑 预测误差 即对 训练 
数据 的 拟合 程度 最高 但对 未 见过 的 数据 
分类 能力 并 不高 一个 适当 的 $ \ alpha 
$ 在 预测误差 与 模型 复杂度 泛化 能力 之间 平衡 
2 . 线性 回归 线性 回归 使用 线性 模型 拟合 
因变量 与 目标 变量 的 关系 是 最简单 的 预测模型 
假设 函数 $ $ h _ \ theta x = 
\ theta ^ Tx = \ theta _ 0x _ 
0 + \ theta _ 1x _ 1 + . 
. . + \ theta _ nx _ n $ 
$ 其中 $ x _ 0 = 1 $ 损失 
函数 $ $ C \ theta = \ frac { 
1 } { 2m } \ sum _ { i 
= 1 } ^ { m } h _ \ 
theta x ^ { i } y ^ { i 
} ^ 2 $ $ 优化 目标 选择 合适 的 
参数 组 $ \ theta $ 使得 损失 函数 最小化 
$ $ min _ \ theta C \ theta $ 
$ 优化 实现 使用 梯度 下 降法 不断 地 进行 
迭代 每一步 的 方向 是 负 梯度方向 $ $ \ 
theta _ j = \ theta _ j \ alpha 
\ frac { \ partial } { \ partial \ 
theta _ j } Cost \ theta = \ theta 
_ j \ alpha \ frac { 1 } { 
m } \ sum _ { i = 1 } 
^ m h _ \ theta x ^ { i 
} y ^ { i } x _ j ^ 
{ i } $ $ 3 . 逻辑 回归 逻辑 
回 归于 线性 回归 有着 惊人 的 相似 却又 有着 
本质 的 不同 线性 回归 的 假设 函数 度量 了 
一个 线性 模型 对 因变量 和 目标 变量 的 拟合 
即 给定 一个 输入 通过 一个 线性变换 得到 一个 输出 
而 逻辑 回归 的 假设 函数 计算 的 是 对于 
给定 输入 其 输出 y = 1 的 概率 但 
逻辑 回归 与 线性 回归 在 计算 形式上 很 相似 
常常 让 误解 为 他们 仅仅 是 假设 函数 的 
不同 假设 函数 $ $ h _ \ theta x 
= g \ theta ^ Tx = g \ theta 
_ 0x _ 0 + \ theta _ 1x _ 
1 + . . . + \ theta _ nx 
_ n $ $ 其中 $ x _ 0 = 
1 $ $ g z = \ frac { 1 
} { 1 + \ exp ^ { z } 
} $ 损失 函数 这里 的 损失 函数 不再 是 
线性 回归 时的求/nr 误差 平方和 因为 误差 平方和 不是 参数 
$ \ theta $ 的 凸函数 不 容易 求解 全局 
最优 解 因此 该用 极大 释然 估计 作为 损失 函数 
能 满足 凸函数 的 要求 $ $ C \ theta 
= \ frac { 1 } { m } \ 
sum _ { i = 1 } ^ my ^ 
{ i } log h _ \ theta x ^ 
{ i } + 1 y ^ { i } 
log 1 h _ \ theta x ^ { i 
} $ $ 优化 目标 $ $ min _ \ 
theta C \ theta $ $ 优化 实现 使用 梯度 
下 降法 不断 地 进行 迭代 每一步 的 方向 是 
负 梯度方向 $ $ \ theta _ j = \ 
theta _ j \ alpha \ frac { \ partial 
} { \ partial \ theta _ j } Cost 
\ theta = \ theta _ j \ alpha \ 
frac { 1 } { m } \ sum _ 
{ i = 1 } ^ m h _ \ 
theta x ^ { i } y ^ { i 
} x _ j ^ { i } $ $ 
注意到 在 优化 实现 上 逻辑 回归 与 线性 回归 
的 形式 一样 的 只是 具体 的 假设 函数 不同 
实际上 这 只是 一个 巧合 巧合 之处 在于 对 各自 
的 损失 函数 求 偏 导数 后 其 梯度 值 
恰好 是 假设 函数 与 y 的 表达式 但 线性 
回归 与 逻辑 回归 的 本质 是 不同 的 3 
. BP 神经网络 这篇文章 已经 介绍 了 BP 神经网络 这里 
只 是从 模型 策略 算法 这三个 层面 来 总结 一下 
BP 模型 就是指 假设 函数 策略 则指 优化 目标 算法 
即指 优化 实现 假设 函数 神经 网络 的 假设 函数 
不是 一个 简单 的 公式 它 是 多个 逻辑 回归 
函数 逐层 迭代 的 结果 形式 上 可以 写成 如下 
$ $ a ^ { 1 } = x $ 
$ $ $ a ^ { 2 } = g 
W ^ { 1 } a ^ { 1 } 
$ $ $ $ a ^ { i } = 
g W ^ { i 1 } a ^ { 
i 1 } $ $ $ $ . . . 
. $ $ $ $ a ^ { L } 
= g W ^ { L 1 } a ^ 
{ L 1 } $ $ $ $ h _ 
W x = a ^ { L } $ $ 
其中 $ L $ 表示 神经 网络 的 层数 $ 
g z = \ frac { 1 } { 1 
+ \ exp ^ { z } } $ $ 
W ^ { i 1 } $ 表示 第 i 
1层 与 第 i 层 单元 的 权值 矩阵 并且 
把 偏置 $ \ theta ^ { i } $ 
放在 了 权值 矩阵 $ W ^ { i 1 
} $ 中 损失 函数 神经 网络 的 损失 函数 
跟 逻辑 回归 非常 类似 但是 神经 网络 的 输出 
单元 可能 有 多个 需要 在 每个 输出 单元 上 
做一个 累加 $ $ Cost W = \ frac { 
1 } { m } \ sum _ { i 
= 1 } ^ m \ sum _ { k 
= 1 } ^ Ky _ k ^ { i 
} log h _ \ theta x ^ { i 
} _ k + 1 y ^ { i } 
_ k log 1 h _ \ theta x ^ 
{ i } _ k $ $ 其中 K 表示 
输出 层 神经 单元 的 个数 m 表示 训练 数据 
实例 个数 优化 目标 各个 算法 的 优化 目标 基本 
上都 是 寻求 适当 的 参数 使得 损失 函数 最小 
$ $ min _ W C \ theta $ $ 
优化 实现 BP 神经网络 利用 反向 传播 逐层 采样 梯度 
下降 4 . k 近邻 损失 函数 knn 损失 函数 
为 0 1 损失 函数 假设 给定 一个 实例 x 
其 K 个 最 近邻 训练 实例 点 构成 的 
集合 是 $ N _ k x $ $ $ 
cost f = \ frac { 1 } { k 
} \ sum _ { x _ i \ in 
N _ k x } I y _ i \ 
ne c _ j = 1 \ frac { 1 
} { k } \ sum _ { x _ 
i \ in N _ k } x I y 
_ i = c _ j $ $ 优化 目标 
$ $ min   cost = = min   1 
\ frac { 1 } { k } \ sum 
_ { x _ i \ in N _ k 
x } I y _ i = c _ j 
= max \ frac { 1 } { k } 
\ sum _ { x _ i \ in N 
_ k x } I y _ i = c 
_ j $ $ 