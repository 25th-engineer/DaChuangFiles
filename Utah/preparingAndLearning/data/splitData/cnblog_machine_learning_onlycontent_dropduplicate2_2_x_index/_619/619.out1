机器 不 学习 jqbxx . com 专注 机器学习 深度 学习 
自然语言 处理 大 数据 个性化 推荐 搜索算法 知识图谱 虽然 我 
不是 专门 研究 迁移 学习 的 但是 作为 一个 AI 
研究者 就如 题图 吴 老师 所说 迁移 学习 极为重要 是 
必须 要 学习 的 今天 就 先 总结 介绍 一些 
迁移 学习 的 基础 知识 目录 如下 迁移 学习 一些 
概念 迁移 学习 简介 迁移 学习 的 分类 迁移 学习 
热门 研究 方向 迁移 学习 一些 概念 在 文章 的 
一开始 先来 学习 迁移 学习 一些 概念 域 一个 域 
D 由 一个 特征 空间 X 和 特征 空间 上 
的 边际 概率分布 P X 组成 其中 X = x1 
x2 . . . xn 举个 例子 对于 一个 有 
文档 其 有 很多 词 袋 表征 bag of words 
representation X 是 所有 文档 表征 的 空间 而 xi 
是 第 i 个 单词 的 二进制 特征 P X 
代表 对 X 的 分布 任务 在给 定 一个 域 
D = { X P X } 之后 一个 任务 
T 由 一个 标签 空间 y 以及 一个 条件 概率分布 
P Y / X 构成 其中 这个 条件 概率分布 通常 
是从 由 特征 标签 对 xi yi 组成 的 训练 
数据 中 学习 得到 源 域 source domain 目标 域 
target domain 在 迁移 学习 中 我们 已有 的 知识 
叫做 源 域 source domain 要 学习 的 新知识 叫 
目标 域 target domain 负 迁移 指 的 是 在 
源 域 上 学习 到 的 知识 对于 目标 域 
上 的 学习 产生 负面 作用 产生 负 迁移 的 
原因 主要 有 两个 一个 是 源 域 和 目标 
域 的 相似 度 很低 无法 做 迁移 另 一个 
是 虽 数据 问 源 域 和 目标 域 是 
相似 的 但是 迁移 学习 方法 不够好 没找到 可 迁移 
的 成分 导致 负 迁移 迁移 学习 简介 先 举 
几个 例子 比如 我们 已经 会 编写 Java 程序 就 
可以 类 比着 来 学习 C + + 都是 面向对象 
的 语言 就 很快 学会 了 或者 在 学会 骑 
自行车 之后 骑 摩托车 也 自己 比较 容易 了 因为 
这 两种 交通 工具 有 许多 相似之处 总结 起来 用 
成语 来说 迁移 学习 就是 举一反三 再 来个 图示 如 
下左图 传统 机器学习 对 不同 的 学习 任务 需要 建立 
不同 的 模型 学习 不同 的 参数 而 对于 迁移 
学习 右图 只 需要 利用 源 域中 的 数据 将 
知识 迁移 到 目标 域 就能 完成 模型 建立 迁移 
学习 的 严格 定义 给定 源 域 Ds = { 
Xs Fs X } 和 学习 任务 Ts 目标 域 
DT = { Xt Ft X } 和 学习 任务 
Tt 迁移 学习 旨在 源 域 不同于 目标 域 或 
学习 任务 Tt 不同 于 学习 任务 Ts 的 条件 
下 通过 使用 学习 任务 Ts 和源域/nr Ds = { 
Xs Fs X } 所 获取 的 知识 来 帮助 
学习 目标 的 在 目标 域 Dt 的 预测 函数 
Ft . 为什么 需要 进行 迁移 学习 数据 的 标签 
很难 获取 当 有些 任务 的 数据 标签 很难 获取 
时 就 可以 通过 其他 容易 获取 标签 且 和该/nr 
任务 相似 的 任务 来 迁移 学习 从头 建立 模型 
是 复杂 和 耗时 的 也即 是 需要 通过 迁移 
学习 来 加快 学习 效率 和 一起 相关 领域 的 
辨析 多任务 学习 区别 在于 在 迁移 学习 中 我们 
主要 关心 在 我们 的 目标 任务 和域上/nr 的 表现 
而 多任务 学习 中 的 目标 是 在 所有 可用 
的 任务 上 都要 表现 良好 尽管 某个 标签 数据 
通常 都被 假定 在 一个 任务 上 当然 现在 迁移 
学习 和 多任务 学习 也 并 没有 很大 的 区别 
比如 归纳 式 迁移 学习 中 当 两个 域 都有 
标签 的 时候 这 就与 多任务 学习 相似 持续 学习 
虽然 多 任务 学习 允许 我们 在 许多 任务 中 
保留 知识 而 不会 对 我们 的 源 任务 造成 
性能 损失 但 只有 在 所有 任务 都 处于 训练 
时间 的 情况 下 这才 是 可能 的 对于 每个 
新 任务 我们 通常 需要 重新 训练 我们 所有 任务 
的 模型 然而 在 现实 世界 中 我们 希望 一个 
代理 能够 通过 使用 它 以往 的 一些 经验 来 
处理 逐渐 变得 复杂 的 任务 为了 达到 这个 目的 
我们 需要 让 一个 模型 在 不 忘记 的 情况 
下 持续 地 学习 这个 机器 学习 的 领域 被 
称为 学会 学习 元 学习 meta learning 终生 学习 或者 
持续 学习 持续 学习 在 最近 的 强化 学习 强化 
学习 以 Google DeepMind 对 通用 学习 代理 的 探索 
而 著称 上 已经 取得 了 成功 也 正在 被 
用于 序 列到 序列 的 模型 上 最近 元 学习 
meta learning 也挺 火爆 的 再来 举个 网上 的 例子 
我们 都 知道 在 金庸 的 武侠 世界 中 有 
各种各样 的 武功 不同 的 武功 都 不一样 有 内功 
也有 外功 那么 里面 的 张无忌 就 特别 厉害 因为 
他 练 成了 九阳 神功 有了 九阳 神功 张无忌 学习 
新的 武功 就 特别 快 在 电影 倚天 屠龙记 之 
魔教 教主 中 张无忌 分分钟 学会 了 张三丰 的 太极拳 
打败 了 玄冥 二老 九阳 神功 就是 一种 学会 学习 
的 武功 我们 希望 神经 网络 也 能 学会 学习 
这样 也 就能 快速 学习 啦 zero shot 学习 如果 
我们 把 迁移 学习 使用 到 极限 并且 想要 仅仅 
从 很少 的 实例 中 学习 这就 分别 得到 了 
few shot one shot 以及 zero shot 学习 让 模型 
执行 one shot 和 zero shot 学习 无疑 属于 机器 
学习 中 最 艰难 的 问题 而 另一方面 这却 是 
我们 人类 天生 就会 的 幼年 的 时候 为了 让 
我们 能够 认出 任何 狗狗 我们 仅仅 需要 被 告知 
一次 「 这是 一条 狗 」 然而 成年人 可以 仅 
通过 在 文中 阅读 就 理解 一个 东西 的 本质 
不 需要 事先 见过 它 one shot 学习 的 新进展 
利用 了 这样 的 思想 即 为了 在 测试 的 
时候 实现 好 的 性能 模型 需要 显 式 地被 
训练 从而 进行 one shot 学习 但 更加 逼真 具有 
概括性 的 zero shot 学习 设置 在 最近 已经 引起 
了 注意 在 零点 学习 中 训练 类别 出现 在 
测试 的 时候 以上 就是 一些 相关 领域 当然 这里 
只是 很 简单 的 介绍 有 一个 概念 详细 请 
自行 了解 迁移 学习 的 分类 按 迁移 情景 分 
归纳 式 迁移 学习 Inductive TL 源 域 和 目标 
域 的 学习 任务 不同 直 推 式 迁移 学习 
Transductive TL 源 域 和 目标 域 不同 学习 任务 
相同 无 监督 迁移 学习 Unsupervised TL 源/ng 域/n 和/c 
目标/n 域/n 均/d 没有/v 标签/n 根据/p 源/ng Domain/w 和/c 目前/t 
Domain/w 之间 的 关系 源 Task 和 目标 Task 之间 
的 关系 以及 任务 方法 更 详细 的 整理 为 
下表 按 迁移 学习 的 基本 方法 分 基于 实例 
的 迁移 学习 方法 在 源 域中 找到 与 目标 
域 相似 的 数据 把 这个 数据 的 权值 进行 
调整 使得 新 的 数据 与 目标 域 的 数据 
进行 匹配 然后 进行 训练 学习 得到 适用 于 目标 
域 的 模型 这样 的 方法 优点 是 方法 简单 
实现 容易 缺点 在于 权重 的 选择 与 相似 度 
的 度量 依赖 经验 且 源 域 与 目标 域 
的 数据 分布 往往 不同 基于 特征 的 迁移 学习 
方法 当 源 域 和 目标 域 含有 一些 共同 
的 交叉 特征 时 我们 可以 通过 特征 变换 将 
源 域 和 目标 域 的 特征 变换 到 相同 
空间 使得 该 空间 中 源 域 数据 与 目标 
域 数据 具有 相同 分布 的 数据 分布 然后 进行 
传统 的 机器学习 优点 是 对 大多数 方法 适用 效果 
较好 缺点 在于 难于 求解 容易 发生 过 适配 需要/v 
注意/v 的/uj 的/uj 是/v 基于/p 特征/n 的/uj 迁移/v 学习/v 方法/n 
和/c 基于/p 实例/n 的/uj 迁移/v 学习/v 方法/n 的/uj 不同/a 是/v 
基于/p 特征/n 的/uj 迁移/v 学习/v 需要/v 进行/v 特征/n 变/v 换来/v 
使得/v 源/ng 域/n 和/c 目标/n 域/n 数据/n 到/v 到/v 同一/b 
特征/n 空间/n 而 基于 实例 的 迁移 学习 只是 从 
实际 数据 中 进行 选择 来 得到 与 目标 域 
相似 的 部分 数据 然后 直接 学习 基于 模型 的 
迁移 学习 方法 源 域 和 目标 域 共享 模型 
参数 也 就是 将 之前 在 源 域中 通过 大量 
数据 训 练好 的 模型 应用 到 目标 域 上 
进行 预测 基于 模型 的 迁移 学习 方法 比较 直接 
这样 的 方法 优点 是 可以 充分 利用 模型 之间 
存在 的 相似性 缺点 在于 模型 参数 不易 收敛 举个 
例子 比如 利用 上 千万 的 图象 来 训练 好一个 
图象识别 的 系统 当 我们 遇到 一个 新的 图象 领域 
问题 的 时候 就 不用 再 去找 几 千万个 图象 
来 训练 了 只需 把 原来 训 练好 的 模型 
迁移 到 新的 领域 在 新的 领域 往往 只需 几 
万张 图片 就够 同样 可以 得到 很高 的 精度 基于 
关系 的 迁移 学习 方法 当 两个 域 是 相似 
的 时候 那么 它们 之间 会 共享 某种 相似 关系 
将 源 域中 学习 到 的 逻辑 网络 关系 应用 
到 目标 域 上来 进行 迁移 比方说 生物病毒 传播 规律 
到 计算机 病毒 传播 规律 的 迁移 这 部分 的 
研究 工作 比较 少 典型 方法 就是 mapping 的 方法 
看 一个 图 来 总结 以上 的 知识 可以 看成 
归纳 式 迁移 学习 是 最 广泛 应用 的 按 
特征 空间 分 同构 迁移 学习 Homogeneous TL 源 域 
和 目标 域 的 特征 维度 相同 分布 不同 异构 
迁移 学习 Heterogeneous TL 源 域 和 目标 域 的 
特征 空间 不同 以 下图 是 做 迁移 学习 分类 
的 一个 梳理 迁移 学习 热门 研究 方向 域 适配 
问题 domain adaptation 有/v 标签/n 的/uj 源/ng 域/n 和无/nr 标签/n 
的/uj 目标/n 域/n 共享/v 相同/d 的/uj 特征/n 和/c 类别/n 但是 
特征 分布 不同 如何 利用 源 域 标定 目标 域 
解决 Domain adaptation 问题 主要 的 思路 就是 将 source 
训 练好 的 模型 能够 用在 target 上 而 域 
适配 问题 最 主要 的 也 就是 如何 减少 source 
域 和 target 域 不同 分布 之间 的 差异 代表性 
论文 有 Domain adaptation via transfer component analysis 基于 特征 
的 迁移 方法 Density ratio estimation in machine learning 基于 
实例 的 迁移 方法 Cross domain video concept detection using 
adaptive svms 基于 模型 的 迁移 方法 等 等 最近 
的 进展 也有 Wasserstein Distance Guided Representation Learning for Domain 
Adaptation 等 用 W GAN 来做 domain adaptation 可以 一看 
多源 迁移 学习 multi source TL 多个 源 域 和 
目标 域 通过 进行 有效 的 域 筛选 从而 进行 
迁移 多源 迁移 学习 可以 有效 利用 存在 的 多个 
可用 域 综合 起来 进行 迁移 达到 较好 的 效果 
当然 现在 如何 衡量 多个 域 之间 的 相关性 和 
多个 域 的 利用 方法 还是 一个 比 较大 的 
问题 代表性 论文 有 Boosting for transfer learning Multi source 
transfer learning with multi view adaboost 等等 深度 迁移 学习 
deep TL 特别 是 近年来 由于 深度 学习 的 火爆 
越来越 多 研究 者 利用 深度 神经 网络 的 结构 
进行 迁移 学习 深度 学习 可以 深度 表征 域中 的 
知识 结构 也 大大 增强 了 模型 的 泛化 能力 
可以 说 利用 深度 学习 做 迁移 学习 的 前景 
还是 很好 的 代表性 论文 有 Simultaneous deep transfer across 
domains and tasks Multi source transfer learning with multi view 
adaboost Learning Transferable Features with Deep Adaptation Networks 等等 更多 
精彩 内容 机器 不 学习 官方网站 jqbxx . com 