一 引言 本 材料 参考 Andrew Ng 大神 的 机器学习 
课程   http / / cs229 . stanford . edu 
在上 一篇 有 监督 学习 回归模型 中 我们 利用 训练 
集 直接 对 条件概率 p y | x θ 建模 
例如 logistic 回归 就 利用 h θ x = g 
θ Tx 对 p y | x θ 建模 其中 
g z 是 sigmoid 函数 假设 现在 有 一个 分 
类 问题 要 根据 一些 动物 的 特征 来 区分 
大象 y = 1 和狗/nr y = 0 给定 这样 
的 一种 数据集 回归模型 比如 logistic 回归 会 试图 找到 
一条 直线 也 就是 决策 边界 来 区分 大象 与 
狗 这 两类 然后 对于 新来 的 样本 回归模型 会 
根据 这个 新 样本 的 特征 计算 这 个 样本 
会 落在 决策 边界 的 哪 一边 从而 得到 相应 
的 分类 结果 现在 我们 考虑 另外 一种 建模 方式 
首先 根据 训练 集中 的 大象 样本 我们 可以 建立 
大象 模型 根据 训练 集中 的 狗 样本 我们 可以 
建立 狗 模型 然后 对于 新来 的 动物 样本 我们 
可以 让 它 与 大象 模型 匹配 看 概率 有 
多少 与 狗 模型 匹配 看 概率 有 多少 哪一个 
概率 大 就是 那个 分类 判别式 模型 Discriminative Model 是 
直接 对 条件概率 p y | x θ 建模 常见 
的 判别式 模型 有 线性 回归模型 线性 判别分析 支持 向量 
机 SVM 神经 网络 等 生成式 模型 Generative Model 则 
会对 x 和y的/nr 联合 分布 p x y 建模 然后 
通过 贝叶斯 公式 来 求得 p yi | x 然后 
选取 使得 p yi | x 最大 的 yi 即 
常见 的 生成式 模型 有 隐 马尔可夫 模型 HMM 朴素 
贝叶斯 模型 高斯 混合模型 GMM LDA 等 二 高斯 判别分析 
Gaussian Discriminant Analysis 高斯 判别分析 GDA 是 一种 生成式 模型 
在 GDA 中 假设 p x | y 满足 多值 
正态分布 多值 正态分布 介绍 如下 2.1 多值 正态分布   multivariate 
normal distribution 一个 n 维 的 多值 正态分布 可以 表示 
为多 变量 高斯分布 其 参数 为 均值 向量 协方差 矩阵 
其 概率密度 表示 为 当 均值 向量 为 2 维 
时 概率密度 的 直观 表示 左边 的 图 表示 均值 
为 0 协方差 矩阵 ∑ = I 中间 的 图 
表示 均值 为 0 协方差 矩阵 ∑ = 0.6 I 
右边 的 图 表示 均值 为 0 协方差 矩阵 ∑ 
= 2I 可以 观察到 协方差 矩阵 越大 概率分布 越 扁平 
协方差 矩阵 越小 概率分布 越高 尖 2.2 高斯 判别分析 模型 
如果 有 一个 分 类 问题 其 训练 集 的 
输入 特征 x 是 随机 的 连续 值 就 可以 
利用 高斯 判别分析 可以 假设 p x | y 满足 
多值 正态分布 即 该 模型 的 概率分布 公式 为 模型 
中 的 参数 为 Φ Σ μ 0 和μ1/nr 于是 
似 然 函数 x 和y的/nr 联合 分布 为 其中 Φ 
是 y = 1 的 概率 Σ 是 协方差 矩阵 
μ 0 是 y = 0 对应 的 特征 向量 
x 的 均值 μ 1 是 y = 1 对应 
的 特征 向量 x 的 均值 于是 得到 它们 的 
计算 公式 如下 于是/nr 这样 就 可以 对 p x 
y 建模 从而 得到 概率 p y = 0 | 
x 与 p y = 1 | x 从而 得到 
分类 标签 其 结果 如下 图 所示 三 朴素 贝叶斯 
模型 在 高斯 判别分析 GDA 中 特征向量 x 是 连续 
实数值 如果 特征向量 x 是 离散 值 可以 利用 朴素 
贝叶斯 模型 3.1 垃圾邮件 分类 假设 我们 有 一个 已 
被 标记 为 是否 是 垃圾 邮件 的 数据集 要 
建立 一个 垃圾 邮件 分类器 用 一种 简单 的 方式 
来 描述 邮件 的 特征 有 一本 词典 如果 邮件 
包含 词典 中的 第 i 个 词 则 设 xi 
= 1 如果 没有 这个 词 则 设 xi = 
0 最后 会 形成 这样 的 特征向量 x 这个 特征向量 
表示 邮件 包含 单词 a 和 单词 buy 但是 不 
包含 单词 aardvark aardwolf zygmurgy 特征向量 x 的 维数 等于 
字典 的 大小 假设 字典 中有 5000个 单词 那么 特征向量 
x 就为 5000 维 的 包含 0/1 的 向量 如果 
我们 建立 多项式 分布 模型 那么 有 25000中 输出 结果 
这 就 意味着 有 接近 25000个 参数 这么 多 的 
参数 要 建模 很 困难 因此 为了 建模 p x 
| y 必须 做出 强 约束 假设 这里 假设 对于 
给定 的 y 特征 x 是 条件 独立 的 这个 
假设 条件 称为 朴素 贝叶斯 假设 得到 的 模型 称为 
朴素 贝叶斯 模型 比如 如果 y = 1 表示 垃圾邮件 
其中 包含 单词 200 buy 以及 单词 300 price 那么 
我们 假设 此时 单词 200 buy x200 单词 300 price 
x300   是 条件 独立 的 可以 表示 为 p 
x200 | y = p x200 | y x300 注意 
这个 假设 与 x200 与 x300 独立 是 不同 的 
x200 与 x300 独立 可以 写作 p x200 = p 
x200 | x300 这个 假设 是 对于 给定 的 y 
x200 与 x300 是 条件 独立 的 因此 利用 上述 
假设 根据 链式法则 得到 该 模型 有 3个 参数   
那么 根据 生成式 模型 的 规则 我们 要 使 联合 
概率 最大 根据 这 3个 参数 意义 可以 得到 它们 
各自 的 计算 公式 这样 就 得到 了 朴素 贝叶斯 
模型 的 完整 模型 对于 新来 的 邮件 特征向量 x 
可以 计算 实际上 只 要 比较 分子 就行了 分母 对于 
y = 0 和y/nr = 1 是 一样 的 这时 
只 要 比较 p y = 0 | x 与 
p y = 1 | x 哪个 大 就 可以 
确定 邮件 是否 是 垃圾 邮件 3.2 拉普拉斯 平滑 朴素 
贝叶斯 模型 可以 在 大部分 情况 下 工作 良好 但是 
该 模型 有 一个 缺点 对 数据 稀疏 问题 敏感 
比如 在 邮件 分类 中 对于 低年级 的 研究生 NIPS 
显得 太 过于 高大 上 邮件 中 可能 没有 出现 
过 现在 新来 了 一个 邮件 NIPS call for papers 
假设 NIPS 这个词 在 词典 中 的 位置 为 35000 
然而 NIPS 这个词 从来 没有 在 训练 数据 中 出现 
过 这 是 第一 次 出现 NIPS 于是 算 概率 
时 由于 NIPS 从未 在 垃圾 邮件 和 正常 邮件 
中 出现 过 所以 结果 只能 是 0 了 于是 
最后 的 后验/nr 概率 对于 这样 的 情况 我们 可以 
采用 拉普拉斯 平滑 对于 未 出现 的 特征 我们 赋予 
一个 小 的 值 而不是 0 具体 平滑 方法 为 
假设 离散 随机变量 取值 为 { 1 2 k } 
原来 的 估计 公式 为 使用 拉普拉斯 平滑 后 新的 
估计 公式 为 即 每个 k 值 出现 次数 加 
1 分母 总的 加 k 类似于 NLP 中的 平滑 具体 
参考 宗 成庆 老师 的 统计 自然语言 处理 一 书 
对于 上述 的 朴素 贝叶斯 模型 参数 计算公式 改为 