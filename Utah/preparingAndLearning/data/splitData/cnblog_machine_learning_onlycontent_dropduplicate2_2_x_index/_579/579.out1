最近 太忙 已经 好 久 没有 写 博客 了 今天 
整理 分享 一篇 关于 损失 函数 的 文章 吧 以前 
对 损失 函数 的 理解 不够 深入 没有 真正 理解 
每个 损失 函数 的 特点 以及 应用 范围 如果 文中 
有 任何 错误 请 各位 朋友 指教 谢谢 ~ 损失 
函数 loss function 是 用来 估量 模型 的 预测 值 
f x 与 真实 值 Y 的 不一致 程度 它 
是 一个 非 负 实值函数 通常 使用 L Y f 
x 来 表示 损失 函数 越小 模型 的 鲁棒性 就 
越好 损失 函数 是 经验 风险 函数 的 核心 部分 
也是 结构 风险 函数 重要 组成部分 模型/n 的/uj 结构/n 风险/n 
函数/n 包括/v 了/ul 经验/n 风险/n 项和/nr 正则/n 项/n 通常 可以 
表示 成 如下 式子 其中 前面 的 均值 函数 表示 
的 是 经验 风险 函数 L 代表 的 是 损失 
函数 后面 的 Φ 是 正则化 项 regularizer 或者 叫 
惩罚 项 penalty term 它 可以 是 L1 也 可以 
是 L2 或者 其他 的 正则 函数 整个 式子 表示 
的 意思 是 找到 使 目标函数 最 小时 的 θ 
值 下面 主要 列出 几种 常见 的 损失 函数 理解 
损失 函数 旨在 表示出 logit 和 label 的 差异 程度 
不同 的 损失 函数 有 不同 的 表示 意义 也 
就是 在 最小化 损失 函数 过程 中 logit 逼近 label 
的 方式 不同 得到 的 结果 可能 也 不同 一般 
情况 下 softmax 和 sigmoid 使用 交叉 熵 损失 logloss 
hingeloss 是 SVM 推导 出 的 hingeloss 的 输入 使用 
原始 logit 即可 一 LogLoss 对数 损失 函数 逻辑 回归 
交叉 熵 损失 有些 人 可能 觉得 逻辑 回归 的 
损失 函数 就是 平方 损失 其实 并 不是 平方 损失 
函数 可以 通过 线性 回归 在 假设 样本 是 高斯分布 
的 条件 下 推导 得到 而 逻辑 回归 得到 的 
并 不是 平方 损失 在 逻辑 回归 的 推导 中 
它 假设 样本 服从 伯努利 分布 0 1 分布 然后 
求得 满足 该 分布 的 似 然 函数 接着 取 
对数 求 极值 等等 而 逻辑 回归 并 没有 求 
似 然 函数 的 极值 而是 把 极大化 当做 是 
一种 思想 进而 推 导出 它 的 经验 风险 函数 
为 最小化 负 的 似 然 函数 即 max F 
y f x min F y f x 从 损失 
函数 的 视角 来看 它 就 成了 log 损失 函 
数了 log 损失 函数 的 标准 形式 刚刚 说到 取 
对数 是 为了 方便 计算 极大 似 然 估计 因为 
在 MLE 最大 似 然 估计 中 直接 求导 比较 
困难 所以 通常 都是 先取 对数 再 求导 找 极值 
点 损失 函数 L Y P Y | X 表达 
的 是 样本 X 在 分类 Y 的 情况 下 
使 概率 P Y | X 达到 最大值 换言之 就是 
利用 已知 的 样本分布 找到 最 有可能 即 最大 概率 
导致 这种 分布 的 参数值 或者 说 什么样 的 参数 
才能 使 我们 观测 到 目前 这 组 数据 的 
概率 最大 因为 log 函数 是 单调 递增 的 所以 
logP Y | X 也会 达到 最大值 因此 在 前面 
加上 负号 之后 最大化 P Y | X 就 等价 
于 最小化 L 了 逻辑 回归 的 P Y = 
y | x 表达式 如下 为了 将 类别 标签 y 
统一 为 1 和0/nr 下面 将 表达式 分开 表示 将 
它 带入 到 上式 通过 推导 可以 得到 logistic 的 
损失 函数 表达式 如下 逻辑 回归 最后 得到 的 目标 
式子 如下 上面 是 针对 二 分类 而言 的 这里 
需要 解释 一下 之所以 有人 认为 逻辑 回归 是 平方 
损失 是 因为 在 使用 梯度 下降 来 求 最优 
解的/nr 时候 它 的 迭代 式子 与 平方 损失 求导 
后的/nr 式子 非常 相似 从而 给 人 一种 直 观上 
的 错觉 这里 有个 PDF 可以 参考 一下 Lecture 6 
logistic regression . pdf . 注意 softmax 使用 的 即为 
交叉 熵 损失 函数 binary _ cossentropy 为 二 分类 
交叉 熵 损失 categorical _ crossentropy 为多 分类 交叉 熵 
损失 当 使用 多 分类 交叉 熵 损失 函数 时 
标签 应该 为多 分类 模式 即使 用 one hot 编码 
的 向量 二 平方 损失 函数 最小二乘 法 Ordinary Least 
Squares 最小二乘 法是/nr 线性 回归 的 一种 最小二乘 法 OLS 
将 问题 转化成 了 一个 凸 优化 问题 在 线性 
回 归中 它/r 假设/vn 样本/n 和/c 噪声/n 都/d 服从/v 高斯分布/nr 
为什么 假 设成 高斯分布 呢 其实 这里 隐藏 了 一个 
小 知识 点 就是 中心 极限 定理 可以 参考 central 
limit theorem 最后 通过 极大 似 然 估计 MLE 可以 
推 导出 最小二乘 式子 最小二乘 的 基本 原则 是 最优 
拟合 直线 应该是 使 各点 到 回归 直线 的 距离 
和 最小 的 直线 即 平方和 最小 换言之 OLS 是 
基于 距离 的 而 这个 距离 就 是 我们 用 
的 最多 的 欧几里得 距离 为什么 它 会 选择 使用 
欧式 距离 作为 误差 度量 呢 即 Mean squared error 
MSE 主要 有 以下 几个 原因 简单 计算 方便 欧氏距离 
是 一种 很好 的 相似性 度量 标准 在 不同 的 
表示 域 变换 后 特征 性质 不变 平方 损失 Square 
loss 的 标准 形式 如下 当 样本 个数 为 n 
时 此时 的 损失 函数 变为 Y f X 表示 
的 是 残差 整个 式子 表示 的 是 残差 的 
平方和 而 我们 的 目的 就是 最小化 这个 目标 函数值 
注 该 式子 未 加入 正则 项 也 就是 最小化 
残差 的 平方和 residual sum of squares RSS 而在 实际 
应用 中 通常 会 使用 均方差 MSE 作为 一项 衡量 
指标 公式 如下 上面 提到 了 线性 回归 这里 额外 
补充 一句 我们 通常 说 的 线性 有 两种 情况 
一种 是 因变量 y 是 自变量 x 的 线性函数 一种 
是 因变量 y 是 参数 α 的 线性函数 在 机器 
学习 中 通常指 的 都是 后 一种 情况 三 指数 
损失 函数 Adaboost 学过 Adaboost 算法 的 人都 知道 它 
是 前 向 分步 加法 算法 的 特例 是 一个 
加 和 模型 损失 函数 就是 指数函数 在 Adaboost 中 
经过 m 此 迭代 之后 可以 得到 fm x Adaboost 
每次 迭 代时 的 目的 是 为了 找到 最小化 下 
列式 子时 的 参数 α   和G/nr 而 指数 损失 
函数 exp loss 的 标准 形式 如下 可以 看出 Adaboost 
的 目标 式子 就是 指数 损失 在 给定 n 个 
样本 的 情况 下 Adaboost 的 损失 函数 为 关于 
Adaboost 的 推导 可以 参考 Wikipedia AdaBoost 或者 统计 学习 
方法 P145 . 四 Hinge 损失 函数 SVM 在 机器学习 
算法 中 hinge 损失 函数 和 SVM 是 息息相关 的 
在 线性 支持 向量 机中 最优化 问题 可以 等价 于 
下列 式子 下面 来 对 式子 做个 变形 令 于是 
原式 就 变成 了 如若 取 λ = 1 / 
2C 式子 就 可以 表示 成 可以 看出 该 式子 
与 下式 非常 相似 前半部 分 中的   l   
就是 hinge 损失 函数 而 后面 相当于 L2 正则 项 
Hinge 损失 函数 的 标准 形式 可以 看出 当 | 
y | = 1时 L y = 0 更多 内容 
参考 Hinge loss 补充 一下 在 libsvm 中 一共 有4/nr 
中核 函数 可以 选择 对应 的 是 t 参数 分别 
是 0 线性 核 1 多项式 核 2 RBF 核 
3 sigmoid 核 五 其它 损失 函数 除了 以上 这 
几种 损失 函数 常用 的 还有 0 1 损失 函数 
绝对值 损失 函数 下面 来 看看 几种 损失 函数 的 
可视化 图像 对着 图 看看 横坐标 看看 纵坐标 再 看看 
每条 线 都 表示 什么 损失 函数 多看 几次 好好 
消化 消化 六 Keras / TensorFlow 中 常用 Cost Function 
总结 mean _ squared _ error 或 msemean _ absolute 
_ error 或 maemean _ absolute _ percentage _ error 
或 mapemean _ squared _ logarithmic _ error 或 mslesquared 
_ h i n g e h i n g 
e c a t e g o r i c 
a l _ hingebinary _ crossentropy 亦 称作 对数 损失 
logloss l o g c o s h c a 
t e g o r i c a l _ 
crossentropy 亦 称作 多类 的 对数 损失 注意 使用 该 
目标函数 时 需要 将 标签 转化 为 形如 nb _ 
samples nb _ classes 的 二 值 序列 sparse _ 
categorical _ crossentrop 如上 但 接受 稀疏 标签 注意 使用 
该 函数 时 仍然 需要 你 的 标签 与 输出 
值 的 维度 相同 你 可能 需要 在 标签 数据 
上 增加 一个 维度 np . expand _ dims y 
1 kullback _ leibler _ divergence 从 预测值 概率分布 Q 
到 真值 概率分布 P 的 信息 增益 用以 度量 两个 
分布 的 差异 . poisson 即 predictions targets * log 
predictions 的 均值 cosine _ proximity 即 预测值 与 真实 
标签 的 余弦 距离 平均值 的 相反数 需要 记住 的 
是 参数 越多 模型 越 复杂 而/c 越/d 复杂/a 的/uj 
模型/n 越/d 容易/a 过拟合/i 过拟合 就是说 模型 在 训练 数据 
上 的 效果 远远 好于 在 测试 集上 的 性能 
此时 可以 考虑 正则化 通过 设置 正则 项 前面 的 
hyper parameter 来 权衡 损失 函数 和 正则 项 减小 
参数 规模 达到 模型简化 的 目的 从而 使 模型 具有 
更好 的 泛化 能力 