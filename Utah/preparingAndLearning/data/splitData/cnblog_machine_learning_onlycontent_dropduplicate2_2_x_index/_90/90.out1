KNN 分类 算法 是 理论 上 比较 成熟 的 方法 
也是 最 简单 的 机器学习 算法 之一 该 方法 的 
思路 是 如果 一个 样本 在 特征 空间 中的 k 
个 最 相似 即 特征 空间 中最 邻近 的 样本 
中 的 大多数 属于 某 一个 类别 则 该 样本 
也 属于 这个 类别 KNN 算法 中 所 选择 的 
邻居 都是/nr 已经 正确 分类 的 对象 该 方法 在 
定 类 决策 上 只 依据 最 邻近 的 一个 
或者 几个 样本 的 类别 来 决定 待 分 样本 
所属 的 类别 一个 对于 KNN 算法 解释 最 清楚 
的 图 如下 所示 蓝/nr 方块/n 和红/nr 三角/m 均/d 是/v 
已有/v 分类/n 数据/n 当前 的 任务 是 将 绿色 圆 
块 进行 分类 判断 判断 是 属于 蓝方 块 或者 
红 三角 当然 这里 的 分类 还跟 K 值 是 
有关 的 如果 K = 3 实 线圈 红 三角 
占 比 2/3 则 判断 为 红 三角 如果 K 
= 5 虚 线圈 蓝 方块 占 比 3/5 则 
判断 为 蓝 方块 由此 可以 看出 knn 算法 实际上 
根本 就 不用 进行 训练 而是 直接 进行 计算 的 
训练 时间 为 0 计算 时间 为 训练 集 规模 
n knn 算法 的 基本 要素 大致 有 3个 1 
K 值 的 选择 2 距离 的 度量 3 分类 
决策 规则 使用 方式 转载 K 值 会对 算法 的 
结果 产生 重大 影响 K 值 较小 意味着 只有 与 
输入 实例 较近 的 训练 实例 才 会对 预测 结果 
起作用 容易 发生 过 拟合 如果 K 值 较大 优点 
是 可以 减少 学习 的 估计误差 缺点 是 学习 的 
近似 误差 增大 这 时与 输入 实例 较远 的 训练 
实例 也 会对 预测 起作用 是 预测 发生 错误 在 
实际 应用 中 K 值 一般 选择 一个 较小 的 
数值 通常 采用 交叉 验证 的 方法 来 选择 最 
有的 K 值 随着 训练 实例 数目 趋向于 无穷 和 
K = 1 时 误差率 不会 超过 贝叶斯 误差率 的 
2倍 如果 K 也 趋向于 无穷 则 误差率 趋向于 贝叶斯 
误差率 算法 中的 分类 决策 规则 往往 是 多数 表决 
即由 输入 实例 的 K 个 最 临近 的 训练 
实例 中的 多数 类 决定 输入 实例 的 类别 距离 
度量 一般 采用 Lp 距离 当 p = 2时 即为 
欧氏距离 在 度量 之前 应该 将 每个 属性 的 值 
规范化 这样 有助于 防止 具有 较大 初始 值域 的 属性 
比 具有 较小 初始 值域 的 属性 的 权重 过大 
knn 算法 在 分类 时 主要 的 不足 是 当 
样本 不 平衡 时 如果 一个 类 的 样本容量 很大 
而 其他 类 样本容量 很 小时 有 可能 导致 当 
输入 一个 新 样本 时 该 样本 的 K 个 
邻居 中 大容量 类 的 样本 占多数 算法 伪代码 1 
搜索 k 近邻 的 算法 kNN A n k 2 
3 # 输入 A n 为 N 个 训练样本 在 
空间 中 的 坐标 k 为 近邻 数 4 # 
输出 x 所属 的 类别 5 6 取 A 1 
~ A k 作为 x 的 初始 近邻 计算 与 
测试 样本 x 间 的 欧式 距离 d x A 
i i = 1 2 . . . . . 
k 7 按 d x A i 升序 排序 8 
取 最远 样本 距离 D = max { d x 
a j | j = 1 2 . . . 
k } 9 10 for i = k + 1 
i = n i + + # 继续 计算 剩下 
的 n k 个 数据 的 欧氏距离 11 计算 a 
i 与 x 间 的 距离 d x A i 
12 if d x A i D 13 then 用 
A i 代替 最远 样本 # 将 后面 计算 的 
数据 直接进行 插入 即可 14 15 最后 的 K 个 
数据 是 有 大小 顺序 的 再 进行 K 个 
样本 的 统计 即可 16 计算 前 k 个 样本 
A i i = 1 2 . . k 所属 
类别 的 概率 17 具有 最 大 概率 的 类别 
即为 样本 x 的 类 python 函数 1 # knn 
k 最 临近 算法 2 # inX 为 待 分类 
向量 dataSet 为 训练 数据集 3 # labels 为 训练 
集 对应 分类 k 最 邻近 算法 4 def classify0 
inX dataSet labels k 5 dataSetSize = dataSet . shape 
0 # 获得 dataSet 的 行数 6 7 diffMat = 
np . tile inX dataSetSize 1 dataSet # 对应 的 
差值 8 sqDiffMat = diffMat * * 2 # 差 
的 平方 9 sqDistances = sqDiffMat . sum axis = 
1 # 差 的 平方 的 和 10 distances = 
sqDistances * * 0.5 # 差 的 平方 的 和的/nr 
平方根 11 # 计算 待 分类 向量 与 每一个 训练 
数据集 的 欧氏距离 12 13 s o r t e 
d D i s t I n d i c 
i e s = distances . argsort # 排序 后 
统计 前面 K 个 数据 的 分类 情况 14 15 
classCount = { } # 字典 16 for i in 
range k 17 voteIlabel = labels s o r t 
e d D i s t I n d i 
c i e s i # labels 得 是 字典 
才 可以 如此 18 classCount voteIlabel = classCount . get 
voteIlabel 0 + 1 19 20 sortedClassCount = sorted classCount 
. iteritems key = operator . itemgetter 1 reverse = 
True # 再次 排序 21 22 return sortedClassCount 0 0 
# 第一 个 就是 最多 的 类别 最后 针对于 K 
值 的 选取 做 最后 的 总结 