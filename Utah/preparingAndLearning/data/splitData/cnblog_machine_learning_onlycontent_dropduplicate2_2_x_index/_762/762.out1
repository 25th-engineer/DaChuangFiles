第一 次 接触 SVM 支持 向量 机 还是 四年前 的 
事情 了 那时 用 它 做 手写体 数字 识别 参考 
了 一些 书 和 文献 照着 人家 的 步骤 用 
Matlab 敲出 了 PCA + SVM 的 代码 识别率 一般 
90 都 没上 不好意思 跟人 打招呼 最 囧 的 是 
后来 参加 一个 面试 人家 问 我 神马 是 支持 
向量 我 都答/nr 不上来 上了 研究生 在 各种 模式识别 和 
机器 学习 相关 的 课上 反复 学习 了 这一 经典 
算法 每次/r 都有/nr 新的/i 体会/n 借此机会 做 一个 总结 SVM 
是 一种 线性 分类器 它 针对 的 也是 简单 的 
有 监督 学习 问题 给定 m 个 样本 x i 
y i y i = + / 1 确定 一个 
线性 分类 面 这个 问题 可以 用 多种 方法 解决 
感知器 Fisher 线性 判别分析 Logistic 回归 等 不同 算法 的 
实现 和 遵循 的 准则 各不相同 感知器 算法 采用 迭代 
的 方法 对 样本 进行 序 贯 处理 根据 新来 
的 样本 调整 线性 分类 面 函数 的 系数 所有 
训练样本 被 正确 划分 即 完成 迭代 Fisher 线性 判别 
分析方法 遵循 的 是 类 内 离 散度 小 类 
间离 散度 大 的 准则 得到 线性 分类器 Logistic 回归 
将 y | x 看作 服从 Bernoulli 分布 取值 为 
0 或 1 的 离散 随机变量 通过 最大化 后验/nr 概率 
P y | x 的 办法 来 得到 线性 分类器 
SVM 遵循 的 准则 是 类 间 间隔 margin 最大 
根据 这个 准则 得到 的 分类 面有 以下 的 特点 
属于 不 同类 的 在 分类 面 垂直 方向 投影 
距离 最近 的 样本点 的 距离 最小 设 这个 分类 
面的 方程 为 w x + b = 0 gamma 
为 跟 margin 成正比 的 一个 量 称为 函数 间隔 
function margin 则 我们 优化 的 目标 为 max gamma 
约束条件 为 1   y i w x i + 
b = gamma 2   w 模 的 归一化 即 
| | w | | = 1 将 约束条件 改为 
y i w / gamma * x i + b 
= 1 用 w 代替 w / gamma 则 SVM 
的 求解 变为 二次 规划 QP 问题 min | | 
w | | s . t y i w x 
i + b = 1 matlab 里 自 带了 求解 
QP 问题 的 quadprog 的 命令 可以 求解 这个 问题 
但 一般 工程 实现 中 不 这么 干 一方面 因为 
它 的 实现 效率 低 另外 一方面 这种 形式 不 
利于 将 SVM 推广 到 高维空间 即 Kernel SVM 一般 
工程 实现 往往 选择 解决 它 的 对偶 问题 Dual 
Problem 如果 一个 优化 问题 满足 KKT 条件 那么 一般 
把 它 叫做 原 问题 Primal Problem 就 可以 转化 
为 求解 它 的 对偶 问题 Dual Problem 对于 上述 
线性 SVM 的 原 问题 它 的 对偶 问题 为 
Max W alpha s . t alpha i = 0 
y 1 * alpha 1 + y 2 * alpha 
2 . . . . . + y m * 
alpha m = 0 得到 alpha i 后 就 可以 
通过 w = alpha 1 * y 1 * x 
1 + alpha 2 * y 2 * x 2 
. . . . . + alpha m * y 
m * x m 得到 w 由于 alpha i = 
0 alpha i 0 对应 的 x i 满足 y 
i w x i + b = 1 对 w 
的 产生 做出 了 贡献 为 支持 向量 对于 alpha 
i = 0 y i w x i + b 
1 为非 支持 向量 上面 介绍 的 SVM 为 样本 
线性 可分 前提下 的 线性 SVM 在 实际 中 的 
样本 往往 在 线性 欧式空间 内 不可 分 解决 这个 
问题 的 办法 有 两个 1 将 样本 映 射到 
高维空间 即 Kernel SVM 2 采用 软 间隔 Soft Margin 
的 SVM 在 线性 SVM 的 对偶 问题 中 的 
目标 函数 和 决策函数 表达式 中 都会 出现 内积 项 
x i x j Kernel SVM 的 主要 思想 是 
用 一个 核 函数 K x i x j = 
f x i f x j 代替 x i x 
j f x i 为 x i 向 高维空间 的 
映射函数 由于 对偶 问题 的 求解 中 x i x 
j 之间 的 所有 运算 都为 内积 运算 所以 没有 
必要 显示 地 求出 f x i 只要 给出 K 
x i x j 即可 用得 最多 的 是 RBF 
和 多项式 核 软 间隔 SVM 的 基本 思想 是 
对于 无法 线性 可分 的 情况 引入 惩罚 项 sigma 
i 和 惩罚 系数 C min | | w | 
| ^ 2 / 2 + C * sigma 1 
+ sigma 2 + . . . . . . 
sigma m s . t y i w x i 
+ b = 1 sigma i y i w x 
i + b = 1 sigma i 意味着 可以 容忍 
一定 错分 的 情况 目标函数 出现 C * sigma 1 
+ sigma 2 + . . . . . . 
sigma m 一项 表明 错分 会 带来 惩罚 这样 就 
避免 了 错分 的 样本 过多 分类 性能 恶化 求解 
SVM 对偶 问题 一般 采用 SMO 方法 参考 链接 给出 
了 具体 的 实现 参考 链接 http / / v 
. 163 . com / movie / 2008/1 / C 
/ 6 / M6SGF6VB4 _ M6SGJVMC6 . htmlhttp / / 
v . 163 . com / movie / 2008/1 / 
9/3 / M6SGF6VB4 _ M6SGJVA93 . htmlhttp / / blog 
. csdn . net / pennyliang / article / details 
/ 7103953 