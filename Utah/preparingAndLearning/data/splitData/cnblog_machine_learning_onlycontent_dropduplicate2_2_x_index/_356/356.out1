写 这个 系列 是 因为 最近 公司 在 搞 技术 
分享 学习 Spark 我 的 任务 是 讲 PySpark 的 
应用 因为 我 主要 用 Python 结合 Spark 就讲 PySpark 
了 然而 我 在 学习 的 过程 中 发现 PySpark 
很 鸡肋 至少 现在 我 觉得 我 不会 拿 PySpark 
做 开发 为什么 呢 原因 如下 1 . PySpark 支持 
的 算法 太少 了 我们 看 一下 PySpark 支持 的 
算法 参考 官方 文档 前面/f 两个/m pyspark/w ./i sql/w 和/c 
pyspark/w ./i streaming/w 是/v 对/p sql/w 和/c streaming/w 的/uj 支持/v 
主要 是 读取数据 和 streaming 处理 这种方式 当然 这是 spark 
的 优势 要是 这也 不支持 真是 见鬼 了 pyspark/w ./i 
ml/w 和/c pyspark/w ./i mllib/w 分别/d 是/v ml/w 的/uj api/w 
和/c mllib/w 的/uj api/w ml 的 算法 真心 少 啊 
而且 支持 的 功能 很 有限 譬如 Lr 逻辑 回归 
和 GBT 目前 只 支持 二 分类 不 支持 多 
分类 mllib 相对 好点 支持 的 算法 也 多点 虽然 
昨天 发 的 博文 讲 mlllib 的 时候 说过 有的 
算法 不 支持 分布式 所以 才会 有限 但是 我 在想 
如果 我 需要 用到 A 算法 而/c Ml/w 和/c Mllib/w 
的/uj 包/v 里面/f 都/d 没有/v 这样 是不是 意味着 要 自己 
开发 分布式 算法 呢 代价 有点 大 诶 感觉 写 
这个 的 时间 不如 多 找找 有用 的 特征 然后 
上 LR 这样 效果 说不定 更好 因为 目前 还 没有 
在 实际 中 用过 所以 以上 只是 我 的 想法 
下面 把 ml 和 mllib 的 所有 api 列出来 这样 
看 的 更 清楚 图一 pyspark . ml 的 api 
图二 pyspark . mllib 的 api 从 上面 两张 图 
可以 看到 mllib 的 功能 比 ml 强大 的 不是 
一点半点 啊 那/r ml/w 这个/r 包的/nr 存在/v 还有/v 什么/r 意义/n 
呢/y 不懂 如果 有 了解 的 欢迎 留言 虽然 有 
这么 多 疑问 但是 我 还是 跟 大家 讲了 用 
的 数据 依然 是 iris 其实 我 真心 想 换个 
数据集 啊 = = 下次 换 上 代码 1 from 
pyspark . sql import SQLContext 2 sqlContext = SQLContext sc 
3 df = sqlContext . read . format com . 
databricks . spark . csv . options header = true 
inferschema = true . load iris . csv 4 # 
Displays the content of the DataFrame to stdout 5 df 
. show 6 7 8 from pyspark . ml . 
feature import StringIndexer 9 indexer = StringIndexer inputCol = Species 
outputCol = labelindex 10 indexed = indexer . fit df 
. transform df 11 indexed . show 12 13 from 
pyspark . sql import Row 14 from pyspark . mllib 
. linalg import Vectors 15 from pyspark . ml . 
classification import NaiveBayes 16 17 # Load and parse the 
data 18 def parseRow row 19 return Row label = 
row labelindex 20 features = Vectors . dense row Sepal 
. Length 21 row Sepal . Width 22 row Petal 
. Length 23 row Petal . Width 24 25 # 
# Must convert to dataframe after mapping 26 parsedData = 
indexed . map parseRow . toDF 27 labeled = StringIndexer 
inputCol = label outputCol = labelpoint 28 data = labeled 
. fit parsedData . transform parsedData 29 data . show 
30 31 # # 训练 模型 32 # Naive Bayes 
33 nb = NaiveBayes smoothing = 1.0 modelType = multinomial 
34 model _ NB = nb . fit data 35 
predict _ data = model _ NB . transform data 
36 traing _ err = predict _ data . filter 
predict _ data label = predict _ data prediction . 
count 37 total = predict _ data . count 38 
nb _ scores = float traing _ err / total 
39 print traing _ err total nb _ scores 40 
# 7 150 0.0466666666667 41 42 43 # Logistic Regression 
# # # # # # # # # # 
# # # # # # # # # # 
# # # # # # # # # # 
# # # # # # # # # # 
# # # # # # # # # # 
# # # # # # # # # 44 
# Logistic regression . Currently this class only supports binary 
classification . 45 from pyspark . ml . classification import 
L o g i s t i c R e 
g r e s s i o n 46 lr 
= L o g i s t i c R 
e g r e s s i o n maxIter 
= 5 regParam = 0.01 47 model _ lr = 
lr . fit data 48 predict _ data = model 
_ lr . transform data 49 traing _ err = 
predict _ data . filter predict _ data label = 
predict _ data prediction . count 50 total = predict 
_ data . count 51 lr _ scores = float 
traing _ err / total 52 print traing _ err 
total float traing _ err / total 53 54 55 
# Decision Tree 56 from pyspark . ml . classification 
import D e c i s i o n T 
r e e C l a s s i f 
i e r 57 dt = D e c i 
s i o n T r e e C l 
a s s i f i e r maxDepth = 
2 labelCol = labelpoint 58 model _ DT = dt 
. fit data 59 predict _ data = model _ 
DT . transform data 60 traing _ err = predict 
_ data . filter predict _ data label = predict 
_ data prediction . count 61 total = predict _ 
data . count 62 dt _ scores = float traing 
_ err / total 63 print traing _ err total 
float traing _ err / total 64 65 66 # 
GBT # # # # # # # # # 
# # # # # # # # # # 
# # # # # # # # # # 
# # # # # # # # # # 
# # # # # # # # # # 
# # # # # # # # # # 
67 # # GBT . Currently this class only supports 
binary classification . 68 from pyspark . ml . classification 
import GBTClassifier 69 gbt = GBTClassifier maxIter = 5 maxDepth 
= 2 labelCol = labelpoint 70 model _ gbt = 
gbt . fit data 71 predict _ data = model 
_ gbt . transform data 72 traing _ err = 
predict _ data . filter predict _ data label = 
predict _ data prediction . count 73 total = predict 
_ data . count 74 dt _ scores = float 
traing _ err / total 75 print traing _ err 
total float traing _ err / total 76 77 78 
# Random Forest 79 from pyspark . ml . classification 
import R a n d o m F o r 
e s t C l a s s i f 
i e r 80 rf = R a n d 
o m F o r e s t C l 
a s s i f i e r numTrees = 
3 maxDepth = 2 labelCol = labelpoint seed = 42 
81 model _ rf = rf . fit data 82 
predict _ data = model _ rf . transform data 
83 traing _ err = predict _ data . filter 
predict _ data label = predict _ data prediction . 
count 84 total = predict _ data . count 85 
dt _ scores = float traing _ err / total 
86 print traing _ err total float traing _ err 
/ total 87 88 # M u l t i 
l a y e r P e r c e 
p t r o n C l a s s 
i f i e r # # # # # 
# # # # # # # # # # 
# # # # # # # # # # 
# # # # # # # # # # 
# # # # # # # # # # 
# # # # # # # # # # 
# # # # 89 # Classifier trainer based on 
the Multilayer Perceptron . Each layer has sigmoid activation function 
output layer has softmax . 90 # Number of inputs 
has to be equal to the size of feature vectors 
. Number of outputs has to be equal to the 
total number of labels . 91 from pyspark . ml 
. classification import M u l t i l a 
y e r P e r c e p t 
r o n C l a s s i f 
i e r 92 mlp = M u l t 
i l a y e r P e r c 
e p t r o n C l a s 
s i f i e r maxIter = 100 layers 
= 150 5 150 blockSize = 1 seed = 11 
93 model _ mlp = mlp . fit parsedData 94 
predict _ data = model _ mlp . transform parsedData 
95 traing _ err = predict _ data . filter 
predict _ data label = predict _ data prediction . 
count 96 total = predict _ data . count 97 
dt _ scores = float traing _ err / total 
98 print traing _ err total float traing _ err 
/ total 因为 数据集 和 上次 讲 pyspark 聚 类 
应用 的 数据 是 一样 的 就不 一步步 的 展示 
了 但是 我 这个 程序 里 只有 NaiveBayes 的 效果 
还行 0.94 的 正确率 其他 的 像 DecisionTree 等 效果 
真心 差 可能 参数 还 需要 调 先 掌握 怎么 
用 再来 调 参 官方 文档 里 关于 参数 的 
解释 也 非常 详细 可以 看看 下一次 讲 回归 我 
决定 不 只写 pyspark . ml 的 应用 了 因为 
实在 是 图样 图 naive 想 弄清楚 pyspark 的 机器学习 
算法 是 怎么 运行 的 跟 普通 的 算法 运行 
有 什么 区别 优势 等 再 写个 pyspark . mllib 
看/v 相同/d 的/uj 算法/n 在/p ml/w 和/c mllib/w 的/uj 包里/nr 
运行/v 效果/n 有/v 什么/r 差异/n 如果 有 是 为什么 去看 
源码 怎么 写 的 此外 我 真的 想 弄清楚 这 
货 在 实际 生产 中 到底 有用吗 毕竟 还是 要 
落实 生产 的 我 之前 想 如果 python 的 sklearn 
能够 在 spark 上 应用 就 好了 后来 在 databricks 
里面 找到 了 一个 包 好像 是 准备 把 sklearn 
弄到 spark 上来 当然 算法 肯定 要 重新 写 不过 
还 没有 发布 期待 发布 的 时候 此外 我 在 
知乎 上 也 看到 过 有人 提问 说 spark 上 
能用 skearn 吗 大概 是 这 意思 应该 很好 搜 
里面 有个 回答 好像 说 可以 不过 不是 直接 用 
等 我 找到 了 把 链接 放出来 其实 换 一种 
想法 不用 spark 也行 直接 用 mapreduce 编程序 但是 mapreduce 
慢 啊 此处 不严谨 因为 并 没有 测 试过 两者 
的 性能 差异 待 补充 在 我 使用 spark 的 
短暂 时间 内 我 个人 认为 spark 的 优势 在于 
数据 处理 快 它 不 需要 像 mapreduce 一样 把 
数据 切分 成 这么 多块 计算 然后再 reduce 合并 而是 
直接 将 数据 导入 的 时候 就 指定 分区 运行 
机制 不同 尤其 是 spark streaming 的 功能 还是 很快 
的 所以 这是 spark 的 优势 鄙人 拙见 如 有错误 
欢迎 指出 而 spark 的 劣势 也 比较 明显 因为 
它 对 设备 的 要求 太高 了 吃 内存 啊 
能 不高 吗 这 也是 它 快 的 原因 你 
把 数据 都 放在 内存 里 取 的 时间 比 
放在 磁 盘里 当然 要快 不过 实际上 在 存储 数据 
或者 输出 结果 的 时候 还是 会 选择 memory + 
disk 的 方式 保险 嘛 前段 时间 看 的 alluxio 
也是 占了 内存 的 优势 恩 说 了 很多 废话 
下周 争取 研究 的 深 一点 不然 在 公司 里 
讲 都 没人 听 = = 