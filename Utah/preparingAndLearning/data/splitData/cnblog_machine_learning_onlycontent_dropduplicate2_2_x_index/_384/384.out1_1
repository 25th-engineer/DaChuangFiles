十分 抱歉 由于 项目 太忙 我会 说 自己 懒 吗 
柳猫/nr 一直 没有 更新 自己 的 手记 现在 就让 柳猫来/nr 
讲讲 十个 常用 的 深度 学习 算法 过去 十年 里 
人们 对 机器 学习 的 兴趣 经历 了 爆炸 式 
的 整 长 我们 几乎 每天 都 可以 在 计算机程序 
行业 会议 和 媒体 上 看到 机器 学习 的 身影 
很多 关于 机器 学习 的 讨论 都 混淆 了 机器学习 
能 做什么 和 人类 希望 机器学习 能 做什么 从 根本 
上 讲 机器学习 是 运用 算法 从 原始 数据 中 
提取 信息 并用 某种 类型 的 模型 进行 表示 然后 
使用 该 模型 对 一些 尚未 用 模型表示 的 其他 
数据 来 进行 推断 神经 网络 就是 机器学习 各类 模型 
中 的 其中 一类 并且 已经 存在 了 至少 50年 
神经 网络 的 基本 单位 是 节点 它 的 想法 
大致 来源于 哺乳动物 大脑 中 的 生物 神经元 生物 大脑 
中的 神经元 节点 之间 的 链接 是 随着 时间 推移 
不断 演化 的 而 神经网络 中的 神经元 节点 链接 也 
借鉴 了 这一点 会 不断 演化 通过 训练 的 方式 
神经/n 网络/n 中/f 很多/m 重要/a 框架/n 的/uj 建立/v 和/c 改进/v 
都/d 完成/v 于/p 二十/m 世纪/n 八十/m 年代/t 中期/t 和/c 九十/m 
年代/t 初期/t 然而 要想 获得 较 好 结果 需要 大量 
的 时间 和 数据 由于 当时 计算机 的 能力 有限 
神经 网络 的 发展 受到 了 一定 的 阻碍 人们 
的 关注度 也 随之 下降 二十一 世纪 初期 计算机 的 
运算 能力 呈 指数级 增长 业界 也 见证 了 计算机 
技术 发展 的 寒武纪 爆炸 这在 之前 都是/nr 无法 想象 
的 深度 学习 以 一个 竞争者 的 姿态 出现 在 
计算 能力 爆炸 式 增长 的 十年 里 脱颖而出 并且 
赢得 了 许多 重要 的 机器学习 竞赛 其 热度 在 
2017年 仍然 不减 如今 在/p 机器/n 学习/v 的/uj 出现/v 的/uj 
地方/n 我们/r 都能/nr 看到/v 深度/ns 学习/v 的/uj 身影/n 这是 柳猫/nr 
自己做 的 一个 小 例子 词 向量 的 t SNE 
投影 通过 相似性 进行 聚 类 最近 我 开始 阅读 
关于 深度 学习 的 学术 论文 根据 我 的 个人 
研究 以下 文章 对 这个 领域 的 发展 产生 了 
巨大 的 影响 1998年 NYU 的 文章 基于 梯度 学习 
的 文档 识别 Gradient Based Learning Applied to Document Recognition 
介绍 了 卷积 神经 网络 在 机器 学习 中 的 
应用 Toronto 2009年 的 文章 深度 波兹曼 机器 Deep Boltzmann 
Machines 针对 波兹曼 机 提出 了 一种 新 的 学习 
算法 其中 包含 许多 隐藏 层 Stanford 和 Google 2012年 
联合 发表 的 文章 使用 大 规模 非 监督 学习 
构建 高层 特征 Building High Level Features Using Large Scale 
Unsupervised Learning 解决 了 仅 利用 未 标记 的 数据 
构建 高级 特定 类 的 特征 检测器 的 问题 Berkeley 
2013年 的 文章 用于 一般 视觉 识别 的 深层 卷积 
激活 特征 DeCAF A Deep Convolutional Activation Feature for Generic 
Visual Recognition 发布 了 名为 DeCAF 的 算法 这是 深度 
卷积 激活 特征 的 一个 开源 实现 使用 相关 的 
网络 参数 视觉 研究 人员 能够 利用 一 系列 视觉 
概念学习 范例 进行 深入 实验 DeepMind 2016年 的 文章 用 
深度 强化 学习 玩 Atari Playing Atari with Deep Reinforcement 
Learning 提出 了 第一 个 可以 成功 地 通过 强化 
学习 从 高维 感官 输入 中 直接 学习 控制策略 的 
深度 学习 模型 柳猫/nr 整理 了 人工智能 工程师 10 个 
用于 解决 机器学习 问题 的 强大 的 深度 学习 方法 
但是 我们 首先 需要 定义 什么 是 深度 学习 如何 
定义 深度 学习 是 很多 人 面临 的 一个 挑战 
因为 它 的 形式 在 过去 的 十年 中 已经 
慢慢 地 发生 了 改变 下图 直观 地 展示 了 
人工智能 机器 学习 和 深度 学习 之间 的 关系 人工智能 
领域 广泛 存在 时间 较长 深度 学习 是 机器学习 领域 
的 一个 子集 而 机器学习 是 人工智能 领域 的 一个 
子集 一般 将 深度 学习 网络 与 典型 前馈 多层 
网络 从 如下 方面 进行 区分 深度 学习 网络 具有 
比 前馈 网络 更多 的 神经元 深度 学习 网络连接 层 
之间 的 方式 更 复杂 深度 学习 网络 需要 有像/nr 
寒武纪 大爆发 式 的 计算 能力 进行 训练 深度 学习 
网络 能够 自动 提取 特征 上文 提到 的 更多 的 
神经元 是 指 近年来 神经元 的 数量 不断 增加 就 
可以 用 更 复杂 的 模型 来 表示 层 也从 
多层 网络 中 每 一层 完全 连接 发展 到 卷积 
神经 网络 中 神经元 片段 的 局部 连接 以及 与 
递归 神经 网络 中 的 同一 神经元 的 循环 连接 
与 前 一层 的 连接 除外 因此 深度 学习 可以 
被 定义 为 以下 四 个 基本 网络 框架 中 
具有 大量 参数 和 层数 的 神经 网络 无 监督 
预 训练 网络 卷积 神经网络 循环 神经网络 递归 神经 网络 
在 这篇文章 中 我 主要 讨论 三个 框架 卷积 神经网络 
Convolutional Neural Network 基本上 就是 用 共享 权重 在 空间 
中 进行 扩展 的 标准 神经网络 卷积 神经网络 主要 是 
通过 内部 卷积 来 识别 图片 内部 卷积 可以 看到 
图像 上 识别 对象 的 边缘 循环 神经网络 Recurrent Neural 
Network 基本上 就是 在 时间 上 进行 扩展 的 标准 
神经网络 它 提取 进入 下 一时间 步的/nr 边沿 而 不是 
在 同一 时间 进入 下 一层 循环 神经网络 主要 是 
为了 识别 序列 例如 语音信号 或者 文本 其 内部 的 
循环 意味着 网络 中 存在 短期 记忆 递归 神经网络 Recursive 
Neural Network 更 类似于 分层 网络 其中 输入 序列 没有 
真正 的 时间 面 但是 必须 以 树状 方式 分层 
处理 以下 10种 方法 均 可 应用于 这些 框架 1 
反向 传播 反向 传播 是 一种 计算 函数 偏 导数 
或 梯度 的 简单 方法 它 的 形式 是 函数 
组合 如 神经网络 在 使用 基于 梯度 的 方法 求解 
最优化 问题 梯度 下降 只是 其中 之一 时 需要 在 
每次 迭代 中 计算 函数 梯度 对于 一个 神经 网络 
其 目标函数 是 组合 形式 那么 应该 如何 计算 梯度 
呢 有 2种 常规 方法 1 微 分解 析法 函数 
形式 已知 的 情况 下 只 需要 用 链式法则 基础 
微积分 计算 导数 2 有限 差分法 近似 微分 这种方法 运算量 
很大 因为 函数 评估 的 数量级 是 O N 其中 
N 是 参数 的 个数 与 微分 解析 法 相比 
这种方法 运算量 更大 但是 在 调试 时 通常 会 使用 
有限 差分 验证 反向 传播 的 效果 2 随机 梯度 
下降 梯度 下降 的 一个 直观 理解 就是 想象 一条 
源自 山顶 的 河流 这条 河流 会 沿着 山势 的 
方向 流向 山麓 的 最低点 而这 也 正是 梯度 下 
降法 的 目标 我们 所 期望 的 最 理想 的 
情况 就是 河流 在 到达 最终 目的地 最低点 之前 不会 
停下 在 机器 学习 中 这/r 等价/n 于/p 我们/r 已经/d 
找到/v 了/ul 从/p 初始/v 点/m 山顶 开始 行走 的 全局 
最小值 或 最优 值 然而 可能 由于 地形 原因 河流 
的 路径 中 会 出现 很多 坑洼 而这 会 使得 
河流 停滞不前 在 机器学习 术语 中 这种 坑洼 称为 局部 
最优 解 而 这 不是 我们 想要 的 结果 有 
很多 方法 可以 解决 局部 最优 问题 因此 由于 地形 
即 函数性质 的 限制 梯度 下降 算法 很容易 卡在 局部 
最小值 但是 如果 能够 找到 一个 特殊 的 山地 形状 
比如 碗 状 术语 称作 凸函数 那么 算法 总是 能够 
找到 最 优点 在 进行 最 优化 时 遇到 这些 
特殊 的 地形 凸函数 自然 是 最好 的 另外 山顶 
初始 位置 即 函数 的 初始值 不同 最终 到达 山底 
的 路径 也 完全 不同 同样 不同 的 流速 即 
梯度 下降 算法 的 学习 速率 或 步长 也 会 
导致 到达 目的地 的 方式 有 差异 是否 会 陷入 
或 避开 一个 坑洼 局部 最小值 都会 受到 这 两个 
因素 的 影响 3 学习率 衰减 调整 随机 梯度 下降 
优化 算法 的 学习 速率 可以 提升 性能 并 减少 
训练 时间 这 被称作 学习率 退火 或 自适应 学习率 训练 
中 最简单 也 最 常用 的 学习率 自适应 方法 就是 
逐渐 降低 学习率 在 训练 初期 使用 较大 的 学习 
率 可以 对 学习率 进行 大幅 调整 在 训练 后期 
降低 学习率 以 一个 较小 的 速率 更新 权重 这种 
方法 在 早期 可以 快速 学习 获得 较好 的 权重 
并在 后期 对 权重 进行 微调 两个 流行 而 简单 
的 学习率 衰减 方法 如下 线性 地 逐步 降低 学习率 
在 特定 时点 大幅 降低 学习率 4 Dropout 拥有 大量 
参数 的 深度 神经 网络 是 非常 强大 的 机器学习 
系统 然而 在 这样 的 网络 中 过拟合 是 一个 
很 严重 的 问题 而且 大型 网络 的 运行 速度 
很慢 这就 使得 在 测试 阶段 通过 结合 多个 不同 
的 大型 神经 网络 的 预测 来 解决 过拟合 问题是 
很 困难 的 Dropout 方法 可以 解决 这个 问题 其 
主要 思想 是 在 训练 过程 中 随机 地 从 
神经 网络 中 删除 单元 以及 相应 的 连接 这样 
可以 防止 单元 间 的 过度 适应 训练 过程 中 
在 指数级 不同 稀疏度 的 网络 中 剔除 样本 在 
测试 阶段 很 容易 通过 使用 具有 较小 权重 的 
单 解开 网络 single untwined network 将 这些 稀疏 网络 
的 预测 结果 求 平均 来 进行 近似 这 能 
有效 地 避免 过拟合 并且 相对于 其他 正则化 方法 能 
得到 更大 的 性能 提升 Dropout 技术 已经 被 证明 
在 计算机 视觉 语音识别 文本/n 分类/n 和/c 计算/v 生物学/n 等/u 
领域/n 的/uj 有/v 监督/vn 学习/v 任务/n 中/f 能/v 提升/v 神经/n 
网络/n 的/uj 性能/n 并在 多个 基准 数据 集中 达到 最 
优秀 的 效果 5 最大/a 池/nr 最大/a 池是/nr 一种/m 基于/p 
样本/n 的/uj 离散化/i 方法/n 目标 是 对 输入 表征 图像 
隐藏 层 输出 矩阵 等 进行 下 采样 降低 维度 
并且 允许 对 子区 域中 的 特征 进行 假设 通过 
提供 表征 的 抽象 形式 这种 方法 可以 在 某种 
程度 上 解决 过拟合 问题 同样 它 也 通过 减少 
学习 参数 的 数目 以及 提供 基本 的 内部表征 转换 
不变性 来 减少 计算 量 最大 池是/nr 通过 将 最大 
过滤器 应用于 通常 不 重叠 的 初始 表 征子 区域 
来 完成 的 6 批量 标准化 当然 包括 深度 网络 
在内 的 神经 网络 需要 仔细 调整 权重 初始值 和 
学习 参数 批量 标准化 能够 使 这个 过程 更加 简单 
权重 问题 无论 怎么 设置 权重 初始值 比如 随机 或 
按 经验 选择 初始/v 权重/n 和/c 学习/v 后的/nr 权重/n 差别/d 
都/d 很大/a 考虑 一 小批 权重 在最 初时 对于 所需 
的 特征 激活 可能会 有 很多 异常值 深度 神经 网络 
本身 就 具有 病态 性 即 初始 层 的 微小 
变化 就 会 导致 后 一层 的 巨大 变化 在 
反向 传播 过程 中 这些 现象 会 导致 梯度 的 
偏移 这 就 意味着 在 学习 权重 以 产生 所 
需要 的 输出 之前 梯度 必须 补偿 异常值 而这 将 
导致 需要 额外 的 时间 才能 收敛 批量 标准化 将 
这些 梯度 从 异常值 调整 为 正常值 并在 小批量 范围内 
通过 标准化 使其 向 共同 的 目标 收敛 学习率 问题 
通常 来说 学习率 都 比较 小 这样 只有 一 小 
部分 的 梯度 用来 校正 权重 因为 异常 激活 的 
梯度 不 应该 影响 已经 学习 好 的 权重 通过 
批量 标准化 这些 异常 激活 的 可能性 会 被 降低 
就 可以 使用 更大 的 学习 率 加速 学习 过程 
电动叉车 轮胎 7 长短期 记忆 长短期 记忆 网络 LSTM 和/c 
其他/r 递归/v 神经网络/n 中的/i 神经元/nz 有/v 以下/f 三/m 个/q 不同/a 
点/m 它 可以 决定 何时 让 输入 进入 神经元 它 
可以 决定 何时 记住 上 一个 时间 步中/nr 计算 的 
内容 它 可以 决定 何时 让 输出 传递 到 下 
一个 时间 戳 LSTM 的 强大 之处 在于 它 可以 
只 基于 当前 的 输入 就 决定 上述 所有 请看 
下方 的 图表 当前 时间戳 的 输入 信号 x t 
决定 了 上述 三点 输 入门 input gate 决定 了 
第一 点 遗忘 门 forget gate 决定 了 第二 点 
输 出门 output gate 决定 了 第三 点 只 依赖 
输入 就 可以 完成 这三项 决定 这是 受到 大脑 工作 
机制 的 启发 大脑 可以 基于 输入 来 处理 突然 
的 上下文 语境 切换 8 Skip gram 词 嵌入 模型 
的 目的 是 针对 每个 词 学习 一个 高维 密集 
表征 其中 嵌入 向量 之间 的 相似性 显示 了 相应 
词语 之间 语义 或 句法 的 相似性 Skip gram 是 
一种 学习 词 嵌入 算法 的 模型 skip gram 模型 
包括 很多 其它 词 嵌入 模型 背后 的 主要 思想 
是 如果/c 两个/m 词汇/n 项有/nr 相似/v 的/uj 上下文/l 则 它们 
是 相似 的 换句话说 假设 有 一个 句子 比如 cats 
are mammals 如果 用 dogs 替换 cats 该 句子 仍然 
是 有 意义 的 因此 在 这个 例子 中 dogs 
和 cats 有 相似 的 上下文 即 are mammals 基于 
以上 假设 我们 可以 考虑 一个 上下文 窗口 包含 K 
个 连续 项 然后 跳过 其中 一个 词 试着 学习 
一个 可以 得到 除了 跳过 的 这个 词 以外 所有 
词 项 并且 可以 预测 跳过 的 词 的 神经 
网络 因此 如果 两个 词 在 一个 大 语料库 中 
多次 具有 相似 的 上下文 那么 这些 词 的 嵌入 
向量 将会 是 相似 的 9 连续 词 袋 模型 
在 自然 语言 处理 中 我们 希望 将 文档 中 
的 每一个 单词 表示 为 一个 数值 向量 使得 出现 
在 相似 上下 文中 的 单词 具有 相似 或 相近 
的 向量 表示 在 连续 词 袋 模型 中 我们 
的 目标 是 利用 一个 特定 单词 的 上下文 预测 
该词 首先 在 一个 大 的 语料库 中 抽取 大量 
的 句子 每 看到 一个 单词 同时 抽取 它 的 
上下文 然后 我们 将 上下文 单词 输入 到 一个 神经 
网络 并 预测 在 这个 上下文 中心 的 单词 当/t 
我们/r 有/v 成千上万/l 个/q 这样/r 的/uj 上下文/l 词汇/n 和/c 中心词/n 
时/n 我们 就 得到 了 一个 神经 网络 数据集 的 
实例 然后 训练 这个 神经网络 在 经过 编码 的 隐藏 
层 的 最终 输出 中 我们 得到 了 特定 单词 
的 嵌入式 表达 当 我们 对 大量 的 句子 进行 
训练 时 也能 发现 类似 上下 文中 的 单词 都 
可以 得到 相似 的 向量 10 迁移 学习 我们 来 
考虑 一下 卷积 神经 网络 是 如何 处理 图像 的 
假设有 一张 图像 对其 应用 卷积 并 得到 像素 的 
组合 作为 输出 假设 这些 输出 是 边缘 再次 应用 
卷积 那么 现在 的 输出 将 是 边缘 或 线 
的 组合 然后 再次 应用 卷积 此时 的 输出 将 
是 线 的 组合 以此类推 可以 把 它 想象 成是在/nr 
每 一层 寻找 一个 特定 的 模式 神经 网络 的 
最后 一层 通常 会 变得 非常 特别 如果 基于 ImageNet 
进行 训练 那么 神经 网络 的 最后 一层 或许 就是 
在 寻找 儿童 狗 或者 飞机 之类 的 完整 图像 
再 往后 倒退 几层 可能 会 看到 神经 网络 在 
寻找 眼睛 耳朵 嘴巴 或者 轮子 等 组成 部分 深度 
卷积 神经网络 中的 每 一层 逐步 建立 起 越来越 高 
层次 的 特征 表征 最后 几层 通常 是 专门 针对 
输入 数据 另一方面 前面 的 层 则 更为 通用 主要 
用来 在 一大 类 图 片中 有 找到 许多 简单 
的 模式 迁移 学习 就是 在 一个 数据 集上 训练 
卷积 神经 网络 时 去掉 最后 一层 在 不同 的 
数据 集上 重新 训练 模型 的 最后 一层 直观 来讲 
就是 重新 训练 模型 以 识别 不同 的 高级 特征 
因此 训练 时间 会 减少 很多 所以 在 没有 足够 
的 数据 或者 需要 太多 的 资源 时 迁移 学习 
是 一个 很 有用 的 工具 总结 深度 学习 是 
非常 注重 技术 实践 所谓 的 百 看 不如 一练 
当然 柳猫/nr 这里 讲 的 还是 非常 肤浅 如果 能够 
引起 小 伙伴们 对 深度 学习 的 兴趣 柳猫就/nr 觉得 
很 开心 了 