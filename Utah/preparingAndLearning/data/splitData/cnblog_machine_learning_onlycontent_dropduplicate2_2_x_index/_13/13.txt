0. 前言
1. 损失函数
2. Margin
3. Cross-Entropy vs. Squared Error
总结
参考资料
0. 前言
“尽管新技术新算法层出不穷，但是掌握好基础算法就能解决手头 90% 的机器学习问题。”
本系列参考书 "Hands-on machine learning with scikit-learn and tensorflow"以及kaggle相关资料，但是这篇文章没有参考，🤣
“观察到的一个有意思的细节：一些喜好机器学习或者数据科学的初学工程师和有机器学习或者数据科学背景的科学家，在工作上的主要区别在于如何对待负面的实验（包括线下和线上）结果。初学者往往就开始琢磨如何改模型，加Feature，调参数；思考如何从简单模型转换到复杂模型。有经验的人往往更加去了解实验的设置有没有问题；实验的Metrics的Comparison是到底怎么计算的；到真需要去思考模型的问题的时候，有经验的人往往会先反思训练数据的收集情况，测试数据和测试评测的真实度问题。初学者有点类似程咬金的三板斧，有那么几个技能，用完了，要是还没有效果，也就完了。而有经验的数据科学家，往往是从问题出发，去看是不是对问题本质的把握（比如优化的目标是不是对；有没有Counterfactual的情况）出现了偏差，最后再讨论模型。”
—— by @洪亮劼
1. 损失函数
前面一篇讨论了PRC、ROC、AUC等评测模型的不同侧重点，另外一方面，模型损失函数（目标函数）是机器学习里另外一个较为本质的问题，由于机器学习的损失函数根据任务、模型的不同，演化出各种形式，下面只谈谈分类模型里常见的情形。
假设一个二分类问题，样本空间是 y={-1,1}，一个分类模型对其进行预测，输出值是f(x)，f > 0 判定为1，f < 0判定为 -1。计算模型预测准确率的时候，样本真实分类 y 如果和 f 符号相同，则表示分类正确，符号相反，则模型分类错误。表示成分类误差，可记为 if y * f(x) > 0 then error = 0 else error = 1。
实际情况中，极少看到直接用分类误差作为模型的目标函数，原因主要有几点，其一，loss = 0 or 1 是平行于 x 轴的射线，求导为 0（原点除外），没法用梯度下降法来优化模型，而实际上这是个NP-Hard离散的非凸优化问题。其二，要训练一个好的模型还要让模型感知到，某个样本尽管分类正确了，但是到底有多“正确”，如果确信度低了还需要继续优化。举个例子：有三种水果，两种模型都进行了训练，在测试集上概率分布表现如下：
模型A：
预测概率
真实概率
分类误差
0.3 0.3 0.4
0 0 1 (苹果)
0
0.3 0.4 0.3
0 1 0 (梨子)
0
0.1 0.2 0.7
1 0 0 (桃子)
1
模型B：
预测概率
真实概率
分类误差
0.1 0.2 0.7
0 0 1 (苹果)
0
0.1 0.8 0.1
0 1 0 (梨子)
0
0.4 0.5 0.1
1 0 0 (桃子)
1
目测可以看出，模型A和模型B分类误差都是0.333，但是模型B更“靠谱”一些。问题来了，这个“靠谱”如何衡量？
2. Margin
margin 是衡量某次预测到底有多“准确”的一个指标，定义为 y*f(x)。简单说，希望正样本预测值为正尽量大，负样本预测值为负尽量小，就需要模型 max margin。
以margin作为横轴，黑色表示分类loss，红色表示log loss，蓝色表示 hinge loss，绿色表示 square error，可以看到随着 margin 变大，loss 总体是单调递减的，但是 squared error 超过1后会递增：
从上图可以看到，hinge loss 在margin达到一定阈值后（很确信分类正确的样本），loss降为0，对整个模型训练其实已经没有影响了。log loss 不管margin多大，loss永远不会降为0，会一直对模型有影响。squared error 里，如果f(x) 输出绝对值可以大于1，那么margin太大的点，对模型反而有不好的影响：
中场休息时间。。。喝口茶~ 欢迎关注公众号：kaggle实战，或博客：http://www.cnblogs.com/daniel-D/
3. Cross-Entropy vs. Squared Error
cross-entropy 可以简单理解为上面的 log loss，在深度学习里面，最后一层往往是通过 softmax 计算出概率分布margin区间为[0,1]。squared error 并不存在上述 margin 太大的点对模型反而有负面影响的情况，这时候该采用 Cross-Entropy or Squared Error?
结论是如果你使用的是神经网络的分类模型，建议使用 Cross-entropy。
做分类任务的深度神经网络，最后一层一般为softmax，softmax 计算公式如下：
一般采用反向传播的梯度下降方法优化，下面先把 softmax 的计算过程逐步剖析开来：（纠正图中一个错误：P=A * V）
上图中O1，O2，O3 表示softmax的输入节点，每个节点先经过指数化得到 A，然后求和得到 S 即分母，倒数得到 V，再各自和指数相乘，得到该节点输出的概率 P。为了图片结构稍微美观点，这里 P2 就没画出来了。对于上图的 O1 来说，导数来源于虚线的链路，箭头上的文字表示局部导数，它的导数实际来自两部分，一是直接相连的 P1，以及无直接链接的P2、P3……对应输入层的某个节点 O1 应用链式法则，P1 节点对其偏导如下
如果 i = j，即图中 O1 到 P1，导数由两条链路组成：
如果 i ≠ j，即图中 O1 到 P3，导数由一条链路组成：
如果模型训练得很好，Pj 和Pi都接近0或者其中一个接近于1，可以看到两种情况梯度都是接近于0，符合预期；如果模型很差，把某个错误的类别的概率也计算成1，那么 Pj 和 Pi 都接近0者其中一个接近于1，梯度也很小，不太符合预期，看看 Cross-entropy 是如何解决这个问题的。
对于Cross-entropy（Logloss）误差，计算公式为：
可以看到 Cross-entropy 只关注正确label上的概率大小，上图中 ，只有连到到 P1 的链路才是对梯度计算有效的，对 i = j 也就是上图中的 O1 来说：
对 i ≠ j 也就是 O2 到 L1 链路来说：
--备注：x 应该是 - 1/p_i，上面梯度应该多个负号，截图太累
如果模型很差，正确 label 上的概率 Pi 接近0，实际上不会影响梯度大小。
但是如果是 Squared Error，上图 P 到 L 连接上的 x 并不包含 logLoss 的倒数形式。由于正确类别和错误类别的损失函数都会影响 Oi 的偏导，整体公式比较复杂，这里就不做详细的推导了，但是整体上是先相乘，然后求和，梯度会很小，给优化带来阻碍。
总结
softmax部分在完全分类正确或者分类完全错误的情况下，该部分偏导都接近于0
logLoss 偏导中有倒数，可以“中和”softmax这个缺点，Squared Error 如果初始化不好很难克服这个问题，使用softmax分类模型的 Loss 推荐使用 cross entropy 而不是 classification error 或 squared error
logLoss 对完全错误分类的惩罚极大，但是其实偏导不会超过1
对于上述 logLoss 这种“虚张声势”的做法，直接看logLoss可能无法这种体现模型的准确性，比如把1个样本分得很错的模型 vs. 把多个样本分得不那么错的模型，可能后者的 logLoss 更小，建议直接用分类误差评估。
参考资料
Why You Should Use Cross-Entropy Error Instead Of Classification Error Or Mean Squared Error For Neural Network Classifier Training
Cross-Entropy vs. Squared Error Training: a Theoretical and Experimental Comparison
The Softmax function and its derivative
What are the impacts of choosing different loss functions in classification to approximate 0-1 loss
附：公众号