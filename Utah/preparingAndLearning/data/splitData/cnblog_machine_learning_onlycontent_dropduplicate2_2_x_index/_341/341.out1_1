依旧 是 唠叨 一下 考完 试了 该去 实习 的 朋友 
都去/nr 实习 了 这几天 最 主要 的 事情 应该 是 
把 win10 滚 回到 win7 了 真的 还是 熟悉 的 
画面 心情好 了 很多 可惜 自己 当初 安装 的 好多 
软件 都 写入 了 注册表 导致 软件 用不了 好处 就是 
重新 清理 了 一下 电脑 顺 便把 虚拟机 重新 安装 
了 一下 现在 正在 备份 系统 是的 一定 要 备份 
重要 数据 不 要 保存 在 C 盘 安装 软件 
不要 安装 在 C 盘 与 空间 无关 数据 才 
是 重点 win7 比较稳定 可以 懒得 备份 但是 linux 一定 
要 备份 不然 在 某一天 启动 系统 突然 说 丢失 
了 某个 内核 文件 然后 你 就得 修复 或者 recovery 
最糟糕 是 重新 安装 没有 几个 命令行 恢复 来 的 
酸 爽 接 着上 一篇 python 机器学习 回归 一 上一 
篇 文章 主要 讲 了 回归 的 入门 以及 引入 
了 代价 函数 这个东西 这一次 就要 涉及到 数学 算法 了 
但是 再 难 的 数学 方程 其 原理 拆解 开来 
不过 是 一个 思维 风暴 关键 是 你 愿不愿意 认真 
的 持续 的 思考 每一次 的 思考 都是 把 接近 
生锈 的 脑袋 运转 一下 好 让 自己 不 那么 
容易 老年痴呆 哈哈 20岁 说 这句话 似乎 有点 早 鉴于 
上一次 介绍 的 繁琐 难懂 这一次 直接 上 数学 公式 
我们 知道 回归 里面 最 简单 的 线性 回归方程 一般 
的 表达式 都是 f x = a * x 当 
只有 单个 影响 因素 x 时 推广 得到 在 多个 
影响 因素 就 会 得到 f x = a 0 
* x 0 + a 2 * x 2 + 
. . . + a n * x n 其中 
a n 是 第 n 个 回归系数 x n 是 
第 n 个 特征 变量 这里 的 一些 专有名词 劳驾 
wiki 上面 找 wiki 不懂 的 人 可以 百度 ps 
最近 最 开心 的 事情 是 入手 * * * 
搭建 好 了 属于 自己 的 shadowsocks 那么 学过 高等代数 
的 朋友 一定 了解 向量 内积 这个东西 其实 举 一个 
很 简单 的 例子 你们 就 明白 这个 概念 非常 
重要 是 机器 学习 中 的 计算 常用 的 向 
量化 这里 我 就 直接 贴图 因为 编辑器 的 字母 
打出 来 真的 很丑 前面 我们 已经 提到 了 成本 
函数 实际上 就是 误差 平方和 目的 是 要 找出 最好 
的 向量 a 这里 的 回归系数 a 也是 上 一篇 
的 theta 后面 继续 使用 thata 表示 回归系数 让 误差 
最小 一般 采取 的 是 求导 的 方式 关于 求导 
的 算法 上 一篇 中 通过 下坡 的 比喻 也很 
明了 这一次 我们 直接 计算 如何 求得 偏 导数 再次 
结合 前面 的 文章 要点 总结 一下 首先 我们 根据 
线性 回归模型 得到 成本 函数 其次 根据 梯度 下 降法 
对 回归 变量 theta 中的 每 一个 元素 求 偏 
导 并在 赋予 初始值 后 不断 的 更新 theta 值 
直至 下降 到 最低点 得到 局部 最小值 加入 只有 一个 
特征 变量 的 情况 下 不包括 常数 x0 则有 按照 
如下 方式 更新 theta 的 值 其中 类似于 求 偏 
导 是 把 其他 变量 当作 常数 处理 的 一个 
过程 此时 特征 变量 是 已知 值 常数 这里 需要 
强调 的 是 x0 作为 常数 theta0 的 的 特征 
变量 实际上 全部 是 常 数值 1 无论 是 针对 
多少 个 样本 i = 1 . . . . 
m 问题 来了 1 怎么 让 你 的 最速 下降 
模型 朝着 正确 的 方向 下降 它 一定 会 下降 
到 局部 最 优点 2 学习 速度 alpha 如何 取值 
以上 两个 问题 依次 做出 解答 针对 问题 1 下图 
演示 的 是 随着 迭代 次数 的 增加 成本 函数 
的 变化 我 真是 画图 技术 越来越 好 我们 的 
目的 是 每一次 迭代 成本 函数 的 更新 值 只会 
越来越 小 至于 小到 什么 程度 打个比方 取 最小 误差值 
的 边界值 为 eptheta = 1/1000 一般 读作 10 的 
负 三次方 针对 问题 2 让 alpha 满足 以下 三 
个 条件 i 取 充分 小 的 alpha 使得 J 
theta 在 每一次 迭代 之后 变得 更小 ii 如果 取值 
太小 收敛 到 猴年马月 iii 如果 取值 太大 J theta 
很 可能 不能 满足 条件 i 甚至 达到 发散 的 
结果 无 终止 的 循环 说 的 太多 画 个 
简单 明了 的 图 给 你们 看看 一般情况 我们 会 
选择 多个 alpha 的 值 0.001 0.01 0.1 然后 分别 
画出 J theta 与 迭代 次数 之间 的 散点图 为 
梯度 下降 算法 选择 最 合适 的 学习 速度 alpha 
罗嗦 了 半天 总 算是 把 算法 的 整个 过程 
描述 出来了 还望 各位 觉得 不对 或者 不 熟悉 的 
地方 提出 来 大家 互相 改进 那么 接下来 就是 matlab 
中 代价 函数 的 实现 python 突然 罢工 了 实际上 
numpy 的 编程 跟 matlab 非常 相似 相似 到 你 
无法 想象 的 程度 代价 函数 function J = computeCost 
X y theta % COMPUTECOST Compute cost for linear regression 
% J = COMPUTECOST X y theta computes the cost 
of using theta as the % parameter for linear regression 
to fit the data points in X and y % 
Initialize some useful values m = length y % number 
of training examples % = = = = = = 
= = = = = = = = = = 
= = = = = = YOUR CODE HERE = 
= = = = = = = = = = 
= = = = = = = = = = 
= % Instructions Compute the cost of a particular choice 
of theta % You should set J to the cost 
. pred = X * theta % the prediction result 
sum = 0 for i = 1 m % iterating 
from 1 to m sum = sum + pred i 
y i ^ 2 end J = 1 / 2 
* m * sum % compute the cost function % 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = end 梯度 下降 算法 只 针对 一个 
特征值 function theta J _ history = gradientDescent X y 
theta alpha num _ iters % GRADIENTDESCENT Performs gradient descent 
to learn theta % theta = GRADIENTDESENT X y theta 
alpha num _ iters updates theta by % taking num 
_ iters gradient steps with learning rate alpha % Initialize 
some useful values m = length y % number of 
training examples J _ history = zeros num _ iters 
1 % initializing all of J to zero before iteration 
for iter = 1 num _ iters % = = 
= = = = = = = = = = 
= = = = = = = = = = 
YOUR CODE HERE = = = = = = = 
= = = = = = = = = = 
= = = = = % Instructions Perform a single 
gradient step on the parameter vector % theta . % 
% Hint While debugging it can be useful to print 
out the values % of the cost function computeCost and 
gradient here . % = = = = = = 
= This is my bug = = = = = 
= I just want to use jacobian but it seems 
% = = = = = something wrong maybe because 
theta1 and theta2 are symbol variables = = = = 
= % syms theta1 theta2 % temp _ theta = 
theta _ 1 theta _ 2 % temp _ rate 
= jacobian J _ history iter temp % theta = 
theta alpha * temp _ rate * theta error = 
X * theta y % the error between prediction and 
observation delta = 1 / m * error * X 
% the J s decrease slope theta = theta alpha 
* delta % = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = % Save the cost J in every 
iteration J _ history iter = computeCost X y theta 
end end 前面 提到 向 量化 的 概念 实际上 上面 
的 1 到 m 个 样本 的 迭代 是 直接 
借助 矩阵 中的 向量 运算 的 改善 代码 如下 sum 
= pred y * pred y 然后 经过 取 学习 
速度 alpha = 0.01 1500次 迭代 后 拟合 效果图 的 
代码 function plotData x y % PLOTDATA Plots the data 
points x and y into a new figure % PLOTDATA 
x y plots the data points and gives the figure 
axes labels of % population and profit . % = 
= = = = = = = = = = 
= = = = = = = = = = 
= YOUR CODE HERE = = = = = = 
= = = = = = = = = = 
= = = = = = % Instructions Plot the 
training data into a figure using the % figure and 
plot commands . Set the axes labels using % the 
xlabel and ylabel commands . Assume the % population and 
revenue data have been passed in % as the x 
and y arguments of this function . % % Hint 
You can use the rx option with plot to have 
the markers % appear as red crosses . Furthermore you 
can make the % markers larger by using plot . 
. . rx MarkerSize 10 figure % open a new 
figure window plot x y rx MarkerSize 10 % plot 
the data ylabel Profit in $ 10 000s % set 
the y axis label xlabel Population of City in 10 
000s % set the x axis label % = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = end 拟合 
结果 如下 图 所示 总结 这里 需要 提到 的 另外 
一个 算法 是 正则方程 Normal Equation 我 看到 机器学习 实战 
这本书 直接 给出 的 算法 就是 庸 正则方程 令 误差 
为 0 然后 反过来 求 theta 的 值 这个 时候 
有 一个 问题 就是 求 逆 因为 不是 所有 的 
矩阵 都是 有逆/nr 矩阵 的 因此在 python 或者 matlab 中就 
给 出了 引入 数值 计算 的 函数 直接 求出 伪 
逆 来 逼近 真正 的 逆 因此 这个 算法 比较 
简单易行 针对 较多 的 特征 值 的 时候 比较 方便 
比如说 当 样本 数量 m 小于 特征 数量 n 的 
时候 好啦 下周 再见 下期 预告 局部 加权 线性 回归 
