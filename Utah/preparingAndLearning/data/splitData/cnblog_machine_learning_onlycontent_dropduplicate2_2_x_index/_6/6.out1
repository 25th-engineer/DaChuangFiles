& * & 2017/6 / 16update 最近 几天 发现 阅读 
这篇文章 的 朋友 比 较多 自己 阅读 发现 部分 内容 
出现 了 问题 进行 了 更新 一 什么 是 PCA 
摘 用一下 百度 百科 的 解释 PCA Principal Component Analysis 
主 成分 分析 是 一种 统计 方法 通过 正交变换 将 
一组 可能 存在 相关性 的 变量 转换 为 一组 线性 
不 相关 的 变量 转换 后的/nr 这组 变量 叫 主 
成分 二 PCA 的 用途 及 原理 用途 数据 降 
维 原理 线性映射 或 线性变换 简单 的 来说 就是 将 
高维 空间数据 投影 到 低维 空间 上 那么 在 数据 
分析 上 我们 是 将 数据 的 主 成分 包含 
信息量 大 的 维度 保留 下来 忽略 掉 对 数据 
描述 不 重要 的 成分 即将 主 成分 维度 组成 
的 向量空间 作为 低 维空间 将 高维 数据 投影 到 
这个 空间 上 就 完成 了 降 维 的 工作 
三 PCA 的 算法 实现 算法 思想 选取 数据 差异 
最大 的 方向 方差 最大 的 方向 方差 反应 的 
是 数据 与其 方差 均值 之间 的 偏离 程度 我们 
通常 认为 方差 越大 数据 的 信息量 就 越大 作为 
第一 个 主 成分 第二个 主 成分 选择 方差 次大 
的 方向 并且 与 第一 个 主 成分 正交 不断 
重复 这个 过程 直到 找到 n 个 主 成分 算法 
步骤 输入 数据集 D = { x1 x2 x3 x4 
. . . . xm } 低 维空间 维数 n 
xi 表示 数据 的 第 i 维 m 表示 数据 
维度 为 m n 表示 最终 要 变换 的 维度 
操作 1 . 对 所有 样本 进行 中心化 对 每个 
维度 减去 这个 维度 的 数据 均值 2 . 计算 
样本 的 协方差 矩阵 3 . 对 协方差 矩阵 做 
特征值 分解 4 . 选取 前 n 个 最大 的 
特征值 对应 的 的 特征向量 构成 特征向量 矩阵 W 输出 
Wm * n * Dh * m = D ′ 
一个 m * n 的 矩阵 乘以 数据集 h * 
m 的 矩阵 得到 h * n 的 矩阵 D 
′ D ′ 就是 降 维 后的/nr 数据 集 h 
* m h * n m n 算法 实现 python3 
. 6 机器学习 实战 代码 # * coding utf 8 
* # Filename pca . py # Author Ljcx pca 
主 成分 分析 降 维 算法 from numpy import * 
import pandas as pd import matplotlib . pyplot as plt 
class PcaM object def _ _ init _ _ self 
pass 读取 数据格式 化成 矩阵 def loadData self filename delim 
= \ t data = pd . read _ csv 
filename x = data list range 4 print x return 
mat x def pca self dataMat maxFeature = 105 meanValue 
= mean dataMat axis = 0 # 去 中心 元数据 
减去 均值 值得 新的 矩阵 均值 为 0 dataRemMat = 
dataMat meanValue # 求 矩阵 的 协方差 矩阵 covMat = 
cov dataRemMat rowvar = 0 print covMatt print covMat # 
求 特征值 和 特徵 向量 feaValue feaVect = linalg . 
eig mat covMat print 特征值 print feaValue print 特征向量 print 
feaVect # 返回 从小到大 的 索引 值 print feaSort + 
str feaValueSort feaValueSort = argsort feaValue feaValueTopN = feaValueSort maxFeature 
+ 1 1 redEigVects = feaVect feaValueTopN # 选择 之后 
的 特征向量 矩阵 print TopN 特征向量 矩阵 print redEigVects print 
shape redEigVects lowDataMat = dataRemMat * redEigVects # 数据 矩阵 
* 特征向量 矩阵 得到 降 维 后的/nr 矩阵 reconMat = 
lowDataMat * redEigVects . T + meanValue # 这 一步 
做 数据恢复 并 没有 看懂 这么做 的 意义 print lowDataMat 
return lowDataMat reconMat def plotW self lowDataMat reconMat fig = 
plt . figure ax = fig . add _ subplot 
111 ax . scatter lowDataMat 0 lowDataMat 1 marker = 
* s = 90 ax . scatter reconMat 0 reconMat 
1 marker = * s = 50 c = red 
plt . show def r e p l a c 
e N a n W i t h M e 
a n self datMat = self . loadData testdata . 
txt numFeat = shape datMat 1 for i in range 
numFeat # values that are not NaN a number meanVal 
= mean datMat nonzero ~ isnan datMat i . A 
0 i # set NaN values to mean datMat nonzero 
isnan datMat i . A 0 i = meanVal print 
datMat return datMat if _ _ name _ _ = 
= _ _ main _ _ p = PcaM dataMat 
= p . r e p l a c e 
N a n W i t h M e a 
n lowDataMat reconMat = p . pca dataMat 2 p 
. plotW dataMat reconMat 得到 降 维 后 数据分布 与 
恢复 之后 的 数据 的 分布 作比较 数据集 data 采用 
的 是 150 * 4 的 鸢尾花 数据集 5.1 3.5 
1.4 0.2 Iris setosa 4.9 3.0 1.4 0.2 Iris setosa 
4.7 3.2 1.3 0.2 Iris setosa 4.6 3.1 1.5 0.2 
Iris setosa 5.0 3.6 1.4 0.2 Iris setosa 5.4 3.9 
1.7 0.4 Iris setosa 4.6 3.4 1.4 0.3 Iris setosa 
5.0 3.4 1.5 0.2 Iris setosa 4.4 2.9 1.4 0.2 
Iris setosa 4.9 3.1 1.5 0.1 Iris setosa 5.4 3.7 
1.5 0.2 Iris setosa 4.8 3.4 1.6 0.2 Iris setosa 
4.8 3.0 1.4 0.1 Iris setosa 4.3 3.0 1.1 0.1 
Iris setosa 5.8 4.0 1.2 0.2 Iris setosa 5.7 4.4 
1.5 0.4 Iris setosa 5.4 3.9 1.3 0.4 Iris setosa 
5.1 3.5 1.4 0.3 Iris setosa 5.7 3.8 1.7 0.3 
Iris setosa 5.1 3.8 1.5 0.3 Iris setosa 5.4 3.4 
1.7 0.2 Iris setosa 5.1 3.7 1.5 0.4 Iris setosa 
4.6 3.6 1.0 0.2 Iris setosa 5.1 3.3 1.7 0.5 
Iris setosa 4.8 3.4 1.9 0.2 Iris setosa 5.0 3.0 
1.6 0.2 Iris setosa 5.0 3.4 1.6 0.4 Iris setosa 
5.2 3.5 1.5 0.2 Iris setosa 5.2 3.4 1.4 0.2 
Iris setosa 4.7 3.2 1.6 0.2 Iris setosa 4.8 3.1 
1.6 0.2 Iris setosa 5.4 3.4 1.5 0.4 Iris setosa 
5.2 4.1 1.5 0.1 Iris setosa 5.5 4.2 1.4 0.2 
Iris setosa 4.9 3.1 1.5 0.1 Iris setosa 5.0 3.2 
1.2 0.2 Iris setosa 5.5 3.5 1.3 0.2 Iris setosa 
4.9 3.1 1.5 0.1 Iris setosa 4.4 3.0 1.3 0.2 
Iris setosa 5.1 3.4 1.5 0.2 Iris setosa 5.0 3.5 
1.3 0.3 Iris setosa 4.5 2.3 1.3 0.3 Iris setosa 
4.4 3.2 1.3 0.2 Iris setosa 5.0 3.5 1.6 0.6 
Iris setosa 5.1 3.8 1.9 0.4 Iris setosa 4.8 3.0 
1.4 0.3 Iris setosa 5.1 3.8 1.6 0.2 Iris setosa 
4.6 3.2 1.4 0.2 Iris setosa 5.3 3.7 1.5 0.2 
Iris setosa 5.0 3.3 1.4 0.2 Iris setosa 7.0 3.2 
4.7 1.4 Iris versicolor 6.4 3.2 4.5 1.5 Iris versicolor 
6.9 3.1 4.9 1.5 Iris versicolor 5.5 2.3 4.0 1.3 
Iris versicolor 6.5 2.8 4.6 1.5 Iris versicolor 5.7 2.8 
4.5 1.3 Iris versicolor 6.3 3.3 4.7 1.6 Iris versicolor 
4.9 2.4 3.3 1.0 Iris versicolor 6.6 2.9 4.6 1.3 
Iris versicolor 5.2 2.7 3.9 1.4 Iris versicolor 5.0 2.0 
3.5 1.0 Iris versicolor 5.9 3.0 4.2 1.5 Iris versicolor 
6.0 2.2 4.0 1.0 Iris versicolor 6.1 2.9 4.7 1.4 
Iris versicolor 5.6 2.9 3.6 1.3 Iris versicolor 6.7 3.1 
4.4 1.4 Iris versicolor 5.6 3.0 4.5 1.5 Iris versicolor 
5.8 2.7 4.1 1.0 Iris versicolor 6.2 2.2 4.5 1.5 
Iris versicolor 5.6 2.5 3.9 1.1 Iris versicolor 5.9 3.2 
4.8 1.8 Iris versicolor 6.1 2.8 4.0 1.3 Iris versicolor 
6.3 2.5 4.9 1.5 Iris versicolor 6.1 2.8 4.7 1.2 
Iris versicolor 6.4 2.9 4.3 1.3 Iris versicolor 6.6 3.0 
4.4 1.4 Iris versicolor 6.8 2.8 4.8 1.4 Iris versicolor 
6.7 3.0 5.0 1.7 Iris versicolor 6.0 2.9 4.5 1.5 
Iris versicolor 5.7 2.6 3.5 1.0 Iris versicolor 5.5 2.4 
3.8 1.1 Iris versicolor 5.5 2.4 3.7 1.0 Iris versicolor 
5.8 2.7 3.9 1.2 Iris versicolor 6.0 2.7 5.1 1.6 
Iris versicolor 5.4 3.0 4.5 1.5 Iris versicolor 6.0 3.4 
4.5 1.6 Iris versicolor 6.7 3.1 4.7 1.5 Iris versicolor 
6.3 2.3 4.4 1.3 Iris versicolor 5.6 3.0 4.1 1.3 
Iris versicolor 5.5 2.5 4.0 1.3 Iris versicolor 5.5 2.6 
4.4 1.2 Iris versicolor 6.1 3.0 4.6 1.4 Iris versicolor 
5.8 2.6 4.0 1.2 Iris versicolor 5.0 2.3 3.3 1.0 
Iris versicolor 5.6 2.7 4.2 1.3 Iris versicolor 5.7 3.0 
4.2 1.2 Iris versicolor 5.7 2.9 4.2 1.3 Iris versicolor 
6.2 2.9 4.3 1.3 Iris versicolor 5.1 2.5 3.0 1.1 
Iris versicolor 5.7 2.8 4.1 1.3 Iris versicolor 6.3 3.3 
6.0 2.5 Iris virginica 5.8 2.7 5.1 1.9 Iris virginica 
7.1 3.0 5.9 2.1 Iris virginica 6.3 2.9 5.6 1.8 
Iris virginica 6.5 3.0 5.8 2.2 Iris virginica 7.6 3.0 
6.6 2.1 Iris virginica 4.9 2.5 4.5 1.7 Iris virginica 
7.3 2.9 6.3 1.8 Iris virginica 6.7 2.5 5.8 1.8 
Iris virginica 7.2 3.6 6.1 2.5 Iris virginica 6.5 3.2 
5.1 2.0 Iris virginica 6.4 2.7 5.3 1.9 Iris virginica 
6.8 3.0 5.5 2.1 Iris virginica 5.7 2.5 5.0 2.0 
Iris virginica 5.8 2.8 5.1 2.4 Iris virginica 6.4 3.2 
5.3 2.3 Iris virginica 6.5 3.0 5.5 1.8 Iris virginica 
7.7 3.8 6.7 2.2 Iris virginica 7.7 2.6 6.9 2.3 
Iris virginica 6.0 2.2 5.0 1.5 Iris virginica 6.9 3.2 
5.7 2.3 Iris virginica 5.6 2.8 4.9 2.0 Iris virginica 
7.7 2.8 6.7 2.0 Iris virginica 6.3 2.7 4.9 1.8 
Iris virginica 6.7 3.3 5.7 2.1 Iris virginica 7.2 3.2 
6.0 1.8 Iris virginica 6.2 2.8 4.8 1.8 Iris virginica 
6.1 3.0 4.9 1.8 Iris virginica 6.4 2.8 5.6 2.1 
Iris virginica 7.2 3.0 5.8 1.6 Iris virginica 7.4 2.8 
6.1 1.9 Iris virginica 7.9 3.8 6.4 2.0 Iris virginica 
6.4 2.8 5.6 2.2 Iris virginica 6.3 2.8 5.1 1.5 
Iris virginica 6.1 2.6 5.6 1.4 Iris virginica 7.7 3.0 
6.1 2.3 Iris virginica 6.3 3.4 5.6 2.4 Iris virginica 
6.4 3.1 5.5 1.8 Iris virginica 6.0 3.0 4.8 1.8 
Iris virginica 6.9 3.1 5.4 2.1 Iris virginica 6.7 3.1 
5.6 2.4 Iris virginica 6.9 3.1 5.1 2.3 Iris virginica 
5.8 2.7 5.1 1.9 Iris virginica 6.8 3.2 5.9 2.3 
Iris virginica 6.7 3.3 5.7 2.5 Iris virginica 6.7 3.0 
5.2 2.3 Iris virginica 6.3 2.5 5.0 1.9 Iris virginica 
6.5 3.0 5.2 2.0 Iris virginica 6.2 3.4 5.4 2.3 
Iris virginica 5.9 3.0 5.1 1.8 Iris virginica 原始 数据集 
协方差 矩阵 cov 方阵 4 * 4 0.68656811 0.0372787 1.27036233 
0.51534691 0.0372787 0.18792128 0.31673091 0.11574868 1.27036233 0.31673091 3.09637221 1.28912434 0.51534691 
0.11574868 1.28912434 0.57956557 特征值 featValue 4 4.20438706 0.24314579 0.07905128 0.02384304 
特征向量 矩阵 featVet 方阵 4 * 4 每 一列 特征值 
对应 一个 特征向量 0.36263433 0.6558202 0.58115529 0.3172613 0.08122848 0.73001455 0.59619427 
0.32408808 0.85629752 0.17703033 0.07265649 0.47972477 0.35868209 0.07509244 0.54911925 0.75111672 根据 
特征值 可以 看出 前 2个 特征值 占 比重 比 较大 
所以 选择 前 两个 特征值 对应 的 特征向量 组 正 
特征向量 矩阵 topX 特征向量 矩阵 redEigVects 0.36263433 0.6558202 0.08122848 0.73001455 
0.85629752 0.17703033 0.35868209 0.07509244 降 维 后的/nr 数据集 data m 
* n * redEigVects n * x = data m 
* x 原/n 数据集/i 乘以/v 选择/v 后的/nr 特征向量/n 矩阵/n 得到/v 
降/v 维/v 后的/nr 数据集/i 2.73363445 0.16331092 2.90803676 0.13076902 2.76491784 0.30475856 
2.7461081 0.34027983 2.29679724 0.75348469 2.83904793 0.0755604 2.64423265 0.17657389 2.90682876 0.56422248 
2.69199575 0.10050325 2.52354747 0.65790634 2.63112977 0.02770681 2.80576609 0.2213837 3.24397251 0.4961847 
2.65975154 1.19234788 2.39988069 1.3506441 2.63931625 0.82429682 2.66585361 0.32535115 2.21575231 0.88473854 
2.6045924 0.52665249 2.32791942 0.4034959 2.56060135 0.44614179 3.23368084 0.14876388 2.32098224 0.11122065 
2.37424051 0.02540228 2.52611151 0.13313497 2.48686648 0.14385237 2.57982864 0.38073938 2.65733554 0.32544096 
2.6511475 0.18387812 2.60676122 0.19129755 2.42744251 0.42388348 2.66443393 0.82625736 2.61352803 1.10619867 
2.69199575 0.10050325 2.88487621 0.08368008 2.64229784 0.61289151 2.69199575 0.10050325 3.00058136 0.47351799 
2.60796922 0.24215591 2.7877468 0.27747216 2.87158978 0.9264554 3.01682706 0.32751508 2.42325291 0.20183533 
2.22620519 0.44833111 2.73402967 0.23640219 2.55483086 0.5164587 2.85867044 0.21405407 2.5598109 0.59232432 
2.72173956 0.12127547 1.26785226 0.6856034 0.91488037 0.3200081 1.44683939 0.50410462 0.16172993 0.82370953 
1.06926494 0.07588127 0.62179131 0.41605337 1.0776218 0.28451223 0.7709864 0.99775123 1.02566911 0.22948323 
0.0293133 0.71825598 0.53097208 1.2595811 0.49291964 0.10079581 0.24356531 0.54627315 0.96584991 0.12363915 
0.19326273 0.24930664 0.91029555 0.46896498 0.6410186 0.35065097 0.21605396 0.33013295 0.92358198 0.54117049 
0.0243815 0.57940308 1.09805709 0.08353883 0.33869628 0.06521013 1.27799588 0.32739624 0.90223634 0.18162211 
0.69625299 0.15142829 0.88215497 0.33038151 1.31344654 0.24473051 1.53980154 0.2672176 0.79419518 0.16132435 
0.32586514 0.36249822 0.08938884 0.70028352 0.2108868 0.67507124 0.11653087 0.30974537 1.3600876 0.4210547 
0.56849174 0.48181501 0.78944915 0.19617369 1.20305302 0.40834664 0.7943564 0.3698655 0.22676318 0.26482035 
0.14548423 0.67770662 0.44401218 0.66800805 0.87209731 0.03293466 0.21028347 0.40044986 0.72660012 1.00517067 
0.33676147 0.50152775 0.31278815 0.20943212 0.35677921 0.28994282 0.62372612 0.02026425 0.92760343 0.74798588 
0.27927231 0.34524124 2.51362245 0.0132104 1.39516536 0.57474647 2.59899587 0.34018141 1.95251737 0.18183938 
2.33165373 0.04311693 3.37972129 0.54417028 0.49952523 1.18975088 2.91455996 0.35005959 2.301322 0.24692319 
2.90125455 0.77832912 1.64426335 0.2418257 1.78400546 0.21666042 2.14768656 0.21424748 1.32538608 0.77613761 
1.56638355 0.53929124 1.88686405 0.11830988 1.93129164 0.04002915 3.4724999 1.16855166 3.77710179 0.24961889 
1.27920388 0.7608497 2.41070022 0.37540785 1.17912435 0.60501224 3.48199197 0.4535556 1.36935481 0.20392106 
2.25831409 0.33226376 2.59703873 0.55659104 1.23933878 0.17879859 1.2724594 0.11608073 2.10450828 0.21178655 
2.37028851 0.46101268 2.82355495 0.37053698 3.2164011 1.36784329 2.14037649 0.21929579 1.42488684 0.14379794 
1.76088622 0.50197081 3.05957239 0.68324898 2.12711239 0.13811243 1.88690536 0.04744858 1.15056621 0.16395972 
2.0901974 0.37053399 2.29653466 0.18143615 1.90504456 0.4086246 1.39516536 0.57474647 2.54569629 0.27441977 
2.40178693 0.30222678 1.92627029 0.18675607 1.50709846 0.37513625 1.7461388 0.07811976 1.88372124 0.11544572 
1.37119204 0.28265084 降 维 后的/nr 数据集 四 PCA 算法 原理 
一句话 总结 这个 算法 的 精髓 数据集 D 乘 一个 
矩阵 W 使得 m * n 的 矩阵 变成 了 
m * x 的 矩阵 数据 从n维/nr 降 到了 x 
维 那么 如何 找到 这个 W 矩阵 很 关键 这里 
找 的 前 x 个 最大 的 特征值 对应 的 
特征向量 构成 的 矩阵 作为 w 我们 先 来看 一下 
W * D 两个 矩阵 相乘 的 意义 就是 将 
右 矩阵 投影 变换 在 以左 矩阵 作为 基 的 
空间 中 原理 中 也 说到 它 就是 一种 线性映射 
线性变换 这里 所说 的 左右 矩阵 跟 我们 数据 是 
列 向量 还是 行向量 有关 如果 是 行向量 那么久 恰好相反 
事实上 并 不用 在意 这个 是 行向量 还是 列 向量 
我们 只要 知道 矩阵 的 数据 是 在行 上 还是 
列上 维度 在行 上 还是 列上 就 很容易 搞清楚 为什么 
要 算 协方差 矩阵 根据 方差 和 协方差 的 计算 
公式 可以 看出 当 均值 为 0 的 时候 协方差 
矩阵 = A * AT 这也 是 为什么 要 对 
数据 要 进行 中心化 数据 减去 均值 的 原因 极大 
简化 了 协方差 矩阵 的 计算 协方差 矩阵 是个 方阵 
且 是个 对称 阵 它 的 的 对角线 就是 方差 
方差 是 反映 数据 信息量 的 大小 我们 通常 说 
的 方差 指 的 是 数据 在 平行 于 坐标 
轴上 的 信息量 传播 也 就是说 每个 维 度上 信息量 
的 传播 这是 在 二维 数据 上 如果 实在 多维 
数据 上 我们 要 度量 数据 在 多个 维度 上 
信息 的 传播 量 以及 非 平行 于 坐标轴 放 
方向 上 数据 的 传播 就是 要 用 协方差 表示 
从 矩阵 中 我们 可以 观察到 数据 在 每一个 维 
度上 的 传播 数据 的 协方差 确定 了 传播 的 
方向 而 方差 确定 了 传播 的 大小 当然 方差 
和 协方差 是 正 相关 的 所以 说 方差 最大 
的 方向 就是 协方差 最大 的 方向 而 协方差 的 
特征向量 topX 总是 指向 最大 方差 的 方向 特征向量 正交 
所以 作为 一组 变换 基 向量 很 合适 其实 这种 
逆向 推到 很 蹩脚 说实话 看 的 明白 很多 东西 
讲 不清楚 工科 出身 数学 功底 差 的 一塌糊涂 PCA 
思想 很 简单 用 Python 实现 也 很简单 几个 步骤 
照着 做 就 好了 但是 用 特征向量 做 线性变换 这是 
个 核心 的 地方 最后 的 目的 都是 向 最大 
方差 靠近 当然 为 什么 要求 协方差 矩阵 我 也是 
理解 了 很久 看到 的 一些 很好 的 文章 给 
大家 希望 对 大家 有所 启发 协方差 矩阵 的 集合 
解释 http / / www . cnblogs . com / 
nsnow / p / 4758202 . htmlPCA 的 数学原理 http 
/ / blog . csdn . net / xiaojidan2011 / 
article / details / 11595869 