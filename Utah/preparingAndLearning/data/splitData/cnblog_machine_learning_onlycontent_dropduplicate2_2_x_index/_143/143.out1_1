python 机器学习 实战 一 版权 声明 本文 为 博主 原创 
文章 转载 请 指明 转载 地址 www . cnblogs . 
com / fydeblog / p / 7140974 . html 前言 
这篇 notebook 是 关于 机器学习 中 监督 学习 的 k 
近邻 算法 将 介绍 2个 实例 分别 是 使用 k 
近邻 算法 改进 约会 网站 的 效果 和 手写 识别系统 
. 操作系统 ubuntu14 . 04       运行 环境 
anaconda python2 . 7 notebook       参考 书籍 
机器学习 实战           notebook   writer 
方阳k/nr 近邻 算法 kNN 的 工作 原理 存在 一个 样本数据 
集合 也 称作 训练样本 集 并且 样本 集中 的 每个 
数据 都 存在 标签 即 我们 知道 样本 集中 每 
一组 数据 与 所属 分类 的 对应 关系 输入 没有 
标签 的 新 数据 后 将 新 数据 的 每个 
特征 与 样本 集中 数据 对应 的 特征 进行 比较 
然后 算法 提取 样本 集中 特征 最 相似 的 分类 
标签 注意事项 在 这里 说一句 默认 环境 python2 . 7 
的 notebook 用 python3 . 6 的 会 出问题 还有 
我 的 目录 可能 跟 你们 的 不一样 你们 自己 
跑 的 时候 记得 改 目录 我会 把 notebook 和 
代码 以及 数据集 放到 结尾 的 百度 云盘 方便 你们 
下载 1 . 改进 约会 网站 的 匹配 效果 1 
1 . 准备 导入 数据 1 from numpy import * 
2 import operator 3 4 def createDataSet 5 group = 
array 1.0 1.1 1.0 1.0 0 0 0 0.1 6 
labels = A A B B 7 return group labels 
先 来点 开胃菜 在 上面 的 代码 中 我们 导入 
了 两个 模块 一个 是 科学计算 包 numpy 一个 是 
运算符 模块 在后面 都会 用到 在 createDataSet 函数 中 我们 
初始 化了 group labels 我们 将 做 这样 一件 事 
1.0 1.1 和 1.0 1.0 对应 属于 labels 中 A 
分类 0 0 和 0 0.1 对应 属于 labels 中的 
B 分类 我们 想 输入 一个 新的 二维 坐标 根据 
上面 的 坐标 来 判断 新的 坐标 属于 那 一类 
在这之前 我们 要 实现 k 近邻 算法 下面 就 开始 
实现 1 def classify0 inX dataSet labels k 2 dataSetSize 
= dataSet . shape 0 3 diffMat = tile inX 
dataSetSize 1 dataSet 4 sqDiffMat = diffMat * * 2 
5 sqDistances = sqDiffMat . sum axis = 1 6 
distances = sqDistances * * 0.5 7 s o r 
t e d D i s t I n d 
i c i e s = distances . argsort 8 
classCount = { } 9 for i in range k 
10 voteIlabel = labels s o r t e d 
D i s t I n d i c i 
e s i 11 classCount voteIlabel = classCount . get 
voteIlabel 0 + 1 12 sortedClassCount = sorted classCount . 
iteritems key = operator . itemgetter 1 reverse = True 
13 return sortedClassCount 0 0 代码 解析 函数 的 第一 
行 是 要 得到 数据集 的 数目 例如 group . 
shape 就是 4 2 shape 0 反应 数据集 的 行 
shape 1 反应 列数 函数 的 第二 行 是 array 
对应 相减 tile 会 生成 关于 Inx 的 dataSetSize 大小 
的 array 例如 InX 是 0 0 则 tile InX 
4 1 是 array 0 0 0 0 0 0 
0 0 然后 与 dataSet 对应 相减 得到 新的 array 
函数 的 第三 行 是 对 第二 步的/nr 结果 进行 
平方 算法 方便 下 一步 算 距离 函数 的 第四行 
是 进行 求和 注意 是 axis = 1 也 就是 
array 每个 二维 数组 成员 进行 求和 行 求和 如果 
是 axis = 0 就是 列 求和 第 五行 是 
进行 平方 距离 的 开 根号 以上 5行 实现 的 
是 距离 的 计算 下面 的 是 选出 距离 最小 
的 k 个 点 对 类别 进行 统计 返回 所占 
数目 多 的 类别 classCount 定义 为 存储 字典 里面 
有 A 和 B 它们 的 值 是 在前 k 
个 距离 最小 的 数据 集中 的 个数 本例 最后 
classCount = { A 1 B 2 } 函数 argsort 
是 返回 array 数组 从小到大 的 排列 的 序号 get 
函数 返回 字典 的 键值 由于 后面 加了 1 所以 
每次 出现 键值 就 加 1 就 可以 就 算出 
键值 出现 的 次数 里 最后 通过 sorted 函数 将 
classCount 字典 分解为 列表 sorted 函数 的 第二 个 参数 
导入 了 运算符 模块 的 itemgetter 方法 按照 第二 个 
元素 的 次序 即 数字 进行 排序 由于 此处 reverse 
= True 是 逆序 所以 按照 从大到/nr 小 的 次序 
排列 1 2 . 准备 数据 从 文本 中 解析 
数据 这 上面 是 k 近邻 的 一个 小 例子 
我 的 标题 还没 介绍 现在 来 介绍 标题 准备 
数据 一般 都 是从 文本文件 中 解析 数据 还是 从 
一个 例子 开始 吧 本次 例子 是 改进 约会 网站 
的 效果 我们 定义 三个 特征 来 判别 三种 类型 
的 人 特征 一 每年 获得 的 飞行 常客 里程数 
特征 二 玩 视频 游戏 所 耗时间 百分比 特征 三 
每周 消费 的 冰淇淋 公升 数 根据 以上 三 个 
特征 来 判断 一个 人 是否 是 自己 不 喜欢 
的 人 还是 魅力 一般 的 人 还是 极具 魅力 
的 人 于是 收集 了 1000个 样本 放在 datingTestSet2 . 
txt 中 共有 1000行 每 一行 有 四列 前 三列 
是 特征 后/f 三列/m 是/v 从属/v 那/r 一类/m 人/n 于是 
问题 来了 我们 这个 文本文件 的 输入 导入到 python 中 
来 处理 于是 需要 一个 转换 函数 file2matrix 函数 输入 
是 文件 名字 字符串 输出 是 训练样本 矩阵 特征 矩阵 
和类/nr 标签 向量 1 def file2matrix filename 2 fr = 
open filename 3 numberOfLines = len fr . readlines # 
get the number of lines in the file 4 returnMat 
= zeros numberOfLines 3 # prepare matrix to return 5 
classLabelVector = # prepare labels return 6 fr = open 
filename 7 index = 0 8 for line in fr 
. readlines 9 line = line . strip 10 listFromLine 
= line . split \ t 11 returnMat index = 
listFromLine 0 3 12 classLabelVector . append int listFromLine 1 
13 index + = 1 14 return returnMat classLabelVector 这个 
函数 比较简单 就不 详细 说明 里 这里 只 介绍 以下 
一些 函数 的 功能 吧 open 函数 是 打开 文件 
里面 必须 是 字符串 由于 后面 没 加 w 所以 
是 读 文件 readlines 函数 是 一次 读完 文件 通过 
len 函数 就 得到 文件 的 行数 zeros 函数 是 
生成 numberOfLines X 3 的 矩阵 是 array 型 的 
strip 函数 是 截掉 所有 的 回车符 split 函数 是以 
输入 参数 为 分隔符 输出 分割 后的/nr 数据 本例 是 
制表键 最后 输出 元素 列表 append 函数 是 向 列表 
中 加入 数据 1 3 . 分析 数据 使用 Matplotlib 
创建 散点图 首先 从上 一步 得到 训练样本 矩阵 和类/nr 标签 
向量 先 更换 一下 路径 cd / home / fangyang 
/ 桌面 / m a c h i n e 
l e a r n i n g i n 
a c t i o n / Ch02 / datingDataMat 
datingLabels = file2matrix datingTestSet2 . txt 1 import matplotlib 2 
import matplotlib . pyplot as plt 3 fig = plt 
. figure 4 ax = fig . add _ subplot 
111 5 ax . scatter datingDataMat 0 datingDataMat 1 15.0 
* array datingLabels 15.0 * array datingLabels # scatter 函数 
是 用来 画 散点图 的 6 plt . show 结果 
显示 1 4 . 准备 数据 归一化 处理 我们 从 
上图 可以 上 出 横坐标 的 特征值 是 远大于 纵坐标 
的 特征值 的 这样 再 算 新 数据 和 数据集 
的 数据 的 距离 时 数字 差值 最大 的 属性 
对 计算 结果 的 影响 最大 我们 就 可能会 丢 
失掉 其他 属性 例如 这个 例子 每年 获取 的 飞行 
常客 里程数 对 计算 结果 的 影响 远 大于 其余 
两个 特征 这 是 我们 不想 看到 的 所以 这里 
采用 归一化 数值 处理 也叫 特征 缩放 用于 将 特征 
缩放 到 同一个 范围内 本例 的 缩放 公式     
  newValue = oldValue min / max min 其中/r min/w 
和/c max/w 是/v 数据/n 集中/v 的/uj 最小/a 特征值/n 和/c 最大/a 
特征值/n 通过 该 公式 可将 特征 缩 放到 区间 0 
1 下面 是 特征 缩放 的 代码 1 def autoNorm 
dataSet 2 minVals = dataSet . min 0 3 maxVals 
= dataSet . max 0 4 ranges = maxVals minVals 
5 normDataSet = zeros shape dataSet 6 m = dataSet 
. shape 0 7 normDataSet = dataSet tile minVals m 
1 8 normDataSet = normDataSet / tile ranges m 1 
# element wise divide 9 return normDataSet ranges m i 
n V a l s n o r m D 
a t a e t 1000 X 3 是 归一化 
后的/nr 数据 range 1X3 是 特征 的 范围 差 即 
最大值 减去 最小值 minVals 1X3 是 最小值 原理 上面 已 
介绍 这里 不在 复述 1 5 . 测试 算法 作为 
完整 程序验证 分类器 好了 我们 已经 有了k/nr 近邻 算法 从 
文本 解析 出 数据 还有 归一化 处理 现在 可以 使用 
之前 的 数据 进行 测试 了 测试代码 如下 1 def 
datingClassTest 2 hoRatio = 0.50 3 datingDataMat datingLabels = file2matrix 
datingTestSet2 . txt # load data setfrom file 4 normMat 
ranges minVals = autoNorm datingDataMat 5 m = normMat . 
shape 0 6 numTestVecs = int m * hoRatio 7 
errorCount = 0.0 8 for i in range numTestVecs 9 
classifierResult = classify0 normMat i normMat numTestVecs m datingLabels numTestVecs 
m 3 10 print the classifier came back with % 
d the real answer is % d % classifierResult datingLabels 
i 11 if classifierResult = datingLabels i errorCount + = 
1.0 12 print the total error rate is % f 
% errorCount / float numTestVecs 13 print errorCount 这里 函 
数用 到里 之前 讲 的 三个 函数 file2matrix autoNorm 和 
classify0 . 这个 函数 将 数据集 分成 两个 部分 一 
部分 当作 分类器 的 训练样本 一 部分 当作 测试 样本 
通过 hoRatio 进行 控制 函数 hoRatio 是 0.5 它 与 
样本 总数 相乘 将 数据集 平分 如果 想把 训练样本 调 
大一些 可 增大 hoRatio 但 最好 不要 超过 0.8 以免 
测试 样本 过少 在 函数 的 最后 加了 错误 累加 
部分 预测 出来 的 结果 不 等于 实际 结果 errorCount 
就 加 1 然后 最后 除以 总数 就 得到 错误 
的 概率 说 了 这么 多 都还/nr 没有 测试 以下 
下面 来 测试 一下 先从 简单 的 开始 已将 上面 
的 函数 放在 kNN . py 中了 1 import kNN 
2 group labels = kNN . createDataSet group   # 
结果 在下 array 1 . 1.1 1 . 1 . 
0 . 0 . 0 . 0.1 labels   # 
结果 在下 A A B B 这个 小 例子 最开始 
提过 有/v 两个/m 分类/n A/w 和B/nr 通过 上 面的 group 
为 训练样本 测试 新的 数据 属于 那 一类 1 kNN 
. classify0 0 0 group labels 3 # 使用 k 
近邻 算法 进行 测试 B     # 结果 是 
B 分类 直观 地 可以 看出 0 0 是 与 
B 所在 的 样本 更近 下面 来 测试 一下 约会 
网站 的 匹配 效果 先将 文本 中 的 数据 导出来 
由于 前面 在 分析 数据 画图 的 时候 已经 用 
到里 file2matrix 这里 就 不 重复 用了 datingDataMat   # 
结果 在下 array 4.09200000 e + 04 8.32697600 e + 
00 9.53952000 e 01 1.44880000 e + 04 7.15346900 e 
+ 00 1.67390400 e + 00 2.60520000 e + 04 
1.44187100 e + 00 8.05124000 e 01 . . . 
2.65750000 e + 04 1.06501020 e + 01 8.66627000 e 
01 4.81110000 e + 04 9.13452800 e + 00 7.28045000 
e 01 4.37570000 e + 04 7.88260100 e + 00 
1.33244600 e + 00 datingLabels   # 由于 过长 只 
截取 一部分 详细 去看 jupyter notebook 然后 对 数据 进行 
归一化 处理 1 normMat ranges minVals = kNN . autoNorm 
datingDataMat # 使用 归一化 函数 normMatarray 0.44832535 0.39805139 0.56233353 0.15873259 
0.34195467 0.98724416 0.28542943 0.06892523 0.47449629 . . . 0.29115949 0.50910294 
0.51079493 0.52711097 0.43665451 0.4290048 0.47940793 0.3768091 0.78571804 rangesarray 9.12730000 e 
+ 04 2.09193490 e + 01 1.69436100 e + 00 
minValsarray 0 . 0 . 0.001156 最后 进行 测试 运行 
之前 的 测试函数 datingClassTest1 kNN . datingClassTest 由于 过长 只 
截取 一部分 详细 去看 jupyter notebook 可以 看到 上面 结果 
出现 错误 32个 错误率 6.4% 所以 这个 系统 还 算 
不错 1 6 . 系统 实现 我们 可以 看到 测试 
固然 不错 但 用户 交互式 很差 所以 结合 上面 我们 
要 写 一个 完整 的 系统 代码 如下 1 def 
classifyPerson 2 resultList = not at all in small doses 
in large doses 3 percentTats = float raw _ input 
percentage of time spent playing video games 4 ffMiles = 
float raw _ input frequent flier miles earned per year 
5 iceCream = float raw _ input liters of ice 
cream consumed per year 6 datingDataMat datingLabels = file2matrix datingTestSet2 
. txt 7 normMat ranges minVals = autoNorm datingDataMat 8 
inArr = array ffMiles percentTats iceCream 9 classifierResult = classify0 
inArr minVals / ranges normMat datingLabels 3 10 print You 
will probably like this person resultList classifierResult 1 运行 情况 
1 kNN . classifyPerson percentage of time spent playing video 
games 10   # 这里 的 数字 都是/nr 用户 自己 
输入 的 frequent flier miles earned per year 10000 liters 
of ice cream consumed per year 0.5 You will probably 
like this person in small doses 这个 就是 由 用户 
自己 输出 参数 并 判断 出 感兴趣 程度 非常 友好 
2 . 手写 识别系统 下面 再 介绍 一个 例子 也是 
用 k 近邻 算法 去 实现 对 一个 数字 的 
判断 首先/d 我们/r 是/v 将/d 宽/a 高是/nr 32X32/i 的/uj 像素/n 
的/uj 黑白/z 图像/n 转换/v 成/n 文本文件/n 存储/l 但 我们 知道 
文本文件 必须 转换成 特征向量 才能 进入 k 近邻 算法 中 
进行 处理 所以 我们 需要 一个 img2vector 函数 去 实现 
这个 功能 img2vector 代码 如下 1 def img2vector filename 2 
returnVect = zeros 1 1024 3 fr = open filename 
4 for i in range 32 5 lineStr = fr 
. readline 6 for j in range 32 7 returnVect 
0 32 * i + j = int lineStr j 
8 return returnVect 这个 函数 挺 简单 的 先用 zeros 
生成 1024 的 一维 array 然后 用 两重 循环 外循环 
以 行 递进 内循环 以 列 递进 将 32X32 的 
文本 数据 依次 赋值 给 returnVect 好了 转换 函数 写 
好了 说 一下 训练 集 和 测试 集 所有 的 
训练 集 都 放在 trainingDigits 文件夹 中 测试 集 放在 
testDigits 文件夹 中 训练 集 有 两千 个 样本 0 
9 各有 200个 测试 集 大约有 900个 样本 这里 注意 
一点 所有 在 文件 夹里 的 命名 方式 是 有 
要求 的 我们 是 通过 命名 方式 来 解析 出 
它 的 真实 数字 然后 与 通过 k 近邻 算法 
得出 的 结果 相对比 例如 945 . txt 这里 的 
数字 是 9 连接符 前面 的 数字 就是 这个 样本 
的 真实 数据 该 系统 实现 的 方法 与 前面 
的 约会 网站 的 类似 就 不多 说 了 系统 
测试代码 如下 1 def h a n d w r 
i t i n g C l a s s 
T e s t 2 hwLabels = 3 trainingFileList = 
listdir trainingDigits # load the training set 4 m = 
len trainingFileList 5 trainingMat = zeros m 1024 6 for 
i in range m 7 fileNameStr = trainingFileList i 8 
fileStr = fileNameStr . split . 0 # take off 
. txt 9 classNumStr = int fileStr . split _ 
0 10 hwLabels . append classNumStr 11 trainingMat i = 
img2vector trainingDigits / % s % fileNameStr 12 testFileList = 
listdir testDigits # iterate through the test set 13 errorCount 
= 0.0 14 mTest = len testFileList 15 for i 
in range mTest 16 fileNameStr = testFileList i 17 fileStr 
= fileNameStr . split . 0 # take off . 
txt 18 classNumStr = int fileStr . split _ 0 
19 vectorUnderTest = img2vector testDigits / % s % fileNameStr 
20 classifierResult = classify0 vectorUnderTest trainingMat hwLabels 3 21 print 
the classifier came back with % d the real answer 
is % d % classifierResult classNumStr 22 if classifierResult = 
classNumStr errorCount + = 1.0 23 print \ nthe total 
number of errors is % d % errorCount 24 print 
\ nthe total error rate is % f % errorCount 
/ float mTest 这里 的 listdir 是从 os 模块 导入 
的 它 的 功能 是 列出 给定 目 录下 的 
所有 文件名 以 字符串 形式 存放 输出 是 一个 列表 
这里 的 split 函数 是 要 分离 符号 得到 该 
文本 的 真实 数据 第一 个 split 函数 是以 小数点 
为 分隔符 例如 1 _ 186 . txt 就 变成 
了 1 _ 186 txt 然后 取 出 第一 个 
就 截 掉了 . txt 第二个 split 函数 是以 连接符 
_ 为 分隔符 就 截掉 后面 的 序号 剩下 前面 
的 字符 数据 1 然后 转成 int 型 就 得到 
了 它 的 真实 数据 其他 的 没什么 跟前 面 
一样 下面 开始 测试 1 kNN . h a n 
d w r i t i n g C l 
a s s T e s t 我们 可以 看到 
最后 结果 错误率 1.2% 可见 效果 还 不错 这里 把 
整个 kNN . py 文件 贴出来 主要 是 上面 已经 
介绍 的 函数 Input inX vector to compare to existing 
dataset 1xN dataSet size m data set of known vectors 
NxM labels data set labels 1xM vector k number of 
neighbors to use for comparison should be an odd number 
Output the most popular class label from numpy import * 
import operator from os import listdir def classify0 inX dataSet 
labels k dataSetSize = dataSet . shape 0 diffMat = 
tile inX dataSetSize 1 dataSet sqDiffMat = diffMat * * 
2 sqDistances = sqDiffMat . sum axis = 1 distances 
= sqDistances * * 0.5 s o r t e 
d D i s t I n d i c 
i e s = distances . argsort classCount = { 
} for i in range k voteIlabel = labels s 
o r t e d D i s t I 
n d i c i e s i classCount voteIlabel 
= classCount . get voteIlabel 0 + 1 sortedClassCount = 
sorted classCount . iteritems key = operator . itemgetter 1 
reverse = True return sortedClassCount 0 0 def createDataSet group 
= array 1.0 1.1 1.0 1.0 0 0 0 0.1 
labels = A A B B return group labels def 
file2matrix filename fr = open filename numberOfLines = len fr 
. readlines # get the number of lines in the 
file returnMat = zeros numberOfLines 3 # prepare matrix to 
return classLabelVector = # prepare labels return fr = open 
filename index = 0 for line in fr . readlines 
line = line . strip listFromLine = line . split 
\ t returnMat index = listFromLine 0 3 classLabelVector . 
append int listFromLine 1 index + = 1 return returnMat 
classLabelVector def autoNorm dataSet minVals = dataSet . min 0 
maxVals = dataSet . max 0 ranges = maxVals minVals 
normDataSet = zeros shape dataSet m = dataSet . shape 
0 normDataSet = dataSet tile minVals m 1 normDataSet = 
normDataSet / tile ranges m 1 # element wise divide 
return normDataSet ranges minVals def datingClassTest hoRatio = 0.50 # 
hold out 10% datingDataMat datingLabels = file2matrix datingTestSet2 . txt 
# load data setfrom file normMat ranges minVals = autoNorm 
datingDataMat m = normMat . shape 0 numTestVecs = int 
m * hoRatio errorCount = 0.0 for i in range 
numTestVecs classifierResult = classify0 normMat i normMat numTestVecs m datingLabels 
numTestVecs m 3 print the classifier came back with % 
d the real answer is % d % classifierResult datingLabels 
i if classifierResult = datingLabels i errorCount + = 1.0 
print the total error rate is % f % errorCount 
/ float numTestVecs print errorCount def classifyPerson resultList = not 
at all in small doses in large doses percentTats = 
float raw _ input percentage of time spent playing video 
games ffMiles = float raw _ input frequent flier miles 
earned per year iceCream = float raw _ input liters 
of ice cream consumed per year datingDataMat datingLabels = file2matrix 
datingTestSet2 . txt normMat ranges minVals = autoNorm datingDataMat inArr 
= array ffMiles percentTats iceCream classifierResult = classify0 inArr minVals 
/ ranges normMat datingLabels 3 print You will probably like 
this person resultList classifierResult 1 def img2vector filename returnVect = 
zeros 1 1024 fr = open filename for i in 
range 32 lineStr = fr . readline for j in 
range 32 returnVect 0 32 * i + j = 
int lineStr j return returnVect def h a n d 
w r i t i n g C l a 
s s T e s t hwLabels = trainingFileList = 
listdir trainingDigits # load the training set m = len 
trainingFileList trainingMat = zeros m 1024 for i in range 
m fileNameStr = trainingFileList i fileStr = fileNameStr . split 
. 0 # take off . txt classNumStr = int 
fileStr . split _ 0 hwLabels . append classNumStr trainingMat 
i = img2vector trainingDigits / % s % fileNameStr testFileList 
= listdir testDigits # iterate through the test set errorCount 
= 0.0 mTest = len testFileList for i in range 
mTest fileNameStr = testFileList i fileStr = fileNameStr . split 
. 0 # take off . txt classNumStr = int 
fileStr . split _ 0 vectorUnderTest = img2vector testDigits / 
% s % fileNameStr classifierResult = classify0 vectorUnderTest trainingMat hwLabels 
3 print the classifier came back with % d the 
real answer is % d % classifierResult classNumStr if classifierResult 
= classNumStr errorCount + = 1.0 print \ nthe total 
number of errors is % d % errorCount print \ 
nthe total error rate is % f % errorCount / 
float mTest 结尾 至此 这个 k 近邻 算法 的 介绍 
到 这里 就 结束 了 希望 这 篇 文章 对 
你 的 学习 有 帮助 百度 云 链接   https 
/ / pan . baidu . com / s / 
1 O u y O u G i 9 r 
8 e a P 9 g g l A z 
B g 