机器学习 中 神经网络 算法 可以说 是 当下 使用 的 最 
广泛 的 算法 神经 网络 的 结构 模仿 自 生物 
神经网络 生物 神经 网络 中 的 每个 神经元 与 其他 
神经元 相连 当 它 兴奋 时 想 下 一级 相连 
的 神经元 发送 化学物质 改变 这些 神经元 的 电位 如果 
某 神经元 的 电位 超过 一个 阈值 则 被 激活 
否则 不 被 激活 误差 逆 传播 算法 error back 
propagation 是 神经 网络 中 最有 代表性 的 算法 也是 
使用 最多 的 算法 之一 误差 逆 传播 算法 理论 
推导 误差 逆 传播 算法 error back propagation 简称 BP 
网络 算法 而 一般 在 说 BP 网络 算法 时 
默认 指用 BP 算法 训练 的 多层 前馈 神经网络 下面 
是 一个 简单 的 BP 神经网络 示意图 其 拥有 一个 
输入 层 一个 隐含 层 一个 输出 层 推导 中 
采用 这种 简单 的 三层 的 神经 网络 定义 相关 
的 一些 变量 如下 假设有 d 个 输入 神经元 有 
l 个 输出 神经元 q 个 隐含 层 神经元 设 
输出 层 第 j 个 神经元 的 阈值 为   
θ j 设 隐含 层 第 h 个 神经元 的 
阈值 为   γ h 输入 层 第 i 个 
神经元 与 隐含 层 第 h 个 神经元 之间 的 
连接 权为 Vih   隐含 层 第 h 个 神经元 
与 输出 层 第 j 个 神经元 之间 的 连接 
权为 Whj   记 隐含 层 第 h 个 神经元 
接收 到 来自 于 输入 层 的 输入 为   
α h 记 输出 层 第 j 个 神经元 接收 
到 来自 于 隐含 层 的 输入 为   β 
j 其中 bh   为 隐含 层 第 h 个 
神经元 的 输出 理论 推导 在 神经 网络 中 神经元 
接收 到 来自 来自 其他 神经元 的 输入 信号 这些 
信号 乘以 权重 累 加到 神经元 接收 的 总 输入 
值 上 随后 与 当前 神经元 的 阈值 进行 比较 
然后 通过 激活 函数 处理 产生 神经元 的 输出 激活 
函数 理想 的 激活 函数 是 阶跃 函数 0 对应 
神经元 抑制 1 对应 神经元 兴奋 然而 阶跃 函数 的 
缺点 是 不连续 不可导 且 不光滑 所以 常用 sigmoid 函数 
作为 激活 函数 代替 阶跃 函数 如 下图 分别 是 
阶跃 函数 和 sigmoid 函数 阶跃 函数 sigmoid 函数 对于 
一个 训练 例 xk yk 假设 神经 网络 的 输出 
为 Yk 则 输出 可表示 为 f * * * 
表示 激活 函数 默认 全部 的 激活 函数 都为 sigmoid 
函数 则 可以 计算 网络 上 xk yk 的 均方差 
误差 为 乘以 1/2 是 为了 求导 时能 正好 抵 
消掉 常数 系数 现在 从 隐含 层 的 第 h 
个 神经元 看 输入 层 总共有 d 个 权重 传递 
参数 传给 他 它 又 总共有 l 个 权重 传递 
参数 传给 输出 层 自身 还有 1 个 阈值 所以 
在 我们 这个 神经 网络 中 一个 隐含 层 神经元 
有 d + l + 1 个 参数 待 确定 
输出 层 每个 神经元 还有 一个 阈值 所以 总共有 l 
个 阈值 最后 总共有 d + l + 1 * 
q + l 个 待定 参数 首先 随机 给出 这些 
待定 的 参数 后面 通过 BP 算法 的 迭代 这些 
参数 的 值 会 逐渐 收敛 于 合适 的 值 
那时 神经网络 也就 训练 完成 了 任意 权重 参数 的 
更新 公式 为 下面 以 隐含 层 到 输出 层 
的 权重 参数 whj 为例 说明 我们 可以 按照 前面 
给出 的 公式 求出 均方差 误差 Ek 期望 其为 0 
或者 为 最小值 而 BP 算法 基于 梯度 下 降法 
gradient descent 来 求解 最优 解 以 目标 的 负 
梯度方向 对 参数 进行 调整 通过 多次 迭代 新的 权重 
参数 会 逐渐 趋 近于 最优 解 对于 误差   
Ek   给定 学习率 learning rate 即 步长 η 有 
再 看一下 参数 的 传递 方向 首先 whj 影响 到 
了 输出 层 神经元 的 输入 值   β j 
  然后 影响 到 输出 值 Yjk   然后再 影响 
到 误差 Ek 所以 可以 列出 如下 关系式 根据 输出 
层 神经元 的 输入 值   β j   的 
定义 得到 对于 激活 函数 sigmoid 函数 很 容易 通过 
求导 证得 下面 的 性质 使用 这个 性质 进行 如下 
推导 令 又 由于 所以 由 前面 的 定义 有 
所以 把 这个 结果 结合 前面 的 几个 式子 代入 
  得到 所以 OK 上面 这个 式子 就是 梯 度了 
通过 不停 地 更新 即 梯度 下 降法 就 可实现 
权重 更新 了 推导 到 这里 就 结束 了 再来 
解释一下 式子 中 各个 元素 的 意义 η 为 学习率 
即 梯度 下降 的 补偿 为 神经网络 输出 层 第 
j 个 神经元 的 输出 值 为 给出 的 训练 
例 xk yk 的 标志 label 即 训练 集 给出 
的 正确 输出 为 隐含 层 第 h 个 神经元 
的 输出 类似 可得 其中 这 部分 的 解法 与 
前面 的 推导 方法 类似 不做 赘述 接下来 是 代码 
部分 这段 代码 网上 也 有 不少 地方 可以 看到 
后面 会 简单 介绍 一下 程序 完整 程序 文件名 NN 
_ Test . py # _ * _ coding utf 
8 _ * _ import numpy as np def tanh 
x return np . tanh x def tanh _ derivative 
x return 1 np . tanh x * np . 
tanh x # sigmod 函数 def logistic x return 1 
/ 1 + np . exp x # sigmod 函数 
的 导数 def logistic _ derivative x return logistic x 
* 1 logistic x class NeuralNetwork def _ _ init 
_ _ self layers activation = tanh if activation = 
= logistic self . activation = logistic self . activation 
_ deriv = logistic _ derivative elif activation = = 
tanh self . activation = tanh self . activation _ 
deriv = tanh _ derivative # 随机 产生 权重 值 
self . weights = for i in range 1 len 
layers 1 # 不算 输入 层 循环 self . weights 
. append 2 * np . random . random layers 
i 1 + 1 layers i + 1 1 * 
0.25 self . weights . append 2 * np . 
random . random layers i + 1 layers i + 
1 1 * 0.25 # print self . weights def 
fit self x y learning _ rate = 0.2 epochs 
= 10000 x = np . atleast _ 2d x 
temp = np . ones x . shape 0 x 
. shape 1 + 1 temp 0 1 = x 
x = temp y = np . array y for 
k in range epochs # 循环 epochs 次 i = 
np . random . randint x . shape 0 # 
随机 产生 一个 数 对应 行号 即 数据集 编号 a 
= x i # 抽出 这行 的 数据集 # 迭代 
将 输出 数据 更新 在 a 的 最后 一行 for 
l in range len self . weights a . append 
self . activation np . dot a l self . 
weights l # 减去 最后 更新 的 数据 得到 误差 
error = y i a 1 deltas = error * 
self . activation _ deriv a 1 # 求 梯度 
for l in range len a 2 0 1 deltas 
. append deltas 1 . dot self . weights l 
. T * self . activation _ deriv a l 
# 反向 排序 deltas . reverse # 梯度 下 降法 
更新 权值 for i in range len self . weights 
layer = np . atleast _ 2d a i delta 
= np . atleast _ 2d deltas i self . 
weights i + = learning _ rate * layer . 
T . dot delta def predict self x x = 
np . array x temp = np . ones x 
. shape 0 + 1 temp 0 1 = x 
a = temp for l in range 0 len self 
. weights a = self . activation np . dot 
a self . weights l return a 简要 说明 def 
tanh x return np . tanh x def tanh _ 
derivative x return 1 np . tanh x * np 
. tanh x # sigmod 函数 def logistic x return 
1 / 1 + np . exp x # sigmod 
函数 的 导数 def logistic _ derivative x return logistic 
x * 1 logistic x 分别 表示 两种 激活 函数 
tanh 函数 和 sigmoid 函数 以 及其 的 导数 有关 
激活 函数 前文 有 提及 if activation = = logistic 
self . activation = logistic self . activation _ deriv 
= logistic _ derivative elif activation = = tanh self 
. activation = tanh self . activation _ deriv = 
tanh _ derivative activation 参数 决定 了 激活 函数 的 
种类 是 tanh 函数 还是 sigmoid 函数 self . weights 
= for i in range 1 len layers 1 # 
不算 输入 层 循环 self . weights . append 2 
* np . random . random layers i 1 + 
1 layers i + 1 1 * 0.25 self . 
weights . append 2 * np . random . random 
layers i + 1 layers i + 1 1 * 
0.25 # print self . weights 以 隐含 层 前后 
层 计算 产生 权重 参数 参数 初始 时 随机 取值 
范围 是 0.25 0.25 x = np . atleast _ 
2d x temp = np . ones x . shape 
0 x . shape 1 + 1 temp 0 1 
= x x = temp y = np . array 
y 创建 并 初始化 要 使用 的 变量 for k 
in range epochs # 循环 epochs 次 i = np 
. random . randint x . shape 0 # 随机 
产生 一个 数 对应 行号 即 数据集 编号 a = 
x i # 抽出 这行 的 数据集 # 迭代 将 
输出 数据 更新 在 a 的 最后 一行 for l 
in range len self . weights a . append self 
. activation np . dot a l self . weights 
l # 减去 最后 更新 的 数据 得到 误差 error 
= y i a 1 deltas = error * self 
. activation _ deriv a 1 # 求 梯度 for 
l in range len a 2 0 1 deltas . 
append deltas 1 . dot self . weights l . 
T * self . activation _ deriv a l # 
反向 排序 deltas . reverse # 梯度 下 降法 更新 
权值 for i in range len self . weights layer 
= np . atleast _ 2d a i delta = 
np . atleast _ 2d deltas i self . weights 
i + = learning _ rate * layer . T 
. dot delta 进行 BP 神经 网络 的 训练 的 
核心 部分 在 代码 中有 相应 注释 def predict self 
x x = np . array x temp = np 
. ones x . shape 0 + 1 temp 0 
1 = x a = temp for l in range 
0 len self . weights a = self . activation 
np . dot a self . weights l return a 
这段 是 预测 函数 其实 就是 将 测试 集 的 
数据 输入 然后 正向 走一遍 训 练好 的 网络 最后 
再 返回 预测 结果 测试 验证 函数 # _ * 
_ coding utf 8 _ * _ from NN _ 
Test import NeuralNetwork import numpy as np nn = NeuralNetwork 
2 2 1 tanh x = np . array 0 
0 0 1 1 0 1 1 y = np 
. array 0 1 1 0 nn . fit x 
y for i in 0 0 0 1 1 0 
1 1 print i nn . predict i 程序 中 
测试 的 是 异或 关系 下面 是 运行 结果 0 
0 array 0.01628435 0 1 array 0.99808061 1 0 array 
0.99808725 1 1 array 0.03867579 显然 与 标准 异或 关系 
近似 