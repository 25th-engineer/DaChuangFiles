要说 2017年 什么 技术 最 火爆 无疑 是 google 领衔 
的 深度 学习 开源 框架 Tensorflow 本文 简述 一下 深度 
学习 的 入门 例子 MNIST 深度 学习 简单 介绍 首先 
要 简单 区别 几个 概念 人工智能 机器学习 深度 学习 神经网络 
这 几个 词 应该 是 出现 的 最为 频繁 的 
但是 他们 有 什么 区别 呢 人工智能 人类 通过 直觉 
可以 解决 的 问题 如 自然语言 理解 图像识别 语音 识别 
等 计算机 很难 解决 而 人工智能 就是 要 解决 这 
类 问题 机器学习 如果 一个 任务 可以 在 任务 T 
上 随着 经验 E 的 增加 效果 P 也 随之 
增加 那么 就 认为 这 个 程序 可以 从 经验 
中 学习 深度 学习 其 核心 就是 自动 将 简单 
的 特征 组合成 更加 复杂 的 特征 并用 这些 特征 
解决问题 神经网络 最初 是 一个 生物学 的 概念 一般 是 
指 大脑 神经元 触点 细胞 等 组成 的 网络 用于 
产生 意识 帮助 生物 思考 和 行动 后来 人工智能 受 
神经 网络 的 启发 发展 出了 人工神经网络 来 一张 图 
就 比较 清楚 了 如 下图 MNIST 解析 MNIST 是 
深度 学习 的 经典 入门 demo 他 是由 6 万张 
训练 图片 和1/nr 万张 测试 图片 构成 的 每张 图片 
都是 28 * 28 大小 如 下图 而且都 是 黑白 
色 构成 这里 的 黑色 是 一个 0 1 的 
浮点数 黑色 越深 表示 数值 越 靠近 1 这些 图片 
是 采集 的 不同 的 人手 写 从0到/nr 9 的 
数字 TensorFlow 将 这个 数据集 和 相关 操作 封装 到了 
库 中 下面 我们 来 一步步 解读 深度 学习 MNIST 
的 过程 上图 就是 4张 MNIST 图片 这些 图片 并 
不是 传统 意义 上 的 png 或者 jpg 格式 的 
图片 因为 png 或者 jpg 的 图片格式 会 带 有 
很多 干扰 信息 如 数据块 图 片头 图 片尾 长度 
等等 这些 图片 会被 处理 成很/nr 简易 的 二维 数组 
如图 可以 看到 矩阵 中有 值 的 地方 构成 的 
图形 跟 左边 的 图形 很 相似 之所以 这样 做 
是 为了 让 模型 更简单 清晰 特征 更 明显 我们 
先看 模型 的 代码 以及 如何 训练 模型 mnist = 
input _ data . read _ data _ sets FLAGS 
. data _ dir one _ hot = True # 
x 是 特征值 x = tf . placeholder tf . 
float32 None 784 # w 表示 每一个 特征值 像素点 会 
影响 结果 的 权重 W = tf . Variable tf 
. zeros 784 10 b = tf . Variable tf 
. zeros 10 y = tf . matmul x W 
+ b # 是 图片 实际 对应 的 值 y 
_ = tf . placeholder tf . float32 None 10 
cross _ entropy = tf . reduce _ mean tf 
. nn . softmax _ cross _ entropy _ with 
_ logits labels = y _ logits = y train 
_ step = tf . train . G r a 
d i e n t D e s c e 
n t O p t i m i z e 
r 0.5 . minimize cross _ entropy sess = tf 
. I n t e r a c t i 
v e e s s i o n tf . 
global _ variables _ initializer . run # mnist . 
train 训练 数据 for _ in range 1000 batch _ 
xs batch _ ys = mnist . train . next 
_ batch 100 sess . run train _ step feed 
_ dict = { x batch _ xs y _ 
batch _ ys } #/i 取得/v y/w 得/ud 最大/a 概率/n 
对应/vn 的/uj 数组/n 索/nr 引来/v 和y_/nr 的/uj 数组/n 索引/nr 对比/v 
如果 索引 相同 则 表示 预测 正确 correct _ prediction 
= tf . equal tf . arg _ max y 
1 tf . arg _ max y _ 1 accuracy 
= tf . reduce _ mean tf . cast correct 
_ prediction tf . float32 print sess . run accuracy 
feed _ dict = { x mnist . test . 
images y _ mnist . test . labels } 首先 
第一行 是 获取 MNIST 的 数据集 我们 逐一 解释一下 x 
图片 的 特征值 这里 使用 了 一个 28 * 28 
= 784列 的 数据 来 表示 一个 图片 的 构成 
也 就是说 每 一个 点 都是 这个 图片 的 一个 
特征 这个 其实 比较 好 理解 因为 每 一个 点 
都会 对 图片 的 样子 和 表达 的 含义 有影响 
只是 影响 的 大小 不同 而已 至于 为什么 要 将 
28 * 28 的 矩阵 摊平 成为 一个 1行 784列 
的 一维 数组 我 猜测 可能 是 因为 这样 做 
会 更加 简单 直观 W 特征值 对应 的 权重 这个 
值 很重要 因为 我们 深度 学习 的 过程 就是 发现 
特征 经过 一系列 训练 从而 得出 每 一个 特征 对 
结果 影响 的 权重 我们 训练 就是 为了 得到 这个 
最佳 权重 值 b 偏置 量 是 为了 去 线性 
话 我 不是 太 清楚 为什么 需要 这个 值 y 
预测 的 结果 单个 样本 被 预测 出来 是 哪个 
数字 的 概率 比如 有 可能 结果 是 1.07476616 4.54194021 
2.98073649 7.42985344 3.29253793 1.96750617   8.59438515 6.65950203 1.68721473 0.9658531 则 
分别 表示 是 0 1 2 3 4 5 6 
7 8 9 的 概率 然后 会 取 一个 最大值 
来 作为 本次 预测 的 结果 对于 这个 数 组 
来说 结果 是 6 8.59438515 y _ 真实 结果 来自 
MNIST 的 训练 集 每一个 图片 所 对应 的 真实 
值 如果 是 6 则 表示 为 0 0 0 
0 0 1 0 0 0 再 下面 两行 代码 
是 损失 函数 交叉 熵 和 梯度 下降 算法 通过 
不断 的 调整 权 重和 偏置 量 的 值 来 
逐步 减小 根据 计算 的 预测 结果 和 提供 的 
真实 结果 之间 的 差异 以 达到 训练 模型 的 
目的 算法 确定 以后 便 可以 开始 训练 模型 了 
如下 for _ in range 1000 batch _ xs batch 
_ ys = mnist . train . next _ batch 
100 sess . run train _ step feed _ dict 
= { x batch _ xs y _ batch _ 
ys } mnist . train . next _ batch 100 
是从 训练 集 里 一次 提取 100张 图片 数据 来 
训练 然后 循环 1000次 以 达到 训练 的 目的 之后 
的 两行 代码 都有 注释 不再 累 述 我们 看 
最后 一行 代码 print sess . run accuracy feed _ 
dict = { x mnist . test . images y 
_ mnist . test . labels } mnist . test 
. images 和 mnist . test . labels 是 测试 
集 用来 测试 accuracy 是 预测 准确率 当 代码运行 起来 
以后 我们 发现 准确率 大概在 92% 左右 浮动 这个 时候 
我们 可能 想 看看 到底 是 什么样 的 图片 让 
预测 不准 则 添加 如下 代码 for i in range 
0 len mnist . test . images result = sess 
. run correct _ prediction feed _ dict = { 
x np . array mnist . test . images i 
y _ np . array mnist . test . labels 
i } if not result print 预测 的 值 是 
sess . run y feed _ dict = { x 
np . array mnist . test . images i y 
_ np . array mnist . test . labels i 
} print 实际 的 值 是 sess . run y 
_ feed _ dict = { x np . array 
mnist . test . images i y _ np . 
array mnist . test . labels i } one _ 
pic _ arr = np . reshape mnist . test 
. images i 28 28 pic _ matrix = np 
. matrix one _ pic _ arr dtype = float 
plt . imshow pic _ matrix pylab . show break 
print sess . run accuracy feed _ dict = { 
x mnist . test . images y _ mnist . 
test . labels } for 循环 内 指明 一旦 result 
为 false 就 表示 出现 了 预测 值 和 实际 
值 不 符合 的 图片 然后 我们 把 值 和 
图片 分别 打 印出来 看看 预测 的 值 是 1.82234347 
4.87242508 2.63052988 6.56350136 2.73666072 2.30682945 8.59051228 7.20512581 1.45552373 0.90134078 对应 
的 是 数字 6 实际 的 值 是 0 . 
0 . 0 . 0 . 0 . 1 . 
0 . 0 . 0 . 0 . 对应 的 
是 数字 5 我们 再 来看 看 图片 是 什么 
样子 的 的确 像 5 又像 6 总体 来说 只有 
92% 的 准确率 还是 比较 低 的 后续 会 解析 
一下 比较 适合 识别 图片 的 卷积 神经网络 准确率 可以 
达到 99% 以上 一些 体会 与 感想 我 本人 是 
一名 iOS 开发 也是 迎着 人工智能 的 浪潮 开始 一路 
学习 我 觉得 人工智能 终将 改变 我们 的 生活 也 
会 成为 未来 的 一个 热门 学科 这 一个 多 
月 的 自学 下来 我 觉得 最为 困难 的 是 
克服 自己 的 畏难 情绪 因为 我 完全 没有 AI 
方面 的 任何 经验 而且 工作 年限 太久 线性代数 概率论 
等 知识 早已 还给 老师 所以 在 开始 的 时候 
总是 反反复复 不停 犹豫 纠结 到底 要 不要 把 时间 
花费 在 研究 深度 学习 上面 但是 后来 一想 假如 
我 不学 AI 的 东西 若干 年后 AI 发展 越发 
成熟 到时候 想学 也会 难以 跟上 步伐 而且 让 电脑 
学会 思考 这 本身 就是 一件 很 让人 兴奋 的 
事情 既然 想学 有 什么 理由 不 去学 呢 与 
大家 共勉 参考 文章 https / / zhuanlan . zhihu 
. com / p / 25482889https / / hit scir 
. gitbooks . io / neural networks and deep learning 
zh _ cn / content / chap1 / c1s0 . 
html 