注 在 吴恩 达 老师 讲 的 机器学习 课程 中 
最开始 介绍 神经 网络 的 应用 时就/nr 介绍 了 含有 
一个 隐藏 层 的 神经 网络 可以 解决 异或 问题 
而 这是 单层 神经网络 也叫 感知机 做 不到 了 当时 
就 觉得 非常 神奇 之后 就 一直 打算 自己 实现 
一下 一直 到 一周 前 才 开始 动手 实现 自己 
参考 机器学习 课程 中 数字 识别 的 作业 题写 了 
代码 对于 作业 题中 给 的 数字 图片 可以 达到 
95% 左右 的 识别 准确度 但是 改成 训练 异或 的 
网络 时 怎么 也 无法 得到 正确 的 结果 后来/t 
查了/nr 一些/m 资料/n 才/d 发现/v 是/v 因为/c 自己/r 有/v 一个/m 
参数设置/n 的/uj 有问题/i 而且 学习率 过小 迭代 的 次数 也 
不够 总之 异或 逻辑 的 实现 不仅 对于 人工神经网络 这一 
算法 是 一大 突破 对于 我 自己 对 误差 反向 
传播 算法 Error Back Propagation BP 的 理解 也 是 
非常 重要 的 过程 因此 记录 于此 什么 是 异或 
在 数字 逻辑 中 异 或是 对 两个 运算元 的 
一种 逻辑 分析 类型 符号为 XOR 或 EOR 或 ⊕ 
与 一般 的 或 OR 不同 当 两两 数值 相同 
时为 否 而 数值 不 同时 为真 异或 的 真值表 
如下 XOR truth t a b l e I n 
p u t O u t p u t A 
B 0 0 0 0 1 1 1 0 1 
1 1 0 0 false1 true 据说 在 人工神经网络 artificial 
neural network ANN 发展 初期 由于 无法 实现 对 多层 
神经网络 包括 异或 逻辑 的 训练 而 造成 了 一场 
ANN 危机 到最后 BP 算法 的 出现 才让 训练 带有 
隐藏 层 的 多层 神经 网络 成为 可能 因此 异或 
的 实现 在 ANN 的 发展史 是 也是 具有 里程碑 
意义 的 异或 之所以 重要 是 因为 它 相对于 其他 
逻辑关系 例 如与 AND 或 OR 等 异 或是 线性 
不可分的 如 下图 在 实际 应用 中 异或门 Exclusive OR 
gate XOR gate 是 数字 逻辑 中 实现 逻辑 异或 
的 逻辑门 这一 函数 能 实现 模 为 2 的 
加法 因此 异或门 可以 实现 计算机 中的 二进制 加法 异或 
的 神经 网络 结构 在 机器学习 课程 中 使用 了 
AND 与 NOR 或非 和 OR 或 的 组合 实现 
了 XNOR 同 或 与 我们 要 实现 的 异或 
XOR 正好 相反 因此 还 是 可以 采用 课程 中 
的 神经 网络结构 如 下图 如果 算上 输入 层 我们 
的 网络 共有 三层 如下 图 所示 其中 第 1层 
和第/nr 2层 中的 1 分别 是 这 两层 的 偏置 
单元 连 线上 是 连接 前后 层 的 参数 输入 
我们 一 共有 四个 训练样本 每个 样本 有 两个 特征 
分别 是 0 0 1 0 0 1 1 1 
理想 输出 参考 上面 的 真值表 样本 中 两个 特征 
相同 时为 0 相异 为 1 参数 随机 初始化 范围 
为 1 1 关于 神经 网络 的 基础 知识 以及 
前 向 传播 反向 传播 的 实现 请 参考 下面 
两 篇文章 写 的 非常 精彩 机器学习 公开课 笔记 4 
神经网络 Neural Network 表示 机器学习 公开课 笔记 5 神经网络 Neural 
Network 学习 代码 原生态 的 代码 下面 的 实现 是 
完全 根据 自己 的 理解 和对/nr 机器学习 课程 中 作业题 
的 模仿 而 写成 的 虽然 代码 质量 不是 非常 
高 但是 算法 的 所有 细节 都 展示 出来 了 
在 66 69 70行 的 注释 是 我 之前 没有 
得到 正确 结果 的 三个 原因 其中 epsilon 确定 的 
是 随机 初始化 参数 的 范围 例如 epsilon = 1 
参数 范围 就是 1 1 1 # * coding utf 
8 * 2 3 Created on Tue Apr 4 10 
47 51 2017 4 5 @ author xin 6 7 
# Neural Network for XOR 8 import numpy as np 
9 import matplotlib . pyplot as plt 10 11 HIDDEN 
_ LAYER _ SIZE = 2 12 INPUT _ LAYER 
= 2 # input feature 13 NUM _ LABELS = 
1 # output class number 14 X = np . 
array 0 0 0 1 1 0 1 1 15 
y = np . array 0 1 1 0 16 
17 18 def rand _ initialize _ weights L _ 
in L _ out epsilon 19 20 Randomly initialize the 
weights of a layer with L _ in 21 incoming 
connections and L _ out outgoing connections 22 23 Note 
that W should be set to a matrix of size 
L _ out 1 + L _ in as 24 
the first column of W handles the bias terms 25 
26 epsilon _ init = epsilon 27 W = np 
. random . rand L _ out 1 + L 
_ in * 2 * epsilon _ init epsilon _ 
init 28 return W 29 30 31 def sigmoid x 
32 return 1.0 / 1.0 + np . exp x 
33 34 35 def sigmoid _ gradient z 36 g 
= np . multiply sigmoid z 1 sigmoid z 37 
return g 38 39 40 def nn _ cost _ 
function theta1 theta2 X y 41 m = X . 
shape 0 # m = 4 42 # 计算 所有 
参数 的 偏 导数 梯度 43 D _ 1 = 
np . zeros theta1 . shape # Δ _ 1 
44 D _ 2 = np . zeros theta2 . 
shape # Δ _ 2 45 h _ total = 
np . zeros m 1 # 所有 样本 的 预测 
值 m * 1 probability 46 for t in range 
m 47 a _ 1 = np . vstack np 
. array 1 X t t + 1 . T 
# 列 向量 3 * 1 48 z _ 2 
= np . dot theta1 a _ 1 # 2 
* 1 49 a _ 2 = np . vstack 
np . array 1 sigmoid z _ 2 # 3 
* 1 50 z _ 3 = np . dot 
theta2 a _ 2 # 1 * 1 51 a 
_ 3 = sigmoid z _ 3 52 h = 
a _ 3 # 预测值 h 就 等于 a _ 
3 1 * 1 53 h _ total t 0 
= h 54 delta _ 3 = h y t 
t + 1 . T # 最后 一层 每 一个 
单元 的 误差 δ _ 3 1 * 1 55 
delta _ 2 = np . multiply np . dot 
theta2 1 . T delta _ 3 sigmoid _ gradient 
z _ 2 # 第二层 每 一个 单元 的 误差 
不包括 偏置 单元 δ _ 2 2 * 1 56 
D _ 2 = D _ 2 + np . 
dot delta _ 3 a _ 2 . T # 
第二层 所有 参数 的 误差 1 * 3 57 D 
_ 1 = D _ 1 + np . dot 
delta _ 2 a _ 1 . T # 第一层 
所有 参数 的 误差 2 * 3 58 theta1 _ 
grad = 1.0 / m * D _ 1 # 
第一层 参数 的 偏 导数 取 所有 样本 中 参数 
的 均值 没有 加 正则 项 59 theta2 _ grad 
= 1.0 / m * D _ 2 60 J 
= 1.0 / m * np . sum y * 
np . log h _ total np . array 1 
y * np . log 1 h _ total 61 
return { theta1 _ grad theta1 _ grad 62 theta2 
_ grad theta2 _ grad 63 J J h h 
_ total } 64 65 66 theta1 = rand _ 
initialize _ weights INPUT _ LAYER HIDDEN _ LAYER _ 
SIZE epsilon = 1 # 之前 的 问题 之一 epsilon 
的 值 设置 的 太小 67 theta2 = rand _ 
initialize _ weights HIDDEN _ LAYER _ SIZE NUM _ 
LABELS epsilon = 1 68 69 iter _ times = 
10000 # 之前 的 问题 之二 迭代 次数 太少 70 
alpha = 0.5 # 之前 的 问题 之三 学习率 太小 
71 result = { J h } 72 theta _ 
s = { } 73 for i in range iter 
_ times 74 cost _ fun _ result = nn 
_ cost _ function theta1 = theta1 theta2 = theta2 
X = X y = y 75 theta1 _ g 
= cost _ fun _ result . get theta1 _ 
grad 76 theta2 _ g = cost _ fun _ 
result . get theta2 _ grad 77 J = cost 
_ fun _ result . get J 78 h _ 
current = cost _ fun _ result . get h 
79 theta1 = alpha * theta1 _ g 80 theta2 
= alpha * theta2 _ g 81 result J . 
append J 82 result h . append h _ current 
83 # print i J h _ current 84 if 
i = = 0 or i = = iter _ 
times 1 85 print theta1 theta1 86 print theta2 theta2 
87 theta _ s theta1 _ + str i = 
theta1 . copy 88 theta _ s theta2 _ + 
str i = theta2 . copy 89 90 plt . 
plot result . get J 91 plt . show 92 
print theta _ s 93 print result . get h 
0 result . get h 1 下面 是 输出 结果 
# 随机 初始化 得到 的 参数 theta1 array 0.18589823 0.77059558 
0.62571502 0.79844165 0.56069914 0.21090703 theta2 array 0.1327994 0.59513332 0.34334931 # 
训练 后 得到 的 参数 theta1 array 3.90903729 7.44497437 7.20130773 
3.76429211 6.93482723 7.21857912 theta2 array 6.5739346 13.33011993 13.3891608 # 同上 
第一 次 迭代 和 最后 一次 迭代 得到 的 参数 
{ theta1 _ 0 array 0.18589823 0.77059558 0.62571502 0.79844165 0.56069914 
0.21090703 theta2 _ 9999 array 6.5739346 13.33011993 13.3891608 theta1 _ 
9999 array 3.90903729 7.44497437 7.20130773 3.76429211 6.93482723 7.21857912 theta2 _ 
0 array 0.1327994 0.59513332 0.34334931 } # 预测值 h 第 
1个 array 里 是 初始 参数 预测 出来 的 值 
第 2个 array 中 是 最后 一次 得到 的 参数 
预测 出来 的 值 array 0.66576877 0.69036552 0.64994307 0.67666546 array 
0.00245224 0.99812746 0.99812229 0.00215507 下面 是 随着 迭代 次数 的 
增加 代价 函数值 J θ 的 变化 情况 更加 精炼 
的 代码 下面 这段 代码 是 我 在 排除 之前 
自己 的 代码 中 的 问题 时 在 Stack Overflow 
上 发现 的 发帖 的 人也 碰到 了 同样 的 
问题 但 原因 不 一样 他 的 代码 里 有 
一点 小 问题 已经 修正 这段 代码 相对于 我 自己 
的 原生态 代码 有了 非常 大 的 改进 没有 限定 
层数 和 每层 的 单元 数 代码 本身 也 比较 
简洁 说明 由于 第 44行 传 的 参数 是 该 
层 的 a 值 而 不是 z 值 所以 第 
11行 需要 做出 一点 修改 其实 直接 传递 a 值 
是 一种 更 方便 的 做法 1 # * coding 
utf 8 * 2 3 import numpy as np 4 
import matplotlib . pyplot as plt 5 6 7 def 
sigmoid x 8 return 1 / 1 + np . 
exp x 9 10 def s _ prime z 11 
return np . multiply z 1.0 z # 修改 的 
地方 12 13 def init _ weights layers epsilon 14 
weights = 15 for i in range len layers 1 
16 w = np . random . rand layers i 
+ 1 layers i + 1 17 w = w 
* 2 * epsilon epsilon 18 weights . append np 
. mat w 19 return weights 20 21 def fit 
X Y w 22 # now each para has a 
grad equals to 0 23 w _ grad = np 
. mat np . zeros np . shape w i 
24 for i in range len w # len w 
equals the layer number 25 m n = X . 
shape 26 h _ total = np . zeros m 
1 # 所有 样本 的 预测 值 m * 1 
probability 27 for i in range m 28 x = 
X i 29 y = Y 0 i 30 # 
forward propagate 31 a = x 32 a _ s 
= 33 for j in range len w 34 a 
= np . mat np . append 1 a . 
T 35 a _ s . append a # 这里 
保存 了 前 L 1层 的 a 值 36 z 
= w j * a 37 a = sigmoid z 
38 h _ total i 0 = a 39 # 
back propagate 40 delta = a y . T 41 
w _ grad 1 + = delta * a _ 
s 1 . T # L 1层 的 梯度 42 
# 倒过来 从 倒数 第二 层 开始 到 第二层 结束 
不包括 第一层 和 最后 一层 43 for j in reversed 
range 1 len w 44 delta = np . multiply 
w j . T * delta s _ prime a 
_ s j # 这里 传递 的 参数 是 a 
而 不是 z 45 w _ grad j 1 + 
= delta 1 * a _ s j 1 . 
T 46 w _ grad = w _ grad i 
/ m for i in range len w 47 J 
= 1.0 / m * np . sum Y * 
np . log h _ total np . array 1 
Y * np . log 1 h _ total 48 
return { w _ grad w _ grad J J 
h h _ total } 49 50 51 X = 
np . mat 0 0 52 0 1 53 1 
0 54 1 1 55 Y = np . mat 
0 1 1 0 56 layers = 2 2 1 
57 epochs = 5000 58 alpha = 0.5 59 w 
= init _ weights layers 1 60 result = { 
J h } 61 w _ s = { } 
62 for i in range epochs 63 fit _ result 
= fit X Y w 64 w _ grad = 
fit _ result . get w _ grad 65 J 
= fit _ result . get J 66 h _ 
current = fit _ result . get h 67 result 
J . append J 68 result h . append h 
_ current 69 for j in range len w 70 
w j = alpha * w _ grad j 71 
if i = = 0 or i = = epochs 
1 72 # print w _ grad w _ grad 
73 w _ s w _ + str i = 
w _ grad 74 75 76 plt . plot result 
. get J 77 plt . show 78 print w 
_ s 79 print result . get h 0 result 
. get h 1 下面 是 输出 的 结果 # 
第一 次 迭代 和 最后 一次 迭代 得到 的 参数 
{ w _ 4999 matrix 1.51654104 e 04 2.30291680 e 
04 6.20083292 e 04 9.15463982 e 05 1.51402782 e 04 
6.12464354 e 04 matrix 0.0004279 0.00051928 0.00042735 w _ 0 
matrix 0.00172196 0.0010952 0.00132499 0.00489422 0.00489643 0.00571827 matrix 0.02787502 0.01265985 
0.02327431 } # 预测值 h 第 1个 array 里 是 
初始 参数 预测 出来 的 值 第 2个 array 中 
是 最后 一次 得到 的 参数 预测 出来 的 值 
array 0.45311095 0.45519066 0.4921871 0.48801121 array 0.00447994 0.49899856 0.99677373 0.50145936 
观察 上面 的 结果 最后 一次 迭代 得到 的 结果 
并 不是 我们 期待 的 结果 也 就是 第 1 
4个 值 接近于 0 第 2 3个 值 接近于 1 
下面 是 代价 函数值 J θ 随着 迭代 次数 增加 
的 变化 情况 从 上图 可以 看到 J θ 的 
值 从 2000 以后 就 一直 停留 在 0.35 左右 
因此 整个 网络 有 可能 收敛 到 了 一个 局部 
最优 解 也 有可能 是 迭代 次数 不够 导致 的 
将 迭代 次数 改成 10000 后 即 epochs = 10000 
基本上 都是/nr 可以 得到 预期 的 结果 的 其实在 迭代 
次数 少 的 情况 下 也 有可能 得到 预期 的 
结果 这 应该 主要 取决于 初始 的 参数 经验 小结 
通过 阅读 别人 的 代码 确实 是 提高 自己 编 
程 能力 的 一种 重要 方法 例如 通过 比较 自己 
的 原生态 版 代码 和 其他 人 写 的 代码 
就 可以 找 出 自己 的 不足之处 其中 最大 的 
收获 是 数据结构/n 对于/p 代码/n 的/uj 结构/n 和/c 逻辑/n 都/d 
非常/d 重要/a 比如 我 自己 写 的 时候 每 一层 
是 分开 的 但 后面 的 代码 中将 整个 网络 
一起 初始化 并 保存 在 一个 list 中 这就 提高 
了 代码 的 可扩展 能力 也 使得 代码 更加 简洁 
此外 要 准确 的 理解 各种 算法 的 细节 最好 
的 方式 就是 自己 实现 一次 完 参考文献 https / 
/ zh . wikipedia . org / wiki / % 
E 9% 80% BB % E 8% BE % 91% 
E 5% BC % 82% E 6% 88% 96https / 
/ zh . wikipedia . org / wiki / % 
E 5% BC % 82% E 6% 88% 96% E 
9% 97% A8https / / muxuezi . github . io 
/ posts / 10 from the perceptron to artificial neural 
networks . htmlhttp / / stackoverflow . com / q 
/ 36369335 / 2803344Coursera Andrew Ng   公开课 第 四周 
第 五周 