学习 了 那么 多 机器学习 模型 一切 都是/nr 为了 实践 
动手 自己 写 写 这些 模型 的 实现 对 自己 
很 有 帮助 的 坚持 共勉 本文 主要 致力于 总结 
贝叶斯 实战 中 程序 代码 的 实现 python 及 朴素 
贝叶斯 模型 原理 的 总结 python 的 numpy 包 简化 
了 很多 计算 另外 本人 推荐 使用 pandas 做 数据 
统计 一 引言 让 你 猜测 一个 身高 2.16 的 
人 的 职业 你 一般 会 猜测 他 是 篮球 
运动员 这个 原理 就是 朴素 贝叶斯 原理 因为 篮球 运动员 
大多 身高 很高 所以 这个 人 具有 篮球 运动员 的 
条件 则 猜测 他 是 篮球 运动员 同理 另一个 升高 
1.58 的 人 你 应该 不会 猜 他 是 篮球 
运动员 二 理论 条件 贝叶斯 公式 p Ci | x 
y = p x y | Ci * p Ci 
/ p x y 计算 每个 类别 的 概率 若 
p C1 | x y   p ~ C1 | 
x y 则 类别 属于 类 C1 否则 不 属于 
类 C1 程序 中 在 模型 训练 的 时候 只需要 
先在 训练样本 中计 算好 先验概率 p Ci 和 条件概率   
p x y | Ci 即可 因为 p x y 
不随 Ci 变化 不影响 p Ci | x y 的 
最好 大小 注 条件 贝叶斯 是 保证 条件 之间 独立 
的 文档 分类 中 是 假设 一个 词汇 出现 与 
其他词汇 是否 出现 无关 然而 同一 主题 的 词汇 一起 
出现 的 概率 很高 存在 关联 所以 这个 假设 过于 
简单 尽管如此 然而 事实 表明 朴素 贝叶斯 的 效果 还 
很好 三 实战 1 文本 分类 应用 过滤 恶意 留言 
等 下面 是 二分 类 问题 文档 只能 属于 0 
和1/nr 两个 类别 1 载入 数据集 6条 文本 及 它们 
各自 的 类别 这 6条 文本 作为 训练 集 from 
numpy import * def loadDataSet postingList = my dog has 
flea problems help please maybe not take him to dog 
park stupid my dalmation is so cute I love him 
stop posting stupid worthless garbage mr licks ate my steak 
how to stop him quit buying worthless dog food stupid 
classVec = 0 1 0 1 0 1 # 1 
is abusive 0 not return postingList classVec2 创建 词汇表 利用 
集合 结构 内 元素 的 唯一性 创建 一个 包含 所有 
词汇 的 词表 def createVocabList dataSet vocabSet = set # 
create empty set for document in dataSet vocabSet = vocabSet 
| set document # union of the two sets return 
list vocabSet 3 把 输入 文本 根据 词表 转化 为 
计算机 可 处理 的 01 向量 形式 eq 测试 文本 
1 love my dalmation 词汇表 cute love help garbage quit 
I problems is park stop flea dalmation licks food not 
him buying posting has worthless ate to maybe please dog 
how stupid so take mr steak my 向 量化 结果 
0 1 0 0 0 0 0 0 0 0 
0 1 0 0 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 
0 1 def setOfWords2Vec vocabList inputSet returnVec = 0 * 
len vocabList for word in inputSet if word in vocabList 
returnVec vocabList . index word = 1 else print the 
word % s is not in my Vocabulary % word 
return returnVec4 训练 模型 在 训练 样本 中 计算 先验概率 
p Ci 和 条件概率   p x y | Ci 
本/r 实例/n 有/v 0/m 和1/nr 两个/m 类别/n 所以 返回 p 
x y | 0 p x y | 1 和p/nr 
Ci 此处 有 两个 改进 的 地方 1 若有 的 
类别 没有 出现 其 概率 就是 0 会 十分 影响 
分类器 的 性能 所以 采取 各 类别 默认 1次 累加 
总 类别 两类 次数 2 这样 不 影响 相对 大小 
2 若 很小 是 数字 相乘 则 结果 会 更小 
再 四舍五入 存在 误差 而且 会 造成 下 溢出 采取 
取 log 乘法 变为 加法 并且 相对 大 小趋势 不变 
def trainNB0 trainMatrix trainCategory numTrainDocs = len trainMatrix numWords = 
len trainMatrix 0 pAbusive = sum trainCategory / float numTrainDocs 
p0Num = ones numWords p1Num = ones numWords # change 
to ones p0Denom = 2.0 p1Denom = 2.0 # change 
to 2.0 for i in range numTrainDocs if trainCategory i 
= = 1 p1Num + = trainMatrix i p1Denom + 
= sum trainMatrix i else p0Num + = trainMatrix i 
p0Denom + = sum trainMatrix i p1Vect = log p1Num 
/ p1Denom # change to log p0Vect = log p0Num 
/ p0Denom # change to log return p0Vect p1Vect pAbusive5 
分类 根据 计算 后 哪个 类别 的 概率 大 则 
属于 哪个 类别 def classifyNB vec2Classify p0Vec p1Vec pClass1 p1 
= sum vec2Classify * p1Vec + log pClass1 # element 
wise mult p0 = sum vec2Classify * p0Vec + log 
1.0 pClass1 if p1 p0 return 1 else return 06 
测试函数 加载 数据集 + 提炼 词表 训练 模型 根据 六条 
训练 集 计算 先验概率 和 条件概率 测试 模型 对 训练 
两条 测试 文本 进行 分类 def testingNB listOPosts listClasses = 
loadDataSet myVocabList = createVocabList listOPosts trainMat = for postinDoc in 
listOPosts trainMat . append setOfWords2Vec myVocabList postinDoc p0V p1V pAb 
= trainNB0 array trainMat array listClasses testEntry = love my 
dalmation thisDoc = array setOfWords2Vec myVocabList testEntry print testEntry classified 
as classifyNB thisDoc p0V p1V pAb testEntry = stupid garbage 
thisDoc = array setOfWords2Vec myVocabList testEntry print testEntry classified as 
classifyNB thisDoc p0V p1V pAb 缺点 词表 只能 记录 词汇 
是否 出现 不能 体现 这个 词汇 出现 的 次数 改进 
方法 采 用词 袋 模型 见 下面 垃圾邮件 分类 实战 
四 实战 2 垃圾邮件 分类 1 对 邮件 的 文本 
划分 成 词汇 长度 小于 2 的 默认 为 不是 
词汇 过滤掉 即可 返回 一串 小写 的 拆分 后的/nr 邮件 
信息 def textParse bigString # input is big string # 
output is word list import re listOfTokens = re . 
split r \ W * bigString return tok . lower 
for tok in listOfTokens if len tok 2 2 文档 
词 袋 模型 使用 数组 代替 集合 数据结构 可以 保存 
词汇 频率 信息 def bagOfWords2VecMN vocabList inputSet returnVec = 0 
* len vocabList for word in inputSet if word in 
vocabList returnVec vocabList . index word + = 1 return 
returnVec3 输入/v 为/p 25/m 封/q 正常/d 邮件/n 和/c 25/m 封/q 
垃圾邮件/n 50/m 封/q 邮件/n 中/f 随机/d 选取/v 10/m 封/q 作为/v 
测试/vn 样本/n 剩余 40 封 作为 训练样本 训练 模型 40 
封 训练样本 训 练出 先验概率 和 条件概率 测试 模型 遍历 
10个 测试 样本 计算 垃圾邮件 分类 的 正确率 def spamTest 
docList = classList = fullText = for i in range 
1 26 wordList = textParse open email / spam / 
% d . txt % i . read # print 
wordList docList . append wordList fullText . extend wordList classList 
. append 1 wordList = textParse open email / ham 
/ % d . txt % i . read docList 
. append wordList fullText . extend wordList classList . append 
0 vocabList = createVocabList docList # create vocabulary trainingSet = 
range 50 testSet = # create test set for i 
in range 10 randIndex = int random . uniform 0 
len trainingSet testSet . append trainingSet randIndex del trainingSet randIndex 
trainMat = trainClasses = for docIndex in trainingSet # train 
the classifier get probs trainNB0 trainMat . append bagOfWords2VecMN vocabList 
docList docIndex trainClasses . append classList docIndex p0V p1V pSpam 
= trainNB0 array trainMat array trainClasses errorCount = 0 for 
docIndex in testSet # classify the remaining items wordVector = 
bagOfWords2VecMN vocabList docList docIndex if classifyNB array wordVector p0V p1V 
pSpam = classList docIndex errorCount + = 1 print classification 
error docList docIndex print the error rate is float errorCount 
/ len testSet # return vocabList fullText 五 小结 上面 
我 处理 的 样本 的 属性值 都是 分 类型 的 
然而 数值 型 的 朴素 贝叶斯 能 处理 吗 1 
朴素 贝叶斯 处理 数值 型 数据 的 方法 1 区间 
离散化 设 阈值 分段 2 高斯 化 求出 概率密度函数 假设 
变量 服从 正态分布 根据 已有 变量 统计 均值 和 方差 
得出 概率密度函数 这样 就 解决 了 计算 连续 值 作为 
分类 的 条件 概率值 参考 http / / blog . 
mythsman . com / p = 26832 除 0 问题 
Laplace 校准 所有 计算 均 加 一 总 类别 数目 
加 n 3 下 溢出 很小 的 值 相乘 四舍五入 
误差 采用 log 乘法 变 相加 4 移除 停用词 也 
可以 提高 文本 分类 的 性能 参考 书 机器学习 实战 
Peter 