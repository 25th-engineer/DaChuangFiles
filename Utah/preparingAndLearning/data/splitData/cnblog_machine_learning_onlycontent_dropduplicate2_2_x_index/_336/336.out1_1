作者 无影 随想 时间 2016年 1月 出处 https / / 
zhaokv . com / machine _ learning / 2016/01 / 
learning from imbalanced data . html 声明 版权所有 转载 请 
注明 出处 这几年来 机器 学习 和 数据挖掘 非常 火热 它们 
逐渐 为 世界 带来 实际 价值 与此同时 越来越 多 的 
机器学习 算法 从 学术界 走向 工业界 而 在 这个 过程 
中 会 有 很多 困难 数据 不 平衡 问题 虽然 
不是 最难 的 但 绝对 是 最重要 的 问题 之一 
一 数据 不 平衡 在 学术 研究 与 教学 中 
很多 算法 都 有一个 基本 假设 那 就是 数据分布 是 
均匀 的 当 我们 把 这些 算法 直接 应用 于 
实际 数据 时 大多数 情况下 都 无法 取得 理想 的 
结果 因为 实际 数据 往往 分布 得很 不均匀 都会 存在 
长尾 现象 也 就是 所谓 的 二八 原理 下图 是 
新浪 微博 交互 分布 情况 可以 看到 大 部分 微博 
的 总 互动 数 被 转发 评论 与 点 赞 
数量 在 0 5 之间 交互 数多 的 微博 多于 
100 非常 之 少 如果 我们 去 预测 一条 微博 
交互 数 所在 档位 预测器 只 需要 把 所有 微博 
预测 为 第一 档 0 5 就能 获得 非常 高的/nr 
准确率 而 这样 的 预测器 没有 任何 价值 那 如何 
来 解决 机器学习 中 数据 不 平衡 问题 呢 这 
便是 这 篇 文章 要 讨论 的 主要 内容 严格地 
讲 任何 数据 集上 都有 数据 不 平衡 现象 这 
往往 由 问题 本身 决定 的 但 我们 只 关注 
那些 分布 差别 比较 悬殊 的 另外 虽然 很多 数据集 
都 包含 多个 类别 但 这里 着重 考虑 二 分类 
因为 解决 了 二 分类 中 的 数据 不 平衡 
问题 后 推而广之 就能 得到 多 分类 情况下 的 解决方案 
综上 这 篇 文章 主要 讨论 如何 解决 二 分类 
中 正负 样本 差 两个 及 以上 数量级 情况下 的 
数据 不 平衡 问题 不平衡 程度 相同 即 正负 样本 
比例 类似 的 两个 问题 解决 的 难易 程度 也 
可能 不同 因为 问题 难易 程度 还 取决于 我们 所 
拥有 数 据有 多大 比如 在 预测 微博 互动 数 
的 问题 中 虽然 数据 不 平衡 但 每个 档位 
的 数据 量 都 很大 最少 的 类别 也有 几万个 
样本 这样 的 问题 通常 比较 容易 解决 而在 癌症 
诊断 的 场景 中 因为 患 癌症 的 人 本来 
就 很少 所以 数据 不但 不 平衡 样本数 还 非常 
少 这样 的 问题 就 非常 棘手 综上 可以 把 
问题 根据 难度 从小 到 大排 个 序 大 数据 
+ 分布 均衡 大 数据 + 分布 不 均衡 小 
数据 + 数据 均衡 小 数据 + 数据 不 均衡 
对于 需要 解决 的 问题 拿到 数据 后 首先 统计 
可用 训练 数据 有 多大 然后 再 观察 数据 分布 
情况 经验 表明 训练 数据 中 每个 类别 有 5000个 
以上 样本 数据量 是 足够 的 正负 样本 差 一个 
数量级 以内 是 可以 接受 的 不太 需要 考虑 数据 
不 平衡 问题 完全 是 经验 没有 理论 依据 仅供参考 
二 如何 解决 解决 这 一 问题 的 基本 思路 
是 让 正负 样本 在 训练 过程 中 拥有 相同 
的 话语权 比如 利用 采样 与 加权 等 方法 为了 
方便 起见 我们 把 数据 集中 样本 较多 的 那一类 
称为 大众 类 样本 较少 的 那一类 称为 小众 类 
1 . 采样 采样 方法 是 通过 对 训练 集 
进行 处理 使其 从不 平衡 的 数据 集 变成 平衡 
的 数据集 在 大部分 情况下 会 对 最终 的 结果 
带来 提升 采样 分 为上 采样 Oversampling 和下/nr 采样 Undersampling 
上 采样 是 把 小 种类 复制 多份 下 采样 
是从 大众 类 中 剔除 一些 样本 或者说 只从 大众 
类 中 选取 部分 样本 随机 采样 最大 的 优点 
是 简单 但 缺点 也 很明显 上 采样 后的/nr 数据 
集中 会 反复 出现 一些 样本 训练 出来 的 模型 
会 有 一定 的 过拟合 而下 采样 的 缺点 显而易见 
那 就是 最终 的 训练 集 丢失 了 数据 模型 
只学到 了 总体 模式 的 一部分 上 采样 会把 小众 
样本 复制 多份 一个点 会在 高维空间 中 反复 出现 这 
会 导致 一个 问题 那 就是 运气 好 就能 分对 
很 多点 否则 分 错 很 多点 为了 解决 这 
一 问题 可以 在 每次 生成 新 数据 点 时 
加入 轻微 的 随机 扰动 经验 表明 这种 做法 非常 
有效 因为 下 采样 会 丢失 信息 如何 减少 信息 
的 损失 呢 第 一种 方法 叫做 EasyEnsemble 利用 模型 
融合 的 方法 Ensemble 多次下 采样 放回 采样 这样 产生 
的 训练 集 才 相互 独立 产生 多个 不同 的 
训练 集 进而 训练 多个 不同 的 分类器 通过 组合 
多个 分类器 的 结果 得到 最终 的 结果 第二 种 
方法 叫做 BalanceCascade 利用 增量 训练 的 思想 Boosting 先 
通过 一次 下 采样 产生 训练 集 训练 一个 分类器 
对于 那些 分类 正确 的 大众 样 本不 放回 然后 
对 这个 更小 的 大众 样本 下 采样 产生 训练 
集 训练 第二个 分类器 以此类推 最终 组合 所有 分类器 的 
结果 得到 最终 结果 第三 种 方法 是 利用 KNN 
试图 挑选 那些 最 具 代表性 的 大众 样本 叫做 
NearMiss 这类 方法 计算 量 很大 感 兴趣 的 可以 
参考 Learning from Imbalanced Data 这篇 综述 的 3 . 
2.1 节 2 . 数据 合成 数据 合成 方法 是 
利用 已有 样本 生成 更多 样本 这类 方法 在 小 
数据 场景 下 有 很多 成功 案例 比如 医学 图像 
分析 等 其中 最 常见 的 一种 方法 叫做 SMOTE 
它 利用 小众 样本 在 特征 空间 的 相似性 来 
生成 新 样本 对于 小众 样本 $ x _ i 
\ in _ { \ min } $ 从它/nr 属于 
小众 类 的 K 近邻 中 随机 选取 一个 样本点 
$ \ hat { x } _ i $ 生成 
一个 新 的 小众 样本 $ x _ { new 
} $ $ x _ { new } = x 
_ i + \ hat { x } x _ 
i \ times \ delta $ 其中 $ \ delta 
\ in 0 1 $ 是 随机数 上图 是 SMOTE 
方法 在 $ K = 6 $ 近邻 下 的 
示意图 黑色 方格 是 生成 的 新 样本 SMOTE 为 
每个 小众 样本 合成 相同 数量 的 新 样本 这 
带来 一些 潜在 的 问题 一 方面 是 增加 了 
类 之间 重叠 的 可能性 另一方面 是 生成 一些 没有 
提供 有益 信息 的 样本 为了 解决 这个 问题 出现 
两种 方法 Borderline SMOTE 与 ADASYN Borderline SMOTE 的 解决 
思路 是 寻找 那些 应该 为之 合成 新 样本 的 
小众 样本 即为 每个 小众 样本 计算 K 近邻 只为 
那些 K 近邻 中有 一半 以上 大众 样本 的 小众 
样本 生成 新 样本 直观 地 讲 只为 那些 周围 
大部分 是 大众 样本 的 小众 样本 生成 新 样本 
因为 这些 样本 往往 是 边界 样本 确定 了 为 
哪些 小众 样本 生成 新 样本 后 再利用 SMOTE 生成 
新 样本 ADASYN 的 解决 思路 是 根据 数据 分布 
情况 为 不同 小众 样本 生成 不同 数量 的 新 
样本 首先 根据 最终 的 平衡 程度 设定 总共 需要 
生成 的 新 小众 样本 数量 $ G $ 然后 
为 每个 小众 样本 $ x _ i $ 计算 
分布 比例 $ \ Gamma _ i $ $ \ 
Gamma _ i = \ frac { \ Delta _ 
i / K } { Z } $ 其中 $ 
\ Gamma _ i $ 是 $ x _ i 
$ K 近邻 中 大众 样本 的 数量 $ Z 
$ 用来 归一化 使得 $ \ sum \ Gamma _ 
i = 1 $ 最后 为 小众 样本 $ x 
_ i $ 生成 新 样本 的 个数 为 $ 
g _ i = \ Gamma _ i \ times 
G $ 确定 个数 后 再利用 SMOTE 生成 新 样本 
3 . 加权 除了 采样 和 生成 新 数据 等 
方法 我们 还 可以 通过 加权 的 方式 来 解决 
数据 不 平衡 问题 即对 不同 类别 分 错 的 
代价 不同 如 下图 kC k 1 C k 2 
. . . 012 . . . k10C 1 2 
. . . C 1 k 2C 2 1 0 
. . . . . . . . . . 
. . . . . . . . . . 
. 横向 是 真实 分类 情况 纵向 是 预测 分类 
情况 C i j 是 把 真实 类别 为 j 
的 样本 预测 为 i 时的/nr 损失 我们 需要 根据 
实际 情况 来 设定 它 的 值 这种 方法 的 
难点 在于 设置 合理 的 权重 实际 应用 中 一般 
让 各个 分类 间 的 加权 损失 值 近似 相等 
当然 这 并不 是 通用 法则 还是 需要 具体 问题 
具体 分析 4 . 一 分类 对于 正负 样本 极不 
平衡 的 场景 我们 可以 换 一个 完全 不同 的 
角度 来 看待 问题 把 它 看做 一 分类 One 
Class Learning 或 异常 检测 Novelty Detection 问题 这类 方法 
的 重点 不 在于 捕捉 类 间 的 差别 而是 
为 其中 一类 进行 建模 经典 的 工作 包括 One 
class SVM 等 三 如何 选择 解决 数据 不 平衡 
问题 的 方法 有 很多 上面 只是 一些 最 常用 
的 方法 而 最 常用 的 方法 也 有 这么 
多种 如何 根据 实际 问题 选择 合适 的 方法 呢 
接下来 谈谈 一些 我 的 经验 在 正负 样本 都 
非常 之 少 的 情况 下 应该 采用 数据 合成 
的 方式 在 负 样本 足够 多 正 样本 非常 
之 少 且 比例 及其 悬殊 的 情况 下 应该 
考虑 一 分类 方法 在 正负 样本 都 足够 多 
且 比例 不是 特别 悬殊 的 情况 下 应该 考虑 
采样 或者 加权 的 方法 采样 和 加权 在 数学 
上 是 等价 的 但 实际 应用 中 效果 却 
有 差别 尤其 是 采样 了 诸如 Random Forest 等 
分类 方法 训练 过程 会 对 训练 集 进行 随机 
采样 在 这种 情况 下 如果 计算资源 允许 上 采样 
往往 要比 加权 好一些 另外 虽然/c 上/f 采样/v 和下/nr 采样/v 
都/d 可以/c 使/v 数据集/i 变得/v 平衡/a 并且 在 数据 足够 
多 的 情况 下 等价 但 两者 也是 有区别 的 
实际 应用 中 我 的 经验 是 如果 计算资源 足够 
且 小众 类 样本 足够 多 的 情况 下 使用 
上 采样 否则 使用 下 采样 因为 上 采样 会 
增加 训练 集 的 大小 进而 增加 训练 时间 同时 
小 的 训练 集 非常 容易 产生 过拟合 对于 下 
采样 如果 计算资源 相对 较多 且 有 良好 的 并行 
环境 应该 选择 Ensemble 方法 四 更进一步 更多 细节 与 
更多 方法 可以 参考 TKDE 上 的 这篇 综述 Learning 
from Imbalanced Data 