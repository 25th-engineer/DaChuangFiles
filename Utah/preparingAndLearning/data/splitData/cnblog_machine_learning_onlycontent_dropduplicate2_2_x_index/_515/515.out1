感觉 狼 厂 有些 把 机器 学习 和 数据挖掘 神话 
了 机器学习 数据挖掘 的 能力 其实 是 有 边界 的 
机器学习 数据挖掘 永远 是 给 大 公司 的 业务 锦上添花 
的 东西 它 可以 帮助 公司 赚 更多 的 钱 
却 不能 帮助 公司 在 与 其他 公司 的 竞争 
中 取得 领先 优势 所以 小 公司 招聘 数据挖掘 / 
机器学习 不是 为了 装逼 就是 在 自寻死路 可是 相比 Java 
和C+/nr + 语言 开发 来说 机器学习 / 数据挖掘 确实是 新 
一些 老人 占 的 坑 少 一些 而且 可以 经常 
接触 一些 新的 东西 还是 赶紧 再次 抓住机会 集中 的 
再 总结 一下 吧 不能 再 拖拖拉拉 了 其实 数据挖掘 
的 主要 任务 是 分类 聚 类 关联 分析 预测 
时序 模式 和 偏差 分析 本 文先 系统 的 介绍 
一下 机器学习 中的 分类 算法 主要 目录 如下 常用 分类 
算法 Bayes 朴素 贝叶斯 的 优缺点 朴素 贝叶斯 的 公式 
Decision Tree 决策树 的 优缺点 决策树 公式 SVM 支持 向量 
机 的 优缺点 支持 向量 机 的 公式 KNNK 近邻 
的 优缺点 K 近邻 的 公式 Logistic Regression 逻辑 回归 
的 优缺点 逻辑 回归 的 公式 逻辑 回归 的 问题 
神经网络 神经 网络 的 优缺点 神经网络 公式 深度 学习 Ensemble 
l e a r n i n g G B 
D T A d a b o o s t 
R a n d o m Forest 参考文献 常用 分类 
算法 Bayes 贝叶斯 分类法 是 基于 贝叶斯 定 定理 的 
统计学 分类 方法 它 通过 预测 一个 给定 的 元组 
属于 一个 特定 类 的 概率 来 进行 分类 朴素 
贝叶斯 分类法 假定 一个 属性值 在 给定 类 的 影响 
独立 于 其他 属性 的 类 条件 独立性 朴素 贝叶斯 
的 优缺点 优点 所需 估计 的 参数 少 对于 缺失 
数据 不 敏感 缺点 假设 属性 之间 相互 独立 这 
往往 并不 成立 喜欢 吃 番茄 鸡蛋 却 不 喜欢 
吃 番茄炒蛋 需要 知道 先验概率 分类 决策 错误率 朴素 贝叶斯 
的 公式 朴素 贝叶斯 求解 P C | F1 . 
. . Fn = p C p F1 . . 
. Fn | C p F1 . . . Fn 
= p C ∏ i = 1np Fi | C 
Decision Tree 决策树 是 一种 简单 但 广泛 使用 的 
分类器 它 通过 训练 数据 构建 决策树 对 未知 的 
数据 进行 分类 决策树 的 每个 内部 节点 表示 在 
一个 属性 上 的 测试 每个 分枝 代表 该 测试 
的 一个 输出 而 每个 树叶 结点 存放 着 一个 
类 标号 在 决策树 算法 中 ID3 基于 信息 增益 
作为 属性 选择 的 度量 C 4.5 基于 信息 增益 
比 作为 属性 选择 的 度量 CART 基于 基尼指数 作为 
属性 选择 的 度量 决策树 代码 11 决策树 的 优缺点 
优点 不 需要 任何 领域 知识 或 参数 假设 适合 
高维 数据 简单 易于 理解 短 时间 内 处理 大量 
数据 得到 可行 且 效果 较好 的 结果 缺点 对于 
各 类别 样本 数量 不 一致 数据 信息 增益 偏向 
于 那些 具有 更多 数值 的 特征 易于 过拟合 忽略 
属性 之间 的 相关性 不支持 在线 学习 决策树 公式 熵 
Entropy S = − ∑ pilogpi 信息 增益 Entropy S 
A = Entropy S − ∑ v ∈ V A 
| Sv | | | Entropy Sv 分裂 信息 SplitInfoR 
= − ∑ j = 1k | Dj | | 
D | log2 | Dj | | D | 增益 
比率 GainRatio R = Gain R SplitInfoR D 基尼指数 Gini 
S = 1 − ∑ imp2iSVM 支持 向量 机 把 
分类 问题 转化 为 寻找 分类 平面 的 问题 并 
通过 最大化 分类 边界点 距离 分类 平面 的 距离 来 
实现 分类 支持 向量 机 的 优缺点 优点 可以 解决 
小 样本 下 机器学习 的 问题 提高 泛化 性能 可以 
解决 高维 非线性 问题 超 高维 文本 分类 仍 受欢迎 
避免 神经 网络结构 选择 和 局部 极小 的 问题 缺点 
缺失 数据 敏感 内存 消耗 大 难以 解释 运行 和 
调差 略 烦人 支持 向量 机 的 公式 转 自 
研究者 July SVM 的 求解 先 导出 12 | | 
w | | 2 继而 引入 拉格朗日 函数 转化 为 
单一 因子 对偶 变量 a 的 求解 如此 求 w 
. b 与 a 的 等价 而 求 a 的 
解法 即为 SMO 把 求 分类 函数 f x = 
ω ∗ x + b 的 问题 转化 为求 w 
b 的 最优 化 问题 即 凸 二次 规划 问题 
妙 从上 图 我们 可以 看出 这条 红色 的 线 
超平面 把 红色 的 点 和 蓝色 的 点 分开了 
超平面 一边 的 点 对应 的 y 全部 是 1 
而 另外 一边 全部 是 1 接着 我们 可以 令 
分类 函数 f x = ω Tx + b 显然 
x 是 超 平 面上 的 点 时 f x 
= 0 那么 我们 不妨 要求 所有 满足 f x 
0 的 点 其 对应 的 y 等于 1 而 
f x 0 则 对应 的 y = 1 的 
数据 点 我 盗用 了 很多 图 回忆 之前 的 
目标 函数 max1 | | ω | | s . 
t . yi ω T + b ≥ 1 i 
= 1 . . . n 这个 问题 等价 于 
max1 | | ω | | 2s . t . 
yi ω T + b ≥ 1 i = 1 
. . . n 很 显然 这 是 一个 凸 
优化 的 问题 更 具体 的 它 是 一个 二 
次优化 问题 目标函数 是 二次 的 约束条件 是 线性 的 
这个 问题 可以 用 任何 现成 的 QP Quadratic Programming 
优化 包 解决 但是 因为 这个 问题 的 特殊性 我们 
还 可以 通过 Lagrange Duality 变换 到 对偶 变量 的 
优化 问题 找到 一种 更 加行 之 有效 的 方法 
求解 首先 我们 给 每一个 约束条件 加上 一个 Lagrange mutiplier 
我们 可以 将 它们 融合 到 目标函数 中去   L 
ω b a = 12 | | ω | | 
2 − ∑ ni = 1 α yi wTxi + 
b − 1 然后 我们 令 θ ω = maxai 
≥ 0L ω b a 容易 验证 当 某个 约束条件 
不满足 时 例如 yi wTxi + b 1 那么 我们 
显然有 θ w = ∞ 而 当 所有 约束条件 都 
满足 时 则有 θ ω = 12 | | ω 
| | 2 亦即 我们 最初 要 最小化 的 量 
那么 我们 现在 的 目标 函数 就 变成 了 minw 
b θ ω = min ω bmaxai ≥ 0L ω 
b α = p ∗ 并且 我们 有d∗/nr ≤ p 
∗ 因为 最大值 中 最小 的 一个 一定 要 大于 
最小值 中 最大 的 一个 总之 p ∗ 提供 了 
一个 第一 个 问题 的 最优 值 p ∗ 的 
一个 下界 在 满足 KKT 条件 时 二者 相等 我们 
可以 通过 求解 第二个 问题 来 求解 第一 个 问题 
先让 L 关于 ω 和b/nr 最小化 我们 分 别把 L 
对 w 和b求/nr 偏 导 ∂ L ∂ ω = 
0 ⟹ ω = ∑ i = 1n α iyixi 
∂ L ∂ b = 0 ⟹ ∑ i = 
1n α iyi = 0 再 带回 L 得到 L 
ω b a = 12 ∑ i j = 1n 
α i α jyiyjxTixj − ∑ i j = 1n 
α i α jyiyjxTixj − b ∑ i = 1n 
α iyi + ∑ i = 1n α i = 
∑ i = 1n α i − 12 ∑ i 
j = 1n α i α jyiyjxTixj 此时 我们 得到 
关于 dual variable   α 的 优化 问题 max α 
∑ ni = 1 α i − 12 ∑ ni 
j = 1 α i α jyiyjxTixjs . t . 
α i ≥ 0 i = 1 . . . 
n ∑ ni = 1 α iyi = 0 这个 
问题 存在 高效 的 算法 不过 求解 过程 就不 在 
这里 介绍 了 对于 一个 数据 点 进行 分类 时 
我们 是 把 x 带入 到 f x = wTx 
+ b 中 然后 根据 其 正负号 来 进行 类别 
划分 的 把 ω = ∑ ni = 1 α 
iyixi 代入 到 f x = wTx + b 我们 
就 可以 得到 f x = ∑ ni = 1 
α iyi xi x + b 这里 的 形式 的 
有趣 之处 在于 对于 新 点 x 的 检测 只需要 
计算 它 与 训练 数 据点 的 内积 即可 为什么 
非 支持 向量 的 α 等于零 呢 因为 对于 非 
支持 向量 来说 L ω b a = 12 | 
| ω | | 2 − ∑ ni = 1 
α yi wTxi + b − 1 中的 yi wTxi 
+ b − 1 是 大于 0 的 而且 α 
i 又 是非 负 的 为了 满足 最大化 α i 
必须 等于 0 悲剧 的 非 支持 向量 就被 无声 
的 秒杀 了 KNNK 近邻 的 优缺点 优点 暂 无缺点 
计算 量 太大 对于 样本 分类 不 均衡 的 问题 
会 产生 误判 K 近邻 的 公式 Logistic Regression 逻辑 
回归 的 优缺点 优点 速度快 简单 易于 理解 直接 看到 
各个 特征 的 权重 能容 易地 更新 模型 吸收 新 
的 数据 如果 想 要 一个 概率 框架 动态 调整 
分类 阀值 缺点 特征 处理 复杂 需要 归一化 和 较多 
的 特征 工程 逻辑 回归 的 公式 如果 是 连续 
的 那么 就是 多重 线性 回归 如果 是 二项分布 就是 
Logistic 回归 如果 是 Poission 分布 就是 Poisson 回归 如果 
是 负 二项分布 那么 就是 负 二项分布 回归 问题 常见 
步骤 是 寻找 h 函数 构造 J 函数 想办法 使得 
J 函数 最小 并 求得 回归 参数 逻辑 回归 的 
h 函数 为 h θ x = g θ Tx 
= 11 + e − θ Tx 其中 h θ 
x 的 值 表示 结果 取 1 的 概率 那么 
对于 输入 x 的 分类 结果 对于 类别 1 和 
类别 0 的 概率 分别为 P y = 1 | 
x θ = h θ x P y = 0 
| x θ = 1 − h θ x 那么 
对于 构造 损失 函数 J 它们 基于 最大 似 然 
估计 推到 得到 的 ∑ ni = 1Cost h θ 
xi yi = − 1m ∑ ni = 1yilogh θ 
xi + 1 − yi log 1 − h θ 
xi 最小化 上式 与 最大化 下 式子 类似 P y 
| x θ = h θ x y 1 − 
h θ x 1 − y 取 似 然 函数 
l θ = logL θ = ∑ ni = 1yi 
logh θ xi + 1 − yi log 1 − 
h θ xi 使用 梯度 上升 的 方法 求解 θ 
同样 如果把 J θ 取 为 − 1ml θ 这样 
通过 梯度 下降 求解 梯度 最小值 梯度 下 降法 求 
最小值 θ j = θ j − α σ σ 
θ iJ θ 代入 后 得到 θ j = θ 
j − α 1m ∑ mi = 1 h θ 
xi − yi xji 然后 θ 的 更新过程 如下 θ 
j = θ j − α 1mxTE 其中 E = 
g A y 正则化 Regularization 正则化 是 在 经验 风 
线上 增加 一个 正则化 项 或者 惩罚 项 正则化 项 
一般 是 模型 复杂度 的 单调 递 增函数 模型 越 
复杂 正则化 就 越大 J θ = 12m ∑ i 
= 1n h θ xi − yi 2 + λ 
∑ j = 1n θ 2j λ 是 正则 项 
系数 多 分类 时 可以 去 样本 被 判定 为 
分类 概率 最大 的 那个 类 逻辑 回归 的 问题 
过拟合 问题 减少 feature 个数 规格化 神经网络 神经 网络 的 
优缺点 优点 分类 准确率 高 并行 处理 能力 强 分布式 
存储 和 学习 能力 强 鲁棒性 较强 不易 受 噪声 
影响 缺点 需要 大量 参数 网络拓扑 阀值 阈值 结果 难以 
解释 训练 时间 过长 神经网络 公式 深度 学习 Ensemble learning 
集成 学习 的 思路 是 在对 新的 实例 进行 分类 
的 时候 把 多个 单 分类器 的 结果 进行 某种 
组合 来 对 最终 的 结果 进行 分类 更好 的 
数据 往往 打败 更好 的 算法 设计 好 的 特征 
大有 脾 益 并且 如果 你 有一个 庞大 的 数据 
集 使用 某种 特定 的 算法 的 性能 可能 并 
不要紧 大可以 挨个 分类器 尝试 并且 选取 最好 的 一个 
可以/c 多/m 从/p 易用性/nr 和/c 性能/n 考虑/v 而且 从 Netfliex 
Prize 的 经验 教训 来看 尝试 各类 分类器 交叉 验证 
集成 方法 往往 能 取得 更好 的 结果 一般 的 
boosting bagging single classifier 集成 学习 的 方法 主要 有 
一下 三种 1 . 在 样本 上 做文章 基 分类器 
为 同一 个 分类 算法 主要/b 有/v bagging/w 和/c boosting/w 
2 . 在 分类 算法 上 做文章 即 用于 训练 
基 分类器 的 样本 相同 基 分类器 的 算法 不同 
3 . 在 样本 属性 集上 做文章 即 在 不同 
的 属性 上 构建 分类器 比较 出名 的 是 randomforest 
Tree 的 算法 这个 有 weka 也有 实现 1998年 Jerome 
Friedman & Trevor Hastie & Robert Tibshirani 发表文章 Additive Logistic 
Regression a Statistical View of Boosting 中 提到 Bagging 是 
一个 纯粹 的 降低 相关度 的 方法 如 果树 的 
节点 具有 很高 的 相关性 bagging 就会 有 很好 的 
效果 GBDT 回归 树 类似 决策树 使用 叶子 节点 的 
平均值 作为 判定 的 结果 如果 不是 叶子 节点 那么 
就 继续 向下 寻找 GBDT 几乎 可 用于 所有 的 
回归 问题 亦 可以 适用 于 二分 类 问题 GBDT 
使用 新 生成 的 树 来 拟合 之前 的 树 
拟合 的 残差 AdaboostAdaboost 目的 就是 从 训练 数据 中 
学习 一 系列 的 弱 分类器 或 基本 分类器 然后/c 
将/d 这些/r 弱/a 分类器/n 组合/v 成/n 一个/m 强/a 分类器/n Adaboost 
的 算法 流程 如下 首先 初始化 训练 数据 的 权值 
分布 每个 训练样本 最 开始 都被/nr 赋予 相同 的 权重 
1 / N 计算 Gm x 在 训练 数据 集上 
的 误差率 em 就是 被 Gm x 误 分类 样本 
的 权值 之和 计算 Gm x 的 系数 am 表示 
Gm x 在 最终 分类器 中 的 重要 程度 α 
m = 12log1 − emem 在 em = 1/2时 am 
= 0 且 am 随着 em 的 减 小而 增大 
更新 训练 数据集 的 权值 分布 目的 得到 样本 的 
新 权值 分布 用于 下 一轮 迭代 Dm + 1 
= wm + 1 1 wm + 1 2 . 
. . wm + 1 i . . . wm 
+ 1 N wm + 1 i = wmiZmexp − 
α myiGm xi i = 1 2 . . . 
N 使得 被 基本 分类器 Gm x 误 分类 样本 
的 权值 增大 而被 正确 分类 样本 的 权值 减小 
就 这样 通过 Zm = ∑ Ni = 1wmiexp − 
α myiGm xi 使得 Dm + 1 成为 一个 概率分布 
然后 组合 各个 弱 分类器 f x = ∑ Mm 
= 1 α mGm x 而 得到 的 最终 分类器 
G x = sign f x = sign ∑ Mm 
= 1 α mGm x Random Forest 随机 森林 指 
通过 多颗 决策树 联合 组成 的 预测模型 可以 对 样本 
或者 特征 取 bagging 参考文献 