考虑 到 学习 知识 的 顺序 及 效率 问题 所以 
后续 的 几种 聚 类 方法 不再 详细 讲解 原理 
也不 再写 python 实现 的 源代码 只 介绍 下 算法 
的 基本 思路 使 大家 对 每种 算法 有个 直观 
的 印象 从而 可以 更好 的 理解 函数 中 参数 
的 意义 及 作用 而 重点 是 放在 如何 使用 
及 使用 的 场景 题外话 今天 看到 一篇 博文 刚 
接触 机器学习 这 一个 月 我 都做 了 什么   
里面 对 机器 学习 阶段 的 划分 很不错 就 目前 
而言 我们 只 要做到 前 两 阶段 即可 因为 前 
两篇 博客 已经 介绍 了 两种 算法 所以 这里 的 
算法 编号 从3/nr 开始 3 . Mean shift1 概述 Mean 
shift 即 均值 迁移 的 基本 思想 在 数据 集中 
选定 一个 点 然后 以 这个 点 为 圆心 r 
为 半径 画 一个 圆 二维 下 是 圆 求出 
这个 点 到 所有 点 的 向量 的 平均值 而 
圆心 与 向量 均值 的 和为/nr 新的 圆心 然后 迭代 
此 过程 直到 满足 一点 的 条件 结束 Fukunage 在 
1975年 提出 后来 Yizong   Cheng 在此 基础 上 加入 
了 核 函数 和 权重 系数 使得 Mean shift 算法 
开始 流行 起来 目前 它 在 聚 类 图像 平滑 
分割 跟踪 等 方面 有着 广泛 的 应用 2 图解 
过程 为了 方便 大家 理解 借用 下 几张 图 来 
说明 Mean shift 的 基本 过程 由 上图 可以 很 
容易 看到 Mean shift 算法 的 核心 思想 就是 不断 
的 寻找 新的 圆 心坐标 直到 密度 最大 的 区域 
3 Mean shift 算法 函数 a 核心 函数 sklearn . 
cluster . MeanShift 核 函数 RBF 核 函数 由 上图 
可知 圆心 或 种子 的 确定 和 半径 或 带宽 
的 选择 是 影响 算法 效率 的 两个 主要 因素 
所以在 sklearn . cluster . MeanShift 中 重点 说明了 这 
两个 参数 的 设定 问题 b 主要参数 bandwidth 半径 或 
带宽 float 型 如果 没有 给 出 则 使用 sklearn 
. cluster . estimate _ bandwidth 计算出 半径 带宽 . 
可选 seeds 圆心 或 种子 数组 类型 即 初始化 的 
圆心 可选 bin _ seeding 布尔值 如果 为真 初始 内核 
位置 不是 所有 点 的 位置 而是 点 的 离散 
版本 的 位置 其 中点 被 分类 到 其 粗糙度 
对应 于 带宽 的 网格 上 将此 选项 设置 为 
True 将 加速 算法 因为 较少 的 种子 将被 初始化 
默认值 False . 如果 种子 参数 seeds 不为 None 则 
忽略 c 主要 属性 cluster _ centers _ 数组 类型 
计算出 的 聚 类 中心 的 坐标 labels _ 数组 
类型 每 个数 据点 的 分类 标签 d 算法 示例 
代码 中有 详细 讲解 内容 from sklearn . datasets . 
samples _ generator import make _ blobs from sklearn . 
cluster import MeanShift estimate _ bandwidth import numpy as np 
import matplotlib . pyplot as plt from itertools import cycle 
# # python 自带 的 迭代 器 模块 # # 
产生 随机 数据 的 中心 centers = 1 1 1 
1 1 1 # # 产生 的 数据 个数 n 
_ samples = 10000 # # 生产 数据 X _ 
= make _ blobs n _ samples = n _ 
samples centers = centers cluster _ std = 0.6 random 
_ state = 0 # # 带宽 也 就是 以 
某个 点 为 核心 时的/nr 搜索 半径 bandwidth = estimate 
_ bandwidth X quantile = 0.2 n _ samples = 
500 # # 设置 均值 偏移 函数 ms = MeanShift 
bandwidth = bandwidth bin _ seeding = True # # 
训练 数据 ms . fit X # # 每个 点 
的 标签 labels = ms . labels _ print labels 
# # 簇 中心 的 点 的 集合 cluster _ 
centers = ms . cluster _ centers _ # # 
总共 的 标签 分类 labels _ unique = np . 
unique labels # # 聚 簇 的 个数 即 分类 
的 个数 n _ clusters _ = len labels _ 
unique print number of estimated clusters % d % n 
_ clusters _ # # 绘图 plt . figure 1 
plt . clf colors = cycle b g r c 
m y k b g r c m y k 
b g r c m y k b g r 
c m y k for k col in zip range 
n _ clusters _ colors # # 根据 lables 中的 
值 是否 等于 k 重新 组成 一个 True False 的 
数组 my _ members = labels = = k cluster 
_ center = cluster _ centers k # # X 
my _ members 0 取出 my _ members 对应 位置 
为 True 的 值 的 横坐标 plt . plot X 
my _ members 0 X my _ members 1 col 
+ . plt . plot cluster _ center 0 cluster 
_ center 1 o markerfacecolor = col markeredgecolor = k 
markersize = 14 plt . title Estimated number of clusters 
% d % n _ clusters _ plt . show 
View Codee 效果图 4 openCV 主要 应用于 图像处理 而 Mean 
shift 多 用于 图像 跟踪 等 所以 对应 图像 处理 
这 部分 而言 openCV 中的 Mean shift 算法 的 功能 
还是 强大 一点 4 . Spectral Clustering1 概述 Spectral Clustering 
SC 即 谱 聚 类 是 一种 基于 图论 的 
聚 类 方法 它 能够 识别 任意 形状 的 样本空间 
且 收敛 于 全局 最 有解 其 基本 思想 是 
利用 样本数据 的 相似矩阵 进行 特征 分解 后 得到 的 
特征向量 进行 聚 类 . 它 与 样本 特征 无关 
而 只 与 样本 个数 有关 基本思路 将 样本 看作 
顶点 样 本间 的 相似 度 看作 带 权的边/nr 从而 
将 聚 类 问题 转 为图 分割 问题 找到/v 一种/m 
图/n 分割/v 的/uj 方法/n 使得/v 连/nr 接不/i 同组/n 的/uj 边的/nr 
权重/n 尽可能/d 低/a 这 意味着 组间 相似 度 要 尽可能 
低 组 内 的 边的/nr 权重 尽可能 高 这 意味着 
组 内 相似 度 要 尽可能 高 . 2 图解 
过程 如上 图 所示 断开 虚线 六个 数据 被 聚 
成 两类 3 Spectral Clustering 算法 函数 a 核心 函数 
sklearn . cluster . p e c t r a 
l C l u s t e r i n 
g 因为 是 基于 图论 的 算法 所以 输入 必须 
是 对称矩阵 b 主要参数 参数 较多 详细 参数 n _ 
clusters 聚 类 的 个数 官方 的 解释 投影 子空间 
的 维度 affinity 核 函数 默认 是 rbf 可选 nearest 
_ neighbors precomputed rbf 或 sklearn . metrics . pairwise 
_ kernels 支持 的 其中 一个 内核 之一 gamma affinity 
指定 的 核 函数 的 内核 系数 默认 1.0 c 
主要 属性 labels _ 每个 数据 的 分类 标签 d 
算法 示例 代码 中有 详细 讲解 内容 from sklearn . 
datasets . samples _ generator import make _ blobs from 
sklearn . cluster import spectral _ clustering import numpy as 
np import matplotlib . pyplot as plt from sklearn import 
metrics from itertools import cycle # # python 自带 的 
迭代 器 模块 # # 产生 随机 数据 的 中心 
centers = 1 1 1 1 1 1 # # 
产生 的 数据 个数 n _ samples = 3000 # 
# 生产 数据 X lables _ true = make _ 
blobs n _ samples = n _ samples centers = 
centers cluster _ std = 0.6 random _ state = 
0 # # 变 换成 矩阵 输入 必须 是 对称矩阵 
metrics _ metrix = 1 * metrics . pairwise . 
pairwise _ distances X . astype np . int32 metrics 
_ metrix + = 1 * metrics _ metrix . 
min # # 设置 谱 聚 类 函数 n _ 
clusters _ = 4 lables = spectral _ clustering metrics 
_ metrix n _ clusters = n _ clusters _ 
# # 绘图 plt . figure 1 plt . clf 
colors = cycle b g r c m y k 
b g r c m y k b g r 
c m y k b g r c m y 
k for k col in zip range n _ clusters 
_ colors # # 根据 lables 中的 值 是否 等于 
k 重新 组成 一个 True False 的 数组 my _ 
members = lables = = k # # X my 
_ members 0 取出 my _ members 对应 位置 为 
True 的 值 的 横坐标 plt . plot X my 
_ members 0 X my _ members 1 col + 
. plt . title Estimated number of clusters % d 
% n _ clusters _ plt . show View Codee 
效果图 5 . Hierarchical Clustering1 概述 Hierarchical Clustering 层次 聚 
类 就是 按照 某种 方法 进行 层次 分类 直到 满足 
某种 条件 为止 主要 分成 两类 a 凝聚 从下到上 首先 
将 每个 对象 作为 一个 簇 然后 合并 这些 原子簇 
为 越来越 大 的 簇 直到 所有 的 对象 都在/nr 
一个 簇 中 或者 某个 终结 条件 被 满足 b 
分裂 从上到下 首先 将 所有 对象 置于 同一个 簇 中 
然后 逐渐 细分 为 越来越 小 的 簇 直到 每个 
对象 自成 一簇 或者 达到 了 某个 终止 条件 较 
少用 2 算法 步骤 a 将 每个 对象 归为 一类 
共 得到 N 类 每类 仅 包含 一个 对象 . 
类 与 类 之间 的 距离 就 是 它们 所 
包含 的 对象 之间 的 距离 . b 找到 最 
接近 的 两个 类 并合 并成 一类 于是 总的 类 
数 少 了 一个 . c 重新 计算 新的 类 
与 所有 旧 类 之间 的 距离 . d 重复 
第 2步 和第/nr 3步 直到 最后 合并 成 一个 类 
为止 此类 包 含了 N 个 对象 . 3 图解 
过程 4 Hierarchical Clustering 算法 函数 a sklearn . cluster 
. A g g l o m e r a 
t i v e C l u s t e 
r i n g b 主要参数 详细 参数 n _ 
clusters 聚 类 的 个数 linkage 指定 层次 聚 类 
判断 相似 度 的 方法 有 以下 三种 ward 组间 
距离 等于 两类 对象 之间 的 最小 距离 即 single 
linkage 聚 类 average 组间 距离 等于 两组 对象 之间 
的 平均 距离 average linkage 聚 类 complete 组间 距离 
等于 两组 对象 之间 的 最大 距离 complete linkage 聚 
类 c 主要 属性 labels _ 每个 数据 的 分类 
标签 d 算法 示例 代码 中有 详细 讲解 内容 from 
sklearn . datasets . samples _ generator import make _ 
blobs from sklearn . cluster import A g g l 
o m e r a t i v e C 
l u s t e r i n g import 
numpy as np import matplotlib . pyplot as plt from 
itertools import cycle # # python 自带 的 迭代 器 
模块 # # 产生 随机 数据 的 中心 centers = 
1 1 1 1 1 1 # # 产生 的 
数据 个数 n _ samples = 3000 # # 生产 
数据 X lables _ true = make _ blobs n 
_ samples = n _ samples centers = centers cluster 
_ std = 0.6 random _ state = 0 # 
# 设置 分层 聚 类 函数 linkages = ward average 
complete n _ clusters _ = 3 ac = A 
g g l o m e r a t i 
v e C l u s t e r i 
n g linkage = linkages 2 n _ clusters = 
n _ clusters _ # # 训练 数据 ac . 
fit X # # 每个 数据 的 分类 lables = 
ac . labels _ # # 绘图 plt . figure 
1 plt . clf colors = cycle b g r 
c m y k b g r c m y 
k b g r c m y k b g 
r c m y k for k col in zip 
range n _ clusters _ colors # # 根据 lables 
中的 值 是否 等于 k 重新 组成 一个 True False 
的 数组 my _ members = lables = = k 
# # X my _ members 0 取出 my _ 
members 对应 位置 为 True 的 值 的 横坐标 plt 
. plot X my _ members 0 X my _ 
members 1 col + . plt . title Estimated number 
of clusters % d % n _ clusters _ plt 
. show View Codee 效果图 参数 linkage 的 取值 依次为 
ward average complete 6 . DBSCAN1 概述 DBSCAN Density Based 
Spatial Clustering of Applications with Noise 具有 噪声 的 基于 
密度 的 聚 类 方法 是 一种 基于 密度 的 
空间 聚 类 算法 该 算法 将 具有 足够 密度 
的 区域 划分 为 簇 即 要求 聚 类空 间中 
的 一定 区域内 所 包含 对象 的 数目 不 小于 
某一 给定 阈值 并在 具有 噪声 的 空间 数据库 中 
发现 任意 形状 的 簇 它 将 簇 定义 为 
密度 相连 的 点 的 最大 集合 2   算法 
步骤 大致 非 详细 DBSCAN 需要 二个 参数 扫描 半径 
eps 和 最小 包含 点数 min _ samples a 遍历 
所 有点 寻找 核心点 b 连通 核心点 并且在 此 过程 
中 扩展 某个 分类 集合 中点 的 个数 3 图解 
过程 在上 图中 第一步 就是 寻找 红色 的 核心 点 
第二 步 就是 用 绿色 箭头 联通 红色 点 图 
中点 以 绿色 线条 为 中心 被 分成 了 两类 
没在 黑色 圆 中的 点 是 噪声 点 4 DBSCAN 
算法 函数 a sklearn . cluster . DBSCANb 主要参数 详细 
参数 eps 两个 样本 之间 的 最大 距离 即 扫描 
半径 min _ samples 作为 核心 点 的话 邻域 即 
以其 为 圆心 eps 为 半径 的 圆 含 圆 
上 的 点 中的 最小 样本数 包括 点 本身 c 
主要 属性 core _ sample _ indices _ 核心 样本 
指数 此参数 在 代码 中有 详细 的 解释 labels _ 
数据 集中 每个 点 的 集合 标签 给 噪声 点 
标签 为 1 d 算法 示例 代码 中有 详细 讲解 
内容 from sklearn . datasets . samples _ generator import 
make _ blobs from sklearn . cluster import DBSCAN import 
numpy as np import matplotlib . pyplot as plt from 
itertools import cycle # # python 自带 的 迭代 器 
模块 from sklearn . preprocessing import StandardScaler # # 产生 
随机 数据 的 中心 centers = 1 1 1 1 
1 1 # # 产生 的 数据 个数 n _ 
samples = 750 # # 生产 数据 此 实验 结果 
受 cluster _ std 的 影响 或者说 受 eps 和 
cluster _ std 差值 影响 X lables _ true = 
make _ blobs n _ samples = n _ samples 
centers = centers cluster _ std = 0.4 random _ 
state = 0 # # 设置 分层 聚 类 函数 
db = DBSCAN eps = 0.3 min _ samples = 
10 # # 训练 数据 db . fit X # 
# 初始化 一个 全是 False 的 bool 类型 的 数组 
core _ samples _ mask = np . zeros _ 
like db . labels _ dtype = bool 这里 是 
关键 点 针对 这行 代码 xy = X class _ 
member _ mask & ~ core _ samples _ mask 
db . core _ sample _ indices _ 表示 的 
是 某个 点在 寻找 核心点 集合 的 过程 中 暂时 
被 标 为 噪声 点 的 点 即 周围 点 
小于 min _ samples 并 不是 最终 的 噪声 点 
在对 核心 点 进行 联通 的 过程 中 这 部分 
点 会被 进行 重新 归类 即 标签 并 不会 是 
表示 噪声 点 的 1 也可 也 这样 理解 这些 
点 不 适合 做 核心点 但是 会被 包含 在 某个 
核心点 的 范围 之内 core _ samples _ mask db 
. core _ sample _ indices _ = True # 
# 每个 数据 的 分类 lables = db . labels 
_ # # 分类 个数 lables 中 包含 1 表示 
噪声 点 n _ clusters _ = len np . 
unique lables 1 if 1 in lables else 0 # 
# 绘图 unique _ labels = set lables 1 np 
. linspace 返回 0 1 之间 的 len unique _ 
labels 个数 2 plt . cm 一个 颜 色映射 模块 
3 生成 的 每个 colors 包含 4个 值 分别 是 
rgba 4 其实 这行 代码 的 意思 就是 生成 4个 
可以 和 光谱 对应 的 颜色 值 colors = plt 
. cm . Spectral np . linspace 0 1 len 
unique _ labels plt . figure 1 plt . clf 
for k col in zip unique _ labels colors # 
# 1 表示 噪声 点 这里 的 k 表示 黑色 
if k = = 1 col = k # # 
生成 一个 True False 数组 lables = = k 的 
设置 成 True class _ member _ mask = lables 
= = k # # 两个 数组 做 & 运算 
找出 即是 核心点 又 等于 分类 k 的 值 markeredgecolor 
= k xy = X class _ member _ mask 
& core _ samples _ mask plt . plot xy 
0 xy 1 o c = col markersize = 14 
1 ~ 优先级 最高 按 位 对 core _ samples 
_ mask 求反 求出 的 是 噪音 点 的 位置 
2 & 于 运算 之后 求出 虽然 刚开始 是 噪音 
点 的 位置 但是 重新 归类 却 属于 k 的 
点 3 对 核心 分类 之后 进行 的 扩展 xy 
= X class _ member _ mask & ~ core 
_ samples _ mask plt . plot xy 0 xy 
1 o c = col markersize = 6 plt . 
title Estimated number of clusters % d % n _ 
clusters _ plt . show View Codee 效果图 如果 不 
进行 第二 步 中的 扩展 所有 的 小圆点 都 应该 
是 噪声 点 不 符合 第一 步 核心点 的 要求 
5 算法 优缺点 a 优点 可以 发现 任意 形状 的 
聚 类 b 缺点 随着 数据 量 的 增加 对 
I / O 内存 的 要求 也 随之 增加 如果 
密度 分布 不 均匀 聚 类 效果 较差 7 . 
Birch1 概述 Birch 利用 层次 方法 的 平衡 迭代 规约 
和聚类/nr 就是 通过 聚 类 特征 CF 形成 一个 聚 
类 特征 树 root 层 的 CF 个数 就是 聚 
类 个数 2 相关 概念 聚 类 特征 CF 每一个 
CF 是 一个 三元组 可以 用 N LS SS 表示 
. 其中 N 代表 了 这个 CF 中 拥有 的 
样本点 的 数量 LS 代表 了 这个 CF 中 拥有 
的 样本点 各 特征 维度 的 和 向量 SS 代表 
了 这个 CF 中 拥有 的 样本点 各 特征 维度 
的 平方和 如上 图 所示 N = 5LS = 3 
+ 2 + 4 + 4 + 3 4 + 
6 + 5 + 7 + 8 = 16 30 
SS = 32 + 22 + 42 + 42 + 
32 42 + 62 + 52 + 72 + 82 
= 54 190 3 图解 过程 对于 上图 中的 CF 
Tree 限定 了 B = 7 L = 5 也 
就是说 内部 节点 最多 有 7个 CF CF90 下 的 
圆 而 叶子 节点 最多 有 5个 CF CF90 到 
CF94 叶子 节点 是 通过 双向链表 连通 的 4 Birch 
算法 函数 a sklearn . cluster . Birchb 主要参数 详细 
参数 n _ clusters 聚 类 的 目标 个数 可选 
threshold 扫描 半径 个人 理解 官方 说法 比较 绕口 设置 
小了 分类 就多 branches _ factor 每个 节 点中 CF 
子 集群 的 最大 数量 默认 为 50 c 主要 
属性 labels _ 每 个数 据点 的 分类 5 算法 
示例 代码 中有 详细 讲解 内容 import numpy as np 
import matplotlib . pyplot as plt from sklearn . datasets 
. samples _ generator import make _ blobs from sklearn 
. cluster import Birch # X 为 样本 特征 Y 
为 样本 簇 类别 共 1000个 样本 每个 样本 2个 
特征 共 4个 簇 簇 中心 在 1 1 0 
0 1 1 2 2 X y = make _ 
blobs n _ samples = 1000 n _ features = 
2 centers = 1 1 0 0 1 1 2 
2 cluster _ std = 0.4 0.3 0.4 0.3 random 
_ state = 9 # # 设置 birch 函数 birch 
= Birch n _ clusters = None # # 训练 
数据 y _ pred = birch . fit _ predict 
X # # 绘图 plt . scatter X 0 X 
1 c = y _ pred plt . show View 
Code6 效果图 分别为 n _ clusters = None 和n_/nr clusters 
= 4                 
                    
        8 . G a u s 
s i a n M i x t u r 
e M o d e l 补 1 概述 正太 
分布 也叫 高斯分布 正太 分布 的 概率密度 曲线 也叫 高斯分布 
概率 曲线 G a u s s i a n 
M i x t u r e M o d 
e l 混合 高斯 模型 GMM 聚 类 算法 大多数 
通过 相似 度 来 判断 而 相似 度 又 大多 
采用 欧式 距离 长短 作为 衡量 依据 而 GMM 采用 
了 新的 判断 依据 概率 即 通过 属于 某一 类 
的 概率 大小 来 判断 最终 的 归属 类别 GMM 
的 基本 思想 就是 任意 形状 的 概率 分布 都 
可以 用 多个 高斯分布 函 数去 近似 也/d 就是说/c GMM/w 
就是/d 有/v 多个/m 单/n 高斯/nr 密度/n 分布/v Gaussian 组成 的 
每个 Gaussian 叫 一个 Component 这些 Component 线性 加成 在 
一起 就 组成 了 GMM 的 概率密度函数 也 就是 下面 
的 函数 2 数学公式 这里 不讲 公式 的 具体 推导 
过程 也不 实现 具体 算法 列出来 公式 只是 方便 理解 
下面 的 函数 中 为什么 需要 那些 参数 K 模型 
的 个数 即 Component 的 个数 聚 类 的 个数 
为 第 k 个 高斯 的 权重 p x | 
k 则为 第 k 个 高斯 概率密度 其 均值 为 
μ k 方差 为 σ k 上述 参数 除了 K 
是 直接 给定 之外 其他 参数 都是/nr 通过 EM 算法 
估算 出来 的 有个 参数 是 指定 EM 算法 参数 
的 3 G a u s s i a n 
M i x t u r e M o d 
e l 算法 函数 a from sklearn . mixture . 
GaussianMixtureb 主要参数 详细 参数 n _ components 高斯 模型 的 
个数 即 聚 类 的 目标 个数 covariance _ type 
通过 EM 算法 估算 参数 时 使用 的 协方差 类型 
默认 是 full full 每个 模型 使用 自己 的 一般 
协方差 矩阵 tied 所用 模型 共享 一个 一般 协方差 矩阵 
diag 每个 模型 使用 自己 的 对角线 协方差 矩阵 spherical 
每个 模型 使用 自己 的 单一 方差 4 算法 示例 
代码 中有 详细 讲解 内容 import matplotlib . pyplot as 
plt from sklearn . datasets . samples _ generator import 
make _ blobs from sklearn . mixture import GaussianMixture # 
X 为 样本 特征 Y 为 样本 簇 类别 共 
1000个 样本 每个 样本 2个 特征 共 4个 簇 簇 
中心 在 1 1 0 0 1 1 2 2 
X y = make _ blobs n _ samples = 
1000 n _ features = 2 centers = 1 1 
0 0 1 1 2 2 cluster _ std = 
0.4 0.3 0.4 0.3 random _ state = 0 # 
# 设置 gmm 函数 gmm = GaussianMixture n _ components 
= 4 covariance _ type = full . fit X 
# # 训练 数据 y _ pred = gmm . 
predict X # # 绘图 plt . scatter X 0 
X 1 c = y _ pred plt . show 
View Code5 效果 图图 16 跟 对比 可以 看出 虽然 
使用 同样 的 数据 但是 不同 的 算法 的 聚 
类 效果 是 不 一样 的 