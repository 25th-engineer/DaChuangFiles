应该 是 去年 的 这个 时候 我 开始 接触 机器 
学习 的 相关 知识 当时 的 入门 书籍 是 数据挖掘 
导论 囫囵吞枣 般 看完 了 各个 知名 的 分类器 决策树 
朴素 贝叶斯 SVM 神经网络 随机 森林 等等 另外 较为 认真 
地 复习 了 统计学 学习 了 线性 回归 也 得以 
通过 orange spss R 做 一些 分类 预测 工作 可是 
对外 说 自己 是 搞 机器 学习 的 还是 不太 
自信 毕竟 和 科班出身 的 各位 大牛 相比 自己 对 
这些 模型 算法 的 理解 只能 算是 知其然 而 不知 
其 所以然 用 起来 总 感觉 哪里 不 对劲 因此 
去年 早早 地 就把 网易 公开 课上 NG 大神 的 
斯坦福 CS229 课程 以及 相应 的 讲义 下载 了 下来 
但 每次 一 想学 看到 每集 1个 多 小时 的 
内容 就 望 而生 却 感觉 没有 足够 的 整块 
的 时间 来 学习 好在 过年 回家 期间 实在 没有 
其他 借口 不 学了 于是 才能有 这篇 学习 笔记 截止 
今天 下午 刚 好学 完了 前 四课 听 Andrew Ng 
讲完 了 GLM 广义 线性 模型 的 相关 内容 真的 
是 感觉 相见恨晚 我 要向 所有 看到 本文 的 同学 
推荐 这个 课程 虽然 是 07年 的 机器学习 三要素 机器 
学习 的 三要素 为 模型 策略 算法 这些 知识 来自 
于 李航 写 的 统计 学习 方法 外国 的 老师 
善于 形象化 地 举例 中国 的 老师 善于 总结 归纳 
这句话 还是 很 有 道理 的 适当 的 归纳 有助于 
理清 我们 的 思路 不知/v 大家/n 是否/v 有和我/nr 类似/v 的/uj 
疑惑/v 线性 回归 是 模型 还是 算法 SVM 是 模型 
还是 算法 接下来 我 会 结合 自己 的 思考 从 
模型 策略 算法 的 角度 整理 学习 笔记 Linear Regression 
线性 回归 1 . 三要素 模型 就是 所 要 学习 
的 条件 概率分布 或 决策函数 相信 线性 回归模型 大家 都 
很熟悉 高三 时就/nr 学过 策略 按照 什么样 的 准则 学习 
或 选择 最优 的 模型 学过 线性 回归 的 同学 
应该 记得 最小化 均方 误差 即 所谓 的 least squares 
在 spss 里 线性 回归 对应 的 模块 就叫 OLS 
即 Ordinary Least Squares 算法 基于 训练 数据集 根据 学习策略 
选择 最优 模型 的 计算 方法 确定 模型 中 每个 
θ i 取值 的 计算 方法 往往 归结为 最优化 问题 
对于 线性 回归 我们/r 知道/v 它/r 是/v 有/v 解析/vn 解的/nr 
即 The normal equations 因为 我 不是 做 科研 的 
所以 解析 解的/nr 推导 没有 细看 我 猜 很多 人 
可能 就是 在 第二 集中 段 看到 这么 复杂 的 
推导 而 放弃 继续 学习 的 在 推导 解析 解 
之前 NG 还 介绍 了 一个 很 重要 的 算法 
2 . gradient descent algorithm 梯度 下降 算法 课程 中 
的 比喻 很 形象 将用 最快 的 速度 最小化 损失 
函数 比作 如何 最快 地 下山 也 就是 每一步 都 
应该 往 坡度 最 陡 的 方向 往下走 而 坡度 
最 陡 的 方向 就是 损失 函数 相应 的 偏 
导数 因此 算法 迭代 的 规则 是 其中 α 是 
算法 的 参数 learning rate α 越大 每一步 下降 的 
幅度 越大 速度 也 会 越快 但 过大 有可能 导致 
算法 不 准确 另外 对于 线性规划 问题 通常 J 函数 
误差 平方和 是 碗 状 的 所以 往往 只 会 
有一个 全局 最优 解 不用 过多 担心 算法 收敛 到 
局部 最优 解 当 训练 集 的 样本 量 大于 
1时 有 两种 算法 batch gradient descent 批量 梯度 下降 
stochastic gradient descent incremental gradient descent 随机 梯度 下降 当 
训练样本 量 很大 时 batch gradient descent 的 每一步 都要 
遍历 整个 训练 集 开销 极大 而 stochastic gradient descent 
则 只 选取 其中 的 一个 样本 因此 后者 的 
速度 要 快于 前者 另外 虽然 stochastic gradient descent 可能 
不会 收敛 但是 实践 当中 大多数 情况 下 得到 的 
结果 都是/nr 真实 最小值 的 一个 足够 好 的 近似 
3 . 为什么 在 选择 策略 时 我们 使用 的 
是 误差 平方和 而 不是 绝对值 或 其他 损失 函数 
首先 我们 得 复习 一下 线性 回归 的 模型 及 
假设 ε i ∼ N 0 σ 2 随机误差 ε 
服从 正态分布 高斯分布 ε i are distributed IID 随机误差 ε 
是 独立 同 分布 的 于是 可以获得 目标 变量 的 
条件 概率分布 整个 训练 集 的 似 然 函数 与 
对数 似 然 函数 分别为 因此 最大化 对数 似 然 
函数 也就 相当于 最小化 4 . Locally weighted linear regression 
LOESS LOESS 与 线性 回归 的 区别 加 权函数 w 
的 一个 选择 是 | x i − x | 
越小 权重 w i 越 接近 1 越大 则 权重 
越小 τ 被称为 bandwidth 参数 控制 权重 下降 的 快慢 
τ 越大 下降 速度 越慢 LOESS 是 一个 非 参数 
算法 对于 不同 的 输入 变量 需要 利用 训练 集 
临时 重新 拟合 参数 线性 回归 是 一个 参数 算法 
参数 个数 是 有限 的 拟合 完 参数 后就/nr 可以 
不 考虑 训练 集 直接 进行 预测 LOESS 可以 缓解 
特征选择 的 需求 是否 为 一个 特征 添加 高次 项 
总结 本来 想 一口气 写完 前 四节课 的 笔记 但 
发现 量 还是 太大 了 还是 分开 写 吧 总结 
一下 收获 记住 了 机器学习 三要素 不会 再把 模型 和 
算法 搞混 了 了解 了 梯度 下降 算法 包括 批量 
梯度 下降 与 随机 梯度 下降 从 最大化 似 然 
函数 的 角度 明白 为什么 线性 回归 使用 误差 平方和 
作为 损失 函数 是 一个 好 的 选择 了解 一个 
非 参数 算法 局部 加权 线性 回归 