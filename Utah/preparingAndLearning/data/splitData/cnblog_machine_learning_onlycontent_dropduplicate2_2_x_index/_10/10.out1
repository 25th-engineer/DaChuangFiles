本 博客 所有 内容 以 学习 研究 和 分享 为主 
如需 转载 请 联系 本人 标明 作者 和 出处 并且 
是 非 商业 用途 谢谢 系列 目录 1 第一 部分 
模型 的 评估 与 数据 处理 2 3 机器学习 基础 
与 实践 一 数据 清洗 4 5 机器学习 基础 与 
实践 二 数据 转换 6 7 机器学习 基础 与 实践 
三 数据 降 维 8 9 10 11 第二 部分 
特征 工程 12 13 机器学习 基础 与 实践 四 特征选择 
14 15 机器学习 基础 与 实践 五 特征提取 16 17 
机器学习 基础 与 实践 六 模型 选择 与 评估 18 
19 20 21 第三 部分 算法 基础 之 有 监督 
算法 22 23 机器学习 基础 与 实践 七 广义 线性 
模型 24 25 机器学习 基础 与 实践 八 最小二乘 法 
26 27 机器学习 基础 与 实践 九 LDA 28 29 
机器学习 基础 与 实践 十 SGD 30 31 机器学习 基础 
与 实践 十一 K 近邻 32 33 机器学习 基础 与 
实践 十二 高斯 过程 34 35 机器学习 基础 与 实践 
十三 决策树 ID3 C 4.5 C 5.0 CART 36 37 
机器学习 基础 与 实践 十四 朴素 贝叶斯 38 39 机器学习 
基础 与 实践 十五 支持 向量 机 40 41 机器学习 
基础 与 实践 十六 集成 学习 Bagging RF AdaBoost Gradient 
Tree Boosting Voting Classifier 42 43 机器学习 基础 与 实践 
十七 感知机 模型 44 45 机器学习 基础 与 实践 十八 
多 分类 算法 46 47 48 49 第四 部分 算法 
基础 之无 监督 算法 50 51 机器学习 基础 与 实践 
十九 K means 52 53 机器学习 基础 与 实践 二十 
Affinity propagation 54 55 机器学习 基础 与 实践 二十一 Mean 
shift 56 57 机器学习 基础 与 实践 二十二 Spectral clustering 
58 59 机器学习 基础 与 实践 二十三 Ward hierachical 60 
61 机器学习 基础 与 实践 二十四 Agglomerative clustering 62 63 
机器学习 基础 与 实践 二十五 DBSCAN 64 65 机器学习 基础 
与 实践 二十六 Gaussian mixtures 66 67 机器学习 基础 与 
实践 二十七 Birch 68 69 70 71 第五 部分 算法 
基础 之 推荐算法 72 73 机器学习 基础 与 实践 二十八 
相似 度 计算 74 75 机器学习 基础 与 实践 二十九 
Arules 关联 规则 76 77 机器学习 基础 与 实践 三十 
Fp Growth 78 79 机器学习 基础 与 实践 三十一 User 
based or Item based 80 81 82 83 第六 部分 
算法 基础 之半 监督 模型 84 85 机器学习 基础 与 
实践 三十二 Label Propagation 86 87 88 89 第七 部分 
算法 基础 之 其他 模型 90 91 机器学习 基础 与 
实践 三十三 概率 图 模型 92 93 机器学习 基础 与 
实践 三十四 最大熵 模型 94 95 机器学习 基础 与 实践 
三十五 规则学习 96 97 机器学习 基础 与 实践 三十六 强化 
学习 98 99 机器学习 基础 与 实践 三十七 条件 随 
机场 100 101 机器学习 基础 与 实践 三十八 保 序 
回归 Isotonic regression 102 103 机器学习 基础 与 实践 三十九 
Probability c a l i b r a t i 
o n C o n t e n t s 
本文 目录 一 . 标准化 的 原因 二 . 适用 
情况 三 . 三种 数据 变换 方法 的 含义 与 
应用 四 . 具体 方法 及 代码 一 标准化 1.1 
  scale 零 均值 单位 方差 1.2   StandardScaler 二 
归一化 2.1   MinMaxScaler 最小 最大值 标准化 2.2   MaxAbsScaler 
绝对值 最大 标准化 2.3   对 稀疏 数据 进行 标准化 
2.4 对 离群 点 进行 标准化 三 正则化 3.1   
  L1 L2 正则化 四 二 值 化 4.1 特征 
二 值 化 五 对 类别 特征 进行 编码 六 
缺失 值 的 插补 七 生成 多项式 特征 八 自定义 
转换 正文 一 . 标准化 的 原因 通常 情况下 是 
为了 消除 量纲 的 影响 譬如 一个 百分制 的 变量 
与 一个 5 分值 的 变量 在 一起 怎么 比较 
只有 通过 数据 标准化 都把 它们 标准 到 同一个 标准 
时才/nr 具有 可比性 一般 标准化 采用 的 是 Z 标准化 
即 均值 为 0 方差 为 1 当然 也 有 
其他 标准化 比如 0 1 标准化 等等 可 根据 自己 
的 数据 分布 情况 和 模型 来 选择 二 . 
适用 情况 看 模型 是否 具有 伸缩 不变性 不是 所有 
的 模型 都 一定 需要 标准化 有些 模型 对 量纲 
不同 的 数据 比较 敏感 譬如 SVM 等 当 各个 
维度 进行 不 均匀 伸缩 后 最优 解与/nr 原来 不等价 
这样 的 模型 除非 原始数据 的 分布 范围 本来 就 
不叫 接近 否则 必须 进行 标准化 以免 模型 参数 被 
分布 范围 较大 或 较小 的 数据 主导 但是 如果 
模型 在 各个 维度 进行 不 均匀 伸缩 后 最优 
解与/nr 原来 等价 例如 logistic regression 等 对于 这样 的 
模型 是否 标准化 理论上 不会 改变 最优 解 但是 由于 
实际 求解 往往 使用 迭代 算法 如果 目标函数 的 形状 
太 扁 迭代 算 法可能 收敛 得 很慢 甚至不 收敛 
所以 对于 具有 伸缩 不变性 的 模型 最好 也 进行 
数据 标准化 三 . 三种 数据 变换 方法 的 含义 
与 应用 Rescaling 重 缩放 / 归一化 通常 是 指 
增加 或者 减少 一个 常数 然后 乘以 / 除以 一个 
常数 来 改变 数据 的 衡量 单位 例如 将 温度 
的 衡量 单位 从 摄氏度 转化 为 华氏温度 Normalizing 正则化 
通常 是 指 除以 向量 的 范数 例如 将 一个 
向量 的 欧氏 长度 等价 于1/nr 在 神经 网络 中 
正则化 通常 是 指 将 向量 的 范围 重 缩放 
至 最小化 或者 一定 范围 使 所有 的 元素 都在 
0 1 范围 内 通常用于 文本 分类 或者 文本 聚 
类 中 Standardizing 标准化 通常 是 为了 消除 不同 属性 
或 样 方间的/nr 不 齐性 使 同 一样 方内 的 
不同 属性 间或 同一 属性 在 不同 样 方内 的 
方差 减小 例如 如果 一个 向量 包含 高斯分布 的 随机 
值 你 可能 会 通过 除以 标准偏差 来 减少 均值 
然后 获得 零 均值 单位 方差 的 标准 正 态 
随机变量 那么 问题 是 当 我们 在 训练 模型 的 
时候 一定 要 对 数据 进行 变换 吗 这得 视 
情况 而定 很多 人 对 多层 感知机 有个 误解 认为 
输入 的 数据 必须 在 0 1 这个 范围 内 
虽然 标准化 后在/nr 训练 模型 效果 会 更好 但 实际上 
并 没有 这个 要求 但是 最 好使 输入 数据 中心 
集中 在 0 周围 所以 把 数据 缩 放到 0 
1 其实 并 不是 一个 好 的 选择 如果 你 
的 输出 激活 函数 的 范围 是 0 1 sigmoid 
函数 的 值域 那你/nr 必须 保证 你 的 目标 值 
也 在 这个 范围 内 但 通常 请款 下 我们 
会 使 输出 激活 函数 的 范围 适应 目标函数 的 
分布 而 不是 让 你 的 数据 来 适应 激活 
函数 的 范围 当 我们 使用 激活 函数 的 范围 
为 0 1 时 有些 人 可能 更 喜欢 把 
目标函数 缩 放到 0.1 0.9 这个 范围 我 怀疑 这种 
小 技巧 的 之所以 流行 起来 是 因为 反向 传播 
的 标准化 太慢 了 导致 的 但 用 这种 方法 
可能 会使 输出 的 后验/nr 概率值 不对 如果 你 使用 
一个 有效 的 训练 算法 的话 完全 不 需要 用 
这种 小 技巧 也 没有 必要 去 避免 溢出 overflow 
四 . 具体 方法 及 代码 一 标准化 1.1   
scale 零 均值 单位 方差 1 from sklearn import preprocessing 
2 import numpy as np 3 # raw _ data 
4 X = np . array 1 . 1 . 
2 . 2 . 0 . 0 . 0 . 
1 . 1 . 5 X _ scaled = preprocessing 
. scale X 6 # output 7 X _ scaled 
= 0 . 1.22474487 1.33630621 8 1.22474487 0 . 0.26726124 
9 1.22474487 1.22474487 1.06904497 10 ＃ scaled 之后 的 数据 
零 均值 单位 方差 11 X _ scaled . mean 
axis = 0 # column mean array 0 . 0 
. 0 . 12 X _ scaled . std axis 
= 0 # column standard deviation array 1 . 1 
. 1 . 1.2   StandardScaler 计算 训练 集 的 
平均值 和 标准差 以便 测试 数据集 使用 相同 的 变换 
1 scaler = preprocessing . StandardScaler . fit X 2 
# out 3 StandardScaler copy = True with _ mean 
= True with _ std = True 4 scaler . 
mean _ 5 # out 6 array 1 . 0 
. 0.33333333 7 scaler . std _ 8 # out 
9 array 0.81649658 0.81649658 1.24721913 10 # 测试 将该 scaler 
用于 输入 数据 变换 之后 得到 的 结果 同上 11 
scaler . transform X 12 # out 13 array 0 
. 1.22474487 1.33630621 1.22474487 0 . 0.26726124 1.22474487 1.22474487 1.06904497 
14 scaler . transform 1 . 1 . 0 . 
15 # scale the new data out 16 array 2.44948974 
1.22474487 0.26726124 注 1 若 设置 with _ mean = 
False 或者 with _ std = False 则 不做 centering 
或者 scaling 处理 2 scale 和 StandardScaler 可以 用于 回归模型 
中的 目标值 处理 二 归一化 将 数据 特征 缩放 至 
某一 范围 scalingfeatures to a range 另外 一种 标准化 方法 
是 将 数据 缩放 至 给定 的 最小值 与 最大值 
之间 通常 是 ０ 与 １ 之间 可用 MinMaxScaler 实现 
或者 将 最大 的 绝对值 缩放 至 单位 大小 可用 
MaxAbsScaler 实现 使用 这种 标准化 方法 的 原因 是 有时 
数据集 的 标准差 非常 非常 小 有时 数据 中 有 
很多 很多零 稀疏 数据 需要 保存 住 ０ 元素 2.1 
  MinMaxScaler 最小 最大值 标准化 公式 X _ std = 
X X . min axis = 0 / X . 
max axis = 0 X . min axis = 0 
X _ scaler = X _ std / max min 
+ min1 # 例子 将 数据 缩放 至 0 1 
间 2 X _ train = np . array 1 
. 1 . 2 . 2 . 0 . 0 
. 0 . 1 . 1 . 3 min _ 
max _ scaler = preprocessing . MinMaxScaler 4 X _ 
train _ minmax = min _ max _ scaler . 
fit _ transform X _ train 5 # out 6 
array 0.5 0 . 1 . 7 1 . 0.5 
0.33333333 8 0 . 1 . 0 . 9 # 
将 上述 得到 的 scale 参数 应用 至 测试数据 10 
X _ test = np . array 3 . 1 
. 4 . 11 X _ test _ minmax = 
min _ max _ scaler . transform X _ test 
# out array 1.5 0 . 1.66666667 12 # 可以 
用 以下 方法 查看 scaler 的 属性 13 min _ 
max _ scaler . scale _ # out array 0.5 
0.5 0.33 . . . 14 min _ max _ 
scaler . min _ # out array 0 . 0.5 
0.33 . . . 2.2   MaxAbsScaler 绝对值 最大 标准化 
与 上述 标准化 方法 相似 但是 它 通过 除以 最大值 
将 训练 集 缩放 至 1 1 这 意味着 数据 
已经 以 ０ 为中心 或者 是 含有 非常 非常 多 
０ 的 稀疏 数据 1 X _ train = np 
. array 1 . 1 . 2 . 2 2 
. 0 . 0 . 3 0 . 1 . 
1 . 4 max _ abs _ scaler = preprocessing 
. MaxAbsScaler 5 X _ train _ maxabs = max 
_ abs _ scaler . fit _ transform X _ 
train 6 # out 7 array 0.5 1 . 1 
. 1 . 0 . 0 . 0 . 1 
. 0.5 8 X _ test = np . array 
3 . 1 . 4 . 9 X _ test 
_ maxabs = max _ abs _ scaler . transform 
X _ test 10 # out 11 array 1.5 1 
. 2 . 12 max _ abs _ scaler . 
scale _ 13 # out 14 array 2 . 1 
. 2 . 其实在 scale 模块 里 也 提供 了 
这 两种 方法   minmax _ scale 和 maxabs _ 
scale2 . 3   对 稀疏 数据 进行 标准化 对 
稀疏 数据 进行 中心化 会 破坏 稀疏 数据 的 结构 
这样 做 没什么 意义 但是 我们 可以 对 稀疏 数据 
的 输入 进行 标准化 尤其 是 特征 在 不同 的 
标准 时 MaxAbsScaler   和 /nr maxabs _ scale 是 专门 
为 稀疏 数据 设计 的 也是 常用 的 方法 但是 
scale   和 /nr StandardScaler 只 接受 scipy . sparse 的 
矩阵 作为 输入 并且 必须 设置 with _ centering = 
False 否则 会 出现   ValueError 且 破坏 稀疏 性 
而且 还会 无意 中 分配 更多 的 内存 导致 内存 
崩溃 RobustScaler 不 适用 于 稀疏 数据 的 输入 但是 
你 可以 用   transform 方法 scalers 接受 压缩 的 
稀疏 行 Compressed Sparse Rows 和 压缩 的 稀疏 列 
Compressed Sparse Columns 的 格式 具体 参考 scipy . sparse 
. csr _ matrix 和 scipy . sparse . csc 
_ matrix 其他 的 稀疏 格式 会 被 转化 成 
压缩 的 稀疏 行 Compressed Sparse Rows 格式 为了 避免 
这种 不 必要 的 内存 拷贝 推荐 使用 CSR 或者 
CSC 的 格式 如果 数据 很小 可以 在 稀疏 矩阵 
上 运用 toarray 方法 2.4 对 离群 点 进行 标准化 
如果 你 的 数据 有 离群 点 上 一篇 我们 
提到 过 对 数据 进行 均差 和 方差 的 标准化 
效果 并 不好 这种 情况 你 可以 使用 robust _ 
scale   和 /nr RobustScaler   作为 替代 它们/r 有对/nr 数据/n 
中心化/l 和/c 数据/n 的/uj 缩放/v 鲁棒性/nr 更强/i 的/uj 参数/n 三 
正则化 3.1     L1 L2 正则化 1 x = 
np . array 1 . 1 . 2 . 2 
2 . 0 . 0 . 3 0 . 1 
. 1 . 4 x _ normalized = preprocessing . 
normalize x norm = l2 5 print x _ normalized 
6 7 # 可以 使用 processing . Normalizer 类/q 实现/v 
对/p 训练/vn 集/q 和/c 测试/vn 集/q 的/uj 拟合/v 和/c 转换/v 
8 normalizer = preprocessing . Normalizer . fit x 9 
print normalizer 10 normalizer . transform x 注 稀疏 数据 
输入 normalize   和 /nr Normalizer   既 接受 稠密 数据 
dense array like 也 接受 稀疏 矩阵 from scipy . 
sparse 作为 输入 稀疏 数据 需要 转换成 压缩 的 稀疏 
行 Compressed Sparse Rows 格式 详见 scipy . sparse . 
csr _ matrix 为了 避免 不 必要 的 内存 拷贝 
推荐 使用 CSR 四 二 值 化 4.1 特征 二 
值 化 特征 二 值 化 是 把 数值 特征 
转化成 布尔值 的 过程 这个 方法 对 符合 多 变量 
伯努利 分布 的 输入 数据 进行 预测 概率 参数 很 
有效 详细 可以 见 这个 例子 sklearn . neural _ 
network . BernoulliRBM . 此外 在 文本处理 中 也 经常 
会 遇到 二 值 特征值 很 可能 是 为了 简化 
概率 推理 即使 在 实际 中 正则化 后的/nr 词频 或者 
TF IDF 的 值 通常 只比 未 正则化 的 效果 
好 一点点 对于   Normalizer Binarizer 工具 类 通常 是 
在 Pipeline 阶段 sklearn . pipeline . Pipeline 的 前期 
过程 会 用到 下面 举 一个 具体 的 例子 1 
# input 2 X = 1 . 1 . 2 
. 3 2 . 0 . 0 . 4 0 
. 1 . 1 . 5 # binary 6 binarizer 
= preprocessing . Binarizer . fit X # fit does 
nothing 7 binarizer 8 Binarizer copy = True threshold = 
0.0 9 # transform 10 binarizer . transform X 11 
# out 12 array 1 . 0 . 1 . 
13 1 . 0 . 0 . 14 0 . 
1 . 0 . 15 16 # 调整 阈值 17 
binarizer = preprocessing . Binarizer threshold = 1.1 18 binarizer 
. transform X 19 # out 20 array 0 . 
0 . 1 . 21 1 . 0 . 0 
. 22 0 . 0 . 0 . 23 注 
稀疏 数据 输入 binarize   和 /nr Binarizer   既 接受 
稠密 数据 dense array like 也 接受 稀疏 矩阵 from 
scipy . sparse 作为 输入 稀疏 数据 需要 转换成 压缩 
的 稀疏 行 Compressed Sparse Rows 格式 详见 scipy . 
sparse . csr _ matrix 为了 避免 不 必要 的 
内存 拷贝 推荐 使用 CSR 五 对 类别 特征 进行 
编码 我们 经常 会 遇到 一些 类别 特征 这些 特征 
不是 离散 型 的 数值 而 是 这样 的 男性 
女性 来自 欧洲 来自 美国 来自 亚洲 使用 Firefox 浏览器 
使用 Chrome 浏览器 使用 Safari 浏览器 使用 IE 浏览器 等等 
这种 类型 的 特征 可以 被 编码 为 整型 int 
如 男性 来自 美国 使用 IE 浏览器 可以 表示 成 
0 1 3 女性 来自 亚洲 使用 Chrome 浏览器 可以 
表示 成 1 2 1 这些 整 数式 的 表示 
不能 直接 作为 sklearn 的 参数 因为 我们 需要 的 
是 连续型 的 输入 而且 我们 通常 是 有序 的 
翻译 这些 特征 而 不是 所有 的 特征 都是 有序化 
的 譬如 浏览器 就是 按 人工 排 的 序列 将 
这些 类别 特征 转化成 sklearn 参数 中 可以 使用 的 
方法 是 使用 one of K 或者 one hot 编码 
独 热 编码 OneHotEncoder 它 可以 把 每 一个 有m种/nr 
类别 的 特征 转化成 m 中二 值 特征 举例 如下 
1 enc = preprocessing . OneHotEncoder 2 # input 3 
enc . fit 0 0 3 1 1 0 0 
2 1 1 0 2 4 OneHotEncoder categorical _ features 
= all dtype = . . . float handle _ 
unknown = error n _ values = auto sparse = 
True 5 # transform 6 enc . transform 0 1 
3 . toarray 7 # out 8 array 1 . 
0 . 0 . 1 . 0 . 0 . 
0 . 0 . 1 . 默认 情况 下 特征 
的 类别 数量 是从 数据集 里 自动 判断 出来 的 
当然 你 也 可以 用 n _ values 这个 参数 
我们 刚刚 举 的 例子 中 有 两种 性别 三种 
地名 和 四种 浏览器 当 我们 fit 之后 就 可以 
将 我们 的 数据 转化 为数 值了 从 结果 中 
来看 第一 个 数字 代表 性别 0 1 代表 男性 
女性 第二 个 数字 代表 地名 0 1 2 代表 
欧洲 美国 亚洲 最后 一个 数字 代表 浏览器 3 0 
1 2 代表 四种 浏览器 此外 字典 格式 也 可以 
编码   Loading features from d i c t s 
O n e H o t E n c o 
d e r 参数 class   sklearn . preprocessing . 
OneHotEncoder n _ values = auto   categorical _ features 
= all   dtype = class float   sparse = 
True   handle _ unknown = error n _ values 
  auto int or array of ints 每个 特征 的 
数量 auto 从 训练 数据 的 范围 中 得到 int 
所有 特征 的 最大 值 number array 每个 特征 的 
最大 值 number categorical _ features all or array of 
indices or mask   确定 哪些 特征 是 类别 特征 
all 默认 所有 特征 都是 类别 特征 意味着 所有 特征 
都要/nr 进行 OneHot 编码 array of indices 类别 特征 的 
数组 索引 mask n _ features 长度 的 数组 切 
dtype = bool 非 类别 型 特征 通常会 放到 矩阵 
的 右边 dtype   number type default = np . 
float 输出 数据 的 类型 sparse   boolean default = 
True 设置 True 会 返回 稀疏 矩阵 否则 返回 数组 
handle _ unknown   str error or ignore 当 一个 
不明 类别 特征 出现 在 变换 中时 报错 还是 忽略 
六 缺失 值 的 插补 上篇 我们 讲了 五种 方法 
来 解决 缺失 值 的 问题 其实 sklearn 里 也 
有一个 工具 Imputer 可以 对 缺失 值 进行 插补 Imputer 
类 可以 对 缺失 值 进行 均值 插补 中位数 插补 
或者 某行 / 列 出现 的 频率 最高 的 值 
进行 插补 也 可以 对 不同 的 缺失 值 进行 
编码 并且 支持 稀疏 矩阵 1 import numpy as np 
2 from sklearn . preprocessing import Imputer 3 # 用 
均值 插补 缺失 值 4 imp = Imputer missing _ 
values = NaN strategy = mean axis = 0 5 
imp . fit 1 2 np . nan 3 7 
6 6 Imputer axis = 0 copy = True missing 
_ values = NaN strategy = mean verbose = 0 
7 X = np . nan 2 6 np . 
nan 7 6 8 print imp . transform X 9 
4 . 2 . 10 6 . 3.666 . . 
. 11 7 . 6 . 12 13 # 对 
稀疏 矩阵 进行 缺失 值 插补 14 import scipy . 
sparse as sp 15 X = sp . csc _ 
matrix 1 2 0 3 7 6 16 imp = 
Imputer missing _ values = 0 strategy = mean axis 
= 0 17 imp . fit X 18 Imputer axis 
= 0 copy = True missing _ values = 0 
strategy = mean verbose = 0 19 X _ test 
= sp . csc _ matrix 0 2 6 0 
7 6 20 print imp . transform X _ test 
21 4 . 2 . 22 6 . 3.666 . 
. . 23 7 . 6 . 在 稀疏 矩阵 
中 缺失 值 被 编码 为 0 存储 为 矩阵 
中 这种 格式 是 适合 于 缺失 值 比 非 
缺失 值 多得多 的 情况 此外 Imputer 类 也 可以 
用于 Pipeline 中 Imputor 类 的 参数 class   sklearn 
. preprocessing . Imputer missing _ values = NaN   
strategy = mean   axis = 0   verbose = 
0   copy = True missing _ values   int 
或 NaN 默认 NaN String 类型 strategy   string 默认 
为 mean 可选 则 mean median most _ frequentaxis   
int 默认 为 0 axis = 0 对列 进行 插值 
axis = 1 对 行 进行 插值 verbose   int 
默认 为 0copy   boolean 默认 为 TrueTrue 会 创建 
一个 X 的 副本 False 在 任何 合适 的 地方 
都会 进行 插值 但是 以下 四 种 情况 计算 设置 
的 copy = Fasle 也会 创建 一个 副本 1 . 
X 不是 浮点 型 数组 2 . X 是 稀疏 
矩阵 而且 miss _ value = 03 . axis = 
0 X 被 编码 为 CSR 矩阵 4 . axis 
= 1 X 被 编码 为 CSC 矩阵 举个 实例 
在用 随机 森林 算法 之前 先用 Imputer 类 进行 处理 
1 import numpy as np 2 3 from sklearn . 
datasets import load _ boston 4 from sklearn . ensemble 
import R a n d o m F o r 
e s t R e g r e s s 
o r 5 from sklearn . pipeline import Pipeline 6 
from sklearn . preprocessing import Imputer 7 from sklearn . 
cross _ validation import cross _ val _ score 8 
9 rng = np . random . RandomState 0 10 
11 dataset = load _ boston 12 X _ full 
y _ full = dataset . data dataset . target 
13 n _ samples = X _ full . shape 
0 14 n _ features = X _ full . 
shape 1 15 16 # Estimate the score on the 
entire dataset with no missing values 17 estimator = R 
a n d o m F o r e s 
t R e g r e s s o r 
random _ state = 0 n _ estimators = 100 
18 score = cross _ val _ score estimator X 
_ full y _ full . mean 19 print Score 
with the entire dataset = % . 2f % score 
20 21 # Add missing values in 75% of the 
lines 22 missing _ rate = 0.75 23 n _ 
missing _ samples = np . floor n _ samples 
* missing _ rate 24 missing _ samples = np 
. hstack np . zeros n _ samples n _ 
missing _ samples 25 dtype = np . bool 26 
np . ones n _ missing _ samples 27 dtype 
= np . bool 28 rng . shuffle missing _ 
samples 29 missing _ features = rng . randint 0 
n _ features n _ missing _ samples 30 31 
# Estimate the score without the lines containing missing values 
32 X _ filtered = X _ full ~ missing 
_ samples 33 y _ filtered = y _ full 
~ missing _ samples 34 estimator = R a n 
d o m F o r e s t R 
e g r e s s o r random _ 
state = 0 n _ estimators = 100 35 score 
= cross _ val _ score estimator X _ filtered 
y _ filtered . mean 36 print Score without the 
samples containing missing values = % . 2f % score 
37 38 # Estimate the score after imputation of the 
missing values 39 X _ missing = X _ full 
. copy 40 X _ missing np . where missing 
_ samples 0 missing _ features = 0 41 y 
_ missing = y _ full . copy 42 estimator 
= Pipeline imputer Imputer missing _ values = 0 43 
strategy = mean 44 axis = 0 45 forest R 
a n d o m F o r e s 
t R e g r e s s o r 
random _ state = 0 46 n _ estimators = 
100 47 score = cross _ val _ score estimator 
X _ missing y _ missing . mean 48 print 
Score after imputation of the missing values = % . 
2f % score 结果 Score with the entire dataset = 
0.56 Score without the samples containing missing values = 0.48 
Score after imputation of the missing values = 0.55 七 
生成 多项式 特征 在 输入 数据 中 增加 非线性 特征 
可以 有效 的 提高 模型 的 复杂度 简单 且 常用 
的 方法 就是 使用 多项式 特征 polynomial features 可以 得到 
特征 的 高阶 交叉 项 1 import numpy as np 
2 from sklearn . preprocessing import P o l y 
n o m i a l F e a t 
u r e s 3 X = np . arange 
6 . reshape 3 2 4 X 5 array 0 
1 6 2 3 7 4 5 8 poly = 
P o l y n o m i a l 
F e a t u r e s 2 9 
poly . fit _ transform X 10 array 1 . 
0 . 1 . 0 . 0 . 1 . 
11 1 . 2 . 3 . 4 . 6 
. 9 . 12 1 . 4 . 5 . 
16 . 20 . 25 . 然而 有 时候 我们 
只 需要 特征 的 交叉 项 可以 设置 interaction _ 
only = True 来 得到 1 X = np . 
arange 9 . reshape 3 3 2 X 3 array 
0 1 2 4 3 4 5 5 6 7 
8 6 poly = P o l y n o 
m i a l F e a t u r 
e s degree = 3 interaction _ only = True 
7 poly . fit _ transform X 8 array 1 
. 0 . 1 . 2 . 0 . 0 
. 2 . 0 . 9 1 . 3 . 
4 . 5 . 12 . 15 . 20 . 
60 . 10 1 . 6 . 7 . 8 
. 42 . 48 . 56 . 336 . 这个 
方法 可能 大家 在 工作 中 比较 少见 但 世界 
上 它 经常 用于 核 方法 中 如 选择 多项式 
核 时 /nr   sklearn . svm . SVC   sklearn 
. decomposition . KernelPCA 八 自定义 转换 如果 以上 的 
方法 觉得 都 不够 譬如 你 想用 对 数据 取 
对数 可以 自己 用   F u n c t 
i o n T r a n s f o 
r m e r 自定义 一个 转化器 并且 可以 在 
Pipeline 中 使用 1 import numpy as np 2 from 
sklearn . preprocessing import F u n c t i 
o n T r a n s f o r 
m e r 3 transformer = F u n c 
t i o n T r a n s f 
o r m e r np . log1p # 括号 
内 的 就是 自定义 函数 4 X = np . 
array 0 1 2 3 5 transformer . transform X 
6 array 0 . 0.69314718 7 1.09861229 1.38629436 告诉 你 
怎么 用 如果 你 在 做一个 分类 任务 时 发现 
第一 主 成分 与 这个 不 相关 你 可以 用 
F u n c t i o n T r 
a n s f o r m e r 把 
第一 列 除去 剩下 的 列 用 PCA 1 import 
matplotlib . pyplot as plt 2 import numpy as np 
3 4 from sklearn . cross _ validation import train 
_ test _ split 5 from sklearn . decomposition import 
PCA 6 from sklearn . pipeline import make _ pipeline 
7 # from sklearn . preprocessing import F u n 
c t i o n T r a n s 
f o r m e r 8 # 如果 报错 
ImportError cannot import name F u n c t i 
o n T r a n s f o r 
m e r 可以 使用 下面 的 语句 9 from 
sklearn . preprocessing import * 10 11 12 def _ 
generate _ vector shift = 0.5 noise = 15 13 
return np . arange 1000 + np . random . 
rand 1000 shift * noise 14 15 16 def generate 
_ dataset 17 18 This dataset is two lines with 
a slope ~ 1 where one has 19 a y 
offset of ~ 100 20 21 return np . vstack 
22 np . vstack 23 _ generate _ vector 24 
_ generate _ vector + 100 25 . T 26 
np . vstack 27 _ generate _ vector 28 _ 
generate _ vector 29 . T 30 np . hstack 
np . zeros 1000 np . ones 1000 31 32 
33 def all _ but _ first _ column X 
34 return X 1 35 36 37 def drop _ 
first _ component X y 38 39 Create a pipeline 
with PCA and the column selector and use it to 
40 transform the dataset . 41 42 pipeline = make 
_ pipeline 43 PCA F u n c t i 
o n T r a n s f o r 
m e r all _ but _ first _ column 
44 45 X _ train X _ test y _ 
train y _ test = train _ test _ split 
X y 46 pipeline . fit X _ train y 
_ train 47 return pipeline . transform X _ test 
y _ test 48 49 50 if _ _ name 
_ _ = = _ _ main _ _ 51 
X y = generate _ dataset 52 plt . scatter 
X 0 X 1 c = y s = 50 
53 plt . show 54 X _ transformed y _ 
transformed = drop _ first _ component * generate _ 
dataset 55 plt . scatter 56 X _ transformed 0 
57 np . zeros len X _ transformed 58 c 
= y _ transformed 59 s = 50 60 61 
plt . show 结果 写 到 这里 基本上 关于 数据 
转化 的 方法 已经 介绍 的 差不多 了 周四 写 
第三篇 数据 降 维 写 的 比较 仓促 有 错误 
的 欢迎 提出来 ~ 