使用 机器学习 排序算法 LambdaMART 有 一段 时间 了 但 一直 
没有 真正 弄清楚 算法 中 的 所有 细节 学习 过程 
中 细 读了 两篇 不错 的 博文 推荐 给 大家 
梯度 提升 树 GBDT 原理 小结 徐博 From RankNet to 
LambdaRank to LambdaMART An Overview 但 经过 一番 搜寻 之后 
发现 目前 网上 并 没有 一篇 透彻 讲解 该 算法 
的 文章 所以 希望 这 篇 文章 能够 达到 此 
目的 本文 主要 参考 微软 研究院 2010年 发表 的 文章 
From RankNet to LambdaRank to LambdaMART An Overview $ ^ 
1 $ 并 结合 自己 的 理解 试图 将 RankNet 
LambdaRank 和 LambdaMART 这三种 算法 的 所有 算法 细节 讲解 
透彻 1 . 概述 RankNet LambdaRank 和 LambdaMART 是 三个 
关系 非常 紧密 的 机器学习 排序算法 简而言之 RankNet 是 最 
基础 基于 神经 网络 的 排序算法 而 LambdaRank 在 RankNet 
的 基础 上 修改 了 梯度 的 计算 方式 也即 
加入 了 lambda 梯度 LambdaMART 结合 了 lambda 梯度 和 
MART 另 称为 GBDT 梯度 提升 树 这三种 算法 在 
工业界 中 应用 广泛 在/p BAT/w 等/u 国内/s 大厂/n 和/c 
微软/a 谷歌/nr 等/u 世界/n 互联网/n 巨头/n 内部/f 都有/nr 大量/n 应用/v 
还 曾经 赢得 Yahoo Learning To Rank Challenge Track 1 
的 冠军 本人 认为 如果 评选 当今 工业界 中 三种 
最 重要 的 机器学习 算法 以 LambdaMART 为 代表 的 
集成 学习 算法 肯定 占有 一席之地 另外 两个 分别 是 
支持 向量 机 和 深度 学习 2 . RankNet2 . 
1 算法 基础 定义 RankNet 解决 如下 搜索 排序问题 给定 
query 集合 每个 query 都 对应 着 一个 文档 集合 
如何 对 每个 query 返回 排序 后的/nr 文档 集合 可以 
想象 这样 的 场景 某位 高考生 在 得知 自己 的 
成绩 后 准备 报考 志愿 听说 最近 西湖 大学 办 
得 不错 所以 就 想到 网上 搜搜 关于 西湖 大学 
的 资料 他 打开 一个 搜索引擎 输入 西湖 大学 四 
个字 然后 点击 搜索 页面 从上到下 显示 了 10条 搜索 
结果 他 认为 排在 上面 的 肯定 比 下面 的 
相关 所以 就 开始 从上往下 一个个 地 浏览 所以 RankNet 
的 目标 就是 对 所有 query 都能 将其 返回 的 
文档 按照 相关性 进行 排序 RankNet 网络 将 输入 query 
的 特征向量 $ x \ in \ mathbb { R 
} ^ n $ 映射 为 一个 实数 $ f 
x \ in \ mathbb { R } $ RankNet 
采用 pairwise 的 方法 进行 模型 训练 具体 地 给定 
特定 query 下 的 两个 文档 $ U _ i 
$ 和$U/nr _ j $ 其 特征向量 分别为 $ x 
_ i $ 和$x/nr _ j $ 经过 RankNet 进行 
前 向 计算 得到 对应 的 分数 为 $ s 
_ i = f x _ i $ 和$s/nr _ 
j = f x _ j $ 用 $ U 
_ i \ rhd U _ j $ 表示 $ 
U _ i $ 比 $ U _ j $ 
排序 更 靠前 如对 某个 query 来说 $ U _ 
i $ 被 标记 为 good $ U _ j 
$ 被 标记 为 bad 继而 可以 用 下面 的 
公式 来 表示 $ U _ i $ 应该 比 
$ U _ j $ 排序 更 靠前 的 概率 
$ $ P _ { ij } \ equiv P 
U _ i \ rhd U _ j \ equiv 
\ frac { 1 } { 1 + e ^ 
{ \ sigma s _ i s _ j } 
} $ $ 这个 概率 实际上 就是 深度 学习 中 
经常 使用 的 sigmoid 函数 参数 $ \ sigma $ 
决定 sigmoid 函数 的 形状 对于 特定 的 query 定义 
$ _ { ij } \ in \ { 0 
\ pm1 \ } $ 为 文档 $ i $ 
和 文档 $ j $ 被 标记 的 标签 之间 
的 关联 即 $ $ _ { ij } = 
\ left \ { \ begin { aligned } 1 
& &       文档 i 比 文档 j 
更 相关 \ \ 0 & &     文档 
i 和 文档 j 相关性 一致 \ \ 1 & 
&     文档 j 比 文档 i 更 相关 
\ end { aligned } \ right . $ $ 
定义 $ \ overline { P } _ { ij 
} = \ frac { 1 } { 2 } 
1 + _ { ij } $ 表示 $ U 
_ i $ 应该 比 $ U _ j $ 
排序 更 靠前 的 已知 概率 则 可以 用 交叉 
熵 定义 优化 目标 的 损失 函数 $ $ C 
= \ overline { P } _ { ij } 
log { P _ { ij } } 1 \ 
overline { P } _ { ij } log 1 
P _ { ij } $ $ 如果 不太熟悉 什么 
是 交叉 熵 可以 参考 宗 成庆 老师 的 统计 
自然语言 处理 2.2节 信息论 基本概念 里面 将 熵 联合 熵 
互信息 相对 熵 交叉/n 熵/g 和/c 困惑/a 度/zg 等/u 概念/n 
都/d 讲得/i 相当/d 清楚/a 结合 以 上 多个 公式 可以 
改写 损失 函数 $ C $ 为 $ $ C 
= \ frac { 1 } { 2 } 1 
_ { ij } \ sigma s _ i s 
_ j + log 1 + e ^ { \ 
sigma s _ i s _ j } $ $ 
对于 $ _ { ij } = 1 $ $ 
$ C = log \ left 1 + e ^ 
{ \ sigma s _ i s _ j } 
\ right $ $ 然而 对于 $ _ { ij 
} = 1 $ $ $ C = log \ 
left 1 + e ^ { \ sigma s _ 
j s _ i } \ right $ $ 可以 
看出 损失 函数 $ C $ 具有 对称性 也即 交换 
$ i $ 和$j/nr $ 的 位置 损失 函数 的 
值 不变 分析 损失 函数 $ C $ 的 趋势 
发现 如果 对 文档 $ U _ i $ 和$U/nr 
_ j $ 的 打分 可以 正确地 拟合 标记 的 
标签 则 $ C $ 趋向于 0 否则 $ C 
$ 趋向于 线性函数 具体 地 假如 $ _ { ij 
} = 1 $ 也即 $ U _ i $ 
应该 比 $ U _ j $ 排序 高 如果 
$ s _ i s _ j $ 则 拟合 
的 分数 可以 正确 排序 文档 $ i $ 和 
文档 $ j $ $ $ \ lim \ limits 
_ { s _ i s _ j \ rightarrow 
\ infty } C = \ lim \ limits _ 
{ s _ i s _ j \ rightarrow \ 
infty } log \ left 1 + e ^ { 
\ sigma s _ i s _ j } \ 
right = log1 = 0 $ $ 如果 $ s 
_ i s _ j $ 则 拟合 的 分数 
不能 正确 排序 文档 $ i $ 和 文档 $ 
j $ $ $ \ lim \ limits _ { 
s _ i s _ j \ rightarrow \ infty 
} C = \ lim \ limits _ { s 
_ i s _ j \ rightarrow \ infty } 
log \ left 1 + e ^ { \ sigma 
s _ i s _ j } \ right = 
log \ left e ^ { \ sigma s _ 
i s _ j } \ right = \ sigma 
s _ i s _ j $ $ 利用 神经 
网络 对 模型 进行 训练 目前 最 有效 的 方法 
就是 反向 传播 算法 反向 传播 算法 中最 核心 部分 
就是 损失 函数 对模型 参数 的 求导 然后 可以 使用 
下面 的 公式 对模型 参数 进行 迭代 更新 $ $ 
w _ k \ leftarrow { w _ k } 
\ eta \ frac { \ partial { C } 
} { \ partial { w _ k } } 
= { w _ k } \ eta \ left 
\ frac { \ partial { C } } { 
\ partial { s _ i } } \ frac 
{ \ partial { s _ i } } { 
\ partial { w _ k } } + \ 
frac { \ partial { C } } { \ 
partial { s _ j } } \ frac { 
\ partial { s _ j } } { \ 
partial { w _ k } } \ right $ 
$ 损失 函数 $ C $ 对 $ s _ 
i $ 和$s/nr _ j $ 的 偏 导数 为 
$ $ \ frac { \ partial { C } 
} { \ partial { s _ i } } 
= \ sigma \ left \ frac { 1 } 
{ 2 } 1 _ { ij } \ frac 
{ 1 } { 1 + e ^ { \ 
sigma s _ i s _ j } } \ 
right = \ frac { \ partial { C } 
} { \ partial { s _ j } } 
$ $ $ s _ i $ 和$s/nr _ j 
$ 对 $ w _ k $ 的 偏 导数 
可根据 神经网络 求 偏 导数 的 方式 求得 求得 了 
损失 函数 $ C $ 对 神经 网络 模型 参数 
$ w _ k $ 的 偏 导数 之后 就 
可以 使用 梯度 下降 算法 对其 更新 这里 的 学习率 
$ \ eta $ 也 是 一个 正数 因为 $ 
\ eta $ 需要 满足 下面 的 不等式 $ $ 
\ delta C = \ sum _ { k } 
\ frac { \ partial { C } } { 
\ partial { w _ k } } \ delta 
w _ k = \ sum _ { k } 
\ frac { \ partial { C } } { 
\ partial { w _ k } } \ left 
\ eta \ frac { \ partial { C } 
} { \ partial { w _ k } } 
\ right = \ eta \ sum _ { k 
} \ left \ frac { \ partial { C 
} } { \ partial { w _ k } 
} \ right ^ 2 0 $ $ 2.2 RankNet 
分解 形式 加速 RankNet 训练 过程 2.1节 中 定义 的 
RankNet 对于 每一个 文档 对 $ U _ i $ 
$ U _ j $ 都将 计算 损失 函数 对 
神经 网络 的 参数 $ w _ k $ 的 
偏 导数 然后 更新 模型 参数 $ w _ k 
$ 这样 做 的 缺点 在于 对模型 参数 更新 慢 
耗时 长 所以 本节 讲解 如何 通过 分解 组合 的 
方式 加快 这 一 训练 过程 对于 给定 的 文档 
对 $ U _ i $ 和$U/nr _ j $ 
损失 函数 $ C $ 对 参数 $ w _ 
k $ 的 偏 导数 为 $ $ \ frac 
{ \ partial { C } } { \ partial 
{ w _ k } } = \ frac { 
\ partial { C } } { \ partial { 
s _ i } } \ frac { \ partial 
{ s _ i } } { \ partial { 
w _ k } } + \ frac { \ 
partial { C } } { \ partial { s 
_ j } } \ frac { \ partial { 
s _ j } } { \ partial { w 
_ k } } = \ sigma \ left \ 
frac { 1 } { 2 } 1 _ { 
ij } \ frac { 1 } { 1 + 
e ^ { \ sigma s _ i s _ 
j } } \ right \ left \ frac { 
\ partial { s _ i } } { \ 
partial { w _ k } } \ frac { 
\ partial { s _ j } } { \ 
partial { w _ k } } \ right = 
\ lambda _ { ij } \ left \ frac 
{ \ partial { s _ i } } { 
\ partial { w _ k } } \ frac 
{ \ partial { s _ j } } { 
\ partial { w _ k } } \ right 
$ $ 其中 $ $ \ lambda _ { ij 
} = \ frac { \ partial { C s 
_ i s _ j } } { \ partial 
{ s _ i } } = \ sigma \ 
left \ frac { 1 } { 2 } 1 
_ { ij } \ frac { 1 } { 
1 + e ^ { \ sigma s _ i 
s _ j } } \ right $ $ 定义 
$ I $ 为 索引 对 $ \ { i 
j \ } $ 的 集合 在 不 损失 信息量 
的 情况 下 可以 将 集合 $ I $ 中的 
索引 对 都 转换成 满足 $ U _ i \ 
rhd U _ j $ 的 形式 另外 集合 $ 
I $ 中的 索引 对 还 应该 满足 最多 只 
出现 一次 的 条件 在此 基础 上 累加 权重 参数 
$ w _ k $ 的 更新 量 $ $ 
\ delta w _ k = \ eta \ sum 
_ { i j \ in I } \ left 
\ lambda _ { ij } \ frac { \ 
partial { s _ i } } { \ partial 
{ w _ k } } \ lambda _ { 
ij } \ frac { \ partial { s _ 
j } } { \ partial { w _ k 
} } \ right = \ eta \ sum _ 
{ i } \ lambda _ i \ frac { 
\ partial { s _ i } } { \ 
partial { w _ k } } $ $ 其中 
$ $ \ lambda _ i = \ sum _ 
{ j \ { i j \ } \ in 
I } \ lambda _ { ij } \ sum 
_ { j \ { j i \ } \ 
in I } \ lambda _ { ij } $ 
$ 通俗 地 说 $ \ lambda _ i $ 
就是 集合 $ I $ 中 所有 $ \ { 
i j \ } $ 的 $ \ lambda _ 
{ ij } $ 的 和$/nr $ 集合 $ I 
$ 中 所有 $ \ { j i \ } 
$ 的 $ \ lambda _ { ij } $ 
的 和 如果 还是 不 太 明白 那看/nr 下面 这个 
例子 就 明白 了 集合 $ I = \ { 
\ { 1 2 \ } \ { 2 3 
\ } \ { 1 3 \ } \ } 
$ 则 $ $ \ delta w _ k = 
\ eta \ sum _ { \ { i j 
\ } \ in I } \ left \ lambda 
_ { ij } \ frac { \ partial { 
s _ i } } { \ partial { w 
_ k } } \ lambda _ { ij } 
\ frac { \ partial { s _ j } 
} { \ partial { w _ k } } 
\ right = \ eta \ left \ lambda _ 
{ 12 } \ frac { \ partial { s 
_ 1 } } { \ partial { w _ 
k } } \ lambda _ { 12 } \ 
frac { \ partial { s _ 2 } } 
{ \ partial { w _ k } } + 
\ lambda _ { 13 } \ frac { \ 
partial { s _ 1 } } { \ partial 
{ w _ k } } \ lambda _ { 
13 } \ frac { \ partial { s _ 
3 } } { \ partial { w _ k 
} } + \ lambda _ { 23 } \ 
frac { \ partial { s _ 2 } } 
{ \ partial { w _ k } } \ 
lambda _ { 23 } \ frac { \ partial 
{ s _ 3 } } { \ partial { 
w _ k } } \ right = \ eta 
\ left \ lambda _ { 12 } + \ 
lambda _ { 13 } \ frac { \ partial 
{ s _ 1 } } { \ partial { 
w _ k } } + \ lambda _ { 
23 } \ lambda _ { 12 } \ frac 
{ \ partial { s _ 2 } } { 
\ partial { w _ k } } + \ 
lambda _ { 23 } \ lambda _ { 13 
} \ frac { \ partial { s _ 3 
} } { \ partial { w _ k } 
} \ right $ $ 于是/nr 可以 得到 $ \ 
lambda _ 1 = \ lambda _ { 12 } 
+ \ lambda _ { 13 } $ $ \ 
lambda _ 2 = \ lambda _ { 23 } 
\ lambda _ { 12 } $ $ \ lambda 
_ 3 = \ lambda _ { 23 } \ 
lambda _ { 13 } $ $ \ lambda _ 
i $ 可以 看成 是 作用 在 排序 文档 上 
的 力 其 正负 代表 了 方向 长度 代表 了 
力 的 大小 最初 的 实现 是 对 每个 文档 
对 都 计算 一遍 梯度 并且 更新 神经 网络 的 
参数值 而 这里 则是 将 同一 个 query 下 的 
所有 文档 对 进行 叠加 然后 更新 一次 网络 的 
权重 参数 这种 分解 组合 形式 实际上 就是 一种 小批量 
学习 方法 不仅 可以 加快 迭代 速度 还 可以 为 
后面 使用 非 连续 的 梯度 模型 打下基础 2.3 模型 
训练 过程 示例 假设 某个 搜索 系统 中 文档 用 
2 维 的 特征向量 表示 给定 一个 query 下 的 
三个 文档 向量 分别为 $ x _ 1 = 5 
4.5 ^ T $ $ x _ 2 = 4 
3.7 ^ T $ 和$x/nr _ 3 = 2 1.8 
^ T $ 标记 情况 为 $ U _ 1 
\ rhd U _ 2 \ rhd U _ 3 
$ 为了 简化 训练 过程 这里 采用 单层 的 神经 
网络 模型 即 输入 层 大小 2 输出 层 大小 
为 1 输出 值 为 $ f x = w 
_ 0 + w _ 1x ^ { 1 } 
+ w _ 2x ^ { 2 } $ 初始化 
$ \ mathbf { w } = 0 1 1 
$ 控制 sigmoid 函数 形状 的 $ \ sigma = 
0.1 $ 神经网络 学习率 $ \ eta = 0.1 $ 
根据 以上 初始值 可以 计算出 $ s _ 1 = 
0.5 $ $ s _ 2 = 0.3 $ 和$s/nr 
_ 3 = 0.2 $ 可见 此时 三个 文档 输出 
的 分数 并不 满足 标记 $ U _ 1 \ 
rhd U _ 2 \ rhd U _ 3 $ 
计算 $ \ lambda _ 1 = \ lambda _ 
{ 12 } + \ lambda _ { 13 } 
= 0.1012 $ $ \ lambda _ 2 = \ 
lambda _ { 23 } \ lambda _ { 12 
} = 0.0002 $ $ \ lambda _ 3 = 
\ lambda _ { 23 } \ lambda _ { 
13 } = 0.1010 $ $ \ delta w _ 
0 = \ eta \ left \ lambda _ 1 
\ frac { \ partial { s _ 1 } 
} { \ partial { w _ 0 } } 
+ \ lambda _ 2 \ frac { \ partial 
{ s _ 2 } } { \ partial { 
w _ 0 } } + \ lambda _ 3 
\ frac { \ partial { s _ 3 } 
} { \ partial { w _ 0 } } 
\ right = 0 $ $ \ delta w _ 
1 = \ eta \ left \ lambda _ 1 
\ frac { \ partial { s _ 1 } 
} { \ partial { w _ 1 } } 
+ \ lambda _ 2 \ frac { \ partial 
{ s _ 2 } } { \ partial { 
w _ 1 } } + \ lambda _ 3 
\ frac { \ partial { s _ 3 } 
} { \ partial { w _ 1 } } 
\ right = 3.032 $ $ \ delta w _ 
2 = \ eta \ left \ lambda _ 1 
\ frac { \ partial { s _ 1 } 
} { \ partial { w _ 2 } } 
+ \ lambda _ 2 \ frac { \ partial 
{ s _ 2 } } { \ partial { 
w _ 2 } } + \ lambda _ 3 
\ frac { \ partial { s _ 3 } 
} { \ partial { w _ 2 } } 
\ right = 2.7286 $ 更新 网络 权重 $ w 
_ 0 = w0 + \ delta w _ 0 
= 0 + 0 = 0 $ $ w _ 
1 = w1 + \ delta w _ 1 = 
1 + 3.032 = 2.032 $ $ w _ 2 
= w2 + \ delta w _ 2 = 1 
+ 2.7286 = 3.7286 $ 使用 更新 后的/nr 权重 重新 
计算 三个 文档 的 分数 分别为 $ s _ 1 
= 26.9387 $ $ s _ 2 = 21.92382 $ 
$ s _ 3 = 10.77548 $ 可见 经过 一轮 
训练 单层 神经 网络 的 输出 分数 已经 可以 很好 
地 拟合 标记 的 标签 3 . 信息检索 评分 信息检索 
研究者 经常 使用 的 排序 质量 评分 指标 有 以下 
四种 MRR Mean Reciprocal Rank 平均 倒数 排名 MAP Mean 
Average Precision 平均 正确率 均值 NDCG Normalized Discounted Cumulative Gain 
归一化 折损 累积 增益 ERR Expected   Reciprocal Rank 预期 
倒数 排名 其中 MRR 和 MAP 只能 对 二级 的 
相关性 排序 等级 相关 和 不相关 进行 评分 而 NDCG 
和 ERR 则 可以 对 多级 的 相关性 排序 等级 
2 进行 评分 NDCG 和 ERR 的 另一个 优点 是 
更 关注 排名 靠前 的 文档 在 计算 分数 时会 
给予 排名 靠前 的 文档 更高 的 权重 但是 这 
两种 评分 方式 的 缺点 是 函数 不连续 不能 进行 
求导 所以 也 就 不能 简单 地 将 这两种 评分 
方式 加入 到 模型 的 损失 函数 中去 3.1 MRR 
对于 一个 查询 $ i $ 来说 $ rank _ 
i $ 表示 第 一个 相关 结果 的 排序 位置 
所以 $ $ MRR Q = \ frac { 1 
} { | Q | } \ sum _ { 
i = 1 } ^ { | Q | } 
\ frac { 1 } { rank _ i } 
$ $ $ | Q | $ 表示 查询 的 
数量 $ MRR $ 表示 搜索 系统 在 查询 集 
$ Q $ 下 的 平均 倒数 排名 值 $ 
MRR $ 只能 度量 检索 结果 只有 一个 并且 相关性 
等级 只有 相关 和 不相关 两种 的 情况 举个 简单 
例子 查询 语句 查询 结果 正确 结果 排序 位置 排序 
倒数 机器学习 快速排序 深度 学习 并行计算 深度 学习 21/2 苹果 
手机 小米 手机 华为 手机 iphone 7iphone 731/3 小米 移动 
电源 小米 移动 电源 华为 充电器 苹果 充 电插头 小米 
移动 电源 11/1 所以 $ MRR Q = \ frac 
{ 1/2 + 1/3 + 1 } { 3 } 
= \ frac { 11 } { 18 } $ 
3.2 MAP 假定 信息 需求 $ q _ j \ 
in Q $ 对应 的 所有 相关 文档 集 合为 
$ { d _ { 1 } . . . 
d _ { mj } } $ $ R _ 
{ jk } $ 是 返回 结果 中 直到 遇到 
$ d _ k $ 后其/nr 所在 位置 前 含 
$ d _ k $ 的 所有 文档 的 集合 
则 定义 $ MAP Q ^ 2 $ 如下 $ 
$ MAP Q = \ frac { 1 } { 
| Q | } \ sum _ { j = 
1 } ^ { | Q | } \ frac 
{ 1 } { m _ j } \ sum 
_ { k = 1 } ^ { m _ 
j } Precision R _ { jk } $/i $/i 
实际上/d 有/v 两种/m 计算/v $/i MAP/w $/i 的/uj 方法/n 或者/c 
说/v 有/v 两种/m $/i MAP/w Q $ 的 定义方法 第 
一种 方法 是 在 每篇 相关 文档 所在 位置 上 
求 正确率 然后 平均 参考 上面 的 公式 另一种 是 
在 每个 召回率 水平 上 计算 此时 的 插值 正确率 
然后 求 11点 平均 正确率 最后 在 不同 查询 之间 
计算 平均 前者 也称 为非 插值 $ MAP Q $ 
一般 提 $ MAP Q $ 都指/nr 前者 所有 这里 
也只 讨论 前者 如果 对 定义 的 公式 不太 理解 
可以 结合 下面 的 例子 进行 理解 查询 1 机器学习 
查询 2 苹果 手机 排序 位置 是否 相关 排序 位置 
是否 相关 1 是 1 否 2 是 2 是 
3 否 3 是 4 否 4 否 5 是 
5 否 6 否 6 是 7 否 7 是 
针对 上面 检索 的 结果 可 计算出 $ AP 1 
= \ left 1 * 1 + 1 * 1 
+ 2/3 * 0 + 2/4 * 0 + 3/5 
* 1 + 3/6 * 0 + 3/7 * 0 
\ right / 3 = \ frac { 13 } 
{ 15 } $ $ AP 2 = \ left 
0 * 0 + 1/2 * 1 + 2/3 * 
1 + 2/4 * 0 + 2/5 * 0 + 
3/6 * 1 + 4/7 * 1 \ right / 
4 = \ frac { 47 } { 84 } 
$ $ MAP Q = \ frac { AP 1 
+ AP 2 } { 2 } = \ frac 
{ 13/15 + 47/84 } { 2 } = \ 
frac { 599 } { 420 } $ 3.3 NDCGNDCG 
是 基于 前 $ k $ 个 检索 结果 进行 
计算 的 设 $ R j m $ 是 评价 
人员 给出 的 文档 $ d $ 对 查询 $ 
j $ 的 相关性 得分 那么 有 $ $ NDCG 
Q k = \ frac { 1 } { | 
Q | } \ sum _ { j = 1 
} ^ { | Q | } Z _ { 
j k } \ sum _ { m = 1 
} ^ { k } \ frac { 2 ^ 
{ R j m } 1 } { log 1 
+ m } $ $ 其中 $ $ DCG _ 
k = \ sum _ { m = 1 } 
^ { k } \ frac { 2 ^ { 
R j m } 1 } { log 1 + 
m } $ $ $ Z _ { j k 
} $ 为 第 $ j $ 个 查询 的 
DCG 归一化 因子 用于 保证 对于 查询 $ j $ 
最 完美 系统 的 $ DCG _ k $ 得分 
是 1 $ Z _ { j k } $ 
也 可以 用 $ \ frac { 1 } { 
IDCG _ k } $ 表示 $ m $ 是 
返回 文档 的 位置 如果 某 查询 返回 的 文档 
数 $ k k $ 那么 上述 公式 只需要 计算 
到 $ k $ 为止 修改 上面 简单 的 例子 
进行 辅助 理解 查询 1 机器学习 查询 2 苹果 手机 
排序 位置 相关 程度 排序 位置 相关 程度 1 3 
1 2 2 2 2 2 3 3 3 3 
4 0 4 1 5 1 5 2 6 2 
6 3 7 2 7 1 对于 查询 1 机器学习 
$ $ DCG _ 7 = \ sum _ { 
m = 1 } ^ { 7 } \ frac 
{ 2 ^ { R j m } 1 } 
{ log 1 + m } = 21.421516 $ $ 
查询 1 返回 结果 的 最佳 相关 程度 排序 为 
3 3 2 2 2 1 0 所以 $ IDCG 
_ 7 = 22.686817 $ $ NDCG _ 7 = 
\ frac { DCG _ 7 } { IDCG _ 
7 } = 0.944227 $ 对于 查询 2 苹果 手机 
$ $ DCG _ 7 = \ sum _ { 
m = 1 } ^ { 7 } \ frac 
{ 2 ^ { R j m } 1 } 
{ log 1 + m } = 18.482089 $ $ 
查询 2 返回 结果 的 最佳 相关 程度 排序 为 
3 3 2 2 2 1 1 所以 $ IDCG 
_ 7 = 23.167716 $ $ NDCG _ 7 = 
\ frac { DCG _ 7 } { IDCG _ 
7 } = 0.797752 $ 最后 可得 $ NDCG Q 
7 = 0.944227 + 0.797752 / 2 = 0.870990 $ 
3.4 ERR $ ERR ^ 3 $ 旨在 改善 NDCG 
计算 当前 结果 时未/nr 考虑 排 在前面 结果 的 影响 
的 缺点 提出 了 一种 基于 级联 模型 的 评价 
指标 首先 定义 $ $ R g = \ frac 
{ 2 ^ g 1 } { 2 ^ { 
g _ { max } } } g \ in 
\ { 0 1 . . . g _ { 
max } \ } $ $ $ g $ 代表 
文档 的 得分 级别 $ g _ { max } 
$ 代表 最大 的 分数 级别 于是 定义 $ $ 
ERR = \ sum _ { r = 1 } 
^ { n } \ frac { 1 } { 
r } \ prod _ { i = 1 } 
^ { r 1 } 1 R _ i R 
_ r $ $ 展开 公式 如下 $ $ ERR 
= R _ 1 + \ frac { 1 } 
{ 2 } 1 R _ 1 R _ 2 
+ \ frac { 1 } { 3 } 1 
R _ 1 1 R _ 2 R _ 3 
+ . . . + \ frac { 1 } 
{ n } 1 R _ 1 1 R _ 
2 . . . 1 R _ { n 1 
} R _ n $ $ 举例来说 $ g _ 
{ max } = 3 $ 查询 机器学习 排序 位置 
相关 程度 13223341 $ R _ 1 = 0.875 R2 
= 0.375 R _ 3 = 0.875 R _ 4 
= 0.125 $ $ ERR = 0.875 + \ frac 
{ 1 } { 2 } * 0.125 * 0.375 
+ \ frac { 1 } { 3 } * 
0.125 * 0.625 * 0.875 + \ frac { 1 
} { 4 } * 0.125 * 0.625 * 0.125 
* 0.125 = 0.913391 $ 4 . LambdaRank4 . 1 
为什么 需要 LambdaRank 先看 一张 论文 原文 中的 图 如下 
所示 这是 一组 用 二元 等级 相关性 进行 排序 的 
链接 地址 其中 浅 灰色 代表 链接 与 query 不相关 
深蓝色 代表 链接 与 query 相关 对于 左边 来说 总的 
pairwise 误差 为 13 而 右边 总的 pairwise 误差 为 
11 但是 大多数 情况 下 我们 更 期望 能 得到 
左边 的 结果 这说明 最 基本 的 pairwise 误差 计算 
方式 并 不能 很好 地 模拟 用户 对 搜索引擎 的 
期望 右边 黑色 箭头 代表 RankNet 计算出 的 梯度 大小 
红色 箭头 是 期望 的 梯度 大小 NDCG 和 ERR 
在 计算误差 时 排名 越 靠前 权重 越大 可以 很好 
地 解决 RankNet 计算误差 时的/nr 缺点 但是 NDCG 和 ERR 
均 是 不可导 的 函数 如何 加入到 RankNet 的 梯度 
计算 中去 4.2   LambdaRank 定义 RankNet 中的 $ \ 
lambda _ { ij } $ 可以 看成 是 $ 
U _ i $ 和$U/nr _ j $ 中间 的 
作用力 如果 $ U _ i \ rhd U _ 
j $ 则 $ U _ j $ 会 给予 
$ U _ i $ 向上 的 大小 为 $ 
| \ lambda _ { ij } | $ 的 
推动力 而 对应 地 $ U _ i $ 会 
给予 $ U _ j $ 向下 的 大小 为 
$ | \ lambda _ { ij } | $ 
的 推动力 如何将 NDCG 等 类似 更 关注 排名 靠前 
的 搜索 结果 的 评价 指标 加入到 排序 结果 之间 
的 推动力 中 去呢 实验 表明 直接 用 $ | 
\ Delta _ { NDCG } | $ 乘以 原来 
的 $ \ lambda _ { ij } $ 就 
可以 得到 很好 的 效果 也即 $ $ \ lambda 
_ { ij } = \ frac { \ partial 
{ C s _ i s _ j } } 
{ \ partial { s _ i } } = 
\ frac { \ sigma } { 1 + e 
^ { \ sigma s _ i s _ j 
} } | \ Delta _ { NDCG } | 
$ $ 其中 $ | \ Delta _ { NDCG 
} | $ 是 交换排序 结果 $ U _ i 
$ 和$U/nr _ j $ 得到 的 NDCG 差值 NDCG/w 
倾向于/i 将/d 排名/v 高/a 并且/c 相关性/l 高的/nr 文档/n 更快/d 地/uv 
向上/d 推动/v 而 排名 地 而且 相关性 较低 的 文档 
较慢 地 向上 推动 另外 还 可以 将 $ | 
\ Delta _ { NDCG } | $ 替换成 其他 
的 评价 指标 5 . LambdaMART5 . 1 MARTLambdaMART 是 
MART 和 LambdaRank 的 结合 所以 要 学习 LambdaMART 首先 
得 了解 什么 是 MART MART 是 Multiple Additive Regression 
Tree 的 简称 很多 时候 又 称为 GBDT Gradient Boosting 
Decision Tree MART 是 一种 集成 学习 算法 不同于 经典 
的 集成 学习 算法 Adaboost 利用 前 一轮 学习 器 
的 误差 来 更新 下 一轮 学习 的 样本 权重 
MART 每次 都 拟合 上 一轮 分类器 产生 的 残差 
举个 例子 便于 理解 比如 一个人 的 年龄 是 50岁 
第 一棵树 拟合 的 结果 是 35岁 第一轮 的 残差 
为 15岁 然后 第 二棵 数 拟合 的 结果 是 
10岁 两棵树 相加 总的 拟合 结果 是 45岁 第二轮 的 
残差 为 5岁 第三 棵 数 拟合 的 结果 为 
2岁 三棵树 相加 拟合 的 结果 是 47岁 第三轮 的 
残差 是 3岁 . . . . . . 只要 
如此 不断 地 进行 下去 拟合 结果 就 可以 达到 
50岁 拟合 残差 的 过程 就是 训练 数据 的 过程 
对于 一个 给定 的 数据集 $ \ { x _ 
i y _ i \ } i = 1 2 
. . . m $ 其中 特征向量 $ x _ 
i \ in \ mathbb { R } ^ n 
$ 标签 $ y _ i \ in \ mathbb 
{ R } $ 可以 用 $ x _ { 
ij } j = 1 2 . . . d 
来 代表 x _ i 的 第 j 个 特征值 
$ 对于 一个 典型 的 回归 决策树 问题 需要 遍历 
所有 特征 $ j $ 的 全部 阈值 $ t 
$ 找到 最优 的 $ j $ 和$t/nr $ 使下 
面的 等式 最小化 $ $ _ j = \ sum 
_ { i \ in L } y _ i 
\ mu _ L ^ 2 + \ sum _ 
{ i \ in R } y _ i \ 
mu _ R ^ 2 $ $ 其中 $ x 
_ { ij } \ leq t $ 的 所有 
样本 落入 左 子树 $ L $ 中 其中 $ 
x _ { ij } t $ 的 所有 样本 
落入 右 子树 $ R $ 中 $ \ mu 
_ L \ mu _ R $ 表示 左 子树 
右 子树 所有 样例 标签 值 的 均值 如果 这 
就是 一棵 最 简单 的 拥有 一个 根 节点 两个 
叶子 节点 的 二叉 回归 树 那么 只 需要 根据 
最优 阈值 切 分为 左右 子树 并且 分别 计算 左右 
子树 的 值 $ \ gamma _ l l = 
1 2 $ 即可 如果 将 划分 子树 的 过程 
继续进行 $ L 1 $ 次 即可 得到 一棵 包含 
$ L $ 个 叶子 节点 的 回归 树 上面 
公式 使用 最 小二 乘法 计算 拟合 误差 所以 通过 
上面 方法 得到 的 模型 又 称为 最小二乘 回归 树 
其实 不管 误差 的 计算 方式 如何 我们 都 可以 
拟合 出 相应 的 回归 树 唯一 的 区别 是 
梯度 的 计算 不同 而已 MART 使用 线性组合 的 方式 
将 拟合 的 树 结合 起来 作为 最后 的 输出 
$ $ F _ n x = \ sum _ 
{ i = 1 } ^ { N } \ 
alpha _ if _ i x $ $ $ f 
_ i x $ 是 单棵/nr 回归 树 函数 $ 
\ alpha _ i $ 是 第 $ i $ 
棵 回归 树 的 权重 在 这里 我们 需要 弄清楚 
为什么 拟合 残差 就能 不断 减少 拟合 误差 假设 拟合 
误差 $ C $ 是 拟合 函数 $ F _ 
n $ 的 函数 $ C F _ n $ 
那么 $ $ \ delta C \ approx \ frac 
{ \ partial { C F _ n } } 
{ \ partial { F _ n } } \ 
delta F _ n $ $ 如果 取 $ \ 
delta F _ n = \ eta \ frac { 
\ partial { C } } { \ partial { 
F _ n } } $ 就 可以 得到 $ 
\ delta C 0 $ 其中 $ \ eta $ 
是 学习率 为 正 实数 所以 只要 函数 $ F 
_ n $ 拟合 误差函数 的 负 梯度 就 可以 
不断 降低 拟合 误差 的 值 设 标签 向量 $ 
y = y _ 1 y _ 2 . . 
. y _ m ^ T $ 如果 用 最小二乘 
的 方式 表示 拟合 误差 则 $ $ C = 
\ frac { 1 } { 2 } F _ 
n y ^ 2 $ $ 那么 $ \ delta 
F _ n = \ eta \ frac { \ 
partial { C } } { \ partial { F 
_ n } } = \ eta F _ n 
y $ 这 其实 就是 上面 提到 的 残差 所以 
拟合 残差 可以 不断 减少 拟合 误差 5.2 逻辑 回归 
+ MART 进行 二 分类 了解 了 MART 之后 下面 
举 一个 MART 实际 应用 的 例子 使用 MART 和 
逻辑 回归 进行 二 分类 用于 分类 的 样本 $ 
x _ i \ in \ mathbb { R } 
^ n $ 标签 $ y _ i \ in 
\ { \ pm1 \ } $ 拟合 函数 $ 
F x $ 为了 简化 表示 我们 表示 条件概率 如下 
$ $ P _ + \ equiv P y = 
1 | x $ $ $ $ P _ \ 
equiv P y = 1 | x $ $ 用 
交叉 熵 表示 损失 函数 $ $ L y F 
= ylog P _ + 1 y log P _ 
$ $ 逻辑 回归 使用 对数 机率 属 于正 例 
概率 / 属于 负 例 概率 进行 建模 $ $ 
F _ n x = \ frac { 1 } 
{ 2 } log \ frac { P _ + 
} { P _ } $ $ $ $ P 
_ + = \ frac { 1 } { 1 
+ e ^ { 2 \ sigma F _ n 
x } } $ $ $ $ P _ = 
1 P _ + = \ frac { 1 } 
{ 1 + e ^ { 2 \ sigma F 
_ n x } } $ $ 将 $ P 
_ + $ 和$P/nr _ $ 带入 $ L y 
F $ 中 得到 $ $ L y F _ 
n = log 1 + e ^ { 2y \ 
sigma F _ n } $ $ $ R _ 
{ jm } $ 表示 落入 第 $ m $ 
棵树 的 第 $ j $ 个 叶子 节点 中的 
样例 集合 可以 通过 下式 对 该 叶子 节点 的 
值 进行 优化 $ $ \ gamma _ { jm 
} = arg \ min _ { \ gamma } 
\ sum _ { x _ i \ in R 
_ { jm } } \ log \ left 1 
+ e ^ { 2 \ sigma   y _ 
i \ left F _ { m 1 } \ 
\ \ left { x _ i } \ right 
+ \ gamma \ right \ } \ right $ 
$ 上式 可以 使用 Newton Raphson 方法 按照 下面 的 
公式 进行 迭代 求解 $ $ \ gamma _ { 
n + 1 } = \ gamma _ { n 
} \ frac { g \ gamma _ n } 
{ g \ gamma _ n } $ $ 5.3 
LambdaMART 基本 定义 LambdaMART 基于 MART 优化 $ \ lambda 
$ 梯度 根据 上面 的 定义 对于 任意 $ U 
_ i $ 和$U/nr _ j $ 有 $ $ 
\ lambda _ { ij } = \ frac { 
\ partial { C s _ i s _ j 
} } { \ partial { s _ i } 
} = \ frac { \ sigma | \ Delta 
_ { Z _ { ij } } | } 
{ 1 + e ^ { \ sigma s _ 
i s _ j } } $ $ $ | 
\ Delta _ { Z _ { ij } } 
| $ 表示 交换 $ U _ i $ 和$U/nr 
_ j $ 的 位置 产生 的 评价 指标 差值 
$ Z $ 可以 是 $ NDCG $ 或者 $ 
ERR $ 等 对于 特定 $ U _ i $ 
累加 其他 所有 排序 项的/nr 影响 得到 $ $ \ 
lambda _ i = \ sum _ { j \ 
{ i j \ } \ in I } \ 
lambda _ { ij } \ sum _ { j 
\ { j i \ } \ in I } 
\ lambda _ { ij } $ $ 为了 简化 
表示 $ $ \ sum _ { \ { i 
j \ } \ r i g h t l 
e f t h a r p o o n 
s I } \ lambda _ { ij } = 
\ sum _ { j \ { i j \ 
} \ in I } \ lambda _ { ij 
} \ sum _ { j \ { j i 
\ } \ in I } \ lambda _ { 
ij } $ $ 于是 我们 可以 更新 损失 函数 
$ $ \ frac { \ partial { C } 
} { \ partial { s _ i } } 
= \ sum _ { j \ { i j 
\ } \ in I } \ frac { \ 
sigma | \ Delta _ { Z _ { ij 
} } | } { 1 + e ^ { 
\ sigma s _ i s _ j } } 
=   \ sum _ { j \ { i 
j \ } \ in I } \ sigma   
| \ Delta _ { Z _ { ij } 
} | \ rho _ { ij } $ $ 
其中 我们 定义 $ $ \ rho _ { ij 
} = \ frac { 1 } { 1 + 
e ^ { \ sigma s _ i s _ 
j } } = \ frac { \ lambda _ 
{ ij } } { \ sigma | \ Delta 
_ { Z _ { ij } } | } 
$ $ 然后 可以 得到 $ $ \ frac { 
\ partial { ^ 2C } } { \ partial 
{ s _ i ^ 2 } } = \ 
sum _ { \ { i j \ } \ 
r i g h t l e f t h 
a r p o o n s I } \ 
sigma ^ 2 | \ Delta _ { Z _ 
{ ij } } | \ rho { ij } 
1 \ rho _ { ij } $ $ 所以 
我们 可以 用 下面 的 公式 计算 第 $ m 
$ 棵树 的 第 $ k $ 个 叶子 节点 
上 的 值 $ $ \ gamma _ { km 
} = \ frac { \ sum _ { x 
_ i \ in R _ { km } } 
\ frac { \ partial { C } } { 
\ partial { s _ i } } } { 
\ sum _ { x _ i \ in R 
_ { km } } \ frac { \ partial 
{ ^ 2C } } { \ partial { s 
_ i ^ 2 } } } = \ frac 
{ \ sum _ { x _ i \ in 
R _ { km } } \ sum _ { 
\ { i j \ } \ r i g 
h t l e f t h a r p 
o o n s I } | \ Delta _ 
{ Z _ { ij } } | \ rho 
_ { ij } } { \ sum _ { 
x _ i \ in R _ { km } 
} \ sum _ { \ { i j \ 
} \ r i g h t l e f 
t h a r p o o n s I 
} | \ Delta _ { Z _ { ij 
} } | \ sigma \ rho _ { ij 
} 1 \ rho _ { ij } } $ 
$ 所以 总结 LambdaMART 算法 如下 6 . 参考文献 1 
. Christopher J . C . Burges . From RankNet 
to LambdaRank to LambdaMART An Overview . Microsoft Research Technical 
Report MSR TR 010 82.2 . Chrisopher D . Manning 
Prabhakar Raghavan Hinrich Schutze 著 王斌 译 . Introduction to 
Information Retrieval 8.4 有序 检索 结果 的 评价 方法 2017年 
10月 北京 第 11 次印刷 . 3 . Olivier Chapelle 
Ya Zhang Pierre Grinspan . Expected Recipocal Rank for Graded 
Relevance . CIKM 2009 . 