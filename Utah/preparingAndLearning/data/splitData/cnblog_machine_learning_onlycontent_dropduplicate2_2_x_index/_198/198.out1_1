一 特征 工程 概述 数据 决定 了 机器 学习 的 
上限 而 算法 只是 尽可能 逼近 这个 上限 这里 的 
数据 指 的 就是 经过 特征 工程 得到 的 数据 
特征 工程 指 的 是 把 原始数据 转变为 模型 的 
训练 数据 的 过程 它 的 目的 就是 获取 更好 
的 训练 数据 特征 使得 机器学习 模型 逼近 这个 上限 
特征 工程 能 使得 模型 的 性能 得到 提升 有时 
甚至 在 简单 的 模型 上 也能 取得 不错 的 
效果 特征 工程 在 机器 学习 中 占有 非常 重要 
的 作用 一般 认为 括 特征 构建 特征提取 特征选择 三个 
部分 特征 构建 比较 麻烦 需要 一定 的 经验  /i 
特征提取/nr 与/p 特征/n 选择/v 都是/nr 为了/p 从/p 原始/v 特征/n 中/f 
找出/v 最/d 有效/a 的/uj 特征/n 它们 之间 的 区别 是 
特征提取 强调 通过 特征 转换 的 方式 得到 一组 具有 
明显 物理 或 统计 意义 的 特征 而 特征选择 是从 
特征 集合 中 挑选 一组 具有 明显 物理 或 统计 
意义 的 特征 子集 两者都 能 帮助 减少 特征 的 
维度 数据冗余 特征提取 有时能 发现 更 有 意义 的 特征 
属性 特征 选择 的 过程 经常 能 表示 出 每个 
特征 的 重要性 对于 模型 构建 的 重要性 本文 主要 
尝试 总结 几个 常用 的 特征 提取 和 特征 选择 
的 方法 二 特征 构建 特征 构建 是 指 从 
原始 数据 中 人工 的 找出 一些 具有 物理 意义 
的 特征 需要 花 时间 去 观察 原始数据 思考 问题 
的 潜在 形式 和 数据结构 对/p 数据/n 敏感性/n 和/c 机器学习/i 
实战/v 经验/n 能/v 帮助/v 特征/n 构建/v 除此之外 属性 分割 和 
结合 是 特征 构建 时常 使用 的 方法 结构性 的 
表格 数据 可以 尝试 组合 二个 三个 不同 的 属性 
构造 新的 特征 如果 存在 时间 相关 属性 可以 划出 
不同 的 时间 窗口 得到 同一 属性 在 不同 时间 
下 的 特征值 也 可以 把 一个 属性 分解 或 
切分 例如 将 数据 中 的 日期 字段 按照 季度 
和 周期 后者 一天 的 上午 下午 和 晚上 去 
构建 特征 总之 特征 构建 是个 非常 麻烦 的 问题 
书 里面 也 很少 提到 具体 的 方法 需要 对 
问题 有 比较 深入 的 理解 三 特征提取 1 . 
PCA 主 成分 分析 PCA 的 思想 是 通过 坐标轴 
转换 寻找 数据分布 的 最优 子空间 从而 达到 降 维 
去 相关 的 目的 下面 的 图 是 直接 从 
机器学习 实战 中 截取 的 原始数据 二维 特征 三分 类 
问题 左图 是 原始数据 进行 PCA 特征 转换 第一 个 
新 坐标轴 选择 的 是 原始 数据 中 方差 最大 
的 方向 线 B 第二个 新 坐标轴 与 第一 个 
坐标轴 正交 且 具有 最大 方差 的 方向 线 C 
当 特征 维度 较 多时 重复 上述 过程 会 发现 
大 部分 的 方差 都 包含 在 前 几个 新的 
坐标轴 中 通过 选择 保留 前 N 个 坐标轴 达到 
降 维 的 效果 下面 中上 是 特征 转换 的 
图右 中下 是 降 维 后的图/nr 在 数学 上 是 
先用 原始数据 协方差 矩阵 的 前 N 个 最大 特征值 
对应 的 特征向量 构成 映射 矩阵 然后 原始 矩阵 左乘/nr 
映射 矩阵 从而 对 原始数据 降 维 下图 右面 列出 
了 两个 随机 变脸 之间 协方差 的 计算 公式 怎么 
计算 矩阵 的 协方差 矩阵 矩阵 的 特征值 特征向量 特征向量 
可以 理解 为 坐标 准 换 中的 新 坐标轴 的 
方向 特征值 表示 矩阵 在 对应 的 特征向量 上 的 
方差 特征值 越大 方差 越大 信息量 越多 2 . LDA 
线性 判别分析 LDA 的 原理 是 将带 上 标签 的 
数据 点 通过 投影 的 方法 投影 到 维度 更低 
的 空间 使得 投 影后 的 点 会 形成 按 
类别 区分 相同 类别 的 点 将 会在 投 影后 
更 接近 不同 类别 的 点 距离 越远 skearn 网站 
上 面有 个 例子 介绍 PCA 与 LDA 的 区别 
分别 通过 PCA 和 LDA 将 4 维 特征 三 
分类 的 Iris 数据 降 维 为 2 维 特征 
然后 再 进行 分类 效果 差不多 可视化 出来 发现 有些 
不同 毕竟 降 维 方式 有些 不同 LDA 算法 的 
主要 步骤 1 分别 计算 每个 类别 i 的 原始 
中心点 2 . 类别 i 投 影后 的 中心点 为 
3 . 衡量 类别 i 投 影后 类别 点 之间 
的 分散 程度 用 方差 来 表示 4 . 使用 
下 面的 式子 表示 LDA 投影 到 w 后的/nr 损失 
函数 最大化 J W 就 可以 求出 最优 的 w 
具体 解法 参考 博客 3 . ICA 独立 成分 分析 
PCA 特征 转换 降 维 提取 的 是 不 相关 
的 部分 ICA 独立 成分 分析 获得 的 是 相互 
独立 的 属性 ICA 算法 本质 寻找 一个 线性变换 z 
= Wx 使得 z 的 各个 特征 分量 之间 的 
独立性 最大 ICA 相比 与 PCA 更能 刻画 变量 的 
随机 统计 特性 且 能 抑制 噪声 ICA 算法 听着 
有点 绕 ICA 认为 观测 到 数据 矩阵 X 是 
可以 由 未知 的 独立 元 举证 与 未知 的 
矩阵 A 相乘 得到 ICA 希望 通过 矩阵 X 求得 
一个 分离 矩阵 W 使得 W 作用 在 X 上 
所 获得 的 矩阵 Y 能够 逼近 独立 源 矩阵 
最后 通过 独立 元 矩阵 表示 矩阵 X 所以 说 
ICA 独立 成分 分析 提 取出 的 特征 中的 独立 
部分 四 特征选择 特征选择 是 剔除 不相关 或者 冗余 的 
特征 减少 有效 特征 的 个数 减少 模型 训练 的 
时间 提高 模型 的 精确度 特征提取 通过 特征 转换 实现 
降 维 特征选择 则是 依靠 统计学 方法 或者 于 机器学习 
模型 本身 的 特征选择 排序 功 能 实现 降 维 
特征选择 是个 重复 迭代 的 过程 有时 可能 自己 认为 
特征选择 做 的 很好 但 实际 中 模型 训练 并 
不太好 所以 每次 特征选择 都要 使用 模型 去 验证 最终 
目的 是 为了 获得 能 训练 出好 的 模型 的 
数据 提升 模型 的 性能 下面 介绍 几个 常用 的 
方法 1 . 运用 统计学 的 方法 衡量 单个 特征 
与 响应 变量 Lable 之间 的 关系 皮尔森 相关系数 Pearson 
Correlation 衡量 变量 之间 的 线性关系 结果 取值 为 1 
1 1 表示 完全 负相关 + 1 表示 正 相关 
0 表示 不 线性相关 但是 并不 表示 没有 其它 关系 
Pearson 系数 有 一个 明显 缺陷 是 只 衡量 线性关系 
如果 关系 是 非线性 的 即使 连个 变量 具有 一一对应 
的 关系 Pearson 关系 也会 接近 0 皮尔森 系数 的 
公式 为 样本 共 变 异数 除以 X 的 标准 
差 和Y的/nr 标准差 的 乘积 sklearn 中 可以 直接 计算 
两个 随机变量 之间 的 Pearson 相关系数 例如 下面 计算 X 
与 X * X 之间 的 相关 系数 约为 0 
x = np . random . uniform 1 1 100000 
print pearsonr x x * * 2 0 0.00230804707612 最大 
信息 系数 MIC 最大 信息 系数 是 根据 互信息 得到 
的 下面 是 互信息 公式 最大 信息 系数 MIC 不仅 
能像/nr Pearson 系数 一样 度量 变量 之间 的 线性关系 还能 
度量 变量 之间 的 非线性 关系 MIC 虽然能 度量 变量 
之间 的 非线性 关系 但当 变量 之间 的 关系 接近 
线性 相关 的 时候 Pearson 相关系数 仍然 是 不可 替代 
的 第一 Pearson 相关系数 计算 速度快 这在 处理 大规模 数据 
的 时候 很 重要 第二 Pearson 相关 系数 的 取值 
区间 是 1 1 而 MIC 和 距离 相关系数 都是 
0 1 这个 特点 使得 Pearson 相关系数 能够 表征 更 
丰富 的 关系 符号 表示 关系 的 正负 绝对值 能够 
表示 强度 当然 Pearson 相关性 有效 的 前提 是 两个 
变量 的 变化 关系 是 单调 的 下面 列举 了 
MIC 和 Pearson 系数 在 变脸 处于 不同 关系 下 
的 表现 情况 2 . 基于/p 机器学习/i 模型/n 的/uj 特征选择/nr 
线性/n 模型/n 和/c 正则化/i 当/t 特征/n 和/c 响应/v 变量/vn 之间/f 
全部都是/n 线性关系/l 并且 特征 之间 均 是 比较 独立 的 
可以 尝试 使用 线性 回归模型 去做 特征选择 因为 越是 重要 
的 特征 在 模型 中 对应 的 系数 就会 越大 
而跟 输出 变量 越是 无关 的 特征 对应 的 系数 
就会 越 接近 与 0 在 很多 实际 的 数据 
当中 往往 存在 多个 互相 关联 的 特征 这时候 模型 
就 会 变得 不 稳定 数据 中 细微 的 变化 
就 可能 导致 模型 的 巨大 变化 模型 的 变化 
本质上 是 系数 或者 叫 参数 可以 理解 成W/nr 这会 
让 模型 的 预测 变得 困难 这种 现象 也 称为 
多重 共线性 例如 假设 我们 有个 数据集 它 的 真实 
模型 应该是 Y = X1 + X2 当 我们 观察 
的 时候 发现 Y = X1 + X2 + e 
e 是 噪音 如果 X1 和 X2 之间 存在 线性关系 
例如 X1 约等于 X2 这个 时候 由于 噪音 e 的 
存在 我们 学到 的 模型 可能 就 不是 Y = 
X1 + X2 了 有 可能 是 Y = 2X1 
或者 Y = X1 + 3X2 通过 在 模型 中 
加入 正则化 项 也能 起到 特征选择 的 作用 L1 正则化 
学到 的 是 比较 稀疏 的 模型 控制 惩罚 项 
系数 alpha 会 迫使 那些 弱 的 特征 所 对应 
的 系数 变为 0 这个 特征 使得 L1 正则化 成为 
一种 很好 的 特征选择 方法 L2 正则化 会 使得 系数 
的 取值 变得 平均 对于 关联 特征 这 意味 则 
他们 能够 获得 更加 相近 的 对应 系数 随机 森林 
模型 随机 森林 由 多棵 决策树 构成 决策树 中的 每个 
节点 都是 关于 某个 特征 的 条件 利用 不 纯度 
可以确定 划分 数据集 的 最优 特征 对于 分类 问题 通常 
采用 基尼 不 纯度 或者 信息 增益 对于 回归 问题 
通常 采用 方差 或者 最小二乘 拟合 当 训练 决策树 的 
时候 可以 计算 出 每个 特征 减少 了 多少 树 
的 不 纯度 对于 一个 决策树 森林 来说 可以 算 
出 每个 特征 平均 减少 了 多少 不 纯度 并把 
它 平均 减少 的 不 纯度 作为 特征选择 的 值 
另一种 常用 的 特征选择 方法 就是 直接 度量 每个 特征 
对模型 精确 率 的 影响 主要 思路 是 打乱 每个 
特征 的 特征值 顺序 并且 度量 顺序 变动 对 模型 
的 精确 率 的 影响 很明显 对于 不 重要 的 
变量 来说 打乱 顺序 对模型 的 精确 率 影响 不会 
太大 但是 对于 重要 的 变量 来说 打乱 顺序 就会 
降低 模型 的 精确 率 要 记住 1 这种 方法 
存在 偏向 对 具有 更 多类 别的 变量 会 更 
有利 2 对于 存在 关联 的 多个 特征 其中 任意 
一个 都 可以 作为 指示器 优秀 的 特征 并且 一旦 
某个 特征 被 选择 之后 其他 特征 的 重要 度 
就会 急剧下降 因为 不 纯度 已经 被 选中 的 那个 
特征 降 下来 了 其他 的 特征 就 很难 再 
降低 那么多 不纯 度了 这样一来 只有 先 被 选中 的 
那个 特征 重要 度 很高 其他 的 关联 特征 重要 
度 往往 较低 在 理解 数据 时 这就 会 造成 
误解 导致 错误 的 认为 先 被 选中 的 特征 
是 很 重要 的 而 其余 的 特征 是 不 
重要 的 但 实际上 这些 特征 对 响应 变量 的 
作用 确实 非常 接近 的 这 跟 Lasso 是 很像 
的 特征 工程 内容 很多 关系 特 征选 强烈建议 仔细看 
结合 Scikit learn 介绍 几种 常用 的 特征选择 方法 则 
篇 干货 的 结尾 有个 例子 比较 了 集中 特征 
方法 对 每个 特征 的 评分 非常 值得 学习 就 
我 自己 而言 由于 很少 碰到 线 性问题 所以 使用 
RF 做 特征选择 的 情况 会 多一些 参考资料 1 . 
特征 工程 技术 和 方法 概括 总结 http / / 
blog . csdn . net / jasonding1354 / article / 
details / 471711152 . 干货 结合 Scikit learn 介绍 几种 
常用 的 特征选择 方法 http / / dataunion . org 
/ 14072 . html3 . 参考资料 2 的 英文原版 http 
/ / m a c h i n e l 
e a r n i n g m a s 
t e r y . com / discover feature engineering 
how to engineer features and how to get good at 
it / 4 . 机器学习 之 特征 工程 http / 
/ www . csuldw . com / 2015/10 / 24/2015 
10 24% 20feature % 20engineering / 5 . 特征提取 与 
特征选择   http / / lanbing510 . info / 2014 
/ 10/22 / Feature Extraction Selection . html6 . PCA 
与 LDA http / / www . cnblogs . com 
/ LeftNotEasy / archive / 2011 / 01/08 / lda 
and pca machine learning . html 