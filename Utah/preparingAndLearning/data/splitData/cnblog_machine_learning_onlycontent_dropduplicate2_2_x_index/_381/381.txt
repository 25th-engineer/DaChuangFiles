前面写了个简单的线性代数系列文章，目的就是让大家在接触SVD分解前，先了解回忆一下线性代数的基本知识，有助于大家理解SVD分解。不至于一下被大量的线性代数操作搞晕。这次终于开始正题——SVD的介绍了。
所谓SVD，就是要把矩阵进行如下转换：A = USVT
the columns of U are the eigenvectors of the AAT matrix and the columns of V are the eigenvectors of the ATA matrix. VT is the transpose of V and S is a diagonal matrix. By definition the nondiagonal elements of diagonal matrices are zero. The diagonal elements of S are a special kind of values of the original matrix. These are termed the singular values of A.
1 The Frobenius Norm
一个矩阵所有元素的平方和再开方称为这个矩阵的Frobenius Norm。特殊情况下，行矩阵的Frobenius Norm为该向量的长度
2 计算A转置 A*At At*A
3 计算S
在SVD中，将AAt的特征值从大到小排列，并开方，得到的就是奇异值。
比如上图中，特征值为40，10.因此奇异值为6.32,3.16。矩阵的奇异值有如下特性：
a 矩阵的奇异值乘积等于矩阵行列式的值 6.32*3.16 = 20 = |A|
b 矩阵A的 Frobenius Norm等于奇异值的平方和的开方
总结一下计算S的步骤：1 计算AT 和ATA；2 计算ATA的特征值，排序并开方。
由此可以得到S，下面来看如何计算 U，VT
4  计算V和VT
利用ATA的特征值来计算特征向量
既然刚才提到V就是特征向量的组合，那么
5 计算U
A = USVT
AV = USVTV = US
AVS-1 = USS-1
U = AVS-1
6 计算SVD
可以看出，SVD可以对矩阵进行分解重建。
7 降维的SVD
如果我们只保留前k个最大的奇异值，前k列个U，前k行个V，相当于将数据中占比不大的噪音进行过滤，这样既可以有效地对数据进行泛化，又起到了降维减少运算量的目的。是不是很奇妙？
8 实际用途
我们实际的工作中，经常会用到这种降维方法。包括现在非常火的推荐问题，以及LSI问题都对SVD有着广泛的应用。
举个最常用的例子，在文本挖掘中：A就是 t (term) 行 d (document) 列的矩阵，每列是一篇文章，每行是一个单词，每个单元格的当前单词在当前文章里的出现次数。 U 是一个 t 行 r 列 的矩阵， V 是一个 r 行 d 列 的矩阵， S 是一个 r 行 r 列的对角矩阵。这里 r 的大小是 A的秩。那么U和V中分别是A的奇异向量，而S是A的奇异值。AA'的正交单位特征向量组成U，特征值组成S'S，A'A的正交单位特征向量组成V，特征值（与AA'相同）组成SS'。
希望大家细细体会，多多交流，一起进步。