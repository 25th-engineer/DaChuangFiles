注 在上 一篇 的 一般 线性 回 归中 使用 的 
假设 函数 是 一元 一次方程 也 就是 二维 平 面上 
的 一条 直线 但是 很多 时候 可能 会 遇到 直线 
方程 无法 很好 的 拟合 数据 的 情况 这个 时候 
可以 尝试 使用 多项式 回归 多项式 回 归中 加入 了 
特征 的 更高 次方 例如 平方 项或/nr 立方 项 也 
相当于 增加 了 模型 的 自由度 用来 捕获 数据 中 
非线性 的 变化 添加 高阶 项的/nr 时候 也 增加 了 
模型 的 复杂度 随着 模型 复杂度 的 升高 模型 的 
容量 以及 拟合 数据 的 能力 增加 可以 进一步 降低 
训练 误差 但 导致 过拟合 的 风险 也 随之 增加 
图 A 模型 复杂度 与 训练 误差 及 测试 误差 
之间 的 关系 0 . 多项式 回归 的 一般 形式 
在 多项式 回 归中 最 重要 的 参数 是 最高 
次方 的 次数 设 最高 次方 的 次数 为 $ 
n $ 且 只有 一个 特征 时 其 多项式 回归 
的 方程 为 $ $ \ hat { h } 
= \ theta _ 0 + \ theta _ 1 
x ^ 1 + \ . . . \   
+ \ theta _ { n 1 } x ^ 
{ n 1 } +   \ theta _ n 
x ^ n   $ $ 如果 令 $ x 
_ 0 = 1 $ 在 多 样本 的 情况 
下 可以 写成 向 量化 的 形式 $ $ \ 
hat { h } = X \ cdot \ theta 
$ $ 其中 $ X $ 是 大小 为 $ 
m \ cdot n + 1 $ 的 矩阵 $ 
\ theta $ 是 大小 为 $ n + 1 
\ cdot 1 $ 的 矩阵 在 这里 虽然 只有 
一个 特征 $ x $ 以及 $ x $ 的 
不同 次方 但是 也 可以 将 $ x $ 的 
高 次方 当做 一个 新 特征 与 多元 回归分析 唯一 
不同 的 是 这些 特征 之间 是 高度 相关 的 
而 不是 通常 要求 的 那样 是 相互 对立 的 
在 这里 有个/nr 问题 在 刚开始 学习 线性 回归 的 
时候 困扰 了 自己 很久 如果 假设 中 出现 了 
高阶 项 那么 这个 模型 还是 线性 模型 吗 此时 
看待 问题 的 角度 不同 得到 的 结果 也 不同 
如果 把 上面 的 假设 看成 是 特征 $ x 
$ 的 方程 那么 该 方程 就 是非 线性方程 如果 
看成 是 参数 $ \ theta $ 的 方程 那么/r 
$/i x/w $/i 的/uj 高阶/nr 项都/nr 可以/c 看做/v 是/v 对应/vn 
$/i \/i theta/w $/i 的/uj 参数/n 那么 该 方程 就是 
线性方程 很明显 在 线性 回归 中 采用 了 后 一种 
解释 方式 因此 多项式 回归 仍然 是 参数 的 线性 
模型 1 . 多项式 回归 的 实现 下面 主要 使用 
了 numpy scipy matplotlib 和 scikit learn 所有 使用 到 
的 函数 的 导入 如下 1 import numpy as np 
2 from scipy import stats 3 import matplotlib . pyplot 
as plt 4 from sklearn . preprocessing import P o 
l y n o m i a l F e 
a t u r e s 5 from sklearn . 
linear _ model import LinearRegression 6 from sklearn . metrics 
import mean _ squared _ error 下 是 使用 的 
数据 是 使用 $ y = x ^ 2 + 
2 $ 并 加入 一些 随机误差 生成 的 只 取了 
10 个数 据点 1 data = np . array 2.95507616 
10.94533252 2 0.44226119 2.96705822 3 2.13294087 6.57336839 4 1.84990823 5.44244467 
5 0.35139795 2.83533936 6 1.77443098 5.6800407 7 1.8657203 6.34470814 8 
1.61526823 4.77833358 9 2.38043687 8.51887713 10 1.40513866 4.18262786 11 m 
= data . shape 0 # 样本 大小 12 X 
= data 0 . reshape 1 1 # 将 array 
转换成 矩阵 13 y = data 1 . reshape 1 
1 14 plt . plot X y b . 15 
plt . xlabel X 16 plt . ylabel y 17 
plt . show 这些 数据 点 plot 出来 如 下图 
1 原始数据 1.1 直线 方程 拟合 下面 先用 直线 方程 
拟合 上面 的 数据 点 1 lin _ reg = 
LinearRegression 2 lin _ reg . fit X y 3 
print lin _ reg . intercept _ lin _ reg 
. coef _ # 4.97857827 0.92810463 4 5 X _ 
plot = np . linspace 3 3 1000 . reshape 
1 1 6 y _ plot = np . dot 
X _ plot lin _ reg . coef _ . 
T + lin _ reg . intercept _ 7 plt 
. plot X _ plot y _ plot r 8 
plt . plot X y b . 9 plt . 
xlabel X 10 plt . ylabel y 11 plt . 
savefig regu 2 . png dpi = 200 2 直线 
拟合 的 效果 可以 使用 函数 mean _ squared _ 
error 来 计算误差 使用 前面 介绍 过 的 Mean squared 
error MSE h = np . dot X . reshape 
1 1 lin _ reg . coef _ . T 
+ lin _ reg . intercept _ print mean _ 
squared _ error h y # 3 . 341.2 使用 
多项式 方程 为了 拟合 2次 方程 需要 有 特征 $ 
x ^ 2 $ 的 数据 这里 可以 使用 函数 
P o l y n o m i a l 
F e a t u r e s 来 获得 
1 poly _ features = P o l y n 
o m i a l F e a t u 
r e s degree = 2 include _ bias = 
False 2 X _ poly = poly _ features . 
fit _ transform X 3 print X _ poly 结果 
如下 2.95507616 8.73247511 0.44226119 0.19559496 2.13294087 4.54943675 1.84990823 3.42216046 0.35139795 
0.12348052 1.77443098 3.1486053 1.8657203 3.48091224 1.61526823 2.60909145 2.38043687 5.66647969 1.40513866 
1.97441465 利用 上面 的 数据 做 线性 回归分析 1 lin 
_ reg = LinearRegression 2 lin _ reg . fit 
X _ poly y 3 print lin _ reg . 
intercept _ lin _ reg . coef _ # 2.60996757 
0.12759678 0.9144504 4 5 X _ plot = np . 
linspace 3 3 1000 . reshape 1 1 6 X 
_ plot _ poly = poly _ features . fit 
_ transform X _ plot 7 y _ plot = 
np . dot X _ plot _ poly lin _ 
reg . coef _ . T + lin _ reg 
. intercept _ 8 plt . plot X _ plot 
y _ plot r 9 plt . plot X y 
b . 10 plt . show 第 3行 得到 了 
训练 后的/nr 参数 即 多项式 方程 为 $ h = 
0.13 x + 0.91 x ^ 2 + 2.61 $ 
结果 中 系数 的 顺序 与 $ X $ 中 
特征 的 顺序 一致 如下 图 所示 3 2次 多项式 
方程 与 原始 数据 的 比较 利用 多项式 回归 代价 
函数 MSE 的 值 下降 到了 0.07 通过观察 代码 可以 
发现 训练 多项式 方程 与 直线 方程 唯一 的 差别 
是 输入 的 训练 集 $ X $ 的 差别 
在 训练 直线 方程 时 直接 输入 了 $ X 
$ 的 值 在 训练 多项式 方程 的 时候 还 
添加 了 我们 计算 出来 的 $ x ^ 2 
$ 这个 新 特征 的 值 由于 $ x ^ 
2 $ 完全 是由 $ x $ 的 值 确定 
的 因此 严格 意义 上 来讲 此时 该 模型 只有 
一个 特征 $ x $ 此时 有个/nr 非常 有趣 的 
问题 假如 一 开始 得到 的 数据 就是 上面 代码 
中 X _ poly 的 样子 且 不 知道 $ 
x _ 1 $ 与 $ x _ 2 $ 
之间 的 关系 此时 相当于 我们 有 10个 样本 每个 
样本 具有 $ x _ 1 x _ 2 $ 
两个 不同 的 特征 这时 假设 函数 为 $ $ 
\ hat { h } = \ theta _ 0 
+ \ theta _ 1 x _ 1 + \ 
theta _ 2 x _ 2 $ $ 直接 按照 
二元 线性 回归方程 来 训练 也 可以 得 到上面 同样 
的 结果 $ \ theta $ 的 值 如果 在 
相同 情况 下 收集到 了 新的 数据 可以 直接 带入 
上面 的 方程 进行 预测 唯一 不同 的 是 我们 
不 知道 $ x _ 2 = x _ 1 
^ 2 $ 这个 隐含 在 数据 内部 的 关系 
所有 也就 无法 画出 3 中的 这条 曲线 一旦 了解到 
了 这 两个 特征 之间 的 关系 数据 的 维度 
就从 3 维 下降 到了 2 维 包含 截距 项$\/nr 
theta _ 0 $ 2 . 持续 降低 训练 误差 
与 过拟合 在上面 实现 多项式 回归 的 过程 中 通过 
引入 高阶 项$x/nr ^ 2 $ 训练 误差 从 3.34 
下降 到了 0.07 减小 了 将近 50倍 那么 训练 误差 
是否 还有 进一步 下降 的 空间 呢 答案 是 肯定 
的 通过 继续 增加 更 高阶 的 项 训练 误差 
可以 进一步 降低 通过 尝试 当 最 高阶 项为$/nr x 
^ { 11 } $ 时 训练 误差 为 3.11 
e 23 几乎 等于 0 了 下面 是 测试 不同 
degree 的 过程 1 # test different degree and return 
loss 2 def try _ degree degree X y 3 
poly _ features _ d = P o l y 
n o m i a l F e a t 
u r e s degree = degree include _ bias 
= False 4 X _ poly _ d = poly 
_ features _ d . fit _ transform X 5 
lin _ reg _ d = LinearRegression 6 lin _ 
reg _ d . fit X _ poly _ d 
y 7 return { X _ poly X _ poly 
_ d intercept lin _ reg _ d . intercept 
_ coef lin _ reg _ d . coef _ 
} 8 9 degree2loss _ paras = 10 for i 
in range 2 20 11 paras = try _ degree 
i X y 12 h = np . dot paras 
X _ poly paras coef . T + paras intercept 
13 _ loss = mean _ squared _ error h 
y 14 degree2loss _ paras . append { d i 
loss _ loss coef paras coef intercept paras intercept } 
15 16 min _ index = np . argmin np 
. array i loss for i in degree2loss _ paras 
17 min _ loss _ para = degree2loss _ paras 
min _ index 18 print min _ loss _ para 
# 19 X _ plot = np . linspace 3 
1.9 1000 . reshape 1 1 20 poly _ features 
_ d = P o l y n o m 
i a l F e a t u r e 
s degree = min _ loss _ para d include 
_ bias = False 21 X _ plot _ poly 
= poly _ features _ d . fit _ transform 
X _ plot 22 y _ plot = np . 
dot X _ plot _ poly min _ loss _ 
para coef . T + min _ loss _ para 
intercept 23 fig ax = plt . subplots 1 1 
24 ax . plot X _ plot y _ plot 
r label = degree = 11 25 ax . plot 
X y b . label = X 26 plt . 
xlabel X 27 plt . ylabel y 28 ax . 
legend loc = best frameon = False 29 plt . 
savefig regu 4 overfitting . png dpi = 200 输出 
为 { coef array 0.7900162 26.72083627 4.33062978 7.65908434 24.62696711 12.33754429 
15.72302536 9.54076366 1.42221981 1.74521649 0.27877112 d 11 intercept array 0.95562816 
loss 3 . 1 0 8 0 2 6 7 
0 0 5 6 7 6 9 3 4 e 
23 } 画出 的 函数 图像 如下 1 degree = 
11时 的 函数 图像 由 1 可以 看到 此时 函数 
图像 穿过 了 每一个 样本点 所有 的 训练 样本 都 
落在 了 拟合 的 曲线 上 训练 误差 接近 与 
0 可以 说 是 近乎 完美 的 模型 了 但是 
这样 的 曲线 与 我们 最 开始 数据 的 来源 
一个 二次方程 加上 一些 随机误差 差异 非常 大 如果 从 
相同 来源 再取 一些 样本点 使用 该 模型 预测 会 
出现 非常 大 的 误差 类似 这种 训练 误差 非常 
小 但是 新 数 据点 的 测试 误差 非常大 的 
情况 就 叫做 模型 的 过拟合 过拟合 出现 时 表示 
模型 过于 复杂 过多 考虑 了 当前 样本 的 特殊 
情况 以及 噪音 模型 学习 到 了 当前 训练样本 非 
全局 的 特性 使得 模型 的 泛化 能力 下降 出现 
过拟合 一般 有 以下 几种 解决 方式 降低 模型 复杂度 
例如 减小 上面 例子 中的 degree 降 维 减小 特征 
的 数量 增加 训练样本 添加 正则化 项./nr 防止 模型 过拟合 
是 机器学习 领域 里 最重要 的 问题 之一 鉴于 该 
问题 的 普遍性 和 重要性 在 满足 要求 的 情况 
下 能/v 选择/v 简单/a 模型/n 时/n 应该/v 尽量/d 选择/v 简单/a 
的/uj 模型/n Referencehttp / / scikit learn . org / 
stable / modules / linear _ model . htmlG é 
ron A . Hands on machine learning with Scikit Learn 
and TensorFlow concepts tools and techniques to build intelligent systems 
M . O Reilly Media Inc . 2017 .   
githubhttps / / www . arxiv vanity . com / 
papers / 1803.09820 / 