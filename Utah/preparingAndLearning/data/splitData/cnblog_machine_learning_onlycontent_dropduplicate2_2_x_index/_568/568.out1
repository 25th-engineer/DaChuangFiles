最近 一直 在 看 机器学习 相关 的 算法 今天 学习 
logistic 回归 在对 算法 进行 了 简单 分析 编程 实现 
之后 通过 实例 进行 验证 一 logistic 概述 个人 理解 
的 回归 就是 发现 变量 之间 的 关系 也 就是 
求 回归系数 经常 用 回归 来 预测 目标值 回归 和 
分类 同 属于 监督 学习 所 不同 的 是 回归 
的 目标 变 量 必须 是 连续 数值 型 今天 
要 学习 的 logistic 回归 的 主要 思想 是 根据 
现有 的 数据 对 分类 边界线 建立 回归 公式 以此 
进行 分类 主要 在 流行 病学 中 应用 较多 比较 
常用 的 情形 是 探索 某 疾病 的 危险 因素 
根据 危险 因素 预测 某 疾病 发生 的 概率 等等 
logistic 回归 的 因变量 可以 是 二 分类 的 也 
可以 是 多 分类 的 但是 二 分类 的 更为 
常用 也 更加 容易 解释 所以 实际 中 最为 常用 
的 就是 二 分类 的 logistic 回归 今天 我们 就 
二 分类 进行 分析 我们 在 回归 分析 中 需要 
一个 函数 可以 接受 所有 的 输入 然后 预测出 类别 
假定 用 0 和1/nr 分别 表示 两个 类别 logistic 函数 
曲线 很像 型 故此 我们 可以 联系 sigmoid 函数 σ 
= 1 / 1 / 1 + e z 为了实现 
logistic 回归 分类器 我们 可以 在 每个 特征 上 乘以 
一个 回归系数 将 所有 的 乘积 相加 将 和值/nr 代入 
sigmoid 函数 中 得到 一个 范围 为 0 1 之间 
的 数 如果 该 数值 大于 0.5 则 被 归入 
1类 否则 被 归为 0类 基于 之前 的 分析 需要 
找到 回归系数 首先 我们 可以 将 sigmoid 函数 的 输入 
形式 记为 z = w0x0 + w1x1 + . . 
. + wnxn 其中 x 为 输入 数据 相应 的 
w 就是 我们 要求 的 系数 为了 求得 最佳 系数 
结合 最优化 理论 我们 可以 选取 梯度 上升 法 优化 
算法 梯度 上升 法的/nr 基本 思想 是 要 找到 函数 
的 最大 值 最好 的 方法 是 沿着 该 函数 
的 梯度方向 寻找 要想 更进一步 的 了解 这个 方法 建议 
去看 Andrew Ng 的 机器学习 课程 记得 在 第二 节 
主要 讲述 的 就是 梯度 下 降法 与 梯度 上升 
所 不同 的 是 它 求得 的 是 函数 的 
最小值 不过 思想 是 一致 的 二 python 实现 基于 
之前 的 分析 在 本节 我们 对 logistic 回归 一步 
一步 采用 python 编程 实现 今天 我 用 的 是 
2.7 版本 的 代码 如下 # coding utf 8 from 
numpy import * import math import matplotlib . pyplot as 
plt # 导入 数据 def loadDataSet dataMat = labelMat = 
fr = open testSet . txt for line in fr 
. readlines lineArr = line . strip . split # 
将 文本 中的 每行 中的 字符 一个个 分开 变成 list 
dataMat . append 1.0 float lineArr 0 float lineArr 1 
labelMat . append int lineArr 2 return dataMat labelMat # 
定义 sigmoid 函数 def sigmoid inX return 1.0 / 1 
+ exp inX # 梯度 上升 方法 求出 回归系数 def 
gradAscent data label dataMat = mat data labelMat = mat 
label . transpose m n = shape dataMat alpha = 
0.001 maxCycles = 500 weights = ones n 1 for 
item in range maxCycles h = sigmoid dataMat * weights 
error = labelMat h # 注意 labelMat 中的 元素 的 
数据类型 应为 int weights = weights + alpha * dataMat 
. transpose * error return weights # 测试 data label 
= loadDataSet print gradAscent data label # # 求出 回归系数 
之后 就 确定 了 不同 数据 类别 之间 的 分隔线 
为了 便于 理解 可以 画出 那条 线 def plotBestFit weights 
dataMat labelMat = loadDataSet dataArr = array dataMat n = 
shape dataArr 0 xcode1 = ycode1 = xcode2 = ycode2 
= for i in range n if int labelMat i 
= = 1 xcode1 . append dataArr i 1 ycode1 
. append dataArr i 2 else xcode2 . append dataArr 
i 1 ycode2 . append dataArr i 2 fig = 
plt . figure ax = fig . add _ subplot 
111 ax . scatter xcode1 ycode1 s = 30 c 
= red marker = s ax . scatter xcode2 ycode2 
s = 30 c = green x = arange 3.0 
3.0 0.1 y = weights 0 weights 1 * x 
/ weights 2 ax . plot x y plt . 
xlabel x1 plt . ylabel y1 plt . show # 
测试 data label = loadDataSet weights = gradAscent data label 
plotBestFit weights . getA # # 改进 的 梯度 上升 
法 def stocGradAscent1 dataMatrix classLabels numIter = 150 m n 
= shape dataMatrix weights = ones n # initialize to 
all ones for j in range numIter dataIndex = range 
m for i in range m alpha = 4 / 
1.0 + j + i + 0.0001 randIndex = int 
random . uniform 0 len dataIndex h = sigmoid sum 
dataMatrix randIndex * weights error = classLabels randIndex h weights 
= weights + alpha * error * dataMatrix randIndex del 
dataIndex randIndex return weights # 测试 data label = loadDataSet 
weights = stocGradAscent1 array data label plotBestFit weights 三 实例 
分析 基于 之前 的 分析 本节 采用 Logistic 回归 来 
预测 患有 疝病 的 马的/nr 存活 问题 代码 如下 def 
classifyVector inX weights prob = sigmoid sum inX * weights 
if prob 0.5 return 1.0 else return 0.0 def colicTest 
frTrain = open h o r s e C o 
l i c T r a i n i n 
g . txt frTest = open horseColicTest . txt trainingSet 
= trainingLabels = for line in frTrain . readlines currLine 
= line . strip . split \ t lineArr = 
for i in range 21 lineArr . append float currLine 
i trainingSet . append lineArr trainingLabels . append float currLine 
21 trainWeights = stocGradAscent1 array trainingSet trainingLabels 1000 errorCount = 
0 numTestVec = 0.0 for line in frTest . readlines 
numTestVec + = 1.0 currLine = line . strip . 
split \ t lineArr = for i in range 21 
lineArr . append float currLine i if int classifyVector array 
lineArr trainWeights = int currLine 21 errorCount + = 1 
errorRate = float errorCount / numTestVec print 错误率 是 errorRate 
return errorRate def multiTest numTests = 10 errorSum = 0.0 
for k in range numTests errorSum + = colicTest print 
平均 错误率 是 numTests errorSum / float numTests multiTest 最后 
可以 看出 错误率 在 35% 左右 通过 调节 步长 还是 
可以 进一步 减小 错误率 Logistic 回归 的 目的 是 寻找 
到 一个 非线性 sigmoid 函数 的 最佳 拟合 参数 可以 
采用 梯度 上升 法 优化 而 在 这个 过程 中 
为了 减少 时间 复杂度 又 可以 使用 随机 梯度 上升 
法来/nr 简化 梯度 上升 法 