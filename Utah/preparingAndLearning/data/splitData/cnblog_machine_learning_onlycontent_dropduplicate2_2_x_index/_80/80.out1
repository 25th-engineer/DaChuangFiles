Boosting 方法 实际上 是 采用 加法 模型 与 前 向 
分布 算法 在上/i 一篇/m 提到/v 的/uj Adaboost/w 算法/n 也/d 可以/c 
用/p 加法/v 模型/n 和前向/nr 分布/v 算法/n 来/v 表示/v 以 决策树 
为 基 学习 器 的 提升 方法 称为 提升 树 
Boosting Tree 对分 类 问题 决策树 是 CART 分类 树 
对 回归 问题 决策树 是 CART 回归 树 1 前/f 
向/p 分布/v 算法/n 引入/v 加法/v 模型/n 在/p 给定/v 了/ul 训练/vn 
数据/n 和/c 损失/n 函数/n $/i L/w y f x $ 
的 条件 下 可以 通过 损失 函数 最小化 来 学习 
加法 模型 然而 对于 这个 问题 是个 很复杂 的 优化 
问题 而且 要 训练 的 参数 非常 的 多 前 
向 分布 算法 的 提出 就 是 为了 解决 模型 
的 优化 问题 其 核心 思想 是 因为 加法 模型 
是由 多 各 模型 相加 在 一起 的 而且在 Boosting 
中 模型 之间 又是 有 先后 顺序 的 因此 可以 
在 执行 每一步 加法 的 时候 对 模型 进行 优化 
那么 每一步 只 需要 学习 一个 模型 和 一个 参数 
通过 这种 方式 来 逐步 逼近 全局 最优 每一步 优化 
的 损失 函数 具体 算法 流程 如下 1 初始化 $ 
f _ 0 x = 0 $ 2 第 m 
次 迭 代时 极小 化 损失 函数 3 更新 模型 
则 $ f _ m   x $ 4 得到 
最终 的 加法 模型 Adaboost 算法 也 可以 用 前 
向 分布 算法 来 描述 在 这里 输入 的 数据集 
是 带有 权重 分布 的 数据 集 损失 函数 是 
指数 损失 函数 2 GBDT 算法 GBDT 是 梯度 提升 
决策树 Gradient Boosting Decision Tree 的 简称 GBDT 可以 说 
是 最好 的 机器学习 算法 之一 GBDT/w 分类/n 和/c 回归/v 
时的基/nr 学习/v 器/n 都是/i CART/w 回归/v 树/v 因为 是 拟合 
残差 的 GBDT/w 和/c Adaboost/w 一样/r 可以/c 用/p 前/f 向/p 
分布/v 算法/n 来/v 描述/v 不同之处 在于 Adaboost 算法 每次 拟合 
基 学习 器 时 输入 的 样本 数据 是 不 
一样 的 每 一轮 迭 代时 的 样本 权重 不一致 
因为 Adaboost 旨在 重点 关注 上 一轮 分类 错误 的 
样本 GBDT 算法 在 每一步 迭 代时 是 输出 的 
值 不 一样 本轮 要 拟合 的 输出 值 是 
之前 的 加法 模型 的 预测 值 和 真实 值 
的 差值 模型 的 残差 也 称为 损失 用于 一个 
简单 的 例子 来 说明 GBDT 假如 某人 的 年龄 
为 30岁 第一次 用 20岁 去 拟合 发现 损失 还有 
10岁 第二次 用 6岁 去 拟合 10岁 发现 损失 还有 
4岁 第三 次 用 3岁 去 拟合 4岁 依次 下去 
直到 损失 在 我们 可 接受 范围 内 以 平方 
误差 损失 函数 的 回归 问题 为例 来 看看 以 
损失 来 拟合 是个 什么 样子 采用 前 向 分布 
算法 在 第 $ m $ 次 迭 代时 我们 
要 优化 的 损失 函数 此时 我 们 采用 平方 
误差 损失 函数 为例 则 上面 损失 函数 变为 问题 
就 成了 对 残差 r 的 拟合 了 然而 对于 
大多数 损失 函数 却没 那么 容易 直接 获得 模型 的 
残差 针对 该 问题 大神 Freidman 提出 了 用 损失 
函数 的 负 梯度 来 拟合 本轮 损失 的 近似值 
拟合 一个 回归 树 关于 GBDT 一般 损失 函数 的 
具体 算法 流程 如下 1 初始化 $ f _ 0 
x $ 2 第 $ m $ 次 迭 代时 
计算 当前 要 拟合 的 残差 $ r _ { 
mi } $ 以 $ r _ { mi } 
$ 为 输出 值 对 $ r _ { mi 
} $ 拟合 一个 回归 树 此时 只是 确定 了 
树 的 结构 但是 还 未确定 叶子 节点 中的 输出 
值 然后 通过 最小化 当前 的 损失 函数 并 求得 
每个 叶子 节点 中的 输出 值 $ c _ { 
mj } $ $ j $ 表示 第 $ j 
$ 个 叶子 节点 更新 当前 的 模型 $ f 
_ m x $ 为 3 依次 迭代 到 我们 
设定 的 基 学习 器 的 个数 $ M $ 
得到 最终 的 模型 其中 $ M $ 表示 基 
学习 器 的 个数 $ J $ 表示 叶子 节点 
的 个数 GBDT 算法 提供 了 众多 的 可选择 的 
损失 函数 通过 选择 不同 的 损失 函数 可以 用来 
处理 分类 回归 问题 比如 用 对数 似 然 损失 
函数 就 可以 处理 分类 问题 大概 的 总结 下 
常用 的 损失 函数 1 对于 分类 问题 可以 选用 
指数 损失 函数 对数 损失 函数 2 对于 回归 问题 
可以 选用 均方差 损失 函数 绝对 损失 函数 3 另外 
还有 huber 损失 函数 和 分位数 损失 函数 也是 用于 
回归 问题 可以 增加 回归 问题 的 健壮性 可以 减少 
异常 点 对 损失 函数 的 影响 3 GBDT 的 
正则化 在 Adaboost 中 我们 会 对 每个 模型 乘 
上一个 弱化 系数 正则化 系数 减小 每个 模型 对 提升 
的 贡献 注意 这个 系数 和 模型 的 权重 不一样 
是 在 权 重上 又 乘以 一个 0 1 之间 
的 小数 在 GBDT 中 我们 采用 同样 的 策略 
对于 每个 模型 乘以 一个 系数 λ 0 λ ≤ 
1 降低 每个 模型 对 拟合 损失 的 贡献 这种 
方法 也 意味着 我们 需要 更多 的 基 学习 器 
第二种 是 每次 通过 按比例 推荐 0.5 0.8 之间 随机 
抽取 部分 样本 来 训练 模型 这种方法 有点 类似 Bagging 
可以 减小 方差 但 同样 会 增加 模型 的 偏差 
可采用 交叉 验证 选取 这种方式 称 为子 采样 采用 子 
采样 的 GBDT 有时 也 称为 随机 梯度 提升 树 
SGBT 第三 种 就是 控制 基 学习 器 CART 树 
的 复杂度 可以 采用 剪枝 正则化 4 GBDT 的 优缺点 
GBDT 的 主要 优点 1 可以 灵活 的 处理 各种 
类型 的 数据 2 预测 的 准确率 高3/nr 使用 了 
一些 健壮 的 损失 函数 如 huber 可以 很好 的 
处理 异常值 GBDT 的 缺点 1 由于 基 学习 器 
之间 的 依赖 关系 难以 并行 化 处理 不过 可以 
通过 子 采样 的 SGBT 来 实现 部分 并行 5 
XGBoost 算法 事实上 对于 树 模型 为 基 学习 器 
的 集成 方法 在 建模 过程 中 可以 分为 两个 
步骤 一是 确定 树 模型 的 结构 二 是 确定 
树 模型 的 叶子 节点 中的 输出 值 5.1 定义/n 
树/v 的/uj 复杂度/n 首先/d 把/p 树/v 拆分/v 成/n 结构/n 部分/n 
$/i q/w $/i 和/c 叶子/nr 节点/n 输出/v 值/n $/i w/w 
$/i 在 这里 $ w $ 是 一个 向量 表示 
各 叶子 节点 中的 输出 值 在 这里 就 囊括 
了 上面 提到 的 两点 确定 树结构 $ q $ 
和 叶子 结点 的 输出 值 $ w $ 从下 
图中 可以 看出 $ q x $ 实际上 是 确定 
输入 值 最终 会 落到 哪个 叶子 节点 上 而 
$ w $ 将会 给出 相应 的 输出 值 具体表现 
示例 如下 引入 正则化 项 $ \ Omega f _ 
t $ 来 控制 树 的 复杂度 从而 实现 有效 
的 控制 模型 的 过拟合 这是 xgboost 中 的 第一 
个 重要 点 式子 中的 $ T $ 为 叶子 
节 点数 5.2 XGBoost 中的 Boosting Tree 模型 和 GBDT 
方法 一样 XGBoost 的 提升 模型 也 是 采用 残差 
不同 的 是 分裂 结点 选取 的 时候 不 一定 
是 最小 平方 损失 其 损失 函数 如下 较 GBDT 
其 根据 树 模型 的 复杂度 加入 了 一项 正则化 
项 5.3 对 目标函数 进行 改写 上面 的 式子 是 
通过 泰勒 展开式 将 损失 函数 展开 为 具有 二阶 
导 的 平方 函数 在 GBDT 中 我们 通过 求 
损失 函数 的 负 梯度 一 阶 导数 利用 负 
梯度 替代 残差 来 拟合 树 模型 在 XGBoost 中 
直接 用 泰勒 展开式 将 损失 函数 展 开成 二项式 
函数 前提 是 损失 函 数一 阶 二阶 都 连续 
可导 而且/c 在/p 这里/r 计算/v 一/m 阶/n 导/v 和/c 二阶/n 
导/v 时/n 可以/c 并行计算/l 假设 此时 我们 定义 好了 树 
的 结构 在后面 介绍 和 GBDT 中 直接 用 残差 
拟合 不同 假设 我们 的 叶 节点 区域 为 上面 
式子 中 $ i $ 代表 样本 $ i $ 
$ j $ 代表 叶子 节点 $ j $ 则 
我们 的 目标 优化 函数 可以 转换成 因为 $ l 
y _ i y _ i ^ { t 1 
} $ 是个 已经 确定 的 常数 可以 舍去 上面 
式子 把 样本 都 合并 到 叶子 节点 中了 此时 
我们 对 $ w _ j $ 求导 并 令 
导数 为 0 可得 其中 $ G _ j = 
\ sum _ { i \ in I _ j 
} g _ i   H _ j = \ 
sum _ { i \ in T _ j } 
h _ j $ 5.4 树 结构 的 打分 函数 
上面 的 Obj 值 代表 当 指定 一个 树结构 时 
在 目标 上面 最多 减少 多少 我们 可以 把 它 
称为 结构 分数 可以 认为 这 是 一个 类似 与 
基尼指数 一样 更 一般 的 对 树结构 进行 打分 的 
函数 如 下面 的 例子 所示 对于 求得 Obj 分数 
最小 的 树结构 我们 可以 枚举 所有 的 可能性 然后 
对比 结构 分数 来 获得 最优 的 树结构 然而 这种 
方法 计算 消耗 太大 更常用 的 是 贪心法 事实上 绝大多数 
树 模型 都是/nr 这样 的 只 考虑 当前 节点 的 
划分 最优 每次 尝试 对 已经 存在 的 叶 节点 
最 开始 的 叶 节点 是 根 节点 进行 分割 
然后 获得 分割 后的/nr 增益 为 在 这里 以 $ 
Gain $ 作为 判断 是否 分割 的 条件 这里 的 
$ Gain $ 可以 看作 是 未分割 前 的 Obj 
减去 分割 后的/nr 左右 Obj 因此 如果 $ Gain 0 
$ 则 此 叶 节点 不做 分割 然而 这样 对于 
每次 分割 还是 需要 列出 所有 的 分割 方案 对于 
特征 的 值 的 个数 为 n 时 总共有 2 
^ n 2   种 划分 而 实际 中 是 
采用 近似 贪心 方法 我们 先 将 所有 样本 按照 
$ g _ i $ 从小到大 排序 然后 进行 遍历 
查看 每个 节点 是否 需要 分割 对于 特征 的 值 
的 个数 为 $ n $ 时 总共有 $ n 
1 $ 种 划分 具体 示例 如下 最 简单 的 
树结构 就是 一个 节点 的 树 我们 可以 算出 这棵 
单 节点 的 树 的 好坏 程度 obj * 假设 
我们 现在 想 按照 年龄 将 这棵 单 节点 树 
进行 分叉 我们 需要 知道 1 按照 年龄 分 是否 
有效 也 就是 是否 减少 了 obj 的 值 2 
如果 可分 那么 以 哪个 年龄 值 来 分 此时 
我们 就是 先 将 年龄 特征 从小到大 排好 序 然后再 
从左到右 遍历 分割 这样 的 分割 方式 我们 就 只要 
对 样本 扫描 一遍 就 可以 分 割出 $ G 
_ L $ $ G _ R $ 然后 根据 
$ Gain $ 的 分数 进行 分割 极大 地 节省 
了 时间 所以 从 这里 看 XGBoost 中 从新 定义 
了 一个 划分 属性 也 就是 这里 的 $ Gain 
$ 而 这个 划分 属性 的 计算 是 由其 目标 
损失 决定 obj 的 5.5 XGBoost 中 其他 的 正则化 
方法 1 像 Adaboost 和 GBDT 中 一样 对 每 
一个 模型 乘以 一个 系数 $ \ lambda 0 \ 
lambda ≤ 1 $ 用来 降低 每个 模型 对 结果 
的 贡献 2 采用 特 征子 采 样方法 和 RandomForest 
中的 特 征子 采样 一样 可以 降低 模型 的 方差 
6 XGBoost 和 GBDT 的 区别 1 将 树 模型 
的 复杂度 加入到 正则 项中/nr 来 避免 过拟合 因此 泛化 
性能 会 由于 GBDT 2 损失 函数 是 用 泰勒 
展开式 展开 的 同时 用 到了 一 阶 导 和 
二阶 导 可以 加快 优化 速度 3 和 GBDT 只 
支持 CART 作为 基 分类器 之外 还 支持 线性 分类器 
在 使用 线性 分类器 的 时候 可以 使用 L1 L2 
正则化 4 引进 了 特征 子 采样 像 RandomForest 那样 
这种方法 既能 降低 过拟合 还能 减少 计算 5 在 寻找 
最佳 分割 点 时 考虑 到 传统 的 贪心 算法 
效率 较低 实现 了 一种 近似 贪心 算法 用来 加速 
和 减小 内存 消耗 除此之外 还 考虑 了 稀疏 数据集 
和 缺失 值 的 处理 对于 特征 的 值 有 
缺失 的 样本 XGBoost 依然 能 自动 找到 其 要 
分裂 的 方向 6 XGBoost 支持 并行处理 XGBoost 的 并行 
不是 在 模型 上 的 并行 而是 在 特征 上 
的 并行 将 特征 列 排序 后以 block 的 形式 
存储 在 内存 中 在 后面 的 迭代 中 重复 
使用 这个 结构 这个 block 也 使得 并行 化 成为 
了 可能 其次 在 进行 节点 分裂 时 计算 每个 
特征 的 增益 最终 选择 增益 最大 的 那个 特征 
去做 分割 那么 各个 特征 的 增益 计算 就 可以 
开 多线程 进行 