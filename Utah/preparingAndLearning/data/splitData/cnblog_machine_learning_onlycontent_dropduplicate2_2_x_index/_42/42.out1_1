1 . 机器学习 通过 对 以往 历史数据 的 学习 建立 
一个 模型 用来 预测 以后 的 数据 进行 预测 和 
分析 1.1 监督 学习 supervised learning 监督 学习 可以 分为 
生成 方法 生成 模型 generative 和 判别 方法 判别 模型 
discreiminative 生成 模型 学习 联合 概率分布 p x y p 
y | x = p x y / p x 
= p y p x | y / p y 
比如 贝叶斯 模型 隐 马尔科夫 模型 HMM 判别 模型 有 
数据 直接 学习 f x 或者 条件 概率分布 p y 
| x 比如 最 近邻 KNN 感知机 perception 决策树 等 
学习 过程 的 三要素 模型 Model 策略 算法 Model 即 
你 要 学习 什么样 的 模型 包括 线性 模型 非线性 
模型 等 取决于 你 要 学习 的 问题 和 数据 
策略 模型 按照 什么样 的 准则 在整个 模型 解 空间 
中 选择 最优 的 模型 其实 就是 损失 代价 函数 
包括 0 1 损失 平方 损失 等 算法 参数 模型 
的 求解 方法 比如 梯度 下 降法 牛顿 法等/nr 风险 
函数   分 险 函数 度量 评价 意义 下 的 
模型 预测 的 好坏 经验 风险 最小化 在 假设 空间 
损失 函数 已经 训练 数据集 确定 的 情况 下 期望 
风险 最小化 设计 的 模型 与 真实 的 误差 但是 
由于 我们 是 无法 知道 数据 的 真实 分布 的 
如果 知道 数据 的 真实 分布 我们 就 不 需要 
学习 模型 参 数了 但是 根据 大数 定律 当 我们 
的 训练 数据 趋于 无穷大 的 时候 经验 风险 最小化 
可以 近似 的 等于 经验 风险 最小化 结构 风险 最小化 
为了 使 学习 的 模型 不 过拟合 引入 正则 项 
对模型 的 参数 进行 惩罚 使得 模型 不能 过于 复杂 
J f 是 模型 的 复杂度 比如 参数 向量 的 
个数 举个 例子 说明 比如 房价 的 交易 价格 Y 
和 房子 面积 X 问题 假如 现在 要 预测 一个 
面积 为 750 的 房子 价格 该 多少 钱 我们 
最 能 想到 的 是 用 一条 曲线 去 拟合 
这些 点 然后 求出 这条 曲线 的 方程 再把 x 
代入 求 解出 Y 这 就是 监督 学习 因为 对于 
每 一条 数据 我们 都 预先 给出 了 正确 的 
结果 上面 这个 问题 又 称为 回归 问题 regression 因为 
预测 的 变量 Y 是 连续 的 如果 预测 的 
变量 不是 连续 的 而是 有 类别 的 就 叫做 
分类 问题 Classification 上面 例子 只用 了 一个 特征 在 
现实 生活 中 其实 有 很多 维 特征 特征 也 
可能 是 无限 维 svm 是 可以 支持 无线 维 
特征 的 算法 1.2 无 监督 学习 unsupervised learning 在有 
监督 问题 中 无论是 分类 还是 回归 我们 的 每个 
数据 都 具有 一个 结果 比如 房价 多少 但是 在 
无 监督 学习 算法 里面 每个 数据 是 没有 结果 
的 我们 只用 特征 而无 监督 学习 则是 学习 如何 
可以 将 这些 数据 分到 各自 不同 的 组 里面 
去     无 监督 学习 的 一个 例子 就是 
聚 类 问题 clustering 2 . 线性 回归 问题 2.1 
流程 对于 解决 房价 问题 其实 我们 是 要将 训练 
数据 输入 到 学习 算法 进而 学习 到 一个 假设 
H 然后 我们 将 输入 变量 输入 到 h 预测出 
房价 价格 2 . 分类 问题 那么 对于 h 我们 
应当 如何 表示 呢 可以 是 因为 只含 有 一个 
特征 变量 所以 也 叫做 单 变量 线性 回归 问题 
2.2 代价 函数 cost fuction 现在 我们 就要 为 我们 
上面 建立 好 的 模型 选择 适当 的 参数 Θ 
我们 选择 的 参数 就 决定 了 我们 预测 的 
准确度 模型 的 预测 值 和 训练 集中 的 真实 
数据 差 就是 建模 误差 我们 的 目标 就是 要 
找出 使得 建模 误差 最小 的 模型 参数 使得 代价 
函数 最小 具体 可以 查看 http / / www . 
cnblogs . com / GuoJiaSheng / p / 3928160 . 
html 3 . 分类 问题 3.1 逻辑 回归 logistic regreesion 
在 分类 问题 中 我们 尝试 对 一个 数据 进行 
分类 比如 判别 它 是否 正确 是否 为 一封 垃圾邮件 
我们 从二/nr 分类 开始 讨论 我们 将 因变量 可能 属于 
的 类别 分为 负 类 正 类 y = { 
0 1 } 0 为 负 类 1 为 正 
类 如 下图 我们 可以 用 线性 回归 的 方法 
拟合 一条 曲线 但是 线性 回归 只能 预测出 连续 值 
但是 我们 的 分类 问题是 要 预测 为 1 或者 
0 我们 可以 这样 分 当 x 0.5 为 1 
当 x 0.5 为 0 这样 也 可以 很好 的 
区分 数据 类别 但是 当 我们 观测 的 数据 越来 
越大 的 时候 如 下图 这时候 由于 新增 了点 如果 
这时候 还 使用 0.5 作为 分隔 点 就会 出错 我们 
可以 得出 线性 回归 因为 预测 的 值 可以 超过 
0 1 所以 并不 适合 解决 这 类 问题 我们 
引入 逻辑 回归 它 的 输出 值 只在 0 1 
范围 之内 h θ x = g θ TX x 
代表 特征 变量 g 代表 逻辑 函数 为 s 形 
函数 其实 就是 sigmod 函数 它 的 值域 为 0.1 
对于 模型 的 理解 为 给出 参数 x 根据 选择 
的 参数 计算出 变量 为 1 或者 0 的 概率 
可能性 当 z 0 是 预测 为 1 当在 z 
0时 预测 为 0 . 比如说 我们 有 一个 模型 
并且 参数 为 3 1 1 即 当 3 + 
x1 + x2 0时 即 x1 + x2 3时 预测 
为 1 否 则为 0 为此 我们 就 可以 画出 
一条线 用于 分类 如 下图 3.2 代价 函数 之前 对于 
线性 回归 的 代价 我们 定义 为 模型 误差 的 
平方和 理论 上在 逻辑 回归 上 我们 也 可以 沿用 
这一 定义 但是 如果 将 h x 代入 我们 得到 
的 模型 误差 平方和 就会 是 一个 非 凸函数 这 
意味着 我们 的 代价 函数 有 许多 局部 最 小解 
这将 影响 我们 使用 梯度 下 降法 求解 全局 最小值 
因此 对于 逻辑 回归 的 代价 函数 我们 重新 定义 
为 然后 就 可以 使用 梯度 下 降法 求 解了 
3.3多 分类 问题 对于 多 分类 问题 我们 无法 仅仅 
使用 0 1 用于 区分 某个 类别 如 下图 一种 
解决 的 办法 就是 使用 一对 多 方案 其实 就是 
在 多 类别 里面 将 一个 类别 作为 正 类 
其他 剩余 的 类别 合起来 作为 一个 类别 这样 就 
转化 为 2分 类 问题 这样 当 我们 要 预测 
的 时候 将 所有 的 分类机 都 执行 一遍 找出 
最 可能 的 类别 模型 作为 预测值 