引言 机器学习 领域 中 所谓 的 降 维 就是指 采用 
某种 映射方法 将 原 高维空间 中 的 数据 点映 射到 
低 维度 的 空间 中 降 维 的 本质 是 
学习 一个 映射函数 f x y 其中 x 是 原始 
数 据点 的 表达 目前 最多 使用 向量 表达形式 y 
是 数据 点 映射 后的/nr 低维 向量 表达 通常 y 
的 维度 小于 x 的 维度 当然 提高 维度 也 
是 可以 的 f 可能 是 显 式 的 或 
隐式 的 线性 的 或非 线性 的 当然 还有 一 
大类 方法 本质上 也是 做了 降 维 叫做 feature selection 
目的 是 从 原始 的 数据 feature 集合 中 挑选 
一 部分 作为 数据 的 表达 目前 大部分 降 维 
算法 处理 向量 表达 的 数据 也 有 一些 降 
维 算法 处理 高阶 张量 表达 的 数据 之所以 使用 
降 维 后的/nr 数据 表示 是因为 1 在 原始 的 
高维空间 中 包含 有 冗余 信息 以及 噪音 信息 在 
实际 应用 例如 图像识别 中 造成了 误差 降低 了 准确率 
而 通过 降 维 我们 希望 减少 冗余 信息 所 
造成 的 误差 提高 识别 或 其他 应用 的 精度 
2 或者 希望 通过 降 维 算法 来 寻找 数据 
内部 的 本质 结构特征 3 通过 降 维 来 加速 
后续 计算 的 速度 4 还有 其他 很多 目的 如 
解决 数据 的 sparse 问题 在 很多 算法 中 降 
维 算法 成为 了 数据 预处理 的 一部分 如 PCA 
事实上 有 一些 算法 如果 没有 降 维 预处理 其实 
是 很难 得到 很好 的 效果 的 如果 你 需要 
处理 数据 但是 数据 原来 的 属性 又 不一定 需要 
全部 保留 那么 PCA 也许 是 一个 选择 主 成分 
分析 算法 PCA Principal Component Analysis PCA 是 最 常用 
的 线性 降 维 方法 它 的 目标 是 通过 
某种 线性 投影 将 高维 的 数据 映 射到 低维 
的 空间 中 表示 并 期望 在所 投影 的 维度 
上 数据 的 方差 最大 以此 使用 较少 的 数据 
维度 同时 保留住 较多 的 原 数 据点 的 特性 
通俗 的 理解 如果 把 所有 的 点 都 映射 
到一起 那么 几乎 所有 的 信息 如 点 和点/nr 之间 
的 距离 关系 都 丢失 了 而 如果 映射 后 
方差 尽可能 的 大 那么 数 据点 则会 分散开来 以此来 
保留 更多 的 信息 可以 证明 PCA 是 丢失 原始数据 
信息 最少 的 一种 线性 降 维 方式 实际上 就是 
最 接近 原始数据 但是 PCA 并不 试图 去 探索 数据 
内在 结构 设 n 维 向量 w 为 目标 子空间 
的 一个 坐标轴 方向 称为 映射 向量 最大化 数据 映射 
后的/nr 方差 有 其中 m 是 数据 实例 的 个数 
xi 是 数据 实例 i 的 向量 表达 x 拔 
是 所有 数据 实例 的 平均 向量 定义 W 为 
包含 所有 映射 向量 为 列 向量 的 矩阵 经过 
线性代数 变换 可以 得到 如下 优化 目标函数 W W = 
I 是 说 希望 结果 的 每一个 feature 都 正交 
这样 每一 维度 之间 不会 有 冗余 信息 其中 tr 
表示 矩阵的迹 A 是 数据 协方差 矩阵 容易 得 到 
最优 的 W 是由 数据 协方差 矩 阵前 k 个 
最大 的 特征值 对应 的 特征向量 作为 列 向量 构成 
的 这些 特征向量 形成 一组 正交基 并且 最好 地 保留 
了 数据 中 的 信息 PCA 的 输出 就是 Y 
= W X 由 X 的 原始 维度 降低 到了 
k 维 因此 不 知道 推导 也 无所谓 只要会 算 
就行 注意 X 需要 均值 化 来看 个 例子 当 
使用 1个 特征向量 的 时候 3 的 基本 轮廓 已经 
保留 下来 了 特征向量 使用 的 越多 就越 与 原始数据 
接近 PCA 追求 的 是 在 降 维 之后 能够 
最大化 保持数据 的 内在 信息 并 通过 衡量 在 投影 
方 向上 的 数据 方差 的 大小 来 衡量 该 
方向 的 重要性 但是 这样 投影 以后 对 数据 的 
区分 作用 并 不大 反而 可能 使 得数 据点 揉杂 
在 一起 无法 区分 这 也是 PCA 存在 的 最大 
一个 问题 这 导致 使用 PCA 在 很多 情况下 的 
分类 效果 并 不好 具体 可以 看 下图 所示 若 
使用 PCA 将 数据 点 投影 至 一维空间 上 时 
PCA 会 选择 2 轴 这 使得 原本 很 容易 
区分 的 两 簇 点 被 揉杂 在 一起 变 
得 无法 区分 而 这时 若 选择 1 轴 将会 
得到 很好 的 区分 结果 Discriminant Analysis 所 追求 的 
目标 与 PCA 不同 不是 希望 保持数据 最多 的 信息 
而是 希望 数据 在 降 维 后 能够 很容易 地被 
区分 开来 后面 会 介绍 LDA 的 方法 是 另一种 
常见 的 线性 降 维 方法 另外 一些 非 线性 
的 降 维 方法 利用 数 据点 的 局部 性质 
也 可以 做到 比较 好 地区 分 结果 例如 LLE 
Laplacian Eigenmap 等 以后 会 介绍 