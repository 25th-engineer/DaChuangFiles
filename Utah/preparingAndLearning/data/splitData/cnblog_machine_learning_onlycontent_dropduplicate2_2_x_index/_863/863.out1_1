这次 作业 的 coding 任务 量 比较 大 总的来说 需要 
实现 neural network knn kmeans 三种 模型 Q11 ~ Q14 
为 Neural Network 的 题目 我 用 单线程 实现 的 
运行 的 时间 比较 长 因此 把这 几道 题 的 
正确 答案 记录 如下 Q11 6Q12 0 . 001Q13 0 
. 01Q14 0.02 ≤ Eout ≤ 0.04 其中 Q11 和 
Q14 的 答案 比较 明显 Q12/i 和/c Q13/i 有/v 两个/m 
答案/n 比较/d 接近/v 参考 了 讨论区 的 内容 最终 也 
调 出来 了 neural network 的 代码 实现 思路 如下 
1 实现 权重 矩阵 W 初始化 def init _ W 
nnet _ struct w _ range 2 实现 计算 每 
一轮 神经元 输出 的 函数 即 bp 算法 中的 forward 
过程 def forward _ process x y W 3 实现 
计算 每 一轮 output error 对于 每个 神经元 输入 score 
的 导数 即 bp 算法 中的 backward 过程 def backward 
_ process x y neuron _ output W 4 利用 
梯度 下降 方法 更新 各层 权重 矩阵 W 的 函数 
def update _ W _ withGD x neuron _ output 
gradient W ita 其中 最难 的 是 步骤 3 要想 
实现 矩阵 化 编程 需要 对 神经 网络 的 每层 
结构 熟练 同时 对于 你 使用 的 编程 语言 的 
矩阵 化 操作 要 非常 熟悉 自己 在 这个 方面 
比较 欠缺 还得 是 熟能生巧 自己 第一 次 写 NNet 
的 算法 从/p 单隐层/nr 隐 层 个数 2 开始 调试 
的 按照 模块 1 2 3 4 的 顺序 各个 
模块 调试 循序渐进 的 调试 速度 比较 慢 但 模块 
质量 高 一些 后面 的 联合 调试 就 省事 一些 
如果 是 特别 复杂 的 网络 如何 对 这种 gradient 
的 算法 进行 调试 呢 因为 gradient 各个 点 的 
gradient 几乎 是 不 可能 都 算到 的 在 网上 
查了/nr gradient checking 方法 http / / ufldl . stanford 
. edu / wiki / index . php / Gradient 
_ checking _ and _ advanced _ optimization NNet 的 
调 参 真的 很 重要 就 Q14 来说 即使 是 
hidden units 的 总 个数 一样 如果 每层 的 个数 
不同 最后 的 结果 也是 有 差别 的 我 第一 
次 比较 粗心 把 NNet 的 结构 按照 3 8 
1 这样 了 发现 结果 没有 8 3 1 这样 
好 后面 多 搜搜 调 参 相关 的 资料 积累 
一下 代码 如下 没有 把 调试 的 代码 删掉 可以 
记录 调试 的 经过 同时 也 防止 以后 犯 类似 
的 错误 确实 乱 了 一些 请 看官 包涵 了 
# encoding = utf8 import sys import numpy as np 
import math from random import * # # # read 
data from local file # return with numpy array def 
read _ input _ data path x = y = 
for line in open path . readlines if line . 
strip = = continue items = line . strip . 
split tmp _ x = for i in range 0 
len items 1 tmp _ x . append float items 
i x . append tmp _ x y . append 
float items 1 return np . array x np . 
array y # # # initialize weight matrix # input 
neural network structure & initilizing uniform value range both low 
and high # each layer s bias need to be 
added # return with inialized W def init _ W 
nnet _ struct w _ range W = for i 
in range 1 len nnet _ struct tmp _ w 
= np . random . uniform w _ range low 
w _ range high nnet _ struct i 1 + 
1 nnet _ struct i W . append tmp _ 
w return W # # # randomly pick sample from 
raw data for Stochastic Gradient Descent # T indicates the 
iterative numbers # return with data for each SGD iteration 
def pick _ SGD _ data x y T sgd 
_ x = np . zeros T x . shape 
1 sgd _ y = np . zeros T for 
i in range T index = randint 0 x . 
shape 0 1 sgd _ x i = x index 
sgd _ y i = y index return sgd _ 
x sgd _ y # # # forward process # 
calculate each neuron s output def forward _ process x 
y W ret = # print W 0 . shape 
# print W 1 . shape pre _ x = 
np . hstack 1 x for i in range len 
W pre _ x = np . tanh np . 
dot pre _ x W i ret . append pre 
_ x pre _ x = np . hstack 1 
pre _ x return ret # # # backward process 
# calcultae the gradient of error and each neuron s 
input score def backward _ process x y neuron _ 
output W ret = L = len neuron _ output 
# print neuron _ output 0 . shape neuron _ 
output 1 . shape # Output layer score = np 
. dot np . hstack 1 neuron _ output L 
2 W L 1 # print score # print score 
. shape gradient = np . array 2 * y 
neuron _ output L 1 0 * tanh _ gradient 
score # print gradient # print gradient . shape ret 
. insert 0 gradient # Hidden layer for i in 
range L 2 1 1 if i = = 0 
score = np . dot np . hstack 1 x 
W i # print score . shape # print gradient 
. shape # print W 1 1 . transpose . 
shape # print score gradient = np . dot gradient 
W 1 1 . transpose * tanh _ gradient score 
# print gradient # print gradient . shapeq ret . 
insert 0 gradient else score = np . dot np 
. hstack 1 neuron _ output i 1 W i 
# print score . shape # print gradient . shape 
# print W i + 1 1 . transpose . 
shape # print . . . . . . gradient 
= np . dot gradient W i + 1 1 
. transpose * tanh _ gradient score # print gradient 
. shape # print = = = = = = 
ret . insert 0 gradient return ret # give a 
numpy array # boardcast tanh gradient to each element def 
tanh _ gradient s ret = np . zeros s 
. shape for i in range s . shape 0 
ret i = 4.000001 / math . exp 2 * 
s i + math . exp 2 * s i 
+ 2 return ret # # # update W with 
Gradient Descent def update _ W _ withGD x neuron 
_ output gradient W ita ret = L = len 
W # print L + str L # print neuron 
_ output 0 . shape neuron _ output 1 . 
shape # print gradient 0 . shape gradient 1 . 
shape # print W 0 . shape W 1 . 
shape # print np . hstack 1 x . transpose 
. shape # print gradient 0 . shape ret . 
append W 0 ita * np . array np . 
hstack 1 x . transpose * gradient 0 for i 
in range 1 L 1 ret . append W i 
ita * np . array np . hstack 1 neuron 
_ output i 1 . transpose * gradient i # 
print len ret return ret # # # calculate Eout 
def calculate _ E W path x y = read 
_ input _ data path error _ count = 0 
for i in range x . shape 0 if predict 
x i y i W error _ count + = 
1 return 1.000001 * error _ count / x . 
shape 0 def predict x y W y _ predict 
= x for i in range 0 len W 1 
y _ predict = np . tanh np . dot 
np . hstack 1 y _ predict W i y 
_ predict = 1 if y _ predict 0 else 
1 return y _ predict = y # # # 
Q11 def Q11 x y R = 20 # repeat 
time Ms = { 6 16 } # hidden units 
M _ lowests = { } for M in Ms 
M _ lowests M = 0 for r in range 
R T = 50000 ita = 0.1 min _ M 
= 1 E _ min = float inf for M 
in Ms sgd _ x sgd _ y = pick 
_ SGD _ data x y T nnet _ struct 
= x . shape 1 M 1 # print nnet 
_ struct w _ range = { } w _ 
range low = 0.1 w _ range high = 0.1 
W = init _ W nnet _ struct w _ 
range # for i in range len W # print 
W i # print sgd _ x sgd _ y 
for t in range T neuron _ output = forward 
_ process sgd _ x t sgd _ y t 
W # print sgd _ x t sgd _ y 
t # print W # print neuron _ output error 
_ neuronInputScore _ gradient = backward _ process sgd _ 
x t sgd _ y t neuron _ output W 
# print error _ neuronInputScore _ gradient W = update 
_ W _ withGD sgd _ x t neuron _ 
output error _ neuronInputScore _ gradient W ita E = 
calculate _ E W test . dat # print str 
r + + str M + + str E M 
_ lowests M + = E for k v in 
M _ lowests . items print str k + + 
str v # # # Q12 def Q12 x y 
ita = 0.1 M = 3 nnet _ struct = 
x . shape 1 M 1 Rs = { 0.001 
0.1 } R _ lowests = { } for R 
in Rs R _ lowests R = 0 N = 
40 T = 30000 for i in range N for 
R in Rs sgd _ x sgd _ y = 
pick _ SGD _ data x y T w _ 
range = { } w _ range low = 1 
* R w _ range high = R W = 
init _ W nnet _ struct w _ range for 
t in range T neuron _ output = forward _ 
process sgd _ x t sgd _ y t W 
error _ neuronInputScore _ gradient = backward _ process sgd 
_ x t sgd _ y t neuron _ output 
W W = update _ W _ withGD sgd _ 
x t neuron _ output error _ neuronInputScore _ gradient 
W ita E = calculate _ E W test . 
dat print str R + + str E R _ 
lowests R + = E for k v in R 
_ lowests . items print str k + + str 
v # # # Q13 def Q13 x y M 
= 3 nnet _ struct = x . shape 1 
M 1 itas = { 0.001 0.01 0.1 } ita 
_ lowests = { } for ita in itas ita 
_ lowests ita = 0 N = 20 T = 
20000 for i in range N for ita in itas 
sgd _ x sgd _ y = pick _ SGD 
_ data x y T w _ range = { 
} w _ range low = 0.1 w _ range 
high = 0.1 W = init _ W nnet _ 
struct w _ range for t in range T neuron 
_ output = forward _ process sgd _ x t 
sgd _ y t W error _ neuronInputScore _ gradient 
= backward _ process sgd _ x t sgd _ 
y t neuron _ output W W = update _ 
W _ withGD sgd _ x t neuron _ output 
error _ neuronInputScore _ gradient W ita E = calculate 
_ E W test . dat print str ita + 
+ str E ita _ lowests ita + = E 
for k v in ita _ lowests . items print 
str k + + str v # # # Q14 
def Q14 x y T = 50000 ita = 0.01 
E _ total = 0 R = 10 for i 
in range R nnet _ struct = x . shape 
1 8 3 1 w _ range = { } 
w _ range low = 0.1 w _ range high 
= 0.1 W = init _ W nnet _ struct 
w _ range sgd _ x sgd _ y = 
pick _ SGD _ data x y T for t 
in range T neuron _ output = forward _ process 
sgd _ x t sgd _ y t W error 
_ neuronInputScore _ gradient = backward _ process sgd _ 
x t sgd _ y t neuron _ output W 
W = update _ W _ withGD sgd _ x 
t neuron _ output error _ neuronInputScore _ gradient W 
ita E = calculate _ E W test . dat 
print E E _ total + = E print E 
_ total * 1.0 / R def main x y 
= read _ input _ data train . dat # 
print x . shape y . shape # Q11 x 
y # Q12 x y # Q13 x y Q14 
x y if _ _ name _ _ = = 
_ _ main _ _ main Q15 ~ Q18 是 
KNN 算法 相关 的 各道 题 几乎 秒 出 结果 
这里 不 记录 答案 了 KNN 的 核心 也 就是 
KNN 函 数了 1 给定 K 个 邻居 数 返回 
这个 点 属于 哪 一类 代码 尽量 写 的 可 
配置 一些 2 numpy 有个 argsort 函数 可以 根据 数组 
的 value 大小 对 下标 index 进行 排序 并 返回 
排序 后的/nr index 利用 好 这个 特性 代码 很 简洁 
3 如果 是 其他 的 语言 应该 实现 一个 类似 
numpy . argsort 的 模块 代码 整体 上 清晰 不少 
能 KNN 的 代码 如下 # encoding = utf8 import 
sys import numpy as np import math from random import 
* # # # read data from local file # 
return with numpy array def read _ input _ data 
path x = y = for line in open path 
. readlines if line . strip = = continue items 
= line . strip . split tmp _ x = 
for i in range 0 len items 1 tmp _ 
x . append float items i x . append tmp 
_ x y . append float items 1 return np 
. array x np . array y # # # 
KNN for binary classification # input all labeled data & 
test sample # return with label def KNN k x 
y test _ x distance = np . sum x 
test _ x * x test _ x axis = 
1 order = np . argsort distance ret = 0 
for i in range k ret + = y order 
i return 1 if ret 0 else 1 # # 
# Q15 calculate Ein def calculate _ Ein x y 
error _ count = 0 k = 5 for i 
in range x . shape 0 1 # tmp _ 
x = np . vstack x 0 i x i 
+ 1 x . shape 0 1 # tmp _ 
y = np . hstack y 0 i y i 
+ 1 x . shape 0 1 ret = KNN 
k x y x i if y i = ret 
error _ count + = 1 return 1.0 * error 
_ count / x . shape 0 # # # 
Q16 calculate Eout def calculate _ Eout x y path 
test _ x test _ y = read _ input 
_ data path error _ count = 0 k = 
1 for i in range test _ x . shape 
0 ret = KNN k x y test _ x 
i if test _ y i = ret error _ 
count + = 1 return 1.0 * error _ count 
/ test _ x . shape 0 def main x 
y = read _ input _ data knn _ train 
. dat print calculate _ Ein x y print calculate 
_ Eout x y knn _ test . dat if 
_ _ name _ _ = = _ _ main 
_ _ main Q19 ~ Q20 是 Kmeans 算法 相关 
的 运行 代码 也 很快 可以 得出 结果 不 记录 
答案 了 Kmeans 的 算法 实现 思路 非常 清晰 1 
实现 初始化 随机 选 各类 中心点 的 功能 题目 中 
是 随机 选 原始数据 的 点 如果 是 其他 的 
选点 方法 单独 拎 出来 一个 模块 不 影响 其他 
模块 2 实现 每次 更新 各 个数 据点 类别 的 
功能 def update _ category x K centers 3 固定 
各个 点 的 类别 更新 各 个 类别 的 center 
点 坐标 def   update _ centers x y K 
模块 实现 上 得益于 numpy 的 矩阵 计算 操作 函数 
应该 掌握 一套 自己 的 矩阵 计算 操作 代码 这样 
可以 随时 拿起来 二次开发 代码 如下 # encoding = utf8 
import sys import numpy as np import math from random 
import * # # # read data from local file 
# return with numpy array def read _ input _ 
data path x = for line in open path . 
readlines if line . strip = = continue items = 
line . strip . split tmp _ x = for 
i in range 0 len items tmp _ x . 
append float items i x . append tmp _ x 
return np . array x # # # input all 
data and category K # return K category centers def 
Kmeans x K T = 50 E _ total = 
0 for t in range T centers = init _ 
centers x K y = np . zeros x . 
shape 0 R = 50 for r in range R 
y = update _ category x K centers centers = 
update _ centers x y K E = calculate _ 
Ein x y centers print E E _ total + 
= E return E _ total * 1.0 / T 
def init _ centers x K ret = order = 
range x . shape 0 np . random . shuffle 
order for i in range K ret . append x 
order i return np . array ret def update _ 
category x K centers y = for i in range 
x . shape 0 category = 1 distance = float 
inf for k in range K d = np . 
sum x i centers k * x i centers k 
axis = 0 if d distance distance = d category 
= k y . append category return np . array 
y def update _ centers x y K centers = 
for k in range K # print np . sum 
x np . where y = = k axis = 
0 # print np . sum x np . where 
y = = k axis = 0 . shape center 
= np . sum x np . where y = 
= k axis = 0 * 1.0 / np . 
array np . where y = = k . shape 
1 centers . append center return np . array centers 
def calculate _ Ein x y centers # print centers 
0 . shape error _ total = 0 for i 
in range x . shape 0 error _ total + 
= np . sum x i centers y i * 
x i centers y i axis = 0 return 1.0 
* error _ total / x . shape 0 def 
main x = read _ input _ data kmeans _ 
train . dat # print x . shape print Kmeans 
x 2 if _ _ name _ _ = = 
_ _ main _ _ main = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
完成 了 这次 作业 后 终于 跟 完了 机器学习 基石 
+ 机器学习 技法 32次 课 8次 coding 作业 个 人上 
完 这门 课后 主要 有 三点 收获 1 通过 coding 
的 作业 题目 实现 了 一些 主流 机器学习 算法 Perceptron 
AdaBoost stump Linear Regression Logistic Regression Decision Tree Neural Network 
KNN Kmeans 以前 都是 用 算法 包 对 各个 算法 
的 理解 不 如实 现 过 一遍 来得 深 和细/nr 
2 以前 对 各个 算法 的 理解 就是 会用 其实 
也 不能 说 太 会用 上 完 课程 后 对 
每个 模型 的 Motivation 有了 一定 的 掌握 模型 为什么 
要 这么 设计 Regularizer 为什么 要 这么 设计 模型 的 
利弊 有 哪些 以及 模型 的 一些 比较 直观 的 
数学原理 推导 3 以前 看待 各个 机器学习 算法 都是 孤立 
的 看待 每个 算法 这个 算法 是 解决 啥 的 
那个 算法 是 解决 啥 的 没有 成 体系 地 
把 各个 算法 拎 起来 台大 这门 课 在整个 授课 
环节 中 都/d 贯穿/v 了/ul 非常/d 强的/nr 体系/n 的/uj 观念/n 
这里 举 两个 例子 a . Linear Network 与 Factorization 
有啥 联系 15 讲 b . Decision Tree 与 AdaBoost 
有啥 关系 8 9 讲 c . Linear Regression 与 
Neural Network 有啥 关系 12 讲 在看 这门 课 之前 
是 绝对 不会 把 上面 的 每组 中 两个 模型 
联系 起来 看待 的 但 这门 课 确实 给 了 
比较 深 的 motivation 非常 强的/nr 全局 主线 最后 谈 
一点 个人 上公 开课 的 体会 1 只听 一遍 走马观花 
学到 的 东西 微乎其微 2 听课 写 作业 实践者 的 
态度 去学 学到 的 东西 比 只 听课 要 多了 
去了 3 听课 写 作业 写 听课 blog 实践者 + 
研究者 的 态度 去学 最好 的 学 就是 教 在 
写 blog 的 过程 中 会 强迫 自己 把 当时 
很多 不 清晰 的 point 都 搞清楚 要不然 真的 写 
不出来 4 循环 进行 3 温故知新 的 道理 大家 都懂/nr 
就看 有 没有 时间 吧 Sign 就 写 到这 了 
. . . . . 