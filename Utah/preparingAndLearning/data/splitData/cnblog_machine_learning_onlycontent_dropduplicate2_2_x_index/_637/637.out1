机器学习 十大 常用 算法 小结 通过 本 篇 文章 可以 
对 ML 的 常用 算法 有个 常识性 的 认识 没有 
代码 没有 复杂 的 理论 推导 就是 图解 一下 知道 
这些 算法 是 什么 它们 是 怎么 应用 的 例子 
主要 是 分类 问题 每个 算法 都 看了 好几 个 
视频 挑出 讲 的 最 清晰 明了 有趣 的 便于 
科普 以后 有 时间 再 对 单个 算法 做 深入 
地 解析 今天 的 算法 如下 决策树 随机 森林 算法 
逻辑 回归 SVM 朴素 贝叶斯 K 最 近邻 算法 K 
均值 算法 Adaboost 算法 神经网络 马尔可夫 1 . 决策树 根据 
一些 feature 进行 分类 每个 节点 提 一个 问题 通过 
判断 将 数据 分为 两类 再继续 提问 这些 问题 是 
根据 已 有 数据 学习 出来 的 再 投入 新 
数据 的 时候 就 可以 根据 这棵 树上 的 问题 
将 数据 划分 到 合适 的 叶子 上 2 . 
随机 森林 视频 在 源 数据 中 随机 选取 数据 
组成 几个 子集 矩阵 是 源 数据 有 1 N 
条 数据 A B C 是 feature 最后 一 列 
C 是 类 别由 S 随机 生成 M 个子 矩阵 
这 M 个 子集 得到 M 个 决策树 将 新 
数据 投入 到这 M 个 树 中 得到 M 个 
分类 结果 计数 看 预测 成哪/nr 一类 的 数目 最多 
就 将此 类别 作为 最后 的 预测 结果 3 . 
逻辑 回归 视频 当 预测 目标 是 概率 这样 的 
值域 需要 满足 大于 等于 0 小于 等于 1 的 
这个 时候 单纯 的 线性 模型 是 做 不到 的 
因为 在 定义域 不在 某个 范围 之内 时 值域 也 
超出 了 规定 区间 所以 此时 需要 这样 的 形状 
的 模型 会 比较 好 那么 怎么 得到 这样 的 
模型 呢 这个 模型 需要 满足 两个 条件 大于 等于 
0 小于 等于 1 大于 等于 0 的 模型 可以 
选择 绝对值 平方 值 这里 用 指数函数 一定 大于 0 
小于 等于 1 用 除法 分子 是 自己 分母 是 
自身 加上 1 那 一定 是 小于 1 的 了 
再做 一下 变形 就 得到 了 logistic regression 模型 通过 
源 数据 计算 可以 得到 相应 的 系数 了 最后 
得到 logistic 的 图形 4 . SVM 视频 support vector 
machine 要将 两类 分开 想 要得 到 一个 超平面 最优 
的 超平面 是 到 两类 的 margin 达到 最大 margin 
就是 超平面 与 离 它 最近 一点 的 距离 如 
下图 Z2 Z1 所以 绿色 的 超平面 比较好 将 这个 
超平面 表示 成 一个 线性方程 在线 上方 的 一类 都 
大于 等于 1 另一类 小于 等于 － 1 点到面 的 
距离 根据 图中 的 公式 计算 所以 得到 total margin 
的 表达式 如下 目标 是 最大化 这个 margin 就 需要 
最小化 分母 于是 变成 了 一个 优化 问题 举个 栗子 
三个点 找到 最优 的 超平面 定义 了 weight vector ＝ 
2 3 － 1 1 得到 weight vector 为 a 
2a 将 两个 点 代入 方程 代入 2 3 另 
其 值 ＝ 1 代入 1 1 另 其 值 
＝ 1 求 解出 a 和 截 矩 w0 的 
值 进而 得到 超平面 的 表达式 a 求 出来 后 
代入 a 2a 得到 的 就是 support vectora 和 w0 
代入 超平面 的 方程 就是 support vector machine5 . 朴素 
贝叶斯 视频 举个 在 NLP 的 应用 给 一段 文字 
返回 情感 分类 这段 文字 的 态度 是 positive 还是 
negative 为了 解决 这个 问题 可以 只看 其中 的 一些 
单词 这段 文字 将 仅 由 一些 单词 和 它们 
的 计数 代表 原始 问题是 给 你 一句话 它 属于 
哪 一类 通过 bayes rules 变成 一个 比较 简单 容易 
求得 的 问题 问题 变成 这一类 中 这句话 出现 的 
概率 是 多少 当然 别忘了 公式 里 的 另外 两个 
概率 栗子 单词 love 在 positive 的 情况 下 出现 
的 概率 是 0.1 在 negative 的 情况 下 出现 
的 概率 是 0.0016 . K 最 近邻 视频 k 
nearest neighbours 给 一个 新的 数据 时 离 它 最近 
的 k 个 点中 哪个 类别 多 这个 数据 就 
属于 哪 一类 栗子 要 区分 猫 和 狗 通过 
claws 和 sound 两个 feature 来 判断 的话 圆形 和 
三角形 是 已知 分类 的 了 那么 这个 star 代表 
的 是 哪 一类 呢 k ＝ 3时 这三条 线 
链接 的 点 就是 最近 的 三个点 那么 圆形 多一些 
所以 这个 star 就是 属于 猫 7 . K 均值 
视频 想 要将 一组 数据 分为 三类 粉色 数值 大 
黄色 数值 小 最 开心 先 初始化 这 里面 选 
了 最简单 的 3 2 1 作为 各类 的 初始值 
剩下 的 数据 里 每个 都与 三个 初始值 计算 距离 
然后 归类 到 离 它 最近 的 初始值 所在 类别 
分好 类 后 计算 每 一类 的 平均值 作为 新 
一轮 的 中心点 几轮 之后 分组 不再 变化 了 就 
可以 停止 了 8 . Adaboost 视频 adaboost 是 bosting 
的 方法 之一 bosting 就是 把 若干 个 分类 效果 
并 不好 的 分类器 综合 起来 考虑 会 得到 一个 
效果 比较 好 的 分类器 下图 左右两个 决策树 单个 看 
是 效果 不怎么 好 的 但是 把 同样 的 数据 
投入 进去 把 两个 结果 加 起来 考虑 就 会 
增加 可信度 adaboost 的 栗子 手写识别 中 在 画板 上 
可以 抓取 到 很多 features 例如 始点 的 方向 始点 
和 终点 的 距离 等等 training 的 时候 会 得到 
每个 feature 的 weight 例如 2 和 3 的 开头 
部分 很像 这个 feature 对 分类 起到 的 作用 很小 
它 的 权重 也 就会 较小 而 这个 alpha 角 
就 具有 很强 的 识别性 这个 feature 的 权重 就会 
较大 最后 的 预测 结果 是 综合 考虑 这些 feature 
的 结果 9 . 神经 网络 视频 Neural Networks 适合 
一个 input 可能 落入 至少 两个 类别 里 NN 由 
若干层 神经元 和 它们 之间 的 联系 组成 第一层 是 
input 层 最后 一层 是 output 层 在 hidden 层 
和 output 层/q 都有/nr 自己/r 的/uj classifierinput 输入 到 网络 
中 被 激活 计算 的 分数 被 传递 到 下一层 
激活 后面 的 神经 层 最后 output 层 的 节点 
上 的 分数 代表 属于 各类 的 分数 下图 例子 
得到 分类 结果 为 class 1 同样 的 input 被 
传输 到 不同 的 节点 上 之所以 会 得到 不同 
的 结果 是 因为 各自 节点 有 不同 的 weights 
和 bias 这 也就是 forward propagation10 . 马尔可夫 视频 Markov 
Chains 由 state 和 transitions 组成 栗子 根据 这 一句话 
the quick brown fox jumps over the lazy dog 要 
得到 markov chain 步骤 先给 每 一个 单词 设定 成 
一个 状态 然后 计算 状态 间 转换 的 概率 这是 
一句话 计算 出来 的 概率 当 你 用 大量 文本 
去做 统计 的 时候 会 得到 更大 的 状态 转移 
矩阵 例如 the 后面 可以 连接 的 单词 及 相应 
的 概率 生活 中 键盘 输入法 的 备选 结果 也 
是 一样 的 原理 模型 会 更 高级 