MNIST 是 一个 入门级 的 计算机 视觉 数据集 它 包含 
各种 手写 数字 图片 1 . MNIST 数据集 MNIST 是不是 
听起来 特 高端 大气 不 知道 这个 是 什么 东西 
= = 手写 数字 分类 问题 所要 用到 的 经典 
MNIST 数据集 = = MNIST 数据集 的 官网 是 Yann 
LeCun s website 自动 下载 和 安装 这个 数据集 的 
python 代码 该段 代码 在 tensorflow / examples / tutorials 
/ mnist / input _ data . py Functions for 
downloading and reading MNIST data . from _ _ future 
_ _ import absolute _ import from _ _ future 
_ _ import division from _ _ future _ _ 
import print _ function import gzip import os import tempfile 
import numpy from six . moves import urllib from six 
. moves import xrange # pylint disable = redefined builtin 
import tensorflow as tf from tensorflow . contrib . learn 
. python . learn . datasets . mnist import read 
_ data _ sets 导入 项目 import tensorflow . examples 
. tutorials . mnist . input _ data import input 
_ data mnist = input _ data . read _ 
data _ sets MNIST _ data / one _ hot 
= True 2 . 运行 TensorFlow 的 I n t 
e r a c t i v e e s 
s i o n 使用 TensorFlow 之前 首先 导入 它 
import tensorflow as tf sess = tf . I n 
t e r a c t i v e e 
s s i o n 3 . 计 算图 为了 
在 Python 中 进行 高效 的 数值 计算 我们 通常 
会 使用 像 NumPy 一类 的 库 将 一些 诸如 
矩阵 乘法 的 耗时 操 作在 Python 环境 的 外部 
来 计算 这些 计算 通常 会 通过 其它 语言 并用 
更为 高效 的 代码 来 实现 但 遗憾 的 是 
每一个 操作 切 换回 Python 环境 时仍/nr 需要 不小 的 
开销 如果 你 想在 GPU 或者 分布式 环境 中 计算 
时 这一 开销 更加 可怖 这一 开销 主要 可能 是 
用来 进行 数据 迁移 TensorFlow 也是 在 Python 外部 完成 
其 主要 工作 但是 进行 了 改进 以 避免 这种 
开销 其 并 没有 采用 在 Python 外部 独立 运行 
某个 耗时 操作 的 方式 而是 先 让 我们 描述 
一个 交互 操 作图 然后 完全 将 其 运行 在 
Python 外部 这与 Theano 或 Torch 的 做法 类似 因此 
Python 代码 的 目的 是 用来 构建 这个 可以 在 
外部 运行 的 计算 图 以及/c 安排/v 计/n 算图/i 的/uj 
哪一/i 部分/n 应/v 该被/i 运行/v 详情请 查看 基本 用法 中的 
计算 图表 一节 4 . 实现 softmax 回归模型 4.1 占位符 
我们 通过 为 输入 图像 x 和 目标 输出 类别 
y _ 创建 节点 来 开始 构建 计算 图 我们 
通过 操作 符号 变量 来 描述 这些 可 交互 的 
操作 单元 可以 用 下面 的 方式 创建 一个 x 
= tf . placeholder float shape = None 784 y 
_ = tf . placeholder float shape = None 10 
x 不是 一个 特定 的 值 而是 一个 占位符 placeholder 
我们 在 TensorFlow 运行 计算 时 输入 这个 值 我们 
希望 能够 输入 任意 数量 的 MNIST 图像 每 一张 
图 展 平成 784 维 的 向量 我们 用 2 
维 的 浮点数 张量 来 表示 这些 图 这个 张量 
的 形状 是 None 784 这里 的 None 表示 此 
张量 的 第一 个 维度 可以 是 任何 长度 的 
输出 类别 值 y _ 也 是 一个 2 维 
张量 其中 每 一行 为 一个 10 维 的 one 
hot 向量 用于 代表 对应 某一 MNIST 图片 的 类别 
虽然 placeholder 的 shape 参数 是 可选 的 但 有了 
它 TensorFlow 能够 自动 捕捉 因 数据 维度 不一致 导致 
的 错误 4.2 变量 我们 的 模型 也 需要 权重 
值 和 偏置 量 当然 我们 可以 把 它们 当做 
是 另外 的 输入 使用 占位符 但 TensorFlow 有 一个 
更好 的 方法 来 表示 它们 Variable 一个 Variable 代表 
一个 可 修改 的 张量 存在 在 TensorFlow 的 用于 
描述 交互性 操作 的 图中 它们 可以 用 于 计算 
输入 值 也 可以 在 计算 中 被 修改 对于 
各种 机器学习 应用 一般 都会 有 模型 参数 可以 用 
Variable 表示 W = tf . Variable tf . zeros 
784 10 b = tf . Variable tf . zeros 
10 我们 在 调用 tf . Variable 的 时候 传入 
初始值 在 这个 例子 里 我们/r 把/p W/w 和b都/nr 初始化/l 
为零/i 向量/n W 是 一个 784x10 的 矩阵 因为/c 我们/r 
有/v 784个/mq 特征/n 和/c 10个/mq 输出/v 值/n b 是 一个 
10 维 的 向量 因为 我们 有 10个 分类 变量 
需要 通过 seesion 初始化 后 才能 在 session 中 使用 
这一 初始 化步骤 为 为 初始值 指定 具体 值 本例 
当中 是 全 为零 并将 其 分配 给 每个 变量 
可以 一次性 为 所有 变量 完成 此 操作 sess . 
run tf . initialize _ all _ variables 5 . 
类别 预测 现在 我们 可以 实现 我们 的 模型 啦 
只需要 一行 代码 计算 每个 分类 的 softmax 概率值 y 
= tf . nn . softmax tf . matmul x 
W + b tf . matmul X W 表示 x 
乘以 W 对应 之前 等式 里面 的 Wx 这里 x 
是 一个 2 维 张量 拥有 多个 输入 然后 再 
加上 b 把 和 输入 到 tf . nn . 
softmax 函数 里面 为了 训练 我们 的 模型 我们 首先 
需要 定义 一个 指标 来 评估 这个 模型 是 好 
的 其实 在 机器学习 我们 通常 定义 指标 来 表示 
一个 模型 是 坏 的 这个 指标 称为 成本 cost 
或 损失 loss 然后 尽量 最小化 这个 指标 但是 这 
两种 方式 是 相同 的 一个 非常 常见 的 非常 
漂亮 的 成本 函数 是 交叉 熵 cross entropy 交叉 
熵 产生于 信息论 里面 的 信息 压缩 编码 技术 但是 
它 后来 演变 成为 从 博弈 论到 机器学习 等 其他 
领域 里 的 重要 技术 手段 它 的 定义 如下 
交叉 熵 是 用来 衡量 我们 的 预测 用于 描述 
真相 的 低效 性 交叉 熵 根据 公式 计算 交叉 
熵 可以 很 容易 的 为 训练 过程 指定 最小化 
误差 用 的 损失 函数 我们 的 损失 函数 是 
目标 类别 和 预测 类别 之间 的 交叉 熵 cross 
_ entropy = tf . reduce _ sum y _ 
* tf . log y 首先 用 tf . log 
计算 y 的 每个 元素 的 对数 接下来 我们 把 
y _ 的 每 一个 元素 和 tf . log 
y 的 对应 元素 相乘 最后 用 tf . reduce 
_ sum 计算 张量 的 所有 元素 的 总和 注意 
这里 的 交叉 熵 不仅仅 用来 衡量 单一 的 一对 
预测 和 真实 值 而是 所有 100幅 图片 的 交叉 
熵 的 总和 对于 100 个数 据点 的 预测 表现 
比 单一 数 据点 的 表现 能 更好 地 描述 
我们 的 模型 的 性能 6 . 训练 模型 现在 
我们 知道 我们 需要 我们 的 模型 做什么 啦 用 
TensorFlow 来 训练 它 是 非常 容易 的 因为 TensorFlow 
拥有 一张 描述 你 各个 计算 单元 的 图 它 
可以 自动 地 使用 反向 传播 算法 backpropagation algorithm 来 
有效 地 确定 你 的 变量 是 如何 影响 你 
想要 最小化 的 那个 成 本值 的 然后 TensorFlow 会用 
你 选择 的 优化 算法 来 不断 地 修改 变量 
以 降低成本 train _ step = tf . train . 
G r a d i e n t D e 
s c e n t O p t i m 
i z e r 0.01 . minimize cross _ entropy 
在 这里 我们 要求 TensorFlow 用 梯度 下降 算法 gradient 
descent algorithm 以 0.01 的 学习 速率 最小化 交叉 熵 
梯度 下降 算法 gradient descent algorithm 是 一个 简单 的 
学习 过程 TensorFlow 只需 将 每个 变量 一点点 地 往 
使 成本 不断 降低 的 方向 移动 当然 TensorFlow 也 
提供 了 其他 许多 优化 算法 只要 简单 地 调整 
一行 代码 就 可以 使用 其他 的 算法 TensorFlow 在 
这里 实际上 所做 的 是 它 会在 后台 给 描述 
你 的 计算 的 那张 图 里面 增加 一 系列 
新 的 计算 操作 单元 用于 实现 反向 传播 算法 
和 梯度 下降 算法 然后 它 返回 给 你 的 
只是 一个 单一 的 操作 当 运行 这个 操作 时 
它 用 梯度 下降 算法 训练 你 的 模型 微调 
你 的 变量 不断 减少 成本 然后 开始 训练 模型 
这里 我们 让 模型 循环 训练 1000次 for i in 
range 1000 batch _ xs batch _ ys = mnist 
. train . next _ batch 50 sess . run 
train _ step feed _ dict = { x batch 
_ xs y _ batch _ ys } 另一种 代码 
写法 for i in range 1000 batch = mnist . 
train . next _ batch 50 train _ step . 
run feed _ dict = { x batch 0 y 
_ batch 1 } 该 循环 的 每个 步骤 中 
我们 都会/nr 随机 抓取 训练 数据 中的 50个 批处理 数 
据点 然后 我们 用 这些 数据 点 作为 参数 替换 
之前 的 占位符 来 运行 train _ step 使用 一 
小 部分 的 随机 数据 来 进行 训练 被称为 随机 
训练 stochastic training 在 这里 更 确切 的 说是 随机 
梯度 下降 训练 在 理想 情况下 我们 希望 用 我们 
所有 的 数据 来 进行 每一步 的 训练 因为 这 
能给/nr 我们 更好 的 训练 结果 但 显然 这 需要 
很大 的 计算 开销 所以 每一次 训练 我们 可以 使用 
不同 的 数据 子集 这样 做 既 可以 减少 计算 
开销 又 可以 最大化 地 学习 到 数据集 的 总体 
特性 7 . 评估 我们 的 模型 首先 让 我们 
找出 那些 预测 正确 的 标签 tf . argmax 是 
一个 非常 有用 的 函数 它 能 给出 某个 tensor 
对象 在 某一 维 上 的 其 数据 最大值 所在 
的 索引 值 由于 标签 向量 是由 0 1 组成 
因此 最大值 1 所在 的 索引 位置 就是 类别 标签 
比如 tf . argmax y 1 各个 预测 数字 中 
概率 最大 的 那一个 而 tf . argmax y _ 
1 代表 正确 的 标签 我们 可以 用 tf . 
equal 来 检测 我们 的 预测 是否 真实 标签 匹配 
索引 位置 一样 表示 匹配 correct _ prediction = tf 
. equal tf . argmax y 1 tf . argmax 
y _ 1 这行 代码 会 给 我们 一组 布尔值 
为了 确定 正确 预测 项的/nr 比例 我们 可以 把 布尔值 
转换成 浮点数 然后 取 平均值 例如 True False True True 
会 变成 1 0 1 1 取 平均值 后 得到 
0.75 . accuracy = tf . reduce _ mean tf 
. cast correct _ prediction float 最后 我们 计算所 学习 
到 的 模型 在 测试 数据集 上面 的 正确率 print 
sess . run accuracy feed _ dict = { x 
mnist . test . images y _ mnist . test 
. labels } # 输出 结果 0.90928 . 代码 from 
tensorflow . examples . tutorials . mnist import input _ 
data mnist = input _ data . read _ data 
_ sets MNIST _ data / one _ hot = 
True print mnist . train . images . shape mnist 
. train . labels . shape print mnist . test 
. images . shape mnist . test . labels . 
shape print mnist . validation . images . shape mnist 
. validation . labels . shape import tensorflow as tf 
sess = tf . I n t e r a 
c t i v e e s s i o 
n x = tf . placeholder tf . float32 None 
784 W = tf . Variable tf . zeros 784 
10 b = tf . Variable tf . zeros 10 
y = tf . nn . softmax tf . matmul 
x W + b y _ = tf . placeholder 
tf . float32 None 10 cross _ entropy = tf 
. reduce _ mean tf . reduce _ sum y 
_ * tf . log y reduction _ indices = 
1 train _ step = tf . train . G 
r a d i e n t D e s 
c e n t O p t i m i 
z e r 0.5 . minimize cross _ entropy tf 
. global _ variables _ initializer . run for i 
in range 1000 batch _ xs batch _ ys = 
mnist . train . next _ batch 100 train _ 
step . run { x batch _ xs y _ 
batch _ ys } correct _ prediction = tf . 
equal tf . argmax y 1 tf . argmax y 
_ 1 accuracy = tf . reduce _ mean tf 
. cast correct _ prediction tf . float32 print accuracy 
. eval { x mnist . test . images y 
_ mnist . test . labels } 