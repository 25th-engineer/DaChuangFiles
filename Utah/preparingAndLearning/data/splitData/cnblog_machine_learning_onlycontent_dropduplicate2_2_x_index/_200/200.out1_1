一 神经 网络 基础 1 . 神经元 模型 神经 网络 
中 最 基本 的 单元 是 神经元 模型 neuron 细胞体 
分为 两部分 前 一 部分 计算 总 输入 值 即 
输入 信号 的 加权 和 或者说 累积 电平 后一/nr 部分 
先 计算 总 输入 值 与 该 神经元 阈值 的 
差值 然后 通过 激活 函数 activation function 的 处理 产生 
输出 从 轴突 传送 给 其它 神经元 M P 神经元 
模型 如下 图 所示 2 . 激活 函数 与 线性 
分类 十分相似 神经元 模型 最 理想 的 激活 函数 也是 
阶跃 函数 即将 神经元 输入 值 与 阈值 的 差值 
映射 为 输出 值 1 或 0 若 差值 大于 
零 输出 1 对应 兴奋 若 差值 小于 零 则 
输出 0 对应 抑制 但 阶跃 函数 不连续 不光滑 故在 
M P 神经元 模型 中 也 采用 Sigmoid 函 数来 
近似 Sigmoid 函数 将 较大 范围 内 变化 的 输入 
值 挤 压到 0 1 输出 值 范围内 所以 也 
称为 挤压 函数 squashing function 更多 激活 函数 参考 https 
/ / www . jianshu . com / p / 
22d9720dbf1a3 . 感知机 Perceptron 感知机 Perceptron 是由 两层 神经元 组成 
的 一个 简单 模型 但 只有 输出 层 是 M 
P 神经元 即 只有 输出 层 神经元 进行 激活 函数 
处理 也 称为 功能 神经元 functional neuron 输入 层 只是 
接受 外界 信号 样本 属性 并 传递 给 输出 层 
输入 层 的 神经元 个数 等于 样本 的 属性 数目 
而 没有 激活 函数 这样一来 感知机 与 之前 线性 模型 
中的 对数 几率 回归 的 思想 基本 是 一样 的 
都是/nr 通过 对 属性 加权 与 另一个 常数 求和 再使用 
sigmoid 函数 将 这个 输出 值 压缩 到 0 1 
之间 从而 解决 分类 问题 不同 的 是 感知机 的 
输出 层 应该 可以 有 多个 神经元 从而 可以 实现 
多 分类 问题 同时 两个 模型 所用 的 参数估计 方法 
十分 不同 感知机 权重 学习 过程 感知机 的 学习 采用 
随机 梯度 下降 算法 SGD 该 算法 的 说明 可以 
参考 http / / www . cnblogs . com / 
NeilZhang / p / 8454890 . html 其中 η ∈ 
0 1 称为 学习率 可以 看出 感知机 是 通过 逐个 
样本 输入 来 更新 权重 首先 设定 好 初始 权重 
一般 为 随机 逐个 地 输入 样本数据 若 输出 值 
与 真实 标记 相同 则 继续 输入 下一个 样本 若不 
一致 则 更新 权重 然后 再 重新 逐个 检验 直到 
每个 样本数据 的 输出 值 都与 真实 标记 相同 容易 
看出 感知机 模型 总是 能将 训练 数据 的 每一个 样本 
都 预测 正确 和/c 决策树/n 模型/n 总是/c 能将/nr 所有/b 训练/vn 
数据/n 都/d 分开/v 一样/r 感知机 模型 很容易 产生 过拟合 问题 
局限性 感知机 只有 输出 层 神经元 进行 激活 函数 处理 
即 只 拥有 一层 功能 神经元 其 学习 能力 非常 
有限 可以 证明 若 二类 模式 是 线性 可分 的 
即 存在 一个 线性 超平面 能将 他们 分开 则 感知机 
的 学习 一定 会 收敛 converge 而 求得 适当 的 
权向量 w = w1 w2 w3 . . 否则 感知机 
学习 过程 将 会 发生 震荡 w 难以 稳定下来 不能 
求得 合适 解 要 解决 非线性 可分 问题 需要 考虑 
使用 多层 功能 神经元 即 神经网络 神经 网络 发展 史上 
经典 问题 异或 问题 单层 网络 不能 解决 4 . 
神经网络 多层 神经 网络 的 拓扑结构 如下 图 所示 在 
神经 网络 中 输入 层 与 输出 层 之间 的 
层 称为 隐含 层 或 隐 层 hidden layer 隐 
层 和 输出 层 的 神经元 都是 具有 激活 函数 
的 功能 神经元 只需 包含 一个 隐 层 便 可以 
称为 多层 神经网络 常用 的 神经 网络 称为 多层 前馈 
神经网络 multi layer feedforward neural network 该 结构 满足 以下 
几个 特点 * 每层 神经元 与 下一层 神经元 之间 完全 
互连 * 神经元 之间 不 存在 同层 连接 * 神经元 
之间 不 存在 跨 层 连接 根据 上面 的 特点 
可以 得知 这里 的 前馈 指 的 是 网络 拓扑结构 
中 不存在 环 或 回路 而 不是 指 该 网络 
只能 向前 传播 而 不能 向后 传播 二 误差 逆 
传播 算法 BP 神经网络 算法 神经 网络 的 学习 主要 
蕴含 在 权 重和 阈值 中 多层 网络 使用 上面 
简单 感知机 的 权重 调整 规则 显然 不够 用了 BP 
神经网络 算法 即 误差 逆 传播 算法 error BackPropagation 正是 
为 学习 多层 前馈 神经网络 而 设计 BP 神经网络 算法 
是 迄今为止 最 成功 的 的 神经 网络 学习 算法 
上图 的 网络 中有 d + l + 1 * 
q + l 个 参数 需要 确定 输入 层 到 
隐 层 的 d × q 个 权重 隐 层 
到 输出 层 q × l 个 权重 q 个 
隐 层 神经元 的 阈值 l 个 输出 神经元 的 
阈值 上图 为 一个 单隐层/nr 前馈 神经 网络 的 拓扑结构 
BP 神经网络 算法 也 使用 梯度 下 降法 gradient descent 
以 单个 样本 的 均方 误差 的 负 梯度方向 对 
权重 进行 调节 sgd 算法 学习率 η ∈ 0 1 
控制 着 沿 反 梯度方向 下降 的 步长 若 步长 
太大 则 下降 太快 容易 产生 震荡 若 步长 太 
小则 收敛 速度 太慢 一般地 常把 η 设置 为 0.1 
有时 更新 权重 时 会将 输出 层 与 隐含 层 
设置 为 不同 的 学习率 可以 看出 BP 算法 首先 
将 误差 反向 传播给 隐 层 神经元 调节 隐 层 
到 输出 层 的 连接 权重 与 输出 层 神经元 
的 阈值 接着 根据 隐含 层 神经元 的 均方 误差 
来 调节 输入 层 到 隐含 层 的 连接 权值 
与 隐含 层 神经元 的 阈值 BP 算法 基本 的 
推导 过程 与 感知机 的 推导 过程 原理 是 相同 
的 下面 给出 调整 隐含 层 到 输出 层 的 
权重 调整 规则 的 推导 过程 1 . 误差函数 2 
. 具体 推导 过程 其它 参数 的 推算 过程 相似 
参考 机器学习 中 神经 网络 的 介绍 上述 算法 的 
推导 是 基于 每次 仅 针对 一个 训练 样例 更新 
权 重和 阈值 标准 BP 算法 这种 算法 参数 更新 
十分 频繁 可能 会 出现 抵消 现象 累计 BP 算法 
针对 累计 误差 最小化 每次 读取 整个 数据集 一遍 后 
才对 参数 进行 更新 其 参数 更新 的 频率 低 
得多 3 . 过拟合 BP 神经网络 强大 的 学习 能力 
常常 容易 造成 过拟合 问题 有 以下 两种 策略 来 
缓解 BP 网络 的 过拟合 问题 早 停 将 数据 
分为 训练 集 与 测试 集 训练 集 用于 学习 
测试 集 用于 评估 性能 若在 训练 过程 中 训练 
集 的 累积 误差 降低 而 测试 集 的 累积 
误差 升高 则 停止 训练 引入 正则化 regularization 基本 思想 
是 在 累积 误差函数 中 增加 一个 用于 描述 网络 
复杂度 的 部分 例如 所有 权值 与 阈值 的 平方和 
其中 λ ∈ 0 1 用于 对 累积 经验 误差 
与 网络 复杂度 这两项 进行 折中 常 通过 交叉 验证法 
来 估计 4 . 全局 最小 与 局部 最小 要 
成为 局部 极 小点 只要 满足 该 点在 参数 空间 
中 的 梯度 为零 局部 极小 可以 有 多个 而 
全局 最小 只有 一个 全局 最小 一定 是 局部 极小 
但 局部 最小 却 不一定 是 全局 最小 显然 在 
很多 机器学习 算法 中 都 试图 找到 目标函数 的 全局 
最小 梯度 下 降法 的 主要 思想 就是 沿着 负 
梯度方向 去 搜索 最优 解 负 梯度方向 是 函数值 下降 
最快 的 方向 若 迭代 到某处 的 梯度 为 0 
则 表示 达到 一个 局部 最小 参数 更新 停止 因此 
在 现实 任务 中 通常 使用 以下 策略 尽可能 地 
去 接近 全局 最小 * 局部 极小解 参数 空间 中 
的 某个 点 其 邻域 点 的 误差 函数值 均 
不小于 该点 的 误差 函数值 * 全局 最 小解 参数 
空间 中 的 某个 点 所有 其他 点 的 误差 
函数值 均 不小于 该点 的 误差 函数值 跳出 局部 最小 
的 方法 以 多组 不同 参数值 初始化 多个 神经网络 按 
标准 方法 训练 迭代 停止 后 取 其中 误差 最小 
的 解 作为 最终 参数 使用 模拟退火 技术 使用 随机 
梯度 下降 即在 计算 梯度 时 加入 了 随机因素 使得 
在 局部 最 小时 计算 的 梯度 仍 可能 不为 
0 从而 迭代 可以 继续 进行 三 神经网络 可视化 推荐 
一个 在线 测试 神经 网络 的 网站 http / / 
playground . tensorflow . org / 下图 为 上述 网站 
通过 两个 神经元 解决 异或 问题 其它 机器学习 算法 监督 
学习 随机 梯度 下降 算法 sgd 和批/nr 梯度 下降 算法 
bgd 监督 学习 决策树 理论 与 实践 上 分类 决策树 
监督 学习 决策树 理论 与 实践 下 回归 决策树 CART 
监督 学习 K 邻近 算法 及 数字 识别 实践 监督 
学习 朴素 贝叶斯 分类理论 与 实践 监督 学习 logistic 进行 
二 分类 python 监督 学习 AdaBoost 元 算法 提高 分类 
性能 无 监督 学习 K 均值 聚 类 算法 对 
未 标注 数据分组 参考 机器学习 周志华 激活 函数 https / 
/ blog . csdn . net / u011826404 / article 
/ details / 53767428 随机 梯度 下降 与 批 梯度 
下降 http / / www . cnblogs . com / 
NeilZhang / p / 8454890 . html 