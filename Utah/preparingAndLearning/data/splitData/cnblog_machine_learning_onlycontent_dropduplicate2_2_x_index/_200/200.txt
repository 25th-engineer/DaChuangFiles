一、神经网络基础
1. 神经元模型
神经网络中最基本的单元是神经元模型（neuron）。
细胞体分为两部分，前一部分计算总输入值（即输入信号的加权和，或者说累积电平），后一部分先计算总输入值与该神经元阈值的差值，然后通过激活函数（activation function）的处理，产生输出从轴突传送给其它神经元。M-P神经元模型如下图所示：
2. 激活函数
与线性分类十分相似，神经元模型最理想的激活函数也是阶跃函数，即将神经元输入值与阈值的差值映射为输出值1或0，若差值大于零输出1，对应兴奋；若差值小于零则输出0，对应抑制。但阶跃函数不连续，不光滑，故在M-P神经元模型中，也采用Sigmoid函数来近似， Sigmoid函数将较大范围内变化的输入值挤压到 (0,1) 输出值范围内，所以也称为挤压函数（squashing function）。
更多激活函数参考： https://www.jianshu.com/p/22d9720dbf1a
3. 感知机（Perceptron）
感知机（Perceptron）是由两层神经元组成的一个简单模型，但只有输出层是M-P神经元，即只有输出层神经元进行激活函数处理，也称为功能神经元（functional neuron）；输入层只是接受外界信号（样本属性）并传递给输出层（输入层的神经元个数等于样本的属性数目），而没有激活函数。这样一来，感知机与之前线性模型中的对数几率回归的思想基本是一样的，都是通过对属性加权与另一个常数求和，再使用sigmoid函数将这个输出值压缩到0-1之间，从而解决分类问题。不同的是感知机的输出层应该可以有多个神经元，从而可以实现多分类问题，同时两个模型所用的参数估计方法十分不同。
感知机权重学习过程
感知机的学习采用随机梯度下降算法（SGD）该算法的说明可以参考：http://www.cnblogs.com/NeilZhang/p/8454890.html
其中 η∈（0，1）称为学习率，可以看出感知机是通过逐个样本输入来更新权重，首先设定好初始权重（一般为随机），逐个地输入样本数据，若输出值与真实标记相同则继续输入下一个样本，若不一致则更新权重，然后再重新逐个检验，直到每个样本数据的输出值都与真实标记相同。容易看出：感知机模型总是能将训练数据的每一个样本都预测正确，和决策树模型总是能将所有训练数据都分开一样，感知机模型很容易产生过拟合问题。
局限性：
感知机只有输出层神经元进行激活函数处理，即只拥有一层功能神经元，其学习能力非常有限。 可以证明若二类模式是线性可分的，即存在一个线性超平面能将他们分开，则感知机的学习一定会收敛（converge）而求得适当的权向量w = （w1，w2,w3…..）; 否则感知机学习过程将会发生震荡，w难以稳定下来，不能求得合适解。
要解决非线性可分问题需要考虑使用多层功能神经元，即神经网络。（神经网络发展史上经典问题：异或问题单层网络不能解决）
4.神经网络
多层神经网络的拓扑结构如下图所示：
在神经网络中，输入层与输出层之间的层称为隐含层或隐层（hidden layer），隐层和输出层的神经元都是具有激活函数的功能神经元。只需包含一个隐层便可以称为多层神经网络，常用的神经网络称为“多层前馈神经网络”（multi-layer feedforward neural network），该结构满足以下几个特点：
* 每层神经元与下一层神经元之间完全互连
* 神经元之间不存在同层连接
* 神经元之间不存在跨层连接
根据上面的特点可以得知：这里的“前馈”指的是网络拓扑结构中不存在环或回路，而不是指该网络只能向前传播而不能向后传播
二、误差逆传播算法（BP神经网络算法）
神经网络的学习主要蕴含在权重和阈值中，多层网络使用上面简单感知机的权重调整规则显然不够用了，BP神经网络算法即误差逆传播算法（error BackPropagation）正是为学习多层前馈神经网络而设计，BP神经网络算法是迄今为止最成功的的神经网络学习算法。
上图的网络中有(d+l+1)*q+l个参数需要确定：输入层到隐层的d×q个权重，隐层到输出层q×l个权重、q个隐层神经元的阈值、l个输出神经元的阈值。
上图为一个单隐层前馈神经网络的拓扑结构，BP神经网络算法也使用梯度下降法（gradient descent），以单个样本的均方误差的负梯度方向对权重进行调节（sgd算法）。
学习率η∈（0，1）控制着沿反梯度方向下降的步长，若步长太大则下降太快容易产生震荡，若步长太小则收敛速度太慢，一般地常把η设置为0.1，有时更新权重时会将输出层与隐含层设置为不同的学习率。
可以看出：BP算法首先将误差反向传播给隐层神经元，调节隐层到输出层的连接权重与输出层神经元的阈值；接着根据隐含层神经元的均方误差，来调节输入层到隐含层的连接权值与隐含层神经元的阈值。BP算法基本的推导过程与感知机的推导过程原理是相同的，下面给出调整隐含层到输出层的权重调整规则的推导过程：
1. 误差函数
2. 具体推导过程
其它参数的推算过程相似，参考《机器学习》中神经网络的介绍。
上述算法的推导是基于每次仅针对一个训练样例更新权重和阈值（标准BP算法），这种算法参数更新十分频繁，可能会出现“抵消”现象。累计BP算法针对累计误差最小化，每次读取整个数据集一遍后才对参数进行更新，其参数更新的频率低得多。
3. 过拟合
BP神经网络强大的学习能力常常容易造成过拟合问题，有以下两种策略来缓解BP网络的过拟合问题：
早停：将数据分为训练集与测试集，训练集用于学习，测试集用于评估性能，若在训练过程中，训练集的累积误差降低，而测试集的累积误差升高，则停止训练。
引入正则化（regularization）：基本思想是在累积误差函数中增加一个用于描述网络复杂度的部分，例如所有权值与阈值的平方和，其中λ∈（0,1）用于对累积经验误差与网络复杂度这两项进行折中，常通过交叉验证法来估计。
4. 全局最小与局部最小
要成为局部极小点，只要满足该点在参数空间中的梯度为零。局部极小可以有多个，而全局最小只有一个。全局最小一定是局部极小，但局部最小却不一定是全局最小。显然在很多机器学习算法中，都试图找到目标函数的全局最小。梯度下降法的主要思想就是沿着负梯度方向去搜索最优解，负梯度方向是函数值下降最快的方向，若迭代到某处的梯度为0，则表示达到一个局部最小，参数更新停止。因此在现实任务中，通常使用以下策略尽可能地去接近全局最小。
* 局部极小解：参数空间中的某个点，其邻域点的误差函数值均不小于该点的误差函数值。
* 全局最小解：参数空间中的某个点，所有其他点的误差函数值均不小于该点的误差函数值。
跳出局部最小的方法：
以多组不同参数值初始化多个神经网络，按标准方法训练，迭代停止后，取其中误差最小的解作为最终参数。
使用“模拟退火”技术
使用随机梯度下降，即在计算梯度时加入了随机因素，使得在局部最小时，计算的梯度仍可能不为0，从而迭代可以继续进行。
三、神经网络可视化
推荐一个在线测试神经网络的网站：http://playground.tensorflow.org/
下图为上述网站通过两个神经元解决异或问题：
其它机器学习算法：
监督学习——随机梯度下降算法（sgd）和批梯度下降算法（bgd）
监督学习——决策树理论与实践（上）：分类决策树
监督学习——决策树理论与实践（下）：回归决策树（CART）
监督学习——K邻近算法及数字识别实践
监督学习——朴素贝叶斯分类理论与实践
监督学习——logistic进行二分类（python）
监督学习——AdaBoost元算法提高分类性能
无监督学习——K-均值聚类算法对未标注数据分组
参考：
《机器学习》 周志华
激活函数： https://blog.csdn.net/u011826404/article/details/53767428
随机梯度下降与批梯度下降： http://www.cnblogs.com/NeilZhang/p/8454890.html