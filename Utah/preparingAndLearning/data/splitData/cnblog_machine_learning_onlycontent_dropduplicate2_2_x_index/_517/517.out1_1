感谢 中国 人民 大学 胡鹤/nr 老师 课 讲得 非常好 ~ 
首先 何谓 tensor 即 高维 向量 例如 矩阵 是 二维 
tensor 是 更 广义 意义上 的 n 维 向量 有 
type + shape TensorFlow 执行 过程 为 定义 图 其中 
定 义子 节点 计算 时只/nr 计算 所需 节点 所 依赖 
的 节点 是 一种 高效 且 适应 大 规模 的 
数据 计算 方便 分布式 设计 对于 复杂 神经 网络 的 
计算 可 将其 拆开 到 其他 核 中 同时 计算 
Theano torch caffe 尤其 是 图像 处理 deeplearning5j H20 MXNet 
TensorFlow 运行 环境 下载 docker 打开 docker quickstart terminal 标 
红 地方 显示 该 docker 虚拟机 IP 地址 即 之后 
的 localhost docker tensorflow / tensorflow / / 自动 找到 
TensorFlow 容器 并 下载 docker images / / 浏览 当前 
容器 docker run p 8888 8888 tensorflow / tensorflow / 
/ 在 8888 端口 运行 会 出现 一个 token 复制 
该 链接 并 替换 掉 localhost 既 可以 打开 TensorFlow 
的 一个 编写 器 jupyter 大体 雏形 # python 导入 
import tensorflow as tf # 定义 变量 节点 x = 
tf . Variable 3 name = x y = tf 
. Variable 4 name = y f = x * 
x * y + y + 2 # 定义 session 
sess = tf . Session # 为 已经 定义 的 
节点 赋值 sess . run x . initializer sess . 
run y . initializer # 运行 session result = sess 
. run f print result # 42 # 释放 空间 
sess . close 还有 一个 更 简洁 的 一种 定义 
并 运行 session 方法 # a better way with tf 
. Session as sess x . initializer . run y 
. initializer . run # 即 evaluate 求解 f 的 
值 result = f . eval 初始化 的 两行 也 
可以 写作 init = tf . global _ variables _ 
initializer init . run 而 session 可以 改作 sess = 
tf . I n t e r a c t 
i v e e s s i o n 运行 
起来 更 方便 init = tf . global _ variables 
_ initializer sess = tf . I n t e 
r a c t i v e e s s 
i o n init . run result = f . 
eval print result 因而 TensorFlow 的 代码 分为 两部分 定义 
部分 和 执行 部分 TensorFlow 是 一个 图 的 操作 
有/v 自动/vn 缺省/n 的/uj 默认/v 图/n 和你/nr 自己/r 定义/n 的/uj 
图/n #/i 系统/n 默认/v 缺省/n 的/uj 图/n x1 = tf 
. Variable 1 x1 . graph is tf . get 
_ default _ graph True # 自定义 的 图 graph 
= tf . Graph with graph . as _ default 
x2 = tf . Variable 2 x2 . graph is 
graph True x2 . graph is tf . get _ 
default _ graph False 节点 的 生命 周期 第二 种 
方法 可以 找出 公共 部分 避免 x 被 计算 2次 
运行 结束 后 所有 节点 的 值 都被 清空 如果 
没有 单独 保存 还需 重新 run 一遍 w = tf 
. constant 3 x = w + 2 y = 
x + 5 z = x * 3 with tf 
. Session as sess print y . eval # 10 
print z . eval # 15 with tf . Session 
as sess y _ val z _ val = sess 
. run y z print y _ val # 10 
print z _ val # 15Linear Regression with TensorFlow 线性 
回归 上 的 应用 y = wx + b = 
wx / / 这里 x 是 相较 于x/nr 多了 一维 
全是 1 的 向量 这里 引用 California housing 的 数据 
TensorFlow 上 向量 是 列 向量 需要 reshape 1 1 
即 转置 成列 向量 使用 normal equation 方法 求解 import 
numpy as np from sklearn . datasets import fetch _ 
california _ housing housing = fetch _ california _ housing 
# 获得 数据 维度 矩阵 的 行列 长度 m n 
= housing . data . shape # np . c 
_ 是 连接 的 含义 加 了 一个 全为 1 
的 维度 housing _ data _ plus _ bias = 
np . c _ np . ones m 1 housing 
. data # 数据量 并不大 可以 直接 用 常量 节点 
装载 进来 但是 之后 海量 数据 无法 使用 会用 minbatch 
的 方式 导入 数据 X = tf . constant housing 
_ data _ plus _ bias dtype = tf . 
float32 name = X # 转置 成列 向量 y = 
tf . constant housing . target . reshape 1 1 
dtype = tf . float32 name = y XT = 
tf . transpose X # 使用 normal equation 的 方法 
求解 theta 之前 线性 模型 中 有 提及 theta = 
tf . matmul tf . matmul tf . matrix _ 
inverse tf . matmul XT X XT y # 求出 
权重 with tf . Session as sess theta _ value 
= theta . eval 如果 是 原本 的 方法 可能 
更 直接 些 但 由于 使用 底层 的 库 不同 
它们 计算 出来 的 值 不 完全 相同 # 使用 
numpy X = housing _ data _ plus _ bias 
y = housing . target . reshape 1 1 theta 
_ numpy = np . linalg . inv X . 
T . dot X . dot X . T . 
dot y # 使用 sklearn from sklearn . linear _ 
model import LinearRegression lin _ reg = LinearRegression lin _ 
reg . fit housing . data housing . target . 
reshape 1 1 这里 不禁 感到 疑惑 为什么 TensorFlow 感觉 
变 复杂 了 呢 其实 这 不过 因为 这里 数据 
规模 较小 进行 大 规模 的 计算 时 TensorFlow 的 
自动 优化 所 发挥 的 效果 是 十分 厉害 的 
使用 gradient descent 梯度 下降 方法 求解 # 使用 gradient 
时 需要 scale 一下 from sklearn . preprocessing import StandardScaler 
scaler = StandardScaler scaled _ housing _ data = scaler 
. fit _ transform housing . data scaled _ housing 
_ data _ plus _ bias = np . c 
_ np . ones m 1 scaled _ housing _ 
data # 迭代 1000次 n _ epochs = 1000 learning 
_ rate = 0.01 # 由于 使用 gradient 写入 x 
的 值 需要 scale 一下 X = tf . constant 
scaled _ housing _ data _ plus _ bias dtype 
= tf . float32 name = X y = tf 
. constant housing . target . reshape 1 1 dtype 
= tf . float32 name = y # 使用 gradient 
需要 有一个 初值 theta = tf . Variable tf . 
random _ uniform n + 1 1 1.0 1.0 name 
= theta # 当前 预测 的 y x 是 m 
* n + 1 theta 是 n + 1 * 
1 刚好 是 y 的 维度 y _ pred = 
tf . matmul X theta name = predictions # 整体 
误差 error = y _ pred y # TensorFlow 求解 
均值 功能强大 可以 指定 维数 也 可以 像 下面 方法 
求 整体 的 mse = tf . reduce _ mean 
tf . square error name = mse # 暂时 自己 
写出 训练 过程 实际 可以 采用 TensorFlow 自带 的 功能 
更 强大 的 自动 求解 autodiff 方法 gradients = 2 
/ m * tf . matmul tf . transpose X 
error training _ op = tf . assign theta theta 
learning _ rate * gradients # 初始化 并 开始 求解 
init = tf . global _ variables _ initializer with 
tf . Session as sess sess . run init for 
epoch in range n _ epochs # 每 运行 100次 
打印 一下 当前 平均误差 if epoch % 100 = = 
0 print Epoch epoch MSE = mse . eval sess 
. run training _ op best _ theta = theta 
. eval 上述 代码 中的 autodiff 如下 可以 自动 求出 
g r a d i e n t g r 
a d i e n t s = tf . 
gradients mse theta 0 使用 Optimizer 上述 的 整个 梯度 
下降 和 迭代 方法 都封/nr 装了/i 在/p 如下/t 方法/n 中/f 
optimizer/w = tf . train . G r a d 
i e n t D e s c e n 
t O p t i m i z e r 
learning _ rate = learning _ rate training _ op 
= optimizer . minimize mse 这样 的 optimizer 还有 很多 
例如 带 冲量 的 optimizer = tf . train . 
M o m e n t u m O p 
t i m i z e r learning _ rate 
= learning _ rate momentum = 0.9 Feeding data to 
training algorithm 当 数据 量 达到 几 G 几十 G 
时 使用 constant 直接 导入 数据 显然是 不 现实 的 
因而 我们 用 placeholder 做一个 占位符 一般 行 都是 none 
即 数据量 是 任意 的 真正 运行 run 的 时候 
再 feed 数据 可以 不断 使用 新 的 数据 A 
= tf . placeholder tf . float32 shape = None 
3 B = A + 5 with tf . Session 
as sess . . . B _ val _ 1 
= B . eval feed _ dict = { A 
1 2 3 } . . . B _ val 
_ 2 = B . eval feed _ dict = 
{ A 4 5 6 7 8 9 } . 
. . print B _ val _ 1 6 . 
7 . 8 . print B _ val _ 2 
9 . 10 . 11 . 12 . 13 . 
14 . 这样 就 可以 通过 定义 min _ batch 
来 分 批次 随机 抽取 指定 数量 的 数据 即 
便是 几 T 的 数据 也 可以 抽取 batch _ 
size = 100 n _ batches = int np . 
ceil m / batch _ size # 有 放回 的 
随机 抽取 数据 def fetch _ batch epoch batch _ 
index batch _ size # 定义 一个 随机 种子 np 
. random . seed epoch * n _ batches + 
batch _ index # not shown in the book indices 
= np . random . randint m size = batch 
_ size # not shown X _ batch = scaled 
_ housing _ data _ plus _ bias indices # 
not shown y _ batch = housing . target . 
reshape 1 1 indices # not shown return X _ 
batch y _ batch # 开始运行 with tf . Session 
as sess sess . run init # 每次 都 抽取 
新的 数据 做 训练 for epoch in range n _ 
epochs for batch _ index in range n _ batches 
X _ batch y _ batch = fetch _ batch 
epoch batch _ index batch _ size sess . run 
training _ op feed _ dict = { X X 
_ batch y y _ batch } # 最终 结果 
best _ theta = theta . eval Saving and Restoring 
models 保存 模型 有时候 运行 几天 的 模型 可能 因故 
暂时 无法 继续 跑 下去 因而 需要 暂时 保持 已 
训 练好 的 部分 模型 到 硬 盘上 init = 
tf . global _ variables _ initializer saver = tf 
. train . Saver # 保存 模型 with tf . 
Session as sess sess . run init for epoch in 
range n _ epochs if epoch % 100 = = 
0 # print Epoch epoch MSE = mse . eval 
save _ path = saver . save sess / tmp 
/ my _ model . ckpt sess . run training 
_ op best _ theta = theta . eval save 
_ path = saver . save sess / tmp / 
my _ model _ final . ckpt # 恢复 模型 
with tf . Session as sess saver . restore sess 
/ tmp / my _ model _ final . ckpt 
best _ theta _ restored = theta . eval 关于 
TensorBoard 众所周知 神经 网络 和 机器学习 大多 是 黑盒 模型 
让人 有点 忐忑 TensorBoard 所 起 的 功能 就是 将 
这个 黑盒 稍微 变白 一些 ~ 启用 tensorboard 输入 docker 
ps 查看 当前 容器 id 进入 容器 使用 tensorboard log 
dir = tf _ logs 命令 打开 已经 存入 的 
tf _ logs 文件 其 生成 代码 如下 所示 from 
datetime import datetime now = datetime . utcnow . strftime 
% Y % m % d % H % M 
% root _ logdir = tf _ logs logdir = 
{ } / run { } / . format root 
_ logdir now . . . mse _ summary = 
tf . summary . scalar MSE mse file _ writer 
= tf . summary . FileWriter logdir tf . get 
_ default _ graph . . . if batch _ 
index % 10 = = 0 summary _ str = 
mse _ summary . eval feed _ dict = { 
X X _ batch y y _ batch } step 
= epoch * n _ batches + batch _ index 
file _ writer . add _ summary summary _ str 
step 