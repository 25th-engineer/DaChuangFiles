原作 MSRA 刘铁 岩 著 分布式 机器学习 算法 理论 与 
实践 这 一部分 叙述 很 清晰 适合 用于 系统 整理 
NN 知识 线性 模型 线性 模型 是 最简单 的 也是 
最 基本 的 机器学习 模型 其 数学 形式 如下 g 
X W = WTX 有时 我们 还会在 WTX 的 基础 
上 额外 加入 一个 偏置 项b/nr 不过 只要 把 X 
扩 展出 一维 常数 分量 就 可以 把 带 偏置 
项的/nr 线性函数 归并到 WTX 的 形式 之中 线性 模型 非常 
简单 明了 参数 的 每 一维 对应 了 相应 特征 
维度 的 重要性 但是 很 显然 线性 模型 也 存在 
一定 的 局限性 首先 线性 模型 的 取值 范围 是 
不 受限 的 依据 w 和x的/nr 具体 取值 它 的 
输出 可以 是 非常 大 的 正数 或者 非常 小 
的 负数 然而 在 进行 分类 的 时候 我们 预期 
得到 的 模型 输出 是 某个 样 本属 于正 类 
如 正面 评价 的 可能性 这个 可能性 通常 是 取值 
在 0 和1/nr 之间 的 一个 概率值 为了 解决 这 
二者 之间 的 差距 人们 通常 会 使用 一个 对数 
几率 函数 对 线性 模型 的 输出 进行 变换 得到 
如下 公式 经过 变换 严格地 讲 g x w 已经 
不再 是 一个 线性函数 而是 由 一个 线性函数 派生 出来 
的 非线性 函数 我们 通常 称 这类 函数 为 广义 
线性函数 对数 几率 模型 本身 是 一个 概率 形式 非常 
适合 用 对数 似 然 损失 或者 交叉 熵 损失 
进行 训练 其次 线性 模型 只能 挖掘 特征 之间 的 
线性组合 关系 无法 对 更加 复杂 更加 强大 的 非线性 
组合 关系 进行 建模 为了 解决 这个 问题 我们 可以 
对 输入 的 各 维 特征 进行 一些 显 式 
的 非线性 预 变换 如 单维/nr 特征 的 指数 对数 
多项式 变换 以及 多维 特征 的 交叉 乘积 等 或者 
采用 核 方法 把 原 特征 空间 隐式 地 映射 
到 一个 高维 的 非线性 空间 再在 高维空间 里 构建 
线性 模型 核 方法 与 支持 向量 机 略 决策树 
与 Boosting 略 神经网络 神经 网络 是 一类 典型 的 
非线性 模型 它 的 设计 受到 生物 神经 网络 的 
启发 人们 通过 对 大脑 生物 机理 的 研究 发现 
其 基本 单元 是 神经元 每个 神经元 通过 树突 从 
上游 的 神经元 那里 获取 输入 信号 经过 自身 的 
加工 处理 后 再通过 轴 突将 输出 信号传 递给 下游 
的 神经元 当 神经元 的 输入 信号 总和 达到 一定 
强度 时 就会 激活 一个 输出 信号 否则 就 没有 
输出 信号 如 . 7a 所示 . 7 神经元 结构 
与 人工神经网络 这种 生物学 原理 如果 用 数学 语言 进行 
表达 就如 . 7b 所示 神经元 对 输入 的 信号 
进行 线性 加权 求和 然后 依据 求和 结果 的 大小 
来 驱动 一个 激活 函数 ψ 用以 生成 输出 信号 
生物 系统 中 的 激活 函数 类似于 阶跃 函数 但是 
由于 阶跃 函数 本身 不 连续 对于 机器学习 而言 不是 
一个 好 的 选择 因此 在 人们 设计 人工神经网络 的 
时候 通常 采用 连续 的 激活 函数 比如 Sigmoid 函数 
双曲 正切函数 tanh 校正 线性 单元 ReLU 等 它们 的 
数学 形式 和 函数 形状 分别 如 . 8 所示 
. 8 常用 的 激活 函数 1 . 全 连接 
神经 网络 最 基本 的 神经 网络 就是 把 前面 
描述 的 神经元 互相 连接起来 形成 层次结构 如 . 9 
所示 我们 称之为 全 连接 神经网络 对于 . 9 中 
这个 网络 而言 最 左边 对应 的 是 输入 节点 
最 右边 对应 的 是 输出 节点 中间 的 三层 
节点 都是 隐含 节点 我们 把 相应 的 层 称为 
隐含 层 每一个 隐含 节点 都 会把 来自 上 一层 
节点 的 输出 进行 加权 求和 再 经过 一个 非 
线性 的 激活 函数 输 出给 下一层 而 输出 层 
则 一般 采用 简单 的 线性函数 或者 进一步 使用 softmax 
函数 将 输出 变成 概率 形式 . 9 全/a 连接/v 
神经/n 网络/n 全/a 连接/v 神经/n 网络/n 虽然/c 看/v 起来/v 简单/a 
但 它 有着 非常 强大 的 表达 能力 早在 20 
世纪 80 年代 人们 就 证明 了 著名 的 通用 
逼近 定理 Universal Approximation Theorem 28 最早 的 通用 逼近 
定理 是 针对 Sigmoid 激活 函数 证明 的 一般 情况下 
的 通用 逼近 定理 在 2001年 被 证明 29 其 
数学 描述 是 在 激活 函数 满足 一定 条件 的 
前提 下 任意 给定 输入 空间 中 的 一个 连续函数 
和 近似 精度 ε 存在/v 自然数/l N/w ε/i 和/c 一个/m 
隐含/v 节/t 点数/n 为/p N/w ε/i 的/uj 单隐层/nr 全/a 连接/v 
神经网络/n 对 这个 连续函数 的 L ∞ 逼近 精度 小于 
ε 这个 定理 非常重要 它 告诉 我们 全 连接 神经 
网络 可以 用来 解决 非常 复杂 的 问题 当 其他 
的 模型 如 线性 模型 支持 向量 机 等 无法 
逼近 这类 问题 的 分类 界面 时 神经 网络 仍然 
可以 所向披靡 得心应手 近年来 人们 指出 深层 网络 的 表达力 
更强 即 表达 某些 逻辑 函数 深层 网络 需要 的 
隐含 节 点数 比 浅层 网络 少 很多 30 这/r 
对于/p 模型/n 存储/l 和/c 优化/vn 而言/c 都是/nr 比较/d 有利/a 的/uj 
因此 人们 越来越 关注 和 使用 更 深层 的 神经 
网络 全 连接 神经 网络 在 训练 过程 中 常常 
选取 交叉 熵 损失 函数 并且 使用 梯度 下 降法 
来 求解 模型 参数 实际 中 为了 减少 每次 模型 
更新 的 代价 使用 的 是 小 批量 的 随机 
梯度 下 降法 要 注意 的 是 虽然 交叉 熵 
损失 是个 凸函数 但 由于 多层 神经 网络 本身 的 
非线性 和非凸/nr 本质 损失 函数 对于 模型 参数 而言 其实 
是 严重 非 凸 的 在 这种 情况 下 使用 
梯度 下 降法 求解 通常 只能 找到 局部 最优 解 
为了 解决 这个 问题 人们 在 实践 中 常常 采用 
多次 随机 初始化 或者 模拟退火 等 技术 来 寻找 全局 
意义 下 更优 的 解 近年 有 研究 表明 在 
满足 一定 条件 时 如果 神经网络 足够 深 它/r 的/uj 
所有/b 局部/n 最优/d 解/v 其实/d 都和/nr 全局/n 最优/d 解/v 具有/v 
非常/d 类似/v 的/uj 损失/n 函数值/i 31 换言之 对于 深层 神经网络 
而言 只能 找到 局部 最优 解 未见得 是 一个 致命 
的 缺陷 在 很多 时候 这个 局部 最优 解 已经 
足够 好 可以 达到 非常 不错 的 实际 预测 精度 
除了 局部 最优 解和 全局 最优 解的/nr 忧虑 之外 其实 
关于 使用 深层 神经网络 还有 另外 两个 困难 首先 因为 
深层 神经 网络 的 表达 能力 太强 很容易 过拟合 到 
训练 数据 上 导致 其 在 测试 数据 上 表现 
欠佳 为了 解决 这个 问题 人们 提出 了 很多 方法 
包括 DropOut 32 数据 扩张 Data Augmentation 33 批量 归一化 
Batch Normalization 34 权值 衰减 Weight Decay 35 提前 终止 
Early Stopping 36 等 通过 在 训练 过程 中 引入 
随机性 伪 训练样本 或 限定 模型 空间 来 提高 模型 
的 泛化 能力 其次 当 网络 很深 时 输出 层 
的 预测误差 很难 顺利 地 逐层 传递 下去 从而 使得 
靠近 输入 层 的 那些 隐含 层 无法 得到 充分 
的 训练 这个 问题 又 称为 梯度 消减 问题 37 
研究 表明 梯度 消减 主要 是 由 神经 网络 的 
非线性 激活 函数 带来 的 因为 非线性 激活 函数 导数 
的 模 都不 太大 在 使用 梯度 下降 法 进行 
优化 的 时候 非线性 激活 函数 导数 的 逐层 连乘 
会 出现 在 梯度 的 计算 公式 中 从而 使 
梯度 的 幅度 逐层 减小 为了 解决 这个 问题 人们 
在 跨 层 之间 引入 了 线性 直连 或者 由 
门电路 控制 的 线性 通路 38 以期 为 梯度 信息 
的 顺利 回传 提供 便利 2 . 卷积 神经 网络 
除了 全 连接 神经网络 以外 卷积 神经网络 Convolutional Neural Network 
CNN 13 也 是 十分 常用 的 网络 结构 尤其 
适用 于 处理 图像 数据 卷积 神经 网络 的 设计 
是 受 生物 视觉 系统 的 启发 研究 表明 每个 
视觉 细胞 只 对于 局部 的 小区域 敏感 而 大量 
视觉 细胞 平铺 在 视野 中 可以 很好 地 利用 
自然 图像 的 空间 局部 相关性 与此 类似 卷积 神经 
网络 也 引入 局部 连接 的 概念 并且 在 空间 
上 平铺 具有 同样 参数 结构 的 滤波器 也 称为 
卷积 核 这些 滤波器 之间 有 很大 的 重叠 区域 
相当于 有个 空域 滑窗/nr 在 滑窗/nr 滑到 不同 空间 位置 
时 对 这个 窗 内 的 信息 使用 同样 的 
滤波器 进行 分析 这样 虽然 网络 很大 但是 由于 不同 
位置 的 滤波器 共享 参数 其实 模型 参数 的 个数 
并不多 参数 效率 很高 . 10 描述 了 一个 2 
× 2 的 卷积 核 将 输入 图像 进行 卷积 
的 例子 所谓 卷积 就是 卷积 核 的 各个 参数 
和 图像 中 空间 位置 对应 的 像素 值 进行 
点乘 再 求和 经过 了 卷积 操作 之后 会 得到 
一个 和原/nr 图像 类似 大小 的 新 图层 其中 的 
每个 点 都是 卷积 核 在某 空间 局部 区域 的 
作用 结果 可能 对 应于 提取 图像 的 边缘 或 
抽取 更加 高级 的 语义 信息 我们 通常 称 这个 
新 图层 为特征 映射 feature map 对于 一幅 图像 可以 
在 一个 卷积 层 里 使用 多个 不同 的 卷积 
核 从而 形成 多维 的 特征 映射 还 可以 把 
多个 卷积 层级 联 起来 不断 抽取 越来越 复杂 的 
语义 信息 . 10 卷积 过程 示意图 除了 卷积 以外 
池化/nr 也是 卷积 神经 网络 的 重要 组成部分 池化的/nr 目的 
是 对 原 特征 映射 进行 压缩 从而 更好 地 
体现 图像 识别 的 平移 不变性 并且 有效 扩大 后续 
卷积 操作 的 感受 野 池化与/nr 卷积 不同 一般 不是 
参数 化 的 模块 而是 用 确定性 的 方法 求出 
局部 区域内 的 平均值 中位数 或 最大值 最小值 近年来 也/d 
有/v 一些/m 学者/n 开始/v 研究/vn 参数/n 化/n 的/uj 池化/nr 算子/n 
39 ./i 11/m 描述/v 了/ul 对/p 图像/n 局部/n 进行/v 2/m 
×/i 2/m 的/uj 最大值/l 池化/nr 操作/v 后的/nr 效果/n . 11 
池化/nr 操作 示意图 在 实际 操作 中 可以/c 把/p 多个/m 
卷积/n 层/q 和/c 多个/m 池化层/nr 交替/v 级联/n 从而 实现 从 
原始 图像 中 不断 抽取 高层 语义 特征 的 目的 
在此之后 还 可以 再 级联 一个 全 连接 网络 在 
这些 高层 语义 特征 的 基础 上 进行 模式识别 或 
预测 这个 过程 如 . 12 所示 . 12 多层 
卷积 神经网络 N1 N2 N3 表示 对应 单元 重复 的 
次数 实践 中 人们 开始 尝试 使用 越来越 深 的 
卷积 神经网络 以 达到 越来 越好 的 图像 分类 效果 
. 13 描述 了 近年来 人们 在 ImageNet 数据集 上 
不断 通过 增加 网络 深度 刷新 错误率 的 历程 其中 
2015年 来自 微软 研究院 的 深达 152层 的 ResNet 网络 
40 在 ImageNet 数据集 上 取得 了 低 达 3.57% 
的 Top 5 错误率 在 特定 任务 上 超越 了 
普通 人类 的 图像 识别 能力 . 13 卷积 神经 
网络 不断 刷新 ImageNet 数据集 的 识别 结果 . 14 
残差 学习 随着 卷积 神经网络 变得 越来越 深 前面 提到 
的 梯度 消减 问题 也 随之 变得 越来越 显著 给 
模型 的 训练 带来 了 很大 难度 为了 解决 这个 
问题 近年来 人们 提出 了 一系列 的 方法 包括 残差 
学习 40 41 如 . 14 所示 高密度 网络 42 
如 . 15 所示 等 实验 表明 这些 方法 可以 
有效地 把 训练 误差传递 到 靠近 输入 层 的 地方 
为 深层 卷积 神经 网络 的 训练 奠定 了 坚实 
的 实践 基础 . 15 高密度 网络 3 . 循环 
神经网络 循环 神经网络 Recurrent Neural Network RNN 14 的 设计 
也 有 很强 的 仿生学 基础 我们 可以 联想 一下 
自己 如何 读书看报 当 我们 阅读 一个 句 子时 不会 
单纯 地 理解 当前 看到 的 那个 字 本身 相反 
我们 之前 读到 的 文字 会在 脑海 里 形成 记忆 
而 这些 记忆 会 帮助 我们 更好 地 理解 当前 
看到 的 文字 这个 过程 是 递归 的 我们 在 
看 下一个 文字 时 当前 文字 和 历史 记忆 又会 
共同 成为 我们 新的 记忆 并 对 我们 理解 下一个 
文字 提供 帮助 其实 循环 神经 网络 的 设计 基本 
就是 依照 这个 思想 我们 用 表示 在 时刻 的 
记忆 它 是由 t 时刻 看到 的 输入 和 时刻 
的 记忆 st 1 共同 作用 产生 的 这个 过程 
可以 用 下式 加以 表示 很显然 这个 式子 里 蕴含 
着 对于 记忆 单元 的 循环 迭代 在 实际 应用 
中 无限 长 时间 的 循环 迭代 并 没有 太大 
意义 比如 当 我们 阅读 文字 的 时候 每个 句子 
的 平均 长度 可能 只有 十 几个字 因此 我们 完全 
可以 把 循环 神经 网络 在 时域 上 展开 然后 
在 展开 的 网络 上 利用 梯度 下 降法 来 
求得 参数 矩阵 U W V 如 . 16 所示 
用 循环 神经 网络 的 术语 我们 称之为 时域 反向 
传播 Back Propagation Through Time BPTT . 16 循环/vn 神经/n 
网络/n 的/uj 展开/v 和全/nr 连接/v 神经网络/n 卷积 神经网络 类似 当 
循环 神经网络 时域 展开 以后 也会 遇到 梯度 消减 的 
问题 为了 解决 这个 问题 人们 提出 了 一套 依靠 
门电路 来 控制 信息 流通 的 方法 也 就是说 在 
循环 神经 网络 的 两层 之间 同时 存在 线性 和 
非线性 通路 而 哪个 通路 开 哪个 通路 关 或者 
多 大 程度 上 开关 则由 一组 门电路 来 控制 
这个 门电路 也是 带 参数 并且 这些 参数 在 神经 
网络 的 优化 过程 中 是 可 学习 的 比较 
著名 的 两类 方法 是 LSTM 43 和 GRU 44 
如 . 17 所示 GRU 相比 LSTM 更加 简单 一些 
LSTM 有 三个 门电路 输 入门 忘记 门 输 出门 
而 GRU 则有 两个 门电路 重置 门 更新 门 二者 
在 实际 中 的 效果 类似 但 GRU 的 训练 
速度 要 快 一些 因此 近年来 有 变得 更加 流行 
的 趋势 . 17 循环 神经网络 中的 门电路 循环 神经 
网络 可以 对 时间 序列 进行 有效 建模 根据 它 
所 处理 的 序列 的 不同 情况 可以 把 循环 
神经 网络 的 应用 场景 分为 点到 序列 序列 到点 
和序/nr 列到 序列 等 类型 如 . 18 所示 . 
18 循环 神经 网络 的 不同 应用 下面 分别 介绍 
几种 循环 神经 网络 的 应用 场景 1 图像 配 
文字 点到 序列 的 循环 神经 网络 应用 在 这个 
应用 中 输入 的 是 图像 的 编码 信息 可以 
通过 卷积 神经 网络 的 中间层 获得 也 可以 直接 
采用 卷积 神经网络 预测 得到 的 类别 标签 输出 则是 
靠 循环 神经 网络 来 驱动 产生 的 一句 自然语言 
文本 用以 描述 该 图像 包含 的 内容 2 情感 
分类 序列 到点 的 循环 神经 网络 应用 在 这个 
应用 中 输入 的 是 一段 文本 信息 时序 序列 
而 输出 的 是 情感 分类 的 标签 正向 情感 
或 反向 情感 循环 神经网络 用于 分析 输入 的 文本 
其 隐含 节点 包含 了 整个 输入 语句 的 编码 
信息 再 通过 一个 全 连接 的 分类器 把 该 
编码 信息 映射 到 合适 的 情感 类别 之中 3 
机器翻译 序 列到 序列 的 循环 神经 网络 应用 在 
这个 应用 中 输入 的 是 一个 语言 的 文本 
时序 序列 而 输出 的 则是 另 一个 语言 的 
文本 时序 序列 循环 神经 网络 在 这个 应用 中 
被 使用 了 两次 第一次 是 用来 对 输入 的 
源语言 文本 进行 分析 和 编码 而 第二 次 则是 
利用 这个 编码 信息 驱动 输出 目标语言 的 一段 文本 
在 使用 序 列到 序列 的 循环 神经 网络 实现 
机器 翻译 时 在 实践 中 会 遇到 一个 问题 
输 出端 翻译 结果 中 的 某个 词 其实 对于 
输入端 各个 词汇 的 依赖 程度 是 不同 的 通过 
把 整个 输入 句子 编码 到 一个 向量 来 驱动 
输出 的 句子 会 导致 信息 粒度 太 粗糙 或者 
长线 的 依赖 关系 被 忽视 为了 解决 这个 问题 
人们 在 标准 的 序列 到 序列 循环 神经 网络 
的 基础 上 引入 了 所谓 注意力 机制 在 它 
的 帮助 下 输出 端的 每个 词 的 产生 会 
利用 到 输入端 不同 词汇 的 编码 信息 而 这种 
注意力 机制 也是 带 参数 的 可以 在 整个 循环 
神经 网络 的 训练 过程 中 自动 习得 神经 网络 
尤其 是 深层 神经 网络 是 一个 高速 发展 的 
研究 领域 随着 整个 学术界 和 工业界 的 持续 关注 
这个 领域 比 其他 的 机器学习 领域 获得 了 更多 
的 发展 机会 不断 有 新的 网络 结构 或 优化 
方法 被 提出 如果 读者 对于 这个 领域 感兴趣 请 
关注 每年 发表 在 机器学习 主流 学术 会议 上 的 
最新 论文 参考文献 ［ 1 ］ Cao Z Qin T 
Liu T Y et al . Learning to Rank From 
Pairwise Approach to Listwise Approach C / / Proceedings of 
the 24th international conference on Machine learning . ACM 2007 
129 136 . ［ 2 ］ Liu T Y . 
Learning to rank for information retrieval J . Foundations and 
Trends in Information Retrieval 2009 3 3 225 331 . 
［ 3 ］ Kotsiantis S B Zaharakis I Pintelas P 
. Supervised Machine Learning A Review of Classification Techniques J 
. Emerging Artificial Intelligence Applications in Computer Engineering 2007 160 
3 24 . ［ 4 ］ Chapelle O Scholkopf B 
Zien A . Semi supervised Learning chapelle o . et 
al . eds . 2006 J . IEEE Transactions on 
Neural Networks 2009 20 3 542 542 . ［ 5 
］ He D Xia Y Qin T et al . 
Dual learning for machine translation C / / Advances in 
Neural Information Processing Systems . 2016 820 828 . ［ 
6 ］ Hastie T Tibshirani R Friedman J . Unsupervised 
Learning M / / The Elements of Statistical Learning . 
New York Springer 2009 485 585 . ［ 7 ］ 
Sutton R S Barto A G . Reinforcement Learning An 
Introduction M . Cambridge MIT press 1998 . ［ 8 
］ Seber G A F Lee A J . Linear 
Regression Analysis M . John Wiley & Sons 2012 . 
［ 9 ］ Harrell F E . Ordinal Logistic Regression 
M / / Regression modeling strategies . New York Springer 
2001 331 343 . ［ 10 ］ Cortes C Vapnik 
V . Support Vector Networks J . Machine Learning 1995 
20 3 273 297 . ［ 11 ］ Quinlan J 
R . Induction of Decision Trees J . Machine Learning 
1986 1 1 81 106 . ［ 12 ］ McCulloch 
Warren Walter Pitts 1943 . A Logical Calculus of Ideas 
Immanent in Nervous Activity EB . Bulletin of Mathematical Biophysics 
. 5 4 115 133 . ［ 13 ］ LeCun 
Y Jackel L D Bottou L et al . Learning 
Algorithms for Classification A Comparison on Handwritten Digit Recognition J 
. Neural networks The Statistical Mechanics Perspective 1995 261 276 
. ［ 14 ］ Elman J L . Finding structure 
in time J . Cognitive Science 1990 14 2 179 
211 . ［ 15 ］ 周志华 . 机器学习 ［ M 
］ . 北京 清华大学出版社 2017 . ［ 16 ］ Tom 
Mitchell . Machine Learning M . McGraw Hill 1997 . 
［ 17 ］ Nasrabadi N M . Pattern Recognition and 
Machine Learning J . Journal of Electronic Imaging 2007 16 
4 049901 . ［ 18 ］ Voorhees E M . 
The TREC 8 Question Answering Track Report C / / 
Trec . 1999 99 77 82 . ［ 19 ］ 
Wang Y Wang L Li Y et al . A 
Theoretical Analysis of Ndcg Type Ranking Measures C / / 
Conference on Learning Theory . 2013 25 54 . ［ 
20 ］ Devroye L Gy  rfi L Lugosi G 
. A Probabilistic Theory of Pattern Recognition M . Springer 
Science & Business Media 2013 . ［ 21 ］ Breiman 
L Friedman J Olshen R A et al . Classification 
and Regression Trees J . 1984 . ［ 22 ］ 
Quinlan J R . C4 . 5 Programs for Machine 
Learning M . Morgan Kaufmann 1993 . ［ 23 ］ 
Iba W Langley P . Induction of One level Decision 
Trees J / / Machine Learning Proceedings 1992 . 1992 
233 240 . ［ 24 ］ Breiman L . Bagging 
predictors J . Machine Learning 1996 24 2 123 140 
. ［ 25 ］ Schapire R E . The Strength 
of Weak Learnability J . Machine Learning 1990 5 2 
197 227 . ［ 26 ］ Schapire R E Freund 
Y Bartlett P et al . Boosting the Margin A 
New Explanation for The Effectiveness of Voting Methods J . 
Annals of Statistics 1998 1651 1686 . ［ 27 ］ 
Friedman J H . Greedy Function Approximation A Gradient Boosting 
Machine J . Annals of statistics 2001 1189 1232 . 
［ 28 ］ Gybenko G . Approximation by Superposition of 
Sigmoidal Functions J . Mathematics of Control Signals and Systems 
1989 2 4 303 314 . ［ 29 ］ Cs 
á ji B C . Approximation with Artificial Neural Networks 
J . Faculty of Sciences Etvs Lornd University Hungary 2001 
24 48 . ［ 30 ］ Sun S Chen W 
Wang L et al . On the Depth of Deep 
Neural Networks A Theoretical View C / / AAAI . 
2016 2066 2072 . ［ 31 ］ Kawaguchi K . 
Deep Learning Without Poor Local Minima C / / Advances 
in Neural Information Processing Systems . 2016 586 594 . 
［ 32 ］ Srivastava N Hinton G Krizhevsky A et 
al . Dropout A Simple Way to Prevent Neural Networks 
from Overfitting J . The Journal of Machine Learning Research 
2014 15 1 1929 1958 . ［ 33 ］ Tanner 
M A Wong W H . The Calculation of Posterior 
Distributions by Data Augmentation J . Journal of the American 
statistical Association 1987 82 398 528 540 . ［ 34 
］ Ioffe S Szegedy C . Batch Normalization Accelerating Deep 
Network Training by Reducing Internal Covariate Shift C / / 
International Conference on Machine Learning . 2015 448 456 . 
［ 35 ］ Krogh A Hertz J A . A 
Simple Weight Decay Can Improve Generalization C / / Advances 
in neural information processing systems . 1992 950 957 . 
［ 36 ］ Prechelt L . Automatic Early Stopping Using 
Cross Validation Quantifying the Criteria J . Neural Networks 1998 
11 4 761 767 . ［ 37 ］ Bengio Y 
Simard P Frasconi P . Learning Long term Dependencies with 
Gradient Descent is Difficult J . IEEE Transactions on Neural 
Networks 1994 5 2 157 166 . ［ 38 ］ 
Srivastava R K Greff K Schmidhuber J . Highway networks 
J . arXiv preprint arXiv 1505.00387 2015 . ［ 39 
］ Lin M Chen Q Yan . Network in Network 
J . arXiv preprint arXiv 1312.4400 2013 . ［ 40 
］ He K Zhang X Ren S et al . 
Deep Residual Learning for Image Recognition C / / Proceedings 
of the IEEE Conference on Computer Vision and Pattern Recognition 
. 2016 770 778 . ［ 41 ］ He K 
Zhang X Ren S et al . Identity Mappings in 
Deep Residual Networks C / / European Conference on Computer 
Vision . Springer 2016 630 645 . ［ 42 ］ 
Huang G Liu Z Weinberger K Q et al . 
Densely Connected Convolutional Networks C / / Proceedings of the 
IEEE Conference on Computer Vision and Pattern Recognition . 2017 
1 2 3 . ［ 43 ］ Hochreiter S Schmidhuber 
J . Long Short term Memory J . Neural Computation 
1997 9 8 1735 1780 . ［ 44 ］ Cho 
K Van Merri  nboer B Gulcehre C et al 
. Learning Phrase Representations Using RNN Encoder decoder for Statistical 
Machine Translation J . arXiv preprint arXiv 1406.1078 2014 . 
［ 45 ］ Cauchy A . M é thode g 
é n é rale pour la r é solution des 
systemes d é quations simultan é es J . Comp 
. Rend . Sci . Paris 1847 25 1847 536 
538 . ［ 46 ］ Hestenes M R Stiefel E 
. Methods of Conjugate Gradients for Solving Linear Systems M 
. Washington DC NBS 1952 . ［ 47 ］ Wright 
S J . Coordinate Descent Algorithms J . Mathematical Programming 
2015 151 1 3 34 . ［ 48 ］ Polyak 
B T . Newton s Method and Its Use in 
Optimization J . European Journal of Operational Research 2007 181 
3 1086 1096 . ［ 49 ］ Dennis Jr J 
E Mor é J J . Quasi Newton Methods Motivation 
and Theory J . SIAM Review 1977 19 1 46 
89 . ［ 50 ］ Frank M Wolfe P . 
An Algorithm for Quadratic Programming J . Naval Research Logistics 
NRL 1956 3 1 2 95 110 . ［ 51 
］ Nesterov Yurii . A method of solving a convex 
programming problem with convergence rate O 1 / k2 J 
. Soviet Mathematics Doklady 1983 27 2 . ［ 52 
］ Karmarkar N . A New Polynomial time Algorithm for 
Linear Programming C / / Proceedings of the Sixteenth Annual 
ACM Symposium on Theory of Computing . ACM 1984 302 
311 . ［ 53 ］ Geoffrion A M . Duality 
in Nonlinear Programming A Simplified Applications oriented Development J . 
SIAM Review 1971 13 1 1 37 . ［ 54 
］ Johnson R Zhang T . Accelerating Stochastic Gradient Descent 
Using Predictive Variance Reduction C / / Advances in Neural 
Information Processing Systems . 2013 315 323 . ［ 55 
］ Sutskever I Martens J Dahl G et al . 
On the Importance of Initialization and Momentum in Deep Learning 
C / / International Conference on Machine Learning . 2013 
1139 1147 . ［ 56 ］ Duchi J Hazan E 
Singer Y . Adaptive Subgradient Methods for Online Learning and 
Stochastic Optimization J . Journal of Machine Learning Research 2011 
12 7 2121 2159 . ［ 57 ］ Tieleman T 
Hinton G . Lecture 6.5 rmsprop Divide the Gradient By 
a Running Average of Its Recent Magnitude J . COURSERA 
Neural networks for machine learning 2012 4 2 26 31 
. ［ 58 ］ Zeiler M D . ADADELTA An 
Adaptive Learning Rate Method J . arXiv preprint arXiv 1212.5701 
2012 . ［ 59 ］ Kingma D P Ba J 
. Adam A Method for Stochastic Optimization J . arXiv 
preprint arXiv 1412.6980 2014 . ［ 60 ］ Reddi S 
Kale S Kumar . On the Convergence of Adam and 
Beyond C / / International Conference on Learning Representations 2018 
. ［ 61 ］ Hazan E Levy K Y Shalev 
Shwartz . On Graduated Optimization for Stochastic Non convex Problems 
C / / International Conference on Machine Learning . 2016 
1833 1841 . 