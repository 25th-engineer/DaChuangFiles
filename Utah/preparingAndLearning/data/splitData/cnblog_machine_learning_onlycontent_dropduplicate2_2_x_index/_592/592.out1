本文 介绍 增强 学习 和自/nr 适应控制 在 监督 学习 中 
算法 是 要 输出 尽量 模仿 训练 集中 的 标签 
y 标签 给 每个 输入 x 一个 清楚 的 正确 
答案 与此 不同 对于 许多 序列 决策 和 控制 问题 
就 很难 对 算法 给出 这种 明确 的 监督 例如 
如果 要 造 一个 四 足 机器人 并 编程 让 
它 行走 起初 我们 并不 知道 让 它 行走 的 
正确 行动 所以 也 不 知道 怎么 模仿 学习 算法 
给出 明确 的 监督 在 增强 学习 框架 中 我 
给 算法 一个 回报 函数 告诉 学习 代理 执行 得 
好坏 在 四 足 行走 的 机器人 例子 中 当 
机器人 往前 走时 回报 函数 就 给予 正反馈 退后 或者 
摔倒 就 给予 负反馈 学习 算法 的 工作 就是 弄 
清楚 怎么 随着 时间 选择 动作 以使 总回报 最大 增强 
学习 的 应用 非常 广泛 如 无人机 运动 机器人 蜂窝 
电话 网络 路由 市场策略 选择 工业 控制 和 高效 的 
页面 排序 我们 对 增强 学习 的 研究 将从 MDP 
马尔科夫 决策 过程 开始 形式化 增强 学习 遇到 的 问题 
1 马尔可夫 决策 过程 一个 马尔可夫 决策 过程 是 元组 
S A { Psa } γ R 其中 S 是 
状态 集合 例如 在 无人机 飞行 中 S 可以 是 
直升机 所 有可能 的 位置 和 方向 A 是 动作 
集合 例如 直升机 操纵杆 能够 转动 的 所有 方向 Psa 
是 状态 转换 概率 对 每一个 状态 s ∈ 和 
动作 a ∈ A Psa 是 在 状态 空间 上 
的 分布 简单 地 说 Psa 给定 的 是 当在 
状态 s 采取行动 a 我们 会 变成 哪种 状态 的 
分布 概率 γ 是 折扣 因子 R × A R 
是 回报 函数 回报 有时 写成 只有 状态 S 的 
函数 R S R MDP 执行 的 动态 过程 如下 
从 状态 s0 开始 在 MDP 中 选择 一些 动作 
a0 ∈ A 来 执行 作为 选择 的 结果 根据 
s1 ~ Ps0a0 MDP 的 状态 随机 切换 到 后继 
状态 s1 然后 再 选择 另外 一个 动作 a1 作为 
动作 的 结果 根据 s2 ~ Ps1a1 状态 再次 切换 
然后 选择 a2 等等 周期性 的 这个 过程 可表示 如下 
访问 状态 序列 s0 s1 . . . 并 执行 
动作 a0 a1 . . . 总回报 为 或者 如果 
回报 仅仅是 状态 的 函数 那么 总回报 可 写作 对 
大 部分 应用 来说 我们 会 使用 更 简单 的 
状态 回报 R s 虽然 状态 动作 回报 R s 
a 的 泛化 也 没有 特别 的 困难 增强 学习 
的 目标 是 随着 时间 选择 动作 以 最大化 总回报 
的 期望值 时间 点 t 的 回报 通过 乘以 因子 
  γ t   打了 折扣 所以 为使 期望 最大 
我们 希望 正 回报 来得 越早 越好 负 回报 尽量 
往 后面 去 在 经济 应用 中 R 是 挣钱 
的 总金额 γ 自然 可以 解释 为 利率 今天 的 
一 英镑 比 明天 的 一 英镑 值钱 一个 策略 
是 一些 从 状态 到 动作 的 映射函数   π 
S A 无论 何时 我们 在 状态 s 执行 了 
动作 a = π s 就 说 在 执行 策略 
  π 定义   π 的 值 函数 为 V 
s 是从 状态 s 开始 根据 策略   π 采取行动 
最终 的 折扣 回报 期望 和 给定 固定 策略   
π 它 的 值 函数 V π 满足 Bellman 方程 
也 就是说 从 s 开始 的 折扣 回报 V π 
s 的 期望 和由/nr 两部分 组成 第一 从 状态 s 
开始 的 立即 回报 R s 第二 未来 折扣 回报 
的 期望 和 仔细检查 第二项 和式 可写 为 这 是从 
状态 s 开始 的 折扣 回报 的 期望 和 s 
符合 分布 Ps π s 也 就是 从 状态 s 
执行 第一 个 动作   π s 后的/nr 状态 分布 
所以 第二项 给 的 是 在 MDP 中/f 第一/m 步后的/nr 
折扣/v 回报/v 的/uj 期望/v 和/c Bellman 方程 能 用于 高效 
解出 V π 特别 是 在 一个 有限 状态 MDP 
| | ∞ 可以 为 每个 状态 写下 这个 方程 
V π s 这 给定 了 一个 有 | | 
个 变量 每个 状态 都 有个 未知 的 V π 
S 的 | | 个 线性方程 的 集合 可以 有效 
解出 V π s 定义 最优 值 函数 也 就是 
能 使用 策略 得到 的 最好 的 折扣 回报 期望 
和 还有 另一个 版本 的 Bellman 方程 第一项 跟 之前 
一样 是 立即 回报 第二项 是 执行 动作 a 后的/nr 
折扣/v 回报/v 未来/t 期望/v 和的/nr 最大值/l 要 特别 注意 这里 
的 max 是 要从 所有 动作 中 选择 回报 最大 
的 动作 而 max 后面 的   γ 乘以/v 求和/v 
项/n 就是/d 执行/v 某/r 动作/n 后的/nr 回报/v 期望/v 要 确保 
你 理解 了 上式 的 含义 定义 策略   π 
* S A 如下 π * s 给 出了 能使 
总回报 最大 的 行动 a 事实上 对 每一个 状态 和 
每一个 策 略有 第一 个 等式 是 说 V π 
* π * 的 值 函数 等于 对 每个 状态 
s 来说 的 最优 值 函数 V * 第二 个 
不等式 是 说 π * 的 值 至少 跟 其它 
策略 一样大 换句话说 π * 就是 最优 策略 注意到   
π * 有 一个 有趣 的 属性 对 所有 的 
状态 它 都 是个 最优 策略 特别 的 并 不是 
说 如果 从 s 开始 就 有 针对 那个 状态 
的 最优 策略 如果 从 其它 的 s 开始 就 
有 针对 s 的 其它 最优 策略 特别 的 同样 
的 策略   π * 对/p 所有/b 的/uj 状态/n 都能/nr 
获得/v 最大值/l 这 意味着 我们 可以 使用 同样 的 策略 
  π * 而 不管 MDP 的 初始 状态 是 
什么 2 值 迭代 和 策略 迭代 现在 介绍 两种 
解决 有限 状态 MDP 的 高效 算法 这里 只 考虑 
有限 状态 和 动作 空间 的 MDP 第一 个 算法 
值 迭代 如下 这个 算 法 可以 看作 是 使用 
Bellman 方程 重复 更新 估计值 函数 这里 R s 是 
回报 函数 V s 是 值 函数 注意 这 两者 
的 区别 有助于 理解 该 算法 有 两种 方法 执行 
算法 内环 的 更新 第 一种 是 先 对 每一个 
状态 s 计算 新 值 V s 并 覆盖 旧 
值 这 被 称为 同步 更新 另一种 是 异步 更新 
以 一定 顺序 循环 遍历 状态 一次 更新 一个 值 
不管 是 同步 更新 还是 异步 更新 值 函数 V 
都会 收敛 到 V * 找到 V * 后 就 
能够 使用 上面 介绍 过 的 如下 方程 找到 最优 
策略 除了 值 迭代 还有 一种 寻找 最优 策略 的 
算法 策略 迭代 所以 内循环 重 复地 为 当前 策略 
计算 值 函数 然后 使用 当前 值 函数 更 新策略 
注意到 步骤 a 可以 通过 解 之前 描述 的 Bellman 
方程 来 完成 在 策略 固定 的 情况 下 它 
是 一个 有 | | 个 变量 的 | | 
个 线性方程 的 集合 在 算法 的 有限 次 迭代 
后 V 将 收敛 到 V * π 将 收敛 
到   π * 值 迭代 和 策略 迭代 都是 
解 MDP 的 标准 算法 关于 哪个 算法 更好 没有 
一致 的 看法 对于 小 的 MDPs 策略 迭代 经常 
很快 很少 迭代 就能 收敛 但 对于 有 大量 状态 
空间 的 MDPs 来说 解 MDPs 会 涉及 大量 线性方程 
就 比较 困难 这种 情况 使用 值 迭代 更好 由于 
这个 原因 实际 中值 迭代 会比 策略 迭 代用 得多 
3 为 MDP 学习 一个 模型 目前 为止 我们 讨论 
了 MDPs 和它的/nr 算法 假定 状态 转换 矩阵 和 回报 
是 已知 的 在 许多 现实 问题 中 并不知道 状态 
转换 矩阵 和 清晰 的 回报 但 可以 从 数据 
中 估计 出来 通常 S A 和 γ 是 知道 
的 例如 在 倒立 摆 问题 中 在 MDP 中 
有 一些 尝试 这里 si j 表示 我们 在 时间 
点 i 尝试 j 时 所处 的 状态 ai j 
是 在 那个 状态 采取 的 相应 动作 实际上 上面 
的 每一次 尝试 都 可以 一直 运行 直到 MDP 终止 
或者 运行 一个 很大 但是 有限 的 时间 步 有了/nr 
这些 在 MDP 中 尝试 的 经验 我们 就 很容易 
推导 出 状态 转换 矩阵 的 最大 似 然 估计 
如果 上面 的 比率 是 0/0 也 就是说 从没 在 
状态 s 采取 动作 a 那就 简单 估计 Psa s 
为 1 / | | 也 就是 估计 Psa 在 
所有 状态 上 均匀分布 使用 类似 的 过程 如果 R 
是 未知 的 我们 也 可以 估计 状态 s 的 
期望 立即 回报 R s 为 在 状态 s 观察到 
的 平均 回报 从 MDP 中 学习 到 一个 模型 
后 就 可以 使用 值 迭代 或者 策略 迭代 来 
解 MDP 该 MDP 使用 的 是 估计 转换 概率 
和 回报 例如 结合模型 学习 和值/nr 迭代 这 是 一个 
可能 的 算法 在 状态 转换 矩阵 未知 的 MDP 
中 学习 注意到 针对 这个 算法 有 一个 简单 的 
优化 能使 它 运行 得 更快 在 使用 值 迭代 
的 内循环 中 如果 不 初始化 V = 0 而是 
用 算法 上 一轮 迭代 使用 的 结果 就 会为 
值 迭代 提供 一个 更好 的 起始 点 使 它 
更快 收敛 4 连续 状态 MDPs 目前 为止 我们 讨论 
的 都是 有限 状态 的 MDPs 现在 讨论 无限 状态 
的 MDPs 算法 例如 一辆 汽车 的 状态 可以 表示 
为 x y θ x . y . θ . 
x y 是 位置 方向   θ x 和 y 
方向 的 速度 x .   和 y . θ 
的 角速度 为 θ . 所以 = R6 是 一个 
无限 状态 的 集合 因为/c 对于/p 一/m 辆车/m 有/v 无限/v 
个/q 位置/v 和/c 方向/n 类似 的 PS4 中的 倒立 摆 
的 状态 为 x θ x θ θ 是 杆 
的 角度 直升机 在 3D 空间 中 的 状态 为 
x y z Φ θ Ψ x y z Φ 
θ Ψ 其中 翻滚 角   Φ 俯仰角   θ 
和 偏航角   Ψ 指定 了 飞机 的 3D 方向 
下面 我们 讨论 状态 空间 = Rn 的 情况 并 
描述 解 MDPs 的 方法 4.1 离散化 或许 解 连续 
状态 MDP 最 简单 的 方法 就是 离散化 状态 空间 
然后 使用 之前 介绍 的 值 迭代 或 策略 迭代 
算法 例如 如果 是 2d 的 状态 那么 就 可以 
用 表格 来 离散化 状态 空间 这里 每 一个 表格 
单元 代表 一个 分离 的 离散 状态   \ \ 
overline { s } \   我们 可以 通过 离散 
状态   \ \ overline { } A \ { 
P _ { \ overline { s } a } 
\ } γ R \ 来 近似 连续 状态 的 
MDP 其中 \ \ overline { } \ 是 离散 
状态 的 集合 \ P _ { \ overline { 
s } a } \ 是 离散 状态 的 状态 
转换 矩阵 我们 可以 使用 值 迭代 或 策略 迭代 
来 解出 离散 状态 MDP   \ \ overline { 
} A \ { P _ { \ overline { 
s } a } \ } γ R \ 的 
\ V ^ { * } \ overline { s 
} \ 和 \ \ pi ^ { * } 
\ overline { s } \ 当 我们 实际 的 
系统 在 连续 值 状态 s ∈ 需要 选择 一个 
动作 来 执行 我们 就 计算 相应 的 离散 状态 
\ \ overline { x } \ 然后 执行 动作 
    \ \ pi ^ { * } \ 
overline { s } \ 这种 离散化 可以 解决 很多 
问题 但是 有 两个 弊端 首先 它 用 的 是 
一种 比较 简单 的 方法 来 表示 V * 和 
π * 特别 是 它 假 定值 函数 在 离散 
区间 取 的 是 常量 值 函数 在 每个 表格 
中 是 分段 常量 为了 更好 地 理解 这种 表示 
的 限制 看 一个 监督 学习 问题 用 一个 函 
数来 拟合 这些 数据 很 清楚 线性 回归 在 这个 
问题 上 会做 得 很好 但 如果 在 x 轴上 
离散化 然后 使用 分段函数 来 对应 每 一个 区间 拟合 
的 数据 会 像这样 这种 分段 常量 表示 对 很多 
平滑 函数 都 不是 一个 好 的 选择 它 会 
导致 输入 不再 平滑 在 不同 的 表格 中 没有 
泛化 能力 使用 这种 表示 我们 也 需要 一种 好 
的 离散化 非常 小 的 表格 单元 来 得到 一种 
好 的 近似 这种 表示 的 第二 个 弊端 被称为 
维度 诅咒 假设 = Rn 我们 要 把 n 个 
状态 的 维度 都 离散 化为 k 个 值 那么 
离散 状态 的 总数 目 为 kn 这种 状态 维度 
的 指数 增长 非常 快 不 适用 于 大型 问题 
例 如有 10d 个 状态 每个 状态变量 离散 化为 100 
个 值 就有 10010 = 1020 个 离散 状态 这 
对于 当代 的 桌面 电脑 来说 还是 太 庞大 了 
一般来说 离散化 对于 1d 和 2d 问题 工作 得 很好 
有/v 简单/a 和/c 快速/d 执行/v 的/uj 好处/d 或许 聪明 加上 
小心 选择 离散化 方法 在 顶多 4d 问题 上 都能 
工作 很好 如果 你 极其 聪明 加 一点 运气 你 
可以 用 到 6d 问题 上 但是 它/r 很少/m 能/v 
运行/v 在/p 比/p 那更/nr 高维/nr 问题/n 上/f 4.2 值 函数 
近似 这里 介绍 一种 在 连续 状态 MDPs 寻找 策略 
的 可选 方法 直接 近似于 V * 而 不是 用 
离散化 这种 方法 被 称为 值 函数 近似 已经 被 
成功 用 到 很多 增强 学习 问题 中 4 . 
2.1 使用 模型 或 模拟器 为实现 值 函数 近似算法 我们 
假定 对于 MDP 有 一个 模型 或者 模拟器 非正式 地 
解释 模拟器 是个 黑盒子 把 连续 值 状态 st   
和 动作 at 作为 输入 输出 下 一个 状态 st 
+ 1 根据 的 是 状态 转换 概率 \ P 
_ { s _ { t } a _ { 
t } } \ 有 许多 方法 得到 这样 的 
模型 一种 是 使用 物理 模拟 例如 PS4 中的 倒立 
摆 的 模拟 器 就是 使用 物理 法则 来 计算 
杆 在 时间 点 t + 1 的 位置 和 
方向 给定 了 当前 时间 点 t 的 状态 和要/nr 
采取 的 动作 a 假定 知道 系统 的 所有 参数 
如 杆 的 长度 质量 等等 也 可使用 现成 的 
模拟 软件包 把 输入 当作 一个 数学 系统 的 完整 
物理 描述 当前 状态 st 和 动作 at 计算 下 
一个 状态 st + 1 还有 一个 可选 的 方法 
是从 MDP 收集 的 数据 中 学习 模型 例如 假设 
我们 在 MDP 中 重复 执行 动作 执行 了 m 
次 尝试 trial 每次 尝试 T 个 时间 步 做 
这些 尝试 可以 随机 选择 动作 执行 一些 特定 的 
策略 或 通过 其它 方法 来 选择 动作 然后 就 
可以 观察到 如下 m 个 状态 序列 现在 用 学习 
算法 来 预测 st + 1 把 它 作为 st 
和 at 的 函数 例如 可以 选择 线性 模型 的 
形式 $ $ s _ { t + 1 } 
= As _ { t } + Ba _ { 
t } $ $ 使用 类似 于 线性 回归 的 
算法 这里 模型 的 参数 是 矩阵 A 和 B 
使用 m 次 尝试 的 数据 来 估计 它们 这 
跟 参数 的 极大 似 然 估计 有关 学习 到 
A 和 B 之后 一个 选择 是 建立 确定性 的 
模型 给定 输入 st 和 at 就 确定 了 st 
+ 1 另 一个 选择 是 建立 随机性 的 模型 
st + 1 是 输入 的 随机 函数 这里   
ε t 是 一个 噪声 项 通常 建模 为   
ε t ~ N 0 Σ 协方差 矩阵   Σ 
也 可以 直接 从 数据 中 估计 这里 我们 把 
下 一个 状态 st + 1 作为 当前 状态 和 
动作 的 线性函数 当然 非 线性函数 也是 可能 的 比如 
可以 学到 模型 st + 1 = A Φ s 
st + B Φ a at Φ s   和 
Φ a   是 状态 和 动作 的 非 特性 
映射 可以 使用 非线性 学习 算法 如 局部 权重 线性 
回归 把 st + 1 作为 st 和 at 的 
函数 来 估计 这些 方法 也 可以 用来 建立 MDP 
的 确定性 或 随机性 模拟器 4 . 2.2 拟合 值 
函数 这里 介绍 一种 趋近 连续 状态 MDP 的 值 
函数 的 拟合 值 迭代 算法 假定 问题 有 连续 
状态 空间 = Rn 但 动作 空间 A 是 很小 
和 离散 的 同 之前 介绍 的 离散 状态 的 
值 迭代 类似 我们 执行 如下 更新 这里 把 和 
换成 了 积分 表示 这是 在 连续 状态 空间 下 
拟合 值 迭代 的 主要 思想 是 在 有限 的 
状态 采样 中 s 1 . . . s m 
趋近 执行 这 一步 特别 的 我们 使用 监督 学习 
算法 线性 回 归来 近似值 函数 作为 状态 的 线性 
或非 线性函数 $ $ V s = \ theta ^ 
{ T } \ phi s $ $ 这里 Φ 
是 一些 状态 的 近似 特征 映射 对于 m 个 
状态 的 有限 采样 的 每一个 状态 拟合 值 迭代 
将 首先 计算 量值 y i 将 是 \ R 
s + \ gamma max _ { a } E 
_ { s ^ { } ~ P _ { 
sa } } V s ^ { } \ 的 
近似 也 就是 上面 方程 的 右边 部分 然后 我们 
将 使用 学习 算法 来 得到 接近   \ R 
s + \ gamma max _ { a } E 
_ { s ^ { } ~ P _ { 
sa } } V s ^ { } \ 的 
V s 或者 换句话说 获得 V s 以 接近 y 
i 详细 算法 如下 上面 所写 的 拟合 值 函数 
使用 线性 回归 算法 以使 V s i 趋近 y 
i 算法 的 那一步 完全 可以 类比 标准 的 监督 
学习 问题 有 一个 训练 集 x 1 y 1 
x 2 y 2 . . . x m y 
m 要 学习 一个 函数 从 x 映 射到 y 
唯一 的 不同 是 这里 的 s 扮演着 x 的 
角色 虽然 上面 的 描述 使用 了 线性 回归 但 
明显 其它 的 回归 算法 如 局部 权重 线性 回归 
也 可以 使用 不像 离散 状态 的 值 迭代 拟合 
值 迭代 不能 证明 会 一直 收敛 不过 实际 中 
经常 是 收敛 的 或 近似 收敛 同时 也 注意 
到 如果 我们 使用 MDP 的 确定性 模拟器 那么 拟合 
值 迭代 就能 通过 在 算法 中 设置 k = 
1 来 简化 这 是因为 方程 \ R s + 
\ gamma \ mathop { max } \ limits _ 
{ a } E _ { s ^ { } 
~ P _ { sa } } V s ^ 
{ } \ 变成 了 一个 确定性 分布 的 期望 
否则 在 上面 的 算法 中 我 不得 不作 k 
次 采样 并 平均 以 近似 期望 见 上面 伪代码 
中 q a 的 定义 最后 拟合 值 迭代 算法 
输出 了 V 它 是 对 V * 的 近似 
这 隐含 定义 了 我们 的 策略 特别 的 当 
系统 在 状态 s 需要 选择 一个 动作 我们 会 
这样 选择 $ $ arg   \ mathop { max 
} \ limits _ { a } E _ { 
s ^ { } ~ P _ { sa } 
} V s ^ { } $ $ 这个 计算 
近似 过程 类似于 拟合 值 迭代 的 内循环 对 每一个 
动作 都取/nr \ s _ { 1 } ^ { 
} \ . . . \ s _ { k 
} ^ { } \ ~ \ P _ { 
sa } \ 以 近似 期望 如果 模拟器 是 确定性 
的 可以 设 k = 1 在 实际 中 经常 
有 其它 的 方法 来 近似 这一步 例如 一个 通用 
的 例子 是 如果 模拟器 的 形式 为 st + 
1 = f st at + ε 其中 f 是 
状态 的 确定性 函数 比如 f st at = Ast 
+ Bat ε 是 零 均值 高斯 噪声 在 这种 
情况 下 可以 选择 动作 $ $ arg \ mathop 
{ max } \ limits _ { a } V 
f s a $ $ 换句话说 这里 设置   ε 
t = 0 例如 忽略 模拟器 的 噪声 设置 k 
= 1 等同 的 这 能够 使用 如下 近似 从 
方程   \ arg   \ mathop { max } 
\ limits _ { a } E _ { s 
^ { } ~ P _ { sa } } 
V s ^ { } \ 推导 这里 期望 是 
指 s ~ Psa 噪声 项 ε/nr t 很小 这 通常 
是 一个 合理 的 近似 尽管如此 对于 不 使用 这些 
近似 的 问题 不得不 使用 模型 采样 k | A 
| 个 状态 以 近似 上面 的 期望 这会 是 
很高 的 计算 复杂度 参考资料 1 http / / cs229 
. stanford . edu / notes / cs229 notes12 . 
pdf 