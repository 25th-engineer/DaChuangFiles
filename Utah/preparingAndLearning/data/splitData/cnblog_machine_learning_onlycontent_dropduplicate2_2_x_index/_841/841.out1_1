转载 请 注明 出处 http / / www . cnblogs 
. com / ymingjingr / p / 4271742 . html 
目录 机器学习 基石 笔记 1 在 何时 可以 使用 机器学习 
1 机器学习 基石 笔记 2 在 何时 可以 使用 机器学习 
2 机器学习 基石 笔记 3 在 何时 可以 使用 机器学习 
3 修改版 机器学习 基石 笔记 4 在 何时 可以 使用 
机器学习 4 机器学习 基石 笔记 5 为什么 机器 可以 学习 
1 机器学习 基石 笔记 6 为什么 机器 可以 学习 2 
机器学习 基石 笔记 7 为什么 机器 可以 学习 3 机器学习 
基石 笔记 8 为什么 机器 可以 学习 4 机器学习 基石 
笔记 9 机器 可以 怎样 学习 1 机器学习 基石 笔记 
10 机器 可以 怎样 学习 2 机器学习 基石 笔记 11 
机器 可以 怎样 学习 3 机器学习 基石 笔记 12 机器 
可以 怎样 学习 4 机器学习 基石 笔记 13 机器 可以 
怎样 学 得 更好 1 机器学习 基石 笔记 14 机器 
可以 怎样 学 得 更好 2 机器学习 基石 笔记 15 
机器 可以 怎样 学 得 更好 3 机器学习 基石 笔记 
16 机器 可以 怎样 学 得 更好 4 十三 Hazard 
of Overfitting 过拟合 的 危害 13.1 What is Overfitting 什么 
是 过拟合 假设 一个 输入 空间 X 是 一维 的 
样本点 数量 为 5 的 回归 问题 其 目标函数 为 
二次函数 但是 标记 包含 噪音 因此 标记 表示 为 使用 
四次 多项式 转换 结合 线性 回归 求解 该 问题 的 
权值 向量 w 得出 的 唯一 解 如 1 所示 
1 二次 的 目标 函数 与 四次 多项式 的 最优 
假设 函数 从 图中 不难 得出 该 假设 函数 的 
很大 因此 四次 的 多项式 函数 有 很差 的 泛化 
能力 依据 2中 分析 此 问题 在 VC 维 变大 
时 变小 但 变大 此种 情况 称作 过拟合 overfitting 意思为 
在 训练 样 本上 拟 合做 的 很好 非常 小 
但是 过度 了 使得 泛化 能力 变差 导致 很大 当然 
还有 一种 情况 是 VC 维 变 小时 出现 了 
越来越 小 同时 也 越来越 小 此种 情况 称为 欠 
拟合 underfitting 其中 如何 解决 欠 拟合 的 问题 已经 
在 12.4节 作了 介绍 不断 地 提高 多项式 次数 使得 
VC 维 提高 达到 拟合 的 效果 但 过拟合 的 
问题 更为 复杂 以后 的 章节 会 更 深入 的 
探讨 过拟合 和坏的/nr 泛化 有所不同 过拟合 指 的 是 和 
变化 的 过程 在 变小 变大 时 称为 过拟合 而 
坏 的 泛化 是 在某 一点 很小 很大 2 VC 
维 与 错误率 之间 的 关系 或许 理解 上 还是 
有些 困难 用 一个 类比 的 方式 便于 人 理解 
将 机器学习 比作 开车 如表 13 1 所示 表 13 
1 机器学习 开车 过拟合 出车祸 使用 过度 的 VC 维 
开得 太快 噪音 颠簸 的 路面 数据量 的 大小 对 
路面 状况 的 观察 其中 第 3 ~ 5行 表示 
构成 第 2行 的 原因 即/v 除了/p VC/w 维度/ns 之外/f 
噪音/n 和/c 训练/vn 数据/n 的/uj 大小/b 对/p 过拟合/i 都/d 有影响/i 
13.2 The Role of Noise and Data Size 噪音 与 
数据 量 所 扮演 的 角色 为了 更 直观 的 
解释 产生 过拟合 的 因素 设计 两个 实验 分别 设计 
两个 目标函数 一个 10次 多项式 另一个 50次 多项式 前者 加上 
噪音 后者 无噪音 生成 如 3 所示 的 数据 点 
3 a 10次 多项式 加上 噪音 生成 的 训练 数据 
b 10次 多项式 没有 噪音 生成 的 训练 数据 使用 
两种 不同 的 学习 模型 二次 式 假设 空间 与 
10次 多项式 假设 空间 分别 根据 以上 两个 生成 的 
训练 数据 进行 学习 首先 使用 两种 模型 学习 13 
3a 生成 的 数据 两种 模型 产生 的 最优 假设 
如 4 所示 其中 绿线 表示 二次 模型 学习 得到 
的 假设 函数 g 红色 表示 10次 模型 学习 得到 
的 假设 函数 g 4 两种 模型 学习 10次 目标函数 
生成 的 数据 这两个 最优 函数 的 错误率 如表 13 
2 所示 在上 显然 二次函数 不如 10次 函数 的 错误率 
低 但是 在 上 10次 函数 远远 的 高于 2次 
函数 能够 知道 是 因为 已知 目标函数 当然 在 现实 
中 是 不 可能 的 说明 在做 10次 函数 生成 
的 训练 数据 上 使用 2次 函数 模型 会 得到 
效果 更优 的 表 13 2 两种 模型 从 10次 
目标函数 生成 数据 产生 的 错误率 0 . 0500.0340 . 
1299.00 继续 使用 这 两种 模型 对 13 3 b 
生成 的 数据 进行 学习 得到 两种 假设 函数 如 
5 所示 5 两种 模型 学习 50次 目标函数 生成 的 
数据 这两个 最优 函数 的 错误率 如表 13 3 所示 
在上 显然 二次函数 不如 10次 函数 的 错误率 低 但是 
在 上 10次 函数 远远 的 高于 2次 函数 说明 
在做 50次 函数 生成 的 训练 数据 上 使用 2次 
函数 模型 会 得到 效果 更优 的 表 13 3 
两种 模型 从 50次 目标函数 生成 数据 产生 的 错误率 
0.0290 . 000010.1207680 难道 在 两种 情况 下 从 2次 
函 数到 10次 函数 都是 过拟合 答案 是 对 的 
为什么 二次 式 模型 与 目标 函数 的 次数 有 
很大 差距 反而 比 10次 多项式 模型 学习 能力 还好 
要从 学习 鸿沟 说起 learning curves 二次函数 和 10次 函数 
的 学习 鸿沟 如 6 所示 6 a 二次函数 学习 
鸿沟 b 10次 函数 学习 鸿沟 从 图中 可以 看出 
数据量 少时 尽管 2次 假设 的 比 10次 函数 的 
大 很多 但是 2次 假设 中和 的 差距 比 10次 
假设 小 的 多 因此在 样本点 不多时 低 次 假设 
的 学习 泛化 能力 更强 即在 灰色 区域 样本 不多 
的 情况 下 中 高次 假设 函数 发生 了 过拟合 
上述 阐述 了 在 含有 噪音 的 情况 下 低 
次 多项式 假设 比 和 目标函数 同次 的 多项式 假设 
表现 更好 那 如何 解释 在 50次 多项式 函数 中 
也是 二次 式 表现 好 的 现象 呢 因为 50次 
目标函数 对于 不论是 2次 假设 还是 10次 假设 都 相当于 
一种 含有 噪音 的 情况 两者 都 无法 做到 50次 
的 目标 函数 因此 相当于 含有 噪音 13.3 Deterministic Noise 
确定性 噪音 数据 样本 由 两个 部分 组成 一是 目标函数 
产生 和 在此 之上 夹杂 的 噪音 其中 假设 噪音 
服从 高斯分布 称作 高斯 噪音 其 强度 为 目标函数 使用 
复杂度 表示 即 次 多项式 函数 不难看出 过拟合 和 噪音 
强度 目标函数 复杂度 上节 最后 说明 高次 也 是 一种 
噪音 形式 和/c 训练/vn 数据量/n N/w 都/d 有着/v 密切/ad 的/uj 
关系/n 以下 通过 固定 某一 参数 对 比 其他 两个 
参数 的 方式 观察 每个 参数 对 过拟合 的 影响 
分为 和 为了 体现 各个 参数 的 影响 通过 编写 
的 程序 完成 一些 规定 的 实验 绘制 成 具体 
的 图像 便于 理解 和上/nr 一节 相同 使用 两 种 
学习 模型 测试 二次 式 模型 和 10次 多项式 模型 
其 最优 假设 函数 分别 表示 为 错误率 满足 并 
使用 作为 过拟合 的 衡量 分别 固定 复杂度 和 噪音 
强度 得到 如 7 所示 的 两幅 图 7 a 
固定 算法 强度 时 与 N 对 拟合度 的 影响 
b 固定 噪音 强度 时与 N 对 拟合度 的 影响 
7 a 表示 在 固定 算法 强度 时 噪音 强度 
与 样本 数据量 N 对 拟合度 的 影响 图中 的 
颜色 表示 过拟合 程度 深红色 的 部分 表示 过拟合 蓝色 
表示 表现 好 从/p 图中/i 得知/v 在/p 噪音/n 强度/n 越大/i 
与/p 样本/n 数据量/n N/w 越/d 小时/n 过拟合 越 严重 有关 
高斯 噪音 产生 的 噪音 又 被称作 随机 噪音 stochastic 
noise 7 b 表示 在 固定 噪音 强度 时 算法 
强度 与 样本 数据量 N 对 拟合度 的 影响 从/p 
图中/i 得知/v 在/p 算法/n 强度/n 越大/i 且/zg 样本/n 数据量/n N/w 
越/d 小时/n 过拟合 越 严重 在 图 的 左下角 的 
表现 与 7 a 的 表现 略有不同 即在 算法 强度 
且 数据量 很 小成 三角形 的 区域 造成 该 现象 
的 原因 是 此处 选用 的 两个 模型 是 二次 
式 与 10次 多项式 而在 低于 10次 的 目标 函数 
中 使用 10次 多项式 模型 学习 即 产生 了 13.1节 
中 提过 的 过度 的 VC 维 使用 有关 算法 
强度 产生 也 相当于 产生 了 噪音 称作 确定性 噪音 
deterministic noise 总结 造成 严重 过拟合 现象 的 原因 有 
四个 数据量 N 少 随机 噪 音高 确定性 噪 音高 
过量 的 VC 维 可能 确定性 噪音 比较 难于 理解 
通过 8 再做 一次 简单 的 解释 8 确定性 噪音 
示意图 其中 蓝色 曲线 表示 目标函数 红色 取消 表示 二次 
式 函数 模型 学习 到 的 曲线 其中 红色 曲线 
的 弯曲 的 形状 使用 2次 函数 是 不可 能 
模仿 的 因此 就 相当于 一种 噪音 为什么 使用 低 
次 函数 学习 效果 却 好呢 这 类似于 教 小孩 
学习 学习 简单 的 问题 反而 有助于 成长 13.4 Dealing 
with Overfitting 处理 过拟合 回忆 13.1节 中的 表 13 1 
提到 了 产生 过拟合 的 三种 原因 本节 提出 防止出现 
过拟合 的 几种 情况 该 情况 与 防止 出 车祸 
的 应对 措施 作 对比 如表 13 4 所示 表 
13 4 防止 过拟合 的 措施 与 防止 出 车祸 
的 措施 的 对比 从 简单 的 模型 出发 开 
慢点 数据清理 / 裁剪 data cleaning / pruning 更 准确 
的 路况 信息 数据 提示 data hinting 获取 更多 的 
路况 信息 正则化 regularization 踩刹车 确认 validation 安装/v 仪表盘/n 从/p 
简单/a 模型/n 出发/v 的/uj 措施/n 在前/i 几节/m 中都/ns 有/v 体现/v 
不再 赘述 本节 主要 介绍 数据清理 以及 数据 提示 而 
正则化 和 确认 则在 后面 的 章节 介绍 以 手写 
数字 数据 为例 介绍 数据清理 和 数据 裁剪 观察 9 
手写 数字 为 1 的 使用 表示 手写 数字 为 
5 的 使用 表示 其中 在 数字 1 中 存在 
一个 数字 5 的 样本点 即 图中 左上角 中的 查看 
该 样本 的 原图 很难 看出 是 数字 5 类似 
这种 离 不同 类别 很近 离 相同 类别 很远 的 
样本 可以 认为 是 噪音 或者 是 离群 点 outlier 
应对 该 种 情况 有 两种 措施 可用 纠正 标识号 
即 数据清理 data cleaning 的 方式 处理 该 情况 删除 
错误 样本 即 数据 裁剪 data pruning 的 方式 处理 
处理 措施 很 简单 但是 发现 样本 是 噪音 或 
离群 点 却 比较 困难 9 手写 数字 的 分布 
情况 继续 介绍 数据 提示 还是 以 手写 数字 集 
为例 将如 10 所示 的 手写 数字 集 略作 修改 
产生 更多 的 手写 数字 样本 达到 增加 数据量 N 
的 目的 如 将 如下 手写 数字 略作 旋转 rotating 
和 平移 shifting 但/c 要/v 注意/v 旋转/v 和/c 平移/v 的/uj 
幅度/n 都/d 不能/v 太大/ns 如 6 转 180 ° 就 
成了 9 还需 注意 这种 方式 产生 的 虚拟 样本 
virtual examples 不在 符合 独立 同 分布 因此 产生 的 
虚拟 样本 与 实际 样本 差距 一定 不宜 太大 10 
手写 数字 集 