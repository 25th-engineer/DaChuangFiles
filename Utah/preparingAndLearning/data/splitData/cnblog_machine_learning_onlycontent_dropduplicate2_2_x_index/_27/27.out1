1 . 从 一个 栗子 开始 Slot Filling 比如 在 
一个 订票 系统 上 我们 的 输入 Arrive Taipei on 
November 2nd 这样 一个 序列 我们 设置 几个 槽 位 
Slot 希望 算法 能够 将 关键词 Taipei 放入 目的地 Destination 
槽 位 将 November 和 2nd 放入 到达 时间 Time 
of Arrival 槽 位 将 Arrive 和 on 放入 其他 
Other 槽 位 实现 对 输入 序列 的 一个 归类 
以便 后续 提取 相应 信息 用 前馈 神经网络 Feedforward Neural 
Network 来 解决 这个 问题 的话 我们 首先 要 对 
输入 序列 向 量化 将 每一个 输入 的 单词 用 
向量 表示 可以 使用 One of N Encoding 或者 是 
Word hashing 等 编码方法 输出 预测 槽 位 的 概率分布 
但是 这样 做 的话 有个/nr 问题 就 出现 了 如果 
现在 又 有一个 输入 是 Leave Taipei on November 2nd 
这里 Taipei 是 作为 一个 出发地 Place of Departure 所以 
我们 应当 是 把 Taipei 放入 Departure 槽 位 而不是 
Destination 槽 位 可是 对于 前馈 网络 来说 对于 同一 
个 输入 输出 的 概率分布 应该 也 是 一样 的 
不 可能 出现 既是 Destination 的 概率 最高 又是 Departure 
的 概率 最高 所以 我们 就 希望 能够 让 神经 
网络 拥有 记忆 的 能力 能够 根据 之前 的 信息 
在 这个 例子 中 是 Arrive 或 Leave 从而 得到 
不同 的 输出 将 两段 序列 中的 Taipei 分别 归入 
Destionation 槽 位 和 Departure 槽 位 2 . RNN 
基本 概念 在 RNN 中 隐 层 神经元 的 输出 
值 都被 保存 到 记忆 单元 中 下一次 再 计算 
输出 时 隐 层 神经元 会将 记忆 单元 中的 值 
认为 是 输入 的 一部分 来 考虑 RNN 中 考虑 
了 输入 序列 顺序 序列 顺序 的 改变 会 影响 
输出 的 结果 常见 变体 Elman Network 将 隐 层 
的 输出 即 记忆 单元 中的 值 作为 下一次 的 
输入 \ h _ t = \ sigma _ h 
W _ hx _ t + U _ h \ 
color { green } { h _ { t 1 
} } + b _ h \ \ y _ 
t = \ sigma _ h W _ yh _ 
t + b _ y \ Jordan Network 将上 一 
时间 点 的 输出 值 作为 输入 \ h _ 
t = \ sigma _ h W _ hx _ 
t + U _ h \ color { green } 
{ y _ { t 1 } } + b 
_ h \ \ y _ t = \ sigma 
_ h W _ yh _ t + b _ 
y \ Bidirectional RNN3 . Long Short term Memory LSTM 
基本 结构 由 Memory Cell Input Gate Output Gate Forget 
Gate 组成 特殊 的 神经 元 结构 包含 4个 input 
三个 Gate 的 控制 信号 以及 输入 的 数据 1个 
output 激活 函数 通常 选用 sigmoid function sigmoid 的 输出 
介于 0 到 1 之间 表征 了 Gate 的 打开 
程度 Traditional LSTM \ \ begin { align } f 
_ t & = \ sigma _ g W _ 
fx _ t + \ color { green } { 
U _ fh _ { t 1 } } + 
b _ f \ \ i _ t & = 
\ sigma _ g W _ i x _ t 
+ \ color { green } { U _ ih 
_ { t 1 } } + b _ i 
\ \ o _ t & = \ sigma _ 
g W _ o x _ t + \ color 
{ green } { U _ oh _ { t 
1 } } + b _ o \ \ c 
_ t & = f _ t \ { \ 
circ } \ c _ { t 1 } + 
i _ t \ { \ circ } \ \ 
sigma _ c W _ cx _ t \ color 
{ green } { + U _ ch _ { 
t 1 } } + b _ c \ \ 
h _ t & = o _ t \ { 
\ circ } \ \ sigma _ h c _ 
t \ end { align } \ Peephole LSTM 在 
大部分 的 情况 下 用 \ \ color { blue 
} { c _ { t 1 } } \ 
取代 \ \ color { green } { h _ 
{ t 1 } } \ \ \ begin { 
align } f _ t & = \ sigma _ 
g W _ fx _ t + \ color { 
green } { U _ f \ color { blue 
} { c _ { t 1 } } } 
+ b _ f \ \ i _ t & 
= \ sigma _ g W _ i x _ 
t + \ color { green } { U _ 
i \ color { blue } { c _ { 
t 1 } } } + b _ i \ 
\ o _ t & = \ sigma _ g 
W _ o x _ t + \ color { 
green } { U _ o \ color { blue 
} { c _ { t 1 } } } 
+ b _ o \ \ c _ t & 
= f _ t \ { \ circ } \ 
c _ { t 1 } + i _ t 
\ { \ circ } \ \ sigma _ c 
W _ cx _ t + b _ c \ 
\ h _ t & = o _ t \ 
{ \ circ } \ \ sigma _ h c 
_ t \ end { align } \ \ x 
_ t \ 表示 输入 向量 \ h _ t 
\ 表示 输出 向量 \ c _ t \ 表示 
记忆 单元 的 状态 向量 \ \ circ \ 代表 
Hadamard product A . k . a . Schur product 
\ W \ 表示 输入 权重 \ U \ 表示 
循环 权重 \ b \ 表示 偏置 \ \ delta 
_ g \ 代表 sigmoid function \ \ delta _ 
c \ 代表 hyperbolic tangent \ \ delta _ h 
\ 表示 hyperbolic tangent peephole LSTM 论文 中 建议 选用 
\ \ delta _ h x = x \ \ 
f _ t \ \ i _ t \ 和\/nr 
o _ t \ 表示 门控 向 量值 \ f 
_ t \ 表示 遗忘 门 向量 表征 记 忆旧 
信息 的 能力 \ i _ t \ 表示 输 
入门 向量 表征 获取 新 信息 的 能力 \ o 
_ t \ 表示 输 出门 向量 表征 输出 信息 
的 能力 补充 知识点 Short term 表示 保留 对 前一 
时间 点 输出 的 短期 记忆 相比 于最/nr 原始 的 
RNN 结构 中 的 记忆 单元 每次 有 新的 输 
入时 记忆体 的 状态 就 会被 更新 因此 是 短期 
的 记忆 而 LSTM 的 记忆体 则 拥有 相对 较长 
的 记忆 时间 由 Forget Gate 决定 所以 是 Long 
Short termLSTM 一般 采用 多 层 结构 组合 Multiple layer 
LSTMKeras 中 实现 了 LSTM GRU Cho Learning Phrase Representations 
using RNN Encoder Decoder for Statistical Machine Translation EMNLP 14 
只有 两个 Gate 容易 训练 SimpleRNN 层 可以 方便 的 
调用 4 . RNN 如何 学习 损失 函数 的 定义 
每 一个 时间 点 的 RNN 的 输出 和 标签 
值 的 交叉 熵 cross entropy 之和 训练 过程 使用 
被称作 Backpropagation through time BPTT 的 梯度 下 降法 训练 
其实 是 比较 困难 的 因为 Total Loss 可能 会 
出现 剧烈 的 抖动 根据 论文 Razvan Pascanu On the 
difficulty of training Recurrent Neural Networks ICML 13 上 对 
RNN 的 分析 损失 函数 的 表面 要么 非常 平坦 
要么 非常 陡峭 The error surface is either very flat 
or very steep 当 你 的 参数值 在 较为 平坦 
的 区域 做 更新 时 因此 该 区域 梯度 值 
比较 小 此时 的 学习 率 一般 会 变得 的 
较大 如果 突然 到达 了 陡峭 的 区域 梯度 值 
陡增 再与 此时 较大 的 学习 率 相乘 参数 就 
有很 大幅度 更新 实线 表示 的 轨迹 因此 学习 过程 
非常 不 稳定 Razvan Pascanu 使用 了 叫做 Clipping 的 
训练 技巧 为 梯度 设置 阈值 超过 该 阈值 的 
梯度 值 都会 被 cut 这样 参数 更新 的 幅度 
就 不会 过大 虚线 表示 的 轨迹 因此 容易 收敛 
为什么 在 RNN 中会 有 这种 问题 是 因为 激活 
函数 选用 了 sigmoid 而不是 ReLU 么 然而 并 不是 
事实上 在 RNN 中 使用 ReLU 反而 效果 会 不如 
Sigmoid 不过 也是 看 你 的 参数 初始化 值 的 
选取 所以 也 不一定 比如 后面 提到 的 Quoc V 
. Le 的 那 篇文章 使用 特别 初始化 技巧 硬 
训 ReLU 的 RNN 得到 了 可 比拟 LSTM 的 
效果 因此 激活 函数 并 不是 这里 的 关键 点 
那 究竟 是 什么 原因 呢 我们 来 分析 梯度 
更新 公式 中的 \ w \ eta \ frac { 
\ partial { L } } { \ partial { 
w } } \ 来 探寻 一番 但是 这样 一个 
偏微分 的 关系 我们 应该 如何 来 分析 呢 这里 
我们 用 一个 技巧 给 w 值 一个 微小 的 
变化 观察 对应 的 Loss 的 变化 情况 假设 当前 
模型 是 1000个 只含 有一个 线性 隐 层 的 RNN 
级联 结构 并 假设 我们 当前 的 输入 是 100000 
只有 第一 个 值 是 1 剩下 全是 0 因此 
最后 的 输出 值 是 \ w ^ { 999 
} \ 现在 假设 我们 \ w \ 的 值 
是 1 那么 RNN 在 最后 时间 点 的 输出 
是 1 给 \ w \ 一个 微小 的 变化 
+ 0.01 此时 的 输出 变成 了 大约 20000 这段 
区域 呈现 出 一个 陡峭 的 趋势 如果 给 \ 
w \ 一个 微小 的 变化 0.01 变为 0.99 测试 
的 输出 基本 变成 0 哪怕 是 \ w \ 
变到 0.01时 输出 依旧 是 0 这段 区域 呈现 出 
一个 平坦 的 趋势 因此 我们 可以 看出 由于 RNN 
采用 时间 序列 的 结构 权重 值 在 不同 时间 
点 被 反复 使用 这种 累积性 的 变化 可能 对 
结果 造成 极大 的 影响 也 可能 会 很长 一段 
时间 保持 平稳 常用 的 技巧 使用 LSTM 单元 LSTM 
单元 可以 处理 梯度 消失 问题 但 无法 处理 梯度 
爆炸 问题 为什么 呢 这 是因为 RNN 和 LSTM 对待 
记忆 单元 的 做法 是 不同 的 RNN 中 每一个 
时间 点 的 记忆 单元 中 的 内容 状态 都会 
更新 而 LSTM 则是 将 记忆 单元 中的 值 与 
输入 值 相加 按 某种 权值 再 更新 状态 记忆 
单元 中的 值 会 始终 对 输出 产生影响 除非 Forget 
Gate 完全 的 关闭 因此 不用 担心 梯度 值 会 
弥散 相反 的 这倒 极易 引起 梯度 爆炸 采用 一些 
更 新颖 的 结构 或 训练方法 比如 Clockwise RNN Jan 
Koutnik A Clockwork RNN JMLR 14 Structurally Constrained Recurrent Network 
SCRN Tomas Mikolov Learning Longer Memory in Recurrent Neural Networks 
ICLR 15 Vanilla RNN Initialized with Identity matrix + ReLU 
activation function Quoc V . Le A Simple Way to 
Initialize Recurrent Networks of Rectified Linear Units arXiv 15 该位 
仁兄 用了 特别 的 初始化 技巧 硬 训 RNN 效果 
可 比拟 甚至 超越 LSTM5 . RNN 的 更多 应用 
场景 Sentiment Analysis 情感 分析 Key Term Extraction 关键字 提取 
Speech Recognition 语音 辨识 Connectionist Temporal Classification CTC 语音 辨识 
中 的 一个 关键 技术 通过 增加 一个 额外 的 
Symbol 代表 NULL 来 解决 叠字 问题 参考 论文 Graves 
Alex and Navdeep Jaitly . Towards end to end speech 
recognition with recurrent neural networks . Proceedings of the 31st 
International Conference on Machine Learning ICML 14 . 2014 . 
Sequence to sequence learning 输入 和 输出 都是 不同 长度 
的 序列 Machine Translation 机器翻译 Syntactic parsing 句法分析 Seq to 
seq Auto encoder 将 文档 转换 为 向量 表示 BoW 
模型 会 忽略 掉 语序 在 某些 情况下 相反 意思 
的 语句 会 产生 相同 的 词 袋 模型 而 
RNN 的 方法 考虑 语序 因此 更为 鲁棒 将 语音 
转换 为 向量 表示 Attention based Model 注意力 模型 Neural 
Turing Machine 神经 图灵机 Reading Comprehension End To End Memory 
Networks . . Sukhbaatar A . Szlam J . Weston 
R . Fergus . NIPS 2015 . 基于 Keras 实现 
的 一个 example 问答 系统 6 . 其他 的 学习 
资料 The Unreasonable Effectiveness of Recurrent Neural Networkshttp / / 
karpathy . github . io / 2015 / 05/21 / 
rnn effectiveness / Understanding LSTM Networkshttp / / colah . 
github . io / posts / 2015 08 Understanding LSTMs 
/ 7 . 本文 参考 资料 Machine Learning 2016 Fall 
Hung yi Lee NTUhttp / / speech . ee . 
ntu . edu . tw / ~ tlkagk / courses 
/ ML _ 2016 / Lecture / RNN % 20 
v2 . pdfDeep Learning Ian Goodfellow and Yoshua Bengio and 
Aaron Courvillehttp / / www . deeplearningbook . org / 
Deep Learning in a Nutshell Sequence Learninghttps / / devblogs 
. nvidia . com / parallelforall / deep learning nutshell 
sequence learning / Long short term memoryhttps / / en 
. wikipedia . org / wiki / Long _ short 
term _ memoryDeep & Structured 未完待续 