本文 介绍 独立 成分 分析 ICA 同 PCA 类似 我们 
是 要 找到 一个 新的 基 来 表示 数据 但 
目的 就 不 一样 了 鸡尾酒会 问题 n 个人 在 
一个 party 上 同时 说话 n 个 麦克风 放置 在 
房间 的 不同 位置 因为 每个 麦克风 跟 每个人 的 
距离 都 不一样 所以 它们 记录 的 说话者 重叠 的 
声音 也 不一样 根据 麦克风 记录 的 声音 如何 分离出 
n 个 说话者 的 声音 呢 为 形式化 这个 问题 
我们 想象 有 一些 数据 s ∈ R 是从 n 
个 独立 的 源 生成 的 我们 观察 到 的 
是 x = As 矩阵 A 是 未知 的 被称作 
混合 矩阵 通过 不断 观察 得到 的 是 { x 
i = 1 . . . m } 我们 的 
目标 是 找到 生成 数据 x i = As i 
的 源 s i 在 鸡尾酒 问题 中 s i 
是个 n 维 向量 sj i 是 讲话者 j 在 
时间 i 发出 的 声音 x i 也 是个 n 
维 向量 xj i 是 麦克风 j 在 时间 点 
i 记录 的 声音 设 W = A 1 为 
一个 分离 矩阵 我们 的 目标 就是 找到 W 这样 
就 能 根据 麦克风 记录 的 声音 x i 来 
恢复 声源 s i = Wx i 为 表示 方便 
起见 使 wiT 表示 W 的 第 i 行 所以 
wi ∈ Rn   第 j 个 源 能够 通过 
计算 sj i = wj i x i 来 恢复 
1 ICA 的 模糊性 W = A 1 能 恢复 
到 什么 程度 如果 没有 源 和 混合 矩阵 的 
先验 知识 不难看出 只 给定 x A 有 一些 固有 
的 模糊性 是 不 可能 被 恢复 的 设 P 
为 一个 n × n 的 排列 矩阵 这 意味着 
P 的/uj 每/zg 一行/m 和每/nr 一列/m 都/d 只有/c 一个/m 1 
下面 是 一些 排 列矩阵 的 例子 如果 z 是 
一个 向量 那么 Pz 就是 z 的 坐标 重排 版本 
的 另一个 向量 只 给定 x i 就 没 办法 
分辨 W 和 PW 原始 信号 的 排列 也是 模糊不清 
的 幸运 的 是 这对 大 部分 应用 都 不重要 
还有 无法 恢复 wi 的 准确 比例 例如 如果 A 
换成 2A 每个 s i 都 换成 0.5 s i 
那么 我们 观察 到 的 依旧 是 x i = 
2A 0.5 s i 同样 如果 A 的 一个 列 
向量 乘以 因子 α 相应 的 源 乘以 因子 1 
/ α 依然 没有 办法 在 只 给定 x i 
的 情况 下 决定 发生 了 什么 所以 我们 无法 
恢复 源 的 准确 比例 不过 对于 很多 应用 来说 
这种 模糊性 都 无关紧要 包括 鸡尾酒会 问题 这 就是 ICA 
中 模糊性 唯一 的 源 了吗 当 si 是非 高斯 
就是 这样 的 那么 高斯 数据 的 困难 是 什么 
呢 看 一个 例子 n = 2 s ~ N 
0 I 其中 I 是 2 × 2 的 单位 
矩阵 标准 正态分布 N 0 I 的 密度 的 轮廓 
是以 原点 为 中心 的 圆 密度 时 旋转 对称 
的 现在 假定 我们 观察 到 x = As 其中 
A 是 混合 矩阵 x 的 分布 也是 高斯 均值 
为 0 协方差 E xxT = E AssTAT = AAT 
设 R 为 一个 任意 的 正交矩阵 所以 RRT = 
RTR = I 使 A = AR 如果 数据 是 
通过 A 而 不是 A 来 混合 的 那么 可 
观察到 x = A s x 的 分布 也是 高斯 
的 均值 为 0 协方差 为 E x x T 
= E A ssT A T = E ARssT AR 
T = ARRTAT = AAT 所以 不管 混合 矩阵 是 
A 还是 A 都能 观察到 数据 符合 N 0 AAT 
分布 所以 就 无法 分辨 源 是 通过 A 还是 
A 混合 的 所以 混合 矩阵 的 旋转 组件 无法 
从 数据 中 找出 来 我们 不能 恢复 原 始源 
上面 的 讨论 是 基于 多元 标准 正态分布 是 旋转 
对称 的 ICA 在 高斯 数据 上 表现 不行 但 
只要 数据 不是 高斯 的 给定 足够 的 数据 我们 
就 能 恢复 出 n 个 独立 的 源 2 
密度 和 线性转换 在 推导 ICA 算法 之前 我们 先 
来 讨论 下 密度 的 线性 转换 的 影响 假定 
随机变量 s 符合 密度 函数 ps s 简单 起见 假设 
s ∈ R 是 一个 实值 现在 随机变量 x 为 
x = As 其中 s ∈ R A ∈ R 
那么 x 的 密度 px 是 什么 设 W = 
A 1 为 计算 特 定值 x 的 概率 容易 
想到 s = Wx 然后 估计 该点 的 ps 得出 
px x = ps Wx 当然 这 是 不对 的 
例如 设 s ~ Uniform 0 1 所以 s 的 
密度 为 ps s = 1 { 0 ≤ s 
≤ 1 } 现在 让 A = 2 那么 x 
= 2s 很明显 x 是 均匀分布 在 区间 0 2 
所以 它 的 密度 为 px x = 0.5 { 
0 ≤ x ≤ 2 } 而 不是 ps Wx 
其中 W = 0.5 = A 1 正确 的 公式 
是 px x = ps Wx | W | 一般地说 
如果 s 是 一个 向 量值 分布 密度 为 ps 
x = As 其中 A 为 可逆矩阵 那么 x 的 
密度 为 px x = ps Wx | W | 
其中 W = A 1 3 ICA 算法 现在 来 
推导 ICA 算法 ICA 算法 归功于 Bell 和 Sejnowski 这里 
使用 最大 似 然 估计 来 解释 算法 原始 论 
文中 的 解释 是 用 一种 称为 infomax principal 的 
复杂 思想 已经 不 适用 于 当前 对 ICA 的 
理解 假设 每个 源 si 的 概率 密度 为 ps 
源 s 的 联合 分布 为 把 联合 分布 建模 
为 边缘 分布 的 乘积 这里 假设 源 是 相互 
独立 的 使用 之前 的 公式 x = As = 
W 1s 的 概率 密度 为 剩下 的 就是 给 
独立 的 源 ps   指定 一个 密度 给定 一个 
实值 随机变量 z 它 的 累积 分布 函数 cdf F 
定义 为 F z0 = P z ≤ z0 = 
∫ pz z dz z 的 密度 就是 对 F 
求导 pz z = F z 所以 要 指定 si 
  的 密度 先 指定 一个 累积 分布 函数 cdf 
一个 cdf 是从 0 到 1 的 单调 递 增函数 
根据 之前 的 讨论 不能 选择 高斯 累积 分布 函数 
因为 ICA 在 高斯 数据 上 无效 要 选择 一个 
合理 的 能从 0 到 1 缓慢 递增 的 函数 
就 选择 sigmoid 函数 g s = 1 / 1 
+ e s 所以 ps s = g s 矩阵 
W 是 模型 的 参数 给定 训练 集 { x 
i i = 1 . . . m } log 
似 然 为 要以 W 为 参数 最大化 该 式 
求导 并 使用 事实   ▽ w | W | 
= | W | W 1 T 很容易 就 导出 
随机 梯度 下降 学习 规则 对于 一个 训练 例子 x 
i 更新 规则 为 其中   α 是 学习率 算法 
收敛 后 就 能够 通过 计算 s i = Wx 
i 来 恢复 原始 信号 写 数据 的 似 然 
时 x i 之间 是 相互 独立 的 所以 训练 
集 的 似 然 为   ∏ i p x 
i W 这个 假设 对于 讲话 数据 和 其它 x 
i 依赖 的 时间 序列 是 明显 不对 的 但 
可以 看到 如果 有 足够 的 数据 即使 训练 集 
是 相关 的 也 不会 影响 算法 的 性能 但是 
对于 连续 训练 例子 是 相关 的 问题 执行 随机 
梯度 下降时 有时 碰到 一些 随机 排列 的 训练 集 
也会 加速 收敛 参考资料 1 http / / cs229 . 
stanford . edu / notes / cs229 notes11 . pdf 
2   http / / blog . csdn . net 
/ stdcoutzyx / article / details / 38037659 