引言 之前 学习 了 逻辑 回归 主要 是 从三/nr 方面 
学习 的 一个 是 coursera 上台 大林 轩 田 老师 
机器学习 公开课 的 逻辑 回归 部分 一个 是 斯坦福 Andrew 
Ng 老师 机器学习 公开课 的 逻辑 回归 部分 另 一个 
是 机器学习 实战 逻辑 回归 部分 前 两者 主要 是 
对 逻辑 回归 理论 的 学习 后者 主要 是 实践 
的 学习 现在 对 其 进行 整理 也 便于 自己 
思考 本文 主要 内容 本文 主要 分为 以下 内容 首先 
大致 介绍 了 逻辑 回归 的 分类 过程 包括 常规 
分类 过程 中 的 计算 权重 与 样本 特征 之间 
乘积 得到 分数 利用 sigmod 函数 将 分数 转换 到 
0 到 1 之间 的 值 然后 利用 这个 值 
进行 分类 然后 对 损失 函数 进行 了 分析 包括 
如何 利用 极大 似 然 得到 最佳 的 权重 向 
量值 以及 梯度 下 降法 时 权重 的 更新 公式 
再 然后 对 解决 过拟合 的 一般 方式 进行 了 
一定 的 说明 包括 正规化 减少 特征 最后 是 机器学习 
实战 的 代码 和 仿真 实验 的 结果 逻辑 回归 
的 分类 过程 跟 一般 的 分类 模型 一样 分类 
的话 需要 找 一个 合适 的 分类 函数 台大 机器学习 
中 称之为 h 函数 hypothesis 函数 台大 机器学习 课程 中 
对 h 得到 的 过程 是 这样 讲解 的 权重 
w 与 样本 特征值 的 乘积 得到 分数 样本 x 
是 有n维/nr 的 每 一维 代表 样本 的 一个 特征 
每一 特征 在 判断 x 属于 哪 一类 时 所占 
的 权重 不 一样 所以 首先 需要 对 x 的 
各个 维度 即 特征向量 的 每 一维 加 权值 w 
这里 的 w 未知 我们 逻辑 回归 的 目的 就是 
求出 这个 w 权重 向量 表示 对 特征向量 每 一维 
所占 的 权重 算 出 一个 作为 判断 分类 的 
一个 分数 s s 的 计算 方式 如下 逻辑 函数 
将 分数 转换 为 类别 然后 有了 分数 s 就 
需要 利用 一个 逻辑 函数 将 s 转换 为 0 
到 1 的 值 0 1 也 就是 分类 的 
结果 0 代表 负 类 1 代表 正 类 逻辑 
回 归中 在 这一步 与 其他 不同 的 是 它 
含 增加 了 一个 sigmod 函数 将 分数 转换 到 
0 到 1 之间 的 一个 值得 到 这个 0 
到 1 之间 的 值 θ s 再 将其 与 
一个 阈值 做 比较 一般 是 0.5 大于 阈值 的 
为 正 类 1 小于 阈值 的 为 负 类 
0 最终 得到 分类 模型 与 样本 特征向量 的 关系 
公式 如下 应用 分类 函数 进行 类别 判断 得到 分类 
模型 函数 之后 如果 想 知道 某一 样本 的 分类 
结果 常见 的 场景 应用 是 将 这个 样本 的 
特征向量 作为 x 输入 到 这个 函数 得到 一个 y 
值 将 y 值 与 阈值 做 比较 大于 阈值 
的 为 正 小于 阈值 的 为 负 对 分类 
函数 模型 的 选取 需要 对 数据 有 一定 的 
了解 或 分析 知道 或者 猜测 预测 函数 的 大概 
形式 比 如是 线性函数 还 是非 线性函数 损失 函数 要 
怎样 去 衡量 输出 的 类别 与 实际 的 类别 
之间 的 差值 呢 这 就 需要 构造 一个 损失 
函数 该 函数 表示 预测 的 输出 h 与 训练 
数据 类别 y 之间 的 偏差 可以 是 二者 之间 
的 差 h y 机器学习 实战 中 选取 这种 形式 
也 可以 是 其他 的 形式 综合考虑 所有 训练 数据 
的 损失 将 Cost 求和 或者 求 平均 记为 J 
θ 函数 表示 所有 训练 数据 预测 值 与 实际 
类别 的 偏差 显然 J θ 函数 的 值 越小 
表示 预测 函数 越 准确 即 h 函数 越 准确 
所以 这 一步 需要 做 的 是 找到 J θ 
函数 的 最小值 找 函数 的 最小值 有 不同 的 
方法 Logistic Regression 实 现时 有的是 梯度 下 降法 机器学习 
实战 中 用了 梯度 下 降法 和 随机 梯度 下 
降法 构造 损失 函数 其实 前面 的 分类 函数 h 
x 还有 一个 深 层次 的 含义 它 代表 了 
样本 x 属于 类别 1 的 概率 Andrew Ng 在 
课程 中 直接 给 出了 Cost 函数 及 J θ 
函数 的 公式 但是 并 没有 给 出 具体 的 
解释 只是 说明 了 这个 函数 来 衡量 h 函数 
预测 的 好坏 是 合理 的 台 大林 轩 田 
老师 倒 是从 原 理上 推导 了 整个 过程 但 
个人 感觉 这 部分 只 是 为了 得到 衡量 分类 
结果 的 一个 指标值 在此 只写出 部分 推导 过程 包括 
极大 似 然 估计 和 梯度 下 降法 的 推导 
其余 的 有兴趣 的 可以 去看 台大 机器学习 相关 部分 
讲 的 很 清楚 极大 似 然 推导 对于 之前 
的 关于 分类 函数 h 代表 样 本属于 类别 1 
的 概率 问题 将此 式 综合 起来 即/v 综合/vn y/w 
=/i 1/m 和y=/nr 0/m 的/uj 情况/n 此/zg 式/k 稍加/d 观察/v 
可以/c 发现/v 和/c 上面/f 的/uj 式子/n 代表/n 的/uj 含义/n 是/v 
一样/r 的/uj y = 1时 1 h x 部分 就 
没有 了 y = 0时 h x 部门 就 没有 
了 取 似 然 函数 为 对数 似 然 函数 
为 最大 似 然 估计 就是 求 使 l θ 
取 最大值 时的θ/nr 其实 这里 可以 使用 梯度 上升 法 
求解 机器学习 实战 书中 即是 用 的 梯度 上升 法 
求得 的 θ 就是 要求 的 最佳 参数 但是 在 
Andrew Ng 的 课程 中将 J θ 取 为 下式 
即 因为 乘 了 一个 负 的 系数 1 / 
m 所以 取 J θ 最小值 时的θ/nr 为 要求 的 
最佳 参数 求 最大值 就 变为 了 求 最小值 梯度 
上升 法就/nr 变为 了 梯度 下 降法 θ 更新过程 可以 
写成 过拟合 问题 对于 线性 回归 或 逻辑 回归 的 
损失 函数 构成 的 模型 可能 会 有些 权重 很大 
有些 权重 很小 导致 过拟合 就是 过分 拟合 了 训练 
数据 使得 模型 的 复杂 度提高 泛化 能力 较差 对 
未知 数据 的 预测 能力 问题 的 主因 过拟合 问题 
往往 源自 过多 的 特征 解决 方法 1 减少 特征 
数量 减少 特征 会 失去 一些 信息 即使 特 征选 
的 很好 可用 人工选择 要 保留 的 特征 特征选择 算法 
2 正则化 特征 较多 时 比较 有效 保留 所有 特征 
但 减少 θ 的 大小 正规化 解决 过拟合 实例 首先 
多 说一句 正则 项 可以 取 不同 的 形式 在 
回归 问题 中 取 平方 损失 就是 参数 的 L2 
范数 也 可以 取 L1 范数 取 平方 损失 时 
模型 的 损失 函数 变为 其中 lambda 是 正则 项 
系数 如果 它 的 值 很大 说明 对 模型 的 
复杂度 惩罚 大 对 拟合 数据 的 损失 惩罚 小 
这样 它 就 不会 过分 拟合 数据 在 训练 数据 
上 的 偏差 较大 在 未知 数据 上 的 方差 
较小 但是 可能 出现 欠 拟合 的 现象 如果 它 
的 值 很小 说明 比较 注重 对 训练 数据 的 
拟合 在 训练 数据 上 的 偏差 会 小 但是 
可能 会 导致 过拟合 接下来 我们 看 斯坦福 机器学习 公开课 
中 的 一个 房价 问题 用 正规化 解决 过拟合 的 
例子 左图 是 适当 拟合 右图 是 过拟合 分析 得到 
过拟合 是 由于 其中 x 三次方 四次方 项的/nr 存在 引起 
的 造成 模型 过于 复杂 直观 上看 也就是 曲线 过于 
弯曲 如果 我们 想 解决 这个 例子 中的 过拟合 问题 
最好 能将 x 的 三次方 四次方 项的/nr 影响 消除 也 
就是 让 θ 3 和θ4/nr 尽可能 的 等于 0 假设 
我们 想 这样 做 的话 一个/m 简单/a 的/uj 办法/n 就是/d 
给/p 原有/v 的/uj Cost/w 函数/n 的/uj 相应/v 项/n 加上/v 两个/m 
略大/i 惩罚/vn 项/n 系数/n 例如 这样在 最小化 Cost 函数 的 
时候 正则化 后的/nr 梯度 下降 算法 θ 的 更新 变为 
逻辑 回归 实战 这 部分 主要 是 以 机器学习 实战 
中 的 例子 为 主导 自己 也 编写 了 整个 
算法 的 过程 首先是 加载 数据 部分 def loadDataSet dataMat 
= labelMat = fr = open testSet . txt for 
line in fr . readlines lineArr = line . strip 
. split dataMat . append 1.0 float lineArr 0 float 
lineArr 1 labelMat . append int lineArr 2 return dataMat 
labelMat/w 定义/n 了/ul 两个/m 数组/n 用于/v 存放/v 数据/n 和/c 类别/n 
加载/v 后即/i 基于/p 数据/n 和/c 真实/d 类别/n 进行/v 梯度/n 下/f 
降法/n 求取/v 最佳/z 的/uj 权重/n 向量/n def/w gradAscent dataMatIn classLabels 
dataMatrix = mat dataMatIn # convert to NumPy matrixlabelMat = 
mat classLabels . transpose # convert to NumPy matrixm n 
= shape dataMatrix alpha = 0 . 001maxCycles = 500weights 
= ones n 1 for k in range maxCycles # 
heavy on matrix operationsh = sigmoid dataMatrix * weights # 
matrix multerror = labelMat h # vector s u b 
t r a c t i o n w e 
i g h t s = weights + alpha * 
dataMatrix . transpose * error # matrix multreturn weights 首先 
将 数据 数组 转换 为 矩阵 方便 之后 的 计算 
alpha 是 步长 最 大循环 500次 权重 向量 初始 化为 
全1/nr 向量 将 数据 域 权重 向量 相乘 输入 sigmod 
函数 得到 分类 结果 此处 的 h 是 一个 向量 
真实 类别 向量 减去 预测 类别 向量 h 得到 分类 
的 错误 度 这也 是 一个 向量 然后 基于 这个 
错误 更新 权重 这里 的 停止 条件 设为 迭代 500次 
最后 会 得到 一个 权重 向量 而 最佳 分类 线 
方程 为 0 = wx 图形化 如下 改进 由于 上面 
的 方法 每次 迭代 都对 所有 的 样本 进行 计算 
计算 量过大 所以 一个 改进 的 方法 是 每次 迭代 
只对 一个 样本 进行 计算 更新 权重 也 基于 这 
个 样本 的 损失 函数 进行 更新 所以 稍加 改动 
代码 如下 def stocGradAscent0 dataMatrix classLabels m n = shape 
dataMatrix alpha = 0 . 01weights = ones n # 
initialize to all onesfor i in range m h = 
sigmoid sum dataMatrix i * weights error = classLabels i 
hweights = weights + alpha * error * dataMatrix i 
return weights 但是 结果 并 不太好 这 是 由于 每次 
用 一个 样本 进行 权重 的 更新 由于 有 一些 
样本 是 很难 正确 区分 的 样本 那 这些 样本 
会 在 每次 迭代 的 过程 中 引发 系数 也 
就是 权重 的 剧烈 波动 如图所示 一个 改进 的 方式 
是 步长 也 随着 迭代 的 次数 进行 改变 这样 
会 缓解 权重 的 波动 进一步 改进 代码 如下 def 
stocGradAscent1 dataMatrix classLabels numIter = 150 m n = shape 
dataMatrix weights = ones n # initialize to all onesfor 
j in range numIter dataIndex = range m for i 
in range m alpha = 4 / 1.0 + j 
+ i + 0.0001 # apha decreases with iteration does 
notrandIndex = int random . uniform 0 len dataIndex # 
go to 0 because of the constanth = sigmoid sum 
dataMatrix randIndex * weights error = classLabels randIndex hweights = 
weights + alpha * error * dataMatrix randIndex del dataIndex 
randIndex return weights/w 另外/c 选择/v 样本/n 时/n 不是/c 第/m i/w 
次/q 迭代/v 时/n 选择/v 第/m i/w 个/q 样本/n 而是 每次 
迭代 都 随机 选择 样本 这样 可以 减少 周期性 波动 
结果 也 很 理想 与 最 开始 用 全部 数据 
进行 权重 更新 时的/nr 结果 差不多 但 计算 量 明显 
少于 前面 的 方法 