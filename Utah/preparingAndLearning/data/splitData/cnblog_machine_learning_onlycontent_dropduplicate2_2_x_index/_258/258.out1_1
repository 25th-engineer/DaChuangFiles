为什么 一些 机器学习 模型 需要 对 数据 进行 归一化 http 
/ / www . cnblogs . com / LBSer / 
p / 4440590 . html 机器学习 模型 被 互联网 行业 
广泛应用 如 排序 参见 排序 学习 实践 推荐 反作弊 定位 
参见 基于 朴素 贝叶斯 的 定位 算法 等 一般 做 
机器学习 应用 的 时候 大 部分 时间 是 花费 在 
特征 处理 上 其中 很 关键 的 一步 就是 对 
特征 数据 进行 归一化 为什么 要 归一化 呢 很多 同学 
并未 搞清楚 维基百科 给出 的 解释 1 归一化/l 后/f 加快/v 
了/ul 梯度/n 下降/v 求/v 最优/d 解的/nr 速度/n 2 归一化 有 
可能 提 高精度 下面 我 简单 扩展 解释 下 这两点 
1 归一化/l 为什么/r 能/v 提高/v 梯度/n 下/f 降法/n 求解/v 最优/d 
解的/nr 速度/n 斯坦福 机器学习 视频 做了 很好 的 解释 https 
/ / class . coursera . org / ml 003 
/ lecture / 21 如下 图 所示 蓝色 的 圈圈 
图 代表 的 是 两个 特征 的 等高线 其中 左图 
两个 特征 X1 和 X2 的 区间 相差 非常大 X1 
区间 是 0 2000 X2 区间 是 1 5 其所 
形成 的 等高线 非常 尖 当 使用 梯度 下 降法 
寻求 最优 解时/nr 很 有可能 走 之 字型 路线 垂直 
等高线 走 从而 导致 需要 迭代 很多 次 才能 收敛 
而 右图 对 两个 原始 特征 进行 了 归一化 其 
对应 的 等高线 显得 很圆 在 梯度 下降 进行 求解 
时能 较快 的 收敛 因此 如果 机器学习 模型 使用 梯度 
下 降法 求 最优 解时/nr 归一化 往往 非常 有 必要 
否则 很难 收敛 甚至 不能 收敛 2 归一化 有 可能 
提 高精度 一些 分类器 需要 计算 样本 之间 的 距离 
如 欧氏距离 例如 KNN 如果 一个 特征 值域 范围 非常 
大 那么 距离 计算 就 主要 取决于 这个 特征 从而 
与 实际 情况 相悖 比如 这时 实际 情况 是 值域 
范围 小 的 特征 更重要 3 归一化 的 类型 1 
线性 归一化 这种 归一化 方法 比较 适 用在 数值 比较 
集中 的 情况 这种方法 有个 缺陷 如果 max 和 min 
不稳定 很容易 使得 归一化 结果 不 稳定 使得 后续 使用 
效果 也 不稳定 实际/n 使用/v 中/f 可以/c 用/p 经验/n 常/d 
量值/n 来/v 替代/n max/w 和/c min/w 2 标准差 标准化 经过 
处理 的 数据 符合标准 正态分布 即 均值 为 0 标准差 
为 1 其 转化 函数 为 其中 μ 为 所有 
样本数据 的 均值 σ 为 所有 样本数据 的 标准差 3 
非线性 归一化 经常 用 在 数据 分化 比 较大 的 
场景 有些 数值 很大 有些 很小 通过 一些 数学 函数 
将 原始 值 进行 映射 该 方法 包括 log 指数 
正切 等 需要 根据 数据 分布 的 情况 决定 非线性 
函数 的 曲线 比如 log V 2 还是 log V 
10 等 