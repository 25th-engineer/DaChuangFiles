快速 寻找 最优 解 基础知识 通过 上文   我们 知道 
了 如果 盲目 使用 随机 算法 或者 遍历 算法 寻找 
最优 解 的话   需要 计算 的 空间 将会 太大 
. 为了 能够 让 大家 直观 的 感受 一下 实际 
应用 的 计算 量 我 这里 再 举个 例子 1997年 
5月 11日 IBM 的 深蓝 AI 战胜 卡 国际象棋 名家 
斯帕 罗夫 . 我们 知道 围棋 的 棋盘 是 19路 
总共 361 格 如果 计算机 需要 计算 10步 则 需要 
计算 的 状态 数量 为 361 ^ 10 = 3 
7 5 8 9 9 7 3 4 5 7 
5 4 5 9 5 8 1 9 3 3 
5 5 6 0 1 个   而 前文 我们 
提到 的当 步 长为 0.01时 需要 计算 的 状态 数量 
为 160000 耗时 约 5秒 而 刚才 提到 的 10步 
需要 的 计算 量 约为 上文 160000 的 2 3 
4 9 3 7 3 3 4 1 0 9 
6 6 2 2 3 8 7 0 8 倍 
. 不难看出 如果 不 使用 一定 的 优化 技巧 使用暴力 
穷举 来 计算 实际 任务 性能 上将 很 容量 成为 
瓶颈 . 相对 遍历 或者 穷举 的 方法 梯度 下 
降法 gradient descent 就是 这样 一种 又 快又准 原理 简单 
又 容易 实现 的 优化 算法 所谓 的 梯度 下 
降法 就是 让 数学 模型 的 参数 coefficient 沿着 梯度 
的 负 方向 进行 快速 迭代 下降 的 过程 该 
过程 会 一直 迭代 下去 直到 误差 的 范围 符合 
用户 的 要求 成功 创建 模型 或者 迭代 次数 达到 
上 限时 失败 在 正式 介绍 梯度 下降 算法 之前 
本章 先 梳理 一下 梯度 下降 法会 涉及到 的 基本 
概念 导数 通常 描述 的 是 在 只有 一个 自变量 
一维 的 函数 空间 内 的 某一 点 随着 函数 
自变量 变化 时 函数 本身 的 变化 率 敏感 程度 
导数 的 计算 结果 为 一个 标量 也 就是 一个 
数值 值 越大 单位 长度 的 自变量 变化 带来 的 
影响 越大 直观 的 几何 意思 是 函数 在某 一点 
的 斜率 当 函数 有 多个 自变量 时 多元 函数 
也叫 多维 函数 由于 求导 可以 针对 不同 的 自变量 
进行 所以 针对 只有 某个 自变量 进行 变化 的 情况 
下 求解 的 导数 被 称为 偏 导数 偏 导数 
描述 的 是 函数 仅 沿着 坐标轴 的 某个 方向 
进行 变化 时的/nr 函数 变化率 既然 函数 可以 沿着 坐标轴 
的 某个 方向 进行 求导 那么 自然 也 可以 对 
沿着 其它 方向 变化 的 函数 进行 求导 比如 沿着 
与 X 轴 成 45度 夹角 的 方向 在 这个 
情况 下 求 出来 的 导数 我们 称之为 方向 导数 
也 就是说 方向 导数 为 多元 多项式 中 比如 二元 
二次函数 某个 点 沿着 某个 向量 方向 进行 求导 后 
得到 的 值 二元 函数 的 自变量 可以 沿着 x 
y 方向 变化 三元 函数 则 可以 沿 x y 
z 方向 进行 求导 梯度 为 多元 函数 的 偏 
导数 组成 的 一个 向量 是 一维 导数 在 高维空间 
的 推广 generalization 同样 描述 的 是 函数 随着 自变量 
进行 变化 时的/nr 变化 率 只是 这个 时候 的 自变量 
由 一维 一个 变成 了 多维 多个 自变量 沿着 梯度方向 
进行 求导 时 导数 值 最大 也 就是说 函数 在 
某个 点 沿着 梯度方向 变化 时 函数 的 值 变化 
最快 如下 图 所示 左边 的 山 是由 函数 f 
x y 代表 高度 的 情况 下 绘制 出来 的 
右边 是 这座 山在/nr 二维 平面 的 投影 形成 的 
等高线图 右图 中的 红点 可以 沿着 任意 方向 前进 但是 
我们 可以 发现 不同 方向 的 高度 变化 是 不 
一样 的 红点 的 箭头 方向 是 梯度方向 也是 高度 
变化 最 剧烈 的 方向 再如 下图 为 一个二元 二次函数 
  f x y = − cos2x   + cos2y 
2 图 中的 X Y 轴 组成 的 平面 上 
的 红色 箭头 的 方向 就是 梯度 的 方向 箭头 
的 长度 由 函数 在 这个 点上 沿 梯度方向 求导 
计算 出来 的 总结 一下 导数 是 一个 数值 代表 
了 函数 在 某个 点 的 变化 率 梯度 一个 
特殊 的 方向 因为 它 的 方向 是由 函数 在 
某个 点 的 偏 导数 决定 的 知道 了 梯度 
的 相关 概念 我们 就 直接 来 说说 简单 版 
的 梯度 下降 算法 也叫 最速 下 降法   Gradient 
descent 1 首先 对于 任意 一个 函数 我们 都 可以 
随机 选择 一个 点 作为 我们 的 起始 点 X 
_ 0 2 为了 保证 函数 F X 在朝 某个 
方向 移动   单位 步长 距离 时 /nr 使 函数值 变 
的 尽可能 的 小 我们 需要 沿着 梯度 负 方向 
的 进行 移动 .   沿着 梯度方向 移动 了 一定 
长度 后 我们 得到 了 点 X _ 1 与 
F X _ 1 . 注意 只有 在 移动 的 
距离 足够 小时 我们 才能 保证 移动 后的点/nr X _ 
1 的 函数值 F X _ 1 小于 F X 
_ 0 .   因为 导数 针对 的 是 某个 
点来 计算 的 不同点 的 导数 不一样   所以 步长 
不宜 设 的 太长 3 计算 F X _ 1 
与 F X _ 0 的 差值 判断 单位 步长 
的 移动 带来 的 函数值 的 变化 是否 已经 小于 
一个 我们 预设 的 非常 小 的 一个 值 如果 
小于 的话 那就 不 需要 再 移动 了 因为 继续移动 
的话 函数值 F X 的 变化 也 只会 是 非常 
小 的 如果 本次 移动 带来 的 函数 值 变化 
非常 大 则 说明 算法 还有 进一步 迭代 的 必要 
则 重复 第二步 和/c 第三/m 步/n 接着/c 我们/r 来说/u 说/v 
一个/m 实例/n 因为/c 例子/n 比较简单/l 我们 在 应用 梯度 算法 
之前 先 使用 在 高中 学过 的 令 导数 等于 
0 计算 驻点 极值 点 的 方法 求 $ $ 
f x = x ^ 4 3x ^ 3 + 
2 $ $ 的 最小值 $ $ f x = 
4x ^ 3 9x ^ 2 = x ^ 2 
4x 9 $ $ 令 f x = 0 可得 
驻点 $ $ x _ 1 = 0 x _ 
2 = 2.25 $ $ 将 驻点 代回 方程 可得 
最大值 2 与 最小值   6.54296875 将 刚才 说 的 
算法 写成 PYTHON 脚本 # * coding cp936 * xOld 
= 0 xNew = 1 # The algorithm starts at 
x = 6 moveDistance = 0.01 # step size precision 
= 0.00001 def evaluateEquation x 计算 原函数 在 自变量 为 
x 时的值/nr return x * * 4 3 * x 
* * 3 + 2 def evaluateGradient x 计算 梯度 
向量 return 4 * x * * 3 9 * 
x * * 2 step = 0 while abs evaluateEquation 
xNew evaluateEquation xOld precision step + = 1 xOld = 
xNew xNew = xOld moveDistance * evaluateGradient xNew print step 
step xold = xOld xnew = xNew print Local minimum 
occurs at xNew with minimum value evaluateEquation xNew print step 
step 运行 后 得到 step 1 xold = 1 xnew 
= 1.05 step 2 xold = 1.05 xnew = 1.10292 
step 3 xold = 1.10292 xnew = 1.1587338169 step 4 
xold = 1.1587338169 xnew = 1.21734197218 step 5 xold = 
1.21734197218 xnew = 1.27855469659 step 6 xold = 1.27855469659 xnew 
= 1.34207564416 step 7 xold = 1.34207564416 xnew = 1.40748858095 
step 8 xold = 1.40748858095 xnew = 1.47424999816 step 9 
xold = 1.47424999816 xnew = 1.54169100548 step 10 xold = 
1.54169100548 xnew = 1.60903167429 step 11 xold = 1.60903167429 xnew 
= 1.67540991642 step 12 xold = 1.67540991642 xnew = 1.73992485396 
step 13 xold = 1.73992485396 xnew = 1.80169165901 step 14 
xold = 1.80169165901 xnew = 1.85990167873 step 15 xold = 
1.85990167873 xnew = 1.91387933776 step 16 xold = 1.91387933776 xnew 
= 1.96312685144 step 17 xold = 1.96312685144 xnew = 2.00734969025 
step 18 xold = 2.00734969025 xnew = 2.04645960885 step 19 
xold = 2.04645960885 xnew = 2.08055667019 step 20 xold = 
2.08055667019 xnew = 2.10989555269 step 21 xold = 2.10989555269 xnew 
= 2.13484344301 step 22 xold = 2.13484344301 xnew = 2.15583674372 
step 23 xold = 2.15583674372 xnew = 2.17334219049 step 24 
xold = 2.17334219049 xnew = 2.1878256603 step 25 xold = 
2.1878256603 xnew = 2.19972976112 step 26 xold = 2.19972976112 xnew 
= 2.20945968856 step 27 xold = 2.20945968856 xnew = 2.21737593374 
step 28 xold = 2.21737593374 xnew = 2.22379211672 step 29 
xold = 2.22379211672 xnew = 2.22897629956 step 30 xold = 
2.22897629956 xnew = 2.23315441132 step 31 xold = 2.23315441132 xnew 
= 2.23651475494 step 32 xold = 2.23651475494 xnew = 2.23921288183 
step 33 xold = 2.23921288183 xnew = 2.24137637832 step 34 
xold = 2.24137637832 xnew = 2.24310930133 step 35 xold = 
2.24310930133 xnew = 2.24449613419 step 36 xold = 2.24449613419 xnew 
= 2.24560522103 step 37 xold = 2.24560522103 xnew = 2.24649169063 
step 38 xold = 2.24649169063 xnew = 2.24719990952 step 39 
xold = 2.24719990952 xnew = 2.24776551743 step 40 xold = 
2.24776551743 xnew = 2.24821710187 step 41 xold = 2.24821710187 xnew 
= 2.2485775668 step 42 xold = 2.2485775668 xnew = 2.24886524544 
Local minimum occurs at 2 . 2488652454412037 with minimum value 
6 . 542955721127889 step 42 可以 看到 脚本 进行 了 
42次 迭代 后 得到 了 最小值 6.54295572   x = 
2.248 与 使用 手工 计算 出来 的 最小值 6.54296875 的 
误差 为 0.00001303 那么 现在 你 心中 的 疑问 可能 
是 1 . 既然 令 导数 为零 可以 直接 求解 
为什么 不 直接 让 电脑 计算 令 导数 为零 的 
方程式 答案 是   答案 在前 两篇 的 文章 我 
眼中 的 机器学习 二 中 已经 提 过了 当 样本数据 
中有 噪点 时 或者 不可解 时 标准/n 解方程/nr 的/uj 方式/n 
无用/v ./i 当然/d 基于/p 矩阵/n 思想/n 的/uj 最小二乘/i 法/l 通过/p 
计算/v 高维空间/i 中/f 的/uj 一个/m 向量/n 在/p 低维/i 子空间/i 的/uj 
投影/n 也是/i 可/v 以求/v 解出/v 误差/n 最小/a 的/uj 解的/nr 但是 
计算 一个 矩阵 的 逆 是 相当 耗费 时间 的 
所以 许多 时候 我们 会 更 倾向 于 使用 计算 
量 并 不算 大 的 常规 的 最优化 算法 比如 
梯度 下降 牛顿 法等./nr 2 . 如何 为 梯度 下降 
算法 选择 合适 的 步长 多大 的 步长 变化 是 
最 有效率 的 关于 这 一点 我 会 在下 一章 
进一步 说明 . http / / mathinsight . org / 
directional _ derivative _ gradient _ i n t r 
o d u c t i o n N y 
k a m p DQ Directional derivative on a mountain 
. From   Math Insight .   http / / 
mathinsight . org / applet / directional _ derivative _ 
mountainKeywords directional derivative 