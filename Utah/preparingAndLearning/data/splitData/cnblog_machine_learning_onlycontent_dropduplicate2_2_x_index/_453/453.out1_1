基础 概念 超 参数 是 在 开始 学习 过程 之前 
设置 值 的 参数 而 不是 通过 训练 得到 的 
参数 数据 通常 情况下 在 机器 学习 过程 中 需要 
对 超 参数 进行 优化 给 学习 器 选择 一组 
最优 超 参数 以 提高 学习 的 性能 和 效果 
比如 树 的 数量 或 树 的 深度 学习率 多种 
模式 以及 k 均值 聚 类 中的 簇 数等 都是 
超 参数 与 超 参数 区别 的 概念 是 参数 
它 是 模型 训练 过程 中 学习 到 的 一部分 
比如 回归系数 神经网络 权重 等 简单 的 描述 参数 是 
模型 训练 获得 的 超 参数 是 人工 配置 参数 
本质上 是 参数 的 参数 每次 改变 超 参数 模型 
都要/nr 重新 训练 超 参数 调 优 在 模型 训练 
过程 中 的 参数 最优化 一般 都是对/nr 参数 的 可能 
值 进行 有效 搜索 然后 用 评价 函数 选 取出 
最优 参数 比如 梯度 下 降法 同理 人工 的 超 
参数 选择 过程 我们 也 可以 采取 类似 参数 搜索 
的 办法 来 提高 效率 如果 进行 人工 试错 的 
方式 会 非常 浪费时间 超 参数 搜索 过程 将 数据集 
分为 训练 集 验证 集 及 测试 集 选择 模型 
性能评价 指标 用 训练 集 对模型 进行 训练 在 验证 
集上 对模型 进行 参数 进行 搜索 用 性能 指标 评价 
参数 好坏 选出 最优 参数 常见 超 参数 搜索算法 网格 
搜索 随机搜索 启发式 搜索 网格 搜索 网格 搜索 是 在 
所有 候选 的 参数 选择 中 通过 循环 遍历 尝试 
每一种 可能性 表现 最好 的 参数 就是 最终 的 结果 
暴力 搜索 原理 在 一定 的 区间 内 通过 循环 
遍历 尝试 每一种 可能性 并 计算 其 约束 函数 和 
目标 函数 的 值 对 满足 约束 条件 的 点 
逐个 比较 其 目标 函数 的 值 将 坏 的 
点 抛弃 保留 好 的 点 最后 便 得到 最优 
解的/nr 近似解 为了 评价 每次 选出 的 参数 的 好坏 
我们 需要 选择 评价 指标 评价 指标 可以 根据 自己 
的 需要 选择 accuracy f1 score f beta percision recall 
等 同时 为了 避免 初始 数据 的 划分 对 结果 
的 影响 我们 需要 采用 交叉 验证 的 方式 来 
减少 偶然性 一般/a 情况/n 下/f 网格/n 搜索/v 需要/v 和/c 交叉/n 
验证/v 相/v 结合/v 使用/v python 的 sklearn 包中/nr 网格 搜索 
函数 GridSearchCV GridSearchCV estimator param _ grid scoring = None 
fit _ params = None n _ jobs = 1 
iid = True refit = True cv = None verbose 
= 0 pre _ dispatch = 2 * n _ 
jobs error _ score = raise return _ train _ 
score = True estimator 所 使用 的 分类器 param _ 
grid 值 为 字典 或者 列表 需要 最 优化 的 
参数 的 取值 范围 如 paramters = { n _ 
estimators range 10 100 10 } scoring 准确度 评价 指标 
默认 None 这时 需要 使用 score 函数 或者 如 scoring 
= roc _ auc fit _ params 字典 类型 数据 
主要 用于 给 fit 方法 传递 参数 n _ jobs 
并 行数 int 个数 1 跟 CPU 核 数 一致 
1 默认值 pre _ dispatch 指定 总共 分发 的 并行 
任务 数 当 n _ jobs 大于 1时 数据 将 
在 每个 运行 点 进行 复制 这 可能 导致 OOM 
而 设置 pre _ dispatch 参数 则 可以 预先 划分 
总共 的 job 数量 使 数据 最多 被 复制 pre 
_ dispatch 次 iid 默认 True 为 True 时 默认 
为 各个 样本 fold 概率分布 一致 误差 估计 为 所有 
样本 之和 而非 各个 fold 的 平均 cv 交叉 验证 
参数 默认 None 使用 三折 交叉 验证 指定 fold 数量 
默认 为 3 也 可以 是 yield 训练 / 测试数据 
的 生成器 refit 默认 为 True 程序 将会 以 交叉 
验证 训练 集 得到 的 最佳 参数 重新 对 所有 
可用 的 训练 集 与 验证 集 进行 训练 作为 
最终 用于 性能 评估 的 最佳 模型 参数 即在 搜索 
参数 结束 后 用 最佳 参数 结果 再次 fit 一遍 
全部 数据集 verbose 日志 冗长度 int 冗长度 0 不 输出 
训练 过程 1 偶尔 输出 1 对 每个 子模型 都 
输出 error _ score 默认 为 raise 可选择 参数 numeric 
在 模型 拟合 过程 中 如果 产生 误差 在 raise 
情况下 误差 分数 将会 提高 如果 选择 numeric 则 fitfailedwarning 
会 提高 return _ train _ score 布尔 类型 数据 
默认 为 Ture 为 False 时 交叉 验证 的 结果 
不 包含 训练 得分 网格 搜索 python 简单 实现 import 
pandas as pd from sklearn import datasets from sklearn . 
model _ selection import GridSearchCV from sklearn . metrics import 
classification _ report from xgboost . sklearn import XGBClassifier iris 
= datasets . load _ iris parameters = { n 
_ estimators range 100 150 10 max _ depth range 
3 5 1 } xgc = XGBClassifier clf = GridSearchCV 
xgc parameters clf . fit iris . data iris . 
target cv _ result = pd . DataFrame . from 
_ dict clf . cv _ results _ best _ 
param = clf . best _ params _ best _ 
score = clf . best _ score _ y _ 
pred = clf . predict iris . data print classification 
_ report y _ true = iris . target y 
_ pred = y _ pred 随机搜索 随机搜索 random search 
是 利用 随机 数去 求函数 近似 的 最优 解的/nr 方法 
区别于 网格 搜索 的 暴力 搜索 方式 原理 在 一定 
的 区间 内 不断 随机 地 而 不是 有 倾向性 
产生 随机 点 并 计算 其 约束 函数 和 目标 
函数 的 值 对 满足 约束 条件 的 点 逐个 
比较 其 目标 函数 的 值 将 坏 的 点 
抛弃 保留 好 的 点 最后 便 得到 最优 解的/nr 
近似解 这种 方法 是 建立 在 概率论 的 基础 上 
所取 随机 点 越多 则 得到 最优 解的/nr 概率 也 
就 越大 这种 方法 存在 精度 较差 的 问题 找到 
近似 最优 解的/nr 效率 高于 网格 搜索 随机搜索 一般 用于 
粗选 或 普查 python 的 sklearn 包中/nr 随机搜索 函数 R 
a n d o m i z e d e 
a r c h C V R a n d 
o m i z e d e a r c 
h C V estimator param _ distributions n _ iter 
= 10 scoring = None fit _ params = None 
n _ jobs = 1 iid = True refit = 
True cv = None verbose = 0 pre _ dispatch 
= 2 * n _ jobs random _ state = 
None error _ score = raise estimator 所 使用 的 
分类器 param _ distributions 值 为 字典 或者 列表 需要 
最 优化 的 参数 的 取值 范围 同时 需要 选择 
一种 rvs 方法 来 进行 抽样 比如 scipy . stats 
. distributionsn _ iter 抽样 参数 默认 为 10 具体 
的 值 选择 需要 根据 模型 的 相应 效果 进行 
评估 scoring 准确度 评价 指标 默认 None 这时 需要 使用 
score 函数 或者 如 scoring = roc _ auc fit 
_ params 字典 类型 数据 主要 用于 给 fit 方法 
传递 参数 n _ jobs 并 行数 int 个数 1 
跟 CPU 核 数 一致 1 默认值 pre _ dispatch 
指定 总共 分发 的 并行 任务 数 当 n _ 
jobs 大于 1时 数据 将 在 每个 运行 点 进行 
复制 这 可能 导致 OOM 而 设置 pre _ dispatch 
参数 则 可以 预先 划分 总共 的 job 数量 使 
数据 最多 被 复制 pre _ dispatch 次 iid 默认 
True 为 True 时 默认 为 各个 样本 fold 概率分布 
一致 误差 估计 为 所有 样本 之和 而非 各个 fold 
的 平均 cv 交叉 验证 参数 默认 None 使用 三折 
交叉 验证 指定 fold 数量 默认 为 3 也 可以 
是 yield 训练 / 测试数据 的 生成器 refit 默认 为 
True 程序 将会 以 交叉 验证 训练 集 得到 的 
最佳 参数 重新 对 所有 可用 的 训练 集 与 
验证 集 进行 训练 作为 最终 用于 性能 评估 的 
最佳 模型 参数 即在 搜索 参数 结束 后 用 最佳 
参数 结果 再次 fit 一遍 全部 数据集 verbose 日志 冗长度 
int 冗长度 0 不 输出 训练 过程 1 偶尔 输出 
1 对 每个 子模型 都 输出 random _ state 随机 
种子 默认 为 None int 类型 或者 随 机状态 实例 
伪随机数 生成器 状态 用于 从 可能 的 值 列表 而不是 
scipy 中 随机 抽样 统计 分布 如果 int 随机 状态 
是 随机数 生成器 所 使用 的 种子 如果 随 机状态 
实例 随机 状态 是 随机数 生成器 如果 没有 随机数 生成器 
是 np . random 所 使用 的 随机 状态 实例 
error _ score 默认 为 raise 可选择 参数 numeric 在 
模型 拟合 过程 中 如果 产生 误差 在 raise 情况下 
误差 分数 将会 提高 如果 选择 numeric 则 fitfailedwarning 会 
提高 随机搜索 python 简单 实现 import pandas as pd from 
sklearn import datasets from sklearn . model _ selection import 
GridSearchCV R a n d o m i z e 
d e a r c h C V from sklearn 
. metrics import classification _ report from xgboost . sklearn 
import XGBClassifier iris = datasets . load _ iris parameters 
= { n _ estimators range 100 150 10 max 
_ depth range 3 5 1 } xgc = XGBClassifier 
clf = R a n d o m i z 
e d e a r c h C V xgc 
parameters cv = 5 clf . fit iris . data 
iris . target cv _ result = pd . DataFrame 
. from _ dict clf . cv _ results _ 
best _ param = clf . best _ params _ 
best _ score = clf . best _ score _ 
y _ pred = clf . predict iris . data 
print classification _ report y _ true = iris . 
target y _ pred = y _ pred 启发式 搜索 
启发式 搜索 Heuristically Search 又 称为 有 信息 搜索 Informed 
Search 它 是 利用 问题 拥有 的 启发 信息 来 
引导 搜索 达到 减少 搜索 范围 降低 问题 复杂度 的 
目的 这种 利用 启发 信息 的 搜索 过程 称为 启发式 
搜索 原理 在 状态 空间 中 的 搜索 对 每一个 
搜索 的 位置 进行 评估 得到 最好 的 位置 再 
从 这个 位置 进行 搜索 直到 目标 这样 可以 省略 
大量 无谓 的 搜索 路径 提高 了 效率 在 启发式 
搜索 中 对 位置 的 估价 是 十分 重要 的 
采用 了 不同 的 估价 可以 有 不同 的 效果 
启发式 搜 索有 模拟 退火算法 SA 遗传算法 GA 列表 搜索算法 
ST 进化 规划 EP 进化 策略 ES 蚁群算法 ACA 人工神经网络 
ANN . . . 等 启发式 搜索 非常 多样化 而且在 
sklearn 包中并/nr 没有 现成 的 函数 如果 有 需要 我们 
可以 针对 某 一种 启发式 算法 的 实现 过程 进行 
了解 然后 用 python 手动 实现 这里 我们 着手 对 
遗传算法 的 实现 过程 做些 了解 遗传算法 遗传算法 Genetic Algorithm 
是 一种 通过 模拟 自然 进化 过程 搜索 最优 解的/nr 
方法 它 的 思想 来自于 进化论 生物 种群 具有 自我 
进化 的 能力 能够 不断 适应 环境 优势 劣汰 之后 
得到 最优 的 种群 个体 进化 的 行为 主要 有 
选择 遗传 变异 遗传算法/n 希望/v 能够/v 通过/p 将/d 初始/v 解/v 
空间/n 进化/v 到/v 一个/m 较好/i 的/uj 解/v 空间/n 原理 遗传算法 
是从 代表 问题 可能 潜在 的 解集 的 一个 种群 
population 开始 的 而 一个 种群 则由 经过 基因 gene 
编码 的 一定 数目 的 个体 individual 组成 每个 个体 
实际上 是 染色体 chromosome 带有 特征 的 实体 染色体 作为 
遗传物质 的 主要 载体 即 多个 基因 的 集合 其 
内部 表现 即 基因型 是 某种 基因组合 它 决定 了 
个体 的 形状 的 外部 表现 如 黑 头发 的 
特征 是由 染色体 中 控制 这 一 特征 的 某种 
基因组合 决定 的 因此 在 一 开始 需要 实现 从 
表现型 到 基因型 的 映射 即 编码 工作 由于 仿照 
基因 编码 的 工作 很 复杂 我们 往往 进行 简化 
如 二进制 编码 初 代 种群 产生 之后 按照 适者生存 
和 优胜劣汰 的 原理 逐 代 generation 演化 产生 出 
越来越 好 的 近似解 在 每 一代 根据 问题 域中 
个体 的 适应度 fitness 大小 选择 selection 个体 并 借助 
于 自然 遗传学 的 遗传 算子 genetic operators 进行 组合 
交叉 crossover 和 变异 mutation 产生 出 代表 新的 解集 
的 种群 这个/r 过程/n 将/d 导致/v 种/m 群像/n 自然/d 进化/v 
一样/r 的/uj 后生/n 代/q 种群/n 比/p 前代/t 更加/d 适应/v 于/p 
环境/n 末代 种群 中的 最优 个体 经过 解码 decoding 可以 
作为 问题 近似 最优 解 遗传算法 的 实现 步骤 初始化 
候选 参 数集 并 编码 为 基因序列 初始化 种群 一组 
参数 编码 为 一个 种群 个体 共 n 个 种群 
个体 即对 参数 进行 编码 同时 设定 进化 代数 m 
个体 评估 计算 各个 种群 个体 的 适应度 适应度 描述 
了 该 个体 对 自然环境 的 适应 能力 表征 了 
其 个体 存活 能力 和 生殖 机会 用 适应度 函数 
表示 选择 运算 选择 是 模拟 自然选择 把 优秀 的 
个体 选择 出来 基于 适应度 以 进行 后续 的 遗传 
和 变异 交叉 运算 交叉 是 模拟 繁殖 后代 的 
基因 重组 变异 运算 变异 是 模拟 基因突变 经过 选择 
交叉 变异 生产 下一代 群体 重复 此 过程 直到 停止 
条件 从 上面 的 实现 步骤 可以 知道 遗传算法 包含 
以下 几个 主要 部分 基因 编码 适应度 函数 遗传 算子 
包含 选择 交叉 变异 有了 基本 遗传算法 的 实现 过程 
后续 就是 对 每个 算法 实现 过程 的 细节 寻找 
合适 的 方法 进行 处理 换到 机器学习 参数 最 优化 
的 问题 上 就是 使用 遗传算法 搜索 参数 空间 获得 
最优 的 模型 性能评价 指标 的 过程 启发式 搜索 先到 
这里 后续 有 时间 可以 尝试 python 代码 实现 参数 
最 优化 的 过程 