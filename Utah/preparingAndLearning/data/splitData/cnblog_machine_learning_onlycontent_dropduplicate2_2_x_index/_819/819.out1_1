转 自 http / / www . cnblogs . com 
/ zhizhan / p / 4432943 . html 决策树 一 
  决策树 优点 1 决策树 易于 理解 和 解释 可以 
可视化 分析 容易 提取 出 规则 2 可以 同时 处理 
标称 型 和 数值 型 数据 3 测试 数据集 时 
运行 速度 比较 快 4 决策树 可以 很好 的 扩展 
到 大型 数据库 中 同时 它 的 大小 独立 于 
数据库 大小 二 决策树 缺点 1 对 缺失 数据 处理 
比较 困难 2 容易 出现 过 拟合 问题 3 忽略 
数据 集中 属性 的 相互 关联 4 ID3 算法 计算 
信息 增益 时 结果 偏向 数值 比 较多 的 特征 
三 改进 措施 1 对 决策树 进行 剪枝 可以 采用 
交叉 验证法 和 加入 正则化 的 方法 2 使用 基于 
决策树 的 combination 算法 如 bagging 算法 randomforest 算法 可以 
解决 过拟合 的 问题 三 应用 领域 企业 管理 实践 
企业 投资 决策 由于 决策树 很好 的 分析 能力 在 
决策 过程 应用 较多 KNN 算法 一 KNN 算法 的 
优点 1 KNN 是 一种 在线 技术 新 数据 可以 
直接 加入 数据集 而 不必 进行 重新 训练 2 KNN 
理论 简单 容易 实现 二 KNN 算法 的 缺点 1 
对于 样本 容量大 的 数据集 计算 量 比较 大 2 
样本 不 平衡 时 预测 偏差 比较 大 如 某 
一类 的 样本 比较 少 而 其它 类 样本 比较 
多 3 KNN 每一次 分类 都会 重新 进行 一次 全局 
运算 4 k 值 大小 的 选择 三 KNN 算法 
应用领域 文本 分类 模式识别 聚类分析 多 分类 领域 支持 向量 
机 SVM 一   SVM 优点 1 解决 小 样本 
下 机器学习 问题 2 解决 非线性 问题 3 无 局部 
极小值 问题 相对于 神经 网络 等 算法 4 可以 很好 
的 处理 高维 数据集 5 泛化 能力 比较 强 二 
SVM 缺点 1 对于 核 函数 的 高维 映射 解释力 
不强 尤其 是 径向 基 函数 2 对 缺失 数据 
敏感 三 SVM 应用领域 文本 分类 图像识别 主要 二 分类 
领域 AdaBoost 算法 一   AdaBoost 算法 优点 1 很好 
的 利用 了 弱 分类器 进行 级联 2 可以 将 
不同 的 分类 算法 作为 弱 分类器 3 AdaBoost 具有 
很高 的 精度 4 相对于 bagging 算法 和 Random Forest 
算法 AdaBoost 充分 考虑 的 每个 分类器 的 权重 二 
Adaboost 算法 缺点 1 AdaBoost 迭代 次数 也 就是 弱 
分类器 数目 不太好 设定 可以 使用 交叉 验证 来 进行 
确定 2 数据 不 平衡 导致 分类 精度 下降 3 
训练 比较 耗时 每次 重新 选择 当前 分类器 最好 切 
分点 三 AdaBoost 应用领域 模式识别 计算机 视觉 领域 用于 二 
分类 和多/nr 分类 场景 朴素 贝叶斯 算法 一   朴素 
贝叶斯 算法 优点 1 对/p 大/a 数量/n 训练/vn 和/c 查询/v 
时/n 具有/v 较高/i 的/uj 速度/n 即使 使用 超 大 规模 
的 训练 集 针对 每 个 项目 通常 也 只会 
有 相对 较少 的 特征 数 并且 对 项目 的 
训练 和 分类 也 仅仅 是 特征 概率 的 数学 
运算 而已 2 支持 增量式 运算 即 可以 实时 的 
对 新增 的 样本 进行 训练 3 朴素 贝叶斯 对 
结果 解释 容易 理解 二 朴素 贝叶斯 缺点 1 由于 
使用 了 样本 属性 独立性 的 假设 所以/c 如果/c 样本/n 
属性/n 有/v 关联/ns 时其/nr 效果/n 不好/d 三 朴素 贝叶斯 应用领域 
文本 分类 欺诈 检测 中 使用 较多 Logistic 回归 算法 
一 logistic 回归 优点 1 计算 代价 不高 易于 理解 
和 实现 二 logistic 回归 缺点 1 容易 产生 欠 
拟合 2 分类 精度 不高 三 logistic 回归 应用领域 用于 
二 分类 领域 可以 得出 概率值 适用于 根据 分类 概率 
排名 的 领域 如 搜索 排名 等 Logistic 回归 的 
扩展 softmax 可以 应用于 多 分类 领域 如 手 写字 
识别 等 人工神经网络 一   神经网络 优点 1 分类 准确度 
高 学习 能力 极强 2 对 噪声 数据 鲁棒性 和 
容错性 较强 3 有 联想 能力 能 逼近 任意 非 
线性关系 二 神经网络 缺点 1 神经网络 参数 较多 权值 和 
阈值 2 黑盒 过程 不能 观察 中间 结果 3 学习 
过程 比较 长 有 可能 陷入 局部 极小值 三 人工神经网络 
应用领域 目前 深度 神经 网络 已经 应用 与 计算机 视觉 
自然语言 处理 语音 识别 等 领域 并 取得 很好 的 
效果 = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = 原文 http / / 
suanfazu . com / t / qian tan wo dui 
ji qi xue xi de dian li jie / 305 
机器学习 方法 非常多 也很 成熟 下面 我 挑 几个 说 
首先 是 SVM 因为 我 做 的 文本 处理 比较 
多 所以 比较 熟悉 SVM SVM 也叫 支持 向量 机 
其 把 数据 映 射到 多维空间 中 以 点 的 
形式 存在 然后 找到 能够 分类 的 最优 超平面 最后 
根据 这个 平面 来 分类 SVM 能对 训练 集 之外 
的 数据 做 很好 的 预测 泛化 错误率 低 计算 
开销 小 结果 易 解释 但 其 对 参数 调节 
和核/nr 函数 的 参数 过于 敏感 个人感觉 SVM 是 二 
分类 的 最好 的 方法 但也 仅限于 二 分类 如果 
要 使用 SVM 进行 多 分类 也是 在 向量空间 中 
实现 多次 二 分类 SVM 有 一个 核心 函数 SMO 
也 就是 序列 最小 最优化 算法 SMO 基本 是 最快 
的 二次 规划 优化 算法 其 核心 就是 找到 最优 
参数 α 计算 超平面 后 进行 分类 SMO 方法 可以 
将 大 优化 问题 分解 为 多个 小优 化 问题 
求解 大大简化 求解 过程 某些 条件 下 把 原始 的 
约束 问题 通过 拉格朗日 函数 转化 为 无约束 问题 如果 
原始 问题 求解 棘手 在 满足 KKT 的 条件 下 
用 求解 对偶 问题 来 代替 求解 原始 问题 使得 
问题 求解 更加 容易   SVM 还有 一个 重要 函数 
是 核 函数 核 函数 的 主要 作用 是 将 
数据 从 低位 空间 映 射到 高维空间 详细 的 内容 
我 就 不说 了 因为 内容 实在 太多 了 总之 
核 函数 可以 很好 的 解决 数据 的 非线性 问题 
而 无需 考虑 映射过程 第二个 是 KNN KNN 将 测试 
集 的 数据 特征 与 训练 集 的 数据 进行 
特征 比较 然后 算法 提取 样本 集中 特征 最 近邻 
数据 的 分类 标签 即 KNN 算法 采用 测量 不同 
特征值 之间 的 距离 的 方法 进行 分类 KNN 的 
思路 很 简单 就是 计算 测试数据 与 类别 中心 的 
距离 KNN 具有 精度高 对 异常值 不 敏感 无 数据 
输入 假定 简单 有效 的 特点 但 其 缺点 也 
很明显 计算 复杂度 太高 要 分类 一个 数据 却要 计算所 
有 数据 这在 大 数据 的 环境 下 是 很 
可怕 的 事情 而且 当 类别 存在 范围 重叠 时 
KNN 分类 的 精度 也 不 太高 所以 KNN 比较 
适合 小量 数据 且 精度 要求 不高 的 数据 KNN 
有 两个 影响 分类 结果 较大 的 函数 一个 是 
数据 归一化 一个 是 距离 计算 如果 数据 不 进行 
归一化 当 多个 特征 的 值域 差别 很大 的 时候 
最终 结果 就 会 受到 较大 影响 第二个 是 距离 
计算 这 应该 算是 KNN 的 核心 了 目前 用 
的 最多 的 距离 计算 公式 是 欧几里得 距离 也 
就是 我们 常用 的 向量 距离 计算方法 个人感觉 KNN 最大 
的 作用 是 可以 随 时间 序列 计算 即 样本 
不能 一次性 获取 只能 随着 时间 一个 一个 得到 的 
时候 KNN 能 发挥 它 的 价值 至于 其他 的 
特点 它 能做 的 很多 方法 都 能做 其他 能做 
的 它 却 做不了 第三 个 就是 Naive Bayes 了 
Naive Bayes 简称 NB 牛X/nr 为啥 它 牛X呢/nr 因为 它 
是 基于 Bayes 概率 的 一种 分类 方法 贝叶斯 方法 
可以 追溯 到 几百 年前 具有 深厚 的 概率 学 
基础 可信度 非常 高 Naive Baye 中文 名叫 朴素 贝叶斯 
为啥 叫 朴素 呢 因为 其 基于 一个 给定 假设 
给定 目标值 时 属性 之间 相互 条件 独立 比如 我 
说 我 喜欢 你 该 假设 就会 假定 我 喜欢 
你 三者 之间 毫无 关联 仔细 想想 这 几乎 是 
不 可能 的 马克思 告诉 我们 事物 之间 是 有 
联系 的 同一个 事物 的 属性 之间 就 更有 联系 
了 所以 单纯 的 使用 NB 算法 效率 并 不高 
大都 是 对 该 方法 进行 了 一定 的 改进 
以便 适应 数据 的 需求 NB 算法 在 文本 分类 
中用 的 非常 多 因为 文本 类别 主要 取决于 关键词 
基于 词频 的 文本 分类 正中 NB 的 下怀 但 
由于 前面 提到 的 假设 该 方法 对 中文 的 
分类 效果 不好 因为 中文 顾左右而言他 的 情况 太多 但对 
直来直去 的 老美 的 语言 效果 良好 至于 核心 算法 
嘛 主要 思想 全在/nr 贝叶斯 里面 了 没啥 可说 的 
第四个 是 回归 回归 有 很多 Logistic 回归 啊 岭回归 
啊 什么 的 根据 不同 的 需求 可以 分 出 
很多 种 这里 我 主要 说说 Logistic 回归 为啥 呢 
因为 Logistic 回归 主要 是 用来 分类 的 而非 预测 
回归 就是 将 一些 数据 点 用 一条 直线 对 
这些 点 进行 拟合 而 Logistic 回归 是 指 根据 
现有 数据 对 分类 边界线 建立 回归 公式 以此 进行 
分类 该 方法 计算 代价 不高 易于 理解 和 实现 
而且 大部分 时间 用于 训练 训练 完成 后 分类 很快 
但 它 容易 欠 拟合 分类 精度 也 不高 主要 
原因 就是 Logistic 主要 是 线性 拟合 但 现实 中 
很多 事物 都 不满足 线性 的 即便 有 二次 拟合 
三次 拟合 等 曲线拟合 也 只能 满足 小 部分 数据 
而 无法 适应 绝大多数 数据 所以 回归 方法 本身 就 
具有 局限性 但 为什么 还要 在 这里 提 出来 呢 
因为 回归 方法 虽然 大多数 都 不合适 但 一旦 合适 
效果 就 非常 好 Logistic 回归 其实 是 基于 一种 
曲线 的 线 这种 连续 的 表示 方法 有 一个 
很大 的 问题 就是 在 表示 跳变 数据 时会/nr 产生 
阶跃 的 现象 说白 了 就是 很难 表示 数据 的 
突然 转折 所以 用 Logistic 回归 必须 使用 一个 称为 
海维 塞德 阶跃 函数 的 Sigmoid 函数 来 表示 跳变 
通过 Sigmoid 就 可以 得到 分类 的 结果 为了 优化 
Logistic 回归 参数 需要 使用 一种 梯度 上升 法 的 
优化 方法 该 方法 的 核心 是 只要 沿着 函数 
的 梯度方向 搜寻 就 可以 找到 函数 的 最佳 参数 
但该/i 方法/n 在/p 每次/r 更新/d 回归系数/n 时都/nr 需要/v 遍历/v 整个/b 
数据集/i 对于 大 数据 效果 还 不 理想 所以 还 
需要 一个 随机 梯度 上升 算法 对 其 进行 改进 
该 方法 一次 仅用 一个 样本 点来 更新 回归系数 所以 
效率 要高 得多 第五个 是 决策树 据 我 了解 决策树 
是 最简单 也是 曾经 最 常用 的 分类 方法 了 
决策树 基于 树 理论 实现 数据 分类 个人 感觉 就是 
数据结构 中的 B + 树 决策树 是 一个 预测模型 他 
代表 的 是 对象 属性 与 对象 值 之间 的 
一种 映射 关系 决策树 计算 复杂度 不高 输出 结果 易于 
理解 对 中间值 缺失 不 敏感 可以 处理 不 相关 
特征 数据 其 比 KNN 好 的 是 可以 了解 
数据 的 内在 含义 但 其 缺点 是 容易 产生 
过度 匹配 的 问题 且 构建 很 耗时 决策树 还有 
一个 问题 就是 如果 不 绘制 树结构 分类 细节 很难 
明白 所以 生成 决策树 然后再 绘制 决策树 最后 再 分类 
才能 更好 的 了解 数据 的 分类 过程 决策树 的 
核心 树 的 分裂 到底 该 选择 什么 来 决定 
树 的 分叉 是 决策树 构建 的 基础 最好 的 
方法 是 利用 信息熵 实现 熵 这个 概念 很 头疼 
很容易 让人 迷糊 简单 来说 就是 信息 的 复杂 程度 
信息 越多 熵 越高 所以 决策树 的 核心 是 通过 
计算 信息熵 划分 数据集 我 还得 说 一个 比较 特殊 
的 分类 方法 AdaBoost AdaBoost 是 boosting 算法 的 代表 
分类器 boosting 基于 元 算法 集成 算法 即 考虑 其他 
方法 的 结果 作为 参考 意见 也 就是 对 其他 
算法 进行 组合 的 一种 方式 说白了 就是 在 一个 
数据 集上 的 随机 数据 使用 一个 分类 训练 多次 
每次 对 分类 正确 的 数据 赋 权值 较小 同时 
增大 分类 错误 的 数据 的 权重 如此 反复 迭代 
直到 达到 所需 的 要求 AdaBoost 泛化 错误率 低 易 
编码 可以 应用 在 大部分 分类器 上 无 参数 调整 
但对 离群 点 敏感 该 方法 其实 并 不是 一个 
独立 的 方法 而是 必须 基于 元 方法 进行 效率 
提升 个人认为 所谓 的 AdaBoost 是 最好 的 分类 方法 
这 句话 是 错误 的 应该 是 AdaBoost 是 比较 
好 的 优化 方法 才对 好了 说 了 这么 多 
了 我 有点 晕了 还有 一些 方法 过几天 再写 总的来说 
机器学习 方法 是 利用 现有 数据 作为 经验 让 机器学习 
以便 指导 以后 再 次 碰到 的 决策 目前 来说 
对于 大 数据 分类 还是/c 要/v 借助/v 分布式/n 处理/v 技术/n 
和云/nr 技术/n 才/d 有可能/i 完成/v 但 一旦 训练 成功 分类 
的 效率 还是 很 可观 的 这就 好比 人 年龄 
越大 看待 问题 越 精准 的 道理 是 一样 的 
这 八个月 里 从 最初 的 理解 到 一步步 实现 
从 需求 的 逻辑 推断 到 实现 的 方法 选择 
每天 都是/nr 辛苦 的 但 每天 也 都是 紧张 刺激 
的 我 每天 都在/nr 想学 了 这个 以后 可以 实现 
什么样 的 分类 其实 想想 都是 让人 兴奋 的 当初 
我 逃避 做 程序员 主要 原因 就是 我 不 喜欢 
做 已经 知道 结果 的 事情 因为 那样 的 工作 
没有 什么 期盼 感 而 现在 我 可以 利用 数据 
分析 得到 我 想象 不到 的 事情 这 不仅 满足 
了 我 的 好奇 感 也 让 我 能在/nr 工作 
中 乐在其中 也许 我 距离 社会 的 技术 需求 还有 
很远 的 距离 但 我 对 自己 充满 信心 因为 
我 不 感到 枯燥 不 感到 彷徨 虽然 有些 力不从心 
但 态度 坚定 = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = http / / blog . csdn 
. net / vola9527 / article / details / 43347747 
简述 机器学习 十大 算法 的 每个 算法 的 核心 思想 
工作 原理 适用 情况 及 优缺点 等 1 C 4.5 
算法 ID3 算法 是以 信息论 为基础 以 信息熵 和 信息 
增益 度 为 衡量标准 从而 实现 对 数据 的 归纳 
分类 ID3 算法 计算 每个 属性 的 信息 增益 并 
选取 具有 最 高增益 的 属性 作为 给定 的 测试 
属性 C 4.5 算法 核心 思想 是 ID3 算法 是 
ID3 算法 的 改进 改进 方面 有 1 用 信息 
增益 率 来 选择 属性 克服 了 用 信息 增益 
选择 属性 时 偏向 选择 取值 多 的 属性 的 
不足 2 在 树 构造 过程 中 进行 剪枝 3 
能 处理 非离散 的 数据 4 能 处理 不 完整 
的 数据 C 4.5 算法 优点 产生 的 分类 规则 
易于 理解 准确率 较高 缺点 1 在 构造 树 的 
过程 中 需要 对 数据 集 进行 多次 的 顺序 
扫描 和 排序 因而 导致 算法 的 低效 2 C 
4.5只 适合于 能够 驻留 于 内存 的 数据 集 当 
训练 集 大得 无法 在 内存 容纳 时 程序 无法 
运行 2 K means 算法 是 一个 简单 的 聚 
类 算法 把 n 的 对象 根据 他们 的 属性 
分为 k 个 分割 k n 算法 的 核心 就是 
要 优化 失真 函数 J 使其 收敛 到 局部 最小值 
但 不是 全局 最小值 其中 N 为 样本 数 K 
是 簇 数 rnk b 表示 n 属于 第 k 
个 簇 uk 是 第 k 个 中心点 的 值 
然后 求出 最优 的 uk 优点 算法 速度 很快 缺点 
是 分组 的 数目 k 是 一个 输入 参数 不 
合适 的 k 可能 返回 较差 的 结果 3 朴素 
贝叶斯 算法 朴素 贝叶斯 法是/nr 基于 贝叶 斯定理 与 特征 
条件 独立 假设 的 分类 方法 算法 的 基础 是 
概率 问题 分类 原理 是 通过 某 对象 的 先验概率 
利用 贝叶斯 公式 计算 出 其后 验 概率 即 该 
对象 属于 某一 类 的 概率 选择 具有 最大 后验/nr 
概率 的 类 作为 该 对象 所属 的 类 朴素 
贝叶斯 假设 是 约束性 很强 的 假设 假设 特征 条件 
独立 但 朴素 贝叶斯 算法 简单 快速 具有 较小 的 
出错率 在 朴素 贝叶斯 的 应用 中 主要 研究 了 
电子 邮件 过滤 以及 文本 分类 研究 4 K 最 
近邻 分类 算法 KNN 分类 思想 比较 简单 从 训练 
样本 中 找出 K 个 与其 最 相近 的 样本 
然后 看这 k 个 样本 中 哪个 类别 的 样本 
多 则 待 判定 的 值 或 说 抽样 就 
属于 这个 类别 缺点 1 K 值 需要 预先 设定 
而不能 自适应 2 当 样本 不 平衡 时 如 一个 
类 的 样本容量 很大 而 其他 类 样本容量 很 小时 
有 可能 导致 当 输入 一个 新 样本 时 该 
样本 的 K 个 邻居 中 大容量 类 的 样本 
占多数 该 算法 在 分类 时有 个 重要 的 不足 
是 当 样本 不 平衡 时 即 一个 类 的 
样本容量 很大 而 其他 类 样本 数量 很 小时 很 
有可能 导致 当 输入 一个 未知 样本 时 该 样本 
的 K 个 邻居 中大 数量 类 的 样本 占多数 
但是 这类 样本 并不 接近 目标 样本 而 数 量小 
的 这类 样本 很 靠近 目标 样本 这个 时候 我们 
有 理由 认为 该 位置 样 本属于 数 量小 的 
样本 所属 的 一类 但是 KNN 却不 关心 这个 问题 
它 只 关心 哪 类 样本 的 数量 最多 而 
不去 把 距离 远近 考虑 在内 因此 我们 可以 采用 
权值 的 方法 来 改进 和该/nr 样本 距离 小 的 
邻居 权值 大 和该/nr 样本 距离 大 的 邻居 权值 
则 相对 较小 由此 将 距离 远近 的 因素 也 
考虑 在内 避免 因 一个 样本 过大 导致 误判 的 
情况 该 算法 适用于 对 样本 容量 比 较大 的 
类 域 进行 自动 分类 5 EM 最大 期望 算法 
EM 算法 是 基于 模型 的 聚 类 方法 是 
在 概率模型 中 寻找 参数 最大 似 然 估计 的 
算法 其中 概率模型 依赖于 无法 观测 的 隐藏 变量 E 
步 估计 隐含 变量 M 步 估计 其他 参数 交替 
将 极值 推向 最大 EM 算法 比 K means 算法 
计算 复杂 收敛 也 较慢 不 适于 大 规模 数据集 
和 高维 数据 但 比 K means 算法 计算结果 稳定 
准确 EM 经常 用 在 机器 学习 和 计算机 视觉 
的 数据 集聚 Data Clustering 领域 6 PageRank 算法 是 
google 的 页面 排序算法 是 基于 从 许多 优质 的 
网页 链接 过来 的 网页 必定 还是 优质 网页 的 
回归 关系 来 判定 所有 网页 的 重要性 也 就是说 
一个 人 有着 越多 牛X/nr 朋友 的 人 他 是 
牛X的/nr 概率 就 越大 优点 完全 独立 于 查询 只 
依赖于 网页 链接 结构 可以 离线 计算 缺点 1 PageRank 
算法 忽略 了 网页 搜索 的 时效性 2 旧 网页 
排序 很高 存在 时间 长 积累 了 大量 的 in 
links 拥有 最新 资讯 的 新 网页 排名 却 很低 
因为 它们 几乎 没有 in links 7 AdaBoostAdaboost 是 一种 
迭代 算法 其 核心 思想 是 针对 同 一个 训练 
集 训练 不同 的 分类器 弱 分类器 然后 把 这些 
弱 分类器 集合起来 构成 一个 更强 的 最终 分类器 强 
分类器 其 算法 本身 是 通过 改变 数据分布 来 实现 
的 它 根据 每 次 训练 集 之中 每个 样本 
的 分类 是否 正确 以及 上次 的 总体 分类 的 
准确率 来 确定 每个 样本 的 权值 将 修改 过 
权值 的 新 数据集 送给 下层 分类器 进行 训练 最后 
将 每次 训练 得到 的 分类器 最后 融合 起来 作为 
最后 的 决策 分类器 整个 过程 如下 所示 1 . 
先 通过 对 N 个 训练样本 的 学习 得到 第一个 
弱 分类器 2 . 将 分 错 的 样本 和 
其他 的 新 数据 一起 构成 一个 新的 N 个 
的 训练样本 通过 对 这个 样本 的 学习 得到 第二 
个 弱 分类器 3 . 将/d 和都分/nr 错了/i 的/uj 样本/n 
加上/v 其他/r 的/uj 新/a 样本/n 构成/v 另/r 一个/m 新的/i N/w 
个/q 的/uj 训练样本/n 通过 对 这个 样本 的 学习 得 
到 第三 个 弱 分类器 4 . 如此 反复 最终 
得到 经过 提升 的 强 分类器 目前 AdaBoost 算法 广泛 
的 应用于 人脸 检测 目标 识别 等 领域 8 Apriori 
算法 Apriori 算法 是 一种 挖掘 关联 规则 的 算法 
用于 挖掘 其 内含 的 未知 的 却又 实际 存在 
的 数据 关系 其 核心 是 基于 两 阶段 频 
集 思想 的 递推 算法 Apriori 算法 分为 两个 阶段 
1 寻找 频繁 项集2/nr 由 频繁 项集找/nr 关联 规则 算法 
缺点 1 在 每一步 产生 侯选 项目 集 时 循环 
产生 的 组合 过多 没有 排除 不 应该 参与 组合 
的 元素 2 每次 计算 项集的/nr 支持 度 时 都对 
数据库 中       的 全部 记录 进行 了 
一遍 扫描 比较 需要 很大 的 I / O 负载 
9 SVM 支持 向量 机 支持 向量 机 是 一种 
基于 分类 边界 的 方法 其 基本 原理 是 以 
二维 数据 为例 如果 训练 数据分布 在 二维 平 面上 
的 点 它们 按照 其 分类 聚集 在 不同 的 
区域 基于 分类 边界 的 分类 算法 的 目标 是 
通过训练 找到 这些 分类 之间 的 边界 直线 的 ― 
― 称为 线性 划分 曲线 的 ― ― 称为 非线性 
划分 对于 多维 数据 如 N 维 可以 将 它们 
视为 N 维空间 中的 点 而 分类 边界 就是 N 
维空间 中的 面 称为 超 面 超 面 比 N 
维空间 少 一维 线性 分类器 使用 超平面 类型 的 边界 
非线性 分类器 使用 超曲面 支持 向量 机 的 原理 是 
将 低维 空间 的 点映 射到 高维空间 使 它们 成为 
线性 可分 再使用 线性 划分 的 原理 来 判断 分类 
边界 在 高维空间 中 是 一种 线性 划分 而在 原有 
的 数据 空间 中 是 一种 非线性 划分 SVM 在 
解决 小 样本 非线性 及 高维 模式识别 问题 中 表现 
出 许多 特有 的 优势 并能够 推广 应用 到 函数 
拟合 等 其他 机器学习 问题 中 10 CART 分类 与 
回归 树 是 一种 决策树 分类 方法 采用 基于 最小 
距离 的 基尼指数 估计 函数 用来 决定 由该 子 数据集 
生成 的 决策树 的 拓展 形 如果 目标 变量 是 
标称 的 称为 分类 树 如果 目标 变量 是 连续 
的 称为 回归 树 分类 树 是 使用 树结构 算法 
将 数据 分成 离散 类 的 方法 优点 1 非常灵活 
可以允许 有 部分 错分 成本 还可 指定 先验 概率分布 可 
使用 自动 的 成本 复杂性 剪枝 来 得到 归纳 性 
更强 的 树 2 在 面对 诸如 存在 缺失 值 
变量 数多 等 问题 时 CART 显得 非常 稳健 转 
自 http / / www . cnblogs . com / 
zhizhan / p / 4432943 . html 决策树 一   
决策树 优点 1 决策树 易于 理解 和 解释 可以 可视化 
分析 容易 提取 出 规则 2 可以 同时 处理 标称 
型 和 数值 型 数据 3 测试 数据集 时 运行 
速度 比较 快 4 决策树 可以 很好 的 扩展 到 
大型 数据库 中 同时 它 的 大小 独立 于 数据库 
大小 二 决策树 缺点 1 对 缺失 数据 处理 比较 
困难 2 容易 出现 过 拟合 问题 3 忽略 数据 
集中 属性 的 相互 关联 4 ID3 算法 计算 信息 
增益 时 结果 偏向 数值 比 较多 的 特征 三 
改进 措施 1 对 决策树 进行 剪枝 可以 采用 交叉 
验证法 和 加入 正则化 的 方法 2 使用 基于 决策树 
的 combination 算法 如 bagging 算法 randomforest 算法 可以 解决 
过拟合 的 问题 三 应用 领域 企业 管理 实践 企业 
投资 决策 由于 决策树 很好 的 分析 能力 在 决策 
过程 应用 较多 KNN 算法 一 KNN 算法 的 优点 
1 KNN 是 一种 在线 技术 新 数据 可以 直接 
加入 数据集 而 不必 进行 重新 训练 2 KNN 理论 
简单 容易 实现 二 KNN 算法 的 缺点 1 对于 
样本 容量大 的 数据集 计算 量 比较 大 2 样本 
不 平衡 时 预测 偏差 比较 大 如 某 一类 
的 样本 比较 少 而 其它 类 样本 比较 多 
3 KNN 每一次 分类 都会 重新 进行 一次 全局 运算 
4 k 值 大小 的 选择 三 KNN 算法 应用领域 
文本 分类 模式识别 聚类分析 多 分类 领域 支持 向量 机 
SVM 一   SVM 优点 1 解决 小 样本 下 
机器学习 问题 2 解决 非线性 问题 3 无 局部 极小值 
问题 相对于 神经 网络 等 算法 4 可以 很好 的 
处理 高维 数据集 5 泛化 能力 比较 强 二 SVM 
缺点 1 对于 核 函数 的 高维 映射 解释力 不强 
尤其 是 径向 基 函数 2 对 缺失 数据 敏感 
三 SVM 应用领域 文本 分类 图像识别 主要 二 分类 领域 
AdaBoost 算法 一   AdaBoost 算法 优点 1 很好 的 
利用 了 弱 分类器 进行 级联 2 可以 将 不同 
的 分类 算法 作为 弱 分类器 3 AdaBoost 具有 很高 
的 精度 4 相对于 bagging 算法 和 Random Forest 算法 
AdaBoost 充分 考虑 的 每个 分类器 的 权重 二 Adaboost 
算法 缺点 1 AdaBoost 迭代 次数 也 就是 弱 分类器 
数目 不太好 设定 可以 使用 交叉 验证 来 进行 确定 
2 数据 不 平衡 导致 分类 精度 下降 3 训练 
比较 耗时 每次 重新 选择 当前 分类器 最好 切 分点 
三 AdaBoost 应用领域 模式识别 计算机 视觉 领域 用于 二 分类 
和多/nr 分类 场景 朴素 贝叶斯 算法 一   朴素 贝叶斯 
算法 优点 1 对/p 大/a 数量/n 训练/vn 和/c 查询/v 时/n 
具有/v 较高/i 的/uj 速度/n 即使 使用 超 大 规模 的 
训练 集 针对 每 个 项目 通常 也 只会 有 
相对 较少 的 特征 数 并且 对 项目 的 训练 
和 分类 也 仅仅 是 特征 概率 的 数学 运算 
而已 2 支持 增量式 运算 即 可以 实时 的 对 
新增 的 样本 进行 训练 3 朴素 贝叶斯 对 结果 
解释 容易 理解 二 朴素 贝叶斯 缺点 1 由于 使用 
了 样本 属性 独立性 的 假设 所以/c 如果/c 样本/n 属性/n 
有/v 关联/ns 时其/nr 效果/n 不好/d 三 朴素 贝叶斯 应用领域 文本 
分类 欺诈 检测 中 使用 较多 Logistic 回归 算法 一 
logistic 回归 优点 1 计算 代价 不高 易于 理解 和 
实现 二 logistic 回归 缺点 1 容易 产生 欠 拟合 
2 分类 精度 不高 三 logistic 回归 应用领域 用于 二 
分类 领域 可以 得出 概率值 适用于 根据 分类 概率 排名 
的 领域 如 搜索 排名 等 Logistic 回归 的 扩展 
softmax 可以 应用于 多 分类 领域 如 手 写字 识别 
等 人工神经网络 一   神经网络 优点 1 分类 准确度 高 
学习 能力 极强 2 对 噪声 数据 鲁棒性 和 容错性 
较强 3 有 联想 能力 能 逼近 任意 非 线性关系 
二 神经网络 缺点 1 神经网络 参数 较多 权值 和 阈值 
2 黑盒 过程 不能 观察 中间 结果 3 学习 过程 
比较 长 有 可能 陷入 局部 极小值 三 人工神经网络 应用领域 
目前 深度 神经 网络 已经 应用 与 计算机 视觉 自然语言 
处理 语音 识别 等 领域 并 取得 很好 的 效果 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = 原文 http / / suanfazu 
. com / t / qian tan wo dui ji 
qi xue xi de dian li jie / 305 机器学习 
方法 非常多 也很 成熟 下面 我 挑 几个 说 首先 
是 SVM 因为 我 做 的 文本 处理 比较 多 
所以 比较 熟悉 SVM SVM 也叫 支持 向量 机 其 
把 数据 映 射到 多维空间 中 以 点 的 形式 
存在 然后 找到 能够 分类 的 最优 超平面 最后 根据 
这个 平面 来 分类 SVM 能对 训练 集 之外 的 
数据 做 很好 的 预测 泛化 错误率 低 计算 开销 
小 结果 易 解释 但 其 对 参数 调节 和核/nr 
函数 的 参数 过于 敏感 个人感觉 SVM 是 二 分类 
的 最好 的 方法 但也 仅限于 二 分类 如果 要 
使用 SVM 进行 多 分类 也是 在 向量空间 中 实现 
多次 二 分类 SVM 有 一个 核心 函数 SMO 也 
就是 序列 最小 最优化 算法 SMO 基本 是 最快 的 
二次 规划 优化 算法 其 核心 就是 找到 最优 参数 
α 计算 超平面 后 进行 分类 SMO 方法 可以 将 
大 优化 问题 分解 为 多个 小优 化 问题 求解 
大大简化 求解 过程 某些 条件 下 把 原始 的 约束 
问题 通过 拉格朗日 函数 转化 为 无约束 问题 如果 原始 
问题 求解 棘手 在 满足 KKT 的 条件 下 用 
求解 对偶 问题 来 代替 求解 原始 问题 使得 问题 
求解 更加 容易   SVM 还有 一个 重要 函数 是 
核 函数 核 函数 的 主要 作用 是 将 数据 
从 低位 空间 映 射到 高维空间 详细 的 内容 我 
就 不说 了 因为 内容 实在 太多 了 总之 核 
函数 可以 很好 的 解决 数据 的 非线性 问题 而 
无需 考虑 映射过程 第二个 是 KNN KNN 将 测试 集 
的 数据 特征 与 训练 集 的 数据 进行 特征 
比较 然后 算法 提取 样本 集中 特征 最 近邻 数据 
的 分类 标签 即 KNN 算法 采用 测量 不同 特征值 
之间 的 距离 的 方法 进行 分类 KNN 的 思路 
很 简单 就是 计算 测试数据 与 类别 中心 的 距离 
KNN 具有 精度高 对 异常值 不 敏感 无 数据 输入 
假定 简单 有效 的 特点 但 其 缺点 也 很明显 
计算 复杂度 太高 要 分类 一个 数据 却要 计算所 有 
数据 这在 大 数据 的 环境 下 是 很 可怕 
的 事情 而且 当 类别 存在 范围 重叠 时 KNN 
分类 的 精度 也 不 太高 所以 KNN 比较 适合 
小量 数据 且 精度 要求 不高 的 数据 KNN 有 
两个 影响 分类 结果 较大 的 函数 一个 是 数据 
归一化 一个 是 距离 计算 如果 数据 不 进行 归一化 
当 多个 特征 的 值域 差别 很大 的 时候 最终 
结果 就 会 受到 较大 影响 第二个 是 距离 计算 
这 应该 算是 KNN 的 核心 了 目前 用 的 
最多 的 距离 计算 公式 是 欧几里得 距离 也 就是 
我们 常用 的 向量 距离 计算方法 个人感觉 KNN 最大 的 
作用 是 可以 随 时间 序列 计算 即 样本 不能 
一次性 获取 只能 随着 时间 一个 一个 得到 的 时候 
KNN 能 发挥 它 的 价值 至于 其他 的 特点 
它 能做 的 很多 方法 都 能做 其他 能做 的 
它 却 做不了 第三 个 就是 Naive Bayes 了 Naive 
Bayes 简称 NB 牛X/nr 为啥 它 牛X呢/nr 因为 它 是 
基于 Bayes 概率 的 一种 分类 方法 贝叶斯 方法 可以 
追溯 到 几百 年前 具有 深厚 的 概率 学 基础 
可信度 非常 高 Naive Baye 中文 名叫 朴素 贝叶斯 为啥 
叫 朴素 呢 因为 其 基于 一个 给定 假设 给定 
目标值 时 属性 之间 相互 条件 独立 比如 我 说 
我 喜欢 你 该 假设 就会 假定 我 喜欢 你 
三者 之间 毫无 关联 仔细 想想 这 几乎 是 不 
可能 的 马克思 告诉 我们 事物 之间 是 有 联系 
的 同一个 事物 的 属性 之间 就 更有 联系 了 
所以 单纯 的 使用 NB 算法 效率 并 不高 大都 
是 对 该 方法 进行 了 一定 的 改进 以便 
适应 数据 的 需求 NB 算法 在 文本 分类 中用 
的 非常 多 因为 文本 类别 主要 取决于 关键词 基于 
词频 的 文本 分类 正中 NB 的 下怀 但 由于 
前面 提到 的 假设 该 方法 对 中文 的 分类 
效果 不好 因为 中文 顾左右而言他 的 情况 太多 但对 直来直去 
的 老美 的 语言 效果 良好 至于 核心 算法 嘛 
主要 思想 全在/nr 贝叶斯 里面 了 没啥 可说 的 第四个 
是 回归 回归 有 很多 Logistic 回归 啊 岭回归 啊 
什么 的 根据 不同 的 需求 可以 分 出 很多 
种 这里 我 主要 说说 Logistic 回归 为啥 呢 因为 
Logistic 回归 主要 是 用来 分类 的 而非 预测 回归 
就是 将 一些 数据 点 用 一条 直线 对 这些 
点 进行 拟合 而 Logistic 回归 是 指 根据 现有 
数据 对 分类 边界线 建立 回归 公式 以此 进行 分类 
该 方法 计算 代价 不高 易于 理解 和 实现 而且 
大部分 时间 用于 训练 训练 完成 后 分类 很快 但 
它 容易 欠 拟合 分类 精度 也 不高 主要 原因 
就是 Logistic 主要 是 线性 拟合 但 现实 中 很多 
事物 都 不满足 线性 的 即便 有 二次 拟合 三次 
拟合 等 曲线拟合 也 只能 满足 小 部分 数据 而 
无法 适应 绝大多数 数据 所以 回归 方法 本身 就 具有 
局限性 但 为什么 还要 在 这里 提 出来 呢 因为 
回归 方法 虽然 大多数 都 不合适 但 一旦 合适 效果 
就 非常 好 Logistic 回归 其实 是 基于 一种 曲线 
的 线 这种 连续 的 表示 方法 有 一个 很大 
的 问题 就是 在 表示 跳变 数据 时会/nr 产生 阶跃 
的 现象 说白 了 就是 很难 表示 数据 的 突然 
转折 所以 用 Logistic 回归 必须 使用 一个 称为 海维 
塞德 阶跃 函数 的 Sigmoid 函数 来 表示 跳变 通过 
Sigmoid 就 可以 得到 分类 的 结果 为了 优化 Logistic 
回归 参数 需要 使用 一种 梯度 上升 法 的 优化 
方法 该 方法 的 核心 是 只要 沿着 函数 的 
梯度方向 搜寻 就 可以 找到 函数 的 最佳 参数 但该/i 
方法/n 在/p 每次/r 更新/d 回归系数/n 时都/nr 需要/v 遍历/v 整个/b 数据集/i 
对于 大 数据 效果 还 不 理想 所以 还 需要 
一个 随机 梯度 上升 算法 对 其 进行 改进 该 
方法 一次 仅用 一个 样本 点来 更新 回归系数 所以 效率 
要高 得多 第五个 是 决策树 据 我 了解 决策树 是 
最简单 也是 曾经 最 常用 的 分类 方法 了 决策树 
基于 树 理论 实现 数据 分类 个人 感觉 就是 数据结构 
中的 B + 树 决策树 是 一个 预测模型 他 代表 
的 是 对象 属性 与 对象 值 之间 的 一种 
映射 关系 决策树 计算 复杂度 不高 输出 结果 易于 理解 
对 中间值 缺失 不 敏感 可以 处理 不 相关 特征 
数据 其 比 KNN 好 的 是 可以 了解 数据 
的 内在 含义 但 其 缺点 是 容易 产生 过度 
匹配 的 问题 且 构建 很 耗时 决策树 还有 一个 
问题 就是 如果 不 绘制 树结构 分类 细节 很难 明白 
所以 生成 决策树 然后再 绘制 决策树 最后 再 分类 才能 
更好 的 了解 数据 的 分类 过程 决策树 的 核心 
树 的 分裂 到底 该 选择 什么 来 决定 树 
的 分叉 是 决策树 构建 的 基础 最好 的 方法 
是 利用 信息熵 实现 熵 这个 概念 很 头疼 很容易 
让人 迷糊 简单 来说 就是 信息 的 复杂 程度 信息 
越多 熵 越高 所以 决策树 的 核心 是 通过 计算 
信息熵 划分 数据集 我 还得 说 一个 比较 特殊 的 
分类 方法 AdaBoost AdaBoost 是 boosting 算法 的 代表 分类器 
boosting 基于 元 算法 集成 算法 即 考虑 其他 方法 
的 结果 作为 参考 意见 也 就是 对 其他 算法 
进行 组合 的 一种 方式 说白了 就是 在 一个 数据 
集上 的 随机 数据 使用 一个 分类 训练 多次 每次 
对 分类 正确 的 数据 赋 权值 较小 同时 增大 
分类 错误 的 数据 的 权重 如此 反复 迭代 直到 
达到 所需 的 要求 AdaBoost 泛化 错误率 低 易 编码 
可以 应用 在 大部分 分类器 上 无 参数 调整 但对 
离群 点 敏感 该 方法 其实 并 不是 一个 独立 
的 方法 而是 必须 基于 元 方法 进行 效率 提升 
个人认为 所谓 的 AdaBoost 是 最好 的 分类 方法 这 
句话 是 错误 的 应该 是 AdaBoost 是 比较 好 
的 优化 方法 才对 好了 说 了 这么 多 了 
我 有点 晕了 还有 一些 方法 过几天 再写 总的来说 机器学习 
方法 是 利用 现有 数据 作为 经验 让 机器学习 以便 
指导 以后 再 次 碰到 的 决策 目前 来说 对于 
大 数据 分类 还是/c 要/v 借助/v 分布式/n 处理/v 技术/n 和云/nr 
技术/n 才/d 有可能/i 完成/v 但 一旦 训练 成功 分类 的 
效率 还是 很 可观 的 这就 好比 人 年龄 越大 
看待 问题 越 精准 的 道理 是 一样 的 这 
八个月 里 从 最初 的 理解 到 一步步 实现 从 
需求 的 逻辑 推断 到 实现 的 方法 选择 每天 
都是/nr 辛苦 的 但 每天 也 都是 紧张 刺激 的 
我 每天 都在/nr 想学 了 这个 以后 可以 实现 什么样 
的 分类 其实 想想 都是 让人 兴奋 的 当初 我 
逃避 做 程序员 主要 原因 就是 我 不 喜欢 做 
已经 知道 结果 的 事情 因为 那样 的 工作 没有 
什么 期盼 感 而 现在 我 可以 利用 数据 分析 
得到 我 想象 不到 的 事情 这 不仅 满足 了 
我 的 好奇 感 也 让 我 能在/nr 工作 中 
乐在其中 也许 我 距离 社会 的 技术 需求 还有 很远 
的 距离 但 我 对 自己 充满 信心 因为 我 
不 感到 枯燥 不 感到 彷徨 虽然 有些 力不从心 但 
态度 坚定 = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = = = = = = = = 
= = = http / / blog . csdn . 
net / vola9527 / article / details / 43347747 简述 
机器学习 十大 算法 的 每个 算法 的 核心 思想 工作 
原理 适用 情况 及 优缺点 等 1 C 4.5 算法 
ID3 算法 是以 信息论 为基础 以 信息熵 和 信息 增益 
度 为 衡量标准 从而 实现 对 数据 的 归纳 分类 
ID3 算法 计算 每个 属性 的 信息 增益 并 选取 
具有 最 高增益 的 属性 作为 给定 的 测试 属性 
C 4.5 算法 核心 思想 是 ID3 算法 是 ID3 
算法 的 改进 改进 方面 有 1 用 信息 增益 
率 来 选择 属性 克服 了 用 信息 增益 选择 
属性 时 偏向 选择 取值 多 的 属性 的 不足 
2 在 树 构造 过程 中 进行 剪枝 3 能 
处理 非离散 的 数据 4 能 处理 不 完整 的 
数据 C 4.5 算法 优点 产生 的 分类 规则 易于 
理解 准确率 较高 缺点 1 在 构造 树 的 过程 
中 需要 对 数据 集 进行 多次 的 顺序 扫描 
和 排序 因而 导致 算法 的 低效 2 C 4.5只 
适合于 能够 驻留 于 内存 的 数据 集 当 训练 
集 大得 无法 在 内存 容纳 时 程序 无法 运行 
2 K means 算法 是 一个 简单 的 聚 类 
算法 把 n 的 对象 根据 他们 的 属性 分为 
k 个 分割 k n 算法 的 核心 就是 要 
优化 失真 函数 J 使其 收敛 到 局部 最小值 但 
不是 全局 最小值 其中 N 为 样本 数 K 是 
簇 数 rnk b 表示 n 属于 第 k 个 
簇 uk 是 第 k 个 中心点 的 值 然后 
求出 最优 的 uk 优点 算法 速度 很快 缺点 是 
分组 的 数目 k 是 一个 输入 参数 不 合适 
的 k 可能 返回 较差 的 结果 3 朴素 贝叶斯 
算法 朴素 贝叶斯 法是/nr 基于 贝叶 斯定理 与 特征 条件 
独立 假设 的 分类 方法 算法 的 基础 是 概率 
问题 分类 原理 是 通过 某 对象 的 先验概率 利用 
贝叶斯 公式 计算 出 其后 验 概率 即 该 对象 
属于 某一 类 的 概率 选择 具有 最大 后验/nr 概率 
的 类 作为 该 对象 所属 的 类 朴素 贝叶斯 
假设 是 约束性 很强 的 假设 假设 特征 条件 独立 
但 朴素 贝叶斯 算法 简单 快速 具有 较小 的 出错率 
在 朴素 贝叶斯 的 应用 中 主要 研究 了 电子 
邮件 过滤 以及 文本 分类 研究 4 K 最 近邻 
分类 算法 KNN 分类 思想 比较 简单 从 训练 样本 
中 找出 K 个 与其 最 相近 的 样本 然后 
看这 k 个 样本 中 哪个 类别 的 样本 多 
则 待 判定 的 值 或 说 抽样 就 属于 
这个 类别 缺点 1 K 值 需要 预先 设定 而不能 
自适应 2 当 样本 不 平衡 时 如 一个 类 
的 样本容量 很大 而 其他 类 样本容量 很 小时 有 
可能 导致 当 输入 一个 新 样本 时 该 样本 
的 K 个 邻居 中 大容量 类 的 样本 占多数 
该 算法 在 分类 时有 个 重要 的 不足 是 
当 样本 不 平衡 时 即 一个 类 的 样本容量 
很大 而 其他 类 样本 数量 很 小时 很 有可能 
导致 当 输入 一个 未知 样本 时 该 样本 的 
K 个 邻居 中大 数量 类 的 样本 占多数 但是 
这类 样本 并不 接近 目标 样本 而 数 量小 的 
这类 样本 很 靠近 目标 样本 这个 时候 我们 有 
理由 认为 该 位置 样 本属于 数 量小 的 样本 
所属 的 一类 但是 KNN 却不 关心 这个 问题 它 
只 关心 哪 类 样本 的 数量 最多 而 不去 
把 距离 远近 考虑 在内 因此 我们 可以 采用 权值 
的 方法 来 改进 和该/nr 样本 距离 小 的 邻居 
权值 大 和该/nr 样本 距离 大 的 邻居 权值 则 
相对 较小 由此 将 距离 远近 的 因素 也 考虑 
在内 避免 因 一个 样本 过大 导致 误判 的 情况 
该 算法 适用于 对 样本 容量 比 较大 的 类 
域 进行 自动 分类 5 EM 最大 期望 算法 EM 
算法 是 基于 模型 的 聚 类 方法 是 在 
概率模型 中 寻找 参数 最大 似 然 估计 的 算法 
其中 概率模型 依赖于 无法 观测 的 隐藏 变量 E 步 
估计 隐含 变量 M 步 估计 其他 参数 交替 将 
极值 推向 最大 EM 算法 比 K means 算法 计算 
复杂 收敛 也 较慢 不 适于 大 规模 数据集 和 
高维 数据 但 比 K means 算法 计算结果 稳定 准确 
EM 经常 用 在 机器 学习 和 计算机 视觉 的 
数据 集聚 Data Clustering 领域 6 PageRank 算法 是 google 
的 页面 排序算法 是 基于 从 许多 优质 的 网页 
链接 过来 的 网页 必定 还是 优质 网页 的 回归 
关系 来 判定 所有 网页 的 重要性 也 就是说 一个 
人 有着 越多 牛X/nr 朋友 的 人 他 是 牛X的/nr 
概率 就 越大 优点 完全 独立 于 查询 只 依赖于 
网页 链接 结构 可以 离线 计算 缺点 1 PageRank 算法 
忽略 了 网页 搜索 的 时效性 2 旧 网页 排序 
很高 存在 时间 长 积累 了 大量 的 in links 
拥有 最新 资讯 的 新 网页 排名 却 很低 因为 
它们 几乎 没有 in links 7 AdaBoostAdaboost 是 一种 迭代 
算法 其 核心 思想 是 针对 同 一个 训练 集 
训练 不同 的 分类器 弱 分类器 然后 把 这些 弱 
分类器 集合起来 构成 一个 更强 的 最终 分类器 强 分类器 
其 算法 本身 是 通过 改变 数据分布 来 实现 的 
它 根据 每 次 训练 集 之中 每个 样本 的 
分类 是否 正确 以及 上次 的 总体 分类 的 准确率 
来 确定 每个 样本 的 权值 将 修改 过 权值 
的 新 数据集 送给 下层 分类器 进行 训练 最后 将 
每次 训练 得到 的 分类器 最后 融合 起来 作为 最后 
的 决策 分类器 整个 过程 如下 所示 1 . 先 
通过 对 N 个 训练样本 的 学习 得到 第一个 弱 
分类器 2 . 将 分 错 的 样本 和 其他 
的 新 数据 一起 构成 一个 新的 N 个 的 
训练样本 通过 对 这个 样本 的 学习 得到 第二 个 
弱 分类器 3 . 将/d 和都分/nr 错了/i 的/uj 样本/n 加上/v 
其他/r 的/uj 新/a 样本/n 构成/v 另/r 一个/m 新的/i N/w 个/q 
的/uj 训练样本/n 通过 对 这个 样本 的 学习 得 到 
第三 个 弱 分类器 4 . 如此 反复 最终 得到 
经过 提升 的 强 分类器 目前 AdaBoost 算法 广泛 的 
应用于 人脸 检测 目标 识别 等 领域 8 Apriori 算法 
Apriori 算法 是 一种 挖掘 关联 规则 的 算法 用于 
挖掘 其 内含 的 未知 的 却又 实际 存在 的 
数据 关系 其 核心 是 基于 两 阶段 频 集 
思想 的 递推 算法 Apriori 算法 分为 两个 阶段 1 
寻找 频繁 项集2/nr 由 频繁 项集找/nr 关联 规则 算法 缺点 
1 在 每一步 产生 侯选 项目 集 时 循环 产生 
的 组合 过多 没有 排除 不 应该 参与 组合 的 
元素 2 每次 计算 项集的/nr 支持 度 时 都对 数据库 
中       的 全部 记录 进行 了 一遍 
扫描 比较 需要 很大 的 I / O 负载 9 
SVM 支持 向量 机 支持 向量 机 是 一种 基于 
分类 边界 的 方法 其 基本 原理 是 以 二维 
数据 为例 如果 训练 数据分布 在 二维 平 面上 的 
点 它们 按照 其 分类 聚集 在 不同 的 区域 
基于 分类 边界 的 分类 算法 的 目标 是 通过训练 
找到 这些 分类 之间 的 边界 直线 的 ― ― 
称为 线性 划分 曲线 的 ― ― 称为 非线性 划分 
对于 多维 数据 如 N 维 可以 将 它们 视为 
N 维空间 中的 点 而 分类 边界 就是 N 维空间 
中的 面 称为 超 面 超 面 比 N 维空间 
少 一维 线性 分类器 使用 超平面 类型 的 边界 非线性 
分类器 使用 超曲面 支持 向量 机 的 原理 是 将 
低维 空间 的 点映 射到 高维空间 使 它们 成为 线性 
可分 再使用 线性 划分 的 原理 来 判断 分类 边界 
在 高维空间 中 是 一种 线性 划分 而在 原有 的 
数据 空间 中 是 一种 非线性 划分 SVM 在 解决 
小 样本 非线性 及 高维 模式识别 问题 中 表现 出 
许多 特有 的 优势 并能够 推广 应用 到 函数 拟合 
等 其他 机器学习 问题 中 10 CART 分类 与 回归 
树 是 一种 决策树 分类 方法 采用 基于 最小 距离 
的 基尼指数 估计 函数 用来 决定 由该 子 数据集 生成 
的 决策树 的 拓展 形 如果 目标 变量 是 标称 
的 称为 分类 树 如果 目标 变量 是 连续 的 
称为 回归 树 分类 树 是 使用 树结构 算法 将 
数据 分成 离散 类 的 方法 优点 1 非常灵活 可以允许 
有 部分 错分 成本 还可 指定 先验 概率分布 可 使用 
自动 的 成本 复杂性 剪枝 来 得到 归纳 性 更强 
的 树 2 在 面对 诸如 存在 缺失 值 变量 
数多 等 问题 时 CART 显得 非常 稳健 