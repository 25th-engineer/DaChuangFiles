python 机器学习 实战 二 版权 声明 本文 为 博主 原创 
文章 转载 请 指明 转载 地址 http / / www 
. cnblogs . com / fydeblog / p / 7159775 
. html 前言 这篇 notebook 是 关于 机器学习 监督 学习 
中的 决策树 算法 内容 包括 决策树 算法 的 构造 过程 
使用 matplotlib 库 绘制 树形图 以及 使用 决策树 预测 隐形眼睛 
类型 . 操作系统 ubuntu14 . 04 win 也 ok   
运行 环境 anaconda python2 . 7 jupyter notebook     
  参考 书籍 机器学习 实 战和 源码     notebook 
writer 方阳/nr 注意事项/n 在 这里 说一句 默认 环境 python2 . 
7 的 notebook 用 python3 . 6 的 会 出问题 
还有 我 的 目录 可能 跟 你们 的 不一样 你们 
自己 跑 的 时候 记得 改 目录 我会 把 notebook 
和 代码 以及 数据集 放到 结尾 的 百度 云盘 方便 
你们 下载 决策树 原理 不断 通过 数据集 的 特征 来 
划分 数据集 直到 遍历 所有 划分 数据集 的 属性 或 
每个 分支 下 的 实例 都 具有 相同 的 分类 
决策树 算法 停止 运行 决策树 的 优缺点 及 适用 类型 
优点 计算 复杂度 不高 输出 结果 易于 理解 对 中间值 
的 缺失 不 敏感 可以 处理 不 相关 特征 数据 
缺点 可能 会 产生 过度 匹配 问题 适用 数据类型 数值 
型 和 标称 型 先 举 一个 小 例子 让 
你 了解 决策树 是 干嘛 的 简单 来说 决策树 算法 
就是 一种 基于 特征 的 分类器 拿 邮件 来说 吧 
试想 一下 邮件 的 类型 有 很多 种 有 需要 
及时 处理 的 邮件 无聊 是 观看 的 邮件 垃圾邮件 
等等 我们 需要 去 区分 这些 比如 根据 邮件 中 
出现 里 你 的 名字 还有 你 朋友 的 名字 
这些 特征 就会 就 可以 将 邮件 分成 两类 需要 
及时 处理 的 邮件 和 其他 邮件 这时候 在 分类 
其他 邮件 例如 邮件 中 出现 buy money 等 特征 
说明 这 是 垃圾 推广 文件 又 可以 将 其他 
文件 分成 无聊 是 观看 的 邮件 和 垃圾 邮件 
了 1 . 决策树 的 构造 1.1 信息 增益 试想 
一下 一个 数据集 是 有 多个 特征 的 我们 该 
从 那个 特征 开始 划分 呢 什么样 的 划分 方式 
会 是 最好 的 我们 知道 划分 数据集 的 大 
原则 是 将 无序 的 数据 变得 更加 有序 这样 
才能 分类 得 更加 清楚 这里 就 提出 了 一种 
概念 叫做 信息 增益 它 的 定义 是 在 划分 
数据集 之前 之后 信息 发生 的 变化 变化 越大 证明 
划分 得 越好 所以 在 划分 数据集 的 时候 获得 
增益 最高 的 特征 就是 最好 的 选择 这里 又会 
扯到 另 一个 概念 信息论 中的 熵 它 是 集合 
信息 的 度量 方式 熵 变化 越大 信息 增益 也就 
越大 信息 增益 是 熵 的 减少 或者 是 数据 
无序 度 的 减少 . 一个 符号 x 在 信息论 
中 的 信息 定义 是 l x = log p 
x 这里 都 是以 2 为 底 不再 复述 则 
熵 的 计算 公式 是 H = ∑ p xi 
log p xi i = 1 2 . . n 
下面 开始 实现 给定 数据集 计算 熵 参考 代码 1 
from math import log # we use log function to 
calculate the entropy 2 import operator1 def calcShannonEnt dataSet 2 
numEntries = len dataSet 3 labelCounts = { } 4 
for featVec in dataSet # the the number of unique 
elements and their occurance 5 currentLabel = featVec 1 6 
if currentLabel not in labelCounts . keys labelCounts currentLabel = 
0 7 labelCounts currentLabel + = 1 8 shannonEnt = 
0.0 9 for key in labelCounts 10 prob = float 
labelCounts key / numEntries 11 shannonEnt = prob * log 
prob 2 # log base 2 12 return shannonEnt 程序 
思路 首先 计算 数据 集中 实例 的 总数 由于 代码 
中 多次 用 到 这个 值 为了 提高 代码 效率 
我们 显 式 地 声明 一个 变量 保存 实例 总数 
. 然后 创建 一个 数据字典 labelCounts 它 的 键值 是 
最后 一列 分类 的 结果 的 数值 . 如果 当前 
键值 不存在 则 扩展 字典 并将 当前 键值 加入 字典 
每个 键值 都 记录 了 当前 类别 出现 的 次数 
最后 使用 所有 类 标签 的 发生 频率 计算 类别 
出现 的 概率 我们 将 用 这个 概率 计算 香农 
熵 让 我们 来 测试 一下 先 自己 定义 一个 
数据集 下表 的 数据 包含 5 个 海洋 动物 特征 
包括 不 浮出水面 是否 可以 生存 以及 是否 有 脚蹼 
我们 可以 将 这些 动物 分成 两类 鱼类 和非/nr 鱼类 
根据 上面 的 表格 我们 可以 定义 一个 createDataSet 函数 
参考 代码 如下 1 def createDataSet 2 dataSet = 1 
1 yes 3 1 1 yes 4 1 0 no 
5 0 1 no 6 0 1 no 7 labels 
= no surfacing flippers 8 # change to discrete values 
9 return dataSet labels 把 所有 的 代码 都 放在 
trees . py 中 以 下在 jupyter cd / home 
/ fangyang / 桌面 / m a c h i 
n e l e a r n i n g 
i n a c t i o n / Ch03 
/ home / fangyang / 桌面 / m a c 
h i n e l e a r n i 
n g i n a c t i o n 
/ Ch03import treesmyDat labels = trees . createDataSet myDat # 
old data set 1 1 yes 1 1 yes 1 
0 no 0 1 no 0 1 no labels no 
surfacing flippers trees . calcShannonEnt myDat # calculate the entropy0 
. 9 7 0 9 5 0 5 9 4 
4 5 4 6 6 8 6 m y D 
a t 0 1 = maybe # change the result 
and look again the entropymyDat # new data set 1 
1 maybe 1 1 yes 1 0 no 0 1 
no 0 1 no trees . calcShannonEnt myDat # the 
new entropy1 . 3709505944546687 我们 可以 看到 当 结果 分类 
改变 熵 也 发生 里 变化 主要 是 因为 最后 
的 结果 发生 里 改变 相应 的 概率 也 发生 
了 改变 根据 公式 熵 也会 改变 1.2 划分 数据集 
前面 已经 得到 了 如何 去求 信息熵 的 函数 但 
我们 的 划分 是以 哪个 特征 划分 的 呢 不 
知道 所以 我们 还要 写 一个 以 给定 特征 划分 
数据集 的 函数 参考 代码 如下 1 def splitDataSet dataSet 
axis value 2 retDataSet = 3 for featVec in dataSet 
4 if featVec axis = = value 5 reducedFeatVec = 
featVec axis # chop out axis used for splitting 6 
reducedFeatVec . extend featVec axis + 1 7 retDataSet . 
append reducedFeatVec 8 return retDataSet 函数 的 三个 输人 参数 
待 划分 的 数据 集 dataSet 划分 数据集 的 特征 
axis 特征 的 返回值 value 输出 是 划分 后的/nr 数据集 
retDataSet 小 知识 python 语言 在 函数 中 传递 的 
是 列表 的 引用 在 函数 内部 对 列表 对象 
的 修改 将 会 影响 该 列表 对象 的 整个 
生存 周期 为了 消除 这个 不良影响 我们 需要 在 函数 
的 开始 声明 一个 新 列表 对象 因为 该 函数 
代码 在 同一 数据 集上 被 调用 多次 为了 不 
修改 原始 数据集 创建 一个 新 的 列表 对象 retDataSet 
这个 函数 也挺 简单 的 根据 axis 的 值 所指 
的 对象 来 进行 划分 数据集 比如 axis = 0 
就 按照 第一 个 特征 来 划分 featVec axis 就是 
空 下面 经过 一个 extend 函数 将 featVec axis + 
1 后面 的 数 存到 reduceFeatVec 中 然后 通过 append 
函 数以 列表 的 形式 存到 retDataSet 中 这里 说 
一下 entend 和 append 函数 的 功能 举个 例子 吧 
a = 1 2 3 b = 4 5 6 
a . append b a 1 2 3 4 5 
6 a = 1 2 3 a . extend b 
a 1 2 3 4 5 6 可见 append 函数 
是 直接 将 b 的 原型 导入 a 中 extend 
是 将 b 中的 元素 导入到 a 中 下面 再来 
测试 一下 myDat labels = trees . createDataSet # i 
n i t i a l i z a t 
i o n m y D a t 1 1 
yes 1 1 yes 1 0 no 0 1 no 
0 1 no trees . splitDataSet myDat 0 1 # 
choose the first character to split the dataset 1 yes 
1 yes 0 no trees . splitDataSet myDat 0 0 
# change the value look the difference of previous results 
1 no 1 no 好了 我们 知道 了 怎样 以 
某个 特征 划分 数据 集了 但 我们 需要 的 是 
最好 的 数据集 划分 方式 所以 要 结合 前面 两个 
函数 计算 以 每个 特征 为 划分 方式 相应 最后 
的 信息 熵 我们 要 找到 最大 信息熵 它 所 
对应 的 特征 就是 我们 要找 的 最好 划分 方式 
所以 有了 函数 c h o o s e B 
e s t F e a t u r e 
T o p i l t 参考 代码 如下 1 
def c h o o s e B e s 
t F e a t u r e T o 
p l i t dataSet 2 numFeatures = len dataSet 
0 1 # the last column is used for the 
labels 3 baseEntropy = calcShannonEnt dataSet # calculate the original 
entropy 4 bestInfoGain = 0.0 bestFeature = 1 5 for 
i in range numFeatures # iterate over all the features 
6 featList = example i for example in dataSet # 
create a list of all the examples of this feature 
7 uniqueVals = set featList # get a set of 
unique values 8 newEntropy = 0.0 9 for value in 
uniqueVals 10 subDataSet = splitDataSet dataSet i value 11 prob 
= len subDataSet / float len dataSet 12 newEntropy + 
= prob * calcShannonEnt subDataSet 13 infoGain = baseEntropy newEntropy 
# calculate the info gain ie reduction in entropy 14 
if infoGain bestInfoGain # compare this to the best gain 
so far 15 bestInfoGain = infoGain # if better than 
current best set to best 16 bestFeature = i 17 
return bestFeature # returns an integer 这个 函数 就是 把 
前面 两个 函数 整合 起来 了 先 算出 特征 的 
数目 由于 最后 一个 是 标签 不算 特征 所以 以 
数据集 长度 来 求 特征 数 时 要 减 1 
然后 求 原始 的 信息 熵 是 为了 跟 新的 
信息熵 进行 比较 选出 变化 最大 所 对应 的 特征 
这里 有 一个 双重 循环 外循环 是 按 特征 标号 
进行 循环 的 下标 从小到大 featList 是 特征 标号 对应 
下 的 每个 样本 的 值 是 一个 列表 而 
uniqueVals 是 基于 这 个 特征 的 所有 可能 的 
值 的 集合 内循环 做 的 是以 特征 集合 中的 
每 一个 元素 作为 划分 最后 求得 这个 特征 下 
的 平均 信息熵 然后 原始 的 信息 熵 进行 比较 
得出 信息 增益 最后 的 if 语句 是 要 找到 
最大 信息 增益 并 得到 最大 信息 增益 所 对应 
的 特征 的 标号 现在 来 测试 测试 import trees 
myDat labels = trees . createDataSet trees . c h 
o o s e B e s t F e 
a t u r e T o p l i 
t myDat # return the index of best character to 
split01 . 3 递归 构建 决策树 好了 到 现在 我们 
已经 知道 如何 基于 最好 的 属性值 去 划分 数据 
集了 现在 进行 下 一步 如何 去 构造 决策树 决策树 
的 实现 原理 得到 原始 数据集 然后 基于 最好 的 
属性值 划分 数据集 由于 特征值 可能 多 于 两个 因此 
可能 存在 大 于 两个 分支 的 数据集 划分 第一 
次 划分 之后 数据 将被 向下 传递 到 树 分支 
的 下 一个 节点 在 这个 节点 上 我们 可以 
再次 划分 数据 因此 我们 可以 采用 递归 的 原则 
处理 数据集 递归 结束 的 条件 是 程序 遍历 完 
所有 划分 数据集 的 属性 或者 每个 分支 下 的 
所有 实例 都 具有 相同 的 分类 这里 先 构造 
一个 majorityCnt 函数 它 的 作用 是 返回 出现 次数 
最多 的 分类 名称 后 面会 用到 def majorityCnt classList 
classCount = { } for vote in classList if vote 
not in classCount . keys classCount vote = 0 classCount 
vote + = 1 sortedClassCount = sorted classCount . iteritems 
key = operator . itemgetter 1 reverse = True return 
sortedClassCount 0 0 这个 函数 在 实战 一中 的 一个 
函数 是 一样 的 复述 一遍 classCount 定义 为 存储 
字典 每当 由于 后面 加了 1 所以 每次 出现 键值 
就 加 1 就 可以 就 算出 键值 出现 的 
次数 里 最后 通过 sorted 函数 将 classCount 字典 分解为 
列表 sorted 函数 的 第二 个 参数 导入 了 运算符 
模块 的 itemgetter 方法 按照 第二 个 元素 的 次序 
即 数字 进行 排序 由于 此处 reverse = True 是 
逆序 所以 按照 从大到/nr 小 的 次序 排列 让 我们 
来 测试 一下 import numpy as np classList = np 
. array myDat . T 1 classListarray yes yes no 
no no dtype = | S21 majorityCnt classList # the 
number of no is 3 yes is 2 so return 
no no 接下来 是 创建 决策树 函数 代码 如下 1 
def createTree dataSet labels 2 classList = example 1 for 
example in dataSet 3 if classList . count classList 0 
= = len classList 4 return classList 0 # stop 
splitting when all of the classes are equal 5 if 
len dataSet 0 = = 1 # stop splitting when 
there are no more features in dataSet 6 return majorityCnt 
classList 7 bestFeat = c h o o s e 
B e s t F e a t u r 
e T o p l i t dataSet 8 bestFeatLabel 
= labels bestFeat 9 myTree = { bestFeatLabel { } 
} 10 del labels bestFeat # delete the best feature 
so it can find the next best feature 11 featValues 
= example bestFeat for example in dataSet 12 uniqueVals = 
set featValues 13 for value in uniqueVals 14 subLabels = 
labels # copy all of labels so trees don t 
mess up existing labels 15 myTree bestFeatLabel value = createTree 
splitDataSet dataSet bestFeat value subLabels 16 return myTree 前面 两个 
if 语句 是 判断 分类 是否 结束 当/t 所有/b 的/uj 
类/q 都相/nr 等时/i 也 就是 属于 同 一类 时 结束 
再 分类 又 或 特征 全部 已经 分类 完成 了 
只 剩下 最后 的 class 也 结束 分类 这是 判断 
递归 结束 的 两个 条件 一般 开始 的 时候 是 
不会 运行 这 两步 的 先 选 最好 的 特征 
使用 c h o o s e B e s 
t F e a t u r e T o 
p l i t 函数 得到 最好 的 特征 然后 
进行 分类 这里 创建 了 一个 大 字典 myTree 它 
将 决策树 的 整个 架构 全 包含 进去 这个 等会 
在 测试 的 时候 说 然后 对 数据 集 进行 
划分 用 splitDataSet 函数 就 可以 得到 划分 后 新的 
数据集 然后 再 进行 createTrees 函数 直到 递归 结束 来 
测试 一下 myTree = trees . createTree myDat labels myTree 
{ no surfacing { 0 no 1 { flippers { 
0 no 1 yes } } } } 再 来说 
说 上面 没 详细 说明 的 大字典 myTree 是 特征 
是 no surfacing 根据 这个 分类 得到 两 个 分支 
0 和 1 0 分支 由于 全是 同 一类 就 
递归 结束 里 1 分支 不满足 递归 结束 条件 继续 
进行 分类 它 又会 生成 它 自己 的 字典 又会 
分成 两个 分支 并且 这 两个 分支 满足 递归 结束 
的 条件 所以 返回 no surfacing 上 的 1 分支 
是 一个 字典 这种 嵌套 的 字典 正是 决策树 算法 
的 结果 我们 可以 使用 它 和 Matplotlib 来 进行 
画 决策 1.4 使用 决策树 执行 分类 这个 就是 将 
测试 合成 一个 函数 定义 为 classify 函数 参考 代码 
如下 1 def classify inputTree featLabels testVec 2 firstStr = 
inputTree . keys 0 3 secondDict = inputTree firstStr 4 
featIndex = featLabels . index firstStr 5 key = testVec 
featIndex 6 valueOfFeat = secondDict key 7 if isinstance valueOfFeat 
dict 8 classLabel = classify valueOfFeat featLabels testVec 9 else 
classLabel = valueOfFeat 10 return classLabel 这个 函数 就是 一个 
根据 决策树 来 判断 新的 测试 向量 是 那种 类型 
这也 是 一个 递归函数 拿 上面 决策树 的 结果 来说 
吧 { no surfacing { 0 no 1 { flippers 
{ 0 no 1 yes } } } } 这是 
就是 我们 的 inputTree 首先 通过 函数 的 第一 句话 
得到 它 的 第一 个 bestFeat 也 就是 no surfacing 
赋 给了 firstStr secondDict 就是 no surfacing 的 值 也 
就是 { 0 no 1 { flippers { 0 no 
1 yes } } } 然后 用 index 函数 找到 
firstStr 的 标号 结果 应该 是 0 根据 下标 把 
测试 向量 的 值 赋 给 key 然后 找到 对应 
secondDict 中的 值 这里 有 一个 isinstance 函数 功能 是 
第一 个 参数 的 类型 等于 后面 参数 的 类型 
则 返回 true 否则 返回 false testVec 列表 第 一位 
是 1 则 valueOfFeat 的 值 是 { 0 no 
1 yes } 是 dict 则 递归调用 这个 函数 再 
进行 classify 知道 不是 字典 也就 最后 的 结果 了 
其实 就是 将 决策树 过 一遍 找到 对应 的 labels 
罢了 这里 有 一个 小 知识 点 在 jupyter notebook 
中 显示 绿色 的 函数 可以 通过 下面 查询 它 
的 功能 例如 isinstance # run it you will see 
a below window which is used to introduce this function 
让 我们 来 测试 测试 trees . classify myTree labels 
1 0 no trees . classify myTree labels 1 1 
yes 1.5 决策树 的 存储 构造 决策树 是 很 耗时 
的 任务 即使 处理 很小 的 数据集 如前 面的 样本数据 
也 要花费 几秒 的 时间 如果 数据集 很大 将会 耗费 
很多 计算 时间 然 而用 创 建好 的 决策树 解决 
分类 问题 可以 很快 完成 因此 为了 节省 计算 时间 
最好/a 能够/v 在/p 每次/r 执行/v 分类/n 时/n 调用/vn 巳/mg 经/n 
构造/v 好/a 的/uj 决策树/n 解决方案 使用 pickle 模块 存储 决策树 
参考 代码 def storeTree inputTree filename import pickle fw = 
open filename w pickle . dump inputTree fw fw . 
close def grabTree filename import pickle fr = open filename 
return pickle . load fr 就是 将 决策树 写到 文件 
中 用 的 时候 在 取出来 测试 一下 就 明白 
了 trees . storeTree myTree c l a s s 
i f i e r t o r a g 
e . txt # run it store the treetrees . 
grabTree c l a s s i f i e 
r t o r a g e . txt { 
no surfacing { 0 no 1 { flippers { 0 
no 1 yes } } } } 决策树 的 构造 
部分 结束 了 下面 介绍 怎样 绘制 决策树 2 . 
使用 Matplotlib 注解 绘制 树形图 前面 我们 看到 决策树 最后 
输出 是 一个 大 字典 非常 丑陋 我们 想 让 
它 更有 层次感 更加 清晰 最好 是 图 形状 的 
于是 我们 要 Matplotlib 去 画 决策树 2.1 Matplotlib 注解 
Matplotlib 提供 了 一个 注解 工具 annotations 它 可以 在 
数据 图形 上 添加 文本 注释 创建 一个 treePlotter . 
py 文件 来 存储 画图 的 相关 函数 首先 是 
使用 文本 注解 绘制 树节点 参考 代码 如下 1 import 
matplotlib . pyplot as plt 2 3 decisionNode = dict 
boxstyle = sawtooth fc = 0.8 4 leafNode = dict 
boxstyle = round4 fc = 0.8 5 arrow _ args 
= dict arrowstyle = 6 7 def plotNode nodeTxt centerPt 
parentPt nodeType 8 createPlot . ax1 . annotate nodeTxt xy 
= parentPt xycoords = axes fraction \ 9 xytext = 
centerPt textcoords = axes fraction \ 10 va = center 
ha = center bbox = nodeType arrowprops = arrow _ 
args 11 12 def createPlot1 13 fig = plt . 
figure 1 facecolor = white 14 fig . clf 15 
createPlot . ax1 = plt . subplot 111 frameon = 
False # ticks for demo puropses 16 plotNode a decision 
node 0.5 0.1 0.1 0.5 decisionNode 17 plotNode a leaf 
node 0.8 0.1 0.3 0.8 leafNode 18 plt . show 
前面 三行 是 定义 文本框 和 箭头 格式 decisionNode 是 
锯齿形 方框 文本框 的 大小 是 0.8 leafNode 是 4 
边 环绕 型 跟 矩形 类似 大小 也是 4 arrow 
_ args 是 指 箭头 我们 在 后面 结果 是 
会 看到 这些 东西 这些 数据 以 字典 类型 存储 
第一 个 plotNode 函数 的 功能 是 绘制 带 箭头 
的 注解 输入 参数 分别 是 文本框 的 内容 文本框 
的 中心 坐标 父 结点 坐标 和 文本框 的 类型 
这些 都是/nr 通过 一个 createPlot . ax1 . annotate 函数 
实现 的 create . ax1 是 一个 全局变量 这个 函 
数不多 将 会用 就行了 第二个 函数 createPlot 就是 生出 图形 
也 没什么 东西 函数 第一行 是 生成 图像 的 画框 
横 纵坐标 最大值 都是 1 颜色 是 白色 下 一个 
是 清屏 下 一个 就是 分 图 111中 第一 个 
1 是 行数 第二个 是 列数 第三个 是 第几个 图 
这里 就 一个 图 跟 matlab 中的 一样 matplotlib 里面 
的 函数 都是 和 matlab 差不多 来 测试 一下 吧 
reset f # clear all the module and datacd 桌面 
/ m a c h i n e l e 
a r n i n g i n a c 
t i o n / Ch03 / home / fangyang 
/ 桌面 / m a c h i n e 
l e a r n i n g i n 
a c t i o n / Ch03import treePlotter import 
matplotlib . pyplot as plttreePlotter . createPlot1 2.2 构造 注解 
树 绘制 一棵 完整 的 树 需要 一些 技巧 我们 
虽然 有 x y 坐标 但是 如何 放置 所有 的 
树节点 却是 个 问题 我们/r 必须/d 知道/v 有/v 多少/m 个/q 
叶/nr 节点/n 以便 可以 正确 确定 x 轴 的 长度 
我们 还 需要 知道 树 有 多少 层 以便 可以 
正确 确定 y 轴 的 高度 这里 定义 了 两个 
新 函数 getNumLeafs 和 getTreeDepth 以求/v 叶/nr 节点/n 的/uj 数目/n 
和树的/nr 层数/n 参考 代码 1 def getNumLeafs myTree 2 numLeafs 
= 0 3 firstStr = myTree . keys 0 4 
secondDict = myTree firstStr 5 for key in secondDict . 
keys 6 if type secondDict key . _ _ name 
_ _ = = dict # test to see if 
the nodes are dictonaires if not they are leaf nodes 
7 numLeafs + = getNumLeafs secondDict key 8 else numLeafs 
+ = 1 9 return numLeafs 10 11 def getTreeDepth 
myTree 12 maxDepth = 0 13 firstStr = myTree . 
keys 0 14 secondDict = myTree firstStr 15 for key 
in secondDict . keys 16 if type secondDict key . 
_ _ name _ _ = = dict # test 
to see if the nodes are dictonaires if not they 
are leaf nodes 17 thisDepth = 1 + getTreeDepth secondDict 
key 18 else thisDepth = 1 19 if thisDepth maxDepth 
maxDepth = thisDepth 20 return maxDepth 我们 可以 看到 两个 
方法 有点 似曾相识 没错 我们 在 进行 决策树 分类 测试 
时 用 的 跟 这个 几乎 一样 分类 测试 中的 
isinstance 函数 换了 一种 方式 去 判断 递归 依然 在 
不过是 每 递归 依次 高度 增加 1 叶子 数 同样 
是 检测 是否 为 字典 不是 字典 则 增加 相应 
的 分支 这里 还 写 了 一个 函数 retrieveTree 它 
的 作用 是 预先 存储 的 树 信息 避免了/i 每次/r 
测试代码/n 时/n 都要/i 从/p 数据/n 中创/nz 建树/n 的/uj 麻烦/an 参考/v 
代码/n 如下/t 1/m def retrieveTree i 2 listOfTrees = { 
no surfacing { 0 no 1 { flippers { 0 
no 1 yes } } } } 3 { no 
surfacing { 0 no 1 { flippers { 0 { 
head { 0 no 1 yes } } 1 no 
} } } } 4 5 return listOfTrees i 这个 
没什么 好说 的 就是 把 决策树 的 结果 存在 一个 
函数 中 方便 调用 跟 前面 的 存储 决策树 差不多 
有了 前面 这些 基础 后 我们 就 可以 来 画 
树 了 参考 代码 如下 1 def plotMidText cntrPt parentPt 
txtString 2 xMid = parentPt 0 cntrPt 0 / 2.0 
+ cntrPt 0 3 yMid = parentPt 1 cntrPt 1 
/ 2.0 + cntrPt 1 4 createPlot . ax1 . 
text xMid yMid txtString va = center ha = center 
rotation = 30 5 6 def plotTree myTree parentPt nodeTxt 
# if the first key tells you what feat was 
split on 7 numLeafs = getNumLeafs myTree # this determines 
the x width of this tree 8 depth = getTreeDepth 
myTree 9 firstStr = myTree . keys 0 # the 
text label for this node should be this 10 cntrPt 
= plotTree . xOff + 1.0 + float numLeafs / 
2.0 / plotTree . totalW plotTree . yOff 11 plotMidText 
cntrPt parentPt nodeTxt 12 plotNode firstStr cntrPt parentPt decisionNode 13 
secondDict = myTree firstStr 14 plotTree . yOff = plotTree 
. yOff 1.0 / plotTree . totalD 15 for key 
in secondDict . keys 16 if type secondDict key . 
_ _ name _ _ = = dict # test 
to see if the nodes are dictonaires if not they 
are leaf nodes 17 plotTree secondDict key cntrPt str key 
# recursion 18 else # it s a leaf node 
print the leaf node 19 plotTree . xOff = plotTree 
. xOff + 1.0 / plotTree . totalW 20 plotNode 
secondDict key plotTree . xOff plotTree . yOff cntrPt leafNode 
21 plotMidText plotTree . xOff plotTree . yOff cntrPt str 
key 22 plotTree . yOff = plotTree . yOff + 
1.0 / plotTree . totalD 23 # if you do 
get a dictonary you know it s a tree and 
the first element will be another dict 24 25 def 
createPlot inTree 26 fig = plt . figure 1 facecolor 
= white 27 fig . clf 28 axprops = dict 
xticks = yticks = 29 createPlot . ax1 = plt 
. subplot 111 frameon = False * * axprops 30 
plotTree . totalW = float getNumLeafs inTree 31 plotTree . 
totalD = float getTreeDepth inTree 32 plotTree . xOff = 
0.5 / plotTree . totalW plotTree . yOff = 1.0 
33 plotTree inTree 0.5 1.0 34 plt . show 第一 
个 函数 是 在 父子 节 点中 填充 文本 信息 
函数 中 是 将 父子 节点 的 横 纵坐标 相加 
除以 2 上面 写 得有 一点点 不 一样 但 原理 
是 一样 的 然后 还是 在 这个 中间 坐标 的 
基础 上 添加 文本 还是 用 的 是 createPlot . 
ax1 这个 全局变量 使用 它 的 成员 函数 text 来 
添加 文本 里面 是 它 的 一些 参数 第二个 函数 
是 关键 它 调用 前面 我们 说过 的 函数 用 
树 的 宽度 用于 计算 放置 判断 节点 的 位置 
主要 的 计算 原则 是 将 它 放在 所有 叶子 
节点 的 中间 而 不仅仅 是 它 子 节点 的 
中间 根据 高度 就 可以 平分 坐标 系了 用 坐标系 
的 最大值 除以 高度 就是 每层 的 高度 这个 plotTree 
函数 也 是个 递归函数 每次 都是 调用 画出 一层 知道 
所有 的 分支 都 不是 字典 后 才算 画 完 
每次 检测 出 是 叶子 就 记录 下 它 的 
坐标 并 写出 叶子 的 信息 和 父子 节点 间 
的 信息 plotTree . xOff 和 plotTree . yOff 是 
用来 追踪 已经 绘制 的 节点 位置 以及 放置 下 
一个 节点 的 恰当 位置 第三个 函数 我们 之前 介绍 
介绍 过 一个 类似 这个 函数调用 了 plotTree 函数 最后 
输出 树状 图 这里 只 说 两点 一点 是 全局变量 
plotTree . totalW 存储 树 的 宽度 全 局 变量 
plotTree . totalD 存储 树 的 深度 还有 一点 是 
plotTree . xOff 和 plotTree . yOff 是 在 这个 
函数 这里 初始化 的 最后 我们 来 测试 一下 cd 
桌面 / m a c h i n e l 
e a r n i n g i n a 
c t i o n / Ch03 / home / 
fangyang / 桌面 / m a c h i n 
e l e a r n i n g i 
n a c t i o n / Ch03import treePlotter 
myTree = treePlotter . retrieveTree 0 treePlotter . createPlot myTree 
改变 标签 重新 绘制 图形 myTree no surfacing 3 = 
maybe treePlotter . createPlot myTree 至此 用 matplotlib 画 决策树 
到此结束 3 使用 决策树 预测 眼睛 类型 隐形眼镜 数据集 是 
非常 著名 的 数据 集 它 包含 很多 患者 眼部 
状况 的 观察 条件 以及 医生 推荐 的 隐形眼镜 类型 
隐形眼镜 类型 包括 硬 材质 软 材质 以及 不 适合 
佩戴 隐形眼镜 数据 来源于 UCI 数据库 为了 更 容易 显示 
数据 将 数据 存储 在 源代码 下载 路径 的 文本 
文件 中 进行 测试 import trees lensesTree = trees . 
createTree lenses lensesLabels fr = open lenses . txt lensesTree 
= trees . createTree lenses lensesLabels lenses = inst . 
strip . split \ t for inst in fr . 
readlines lensesLabels = age prescript astigmatic tearRate lensesTree = trees 
. createTree lenses lensesLabels lensesTree { tearRate { normal { 
astigmatic { no { age { pre soft presbyopic { 
prescript { hyper soft myope no lenses } } young 
soft } } yes { prescript { hyper { age 
{ pre no lenses presbyopic no lenses young hard } 
} myope hard } } } } reduced no lenses 
} } 这样 看 非常 乱 看不出 什么 名堂 画出 
决策树 树状 图 看看 treePlotter . createPlot lensesTree 这就 非常 
清楚 了 但 还是 有 一个 问题 决策树 非常 好 
地 匹配 了 实验 数据 然而 这些 匹配 选项 可能 
太 多了 我们 将 这种 问题 称之为 过度 匹配 overfitting 
为了 减少 过度 匹配 问题 我们 可以 裁剪 决策树 去掉 
一些 不 必要 的 叶子 节点 如果 叶子 节点 只能 
增加 少许 信息 则 可以 删除 该 节点 将 它 
并 人 到 其他 叶子 节 点中 这个 将 在后面 
讨论 吧 结尾 这篇 notebook 写了 两天 多 接近 三天 
好累 希望 这 篇 关于 决策树 的 博客 能够 帮助 
到 你 如果 发现 错误 还望 不吝指教 谢谢 觉得 不错 
的 赐 我 金笔 吧 哈哈 我 需要 鼓励 鼓励 
^ _ _ ^ 嘻嘻 百度 云盘 链接 https / 
/ pan . baidu . com / s / 1eSeRQIQ 
密码 3zwm 