引言 随机 森林 在 机器学习 实战 中 没有 讲到 我 
是从 伯克利 大学 的 一个 叫 breiman 的 主页 中 
看到 相关 的 资料 这个 breiman 好像 是 随机 森林 
算法 的 提出者 网址 如下 http / / www . 
stat . berkeley . edu / ~ breiman / RandomForests 
/ cc _ home . htm 随机 森林 算法 简介 
随机 森林 说白 了 就是 很多 个 决策树 组成 在一起 
就 形成 了 森林 关键 在于 如何 创建 森林 里 
的 每 一棵树 随机 森林 用到 的 方法 bootstrap 法 
通俗 的 讲 就是 有 放回 的 抽取 样本 这里有 
个 理论 依据 在 这 说明/v 有/v 放回/v 的/uj 抽取/v 
方法/n 大概/d 有/v 三分之一/mq 的/uj 样本/n 不会/v 被/p 抽/v 取到/v 
在此 我 简单 说 一下 这个 原因 三分之一 的 样本 
不会 被 抽 取到 的 原因 当 N 足够 大 
时 将 收敛 于1／/nr e ≈ 0.368 这表明 原始 样 
本集 D 中 接近 37％ 的 样本 不会 出现 在 
bootstrap 样本 中 这些 数据 称为 袋 外 Out Of 
Bag OOB 数据 使用 这些 数据 来 估计 模型 的 
性能 称为 OOB 估计 构建 决策树 除了 随机 抽取 样本 
外 构建 一个 决策树 还需要 随机 抽取 样本 的 特征 
比如 样本 总共有 100 维 特征 我们 随机 抽取 其中 
的 10 维 特征 构建 决策树 如果 我们 想 构建 
200棵 决策树 的 随机 森林 我们 就 要 这样 随机 
抽取 200次 每次/r 抽取/v 10/m 维/v 特征/n 构建/v 一个/m 决策树/n 
而/c 用于/v 构建/v 决策树/n 的/uj 样本/n 也/d 需要/v 用/p 之前/f 
说/v 的/uj 那种/r bootstrap/w 法有/nr 放回/v 的/uj 随机/d 抽取/v 简单/a 
说/v 一下/m 用/p 于/p 构建/v 一个/m 决策树/n 的/uj 样本/n 集/q 
生成/v 的/uj 过程/n bootstrap/w 法/l 抽取/v 样本/n 过程/n 随机/d 抽取/v 
1个/mq 样本/n 然后 让 回 然后再 随机 抽取 1个 样本 
这样 抽取 N 次 可以 得到 N 个 样本 的 
数据集 用 这 N 个 样本 的 数据集 按照 之前 
随机 选取 的 10 维 特征 遍历 这 10 维 
特征 对 数据 集 进行 划分 得到 一棵 决策树 如果 
要 构建 200 棵树 就 需要 随机 抽取 200次 10 
维 特征 随机 抽取 200次 N 个样 本集 随机 森林 
分类 得到 了 200 棵树 的 随机 森林 如何 用作 
分类 呢 随机 森林 中用 的 OOB 数据测试 随机 森林 
的 分类 结果 之前 说到 bootstrap 方法 会 造成 大概 
三分之一 的 数据 不 会被 采样 的 这 部分 数据 
就 被 称之为 OOB 数据 将 这 部分 数据 放入 
森林 中 每 一棵树 会对 相应 的 数据 得到 一个 
分类 结果 那么/r 最后/f 的/uj 结果/n 会/v 根据/p 投票/n 来/v 
确定/v 为什么/r 不用/v 交叉/n 验证/v 的/uj 方法/n 二用/b OOB/w 的/uj 
方法/n 有/v 一个/m 问题/n 就是/d 这种/r OOB/w 的/uj 方法/n 跟/p 
交叉/n 验证/v 中/f 随机/d 抽取/v 样本/n 有/v 什么/r 区别/n 比如 
十 折 交叉 验证 中 就是 把 数据集 平均 分为 
10 份儿 随机 选取 其中 的 9 份儿 用作 训练 
1 份儿 用作 测试 重复 十次 取 平均值 那么 OOB 
这种 有 放回 的 重 采样 和/c 交叉/n 验证/v 有/v 
什么/r 区别/n 呢/y 一个/m 很/zg 重要/a 的/uj 区别/n 根据/p 作者/n 
的/uj 说法/v 在/p 于/p 计算/v 量/n 用 交叉 验证 CV 
估计 组合 分类器 的 泛化 误差 时 可能 导致 很大 
的 计算 量 从而 降低 算法 的 运行 效率 而 
采用 OOB 数 据估计 组合 分类器 的 泛化 误差 时 
可以 在 构建 各 决策树 的 同时 计算出 OOB 误差率 
最终 只 需 增加 少量 的 计算 就 可以 得到 
相对于 交叉 验证 00B 估计 是 高效 的 且 其 
结果 近似于 交叉 验证 的 结果 