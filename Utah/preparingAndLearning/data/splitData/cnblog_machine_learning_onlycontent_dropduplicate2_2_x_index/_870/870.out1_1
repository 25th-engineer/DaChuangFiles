部分 VIII 机器学习 OpenCV Python 中文 教程 搬运 目录 46 
K 近邻 k Nearest Neighbour 46.1 理解 K 近邻 目标 
• 本节 我们 要 理解 k 近邻 kNN 的 基本 
概念 原理 kNN 可以 说 是 最简单 的 监督 学习 
分类器 了 想法 也 很 简单 就是 找出 测试数据 在 
特征 空间 中 的 最近 邻居 我们 将 使用 下面 
的 图片 介绍 它 上 图中 的 对象 可以 分成 
两组 蓝色 方块 和 红色 三角 每 一组 也 可以 
称为 一个 类 我们 可以 把 所有 的 这些 对象 
看成 是 一个 城镇 中 房子 而 所有 的 房子 
分别 属于 蓝色 和 红色 家族 而 这个 城镇 就是 
所谓 的 特征 空间 你 可以 把 一个 特征 空间 
看成 是 所有 点 的 投影 所在 的 空间 例如 
在 一个 2D 的 坐标 空间 中 每个 数据 都 
两个 特征 x 坐标 和 y 坐标 你 可以 在 
2D 坐标空间 中 表示 这些 数据 如果 每个 数据 都有 
3 个 特征 呢 我们 就 需要 一个 3D 空间 
N 个 特征 就 需要 N 维空间 这个 N 维空间 
就是 特征 空间 在上 图中 我们 可以 认为 是 具有 
两个 特征 色 2D 空间 现在 城镇 中 来 了 
一个 新人 他 的 新 房子 用 绿色 圆盘 表示 
我们 要 根据 他 房子 的 位置 把 他 归为 
蓝色 家族 或 红色 家族 我们 把 这 过程 成为 
分类 我们 应该 怎么做 呢 因为 我们 正在 学习 看 
kNN 那 我们 就 使用 一下 这个 算法 吧 一个 
方法 就是 查看 他 最近 的 邻居 属于 那个/nr 家族 
从 图像 中 我们 知道 最近 的 是 红色 三角 
家族 所以 他 被 分到 红色 家族 这种 方法 被 
称为 简单 近邻 因为 分类 仅仅 决定 与 它 最近 
的 邻居 但是 这里 还有 一个 问题 红色 三角 可能 
是 最近 的 但 如果 他 周围 还有 很多 蓝色 
方块 怎么办 呢 此时 蓝色 方块 对 局部 的 影响 
应该 大于 红色 三角 所以 仅仅 检测 最近 的 一个 
邻居 是 不足 的 所以 我们 检测 k 个 最近 
邻居 谁在 这 k 个 邻居 中 占据 多数 那 
新的 成员 就 属于 谁 那一类 如果 k 等于 3 
也 就是 在 上面 图像 中 检测 3 个 最近 
的 邻居 他/r 有/v 两个/m 红的和/nr 一个/m 蓝的/nr 邻居/v 所以 
他 还是 属于 红色 家族 但是 如果 k 等于 7 
呢 他 有 5 个 蓝色 和 2 个 红色 
邻居 现在 他 就 会被 分到 蓝色 家族 了 k 
的 取值 对 结果 影响 非常 大 更 有趣 的 
是 如果 k 等于 4 呢 两个 红 两个 蓝 
这 是 一个 死结 所以 k 的 取值 最好 为 
奇数 这 中 根据 k 个 最近 邻居 进行 分类 
的 方法 被 称为 kNN 在 kNN 中 我们 考虑 
了 k 个 最近 邻居 但是 我们 给 了 这些 
邻居 相等 的 权重 这样 做 公平 吗 以 k 
等于 4 为例 我们 说 她 是 一个 死结 但是 
两 个 红色 三角比 两个 蓝色 方块 距离 新 成员 
更 近 一些 所以 他 更 应该 被 分为 红色 
家族 那用 数学 应该 如何 表示 呢 我们 要 根据 
每个 房子 与 新 房子 的 距离 对 每个 房子 
赋予 不同 的 权重 距离 近 的 具有 更高 的 
权重 距离远 的 权重 更低 然后 我们 根据 两 个 
家族 的 权重 和来/nr 判断 新 房子 的 归属 谁 
的 权重 大 就 属于 谁 这 被 称为 修 
改过 的 kNN 那这/nr 里面 些 是 重要 的 呢 
• 我们 需要 整个 城镇 中 每个 房子 的 信息 
因为 我们 要 测量 新来者 到 所有 现存 房子 的 
距离 并 在 其中 找到 最近 的 如果 那里 有 
很多 房子 就要 占用 很大 的 内存 和 更多 的 
计算 时间 • 训练 和 处理 几乎 不 需要 时间 
现在 我们 看看 OpenCV 中的 kNN 46 . 1.1 OpenCV 
中的 kNN 我们 这里 来 举 一个 简单 的 例子 
和/c 上面/f 一样/r 有/v 两个/m 类/q 下 一节 我们 会 
有一个 更好 的 例子 这里 我们 将 红色 家族 标记 
为 Class 0 蓝色 家族 标记 为 Class 1 还要 
再 创建 25 个 训练 数据 把 它们 非 别 
标记 为 Class 0 或者 Class 1 Numpy 中 随机数 
产生器 可以 帮助 我们 完成 这个 任务 然后 借助 Matplotlib 
将 这些 点 绘制 出来 红色 家族 显示 为 红色 
三角 蓝色 家族 显示 为 蓝色 方块 import cv2 import 
numpy as np import matplotlib . pyplot as plt # 
Feature set containing x y values of 25 known / 
training data trainData = np . random . randint 0 
100 25 2 . astype np . float32 # Labels 
each one either Red or Blue with numbers 0 and 
1 responses = np . random . randint 0 2 
25 1 . astype np . float32 # Take Red 
families and plot them red = trainData responses . ravel 
= = 0 plt . scatter red 0 red 1 
80 r ^ # Take Blue families and plot them 
blue = trainData responses . ravel = = 1 plt 
. scatter blue 0 blue 1 80 b s plt 
. show 你 可能 会 得到 一个 与 上面 类似 
的 图形 但 不会 完全 一样 因为 你 使用 了 
随机数 产生器 每次 你 运行 代码 都会 得到 不同 的 
结果 下面 就是 kNN 算法 分类器 的 初始化 我们 要 
传入 一个 训练 数据集 以及 与 训练 数据 对应 的 
分类 来 训练 kNN 分类器 构建 搜索 树 最后 要 
使用 OpenCV 中的 kNN 分类器 我们 给 它 一个 测试数据 
让 它 来 进行 分类 在 使用 kNN 之前 我们 
应该 对 测试数据 有所 了解 我们 的 数据 应该 是 
大小 为 数据 数目 乘以 特征 数目 的 浮点 性 
数组 然后 我们 就 可以 通过 计算 找到 测试数据 最近 
的 邻居 了 我们 可以 设置 返回 的 最近 邻居 
的 数目 返回值 包括 1 . 由 kNN 算法 计算 
得到 的 测试数据 的 类别 标志 0 或 1 如果 
你 想 使用 最 近邻 算法 只 需要 将 k 
设置 为 1 k 就是 最 近邻 的 数目 2 
. k 个 最近 邻居 的 类别 标志 3 . 
每个 最近 邻居 到 测试数据 的 距离 让 我们 看看 
它 是 如何 工作 的 测试数据 被 标记 为 绿色 
newcomer = np . random . randint 0 100 1 
2 . astype np . float32 plt . scatter newcomer 
0 newcomer 1 80 g o knn = cv2 . 
KNearest knn . train trainData responses ret results neighbours dist 
= knn . find _ nearest newcomer 3 print result 
results \ n print neighbours neighbours \ n print distance 
dist plt . show 下面 是 我 得到 的 结果 
result 1 . neighbours 1 . 1 . 1 . 
distance 53 . 58 . 61 . 这 说明 我们 
的 测试数据 有 3 个 邻居 他们 都是/nr 蓝色 所以 
它 被 分为 蓝色 家族 结果 很 明显 如下 图 
所示 如果 我们 有 大量 的 数据 要 进行 测试 
可以 直接 传入 一个 数组 对应 的 结果 同样 也是 
数组 # 10 new comers newcomers = np . random 
. randint 0 100 10 2 . astype np . 
float32 ret results neighbours dist = knn . find _ 
nearest newcomer 3 # The results also will contain 10 
labels . 46.2 使用 kNN 对 手写 数字 OCR 目标 
• 要 根据 我们 掌握 的 kNN 知识 创建 一个 
基本 的 OCR 程序 • 使用 OpenCV 自带 的 手写 
数字 和 字母 数据测试 我们 的 程序 46 . 2.1 
手写 数字 的 OCR 我们 的 目的 是 创建 一个 
可以 对 手写 数字 进行 识别 的 程序 为了 达到 
这个 目的 我们 需要 训练 数据 和 测试数据 OpenCV 安装包 
中有 一副 图片 / samples / python2 / data / 
digits . png 其中 有 5000 个 手写 数字 每个 
数字 重复 500遍 每个 数字 是 一个 20x20 的 小 
图 所以 第一 步 就是 将 这个 图像 分割 成 
5000个 不同 的 数字 我们 在 将 拆分 后的/nr 每一个 
数字 的 图像 重 排成 一行 含有 400 个 像素点 
的 新 图像 这个 就是 我们 的 特征 集 所有 
像素 的 灰度 值 这 是 我们 能 创建 的 
最简单 的 特征 集 我们 使用 每个 数字 的 前 
250 个 样本 做 训练 数据 剩余 的 250 个 
做 测试数据 让 我们 先 准备 一下 import numpy as 
np import cv2 from matplotlib import pyplot as plt img 
= cv2 . imread digits . png gray = cv2 
. cvtColor img cv2 . COLOR _ BGR2GRAY # Now 
we split the image to 5000 cells each 20x20 size 
cells = np . hsplit row 100 for row in 
np . vsplit gray 50 # Make it into a 
Numpy array . It size will be 50 100 20 
20 x = np . array cells # Now we 
prepare train _ data and test _ data . train 
= x 50 . reshape 1 400 . astype np 
. float32 # Size = 2500 400 test = x 
50 100 . reshape 1 400 . astype np . 
float32 # Size = 2500 400 # Create labels for 
train and test data k = np . arange 10 
train _ labels = np . repeat k 250 np 
. newaxis test _ labels = train _ labels . 
copy # Initiate kNN train the data then test it 
with test data for k = 1 knn = cv2 
. KNearest knn . train train train _ labels ret 
result neighbours dist = knn . find _ nearest test 
k = 5 # Now we check the accuracy of 
classification # For that compare the result with test _ 
labels and check which are wrong matches = result = 
= test _ labels correct = np . count _ 
nonzero matches accuracy = correct * 100.0 / result . 
size print accuracy 现在 最 基本 的 OCR 程序 已经 
准备好 了 这个 示例 中 我们 得到 的 准确率 为 
91% 改善 准确度 的 一个 办法 是 提供 更多 的 
训练 数据 尤其 是 判断 错误 的 那些 数字 为了/p 
避免/v 每次/r 运行/v 程序/n 都要/nr 准备/v 和/c 训练/vn 分类器/n 我们 
最好 把 它 保留 这样 在 下次 运行 是 时 
只 需要 从 文件 中 读取 这些 数据 开始 进行 
分类 就 可以 了 Numpy 函数 np . savetxt np 
. load 等 可以 帮助 我们 搞定 这些 # save 
the data np . savez knn _ data . npz 
train = train train _ labels = train _ labels 
# Now load the data with np . load knn 
_ data . npz as data print data . files 
train = data train train _ labels = data train 
_ labels 在 我 的 系统 中 占用 的 空间 
大概 为 4.4 M 由于 我们 现在 使用 灰度 值 
unint8 作 为特征 在 保存 之前 最好 先 把 这些 
数据 装 换成 np . uint8 格式 这样 就 只 
需要 占用 1.1 M 的 空间 在 加载 数据 时再/nr 
转会 到 float32 46 . 2.2 英文 字母 的 OCR 
接下来 我们 来做 英文 字母 的 OCR 和 上面 做法 
一样 但是/c 数据/n 和/c 特征/n 集/q 有/v 一些/m 不同/a 现在 
OpenCV 给出 的 不是 图片 了 而是 一个 数据文件 / 
samples / cpp / letter recognition . data 如果 打开 
它 的话 你 会 发现 它 有 20000 行 第 
一样 看上去 就 像是 垃圾 实际上 每 一行 的 第一 
列 是 我们 的 一个 字母 标记 接下来 的 16 
个 数字 是 它 的 不同 特征 这些 特征 来源于 
UCI Machine L e a r n i n g 
R e p o s i t o r y 
你 可以 在 此 页 找到 更多 相关 信息 有 
20000 个 样本 可以 使用 我们 取 前 10000 个 
作为 训练样本 剩下 的 10000 个 作为 测试 样本 我们 
应在 先把 字母表 转换成 asc 码 因为 我们 不 正 
直接 处理 字母 import cv2 import numpy as np import 
matplotlib . pyplot as plt # Load the data converters 
convert the letter to a number data = np . 
loadtxt letter recognition . data dtype = float32 delimiter = 
converters = { 0 lambda ch ord ch ord A 
} # split the data to two 10000 each for 
train and test train test = np . vsplit data 
2 # split trainData and testData to features and responses 
responses trainData = np . hsplit train 1 labels testData 
= np . hsplit test 1 # Initiate the kNN 
classify measure accuracy . knn = cv2 . KNearest knn 
. train trainData responses ret result neighbours dist = knn 
. find _ nearest testData k = 5 correct = 
np . count _ nonzero result = = labels accuracy 
= correct * 100 . 0/10000 print accuracy 准确率 达 
到了 93.22% 同样 你 可以 通过 增加 训练 样本 的 
数量 来 提高 准确率 47 支持 向量 机 47.1 理解 
SVM 目标 • 对 SVM 有 一个 直观 理解 原理 
47 . 1.1 线性 数据 分割 如下 图 所示 其中 
含有 两类 数据 红的和/nr 蓝的/nr 如果 是 使用 kNN 对于 
一个 测试 数据 我们 要 测量 它 到 每一个 样本 
的 距离 从而 根据 最近 邻居 分类 测量 所有 的 
距离 需要 足够 的 时间 并且 需要 大量 的 内存 
存储 训练样本 但是 分类 下 图 所示 的 数据 真的 
需要 占用 这么多 资源 吗 我们 在 考虑 另外 一个 
想法 我们 找到 了 一条 直线 f x = ax 
1 + bx 2 + c 它 可以 将 所有 
的 数据 分割 到 两个 区域 当 我们 拿到 一个 
测试数据 X 时 我们 只 需要 把 它 代入 f 
x 如果 | f X | 0 它 就 属于 
蓝色 组 否则 就 属于 红色 组 我们 把 这条 
线 称为 决定 边界 Decision _ Boundary 很 简单 而且 
内存 使用 效率 也 很高 这种 使用 一条 直线 或者 
是 高位 空 间种 的 超平面 上述 数据 分成 两组 
的 方法 成为 线性 分割 从上 图中 我们 看到 有 
很多 条 直线 可以 将 数据 分为 蓝红 两组 那 
一条 直线 是 最好 的 呢 直觉 上 讲 这条 
直线 应该 是 与 两组 数据 的 距离 越远 越好 
为什么 呢 因为 测试数据 可能有 噪音 影响 真实 数据 + 
噪声 这些 数据 不 应该 影响 分类 的 准确性 所以 
这条 距离远 的 直线 抗 噪声 能力 也就 最强 所以 
SVM 要做 就是 找到 一条 直线 并使 这条 直线 到 
训练样本 各组 数据 的 最短距离 最大 下 图中 加粗 的 
直线 经过 中心 要 找到 决定 边界 就 需要 使用 
训练 数据 我们 需要 所有 的 训练 数据 吗 不 
只需要 那些 靠近 边界 的 数据 如上图 中 一个 蓝色 
的 圆盘 和 两个 红色 的 方块 我们 叫 他们 
支持 向量 经过 他们 的 直线 叫做 支持 平面 有了/nr 
这些 数据 就 足以 找到 决定 边界 了 我们 担心 
所有 的 数据 这 对于 数据 简化 有帮助 We need 
not worry about all the data . It helps in 
data reduction . 到底 发生 了 什么 呢 首先 我们 
找到 了 分别 代表 两 组 数据 的 超平面 例如 
蓝色 数据 可以 用   表示 而 红色 数据 可以 
用   表示 ω 叫做 权重 向量   x 为 
特征向量 b 0 被 成为 bias 截距 权重 向量 决定 
了 决定 边界 的 走向 而 bias 点 决定 了 
它 决定 边界 的 位置 决定 边界 被 定义 为 
这两个 超平面 的 中间线 平面 表达式 为   从/p 支持/v 
向/p 量到/i 决定/v 边界/n 的/uj 最短距离/i 为/p  /i 边缘 长度 
为 这个 距离 的 两倍 我们 需要 使 这个 边缘 
长度 最大 我们 要 创建 一个 新 的 函数   
并使 它 的 值 最小 其中 t i 是 每 
一组 的 标记 . 47 . 1.2 非线性 数据 分割 
想象 一下 如果 一组 数据 不能 被 一条 直线 分为 
两组 怎么办 例如 在 一维空间 中 X 类 包含 的 
数据 点 有 3 3 O 类 包含 的 数据 
点 有 1 1 很 明显 不 可能 使用 线性 
分割 将 X 和 O 分开 但是 有 一个 方法 
可以 帮 我们 解决 这个 问题 使用 函数   对这 
组 数据 进行 映射 得到 的 X 为 9 O 
为 1 这时 就 可以 使用 线性 分割 了 或者 
我们 也 可以 把 一维 数据 转换成 两维 数据 我们 
可以 使用 函数 对 数据 进行 映射 这样 X 就 
变成 了 3 9 和 3 9 而 O 就 
变成 了 1 1 和 1 1 同样 可以 线性 
分割 简单 来说 就是 在 低 维空间 不能 线性 分割 
的 数据 在 高维空间 很 有可能 可以 线性 分割 通常 
我们 可以 将 d 维 数据 映 射到 D 维 
数据 来 检测 是否 可以 线性 分割 D d 这种 
想法 可以 帮助 我们 通过 对 低维 输入 特征 空间 
的 计算 来 获得 高维空间 的 点积 我们 可以 用 
下面 的 例子 说明 假设 我们 有 二维 空间 的 
两个 点 p = p 1 p 2 和 q 
= q 1 q 2 用 Ø 表示 映射函数 它 
可以 按 如下 方式 将 二维 的 点映 射到 三维空间 
中 我们 要 定义 一个 核 函数 K p q 
它 可以 用 来 计算 两个 点 的 内积 如下 
所示 这说明 三维空间 中的 内积 可以 通过 计算 二维 空间 
中 内积 的 平方 来 获得 这可以 扩展 到 更高 
维 的 空间 所以 根据 低维 的 数据 来 计算 
它们 的 高维 特征 在 进行 完 映射 后 我们 
就 得到 了 一个 高维 空间数据 除了 上面 的 这些 
概念 之外 还有 一个 问题 需要 解决 那 就是 分类 
错误 仅仅 找到 具有 最大 边缘 的 决定 边界 是 
不够 的 我们 还 需要 考虑 错误 分类 带来 的 
误差 有时 我们 找到 的 决定 边界 的 边缘 可能 
不是 最大 的 但是 错误 分类 是 最少 的 所以 
我们 需要 对 我们 的 模型 进行 修正 来 找到 
一个 更好 的 决定 边界 最大 的 边缘 最小 的 
错误 分类 评判 标准 就 被 修改 为 下图 显示 
这个 概念 对于 训练 数据 的 每一个 样本 又 增加 
了 一个 参数 ξ i 它 表示 训练样本 到 他们 
所属 类 实际 所属 类 的 超平面 的 距离 对于 
那些 分类 正确 的 样本 这个 参数 为 0 因为 
它们 会 落在 它们 的 支持 平面 上 现在 新 
的 最优 化 问题 就 变成 了 参数 C 的 
取值 应该 如何 选择 呢 很 明显 应该 取决于 你 
的 训练 数据 虽然 没有 一个 统一 的 答案 但是 
在 选取 C 的 取值 时 我们 还是 应该 考虑 
一下 下面 的 规则 • 如果 C 的 取值 比较 
大 错误 分类 会 减少 但是 边缘 也会 减小 其实 
就是 错误 分类 的 代价 比较 高 惩罚 比较 大 
在 数据 噪声 很小 时 我们 可以 选取 较大 的 
C 值 • 如果 C 的 取值 比较 小 边缘 
会 比较 大 但 错误 分类 的 数量 会 升高 
其实 就是 错误 分类 的 代价 比较 低 惩罚 很小 
整个 优化 过程 就是 为了 找到 一个 具有 最大 边缘 
的 超平面 对 数据 进行 分类 如果 数据 噪声 比 
较大 时 应该 考虑 47.2 使用 SVM 进行 手写 数据 
OCR 目标 本节 我们 还是 要 进行 手写 数据 的 
OCR 但 这次 我们 使用 的 是 SVM 而 不是 
kNN 手写 数字 的 OCR 在 kNN 中 我们 直接 
使用 像素 的 灰度 值 作为 特征向量 这次 我们 要 
使用 方向 梯度 直方图 Histogram of Oriented Gradients HOG 作为 
特征向量 在 计算 HOG 前 我们 使用 图片 的 二阶 
矩 对其 进行 抗 扭 斜 deskew 处理 所以 我们 
首先 要 定义 一个 函数 deskew 它 可以 对 一个 
图像 进行 抗 扭 斜 处理 下面 就是 deskew 函数 
def deskew img m = cv2 . moments img if 
abs m mu02 1e 2 return img . copy skew 
= m mu11 / m mu02 M = np . 
float32 1 skew 0.5 * SZ * skew 0 1 
0 img = cv2 . warpAffine img M SZ SZ 
flags = affine _ flags return img 下图 显示 了 
对 含有 数字 0 的 图片 进行 抗 扭 斜 
处理 后的/nr 效果 左侧 是 原始 图像 右侧 是 处理 
后的/nr 结果 接下来 我们 要 计算 图像 的 HOG 描述符 
创建 一个 函数 hog 为此 我们 计算 图像 X 方向 
和 Y 方向 的 Sobel 导数 然后 计算 得到 每个 
像素 的 梯度 的 方向 和 大小 把 这个 梯度 
转换成 16 位 的 整数 将 图像 分为 4 个 
小 的 方块 对 每 一个 小 方块 计算 它们 
的 朝向 直方图 16 个 bin 使用 梯度 的 大小 
做 权重 这样 每 一个 小 方块 都会 得到 一个 
含有 16 个 成员 的 向量 4 个 小方块 的 
4 个 向量 就 组成 了 这个 图像 的 特征向量 
包含 64 个 成员 这 就是 我们 要 训练 数据 
的 特征向量 def hog img gx = cv2 . Sobel 
img cv2 . CV _ 32F 1 0 gy = 
cv2 . Sobel img cv2 . CV _ 32F 0 
1 mag ang = cv2 . cartToPolar gx gy # 
quantizing binvalues in 0 . . . 16 bins = 
np . int32 bin _ n * ang / 2 
* np . pi # Divide to 4 sub squares 
bin _ cells = bins 10 10 bins 10 10 
bins 10 10 bins 10 10 mag _ cells = 
mag 10 10 mag 10 10 mag 10 10 mag 
10 10 hists = np . bincount b . ravel 
m . ravel bin _ n for b m in 
zip bin _ cells mag _ cells hist = np 
. hstack hists return hist 最后 和 前面 一样 我们 
将 大图 分割 成小图/nr 使用 每个 数字 的 前 250 
个 作为 训练 数据 后 250 个 作为 测试数据 全部 
代码 如下 所示 import cv2 import numpy as np SZ 
= 20 bin _ n = 16 # Number of 
bins svm _ params = dict kernel _ type = 
cv2 . SVM _ LINEAR svm _ type = cv2 
. SVM _ C _ SVC C = 2.67 gamma 
= 5.383 affine _ flags = cv2 . WARP _ 
INVERSE _ MAP | cv2 . INTER _ LINEAR def 
deskew img m = cv2 . moments img if abs 
m mu02 1e 2 return img . copy skew = 
m mu11 / m mu02 M = np . float32 
1 skew 0.5 * SZ * skew 0 1 0 
img = cv2 . warpAffine img M SZ SZ flags 
= affine _ flags return img def hog img gx 
= cv2 . Sobel img cv2 . CV _ 32F 
1 0 gy = cv2 . Sobel img cv2 . 
CV _ 32F 0 1 mag ang = cv2 . 
cartToPolar gx gy bins = np . int32 bin _ 
n * ang / 2 * np . pi # 
quantizing binvalues in 0 . . . 16 bin _ 
cells = bins 10 10 bins 10 10 bins 10 
10 bins 10 10 mag _ cells = mag 10 
10 mag 10 10 mag 10 10 mag 10 10 
hists = np . bincount b . ravel m . 
ravel bin _ n for b m in zip bin 
_ cells mag _ cells hist = np . hstack 
hists # hist is a 64 bit vector return hist 
img = cv2 . imread digits . png 0 cells 
= np . hsplit row 100 for row in np 
. vsplit img 50 # First half is trainData remaining 
is testData train _ cells = i 50 for i 
in cells test _ cells = i 50 for i 
in cells # # # # # # Now training 
# # # # # # # # # # 
# # # # # # # # # # 
# # # # deskewed = map deskew row for 
row in train _ cells hogdata = map hog row 
for row in deskewed trainData = np . float32 hogdata 
. reshape 1 64 responses = np . float32 np 
. repeat np . arange 10 250 np . newaxis 
svm = cv2 . SVM svm . train trainData responses 
params = svm _ params svm . save svm _ 
data . dat # # # # # # Now 
testing # # # # # # # # # 
# # # # # # # # # # 
# # # # # deskewed = map deskew row 
for row in test _ cells hogdata = map hog 
row for row in deskewed testData = np . float32 
hogdata . reshape 1 bin _ n * 4 result 
= svm . predict _ all testData # # # 
# # # # Check Accuracy # # # # 
# # # # # # # # # # 
# # # # # # # # # # 
mask = result = = responses correct = np . 
count _ nonzero mask print correct * 100.0 / result 
. size 准确率 达到 了 94% 你 可以 尝试 一下 
不同 的 参数值 看看 能 不能 达到 更高 的 准确率 
或者 也 可以 读 一下 这个 领域 的 文章 并用 
代码 实现 它 48 K 值 聚 类 48.1 理解 
K 值 聚 类 目标 • 本节 我们 要 学习 
K 值 聚 类 的 概念 以及 它 是 如何 
工作 的 原理 我 将用 一个 最 常用 的 例子 
来 给 大家 介绍 K 值 聚 类 48 . 
1.1 T 恤 大小 问题 话说 有 一个 公司 要 
生产 一批 新的 T 恤 很明显 他们 要 生产 不同 
大小 的 T 恤 来 满足 不同 顾客 的 需求 
所以 这个 公司 收集 了 很多 人 的 身高 和 
体重 信息 并 把 这些 数据 绘制 在 图上 如下 
所示 肯定 不能 把 每个 大小 的 T 恤 都 
生产 出来 所以 他们 把 所有 的 人 分为 三组 
小 中 大 这 三组 要 覆盖 所有 的 人 
我们 可以 使用 K 值 聚 类 的 方法 将 
所有 人 分为 3 组 这个 算 法 可以 找到 
一个 最好 的 分法 并能 覆盖 所有 人 如果 不 
能 覆盖 全部 人 的话 公司 就 只能 把 这些 
人 分为 更多 的 组 可能 是 4 个 或 
5 个 甚至 更多 如 下图 48 . 1.2 它 
是 如何 工作 的 这个 算法 是 一个 迭代 过程 
我们 会 借助 图片 逐步 介绍 它 考虑 下面 这 
组 数据 你 也 可以 把 它 当成 T 恤 
问题 我们 需要 把 他们 分成 两组 第一步 随机 选取 
两个 重心点 C 1 和 C 2 有时 可以 选取 
数据 中 的 两个 点 作为 起始 重心 第二步 计算 
每个 点到 这两个 重心点 的 距离 如果 距离 C 1 
比较 近 就 标记 为 0 如果 距离 C 2 
比较 近 就 标记 为 1 如果 有 更多 的 
重心点 可以 标记 为 2 3 等 在 我们 的 
例子 中 我们 把 属于 0 的 标记 为 红色 
属于 1 的 标记 为 蓝色 我们 就 会 得到 
下面 这幅 图 第三步 重新 计算 所有 蓝色 点 的 
重心 和 所有 红色 点 的 重心 并以 这 两个 
点 更新 重心点 的 位置 图片 只 是 为了 演示 
说明 而已 并不 代表 实际 数据 重复 步骤 2 更新 
所有 的 点 标记 我们 就 会 得到 下面 的 
图 继续 迭代 步骤 2 和 3 直到 两个 重心点 
的 位置 稳定下来 当然 也 可以 通过 设置 迭代 次数 
或者 设置 重心 移动 距离 的 阈值 来 终止 迭代 
此时 这些 点 到 它们 相应 重心 的 距离 之和 
最小 此时 这些 点 到 它们 相应 重心 的 距离 
之和 最小 简单 来说 C 1 到 红色 点 的 
距离 与 C 2 到 蓝色 点 的 距离 之和 
最小 最终 结果 如下 图 所示 这 就是 对 K 
值 聚 类 的 一个 直观 解释 要 想知道 更多 
细节 和 数据 解释 你 应该 读 一本 关于 机器 
学习 的 教科书 或者 参考 更多 资源 中 的 链接 
这 只是 K 值 聚 类 的 基础 现在 对 
这个 算法 有 很多 改进 比如 如何 选取 好 的 
起始 重心点 怎样 加速 迭代 过程 等 更多 资源 1 
. Machine Learning Course Video lectures by Prof . Andrew 
Ng Some of the images are taken from this 48.2 
OpenCV 中的 K 值 聚 类 目标 • 学习 使用 
OpenCV 中的 函数 cv2 . kmeans 对 数据 进行 分类 
48 . 2.1 理解 函数 的 参数 输入 参数 理解 
函数 的 参数 输入 参数 1 . samples 应该 是 
np . float32 类型 的 数据 每个 特征 应该 放在 
一列 2 . nclusters K 聚 类 的 最终 数目 
3 . criteria 终止 迭代 的 条件 当 条件 满足 
时 算法 的 迭代 终止 它 应该 是 一个 含有 
3 个 成员 的 元组 它们 是 typw max _ 
iter epsilon • type 终止 的 类型 有 如下 三 
种 选择 – cv2 . TERM _ CRITERIA _ EPS 
只有 精确度 epsilon 满足 是 停止 迭代 – cv2 . 
TERM _ CRITERIA _ MAX _ ITER 当 迭代 次数 
超过 阈值 时 停止 迭代 – cv2 . TERM _ 
CRITERIA _ EPS + cv2 . TERM _ CRITERIA _ 
MAX _ ITER 上面 的 任何 一个 条件 满足 时 
停止 迭代 • max _ iter 表示 最大 迭代 次数 
• epsilon 精确度 阈值 4 . attempts 使用 不同 的 
起始 标记 来 执行 算法 的 次数 算法 会 返回 
紧密度 最好 的 标记 紧密度 也会 作为 输出 被 返回 
5 . flags 用来 设置 如何 选择 起始 重心 通常 
我们 有 两个 选择 cv2 . KMEANS _ PP _ 
CENTERS 和 cv2 . KMEANS _ RANDOM _ CENTERS 输出 
参数 1 . compactness 紧密度 返回 每个 点 到 相应 
重心 的 距离 的 平方和 2 . labels 标志 数组 
与 上 一节 提到 的 代码 相同 每个 成员 被 
标记 为 0 1 等 3 . centers 由 聚 
类 的 中心 组成 的 数组 现在 我们 用 3 
个 例子 来 演示 如何 使用 K 值 聚 类 
48 . 2.2 仅有 一个 特征 的 数据 假设 我们 
有 一组 数据 每个 数据 只有 一个 特征 1 维 
例如 前面 的 T 恤 问题 我们 只 使用 人们 
的 身高 来 决定 T 恤 的 大小 我们 先 
来 产生 一些 随机 数据 并 使用 Matplotlib 将 它们 
绘制 出来 import numpy as np import cv2 from matplotlib 
import pyplot as plt x = np . random . 
randint 25 100 25 y = np . random . 
randint 175 255 25 z = np . hstack x 
y z = z . reshape 50 1 z = 
np . float32 z plt . hist z 256 0 
256 plt . show 现在 我们 有 一个 长度 为 
50 取值 范围 为 0 到 255 的 向量 z 
我 已经 将 向量 z 进行 了 重排 将 它 
变成 了 一个 列 向量 当 每个 数据 含有 多个 
特征 是 这 会很 有用 然后 我们 数据类型 转换成 np 
. float32 我们 得到 下图 现在 我们 使用 KMeans 函数 
在这之前 我们 应该 首先 设置 好 终止 条件 我 的 
终止 条件 是 算法 执行 10 次 迭代 或者 精确度 
epsilon = 1.0 # Define criteria = type max _ 
iter = 10 epsilon = 1.0 criteria = cv2 . 
TERM _ CRITERIA _ EPS + cv2 . TERM _ 
CRITERIA _ MAX _ ITER 10 1.0 # Set flags 
Just to avoid line break in the code flags = 
cv2 . KMEANS _ RANDOM _ CENTERS # Apply KMeans 
compactness labels centers = cv2 . kmeans z 2 None 
criteria 10 flags 返回值 有 紧密度 compactness 标志 和 中心 
在 本例 中 我 的 到 的 中心 是 60 
和 207 标志 的 数目 与 测试 数据 的 多少 
是 相同 的 每个 数据 都会 被 标记 上 0 
1 等 这 取决 与 它们 的 中心 是 什么 
现在 我们 可以 根据 它们 的 标志 将 把 数据 
分 两组 A = z labels = = 0 B 
= z labels = = 1 现在 将 A 组 
数用 红色 表示 将 B 组 数据 用 蓝色 表示 
重心 用 黄色 表示 # Now plot A in red 
B in blue centers in yellow plt . hist A 
256 0 256 color = r plt . hist B 
256 0 256 color = b plt . hist centers 
32 0 256 color = y plt . show 下面 
就是 结果 含有 多个 特征 的 数据 在 前面 的 
T 恤 例子 中 我们 只 考虑 了 身高 现在 
我们 也 把 体重 考虑 进去 也 就是 两个 特征 
在前 一节 我们 的 数据 是 一个 单列 向量 每 
一个 特征 被 排列成 一列 每 一行 对应 一个 测试 
样本 在 本例 中 我们 的 测试数据 适应 50x2 的 
向量 其中 包含 50 个人 的 身高 和 体重 第一列 
对应 与 身高 第二列 对应 与 体重 第一行 包含 两个 
元素 第一个 是 第一 个人 的 身高 第二个 是 第一 
个人 的 体重 剩下 的 行 对应 与 其他人 的 
身高 和 体重 如下 图 所示 现在 我们 来 编写 
代码 import numpy as np import cv2 from matplotlib import 
pyplot as plt X = np . random . randint 
25 50 25 2 Y = np . random . 
randint 60 85 25 2 Z = np . vstack 
X Y # convert to np . float32 Z = 
np . float32 Z # define criteria and apply kmeans 
criteria = cv2 . TERM _ CRITERIA _ EPS + 
cv2 . TERM _ CRITERIA _ MAX _ ITER 10 
1.0 ret label center = cv2 . kmeans Z 2 
None criteria 10 cv2 . KMEANS _ RANDOM _ CENTERS 
# Now separate the data Note the flatten A = 
Z label . ravel = = 0 B = Z 
label . ravel = = 1 # Plot the data 
plt . scatter A 0 A 1 plt . scatter 
B 0 B 1 c = r plt . scatter 
center 0 center 1 s = 80 c = y 
marker = s plt . xlabel Height plt . ylabel 
Weight plt . show 下面 是 我 得到 的 结果 
48 . 2.3 颜色 量化 颜色 量化 就是 减少 图片 
中 颜色 数目 的 一个 过程 为什么 要 减少 图 
片中 的 颜色 呢 减少 内存 消耗 有些 设备 的 
资源 有限 只能 显示 很少 的 颜色 在 这种 情况下 
就 需要 进行 颜色 量化 我们 使用 K 值 聚 
类 的 方法 来 进行 颜色 量化 没有 什么 新 
的 知识 需要 介绍 了 现在 有 3 个 特征 
R G B 所以 我们 需要 把 图片 数据 变 
形成 Mx3 M 是 图片 中 像素点 的 数目 的 
向量 聚 类 完成后 我们 用 聚 类 中心 值 
替换 与其 同组 的 像素 值 这样 结果 图片 就只 
含有 指定 数目 的 颜色 了 下面 是 代码 import 
numpy as np import cv2 img = cv2 . imread 
home . jpg Z = img . reshape 1 3 
# convert to np . float32 Z = np . 
float32 Z # define criteria number of clusters K and 
apply kmeans criteria = cv2 . TERM _ CRITERIA _ 
EPS + cv2 . TERM _ CRITERIA _ MAX _ 
ITER 10 1.0 K = 8 ret label center = 
cv2 . kmeans Z K None criteria 10 cv2 . 
KMEANS _ RANDOM _ CENTERS # Now convert back into 
uint8 and make original image center = np . uint8 
center res = center label . flatten res2 = res 
. reshape img . shape cv2 . imshow res2 res2 
cv2 . waitKey 0 cv2 . d e s t 
r o y A l l W i n d 
o w s 下面 是 K = 8 的 结果 
