转 自 http / / blog . csdn . net 
/ woaidapaopao / article / details / 77806273 第一 部分 
深度 学习 1 神经 网络 基础 问题 1 Backpropagation 要能 
推倒 后向 传播 是 在 求解 损失 函数 L 对 
参数 w 求导 时候 用到 的 方法 目的 是 通过 
链式法则 对 参数 进行 一层 一层 的 求导 这里 重点 
强调 要将 参数 进行 随机 初始化 而 不是 全部 置 
0 否则 所有 隐 层 的 数值 都会 与 输入 
相关 这 称为 对称 失效 大致 过程 是 首/m 先前/t 
向/p 传导/n 计算/v 出/v 所有/b 节点/n 的/uj 激活/a 值/n 和/c 
输出/v 值/n 计算 整体 损失 函数 然后 针对 第 L 
层 的 每个 节点 计算出 残差 这里 是 因为 UFLDL 
中说 的 是 残差 本质 就是 整体 损失 函数 对 
每 一层 激活 值 Z 的 导数 所以 要对 W 
求导 只要 再 乘上 激活 函数 对 W 的 导数 
即可 2 梯度 消失 梯度 爆炸 梯度 消失 这 本质上 
是 由于 激活 函数 的 选择 导致 的 最 简单 
的 sigmoid 函数 为例 在 函数 的 两端 梯度 求导 
结果 非常 小 饱和 区 导致 后向 传播 过程 中 
由于 多次 用到 激活 函数 的 导 数值 使得 整体 
的 乘积 梯度 结果 变得 越来越 小 也就 出现 了 
梯度 消失 的 现象 梯度 爆炸 同理 出现 在 激活 
函数 处在 激活 区 而且 权重 W 过大 的 情况 
下 但是 梯度 爆炸 不如 梯度 消失 出现 的 机会 
多 3 常用 的 激活 函数 激活 函数 公式 缺点 
优点 Sigmoid σ x = 1 / 1 + e 
− x σ x = 1 / 1 + e 
− x 1 会有 梯度 弥散 2 不是 关于 原点 
对称 3 计算 exp 比较 耗时 Tanhtanh x = 2 
σ 2x − 1tanh ⁡ x = 2 σ 2x 
− 1 梯度 弥散 没 解决 1 解决 了 原点 
对称 问题 2 比 sigmoid 更快 ReLUf x = max 
0 x f x = max 0 x 梯度 弥散 
没 完全 解决 在 部分 相当于 神经元 死亡 而且 不会 
复活 1 解决 了 部分 梯度 弥散 问题 2 收敛 
速度 更快 Leaky ReLUf x = 1 x 0 α 
x + 1 x = 0 x f x = 
1 x 0 α x + 1 x = 0 
x 解决 了 神经 死亡 问题 Maxoutmax wT1x + b1 
wT2x + b2 max w1Tx + b1 w2Tx + b2 
参数 比较 多 本质上 是 在 输出 结果 上 又 
增加 了 一层 克服 了 ReLU 的 缺点 比较 提倡 
使用 4 参数 更 新方法 方法 名称 公式 Vanilla updatex 
+ = learning _ rate * dxMomentum update 动量 更新 
v = mu * v learning _ rate * dx 
# integrate velocityx + = v # integrate positionNesterov Momentumx 
_ ahead = x + mu * vv = mu 
* v learning _ rate * dx _ aheadx + 
= vAdagrad 自适应 的 方法 梯度 大 的 方向 学习率 
越来越 小 由 快到 慢 cache + = dx * 
* 2x + = learning _ rate * dx / 
np . sqrt cache + eps Adamm = beta1 * 
m + 1 beta1 dxv = beta2 * v + 
1 beta2 dx * * 2 x + = learning 
_ rate * m / np . sqrt v + 
eps 5 解决 overfitting 的 方法 dropout regularization batch normalizatin 
但是 要 注意 dropout 只在 训练 的 时候 用 让 
一部分 神经元 随机 失 活 Batch normalization 是 为了 让 
输出 都是 单位 高斯 激活 方法 是 在 连接 和 
激活 函数 之间 加入 BatchNorm 层 计算 每个 特征 的 
均值 和 方差 进行 规则化 2 CNN 问题 1 思想 
改变 全 连接 为 局部 连接 这 是 由于 图片 
的 特殊性 造成 的 图像 的 一部分 的 统计 特性 
与 其他 部分 是 一样 的 通过 局部 连接 和 
参数 共享 大 范围 的 减少 参数值 可以 通过 使用 
多个 filter 来 提取 图片 的 不同 特征 多 卷积 
核 2 filter 尺寸 的 选择 通常 尺寸 多为 奇数 
1 3 5 7 3 输出 尺寸 计算公式 输出 尺寸 
= N F + padding * 2 / stride + 
1 步长 可以 自由 选择 通过 补零 的 方式 来 
实现 连接 4 pooling 池化的/nr 作用 虽然 通过 . 卷积 
的 方式 可以 大 范围 的 减少 输出 尺寸 特征 
数 但是 依然 很难 计算 而且 很容易 过拟合 所以 依然 
利用 图片 的 静态 特性 通过 池化的/nr 方式 进一步 减少 
尺寸 5 常用 的 几个 模型 这个 最好 能 记住 
模型 大致 的 尺寸 参数 名称 特点 LeNet5 – 没啥 
特点 不过 是 第一 个 CNN 应该 要 知道 AlexNet 
引入 了 ReLU 和 dropout 引入 数据 增强 池化/nr 相互之间/l 
有/v 覆盖/v 三个/m 卷积/n 一个/m 最大/a 池化+/nr 三个/m 全/a 连接/v 
层/q VGGNet/w 采用/v 1/m */i 1/m 和3*/nr 3/m 的/uj 卷积/n 
核/n 以及/c 2/m */i 2/m 的/uj 最大/a 池化/nr 使得/v 层数/n 
变得/v 更深/d 常用 VGGNet 16 和 VGGNet19Google Inception Net 我 
称为 盗梦 空间 网络 这个 在 控制 了 计算 量 
和参/nr 数量 的 同时 获得 了 比较 好 的 分类 
性能 和/c 上面/f 相比/v 有/v 几个/m 大/a 的/uj 改进/v 1 
去除 了 最后 的 全 连接 层 而是 用 一个 
全局 的 平均 池化来/nr 取代 它 2 引入 Inception Module 
这 是 一个 4个 分支 结合 的 结构 所有 的 
分支 都用 到了 1 * 1 的 卷积 这 是因为 
1 * 1 性价比 很高 可以 用 很少 的 参数 
达到 非线性 和 特征 变换 3 Inception V2 第二 版 
将 所有 的 5 * 5 变成 2个 3 * 
3 而且 提出来 著名 的 Batch Normalization 4 Inception V3 
第三版 就 更 变态 了 把 较大 的 二维 卷积 
拆成 了 两个 较小 的 一维 卷积 加速 运算 减少 
过拟合 同时 还 更改 了 Inception Module 的 结构 微软 
ResNet 残差 神经网络 Residual Neural Network 1 引入 高速公路 结构 
可以 让 神经 网络 变得 非常 深 2 ResNet 第二 
个 版本 将 ReLU 激活 函数 变成 y = x 
的 线性函数 2 RNN1 RNN 原理 在 普通 的 全 
连接 网络 或 CNN 中 每层 神经元 的 信号 只能 
向上/nr 一层 传播 样本 的 处理 在 各个 时刻 独立 
因此 又 被 成为 前 向 神经网络 Feed forward + 
Neural + Networks 而在 RNN 中 神经元 的 输出 可以 
在 下 一个 时间 戳 直接 作用 到 自身 即 
第 i 层 神经元 在 m 时刻 的 输入 除了 
i 1 层 神经元 在 该 时刻 的 输出 外 
还包括 其 自身 在 m 1 时刻 的 输出 所以 
叫 循环 神经网络 2 RNN LSTM GRU 区别 RNN 引入 
了 循环 的 概念 但是 在 实际 过程 中 却 
出现 了 初始 信息 随 时间 消失 的 问题 即 
长期 依赖 Long Term Dependencies 问题 所以 引入 了 LSTM 
LSTM 因为 LSTM 有进有出 且 当前 的 cell informaton 是 
通过 input gate 控制 之后 叠加 的 RNN 是 叠 
乘 因此 LSTM 可以 防止 梯度 消失 或者 爆炸 推导 
forget gate input gate cell state hidden information 等 因为 
LSTM 有进有出 且 当前 的 cell informaton 是 通过 input 
gate 控制 之后 叠加 的 RNN 是 叠 乘 因此 
LSTM 可以 防止 梯度 消失 或者 爆炸 的 变化 是 
关键 下图 非常 明确 适合 记忆 GRU 是 LSTM 的 
变体 将 忘记 门 和 输入 们 合成 了 一个 
单一 的 更新 门 3 LSTM/w 防止/v 梯度/n 弥散/v 和/c 
爆炸/v LSTM/w 用/p 加/v 和的/nr 方式/n 取代/v 了/ul 乘积/n 使得 
很难 出现 梯度 弥散 但是 相应 的 更大 的 几率 
会 出现 梯度 爆炸 但是 可以 通过 给 梯度 加 
门限 解决 这 一 问题 4 引出 word2vec 这个 也 
就是 Word Embedding 是 一种 高效 的 从 原始 语料 
中 学习 字词 空间 向量 的 预测模型 分为 CBOW Continous 
Bag of Words 和 Skip Gram 两种 形式 其中 CBOW 
是从 原始 语句 推测 目标 词汇 而 Skip Gram 相反 
CBOW 可以 用于 小 语料库 Skip Gram 用于 大 语料库 
具体 的 就 不是 很 会了 3 GAN1 GAN 的 
思想 GAN 结合 了 生成 模型 和 判别 模型 相当于 
矛与盾 的 撞击 生成 模型 负责 生成 最好 的 数据 
骗过 判别 模型 而 判别 模型 负责 识别 出 哪些 
是 真的 哪些 是 生成 模型 生成 的 但是 这些 
只是 在 了解 了 GAN 之后 才 体会 到 的 
但是 为什么 这样 会 有效 呢 假设 我们 有 分布 
Pdata x 我们 希望 能 建立 一个 生成 模型 来 
模拟 真实 的 数据 分布 假设 生成 模型 为 Pg 
x θ θ 我们 的 目的 是 求解 θ θ 
的 值 通常 我们 都是用/nr 最大 似 然 估计 但是 
现在 的 问题 是 由于 我们 相用/nr NN 来 模拟 
Pdata x 但是 我们 很难 求解 似 然 函数 因为 
我们 没 办法 写出 生成 模型 的 具体 表达形式 于是 
才 有了 GAN 也 就是 用 判别 模型 来 代替 
求解 最大 似 然 的 过程 在最 理想 的 状态 
下 G 可以 生成 足以 以假乱真 的 图片 G z 
对于 D 来说 它 难以 判定 G 生成 的 图片 
究竟 是不是 真实 的 因此 D G z = 0.5 
这样 我们 的 目的 就 达成 了 我们 得到 了 
一个 生成式 的 模型 G 它 可以 用来 生成 图片 
2 GAN 的 表达式 通过 分析 GAN 的 表达 可以 
看出 本质 上 就是 一个 minmax 问题 其中 V D 
G 可以 看成 是 生成 模型 和 判别 模型 的 
差异 而 minmaxD 说 的 是 最大 的 差异 越小 
越好 这种 度量 差异 的 方式 实际上 叫做 Jensen Shannon 
divergence 3 GAN 的 实际 计算 方法 因为 我们 不 
可能 有 Pdata x 的 分布 所以 我们 实际 中 
都是 用 采样 的 方式 来 计算 差异 也 就是 
积分 变 求和 具体 实现 过程 如下 有 几个 关键 
点 判别 方程 训练 K 次 而 生成 模型 只 
需要 每次 迭代 训练 一次 先 最大化 梯度 上升 再 
最小化 梯度 下降 但是 实际 计算 时V的/nr 后面 一项 在 
D x 很小 的 情况 下 由于 log 函数 的 
原因 会 导致 更新 很慢 所以 实际 中 通常 将 
后 一项 的 log 1 D x 变为 logD x 
实际 计算 的 时候 还 发现 不论 生成器 设计 的 
多好 判别 器 总是 能 判断 出 真假 也 就是 
loss 几乎 都是 0 这 可能 是 因为 抽样 造成 
的 生成 数据 与 真实 数据 的 交集 过小 无论 
生成 模型 多好 判别 模型 也 能 分辨 出来 解决 
方法 有 两个 1 用 WGAN 2 引入 随 时间 
减少 的 噪声 4 对 GAN 有一些 改 进有 引入 
f divergence 取代 Jensen Shannon divergence 还有 很多 这里 主要 
介绍 WGAN5 WGAN 上面 说 过了 用 f divergence 来 
衡量 两个 分布 的 差异 而 WGAN 的 思路 是 
使用 Earth Mover distance 挖掘机 距离 Wasserstein distance 第二 部分 
机器 学习准备 1 决策树 树 相关 问题 1 各种 熵 
的 计算 熵 联合 熵 条件 熵 交叉 熵 KL 
散度 相对 熵 熵 用于 衡量 不确定性 所以 均分 的 
时候 熵 最大 KL 散度 用于 度量 两个 分布 的 
不 相似性 KL p | | q 等于 交叉 熵 
H p q 熵 H p 交叉 熵 可以 看成 
是 用 q 编码 P 所需 的 bit 数 减去 
p 本身 需要 的 bit 数 KL 散度 相当于 用 
q 编码 p 需要 的 额外 bits 交 互信息 Mutual 
information I x y = H x H x | 
y = H y H y | x 表示 观察到 
x 后 y 的 熵 会 减少 多少 2 常用 
的 树 搭建 方法 ID3 C 4.5 CART 上述 几 
种树 分别 利用 信息 增益 信息 增益 率 Gini 指数 
作为 数据 分割 标准 其中 信息 增益 衡量 按照 某个 
特征 分割 前后 熵 的 减少 程度 其实 就是 上面 
说 的 交互 信息 用 上述 信息 增益 会 出现 
优先选择 具有 较多 属性 的 特征 毕竟 分 的 越细的/nr 
属性 确定性 越高 所以 提出 了 信息 增益 率 的 
概念 让 含有 较多 属性 的 特征 的 作用 降低 
CART 树 在 分类 过程 中 使用 的 基尼指数 Gini 
只能 用于 切分 二叉树 而且 和 ID3 C 4.5 树 
不同 Cart 树 不会 在 每一个 步骤 删除 所用 特征 
3 防止 过拟合 剪枝/n 剪枝/n 分为/v 前/f 剪枝/n 和后/nr 剪枝/n 
前 剪枝 本质 就是 早 停止 后/f 剪枝/n 通常/d 是/v 
通过/p 衡量/v 剪枝/n 后/f 损失/n 函数/n 变化/vn 来/v 决定/v 是否/v 
剪枝/n 后 剪枝 有 错误率 降低 剪枝 悲观 剪枝 代价 
复杂度 剪枝 4 前 剪枝 的 几种 停止 条件 节 
点中 样 本为 同 一类 特征 不足 返回 多类 如果 
某 个 分支 没有 值 则 返回 父 节点 中的 
多类 样本 个数 小于 阈值 返回 多类 2 逻辑 回归 
相关 问题 1 公式 推导 一定 要 会 2 逻辑 
回归 的 基本 概念 这个 最好 从 广义 线性 模型 
的 角度 分析 逻辑 回归 是 假设 y 服从 Bernoulli 
分布 3 L1 norm 和 L2 norm 其实 稀疏 的 
根本 还是 在于 L0 norm 也 就是 直接 统计 参数 
不为 0 的 个数 作为 规则 项 但 实际上 却 
不好 执行 于是/nr 引入 了 L1 norm 而 L1norm 本质上 
是 假设 参数 先验 是 服从 Laplace 分布 的 而 
L2 norm 是 假设 参数 先验 为 Gaussian 分布 我们 
在 网上 看到 的 通常 用 图像 来 解答 这个 
问题 的 原理 就在 这 但是 L1 norm 的 求解 
比较 困难 可以 用 坐标轴 下 降法 或是 最小 角回 
归法 求解 4 LR 和 SVM 对比 首先 LR 和 
SVM 最大 的 区别 在于 损失 函数 的 选择 LR 
的 损失 函数 为 Log 损失 或者说 是 逻辑 损失 
都 可以 而 SVM 的 损失 函数 为 hinge loss 
其次 两者 都是 线性 模型 最后 SVM 只 考虑 支持 
向量 也 就是 和 分类 相关 的 少数 点 5 
LR/w 和/c 随机/d 森林/n 区别/n 随机/d 森林/n 等/u 树/v 算法/n 
都/d 是非/v 线性/n 的/uj 而 LR 是 线性 的 LR 
更 侧重 全局 优化 而 树 模型 主要 是 局部 
的 优化 6 常用 的 优化 方法 逻辑 回归 本身 
是 可以 用 公式 求解 的 但是 因为 需要 求 
逆 的 复杂度 太高 所以 才 引入 了 梯度 下降 
算法 一 阶 方法 梯度 下降 随机 梯度 下降 mini 
随机 梯度 下降 降法 随机 梯度 下降 不 但 速度 
上 比 原始 梯度 下降 要快 局部/n 最优化/v 问题/n 时/n 
可以/c 一定/d 程度/n 上/f 抑制/v 局部/n 最优/d 解的/nr 发生/v 二阶 
方法 牛顿 法 拟 牛顿 法 这里/r 详细/ad 说/v 一下/m 
牛顿/nr 法的/nr 基本/n 原理/n 和/c 牛顿/nr 法的/nr 应用/v 方式/n 牛顿 
法 其实 就是 通过 切线 与 x 轴 的 交点 
不断更新 切线 的 位置 直到 达到 曲线 与 x 轴 
的 交点 得到 方程解 在 实际 应用 中 我们 因为 
常常 要 求解 凸 优化 问题 也 就是 要 求解 
函 数一 阶 导数 为 0 的 位置 而 牛顿 
法 恰好 可以 给 这种 问题 提供 解决 方法 实际 
应用 中 牛顿 法 首先 选择 一个 点 作为 起始 
点 并 进行 一次 二阶 泰勒 展开 得到 导数 为 
0 的 点 进行 一个 更新 直到 达到 要求 这时 
牛顿 法 也就 成了 二阶 求解 问题 比 一 阶 
方法 更快 我们 常常 看到 的 x 通常 为 一个 
多 维 向量 这 也就 引出 了 Hessian 矩阵 的 
概念 就是 x 的 二阶 导数 矩阵 缺点 牛顿 法是/nr 
定长 迭代 没有 步长 因子 所以 不能 保证 函数值 稳定 
的 下降 严重 时 甚至 会 失败 还有 就是 牛顿 
法要/nr 求函数 一定是 二阶 可导 的 而且 计算 Hessian 矩阵 
的 逆 复杂度 很大 拟 牛顿 法 不用 二阶 偏 
导 而是 构 造出 Hessian 矩阵 的 近似 正定 对称矩阵 
的 方法 称为 拟 牛顿 法 拟 牛顿 法的/nr 思路 
就是 用 一个 特别 的 表达 形式 来 模拟 Hessian 
矩阵 或者 是 他 的 逆 使得 表达式 满足 拟 
牛顿 条件 主要 有 DFP 法 逼近 Hession 的 逆 
BFGS 直接 逼近 Hession 矩阵 L BFGS 可以 减少 BFGS 
所需 的 存储空间 3 SVM 相关 问题 1 带 核 
的 SVM 为什么 能 分类 非线性 问题 核 函数 的 
本质 是 两个 函数 的 內 积 而 这个 函数 
在 SVM 中 可以 表示 成 对于 输入 值 的 
高维 映射 注意 核 并 不是 直接 对应 映射 核 
只不过 是 一个 內 积 2 RBF 核 一定是 线性 
可分 的 吗 不一定 RBF 核 比较 难调 参 而且 
容易 出现 维度 灾难 要知道 无穷 维 的 概念 是从 
泰勒 展开 得出 的 3 常用 核 函数 及 核 
函数 的 条件 核 函数 选择 的 时候 应该 从 
线性 核 开始 而且在 特征 很多 的 情况 下 没有 
必要 选择 高斯 核 应该 从 简单 到 难 的 
选择 模型 我们 通常 说 的 核 函数 指 的 
是 正定 和 函数 其 充要条件 是 对于 任意 的 
x 属于 X 要求 K 对应 的 Gram 矩阵 要是 
半 正定矩阵 RBF 核 径向 基 这类 函数 取值 依赖 
于特 定点 间 的 距离 所以 拉普拉斯 核 其实 也是 
径向 基 核 线性 核 主要 用于 线性 可分 的 
情况 多项式 核 4 SVM 的 基本 思想 间隔 最大化 
来 得到 最优 分离 超平面 方法 是 将 这个 问题 
形式化 为 一个 凸 二次 规划 问题 还 可以 等 
价位 一个 正则化 的 合页 损失 最小化 问题 SVM 又有 
硬 间隔 最大化 和软/nr 间隔 SVM 两种 这时 首先 要 
考虑 的 是 如何 定义 间隔 这就 引出 了 函数 
间隔 和 几何 间隔 的 概念 这里 只 说 思路 
我们 选择 了 几何 间隔 作为 距离 评定 标准 为什么 
要 这样 怎么 求 出来 的 要知道 我们 希望 能够 
最大化 与 超平面 之间 的 几何 间隔 x 同时 要求 
所 有点 都 大于 这个 值 通过 一些 变化 就 
得到 了 我们 常见 的 SVM 表达式 接着 我们 发现 
定义出 的 x 只是 由 个别 几个 支持 向量 决定 
的 对于 原始 问题 primal problem 而言 可以 利用 凸函数 
的 函数 包来/nr 进行 求解 但是 发现 如果 用 对偶 
问题 dual 求解 会 变得 更 简单 而且 可以 引入 
核 函数 而 原始 问题 转为 对偶 问题 需要 满足 
KKT 条件 这个 条件 应该 细细 思考 一下 到 这里 
还 都是 比较好 求解 的 因为 我们 前面 说过 可以 
变成 软 间隔 问题 引入 了 惩罚 系数 这样 还 
可以 引出 hinge 损失 的 等价 形式 这样 可以 用 
梯度 下降 的 思想 求解 SVM 了 我 个人 认 
为难 的 地方 在于 求解 参数 的 SMO 算法 5 
是否 所有 的 优化 问题 都 可以 转化 为 对偶 
问题 这个 问题 我 感觉 非常 好 有了/i 强/a 对偶/n 
和弱/nr 对偶/n 的/uj 概念/n 用 知乎 大神 的 解释 吧 
6 处理 数据偏斜 可以 对 数量 多 的 类 使得 
惩罚 系数 C 越小 表示 越 不重视 相反 另 数量 
少 的 类 惩罚 系数 变大 4 Boosting 和 Bagging 
1 随机 森林 随机 森林 改变 了 决策树 容易 过拟合 
的 问题 这 主要 是 由 两个 操作 所 优化 
的 1 Boostrap 从袋/nr 内有 放回 的 抽取 样本 值 
2 每次 随机 抽取 一定 数量 的 特征 通常 为 
sqr n 分类 问题 采用 Bagging 投票 的 方式 选择 
类别 频次 最高 的 回归 问题 直 接取 每颗 树 
结果 的 平均值 常见 参数 误差 分析 优点 缺点 1 
树 最大 深度 2 树 的 个数 3 节点 上 
的 最小 样本数 4 特征 数 sqr n oob out 
of bag 将 各个 树 的 未 采样 样本 作为 
预测 样本 统计 误差 作为 误 分率 可以 并行计算 不需要 
特征选择 可以 总结 出 特征 重要性 可以 处理 缺失 数据 
不 需要 额外 设计 测试 集在 回归 上 不能 输出 
连续 结果 2 Boosting 之 AdaBoostBoosting 的 本质 实际上 是 
一个 加法 模型 通过 改变 训练样本 权重 学习 多个 分类器 
并 进行 一些 线性组合 而 Adaboost 就是 加法 模型 + 
指数 损失 函数 + 前项 分布 算法 Adaboost 就是 从弱/nr 
分类器 出发 反复 训练 在其中 不断 调整 数据 权重 或者 
是 概率分布 同时 提高 前 一轮 被 弱 分类器 误 
分 的 样本 的 权值 最后 用 分类器 进行 投票 
表决 但是 分类器 的 重要性 不同 3 Boosting 之 GBDT 
将 基 分类器 变成 二叉树 回归 用 二叉 回归 树 
分类 用 二叉 分类 树 和 上面 的 Adaboost 相比 
回归 树 的 损失 函数 为 平方 损失 同样 可以 
用 指数 损失 函数 定义 分类 问题 但是 对于 一般 
损失 函数 怎么 计算 呢 GBDT 梯度 提升 决策树 是 
为了 解决 一般 损失 函数 的 优化 问题 方法 是 
用 损失 函数 的 负 梯度 在 当前 模型 的 
值 来 模拟 回归 问题 中 残差 的 近似值 注 
由于 GBDT 很容易 出现 过拟合 的 问题 所以 推荐 的 
GBDT 深度 不要 超过 6 而 随机 森林 可以 在 
15 以上 4 GBDT 和 Random Forest 区别 这个 就 
和 上面 说 的 差不多 5 Xgboost 这个 工具 主要 
有 以下 几个 特点 支持 线性 分类器 可以 自定义 损失 
函数 并且 可以 用 二阶 偏 导 加入 了 正则化 
项 叶节/nr 点数 每个 叶 节点 输出 score 的 L2 
norm 支持 特征 抽样 在 一定 情况 下 支持 并行 
只有在 建树 的 阶段 才会 用到 每个 节点 可以 并行 
的 寻找 分裂 特征 5 KNN 和 Kmean 1 KNN 
和/c Kmean/w 缺点/n 都/d 属于/v 惰性/n 学习/v 机制/n 需要 大量 
的 计算 距离 过程 速度慢 的 可以 但是/c 都有/nr 相应/v 
的/uj 优化/vn 方法/n 2 KNNKNN 不 需要 进行 训练 只要 
对于 一个 陌生 的 点 利用 离 其 最近 的 
K 个 点 的 标签 判断 其 结果 KNN 相当于 
多数 表决 也就 等价 于 经验 最小化 而 KNN 的 
优化 方式 就是 用 Kd 树 来 实现 3 Kmean 
要求 自定义 K 个 聚 类 中心 然后 人为 的 
初始化 聚 类 中心 通过 不断 增加 新 点 变换 
中心 位置 得到 最终 结果 Kmean 的 缺点 可以用 Kmean 
+ + 方法 进行 一些 解决 思想 是 使得 初始 
聚 类 中心 之间 的 距离 最大化 6 EM 算法 
HMM CRF 这三个 放在 一起 不是 很 恰当 但是/c 有/v 
互相/d 有/v 关联/ns 所以 就 放在 这里 一起 说 了 
注意 重点 关注 算法 的 思想 1 EM 算法 EM 
算法 是 用于 含有 隐 变量 模型 的 极大 似 
然 估计 或者 极大 后验/nr 估计 有 两步 组成 E 
步 求 期望 expectation M 步 求 极大 maxmization 本质上 
EM 算法 还是 一个 迭代 算法 通过 不断 用 上一代 
参数 对 隐 变量 的 估计 来 对 当前 变量 
进行 计算 直到 收敛 注意 EM 算法 是 对 初值 
敏感 的 而且 EM 是 不断 求解 下界 的 极大化 
逼近 求解 对数 似 然 函数 的 极大化 的 算法 
也 就是说 EM 算法 不能 保证 找到 全局 最优 值 
对于 EM 的 导出 方法 也 应该 掌握 2 HMM 
算法 隐 马尔可夫 模型 是 用于 标注 问题 的 生成 
模型 有几个 参数 π π A B 初始状态 概率 向量 
π π 状态 转移 矩阵 A 观测 概率 矩阵 B 
称为 马尔科夫 模型 的 三要素 马尔科夫 三个 基本问题 概率 计算 
问题 给定 模型 和 观测 序列 计算 模型 下 观测 
序列 输出 的 概率 – 前 向 后向 算法 学习 
问题 已知 观测 序列 估计 模型 参数 即用 极大 似 
然 估计 来 估计 参数 – Baum Welch 也 就是 
EM 算法 和 极大 似 然 估计 预测 问题 已知 
模型 和 观测 序列 求解 对应 的 状态 序列 – 
近似算法 贪心 算法 和维/nr 比特 算法 动态规划 求 最优 路径 
3 条件 随 机场 CRF 给定 一组 输入 随机变量 的 
条件 下 另一组 输出 随机变量 的 条件 概率分布 密度 条件 
随 机场 假设 输出 变量 构成 马尔科夫 随 机场 而 
我们 平时 看到 的 大多 是 线性 链条 随 机场 
也 就是 由 输入 对 输出 进行 预测 的 判别 
模型 求解 方法 为 极大 似 然 估计 或 正则化 
的 极大 似 然 估计 之所以 总把 HMM 和 CRF 
进行 比较 主要/b 是/v 因为/c CRF/w 和/c HMM/w 都/d 利用/n 
了/ul 图/n 的/uj 知识/v 但是 CRF 利用 的 是 马尔科夫 
随 机场 无向图 而 HMM 的 基础 是 贝叶斯 网络 
有向图 而且 CRF 也有 概率 计算 问题 学习 问题 和 
预测 问题 大致 计算 方法 和 HMM 类似 只不过 不 
需要 EM 算法 进行 学习 问题 4 HMM 和 CRF 
对比 其 根本 还是 在于 基本 的 理念 不同 一个 
是 生成 模型 一个 是 判别 模型 这也 就 导致 
了 求解 方式 的 不同 7 常见 基础问题 1 数据 
归一化 或者 标准化 注意 归一化 和 标准化 不同 的 原因 
要 强调 能不 归一化 最好 不 归一化 之所以 进行 数据 
归一化 是 因为 各 维度 的 量纲 不相同 而且 需要 
看 情况 进行 归一化 有些 模型 在各 维度 进行 了 
不 均匀 的 伸缩 后 最优 解与/nr 原来 不等价 如 
SVM 需要 归一化 有些 模型 伸缩 有与/nr 原来 等价 如 
LR 则 不用 归一化 但是 实际 中 往往 通过 迭代 
求解 模型 参数 如果 目标函数 太 扁 想象 一下 很扁 
的 高斯 模型 迭代 算法 会 发生 不 收敛 的 
情况 所以 最坏 进行 数据 归一化 补充 其实 本质 是 
由于 loss 函数 不同 造成 的 SVM 用了 欧拉 距离 
如果 一个 特征 很大 就会 把 其他 的 维度 dominated 
而 LR 可以 通过 权重 调整 使得 损失 函数 不变 
2 衡量 分类器 的 好坏 这里 首先 要 知道 TP 
FN 真的 判成 假 的 FP 假 的 判 成真 
TN 四种 可以 画 一个 表格 几种 常用 的 指标 
精度 precision = TP / TP + FP = TP 
/ ~ P ~ p 为 预测 为真 的 数量 
召回率 recall = TP / TP + FN = TP 
/ PF1 值 2 / F1 = 1 / recall 
+ 1 / precisionROC 曲线 ROC 空间 是 一个 以 
伪 阳性率 FPR false positive rate 为 X 轴 真阳性率 
TPR true positive rate 为 Y 轴 的 二维 坐标系 
所 代表 的 平面 其中 真 阳率/nr TPR = TP 
/ P = recall 伪 阳率/nr FPR = FP / 
N 3 SVD 和 PCAPCA 的 理念 是 使得 数据 
投 影后 的 方差 最大 找到 这样 一个 投影 向量 
满足 方差 最大 的 条件 即可 而 经过 了 去除 
均值 的 操作 之后 就 可以 用 SVD 分解 来 
求解 这样 一个 投影 向量 选择 特征值 最大 的 方向 
4 防止 过拟合 的 方法 过拟合 的 原因 是 算法 
的 学习 能力 过强 一些 假设 条件 如 样本 独立 
同 分布 可能 是 不 成立 的 训练样本 过少 不能 
对 整个 空间 进行 分布 估计 处理 方法 早 停止 
如在 训练 中 多次 迭代 后 发现 模型 性能 没有 
显著 提高 就 停止 训练 数据集 扩增 原有 数据 增加 
原有 数据 加 随机噪声 重 采样 正则化 交叉 验证 特征选择 
/ 特征 降 维 5 数据 不 平衡 问题 这 
主要 是 由于 数据分布 不 平衡 造成 的 解决 方法 
如下 采样 对 小 样本 加 噪声 采样 对 大样本 
进行 下 采样 进行 特殊 的 加权 如在 Adaboost 中 
或者 SVM 中 采用 对 不平衡 数据集 不 敏感 的 
算法 改变 评价 标准 用 AUC / ROC 来 进行 
评价 采用 Bagging / Boosting / ensemble 等 方法 考虑 
数据 的 先验 分布 