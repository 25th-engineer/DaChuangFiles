一 引言 本 材料 参考 Andrew Ng 大神 的 机器学习 
课程   http / / cs229 . stanford . edu 
以及 斯坦福 无 监督 学习 UFLDL tutorial   http / 
/ ufldl . stanford . edu / wiki / index 
. php / UFLDL _ Tutorial 机器学习 中的 回归 问题 
属于 有 监督 学习 的 范畴 回归 问题 的 目标 
是 给定 D 维 输入 变量 x 并且 每 一个 
输入 矢量 x 都有 对应 的 值 y 要求 对于 
新来 的 数据 预测 它 对应 的 连续 的 目标 
值 t 比如 下面 这个 例子 假设/vn 我们/r 有/v 一个/m 
包含/v 47个/mq 房子/n 的/uj 面积/n 和/c 价格/n 的/uj 数据/n 集/q 
如下/t 我们 可以 在 Matlab 中 画出来 这组 数据集 如下 
看到 画 出来 的 点 是不是 有点 像 一条 直线 
我们 可以 用 一条 曲线 去 尽量 拟合 这些 数据 
点 那么 对于 新来 的 输入 我 么 就 可以 
将 拟合 的 曲线 上 返回 对应 的 点 从而 
达到 预测 的 目的 如果 要 预测 的 值 是 
连续 的 比如 上述 的 房价 那么 就 属于 回归 
问题 如果 要 预测 的 值 是 离散 的 即 
一个 个 标签 那么 就 属于 分类 问题 这个 学习 
处理过程 如下 图 所示 上述 学习 过程 中 的 常用 
术语 包含 房子 面积 和 价格 的 数据 集 称为 
训练 集 training set 输入 变量 x 本例 中为 面积 
为特征 features 输出 的 预测 值 y 本例 中为 房价 
为 目标值 target 拟合 的 曲线 一般 表示 为 y 
= h x 称为 假设 模型 hypothesis 训练 集 的 
条目 数 称为 特征 的 维数 本例 为 47 二 
线性 回归模型 线性 回归模型 假设 输入 特征 和 对应 的 
结果 满足 线性关系 在 上述 的 数据 集中 加上 一维 
房间 数量 于是 数据集 变为 于是 输入 特征 x 是 
二维 的 矢量 比如 x1 i 表示 数据 集中 第 
i 个 房子 的 面积 x2 i 表示 数据 集中 
第 i 个 房子 的 房间 数量 于是/nr 可以 假设 
输入 特征 x 与 房价 y 满足 线性函数 比如 这里 
θ i 称为 假设 模型 即 映射 输入 特征 x 
与 结果 y 的 线性函数 h 的 参数 parameters 为了 
简化 表示 我们 在 输入 特征 中 加入 x0 = 
1 于是 得到 参数 θ 和 输入 特征 x 都为 
矢量 n 是 输入 的 特征 x 的 个数 不 
包含 x0 现在 给定 一个 训练 集 我们 应该 怎么 
学习 参数 θ 从而 达到 比较 好 的 拟合 效果 
呢 一个 直观 的 想法 是 使得 预测值 h x 
尽可能 接近 y 为了 达到 这个 目的 我们 对于 每一个 
参数 θ 定义 一个 代价 函数 cost function 用来 描述 
h x i 与 对应 的 y i 的 接近 
程度 前面 乘上 的 1/2 是 为了 求导 的 时候 
使 常数 系数 消失 于是 我们 的 目标 就 变为 
了 调整 θ 使得 代价 函数 J θ 取得 最小值 
方法 有 梯度 下 降法 最小二乘 法等/nr 2.1 梯度 下 
降法 现在 我们 要 调整 θ 使得 J θ 取得 
最小值 为了 达到 这个 目的 我们 可以 对 θ 取 
一个 随机 初始值 随机 初始化 的 目的 是 使 对称 
失效 然后 不断 地 迭代 改变 θ 的 值 来使 
J θ 减小 知道 最终 收敛 取得 一个 θ 值 
使得 J θ 最小 梯度 下 降法 就 采用 这样 
的 思想 对 θ 设定 一个 随机 初值 θ 0 
然后 迭代 进行 以下 更新 直到 收敛 这里 的 α 
称为 学习率 learning rate 梯度方向 由 J θ 对 θ 
的 偏 导数 决定 由于 要求 的 是 最小值 因此 
对 偏 导数 取 负值 得到 梯度方向 将 J θ 
代入 得到 总的 更新 公式 这样 的 更新 规则 称为 
LMS update rule least mean squares 也 称为 Widrow Ho 
ﬀ learning rule 对于 如下 更新 参数 的 算法 由于 
在 每一次 迭代 都 考察 训练 集 的 所有 样本 
而 称为 批量 梯度 下降 batch gradient descent 对于 引言 
中的 房价 数据集 运行 这种 算法 可以 得到 θ 0 
= 71.27 θ 1 = 1.1345 拟合 曲线 如 下图 
如果 参数 更新 计算 算法 如下 这里 我们 按照 单个 
训练样本 更新 θ 的 值 称为 随机 梯度 下降 stochastic 
gradient descent 比较 这 两种 梯度 下降 算法 由于 batch 
gradient descent 在 每一步 都 考虑 全部 数据集 因而 复杂度 
比较 高 随机 梯度 下降 会 比较 快 地 收敛 
而且 在 实际 情况 中 两种 梯度 下降 得到 的 
最优 解J/nr θ 一般 会 接近 真实 的 最小值 所以 
对于 较大 的 数据 集 一般 采用 效率 较高 的 
随机 梯度 下 降法 2.2 最小二乘 法 LMS 梯度 下降 
算法 给出 了 一种 计算 θ 的 方法 但是 需要 
迭代 的 过程 比较 费时 而且 不太 直观 下面 介绍 
的 最小二乘 法是/nr 一种 直观 的 直接 利用 矩阵 运算 
可以 得到 θ 值 的 算法 为了 理解 最小二乘 法 
首先 回顾 一下 矩阵 的 有关 运算 假设 函数 f 
是 将 m * n 维 矩阵 映射 为 一个 
实数 的 运算 即 并且 定义 对于 矩阵 A 映射 
f A 对 A 的 梯度 为 因此 该 梯度 
为 m * n 的 矩阵 例如 对于 矩阵 A 
= 而且 映射函数 f A 定义 为 F A = 
1 . 5A11 + 5A122 + A21A22 于是 梯度 为 
另外 对于 矩阵的迹 的 梯度 运算 有 如下 规则 下面 
我们/r 将/d 测试/vn 集中/v 的/uj 输入/v 特征/n x/w 和/c 对应/vn 
的/uj 结果/n y/w 表示/v 成/n 矩阵/n 或者/c 向量/n 的/uj 形式/n 
有 对于 预测模型 有 即 于是/nr 可以 很容易 得到 所以 
可以 得到 于是 我们 就 将 代价 函数 J θ 
表示 为了 矩阵 的 形式 就 可以 用 上述 提到 
的 矩阵 运算 来 得到 梯度 令 上述 梯度 为 
0 得到 等式 于是 得到 θ 的 值 这 就是 
最 小二 乘法 得到 的 假设 模型 中 参数 的 
值 2.3 加权 线性 回归 首先 考虑 下 图中 的 
几种 曲线拟合 情况 最 左边 的 图 使用 线性 拟合 
但是 可以 看到 数 据点 并不 完全 在 一条 直线 
上 因而 拟合 的 效果 并 不好 如果 我们 加入 
x 2项 得到 如 中间 图 所示 该 二次曲线 可以 
更好 的 拟 合数 据点 我们 继续 加入 更 高次 
项 可以 得到 最 右边 图 所示 的 拟合 曲线 
可以 完美地 拟 合数 据点 最 右边 的 图中 曲线 
为 5 阶 多项式 可是 我们 都很/nr 清醒 地 知道 
这个 曲线 过于 完美 了 对于 新来 的 数据 可能 
预测 效果 并 不会 那么 好 对于 最 左边 的 
曲线 我们 称之为 欠 拟合 过小 的 特征 集合 使得 
模型 过于 简单 不能 很好 地 表达 数据 的 结构 
最 右边 的 曲线 我们 称之为 过拟合 过大 的 特征 
集合 使得 模型 过于 复杂 正如 上述 例子 表明 在 
学习 过程 中 特征 的 选择 对于 最终 学习 到 
的 模型 的 性能 有 很大 影响 于是 选择 用 
哪个 特征 每个 特征 的 重要性 如何 就 产生 了 
加权 的 线性 回归 在 传统 的 线性 回 归中 
学习 过程 如下 而 加权 线性 回归 学习 过程 如下 
二者 的 区别 就 在于 对 不同 的 输入 特征 
赋予 了 不同 的 非负值 权重 权重 越大 对于 代价 
函数 的 影响 越大 一般 选取 的 权重 计算公式 为 
其中 x 是 要 预测 的 特征 表示 离 x 
越近的/nr 样本 权重 越大 越远 的 影响 越小 三 logistic 
回归 与 Softmax 回归 3.1   logistic 回归 下面 介绍 
一下 logistic 回归 虽然 名曰 回归 但 实际上 logistic 回归 
用于 分类 问题 logistic 回归 实质上 还是 线性 回归模型 只是 
在 回归 的 连续 值 结果 上 加了 一层 函数 
映射 将 特征 线性 求和 然后 使用 g z 作 
映射 将 连续 值 映 射到 离散 值 0/1 上 
对于 sigmoid 函数 为 0/1 两类 而 对于 双曲正弦 tanh 
函数 为 1 / 1 两类 采用 假设 模型 为 
而 sigmoid 函数 g z 为 当 z 趋 近于 
∞ g z 趋 近于 0 而 z 趋 近于 
∞ g z 趋 近于 1 从而 达到 分类 的 
目的 这里 的 那么 对于 这样 的 logistic 模型 怎么 
调整 参数 θ 呢 我们 假设 由于 是 两类 问题 
即 于是 得到 似 然 估计 为 对 似 然 
估 计取 对数 可以 更容易 地 求解 接下来 是 θ 
的 似 然 估计 最大化 可以 考虑 上述 的 梯度 
下 降法 于是 得到 得到 类似 的 更新 公式 虽然 
这个 更 新规则 类似于 LMS 得到 的 公式 但是 这 
两种 是 不同 算法 因为 这里 的 h θ x 
i 是 一个 关于 θ Tx i 的 非线性 函数 
3.2   Softmax 回归 logistic 回归 是 两类 回归 问题 
的 算法 如果 目标 结果 是 多个 离散 值 怎么办 
Softmax 回归模型 就是 解决 这个 问题 的 Softmax 回归模型 是 
logistic 模型 在 多分 类 问题 上 的 推广 在 
Softmax 回 归中 类 标签 y 可以 去 k 个 
不同 的 值 k 2 因此 对于 y i 从属于 
{ 1 2 3 k } 对于 给定 的 测试 
输入 x 我们 要 利用 假设 模型 针对 每 一个 
类别 j 估算 概率值 p y = j | x 
于是 假设 函数 h θ x i 形式 为 其中 
θ 1 θ 2 θ 3 θ k 属于 模型 
的 参数 等式 右边 的 系数 是 对 概率分布 进行 
归一化 使得 总 概率 之和 为 1 于是 类似于 logistic 
回归 推广 得到 新的 代价 函数 为 可以 看到 Softmax 
代价 函数 与 logistic 代价 函数 形式 上 非常 相似 
只是 Softmax 函数 将 k 个 可能 的 类别 进行 
了 累加 在 Softmax 中将 x 分为 类别 j 的 
概率 为 于是 对于 Softmax 的 代价 函数 利用 梯度 
下 降法 使 的 J θ 最小 梯度 公式 如下 
表示 J θ 对 第 j 个 元素 θ j 
的 偏 导数 每一次 迭代 进行 更新 3.3 Softmax 回归 
vs logistic 回归 特别地 当 Softmax 回 归中 k = 
2时 Softmax 就 退 化为 logistic 回归 当 k = 
2时 Softmax 回归 的 假设 模型 为 我们 令 ψ 
= θ 1 并且 两个 参数 都 剪去 θ 1 
得到 于是 Softmax 回归 预测 得到 两个 类别 的 概率 
形式 与 logistic 回归 一致 现在 如果 有 一个 k 
类 分类 的 任务 我们 可以 选择 Softmax 回归 也 
可以 选择 k 个 独立 的 logistic 回归 分类器 应该 
如何 选择 呢 这一 选择 取决于 这 k 个 类别 
是否 互斥 例如 如果 有 四个 类别 的 电影 分别为 
好莱坞 电影 港台 电影 日韩 电影 大陆 电影 需要 对 
每一个 训练 的 电影 样本 打上 一个 标签 那么 此时 
应 选择 k = 4 的 Softmax 回归 然而 如果 
四个 电影 类别 如下 动作 喜剧 爱情 欧美 这些 类别 
并 不是 互斥 的 于是 这种 情况 下 使用 4个 
logistic 回归 分类器 比较 合理 四 一般 线性 回归模型 首先 
定义 一个 通用 的 指数 概率分布 考虑 伯努利 分布 有 
再考虑 高斯分布 一般 线性 模型 满足 1 . y | 
x θ 满足 指数分布 族 E η 2 . 给定 
特征 x 预测 结果 为 T y = E y 
| x 3 . 参数 η = θ Tx 对于 
第二 部分 的 线性 模型 我们 假设 结果 y 满足 
高斯分布 Ν μ σ 2 于是 期望 μ =   
η 所以 很显然 从 一般 线性 模型 的 角度 得到 
了 第二 部分 的 假设 模型 对于 logistic 模型 由于 
假设 结果 分为 两类 很 自然 地 想到 伯努利 分布 
并且 可以 得到 于是 y | x θ 满足 B 
Φ E y | x θ =   Φ 所以 
于是/nr 得到 了 与 logistic 假设 模型 的 公式 这也 
解释 了 logistic 回归 为何 使用 这个 函数 