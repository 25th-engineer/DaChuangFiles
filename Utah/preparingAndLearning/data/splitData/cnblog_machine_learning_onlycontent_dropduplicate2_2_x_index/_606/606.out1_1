本文 转 自 lytforgood 机器学习 总结 sklearn 参数 解释 实验 
数据集 选取 1 分类 数据 选取 load _ iris 鸢尾花 
数据集 from sklearn . datasets import load _ iris data 
= load _ iris data . data 10 25 50 
data . target 10 25 50 list data . target 
_ names list data . feature _ names 2 回归 
数据 选取 from sklearn . datasets import load _ boston 
boston = load _ boston print boston . data . 
shape boston . feature _ names 数据集 切 分为 训练 
集 验证 集 GBDT 系数 说明 参考 G r a 
d i e n t B o o s t 
i n g C l a s s i f 
i e r 支持 二进制 和 多类 分类 from sklearn 
. datasets import make _ hastie _ 10 _ 2 
from sklearn . ensemble import G r a d i 
e n t B o o s t i n 
g C l a s s i f i e 
r X y = make _ hastie _ 10 _ 
2 random _ state = 0 X _ train X 
_ test = X 2000 X 2000 y _ train 
y _ test = y 2000 y 2000 clf = 
G r a d i e n t B o 
o s t i n g C l a s 
s i f i e r loss = deviance # 
# 损失 函数 默认 deviance deviance 具有 概率 输出 的 
分类 的 偏差 n _ estimators = 100 # # 
默认 100 回归 树 个数 弱 学习 器 个数 learning 
_ rate = 0.1 # # 默认 0.1 学习 速率 
/ 步长 0.0 1.0 的 超 参数 每个 树 学习 
前 一个 树 的 残差 的 步长 max _ depth 
= 3 # # 默认值 为 3 每个 回归 树 
的 深度 控制 树 的 大小 也 可用 叶 节点 
的 数量 max leaf nodes 控制 subsample = 1 # 
# 树 生成 时对/nr 样本 采样 选择 子样本 1.0 导致 
方差 的 减少 和 偏差 的 增加 min _ samples 
_ split = 2 # # 生成 子 节点 所需 
的 最小 样本数 如果 是 浮点数 代表 是 百分比 min 
_ samples _ leaf = 1 # # 叶 节点 
所需 的 最小 样本数 如果 是 浮点数 代表 是 百分比 
max _ features = None # # 在 寻找 最佳 
分割 点 要 考虑 的 特征 数量 auto 全选 / 
sqrt 开方 / log2 对数 / None 全选 / int 
自定义 几个 / float 百分比 max _ leaf _ nodes 
= None # # 叶 节点 的 数量 None 不限 
数量 min _ impurity _ split = 1e 7 # 
# 停止 分裂 叶子 节点 的 阈值 verbose = 0 
# # 打印输出 大于 1 打印 每 棵树 的 进度 
和 性能 warm _ start = False # # True 
在前面 基础 上 增量 训练 重设 参数 减少 训练 次数 
False 默认 擦除 重新 训练 random _ state = 0 
# # 随机 种子 方便 重现 . fit X _ 
train y _ train # # 多 类别 回归 建议 
使用 随机 森林 print clf . score X _ test 
y _ test # # tp / tp + fp 
正 实例 占 所有 正 实例 的 比例 test _ 
y = clf . predict X _ test test _ 
y = clf . predict _ proba X _ test 
1 # # 预测 概率 print clf . feature _ 
importances _ # # 输出 特征 重要性 print clf . 
train _ score _ # # 每次 迭代 后 分数 
# # test _ y = clf . predict X 
_ test # # from sklearn . metrics import precision 
_ score # # precision _ score test _ y 
y _ test average = micro # # tp / 
tp + fp # # from sklearn import metrics # 
# fpr tpr thresholds = metrics . roc _ curve 
y _ test test _ y # # print auc 
% . 4g % metrics . auc fpr tpr y 
_ pre = clf . predict X _ test y 
_ pro = clf . predict _ proba X _ 
test 1 # # 预测 概率 from sklearn import metrics 
fpr tpr thresholds = metrics . roc _ curve y 
_ test y _ pro print auc % . 4g 
% metrics . auc fpr tpr x % 10000/100 x 
% 100 # auc 表示 一 print AUC Score Train 
% f % metrics . roc _ auc _ score 
y _ test y _ pro # auc 表示 二 
两种 方式 等价 print Accuracy % . 4g % metrics 
. accuracy _ score y _ test y _ pre 
# # 等价 于 clf . score X _ test 
y _ test sklearn . ensemble . G r a 
d i e n t B o o s t 
i n g R e g r e s s 
o r i m p o r t numpy as 
np from sklearn . metrics import mean _ squared _ 
error from sklearn . datasets import make _ friedman1 from 
sklearn . ensemble import G r a d i e 
n t B o o s t i n g 
R e g r e s s o r X 
y = make _ friedman1 n _ samples = 1200 
random _ state = 0 noise = 1.0 X _ 
train X _ test = X 200 X 200 y 
_ train y _ test = y 200 y 200 
est = G r a d i e n t 
B o o s t i n g R e 
g r e s s o r loss = ls 
# # 默认 ls 损失 函数 ls 是 指 最小二乘 
回归 lad 最小 绝对 偏差 huber 是 两者 的 组合 
n _ estimators = 100 # # 默认 100 回归 
树 个数 弱 学习 器 个数 learning _ rate = 
0.1 # # 默认 0.1 学习 速率 / 步长 0.0 
1.0 的 超 参数 每个 树 学习 前 一个 树 
的 残差 的 步长 max _ depth = 3 # 
# 默认值 为 3 每个 回归 树 的 深度 控制 
树 的 大小 也 可用 叶 节点 的 数量 max 
leaf nodes 控制 subsample = 1 # # 用于 拟合 
个别 基础 学习 器 的 样本 分数 选择 子样本 1.0 
导致 方差 的 减少 和 偏差 的 增加 min _ 
samples _ split = 2 # # 生成 子 节点 
所需 的 最小 样本数 如果 是 浮点数 代表 是 百分比 
min _ samples _ leaf = 1 # # 叶 
节点 所需 的 最小 样本数 如果 是 浮点数 代表 是 
百分比 max _ features = None # # 在 寻找 
最佳 分割 点 要 考虑 的 特征 数量 auto 全选 
/ sqrt 开方 / log2 对数 / None 全选 / 
int 自定义 几个 / float 百分比 max _ leaf _ 
nodes = None # # 叶 节点 的 数量 None 
不限 数量 min _ impurity _ split = 1e 7 
# # 停止 分裂 叶子 节点 的 阈值 verbose = 
0 # # 打印输出 大于 1 打印 每 棵树 的 
进度 和 性能 warm _ start = False # # 
True 在前面 基础 上 增量 训练 False 默认 擦除 重新 
训练 增加 树 random _ state = 0 # # 
随机 种子 方便 重现 . fit X _ train y 
_ train mean _ squared _ error y _ test 
est . predict X _ test import numpy as np 
from sklearn import ensemble from sklearn import datasets from sklearn 
. utils import shuffle from sklearn . metrics import mean 
_ squared _ error from sklearn . metrics import r2 
_ score boston = datasets . load _ boston X 
y = shuffle boston . data boston . target random 
_ state = 13 # 抽取 X = X . 
astype np . float32 offset = int X . shape 
0 * 0.9 # 设置 取 0.9 做 样本 X 
_ train y _ train = X offset y offset 
X _ test y _ test = X offset y 
offset # # 参数 可以 放入 一个 字典 当中 params 
= { n _ estimators 500 max _ depth 4 
min _ samples _ split 2 learning _ rate 0.01 
loss ls } clf = ensemble . G r a 
d i e n t B o o s t 
i n g R e g r e s s 
o r * * params clf . fit X _ 
train y _ train mse = mean _ squared _ 
error y _ test clf . predict X _ test 
r2 = r2 _ score y _ test clf . 
predict X _ test print MSE % . 4f % 
mse # # 输出 均方 误差 print r ^ 2 
on test data % f % r2 # # R 
^ 2 拟合 优 度 = 预测值 均值 ^ 2 
之和 / 真 实值 均值 ^ 2 之和 # # 
绘图 查看 import matplotlib . pyplot as plt test _ 
score = np . zeros params n _ estimators dtype 
= np . float64 # # 计算 每次 迭代 分数 
变化 for i y _ pred in enumerate clf . 
staged _ predict X _ test test _ score i 
= clf . loss _ y _ test y _ 
pred plt . figure figsize = 12 6 plt . 
subplot 1 2 1 plt . title Deviance plt . 
plot np . arange params n _ estimators + 1 
clf . train _ score _ b label = Training 
Set Deviance plt . plot np . arange params n 
_ estimators + 1 test _ score r label = 
Test Set Deviance plt . legend loc = upper right 
plt . xlabel Boosting Iterations plt . ylabel Deviance # 
# 输出 特征 重要性 feature _ importance = clf . 
feature _ importances _ # make importances relative to max 
importance feature _ importance = 100.0 * feature _ importance 
/ feature _ importance . max sorted _ idx = 
np . argsort feature _ importance # # 返回 的 
是 数组 值 从小到大 的 索引 值 pos = np 
. arange sorted _ idx . shape 0 + . 
5 plt . subplot 1 2 2 plt . barh 
pos feature _ importance sorted _ idx align = center 
plt . yticks pos boston . feature _ names sorted 
_ idx plt . xlabel Relative Importance plt . title 
Variable Importance plt . show 网格 搜索 调整 超 参数 
from sklearn . model _ selection import GridSearchCV clf = 
GridSearchCV estimator # # 模型 param _ grid # # 
参数 字典 或者 字典 列表 scoring = None # # 
评价 分数 的 方法 fit _ params = None # 
# fit 的 参数 字典 n _ jobs = 1 
# # 并 行数 1 全部 启动 iid = True 
# # 每个 cv 集上 等价 refit = True # 
# 使用 整个 数据集 重新 编制 最佳 估计量 cv = 
None # # 几 折 交叉 验证 None 默认 3 
verbose = 0 # # 控制 详细 程度 越高 消息 
越多 pre _ dispatch = 2 * n _ jobs 
# # 总 作业 的 确切 数量 error _ score 
= raise # # 错误 时 选择 的 分数 return 
_ train _ score = True # # 如果 False 
该 cv _ results _ 属性 将 不包括 训练 得分 
clf . cv _ results _ # # 结果 表 
常看 mean _ test _ score std _ test _ 
score clf . cv _ results _ . keys # 
# clf . cv _ results _ mean _ test 
_ score clf . best _ estimator _ # # 
最优 模型 clf . best _ score _ # # 
最优 分数 clf . best _ params _ # # 
最优 参数 from sklearn . model _ selection import train 
_ test _ split from sklearn . model _ selection 
import GridSearchCV from sklearn . metrics import classification _ report 
from sklearn import metrics from sklearn . datasets import make 
_ hastie _ 10 _ 2 from sklearn . ensemble 
import G r a d i e n t B 
o o s t i n g C l a 
s s i f i e r X y = 
make _ hastie _ 10 _ 2 random _ state 
= 0 X _ train X _ test y _ 
train y _ test = train _ test _ split 
X y test _ size = 0.5 random _ state 
= 0 # # test _ size 测试 集合 所占 
比例 # # 设置 参数 tuned _ parameters = { 
n _ estimators range 20 81 10 max _ depth 
range 3 14 2 learning _ rate 0.1 0.5 1.0 
subsample 0.6 0.7 0.75 0.8 0.85 0.9 } # # 
设置 分数 计算方法 精度 / 召回 scores = precision recall 
# # roc _ auc for score in scores print 
评测 选择 % s % score clf = GridSearchCV G 
r a d i e n t B o o 
s t i n g C l a s s 
i f i e r tuned _ parameters cv = 
5 scoring = % s _ macro % score clf 
. fit X _ train y _ train print clf 
. best _ params _ means = clf . cv 
_ results _ mean _ test _ score # # 
tp / tp + fp stds = clf . cv 
_ results _ std _ test _ score for mean 
std params in zip means stds clf . cv _ 
results _ params print % 0.3 f + / % 
0.03 f for % r % mean std * 2 
params # # 预测 y _ true y _ pred 
= y _ test clf . predict X _ test 
# # y _ true y _ pred = y 
_ test clf . predict _ proba X _ test 
print classification _ report y _ true y _ pred 
# # print Accuracy % . 4g % metrics . 
accuracy _ score y _ true y _ pred XGBoostxgb 
原始 from sklearn . model _ selection import train _ 
test _ split from sklearn import metrics from sklearn . 
datasets import make _ hastie _ 10 _ 2 import 
xgboost as xgb # 记录 程序 运行时间 import time start 
_ time = time . time X y = make 
_ hastie _ 10 _ 2 random _ state = 
0 X _ train X _ test y _ train 
y _ test = train _ test _ split X 
y test _ size = 0.5 random _ state = 
0 # # test _ size 测试 集合 所占 比例 
# xgb 矩阵 赋值 xgb _ train = xgb . 
DMatrix X _ train label = y _ train xgb 
_ test = xgb . DMatrix X _ test label 
= y _ test # # 参数 params = { 
booster gbtree silent 1 # 设置成 1 则 没有 运行 
信息 输出 最好 是 设置 为 0 . # nthread 
7 # cpu 线程数 默认 最大 eta 0.007 # 如同 
学习率 min _ child _ weight 3 # 这个 参数 
默认 是 1 是 每个 叶子 里面 h 的 和 
至少 是 多少 对 正负 样本 不均衡 时的/nr 0 1 
分类 而言 # 假设 h 在 0.01 附近 min _ 
child _ weight 为 1 意味着 叶子 节 点中 最少 
需要 包含 100 个 样本 # 这个 参数 非常 影响 
结果 控制 叶子 节 点中 二阶 导 的 和的/nr 最小值 
该 参数值 越小 越 容易 overfitting max _ depth 6 
# 构 建树 的 深度 越大 越 容易 过拟合 gamma 
0.1 # 树 的 叶子 节点 上 作 进一步 分区 
所需 的 最小 损失 减少 越大 越 保守 一般 0.1 
0.2 这样子 subsample 0.7 # 随机 采样 训练样本 colsample _ 
bytree 0.7 # 生成树 时 进行 的 列 采样 lambda 
2 # 控制 模型 复杂度 的 权重 值 的 L2 
正则化 项 参数 参数 越大 模型 越 不容易 过拟合 # 
alpha 0 # L1 正则 项 参数 # scale _ 
pos _ weight 1 # 如果 取值 大于 0 的话 
在 类别 样本 不 平衡 的 情况 下 有助于 快速 
收敛 # objective multi softmax # 多 分类 的 问题 
# num _ class 10 # 类 别数 多 分类 
与 multisoftmax 并用 seed 1000 # 随机 种子 # eval 
_ metric auc } plst = list params . items 
num _ rounds = 100 # 迭代 次数 watchlist = 
xgb _ train train xgb _ test val # 训练 
模型 并 保存 # early _ stopping _ rounds 当 
设置 的 迭代 次数 较大 时 early _ stopping _ 
rounds 可 在 一定 的 迭代 次数 内 准确率 没有 
提升 就 停止 训练 model = xgb . train plst 
xgb _ train num _ rounds watchlist early _ stopping 
_ rounds = 100 pred _ margin = 1 # 
model . save _ model . / model / xgb 
. model # 用于 存 储训 练出 的 模型 print 
best best _ ntree _ limit model . best _ 
ntree _ limit y _ pred = model . predict 
xgb _ test ntree _ limit = model . best 
_ ntree _ limit print error = % f % 
sum 1 for i in range len y _ pred 
if int y _ pred i 0.5 = y _ 
test i / float len y _ pred # 输出 
运行 时长 cost _ time = time . time start 
_ time print xgboost success \ n cost time cost 
_ time s . . . . . . xgb 
使用 sklearn 接口 推荐 官方 会 改变 的 函数 名 
是 eta learning _ ratelambda reg _ lambdaalpha reg _ 
alphafrom sklearn . model _ selection import train _ test 
_ split from sklearn import metrics from sklearn . datasets 
import make _ hastie _ 10 _ 2 from xgboost 
. sklearn import XGBClassifier X y = make _ hastie 
_ 10 _ 2 random _ state = 0 X 
_ train X _ test y _ train y _ 
test = train _ test _ split X y test 
_ size = 0.5 random _ state = 0 # 
# test _ size 测试 集合 所占 比例 clf = 
XGBClassifier silent = 0 # 设置成 1 则 没有 运行 
信息 输出 最好 是 设置 为 0 . 是否 在 
运行 升级 时 打印消息 # nthread = 4 # cpu 
线程数 默认 最大 learning _ rate = 0.3 # 如同 
学习率 min _ child _ weight = 1 # 这个 
参数 默认 是 1 是 每个 叶子 里面 h 的 
和 至少 是 多少 对 正负 样本 不均衡 时的/nr 0 
1 分类 而言 # 假设 h 在 0.01 附近 min 
_ child _ weight 为 1 意味着 叶子 节 点中 
最少 需要 包含 100 个 样本 # 这个 参数 非常 
影响 结果 控制 叶子 节 点中 二阶 导 的 和的/nr 
最小值 该 参数值 越小 越 容易 overfitting max _ depth 
= 6 # 构 建树 的 深度 越大 越 容易 
过拟合 gamma = 0 # 树 的 叶子 节点 上 
作 进一步 分区 所需 的 最小 损失 减少 越大 越 
保守 一般 0.1 0.2 这样子 subsample = 1 # 随机 
采样 训练样本 训练 实例 的 子 采样 比 max _ 
delta _ step = 0 # 最大 增量 步长 我们 
允许 每个 树 的 权重 估计 colsample _ bytree = 
1 # 生成树 时 进行 的 列 采样 reg _ 
lambda = 1 # 控制 模型 复杂度 的 权重 值 
的 L2 正则化 项 参数 参数 越大 模型 越 不容易 
过拟合 # reg _ alpha = 0 # L1 正则 
项 参数 # scale _ pos _ weight = 1 
# 如果 取值 大于 0 的话 在 类别 样本 不 
平衡 的 情况 下 有助于 快速 收敛 平衡 正负 权重 
# objective = multi softmax # 多 分类 的 问题 
指定 学习 任务 和 相应 的 学习 目标 # num 
_ class = 10 # 类 别数 多 分类 与 
multisoftmax 并用 n _ estimators = 100 # 树 的 
个数 seed = 1000 # 随机 种子 # eval _ 
metric = auc clf . fit X _ train y 
_ train eval _ metric = auc y _ true 
y _ pred = y _ test clf . predict 
X _ test print Accuracy % . 4g % metrics 
. accuracy _ score y _ true y _ pred 
# 回归 # m _ regress = xgb . XGBRegressor 
n _ estimators = 1000 seed = 0 网格 搜索 
可以 先 固定 一个 参数 最 优化 后 继续 调整 
第一步 确定 学习 速率 和 tree _ based 给 个 
常见 初始值 根据 是否 类别 不平衡 调节 max _ depth 
min _ child _ weight gamma subsample scale _ pos 
_ weightmax _ depth = 3 起始值 在 4 6 
之间 都是/nr 不错 的 选择 min _ child _ weight 
比 较小 的 值 解决 极不 平衡 的 分类 问题 
eg 1subsample colsample _ bytree = 0.8 这个 是 最 
常见 的 初始 值了 scale _ pos _ weight = 
1 这个 值 是因为 类别 十分 不 平衡 第二步 max 
_ depth 和 min _ weight 对 最终 结果 有 
很大 的 影响 max _ depth range 3 10 2 
min _ child _ weight range 1 6 2 先 
大 范围 地 粗调 参数 然后再 小 范围 地 微调 
第三步 gamma 参数 调 优 gamma i / 10.0 for 
i in range 0 5 第四步 调整 subsample 和 colsample 
_ bytree 参数 subsample i / 100.0 for i in 
range 75 90 5 colsample _ bytree i / 100.0 
for i in range 75 90 5 第五步 正则化 参数 
调 优 reg _ alpha 1e 5 1e 2 0.1 
1 100 reg _ lambda 第六步 降低 学习 速率 learning 
_ rate = 0.01 from sklearn . model _ selection 
import GridSearchCV tuned _ parameters = { n _ estimators 
100 200 500 max _ depth 3 5 7 # 
# range 3 10 2 learning _ rate 0.5 1.0 
subsample 0.75 0.8 0.85 0.9 } tuned _ parameters = 
{ n _ estimators 100 200 500 1000 } clf 
= GridSearchCV XGBClassifier silent = 0 nthread = 4 learning 
_ rate = 0.5 min _ child _ weight = 
1 max _ depth = 3 gamma = 0 subsample 
= 1 colsample _ bytree = 1 reg _ lambda 
= 1 seed = 1000 param _ grid = tuned 
_ parameters scoring = roc _ auc n _ jobs 
= 4 iid = False cv = 5 clf . 
fit X _ train y _ train # # clf 
. grid _ scores _ clf . best _ params 
_ clf . best _ score _ print clf . 
best _ params _ y _ true y _ pred 
= y _ test clf . predict X _ test 
print Accuracy % . 4g % metrics . accuracy _ 
score y _ true y _ pred y _ proba 
= clf . predict _ proba X _ test 1 
print AUC Score Train % f % metrics . roc 
_ auc _ score y _ true y _ proba 
from sklearn . model _ selection import GridSearchCV parameters = 
{ learning _ rate 0.01 0.1 0.3 n _ estimators 
1000 1200 1500 2000 2500 } clf = GridSearchCV XGBClassifier 
max _ depth = 3 min _ child _ weight 
= 1 gamma = 0.5 subsample = 0.6 colsample _ 
bytree = 0.6 objective = binary logistic # 逻辑 回归 
损失 函数 scale _ pos _ weight = 1 reg 
_ alpha = 0 reg _ lambda = 1 seed 
= 27 param _ grid = parameters scoring = roc 
_ auc clf . fit X _ train y _ 
train print clf . best _ params _ y _ 
pre = clf . predict X _ test y _ 
pro = clf . predict _ proba X _ test 
1 print AUC Score % f % metrics . roc 
_ auc _ score y _ test y _ pro 
print Accuracy % . 4g % metrics . accuracy _ 
score y _ test y _ pre 输出 特征 重要性 
import pandas as pd import matplotlib . pylab as plt 
feat _ imp = pd . Series clf . booster 
. get _ fscore . sort _ values ascending = 
False feat _ imp . plot kind = bar title 
= Feature Importances plt . ylabel Feature Importance Score plt 
. show GBDT 输出 新 特征 + blending / stacking 
/ 联 级 森林 R 生成 新 特征 library xgboost 
training iris x1 = rep 0 50 x2 = rep 
1 50 x3 = rep 2 50 x = c 
x1 x2 x3 d = training c 1 4 training 
= data . frame d x ind sample 2 nrow 
training replace = TRUE prob = c 0.7 0.3 # 
对 数据 分成 两部分 70% 训练 数据 30% 检测 数据 
traindata training ind = = 1 # 训练 集 testdata 
training ind = = 2 # 测试 集 traindatax = 
as . matrix traindata c 1 4 traindatay = as 
. matrix traindata 5 testdatax = as . matrix testdata 
c 1 4 testdatay = as . matrix testdata 5 
# # 多 分类 默认 从0/nr 开始 bst xgboost data 
= traindatax label = traindatay max . depth = 3 
eta = 0.1 nround = 1000 objective = multi softmax 
num _ class = 3 pred predict bst testdatax new 
_ feature _ train predict bst traindatax predleaf = T 
new _ feature _ test predict bst testdatax predleaf = 
T t _ train = cbind traindatax new _ feature 
_ train traindatay t _ test = cbind testdatax new 
_ feature _ test testdatay python 生成 GBDT 特征 clf 
. apply X _ train 