1 . 基本思想 综合 某些 专家 的 判断 往往 要 
比 一个 专家 单独 的 判断 要好 在 强可/nr 学习 
和 弱 可 学习 的 概念 上 来说 就是 我们 
通过 对 多个 弱 可 学习 的 算法 进行 组合 
提升 或者说 是 强化 得到 一个 性能 赶 超强 可 
学习 算法 的 算法 如何 地 这些 弱 算法 进行 
提升 是 关键 AdaBoost 算法 是 其中 的 一个 代表 
2 . 分类 算法 提升 的 思路 1 . 找到 
一个 弱 分类器 分类器 简单 快捷 易 操作 如果 它 
本身 就 很复杂 而且 效果 还 不错 那么 进行 提升 
无疑 是 锦上添花 增加 复杂度 甚至 上 性能 并 没有 
得到 提升 具体 情况 具体 而论 2 . 迭代 寻找 
N 个 最优 的 分类器 最优 的 分类器 就是说 这 
N 个 分类器 分别 是 每 一轮 迭代 中 分类 
误差 最小 的 分类器 并且 这 N 个 分类器 组合 
之后 是 分类 效果 最优 的 在 迭代 求解 最优 
的 过程 中 我们 需要 不断 地 修改 数据 的 
权重 AdaBoost 中 是 每 一轮 迭代 得到 一个 分类 
结果 与 正确 分类 作比较 修改 那些 错误 分类 数据 
的 权重 减小 正确 分类 数据 的 权重 后 一个 
分类器 根据 前 一个 分类器 的 结果 修改 权重 在 
进行 分类 因此 可以 看出 迭代 的 过程 中 分类器 
的 效果 越来越 好 所以 需要 给 每个 分类器 赋予 
不同 的 权重 最终 我们 得到 了 N 个 分类器 
和 每个 分类器 的 权重 那么 最终 的 分类器 也 
得到 了 3 . 算法 流程 数据 默认 M * 
N M 行 N 列 M 条 数据 N 维 
输入 训练 数据集 弱 学习 算法 xi 表示 数据 i 
数据 i 是个 N 列 / 维 的 yi 表示 
数据 的 分类 为 yi Y = { 1 1 
} 表示 xi 在 某种 规则 约束 下 的 分类 
可能 为 1 或 + 1 输出 最终 分类器 G 
x 1 初始化 训练 数据 的 权值 分布 初始化 的 
时候 每 一条 数据 权重 均等 M 表示 数据 的 
个数 i = 1 2 3 M2 j = 1 
2 3 J 表示 迭代 的 次数 / 或者 最终 
分类器 的 个数 取决于 是否 能够 使 分类 误差 为 
0 a 使用 具有 权值 分布 Dj 的 训练 数据集 
学习 得到 基本 的 分类器 Gj x X { 1 
+ 1 } b 计算 Gj x 在 训练 集上 
的 分类 误差率 求 的 是 分 错 类别 的 
数据 的 权重 值 和 表示 第 i 个 数据 
的 权重 Dj i c 计算 Gj x 第 j 
个 分类器 的 系数 权重 ln 表示 以 E 为 
底 的 自然对数 跟 ej 没什么 关系 ej 表示 的 
是 分类 错误率 d 更新 训练 数据集 的 权重 Dj 
+ 1 数据集 的 权重 是 根据 上 一次 权重 
进行 更新 的   i = 1 2 3 M 
xi 表示 第 i 条 数据 Z 是 规范化 因子 
他 表示 所有 数据 权重 之和 它 使 Dj + 
1 成为 一个 概率分布 3 构建 基本 分类器 的 线性组合 
得到 最终 的 分类器 4 . 用 一组 数据 来 
具体 解说 一下 Adaboost 的 实现 过程 Data 5 * 
2 原始 类别 1 . 初始化 数据 权重 D1 = 
1/5 1/5 1/5 1/5 1/5 五条 数据 所 以是 5列 
w = 1 / m2 . 分类器 通过 计算 得到 
误差率 最 小时 V 的 值 但是 最小 误差率 是由 
分类 结果 G x 得到 的 所以 这个 V 值 
我们 只有 通过 穷举 得到 1 . 按 第一 维度 
来 分类 我们 找到 第 一维 所有 数据 的 极值 
min = 1 max = 2 我们 从 最小 的 
数据 1 开始 每次 增加 0.5 即 V = min 
+ 0.5 * n n 表示 次数 当 v = 
1 + 0.5 * 1 = 1.5时 分类 结果 G 
x G x = 1 1.5 1 2 1.5 1 
1.3 1.5 1 1 1.5 1 2 1.5 1 G 
x = 1 1 1 1 1 误差率 为 e1 
e1 = sum D G xi = yi 误/v 分类/n 
点/m 的/uj 权重/n 和/c 我们/r 来/v 比较/d 一下/m 分类器/n 的/uj 
分类/n 结果/n 和/c 原始/v 类别/n 就/d 知道/v 那些/r 分/v 错了/i 
G x = 1 1 1 1 1 Lables = 
1 1 1 1 1 对比 一下 可以 发现 第 
2 3 4 5 都分/nr 错了 e1 = D 2 
+ D 3 + D 4 + D 5 = 
0.8 交换 一下 符号 即 分类 结果 G x G 
x = 1 1.5 1 2 1.5 1 1.3 1.5 
1 1 1.5 1 2 1.5 1 G x = 
1 1 1 1 1 误差率 为 e1 G x 
= 1 1 1 1 1 Lables = 1 1 
1 1 1 对比 一下 可以 发现 第 1个 错了 
e1 = D 1 = 0.2 分类器 权重 alpha Alpha 
= 0.5 * ln 1 0.2 / 0.2 更新 数据 
权重 D sum D1 = 1D2 = D1 1 * 
e alpha * 1 / sum D1 D1 1 * 
e alpha * 1 / 1 . . e 的 
系数 最后 的 + 1 取决于 是否 正确 分类 分 
正确 了 就是 1 分 错误 了 就是 1 前面 
公式 中 也有 写到 这里 的 计算 公式 是 统计 
学习 方法 中的 跟 机器学习 实战 中的 D 的 计算 
有一点 出入 在 机器学习 实战 中 D 是 这么 计算 
的 D2 = D1 1 * e alpha * 1 
D2 = D2 / sum D2 但是 就 结果 而言 
好像 影响 不大 只是 对 这个 加权 误差 有影响 我们 
得到 两个 分类器 当 v = 1 + 0.5 * 
2 = 1.5时 重复 以上 步骤 得到 两个 分类器 当 
v = 1 + 0.5 * s 时 一共 寻找 
了 2s 次 当 我们 从 最小值 找 分类 阈值 
直到 最大值 时 我们 得到 了 2s 个 分类器 s 
表示 寻找 的 次数 我们 记录 效果 最好 的 分类器 
即 分类 误差 最小 的 分类器 那么 我们 在 一个 
维度 上 的 寻找 就 完成 了 2 . 接下来 
在 第二 个 维度 上 寻找 同样 得到 2s 个 
分类器 3 . 直到 第 N 维 总共 得到 N 
* 2s 个 分类器 最终 在 这么 多 分类器 找到 
一个 最优 的 分类器 一次 迭代 完成 3 . 接下来 
将 上面 这个 过程 重复 J 次 J 表示 迭代 
次数 如果 h 次 h J 就 得到 了 误差 
为 0 的 分类器 那么 提前结束 迭代 按 所 给 
数据 迭代 三次 就 能够 找到 误差 为零 的 分类器 
看到 这里 应该 对 整个 过程 有了/nr 一个 了解 对于 
数据 权重 D 和 分类器 的 权重 alpha 以及 分类 
误差率 e 的 计算 都有 了 一个 了解 看一下 代码 
源码 源码 是 按照 机器学习 实战 来写 的 因为 个人 
对于 python 不 太熟 机器学习 实战 中 的 代码 运用 
矩阵 来做 很多 公式 中的 乘法 有 很大 的 技巧性 
可能 开始 看 的 时候 没法 理解 这样做 需要 和 
理论 结合 理论 则是 是 来自 统计 学习 方法 1 
# * coding utf 8 * 2 # Filename AdaBoost 
. py 3 # Author Ljcx 4 5 6 AdaBoost 
提升 算法 自适应 boosting 7 优点 泛化 错误率 低 易 
编码 可以 应用 在 大部分 分类器 上 无 参数 调整 
8 缺点 对 离群 点 敏感 9 10 bagging 自举 
汇聚 法 bootstrap aggregating 11 基于 数据 随机 重 抽样 
的 分类器 构建 方法 12 原始数据 集中 重新 选择 次 
得到 个 新 数据集 将 磨 沟 算法 分别 作用 
于 这个 数据集 13 最后 进行 投票 选择 投票 最多 
的 类别 作为 分类 类别 14 15 boosting 类似于 bagging 
多个 分类器 类型 都是 相同 的 16 17 boosting 是 
关注 那些 已有 分类器 错分 的 数据 来 获得 新的 
分类器 18 bagging 则是 根据 已 训练 的 分类器 的 
性能 来 训练 的 19 20 boosting 分类器 权重 不相等 
权重 对 应与 上 一轮 迭代 成功 度 21 bagging 
分类器 权重 相等 22 23 from numpy import * 24 
25 26 class Adaboosting object 27 28 def loadSimpData self 
29 datMat = matrix 30 1 . 2.1 31 2 
. 1.1 32 1.3 1 . 33 1 . 1 
. 34 2 . 1 . 35 classLabels = 1.0 
1.0 1.0 1.0 1.0 36 return datMat classLabels 37 38 
def stumpClassify self datMat dimen threshVal threshIneq 39 40 通过 
阈值 比较 进行 分类 41 dataMat 数据 矩阵 42 dimen 
表示 列 下标 43 threshVal 阈值 44 threshIneq 不等号 lt 
gt 45 只是 简单 的 将 数据 分为 两类 1 
1 初始化 了 一个 全1的/nr 矩阵 我们 判断 一下 阈值 
第 i 列 小于 / 大于 阈值 的 就为 1 
因为 我们 并不 清楚 这个 划分 标准 所以 要 大于 
小 于都 试 一次 46 47 每 一个 维度 的 
所有 数据 跟 阈值 比较 就 相当于 找到 一个 点 
划分 所有 数据 48 49 50 # print data 51 
# print datMat 52 retArr = ones shape datMat 0 
1 # m 数据量 行 1列 列 向量 53 if 
threshIneq = = lt 54 retArr datMat dimen = threshVal 
= 1.0 # 小于 阈值 的 列 都为 1 55 
else 56 retArr datMat dimen threshVal = 1.0 # 大于 
阈值 的 列 都为 1 57 # print retArr 58 
# print retArr 59 return retArr 60 61 def buildStump 
self dataArr classLables D 62 63 单层 决策树 生成 函数 
64 65 dataMatrix = mat dataArr 66 lableMat = mat 
classLables . T 67 m n = shape dataMatrix 68 
numSteps = 10.0 # 步数 影响 的 是 迭代 次数 
步长 69 bestStump = { } # 存储 分类器 的 
信息 70 bestClassEst = mat zeros m 1 # 最好 
的 分类器 71 minError = inf # 迭代 寻找 最小 
错误率 72 for i in range n 73 # 求出 
每 一列 数据 的 最大 最小值 计算 步长 74 rangeMin 
= dataMatrix i . min 75 rangeMax = dataMatrix i 
. max 76 stepSize = rangeMax rangeMin / numSteps 77 
# j 唯一 的 作用 用 步 数去 生成 阈值 
从 最小值 大 最大值 都与 数据 比较 一边 了 一遍 
78 for j in range 1 int numSteps + 1 
79 threshVal = rangeMin + float j * stepSize # 
阈值 80 for inequal in lt gt 81 predictedVals = 
self . stumpClassify 82 dataMatrix i threshVal inequal 83 errArr 
= mat ones m 1 84 errArr predictedVals = = 
lableMat = 0 # 为 1 的 表示 i 分 
错 的 85 weightedError = D . T * errArr 
# 分 错 的 个数 * 权重 开始 权重 = 
1 / M 行 86 # print split dim % 
d thresh % . 2f thresh ineqal \ 87 # 
% s the weighted error is % . 3f % 
i threshVal inequal weightedError 88 if weightedError minError # 寻找 
最小 的 加权 错误率 然后 保存 当前 的 信息 89 
minError = weightedError 90 bestClassEst = predictedVals . copy # 
分类 结果 91 bestStump dim = i 92 bestStump thresh 
= threshVal 93 bestStump ineq = inequal 94 # print 
bestStump 95 # print minError 96 # print bestClassEst # 
类别 估计 97 return bestStump minError bestClassEst 98 99 def 
adaBoostingDs self dataArr classLables numIt = 40 100 101 基于 
单层 决策树 的 AdaBoosting 训练 过程 102 103 weakClassArr = 
# 最佳 决策树 数组 104 m = shape dataArr 0 
105 D = mat ones m 1 / m 106 
aggClassEst = mat zeros m 1 107 for i in 
range numIt 108 bestStump minError bestClassEst = self . buildStump 
109 dataArr classLables D 110 print bestStump bestStump 111 print 
D D . T 112 alpha = float 113 0.5 
* log 1.0 minError / max minError 1e 16 114 
bestStump alpha = alpha 115 weakClassArr . append bestStump 116 
print alpha alpha 117 print classEst bestClassEst . T # 
类别 估计 118 119 expon = multiply 1 * alpha 
* mat classLables . T bestClassEst 120 D = multiply 
D exp expon 121 D = D / D . 
sum 122 123 aggClassEst + = alpha * bestClassEst 124 
print aggClassEst aggClassEst . T 125 # 累加 错误率 126 
aggErrors = multiply sign aggClassEst = 127 mat classLables . 
T ones m 1 128 # 错误率 平均值 129 errorsRate 
= aggErrors . sum / m 130 print total error 
errorsRate \ n 131 if errorsRate = = 0.0 132 
break 133 print weakClassArr weakClassArr 134 return weakClassArr 135 136 
def adClassify self datToClass classifierArr 137 138 预测 分类 139 
datToClass 待 分类 数据 140 classifierArr 训 练好 的 分类器 
数组 141 142 dataMatrix = mat datToClass 143 m = 
shape dataMatrix 0 144 aggClassEst = mat zeros m 1 
145 print 146 for i in range len classifierArr # 
有 多少 个 分类器 迭代 多少次 147 # 调用 第一 
个 分类 器 进行 分类 148 classEst = self . 
stumpClassify dataMatrix classifierArr i dim 149 classifierArr i thresh 150 
classifierArr i ineq 151 152 # alpha 表示 每个 分类器 
的 权重 153 print classEst 154 aggClassEst + = classifierArr 
i alpha * classEst 155 print aggClassEst 156 return sign 
aggClassEst 157 158 159 if _ _ name _ _ 
= = _ _ main _ _ 160 adaboosting = 
Adaboosting 161 D = mat ones 5 1 / 5 
162 dataMat lableMat = adaboosting . loadSimpData 163 # 训练 
分类器 164 classifierArr = adaboosting . adaBoostingDs dataMat lableMat 40 
165 # 预测 数据 166 result = adaboosting . adClassify 
0 0 classifierArr 167 print result 运行 结果 可以 看到 
迭代 三次 加权 错误率 为 0 最后 有 一个 对 
数据 0 0 的 预测 weakClassArr 表示 保存 的 三个 
分类器 的 信息 我们 用 这个 分类器 对 数据 进行 
预测 三个 小数 对应 的 是 三个 分类器 前 N 
个 分类 加权 分类 结果 累加 对应 的 1 1 
1 表示 三个 分类器 对 这个 数据 分类 是 1 
最后 一个 表示 增强 分类器 对 这个 数据 的 加权 
求和 分类 结果 为 1 