一 二次 代价 函数 1 . 形式 其中 C 为 
代价 函数 X 表示 样本 Y 表示 实际 值 a 
表示 输出 值 n 为 样本 总数 2 . 利用 
梯度 下 降法 调整 权值 参数 大小 推导 过程 如下 
图 所示 根据 结果 可得 权重 w 和 偏置 b 
的 梯度 跟 激活 函数 的 梯度 成正比 即 激活 
函数 的 梯度 越大 w 和b的/nr 大小 调整 的 越快 
训练 速度 也 越快 3 . 激活 函数 是 sigmoid 
函数 时 二次 代价 函数 调整 参数 过程 分析 理想 
调整 参数 状态 距离 目标 点 远 时 梯度 大 
参数 调整 较快 距离 目标 点 近时 梯度 小 参数 
调整 较慢 如果 我 的 目标 点 是 调整 到 
M 点 从A点/nr = = B 点 的 调整 过程 
A 点 距离 目标 点 远 梯度 大 调整 参数 
较快 B 点 距离 目标 较近 梯度 小 调整 参数 
慢 符合 参数 调整 策略 如果 我 的 目标 点 
是 调整 到 N 点 从B点/nr = = A 点 
的 调整 过程 A 点 距离 目标 点 近 梯度 
大 调整 参数 较快 B 点 距离 目标 较远 梯度 
小 调整 参数 慢 不 符合 参数 调整 策略 二 
交叉 熵 代价 函数 1 . 形式 其中 C 为 
代价 函数 X 表示 样本 Y 表示 实际 值 a 
表示 输出 值 n 为 样本 总数 2 . 利用 
梯度 下 降法 调整 权值 参数 大小 推导 过程 如下 
图 所示 根据 结果 可得 权重 w 和 偏置 b 
的 梯度 跟 激活 函数 的 梯度 无关 而 和 
输出 值 与 实际 值 的 误差 成正比 即 误差 
越大 w 和b的/nr 大小 调整 的 越快 训练 速度 也 
越快 3 . 激活 函数 是 sigmoid 函数 时 二次 
代价 函数 调整 参数 过程 分析 理想 调整 参数 状态 
距离 目标 点 远 时 梯度 大 参数 调整 较快 
距离 目标 点 近时 梯度 小 参数 调整 较慢 如果 
我 的 目标 点 是 调整 到 M 点 从A点/nr 
= = B 点 的 调整 过程 A 点 距离 
目标 点 远 误差 大 调整 参数 较快 B 点 
距离 目标 较近 误差 小 调整 参数 较慢 符合 参数 
调整 策略 如果 我 的 目标 点 是 调整 到 
N 点 从B点/nr = = A 点 的 调整 过程 
A 点 距离 目标 点 近 误差 小 调整 参数 
较慢 B 点 距离 目标 较远 误差 大 调整 参数 
较快 符合 参数 调整 策略 总结 如果 输出 神经元 是 
线性 的 选择 二次 代价 函数 较为 合适 如果 输出 
神经元 是 型函数 sigmoid 函数 选择 交叉 熵 代价 函数 
较为 合适 如果 输出 神经元 是 softmax 回归 的 代价 
函数 选择 对数 释然 代价 函数 较为 合适 二 利用 
代价 函数 优化 MNIST 数据集 识别 程序 1 . 在 
Tensorflow 中 代价 函数 的 选择 如果 输出 神经元 是 
线性 的 选择 二次 代价 函数 较为 合适 loss = 
tf . reduce _ mean tf . square 如果 输出 
神经元 是 型函数 sigmoid 函数 选择 交叉 熵 代价 函数 
较为 合适 loss = tf . reduce _ mean tf 
. nn . sigmoid _ cross _ entropy _ with 
_ logits 如果 输出 神经元 是 softmax 回归 的 代价 
函数 选择 对数 释然 代价 函数 较为 合适 loss = 
tf . reduce _ mean tf . nn . softmax 
_ cross _ entropy _ with _ logits 2 . 
通过 代价 函数 选择 对 MNIST 数据集 分类程序 优化 # 
使用 交叉 熵 代价 函数 1 import os 2 os 
. environ TF _ CPP _ MIN _ LOG _ 
LEVEL = 2 3 import tensorflow as tf 4 from 
tensorflow . examples . tutorials . mnist import input _ 
data 5 # 载入 数据集 6 mnist = input _ 
data . read _ data _ sets MNIST _ data 
one _ hot = True 7 # 每个 批次 的 
大小 即 每次 训练 的 图片 数量 8 batch _ 
size = 50 9 # 计算 一共 有 多少 个 
批次 10 n _ bitch = mnist . train . 
num _ examples / / batch _ size 11 # 
定义 两个 placeholder 12 x = tf . placeholder tf 
. float32 None 784 13 y = tf . placeholder 
tf . float32 None 10 14 # 创建 一个 只有 
输入 层 784个 神经元 和 输出 层 10个 神经元 的 
简单 神经网络 15 Weights = tf . Variable tf . 
zeros 784 10 16 Biases = tf . Variable tf 
. zeros 10 17 Wx _ plus _ B = 
tf . matmul x Weights + Biases 18 prediction = 
tf . nn . softmax Wx _ plus _ B 
19 # 交叉 熵 代价 函数 20 loss = tf 
. reduce _ mean tf . nn . softmax _ 
cross _ entropy _ with _ logits labels = y 
logits = prediction 21 # 使用 梯度 下 降法 22 
train _ step = tf . train . G r 
a d i e n t D e s c 
e n t O p t i m i z 
e r 0.15 . minimize loss 23 # 初始化 变量 
24 init = tf . global _ variables _ initializer 
25 # 结果 存放 在 一个 布尔 型 列表 中 
26 correct _ prediction = tf . equal tf . 
argmax y 1 tf . argmax prediction 1 # argmax 
返回 一维 张量 中 最大 的 值 所在 的 位置 
标签 值 和 预测 值 相同 返回 为 True 27 
# 求 准确率 28 accuracy = tf . reduce _ 
mean tf . cast correct _ prediction tf . float32 
# cast 函数 将 correct _ prediction 的 布尔 型 
转换 为 浮点 型 然后 计算 平均值 即为 准确率 29 
30 with tf . Session as sess 31 sess . 
run init 32 # 将 测试 集 循环 训练 20次 
33 for epoch in range 21 34 # 将 测试 
集中 所有 数据 循环 一次 35 for batch in range 
n _ bitch 36 batch _ xs batch _ ys 
= mnist . train . next _ batch batch _ 
size # 取 测试 集中 batch _ size 数量 的 
图片 及 对应 的 标签 值 37 sess . run 
train _ step feed _ dict = { x batch 
_ xs y batch _ ys } # 将上 一行 
代码 取到 的 数据 进行 训练 38 acc = sess 
. run accuracy feed _ dict = { x mnist 
. test . images y mnist . test . labels 
} # 准确率 的 计算 39 print Iter + str 
epoch + Testing Accuracy = + str acc View Code 
# 执行 结果 1 Iter 0 Testing Accuracy = 0.8323 
2 Iter 1 Testing Accuracy = 0.8947 3 Iter 2 
Testing Accuracy = 0.9032 4 Iter 3 Testing Accuracy = 
0.9068 5 Iter 4 Testing Accuracy = 0.909 6 Iter 
5 Testing Accuracy = 0.9105 7 Iter 6 Testing Accuracy 
= 0.9126 8 Iter 7 Testing Accuracy = 0.9131 9 
Iter 8 Testing Accuracy = 0.9151 10 Iter 9 Testing 
Accuracy = 0.9168 11 Iter 10 Testing Accuracy = 0.9178 
12 Iter 11 Testing Accuracy = 0.9173 13 Iter 12 
Testing Accuracy = 0.9181 14 Iter 13 Testing Accuracy = 
0.9194 15 Iter 14 Testing Accuracy = 0.9201 16 Iter 
15 Testing Accuracy = 0.9197 17 Iter 16 Testing Accuracy 
= 0.9213 18 Iter 17 Testing Accuracy = 0.9212 19 
Iter 18 Testing Accuracy = 0.9205 20 Iter 19 Testing 
Accuracy = 0 . 9215View Code # 使用 二次 代价 
函数 1 import os 2 os . environ TF _ 
CPP _ MIN _ LOG _ LEVEL = 2 3 
import tensorflow as tf 4 from tensorflow . examples . 
tutorials . mnist import input _ data 5 # 载入 
数据集 6 mnist = input _ data . read _ 
data _ sets MNIST _ data one _ hot = 
True 7 # 每个 批次 的 大小 即 每次 训练 
的 图片 数量 8 batch _ size = 100 9 
# 计算 一共 有 多少 个 批次 10 n _ 
bitch = mnist . train . num _ examples / 
/ batch _ size 11 # 定义 两个 placeholder 12 
x = tf . placeholder tf . float32 None 784 
13 y = tf . placeholder tf . float32 None 
10 14 # 创建 一个 只有 输入 层 784个 神经元 
和 输出 层 10个 神经元 的 简单 神经网络 15 Weights 
= tf . Variable tf . zeros 784 10 16 
Biases = tf . Variable tf . zeros 10 17 
Wx _ plus _ B = tf . matmul x 
Weights + Biases 18 prediction = tf . nn . 
softmax Wx _ plus _ B 19 # 二次 代价 
函数 20 loss = tf . reduce _ mean tf 
. square y prediction 21 # 使用 梯度 下 降法 
22 train _ step = tf . train . G 
r a d i e n t D e s 
c e n t O p t i m i 
z e r 0.2 . minimize loss 23 # 初始化 
变量 24 init = tf . global _ variables _ 
initializer 25 # 结果 存放 在 一个 布尔 型 列表 
中 26 correct _ prediction = tf . equal tf 
. argmax y 1 tf . argmax prediction 1 # 
argmax 返回 一维 张量 中 最大 的 值 所在 的 
位置 标签 值 和 预测 值 相同 返回 为 True 
27 # 求 准确率 28 accuracy = tf . reduce 
_ mean tf . cast correct _ prediction tf . 
float32 # cast 函数 将 correct _ prediction 的 布尔 
型 转换 为 浮点 型 然后 计算 平均值 即为 准确率 
29 30 with tf . Session as sess 31 sess 
. run init 32 # 将 测试 集 循环 训练 
20次 33 for epoch in range 21 34 # 将 
测试 集中 所有 数据 循环 一次 35 for batch in 
range n _ bitch 36 batch _ xs batch _ 
ys = mnist . train . next _ batch batch 
_ size # 取 测试 集中 batch _ size 数量 
的 图片 及 对应 的 标签 值 37 sess . 
run train _ step feed _ dict = { x 
batch _ xs y batch _ ys } # 将上 
一行 代码 取到 的 数据 进行 训练 38 acc = 
sess . run accuracy feed _ dict = { x 
mnist . test . images y mnist . test . 
labels } # 准确率 的 计算 39 print Iter + 
str epoch + Testing Accuracy = + str acc View 
Code # 执行 结果 1 Iter 0 Testing Accuracy = 
0.8325 2 Iter 1 Testing Accuracy = 0.8711 3 Iter 
2 Testing Accuracy = 0.8831 4 Iter 3 Testing Accuracy 
= 0.8876 5 Iter 4 Testing Accuracy = 0.8942 6 
Iter 5 Testing Accuracy = 0.898 7 Iter 6 Testing 
Accuracy = 0.9002 8 Iter 7 Testing Accuracy = 0.9014 
9 Iter 8 Testing Accuracy = 0.9036 10 Iter 9 
Testing Accuracy = 0.9052 11 Iter 10 Testing Accuracy = 
0.9065 12 Iter 11 Testing Accuracy = 0.9073 13 Iter 
12 Testing Accuracy = 0.9084 14 Iter 13 Testing Accuracy 
= 0.909 15 Iter 14 Testing Accuracy = 0.9095 16 
Iter 15 Testing Accuracy = 0.9115 17 Iter 16 Testing 
Accuracy = 0.912 18 Iter 17 Testing Accuracy = 0.9126 
19 Iter 18 Testing Accuracy = 0.913 20 Iter 19 
Testing Accuracy = 0.9136 21 Iter 20 Testing Accuracy = 
0 . 914View Code 结论 二者 只有 代价 函数 不同 
正确率 达到 90% 所用 迭代 次数 使用 交叉 熵 代价 
函数 为 第三 次 使用 二次 代价 函数 为 第六 
次 在 MNIST 数据集 分类 中 使用 交叉 熵 代价 
函数 收敛 速度 较快 最终 正确率 使用 交叉 熵 代价 
函数 为 92.15% 使用 二次 代价 函数 为 91.4% 在 
MNIST 数据集 分类 中 使用 交叉 熵 代价 函数 识别 
准确率 较高 三 拟合 问题 参考 文章 https / / 
blog . csdn . net / willduan1 / article / 
details / 530707771 . 根据 拟合 结果 分类 欠 拟合 
模型 没有 很好 地 捕捉 到 数据 特征 不 能够 
很好 地 拟合 数据 正确 拟合 过拟合 模型 把 数据 
学习 的 太 彻底 以至于 把 噪声 数据 的 特征 
也 学习 到了 这样 就 会 导致 在 后期 测试 
的 时候 不 能够 很好 地 识别 数据 即 不能 
正确 的 分类 模型 泛化 能力 太差 2 . 解决 
欠 拟合 和 过拟合 解决 欠 拟合 常用 方法 添加 
其他 特征 项 有时候 我们 模型 出现 欠 拟合 的 
时候 是因为 特征 项 不够 导致 的 可以 添加 其他 
特征 项来/nr 很好 地 解决 添加 多项式 特征 这个 在 
机器学习 算法 里面 用 的 很 普遍 例如/v 将/d 线性/n 
模型/n 通过/p 添加/v 二次/m 项/n 或者/c 三次/m 项使/nr 模型/n 泛化/v 
能力/n 更强/i 减少 正则化 参数 正则化 的 目的 是 用来 
防止 过拟合 的 但是 现在 模型 出现 了 欠 拟合 
则 需要 减少 正则化 参数 解决 过拟合 常用 方法 增加 
数据集 正则化 方法 Dropout 通俗 一点 讲 就是 dropout 方法 
在 训练 的 时候 让 神经元 以 一定 的 概率 
不 工作 四 初始化 优化 MNIST 数据集 分类 问题 # 
改变 初始化 方法 Weights = tf . Variable tf . 
truncated _ normal 784 10 Biases = tf . Variable 
tf . zeros 10 + 0.1 五 优 化器 优化 
MNIST 数据集 分类 问题 大多数 机器学习 任务 就是 最小化 损失 
在 损失 定义 的 情况 下 后面 的 工作 就 
交给 优 化器 因为 深度 学习 常见 的 是 对于 
梯度 的 优化 也 就是说 优 化器 最后 其实 就是 
各种 对于 梯度 下降 算法 的 优化 1 . 梯度 
下 降法 分类 及 其 介绍 标准 梯度 下 降法 
先 计算 所有 样本 汇总 误差 然后 根据 总 误差 
来 更新 权值 随机 梯度 下 降法 随机 抽取 一个样 
本来 计算误差 然后 更新 权值 批量 梯度 下 降法 是 
一种 折中 方案 从总/nr 样本 中 选取 一个 批次 batch 
然后 计算 这个 batch 的 总 误差 根据 总 误差 
来 更新 权值 2 . 常见 优 化器 介绍 参考 
文章 https / / www . leiphone . com / 
news / 201706 / e0PuNeEzaXWsMPZX . html3 . 优 化器 
优化 MNIST 数据集 分类 问题 # 选择 Adam 优 化器 
1 import os 2 os . environ TF _ CPP 
_ MIN _ LOG _ LEVEL = 2 3 import 
tensorflow as tf 4 from tensorflow . examples . tutorials 
. mnist import input _ data 5 # 载入 数据集 
6 mnist = input _ data . read _ data 
_ sets MNIST _ data one _ hot = True 
7 # 每个 批次 的 大小 即 每次 训练 的 
图片 数量 8 batch _ size = 50 9 # 
计算 一共 有 多少 个 批次 10 n _ bitch 
= mnist . train . num _ examples / / 
batch _ size 11 # 定义 两个 placeholder 12 x 
= tf . placeholder tf . float32 None 784 13 
y = tf . placeholder tf . float32 None 10 
14 # 创建 一个 只有 输入 层 784个 神经元 和 
输出 层 10个 神经元 的 简单 神经网络 15 Weights = 
tf . Variable tf . zeros 784 10 16 Biases 
= tf . Variable tf . zeros 10 17 Wx 
_ plus _ B = tf . matmul x Weights 
+ Biases 18 prediction = tf . nn . softmax 
Wx _ plus _ B 19 # 交叉 熵 代价 
函数 20 loss = tf . reduce _ mean tf 
. nn . softmax _ cross _ entropy _ with 
_ logits labels = y logits = prediction 21 # 
使用 Adam 优 化器 22 train _ step = tf 
. train . AdamOptimizer 1e 2 . minimize loss 23 
# 初始化 变量 24 init = tf . global _ 
variables _ initializer 25 # 结果 存放 在 一个 布尔 
型 列表 中 26 correct _ prediction = tf . 
equal tf . argmax y 1 tf . argmax prediction 
1 # argmax 返回 一维 张量 中 最大 的 值 
所在 的 位置 标签 值 和 预测 值 相同 返回 
为 True 27 # 求 准确率 28 accuracy = tf 
. reduce _ mean tf . cast correct _ prediction 
tf . float32 # cast 函数 将 correct _ prediction 
的 布尔 型 转换 为 浮点 型 然后 计算 平均值 
即为 准确率 29 30 with tf . Session as sess 
31 sess . run init 32 # 将 测试 集 
循环 训练 20次 33 for epoch in range 21 34 
# 将 测试 集中 所有 数据 循环 一次 35 for 
batch in range n _ bitch 36 batch _ xs 
batch _ ys = mnist . train . next _ 
batch batch _ size # 取 测试 集中 batch _ 
size 数量 的 图片 及 对应 的 标签 值 37 
sess . run train _ step feed _ dict = 
{ x batch _ xs y batch _ ys } 
# 将上 一行 代码 取到 的 数据 进行 训练 38 
acc = sess . run accuracy feed _ dict = 
{ x mnist . test . images y mnist . 
test . labels } # 准确率 的 计算 39 print 
Iter + str epoch + Testing Accuracy = + str 
acc View Code # 执行 结果 Iter 1 Testing Accuracy 
= 0.9224 Iter 2 Testing Accuracy = 0.9293 Iter 3 
Testing Accuracy = 0.9195 Iter 4 Testing Accuracy = 0.9282 
Iter 5 Testing Accuracy = 0.926 Iter 6 Testing Accuracy 
= 0.9291 Iter 7 Testing Accuracy = 0.9288 Iter 8 
Testing Accuracy = 0.9274 Iter 9 Testing Accuracy = 0.9277 
Iter 10 Testing Accuracy = 0.9249 Iter 11 Testing Accuracy 
= 0.9313 Iter 12 Testing Accuracy = 0.9301 Iter 13 
Testing Accuracy = 0.9315 Iter 14 Testing Accuracy = 0.9295 
Iter 15 Testing Accuracy = 0.9299 Iter 16 Testing Accuracy 
= 0.9303 Iter 17 Testing Accuracy = 0.93 Iter 18 
Testing Accuracy = 0.9304 Iter 19 Testing Accuracy = 0.9269 
Iter 20 Testing Accuracy = 0 . 9273View Code 注意 
不同 优 化器 参数 的 设置 是 关键 在 机器 
学习 中 参数 的 调整 应该 是 技术 加 经验 
而 不是 盲目 调整 这边 是 我 以后 需要 学习 
和 积累 的 地方 六 根据 今天 所 学 内容 
对 MNIST 数据集 分类 进行 优化 准确率 达到 95% 以上 
# 优化 程序 1 import os 2 os . environ 
TF _ CPP _ MIN _ LOG _ LEVEL = 
2 3 import tensorflow as tf 4 from tensorflow . 
examples . tutorials . mnist import input _ data 5 
# 载入 数据集 6 mnist = input _ data . 
read _ data _ sets MNIST _ data one _ 
hot = True 7 # 每个 批次 的 大小 即 
每次 训练 的 图片 数量 8 batch _ size = 
50 9 # 计算 一共 有 多少 个 批次 10 
n _ bitch = mnist . train . num _ 
examples / / batch _ size 11 # 定义 两个 
placeholder 12 x = tf . placeholder tf . float32 
None 784 13 y = tf . placeholder tf . 
float32 None 10 14 # 创建 一个 只有 输入 层 
784个 神经元 和 输出 层 10个 神经元 的 简单 神经网络 
15 Weights1 = tf . Variable tf . truncated _ 
normal 784 200 16 Biases1 = tf . Variable tf 
. zeros 200 + 0.1 17 Wx _ plus _ 
B _ L1 = tf . matmul x Weights1 + 
Biases1 18 L1 = tf . nn . tanh Wx 
_ plus _ B _ L1 19 20 Weights2 = 
tf . Variable tf . truncated _ normal 200 50 
21 Biases2 = tf . Variable tf . zeros 50 
+ 0.1 22 Wx _ plus _ B _ L2 
= tf . matmul L1 Weights2 + Biases2 23 L2 
= tf . nn . tanh Wx _ plus _ 
B _ L2 24 25 Weights3 = tf . Variable 
tf . truncated _ normal 50 10 26 Biases3 = 
tf . Variable tf . zeros 10 + 0.1 27 
Wx _ plus _ B _ L3 = tf . 
matmul L2 Weights3 + Biases3 28 prediction = tf . 
nn . softmax Wx _ plus _ B _ L3 
29 30 # 交叉 熵 代价 函数 31 loss = 
tf . reduce _ mean tf . nn . softmax 
_ cross _ entropy _ with _ logits labels = 
y logits = prediction 32 # 使用 梯度 下 降法 
33 train _ step = tf . train . AdamOptimizer 
2e 3 . minimize loss 34 # 初始化 变量 35 
init = tf . global _ variables _ initializer 36 
# 结果 存放 在 一个 布尔 型 列表 中 37 
correct _ prediction = tf . equal tf . argmax 
y 1 tf . argmax prediction 1 38 # 求 
准确率 39 accuracy = tf . reduce _ mean tf 
. cast correct _ prediction tf . float32 40 41 
with tf . Session as sess 42 sess . run 
init 43 # 将 测试 集 循环 训练 50次 44 
for epoch in range 51 45 # 将 测试 集中 
所有 数据 循环 一次 46 for batch in range n 
_ bitch 47 batch _ xs batch _ ys = 
mnist . train . next _ batch batch _ size 
# 取 测试 集中 batch _ size 数量 的 图片 
及 对应 的 标签 值 48 sess . run train 
_ step feed _ dict = { x batch _ 
xs y batch _ ys } # 将上 一行 代码 
取到 的 数据 进行 训练 49 test _ acc = 
sess . run accuracy feed _ dict = { x 
mnist . test . images y mnist . test . 
labels } # 准确率 的 计算 50 print Iter + 
str epoch + Testing Accuracy = + str test _ 
acc View Code # 执行 结果 1 Iter 0 Testing 
Accuracy = 0.6914 2 Iter 1 Testing Accuracy = 0.7236 
3 Iter 2 Testing Accuracy = 0.8269 4 Iter 3 
Testing Accuracy = 0.8885 5 Iter 4 Testing Accuracy = 
0.9073 6 Iter 5 Testing Accuracy = 0.9147 7 Iter 
6 Testing Accuracy = 0.9125 8 Iter 7 Testing Accuracy 
= 0.922 9 Iter 8 Testing Accuracy = 0.9287 10 
Iter 9 Testing Accuracy = 0.9248 11 Iter 10 Testing 
Accuracy = 0.9263 12 Iter 11 Testing Accuracy = 0.9328 
13 Iter 12 Testing Accuracy = 0.9316 14 Iter 13 
Testing Accuracy = 0.9387 15 Iter 14 Testing Accuracy = 
0.9374 16 Iter 15 Testing Accuracy = 0.9433 17 Iter 
16 Testing Accuracy = 0.9419 18 Iter 17 Testing Accuracy 
= 0.9379 19 Iter 18 Testing Accuracy = 0.9379 20 
Iter 19 Testing Accuracy = 0.9462 21 Iter 20 Testing 
Accuracy = 0.9437 22 Iter 21 Testing Accuracy = 0.9466 
23 Iter 22 Testing Accuracy = 0.9479 24 Iter 23 
Testing Accuracy = 0.9498 25 Iter 24 Testing Accuracy = 
0.9481 26 Iter 25 Testing Accuracy = 0.9489 27 Iter 
26 Testing Accuracy = 0.9496 28 Iter 27 Testing Accuracy 
= 0.95 29 Iter 28 Testing Accuracy = 0.9508 30 
Iter 29 Testing Accuracy = 0.9533 31 Iter 30 Testing 
Accuracy = 0.9509 32 Iter 31 Testing Accuracy = 0.9516 
33 Iter 32 Testing Accuracy = 0.9541 34 Iter 33 
Testing Accuracy = 0.9513 35 Iter 34 Testing Accuracy = 
0.951 36 Iter 35 Testing Accuracy = 0.9556 37 Iter 
36 Testing Accuracy = 0.9527 38 Iter 37 Testing Accuracy 
= 0.9521 39 Iter 38 Testing Accuracy = 0.9546 40 
Iter 39 Testing Accuracy = 0.9544 41 Iter 40 Testing 
Accuracy = 0.9555 42 Iter 41 Testing Accuracy = 0.9546 
43 Iter 42 Testing Accuracy = 0.9553 44 Iter 43 
Testing Accuracy = 0.9534 45 Iter 44 Testing Accuracy = 
0.9576 46 Iter 45 Testing Accuracy = 0.9535 47 Iter 
46 Testing Accuracy = 0.9569 48 Iter 47 Testing Accuracy 
= 0.9556 49 Iter 48 Testing Accuracy = 0.9568 50 
Iter 49 Testing Accuracy = 0.956 51 Iter 50 Testing 
Accuracy = 0 . 9557View Code # 写 在后面 呀呀 
呀呀 本来 想着 先把 python 学 差不多 再开始 机器学习 和 
这些 框架 的 学习 老师 触 不及 防 的 任务 
给了 论文 让 我 搭 一个 模型 出来 我 只能 
硬着头皮 上 了 不想 用 公式 编译器 了 手写 版 
计算 过程   请 忽略 那 丑丑 的 字儿 加油 
哦 小伙 郭 