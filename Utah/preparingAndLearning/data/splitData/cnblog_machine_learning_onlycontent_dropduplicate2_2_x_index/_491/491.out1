异常 检测 Anomaly Detection 基本 假设 多数 情况下 数 据点 
落入 正常 的 取值 范围 但是 当 异常 行为 发生 
时 数 据点 的 取值 落入 正常 取值 范围 之外 
如 所示 所以 可以 利用 高斯分布 计算 行为 发生 的 
概率 如果 是 概率 小于 给定 阈值 则 认为 发生 
了 异常 行为 基本 过程 是 利用 训练 数 据点 
建立 模型 $ p x $ 对于 新 的 数据 
点 $ x _ { new } $ 如果 $ 
p x _ { new } \ epsilon $ 则 
发生 异常 否则 正常 异常 检测 的 应用 包括 欺诈 
检测 Fraud detection 制造业 Manufacturing 数据中心 监视 电脑 Monitering computers 
in data center 高斯分布 对于 一元 高斯分布 $ x \ 
sim N \ mu \ sigma ^ 2 $ 表达式 
如下 其中 $ \ mu $ 表示 均值 对应 于 
分布 的 对称轴 $ \ sigma $ 表示 数 据点 
的 离散 程度 $ \ sigma $ 越大 函数 图像 
的 下端 张口 越大 峰值 越低 反之 $ \ sigma 
$ 越小 图像 下端 张口 越小 峰值 越高 如 所示 
$ $ p x \ mu \ sigma ^ 2 
= \ frac { 1 } { \ sqrt { 
2 \ pi } \ sigma } exp \ frac 
{ x \ mu ^ 2 } { 2 \ 
sigma ^ 2 } $ $ 参数估计 高斯分布 的 总体 
参数 $ \ mu $ 和$\/nr sigma $ 可以 使用 
样本数据 点 进行 估计 如下 $ $ \ mu = 
\ frac { 1 } { m } \ sum 
\ limits _ { i = 1 } ^ { 
m } x ^ { i } $ $ $ 
$ \ sigma ^ 2 = \ frac { 1 
} { m } \ sum \ limits _ { 
i = 1 } ^ { m } x ^ 
{ i } \ mu ^ 2 $ $ 注意 
在 统计学 中 参数 $ \ sigma ^ 2 $ 
的 系数 为 $ \ frac { 1 } { 
m 1 } $ 而在 机器学习 中 习惯 使用 $ 
\ frac { 1 } { m } $ . 
异常 检测 算法 对于 训练 数据集 $ \ { x 
^ { 1 } x ^ { 2 } \ 
ldots x ^ { m } \ } $ 其中 
数 据点 $ x ^ { i } \ in 
R ^ n $ 并 假设 每个 特征 均 服从 
高斯分布 即 $ x ^ { i } _ j 
\ sim N \ mu \ sigma ^ 2 $ 
可 如下 建立 模型 $ p x $ \ begin 
{ align * } p x & = p x 
_ 1 \ mu _ 1 \ sigma _ 1 
^ 2 p x _ 2 \ mu _ 2 
\ sigma _ 2 ^ 2 \ ldots   p 
x _ n \ mu _ n \ sigma _ 
n ^ 2 \ \ & = \ prod \ 
limits _ { j = 1 } ^ n   
p x _ j \ mu _ j \ sigma 
_ j ^ 2 \ end { align * } 
算法 步骤 1 . 特征选择 选择 能够 指示 异常 行为 
的 特征 2 . 参数估计 用 训练 数据集 估计 每个 
特征 的 整体 均值 $ \ mu _ j $ 
和 方差 $ \ sigma _ j ^ 2 $ 
即 $ \ mu _ j = \ frac { 
1 } { m } \ sum \ limits _ 
{ i = 1 } ^ { m } x 
^ { i } _ j $ $ \ sigma 
^ 2 _ j = \ frac { 1 } 
{ m } \ sum \ limits _ { i 
= 1 } ^ { m } x ^ { 
i } _ j \ mu _ j ^ 2 
$ 3 . 用 估计 得到 的 参数 $ \ 
mu _ 1 \ mu _ 2 \ ldots \ 
mu _ n $   $ \ sigma ^ 2 
_ 1 \ sigma ^ 2 _ 2 \ ldots 
\ sigma ^ 2 _ n $ 建立 模型 $ 
p x $ 4 . 对于 给定 新的 数据 点 
$ x _ { new } $ 计算 $ p 
x _ { new } $ 如果 $ p x 
_ { new } \ epsilon $ 则 发生 异常 
否则 正常 算法 评估 给定 训练 数据集 去掉 标签 建立 
模型 中 $ \ { x ^ { 1 } 
x ^ { 2 } \ ldots x ^ { 
m } \ } $ 训练 模型 $ p x 
$ 在 交叉 验证 集 带 标签 中 如果 $ 
p x _ { cv } \ epsilon $ 则 
预测 $ y = 1 $ 否则 预测 $ y 
= 0 $ 最后 计算 指标 Precision / Recall / 
F1Score 等 来 评估 算法 性能 注意 也 可以 用 
验证 集 来 选择 阈值 $ \ epsilon $ . 
异常 检测 与 监督 式 学习 对比 特征选择 选择 的 
特征 需要 近似 服从于 高斯分布 如果 明显 不 服从 高斯分布 
可以 做 适当 的 转换 例如 $ log x log 
x + c \ sqrt { x } x ^ 
{ 1/3 } $ 等 多元 高斯分布 之前 的 模型 
假设 各个 特征 之间 是 相互 独立 的 因此 模型 
$ p x $ 将 各 特征 取值 的 概率 
相乘 $ P AB = P B P A | 
B = P A P B | A $ 当且仅当 
事件 AB 相互 独立 时 才有 $ P AB = 
P A P B $ 然而 当 各个 特征 之间 
存在 依赖 关系 时 一元 的 高斯 模型 将 不能 
很好 的 刻画 $ p x $ 需要 多元 高斯 
模型 模型 $ p x $ 的 建立 不再 是 
各个 概率 相乘 而 直接 用 多元 高斯分布 进行 刻画 
$ $ p x \ mu \ Sigma = \ 
frac { 1 } { 2 \ pi ^ { 
n / 2 } | \ Sigma | ^ { 
1/2 } } \ exp \ left \ frac { 
1 } { 2 } x \ mu ^ T 
\ Sigma ^ { 1 } x \ mu \ 
right $ $   其中 $ \ mu $ 是 
$ n $ 维 行向量 $ \ mu = \ 
frac { 1 } { m } \ sum \ 
limits _ { i = 1 } ^ { m 
} x ^ { i } $ $ \ Sigma 
$ 是 $ n \ times n $ 协方差 矩阵 
$ \ Sigma = \ frac { 1 } { 
m } \ sum \ limits _ { i = 
1 } ^ { m } x ^ { i 
} \ mu x ^ { i } \ mu 
^ T $ 给 出了 在 不同 参数 取值 下 
的 二维 高斯 模型 及其 对应 的 等高线图 多元 高斯 
模型 和 一元 高斯 模型 的 关系 当 协方差 矩阵 
$ \ Sigma $ 是 对角 阵 且 对角线 元为/nr 
一元 高斯分布 的 估计 参数 $ \ sigma _ j 
^ 2 $ 时 两个 模型 是 等价 的 区别 
在于 前者 能够 自动 获取 特征 之间 的 依赖 关系 
而 后者 不能 后者 假设 特征 之间 是 独立 的 
当 特征 数 $ n $ 很大 时 前者 计算 
代价 高昂 而 后者 计算 速度快 前者 适用 于$m/nr n 
$ 一般 要求 $ m 10n $ 的 情况 而 
后者 当 $ m $ 很小 时 依然 适用 推荐 
系统 电影 推荐 系统 问题 根据 用户 对 已 看过 
电影 的 打分 对 用户 未 看过 的 电影 下 
表中 以 表示 进行 打分 估计 以 给 其 推荐 
合适 的 电影 符号 说明 $ n _ u $ 
表示 用户数量 $ n _ m $ 表示 电影 数量 
$ r i j $ 是 符号 变量 如果 用户 
$ j $ 已经 对 电影 $ i $ 进行 
评 分则 $ r i j = 1 $ 反之 
如果 用户 $ j $ 尚未 对 电影 $ i 
$ 进行 评 分则 $ r i j = 0 
$ . $ y ^ { i j } $ 
表示 用户 $ j $ 对 电影 $ i $ 
的 评分 如果 用户 $ j $ 对 电影 $ 
i $ 已经 评分 即 $ r i j = 
1 $ . M o v i e U s 
e r 1 U s e r 2 U s 
e r 3 U s e r 4 x 1 
x 2 m o v i e 1 5 5 
0 0 0 . 90movie25 01.00 . 01movie3 40 0 
. 990movie400540 . 11 . 0movie5005 00.9 基于 内容 的 
推荐 对 每 一部 电影 $ i $ 抽出 若干 
特征 然后 每个 用户 $ j $ 学习 一个 参数 
向量 $ \ theta ^ { j } $ 然后 
用 $ \ theta ^ { j } ^ Tx 
^ { i } $ 来 估计 用户 $ j 
$ 对 电影 $ i $ 的 评分 例如 对于 
上面 的 表格 我们 对 每 一个 电影 抽 取出 
2个 特征 $ x _ 1 x _ 2 $ 
对应 表格 最后 2列 然后 每个 用户 $ j $ 
学习 一个 参数 向量 $ \ theta ^ { j 
} \ in R ^ 3 $ 包含 bias 项$\/nr 
theta _ 0 = 1 $ 以及 $ x _ 
1 x _ 2 $ 的 系数 $ \ theta 
_ 1 \ theta _ 2 $ 然后 就 可以 
用 $ \ theta ^ { j } ^ Tx 
^ { i } $ 来 预测 评分 为了 学习 
参数 $ \ theta $ 定义 代价 函数 为 $ 
$ J \ theta ^ { 1 } \ theta 
^ { 2 } \ ldots \ theta ^ { 
n _ u } = \ frac { 1 } 
{ 2 } \ sum \ limits _ { j 
= 1 } ^ { n _ u } \ 
sum \ limits _ { i r i j = 
1 } \ theta ^ { j } ^ Tx 
^ { i } y ^ { i j } 
^ 2 + \ frac { \ lambda } { 
2 } \ sum \ limits _ { j = 
1 } ^ { n _ u } \ sum 
\ limits _ { k = 1 } ^ n 
\ theta ^ { j } _ k ^ 2 
$ $ 梯度 下 降法 的 参数 更新 $ $ 
\ theta _ k ^ { j } = \ 
theta _ k ^ { j } \ alpha \ 
left \ sum \ limits _ { i r i 
j = 1 } \ theta ^ { j } 
^ Tx ^ { i } y ^ { i 
j } x _ k ^ { i } + 
\ lambda \ theta _ k ^ { j } 
\ right \ quad k 0 $ $   $ 
$ \ theta _ k ^ { j } = 
\ theta _ k ^ { j } \ alpha 
\ sum \ limits _ { i r i j 
= 1 } \ theta ^ { j } ^ 
Tx ^ { i } y ^ { i j 
} x _ k ^ { i } \ quad 
k = 0 $ $ 协同 过滤 Collaborative Filtering 基于 
内容 的 推荐 假设 电影 的 特征 如 $ x 
_ 1 $ $ x _ 2 $ 是 已知 
的 仅 需要 学习 参数 $ \ theta $ 然而 
实际 中 电影 的 特征 是 未知 的 现在 假定 
已知 用户 的 参数 $ \ theta $ 需要 学习 
电影 的 特征 $ x $ 与 上面 的 代价 
函数 类似 定义 $ $ J x ^ { 1 
} x ^ { 2 } \ ldots x ^ 
{ n _ m } = \ frac { 1 
} { 2 } \ sum \ limits _ { 
i = 1 } ^ { n _ m } 
\ sum \ limits _ { i r i j 
= 1 } \ theta ^ { j } ^ 
Tx ^ { i } y ^ { i j 
} ^ 2 + \ frac { \ lambda } 
{ 2 } \ sum \ limits _ { i 
= 1 } ^ { n _ m } \ 
sum \ limits _ { k = 1 } ^ 
n x ^ { i } _ k ^ 2 
$ $ 这样 我们 发现 给定 电影 特征 $ x 
$ 可以 学习 到 用户 参数 $ \ theta $ 
反之 给定 用户 参数 $ \ theta $ 可以 学习 
到 特征 $ x $ 因此 可以 先 随机 猜 
一个 $ \ theta $ 然后 学习 $ x $ 
再由 学习 到 的 $ x $ 学习 $ \ 
theta $ 然后 不断 重复 即可 然而 事实上 两个 参数 
$ x \ theta $ 可以 如下 同时 更新 从而 
得到 协同 过滤 的 推荐算法 $ $ J x ^ 
{ 1 } x ^ { 2 } \ ldots 
x ^ { n _ m } \ theta ^ 
{ 1 } \ theta ^ { 2 } \ 
ldots \ theta ^ { n _ u } = 
\ frac { 1 } { 2 } \ sum 
\ limits _ { i r i j = 1 
} \ theta ^ { j } ^ Tx ^ 
{ i } y ^ { i j } ^ 
2 + \ frac { \ lambda } { 2 
} \ sum \ limits _ { j = 1 
} ^ { n _ u } \ sum \ 
limits _ { k = 1 } ^ n \ 
theta ^ { j } _ k ^ 2 + 
\ frac { \ lambda } { 2 } \ 
sum \ limits _ { i = 1 } ^ 
{ n _ m } \ sum \ limits _ 
{ k = 1 } ^ n x ^ { 
i } _ k ^ 2 $ $ 协同 过滤 
算法 步骤 1 . 初始化 参数 $ x ^ { 
1 } x ^ { 2 } \ ldots x 
^ { n _ m } \ theta ^ { 
1 } \ theta ^ { 2 } \ ldots 
\ theta ^ { n _ u } $ 为 
随机数 其中 $ x \ in R ^ n $ 
表示 电影 特征 $ \ theta \ in R ^ 
n $ 表示 用户 参数 注 不 包含 bias 参数 
$ \ theta _ 0 $ 2 . 使用 梯度 
下降 或者 其他 高级 优化 算法 进行 参数 更新 $ 
$ x _ k ^ { i } = x 
_ k ^ { i } \ alpha \ left 
\ sum \ limits _ { i r i j 
= 1 } \ theta ^ { j } ^ 
Tx ^ { i } y ^ { i j 
} x _ k ^ { i } + \ 
lambda x _ k ^ { i } \ right 
$ $ $ $ \ theta _ k ^ { 
j } = \ theta _ k ^ { j 
} \ alpha \ left \ sum \ limits _ 
{ i r i j = 1 } \ theta 
^ { j } ^ Tx ^ { i } 
y ^ { i j } x _ k ^ 
{ i } + \ lambda \ theta _ k 
^ { j } \ right $ $ 3 . 
用 学习 到 的 参数 $ \ theta $ 和$x/nr 
$ 预测 电影 评分 $ \ theta ^ Tx $ 
低 秩 矩阵 分解 Low rank matrix factorization 协同 过滤 
与 低 秩 矩阵 分解 协同 过滤 算法 要求 评分 
矩阵 $ Y $ 中 元素 $ y ^ { 
i j } $ 越 接近 $ \ theta ^ 
{ j } ^ T x ^ { i } 
$ 越好 因 此参数 $ \ theta $ 和$x/nr $ 
的 求解 实际上/d 等价/n 于/p 寻找/v 两个/m 矩阵/n $/i X/w 
$/i 和$\/nr Theta/w $/i 使得/v $/i Y/w \ approx X 
\ Theta ^ T $ 从而 协同 过滤 问题 可以 
转化 为 低 秩 矩阵 分解 问题 均值 归一化 对于 
尚未 评分 任何 电影 的 用户 可以 对 $ Y 
$ 矩阵 按 行 求 平均值 作为 该 用户 的 
初始 评分 用 均值 化 矩阵 $ Y \ mu 
$ 进行 参数 学习 然后 用 $ \ theta ^ 
{ j } ^ T \ theta ^ { i 
} + \ mu _ i $ 进行 评分 预测 
参考文献 1 Andrew Ng Coursera 公开课 第 九周 2   
Recommender Systems Collaborative Filtering .   http / / recommender 
systems . org / collaborative filtering / 3 Wikipedia   
Low rank approximation   https / / en . wikipedia 
. org / wiki / Low rank _ approximation 