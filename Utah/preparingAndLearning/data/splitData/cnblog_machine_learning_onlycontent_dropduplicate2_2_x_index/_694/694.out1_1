注 这个 报告 是 我 在 10年 7月 的 时候 
写 的 博士 一年级 最近 整理 电脑 的 时候 翻到 
当时 初学 一些 KDD 上 的 paper 的 时候 总结 
的 现在 拿出 来 分享 一下 毕竟 是 初学 的 
时候 写 的 有些 东西 的 看法 也 在 变化 
看 的 人 可以 随便 翻翻 有错 指正 我 重点 
部分 是 第 3 章和 第 4 章 对应 的 
两篇 paper 具体 可以 在 参考 文献 里 找到 当时 
还 算 比较 新 如 引用 务必 请 注明 本文 
出自 http / / www . cnblogs . com / 
xbinworld1 介绍 在 计算机 视觉 模式识别 数据挖掘 很多 应用 问题 
中 我们 经常 会 遇到 很高 维度 的 数据 高 
维度 的 数据 会 造成 很多 问题 例如 导致 算法 
运行 性能 以及 准确性 的 降低 特征 选取 Feature Selection 
技术 的 目标 是 找到 原始数据 维度 中 的 一个 
有用 的 子集 再运 用 一些 有效 的 算法 实现 
数据 的 聚 类 分类 以及 检索 等 任务 特征 
选取 的 目标 是 选择 那些 在 某一 特定 评价 
标准 下 的 最重要 的 特征 子集 这个 问题 本质 
上 是 一个 综合 的 优化 问题 具有 较高 的 
计算 代价 传统 的 特征 选取 方法 往往 是 独立 
计算 每 一个 特征 的 某一 得分 然后 根据 得分 
的 高低 选取 前 k 个 特征 这种 得分 一般 
用来 评价 某一 特征 区分 不同 聚 类 的 能力 
这样 的 方法 在 二分 问题 中 一般 有 不错 
的 结果 但是 在 多类 问题 中 很 有可能 会 
失败 基于 是否 知道 数据 的 lebal 信息 特征提取/nr 方法/n 
可以/c 分为/v 有/v 监督/vn 和无/nr 监督/vn 的/uj 方法/n 有 监督 
的 的 特征 提取 方法 往往 通过 特征 与 label 
之间 的 相关性 来 评估 特征 的 重要性 但是 label 
的 代价 是 高昂 的 很难 在 大 数据 集上 
进行 计算 因此 无 监督 的 特征 提取 方法 就 
显得 尤为 重要 无 监督 的 方法 只 利用 数据 
本身 所有 的 信息 而 无法 利用 数据 label 的 
信息 因此 要 得到 更好 的 结果 往往 更 困难 
特征 选取 是 一个 热门 的 研究 领域 近年来 很多 
相关 工作 2 3 4 被 提出 使得 特征 选取 
越来越 多 的 受到 关注 另外 一些 关于 数据 谱分析 
以及 L1 正则化 模型 的 研究 也 启发 了 特征 
选取 问题 一些 新的 工作 的 开展 并且 随着 计算机 
与 网络 的 发展 人们 越来越 多 的 关注 大 
规模 数据 的 处理 问题 使得 研究 与 应用 能够 
真正 衔接 在 一起 因此 无 监督 的 特征 选取 
方法 的 研究 显得 更加 重要 在 本 报告 中 
我们 重点 关注 无 监督 的 特征提取 方法 2 特征 
选取 相关 工作 特征 选取 方法 也 可以 分为 包装 
Wrapper 类 方法 与 过滤 Filter 类 方法 包装 类型 
的 方法 经常 使用 到 的 算法 是 聚 类 
有 很多 算法 同时 考虑 数据 的 特征提取 以及 聚 
类 为了 找到 一些 特征 能够 更好 的 提高 数据 
聚 类 的 性能 然而 包装 类型 的 算法 往往 
具有 较高 的 计算 代价 因此 很难 被 运用 到 
大 规模 的 数据挖掘 分析 工作 中 过滤 类型 的 
方法 相对 来说 比较 常见 也 很容易 得到 扩展 最大 
方差 Maximum Variance 的 方法 也许 是 其中 最 简单 
但 也 十分 有效 的 算法 该 方法 本质上 是 
将 数据 投影 到 最大 方差 的 方向 上 PCA 
6 也 使用 相同 的 思想 但是 它 使用 转换 
了 的 特征 而 不是 原始数据 特征 的 一个 子集 
虽然 最大 方差 的 标准 可以 有效 的 找到 特征 
来 表示 数据 但是 却 不能 很好 地 区分 数据 
Laplacian Score 算法 可以 有效 的 提取 出 那些 体现 
数据 潜在 流形 结构 的 特征 Fisher Score 算法 可以 
有效 的 区分 数据 它 给 最 有效 区 分数 
据点 不 同类 数 据点 尽可能 的 分开 而 同 
一类 的 数据 点 尽可能 的 聚在一起 的 特征 赋予 
最高 的 分值 2.1 降 维 方法 特征 选取 算法 
和降维/nr 算法 有着 非常 密切 的 联系 很多 算法 的 
设计 都 来源于 一些 经典 的 降 维 算法 下面 
简单 介绍 几种 常见 的 降 维 算法 特征 选取 
本质上 也 是 一种 降 维 Principal Component Analysis 6 
PCA 是 最 常用 的 线性 降 维 方法 它 
的 目标 是 通过 某种 线性 投影 将 高维 的 
数据 映 射到 低维 的 空间 中 表示 并 期望 
在所 投影 的 维度 上 数据 的 方差 最大 以此 
使用 较少 的 数据 维度 同时 保留住 较多 的 原 
数 据点 的 特性 具体 实现 步骤 如下 X 是 
矩阵 P 表示 维度 N 表示 数据 个数 Y 是 
矩阵 d 表示 降 维 后的/nr 维度 N 表示 数据 
个数 步骤 1 先 对 数据 进行 中心化 预处理 步骤 
2 取 协方差 矩阵 最大 的 d 个 特征值 对应 
的 特征向量 作为 投影 方向 W 步骤 3 降 维 
后 数据 由 P 维 降低 到 d 维 将 
PCA 的 通过 Kernel 的 方法 也 可以 运用 在 
非线性 降 维 中 即 KPCA 10 Laplacian Eigenmaps 8 
的 直观 思想 是 希望 相互间 有 关系 的 点 
如 在 一个 图中 相连 的 点 在 降 维 
后的/nr 空间 中 尽可能 的 靠近 Laplacian Eigenmaps 可以 反映 
出 数据 内在 的 流形 结构 算法 具体 实现 步骤 
步骤 1 构建 图 使用 某 一种 方法 来 将 
所有 的 点 构 建成 一个 图 例如 使用 KNN 
算法 将 每个 点 最近 的 K 个 点 连 
上边 K 是 一个 预先 设定 的 值 步骤 2 
确定 权重 确 定点 与 点 之间 的 权重 大小 
例如 选用 热核 函数 来 确定 如果 点 i 和点j/nr 
相连 那么 它们 关系 的 权重 设定 为 1 另外 
一种 可选 的 简化 设定 是 如果 点 i j 
相连 权 重为 1 否则 权 重为 0 步骤 3 
特征 映射 计算 拉普拉斯 矩阵 L 的 特征向量 与 特征值 
2 其中 D 是 对角 矩阵 使用 最小 的 m 
个 非零 特征值 对应 的 特征向量 作为 降 维 后的/nr 
结果 输出 Locally linear embedding 7 LLE 是 一种 非线性 
降 维 算法 它 能够 使 降 维 后的/nr 数据 
较好 地 保持 原有 流形 结构 使用 LLE 将 三维 
数据 映 射到 二维 之后 映射 后的/nr 数据 仍能 保持 
原有 的 数据 流形 说明 LLE 有效 地 保持 了 
数据 原有 的 流行 结构 但是 LLE 在 有些 情况下 
也 并不 适用 如果 数据分布 在整个 封闭 的 球 面上 
LLE 则 不能 将 它 映射 到 二 维空间 且 
不能 保持 原有 的 数据 流形 那么 我们 在 处理 
数据 中 首先 假设 数据 不是 分布 在 闭合 的 
球面 或者 椭球 面上 LLE 算法 认为 每 一个 数 
据点 都 可以 由 其 近邻 点 的 线性 加权 
组合 构造 得到 算法 的 主要 步骤 分为 三步 1 
寻找 每个 样本点 的 k 个 近邻 点 2 由 
每个 样本点 的 近邻 点 计算出 该 样本点 的 局部 
重建 权值 矩阵 3 由该 样本点 的 局部 重建 权值 
矩阵 和其/nr 近邻 点 计算出 该 样本点 的 输出 值 
具体 的 算法 流程 如下 步骤 1 算法 的 第一步 
是 计算 出 每个 样本点 的 k 个 近邻 点 
例如 采用 KNN 的 策略 把 相对于 所求 样本点 距离 
常用 欧氏距离 最近 的 k 个 样本 点 规定 为 
所求 样本点 的 个 近邻 点 k 是 一个 预先 
给 定值 步骤 2 计算出 样本点 的 局部 重建 权值 
矩阵 W 首先 定义 重构 误差 3 以及 局部 协方差 
矩阵 C 4 其中 表示 一个 特定 的 点 它 
的 的 K 个 近邻 点 用 表示 于是 目标函数 
最小化 5 其中 得到 6 步骤 3 将 所有 的 
样本 点映 射到 低维 空间 中 映射 条件 满足 如下 
所示 7 限制 条件 上式 可以 转化 为 8 其中 
要使 损失 函数值 达到 最小 则取 Y 为 M 的 
最小 m 个 非零 特征值 所 对应 的 特征向量 在 
处理 过程 中 将 M 的 特征值 从小到大 排列 第一 
个 特征值 几乎 接近 于零/nr 那么 舍去 第一个 特征值 通常 
取 第 2 到 m + 1间 的 特征值 所 
对应 的 特征向量 作为 输出 结果 本文 接下来 重点 介绍 
无 监督 多聚 类 特征 选取 5 第 3 章 
和无/nr 监督 特征 选取 PCA 1 第 4 章 两个 
新 提出 的 特征 选取 算法 3 无 监督 多聚 
类 特征 选取 特征 选取 的 一般 问题 是 不考虑 
数据 本身 的 结构 的 而 事实上 很多 数据 本身 
具有 多聚 类 结构特征 一个 好 的 特征 选取 方法 
应该 考虑 到 下面 两点 l 所 选取 的 特征 
应该 可以 最好 地 保持 数据 的 聚 类 结构特征 
最近 一些 研究 表明 一些 人 为 产生 的 数据 
存在 着 内在 的 数据 流行 结构 这个 因素 应该 
被 考虑 在 聚 类 算法 中 l 所 选取 
的 特征 应该 可以 覆盖 数据 中 所有 可能 的 
聚 类 因为 不同 的 特征 维度 在 区分 不同 
的 聚 类 时 具有 不同 的 效果 所以 如果 
选取 的 特征 仅仅 可以 区分 某 一些 聚 类 
而不能 区分 所有 的 聚 类 是 不 合适 的 
3.1 谱 嵌入 聚类分析 在 第 2 章中 讨论 过 
Laplacian Eigenmaps 假设 是 公式 2 的 特征向量 Y 的 
每 一行 是 一个 数 据点 的 降 维 表示 
其中 K 是 数据 的 内在 维度 每 一个 体现 
数据 在 该 维度 可以 理解 成 一个 主题 或者 
是 一个 概念 上 的 数据 分布 当 使用 聚类分析 
时 每 一个 可以 体现 数据 在 这一个 聚 类 
上 的 分布 因此 K 往往 可以 设定 成 数据 
聚 类 的 个数 3.2 学习 稀疏 系数 在 得到 
Y 之后 我们 可以 衡量 每一个 内在 维度 的 重要性 
也 就是 Y 的 每 一列 同时 可以 衡量 每 
一个 特征 区分 数据 聚 类 的 能力 给定 一个 
通过 最小化 拟合 错误 可以 找到 一个 相关 的 特征 
子集 如下 9 其中 是 一个 M 维度 的 向量 
X 是 N * M 维 矩阵 表示 L1 norm 
包含 了 用来 近似 时每/nr 一个 特征 的 系数 由于 
L1 norm 的 性质 当 足够 大 时 的 某些 
系数 将会 变为 0 因此 我们 可以 选 取 一些 
最 相关 的 特征 子集 公式 9 本质上 是 一个 
回归 问题 称作 LASSO 3.3 特征 选取 我们 需要 从M个/nr 
特征 的 数据 中 选取 d 个 特征 对于 一个 
含有 K 个 聚 类 的 数据 来说 我们 可以 
用 上面 提到 的 方法 来 计算 出 K 个 
系数 的 系数 向量 每 一个 的 非零 元素 个数 
为 d 对应 d 个 特征 显然 如果 把 所有 
选取 的 特征 都 用上 很 有可能 会 大于 d 
个 特征 于是 使用 下面 这种 简单 的 策略 来 
选 取出 d 个 特征 定义 每 一个 特征 的 
MCFS score 如下 10 将 所有 的 特征 根据 他们 
MCFS score 降序 排列 选取 前 d 个 特征 3.3 
计算 复杂度 分析 算法 的 计算 复杂度 从 下面 几点 
分析 l P 近邻 图 需要 步来/nr 构建 同时 需要 
步来/nr 找到 找到 每 一个 点 的 p 个 近邻 
l 在 构建 好 p 近邻 图 的 基础 上 
需要 计算公式 2 的 前 k 个 特征向量 利用 Lanczos 
algorithm 需要 时间 l 使用 LARs 来 解 公式 9 
限制 是 需要 时间 因此 我们 需要 的 时间 来 
计算 K 聚 类 的 问题 l 前 d 个 
特征 选取 需要 的 时间 考虑 以及 p 固定 为 
一个 常数 5 MCFS 算法 的 总的 复杂度 是 4 
无 监督 特征 选取 PCAPCA 是 一种 重要 的 线性 
降 维 算法 广泛 运用 在 社会 经济 生物 等 
各种 数据 上 在 第 2 章中 简单 讨论 过 
PCA 这里 我们 从 另外 一种 角度 来 描述 PCA 
4.1 子空间 选取 给定 一个 数据 矩阵 m 为 数据 
个数 n 为 数据 维度 令 是 数据 降 维 
后的/nr 维度 特征 数 并 假设 A 的 列 经过 
中心化 于是 PCA 返回 的 是 矩阵 A 的 前 
k 个 left singular vectors 一个 的 矩阵 并且 将 
数据 投影 到 列 向量 所在 的 子空间 中 令 
为子 空间 的 投影 矩阵 于是 最佳 投影 实际上 是 
在 所有 可能 的 K 维 子空间 中 最小化 11 
我们 希望 找到 一种 高效 的 关于 m 和n/nr 多项式 
时间内 无 监督 的 特征 选取 算法 能够 挑选 出 
k 个 特征 使得 PCA 只作 用在 这 k 个 
特征 上 的 结果 和 作用 全部 特征 上 的 
结果 很 接近 为了 定义 这个 接近 程度 令 C 
是 一个 矩阵 只/d 包含/v 有/v 从A中/nr 选取/v 出来/v 的/uj 
特征/n 通过 计算 下面 的 差 来 衡量 特征 选取 
的 好坏 12 这里 代表 投影 矩阵 投影 到 的 
列 空间 张成 的 K 维空间 中 表示 矩阵 的 
伪 逆 这 等价 于 Column Subset Selection Problem CSSP 
问题 在 现代 统计 数据 分析 中 从 高维 数据 
中 选取 出 原始 的 特征 feature selection 比 选 
取出 经过 操作 后的/nr 特征 feature extraction 在 很多 等 
方面 都更/nr 有优势 4.1 两 阶段 CSSP 在 这 一节 
中 介绍 一种 两 阶段 的 CSSP 具体 步骤 如下 
算法 1 Input 矩阵 A 整数 kOutput 矩阵 C 包含 
A 中的 k 列 1 . 起始 设置 l 计算 
A 的 前 k 个 right singular vectors 表示 成l/nr 
计算 采样 概率 对 每一个 jl 令 2 . 随机 
阶段 l 对于 第 j 列 的 概率 为 放缩 
因子 是 l 生成 采样 矩阵 放缩 矩阵 3 . 
确定 阶段 l 选取 矩阵 的 k 列 生成 采样 
矩阵 l 返回 A 的 k 列 也 就是 返回 
4 . 重复 第 2步 和第/nr 3步 40次 返回 使得 
最小 的 列 具体 来看 算法 1 先要 计算 A 
每 一列 的 概率 概率分布 依赖于 A 的 前 k 
个 right singular vectors 写成 13 由 上式 可以 知道 
只要 得到 就 可以 算出 而 本 算法 的 时间 
复杂度 主要 取决于 计算 所有 的 所 花费 的 时间 
在 随机 阶段 算法 1 随机 地 选择 中的 列 
作为 下 一 阶段 的 输入 对于 第 j 列 
的 概率 为 如果 第 j 列 被 选择 则 
放缩 因子 等于 因此 在 这个 阶段 的 末尾 我们 
将 得到 中的 列 以及 它们 相应 的 放缩 因子 
因为 随机 采样 一般 会 不等于 c 然而 很大 概率 
下 不会 比 c 大 很多 为了 当 便 表示 
选出 的 列 和 放缩 因子 我们 使用 下面 的 
形式 首先 定义 一个 的 采样 矩阵 初始值 为 空值 
当 第 j 列 被选 中时 就将 加到 中 然后 
定义 对角 放缩 矩阵 当 第 j 列 被 选取 
时 的 第 j 个 对角 元素 是 因此 随机 
阶段 的 输出 结果 就是 在 确定 阶段 从上/nr 一个 
阶段 挑选 出来 的 列中 选取 k 列 实际上 就是 
定义 了 一个 采样 矩阵 在 这个 阶段 之后 就 
得到 了 矩阵 作为 最后 的 结果 6 参考文献 1 
Boutsidis C . Mahoney M . W . Drineas P 
. Unsupervised feature selection for principal components analysis . In 
Proceeding of the 14th ACM I G K D D 
i n t e r n a t i o 
n a l conference on Knowledge discovery and data mining 
2008 61 69 . 2 Yu L . Ding C 
. Loscalzo . Stable feature selection via dense feature groups 
. In Proceeding of the 14th ACM I G K 
D D i n t e r n a t 
i o n a l conference on Knowledge discovery and 
data mining 2008 803 811 . 3 Forman G . 
Scholz M . Rajaram . Feature shaping for linear SVM 
classifiers . In Proceedings of the 15th ACM I G 
K D D i n t e r n a 
t i o n a l conference on Knowledge discovery 
and data mining 2009 299 308 . 4 Loscalzo . 
Yu L . Ding C . Consensus group stable feature 
selection . In Proceedings of the 15th ACM I G 
K D D i n t e r n a 
t i o n a l conference on Knowledge discovery 
and data mining 2009 567 576 . 5 D Cai 
C Zhang X He . Unsupervised Feature Selection for Multi 
Cluster Data . To be appeared in SIGKDD2010 . 6 
Smith L . I . A tutorial on principal components 
analysis . Cornell University USA . 2002 51 52 . 
7 Roweis . T . Saul L . K . 
Nonlinear dimensionality reduction by locally linear embedding . Science . 
2000 290 5500 2323 . 8 Belkin M . Niyogi 
P . Laplacian eigenmaps and spectral techniques for embedding and 
clustering . Advances in neural information processing systems . 2002 
1585 592 . 9 Tenenbaum J . B . Silva 
V . Langford J . C . A global geometric 
framework for nonlinear dimensionality reduction . Science . 2000 290 
5500 2319 . 10 Scholkopf B . Smola A . 
J . Muller K . R . Kernel principal component 
analysis . Lecture notes in computer science . 1997 1327583 
588 . 