声明 本 博客 整理 自 博友 @ zhouyong 计算 广告 
与 机器学习 － 技术 共享 平台 尊重 原创 欢迎 感兴趣 
的 博友 查看 原文 写 在前面 记得 在 Pattern Recognition 
And Machine Learning 一 书中 的 开头 有 讲到 概率论 
决策论 信息论 3个 重要 工具 贯穿 着 PRML 整本书 虽然 
看起来 令人生畏 确实 如此 其实 这 3大 理论 在 机器 
学习 的 每一种 技法 中 或多或少 都会 出现 其 身影 
不 局限 在 概率模型 PRML 书中 原话 This chapter also 
provides a self contained introduction to three important tools that 
will be used throughout the book namely probability theory decision 
theory and information theory . Although these might sound like 
daunting topics they are in fact straightforward and a clear 
understanding of them is essential if machine learning techniques are 
to be used to best effect in practical applications . 
怀念 好 学生 时代 那些年 － 书本 啃过 的 印记 
本章 主要 讨论 信息论 Information Theory 中 一个 非常 重要 
的 概念 信息熵 以及 概率模型 的 一个 学习 准则 最大熵 
理论 基本概念 熵 与 信息熵 如何 理解 熵 的 含义 
自然界 的 事物 如果 任其/nr 自身 发展 最终 都会 达到 
尽可能 的 平衡 或 互补 状态 举例 一盒 火柴 人为 
或 外力 有序 地 将其 摆放 在 一个 小 盒子 
里 如果 不 小心 火柴盒 打翻 了 火柴 会 散乱 
地 洒在 地板 上 此时 火柴 虽然 很乱 但 这是 
它 自身 发展 的 结果 上面 描述 的 其实 是 
自然界 的 熵 在 自然界 中 熵 可以 这样 表述 
熵 是 描述 事物 无序性 的 参数 熵 越大 则 
无序性 越强 那么 在 信息论 中 我们 用 熵 表示 
一个 随机 变量 的 不确定性 那么 如何 量化 信息 的 
不确定性 呢 信息熵 公式 定义 设 一次 随机事件 用 随机变量 
\ X \ 表示 它 可能会 有\/nr x _ 1 
x _ 2 x _ 3 \ cdots x _ 
m \ 共 \ m \ 个 不同 的 结果 
每个 结果 出现 的 概率 分别为 \ p _ 1 
p _ 2 p _ 3 \ cdots p _ 
m \ 那么 \ X \ 的 不 确定 度 
即 信息熵 为 $ $ H X = \ sum 
_ { i = 1 } ^ { m } 
p _ i \ cdot \ log _ { 2 
} \ frac { 1 } { p _ i 
} = \ sum _ { i = 1 } 
^ { m } p _ i \ cdot \ 
log _ { 2 } p _ i \ qquad 
ml . 1 . 2.1 $ $ ① . 信息熵 
的 物理 意义 一个 事件 用 随机变量 \ X \ 
表示 可能 的 变化 越多 那么 它 携带 的 信息量 
就 越大 与 变量 具体 取值 无关 只跟 值 的 
种类 多 少 以及 发生 概率 有关 ② . 系统 
熵 举例 对于 一个 分类 系统 来说 假设 类别 \ 
C \ 可能 的 取值 为 \ c _ 1 
c _ 2 \ cdots c _ k \ \ 
k \ 是 类别 总数 每 一个 类别 出现 的 
概率 分别 是 \ p c _ 1 p c 
_ 2 \ cdots p c _ k \ 此时 
分类 系统 的 熵 可以 表示 为 $ $ H 
C = \ sum _ { i = 1 } 
^ { k } p c _ i \ cdot 
\ log _ { 2 } p c _ i 
\ qquad n . ml . 1 . 2.1 $ 
$ 分类 系统 的 作用 就是 输 出 一个 特征向量 
文本 特征 ID 特征 属性 特征 等 属于 哪个 类别 
的 值 而 这个 值 可能 是 \ c _ 
1 c _ 2 \ cdots c _ k \ 
因此 这个 值 所 携带 的 信息 量 就是 公式 
\ n . ml . 1 . 2.1 \ 这么 
多 条件 熵 设 \ X Y \ 为 两个 
随机变量 在 \ X \ 发生 的 前提 下 \ 
Y \ 发生 所 新 带来 的 熵 定义 为 
\ Y \ 的 条件 熵 Conditional Entropy 用 \ 
H Y | X \ 表示 计算 公式 如下 $ 
$ H Y | X = \ sum _ { 
x _ i y _ j } ^ { m 
n } p x _ i y _ j \ 
cdot log _ 2 p y _ j | x 
_ i \ qquad ml . 1 . 2.2 $ 
$ 其 物理 含义 是 当 变量 \ X \ 
已知 时 变量 \ Y \ 的 平均 不 确定 
性 是 多少 公式 \ ml . 1 . 2.2 
\ 推导 如下 假设 变量 \ X \ 取值 有\/nr 
m \ 个 那么 \ H Y | X = 
x _ i \ 是 指 变量 \ X \ 
被 固定 为 值 \ x _ i \ 时的/nr 
条件 熵 \ H Y | X \ 时指/nr 变量 
\ X \ 被 固 定时 的 条件 熵 那么 
二者 之间 的 关系 时 $ $ \ begin { 
align } H Y | X & = p x 
_ 1 \ cdot H Y | X = x 
_ 1 + \ cdots + p x _ m 
\ cdot H Y | X = x _ m 
\ \ & = \ sum _ { i = 
1 } ^ { m } p x _ i 
\ cdot H Y | X = x _ i 
\ end { align } \ quad n . ml 
. 1 . 2.2 $ $ 根据 公式 \ n 
. ml . 1 . 2.2 \ 继续 推导 \ 
Y \ 的 条件 熵 $ $ \ begin { 
align } H Y | X & = \ sum 
_ { i = 1 } ^ { m } 
p x _ i \ cdot H Y | X 
= x _ i \ \ & = \ sum 
_ { i = 1 } ^ { m } 
p x _ i \ cdot \ left \ sum 
_ { j = i } ^ { n } 
p y _ j | x _ i \ cdot 
log _ 2 p y _ j | x _ 
i \ right \ \ & = \ sum _ 
{ i = 1 } ^ { m } \ 
sum _ { j = 1 } ^ { n 
} p y _ j x _ i \ cdot 
log _ 2 p y _ j | x _ 
i \ \ & = \ sum _ { x 
_ i y _ j } ^ { m n 
} p x _ i y _ j \ cdot 
log _ 2 p y _ j | x _ 
i \ end { align } \ qquad \ qquad 
n . ml . 1 . 2.3 $ $ 注 
条件 熵 里面 是 联合 概率分布 累加 公式 \ n 
. ml . 1 . 2.3 \ 推导 过程 可 
参考 第 3 章 深入浅出 ML 之 Based Tree Classification 
Family 中 3 . 1.2 节 条件 熵 部分 联合 
熵 一个 随机 变量 的 不确定性 可以用 熵 来 表示 
这一 概念 可以 直接 推广 到 多个 随机变量 联合 熵 
计算 Joint Entropy 设 \ X Y \ 为 两个 
随机变量 \ p x _ i y _ j \ 
表示 其 联合 概率 用 \ H XY \ 表示 
联合 熵 计算公式 为 $ $ H XY = \ 
sum _ { i = 1 } ^ { m 
} \ sum _ { j = 1 } ^ 
{ n } p x _ i y _ j 
\ cdot log _ { 2 } p x _ 
i y _ j \ qquad ml . 1 . 
2.3 $ $ 条件 熵 联合 熵 熵 之间 的 
关系 $ $ H Y | X = H X 
Y H X \ qquad \ qquad n . ml 
. 1 . 2.4 $ $ 公式 推导 如下 $ 
$ \ begin { align } H X Y H 
X & = \ sum _ { i = 1 
} ^ { m } \ sum _ { j 
= 1 } ^ { n } p x _ 
i y _ j \ cdot log _ 2 p 
x _ i y _ j + \ sum _ 
{ i = 1 } ^ { m } \ 
underline { p x _ i } \ cdot log 
_ 2 p x _ i \ \ & = 
\ sum _ { i = 1 } ^ { 
m } \ sum _ { j = 1 } 
^ { n } p x _ i y _ 
j \ cdot log _ 2 p x _ i 
y _ j + \ sum _ { i = 
1 } ^ { m } \ underline { \ 
left \ sum _ { j = 1 } ^ 
{ n } p x _ i y _ j 
\ right } \ cdot log _ 2 p x 
_ i \ \ & = \ sum _ { 
i = 1 } ^ { m } \ sum 
_ { j = 1 } ^ { n } 
p x _ i y _ j \ cdot \ 
left log _ 2 p x _ i y _ 
j log _ 2 p x _ i \ right 
\ \ & = \ sum _ { i = 
1 } ^ { m } \ sum _ { 
j = 1 } ^ { n } p x 
_ i y _ j \ cdot log _ 2 
p y _ j | x _ i \ \ 
& = H Y | X \ qquad \ qquad 
\ qquad \ qquad \ qquad \ qquad n . 
ml . 1 . 2.5 \ end { align } 
$ $ 联合 熵 特点 \ H XY \ geq 
H X \ 联合 系统 的 熵 不小于 子系统 的 
熵 即 增加 一个 新 系统 不会 减少 不确定性 \ 
H XY \ leq H X + H Y \ 
子系统 可加 性 \ H XY \ geq 0 \ 
非 负性 相对 熵 KL 距离 相对 熵 概念 相对 
熵 又 称为 交叉 熵 或 KL 距离 是 Kullback 
Leibler 散度 Kullback Leibler Divergence 的 简称 它 主要 用于 
衡量 相同 事件 空间 里 的 两个 概率分布 的 差异 
简单 介绍 其 背景 根据 香农 的 信息 论 给定 
一个 字符集 的 概率分布 我们 可以 设计 一种 编码 使得 
表示 该 字符集 组成 的 每个 字符串 平均 需要 的 
比特 数 最少 比如 Huffman 编码 假设 字符集 是 \ 
X \ 对 \ x \ in X \ 其 
出现 概率 为 \ P x \ 那么 其 最优 
编码 平均 需要 的 比特 数 即 每一 个字符 需要 
的 比特 数 等于 这个 字符集 的 熵 公式 见 
\ ml . 1 . 2.1 \ 即 最优 编码 
时 字符 \ x \ 的 编码 长度 等于 \ 
log _ 2 { \ frac { 1 } { 
P x } } \ 在 同样 的 字符 集上 
假设 存在 另一个 概率分布 \ Q x \ 如果 根据 
\ Q x \ 分布 进行 编码 那么 表示 这些 
字符 就会 比 理想 情况 多 用 一些 比特 数 
而 KL 距离 就是 用来 衡量 这种 情况 下 平均 
每 个字符 多用 的 比特 数 可 用来 度量 两个 
分布 的 距离 KL 距离 计算公式 这里 用 \ D 
P | | Q \ 表示 KL 距离 计算 公式 
如下 $ $ D P | | Q = \ 
sum _ { x \ in X } P x 
\ cdot log _ 2 \ frac { P x 
} { Q x } \ qquad \ qquad ml 
. 1 . 2.4 $ $ 从 公式 \ ml 
. 1 . 2.4 \ 可以 看出 当 两个 概率分布 
完全相同 时 KL 距离 为 0 概率分布 \ P x 
\ 的 信息 熵 如 公式 \ ml . 1 
. 2.1 \ 所示 说 的 是 如果 按照 概率分布 
\ P x \ 编码 时 描述 这个 随机事件 至少 
需要 多少 比特 编码 因此 KL 距离 的 物理 意义 
可以 这样 表达 在 相同 的 事件 空间 里 概率分布 
为 \ P x \ 的 事件 空间 若 用 
概率分布 \ Q x \ 编码 时 平均 每个 基本 
事件 符号 编码 长度 增加 了 多少 比特 数 通过 
信息熵 可知 不 存在 其它 比 按照 随机事件 本身 概率分布 
更好 的 编码 方式 了 所以 \ D P | 
| Q \ 始终 是 大于 等于 0 的 虽然 
KL 被称为 距离 但是 其 不满足 距离 定义 的 3个 
条件 1 非 负性 2 对称性 不满足 3 三角 不等式 
不满足 KL 距离 示例 假设有 一 个字符 发射器 随机 发出 
0 和1/nr 两种 字符 真实 发出 的 概率 分布 为 
\ A \ 现在 通过 样本 观察 得到 概率分布 \ 
B \ 和\/nr C \ 各个 分布 的 具体 情况 
如下 1 . \ A 0 = 1/2 A 1 
= 1/2 \ 2 . \ B 0 = 1/4 
B 1 = 3/4 \ 3 . \ C 0 
= 1/8 C 1 = 7/8 \ 那么 可以 计算 
出 相对 熵 如下 \ D A | | B 
= 1/2 \ cdot log _ 2 \ frac { 
1/2 } { 1/4 } + 1/2 \ cdot log 
_ 2 \ frac { 1/2 } { 3/4 } 
= 1/2 \ cdot log _ 2 4/3 \ \ 
D A | | C = 1/2 \ cdot log 
_ 2 \ frac { 1/2 } { 1/8 } 
+ 1/2 \ cdot log _ 2 \ frac { 
1/2 } { 7/8 } = 1/2 \ cdot log 
_ 2 16/7 \ 可以 看到 用 \ B 和C\/nr 
两种 方式 进行 编码 其 结果 都是 的 平均 编码 
长度 增加 了 同时 也 能 发现 按照 概率分布 \ 
B \ 进行 编码 要比 按照 \ C \ 进行 
编码 平均 每个 符号 增加 的 比特 数目 要 少 
从 分布 熵 也 可以 看出 实际上 \ B \ 
要比 \ C \ 更 接近 实际 分布 如果 实际 
分布 为 \ C \ 而用 \ A \ 分布 
来 编码 这 个字符 发射器 的 每个 字符 同样 可以 
得到 \ D C | | A = 1/8 \ 
cdot log _ 2 \ frac { 1/8 } { 
1/2 } + 7/8 \ cdot log _ 2 \ 
frac { 7/8 } { 1/2 } = 7/8 \ 
log _ 2 { 7 } 2 0 \ 从 
示例 中 我们 可以 得出 结论 对于 一个 信息源 进行 
编码 按照 其 本身 的 概率 分布 进行 编码 每 
个字符 的 平均 比特 数 最少 这 也是 信息熵 的 
概念 用于 衡量 信息源 本身 的 不确定性 此外 可以 看出 
KL 距离 不 满足 对称性 即 \ D P | 
| Q \ 不一定 等于 \ D Q | | 
P \ 相对 熵 应用 场景 推荐 系统 － 物品 
之间 相似 度 在 使用 LDA Latent Dirichlet Allocation 计算 
物品 之间 的 内容 相似 度 时 我们 可以 先 
计算出 物品 在 Topic 上 的 分布 然后 利用 两个 
物品 的 Topic 话题 分布 计算 物品 的 相似 度 
比如 如果 两个 物品 的 Topic 分布 相似 处在 同一 
个 事件 空间 则 认为 两个 物品 具有 较高 的 
相似 度 反之 则 认为 两个 物品 的 相似 度 
较低 这种 Topic 分布 的 相似 度 可以 利用 KL 
散度 来 计算 $ $ D P | | Q 
= \ sum _ { i \ in X } 
p x _ i \ cdot log _ 2 { 
\ frac { p x _ i } { q 
x _ i } } \ qquad n . ml 
. 1 . 2.6 $ $ 其中 \ p \ 
和\/nr q \ 是 两个 分布 \ X \ 为 
话题 集合 \ x _ i \ 表示 第 \ 
i \ 个 话题 KL 散度 越大 说明 分布 的 
相似 度 越低 互信息 如果 说 相对 熵 KL 距离 
衡量 的 是 相同 事件 空间 里 的 两个 事件 
的 相似 度 大小 那么 互信息 通常 用来 衡量 不同 
事件 空间 里 的 两个 信息 随机事件 变量 的 相关性 
大小 互信息 计算公式 设 \ X \ 和\/nr Y \ 
为 两个 离散 随机变量 事件 \ Y = y _ 
j \ 的 出现 对于 事件 \ X = x 
_ i \ 的 出现 的 互 信息量 \ I 
x _ i y _ j \ 定义 为 $ 
$ I x _ i y _ j = log 
_ 2 { \ frac { p x _ i 
| y _ j } { p x _ i 
} } = log _ 2 { \ frac { 
p x _ i y _ j } { p 
x _ i p y _ j } } \ 
qquad ml . 1 . 2.5 $ $ 对于 事件 
\ X \ 和\/nr Y \ 来说 它们 之间 的 
互信息 用 \ I X Y \ 表示 公式 为 
$ $ I X Y = \ sum _ { 
i = 1 } ^ { m } \ sum 
_ { j = 1 } ^ { n } 
p x _ i y _ j \ cdot log 
_ 2 { \ frac { p x _ i 
y _ j } { p x _ i p 
y _ j } } \ qquad ml . 1 
. 2.6 $ $ 公式 解释 互信息 就是 随机事件 \ 
X \ 的 不确定性 即 熵 \ H X \ 
以及 在 给定 随机变量 \ Y \ 条件 下 的 
不确定性 即 条件 熵 \ H X | Y \ 
之间 的 差异 即 $ $ I X Y = 
H X H X | Y \ qquad n . 
ml . 1 . 2.7 $ $ 互信息 与 决策树 
中 的 信息 增益 等价 互信息 \ \ L o 
n g l e f t r i g h 
t a r r o w \ 信息 增益 . 
所谓 两个 事件 相关性 的 量化 度量 就是 在 了解 
了 其中 一个 事件 \ Y \ 的 前提 下 
对 消除 另 一个 事件 \ X \ 不确定性 所 
提供 的 信息量 互信息 与 其它 熵 之间 的 关系 
\ H X | Y = H X Y H 
Y \ \ I X Y = H X + 
H Y H X Y \ \ I X Y 
= H X H X | Y \ \ I 
X X = H X \ 互信息 应用 场景 机器学习 
－ feature label 之间 相关性 计算 随机事件 之间 不同 的 
事件 空间 的 相关性 最大熵 模型 Maximum Entropy Model 最大熵 
原理 在 介绍 最大熵 模型 之前 我们 先 了解 一下 
最大熵 原理 因为 最大熵 原理 是 选择 最优 概率模型 的 
一个 准则 最大熵 原理 在 概率模型 空间 集合 中 在 
满足 给定 约束 条件 的 前提 下 使 信息熵 最大化 
得到 的 概率模型 就是 最优 的 模型 理解 最大熵 原理 
通常用 约束条件 来 确定 概率模型 的 集合 假设 离散 随机变量 
\ X \ 的 概率分布 是 \ P X \ 
其 信息熵 可用 公式 \ ml . 1 . 2.1 
\ 表示 并且 熵 满足 以下 不等式 $ $ 0 
\ leq H X \ leq log _ 2 | 
X | \ qquad \ quad ml . 1 . 
2.7 $ $ 其中 \ | X | \ 是 
\ X \ 的 取值 个数 当且仅当 \ X \ 
的 分布 是 均匀分布 时 右边 的 等号 才 成立 
也 就是说 当 \ X \ 服从 均匀分布 时 熵 
最大 根据 最大熵 原理 学习 概率模型 坚持 的 原则 首先 
必须 满足 已有 的 事实 即 约束条件 但对 不 确定 
的 部分 不 做 任何 假设 坚持 无偏 原则 最大熵 
原理 通过 熵 的 最大化 来 表示 等 可能性 最大熵 
原理 举例 本 示例 来自 统计 学习 方法 第 6 
章－/nr 李航 老师 问题 假设 随机变量 \ X \ 有 
5个 取值 \ \ { A B C D E 
\ } \ 要 估计 各个 取值 的 概率 \ 
P A P B P C P D P E 
\ 首先 这些 概率 只 满足 以下 约束条件 $ $ 
P A + P B + P C + P 
D + P E = 1 \ qquad exp . 
ml . 1 . 2.1 $ $ 满足 这个 约束 
条件 的 概率 分布 有 无穷 多个 但是 在 没有 
任何 其它 信息 的 情况 下 根据 最大熵 原理 和 
无偏 原则 选择 熵 最大 时 对应 的 概率分布 即 
各个 取值 概率 相等 是 一个 不错 的 概率 估计 
方法 即有 $ $ P A = P B = 
P C = P D = P E = \ 
frac { 1 } { 5 } \ qquad exp 
. ml . 1 . 2.2 $ $ 等 概率 
坚持 了 最大熵 的 无偏 原则 因为 没有 更多 信息 
此种 判断 是 合理 的 现在 从 先验 知识 中 
得到 一些 信息 \ A 和B\/nr 的 概率值 之和 满足 
以下 条件 $ $ P A + P B = 
\ frac { 3 } { 10 } \ qquad 
exp . ml . 1 . 2.3 $ $ 同样 
的 满足 公式 \ exp . ml . 1 . 
2.1 和 exp . ml . 1 . 2.3 \ 
两个 约束 条件 的 概率分布 仍有 无穷 多个 在 缺少 
其它 信息 的 情况 下 坚持 无偏 原则 得到 $ 
$ \ begin { align } P A = P 
B = \ frac { 3 } { 20 } 
\ qquad exp . ml . 1 . 2.4 \ 
\ P C = P D = P E = 
\ frac { 7 } { 30 } \ qquad 
exp . ml . 1 . 2.5 \ end { 
align } $ $ 还 可以 继续 按照 满足 约束 
条件下 的 求 等 概率 的 方法 估计 概率分布 以上 
概率模型 学习 的 方法 正是 遵循 了 最大熵 原理 最大熵 
模型 定义 最大熵 原理 是 统计 学习 的 一般 原理 
将 它 应用 到 分类 问题 中 即 得到 最大熵 
模型 最大熵 模型 引入 训练 数据集 \ D = \ 
{ x ^ { 1 } y ^ { 1 
} x ^ { 2 } y ^ { 2 
} \ cdots x ^ { N } y ^ 
{ N } \ } \ 学习 的 目标 是 
用 最大熵 原理 选择 最优 的 分类 模型 假设 分类 
模型 是 一个 条件 概率分布 \ P y | x 
x \ in X \ subseteq R ^ n \ 
表示 输入 特征向量 \ y \ in Y \ \ 
X \ 和\/nr Y \ 分别 是 输入 特征向量 和 
输出 标签 的 集合 这个 模型 表示 的 是 对于 
给定 的 输入 \ x \ 以 条件概率 \ P 
y | x \ 计算 得到 标签 \ y \ 
首先 考虑 模型 应 满足 的 条件 给定 训练 集 
可以 计算 得到 经验 联合 分布 \ P x y 
\ 和 边缘 分布 \ P x \ 的 经验 
分布 分别 以 \ \ tilde { P } x 
y \ 和\/nr \ tilde { P } x \ 
表示 即 $ $ \ begin { align } \ 
tilde { P } x = \ tilde { x 
} y = \ tilde { y } & = 
\ frac { freq x = \ tilde { x 
} y = \ tilde { y } } { 
N } \ qquad 1 \ \ \ tilde { 
P } x = \ tilde { x } & 
= \ frac { freq x = \ tilde { 
x } } { N } \ qquad \ qquad 
\ 2 \ end { align } \ qquad ml 
. 1 . 2.8 $ $ 其中 \ freq x 
= \ tilde { x } y = \ tilde 
{ y } \ 表示 训练 集中 样本 \ \ 
tilde { x } \ tilde { y } \ 
出现 的 频数 \ freq \ tilde { x } 
\ 表示 训练 集中 输入 \ \ tilde { x 
} \ 向量 出现 的 频数 \ N \ 表示 
训练 集 容量 特征函数 Feature Function 定义 特征函数 \ f 
x y \ 用于 描述 输入 \ x \ 和 
输出 \ y \ 之间 满足 的 某 一种 事实 
$ $ f x y = \ begin { cases 
} \ displaystyle 1 & x 与 y 满足 某一 
事实 \ \ 0 & 其它 \ end { cases 
} \ qquad \ qquad ml . 1 . 2.9 
$ $ 这 是 一个 二 值 函数 也 可以 
是 任意 实值函数 当 \ x \ 与 \ y 
\ 满足 这个 事实 时 取值 为 1 否 则为 
0 . ① . 特征函数 \ f x y \ 
关于 经验 分布 \ \ tilde { P } x 
y \ 的 期望值 用 \ E _ { \ 
tilde { P } } f \ 表示 如下 $ 
$ E _ { \ tilde { P } } 
= \ sum _ { x y } \ tilde 
{ P } x y \ cdot f x y 
\ qquad \ qquad n . ml . 1 . 
2.8 $ $ ② . 特征函数 \ f x y 
\ 关于 模型 \ P y | x \ 与 
经验 分布 \ \ tilde { P } x \ 
的 期望值 用 \ E _ P f \ 表示 
如下 $ $ E _ P f = \ sum 
_ { x y } \ tilde { P } 
x \ cdot P y | x \ cdot f 
x y \ qquad n . ml . 1 . 
2.9 $ $ ③ . 如果 模型 能够 获取 训练 
数据 中 足够 的 信息 那么 就 可以 假设 这两个 
期望值 相等 即 $ $ \ sum _ { x 
y } \ tilde { P } x y \ 
cdot f x y ＝ \ sum _ { x 
y } \ tilde { P } x \ cdot 
P y | x \ cdot f x y \ 
qquad n . ml . 1 . 2.10 $ $ 
注 公式 \ n . ml . 1 . 2.10 
\ 是 频率 学派 － 点估计 求 参数 套路 之所以 
假设 相等 是 因为 有\/nr p x y = p 
y | x \ cdot p x \ 我们 将 
公式 \ n . ml . 1 . 2.10 \ 
作为 概率模型 学习 的 约束条件 假如有 \ n \ 个 
特征函数 \ f _ { i } x y i 
= 1 2 \ cdots n \ 那么 就有 \ 
n \ 个 约束条件 最大熵 模型 定义 假设 满足 所有 
约束 条件 的 模型 集 合为 $ $ \ mathcal 
{ C } = \ { P \ in \ 
mathcal { P } | E _ { P } 
f _ i = E _ { \ tilde { 
P } } f _ i i = 1 2 
\ cdots n \ } \ qquad ml . 1 
. 2.10 $ $ 定义 在 条件 概率分布 \ P 
y | x \ 上 的 条件 熵 为 $ 
$ H P = \ sum _ { x y 
} \ tilde { P } x \ cdot P 
y | x \ cdot \ log { P y 
| x } \ qquad ml . 1 . 2.11 
$ $ 模型 集合 \ \ mathcal { C } 
\ 中 条件 熵 \ H P \ 最大 的 
模型 称为 最大熵 模型 注 最大熵 模型 中 \ \ 
log \ 是 指 以 \ e \ 为 底 
的 对数 与 信息熵 公式 中 以 2 为 底 
不同 本文 如无 特殊 说明 \ \ log \ 均指 
自然对数 最大熵 模型 参数 学习 最大熵 模型 学习 过程 即为 
求解 最大熵 模型 的 过程 最大熵 模型 的 学习 问题 
可以 表示 为 带有 约束 的 最优 化 问题 示例 
学习 最大熵 原理 示例 中的 最大熵 模型 为了 简便 这里 
分别 以 \ y _ 1 y _ 2 y 
_ 3 y _ 4 y _ 5 \ 表示 
\ A B C D 和E\/nr 最大熵 模型 学习 的 
最优 化 问题 可以 表示 为 $ $ \ begin 
{ align } & min \ quad H P = 
\ sum _ { i = 1 } ^ { 
5 } P y _ i \ cdot log { 
P y _ i } \ \ & s . 
t . \ quad P y _ 1 + P 
y _ 2 = \ tilde { P } y 
_ 1 + \ tilde { P } y _ 
2 = \ frac { 3 } { 10 } 
\ \ & \ qquad \ sum _ { i 
= 1 } ^ { 5 } P y _ 
i = \ sum _ { i = 1 } 
^ { 5 } \ tilde { P } y 
_ i = 1 \ end { align } \ 
qquad \ quad exp . ml . 1 . 2.5 
$ $ 提示 这 里面 没有 特征 \ x \ 
和 特征函数 \ f _ i x y \ 的 
约束 将带 约束 优化 问题 转化 为 无约束 优化 问题 
引入 拉格朗日 乘子 \ w _ 0 w _ 1 
\ 定义 朗 格朗日 函数 $ $ L P w 
= \ sum _ { i = 1 } ^ 
{ 5 } P y _ i log { P 
y _ i } + w _ 1 \ left 
P y _ 1 + P y _ 2 \ 
frac { 3 } { 10 } \ right + 
w _ 0 \ left \ sum _ { i 
= 1 } ^ { 5 } P y _ 
i 1 \ right \ exp . ml . 1 
. 2.6 $ $ 根据 拉格朗日 对偶性 可以 通过 求解 
对偶 最优化 问题 得到 原始 最优化 问题 的 解 所以 
求解 对偶 问题 \ \ max _ { w } 
\ min _ { P } L P w \ 
求解 过程 如下 首先 求解 \ L P w \ 
关于 \ P \ 的 极小 化 问题 为此 固定 
\ w _ 0 w _ 1 \ 求 偏 
导数 $ $ \ begin { align } & \ 
frac { \ partial L P w } { \ 
partial P y _ 1 } = 1 + log 
_ 2 P y _ 1 + w _ 1 
+ w _ 0 \ \ & \ frac { 
\ partial L P w } { \ partial P 
y _ 2 } = 1 + log _ 2 
P y _ 2 + w _ 1 + w 
_ 0 \ \ & \ frac { \ partial 
L P w } { \ partial P y _ 
3 } = 1 + log _ 2 P y 
_ 3 + w _ 0 \ \ & \ 
frac { \ partial L P w } { \ 
partial P y _ 4 } = 1 + log 
_ 2 P y _ 4 + w _ 0 
\ \ & \ frac { \ partial L P 
w } { \ partial P y _ 5 } 
= 1 + log _ 2 P y _ 5 
+ w _ 0 \ \ \ end { align 
} $ $ 令 各 偏 导数 等于 0 可 
解得 $ $ \ begin { align } & P 
y _ 1 = P y _ 2 = e 
^ { w _ 1 w _ 0 1 } 
\ \ & P y _ 3 = P y 
_ 4 = P y _ 5 = e ^ 
{ w _ 0 1 } \ end { align 
} $ $ 于是 极小 化 结果 为 $ $ 
\ min _ { P } \ L P w 
= L P _ w w = 2 e ^ 
{ w _ 1 w _ 0 1 } 3 
e ^ { w _ 0 1 } \ frac 
{ 3 } { 10 } w _ 1 w 
_ 0 $ $ 下面 再 求 解对 偶函数 \ 
L P _ w w \ 关于 \ w \ 
的 极大 化 问题 $ $ \ max _ { 
w } \ L P _ w w = 2 
e ^ { w _ 1 w _ 0 1 
} 3 e ^ { w _ 0 1 } 
\ frac { 3 } { 10 } w _ 
1 w _ 0 \ qquad exp . ml . 
1 . 2.7 $ $ 分别 求 \ L P 
_ w w \ 对 \ w _ 0 w 
_ 1 \ 的 偏 导数 并 令 其为 0 
得到 $ $ \ begin { align } & e 
^ { w _ 1 w _ 0 1 } 
= \ frac { 3 } { 20 } \ 
\ & e ^ { w _ 0 1 } 
= \ frac { 7 } { 30 } \ 
end { align } $ $ 于是 得到 所求 的 
概率 分布 为 $ $ \ begin { align } 
& P y _ 1 = P y _ 2 
= \ frac { 3 } { 20 } \ 
\ & P y _ 3 = P y _ 
4 = P y _ 5 = \ frac { 
7 } { 30 } \ end { align } 
$ $ 最大熵 模型 学习 一般 流程 对于 给定 的 
训练 \ D = \ { x ^ { 1 
} y ^ { 1 } x ^ { 2 
} y ^ { 2 } \ cdots x ^ 
{ N } y ^ { N } \ } 
\ 以及 特征函数 \ f _ i x y i 
= 1 2 \ cdots n \ 最大熵 模型 的 
学习 等价 于带/nr 约束 的 最优 化 问题 $ $ 
\ begin { align } & \ max _ { 
P \ in \ mathcal { C } } \ 
quad H P = \ sum _ { x y 
} \ tilde { P } x \ cdot P 
y | x \ cdot log P y | x 
\ \ & s . t . \ quad E 
_ P f _ i = E _ { \ 
tilde { P } } f _ i \ i 
= 1 2 \ cdots n \ \ & \ 
qquad \ sum _ { y } P y | 
x = 1 \ end { align } \ qquad 
\ quad ml . 1 . 2.12 $ $ 按照 
最 优化 问题 的 习惯 思路 将 求 最大值 问题 
改写 为求 等价 的 最小值 问题 即 $ $ \ 
begin { align } & \ min _ { P 
\ in \ mathcal { C } } \ quad 
H P = \ sum _ { x y } 
\ tilde { P } x \ cdot P y 
| x \ cdot log P y | x \ 
\ & s . t . \ quad E _ 
P f _ i E _ { \ tilde { 
P } } f _ i = 0 \ i 
= 1 2 \ cdots n \ \ & \ 
qquad \ sum _ { y } P y | 
x = 1 \ end { align } \ qquad 
\ quad ml . 1 . 2.13 $ $ 求解 
约束 最优化 问题 \ ml . 1 . 2.13 \ 
所 得出 的 解 就是 最大熵 模型 学习 的 解 
将 约束 最 优化 的 原始 问题 转换 为 无约束 
最 优化 的 对偶 问题 具体 推导 过程 如下 首先 
引入 拉格朗日 乘子 \ w _ 0 w _ 1 
\ cdots w _ n \ 定义 拉格朗日 函数 \ 
L P w \ 表达式 为 $ $ \ begin 
{ align } L P w & = H P 
+ w _ 0 \ cdot \ left 1 \ 
sum _ { y } P y | x \ 
right + \ sum _ { i = 1 } 
^ { n } w _ i \ cdot \ 
left E _ { \ tilde { P } } 
f _ i E _ P f _ i \ 
right \ \ & = \ sum _ { x 
y } \ tilde { P } x \ cdot 
P y | x \ cdot log { P y 
| x } + w _ 0 \ cdot \ 
left 1 \ sum _ { y } P y 
| x \ right \ \ & \ qquad + 
\ sum _ { i = 1 } ^ { 
n } w _ i \ cdot \ left \ 
sum _ { x y } \ tilde { P 
} x y \ cdot f _ i x y 
\ sum _ { x . y } \ tilde 
{ P } x \ cdot P y | x 
\ cdot f _ i x y \ right \ 
end { align } \ quad ml . 1 . 
2.14 $ $ 最 优化 的 原始 问题是 $ $ 
\ min _ { P \ in \ mathcal { 
C } } \ max _ { w } L 
P w \ qquad \ qquad ml . 1 . 
2.15 $ $ 对偶 问题是 $ $ \ max _ 
{ w } \ min _ { P \ in 
\ mathcal { C } } L P w \ 
qquad \ qquad ml . 1 . 2.16 $ $ 
通俗 的 讲 由 _ 最小 最大 问题 _ 转化 
为 _ 最大 最 小问题 _ 由于 最大熵 模型 对应 
的 朗 格朗日 函数 \ L P w \ 是 
参数 \ P \ 的 凸函数 所以 原始 问题 \ 
ml . 1 . 2.15 \ 的 解与/nr 对偶 问题 
\ ml . 1 . 2.16 \ 的 解是/nr 等价 
的 因此 可以 通过 求解 对偶 问题 来 得到 原始 
问题 的 解 其次 求 对偶 问题 \ ml . 
1 . 2.16 \ 内部 的 极小 化 问题 \ 
\ min _ { P \ in \ mathcal { 
C } } L P w \ \ \ min 
_ { P \ in \ mathcal { C } 
} L P w \ 是 乘子 \ w \ 
的 函数 将其 记作 $ $ \ Psi w = 
\ min _ { P \ in \ mathcal { 
C } } L P w = L P _ 
w w \ qquad ml . 1 . 2.17 $ 
$ \ \ Psi w \ 称 为对 偶函数 \ 
Latex \ Psi \ = \ Psi 将 其解 记作 
$ $ P _ w = arg \ min _ 
{ P \ in \ mathcal { C } } 
L P w = P _ w y | x 
\ qquad n . ml . 1 . 2.11 $ 
$ 具体 地 固定 \ w _ i \ 求 
\ L P w \ 对 \ P y | 
x \ 的 偏 导数 $ $ \ begin { 
align } \ frac { \ partial L P w 
} { \ partial P y | x } & 
= \ sum _ { x y } \ tilde 
{ P } x \ cdot \ left logP y 
| x + 1 \ right \ sum _ { 
y } w _ 0 \ sum _ { x 
y } \ left \ tilde { P } x 
\ cdot \ sum _ { i = 1 } 
^ { n } w _ i \ cdot f 
_ i x y \ right \ \ & = 
\ sum _ { x y } \ tilde { 
P } x \ cdot \ left logP y | 
x + 1 w _ 0 \ sum _ { 
i = 1 } ^ { n } w _ 
i \ cdot f _ i x y \ right 
\ qquad n . ml . 1 . 2.12 \ 
end { align } $ $ 令 偏 导数 等于 
0 在 \ \ tilde { P } x 0 
\ 的 情况 下 求得 $ $ P y | 
x = \ exp { \ left \ sum _ 
{ i = 1 } ^ { n } w 
_ i \ cdot f _ i x y + 
w _ 0 1 \ right } = \ frac 
{ \ exp \ left \ sum _ { i 
= 1 } ^ { n } w _ i 
\ cdot f _ i x y \ right } 
{ \ exp 1 w _ 0 } \ quad 
n . ml . 1 . 2.13 $ $ 由于 
\ \ sum _ { y } P y | 
x = 1 \ 可得 $ $ P _ w 
y | x = \ frac { 1 } { 
Z _ w x } \ exp \ left \ 
sum _ { i = 1 } ^ { n 
} w _ i \ cdot f _ i x 
y \ right \ qquad \ quad n . ml 
. 1 . 2.14 $ $ 其中 $ $ Z 
_ w x = \ sum _ { y } 
\ exp \ left \ sum _ { i = 
1 } ^ { n } w _ i \ 
cdot f _ i x y \ right \ qquad 
\ quad n . ml . 1 . 2.15 $ 
$ \ Z _ w x \ 称为 归一化 因子 
\ f _ i x y \ 是 特征函数 \ 
w _ i \ 是 第 \ i \ 个 
参数 特征 权值 公式 \ n . ml . 1 
. 2.14 \ \ n . ml . 1 . 
2.15 \ 表示 的 模型 \ P _ w = 
P _ w y | x \ 就是 最大熵 模型 
\ w \ 是 最大熵 模型 中 的 参数 向量 
最后 求解 对偶 问题 外部 的 极大 化 问题 对偶 
问题 外部 极大化 表达式 $ $ \ max _ { 
w } \ Psi w \ qquad \ qquad ml 
. 1 . 2.18 $ $ 将 其解 记作 \ 
w ^ @ \ 即 \ w ^ @ = 
arg \ max _ { w } \ Psi w 
\ 也 就是说 可以 应用 最 优化 算法 求 对 
偶函数 \ \ Psi w \ 的 极大化 得到 \ 
w ^ @ \ 用 其 表示 \ P ^ 
@ = P _ { w ^ @ } = 
P _ { w ^ @ } y | x 
\ 是 学习 到 的 最优 模型 最大熵 模型 最大熵 
模型 的 学习 归结 为对 偶函数 \ \ Psi w 
\ 的 极大化 对 偶函数 极大化 与 极大 似 然 
估计 等价 从 最大熵 模型 的 学习 过程 可以 看出 
最大熵 模型 是由 \ n . ml . 1 . 
2.14 \ 和\/nr n . ml . 1 . 2.15 
\ 表示 的 条件 概率分布 下面 证明 对偶 函数 的 
极大化 等价 于 最大熵 模型 的 极大 似 然 估计 
对 偶函数 极大化 ＝ 极大 似 然 估计 已知 训练 
数据 的 经验 概率分布 \ \ tilde { P } 
x y \ 条件 概率分布 分布 \ P y | 
x \ 的 对数 似 然 函数 表示 为 $ 
$ L _ { \ tilde { P } } 
P _ w = \ log \ prod _ { 
x y } P y | x ^ { \ 
tilde { P } x y } = \ sum 
_ { x y } \ tilde { P } 
x y \ cdot \ log P y | x 
\ qquad ml . 1 . 2.19 $ $ 当 
条件 概率分布 \ P y | x \ 是 最大熵 
模型 公式 \ n . ml . 1 . 2.14 
和n/nr . ml . 1 . 2.15 \ 时 对数 
似 然 函数 \ L _ { \ tilde { 
P } } P _ w \ 为 $ $ 
\ begin { align } L _ { \ tilde 
{ P } } P _ w & = \ 
sum _ { x y } \ tilde { P 
} x y \ cdot \ log P y | 
x \ \ & = \ sum _ { x 
y } \ left \ tilde { P } x 
y \ cdot \ sum _ { i = 1 
} ^ { n } w _ i f _ 
i x y \ right \ sum _ { x 
y } \ tilde { P } x y \ 
cdot log Z _ w x \ \ & = 
\ sum _ { x y } \ left \ 
tilde { P } x y \ cdot \ sum 
_ { i = 1 } ^ { n } 
w _ i f _ i x y \ right 
\ sum _ { x } \ tilde { P 
} x \ cdot log Z _ w x \ 
end { align } \ quad ml . 1 . 
2.20 $ $ 再看 对 偶函数 \ \ Psi w 
\ 由 公式 \ ml . 1 . 2.14 \ 
和 公式 \ ml . 1 . 2.17 \ 可得 
$ $ \ begin { align } \ Psi w 
& = \ sum _ { x y } \ 
tilde { P } x \ cdot P _ w 
y | x \ cdot \ log P _ w 
y | x \ \ & \ qquad \ quad 
+ \ sum _ { i = 1 } ^ 
{ n } w _ i \ cdot \ left 
\ sum _ { x y } \ tilde { 
P } x y f _ i x y \ 
sum _ { x y } \ tilde { P 
} x P _ w y | x f _ 
i x y \ right \ \ & = \ 
sum _ { x y } \ tilde { P 
} x y \ sum _ { i = 1 
} ^ { n } w _ i f _ 
i x y + \ sum _ { x y 
} \ tilde { P } x P _ w 
y | x \ left \ underline { log P 
_ w y | x \ sum _ { i 
= 1 } ^ { n } w _ i 
f _ i x y } \ right \ \ 
& = \ sum _ { x y } \ 
tilde { P } x y \ sum _ { 
i = 1 } ^ { n } w _ 
i f _ i x y \ sum _ { 
x y } \ tilde { P } x P 
_ w y | x \ cdot \ underline { 
\ log Z _ w x } \ \ & 
= \ sum _ { x y } \ tilde 
{ P } x y \ sum _ { i 
= 1 } ^ { n } w _ i 
f _ i x y \ sum _ { x 
} \ tilde { P } x \ log Z 
_ w x \ end { align } \ quad 
ml . 1 . 2.21 $ $ 其中 第二步 推导 
第三步 中用 到了 $ $ \ sum _ { i 
= 1 } ^ { n } w _ i 
\ cdot f _ i x y = \ log 
P _ w y | x \ cdot Z _ 
w x \ qquad n . ml . 1 . 
2.16 $ $ 根据 公式 \ n . ml . 
1 . 2.14 \ 得到 在 最后 一步 用 到了 
\ \ sum _ { y } P y | 
x = 1 \ 的 性质 即 $ $ \ 
begin { align } \ sum _ { x y 
} \ tilde { P } x P _ w 
y | x \ log Z _ w x & 
= \ sum _ { x } \ tilde { 
P } x \ left \ sum _ { y 
} P _ w y | x \ right \ 
log Z _ w x \ \ & = \ 
sum _ { x } \ tilde { P } 
x \ log Z _ w x \ end { 
align } \ qquad n . ml . 1 . 
2.17 $ $ 比较 公式 \ ml . 1 . 
2.20 \ 和\/nr ml . 1 . 2.21 \ 可以 
发现 $ $ \ Psi w = L _ { 
\ tilde { P } } P _ w \ 
qquad \ qquad ml . 1 . 2.22 $ $ 
即对 偶函数 \ \ Psi w \ 等价 于 对数 
似 然 函数 \ L _ { \ tilde { 
P } } P _ w \ 于是 最大熵 模型 
学习 中的 对 偶函数 极大化 等价 于 最大熵 模型 的 
极大 似 然 估计 的 结论 得以 证明 总结 最大熵 
模型 的 学习 问题 就 转化 为 具体 求解 对数 
似 然 函数 极大化 或 对 偶函数 极大化 的 问题 
可以 将 最大熵 模型 写成 更为 一般 的 形式 $ 
$ \ begin { align } P _ w y 
| x & = \ frac { 1 } { 
Z _ w x } \ cdot \ exp \ 
left \ sum _ { i = 1 } ^ 
{ n } w _ i \ cdot f _ 
i x y \ right \ \ Z _ w 
x & = \ sum _ { y } \ 
exp \ left \ sum _ { i = 1 
} ^ { n } w _ i \ cdot 
f _ i x y \ right \ end { 
align } \ qquad ml . 1 . 2.23 $ 
$ 这里 \ x \ in R ^ n \ 
为 输入 向量 \ y \ in \ { 1 
2 \ cdots K \ } \ 为 输出 \ 
w \ in R ^ n \ 为 权值 向量 
\ f _ i x y i = 1 2 
\ cdots n \ 为 任意 实值 特征函数 小结 ① 
. 最大熵 模型 与 LR 模型 有 类似 的 形式 
它们 又 称为 对数 线性 模型 Log Linear Model ② 
. 模型 学习 就是 在 给定 的 训练 数据 条件 
下 对模型 进行 极大 似 然 估计 或 正则化 的 
极大 似 然 估计 