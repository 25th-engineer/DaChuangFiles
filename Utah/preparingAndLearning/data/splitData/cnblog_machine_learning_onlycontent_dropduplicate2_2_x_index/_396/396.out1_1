原文 转 自 http / / elevencitys . com / 
p = 1854 深度 学习 是 机器 学习 研究 中 
的 一个 新的 领域 其 动机 在于 建立 模拟 人脑 
进行 分析 学习 的 神经 网络 它 模仿 人脑 的 
机制 来 解释 数据 例如 图像 声音 和 文本 深度 
学习 是 无 监督 学习 的 一种 深度 学习 的 
概念 源于 人工神经网络 的 研究 含多 隐 层 的 多层 
感知器 就是 一种 深度 学习 结构 深度 学习 通过 组合 
低层 特征 形成 更加 抽象 的 高层 表示 属性 类别 
或 特征 以 发现 数据 的 分布式 特征 表示 深度 
学习 的 概念 由 Hinton 等人 于 2006年 提出 基于 
深 信度 网 DBN 提出 非 监督 贪心 逐层 训练 
算法 为 解决 深层 结构 相关 的 优化 难题 带来 
希望 随后 提出 多层 自动 编码器 深层 结构 此外 Lecun 
等人 提出 的 卷积 神经 网络 是 第一 个 真正 
多层 结构 学习 算法 它 利用 空间 相对 关系 减少 
参数 数目 以 提高 训练 性能 一 Deep Learning 的 
前世 今生 图灵 在 1950 年的/nr 论文 里 提出 图灵 
试验 的 设想 即 隔墙 对话 你 将 不 知道 
与 你 谈话 的 是 人 还是 电脑 1 这 
无疑 给 计算机 尤其 是 人工智能 预设 了 一个 很高 
的 期望值 但是 半个 世纪 过去 了 人工智能 的 进展 
远远 没有 达到 图灵 试验 的 标准 这 不仅 让 
多年 翘首以待 的 人们 心灰意冷 认为 人工智能 是 忽悠 相关 
领域 是 伪科学 2008 年 6 月 连线 杂志主编 Chris 
Anderson 发表文章 题目 是 理论 的 终极 数据 的 泛滥 
将 让 科学 方法 过时 并且 文中 还 引述 经典著作 
人工智能 的 现代 方法 的 合著者 时任 Google 研究 总监 
的 Peter Norvig 的 言论 说 一切 模型 都是 错 
的 进而言之 抛弃 它们 你 就会 成功 2 言下之意 精巧 
的 算法 是 无 意义 的 面对 海量 数据 即便 
只用 简单 的 算法 也 能 得到 出色 的 结果 
与其 钻研 算法 不如 研究 云计算 处理 大 数据 如果 
这 番 言论 发生 在 2006 年 以前 可能 我 
不会 强力 反驳 但是 自 2006 年以来 机器学习 领域 取得 
了 突破性 的 进展 图灵 试验 至少 不是 那么 可望而不可即 
了 至于 技术手段 不 仅仅 依赖 于 云计算 对 大 
数据 的 并行 处理 能力 而且 依赖于 算法 这个 算法 
就是 Deep Learning 借助于 Deep Learning 算法 人类 终于 找到 
了 如何 处理 抽象概念 这个 亘古 难题 的 方法 于是 
学界 忙着 延揽 相关 领域 的 大师 Alex Smola 加盟 
CMU 就是 这个 背景 下 的 插曲 悬念 是 Geoffrey 
Hinton 和 Yoshua Bengio 这两位 牛人 最后 会 加盟 哪 
所大学 Geoffrey Hinton 曾经 转战 Cambridge CMU 目前 任教 University 
of Toronto 相信 挖 他 的 名校 一定 不少 Yoshua 
Bengio 经历 比较 简单 McGill University 获得 博士后 去 MIT 
追随 Mike Jordan 做 博士后 目前 任教 University of Montreal 
Deep Learning 引爆 的 这场 革命 不仅 学术 意义 巨大 
而且 离 钱 很近 实在太 近了 如果 把 相关 技术 
难题 比喻 成 一座 山 那么 翻过 这座 山 山后 
就是 特大 露天 金矿 技术难题 解决 以后 剩下 的 事情 
就是 动用 资本 和 商业 的 强力 手段 跑马 圈地 
了 于是 各 大公司 重兵 集结 虎视眈眈 Google 兵 分两路 
左路 以 Jeff Dean 和 Andrew Ng 为首 重点 突破 
Deep Learning 等等 算法 和 应用 3 Jeff Dean 在 
Google 诸位 Fellows 中 名列榜首 GFS 就是 他 的 杰作 
Andrew Ng 本科 时 就读 CMU 后来 去 MIT 追随 
Mike Jordan Mike Jordan 在 MIT 人缘 不好 后来 愤然 
出走 UC Berkeley Andrew Ng 毫不犹豫 追随 导师 也 去了 
Berkeley 拿到 博士后 任教 Stanford 是 Stanford 新生代 教授 中 
的 佼佼者 同时 兼职 Google Google 右 路军 由 Amit 
Singhal 领军 目标 是 构建 Knowledge Graph 基础 设施 1996 
年 Amit Singhal 从 Cornell University 拿到 博士 学位 后 
去 Bell Lab 工作 2000 年 加盟 Google 据说 他 
去 Google 面试时 对 Google 创始人 Sergey Brian 说 Your 
engine is excellent but let me rewirte it 4 换了 
别人 说不定 一个 大 巴掌 就 扇 过去了 但是 Sergey 
Brian 大人大量 不仅 不 怪罪 小伙子 的 轻狂 反而 真的 
让 他 从事 新一代 排名 系统 的 研发 Amit Singhal 
目前 任职 Google 高级 副总裁 掌管 Google 最 核心 的 
业务 搜索引擎 Google 把 王牌 中 之 王牌 押宝 在 
Deep Learning 和 Knowledge Graph 上 目的 是 更快 更 
大地 夺取 大 数据 革命 的 胜利果实 Reference 1 Turing 
Test . http / / en . wikipedia . org 
/ wiki / Turing _ test 2 The End of 
Theory The Data Deluge Makes the Scientific Method Obsoletehttp / 
/ www . wired . com / science / discoveries 
/ magazine / 16 07 / pb _ theory 3 
Introduction to Deep Learning . http / / en . 
wikipedia . org / wiki / Deep _ learning 4 
Interview with Amit Singhal Google Fellow . http / / 
searchengineland . com / interview with amit singhal google fellow 
121342 二 Deep Learning 的 基本 思想 和 方法 实际 
生活 中 人们 为了 解决 一个 问题 如 对象 的 
分类 对象 可是 是 文档 图像 等 首先 必须 做 
的 事情 是 如何 来 表达 一个 对象 即 必须 
抽取 一些 特征 来 表示 一个 对象 如 文本 的 
处理 中 常 常用词 集合 来 表示 一个 文档 或 
把 文档 表示 在 向量空间 中 称为 VSM 模型 然后 
才能 提出 不同 的 分类 算法 来 进行 分类 又如 
在 图像 处理 中 我们 可以 用 像素 集合 来 
表示 一个 图像 后来 人们 提出 了 新的 特征 表示 
如 SIFT 这种 特征 在 很多 图像 处理 的 应用 
中 表现 非常 良好 特征 选取 得 好坏 对 最终 
结果 的 影响 非常 巨大 因此 选取 什么 特征 对于 
解决 一个 实际 问题 非常 的 重要 然而 手 工地 
选取 特征 是 一件 非常 费力 启发式 的 方法 能/v 
不能/v 选取/v 好/a 很大/a 程度/n 上/f 靠/v 经验/n 和/c 运气/n 
既然 手工 选取 特征 不太好 那么 能 不能 自动 地 
学习 一些 特征 呢 答案 是 能 Deep Learning 就是 
用来 干 这个 事情 的 看 它 的 一个 别名 
Unsupervised Feature Learning 就 可以 顾名思义 了 Unsupervised 的 意思 
就是 不 要人 参与 特征 的 选取 过程 因此 自动 
地 学习 特征 的 方法 统称为 Deep Learning 1 Deep 
Learning 的 基本 思想 假设 我们 有 一个 系统 它 
有n层/nr S1 Sn 它 的 输入 是 I 输出 是 
O 形象 地 表示 为 I = S1 = S2 
= . . = Sn = O 如果 输出 O 
等于 输入 I 即 输入 I 经过 这个 系统 变化 
之后 没有 任何 的 信息 损失 保持 了 不变 这 
意味着 输入 I 经过 每 一层 Si 都 没有 任何 
的 信息 损失 即在 任何 一层 Si 它 都是 原有 
信息 即 输入 I 的 另外 一种 表示 现在 回到 
我们 的 主题 Deep Learning 我们 需要 自动 地 学习 
特征 假设 我们 有 一堆 输入 I 如 一堆 图像 
或者 文本 假设 我们 设计 了 一个 系统 有n层/nr 我们 
通过 调整 系统 中 参数 使得 它 的 输出 仍然 
是 输入 I 那么 我们 就 可以 自动 地 获取 
得到 输入 I 的 一系列 层次 特征 即 S1 Sn 
另外 前面 是 假设 输出 严格地 等于 输入 这个 限制 
太 严格 我们 可以 略微 地 放松 这个 限制 例如 
我们 只要 使得 输入 与 输出 的 差别 尽可能 地 
小 即可 这个 放松 会 导致 另外 一 类 不同 
的 Deep Learning 方法 上述 就是 Deep Learning 的 基本 
思想 2 Deep Learning 的 常用 方法 a . AutoEncoder 
最 简单 的 一种 方法 是 利用 人工神经网络 的 特点 
人工神经网络 ANN 本身 就是 具有 层次 结构 的 系统 如果 
给 定 一个 神经 网络 我们 假设 其 输出 与 
输入 是 相同 的 然后 训练 调整 其 参数 得到 
每 一层 中的 权重 自然地 我们 就 得到 了 输入 
I 的 几种 不同 表示 每 一层 代表 一种 表示 
这些 表示 就是 特征 在 研究 中 可以 发现 如果 
在 原有 的 特征 中 加入 这些 自动 学习 得到 
的 特征 可以 大大 提高 精确度 甚至在 分类 问题 中 
比 目前 最好 的 分类 算法 效果 还 要好 这种方法 
称为 AutoEncoder 当然 我们 还 可以 继续 加上 一些 约束 
条件 得到 新的 Deep Learning 方法 如 如果 在 AutoEncoder 
的 基础上 加上 L1 的 Regularity 限制 L1 主要 是 
约束 每 一层 中的 节点 中 大部分 都 要为 0 
只有 少数 不为 0 这 就是 Sparse 名字 的 来源 
我们 就 可以 得到 Sparse AutoEncoder 方法 b . Sparse 
Coding 如果 我们 把 输出 必须 和 输入 相等 的 
限制 放松 同时 利用 线性代数 中 基 的 概念 即 
O = w1 * B1 + W2 * B2 + 
. + Wn * Bn Bi 是 基 Wi 是 
系数 我们 可以 得到 这样 一个 优化 问题 Min | 
I – O | 通过 求解 这个 最 优化 式子 
我们 可以 求得 系数 Wi 和基/nr Bi 这些 系数 和 
基础 就是 输入 的 另外 一种 近似 表达 因此 它们 
可以 特征 来 表达 输入 I 这个 过程 也 是 
自动 学习 得到 的 如果 我们 在 上述 式子 上 
加上 L1 的 Regularity 限制 得到 Min | I – 
O | + u * | W1 | + | 
W2 | + + | Wn | 这种 方法 被 
称为 Sparse Coding c   Restrict     Boltzmann   
Machine RBM 假设 有 一个 二部 图 每 一层 的 
节点 之间 没有 链接 一层 是 可视 层 即 输入 
数据 层 v 一层 是 隐藏 层 h 如果 假设 
所有 的 节点 都是 二 值 变量 节点 只能 取 
0 或者 1 值 同时 假设 全 概率分布 p v 
h 满足 Boltzmann 分布 我们 称 这个 模型 是 Restrict 
    Boltzmann   Machine RBM 下面 我们 来 看看 
为什么 它 是 Deep Learning 方法 首先 这个 模型 因为 
是 二部 图 所以在 已知 v 的 情况 下 所有 
的 隐藏 节点 之间 是 条件 独立 的 即 p 
h | v = p h1 | v . . 
p hn | v 同理 在 已知 隐藏 层 h 
的 情况 下 所有 的 可视 节点 都是/nr 条件 独立 
的 同时 又 由于 所有 的 v 和h/nr 满足 Boltzmann 
分布 因此 当 输入 v 的 时候 通过 p h 
| v 可以 得到 隐藏 层 h 而 得到 隐藏 
层 h 之后 通过 p v | h 又 能 
得到 可视 层 通过 调整 参数 我们 就是 要 使得 
从 隐藏 层 得到 的 可视 层 v1 与 原来 
的 可视 层 v 如果 一样 那么 得到 的 隐藏 
层 就是 可视 层 另外 一种 表达 因此 隐藏 层 
可以 作为 可视 层 输入 数据 的 特征 所以 它 
就是 一种 Deep Learning 方法 如果 我们 把 隐藏 层 
的 层数 增加 我们 可以 得到 Deep   Boltzmann   
Machine DBM 如果 我们 在 靠近 可视 层 的 部分 
使用 贝叶斯 信念 网络 即 有向图 模型 当然 这里 依然 
限制 层 中 节点 之间 没有 链接 而 在最 远离 
可视 层 的 部分 使用 Restrict     Boltzmann   
Machine 我们 可以 得到 Deep Belief Net DBN 当然 还有 
其它 的 一些 Deep Learning 方法 在 这里 就不 叙述 
了 总之 Deep Learning 能够 自动 地 学习 出 数据 
的 另外 一种 表示 方法 这种 表示 可以 作 为特征 
加入 原有 问题 的 特征 集合 中 从而 可以 提高 
学习 方法 的 效果 是 目前 业界 的 研究 热点 
三 深度 学习 Deep Learning 算法 简介 查看 最新 论文 
Yoshua Bengio Learning Deep Architectures for AI Foundations and Trends 
in Machine Learning 2 1 2009 深度 Depth 从 一个 
输入 中 产生 一个 输出 所 涉及 的 计算 可以 
通过 一个 流向 图 flow graph 来 表示 流向 图 
是 一种 能够 表示 计算 的 图 在 这种 图中 
每一个 节点 表示 一个 基本 的 计算 并且 一个 计算 
的 值 计算 的 结果 被 应用 到 这个 节点 
的 孩子 节点 的 值 考虑 这样 一个 计算 集合 
它 可以 被 允许 在 每一个 节点 和 可能 的 
图 结构 中 并 定义 了 一个 函数 族 输入 
节点 没有 孩子 输出 节点 没有 父亲 对于 表达   
的 流向 图 可以/c 通过/p 一个/m 有/v 两个/m 输入/v 节点/n 
 /i 和 的/nr 图/n 表示/v 其中 一个 节点 通过 使用 和 /nr 
作为 输入 例如 作为 孩子 来 表示   一个 节点 
仅 使用 作为 输入 来 表示 平方 一个 节点 使用 
  和 /nr 作为 输入 来 表示 加法 项 其 值 
为   最后 一个 输出 节点 利用 一个 单独 的 
来自 于 加法 节点 的 输入 计算 SIN 这种 流向 
图 的 一个 特别 属性 是 深度 depth 从 一个 
输入 到 一个 输出 的 最长 路径 的 长度 传统 
的 前馈 神经网络 能够 被 看做 拥有 等于 层数 的 
深度 比如 对于 输出 层 为 隐 层数 加 1 
SVMs 有 深度 2 一个 对 应于 核 输出 或者 
特征 空间 另 一个 对 应于 所 产生 输出 的 
线性 混合 深度 架构 的 动机 学习 基于 深度 架构 
的 学习 算法 的 主要 动机 是 不 充分 的 
深度 是 有害 的 大脑 有 一个 深度 架构 认知过程 
是 深度 的 不 充分 的 深度 是 有害 的 
在 许多 情形 中 深度 2 就 足够 比如 logical 
gates formal threshold neurons sigmoid neurons Radial Basis Function RBF 
units like in SVMs 表示 任何 一个 带有 给定 目标 
精度 的 函数 但是 其 代价 是 图中 所 需要 
的 节点 数 比如 计算 和 参数 数量 可能 变 
的 非常 大 理论 结果 证实 那些 事实上 所 需要 
的 节点 数 随着 输入 的 大小 指数 增长 的 
函数 族 是 存在 的 这 一点 已经 在 logical 
gates formal threshold neurons 和 rbf 单元 中 得到 证实 
在 后者 中 Hastad 说明了 但 深度 是 d 时 
函数 族 可以 被 有效 地 紧地 使用 O n 
个 节点 对于 n 个 输入 来 表示 但是 如果 
深度 被 限制 为 d 1 则 需要 指数 数量 
的 节点 数 O 2 ^ n 我们 可以 将 
深度 架构 看做 一种 因子 分解 大部分 随机 选择 的 
函数 不能 被 有效 地 表示 无论是 用 深 地 
或者 浅 的 架构 但是 许多 能够 有效 地被 深度 
架构 表示 的 却不能 被用 浅 的 架构 高效 表示 
see the polynomials example in the   Bengio survey paper 
一个 紧 的 和 深度 的 表示 的 存在 意味着 
在 潜在 的 可被 表示 的 函数 中 存在 某种 
结构 如果 不 存在 任何 结构 那将 不 可能 很好 
地 泛化 大脑 有 一个 深度 架构 例如 视觉皮质 得到 
了 很好 的 研究 并 显示 出 一系列 的 区域 
在/p 每一个/i 这种/r 区域/n 中/f 包含/v 一个/m 输入/v 的/uj 表示/v 
和从/nr 一个/m 到/v 另一个/i 的/uj 信号/n 流/n 这里 忽略 了 
在 一些 层次 并行 路径 上 的 关联 因此 更 
复杂 这个 特征 层次 的 每 一层 表示 在 一个 
不同 的 抽象 层 上 的 输入 并在 层次 的 
更 上层 有着 更多 的 抽象 特征 他们 根据 低层 
特征 定义 需要 注意 的 是 大脑 中 的 表示 
是 在中间 紧密 分布 并且 纯 局部 他们 是 稀疏 
的 1% 的 神经元 是 同时 活动 的 给定 大量 
的 神经元 任然有/nr 一个/m 非常/d 高/a 效地/i 指数级 高效 表示 
认知过程 看起来 是 深度 的 人类 层次化 地 组织 思想 
和 概念 人类 首先 学习 简单 的 概念 然后 用 
他们 去 表示 更 抽象 的 工程师 将 任务 分解 
成 多个 抽象层次 去 处理 学习 / 发现 这些 概念 
知识 工程 由于 没有 反省 而 失败 是 很 美好 
的 对 语言 可 表达 的 概念 的 反省 也 
建议 我们 一个 稀疏 的 表示 仅 所有 可能 单词 
/ 概念 中 的 一个 小 的 部分 是 可被 
应用 到 一个 特别 的 输入 一个 视觉 场景 学习 
深度 架构 的 突破 2006 年前 尝试 训练 深度 架构 
都 失败 了 训练/vn 一个/m 深度/ns 有/v 监督/vn 前馈/v 神经网络/n 
趋向/n 于/p 产生/n 坏/a 的/uj 结果/n 同时 在 训练 和 
测试 误差 中 然后 将其 变浅 为 1 1 或者 
2个 隐 层 2006年 的 3篇 论文 改变 了 这种 
状况 由 Hinton 的 革命性 的 在 深度 信念 网 
Deep Belief Networks DBNs 上 的 工作 所 引领 Hinton 
G . E . Osindero . and Teh Y . 
  A fast learning algorithm for deep belief nets   
. Neural Computation 18 1527 1554 2006Yoshua Bengio Pascal Lamblin 
Dan Popovici and Hugo Larochelle   Greedy Layer Wise Training 
of Deep Networks in J . Platt et al . 
Eds Advances in Neural Information Processing Systems 19 NIPS 2006 
pp . 153 160 MIT Press 2007Marc Aurelio Ranzato Christopher 
Poultney Sumit Chopra and Yann LeCun   Efficient Learning of 
Sparse Representations with an Energy Based Model in J . 
Platt et al . Eds Advances in Neural Information Processing 
Systems NIPS 2006 MIT Press 2007 在 这 三篇 论文 
中 以下 主要 原理 被发现 表示 的 无 监督 学习 
被 用于 预 训练 每 一层 在 一个 时间 里 
的 一个 层次 的 无 监督 训练 接着 之前 训练 
的 层次 在 每 一层 学习 到 的 表示 作为 
下 一层 的 输入 用 无 监督 训练 来 调整 
所 有层 加上 一个 或者 更多 的 用于 产生 预测 
的 附加 层 DBNs 在 每 一层 中 利用 用于 
表示 的 无 监督 学习 RBMs Bengio et al paper 
探讨/v 和/c 对比/v 了/ul RBMs/w 和/c auto/w encoders 通过 一个 
表示 的 瓶颈 内在 层 预测 输入 的 神经 网络 
Ranzato et al paper 在 一个 convolutional 架构 的 上下 
文中 使用 稀疏 auto encoders 类似于 稀疏 编码 Auto encoders 
和 convolutional 架构 将 在 以后 的 课程 中 讲解 
从 2006 年以来 大量 的 关于 深度 学习 的 论文 
被 发表 一些 探讨 了 其他 原理 来 引导 中间 
表示 的 训练 查看 Learning Deep Architectures for AI 四 
拓展 学习 推荐 Deep Learning 经典 阅读 材料 The monograph 
or review paper   Learning Deep Architectures for AI   
Foundations & Trends in Machine Learning 2009 . The ICML 
2009 Workshop on Learning Feature Hierarchies   webpage   has 
a   list of references . The LISA   public 
wiki   has a   reading list   and a 
  bibliography . Geoff Hinton has   readings   from 
last year s   NIPS tutorial . Deep Learning 工具 
  Theano Theano 是 deep learning 的 Python 库 要求 
首先 熟悉 Python 语言 和 numpy 建议 读者 先看 Theano 
basic tutorial 然后 按照 Getting Started   下载 相关 数据 
并用 gradient descent 的 方法 进行 学习 学习 了 Theano 
的 基本 方法 后 可以 练习 写 以下 几个 算法 
有 监督 学习 Logistic Regression   using Theano for something 
simpleMultilayer perceptron   introduction to layersDeep Convolutional Network   a 
simplified version of LeNet5 无 监督 学习 Auto Encoders Denoising 
Autoencoders   description of a u t o e n 
c o d e r s t a c k 
e d Denoising Auto Encoders   easy steps into unsupervised 
pre training for deep netsRestricted Boltzmann Machines   single layer 
generative RBM modelDeep Belief Networks     unsupervised generative pre 
training of stacked RBMs followed by supervised fine tuning 最后 
呢 推荐 给 大家 基本 ML 的 书籍 Chris Bishop 
Pattern Recognition and Machine Learning 2007Simon Haykin Neural Networks a 
Comprehensive Foundation 2009 3rd edition Richard O . Duda Peter 
E . Hart and David G . Stork Pattern Classification 
2001 2nd edition 五 应用 实例 1 计算机 视觉 ImageNet 
Classification with Deep Convolutional Neural Networks Alex Krizhevsky Ilya Sutskever 
Geoffrey E Hinton NIPS 2012 . Learning Hierarchical Features for 
Scene Labeling Clement Farabet Camille Couprie Laurent Najman and Yann 
LeCun IEEE Transactions on Pattern Analysis and Machine Intelligence 2013 
. Learning Convolutional Feature Hierachies for Visual Recognition Koray Kavukcuoglu 
Pierre Sermanet Y Lan Boureau Karol Gregor Micha & euml 
l Mathieu and Yann LeCun Advances in Neural Information Processing 
Systems NIPS 2010 23 2010.2 语音识别 微软 研究 人员 通过 
与 hintion 合作 首先 将 RBM 和 DBN 引入 到 
语音识别 声学 模型 训练 中 并且在 大 词汇量 语音 识别 
系统 中 获得 巨大 成功 使得 语音 识别 的 错误率 
相对 减低 30% 但是 DNN 还 没有 有效 的 并行 
快 速算法 目前 很多 研究 机构 都是 在 利用 大 
规模 数据 语料 通过 GPU 平台 提高 DNN 声学 模型 
的 训练 效率 在 国际 上 IBM google 等 公司 
都 快速 进行 了 DNN 语音 识别 的 研究 并且 
速度 飞快 国内 方面 科大 讯 飞 百度 中科院 自动化所 
等 公司 或 研究 单位 也 在 进行 深度 学习 
在 语音 识别 上 的 研究 3 自然语言 处理 等 
其他 领域 很多 机构 在 开展 研究 但 目前 深度 
学习 在 自然 语言 处理 方面 还 没有 产生 系统性 
的 突破 六 参考 链接 1 . http / / 
baike . baidu . com / view / 9964678 . 
htm subLemmaId = 10105430 & fromenter = deep + learning2 
. http / / www . cnblogs . com / 
ysjxw / archive / 2011/10 / 08/2201819 . html3 . 
http / / blog . csdn . net / abcjennifer 
/ article / details / 7826917 