1 . KNN 原理 存在 一个 样本数据 集合 也 称作 
训练样本 集 并且 样本 集中 每个 数据 都 存在 标签 
即 我们 知道 样本 集中 每 一个 数据 与 所属 
分类 的 对应 关系 输入 没有 标签 的 新 数据 
后 将 新 数据 的 每个 特征 与 样本 集中 
数据 对应 的 特征 进行 比较 然后 算法 提取 样本 
集中 最 相似 数据 最 近邻 的 分类 标签 一般来说 
只 选择 样本 数据集 中前 $ k $ 个 最 
相似 的 数据 这 就是 KNN 算法 $ k $ 
的 出处 通常 $ k $ 是 不大 于 20 
的 整数 最后 选择 $ k $ 个 最 相似 
数据 中 出现 次数 最多 的 分类 作为 新 数据 
的 分类 2 . 实验 准备 Pythonscikit learn 一个 基于 
python 的 机器学习 库 3 . 实验 代码 代码 有 
两个 版本 一个 是 自己 编写 的 简单 的 KNN 
算法 实现 一个 是 基于 scikit learn 库 中 KNN 
算法 实现 的 数据 均 采用 scikit learn 中的 手写体 
数据集 版本 1 自己 编写 # * coding utf 8 
* This script is an exercise on KNN . Created 
on Tue Nov 03 21 21 39 2015 @ author 
90Zeng import numpy as np from sklearn import datasets import 
operator # function classify def classify0 inX dataSet labels k 
dataSetSize = dataSet . shape 0 # 计算 输入 的 
向量 inX 与 所有 样本 的 距离 diffMat = np 
. tile inX dataSetSize 1 dataSet sqDiffMat = diffMat * 
* 2 sqDistances = sqDiffMat . sum axis = 1 
distances = sqDistances * * 0.5 # 对 距离 大小 
进行 排序 s o r t e d D i 
s t I n d i c e s = 
distances . argsort classCount = { } # 选择 距离 
最小 的 K 个 点 for i in range k 
voteLabel = labels s o r t e d D 
i s t I n d i c e s 
i classCount voteLabel = classCount . get voteLabel 0 + 
1 # 按照 类别 的 数量 多少 进行 排序 sortedClassCount 
= sorted classCount . iteritems key = operator . itemgetter 
1 reverse = True return sortedClassCount 0 0 # 返回 
类别 数 最多 的 类别 名称 # end of function 
classify def h a n d w r i t 
i n g C l a s s T e 
s t # 导入 数据 digits = datasets . load 
_ digits totalNum = len digits . data # 选出 
90% 样本 作为 训练样本 其余 10% 测试 trainNum = int 
0.8 * totalNum trainX = digits . data 0 trainNum 
trainY = digits . target 0 trainNum testX = digits 
. data trainNum testY = digits . target trainNum errorCount 
= 0 testExampleNum = len testX for i in range 
testExampleNum # 测试 样本 在 测试 集中 真实 的 类别 
trueLabel = testY i classifierResult = classify0 testX i trainX 
trainY 5 print \ nThe classifier came back with % 
d the real answer is % d \ % classifierResult 
trueLabel if trueLabel = classifierResult errorCount + = 1 else 
pass print \ nThe total number of errors is % 
d % errorCount print \ nthe total error rate is 
% f % errorCount / float testExampleNum if _ _ 
name _ _ = = _ _ main _ _ 
print start . . . h a n d w 
r i t i n g C l a s 
s T e s t 运行 结果 版本 2 使用 
库函数 # * coding utf 8 * This script is 
an exercise on KNN . Created on Tue Nov 06 
21 26 39 2015 @ author ZengJiulin print _ _ 
doc _ _ import numpy as np from sklearn import 
neighbors datasets digits = datasets . load _ digits totalNum 
= len digits . data # 选出 90% 样本 作为 
训练样本 其余 10% 测试 trainNum = int 0.8 * totalNum 
trainX = digits . data 0 trainNum trainY = digits 
. target 0 trainNum testX = digits . data trainNum 
testY = digits . target trainNum n _ neighbors = 
10 clf = neighbors . K N e i g 
h b o r s C l a s s 
i f i e r n _ neighbors weights = 
uniform clf . fit trainX trainY Z = clf . 
predict testX print \ nthe total error rate is % 
f % 1 np . sum Z = = testY 
/ float len testX 运行 结果 4 . 总结 KNN 
的 优点 精度高 对 异常值 不 敏感 无 数据 输入 
假定 缺点 计算 复杂度 高 要 计算 待 分类 样本 
与 所有 已知 类别 样本 的 距离 空间 复杂度 高 
存储 所有 样本点 和 目标 样本 的 距离 