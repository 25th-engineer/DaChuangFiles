最近 在 学习 机器学习 学习/v 和/c 积累/v 和/c 一些/m 关于/p 
机器/n 学习/v 的/uj 算法/n 今天 介绍 一种 机器学习 里面 各种 
分类 算法 的 比较 # / usr / bin / 
python # * coding utf 8 * = = = 
= = = = = = = = = = 
= = = = = = = = Classifier comparison 
= = = = = = = = = = 
= = = = = = = = = = 
= A comparison of a several classifiers in scikit learn 
on synthetic datasets . The point of this example is 
to illustrate the nature of decision boundaries of different classifiers 
. 与 其他 的 机器 学习 的 分类 的 算法 
在 合成 数据 方面 相比较 本 示例 为了 说明 不同 
算法 边界 的 性质 This should be taken with a 
grain of salt as the intuition conveyed by these examples 
does not necessarily carry over to real datasets . Particularly 
in high dimensional spaces data can more easily be separated 
linearly and the simplicity of classifiers such as naive Bayes 
and linear SVMs might lead to better generalization than is 
achieved by other classifiers . The plots show training points 
in solid colors and testing points semi transparent . The 
lower right shows the classification accuracy on the test set 
. print _ _ doc _ _ # Code source 
Ga ë l Varoquaux # Andreas M ü ller # 
Modified for documentation by Jaques Grobler # License BSD 3 
clause import numpy as np import matplotlib . pyplot as 
plt from matplotlib . colors import ListedColormap from sklearn . 
model _ selection import train _ test _ split from 
sklearn . preprocessing import StandardScaler from sklearn . datasets import 
make _ moons make _ circles make _ classification from 
sklearn . neural _ network import MLPClassifier from sklearn . 
neighbors import K N e i g h b o 
r s C l a s s i f i 
e r from sklearn . svm import SVC from sklearn 
. gaussian _ process import G a u s s 
i a n P r o c e s s 
C l a s s i f i e r 
from sklearn . gaussian _ process . kernels import RBF 
from sklearn . tree import D e c i s 
i o n T r e e C l a 
s s i f i e r from sklearn . 
ensemble import R a n d o m F o 
r e s t C l a s s i 
f i e r A d a B o o 
s t C l a s s i f i 
e r from sklearn . naive _ bayes import GaussianNB 
from sklearn . discriminant _ analysis import Q u a 
d r a t i c D i s c 
r i m i n a n t A n 
a l y s i s h = . 02 
# step size in the mesh names = Nearest Neighbors 
Linear SVM RBF SVM Gaussian Process Decision Tree Random Forest 
Neural Net AdaBoost Naive Bayes QDA classifiers = K N 
e i g h b o r s C l 
a s s i f i e r 3 SVC 
kernel = linear C = 0.025 SVC gamma = 2 
C = 1 G a u s s i a 
n P r o c e s s C l 
a s s i f i e r 1.0 * 
RBF 1.0 warm _ start = True D e c 
i s i o n T r e e C 
l a s s i f i e r max 
_ depth = 5 R a n d o m 
F o r e s t C l a s 
s i f i e r max _ depth = 
5 n _ estimators = 10 max _ features = 
1 MLPClassifier alpha = 1 A d a B o 
o s t C l a s s i f 
i e r GaussianNB Q u a d r a 
t i c D i s c r i m 
i n a n t A n a l y 
s i s X y = make _ classification n 
_ features = 2 n _ redundant = 0 n 
_ informative = 2 random _ state = 1 n 
_ clusters _ per _ class = 1 # print 
X # print len y rng = np . random 
. RandomState 2 # print X . shape X + 
= 2 * rng . uniform size = X . 
shape # print X linearly _ separable = X y 
datasets = make _ moons noise = 0.3 random _ 
state = 0 make _ circles noise = 0.2 factor 
= 0.5 random _ state = 1 linearly _ separable 
figure = plt . figure figsize = 27 9 i 
= 1 # iterate over datasets for ds _ cnt 
ds in enumerate datasets 上面 的 循环 ds _ cnt 
是从 0 datasets 的 长度 变换 ds 代表 datasets 的 
每个 值 在 这里 相当于 每 个 数据 生成 方法 
的 返回值 # preprocess dataset split into training and test 
part 将 ds 的 返回值 赋值 给 X y X 
y = ds 标准化 均值 去除 和按/nr 方差 比例 缩放 
数据集 的 标准化 当 个体 特征 太过 或 明显 不 
遵从 高斯 正态分布 时 标准化 表现 的 效果 较差 实际 
操作 中 经常 忽略 特征 数据 的 分布 形状 移除 
每个 特征 均值 划分 离散 特征 的 标准差 从而 等级化 
进而 实现 数据 中心化 通过 删除 平均值 和 缩放 到 
单位 方差 来 标准化 特征 X = StandardScaler . fit 
_ transform X 定义 了 四个 变量 利用 数据 分割 
函数 将 数据 分为 训练 数据集 和 测试 数据集 以及 
训练 数据集 和 测试 数据集 对应 的 整数 标签 X 
_ train X _ test y _ train y _ 
test = train _ test _ split X y test 
_ size = . 4 random _ state = 42 
x _ min x _ max = X 0 . 
min . 5 X 0 . max + . 5 
y _ min y _ max = X 1 . 
min . 5 X 1 . max + . 5 
# print X 0 xx yy = np . meshgrid 
np . arange x _ min x _ max h 
np . arange y _ min y _ max h 
# just plot the dataset first cm = plt . 
cm . RdBu 红色 和 蓝色 cm _ bright = 
ListedColormap # FF0000 # 0000FF ax = plt . subplot 
len datasets len classifiers + 1 i if ds _ 
cnt = = 0 ax . set _ title Input 
data # Plot the training points scatter 函数 绘制 散列 
图 深红色 和 深蓝色 是 划分 出来 的 训练 数据 
ax . scatter X _ train 0 X _ train 
1 c = y _ train cmap = cm _ 
bright # and testing points 浅 红色 和 浅蓝色 是 
划分 出来 的 测试数据 这样 就 形成 了 四种 颜色 
的 数据 ax . scatter X _ test 0 X 
_ test 1 c = y _ test cmap = 
cm _ bright alpha = 0.6 ax . set _ 
xlim xx . min xx . max ax . set 
_ ylim yy . min yy . max ax . 
set _ xticks ax . set _ yticks i + 
= 1 # iterate over classifiers for name clf in 
zip names classifiers ax = plt . subplot len datasets 
len classifiers + 1 i clf . fit X _ 
train y _ train score = clf . score X 
_ test y _ test # Plot the decision boundary 
. For that we will assign a color to each 
# point in the mesh x _ min x _ 
max x y _ min y _ max . if 
hasattr clf decision _ function Z = clf . decision 
_ function np . c _ xx . ravel yy 
. ravel else Z = clf . predict _ proba 
np . c _ xx . ravel yy . ravel 
1 # Put the result into a color plot Z 
= Z . reshape xx . shape ax . contourf 
xx yy Z cmap = cm alpha = . 8 
# Plot also the training points ax . scatter X 
_ train 0 X _ train 1 c = y 
_ train cmap = cm _ bright # and testing 
points ax . scatter X _ test 0 X _ 
test 1 c = y _ test cmap = cm 
_ bright alpha = 0.6 ax . set _ xlim 
xx . min xx . max ax . set _ 
ylim yy . min yy . max ax . set 
_ xticks ax . set _ yticks if ds _ 
cnt = = 0 ax . set _ title name 
ax . text xx . max . 3 yy . 
min + . 3 % . 2f % score . 
lstrip 0 size = 15 h o r i z 
o n t a l a l i g n 
m e n t = right i + = 1 
plt . tight _ layout plt . show 