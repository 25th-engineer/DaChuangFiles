版权 声明 本文 由 LeftNotEasy 发布 于 http / / 
leftnoteasy . cnblogs . com 本文 可以 被 全部 的 
转载 或者 部分 使用 但 请 注明 出处 如果 有 
问题 请 联系 wheeleast @ gmail . com 也 可以 
加 我 的 微博 @ leftnoteasy 前言 上一次 写了 关于 
PCA 与 LDA 的 文章 PCA 的 实现 一般 有 
两种 一种 是 用 特征值 分解 去 实现 的 一种 
是 用 奇异 值 分解 去 实现 的 在上 篇 
文章 中 便是 基于 特征值 分解 的 一种 解释 特征值 
和 奇异 值 在 大部分 人 的 印象 中 往往 
是 停留 在 纯粹 的 数学 计算 中 而且 线性代数 
或者 矩阵 论 里面 也很 少讲 任何 跟 特征值 与 
奇异 值 有关 的 应用 背景 奇异 值 分解 是 
一个 有着 很 明显 的 物理 意义 的 一种 方法 
它 可以 将 一个 比较 复杂 的 矩阵 用 更小 
更 简单 的 几个 子 矩阵 的 相乘 来 表示 
这些 小 矩阵 描述 的 是 矩阵 的 重要 的 
特性 就像 是 描述 一个 人 一样 给 别人 描述 
说 这个 人 长得 浓眉大眼 方脸 络腮胡 而且 带个 黑框 
的 眼镜 这样 寥寥 的 几个 特征 就让 别人 脑海 
里面 就 有一个 较为 清楚 的 认识 实际上 人 脸上 
的 特征 是 有着 无数种 的 之所以 能 这么 描述 
是 因为 人 天生 就 有着 非常 好 的 抽取 
重要 特征 的 能力 让 机器 学会 抽取 重要 的 
特征 SVD 是 一个 重要 的 方法 在 机器学习 领域 
有 相当多 的 应用 与 奇异 值 都可以 扯上 关系 
比如 做 feature reduction 的 PCA 做 数据压缩 以 图像压缩 
为 代表 的 算法 还有 做 搜索引擎 语义 层次 检索 
的 LSI Latent Semantic Indexing 另外 在 这里 抱怨 一下 
之前 在 百度 里面 搜索 过 SVD 出来 的 结果 
都是/nr 俄罗斯 的 一种 狙击枪 AK47 同 时代 的 是 
因为 穿越火线 这个 游戏 里面 有 一把 狙击枪 叫做 SVD 
而在 Google 上面 搜索 的 时候 出来 的 都是 奇异 
值 分解 英文 资料 为主 想 玩玩 战争 游戏 玩玩 
COD 不是 非常 好吗 玩 山寨 的 CS 有 神马 
意思 啊 国内 的 网页 中 的 话语权 也被 这些 
没有 太多 营养 的 帖子 所 占据 真心 希望 国内 
的 气氛 能够 更 浓 一点 搞 游戏 的 人 
真正 是 喜欢 制作 游戏 搞 Data Mining 的 人 
是 真正 喜欢 挖 数据 的 都 不是 仅仅 为了 
混口饭吃 这样/r 谈/v 超越/v 别/r 人才/n 有/v 意义/n 中文 文 
章中 能 踏踏实实 谈谈 技术 的 太少 了 改变 这个 
状况 从我/nr 自己 做起 吧 前面 说 了 这么 多 
本文 主要 关注 奇异 值 的 一些 特性 另外 还 
会 稍稍 提及 奇异 值 的 计算 不过 本文 不 
准备 在 如何 计算 奇异 值 上 展开 太多 另外 
本文 里面 有 部分 不算 太深 的 线性代数 的 知识 
如果 完全 忘记 了 线性代数 看 本文 可能会 有些 困难 
一 奇异 值 与 特征值 基础知识 特征值/n 分解/v 和/c 奇异/a 
值/n 分解/v 在/p 机器学习/i 领域/n 都是/nr 属于/v 满地/n 可见/v 的/uj 
方法/n 两者 有着 很 紧密 的 关系 我 在 接下来 
会 谈到 特征值/n 分解/v 和/c 奇异/a 值/n 分解/v 的/uj 目的/n 
都是/nr 一样/r 就是 提取 出 一个 矩阵 最 重要 的 
特征 先 谈谈 特征值 分解 吧 1 特征值 如果 说 
一个 向量 v 是 方阵 A 的 特征向量 将 一定 
可以 表示 成 下面 的 形式 这时候 λ 就 被称为 
特征向量 v 对应 的 特征值 一个 矩阵 的 一组 特征向量 
是 一组 正交 向量 特征值 分解 是 将 一个 矩阵 
分解成 下面 的 形式 其中 Q 是 这个 矩阵 A 
的 特征向量 组成 的 矩阵 Σ 是 一个 对角 阵 
每一个 对角 线上 的 元素 就 是 一个 特征值 我 
这里 引用 了 一些 参考 文献 中 的 内容 来 
说明 一下 首先 要 明确 的 是 一个 矩阵 其实 
就是 一个 线性变换 因为 一个 矩阵 乘以 一个 向量 后 
得到 的 向量 其实 就 相当于 将 这个 向量 进行 
了 线性变换 比如说 下面 的 一个 矩阵 它 其实 对应 
的 线性变换 是 下面 的 形式 因为 这个 矩阵 M 
乘以 一个 向量 x y 的 结果 是 上面 的 
矩阵 是 对称 的 所以 这个 变换 是 一个 对 
x y 轴 的 方向 一个 拉伸 变换 每一个 对角 
线上 的 元素 将 会对 一个 维度 进行 拉伸 变换 
当值 1时 是 拉长 当值 1 时时 缩短 当 矩阵 
不是 对称 的 时候 假如 说 矩阵 是 下面 的 
样子 它 所 描述 的 变换 是 下面 的 样子 
这 其实 是 在 平面 上 对 一个 轴 进行 
的 拉伸 变换 如 蓝色 的 箭头 所示 在 图中 
蓝色 的 箭头 是 一个 最 主要 的 变化 方向 
变化 方向 可能 有 不止 一个 如果 我们 想 要 
描述 好 一个 变换 那 我们 就 描述 好 这个 
变换 主要 的 变化 方向 就 好了 反 过头 来 
看看 之前 特征值 分解 的 式子 分解 得到 的 Σ 
矩阵 是 一个 对角 阵 里面 的 特征 值 是由 
大 到 小 排列 的 这些 特征值 所 对应 的 
特征向量 就是 描述 这个 矩阵 变化 方向 从 主要 的 
变化 到 次要 的 变化 排列 当 矩阵 是 高维 
的 情况 下 那么 这个 矩阵 就是 高维空间 下 的 
一个 线性变换 这个 线性 变化 可能 没法 通过 图片 来 
表示 但是 可以 想象 这个 变换 也 同样 有 很多 
的 变换 方向 我们 通过 特征值 分解 得到 的 前 
N 个 特征向量 那么 就 对应 了 这个 矩阵 最主要 
的 N 个 变化 方向 我们 利用 这 前 N 
个 变化 方向 就 可以 近似 这个 矩阵 变换 也 
就是 之前 说 的 提取 这个 矩阵 最 重要 的 
特征 总结 一下 特征值 分解 可以 得到 特征值 与 特征向量 
特征值 表示 的 是 这个 特征 到底有 多 重要 而 
特征向量 表示 这个 特征 是 什么 可以 将 每一个 特征向量 
理解 为 一个 线性 的 子空间 我们 可以 利用 这些 
线性 的 子空间 干 很多 的 事情 不过 特征值 分解 
也 有 很多 的 局限 比如说 变换 的 矩阵 必须 
是 方阵 说 了 这么 多 特征值 变换 不 知道 
有 没有 说 清楚 请 各位 多提 提意见 2 奇异 
值 下面 谈谈 奇异 值 分解 特征值 分解 是 一个 
提取 矩阵 特征 很 不错 的 方法 但是 它 只是 
对 方阵 而言 的 在 现实 的 世界 中 我们 
看到 的 大部分 矩阵 都 不是 方阵 比如说 有N个/nr 学生 
每个 学生 有M科/nr 成绩 这样 形成 的 一个 N * 
M 的 矩阵 就 不 可能 是 方阵 我们 怎样 
才能 描述 这样 普通 的 矩阵 呢 的 重要 特征 
呢 奇异 值 分解 可以 用来 干 这个 事情 奇异/a 
值/n 分解/v 是/v 一个/m 能/v 适用/v 于/p 任意/v 的/uj 矩阵/n 
的/uj 一种/m 分解/v 的/uj 方法/n 假设 A 是 一个 N 
* M 的 矩阵 那么 得到 的 U 是 一个 
N * N 的 方阵 里面 的 向量 是 正交 
的 U 里面 的 向量 称为 左 奇异 向量 Σ 
是 一个 N * M 的 矩阵 除了 对角线 的 
元素 都是 0 对角 线上 的 元素 称为 奇异 值 
V V 的 转置 是 一个 N * N 的 
矩阵 里面 的 向量 也是 正交 的 V 里面 的 
向量 称为 右 奇异 向量 从/p 图片/n 来/v 反映/v 几个/m 
相乘/v 的/uj 矩阵/n 的/uj 大小/b 可得/v 下面/f 的/uj 图片/n 那么/r 
奇异/a 值/n 和/c 特征值/n 是/v 怎么/r 对应/vn 起来/v 的/uj 呢/y 
首先 我们 将 一个 矩阵 A 的 转置 * A 
将会 得到 一个 方阵 我们 用 这个 方阵 求 特征值 
可以 得到       这里 得到 的 v 就是 
我们 上 面的 右 奇异 向量 此外 我们 还 可以 
得到 这里 的 σ 就是 上面 说 的 奇异 值 
u 就是 上面 说 的 左 奇异 向量 奇异 值 
σ 跟 特征值 类似 在 矩阵 Σ 中 也 是从 
大 到 小 排列 而且 σ 的 减少 特别 的 
快 在 很多 情况 下 前 10% 甚至 1% 的 
奇异 值 的 和就/nr 占了 全部 的 奇异 值 之和 
的 99% 以上 了 也 就是说 我们 也 可以 用 
前 r 大 的 奇异 值 来 近似 描述 矩阵 
这里 定义 一下 部分 奇异 值 分解 r 是 一个 
远 小于 m n 的 数 这样 矩阵 的 乘法 
看起来 像是 下面 的 样子 右边 的 三个 矩阵 相乘 
的 结果 将会 是 一个 接近于 A 的 矩阵 在这儿 
r/w 越/d 接近/v 于n/nr 则/d 相乘/v 的/uj 结果/n 越/d 接近/v 
于A/nr 而 这三个 矩阵 的 面积 之和 在 存储 观点 
来说 矩阵 面积 越小 存储量 就 越小 要 远远 小于 
原始 的 矩阵 A 我们 如果 想 要 压缩 空间 
来 表示 原 矩阵 A 我们 存下 这里 的 三个 
矩阵 U Σ V 就 好了 二 奇异 值 的 
计算 奇异 值 的 计算 是 一个 难题 是 一个 
O N ^ 3 的 算法 在 单机 的 情况 
下 当然 是 没问题 的 matlab 在 一秒钟 内 就 
可以 算出 1000 * 1000 的 矩阵 的 所有 奇异 
值 但是 当 矩阵 的 规模 增长 的 时候 计算 
的 复杂度 呈 3 次方 增长 就 需要 并行计算 参与 
了 Google 的 吴军 老师 在 数学 之美 系列 谈到 
SVD 的 时候 说起 Google 实现 了 SVD 的 并行 
化 算法 说 这是 对 人类 的 一个 贡献 但是 
也 没有 给 出 具体 的 计算 规模 也 没有 
给 出 太多 有价值 的 信息 其实 SVD 还是 可以 
用 并行 的 方式 去 实现 的 在 解大/nr 规模 
的 矩阵 的 时候 一般 使用 迭代 的 方法 当 
矩阵 的 规模 很大 比如说 上亿 的 时候 迭代 的 
次数 也 可能 会上 亿次 如果 使用 Map Reduce 框 
架去 解 则 每次 Map Reduce 完成 的 时候 都会 
涉及 到 写 文件 读 文件 的 操作 个人 猜测 
Google 云计算 体系 中 除了 Map Reduce 以外 应该 还有 
类似 于 MPI 的 计算 模型 也 就是 节点 之间 
是 保持 通信 数据 是 常驻 在 内存 中的 这种 
计算 模型 比 Map Reduce 在 解决 迭代 次数 非常多 
的 时候 要快 了 很多 倍 Lanczos 迭代 就是 一种 
解 对称 方阵 部分 特征值 的 方法 之前 谈到 了 
解A/nr * A 得到 的 对称 方阵 的 特征值 就是 
解A的/nr 右 奇异 向量 是 将 一个 对称 的 方程 
化为 一个 三 对角 矩阵 再 进行 求解 按 网上 
的 一些 文献 来看 Google 应该 是 用 这种 方法 
去做 的 奇异 值 分解 的 请见 Wikipedia 上面 的 
一些 引用 的 论文 如果 理解 了 那些 论文 也 
几乎 可以 做出 一个 SVD 了 由于 奇异 值 的 
计算 是 一个 很 枯燥 纯数学 的 过程 而且 前人 
的 研究 成果 论文 中 几乎 已经 把 整个 程序 
的 流程图 给 出来了 更多 的 关于 奇异 值 计算 
的 部分 将在 后面 的 参考 文献 中 给出 这里 
不 再 深入 我 还是 focus 在 奇异 值 的 
应用 中去 三 奇异 值 与 主 成分 分析 PCA 
主 成分 分析 在上 一节 里面 也 讲 了 一些 
这里 主要 谈谈 如何 用 SVD 去 解 PCA 的 
问题 PCA 的 问题 其实 是 一个 基 的 变换 
使得/v 变换/v 后的/nr 数据/n 有着/nr 最大/a 的/uj 方差/n 方差 的 
大小 描述 的 是 一个 变量 的 信息量 我们 在 
讲 一个 东西 的 稳定性 的 时候 往往 说 要 
减小 方差 如果 一个 模型 的 方差 很大 那就 说明 
模型 不 稳定 了 但是 对于 我们 用于 机器 学习 
的 数据 主要 是 训练 数据 方差 大才 有意义 不然 
输入 的 数据 都是/nr 同一个 点 那 方差 就为 0 
了 这样 输入 的 多个 数据 就 等同 于 一个 
数据 了 以 下面 这张 图为 例子 这个 假设 是 
一个 摄像机 采集 一个 物体 运动 得到 的 图片 上面 
的 点 表示 物体 运动 的 位置 假如 我们 想 
要用 一条 直线 去 拟合 这些 点 那 我们 会 
选择 什么 方向 的 线呢 当然 是 图上 标有 signal 
的 那条 线 如果 我们 把 这些 点 单纯 的 
投影 到 x 轴 或者 y 轴上 最后 在 x 
轴 与 y 轴上 得到 的 方差 是 相似 的 
因为 这些 点 的 趋势 是 在 45度 左右 的 
方向 所以 投影 到 x 轴 或者 y 轴上 都是 
类似 的 如果 我们 使用 原来 的 xy 坐标系 去看 
这些 点 容易 看 不 出来 这些 点 真正 的 
方向 是 什么 但是 如果 我们 进行 坐标系 的 变化 
横轴 变成 了 signal 的 方向 纵轴 变成 了 noise 
的 方向 则 就 很容易 发现 什么 方向 的 方差 
大 什么 方向 的 方差 小了 一般来说 方差 大 的 
方向 是 信号 的 方向 方差 小 的 方向 是 
噪声 的 方向 我们 在 数据 挖掘 中 或者 数字 
信号 处理 中 往往 要 提高 信号 与 噪声 的 
比例 也 就是 信噪比 对 上图 来说 如果 我们 只 
保留 signal 方向 的 数据 也 可以 对 原 数据 
进行 不错 的 近似 了 PCA 的 全部 工作 简单 
点 说 就是 对 原始 的 空间 中 顺序 地 
找 一组 相互 正交 的 坐标轴 第一 个 轴 是 
使得 方差 最大 的 第二个 轴 是 在与 第一个 轴 
正交 的 平面 中 使得 方差 最大 的 第三个 轴 
是 在与 第 1 2个 轴 正交 的 平面 中 
方差 最大 的 这样 假设在 N 维空间 中 我们 可以 
找到 N 个 这样 的 坐标轴 我们 取 前 r 
个 去 近似 这个 空间 这样 就 从 一个 N 
维 的 空间 压缩 到 r 维 的 空间 了 
但是 我们 选择 的 r 个 坐标轴 能够 使得 空间 
的 压缩 使得 数据 的 损失 最小 还是 假设 我们 
矩阵 每 一行 表示 一个 样本 每 一列 表示 一个 
feature 用 矩阵 的 语言 来 表示 将 一个 m 
* n 的 矩阵 A 的 进行 坐标轴 的 变化 
P 就是 一个 变换 的 矩阵 从 一个 N 维 
的 空间 变换 到 另一个 N 维 的 空间 在 
空间 中 就会 进行 一些 类似于 旋转 拉伸 的 变化 
而将 一个 m * n 的 矩阵 A 变换 成 
一个 m * r 的 矩阵 这样 就 会 使得 
本来 有n个/nr feature 的 变成 了 有r个/nr feature 了 r 
n 这 r 个 其实 就是 对 n 个 feature 
的 一种 提炼 我们 就 把 这个 称为 feature 的 
压缩 用 数学 语言 表示 就是 但是 这个 怎么 和 
SVD 扯上 关系 呢 之前 谈到 SVD 得出 的 奇异 
向量 也 是从 奇异 值 由 大 到 小 排列 
的 按 PCA 的 观点 来看 就是 方差 最大 的 
坐标轴 就是 第一 个 奇异 向量 方差 次大 的 坐标轴 
就是 第二个 奇异 向量 我们 回忆 一下 之前 得到 的 
SVD 式子 在 矩阵 的 两边 同时 乘 上一个 矩阵 
V 由于 V 是 一个 正交 的 矩阵 所以 V 
转置 乘以 V 得到 单位阵 I 所以 可以 化成 后面 
的 式子 将 后面 的 式子 与 A * P 
那个 m * n 的 矩阵 变换 为 m * 
r 的 矩阵 的 式子 对照 看看 在 这里 其实 
V 就是 P 也 就是 一个 变化 的 向量 这里 
是 将 一个 m * n 的 矩阵 压缩 到 
一个 m * r 的 矩阵 也 就是 对列 进行 
压缩 如果 我们 想 对 行 进行 压缩 在 PCA 
的 观点 下 对 行 进行 压缩 可以 理解 为 
将 一些 相似 的 sample 合并 在 一起 或者 将 
一些 没有 太大 价值 的 sample 去掉 怎么办 呢 同样 
我们 写出 一个 通用 的 行 压缩 例子 这样 就 
从 一个 m 行 的 矩阵 压缩 到 一个 r 
行 的 矩阵 了 对 SVD 来说 也 是 一样 
的 我们 对 SVD 分解 的 式子 两边 乘以 U 
的 转置 U 这样 我们 就 得到 了 对 行 
进行 压缩 的 式子 可以 看出 其实 PCA 几乎 可以 
说是 对 SVD 的 一个 包装 如果 我们 实现 了 
SVD 那也 就 实现 了 PCA 了 而且 更好 的 
地方 是 有了 SVD 我们 就 可以 得到 两个 方向 
的 PCA 如果 我们 对 A A 进行 特征值 的 
分解 只能 得到 一个 方向 的 PCA 四 奇异 值 
与 潜在 语义 索引 LSI 潜在 语义 索引 Latent Semantic 
Indexing 与 PCA 不太 一样 至少 不 是 实现 了 
SVD 就 可以 直接 用 的 不过 LSI 也 是 
一个 严重 依赖 于 SVD 的 算法 之前 吴军 老师 
在 矩阵 计算 与 文本处理 中的 分类 问题 中 谈到 
三个 矩阵 有 非常 清楚 的 物理 含义 第一 个 
矩阵 X 中的 每 一行 表示 意思 相关 的 一类 
词 其中 的 每个 非零 元素 表示 这类 词中 每个 
词 的 重要性 或者说 相关性 数值 越大 越 相关 最后 
一个 矩阵 Y 中的 每 一列 表示 同一 主题 一类 
文章 其中 每个 元素 表示 这类 文章 中 每篇 文章 
的 相关性 中间/f 的/uj 矩阵/n 则/d 表示/v 类/q 词/n 和/c 
文章/n 雷/n 之间/f 的/uj 相关性/l 因此 我们 只要 对 关联矩阵 
A 进行 一次 奇异 值 分解 w 我们 就 可以 
同时 完成 了 近义词 分类 和 文章 的 分类 同时 
得到 每类 文章 和 每类 词 的 相关性 上面 这段话 
可能 不 太 容易 理解 不过 这 就是 LSI 的 
精髓 内容 我 下面 举 一个 例子 来 说明 一下 
下面 的 例子 来自 LSA tutorial 具体 的 网址 我 
将在 最后 的 引用 中 给出 这 就是 一个 矩阵 
不过 不 太 一样 的 是 这里 的 一行 表示 
一个词 在 哪些 title 中 出现 了 一行 就是 之前 
说 的 一维 feature 一列 表示 一个 title 中有 哪些 
词 这个 矩阵 其实 是 我们 之前 说 的 那种 
一行 是 一个 sample 的 形式 的 一种 转置 这个 
会 使得 我们 的 左右 奇异 向量 的 意义 产生 
变化 但是 不会 影响 我们 计算 的 过程 比如说 T1 
这个 title 中 就有 guide investing market stock 四个 词 
各 出现 了 一次 我们 将 这个 矩阵 进行 SVD 
得到 下面 的 矩阵 左 奇异 向量 表示 词 的 
一些 特性 右 奇异 向量 表示 文档 的 一些 特性 
中间 的 奇异 值 矩阵 表示 左 奇异 向量 的 
一行 与 右 奇异 向量 的 一列 的 重要 程序 
数字 越大 越 重要 继续 看 这个 矩阵 还 可以 
发现 一些 有意思 的 东西 首先 左 奇异 向量 的 
第一 列 表示 每 一个 词 的 出现 频繁 程度 
虽然 不是 线性 的 但是 可以 认为 是 一个 大概 
的 描述 比如 book 是 0.15 对应 文档 中 出现 
的 2次 investing 是 0.74 对应 了 文档 中 出现 
了 9次 rich 是 0.36 对应 文档 中 出现 了 
3次 其次 右 奇异 向量 中一 的 第一 行 表示 
每 一篇 文档 中 的 出现 词 的 个数 的 
近似 比如说 T6 是 0.49 出现 了 5个 词 T2 
是 0.22 出现 了 2个 词 然后 我们 反 过头 
来看 我们/r 可以/c 将/d 左/m 奇异/a 向量/n 和右/nr 奇异/a 向量/n 
都取后/nr 2/m 维/v 之前 是 3 维 的 矩阵 投影 
到 一个 平面 上 可以 得到 在 图上 每 一个 
红色 的 点 都 表示 一个词 每 一个 蓝色 的 
点 都 表示 一篇 文档 这样 我们 可以 对 这些 
词 和 文档 进行 聚 类 比如说 stock 和 market 
可以 放在 一类 因为 他们 老 是 出现 在 一起 
real 和 estate 可以 放在 一类 dads guide 这种 词 
就 看起来 有点 孤 立了 我们 就 不 对 他们 
进行 合并 了 按 这样 聚 类 出现 的 效果 
可以 提取 文档 集合 中的 近义词 这样 当 用户 检索 
文档 的 时候 是 用 语义 级别 近义词 集合 去 
检索 了 而 不是 之前 的 词 的 级别 这样 
一 减少 我们 的 检索 存储量 因为 这样 压缩 的 
文档 集合 和 PCA 是 异曲同工 的 二 可以 提高 
我们 的 用户 体验 用户 输入 一个 词 我们 可以 
在 这个 词 的 近义词 的 集合 中 去找 这是 
传统 的 索引 无法 做到 的 不 知道 按 这样 
描述 再 看看 吴军 老师 的 文章 是不是 对 SVD 
更 清楚 了 D 参考资料 1 A Tutorial on Principal 
Component Analysis Jonathon Shlens 这是 我 关于 用 SVD 去做 
PCA 的 主要 参考 资料 2 http / / www 
. ams . org / samplings / feature column / 
fcarc svd 关于 svd 的 一篇 概念 好文 我 开头 
的 几个 图 就是 从 这儿 截取 的 3 http 
/ / www . puffinwarellc . com / index . 
php / news and articles / articles / 30 singular 
value decomposition tutorial . html 另一 篇 关于 svd 的 
入门 好文 4 http / / www . puffinwarellc . 
com / index . php / news and articles / 
articles / 33 latent semantic analysis tutorial . htmlsvd 与 
LSI 的 好文 我 后面 LSI 中 例子 就是 来 
自此 5 http / / www . miislita . com 
/ information retrieval tutorial / svd lsi tutorial 1 understanding 
. html 另一 篇 svd 与 LSI 的 文章 也 
还是 不错 深 一点 也 比较 长 6 Singular Value 
Decomposition and Principal Component Analysis Rasmus Elsborg Madsen Lars Kai 
Hansen and Ole Winther 2004 跟 1 里面 的 文章 
比较 类似 