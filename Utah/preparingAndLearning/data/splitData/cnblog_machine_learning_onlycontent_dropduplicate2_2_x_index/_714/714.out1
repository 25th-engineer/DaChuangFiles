版权 声明 本文 由 LeftNotEasy 发布 于 http / / 
leftnoteasy . cnblogs . com 本文 可以 被 全部 的 
转载 或者 部分 使用 但 请 注明 出处 如果 有 
问题 请 联系 wheeleast @ gmail . com 也 可以 
加 我 的 微博   @ leftnoteasy 前言 决策树 这种 
算法 有着 很多 良好 的 特性 比如说 训练 时间 复杂度 
较低 预测 的 过程 比较 快速 模型 容易 展示 容易 
将 得到 的 决策 树 做成 图片 展示 出来 等 
但是 同时 单/n 决策树/n 又/d 有/v 一些/m 不好/d 的/uj 地方/n 
比如说 容易 over fitting 虽然 有 一些 方法 如 剪枝 
可以 减少 这种 情况 但是 还是 不够 的 模型 组合 
比如说 有 Boosting Bagging 等 与 决策树 相关 的 算法 
比较 多 这些 算法 最终 的 结果 是 生成 N 
可能会 有 几百棵 以上 棵树 这样 可以 大大 的 减少 
单 决策树 带来 的 毛病 有点 类似于 三个臭皮匠 等于 一个 
诸葛亮 的 做法 虽然 这 几百棵 决策树 中的 每 一棵 
都 很简单 相对于 C 4.5 这种 单 决策树 来说 但是 
他们 组合 起来 确 是 很 强大 在 最近 几年 
的 paper 上 如 iccv 这种 重量级 的 会议 iccv 
09年 的 里面 有 不少 的 文章 都是 与 Boosting 
与 随机 森林 相关 的 模型 组合 + 决策树 相关 
的 算法 有 两种 比较 基本 的 形式 随机 森林 
与 GBDT Gradient Boost Decision Tree 其他 的 比较 新的 
模型 组合 + 决策树 的 算法 都是/nr 来自 这 两种 
算法 的 延伸 本文 主要 侧重 于 GBDT 对于 随机 
森林 只是 大概 提提 因为 它 相对 比较 简单 在看 
本文 之前 建议 先 看看 机器学习 与 数学 3 与 
其中 引用 的 论文 本文 中的 GBDT 主要 基于 此 
而 随机 森林 相对 比较 独立 基础 内容 这里 只 
是 准备 简单 谈谈 基础 的 内容 主要 参考 一下 
别人 的 文章 对于 随机 森林 与 GBDT 有 两个 
地方 比较 重要 首先 是 information gain 其次是 决策树 这里 
特别 推荐 Andrew Moore 大牛 的 Decision Trees Tutorial 与 
Information Gain Tutorial Moore 的 Data Mining Tutorial 系列 非常 
赞 看懂 了 上面 说 的 两个 内容 之后 的 
文章 才能 继续 读 下去 决策树 实际上 是 将 空间 
用 超平面 进行 划分 的 一种 方法 每次 分割 的 
时候 都将 当前 的 空间 一分为二 比如说 下面 的 决策树 
就是 将 空间 划分 成 下面 的 样子 这样 使得 
每一个 叶子 节点 都是 在 空间 中 的 一个 不 
相交 的 区域 在 进行 决策 的 时候 会 根据 
输入 样本 每 一维 feature 的 值 一步 一步 往下 
最后 使得 样本 落入 N 个 区域 中 的 一个 
假设有 N 个 叶子 节点 随机 森林 Random Forest 随机 
森林 是 一个 最近 比较 火 的 算法 它 有 
很多 的 优点 在 数据 集上 表现 良好 在 当前 
的 很多 数据 集上 相对 其他 算法 有着 很大 的 
优势 它 能够 处理 很高 维度 feature 很多 的 数据 
并且 不 用做 特征选择 在 训练 完 后 它 能够 
给 出 哪些 feature 比较 重要 在 创建 随机 森林 
的 时候 对 generlization error 使用 的 是 无偏估计 训练 
速度快 在 训练 过程 中 能够 检测 到 feature 间 
的 互相 影响 容易 做成 并行 化 方法 实现 比较简单 
随机 森林 顾名思义 是 用 随机 的 方式 建立 一个 
森林 森林 里面 有 很多 的 决策 树 组成 随机 
森林 的 每 一棵 决策树 之间 是 没有 关联 的 
在 得到 森林 之后 当 有 一个 新的 输入 样本 
进入 的 时候 就让 森林 中的 每 一棵 决策树 分别 
进行 一 下判断 看看 这个 样 本 应该 属于 哪 
一类 对于 分类 算法 然后 看看 哪 一类 被 选择 
最多 就 预测 这 个样 本为 那一类 在 建立 每 
一棵 决策树 的 过程 中 有 两点 需要 注意 采样 
与 完全 分裂 首先 是 两个 随机 采样 的 过程 
random forest 对 输入 的 数据 要 进行 行 列 
的 采样 对于 行 采样 采用 有 放回 的 方式 
也 就是 在 采样 得到 的 样本 集合 中 可能 
有 重复 的 样本 假设 输入 样 本为 N 个 
那么 采样 的 样本 也为 N 个 这样 使得 在 
训练 的 时候 每 一棵树 的 输入 样本 都 不是 
全部 的 样本 使得 相对 不 容易 出现 over fitting 
然后 进行 列 采样 从M个/nr feature 中 选择 m 个 
m M 之后 就是 对 采样 之后 的 数据 使用 
完全 分裂 的 方式 建立 出 决策树 这样 决策树 的 
某一个 叶子 节点 要么 是 无法 继续 分裂 的 要么 
里面 的 所有 样本 的 都是 指向 的 同一 个 
分类 一般 很多 的 决策 树 算法 都 一个 重要 
的 步骤 剪枝 但是 这里 不 这样 干 由于 之前 
的 两个 随机 采样 的 过程 保证 了 随机性 所以 
就算 不 剪枝 也 不会 出现 over fitting 按 这种 
算法 得到 的 随机 森林 中的 每 一棵 都是 很弱 
的 但是 大家 组合 起来 就 很 厉害 了 我 
觉得 可以 这样 比喻 随机 森林 算法 每 一棵 决策树 
就是 一个 精通于 某一个 窄 领域 的 专家 因为 我们 
从M个/nr feature 中 选择 m 让 每 一棵 决策树 进行 
学习 这样 在 随机 森林 中 就有 了 很多 个 
精通 不同 领域 的 专家 对 一个 新的 问题 新的 
输入 数据 可以 用 不同 的 角度 去 看待 它 
最终 由 各个 专家 投票 得到 结果 随机 森林 的 
过程 请 参考 Mahout 的 random forest 这个 页面 上 
写 的 比较 清楚 了 其中 可能 不 明白 的 
就是 Information Gain 可以 看看 之前 推荐 过 的 Moore 
的 页面 Gradient Boost Decision Tree GBDT 是 一个 应用 
很 广泛 的 算法 可以 用来 做 分类 回归 在 
很多 的 数据 上 都有 不错 的 效果 GBDT 这个 
算法 还有 一些 其他 的 名字 比如说 MART Multiple Additive 
Regression Tree GBRT Gradient Boost Regression Tree Tree Net 等 
其实 它们 都是/nr 一个 东西 参考 自 wikipedia – Gradient 
Boosting 发明者 是 FriedmanGradient Boost 其实 是 一个 框架 里面 
可以 套 入 很多 不同 的 算法 可以 参考 一下 
机器学习 与 数学 3 中的 讲解 Boost 是 提升 的 
意思 一般 Boosting 算法 都是/nr 一个 迭代 的 过程 每一次 
新的 训练 都是/nr 为了 改进 上 一次 的 结果 原始 
的 Boost 算法 是 在 算法 开始 的 时候 为 
每一个 样本 赋 上一个 权重 值 初始 的 时候 大家 
都是/nr 一样 重要 的 在 每一步 训练 中 得到 的 
模型 会使 得数 据点 的 估计 有对 有错 我们 就 
在 每一步 结束 后 增 加分 错 的 点 的 
权重 减少 分对 的 点 的 权重 这样 使得 某些 
点 如果 老是 被 分 错 那么 就 会被 严重 
关注 也 就被 赋 上一个 很高 的 权重 然后 等 
进行 了 N 次 迭代 由 用户 指定 将会 得到 
N 个 简单 的 分类器 basic learner 然后 我们 将 
它们 组合 起来 比如说 可以 对 它们 进行 加权 或者 
让 它们 进行 投票 等 得到 一个 最终 的 模型 
而 Gradient Boost 与 传统 的 Boost 的 区别 是 
每一次 的 计算 是 为了 减少 上 一次 的 残差 
residual 而 为了 消除 残差 我们 可以 在 残差 减少 
的 梯度 Gradient 方向 上 建立 一个 新 的 模型 
所以 说 在 Gradient Boost 中 每个 新 的 模型 
的 简历 是 为了 使得 之前 模型 的 残差 往 
梯度方向 减少 与 传统 Boost 对 正确 错误 的 样本 
进行 加权 有着 很大 的 区别 在 分类 问题 中 
有 一个 很 重要 的 内容 叫做 Multi Class Logistic 
也 就是 多 分类 的 Logistic 问题 它 适用 于 
那些 类 别数 2 的 问题 并且在 分类 结果 中 
样本 x 不是 一定 只 属于 某一个 类 可以 得到 
样本 x 分别 属于 多个 类 的 概率 也 可以 
说 样本 x 的 估计 y 符合 某一个 几何 分布 
这 实际上 是 属于 Generalized Linear Model 中 讨论 的 
内容 这里 就 先不 谈了 以后 有 机会 再 用 
一个 专门 的 章节 去做 吧 这里 就 用 一个 
结论 如果 一个 分 类 问题 符合 几何 分布 那么 
就 可以 用 Logistic 变换 来 进行 之后 的 运算 
假设 对于 一个 样本 x 它 可能 属于 K 个 
分类 其 估计值 分别为 F1 x FK x Logistic 变换 
如下 logistic 变换 是 一个 平滑 且 将 数据 规范化 
使得 向量 的 长度 为 1 的 过程 结果 为 
属于 类别 k 的 概率 pk x 对于 Logistic 变换 
后的/nr 结果 损失 函数 为 其中 yk 为 输入 的 
样本数据 的 估计值 当 一个 样本 x 属于 类别 k 
时 yk = 1 否则 yk = 0 将 Logistic 
变换 的 式子 带入 损失 函数 并且 对其 求导 可以 
得到 损失 函数 的 梯度 上面 说 的 比较 抽象 
下面 举个 例子 假设 输入 数据 x 可能 属于 5个 
分类 分别为 1 2 3 4 5 训练 数据 中 
x 属于 类别 3 则 y = 0 0 1 
0 0 假设 模型 估计 得到 的 F x = 
0 0.3 0.6 0 0 则 经过 Logistic 变换 后的/nr 
数据 p x = 0.16 0.21 0.29 0.16 0.16 y 
p 得到 梯度 g 0.16 0.21 0.71 0.16 0.16 观察 
这里 可以 得到 一个 比较 有意思 的 结论 假设 gk 
为 样本 当某 一维 某一个 分类 上 的 梯度 gk 
0时 越大 表示 其 在 这 一维 上 的 概率 
p x 越 应该 提高 比如说 上面 的 第三 维 
的 概率 为 0.29 就 应该 提高 属于 应 该往 
正确 的 方向 前进 越小 表示 这个 估计 越 准确 
gk 0时 越小 负 得 越多 表示 在 这 一维 
上 的 概率 应该 降低 比如说 第 二维 0.21 就 
应该 得到 降低 属于 应该 朝着 错误 的 反方向 前进 
越大 负 得 越少 表示 这个 估计 越 不 错误 
总的来说 对于 一个 样本 最 理想 的 梯度 是 越 
接近 0 的 梯度 所以 我们 要 能够 让 函数 
的 估计值 能够 使得 梯度 往 反方向 移动 0 的 
维度 上 往 负 方向 移动 0 的 维度 上 
往 正方向 移动 最终 使得 梯度 尽量 = 0 并且 
该 算法 在 会 严重 关注 那些 梯度 比 较大 
的 样本 跟 Boost 的 意思 类似 得到 梯度 之后 
就是 如何 让 梯度 减少 了 这里 是 用 的 
一个 迭代 + 决策树 的 方法 当 初始化 的 时候 
随便 给 出 一个 估计 函数 F x 可以 让 
F x 是 一个 随机 的 值 也 可以 让 
F x = 0 然后 之后 每 迭代 一步 就 
根据 当前 每一个 样本 的 梯度 的 情况 建立 一棵 
决策树 就让 函数 往 梯度 的 反方向 前进 最终 使得 
迭代 N 步后/nr 梯度 越小 这里 建立 的 决策 树 
和 普通 的 决策 树 不太 一样 首先 这个 决策树 
是 一个 叶子 节 点数 J 固定 的 当 生成 
了 J 个 节点 后 就 不再 生成 新的 节 
点了 算法 的 流程 如下 参考 自 treeBoost 论文 0 
. 表示 给 定 一个 初始值 1 . 表示 建立 
M 棵 决策树 迭代 M 次 2 . 表示 对 
函数 估计值 F x 进行 Logistic 变换 3 . 表示 
对于 K 个 分类 进行 下面 的 操作 其实 这个 
for 循环 也 可以 理解 为 向量 的 操作 每一个 
样本点 xi 都 对应 了 K 种 可能 的 分类 
yi 所以 yi F xi p xi 都是/nr 一个 K 
维 的 向量 这样 或许 容易 理解 一点 4 . 
表示 求得 残差 减少 的 梯度方向 5 . 表示 根据 
每 一个 样本点 x 与其 残差 减少 的 梯度方向 得到 
一棵 由 J 个 叶子 节点 组成 的 决策树 6 
. 为 当 决策树 建立 完成后 通过 这个 公式 可以 
得到 每 一个 叶子 节点 的 增益 这个 增益 在 
预测 的 时候 用 的 每个 增益 的 组成 其实 
也 是 一个 K 维 的 向量 表示 如果 在 
决策树 预测 的 过程 中 如果 某 一个 样本点 掉入 
了 这个 叶子 节点 则 其 对应 的 K 个 
分类 的 值 是 多少 比如说 GBDT 得到 了 三棵 
决策树 一个 样本点 在 预测 的 时候 也会 掉入 3个 
叶子 节点 上 其 增益 分别为 假设 为 3 分类 
的 问题 0.5 0.8 0.1   0.2 0.6 0.3   
0.4 0.3 0.3 那么 这样 最终 得到 的 分类 为 
第二个 因为 选择 分类 2 的 决策树 是 最多 的 
7 . 的 意思 为 将 当前 得到 的 决策树 
与 之前 的 那些 决策树 合并 起来 作为 新 的 
一个 模型 跟 6 中所 举 的 例子 差不多 GBDT 
的 算法 大概 就 讲到 这里 了 希望 能够 弥补 
一下 上 一篇 文章 中 没有 说 清楚 的 部分 
实现 看 明白 了 算法 就 需要 去 实现 一下 
或者 看看 别人 实现 的 代码 这里 推荐 一下 wikipedia 
中的 gradient boosting 页面 下面 就 有 一些 开源 软件 
中 的 一些 实现 比如说 下面 这个 http / / 
elf project . sourceforge . net / 参考资料 除了 文章 
中的 引用 的 内容 已经 给 出了 链接 外 主要 
还是 参考 Friedman 大牛 的 文章 Greedy function approximation A 
Gradient Boosting Machine 