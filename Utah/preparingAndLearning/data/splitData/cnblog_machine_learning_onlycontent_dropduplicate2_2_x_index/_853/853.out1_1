近几年 机器学习 异常 火爆 可以 用 来 解决 各种各样 的 
问题 但却 很少 有人 意识到 机器学习 本身 也 容易 受到 
攻击 终于 Ian Goodfellow 和 Papernot 首次 将 机器学习 的 
攻击 提出 并且 做 了 很多 非常 重要 的 研究 
这里 给出 这 二位 大牛 的 博客 的 翻译 有兴趣 
的 朋友 可以 关注 一下 觉得 还是 很 有意思 的 
研究 本文 也是 安全 方面 的 学习 有兴趣 的 希望 
可以 一起 讨论 学习 ~ ~ 转载 请 注明 出处 
一 背景 直到 几年 前 机器学习/i 算法/n 在/p 许多/m 有/v 
意义/n 的/uj 任务/n 上/f 都/d 没有/v 很好/i 地/uv 发挥/v 作用/v 
比如 识别 物体 或 翻译 因此 当 机器学习 算法 没能 
做 正确 的 事情 时 这是 规则 而 不是 例外 
今天 机器学习 算法 已经 进入 了 下 一个 发展 阶段 
当 呈现 自然 产生 的 输入 时 它们 可以 比 
人类 表现 得 更好 机器学习 还 没有 达到 真正 的 
人类 水平 因为 当 面对 一个 微不足道 的 对手 时 
大多数 机器学习 算法 都 失败 了 换句话说 我们 已经 达到 
了 机器 学习 的 目的 但 很容易 被 打破 这篇 
博客 文章 介绍 了 我们 新的 Clever Hans 博客 我们 
将 讨论 攻击者 破坏 机器学习 算法 的 各种 方法 从 
学术 角度 讲 我们 的 话题 是 机器 学习 的 
安全性 和 保密性 这个 博客 是由 Lan Goodfellow 和 /nr Nicolas 
Papernot 共同 撰写 的 Lan 是 OpenAI 的 一名 研究 
科学家 也是 宾夕法尼亚州 立 大学 安全 研究 的 博士生 我们 
共同 创建 了 开源 库   cleverhans 用来 对 机器学习 
模型 的 脆弱性 进行 基准测试 这个 博客 为 我们 提供 
了 一种 非正式 的 分享 关于 机器学习 安全 和 隐私 
的 想法 这些 想法 对于 传统 的 学术 出版 来说 
还 不够 具体 还 可以 分享 与 cleverhans   库 
相关 的 新闻 和 更新 二 机器学习 安全 与 隐私 
一个 安全 的 系统 是 可以 依赖 的 并且 可以 
保证 像 预期 的 一样 运行 当 我们 试图 为 
系统 的 行为 提供 担保 时 我们 会 想到 一个 
特定 的 威胁 模型 威胁 模型 是 一组 正式 定义 
的 关于 任何 攻击者 的 能力 和 目标 的 假设 
这些 攻击 者 可能 希望 系统 的 行为 不 正常 
到 目前 为止 大多数 机器学习 都是 用 一个 非常 弱 
的 威胁 模型 来 开发 的 在 这个 模型 中 
没有 对手 机器学习 系统 的 设计 是 为了 在 面对 
自然 时 表现 出 正确 的 行为 今天 我们 开始 
设计 机器学习 系统 即使 面对 一个 恶意 的 人 或 
一个 恶意 的 机器学习 对手 我们 也 能 做出 正确 
的 行为 例如 机器学习 系统 可能 在 模型 被 训练 
学习 阶段 或 模型 进行 预测 推理 阶段 时被/nr 对手 
攻击 对手 也 有 不同 程度 的 能力 可能 包括 
对模型 内部 结构 和 参数 的 访问 或者 对模型 输入 
和 输出 的 访问 为了 破坏 机器学习 模型 攻击者 可以 
破坏 其 机密性 完整性 或 可用性 这些 性质 构成 了 
CIA confidentiality   integrity or   availability 的 安全 模型 
保密性 机器学习 系统 必须 保证 未 得到 授权 的 用户 
无法 接触 到 信息 在 实际 操作 中 把 保密性 
作为 隐私 性 来 考虑 会 容易 得多 就是说 模型 
不可以 泄露 敏感数据 比如 假设 研究员 们 设计 了 一个 
可以 检查 病人 病历 给 病人 做 诊断 的 机器学习 
模型 这样 的 模型 可以 对 医生 的 工作 起到 
很大 的 帮助 但是 必须 要 保证 持有 恶意 的 
人 没办 法分析 这个 模型 也 没 办法 把 用来 
训练 模型 的 病人 数据恢复 出来 完整性 如果 攻击者 可以 
破坏 模型 的 完整性 那么 模型 的 预测 结果 就 
可能会 偏离 预期 比如 垃圾邮件 会 把 自己 伪装成 正常 
邮件 的 样子 造成 垃圾 邮件 识别器 的 误 识别 
可用性 系统 的 可用性 也 可以 成为 攻击 目标 比如 
如果 攻击者 把 一个 非常 难以 识别 的 东西 放在 
车辆 会 经过 的 路边 就 有可能 迫使 一辆 自动 
驾驶 汽车 进入 安全 保护 模式 然后 停车 在 路边 
三 机器学习 攻击 方法 当然 到 目前 为止 所有 这些 
都是/nr 假设 的 到 目前 为止 安全 研究 人员 已经 
证明 了 哪些 类型 的 攻击 本 博客 的 后续 
文章 将 会给 出 更多 的 例子 但是 我们 从三个/nr 
方面 开始 在 训练 时的/nr 完整性 攻击 在 推理 过程 
中 的 完整性 攻击 以及 隐私 攻击 3.1 在 训练 
集中 下毒   在 训练 时 对模型 进行 完整性 攻击 
攻击者 可以 通过 修改 现有 训练 数据 或者 给 训练 
集 增加 额外 数据 的 方法 来 对 训练 过程 
的 完整性 造成 影响 比如 假设 莫里亚蒂 教授 要给 福尔摩斯 
栽赃 一个 罪名 他 就 可以 让 一个 没 被 
怀疑 的 同伙 送给 福尔摩斯 一双 独特 华丽 的 靴子 
当 福尔摩斯 穿着 这双 靴子 在 他 经常 协助 破案 
的 警察 面前 出现 过 以后 这些 警察 就 会把 
这双 靴子 和他/nr 联系 起来 接下来 莫里亚蒂 教授 就 可以 
穿 一双 同样 的 靴子 去 犯罪 留下 的 脚印 
会 让 福尔摩斯 成为 被 怀疑 的 对象 干扰 机器学习 
模型 的 训练 过程 体现 的 攻击 策略 是 当 
用于 生产 时让/nr 机器学习 模型 出现 更多 错误 预测 具体来说 
这样 的 方法 可以 在 支持 向量 机 SVM 的 
训练 集中 下毒 由于 算法 中 预测 误差 是 以 
损失 函数 的 凸 点 衡量 的 这 就让 攻击者 
有机会 找到 对 推理 表现 影响 最大 的 一组 点 
进行 攻击 BNL12 即便 在 更 复杂 的 模型 中 
也 有可能 找到 高效 的 攻击点 深度 神经 网络 就是 
这样 只要 它们 会 用到 凸 优化 3.2 用 对抗性 
的 样本 让 模型 出错   在 推理 时 进行 
完整性 攻击 实际上 让 模型 出错 是 非常 简单 的 
一件 事情 以至于 攻击者 都没 必要 花 功夫 在 训练 
机器学习 模型 参数 的 训练 集中 下毒 他们 只要 在 
推理 阶段 模型 训练 完成 之后 的 输入 上 动 
动手脚 就 可以 立即 让 模型 得出 错误 的 结果 
要 找到 能让 模型 做出 错误 预测 的 干扰 有 
一种 常用 方法 是 计算 对抗性 样本   SZS13 . 
它们 带 有的 干扰 通常 很 微小 人类 很难 发现 
但 它们 却 能 成功 地 让 模型 产生 错误 
的 预测 比如 下面 这张 图   GSS14 用 机器学习 
模型 识别 最 左侧 的 图像 可以 正确 识别 出来 
这 是 一只 熊猫 但是 对 这张 图像 增加 了 
中间 所示 的 噪声 之后 得到 的 右侧 图像 就 
会被 模型 识 别成 一只 长臂猿 而且 置信度 还 非常 
高 值得 注意 的 是 虽然 人类 无法 用 肉眼 
分辨 但是 图像 中 施加 的 干扰 已经 足以 改变 
模型 的 预测 结果 确实 这种 干扰 是 在 输入 
领域 中 通过 计算 最小 的 特定 模 得到 的 
同时 它 还能 增大 模型 的 预测误差 它 可以 有效地 
把 本来 可以 正确 分类 的 图像 移过 模型 判定 
区域 的 边界 从而 成为 另一种 分类 下面 这张 图 
就是 对于 能 分出 两个 类别 的 分类器 出现 这种 
现象 时候 的 示意 许多 基于 对抗性 样本 的 攻击 
需要 攻击者 知道 机器学习 模型 中 的 参数 才能 把 
所需 的 干扰 看作 一个 优化 问题 计算出来 另一方面 也 
有 一些 后续 研究 考虑 了 更 现实 的 威胁 
模型 这种 模型 里 攻击者 只能 跟 模型 互动 给 
模型 提供 输入 以后 观察 它 的 输出 举例 来讲 
这种/r 状况/n 可以/c 发生/v 在/p 攻击者/n 想要/v 设计/vn 出/v 能/v 
骗过/v 机器学习/i 评/v 分系统/l 从而/c 得到/v 高/a 排名/v 的/uj 网站/n 
页面/n 或者 设计 出 能 骗过 垃圾邮件 筛选器 的 垃圾 
邮件 的 时候 在 这些 黑盒 情境 中 机器学习 模型 
的 工作 方式 可以 说 像 神谕 一样 发起 攻击 
的 策略 首先 对 神谕 发起 询问 对模型 的 判定 
区域 边界 做出 一个 估计 这样 的 估计 就 成为 
了 一个 替代 模型 然后 利用 这个 替代 模型 来 
制作 会被 真正 的 模型 分类 错误 的 对抗性 样本 
  PMG16 这样 的 攻击 也 展现 出了 对抗性 样本 
的 可 迁移性 用来 解决 同样 的 机器 学习 任务 
的 不同 的 模型 即便 模型 与 模型 之间 的 
架构 或者 训练 数据 不 一样 对抗性 样本 还是 会 
被 不同 的 模型 同时 误判 SZS13 3.3 机器学习 中的 
隐私 问题 机器学习 中的 隐私 问题 就 不需要 攻击者 也能 
讲 明白 了 例如 说 机器学习 算法 缺乏 公平性 和 
透明性 的 问题 已经 引起 领域内 越来越 多 人 的 
担心 事实上 已经 有人 指出 训练 数据 中 带有 的 
社会 偏见 会 导致 最终 训练 完成后 的 预测 模型 
也 带有 这些 偏见 下面 重点 说一说 在有 攻击者 情况下 
的 隐私 问题 攻击者 的 目的 通常 是 恢复 一 
部分 训练 机器学习 模型 所用 的 数据 或者 通过 观察 
模型 的 预测 来 推断 用户 的 某些 敏感 信息 
举例来说 智能 手机 的 虚拟键盘 就 可以 通过 学习 用户 
的 输入 习惯 达到 更好 的 预测 自动 完成 效果 
但是 某 一个 用户 的 输入 习 惯下 的 特定 
字符 序列 不 应该 也 出现 在 别的 手机 屏幕 
上 除非 已经 有 一个 比例 足够 大 的 用户 
群 也 会打 同样 的 一串 字符 在 这样 的 
情况 下 隐私 攻击 会 主要 在 推理 阶段 发挥作用 
不过 要 缓解 这个 问题 的话 一般 都 需要 在 
学习 算法 中 增加 一些 随机性 CMS11 比如 攻击者 有 
可能 会 想 办法 进行 成员 推测 查询 想 要知道 
模型 训练 中 有 没有 使用 某个 特定 的 训练 
点 近期 就 有 一篇 论文 在 深度 神经网络 场景 
下 详细 讨论 了 这个 问题 与 制作 对抗性 样本 
时对/nr 梯度 的 用法 相反 SSS16 这 可以 改变 模型 
对 正确 答案 的 置信度 成员 推测 攻击 会 沿着 
梯度方向 寻找 分类 置信度 非常 高的点/nr 已经 部署 的 模型 
中 也 还 可以 获得 有关 训练 数据 的 更多 
总体 统计 信息 AMS15 四 总结 现在 是 2016年 12月 
目前 我们 知道 许多 攻击 机器学习 模式 的 方法 而且 
很 少有 防御 的 方法 我们 希望 到 2017年 12月 
我们 将 有更/nr 有效 的 防御 措施 这个 博客 的 
目标 是 推动 机器学习 安全 和 隐私 的 研究 状态 
通过 记录 他们 所 发生 的 进展 在 涉及 到 
这些 话题 的 研究 人员 的 社区 内 引发 讨论 
并 鼓励 新一代 的 研究 人员 加入 这个 社区 References 
AMS15 Ateniese G . Mancini L . V . Spognardi 
A . Villani A . Vitali D . & Felici 
G . 2015 . Hacking smart machines with smarter ones 
How to extract meaningful data from machine learning classifiers . 
International Journal of Security and Networks 10 3 137 150 
. BS16 Barocas . & Selbst A . D . 
2016 . Big data s disparate impact . California Law 
Review 104 . BNL12 Biggio B . Nelson B . 
& Laskov P . 2012 . Poisoning attacks against support 
vector machines . arXiv preprint arXiv 1206.6389 . CMS11 Chaudhuri 
K . Monteleoni C . & Sarwate A . D 
. 2011 . Differentially private empirical risk minimization . Journal 
of Machine Learning Research 12 Mar 1069 1109 . GSS03 
Garfinkel . Spafford G . & Schwartz A . 2003 
. Practical UNIX and Internet security . O Reilly Media 
Inc . GSS14 Goodfellow I . J . Shlens J 
. & Szegedy C . 2014 . Explaining and harnessing 
adversarial examples . arXiv preprint arXiv 1412.6572 . PMG16 Papernot 
N . McDaniel P . Goodfellow I . Jha . 
Berkay Celik Z . & Swami A . 2016 . 
Practical Black Box Attacks against Deep Learning Systems using Adversarial 
Examples . arXiv preprint arXiv 1602.02697 . PMS16 Papernot N 
. McDaniel P . Sinha A . & Wellman M 
. 2016 . Towards the Science of Security and Privacy 
in Machine Learning . arXiv preprint arXiv 1611.03814 . SSS16 
Shokri R . Stronati M . & Shmatikov V . 
2016 . Membership Inference Attacks against Machine Learning Models . 
arXiv preprint arXiv 1610.05820 . SZS13 Szegedy C . Zaremba 
W . Sutskever I . Bruna J . Erhan D 
. Goodfellow I . & Fergus R . 2013 . 
Intriguing properties of neural networks . arXiv preprint arXiv 1312.6199 
. 原文 链接 http / / www . cleverhans . 
io / security / privacy / ml / 2016 / 
12/15 / breaking things is easy . html 转载 请 
注明 出处 