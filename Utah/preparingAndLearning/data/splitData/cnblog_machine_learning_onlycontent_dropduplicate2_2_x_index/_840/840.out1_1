转载 请 注明 出处 http / / www . cnblogs 
. com / ymingjingr / p / 4271742 . html 
目录 机器学习 基石 笔记 1 在 何时 可以 使用 机器学习 
1 机器学习 基石 笔记 2 在 何时 可以 使用 机器学习 
2 机器学习 基石 笔记 3 在 何时 可以 使用 机器学习 
3 修改版 机器学习 基石 笔记 4 在 何时 可以 使用 
机器学习 4 机器学习 基石 笔记 5 为什么 机器 可以 学习 
1 机器学习 基石 笔记 6 为什么 机器 可以 学习 2 
机器学习 基石 笔记 7 为什么 机器 可以 学习 3 机器学习 
基石 笔记 8 为什么 机器 可以 学习 4 机器学习 基石 
笔记 9 机器 可以 怎样 学习 1 机器学习 基石 笔记 
10 机器 可以 怎样 学习 2 机器学习 基石 笔记 11 
机器 可以 怎样 学习 3 机器学习 基石 笔记 12 机器 
可以 怎样 学习 4 机器学习 基石 笔记 13 机器 可以 
怎样 学 得 更好 1 机器学习 基石 笔记 14 机器 
可以 怎样 学 得 更好 2 机器学习 基石 笔记 15 
机器 可以 怎样 学 得 更好 3 机器学习 基石 笔记 
16 机器 可以 怎样 学 得 更好 4 十一 Linear 
Models for Classification 用于 分类 的 线性 模型 11.1 Linear 
Models for Binary Classification 用于 二元 分类 的 线性 模型 
目前 叙述 的 算法 模型 主要有 3类 线性 二元 分类 
线性 回归 logistic 回归 这三个 模型 的 最主要 的 相同 
点在 假设 函数 和 错误 函数 中都 出现 了 线性 
得分 函数 linear scoring function 如 公式 11 1 所示 
公式 11 1 三类 模型 与 得分 s 之间 的 
关系 如 1 所示 1 三类 模型 与 得分 s 
的 关系 最左 为 线性 二元 分类 其 假设 函数 
为 一般 使用 0/1 错误 通过 求解 最优 权值 向量 
w 比较 困难 中间 为 线性 回归模型 其 假设 函数 
为 一般 使用 平方 错误 可 直接 通过 解析 解 
求解 最优 w 最右 为 logistic 回归模型 假设 函数 为 
使用 交叉 熵 错误 通过 梯度 下 降法 求出 近似 
的 w 从 上述 分析 不难看出 线性 二元 分类 问题 
的 求解 方式 最为 困难 但 与 另外 两种 模型 
存在着 共同点 得分 s 能否 利用 这 两种 模型 的 
算法 近似 求得 二分 类 问题 的 最优 w 呢 
回顾 10.2节 logistic 回归 的 错误 可用 符号 表示 其中 
CE 为 交叉 熵 cross entropy 的 缩写 可以 写成 
公式 11 2 所示 公式 11 2 是否 二元 分类 
模型 和 线性 回归模型 的 错误 函数 可以 写成 关于 
的 形式 答案 是 可以 的 如 2 所示 2 
三类 模型 的 错误 函数 二元 分类 模型 和 线性 
回归模型 错误 函数 中的 转换 都用 到了 的 性质 接着 
观察 三类 模型 的 错误 函数 与 ys 之间 的 
关系 本节 开头 回顾 了 s 的 物理 意义 为 
得分 此处 ys 的 物理 意义 是 正确 的 得分 
因此 ys 越大 越好 表示 两者 接近 且 同号 根据 
2中 的 三类 模型 的 错误 函数 有关 ys 的 
公式 可以 得出 如 3 所示 的 关系 图 3 
三类 模型 的 错误 函数 与 ys 的 关系 图 
其中 蓝色 的 折线 表示 0/1 错误 在 ys 大于 
0时 反之 红色 的 抛物线 表示 平方 错误 在 时与 
在 该 范围内 所 表现 出 的 特征 相似 但是 
在 时与 在 该 范围内 所 表达 的 效果 相去甚远 
因此 只有 在 很小 的 情况 下 可以 使用 取代 
墨绿 的 曲线 表示 同样 如 3 所示 也 只有 
在 很小 的 情况 下 和 可 互相 取代 但是 
跟 想得到 的 错误 曲线 还有 一些 差距 因此 略 
做 转变 得到 公式 11 3 公式 11 3 其中 
表示 缩放 的 scaled 即对 做 了 一个 换 底 
因此 可以 得到 4 4 关于 ys 的 图 如 
4中 墨绿色 的 线 表示 从 图中 可以 看出 该 
错误 函数 很 适合 做 的 上限 在 很小 的 
情况 下 和 可 互相 取代 如 公式 11 4 
所示 公式 11 4 通过 公式 11 4 可以 得出 
和的/nr 上限 如 公式 11 5 和 公式 11 6 
所示 公式 11 5 公式 11 6 再通过 VC 限制 
理论 可以 得到 公式 11 7 公式 11 7 第一 
个 不等号 连接 的 是 在 VC 限制 下 和其/nr 
上界 概念 见 7.4节 其中 函数 也是 在 7.4节 中 
提到 过 的 模型 复杂度 在 二元 分类 中 可以 
写成 的 形式 因此 得到 如 下结论 小 的 可以 
通过 小 的 得出 同理 可以 证明 小 的 也 
可以 通过 小 的 得出 即 线性 回归模型 和 logistic 
回归模型 可以 用作 二元 分类 算法 流程 一般 是 在 
输出 空间 的 情况 下 通过/p 线性/n 回归/v 和/c logistic/w 
回归/v 相/v 对应/vn 的/uj 求解/v 方法/n 求出/v 最优/d 将 求得 
的 代入 公式 sign 得到 最优 假设 函数 三类 模型 
做 分类 的 利弊 分析 如表 11 1 所示 表 
11 1 三类 模型 做 分类 的 利弊 分析 二元 
分类 线性 回归 Logistic 回归 好处 在 线性 可分 的 
情况下 可以 保证 完成 最 容易 的 优化 算法 容易 
的 优化 算法 坏 处在 线性 不 可分 的 情况 
需要 使用 启发式 pocket 在 非常大 时 相对于 是 一个 
很 宽松 的 上界 在 ys 为 负 时 是 
一个 宽松 的 上界 线性 回归 一般 只 作为 PLA 
pocket logistic 回归 的 初始 向量 logistic 回归 经常 取代 
pocket 算法 11.2 Stochastic Gradient Descent 随机 梯度 下降 如 
公式 11 8 为 迭代 优化 算法 的 通式 学过 
的 PLA 的 迭代 算 法如 公式 11 9 logistic 
回 归中 梯度 下降 的 迭代 公式 如 公式 11 
10 公式 11 8 公式 11 9 公式 11 10 
对比 以上 两种 迭代 优化 方法 PLA 与 logistic 回归 
的 梯度 下降 发现 PLA 只 需要 通过 一个 样本点 
便可 计算出 即 每次 迭代 的 时间 复杂度 为 logistic 
回归 的 梯度 下降 需要 遍历 所有 的 样本 点 
才能 计算出 即 每次 迭代 的 时间 复杂度 为 有 
无可 能将 logistic 回归 每次 迭代 时间 复杂度 降为 观察 
公式 11 10 方向 向量 v v ≈ 该 梯度 
是 通过 所有 的 样本点 加权 求和 再取 平均 得到 
的 如何 使用 一个 样本点 的 取值 近似 整体 的 
平均值 可以 将 求 平均 的 过程 理解 为求 期望值 
此处 使用 在 N 个 样本 中 随机 抽取 一个 
样本点 求出 的 梯度 取代 原来 的 期望 梯度 这种 
随机 选取 的 梯度 称为 随机 梯度 stochastic gradient 可用 
符号 表示 而 真实 的 梯度 与 随机 梯度 的 
关系 如 公式 11 11 公式 11 11 随机 梯度 
值 可以 看做 真实 的 梯度 值 加上 一个 噪音 
使用 随机 梯度 取代 真实 梯度 做 梯度 下降 的 
算法 称作 随机 梯度 下降 stochastic gradient descent 简称 SGD 
这种 替代 的 理论 基础 是 在 迭代 次数 足够 
多 的 情况 下 平均 的 随机 梯度 和 平均 
的 真实 梯度 相差 不大 该 算法 的 优点 是 
简单 容易 计算 适用于 大 数据 或者 流式 数据 缺点 
是 不 稳定 Logistic 回归 的 随机 梯度 下降 的 
迭代 如 公式 11 12 所示 公式 11 12 是否 
联想到 了 其他 的 迭代 算法 PLA 如 公式 11 
13 所示 公式 11 13 因此 logistic 回归 随机 梯度 
下降 类似于 软 的 PLA 为什么 称为 软 的 原因 
是 它 的 之前 的 权值 并 没有 那么 绝对 
不是 1 就是 0 而是 一个 在 0 ~ 1 
之间 的 值 在 公式 11 12中 如果 且 始终 
是 一个 很大 的 值 则 logistic 回归 随机 梯度 
下降 相当 于是 PLA SGD 需要 调试 两个 参数 迭代 
步骤 t 和 学习 速率 调试 迭代 步骤 是 因为 
不 知道 真实 的 梯度 值 是否 接近 0 只能 
假设 足够 步数 后是/nr 已经 做到 足够 好 即 通常 
设置 一个 大 的 数值 作为 步数 学习 速率 通常 
也 很难 选定 林 老师 推荐 的 是 数字 为 
0.1126 11.3 Multiclass via Logistic Regression 通过 logistic 回归 实现 
多 类别 分类 多 类别 分类 有 许多 应用 场景 
特别 是 在 识别 recognition 领域 如 5 为 输出 
空间 y 为 四 类别 的 情况 即 5 四分 
类 问题 实际 多 类别 问题 也 可以 使用 二元 
分类 问题 的 思路 进行 分类 如 将 原 四类 
问题 分解 为 是否 为 即将 与 其他 的 类别 
分离 生成 一个 新 的 二元 分类 问题 即 通过 
此 方式 得到 一个 分类 超平面 如 6 所示 6 
以 是否 为 进行 二元 分类 同理 可以 以 是否 
为 生成 一个 新的 二元 分类 问题 即 该 分类 
超平面 如 7 所示 7 以 是否 为 进行 二元 
分类 另外 两种 情况 就不 一一列举 最终 以 是否 为 
每个 类别 得到 的 二元 分 类如 8 8 四个 
类别 各自 的 二元 分类 情况 当 将 8 的 
四种 情况 合并 在 一个 图中 会 发现 有 一些 
无法 处理 的 情形 如 9 所示 8 四种 情况 
合并 图 其中 四个 边缘 的 三角 阴影 所在 的 
区域 为 相邻 两个 类别 都 争夺 的 区域 如 
最 上方 的 三角 区域 是 类别 和 类别 重叠 
的 区域 还有 图 正中 的 区域 又 不属于 任何 
类别 这些 问题 如何 解决 使用 以前 学过 的 软性 
分类 还是 关于 类别 的 二元 分类 问题 此处 不再 
使用 硬 划分 而是 使用 该 样本点 是 的 可能性 
即 如 9 所示 9 关于 类别 的 软化 分 
余下 三 种 情况 不 再 一一 举例 最终/d 得到/v 
的/uj 四种/m 类别/n 的/uj 分类/n 情况/n 和/c 合并/v 后的/nr 情况/n 
分别/d 如/v 10 和 11 所示 10 四个 类别 各自 
的 软 二元 分类 情况 11 四个 类别 软 二元 
分类 合并 后 情况 如何 判断 样本点 属于 哪个 类别 
可以 分别 计算 样本 点在 四种 软 二元 分类 情况 
下 概率 选择 其中 概率 最大 的 一个 作为 所属 
类别 如 公式 11 14 所示 公式 11 14 其中 
求 概率 的 公式 使用 logistic 函数 k 表示 类别 
注意到 logistic 函数 是 一个 单调 函数 因此 可以 消去 
该 函数 直接 使用 个 类别 的 得分 值 作比较 
如 公式 11 5 所示 公式 11 15 用此 种 
思路 设计 的 算法 称作 一对多 One Versue All 简称为 
OVA 表示 一个 类别 对 其他 所有 类别 算法 流程 
如下 在整个 训练 数据集 D 上 在 y = k 
时为 + 1 y ≠ k 时为 1 符号 取 
1 或者 0 使用 logistic 函数 计算 各 个 类别 
的 权值 向量 返回 假设 函数 g 该 算法 的 
优点 是 简单 有效 易于 类似于 logistic 函数 的 二元 
分类 问题 扩展 成多/nr 类别 分类 缺点 是 当 类别 
特别 多时 产生 了 不 平衡 的 现象 如 类别 
特别 多 则 + 1 的 数据 量 就 很少 
大部分 都是 1 数据 量 严重 不 平衡 11.4 Multiclass 
via Binary Classification 通过 二元 分类 实现 多 类别 分类 
上 一节 的 最后 提到 OVA 的 方式 在 类别 
非常多 的 情况 下 出现 了 训练 数据 严重 失衡 
的 现象 于是 本节 介绍 一种 应对 这类 不 平衡 
问题 的 方法 还是 上节 中 使用 的 四分 类 
问题 不像 OVA 在整个 数据 集中 计算 是否 为 的 
权值 向量 w 此种 方法 是 任意 选择 四类 中的 
两类 如 类别 和 类别 将 两个 类别 分别 设为 
+ 1 和 1 形式 如 在 包含 两类 的 
数据 集上 计算 权值 向量 w 如 12 12 类别 
和 类别 的 二分 类如 上述 情况 相同 从 四种 
类别 中 选取 两种 做 二元 分类 一共 可得 6种 
对比 各对 比如 13 所示 13 6种 对比 情况 如 
13 得到 6个 不同 的 权值 向量 w 如何 判断 
某 新进 样本 属于 哪个 分类 如 11 14中 紫色 
的 样本 点在 6中 情况下 所属 的 类别 前 三种 
属于 第 4种 属于 后 两种 属于 只 需要 找 
出在 所有 对比 中 胜利 次数 最多 的 类别 因此 
该点 属于 这种 方式 如同 投票选举 样本点 属于 所有 类别 
对比 中 赢得 次数 最多 的 那种 类别 14 某 
点在 6种 情形 下 的 隶属 情况 这种 分类 方式 
称为 一对一 one vervuse one 简称 OVO 其 算法 流程 
如下 所有 类别 的 任意 两个 类别 做 对比 使用 
二元 分类 算法 在 数据集 D 求出 最佳 的 权值 
向量 通过 投票 返回 假设 函数 g 其 优点 是 
简单 有效 在做 两两 对比 时 每次 使用 的 不是 
全部 训练 数据 而是 仅 属于 当前 两类 的 训练 
数据 能将/i 所有/b 类似/v 于/p 二元/m 分类/n 的/uj 算法/n 扩展/v 
成/n 多元/m 分类/n 问题/n 缺点 是 对比 次数 是 即 
其中 K 表示 类 别数 因此 就 需要 花费 更多 
的 存储空间 计算 时间 