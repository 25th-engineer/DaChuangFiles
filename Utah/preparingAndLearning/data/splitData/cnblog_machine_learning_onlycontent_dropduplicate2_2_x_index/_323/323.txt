伪代码实现：LR、梯度下降、最小二乘、KNN、Kmeans;
LR,SVM,XGBOOST推公式(手推)
LR,SVM,RF,KNN，EM，Adaboost,PageRank，GBDT，Xgboost，HMM，DNN，推荐算法，聚类算法，等等机器学习领域的算法
基本知识：
1）监督与非监督区别；
是否有监督（supervised），就看输入数据是否有标签（label）。输入数据有标签，则为有监督学习，没标签则为无监督学习。
半监督学习：综合利用有类标的数据和没有类标的数据，来生成合适的分类函数。利用少量标注样本和大量未标注样本进行机器学习，从概率学习角度可理解为研究如何利用训练样本的输入边缘概率 P( x )和条件输出概率P ( y | x )的联系设计具有良好性能的分类器。
2）L1L2区别；
（核心：L2对大数，对outlier离群点更敏感！）
下降速度：最小化权值参数L1比L2变化的快
模型空间的限制：L1会产生稀疏 L2不会。
L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。
3）生成模型和判别模型区别 像贝叶斯，lda 等就是生成模型，计算过概率分布之类的
监督学习分为生成模型和判别模型。
生成模型：由数据学习联合概率分布，然后求出条件概率分布作为预测的模型。给定x产生出y的生成关系。
eg：朴素贝叶斯、隐马尔科夫
判别模型：由数据直接学习决策函数或者条件概率分布作为预测模型。给定x应该预测什么样的输出y。
eg：KNN、感知机、决策树、逻辑斯蒂回归、最大熵、svm、提升方法、条件随机场
算法的优缺点以及相应解决方案：k-means, KNN, apriori
算法原理：LR、KNN、k-means、apriori、ID3（C45,CART）、SVM、神经网络，协同过滤，em算法
常见问题：
1）svm算法的原理、如何组织训练数据、如何调节惩罚因子、如何防止过拟合、svm的泛化能力、增量学习
1、间隔最大的分离超平面
2、min 2/|w| st yi(wi+b)>=1
3、拉格朗日乘子法算凸二次规划
对偶 max(a)min(w,b)L(w,b,a)
w b
4、f=sign(wx+b)
C间隔(C>0)-间隔越大 错误分类惩罚大 间隔越小 对错误分类惩罚小
调整C正则化特征
泛化能力强
2）神经网络参数相关。比如，参数的范围？如何防止过拟合？隐藏层点的个数多了怎样少了怎样？什么情况下参数是负数？
初始权重 -0.5-0.5 0-1
隐藏节点个数
传输函数
学习速率(太大不稳定 太小太慢)
减少隐藏节点个数
多了过拟合
不起作用？？
3）为什么要用逻辑回归？
逻辑回归的优点：
1.实现简单；
2.分类计算量小、速度快，存储资源低
缺点：
1、容易欠拟合，一般准确度不太高
2、只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分；
4）决策树算法是按什么来进行分类的?
ID3 信息增益
C4.5 信息增益率
CART 基尼系数(二叉树)
5) 朴素贝叶斯公式
P(Y|X)=P(X,Y)/P(X)=(P(X|Y)*P(Y))/P(X) –通过朴素贝叶斯条件独立展开
P(A|B)=P(B|A)*P(A)/P(B)
对于给出的待分类项，求解在此项出现的条件下各个目标类别出现的概率，哪个最大，就认为此待分类项属于哪个类别
7）svm中rbf核函数与高斯核函数的比较
在SVM的应用中，径向基函数就是指高斯核函数；
exp(-(x-c)^2/r^2) r方差c均值 svm里面 c为xj
8）说一下SVM的实现和运用过程
见上
10）简单说说决策树分析
特征选择、生成树、剪枝 if-then规则
12）SVM有哪些优势，（x,y,z）三个特征如何用径向基核函数抽取第四维特征
SVM算法优点：
可用于线性/非线性分类，也可以用于回归；
低泛化误差；
容易解释；
缺点：
对参数和核函数的选择比较敏感；
原始的SVM只比较擅长处理二分类问题；
14）如何用Logic regression建立一个广告点击次数预测模型
输入x
用户特征
人口属性如年龄、性别、收入;兴趣标签;历史点击率
广告特征
广告类别、广告id、广告主id、行业、素材图像特征等
上下文特征
广告位、地域、时间、操作系统、浏览器等
输出(h(x))
用户是否会点击某个广告(点击的概率是多大)
特征处理-离散化-交叉特征-归一-onehot
用户->从广告集合里规则抽取部分->ctr预估这部分广告
15）举一个适合采用层次分析法的例子
目标->准则->方案
构建对比较矩阵(某一准则比重)
排序计算方案权重
买钢笔->价格外观质量->可供选择的笔
17）关联分析中的极大频繁项集；FP增长算法
1、扫描数据库一遍，得到频繁项的集合F和每个频繁项的支持度。把F按支持度递降排序。
2、构造原始FPTree 以null为根 顺序插入
3、构建频繁项集。同一个频繁项在PF树中的所有节点的祖先路径的集合。
比如I3在FP树中一共出现了3次，其祖先路径分别是{I2，I1：2(频度为2)}，{I2：2}和{I1：2}。这3个祖先路径的集合就是频繁项I3的条件模式基。
22) 如何解决过拟合问题
简化模型-正则化系数
23) L1和L2正则的区别，如何选择L1和L2正则
见上
24) 随机森林的学习过程
随机森林是有很多随机得决策树构成，它们之间没有关联。得到RF以后，在预测时分别对每一个决策树进行判断，最后使用Bagging的思想进行结果的输出
学习过程:
现在有N个训练样本，每个样本的特征为M个，需要建K颗树
1从N个训练样本中有放回的取N个样本作为一组训练集（其余未取到的样本作为预测分类，评估其误差）
2从M个特征中取m个特征左右子集特征(m<M)
25) 随机森林中的每一棵树是如何学习的
决策树
26) 随机森林学习算法中CART树的基尼指数是什么
变量的不确定性
30） k-mean shift的机制，能不能用伪码实现
迭代
初始K个中心点(第二次开始使用上次生成的中心点)
map计算每个样本距离那个近 输出 中心 样本
reduce 计算每个簇的新的中心点 满足停止条件停止 不满足输出 新中心点
31）实现最小二乘法。
1/2(h(x)-y)求偏导
32）Bagging与Boosting
Bagging
从N样本中有放回的采样N个样本
对这N个样本在全属性上建立分类器(CART,SVM)
重复上面的步骤，建立m个分类器
预测的时候使用投票的方法得到结果
Boosting
boosting在训练的时候会给样本加一个权重，然后使loss function尽量去考虑那些分错类的样本（比如给分错类的样本的权重值加大）
18）线性分类器与非线性分类器的区别及优劣
线性分类器：模型是参数的线性函数，分类平面是（超）平面；
非线性分类器：模型分界面可以是曲面或者超平面的组合。 可以解决线性不可分问题(异或问题)
典型的线性分类器有感知机，LDA，逻辑斯特回归，SVM（线性核）；
典型的非线性分类器有kNN，决策树，SVM（非线性核）
朴素贝叶斯（有文章说这个本质是线性的， http://dataunion.org/12344.html ）
用MapReduce写好友推荐，userCF和ItemCF
在一堆单词里面找出现次数最多的k个
map-每个map的cleanup()输出topk
reduce-汇总map topk
hadoop原理：
HDFS+MapReduce
shuffle如何排序：
MapReduce提供Partitioner接口，它的作用就是根据key或value及reduce的数量来决定当前的这对输出数据最终应该交由哪个reduce task处理，默认对key hash后再以reduce task数量取模。
Shuffle在map端的过程：1、输入数据经过mapper输出key-value对 2经过分区操作、内存的排序、每个分组合并操作
Shuffle在reduce端的过程：1Copy map结果过程 2 Merge阶段 3生成Reducer的输入文件
map如何切割数据：
splitSize不小于minSize以及不大于blockSiz，如果map任务的文件分割大小splitSize超过blockSize，那么每个map任务可能需要跨多个datanode获取文件，会导致额外增加网络开销，因为通常一个map会被分配在该文件所在的datanode上运行，这样本地读取文件，则不需要网络开销。mapper函数对block的每行数据进行处理
如何处理数据倾斜：
1.增加reduce 的jvm内存 单个值有大量记录的不适用
2.增加reduce 个数 单个值有大量记录的不适用 唯一值较多的适用(某些reduce分配了多个较多记录)
3.明白数据倾斜的分布自己实现分区
4.单独处理倾斜数据-实现新的key使不倾斜 特定情况特殊分析
join的mr代码：
reduce side join-跨机器传输数据量大
map同时读取两个文件File1和File2 标签来自文件1还是文件2
reduce函数获取key相同的来自File1和File2文件的value list进行join（笛卡尔乘积)
map side join- 小表连大表
小表复制到每个map task
map扫描大表 在hash table中查找是否有相同的key的记录连接输出
SemiJoin-半连接优化reduce side join
File1的join的key抽出放入内存中File3(较小)
map期间File2在File3的进行reduce不在的跳过(减少了map端的输出)
动态规划
动态规划：待求解的问题分解为若干个子阶段(下一阶段需要上一阶段结果)
初始状态→│决策１│→│决策２│→…→│决策ｎ│→结束状态
1）问题的阶段
2）每个阶段的状态
3）从前一个阶段转化到后一个阶段之间的递推关系。
树结构
class TreeNode{
int value;
TreeNode left;
TreeNode right;
}
链表结构
每个节点Node都有一个值val和指向下个节点的链接next
class Node {
int val;
Node next;
Node(int x) {
val = x;
next = null;
}
}
SVM原理-SVM核技巧原理，如何选择核函数
泛化误差界的公式为：
R(w)≤Remp(w)+Ф(n/h)
公式中R(w)就是真实风险，Remp(w)就是经验风险(分类器在给定样本上的误差)，Ф(n/h)就是置信风险(多大程度上可以信任分类器在未知数据上分类的结果)。
统计学习的目标从经验风险最小化变为了寻求经验风险与置信风险的和最小，即结构风险最小。
SVM正是这样一种努力最小化结构风险的算法。
SVM算法要求的样本数是相对比较少的(小样本，并不是说样本的绝对数量少)
非线性，是指SVM擅长应付样本数据线性不可分的情况，主要通过松弛变量（也有人叫惩罚变量）和核函数技术来实现
硬间隔支持向量机(线性分类器)
软间隔支持向量机(线性分类器)
非线性支持向量机(核技巧与软间隔最大化)
线性可分：线性核函数
线性不可分：选择非线性核函数：多项式核函数、高斯核函数、拉普拉斯核函数、sigmoid核函数
PageRank原理
PageRank两个基本假设：
1.数量假设：入链数量越多，那么这个页面越重要。
2.质量假设：越是质量高的页面指向页面A，则页面A越重要。
利用以上两个假设，PageRank算法刚开始赋予每个网页相同的重要性得分，通过迭代递归计算来更新每个页面节点的PageRank得分，直到得分稳定为止。
步骤如下：
1）在初始阶段：网页通过链接关系构建起Web图，每个页面设置相同的PageRank值，通过若干轮的计算，会得到每个页面所获得的最终PageRank值。随着每一轮的计算进行，网页当前的PageRank值会不断得到更新。
2）在一轮中更新页面PageRank得分的计算方法：在一轮更新页面PageRank得分的计算中，每个页面将其当前的PageRank值平均分配到本页面包含的出链上，这样每个链接即获得了相应的权值。而每个页面将所有指向本页面的入链所传入的权值求和，即可得到新的PageRank得分。当每个页面都获得了更新后的PageRank值，就完成了一轮PageRank计算。
优点：
是一个与查询无关的静态算法，所有网页的PageRank值通过离线计算获得；有效减少在线查询时的计算量，极大降低了查询响应时间。
缺点：
1）人们的查询具有主题特征，PageRank忽略了主题相关性，导致结果的相关性和主题性降低
2）旧的页面等级会比新页面高。因为即使是非常好的新页面也不会有很多上游链接，除非它是某个站点的子站点。
http://blog.csdn.net/hguisu/article/details/7996185
AUC的定义和本质，有哪些计算方法
ROC曲线AUC为ROC曲线下的面积 越大分类越准
ROC曲线
横轴：负正类率FPR FP / (FP+TN=N) 直观解释：实际是0负中，错猜多少
纵轴：真正类率TPR TP / (TP+FN=P) 直观解释：实际是1正的中，猜对多少
auc的直观含义是任意取一个正样本和负样本，正样本得分大于负样本的概率。
分类器能输出score：
1.先把score排序一边扫描一边计算AUC近似的认为是一个一个矩形面积累加(阶梯状的)计算麻烦
2.统计一下所有的 M×N(M为正类样本的数目，N为负类样本的数目)个正负样本对中，有多少个组中的正样本的score大于负样本的score。当二元组中正负样本的 score相等的时候，按照0.5计算。然后除以MN。实现这个方法的复杂度为O(n^2)。n为样本数（即n=M+N）
3.对score从大到小排序，最大score对应的sample 的rank为n，第二大score对应sample的rank为n-1，以此类推
然后把所有的正类样本的rank相加，再减去正类样本的score为最小的那M个值的情况。得到的就是所有的样本中有多少对正类样本的score大于负类样本的score。然后再除以M×N。即
AUC=((所有的正例位置相加)-(M*(M+1)/2))/(M*N)
http://blog.csdn.net/pzy20062141/article/details/48711355
gbdt和xgboost区别
传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。
传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。
xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。
Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）
列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。
对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。
xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。
可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。
https://www.zhihu.com/question/41354392/answer/128008021?group_id=773629156532445184
具体怎么做预处理，特征工程，模型融合常用方式，融合一定会提升吗
标准化
数据归一化规范化Normalization
特征二值化Binarization
类别数据编码 OneHot 编码
标签二值化
类别编码
生成多项式特征
常见融合框架原理，优缺点，bagging，stacking，boosting，为什么融合能提升效果
个人理解是按照不同的思路来组合基础模型，在保证准确度的同时也提升了模型防止过拟合的能力。针对弱学习器(泛化能力弱)效果明显，个体学习器满足：1好而不同，具有多样性2不能太坏
Boosting(串行-减少偏差)
Bagging(并行-减少方差)
Stacking
不同模型之间有差异，体现不同表达能力
2G内存里找100TB数据的中位数
如何在海量数据中查找给定部分数据最相似的top200向量，向量的维度也很高
KD树，聚类，hash
散列分治：大文件散列映射多个小文件-小文件top-合并大文件top堆排序/快排
找出5亿个int型数的中位数：
首先将这5亿个int型数划分为2^16个区域，然后读取数据统计落到各个区域里的数的个数，根据统计结果就可以判断中位数落到哪个区域，并知道这个区域中的第几大数刚好是中位数。然后，第二次扫描只统计落在这个区域中的那些数就可以了。
http://www.epubit.com.cn/article/290
为什么LR需要归一化或者取对数，为什么LR把特征离散化后效果更好
目的是它能够让它符合我们所做的假设，使我们能够在已有理论上对其分析
LR更适合处理稀疏数据
逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；(哑变量)
特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。
为什么把特征组合之后还能提升
反正这些基本都是增强了特征的表达能力，或者说更容易线性可分吧
单链表如何判断有环
算法的思想是设定两个指针p, q，其中p每次向前移动一步，q每次向前移动两步。那么如果单链表存在环，则p和q相遇；否则q将首先遇到null。
(p和q同时在操场跑步，其中q的速度是p的两倍，当他们两个同时出发时，p跑一圈到达起点，而q此时也刚 好跑完两圈到达起点。)
http://www.cnblogs.com/chengyeliang/p/4454290.html
从大数据中找出topk
http://www.epubit.com.cn/article/290
各个损失函数之间区别
http://blog.csdn.net/google19890102/article/details/50522945 http://blog.csdn.net/shenxiaoming77/article/details/51614601
哪些优化方法，随机梯度下降，牛顿拟牛顿原理
http://www.tuicool.com/articles/EfInM3Q
特征选择方法有哪些(能说出来10种以上加分)
相关系数法 使用相关系数法，先要计算各个特征对目标值的相关系
构建单个特征的模型，通过模型的准确性为特征排序，借此来选择特征
通过L1正则项来选择特征：L1正则方法具有稀疏解的特性，因此天然具备特征选择的特性
(分别使用L1和L2拟合，如果两个特征在L2中系数相接近，在L1中一个系数为0一个不为0，那么其实这两个特征都应该保留，原因是L1对于强相关特征只会保留一个)
训练能够对特征打分的预选模型：RandomForest和LogisticRegression/GBDT等都能对模型的特征打分，通过打分获得相关性后再训练最终模型；
通过特征组合后再来选择特征：如对用户id和用户特征最组合来获得较大的特征集再来选择特征，这种做法在推荐系统和广告系统中比较常见
通过深度学习来进行特征选择
传统用前进或者后退法的逐步回归来筛选特征或者对特征重要性排序，对于特征数量不多的情况还是适用的。
方差选择法计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征
卡方检验 经典的卡方检验是检验定性自变量对定性因变量的相关性
互信息法 互信息法经典的互信息也是评价定性自变量对定性因变量的相关性的
线性判别分析法（LDA）
主成分分析法（PCA）
https://www.zhihu.com/question/28641663/answer/41653367
信息熵和基尼指数的关系(信息熵在x=1处一阶泰勒展开就是基尼指数)
如何克服过拟合，欠拟合
L0，L1，L2正则化(如果能推导绝对是加分项，一般人最多能画个等高线，L0是NP问题)
怎么衡量两个商品的性价比
19）特征比数据量还大时，选择什么样的分类器
20）对于维度很高的特征，你是选择线性还是非线性分类器
21) 对于维度极低的特征，你是选择线性还是非线性分类器
6) 讲em算法
11）推荐系统中基于svd方法
13）userCF和ItemCF在实际当中如何使用,提供具体操作，以及它们的优势（推荐系统）
28） 如何搭建一个推荐平台，给出具体的想法，
27)支持向量机、图模型、波尔茨曼机，内存压缩、红黑树、并行度
9）谈谈DNN
29） 实现一个中文输入法
怎么用模型来查找异常用户
快速排序
通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序
可以开始寻找比6小的数从右往左找第二次比较，这次要变成找比k大的了，而且要从前往后找
所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置
归并排序
归并操作(merge)，也叫归并算法，指的是将两个顺序序列合并成一个顺序序列的方法。
第一次归并后：{6,202},{100,301},{8,38},{1}，比较次数：3；
第二次归并后：{6,100,202,301}，{1,8,38}，比较次数：4；
堆排序
堆实际上是一棵完全二叉树
即任何一非叶节点的关键字不大于或者不小于其左右孩子节点的关键字。
即每次调整都是从父节点、左孩子节点、右孩子节点三者中选择最大者跟父节点进行交换
插入排序–直接插入排序
先将序列的第1个记录看成是一个有序的子序列，然后从第2个记录逐个进行插入，直至整个序列有序为止O（n^2）.
插入排序—希尔排序
选择一个增量序列
按增量序列个数k，对序列进行k 趟排序；
冒泡排序
自上而下对相邻的两个数依次进行比较和调整，让较大的数往下沉，较小的往上冒
桶排序/基数排序
把数据分组，放在一个个的桶中，然后对每个桶里面的在进行排序。
可以把桶设为大小为10的范围 算hash值分桶
对A[1..n]从头到尾扫描一遍，把每个A[i]放入对应的桶B[j]中
再对这100个桶中每个桶里的数字排序，这时可用冒泡，选择，乃至快排
最后，依次输出每个桶里面的数字，且每个桶中的数字从小到大输出，这 样就得到所有数字排好序的一个序列了。
java虚拟机
Java虚拟机运行时数据区分为以下几个部分：
方法区、Java栈、Java堆、程序计数器。
程序计数器CPU必须具有某些手段来确定下一条指令的地址使其保持的总是将要执行的下一条指令的地址 用来计数的，指示指令在存储器的存放位置，也就是个地址信息
Java栈
线程私有 保存一个方法的局部变量、操作数栈、常量池指针
方法区
保存装载的类信息 类型的常量池 方法信息
Java堆
Java堆是所有线程共享的存放对象实例被成为GC堆 应用系统对象都保存在Java堆中
gc新生代算法
标记清除–首先通过根节点，标记所有从根节点开始的可达对象未被标记的对象就是未被引用的垃圾对象。然后，在清除阶段，清除所有未被标记的对象。
标记压缩–将所有的存活对象压缩到内存的一端。之后，清理边界外所有的空间。
复制算法–将原有的内存空间分为两块，每次只使用其中一块，在垃圾回收时，将正在使用的内存中的存活对象复制到未使用的内存块中，之后，清除正在使用的内存块中的所有对象，交换两个内存的角色，完成垃圾回收
分代思想
依据对象的存活周期进行分类，短命对象归为新生代，长命对象归为老年代。
根据不同代的特点，选取合适的收集算法
少量对象存活，适合复制算法
大量对象存活，适合标记清理或者标记压缩
java基础
线程进程共享内存
一个进程中可以包含若干个线程 进程作为分配资源的基本单位 进程间相互独立 进程在执行过程中拥有独立的内存单元
线程作为独立运行和独立调度的基本单位
子进程和父进程有不同的代码和数据空间，而多个线程则共享数据空间
操作系统的设计，因此可以归结为三点：
（1）以多进程形式，允许多个任务同时运行；
（2）以多线程形式，允许单个任务分成不同的部分运行；
（3）提供协调机制，一方面防止进程之间和线程之间产生冲突，另一方面允许进程之间和线程之间共享资源。
简单移动平均法
sma[t]= mean(ts[1:t])
加权移动平均法
wma[t]= sum( ts[(t-k+1):t] * weight )
k期移动平均法(简单移动平均法的优化,取最近K期的均值)
kwma[t]= mean(ts[(t-k+1):t])
指数平滑法 平滑系数α的选择(时间越远权重小)
ewma[t]= a*ts[t] + (1-a)*ewma[t-1]
当时间数列无明显的趋势变化，可用一次指数平滑预测。
二次指数平滑预测
二次指数平滑是对一次指数平滑的再平滑。它适用于具线性趋势的时间数列。
三次指数平滑预测
三次指数平滑预测是二次平滑基础上的再平滑。
它们的基本思想都是：预测值是以前观测值的加权和，且对不同的数据给予不同的权，新数据给较大的权，旧数据给较小的权。
mapreduce 两个大文本连接
reduce side join 重分区连接 reduce端连接
map函数同时读取两个文件File1和File2对每条数据打一个标签（tag）
在reduce阶段，reduce函数获取key相同的来自File1和File2文件的value list， 然后对于同一个key，对File1和File2中的数据进行join
map side join 小表连大表
Semi Join 半连接 另一个map端连接 连接的数据集中有一个数据集非常大，但同时这个数据集可以被过滤成小到可以放在内存中。
File1，将其参与join的key抽取出来，保存到文件File3 将File2中不在File3中的key对应的记录过滤掉，剩下的reduce阶段的工作与reduce side join相同
int值排序
int+出现次数
hive
spark
RDD作为数据结构，本质上是一个只读的分区记录集合
数据分区的集合，能根据本地性快速访问到数据的偏好位置，依赖关系，计算方法，是否是哈希/范围分区的元数据
KPI统计
电影推荐
矩阵乘法
1 0 3 1 6
4 8 7 9 5
4 6
K V
1 1 a 1 1 a第一行 b第一列 a矩阵 a第一列 值
1 2 a 1 1 a第一行 b第二列 a矩阵 a第一列 值
1 1 a 3 3 a第一行 b第一列 a矩阵 a第三列 值
1 2 a 3 3 a第一行 b第二列 a矩阵 a第三列 值
把要乘的数放在一起
mahout推荐
协同过滤
基于物品推荐
建立物品的同现矩阵-AB物品同时出现的次数
建立用户对物品的评分矩阵
同现矩阵X评分矩阵=物品推荐分数
优化：压缩输出文件、合并小文件har 读入多个文件作为一条input
自定义排序 自定义分组 自定义分区
top k 有序树
问企业公司专注于哪里学习哪方面知识
面试结果
面积经验
1.国内互联网公司和国外FLAGS的面试有很大不同，前者重视项目、实习经验、专业积累，后者看重的是你聪不聪明与编程题刷得多不多。在求职开始的时候心里得有个谱，合理分配时间、精力，并理性看待自己挂了笔试或面试。
2.简历讲究简单粗暴有逼格，简历包含联系方式、教育背景、实习经历、项目介绍、牛逼的荣誉、岗位相关的技能就好，谓之简单粗暴，有逼格则指的是实习、项目多用数字量化描述，省去叙事的过程，结果导向。还有一点经验，可以适当装逼，但不能装逼得超过可掌控范围。
投简历的时候，多走内推渠道，省去大部分笔试或者电话面试，不吃力又讨好。内推渠道有以下几种，我按照靠谱程度排个序：1) 总监以下的内部技术员工 2) HR 3)总监以上的高管 4)北邮人论坛 5)Linked In 6)知乎 7)NLP job 8)微信公众号。1)和2)是比较靠谱的，3)~7)只能作为备胎。还有一个非主流但很有效的捷径是参加企业举办的比赛并取得好名次。
面试时要抓住提问环节问一些实质性的问题，比如具体的技术问题、部门组织架构、部门战略地位、以后的工作团队、对个人的定位、KPI怎样给出等，尤以部门组织架构、战略地位、团队这类大又可说的问题最佳。京东面试官给我讲了百度架构部门的痛点，在之后的面试中我就经常和面试官聊关于架构部门和业务部门的话题，学到很多，大局观也慢慢改善。
5.在精力允许的情况下多面，多拿offer，一方面涨见识、谈资，一方面在谈理想公司的offer时能争取到更好的薪资待遇。
1.笔试题
－ 在互联网分析中，通常会关注用户的【第N日激活率】来衡量用户质量。用户的第N日激活指的是用户在注册的第N天登录网站或App，即算为该用户的第N日激活。第N日激活率指的是某日注册用户在第N天激活的数量除以该日所有注册用户数量。
“赤兔”是领英中国针对中国职场人士推出的一款本土化的社交应用。如果你是领英中国的数据分析师，你会从哪些方面和维度来设计各项指标，给产品的设计和运营提供数据上的支持？请给出具体的维度和指标，并说明原因。
－ 网易游戏的一道笔试题是给定一条包含3个join的SQL语句，要求写代码模拟实现SQL的功能。
2.编程题
－ 最少时间复杂度求数组中第k大的数，写code
－ 去除字符串S1中的字符使得最终的字符串S2不包含’ab’和’c’，写code
－ 长度为N的序列Sequence=abc….Z，问有多少不同的二叉树形态中序遍历是这个，写递推公式
－ 给定整数n和m，问能不能找出整数x，使得x以后的所有整数都可以由整数n和m组合而成
－ 中序遍历二叉树，利用O(1)空间统计遍历的每个节点的层次，写bug free的code
－ 排序二叉树转双向链表
－ 一个运算序列只有+、*、数字，计算运算序列的结果
3.机器学习&数据挖掘问题
－ L1和L2正则项 >> 它们间的比较
－ 各个模型的Loss function，牛顿学习法、SGD如何训练
－介绍LR、RF、GBDT ，分析它们的优缺点，是否写过它们的分布式代码
－ 介绍SVD、SVD++
－ 是否了解线性加权、bagging、boosting、cascade等模型融合方式
－ 推荐系统的冷启动问题如何解决
－ 是否了解A/B Test以及A/B Test结果的置信度
－ 特征工程经验
－ 是否了解mutual infomation、chi-square、LR前后向、树模型等特征选择方式
4.解决方案类题目
－ 为今日头条设计一个热门评论系统，支持实时更新
给定淘宝上同类目同价格范围的两个商品A和B，如何利用淘宝已有的用户、商品数据、搜索数据、评论数据、用户行为数据等所有能拿到的数据进行建模，判断A和B统计平均性价比高低。统计平均性价比的衡量标准是大量曝光，购买者多则高。
－ 有n个elements和1个Compare(A, B)函数，用Compare函数作为排序算法中的比较算子给elements排序。Compare函数有p的可能比较错。排序完取Top m个元素，本来就在Top m并被正确分在Top m的元素个数是x。问x的数学期望。
－ 如何预测双十一支付宝的负载峰值。