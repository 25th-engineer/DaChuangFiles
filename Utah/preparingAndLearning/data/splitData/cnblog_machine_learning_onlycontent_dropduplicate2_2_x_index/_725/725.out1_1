特征选择 方法 初识 1 为什么 要 做 特征选择 在 有限 
的 样本 数 目下 用 大量 的 特征 来 设计 
分类器 计算 开销 太大 而且 分类 性能 差 2 特征 
选择 的 确切 含义 将 高维空间 的 样本 通过 映射 
或者 是 变换 的 方式 转换 到 低 维空间 达到 
降 维 的 目的 然后 通过 特征 选取 删 选 
掉 冗余 和不/nr 相关 的 特征 来 进一步 降 维 
3 特征 选取 的 原则 获取 尽可能 小 的 特征 
子集 不 显著 降低 分类 精度 不影响/i 类/q 分布/v 以及/c 
特征/n 子集/n 应/v 具有/v 稳定/a 适应性/n 强等/nr 特点/n 主要/b 有三/nr 
种方法/i 1 Filter 方法 其 主要 思想 是 对 每 
一维 的 特征 打分 即 给 每 一维 的 特征 
赋予 权重 这样 的 权重 就 代表 着 该 维 
特征 的 重要性 然后 依据 权重 排序 主要 的 方法 
有 Chi squared test 卡方检验 information gain 信息 增益 详细 
可见 简单 易学 的 机器学习 算法 决策树 之 ID3 算法 
correlation coefficient scores 相关系数 2 Wrapper 方法 其 主要 思想 
是 将 子集 的 选择 看作 是 一个 搜索 寻优 
问题 生成 不同 的 组合 对 组合 进行 评价 再 
与 其他 的 组合 进行 比较 这样 就将 子集 的 
选择 看作 是 一个 是 一个 优化 问题 这里 有 
很多 的 优化 算法 可以 解决 尤其 是 一些 启发式 
的 优化 算法 如 GA PSO DE ABC 等 详见 
优化 算法 人工 蜂群 算法 ABC 优化 算法 粒 子群 
算法 PSO 主要 方法 有 递归 特征 消除 算法 3 
Embedded 方法 其 主要 思想 是 在 模型 既定 的 
情况 下 学习 出 对 提高 模型 准确性 最好 的 
属性 这 句话 并 不是 很好 理解 其实 是 讲 
在 确定 模型 的 过程 中 挑选 出 那些 对 
模型 的 训练 有 重要 意义 的 属性 简单 易学 
的 机器学习 算法 岭回归 Ridge Regression 岭回归 就是 在 基本 
线性 回归 的 过程 中 加入 了 正则 项 总结 
以及 注意 点 这篇文章 中 最后 提到 了 一点 就是 
用 特征选择 的 一点 Trap 个人 的 理解 是 这样 
的 特征选择 不同于 特征提取 特征 和 模型 是 分不开 选择 
不同 的 特征 训 练出 的 模型 是 不同 的 
在 机器学习 = 模型 + 策略 + 算法 的 框架 
下 特征选择 就是 模型 选择 的 一部分 是 分不开 的 
这样 文章 最后 提到 的 特征 选择 和 交叉 验证 
就 好 理解 了 是 先 进行 分组 还是 先 
进行 特征选择 答案 是 当然 是 先 进行 分组 因为 
交叉 验证 的 目的 是 做 模型 选择 既然 特征选择 
是 模型 选择 的 一部分 那么 理所应当 是 先 进行 
分组 如果 先 进行 特征选择 即 在整个 数据 集中 挑选 
择机 这样 挑选 的 子集 就 具有 随机性 我们 可以 
拿 正则化 来 举例 正则化 是 对 权重 约束 这样 
的 约束 参数 是 在 模型 训练 的 过程 中 
确定 的 而 不是 事先 定好 然后 再 进行 交叉 
验证 的 特征选择 方法 具体 分 细节 总结 1 去掉 
取值 变化 小 的 特征 Removing features with low variance 
该 方法 一般 用在 特征选择 前 作为 一个 预处理 的 
工作 即 先 去掉 取值 变化 小 的 特征 然后 
再 使用 其他 的 特征选择 方法 选择 特征 2 单 
变量 特征选择 Univariate feature selection 单 变量 特征选择 能够 对 
每一个 特征 进行 测试 衡量 该 特征 和 响应 变量 
之间 的 关系 根据 得分 扔掉 不好 的 特征 对于 
回归 和 分类 问题 可以 采用 卡方检验 等 方式 对 
特征 进行 测试 2.1 Pearson 相关系数 Pearson Correlation 皮尔森 相关 
系数 是 一种 最 简单 的 能/v 帮助/v 理解/v 特征/n 
和/c 响应/v 变量/vn 之间/f 关系/n 的/uj 方法/n 该 方法 衡量 
的 是 变量 之间 的 线性 相关性 结果 的 取值 
区间 为 1 1 1 表示 完全 的 负相关 这个 
变量 下降 那个 就 会 上升 + 1 表示 完全 
的 正相关 0 表示 没有 线性相关 2.2 互信息 和 最大 
信息 系数 Mutual information and maximal information coefficient MIC 以上 
就是 经典 的 互信息 公式 了 想把 互信息 直接 用于 
特征选择 其实 不是 太 方便 1 它 不属于 度量 方式 
也 没有 办法 归一化 在 不同 数据 及 上 的 
结果 无法 做 比较 2 对于 连续 变量 的 计算 
不是 很 方便 X 和Y/nr 都是 集合 x y 都是 
离散 的 取值 通常 变量 需要 先 离散化 而 互 
信息 的 结果 对 离散化 的 方式 很 敏感 2.3 
距离 相关系数 Distance correlation 距离 相关 系数 是 为了 克服 
Pearson 相关 系数 的 弱点 而生 的 Pearson 相关 系数 
是 0 我们 也 不能 断定 这 两个 变量 是 
独立 的 有 可能 是 非 线性相关 但 如果 距离 
相关 系数 是 0 那么 我们 就 可以 说 这 
两个 变量 是 独立 的 2.4 基于 学习 模型 的 
特征 排序 Model based ranking 这种 方法 的 思路 是 
直接 使用 你 要用 的 机器学习 算法 针对 每个 单独 
的 特征 和 响应 变量 建立 预测模型 其实 Pearson 相关系数 
等价 于 线性 回归 里 的 标准化 回归系数 假如 某 
个 特征 和 响应 变量 之间 的 关系 是 非线性 
的 可以 用 基于 树 的 方法 决策树 随机 森林 
或者 扩展 的 线性 模型 等 基于 树 的 方法 
比较 易于 使用 因为 他们 对 非线性 关系 的 建模 
比较好 并且 不 需要 太多 的 调试 但 要 注意 
过拟合 问题 因 此树 的 深度 最好 不要 太大 再就是 
运用 交叉 验证 3 线性/n 模型/n 和/c 正则化/i 单/n 变量/vn 
特征选择/nr 方法/n 独立/v 的/uj 衡量/v 每个/r 特征/n 与/p 响应/v 变量/vn 
之间/f 的/uj 关系/n 另一种 主流 的 特征选择 方法 是 基于 
机器学习 模型 的 方法 有些 机器学习 方法 本身 就 具有 
对 特征 进行 打分 的 机制 或者 很 容易 将其 
运用 到 特征选择 任务 中 例如 回归模型 SVM 决策树 随机 
森林 等等 3.1 正则化 模型 正则化 就是 把 额外 的 
约束 或者 惩罚 项 加到 已有 模型 损失 函数 上 
以 防止 过拟合 并 提高 泛化 能力 损失 函数 由 
原来 的 E X Y 变为 E X Y + 
alpha | | w | | w 是 模型 系数 
组成 的 向量 有些地方 也叫 参数 parameter coefficients | | 
| | 一般 是 L1 或者 L2 范数 alpha 是 
一个 可调 的 参数 控制 着 正则化 的 强度 当 
用在 线性 模型 上 时 L1/i 正则化/i 和/c L2/i 正则化/i 
也/d 称为/v Lasso/w 和/c Ridge/w 3.2 L1 正则化 / LassoL1 
正则化 将 系数 w 的 l1 范数 作为 惩罚 项 
加到 损失 函 数上 由于 正则 项 非零 这就 迫使 
那些 弱 的 特征 所 对应 的 系数 变成 0 
因此 L1 正则化 往往 会使 学到 的 模型 很 稀疏 
系数 w 经常 为 0 这个 特性 使得 L1 正则化 
成为 一种 很好 的 特征选择 方法 3.3 L2 正则化 / 
Ridge regressionL2 正则化 将 系数 向量 的 L2 范数 添加到 
了 损失 函数 中 由于 L2 惩罚 项中/nr 系数 是 
二次方 的 这 使得 L2 和 L1 有着 诸多 差异 
最 明显 的 一点 就是 L2 正则化 会 让 系数 
的 取值 变得 平均 对于 关联 特征 这 意味着 他们 
能够 获得 更 相近 的 对应 系数 还是 以 Y 
= X1 + X2 为例 假设 X1 和 X2 具有 
很强 的 关联 如果 用 L1 正则化 不论 学到 的 
模型 是 Y = X1 + X2 还是 Y = 
2X1 惩罚 都是 一样 的 都是 2alpha 但是 对于 L2 
来说 第一 个 模型 的 惩罚 项是/nr 2alpha 但 第二 
个 模型 的 是 4 * alpha 可以 看出 系数 
之和 为 常数 时 各 系数 相 等时 惩罚 是 
最小 的 所以 才 有了 L2 会 让 各个 系数 
趋于 相同 的 特点 可以 看出 L2 正则化 对于 特征选择 
来说 一种 稳定 的 模型 不像 L1 正则化 那样 系数 
会 因为 细微 的 数据 变化 而 波动 所以 L2 
正则化 和 L1 正则化 提供 的 价值 是 不同 的 
L2 正则化 对于 特征 理解 来说 更加 有用 表示 能力强 
的 特征 对应 的 系数 是 非零 4 随机 森林 
随机 森林 具有 准确率 高 鲁棒 性好 易于 使用 等 
优点 这 使得 它 成为 了 目前 最 流行 的 
机器学习 算法 之一 随机 森林 提供 了 两种 特征选择 的 
方法 mean decrease impurity 和 mean decrease accuracy 4.1 平均 
不 纯度 减少 mean decrease impurity 随机 森林 由 多个 
决策树 构成 决策树 中的 每一个 节点 都是/nr 关于 某个 特征 
的 条件 为的是 将 数据集 按照 不同 的 响应 变量 
一分为二 利用 不 纯度 可以确定 节点 最优 条件 对于 分类 
问题 通常 采用 基尼 不 纯度 或者 信息 增益 对于 
回归 问题 通常 采用 的 是 方差 或者 最小二乘 拟合 
当 训练 决策树 的 时候 可以 计算 出 每个 特征 
减少 了 多少 树 的 不 纯度 对于 一个 决策树 
森林 来说 可以 算 出 每个 特征 平均 减少 了 
多少 不 纯度 并把 它 平均 减少 的 不 纯度 
作为 特征选择 的 值 4.2 平均 精确 率 减少 Mean 
decrease accuracy 另一种 常用 的 特征选择 方法 就是 直接 度量 
每个 特征 对模型 精确 率 的 影响 主要 思路 是 
打乱 每个 特征 的 特征值 顺序 并且 度量 顺序 变动 
对 模型 的 精确 率 的 影响 很明显 对于 不 
重要 的 变量 来说 打乱 顺序 对模型 的 精确 率 
影响 不会 太大 但是 对于 重要 的 变量 来说 打乱 
顺序 就会 降低 模型 的 精确 率 5 两种 顶层 
特征选择 算法 之所以 叫做 顶层 是 因为 他们 都是/nr 建立 
在 基于 模型 的 特征选择 方法 基础 之上 的 例如 
回归 和 SVM 在 不同 的 子集 上 建立 模型 
然后 汇总 最终 确定 特征 得分 5.1 稳定 性选择 Stability 
selection 稳定性 选择 是 一种 基于 二次 抽样 和 选择 
算法 相结合 较 新的 方法 选择 算法 可以 是 回归 
SVM 或 其他 类似 的 方法 它 的 主要 思想 
是 在 不同 的 数据 子集 和 特征 子集 上 
运行 特征选择 算法 不断 的 重复 最终 汇总 特征选择 结果 
比如 可以 统计 某个 特征 被 认为 是 重要 特征 
的 频率 被选为 重要 特征 的 次数 除以 它 所在 
的 子集 被 测试 的 次数 理想 情况下 重要 特征 
的 得分 会 接近 100% 稍 微弱 一点 的 特征 
得 分会 是非 0 的 数 而 最 无用 的 
特征 得分 将会 接近于 0 5.2 递归 特征 消除 Recursive 
feature elimination RFE 递归 特征 消除 的 主要 思想 是 
反复 的 构建 模型 如 SVM 或者 回归模型 然后 选出 
最好 的 或者 最差 的 的 特征 可以 根据 系 
数来 选 把 选 出来 的 特征 放到 一遍 然后 
在 剩余 的 特征 上 重复 这个 过程 直到 所有 
特征 都 遍历 了 这个 过程 中 特征 被 消除 
的 次序 就是 特征 的 排序 因此 这 是 一种 
寻找 最优 特征 子集 的 贪心 算法 RFE 的 稳定 
性 很大 程度 上 取决于 在 迭代 的 时候 底层 
用 哪种 模型 例如 假如 RFE 采用 的 普通 的 
回归 没有 经过 正则化 的 回归 是 不稳定的 那么 RFE 
就是 不 稳定 的 假如 采用 的 是 Ridge 而用 
Ridge 正则化 的 回归 是 稳定 的 那么 RFE 就是 
稳定 的 7 特征 获取 方法 的 选取 原则 a 
处理 的 数据 类型 b 处理 的 问题 规模 c 
问题 需要 分类 的 数量 d 对 噪声 的 容忍 
能力 e 无 噪声 环境 下 产生 稳定 性好 最优 
特征 子集 的 能力 互信息 Mutual Informantionyj 对 xi 的 
互信息 定义 为 后验/nr 概率 与 先验概率 比值 的 对数 
互信息 越大 表明 yj 对于 确定 xi 的 取值 的 
贡献度 越大 实际上 互信息 衡量 的 是 xi 与 y 
的 独立性 如果 他俩 独立 则 互信息 发 值 为零 
则 xi 与 y 不相关 则 可以 剔除 xi 反之 
如果 互信息 发 值 越大 则 他们 的 相关性 越大 
基于 期望 交叉 熵 的 特征 项 选择 p ci 
| w 表示 在 出现 词条 w 时 文档 属于 
类别 ci 的 概率 交叉 熵 反应 了 文本 类别 
的 概率 分布 与 在 出现 了 某个 词条 的 
情况 下 文本 类别 的 概率分布 之间 的 距离 词条 
的 交叉 熵 越大 对 文本 类别 分布 影响 也 
就 越大 如果 使用 具有 对称性 的 交叉 熵 那 
公式 就 变成 了 特征选择 前 向搜索 后/f 向搜索/nr 1/m 
初始化/l 特/d 征集/v F/w 为/p 空2/nr 扫描/v i/w 从1到/nr n/w 
如果 第 i 个 特征 不再 F 中 那么 将 
特征 i 和F/nr 放到 一起 Fi 在 只 使用 Fi 
中 特征 的 情况下 利用 交叉 验证 来 得到 Fi 
的 错误率 3 从 上步 中 得到 的 n 个 
Fi 中 选择 出 错误率 最小 的 Fi 更新 F 
为 Fi 如果 F 中的 特征 数 达到 n 或者 
预 设定 的 阈值 如果 有 那么 输出 整个 搜索 
过程 中 最好 的 F 没 达到 转到 2 这里 
指 不断 地 使用 不同 的 特 征集 来 测试 
学习 算法 即 每次 增 量地 从 剩余 未 选中 
的 特征 选出 一个 加入 特征 集中 待 达到 阈值 
或者 n 时 从 所有 的 F 中 选出 错误率 
最小 的 开始 然后/c 每次/r 循环/vn 删除/v 一个/m 特征/n 直到/v 
然后/c 选择/v 最佳/z 的/uj F/w 向前/t 和/c 向后/i 搜索/v 的/uj 
时间/n 复杂度/n 都/d 比较/d 高/a 时间 复杂度 为 O n2 
特征选择 算法 实现 http / / blog . csdn . 
net / fighting _ one _ piece / article / 
details / 37912051 更多 详细信息 参见   http / / 
www . tuicool . com / articles / ieUvaq 参考 
自 http / / blog . csdn . net / 
google19890102 / article / details / 40019271 