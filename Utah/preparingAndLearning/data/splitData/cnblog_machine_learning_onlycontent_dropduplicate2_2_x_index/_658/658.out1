机器学习 相关 知识 写 在前面 的话 保持 怀疑 的 态度 
在 全新 的 数据 集上 测试 分类器 天下 没有 免费 
的 午餐 没有 适用 的 最好 学习 方法 具体 问题 
具体 对待 正确对待 缺失 值 不同/a 参数/n 的/uj 设置/vn 可能会/i 
对/p 结果/n 产生/n 不同/a 的/uj 影响/vn 不同/a 算法/n 都有/nr 对应/vn 
的/uj 假设/vn 数据挖掘/n 的/uj 结果/n 总/b 会/v 误导/n 人/n 保持 
求真 的 态度 数据类型 连续型 离散 型 标称 型 概念 
离散化 归一化 正则化 度量 指标 欧几里得 距离 以 \ R 
\ 为 实 数域 对于 任意 一个 正整数 n 实数 
的 n 元组 的 全体 构成 了 R 上 的 
一个 n 维 向量空间 用 \ R ^ n \ 
来 表示 \ R ^ n \ 中的 元素 可以 
写成 $ X = x _ 1 x _ 2 
. . . x _ n $ . 欧式 范数 
定义 \ R ^ n \ 上 的 距离 函数 
为 \ d x y = \ Vert x y 
\ Vert = \ sqrt { \ displaystyle \ sum 
_ { i = 1 } ^ n x _ 
i y _ i ^ 2 } \ 可以 使用 
上面 的 公式 来 度量 n 维空间 中 任意 两点 
之间 的 距离 这 也是 在 推荐 系统 中 度量 
样本点 的 相似 程度 的 一种 方式 余弦 相似 度 
余弦 相似 度 是 基于 n 维空间 向量 的 所谓 
向量 就是 在 向量 的 空间 中 的 坐标 原点 
指向 该 空间 某 一个 点 的 度量 表示 n 
维空间 由 n 条 互相 正交 的 基 向量 构成 
彼此 无关 也是 构成 整个 向量空间 的 基础 结构 我们 
能够 比较 直观 理解 的 是 三维 向量 超过 三维 
就 比较 难以 理解 了 点积 对于 任意 两个 向量 
x y 点积 x y 定义 为 \ x y 
= \ displaystyle \ sum _ { i = 1 
} ^ nx _ iy _ i \ 向量 长度 
定义 为 \ \ Vert x \ Vert = \ 
sqrt { x x } = \ sqrt { \ 
displaystyle \ sum _ { i = 1 } ^ 
n x _ i ^ 2 } \ 余弦 相似 
度 定义 为 \ cos \ theta = \ frac 
{ A \ bullet B } { \ Vert A 
\ Vert \ Vert B \ Vert } = \ 
frac { \ displaystyle \ sum _ { i = 
1 } ^ nA _ i \ times B _ 
i } { \ sqrt { \ displaystyle \ sum 
_ { i = 1 } ^ n A _ 
i ^ 2 \ times \ displaystyle \ sum _ 
{ i = 1 } ^ n B _ i 
^ 2 } } \ 越 相似 二者 夹角 越小越 
趋向于 1 若 两 向量 朝 逆 方向 延伸 则 
度 量值 趋向于 1 可以 明显 的 从 数值 中 
看到 趋势 皮尔逊 距离 模型 评估 模型 评估 帮助 我们 
找到 最佳 的 模型 来 代表 给定 的 数据 集 
并且 能够 选择 出 的 模型 在 未来 的 未知 
数据 中 取得 较好 的 效果 在 原来 的 训练 
集合 上来 评估 一个 模型 的 好坏 是 不太 合适 
的 因为 这样 可能 会 造成 过拟合 的 现象 也 
就是说 可能会 因为 训练 集 某些 隐含 的 因子 使得 
这个 分类 模型 有 很好 的 准确率 但是 当 我们 
应用 这个 模型 到 新的 数据 集上 就 不一定 有 
同样 的 结果 这 就是 过拟合 的 现象 没有 在 
新的 数据 集上 获得 同样 的 效果 模型 的 泛化 
能力 所以 我们 需要 引入 没有 污染 过 任何 经过 
训练 的 数据 我们 称为 被 污染 过 的 数据 
进行 测试 这里 有 两种 方式 进行 评估 Hold out 
这种方式 针对 大 的 数据集 把 数据 分成 三份 训练 
集 构建 模型 验证 集 细化 模型 参数 选择 最佳 
模型 测试 集 衡量 模型 的 性能 这 部分 数据 
不 参与 前面 两个 步骤 Cross Validation 针对 数据 较少 
可以 采用 交叉 验证 的 方式 取得 一个 比较 稳定 
的 平均值 常用 的 k fold 也是 就是 所谓 的 
k 折 交叉 验证 方式 把 数据 集合 分成 k 
份 每次 选择 其中 一份 作为 测试 集 其余 的 
作为 训练 集 重复 k 次 最后 取得 每次 结果 
的 平均值 分类 评估 Confusion Matrix 混淆 矩阵 用来 描述 
正确 分类 和 错误 分类 的 各个 指标 下面 是 
一个 二 分类 问题 混淆 矩阵 的 实例 Accuracy 准确率 
the proportion of the total number of predictions that were 
correct . 准确率 Positive Predictive Value or Precision 正确率 the 
proportion of positive cases that were correctly identified . Negative 
Predictive Value the proportion of negative cases that were correctly 
identified . Sensitivity or Recall 敏感度 召回率 the proportion of 
actual positive cases which are correctly identified . Specificity 特异性 
the proportion of actual negative cases which are correctly identified 
. 特异性 ROC Chart 回归 评估 Root Mean Squared Error 
均方差 用来 衡量 回归模型 中的 错误率 注意 不同 模型 计算 
时 需要 注意 度量 单位 的 统一 \ RMSE = 
\ sqrt { \ frac { \ displaystyle \ sum 
_ { i = 1 } ^ n p _ 
i a _ i ^ 2 } { n } 
} \ \ a = actual \ target \ \ 
p = predicted \ target \ Relative Squared Error \ 
RSE = \ frac { \ displaystyle \ sum _ 
{ i = 1 } ^ n p _ i 
a _ i ^ 2 } { \ displaystyle \ 
sum _ { i = 1 } ^ n \ 
overline a a _ i ^ 2 } \ Mean 
Absolute Error \ MAE = \ frac { \ displaystyle 
\ sum _ { i = 1 } ^ n 
\ vert p _ i a _ i \ vert 
} { n } \ Relative Absolute Error \ RAE 
= \ frac { \ displaystyle \ sum _ { 
i = 1 } ^ n \ vert p _ 
i a _ i \ vert } { \ displaystyle 
\ sum _ { i = 1 } ^ n 
\ vert \ overline a a _ i \ vert 
} \ Coefficient of Determination 优化 方法 最小二乘 法 梯度 
上升 下降 寻求 最大 最小 值 梯度 上升 法 基于 
的 思想 是 要 找到 函数 的 最值 最好 的 
方法 就是 沿着 函数 的 地图 方向 进行 从 初始 
点 进行 最值 寻找 的 过程 中 需要 计算 当期 
点 的 梯度 并且 沿着 这个 方向 移动 具体 移动 
的 步伐 通过 步长 来 设定 到达 下 一个 点 
之后 重新 计算 梯度 不 迭代 直到 满足条件 寻找 到 
最优 值 或者 到达 误差 允许 的 范围内 随机 梯度 
随机 梯度 在 针对 大 数据集 的 时候 显得 特别 
有用 虽然 损失 了 一定 的 精度 但是 换来 了 
较快 的 收敛 速度 达到 一个 与 全局 最优 较近 
位置 的 点 它 是 针对 每个 点 选择 盖点/nr 
最优 的 方向 进行 移动 不 一定 要 对 整个 
数据集 集合 操作 就 可能 到达 收敛 了 而 BGD 
批处理 梯度 下降 在 每一次 梯度 的 更新 操作 中 
需要 对 整个 数据集 进行 计算 在 大 数据 处理 
过程 中 无疑 增大 了 计算 量 最大 似 然 
估计 风 刮 的 常用 算法 建模 分类 分 类 
问题 实际 是 一类 监督 学习 问题 根据 训练 数据 
的 类 标记 来 引导 分类器 选择 合适 的 参数 
较好 的 拟合 样本空间 中 的 数据 并且 能够 对 
未来 的 数据 有 一定 的 预测 能力 泛化 能力 
分类 算法 大致 可以 按照 如下 四 种 方式 进行 
分类 频率 表 ZeroRWeka 中 提供 了 ZeroR 算法 是 
最简单 的 分类器 主要 思想 是 按照 类 中 大多数 
数据 所属 的 类别 类 进行 分类 基本上 没有 使用 
认为 关于 ml 的 理论 它 的 作用 就是 给 
其他 分类器 提供 一个 模型 的 基线 衡量 一个 算法 
的 性能 换 句话 说 我们 用 的 模型 再 
怎么 不行 也 不能 比 这个 baseline 还要 低 吧 
构造 目标 的 频率 表 选择 频率 最大 的 类别 
作为 目标 的 预测 值 下面 是 ZeroR 的 混淆 
矩阵 我们 可以 看到 这个 算法 的 用途 只是 在于 
提供 一个 baseline 而已 OneROneR 是 One Rule 的 简称 
也 就是 按照 一个 规则 进行 分类 它 会 针对 
每个 属性 上 进行 测试 对 目标 进行 分类 然后 
从 每个 属性 中 得到 的 规则 中 选择 最小 
误 分类 的 规则 作为 最终 的 分类 模型 比如 
下面 的 例子 很好 的 说明 上述 的 描述 一个 
较小 的 总体 分类 误差 意味着 对 正确 分类 提供 
了 更多 的 贡献 这 是 这个 分 类 问题 
的 混淆 矩阵 Naive Bayesian 贝叶斯 分类 依托 于 贝叶斯 
理论 基于 先验概率 的 理解 根据 样本数据 的 统计 特征 
得出 后验/nr 概率 是 一种 非常 简单 而且 有效 的 
分类 方法 在 数据 较少 的 情况下 也 依然 有效 
同时 也 可以 处理 多类 分类 问题 缺点 在于 对 
输入 数据 的 准备 方式 较为 敏感 补充 贝叶斯 公式 
贝叶斯 理论 to be continue . . . 朴素 贝叶斯 
的 前提 条件 是 假设 给定 目标值 时 属性 之间 
彼此 相互 独立 然后 应用 贝叶斯 公式 通过 $ P 
c P x P x / c $ 计算出 后验/nr 
概率 \ P c / x \ 虽然 我们 不能 
确定 给定 的 属性 之间 是否 真的 相互 独立 但是 
在 这么 一个 假设 的 前提 下 朴素 贝叶斯 分类 
确实 取得 了 非常 不错 的 效果 下面 举个 栗子 
看看 贝叶斯 的 分类 机制 给定 一个 属性 集合 计算 
它 属于 其 中 某个 类别 的 一个 后验/nr 概率 
然后 选择 概率 最大 的 那个 类别 作为 它 的 
分类 需要 注意 的 一点 就是 由于 贝叶斯 针对 多个 
变量 的 连乘 操作 我们 在 操作 中 需要 进行 
加 一 避免 0 的 出现 比如 途中 当 outlook 
为 overcast 的 时候 Play Golf = no 上面 是 
对 离散 型 的 分类 处理 数值 型 预测 任务 
改 怎么 处理 呢 我们 需要 把 数值 型 数据 
转换成 离散 型 通常 是 假定 正态分布 然后 将其 离散化 
. 然后 构造 频率 表 再使用 贝叶斯 理论 关于 正态分布 
下面 举个 例子 说明 数值 型 离散化 使用 贝叶斯 Decision 
Tree 决策树 可 用来 构建 分类 和 回归模型 能够 处理 
离散 型 和 连续型 的 数据 分类 的 过程 就是 
不断 的 把 数据 集合 按照 属性 分成 一个 个 
较小 的 子集 属性 的 选择 可用 比如 信息 增益 
等 方式 对 每个 选择 的 属性 做 一次 判断 
然后 将 数据 归入 到 特定 的 子集 中 不断 
的 进行 直到 分类 完全 这个 用来 做 分析 判断 
的 属性 所在 的 节点 叫做 决策 节点 也 是 
最终 形成 树 的 中间 节点 最后 形成 的 树 
的 叶子 节点 表示 纯 类 构造 决策树 的 核心 
算法 来自 J . R . Quinlan 发表 的 ID3 
算法 ID3 使用 的 是 自顶向下 的 贪婪 算法 不 
经过 回退 操作 在整个 可能 的 空间 中 进行 搜索 
. ID3 使用 熵 和 信息 增益 来 构建 决策树 
熵 ID3 使用 熵 平均 信息量 来 计算 样 本间 
的 同质性 即 如果 样本 完全 同质 则 信息熵 为 
0 如果 样本 均匀 包含 各种 可能性 则 熵 为 
1 . 熵 在 某种 程度 上 表达 了 不确定性 
的 多少 不确定性 越大 信息熵 越大 数 学之 美一 书中 
阐述 过 香农 的 信息 熵 理论 使用 id3 进行 
构建 决策树 时候 需要 计算 两类 熵 a 单个 属性 
的 信息 熵 b 多个 属性 的 信息 熵 信息 
增益 信息 增益 是 基于 在 针对 数据集 上一个 属性 
上 分类 之后 熵 的 减少 来 定义 的 构造 
决策树 的 过程 实际上 就是 不断 的 寻找 分裂 属性 
并且 返回 最大 的 信息 增益值 这 意味着 分裂 后的/nr 
分支 所在 的 集合 信息熵 越小 它 所在 的 分支 
越纯/nr 从而 达到 了 分类 的 效果 下面 是 计算 
步骤 1 5 1 计算 分类 目标 的 熵 2 
将 数据集 按照 不同 的 属性 划分 并且 计算 他们 
分裂 之后 的 熵 和 信息 增益 3 选择 最大 
的 信息 增益 作为 决策 节点 这里 选择 的 是 
outlook . 4 当 摸 个 分支 的 熵 为 
0 的 时候 则 表示 已经 分类 完成 直接 作为 
叶子 节点 否则 的话 继续 选择 属性 进行 分裂 5 
递归 的 运行 id3 算法 直到 数据集 被 完全 划分 
而且 没有 决策 节点 避免 过拟合 binningavoiding overfittingsuper a t 
t r i b u t e s m i 
s s i n g values 协方差 矩阵 Linear Discriminant 
Analysis 线性 判别分析 被 R . A . Fisher 在 
1936年 首次 作为 一种 分类 方法 提出 具有 与 复杂 
方法 相当 的 精确度 LDA 基于 这样 一种 思想 就是 
在 预测 变量 的 线性组合 空间 中 搜索 最佳 的 
用来 分开 两个 类 target 的 线性组合 Finsher 定义 了 
下面 的 代价 函数 Logistic Regression 逻辑 回归 产生 一条 
逻辑 曲线 将 值 映 射到 0 1 之间 逻辑 
回 归于 线性 回归 有点 类似 不过 这条 曲线 使用 
的 是 目标 变量 的 可能性 的 自然对数 而不是 概率 
来 构造 而且 预测 变量 不必 满足 正态分布 或在 同 
一个 组 内 要求 的 相同 方差 逻辑 回归 不仅 
可以 处理 数值 型 数据 而且 可以 处理 分类 型 
变量 . 逻辑 回归 使用 sigmoid 函数 作为 回归 分类器 
我们 通过 在 每个 特征 上 乘以 一个 回归系数 然后 
把 所有 的 结果 相加 得到 一个 总和 带入 到 
sigmoid 这个 阶跃 函数 中 进而 得到 一个 0 ~ 
1 之间 的 实数 我们 把 大于 0.5 的 归入 
到 1类 小于 0.5 的 归入 到 0类 所以 逻辑 
回归 也 可以 被 看成 是 一个 概率 估计 分类器 
那么 最佳 的 回归系数 怎么 确定 呢 这 就 需要 
采用 最 优化 的 方法 来 进行 对 参数 的 
确定 了 拟合 训练 数据 学习 出 最佳 的 回归系数 
就是 逻辑 回归 训练 的 任务 了 这里/r 让人/i 疑问/v 
的/uj 是/v 可能性/n 和/c 概率/n 有/v 哪里/r 不同/a 吗/y 优点 
计算 代价 不高 易于 理解 和 实现 缺点 容易 欠 
拟合 分类 精度 可能 不高 数据类型 数值 型 和 标称 
型 图中 等式 可以 等价 于下 面的 式子 我们/r 可以/c 
使用/v 类似/v 于/p 线性/n 回归/v 中的/i 最小二乘/i 法来/nr 确定/v 代价/n 
函数/n 中的/i 参数/n 在 逻辑 回归 中 我们 采用 的 
是 最大 似 然 估计 来 确定 这些 参数 相似 
函数 K Nearest NeighborsK 近邻 是 比较 简答 基于 相似 
度 来 分类 的 一个 算法 核心 思想 就是 根据 
距离 来 寻找 与 待 分类 点 的 k 个 
邻居 然后/c 每个/r 对应/vn 邻居/v 节点/n 都有/nr 自己/r 的/uj 类/q 
标号/n 我们 选择 这 k 个 邻居 中 类 标号 
数目 最多 的 作为 待 分类 节点 的 标签 类似 
于 一种 投票 机制 k 近邻 是 一种 lazy 算法 
处理 待 分类 节点 的 时候 才 去 寻找 他 
对应 的 邻居 距离 度量 公式 我们 可以 看到 上面 
的 公式 主要 是 针对 连续型 变量 的 在 处理 
分类 变量 的 时候 我们 需要 采用 Hamming distance 当 
数据 集中 同时 存在 s 数值 型 和 分类 型 
变量 的 时候 它 同时 带来 了 将 数值 变量 
映 射到 0 1 的 标准 化 问题 下面 是 
计算 Hamming distance 距离 的 表达式 K 的 选择 一般 
选择 k 的 范围 为 3 10 当然 只是 个别 
的 经验 通过 交叉 验证 的 方式 可以 帮助 我们 
选择 合适 的 k 值 并 通过 独立 的 验证 
集 来 验证 这个 k 是否 合适 这对 分类器 的 
分类 效果 至关重要 举个 栗子 在 下面 的 栗子 中 
是 一个 信用卡 违约 的 一个 预测 问题 有/v 两个/m 
属性/n age/w 和/c loan/w 都是 数值 型 变量 目标 变量 
是 是否 违约 给定 一个 样本 age = 48 loan 
= 142 000 用来 判断 他 是否 会 违约 我们 
选择 和他/nr 最近 的 3个 邻居 并 选择 邻 居中 
类 标记 数目 最多 的 标记 来 作为 该 样本 
的 预测 值 在 这个 例子 中 我们 得出 default 
= Y 标准化 距离 当 样本 中 的 数据 的 
度量 单位 不 一致 的 时候 使用 这些 距离 度量 
公式 的 时候 有 一个 非常 大 的 弊端 就是 
量纲 较大 的 属性 产生 的 影响 会 显得 比较 
大 很 肯 能 掩盖 掉 了 一些 很 重要 
的 属性 的 影响 为此 我们 需要 对 数据 进行 
标准化 使 他们 处于 同一 个 水平 进行 比较 和 
度量 对于 标准化 后的/nr 数据 进行 训练 不能 对 新来 
的 未知 数据 的 预测 有 较好 的 鲁棒性 下图 
是 标准化 转换 计算公式 其他 Artificial Neural Network 人工神经网络 是 
基于 生物 神经 网络 的 一个 系统 模仿 大脑 内 
的 神经 元 之间 的 触发 方式 到达 临界条件 才会 
激活 或者 抑制 人工神经网络 ANN 由 三个 部分 组成 输入 
层 隐含 层 输出 层 传递 方式 前 向 传播 
后向 反馈 Support Vector Machine 回归 聚 类 关联 规则 
特征选择 概念 维度 处理 PCA 主 成分 分析 SVD 奇异 
值 分解 附录 数据挖掘 概览 Data mining Map 