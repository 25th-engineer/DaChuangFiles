目录 一 L0 L1 范 数二 L2 范数 三 核 
范数 今天 我们 聊聊 机器学习 中 出现 的 非常 频繁 
的 问题 过拟合 与 规则化 我们 先 简单 的 来 
理解 下 常用 的 L0 L1 L2 和核/nr 范数 规则化 
最后 聊 下 规则化 项 参数 的 选择 问题 这里 
因为 篇幅 比较 庞大 为了 不 吓到 大家 我 将 
这个 五 个 部分 分成 两 篇 博文 知识 有限 
以下 都是 我 一些 浅显 的 看法 如果 理解 存在 
错误 希望 大家 不吝指正 谢谢 监督 机器学习 问题 无非 就是 
minimizeyour error while regularizing your parameters 也 就是 在 规则化 
参数 的 同时 最小化 误差 最小化 误差 是 为了 让 
我们 的 模型 拟合 我们 的 训练 数据 而 规则化 
参数 是 防止 我们 的 模型 过分 拟合 我们 的 
训练 数据 多么 简约 的 哲学 啊 因为 参数 太多 
会 导致 我们 的 模型 复杂度 上升 容易 过拟合 也 
就是 我们 的 训练 误 差会 很小 但 训练 误差 
小 并 不是 我们 的 最终 目标 我们 的 目标 
是 希望 模型 的 测试 误差 小 也 就是 能 
准确 的 预测 新的 样本 所以 我们 需要 保证 模型 
简单 的 基础 上 最小化 训练 误差 这样 得到 的 
参数 才具 有好 的 泛化 性能 也 就是 测试 误差 
也 小 而 模型 简单 就是 通过 规则 函数 来 
实现 的 另外 规则 项的/nr 使用 还 可以 约束 我们 
的 模型 的 特性 这样 就 可以 将 人 对 
这个 模型 的 先验 知识 融入 到 模型 的 学习 
当中 强行 地 让 学习 到 的 模型 具有 人 
想要 的 特性 例如 稀疏 低 秩 平滑 等等 要知道 
有时候 人 的 先验 是 非常 重要 的 前人 的 
经验 会 让 你 少 走 很多 弯路 这 就是 
为什么 我们 平时 学习 最好 找个 大牛 带带 的 原因 
一句 点拨 可以 为 我们 拨开 眼前 乌云 还 我们 
一片 晴空万里 醍醐灌顶 对 机器 学习 也 是 一样 如果 
被 我们 人 稍微 点拨 一下 它 肯定 能 更快 
的 学习 相应 的 任务 只是 由于 人 和 机器 
的 交流 目前 还 没有 那么 直接 的 方法 目前 
这个 媒介 只能 由 规则 项来/nr 担当 了 还有 几 
种 角度 来 看待 规则化 的 规则化 符合 奥卡姆 剃刀 
Occam s razor 原理 这 名字 好 霸气 razor 不过 
它 的 思想 很 平易近人 在 所有 可能 选择 的 
模型 中 我们 应该 选择 能够 很好 地 解释 已知 
数据 并且 十分 简单 的 模型 从 贝叶斯 估计 的 
角度 来看 规则化/n 项/n 对应/vn 于/p 模型/n 的/uj 先验概率/l 民间 
还有 个 说法 就是 规则化 是 结构 风险 最小化 策略 
的 实现 是 在 经验 风险 上加 一个 正则化 项 
regularizer 或 惩罚 项 penalty term 一般来说 监督 学习 可以 
看做 最小化 下面 的 目标 函数 其中 第一项 L yi 
f xi w 衡量 我们 的 模型 分类 或者 回归 
对 第 i 个 样本 的 预测 值 f xi 
w 和 真实 的 标签 yi 之前 的 误差 因为 
我们 的 模型 是 要 拟合 我们 的 训练 样本 
的 嘛 所以 我们 要求 这 一项 最小 也 就是 
要求 我们 的 模型 尽量 的 拟合 我们 的 训练 
数据 但 正如 上面 说 言 我们 不仅 要 保证 
训练 误差 最小 我们 更 希望 我们 的 模型 测试 
误差 小 所以 我们 需要 加 上 第二 项 也 
就是 对 参数 w 的 规则化 函数 Ω w 去 
约束 我们 的 模型 尽量 的 简单 OK 到 这里 
如果 你 在 机器学习 浴血奋战 多年 你 会 发现 哎哟哟 
机器/n 学习/v 的/uj 大部/n 分带/n 参/n 模型/n 都和/nr 这个/r 不但/c 
形似/v 而且 神似 是的 其实 大部分 无非 就是 变换 这两项 
而已 对于 第一项 Loss 函数 如果 是 Square loss 那 
就是 最 小二 乘了 如果 是 Hinge Loss 那 就是 
著名 的 SVM 了 如果 是 exp Loss 那 就是 
牛逼 的 Boosting 了 如果 是 log Loss 那 就是 
Logistic Regression 了 还有 等等 不同 的 loss 函数 具有 
不同 的 拟合 特性 这个 也得 就 具体 问题 具体 
分析 的 但 这里 我们 先不 究 loss 函数 的 
问题 我们 把 目光 转向 规则 项Ω/nr w 规则化 函数 
Ω w 也 有 很多 种 选择 一般 是 模型 
复杂度 的 单调 递 增函数 模型 越 复杂 规则化 值 
就 越大 比如 规则化 项 可以 是 模型 参数 向量 
的 范数 然而 不同 的 选择 对 参数 w 的 
约束 不同 取得 的 效果 也 不同 但 我们 在 
论文 中 常见 的 都 聚集 在 零 范数 一 
范数 二 范数 迹 范数 Frobenius 范数 和核/nr 范数 等等 
这么 多 范数 到底 它们 表达 啥意思 具 有啥 能力 
什么 时候 才能 用 什么 时候 需要 用 呢 不急 
不急 下面 我们 挑 几个 常见 的 娓娓道来 一 L0 
范数 与 L1 范数 L0 范数 是 指向 量 中非 
0 的 元素 的 个数 如果 我们 用 L0 范 
数来 规则化 一个 参数 矩阵 W 的话 就是 希望 W 
的 大部分 元素 都是 0 这太 直观 了 太 露骨 
了吧 换句话说 让 参数 W 是 稀疏 的 OK 看到 
了 稀疏 二字 大家/n 都/d 应该/v 从/p 当下/t 风风火火/l 的/uj 
压缩感知 和 稀疏 编码 中 醒悟 过来 原 来用 的 
漫山遍野 的 稀疏 就是 通过 这 玩意 来 实现 的 
但 你 又 开始 怀疑 了 是 这样 吗 看到 
的 papers 世界 中 稀疏 不是 都 通过 L1 范数 
来 实现 吗 脑海 里 是不是 到处 都是 | | 
W | | 1 影子 呀 几乎 是 抬头 不见 
低头 见 没错 这 就是 这 节 的 题目 把 
L0 和 L1 放在 一起 的 原因 因为 他们 有着 
某种 不 寻常 的 关系 那 我们 再 来 看看 
L1 范数 是 什么 它 为什么 可以 实现 稀疏 为什么 
大家 都用/nr L1 范数 去 实现 稀疏 而 不是 L0 
范数 呢 L1 范数 是 指向 量 中 各个 元素 
绝对值 之和 也 有个 美称 叫 稀疏 规则 算子 Lasso 
regularization 现在 我们 来 分析 下 这个 价值 一个亿 的 
问题 为什么 L1 范数 会使 权值 稀疏 有人 可能 会 
这样 给 你 回答 它 是 L0 范数 的 最优 
凸 近似 实际上 还 存在 一个 更 美的 回答 任何 
的 规则化 算子 如果 他 在 Wi = 0 的 
地方 不 可微 并且 可以 分解 为 一个 求和 的 
形式 那么 这个 规则化 算子 就 可以 实现 稀疏 这 
说是 这么 说 W 的 L1 范数 是 绝对值 | 
w | 在 w = 0处 是 不可 微 但这 
还是 不够 直观 这里 因为 我们 需要 和 L2 范数 
进行 对比 分析 所以 关于 L1 范数 的 直观 理解 
请 待会 看看 第二节 对了 上面 还有 一个 问题 既然 
L0 可以 实现 稀疏 为什么 不用 L0 而 要用 L1 
呢 个人 理解 一 是因为 L0 范数 很难 优化 求解 
NP 难 问题 二 是 L1 范数 是 L0 范数 
的 最优 凸 近似 而且 它 比 L0 范数 要 
容易 优化 求解 所以/c 大家/n 才/d 把/p 目光/n 和/c 万千/m 
宠爱/v 转/v 于/p L1/i 范数/n OK 来个 一句话 总结 L1 
范数 和 L0 范数 可以 实现 稀疏 L1 因 具有 
比 L0 更好 的 优化 求解 特性 而被 广泛应用 好 
到 这里 我们 大概 知道 了 L1 可以 实现 稀疏 
但 我们 会 想 呀 为什么 要 稀疏 让 我们 
的 参数 稀疏 有 什么 好处 呢 这里 扯 两点 
1 特征选择 Feature Selection 大家 对 稀疏 规则化 趋之若鹜 的 
一个 关键 原因 在于 它 能 实现 特征 的 自动 
选择 一般来说 xi 的 大部分 元素 也 就是 特征 都是 
和 最终 的 输出 yi 没有 关系 或者 不 提供 
任何 信息 的 在 最小化 目标函数 的 时候 考虑 xi 
这些 额外 的 特征 虽然 可以 获得 更小 的 训练 
误差 但在 预测 新的 样本 时 这些 没用 的 信息 
反而会 被 考虑 从而 干扰 了 对 正确 yi 的 
预测 稀疏 规则化 算子 的 引入 就是 为了 完成 特征 
自动 选择 的 光荣 使命 它 会 学习 地 去掉 
这些 没有 信息 的 特征 也 就是 把 这些 特征 
对应 的 权重 置 为 0 2 可 解释性 Interpretability 
另一个 青睐 于 稀疏 的 理由 是 模型 更容易 解释 
例如 患 某 种病 的 概率 是 y 然后 我们 
收集到 的 数据 x 是 1000 维 的 也 就是 
我们 需要 寻找 这 1000种 因素 到底 是 怎么 影响 
患上 这 种病 的 概率 的 假设 我们 这个 是个 
回归模型 y = w1 * x1 + w2 * x2 
+ + w1000 * x1000 + b 当然 了 为了 
让 y 限定 在 0 1 的 范围 一般 还得 
加个 Logistic 函数 通过学习 如果 最后 学习 到 的 w 
* 就 只有 很少 的 非零 元素 例如 只有 5个 
非零 的 wi 那么 我们 就 有 理由 相信 这些 
对应 的 特征 在 患病 分析 上面 提供 的 信息 
是 巨大 的 决策性 的 也 就是说 患 不患 这种 
病 只和 这 5个 因素 有关 那 医生 就好 分析 
多了 但 如果 1000个 wi 都非0/nr 医生 面对 这 1000种 
因素 累 觉 不爱 二 L2 范数 除了 L1 范数 
还有 一种 更 受 宠幸 的 规则化 范数 是 L2 
范数 | | W | | 2 它 也 不逊于 
L1 范数 它 有 两个 美称 在 回归 里面 有人 
把 有它的/nr 回归 叫 岭回归 Ridge Regression 有人 也叫 它 
权值 衰减 weight decay 这 用 的 很多 吧 因为 
它 的 强大 功效 是 改善 机器学习 里面 一个 非常 
重要 的 问题 过拟合 至于 过拟合 是 什么 上面 也 
解释 了 就是 模型 训练 时候 的 误差 很小 但在 
测试 的 时候 误差 很大 也 就是 我们 的 模型 
复杂 到 可以 拟合 到 我们 的 所有 训练样本 了 
但在 实际 预测 新的 样本 的 时候 糟糕 的 一塌糊涂 
通俗 的 讲 就是 应试 能力 很强 实际 应用 能力 
很差 擅长 背诵 知识 却不 懂得 灵活 利用 知识 例如 
下图 所示 来自 Ng 的 course 上面 的 图 是 
线性 回归 下面 的 图 是 Logistic 回归 也 可以 
说 是 分类 的 情况 从左到右 分别 是 欠 拟合 
underfitting 也称 High bias 合适 的 拟合 和 过拟合 overfitting 
也称 High variance 三种 情况 可以 看到 如果 模型 复杂 
可以 拟合 任意 的 复杂 函数 它 可以 让 我们 
的 模型 拟合 所有 的 数据 点 也 就是 基本上 
没有 误差 对于 回归 来说 就是 我们 的 函数 曲线 
通过 了 所有 的 数据 点 如上 图右 对 分类 
来说 就是 我们 的 函数 曲线 要把 所有 的 数据 
点 都 分类 正确 如下 图右 这 两种 情况 很 
明显 过拟合 了 OK 那 现在 到 我们 非常 关键 
的 问题 了 为什么 L2 范数 可以 防止 过拟合 回答 
这个 问题 之前 我们 得 先 看看 L2 范数 是个 
什么 东西 L2 范数 是 指向 量 各 元素 的 
平方和 然后 求 平方根 我们 让 L2 范数 的 规则 
项||/nr W | | 2 最小 可以 使得 W 的 
每个 元素 都 很小 都/d 接近/v 于0/nr 但 与 L1 
范数 不同 它 不会 让 它 等于 0 而是 接近于 
0 这里 是 有 很大 的 区别 的 哦 而 
越小 的 参数 说明 模型 越 简单 越/d 简单/a 的/uj 
模型/n 则/d 越不/nr 容易/a 产生/n 过拟合/i 现象/n 为什么 越小 的 
参数 说明 模型 越 简单 我 也 不懂 我 的 
理解 是 限制 了 参数 很小 实际上 就 限制 了 
多项式 某些 分量 的 影响 很小 看 上面 线性 回归 
的 模型 的 那个 拟合 的 图 这样 就 相当于 
减少 参数 个数 其实 我 也 不太 懂 希望 大家 
可以 指点 下 这里 也 一句话 总结 下 通过 L2 
范数 我们 可以 实现 了 对模型 空间 的 限制 从而 
在 一定 程度 上 避免 了 过拟合 L2 范数 的 
好处 是 什么 呢 这里 也 扯上 两点 1 学习 
理论 的 角度 从 学习 理论 的 角度 来说 L2 
范数 可以 防止 过拟合 提升 模型 的 泛化 能力 2 
优化 计算 的 角度 从 优化 或者 数值 计算 的 
角度 来说 L2 范数 有助于 处理 condition number 不好 的 
情况 下 矩阵 求 逆 很 困难 的 问题 哎 
等等 这 condition number 是 啥 我 先 google 一下 
哈 这里 我们 也 故作 高雅 的 来 聊聊 优化 
问题 优化 有两 大 难题 一是 局部 最小值 二 是 
ill condition 病态 问题 前者 俺 就 不说 了 大家 
都懂吧/nr 我们 要 找 的 是 全局 最小值 如果 局部 
最小值 太多 那 我们 的 优化 算法 就 很容易 陷入 
局部 最小 而 不能自拔 这 很 明显 不是 观众 愿意 
看到 的 剧情 那 下面 我们 来 聊聊 ill condition 
ill condition 对应 的 是 well condition 那 他们 分别 
代表 什么 假设 我们 有个 方程组 AX = b 我们 
需要 求解 X 如果 A 或者 b 稍微 的 改变 
会 使得 X 的 解 发生 很大 的 改变 那么 
这个 方程组 系统 就是 ill condition 的 反之 就是 well 
condition 的 我们 具体 举个 例子 吧 咱们 先看 左边 
的 那个 第一行 假设 是 我们 的 AX = b 
第二行 我们 稍微 改变 下 b 得到 的 x 和没/nr 
改变 前 的 差别 很大 看到 吧 第三行 我们 稍微 
改变 下 系数 矩阵 A 可以 看到 结果 的 变化 
也 很大 换句话 来说 这个 系统 的 解对 系数 矩阵 
A 或者 b 太 敏感 了 又 因为 一般 我们 
的 系数 矩阵 A 和b/nr 是从 实验 数据 里面 估计 
得到 的 所以 它 是 存在 误差 的 如果 我们 
的 系统 对 这个 误差 是 可以 容忍 的 就 
还好 但 系统 对 这个 误差 太 敏感 了 以至于 
我们 的 解的/nr 误差 更大 那/r 这个/r 解就/nr 太不/i 靠谱/i 
了/ul 所以 这个 方程组 系统 就是 ill conditioned 病态 的 
不 正常 的 不 稳定 的 有 问题 的 哈哈 
这 清楚 了吧 右边 那个 就叫 well condition 的 系统 
了 还是 再 啰嗦 一下吧 对于 一个 ill condition 的 
系统 我 的 输入 稍微 改变 下 输出 就 发生 
很大 的 改变 这不 好啊 这 表明 我们 的 系统 
不能 实用 啊 你 想想 看 例如 对于 一个 回归 
问题 y = f x 我们 是 用 训练样本 x 
去 训练 模型 f 使得 y 尽量 输出 我们 期待 
的 值 例如 0 那 假如 我们 遇到 一个 样本 
x 这个 样本 和 训练样本 x 差别 很小 面对 他 
系统 本 应该 输出 和 上面 的 y 差不多 的 
值 的 例如 0.00001 最后 却给 我 输出 了 一个 
0.9999 这 很 明显 不 对呀 就好像 你 很 熟悉 
的 一个 人 脸上 长了 个 青春痘 你 就不 认识 
他 了 那你/nr 大脑 就 太 差劲 了 哈哈 所以 
如果 一个 系统 是 ill conditioned 病态 的 我们 就 
会对 它 的 结果 产生 怀疑 那 到底 要 相信 
它 多少 呢 我们 得 找个 标准 来 衡量 吧 
因为 有些 系统 的 病 没 那么 重 它 的 
结果 还是 可以 相信 的 不能 一刀切 吧 终于 回来 
了 上面 的 condition number 就是 拿 来 衡量 ill 
condition 系统 的 可信度 的 condition number 衡量 的 是 
输入 发生 微小 变化 的 时候 输出 会 发生 多大 
的 变化 也 就是 系统对 微小 变化 的 敏感度 condition 
number 值 小 的 就是 well conditioned 的 大 的 
就是 ill conditioned 的 如果 方阵 A 是非 奇异 的 
那么 A 的 conditionnumber 定义 为 也 就是 矩阵 A 
的 norm 乘以 它 的 逆 的 norm 所以 具体 
的 值 是 多少 就要 看 你 选择 的 norm 
是 什么 了 如果 方阵 A 是 奇异 的 那么 
A 的 condition number 就是 正无穷 大了 实际上 每一个 可逆 
方阵 都 存在 一个 condition number 但 如果 要 计算 
它 我们 需要 先 知道 这 个 方阵 的 norm 
范数 和 Machine Epsilon 机器 的 精度 为什么 要 范数 
范数 就 相当于 衡量 一个 矩阵 的 大小 我们 知道 
矩阵 是 没有 大小 的 当 上面 不是 要 衡量 
一个 矩阵 A 或者 向量 b 变化 的 时候 我们 
的 解x/nr 变化 的 大小 吗 所以/c 肯定/v 得要/i 有/v 
一个/m 东西/ns 来/v 度量/n 矩阵/n 和/c 向量/n 的/uj 大小/b 吧/y 
对了 他 就是 范数 表示 矩阵 大 小 或者 向量 
长度 OK 经过 比较 简单 的 证明 对于 AX = 
b 我们 可以 得到 以下 的 结论 也/d 就是/d 我们/r 
的/uj 解x的/nr 相对/d 变化/vn 和A/nr 或者/c b/w 的/uj 相对/d 变化/vn 
是/v 有像/nr 上面/f 那样/r 的/uj 关系/n 的/uj 其中 k A 
的 值 就 相当于 倍率 看到 了 吗 相当于 x 
变化 的 界 对 condition number 来个 一句话 总结 conditionnumber 
是 一个 矩阵 或者 它 所 描述 的 线性系统 的 
稳定性 或者 敏感度 的 度量 如果 一个 矩阵 的 condition 
number 在 1 附近 那么 它 就是 well conditioned 的 
如果 远大于 1 那么 它 就是 ill conditioned 的 如果 
一个 系统 是 ill conditioned 的 它 的 输出 结果 
就 不要 太 相信 了 好了 对 这么 一个 东西 
已经 说 了 好多 了 对了 我们 为什么 聊到 这个 
的 了 回到 第一 句话 从 优化 或者 数值 计算 
的 角度 来说 L2 范数 有助于 处理 condition number 不好 
的 情况 下 矩阵 求 逆 很 困难 的 问题 
因为 目标函数 如果 是 二次 的 对于 线性 回归 来说 
那/r 实际上/d 是/v 有/v 解析/vn 解的/nr 求导 并 令 导数 
等于零 即可 得到 最优 解为/nr 然而 如果 当 我们 的 
样本 X 的 数目 比 每个 样本 的 维度 还要 
小 的 时候 矩阵 XTX 将会 不是 满秩的/nr 也 就是 
XTX 会 变得 不可逆 所以 w * 就 没办法 直接 
计算 出来 了 或者 更 确切 地 说 将会 有 
无穷 多个 解 因为 我们 方程组 的 个数 小于 未知数 
的 个数 也 就是说 我们 的 数据 不 足以 确定 
一个 解 如果/c 我们/r 从/p 所有/b 可行/v 解里/nr 随机/d 选/zg 
一个/m 的话/u 很 可能 并 不是 真正 好 的 解 
总而言之 我们 过拟合 了 但 如果 加上 L2 规则 项 
就 变成 了 下面 这种 情况 就 可以 直接 求 
逆 了 这 里面 专业点 的 描述 是 要 得到 
这个 解 我们 通常 并不 直接 求 矩阵 的 逆 
而是 通过 解 线性方程组 的 方式 例如 高斯消 元法 来 
计算 考虑 没有 规则 项的/nr 时候 也 就是 λ = 
0 的 情况 如果 矩阵 XTX 的 condition number 很大 
的 话 解 线性方程组 就 会在 数值 上 相当 不 
稳定 而 这个 规则 项的/nr 引入 则 可以 改善 condition 
number 另外 如果 使用 迭代 优化 的 算法 condition number 
太大 仍然 会 导致 问题 它 会 拖慢 迭代 的 
收敛 速度 而/c 规则/n 项从/nr 优化/vn 的/uj 角度/n 来看/u 实际上 
是 将 目标函数 变成 λ strongly convex λ 强凸/nr 的 
了 哎哟哟 这里 又 出现 个 λ 强凸/nr 啥 叫 
λ 强凸呢/nr 当 f 满足 时 我们 称 f 为 
λ stronglyconvex 函数 其中 参数 λ 0 当 λ = 
0时 退回 到 普通 convex 函数 的 定义 在 直观 
的 说明 强凸/nr 之前 我们 先 看看 普通 的 凸 
是 怎样 的 假设 我们 让 f 在 x 的 
地方 做 一 阶 泰勒 近似 一 阶 泰勒 展开 
忘 了吗 f x = f a + f a 
x a + o | | x a | | 
. 直观 来讲 convex 性质 是 指 函数 曲线 位于 
该点 处 的 切线 也 就是 线性 近似 之上 而 
strongly convex 则 进一步 要求 位于 该处 的 一个 二次函数 
上方 也 就是说 要 求函数 不要 太 平坦 而是 可以 
保证 有 一定 的 向上 弯曲 的 趋势 专业点 说 
就是 convex 可以 保证 函数 在 任意 一点 都 处于 
它 的 一 阶 泰勒 函数 之上 而 strongly convex 
可以 保证 函数 在 任意 一点 都 存在 一个 非常 
漂亮 的 二次 下界 quadratic lower bound 当然 这 是 
一个 很强 的 假设 但是 同时 也 是 非常 重要 
的 假设 可能 还 不好 理解 那 我们 画 个 
图 来 形象 的 理解 下 大家 一 看到 上面 
这个 图 就 全 明白 了吧 不用 我 啰嗦 了吧 
还是 啰嗦 一下吧 我们 取 我们 的 最优 解w*/nr 的 
地方 如果 我们 的 函数 f w 见 左图 也 
就是 红色 那个 函数 都会 位于 蓝色 虚线 的 那根 
二次函数 之上 这样 就算 wt 和w*/nr 离 的 比较 近 
的 时候 f wt 和f/nr w * 的 值 差别 
还是 挺大 的 也 就是 会 保证 在 我们 的 
最优 解w*/nr 附近 的 时候 还 存在 较大 的 梯度 
值 这样 我们 才 可以 在 比较 少 的 迭代 
次数 内 达到 w * 但 对于 右图 红色 的 
函数 f w 只 约束 在 一个 线性 的 蓝色 
虚线 之上 假设 是 如 右图 的 很 不幸 的 
情况 非常 平坦 那在/nr wt 还 离 我们 的 最优 
点 w * 很远 的 时候 我们 的 近似 梯度 
f wt f w * / wt w * 就 
已经 非常 小了 在 wt 处 的 近似 梯度 ∂ 
f / ∂ w 就 更 小了 这样 通过 梯度 
下降 wt + 1 = wt α * ∂ f 
/ ∂ w 我们 得到 的 结果 就是 w 的 
变化 非常 缓慢 像 蜗牛 一样 非常 缓慢 的 向 
我们 的 最优 点 w * 爬动 那在/nr 有限 的 
迭代 时间内 它 离 我们 的 最优 点 还是 很远 
所以 仅 仅靠 convex 性质 并 不能 保证 在 梯度 
下降 和 有限 的 迭代 次数 的 情况 下 得到 
的 点 w 会 是 一个 比 较好 的 全局 
最 小点 w * 的 近似 点 插 个 话 
有 地方 说 实际上 让 迭代 在 接近 最优 的 
地方 停止 也 是 一种 规则化 或者 提高 泛化 性能 
的 方法 正如 上面 分析 的 那样 如果 f w 
在 全局 最 小点 w * 周围 是 非常 平坦 
的 情况 的话 我们 有 可能 会 找到 一个 很远 
的 点 但 如果 我们 有 强凸/nr 的话 就能 对 
情况 做 一些 控制 我们 就 可以 得到 一个 更好 
的 近似解 至于 有多/nr 好嘛 这 里面 有 一个 bound 
这个 bound 的 好坏 也要 取决于 strongly convex 性质 中的 
常数 α 的 大小 看到 这里 不 知道 大家 学 
聪明 了 没有 如果 要 获得 strongly convex 怎么做 最 
简单 的 就是 往里面 加入 一项 α / 2 * 
| | w | | 2 呃 讲个 strongly convex 
花了 那么多 的 篇幅 实际上 在 梯度 下降 中 目标函数 
收敛 速率 的 上界 实际上 是 和 矩阵 XTX 的 
condition number 有关 XTX 的 condition number 越小 上界 就 
越小 也 就是 收敛 速度 会 越快 这一个 优化 说 
了 那么 多 的 东西 还是 来个 一句话 总结 吧 
L2 范数 不但 可以 防止 过拟合 还 可以 让 我们 
的 优化 求解 变得 稳定 和 快速 好了 这里 兑现 
上面 的 承诺 来 直观 的 聊聊 L1 和 L2 
的 差别 为什么 一个 让 绝对值 最小 一个 让 平方 
最小 会 有 那么 大 的 差别 呢 我 看到 
的 有 两种 几何 上 直观 的 解析 1 下降 
速度 我们 知道 L1 和 L2 都是 规则化 的 方式 
我们 将 权 值参 数以 L1 或者 L2 的 方式 
放到 代价 函数 里面 去 然后 模型 就 会 尝试 
去 最小化 这些 权值 参数 而 这个 最小化 就像 一个 
下坡 的 过程 L1 和 L2 的 差别 就 在于 
这个 坡 不同 如 下图 L1 就是 按 绝对值 函数 
的 坡 下降 的 而 L2 是 按 二次函数 的 
坡 下降 所以 实际上 在 0 附近 L1 的 下降 
速度 比 L2 的 下降 速度 要快 所以 会 非常 
快得 降到 0 不过 我 觉得 这里 解释 的 不太 
中肯 当然 了 也 不 知道 是不是 自己 理解 的 
问题 L1 在 江湖上 人称 Lasso L2 人称 Ridge 不过 
这 两个 名字 还 挺 让人 迷糊 的 看 上面 
的 图片 Lasso 的 图 看起来 就像 ridge 而 ridge 
的 图 看起来 就像 lasso 2 模型 空间 的 限制 
实际上 对于 L1 和 L2 规则化 的 代价 函数 来说 
我们 可以 写成 以下 形式 也 就是说 我们 将 模型 
空间 限制 在 w 的 一个 L1 ball 中 为了 
便于 可视化 我们 考虑 两维 的 情况 在 w1 w2 
平面 上 可以 画出 目标函数 的 等高线 而 约束条件 则 
成为 平面 上 半径 为 C 的 一个 norm ball 
等高线 与 norm ball 首次 相交 的 地方 就是 最优 
解 可以 看到 L1 ball 与 L2 ball 的 不同 
就 在于 L1 在 和 每个 坐标轴 相交 的 地方 
都有 角 出现 而 目标 函数 的 测地线 除非 位置 
摆得 非常好 大部分 时候 都会 在 角 的 地方 相交 
注意到 在 角 的 位置 就 会 产生 稀疏 性 
例 如图 中的 相 交点 就有 w1 = 0 而 
更 高维 的 时候 想象 一下 三维 的 L1 ball 
是 什么样 的 除了 角 点 以外 还有 很 多边 
的 轮廓 也 是 既有 很大 的 概率 成为 第一 
次 相交 的 地方 又 会 产生 稀疏 性 相比之下 
L2 ball 就 没有 这样 的 性质 因为 没 有角 
所以 第一 次 相交 的 地方 出现 在 具有 稀疏 
性 的 位置 的 概率 就 变得 非常 小了 这 
就从 直观 上来 解释 了 为什么 L1 regularization 能 产生 
稀疏 性 而 L2 regularization 不行 的 原因 了 因此 
一句话 总结 就是 L1 会 趋向 于 产生 少量 的 
特征 而 其他 的 特征 都是 0 而 L2 会 
选择 更多 的 特征 这些 特征 都会 接近于 0 Lasso 
在 特征选择 时候 非常 有用 而 Ridge 就 只是 一种 
规则化 而已 三 核 范数 核 范数 | | W 
| | * 是 指 矩阵 奇异 值 的 和 
英文 称 呼叫 Nuclear Norm 这个 相对于 上面 火热 的 
L1 和 L2 来说 可能 大家 就会 陌 生点 那它 
是 干嘛 用 的 呢 霸气 登场 约束 Low Rank 
低 秩 OK OK 那 我们 得 知道 Low Rank 
是 啥 用来 干啥 的 我们 先来 回忆 下 线性代数 
里面 秩 到底 是 啥 举个 简单 的 例子 吧 
对 上面 的 线性方程组 第一/m 个/q 方程/n 和/c 第二个/m 方程/n 
有/v 不同/a 的/uj 解/v 而/c 第/m 2个/mq 方程/n 和第/nr 3个/mq 
方程/n 的/uj 解/v 完全相同/l 从 这个 意义 上 说 第 
3个 方程 是 多余 的 因为 它 没有 带来 任何 
的 信息量 把 它 去掉 所得 的 方程组 与 原来 
的 方程组 同 解 为了 从 方程组 中去 掉 多余 
的 方程 自然 就 导 出了 矩阵 的 秩 这一 
概念 还 记得 我们 怎么 手工 求 矩阵 的 秩 
吗 为了 求 矩阵 A 的 秩 我们 是 通过 
矩阵 初等变换 把 A 化为 阶梯 型 矩阵 若 该 
阶梯 型 矩阵 有r个/nr 非 零行 那A的/nr 秩 rank A 
就 等于 r 从 物理 意义 上 讲 矩阵 的 
秩 度量 的 就是 矩阵 的 行列 之间 的 相关性 
如果 矩阵 的 各行 或 列 是 线性 无关 的 
矩阵 就是 满秩的/nr 也 就是 秩 等于 行数 回 到上面 
线性方程组 来说 吧 因为 线性方程组 可以用 矩阵 描述 嘛 秩 
就 表示 了 有 多少 个 有用 的 方程 了 
上面 的 方程组 有 3个 方程 实际上 只有 2个 是 
有用 的 一个 是 多余 的 所以 对应 的 矩阵 
的 秩 就是 2 了 OK 既然 秩 可以 度量 
相关性 而 矩阵 的 相关性 实际上 有带 有了 矩阵 的 
结构 信息 如果 矩阵 之间 各行 的 相关性 很强 那么 
就 表示 这个 矩阵 实际 可以 投影 到 更 低维 
的 线性 子空间 也 就是 用 几个 向量 就 可以 
完全 表达 了 它 就是 低 秩 的 所以 我们 
总结 的 一点 就是 如果 矩阵 表达 的 是 结构性 
信息 例如 图像 用户 推荐表 等等 那么 这个 矩阵 各行 
之间 存在 这 一定 的 相关性 那 这个 矩阵 一般 
就是 低 秩 的 如果 X 是 一个 m 行 
n 列 的 数值 矩阵 rank X 是 X 的 
秩 假如 rank X 远 小于 m 和n/nr 则 我们 
称 X 是 低 秩 矩阵 低 秩 矩阵 每行 
或 每 列 都 可以 用 其他 的 行 或 
列 线性 表出 可见 它 包含 大量 的 冗余 信息 
利用 这种 冗余 信息 可以 对 缺失 数据 进行 恢复 
也 可以 对 数据 进行 特征提取 好了 低 秩 有了 
那 约束 低 秩 只是 约束 rank w 呀 和/c 
我们/r 这/r 节/t 的/uj 核/n 范数/n 有/v 什么/r 关系/n 呢/y 
他们 的 关系 和 L0 与 L1 的 关系 一样 
因为 rank 是非 凸 的 在 优化 问题 里面 很难 
求解 那么 就 需要 寻找 它 的 凸 近似 来 
近似 它 了 对 你 没 猜错 rank w 的 
凸 近似 就是 核 范数 | | W | | 
* 好了 到 这里 我 也 没什么 好说 的 了 
因为 我 也是 稍微 翻看 了 下 这个 东西 所以 
也 还 没有 深入 去看 它 但 我 发现 了 
这 玩意 还有 很多 很 有意思 的 应用 下面 我们 
举 几个 典型 的 吧 1 矩阵 填充 Matrix Completion 
我们 首先 说说 矩阵 填充 用 在哪 一个 主流 的 
应用 是 在 推荐 系统 里面 我们 知道 推荐 系统 
有 一种 方法 是 通过 分析 用户 的 历史 记录 
来 给 用户 推荐 的 例如 我们 在 看 一部 
电影 的 时候 如果 喜欢 看 就 会给 它 打个 
分 例如 3颗 星 然后 系统 例如 Netflix 等 知名 
网站 就 会 分析 这些 数据 看看 到底 每部 影片 
的 题材 到底 是 怎样 的 针对 每个人 喜欢 怎样 
的 电影 然后 会给 对应 的 用户 推荐 相似 题材 
的 电影 但 有一个 问题 是 我们 的 网站 上面 
有 非常 多 的 用户 也有 非常多 的 影片 不是 
所有 的 用户 都 看过 说 有的 电影 不是 所有 
看过 某 电影 的 用户 都 会给 它 评分 假设 
我们 用 一个 用户 影片 的 矩阵 来 描述 这些 
记录 例如 下图 可以 看到 会 有 很多 空白 的 
地方 如果 这些 空白 的 地方 存在 我们 是 很难 
对 这个 矩阵 进行 分析 的 所以 在 分析 之前 
一般 需要 先 对其 进行 补全 也叫 矩阵 填充 那 
到底 怎么 填 呢 如何 才能 无中生有 呢 每个 空白 
的 地方 的 信息 是否 蕴含 在 其他 已有 的 
信息 之 上了 呢 如果 有 怎么 提取 出来 呢 
Yeah 这 就是 低 秩 生效 的 地方 了 这叫 
低 秩 矩阵 重构 问题 它 可以 用 如下 的 
模型 表述 已知 数据 是 一个 给定 的 m * 
n 矩阵 A 如果 其中 一些 元素 因为 某种原因 丢失 
了 我们 能否 根据 其他 行 和列的/nr 元素 将 这些 
元素 恢复 当然 如果 没有 其他 的 参考 条件 想要 
确定 这些 数据 很 困难 但 如果 我们 已知 A 
的 秩 rank A m 且 rank A n 那么 
我们 可以 通过 矩阵 各行 列 之间 的 线性相关 将 
丢失 的 元素 求出 你 会问 这种 假定 我们 要 
恢复 的 矩阵 是 低 秩 的 合理 吗 实际上 
是 十分 合理 的 比如 一个 用户 对 某 电影 
评分 是 其他 用户 对 这部 电影 评分 的 线性组合 
所以 通过 低 秩 重构 就 可以 预测 用户 对其 
未 评价 过 的 视频 的 喜好 程度 从而 对 
矩阵 进行 填充 2 鲁棒 PCA 主 成分 分析 这种 
方法 可以 有效 的 找出 数据 中最 主要 的 元素 
和 结构 去除 噪音 和 冗余 将 原有 的 复杂 
数据 降 维 揭示 隐藏 在 复杂 数据 背后 的 
简单 结构 我们 知道 最 简单 的 主 成分 分析 
方法 就是 PCA 了 从 线性代数 的 角度 看 PCA 
的 目标 就是 使用 另一组 基 去 重新 描述 得到 
的 数据 空间 希望 在 这组 新的 基 下 能 
尽量 揭示 原有 的 数据 间 的 关系 这个 维度 
即 最 重要 的 主 元 PCA 的 目标 就是 
找到 这样 的 主 元 最大 程度 的 去除 冗余 
和 噪音 的 干扰 鲁棒 主 成分 分析 Robust PCA 
考虑 的 是 这样 一个 问题 一般 我们 的 数据 
矩阵 X 会 包含结构 信息 也 包含 噪声 那么 我们 
可以 将 这个 矩阵 分解 为 两个 矩阵 相加 一个 
是 低 秩 的 由于 内部 有 一定 的 结构 
信息 造成 各行 或 列 间 是 线性 相关 的 
另 一个 是 稀疏 的 由于 含 有噪声 而 噪声 
是 稀疏 的 则 鲁棒 主 成分 分析 可以 写成 
以下 的 优化 问题 与 经典 PCA 问题 一样 鲁棒 
PCA 本质上 也是 寻找 数据 在 低维 空间 上 的 
最佳 投影 问题 对于 低 秩 数据 观测 矩阵 X 
假如 X 受到 随机 稀疏 噪声 的 影响 则 X 
的 低 秩 性 就会 破坏 使 X 变成 满秩的/nr 
所以 我们 就 需要 将 X 分解成 包含 其 真实 
结构 的 低 秩 矩阵 和 稀疏 噪声 矩阵 之和 
找到 了 低 秩 矩阵 实际上 就 找到 了 数据 
的 本质 低 维空间 那 有了 PCA 为什么 还有 这个 
Robust PCA 呢 Robust 在哪 因为 PCA 假设 我们 的 
数据 的 噪声 是 高斯 的 对于 大 的 噪声 
或者 严重 的 离群 点 PCA 会被 它 影响 导致 
无法 正常 工作 而 Robust PCA 则 不 存在 这个 
假设 它 只是 假设 它 的 噪声 是 稀疏 的 
而 不管 噪声 的 强弱 如何 由于/c rank/w 和/c L0/i 
范数/n 在/p 优化/vn 上/f 存在/v 非/h 凸/v 和非/nr 光滑/a 特性/n 
所以 我们 一般 将 它 转换成 求解 以 下一个 松弛 
的 凸 优化 问题 说 个 应用 吧 考虑 同 
一副 人脸 的 多幅 图像 如果 将 每 一副 人脸 
图像 看成 是 一个 行向量 并 将 这些 向量 组成 
一个 矩阵 的话 那么 可以 肯定 理论上 这个 矩阵 应当 
是 低 秩 的 但是 由于 在 实际 操作 中 
每幅 图像 会 受到 一定 程度 的 影响 例如 遮挡 
噪声 光照 变化 平移 等 这些 干扰 因素 的 作用 
可以 看做 是 一个 噪声 矩阵 的 作用 所以 我们 
可以 把 我们 的 同一个 人脸 的 多个 不同 情况下 
的 图片 各自 拉长 一列 然后 摆成 一个 矩阵 对 
这个 矩阵 进行 低 秩和 稀疏 的 分解 就 可以 
得到 干净 的 人脸 图像 低 秩 矩阵 和 噪声 
的 矩阵 了 稀疏 矩阵 例如 光照 遮挡 等等 至于 
这个 的 用途 你 懂得 3 背景 建模 背景 建模 
的 最简单 情形 是从 固定 摄相机 拍摄 的 视频 中 
分离 背景 和 前景 我们 将 视频 图像 序列 的 
每 一帧 图像 像素 值 拉成 一个 列 向量 那么 
多 个 帧 也就是 多个 列 向量 就 组成 了 
一个 观测 矩阵 由于 背景 比较稳定 图像 序列帧 与 帧 
之间 具有 极大 的 相似性 所以 仅 由 背景 像素 
组成 的 矩阵 具有 低 秩 特性 同时 由于 前景 
是 移动 的 物体 占据 像素 比例 较低 故 前景 
像素 组成 的 矩阵 具有 稀疏 特性 视频 观测 矩阵 
就是 这 两种 特性 矩阵 的 叠加 因此 可以 说 
视频 背景 建模 实现 的 过程 就是 低 秩 矩阵 
恢复 的 过程 4 变换 不 变低 秩 纹理 TILT 
以上 章节 所 介绍 的 针对 图像 的 低 秩 
逼近 算法 仅仅 考虑 图像 样本 之间 像素 的 相似性 
却 没有 考虑 到 图像 作为 二维 的 像素 集合 
其 本身 所 具有 的 规律性 事实上 对于 未 加 
旋转 的 图像 由于 图像 的 对称性 与 自 相似性 
我们 可以 将 其 看做 是 一个 带 噪声 的 
低 秩 矩阵 当 图像 由 端正 发生 旋转 时 
图像 的 对称性 和 规律性 就会 被 破坏 也 就是说 
各行 像素 间 的 线性 相关性 被 破坏 因此 矩阵 
的 秩 就会 增加 低 秩 纹理 映射 算法 T 
r a n s f o r m I n 
v a r i a n t Low rank Textures 
TILT 是 一种 用 低 秩 性 与 噪声 的 
稀疏 性 进行 低 秩 纹理 恢复 的 算法 它 
的 思想 是 通过 几何变换 τ 把 D 所 代表 
的 图像 区域 校正 成 正则 的 区域 如 具有 
横 平竖直 对称 等 特性 这些 特性 可以 通过 低 
秩 性 来 进行 刻画 低 秩 的 应用 非常 
多 大家 有 兴趣 的 可以 去 找些 资料 深入 
了解 下 四 规则化 参数 的 选择 现在 我们 回过头来 
看看 我们 的 目标 函数 里面/f 除了/p loss/w 和/c 规则/n 
项/n 两块/m 外/f 还有 一个 参数 λ 它 也 有个 
霸气 的 名字 叫 hyper parameters 超 参 你 不要 
看 它 势单力薄 的 它 非常 重要 它 的 取值 
很大 时候 会 决定 我们 的 模型 的 性能 事关 
模型 生死 它/r 主要/b 是/v 平衡/a loss/w 和/c 规则/n 项/n 
这两项/nz 的/uj λ 越大 就 表示 规则 项 要比 模型 
训练 误差 更重要 也 就是 相比 于要/nr 模型 拟合 我们 
的 数据 我们 更 希望 我们 的 模型 能 满足 
我们 约束 的 Ω w 的 特性 反之亦然 举个 极端 
情况 例如 λ = 0时 就 没有 后面 那 一项 
代价 函数 的 最小化 全部 取决于 第一项 也 就是 集 
全力 使得 输出 和 期待 输出 差别 最小 那 什么 
时候 差别 最小 啊 当然 是 我们 的 函数 或者 
曲线 可以 经过 所有 的 点了 这时候 误差 就 接近 
0 也 就是 过拟合 了 它 可以 复杂 的 代表 
或者 记忆 所有 这些 样本 但 对于 一个 新 来 
的 样本 泛化 能力 就 不行 了 毕竟/d 新的/i 样本/n 
会/v 和/c 训练样本/n 有/v 差别/d 的/uj 嘛/y 那 我们 真正 
需要 什么 呢 我们 希望 我们 的 模型 既可以 拟合 
我们 的 数据 又 具有 我们 约束 它 的 特性 
只有 它们 两者 的 完美 结合 才能 让 我们 的 
模型 在 我们 的 任务 上 发挥 强大 的 性能 
所以 如何 讨好 它 是 非常 重要 在这点上 大家 可能 
深有体会 还 记得 你 复现 了 很多 论文 然后 复现 
出来 的 代码 跑 出来 的 准确率 没有 论 文说 
的 那么 高 甚至 还 差 之 万里 这时候 你 
就会 怀疑 到底 是 论文 的 问题 还是 你 实现 
的 问题 实际上 除了 这 两个 问题 我们 还 需要 
深入 思考 另 一个 问题 论文 提出 的 模型 是否 
具有 hyper parameters 论文 给出 了 它们 的 实验 取值 
了吗 经验 取值 还是 经过 交叉 验证 的 取值 这个 
问题 是 逃不掉 的 因为 几乎 任何 一个 问题 或者 
模型 都会/nr 具有 hyper parameters 只是 有时候 它 是 隐藏着 
的 你 看不到 而已 但 一旦 你 发现 了 证明 
你俩 有缘 那请/nr 试着 去 修改 下 它 吧 有 
可能 有 奇迹 发生 哦 OK 回到 问题 本身 我们 
选择 参数 λ 的 目标 是 什么 我们/r 希望/v 模型/n 
的/uj 训练/vn 误差/n 和/c 泛化/v 能力/n 都/d 很强/i 这时候 你 
有 可能 还 反映 过来 这 不是 说 我们 的 
泛化 性能 是 我们 的 参数 λ 的 函数 吗 
那 我们 为什么 按 优化 那一套 选择 能 最大化 泛化 
性能 的 λ 呢 Oh sorry to tell you that 
因为 泛化 性能 并 不是 λ 的 简单 的 函数 
它 具有 很多 的 局部 最大值 而且 它 的 搜索 
空间 很大 所以 大家 确定 参数 的 时候 一是 尝试 
很多 的 经验值 这 和 那些 在 这个 领域 摸 
爬 打滚 的 大师 是 没 得比 的 当然 了 
对于 某些 模型 大师 们 也 整理 了 些 调 
参 经验 给 我们 例如 Hinton 大哥 的 那篇 A 
Practical Guide to Training R e s t r i 
c t e d B o l t z m 
a n n Machines 等等 还有 一种 方法 是 通过 
分析 我们 的 模型 来 选择 怎么做 呢 就是 在 
训练 之前 我们/r 大概/d 计/n 算下/v 这时候/r 的/uj loss/w 项的值/nr 
是/v 多少/m Ω w 的 值 是 多少 然后 针对 
他们 的 比例 来 确定 我们 的 λ 这种 启发式 
的 方法 会 缩小 我们 的 搜索 空间 另外 一种 
最 常见 的 方法 就是 交叉 验证 Cross validation 了 
先 把 我们 的 训练 数据库 分成 几份 然后 取 
一部 分做 训练 集 一 部分 做 测试 集 然后 
选择 不同 的 λ 用 这个 训练 集 来 训练 
N 个 模型 然后 用 这个 测试 集 来 测试 
我们 的 模型 取 N 模型 里面 的 测试 误差 
最小 对应 的 λ 来 作为 我们 最终 的 λ 
如果 我们 的 模型 一次 训练 时间 就 很长 了 
那么 很 明显 在 有限 的 时间 内 我们 只能 
测试 非常少 的 λ 例如 假设 我们 的 模型 需要 
训练 1天 这在 深度 学习 里面 是 家常便饭 了 然后 
我们 有 一个 星期 那 我们 只能 测试 7个 不同 
的 λ 这 就让 你 遇到 最好 的 λ 那是 
上辈子 积 下来 的 福气 了 那有/nr 什么/r 方法/n 呢/y 
两种 一是 尽量 测试 7个 比较 靠谱 的 λ 或者说 
λ 的 搜索 空间 我们 尽量 广点/nr 所以 一般 对 
λ 的 搜索 空间 的 选择 一般 就是 2 的 
多少 次方 了 从 10 到 10 啊 什么 的 
但 这种 方法 还是 不 大 靠谱 最好 的 方法 
还是 尽量 让 我们 的 模型 训练 的 时间 减少 
例如 假设 我们 优化 了 我们 的 模型 训练 使得 
我们 的 训练 时间 减少 到 2 个 小时 那么 
一个 星期 我们 就 可以 对模型 训练 7 * 24/2 
= 84次 也 就是说 我们 可以 在 84个 λ 里面 
寻找 最好 的 λ 这 让 你 遇见 最好 的 
λ 的 概率 就 大多 了吧 这 就是 为什么 我们 
要 选择 优化 也就是 收敛 速度快 的 算法 为什么 要 
用 GPU 多核 集群 等 来 进行 模型 训练 为什么 
具有 强大 计算机 资源 的 工业界 能做 很多 学术界 也 
做不了 的 事情 当然 了 大 数据 也 是 一个 
原因 的 原因 了 努力 做个 调 参 高手 吧 
祝愿 大家 都能/nr 调得 一手 好 参 五 参考资料 1 
  http / / fastml . com / large scale 
l1 feature selection with vowpal wabbit / 2   http 
/ / www . stat . purdue . edu / 
~ vishy / introml / notes / Optimization . pdf 
3   http / / www . stanford . edu 
/ ~ boyd / cvxbook / bv _ cvxbook . 
pdf 4   GradientDescent Wolfe s Condition and Logistic Regression 
5   http / / nm . mathforcollege . com 
/ mws / gen / 04sle / mws _ gen 
_ sle _ spe _ adequacy . pdf 本文 完全 
转 自 zouxy09 的 文章 机器学习 中的 范数 规则化 之 
一 L0 L1 与 L2 范数 机器学习 中的 范数 规则化 
之 二 核 范数 与 规则 项 参数 选择 版权 
声明 转载 请 注明 出处 http / / www . 
cnblogs . com / TenosDoIt / p / 3708996 . 
html 