一 引言 支持 向量 机 这 部分 确实 很多 想要 
真正 的 去 理解 它 不 仅仅 知道 理论 还要 
进行 相关 的 代码 编写 和 测试 二者 想 和 
结合 才能 更好 的 帮助 我们 理解 SVM 这一 非常 
优秀 的 分类 算法 支持 向量 机 是 一种 二类 
分类 算法 假设 一个 平面 可以 将 所有 的 样本 
分为 两类 位 于正 侧 的 样本 为 一类 值 
为 + 1 而 位于 负 一侧 的 样本 为 
另外 一类 值 为 1 我们 说 分类 不 仅仅 
是 将 不同 的 类别 样 本分 隔开 还要 以比 
较大 的 置信度 来 分隔 这些 样本 这样 才能 使 
绝大部分 样本 被 分开 比如 我们 想 通过 一个 平面 
将 两个 类别 的 样本 分开 如果 这些 样本 是 
线性 可分 或者 近视 线性 可分 那么 这样 的 平面 
有 很多 但是 如果 我们 加上 要 以 最大 的 
置信度 来 将 这些 样本 分开 那么 这样 的 平面 
只有 一条 那么 怎么 才能 找到 这样 的 平面 呢 
这里 不得不 提到 几个 概念 1 几何 间隔 几何 间隔 
的 概念 简单 理解 就是 样本 点到 分隔 平面 的 
距离 2 间隔 最大化 想要 间隔 最大化 我们 必须 找到 
距离 分隔 平面 最近 的 点 并且 使得 距离 平面 
最近 的 点 尽可能 的 距离 平面 最远 这样 每 
一个样 本就 都能够 以比 较大 的 置信度 被 分隔 开 
算法 的 分类 预测 能力 也 就 越好 显然 SVM 
算法 的 关键 所在 就是 找到 使得 间隔 最大化 的 
分隔 超平面 如果 特征 是 高 维度 的 情况 我们 
称 这样 的 平面 为 超平面 这里 关于 SVM 学习 
推荐 两本书 统计 学习 方法 李航 和 机器学习 实战 二者 
结合 可以 帮助 我们 理解 svm 算法 2 支持 向量 
机 关于 支持 向量 机 的 推导 无论是 书上 还是 
很多 很 优秀 的 博客 都写 的 非常 清楚 大家 
有 兴趣 可以 看 上面 推荐 的 统计 与 学习 
方法 书 写 的 浅显易懂 或者 看这 几篇 博客 http 
/ / blog . csdn . net / app _ 
12062011 / article / details / 50536369 机器学习 算法 支持 
向量 机 系列 博客 http / / blog . csdn 
. net / zouxy09 / article / details / 16955347http 
/ / blog . csdn . net / zouxy09 / 
article / details / 17291543http / / blog . csdn 
. net / zouxy09 / article / details / 17291805http 
/ / blog . csdn . net / zouxy09 / 
article / details / 17292011 这两位 博主 都 重点 讲解 
了 SVM 的 推导 过程 这里 我 就 本着 站在 
巨人 的 肩膀 上 的 思想 不再 赘述 我 的 
侧重点 在于 实际 的 代码 编写 上 比较 理论 总归 
要 回到 实践 上 这也 是 每个 算法 的 归宿 
所在 好了 下面 我 就 简要 写出 简要 介绍 一下 
线性 支持 向量 机 近似 线性 支持 向量 机 以及 
非线性 支持 向量 机 核 函数 1 线性 支持 向量 
机 求解 线性 支持 向量 机 的 过程 是 凸 
二次 规划 问题 所谓 凸 二次 规划 问题 就是 目标函数 
是 凸 的 二次 可微 函数 约束 函数 为 仿射 
函数 满足 f x = a * x + b 
a x 均为 n 为 向量 而 我们 说 求解 
凸 二次 规划 问题 可以 利用 对偶 算法 即 引入 
拉格朗日 算子 利用 拉格朗日 对偶性 将 原始 问题 的 最优 
解 问题 转化 为 拉格朗日 对偶 问题 这样 就将 求 
w * b 的 原始 问题 的 极小 问题 转化 
为求 alpha * alpha = 0 的 对偶 问题 的 
极大 问题 即 求出 alpha * 在 通过 KKT 条件 
求出 对应 的 参数 w * b 从而 找到 这样 
的 间隔 最大化 超平面 进而 利用 该 平面 完成 样本 
分类 2 近似 线性 支持 向量 机 当 数据集 并 
不是 严格 线性 可 分时 即 满足 绝不 部分 样本点 
是 线性 可分 存在 极少 部分 异常 点 这里 也 
就是说 存在 部分 样本 不能 满足 约束条件 此时 我们 可以 
引入 松弛 因子 这样 这些 样本 点到 超平面 的 函数 
距离 加上 松弛 因子 就能 保证 被 超平面 分隔 开来 
当然 添加 了 松弛 因子 sigma 我们 也 会 添加 
对应 的 代价 项 使得 alpha 满足 0 = alpha 
= C3 非线性 支持 向量 机 显然 当 数据集 不是 
线性 可分 的 即 我们 不能 通过 前面 的 线性 
模型 来 对 数据 集 进行 分类 此时 我们 必须 
想 办法 将 这些 样本 特征 符合 线性 模型 才能 
通过 线性 模型 对 这些 样本 进行 分类 这 就要 
用到 核 函数 核 函数 的 功能 就是 将 低维 
的 特征 空间 映 射到 高维 的 特征 空间 而在 
高维 的 特征 空间 中 这些 样本 进过 转化 后 
变成 了 线性 可分 的 情况 这样 在 高维空间 中 
我们 就 能够 利用 线性 模型 来 解决 数据集 分类 
问题 好了 我们 就 只讲 这么 写 大致 的 概念 
如果 想要 透彻 理解 SVM 建议 还是 要 看看 上面 
的 书 和 博客 文章 篇幅 有限 我 这里 的 
中心 在于 凸 二次 规划 的 优化 算法 SMO 序列 
最小 最优化 算法 3 SMO 算法 SMO 是 一种 用于 
训练 SVM 的 强大 算法 它 将 大 的 优化 
问题 分解 为 多个 小 的 优化 问题 来 进行 
求解 而 这些 小 优化 问题 往往 很容易 求解 并且 
对 它们 进行 顺序 求 解和 对 整体 求解 结果 
是 一致 的 在 结果 一致 的 情况 下 显然 
SMO 算法 的 求解 时间 要 短 很多 这样 当 
数据集 容量 很大 时 SMO 就是 一致 十分 高效 的 
算法 SMO 算法 的 目标 是 找到 一 系列 alpha 
和b/nr 而 求出 这些 alpha 我们 就 能 求出 权重 
w 这样 就 能 得到 分隔 超平面 从而 完成 分类 
任务 SMO 算法 的 工作 原理 是 每次 循环 中 
选择 两个 alpha 进行 优化 处理 一旦 找到 一 对 
合适 的 alpha 那么 就 增大 其中 一个 而 减少 
另外 一个 这里 的 合适 意味着 在 选择 alpha 对时 
必须 满足 一定 的 条件 条件 之一 是 这 两个 
alpha 不满足 最优化 问题 的 kkt 条件 另外 一个 条件 
是 这 两个 alpha 还 没有 进行 区间 化 处理 
对于 SMO 算法 编写 我们 采用 由 简单 到 复杂 
的 方法 层层 递进 完成 最终 的 SMO 算法 实现 
最后 通过 实际 的 用 例 对 SVM 模型 进行 
训练 并 验证 准确性 1 简化版 SMO 算法 简化版 SMO 
算法 省略 了 确定 要 优化 的 最佳 alpha 对 
的 步骤 而是 首先 在 数据 集上 进行 遍历 每一个 
alpha 再在 剩余 的 数据 集中 找到 另外 一个 alpha 
构成 要 优化 的 alpha 对 同时 对 其 进行 
优化 这里 的 同时 是 要 确保 公式 Σ α 
i * label i = 0 所以 改变 一个 alpha 
显然 会 导致 等式 失效 所以 这里 需要 同时 改变 
两个 alpha 接下 来看 实际 的 代码 简易版 SMO 算法 
的 辅助 函数 # SMO 算法 相关 辅助 中的 辅助 
函数 # 1 解析 文本 数据 函数 提取 每个 样本 
的 特征 组成 向量 添加到 数据 矩阵 # 添加 样本 
标 签到 标签 向量 def loadDataSet filename dataMat = labelMat 
= fr = open filename for line in fr . 
readlines lineArr = line . strip . split \ t 
dataMat . append float lineArr 0 float lineArr 1 labelMat 
. append float lineArr 2 return dataMat labelMat # 2 
在 样本 集中 采取 随机 选择 的 方法 选取 第二 
个 不等 于 第一 个 alphai 的 # 优化 向量 
alphaj def selectJrand i m j = i while j 
= = i j = int random . uniform 0 
m return j # 3 约束 范围 L = alphaj 
= H 内 的 更新 后的/nr alphaj 值 def clipAlpha 
aj H L if aj H aj = H if 
L aj aj = L return aj 上面 是 简易 
版 SMO 算法 需要 用到 的 一些 功能 我们 将 
其 包装 成 函数 需要 时 调用 即可 接下 来看 
算法 的 伪代码 # SMO 算法 的 伪代码 # 创建 
一个 alpha 向量 并 将其 初始 化为 0 向量 # 
当 迭代 次数 小于 最大 迭代 次数 时 w 外循环 
# 对 数据 集中 每个 数据 向量 内循环 # 如果 
该 数据 向量 可以 被 优化 # 随机 选择 另外 
一个 数据 向量 # 同时 优化 这两个 向量 # 如果 
两个 向量 都 不能 被 优化 退出 内循环 # 如果 
所有 向量 都 没有 被 优化 增加 迭代 次数 继续 
下一次 循环 实际 代码 如下 # @ dataMat 数据 列表 
# @ classLabels 标签 列表 # @ C 权衡 因子 
增加 松弛 因子 而在 目标 优化 函数 中 引入 了 
惩罚 项 # @ toler 容错率 # @ maxIter 最大 
迭代 次数 def smoSimple dataMat classLabels C toler maxIter # 
将 列表 形式 转为 矩阵 或 向量 形式 dataMatrix = 
mat dataMatIn labelMat = mat classLabels . transpose # 初始化 
b = 0 获取 矩阵 行列 b = 0 m 
n = shape dataMatrix # 新建 一个 m 行 1列 
的 向量 alphas = mat zeros m 1 # 迭代 
次数 为 0 iter = 0 while iter maxIter # 
改变 的 alpha 对数 a l p h a P 
a i r s C h a n g e 
d = 0 # 遍历 样本 集中 样本 for i 
in range m # 计算 支持 向量 机 算法 的 
预测 值 fXi = float multiply alphas labelMat . T 
* \ dataMatrix * dataMatrix i . T + b 
# 计算 预测值 与 实际 值 的 误差 Ei = 
fXi float labelMat i # 如果 不 满足 KKT 条件 
即 labelMat i * fXi 1 labelMat i * fXi 
1 toler # and alpha C 或者 labelMat i * 
fXi 1 labelMat i * fXi 1 toler and alpha 
0 if labelMat i * Ei toler and alpha C 
or \ labelMat i * Ei toler and alpha i 
0 # 随机 选择 第二个 变量 alphaj j = selectJrand 
i m # 计算 第二个 变量 对应 数据 的 预测 
值 fXj = float multiply alphas labelMat . T * 
\ dataMatrix * dataMatrix j . T + b # 
计算 与 测试 与 实际 值 的 差值 Ej = 
fXj float label j # 记录 alphai 和 alphaj 的 
原始 值 便于 后续 的 比较 alphaIold = alphas i 
. copy alphaJold = alphas j . copy # 如何 
两个 alpha 对应 样本 的 标签 不相同 if labelMat i 
= labelMat j # 求出 相应 的 上下 边界 L 
= max 0 alphas j alphas i H = min 
C C + alphas j alphas i else L = 
max 0 alphas j + alphas i C H = 
min C alphas j + alphas i if L = 
= H print L = = H continue # 根据 
公式 计算 未经 剪辑 的 alphaj # eta = 2.0 
* dataMatrix i * dataMatrix j . T \ dataMatrix 
i * dataMatrix i . T \ dataMatrix j * 
dataMatrix j . T # 如果 eta = 0 跳出 
本次 循环 if eta = 0 print eta = 0 
continue alphas j = labelMat j * Ei Ej / 
eta alphas j = clipAlpha alphas j H L # 
# 如果 改变 后的/nr alphaj 值 变化 不大 跳出 本次 
循环 if abs alphas j alphaJold 0.00001 print j not 
moving \ enough continue # 否则 计算 相应 的 alphai 
值 alphas i + = labelMat j * labelMat i 
* alphaJold alphas j # 再 分别 计算 两个 alpha 
情况下 对于 的 b 值 b1 = b Ei labelMat 
i * alphas i alphaIold * \ dataMatrix i * 
dataMat i . T \ labelMat j * alphas j 
alphaJold * \ dataMatrix i * dataMatrix j . T 
b2 = b Ej labelMat i * alphas i alphaIold 
* \ dataMatrix i * dataMatrix j . T \ 
labelMat j * alphas j alphaJold * \ dataMatrix j 
* dataMatrix j . T # 如果 0 alphai C 
那么 b = b1 if 0 alphas i and C 
alphas i b = b1 # 否则 如果 0 alphai 
C 那么 b = b1 elif 0 alphas j and 
C alphas j b = b2 # 否则 alphai alphaj 
= 0 或 C else b = b1 + b2 
/ 2.0 # 如果 走 到此 步 表面 改变 了 
一对 alpha 值 a l p h a P a 
i r s C h a n g e d 
+ = 1 print iter & d i % d 
paird changed % d % iter i a l p 
h a P a i r s C h a 
n g e d # 最后 判断 是否 有 改变 
的 alpha 对 没有 就 进行 下一次 迭代 if a 
l p h a P a i r s C 
h a n g e d = = 0 iter 
+ = 1 # 否则 迭代 次数 置 0 继续 
循环 else iter = 0 print iteration number % d 
% iter # 返回 最后 的 b 值 和 alpha 
向量 return b alphas 上面 的 代码 量 看起来 很多 
但 事实上 只要 理解 了 SVM 算法 的 理论 知识 
就 很容易 理解 其 只不过是 将 理论 转化 为 机器 
可以 运行 的 语言 而已 上面 代码 在 一台 性能 
一般 的 笔记本 上 对 100个 样本 的 数据 集上 
运行 收敛 时间 14.5秒 取得 了 令人满意 的 分类 效果 
当然 上面 的 代码 通过 对 整个 数据集 进行 两次 
遍历 的 方法 来 寻找 alpha 对 的 方法 显然 
存在 一定 的 不足 如果 数据集 规模 较小 的 情况 
下 或许 还 可以 满足 要求 但是 对于 大 规模 
的 数据 集 而言 上面 的 代码 显然 收敛 速度 
非常 慢 所以 接下来 我们 在 此 基础上 对 选取 
合适 的 alpha 对 方法 进行 改进 采用 启发式 的 
方法 来 选取 合适 的 alpha 对 从而 提升 运算 
效率 2 启发式 选取 alpha 变量 的 SMO 算法 启发式 
的 SMO 算法 一个 外循环 来 选择 第一个 alpha 值 
并且 其 选择 过程 会 在下面 两种 方法 之间 进行 
交替 1 在 所有 数据集 上 进行 单遍/nr 扫描 2 
另一种 方法 是 在 间隔 边界 上 样本点 进行 单遍/nr 
扫描 所谓 间隔 边界 上 的 点 即为 支持 向量 
点 显然 对于 整个 数据集 遍历 比较 容易 而 对于 
那些 处于 间隔 边界 上 的 点 我们 还 需要 
事先 将 这些 点 对应 的 alpha 值 找出来 存放 
在 一个 列表 中 然后 对 列表 进行 遍历 此外 
在 选择 第一 个 alpha 值 后 算法 会 通过 
一个 内循环 来 选择 第二个 值 在 优化 的 过程 
中 依据 alpha 的 更新 公式 α new unc = 
aold + label * Ei Ej / η η = 
dataMat i * dataMat i . T + dataMat j 
* dataMat j . T 2 * dataMat i * 
dataMat j . T 可知 alpha 值 的 变化 程度 
更 Ei Ej 的 差值 成正比 所以 为了 使 alpha 
有 足够 大 的 变化 选择 使 Ei Ej 最大 
的 alpha 值 作为 另外 一个 alpha 所以 我们 还 
可以 建立 一个 全局 的 缓存 用于 保存 误差值 便于 
我们 选择 合适 的 alpha 值 下面 是 创建 的 
一个 数据结构 类 便于 我们 存取 算法 中 需要 用到 
的 重要 数据 # 启发式 SMO 算法 的 支持 函数 
# 新建 一个 类 的 收据 结构 保存 当前 重要 
的 值 class optStruct def _ _ init _ _ 
self dataMatIn classLabels C toler self . X = dataMatIn 
self . labelMat = classLabels self . C = C 
self . tol = toler self . m = shape 
dataMatIn 0 self . alphas = mat zeros self . 
m 1 self . b = 0 self . eCache 
= mat zeros self . m 2 # 格式化 计算误差 
的 函数 方便 多次 调用 def calcEk oS k fXk 
= float multiply oS . alphas oS . labelMat . 
T * \ oS . X * oS . X 
k . T + oS . b Ek = fXk 
float oS . labelMat k return Ek # 修改 选择 
第二个 变量 alphaj 的 方法 def selectJ i oS Ei 
maxK = 1 maxDeltaE = Ej = 0 # 将 
误差 矩阵 每 一行 第一列 置 1 以此 确定 出 
误差 不为 0 # 的 样本 oS . eCache i 
= 1 Ei # 获取 缓存 中 Ei 不为 0 
的 样本 对应 的 alpha 列表 validEcacheList = nonzero oS 
. Cache 0 . A 0 # 在 误差 不为 
0 的 列表 中 找出 使 abs Ei Ej 最大 
的 alphaj if len validEcacheList 0 for k in validEcacheList 
if k = = i continue Ek = calcEk oS 
k deltaE = abs Ei Ek if deltaE maxDeltaE maxK 
= k maxDeltaE = deltaE Ej = Ek return maxK 
Ej else # 否则 就从 样本 集中 随机 选取 alphaj 
j = selectJrand i oS . m Ej = calcEk 
oS j return j Ej # 更新 误差 矩阵 def 
updateEk oS k Ek = calcEk oS k oS . 
eCache k = 1 Ek 好了 有了/nr 这些 辅助性 的 
函数 我们 就 可以 很 容易 的 实现 启发式 的 
SMO 算法 的 具体 代码 # SMO 外循环 代码 def 
smoP dataMatIn classLabels C toler maxIter kTup = lin 0 
# 保存 关键 数据 oS = optStruct mat dataMatIn mat 
classLabels . transpose C toler iter = 0 enrireSet = 
True a l p h a P a i r 
s C h a n g e d = 0 
# 选取 第一个 变量 alpha 的 三种 情况 从 间隔 
边界 上 选取 或者 整个 数据集 while iter maxIter and 
a l p h a P a i r s 
C h a n g e d 0 or entireSet 
a l p h a P a i r s 
C h a n g e d = 0 # 
没有 alpha 更新 对 if entireSet for i in range 
oS . m a l p h a P a 
i r s C h a n g e d 
+ = innerL i oS print fullSet iter % d 
i % d pairs changed % d % \ iter 
i a l p h a P a i r 
s C h a n g e d else # 
统计 alphas 向量 中 满足 0 alpha C 的 alpha 
列表 nonBoundIs = nonzero oS . alphas . A 0 
* oS . alphas . A C 0 for i 
in nonBoundIs a l p h a P a i 
r s C h a n g e d + 
= innerL i oS print non bound iter % d 
i % d pairs changed % d % \ iter 
i a l p h a P a i r 
s C h a n g e d iter + 
= 1 if entireSet entireSet = False # 如果 本次 
循环 没有 改变 的 alpha 对 将 entireSet 置 为 
true # 下个 循环 仍 遍历 数据集 elif a l 
p h a P a i r s C h 
a n g e d = = 0 entireSet = 
True print iteration number % d % iter return oS 
. b oS . alphas # 内循环 寻找 alphaj def 
innerL i oS # 计算误差 Ei = calcEk oS i 
# 违背 kkt 条件 if oS . labelMat i * 
Ei oS . tol and oS . alphas i oS 
. C or \ oS . labelMat i * Ei 
oS . tol and oS . alphas i 0 j 
Ej = selectJ i oS Ei alphaIold = alphas i 
. copy alphaJold = alphas j . copy # 计 
算上 下界 if oS . labelMat i = oS . 
labelMat j L = max 0 oS . alphas j 
oS . alphas i H = min oS . C 
oS . C + oS . alphas j oS . 
alphas i else L = max 0 oS . alphas 
j + oS . alphas i oS . C H 
= min oS . C oS . alphas j + 
oS . alphas i if L = = H print 
L = = H return 0 # 计算 两个 alpha 
值 eta = 2.0 * oS . X i * 
oS . X j . T oS . X i 
* oS . X i . T \ oS . 
X j * oS . X j . T if 
eta = 0 print eta = 0 return 0 oS 
. alphas j = oS . labelMat j * Ei 
Ej / eta oS . alphas j = clipAlpha oS 
. alphas j H L updateEk oS j if abs 
oS . alphas j alphaJold 0.00001 print j not moving 
enough return 0 oS . alphas i + = oS 
. labelMat j * oS . labelMat i * \ 
alphaJold oS . alphas j updateEk oS i # 在 
这两个 alpha 值 情况 下 计算 对应 的 b 值 
# 注 非线性 可分 情况 将 所有 内积 项 替换 
为 核 函数 K i j b1 = oS . 
b Ei oS . labelMat i * oS . alphas 
i alphaIold * \ oS . X i * oS 
. X i . T \ oS . labelMat j 
* oS . alphas j alphaJold * \ oS . 
X i * oS . X j . T b2 
= oS . b Ej oS . labelMat i * 
oS . alphas i alphaIold * \ oS . X 
i * oS . X j . T \ oS 
. labelMat j * oS . alphas j alphaJold * 
\ oS . X j * oS . X j 
. T if 0 oS . alphas i and oS 
. C oS . alphas i oS . b = 
b1 elif 0 oS . alphas j and oS . 
C oS . alphas j oS . b = b2 
else oS . b = b1 + b2 / 2.0 
# 如果 有 alpha 对 更新 return 1 # 否则 
返回 0 else return 0 显然 上面 的 SMO 完整 
代码 是 分为 内外 两 个 循环 函 数来 编写 
的 采取 这样 的 结构 可以 更 方便 我们 去 
理解 选取 两个 alpha 的 过程 既然 我们 已经 计算 
出了 alpha 值 和b值/nr 那么 显然 我们 可以 利用 公式 
w * = Σ α i * label i * 
dataMat i 计算 出 相应 的 权值 参数 然后 就 
可以 得到 间隔 超平面 的 公式 w * x + 
b * 来 完成 样本 的 分类 了 由于 SVM 
算法 是 一种 二类 分类 算法 正值 为 1 负值 
为 1 即 分类 的 决策函数 为 跳跃 函数 sign 
w * x + b * 然后 我们 可以 编写 
一 小段 测试代码 来 利用 SMO 算法 得到 的 alpha 
值 和b值/nr 计算 分类 决策函数 从而 实现 具体 的 预测 
分类 了 # 求 出了 alpha 值 和 对应 的 
b 值 就 可以 求出 对应 的 w 值 以及 
分类 函数值 def predict alphas dataArr classLabels X = mat 
dataArr labelMat = mat classLabels m n = shape X 
w = zeros n 1 for i in range m 
w + = multiply alphas i * labelMat i X 
i . T result = dataArr 0 * mat ws 
+ b return sign result 看一下 分类 效果 3 核 
函数 核 函数 的 目的 主要 是 为了 解决 非线性 
分类 问题 通过 核 技巧 将 低维 的 非线性 特征 
转化 为 高维 的 线性 特征 从而 可以 通过 线性 
模型 来 解决 非线性 的 分类 问题 如 下图 当 
数据集 不是 线性 可 分时 即 数据集 分布 是 下面 
的 圆形 该 怎么办 呢 显然 此时 数据集 线性 不可分 
我们 无法 用 一个 超平面 来 将 两种 样 本分 
隔开 那么 我们 就 希望 将 这些 数据 进行 转化 
转化 之后 的 数据 就 能够 通过 一个 线性 超平面 
将 不同 类别 的 样本 分开 这 就 需要 核 
函数 核 函数 的 目的 主要 是 为了 解决 非线性 
分类 问题 通过 核 技巧 将 低维 的 非线性 特征 
转化 为 高维 的 线性 特征 从而 可以 通过 线性 
模型 来 解决 非线性 的 分类 问题 而 径向 基 
核 函数 是 SVM 中 常用 的 一个 核 函数 
径向 基 核 函数 是 一个 采用 向量 作为 自变量 
的 函数 能够 基于 向量 距离 运算 输出 一个 标量 
径向 基 核 函数 的 高斯 版本 公式 为 k 
x y = exp | | x y | | 
2/2 σ 2 其中 σ 为 到达 率 决定了 函数值 
跌落 至 0 的 速度 下面 通过 代码 编写 高斯 
核 函数 # 径向 基 核 函数 是 svm 常用 
的 核 函数 # 核 转换 函数 def kernelTrans X 
A kTup m n = shape X K = mat 
zeros m 1 # 如 果核 函数 类型 为 lin 
if kTup 0 = = lin K = X * 
A . T # 如 果核 函数 类型 为 rbf 
径向 基 核 函数 # 将 每个 样本 向量 利用 
核 函数 转为 高维空间 elif kTup 0 = = rbf 
for j in range m deltaRow = X j A 
K j = deltaRow * deltaRow . T K = 
exp K / 1 * kTup 1 * * 2 
else raise NameError Houston we Have a Problem \ That 
Kernel is not recognised return K # 对 核 函数 
处理 的 样本 特征 存入 到 optStruct 中 class optStruct 
def _ _ init _ _ self dataMatIn classLabels C 
toler kTup self . X = dataMatIn self . labelMat 
= classLabels self . C = C self . tol 
= toler self . m = shape dataMatIn 0 self 
. alphas = mat zeros self . m 1 self 
. b = 0 self . eCache = mat zeros 
self . m 2 self . K = mat zeros 
self . m self . m for i in range 
self . m self . K i = kernelTrans self 
. X self . X i kTup 需要 说明 的 
是 这里 引入 了 一个 变量 kTup kTup 是 一个 
包含 核 信息 的 元组 它 提供 了 选取 的 
核 函数 的 类型 比如 线性 lin 或者 径向 基 
核 函数 rbf 以及 用户 提供 的 到达 率 σ 
有了 高斯 核 函数 之后 我们 只要 将 上面 的 
SMO 算法 中 所有 的 内积 项 替换 为 核 
函数 即可 比如 讲 dataMat i * dataMat j . 
T 替换 为 k i j 即可 替换 效果 如下 
def innerL i oS # 计算误差 Ei = calcEk oS 
i # 违背 kkt 条件 if oS . labelMat i 
* Ei oS . tol and oS . alphas i 
oS . C or \ oS . labelMat i * 
Ei oS . tol and oS . alphas i 0 
j Ej = selectJ i oS Ei alphaIold = alphas 
i . copy alphaJold = alphas j . copy # 
计 算上 下界 if oS . labelMat i = oS 
. labelMat j L = max 0 oS . alphas 
j oS . alphas i H = min oS . 
C oS . C + oS . alphas j oS 
. alphas i else L = max 0 oS . 
alphas j + oS . alphas i oS . C 
H = min oS . C oS . alphas j 
+ oS . alphas i if L = = H 
print L = = H return 0 # 计算 两个 
alpha 值 eta = 2.0 * oS . K i 
j oS . K i i oS . K j 
j if eta = 0 print eta = 0 return 
0 oS . alphas j = oS . labelMat j 
* Ei Ej / eta oS . alphas j = 
clipAlpha oS . alphas j H L updateEk oS j 
if abs oS . alphas j alphaJold 0.00001 print j 
not moving enough return 0 oS . alphas i + 
= oS . labelMat j * oS . labelMat i 
* \ alphaJold oS . alphas j updateEk oS i 
# 在 这两个 alpha 值 情况 下 计算 对应 的 
b 值 # 注 非线性 可分 情况 将 所有 内积 
项 替换 为 核 函数 K i j b1 = 
oS . b Ei oS . labelMat i * oS 
. alphas i alphaIold * \ oS . K i 
i \ oS . labelMat j * oS . alphas 
j alphaJold * \ oS . k i j b2 
= oS . b Ej oS . labelMat i * 
oS . alphas i alphaIold * \ oS . k 
i j \ oS . labelMat j * oS . 
alphas j alphaJold * \ oS . k i j 
if 0 oS . alphas i and oS . C 
oS . alphas i oS . b = b1 elif 
0 oS . alphas j and oS . C oS 
. alphas j oS . b = b2 else oS 
. b = b1 + b2 / 2.0 # 如果 
有 alpha 对 更新 return 1 # 否则 返回 0 
else return 0 有了 核 函数 我们 就 能对 非线性 
的 数据 集 进行 分类 预测 了 接下来 就是 编写 
代码 利用 核 函数 进行 测试 需要 说明 的 是 
在 优化 的 过程 中 我们 仅仅 需要 找到 支持 
向量 和其/nr 对应 的 alpha 值 而 对于 其他 的 
样本 值 可以 不用 管 甚至 可以 舍弃 因为 这些 
样本 将 不会 对 分类 预测 函数 造成 任何 影响 
这 也 就是 SVM 相比 KNN 算法 的 优秀 的 
地方 所在 # 测试 核 函数 # 用户 指定 到达 
率 def testRbf k1 = 1.3 # 第一 个 测试 
集 dataArr labelArr = loadDataSet testSetRBF . txt b alphas 
= smoP dataArr labelArr 200 0.0001 10000 rbf k1 dataMat 
= mat dataArr labelMat = mat labelArr . transpose svInd 
= nonzero alphas . A 0 0 sVs = dataMat 
svInd labelSV = labelMat svInd print there are % d 
Support Vectors % shape sVs 0 m n = shape 
dataMat errorCount = 0 for i in range m kernelEval 
= kernelTrans sVs dataMat i rbf k1 predict = kernelEval 
. T * multiply labelSV alphas svInd + b if 
sign predict = sign labelArr i errorCount + = 1 
print the training error rate is % f % float 
errorCount / m # 第二 个 测试 集 dataArr labelArr 
= loadDataSet testSetRBF2 . txt dataMat = mat dataArr labelMat 
= mat labelArr . transpose errorCount = 0 m n 
= shape dataMat for i in range m kernelEval = 
kernelTrans sVs dataMat i rbf k1 predict = kernelEval . 
T * multiply labelSV alphas svInd + b if sign 
predict = sign labelArr i errorCount + = 1 print 
the training error rate is % f % float errorCount 
/ m 当 用户 输入 σ = 1.3时 的 实验 
结果 为 当 σ = 0.1时 实验 结果 为 通过 
输入 不同 的 σ 值 当然 迭代 次数 也会 有 
一定 的 影响 我们 只 讨论 σ 值 我们 发现 
测试 错误率 训练 误差率 支持 向量 个数 都会 发生 变化 
在 一定 的 范围 内 支持 向量 数目 的 下降 
会/v 使得/v 训练/vn 错误率/n 和/c 测试/vn 错误率/n 都/d 下降/v 但是 
当 抵达 某处 的 最优 值 时 再次 通过 增大 
σ 值 的 方法 减少 支持 向量 此时 训练 错误率 
下降 而 测试 误差 上升 简言之 对于 固定 的 数据 
集 支持 向量 的 数目 存在 一个 最优 值 如果 
支持 向量 太少 会 得到 一个 很 差 的 决策 
边界 而 支持 向量 太多 也 相当于 利用 整个 数据集 
进行 分类 就 类似于 KNN 算法 显然 运算速度 不高 三 
SVM 实例 手写识别 问题 相较 于 第二张 的 KNN 算法 
尽管 KNN 也能 取得 不错 的 效果 但是 从 节省 
内存 的 角度 出发 显然 SVM 算法 更胜一筹 因为 其 
不 需要 保存 真个 数据集 而 只 需要 其 作用 
的 支持 向量 点 而 取得 不错 的 分类 效果 
# 实例 手写识别 问题 # 支持 向量 机 由于 只 
需要 保存 支持 向量 所以 相对于 KNN 保存 整个 数据集 
占用 更少 内存 # 且 取得 可比 的 效果 # 
基于 svm 的 手写 数字 识别 def loadImages dirName from 
os import listdir hwLabels = trainingFileList = listdir dirName m 
= len trainingFileList trainingMat = zeros m 1024 for i 
in range m fileNameStr = trainingFileList i fileStr = fileNameStr 
. split . 0 classNumStr = int fileStr . split 
_ 0 if classNumStr = = 9 hwLabels . append 
1 else hwLabels . append 1 trainingMat i = img2vector 
% s / % s % dirName fileNameStr return hwLabels 
trainingMat # 将 图像 转为 向量 def img2vector fileaddir featVec 
= zeros 1 1024 fr = open filename for i 
in range 32 lineStr = fr . readline for j 
in range 32 featVec 0 32 * i + j 
= int lineStr j return featVec # 利用 svm 测试 
数字 def testDigits kTup = rbf 10 # 训练 集 
dataArr labelArr = loadDataSet trainingDigits b alphas = smoP dataArr 
labelArr 200 0.0001 10000 kTup dataMat = mat dataArr labelMat 
= mat labelArr . transpose svInd = nonzero alphas . 
A 0 0 sVs = dataMat svInd labelSV = labelMat 
svInd print there are % d Support Vectors % shape 
sVs 0 m n = shape dataMat errorCount = 0 
for i in range m kernelEval = kernelTrans sVs dataMat 
i kTup predict = kernelEval . T * multiply labelSV 
alphas svInd + b if sign predict = sign labelArr 
i errorCount + = 1 print the training error rate 
is % f % float errorCount / m # 测试 
集 dataArr labelArr = loadDataSet testDigits . txt dataMat = 
mat dataArr labelMat = mat labelArr . transpose errorCount = 
0 m n = shape dataMat for i in range 
m kernelEval = kernelTrans sVs dataMat i rbf k1 predict 
= kernelEval . T * multiply labelSV alphas svInd + 
b if sign predict = sign labelArr i errorCount + 
= 1 print the training error rate is % f 
% float errorCount / m 下面 来 看一下 在 kTup 
= rbf 20 情况下 的 测试 误差率 和 支持 向量 
个数 情况 并且 通过 尝试 不同 的 σ 值 以及 
尝试 了 线性 核 函数 可以 得到 关于 不同 σ 
值 的 书写 数字 识别 性能 内核 模式 设置 训练 
错误率 % 测试 错误率 % 支持 向量 数 rbf 0 
. 1052402rbf 503 . 2402rbf 1000 . 599rbf 500.22 . 
241rbf 1004.54 . 326Linear2 . 72.238 由 上图 可以 看出 
σ 值 在 取 10时 取得 了 最好 的 分类 
效果 这也 印证 了 我们 上面 的 叙述 即 对于 
固定 的 数据 集 存在 最优 的 支持 向量 个数 
使得 分类 错误率 最低 支持 向量 的 个数 会 随着 
σ 值 的 增大 而 逐渐 减少 但是 分类 错误率 
确实 一个 先 降低 后 升高 的 过程 即 最小 
的 分类 错误率 并不 意味着 最少 的 支持 向量 个数 
4 总结 支持 向量 机 是 一种 通过 求解 凸 
二次 规划 问题 来 解决 分类 问题 的 算法 具有 
较低 的 泛化 错误率 而 SMO 算法 可以 通过 每次 
只 优化 两个 alpha 值 来 加快 SVM 的 训练 
速度 核 技巧 是 将 数据 由 低 维空间 映 
射到 高维空间 可以 将 一个 低 维空间 中的 非线性 问题 
转换 为 高维空间 下 的 线性 问题 来 求解 而 
径向 基 核 函数 是 一个 常用 的 度量 两个 
向量 距离 的 核 函数 最后 支持 向量 机 的 
优缺点 优点 泛化 错误率 低 计算 开销 不大 缺点 对 
参数 调节 和核/nr 函数 的 选择 敏感 且 仅 适用 
于 二类 分类 