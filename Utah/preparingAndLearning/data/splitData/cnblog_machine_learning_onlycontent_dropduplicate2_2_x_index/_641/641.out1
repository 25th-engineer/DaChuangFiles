机器学习 可分为 监督 学习 和无/nr 监督 学习 有/v 监督/vn 学习/v 
就是/d 有/v 具体/a 的/uj 分类信息/i 比如 用 来 判定 输入 
的 是 输入 a b c 中的 一类 无 监督 
学习 就是 不 清楚 最后 的 分类 情况 也 不会 
给 目标值 K 近邻 算法 属于 一种 监督 学习 分类 
算法 该 方法 的 思路 是 如果 一个 样本 在 
特征 空间 中的 k 个 最 相似 即 特征 空间 
中最 邻近 的 样本 中 的 大多数 属于 某 一个 
类别 则 该 样本 也 属于 这个 类别 需要 进行 
分类 分类 的 依据 是 什么 呢 每个 物体 都有 
它 的 特征点 这个 就是 分类 的 依据 特征点 可以 
是 很多 越多 分类 就越 精确 机器学习 就是 从 样本 
中 学习 分类 的 方式 那么 就 需要 输入 我们 
的 样本 也 就是 已经 分好 类 的 样本 比如 
特征点 是 A B2 个 特征 输入 的 样本 甲乙丙丁 
分别为 1.0 1.1 1.0 1.0 0 . 0 . 0.0 
0.1 那么 就 开始 输入 目标值 当然 也 要给 特征 
了 最终 的 目标 就是 看 特征 接近 A 的 
多 还是 B 的 多 如果 把 这些 当做 坐标 
几个 特征 点 就是 几 纬 坐标 那么 就是 坐标 
之间 的 距离 那么 问题 来了 要 怎么 看 接近 
A 的 多 还是 B 的 多 我 就 直接 
贴 代码 了 基于 python 首先 输入 特征 量 labels 
和 样本 group 一 开始 需要 导入 的 模块 # 
coding = utf 8 # 科学计算 包 # from numpy 
import * import numpy # 运算符 模块 import operator 数据 
样本 和 分类 模拟 # 手动 建立 一个 数据源 矩阵 
group 和 数据源 的 分类 结果 labels def createDataSet group 
= numpy . array 1.0 1.1 1.0 1.0 5 . 
2 . 5.0 0.1 labels = A A B B 
return group labels 然后 进行 KNN 算法 # newInput 为 
输入 的 目标 dataset 是 样本 的 矩阵 label 是 
分类 k 是 需要 取 的 个数 def kNNClassify newInput 
dataSet labels k # 读取 矩阵 的 行数 也 就是 
样本 数量 numSamples = dataSet . shape 0 print numSamples 
numSamples # 变成 和 dataSet 一样 的 行数 行数 = 
原来 * numSamples 列数 = 原来 * 1 然后 每个 
特征点 和 样本 的 点 进行 相减 diff = numpy 
. tile newInput numSamples 1 dataSet print diff diff # 
平方 squaredDiff = diff * * 2 print squaredDiff squaredDiff 
# axis = 0 按 列 求和 1 为 按 
行 求和 squaredDist = numpy . sum squaredDiff axis = 
1 print squaredDist squaredDist # 开 根号 距离 就 出来 
了 distance = squaredDist * * 0.5 print distance distance 
# 按 大小 逆序 排列 s o r t e 
d D i s t I n d i c 
e s = numpy . argsort distance print s o 
r t e d D i s t I n 
d i c e s s o r t e 
d D i s t I n d i c 
e s classCount = { } for i in range 
k # 返回 距离 key 对应 类别 value voteLabel = 
labels s o r t e d D i s 
t I n d i c e s i print 
voteLabel voteLabel # 取 前 几个 K 值 但是 K 
前 几个 值 的 大小 没有 去 比较 都是 等效 
的 classCount voteLabel = classCount . get voteLabel 0 + 
1 print classCount classCount maxCount = 0 # 返回 占有率 
最大 的 sortedClassCount = sorted classCount . iteritems key = 
operator . itemgetter 1 reverse = True return sortedClassCount 0 
0 最后 进行 测试 dataSet labels = createDataSet testX = 
numpy . array 0 0 k = 3 outputLabel = 
kNNClassify testX dataSet labels k print Your input is testX 
and classified to class outputLabel 可以 发现 输出 numSamples 4 
diff 1 . 1.1 1 . 1 . 5 . 
2 . 5 . 0.1 squaredDiff 1.00000000 e + 00 
1.21000000 e + 00 1.00000000 e + 00 1.00000000 e 
+ 00 2.50000000 e + 01 4.00000000 e + 00 
2.50000000 e + 01 1.00000000 e 02 squaredDist 2.21 2 
. 29 . 25.01 distance 1.48660687 1.41421356 5.38516481 5.0009999 s 
o r t e d D i s t I 
n d i c e s 1 0 3 2 
voteLabel A voteLabel A voteLabel B classCount { A 2 
B 1 } Your input is 0 0 and classified 
to class A 这里 我 之前 一直 有个/nr 疑问 关于 
K 的 取值 结果 也许 跟 K 的 取值 产生 
变化 只要在 K 的 取值 范围内 们 所有 特征点 距离 
远近 也就 没有 关系 了 所以 才叫 K 近邻 分类 
算法 