在 机器 学习 的 实践 中 我们 通常 会 遇到 
实际 数据 中 正负 样本 比例 不 平衡 的 情况 
也叫 数据 倾斜 对于 数据 倾斜 的 情况 如果 选取 
的 算法 不 合适 或者 评价 指标 不 合适 那么 
对于 实际应用 线上 时 效果 往往 会 不尽人意 所以 如何 
解决 数据 不 平衡 问题 是 实际 生产 中 非常 
常见 且 重要 的 问题 什么 是 类别 不 平衡 
问题 我们 拿到 一份 数据 时 如果 是 二分 类 
问题 通常会 判断 一下 正负 样本 的 比例 在 机器 
学习 中 通常 会 遇到 正负 样本 极不 均衡 的 
情况 如 垃圾 邮件 的 分类 等 在 目标 检测 
SSD 中 也 经常 遇到 数据 不 平衡 的 情况 
检测器 需要 在 每张 图像 中 评价 一万个 到 十万 
个 候选 位置 然而 其中 只有 很少 的 点 真的 
含有 目标 物体 这就 导致 了 训练 效率 低下 和 
简单 的 负面 样本 引发 整个 模型 表现 下降 的 
问题 如何 解决 不 平衡 问题 1 . 从 数据 
角度 主动 获取 获取 更多 的 少量 样本数据 针对 少量 
样本数据 可以 尽 可能 去 扩大 这些 少量 样本 的 
数据 集 或者 尽可 能去 增加 他们 特有 的 特征 
来 丰富 数据 的 多样性 譬如 如果 是 一个 情感 
分析 项目 在 分析 数据 比例 时 发现 负 样本 
消极 情感 的 样本 数量 较少 那么 我们 可以 尽 
可能 在 网站 中 搜集 更多 的 负 样本 数量 
算法 采样 上 采样 下 采样 生成 合成 数据 ADASYN 
采 样方法 ADASYN 为 样本 较少 的 类 生成 合成 
数据 其 生成 的 数据 与 更容易 学习 的 样本 
相比 更难 学习 基本 思想 是 根据 学习 难度 的 
不同 对 不同 的 少数类 的 样本 使用 加权 分布 
其中 更难 学习 的 少数类 的 样本 比 那些 更容易 
学习 的 少数类 的 样本 要 产生 更多 的 合成 
数据 因此 ADASYN 方法 通过 以下 两种 方式 改善 了 
数据 分布 的 学习 1 减少 由于 类别 不平衡 带来 
的 偏差 2 自适应 地 将 分类 决策 边界 转移 
到 困难 的 例子 SMOTE 采 样方法 从 少数类 创建 
新的 合成 点 以 增加 其 基数 但是 SMOTE 算法 
也 有 一定 的 局限性 具体 有 两项 一是 在 
近邻 选择 时 存在 一定 的 盲目性 在 算法 执行 
过程 中 需要 确定 Ｋ 值 即 选择 几个 近邻 
样本 这个 需要 根据 具体 的 实验 数据 和 实验 
人 自己 解决 二 是 该 算法 无法 克服 非平衡 
数据集 的 数据 分布 问题 容易 产生 分布 边缘化 的 
问题 由于 负 类 样本 的 分布 决定了 其 可选择 
的 近邻 如果 一个 负 类 样本 处在 负 类 
样本 的 边缘 则 由此 负 类 样本 和 近邻 
样本 产生 的 样本 也会 处在 边缘 从而 无法 确定 
正负 类 的 分类 边界 下图 是 以前 做 的 
一个 项目 应用 个 各种 采 样方法 做 数据 增强 
的 情况 效果 不 明显 因为 原始数据 的 分布 重合 
太 明显 可视化 不 容易 显示 出 效果 数据 增强 
加 噪音 增强 模型 鲁棒性 对 不同 性质 的 数据 
也 可以 做 不同 的 augmentation 改变 权重 设定 惩罚 
因子 如 libsvm 等 算法 里 设置 的 正负 样本 
的 权重 项等/nr 惩罚 多 样本 类别 其实 还 可以 
加权 少 样本 类别 注意 在 选择 采样法 事 需要 
注意 一个 问题 如果 你 的 实际 数据 是 数据 
不 平衡 的 在 训练 模型 时 发现 效果 不好 
于是 采取 了 采样法 平衡 的 数据 的 比例 再来 
进行 训练 然后 去 测试 数据 上 预测 这个 时候 
算法 的 效果 是否 会 有 偏差 呢 此时 你 
的 训练 样本 的 分布 与 测试 样本 的 分布 
已经 发生 了 改变 这样 做 反而 会 产生 不好 
的 效果 在 实际 情况 中 我们 尽 可能 的 
需要 保持 训练 和 测试 的 样本 的 概率分布 是 
一致 的 如果 测试 样本 的 分布 是 不 平衡 
的 那么 训练样本 尽可能 与 测试 样本 的 分布 保持一致 
哪怕/c 拿/v 到手/d 的/uj 是/v 已经/d 清洗/v 和/c 做过/i 预处理/vn 
后的/nr 平衡/a 的/uj 数据/n 具体 原因 感兴趣 的 可以 仔细 
思考 一下 2 . 从 评价 指标 角度 谨慎 选择 
AUC 作为 评价 指标 对于 数据 极端 不 平衡 时 
可以 观察 观察 不同 算法 在 同 一份 数据 下 
的 训练 结果 的 precision 和 recall 这样 做 有 
两个 好处 一 是 可以 了解 不同 算法 对于 数据 
的 敏感 程度 二 是 可以 明确 采取 哪种 评价 
指标 更 合适 针对 机器学习 中 的 数据 不 平衡 
问题 建议 更多 PR Precision Recall 曲线 而非 ROC 曲线 
具体 原因 画图 即可 得知 如果 采用 ROC 曲线 来 
作为 评价 指标 很容易 因为 AUC 值 高而/nr 忽略 实际 
对 少 两 样本 的 效果 其实 并不 理想 的 
情况 不要 只看 Accuracy Accuracy 可以 说 是 最 模糊 
的 一个 指标 了 因为 这个 指标 高 可能 压根 
就 不能 代表 业务 的 效果 好 在 实际 生产 
中 我们 可能 更 关注 precision / recall / mAP 
等 具体 的 指标 具体 侧重 那个 指标 得 结合 
实际 情况 看 3 . 从 算法 角度 选择 对 
数据 倾斜 相对 不 敏感 的 算法 如 树 模型 
等 集成 学习 Ensemble 集成 算法 首先 从 多数 类 
中 独立 随机 抽取 出 若干 子集 将 每个 子集 
与 少数类 数据 联合起来 训练 生成 多个 基 分类器 再 
加权 组成 新的 分类器 如 加法 模型 Adaboost 随机 森林 
等 将 任务 转换成 异常 检测 问题 譬如 有 这样 
一个 项目 需要 从 高压线 的 航拍 图片 中 将 
松动 的 螺丝 / 零件 判断 为 待 检测 站点 
即 负 样本 其他 作为 正 样本 这样 来看 数据 
倾斜 是 非常 严重 的 而且 在 图像 质量 一般 
的 情况 下 小物体 检测 的 难度 较大 所以 不如 
将其 转换 为 无 监督 的 异常 检测 算法 不用 
过多 的 去 考虑 将 数据 转换 为 平衡 问题 
来 解决 目标 检测 中的 不 平衡 问题 的 进展 
1 . GHM _ Detection 论文 https / / arvix 
. org / pdf / 1811.05181 . pdfgithub https / 
/ github . com / libuyu / GHM _ Detection 
本文 是 香港 中文 大学 发表 于 AAAI 2019 的 
工作 文章 从 梯度 的 角度 解决 样本 中 常见 
的 正负 样本 不 均衡 的 问题 从 梯度 的 
角度 给 计算 loss 的 样本 加权 相比 与 OHEM 
的 硬 截断 这种 思路 和 Focal Loss 一样 属于 
软 截断 文章 设计 的 思路 不仅 可以 用于 分类 
loss 改进 对 回归 loss 也 很容易 进行 嵌入 不 
需要 考虑 Focal Loss 的 超 参 设计 问题 同时 
文章 提出 的 方法 效果 比 Focal Loss 更好 创新 
点 相当于 FL 的 下 一步 方案 给出 了 解决 
class imbalance 的 另一 种 思路 开了 一条路 估计 下 
一步 会 有 很多 这 方面 的 paper 出现 2 
. Focal Loss for Dense Object Detection 论文 Focal Loss 
https / / arxiv . org / abs / 1708 
. 02002RetinaNet https / / github . com / unsky 
/ RetinaNetgithub https / / github . com / unsky 
/ focal loss 本文 通过 重塑 标准 交叉 熵 损失 
来 解决 这 一类 不 平衡 问题 他们 的 想法 
是 降低 简单 的 负面 样本 所占 的 权重 所以 
他们 提出 的 焦点 损失 Focal Loss 方法 将 训练 
集中 在 一系列 难点 上 并且 防止 了 大量 的 
简单 负面 例子 在 训练 过程 中 阻碍 探测器 学习 
如上图 参数 γ 的 值 选择 得 越大 模型 就 
会对 已经 得到 了 很好 的 分类 的 样本 忽略 
得 越多 越/d 专注/v 于难的/nr 样本/n 的/uj 学习/v 这样 的 
机制 就 让 他们 的 检测器 在 密集 对象 检测 
这样 的 真实 正面 样本 比例 很低 的 情况 下 
取得 了 很高 的 准确率 对于 应对 样本 不 平衡 
问题 的 关键 方法 焦距 损失 作者 们 在 论文 
中 还 提出 了 两种 不同 的 表现 形式 都 
起到 了 很好 的 效果 . 3 . 在线 困难 
样例 挖掘 online hard example mining OHEM 目标 检测 的 
另一个 问题 是 类别 不平衡 图像 中 大部分 的 区域 
是 不 包含 目标 的 而 只有 小 部分 区域 
包含 目标 此外 不同 目标 的 检测 难度 也 有 
很大 差异 绝大部分 的 目标 很 容易 被 检测 到 
而有 一小部分 目标 却 十分 困难 OHEM 和 Boosting 的 
思路 类似 其 根据 损失 值 将 所有 候选 区域 
进行 排序 并 选择 损失 值 最高 的 一部分 候选 
区域 进行 优化 使 网络 更 关注 于 图像 中 
更 困难 的 目标 此外 为了 避免 选 到 相互 
重叠 很大 的 候选 区域 OHEM 对 候选 区域 根据 
损失 值 进行 NMS 总之 针对 数据 不 平衡 问题 
有 多重 解决 方式 但是 不能 为了 解决 这个 问题 
就 去 改变 数据 的 真实 分布 来 得到 更好 
的 结果 可以 从 算法 loss function 的 设计 等 
等 多种 角度 来 选择 解决 数据 不 平衡 的 
方法 