一 概念 XGBoost 全 名叫 eXtreme Gradient Boosting 极端 梯度 
提升 经常 被用 在 一些 比赛 中 其 效果 显著 
它 是 大 规模 并行 boosted tree 的 工具 它 
是 目前 最快 最好 的 开源 boosted tree 工具包 XGBoost 
所 应用 的 算法 就是 GBDT gradient boosting decision tree 
的 改进 既 可以 用于 分类 也 可以 用于 回归 
问题 中 1 回归 树 与 决策树 事实上 分类 与 
回归 是 一个 型号 的 东西 只不过 分类 的 结果 
是 离散 值 回归 是 连续 的 本质 是 一样 
的 都是 特征 feature 到 结果 / 标签 label 之间 
的 映射 说说 决策树 和 回归 树 在上面 决策树 的 
讲解 中 相信 决策树 分类 已经 很好 理解 了 分类 
树 的 样本 输出 即 响应值 是 类 的 形式 
如 判断 蘑菇 是 有毒 还是 无毒 周末 去 看电影 
还是 不去 而 回归 树 的 样本 输出 是 数值 
的 形式 比如 给 某人 发放 房屋 贷款 的 数额 
就是 具体 的 数值 可以 是 0 到 120 万元 
之间 的 任意 值 那么 这时候 你 就 没法 用 
上述 的 信息 增益 信息 增益 率 基尼系数 来 判定 
树 的 节点 分裂 了 你 就会 采用 新的 方式 
预测误差 常用 的 有 均方 误差 对数 误 差等 而且 
节点 不再 是 类别 是 数值 预测值 那么 怎么 确定 
呢 有的是 节点 内 样本均值 有的是 最优化 算 出来 的 
比如 Xgboost 2 boosting 集成 学习 boosting 集成 学习 由 
多个 相 关联 的 决策树 联合 决策 什么 叫 相关联 
举个 例子 有 一个 样本 数据 标签 是 2 4 
5 4 第 一棵 决策树 用 这个 样本 训 练得 
预测 为 3.3 那么 第 二棵 决策树 训练 时的/nr 输入 
这个 样 本就 变成 了 2 4 5 0.7 也 
就是说 下 一棵 决策树 输入 样本 会 与 前面 决策树 
的 训练 和 预测 相关 与之 对比 的 是 random 
foreast 随机 森林 算法 各个 决策树 是 独立 的 每个 
决策树 在 样本 堆里 随机 选 一批 样本 随机 选 
一批 特征 进行 独立 训练 各个 决策树 之间 没有 啥 
毛线 关系 所以 首先 Xgboost 首先 是 一个 boosting 的 
集成 学习 这样 应该 很 通俗 了 3 这个 时候 
大家 就 能 感觉 到 一个 回归 树 形成 的 
关键 点 1 分 裂点 依据 什么 来 划分 如 
前面 说 的 均方 误差 最小 loss 2 分类 后的/nr 
节点 预测值 是 多少 如 前面 说 有 一种 是 
将 叶子 节 点下 各 样本 实际 值得 均值 作为 
叶子 节点 预测误差 或者 计算 所得 二 集成 思想 在 
学习 XGBoost 之前 我们 得 需要 先 明白 集成 思想 
集成 学习 方法 是 指 将 多个 学习 模型 组合 
以 获得 更好 的 效果 使 组合 后的/nr 模型 具有 
更强 的 泛化 能力 另外 XGBoost 是以 分类 回归 树 
CART 树 进行 组合 故在 此 之前 我们 先 看下 
CART 树 CART 树 具体 原理 请 自行 复习 或者 
可以 留言 如下 通过 输入 用户 年龄 性别 进行 判断 
用户 是否 喜欢 玩 游戏 的 得分 值 由此 得到 
一颗 CART 树 模型 我们 知道 对于 单个 的 决策树 
模型 容易 出现 过拟合 并且 不能 在 实际 中 有效 
应用 所以 出现 了 集成 学习 方法 如 下图 通过 
两棵树 组合 进行 玩游戏 得 分值 预测 其中 tree1 中 
对 小 男生 的 预测 分值 为 2 tree2 对 
小 男生 的 预测 分值 为 0.9 则 该 小 
男生 的 最后 得 分值 为 2.9 将 上面 集成 
学习 方法 推广 到 一般 情况 可知 其 预测模型 为 
其中 为 树 的 总 个数 表示 第 颗 树 
表示 样本 的 预测 结果 损失 函数 为 三 分析 
XGboost 思路 首先 明确 下 我们 的 目标 希望 建立 
K 个 回归 树 使得 树群 的 预测 值 尽量 
接近 真实 值 准确率 而且有 尽 量大 的 泛化 能力 
更为 本质 的 东西 从 数学 角度 看这 是 一个 
泛 函 最优化 多目标 看下 目标函数 其中 i 表示 第 
i 个 样本 表示 第 i 个 样本 的 预测误差 
误差 越小 越好 不然 你 算得 上 预测 么 后面 
表示 树 的 复杂度 的 函数 越小 复杂度 越低 泛化 
能力 越强 这 意味着 啥 不用 我 多 说 表达式 
为 T 表示 叶子 节点 的 个数 w 表示 节点 
的 数值 这是 回归 树 的 东西 分类 树 对应 
的 是 类别 直观 上看 目标 要求 预测误差 尽 量小 
叶子 节点 尽量少 节点 数值 尽量 不 极端 这个 怎么 
看 如果 某 个 样本 label 数值 为 4 那么 
第一 个 回归 树 预测 3 第二 个 预测 为 
1 另外 一组 回归 树 一个 预测 2 一个 预测 
2 那么 倾向 后 一种 为什么 呢 前一种 情况 第 
一棵树 学 的 太多 太 接近 4 也 就 意味着 
有 较大 的 过拟合 的 风险 ok 听 起来 很 
美好 可 是 怎么 实现 呢 上面 这个 目标函数 跟 
实际 的 参数 怎么 联系起来 记得 我们 说过 回归 树 
的 参数 1 选取 哪个 feature 分裂 节点 呢 2 
节点 的 预测 值 总 不能 靠 取 平均值 这么 
粗暴 不讲道理 的 方式 吧 好歹 高级 一点 上述 形而上 
的 公式 并 没有 直接 解决 这 两个 那么 是 
如何 间接 解决 的 呢 先 说 答案 贪心 策略 
+ 最优化 二次 最优化 通俗 解释 贪心 策略 就是 决策 
时刻 按照 当前 目标 最优化 决定 说白 了 就是 眼前 
利益 最大化 决定 目光短浅 策略 他 的 优缺点 细节 大家 
自己 去 了解 经典 背包问题 等等 这里 是 怎么 用 
贪心 策略 的 呢 刚 开始 你 有 一群 样本 
放在 第一 个 节点 这时候 T = 1T = 1 
ww 多少 呢 不 知道 是 求 出来 的 这时候 
所有 样本 的 预测 值 都是 ww 这个地方 自己 好好 
理解 决策树 的 节点 表示 类别 回归 树 的 节点 
表示 预测值 带入 样本 的 label 数值 此时 loss function 
变为 如果 这里 的 l w − yi 误差 表示 
用 的 是 平方 误差 那么 上述 函数 就是 一个 
关于 w 的 二次函数 求 最小值 取 最小值 的 点 
就是 这个 节点 的 预测 值 最小 的 函数值 为 
最小 损失 函数 这里 处理 的 就是 二次函数 最优化 要是 
损失 函数 不是 二次函数 咋办 哦 泰勒 展开式 会否 不是 
二次 的 想办法 近 似为 二次 接着 来 接下来 要 
选个 feature 分裂 成 两个 节点 变成 一棵 弱小 的 
树苗 那么 需要 1 确定 分裂 用 的 feature how 
最 简单 的 是 粗暴 的 枚举 选择 loss function 
效果 最好 的 那个 关于 粗暴 枚举 Xgboost 的 改良 
并行 方式 咱们 后面 看 2 如何 确立 节点 的 
ww 以及 最小 的 loss function 大声 告诉 我 怎么 
做 对 二次函数 的 求 最值 细节 的 会 注意 
到 计算 二次 最值 是不是 有 固定 套路 导数 = 
0 的 点 ok 那么 节奏 是 选择 一个 feature 
分裂 计算 loss function 最小值 然后再 选 一个 feature 分裂 
又 得到 一个 loss function 最小值 你 枚举 完 找 
一个 效果 最好 的 把 树 给 分裂 就 得到 
了 小树苗 在 分裂 的 时候 你 可以 注意到 每次 
节点 分裂 loss function 被 影响 的 只有 这个 节点 
的 样本 因而 每次 分裂 计算 分裂 的 增益 loss 
function 的 降低 量 只 需要 关注 打算 分裂 的 
那个 节点 的 样本 接下来 继续 分裂 按照 上述 的 
方式 形成 一棵树 再 形成 一棵树 每次 在 上 一次 
的 预测 基础 上 取 最优 进一步 分裂 / 建树 
是不是 贪心 策略 凡是 这种 循环 迭代 的 方式 必定 
有 停止 条件 什么 时候 停止 呢 1 当 引入 
的 分裂 带来 的 增益 小于 一个 阀值 的 时候 
我们 可以 剪 掉 这个 分裂 所以 并 不是 每一次 
分裂 loss function 整体 都会 增加 的 有点 预 剪枝 
的 意思 阈值 参数 为   γ 正则 项里/nr 叶子 
节 点数 T 的 系数 2 当 树 达到 最 
大 深度 时则/nr 停止 建立 决策树 设置 一个 超 参数 
max _ depth 这个 好 理解 吧 树 太深 很容易 
出现 的 情况 学习 局部 样本 过拟合 3 当 样 
本权 重和 小于 设定 阈值 时则/nr 停止 建树 这个 解释 
一下 涉及 到 一个 超 参数 最小 的 样本 权 
重和 min _ child _ weight 和 GBM 的 min 
_ child _ leaf 参数 类似 但 不 完全 一样 
大意 就是 一个 叶子 节点 样本 太少 了 也 终止 
同样 是 过拟合 4 貌似 看到过 有树的/nr 最大 数量 的 
这个 不 确定 那 节点 分裂 的 时候 是 按照 
哪个 顺序 来 的 比如/v 第一/m 次/q 分裂/v 后有/nr 两个/m 
叶子/nr 节点/n 先 裂 哪一个 答 同一 层级 的 多 
机 并行 确立 如何 分裂 或者 不 分裂 成为 叶子 
节点 四 原理 推导 上面 一 部分 我们 知道 了 
集成 学习 方法 的 预测模型 因为 XGBoost 也是 集成 学习 
方法 的 一种 对于 XGBoost 的 预测模型 同样 可以 表示 
为 其中 为 树 的 总 个数 表示 第 颗 
树 表示 样本 的 预测 结果 其中 损失 函数 也 
同样 表示 为 其中 为 样本 的 训练 误差 表示 
第 棵树 的 正则 项 看到 了 这里 我们 可能 
会 想到 现在 知道 了 模型 预测 函数 和 损失 
函数 那 我们 是 不是 直接 就能 求出 其 预测模型 
了 呢 答案 肯定 不是 我们 首先 需要 明确 知道 
优化 和 求解 的 参数 是 什么 呢 由 上面 
的 预测 模型 中 我们 可以 看到 对于 每 棵树 
的 预测 值 是 如何 计算 的 想到 这里 你 
就 已经 知道 了 需要 做 的 事 了 我 
需要 求解 和 优化 的 就是 每个 叶子 节点 的 
得分 值 也 就是 的 值 另外 我们 知道 XGBoost 
是以 CART 树 中的 回归 树 作为 基 分类器 在 
给定 训练 数据 后 其 单个 树 的 结构 叶子 
节点 个数 树 深度 等等 基本 可以 确定 了 但 
XGBoost 并 不是 简单 重复 的 将 几个 CART 树 
进行 组合 它 是 一种 加法 模型 将 模型 上次 
预测 由 t 1 棵树 组合而成 的 模型 产生 的 
误差 作为 参考 进行 下 一棵树 第 t 棵树 的 
建立 以此 每 加入 一棵树 将其 损失 函数 不断 降低 
如 下图 就为 加法 模型 案例 它 将 模型 预测值 
与 实际 值 残差 作为 下 一颗 树 的 输入 
数据 对于 加法 策略 可以 表示 如下 初始化 模型 中 
没有 树 时 其 预测 结果 为 0 往 模型 
中 加入 第 一棵树 往 模型 中 加入 第二 棵树 
往 模型 中 加入 第 t 棵树 其中 表示 第 
棵树 表示 组合 棵树 模型 对 样本 的 预测 结果 
我们 知道 每次 往 模型 中 加入 一棵树 其 损失 
函数 便会 发生变化 另外 在 加入 第 t 棵树 时 
则 前面 第 t 1 棵树 已经 训练 完成 此时 
前面 t 1/m 棵树/i 的/uj 正则/n 项和/nr 训练/vn 误差/n 都成/i 
已知/v 常数项/i 对于 每 棵树 的 正则 项 部分 我们 
将 在 后面 再 细说 如果 损失 函数 采用 均方 
误差 时 其 目标 损失 函数 变为 另外 对于 目标 
损失 函数 中的 正则 项 复杂度 部分 我们 从 单一 
的 树 来 考虑 对于 其中 每 一棵 回归 树 
其 模型 可以 写成 其中 为 叶子 节点 的 得分 
值 表示 样本 对应 的 叶子 节点 为 该 树 
的 叶子 节点 个数 因此 在 这里 我们 将 该 
树 的 复杂度 写成 复杂度 计算 例子 如下 此时 对于 
XGBoost 的 目标 函数 我们 可以 写 为 现在 我们 
只 需要 找到 f t 来 优化 上式 目标 在 
推导 之前 我们 先 介绍 下 泰勒 展开式 这里 我们 
用 泰勒 展开式 来 近似 原来 的 目标 函数 将 
看作 则 原 目标函数 可以 写成 令 同时 对于 第 
t 棵树 时 为 常数 同时 去除 所有 常数项 故 
目标 损失 函数 可以 写成 由 上面 介绍 书 的 
复杂度 时 我们 知道 同时 我们 将 目标函数 全部 转换成 
在 第 t 棵 树叶子 节点 的 形式 因为 目前 
对于 可以 看做 是 每个 样本 在 第 t 棵树 
的 叶子 节 点得 分值 相关 函数 的 结果 之和 
所以 我们 也 能从 第 t 棵树 的 叶子 节点 
上 来 表示 上式 中 前 两行 i = 1 
~ n 求和 为 在 样本 中 遍历 后 两行 
j = 1 ~ T 求和 为 在 叶子 节点 
上 遍历 其中 为 第 t 棵树 中 总 叶子 
节点 的 个数 表示 在 第 个 叶子 节点 上 
的 样本 为 第 个 叶子 节点 的 得分 值 
在 这里 令 则 对 求 偏 导 并 使其 
导函数 等于 0 则有 求 解得 其 目标函数 可以为 根据 
目标函数 如何 分裂 样本数据 呢 五 总结 1 Xgboost 的 
一些 重点 w 是 最优化 求 出来 的 不是 啥 
平均值 或 规则 指定 的 这个 算 是 一个 思路 
上 的 新颖 吧 正则化 防止 过拟合 的 技术 上述 
看到 了 直接 loss function 里面 就有 支持 自定义 loss 
function 只要能 泰勒 展开 能求一/nr 阶/n 导/v 和/c 二阶/n 导/v 
就行 支持 并行 化 这个 地方 有 必要 说明 下 
因为 这是 xgboost 的 闪光点 直接 的 效果 是 训练 
速度快 boosting 技术 中下 一棵树 依赖 上述 树 的 训练 
和 预测 所以 树 与 树 之间 应该 是 只能 
串行 那么 大家 想想 哪里 可以 并行   没错 在 
选择 最佳 分 裂点 进行 枚举 的 时候 并行 据说 
恰好 这个 也是 树 形成 最 耗时 的 阶段 Attention 
同 层级 节点 可 并行 具体 的 对于 某个 节点 
节点 内 选择 最佳 分 裂点 候选 分 裂点 计算 
增益 用 多线程 并行 – 较少 的 离散 值 作为 
分割 点 倒是 很 简单 比如 是否 是 单身 来 
分裂 节点 计算 增益 是 很 easy 但是 月收入 这种 
feature 取值 很多 从 5k ~ 50k 都有 总 不 
可能 每个 分割 点 都来 试一下 计算 分裂 增益 吧 
比如 月收入 feature 有 1000个 取值 难道 你 把这 1000个 
用作 分割 候选 缺点 1 计算 量 缺点 2 出现 
叶子 节点 样本 过少 过拟合 我们 常用 的 习惯 就是 
划分 区间 那么 问题 来了 这个 区间 分割 点 如何 
确定 难道 平均 分割 作者 是 这么 做 的 方法 
名字 Weighted Quantile Sketch 大家 还 记得 每个 样本 在 
节点 将要 分裂 的 节点 处 的 loss function 一 
阶 导数 gi 和 二阶 导数 hi 衡量 预测 值 
变化 带来 的 loss function 变化 举例来说 将 样本 月收入 
进行 升 序 排列 5k 5.2 k 5.3 k 52k 
分割线 为 收入 1 收入 2 收入 j 满足 每个 
间隔 的 样本 的 hi 之和 / 总 样本 的 
hi 之和 为 某个 百分比 ϵ 我 这个 是 近似 
的 说法 那么 可以 一共 分成 大约 1 / ϵ 
个 分 裂点 XGBoost 还 特别 设计 了 针对 稀疏 
数据 的 算法 假设 样本 的 第 i 个 特征 
缺 失时 无法 利用 该 特征 对 样本 进行 划分 
这里 的 做法 是 将该 样本 默认 地 分到 指定 
的 子 节点 至于 具体 地 分到 哪个 节点 还需要 
某 算法 来 计算 算法 的 主要 思想 是 分别/d 
假设/vn 特征/n 缺失/v 的/uj 样本/n 属于/v 右/f 子树/nr 和左/nr 子树/nr 
而且 只在 不 缺失 的 样本 上 迭代 分别/d 计算/v 
缺失/v 样/n 本属于/i 右/f 子树/nr 和左/nr 子树/nr 的/uj 增益/n 选择 
增益 最大 的 方向 为 缺失 数据 的 默认 方向 
咋 一看 如果 缺失 情况 为 3个 样本 那么 划分 
的 组合 方式 岂不是 有 8种 指数级 可能性 啊 仔细 
一看 应该 是 在 不 缺失 样本 情况 下 分裂 
后 把 第一 个 缺失 样本 放 左边 计 算下 
loss function 和放/nr 右边 进行 比较 同样 对付 第二个 第三个 
缺失 样本 这么 看来 又 是 可以 并行 的 答 
论文 中 枚举 指 的 不是 枚举 每个 缺失 样本 
在 左边 还是 在 右边 而是 枚举 缺失 样本 整体 
在 左边 还是 在 右边 两种 情况 分裂 点 还是 
只 评估 特征 不 缺失 的 样本 可 实现 后 
剪枝 交叉 验证 方便 选择 最好 的 参数 early stop 
比如 你 发现 30 棵树 预测 已经 很好 了 不用 
进一步 学习 残差 了 那么 停止 建树 行 采样 列 
采样 随机 森林 的 套路 防止 过拟合 Shrinkage 你 可以 
是 几个 回归 树 的 叶子 节点 之和 为 预测 
值 也 可以 是 加权 比如 第 一棵树 预测值 为 
3.3 label 为 4.0 第二 棵树 才学 0.7 . 再 
后面 的 树 还 学 个 鬼 所以 给 他 
打个 折扣 比如 3 折 那么 第二 棵树 训练 的 
残差 为 4.0 3.3 * 0.3 = 3.01 这 就 
可以 发挥 了 啦 以此类推 作用 是 啥 防止 过拟合 
如果 对于 伪 残差 学习 那 更像 梯度 下降 里面 
的 学习率 xgboost 还 支持 设置 样本 权重 这个 权重 
体现在 梯度 g 和 二阶 梯度 h 上 是不是 有点 
adaboost 的 意思 重点 关注 某些 样本 2 与 GDBT 
深度 学习 对 比下 Xgboost 第一 感觉 就是 防止 过拟合 
+ 各种 支持 分布式 / 并行 所以 一般 传言 这种 
大杀器 效果 好 集成 学习 的 高配/nr + 训练 效率 
高 分布式 与 深度 学习 相比 对 样本 量 和 
特征 数据类型 要求 没 那么 苛刻 适用 范围 广 说 
下 GBDT 有 两种 描述 版本 把 GBDT 说 成 
一个 迭代 残差 树 认为 每 一棵 迭代 树 都在 
学习 前 N 1 棵树 的 残差 把 GBDT 说 
成 一个 梯度 迭代 树 使用 梯度 迭代 下 降法 
求解 认为 每 一棵 迭代 树 都在 学习 前 N 
1 棵树 的 梯度 下降 值 有 说法 说 前者 
是 后者 在 loss function 为 平方 误 差下 的 
特殊 情况 这里 说 下 我 的 理解 仍然 举个 
例子 第 一棵树 形成 之后 有 预测 值 y ̂ 
  iy ^ i 真 实值 label 为 yiyi 前者 
版本 表示 下 一棵 回归 树 根据 样本 xi yi 
− y ̂   i xi yi − y ^ 
i 进行 学习 后者 的 意思 是 计算 loss function 
在 第一 棵树 预测值 附近 的 梯度 负值 作为 新的 
label 也 就是 对应 xgboost 中的 − gi − giXgboost 
和 深度 学习 的 关系 陈天奇 在 Quora 上 的 
解答 如下 不同 的 机器学习 模型 适用 于 不同 类型 
的 任务 深度 神经网络 通过 对 时空 位置 建模 能够 
很好 地 捕获 图像 语音 文本 等 高维 数据 而 
基于 树 模型 的 XGBoost 则能 很好 地 处理 表格 
数据 同时 还 拥有 一些 深度 神经 网络 所 没有 
的 特性 如 模型 的 可 解释性 输入 数据 的 
不变性 更 易于 调 参 等 这 两类 模型 都 
很重要 并 广泛 用于 数据 科学 竞赛 和 工业界 举例来说 
几乎 所有 采用 机器学习 技术 的 公司 都在/nr 使用 tree 
boosting 同时 XGBoost 已经 给 业界 带来 了 很大 的 
影响 六 XGboost 参数 解析 XGBoost 参数 在 运行 XGBoost 
程序 之前 必须 设置 三种 类型 的 参数 通用 类型 
参数 general parameters booster 参数 和 学习 任务 参数 task 
parameters 一般 类型 参数 general parameters – 参数 决定 在 
提升 的 过程 中 用 哪种 booster 常见/a 的/uj booster/w 
有/v 树/v 模型/n 和/c 线性/n 模型/n Booster 参数 该 参数 
的 设置 依赖 于 我们 选择 哪 一种 booster 模型 
学习 任务 参数 task parameters 参数 的 设置 决定 着 
哪 一种 学习 场景 例如 回归 任务 会 使用 不同 
的 参数 来 控制 着 排序 任务 命令行 参数 一般 
和 xgboost 的 CL 版本 相关 Booster 参数 1 . 
eta 默认 是 0.3   和 GBM 中的 learning rate 
参数 类似 通过 减少 每一步 的 权重 可以 提高 模型 
的 鲁棒性 典型值 0.01 0.22 . min _ child _ 
weight 默认 是 1   决定 最 小叶子 节 点样 
本权 重和 当 它 的 值 较大 时 可以 避免 
模型 学习 到 局部 的 特殊 样本 但 如果 这个 
值 过高 会 导致 欠 拟合 这个 参数 需要用 cv 
来 调整 3 . max _ depth 默认 是 6 
  树 的 最大 深度 这个 值 也是 用来 避免 
过拟合 的 3 104 . max _ leaf _ nodes 
  树上 最大 的 节点 或 叶子 的 数量 可以 
代替 max _ depth 的 作用 应为 如果 生成 的 
是 二叉树 一个 深度 为 n 的 树 最多 生成 
2n 个 叶子 如果 定义 了 这个 参数 max _ 
depth 会被 忽略 5 . gamma 默认 是 0   
在 节点 分裂 时 只有在 分裂 后 损失 函数 的 
值 下降 了 才会 分裂 这个 节点 Gamma 指定 了 
节点 分裂 所需 的 最小 损失 函数 下降 值 这个 
参数值 越大 算法 越 保守 6 . max _ delta 
_ step 默认 是 0   这 参数 限制 每颗 
树 权重 改变 的 最大 步长 如果 是 0 意味着 
没有 约束 如果 是 正值 那么 这个 算 法会 更 
保守 通常 不 需要 设置 7 . subsample 默认 是 
1   这个 参数 控制 对于 每 棵树 随机 采样 
的 比例 减小 这个 参数 的 值 算法 会 更加 
保守 避免 过拟合 但是 这个 值 设置 的 过小 它 
可能 会 导致 欠 拟合 典型值 0.5 18 . colsample 
_ bytree 默认 是 1   用来 控制 每颗 树 
随机 采样 的 列数 的 占 比 每 一列 是 
一个 特征 0.5 19 . colsample _ bylevel 默认 是 
1   用来 控制 的 每 一级 的 每一次 分裂 
对 列数 的 采样 的 占 比 10 . lambda 
默认 是 1   权重 的 L2 正则化 项 11 
. alpha 默认 是 1   权重 的 L1 正则化 
项 12 . scale _ pos _ weight 默认 是 
1   各类 样本 十分 不 平衡 时 把 这个 
参数设置 为 一个 正数 可以 使 算法 更快 收敛 通用 
参数 1 ． booster 默认 是 gbtree 选择 每次 迭代 
的 模型 有 两种 选择 gbtree 基于 树 的 模型 
gbliner 线性 模型 2 ． silent 默认 是 0 当 
这个 参数值 为 1 的 时候 静默 模式 开启 不会 
输 出 任何 信息 一般 这个 参数 保持 默认 的 
0 这样 可以 帮 我们 更好 的 理解 模型 3 
． nthread 默认值 为 最大 可能 的 线程数 这个 参数 
用来 进行 多线程 控制 应当 输入 系统 的 核 数 
如果 你 希望 使用 cpu 全部 的 核 就 不要 
输入 这个 参数 算法 会 自动 检测 学习 目标 参数 
1 ． objective 默认 是 reg linear 这个 参数 定义 
需要 被 最小化 的 损失 函数 最 常用 的 值 
有 binary logistic 二 分类 的 逻辑 回归 返回 预测 
的 概率 非 类别 multi softmax 使用 softmax 的 多 
分类器 返回 预测 的 类别 在 这种 情况 下 你 
还要 多 设置 一个 参数 num _ class 类别 数目 
2 ． eval _ metric 默认值 取决于 objective 参数 的 
取之 对于 有效 数据 的 度量 方法 对于 回归 问题 
默认值 是 rmse 对于 分类 问题 默认 是 error 典型值 
有 rmse 均方根 误差 mae 平均 绝对误差 logloss 负 对数 
似 然 函数值 error 二 分类 错误率 merror 多 分类 
错误率 mlogloss 多 分类 损失 函数 auc 曲 线下 面积 
3 ． seed 默认 是 0 随机数 的 种子 设置 
它 可以 复现 随机 数据 的 结果 也 可以 用于 
调整 参数 文章 参考 1 https / / blog . 
csdn . net / huibeng7187 / article / details / 
775880012 https / / blog . csdn . net / 
github _ 38414650 / article / details / 76061893 