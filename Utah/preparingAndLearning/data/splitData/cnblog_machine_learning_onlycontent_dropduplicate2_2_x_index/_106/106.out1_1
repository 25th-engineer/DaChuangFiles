RESCALING attribute data to values to scale the range in 
0 1 or − 1 1 is useful for the 
optimization algorithms such as gradient descent that are used within 
machine learning algorithms that weight inputs e . g . 
regression and neural networks . Rescaling is also used for 
algorithms that use distance measurements for example K Nearest Neighbors 
KNN . Rescaling like this is sometimes called normalization . 
MinMaxScaler class in python skikit learn does this . NORMALIZING 
attribute data is used to rescale components of a feature 
vector to have the complete vector length of 1 . 
This is scaling by unit length . This usually means 
dividing each component of the feature vector by the Euclidiean 
length of the vector but can also be Manhattan or 
other distance measurements . This pre processing rescaling method is 
useful for sparse attribute features and algorithms using distance to 
learn such as KNN . Python scikit learn Normalizer class 
can be used for this . STANDARDIZING attribute data is 
also a preprocessing method but it assumes a Gaussian distribution 
of input features . It standardizes to a mean of 
0 and a standard deviation of 1 . This works 
better with linear regression logistic regression and linear discriminate analysis 
. Python StandardScaler class in scikit learn works for this 
. from https / / www . zhihu . com 
/ question / 28641663 / answer / 110165221 目录 1 
特征 工程 是 什么 2 数据 预处理 2.1 无量纲 化 
2 . 1.1 标准化 2 . 1.2 区间 缩放 法2./nr 
1.3 标准化 与 归一化 的 区别 2.2 对 定量 特征 
二 值 化 2.3 对 定性 特征 哑 编码 2.4 
缺失 值 计算 2.5 数据 变换 3 特征选择 3.1 Filter3 
. 1.1 方差 选择法 3 . 1.2 相关 系数法 3 
. 1.3 卡方检验 3 . 1.4 互信息 法 3.2 Wrapper3 
. 2.1 递归 特征 消 除法 3.3 Embedded3 . 3.1 
基于 惩罚 项的/nr 特征 选择法 3 . 3.2 基于 树 
模型 的 特征 选择法 4 降 维 4.1 主 成分 
分析法 PCA 4.2 线性 判别 分析法 LDA 5 总结 6 
参考资料 1 特征 工程 是 什么 有 这么 一 句话 
在 业界 广泛 流传 数据 和 特征 决定 了 机器 
学习 的 上限 而 模型 和 算法 只是 逼近 这个 
上限 而已 那 特征 工程 到底 是 什么 呢 顾名思义 
其 本质 是 一项 工程 活动 目的/n 是/v 最大限度/i 地/uv 
从/p 原始/v 数据/n 中/f 提取/v 特征/n 以供/i 算法/n 和/c 模型/n 
使用/v 通过 总结 和 归纳 人们认为 特征 工程 包括 以下 
方面 特征 处理 是 特征 工程 的 核心 部分 sklearn 
提供 了 较为 完整 的 特征 处理 方法 包括 数据 
预处理 特征选择 降 维 等 首次 接触到 sklearn 通常 会被 
其 丰富 且 方便 的 算法 模型库 吸引 但是 这里 
介绍 的 特征 处理 库 也 十分 强大 本文 中 
使用 sklearn 中的 IRIS 鸢尾花 数据集 来 对 特征 处理 
功能 进行 说明 IRIS 数据集 由 Fisher 在 1936年 整理 
包含 4个 特征 Sepal . Length 花萼 长度 Sepal . 
Width 花萼 宽度 Petal . Length 花瓣 长度 Petal . 
Width 花瓣 宽度 特征值 都为 正 浮点数 单位 为 厘米 
目标值 为 鸢尾花 的 分类 Iris Setosa 山 鸢尾 Iris 
Versicolour 杂色 鸢尾 Iris Virginica 维吉尼亚 鸢尾 导入 IRIS 数据集 
的 代码 如下 from sklearn . datasets import load _ 
iris # 导入 IRIS 数据集 iris = load _ iris 
# 特征 矩阵 iris . data # 目标 向量 iris 
. target2 数据 预处理 通过 特征提取 我们 能 得到 未经 
处理 的 特征 这时 的 特征 可能 有 以下 问题 
不属于 同一 量纲 即 特征 的 规格 不 一样 不能够 
放在 一起 比较 无量纲 化 可以 解决 这 一 问题 
信息冗余 对于 某些 定量 特征 其 包含 的 有效 信息 
为 区间 划分 例如 学习成绩 假若 只 关心 及格 或不 
及格 那么 需要 将 定量 的 考分 转换成 1 和 
0 表示 及格 和未/nr 及格 二 值 化 可以 解决 
这 一 问题 定性 特征 不能 直接 使用 某些 机器学习 
算法 和 模型 只能 接受 定量 特征 的 输入 那么 
需要 将 定性 特征 转换 为 定量 特征 最 简单 
的 方式 是 为 每一种 定性 值 指定 一个 定 
量值 但是 这种 方式 过于 灵活 增加 了 调 参 
的 工作 通常 使用 哑 编码 的 方式 将 定性 
特征 转换 为 定量 特征 假设有 N 种 定性 值 
则将 这 一个 特征 扩展 为 N 种 特征 当 
原始 特征值 为 第 i 种 定性 值 时 第 
i 个 扩展 特征 赋值 为 1 其他 扩展 特征 
赋值 为 0 哑 编码 的 方式 相比 直接 指定 
的 方式 不用 增加 调 参 的 工作 对于 线性 
模型 来说 使用 哑 编码 后的/nr 特征 可达到 非线性 的 
效果 存在 缺失 值 缺失 值 需要 补充 信息 利用率 
低 不同 的 机器学习 算法 和 模型 对 数据 中 
信息 的 利用 是 不同 的 之前 提到 在 线性 
模型 中 使用 对 定性 特征 哑 编码 可以 达到 
非线性 的 效果 类 似地 对 定量 变量 多项式 化 
或者 进行 其他 的 转换 都能/nr 达到/v 非线性/b 的/uj 效果/n 
我们 使用 sklearn 中的 preproccessing 库 来 进行 数据 预处理 
可以 覆盖 以上 问题 的 解决方案 2.1 无量纲 化 无量纲 
化 使 不同 规格 的 数据 转换 到 同一 规格 
常见/a 的/uj 无量纲/i 化/n 方法/n 有/v 标准化/vn 和/c 区间/n 缩放/v 
法/l 标准化 的 前提 是 特征值 服从 正态分布 标准化 后 
其 转换成 标准 正态分布 区间 缩放 法利 用了 边界值 信息 
将 特征 的 取值 区间 缩放 到 某个 特点 的 
范围 例如 0 1 等 2 . 1.1 标准化 标准化 
需要 计算 特征 的 均值 和 标准差 公式 表达 为 
使用 preproccessing 库 的 StandardScaler 类 对 数据 进行 标准化 
的 代码 如下 from sklearn . preprocessing import StandardScaler # 
标准化 返回值 为 标准化 后的/nr 数据 StandardScaler . fit _ 
transform iris . data 2 . 1.2 区间/n 缩放/v 法/l 
区间/n 缩放/v 法的/nr 思路/n 有/v 多种/m 常见 的 一种 为 
利用 两个 最 值 进行 缩放 公式 表达 为 使用 
preproccessing 库 的 MinMaxScaler 类 对 数据 进行 区间 缩放 
的 代码 如下 from sklearn . preprocessing import MinMaxScaler # 
区间 缩放 返回值 为 缩 放到 0 1 区间 的 
数据 MinMaxScaler . fit _ transform iris . data 2 
. 1.3 标准化 与 归一化 的 区别 简单 来说 标准化 
是 依照 特征 矩阵 的 列 处理 数据 其 通过 
求 z score 的 方法 将 样本 的 特征值 转换 
到 同一 量纲 下 归一化 是 依照 特征 矩阵 的 
行 处理 数据 其 目的 在于 样本 向量 在 点乘 
运算 或 其他 核 函数 计算 相似性 时 拥有 统一 
的 标准 也 就是说 都 转化 为 单位向量 规则 为 
l2 的 归一化 公式 如下 使用 preproccessing 库 的 Normalizer 
类 对 数据 进行 归一化 的 代码 如下 from sklearn 
. preprocessing import Normalizer # 归一化 返回值 为 归一化 后的/nr 
数据 Normalizer . fit _ transform iris . data 2.2 
对 定量 特征 二 值 化 定量 特征 二 值 
化 的 核心 在于 设定 一个 阈值 大于 阈值 的 
赋值 为 1 小于 等于 阈值 的 赋值 为 0 
公式 表达 如下 使用 preproccessing 库 的 Binarizer 类 对 
数据 进行 二 值 化 的 代码 如下 from sklearn 
. preprocessing import Binarizer # 二 值 化 阈值 设置 
为 3 返回值 为 二 值 化 后的/nr 数据 Binarizer 
threshold = 3 . fit _ transform iris . data 
2.3 对 定性 特征 哑 编码 由于 IRIS 数据集 的 
特征 皆为 定量 特征 故 使用 其 目标 值 进行 
哑 编码 实际上 是 不 需要 的 使用 preproccessing 库 
的 OneHotEncoder 类 对 数据 进行 哑 编码 的 代码 
如下 from sklearn . preprocessing import OneHotEncoder # 哑 编码 
对 IRIS 数据集 的 目标 值 返回值 为 哑 编码 
后的/nr 数据 OneHotEncoder . fit _ transform iris . target 
. reshape 1 1 2.4 缺失 值 计算 由于 IRIS 
数据集 没有 缺失 值 故 对 数据集 新增 一 个 
样本 4个 特征 均 赋值 为 NaN 表示 数据 缺失 
使用 preproccessing 库 的 Imputer 类 对 数据 进行 缺失 
值 计算 的 代码 如下 from numpy import vstack array 
nan from sklearn . preprocessing import Imputer # 缺失 值 
计算 返回值 为 计算 缺失 值 后的/nr 数据 # 参数 
missing _ value 为 缺失 值 的 表示 形式 默认 
为 NaN # 参数 strategy 为 缺失 值 填充 方式 
默认 为 mean 均值 Imputer . fit _ transform vstack 
array nan nan nan nan iris . data 2.5 数据 
变换 常见 的 数据 变 换有 基于 多项式 的 基于 
指数函数 的 基于 对数函数 的 4个 特征 度 为 2 
的 多项式 转换 公式 如下 使用 preproccessing 库 的 P 
o l y n o m i a l F 
e a t u r e s 类 对 数据 
进行 多项式 转换 的 代码 如下 from sklearn . preprocessing 
import P o l y n o m i a 
l F e a t u r e s # 
多项式 转换 # 参数 degree 为度 默认值 为 2 P 
o l y n o m i a l F 
e a t u r e s . fit _ 
transform iris . data 基 于单 变 元 函数 的 
数据 变换 可以 使用 一个 统一 的 方式 完成 使用 
preproccessing 库 的 F u n c t i o 
n T r a n s f o r m 
e r 对 数据 进行 对数函数 转换 的 代码 如下 
from numpy import log1p from sklearn . preprocessing import F 
u n c t i o n T r a 
n s f o r m e r # 自定义 
转换 函数 为 对数函数 的 数据 变换 #/i 第一/m 个/q 
参数/n 是/v 单变元/nr 函数/n F u n c t i 
o n T r a n s f o r 
m e r log1p . fit _ transform iris . 
data 3 特征选择 当 数据 预处理 完成后 我们/r 需要/v 选择/v 
有/v 意义/n 的/uj 特征/n 输入/v 机器/n 学习/v 的/uj 算法/n 和/c 
模型/n 进行/v 训练/vn 通常 来说 从 两个 方面 考虑 来 
选择 特征 特征 是否 发散 如果 一个 特征 不 发散 
例如 方差 接近于 0 也 就是说 样本 在 这个 特征 
上 基本上 没有 差异 这个 特征 对于 样本 的 区分 
并 没有 什么 用 特征 与 目标 的 相关性 这点 
比较 显见 与 目标 相关性 高的/nr 特征 应当 优选 选择 
除 方差 法外 本文 介绍 的 其他 方法 均 从 
相关性 考虑 根据 特征选择 的 形式 又 可以 将 特征选择 
方法 分为 3种 Filter 过滤法 按照 发散 性 或者 相关性 
对 各个 特征 进行 评分 设定 阈值 或者 待 选择 
阈值 的 个数 选择 特征 Wrapper 包装法 根据 目标函数 通常 
是 预测 效果 评分 每次 选择 若干 特征 或者 排除 
若干 特征 Embedded 嵌 入法 先 使用 某些 机器 学习 
的 算法 和 模型 进行 训练 得到 各 个 特征 
的 权值 系数 根据 系数 从大到/nr 小 选择 特征 类似于 
Filter 方法 但是 是 通过 训练 来 确定 特征 的 
优劣 我们 使用 sklearn 中的 feature _ selection 库 来 
进行 特征选择 3.1 Filter3 . 1.1 方差 选择法 使用 方差 
选择法 先要 计算 各 个 特征 的 方差 然后 根据 
阈值 选择 方差 大于 阈值 的 特征 使用 feature _ 
selection 库 的 V a r i a n c 
e T h r e s h o l d 
类 来 选择 特征 的 代码 如下 from sklearn . 
feature _ selection import V a r i a n 
c e T h r e s h o l 
d # 方差 选择法 返回值 为特征 选择 后的/nr 数据 # 
参数 threshold 为 方差 的 阈值 V a r i 
a n c e T h r e s h 
o l d threshold = 3 . fit _ transform 
iris . data 3 . 1.2 相关 系数法 使用 相关 
系数法 先要 计算 各 个 特征 对 目标值 的 相关 
系数 以及 相关 系数 的 P 值 用 feature _ 
selection 库 的 SelectKBest 类 结合 相关 系数 来 选择 
特征 的 代码 如下 from sklearn . feature _ selection 
import SelectKBest from scipy . stats import pearsonr # 选择 
K 个 最好 的 特征 返回 选择 特征 后的/nr 数据 
# 第一 个 参数 为 计算 评估 特征 是否 好 
的 函数 该 函数 输入 特征 矩阵 和 目标 向量 
输出 二 元组 评分 P 值 的 数组 数组/n 第/m 
i/w 项为第/nr i/w 个/q 特征/n 的/uj 评分/n 和P值/nr 在此 定义 
为 计算 相关系数 # 参数 k 为 选择 的 特征 
个数 SelectKBest lambda X Y array map lambda x pearsonr 
x Y X . T . T k = 2 
. fit _ transform iris . data iris . target 
3 . 1.3 卡方检验 经典 的 卡方检验 是 检验 定性 
自变量 对 定性 因变量 的 相关性 假设 自变量 有N种/nr 取值 
因变量 有M种/nr 取值 考虑 自变量 等于 i 且 因变量 等于 
j 的 样本 频数 的 观察 值 与 期望 的 
差距 构建 统计量 不难 发现 这个 统计量 的 含义 简而言之 
就是 自变量 对 因变量 的 相关性 用 feature _ selection 
库 的 SelectKBest 类 结合 卡方检验 来 选择 特征 的 
代码 如下 from sklearn . feature _ selection import SelectKBest 
from sklearn . feature _ selection import chi2 # 选择 
K 个 最好 的 特征 返回 选择 特征 后的/nr 数据 
SelectKBest chi2 k = 2 . fit _ transform iris 
. data iris . target 3 . 1.4 互信息 法 
经典 的 互信息 也是 评价 定性 自变量 对 定性 因变量 
的 相关性 的 互信息 计算 公式 如下 为了 处理 定量 
数据 最大 信息 系数法 被 提出 使用 feature _ selection 
库 的 SelectKBest 类 结合 最大 信息 系数法 来 选择 
特征 的 代码 如下 from sklearn . feature _ selection 
import SelectKBest from minepy import MINE # 由于 MINE 的 
设计 不是 函 数式 的 定义 mic 方法 将 其为 
函 数式 的 返回 一个 二 元组 二 元组 的 
第 2项 设置成 固定 的 P 值 0.5 def mic 
x y m = MINE m . compute _ score 
x y return m . mic 0.5 # 选择 K 
个 最好 的 特征 返回 特征选择 后的/nr 数据 SelectKBest lambda 
X Y array map lambda x mic x Y X 
. T . T k = 2 . fit _ 
transform iris . data iris . target 3.2 Wrapper3 . 
2.1 递归 特征 消 除法 递归 消除 特征 法 使用 
一个 基 模型 来 进行 多轮 训练 每 轮 训练 
后 消除 若干 权值 系数 的 特征 再 基于 新的 
特征 集 进行 下 一轮 训练 使用 feature _ selection 
库 的 RFE 类 来 选择 特征 的 代码 如下 
from sklearn . feature _ selection import RFE from sklearn 
. linear _ model import L o g i s 
t i c R e g r e s s 
i o n # 递归 特征 消 除法 返回 特征选择 
后的/nr 数据 # 参数 estimator 为 基 模型 # 参数 
n _ features _ to _ select 为 选择 的 
特征 个数 RFE estimator = L o g i s 
t i c R e g r e s s 
i o n n _ features _ to _ select 
= 2 . fit _ transform iris . data iris 
. target 3.3 Embedded3 . 3.1 基于/p 惩罚/vn 项的/nr 特征/n 
选择法/i 使/v 用带/i 惩罚/vn 项的基/nr 模型/n 除了 筛选出 特征 外 
同时 也 进行 了 降 维 使用 feature _ selection 
库 的 SelectFromModel 类 结合带 L1 惩罚 项的/nr 逻辑 回归模型 
来 选择 特征 的 代码 如下 from sklearn . feature 
_ selection import SelectFromModel from sklearn . linear _ model 
import L o g i s t i c R 
e g r e s s i o n # 
带 L1 惩罚 项的/nr 逻辑 回归 作为 基 模型 的 
特征选择 SelectFromModel L o g i s t i c 
R e g r e s s i o n 
penalty = l1 C = 0.1 . fit _ transform 
iris . data iris . target 实际上 L1 惩罚 项降维/nr 
的 原理 在于 保留 多个 对 目标值 具有 同等 相关性 
的 特征 中 的 一个 所以 没 选 到 的 
特征 不 代表 不 重要 故 可 结合 L2 惩罚 
项来/nr 优化 具体 操作 为 若 一个 特征 在 L1 
中的 权值 为 1 选择 在 L 2中 权值 差别 
不大 且 在 L 1中 权值 为 0 的 特征 
构成 同类 集合 将 这一 集合 中的 特征 平分 L1 
中的 权值 故 需要 构建 一个 新 的 逻辑 回归模型 
from sklearn . linear _ model import L o g 
i s t i c R e g r e 
s s i o n class LR L o g 
i s t i c R e g r e 
s s i o n def _ _ init _ 
_ self threshold = 0.01 dual = False tol = 
1e 4 C = 1.0 fit _ intercept = True 
intercept _ scaling = 1 class _ weight = None 
random _ state = None solver = liblinear max _ 
iter = 100 multi _ class = ovr verbose = 
0 warm _ start = False n _ jobs = 
1 # 权值 相近 的 阈值 self . threshold = 
threshold L o g i s t i c R 
e g r e s s i o n . 
_ _ init _ _ self penalty = l1 dual 
= dual tol = tol C = C fit _ 
intercept = fit _ intercept intercept _ scaling = intercept 
_ scaling class _ weight = class _ weight random 
_ state = random _ state solver = solver max 
_ iter = max _ iter multi _ class = 
multi _ class verbose = verbose warm _ start = 
warm _ start n _ jobs = n _ jobs 
# 使用 同样 的 参数 创建 L2 逻辑 回归 self 
. l2 = L o g i s t i 
c R e g r e s s i o 
n penalty = l2 dual = dual tol = tol 
C = C fit _ intercept = fit _ intercept 
intercept _ scaling = intercept _ scaling class _ weight 
= class _ weight random _ state = random _ 
state solver = solver max _ iter = max _ 
iter multi _ class = multi _ class verbose = 
verbose warm _ start = warm _ start n _ 
jobs = n _ jobs def fit self X y 
sample _ weight = None # 训练 L1 逻辑 回归 
super LR self . fit X y sample _ weight 
= sample _ weight self . coef _ old _ 
= self . coef _ . copy # 训练 L2 
逻辑 回归 self . l2 . fit X y sample 
_ weight = sample _ weight cntOfRow cntOfCol = self 
. coef _ . shape # 权值 系数 矩阵 的 
行数 对应 目标值 的 种类 数目 for i in range 
cntOfRow for j in range cntOfCol coef = self . 
coef _ i j # L1 逻辑 回归 的 权值 
系数 不为 0 if coef = 0 idx = j 
# 对 应在 L2 逻辑 回归 中的 权值 系数 coef1 
= self . l2 . coef _ i j for 
k in range cntOfCol coef2 = self . l2 . 
coef _ i k # 在 L2 逻辑 回 归中 
权值 系数 之差 小于 设定 的 阈值 且 在 L 
1中 对应 的 权值 为 0 if abs coef1 coef2 
self . threshold and j = k and self . 
coef _ i k = = 0 idx . append 
k # 计算 这 一类 特征 的 权值 系数 均值 
mean = coef / len idx self . coef _ 
i idx = mean return self 使用 feature _ selection 
库 的 SelectFromModel 类 结合带 L1 以及 L2 惩罚 项的/nr 
逻辑 回归模型 来 选择 特征 的 代码 如下 from sklearn 
. feature _ selection import SelectFromModel #/i 带/v L1/i 和/c 
L2/i 惩罚/vn 项的/nr 逻辑/n 回归/v 作为/v 基/a 模型/n 的/uj 特征选择/nr 
# 参数 threshold 为 权值 系数 之差 的 阈值 SelectFromModel 
LR threshold = 0.5 C = 0.1 . fit _ 
transform iris . data iris . target 3 . 3.2 
基于 树 模型 的 特征 选择法 树 模型 中 GBDT 
也可 用来 作为 基 模型 进行 特征选择 使用 feature _ 
selection 库 的 SelectFromModel 类 结合 GBDT 模型 来 选择 
特征 的 代码 如下 from sklearn . feature _ selection 
import SelectFromModel from sklearn . ensemble import G r a 
d i e n t B o o s t 
i n g C l a s s i f 
i e r # GBDT 作为 基 模型 的 特征选择 
SelectFromModel G r a d i e n t B 
o o s t i n g C l a 
s s i f i e r . fit _ 
transform iris . data iris . target 4 降 维 
当 特征选择 完成后 可以 直接 训练 模型 了 但是 可能 
由于 特征 矩阵 过大 导致 计算 量大 训练 时间 长 
的 问题 因此 降低 特征 矩阵 维度 也是 必不可少 的 
常见 的 降 维 方法 除了 以上 提到 的 基于 
L1 惩罚 项的/nr 模型 以外 另外 还 有主 成分 分析法 
PCA 和 线性 判别分析 LDA 线性 判别分析 本身 也 是 
一个 分类 模型 PCA/w 和/c LDA/w 有/v 很多/m 的/uj 相似点/i 
其 本质 是 要将 原始 的 样本 映 射到 维度 
更低 的 样本 空间 中 但是 PCA 和 LDA 的 
映射 目标 不 一样 PCA 是 为了 让 映射 后的/nr 
样本 具有 最大 的 发散 性 而/c LDA/w 是/v 为了/p 
让/v 映射/v 后的/nr 样本/n 有/v 最好/a 的/uj 分类/n 性能/n 所以 
说 PCA 是 一种 无 监督 的 降 维 方法 
而 LDA 是 一种 有 监督 的 降 维 方法 
4.1 主 成分 分析法 PCA 使用 decomposition 库 的 PCA 
类 选择 特征 的 代码 如下 from sklearn . decomposition 
import PCA # 主 成分 分析法 返回 降 维 后的/nr 
数据 # 参数 n _ components 为主 成分 数目 PCA 
n _ components = 2 . fit _ transform iris 
. data 4.2 线性 判别 分析法 LDA 使用 lda 库 
的 LDA 类 选择 特征 的 代码 如下 from sklearn 
. lda import LDA # 线性 判别 分析法 返回 降 
维 后的/nr 数据 # 参数 n _ components 为 降 
维 后的/nr 维数 LDA n _ components = 2 . 
fit _ transform iris . data iris . target 5 
总结 再 让 我们 回归 一下 本文 开始 的 特征 
工程 的 思维导图 我们 可以 使用 sklearn 完成 几乎 所有 
特征 处理 的 工作 而且 不管 是 数据 预处理 还是 
特征选择 抑或 降 维 它们 都是/nr 通过 某个 类 的 
方法 fit _ transform 完成 的 fit _ transform 要不 
只带 一个 参数 特征 矩阵 要不 带 两个 参数 特征 
矩阵 加 目标 向量 这些 难道 都是 巧合 吗 还是 
故意 设计 成 这样 方法 fit _ transform 中有 fit 
这一 单词 它/r 和/c 训练/vn 模型/n 的/uj fit/w 方法/n 有/v 
关联/ns 吗/y 接下来 我 将在 使用 sklearn 优雅 地 进行 
数据挖掘 中 阐述 其中 的 奥妙 6 参考资料 FAQ What 
is dummy coding IRIS 鸢尾花 数据集 卡方检验 干货 结合 Scikit 
learn 介绍 几种 常用 的 特征选择 方法 机器学习 中 有 
哪些 特征 选择 的 工程 方法 机器学习 中的 数学 4 
线性 判别分析 LDA 主 成分 分析 PCA 