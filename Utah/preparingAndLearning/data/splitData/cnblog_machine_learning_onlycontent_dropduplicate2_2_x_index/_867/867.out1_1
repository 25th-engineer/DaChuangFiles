通常 在 Data Science 中 预处理 数据 有 一个 很 
关键 的 步骤 就是 数据 的 标准化 这里 主要 引用 
sklearn 文档 中 的 一些 东西 来 说明 主要 把 
各个 标准化 方法 的 应用 场景 以及 优缺点 总结 概括 
以来 充当 笔记 首先 我 要 引用 我 自己 的 
文章 Feature Preprocessing on Kaggle 里面 关于 Scaling 的 描述 
Tree based models doesn t depend on scalingNon tree based 
models hugely depend on scaling 一 标准化 / 归一化 的 
好处 1.1 提升 模型 精度 在 机器学习 算法 的 目标 
函数 例如 SVM 的 RBF 内核 或 线性 模型 的 
l1 和 l2 正则化 许多 学习 算法 中 目标函数 的 
基础 都是 假设 所有 的 特征 都是 零 均值 并且 
具有 同一 阶 数上 的 方差 如果 某 个 特征 
的 方差 比 其他 特征 大几 个 数量级 那么 它 
就会 在 学习 算法 中 占据 主导 位置 导致 学习 
器 并 不能 像 我们 说 期望 的 那样 从 
其他 特征 中 学习 举 一个 简单 的 例子 在 
KNN 中 我们 需要 计算 待 分类 点 与 所有 
实例 点 的 距离 假设 每个 实例 点 instance 由 
n 个 features 构成 如果 我们 选用 的 距离 度量 
为 欧式 距离 如果 数据 预先 没有 经过 归一化 那么 
那些 绝对值 大 的 features 在 欧式 距离 计算 的 
时候 起 了 决定性 作用 soga 从 经验 上 说 
归一化 是 让 不同 维度 之间 的 特征 在 数值 
上 有 一定 比较性 可以 大大 提高 分类器 的 准确性 
1.2 提升 收敛 速度 对于 线性 model 来说 数据 归一化 
后 最优 解的/nr 寻优 过程 明显 会 变得 平缓 更容易 
正确 的 收敛 到 最优 解 比较 这 两个 图 
前者 是 没有 经过 归一化 的 在 梯度 下降 的 
过程 中 走 的 路径 更加 的 曲折 而 第二个 
图 明显 路径 更加 平缓 收敛 速度 更快 对于 神经网络 
模型 避免 饱和 是 一个 需要 考虑 的 因素 通常 
参数 的 选择 决定 于 input 数据 的 大小 范围 
二 标准化 / 归一化 方法 sklearn 的 preprocessing 提供 了 
可以 满足 需求 的 归一化 方法 2.1 StandardScaler 标准化 数据 
通过 减去 均值 然后 除以 方差 或 标准差 这种 数据 
标准化 方法 经过 处理 后 数据 符合标准 正态分布 即 均值 
为 0 标准差 为 1 转化 函数 为 x = 
x ? ? / ? ? 适用于 如果 数据 的 
分布 本身 就 服从 正态分布 就 可以 用 这个 方法 
通常 这种 方法 基本 可 用于 有 outlier 的 情况 
但是 在 计算 方差 和 均值 的 时候 outliers 仍然 
会 影响 计算 所以 在 出现 outliers 的 情况 下 
可能 会 出现 转换 后的数/nr 的 不同 feature 分布 完全 
不同 的 情况 如 下图 经过 StandardScaler 之后 横坐标 与 
纵坐标 的 分布 出现 了 很大 的 差异 这 可能 
是 outliers 造成 的 2.2 MinMaxScaler 将 特征 缩放 至 
特定 区间 将 特征 缩 放到 给定 的 最小值 和 
最大值 之间 或者 也 可以 将 每个 特征 的 最大 
绝对值 转换 至 单位 大小 这种 方法 是 对 原始 
数据 的 线性变换 将 数据 归 一到 0 1 中间 
转换 函数 为 x = x min / max min 
这种方法 有个 缺陷 就是 当 有新/nr 数据 加 入时 可能 
导致 max 和 min 的 变化 需要 重新 定义 敲 
黑板 这种方法 对于 outlier 非常 敏感 因为 outlier 影响 了 
max 或 min 值 所以 这种 方法 只 适用 于 
数据 在 一个 范围 内 分布 的 情况 2.3 RobustScaler 
如果 你 的 数据 包含 许多 异常值 使用 均值 和 
方差 缩放 可能 并 不是 一个 很好 的 选择 这种 
情况 下 你 可以 使用 robust _ scale 以及 RobustScaler 
作为 替代品 它们 对 你 的 数据 的 中心 和 
范围 使用 更有 鲁棒性 的 估计 This Scaler removes the 
median 中位数 and scales the data according to the quantile 
range 四分位距 离 也 就是说 排除 了 outliers 2.4 0 
1 还是 1 1 假设 我们 有 一个 只有 一个 
hidden layer 的 多层 感知机 MLP 的 分类 问题 每个 
hidden unit 表示 一个 超平面 每个 超平面 是 一个 分类 
边界 参数 w weight 决定 超平面 的 方向 参数 b 
bias 决定 超平面 离 原点 的 距离 如果 b 是 
一些 小 的 随机 参数 事实上 b 确实 被 初始 
化为 很小 的 随机 参数 那么 所有 的 超平面 都 
几乎 穿过 原点 所以 如果 data 没有 中心化 在 原点 
周围 那么 这个 超平面 可能 没有 穿 过 这些 data 
也 就是说 这些 data 都在 超平面 的 一侧 这样的话 局部 
极 小点 local minima 很 有可能 出现 所以 在 这种 
情况 下 标准化 到 1 1 比 0 1 更好 
1 在 分类 聚 类 算法 中 需要 使用 距离 
来 度量 相似性 的 时候 或者 使用 PCA 技术 进行 
降 维 的 时候 StandardScaler 表现 更好 2 在 不 
涉及 距离 度量 协方差 计算 数据 不 符合 正太 分布 
的 时候 可以 使用 MinMaxScaler 比如 图像 处理 中 将 
RGB 图像 转换 为 灰度 图像 后 将其 值 限定 
在 0 255 的 范围 原因 是 使用 MinMaxScaler 其 
协方差 产生了 倍 数值 的 缩放 因此 这种 方式 无法 
消除 量纲 对 方差 协方差 的 影响 对 PCA 分析 
影响 巨大 同时 由于 量纲 的 存在 使用 不同 的 
量纲 距离 的 计算 结果 会 不同 而在 StandardScaler 中 
新的 数据 由于 对 方差 进行 了 归一化 这时候 每个 
维度 的 量纲 其实 已经 等价 了 每个 维度 都 
服从 均值 为 0 方差 1 的 正态分布 在 计算 
距离 的 时候 每个 维度 都是 去 量纲 化 的 
避免 了 不同 量纲 的 选取 对 距离 计算 产生 
的 巨大 影响 Reference 预处理 数据 sklearn 数据 标准化 / 
归一化 normalization 机器学习 笔记 为什么 要 对 数据 进行 归一化 
处理 Compare the effect of different scalers on data with 
outliers 数据 归一化 和 两种 常用 的 归一化 方法 Should 
I normalize / standardize / rescale the data 