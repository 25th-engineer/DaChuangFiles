前言 找 工作 时 IT 行业 除了 常见 的 软件 
开发 以外 机器学习 岗位 也 可以 当作 是 一个 选择 
不少 计算机 方向 的 研究生 都会 接触 这个 如果 你 
的 研究 方向 是 机器学习 / 数据挖掘 之类 且 又对 
其 非常 感兴趣 的话 可以 考虑 考虑 该 岗位 毕竟 
在 机器 智能 没 达到 人类 水平 之前 机器学习 可以 
作为 一种 重要 手段 而 随着 科技 的 不断 发展 
相信 这 方面 的 人才 需求 也 会 越来越 大 
纵观 IT 行业 的 招聘 岗位 机器学习 之类 的 岗位 
还是 挺 少 的 国内 大点 的 公司 里 百度 
阿里 腾讯 网易 搜狐 华为 华为 的 岗位 基本 都是/nr 
随机 分配 机器学习 等 岗位 基本 面向 的 是 博士 
等会 有 相关 职位 另外 一些 国内 的 中小型 企业 
和 外企 也会 招 一小部分 当然 了 其中 大部分 还是 
百度 北京 要 人 最多 上百人 阿里 的 算法 岗位 
很大 一部分 也是 搞 机器学习 相关 的 另外 本人 有幸 
签约 了 网易 杭州 研究院 的 深度 学习 算法 岗位 
打算 从事 机器学习 领域 至少 5年 非常感谢 小 易 收留 
了 我 下面 是 本人 在 找 机器学习 岗位 工作 
时 总结 的 常见 机器学习 算法 主要 是 一些 常规 
分类器 大概 流程 和 主要 思想 希望 对 大家 找 
机器学习 岗位 时 有点 帮助 实际上 在 面试 过程 中 
懂 这些 算法 的 基本 思想 和 大概 流程 是 
远远 不够 的 那些 面试官 往往 问 的 都是 一些 
公司 内部 业务 中 的 课题 往往 要求 你 不仅 
要 懂得 这些 算法 的 理论 过程 而且 要 非常 
熟悉 怎样 使用 它 什么 场合 用 它 算法 的 
优缺点 以及 调 参 经验 等等 说白了 就是 既要 会点 
理论 也要 会点 应用 既要 有点 深度 也要 有点 广度 
否则 运气 不好 的话 很容易 就被 刷掉 因为 每个 面试官 
爱好 不同 朴素 贝叶斯 有 以下 几个 地方 需要 注意 
1 . 如果 给出 的 特征向量 长度 可能 不同 这是 
需要 归一 化为 通 长度 的 向量 这里 以 文本 
分类 为例 比如 说是 句子 单词 的话 则 长度 为 
整个 词汇量 的 长度 对应 位置 是 该 单词 出现 
的 次数 2 . 计算 公式 如下 其中 一项 条件概率 
可以 通过 朴素 贝叶斯 条件 独立 展开 要 注意 一点 
就是 的 计算 方法 而由 朴素 贝叶斯 的 前提 假设 
可知 = 因此 一般 有 两种 一种 是 在 类别 
为 ci 的 那些 样本 集中 找到 wj 出现 次数 
的 总和 然后 除 以该 样本 的 总和 第二 种 
方法 是 类别 为 ci 的 那些 样本 集中 找到 
wj 出现 次数 的 总和 然后 除 以该 样本 中 
所有 特征 出现 次数 的 总和 3 . 如果 中的 
某 一项 为 0 则 其 联合 概率 的 乘积 
也 可能 为 0 即 2中 公式 的 分子 为 
0 为了 避免 这种 现象 出现 一般 情况 下 会将 
这 一项 初始 化为 1 当然 为了 保证 概率 相等 
分母 应 对应 初始 化为 2 这里 因为 是 2类 
所以 加 2 如果 是 k 类 就 需要 加 
k 术语 上 叫做 laplace 光滑 分母 加 k 的 
原因 是 使之 满足 全 概率 公式 朴素 贝叶斯 的 
优点 对 小 规模 的 数据 表现 很好 适合 多 
分类 任务 适合 增量 式 训练 缺点 对 输入 数据 
的 表达 形式 很 敏感 决策树 决策树 中 很 重要 
的 一点 就是 选择 一个 属性 进行 分枝 因此 要 
注意 一下 信息 增益 的 计算 公式 并 深入 理解 
它 信息熵 的 计算 公式 如下 其中 的 n 代表 
有n个/nr 分类 类别 比如 假设 是 2类 问题 那么 n 
= 2 分别 计算 这 2类 样本 在 总 样本 
中 出现 的 概率 p1 和 p2 这样 就 可以 
计算出 未 选中 属性 分枝 前 的 信息 熵 现在 
选中 一个 属性 xi 用来 进行 分枝 此时 分枝 规则 
是 如果 xi = vx 的话 将 样本 分到 树 
的 一个 分支 如果 不 相等 则 进入 另 一个 
分支 很显然 分支 中的 样本 很 有可能 包括 2个 类别 
分别 计算 这 2个 分支 的 熵 H1 和 H2 
计算出 分枝 后的总/nr 信息熵 H = p1 * H1 + 
p2 * H2 . 则 此时 的 信息 增益 Δ 
H = H H 以 信息 增益 为 原则 把 
所有 的 属性 都 测试 一边 选择 一个 使 增益 
最大 的 属性 作为 本次 分枝 属性 决策树 的 优点 
计算 量 简单 可解释 性强 比较 适合 处理 有 缺失 
属性值 的 样本 能够 处理 不 相关 的 特征 缺点 
容易 过拟合 后续 出现 了 随机 森林 减小 了 过拟合 
现象 Logistic 回归 Logistic 是 用来 分类 的 是 一种 
线性 分类器 需要 注意 的 地方 有 1 . logistic 
函数 表达式 为 其 导数 形式 为 2 . logsitc 
回归 方法 主要 是 用 最大 似 然 估计 来 
学习 的 所以 单个 样本 的 后验/nr 概率 为 到 
整个 样本 的 后验/nr 概率 其中 通过 对数 进一步 化简 
为 3 . 其实 它 的 loss function 为 l 
θ 因此 我们 需 使 loss function 最小 可采用 梯度 
下 降法 得到 梯度 下 降法 公式 为 Logistic 回归 
优点 1 实现 简单 2 分类 时 计算 量 非常 
小 速度 很快 存储资源 低 缺点 1 容易 欠 拟合 
一般 准确度 不 太高 2 只能 处理 两 分类 问题 
在此 基础 上 衍生 出来 的 softmax 可以 用于 多 
分类 且 必须 线性 可分 线性 回归 线性 回归 才是 
真正 用于 回归 的 而 不像 logistic 回归 是 用于 
分类 其 基本 思想 是 用 梯度 下 降法 对 
最小二乘 法 形式 的 误差函数 进行 优化 当然 也 可以 
用 normal equation 直接 求得 参数 的 解 结果 为 
而在 LWLR 局部 加权 线性 回归 中 参数 的 计算 
表达式 为 因为 此时 优化 的 是 由此可见 LWLR 与 
LR 不同 LWLR 是 一个 非 参数 模型 因为 每次 
进行 回归 计算 都要 遍历 训练样本 至少 一次 线性 回归 
优点 实现 简单 计算 简单 缺点 不能 拟合 非线性 数据 
KNN 算法 KNN 即 最 近邻 算法 其主要 过程 为 
1 . 计算 训练样本 和 测试 样本 中 每个 样本点 
的 距离 常见 的 距离 度量 有 欧式 距离 马氏 
距离 等 2 . 对 上面 所有 的 距离 值 
进行 排序 3 . 选 前 k 个 最小 距离 
的 样本 4 . 根据 这 k 个 样本 的 
标签 进行 投票 得到 最后 的 分类 类别 如何 选择 
一个 最佳 的 K 值 这 取决于 数据 一般 情况 
下 在 分类 时 较大 的 K 值 能够 减小 
噪声 的 影响 但 会使 类别 之间 的 界限 变得 
模糊 一个 较好 的 K 值 可 通过 各种 启发式 
技术 来 获取 比如 交叉 验证 另外 噪声 和非/nr 相关性 
特征向量 的 存在 会使 K 近邻 算法 的 准确性 减小 
近邻 算法 具有 较强 的 一致性 结果 随着 数据 趋于 
无限 算法 保证 错误率 不会 超过 贝叶斯 算法 错误率 的 
两倍 对于 一些 好 的 K 值 K 近邻 保证 
错误率 不会 超过 贝叶斯 理论 误差率 注 马氏 距离 一定 
要 先 给出 样 本集 的 统计 性质 比如 均值 
向量 协方差 矩阵 等 关于 马氏 距离 的 介绍 如下 
KNN 算法 的 优点 1 . 思想 简单 理论 成熟 
既 可以 用来 做 分类 也 可以 用来 做 回归 
2 . 可 用于 非线性 分类 3 . 训练 时间 
复杂度 为 O n 4 . 准确度 高 对 数据 
没有 假设 对 outlier 不 敏感 缺点 1 . 计算 
量大 2 . 样本 不 平衡 问题 即 有些 类别 
的 样本 数量 很多 而 其它 样本 的 数量 很少 
3 . 需要 大量 的 内存 SVM 要 学会 如何 
使用 libsvm 以及 一些 参数 的 调节 经验 另外 需要 
理 清楚 svm 算法 的 一些 思路 1 . svm 
中的 最优 分类 面 是 对 所有 样本 的 几何 
裕量 最大 为什么 要 选择 最大 间隔 分类器 请 从 
数学 角度 上 说明 网易 深度 学习 岗位 面试 过程 
中 有被 问到 答案 就是 几何 间隔 与 样本 的 
误 分次 数间 存在 关系 其中 的 分母 就是 样本 
到 分类 间隔 距离 分子 中的 R 是 所有 样本 
中 的 最长 向 量值 即 经过 一 系列 推导 
可得 为 优化 下面 原始 目标 2 . 下面 来 
看看 拉格朗日 理论 可以 将 1中 的 优化 目标 转换 
为 拉格朗日 的 形式 通过 各种 对偶 优化 KKD 条件 
最后 目标函数 为 我们 只 需要 最小化 上述 目标函数 其中 
的 α 为 原始 优化 问题 中的 不等式 约束 拉格朗日 
系数 3 . 对 2中 最后 的 式子 分别 w 
和b/nr 求导 可得 由 上面 第 1 式子 可以 知道 
如果 我们 优化 出了 α 则 直接 可以 求出 w 
了 即 模型 的 参数 搞定 而 上面 第 2个 
式子 可以 作为 后续 优化 的 一个 约束条件 4 . 
对 2中 最后 一个 目标 函 数用 对偶 优化 理论 
可以 转换 为 优化 下面 的 目标 函数 而 这个 
函数 可以用 常用 的 优化 方法 求得 α 进而 求得 
w 和b/nr 5 . 按照 道理 svm 简单 理论 应该 
到此结束 不过 还是 要 补充 一点 即在 预测 时有 那个 
尖括号 我们 可以 用 核 函数 代替 这 也是 svm 
经常 和核/nr 函数 扯 在 一起 的 原因 6 . 
最后 是 关于 松弛 变量 的 引入 因此 原始 的 
目标 优化 公式 为 此时 对应 的 对偶 优化 公式 
为 与 前面 的 相比 只是 α 多了 个 上界 
SVM 算法 优点 可 用于 线性 / 非线性 分类 也 
可以 用于 回归 低 泛化 误差 容易 解释 计算 复杂度 
较低 缺点 对 参数 和核/nr 函数 的 选择 比较 敏感 
原始 的 SVM 只 比较 擅长 处理 二分 类 问题 
Boosting 主要 以 Adaboost 为例 首先 来 看看 Adaboost 的 
流程图 如下 从 图中 可以 看到 在 训练 过程 中 
我们 需要 训练 出 多个 弱 分类器 图 中为 3个 
每个 弱 分类器 是由 不同 权重 的 样本 图 中为 
5个 训练样本 训练 得到 其中 第一 个 弱 分类器 对应 
输入 样本 的 权值 是 一样 的 而 每个 弱 
分类器 对 最终 分类 结果 的 作用 也 不同 是 
通过 加权平均 输出 的 权值 见上图 中 三角形 里面 的 
数值 那么 这些 弱 分类器 和其/nr 对应 的 权值 是 
怎样 训练 出来 的 呢 下面 通过 一个 例子 来 
简单 说明 书中 machine learning in action 假设 的 是 
5个 训练样本 每个 训练样本 的 维度 为 2 在 训练 
第一 个 分类器 时 5个 样本 的 权重 各为 0.2 
. 注意 这里 样本 的 权值 和 最终 训练 的 
弱 分类器 组 对应 的 权值 α 是 不同 的 
样本 的 权重 只在 训练 过程 中用 到 而 α 
在 训练 过程 和 测试过程 都有 用到 现在 假设 弱 
分类器 是 带 一个 节点 的 简单 决策树 该 决策树 
会 选择 2个 属性 假设 只有 2个 属性 的 一个 
然后 计算 出 这个 属性 中的 最佳值 用来 分类 Adaboost 
的 简单 版本 训练 过程 如下 1 . 训练 第一 
个 分类器 样本 的 权值 D 为 相同 的 均值 
通过 一个 弱 分类器 得到 这 5个 样本 请 对应 
书中 的 例子 来看 依旧 是 machine learning in action 
的 分类 预测 标签 与 给出 的 样本 真实 标签 
对比 就 可能 出现 误差 即 错误 如果 某 个 
样本 预测 错误 则 它 对应 的 错误 值 为 
该 样本 的 权重 如果 分类 正确 则 错误 值 
为 0 . 最后 累加 5个 样本 的 错误率 之和 
记为 ε 2 . 通过 ε 来 计算 该 弱 
分类器 的 权重 α 公式 如下 3 . 通过 α 
来 计算 训练 下一个 弱 分类器 样本 的 权重 D 
如果 对应 样本 分类 正确 则 减小 该 样本 的 
权重 公式 为 如果 样本 分类 错误 则 增加 该 
样本 的 权重 公式 为 4 . 循环 步骤 1 
2 3 来 继续 训练 多个 分类器 只是 其 D 
值 不同 而已 测试过程 如下 输入 一个 样本 到 训练 
好 的 每个 弱 分类 中 则 每个 弱 分类 
都 对应 一个 输出 标签 然后 该 标签 乘以 对应 
的 α 最后 求和 得到 值 的 符号 即为 预测 
标签 值 Boosting 算法 的 优点 低 泛化 误差 容易 
实现 分类 准确率 较高 没有 太多 参数 可以 调 缺点 
对 outlier 比较 敏感 聚 类 根据 聚 类 思想 
划分 1 . 基于 划分 的 聚 类 K means 
k medoids 每 一个 类别 中 找 一个 样本点 来 
代表 CLARANS . k means 是 使下 面的 表达式 值 
最小 k means 算法 的 优点 1 k means 算法 
是 解决 聚 类 问题 的 一种 经典 算法 算法 
简单 快速 2 对 处理 大 数据集 该 算法 是 
相对 可伸缩 的 和 高效率 的 因为 它 的 复杂度 
大约 是 O nkt 其中 n 是 所有 对象 的 
数目 k 是 簇 的 数目 t 是 迭代 的 
次数 通常 k n 这个 算法 通常 局部收敛 3 算法 
尝试 找 出使 平方 误差 函数值 最小 的 k 个 
划分 当 簇 是 密集 的 球状 或 团状 的 
且 簇 与 簇 之间 区别 明显 时 聚 类 
效果 较好 缺点 1 k 平均 方法 只有 在 簇 
的 平均值 被 定义 的 情况 下 才能 使用 且 
对 有些 分类 属性 的 数据 不 适合 2 要求 
用户 必须 事先 给出 要 生成 的 簇 的 数目 
k 3 对 初值 敏感 对于 不同 的 初始值 可能 
会 导致 不同 的 聚 类 结果 4 不 适合 
于 发现 非 凸面 形状 的 簇 或者 大小 差别 
很大 的 簇 5 对于 噪声 和 孤立 点 数据 
敏感 少量 的 该类 数据 能够 对 平均值 产生 极大 
影响 2 . 基于 层次 的 聚 类 自底向上 的 
凝聚 方法 比如 AGNES 自上 向下 的 分裂 方法 比如 
DIANA 3 . 基于 密度 的 聚 类 DBSACN OPTICS 
BIRCH CF Tree CURE . 4 . 基于 网格 的 
方法 STING WaveCluster . 5 . 基于 模型 的 聚 
类 EM SOM COBWEB . 以上 这些 算法 的 简介 
可 参考 聚 类 百度 百科 推荐 系统 推荐 系统 
的 实现 主要 分为 两个 方面 基于 内容 的 实现 
和 协同 滤波 的 实现 基于 内容 的 实现 不同 
人 对 不同 电影 的 评分 这个 例子 可以 看做 
是 一个 普通 的 回归 问题 因此 每部 电影 都 
需要 提前 提取 出 一个 特征向量 即 x 值 然后 
针对 每个 用户 建模 即 每个 用户 打的 分值 作为 
y 值 利用 这些 已 有的 分值 y 和 电影 
特征值 x 就 可以 训练 回归模型 了 最 常见 的 
就是 线性 回归 这样 就 可以 预测 那些 用 户 
没有 评分 的 电影 的 分数 值得 注意 的 是 
需 对 每个 用户 都 建立 他 自己 的 回归模型 
从 另一个 角度 来看 也 可以 是 先 给定 每个 
用户 对 某种 电影 的 喜好 程度 即 权值 然 
后学 出 每部 电影 的 特征 最后 采用 回归 来 
预测 那些 没有 被 评分 的 电影 当然 还 可以 
是 同时 优化 得到 每个 用户 对 不同 类型 电影 
的 热爱 程度 以及 每部 电影 的 特征 具体 可以 
参考 Ng 在 coursera 上 的 ml 教程 https / 
/ www . coursera . org / course / ml 
基于 协同 滤波 的 实现 协同 滤波 CF 可以 看做 
是 一个 分 类 问题 也 可以 看做 是 矩阵 
分解 问题 协同 滤波 主要 是 基于 每个 人 自己 
的 喜好 都 类似 这一 特征 它 不 依赖 于 
个人 的 基本 信息 比如 刚刚 那个 电影 评分 的 
例子 中 预测 那些 没有 被 评分 的 电影 的 
分数 只 依赖于 已经 打分 的 那些 分数 并不 需要 
去 学习 那些 电影 的 特征 SVD 将 矩阵 分解为 
三个 矩阵 的 乘积 公式 如下 所示 中间 的 矩阵 
sigma 为 对角 矩阵 对角 元素 的 值 为 Data 
矩阵 的 奇异 值 注意 奇异 值 和 特征值 是 
不同 的 且 已经 从大到/nr 小 排列 好了 即使 去掉 
特征值 小 的 那些 特征 依然 可以 很好 的 重构 
出 原始 矩阵 如下 图 所示 其中 更深 的 颜色 
代表 去掉 小 特征值 重构 时的/nr 三个 矩阵 果 m 
代表 商品 的 个数 n 代表 用户 的 个数 则 
U 矩阵 的 每 一行 代表 商品 的 属性 现在 
通过 降 维 U 矩阵 取 深色 部分 后 每 
一个 商品 的 属性 可以用 更低 的 维度 表示 假设 
为 k 维 这样 当 新来 一个 用户 的 商品 
推荐 向量 X 则 可以 根据 公式 X * U1 
* inv S1 得到 一个 k 维 的 向量 然后 
在 V 中 寻找 最 相似 的 那 一个 用户 
相似 度 测量 可用 余弦公式 等 根据 这个 用户 的 
评分 来 推荐 主要 是 推荐 新 用户 未 打分 
的 那些 商品 具体 例子 可以 参考 网页 SVD 在 
推荐 系统 中 的 应用 另外 关于 SVD 分解 后 
每个 矩阵 的 实际 含义 可以 参考 google 吴军 的 
数学 之美 一 书 不过 个人感觉 吴军 解释 UV 两个 
矩阵 时 好像 弄 反了 不 知道 大家 怎样 认为 
或者 参考 machine learning in action 其中 的 svd 章节 
pLSA pLSA 由 LSA 发展 过来 而 早期 LSA 的 
实现 主要 是 通过 SVD 分解 pLSA 的 模型 图 
如下 公式 中 的 意义 如下 具体 可以 参考 2010 
龙星 计划 机器学习 中 对应 的 主题 模型 那一 讲 
LDA 主题 模型 概率 图 如下 和 pLSA 不同 的 
是 LDA 中 假设 了 很多 先验 分布 且 一般 
参数 的 先验 分布 都 假设 为 Dirichlet 分布 其/r 
原因/n 是/v 共轭/n 分布/v 时/n 先验概率/l 和后验/nr 概率/n 的/uj 形式/n 
相同/d GDBT GBDT Gradient   Boosting   Decision   Tree 
  又叫   MART Multiple   Additive   Regression   
Tree 好像 在 阿里 内部 用得 比较 多 所以 阿里 
算法 岗位 面试 时 可能会 问到 它 是 一种 迭代 
的 决策树 算法 该 算法 由 多棵 决策树 组成 所有 
树 的 输出 结果 累加 起来 就是 最终 答案 它 
在被 提出 之初 就和 SVM 一起 被 认为 是 泛化 
能力 generalization 较强 的 算法 近些年 更 因为 被 用于 
搜索 排序 的 机器学习 模型 而 引起 大家 关注 GBDT 
是 回归 树 不是 分类 树 其 核心 就 在于 
每 一棵树 是从 之前 所有 树 的 残差 中 来 
学习 的 为了 防止 过拟合 和 Adaboosting 一样 也 加入 
了 boosting 这 一项 关于 GDBT 的 介绍 可以 可以 
参考 GBDT MART 迭代 决策树 入门教程 | 简介 Regularization 作用 
是 网易 电话 面试 时有 问到 1 . 数值 上 
更容易 求解 2 . 特征 数目 太大 时更/nr 稳定 3 
. 控制 模型 的 复杂度 光滑性 复杂性 越小 且 越 
光滑 的 目标 函数 泛化 能力 越强 而 加入 规则 
项 能使 目标函数 复杂度 减小 且 更 光滑 4 . 
减小 参数 空间 参数 空间 越小 复杂度 越低 5 . 
系数 越小 模型 越 简单 而 模型 越 简单 则 
泛化 能力 越强 Ng 宏观 上 给出 的 解释 6 
. 可以 看成 是 权值 的 高斯 先验 异常 检测 
可以估计 样本 的 密度 函数 对于 新 样本 直接 计算 
其 密度 如果 密度 值 小于 某一 阈值 则 表示 
该 样本 异常 而 密度 函数 一般 采用 多维 的 
高斯分布 如果 样本 有n维/nr 则 每 一维 的 特征 都 
可以 看作 是 符合 高斯分布 的 即使 这些 特征 可视化 
出来 不太 符合 高斯分布 也 可以 对 该 特征 进行 
数学 转换 让其 看起来 像 高斯分布 比如说 x = log 
x + c x = x ^ 1 / c 
等 异常 检测 的 算法 流程 如下 其中 的 ε 
也 是 通过 交叉 验证 得到 的 也 就是说 在 
进行 异常 检测 时 前面 的 p x 的 学习 
是 用 的 无 监督 后面 的 参数 ε 学习 
是 用 的 有 监督 那么 为什么 不 全部 使用 
普通 有 监督 的 方法 来 学习 呢 即把 它 
看做 是 一个 普通 的 二分 类 问题 主要 是 
因为 在 异常 检测 中 异常 的 样本 数量 非常 
少 而 正常 样本 数量 非常 多 因此 不 足以 
学习 到 好 的 异常 行为 模型 的 参数 因为 
后面 新来 的 异常 样本 可能 完全 是 与 训练 
样本 中 的 模式 不同 另外 上面 是 将 特征 
的 每 一维 看成 是 相互 独立 的 高斯分布 其实 
这样 的 近似 并 不是 最好 的 但是 它 的 
计算 量 较小 因此 也 常被 使用 更好 的 方法 
应该 是 将 特征 拟 合成 多维 高斯分布 这时 有 
特征 之间 的 相关性 但 随之 计算 量 会变 复杂 
且 样本 的 协方差 矩阵 还 可能 出现 不 可逆 
的 情况 主要 在 样本数 比 特征 数 小 或者 
样本 特征 维数 之间 有 线性关系 时 上面 的 内容 
可以 参考 Ng 的 https / / www . coursera 
. org / course / mlEM 算法 有时候 因为 样本 
的 产生 和 隐含 变量 有关 隐含 变量 是 不能 
观察 的 而 求 模型 的 参数 时 一般 采用 
最大 似 然 估计 由于 含 有了 隐含 变量 所以 
对 似 然 函数参数 求导 是 求 不 出来 的 
这时 可以 采用 EM 算法 来 求 模型 的 参数 
的 对应 模型 参数 个数 可能 有 多个 EM 算法 
一般 分为 2步 E 步 选取 一组 参数 求 出在 
该 参 数下 隐含 变量 的 条件 概率值 M 步 
结合 E 步 求出 的 隐含 变量 条件概率 求出 似 
然 函数 下界 函数 本质上 是 某个 期望 函数 的 
最大 值 重复 上面 2步 直至 收敛 公式 如下 所示 
M 步 公式 中 下界 函数 的 推导 过程 EM 
算法 一个 常见 的 例子 就是 GMM 模型 每个 样本 
都 有可能 由 k 个 高斯 产生 只不过 由 每个 
高斯 产生 的 概率 不同 而已 因此/c 每个/r 样本/n 都有/nr 
对应/vn 的/uj 高斯分布/nr k 个中 的 某一个 此时 的 隐含 
变量 就是 每个 样本 对应 的 某个 高斯分布 GMM 的 
E 步 公式 如下 计算 每个 样本 对应 每个 高斯 
的 概率 更 具体 的 计算 公式 为 M 步 
公式 如下 计算 每个 高斯 的 比重 均值 方差 这 
3个 参数 关于 EM 算法 可以 参考 Ng 的 cs229 
课程 资料 或者 网易 公开课 斯坦福大学 公开课 机器学习 课程 Apriori 
Apriori 是 关联 分析 中 比较 早 的 一种 方法 
主要 用来 挖掘 那些 频繁 项 集合 其 思想 是 
1 . 如果 一个 项目 集合 不是 频繁 集合 那么 
任何 包含 它 的 项目 集合 也 一定 不是 频繁 
集合 2 . 如果 一个 项目 集合 是 频繁 集合 
那么 它 的 任何 非空 子集 也是 频繁 集合 Aprioir 
需要 扫描 项目表 多遍 从 一个 项目 开始 扫描 舍去 
掉 那些 不 是 频繁 的 项目 得到 的 集合 
称为 L 然后 对 L 中的 每个 元素 进行 自 
组合 生成 比 上次 扫描 多 一个 项目 的 集合 
该集 合 称为 C 接着 又 扫描 去掉 那些 非 
频繁 的 项目 重复 看 下面 这个 例子 元素 项目 
表格 如果 每个 步骤 不 去掉 非 频繁 项目 集 
则 其 扫描 过程 的 树形 结构 如下 在其中 某个 
过程 中 可能 出现 非 频繁 的 项目 集 将其 
去掉 用 阴影 表示 为 上面 的 内容 主要 参考 
的 是 machine learning in action 这本书 FP Growth FP 
Growth 是 一种 比 Apriori 更 高效 的 频繁 项 
挖掘 方法 它 只 需要 扫描 项目表 2次 其中 第 
1次 扫描 获得 当 个 项目 的 频率 去掉 不 
符合 支持度 要求 的 项 并对 剩下 的 项 排序 
第 2遍 扫描 是 建立 一颗 FP Tree frequent patten 
tree 接下来 的 工作 就是 在 FP Tree 上 进行 
挖掘 比如说 有 下表 它 所 对应 的 FP _ 
Tree 如下 然后 从 频率 最小 的 单项 P 开始 
找出 P 的 条件 模式 基 用 构造 FP _ 
Tree 同样 的 方法 来 构造 P 的 条件 模式 
基 的 FP _ Tree 在 这 棵树 上 找出 
包含 P 的 频繁 项集/nr 依次 从m/nr b a c 
f 的 条件 模式 基 上 挖掘 频繁 项集/nr 有些 
项 需要 递归 的 去 挖掘 比较 麻烦 比如 m 
节点 具体 的 过程 可以 参考 博客 Frequent   Pattern 
  挖掘 之二 FP   Growth 算法 里面 讲得 很详细 
参考资料 Harrington P . 2012 . Machine Learning in Action 
Manning Publications Co . 最 近邻 算法 维基百科 马氏 距离 
维基百科 聚 类 百度 百科 https / / www . 
coursera . org / course / mlSVD 在 推荐 系统 
中 的 应用 吴军 and 谷歌 2012 . 数学 之美 
人民邮电出版社 . 2010 龙星 计划 机器学习 对应 的 视频 教程 
2010 龙星 计划 机器学习 视频教程 GBDT MART 迭代 决策树 入门教程 
| 简介 Ng 的 cs229 课程 资料 斯坦福大学 公开课 机器学习 
课程 Frequent   Pattern   挖掘 之二 FP   Growth 
算法 