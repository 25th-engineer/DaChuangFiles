1 . 前言 熟悉 机器 学习 的 童鞋 都 知道 
优化 方法 是 其中 一个 非常 重要 的 话题 最 
常见 的 情形 就是 利用 目标 函数 的 导数 通过 
多次 迭代 来 求解 无约束 最优化 问题 实现 简单 coding 
方便 是 训练 模型 的 必备 利器 之一 这篇 博客 
主要 总结 一下 使用 导数 的 最优化 方法 的 几个 
基本 方法 梳理 梳理 相关 的 数学 知识 本人 也 
是 一边 写 一边 学 如 有 问题 欢迎 指正 
共同 学习 一起 进步 2 . 几个 数学 概念 1 
梯度 一 阶 导数 考虑 一座 在 x1 x2 点 
高度 是 f x1 x2 的 山 那么 某 一点 
的 梯度 方向 是 在 该点 坡度 最 陡 的 
方向 而 梯度 的 大小 告诉 我们 坡度 到底有 多 
陡 注意 梯度 也 可以 告诉 我们 不 在 最快 
变化 方向 的 其他 方向 的 变化 速度 二维 情况 
下 按照 梯度方向 倾斜 的 圆 在 平面 上 投影 
成 一个 椭圆 对于 一个 含有 n 个 变量 的 
标量 函数 即 函数 输入 一个 n 维 的 向量 
输 出 一个 数值 梯度 可以 定义 为 2 Hesse 
矩阵 二阶 导数 Hesse 矩阵 常被 应用于 牛顿 法 解决 
的 大规模 优化 问题 后面 会 介绍 主要 形式 如下 
当 f x 为 二次函数 时 梯度 以及 Hesse 矩阵 
很容易 求得 二次函数 可以 写成 下列 形式 其中 A 是 
n 阶 对称矩阵 b 是 n 维 列 向量 c 
是 常数 f x 梯度 是 Ax + b Hesse 
矩阵 等于 A 3 Jacobi 矩阵 Jacobi 矩阵 实际上 是 
向 量值 函数 的 梯度 矩阵 假设 F Rn → 
Rm 是 一个 从n维/nr 欧氏 空间 转换 到 m 维 
欧氏 空间 的 函数 这个 函数 由 m 个 实 
函数 组成 这些 函数 的 偏 导数 如果 存在 可以 
组成 一个 m 行 n 列 的 矩阵 m by 
n 这 就是 所谓 的 雅可比 矩阵 总结 一下 a 
如果 f x 是 一个 标量 函数 那么 雅 克比 
矩阵 是 一个 向量 等于 f x 的 梯度 Hesse 
矩阵 是 一个 二维 矩阵 如果 f x 是 一个 
向 量值 函数 那么 Jacobi 矩阵 是 一个 二维 矩阵 
Hesse 矩阵 是 一个 三维 矩阵 b 梯度 是 Jacobian 
矩阵 的 特例 梯度 的 jacobian 矩阵 就是 Hesse 矩阵 
一 阶 偏 导 与 二阶 偏 导 的 关系 
3 . 优化 方法 1 Gradient DescentGradient descent 又叫 steepest 
descent 是 利用 一 阶 的 梯度 信息 找到 函数 
局部 最优 解的/nr 一种 方法 也是 机器学习 里面 最 简单 
最 常用 的 一种 优化 方法 Gradient descent 是 line 
search 方法 中 的 一种 主要 迭代 公式 如下 其中 
是 第 k 次 迭代 我们 选择 移动 的 方向 
在 steepest descent 中 移动 的 方向 设定 为 梯度 
的 负 方向 是 第 k 次 迭 代用 line 
search 方法 选择 移动 的 距离 每次 移动 的 距离 
系数 可以 相同 也 可以 不同 有时候 我们 也 叫 
学习率 learning rate 在 数学 上 移动 的 距离 可以 
通过 line search 令 导数 为零 找到 该 方向 上 
的 最小值 但是 在 实际 编程 的 过程 中 这样 
计算 的 代价 太大 我们 一般 可以 将 它 设定 
位 一个 常量 考虑 一个 包含 三个 变量 的 函数 
计算 梯度 得到 设定 learning rate = 1 算法 代码 
如下 # Code from Chapter 11 of Machine Learning An 
Algorithmic Perspective # by Stephen Marsland http / / seat 
. massey . ac . nz / personal / s 
. r . marsland / MLBook . html # Gradient 
Descent using steepest descent from numpy import * def Jacobian 
x return array x 0 0.4 * x 1 1.2 
* x 2 def steepest x0 i = 0 iMax 
= 10 x = x0 Delta = 1 alpha = 
1 while i iMax and Delta 10 * * 5 
p = Jacobian x xOld = x x = x 
+ alpha * p Delta = sum x xOld * 
* 2 print epoch i print x \ n i 
+ = 1 x0 = array 2 2 2 steepest 
x0 View CodeSteepest gradient 方法 得到 的 是 局部 最优 
解 如果 目标函数 是 一个 凸 优化 问题 那么 局部 
最优 解 就是 全局 最优 解 理想 的 优化 效 
果如 下图 值得 注意 一点 的 是 每一次 迭代 的 
移动 方向 都与 出发点 的 等高线 垂直 需要 指出 的 
是 在 某些 情况下 最速 下 降法 存在 锯齿 现象 
zig zagging 将 会 导致 收敛 速度 变慢 粗略 来讲 
在 二次函数 中 椭球面 的 形状 受 hesse 矩阵 的 
条件 数 影响 长轴 与 短 轴 对应 矩阵 的 
最小 特征值 和 最大 特征值 的 方向 其 大小 与 
特征值 的 平方根 成反比 最大 特征值 与 最小 特征值 相差 
越大 椭球面 越扁/nr 那么 优化 路径 需要 走 很大 的 
弯路 计算 效率 很低 2 Newton s method 在 最速 
下 降法 中 我们 看到 该 方法 主要 利用 的 
是 目标 函数 的 局部 性质 具有 一定 的 盲目性 
牛顿 法则 是 利用 局部 的 一 阶 和 二阶 
偏 导 信息 推测 整个 目标函数 的 形状 进而 可 
以求 得出 近似 函数 的 全局 最小值 然后 将 当前 
的 最小值 设定 近似 函数 的 最小值 相比 最速 下 
降法 牛顿 法 带有 一定 对 全局 的 预测 性 
收敛 性质 也 更 优良 牛顿 法的/nr 主要 推导 过程 
如下 第一步 利用 Taylor 级数 求得 原 目标函数 的 二阶 
近似 第二步 把   x 看做 自变量 所有 带有 x 
^ k 的 项 看做 常量 令 一 阶 导数 
为 0 即 可求 近似 函数 的 最小值 即 第三步 
将 当前 的 最小值 设定 近似 函数 的 最小值 或者 
乘以 步长 与 1 中 优化 问题 相同 牛顿 法的/nr 
代码 如下 Newton . py # Code from Chapter 11 
of Machine Learning An Algorithmic Perspective # by Stephen Marsland 
http / / seat . massey . ac . nz 
/ personal / s . r . marsland / MLBook 
. html # Gradient Descent using Newton s method from 
numpy import * def Jacobian x return array x 0 
0.4 * x 1 1.2 * x 2 def Hessian 
x return array 1 0 0 0 0.4 0 0 
0 1.2 def Newton x0 i = 0 iMax = 
10 x = x0 Delta = 1 alpha = 1 
while i iMax and Delta 10 * * 5 p 
= dot linalg . inv Hessian x Jacobian x xOld 
= x x = x + alpha * p Delta 
= sum x xOld * * 2 i + = 
1 print x x0 = array 2 2 2 Newton 
x0 View Code 上面 例子 中 由于 目标函数 是 二次 
凸函数 Taylor 展开 等于 原函数 所以 能 一次 就 求出 
最优 解 牛顿 法 主要 存在 的 问题 是 Hesse 
矩阵 不可逆 时 无法 计算 矩阵 的 逆 计算 复杂 
为 n 的 立方 当 问题 规模 比较 大 时 
计算 量 很大 解决 的 办法 是 采用 拟 牛顿 
法如 BFGS L BFGS DFP Broyden s Algorithm 进行 近似 
如果 初始值 离 局部 极小值 太远 Taylor 展开 并 不能 
对 原函数 进行 良好 的 近似 3 Levenberg – Marquardt 
A l g o r i t h m L 
e v e n b e r g – Marquardt 
algorithm 能 结合 以上 两种 优化 方法 的 优点 并对 
两者 的 不足 做出 改进 与 line search 的 方法 
不同 LMA 属于 一种 信赖 域 法 trust region 牛顿 
法 实际上 也 可以 看做 一种 信赖 域 法 即 
利用 局部 信息 对 函数 进行 建模 近似 求取 局部 
最小值 所谓 的 信赖 域 法 就是 从 初始 点 
开始 先 假设 一个 可以 信赖 的 最大 位移 s 
牛顿 法 里面 s 为 无穷大 然后 在 以当 前点 
为中心 以 s 为 半径 的 区域 内 通过 寻找 
目标 函数 的 一个 近似 函数 二次 的 的 最优 
点 来 求解 得到 真正 的 位移 在 得到 了 
位移 之后 再 计算 目标 函数值 如果 其 使 目标 
函数值 的 下降 满足 了 一定 条件 那么 就 说明 
这个 位移 是 可靠 的 则 继续 按此 规则 迭代计算 
下去 如果 其 不能 使 目标 函数值 的 下降 满足 
一定 的 条件 则应 减小 信赖 域 的 范围 再 
重新 求解 LMA 最早 提出 是 用来 解决 最 小二 
乘法 曲线拟合 的 优化 问题 的 对于 随机 初始化 的 
已知 参数 beta 求得 的 目标 值 为 对 拟合 
曲线 函数 进行 一 阶 Jacobi 矩阵 的 近似 进而 
推 测出 S 函数 的 周边 信息 位移 是 多少 
时 得到 S 函数 的 最小值 呢 通过 几何 的 
概念 当 残差 垂直于 J 矩阵 的 span 空 间时 
S 取得 最小 至于 为什么 请 参考 之前 博客 的 
最后 一 部分 我们 将 这个 公式 略加修改 加入 阻尼系数 
得到 就是 莱文 贝格 － 马 夸特 方法 这种 方法 
只 计算 了 一 阶 偏 导 而且 不是 目标函数 
的 Jacobia 矩阵 而是 拟合 函数 的 Jacobia 矩阵 当 
大 的 时候 可信 域 小 这种 算法 会 接近 
最速 下 降法 小 的 时候 可信 域 大 会 
接近 高斯 牛顿 方法 算法 过程 如下 给定 一个 初识 
值 x0 当 并且 没有 到达 最大 迭代 次数 时 
重复 执行 算出 移动 向 量计算 更新 值 计算 目标函数 
真实 减少量 与 预测 减少量 的 比率 if 接受 更新 
值 else if 说明 近似 效果 很好 接受 更新 值 
扩大 可信 域 即 减小 阻尼系数 else 目标函数 在 变大 
拒绝 更新 值 减小 可信 域 即 增加 阻尼系数 直到 
达到 最大 迭代 次数 维基百科 在 介绍 Gradient descent 时用/nr 
包含 了 细长 峡谷 的 Rosenbrock function 展示 了 zig 
zagging 锯齿 现象 用 LMA 优化 效率 如何 套用 到 
我们 之前 LMA 公式 中 有 代码 如下 L e 
v e n b e r g M a r 
q u a r d t . py # Code 
from Chapter 11 of Machine Learning An Algorithmic Perspective # 
by Stephen Marsland http / / seat . massey . 
ac . nz / personal / s . r . 
marsland / MLBook . html # The Levenberg Marquardt algorithm 
from numpy import * def function p r = array 
10 * p 1 p 0 * * 2 1 
p 0 fp = dot transpose r r # = 
100 * p 1 p 0 * * 2 * 
* 2 + 1 p 0 * * 2 J 
= array 20 * p 0 10 1 0 grad 
= dot transpose J transpose r return fp r grad 
J def lm p0 tol = 10 * * 5 
maxits = 100 nvars = shape p0 0 nu = 
0.01 p = p0 fp r grad J = function 
p e = sum dot transpose r r nits = 
0 while nits maxits and linalg . norm grad tol 
nits + = 1 fp r grad J = function 
p H = dot transpose J J + nu * 
eye nvars pnew = zeros shape p nits2 = 0 
while p = pnew . all and nits2 maxits nits2 
+ = 1 dp resid rank s = linalg . 
lstsq H grad pnew = p dp fpnew rnew gradnew 
Jnew = function pnew enew = sum dot transpose rnew 
rnew rho = linalg . norm dot transpose r r 
dot transpose rnew rnew rho / = linalg . norm 
dot transpose grad pnew p if rho 0 update = 
1 p = pnew e = enew if rho 0.25 
nu = nu / 10 else nu = nu * 
10 update = 0 print fp p e linalg . 
norm grad nu p0 = array 1.92 2 lm p0 
View Code 大概 5 次 迭代 就 可以 得到 最优 
解 1 1 . Levenberg – Marquardt algorithm 对 局部 
极小值 很 敏感 维基百 科举 了 一个 二 乘法 曲线拟合 
的 例子 当 使用 不同 的 初始值 时 得到 的 
结果 差距 很大 我 这里 也有 python 代码 就不 细说 
了 4 Conjugate Gradients 共轭 梯度 法 也是 优化 模型 
经常 经常 要 用到 的 一个 方法 背后 的 数学 
公式 和 原理 稍微 复杂 一些 光 这一个 优化 方法 
就 可以 写 一篇 很长 的 博文 了 所以 这里 
并不 打算 详细 讲解 每一步 的 推导 过程 只 简单 
写 一下 算法 的 实现 过程 与 最速 梯度 下降 
的 不同 共轭 梯度 的 优点 主要 体现 在 选择 
搜索 方向 上 在 了解 共轭 梯度 法 之前 我们 
首先 简单 了解 一下 共轭 方向 共轭/n 方向/n 和/c 马氏/nr 
距离/n 的/uj 定义/n 有/v 类似之处/l 他们 都 考虑 了 全局 
的 数据分布 如上图 d 1 方向 与 二次函数 的 等值线 
相切 d 1 的 共轭 方向 d 2 则 指向 
椭圆 的 中心 所以 对于 二维 的 二次函数 如果 在 
两个 共轭 方向 上 进行 一维 搜索 经过 两次 迭代 
必然 达到 最 小点 前面 我们 说过 等值线 椭圆 的 
形状 由 Hesse 矩阵 决定 那么 上图 的 两个 方向 
关于 Hessen 矩阵 正交 共轭 方向 的 定义 如下 如果 
椭圆 是 一个 正圆 Hessen 矩阵 是 一个 单位矩阵 上面 
等价 于 欧几里得 空间 中的 正交 在 优化 过程 中 
如果 我们 确定 了 移动 方向 GD 垂直于 等值线 CG 
共轭 方向 然后 在 该 方向 上 搜索 极小值 点 
恰好 与 该处 的 等值线 相切 然后 移动 到 最小值 
点 重复 以上 过程 那么 Gradient Descent 和 Conjugate gradient 
descent 的 优化 过程 可以 用 下图 的 绿线 与 
红线 表示 讲 了 这么 多 共轭 梯度 算法 究竟 
是 如何 算 的 呢 给定 一个 出发点 x0 和 
一个 停止 参数 e 第一 次 移动 方向 为 最速 
下降 方向 while 用 Newton Raphson 迭代计算 移动 距离 以便 
在 该 搜索 方向 移动 到 极小 公式 就不 写了 
具体 思路 就是 利用 一 阶 梯度 的 信息 向 
极小值 点 跳跃 搜索 移动 当前 的 优化 解 x 
用 Gram Schmidt 方法 构造 下一个 共轭 方向 即 按照 
的 确定 公式 又 可以 分为 FR 方法 和 PR 
和 HS 等 在 很多 的 资料 中 介绍/v 共轭/n 
梯度/n 法/l 都举了/nr 一个/m 求/v 线性方程组/n Ax = b 近似解 
的 例子 实际上 就 相当于 这里 所说 的 还是 用 
最 开始 的 目标 函数       来 编写 
共轭 梯度 法的/nr 优化 代码 # Code from Chapter 11 
of Machine Learning An Algorithmic Perspective # by Stephen Marsland 
http / / seat . massey . ac . nz 
/ personal / s . r . marsland / MLBook 
. html # The conjugate gradients algorithm from numpy import 
* def Jacobian x # return array . 4 * 
x 0 2 * x 1 return array x 0 
0.4 * x 1 1.2 * x 2 def Hessian 
x # return array . 2 0 0 1 return 
array 1 0 0 0 0.4 0 0 0 1.2 
def CG x0 i = 0 k = 0 r 
= Jacobian x0 p = r betaTop = dot r 
. transpose r beta0 = betaTop iMax = 3 epsilon 
= 10 * * 2 jMax = 5 # Restart 
every nDim iterations nRestart = shape x0 0 x = 
x0 while i iMax and betaTop epsilon * * 2 
* beta0 j = 0 dp = dot p . 
transpose p alpha = epsilon + 1 * * 2 
# Newton Raphson iteration while j jMax and alpha * 
* 2 * dp epsilon * * 2 # Line 
search alpha = dot Jacobian x . transpose p / 
dot p . transpose dot Hessian x p print N 
R x alpha p x = x + alpha * 
p j + = 1 print x # Now construct 
beta r = Jacobian x print r r betaBottom = 
betaTop betaTop = dot r . transpose r beta = 
betaTop / betaBottom print Beta beta # Update the estimate 
p = r + beta * p print p p 
print k + = 1 if k = = nRestart 
or dot r . transpose p = 0 p = 
r k = 0 print Restarting i + = 1 
print x x0 = array 2 2 2 CG x0 
View Code 参考资料 1 Machine Learning An Algorithmic Perspective chapter 
11 2 最优化 理论 与 算法 第 2版 陈 宝林 
3 wikipedia 