1 . Classification 这篇文章 我们 来 讨论 分类 问题 classification 
problems 也 就是说 你 想 预测 的 变量 y 是 
一个 离散 的 值 我们 会 使用 逻辑 回归 算法 
来 解决 分类 问题 之前 的 文章 中 我们 讨论 
的 垃圾 邮件 分类 实际上 就是 一个 分 类 问题 
类似 的 例子 还有 很多 例如 一个 在线 交易 网站 
判断 一次 交易 是否 带有 欺诈性 有些 人 可以 使用 
偷来 的 信用卡 你 懂 的 再如 之前 判断 一个 
肿瘤 是 良性 的 还是 恶性 的 也 是 一个 
分 类 问题 在 以上 的 这些 例子 中 我们 
想 预测 的 是 一个 二 值 的 变量 或者 
为 0 或者 为 1 或者 是 一封 垃圾邮件 或者 
不是 或者 是 带有 欺诈性 的 交易 或者 不是 或者 
是 一个 恶性肿瘤 或者 不是 我们 可以 将 因变量 dependant 
variable 可能 属于 的 两个 类 分别 称为 负向 类 
negative class 和 正向 类 positive class 可以 使用 0 
来 代表 负向 类 1 来 代表 正向 类 现在 
我们 的 分类 问题 仅仅 局限 在 两类 上 0 
或者 1 之后 我们 会 讨论 多分 类 问题 也 
就是说 变量 y 可以 取 多个 值 例如 0 1 
2 3 那么 我们 如何 来 解决 一个 分 类 
问题 呢 来看 以下 例子 现在 有 这样 一个 分类 
任务 需要 根据 肿瘤 大小 来 判断 肿瘤 的 良性 
与否 训练 集 如上 图 所示 横轴 代表 肿瘤 大小 
纵轴 表示 肿瘤 的 良性 与否 注意 纵轴 只有 两个 
取值 1 代表 恶性肿瘤 和0/nr 代表 良性肿瘤 通过 之前 的 
博文 我们 已经 知道 对于 以上 数据集 使用 线性 回归 
来 处理 实际上 就是 用 一条 直线 去 拟合 这些 
数据 因此 你 得到 的 Hypothesis 可能 如下 那么 如果 
你 想 做出 预测 一种 可行 的 方式 是 如下 
从 以上 这个 例子 来看 似乎 线性 回归 也 能 
很好 的 解决 分类 问题 现在 我们 对 以上 问题 
稍作 一些 改动 将 横轴 向右 扩展 并且 增加 一个 
训练样本 如下 此时 我们 使用 线性 回归 会 得到 一条 
新的 直线 此时 我们 再用 0.5 作为 阈值 来 预测 
肿瘤 的 良性 与否 就不 合适 了 2 . Hypothesis 
Representation3 . Decision boundary 强调 一下 决策 边界 不 是 
训练 集 的 属性 而是 假设 本身 及其 参数 的 
属性 只要 我们 给 定了 参数 向量 θ 决策 边界 
就 确定 了 我们 不是 用 训练 集 来 定义 
的 决策 边界 我们 用 训练 集 来 拟合 参数 
θ 以后 我们 将 谈论 如何 做到 这 一点 但是 
一旦 你 有 参数 θ 它 就 确定 了 决策 
边界 4 . Cost function1 现在 我们 来 讨论 如何 
拟合 逻辑 回 归中 模型 的 参数 θ 具体来说 我们 
需要 定义 optimization objective 或者 cost function 来 拟合 参数 
θ 这 便是 监督 学习 问题 中 的 逻辑 回归模型 
的 拟合 问题 如上 图 所示 我们 有 一个 训练 
集 里面 有m个/nr 训练样本 同 之前 一样 我们 的 每个 
样本 使用 n + 1 维 的 特征向量 表示 x0 
= 1 并且 由于 是 分类 问题 我们 训练 集中 
的 所有 y 取值 不是 0 就是 1 假设 函数 
的 参数 即为 θ 那么 对于 这个 给定 的 训练 
集 我们 如何 拟合 参数 θ 或者说 是 选择 参数 
θ 之前 我们 使用 线性 回归模型 来 拟合 假说 参数 
θ 时 使用 了 如下 的 代价 函数 我们 稍作 
改变 将 原先 的 1/2 m 中的 原先 的 1/2 
放到 了 求和 符号 里面 去了 现在 我们 使用 另一种 
方式 来 书写 代价 函数 现在 我们 能更/nr 清楚 的 
看到 代价 函数 是 这个 Cost 函数 代价 项 在 
训练 集 范围 上 的 求和 再 求 均值 乘以 
1 / m 我们 稍微 简化 一下 这个 式子 去掉 
这些 上 标会 显得 方便 一些 所以 Cost 函数 直接 
定义 为 对 这个 代价 项 Cost 函数 的 理解 
是 这样 的 y 我 所 期望 的 值 通过学习 
算法 如果 想 要 达到 这个 值 那么 假设 h 
x 所 需要 付出 的 代价 即为 这个 代价 项 
这个 希望 的 预测 值 是 h x 而 实际 
值 则是 y 干脆 全部 去掉 那些 上标 好了 显然 
在 线性 回 归中 代价 项 Cost 函数 会被 定义 
为 1/2 乘以 预测值 h 和 实际 值 观测 的 
结果 y 的 差 的 平方 这个 代 价值 可以 
很好 地 用在 线性 回归 里 但是 对于 逻辑 回归 
却是 不 合适 的 2 如果 我们 可以 最小化 代价 
函数 J θ 中 的 代价 项 Cost 函数 那么 
我们 的确 可以 使用 该 代价 项 但 实际上 如果 
我们 使用 该 代价 项 那么 代价 函数 J θ 
会 变成 关于 参数 θ 的 非 凸函数 Why 对于 
逻辑 回归 来说 这里 的 h 函数 是非 线性 的 
可以 说 是 一个 很 复杂 的 非线性 函数 因此 
如果 用 h 函 数来 构造 我们 在 线性 回归 
中所 使用 的 代价 项 Cost 函数 接着 再用 该 
代价 项来/nr 构造 代价 函数 J θ 那么 J θ 
可能 是 一个 这样 的 函数 有 很多 局部 最优 
值 实际上 这 就是 一个 非 凸函数 不难 发现 如果 
你 把 梯度 下 降法 用 在 一个 这样 的 
函数 上 的话 我们 并 不能 保证 它 会 收敛 
到 全局 最小值 显然 我们 希望 我们 的 代价 函数 
J θ 是 一个 凸函数 也 就是 一个 单 弓形 
函数 如下 图 所示 如果 对 它 使用 梯度 下 
降法 那么 我们 可以 保证 梯度 下降 法会 收敛 到 
该 函数 的 全局 最小值 因此 我们 在 逻辑 回归 
中 使用 这个 代价 项 Cost 函数 的 问题 在于 
非线性 的 sigmoid 函数 的 出现 导致 J θ 成为 
一个 非 凸函数 3 我们 需要 做 的 是 另外 
找 一个 本身 是 凸函数 的 代价 项 Cost 函数 
可以 让 我们 使用 类似 于 梯度 下降 的 算法 
来 找到 一个 全局 最小值 以下 就是 一个 我们 将要 
在 逻辑 回归 中 使用 的 代价 项 Cost 函数 
5 . Simplified cost function and gradient descent 注意 此时 
θ 是 变量 我们 的 目标 就是 找 出使 J 
θ 最小 的 θ 值 