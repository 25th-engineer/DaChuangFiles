一 前述 决策树 是 一种 非线性 有 监督 分类 模型 
随机 森林 是 一种 非线性 有 监督 分类 模型 线性 
分类 模型 比如说 逻辑 回归 可能 会 存在 不 可分 
问题 但是 非线性 分类 就 不存在 二 具体 原理 ID3 
算法 1 相关 术 语根 节点 最 顶层 的 分类 
条件 叶 节点 代表 每 一个 类别 号 中间 节点 
中间 分类 条件 分枝 代表 每 一个 条件 的 输出 
二叉树 每 一个 节点 上 有 两个 分 枝多 叉 
树 每 一个 节点 上 至少 有 两个 分枝 2 
决策树 的 生成 数据 不断 分裂 的 递归 过程 每一次 
分裂 尽可能 让 类别 一样 的 数据 在 树 的 
一边 当 树 的 叶子 节点 的 数据 都是/nr 一类 
的 时候 则 停止 分类 if else 语句 3 如何 
衡量 纯粹 度 举例 箱子 1 100个 红球 箱子 2 
50个 红球   50个 黑球 箱子 3 10个 红球   
30个 蓝球 60 绿球 箱子 4 各个 颜色 均 10个 
球 凭 人 的 直觉 感受 箱子 1 是 最 
纯粹 的 箱子 4 是 最 混乱 的 如何 把人 
的 直觉 感受 进行 量化 呢 将 这种 纯粹 度 
用 数据 进行 量化 计算机 才能 读懂 举例 度量 信息 
混乱 程度 指标 熵 的 介绍 熵 公式 举例 熵 
代表 不确定性 不确定性 越大 熵 越大 代表 内部 的 混乱 
程度 比如 两个 集合   A 有 5个 类别 每个 
类别 2个 值 则 每个 概率 是 0.2 比如 B 
有 两个 类别 每个 类别 5个 则 每个 概率 是 
0.5 显然 0.5 大于 0.2 所以 熵 大 混乱 程度 
比较 大 信息熵 H X 信息熵 是 香农 在 1948年 
提出 来 量化 信息 的 信息量 的 熵 的 定义 
如下 n 代表 种类 每 一个 类别 中 p1 代表 
某个 种类 的 概率 * log 当前 种类 的 概率 
然后 将 各个 类别 计算结果 累加 以上 例子 车祸 的 
信息 熵 是 4/9 l o g 4/9 + 5/9 
l o g 5/9 条件 熵 H X Y 类似于 
条件概率 在 知道 X 的 情况 下 Y 的 不确定性 
以上 例子 在 知道 温度 的 情况 下 求 车祸 
的 概率 hot 是 3个 其中 两个 没 有 车祸 
一个 车祸 则 3/9 2/3 l o g 2/3 + 
1/3 l o g 1/3 mild 是 2个 其中 一个 
车祸 一个 没有 车祸 则 2/9 1/2 l o g 
1/2 + 1/2 l o g 1/2 cool 是 4个 
其中 三个 车祸 一个 没有 车祸 则 4/9 3/4 l 
o g 3/4 + 1/4 l o g 1/4 以上 
相加 即为 已知 温度 的 情况 下 车祸 的 条件 
熵 信息 增益 代表 的 熵 的 变化 程度 特征 
Y 对 训练 集 D 的 信息 增益 g D 
Y = H X H X Y 以上 车祸 的 
信息 熵 已知 温度 的 条件 熵 就是 信息 增益 
信息 增益 即是 表示 特征 X 使得 类 Y 的 
不确定性 减少 的 程度 分类 后的/nr 专一性 希望 分类 后的/nr 
结果 是 同类 在 一起 信息 增益 越大 熵 的 
变化 程度 越大 分 的 越 干脆 越 彻底 不确定性 
越小 在 构建 决策树 的 时候 就是 选择 信息 增益 
最大 的 属性 作为 分裂 条件 ID3 使得 在 每个 
非 叶子 节点 上 进行 测试 时 都能 获得 最大 
的 类别 分类 增益 使 分类 后 数据集 的 熵 
最小 这样 的 处理 方法 使得 树 的 平均 深度 
较小 从而 有效 提高 了 分类 效率 C 4.5 算法 
有时候 给 个 特征 它 分 的 特别 多 但是 
完 全分 对了 比如 训练 集 里面 的 编号 信息 
增益 特别 大 都 甚至 等 于根 节 点了 那 
肯定 是 不 合适 的 问题 在于 行 编号 的 
分类 数目 太多 了 分类 太多 意味着 这个 特征 本身 
的 熵 大 大 到 都快是/nr 整个 H X 了 
为了 避免 这种 现象 的 发生 我们 不是 用 信息 
增益 本身 而是 用 信息 增益 除以 这个 特征 本身 
的 熵值 看 除 之后 的 值 有 多大 这 
就是 信息 增益 率 如果 用 信息 增益 率 就是 
C4 . 5CART 算法 CART 使用 的 是 GINI 系数 
相比 ID3 和C/nr 4.5 CART 应用 要 多一些 既 可以 
用于 分类 也 可以 用于 回归 CART 假设 决策树 是 
二叉树 内部 结点 特征 的 取值 为 是 和 否 
左 分支 是 取值 为 是 的 分支 右 分支 
是 取值 为 否 的 分支 这样 的 决策树 等价 
于 递归 地 二分 每个 特征 将 输入 空间 即 
特征 空间 划分 为 有限 个 单元 并 在 这些 
单元 上 确定 预测 的 概率分布 也 就是 在 输入 
给定 的 条件 下 输出 的 条件 概率分布 CART 算法 
由 以下 两步 组成 决策树 生成 基于 训练 数据集 生成 
决策树 生成 的 决策 树 要尽 量大 决策树 剪枝 用 
验证 数据集 对 已 生成 的 树 进行 剪枝 并 
选择 最优 子树 这时 损失 函数 最小 作为 剪枝 的 
标准 CART 决策树 的 生成 就是 递归 地 构建 二叉 
决策树 的 过程 CART 决策树 既 可以 用于 分类 也 
可以 用于 回归 本文 我们 仅 讨论 用于 分类 的 
CART 对 分类 树 而言 CART 用 Gini 系数 最小化 
准则 来 进行 特征选择 生成 二叉树 GINI 系数 其实 是 
用 直 线段 代替 曲 线段 的 近似 GINI 系数 
就是 熵 的 一 阶 近似 公式 如下 比如 两个 
集合   A 有 5个 类别 每个 类别 2个 值 
则 每个 概率 是 0.2 比如 B 有 两个 类别 
每个 类别 5个 则 每个 概率 是 0.5 假设 C 
有 一个 类别 则 基尼 系数 是 0 类别 越多 
基尼系数/nz 越/d 接近/v 于1/nr 所以 我们 希望 基尼系数 越小 越好 
CART 生成 算法 如下 输入 训练 数据集 停止 计算 的 
条件 输出 CART 决策树 过程 损失 函数 以上 构 建好 
分类 树 之后 评价 函数 如下 希望 它 越小 越好 
类似 损失 函 数了 三 解决 过拟合 问题 方法 1 
背景 叶子 节点 的 个数 作为 加权 叶子 节点 的 
熵 乘以 加权 的 加 和 就是 评价 函数 这就是 
损失 函数 这个 损失 函数 肯定 是 越小越 好了 如何 
评价 呢 用 训练 数据 来 计算 损失 函数 决策树 
不断 生长 的 时候 看看 测试数据 损失 函数 是不是 变得 
越 低了 这 就是 交互式 的 做 调 参 的 
工作 因为 我们 可能 需要 做 一些 决策 树叶子 节点 
的 剪枝 因为 并 不是 树 越高 越好 因为 树 
如果 非常 高 的话 可能 过拟合 了 2 解决 过拟合 
两种 方法 剪枝 随机 森林 3 解决 过拟合 方法 之 
剪枝 为什么 要 剪枝 决策树 过拟合 风险 很大 理论 上 
可以 完全 分得开 数据 想象 一下 如 果树 足够 庞大 
每个 叶子 节点 不就 一个 数据 了嘛 剪枝 策略 预 
剪枝 后 剪枝 预 剪枝 边/d 建立/v 决策树/n 边/d 进行/v 
剪枝/n 的/uj 操作/v 更 实用 后 剪枝 当 建立 完 
决策树 后来 进行 剪枝 操作 预 剪枝 用 的 多 
边/d 生成树/i 的/uj 时候/n 边/d 剪枝/n 限制 深度 叶子 节点 
个数 叶子 节点 样本数 信息 增益 量等 树 的 高度 
每个 叶 节点 包含 的 样本 最小 个数 每个 叶 
节点 分裂 的 时候 包含 样本 最小 的 个数 每个 
叶 节点 最小 的 熵值 等 max _ depth min 
_ sample _ split min _ sample _ leaf min 
_ weight _ fraction _ leaf max _ leaf _ 
nodes max _ features 增加 min _ 超 参 减小 
max _ 超 参 后 剪枝 叶子 节点 个数 越多 
损失 越大 还是 生成 一颗 树 后 再去 剪枝 alpha 
值 给定 就行 后 剪枝 举例 4 解决 过拟合 方法 
之 随机 森林 思想 Bagging 的 策略 从 样本 集中 
重 采样 有 可能 存在 重复 选出 n 个 样本 
在 所有 属性 上 对这 n 个 样本 建立 分类器 
ID3 C 4.5 CART SVM Logistic 回归 等 重复 上面 
两步 m 次 产生 m 个 分类器 将 待 预测 
数据 放 到这 m 个 分类器 上 最后 根据 这 
m 个 分类器 的 投票 结果 决定 待 预测 数据 
属于 那 一类 即 少数 服从 多数 的 策略 在/p 
Bagging/w 策略/n 的/uj 基础/n 上/f 进行/v 修改/v 后的/nr 一种/m 算法/n 
从样/nr 本集/i 中用/n Bootstrap/w 采样/v 选出/v n/w 个/q 样本/n 从 
所有 属性 中 随机 选择 K 个 属性 选择 出 
最佳 分割 属性 作为 节点 创建 决策树 重复 以上 两步 
m 次 即 建立 m 棵 CART 决策树 这 m 
个 CART 形成 随机 森林 样本 随机 属性 随机 通过 
投票 表决 结果 决定 数据 属于 那 一类 当 数据集 
很大 的 时候 我们 随机 选取 数据集 的 一部分 生成 
一棵树 重复 上述 过程 我们 可以 生成 一堆 形态各异 的 
树 这些 树 放在 一起 就叫 森林 随机 森林 之所以 
随机 是 因为 两 方面 样本 随机 + 属性 随机 
选取 过程 取 某些 特征 的 所有 行 作为 每一个 
树 的 输入 数据 然后 把 测试数据 带入 到 每一个 
数 中 计算结果 少数 服从 多数 即可 求出 最终 分类 
随机 森林 的 思考 在 随机 森林 的 构建 过程 
中 由于 各 棵树 之间 是 没有 关系 的 相对 
独立 的 在 构建 的 过程 中 构建 第 m 
棵 子树 的 时候 不会 考虑 前面 的 m 1 
棵树 因此 引出 提升 的 算法 对分 错 的 样本 
加权 提升 是 一种 机器学习 技术 可以 用于 回归 和 
分类 的 问题 它 每一步 产生 弱 预测模型 如 决策树 
并 加权 累 加到 总 模型 中 如果 每 一步 
的 弱 预测模型 的 生成 都是 依据 损失 函数 的 
梯度 方式 的 那么 就 称为 梯度 提升 Gradient boosting 
提升 技术 的 意义 如果 一个 问题 存在 弱 预测模型 
那么 可以 通过 提升 技术 的 办法 得到 一个 强 
预测模型 四 代码 决策树 决策树 的 训练 集 必须 离散化 
因为 如果 不 离散化 的话 分类 节点 很多 package com 
. bjsxt . rf import org . apache . spark 
. mllib . tree . DecisionTree import org . apache 
. spark . mllib . util . MLUtils import org 
. apache . spark . { SparkContext SparkConf } object 
C l a s s i f i c a 
t i o n D e c i s i 
o n T r e e { val conf = 
new SparkConf conf . setAppName analysItem conf . setMaster local 
3 val sc = new SparkContext conf def main args 
Array String Unit = { val data = MLUtils . 
loadLibSVMFile sc 汽车 数据 样本 . txt / / Split 
the data into training and test sets 30% held out 
for testing val splits = data . randomSplit Array 0.7 
0.3 val trainingData testData = splits 0 splits 1 / 
/ 指明 类别 val numClasses = 2 / / 指定 
离散 变量 未 指明 的 都 当作 连续变量 处理 / 
/ 1 2 3 4 维度 进来 就 变成 了 
0 1 2 3 / / 这里 天气 维度 有 
3类 但是 要 指明 4 这里 是个 坑 后面 以此类推 
val c a t e g o r i c 
a l F e a t u r e s 
I n f o = Map Int Int 0 4 
1 4 2 3 3 3 / / 设定 评判 
标准 val impurity = entropy / / 树 的 最大 
深度 太深 运算 量大 也 没有 必要 剪枝 防止 模型 
的 过拟合 val maxDepth = 3 / / 设置 离散化 
程度 连续 数据 需要 离散化 分成 32个 区间 默认 其实 
就是 32 分割 的 区间 保证 数量 差不多 这里 可以 
实现 把 数据 分到 0 31 这些 数 中去 这个 
参数 也 可以 进行 剪枝 val maxBins = 32 / 
/ 生成 模型 val model = DecisionTree . trainClassifier trainingData 
numClasses c a t e g o r i c 
a l F e a t u r e s 
I n f o impurity maxDepth maxBins / / 测试 
val labelAndPreds = testData . map { point = val 
prediction = model . predict point . features point . 
label prediction } val testErr = labelAndPreds . filter r 
= r . _ 1 = r . _ 2 
. count . toDouble / testData . count / / 
错误率 的 统计 println Test Error = + testErr println 
Learned classification tree model \ n + model . toDebugString 
} } 样本数据 将 第 5列 数据 离散化 结果 深度 
为 3 一共 15个 节点 随机 森林 package com . 
bjsxt . rf import org . apache . spark . 
{ SparkContext SparkConf } import org . apache . spark 
. mllib . util . MLUtils import org . apache 
. spark . mllib . tree . RandomForest object C 
l a s s i f i c a t 
i o n R a n d o m F 
o r e s t { val conf = new 
SparkConf conf . setAppName analysItem conf . setMaster local 3 
val sc = new SparkContext conf def main args Array 
String Unit = { / / 读取数据 val data = 
MLUtils . loadLibSVMFile sc 汽车 数据 样本 . txt / 
/ 将 样本 按 7 3 的 比例 分成 val 
splits = data . randomSplit Array 0.7 0.3 val trainingData 
testData = splits 0 splits 1 / / 分类 数 
val numClasses = 2 / / c a t e 
g o r i c a l F e a 
t u r e s I n f o 为 
空 意味着 所有 的 特征 为 连续型 变量 val c 
a t e g o r i c a l 
F e a t u r e s I n 
f o = Map Int Int 0 4 1 4 
2 3 3 3 / / 树 的 个数 val 
numTrees = 3 / / 特征 子集 采样 策略 auto 
表示 算法 自主 选取 / / auto 根据 特征 数量 
在 4个 中 进行 选择 / / 1 all 全部 
特征 2 sqrt 把 特征 数量 开 根号 后 随机 
选择 的 3 log2 取 对 数个 4 onethird 三分之一 
val f e a t u r e u b 
s e t t r a t e g y 
= auto / / 纯度 计算 val impurity = entropy 
/ / 树 的 最大 层次 val maxDepth = 3 
/ / 特征 最大 装箱 数 即 连续 数据 离散化 
的 区间 val maxBins = 32 / / 训练 随机 
森林 分类器 trainClassifier 返回 的 是 R a n d 
o m F o r e s t M o 
d e l 对象 val model = RandomForest . trainClassifier 
trainingData numClasses c a t e g o r i 
c a l F e a t u r e 
s I n f o numTrees f e a t 
u r e u b s e t t r 
a t e g y impurity maxDepth maxBins / / 
打印 模型 println model . toDebugString / / 保存 模型 
/ / model . save sc 汽车保险 / / 在 
测试 集上 进行 测试 val count = testData . map 
{ point = val prediction = model . predict point 
. features / / Math . abs prediction point . 
label prediction point . label } . filter r = 
r . _ 1 = r . _ 2 . 
count println Test Error = + count . toDouble / 
testData . count . toDouble } } 结果 根据 DEBUGTREE 
画图 举例 