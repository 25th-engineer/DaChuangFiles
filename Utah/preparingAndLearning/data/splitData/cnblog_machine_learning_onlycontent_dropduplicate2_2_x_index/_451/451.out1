基础 概念 XGBoost eXtreme Gradient Boosting 是 GradientBoosting 算法 的 
一个 优化 的 版本 针对 传统 GBDT 算法 做 了 
很多 细节 改进 包括 损失 函数 正则化 切 分点 查找 
算法 优化 等 xgboost 的 优化 点 相对于 传统 的 
GBM XGBoost 增加 了 正则 化步骤 正则化 的 作用 是 
减少 过拟合 现象 xgboost 可以 使 用 随机 抽取 特征 
这个 方法 借鉴 了 随机 森林 的 建模 特点 可以 
防止 过拟合 速度 上 有 很好 的 优化 主要 体现 
在 以下 方面 1 现 了 分裂 点 寻找 近似算法 
先 通过 直方图 算法 获得 候选 分割 点 的 分布 
情况 然后 根据 候选 分割 点将 连续 的 特征 信息 
映射 到 不同 的 buckets 中 并 统计 汇总 信息 
2 xgboost 考虑 了 训练 数据 为 稀疏 值 的 
情况 可以为 缺失 值 或者 指定 的 值 指定 分支 
的 默认 方向 这 能 大大 提升 算法 的 效率 
3 正常 情况 下 Gradient Boosting 算法 都是 顺序 执行 
所以 速度 较慢 xgboost 特征 列 排序 后以 块 的 
形式 存储 在 内存 中 在 迭代 中 可以 重复 
使用 因而 xgboost 在 处理 每个 特征 列 时 可以 
做到 并行 总的来说 xgboost 相对于 GBDT 在 模型 训练 速度 
以及 在 降低 过拟合 上 不少 的 提升 XGBOOST 原理 
基于 不同 y 的 理解 我们 可以 有 不同 的 
问题 如 回归 分类 排序 等 我们 需要 找到 一种 
方法 来 找到 最好 的 参数 给定 的 训练 数据 
为了 达到 这个 目的 我们 需要 定义 一个 目标函数 用 
它 来 测量 模型 的 性能 目标函数 包含 两 部分 
损失 函数 + 正则 项 其中 L 为 损失 函数 
Ω 是 正则 项 在 预测 数据 时 常用 的 
损失 函数 比如 均方根 误差 另外 一个 常用 的 损失 
函数 是 logistic 损失 函数 其中 我们 是 通关 不断 
优化 降低 损失 函数 来 提高 模型 的 性能 另外 
正则 项的/nr 主要 作用 是 防止 模型 的 过拟合 现象 
对于 xgboost 而言 它 的 目标 函数 可以 写成 xgboost 
比 传统 解决 最 优化 问题 常用 的 梯度 迭代 
方法 要跟 难 它 并 不是 一次性 训练 所有 树 
而是 采用 一种 加法 策略 一次 添加 一个 树 因此 
步骤 t 的 预测 值 y 可以 写 根据 上面 
的 推理 过程 我们 可以 把 目标函数 写成 如果 考虑 
使用 均方 误差 作为 损失 函数 的 最终 可以 使用 
的 目标 函数 可写 成对 以上 目标函数 进行 二阶 泰勒 
展开 得到 以下 公式 其中 gi 和 hi 被 定义 
为 最终 的 目标 函数 表示 形式 为 从中 可以 
看到 xgboost 的 最优 化 问题 将 取决于 目标函数 过程 
中 gi 和 hi 取值 补充 理解 补充 理解 部分 
主要 是 对 上面 提到 的 正则化 和 泰勒 展开式 
做 补充 解释 泰勒 展开式 泰勒公式 是 一个 用 函数 
在某 点 的 信息 描述 其 附近 取值 的 公式 
如果 函数 足够 平滑 的话 在 已知 函数 在某 一点 
的 各阶 导 数值 的 情况 之下 泰勒公式 可以 用 
这些 导 数值 做 系数 构建 一个 多项式 来 近似 
函数 在 这 一点 的 邻域 中的 值 泰勒公式 还给 
出 了 这个 多项式 和 实际 的 函数值 之间 的 
偏差 在 机器 学习 中 使用 泰勒 展开式 的 目的 
是 通过 泰勒 展开式 函数 的 局部 近似 特性 来 
简化 复杂 函数 的 表达 例如 函数 y = x 
^ 3 当 自变量 有 变化 时 即 △ x 
自变量 y 会 变化 △ y 带入 到 函数 里面 
就有 当 △ x 0时 上式 的 后 两项 是 
△ x 的 高阶 无穷小 舍去 的话 上式 就 变成 
了 也就是说 当 自变量 x 足够 小 的 时候 也 
就是 在某 点 的 很小 的 邻 域内 Δ y 
是 可以 表示 成Δx/nr 的 线性 函数 的 线性函数 计算起来 
求导 起来 会 很 方便 对于 一般函数 当 在某 点 
很小 领域 内 我们 也 可以 写成 类似 上面 的 
这种 自变量 和 因变量 之间 线性关系 变化 一下 形式 Δ 
y = f x f x0 Δ x = x 
x0 在 代入 上式 这个 就是 在 x 0点 邻 
域内 舍掉 高阶 无穷 小项 以后 得到 的 局部 线性 
近似 公式 了 为了 提高 近似 的 精确度 于是 把 
上面 的 一次 近似 多项式 修正 为 二次 多项式 再进一步 
二次 修正 为 三次 一直 下去 就 得到 n 阶 
泰勒 多项式 只做 一次 近似 的 公式 如下 近似 的 
多项式 和 原始 函数 是 通过 同 一点 x0 进行 
二次 近似 的 公式 如下 近似 的 多项式 和 原始 
函数 既 过 同 一点 而且 在 同一 点 的 
导数 相同 也 就是 多项式 表达 的 函数 在 x 
0点 的 切线 也 相同 最终 n 阶 泰勒 展开 
公式 展开 越多 近似 程度 越高 L1/i 正则化/i 和/c L2/i 
正则化/i L1/i 正则化/i 和/c L2/i 正则化/i 可以/c 看做/v 是/v 损失/n 
函数/n 的/uj 惩罚/vn 项/n 所谓 惩罚 是 指 对 损失 
函数 中的 某些 参数 做 一些 限制 对于 线性 回归模型 
使用 L1 正则化 的 模型 建 叫做 Lasso 回归 使用 
L2 正则化 的 模型 叫做 Ridge 回归 岭回归 通常 情况下 
L1 正则化 是 在 损失 函数 上 加入 一项 α 
| | w | | 1 L2 正则化 则是 在 
损失 函数 上 加入 一项 α | | w | 
| 22 一般 回归分析 中 回归 w 表示 特征 的 
系数 从/p 上式/b 可以/c 看到/v 正则化/i 项是对/nr 系数/n 做了/i 处理/v 
限制 L1 正则化 和 L2 正则化 的 说明 如下 L1 
正则化 是 指 权值 向量 w 中 各个 元素 的 
绝对值 之和 通常 表示 为 | | w | | 
1 L1 正则化 可以 产生 稀疏 权值 矩阵 即 产生 
一个 稀疏 模型 可以 用于 特征选择 L2 正则化 是 指 
权值 向量 w 中 各个 元素 的 平方和 然后再 求 
平方根 通常 表示 为 | | w | | 2 
L2 正则化 可以 防止 模型 过拟合 overfitting 一定 程度 上 
L1 也 可以 防止 过拟合 L1 正则化 的 作用 阐述 
L1 正则化 有助于 生成 一个 稀疏 权值 矩阵 稀疏 矩阵 
指 的 是 很多 元素 为 0 只有 少数 元素 
是非 零值 的 矩阵 即 得到 的 线性 回归模型 的 
大部分 系数 都是 0 . 进而 可以 用于 特征选择 通常 
机器学习 中 特征 数量 很多 例如 文本处理 时 如果 将 
一个 词组 term 作为 一个 特征 那么 特征 数量 会 
达到 上万个 bigram 在 预测 或 分类 时 那么 多 
特征 显然 难以 选择 但是 如果 代入 这些 特征 得到 
的 模型 是 一个 稀疏 模型 表示 只有 少数 特征 
对 这个 模型 有 贡献 绝大部分 特征 是 没有 贡献 
的 或者 贡献 微小 因为 它们 前面 的 系数 是 
0 或者 是 很小 的 值 即使 去掉 对模型 也 
没有 什么 影响 此时 我们 就 可以 只 关注 系数 
是非 零值 的 特征 这 就是 稀疏 模型 与 特征 
选择 的 关系 L2 正则化 的 作用 阐述 L2 正则化 
可以 在 拟合 过程 构造 一个 所有 参数 都 比较 
小 的 模型 因为 一般 认为 参数值 小 的 模型 
比较简单 能 适应 不同 的 数据集 也 在 一定 程度 
上 避免 了 过拟合 现象 可以 设想 一下 对于 一个 
线性 回归方程 若 参数 很大 那么 只要 数据 偏移 一点点 
就会 对 结果 造成 很大 的 影响 但 如果 参数 
足够 小 数据 偏移 得多 一点 也 不会 对 结果 
造成 什么 影响 专业 一点 的 说法 是 抗 扰动 
能力强 python 中 xgboost 的 使用 xgboost 的 参数 可以 
分为 三类 通用 参数 booster 参数 及 目标 参数 1 
通用 参数 booster 选择 基 分类器 可选 参数 gbtree 和 
gblinear 默认 为 gbtreesilent 是否 打印 模型 信息 0 表示 
打印 1 表示 不 打印 默认 0nthread 线程数 选择 默认 
为 最大 可用 线程数 num _ pbuffer 预测 缓冲区 的 
大小 通常 设置 为 训练 实例 的 数量 缓冲区 用于 
保存 最后 的 预测 结果 提高 一步 num _ feature 
boosting 过程 中用 到 的 特征 维数 一般 xgboost 会 
自动 设置 2 booster 参数 eta 学习 步长 相当于 其他 
集合 模型 中的 learning _ rate 默认 为 0.3 一般 
范围 0.01 0 . 2gamma 最小 损失 函数值 默认 为 
0 对于 一个 节点 的 划分 只有 在其 loss function 
得到 结果 大于 0 的 情况下 才 进行 max _ 
depth 树 的 最大 深度 默认 为 6 用于 控制 
过拟合 min _ child _ weight 子 节点 最小 的 
样本 权重 默认 为 1 用于 控制 过拟合 max _ 
delta _ step 每 棵树 权重 改变 的 最大 步长 
默认 为 0 一般 不 设置 subsample 随机 采样 的 
比例 默认 为 1 用户 控制 过拟合 colsample _ bytree 
随机 抽取 特征 比例 默认 1 用户 控制 过拟合 colsample 
_ bylevel 每个 层级 随机 抽取 特征 比例 默认 为 
1lambda l2 正则 项 参数 默认 为 1alpha l1 正则 
项 参数 默认 为 1tree _ method 树 的 构造方法 
默认 为 autoauto 启发式 方法 exact 精确 贪婪 算法 approx 
近似 贪婪 算法 hist 垂直 最优化 贪婪 算法 scale _ 
pos _ weight 类别 处理 不 平衡 处理 默认 为 
0 大于 0 的 取值 可以 处理 类别 不 平衡 
的 问题 帮助 模型 更快 收敛 updater 更新 树 的 
构建 方法 refresh _ leaf 节点 更新 默认 为 trueprocess 
_ type boosting 处理 方式 选择 默认 为 defaultmax _ 
leaves 树 的 最大 节点 数 增加 默认 为 03 
目标 参数 objective 损失 函数 选择 默认 为 reg linear 
可选 参数 如下 reg linear – 线性 回归 reg logistic 
– 逻辑 回归 binary logistic – 二 分类 的 逻辑 
回归 问题 输出 为 概率 binary logitraw – 二 分类 
的 逻辑 回归 问题 输出 的 结果 为 wTx count 
poisson – 计数 问题 的 poisson 回归 输出 结果 为 
poisson 分布 在 poisson 回 归中 max _ delta _ 
step 的 缺省值 为 0.7 used to safeguard optimization multi 
softmax – 让 XGBoost 采用 softmax 目标函数 处理 多分 类 
问题 同时 需要 设置 参数 num _ class 类别 个数 
multi softprob – 和 softmax 一样 但是 输出 的 是 
ndata * nclass 的 向量 可以 将该 向量 reshape 成 
ndata 行 nclass 列 的 矩阵 每行 数据表示 样 本所 
属于 每个 类别 的 概率 rank pairwise – 通过 最小化 
pairwise 损失 做 排名 任务 eval _ metric 最优化 损失 
函数 的 方法 rmse 均方根 误差 mae 平均 绝对误差 logloss 
负 对数 似 然 函数值 error 二 分类 错误率 merror 
多 分类 错误率 mlogloss 多 分类 logloss 损失 函数 auc 
曲 线下 面积 排名 估计 ndcg 归一化 累积 增益 python 
代码 实现 import xgboost as xgb from sklearn . datasets 
import load _ boston import pandas as pd from sklearn 
. cross _ validation import train _ test _ split 
from sklearn . metrics import accuracy _ score boston = 
load _ boston # 查看 波士顿 数据集 的 keys print 
boston . keys boston _ data = boston . data 
target _ var = boston . target feature = boston 
. feature _ names boston _ df = pd . 
DataFrame boston _ data columns = boston . feature _ 
names boston _ df tar _ name = target _ 
var # 查看 目标 变量 描述统计 print boston _ df 
tar _ name . describe # 把 数据集 转变为 二 
分类 数据 boston _ df . loc boston _ df 
tar _ name = 21 tar _ name = 0 
boston _ df . loc boston _ df tar _ 
name 21 tar _ name = 1 x _ train 
x _ test y _ train y _ test = 
train _ test _ split boston _ df feature boston 
_ df tar _ name test _ size = 0.30 
random _ state = 1 train _ data = xgb 
. DMatrix x _ train label = y _ train 
dtrain = xgb . DMatrix x _ train dtest = 
xgb . DMatrix x _ test params = { booster 
gbtree objective binary logistic eval _ metric auc max _ 
depth 6 subsample 0.75 colsample _ bytree 0.75 eta 0.03 
} watchlist = train _ data train bst = xgb 
. train params train _ data num _ boost _ 
round = 100 evals = watchlist # 度量 xgboost 的 
准确性 y _ train _ pred = bst . predict 
dtrain = 0.5 * 1 y _ test _ pred 
= bst . predict dtest = 0.5 * 1 tree 
_ train = accuracy _ score y _ train y 
_ train _ pred tree _ test = accuracy _ 
score y _ test y _ test _ pred print 
xgboost train / test accuracies % . 3f / % 
. 3f % tree _ train tree _ test 结果 
为 xgboost train / test accuracies 0.980 / 0.868 参考资料 
http / / xgboost . readthedocs . io / en 
/ latest / model . html 