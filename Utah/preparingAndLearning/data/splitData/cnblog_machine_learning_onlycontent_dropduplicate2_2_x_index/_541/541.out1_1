提纲 机器学习 为什么 可能 引入 计算 橙 球 概率 问题 
通过 用 Hoeffding s inequality 解决 上面 的 问题 并 
得出 PAC 的 概念 证明 采样 数据 学习 到 的 
h 的 错误率 可以 和 全局 一致 是 PAC 的 
将 得到 的 理论 应用 到 机器学习 证明 实际 机器 
是 可以 学习 机器 学习 的 大多数 情况下 是 让 
机器 通过 现有 的 训练 集 D 的 学习 以 
获得 预测 未知 数据 的 能力 即 选择 一个 最佳 
的 h 做为 学习 结果 那么 这种 预测 是 可能 
的 么 为什么 在 采样 数据 上 得到 的 h 
可以 认为 适用 于 全局 也 就是说 其 泛 化性 
的 本质 是 什么 课程 首先 引入 一个 情景 如果 
有 一个 装有 很多 数量 很大 以至于 无法 通过 数数 
解决 橙 色球 和 绿色 球 的 罐子 我们 能 
不能 推断 橙 色球 的 比例 很 明显 的 思路 
是 利用 统计 中 抽样 的 方法 既然 我们 无法 
穷尽 数遍 所有 罐子 中的 球 不如 随机 取出 几个 
球 算出 其中 两种 颜色 球 的 比例 去 近似 
得到 我们 要 的 答案 这样 真的 可以 么 我们 
都 知道 小 概率 事件 也 会 发生 假如 罐子 
里面 大部分 都是 橙 色球 而 我们 恰巧 取出 的 
都是 绿色 这样 我们 就 判断 错了 那么 到底 通过 
抽样 得出 的 比例 能够 说明 什么 呢 似乎 两者 
不能 直接 划等号 由此 课程 中 引入 了 一个 非常 
重要 的 概念 PAC 要 理解 这个 先得 理解 一个 
超级 重要 的 不等式 Hoeffding s inequality 这个 不等 书 
说明 了 对于 未知 的 那个 概率 我们 的 抽样 
概率 可以 根 它 足够 接近 只要 抽样 的 样本 
够大 或者 容忍 的 限制 变松 这个 和 我们 的 
直觉 是 相符 的 式子 最后 给 出了 PAC 的 
概念 即 概率 上 几乎 正确 所以 我们 通过 采用 
算出 的 橙 球 的 概率 和 全局 橙 球 
的 概率 相等 是 PAC 的 这些/r 和/c 机器/n 学习/v 
有/v 什么/r 关系/n 其实 前 文中 提到 的 例子 可以 
和 机器 学习 问题 一一对应 映射 中最 关键 的 点 
是 讲 抽样 中 橙 球 的 概率 理解 为 
样本 数据集 D 上 h x 错误 的 概率 以此 
推算 出 在 所有 数据 上 h x 错误 的 
概率 这 也是 机器学习 能够 工作 的 本质 即 我们 
为啥 在 采样 数据 上 得到 了 一个 假设 就 
可以 推到 全局 呢 因为 两者 的 错误 率 是 
PAC 的 只要 我们 保证 前者 小 后者 也就 小了 
请注意 以上 都是 对 某个 特定 的 假设 其 在 
全局 的 表现 可以 和其在/nr DataSet 的 表现 PAC 保证 
DataSet 表现 好 就 能够 推断 其 能 泛化 可是 
我们 往往 有 很多 假设 我们 实际上 是 从 很多 
假设 中 挑 一个 表现 最好 Ein 最小 的 作为 
最终 的 假设 那么 这样 挑 的 过程 中 最小 
的 Ein 其 泛化 能力 一定 是 最好 么 肯定 
不是 上面 的 例子 很 形象 每 一个 罐子 都是/nr 
一个 假设 集合 我们 默认 是 挑 表现 最好 的 
也 就是 全 绿色 错误率 为 0 的 那个 假设 
但是 当 从 众多 假设 选择 时 得到 全对 的 
概率 也 在 增加 就像 丢 硬币 一样 当 有个 
150个 童鞋 同时 丢 硬币 5次 那么 这些 人 中 
出现 5 面 同时 朝上 的 概率 为 99% 所以 
表现 好 的 有 可能 是 小 概率 事件 发生 
毕竟 对于 每个 假设 其 泛化 能力 是 PAC 其 
不一定 就 有好 的 泛化 能力 Ein 和 Eout 相同 
我们 称 这次 数据 是 坏 数据 可以 理解 为 
选 到了 泛化 能力差 的 假设 在 坏 数据 上 
Ein 和 Eout 的 表现 是 差别 很大 的 这 
就是 那个 小 概率 事件 Hoeffding s inequality 告诉 我们 
每个 h 在 采样 数据 上 Ein 和 Eout 差别 
很大 的 概率 很低 坏 数据 由于 有 这个 bound 
那么 我们 每次 选取 Ein 最小 的 h 就是 合理 
的 因为 如果 M 小 N 大 出现 表现 好 
的 坏 数据 的 假设 几率 降低 了 我们/r 选择/v 
表现/v 后就/nr 有信心/i 认为/v 其/r 有/v 良好/a 的/uj 泛化/v 能力/n 
总结 一下 M 小 N 足够 大 可以 使得 假设 
具有 良好 的 泛化 能力 如果 同时 Ein 很小 那么 
这个 假设 就是 有效 地 机器 是 可以 学习 的 
学习 到 的 就是 这个 表现 最好 的 假设 整体 
证明 机器 可以 学习 分了 两个 层面 首先 对于 单个 
假设 根据 Hoeffding 不等式 当 N 很大 时 其 泛化 
能力强 是 PAC 的 而 实际上 机器学习 是从 众多 假设 
中 挑 Ein 最小 的 通过 测试 集 找 假设 
这个 的 理论 基础 是 当 M 不大 N 大 
选 到 泛化 能力差 的 假设 概率 低 用到 了 
单个 假设 的 结论 