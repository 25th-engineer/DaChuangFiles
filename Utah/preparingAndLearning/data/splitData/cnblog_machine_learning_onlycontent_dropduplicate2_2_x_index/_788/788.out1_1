背景 提及 机器学习 Machine Learning 大 多数人 的 脑海 中 
首先 浮现 出来 的 就是 各种 机器 学习 的 模型 
策略 当 一个 问题 的 数据 集 data set 确定 
后 我们 便 开始 观察 数据 处理 特征 确定 模型 
然而 为什么 机器学习 这个 工具 可以 大概 正确 地 预测 
数据 的 目标 结果 在 某个 数据 集上 的 学习 
是否 具有 可行性 feasibility of learning 机器 学习 的 学习 
理论 对 这些 问题 作出 了 解释 本文 以 理论 
推导 为主 结合 具体 的 学习 模型 来 介绍 学习 
理论 的 内容 学习 问题 存在 未知 的 目标 函数 
\ f \ mathcal { X } \ rightarrow \ 
mathcal { Y } \ 和样/nr 本集 \ \ mathcal 
{ D } = \ { \ mathbf { x 
_ 1 } y _ 1 \ mathbf { x 
_ 2 } y _ 2 \ cdots \ mathbf 
{ x _ N } y _ N \ } 
\ 其中 \ \ mathbf { x _ i } 
\ in \ mathcal { X } y _ i 
\ in \ mathcal { Y } i = 1 
2 \ cdots N \ 那么 我们 能否 通过 学习 
算法 在 假设 空间 \ \ mathcal { H } 
\ 中 找到 一个 假设 \ h \ 使得 该 
假设 近 似地 等于 目标函数 \ f \ 注 这里 
提到 \ f \ 是 不 可知 的 unknown 即 
无 解析 解 analytic solution 我们 的 任务 就是 使用 
数据 来 构造 一个 经验 解 empirical solution 如果 有 
解析 解 就 不 需要 机器学习 方法 我们 仅对 二 
分类 dichotomy 问题 进行 讨论 该 问题 中 的 假设 
\ h \ { x _ 1 x _ 2 
\ cdots x _ n \ } \ rightarrow \ 
{ 1 + 1 \ } \ 其中 \ \ 
{ x _ 1 x _ 2 \ cdots x 
_ n \ } \ 是 输入 空 间或 样本空间 
\ \ { 1 + 1 \ } \ 是 
输出 空间 样本 集中 的 输入 向量 集合 \ \ 
{ \ mathbf { x _ 1 } \ mathbf 
{ x _ 2 } \ cdots \ mathbf { 
x _ N } \ } \ 是 输入 空间 
的 一个 子集 \ \ mathbf { x _ i 
} \ 是 一个 \ n \ 维 向量 我们 
知道 样本 内 的 实例 的 标记 需要 预测 除 
样本 外 的 实例 的 标记 若能 通过学习 找到 一个 
假设 使得 对 不同 的 实例 预测 的 标记 与 
通过 \ f \ 得到 的 标记 相近 那么 这个 
学习 过程 是 可行 的 否则 就是 不 可行 的 
若 学习 不 可行 无论 选择 什么 学习 算法 什么样 
的 假设 空间 哪怕 一个 假设 对 样本 集 拟合 
地 再好 对 未知 数据 的 预测 都是 不 可信 
的 第一 部分 首先 学习 的 可行性 的 讨论 基于 
一个 这样 的 假设 样 本集 \ \ mathcal { 
D } \ 是 在 样本空间 内 依据 概率分布 \ 
P \ 随机 抽取 的 属于 独立 同 分布 我们 
不 知道 具体 的 概率分布 \ P \ 如果 样 
本集 是 被人 精心设计 构造 的 非随机 抽取 那么 通过学习 
得到 的 假设 和 真实 的 结果 一定 是 大相径庭 
的 在 这个 假设 下 我们 可以 以 概率 的 
方式 使用 \ \ mathcal { D } \ 来 
推断 \ \ mathcal { D } \ 以外 的 
实例 一 引入 Hoeffding 不等式 Hoeffding 不等式 是 关于 一组 
随机变量 均值 的 概率 不等式 如果 \ X _ 1 
X _ 2 \ cdots X _ n \ 为 
一组 独立 同 分布 的 参数 为 \ p \ 
的 伯努利 分布 随机变量 \ n \ 为 随机变量 的 
个数 定义 这组 随机变量 的 均值 为 \ \ overline 
{ X } = \ frac { 1 } { 
n } \ sum _ { i = 1 } 
^ n X _ i \ 对于 任意 \ \ 
epsilon 0 \ Hoeffding 不等式 可以 表示 为 \ P 
\ { | \ overline { X } E \ 
overline { X } | \ epsilon \ } \ 
leq 2e ^ { 2 \ epsilon ^ 2 n 
} \ 二 联系 学习 问题 介绍 假设 空间 hypothesis 
set 假设 空间 是 所有 假设 的 集合 在 \ 
h \ { x _ 1 x _ 2 \ 
cdots x _ n \ } \ rightarrow \ { 
1 + 1 \ } \ 中 我们 设 \ 
x _ i \ 的 可能 取值 有 \ N 
_ i \ 的 那么 对应 的 假设 空间 的 
大小 为 \ 2 \ times \ prod _ { 
i = 1 } ^ { n } N _ 
i \ 我们 先 讨论 假设 空间 中 只有 一个 
假设 的 情况 然后 扩展到 有限 个 假设 最后 扩展 
到 无限 个 假设 只有 一个 假设 的 情况 \ 
\ mathcal { H } = \ { h \ 
} \ 样本 内 误差 in sample error \ E 
_ { in } h = \ frac { 1 
} { N } \ sum _ { i = 
1 } ^ N I \ { h x _ 
i \ neq f x _ i \ } \ 
样本 外 误差 out sample error \ E _ { 
out } h = P \ { h x \ 
neq f x \ } \ 在 学习 问题 learning 
problem 中 样本 内 误差 和 样本 外 误差 分别 
相当于 Hoeffding 不等式 中的 \ \ overline { X } 
\ 和 \ E \ overline { X } \ 
此时 Hoeffding 不等式 转变为 \ P \ { | E 
_ { in } h E _ { out } 
h | \ epsilon \ } \ leq 2e ^ 
{ 2 \ epsilon ^ 2 N } \ 从 
上面 的 公式 可知 拟合 训练 数据 的 假设 与 
该 假设 针对 整个 样本空间 的 预测 这 两者 的 
错误 率 差别 很大 的 那种 情况 发生 的 概率 
是 很小 的 然而 这 不是 一个 学习 过程 只是 
验证 一个 假设 的 预测 能力 它 不能 证明 这个 
假设 对应 的 样本 外 误差 是 最小 的 不能 
证明 这个 假设在 假设 空间 内 是 最佳 假设 有限 
假设 空间 情况 \ \ mathcal { H } = 
\ { h _ 1 h _ 2 \ cdots 
h _ M \ } \ 在 假设 空间 \ 
\ mathcal { H } \ 中 每个 假设 \ 
h _ i \ 都是 固定 不变 的 给定 一个 
的 样本 集 学习 算法 的 任务 就是 在 给定 
不变 的 M 个 假设 中 找到 一个 最终 的 
假设 \ g \ 使得 对于 任意 \ i \ 
in \ { 1 2 \ cdots M \ } 
\ 都有 \ P \ { | E _ { 
in } g E _ { out } g | 
\ epsilon \ } \ leqslant P \ { | 
E _ { in } h _ i E _ 
{ out } h _ i | \ epsilon \ 
} \ 其 上界 为 \ P \ { | 
E _ { in } g E _ { out 
} g | \ epsilon \ } \ leqslant \ 
\ sum _ { i = 1 } ^ M 
P \ { | E _ { in } h 
_ i E _ { out } h _ i 
| \ epsilon \ } \ leqslant 2Me ^ { 
2 \ epsilon ^ 2 N } \ 令 \ 
\ delta = P \ { | E _ { 
in } g E _ { out } g | 
\ epsilon \ } \ 则 \ N \ geqslant 
\ frac { 1 } { 2 \ epsilon ^ 
2 } lnM + ln { \ frac { 1 
} { \ delta } } \ 也 就是说 在 
有限 个 假设 且 样 本集 足够 大 的 情况 
下 样本 内 误差 和 样本 外 误差 的 差 
的 绝对值 超过 \ \ epsilon \ 的 概率 为 
\ \ delta \ 三 总结 再次 回到 学习 问题 
回 到 问题 我们 能否 通过 学习 算法 在 假设 
空间 \ \ mathcal { H } \ 中 找到 
一个 假设 \ h \ 使得 该 假设 近 似地 
等于 目标函数 \ f \ 这里 的 近 似地 等于 
意味着 \ E _ { out } g \ approx 
0 \ 确定性 答案 deterministic answer 不能 我们 不能 确定 
任何 一个 在 \ \ mathcal { D } \ 
之外 的 实例 的 标记 概 率性 答案 probabilistic answer 
可以 我们 可以 得到 一个 在 \ \ mathcal { 
D } \ 之外 的 实例 的 最 可能 的 
标记 结果 根据 上文 的 讨论 可知 学习 问题 可以 
分为 两 个子 问题 我们 是否 可以 确定 \ E 
_ { out } g \ approx E _ { 
in } g \ 为了 令 \ E _ { 
} g \ approx 0 \ 我们 是否 可以 使 
\ E _ { in } g \ 尽可能 小 
复杂性 权衡 假设 空间 的 复杂性 当 假设 空间 的 
大小 \ M \ 变大 \ E _ { in 
} g \ 偏离 \ E _ { out } 
g \ 的 可能性 就 越大 但是 由于 \ g 
\ 来源于 \ \ mathcal { H } \ 假设 
空间 的 复杂性 增加 了 我们 找到 一个 较小 的 
\ E _ { in } g \ 的 概率 
在 学习 理论 中 对 \ \ mathcal { H 
} \ 的 复杂性 的 权衡 是 一个 很 重要 
的 话题 目标函数 的 复杂性 \ f \ 复杂 而 
\ \ mathcal { H } \ 简单 时 很难 
找到 一个 \ E _ { in } g \ 
的 假设 而 \ f \ 复杂 同时 \ \ 
mathcal { H } \ 复杂 时 又会 遇到 了 
\ E _ { in } g \ 偏离 \ 
E _ { out } g \ 的 问题 当 
\ f \ 过于 复杂 时 学习 是 不 可行 
的 实际 问题 中 大多数 的 目标 函数 都 不会 
过于 复杂 只要 保证 \ \ mathcal { H } 
\ 的 复杂性 可以 给 我们 一个 较小 的 Hoeffing 
边界 那么 样 本集 的 匹配 程度 就 决定 了 
我们 学习 \ f \ 的 成败 学习 模型 probably 
approximately correct PAC 我们 训练 学习 器 的 目标 是 
能够 从 合理 数量 的 训练 数据 中 通过 合理 
的 计算 量 可靠 地 学习 到 知识 这里 的 
知识 指 目标函数 \ f \ 机器 学习 的 现实 
情况 除非 对 每个 可能 的 数据 进行 训练 否则 
总会 存在 多个 假设 使得 真实 错误率 不 为零 即 
学习 器 无法 保证 和 目标函数 完全一致 此外 训练样本 是 
随机 选取 的 训练样本 总 有 一定 的 误导 性 
为此 我们 要 弱化 对 学习 器 的 要求 我们 
不 要求 学习 器 输出 零 错误率 的 假设 只 
要求 错误率 被 限制 在某 常数 \ \ epsilon \ 
范围 内 \ \ epsilon \ 可为 任意 小 不/d 
要求/v 学习/v 器/n 对/p 所有/b 任意/v 抽取/v 的/uj 数据/n 都能/nr 
成功/a 预测/vn 只 要求 其 失败 的 概率 被 限定 
在 某个 常数 \ \ delta \ 的 范围 内 
\ \ delta \ 可取 任意 小 简而言之 我们 只 
要求 学习 器 可能 学习 到 一个 近似 正确 的 
假设 故 得到 了 可能 近似 正确 学习 PAC 学习 
一个 可 PAC 学习 的 学习 器 要 满足 两个 
条件 学习 器 必须 以 任 意高 的 概率 输出 
一个 错误率 任意 低 的 假设 学习 过程 的 时间 
最 多以 多项式 方式 增长 对于 PAC 学习 来说 训练样本 
的 数量 和 学习 所需 的 计算 资源 是 密切 
相关 的 如果 学习 器 对 每个 训练样本 需要 某 
最小 处理 时间 那么 为了 使 目标函数 \ f \ 
是 可 PAC 学习 的 学习 器 必须 在 多项式 
数量 的 训练 样本 中 进行 学习 第二 部分 在 
第一 部分 中 我们 讨论 了 有限 假设 空间 的 
情况 不过 现实 中 遇到 的 大多数 学习 问题 的 
假设 集 的 大小 都是/nr 无限 的 此时 \ P 
\ { | E _ { in } g E 
_ { out } g | \ epsilon \ } 
\ 的 上界 \ 2Me ^ { 2 \ epsilon 
^ 2 N } \ 中的 \ M \ 趋于 
无穷 使得 该 上界 没有 任何 意义 了 因此 在 
第二 部分 中 我们 将 推导 出 一个 更加 精确 
的 上界 一 泛化 理论 Theory of Generalization 泛化 误差 
generalization error 在 第一 部分 的 最后 我们 得到 \ 
P \ { | E _ { in } g 
E _ { out } g | \ epsilon \ 
} \ leqslant 2Me ^ { 2 \ epsilon ^ 
2 N } \ 我们 选择 一个 容忍度 tolerance level 
\ \ delta = P \ { | E _ 
{ in } g E _ { out } g 
| \ epsilon \ } \ 那么 至少 有 \ 
1 \ delta \ 概率 使得 \ E _ { 
out } g \ leqslant E _ { in } 
g + \ sqrt { \ frac { 1 } 
{ 2N } ln { \ frac { 2M } 
{ \ delta } } } \ \ E _ 
{ out } g \ geqslant E _ { in 
} g \ sqrt { \ frac { 1 } 
{ 2N } ln { \ frac { 2M } 
{ \ delta } } } \ 即 泛化 边界 
在 有限 假设 集中 当 \ M \ 一 定时 
随着 样本数 \ N \ 的 增加 \ E _ 
{ out } g \ approx E _ { in 
} g \ 而 对于 有 一个 无限 大小 的 
假设 集 模型 来说 我们 可以 找 一个 更 精确 
的 值 来 取代 假设 空间 的 实际 大小 \ 
M \ 当 \ M \ 趋于 无穷 时 该 
值 仍 是 一个 有穷 值 为什么 \ M \ 
可以 优化 呢 在 第一 部分 我们 利用 联合 界 
union bound 令 \ P \ { \ bigcup _ 
{ i = 1 } ^ { M } A 
_ i \ } \ leqslant \ sum _ { 
i = 1 } ^ M P \ { A 
_ i \ } \ 然而 在 一个 典型 的 
学习 模型 里 许多 假设 都很 类似 下面 我们 引入 
成长 函数 和 VC 维 等 概念 引入 概念 二 
分类 dichotomies 对于 某个 \ h \ in \ mathcal 
{ H } \ \ h \ mathbf { x 
_ 1 } \ cdots h \ mathbf { x 
_ N } \ 是 该 假设 所 对应 的 
二分 类 可见 \ h \ mathbf { x _ 
1 } \ cdots h \ mathbf { x _ 
N } \ in \ { + 1 1 \ 
} ^ N \ 成长 函数 growth function 令 \ 
\ mathcal { H } \ mathbf { x _ 
1 } \ cdots \ mathbf { x _ N 
} = \ { h \ mathbf { x _ 
1 } \ cdots h \ mathbf { x _ 
N } | h \ in \ mathcal { H 
} \ } \ 则 一个 假设 空间 的 成长 
函数 为 \ m _ \ mathcal { H } 
N = \ max _ { \ mathbf { x 
_ 1 } \ cdots \ mathbf { x _ 
N } \ in \ mathcal { X } } 
| \ mathcal { H } \ mathbf { x 
_ 1 } \ cdots \ mathbf { x _ 
N } | \ 对于 不同 的 \ \ mathcal 
{ D } \ 由于 内部 \ N \ 个 
点 的 分布 方式 不同 其 \ | \ mathcal 
{ H } \ mathbf { x _ 1 } 
\ cdots \ mathbf { x _ N } | 
\ 也 可能 不 同 成长 函数 是 假设 空间 
在 有限 的 样本 集上 的 可以 产生 的 不同 
假设 的 数量 而 不是 在 整个 输入 空间 里 
因此 成长 函数 的 值 取决于 N 个 样本 的 
可行 的 分类 结果 任意 一个 样本 的 分类 结果 
是 + 1 或 1 的 数目 打散 shatter 若 
一个 假设 空间 \ \ mathcal { H } \ 
能 产生 样本 集上 的 所有 假设 即 \ \ 
mathcal { H } \ mathbf { x _ 1 
} \ cdots \ mathbf { x _ N } 
= \ { 1 + 1 \ } ^ N 
\ 此时 我们 说 \ \ mathcal { H } 
\ 可以 打散 \ \ mathbf { x _ 1 
} \ cdots \ mathbf { x _ N } 
\ \ m _ \ mathcal { H } N 
= 2 ^ N \ 可以 看出 对于 任意 正整数 
\ N \ 都 满足 \ m _ \ mathcal 
{ H } N \ leqslant2 ^ N \ 断点 
break point 如果 没有 一个 大小 为 \ k \ 
的 数据 集 可以 被 \ \ mathcal { H 
} \ 打散 则 \ k \ 是 \ \ 
mathcal { H } \ 的 一个 断点 此时 \ 
m _ \ mathcal { H } k 2 ^ 
k \ VC 维 Vapnik Chervonenkis dimension 一个 假设 空间 
\ \ mathcal { H } \ 的 VC 维 
\ d _ { vc } \ mathcal { H 
} \ 是 满足 \ m _ \ mathcal { 
H } N = 2 ^ N \ 的 最大 
整数 \ N \ 如果 对于 任意 \ N \ 
都有 \ m _ \ mathcal { H } N 
= 2 ^ N \ 即 没有 断点 则 \ 
d _ { vc } \ mathcal { H } 
= \ infty \ 假设 空间 的 有效 大小 给定 
一个 \ \ mathcal { D } \ 假设 空间 
中 便 存在 多个 具有 相同 的 二分法 的 假设 
对于 我们 的 学习 问题 来说 这些 具有 相同 二分法 
的 假设 就是 冗余 因此 尽管 假设 空间 的 实际 
大小 是 无穷 的 但 我们 有 可能 可以 找到 
一个 有穷 的 有效 大小 只要 \ \ mathcal { 
H } \ 存在 断点 即 \ d _ { 
vc } \ mathcal { H } \ neq \ 
infty \ \ m _ \ mathcal { H } 
N \ 就是 关于 \ N \ 的 多项式 若 
\ m _ \ mathcal { H } N \ 
可以 取代 \ M \ 则 随着 \ N \ 
的 增大 泛化 误差 \ \ sqrt { \ frac 
{ 1 } { 2N } ln { \ frac 
{ 2M } { \ delta } } } \ 
将 逐渐 减小 为 \ 0 \ 相反地 若 不 
存在 断点 泛化 误差 永远 不会 趋向于 \ 0 \ 
注 在 有些 问题 里 只有 \ N \ 个 
点 的 分布 方式 极其 特殊 时 才能 被 \ 
\ mathcal { H } \ 打散 这时 虽然 这时 
\ m _ \ mathcal { H } N = 
2 ^ N \ 但 考虑 平均 情况 \ m 
_ \ mathcal { H } N \ 仍是 关于 
\ N \ 的 多项式 下面 证明 只要 \ \ 
mathcal { H } \ 存在 断点 \ m _ 
\ mathcal { H } N \ 就是 关于 \ 
N \ 的 多项式 定义 \ B N k \ 
为 \ \ mathcal { H } \ mathbf { 
x _ 1 } \ cdots \ mathbf { x 
_ N } \ 的 最大 数目 其中 使 \ 
\ mathcal { H } \ mathbf { x _ 
1 } \ cdots \ mathbf { x _ N 
} \ 数目 最大 的 样本 集 \ \ mathcal 
{ D } \ 中 不 存在 可以 被 \ 
\ mathcal { H } \ 打散 的 数目 为 
\ k \ 的 子集 \ B N k \ 
是 比 \ 2 ^ N \ 更 准确 的 
\ m _ \ mathcal { H } N \ 
的 上界 \ m _ \ mathcal { H } 
N \ leqslant B N k \ leqslant2 ^ N 
\ space \ space \ text { if k is 
a break point for } \ mathcal { H } 
\ 可以 根据 下式 利用 动态规划 求出 具体 的 \ 
B N k \ 的 值 \ B N k 
\ leqslant B N 1 k + B N 1 
k 1 \ 也 可以 直接 利用 归纳法 证明 \ 
B N k \ leqslant \ sum _ { i 
= 0 } ^ { k 1 } \ binom 
{ N } { i } \ 最后 由于 断点 
\ k = d _ { vc } \ mathcal 
{ H } + 1 \ 而且 给 定 一个 
\ \ mathcal { H } \ 后 断点 和 
VC 维 就是 固定不动 的 所以 \ m _ \ 
mathcal { H } N \ leqslant B N k 
\ leqslant \ sum _ { i = 0 } 
^ { d _ { vc } \ mathcal { 
H } } \ binom { N } { i 
} \ leqslant N ^ { d _ { vc 
} \ mathcal { H } } + 1 \ 
是 一个 关于 \ N \ 的 多项式 1 动态规划 
初始状态 \ B N 1 = 1 \ \ k 
1 \ 时 \ B 1 k = 2 \ 
建立 递归 将 \ h \ mathbf { x _ 
1 } \ cdots h \ mathbf { x _ 
N } \ 划分为 \ h \ mathbf { x 
_ 1 } \ cdots h \ mathbf { x 
_ { N 1 } } \ 和 \ h 
\ mathbf { x _ N } \ 两部分 在前 
一 部分 仅 出现 过 一次 的 序列 \ _ 
1 \ 的 数目 记作 \ \ alpha \ 在 
剩下 的 \ B N k \ alpha \ 个 
序列 中 将 \ h \ mathbf { x _ 
{ N 1 } } = + 1 \ 分为 
一类 \ _ 2 ^ + \ \ h \ 
mathbf { x _ { N 1 } } = 
1 \ 分为 一类 \ _ 2 ^ \ 容易 
得知 这 两类 里 的 序列 数目 相等 记作 \ 
\ beta \ 此时 \ B N k = \ 
alpha + 2 \ beta \ 在 集合 \ _ 
1 + _ 2 ^ + \ 中 在 第一 
部分 中 不 存在 可以 被 \ \ mathcal { 
H } \ 打散 的 数目 为 \ k \ 
的 子集 即 \ \ alpha + \ beta \ 
leqslant B N 1 k \ 在 集合 \ _ 
2 ^ \ 不 存在 可以 被 \ \ mathcal 
{ H } \ 打散 的 数目 为 \ k 
1 \ 的 子集 若 存在 则在 集合 \ _ 
2 ^ + _ 2 ^ + \ 中 也 
存在 可以 被 \ \ mathcal { H } \ 
打散 的 数目 为 \ k \ 的 子集 与 
初始 定义 矛盾 即 \ \ beta \ leqslant B 
N 1 k 1 \ 综上 \ B N k 
\ leqslant B N 1 k + B N 1 
k 1 \ 利用 动态规划 求出 \ B N k 
\ leqslant \ sum _ { i = 0 } 
^ { k 1 } \ binom { N } 
{ i } \ 时间 复杂度 为 \ O N 
* k \ 空间 复杂度 为 \ O min N 
k \ 2 归纳法 这里 直接 用 归纳法 证明 上式 
的 正确性 当 \ k = 1 \ 或 \ 
N = 1 \ 时 上式 成立 假设 对于 所有 
的 \ N \ leqslant N _ 0 \ 上式 
正确 当 \ N = N _ 0 + 1 
\ 时 \ B N _ 0 + 1 k 
\ leqslant \ sum _ { i = 0 } 
^ { k 1 } \ binom { N _ 
0 } { i } + \ sum _ { 
i = 0 } ^ { k 2 } \ 
binom { N _ 0 } { i } = 
1 + \ sum _ { i = 1 } 
^ { k 1 } \ binom { N _ 
0 } { i } + \ sum _ { 
i = 1 } ^ { k 1 } \ 
binom { N _ 0 } { i 1 } 
= 1 + \ sum _ { i = 1 
} ^ { k 1 } \ binom { N 
_ 0 + 1 } { i } = \ 
sum _ { i = 0 } ^ { k 
1 } \ binom { N _ 0 + 1 
} { i } \ 用 \ m _ \ 
mathcal { H } N \ 取代 \ M \ 
若 用 \ m _ \ mathcal { H } 
N \ 取代 \ M \ 则 对应 的 Hoeffing 
不等式 转变为 VC 不等式 \ P \ { \ sup 
_ { h \ in \ mathcal { H } 
} | E _ { in } h E _ 
{ out } h | \ epsilon \ } \ 
leqslant 4m _ \ mathcal { H } 2N e 
^ { \ frac { 1 } { 8 } 
\ epsilon ^ 2 N } \ 给定 任意 的 
容忍度 \ \ delta 0 \ 那么 至少 有 \ 
1 \ delta \ 概率 使得 \ E _ { 
out } g \ leqslant E _ { in } 
g + \ sqrt { \ frac { 8 } 
{ N } ln { \ frac { 4m _ 
\ mathcal { H } 2N } { \ delta 
} } } \ 即 VC 泛化 边界 尽管 比 
之前 的 泛化 边界 的 限定 更弱 但 只要 VC 
维 不是 无穷大 的 那么 泛化 误差 最终 会 收敛 
于 \ 0 \ 成长 函数 作为 假设 空间 的 
有效 大小 取代 了 假设 空间 的 实际 大小 \ 
M \ 因此 VC 泛化 边界 确定 了 无限 大小 
的 假设 空间 上 学习 的 可行性 二 VC 边界 
的 证明 \ P \ { \ sup _ { 
h \ in \ mathcal { H } } | 
E _ { in } h E _ { out 
} h | \ epsilon \ } \ leqslant 4m 
_ \ mathcal { H } 2N e ^ { 
\ frac { 1 } { 8 } \ epsilon 
^ 2 N } \ 对于 任意 的 目标 函数 
\ f \ 或者 \ P y | \ mathbf 
{ x } \ 都 成立 每个 数据集 都是 独立 
同 分布 的 iid 数据 集中 的 每个 数据 依 
\ P \ mathbf { x } y \ 独立 
生成 难题 \ E _ { out } h \ 
取决于 整个 输入 空间 不 容易 处理 解决 避免 计算 
\ E _ { out } h \ 找到 \ 
E _ { in } \ 和 \ E _ 
{ out } \ 的 差值 和 \ E _ 
{ in } \ 和 另一个 独立 的 数据 集 
的 样本 内 误差 的 差值 的 关系 泛化 误差 
和 样本 内 偏差 的 联系 第二 个 大小 为 
\ N \ 的 数据 集 \ \ mathcal { 
D } \ 从 \ P \ mathbf { x 
} y \ 抽样 而得 且 独立 于 \ \ 
mathcal { D } \ 对于 只有 一个 假设 的 
情况 随着 \ N \ 的 数量 的 增大 \ 
E _ { in } h \ 和 \ E 
_ { in } h \ 可以 粗略地 看作 以 
\ E _ { out } h \ 为 均值 
的 高斯分布 因此 可以 直观 地 看出 \ P \ 
{ | E _ { in } h E _ 
{ out } h | \ text { is large 
} \ } \ leqslant 2P \ { | E 
_ { in } h E _ { in } 
h | \ text { is large } \ } 
\ 对于 多个 假设 的 情况 \ P \ { 
\ sup _ { h \ in \ mathcal { 
H } } | E _ { in } h 
E _ { in } h | \ frac { 
\ epsilon } { 2 } \ } \ geqslant 
P \ { \ sup _ { h \ in 
\ mathcal { H } } | E _ { 
in } h E _ { in } h | 
\ frac { \ epsilon } { 2 } \ 
space and \ space \ sup _ { h \ 
in \ mathcal { H } } | E _ 
{ in } h E _ { out } h 
| \ epsilon \ } = P \ { \ 
sup _ { h \ in \ mathcal { H 
} } | E _ { in } h E 
_ { out } h | \ epsilon \ } 
\ times P \ { \ sup _ { h 
\ in \ mathcal { H } } | E 
_ { in } h E _ { in } 
h | \ frac { \ epsilon } { 2 
} \ space | \ space \ sup _ { 
h \ in \ mathcal { H } } | 
E _ { in } h E _ { out 
} h | \ epsilon \ } \ 设 \ 
h ^ { * } \ 为 任意 一个 满足 
\ | E _ { in } h ^ { 
* } E _ { out } h ^ { 
* } | \ epsilon \ 的 假设 则 \ 
P \ { \ sup _ { h \ in 
\ mathcal { H } } | E _ { 
in } h E _ { in } h | 
\ frac { \ epsilon } { 2 } \ 
space | \ space \ sup _ { h \ 
in \ mathcal { H } } | E _ 
{ in } h E _ { out } h 
| \ epsilon \ } \ geqslant P \ { 
| E _ { in } h ^ { * 
} E _ { in } h ^ { * 
} | \ frac { \ epsilon } { 2 
} \ space | \ space \ sup _ { 
h \ in \ mathcal { H } } | 
E _ { in } h E _ { out 
} h | \ epsilon \ } \ geqslant P 
\ { | E _ { in } h ^ 
{ * } E _ { out } h ^ 
{ * } | \ leqslant \ frac { \ 
epsilon } { 2 } \ space | \ space 
\ sup _ { h \ in \ mathcal { 
H } } | E _ { in } h 
E _ { out } h | \ epsilon \ 
} \ geqslant 1 2e ^ { \ frac { 
1 } { 2 } \ epsilon ^ 2 N 
} \ 我们 假设 \ e ^ { \ frac 
{ 1 } { 2 } \ epsilon ^ 2 
N } \ frac { 1 } { 4 } 
\ 否则 \ 4m _ \ mathcal { H } 
2N e ^ { \ frac { 1 } { 
8 } \ epsilon ^ 2 N } \ 恒 
大于 \ 1 \ 没有 证明 的 必要 里 此时 
可以 得到 \ P \ { \ sup _ { 
h \ in \ mathcal { H } } | 
E _ { in } h E _ { out 
} h | \ epsilon \ } \ leqslant 2P 
\ { \ sup _ { h \ in \ 
mathcal { H } } | E _ { in 
} h E _ { in } h | \ 
frac { \ epsilon } { 2 } \ } 
\ 用 成长 函数 限定 最坏 情况 偏差 一种 选取 
\ \ mathcal { D } \ 和 \ \ 
mathcal { D } \ 的 方法 是 首先 从 
输入 空间 中 随机 地 抽取 \ 2N \ 个 
样本 然后 随机 地 划分 成 \ \ mathcal { 
D } \ 和 \ \ mathcal { D } 
\ 即 此时 \ P \ { \ sup _ 
{ h \ in \ mathcal { H } } 
| E _ { in } h E _ { 
in } h | \ frac { \ epsilon } 
{ 2 } \ } = \ sum _ { 
} P \ { \ } \ times P \ 
{ \ sup _ { h \ in \ mathcal 
{ H } } | E _ { in } 
h E _ { in } h | \ frac 
{ \ epsilon } { 2 } \ space | 
\ space \ } \ leqslant \ sup _ { 
} P \ { \ sup _ { h \ 
in \ mathcal { H } } | E _ 
{ in } h E _ { in } h 
| \ frac { \ epsilon } { 2 } 
\ space | \ space \ } \ 假设 \ 
M \ leqslant m _ { \ mathcal { H 
} } 2N \ 则 \ P \ { \ 
sup _ { h \ in \ mathcal { H 
} } | E _ { in } h E 
_ { in } h | \ frac { \ 
epsilon } { 2 } \ space | \ space 
\ } = P \ { \ sup _ { 
h \ in \ { h _ 1 \ cdots 
h _ M \ } } | E _ { 
in } h E _ { in } h | 
\ frac { \ epsilon } { 2 } \ 
space | \ space \ } \ leqslant \ sum 
_ { i = 1 } ^ { M } 
P \ { | E _ { in } h 
_ i E _ { in } h _ i 
| \ frac { \ epsilon } { 2 } 
\ space | \ space \ } \ leqslant M 
\ times \ sup _ { h \ in \ 
mathcal { H } } P \ { | E 
_ { in } h E _ { in } 
h | \ frac { \ epsilon } { 2 
} \ space | \ space \ } \ 综上 
\ P \ { \ sup _ { h \ 
in \ mathcal { H } } | E _ 
{ in } h E _ { in } h 
| \ frac { \ epsilon } { 2 } 
\ } \ leqslant m _ { \ mathcal { 
H } } 2N \ times \ sup _ { 
} \ sup _ { h \ in \ mathcal 
{ H } } P \ { | E _ 
{ in } h E _ { in } h 
| \ frac { \ epsilon } { 2 } 
\ space | \ space \ } \ 限定 样本 
内 偏差 首先 引入 不 放回 取样 的 Hoeffing 不等式 
令 \ \ mathcal { A } = \ { 
a _ 1 \ cdots a _ { 2N } 
\ } \ 其中 \ a _ i \ in 
0 1 \ 令 \ \ mu = \ frac 
{ 1 } { 2N } \ sum _ { 
i = 1 } ^ { 2N } a _ 
i \ 令 \ \ mathcal { D } = 
{ z _ 1 \ cdots z _ N } 
\ 为 一个 从 \ \ mathcal { A } 
\ 中 不 放回 随机取样 而 得到 的 大小 为 
\ N \ 的 样本 则 \ P \ { 
| \ frac { 1 } { N } \ 
sum _ { i = 1 } ^ { N 
} z _ i \ mu | \ epsilon \ 
} \ leqslant 2e ^ { 2 \ epsilon ^ 
2N } \ 当 \ h \ mathbf { x 
} _ i \ ne y _ i \ 时 
\ a _ i = 1 \ 否 则为 \ 
0 \ 此时 \ E _ { in } h 
= \ frac { 1 } { N } \ 
sum _ { a _ i \ in \ mathcal 
{ D } } a _ i \ text { 
and } E _ { in } h = \ 
frac { 1 } { N } \ sum _ 
{ a _ i \ in \ mathcal { D 
} } a _ i \ \ \ mu = 
\ frac { 1 } { 2N } \ sum 
_ { i = 1 } ^ { 2N } 
a _ i = \ frac { E _ { 
in } h + E _ { in } h 
} { 2 } \ 带入 Hoeffing 不等式 得 \ 
P \ { | E _ { in } h 
E _ { in } h | 2t \ space 
| \ space \ } \ leqslant 2e ^ { 
2t ^ 2N } \ 最后 令 \ 4t = 
\ epsilon \ 得 \ P \ { | E 
_ { in } h E _ { in } 
h | \ frac { \ epsilon } { 2 
} \ space | \ space \ } \ leqslant 
2e ^ { \ frac { 1 } { 8 
} \ epsilon ^ 2N } \ 第三 部分 一 
泛化 边界 的 解释 VC 泛化 边界 适用 于 所有 
的 假设 集 学习 算法 输入 空间 概率分布 和 二分 
目标函数 但 它 是 一个 非常 宽松 的 边界 推导 
过程 中 适用 的 Hoeffing 不等式 本身 就是 松弛 的 
为了 使 我们 的 VC 分析 结果 独立 于 输入 
空间 \ \ mathcal { X } \ 的 概率分布 
\ P \ \ m _ { \ mathcal { 
H } } N \ 给 出了 在 最坏 样 
本集 的 情况下 的 估计 限定 \ m _ { 
\ mathcal { H } } N \ 时 我们 
简单 地 选择 了 \ d _ { vc } 
\ mathcal { H } \ 的 多项式 虽然 VC 
分析 得到 的 泛化 边界 很 松弛 但 它 是 
我们 评估 无限 假设 集 下 学习 的 可行性 的 
一个 依据 对 不同 的 学习 模型 来说 VC 分析 
有着 同样 松弛 的 边界 因此 我们 可以 比较 不同 
的 模型 在 同一 样 本集 下 泛化 能力 的 
好坏 样本 复杂度 样本 复杂度 是 指 达到 我们 期望 
的 泛化 能力 所 需要 的 样本 数目 给定 \ 
\ delta 0 \ 假设 我们 期望 泛化 误差 不超过 
\ \ epsilon \ 则 \ N \ geqslant \ 
frac { 8 } { \ epsilon ^ 2 } 
ln \ frac { 4m _ { \ mathcal { 
H } } 2N } { \ delta } \ 
geqslant \ frac { 8 } { \ epsilon ^ 
2 } ln \ frac { 4 2N ^ { 
d _ { vc } } + 1 } { 
\ delta } \ \ N \ 和 \ d 
_ { vc } \ 的 比例 大概 是 \ 
10000 1 \ 这 是 一个 严格 的 最大 界限 
在 实践 中 这一 比例 大约 是 \ 10 1 
\ 对模型 复杂度 的 惩罚 项 对模型 复杂度 的 惩罚 
项 就是 给定 一个样 本集 \ \ mathcal { D 
} \ 即 给定 \ N \ 时 我们 可以 
期望 的 泛化 能力 \ E _ { out } 
g \ leqslant E _ { in } g + 
\ Omega N \ mathcal { H } \ delta 
\ 这里 的 \ \ Omega N \ mathcal { 
H } \ delta \ 就是 模型 的 复杂度 的 
惩罚 项 \ \ Omega N \ mathcal { H 
} \ delta = \ sqrt { \ frac { 
8 } { N } ln { \ frac { 
4m _ \ mathcal { H } 2N } { 
\ delta } } } \ leqslant \ sqrt { 
\ frac { 8 } { N } ln { 
\ frac { 4 2N ^ { d _ { 
vc } } + 1 } { \ delta } 
} } \ 因此 我们 需要 一个 权衡 模型 复杂度 
的 增长 会 降低 \ E _ { in } 
\ 但 增加 \ \ Omega N \ mathcal { 
H } \ delta \ 一个 理想 的 模型 应该 
是 使 最小化 这两项 的 组合 测试 集 尽管 我们 
的 VC 分析 是 基于 二分 目标函数 的 但 我们 
可以 把 它 扩大 到 其他 类型 的 目标 函数 
甚至 是 回归 目标函数 二 近似 泛化 权衡 Approximation Generalization 
Tradeoff 在 \ \ mathcal { H } \ 上 
选择 的 假设 既 要在 样本 集上 近似 地 接近 
目标函数 \ f \ 又要 在 新的 数据 集上 具有 
较好 的 泛化 能力 VC 泛化 边界 提供 了 考虑 
权衡 的 一种 方法 如果 \ \ mathcal { H 
} \ 太 简单 我们 可能 无法 在 样本 集上 
近似 地 接近 目标函数 即 无法 得到 一个 较小 的 
\ E _ { in } \ 如果 \ \ 
mathcal { H } \ 太 复杂 我们 可能 得到 
一个 泛化 能力 较差 的 假设 而 偏差 方差分析 提供 
给 我们 另一种 方法 来 看待 近似 泛化 权衡 问题 
VC 分析 基于 0 1 误差 计算 用 \ E 
_ { in } \ 加上 一个 惩罚 项 \ 
\ Omega \ 来 限定 \ E _ { out 
} \ 而 偏差 方差分析 中 基于 平方 误差 计算 
将 \ E _ { out } \ 分解 为 
两个 不同 的 部分 此时 样本 外 误差 为 \ 
E _ { out } g ^ { \ mathcal 
{ D } } = E _ { \ mathbf 
{ x } } g ^ { \ mathcal { 
D } } \ mathbf { x } f \ 
mathbf { x } ^ 2 \ 可见 我们 的 
最终 假设 依赖于 样 本集 \ \ mathcal { D 
} \ 要想 不 依赖 于 某个 样 本集 我们 
可以 计算 所有 数据集 的 期望 \ E _ { 
\ mathcal { D } } { E _ { 
out } g ^ { \ mathcal { D } 
} } = E _ { \ mathcal { D 
} } E _ { \ mathbf { x } 
} g ^ { \ mathcal { D } } 
\ mathbf { x } f \ mathbf { x 
} ^ 2 = E _ { \ mathbf { 
x } } E _ { \ mathcal { D 
} } g ^ { \ mathcal { D } 
} \ mathbf { x } f \ mathbf { 
x } ^ 2 = E _ { \ mathbf 
{ x } } E _ { \ mathcal { 
D } } g ^ { \ mathcal { D 
} } \ mathbf { x } ^ 2 2E 
_ { \ mathcal { D } } g ^ 
{ \ mathcal { D } } \ mathbf { 
x } f \ mathbf { x } + f 
\ mathbf { x } ^ 2 \ 用 \ 
\ overline { g } \ mathbf { x } 
\ 来 表示 \ E _ { \ mathcal { 
D } } g ^ { \ mathcal { D 
} } \ mathbf { x } \ 则 \ 
E _ { \ mathcal { D } } { 
E _ { out } g ^ { \ mathcal 
{ D } } } = E _ { \ 
mathbf { x } } E _ { \ mathcal 
{ D } } g ^ { \ mathcal { 
D } } \ mathbf { x } ^ 2 
2 \ overline { g } \ mathbf { x 
} f \ mathbf { x } + f \ 
mathbf { x } ^ 2 = E _ { 
\ mathbf { x } } E _ { \ 
mathcal { D } } g ^ { \ mathcal 
{ D } } \ mathbf { x } \ 
overline { g } \ mathbf { x } ^ 
2 + \ overline { g } \ mathbf { 
x } f \ mathbf { x } ^ 2 
\ \ bias \ mathbf { x } = \ 
overline { g } \ mathbf { x } f 
\ mathbf { x } ^ 2 \ 表示 我们 
使用 不同 的 样本 集 学习 而 得到 的 平均 
假设 与 目标 函数 的 偏差 由于 \ \ overline 
{ g } \ mathbf { x } \ 不受 
样 本集 的 限制 因此 偏 差值 仅 受 学习 
模型 本身 的 限制 \ var \ mathbf { x 
} = E _ { \ mathcal { D } 
} g ^ { \ mathcal { D } } 
\ mathbf { x } \ overline { g } 
\ mathbf { x } ^ 2 \ 表示 基于 
不 同样 本集 得到 的 假设 与 平均 假设 之间 
的 差异 程度 综上 我们 将 样本 外 误差 分解为 
\ E _ { \ mathcal { D } } 
{ E _ { out } g ^ { \ 
mathcal { D } } } = E _ { 
\ mathbf { x } } bias \ mathbf { 
x } + var \ mathbf { x } = 
bias + var \ 我们 的 推导 过程 忽略 了 
数据 的 噪声 虽然 噪声 是 不可避免 的 但 我们 
关心 的 主要 是 偏差 和 方差 使用 偏差 方差 
分解 方法 来 看待 近似 泛化 权衡 偏差 方差 分解 
只是 一个 理论 观点 在 实际 问题 中 我们 没有 
产生 \ \ overline { g } \ mathbf { 
x } \ 的 多个 样 本集 我们 只有 一个样 
本集 因此 较 简单 的 模型 \ \ mathcal { 
H } \ 较 简单 可能 会 产生 一个 较小 
的 样本 外 误差 但是 随着 \ N \ 的 
增加 \ var \ 会 逐渐 减小 此时 \ bias 
\ 是 \ E _ { out } \ 的 
主要 组成 较 复杂 的 模型 \ \ mathcal { 
H } \ 较 复杂 的 表现 会 更好 与 
VC 分析 的 一些 差异 在 VC 分析 中 \ 
E _ { out } \ 被 认为 是 \ 
E _ { in } \ 和 以 \ \ 
Omega N \ mathcal { H } \ delta \ 
为 上界 的 泛化 误差 的 和 在 偏差 方差分析 
里 中 \ E _ { out } \ 被 
看作 是 偏差 和 方差 的 和 不过 随着 样本数 
的 增多 不管 是 泛化 误差 还是 方差 都在 降低 
VC 分析 独立 于 学习 算法 \ \ mathcal { 
A } \ 而 偏差 方差分析 中 假设 空间 \ 
\ mathcal { H } \ 和 \ \ mathcal 
{ A } \ 都对 结果 产生 影响 相同 的 
\ \ mathcal { H } \ 下 不同 的 
\ \ mathcal { A } \ 将 产生 不同 
的 \ g ^ { \ mathcal { D } 
} \ 最终 产生 不同 的 偏差 和 方差 VC 
分析 要求 最小化 样 本集 的 0 1 误差 即 
\ E _ { in } \ 而 尽管 偏差 
方差分析 是 基于 平方 误差 计算 但是 学习 算法 并不 
一定 是 采取 最小化 平方 误差 的 策略 来 产生 
\ g ^ { \ mathcal { D } } 
\ 不过 一旦 产生 \ g ^ { \ mathcal 
{ D } } \ 我们 就 用 平方 误差 
法来/nr 计算 \ g ^ { \ mathcal { D 
} } \ 的 偏差 和 方差 三 补充 误差 
和 噪声 误差 测量 总体 误差 \ E h f 
\ 每 一个 点 的 误差 \ e h \ 
mathbf { x } f \ mathbf { x } 
\ \ E h f \ 可以 是 每 一个 
点 的 误差 和的/nr 平均值 也 可以 是 用户 自定义 
的 函数 在 之前 的 讨论 中 \ E _ 
{ in } h = \ frac { 1 } 
{ N } \ sum _ { i = 1 
} ^ { N } e h \ mathbf { 
x } _ i f \ mathbf { x } 
_ i \ \ E _ { out } h 
= E _ { \ mathbf { x } } 
e h \ mathbf { x } f \ mathbf 
{ x } \ 有 两种 类型 的 误差 错误 
接受 false accept 和 错误 拒绝 false reject 误差 测量 
的 方式 应该 取决于 我们 设计 的 系统 的 用途 
我们 根据 FA 和 FR 两者 不同 错误 成本 也 
可以 称作 权重 来 考虑 不同 的 算法 设计 策略 
噪声 目标 目标函数 不一定 是 一个 函数 数据 总是 在 
存在 噪声 的 情况 下 生成 因此 我们 不 使用 
函数 \ y = f \ mathbf { x } 
\ 而是 选择 概率分布 \ P \ { y | 
\ mathbf { x } \ } \ 即 \ 
\ mathbf { x } y \ 由 联合 概率分布 
\ P \ { y | \ mathbf { x 
} \ } P \ { \ mathbf { x 
} \ } \ 独立 生成 此时 噪声 目标 就 
等于 确定性 目标 \ f \ mathbf { x } 
\ 加上 噪声 部分 \ y f \ mathbf { 
x } \ 通常 来说 我们 对 学习 可行性 的 
分析 适用于 噪声 目标 后记 本文 是 笔者 在 学习 
加州理工学院 公开课 机器学习 与 数据挖掘 时的/nr 一些 笔记 后来 阅读 
了 配套 教材 Learning From Data 将 该书 所 介绍 
的 学习 可行性 的 推导 证明 进行 了 整理 