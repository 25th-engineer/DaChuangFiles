kmeans 聚 类 相信 大家 都 已经 很 熟悉 了 
在 Python 里 我们 用 kmeans 通常 调用 Sklearn 包 
当然 自己 写 也 很简单 那么/r 在/p Spark/w 里/f 能/v 
不能/v 也/d 直接/ad 使用/v sklean/w 包呢/nr 目前 来说 直接 使用 
有点 困难 不过 我 看到 spark packages 里 已经 有了/nr 
但 还 没有 发布 不过 没关系 PySpark 里 有 ml 
包 除了 ml 包 还 可以 使用 MLlib 这个 在 
后期 会写 也很 方便 首先 来看 一下 Spark 自带 的 
例子 1 from pyspark . mllib . linalg import Vectors 
2 from pyspark . ml . clustering import KMeans 3 
from pyspark . sql import SQLContext 4 from pyspark . 
mllib . linalg import Vectors 5 # 导入 数据 6 
data = Vectors . dense 0.0 0.0 Vectors . dense 
1.0 1.0 Vectors . dense 9.0 8.0 Vectors . dense 
8.0 9.0 7 df = sqlContext . createDataFrame data features 
8 # kmeans 模型 9 kmeans = KMeans k = 
2 seed = 1 10 model = kmeans . fit 
df 11 # 簇 心 数量 12 centers = model 
. clusterCenters 13 len centers 14 # 2 15 # 
训练 模型 16 transformed = model . transform df . 
select features prediction 17 rows = transformed . collect 18 
rows 0 . prediction = = rows 1 . prediction 
19 # True 20 rows 2 . prediction = = 
rows 3 . prediction 21 # True 这个 例子 很 
简单 导入 的 数据 是 四个 稠密 向量 可以 自己 
在 二维 向量 里 画 一下 设定 了 两个 簇 
心 最后 验证 预测 的 结果 是否 正确 显示 为 
True 证明 预测 正确 算法 中 具体 的 参数 可以 
参考 API 中的 说明 然而 实际 生产 中 我们 的 
数据集 不 可能 以 这样 的 方式 一条条 写进去 一般 
是 读取 文件 关于 怎么 读取 文件 可以 具体 看 
我 的 这篇 博文 这里 我们 采用 iris 数据集 不要 
问 我 为什么 又是 iris 数据集 因为 真的 太 方便 
了 来 给 大家 讲解 一下 我 的 数据 集 
是 csv 格式 的 而 Spark 又 不能 直接 读取 
csv 格式 的 数据 这里 我们 有 两个 方式 一是 
我 提到 的 这篇 博文 里 有写 怎么 读取 csv 
文件 二 是 安装 spark csv 包 在 这里 下载 
github 地址 在 这里 按照 步骤 安装 可以了 这里 友情 
提示 一下 大家 github 的 安装 方法 是 $ SPARK 
_ HOME / bin / spark shell packages com . 
databricks spark csv _ 2.11 1 . 4.0 如 果报 
错了 可以 把 packages 换成 jars 如果 还是 不行 在 
加 一个 common csv . jars 包 放到 lib 下面 
就 可以 了 我 因为 这个 耽误 了 不少 时间 
不过 具体 问题 也 得 具体 分析 安装 好 这个 
包 以后 就 可以 读取数据 了 1 from pyspark . 
sql import SQLContext 2 sqlContext = SQLContext sc 3 data 
= sqlContext . read . format com . databricks . 
spark . csv . options header = true inferschema = 
true . load iris . csv 4 data . show 
读取数据 以后 我们 来看 一下 数据集 1 + + + 
+ + + + 2 | row . id | 
Sepal . Length | Sepal . Width | Petal . 
Length | Petal . Width | Species | 3 + 
+ + + + + + 4 | 1 | 
5.1 | 3.5 | 1.4 | 0.2 | 0 | 
5 | 2 | 4.9 | 3.0 | 1.4 | 
0.2 | 0 | 6 | 3 | 4.7 | 
3.2 | 1.3 | 0.2 | 0 | 7 | 
4 | 4.6 | 3.1 | 1.5 | 0.2 | 
0 | 8 | 5 | 5.0 | 3.6 | 
1.4 | 0.2 | 0 | 9 | 6 | 
5.4 | 3.9 | 1.7 | 0.4 | 0 | 
10 | 7 | 4.6 | 3.4 | 1.4 | 
0.3 | 0 | 11 | 8 | 5.0 | 
3.4 | 1.5 | 0.2 | 0 | 12 | 
9 | 4.4 | 2.9 | 1.4 | 0.2 | 
0 | 13 | 10 | 4.9 | 3.1 | 
1.5 | 0.1 | 0 | 14 | 11 | 
5.4 | 3.7 | 1.5 | 0.2 | 0 | 
15 | 12 | 4.8 | 3.4 | 1.6 | 
0.2 | 0 | 16 | 13 | 4.8 | 
3.0 | 1.4 | 0.1 | 0 | 17 | 
14 | 4.3 | 3.0 | 1.1 | 0.1 | 
0 | 18 | 15 | 5.8 | 4.0 | 
1.2 | 0.2 | 0 | 19 | 16 | 
5.7 | 4.4 | 1.5 | 0.4 | 0 | 
20 | 17 | 5.4 | 3.9 | 1.3 | 
0.4 | 0 | 21 | 18 | 5.1 | 
3.5 | 1.4 | 0.3 | 0 | 22 | 
19 | 5.7 | 3.8 | 1.7 | 0.3 | 
0 | 23 | 20 | 5.1 | 3.8 | 
1.5 | 0.3 | 0 | 24 + + + 
+ + + + 25 only showing top 20 rows 
第二步 提取 特征 我们 在上 一步 导入 的 数据 中 
label 是 String 类型 的 但在 Spark 中 要 变成 
数值 型 才能 计算 不然 就会 报错 可以 利用 StringIndexer 
功能 将 字符串 转化 为 数值 型 1 from pyspark 
. ml . feature import StringIndexer 2 3 feature = 
StringIndexer inputCol = Species outputCol = targetlabel 4 target = 
feature . fit data . transform data 5 target . 
show targetlabel 这 一列 就是 Species 转化成 数值 型 的 
结果 1 + + + + + + + + 
2 | row . id | Sepal . Length | 
Sepal . Width | Petal . Length | Petal . 
Width | Species | targetlabel | 3 + + + 
+ + + + + 4 | 1 | 5.1 
| 3.5 | 1.4 | 0.2 | 0 | 0.0 
| 5 | 2 | 4.9 | 3.0 | 1.4 
| 0.2 | 0 | 0.0 | 6 | 3 
| 4.7 | 3.2 | 1.3 | 0.2 | 0 
| 0.0 | 7 | 4 | 4.6 | 3.1 
| 1.5 | 0.2 | 0 | 0.0 | 8 
| 5 | 5.0 | 3.6 | 1.4 | 0.2 
| 0 | 0.0 | 9 | 6 | 5.4 
| 3.9 | 1.7 | 0.4 | 0 | 0.0 
| 10 | 7 | 4.6 | 3.4 | 1.4 
| 0.3 | 0 | 0.0 | 11 | 8 
| 5.0 | 3.4 | 1.5 | 0.2 | 0 
| 0.0 | 12 | 9 | 4.4 | 2.9 
| 1.4 | 0.2 | 0 | 0.0 | 13 
| 10 | 4.9 | 3.1 | 1.5 | 0.1 
| 0 | 0.0 | 14 | 11 | 5.4 
| 3.7 | 1.5 | 0.2 | 0 | 0.0 
| 15 | 12 | 4.8 | 3.4 | 1.6 
| 0.2 | 0 | 0.0 | 16 | 13 
| 4.8 | 3.0 | 1.4 | 0.1 | 0 
| 0.0 | 17 | 14 | 4.3 | 3.0 
| 1.1 | 0.1 | 0 | 0.0 | 18 
| 15 | 5.8 | 4.0 | 1.2 | 0.2 
| 0 | 0.0 | 19 | 16 | 5.7 
| 4.4 | 1.5 | 0.4 | 0 | 0.0 
| 20 | 17 | 5.4 | 3.9 | 1.3 
| 0.4 | 0 | 0.0 | 21 | 18 
| 5.1 | 3.5 | 1.4 | 0.3 | 0 
| 0.0 | 22 | 19 | 5.7 | 3.8 
| 1.7 | 0.3 | 0 | 0.0 | 23 
| 20 | 5.1 | 3.8 | 1.5 | 0.3 
| 0 | 0.0 | 24 + + + + 
+ + + + 25 only showing top 20 rows 
最后 一步 模型 训练 和 验证 1 from pyspark . 
sql import Row 2 from pyspark . ml . clustering 
import KMeans 3 from pyspark . mllib . linalg import 
Vectors 4 5 # 把 数据格式 转化成 稠密 向量 6 
def transData row 7 return Row label = row targetlabel 
8 features = Vectors . dense row Sepal . Length 
9 row Sepal . Width 10 row Petal . Length 
11 row Petal . Width 12 13 # 转化成 Dataframe 
格式 14 transformed = target . map transData . toDF 
15 kmeans = KMeans k = 3 16 model = 
kmeans . fit transformed 17 18 predict _ data = 
model . transform transformed 19 20 train _ err = 
predict _ data . filter predict _ data label = 
predict _ data prediction . count 21 total = predict 
_ data . count 22 print traing _ err total 
float train _ err / total 到这 一步 就 结束 
了 总结 一下 用 pyspark 做 机器学习 时 数据格式 要 
转成 需要 的 格式 不然 很 容易 出错 下周 写 
pyspark 在 机器 学习 中 如何 做 分类 