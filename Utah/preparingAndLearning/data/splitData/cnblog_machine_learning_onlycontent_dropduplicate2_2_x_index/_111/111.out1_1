Spark 提供 了 常用 机器学习 算法 的 实现 封/q 装于/i 
spark/w ./i ml/w 和/c spark/w ./i mllib/w 中/f ./i spark/w 
./i mllib/w 是/v 基于/p RDD/w 的/uj 机器学习/i 库/n spark . 
ml 是 基于 DataFrame 的 机器学习 库 . 相对于 RDD 
DataFrame 拥有 更 丰富 的 操作 API 可以 进行 更 
灵活 的 操作 . 目前 spark . mllib 已经 进入 
维护 状态 不再 添加 新 特性 . 本文 将 重点 
介绍 pyspark . ml 测试环境 为 Spark 2.1 Python API 
. 首先 介绍 pyspark . ml 中的 几个 基类 ML 
DataSet 即为 pyspark . sql . DataFrame 作为 数据集 使用 
pyspark . ml . Transformer 代表 将 数据集 转换 到 
另一个 数据集 的 算法 pyspark . ml . Estimator 代表 
根据 数据 和 参数 创建 模型 的 算法 包含 方法 
fit dataset params 根据 训练 数据集 和 参数 进行 训练 
返回 训 练好 的 模型 对象 pyspark . ml . 
Model 代表 训 练好 的 模型 的 基类 通常 由 
Estimator . fit 创建 . 包含 的 方法 有 transform 
df 将 输入 数据集 代入 模型 变换 为 输出 数据集 
save path 保存 训 练好 的 模型 load path 从文件 
中 加载 模型 pyspark . ml . Pipeline 用于 将 
多个 步骤 组合 为 管道 进行 处理 可以/c 建立/v 线性/n 
管道/n 和/c 有向/i 无/v 环/v 图/n 管道/n ./i pyspark/w ./i 
ml/w 下/f 将/d 不同/a 算法/n 封装/v 到/v 不同/a 的/uj 包中/nr 
pyspark . ml . linalg 线性代数 工具包 . 包括 V/w 
e/w c/w t/w o/w r/w D/w e/w n/w s/w e/w 
V/w e/w c/w t/w o/w r/w p/w a/w r/w s/w 
e/w V/w e/w c/w t/w o/w r/w M/w a/w t/w 
r/w i/w x/w D/w e/w n/w s/w e/w M/w a/w 
t/w r/w i/w x/w p/w a/w r/w s/w e/w M/w 
a/w t/w r/w i/w x/w p/w y/w s/w p/w a/w 
r/w k/w ./i ml/w ./i feature/w 特征/n 和/c 预处理/vn 算法/n 
包./nr 包括 T o k e n i z e 
r N o r m a l i z e 
r t o p W o r d s R 
e m o v e r P C A N 
G r a m W o r d 2 V 
e c p y s p a r k . 
ml . classification 分类 算法 包./nr 包括 L o g 
i s t i c R e g r e 
s s i o n D e c i s 
i o n T r e e C l a 
s s i f i e r R a n 
d o m F o r e s t C 
l a s s i f i e r N 
a i v e B a y e s M 
u l t i l a y e r P 
e r c e p t r o n C 
l a s s i f i e r O 
n e V s R e s t p y 
s p a r k . ml . clustering 聚 
类 算法 包./nr 包括 KMeansLDApyspark . ml . regression 回归 
算法 包./nr 包括 L i n e a r R 
e g r e s s i o n G 
e n e r a l i z e d 
L i n e a r R e g r 
e s s i o n D e c i 
s i o n T r e e R e 
g r e s s o r R a n 
d o m F o r e s t R 
e g r e s s o r p y 
s p a r k . ml . recommendation 推荐 
系统 算法 包./nr 包括 ALSpyspark . ml . tuning 校验 
工具包 pyspark . ml . evaluation 评估 工具包 pyspark . 
ml 中的 算法 大多数 为 Estimator 的 派生类 . 大多数 
算法 类 均 拥有 对应 的 Model 类 . 如 
classification . NaiveBayes 和 classification . NaiveBayesModel . 算法 类 
的 fit 方法 可以 生成 对应 的 Model 类 . 
应用 示例 pyspark . ml 使用 了 统一 风格 的 
接口 这里 只 展示 部分 算法 . 首先 用 NaiveBayes 
分类器 做 一个 二 分类 from pyspark . sql import 
Row from pyspark . ml . linalg import Vectors df 
= spark . createDataFrame . . . Row label = 
0.0 weight = 0.1 features = Vectors . dense 0.0 
0.0 . . . Row label = 0.0 weight = 
0.5 features = Vectors . dense 0.0 1.0 . . 
. Row label = 1.0 weight = 1.0 features = 
Vectors . dense 1.0 0.0 nb = NaiveBayes smoothing = 
1.0 modelType = multinomial weightCol = weight model = nb 
. fit df # 构造 模型 test0 = sc . 
parallelize Row features = Vectors . dense 1.0 0.0 . 
toDF result = model . transform test0 . head # 
预测 result . prediction 1.0 result . probability DenseVector 0.32 
. . . 0.67 . . . result . rawPrediction 
DenseVector 1.72 . . . 0.99 . . . model 
. transform 将 输入 的 一行 Row 作为 一个 样本 
产生 一行 输出 . 这里 我们 只 输入 了 一个 
测试 样本 所以 直接 使用 head 取出 唯 一一行 输出 
. 使用 L o g i s t i c 
R e g r e s s i o n 
和 OneVsRest 做多 分类 from pyspark . sql import Row 
from pyspark . ml . linalg import Vectors df = 
sc . parallelize . . . Row label = 0.0 
features = Vectors . dense 1.0 0.8 . . . 
Row label = 1.0 features = Vectors . sparse 2 
. . . Row label = 2.0 features = Vectors 
. dense 0.5 0.5 . toDF lr = L o 
g i s t i c R e g r 
e s s i o n maxIter = 5 regParam 
= 0.01 ovr = OneVsRest classifier = lr model = 
ovr . fit df # 进行 预测 test0 = sc 
. parallelize Row features = Vectors . dense 1.0 0.0 
. toDF model . transform test0 . head . prediction 
1.0 test1 = sc . parallelize Row features = Vectors 
. sparse 2 0 1.0 . toDF model . transform 
test1 . head . prediction 0.0 test2 = sc . 
parallelize Row features = Vectors . dense 0.5 0.4 . 
toDF model . transform test2 . head . prediction 2.0 
使用 PCA 进行 降 维 from pyspark . ml . 
linalg import Vectors data = Vectors . sparse 5 1 
1.0 3 7.0 . . . Vectors . dense 2.0 
0.0 3.0 4.0 5.0 . . . Vectors . dense 
4.0 0.0 0.0 6.0 7.0 df = spark . createDataFrame 
data features pca = PCA k = 2 inputCol = 
features outputCol = pca _ features model = pca . 
fit df model . transform df . head . pca 
_ features DenseVector 1.648 . . . 4.013 . . 
. Estimator 和 Transformer 均为 PipelineStage 的 派生类 pipeline 由 
一系列 Stage 组成 . 调用 pipeline 对象 的 fit 方法 
将会 依次 执行 Stage 并 生成 一个 最 终模型 . 
from pyspark . ml import Pipeline from pyspark . ml 
. classification import L o g i s t i 
c R e g r e s s i o 
n from pyspark . ml . feature import HashingTF Tokenizer 
data = 0 a b c d e spark 1.0 
1 b d 0.0 2 spark f g h 1.0 
3 hadoop mapreduce 0.0 df = spark . createDataFrame data 
id text label # build pipeline tokenizer = Tokenizer inputCol 
= text outputCol = words hashingTF = HashingTF inputCol = 
tokenizer . getOutputCol outputCol = features lr = L o 
g i s t i c R e g r 
e s s i o n maxIter = 10 regParam 
= 0.001 pipeline = Pipeline stages = tokenizer hashingTF lr 
# train model = pipeline . fit df data2 = 
4 spark i j k 5 l m n 6 
spark hadoop spark 7 apache hadoop test = spark . 
createDataFrame data2 id text result = model . transform test 
result = result . select id text probability prediction result 
. collect Row id = 4 text = u spark 
i j k probability = DenseVector 0.1596 0.8404 prediction = 
1.0 Row id = 5 text = u l m 
n probability = DenseVector 0.8378 0.1622 prediction = 0.0 Row 
id = 6 text = u spark hadoop spark probability 
= DenseVector 0.0693 0.9307 prediction = 1.0 Row id = 
7 text = u apache hadoop probability = DenseVector 0.9822 
0.0178 prediction = 0.0 本文 示例 来源于 官方 文档 更多 
内容 请 参考 pyspark . ml 文档 Spark ML 编程 
指导 