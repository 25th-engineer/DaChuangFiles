注 最近 在 工作 中 高频率 的 接触 到 了 
SVM 模型 而且 还有 使用 SVM 模型 做 回归 的 
情况 即 SVR 另外 考虑 到 自己 从 第一 次 
知道 这个 模型 到 现在 也 差不多 两年 时间 了 
从最/nr 开始/v 的/uj 腾云驾雾/l 到/v 现在/t 有了/nr 一点/m 直观/n 的/uj 
认识/v 花费 了 不少 时间 因此 在 这里 做 个 
总结 比较 一下 使用 同 一个 模型 做 分类 和 
回归 之间 的 差别 也 纪念 一下 与 SVM 相遇 
的 两周年 这篇 总结 不会 涉及 太多 公式 只是 希望 
通过 可视化 的 方法 对 SVM 有 一个 比较 直观 
的 认识 由于 代码 比较 多 没有 放到 正 文中 
所有 代码 都 可以 在 github 中 link0 . 支持 
向量 机 support vector machine SVM 原始 SVM 算法 是由 
弗拉基米尔 万普 尼克 和亚/nr 历克塞 泽/nr 范兰/nr 杰斯/l 于/p 1963年/tdq 
发明/v 的/uj 1992年 Bernhard E . Boser Isabelle M . 
Guyon 和 弗拉基米尔 万普 尼克 提出 了 一种 通过 将 
核 技巧 应用于 最大 间隔 超平面 来 创建 非线性 分类器 
的 方法 当前 标准 的 前身 软 间隔 由 Corinna 
Cortes/w 和/c Vapnik/w 于/p 1993年/tdq 提出/v 并于 1995年 发表 上个世纪 
90 年代 由于 人工神经网络 RNN 的 衰落 SVM 在 很长 
一段 时间 里 都是 当时 的 明星 算法 被 认为 
是 一种 理论 优美 且 非常 实用 的 机器学习 算法 
在 理论 方面 SVM 算法 涉及到 了 非常 多 的 
概念 间隔 margin 支持 向量 support vector 核 函数 kernel 
对偶 duality 凸 优化 等 有些 概念 理解 起来 比较 
困难 例如 kernel trick 和 对偶 问题 在 应用 方法 
SVM/w 除了/p 可以/c 当做/v 有/v 监督/vn 的/uj 分类/n 和/c 回归模型/i 
来/v 使用/v 外/f 还 可以 用在 无 监督 的 聚 
类 及 异常 检测 相对于 现在 比较 流行 的 深度 
学习 适用 于 解决 大 规模 非线性 问题 SVM 非常 
擅长 解决 复杂 的 具有 中 小规模 训练 集 的 
非线性 问题 甚至在/i 特征/n 多于/d 训练样本/n 时/n 也能/i 有/v 非常/d 
好/a 的/uj 表现/v 深度 学习 此时 容易 过拟合 但是 随着 
样本量 $ m $ 的 增加 SVM 模型 的 计算 
复杂度 会 呈 $ m ^ 2 $ 或 $ 
m ^ 3 $ 增加 在 下面 的 例子 中 
均 使用 上 一篇 博客 中 提到 的 鸢尾属 植物 
数据集 Iris data set1 . SVM 的 前身 感知机 Perceptron 
感知机 可以 看做 是 低 配 版 的 线性 SVM 
从 数学 上 可以 证明 在 线性 可分 的 两类 
数据 中 感知机 可以 在 有限 步骤 中 计算 出 
一条 直线 或 超平面 将 这 两类 完全 分开 如果 
这 两类 距离 越近/nr 所需 的 步骤 就 越多 此时 
感知机 只 保证 给 出 一个 解 但是 解不/nr 唯一 
如下 图 所示 感知机 训练 出来 的 3个 不同 的 
线性 分类器 1.1 对 二分 类 问题 的 具体 描述 
训练样本 $ x \ in \ mathbb { R } 
^ { n } $ 标签 $ y \ in 
{ 1 1 } $ 对于 线性 分类器 来说 参数 
$ w \ in \ mathbb { R } ^ 
n $ and $ b \ in \ mathbb { 
R } $ 决策 边界 Decision boundary $ w \ 
cdot x + b = 0 $ 对于 一个 新的 
点 $ x $ 做 分类 时 预测 标签 为 
$ sign w \ cdot x + b $ 参考 
上面 的 描述 在 分类 正确 的 情况 下 如果 
一个 点 $ x $ 的 标签 为 $ y 
= 1 $ 预测值 $ w \ cdot x + 
b 0 $ 分类 为 1 标签 为 1 预测 
值 小于 0 分类 为 1 . 那么 可以 使用 
$ y w \ cdot x + b 0 $ 
来 统一 表示 分类 正确 的 情况 反之 可以 使用 
$ y w \ cdot x + b 0 $ 
来 表示 分类 错误 的 情况 1.2 代价 函数 在 
分类 正确 时 即 $ y w \ cdot x 
+ b 0 $ $ loss = 0 $ 在 
分类 错误 时 即 $ y w \ cdot x 
+ b \ leq 0 $ $ loss = y 
w \ cdot x + b $ . 1.3 算法 
的 流程 利用 随机 梯度 下降 的 方式 训练 模型 
每次 只 使用 一个 样本 根据 代价 函数 的 梯度 
更新 参数 step1 初始化 $ w = 0 b = 
0 $ step2 循环 从 训练 集 取 样本 每 
次一个 if $ y w \ cdot x + b 
\ leq 0 $ 该 样本 分类 错误 w = 
w + yxb = b + y 从 流程 上 
来看 每次 取 出 一个 样本点 训练 模型 而且 只在 
分 错 的 情况 下 更新 参数 最终 所有 样本 
都 分类 正确 时 模型 训练 过程 结束 2 . 
SVM 线性 可 分在 两类 样本 线性 可分 的 情况 
下 感知机 可以 保证 找到 一个 解 完全 正确 的 
区分 这 两类 样本 但是 解不/nr 唯一 而且 这些 决策 
边界 的 质量 也 不相同 直观 上 来看 这条 线 
两边 的 间隔 越大 越好 那么/r 有/v 没有/v 一种/m 方法/n 
可以/c 直接/ad 找到/v 这个/r 最优/d 解呢/nr 这 就是 线性 SVM 
所做 的 事情 从 直观 上 来看 约束条件 越多 对模型 
的 限制 也 就 越大 因此 解的/nr 个数 也就 越少 
感知机 的 解不/nr 唯一 那么 给 感知机 的 代价 函数 
加上 更强 的 约束条件 好像 就 可以 减 少解 的 
个数 事实上 也 是 这样 的 2.1 SVM 的 代价 
函数 在 分类 正确 时 即 $ y w \ 
cdot x + b 1 $ $ loss = 0 
$ 在 分类 错误 时 即 $ y w \ 
cdot x + b \ leq 1 $ $ loss 
= y w \ cdot x + b $ . 
比较 一下 可以 发现 原来 $ w \ cdot x 
+ b $ 只需要 大于 0 或 小于 0 就 
可以 了 但是 现在 需要 大于 1 或 小于 1 
. 在 这里 为什么 选择 1 我 还 没有 很 
直观 的 解释 但是 有 一点 非常 重要 原来 的 
决策 边界 只是 一条 直线 现在 则 变成 了 一条 
有 宽度 的 条带 原来 差异 非常 小 的 两个 
点 例如 $ w \ cdot x + b = 
0 $ 附近 的 两个 点 就 可以 被 分成 
不同 的 两类 但是 现在 至少 要 相差 $ \ 
frac { 2 } { | | w | | 
} $ 才可以 如下 图 所示 设 样 本属于 两个 
类 用 该 样本 训练 SVM 得到 的 最大 间隔 
超平面 在 超 平 面上 的 样本点 也 称为 支持 
向量 2.2 决策 边界 以及 间隔 来自 wiki 为了 统一 
起见 下面 还是 将 决策 边界 定义 为 $ w 
\ cdot x + b = 0 $ 两边 的 
边界 两条 虚线 分别为 $ w \ cdot x + 
b = 1 $ 和$w/nr \ cdot x + b 
= 1 $ 此时 只是 b 的 符号 不同 其他 
性质 都 相同 . 其中 $ w b $ 就是 
模型 训练 时 需要 优化 的 参数 由 上面 的 
示意图 可以 得到 以下 信息 两条 虚线 之间 的 距离 
为 $ \ frac { 2 } { | | 
w | | } $ 待 优化 参数 $ w 
$ 的 方向 就是 决策 边界 的 法向量 方向 $ 
w $ 与 决策 边界 垂直 此时 边界 上 一 
共有 3个 点 这 三个点 也就是 此时 的 支持 向量 
下面 是 计算 两条 虚线 之间 距离 的 过程 将 
决策 边界 的 向量 表示 $ w x + b 
= 0 $ 展开 后 可以 得到 $ w1 * 
x1 + w2 * x2 + b = 0 $ 
. 转化成 截距 式 可以 得到 $ x2 = w1 
/ w2 * x1 b / w2 $ 因此 其 
斜率 为 $ w1 / w2 $ 截距 为 $ 
b / w2 $ 直线 的 方向 向量 为 $ 
1 w1 / w2 $ 可以 取 x = 1 
b = 0时 得到 y 的 值 直线 的 法向量 
为 $ w = w1 w2 $ 因此 对于 直线 
$ w \ cdot x + b = 1 $ 
来说 截距 式 为 $ x2 = w1 / w2 
* x1 + 1 b / w2 $ 相当于 沿着 
$ x2 $ 轴 向上 平 移了 $ \ frac 
{ 1 } { w _ 2 } $ 计算 
可 得该 直线 与 $ w \ cdot x + 
b = 0 $ 沿 法向量 方向 的 距离 为 
$ \ gamma = \ sqrt { \ frac { 
1 } { w _ 1 ^ 2 + w 
_ 2 ^ 2 } } = \ frac { 
1 } { | | w | | } $ 
参考 . margin 的 宽度 $ \ gamma $ 2.3 
优化 目标 在 SVM 中 优化 的 目标 就是 最大化 
margin 的 宽度 $ \ gamma $ 因为 $ \ 
gamma = \ frac { 1 } { | | 
w | | } $ 其中 $ | | w 
| | $ 是 待 优化 参数 $ w $ 
的 模 长 因此 优化 目标 等价 于 最小化 $ 
| | w | | $ 可以 表示 为 为 
对于 $ x ^ { 1 } y ^ { 
1 } \ . . . \ x ^ { 
m } y ^ { m } \ in \ 
mathbb { R ^ d } \ times \ { 
1 1 \ } $ $ \ min _ { 
w \ in \ mathbb { R } ^ d 
b \ in \ mathbb { R } } | 
| w | | ^ 2 $ s . t 
. $ y ^ { i } w \ cdot 
x ^ { i } + b ≥ 1 $ 
对于 所有 的 $ i = 1 2 . . 
. m/w $/i 成立/v 下面/f 是/v 分别/d 使用/v 感知机/n 和/c 
SVM/w 对/p 鸢尾属/i 数据/n 集中/v setosa/w 这一类/i 和非/nr setosa/w 进行/v 
分类/n 的/uj 效果/n 比较/d 感知机 线性 分类器 线性 SVM 的 
分类 效果 比较 和 可以 看到 SVM 确定 的 决策 
边界 周围 的 margin 更大 一些 因此 对 更多 未知 
的 样本 进行 分类 时 在 边界 上 的 一些 
点 可以 得到 更 准确 的 分类 结果 3 . 
SVM 线性 不可 分在 中 可以 看到 setosa 这一类 与 
其他 两类 是 线性 可分 的 但是 virginica 这一类 与与 
之 相邻 的 versicolor 有 一些 点 是 重合 的 
也 就是说 是 线性 不可分的 此时 仍然 可以 使用 SVM 
来 进行 分类 原理 是 在 代价 函数 中 加入 
了 一个 松弛 变量 slack $ \ xi $ 对于 
$ x ^ { 1 } y ^ { 1 
} \ . . . \ x ^ { m 
} y ^ { m } \ in \ mathbb 
{ R ^ d } \ times \ { 1 
1 \ } $ $ \ min _ { w 
\ in \ mathbb { R } ^ d b 
\ in \ mathbb { R } } | | 
w | | ^ 2 + C \ sum _ 
{ i = 1 } ^ { m } { 
\ xi ^ i } $ s . t . 
$ y ^ { i } w \ cdot x 
^ { i } + b ≥ 1 \ xi 
_ i $ 对于 所有 的 $ i = 1 
2 . . . m $ 成立 上面 的 优化 
目标 加入 松弛 变量 后 就 可以 允许 一定 程度 
的 违反 两边 的 边界 由 上式 中的 C 来 
控制 允许 一定 的 错误 分类 从而 将 两类 原来 
线性 不 可分 的 两类 数据 分开 下面 是 $ 
C = 1000 $ 时 对 virginica 和非/nr virginica 的 
分类 效果 加入 松弛 变量 后的/nr SVM 分类 效果 C 
作为 SVM 模型 的 超 参数 之一 需要 从 一个 
较大 的 范围 中 一步 一步 的 筛选 直到 找到 
最 适合 的 C C 值 越大 表示 错误 分类 
的 代价 越大 就越 趋于 拒绝 错误 分类 即 hard 
margin C 值 越小 表示 错误 分类 的 代价 越小 
就越 能 容忍 错误 分类 即 soft margin 即使 是 
在 线性 可分 的 情况 下 如果 C 设置 的 
非常 小 也 可能 导致 错误 分类 的 出现 在 
线性 不 可分 的 情况 下 设置 过大 的 C 
值 会 导致 训练 无法 收敛 4 . SVR 利用 
SVM 做 回归分析 支持 向量 回归模型 Support Vector Regression SVR 
是 使用 SVM 来 拟合 曲线 做 回归分析 分类/n 和/c 
回归/v 问题/n 是/v 有/v 监督/vn 机器学习/i 中/f 最重要/i 的/uj 两类/m 
任务/n 与 分类 的 输出 是 有限 个 离散 的 
值 例如 上面 的 $ \ { 1 1 \ 
} $ 不同 的 是 回归模型 的 输出 在 一定 
范围 内 是 连续 的 下面 不再 考虑 不同 鸢尾花 
的 类型 而是 使用 花瓣 的 长度 相当于 自变量 x 
来 预测 花瓣 的 宽度 相当于 因变量 y 下 图中 
从 所有 150个 样本 中 随机 取出 了 80% 作为 
训练 集 训练 SVR 模型 的 训练样本 下面 是 使用 
线性 SVR 训练 出来 的 回归线 SVR 模型 训练 出来 
的 回归线 与 SVM 是 使用 一个 条 带来 进行 
分类 一样 SVR 也是 使用 一个 条 带来 拟合 数据 
这个 条带 的 宽度 可以 自己 设置 利用 参数 $ 
\ epsilon $ 来 控制 SVR 模型 回归 效果 示意图 
其中 带红色 环 的 点 表示支持 向量 在 SVM 模型 
中 边界 上 的 点 以及 两条 边界 内部 违反 
margin 的 点 被 当做 支持 向量 并且 在 后续 
的 预测 中 起作用 在 SVR 模型 中 边界 上 
的 点 以及 两条 边界 以外 的 点 被 当做 
支持 向量 在 预测 中 起作用 按照 对偶 形式 的 
表示 最终 的 模型 是 所有 训练样本 的 线性组合 其他 
不 是 支持 向量 的 点 的 权重 为 0 
. 下面 补充 SVR 模型 的 代价 函数 的 图形 
soft margin SVR 的 代价 函数 从中 可以 看到 在 
margin 内部 的 这些 点 的 error 都为 0 只有 
超出 了 margin 的 点 才会 计算 error 因此 SVR 
的 任务 就是 利用 一条 固定 宽度 的 条带 宽度 
由 参数 $ \ epsilon $ 来 控制 覆盖 尽可能 
多 的 样本点 从而 使得 总 误差 尽可能 的 小 
Referencehttps / / zh . wikipedia . org / wiki 
/ % E 6% 94% AF % E 6% 8C 
% 81% E 5% 90% 91% E 9% 87% 8F 
% E 6% 9C % BAhttps / / zhuanlan . 
zhihu . com / p / 26263309   直线 方程 
的 各种 形式 https / / github . com / 
ageron / handson ml / blob / master / 05 
_ support _ vector _ machines . ipynbhttp / / 
www . svms . org / regression / SmSc98 . 
pdfhttp / / www . robots . ox . ac 
. uk / ~ az / lectures / ml / 
edx UCSanDiegoX DSE220x   Machine Learning F u n d 
a m e n t a l s h t 
t p s / / github . com / OnlyBelter 
/ jupyter note / blob / master / machine _ 
learning / SVM / 04 _ how % 20SVM % 
20becomes % 20to % 20SVR . ipynb 文中 代码 