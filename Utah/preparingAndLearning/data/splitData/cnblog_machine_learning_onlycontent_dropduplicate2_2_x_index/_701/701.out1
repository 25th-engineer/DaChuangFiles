注 最近 开始 学习 人工智能 选修课 老师 提纲挈领 的 介绍 
了 一番 听完 课 只 了解 了 个 大概 剩下 
的 细节 只能 自己 继续 摸索 从 本质 上 讲 
机器学习 就是 一个 模型 对 外界 的 刺激 训练样本 做出 
反应 趋利避害 评价 标准 1 . 什么 是 逻辑 回归 
许多人 对 线性 回归 都 比较 熟悉 但 知道 逻辑 
回归 的 人 可能 就要 少 的 多 从大的/nr 类别 
上 来说 逻辑 回归 是 一种 有 监督 的 统计 
学习 方法 主要 用于 对 样本 进行 分类 在 线性 
回归模型 中 输出 一般 是 连续 的 例如 $ $ 
y = f x = ax + b $ $ 
对于 每一个 输入 的 x 都 有一个 对应 的 y 
输出 模型/n 的/uj 定义域/n 和/c 值域/n 都/d 可以/c 是/v ∞ 
+ ∞ 但是 对于 逻辑 回归 输入 可以 是 连续 
的 ∞ + ∞ 但 输出 一般 是 离散 的 
即 只有 有限 多个 输出 值 例如 其 值域 可以 
只有 两个 值 { 0 1 } 这 两个 值 
可以 表示 对 样本 的 某种 分类 高/低/nr 患病 / 
健康 阴性 / 阳性 等 这 就是 最 常见 的 
二分 类 逻辑 回归 因此 从 整体 上 来说 通过 
逻辑 回归模型 我们 将 在整个 实数 范围 上 的 x 
映 射到 了 有限 个 点上 这样 就 实现 了 
对 x 的 分类 因为 每次 拿 过来 一个 x 
经过 逻辑 回归分析 就 可以 将 它 归入 某一 类 
y 中 逻辑 回归 与 线性 回归 的 关系 逻辑 
回归 也 被 称为 广义 线性 回归模型 它 与 线性 
回归模型 的 形式 基本上 相同 都 具有 ax + b 
其中 a 和b是/nr 待 求 参数 其 区别 在于 他们 
的 因变量 不同 多重 线性 回归 直接 将 ax + 
b 作为 因变量 即 y = ax + b 而 
logistic 回归 则 通过 函数 将 ax + b 对应 
到 一个 隐 状态 p p = S ax + 
b 然后 根据 p 与 1 p 的 大小 决定 
因变量 的 值 这里 的 函数 就是 Sigmoid 函数 $ 
$ t = \ frac { 1 } { 1 
+ e ^ { t } } $ $ 将 
t 换成 ax + b 可以 得到 逻辑 回归模型 的 
参数 形式 $ $ p x a b = \ 
frac { 1 } { 1 + e ^ { 
ax + b } }   1 $ $ sigmoid 
函数 的 图像 通过 函数 的 作用 我们 可以 将 
输出 的 值 限制 在 区间 0 1 上 p 
x 则 可以 用来 表示 概率 p y = 1 
| x 即 当 一个 x 发生 时 y 被 
分到 1 那 一组 的 概率 可是 等等 我们 上面 
说 y 只有 两种 取值 但是 这里 却 出现 了 
一个 区间 0 1 这 是 什么 鬼 其实 在 
真实 情况 下 我们 最终 得到 的 y 的 值 
是 在 0 1 这个 区间 上 的 一个 数 
然后 我们 可以 选择 一个 阈值 通常 是 0.5 当 
y 0.5时 就将 这个 x 归到 1 这一类 如果 y 
0.5 就将 x 归到 0 这一类 但是 阈值 是 可以 
调整 的 比如说 一个 比较 保守 的 人 可能 将 
阈值 设为 0.9 也 就是说 有 超过 90% 的 把握 
才 相信 这个 x 属于 1 这一类 了解 一个 算法 
最好 的 办法 就是 自己 从头 实现 一次 下面 是 
逻辑 回归 的 具体 实现 逻辑 回归模型 的 代价 函数 
逻辑 回归 一般 使用 交叉 熵 作为 代价 函数 关于 
代价 函数 的 具体 细节 请 参考 代价 函数 这里 
只 给出 交叉 熵 公式 $ $ J \ theta 
= \ frac { 1 } { m } \ 
sum _ { i = 1 } ^ { m 
} { y ^ { i } \ log h 
_ \ theta x ^ { i } + 1 
y ^ { i } \ log 1 h _ 
\ theta x ^ { i } } $ $ 
m 训练样本 的 个数 h θ x 用 参数 θ 
和x/nr 预测 出来 的 y 值 y 原 训练样本 中的 
y 值 也 就是 标准 答案 上 角标 i 第 
i 个 样本 2 . 数据 准备 下面 的 数据 
来自 机器学习 实战 中的 示例 0.017612 14.053064 0 1.395634 4.662541 
1 0.752157 6.538620 0 1.322371 7.152853 0 0.423363 11.054677 0 
0.406704 7.067335 1 0.667394 12.741452 0 2.460150 6.866805 1 0.569411 
9.548755 0 0.026632 10.427743 0 上面 的 数据 一共 是 
3列 10行 其中 前 两 列为 x1 和 x2 的 
值 第 3列 表示 y 的 值 10行 表示 取了 
10个 样本点 我们 可以 将 这些 数据 当做 训练 模型 
参数 的 训练样本 见到 训练样本 就 可以 比较 直观 的 
理解 算法 的 输入 以及 我们 如何 利用 这些 数据 
来 训练 逻辑 回归 分类器 进 而用 训 练好 的 
模型 来 预测 新的 样本 检测 样本 从 逻辑 回归 
的 参数 形式 式子 1 我们 可以 看到 逻辑 回归模型 
中有 两个 待定 参数 a x 的 系数 和b/nr 常数项 
我们 现在 给 出来 的 数据 有 两个 特征 x1 
x2 因此 整个 模型 就 增加 了 一项 ax1 + 
cx2 + b 为了 形式上 的 统一 我们 使用 带下 
标的 a 表示 不同 的 参数 a0 表示 常数项 b 
并作 x0 的 参数 x0 = 1 a1 a2 分别 
表示 x1 和 x2 的 参数 就 可以 得到 $ 
$ a _ 0x _ 0   + a _ 
1x _ 1   + a _ 2x _ 2 
$ $ 这样 统一 起来 后 就 可以 使用 矩阵 
表示 了 比起 前面 展开 的 线性 表示 方式 用 
矩阵 表示 模型 和 参数 更加 简便 而且 矩阵 运算 
的 速度 也 更快 $ $ \ begin { bmatrix 
} a _ 0 & a _ 1 & a 
_ 2 \ end { bmatrix }   \ begin 
{ bmatrix } x _ 0 \ \ x _ 
1 \ \ x _ 2 \ end { bmatrix 
} = a ^ { \ mathrm { T } 
} X $ $ 将上 面的 式子 带入 到 1 
式 我们 就 可以 得到 逻辑 回归 的 另一种 表示 
形式 了 $ $ p x a = \ frac 
{ 1 } { 1 + e ^ { a 
^ { \ mathrm { T } } X } 
}   2 $ $ 此时 可以 很 清楚 的 
看到 我们 后面 的 行动 都是/nr 为了 确定 一个 合适 
的 a 一个 参数 向量 使得 对于 一个 新来 的 
X 也 是 一个 向量 我们 可以 尽 可能 准确 
的 给出 一个 y 值 0 或者 1 . 注 
数据 是 二维 的 也 就是说 这组 观察 样本 中 
有 两个 自变量 即 两个 特征 feature 3 . 训练 
分类器 就像 上面 说 的 训练 分类器 的 过程 就是 
根据 已经 知道 的 数据 训练样本 确定 一个 使得 代价 
函数 的 值 最小 的 a 参数 向量 / 回归系数 
的 过程 逻辑 回归模型 属于 有 监督 的 学习 方法 
上面 示例 数据 中的 第 3列 其实 是 训练样本 提供 
的 标准答案 也 就是说 这些 数据 是 已经 分好 类 
的 两类 0 或者 1 在 训练 阶段 我们 要 
做 的 就是 利用 训练样本 和 2 式 中的 模型 
估计 一个 比较 合适 的 参数 a 使得 仅 通过 
前面 两列 数据 观察 值 / 测量 值 就 可以 
估计 一个 值 h a 这个 值 越 接近 标准 
答案 y 说明 我们 的 模型 预测 的 越 准确 
下面 是 估计 回归系数 a 的 值 的 过程 还是 
借鉴 了 机器学习 实战 中的 代码 做了 少量 修改 其中 
计算 参数 梯度 即 代价 函数 对 每个 参数 的 
偏 导数 下面 代码 中的 第 36 38行 的 详细 
推导 过程 可以 参考 这里 1 2 Created on Oct 
27 2010 3 Logistic Regression Working Module 4 @ author 
Peter 5 6 from numpy import * 7 import os 
8 9 path = D \ MechineLearning \ MLiA _ 
SourceCode \ m a c h i n e l 
e a r n i n g i n a 
c t i o n \ Ch05 10 training _ 
sample = trainingSample . txt 11 testing _ sample = 
testingSample . txt 12 13 # 从文件 中 读入 训练样本 
的 数据 同 上面 给出 的 示例 数据 14 # 
下面 第 20行 代码 中的 1.0 表示 x0 = 1 
15 def loadDataSet p file _ n 16 dataMat = 
labelMat = 17 fr = open os . path . 
join p file _ n 18 for line in fr 
. readlines 19 lineArr = line . strip . split 
20 dataMat . append 1.0 float lineArr 0 float lineArr 
1 # 三个 特征 x0 x1 x2 21 labelMat . 
append int lineArr 2 # 标准答案 y 22 return dataMat 
labelMat 23 24 def sigmoid inX 25 return 1.0 / 
1 + exp inX 26 27 # 梯度 下 降法 
求 回归系数 a 由于 样本量 少 我 将 迭代 次数 
改成 了 1000次 28 def gradAscent dataMatIn classLabels 29 dataMatrix 
= mat dataMatIn # convert to NumPy matrix 30 labelMat 
= mat classLabels . transpose # convert to NumPy matrix 
31 m n = shape dataMatrix 32 alpha = 0.001 
# 学习率 33 maxCycles = 1000 34 weights = ones 
n 1 35 for k in range maxCycles # heavy 
on matrix operations 36 h = sigmoid dataMatrix * weights 
# 模型 预测值 90 x 1 37 error = h 
labelMat # 真 实值 与 预测 值 之间 的 误差 
90 x 1 38 temp = dataMatrix . transpose * 
error # 交叉 熵 代价 函数 对 所有 参数 的 
偏 导数 3 x 1 39 weights = weights alpha 
* temp # 更新 权重 40 return weights 41 42 
# 下面 是 我 自己 写 的 测试函数 43 def 
test _ logistic _ regression 44 dataArr labelMat = loadDataSet 
path training _ sample # 读入 训练样本 中的 原始数据 45 
A = gradAscent dataArr labelMat # 回归系数 a 的 值 
46 h = sigmoid mat dataArr * A # 预测 
结果 h a 的 值 47 print dataArr labelMat 48 
print A 49 print h 50 # plotBestFit A 51 
52 test _ logistic _ regression 上面 代码 的 输出 
如下 一个 元组 包含 两个 数组 第一 个 数组 是 
所有 的 训练 样本 中 的 观察 值 也 就是 
X 包括 x0 x1 x2 第二个 数组 是 每组 观察 
值 对应 的 标准 答案 y 1.0 0.017612 14.053064 1.0 
1.395634 4.662541 1.0 0.752157 6.53862 1.0 1.322371 7.152853 1.0 0.423363 
11.054677 1.0 0.406704 7.067335 1.0 0.667394 12.741452 1.0 2.46015 6.866805 
1.0 0.569411 9.548755 1.0 0.026632 10.427743 0 1 0 0 
0 1 0 1 0 0 本次 预测 出来 的 
回归系数 a 包括 a0 a1 a2 1.39174871 0.5227482 0.33100373 根据 
回归系数 a 和 2 式 中的 模型 预测 出来 的 
h a 这里 预测 得到 的 结果 都是 区间 0 
1 上 的 实数 0.03730313 0.64060602 0.40627881 0.4293251 0.07665396 0.23863652 
0.0401329 0.59985228 0.11238742 0.11446212 标准 答案 是 { 0 1 
} 如何将 预测 到 的 结果 与 标准 答案 y 
进行 比较 呢 取 0.5 作为 阈值 大于 该 值 
的 样本 就 划分 到 1 这 一组 小于 等于 
该 值 的 样本 就 划分 到 0 这 一组 
这样 就 可以 将 数据 分为 两类 检查一下 结果 可以 
看到 我们 现在 分 出来 的 1 这一类 中 包括 
原来 y = 1 的 两个 样本 另一 类 包括 
原来 y = 0 的 所有 样本 和 一个 y 
= 1 的 样本 分 错了 鉴于 我们 选 择取 
的 样本 比较 少 只有 10个 这样 的 效果 其实 
还算 非常 不错 的 4 . 结果 展示 上面 已经 
求出 了 一组 回归系数 它 确定 了 不同 类别 数据 
之间 的 分割线 可以 利用 X 内部 x1 与 x2 
之间 的 关系 的 关系 画出 该 分割线 从而 更 
直观 的 感受 到 分类 的 效果 添加 下面 一段 
代码 1 # 分类 效果 展示 参数 weights 就是 回归系数 
2 def plotBestFit weights 3 import matplotlib . pyplot as 
plt 4 dataMat labelMat = loadDataSet path training _ sample 
5 dataArr = array dataMat 6 n = shape dataArr 
0 7 xcord1 = ycord1 = 8 xcord2 = ycord2 
= 9 for i in range n 10 if int 
labelMat i = = 1 11 xcord1 . append dataArr 
i 1 ycord1 . append dataArr i 2 12 else 
13 xcord2 . append dataArr i 1 ycord2 . append 
dataArr i 2 14 fig = plt . figure 15 
ax = fig . add _ subplot 111 16 ax 
. scatter xcord1 ycord1 s = 30 c = red 
marker = s 17 ax . scatter xcord2 ycord2 s 
= 30 c = green 18 x = arange 3.0 
3.0 0.1 19 y = weights 0 weights 1 * 
x / weights 2 # x2 = f x1 20 
ax . plot x . reshape 1 1 y . 
reshape 1 1 21 plt . xlabel X1 plt . 
ylabel X2 22 plt . show 将上 面的 test _ 
logistic _ regression 函数 中 的 最后 一句 注释 去掉 
调用 plotBestFit 函数 就 可以 看到 分类 的 效果 了 
这里 说明 一下 上面 代码 中的 第 19行 这里 设置 
了 sigmoid 函数 的 取值 为 1/2 也 就是说 取 
阈值 为 0.5 来 划分 最后 预测 的 结果 这样 
可以 得到 $ $ e ^ { a ^ { 
\ mathrm { T } } X } = 1 
$ $ 即 $ a ^ TX = 0 $ 
可以 推出 $ x _ 2   = a _ 
0x _ 0   a _ 1x _ 1 / 
a _ 2 $ 同 第 19行 也 就是说 这里 
的 $ y $ 实际上 是 $ x _ 1 
$ 而 $ x $ 是 $ x _ 1 
$ 因此 下 图 表示 的 是 $ x _ 
1 $ 与 $ x _ 2 $ 之间 的 
关系 分类 效果图 如下 三个 红色 的 点 是 原来 
$ y = 1 $ 的 样本 有 一个 分 
错了 这里 相当于 将 所有 的 数据 用 二维 坐标 
x1 x2 表示 了 出来 而且 根据 回归 参数 画出 
的 线 将 这些 点 一分为二 如果 有 新的 样本 
不 知道 在 哪 一类 只用 将 该点 画在 图上 
看 它 在 这条 直线 的 哪 一边 就 可以 
分类 了 下面 是 使用 90个 训练样本 得到 的 结果 
可以 看出 一个 非常 明显 的 规律 是 $ y 
= 1 $ 的 这一类 样本 红色 的 点 具有 
更小 的 $ x _ 2 $ 值 当 $ 
x _ 2 $ 相 近时 则 具有 更大 的 
$ x _ 1 $ 值 此时 计算 出来 的 
回归系数 a 为 5.262118 0.60847797 0.75168429 5 . 预测 新 
样本 添加 一个 预测 函数 如下 直接 将 上面 计算 
出来 的 回归系数 a 拿来 使用 测试数据 其实 也是 机器学习 
实战 这 本书 中 的 训练 数据 我 拆 成了 
两份 前面 90行 用来 做 训练 数据 后面 10行 用来 
当 测试数据 1 def predict _ test _ sample 2 
A = 5.262118 0.60847797 0.75168429 # 上面 计算 出来 的 
回归系数 a 3 dataArr labelMat = loadDataSet path testing _ 
sample 4 h _ test = sigmoid mat dataArr * 
mat A . transpose # 将 读入 的 数据 和A/nr 
转化成 numpy 中的 矩阵 5 print h _ test # 
预测 的 结果 调用 上面 的 函数 可以 得到 以下 
结果 即 h a 0.99714035 0.04035907 0.12535895 0.99048731 0.98075409 0.97708653 
0.09004989 0.97884487 0.28594188 0.00359693 下面 是 我们 的 测试数据 原来 
的 训练样本 后 十行 的 数据 包括 标准答案 y 0.089392 
0.715300 1 1.825662 12.693808 0 0.197445 9.744638 0 0.126117 0.922311 
1 0.679797 1.220530 1 0.677983 2.556666 1 0.761349 10.693862 0 
2.168791 0.143632 1 1.388610 9.341997 0 0.317029 14.739025 0 比较 
我们 预测 得到 的 h a 和 标准 答案 y 
如果 按照 0.5 为 分界线 的话 我们 利用 前 90个 
样本 训练 出来 的 分类器 对 后面 10个 样本 的 
类型 预测 全部 正确 附件 github 上 的 代码 更新 
到 python3 . 6 2019 1 6 完整 代码 https 
/ / github . com / OnlyBelter / MachineLearning _ 
examples / tree / master / de _ novo / 
regression 训练 数据 https / / github . com / 
OnlyBelter / MachineLearning _ examples / blob / master / 
de _ novo / data / Logistic _ Regression trainingSample 
. txt 测试数据 https / / github . com / 
OnlyBelter / MachineLearning _ examples / blob / master / 
de _ novo / data / Logistic _ Regression testingSample 
. txt 参考 http / / baike . baidu . 
com / item / logistic % E 5% 9B % 
9E % E 5% BD % 92https / / en 
. wikipedia . org / wiki / Sigmoid _ function 
机器学习 实战 哈林顿 著 李锐 等 译 人民邮电出版社 2013年 6月 
第一版 