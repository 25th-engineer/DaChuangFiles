一 引言 假设 我们 现 有 一些 数据 点 我们 
用 一条 直线 对 这些 点 进行 拟合 这个 拟合 
的 过程 就 称作 回归 利用 logistic 回归 进行 分类 
的 主要 思想 是 根据 现有 数据 对 分类 边界线 
建立 回归 公式 以此 进行 分类 我们 知道 logistic 回归 
主要 是 进行 二 分类 预测 也即 是 对于 0 
~ 1 之间 的 概率值 当 概率 大于 0.5 预测 
为 1 小于 0.5 预测 为 0 . 显然 我们 
不能不 提到 一个 函数 即 sigmoid = 1 / 1 
+ exp inX 该 函数 的 曲线 类似 于 一个 
s 型 在 x = 0处 函数值 为 0.5 . 
于是 为了实现 logistic 分类器 我们 可以 在 每个 特征 上都 
乘以 一个 回归系数 然后 所有 的 相乘 结果 进行 累加 
将 这个 总结 作为 输入 输入 到 sigmoid 函数 中 
从而 得到 一个 大小 为 0 ~ 1 之间 的 
值 当 该 值 大于 0.5 归类 为 1 否则 
归类 为 0 这样 就 完成 了 二 分类 的 
任务 所以 logistic 回归 可以 看成 是 一种 概率 估计 
二 基于 最优化 方法 的 最佳 回归系数 确定 sigmoid 函数 
的 输入 记为 z 即 z = w0x0 + w1x1 
+ w2x2 + . . . + wnxn 如果 用 
向量 表示 即为 z = wTx 它 表示 将 这 
两个 数值 向量 对应 元素 相乘 然后 累 加起来 其中 
向量 x 是 分类器 的 输入 数据 w 即 为 
我们 要 拟合 的 最佳 参数 从而 使 分类器 预测 
更加 准确 也 就是说 logistic 回归 最 重要 的 是 
要 找到 最佳 的 拟合 参数 1 梯度 上升 法 
梯度 上升 法 等同 于 我们 熟知 的 梯度 下 
降法 前者 是 寻找 最大值 后者 寻找 最小值 它 的 
基本 思想 是 要 找到 某 函数 的 最大值 最好 
的 方法 就是 沿着 该 函数 的 梯度方向 搜寻 如果 
函数 为 f 梯度 记为 D a 为 步长 那么 
梯度 上升 法的/nr 迭代 公式 为 w w + a 
* Dwf w 该 公式 停止 的 条件 是 迭代 
次数 达到 某个 指 定值 或者 算法 达到 某个 允许 
的 误差 范围 我们 熟知 的 梯度 下 降法 迭代 
公式 为 w w a * Dwf w 2 使用 
梯度 上升 法 寻找 最佳 参数 假设有 100个 样本点 每个 
样本 有 两个 特征 x1 和 x2 . 此外 为 
方便 考虑 我们 额外 添加 一个 x0 = 1 将 
线性函数 z = wTx + b 转为 z = wTx 
此时 向量 w 和x的/nr 维度 均价 1 . 那么 梯度 
上升 法的/nr 伪代码 如下 初始化 每个 回归系数 为 1 重复 
R 次 计算 整个 数据集 梯度 使用 alpha * gradient 
更新 回归系数 的 向量 返回 回归系数 代码 如下 # 预处理 
数据 def loadDataSet # 创建 两个 列表 dataMat = labelMat 
= # 打开 文本 数据集 fr = open testSet . 
txt # 遍历 文本 的 每 一行 for line in 
fr . readlines # 对当 前行 去除 首尾 空格 并按 
空格 进行 分离 lineArr = line . strip . split 
# 将 每 一行 的 两个 特征 x1 x2 加上 
x0 = 1 组成 列表 并 添加到 数据集 列表 中 
dataMat . append 1.0 float lineArr 0 float lineArr 1 
# 将 当 前行 标签 添加到 标签 列表 labelMat . 
append int lineArr 2 # 返回 数据 列表 标签 列表 
return dataMat labelMat # 定义 sigmoid 函数 def sigmoid inx 
return 1.0 / 1 + exp inx # 梯度 上升 
法 更新 最优 拟合 参数 # @ dataMatIn 数据集 # 
@ classLabels 数据 标签 def gradAscent dataMatIn classLabels # 将 
数据集 列表 转为 Numpy 矩阵 dataMatrix = mat dataMatIn 将 
数据集 标签 列表 转为 Numpy 矩阵 并 转置 labelMat = 
mat classLabels . transpose # 获取 数据集 矩阵 的 行数 
和 列数 m n = shape dataMat # 学习 步长 
alpha = 0.001 # 最大 迭代 次数 maxCycles = 500 
# 初始化 权值 参数 向量 每个 维度 均为 1.0 weights 
= one n 1 # 循环 迭代 次数 for k 
in range maxCycles # 求 当前 的 sigmoid 函数 预测 
概率 h = sigmoid dataMatrix * weights # * * 
* * * * * * * * * * 
* * * * * * * * * * 
* * * * * * * * * * 
* * * * * * * * * * 
* * * * * # 此处 计算 真实 类别 
和 预测 类别 的 差值 # 对 logistic 回归 函数 
的 对数 释然 函数 的 参数 项求偏/nr 导 error = 
labelMat h # 更新 权值 参数 weights = weights + 
alpha * dataMatrix . transpose * error # * * 
* * * * * * * * * * 
* * * * * * * * * * 
* * * * * * * * * * 
* * * * * * * * * * 
* * * * * return weights 这里 对 上述 
代码 的 星号 以内 的 代码 进行 相应 的 说明 
我们 知道 对 回归系数 进行 更新 的 公式 为 w 
w + alpha * gradient 其中 gradient 是 对 参数 
w 求 偏 导数 则 我们 可以 通过 求导 验证 
logistic 回归 函数 对 参数 w 的 梯度 为 yi 
sigmoid wTx * x 证明 过程 如下 所示 我们 还 
可以 通过 matpotlib 画出 决策 的 边界 集体 代码 为 
def plotBestFit wei import matplotlib . pyplot as plt weights 
= wei . getA dataMat labelMat = loadDataSet dataArr = 
array dataMat n = shape dataArr 0 xcord1 = ycord1 
= xcord2 = ycord2 = for i in range n 
if int labelMat i = = 1 xcord1 . append 
dataArr i 1 ycord1 . append dataArr i 2 else 
xcord2 . append dataArr i 1 ycord2 . append dataArr 
i 2 fig = plt . figure ax = fig 
. add _ subplot 111 ax . scatter xcord1 ycord1 
s = 30 c = red marker = s ax 
. scatter xcord2 ycord2 s = 30 c = green 
x = arange 3.0 3.0 0.1 y = weights 0 
weights 1 * x / weights 2 ax . plot 
x y plt . xlabel X1 plt . ylabel X2 
plt . show 3 随机/d 梯度/n 上升/v 法/l 我们/r 知道/v 
梯度/n 上升/v 法/l 每次/r 更新/d 回归系数/n 都/d 需要/v 遍历/v 整个/b 
数据集/i 当 样本 数量 较 小时 该 方法 尚可 但是 
当 样本数据 集 非常 大 且 特征 非常 多时 那么 
随机 梯度 下 降法 的 计算 复杂度 就会 特别高 一种 
改进 的 方法 是 一次 仅用 一个 样本 点来 更新 
回归系数 即 岁 集 梯度 上升 法 由于 可以 在 
新 样本 到来 时对/nr 分类器 进行 增量式 更新 因此 随机 
梯度 上升 法是/nr 一个 在线 学习 算法 随机 梯度 上升 
法 可以 写成 如下 伪代码 所有 回归系数 初始 化为 1对 
数据集 每个 样本 计算 该 样本 的 梯度 使用 alpha 
* gradient 更新 回顾 系 数值 返回 回归系数 值 # 
梯度 上升 算法 def stocGradAscent dataMatrix classLabels # 为 便于 
计算 转为 Numpy 数组 dataMat = array dataMatrix 获取 数据集 
的 行数 和 列数 m n = shape dataMatrix # 
初始化 权值 向量 各个 参数 为 1.0 weights = ones 
n # 设置 步 长为 0.01 alpha = 0.01 # 
循环 m 次 每次 选取 数据集 一个 样本 更新 参数 
for i in range m # 计算 当前 样本 的 
sigmoid 函数值 h = sigmoid dataMatrix i + weights # 
计算 当前 样本 的 残差 代替 梯度 error = classLabels 
i h # 更新 权值 参数 weights = weights + 
alpha * error * dataMatrix i return weights 我们 知道 
评判 一个 优化 算法 的 优劣 的 可靠 方法 是 
看 其 是否 收敛 也 就是说 参数 的 值 是否 
达到 稳 定值 此外 当 参数值 接近 稳定 时 仍然 
可能 会 出现 一些 小 的 周期性 的 波动 这种 
情况 发生 的 原因 是 样本 集 中 存在 一些 
不能 正确 分类 的 样本点 数据 集并 非线性 可分 所以 
这些 点 在 每次 迭代 时会 引发 系数 的 剧烈 
改变 造成 周期性 的 波动 显然 我们 希望 算法 能够 
避免 来回 波动 从而 收敛 到 某个 值 并且 收敛 
速度 也要 足 够快 为此 需要 对 上述 随机 梯度 
上升 法 代码 进行 适当 修改 代码 如下 # @ 
dataMatrix 数据集 列表 # @ classLabels 标签 列表 # @ 
numIter 迭代 次数 默认 150 def stocGradAscent1 dataMatrix classLabels numIter 
= 150 # 将 数据集 列表 转为 Numpy 数组 dataMat 
= array dataMatrix # 获取 数据集 的 行数 和 列数 
m n = shape dataMat # 初始化 权值 参数 向量 
每个 维度 均为 1 weights = ones n # 循环 
每次 迭代 次数 for j in range numIter # 获取 
数据集 行 下标 列表 dataIndex = range m # 遍历 
行 列表 for i in range m # 每次 更新 
参数 时 设置 动态 的 步长 且为 保证 多次 迭代 
后对新/nr 数据 仍然 具有 一定 影响 # 添加 了 固定 
步长 0.1 alpha = 4 / 1.0 + j + 
i + 0.1 # 随机 获取 样本 randomIndex = int 
random . nuiform 0 len dataIndex # 计算 当前 sigmoid 
函数值 h = sigmoid dataMat randomIndex * weights # 计算 
权值 更新 # * * * * * * * 
* * * * * * * * * * 
* * * * * * * * * * 
* * * * * * * * * * 
* * * * * * * * * * 
error = classLabels h weights = weights + alpha * 
error * dataMat randomIndex # * * * * * 
* * * * * * * * * * 
* * * * * * * * * * 
* * * * * * * * * * 
* * * * * * * * * * 
* * # 选取 该 样本 后 将该 样本 下标 
删除 确保 每次 迭 代时 只 使用 一次 del dataIndex 
randomIndex return weights 上述 代码 中有 两处 改进 的 地方 
1 alpha 在 每次 迭代 更新 是 都会 调整 这会 
缓解 数据 波动 或者 高频 运动 此外 alpha 还有 一个 
常数项 目的 是 为了 保证 在 多次 迭代 后 仍然 
对 新 数据 具有 一定 的 影响 如果 要 处理 
的 问题 是 动态 变化 的 可以 适当 加大 该 
常数项 从而 确保 新的 值 获得 更大 的 回归系数 2 
第二个 改进 的 地方 是 选择 随机 的 样本 对 
参数 进行 更新 由于 增加 了 随机性 这就 防止 参数值 
发生 周期性 的 波动 3 并且 采用 上述 改进 算法 
后 收敛 速度 更快 此时 在 利用 matplotlib 可视化 回归 
情况 可以 看出 经过 改进 的 随机 梯度 上升 法 
取得 不错 的 分类 效果 并且 计算 量 更 小了 
三 logistic 回归 实例 从/p 疝气/n 病症/n 预测/vn 病/n 马/n 
死亡率/n 现有/b 数据集/i 有/v 100个/mq 样本/n 和/c 20个/mq 特征/n 但是 
数据 存在 一定 的 问题 即 数据集 有 30% 的 
缺失 因此 我们 在 对 病 马 进行 预测 死亡率 
前 首先 要 解决 数据 的 缺失 问题 我们 可能 
会 遇到 数据 缺失 的 情况 但 有时候 数据 相当 
昂贵 扔掉 和 重新 获取 均 不可取 这 显然 是 
会 带来 更多 的 成本 负担 所以 我们 可以 选 
取 一些 有效 的 方法 来 解决 该 类 问题 
比如 1 使用 可用 特征 的 均值 填补 缺失 值 
2 使用 特殊 值 来 填补 缺失 的 特征 如 
13 忽 略有 缺失 值 的 样本 4 使用 相似 
样本 的 平均值 填补 缺失 值 5 使用 另外 的 
机器学习 算法 预测 缺失 值 这里 我们 根据 logstic 回归 
的 函数 特征 选择 实数 0 来 替换 所有 缺失 
值 而这 恰好 能 适用 logistic 回归 因此 它 在 
参数 更新 时 不会 影响 参数 的 值 即 如果 
某 特征 对应 值 为 0 那么 由 公式 w 
w + alpha * gradient 可知 w 不会 发生 改变 
此外 由于 sigmoid 0 = 0.5 表面 该 特征 对 
结果 的 预测 不 具有 任何 倾向性 因此 不会 对 
误差 造成 影响 当然 如果 是 发生 有 样本 的 
类 标签 缺失 的 情况 此时 我们 最好 的 办法 
是 将该 样本 舍弃 这 是因为 标签 与 特征 不同 
我们 很难 确定 采用 某个 合适 的 值 替换 掉 
下面 来看 回归 分类器 代码 # 实例 从疝/nr 气病/i 预测/vn 
病/n 马的/nr 死亡率/n # 1 准备 数据 处理 数据 的 
缺失 值 # 这里 将 特征 的 缺失 值 补 
0 从而 在 更新 时不/nr 影响 系数 的 值 # 
2 分类 决策函数 def clasifyVector inX weights # 计算 logistic 
回归 预测 概率 prob = sigmoid inX * weights # 
大于 0.5 预测 为 1 if prob 0.5 return 1.0 
# 否则 预测 为 0 else return 0.0 # logistic 
回归 预测 算法 def colicTest # 打开 训练 数据集 frTrain 
= open h o r s e C o l 
i c T r a i n i n g 
. txt # 打开 测试 数据集 frTest = open horseColicTest 
. txt # 新建 两个 孔 列表 用于 保存 训练 
数据集 和 标签 trainingSet = trainingLabels = # 读取 训练 
集 文档 的 每 一行 for line in frTrain . 
readlines # 对当 前行 进行 特征 分割 currLine = line 
. strip . split # 新建 列表 存储 每个 样本 
的 特征向量 lineArr = # 遍历 每个 样本 的 特征 
for i in range 21 将该 样本 的 特征 存入 
lineArr 列表 lineArr . append float currLine i # 将该 
样本 标签 存入 标签 列表 trainingLabels . append currLine 21 
# 将该 样本 的 特征向量 添加到 数据集 列表 trainingSet . 
append lineArr # 调用 随机 梯度 上升 法 更新 logistic 
回归 的 权值 参数 trainWeights = stocGradAscent1 trainingSet trainingLabels 500 
# 统计 测试 数据集 预测 错误 样本 数量 和 样本 
总数 errorCount = 0 numTestVec = 0.0 # 遍历 测试 
数据集 的 每个 样本 for line in frTest . readlines 
# 样本 总数 加 1 numTestVec + = 1.0 # 
对当 前行 进行 处理 分割 出 各个 特征 及 样本 
标签 currLine = line . strip . split # 新建 
特征向量 lineArr = # 将 各个 特征 构成 特征向量 for 
i in range 21 lineArr . append float currLine i 
# 利用 分类 预测 函数 对 该 样本 进行 预测 
并与 样本 标签 进行 比较 if clasifyVector lineArr trainWeights = 
currLine 21 # 如果 预测 错误 错误 数 加 1 
errorCount + = 1 # 计算 测试 集 总的 预测 
错误率 errorRate = float errorCount / numTestVec # 打印 错误率 
大小 print the error rate of this test is % 
f % errorRate # 返回 错误率 return errorRate # 多 
次测试 算法 求取 预测误差 平均值 def multTest # 设置 测试 
次数 为 10次 并 统计 错误率 总和 numTests = 10 
errorRateSum = 0.0 # 每一 次测试 算法 并 统计 错误率 
for k in range numTests errorRateSum + = colicTest # 
打印 出 测试 10次 预测 错误率 平均值 print after % 
d iterations the average error rate is % f \ 
% numTests errorRateSum / float numTests 这样 经过 10次 的 
迭代 平均误差 为 35% 显然 这 跟 30% 的 数据 
缺失 有 一定 的 关系 当然 如果 我们 调整 合适 
的 步长 和 迭代 次数 相信 错误率 会 有所 下降 
四 小结 logistic 回归 的 目的 是 寻找 一个 非 
线性函数 sigmoid 的 最佳 拟合 参数 从而 来 相对 准确 
的 预测 分类 结果 为了 找 出 最佳 的 函数 
拟合 参数 最 常用 的 优化 算法 为 梯度 上升 
法 当然 我们 为了 节省 计算 损耗 通常 选择 随机 
梯度 上升 法来/nr 迭代 更新 拟合 参数 并且 随机 梯度 
上升 法是/nr 一种 在线 学习 算法 它 可以 在 新 
数据 到 来时 完成 参数 的 更新 而 不 需要 
重新 读取 整个 数据集 来 进行 批处理 运算 总的来说 logistic 
回归 算法 其 具有 计算 代价 不高 易于 理解 和 
实现 等 优点 此外 logistic 回归 算法 容易 出现 欠 
拟合 以及 分类 精度 不 太高 的 缺点 