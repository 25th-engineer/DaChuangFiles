1 . MNIST 数据集 1.1 概述 Tensorflow 框架 载 tensorflow 
. contrib . learn . python . learn . datasets 
包中/nr 提供 多个 机器学习 的 数据集 本节 介绍 的 是 
MNIST 数据集 其 功能 都 定义 在 mnist . py 
模块 中 MNIST 是 一个 入门级 的 计算机 视觉 数据集 
它 包含 各种 手写 数字 图片 图 11 它 也 
包含 每 一张 图片 对应 的 标签 告诉 我们 这个 
是 数字 几 比如 上面 这 四张 图片 的 标签 
分别 是 5 0 4 11.2 加载 有 两种 方式 
可以 获取 MNIST 数据集 1 自动 下载 TensorFlow 框架 提供 
了 一个 函数 read _ data _ sets 该 函数 
能够 实现 自动 下载 的 功能 如下 所示 的 程序 
就 能够 自动 下载 数据集 from tensorflow . examples . 
tutorials . mnist import input _ datamnist = input _ 
data . read _ data _ sets MNIST _ data 
/ one _ hot = True / / 由于 input 
_ data 只是 对 read _ data _ sets 进行 
了 包装 其 什么 也 没有 做 所以 我们 可以 
直接 使用 read _ data _ sets . From tensorflow 
. contrib . learn . python . learn . datasets 
. mnist import read _ data _ setsMnist = read 
_ data _ sets MNIST _ data one _ hot 
= True 2 手动 下载 用户 也 能够 手动 下载 
数据集 然后 向 read _ data _ sets 函数 传递 
所在 的 本地 目录 如下 所示 from tensorflow . examples 
. tutorials . mnist import input _ datamnist = input 
_ data . read _ data _ sets / tmp 
/ MNIST _ data / False one _ hot = 
True PS MNIST 数据集 可以 Yann LeCun s website 进行 
下载 如 所示 是 下载 后的/nr 目录 / tmp / 
MNIST _ data / 图 121.3 结构 1 数据 分类 
自动 下载 方式 的 数据 集 被 分成 如表 11 
所示 的 三 部分 这样 的 切分 很重要 在/p 机器学习/i 
模型/n 设计/vn 时/n 必须/d 有/v 一个/m 单独/d 的/uj 测试/vn 数据集/i 
不/d 用于/v 训练/vn 而是 用来 评估 这个 模型 的 性能 
从而 更加 容易 把 设计 的 模型 推广 到 其他 
数据 集上 泛化 表 11 数据集 目的 mnist . train55000 
组 图片 和 标签 用于 训练 mnist . test10000 组 
图片 和 标签 用于 最终 测试 训练 的 准确性 mnist 
. validation5000 组 图片 和 标签 用于 迭代 验证 训练 
的 准确性 PS 若是 手动 下载 则 只有 两 部分 
即 表 中的 train 和 test 两部分 2 数据 展开 
正如 前面 提到 的 一样 每一个 MNIST 数据 单元 有 
两部分 组成 一张 包含 手写 数字 的 图片 和 一个 
对应 的 标签 我们 把 这些 图片 设为 X 把 
这些 标签 设为 Y 训练/vn 数据集/i 和/c 测试/vn 数据集/i 都/d 
包含/v X/w 和Y/nr 比如 训练 数据集 的 图片 是 mnist 
. train . images 训练 数据集 的 标签 是 mnist 
. train . labels 其中 每 一张 图片 包含 28 
像素 * 28 像素 我们 可以 用 一个 数字 数组 
来 表示 这 张 图片 图 13TensorFlow 把 这个 数 
组展 开成 一个 向量 数组 长度 是 28x28 = 784 
即 TensorFlow 将 一个二维 的 数组 展开 成 一个 一维 
的 数组 从 28 28 数组 转换 为 784 数组 
因此 在 MNIST 训练 数据 集中 mnist . train . 
images 是 一个 形状 为 60000 784 的 张量 第一 
个 维度 数字 用来 索引 图片 第二 个 维度 数字 
用来 索引 每张 图片 中的 像素点 在此 张量 里 的 
每一个 元素 都/d 表示/v 某/r 张/q 图片/n 里/f 的/uj 某个/r 
像素/n 的/uj 灰度/n 值/n 值 介于 0 和1/nr 之间 如图 
14 所示 的 二维结构 图 14 相 对应 的 MNIST 
数据集 的 标签 是 介于 0 到 9 的 数字 
用来 描述 给定 图片 里 表示 的 数字 为了 用 
于 这个 教程 我们 使 标签 数据 是 one hot 
vectors 一个 one hot 向量 除了 某 一位 的 数字 
是 1 以外 其余 各 维度 数字 都是 0 所以 
在此 教程 中 数字 n 将 表示 成 一个 只有 
在 第 n 维度 从0/nr 开始 数字 为 1 的 
10 维 向量 比如 标签 0 将 表示 成 1 
0 0 0 0 0 0 0 0 0 0 
因此 mnist . train . labels 是 一个 60000 10 
的 数字 矩阵 图 152 . MNIST 分类 学习 2.1 
实现理论 2 . 1.1 M P 神经元 模型 传统 的 
M P 神经元 模型 中 每个 神经元 都 接收 来自 
n 个 其它 神经元 传递 过来 的 输入 信号 这些 
输入 信号 通过 待 权重 的 连接 进行 传递 神经元 
接 收到 的 总 输入 值 将与 神经元 的 阈值 
进行 比较 然后 通过 激活 函数 处理 以 产生 神经元 
的 输出 如图 21 所示 的 一个 神经元 模型 图 
21PS 图中 所示 的 值 都为 real value 并非 为 
向量 或 矩阵 2 . 1.2 softmax 函数 softmax 函数 
与 sigmoid 函数 类似 都 可以 作为 神经 网络 的 
激活 函数 sigmoid 将 一个 real value 映 射到 0 
1 的 区间 当然 也 可以 是 1 1 这样 
可以 用来 做 二 分类 而 softmax 把 一个 k 
维 的 real value 向量 a1 a2 a3 a4 . 
映射 成 一个 b1 b2 b3 b4 . 其中 bi 
是 一个 0 1 的 常数 然后 可以 根据 bi 
的 大小 来 进行 多 分类 的 任务 如 取 
权重 最大 的 一维 所以 对于 MNIST 分类 任务 是 
多 分类 类型 所以 需要 使用 softmax 函数 作为 神经 
网络 的 激活 函数 2 . 1.3 MNIST 模型 分析 
正 如图 14所 分析 的 输入 的 训练 图片 或 
测试 图片 为 一个 60000 784 的 矩阵 每张 图片 
都是/nr 一个 784 的 向量 输出 为 一个 60000 10 
的 矩阵 每张 图片 都 对应 有一个 10 的 向量 
标签 所以/c 对于/p 每/zg 张/q 图片/n 的/uj 输入/v 和/c 每个/r 
标签/n 的/uj 输出/v 其 神经网络 模型 可表示 为 错误 未找到 
引用 源 所示 的 简化 版本 图中 所有 值 都为 
read value 图 22 前馈 神经网络 一个 带有 10个 神经元 
的 隐藏 层 如果把 它 写成 一个 等式 我们 可以 
得到 我们 也 可以 用 向量 表示 这个 计算 过程 
用 矩阵 乘法 和 向量 相加 这 有助于 提高 计算 
效率 也 是 一种 更 有效 的 思考 方式 更进一步 
可以 写成 更加 紧凑 的 方式 式 中 B 和Y/nr 
都为 一个 10 类型 的 向量 X 为 一个 784 
类型 的 向量 W 是 一个 10 784 类型 的 
矩阵 2.2 TensorFlow 实现 对于 机器学习 中 的 监督 学习 
任务 可以 分 四个 步骤 完成 如下 所示 模型 选择 
选择 一个 estimator 对象 模型 训练 根据 训练 数据集 来 
训练 模型 模型 测试 测量 模型 的 泛化 能力 即 
对其 评分 模型 应用 进行 实际 预测 或 应用 2 
. 2.1 模型 选择 由于 我们 已经 选择 神经 网络 
为 监督 学习 任务 的 模型 即 式 3 所示 
的 等式 我们 可 通过 使用 下标 来 表明 等式 
中 变量 的 维数 如下 所示 所以在 TensorFlow 中的 实现 
就 需要 定义 相应 的 变量 和 等式 但是 式 
4 中的 X 是 一个 784 的 向量 而 实际 
待 训练 的 输入 数据 是 一个 60000 784 的 
矩阵 所以 需要 对 式 4 进行 稍微 的 变形 
使其 满足 数据 输入 和 数据 输出 的 要求 即 
如下 所示 的 等式 X 是 输入 参数 为 训练 
数据 即 多张 图像 W 和B是/nr 未知 参数 即 通过 
神经 网络 来 训练 的 数据 Y 为 输出 参数 
为 图像 标签 将 使用 该 值 与 已知 标签 
进行 比较 如下 所示 是 TensorFlow 的 实现 # Create 
the modelx = tf . placeholder float None 784 W 
= tf . Variable tf . zeros 784 10 b 
= tf . Variable tf . zeros 10 y = 
tf . nn . softmax tf . matmul x W 
+ b 2 . 2.2 模型 训练 1 模型 评估 
我们 可以 创建 一个 模型 model 但 我们 仍然 不 
知道 模型 的 好坏 为了 评估 一个 TensorFlow 模型 的 
性能 我们 可以 提供 一个 期望值 然后 比较 模型 产生 
值 和 期望值 之差 来 进行 评估 传统 方法 采用 
均分 误差 法 评估 一个 模型 的 性能 首先 提供 
一个 期望 向量 然后 对 产生 值 f x 和 
期望值 y 两个 向量 的 每个 元素 进行 取 平方差 
然后 求出 每个 元素 的 总和 由于 传递 神经 网络 
采用 梯度 下 降法 来 逐渐 调整 式 4 中的 
W 和B/nr 参数 即 逐步 减少 均分 误差 的 值 
然而 若以 均分 误差 为 标准 逐步 调整 参数 其 
归约 的 速度 非常 慢 所以 提出 以 交叉 熵 
法为/nr 标准 评估 模型 的 值 如下 所示 如下 所示 
的 TensorFlow 实现 # Define loss and optimizery _ = 
tf . placeholder float None 10 cross _ entropy = 
tf . reduce _ sum y _ * tf . 
log y 2 训练 过程 TensorFlow 提供 多 个 优化 
器 来 逐步 优化 模型 即 逐步 优化 未知 参数 
优 化器 以 用户 指定 的 评估 的 误差 为 
优化 目标 即 最小化 模型 评估 的 误差 或 最大化 
模型 评估 的 误差 优 化器 基于 梯度 下 降法 
自动 修改 神经 网络 的 训练 参数 即 W 和b的/nr 
值 如下 是以 G r a d i e n 
t D e s c e n t O p 
t i m i z e r 优 化器 为 
示例 的 训练 过程 # Traintrain _ step = tf 
. train . G r a d i e n 
t D e s c e n t O p 
t i m i z e r 0.01 . minimize 
cross _ entropy for i in range 1000 batch _ 
xs batch _ ys = mnist . train . next 
_ batch 100 sess . run train _ step feed 
_ dict = { x batch _ xs y _ 
batch _ ys } 2 . 2.3 模型 测试 为了 
评估 模型 的 泛化 性能 我们 通过 比较 产生 值 
f x 和 期望值 y 之间 的 差异 来 进行 
评测 性能 由于 本节 的 MNIST 数据 标签 输出 值 
是 一个 one hot 的 便利 向量 中的 元素 直邮 
一个 为 1 所以 使用 特性 的 比较 方式 如下 
所示 是 TensorFlow 的 实现 # Test trained modelcorrect _ 
prediction = tf . equal tf . argmax y 1 
tf . argmax y _ 1 accuracy = tf . 
reduce _ mean tf . cast correct _ prediction float 
print sess . run accuracy feed _ dict = { 
x mnist . test . images y _ mnist . 
test . labels } 其中 tf . argmax 能 给出 
某个 tensor 对象 在 某一 维 上 的 其 数据 
最大值 所在 的 索引 值 由于 标签 向量 是由 0 
1 组成 因此 最大值 1 所在 的 索引 位置 就是 
类别 标签 tf . cast 类型转换 将 一个 tensor 对象 
的 所有 元素 类型转换 为 另一种 类型 即 上述 将 
tf . equal 方法 生成 的 布尔值 转换成 浮点数 tf 
. reduce _ mean 求 矩阵 或 向量 的 平均值 
若 x = 1 . 1 . 2 . 2 
. 则 tf . reduce _ mean x = = 
1.5 = 1 + 1 + 2 + 2 / 
4 上述 三个 小节 的 完整 程序 如下 所示 from 
_ _ future _ _ import print _ functionimport tensorflow 
as tffrom tensorflow . examples . tutorials . mnist import 
input _ data # Import datamnist = input _ data 
. read _ data _ sets / tmp / MNIST 
_ data / False one _ hot = True # 
Create the modelx = tf . placeholder float None 784 
W = tf . Variable tf . zeros 784 10 
b = tf . Variable tf . zeros 10 y 
= tf . nn . softmax tf . matmul x 
W + b # Define loss and optimizery _ = 
tf . placeholder float None 10 cross _ entropy = 
tf . reduce _ sum y _ * tf . 
log y train _ step = tf . train . 
G r a d i e n t D e 
s c e n t O p t i m 
i z e r 0.01 . minimize cross _ entropy 
init = tf . initialize _ all _ variables sess 
= tf . Session sess . run init # Trainfor 
i in range 1000 batch _ xs batch _ ys 
= mnist . train . next _ batch 100 sess 
. run train _ step feed _ dict = { 
x batch _ xs y _ batch _ ys } 
# Test trained model # 下述 y 的 值 是 
在 上述 训练 最后 一步 已经 计算 获得 所以 能够 
与 原始 标签 y _ 进行 比较 correct _ prediction 
= tf . equal tf . argmax y 1 tf 
. argmax y _ 1 accuracy = tf . reduce 
_ mean tf . cast correct _ prediction float print 
sess . run accuracy feed _ dict = { x 
mnist . test . images y _ mnist . test 
. labels } 3 . 参考文献 TensorFlow 中文 社区 sigmoid 
和 softmax 总结 交叉 熵 代价 函数 作用 及 公式 
推导 