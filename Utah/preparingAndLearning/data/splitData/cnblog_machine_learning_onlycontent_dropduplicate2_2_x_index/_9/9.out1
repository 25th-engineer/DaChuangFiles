本 博客 所有 内容 以 学习 研究 和 分享 为主 
如需 转载 请 联系 本人 标明 作者 和 出处 并且 
是 非 商业 用途 谢谢 想 写 这个 系列 很久 
了 最近 刚 好 项目 结束 了 闲 下来 有点/nr 
时间 于是 决定 把 之前 学过 的 东西 做个 总结 
之前 看过 一些 机器学习 方面 的 书 每本书 都 各有 
侧重点 机器学习 实战 和 集体 智慧 编程 更 偏向 与 
实战 侧重于 对 每个 算法 的 实际 操作 过程 但是 
没有 对 整个 数据挖掘 项目 做 介绍 李航/i 老师/n 的/uj 
统计/v 学习/v 方法/n 和/c 周志华/nr 老师/n 的/uj 机器/n 学习/v 这/r 
两本书/n 侧重/v 对/p 原理/n 的/uj 讲解/v 和/c 公式/n 的/uj 推导/v 
但是 实战 方面 可能 会 少一点 我 结合 之前 看过 
的 书 以及 自己 的 一些 项目 经验 做 了 
一些 总结 一是 回顾 自己 还有 哪些 遗漏 二 是 
希望 给 新 入门 的 同学 一个 参考 至于 编程语言 
主要 用 python 也会有 少部分 R java 和 scala 之类 
毕竟 实际 项目 中 也 不 可能 使用 一种 语言 
此外 本 系列 所 用到 的 所有 数据 我会 传到 
Github 上 需要 的 同学 可以 自行 下载 为 保证 
文章 质量 每周 二 周四 更新 下面 是 主要 的 
目录 可能 会 根据 实际 情况 调整 第一 部分 模型 
的 评估 与 数据 处理 机器学习 基础 与 实践 一 
数据 清洗 机器学习 基础 与 实践 二 数据 转换 机器学习 
基础 与 实践 三 数据 降 维 第二 部分 特征 
工程 机器学习 基础 与 实践 四 特征选择 机器学习 基础 与 
实践 五 特征提取 机器学习 基础 与 实践 六 模型 选择 
与 评估 第三 部分 算法 基础 之 有 监督 算法 
机器学习 基础 与 实践 七 广义 线性 模型 机器学习 基础 
与 实践 八 最小二乘 法 机器学习 基础 与 实践 九 
LDA 机器学习 基础 与 实践 十 SGD 机器学习 基础 与 
实践 十一 K 近邻 机器学习 基础 与 实践 十二 高斯 
过程 机器学习 基础 与 实践 十三 决策树 ID3 C 4.5 
C 5.0 CART 机器学习 基础 与 实践 十四 朴素 贝叶斯 
机器学习 基础 与 实践 十五 支持 向量 机 机器学习 基础 
与 实践 十六 集成 学习 Bagging RF AdaBoost Gradient Tree 
Boosting Voting Classifier 机器学习 基础 与 实践 十七 感知机 模型 
机器学习 基础 与 实践 十八 多 分类 算法 第四 部分 
算法 基础 之无 监督 算法 机器学习 基础 与 实践 十九 
K means 机器学习 基础 与 实践 二十 Affinity propagation 机器学习 
基础 与 实践 二十一 Mean shift 机器学习 基础 与 实践 
二十二 Spectral clustering 机器学习 基础 与 实践 二十三 Ward hierachical 
机器学习 基础 与 实践 二十四 Agglomerative clustering 机器学习 基础 与 
实践 二十五 DBSCAN 机器学习 基础 与 实践 二十六 Gaussian mixtures 
机器学习 基础 与 实践 二十七 Birch 第五 部分 算法 基础 
之 推荐算法 机器学习 基础 与 实践 二十八 相似 度 计算 
机器学习 基础 与 实践 二十九 Arules 关联 规则 机器学习 基础 
与 实践 三十 Fp Growth 机器学习 基础 与 实践 三十一 
User based or Item based 第六 部分 算法 基础 之半 
监督 模型 机器学习 基础 与 实践 三十二 Label Propagation 第七 
部分 算法 基础 之 其他 模型 机器学习 基础 与 实践 
三十三 概率 图 模型 机器学习 基础 与 实践 三十四 最大熵 
模型 机器学习 基础 与 实践 三十五 规则学习 机器学习 基础 与 
实践 三十六 强化 学习 机器学习 基础 与 实践 三十七 条件 
随 机场 机器学习 基础 与 实践 三十八 保 序 回归 
Isotonic regression 机器学习 基础 与 实践 三十九 Probability calibration 正文 
按照 我 做 项目 的 经验 来了 项目 首先 是 
分析 项目 的 目的 和 需求 了解 这个 项目 属于 
什么问题 要 达到 什么 效果 然后 提取 数据 做 基本 
的 数据 清洗 第三步 是 特征 工程 这个 属于 脏活 
累活 需要 耗费 很大 的 精力 如果 特征 工程 做 
的 好 那么 后面 选择 什么 算法 其实 差异 不大 
反之 不管 选择 什么 算法 效果/n 都/d 不会/v 有/v 突破性/n 
的/uj 提高/v 第四步 是 跑 算法 通常 情况下 我会 把 
所有 能跑 的 算法 先跑 一遍 看看 效果 分析 一下 
precesion / recall 和 f1 score 看看 有 没有 什么 
异常 譬如 有 好几 个 算法 precision 特别 好 但是 
recall 特别 低 这 就要 从 数据 中 找 原因 
或者 从 算法 中看 是不是 因为 算法 不 适合 这个 
数据 如果 没有 异常 那么 就 进行 下 一步 选择 
一 两个 跑 的 结果 最好 的 算法 进行 调 
优 调 优 的 方法 很多 调整 参数 的话 可以 
用 网格 搜索 随机搜索 等 调整 性能 的话 可以 根据 
具体 的 数据 和 场景 进行 具体 分析 调 优 
后再 跑 一边 算法 看 结果 有 没有 提高 如果 
没有 找 原因 数据 or 算法 是 数据 质量 不好 
还是 特征 问题 还是 算 法 问题 一个一个 排查 找 
解决 方法 特征 问题 就 回到 第三步 再 进行 特征 
工程 数据 质量 问题 就 回到 第一步 看 数据 清洗 
有 没有 遗漏 异常值 是否 影响 了 算法 的 结果 
算法 问题 就 回到 第四步 看 算法 流程 中 哪 
一步 出了 问题 如果 实在 不行 可以 搜 一下 相关 
的 论文 看看 论文 中 有 没有 解决 方法 这样 
反复 来 几遍 就 可以 出 结果 了 写 技术 
文档 和 分析 报告 再向 业务 人员 或 产品 讲解 
我们 做 的 东西 然后 他们 再 提 建议 / 
该 需求 不断 循环 最后 代码 上线 改 bug 直到 
结 项 直观 来看 可以 用 一个 流程图 来 表示 
今天 讲 数据 清洗 为什么 要 进行 数据 清洗 呢 
我们 在 书上 看到 的 数据 譬如 常见 的 iris 
数据集 房价 数据 电影 评分 数据集 等等 数据 质量 都 
很高 没有 缺失 值 没有 异常 点 也 没有 噪音 
而在 真实 数据 中 我们 拿到 的 数据 可能 包含 
了 大量 的 缺失 值 可能 包含 大量 的 噪音 
也 可能 因为 人工录入 错误 导致 有 异常 点 存在 
对 我们 挖 据 出 有效 信息 造成了 一定 的 
困扰 所以 我们 需要 通过 一些 方法 尽量 提高 数据 
的 质量 数据 清洗 一般 包括 以下 几个 步骤 一 
. 分析 数据 二 . 缺失 值 处理 三 . 
异常值 处理 四 . 去 重 处理 五 . 噪音 
数据处理 六 . 一些 实用 的 数据处理 小工具 一 . 
分析 数据 在 实际 项目 中 当 我们 确定 需求 
后 就会 去找 相应 的 数据 拿到 数据 后 首先 
要 对 数据 进行 描述性 统计分析 查看 哪些 数据 是 
不合理 的 也 可以 知道 数据 的 基本 情况 如果 
是 销售额 数据 可以 通过 分析 不同 商品 的 销售 
总额 人均 消费额 人均 消费 次 数等 同一 商品 的 
不同 时间 的 消费额 消费 频次 等等 了解 数据 的 
基本 情况 此外 可以 通过 作图 的 方式 了解 数据 
的 质量 有无 异常 点 有无 噪音 等 举个 例子 
这里 数据 较少 就 直接 用 R 作图 了 1 
# 一组 年薪 超过 10 万元 的 经理 收入 2 
pay = c 11 19 14 22 14 28 13 
81 12 43 11 16 31 16 23.42 22 26 
17 22 13 27 180 16 43 82 14 11 
51 76 28 66 29 14 14 65 37 16 
37 35 39 27 14 17 13 38 28 40 
85 32 25 26 16 12 54 40 18 27 
16 14 33 29 77 50 19 34 3 par 
mfrow = c 2 2 # 将 绘图 窗口 改成 
2 * 2 可 同时 显示 四幅 图 4 hist 
pay # 绘制 直方图 5 dotchart pay # 绘制 点 
图 6 barplot pay horizontal = T # 绘制 箱 
型 图 7 qqnorm pay qqline pay # 绘制 Q 
Q 图 从 上面 四幅 图 可以 很 清楚 的 
看出 180 是 异常值 即 第 23个 数据 需要 清理 
python 中 也 包含 了 大量 的 统计 命令 其中 
主要 的 统计 特征函数 如下 图 所示 二 . 缺失 
值 处理 缺失 值 在 实际 数据 中 是 不可避免 
的 问题 有的 人 看到 有 缺失 的 数据 就 
直接 删除 了 有的 人 直接 赋予 0 值 或者 
某 一个 特殊 的 值 那么 到底 该 怎么 处理 
呢 对于 不同 的 数据 场景 应该 采取 不同 的 
策略 首先 应该 判断 缺失 值 的 分布 情况 1 
import scipy as sp 2 data = sp . genfromtxt 
web _ traffic . tsv delimiter = \ t 数据 
情况 如下 1 data 2 array 1.00000000 e + 00 
2.27200000 e + 03 3 2.00000000 e + 00 nan 
4 3.00000000 e + 00 1.38600000 e + 03 5 
. . . 6 7.41000000 e + 02 5.39200000 e 
+ 03 7 7.42000000 e + 02 5.90600000 e + 
03 8 7.43000000 e + 02 4.88100000 e + 03 
9 10 print data 10 11 1.00000000 e + 00 
2.27200000 e + 03 12 2.00000000 e + 00 nan 
13 3.00000000 e + 00 1.38600000 e + 03 14 
4.00000000 e + 00 1.36500000 e + 03 15 5.00000000 
e + 00 1.48800000 e + 03 16 6.00000000 e 
+ 00 1.33700000 e + 03 17 7.00000000 e + 
00 1.88300000 e + 03 18 8.00000000 e + 00 
2.28300000 e + 03 19 9.00000000 e + 00 1.33500000 
e + 03 20 1.00000000 e + 01 1.02500000 e 
+ 03 21 22 data . shape 23 743 2 
可以 看到 第 2列 已经 出现 了 缺失 值 现在 
我们 来看 一下 缺失 值 的 数量 1 x = 
data 0 2 y = data 1 3 sp . 
sum sp . isnan y 4 8 在 743个 数据 
里 只有 8个 数据 缺失 所以 删除 它们 对于 整体 
数据 情况 影响 不大 当然 这是 缺失 值 少 的 
情况 下 在 缺失 值 值 比较 多 而 这个 
维度 的 信息 还 很 重要 的 时候 因为 缺失 
值 如果 占了 95% 以上 可以 直接 去 掉 这个 
维度 的 数据 了 直接 删除 会对 后面 的 算法 
跑 的 结果 造成 不好 的 影响 我们 常用 的 
方法 有 以下 几种 1 . 直接 删除 适合 缺失 
值 数量 较小 并且 是 随机 出现 的 删除 它们 
对 整体 数据 影响 不大 的 情况 2 . 使用 
一个 全局 常量 填充 譬如 将 缺失 值 用 Unknown 
等 填充 但是 效果 不 一定 好 因为 算法 可能会 
把 它 识别 为 一个 新的 类别 一般 很少 用 
3 . 使用 均值 或 中位数 代替 优点 不会 减少 
样本 信息 处理 简单 缺点 当 缺失 数据 不是 随机 
数据 时会/nr 产生 偏差 . 对于 正常 分布 的 数据 
可以 使用 均值 代替 如果 数据 是 倾斜 的 使用 
中位数 可能 更好 4 . 插 补法 1 随机 插 
补法 从 总体 中 随机 抽取 某个 样本 代替 缺失 
样本 2 多 重插 补法 通过 变量 之间 的 关系 
对 缺失 数据 进行 预测 利用 蒙特卡洛 方法 生成 多个 
完整 的 数据 集 在 对 这些 数据 集 进行 
分析 最后 对 分析 结果 进行 汇总 处理 3 热 
平台 插补 指在 非 缺失 数据 集中 找到 一个 与 
缺失 值 所在 样本 相似 的 样本 匹配 样本 利用 
其中 的 观测值 对 缺失 值 进行 插补 优点 简单易行 
准 去 率 较高 缺点 变量 数量 较 多时 通常 
很难 找到 与 需要 插补 样本 完全 相同 的 样本 
但 我们 可以 按照 某些 变量 将 数据 分层 在 
层 中 对 缺失 值 实用 均值 插补 4 拉格朗日/i 
差值/n 法和/nr 牛顿/nr 插值法/n 简单 高效 数值分析 里 的 内容 
数学公式 以后 再补 = = 5 . 建模 法 可以 
用 回归 使用 贝叶斯 形式化 方法 的 基于 推理 的 
工具 或 决策树 归纳 确定 例如 利用 数据 集中 其他 
数据 的 属性 可以 构造 一棵 判定 树 来 预测 
缺失 值 的 值 以上 方法 各有 优缺点 具体 情况 
要 根据 实际 数据 分 分布 情况 倾斜 程度 缺失 
值 所占 比例 等等 来 选择 方法 一般而言 建模 法是/nr 
比较 常用 的 方法 它 根据 已 有的 值 来 
预测 缺失 值 准确率 更高 三 . 异常值 处理 异常值 
我们 通常 也 称为 离群 点 在 讲 分析 数据 
时 我们 举了 个 例子 说明 如何 发现 离群 点 
除了 画图 画图 其实 并不 常用 因为 数据量 多时 不好 
画图 而 且慢 还有 很多 其他 方法 1 . 简单 
的 统计分析 拿到 数据 后 可以 对 数据 进行 一个 
简单 的 描述性 统计分析 譬如 最大 最小值 可以 用来 判断 
这个 变量 的 取值 是否 超过 了 合理 的 范围 
如 客户 的 年龄 为 20岁 或 200岁 显然是 不合 
常理 的 为 异常值 在 python 中 可以 直接 用 
pandas 的 describe 1 import pandas as pd 2 data 
= pd . read _ table web _ traffic . 
tsv header = None 3 data . describe 4 0 
1 5 count 743.000000 735.000000 6 mean 372.000000 1962.165986 7 
std 214.629914 860.720997 8 min 1.000000 472.000000 9 25% 186.500000 
1391.000000 10 50% 372.000000 1764.000000 11 75% 557.500000 2217.500000 12 
max 743.000000 5906 . 0000002.3 ∂ 原则 如果 数据 服从 
正态分布 在 3 ∂ 原则 下 异常值 为 一组 测 
定值 中 与 平均值 的 偏差 超过 3倍 标准差 的 
值 如果 数据 服从 正态分布 距离 平均值 3 ∂ 之外 
的 值 出现 的 概率 为 P | x u 
| 3 ∂ = 0.003 属于 极 个别 的 小 
概率 事件 如果 数据 不 服从 正态分布 也 可以 用 
远离 平均值 的 多少 倍 标准差 来 描述 3 . 
箱 型 图 分析 箱 型 图 提供 了 识别 
异常值 的 一个 标准 如果 一个 值 小于 QL01 . 
5IQR 或 大于 OU 1 . 5IQR 的 值 则 
被 称为 异常值 QL 为 下 四分位数 表示 全部 观察 
值 中有 四分之一 的 数据 取值 比 它 小 QU 
为上 四分位数 表示 全部 观察 值 中有 四分之一 的 数据 
取值 比 它 大 IQR 为 四分位数 间距 是 上 
四分位数 QU 与 下 四分位数 QL 的 差值 包含 了 
全部 观察 值 的 一半 箱 型 图 判断 异常值 
的 方法 以 四分位数 和 四分位距 为基础 四分位数 具有 鲁棒性 
25% 的 数据 可以 变得 任意 远 并且 不 会 
干扰 四分位数 所以 异常值 不能 对 这个 标准 施加影响 因此 
箱 型 图 识别 异常值 比较 客观 在/p 识别/v 异常值/i 
时/n 有/v 一定/d 的/uj 优越性/b 4 . 基于 模型 检测 
首先 建立 一个 数据模型 异常 是 那些 同 模型 不能 
完美 拟合 的 对象 如果 模型 是 簇 的 集合 
则 异常 是 不显著 属于 任何 簇 的 对象 在 
使用 回归模型 时 异常 是 相对 远离 预测值 的 对象 
优缺点 1 . 有 坚实 的 统计 学 理论 基础 
当 存在 充分 的 数据 和 所用 的 检验 类型 
的 知识 时 这些 检验 可能 非常 有效 2 . 
对于 多 元数据 可用 的 选择 少 一些 并且 对于 
高维 数据 这些 检测 可能性 很差 5 . 基于 距离 
通常 可以 在 对象 之间 定义 邻近 性 度量 异常 
对象 是 那些 远离 其他 对象 的 对象 优缺点 1 
. 简单 2 . 缺点 基于 邻近 度 的 方法 
需要 O m2 时间 大 数据集 不适用 3 . 该 
方法 对 参数 的 选择 也 是 敏感 的 4 
. 不能 处理 具有 不同 密度 区域 的 数据 集 
因为 它 使用 全局 阈值 不能 考虑 这种 密度 的 
变化 6 . 基于 密度 当 一个 点 的 局部 
密度 显著 低于 它 的 大部分 近邻 时才/nr 将其 分类 
为 离群 点 适合 非 均匀分布 的 数据 优缺点 1 
. 给出 了 对象 是 离群 点 的 定量 度量 
并且 即使 数据 具有 不同 的 区域 也 能够 很好 
的 处理 2 . 与 基于 距离 的 方法 一样 
这些 方法 必然 具有 O m2 的 时间 复杂度 对于 
低维 数据 使用 特定 的 数据结构 可以 达到 O mlogm 
3 . 参数 选择 困难 虽然 算法 通过观察 不同 的 
k 值 取得 最大 离群 点得 分来 处理 该 问题 
但是 仍然 需要 选择 这些 值 的 上下 界 7 
. 基于 聚 类 基于 聚 类 的 离群 点 
一个 对象 是 基于 聚 类 的 离群 点 如果 
该 对象 不强 属于 任何 簇 离群 点 对 初始 
聚 类 的 影响 如果 通过 聚 类 检测 离群 
点 则 由于 离群 点 影响 聚 类 存在 一个 
问题 结构 是否 有效 为了 处理 该 问题 可以 使用 
如下 方法 对象 聚 类 删除 离群 点 对象 再次 
聚 类 这个 不能 保证 产生 最优 结果 优缺点 1 
. 基于 线性 和 接近 线性 复杂度 k 均值 的 
聚 类 技术 来 发现 离群 点 可能 是 高度 
有效 的 2 . 簇 的 定义 通常 是 离群 
点 的 补 因此 可能 同时 发现 簇 和 离群 
点 3/m ./i 产生/n 的/uj 离群/n 点/m 集/q 和/c 它们/r 
的/uj 得分/v 可能/v 非常/d 依赖/v 所用/b 的/uj 簇/zg 的/uj 个数/n 
和/c 数据/n 中/f 离群/n 点/m 的/uj 存在/v 性/n 4 . 
聚 类 算法 产生 的 簇 的 质量 对 该 
算法 产生 的 离群 点 的 质量 影响 非常 大 
处理 方法 1 . 删除 异常值 明显 看出 是 异常 
且 数量 较少 可以 直接 删除 2 . 不 处理 
如果 算法 对 异常值 不 敏感 则 可以 不 处理 
但 如果 算法 对 异常值 敏感 则 最好 不要 用 
如 基于 距离 计算 的 一些 算法 包括 kmeans knn 
之类 的 3 . 平均值 替代 损失 信息 小 简单 
高效 4 . 视为 缺失 值 可以 按照 处理 缺失 
值 的 方法 来 处理 四 . 去 重 处理 
以 DataFrame 数据格式 为例 1 # 创建 数据 data 里 
包含 重复 数据 2 data = pd . DataFrame { 
v1 a * 5 + b * 4 v2 1 
2 2 2 3 4 4 5 3 } 3 
data 4 v1 v2 5 0 a 1 6 1 
a 2 7 2 a 2 8 3 a 2 
9 4 a 3 10 5 b 4 11 6 
b 4 12 7 b 5 13 8 b 3 
14 15 # DataFrame 的 duplicated 方法 返回 一个 布尔 
型 Series 表示 各行 是否 是 重复 行 16 data 
. duplicated 17 0 False 18 1 False 19 2 
True 20 3 True 21 4 False 22 5 False 
23 6 True 24 7 False 25 8 False 26 
dtype bool 27 28 # drop _ duplicates 方法 用于 
返回 一个 移 除了 重复 行 的 DataFrame 29 data 
. drop _ duplicates 30 v1 v2 31 0 a 
1 32 1 a 2 33 4 a 3 34 
5 b 4 35 7 b 5 36 8 b 
3 37 38 # 这 两个 方法 默认 会 判断 
全部列 你 也 可以 指定 部 分列 进行 重复 项 
判断 假设 你 还有 一列 值 且 只 希望 根据 
v 1列 过滤 重复 项 39 data v3 = range 
9 40 data 41 v1 v2 v3 42 0 a 
1 0 43 1 a 2 1 44 2 a 
2 2 45 3 a 2 3 46 4 a 
3 4 47 5 b 4 5 48 6 b 
4 6 49 7 b 5 7 50 8 b 
3 8 51 data . drop _ duplicates v1 52 
v1 v2 v3 53 0 a 1 0 54 5 
b 4 5 55 56 # duplicated 和 drop _ 
duplicates 默认 保留 的 是 第一个 出现 的 值 组合 
传入 take _ last = True 则 保留 最后 一个 
57 data . drop _ duplicates v1 v2 take _ 
last = True 58 v1 v2 v3 59 0 a 
1 0 60 3 a 2 3 61 4 a 
3 4 62 6 b 4 6 63 7 b 
5 7 64 8 b 3 8 如果 数据 是 
列表 格式 的 有 以下 几种 方法 可以 删除 1 
list0 = b c d b c a a 2 
3 方法 1 使用 set 4 5 list1 = sorted 
set list0 key = list0 . index # sorted output 
6 print list1 7 8 方法 2 使用 { } 
. fromkeys . keys 9 10 list2 = { } 
. fromkeys list0 . keys 11 print list2 12 13 
方法 3 set + sort 14 15 list3 = list 
set list0 16 list3 . sort key = list0 . 
index 17 print list3 18 19 方法 4 迭代 20 
21 list4 = 22 for i in list0 23 if 
not i in list4 24 list4 . append i 25 
print list4 26 27 方法 5 排序 后 比较 相邻 
2个 元素 的 数据 重复 的 删除 28 29 def 
sortlist list0 30 list0 . sort 31 last = list0 
1 32 for i in range len list0 2 1 
1 33 if list0 i = = last 34 list0 
. remove list0 i 35 else 36 last = list0 
i 37 return list0 38 39 print sortlist list0 五 
. 噪音 处理 噪音 是 被 测量 变量 的 随机误差 
或 方差 我们 在 上文 中 提到 过 异常 点 
离群 点 那么 离群 点 和 噪音 是不是 一回事 呢 
我们 知道 观 测量 Measurement = 真实 数据 True Data 
+ 噪声 Noise 离群 点 Outlier 属于 观 测量 既 
有 可能 是 真实 数据 产生 的 也 有可能 是 
噪声 带来 的 但是/c 总的来说/c 是/v 和/c 大部分/m 观/vg 测量/vn 
之间/f 有/v 明显/a 不同/a 的/uj 观测值/i 噪音 包括 错误 值 
或 偏离 期望 的 孤立点 值 但 也 不能 说 
噪声 点 包含 离群 点 虽然 大部分 数据挖掘 方法 都将 
离群 点视 为 噪声 或 异常 而 丢弃 然而 在 
一些 应用 例如 欺诈 检测 会 针对 离群 点 做 
离群 点 分析 或 异常 挖掘 而且 有些 点在 局部 
是 属于 离群 点 但从 全局 看 是 正常 的 
我 在 quora 上 看到 过 一个 解释 噪音 与 
离群 点 的 有趣 的 例子 Outlier you are enumerating 
meticulously everything you have . You found 3 dimes 1 
quarter and wow a 100 USD bill you had put 
there last time you bought some booze and had totally 
forgot there . The 100 USD bill is an outlier 
as it s not commonly expected in a pocket . 
Noise you have just come back from that club and 
are pretty much wasted . You try to find some 
money to buy something to sober up but you have 
trouble reading the figures correctly on the coins . You 
found 3 dimes 1 quarter and wow a 100 USD 
bill . But in fact you have mistaken the quarter 
for a dime this mistake introduces noise in the data 
you have access to . To put it otherwise data 
= true signal + noise . Outliers are part of 
the data . 翻译 过来 就是 离群 点 你/r 正在/t 
从/p 口袋/n 的/uj 零钱/n 包/v 里面/f 穷举/n 里面/f 的/uj 钱/n 
你 发现 了 3个 一角 1个 五毛 和/c 一张/m 100元/mq 
的/uj 毛/nr 爷爷/n 向你/nr 微笑/vn 这个 100元 就是 个 离群 
点 因为 并不 应该 常 出现 在 口袋 里 . 
. 噪声 你 晚上 去 三里屯 喝 的 酩酊大醉 很 
需要 买 点 东西 清醒 清醒 这时候 你 开始 翻 
口袋 的 零钱包 嘛 你 发现 了 3个 一角 1个 
五毛 和/c 一张/m 100元/mq 的/uj 毛/nr 爷爷/n 向你/nr 微笑/vn 但是 
你 突然 眼晕 把 那 三个 一角 看成 了 三个 
1元 . . . 这样 错误 的 判断 使得 数据 
集中 出现 了 噪声 那么 对于 噪音 我们 应该 如何 
处理 呢 有 以下 几种 方法 1 . 分箱 法 
分箱 方法 通过考察 数据 的 近邻 即 周围 的 值 
来 光滑 有序 数据 值 这些 有序 的 值 被 
分布 到 一些 桶 或 箱 中 由于 分箱 方法 
考察 近邻 的 值 因此 它 进行 局部 光滑 用箱 
均值 光滑 箱 中 每一个 值 被 箱 中的 平均值 
替换 用箱 中位数 平滑 箱 中的 每一个 值 被 箱 
中的 中位数 替换 用箱 边界 平滑 箱 中 的 最大 
和 最小值 同样 被 视为 边界 箱 中的 每一个 值 
被 最近 的 边界值 替换 一般而言 宽度 越大 光滑 效果 
越 明显 箱 也 可以 是 等宽 的 其中 每个 
箱 值 的 区间 范围 是个 常量 分箱 也 可以 
作为 一种 离散化 技术 使用 . 2 .   回归 
法 可以 用 一个 函数 拟合 数据 来 光滑 数据 
线性 回归 涉及 找出 拟合 两个 属性 或 变量 的 
最佳 直线 使得 一个 属性 能够 预测 另一个 多 线性 
回归 是 线性 回归 的 扩展 它 涉及 多 于 
两个 属性 并且 数据 拟合 到 一个 多 维面 使用 
回归 找出 适合 数据 的 数学 方程式 能够 帮助 消除 
噪声 六 . 一些 实用 的 数据处理 小工具 1 . 
去掉 文件 中 多余 的 空行 空行 主要指 的 是 
\ n \ r \ r \ n \ n 
\ r 等 在 python 中 有个 strip 的 方法 
该 方法 可以 去掉 字符串 两端 多余 的 空白 此处 
的 空白 主要 包括 空格 制表符 \ t 换行符 不过 
亲 测 以后 发现 strip 可以 匹配 掉 \ n 
\ r \ n \ n \ r 等 但是 
过滤 不掉 单独 的 \ r 为了 万无一失 我 还是 
喜欢 用 麻烦 的 办法 如下 1 # * coding 
utf 8 * 2 # 文本格式 化 处理 过滤掉 空行 
3 4 file = open 123 . txt 5 6 
i = 0 7 while 1 8 line = file 
. readline . strip 9 if not line 10 break 
11 i = i + 1 12 line1 = line 
. replace \ r 13 f1 = open filename . 
txt a 14 f1 . write line1 + \ n 
15 f1 . close 16 print str i 2 . 
如何 判断 文件 的 编码 格式 1 # * coding 
utf8 * 2 # 批量 处理 编码 格式 转换 优化 
3 import os 4 import chardet 5 6 path1 = 
E / / 2016txtutf / 7 def dirlist path 8 
filelist = os . listdir path 9 for filename in 
filelist 10 filepath = os . path . join path 
filename 11 if os . path . isdir filepath 12 
dirlist filepath 13 else 14 if filepath . endswith . 
txt 15 f = open filepath 16 data = f 
. read 17 if chardet . detect data encoding = 
utf 8 18 print filepath + + chardet . detect 
data encoding 19 20 dirlist path1 3 . 文件 编码 
格式 转换 gbk 与 utf 8 之间 的 转换 这个 
主要 是 在 一些 对 文件 编码 格式 有 特殊 
需求 的 时候 需要 批量 将 gbk 的 转 utf 
8 的 或者 将 utf 8 编码 的 文件 转成 
gbk 编码 格式 的 1 # * coding gbk * 
2 # 批量 处理 编码 格式 转换 3 import codecs 
4 import os 5 path1 = E / / dir 
/ 6 def ReadFile filePath encoding = utf 8 7 
with codecs . open filePath r encoding as f 8 
return f . read 9 10 def WriteFile filePath u 
encoding = gbk 11 with codecs . open filePath w 
encoding as f 12 f . write u 13 14 
def UTF8 _ 2 _ GBK src dst 15 content 
= ReadFile src encoding = utf 8 16 WriteFile dst 
content encoding = gbk 17 18 def GBK _ 2 
_ UTF8 src dst 19 content = ReadFile src encoding 
= gbk 20 WriteFile dst content encoding = utf 8 
21 22 def dirlist path 23 filelist = os . 
listdir path 24 for filename in filelist 25 filepath = 
os . path . join path filename 26 if os 
. path . isdir filepath 27 dirlist filepath 28 else 
29 if filepath . endswith . txt 30 print filepath 
31 # os . rename filepath filepath . replace . 
txt . doc 32 try 33 UTF8 _ 2 _ 
GBK filepath filepath 34 except Exception ex 35 f = 
open error . txt a 36 f . write filepath 
+ \ n 37 f . close 38 39 dirlist 
path1 刚 写完 比较 粗糙 以后 会 不断 修改 下篇 
写 数据 转换 方面 的 内容 包括 标准化 归一化 正则化 
等 如果 有 错误 欢迎 指正 ps 声明 一下 上周 
发现 知乎 大 V 某某 大师 抄袭 我 的 原 
数据分析 / 数据挖掘 / 机器学习 必读 书目 虽然 只是 自己 
平常 读书 的 一些 记录 但是 也是 我 的 文字 
不想 被 抄袭 如需 转载 请 联系 本人 署名 并 
标明 出处 且 是非 商业用途 谢谢 ~ 参考文献 1 . 
http / / blog . csdn . net / u012950655 
/ article / details / 169460212 . http / / 
my . oschina . net / dfsj66011 / blog / 
6015463 . http / / www . cnblogs . com 
/ nzyjlr / p / 4174145 . html4 . https 
/ / www . quora . com / What is 
the basic difference between noise and outliers in Data mining6 
. 数据挖掘 概念 与 技术 Jiawei Han Micheline Kamber Jian 
Pei 