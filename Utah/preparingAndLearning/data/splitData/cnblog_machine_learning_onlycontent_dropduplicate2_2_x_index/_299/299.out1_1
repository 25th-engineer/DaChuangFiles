人工智能 深度 学习 机器学习 无论 你 在 做什么 如果 你 
对 它 不是 很 了解 的话 去 学习 它 否则 
的话 不用 三年 你 就 跟不上 时代 的 潮流 了 
马克 . 库班 马克 . 库班 的 这个 观点 可能 
听 起来 很 极端 但是 它 所 传达 的 信息 
是 完全 正确 的 我们 正 处于 一场 革命 的 
旋涡 之中 一场 由 大 数据 和 计算 能力 引起 
的 革命 只 需要 一 分钟 我们 来 想象 一下 
在 20 世纪初 如果 一个人 不 了解 电力 他 / 
她 会 觉得 如何 你 会 习惯于 以 某种 特定 
的 方式 来 做事情 日复一日 年复一年 而 你 周围 的 
一切 事情 都在/nr 发生 变化 一件 需要 很多 人 才能 
完成 的 事情 仅 依靠 一 个人 和 电力 就 
可以 轻松 搞定 而 我们 今天 正 以 机器学习 和 
深度 学习 的 方式 在 经历 一场 相似 的 旅程 
所以 如果 你 还 没有 探索 或 理解 深度 学习 
的 神奇 力量 那你/nr 应该/v 从/p 今天/t 就/d 开始/v 进入/v 
这/r 一/m 领域/n 谁 应该 读 这篇文章 如果 你 是 
一个 想 学习 或 理解 深度 学习 的 人 这 
篇 文章 是 为 你 量身 定做 的 在 本文 
中 我 将 介绍 深度 学习 中 常用 的 各种 
术语 如果 你 想 知道 我 为什么 要写 这篇文章 我 
之所以 在 写 是 因为 我 希望 你 开始 你 
的 深度 学习 之旅 而 不会 遇到 麻烦 或是 被 
吓倒 当 我 第一 次 开始 阅读 关于 深度 学习 
资料 的 时候 有几个 我 听说 过 的 术语 但是 
当 我 试图 理解 它 的 时候 它 却是 令人 
感到 很 迷惑 的 而 当 我们 开始 阅读 任何 
有关 深度 学习 的 应用 程序 时 总会 有 很多 
个 单词 重复 出现 在 本文 中 我 为 你 
创建 了 一个 类似于 深度 学习 的 字典 你 可以 
在 需要 使用 最 常用 术语 的 基本 定义 时 
进行 参考 我 希望 在 你 阅读 这篇文章 之后 你 
就 不会 再 受到 这些 术语 的 困扰 了 与 
主题 相关 的 术语 为了 帮助 你 了解 各种 术语 
我 已经 将 它们 分成 3组 如果 你 正在 寻找 
特定 术语 你 可以 跳到 该 部分 如果 你 是 
这个 领域 的 新手 那我/nr 建议 你 按照 我 写 
的 顺序 来 通读 它们 1 . 神经 网络 基础 
Basics of Neural Networks 常用 激活 函数 Common Activation Functions 
2 . 卷积 神经网络 Convolutional Neural Networks 3 . 循环 
神经网络 Recurrent Neural Networks 神经 网络 基础 1 神经元 Neuron 
就像 形成 我们 大脑 基本 元素 的 神经元 一样 神经元 
形成 神经 网络 的 基本 结构 想象 一下 当 我们 
得到 新 信息 时 我们 该 怎么 做 当 我们 
获取 信息 时 我们 一般 会 处理 它 然后 生成 
一个 输出 类 似地 在 神经 网络 的 情况 下 
神经元 接收 输入 处理 它 并 产生 输出 而 这个 
输出 被 发送 到 其他 神经元 用于 进一步 处理 或者 
作为 最终 输出 进行 输出 2 权重 Weights 当 输入 
进入 神经元 时 它 会 乘以 一个 权重 例如 如果 
一个 神经元 有 两个 输入 则 每个 输入 将 具有 
分配给 它 的 一个 关联 权重 我们 随机 初始化 权重 
并在 模型 训练 过程 中 更新 这些 权重 训练 后的/nr 
神经 网络 对 其 输入 赋予 较高 的 权重 这是 
它 认为 与 不 那么 重要 的 输入 相比 更为 
重要 的 输入 为零 的 权重 则 表示 特定 的 
特征 是 微不足道 的 让 我们 假设 输入 为 a 
并且 与其 相 关联 的 权重 为 W1 那么 在 
通过 节点 之后 输入 变为 a * W13 偏差 Bias 
除了 权重 之外 另 一个 被 应用于 输入 的 线性 
分量 被称为 偏差 它 被 加到 权重 与 输入 相乘 
的 结果 中 基本上 添加 偏差 的 目的 是 来 
改变 权重 与 输入 相乘 所得 结果 的 范围 的 
添加 偏差 后 结果 将 看起来 像 a * W1 
+ 偏差 这是 输入 变换 的 最终 线性 分量 4 
激活 函数 Activation Function 一旦 将 线性 分量 应用于 输入 
将会 需要 应用/nr 一个 非 线性函数 这 通过 将 激活 
函数 应用于 线性组合 来 完成 激活 函数 将 输入 信号 
转换 为 输出 信号 应用 激活 函数 后的/nr 输出 看起来 
像 f a * W1 + b 其中 f 就是 
激活 函数 在下 图中 我们 将 n 个 输入 给 
定为 X1 到 Xn 而与 其 相应 的 权重 为 
Wk1 到 Wkn 我们 有 一个 给 定值 为 bk 
的 偏差 权重 首先 乘以 与其 对应 的 输入 然后 
与 偏差 加在一起 而 这个 值 叫做 u U = 
Σ W * X + b 激活 函数 被 应用于 
u 即 f u 并且 我们 会 从 神经元 接收 
最终 输出 如 yk = f u 常用 的 激活 
函数 最 常用 的 激活 函数 就是 Sigmoid ReLU 和 
softmaxa Sigmoid 最 常用 的 激活 函数 之一 是 Sigmoid 
它 被 定义 为 来源 维基百科 Sigmoid 变换 产生 一个 
值 为 0 到 1 之间 更 平滑 的 范围 
我们 可能 需要 观察 在 输入 值 略有 变化 时 
输出 值 中 发生 的 变化 光滑 的 曲线 使 
我们 能够 做到 这 一点 因此 优于 阶跃 函数 b 
ReLU 整流 线性 单位 与 Sigmoid 函数 不同 的 是 
最近 的 网络 更 喜欢 使用 ReLu 激活 函数 来 
处理 隐藏 层 该 函数 定义 为 当 X 0时 
函数 的 输出 值 为 X 当 X = 0时 
输出 值 为 0 函数 图 如下 图 所示 来源 
cs231n 使用 ReLU 函数 的 最主要 的 好处 是 对于 
大于 0 的 所有 输入 来说 它 都 有一个 不变 
的 导 数值 常数 导 数值 有助于 网络 训练 进行 
得 更快 c   Softmax Softmax 激活 函数 通常用于 输出 
层 用于 分类 问题 它 与 sigmoid 函数 是 很 
类似 的 唯一 的 区别 就是 输出 被 归一 化为 
总和 为 1 Sigmoid 函数 将 发挥 作用 以防 我们 
有一个 二进制 输出 但是 如果 我们 有 一个 多类 分类 
问题 softmax 函数 使为 每个 类 分配 值 这种 操作 
变得 相当 简单 而这 可以 将 其 解释 为 概率 
以 这种 方式 来 操作 的 话 我们 很 容易 
看到 假设 你 正在 尝试 识别 一个 可能 看 起来 
像 8 的 6 该 函数 将为 每个 数字 分配 
值 如下 我们 可以 很容易 地 看出 最高 概率 被 
分配 给 6 而 下 一个 最高 概率 分配给 8 
依此类推 5 神经网络 Neural Network 神经 网络 构成 了 深度 
学习 的 支柱 神经 网络 的 目标 是 找到 一个 
未知 函数 的 近似值 它 由 相互 联系 的 神经元 
形成 这些 神经元 具 有权 重和 在 网络 训练 期间 
根据 错误 来 进行 更新 的 偏差 激活 函数 将 
非 线性变换 置于 线性组合 而 这个 线性组合 稍后 会 生成 
输出 激活 的 神经元 的 组合 会 给出 输出 值 
一个 很好 的 神经 网络 定义 神经网络 由 许多 相互 
关联 的 概念化 的 人造 神经元 组成 它们 之间 传递 
相互 数据 并且 具有 根据 网络 经验 调整 的 相关 
权重 神经元 具有 激活 阈值 如果 通过 其 相关 权重 
的 组合 和 传递 给 他们 的 数据 满足 这个 
阈值 的话 其 将被 解雇 发射 神经元 的 组合 导致 
学习 6 输入 / 输出 / 隐藏 层 Input / 
Output / Hidden Layer 正如 它们 名字 所 代表 的 
那样 输入 层 是 接收 输入 那 一层 本质上 是 
网络 的 第一 层 而 输出 层 是 生成 输出 
的 那 一层 也 可以 说 是 网络 的 最终 
层 处理 层 是 网络 中 的 隐藏 层 这些 
隐藏 层 是 对 传入 数据 执行 特定 任务 并 
将其 生成 的 输出 传递 到 下 一层 的 那些 
层 输入 和 输出 层 是 我们 可见 的 而 
中间层 则是 隐藏 的 来源 cs231n7 MLP 多层 感知器 单个 
神经元 将 无法 执行 高度 复杂 的 任务 因此 我们 
使用 堆栈 的 神经元 来生 成 我们 所 需要 的 
输出 在最 简单 的 网络 中 我们 将 有一个 输入 
层 一个 隐藏 层 和 一个 输出 层 每个 层 
都有 多个 神经元 并且 每个 层 中 的 所有 神经元 
都 连接 到 下 一层 的 所有 神经元 这些 网络 
也 可以 被 称为 完全 连接 的 网络 8 正向 
传播 Forward Propagation 正向 传播 是 指 输入 通过 隐藏 
层 到 输出 层 的 运动 在 正向 传播 中 
信息 沿着 一个 单一 方向 前进 输入 层 将 输入 
提供给 隐藏 层 然后 生成 输出 这 过程 中 是 
没有 反向 运动 的 9 成本 函数 Cost Function 当 
我们 建立 一个 网络 时 网络 试图 将 输出 预 
测得 尽可能 靠近 实际 值 我们 使用 成本 / 损失 
函数 来 衡量 网络 的 准确性 而 成本 或 损失 
函数 会在 发生 错误 时 尝试 惩罚 网络 我们 在 
运行 网络 时的/nr 目标 是 提高 我们 的 预测 精度 
并 减少 误差 从而 最大限度 地 降低 成本 最 优化 
的 输出 是 那些 成本 或 损失 函数值 最小 的 
输出 如果 我 将 成本 函数 定义 为 均方 误差 
则 可以 写 为 C = 1 / m ∑ 
y – a ^ 2 其中 m 是 训练 输入 
的 数量 a 是 预测 值 y 是 该 特定 
示例 的 实际 值 学习 过程 围绕 最小化 成本 来 
进行 10 梯度 下降 Gradient Descent 梯度 下降 是 一种 
最小化 成本 的 优化 算法 要 直观 地 想一想 在 
爬山 的 时候 你 应该 会 采取 小 步骤 一步 
一步 走 下来 而 不是 一下子 跳下来 因此 我们 所做 
的 就是 如果 我们 从 一个 点 x 开始 我们 
向下/nr 移动 一点 即 Δ h 并将 我们 的 位置 
更新 为 x Δ h 并且 我们 继续 保持 一致 
直到 达到 底部 考虑 最 低成本 点 图 https / 
/ www . youtube . com / watch v = 
5u4G23 _ OohI 在 数学 上 为了 找到 函数 的 
局部 最小值 我们 通常 采取 与 函数 梯度 的 负数 
成 比例 的 步长 你 可以 通过 这 篇 文章 
来 详细 了解 梯度 下降 11 学习率 Learning Rate 学习率 
被 定义 为 每次 迭代 中 成本 函数 中 最小化 
的 量 简单 来说 我们 下降 到 成本 函数 的 
最小值 的 速率 是 学习率 我们 应该 非常 仔细 地 
选择 学习率 因为 它 不 应该 是 非常 大 的 
以至于 最佳 解决 方案 被 错过 也 不 应该 非常 
低 以至于 网络 需要 融合 http / / cs231n . 
github . io / neural networks 3/12 反向 传播 Backpropagation 
当 我们 定义 神经 网络 时 我们 为 我们 的 
节点 分配 随机 权重 和 偏差 值 一旦 我们 收到 
单次 迭代 的 输出 我们 就 可以 计算 出 网络 
的 错误 然后 将该 错误 与 成本 函数 的 梯度 
一起 反馈 给 网络 以 更新 网络 的 权重 最后 
更新 这些 权重 以便 减少 后续 迭代 中 的 错误 
使用 成本 函数 的 梯度 的 权重 的 更新 被称为 
反向 传播 在 反向 传播 中 网络 的 运动 是 
向后 的 错误 随着 梯度 从 外层 通过 隐藏 层 
流回 权重 被 更新 13 批次 Batches 在 训练 神经 
网络 的 同时 不用 一次 发送 整个 输入 我们 将 
输入 分成 几个 随机 大小 相等 的 块 与 整个 
数据集 一次性 馈送 到 网络 时 建立 的 模型 相比 
批量 训练 数据 使得 模型 更加 广义 化 14 周期 
Epochs 周期 被 定义 为 向前 和 向后 传播 中 
所有 批次 的 单次 训练 迭代 这 意味着 1个 周期 
是 整个 输入 数据 的 单次 向前 和 向后 传递 
你 可以 选择 你 用来 训练 网络 的 周期 数量 
更多 的 周期 将 显示 出 更高 的 网络 准确性 
然而 网络 融合 也 需要 更长 的 时间 另外 你 
必须 注意 如果 周期数 太高 网络 可能 会 过度 拟合 
15 丢弃 Dropout Dropout 是 一种 正则化 技术 可 防止 
网络 过度 拟合 套 顾名思义 在 训练 期间 隐藏 层 
中的 一定 数量 的 神经元 被 随机 地 丢弃 这 
意味着 训练 发生 在 神经 网络 的 不同 组合 的 
神经 网络 的 几个 架构 上 你 可以 将 Dropout 
视为 一种 综合 技术 然后 将 多个 网络 的 输出 
用于 产生 最终 输出 来源 Original paper16 批量 归一化 Batch 
Normalization 作为 一个 概念 批量 归一化 可以 被 认为 是 
我们 在 河流 中 设定 为 特定 检查点 的 水坝 
这样 做 是 为了 确保 数据 的 分发 与 希望 
获得 的 下一层 相同 当 我们 训练 神经 网络 时 
权重 在 梯度 下降 的 每个 步骤 之后 都会 改变 
这会 改变 数据 的 形状 如何 发送到 下一层 但是 下 
一层 预期 分布 类似于 之前 所 看到 的 分布 所以 
我们 在 将 数据 发送 到 下一层 之前 明确 规范化 
数据 卷积 神经网络 17 滤波器 Filters CNN 中的 滤波器 与 
加权 矩阵 一样 它 与 输入 图像 的 一部分 相乘 
以 产生 一个 回旋 输出 我们 假设 有一个 大小 为 
28 * 28 的 图像 我们 随机 分配 一个 大小 
为 3 * 3 的 滤波器 然后 与 图像 不同 
的 3 * 3 部分 相乘 形成 所谓 的 卷积 
输出 滤波器 尺寸 通常 小于 原始 图像 尺寸 在 成本 
最小化 的 反向 传播 期间 滤波器 值 被 更新 为重 
量值 参考 一下 下图 这里 filter 是 一个 3 * 
3 矩阵 与 图像 的 每个 3 * 3部 分相 
乘以 形成 卷积 特征 18 卷积 神经网络 CNN 卷积 神经 
网络 基本 上 应用 于 图像 数据 假设 我们 有 
一个 输入 的 大小 28 * 28 * 3 如果 
我们 使用 正常 的 神经 网络 将有 2352 28 * 
28 * 3 参数 并且 随着 图像 的 大小 增加 
参数 的 数量 变得 非常 大 我们 卷积 图像 以 
减少 参数 数量 如 上面 滤波器 定义 所示 当/t 我们/r 
将/d 滤波器/nz 滑/v 动到/v 输入/v 体积/n 的/uj 宽度/n 和/c 高度/n 
时/n 将 产生 一个二维 激活 图 给出 该 滤波器 在 
每个 位置 的 输出 我们 将 沿 深度 尺寸 堆叠 
这些 激活 图 并 产生 输出量 你 可以 看到 下 
面的 图 以 获得 更 清晰 的 印象 19 池化/nr 
Pooling 通常在 卷积 层 之间 定期 引入 池层/nr 这 基本上 
是 为了 减少 一些 参数 并 防止 过度 拟合 最 
常见 的 池化/nr 类型 是 使用 MAX 操作 的 滤波器 
尺寸 2 2 的 池层/nr 它 会做 的 是 它 
将 占用 原始 图像 的 每个 4 * 4 矩阵 
的 最大值 来源 cs231n 你 还 可以 使用 其他 操作 
如 平均 池 进行 池化/nr 但是 最大 池 数量 在 
实践 中 表现 更好 20 填充 Padding 填充 是 指在 
图像 之间 添加 额外 的 零层 以使 输出 图像 的 
大小 与 输入 相同 这 被 称为 相同 的 填充 
在 应用 滤波器 之后 在 相同 填充 的 情况 下 
卷积 层 具有 等于 实际 图像 的 大小 有效 填充 
是 指 将 图像 保持 为 具有 实际 或 有效 
的 图像 的 所有 像素 在 这种 情况 下 在 
应用 滤波器 之后 输出 的 长度 和 宽度 的 大小 
在 每个 卷积 层 处 不断 减小 21 数据 增强 
Data Augmentation 数据 增强 是 指 从 给定 数据 导出 
的 新 数据 的 添加 这 可能 被 证明 对 
预测 有益 例如 如果 你 使 光线 变亮 可能 更容易 
在 较 暗 的 图像 中 看到 猫 或者 例如 
数字 识别 中的 9 可能 会 稍微 倾斜 或 旋转 
在 这种 情况 下 旋转 将 解决问题 并 提高 我们 
的 模型 的 准确性 通过 旋转 或 增亮 我们 正在 
提高 数据 的 质量 这 被 称为 数据 增强 循环 
神经网络 22 循环 神经元 Recurrent Neuron 循环 神经元 是 在 
T 时间内 将 神经元 的 输出 发 送回 给 它 
如果 你 看图 输出 将 返回 输入 t 次 展开 
的 神经元 看起来 像 连接 在 一起 的 t 个 
不同 的 神经元 这个 神经元 的 基本 优点 是 它 
给 出了 更 广义 的 输出 23 循环 神经网络 RNN 
循环 神经网络 特别 用于 顺序 数据 其中 先前 的 输出 
用于 预测 下 一个 输出 在 这种 情况 下 网络 
中有 循环 隐藏 神经元 内 的 循环 使 他们 能够 
存储 有关 前 一个 单词 的 信息 一段 时间 以便 
能够 预测 输出 隐藏 层 的 输出 在 t 时间戳 
内 再次 发送到 隐藏 层 展开 的 神经元 看起来 像 
上图 只有 在 完成 所有 的 时间 戳 后 循环 
神经元 的 输出 才能 进入 下 一层 发送 的 输出 
更 广泛 以前 的 信息 保留 的 时间 也 较长 
然后 根据 展开 的 网络 将 错误 反向 传播 以 
更新 权重 这 被 称为 通过 时间 的 反向 传播 
BPTT 24 消失 梯度 问题 Vanishing Gradient Problem 激活 函数 
的 梯度 非常 小 的 情况下 会 出现 消失 梯度 
问题 在 权重 乘以 这些 低 梯度 时的/nr 反向 传播 
过程 中 它们 往往 变得 非常 小 并且 随着 网络 
进一步 深入 而 消失 这 使得 神经网络 忘记了 长距离 依赖 
这对 循环 神经网络 来说 是 一个 问题 长期 依赖 对于 
网络 来说 是 非常 重要 的 这 可以 通过 使用 
不 具有 小 梯度 的 激活 函数 ReLu 来 解决 
25 激增 梯度 问题 Exploding Gradient Problem 这与 消失 的 
梯度 问题 完全 相反 激活 函数 的 梯度 过大 在 
反向 传播 期间 它 使 特定 节点 的 权重 相对于 
其他 节点 的 权重 非常 高 这 使得 它们 不 
重要 这 可以 通过 剪切 梯度 来 轻松 解决 使其 
不 超过 一定 值 作者   多 啦 A 亮 
& 阿童木 来自 机器人 圈 jiqirenchanye 