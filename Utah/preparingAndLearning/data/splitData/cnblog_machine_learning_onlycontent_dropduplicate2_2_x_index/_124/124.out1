http / / www personal . umich . edu / 
~ jizhu / jizhu / wuke / Friedman AoS01 . 
pdfhttps / / www . cnblogs . com / bentuwuying 
/ p / 6667267 . htmlhttps / / www . 
cnblogs . com / ModifyRong / p / 7744987 . 
htmlhttps / / www . cnblogs . com / bentuwuying 
/ p / 6264004 . html1 . 简介 gbdt 全称 
梯度 下降 树 在 传统 机器学习 算法 里面 是 对 
真实 分布 拟合 的 最好 的 几种 算法 之一 在 
前几年 深度 学习 还 没有 大行其道 之前 gbdt 在 各种 
竞赛 是 大放异彩 原因 大概 有 几个 一是 效果 确实 
挺 不错 二 是 即 可以 用于 分类 也 可以 
用于 回归 三 是 可以 筛选 特征 这 三点 实在 
是 太 吸引人 了 导致 在 面试 的 时候 大家 
也 非常 喜欢 问 这个 算法   gbdt 的 面试 
考核 点 大致 有 下面 几个 gbdt 的 算法 的 
流程 gbdt 如何 选择 特征 gbdt 如何 构建 特征 gbdt 
如何 用于 分类 gbdt 通过 什么 方式 减少 误差 gbdt 
的 效果 相比 于 传统 的 LR SVM 效果 为什么 
好一些 gbdt 如何 加速 训练 gbdt 的 参数 有 哪些 
如何 调 参 gbdt 实战 当中 遇到 的 一些 问题 
gbdt 的 优缺点 2 . 正式 介绍 首先 gbdt 是 
通过 采用 加法 模型 即 基 函数 的 线性组合 以及 
不断 减小 训练 过程 产生 的 残差 来 达到 将 
数据 分类 或者 回归 的 算法 gbdt 的 训练 过程 
我们 通过 一张 图片 图片 来源 来 说明 gbdt 的 
训练 过程 图 1 GBDT 的 训练 过程 gbdt 通过 
多轮 迭代 每 轮 迭代 产生 一个 弱 分类器 每个 
分类器 在上 一轮 分类器 的 残差 基础 上 进行 训练 
对 弱 分类器 的 要求 一般 是 足够 简单 并且/c 
是/v 低/a 方差/n 和高/nr 偏差/n 的/uj 因为 训练 的 过程 
是 通过 降低 偏差 来 不断 提高 最终 分类器 的 
精度 此处 是 可以 证明 的 弱 分类器 一般 会 
选择 为 CART TREE 也 就是 分类 回归 树 由于/c 
上述/b 高/a 偏差/n 和/c 简单/a 的/uj 要求/v  /i 每个/r 分类/n 
回归/v 树/v 的/uj 深度/ns 不会/v 很深/i 最终 的 总 分类器 
是 将 每 轮 训练 得到 的 弱 分类器 加权 
求和 得到 的 也 就是 加法 模型 模型 最终 可以 
描述 为 Fm x = ∑ m = 1MT x 
θ m Fm x = ∑ m = 1MT x 
θ m 模型 一共 训练 M 轮 每 轮 产生 
一个 弱 分类器   T x θ m T x 
θ m 弱 分类器 的 损失 函数 θ ^ m 
= argmin θ m ∑ i = 1NL yi Fm 
− 1 xi + T xi θ m θ ^ 
m = arg ⁡ min θ m ⁡ ∑ i 
= 1NL yi Fm − 1 xi + T xi 
θ m Fm − 1 x Fm − 1 x 
  为 当前 的 模型 gbdt 通过 经验 风险 极小 
化 来 确定 下 一个 弱 分类器 的 参数 具体 
到 损失 函数 本身 的 选择 也 就是 L 的 
选择 有 平方 损失 函数 0 1 损失 函数 对数 
损失 函数 等等 如果 我们 选择 平方 损失 函数 那么 
这个 差值 其实 就是 我们 平常 所说 的 残差 但是 
其实 我们 真正 关注 的 1 . 是 希望 损失 
函数 能够 不断 的 减小 2 . 是 希望 损失 
函数 能够 尽可能 快 的 减小 所以 如何 尽可能 快 
的 减小 呢 让 损失 函数 沿着 梯度方向 的 下降 
这个 就是 gbdt 的 gb 的 核心 了 利用 损失 
函数 的 负 梯度 在 当前 模型 的 值 作为 
回归 问题 提升 树 算法 中的 残差 的 近似值 去 
拟合 一个 回归 树 gbdt 每 轮 迭代 的 时候 
都去 拟合 损失 函数 在 当前 模型 下 的 负 
梯度 这样 每 轮 训练 的 时候 都 能够 让 
损失 函数 尽可能 快 的 减小 尽快 的 收敛 达到 
局部 最优 解 或者 全局 最优 解 gbdt 如何 选择 
特征 gbdt 选择 特征 的 细节 其实 是 想问 你 
CART Tree 生成 的 过程 这里 有 一个 前提 gbdt 
的 弱 分类器 默认 选择 的 是 CART TREE 其实 
也 可以 选择 其他 弱 分类器 的 选择/v 的/uj 前提/n 
是/v 低/a 方差/n 和高/nr 偏差/n 框架 服从 boosting 框架 即可 
下面 我们 具体 来说 CART TREE 是 一种 二叉树 如何 
生成 CART TREE 生成 的 过程 其实 就是 一个 选择 
特征 的 过程 假设 我们 目前 总共有 M 个 特征 
第一步 我们 需要 从中/nr 选择 出 一个 特征 j 做为 
二叉树 的 第一 个 节点 然后 对 特征 j 的 
值 选择 一个 切 分点 m . 一个 样本 的 
特征 j 的 值 如果 小于 m 则 分为 一类 
如果 大于 m 则 分为 另外 一类 如此 便 构建 
了 CART 树 的 一个 节点 其他 节点 的 生成 
过程 和 这个 是 一样 的 现在 的 问题 是 
在 每 轮 迭代 的 时候 如何 选择 这个 特征 
j 以及 如何 选择 特征 j 的 切 分点 m 
原始 的 gbdt 的 做法 非常 的 暴力 首先 遍历 
每个 特征 然后 对 每个 特征 遍历 它 所有 可能 
的 切 分点 找到 最优 特征 m 的 最优 切 
分点 j 如何 衡量 我们 找到 的 特征 m 和切/nr 
分点 j 是 最优 的 呢 我们 用 定义 一个 
函数 FindLossAndSplit 来 展示 一下 求解 过程 1 def findLossAndSplit 
x y 2 # 我们 用 x 来 表示 训练 
数据 3 # 我们 用 y 来 表示 训练 数据 
的 label 4 # x i 表示 训练 数据 的 
第 i 个 特征 5 # x _ i 表示 
第 i 个 训练样本 6 7 # minLoss 表示 最小 
的 损失 8 minLoss = Integet . max _ value 
9 # feature 表示 是 训练 的 数据 第几 纬度 
的 特征 10 feature = 0 11 # split 表示 
切 分点 的 个数 12 split = 0 13 14 
# M 表示 样本 x 的 特征 个数 15 for 
j in range 0 M 16 # 该 维 特征 
下 特征值 的 每个 切 分点 这里 具体 的 切分 
方式 可以 自己 定义 17 for c in range 0 
x j 18 L = 0 19 # 第一类 20 
R1 = { x | x j = c } 
21 # 第二类 22 R2 = { x | x 
j c } 23 # 属于 第一 类 样本 的 
y 值 的 平均值 24 y1 = ave { y 
| x 属于 R1 } 25 # 属于 第二类 样本 
的 y 值 的 平均值 26 y2 = ave { 
y | x 属于 R2 } 27 # 遍历 所有 
的 样本 找到 loss funtion 的 值 28 for x 
_ 1 in all x 29 if x _ 1 
属于 R1 30 L + = y _ 1 y1 
^ 2 31 else 32 L + = y _ 
1 y2 ^ 2 33 if L minLoss 34 minLoss 
= L 35 feature = i 36 split = c 
37 return minLoss feature split 如果 对 这段 代码 不是 
很 了解 的 可以 先 去 看看 李航 第五 章中 
对 CART TREE 算法 的 叙述 在 这里 我们 先 
遍历 训练样本 的 所有 的 特征 对于 特征 j 我们 
遍历 特征 j 所有 特征值 的 切 分点 c 找到 
可以 让 下面 这个 式子 最小 的 特征 j 以及 
切 分点 c . gbdt 如何 构建 特征 其实 说 
gbdt 能够 构建 特征 并非 很 准确 gbdt 本身 是 
不能 产生 特征 的 但是 我们 可以 利用 gbdt 去 
产生 特征 的 组合 在 CTR 预 估中 工业界 一般 
会 采用 逻辑 回归 去 进行 处理 在 我 的 
上一 篇 博文 当中 已经 说过 逻辑 回归 本身 是 
适合 处理 线性 可分 的 数据 如果 我们 想 让 
逻辑 回归 处理 非线性 的 数据 其中 一种 方式 便是 
组合 不同 特征 增强 逻辑 回归 对 非线性 分布 的 
拟合 能力 长久以来 我们 都是/nr 通过 人工 的 先验 知识 
或者 实验 来 获得 有效 的 组合 特征 但是 很多 
时候 使用 人工 经验 知识 来 组合 特征 过于 耗费 
人力 造成了 机器学习 当中 一个 很 奇特 的 现象 有/v 
多少/m 人工/n 就/d 有/v 多少/m 智能/n 关键 是 这样 通过 
人工 去 组合 特征 并不 一定 能够 提升 模型 的 
效果 所以 我们 的 从业者 或者 学 界 一直 都 
有一个 趋势 便是 通过 算法 自动 高效 的 寻找 到 
有效 的 特征 组合 Facebook 在 2014年 发表 的 一篇 
论文 便是 这种 尝试 下 的 产物 利用 gbdt 去 
产生 有效 的 特征 组合 以便 用于 逻辑 回归 的 
训练 提升 模型 最终 的 效果 图 2 用 GBDT 
构造 特征 如图 2 所示 我们 使用 GBDT 生成 了 
两棵树 两颗 树 一 共有 五个 叶子 节点 我们 将 
样本 X 输入 到 两颗 树 当中 去 样本 X 
落在 了 第一 棵树 的 第二个 叶子 节点 第二颗 树 
的 第一 个 叶子 节点 于是 我们 便 可以 依次 
构建 一个 五 纬 的 特征向量 每一个 纬度 代表 了 
一个 叶子 节点 样本 落 在 这个 叶子 节点 上面 
的话 那么 值 为 1 没有 落在 该 叶子 节点 
的话 那么 值 为 0 . 于是 对于 该 样本 
我们 可以 得到 一个 向量 0 1 0 1 0 
作为 该 样本 的 组合 特征 和 原来 的 特征 
一起 输入 到 逻辑 回归 当中 进行 训练 实验 证明 
这样 会 得到 比较 显著 的 效果 提升 GBDT 如何 
用于 分类 首先 明确 一点 gbdt 无论 用于 分类 还是 
回归 一直都 是 使用 的 CART 回归 树 不会 因为 
我们 所 选择 的 任务 是 分类 任务 就 选用 
分类 树 这 里面 的 核心 是 因为 gbdt 每 
轮 的 训练 是 在上 一轮 的 训练 的 残差 
基础 之上 进行 训练 的 这里 的 残差 就是 当前 
模型 的 负 梯度 值 这个 要求 每 轮 迭代 
的 时候 弱 分类器 的 输出 的 结果 相减 是 
有 意义 的 残差 相减 是 有 意义 的 如果 
选用 的 弱 分类器 是 分类 树 类别 相减 是 
没有 意义 的 上 一轮 输出 的 是 样本 x 
属于 A 类 本 一轮 训练 输出 的 是 样本 
x 属于 B 类 A 和 B 很多 时候 甚至 
都 没有 比较 的 意义 A 类 B 类 是 
没有 意义 的 我们 具体 到 分类 这个 任务 上面 
来 我们 假设 样本 X 总共有 K 类 来 了 
一个 样本 x 我们 需要 使用 gbdt 来 判断 x 
属于 样本 的 哪 一类 图三 gbdt 多 分类 算法 
流程 第一步 我们 在 训练 的 时候 是 针对 样本 
X 每个 可能 的 类 都 训练 一个 分类 回归 
树 举例说明 目前 样本 有 三类 也 就是 K = 
3 样本 x 属于 第二类 那么 针对 该 样本 x 
的 分类 结果 其实 我们 可以 用 一个 三维 向量 
0 1 0 来 表示 0 表示 样本 不属于 该类 
1 表示 样 本属于 该类 由于 样本 已经 属于 第二类 
了 所以 第二类 对应 的 向量 维度 为 1 其他 
位置 为 0 针对 样本 有 三类 的 情况 我们 
实质上 是 在 每 轮 的 训练 的 时候 是 
同时 训练 三颗 树 第一颗 树 针对 样本 x 的 
第一 类 输入 为 x 0 x 0 第二颗 树 
输入 针对 样本 x 的 第二 类 输入 为 x 
1 x 1 第三颗 树 针对 样本 x 的 第三 
类 输入 为 x 0 x 0 在 这里 每颗 
树 的 训练 过程 其实 就是 就是 我们 之前 已经 
提到 过 的 CATR TREE 的 生成 过程 在 此处 
我们 参照 之前 的 生成树 的 程序 即 可以 就 
解出 三颗 树 以及 三颗 树 对 x 类别 的 
预测 值 f1 x f2 x f3 x f1 x 
f2 x f3 x 那么 在 此类 训练 中 我们 
仿照 多 分类 的 逻辑 回归 使用 softmax 来 产生 
概率 则 属于 类别 1 的 概率 p1 = exp 
f1 x / ∑ k = 13exp fk x p1 
= exp f1 x / ∑ k = 13exp fk 
x 并且 我们 我们 可以 针对 类别 1 求出 残差 
y11 x = 0 − p1 x y11 x = 
0 − p1 x 类别 2 求出 残差 y22 x 
= 1 − p2 x y22 x = 1 − 
p2 x 类别 3 求出 残差 y33 x = 0 
− p3 x y33 x = 0 − p3 x 
. 然后 开始 第二 轮 训练 针对 第一类 输入 为 
x y11 x y11 x 针对 第二类 输入 为 x 
y22 x y22 x 针对 第三类 输入 为 x y33 
x y33 x . 继续 训练 出 三颗 树 一直 
迭代 M 轮 每 轮 构建 3颗 树 所以 当 
K = 3 我们 其实 应该 有 三个 式子 F1M 
x = ∑ m = 1MC1m ^ I x ϵ 
R1m F1M x = ∑ m = 1MC1m ^ I 
x ϵ R1m F2M x = ∑ m = 1MC2m 
^ I x ϵ R2m F2M x = ∑ m 
= 1MC2m ^ I x ϵ R2m F3M x = 
∑ m = 1MC3m ^ I x ϵ R3m F3M 
x = ∑ m = 1MC3m ^ I x ϵ 
R3m 当 训练 完毕 以后 新来 一个 样本 x1 我们 
需要 预测 该 样本 的 类别 的 时候 便 可以 
有 这三个 式子 产生 三个 值 f1 x f2 x 
f3 x f1 x f2 x f3 x 样 本属于 
某个 类别 c 的 概率 为 pc = exp fc 
x / ∑ k = 13exp fk x pc = 
exp fc x / ∑ k = 13exp fk x 
GBDT 多 分类 举例说明 上面 的 理论 阐述 可能 仍旧 
过于 难懂 我们 下面 将 拿 Iris 数据 集中 的 
六个 数据 作为 例子 来 展示 gbdt 多 分类 的 
过程 样本 编号 花萼 长度 cm 花萼 宽度 cm 花瓣 
长度 cm 花瓣/n 宽度/n 花的/nr 种类/n 15/m ./i 13.51/mx ./i 
40.2/mx 山/n 鸢尾/n 24/m ./i 93.01/mx ./i 40.2/mx 山/n 鸢尾/n 
37/m ./i 03.24/mx ./i 71.4/mx 杂色/n 鸢尾/n 46/m ./i 43.24/mx 
./i 51.5/mx 杂色/n 鸢尾/n 56/m ./i 33.36/mx ./i 02.5/mx 维吉尼亚/ns 
鸢尾/n 65/m ./i 82.75/mx ./i 11.9/mx 维吉尼亚/ns 鸢尾/n 图/n 四/m 
Iris 数据集 这 是 一个 有 6个 样本 的 三分 
类 问题 我们 需要 根据 这个 花的/nr 花萼 长度 花萼 
宽度 花瓣 长度 花瓣 宽度 来 判断 这个 花属/nr 于山 
鸢尾 杂色 鸢尾 还是 维吉尼亚 鸢尾 具体 应用 到 gbdt 
多 分类 算法 上面 我们 用 一个三维 向量 来 标志 
样本 的 label 1 0 0 表示 样 本属 于山 
鸢尾 0 1 0 表示 样 本属于 杂色 鸢尾 0 
0 1 表示 属于 维吉尼亚 鸢尾 gbdt 的 多 分类 
是 针对 每个 类 都 独立 训练 一个 CART Tree 
所以 这里 我们 将 针对 山 鸢尾 类别 训练 一个 
CART Tree 1 杂色 鸢尾 训练 一个 CART Tree 2 
维吉尼亚 鸢尾 训练 一个 CART Tree 3 这三个 树 相互 
独立 我们 以 样本 1 为例 针对 CART Tree1 的 
训练 样本 是 5.1 3.5 1.4 0.2 5.1 3.5 1.4 
0.2 label 是 1 最终 输入 到 模型 当中 的 
为 5.1 3.5 1.4 0.2 1 5.1 3.5 1.4 0.2 
1 针对 CART Tree2 的 训练 样本 也是 5.1 3.5 
1.4 0.2 5.1 3.5 1.4 0.2 但是 label 为 0 
最终 输入 模型 的 为 5.1 3.5 1.4 0.2 0 
5.1 3.5 1.4 0.2 0 . 针对 CART Tree 3 
的 训练样本 也是 5.1 3.5 1.4 0.2 5.1 3.5 1.4 
0.2 label 也为 0 最终 输入 模型 当中 的 为 
5.1 3.5 1.4 0.2 0 5.1 3.5 1.4 0.2 0 
. 下面 我们 来看 CART Tree1 是 如何 生成 的 
其他 树 CART Tree2 CART Tree 3 的 生成 方式 
是 一样 的 CART Tree 的 生成 过程 是从 这四个 
特征 中 找 一个 特征 做为 CART Tree1 的 节点 
比如 花萼 长度 做为 节点 6个 样本 当中 花萼 长度 
大于 5.1 cm 的 就是 A 类 小于 等于 5.1 
cm 的 是 B 类 生成 的 过程 其实 非常 
简单 问题 1 . 是 哪个 特征 最合适 2 . 
是 这个 特征 的 什么 特征值 作为 切 分点 即使 
我们 已经 确定 了 花萼 长度 做为 节点 花萼 长度 
本身 也 有 很多 值 在 这里 我们 的 方式 
是 遍历 所有 的 可能性 找到 一个 最好 的 特征 
和它/nr 对应 的 最优 特征值 可以 让 当前 式子 的 
值 最小 我们 以 第一 个 特征 的 第一 个 
特征值 为例 R1 为 所有 样本 中 花萼 长度 小于 
5.1 cm 的 样本 集合 R2 为 所有 样本 当中 
花萼 长度 大于 等于 5 . 1cm 的 样本 集合 
所以   R1 = { 2 } R1 = { 
2 } R2 = { 1 3 4 5 6 
} R2 = { 1 3 4 5 6 } 
. 图 5 节点 分裂 示意图 y1 为 R1 所有 
样本 的 label 的 均值   1/1 = 11/1 = 
1 y2 为 R2 所有 样本 的 label 的 均值 
  1 + 0 + 0 + 0 + 0 
/ 5 = 0.2 1 + 0 + 0 + 
0 + 0 / 5 = 0.2 下面 便 开始 
针对 所有 的 样本 计算 这个 式子 的 值 样本 
1 属于 R2 计算 的 值 为 1 − 0.2 
2 1 − 0.2 2 样本 2 属于 R1 计算 
的 值 为 1 − 1 2 1 − 1 
2 样本 3 4 5 6 同理 都是 属于 R2 
的 所以 值 是 0 − 0.2 2 0 − 
0.2 2 . 把这 六个 值 加起来 便是 山 鸢尾 
类型 在 特征 1 的 第一 个 特征值 的 损失 
值 这里 算出来 1 0.2 ^ 2 + 1 1 
^ 2 + 0 0.2 ^ 2 + 0 0.2 
^ 2 + 0 0.2 ^ 2 + 0 0.2 
^ 2 = 0.84 接着 我们 计算 第一 个 特征 
的 第二 个 特征值 计算 方式 同上 R1 为 所有 
样本 中 花萼 长度 小于 4.9 cm 的 样本 集合 
R2 为 所有 样本 当中 花萼 长度 大于 等于 4.9 
cm 的 样本 集合 . 所以   R1 = { 
} R1 = { } R1 = { 1 2 
3 4 5 6 } R1 = { 1 2 
3 4 5 6 } .   y1 为 R1 
所有 样本 的 label 的 均值 = 0 y2 为 
R2 所有 样本 的 label 的 均值   1 + 
1 + 0 + 0 + 0 + 0 / 
6 = 0.3333 1 + 1 + 0 + 0 
+ 0 + 0 / 6 = 0.3333 图 6 
第一 个 特征 的 第二 个 特侦 值 的 节点 
分裂 情况 我们 需要 针对 所有 的 样本 样本 1 
属于 R2 计算 的 值 为 1 − 0.333 2 
1 − 0.333 2 样本 2 属于 R2 计算 的 
值 为 1 − 0.333 2 1 − 0.333 2 
样本 3 4 5 6 同理 都是 属于 R2 的 
所以 值 是 0 − 0.333 2 0 − 0.333 
2 . 把这 六个 值 加起来 山 鸢尾 类型 在 
特征 1 的 第二 个 特征值 的 损失 值 这里 
算出来 1 0.333 ^ 2 + 1 0.333 ^ 2 
+ 0 0.333 ^ 2 + 0 0.333 ^ 2 
+ 0 0.333 ^ 2 + 0 0.333 ^ 2 
= 2.244189 . 这里 的 损失 值 大于 特征 一 
的 第一 个 特征值 的 损失 值 所以 我们 不 
取 这个 特征 的 特征值 图 7 所有 情况 说明 
这样 我们 可以 遍历 所有 特征 的 所有 特征值 找到 
让 这个 式子 最小 的 特征 以及 其 对应 的 
特征值 一 共有 24种 情况 4个 特征 * 每个 特征 
有 6个 特征值 在 这里 我们 算 出来 让 这个 
式子 最小 的 特征 花萼 长度 特征值 为 5.1 cm 
这个 时候 损失 函数 最小 为 0.8 于是 我们 的 
预测 函数 此时 也 可以 得到 f x = ∑ 
x ϵ R1y1 ∗ I x ϵ R1 + ∑ 
x ϵ R2y2 ∗ I x ϵ R2 f x 
= ∑ x ϵ R1y1 ∗ I x ϵ R1 
+ ∑ x ϵ R2y2 ∗ I x ϵ R2 
此处 R1 = { 2 } R2 = { 1 
3 4 5 6 } y1 = 1 y2 = 
0.2 训练 完 以后 的 最终 式子 为 f1 x 
= ∑ x ϵ R11 ∗ I x ϵ R1 
+ ∑ x ϵ R 20.2 ∗ I x ϵ 
R2 f1 x = ∑ x ϵ R11 ∗ I 
x ϵ R1 + ∑ x ϵ R 20.2 ∗ 
I x ϵ R2 借 由 这个 式子 我们 得到 
对 样本 属于 类别 1 的 预测 值   f1 
x = 1 + 0.2 ∗ 5 = 2f1 x 
= 1 + 0.2 ∗ 5 = 2 同理 我们 
可以 得到 对 样本 属于 类别 2 3 的 预测 
值 f2 x f2 x f3 x f3 x . 
样 本属于 类别 1 的 概率 即为 p1 = exp 
f1 x / ∑ k = 13exp fk x p1 
= exp f1 x / ∑ k = 13exp fk 
x 下面 我们 用 代码 来 实现 整个 找 特征 
的 过程 大家 可以 自己 再 对照 代码 看看 1 
# 定义 训练 数据 2 train _ data = 5.1 
3.5 1.4 0.2 4.9 3.0 1.4 0.2 7.0 3.2 4.7 
1.4 6.4 3.2 4.5 1.5 6.3 3.3 6.0 2.5 5.8 
2.7 5.1 1.9 3 4 # 定义 label 5 label 
_ data = 1 0 0 1 0 0 0 
1 0 0 1 0 0 0 1 0 0 
1 6 # index 表示 的 第 几类 7 def 
f i n d B e s t L o 
s s A n d p l i t train 
_ data label _ data index 8 sample _ numbers 
= len label _ data 9 feature _ numbers = 
len train _ data 0 10 current _ label = 
11 12 # define the minLoss 13 minLoss = 10000000 
14 15 # feature represents the dimensions of the feature 
16 feature = 0 17 18 # split represents the 
detail split value 19 split = 0 20 21 # 
get current label 22 for label _ index in range 
0 len label _ data 23 current _ label . 
append label _ data label _ index index 24 25 
# trans all features 26 for feature _ index in 
range 0 feature _ numbers 27 # # current feature 
value 28 current _ value = 29 30 for sample 
_ index in range 0 sample _ numbers 31 current 
_ value . append train _ data sample _ index 
feature _ index 32 L = 0 33 # # 
different split value 34 print current _ value 35 for 
index in range 0 len current _ value 36 R1 
= 37 R2 = 38 y1 = 0 39 y2 
= 0 40 41 for index _ 1 in range 
0 len current _ value 42 if current _ value 
index _ 1 current _ value index 43 R1 . 
append index _ 1 44 else 45 R2 . append 
index _ 1 46 47 # # calculate the samples 
for first class 48 sum _ y = 0 49 
for index _ R1 in R1 50 sum _ y 
+ = current _ label index _ R1 51 if 
len R1 = 0 52 y1 = float sum _ 
y / float len R1 53 else 54 y1 = 
0 55 56 # # calculate the samples for second 
class 57 sum _ y = 0 58 for index 
_ R2 in R2 59 sum _ y + = 
current _ label index _ R2 60 if len R2 
= 0 61 y2 = float sum _ y / 
float len R2 62 else 63 y2 = 0 64 
65 # # trans all samples to find minium loss 
and best split 66 for index _ 2 in range 
0 len current _ value 67 if index _ 2 
in R1 68 L + = float current _ label 
index _ 2 y1 * float current _ label index 
_ 2 y1 69 else 70 L + = float 
current _ label index _ 2 y2 * float current 
_ label index _ 2 y2 71 72 if L 
minLoss 73 feature = feature _ index 74 split = 
current _ value index 75 minLoss = L 76 print 
minLoss 77 print minLoss 78 print split 79 print split 
80 print feature 81 print feature 82 return minLoss split 
feature 83 84 f i n d B e s 
t L o s s A n d p l 
i t train _ data label _ data 0 3 
总结 目前 我们 总结 了 gbdt 的 算法 的 流程 
gbdt 如何 选择 特征 如何 产生 特征 的 组合 以及 
gbdt 如何 用于 分类 这个 目前 可以 认为 是 gbdt 
最 经常 问到 的 四个 部分 至于 剩余 的 问题 
因为 篇幅 的 问题 我们 准备 再 开 一个 篇幅 
来 进行 总结 . Practical Lessons from Predicting Clicks on 
Ads at FacebookABSTRACT 这篇 paper 中 作者 结合 GBDT 和 
LR 取得 了 很好 的 效果 比 单个 模型 的 
效果 高出 3% 随后 作者 研究 了 对 整体 预测 
系统 产生 影响 的 几个 因素 发现 Feature 能/v 挖掘/v 
出/v 用户/n 和/c 广告/n 的/uj 历史/n 信息/n + Model GBDT 
+ LR 的 贡献 程度 最大 而 其他 因素 数据 
实时性 模型 学习 速率 数据 采样 的 影响 则 较小 
1 . INTRODUCTION 介绍 了 先前 的 一些 相关 paper 
包括 Google Yahoo MS 的 关于 CTR Model 方面 的 
paper 而在 Facebook 广告 系统 是由 级联 型 的 分类器 
a cascade of classifiers 组成 而 本篇 paper 讨论 的 
CTR Model 则是 这个 cascade classifiers 的 最后 一 环节 
2 . EXPERIMENTAL SETUP 作者 介绍 了 如何 构建 training 
data 和 testing data 以及 Evaluation Metrics 包括 Normalized Entropy 
和 Calibration Normalized Entropy 的 定义 为 每次 展 现时 
预测 得到 的 log loss 的 平均值 除以 对 整个 
数据集 的 平均 log loss 值 之所以 需要 除以 整个 
数据集 的 平均 log loss 值 是 因为 backgroud CTR/w 
越/d 接近/v 于0或/nr 1/m 则 越 容易 预测 取得 较好 
的 log loss 值 而 做了 normalization 后 NE 便 
会对 backgroud CTR 不 敏感 了 这个 Normalized Entropy 值 
越低 则 说明 预测 的 效果 越好 下面 列出 表达式 
Calibration 的 定义 为 预估 CTR 除以 真实 CTR 即 
预测 的 点击数 除以 真实 观察到 的 点击数 这个 值 
越 接近 1 则 表明 预测 效果 越好 3 . 
PREDICTION MODEL STRUCTURE 作者 介绍 了 两种 Online Learning 的 
方法 包括 Stochastic Gradient Descent SGD based LR 和 Bayesian 
online learning scheme for probit regression BOPR BOPR 每 轮 
迭 代时 的 更新 公式 为 3.1 Decision tree feature 
transformsLinear Model 的 表达 能力 不够 需要 feature transformation 第 
一种 方法 是 对 连续 feature 进行 分段 处理 怎样 
分段 以及 分段 的 分界点 是 很 重要 的 第二 
种 方法 是 进行 特征 组合 包括 对 离散 feature 
做 笛卡尔积 或者 对 连续 feature 使用 联合 分段 joint 
binning 比如 使用 k d tree 而 使用 GBDT 能 
作为 一种 很好 的 feature transformation 的 工具 我们 可以 
把 GBDT 中的 每 棵树 作为 一种 类 别的 feature 
把 一个 instance 经过 GBDT 的 流程 即从 根 节点 
一直 往下 分叉 到 一个 特定 的 叶子 节点 作为 
一个 instance 的 特征 组合 的 过程 这里 GBDT 采用 
的 是 Gradient Boosting Machine + L2 TreeBoost 算法 这里 
是 本篇 paper 的 重点 部分 放 一张 经典 的 
原图 3.2 Data freshnessCTR 预估 系统 是 在 一个 动态 
的 环境 中 数据 的 分布 随时 在 变化 所以 
本文 探讨 了 data freshness 对 预测 效果 的 影响 
表明 training data 的 日期 越 靠近 效果 越好 3.3 
Online linear classifier 探讨 了 对 SGD based LR 中 
learning rate 的 选择 最好 的 选择 为 1 global 
效果 差 的 原因 每个 维度 上 训练 样本 的 
不平衡 每个 训练样本 拥 有 不同 的 feature 那些 拥有 
样本 数 较少 的 维度 的 learning rate 下降 过快 
导致 无法 收敛 到 最优 值 2 per weight 差 
的 原因 虽然 对于 各 个 维度 有所 区分 但是 
其 对于 各 个 维度 的 learning rate 下降 速度 
都 太快 了 训练过 早 结束 无法 收敛 到 最优 
值 SGD based LR vs BOPR1 SGD based LR 对比 
BOPR 的 优势 1 1 模型 参数 少 内存 占用 
少 SGD based LR 每个 维度 只有 一个 weight 值 
而 BOPR 每个 维度 有 1个 均值 + 1个 方 
差值 1 2 计算 速度快 SGD LR 只需 1次 内积 
计算 BOPR 需要 2次 内积 计算 2 BOPR 对比 SGD 
based LR 的 优势 2 1 BOPR 可以 得到 完整 
的 预测 点击 概率分布 4 ONLINE DATA JOINEROnline Data Joiner 
主要 是 用于 在线 的 将 label 与 相应 的 
features 进行 join 同时 作者 也 介绍 了 正负 样本 
的 选取 方式 以及 选取 负 样本 时候 的 waiting 
time window 的 选择 5 CONTAINING MEMORY AND LATENCY 作者 
探讨 了 GBDT 中 tree 的 个数 各种 类型 的 
features 包括 contextual features 和 historical features 对 预测 效果 
的 影响 结论 如下 1 NE 的 下降 基本 来自 
于前/nr 500 棵树 2 最后 1000 棵树 对 NE 的 
降低 贡献 低于 0.1% 3 Submodel 2 过拟合 数据 量 
较少 只有 其余 2个 模型 的 约 四分之一 4 Importance 
为 feature 带来 的 累积 信息 增益 / 平方差 的 
减少 5 Top 10 features 贡献 了 将近 一半 的 
importance6 最后 的 300个 features 的 贡献 不足 1% 6 
COPYING WITH MASSIVE TRANING DATA 作者 探讨 了 如何 进行 
样本 采样 的 过程 包括了 均匀 采样 Uniform subsampling 和负/nr 
样本 降 采样 Negative down sampling 以及对 预测 效果 的 
影响 一 . GBDT 的 经典 paper Greedy Function Approximation 
A Gradient Boosting Machine AbstractFunction approximation 是从 function space 方面 
进行 numerical optimization 其 将 stagewise additive expansions 和 steepest 
descent minimization 结合 起来 而 由此而来 的 Gradient Boosting Decision 
Tree GBDT 可以/c 适用/v 于/p regression/w 和/c classification/w 都 具有 
完整 的 鲁棒性 高 解释 性好 的 优点 1 . 
Function estimation 在 机器 学习 的 任务 中 我们 一般 
面对 的 问题 是 构造 loss function 并 求解 其 
最小值 可以 写成 如下 形式 通常 的 loss function 有 
1 . regression 均方 误差 y F 2 绝对误差 | 
y F | 2 . classification negative binomial log likelihood 
log 1 + e 2yF 一般 情况 下 我们 会 
把 F x 看做 是 一系列 带 参数 的 函数 
集合 F x P 于是 进一步 将 其 表示 为 
additive 的 形式 1.1 Numerical optimizatin 我们 可以 通过 选取 
一个 参数 模型 F x P 来 将 function optimization 
问题 转化 为 一个 parameter optimization 问题 进一步 我们 可以 
把 要 优化 的 参数 也 表示 为 additive 的 
形式 1.2 Steepest descent 梯度 下降 是 最简单 最 常用 
的 numerical optimization method 之一 首先 计算 出 当前 的 
梯度 where 而 梯度 下降 的 步 长为 where   
称为 line search 2 . Numerical optimization in function space 
现在 我们 考虑 无 参数 模型 转而 考虑 直接 在 
function space 进行 numerical optimization 这时候 我们 将 在 每个 
数 据点 x 处 的 函数值 F x 看做 是 
一个 参数 仍然 是 来 对 loss funtion 求解 最小值 
在 function space 为了 表示 一个 函数 F x 理想 
状况 下 有 无数 个 点 但在 现实 中 我们 
用 有限 个 N 个 离散 点来 表示 它 按照 
之前 的 numerical optimization 的 方式 我们 需要 求解 使用 
steepest descent 有 where   and3 . Finite data 当 
我们 面对 的 情况 为 用 有限 的 数据 集 
表示 x y 的 联合 分布 的 时候 上述 的 
方法 就 有点 行不通 了 我们 可以 试试 greedy stagewise 
的 方法 但是 对于 一般 的 loss function 和 base 
learner 来说 9 式 是 很难 求解 的 给 定了 
m 次 迭代 后的/nr 当前 近似 函数 Fm 1 x 
当 步长 的 direction 是 指数函数 集合 当中 的 一员 
时 可以 看做 是 在 求解 最优 值 上 的 
greedy step 同样 它 也 可以 被 看做 是 相同 
限制 下 的 steepest descent step 作为 比较 给 出了 
在 无 限制 条件 下 在 Fm 1 x 处 
的 steepest descent step direction 一种 行之有效 的 方法 就是 
在 求解 的 时候 把 它 取 为 无 限制 
条件 下 的 负 梯度方向 where 这 就把 9 式 
中 较难 求解 的 优化 问题 转化 为 了 一个 
基于 均方 误差 的 拟合 问题 Gradient Boosting 的 通用 
解法 如下 二 . 对于 GBDT 的 一些 理解 1 
. BoostingGBDT 的 全称 是 Gradient Boosting Decision Tree Gradient 
Boosting 和 Decision Tree 是 两个 独立 的 概念 因此 
我们 先 说说 Boosting Boosting 的 概念 很好 理解 意思 
是 用 一些 弱 分类器 的 组合 来 构造 一个 
强 分类器 因此 它 不是 某个 具体 的 算法 它 
说 的 是 一种 理念 和/c 这个/r 理念/n 相/v 对应/vn 
的/uj 是/v 一次性/d 构造/v 一个/m 强/a 分类器/n 像 支持 向量 
机 逻辑 回归 等 都 属于 后者 通常 我们 通过 
相加 来 组合 分类器 形式 如下 2 . Gradient Boosting 
Modeling GBM 给定 一个 问题 我们 如何 构造 这些 弱 
分类器 呢 Gradient Boosting Modeling   GBM 就是 构造   
这些 弱 分类 的 一种 方法 同样 它 指 的 
不是 某个 具体 的 算法 仍然 只是 一个 理念 在 
理解   Gradient Boosting   Modeling 之前 我们 先 看看 
一个 典型 的 优化 问题 针对 这种 优化 问题 有 
一个 经典 的 算法 叫 Steepest Gradient Descent 也 就是 
最深 梯度 下 降法 这个 算法 的 过程 大致 如下 
以上 迭代 过程 可以 这么 理解 整个 寻优 的 过程 
就是 个 小步 快跑 的 过程 每 跑 一 小步 
都往/nr 函数 当前 下降 最快 的 那个 方向 走 一点 
这样/r 寻优/n 得到/v 的/uj 结果/n 可以/c 表示/v 成加和/nr 形式/n 即 
这个 形式 和 以上 Fm x 是不是 非常 相似 Gradient 
Boosting 正是 由 此 启发 而来 构造 Fm x 本身 
也 是 一个 寻优 的 过程 只不过 我们 寻找 的 
不是 一个 最 优点 而是 一个 最优 的 函数 优化 
的 目标 通常 都是/nr 通过 一个 损失 函 数来 定义 
即 其中 Loss F xi yi 表示 损失 函数 Loss 
在 第 i 个样 本上 的 损失 值 xi 和 
yi 分别 表示 第 i 个 样本 的 特征 和 
目标 值 常见 的 损失 函数 如 平方差 函数 类似 
最深 梯度 下 降法 我们 可以 通过 梯度 下 降法 
来 构造 弱 分类器 f1 f2 . . . fm 
只不过 每次 迭 代时 令 即对 损失 函数 L 以 
F 为 参考 求取 梯度 这里 有个 小问题 一个 函数 
对 函数 的 求导 不好 理解 而且 通常 都 无法 
通过 上述 公式 直接 求解 到 梯度 函数 gi 为此 
采取 一个 近似 的 方法 把 函数 Fi − 1 
理解 成在/nr 所有 样本 上 的 离散 的 函数值 即 
不难理解 这 是 一个 N 维 向量 然后 计算 这 
是 一个 函数 对 向量 的 求导 得到 的 也 
是 一个 梯度 向量 注意 这里 求导 时的/nr 变量 还是 
函数 F 不是 样本 xk 严格来说 g ̂ i xk 
for k = 1 2 . . . N 只是 
描述 了 gi 在 某些 个别 点上 的 值 并不 
足以 表达 gi 但 我们 可以 通过 函数 拟合 的 
方法 从ĝ/nr i xk for k = 1 2 . 
. . N 构造 gi 这样 我们 就 通过 近似 
的 方法 得到 了 函数 对 函数 的 梯度 求导 
因此 GBM 的 过程 可以 总结 为 如下 常量 函数 
f0 通常 取 样本 目标值 的 均值 即 3 . 
Gradient Boosting Decision Tree 以上 Gradient Boosting Modeling 的 过程 
中 还 没有 说 清楚 如何 通过 离散 值 g 
̂ i − 1 xj for j = 1 2 
3 . . . N 构造 拟合 函数 gi − 
1 函数 拟合 是个 比较 熟知 的 概念 有 很多 
现成 的 方法 不过 有 一种 拟合 方法 广为 应用 
那 就是 决策树 Decision Tree 有关 决策树 的 概念 理解 
GBDT 重点 首先是 Gradient Boosting 其次 才是 Decision Tree GBDT 
是 Gradient Boosting 的 一种 具体 实例 只不过 这里 的 
弱 分类器 是 决策树 如果 你 改用 其他 弱 分类器 
XYZ 你 也 可以 称之为 Gradient Boosting XYZ 只不过 Decision 
Tree 很好用 GBDT 才 如此 引人注目 4 . 损失 函数 
谈到 GBDT 常听到 一种 简单 的 描述 方式 先 构造 
一个 决策 树 然后 不断 在 已有 模型 和 实际 
样本 输出 的 残差 上 再 构造 一颗 树 依次 
迭代 其实 这个 说法 不 全面 它 只是 GBDT 的 
一种 特殊 情况 为了 看清 这个 问题 需要 对 损失 
函数 的 选择 做 一些 解释 从对/nr GBM 的 描述 
里 可以 看到 Gradient Boosting 过程 和 具体 用 什么样 
的 弱 分类器 是 完全 独立 的 可以 任意 组合 
因此 这里 不再 刻意 强调 用 决策树 来 构造 弱 
分类器 转而 我们 来 仔细 看看 弱 分类器 拟合 的 
目标 值 即 梯度 g ̂ i − 1 xj 
之前 我们 已经 提到 过 5 . GBDT 和 AdaBoostBoosting 
是 一类 机器学习 算法 在 这个 家族 中 还有 一种 
非常 著名 的 算法 叫 AdaBoost 是 Adaptive Boosting 的 
简称 AdaBoost 在 人脸 检测 问题 上 尤其 出名 既然 
也是 Boosting 可以 想象 它 的 构造 过程 也 是 
通过 多个 弱 分类器 来 构造 一个 强 分类器 那 
AdaBoost 和 GBDT 有 什么 区别 呢 两者 最大 的 
区别 在于 AdaBoost 不属于 Gradient Boosting 即 它 在 构造 
弱 分类器 时并/nr 没有 利用 到 梯度 下 降法 的 
思想 而是 用 的 Forward Stagewise Additive Modeling FSAM 为了 
理解 FSAM 在 回过头来 看看 之前 的 优化 问题 严格来说 
之前 描述 的 优化 问题 要求 我们 同时 找出 α 
1 α 2 . . . α m 和 f1 
f2 f3 . . . fm 这个 问题 很 难 
为此 我们 把 问题 简化 为 分阶段 优化 每个 阶段 
找出 一个 合适 的 α 和f/nr 假设 我们 已经 得到 
前 m 1 个 弱 分类器 即 Fm − 1 
x 下 一步 在 保证 Fm − 1 x 不变 
的 前提 下 寻找 合适 的 α mfm x 按照 
损失 函数 的 定义 我们 可以 得到 如果 Loss 是 
平方差 函数 则 我们 有 这里 yi − Fm − 
1 xi 就是 当前 模型 在 数据 上 的 残差 
可以 看出 求解 合适 的 α mfm x 就是 在 
这 当前 的 残差 上 拟合 一个 弱 分类器 且 
损失 函数 还是 平方差 函数 这 和 GBDT 选择 平方差 
损失 函数 时 构造 弱 分类器 的 方法 恰好 一致 
1 拟合 的 是 残差 对应 于 GBDT 中的 梯度方向 
2 损失 函数 是 平方差 函数 对应 于 GBDT 中用 
Decision Tree 来 拟合 残差 其中 wim − 1 = 
exp − yi Fm − 1 xi 和要/nr 求解 的 
α mfm x 无关 可以 当成 样本 的 权重 因此 
在 这种 情况 下 构造 弱 分类器 就是 在 对 
样本 设置 权重 后的/nr 数据 上 拟合 且 损失 函数 
还是 指数 形式 这个 就是 AdaBoost 不过 AdaBoost 最早 并 
不是 按 这个 思路 推 出来 的 相反 是 在 
AdaBoost 提出 5 年后 人们 才 开始 用 Forward Stagewise 
Additive Modeling 来 解释 AdaBoost 背后 的 原理 为什么 要 
把 平方差 和 指数 形式 Loss 函数 单独 拿出 来说 
呢 这 是因为 对 这 两个 损失 函数 来说 按照 
Forward Stagewise Additive Modeling 的 思路 构造 弱 分类器 时 
比较 方便 如果 是 平方差 损 失 函数 就在 残差 
上 做 平方差 拟合 构造 弱 分类器 如果 是 指数 
形式 的 损失 函数 就在 带 权 重 的 样本 
上 构造 弱 分类器 但 损失 函数 如果 不是 这 
两种 问题 就 没 那么 简单 比如 绝对 差值 函数 
虽然 构造 弱 分类器 也 可以 表示 成在/nr 残差 上 
做 绝对 差值 拟合 但这 个子 问题 本身 也 不容 
易解 因为 我们 是 要 构造 多个 弱 分类器 的 
所以 我们 当然 希望 构造 弱 分类器 这 个子 问题 
比较 好 解 因此 FSAM 思路 无法 推广 到 其他 
一些 实用 的 损失 函 数上 相比而言 Gradient Boosting Modeling 
GBM 有 什么 优势 呢 GBM 每次 迭 代时 只需要 
计算 当前 的 梯度 并在 平方差 损 失 函数 的 
基础 上 拟合 梯度 虽然 梯度 的 计算 依赖 原始 
问题 的 损失 函数 形式 但 这 不是 问题 只要 
损失 函数 是 连续 可微 的 梯度 就 可以 计算 
至于 拟合 梯度 这 个子 问题 我们 总是 可以 选 
择 平方差 函数 作为 这 个子 问题 的 损失 函数 
因为 这 个子 问题 是 一个 独立 的 回归 问题 
因此 FSAM 和 GBM 得到 的 模型 虽然 从 形式 
上 是 一样 的 都是 若干 弱 模型 相加 但是 
他们 求 解弱/nr 分类器/n 的/uj 思路/n 和/c 方法/n 有/v 很大/a 
的/uj 差别/d 只有 当 选择 平方差 函数 为 损失 函数 
时 这 两种 方法 等同 6 . 为何 GBDT 受人 
青睐 以 上 比较 了 GBM 和 FSAM 可以 看到 
GBM 在 损失 函数 的 选择 上 有 更大 的 
灵活性 但这 不足以 解释 GBDT 的 全部 优势 GBDT 是 
拿 Decision Tree 作为 GBM 里 的 弱 分类器 GBDT 
的 优势 首先 得益于 Decision Tree 本身 的 一些 良好 
特性 具体 可以 列举如下 Decision Tree 可以 很好 的 处理 
missing feature 这是 他 的 天然 特性 因为 决策树 的 
每个 节点 只 依赖 一个 feature 如果 某个 feature 不 
存在 这颗 树 依然 可以 拿来 做 决策 只是 少 
一些 路径 像 逻辑 回归 SVM 就 没 这个 好处 
Decision Tree 可以 很好 的 处理 各种 类型 的 feature 
也是 天然 特性 很好 理解 同样 逻辑 回归 和 SVM 
没 这样 的 天然 特性 对 特征 空间 的 outlier 
有 鲁棒性 因为 每个 节点 都是 x ? ? 的 
形式 至于 大 多少 小 多少 没有 区别 outlier 不会 
有 什么 大 的 影响 同样 逻辑 回归 和 SVM 
没有 这样 的 天然 特性 如果 有不/nr 相关 的 feature 
没什么 干扰 如果 数据 中有 不相关的 feature 顶多 这个 feature 
不 出现 在 树 的 节点 里 逻辑 回归 和 
SVM 没有 这样 的 天然 特性 但是 有 相应 的 
补救 措施 比如 逻辑 回归 里 的 L1 正则化 数据 
规模 影响 不大 因为 我们 对 弱 分类器 的 要求 
不高 作为 弱 分类器 的 决策树 的 深 度 一般 
设 的 比较 小 即使 是 大 数据 量 也 
可以 方便 处理 像 SVM 这种 数据 规模 大 的 
时候 训练 会 比较 麻烦 当然 Decision Tree 也 不是 
毫无 缺陷 通常在 给定 的 不带 噪音 的 问题 上 
他 能 达到 的 最佳 分类 效果 还是 不如 SVM 
逻辑 回归 之类 的 但是 我们 实际 面对 的 问题 
中 往往 有 很大 的 噪音 使得 Decision Tree 这个 
弱势 就不 那么 明显 了 而且 GBDT 通过 不断 的 
叠加 组合 多个 小 的 Decision Tree 他 在 不带 
噪音 的 问题 上 也 能达到 很好 的 分类 效果 
换句话说 通过 GBDT 训练 组合 多个 小 的 Decision Tree 
往往 要比 一次性 训练 一个 很大 的 Decision Tree 的 
效果 好 很多 因此 不能 把 GBDT 理解为 一颗 大 
的 决策树 几颗 小树 经过 叠加 后就/nr 不再 是 颗 
大树 了 它 比 一颗 大树 更强 