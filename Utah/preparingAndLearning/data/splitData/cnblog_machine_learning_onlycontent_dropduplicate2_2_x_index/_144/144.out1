python 机器学习 实战 三 版权 声明 本文 为 博主 原创 
文章 转载 请 指明 转载 地址 www . cnblogs . 
com / fydeblog / p / 7277205 . html 前言 
这篇 博客 是 关于 机器学习 中 基于 概率论 的 分类 
方法 朴素 贝叶斯 内容 包括 朴素 贝叶斯 分类器 垃圾 邮件 
的 分类 解析 RSS 源 数据 以及 用 朴素 贝叶斯 
来 分析 不同 地区 的 态度 . 操作系统 ubuntu14 . 
04 运行 环境 anaconda python2 . 7 jupyter notebook 参考 
书籍 机器学习 实 战和 源码 机器学习 周志华 notebook writer 方阳/nr 
注意事项/n 在 这里 说一句 默认 环境 python2 . 7 的 
notebook 用 python3 . 6 的 会 出问题 还有 我 
的 目录 可能 跟 你们 的 不一样 你们 自己 跑 
的 时候 记得 改 目录 我会 把 notebook 和 代码 
以及 数据集 放到 结尾 的 百度 云盘 方便 你们 下载 
1 . 基于 贝叶斯 决策 理论 的 分类 方法 朴素 
贝叶斯 的 特点 优 点 在 数据 较少 的 情况 
下 仍然 有效 可以 处理 多 类别 问题 缺 点 
对于 输入 数据 的 准备 方式 较为 敏感 适用 数据类型 
标称 型 数据 贝叶斯 决策 理论 的 核心 思想 选择 
具有 最高 概率 的 决策 最小化 每个 样本 的 条件 
风险 则 总体 风险 也就 最小 就是 选择 最高 概率 
减小 风险 2 . 条件概率 2.1 简单 回顾 条件概率 在 
朴素 贝叶斯 里面 是 必不可少 的 一环 下面 来 简单 
介绍 介绍 假设 现在 有一个 装了 7块 石头 的 罐子 
其中 3块 是 灰色 的 4块 是 黑色 的 如果 
从 罐子 中 随机 取出 一块 石头 那么 是 灰色 
石头 的 可能性 是 多少 由于 取 石头 有 7 
种 可能 其中 3种 为 灰色 所以 取出 灰色 石头 
的 概率 为 3/7 那么 取到 黑色 石头 的 概率 
又是 多少 呢 很显然 是 4/7 如果 这 7块 石头 
放在 两个 桶 中 那么 上述 概率 应该 如何 计算 
设 两个 桶 分为 A B A 桶 装了 2个 
灰色 和 2个 黑色 的 石头 B 桶 装了 1个 
灰色 和 2个 黑色 的 石头 要 计算 P gray 
或者 P black 事 先得 知道 石头 所在 桶 的 
信息 会 不会 改变 结果 你/r 有/v 可能/v 巳/mg 经/n 
想到/v 计算/v 从B桶/nr 中/f 取到/v 灰色/n 石头/n 的/uj 概率/n 的/uj 
办法/n 这 就是 所谓 的 条件 概率 . 来 计算 
P gray | bucketB 这个 是 条件概率 在 已知 是从 
B 桶 拿出 石头 的 条件 下 拿到 灰色 石头 
的 概率 计算公式 P gray | bucketB = P gray 
and bucketB / P bucketB 将 两者 同时 发生 的 
概率 除以 前提 条件 发生 的 概率 我们 知道 P 
bucketB 就是 3/7 B 桶 的 石头 数 / 总 
石 头数 P gray and bucketB 是 1/7 B/w 桶/zg 
中的/i 灰色/n 石/n 头数/n //i 总/b 石/n 头数/n 所以 P 
gray | bucketB = 1/3 这里 说 一下 P gray 
and bucketB 它 等于 P bucketB | gray 乘以 P 
gray 的 先 发生 gray 然后 在 gray 的 基础 
上 发生 bucketB 就是 gray and bucketB 所以 这里 的 
公式 还 可以 变 一下 P gray | bucketB = 
P gray and bucketB / P bucketB = P bucketB 
| gray * P gray / P bucketB 一般 情况 
下 写成 p c | x = p x | 
c * p c / p x 这 就是 贝叶斯 
准则 2.2 使用 条件概率 进行 分类 贝叶斯 决策论 中 真正 
比较 的 是 条件概率 p c1 | x y 和p/nr 
c2 | x y 这些 符号 所 代表 的 具体 
意义 是 给定 某个 由 x y 表示 的 数据 
点 想 知道 该 数据 点 来自 类别 c1 的 
概率 是 多少 数 据点 来自 类别 c2 的 概率 
又是 多少 如果 p c1 | x y p c2 
| x y 属于 类别 c1 如果 p c2 | 
x y p c1 | x y 属于 类别 c2 
这些 概率 可以 有 2.1 的 贝叶斯 准则 计算 3 
. 使用 朴素 贝叶斯 进行 留言 分类 朴素 贝叶斯 的 
一般 过程 1 收集 数据 可以 使用 任何 方法 本章 
使用 RSS 源 2 准备 数据 需要 数值 型 或者 
布尔 型 数据 3 分析 数据 有 大量 特征 时 
绘制 特征 作用 不大 此时 使用 直方图 效果 更好 4 
训练 算法 计算 不同 的 独立 特征 的 条件 概率 
5 测试 算法 计算 错误率 6 使用 算法 一个 常见 
的 朴素 贝叶斯 应用 是 文档 分类 可以 在 任意 
的 分类 场景 中 使用 朴素 贝叶斯 命 类 器 
不一定 非 要是 文本 朴素 贝叶斯 的 两个 假设 1 
特征 之间 是 统计 独立 的 即/v 一个/m 特征/n 或者/c 
单词/nr 出现/v 的/uj 可能性/n 与/p 它/r 和/c 其他/r 单词/n 相邻/v 
没有/v 关系/n 2 每个 特征 同等 重要 以上 两个 假设 
是 有 问题 的 不够 严谨 但 处理 方便 实际 
效果 却 很好 3.1 准备 数据 从 文本 中 构建 
词 向量 词表 到 向量 的 转换 函数 如下 1 
def loadDataSet 2 postingList = my dog has flea problems 
help please 3 maybe not take him to dog park 
stupid 4 my dalmation is so cute I love him 
5 stop posting stupid worthless garbage 6 mr licks ate 
my steak how to stop him 7 quit buying worthless 
dog food stupid 8 classVec = 0 1 0 1 
0 1 # 1 is abusive 0 not 9 return 
postingList classVec 10 11 def createVocabList dataSet 12 vocabSet = 
set # create empty set 13 for document in dataSet 
14 vocabSet = vocabSet | set document # union of 
the two sets 15 return list vocabSet 16 17 def 
setOfWords2Vec vocabList inputSet 18 returnVec = 0 * len vocabList 
19 for word in inputSet 20 if word in vocabList 
21 returnVec vocabList . index word = 1 22 else 
print the word % s is not in my Vocabulary 
% word 23 return returnVec 第一 个 loadDataSet 函数 是 
返回 词条 切分 后的/nr 文档 集合 postlist 选自 斑点 犬 
爱好者 留言板 和 类别 标签 集合 classvec 1 代表 侮辱 
0 则是 正常 言论 第二个 createVocabList 函数 会 返回 输入 
数据集 所有 不 重复 词汇 的 列表 第三个 setOfWords2Vec 函数 
的 功能 是 遍历 输入 vocablist 的 所有 单词 如果 
当初 出现 了 InputSet 中的 单词 returnVec 对应 位数 的 
值 返回 1 无 则 返回 0 简单 来讲 第一 
个 函数 的 作用 是 界定 训练 类别 看 之后 
的 文档 是否 含有 类别 中 的 词汇 第二个 函数 
的 作用 是 将 一篇 文档 做成 列表 方便 后面 
进行 标记 第三个 函数 则是 将 第二 个 函数 生成 
的 列表 根据 第 一个 类别 词汇 进行 标记 将 
单词 转化成 数字 方便 后面 计算 条件概率 测试 一下 吧 
所有 函数 都 放在 bayes 中 cd 桌面 / m 
a c h i n e l e a r 
n i n g i n a c t i 
o n / Ch04 / home / fangyang / 桌面 
/ m a c h i n e l e 
a r n i n g i n a c 
t i o n / Ch04import bayeslistOPosts listClasses = bayes 
. loadDataSet myVocabList = bayes . createVocabList listOPosts myVocabListbayes . 
setOfWords2Vec myVocabList listOPosts 0 bayes . setOfWords2Vec myVocabList listOPosts 3 
3.2 训练 算法 从词向/nr 量计算/l 概率/n 根据/p 上面/f 介绍/v 的/uj 
三个/m 函数/n 我们 知道 如何 将 一组 单词 转换 为 
一组 数字 也 知道 一个 词 是否 出现 在 一篇 
文档 中 现在 已知 文档 的 类别 让 我们 使用 
转换 得到 的 数字 来 计算 条件概率 吧 还是 根据 
上面 的 贝叶斯 准则 来 计算 条件概率 不过 公式 会 
有一点 不 一样 p ci | w = p w 
| ci * p ci / p w 这里 的 
ci 表示 所属 类别 这里/r 有/v 两种/m 可能性/n 1/m 和0/nr 
w 为 向量 由 多个 数值 组成 我们 根据 上面 
的 公式 对 每个 类 进行 计算 然后 比较 这 
两个 概率值 的 大小 计算 过程 如下 首先 可以 通过 
类别 i 侮辱性 留言 或非 侮辱性 留言 中 文档 数 
除以 总 的 文档 数来 计算 概率 p ci 接下来 
计算 p w | ci 由于 p w | ci 
= p w0 w1 w2 . . wn | ci 
又 因为 所有 词 都 相互 独立 所以 p w 
| ci = p w0 | ci p w1 | 
ci p w2 | ci . . . p wn 
| ci 于是 函数 的 伪代码 相应 如下 计算 每个 
类别 中 的 文档 数目 对 每篇 训练 文档 对 
每个 类别 如果 词条 出现 文档 中 ― 增加 该 
词条 的 计 数值 增加 所有 词条 的 计 数值 
对 每个 类别 对 每个 词条 将该 词条 的 数目 
除以 总 词条 数目 得到 条件概率 返回 每个 类别 的 
条件概率 参考 代码 如下 1 def trainNB0 trainMatrix trainCategory 2 
numTrainDocs = len trainMatrix 3 numWords = len trainMatrix 0 
4 pAbusive = sum trainCategory / float numTrainDocs 5 p0Num 
= zeros numWords p1Num = zeros numWords 6 p0Denom = 
0.0 p1Denom = 0.0 7 for i in range numTrainDocs 
8 if trainCategory i = = 1 9 p1Num + 
= trainMatrix i 10 p1Denom + = sum trainMatrix i 
11 else 12 p0Num + = trainMatrix i 13 p0Denom 
+ = sum trainMatrix i 14 p1Vect = p1Num / 
p1Denom 15 p0Vect = p0Num / p0Denom 16 return p0Vect 
p1Vect pAbusive 输入 的 trainMatrix 是 文档 经过 setOfWords2Vec 函数 
转换 后的/nr 列表 trainCategory 是 每篇 文档 构成 类别 标签 
向量 输出 是 返回 每个 类别 的 概率 pAbusive 等于 
类别 和 除以 训练 的 样本 数 这个 就是 说明 
一下 文档 类别 的 概率分布 没有 什么 其他 意思 由于 
要 算 每 一个 词语 的 概率 这里 用 到里 
numpy 的 array 数组 可以 很 方便 的 计算 每个 
词语 的 概率 即是 用 p0Num 和 p1Num 来 统计 
不同 类别 样本 的 词语 所 出现 的 次数 最后 
对 每个 元素 除 以该 类别 中的 总 词数 来 
测试 一下 吧 from numpy import * reload bayes module 
bayes from bayes . py listOPosts listClasses = bayes . 
loadDataSet myVocabList = bayes . createVocabList listOPosts trainMat = for 
postinDoc in listOPosts trainMat . append bayes . setOfWords2Vec myVocabList 
postinDoc p0V p1V pAb = bayes . trainNB0 trainMat listClasses 
p0Vp1V 看一看 在 给定 文档 类别 条件 下 词汇 表中 
单词 的 出现 概率 看看 是否 正确 . 词汇 表中 
的 第一 个 词 是 cute 其 在 类别 0 
中 出现 1次 而在 类别 1中 从未 出现 对应 的 
条件 概率 分别为 0.04166667 与 0.0 该 计算 是 正确 
的 我们 找找 所有 概率 中的 最大值 该 值 出现 
在 p 1 数组 第 21个 下标 位置 大小 为 
0.15789474 . 可以 查到 该 单词 是 stupid 这 意味着 
它 最能 表征 类别 1 的 单词 3.3 测试 算法 
根据 现实 情况 修改 分类器 利用 贝叶斯 分类器 进行 文档 
文类 时 要 计算 每个 元素 的 条件概率 并 相乘 
若 其中 有 一个 概率值 等于 0 那么 最后 的 
乘积 也为 0 为 降低 这种 影响 可以 将 所有 
词 的 出现 数 初始 化为 1 并将 分母 初始 
化为 2 相应 的 trainNB0 的 第 4行 和第/nr 5行 
修改 为 p0Num = ones numWords   p1Num = ones 
numWords           # change to ones 
p0Denom = 2.0 p1Denom = 2.0         
                    
                    
                    
                    
# change to 2.0 另 一个 问题 是 向下 溢出 
乘积 p w0 | ci p w1 | ci p 
w2 | ci . . . p wn | ci 
太小 的 缘故 解决 的 办法 是 对 乘积 取 
对数 相应 的 trainNB0 的 第 13行 和第/nr 14行 修改 
为 p1Vect = log p1Num / p1Denom       
        # change to log p0Vect = 
log p0Num / p0Denom             
  # change to log 将 更 改好 的 函数 
命名为 trainNB0 _ change 现在 已经 准备好 构建 完整 的 
分类器 了 当 使用 numpy 向量 处理 功能 时 这 
一切 变得 十分 简单 . 参考 代码 如下 1 def 
classifyNB vec2Classify p0Vec p1Vec pClass1 2 p1 = sum vec2Classify 
* p1Vec + log pClass1 # element wise mult 3 
p0 = sum vec2Classify * p0Vec + log 1.0 pClass1 
4 if p1 p0 5 return 1 6 else 7 
return 0 8 def testingNB 9 listOPosts listClasses = loadDataSet 
10 myVocabList = createVocabList listOPosts 11 trainMat = 12 for 
postinDoc in listOPosts 13 trainMat . append setOfWords2Vec myVocabList postinDoc 
14 p0V p1V pAb = trainNB0 _ change array trainMat 
array listClasses 15 testEntry = love my dalmation 16 thisDoc 
= array setOfWords2Vec myVocabList testEntry 17 print testEntry classified as 
classifyNB thisDoc p0V p1V pAb 18 testEntry = stupid garbage 
19 thisDoc = array setOfWords2Vec myVocabList testEntry 20 print testEntry 
classified as classifyNB thisDoc p0V p1V pAb 第一 个 函数 
就是 两个 类别 的 条件概率 进行 比较 输出 最终 的 
类别 信息 第二个 函数 就是 一个 测试函数 函数 前面 部分 
跟 上面 一样 后面 引入 两个 测试 样本 进行 分类 
reload bayes module bayes from bayes . pyc bayes . 
testingNB love my dalmation classified as 0 stupid garbage classified 
as 13.4 文档 词 袋 模型 我们 将 每个 词 
的 出现 与否 作为 一个 特征 这 可以 被 描述 
为 词集 模型 上面 就是 词集 模型 如果 一个 词 
在 文档 中 出现 不止 一次 这 可能 意味着 包含 
该词 是否 出现 在 文档 中 所 不能 表达 的 
某种 信息 这种 方法 被 称为 词 袋 模型 词集 
和词袋/nr 的 区别 在 词 袋中 每个 单词 可以 出现 
多次 而在 词 集中 每个 词 只能 出现 一次 为 
适应 词 袋 模型 需要 对 函数 setOfWords2Vec 稍加 修改 
修改后 的 函数 为 bagOfWords2Vec 代码 如下 1 def bagOfWords2VecMN 
vocabList inputSet 2 returnVec = 0 * len vocabList 3 
for word in inputSet 4 if word in vocabList 5 
returnVec vocabList . index word + = 1 6 return 
returnVec 这个 返回 的 列表 表现 的 是 单词 出现 
的 次数 还 不再 是 是否 出现 4 . 使用 
朴素 贝叶斯 过滤 垃圾邮件 4.1 准备 数据 切分 文本 前面 
介绍 的 词 向量 是 直接 给定 的 下面 来 
介绍 如何 从 文本 中 构建 自己 的 词 列表 
先从 一个 文本 字符串 介绍 mySent = This book is 
the best book on python or M . L . 
I have ever laid eyes upon . mySent . split 
可以 看到 切分 的 结果 不错 但是 标点符号 也被 当成 
了 词 的 一部分 . 解决 方法 可以 使用 正 
则 表示 式 来 切分 句子 其中 分隔符 是 除 
单词 数字 外 的 任意 字符串 import reregEx = re 
. compile \ \ W * listOfTokens = regEx . 
split mySent listOfTokens 可以 看到 里面 的 标点 没有 了 
但 剩下 一些 空字符 还要 进行 一步 去掉 这些 空字符 
tok for tok in listOfTokens if len tok 0 空字符 
消 掉了 我们 可以 看到 有的 词 首字母 是 大写 
的 这对 句子 查找 很 有用 但 我们 是 构建 
词 袋 模型 所以 还是 希望 格式 统一 还要 处理 
一下 tok . lower for tok in listOfTokens if len 
tok 0 可以 看到 大写 全部 变成 了 小写 如果 
是 想从 小写 变成 大写 只需 将 tok . lower 
改成 top . upper 即可 我们 构建 一个 testParse 函数 
来 切分 文本 代码 如下 1 def textParse bigString # 
input is big string # output is word list 2 
import re 3 listOfTokens = re . split r \ 
W * bigString 4 return tok . lower for tok 
in listOfTokens if len tok 2 4.2 测试 算法 使用 
朴素 贝叶斯 进行 交叉 验证 参考 代码 如下 1 def 
spamTest 2 docList = classList = fullText = 3 for 
i in range 1 26 4 wordList = textParse open 
email / spam / % d . txt % i 
. read 5 docList . append wordList 6 fullText . 
extend wordList 7 classList . append 1 8 wordList = 
textParse open email / ham / % d . txt 
% i . read 9 docList . append wordList 10 
fullText . extend wordList 11 classList . append 0 12 
vocabList = createVocabList docList # create vocabulary 13 trainingSet = 
range 50 testSet = # create test set 14 for 
i in range 10 15 randIndex = int random . 
uniform 0 len trainingSet 16 testSet . append trainingSet randIndex 
17 del trainingSet randIndex 18 trainMat = trainClasses = 19 
for docIndex in trainingSet # train the classifier get probs 
trainNB0 20 trainMat . append bagOfWords2VecMN vocabList docList docIndex 21 
trainClasses . append classList docIndex 22 p0V p1V pSpam = 
trainNB0 array trainMat array trainClasses 23 errorCount = 0 24 
for docIndex in testSet # classify the remaining items 25 
wordVector = bagOfWords2VecMN vocabList docList docIndex 26 if classifyNB array 
wordVector p0V p1V pSpam = classList docIndex 27 errorCount + 
= 1 28 print classification error docList docIndex 29 print 
the error rate is float errorCount / len testSet 30 
# return vocabList fullText 第一 个 循环 是 对 垃圾 
邮件 和非/nr 垃圾邮件 进行 切分 然后 生成 词 列表 和类/nr 
标签 第二 个 循环 是 0 到 50 个数 中 
随机 生成 10个 序号 第三 个 循环 是 将 第二 
个 循环 得到 的 序号 映 射到 词 列表 得到 
训练 集 和 相应 的 类别 然后 进行 训练 算法 
第四 个 循环 是 进行 错误率 计算 分类 出 的 
类别 与 实际 类别 相比较 累计 错误 的 样本 数 
最后 除以 总数 得到 错误率 bayes . spamTest the error 
rate is 0 . 0bayes . spamTest 每次 运行 得出 
的 结果 可能 不 太 一样 因为 是 随机 选 
的 序号 5 . 使用 朴素 贝叶斯 分类器 从 个人 
广告 中 获取 区域 倾向 在 这个 最后 的 例子 
当中 我们 将 分别 从 美国 的 两个 城市 中 
选取 一些 人 通过 分析 这些 人 发布 的 征婚 
广告 信息 来 比较 这 两个 城市 的 人们 在 
广告 用词 上 是否 不同 如果 结论 确实 是 不同 
那么 他们 各自 常用 的 词 是 哪些 从 人们 
的 用词 当中 我们 能否 对 不同 城市 的 人 
所 关心 的 内容 有所 了解 下面 将 使用 来自 
不同 城市 的 广告 训练 一个 分类器 然后 观察 分类器 
的 效果 我们 的 目的 并 不是 使用 该 分类 
器 进行 分类 而是 通过 观察 单词 和 条件 概率值 
来 发现 与 特定 城市 相关 的 内容 5.1 收集 
数据 导入 RSS 源 接下来 要做 的 第一 件事 是 
使用 python 下载 文本 而 利用 RSS 这 很容易 得到 
而 Universal Feed Parser 是 python 最 常用 的 RSS 
程序库 由于 python 默认 不会 安装 feedparser 所以 需要 自己 
手动 安装 这里 附上 ubuntu 下 的 安装 方法 第一步 
wget http / / pypi . python . org / 
packages / source / f / feedparser / feedparser 5 
. 1.3 . tar . gz # md5 = f 
2 2 5 3 d e 7 8 0 8 
5 a 1 d 5 7 3 8 f 6 
2 6 f c c 1 d 8 f 7 
1 第二步 tar zxf feedparser 5 . 1.3 . tar 
. gz 第三步 cd feedparser 5 . 1.3 第四步 python 
setup . py install 具体 可以 看到 这 个 链接 
blog . csdn . net / tinkle181129 / article / 
details / 45343267 相关 文档 http / / code . 
google . com / p / feedparser / import feedparserny 
= feedparser . parse http / / newyork . craigslist 
. org / stp / index . rss 上面 是 
打开 了 Craigslist 上 的 RSS 源 要 访问 所有 
条目 的 列表 输入 以下 代码 ny entries len ny 
entries Out 25 可以 构建 一个 类似 spamTest 的 函数 
来 对 测试过程 自动化 1 def calcMostFreq vocabList fullText 2 
import operator 3 freqDict = { } 4 for token 
in vocabList 5 freqDict token = fullText . count token 
6 sortedFreq = sorted freqDict . iteritems key = operator 
. itemgetter 1 reverse = True 7 return sortedFreq 30 
8 9 def localWords feed1 feed0 10 import feedparser 11 
docList = classList = fullText = 12 minLen = min 
len feed1 entries len feed0 entries 13 for i in 
range minLen 14 wordList = textParse feed1 entries i summary 
15 docList . append wordList 16 fullText . extend wordList 
17 classList . append 1 # NY is class 1 
18 wordList = textParse feed0 entries i summary 19 docList 
. append wordList 20 fullText . extend wordList 21 classList 
. append 0 22 vocabList = createVocabList docList # create 
vocabulary 23 top30Words = calcMostFreq vocabList fullText # remove top 
30 words 24 for pairW in top30Words 25 if pairW 
0 in vocabList vocabList . remove pairW 0 26 trainingSet 
= range 2 * minLen testSet = # create test 
set 27 for i in range 20 28 randIndex = 
int random . uniform 0 len trainingSet 29 testSet . 
append trainingSet randIndex 30 del trainingSet randIndex 31 trainMat = 
trainClasses = 32 for docIndex in trainingSet # train the 
classifier get probs trainNB0 33 trainMat . append bagOfWords2VecMN vocabList 
docList docIndex 34 trainClasses . append classList docIndex 35 p0V 
p1V pSpam = trainNB0 array trainMat array trainClasses 36 errorCount 
= 0 37 for docIndex in testSet # classify the 
remaining items 38 wordVector = bagOfWords2VecMN vocabList docList docIndex 39 
if classifyNB array wordVector p0V p1V pSpam = classList docIndex 
40 errorCount + = 1 41 print the error rate 
is float errorCount / len testSet 42 return vocabList p0V 
p1VlocalWords 函数 与 之前 介绍 的 spamTest 函数 类似 不同 
的 是 它 是 使用 两个 RSS 作为 参数 上面 
还 新增 了 一个 辅助 函数 calcMostFreq 该 函数 遍历 
词汇表 中的 每个 词 并 统计 它 在 文本 中 
出现 的 次数 然后 根据 出现 次数 从高 到 低 
对 词典 进行 排序 最后 返回 排序 最高 的 30个 
单词 下面 来 测试 一下 cd 桌面 / m a 
c h i n e l e a r n 
i n g i n a c t i o 
n / Ch04 / home / fangyang / 桌面 / 
m a c h i n e l e a 
r n i n g i n a c t 
i o n / Ch04import bayesimport feedparserny = feedparser . 
parse http / / newyork . craigslist . org / 
stp / index . rss sf = feedparser . parse 
http / / sfbay . craigslist . org / stp 
/ index . rss vocabList pSF pNY = bayes . 
localWords ny sf the error rate is 0 . 15vocabList 
pSF pNY = bayes . localWords ny sf the error 
rate is 0.4 我们 会 发现 这里 的 错误率 要 
远高于 垃圾邮件 中的 错误率 这 是因为 这里 关注 的 是 
单词 概率 而 不是 实际 分类 可以 通过 calcMostFreq 函数 
改变 移除 单 词数 降低 错误率 因为 次数 最多 的 
前 30个 单词 涵盖 了 所有 用词 的 30% 产生/n 
这种/r 现象/n 的/uj 原因/n 是/v 语言/n 中/f 大部分/m 都是/nr 冗余/a 
和/c 结构/n 辅助性/n 内容/n 5.2 分析 数据 显示 地域 相关 
的 用词 将 pSF 和 pNY 进行 排序 然后 按照 
顺序 将 词 打印 出来 这里 用 getTopWords 函数 表示 
这个 功能 1 def getTopWords ny sf 2 import operator 
3 vocabList p0V p1V = localWords ny sf 4 topNY 
= topSF = 5 for i in range len p0V 
6 if p0V i 6.0 topSF . append vocabList i 
p0V i 7 if p1V i 6.0 topNY . append 
vocabList i p1V i 8 sortedSF = sorted topSF key 
= lambda pair pair 1 reverse = True 9 print 
SF * * SF * * SF * * SF 
* * SF * * SF * * SF * 
* SF * * SF * * SF * * 
SF * * SF * * SF * * SF 
* * SF * * SF * * 10 for 
item in sortedSF 11 print item 0 12 sortedNY = 
sorted topNY key = lambda pair pair 1 reverse = 
True 13 print NY * * NY * * NY 
* * NY * * NY * * NY * 
* NY * * NY * * NY * * 
NY * * NY * * NY * * NY 
* * NY * * NY * * NY * 
* 14 for item in sortedNY 15 print item 0 
输入 是 两个 RSS 源 然后 训练 并 测试 朴素 
贝叶斯 分类器 返回 使用 的 概率值 然后 创建 两个 列表 
用于 元组 的 存储 与 之前 返回 排名 最高 的 
x 个 单词 不同 这里 可以 返回 大于 某个 阈值 
的 所有 词 这些 元组 会 按照 它们 的 条件概率 
进行 排序 bayes . getTopWords ny sf 值得 注意 的 
现象 是 程序 输出 了 大量 的 停用词 移除 固定 
的 停用词 比如 there 等等 看看 结果 会 如何 变化 
依 本书 作者 的 经验 来看 这样 会使 分类 错误率 
降低 小结 1 对于 分类 而言 使用 概率 有时 要 
比 使用 硬 规则 更为 有效 2 贝叶斯 概率 及 
贝叶斯 准则 提供 了 一种 利用 已知 值 来 估计 
未知 概率 的 有效 方法 3 独立性 假设 是 指 
一个 词 的 出现 概率 并不 依赖于 文档 中 的 
其他 词 这个 假设 过于 简单 这 就是 之所以 称为 
朴素 贝叶斯 的 原因 4 下 溢出 就是 其中 一个 
问题 它 可以 通过 对 概率 取 对数 来 解决 
5 词 袋 模型 在 解决 文档 分类 问题 上 
比 词集 模型 有所提高 6 移除 停用词 可降低 错误率 7 
花/v 大量/n 时间/n 对/p 切分/ad 器/n 进行/v 优化/vn 百度/n 云/ns 
链接/n https / / pan . baidu . com / 
s / 1 L g K U L 7 f 
4 j a 7 m z 0 j s y62qgimport 
feedparser 