在 机器 学习 和 数据 挖掘 中 我们 经常 需要 
知道 个体 间 差异 的 大小 进而 评价 个体 的 
相似性 和 类别 最 常见 的 是 数据 分析 中 
的 相关 分析 数据挖掘 中的 分类 和聚类/nr 算法 如 K 
最 近邻 KNN 和 K 均值 K Means 等等 根据 
数据 特性 的 不同 可以 采用 不同 的 度量 方法 
一般而言 定义 一个 距离 函数 d x y 需要 满足 
下面 几个 准则 1 d x x = 0   
                  / 
/ 到 自己 的 距离 为 02 d x y 
= 0                 
  / / 距离 非 负 3 d x y 
= d y x             
      / / 对称性 如果 A 到 B 
距离 是 a 那么 B 到 A 的 距离 也 
应该 是 a4 d x k + d k y 
= d x y     / / 三角形 法则 
两边 之和 大于 第 三边 这篇 博客 主要 介绍 机器学习 
和 数据 挖掘 中 一些 常见 的 距离 公式 包括 
闵可夫 斯基 距离 欧几里得 距离 曼哈顿 距离 切比雪夫 距离 马氏 
距离 余弦 相似 度 皮尔逊 相关系数 汉明 距离 杰 卡德 
相似系数 编辑 距离 DTW 距离 KL 散度 1 . 闵可夫 
斯基 距离 闵可夫 斯基 距离 Minkowski distance 是 衡量 数值 
点 之间 距离 的 一种 非常 常见 的 方法 假设 
数值 点 P 和 Q 坐标 如下 那么 闵可夫 斯基 
距离 定义 为 该 距离 最 常用 的 p 是 
2 和 1 前者 是 欧几里得 距离 Euclidean distance 后者 
是 曼哈顿 距离 Manhattan distance 假设在 曼哈顿 街区 乘坐 出租车 
从 P 点到 Q 点 白色 表示 高楼大厦 灰色 表示 
街道 绿色 的 斜线 表示 欧几里得 距离 在 现实 中 
是 不 可能 的 其他 三条 折线 表示 了 曼哈顿 
距离 这三条 折线 的 长度 是 相等 的 当 p 
趋 近于 无穷大 时 闵可夫 斯基 距离 转化成 切比雪夫 距离 
Chebyshev distance 我们 知道 平面 上到 原点 欧几里得 距离 p 
= 2 为 1 的 点 所 组成 的 形状 
是 一个 圆 当 p 取 其他 数值 的 时候 
呢 注意 当 p     1 时 闵可夫 斯基 
距离 不再 符合 三角形 法则 举个 例子 当 p   
  1 0 0 到 1 1 的 距离 等于 
1 + 1 ^ { 1 / p }   
  2 而 0 1 到 这 两个 点 的 
距离 都是 1 闵可夫 斯基 距离 比较 直观 但是 它 
与 数据 的 分布 无关 具有 一定 的 局限性 如果 
x 方向 的 幅 值 远远 大于 y 方向 的 
值 这个 距离 公式 就会 过度 放大 x 维度 的 
作用 所以 在 计算 距离 之前 我们 可能 还 需要 
对 数据 进行   z transform   处理 即 减去 
均值 除以 标准差 该 维 度上 的 均值 该 维 
度上 的 标准差 可以 看到 上述 处理 开始 体现 数据 
的 统计 特性 了 这种 方法 在 假设 数据 各个 
维度 不 相关 的 情况 下 利用 数据分布 的 特性 
计算 出 不同 的 距离 如果 维度 相互之间 数据 相关 
例如 身高 较高 的 信息 很 有可能 会 带来 体重 
较重 的 信息 因为 两者 是 有 关联 的 这时候 
就要 用到 马氏 距离 Mahalanobis distance 了 2 . 马氏 
距离 考虑 下面 这张 图 椭圆 表示 等高线 从 欧几里得 
的 距离 来 算 绿 黑 距离 大于 红黑 距离 
但是 从 马氏 距离 结果 恰好相反 马氏 距离 实际上 是 
利用 Cholesky transformation 来 消除 不同 维度 之间 的 相关性 
和 尺度 不同 的 性质 假设 样本点 列 向量 之间 
的 协方差 对称矩阵 是     通过 Cholesky Decomposition 实际上 
是 对称矩阵 LU 分解 的 一种 特殊 形式 可 参考 
之前 的 博客 可以 转化 为 下 三角 矩阵 和上/nr 
三角 矩阵 的 乘积     消除 不同 维度 之间 
的 相关性 和 尺度 不同 只 需要 对 样本点 x 
做 如下 处理   处理 之后 的 欧几里得 距离 就是 
原 样本 的 马氏 距离 为了 书写 方便 这里 求 
马氏 距离 的 平方 下图 蓝色 表示 原 样本点 的 
分布 两颗 红星 坐标 分别 是 3 3 2 2 
由于 x y 方向 的 尺度 不同 不能 单纯 用 
欧几里得 的 方法 测量 它们 到 原点 的 距离 并且 
由于 x 和 y 是 相关 的 大致 可以 看出 
斜向 右上 也 不能 简单 地 在 x 和 y 
方向 上 分别 减去 均值 除以 标准差 最 恰当 的 
方法 是 对 原始 数据 进行 Cholesky 变换 即 求 
马氏 距离 可以 看到 右边 的 红星 离 原点 较近 
将 上面 两个 图 的 绘制 代码 和求/nr 马氏 距离 
的 代码 贴 在 这里 以备 以后 查阅 1 # 
* coding = utf 8 * 2 3 # code 
related at http / / www . cnblogs . com 
/ daniel D / 4 5 import numpy as np 
6 import pylab as pl 7 import scipy . spatial 
. distance as dist 8 9 10 def plotSamples x 
y z = None 11 12 stars = np . 
matrix 3 . 2 . 0 . 3 . 2 
. 0 . 13 if z is not None 14 
x y = z * np . matrix x y 
15 stars = z * stars 16 17 pl . 
scatter x y s = 10 # 画 gaussian 随机 
点 18 pl . scatter np . array stars 0 
np . array stars 1 s = 200 marker = 
* color = r # 画 三个 指 定点 19 
pl . axhline linewidth = 2 color = g # 
画 x 轴 20 pl . axvline linewidth = 2 
color = g # 画 y 轴 21 22 pl 
. axis equal 23 pl . axis 5 5 5 
5 24 pl . show 25 26 27 # 产生 
高斯分布 的 随机 点 28 mean = 0 0 # 
平均值 29 cov = 2 1 1 2 # 协方差 
30 x y = np . random . multivariate _ 
normal mean cov 1000 . T 31 plotSamples x y 
32 33 covMat = np . matrix np . cov 
x y # 求 x 与 y 的 协方差 矩阵 
34 Z = np . linalg . cholesky covMat . 
I # 仿射 矩阵 35 plotSamples x y Z 36 
37 # 求 马氏 距离 38 print \ n 到 
原点 的 马氏 距离 分别 是 39 print dist . 
mahalanobis 0 0 3 3 covMat . I dist . 
mahalanobis 0 0 2 2 covMat . I 40 41 
# 求 变换 后的/nr 欧几里得 距离 42 dots = Z 
* np . matrix 3 2 0 3 2 0 
. T 43 print \ n 变换 后到 原点 的 
欧几里得 距离 分别 是 44 print dist . minkowski 0 
0 np . array dots 0 2 dist . minkowski 
0 0 np . array dots 1 2 View Code 
马氏 距离 的 变换 和 PCA 分解 的 白化 处理 
颇有 异曲同工 之妙 不同之处 在于 就 二维 来看 PCA 是 
将 数据 主 成分 旋 转到 x 轴 正交矩阵 的 
酉 变换 再在 尺度 上 缩放 对角 矩阵 实现 尺度 
相同 而 马氏 距离 的 L 逆 矩阵 是 一个 
下 三角 先在 x 和 y 方向 进行 缩放 再在 
y 方向 进行 错 切 想象 矩 形变 平行四边形 总体 
来说 是 一个 没有 旋转 的 仿射变换 3 . 向量 
内积 向量 内积 是 线性代数 里 最为 常见 的 计算 
实际上 它 还是 一种 有效 并且 直观 的 相似性 测量 
手段 向量 内积 的 定义 如下 直观 的 解释 是 
如果 x 高的/nr 地方 y 也 比较 高 x 低 
的 地方 y 也 比较 低 那么 整体 的 内积 
是 偏大 的 也 就是说 x 和 y 是 相似 
的 举个 例子 在 一段 长 的 序列 信号 A 
中 寻找 哪 一段 与 短 序列 信号 a 最 
匹配 只 需要 将 a 从 A 信号 开头 逐个 
向后 平移 每次 平移 做 一次 内积 内积 最大 的 
相似 度 最大 信号 处理 中 DFT 和 DCT 也是 
基于 这种 内积 运算 计算 出 不同 频 域内 的 
信号 组分 DFT 和 DCT 是 正交 标准 基 也 
可以 看做 投影 向量 和 信号 都是 离散 值 如果 
是 连续 的 函数值 比如 求 区间 1 1   
两个 函数 之间 的 相似 度 同样 也 可以 得到 
系数 组分 这种 方法 可以 应用于 多项式 逼近 连续函数 也 
可以 用到 连续函数 逼近 离散 样本点 最小二乘 问题 OLS coefficients 
中 扯 得 有点 远了 向量 内积 的 结果 是 
没有 界限 的 一种 解决 办法 是 除以 长度 之后 
再 求 内积 这 就是 应用 十分 广泛 的 余弦 
相似 度 Cosine similarity 余弦 相似 度 与 向量 的 
幅 值 无关 只 与 向量 的 方向 相关 在 
文档 相似 度 TF IDF 和 图片 相似性 histogram 计 
算上 都有 它 的 身影 需要 注意 一点 的 是 
余弦 相似 度 受到 向量 的 平移 影响 上式 如果 
将 x 平 移到 x + 1 余弦 值 就会 
改变 怎样 才能 实现 平移 不变性 这 就是 下面 要说 
的 皮尔逊 相关系数 Pearson correlation 有时候 也 直接 叫 相关系数 
皮尔逊 相关系数 具有 平移 不变性 和 尺度 不变性 计算出 了 
两个 向量 维度 的 相关性 不过 一般 我们 在 谈论 
相关 系数 的 时候 将 x 与 y 对应 位置 
的 两个 数值 看作 一个 样本点 皮尔逊 系数 用来 表示 
这些 样本 点 分布 的 相关性 由于 皮尔逊 系数 具有 
的 良好 性质 在 各个 领域 都 应用 广泛 例如 
在 推荐 系统 根据 为 某一 用户 查找 喜好 相似 
的 用户 进而 提供 推荐 优点 是 可以 不受 每个 
用户 评分 标准 不同 和 观看 影片 数量 不 一样 
的 影响 4 . 分类 数 据点 间 的 距离 
汉明 距离 Hamming distance 是 指 两个 等长 字符串 s1 
与 s2 之间 的 汉明 距离 定义 为 将 其中 
一个 变为 另外 一个 所 需要 作 的 最小 替换 
次数 举个 维基百科 上 的 例子 还 可以 用 简单 
的 匹配 系数 来 表示 两 点 之间 的 相似 
度 匹配 字符 数 / 总 字符 数 在 一些 
情况 下 某些 特定 的 值 相等 并 不能 代表 
什么 举个 例子 用 1 表示 用户 看过 该 电影 
用 0 表示 用户 没有 看过 那么 用户 看 电影 
的 的 信息 就 可用 0 1 表示 成 一个 
序列 考虑 到 电影 基数 非常 庞大 用户 看过 的 
电影 只占 其中 非常 小 的 一部分 如果 两个 用户 
都 没有 看过 某 一部 电影 两个 都是 0 并 
不能 说明 两者 相似 反而 言之 如果 两个 用户 都 
看过 某 一部 电影 序列 中 都是 1 则 说明 
用户 有 很大 的 相似 度 在 这个 例子 中 
序列 中 等于 1 所占 的 权重 应该 远远 大于 
0 的 权重 这就 引出 下面 要说 的 杰 卡德 
相似系数 Jaccard similarity 在 上面 的 例子 中 用 M11 
表示 两 个 用户 都 看过 的 电影 数目 M10 
表示 用户 A 看过 用户 B 没 看过 的 电影 
数目 M01 表示 用户 A 没 看过 用户 B 看过 
的 电影 数目 M00 表示 两 个 用户 都 没有 
看过 的 电影 数目 Jaccard 相似性 系数 可以 表示 为 
Jaccard similarity 还 可以 用 集合 的 公式 来 表达 
这里 就 不多 说 了 如果 分类 数值 点 是 
用 树形 结构 来 表示 的 它们 的 相似性 可以 
用 相同 路径 的 长度 来 表示 比如 / product 
/ spot / ballgame / basketball 离 product / spot 
/ ballgame / soccer / shoes 的 距离 小于 到 
/ product / luxury / handbags 的 距离 以为 前者 
相 同父 节点 路径 更长 5 . 序列 之间 的 
距离 上一 小节 我们 知道 汉明 距离 可以 度量 两个 
长度 相同 的 字符串 之间 的 相似 度 如果 要 
比较 两个 不同 长度 的 字符串 不仅 要 进行 替换 
而且 要 进行 插入 与 删除 的 运算 在 这种 
场合 下 通常 使用 更加 复杂 的 编辑 距离 Edit 
distance Levenshtein distance 等 算法 编辑 距离 是 指 两个 
字串 之间 由 一个 转成 另一个 所需 的 最少 编辑 
操作 次数 许可 的 编辑 操作 包括 将 一个 字符 
替换成 另一 个字符 插入 一 个字符 删除 一 个字符 编辑 
距离 求 的 是 最少 编辑 次数 这 是 一个 
动态 规划 的 问题 有兴趣 的 同学 可以 自己 研究 
研究 时间 序列 是 序列 之间 距离 的 另外 一个 
例子 DTW 距离 Dynamic Time Warp 是 序列 信号 在 
时间 或者 速度 上 不 匹配 的 时候 一种 衡量 
相似 度 的 方法 神马 意思 举个 例子 两份 原本 
一样 声音 样本 A B 都说了/nr 你好 A 在 时间 
上 发生 了 扭曲 你 这个 音 延长 了 几秒 
最后 A 你 ~ ~ ~ 好 B 你好 DTW 
正是 这样 一种 可以 用来 匹配 A B 之间 的 
最短距离 的 算法 DTW 距离 在 保持 信号 先后 顺序 
的 限制 下 对 时间 信号 进行 膨胀 或者 收缩 
找到 最优 的 匹配 与 编辑 距离 相似 这 其实 
也 是 一个 动态 规划 的 问题 实现 代码 转 
自   McKelvin s Blog   1 # / usr 
/ bin / python2 2 # * coding UTF 8 
* 3 # code related at http / / blog 
. mckelv . in / articles / 1453 . html 
4 5 import sys 6 7 distance = lambda a 
b 0 if a = = b else 1 8 
9 def dtw sa sb 10 11 dtw u 干啦今/nr 
今 今 今 今天天气 气气 气气 好 好好好 啊啊啊 u 
今天 天气 好 好啊 12 2 13 14 MAX _ 
COST = 1 32 15 # 初始化 一个 len sb 
行 i len sa 列 j 的 二维 矩阵 16 
len _ sa = len sa 17 len _ sb 
= len sb 18 # BUG 这样 是 错误 的 
浅 拷贝 dtw _ array = MAX _ COST * 
len sa * len sb 19 dtw _ array = 
MAX _ COST for i in range len _ sa 
for j in range len _ sb 20 dtw _ 
array 0 0 = distance sa 0 sb 0 21 
for i in xrange 0 len _ sb 22 for 
j in xrange 0 len _ sa 23 if i 
+ j = = 0 24 continue 25 nb = 
26 if i 0 nb . append dtw _ array 
i 1 j 27 if j 0 nb . append 
dtw _ array i j 1 28 if i 0 
and j 0 nb . append dtw _ array i 
1 j 1 29 min _ route = min nb 
30 cost = distance sa j sb i 31 dtw 
_ array i j = cost + min _ route 
32 return dtw _ array len _ sb 1 len 
_ sa 1 33 34 35 def main argv 36 
s1 = u 干啦今/nr 今 今 今 今天天气 气气 气气 
好 好好好 啊啊啊 37 s2 = u 今天 天气 好 
好啊 38 d = dtw s1 s2 39 print d 
40 return 0 41 42 if _ _ name _ 
_ = = _ _ main _ _ 43 sys 
. exit main sys . argv View Code6 . 概率分布 
之间 的 距离 前面 我们 谈论 的 都是 两个 数值 
点 之间 的 距离 实际上 两个 概率分布 之间 的 距离 
是 可以 测量 的 在 统计学 里面 经常 需要 测量 
两组 样本分布 之间 的 距离 进而 判断 出 它们 是否 
出自 同一 个 population 常见 的 方法 有 卡方检验 Chi 
Square 和 /nr KL 散度 KL Divergence 下面 说一说 KL 散度 
吧 先从 信息熵 说起 假设 一篇 文章 的 标题 叫做 
黑洞 到底 吃 什么 包含 词语 分别 是 { 黑洞 
到底 吃什么 } 我们 现在 要 根据 一个 词语 推测 
这 篇 文章 的 类别 哪个 词语 给予 我们 的 
信息 最多 很容易 就 知道 是 黑洞 因为 黑洞 这个 
词语 在 所有 的 文档 中 出现 的 概率 太低 
啦 一旦 出现 就 表明 这 篇 文章 很 可能 
是 在 讲 科普知识 而 其他 两个 词语 到底 和 
吃什么 出现 的 概率 很高 给予 我们 的 信息 反而 
越少 如何 用 一个 函数 h x 表示 词语 给予 
的 信息量 呢 第一 肯定 是 与 p x 相关 
并且 是 负相关 第二 假设 x 和 y 是 独立 
的 黑洞 和 宇宙 不 相互 独立 谈到 黑洞 必然会 
说 宇宙 即 p x y = p x p 
y 那么 获得 的 信息 也是 叠加 的 即 h 
x y = h x + h y 满足 这 
两个 条件 的 函数 肯定 是 负 对数 形式 对 
假设 一个 发送者 要将 随机变量 X 产生 的 一长串 随机 
值 传送 给 接收者 接受者 获得 的 平均 信息量 就是 
求 它 的 数学期望 这 就是 熵 的 概念 另外 
一个 重要 特点 是 熵 的 大小 与 字符 平均 
最短 编码 长度 是 一样 的 shannon 设有 一个 未知 
的 分布 p x 而 q x 是 我们 所 
获得 的 一个 对 p x 的 近似 按照 q 
x 对 该 随机变量 的 各个 值 进行 编码 平均 
长度 比 按照 真实 分布 的 p x 进行 编码 
要 额外 长 一些 多 出来 的 长度 这 就是 
KL 散度 之所以 不说 距离 是 因为 不 满足 对称性 
和 三角形 法则 即 KL 散度 又叫 相对 熵 relative 
entropy 了解 机器 学习 的 童鞋 应该 都 知道 在 
Softmax 回归 或者 Logistic 回归 最后 的 输出 节点 上 
的 值 表示 这个 样本 分到 该类 的 概率 这 
就是 一个 概率分布 对于 一个 带有 标签 的 样本 我们 
期望 的 概率分布 是 分到 标签 类 的 概率 是 
1   其他 类 概率 是 0 但是 理想 很 
丰满 现实 很 骨感 我们 不 可能 得到 完美 的 
概率 输出 能做 的 就是 尽量 减小 总 样本 的 
KL 散度 之和 目标函数 这 就是 Softmax 回归 或者 Logistic 
回 归中 Cost function 的 优化 过程 啦 PS 因为 
概率 和为/nr 1 一般 的 logistic 二 分类 的 图 
只画 了 一个 输出 节点 隐藏 了 另外 一个 待 
补充 的 方法 卡方检验 Chi Square 衡量 categorical attributes 相关性 
的 mutual i n f o r m a t 
i o n p e a r m a n 
s rank coefficientEarth Mover s DistanceSimRank 迭代 算法 等 参考资料 
距离 和 相似性 度量 Machine Learning Measuring Similarity and DistanceWhat 
is Mahalanobis distance Cosine similarity Pearson correlation and OLS coefficients 
机器学习 中的 相似性 度量 动态 时间 归整 | DTW | 
Dynamic Time Warping 