关于 对 信息 熵 信息 增益 是 信息论 里 的 
概念 是 对 数据 处理 的 量化 这几个 概念 主要 
是 在 决策树 里 用到 的 概念 因为 在 利用 
特征 来 分类 的 时候 会对 特征 选取 顺序 的 
选择 这几个 概念 比较 抽象 我 也 花了 好长时间 去 
理解 自己 认为 的 理解 废话 不多 说 接下来 开始 
对 这几个 概念 解释 防止 自己 忘记 的 同时 望 
对 其他人 有个 借鉴 的 作用 如 有错误 还请 指出 
1 信息 这个 是 熵 和 信息 增益 的 基础 
概念 我 觉得 对于 这个 概念 的 理解 更 应该 
把 他 认为 是 一用 名称 就 比如 鸡 加引号 
意思 是 说 这个 是 名称 是 用来 修饰 鸡 
没 加引号 是 说 存在 的 动物 即 鸡 狗 
是 用来 修饰 狗 的 但是 假如 在 鸡 还未 
被 命名 为 鸡 的 时候 鸡 被 命名 为 
狗 狗 未被 命名为 狗 的 时候 狗 被 命名 
为 鸡 那么 现在 我们 看到 狗 就会 称 其为 
鸡 见到 鸡 的话 会 称 其为 鸡 同理 信息 
应该 是 对 一个 抽象 事物 的 命名 无论 用 
不用 信息 来 命名 这种 抽象 事物 或者 用 其他 
名称 来 命名 这种 抽象 事物 这种 抽象 事物 是 
客观 存在 的 引用 香农 的话 信息 是 用来 消除 
随机 不确定性 的 东西 当然 这 句话 虽然 经典 但是 
还是 很难 去 搞 明白 这种 东西 到底 是 个 
什么样 可能 在 不同 的 地方 来说 指 的 东西 
又 不一样 从 数学 的 角度 来说 可能 更加 清楚 
一些 数学 本来 就 是 建造 在 悬崖 之上 的 
一种 理论 一种 抽象 的 理论 利用 抽象 来 解释 
抽象 可能 更加 恰当 同时 也 是 在 机器学习 决策树 
中用 的 定义 如果 带 分类 的 事物 集合 可以 
划分 为 多个 类别 当中 则 某个 类 xi 的 
信息 定义 如下 I x 用来 表示 随机变量 的 信息 
p xi 指 是 当 xi 发生 时的/nr 概率 这里 
说 一下 随机变量 的 概念 随机变量 时 概率论 中的 概念 
是从 样本空间 到 实数集 的 一个 映射 样本空间 是 指 
所有 随机事件 发生 的 结果 的 并 集 比如 当 
你 抛 硬币 的 时候 会 发生 两个 结果 正面 
或 反面 而 随机事件 在 这里 可以 是 硬币 是 
正面 硬币 是 反面 两个 随机事件 而 { 正面 反面 
} 这个 集合 便是 样本空间 但是 在 数学 中 不会 
说 用 正面 反面 这样 的 词语 来 作为 数学 
运算 的 介质 而是 用 0 表示 反面 用 1 
表示 正面 而 正面 1 反面 0 这样 的 映射 
便 为 随机变量 即 类似 一个 数学 函数 2 熵 
既然 信息 已经 说完 熵 说 起来 就 不会 那么 
的 抽象 更多 的 可能 是 概率论 的 定义 熵 
是 约翰 . 冯./nr 诺依曼 建议 使用 的 命名 当然 
是 英文 最初 原因 是 因为 大家 都不/nr 知道 它 
是 什么 意思 在 信息论 和 概率论 中 熵 是 
对 随机变量 不确定性 的 度量 与 上边 联系起来 熵 便是 
信息 的 期望值 可以 记作 熵 只 依赖 X 的 
分布 和X的/nr 取值 没有 关系 熵 是 用来 度量 不确定性 
当 熵 越大 概率 说 X = xi 的 不确定性 
越大 反之 越小 在 机器 学期 中 分类 中说 熵 
越大 即 这个 类别 的 不确定性 更大 反之 越小 当 
随机变量 的 取值 为 两个 时 熵 随 概率 的 
变化 曲线 如 下图 当 p = 0 或 p 
= 1时 H p = 0 随机变量 完全 没有 不确定性 
当 p = 0.5时 H p = 1 此时 随机变量 
的 不确定性 最大 条件 熵 条件 熵 是 用来 解释 
信息 增益 而 引入 的 概念 概率 定义 随机变量 X 
在 给定 条件下 随机变量 Y 的 条件 熵 对 定义 
描述 为 X 给定 条件下 Y 的 条件 干率/nr 分布 
的 熵 对 X 的 数学期望 在 机器学习 中为 选定 
某个 特征 后的熵/nr 公式 如下 这里 可能会 有 疑惑 这个 
公式 是 对 条件概率 熵 求 期望 但是 上边 说是 
选定 某个 特征 的 熵 没错 是 选定 某个 特征 
的 熵 因为 一个 特征 可以 将 待 分类 的 
事物 集合 分为 多类 即 一个 特征 对 应着 多个 
类别 因此 在此 的 多个 分类 即为 X 的 取值 
3 信息 增益 信息 增益 在 决策树 算法 中 是 
用来 选择 特征 的 指标 信息 增益 越大 则 这个 
特征 的 选择性 越好 在 概率 中 定义 为 待 
分类 的 集合 的 熵 和 选定 某个 特征 的 
条件 熵 之差 这里 只 的 是 经验 熵 或 
经验 条件 熵 由于 真正 的 熵 并不知道 是 根据 
样本 计算 出来 的 公式 如下 注意 这里 不 要 
理解 偏差 因为 上边 说 了 熵 是 类别 的 
但是 在 这里 又 说是 集合 的 熵 没 区别 
因为 在 计算 熵 的 时候 是 根据 各个 类别 
对应 的 值 求 期望 来 等到 熵 4 信息 
增益 算法 举例 摘自 统计 学习 算法 训练 数据 集合 
D | D | 为 样本容量 即 样本 的 个数 
D 中 元素 个数 设有 K 个 类 Ck 来 
表示 | Ck | 为 Ci 的 样本 个数 | 
Ck | 之和 为 | D | k = 1 
2 . . . . . 根据 特征 A 将 
D 划分为 n 个 子集 D1 D2 . . . 
. . Dn | Di | 为 Di 的 样本 
个数 | Di | 之和 为 | D | i 
= 1 2 . . . . 记 Di 中 
属于 Ck 的 样本 集 合为 Dik 即 交集 | 
Dik | 为 Dik 的 样本 个数 算法 如下 输入 
D A 输出 信息 增益 g D A 1 D 
的 经验 熵 H D 此处 的 概率 计算 是 
根据 古典 概率 计算 由于 训练 数据集 总 个数 为 
| D | 某个 分类 的 个数 为 | Ck 
| 在 某个 分类 的 概率 或 说 随机变量 取 
某 值 的 概率 为 | Ck | / | 
D | 2 选定 A 的 经验 条件 熵 H 
D | A 此处 的 概率 计算 同上 由于 | 
Di | 是 选定 特征 的 某个 分类 的 样本 
个数 则 | Di | / | D | 可以 
说 为 在 选定 特征 某个 分类 的 概率 后边 
的 求和 可以 理解 为 在 选定 特征 的 某个 
类 别下 的 条件 概率 的 熵 即 训练 集为 
Di 交集 Dik 可以理解 在 Di 条件 下 某个 分类 
的 样本 个数 即 k 为 某个 分类 就是 缩小 
训练 集为 Di 的 熵 3 信息 增益 