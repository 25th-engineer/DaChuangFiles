作者 zhbzz2007 出处 http / / www . cnblogs . 
com / zhbzz2007 欢迎 转载 也请 保留 这段 声明 谢谢 
本文 主要 是 翻译 及 整理 MSRA 刘铁 岩 团队 
在 NIPS 2016 会议 上 发表 的 论文 Dual Learning 
for Machine Translation 对于 论 文中 的 算法 思想 可能 
还 没有 理解 透彻 还请 诸位 大牛 多多指教 1 简介 
最 先进 的 机器 翻译 系统 包括 基于 短语 的 
统计 机器翻译 方法 最近 出现 的 基于 神经 网络 的 
翻译 方法 严重 依赖 于 对齐 的 平行 训练 语料 
然而 实际 收集 这些 平行 语料 数据 的 代价 非常大 
因此 语料 的 规模 也 往往 有限 这将 会 限制 
相关 的 研究 和 应用 我们 知道 在 互联网 中 
存在 海量 的 单语/nr 数据 很 自然 的 想到 能否 
利用 它们 去 提升 机器翻译 系统 的 效果 呢 实际上 
基于 这个 想法 研究 人员 已经 提出 了 许多 不同 
的 方法 这里 可以 粗略 的 分为 两类 第一类 目标语言 
的 单语/nr 语料 被 用于 训练 语言 模型 然后 集成 
到 翻译 模型 从/p 平行/n 双/n 语语/n 料中/i 训/vn 练出/v 
中 最终 提升 翻译 质量 第二类 通过 使用 翻译 模型 
从/p 对齐/d 的/uj 平/n 行语/i 料中/i 训练/vn 从单语/nr 数据/n 中/f 
生成/v 伪/l 双/n 语句/n 对/p 然后 在 后续 的 训练 
过程 中 这些 伪 双 语句 对 被 用于 扩充 
训练 数据 尽管 上述 方法 能够 在 一定 程度 上 
提升 翻译 系统 的 效果 但是 它们 依然 存在 一定 
的 局限性 第一类 的 方法 只 使用 了 单语/nr 数据 
来 训练 语言 模型 并 没有 解决 平行 训练 数据 
不足 这个 问题 尽管 第二类 方法 可以 扩充 平行 训练 
数据 但是 并 不能 保 证伪 双 语句 对 的 
质量 在 这篇文章 中 刘铁 岩 团队 提出 了 一种 
Dual Learning 对偶 学习 机制 可以 有效 地 利用 单语/nr 
数据 源语言 与 目标语言 通过 使用 他们 提出 的 机制 
单语/nr 数据 与 平行 双语 数据 扮演着 相似 的 角色 
在 训练 过程 中 可以 显著 降低 对 平行 双语 
数据 的 要求 对偶 学习 机制 应用 在 机器 翻译 
中 可以想象 成 两个 agent 机器 在 玩 通信 游戏 
如下 图 所示 第一 个 机器 只 理解 语言 A 
通过 噪声 信道 发送 一条 信息 是 语言 A 给 
第二 个 机器 噪声 信道 通过 翻译 模型 会将 语言 
A 转换成 语言 B 第二个 机器 只 理解 语言 B 
接收 到 翻译 过来 的 信息 是 语言 B 它 
检查 这条 信息 并 通知 第一 个 机器 第二个 机器 
可能 无法 确认 这个 翻译 的 正确性 因为 它 不 
知道 原始 的 消息 然后 它 通过 另一个 噪声 信道 
使用 另外 一个 翻译 模型 将 接收 到 的 消息 
从 语言 B 转换 为 语言 A 将 接收 到 
的 信息 发送 给 第一个 机器 从 第二个 机器 接收 
到 信息 后 第一 个 机器 会 检查 它 并 
通知 第二个 机器 它 接 收到 的 信息 是否 包含 
它 原来 的 信息 通过 这个 反馈 两个 机器 就 
可以 知道 这 两个 通信 信道 也 就是 两个 翻译 
模型 表现 是否 良好 以及 能否 提高 它们 的 效果 
这个 游戏 也 可以 从 第二个 机器 开始 那么 原始 
的 消息 就 是 语言 B 这 两个 机器 将会 
经过 一个 对称 化 的 过程 通过 反馈 从而 提高 
两个 信道 翻译 模型 的 效果 从 上面 的 描述 
中 很 容易 发现 尽管 两个 机器 可能 没有 对齐 
的 双语 语料 它们 依然 可以 获得 两个 翻译 模型 
的 翻译 质量 方面 的 反馈 然后 基于 反馈 持续 
提升 模型 的 效果 这个 游戏 可以 玩 任意 轮 
两个 翻译 模型 通过 强化 过程 例如 通过 策略 梯度 
方法 得到 持续改善 通过 这种 方式 他们 开发 了 一个 
通用 的 学习 框架 通过 对偶 学习 游戏 这个 框架 
可 用于 训练 机器翻译 模型 对偶 学习 机制 很 一些 
不同 的 特点 首先 我们 通过 强化 学习 从未/nr 标注 
数据 中 训练 翻译 模型 这个工作 显著 降低 了 对 
对齐 双语 数据 的 要求 它 打开 了 一个 新的 
窗口 可以 从头开始 甚至 不 使用 任何 平行 数据 学习 
一个 翻译 模型 实验 结果 显示 这个 方法 很 有 
前景 其次 显示 出 深度 强化 学习 DRL 在 复杂 
真实世界 中 的 应用 而 不仅仅 是 在 游戏 这个 
领域 在 最近 几年 深度 强化 学习 吸引 了 很多 
科研 人员 的 注意力 但是 大 部分 应用 还是 集中 
在 视频 或者 棋盘 游戏 将 深度 强化 学习 应用 
到 更加 复杂 的 应用 规则 没有 事先 定义 好 
并且 没有 明确 的 奖励 信号 依然 存在 很大 挑战 
对偶 学习 提供 了 一种 很 有 前景 的 方式 
可以 在 真实世界 应用 中 例如 机器翻译 抽 取出 强化 
学习 需要 的 奖励 信号 2 相关 背景 对偶 学习 
框架 可以 应用 到 基于 短语 的 统计 机器 翻译 
和 神经 机器翻译 中 在 这篇文章 中 我们 主要 聚焦 
在 后者 神经 机器翻译 因为 它 作为 一个 端 到 
端的 系统 很 简单 不 需要 人工 设计 精巧 的 
工程 神经 机器翻译 系统 通常 是 通过 基于 编码 解码 
框架 的 循环 神经网络 RNN 来 实现 这个 框架 从 
源语言 句子 \ x = { x1 x2 . . 
. x _ { Tx } } \ 到 目标语言 
\ y = { y1 y2 . . . y 
_ { Ty } } \ 学习 了 一个 概率 
映射 P y | x 其中 xi 和 yt 分别 
是 句子 x 的 第 i 个 词 和 句子 
y 的 第 t 个 词 更 具体 一些 神经 
机器 翻译 的 编码器 读取 源语言 句子 x 然后 基于 
RNN 生成 \ T _ { x } \ 个 
状态 \ h _ { i } = f h 
_ { i 1 } x _ { i } 
\ \ \ \ \ \ 1 \ \ h 
_ { i } \ 是 时刻 t 的 隐 
状态 函数 f 是 循环 单元 例如 Long Short Term 
Memory LSTM 单元 或者 Grated Recurrent Unit GRU 然后 神经 
网络 的 解码器 计算 每个 目标 词 \ y _ 
{ t } \ 的 条件 概率 对于 \ y 
_ { t } \ 已知 它 先前 的 词 
\ y _ { t } \ 和 源语言 句子 
例如 基于 概率 链式法则 使用 \ P y _ { 
t } | y _ { t } x \ 
来 确定 \ P y | x \ \ P 
y _ { t } | y _ { t 
} x \ 如 下列 所示 \ P y _ 
{ t } | y _ { t } x 
\ varpropto exp y _ { t } r _ 
{ t } c _ { t } \ \ 
\ \ \ \ 2 \ \ r _ { 
t } = g r _ { t 1 } 
y _ { t 1 } c _ { t 
} \ \ \ \ \ \ 3 \ \ 
c _ { t } = q r _ { 
t 1 } h _ { 1 } . . 
. h _ { T _ { x } } 
\ \ \ \ \ \ 4 \ 其中 \ 
r _ { t } \ 是 解码器 RNN 在 
时刻 t 的 隐 状态 相 似地 也 是 通过 
LSTM 或者 GRU 进行 计算 \ c _ { t 
} \ 根据 编码器 的 隐 状态 定义 了 生成 
词 \ y _ { t } \ 的 上下文 
信息 \ c _ { t } \ 可以 是 
句子 x 的 全局 信息 例如 \ c _ { 
1 } = c _ { 2 } = . 
. . = c _ { T _ { y 
} } = h _ { T _ { x 
} } \ 或者 是 局部 信息 局部 信息 通过 
注意力 机制 实现 例如 \ c _ { t } 
= \ sum _ { i = 1 } ^ 
{ T _ { x } } \ alpha _ 
{ i } h _ { i } \ alpha 
_ { i } = \ frac { exp \ 
{ a h _ { i } r _ { 
t 1 } \ } } { \ sum _ 
{ j } exp \ { a h _ { 
j } r _ { t 1 } \ } 
} \ 其中 \ a . . \ 是 一个 
前馈 神经网络 我们 将 神经 网络 中 待 优化 的 
所有 参数 定义 为 \ \ Theta \ 将 用于 
训练 的 源语言 目标语言 数据集 定义 为 D 然后 要 
学习 的 目标 函数 就是 寻找 最优 的 参数 \ 
\ Theta ^ { * } \ \ \ Theta 
^ { * } = argmax _ { \ Theta 
} \ sum _ { x y \ in { 
D } } \ sum _ { t = 1 
} ^ { T _ { y } } logP 
y _ { t } | y _ { t 
} x \ Theta \ \ \ \ \ \ 
5 \ 3 对偶 学习 在 机器 翻译 中的 应用在 
这 章中 我们 将 会 介绍 对偶 学习 机制 在 
神经 机器翻译 中的 应用 注意到 翻译 任务 经常 是 两个 
方向 我们 首先 设计 一个 有 两个 机器人 的 游戏 
包含/v 前/f 向/p 翻译/v 步骤/n 和/c 反向/v 翻译/v 步骤/n 即使 
只 使用 单语/nr 数据 也 可以 给 两个 对偶 翻译 
模型 提供 质量 反馈 然后 我们 提出 了 对偶 学习 
算法 称之为 对偶 神经 机器翻译 简称 dual NMT 在 游戏 
中 基于 反馈 回来 的 质量 提升 两个 翻译 模型 
有/v 两个/m 单语/nr 语料/n \ D _ { A } 
\ 和 \ D _ { B } \ 分别 
包含 语言 A 和 语言 B 的 句子 需要 注意 
的 是 这 两个 语料 并不需要 互相 对齐 甚至 互相 
之间 一点 关系 都 没有 假设 我们 右 两个 弱 
翻译 模型 可以 将 句子 从 语言 A 翻译 到 
语言 B 反之亦然 我们 的 目标 是 使用 单语/nr 语料 
而非 平行 语料 来 提高 两 个 模型 的 准确率 
从/p 任何/r 一个/m 单语/nr 数据/n 的/uj 句子/n 开始/v 我们 首先 
将 其 翻译 为 另一种 语言 然后再 将其 翻译 回 
原始 语言 通过 评估 这两个 翻译 结果 我们 将 会 
了解 到 两个 翻译 模型 的 质量 并 根据 此 
来 提升 它们 这个 过程 可以 迭代 很 多轮 直到 
翻译 模型 收敛 假设 语料 \ D _ { A 
} \ 有 \ N _ { A } \ 
个 句子 \ D _ { B } \ 有 
\ N _ { B } \ 个 句子 定义 
\ P . | s \ Theta _ { AB 
} \ 和 \ P . | s \ Theta 
_ { BA } \ 为 两个 神经 翻译 模型 
这里 \ \ Theta _ { AB } \ 和 
\ \ Theta _ { BA } \ 是 它们 
的 参数 正如 第 2 章中 所 描述 假设 我们 
已经 有 两个 训 练好 的 语言 模型 \ LM 
_ { A } . \ 和 \ LM _ 
{ B } . \ 很 容易 获得 因为 它们 
只 需要 单语/nr 数据 每个 语言 模型 获取 一个 句子 
作为 输入 然后 输出 一个 实数值 用于 表示 这个 句子 
是 它 所属 语言 自然 句子 的 自信度 这里 语言 
模型 既 可以 使用 其他 资源 也 可以 仅 仅 
使用 单语/nr 数据 \ D _ { A } \ 
和 \ D _ { B } \ 如果 游戏 
是从 \ D _ { A } \ 中的 句子 
s 开始 定义 \ s _ { mid } \ 
作为 中间 翻译 输出 这个 中间 步骤 有 一个 中间 
的 奖励 \ r _ { 1 } = LM 
_ { B } s _ { mid } \ 
表示 输出 句子 在 语言 B 中 的 自然 程度 
已知 中间 翻译 输出 \ s _ { mid } 
\ 我们 使用 从 \ s _ { mid } 
\ 还原 过来 的 s 的 对数 概率 作为 通信 
的 奖励 我们 将 会 交替 使用 重构 和 通信 
数学上 定义 奖励 \ r _ { 2 } = 
log P s | s _ { mid } \ 
Theta _ { BA } \ 我们 简单 采用 语言 
模型 奖励 和 通信 奖励 的 线性组合 作为 整体 奖励 
例如 \ r = \ alpha r _ { 1 
} + 1 \ alpha r _ { 2 } 
\ 这里 \ \ alpha \ 是 超 参数 由于 
游戏 的 奖励 可以 视为 s \ s _ { 
mid } \ 以及 翻译 模型 \ \ Theta _ 
{ AB } \ 和 \ \ Theta _ { 
BA } \ 的 函数 因此 我们 可以 通过 策略 
梯度 方法来 优化 翻译 模型 中 的 参数 从而 达到 
奖励 最大化 这个 方法 在 强化 学习 中 应用 很 
广泛 我们 基于 翻译 模型 \ P . | s 
\ Theta _ { BA } \ 采样 出 \ 
s _ { mid } \ 然后 我们 计算 期望 
奖励 \ E r \ 关于 参数 \ \ Theta 
_ { AB } \ 和 \ \ Theta _ 
{ BA } \ 的 梯度 根据 策略 梯度 定理 
很容易 得到 \ \ bigtriangledown _ { \ Theta _ 
{ BA } } E r = E 1 \ 
alpha \ bigtriangledown _ { \ Theta _ { BA 
} } log P s | s _ { mid 
\ Theta _ { BA } } \ \ \ 
\ \ 6 \ \ \ bigtriangledown _ { \ 
Theta _ { AB } } E r = E 
r \ bigtriangledown _ { \ Theta _ { AB 
} } log P s _ { mid } | 
s \ Theta _ { AB } \ \ \ 
\ \ 7 \ 这里 期望 替换 掉 \ s 
_ { mid } \ 基于 公式 6 和 公式 
7 我们 可以 采用 任何 的 采样 方法 来 估计 
期望 的 梯度 考虑到 随机 采样 将会 带来 非常 大 
的 方差 并且 会 导致 机器 翻译 中 出现 不 
合理 的 结果 针对 梯度 计算 我们/r 使用/v 束/nr 搜索/v 
来/v 获取/v 更加/d 有/v 意义/n 的/uj 结果/n 更加 合理 的 
中间 翻译 输出 例如 我们 贪婪 地 产生 top K 
个 高 概率 的 中间 翻译 输出 然后 使用 束 
搜索 的 平均值 来 近似 真实 的 梯度 如果 游戏 
是 在 \ D _ { B } \ 中的 
句子 s 开始 梯度 的 计算 就是 一个 对称 在此 
我们 忽略 掉 它 游戏 可以 重复 很 多轮 在 
每 一轮 中 一个 句子 从 \ D _ { 
A } \ 中 采样 另一个 句子 是从 \ D 
_ { B } \ 中 采样 我们 基于 游戏 
分别 从 两个 句子 开始 来 更新 这两个 翻译 模型 
具体 的 细节 在 算法 1中 给出 4 相关 实验 
我们 做 了 一系列 实验 来 测试 提出 的 对偶 
学习 机制 在 机器 翻译 上 的 效果 4.1 实验 
设置 我们 使用 两个 基准 系统 和 对偶 机器翻译 方法 
进行 对比 1 标准 神经 机器翻译 简称 NMT 2 最近 
提出 的 基于 NMT 的 方法 通过/p 单语/nr 语料/n 生成/v 
伪/l 双/n 语句/n 对/p 用于/v 辅助/vn 训练/vn 简称 pseudo NMT 
我们 的 所有 实验 都是 使用 Theano 实现 的 辅助 
NMT 系统 来 完成 我们 评估 这些 算法 在 一对 
翻译 任务 上 的 效果 包括 英语 翻译 为 法语 
En Fr 和 法语 到 英语 Fr En 具体 地 
我们 使用 相同 的 双语 语料 语料 来源于 WMT14 共有 
1200 万句 对 然后 将 newstest2012 和 newstest2013 作为 开发 
数据集 newstest2014 作为 测试 数据集 WMT14 提供 的 News Crawl 
articles from 2012 作为 单语/nr 数据 我们 使用 GRU 网络 
并 遵循 论文 1 D . Bahdanau K . Cho 
and Y . Bengio . Neural machine translation by jointly 
learning to alignand translate . ICLR 2015 . 中 的 
实践 来 设置 实验 参数 对于 每 种 语言 我们 
用 平行 语料 中最 频繁 的 3 万个 词 构造 
词汇表 将 包含 至少 一个 OOV 单词 的 句子 删除 
每个 词 被 映 射到 620 维 的 连续 向量空间 
循环 单元 的 维度 是 1000 我们 将 训练 集中 
超过 50个 词 的 句子 删除 batch 的 大小 为 
80 每 20个 batch 被 预 取 并 按照 句子 
长度 排序 对于 基准 的 NMT 模型 我们 完全 按照 
论文 1 提出 的 设置 对于 基准 的 pseudo NMT 
模型 我们/r 使用/v 训练/vn 好/a 的/uj NMT/w 模型/n 从单语/nr 数据/n 
中/f 生成/v 伪/l 双/n 语句/n 对/p 删除 超过 50个 词 
的 句子 将 生成 的 数据 与 原始 平行 训练 
数据 融合 在 一起 然后 训练 这个 模型 并 用于 
测试 每个 基准 系统 通过 AdaDelta 算法 在 K40m GPU 
进行 训练 直到 它们 的 效果 在 开发 集 上 
不再 提升 为止 对偶 神经 机器翻译 简称 dual NMT 模型 
需要 每 种 语言 的 语言 模型 我们 对于 每 
种 语言 使用 相应 的 单语/nr 语料 训练 了 基于 
RNN 的 语言 模型 实验 中 语言 模型 就 固定 
了 然后 收到 信息 的 对数 似 然 作为 通信 
信道 例如 翻译 模型 的 奖励 在 玩 游戏 时 
我们 使用 暖 启动 翻译 模型 已经/d 从双/nr 语语/n 料中/i 
训练/vn 出来/v 的/uj 来 初始化 信道 然后 观察 dual NMT 
模型 能否 有效 提升 模型 翻译 准确率 在 我们 的 
实验 中 为了/p 从/p 双语/nz 数据/n 训练/vn 的/uj 初始模型/n 平滑/a 
过渡/v 到/v 完全/ad 从单语/nr 数据/n 训练/vn 的/uj 模型/n 我们 采用 
了 以下 软着陆 策略 在 每次 对偶 学习 过程 的 
开始 对于 每个 mini batch 我们/r 使用/v 单语/nr 数据/n 一半/m 
的/uj 句子/n 和/c 双语/nz 数据/n 从 数据 中 采样 出来 
的 用于 训练 初始模型 中 一半 的 句子 目标 就是 
基 于单 语 数据 最大化 奖励 在 第 3 部分 
定义 的 的 加权 之和 以及 基于 双语 数据 的 
似 然 度 在 第 2 部分 定义 随着 训练 
过程 的 持续 我们 逐渐 在 mini batch 中 增加 
单语/nr 数据 的 比例 直到 完全 不 使用 双语 数据 
这里 我们 在 实验 中 测试 了 两个 测试 第一 
个 设置 参考 Large 我们 在 软着陆 阶段 使用 全部 
的 1200 万双 语句 对 也 就是 暖 启动 模型 
是 基于 全部 双语 数据 进行 学习 的 第二个 设置 
参考 Small 我们 在 1200 万双 语句 对 中 随机 
采样 了 10% 的 数据 并在 软着陆 阶段 使用 它 
对于 每个 设置 我们 都是/nr 训练 对偶 机器翻译 算法 一周 
我们 在 中间 翻译 过程 中 设置 束 搜索 大小 
为 2 实验 中 的 所有 超 参数 通过 交叉 
验证 来 设置 我们 使用 BLEU 作为 评估 标准 由 
moses 提供 的 脚本 工具 进行 计算 遵循 常规 的 
实践 在 测试 阶段 如同 先前 许多 的 工作 我们 
对于 所有 的 算法 均 使用 大小 为 12 的 
束 搜索 4.2 实验 结果 分析 我们 在 这部分 分析 
实验 结果 回忆 之前 提到 的 两个 基线 系统 英语 
法语 和 法语 英语 是 分别 训练 的 但是 dual 
NMT 系统 一起 训练 这两个 基线 系统 我们 在 表 
1中 总结 了 所有 系统 的 效果 在 源语言 句子 
的 各个 长 度上 的 BLEU 分值 曲线 在 中 
画出 从表/nr 1中 我们 可以 看到 dual NMT 系统 在 
所有 的 设置 中 均 超过 了 基线 系统 在从 
英语 翻译 为 法语 的 任务 上 dual NMT 系统 
分别 在 第一 种 / 第二种 暖 启动 方式 超过 
基线 NMT 系统 大约 2.1 / 3.4 个 百分点 超过 
pseudo NMT 大约 1.7 / 3.1 个 百分点 在从 法语 
翻译 为 英语 的 任务 上 提升 更加 显著 dual 
NMT 系统 在 第一 个 / 第二个 暖 启动 方式 
上 分别 超过 NMT 大约 2.3 / 5.2 个 百分点 
超过 pseudo NMT 大约 2.1 / 4.3 个 百分点 令人 
惊讶 的 是 在 只有 10% 的 双语 数据 上 
法语 翻译 为 英语 的 任务 中 相比 使用 100% 
的 常规 NMT 系统 dual NMT 取得 了 可比 的 
翻译 正确率 这些 结果 显示 了 dual NMT 算法 的 
有效性 另外 我们 也 观察 到 如下 结果 尽管 pseudo 
NMT 的 效果 超过 NMT 它 的 提升 并 不显著 
我们 认为 可能/v 是/v 从单语/nr 数据/n 中/f 生成/v 的/uj 伪/l 
双/n 语句/n 对/p 质量/n 不好/d 这个 限制 了 pseudo NMT 
的 效果 提升 需要/v 注意/v 的/uj 就是/d 需要/v 仔细/ad 选择/v 
和/c 过滤/v 生成/v 的/uj 伪/l 双/n 语句/n 对/p 以便 pseudo 
NMT 可以 取得 更好 的 效果 当 平行 双语 数据 
较 少时 dual NMT 可以 有 更大 的 提升 这个 
显示 了 对偶 学习 机器 可以 很好 的 利用 单语/nr 
数据 因此 我们 认为 dual NMT/w 在/p 较少/d 的/uj 有/v 
标签/n 平行/n 数据/n 和/c 更大/i 的/uj 单/n 语句/n 对上/i 更/d 
有用/v dual NMT 打开 了 一个 新的 窗口 可以 从头 
开始 学习 一个 翻译 模型 我们 在 源语言 句子 的 
各个 长 度上 的 BLEU 分值 曲线 在 中 画出 
从 这个 图中 我们 可以 看出 dual NMT 算法 在 
所有 的 长度 上 超过 了 基 准系统 我们 对 
dual NMT 算法 做 了 一些 更 深入 的 研究 
如表 2 所示 我们 研究 了 各个 算法 的 重构 
后的/nr 效果 对于 测试 集 的 每个 句子 我们 将 
它 翻译 到 第 4次 并 返回 然后 使用 BLEU 
分值 来 检查 返回 的 翻译 句子 我们 使用 束 
搜索 生成 所有 的 翻译 结果 很容易 地 从表/nr 2中 
观察到 dual NMT/w 的/uj 重构/n 后的/nr BLEU/w 分数/n 比/p NMT/w 
和/c pseudo/w NMT 更高 实际上 在从 大 规模 平行 数据 
上 训练 的 暖 启动 模型 上 dual NMT 超出 
NMT 大约 11.9 / 9.6 在 10% 数据 上 训练 
的 暖 启动 模型 上 dual NMT 超出 NMT 大约 
20.7 / 17.8 我们 在 表 3 上 列出 了 
几个 例句 用于 对 比 使用 对偶 学习 之前 和 
之后 模型 的 重构 结果 很明显 在 对偶 学习 之后 
重构 的 效果 在 两个 方向 英语 法语 英语 法语 
英语 法语 上都 有 很大 提升 总之 所有 的 结果 
均 显示 出 对偶 学习 机器 很有 前景 并且 可以 
更好 地 利用 单语/nr 数据 5 扩展 在 这部分 我们 
讨论 对偶 学习 机制 可能 的 扩展 首先 尽管 在 
我们 在 这篇文章 中 集中 在 机器 翻译 任务 上 
但是 对偶 学习 的 基本 思想 具有 通用 的 应用 
性 只要 两个 任务 具备 对偶 形式 我们 就 可以 
利用 强化 学习 算法 将 对偶 机 学习 机制 应用到 
从未 标注 数据 中 同时 学习 两个 任务 实际上 许多 
人工智能 任务 天然 的 是 对偶 形式 例如 语音 识别 
和 语音 合成 图像 抓取 和 图像 合成 问题 回答 
和 问题 生成 搜索 匹配 查询 词 与 文档 的 
相似 度 和 关键词 抽取 从 文档 中 抽取 关键字 
/ 查询 词 等等 对于 更多 的 对偶 任务 而 
不仅仅 是 机器 翻译 设计 和 测试 对偶 学习 算法 
将 会很 有意义 第二 尽管 我们 将 对偶 学习 集中 
在 两个 任务 上 但是 我们 的 技术 并不 仅仅 
局限 在 两个 任务 实际上 我们 主要 的 想法 是 
形成 一个 闭环 目的 是 我们 通过 比较 原始 输入 
数据 和 最终 输出 数据 能够 提取 出 反馈 信号 
因此 如果 有 多于 两个 相关 的 任务 可以 形成 
闭环 我们 可以 应用 这个 技术 来 提升 每个 任务 
在 无 标签 数据 的 效果 例如 对于 英语 句子 
x 我们 可以 先将 它 翻译 为 中文 句子 y 
然后 将 y 翻译 为 法语 句子 z 最终 再将 
z 翻译 为 英文 句子 \ x ^ { } 
\ 句子 x 和 \ x ^ { } \ 
的 相似 度 可以 表示 闭 环中 三个 翻译 模型 
的 有效性 我们 基于 闭环 中的 反馈 信号 可以 再次 
使用 策略 梯度 方法来 更新 和 提升 这些 模型 我们 
更 愿意 将 这种 通用 的 对偶 学习 命名为 闭环 
学习 并且 在 未来 会 测试 它 的 有效性 6 
展望 我们 计划 在 后续 探索 如下 的 方向 第一 
在 实验 中 我们 使用 了 双语 数据 用于 暖 
启动 dual NMT 的 训练 更加 激动 的 方向 就是 
从头 学习 例如 直接/ad 从/p 两种/m 语言/n 的/uj 单语/nr 数据/n 
可能 需要 词汇 词典 开始 学习 第二 dual NMT 是 
基于 NMT 系统 的 我们 基本 的 想法 也 可以 
用于 基于 短语 的 统计 机器翻译 系统 中 我们 将 
会 探索 这个 方向 第三 我们 仅仅 考虑 一 对 
语言 我们 将 会 进行 扩展 使用 单语/nr 数据 联合 
训练 至少 3种 语言 的 翻译 模型 7 从 控制 
系统 的 角度 思考 对偶 学习 在 第 5 章中 
提到 了 对偶 学习 可以 视为 一个 闭环 学习 闭环 
学习 的 概念 来源于 反馈 控制系统 反馈 控制系统 输入 信号 
经过 控制器 执行器 得到 输出 信号 然后 再将 输出 信号 
采集 回来 输入 信号 减去 采集 回来 的 输出 信号 
得到 误差 根据 误差 来 调节 控制器 使得 输出 能够 
跟随 输入 信号 对偶 神经 机器翻译 系统 可以 视为 一个 
反馈 控制系统 这里 以 中文 翻译 为 英文 再将 英文 
翻译 为 中文 为例 通过 中 英 翻译 模型 将 
中文 翻译 为 英文 就是 将 输入 信号 通过 控制器 
执行器 转换 为 输出 信号 也 就是 英文 通过 英 
中 翻译 模型 将 英文 翻译 为 中文 就是 将 
输出 信号 通过 信号 采集器 返回 给 输入端 转换 为 
与 输入 信号 同等 量纲 的 信号 也即 中文 通过 
指标 比较 原始 中文 与 翻译 过来 的 中文 的 
相似性 来 评估 两个 翻译 模型 的 效果 也 就是 
计算 输入 信号 与 采集 回来 的 信号 之间 的 
误差 从而 通过 控制器 的 调节 调整 输出 信号 的 
变化 循环 迭代 直到 输出 信号 能够 跟随 输入 信号 
在 控制 系统 的 设计 中 需要 考虑 三 个 
指标 稳定性 准确性 快速性 分别 将 这三个 指标 对应 到 
对偶 机器翻译 系统 中 稳定性 就是 对偶 机器翻译 系统 中 
两个 翻译 模型 的 稳定性 如果 翻译 系统 存在 漏 
翻译 翻译 质量 低 等 问题 能否 在 迭代 过程 
中 逐渐 改善 如果 不能 得到 改善 将 会 导致 
翻译 质量 会 越来越 低 而非 越来越 好 这个 系统 
就 会 逐渐 的 变差 也 就是 这个 系统 是 
不 稳定 的 准确性 也 就是 对偶 机器翻译 系统 最终 
稳定 时 翻译 质量 能 达到 多少 快速性 如果 对偶 
翻译 系统 需要 达到 一个 期望 的 准确度 需要 多少 
时间 能 达到 另外 如果 要 超越 NMT 基线 系统 
需要 多少 时间 因此 我们 可以 发现 对偶 机器翻译 系统 
是 反馈控制 系统 的 一个 特例 对偶 机器翻译 系统 在 
设计 时 同 反馈控制 系统 一样 需要 考虑 稳定性 准确性 
和 快速性 三个 指标 很 期待 对偶 学习 以 及其 
在 机器 翻译 领域 的 发展 8 ReferenceDi He Yingce 
Xia Tao Qin Liwei Wang Nenghai Yu Tie Yan Liu 
and Wei Ying Ma Dual Learning for Machine Translation NIPS 
2016 . 演讲 | 微软 亚洲 研究院 刘铁 岩 对偶 
学习 推动 人工智能 的 新浪潮 研究 | 对偶 学习 一种 
新的 机器学习 范式 