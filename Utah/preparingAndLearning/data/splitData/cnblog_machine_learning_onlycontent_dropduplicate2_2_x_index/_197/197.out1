一 概述 这会 是 激动人心 的 一章 因为 我们 将 
首次 接触到 最优化 算法 仔细 想想 就 会 发现 其实 
我们 日常 生活 中 遇到 过 很多 最优化 问题 比如 
如何 在 最 短时间 内 从A点/nr 到达 B 点 如何 
投入 最少 工作量 却 获得 最大 的 效益 如何 设计 
发动机 使得 油耗 最少 而 功率 最大 可见 最 优化 
的 作用 十分 强大 接下来 我们 介绍 几个 最 优化 
算法 并 利用 它们 训练 出 一个 非 线性函数 用于 
分类 假设 现在 有 一些 数据 点 我们 用 一条 
直线 对 这些 点 进行 拟合 该线 称为 最佳 拟合 
直线 这个 拟合 过程 就 称作 回归 利用 Logistic 回归 
进行 分类 的 主要 思想 是 根据 现有 数据 对 
分类 边界线 建立 回归 公式 以此 进行 分类 这里 的 
回归 一 词 源于 最佳 拟合 表示 要 找到 最佳 
拟合 参 数集 其 背后 的 数学 分析 将 在下 
一 部分 介绍 训练 分类器 时的/nr 做法 就是 寻找 最佳 
拟合 参数 使用 的 是 最优化 算法 接下来 介绍 这个 
二 值 型 输出 分类器 的 数学原理 Logistic 回归 的 
一般 过程 1 收集 数据 采用 任意 方法 收集 数据 
2 准备 数据 由于 需要 进行 距离 计算 因此 要求 
数据类型 为 数值 型 另外 结构化 数据格式 则 最佳 3 
分析 数据 采用 任意 方法 对 数据 进行 分析 4 
训练 算法 大 部分 时间 将 用于 训练 训练 的 
目的 是 为了 找到 最佳 的 分类 回归系数 5 测试 
算法 一旦 训练 步骤 完成 分类 将会 很快 6 使用 
算法 首先 我们 需要 输入 一些 数据 并 将其 转换成 
对应 的 结构化 数值 接着 基于 训 练好 的 回归系数 
就 可以 对 这些 数值 进行 简单 的 回归 计算 
判定 它们 属于 哪个 类别 在这之后 我们 就 可以 在 
输出 的 类别 上 做 一些 其他 分析 工作 二 
优缺点 优点 计算 代价 不高 易于 理解 和 实现 缺点 
容易 欠 拟合 分类 精度 可能 不高 适用 数据类型 数值 
型 和 标称 型 数据 三 数学公式 我们 想要 的 
函数 应该是 能 接受 所有 的 输入 然后 预测出 类别 
例如 在 两个 类 的 情况 下 上述 函数 输出 
0 或 1 或许 你 之前 接触 过 具有 这种 
性质 的 函数 该 函数 称为 海维 塞德 阶跃 函数 
Heaviside step function 或者 直接 称 为 单位 阶跃 函数 
然而 海维 塞德 阶跃 函数 的 问题 在于 该 函数 
在 跳跃 点上 从0/nr 瞬间 跳跃 到 1 这个 瞬间 
跳跃 过程 有时 很难 处理 幸好 另一个 函数 也有 类似 
的 性质 且 数学上 更 易处理 这 就是 Sigmoid 函数 
Sigmoid 函数 具体 的 计算 公式 如下 1 给 出了 
Sigmoid 函数 在 不同 坐标 尺度 下 的 两条 曲线图 
当 x 为 0时 Sigmoid 函数值 为 0.5 随着 x 
的 增大 对应 的 Sigmoid 值 将 逼近 于1/nr 而 
随着 x 的 减小 Sigmoid 值 将 逼近 于0/nr 如果 
横坐标 刻度 足够 大 1 下图 Sigmoid 函数 看起来 很 
像 一个 阶跃 函数 1 两种 坐标 尺度 下 的 
Sigmoid 函数 图 上图 的 横坐标 为 5 到 5 
这时 的 曲线 变化 较为 平滑 下图 横坐标 的 尺度 
足够 大 可以 看到 在 x = 0点 处 Sigmoid 
函数 看起来 很像 阶跃 函数 因此 为了实现 Logistic 回归 分类器 
我们 可以 在 每个 特征 上都 乘以 一个 回归系数 然后 
把 所有 的 结果 值 相加 将 这个 总和 代入 
Sigmoid 函数 中 进而 得到 一个 范围 在 0 ~ 
1 之间 的 数值 任何 大于 0.5 的 数据 被 
分 入 1类 小于 0.5 即被 归入 0类 所以 Logistic 
回归 也 可以 被 看成 是 一种 概率 估计 确定 
了 分类器 的 函数 形式 之后 现在 的 问题 变成 
了 最佳 回归系数 是 多少 如何 确定 它们 的 大小 
四 基于 最优化 方法 的 最佳 回归系数 确定 Sigmoid 函数 
的 输入 记为 z 由 下面 公式 得出 其中 wi 
为 最佳 拟合 参数 每一 列 数据 都 对应 一个 
w 拟合 参数 如果 采用 向量 的 写法 上述 公式 
可以 写成 z = wx 它 表示 将 这 两个 
数值 向量 对应 元素 相乘 然后 全部 加起来 即 得到 
z 值 其中 的 向量 x 是 分类器 的 输入 
数据 向量 w 也 就是 我们 要 找到 的 最佳 
参数 系数 从而 使得 分类器 尽可能 地 精确 为了 寻找 
该 最佳 参数 需要 用到 最优化 理论 的 一些 知识 
下面 首先 介绍 梯度 上升 的 最优化 方法 我们 将 
学习 到 如何 使用 该 方法 求得 数据集 的 最佳 
参数 接下来 展示 如何 绘制 梯度 上升 法 产生 的 
决策 边界 图 该图 能将 梯度 上升 法的/nr 分类 效果 
可视化 地 呈现 出来 最后 我们 将 学习 随机 梯度 
上升 算法 以及 如何 对 其 进行 修改 以 获得 
更好 的 结果 梯度 上升 法 我们 介绍 的 第一 
个 最优化 算法 叫做 梯度 上升 法 梯度 上升 法 
基于 的 思想 是 要 找到 某 函数 的 最大值 
最好 的 方法 是 沿着 该 函数 的 梯度方向 探寻 
如果 梯度 记为 ∇ 则 函数 f x y 的 
梯度 由 下式 表示 这是 机器学习 中最 易 造成 混淆 
的 一个 地方 但在 数学上 并 不难 需要 做 的 
只是 牢记 这些 符号 的 意义 这个 梯度 意味着 要 
沿 x 的 方向 移动 沿 y 的 方向 移动 
其中 函数 f x y 必须 要 在 待 计算 
的 点 上有 定义 并且 可微 一个 具体 的 函数 
例子 见 2 2 梯度 上升 算法 到达 每个 点 
后 都会 重新 估计 移动 的 方向 从 P0 开始 
计算 完 该点 的 梯度 函数 就 根据 梯度 移动 
到下 一点 P1 在 P 1点 梯度 再次 被 重新 
计算 并 沿 新的 梯度方向 移动 到 P2 如此 循环 
迭代 直到 满足 停止 条件 迭代 的 过程 中 梯度 
算子 总是 保证 我们 能 选取 到 最佳 的 移动 
方向 2中 的 梯度 上升 算法 沿 梯度方向 移动 了 
一步 可以 看到 梯度 算子 总是 指向 函数值 增长 最快 
的 方向 这里 所说 的 是 移动 方向 而 未提到 
移 动量 的 大小 该 量值 称为 步长 记 做 
α 用 向量 来 表示 的话 梯度 上升 算法 的 
迭代 公式 如下 该 公式 将 一直 被 迭代 执行 
直至 达到 某个 停止 条件 为止 比如 迭代 次数 达到 
某个 指 定值 或 算法 达到 某个 可以允许 的 误差 
范围 五 训练 算法 使用 梯度 上升 找到 最佳 参数 
3 一个 简单 数据集 下面 将 采用 梯度 上升 法 
找到 Logistic 回归 分类器 在此 数据 集上 的 最佳 回归系数 
3 中有 100个 样本点 每个 点 包含 两个 数值 型 
特征 X1 和 X2 在此 数据 集上 我们 将 通过 
使用 梯度 上升 法 找到 最佳 回归系数 也 就是 拟 
合出 Logistic 回归模型 的 最佳 参数 梯度 上升 法的/nr 伪代码 
如下 每个 回归系数 初始 化为 1 重复 R 次 计算 
整个 数据集 的 梯度 使用 alpha × gradient 更新 回归系数 
的 向量 返回 回归系数 程序清单 5 1 Logistic 回归 梯度 
上升 优化 算法 1 def loadDataSet # 每行 前 两个 
值 分别 是 X1 和 X2 第三个 值 是 数据 
对应 的 类别 标签 此外 为了 方便 计算 该 函数 
还将 X0 的 值 设为 1.0 2 dataMat = labelMat 
= 3 fr = open testSet . txt 4 for 
line in fr . readlines 5 lineArr = line . 
strip . split 6 dataMat . append 1.0 float lineArr 
0 float lineArr 1 7 labelMat . append int lineArr 
2 8 return dataMat labelMat # 分类 用 1 0 
表示 9 10 def sigmoid inX 11 return 1.0 / 
1 + exp inX 12 13 def gradAscent dataMatIn classLabels 
# 梯度 上升 算法 变量 alpha 是 向 目标 移动 
的 步长 maxCycles 是 迭代 次数 14 dataMatrix = mat 
dataMatIn # 数据集 转 矩阵 15 labelMat = mat classLabels 
. transpose # 分类 向量 转置 16 m n = 
shape dataMatrix 17 alpha = 0.001 18 maxCycles = 500 
19 weights = ones n 1 # weights 与 列 
的 数量 相同 20 for k in range maxCycles # 
heavy on matrix operations 21 h = sigmoid dataMatrix * 
weights # 把 Z 代入 到 Sigmoid 函数 当中 22 
error = labelMat h # 向量 的 减法 23 weights 
= weights + alpha * dataMatrix . transpose * error 
# 数学公式 alpha = 移动 步长 24 return weights 接下来 
看看 实际效果 在 Python 提示符 下 敲入 下面 的 代码 
1 import logRegres 2 dataArr labelMat = logRegres . loadDataSet 
3 logRegres . gradAscent dataArr labelMat 4 matrix 4.12414349 0.48007329 
0.6168482 求 出了 数据集 每 一列 的 最佳 拟合 参 
数据 w 的 值 六 分析 数据 画出 决策 边界 
上面 已经 解出 了 一组 回归系数 它 确定 了 不同 
类别 数据 之间 的 分隔线 那么 怎样 画出 该 分隔线 
从而 使得 优化 的 过程 便于 理解 呢 下面 将 
解决 这个 问题 打开 logRegres . py 并 添加 如下 
代码 画出 数据集 和 Logistic 回归 最佳 拟合 直线 的 
函数 1 def plotBestFit weights 2 import matplotlib . pyplot 
as plt 3 dataMat labelMat = loadDataSet 4 dataArr = 
array dataMat 5 n = shape dataArr 0 6 xcord1 
= ycord1 = 7 xcord2 = ycord2 = 8 for 
i in range n 9 if int labelMat i = 
= 1 10 xcord1 . append dataArr i 1 ycord1 
. append dataArr i 2 11 else 12 xcord2 . 
append dataArr i 1 ycord2 . append dataArr i 2 
13 fig = plt . figure 14 ax = fig 
. add _ subplot 111 15 ax . scatter xcord1 
ycord1 s = 30 c = red marker = s 
16 ax . scatter xcord2 ycord2 s = 30 c 
= green 17 x = arange 3.0 3.0 0.1 18 
y = weights 0 weights 1 * x / weights 
2 # ① 最佳 拟合 直线 19 ax . plot 
x y 20 plt . xlabel X1 plt . ylabel 
X2 21 plt . show 程序清单 5 2中 的 代码 
是 直接 用 Matplotlib 画 出来 的 唯一 要 指出 
的 是 ① 处 设置 了 sigmoid 函数 为 0 
回忆 5.2节 0 是 两个 分类 类别 1 和 类别 
0 的 分界 处 因此 我们 设定 0 = w0x0 
+ w1x1 + w2x2 然后 解出 X2 和 X1 的 
关系 式 即 分隔线 的 方程 注意 X0 ＝ 1 
运行 程序清单 5 2 的 代码 在 Python 提示符 下 
输入 1 from numpy import * 2 reload logRegres 3 
module logRegres from logRegres . py 4 logRegres . plotBestFit 
weights . getA 梯度 上升 算法 在 500次 迭代 后 
得到 的 Logistic 回归 最佳 拟合 直线 这个 分类 结果 
相当 不错 从图/nr 上看 只 错 分了 两 到 四个 
点 但是 尽管 例子 简单 且 数据集 很小 这个 方法 
却 需要 大量 的 计算 300次 乘法 因此 下 一节 
将对 该 算法 稍作 改进 从而 使 它 可以 用 
在 真实 数据 集上 七 训练 算法 随机/d 梯度/n 上升/v 
梯度/n 上升/v 算法/n 在/p 每次/r 更新/d 回归系数/n 时都/nr 需要/v 遍历/v 
整个/b 数据集/i 该 方法 在 处理 100个 左右 的 数据 
集 时尚 可 但/c 如果/c 有/v 数十/m 亿/m 样本/n 和/c 
成千上万/l 的/uj 特征/n 那么 该 方法 的 计算 复杂度 就 
太高 了 一种 改进 方法 是 一次 仅用 一个 样本 
点来 更新 回归系数 该 方法 称为 随机 梯度 上升 算法 
由于 可以 在 新 样本 到来 时对/nr 分类器 进行 增量式 
更新 因而 随机 梯度 上升 算法 是 一个 在线 学习 
算法 与 在线 学习 相 对应 一次 处理 所 有 
数据 被 称作 是 批处理 随机 梯度 上升 算法 可以 
写成 如下 的 伪代码 所有 回归系数 初始 化为 1对 数据 
集中 每个 样本 计算 该 样本 的 梯度 使用 alpha 
× gradient 更新 回归系数 值 返回 回归系数 值 程序清单 5 
3 随机 梯度 上升 算法 1 def stocGradAscent0 dataMatrix classLabels 
2 m n = shape dataMatrix 3 alpha = 0.01 
4 weights = ones n # 初始化 所 有数 为 
1 的 向量 5 for i in range m 6 
h = sigmoid sum dataMatrix i * weights # 两向 
理 相乘 h = 一个 数值 7 error = classLabels 
i h # error 这 一个 数值 8 weights = 
weights + alpha * error * dataMatrix i 9 return 
weights 可以 看到 随机 梯度 上升 算法 与 梯度 上升 
算法 在 代码 上 很 相似 但也 有 一些 区别 
第一 后者 的 变量 h 和 误差 error 都是 向量 
而 前者 则 全是 数值 第二 前者 没有 矩阵 的 
转换 过程 所有 变量 的 数据 类型 都是 NumPy 数组 
测试代码 1 dataArr labelMat = logRegres . loadDataSet 2 weights 
= logRegres . stocGradAscent0 array dataArr labelMat 3 weights 4 
array 1.01702007 0.85914348 0.36579921 5 logRegres . plotBestFit weights 执行 
完毕 后将/nr 得到 5 所示 的 最佳 拟合 直 线图 
该图 与 4 有 一些 相似之处 可以 看到 拟合 出来 
的 直线 效果 还 不错 但 并不 像 4 那样 
完美 这里 的 分类器 错分 了 三分之一 的 样本 5 
随机 梯度 上升 算法 在 上述 数据 集上 的 执行 
结果 最佳 拟合 直线 并非 最佳 分类 线 直接 比较 
程序清单 5 3 和 程序清单 5 1 的 代码 结果 
是 不 公平 的 后者 的 结果 是 在整个 数据 
集上 迭代 了 500次 才 得到 的 一个 判断 优化 
算法 优劣 的 可靠 方法 是 看 它 是否 收敛 
也 就是说 参数 是否 达到 了 稳定 值 是否 还 
会 不断 地 变化 对此 我们 在 程序清单 5 3中 
随机 梯度 上升 算 法上 做了 些 修改 使其 在整个 
数据集 上 运行 200次 最终 绘制 的 三个 回归系数 的 
变化 情况 如 6 所示 6 运行 随机 梯度 上升 
算法 在 数据集 的 一次 遍历 中 回归 系数 与 
迭代 次数 的 关系 图 回归系数 经过 大量 迭代 才能 
达到 稳 定值 并且 仍然 有 局部 的 波动 现象 
6 展示 了 随机 梯度 上升 算法 在 200次 迭代 
过程 中 回归 系数 的 变化 情况 其中 的 系数 
2 也 就是 5 中的 X2 只经 过了 50次 迭代 
就 达到 了 稳定 值 但 系数 1 和0则/nr 需要 
更 多次 的 迭代 另外 值得 注意 的 是 在 
大 的 波动 停止 后 还有 一些 小 的 周期性 
波动 不难理解 产生 这种 现象 的 原因 是 存在 一些 
不能 正确 分类 的 样本点 数据 集并 非线性 可分 在 
每次 迭代 时会 引发 系数 的 剧烈 改变 我们 期望 
算法 能避免 来回 波动 从而 收敛 到 某个 值 另外 
收敛 速度 也 需要 加快 对于 6 存在 的 问题 
可以 通过 修改 程序清单 5 3 的 随机 梯度 上升 
算法 来 解决 具体 代码 如下 程序清单 5 4 改进 
的 随机 梯度 上升 算法 1 def stocGradAscent1 dataMatrix classLabels 
numIter = 150 2 m n = shape dataMatrix 3 
weights = ones n # initialize to all ones 4 
for j in range numIter 5 dataIndex = range m 
6 for i in range m 7 alpha = 4 
/ 1.0 + j + i + 0.0001 # apha 
decreases with iteration does not   / / ① alpha 
每次 迭代 时 需要 调整 8 randIndex = int random 
. uniform 0 len dataIndex # go to 0 because 
of the constant / / ②   随机 选取 更新 
9 h = sigmoid sum dataMatrix randIndex * weights 10 
error = classLabels randIndex h 11 weights = weights + 
alpha * error * dataMatrix randIndex 12 del dataIndex randIndex 
13 return weights 程序清单 5 4 与 程序清单 5 3 
类似 但 增加 了 两处 代码 来 进行 改进 第一处 
改 进在 ① 处 一方面 alpha 在 每次 迭代 的 
时候 都会 调整 这会 缓解 6 上 的 数据 波动 
或者 高频 波动 另外 虽然 alpha 会 随着 迭代 次数 
不断 减小 但 永远 不会 减 小到 0 这 是因为 
① 中 还 存在 一个 常数项 必须 这样 做 的 
原因 是 为了 保证 在 多次 迭代 之后 新 数据 
仍然 具有 一定 的 影响 如果 要 处理 的 问题 
是 动态 变化 的 那么 可以 适当 加大 上述 常数项 
来 确保 新的 值 获得 更大 的 回归系数 另一 点 
值得 注意 的 是 在 降低 alpha 的 函数 中 
alpha 每次 减少 1 / j + i 其中 j 
是 迭代 次数 i 是 样本点 的 下标 这样 当 
j max i 时 alpha 就 不是 严格 下降 的 
避免 参数 的 严格 下降 也 常见于 模拟 退火算法 等 
其他 优化 算法 中 程序清单 5 4 第二个 改进 的 
地方 在 ② 处 这里 通过 随机 选 取样 本来 
更新 回归系数 这种 方法 将 减少 周期性 的 波动 如 
6中 的 波动 具体 实现 方法 与 第 3 章 
类似 这种方法 每次 随机 从 列表 中 选出 一个 值 
然后 从 列表 中 删掉 该 值 再 进行 下一次 
迭代 此外 改进 算法 还 增加 了 一个 迭代 次数 
作为 第 3个 参数 如果 该 参数 没有 给定 的话 
算法 将 默认 迭代 150次 如果 给定 那么 算法 将 
按照 新的 参数值 进行 迭代 与 stocGradAscent1 类似 7 显示 
了 每次 迭 代时 各个 回归系数 的 变化 情况 7 
使用 样本 随机 选择 和 alpha 动态 减少 机制 的 
随机 梯度 上升 算法 stocGradAscent1 所 生成 的 系数 收敛 
示意图 该 方法 比 采用 固定 alpha 的 方法 收敛 
速度 更快 比较 7 和 6 可以 看到 两 点 
不同 第一点 是 7中 的 系数 没有 像 6里 那样 
出现 周期性 的 波动 这 归功于 stocGradAscent1 里 的 样本 
随机 选择 机制 第二点 是 7 的 水平 轴比 6 
短 了 很多 这 是 由于 stocGradAscent1 可以 收敛 得 
更快 这次 我们 仅 仅对 数据集 做了 20次 遍历 而 
之前 的 方法 是 500次 下面 看看 在 同一 个 
数据 集上 的 分类 效果 1 weights = logRegres . 
stocGradAscent1 array dataArr labelMat 2 weights 3 array 14.38360334 0.9962485 
1.96508465 4 logRegres . plotBestFit weights 8 使用 改进 的 
随机 梯度 上升 算法 得到 的 系数 程序运行 之后 应该 
能 看到 类似 8 的 结果 图 该 分隔线 达到 
了 与 GradientAscent 差不多 的 效果 但是 所 使用 的 
计算 量 更少 八 示例 从/p 疝气/n 病症/n 预测/vn 病/n 
马的/nr 死亡率/n 本节/t 将/d 使用/v Logistic/w 回归/v 来/v 预测/vn 患有/v 
疝病/i 的/uj 马的/nr 存活/vn 问题/n 这里 的 数据 包含 368个 
样本 和 28个 特征 我 并非 育 马 专家 从 
一些 文献 中 了解到 疝病 是 描述 马 胃肠 痛 
的 术语 然而 这种 病 不一定 源自 马的/nr 胃肠 问题 
其他 问题 也 可能 引发 马 疝病 该 数据 集中 
包含 了 医院 检测 马 疝病 的 一些 指标 有的 
指标 比较 主观 有的 指标 难以 测量 例如 马的/nr 疼痛 
级别 示例 使用 Logistic 回归估计 马 疝病 的 死亡率 1 
收集 数据 给定 数据文件 2 准备 数据 用 Python 解析 
文本文件 并 填充 缺失 值 3 分析 数据 可视化 并 
观察 数据 4 训练 算法 使用 优化 算法 找到 最佳 
的 系数 5 测试 算法 为了 量化 回归 的 效果 
需要 观察 错误率 根据 错误率 决定 是否 回 退到 训练 
阶段 通过 改变 迭代 的 次数 和 步长 等 参数 
来 得到 更好 的 回归系数 6 使用 算法 实现 一个 
简单 的 命令行 程序 来 收集 马的/nr 症状 并 输出 
预测 结果 并非 难事 这可以 做为 留给 读者 的 一道 
习题 另外 需要 说明 的 是 除了 部分 指标 主观 
和 难以 测量 外 该 数据 还 存在 一个 问题 
数据集 中有 30% 的 值 是 缺失 的 下面 将 
首先 介绍 如何 处理 数据 集中 的 数据 缺失 问题 
然后/c 再/d 利用/n Logistic/w 回归/v 和/c 随机/d 梯度/n 上升/v 算法/n 
来/v 预测/vn 病/n 马的/nr 生死/v 1 准备 数据 处理 数据 
中 的 缺失 值 数据 中 的 缺失 值 是个 
非常 棘手 的 问题 有/v 很多/m 文献/n 都/d 致力于/n 解决/v 
这个/r 问题/n 那么 数据 缺失 究竟 带来 了 什么 问题 
假设有 100个 样本 和 20个 特征 这些 数据 都是/nr 机器 
收集 回来 的 若 机器 上 的 某个 传感器 损坏 
导致 一个 特征 无效 时该/nr 怎么办 此时 是否 要 扔掉 
整个 数据 这种 情况 下 另外 19个 特征 怎么办 它们 
是否 还 可用 答案 是 肯定 的 因为 有时候 数据 
相当 昂贵 扔掉 和 重新 获取 都是 不 可取 的 
所以 必须 采用 一些 方法 来 解决 这个 问题 下面 
给出 了 一些 可选 的 做法 • 使用 可用 特征 
的 均值 来 填补 缺失 值 • 使用 特殊 值 
来 填补 缺失 值 如 1 • 忽 略有 缺失 
值 的 样本 • 使用 相似 样本 的 均值 添补 
缺失 值 • 使用 另外 的 机器学习 算法 预测 缺失 
值 现在 我们 对 下 一节 要用 的 数据 集 
进行 预处理 使其 可以 顺利 地 使用 分类 算法 在 
预处理 阶段 需要 做 两件事 第一 所有 的 缺失 值 
必须 用 一个 实数值 来 替换 因为 我们 使用 的 
NumPy 数据类型 不允许 包含 缺失 值 这里 选择 实数 0 
来 替换 所有 缺失 值 恰好/d 能/v 适用/v 于/p Logistic/w 
回归/v 这样 做 的 直觉 在于 我们 需要 的 是 
一个 在 更新 时 不会 影响 系数 的 值 回归系数 
的 更新 公式 如下 weights = weights + alpha * 
error * dataMatrix randIndex 如果 dataMatrix 的 某 特征 对应 
值 为 0 那么 该 特征 的 系数 将 不做 
更新 即     weights = weights 另外 由于 sigmoid 
0 = 0.5 即 它 对 结果 的 预测 不 
具有 任何 倾向性 因此 上述 做法 也 不会 对 误差 
项 造成 任何 影响 基于 上述 原因 将 缺失 值 
用 0 代替 既 可以 保留 现有 数据 也 不 
需要 对 优化 算法 进行 修改 此外 该 数据 集中 
的 特征 取值 一般 不为 0 因此 在 某种 意义 
上 说 它 也 满足 特殊 值 这个 要求 预处理 
中 做 的 第二 件事 是 如果 在 测试 数据集 
中 发现 了 一条 数据 的 类别 标签 已经 缺失 
那么 我们 的 简单 做法 是 将 该条 数据 丢弃 
这 是因为 类别 标签 与 特征 不同 很难 确定 采用 
某个 合适 的 值 来 替换 采用 Logistic 回归 进行 
分类 时 这种 做法 是 合理 的 而 如果 采用 
类似 kNN 的 方法 就 可能 不太 可行 原始 的 
数据 集 经过 预处理 之后 保存 成 两个 文件 horseCol 
icTest . txt 和ho/nr r s e C o l 
i c T r a i n i n g 
. txt 如果/c 想/v 对/p 原始/v 数据/n 和/c 预处理/vn 后的/nr 
数据/n 做/v 个/q 比较/d 可以 在 http / / archive 
. ics . uci . edu / ml / datasets 
/ Horse + Colic 浏览 这些 数据 现在 我们 有 
一个 干净 可用 的 数据 集 和 一个 不错 的 
优化 算法 下面 将 把 这些 部分 融合 在 一起 
训练 出 一个 分类器 然后 利用 该 分类器 来 预测 
病 马的/nr 生死 问题 2 测试 算法 用 Logistic 回归 
进行 分类 本章 前面 几节 介绍 了 优化 算法 但 
目前 为止 还 没有 在 分类上 做 任何 实际 尝试 
使用 Logistic 回归 方法 进行 分类 并不 需要 做 很多 
工作 所需 做 的 只是 把 测试 集上 每个 特征向量 
乘以 最优化 方法 得来 的 回归系数 再 将该 乘积 结果 
求和 最后 输入 到 Sigmoid 函数 中 即可 如果 对应 
的 Sigmoid 值 大于 0.5 就 预测 类别 标签 为 
1 否 则为 0 下面 看看 实际 运行 效果 打开 
文本 编辑器 并将 下列 代码 添加到 logRegres . py 文件 
中 程序清单 5 5 Logistic 回归 分类 函数 1 def 
classifyVector inX weights 2 prob = sigmoid sum inX * 
weights 3 if prob 0.5 return 1.0 4 else return 
0.0 5 6 def colicTest 7 frTrain = open h 
o r s e C o l i c T 
r a i n i n g . txt frTest 
= open horseColicTest . txt 8 trainingSet = trainingLabels = 
9 for line in frTrain . readlines 10 currLine = 
line . strip . split \ t 11 lineArr = 
12 for i in range 21 13 lineArr . append 
float currLine i 14 trainingSet . append lineArr 15 trainingLabels 
. append float currLine 21 16 trainWeights = stocGradAscent1 array 
trainingSet trainingLabels 1000 17 errorCount = 0 numTestVec = 0.0 
18 for line in frTest . readlines 19 numTestVec + 
= 1.0 20 currLine = line . strip . split 
\ t 21 lineArr = 22 for i in range 
21 23 lineArr . append float currLine i 24 if 
int classifyVector array lineArr trainWeights = int currLine 21 25 
errorCount + = 1 26 errorRate = float errorCount / 
numTestVec 27 # print the error rate of this test 
is % f % errorRate 28 return errorRate 29 30 
def multiTest 31 numTests = 10 errorSum = 0.0 32 
for k in range numTests 33 errorSum + = colicTest 
34 # print after % d iterations the average error 
rate is % f % numTests errorSum / float numTests 
35 程序清单 5 5 的 第一 个 函数 是 classifyVector 
它 以 回归系数 和 特征向量 作为 输入 来 计算 对应 
的 Sigmoid 值 如果 Sigmoid 值 大于 0.5 函数 返回 
1 否则 返回 0 接下来 的 函数 是 colicTest 是 
用于 打开 测试 集 和 训练 集 并对 数据 进行 
格式化 处理 的 函数 该 函数 首先 导入 训练 集 
同前 面 一样 数据 的 最后 一列 仍然 是 类别 
标签 数据 最初 有 三个 类别 标签 分别 代表 马的/nr 
三种 情况 仍 存活 已经 死亡 和 已经 安乐死 这里 
为了 方便 将 已经 死亡 和 已经 安乐死 合并 成 
未能 存活 这个 标签 数据 导入 之后 便 可以 使用 
函数 stocGradAscent1 来 计算 回归系数 向量 这里 可以 自由 设定 
迭代 的 次数 例如 在 训练 集上 使用 500次 迭代 
实验 结果 表明 这 比 默认 迭代 150次 的 效果 
更好 在 系数 计算 完成 之后 导入 测试 集并 计算 
分类 错误率 整体 看来 colicTest 具有 完全 独立 的 功能 
多次 运行 得到 的 结果 可能 稍 有 不同 这 
是因为 其中 有 随机 的 成分 在 里面 如果在 stocGradAscent1 
函数 中 回归系数 已经 完全 收敛 那么 结果 才将 是 
确定 的 最后 一个 函数 是 multiTest 其 功能 是 
调用函数 col icTest 10次 并 求 结果 的 平均值 下面 
看一下 实际 的 运行 效果 在 Python 提示符 下 输入 
1 import logRegres 2 logRegres . multiTest 3 4 RuntimeWarning 
overflow encountered in exp 5 the error rate of this 
test is 0.328358 6 the error rate of this test 
is 0.343284 7 the error rate of this test is 
0.432836 8 the error rate of this test is 0.402985 
9 the error rate of this test is 0.343284 10 
the error rate of this test is 0.343284 11 the 
error rate of this test is 0.283582 12 the error 
rate of this test is 0.313433 13 the error rate 
of this test is 0.432836 14 the error rate of 
this test is 0.283582 15 after 10 iterations the average 
error rate is 0.350746 这边 有 一个 警告 是 可能 
溢出 的 警告 从 上面 的 结果 可以 看到 10次 
迭代 之后 的 平均 错误率 为 35% 事实上 这个 结果 
并不 差 因为有 30% 的 数据 缺失 当然 如果 调整 
colicTest 中的 迭代 次数 和 stochGradAscent1 中的 步长 平均 错误率 
可以 降到 20% 左右 第 7 章中 我们 还 会 
再次 使用 到 这个 数据集 