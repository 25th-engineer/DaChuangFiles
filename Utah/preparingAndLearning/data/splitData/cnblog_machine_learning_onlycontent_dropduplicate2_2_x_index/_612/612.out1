本章 导读 机器学习 machine learning ML 是 一门 涉及 概率论 
统计学 逼近 论 凸 分析 算法 复杂度 理论 等 多 
领域 的 交叉 学科 ML 专注 于 研究 计算机 模拟 
或 实现 人类 的 学习 行为 以 获取 新知识 新技能 
并 重组 已 学习 的 知识 结构 使 之 不断 
改善 自身 MLlib 是 Spark 提供 的 可扩展 的 机器学习 
库 MLlib 已经 集成 了 大量 机器 学习 的 算法 
由于 MLlib 涉及 的 算法 众多 笔者 只对 部分 算法 
进行 了 分析 其余 算法 只是 简单 列出 公式 读者 
如果 想 要对 公式 进行 推理 需要 自己 寻找 有关 
概率论 数理统计 数理分析 等 方面 的 专门 著作 本章 更 
侧重于 机器学习 API 的 使用 基本 能够 满足 大多数 读者 
的 需要 1 .   机器学习 概率 机器学习 也 属于 
人工智能 的 范畴 该 领域 主要 研究 的 对象 是 
人工智能 尤其 是 如何 在 经验 学习 中 改善 具体 
算法 机器学习 是 人工智能 研究 较 为 年轻 的 分支 
它 的 发展 过程 大致 可 分为 如下 4个 阶段 
第一阶段 20 世纪 50 年代 中叶 至 60 年代 中叶 
属于 热烈 时期 第二阶段 20 世纪 60 年代 中叶 至 
70 年代 中叶 称为 冷静 时期 第三阶段 20 世纪 70 
年代 中叶 至 80 年代 中叶 称为 复兴 时期 第四阶段 
从 1986年 开始 至今 1   机器 学习 的 组成 
机器 学习 的 基本 结构 由 环境 知识库 和 执行 
部分 三 部分 组成 环境 向 学习 部分 属于 知识库 
的 一部分 提供 某些 信息 学习 部分 利用 这些 信息 
修改 知识库 以 增进 执行 部分 完成 任务 的 效能 
执行 部分 根据 知识库 完成 任务 同时 把 获得 的 
信息 反馈 给 学习 部分 2   学习策略 学习策略 是 
指 机器学习 过程 中 所 采用 的 推理 策略 学习 
系统 一般 由 学习 和 环境 两部分 组成 环境 如 
书本 或 教师 提供 信息 学习 部分 则 实现 信息 
转换 存储 并从 中 获取 有用 的 信息 学习 过程 
中 学生 学习 部分 使用 的 推理 越少 他 对 
教师 环境 的 依赖 就 越大 教师 的 负担 也就 
越重 根据 学生 实现 信息 转换 所需 推理 的 多少 
和 难易 程度 以 从 简单 到 复杂 从少到/nr 多 
的 次序 可以 将 学习策略 分为 以下 6种 基本 类型 
机械学习 rote learning 学习者 不 需要 任何 推理 或 转换 
直接 获取 环境 所 提供 的 信息 属于 此类 的 
如 塞缪尔 的 跳棋 程序 示教 学习 learning from instruction 
学习者 从 环境 获取信息 把 知识 转换成 内部 可 使用 
的 表示 形式 并将 新 知识 和 原有 知识 有机 
地 合为一体 此种 学习策略 需要 学生 有 一定 程度 的 
推理 能力 但 环境 仍 要做 大量 的 工作 典型 
应用 是 FOO 程序 演绎 学习 learning by deduction 学习者 
通过 推理 获取 有用 的 知识 典型 应用 是 宏 
操作 macro operation 学习 类比 学习 learning by analogy 学习者 
根据 两个 不同 领域 源 域 目标 域 中 的 
知识 相似性 通过 类比 从源域/nr 的 知识 推导 出 目标 
域 的 相应 知识 此类 应用 如 卢瑟福 类比 基于 
解释 的 学习 explanation based learning EBL 学习者 根据 教师 
提供 的 目标 概念 和此/nr 概念 的 例子 领域 理论 
及 可操作 准则 首先 给 出 解释 说明 为什么 该 
例子 满足 目标 概念 然后 将 解释 推广 未 目标 
概念 的 一个 满足 可操作 准则 的 充分条件 著名 的 
EBL 系统 由 迪 乔恩 G . DeJong 的 GENESIS 
等 归纳 学习 learning from induction 由 环境 提供 某 
概念 的 一些 实例 或 反例 让 学习 者 通过 
归纳推理 得出 该 概念 的 一般 描述 归纳 学习 是 
最 基本 的 发展 也 较为 成熟 的 学习 方法 
在 人工智能 领域 中 已 得到 广泛 的 研究 和 
应用 学习策略 还 可以 从所/nr 获取 知识 的 表示 形式 
应用 领域 等 维度 分类 3   应用领域 目前 机器学习 
广泛 应用于 数据挖掘 计算机 视觉 自然语言 处理 生物 特征 识别 
搜索引擎 医学 诊断 检测 信用卡 欺诈 证券 市场 分析 DNA 
序列 测序 语音 和 手写识别 战略游戏 和 机器人 等 领域 
2 .   Spark MLlib 总体设计 MLlib machine learning library 
是 Spark 提供 的 可扩展 的 机器学习 库 MLlib 中 
已经 包含 了 一些 通用 的 学习 算法 和 工具 
如 分类 回归 聚 类 协同 过滤 降 维 以及 
底层 的 优化 原语 等 算法 和 工具 MLlib 提供 
的 API 主要 分为 以下 两类 spark . mllib 包中/nr 
提供 的 主要 API spark . ml 包中/nr 提供 的 
构建 机器学习 工作流 的 高层次 的 API 3 .   
数据类型 MLlib 支持 存储 在 一台 机器 上 的 局部 
向量 和 矩阵 以及 由 一个 或 多个 RDD 支持 
的 分布式 矩阵 局部 向量 和 局部 矩阵 是 提供 
公共 接口 的 简单 数据模型 Breeze 和 jblas 提供 了 
底层 的 线性 代数运算 Breeze 提供 了 一组 线性代数 和 
数字 计算 的 库 具体 信息 访问 http / / 
www . scalanlp . org / jblas 提供 了 使用 
Java 开发 的 线性代数 库 具体 信息 访问 http / 
/ jblas . org / 3.1   局部 向量 MLlib 
支持 两种 局部 向量 类型 密集 向量 dense 和 稀疏 
向量 sparse 密集 向量 由 double 类型 的 数组 支持 
而 稀疏 向量 则由 两个 平行 数组 支持 例如 向量 
1.0 0.0 3.0 由 密集 向量 表示 的 格式 为 
1.0 0.0 3.0 由 稀疏 向量 表示 的 格式 为 
3 0 2 1.0 3.0 注意 这里 对 稀疏 向量 
做些 解释 3 是 向量 1.0 0.0 3.0 的 长度 
除去 0 值 外 其他 两个 值 的 索引 和值/nr 
分别 构成 了 数组 0 2 和 数组 1.0 3.0 
有关 向量 的 类 如图所示 Vector 是 所有 局部 向量 
的 基类 Dense Vector 和 SparseVector 都是 Vector 的 具体 
实现 Spark 官方 推荐 使用 Vectors 中 实现 的 工厂 
方法 创建 局部 向量 就像 下面 这样 import org . 
apache . spark . mllib . linalg . { Vector 
Vectors } / / 创建 密集 向量 1.0 0.0 3.0 
val dv Vector = Vectors . dense 1.0 0.0 3.0 
/ / 给 向量 1.0 0.0 3.0 创建 疏 向量 
val svl Vector = Vectors . sparse 3 Array 0 
2 Array 1.0 3.0 / / 通过 指 定非 0 
的 项目 创建 稀疏 向量 1.0 0.0 3.0 val sv2 
Vector = Vectors . sparse 3 Seq 0 1.0 2 
3.0 注意 Scala 默认 会 导入 scala . collection . 
immutable . Vector 所以 必须 显 式 导入 org . 
apache . spark . mllib . linalg . Vector 才能 
使用 MLlib 才能 使用 MLlib 提供 的 Vector 上面 例子 
中 以 数组 为 参数 调用 Vectors 的 sparse 接口 
见 如下 代码 使用 Seq 创建 稀疏 向量 其本质 依然 
是 使用 数组 见 如下 代码 3.2   标记 点 
标记 点 是 将 密集 向量 或者 稀疏 向量 与 
应答 标签 相关联 在 MLlib 中 标记 点 用于 监督 
学习 算法 MLlib 使用 double 类型 存储 标签 所以/c 我们/r 
能在/nr 回归/v 和/c 分类/n 中/f 使用/v 标记/n 点/m 如果 只有 
两种 分类 可以 使用 二分法 一个 标签 要么 是 1.0 
要么 是 0.0 如果 有 很多 分类 标签 应该 从零开始 
0 1 2 . . . . 标记 点 由 
样例 类 LabeledPoint 来 表示 其 使用 方式 如下 import 
org . apache . spark . mllib . linalg . 
Vectors import org . apache . spark . regression . 
LabeledPoint / / 使用 标签 1.0 和 一个 密集 向量 
创建 一个 标记 点 val pos = LabeledPoint 1.0 Vectors 
. dense 1.0 0.0 3.0 / / 使用 标签 0.0 
和 一个 疏 向量 创建 一个 标记 点 val neg 
= LabeledPoint 0.0 Vectors . sparse 3 Array 0 2 
Array 1.0 3.0 用 稀疏 的 训练 数据 做 练习 
是 很 常见 的 好在 MLlib 支持 读取 存储 在 
LIBSVM 格式 中的 训练 例子 LIBSVM 格式 是 一种 每 
一行 表示 一个 标签 稀疏 特征向量 的 文本格式 其 格式 
如下 label index1 value1 index2 value2 . . . LIBSVM 
是 林智 仁 教授 等 开发 设计 的 一个 简单 
易用 和 快速 有效 的 SVM 模式识别 与 回归 的 
软件包 MLlib 已经 提供 了 MLUtils . loadLibSVMFile 方法 读取 
存储 在 LIBSVM 格式 文本文件 中的 训练 数据 见 如下 
代码 import org . apache . spark . mllib . 
regression . LabeledPoint import org . apache . spark . 
mllib . util . MLUtils import org . apache . 
spark . rdd . RDD val examples RDD LabeledPoint = 
MLUtils . loadLibSVMFile sc data / mllib / sample _ 
libsvm _ data . txt 3.3   局部 矩阵 MLlib 
支持 数据 存储 在 单个 double 类型 数组 的 密 
矩阵 先 来看 这样 一个 矩阵 这个 矩阵 是 如何 
存储 的 它 只是 存储 到 一维 数组 1.0 3.0 
5.0 2.0 4.0 6.0 这个 矩阵 的 尺寸 是 3 
* 2 即 3行 2列 有关 局部 矩阵 的 类如 
下图 所示 局部 矩阵 的 基类 是 Matrix 目前 有 
一个 实现 类 DenseMatrix Spark 官方 推荐 使用 Matrices 中 
实现 的 工厂 方法 创建 局部 矩阵 例如 import org 
. apache . spark . mllib . linalg . { 
Matrix Matrices } / / 创建 密 矩阵 1.0 2.0 
3.0 4.0 5.0 6.0 val dm Matrix = Matrices . 
dense 3 2 Array 1.0 3.0 5.0 2.0 4.0 6.0 
3.4   分布式 矩阵 分布式 矩阵 分布式 地 存储 在 
一个 或者 多个 RDD 中 如何 存储 数据 量 很大 
的 分布式 矩阵 最 重要 的 在于 选择 一个 正确 
的 格式 如果 将 分布式 矩阵 转换 为 不同 格式 
可能 需要 全局 的 shuffle 成本 非常 昂贵 有关 分布式 
矩阵 的 类 如图所示 迄今为止 MLlib 已经 实现 了 4种 
类型 的 分布式 矩阵 RowMatrix 最 基本 的 分布式 矩阵 
类型 是 面向 行 且 行 索引 无 意义 的 
分布式 矩阵 RowMatrix 的 行 实际 是 多个 局部 向量 
的 RDD 列 受限于 integer 的 范围 大小 RowMatrix 适用 
于 列数 不大 以便 单个 局部 向量 可以 合理 地 
传递 给 Driver 也能 在 单个 节点 上 存储 和 
操作 的 情况 下面 展示 了 可以 使用 RDD Vector 
实例 来 构建 RowMatrix 的 例子 import org . apache 
. spark . mllib . linalg . Vector import org 
. apache . spark . mllib . linalg . distributed 
. RowMatrix val rows RDD Vector = . . . 
val mat RowMatrix = new RowMatrix rows val m = 
mat . numRows val n = mat . numCols IndexedRowMatrix 
与 RowMatrix 类似 但却 面向 索引 的 分布式 矩阵 IndexedRowMatrix 
常 用于 识别 行 或者 用于 执行 连接 操作 可以 
使用 RDD IndexedRow 实例 创建 IndexedRowMatrix IndexedRow 的 实现 如下 
@ Experimental case class IndexedRow index Long vector Vector 通过 
删除 IndexedRowMatrix 的 行 索引 可以 将 IndexedRowMatrix 转换 为 
RowMatrix 下面 的 例子 演示 了 如何 使用 IndexedRowMatrix import 
org . apache . spark . mllib . linalg . 
distributed . { IndexedRow IndexedRowMatrix RowEntry } val rows RDD 
IndexedRow = . . . val mat IndexedRowMatrix = new 
IndexedRowMatrix rows val m = mat . numRows val n 
= mat . numCols val rowMat RowMatrix = mat . 
toRowMatrix CoordinateMatrix 使用 坐标 列表 COO 格式 存储 的 分布式 
矩阵 支持 CoordinateMatrix 的 RDD 实际 是 i Long j 
Long value Double 这样 的 三元组 i 是 行 索引 
j 是 列 索引 value 是 实际 存储 的 值 
CoordinateMatrix/w 适用/v 于行和/nr 列/v 都/d 很大/a 且/zg 矩阵/n 很/zg 稀疏/a 
的/uj 情况/n 可以 使用 RDD MatrixEntry 实例 创建 CoordinateMatrix MatrixEntry 
的 实现 如下 @ Experimental case class MatrixEntry i Long 
j Long value Double 通过 调用 CoordinateMatrix 的 t o 
I n d e x e d R o w 
M a t r i x 方法 可以 将 CoordinateMatrix 
转换 为 IndexedRowMatrix 下面 的 例子 演示 了 CoordinateMatrix 的 
使用 import org . apache . spark . mllib . 
linalg . distributed . { CoordinateMatrix MatrixEntry } val entries 
RDD MatriEntry = . . . val mat CoordinateMatrix = 
new CoordinateMatrix entries val m = mat . numRows val 
n = mat . numCols val indexedRowMatrix = mat . 
t o I n d e x e d R 
o w M a t r i x BlockMatrix 由 
RDD MatrixBlock 支持 的 分布式 矩阵 MatrixBlock 实际 是 Int 
Int Matrix 这样 的 二元 组 Int Int 是 Block 
的 索引 Matrix 是 记录 块 大小 的 子 矩阵 
BlockMatrix 支持 与 其他 BlockMatrix 的 add 和 multiply 还 
提供 validate 方法 用于 校验 当前 BlockMatrix 是否 恰当 构建 
通过 调用 I n d e x e d R 
o w R o w M a t r i 
x 或者 CoordinateMatrix 的 toBlockMatrix 方法 可以 方便 转换 为 
BlockMatrix toBlockMatrix 方法 创建 的 Block 的 默认 大小 是 
1024 x 1024 可以 使用 toBlockMatrix rowsPerBlock colsPerBlock 方法 改变 
Block 的 大小 下面 的 例子 演示 了 BlockMatrix 的 
使用 import org . apache . spark . mllib . 
linalg . distributed . { BlockMatrix CoordinateMatrix MatrixEntry } val 
entries RDD MatrixEntry = . . . val coordMat CoordinateMatrix 
= new CoordinateMatrix entries val matA BlockMatrix = coordMat . 
toBlockMatrix . cache matA . validate val ata = matA 
. transpose . multiply matA 注意 由于 MLlib 会 缓存 
矩阵 的 大小 所以 支持 分布式 矩阵 的 RDD 必须 
要 有 明确 的 类型 否则 会 导致 出错 4 
.   基础 统计 MLlib 提供 了 很多 统计 方法 
包括 摘要 统计 相关 统计 分层抽样 假设 校验 随机数 生成 
等 这些 都 涉及 统计学 概率论 的 专业 知识 4.1 
  摘要 统计 调用 Statistics 类 的 colStats 方法 可以获得 
RDD Vector 的 列 的 摘要 统计 colStats 方法 返回 
了 M u l t i v a r i 
a t e t a t i s t i 
c a l u m m a r y 对象 
M u l t i v a r i a 
t e t a t i s t i c 
a l u m m a r y 对象 包含 
了 列 的 最大值 最小值 平均值 方差 非零 元素 的 
数量 以及 总数 下面 的 例子 演示 了 如何 使用 
colStats import org . apache . spark . mllib . 
linalg . Vector import org . apache . spark . 
stat . { M u l t i v a 
r i a t e t a t i s 
t i c a l u m m a r 
y Statistics } val observations RDD Vector = . . 
. val summary M u l t i v a 
r i a t e t a t i s 
t i c a l u m m a r 
y = Statistics . colStats observations println summary . mean 
/ / 每个 列 值 组成 的 密集 向量 println 
summary . variance / / 列 向量 方差 println summary 
. numNonzeros / / 每个 列 的 非 零值 个数 
colStats 实际 使用 了 RowMatrix 的 c o m p 
u t e C o l u m n u 
m m a r y t a t i s 
t i c s 方法 见 代码 如下 4.2   
相关 统计 计算 两个 序列 之间 的 相关性 是 统计 
中 通用 的 操作 MLlib 提供 了 计算 多个 序列 
之间 相关 统计 的 灵活性 目前 支持 的 关联 方法 
运用 了 皮尔森 相关系数 Pearson correlation coefficient 和斯/nr 皮尔森 相关 
系统 Spearman s rank correlation coefficient 1 .   皮尔森 
相关系数 皮尔森 相关 系数 也 称为 皮尔森 积 矩 相关系数 
Pearson product moment correlation coefficient 是 一种 线性 相关系数 皮尔森 
相关 系数 是 用来 反映 两个 变量 线性相关 程度 的 
统计量 相关系数 用 r 表示 其中 n 为 样本 量 
xi yi sx sy   分别为 两个 变量 的 观测值 
和 均值 r 描述 的 是 两个 变量 间 线性相关 
强弱 的 程度 r 的 取值 在 1 与 + 
1 之间 若 r 0 表明 两个 变量 是 正相关 
即 一个 变量 的 值 越大 另一个 变量 的 值 
也会 越大 若 r 0 表明 两个 变量 是 负相关 
即 一个 变量 的 值 越大 另一个 变量 的 值 
反而会 越小 r 的 绝对值 越大 表明 相关性 越强 要 
注意 的 是 这里 并 不 存在 因果关系 若 r 
= 0 表明 两个 变量 间 不是 线性相关 但 有可能 
是 其他 方式 的 相关 比如 曲线 方式 2 . 
  斯 皮尔森 秩 相关系数 斯 皮尔森 秩 相关 系数 
也 称为 Spearman 的 p 是由 Charles Spearman 命名 的 
一般用 希腊字母 ps rho 或 rs 表示 Spearman 秩 相关 
系数 是 一种 无 参数 与 分布 无关 的 校验 
方法 用于 度量 变量 之间 联系 的 强弱 在 没有 
重复 数据 的 情况 下 如果 一个 变量 是 另外 
一个 变量 的 严格 单调 函数 则 Spearman 秩 相关系数 
就是 + 1 或 1 称 变量 完全 Spearman 秩 
相关 注意 和 Pearson 完全 相关 的 区别 只有 当 
两 变量 存在 线性关系 时 Pearson 相关系数 才为 + 1 
或 1 Spearman 秩 相关 系数 为 Statistics 提供 了 
计算 序列 之间 相关性 的 方法 默认 情况 下 使用 
皮尔森 相关系数 使用 方法 如下 import org . apache . 
spark . SparkContext import org . apache . spark . 
mllib . linalg . _ import org . apache . 
spark . mllib . stat . Statistics val sc SparkContext 
= . . . val seriesX RDD Double = . 
. . / / a series val seriesY RDD Double 
= . . . //i //i 和/c seriesX/w 必须/d 有/v 
相同/d 的/uj 分区/n 和/c 基数/n val correlation Double = Statistics 
. corr seriesX seriesY pearson val data RDD Vector = 
. . . / / 每个 向量 必须 是 行 
不能 是 列 val correlMatrix Matrix = Statistics . corr 
data pearson Statistics 中 相关性 的 实现 见 代码 如下 
其 实质 是 代理 了 Correlations Correlations 中 相关性 的 
实现 见 代码 如下 4.3   分层抽样 分层抽样 Stratified sampling 
是 先将 总体 按 某种 特征 分为 若干 次级 层 
然后 再从 每 一层 内 进行 独立 取样 组成 一个 
样本 的 统计学 计算方法 为了 对 分层抽样 有更/nr 直观 的 
感受 请看 下面 的 例子 某市 现有 机动车 共 1 
万辆 其中 大巴 车 500辆 小轿车 6000辆 中 拔 车 
1000辆 越野车 2000辆 工程车 500辆 现在 要 了解 这些 车辆 
的 使用 年限 决定 采用 分层抽样 方式 抽取 100个 样本 
按照 车辆 占 比 各类 车辆 的 抽样 数量 分别为 
5 60 10 20 5/m ./i 摘要/v 统计/v 和/c 相关/v 
统计/v 都/d 集成/v Statistics/w 中/f 而 分层抽样 只需要 调用 RDD 
K V 的 sampleByKey 和 sampleByKeyExact 即可 为了 分层抽样 其中 
的 键 可以 被 认为 是 标签 值 是 具体 
的 属性 sampleByKey 方法 采用 掷 硬币 的 方式 来 
决定 是否 将 一个 观测值 作为 采样 因此 需要 一个 
预期 大小 的 样本数据 sampleByKeyExact 则 需要 更多 更 有效 
的 资源 但是 样本数据 的 大小 是 确定 的 sampleByKeyExact 
方法 允许 用户 采用 符合 fk * nk V k 
∈ K 其中 fk 是 键 k 的 函数 nk 
是 RDD K V 中键 为 k 的 K V 
对 K 是 键 的 集合 下例 演示 了 如何 
使用 分层抽样 import org . apache . spark . SparkContext 
import org . apache . spark . SparkContext import org 
. apache . spark . rdd . PairRDDFunctions val sc 
SparkContext = . . . val data = . . 
. / / an RDD K V of any key 
value pairs val fractions Map K . Double = . 
. . / / specify the exact fraction desired from 
each key val exactSample = data . sampleByKeyExact withReplacement = 
false fractions 4.4   假设 校验 假设 校验 hypothesis testing 
  是 数理 统计学 中 根据 一定 假设 条件 由 
样本 推断 总体 的 一种 方法 如果 对 总体 的 
某种 假设 是 真实 的 那么 不 利于 或 不能 
支持 这一 假设 的 事件 A 小 概率 事件 在 
一次 试验 中 几乎 不 可能 发生 要是 在 一次 
试验中 A 竟然 发生 了 就有 理由 怀疑 该 假设 
的 真实性 拒绝 这一 假设 小 概率 原理 可以用 图 
表示 H0 表示 原 假设 H1 表示 备选 假设 常见 
的 假设 校验 有 如下 几种 双边 校验 H0 u 
= u0 H1 u = / u0 右侧 单边 校验 
H0 u = u0 H1 u u0 左侧 单边 校验 
H0 u = u0 H1 u u0 假设 校验 是 
一个 强大 的 工具 无论 结果 是否 偶然 的 都 
可以 决定 结果 是否 具有 统计 特征 MLlib 目前 支持 
皮尔森 卡方 测试 输入 数据 的 类型 决定了 是 做 
卡方 适合度 检测 还是 独立性 检测 卡方 适合度 检测 的 
输入 数据类型 应当 是 向量 而 卡方 独立性 检测 需要 
的 数据 类型 是 矩阵 RDD LabeledPoint 可以 作为 卡方 
检测 的 输入 类型 下列 演示 了 如何 使用 假设 
校验 import org . apache . spark . SparkContext import 
org . apache . spark . mllib . linalg . 
_ import org . apache . spark . mllib . 
regression . LabeledPoint import org . apache . spark . 
mllib . stat . Statistics . _ val sc SparkContext 
= . . . val vec Vector = . . 
. / / 事件 的 频率 组成 的 vector val 
g o o d n e s s O f 
F i t T e s t R e s 
u l t = Statistics . chiSqTest vec println g 
o o d n e s s O f F 
i t T e s t R e s u 
l t val mat Matrix = . . . / 
/ 偶然性 matrix val i n d e p e 
n d e n c e T e s t 
R e s u l t = Statistics . chiSqTest 
mat println i n d e p e n d 
e n c e T e s t R e 
s u l t val obs RDD LabeledPoint = . 
. . / / feature label pairs val f e 
a t u r e T e s t R 
e s u l t s Arra ChiSqTestResult = Statistics 
. chiSqTest obs var i = 1 f e a 
t u r e T e s t R e 
s u l t s . foreach { result = 
println s Column $ i \ n result i + 
= 1 } / / summary of the test4 . 
5   随机数 生成 随机数 可以 看做 随机变量 什么 是 
随机变量 将 一枚 质地 均匀 的 硬币 抛掷 3次 记录 
它 的 结果 有 其中 u 代表 正面 朝上 d 
代表 反面 朝上 整个 集合 Ω 是 抛掷 3次 硬币 
的 样本空间 正面 朝上 的 次数 可能 是 0 1 
2 3 由于 样本空间 Ω 中 的 结果 都是/nr 随机 
发生 的 所以 出现 正面 的 次数 X 是 随机 
的 X 即为 随机变量 如果 抛掷 硬币 直到 出现 正 
面的 抛掷 次数 为 Y 那么 Y 的 取值 可能 
是 0 1 2 3 . . . 如果 随机变量 
的 取值 是 有限 的 比如 X 或者 是 可 
列 的 比如 Y 那么 就 称为 离散 随机变量 刚才 
说 的 抛掷 3次 硬币 的 情况 下 使用 P 
X =   取值 的 方式 表达 每种 取值 的 
概率 我们 不难 得出 如果 样本空间 上 随机 变量 的 
取值 用 x1 x2 x3 . . . . 表示 
那么 存在 满足 p x1 = P X = xi 
和Σ/nr ip xi = 1   的 函数 p 这个 
函数 p 称为 随机变量 X 的 概率 质量 函数 或者 
频率 函数 如果 X 取值 累积 某个 范围 的 值 
那么 其 累积 分布 函数 定义 如下 累积 分布 函数 
满足 同一 样本空间 上 两个 离散 随机变量 X 和Y的/nr 可能 
取值 分别为 x1 x2 x3 . . . 和 y1 
y2 y3 . . . 如果 对 所有 i 和j/nr 
满足 则 X 和Y是/nr 独立 的 将此 定义 推广 到 
两个 以上 离散 随机变量 的 情形 如果 对 所有 i 
j   和 k/nr 满足 则 X Y 和Z是/nr 相互 独立 
的 刚才 所说 的 X Y 的 取值 都是 离散 
的 还有 一种 情况 下 取值 是 连续 的 以 
人 的 寿命 为例 可以 是 任意 的 正 实数值 
与 频率 函数 相对 的 是 密度 函数 ƒ x 
ƒ x 有 这些 性质 ƒ x   ≥ 0 
ƒ 分段 连续 且 ∫ ∞ ∞ ƒ x dx 
= 1 如果 X 是 具有 密度 函数 ƒ 的 
随机变量 那么 对于 任意 的 a b X 落在 区间 
a b 上 的 概率 是 密度 函数 从a到/nr b 
的 下方 面积 随机数 生成 对于 随机 算法 随机 协议 
和 随机 性能 测试 都很 有用 MLlib 支持 均匀分布 标准 
正态分布 泊松分布 等 生成 随机 RDD MLlib 有关 随机数 的 
类 如图所示 以 泊松分布 为例 先 看看 它 的 数学 
定义 参数 为 λ λ 0 的 泊松 频率 函数 
是 当 λ = 0.1 1 5 10时 的 泊松分布 
如图所示 RandomRDDs 提供 了 工厂 方法 创建 RandomRDD 和 RandomVectorRDD 
下面 的 例子 中 生成 了 一个 包含 100 万个 
double 类型 随机数 的 RDD double 其 值 符合标准 正太 
分布 N 0 1 分布 于 10个 分区 然后 将其 
映 射到 N 1 4 import org . apache . 
spark . SparkContext import org . apache . spark . 
mllib . random . RandomRDDs . _ val sc SparkContext 
= . . . val u = normalRDD sc 1000000L 
10 val v = u . map x = 1.0 
+ 2.0 * x 5 . 分类 和 回归 MLlib 
支持 多种多样 的 分析 方法 例如 二元 分类 多元 分类 
和 回归 表 11 1 列出 了 各类 问题 的 
支持 算法 5.1   数学公式 许多 标准 的 机器 学习 
方法 都 可以 配 制成 凸 优化 问题 即/v 找到/v 
一个/m 极小/d 的/uj 凸函数/l ƒ/i 依赖/v 于/p 一个/m d/w 项的/nr 
可变/v 向量/n w/w 形式上 我们 可以 写 为 优化 问题 
minw ε R d ƒ w 其中 所述 目标函数 的 
形式 为 这里 的 向量 xi   ε Rd   
是 训练 数据 1   ≤   i   ≤ 
n   并且 yi   ε     Rd   
是 想要 预测 数据 的 相应 的 标签 如果 L 
w xi yi 能/v 表示/v 为/p  /i w/w τ/i X/w 
和y的/nr 函数/n 我们 就 说 这个 方法 是 线性 的 
几个/m MLlib/w 的/uj 分类/n 和/c 回归/v 算法/n 都/d 属于/v 这/r 
一类/m 并 在 这里 讨论 目标函数 ƒ 有 两个 部分 
控制 该 模型 的 复杂 的 正则化 部分 和 用于 
在 训练 数据 上 测量 模型 的 误差 的 损失 
部分 损失 函数 L w 是 典型 的 基于 w 
的 凸函数 固定 的 正则化 参数 λ   ≥ 0 
定义 了 最小 损失 即 训练 误差 和 最小化 模型 
的 复杂性 即 避免 过度 拟合 这 两个 目标 之间 
的 权衡 1   损失 函数 在 统计学 统计 决策 
理论 和 经济学 中 损失 函数 是 指 一种 将 
一个 事件 在 一个 样本空间 中 的 一个 元素 映射 
到 一个 表达 与其 事件 相关 的 经济 成本 和 
机会 成本 的 实 数上 的 一种 函数 通常 而言 
损失/n 函数/n 由/p 损失/n 项和/nr 正则/n 项/n 组成/v 表 11 
2 列出 了 常用 的 损失 函数 这里 对表 11 
2中 的 一些 内容 做些 说明 Hinge loss 常 用于 
软 间隔 支持 向量 机 的 损失 函数 Logistic loss 
常 用于 逻辑 回归 的 损失 函数 Squared loss 常 
用于 最小二乘 的 损失 函数 Gradient or sub gradient 梯度 
与 次梯度 2   正规化 正规化 的 目的 是 鼓励 
简单 的 模型 并 避免 过度 拟合 MLlib 支持 以下 
正规化 如表 11 3 所示 这里 的 sign w 是由 
向量 w 中所 有项 的 符号 ± 1 组成 的 
向量 平滑 度 L2 正规化 问题 一般 比 L1 正规化 
容易 解决 然而 L1 正规化 能 帮助 促进 稀疏 权重 
导致 更小 更可 解释 的 模型 其中 后者 于 特征选择 
是 有用 的 没有 任何 正规化 特别 是 当 训练 
实例 的 数目 是 小 的 不 建议 训练 模型 
3   优化 线性 方法 使用 凸 优化 来 优化 
目标函数 MLlib 使用 两 种方法 新 元和 L BFGS 来 
描述 优化 部分 目前 大多数 算法 的 API 支持 随机 
梯度 下降 SGD 并 有 一些 支持 L BFGS 5.2 
  线性 回归 线性 回归 是 一类 简单 的 指导 
学习 方法 线性 回归 是 预测 定量 响应 变量 的 
有用 工具 很多/m 统计/v 学习/v 方法/n 都/d 是从/v 线性/n 回归/v 
推广/v 和/c 扩展/v 得到/v 的/uj 所以 我们 有 必要 重点 
理解 它 1 . 简单 线性 回归 简单 线性 回归 
非常简单 只 根据 单一 的 预测 变量 X 预测 定量 
响应 变量 Y 它 假定 X 与 Y 之间 存在 
线性关系 其 数学 关系 如下 ≈ 表示 近似 这种 线性关系 
可以 描述 为 Y 对 X 的 回归 β 0 
和β1/nr 是 两个 未知 的 常量 被 称为 线性 模型 
的 系数 它们 分别 表示 线性 模型 中的 截距 和 
斜率 β 0 和β1/nr 怎么 得到 呢 通过 大量 样本数据 
估算 出 估计值 假如 样本数据 如下 x1 y1 x2 y2 
. . . . x3 y3 此时 问题 转换 为 
在 坐标 中 寻找 一条 与 所有 点 的 距离 
最大程度 接近 的 直线 问题 如 7 所示 使用 最小二乘 
方法 最终 求得 的 估计值 β 0 β 1 实际 
情况 所有 的 样本 或者 真实 数据 不 可能 真的 
都在/nr 一条 直线 上 每个 坐标 都会 有 误差 所以 
可以 表示 为 如下 关系 上式 也 称为 总体 回归直线 
是 对 X 和Y/nr 之间 真实 关系 的 最佳 线性 
近似 2 . 多元 线性 回归 相比 简单 线性 回归 
实践 中 常常 不止 一个 预测 变量 这就 要求 对 
简单 线性 回归 进行 扩展 虽然 可以 给 每个 预测 
变量 单独 建立 一个 简单 线性 回归模型 但 无法 做 
出 单一 的 预测 更好 的 方法 是 扩展 简单 
线性 回归模型 使 它 可以 直接 包含 多个 预测 变量 
一般 情况 下 假设有 p 个 不同 的 预测 变量 
多元 线性 回归模型 为 其中 Xj 代表 第 j 个 
预测 变量 β j 代表 第 j 个 预测 变量 
和 响应 变量 之间 的 关联 5.3   分类 5.2节 
的 线性 回归模型 中 假设 响应 变量 Y 是 定量 
的 但 很多 时候 Y 却是 定性 的 比如 杯子 
的 材质 是 定性 变量 可以 是 玻璃 塑料 或 
不锈钢 等 定性 变量 也叫 分类 变量 预测 定性 响应值 
是 指 对 观测 分类 分类 的 目标 是 划分 
项目 分类 最 常见 的 分类 类型 是 二元 分类 
二元 分类 有 两种 分类 通常 命名 为 正和 负 
如果 有 两个 以上 的 分类 它 被 称为 多元 
分类 MLlib 支持 两种 线性 方法 分类 线性 支持 向量 
机 和 逻辑 回归 线性 支持 向量 机 仅 支持 
二元 分类 而/c 逻辑/n 回归/v 对/p 二元/m 分类/n 和/c 多元/m 
分类/n 都/d 支持/v 对于 这 两种 方法 MLlib 支持 L1 
和 L2 正规化 变体 MLlib 中 使用 RDD LabeledPoint 代表 
训练 数据集 其中 标签 引 从0/nr 开始 如 0 1 
2 . . . 对于 二元 标签 γ 在 MLlib 
中 使用 0 表示 负 使用 + 1 表示 正 
1 . 线性 支持 向量 机 线性 支持 向量 机 
SVM 是 用于 大 规模 分类 任务 的 标准 方法 
正是 在 介绍 损失 函数 时 提到 的 默认 情况 
下 线性 支持 向量 机 使用 L2 正规化 训练 MLlib 
也 支持 选择 L1 正规化 在 这种 情况 下 问题 
就 变成 了 线 性问题 线性 支持 向量 机 算法 
输出 SVM 模型 给定 一个 新的 数据 点 记为 X 
该 模型 基于 w τ x 的 值 做 预测 
默认 情况 下 如果 w τ x   ≥ 0 
则 结果 是 正 的 否 则为 负 下例 展示 
了 如何 加载 样本 数据集 执行 训练 算法 import org 
. apache . spark . mllib . classification . { 
SVMModel SVMWithSGD } import org . apache . spark . 
mllib . evaluation . B i n a r y 
C l a s s i f i c a 
t i o n M e t r i c 
s import org . apache . spark . mllib . 
util . MLUtils / / 加载 LIBSVM 格式 的 训练 
数据 val data = MLUtils . loadLibSVMFile sc data / 
mllib / sample _ libsbvm _ data . txt / 
/ 将 数据 切分 为 训练 数据 60% 和 测试数据 
40% val splits = data . randomSplit Array 0.6 0.4 
seed = 11L val training = splits 0 . cache 
val test = splits 1 / / 运行 训练 算法 
构建 模型 val numIterations = 100 val model = SVMWithSGD 
. train training numIterations / / 在 测试 数据 上 
计算 原始 分数 val scoreAndLabels = test . map { 
point = val score = model . predict point . 
features score point . label } / / 获取 评估 
指标 val metrics = new B i n a r 
y C l a s s i f i c 
a t i o n M e t r i 
c s scoreAndLabels val auROC = metrics . areaUnderROC println 
Area under ROC = + auROC / / 保存 和 
加载 模型 model . save sc myModelPath val sameModel = 
SVMModel . load sc myModelPath SVMWithSGD . train 默认 执行 
L2 正规化 可以 设置 正则化 参数 为 1.0 为 执行 
L1 正规化 配置 及 优化 SVMWithSGD 的 代码 如下 import 
org . apache . spark . mllib . optimization . 
L1Updater val svmAlg = new SVMWithSGD svmAlg . optimizer . 
setNumIterations 200 . setRegParam 0.1 . setUpdater new L1Updater val 
modelL1 = svmAlg . run training 2 . 逻辑 回归 
逻辑 回归 被 广泛 用于 预测 二元 响应 它 正是 
在 介绍 损失 函数 时 提到 的 对于 二元 分类 
问题 该 算法 输出 二 元逻辑 回归模型 给定 一个 新的 
数据 点 记为 X 该 模型 基于 应用逻辑 函数 做 
预测 其中 z =   w τ x 默认 情况 
下 如果 ƒ w τ x 0.5 输出 为 正 
否 则为 负 虽然 不像 线性 支持 向量 机 逻辑 
回归模型 中 ƒ z 的 原始 输出 具有 一 概率 
解释 即 X 是 正 概率 二 元逻辑 回归 可以 
推广 到 多 元逻辑 回归 来 训练 和 预测 多元 
分类 问题 对于 多元 分类 问题 该 算法 将 输出 
一个多 元逻辑 回归模型 其中 包含 K 1个 二 元逻辑 回归模型 
MLlib 实现 了 两种 算法 来 解决 逻辑 回归分析 小批量 
梯度 下降 和L/nr BFGS Spark 官方 推荐 L BFGS 因为 
它 比 小批 梯度 下降 的 收敛 更快 下例 演示 
了 如何 使用 逻辑 回归 / / 运行 训练 算法 
构建 模型 val model = new L o g i 
s t i c R e g r e s 
s i o n W i t h L B 
F G . setNumClasses 10 . run training / / 
在 测试 数据 上 计算 原始 分数 val p r 
e d i c t i o n A n 
d L a b e l s = test . 
map { case LabeledPoint label features = val prediction = 
model . predict features prediction label } / / 获取 
评估 指标 val metrics = new M u l t 
i c l a s s M e t r 
i c s p r e d i c t 
i o n A n d L a b e 
l s val precision = metrics . precision println Precision 
= + precision / / 保存 和 加载 模型 model 
. save sc myModelPath val samleModel = L o g 
i s t i c R e g r e 
s s i o n M o d e l 
. load sc myModelPath 5.4   回归 1 . 线性 
最小二乘 套索 和 岭回归 线性 最小二乘 公式 是 回归 问题 
最 常见 的 公式 在 介绍 损失 函数 时也/nr 提到 
过 它 的 公式 多种多样 的 回归 方法 通过 使用 
不同 的 正规化 类型 都 派生 自 线性 最小二乘 例如 
普通 最小二乘 或 线性 最小二乘 使用 非 正规化 岭回归 使用 
L2 正规化 套索 使用 L1 正规化 对于 所有 这些 模型 
的 损失 和 训练 误差 就是 均方 误差 下面 的 
例子 演示 了 如何 使用 线性 回归 / / 加载 
解析 数据 val data = sc . textFile data / 
mllib / ridge data / lpsa . data val parsedData 
= data . map { line = val parts = 
line . split LabeledPoint parts 0 . toDouble Vectors . 
dense parts 1 . split . map _ . toDouble 
. cache / / 构建 模型 val numIterations = 100 
val model = L i n e a r R 
e g r e s s i o n W 
i t h G D . train parsedData numIterations / 
/ 使用 训练样本 计算 模型 并且 计算 训练 误差 val 
valueAndPreds = parsedData . map { point = val prediction 
= model . predict point . features point . label 
prediction } val MSE = valueAndPreds . map { case 
v p = math . pow v p 2 } 
. mean println training Mean Squared Error = + MSE 
/ / 保存 与 加载 模型 model . save sc 
myModelPath val sameModel = L i n e a r 
R e g r e s s i o n 
M o d e l . load sc myModelPath } 
2 . 流 线性 回归 流式 数据 可以 适用 于 
线上 的 回归模型 每当 有新/nr 数据 到达 时 更新 模型 
的 参数 MLlib 目前 使用 普通 最 小二 乘法 支持 
流 线性 回归 除了 每批 数据 到达 时 模型 更新 
最新 的 数据 外 实际 与 线下 的 执行 是 
类似 的 下面 的 例子 假设 已经 初始化 好了 StreamingContext 
ssc 来 演示 流 线性 回归 val numFeatures = 3 
val model = new t r e a m i 
n g L i n e a r R e 
g r e s s i o n W i 
t h G D . s e t I n 
i t i a l W e i g h 
t s Vectors . zeros numFeatures model . trainOn trainingData 
model . predictOnValues testData . map lp = lp . 
label lp . features . print ssc . start ssc 
. awaitTermination 6 .   决策树 决策树 是 分类 和 
回归 的 机器 学习 任务 中 常用 的 方法 决策树 
广泛 使用 因为 它们 很容易 解释 处理 分类 的 功能 
延伸到 多元 分类 设置 不 需要 缩放 功能 并能 捕捉到 
非线性 和 功能 的 交互 MLlib/w 使用/v 连续/a 和/c 分类/n 
功能/n 支持/v 决策树/n 的/uj 二元/m 和/c 多元/m 的/uj 分类/n 和/c 
回归/v 通过 行 实现 分区 数据 允许 分布式 训练 数以百万计 
的 实例 6.1   基本 算法 决策树 是 一个 贪心 
算法 即在 特性 空间 上 执行 递归 的 二元 分割 
决策树 为 每个 最底部 叶 分区 预测 相同 的 标签 
为了 在 每个 树节点 上 获得 最大 的 信息 每个 
分区 是从 一组 可能 的 划分 中 选择 的 最佳 
分裂 1 . 节点 不 纯度 和 信息 增益 节点 
不 纯度 是 节点 上 标签 的 均匀性 的 量度 
当前 实现 提供 了 两种 分类 不 纯度 测量 的 
方法 基尼 不 纯度 和嫡/nr 和 一种 回归 不 纯度 
测量 的 方法 方差 如表 11 4 所示 信息 增益 
是 父 节点 不 纯度 与 两个 子 节点 不 
纯度 的 加权 总和 之间 的 差 假设 将有 s 
个 分区 大小 为 N 的 数据集 D 划分 为 
两个 数据集 Dleft 和 Dright 那么 信息 增益 为 2 
. 划分 候选人 1   连续 特征 对 于单 机上 
实现 的 小 数据集 给 每个 连续 特征 划分 的 
候选人 在此 特征 上 有 唯一 值 有些 实现 了 
对 特征值 排序 为了 加速 计算 使用 这些 有序 的 
唯一 值 划分 候选人 对于 大 的 分布式 数据集 排序 
是 很 昂贵 的 通过 在 样本数据 分数 上 执行 
位 计算 实现 了 计算 近似 的 划分 候选人 集合 
有序 划分 创建 了 箱 可使用 maxBins 参数 指定 这样 
的 容器 的 最大 数量 2   分类 特征 对于 
有M种/nr 可能 值 的 分类 特征 将 会有 2M 1 
1个 划分 候选人 对于 二元 0/1 分类 和 回归 我们 
可以 通过 平均 标签 排序 的 分类 特征值 减少 划分 
候选人 至 M 1 多得 数据量 例如 对于 一个 有A/nr 
B 和C/nr 三个 分类 的 分类 特征 的 二元 分类 
问题 其 相应 的 标签 1 的 比例 是 0.2 
0.6 和 0.4时 分类 特征 是 有序 的 A C 
B 两个 划分 候选人 分别 是 A | C B 
和A/nr C | B 其中 | 标记 划分 在 多元 
分类 中 共有 2M 1 1种 可能 的 划分 无论 
何时 都 可能 被 使用 当 2M 1 1 比 
参数 maxBins 大 时 我们/r 使用/v 与/p 二元/m 分类/n 和/c 
回归/v 相/v 类似/v 的/uj 方法/n M 种 分类 特征 用不 
纯度 排序 最终 得到 需要 考虑 的 M 1个 划分 
候选人 3 . 停止 规则 递归 树 的 构建 当 
满足 下面 三个 条件 之一 时 会停 在 一个 节点 
节点 的 深度 与 maxBins 相等 没有 划分 候选人 导致 
信息 增益 大于 minInfoGain 没有/v 划分/v 候选人/n 产生/n 的/uj 子/ng 
节点/n 都/d 至少/d 有mi/nr n/w I/w n/w s/w t/w a/w 
n/w c/w e/w s/w P/w e/w r/w N/w o/w d/w 
e/w 个/q 训练/vn 实例/n 6.2   使用 例子 下面 的 
例子 演示 了 使用 基尼 不 纯度 作为 不 纯度 
算法 且 树 深为 5 的 决策树 执行 分类 import 
org . apache . spark . mllib . tree . 
DecisionTree import org . apache . spark . mllib . 
tree . model . D e c i s i 
o n T r e e M o d e 
l import org . apache . spark . mllib . 
util . MLUtils val data = MLUtils . loadLibSVMFile sc 
data / mllib / sample _ libsvm _ data . 
txt val splits = data . randomSplit Array 0.7 0.3 
/ / 训练 决策树 模型 / / 空ca/nr t e 
g o r i c a l F e a 
t u r e s I n f o 说明 
所有 的 特征 是 连续 的 val numClasses = 2 
val c a t e g o r i c 
a l F e a t u r e s 
I n f o = Map Int Int val impurity 
= gini val maxDepth = 5 val maxBins = 32 
/ / trainingData 是 训练 数据 val model = DecisionTree 
. trainClassifier trainingData numClasses c a t e g o 
r i c a l F e a t u 
r e s I n f o impurity maxDepth maxBins 
/ / 在 测试 实例 上 计算 val labelAndPreds = 
testData . map { point = val prediction = model 
. predict point . features point . label prediction } 
val testErr = labelAndPreds . filter r = r . 
_ 1 = r . _ 2 . count . 
toDouble / testData . count println Test Error = + 
testErr println Learned classification tree model \ n + model 
. toDebugString model . save sc myModelPath val sameModel = 
D e c i s i o n T r 
e e M o d e l . load sc 
myModelPath 下面 的 例子 演示 了 使用 方差 作为 不 
纯度 算法 且 树 深为 5 的 决策树 执行 分类 
val c a t e g o r i c 
a l F e a t u r e s 
I n f o = Map Int Int val impurity 
= variance val maxDepth = 5 val maxBins = 32 
val model = DecisionTree . trainRegressor trainingData c a t 
e g o r i c a l F e 
a t u r e s I n f o 
impurity maxDepth maxBins val l a b e l s 
A n d P r e d i c t 
i o n s = testData . map { point 
= val prediction = model . predict point . features 
point . label prediction } val testMSE = l a 
b e l s A n d P r e 
d i c t i o n s . map 
{ case v p = math . pow v p 
2 } . mean println Test Mean Squared Error = 
+ testMSE println Learned regression tree model \ n + 
model . toDebugString model . save sc myModelPath val sameModel 
= D e c i s i o n T 
r e e M o d e l . load 
sc myModelPath 7 .   随机 森林 合奏 是 一个 
创建 由 其他 模型 的 集合 组合而成 的 模型 的 
学习 算法 MLlib 支持 两个 主要 的 合奏 算法 梯度 
提升 决策树 和 随机 森林 它们 都 使用 决策树 作为 
其 基础 模型 梯度 提升 决策树 和 随机 森林 虽然 
都是 决策树 合奏 的 学习 算法 但是 训练 过程 是 
不同 的 关于 合奏 有 以下 几个 权衡 点 GBT 
每次 都要 训练 一颗 树 所以 它们 比 随机 森林 
需要 更长 的 时间 来 训练 随机 森林 可以 平行 
地 训练 多 棵树 另一方面 GBT 往往 比 随机 森林 
更 合理 地 使用 更小 浅 的 树 并且 训练 
小树 会 花费 更少 的 时间 随机 森林 更不 易发生 
过度 拟合 随机 森林 训练 更多 的 树 会 减少 
多半 的 过度 拟合 而 GBT 训练 更多 的 树 
会 增加 过度 拟合 在 统计 语言 中 随机 森林 
通过 使用 更多 的 树木 减少 方差 而 GBT 通过 
使用 更多 的 树木 减少 偏差 随机 森林 可以 更容易 
调整 因为 性 能与 树木 的 数量 是 单调 增加 
的 但 如果 GBT 树木 的 数量 增长 过大 性能 
可能 开始 下降 总之 两种 算法 都是/nr 有效 的 具体 
选择 应 取决于 特定 的 数据集 随机 森林 是 分类 
与 回归 中最 成功 的 机器学习 模型 之一 为了 减少 
过度 拟合 的 风险 随机 森林 将 很多 决策树 结合起来 
和 决策树 相似 随机 森林 处理 分类 的 功能 延伸到 
多元 分类 设置 不 需要 缩放 功能 并能 捕捉到 非线性 
和 功能 的 交互 MLlib/w 使用/v 连续/a 和/c 分类/n 功能/n 
支持/v 随机/d 森林/n 的/uj 二元/m 和/c 多元/m 的/uj 分类/n 和/c 
回归/v 7.1   基本 算法 随机 森林 训练 一个 决策树 
的 集合 所以 训练 可以 并行 该 算法 随机性 注入 
训练 过程 使 每个 决策树 会 有一点 不同 结合 每 
棵树 的 预测 降低 了 预测 的 方差 改进 了 
测试 数据 的 性能 1 . 随机 注入 算法 随机性 
注入 训练 的 过程 包括 1   每次 迭代 对 
原始 数据集 进行 二次 采样 获得 不同 的 训练 集 
即 引导 2   考虑 在 树 的 每个 节点 
上将 特征 的 不同 随机 子集 分割 除了 这些 随机性 
每个 决策树 个体 都以/nr 同样 的 方法 训练 2 . 
预测 对 随机 森林 做 预测 就必须 聚合 它 的 
决策树 集合 的 预测 分类 和 回归 的 聚合 是 
不同 的 分类 采用 多数 表决 每 棵树 的 预测 
作为 对 分类 的 一次 投票 收到 最多 投票 的 
分类 就是 预测 结果 回归 采用 平均值 每 棵树 都 
有一个 预测值 这些 树 的 预测 值 的 平均值 就是 
预测 结果 7.2   使用 例子 下面 例子 演示 了 
使用 随机 森林 执行 分类 / / 训练 随机 森林 
模型 / / 空ca/nr t e g o r i 
c a l F e a t u r e 
s I n f o 说明 所有 特征 是 连续 
的 val numClasses = 2 val c a t e 
g o r i c a l F e a 
t u r e s I n f o = 
Map Int Int val numTrees = 3 / / use 
more practice val f e a t u r e 
u b s e t t r a t e 
g y = auto / / Let the algorithm choose 
. val impurity = gini val maxDepth = 4 val 
maxBins = 32 val model = RandomForest . trainClassifier trainingData 
numClasses c a t e g o r i c 
a l F e a t u r e s 
I n f o numTrees f e a t u 
r e u b s e t t r a 
t e g y impurity maxDepth maxBins val labelAndPreds = 
testData . map { point = val prediction = model 
. predict point . features point . label prediction } 
val testErr = labelAndPreds . filter r = r . 
_ 1 = r . _ 2 . count . 
toDouble / testData . count println Test Error = + 
testErr println Learned classification forest model \ n + model 
. toDebugString model . save sc myModelPath val sameModel = 
R a n d o m F o r e 
s t M o d e l . load sc 
myModelPath 下面 例子 演示 了 使用 随机 森林 执行 回归 
val numClasses = 2 val c a t e g 
o r i c a l F e a t 
u r e s I n f o = Map 
Int Int val numTrees = 3 / / Use more 
in practice . val f e a t u r 
e u b s e t t r a t 
e g y = auto / / Let the algorithm 
choose val impurity = variance val maxDepth = 4 val 
maxBins = 32 val model = RandomForest . trainRegressor trainingData 
c a t e g o r i c a 
l F e a t u r e s I 
n f o numTrees f e a t u r 
e u b s e t t r a t 
e g y impurity maxDepth maxBins val l a b 
e l A n d P r e d i 
c t i o n s = testData . map 
{ point = val prediction = model . predict point 
. features point . label prediction } val testMSE = 
l a b e l A n d P r 
e d i c t i o n s . 
map { case v p = math . pow v 
p 2 } . mean println Test Mean Squared Error 
= + testMSE println Learned regression forest model \ n 
+ model . toDebugString model . save sc myModelPath val 
sameModel = R a n d o m F o 
r e s t M o d e l . 
load sc myModelPath 8 .   梯度 提升 决策树 GBT 
迭代 训练 决策树 以便 最小化 损失 函数 和 决策树 相似 
随机 森林 处理 分类 的 功能 延伸到 多元 分类 设置 
不 需要 缩放 功能 并能 捕捉到 非线性 和 功能 的 
交互 MLlib/w 使用/v 连续/a 和/c 分类/n 功能/n 支持/v 梯度/n 提升/v 
决策树/n 的/uj 二元/m 和/c 多元/m 的/uj 分类/n 和/c 回归/v 8.1 
  基本 算法 GBT 迭代 训练 一个 决策树 的 序列 
在 每次 迭代 中 算法 使用 当前 合奏 来 预测 
每个 训练 实例 的 标签 然后 将 预测 与 真实 
的 标签 进行 比较 数据集 被 重新 贴上标签 将 重点 
放在 预测 不佳 的 训练 实例 上 因此 在下 一 
迭代 中 决策树 将 帮助 纠正 先前 的 错误 重贴 
标签 的 具体 机制 是由 损失 函数 定义 的 随着 
每次 迭代 GBT 进一步 减少 训练 数据 上 的 损失 
函数 表 11 5列 出了 MLlib 中 GBT 支持 的 
损失 函数 请注意 每个 损失 只 适用 于 分类 或 
回归 之一 其中 N 表示 实例 数量 y1 表示 实例 
i 的 标签 xi 表示 实例 i 的 特征 F 
Xi 表示 实例 i 的 模型 预测 标签 8.2   
使用 例子 下例 演示 了 用 LogLoss 作为 损失 函数 
使用 GBT 执行 分类 的 例子 / / 训练 G 
r a d i e n t B o o 
s t e d T r e e 模型 / 
/ 默认 使用 LogLoss val boostingStrategy = BoostingStrategy . defaultParams 
Classification boostingStrategy . numIterations = 3 / / Note Use 
more iterations in practice . boostingStrategy . treeStrategy . numClasses 
= 2 boostingStrategy . treeStrategy . maxDepth = 5 / 
/ 空ca/nr t e g o r i c a 
l F e a t u r e s I 
n f o 说明 所有 特征 是 连续 的 boostingStrategy 
. treeStrategy . c a t e g o r 
i c a l F e a t u r 
e s I n f o = Map Int Int 
val model = G r a d i e n 
t B o o s t e d T r 
e e s . train trainingData boostingStrategy val labelAndPreds = 
testData . map { point = val prediction = model 
. predict point . features point . label prediction } 
val testErr = labelAndPreds . filter r = r . 
_ 1 = r . _ 2 . count . 
toDouble / testData . count println Test Error = + 
testErr println Learned classification GBT model \ n + model 
. toDebugString model . save sc myModelPath val sameModel = 
G r a d i e n t B o 
o s t e d T r e e s 
M o d e l . load sc myModelPath 下例 
演示 了 用 Squared Error   作为 损失 函数 使用 
GBT 执行 回归 的 例子 / / 训练 G r 
a d i e n t B o o s 
t e d T r e e s 模型 / 
/ defaultParams 指定 了 Regression 默认 使用 SquaredError val boostingStrategy 
= BoostingStrategy . defaultParams Regression boostingStrategy . numIterations = 3 
/ / Note Use more iterations in practice . boostingStrategy 
. treeStrategy . maxDepth = 5 / / 空ca/nr t 
e g o r i c a l F e 
a t u r e s I n f o 
说明 所有 特征 是 连续 的 boostingStrategy treeStrategy . c 
a t e g o r i c a l 
F e a t u r e s I n 
f o = Map Int Int val model = G 
r a d i e n t B o o 
s t e d T r e e s . 
train trainingData boostingStrategy val l a b e l s 
A n d P r e d i c t 
i o n s = testData . map { point 
= val prediction = model . predict point . features 
point . label prediction } val testMSE = l a 
b e l s A n d P r e 
d i c t i o n s . map 
{ case v p = math . pow v p 
2 } . mean println Test Mean Squared Error = 
+ testMSE println Learned regression GBT model \ n + 
model . toDebugString model . save sc myModelPath val sameModel 
= G r a d i e n t B 
o o s t e d T r e e 
s M o d e l . load sc myModelPath 
9 .   朴素 贝叶斯 9.1   算法 原理 j 
我们 首先 来 介绍 一些 数学 中 的 理论 然后 
来看 朴素 贝叶斯 条件概率 A 和B/nr 表示 两个 事件 且 
P B   ≠ 0 B 事件 发生 的 概率 
不等于 0 则 给定 事件 B 发生 的 条件 下 
事件 A 发生 的 条件 概率 定义 为 使用 条件概率 
推导 出 乘法 定律 A 和B/nr 表示 两个 事件 且 
P B   ≠ 0 B 事件 发生 的 概率 
不等于 0 那么 将 乘法 定律 扩展 为 全 概率 
定律 事件 B1 B2 . . . Bn 满足 Uni 
= 1 Bi = Ω Bi ∩ Bj = Ø 
i   ≠ j 且 对 所有 的 i P 
Bi 0 那么 对于 任意 的 A 满足 贝叶斯 公式 
事件 事件 A B1 B2 . . . Bn   
其中 Bi 不 相交 Uni = 1   Bi = 
Ω 且 对 所有 的 i P Bi 0 那么 
朴素 贝叶斯 分类 算法 是 一种 基于 每对 特征 之间 
独立性 的 假设 的 简单 的 多元 分类 算法 朴素 
贝叶斯 的 思想 对于 给出 的 待 分 类项 求解 
在 此项 出现 的 条件 下 各个 类别 出现 的 
概率 哪个 最大 就 认为 此 待 分 类项 属于 
哪个 类别 朴素 贝叶斯 分类 的 定义 如下 1   
设 x = { a1 a2 . . . am 
} 为 待 分 类项 每个 ai 为 x 的 
一个 特征 属性 2 类别 集合 C = { y1 
y2 . . . yn } 3   计算 P 
y1 | x P y2 | x . . . 
P yn | x 4   如果 P yk | 
x = max { y1 | x P y2 | 
x . . . P yn | x   } 
则 x   ε   yk 关键 在 第三 步 
1   找到 一个 已知 分类 的 待 分 类项 
集合 这个 集合 叫做 训练样本 集 2   统计 得到 
在 各类 别下 各个 特征 属性 的 条件 概率 估计 
即 P a1 | y1 P a2 | y1 . 
. . P am | y1   P a1 | 
y2 P a2 | y2 . . . P am 
| y2 . . . P a1 | yn P 
a2 | yn . . . P am | yn 
3   如果 各 个 特征 属性 都是 独立 的 
则 根据 贝叶斯 公式 可以 得到 以下 推导 朴素 贝叶斯 
能够 被 非常 有效 的 训练 它 被 单独 传给 
训练 数据 计算 给定 标签 特征 的 条件 概率分布 并 
给出 观察 结果 用于 预测 MLlib 支持 多项 朴素 贝叶斯 
和 伯努利 朴素 贝叶斯 这些 模型 典型 的 应用 是 
文档 分类 在这方面 每个 观察 是 一个 文档 每个 特征 
代表 一个 条件 其 值 是 条件 的 频率 在 
多项 朴素 贝叶斯 中 或 一个 由 零个 或 一个 
指示 该 条件 是否 在 文档 中 找到 在 伯努利 
朴素 贝叶斯 中 特征值 必须 是非 负 的 模型 类型 
选择 使用 可选 的 参数 多项 或 伯努利 多项 作为 
默认 模型 通过 设置 参数 λ 默认 为 1.0 添加剂 
平滑 为 文档 分类 输入 特征向量 通常 是 稀疏 的 
因为 稀疏 向量 能 利用 稀疏 性 的 优势 因为 
训练 数据 只 使用 一次 所以 没有 必要 缓存 它 
9.2   使用 例子 下面 的 例子 演示 了 如何 
使用 多项 朴素 贝叶斯 import org . apache . spark 
. mllib . classification . { NaiveBayes NaiveBayesModel } import 
org . apache . spark . mllib . linalg . 
Vectors import org . apache . spark . mllib . 
regression . LabeledPoint val data = sc . textFile data 
/ mllib / sample _ naive _ bayes _ data 
. txt val parsedData = data . map { line 
= val parts = line . split LabeledPoint parts 0 
. toDouble Vectors . dense parts 1 . split . 
map _ . toDouble } val splits = parsedData . 
randomSplit Array 0.6 0.4 seed = 11L val training = 
splits 0 val test = splits 1 val model = 
NaiveBayes . train training lambda = 1.0 modelType = multinomial 
val p r e d i c t i o 
n A n d L a b e l = 
test . map p = model . predict p . 
features p . label val accuracy = 1.0 * p 
r e d i c t i o n A 
n d L a b e l . filter x 
= x . _ 1 = = x . _ 
2 . count / test . count model . save 
sc myModelPath val sameModel = G r a d i 
e n t B o o s t e d 
T r e e s M o d e l 
. load sc myModelPath 10 .   保 序 回归 
10.1   算法 原理 保 序 回 归属于 回归 算法 
其 定义 为 给定 一个 有限 的 实数 集合 Y 
=     { y1 y2 . . . yn 
}   表示 观测 响应   X =     
{ x1 x2 . . . xn }   表示 
未知 的 响应值 进行 拟合 找到 一个 最小化 函数 并 
使用 x1 ≤ x2 ≤ . . . ≤ xn 
  对 目标 排序 其中 ω i   是 大于 
0 的 权重 最终 的 函数 被 称为 保 序 
回归 并且 它 是 唯一 的 它 可以 看做 是 
排序 限制 下 的 最小二乘 问题 基本上 保 序 回归 
是 拟合 原始数据 点 最佳 的 单调 函数 MLlib 支持 
PAVA 此 算法 使用 一种 办法 来 平行 化 保 
序 回归 保 序 回归 有一个 可选 参数 isotonic 默认值 
是 true 此参数 指定 保 序 回归 是 保 序 
的 单调 增加 还是 不保 序 的 单调 减少 保 
序 回归 的 结果 被 视为 分段 线性函数 因此 预测 
的 规则 是 1   如果 预测 输入 能 准备 
匹配 训练 特征 那么 返回 相关 预测 如果 有 多个 
预测 匹配 训练 特征 那么 会 返回 其中之一 2   
如果 预测 输入 比 所有 的 训练 特征 低 或者 
高 那么 最低 和 最高 的 训练 特征 各自 返回 
如果 有 多个 预测 比 所有 的 训练 特征 低 
或者 高 那么 都会 返回 3   如果 预测 输入 
介于 两个 训练 特征 那么/r 预测/vn 会/v 被/p 视为/v 分段/n 
线性函数/i 和从最/nr 接近/v 的/uj 训练/vn 特征/n 中/f 计算/v 得到/v 的/uj 
插值/n 10.2   使用 例子 下面 的 例子 演示 了 
如何 使用 保 序 回归 import org . apache . 
spark . mllib . regression . { I s o 
t o n i c R e g r e 
s s i o n I s o t o 
n i c R e g r e s s 
i o n M o d e l } / 
/ 省略 数据 加载 及 样本 划分 的 代码 val 
model = new I s o t o n i 
c R e g r e s s i o 
n . setIsotonic true . run training val p r 
e d i c t i o n A n 
d L a b e l = test . map 
{ point = val predictedLabel = model . predict point 
. _ 2 predictedLabel point . _ 1 } val 
meanSquareError = p r e d i c t i 
o n A n d L a b e l 
. map { case p 1 = math . pow 
p 1 2 } . mean println Mean Squared Error 
= + meanSquaredError model . save sc myModelPath val sameModel 
= I s o t o n i c R 
e g r e s s i o n M 
o d e l . load sc myModelPath 11 . 
  协同 过滤 协同 过滤 通常用于 推荐 系统 这些 技术 
旨在 填补 用户 关联矩阵 的 缺失 项 MLlib 支持 基于 
模型 的 协同 过滤 用户/n 和/c 产品/n 可以/c 预测/vn 缺失/v 
项的/nr 潜在/t 因素/n 的/uj 小/a 集合/v 来/v 描述/v MLlib 采用 
交替 最小二乘 算法 来 学习 这些 潜在 的 因素 矩阵 
分解 的 标准 方法 基于 协同 过滤 处理 用户 项 
矩阵 的 条目 是 明确 的 现实 世界 的 用 
例 只能 访问 隐式 反馈 是 更 常见 的 例如 
浏览 点击 购买 喜欢 股份 等 下面 的 例子 演示 
了 如何 使用 协同 过滤 import org . apache . 
spark . mllib . recommendation . ALS import org . 
apache . spark . mllib . recommendation . M a 
t r i x F a c t o r 
i z a t i o n M o d 
e l import org . apache . spark . mllib 
. recommendation . Rating val data = sc . textFile 
data / mllib / als / test . data val 
ratings = data . map _ . split match { 
case Array user item rate = Rating user . toInt 
item . toInt rate . toDouble } / / 使用 
ALS 构建 推荐 模型 val rank = 10 val numIterations 
= 20 val model = ALS . train ratings rank 
numIterations 0.01 / / 模型 计算 val usersProducts = ratings 
. map { case Rating user product rate = user 
product } val predictions = model . predict usersProducts . 
map { case Rating user product rate = user product 
rate } val ratesAndPreds = ratings . map { case 
Rating user product rate = user product rate } . 
join predictions val MSE = ratesAndPres . map { case 
user product r1 r2 = val err = r1 r2 
err * err } . mean println Mean Squared Error 
= + MSE model . save sc myModelPath val sameModel 
= M a t r i x F a c 
t o r i z a t i o n 
M o d e l . load sc myModelPath 12 
.   聚 类 聚类分析 又称 群 分析 它 是 
研究 样品 或 指标 分类 问题 的 一种 统计 分析 
方法 聚类分析 以 相似性 为基础 在 一个 聚 类 中的 
模式 之间 比 不在 同 一聚 类 中的 模式 之间 
具有 更多 的 相似性 MLlib 支持 的 聚 类 算法 
如下 K means 高斯 混合 Gaussian mixture power iteration clustering 
PIC latent   Dirichlet allocation LDA 流式 K means . 
12.1   K meansK means 算法 是 硬 聚 类 
算法 是 典型 的 基于 原型 的 目标 函数 聚 
类 方法 的 代表 它 是 数据 点到 原型 的 
某种 距离 作为 优化 的 目标 函数 利用 函数 求 
极值 的 方法 得到 迭代 运算 的 调整 规则 聚 
类 属于 无 监督 学习 以往 的 回归 朴素 贝叶斯 
SVM 等 都是 有 类别 标签 y 的 也 就是说 
样本 中 已经 给 出了 样本 的 分类 而 聚 
类 的 样本 中 却没有 给定 y 只有 特征 x 
比如 假设 宇宙 中 的 星星 可以 表示 成 三维空间 
中点 集 x y z 聚 类 的 目的 是 
找到 每个 样本 x 潜在 的 类别 y 并 将同 
类别 y 的 样本 x 放在 一起 在 聚 类 
问题 中 训练样本 X =     { x1 x2 
. . . xm } 每个 xi   ε Rn 
K means 算法 是 将 样本 聚 类 成k个/nr 簇 
具体 算法 描述 如下 1   随机 选取 k 个 
聚 类 质心 点 为 µ 1 µ 2 . 
. . µ k   ε Rn 2   重复 
下面 过程 直到 收敛 对 每一个 样本 i 计算 它 
应该 属于 的 类 对于 每一个 类 重新 计算 该类 
的 质心 k 是 我们 事先 给定 的 聚 类 
数 ci 代表 样本 i 与 k 个 类 中 
距离 最近 的 那个 类 ci 的 值 是 1 
到 k 中 的 一个 质心 uj 代表 我们 对 
属于 同 一个 类 的 样本 中心点 的 猜测 拿 
星团 模型 来 解释 就是 要 将 所有 的 星星 
聚 成k个/nr 星团 首先 随机 选取 k 个 宇宙 中的 
点 或者 k 个 星星 作为 k 个 星团 的 
质心 然后 第一步 对于 每一个 星星 计算 其 到 k 
个 质 心中 每一个 的 距离 接着 选取 距离 最近 
的 那个 星团 作为 ci 这样 经过 第一步 每一个 星星 
都 有了 所属 的 星团 第二步 对于 每 一个 星团 
重新 计算 它 的 质心 uj 对 里面 所有 的 
星星 坐标 求 平均 重复/d 迭代/v 第一步/m 和/c 第二/m 步/n 
直到/v 质/ng 心不变/l 或者/c 变化/vn 很小/a 8 演示 了 以上 
过程 下面 的 例子 演示 了 K means 算法 的 
使用 import org . apache . spark . mllib . 
clustering . { KMeans KMeansModel } import org . apache 
. spark . mllib . linalg . Vectors val data 
= sc . textFile data / mllib / kmeans _ 
data . txt val parsedData = data . map s 
= Vectors . dense s . split . map _ 
. toDouble . cache val numClusters = 2 val numIterations 
= 20 val clusters = KMeans . train parsedData numClusters 
numIterations val WSSSE = clusters . computeCost parsedData println Within 
Set Sum of Squared Errors = + WSSSE clusters . 
save sc myModelPath val sameModel = KMeansModel . load sc 
myModelPath 12.2   高斯 混合 K means 的 结果 是 
每个 数 据点 被 分配 到 其中 某一个 cluster 了 
而 高斯 混合 则 给出 这些 数据 点 被 分配 
到 每个 cluster 的 概率 高斯 混合 的 算法 与 
K means 算法 类似 MLlib 中 高斯 混合 的 使用 
例子 import org . apache . spark . mllib . 
clustering . GaussianMixture import org . apache . spark . 
mllib . clustering . G a u s s i 
a n M i x t u r e M 
o d e l import org . apache . spark 
. mllib . linalg . Vectors val data = sc 
. textFile data / mllib / gmm _ data . 
txt val parsedData = data . map s = Vectors 
. dense s . trim . split . map _ 
. toDouble . cache val gmm = new GaussianMixture . 
setK 2 . run parsedData gmm . save sc myGMMModel 
val sameModel = G a u s s i a 
n M i x t u r e M o 
d e l . load sc myGMMModel for i 0 
until gmm . k { println weight = % f 
\ nmu = % s \ nsigma = \ n 
% s \ n format gmm . weights i gmm 
. gaussians i . mu gmm . gaussians i . 
sigma } 12.3   快速 迭代 聚 类 快速 迭代 
聚 类 是 一种 简单 可 扩展 的 图 聚 
类 方法 其 使用 例子 如下 import org . apache 
. spark . mllib . clustering . { P o 
w e r I t e r a t i 
o n C l u s t e r i 
n g P o w e r I t e 
r a t i o n C l u s 
t e r i n g M o d e 
l } import org . apache . spark . mllib 
. linalg . Vectors val similarities RDD Long Long Double 
= . . . val pic = new P o 
w e r I t e r a t i 
o n C l u s t e r i 
n g . setK 3 . setMaxIterations 20 val model 
= pic . run similarities model . assignments . foreach 
{ a = println s $ { a . id 
} $ { a . cluster } } model . 
save sc myModelPath val sameModel = P o w e 
r I t e r a t i o n 
C l u s t e r i n g 
M o d e l . load sc myModelPath 12.4 
  latent Dirichlet allocationlatent Dirichlet allocation LDA 是 一个 三层 
贝叶斯 概率模型 包含 词 主题 和 文档 三层 结构 文档 
到 主题 服从 Dirichlet 分布 主题 到 词 服从 多项式 
分布 LDA 是 一种 非 监督 机器学习 技术 可以 用 
来 识别 大 规模 文档 集 或 语料库 中 潜藏 
的 主题 信息 它 采用 了 词 袋 的 方法 
这种 方法 将 每 一篇 文档 视为 一个 词频 向量 
从而 将 文本 信息 转化 为了 易于 建模 的 数字 
信息 但是 词 袋 方法 没有 考虑 词 与 词 
之间 的 顺序 这 简化 了 问题 的 复杂性 同时 
也 为 模型 的 改进 提供 了 契机 每 一篇 
文档 代表 了 一些 主题 所 构成 的 一个 概率分布 
而 每 一个 主题 又 代表 了 很多 单词 所 
构成 的 一个 概率分布 由于 Dirichlet 分布 随机 向量 各 
分量 间 的 弱 相关性 之所以 还 有点 相关 是 
因为 各 分量 之和 必须 为 1 使得 我们 假想 
的 潜在 主题 之间 也 几乎 是 不相关的 这与 很多 
实际 问题 并不 相符 从而 造成了 LDA 的 又一个 遗留问题 
对于 语料库 中的 每篇 文档 LDA 定义 了 如下 生成 
过程 1   对 每 一篇 文档 从 主题 分布 
中 抽取 一个 主题 2   从 上述 被 抽到 
的 主题 所 对应 的 单词 分布 中 抽取 一个 
单词 3   重复 上述 过程 直至 遍历 文档 中 
的 每一个 单词 下例 演示 了 LDA 的 使用 12.5 
  流式 K means 当 数据流 到达 我们 可能 想要 
动态地 估算 cluster 并 更新 它们 该 算法 采用 了 
小 批量 的 K means 更 新规则 对 每 一批 
数据 将 所有 的 点 分配 到 最近 的 cluster 
并 计算 最新 的 cluster 中心 然后 更新 每个 cluster 
的 公式 为 ci 是 前 一次 计算 得到 的 
cluster 中心 ni 是 已经 分 配到 cluster 的 点数 
xi 是从 当前 批次 得到 的 cluster 的 新 中心 
mi 是 当前 批次 加入 cluster 的 点数 衰减 因子 
a 可被 用于 忽略 过去 的 数据 a = 1时 
所有 数据 都 从一 开始 就 被 使用 a = 
0时 只有 最近 的 数据 将 被 使用 这 类似于 
一个 指数 加权 移动 平均值 下面 的 例子 演示 了 
流失 K means 的 使用 13 .   维数 减缩 
维数 减缩 是 减少 所 考虑 变量 的 数量 的 
过程 维数 减缩 有 两种 方式 奇异 值 分解 主 
成分 分析 13.1   奇异 值 分解 奇异 值 分解 
将 一个 矩阵 因子 分解为 三个 矩阵 U Σ   
和 V/nr 其中 U 是 正交矩阵 其 列 被称为 左 奇异 
向量 Σ 是 对角 矩阵 其 对角线 是非 负 的 
且 以降 序 排列 因此 被 称为 奇异 值 V 
也是 正交矩阵 其 列 被称为 右 奇异 向量 对于 大 
的 矩阵 除了 顶部 奇异 值 和它的/nr 关联 奇异 值 
我们 不 需要 完全 分解 这样 可以 节省 存储 去 
噪声 和 恢复 矩阵 的 低 秩 结构 如果 我们 
保持 前 7个 顶部 奇异 值 那么 最终 的 低 
秩 矩阵 为 假设 n 小于 m 奇异/a 值/n 和右/nr 
奇异/a 值/n 向量/n 来源于/v 特征值/n 和/c Gramian/w 矩阵/n A/w τ/i 
A/w 的/uj 特征向量/n 矩阵 存储 左 奇异 向量 U 通过 
矩阵 乘法 U = A VS 1 计算 使用 的 
实际 方法 基于 计算成本 自动 被 定义 1   如果 
n 100   或者 k n / 2 我们 首先 
计算 Gramian 矩阵 然后再 计算 其 顶部 特征向量 并将 特征向量 
本地化 到 Driver 这 需要 单次 传递 在 每个 Executor 
和 Driver 上 使用 O n2 的 存储 并 花费 
Driver 上 O n2k 的 时间 2   否则 将 
使用 分布式 的 方式 计算 A τ A 的 值 
并且 发送给 ARPACK 计算 顶部 特征向量 这 需要 O k 
次 传递 每个 Executor 上 使用 O n 的 存储 
在 Driver 上 使用 O nk 的 存储 下面 的 
例子 演示 了 SVD 的 使用 13.2   主 成分 
分析 主 成分 分析 是 一种 统计 方法 此 方法 
找到 一个 旋转 使得 第一 坐标 具 有可能 的 最大 
方差 并且 每个 随后 的 坐标 都 具有 可能 的 
最大 方差 旋转 矩阵 的 列 被称为 主 成分 下面 
的 例子 演示 了 使用 RowMatrix 计算 主 成分 14 
.   特征提取 与 转型 14.1   术语 频率 反转 
术语 频率 反转 是 一个 反映 文集 的 文档 中 
的 术语 的 重要性 广泛 应用于 文本 挖掘 的 特征 
矢量化 方法 术语 表示 为 t 文档 表示 为 d 
文集 表示 为 D 术语 频率 TF t d 表示 
术语 t 在 文档 d 中 出现 的 频率 文档 
频率 DF t D 表示 包含 术语 t 的 文档 
数量 如果 我们 仅 使用 术语 频率 来 测量 重要性 
则 很容易 过度 强调 术语 出现 的 很 频繁 而 
携带 的 关于 文档 的 信息 很少 例如 a the 
  和 of 等 如果 术语 非常 频繁 地跨 文集 
出现 这 意味着 它 并 没有 携带 文档 的 特定 
信息 反转 文档 频率 是 一个 术语 提供 了 多少 
信息 的 数值 度量 | D |   表示 文 
集中 的 文档 总数 因为 使用 了 对数 如果 一个 
术语 出现 于 所有 的 文档 中 它 的 IDF 
值 变 0 需要 注意 的 是 应用 一个 平滑 
项 以 避免 被 零 除 TF IDF 方法 基于 
TF 与 IDF 它 的 公式 如下 这里/r 有/v 一些/m 
术语/n 频率/n 和/c 文档/n 频率/n 定义/n 的/uj 变种/n 在 MLlib 
中 为了 使 TF 和 IDF 更 灵活 将 它们 
分开了 MLlib 实现 术语 频率 时 使用 了 哈希 应用 
欧冠 哈希 函数 将 原始 特征 映射 到了 索引 术语 
术语 频率 因此 依赖于 map 的 索引 计算 这种 方法 
避免 了 需要 计算 一个 全局 术语 到 索引 图 
这 对于 大型 语料库 开销 会 很大 但 由于 不同 
的 原始 特征 在 哈希 后 可能 变为 同样 的 
术语 所以 存在 潜在 的 哈希 冲突 为了 降低 碰撞 
的 机会 我们 可以 增加 目标 特征 的 维度 即 
哈希表 中的 桶 数 默认 的 特征 维度 是 220 
= 1048576 . 下面 的 例子 演示 了 HashingTF 的 
使用 14.2   单词 向量 转换 Word2Vec 计算 由 单词 
表示 的 分布式 向量 分布式 表征 的 主要 优点 是 
类似 的 单词 在 矢量空间 是 接近 的 这 使得 
泛化 小说 模式 更 容易 和 模型 估计 更 稳健 
分布式 向量 表示 被 证实 在 许多 自然语言 处理 应用 
中 有用 例如 命名 实体 识别 消 歧 解析 标记 
和 机器 翻译 MLlib 的 Word2Vec 实现 采用 了 skip 
gram 模型 skip gram 的 训练 目标 是 学习 单词 
的 向量 表示 其 善于 在 同一 个 句子 预测 
其 上下文 给 定了 单词 序列 w1 w2 . . 
. w τ skip gram 模型 的 目标 是 最大化 
平均 对数 似 然 公式 如下 其中 k 是 训练 
窗口 的 大小 每个 单词 w 与 两个 向量 uw 
和 vw 关联 并且 由 单独 的 向量 来 表示 
通过 给定 的 单词 wj 正确 预测 wi 的 概率 
是由 softmax 模型 决定 的 softmax 模型 如下 w 表示 
词汇量 由于 计算 logp wi | wj 的 成本 与 
V 成正比 V 很容易 就 达到 百万 以上 所以 softmax 
这种 skip gram 模型 是 很 昂贵 的 为了 加速 
Word2Vec 计算 MLlib 使用 分级 softmax 它 可以 减少 计算 
logp wi | wj   复杂度 到 O log V 
下面 的 例子 演示 了 Word2Vec 的 使用 14.3   
标准 尺度 通过 缩放 到 单位 方差 和/或/nr 通过 在 
训练 集 的 样本 上 使用 列 摘要 统计 溢出 
均值 使 特征 标准化 这是 常见 的 预处理 步骤 例如 
当/t 所有/b 的/uj 特征/n 都/d 有/v 单位/n 方差/n 和/或/nr 零/m 
均值/n 时/n 支持/v 向量/n 机/n 的/uj RBF/w 核/n 或者/c L1/i 
和/c L2/i 正规化/b 线性/n 模型/n 通常/d 能/v 更好/d 地/uv 工作/vn 
标准化 可以 提高 在 优化 过程 中 的 收敛 速度 
并且 还 可以 防止 在 模型 训练 期间 非常大 的 
差异 会对 特征 发挥 过大 的 影响 StandardScaler 的 构造器 
有 两个 参数 withMean 默认 false 用于 缩放 前 求 
均值 这将 建立 一个 密集 的 输出 所以 不能 在 
稀疏 输入 上 正常 工作 并将 引发 异常 withStd 默认 
true 缩放 数据 到 标准单位 误差 以下 例子 演示 了 
StandardScaler 的 使用 14.4   正规化 尺度 正规化 尺度 把 
样本 划分 为 单位 Lp 范式 即 维度 这 是 
一种 常见 的 对 文本 分类 或 集群 化 的 
操作 例如 两个 L2 正规化 TF IDF 向量 的 点积 
是 这些 向量 的 余弦 近似值 设/v 二维/m 空间/n 内/n 
有/v 两个/m 向量/n a/w 和b/nr 它们 的 夹角 为 θ 
0 ≤ θ ≤ π 则 点积 定义 为 以下 
实数 MLlib 提供 Normalizer 支持 正规化 Normalizer 有 以下 构造 
参数 p   正规化 到 Lp 空间 默认 为 2 
下面 的 例子 演示 了 Normalizer 的 使用 14.5   
卡方 特征 选择器 ChiSqSelector 用于 卡方 特征选择 它 运转 在 
具有 分类 特征 的 标签 数据 上 ChiSqSelector 对 基于 
分类 进行 独立 卡方 测试 的 特征 排序 并且 过滤 
选择 最接近 标签 的 顶部 特征 ChiSqSelector 有 以下 构造器 
参数 numTopFeatures 选择器 将要 过滤 选择 的 顶部 特征 数量 
下边 的 例子 演示 了 ChiSqSelector 的 使用 14.6   
Hadamard 积 E l e m e n t w 
i s e P r o d u c t 
采用 逐个 相乘 的 方式 使用 给定 的 权重 与 
每个 输入 向量 相乘 换言之 它 采用 一个 标量 乘法器 
扩展 数据集 的 每 一列 这 表示 Hadamard 积 对 
输入 向量 v 使用 转换 向量 w 最终 生成 一个 
结果 向量 Hadamard 积 可由 以下 公式 表示 E l 
e m e n t w i s e P 
r o d u c t 的 构造器 参数 为 
w 转换 向量 下面 代码 演示 了 E l e 
m e n t w i s e P r 
o d u c t 的 使用 15 .   
频繁 模式 挖掘 分析 大规模 数据集 的 第一 个 步骤 
通常 是 挖掘 频繁 项目 项目 集 亚 序列 或 
其他 子结构 这在 数据挖掘 中 作为 一个 活跃 的 研究 
主题 已 多年 了 其 数学原理 读者 可以 取 维基百科 
了解 MLlib 提供 了 频繁 模式 挖掘 的 并行 实现 
FP growth 算法 FP growth 给定 一个 交易 数据集 FP 
growth 的 第一 步骤 是 计算 项目 的 频率 并 
确定 频繁 项目 FP growth 虽然 与 Apriori 类 算法 
有 相同 的 设计 目的 但是 FP growth 的 第二 
步 使用 后缀 树 FP 树 结构 对 交易 数据编码 
且 不会 显 式 生成 候 选集 生成 候 选集 
通常 开销 很大 第二步 之后 就 可以 从 FP 树 
中 抽取 频繁 项目 集 MLlib 中 实现 了 FP 
growth 的 平行 版本 叫做 PFP PFP 可以 将 FP 
growth 的 工作 分发 到 其他 机器 比 单机 运行 
有 更好 的 扩展性 FPGrowth 有 以下 参数 minSupport 项目 
集 被 确定 为 频繁 的 最小 数量 numPartitions 分发 
任务 的 数量 下面 的 例子 演示 了 FPGrowth 的 
使用 16 .   预言 模型 标记 语言 预言 模型 
标记 是 一种 基于 XML 的 语言 它 能够 定义 
和 共享 应用 程序 之间 的 预测模型 MLlib 支持 将 
模型 导出 为 预言 模型 标记 语言 表 11 6列 
出了 MLlib 模型 导出 为 PMML 的 相应 模型 下面 
的 例子 演示 了 将 KMeamsModel 导出 为 PMML 格式 
17 .   管道 Spark1 . 2 增加 了 一个 
新包 spark . ml 目的 是 提供 一套 高 层次 
的 API 帮助 用户 创建 调试 机器 学习 的 管道 
spark . ml 的 标准化 API 用于 将 多种 机器学习 
算法 组合 到 一个 管道 或 工作流 中 下面 列出 
了 Spark ML API 的 主要 概念 ML   DataSet 
由 Hive table 或者 数据源 的 数据 构成 的 可容纳 
各种 数据 类型 的 DataFrame 作为 数据集 例如 数据集 可以 
由 不同 的 列 分别 存储 文本 特征向量 标签 和 
预测 值 Transformer 是 一种 将 DataFrame 转换 为 另一个 
DataFrame 的 算法 例如 ML 模型 是 一个 将 特征 
RDD 转换 为 预测 值 RDD 的 Transformer Estimator 适用于 
DataFrame 并 生成 一个 Transformer 例如 学习 算法 是 一个 
在 数据 集上 训练 并 生成 一个 模型 的 Estimator 
Pipeline 链接 多个 Transformer 和 Estimator 一起 构成 ML 的 
工作 流 Param 所有 Transformer 和 Estimator 用于 指定 参数 
的 通用 API 17.1   管道 工作 原理 机器学习 中 
运行 一 系列 的 算法 取 处理 数据 或者 从 
数据 学习 的 场景 是 很 常见 的 例如 一个 
简单 的 文本文档 处理 工作流 可能 包含 以下 阶段 1 
  将 文档 文本 切 分成 单词 2   将 
文档 的 单词 转换 为 数字化 的 特征向量 3   
使用 特征向量 和 标签 学习 一个 预测模型 Spark ML 以 
一系列 按序 运行 的 PipelineStage 组成 的 管道 来 表示 
这样 的 工作 流 这 一系列 的 Stage 要么 是 
Transformer 要么 是 Estimator 数据集 通过 管道 中 的 每个 
Stage 都会 被 修改 比如 Transformer 的 transform 方法 将 
在 数据 集上 被 调用 Estimator 的 fit 方法 被 
调用 生成 一个 Transformer 然后 此 Transformer 的 transform 方法 
也 将在 数据 集上 被 调用 9 展示 了 简单 
文本文档 工作流 例子 使用 管道 的 处理 流程 9 说明 
整个 管道 由 3个 Stage 组成 Tokenizer 和 HashingTF 都是 
Transformer L o g i s t i c R 
e g r e s s i o n 是 
Estimator 每个 圆柱体 都 说明 它 本身 是 一个 DataFrame 
整个 处理 流程 如下 1   在 由 原生 文本文档 
构成 的 原始数据 集上 应用 Pipeline . fit 方法 2 
Tokenizer . transform 将 原生 文本文档 切 分为 单词 并向 
数据集 增加 单词 列 3 HashingTF . transform 将 单词 
列 转换 为 特征向量 并向 数据集 增加 向量 列 4 
  因为 L o g i s t i c 
R e g r e s s i o n 
是 Estimator 所以 管道 第一次 调用 L o g i 
s t i c R e g r e s 
s i o n . fit 生成 了 L o 
g i s t i c R e g r 
e s s i o n M o d e 
l 如果 管道 中 还有 更多 的 Stage 将会 传递 
数据集 到 下一个 Stage 之前 在 数据 集上 调用 L 
o g i s t i c R e g 
r e s s i o n M o d 
e l 的 transform 管道 本身 是 一个 Estimator 因此 
调用 Pipeline 的 fit 方法 最后 生成 了 PipelineModel PipelineModel 
也 是 一个 Transformer 这个 PipelineModel 会在 测试 时间 使用 
测试过程 如 10 所示 10 说明 PipelineModel 的 测试 过程 
与 9 的 管道 有 相同 的 Stage 数量 但是 
9 的 管道 中 的 所有 Estimator 在 此时 都 
已经 变为 Transformer 当在 测试数据 集上 调用 PipelineModel 的 transform 
方法 时 数据 在 管道 中 按序 通过 每个 Stage 
的 transform 方法 都会 更新 数据集 并将 数据集 传递 给 
下一个 Stage 刚才 介绍 的 例子 中 管道 是 线性 
的 即 每个 stage 都 使用 由 上一个 stage 生产 
的 数据 只要 数据流 图 构成 了 DAG 它 就 
有可能 不是 线性 的 如果 管道 构成 了 DAG 那么 
这些 Stage 就必须 指定 拓扑 顺序 17.2   管道 API 
介绍 Spark ML 的 Transformer 和 Estimator 指定 参数 具有 
统一 的 API 有 两种 方式 指定 参数 给 实例 
设置 参数 例如 Ir 是 L o g i s 
t i c R e g r e s s 
i o n 的 实例 可以 调用 lr . setMaxIter 
10 使得 调用 lr . fit 时 最多 迭代 10次 
传递 ParamMap 给 fit 或者 transform 方法 通过 这种 方式 
指定 的 参数值 将会 覆盖 所有 由 set 方式 指定 
的 参数值 下面 的 例子 演示 了 Estimator Transformer 和 
Param 的 使用 17.3   交叉 验证 模型 选择 是 
Spark ML 中 很 重要 的 课题 通过 对 整个 
管道 的 调整 而 不是 对 管道 中 的 每个 
元素 的 调整 促成 对 管道 模型 的 选择 当前 
spark . ml 使用 CrossValidator 支持 模型 选择 CrossValidator 本身 
携带 一个 Estimator 一组 ParamMap 以及 一个 Evaluator CrossValidator 开始 
先将 数据集 划分为 多组 每组 都由 训练 数据集 和 测试 
数据集 组成 例如 需要 划分 3组 那么 CrossValidator 将 生成 
三个 数据集 对 训练 测试 每 一对 都 使用 2/3 
的 数据 用于 训练 1/3 的 数据 用于 测试 CrossValidator 
会 迭代 ParamMap 的 集合 对于 每个 ParamMap 它 都会 
训练 给定 的 Estimator 并 使用 给定 的 Evaluator 计算 
ParamMap 将会 产出 最佳 的 计算 模型 对 多个 数据集 
对 求 平均 CrossValidator 最终 使用 这个 最佳 的 ParamMap 
和 整个 数据集 拟合 Estimator 下边 的 例子 演示 了 
CrossValidator 的 使用 使用 ParamGridBuilder 构造 网格 参数 hashingTF . 
numFeatures 有 3个 值 r . regParam 有 2个 值 
这个 网格 将 会有 3 * 2 = 6个 参数设置 
供 CrossValidator 选择 使用 了 2组 数据集 对 那么 一 
共有 3 * 2 * 2 = 12种 不同 的 
模型 被 训练 下面 的 代码 演示 了 交叉 验证 
的 使用 