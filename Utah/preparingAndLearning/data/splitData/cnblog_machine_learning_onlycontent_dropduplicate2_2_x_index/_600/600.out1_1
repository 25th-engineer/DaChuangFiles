本文 由 LeftNotEasy 原创 可以 转载 但 请 保留 出处 
和 此行 如果 有 商业 用途 请 联系 作者 wheeleast 
@ gmail . com 一 . 简单 的 说 贝叶 
斯定理 贝叶 斯定理 用 数学 的 方法 来 解释 生活 
中 大家 都 知道 的 常识 形式 最 简单 的 
定理 往往 是 最好 的 定理 比如说 中心 极限 定理 
这样 的 定理 往往 会 成为 某 一个 领域 的 
理论 基础 机器 学习 的 各种 算法 中 使用 的 
方法 最 常见 的 就是 贝叶 斯定理 贝叶 斯定理 的 
发现 过程 我 没有 找到 相应 的 资料 不过 我 
相信 托马斯 . 贝叶斯 1702 1761 是 通过 生活 中 
的 一些 小 问题 去 发现 这个 对 后世 影响 
深远 的 定理 的 而且 我 相信 贝叶斯 发现 这个 
定理 的 时候 还 不 知道 它 居然 有 这么 
大 的 威力 呢 下面 我 用 一个 小 例子 
来 推出 贝叶 斯定理 已知 有N个/nr 苹果 和M个/nr 梨子 苹果 
为 黄色 的 概率 为 20% 梨子 为 黄色 的 
概率 为 80% 问 假如 我 在 这堆 水果 中 
观察 到 了 一个 黄色 的 水果 问 这个 水果 
是 梨子 的 概率 是 多少 用 数学 的 语言 
来 表达 就是 已知 P apple = N / N 
+ M P pear = M / N + M 
P yellow | apple = 20% P yellow | pear 
= 80% 求 P pear | yellow . 要 想得到 
这个 答案 我们 需要 1 . 要求 出 全部 水果 
中 为 黄色 的 水果 数目 2 . 求出 黄色 
的 梨子 数目 对于 1 我们 可以 得到 P yellow 
* N + M P yellow = p apple * 
P yellow | apple + P pear * p yellow 
| pear 对于 2 我们 可以 得到 P yellow | 
pear * M2 / 1 可得 P pear | yellow 
= P yellow | pear * p pear / P 
apple * P yellow | apple + P pear * 
P yellow | pear 化简 可得 P pear | yellow 
= P yellow pear / P yellow 用 简单 的话 
来 表示 就是 在 已知 是 黄色 的 能 推出 
是 梨子 的 概率 P pear | yellow 是 黄色 
的 梨子 占 全部 水果 的 概率 P yellow pear 
除 上 水果 颜色 是 黄色 的 概率 P yellow 
. 这个 公式 很简单 吧 我们 将 梨 子代 换为 
A 黄色 代 换为 B 公式 可以 写成 P A 
| B = P A B / P B 可得 
P A B = P A | B * P 
B . 贝叶斯 公式 就这样 推 出来 了 本文 的 
一个 大概 的 思路 先讲 一 讲 我 概括 出 
的 一个 基本 的 贝叶斯 学习 框架 然后 再举 几个 
简单 的 例子 说明 这些 框架 最后 再 举出 一个 
复杂 一点 的 例子 也都 是以 贝叶斯 机器学习 框架 中的 
模块 来 讲解 二 . 贝叶斯 机器学习 框架 对于 贝叶斯 
学习 我 每本书 都有 每 本书 的 观点 和 讲解 
的 方式 方法 有些 讲得 很 生动 有些 讲 得很 
突兀 对于 贝叶斯 学习 里面 到底 由 几个 模块 组成 
的 我 一直 没有 看到 很 官方 的 说法 我 
觉得 要 理解 贝叶斯 学习 下面 几个 模块 是 必须 
的 1 贝叶斯 公式 机器学习 问题 中 有 一大 类 
是 分类 问题 就是 在 给定 观测 数据 D 的 
情况 下 求出 其 属于 类别 也 可以 称为 是 
假设 h h ∈ { h0 h1 h2 } 的 
概率 是 多少 也 就是 求出 P h | D 
可得 P h D = P h | D * 
P D = P D | h * P h 
所以 P h | D = P D | h 
* P h / P D 对于 一个 数据集 下面 
的 所有 数据 P D 恒定 不变 所以 可以 认为 
P D 为 常数 得到 P h | D ∝ 
P D | h * P h 我们 往往 不用 
知道 P h | D 的 具体 的 值 而是 
知道 例如 P h1 | D P h2 | D 
值 的 大小 关系 就是 了 这个 公式 就是 机器学习 
中的 贝叶斯 公 式 一般来说 我们 称 P h | 
D 为 模型 的 后验/nr 概率 就是 从 数据 来 
得到 假设 的 概率 P h 称为 先验概率 就是 假设 
空间 里面 的 概率 P D | h 是 模型 
的 likelihood 概率 Likelihood 似 然 这个 概率 比较 容易 
让人 迷惑 可以 认为 是 已知 假设 的 情况 下 
求出 从 假设 推出 数据 的 概率 在 实际 的 
机器 学习 过程 中 往往 加入 了 很多 的 假设 
比如 一个 英文 翻译 法文 的 问题 给 出 一个 
英文 句子 问 哪一个 法文 句子 是 最 靠谱 的 
P f = 法文 句子 | e = 英文 句子 
= P e | f * p f p e 
| f 就是 likelihood 函数 P e | f 写成 
下面 的 更 清晰 一点 p e | f ∈ 
{ f1 f2 } 可以 认为 从 输入 的 英文 
句子 e 推出 了 很多 种 不同 的 法文 句子 
f p e | f 就是 从 这些 法文 句子 
中 的 某一个 推出 原 句子 e 的 概率 本文 
之后 的 内容 也 将对 文章 中 没有 提到 的 
一些 内容 也是 贝叶斯 学习 中 容易 疑惑 忽略 但是 
很 重要 的 问题 进行 一些 解释 2 先验 分布 
估计 likelihood 函数 选择 贝叶斯 方法 中 等号 右边 有 
两个 部分 先验概率 与 likelihood 函数 先验概率 是 得到 在 
假设 空间 中 某一个 假设 出现 的 概率 是 多少 
比如说 在 街上 看到 一个 动物 是 长 有毛 的 
问 1 . 这个 动物 是 哈巴狗 的 概率 是 
多少 2 . 这个 动物 是 爪哇虎 的 概率 是 
多少 见 下图 虽然/c 两个/m 假设/vn 的/uj likelihood/w 函数/n 都/d 
非常/d 的/uj 接近/v 于1/nr 除非 这个 动物 病了 但是 由于 
爪哇虎 已经 灭绝 了 所以 爪哇虎 的 先验概率 为 0 
所以 P 爪哇虎 | 有毛 的 动物 的 概率 也为 
0 先验 概率分布 估计 在 观测 的 时候 对于 变量 
是 连续 的 情况 下 往往 需要 一个 先验 分布 
来 得到 稀疏 数据 集中 没有 出现 过 的 给出 
的 某一个 假设 在 假设 空间 中 的 概率 比如说 
有 一个 很大 很大 的 均匀 金属 圆盘 问 这个 
金属 圆盘 抛到 空中 掉下来 正面 朝上 的 概率 这个 
实验 的 成本 比较 高 金属 圆盘 又大 又 重 
所以 只能 进行 有限 次数 的 实验 可能 出现 的 
是 正面 向上 4次 反面 向上 1次 但是 我们 如果 
完全 根据 这个 数据集 去 计算 先验概率 可能 会 出现 
很大 的 偏差 不过 由于 我们 已知 圆盘 是 均匀 
的 我们 可以 根据 这个 知识 假设 P X = 
正面 = 0.5 我们 有的 时候 已 知了 分布 的 
类型 但是 不 知道 分布 的 参数 还 需要 根据 
输入 的 数据 对 分布 的 参数 进行 估计 甚至 
对 分布 还 需要 进行 一些 修正 以 满足 我们 
算法 的 需求 比如说 我们 已知 某一个 变量 x 的 
分布 是 在 某一个 连续 区间 均匀分布 我们 观察 了 
1000次 该 变量 从小到大 排序 结果 是 1 1.12 1.5 
199.6 200 那 我们 是否 就 可以 估计 变量 的 
分布 是从 1 200 均匀分布 的 如果 出现 一个 变量 
是 0.995 那 我们 就 能说 P 0.995 = 0 
如果 出现 一个 200.15 怎么办 呢 所以 我们 这个 时候 
可能 需要 对 概率 的 分布 进行 一定 的 调整 
可能 在 x 1 x 200 的 范围内 的 概率 
是 一个 下降 的 直线 整个 概率密度函数 可能 是 一个 
梯形 的 或者 对 区域 外 的 值 可以 给 
一个 很小 很小 的 概率 这个 我 在 之后 还 
将会 举出 一些 例子 来 说明 Likelihood 函数 选择 对于 
同 一个 模型 likelihood 函数 可能 有 不同 的 选择 
对于 这些 选择 可能 有些 比较 精确 但是 会 搜索 
非常大 的 空间 可能 有些 比较 粗糙 但是 速度 会 
比较 快 我们 需要 选择 不同 的 likelihood 函数 来 
计算 后验/nr 概率 对于 这些 Likelihood 函数 可能 还 需要 
加上 一些 平滑 等 技巧 来 使得 最大 的 降低 
数据 中 噪声 或者 假设 的 缺陷 对 结果 的 
影响 我 所 理解 的 用 贝叶斯 的 方法 来 
估计 给定 数据 的 假设 的 后验/nr 概率 就是 通过 
prior * likelihood 变换 到 后验/nr 分布 是 一个 分布 
变换 的 过程 3 loss function 损失 函数 x 是 
输入 的 数据 y x 是 推 测出 的 结果 
的 模型 t 是 x 对应 的 真实 结果 L 
t y x 就是 loss function E L 表示 使用 
模型 y 进行 预测 使用 L 作为 损失 函数 的 
情况 下 模型 的 损失 时 多少 通常 来说 衡量 
一个 模型 是否 能够 准确 的 得到 结果 损失 函数 
是 最 有效 的 一个 办法 最 常用 最 简单 
的 一种 损失 函数 是 不过 我 一直 不 知道 
为什么 这里 用 的 平方 而 不是 直接 用 绝对值 
有 详细 一点 的 解释 吗 p4 Model Selection 模型 
选择 前文 说到 了 对于 likelihood 函数 可以 有 不同 
的 选择 对于 先验 的 概率 也 可以 有 不同 
的 选择 不过 假设 我们 一个 构造 完整 的 测试 
集 和 一个 恰当 的 损失 函数 最终 的 结果 
将会 是 确定 的 量化 的 我们 很 容易 得 
到 两个 不 同参数 方法 的 模型 的 优劣 性 
不过 通常 情况下 我们 的 测试 集 是 不够 完整 
我们 的 损失 函数 也是 不那么 的 精确 所以 对于 
在 这个 测试 集上 表现 得 非常 完美 的 模型 
我们 常常 可能 还 需要 打 一个 问号 是否 是 
训练 集 和 测试 集 过于 相像 模型 又 过于 
复杂 导致 了 over fitting 后文 将会 详细 介绍 over 
fitting 的 产生 Model Selection 本质 上 来说 是 对模型 
的 复杂度 与 模型 的 准确性 做 一个 平衡 本文 
后面 将 有 一些 类似 的 例子 Example 1 Sequential 
概率 估计 注 此 例子 来自 PRML chapter 2 . 
1.1 对于 概率密度 的 估计 有 很多 的 方法 其中 
一种 方法 叫做 Sequential 概率 估计 这种 方法 是 一个 
增量 的 学习 过程 在 每 看到 一个 样本 的 
时候 都是 把 之前 观测 的 数据 作为 先验概率 然后 
在 得到 新 数据 的 后验/nr 概率 后 再把 当前 
的 后验/nr 概率 作为 下 一次 预测 时候 的 先验概率 
传统 的 二项式 分布 是 由于 传统 的 二项式 分布 
的 概率 μ 是 完全 根据 先验概率 而 得到 的 
而 这个 先验 分布 之前 也 提到 过 可能 会 
由于 实验 次数 不够 而 有 很大 的 偏差 而且 
我们 无法 得知 μ 的 分布 只 知道 一个 μ 
的 期望 这样 对于 某些 机器 学习 的 方法 是 
不利 的 为了 减少 先验 分布 对 μ 的 影响 
获取 μ 的 分布 我们 加入 了 两个 参数 a 
b 表示 X = 0 与 X = 1 的 
出现 的 次数 这个 取值 将会 改变 μ 的 分布 
beta 分布 的 公式 如下 对于 不同 a b 的 
取值 将 会对 μ 的 概率密度函数 产生 下面 的 影响 
图片 来自 PRML 在 观测 数据 的 过程 中 我们 
可以 随时 的 利用 观测 数据 的 结果 改变 当前 
μ 的 先验 分布 我们 可以 将 Beta 分布 加入 
两个 参数 m l 表示 观测 到 的 X = 
0 X = 1 的 次数 之前 的 a b 
是 一个 先验 的 次数 不是 当前 观测 到 的 
我们 令 a b 表示 加入 了 观测 结果 的 
新的 a b 带入 原式 可以/c 得到/v 我们/r 可以/c 利用/n 
观测/vn 后的μ/nr 后验/nr 概率/n 更新/d μ/i 的/uj 先验概率/l 以 进行 
下一次 的 观测 这样 对 不时 能够 得到 新的 数据 
并且 需要 real time 给出 结果 的 情况 下 很 
有用 不过 Sequential 方法 有对/nr 数据 一个 i . i 
. d 独立 同 分布 的 假设 要求 每次 处理 
的 数据 都是/nr 独立 同 分布 的 Example 2 拼 
写检查 这篇文章 的 中心思想 来自 怎样 写 一个 拼写 检查 
器 如果 有 必要 请 参见 原文 本 例子 主要 
谈谈 先验 分布 对 结果 的 影响 直接 给出 拼写 
检查 器 的 贝叶斯 公式 P c | w 表示 
单词 w wrong 正确 的 拼写 为 单词 c correct 
的 概率 P w | c 表示 likelihood 函数 在 
这里 我们 就 简单 的 认 为 两个 单词 的 
编辑 距离 就 是 它们 之间 的 likelihood P c 
表示 单词 c 在 整体 文档 集合 中的 概率 也 
就是 单词 c 的 先验概率 我们 在 做 单词 拼写 
检查 的 时候 肯定 会 直观 的 考虑 如果 用户 
输入 的 单词 如果 在 字典 中 没有 出现 过 
则 应该 将其 修正 为 一个 字典 中 出现 了 
的 而且 与 用户 输入 最 接近 的 词 如果 
用户 输入 的 词 在 字典 中 出现 过了 但是 
词频 非常 的 小 则 我们 可以 为 用户 推荐 
一个 比较 接近 这个 单词 但是 词频 比 较高 的 
词 先验概率 P c 的 统计 是 一个 很 重要 
的 内容 一般来说 有 两种 可行 的 办法 一种 是 
利用 某些 比较 权威 的 词频 字典 一种 是 在 
自己 的 语料库 也 就是 待 进行 拼写 检查 的 
语料 中 进行 统计 我 建议 是 用 后面 的 
方法 进行 统计 这样 词 的 先验概率 才会 与 测试 
的 环境 比较 匹配 比如说 一个 游戏 垂直搜索 网站 需要 
对 用户 输入 的 信息 进行 拼写 纠正 那么 使用 
通用 环境 下 统计 出 的 先验概率 就不 太 适用 
了 Example 3 奥卡姆 剃刀 与 Model Selection 给出 下面 
的 一个 图 来自 Mackey 的 书 问 大树 背后 
有 多少 个 箱子 其实 答案 肯定 是 有 很多 
的 一个 两个 乃至 N 箱子 都是 有可能 的 比如说 
后面 有一/nr 连排 的 箱子 排成 一条 直线 我们 只能 
看到 第一 个 但是 最 正确 也是 最 合理 的 
解释 就是 一个 箱子 因为 如果 大树 背后 有 两个 
乃至 多个 箱子 为什么 从 大树 正面 看 起来 两边 
的 高度 一样 颜色 也 一样 这样 是不是 太 巧合 
了 如果 我们 的 模型 根据 这 张 图片 告诉 
我们 大树 背后 最 有可能 有 两个 箱子 这样 的 
模型 的 泛化 能力 是不是 太差 了 所以 说 本质 
上 来说 奥卡姆 剃刀 或者 模型 选择 也是 人 生活 
中 的 一种 通常 行为 的 数学 表示 是 一种 
化繁为简 的 过程 数学 之美 番外篇 平凡 而又 神奇 的 
贝叶斯 方法 这篇文章 中说 的 奥卡姆 剃刀 工作 在 likelihood 
上 对于 模型 的 先验 分布 并 没有 什么 影响 
我 这里 不 太 同意 这个 说法 奥卡姆 剃刀 是 
剪掉 了 复杂 的 模型 复杂 的 模型 也是 不 
常见 的 先验概率 比 较低 的 最终 的 结果 是 
选择 了 先验概率 比 较高 的 模型 Example 4 曲线拟合 
该 例子 来自 PRML 问题 给定 一些 列 的 点 
x = { x1 x2 . . . xn } 
t = { t1 t2 . . tn } 要求 
用 一个 模型 去 拟合 这个 观测 能够 使得 给定 
一个 新 点 x 能够 给 出 一个 t . 
已知 给定 的 点 是由 y = 2 π x 
加上 正态分布 的 噪声 而 得到 的 10个 点 如上图 
为了 简单 起见 我们 用 一个 多项式 去 拟合 这条 
曲线 为了 验证 我们 的 公式 是否 正确 我们 加入 
了 一个 loss function 在 loss function 最小 的 情况 
下 我们 绘制 了 不同 维度 下 多项式 生成 的 
曲线 在 M 值 增高 的 情况 下 曲线 变得 
越来越 陡峭 当 M = 9 的 时候 该 曲线 
除了 可以 拟合 输入 样本点 外 对 新 进来 的 
样本 点 已经 无法 预测 了 我们 可以 观测 一下 
多项式 的 系数 可以 看出 当 M 维度 增加 的 
时候 系数 也 膨胀 得 很厉害 为了 消除 这个 系数 
带来 的 影响 我们 需要 简化 模型 我们 为 loss 
function 加入 一个 惩罚 因子 我们 把 w 的 L2 
距离 乘 上一个 系数 λ 加入 新的 loss function 中 
这 就是 一个 奥卡姆 剃刀 把 原本 复杂 的 系数 
变为 简单 的 系数 如果 要 更 具体 的 量化 
的 分析 请见 PRML 1.1节 如果 我们 要 考虑 如何 
选择 最 合适 的 维度 我们 也 可以 把 维度 
作为 一个 loss function 的 一部分 这 就是 Model Selection 
的 一种 但是 这个 问题 还 没有 解决 得 很好 
目前 我们 得到 的 模型 只能 预测 出 一个 准确 
的 值 输入 一个 新的 x 给 出 一个 t 
但是 不能 描述 t 有 什么样 的 概率密度函数 概率密度函数 是 
很 有用 的 假如 说 我们 的 任务 修正 为 
给出 N 个 集合 每个 集合 里面 有 若干 个 
点 表示 一条 曲线 给 出 一个 新的 点 问 
这个 新的 点 最 可能 属于 哪 一条 曲线 如果 
我们 仅仅 用 新的 点到 这些 曲线 的 距离 作为 
一个 衡量标准 那/r 很难/i 得到/v 一个/m 比较/d 有/v 说服力/l 的/uj 
结果/n 为了 能够 获取 t 值 的 一个 分布 我们 
不妨 假设 t 属于 一个 均值 为 y x 方差 
为 1 / β 的 一个 高斯分布 在 之前 的 
E w 我们 加入 了 一个 w 的 L2 距离 
这个 看 起来 有 一点 突兀 的 感觉 为什么 要 
加上 一个 这样 的 距离 呢 为什么 不是 加入 一个 
其他 的 东西 我们 可以 用 一个 贝叶斯 的 方法 
去 替代 它 得到 一个 更 有 说服力 的 结果 
我们 令 p w 为 一个 以 0 为 均值 
α 为 方差 的 高斯分布 这个 分布 为 w 在 
0点 附近 密度 比较 高 作为 w 的 先验概率 这样 
在 计算 最大化 后验/nr 概率 的 时候 w 的 绝对值 
越小 后验/nr 概率 将 会 越大 我们 可以 得到 新的 
后验/nr 概率 这个 式子 看起来 是不是 有点 眼熟 啊 我们 
令 λ = α / β 可以 得到 类似于 之前 
损失 函数 的 一个 结果 了 我们 不仅 还 是 
可以 根据 这个 函数 来 计算 最优 的 拟合 函数 
而且 可以 得到 相应 的 一个 概率分布 函数 可以为 机器 
学习 的 很多 其他 的 任务 打下基础 这里 还 想再 
废话 一句 其实 很多 机器学习 里面 的 内容 都与 本处 
所说 的 曲线拟合 算法 类似 如果 我们 不 用 什么 
概率 统计 的 知识 可以 得到 一个 解决 的 方案 
就像 我们 的 第一 个 曲线拟合 方案 一样 而且 还 
可以 拟合 得 很好 不过 唯一 缺少 的 就是 概率分布 
有了 概率分布 可以 做 很多 的 事情 包括 分类 回归 
等等 都 需要 这些 东西 从 本质 上 来说 Beta 
分布 和 二项式 分布 Dirichlet 分布 和 多项式 分布 曲线拟合 
中 直接 计算 w 和 通过 高斯分布 估计 w 都是 
类似 的 关系 Beta 分布 和 Dirichlet 分布 提供 的 
是 μ 的 先验 分布 有了/nr 这个 先验 分布 我们 
可以 去 更好 的 做 贝叶斯 相关 的 事情 后记 
本文 就 写到 这里 花了 大概 4个 晚上 来写 这篇文章 
也 感谢 我 女朋友 的 支持 我 也 希望 能够 
用 它 去 总结 一下 最近 学习 的 一些 心得 
看看 是否 自己 能够 把 它 讲出来 我 觉得 学习 
的 过程 是 一个 爬山 的 过程 常 常有 的 
时候 觉得 自己 快到 山峰 了 结果 路 有向 下了 
自己 不停 有着 挫折 和 兴奋 的 感觉 不过 学习 
的 感觉 总体 来说 快乐 的 我 也想 能够 把 
自己 的 这份 快乐 带给 大家 D 参考资料 数学 之美 
番外篇 平凡 而又 神奇 的 贝叶斯 方法 PongbaPattern Recognition and 
Machine Learning Bishop 一些 Wikipedia 上面 的 内容 