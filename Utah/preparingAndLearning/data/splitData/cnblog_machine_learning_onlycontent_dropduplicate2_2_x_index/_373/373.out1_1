前言 最近 几周 花了 点 时间 学习 了 下 今年 
暑假 龙星 计划 的 机器学习 课程 具体 的 课程 资料 
参考 见 附录 本 课程 选 讲了 写 ML 中 
的 基本 模型 同时 还 介绍 了 最近 几年 比较 
热门 比较 新的 算法 另外 也将 ML 理论 和 实际 
问题 结合 了 起来 比如 将 其 应用 在 视觉 
上 web 上 的 等 总之 虽然 课程 内容 讲 
得 不是 特别 细 毕竟 只 有 那么 几节课 但是/c 
内容/n 还/d 算/v 比较/d 新/a 和/c 比较/d 全的/nr 学 完 
这些 课后 收获 还算 不少 的 至少 了解到 了 自己 
哪 方面 的 知识 比 较弱 下面 是 课程 中 
做 的 一些 简单 笔记 第 1 课   绪论 
课 机器学习 中 3个 比 不可 少 的 元素 数据 
模型 和 算法 现在 数据 来源 比较 广泛 每天 都 
可以 产生 T 级 以上 的 数据 模型 的话 就是 
机器学习 课程 中 需要 研究 的 各种 模型 算法 就是 
怎样 通过 数据 和 模型 来 学习 出 模型 中 
的 参数 但是/c 余/m 老师/n 在/p 课堂/n 上/f 提出/v 一个/m 
观点/n 就是/d 这/r 3个/mq 元素/n 都/d 不重要/i 最 重要 的 
是 需求 一旦 有了/nr 需求 就会 采用 各种 方法 取 
求解 问题 了 不愧 是 百度 公司 的 技术 副总监 
另外 机器 学习 的 主要 应用 场合 包括 计算机 视觉 
语音识别 自然 语音 处理 搜索 推荐 系统 无人驾驶 问答 系统 
等 第 2 课 线性 模型 线性 回归模型 需要 解决 
下面 3个 问题 1 . 怎样 从 训练 数 据估计 
线性 模型 的 参数 即 截距 和 斜率 2 . 
学习 到 的 线性 模型 性能 怎样 我们 是否 可以 
找到 更好 的 模型 3 . 模型 中 2个 参数 
的 重要性 怎么 估计 解决 第 1个 问题 是 一个 
优化 问题 即 求 得使 损失 函数 最小 的 参数 
这里 的 损失 函数 是 平方 项的/nr 也 称为 线性 
最小二乘 思想 线性 模型 的 表达式 为 其中 噪声 参数 
为 0 均值 的 高斯 噪声 如果 后面 求出 的 
噪声 不是 一个 均值 为 0 方差 相同 的 类似 
高斯分布 的 随机变量 则 说明 这个 模型 还 可以 被 
改进 比如说 将 x 首先 映 射到 非 线性函数 中去 
然后 对 非线性 函 数用 最小二乘 法做/nr 线性 回归 至于 
怎样 得到 非线性 映射函数 f x 则 要么 通过 人为 
观察 推测 要么 通过 机器学习 中的 特征 学习 来 自动 
获得 更 广义 的 线性 模型 并不 一定 是 一个 
线性方程 只是 其 参数 可能 是 线性 的 线性 模型 
能够 模拟 非 线性函数 残差 可以 看做 是 噪声 的 
近似 但是 一般来说 残差 要比 噪声 小 所以在 线性 模型 
中 噪声 项就/nr 可以 用 残差 来 估计 不过 其 
分母 不是 1 / n 而是 1 / n p 
因为 需要 达 一个 无偏估计 特征向量 元素 属性 的 重要 
性 评价 常见 的 有 以下 2 种方法 第一 是 
抽掉 一个 特征 想 然后 计算 其 残差 变化 值 
与 全部 特征 都 用上 的 比值 所 得到 的 
分数 为 F score F score 越大 说明 该 属性 
越 重要 第 2种 方法 是 采用 t 分布 来 
假设检验 得到 Z score 即 假设 对应 特征 属性 不 
存在 即 其 值 为 0 时 出现 样本数据 的 
概率 为 Z score 如果 Z score 越大 说明 该 
属性 越 不重要 第 3 课 过拟合/i 和/c 规则/n 项/n 
Regularization/w 中文/nz 意思/n 是/v 规则/n 指 的 是 在 overfitting 
和 underfitting 之间 做 平衡 通过 限制 参数 空间 来 
控制 模型 的 复杂度 测试 误差 和 训练 误差 之间 
差 一个 规则 项 其 公式 为 模型/n 越/d 复杂/a 
说明/v 模型/n 越/d 不稳定/i 学习 到 的 目标 函数 越 
不光滑 也就 越 容易 over fitting 所以 需要 控制 模型 
的 复杂度 一般来说 有2/nr 种方法 即 减少 模型 中 参数 
的 个数 或者 减小 参数 的 空间 大小 目前 用 
得 最多 的 就是 减小 参数 的 空间 大小 是 
通过 规则 项 达到 的 规则 项的/nr 引入 同时 也 
需要 引入 一个 调节 的 参数 该 参数 的 大小 
一般 通过 交叉 验证 获得 如果 规则 项是/nr 2次 的 
则 也 称为 ridge 回归 规则 项是/nr 一次 的 则 
称为 lasso 回归 Ridge 回归 的 优点 是 解 比较稳定 
且 允许 参数 的 个数 大于 样本 的 个数 Lasson 
回归 的 优点 是 有稀/nr 疏解 不过 解 不一定 稳定 
如果 碰到 参数 个数 大于 样本 个数 这时候 就 不能 
够用 参数 个数 来做 规则 化了 而是 采用 缩小 参数 
空间 的 方法 这样的话 既在 统计学 上 对 特征 数量 
集 大 时有 鲁棒性 同时 在 数值 计 算上 方程解 
也 具备 稳定性 第 4 课 线性 分类器 很好 的 
理解 线性 分类器 可以 理解 很多 ml 的 概念 以及 
非 线 性问题 线性 分类器 是 在 实际 应用 过程 
中 最 有用 的 模型 据 余 老师 讲 从 
06年 开始 人工神经网络 又 开始 热 起来 了 主要 体现 
在 deep learning 领域 svm 理论 很 完美 应用 场合 
也 很广 同理 logistic 回归 应用 场合 也 非常 广 
和 svm 差不多 当 数据 为 大样本 数据 时 用 
线性 SVM 模型 比较好 第 5 课 非线性 svmRKHS 表示 
定理 即 模型 的 参数 是 在 训练 样本 的 
线性 子空间 中 是 训练 样本 的 线性组合 这 不仅 
适用 于 svm 对 其他 的 模型 比如 感知机 RBF 
网络 LVQ boosting logistic 回归 等 模型 都 成立 Kernel 
可以 简单 理解 为 表示 2个 值 相似 度 的 
测量 通过 核 函数 可以 更好 的 了解 regularization 所需 
优化 的 目标 函数 可以 写成 参数 形式 参数 形式 
的 对偶 形式 和非/nr 参数 形式 这 3种 如果在 非 
参数 形式 中 其 规则 项 是由 所 学习 到 
的 函数 f x 来 控制 的 它 的 模 
与 对应 核 函数 进行 特征函数 分解 时的/nr 特征值 系数 
成反比 即 特征函数 分解 中非 主 成分 的 函数 对应 
的 特征 系数 小 得到 的 惩罚 就 大 就会 
更加 被 抑制 因此 我们 保留 的 主要 是 主 
成分 的 那些 特征函数 从 上面 可以 看出 核 函数 
是 有 一定 的 结构 的 该 结构 决定 了 
最终 的 目标 函数 f x 长得 什么样 逻辑 回归 
和 svm 的 区别 只是 loss 函数 的 不同 logstic 
回归 的 loss 函数 为 logstic 函数 核 svm 的 
loss 函数 为 hinge loss 两者 有着 相同 的 性能 
逻辑 回归 是 带 概率 的 输出 更容易 用于 多分 
类 问题 不过 目前 这 2 种方法 都是 旧 方法 
了 LVQ 中文名 为 学习 矢量化 它 是 一个 基于 
模型 的 有 监督 学习 分类器 因此 我们 在 设计 
一个 模型 时 需要 考虑 采用 什么样 的 loss 函数 
采用 什么样 的 基 函数 h x h x 是 
有限 维 的 还是 无限 维 的 是否 需要 学习 
h x 用 什么样 的 方法 来 优化 目标函数 QP 
LBFGS 还是 梯度 下降 等 理论 上 使用 kernel 理论 
可以 实现 用 有限 的 计算 完成 无限 空间 的 
学习 问题 但是 在 实际 问题 中 由于 其 复杂度 
是 样本 个数 N 的 3 次方 所以 当 样本数据 
很多时 基本上 是 无法 实现 的 参数/n 模型/n 和非/nr 参数/n 
模型/n 的/uj 区别/n 不是/c 看/v 模型/n 中/f 是否/v 有/v 参数/n 
所有 的 模型 都是 有 参数 的 非 参数 模型 
是 指 随着 样本数 的 增加 其 模型 中 的 
参数 的 个数 也 跟着 增加 反之 就为 参数 模型 
了 常见 的 非 参数 模型 有 高斯 过程 核 
svm dirichlet 过程 等 第 6 课 模型 选择 模型 
选择 在 实际 应用 过程 中 非常 有用 一般 把 
与 模型 有关 的 数据 分为 3 部分 训练 数据 
验证 数据 和 测试数据 如下 图 所示 其中 训练 数据 
和 验证 数据 都是 已 有的 样本数据 即已 观察 到了 
的 数据 测试数据 是 未来 实际 应用 中 产生 的 
数据 是 事先 不 知道 的 模型 的 参数 分为 
2 部分 第一 部分 是 模型 确定 后 通过 训练样本 
学习 得到 的 参数 另一 部分 是 手动输入 的 参数 
也叫做 超 参数 是 用来 控制 模型 的 复杂度 的 
也 就是 来 控制 模型 本身 长 什么样 的 它 
是由 验证 数据 来 调节 的 模型 选择 问题 就是说 
怎样 验证 一个 模型 是否 好 模型 的 好坏 最终 
是 要看 它 在 测试数据 集上 的 表现 因此在 未 
观测 到 测试数据 时 我们 只能 用 验证 数据集 来 
代替 它 进行 测试 一般 采用 的 方法 为 交叉 
验证 比如说 LOOCV 即 留 一 法 交叉 验证 类似 
的 还有 k 折 交叉 验证 交叉 验证 的 主要 
目的 是 防止 训练 出来 的 模型 过拟合 但是 在 
当今 由于 数据 都是 海量 的 交叉 验证 方法 使用 
越来越 少了 因为 如果 训练 数据集 非常 大 的话 一般 
不会 产生 过拟合 现象 还有 一些 方法 是 不 需要 
通过 验证 而 直接 来 评价 模型 好坏 的 比 
如是 AIC BIC MDL SRM 等 第 7 课 模型 
平均 本文 中讲 的 model 是 指 的 一个 learning 
algorithm 甚至 比 learning algorithm 所指 的 范围 还要 小 
因为 在 一个 learning algorithm 里 不同/a 的/uj 参数/n 调节/vn 
和/c 不同/a 的/uj 输入/v 特征/n 都会/nr 导致/v 不同/a 的/uj model/w 
模型/n 选择/v 的/uj 目标/n 是/v 使/v 模型/n 有/v 更好/d 的/uj 
可/v 解释性/n 和/c 更好/d 的/uj 性能/n 而 模型 平均 的 
目标 只需 要使 模型 有 更好 的 性能 即可 因为 
模型 平均 过程 中用 到 了 很多 模型 而 模型 
个数 越 多则 其 可 解释性 就 越低 模型 平均 
的 英文 名称 有 model ensemble model blending model combination 
model averaging . Model selection 和 model combination 的 不同 
使用 体现在 如果 某 个 模型 以 绝对 的 优势 
好于 其他 所有 模型 那么 这 时候 我们 就 采用 
model selection 因为 不仅 有好 的 性能 还 可以 获得 
好 的 可 解释性 如果 所有 的 模型 在 性能 
表现 上 都 差不多 没有 所谓 的 好坏 且 模型 
本身 又 有 很大 的 不同 这时候 就 可以 采用 
model combination 来 大大 提高 其 性能 了 通常 来说 
model combination 比 model selection 要 稳定 些 那么 该 
怎样 构造 差异性 大 的 模型 呢 可以 从 下面 
四 个 方面 入手 1 . 不同 的 学习 算法 
2 . 不 同参数 调整 3 . 有 差异 的 
输入 特征 4 . 引入 随机 思想 比如 bagging 关于 
指数 权值 的 模型 平均 只是 在 均一 模型 平均 
即 采用 投票 的 方式 的 基础 上 将 投票 
权值 改为 模型 误差 的 指数 形式 而 不是 相同 
的 均值 如果 所 学习 到 的 一个 模型 的 
误差 越大 则 其 权值 越低 理论 上 比较 完美 
不过 在 张 老师 讲 他 自己 实验 的 时候 
发现 并 没有 什么 提高 有时候 效果 还 不如 voting 
Stacking 和 指数 权值 的 模型 平均 有点 类似 也是 
先 学习 出 各个 模型 然后 把 学习 出 的 
模型 作为 第二层 学习 的 输入 优化 最小 的 第二层 
的 误差 来 学习 模型 的 权值 Bagging 也 是 
一种 均一 模型 平均 它 的 所有 模型 的 学习 
算法 一样 只是 输入 样本 采用 bootstrip 获得 因为 是 
采用 boostrip 获得 的 所以 其 训练样本 有些 不 一定 
用 到了 而 有些 则 重 复用 到了 这样 每个 
学习 出来 的 model 不是 很 稳定 因而 这也 扩大 
了 model 之间 的 差异性 提高 了 集群 学习 的 
性能 Bagging 是 减小 学习 的 方差 而 boosting 是 
减小 学习 的 偏差 最后 模型 平均 的 一个 比较 
出名 的 应用 场合 就是 把 决策树 改造 成 随机 
森林 的 例子 因为 单颗/nr 决策树 虽然 有可 解释性 能够 
很好 的 处理 非 均匀 的 特征 以及 是 一种 
非 线性 的 方法 但是 它 的 最大 缺点 就是 
分类 结果 不 准确 因此在 样本 选择 和 输入 特征选择 
方面 采用 了 随机 的 方法 得到 不同 的 模型 
后 再做 平均 就成 了 随机 森林 理论 和 实验 
表明 随机 森林 的 效果 要比 决策树 好 很多 第 
8 课 BoostingBoosting 既 可以 看做 是 signal learning 也 
可以 看做 是 ensemble learning 本课 中将 其 看做 是 
ensemble learning 它/r 是由/i 多个/m 弱/a 分类器/n 组合/v 成/n 一个/m 
强/a 分类器/n 但是 这里 所指 的 弱 分类器 满足 的 
条件 其实 并 不弱 因为 它 需要 满足 对 样本 
的 所以 加权 情况 的 分类 效果 都要/nr 大于 0.5 
因此 现在 有 不少 学者 不 称 这些 为 弱 
分类器 了 而 称为 基本 分类器 Boosting 中最 常用 的 
算法 是 AdaBoosting AdaBoosting 是 对 分类 错误 的 样本 
加大 其 权重 来 达到 resamble 的 效果 且 采用 
贪婪 算法 进行 loss 的 函数 的 优化 VC 维 
的 传统 定义 为 对 一个 指标 函 数集 如果 
存在 H 个 样本 能够 被 函数 集中 的 函数 
按 所有 可能 的 2 的 K 次方 种 形式 
分开 则 称 函 数集 能够 把 H 个 样本 
打散 函 数集 的 VC 维 就是 它 能 打散 
的 最大 样本 数目 H AdaBoosting 不是 最大 margin 的 
但 为什么 比 最大 marign 的 boosting 效果 要 好呢 
课程 中 从 传统 的 boosting 分析 来 做了 一定 
的 解释 但是 仍 不能够 解释 当 训练 误差 为 
0时 其 泛化 误差 还在 减小 这一 问题 后面 的 
学者 又 提出 了 从 margin bound 方面 来 解释 
这个 问题 另外 从 另一个 角度 来 更好 的 理解 
boosing 的 方法 是 greedy boosting 即 寻找 样本 权重 
d 和弱/nr 分类器 权重 w 的 过程 是 一个 贪婪 
过程 最后 老师 讲 了 一个 general loss 函数 以及 
利用 这个 函数 进行 的 general boosting 第 9 课 
  学习理论 概论 这 节 课 的 内容 比较 理论化 
听 不太 懂 机器学习 理论 的 主要 目标 是 平均 
一个 学习 算法 的 好坏 即 怎样 通过 训练 误差 
来 估计 测试 误差 可以 通过 一致性 收敛 来 估计 
训练 误差 和 测试 误差 之间 的 关系 即 测试 
误差 以大 概率 事件 小于 训练 误差 加上 某个 值 
这个 值 的 大小 与 训练 样本数 以及 概率值 有关 
证明 上面 的 一致性 收敛 需要 用到 切比雪夫 不等式 VC 
维 covering numbers 这 几种 技术 其中 covering numbers 定义 
为 attain 训练样本 的 预测 函数 的 个数 具体 是 
什么 没 有理解 清楚 我们 可以 用 VC 维 来 
估计 convering number 最后 老师 还讲 了 一个 Rademacher 复杂度 
并说 了 下 它 和 VC 维 之间 的 关系 
真心 不懂 Rademacher 是个 什么 东东 第 10 课   
机器学习 中的 优化 问题 机器学习 中 大部分 问题 都 可以 
归结 为 参数 优化 问题 即 找到 最 适合 目标函数 
的 参数 该 参数 一般 满 足使 目标函数 最大 或者 
最小 常见 的 优化 方法 有 梯度 下 降法 该 
方法 是 每次 沿着 梯度 下降 最快 的 那个 方向 
寻找 函数值 不断 迭代 就 可以 寻找 到 近似 的 
极值 该 方法 的 学习 速率 即 每次 沿 梯度方向 
前进 的 距离 和 收敛 速率 是 最 值得 关注 
的 一般来讲 如果 函数 是 光滑 且 是 严格 为 
凸函数 的 则 其 收敛 速度 最快 其实 是 光滑 
但不 严格凸 的 最慢 的 要数 非 光滑 函数 因此当 
函数 有 一部分 是 光滑 而 另一 部分 不光滑 时 
我们 可以 采用 Proximal 梯度 下 降法 该 方法 是 
最近 几年 热门 起来 的 效果 比 梯度 下降 要好 
更新 的 类似 的 算法 还有 Nestervo 这个 学者 的 
Accelerated 梯度 法 全是 数学公式 完全 看 不懂 为了 求出 
局部 极值 点 一般 可以 采用 近似 泰勒 展开 中的 
H 矩阵 来 求得 典型 的 算法 有 LBFGS 另外 
当 需要 优化 的 参数 为 一个 向量 时 不一定 
需要 把 这个 向量 的 元素 对 等 考虑 我们 
可以 分开 优化 即 每次 只 优化 参数 向量 中 
的 一个 其它 的 保持 不变 这样 循环 直到 收敛 
最后 老师 讲了 凸函数 的 优化 问题 还 可以 采用 
Dual 梯度 下 降法 实话 说 这种 纯 数学公式 的 
东西 太 乏味 了 第 11 课   Online learningOnline 
learning 指 的 是 每当 来 一个 数据 就会 学习 
一个 最优 的 预测 函数 其 最优 的 准则 是 
当前 位置 loss 函数值 最小 因此 每一步 的 预测 函数 
都 有可能 不同 这 就是 Online learning 其实 很 早前 
就有 online learning 的 例子 比如说 感知机 学习 规则 在 
了解 Online learning 之前 需要 了解 regret 分析 这个 概率 
regret 指 的 是 Online learning 中 每次 学习 的 
误差 减去 使用 用 当前 为止 的 最优 函数 而 
产生 的 误差 的 平均值 当然 我们 希望 regret 越小 
越好 Online learning 的 关键 是 需要 更 不断 新 
状态 其实 Online learning 也 是 一个 优化 问题 我们 
可以 把 第 10 讲 的 优化 问题 全部 转换成 
对应 的 Online learning 比如说 凸 优化 梯度 下 降法 
proximal descent 其中 将 proximal descent 转换成 online 版本 可以 
采用 L1 规则化 Dual averaging 保持 second order 信息 等 
统计 梯度 下降 可以 用来 优化 大规模 的 数据 它 
的 不同 变种 主要 来源 于 不同 的 proximal 函数 
不同 的 学习率 是否是 dual averaging 是否是 averaging 是否是 acceleration 
等 第 12 课 sparsity modelSparsity model 的 出现 时 
为了 解决 统计 学习 中的 维数 灾难 问题 的 即 
样本 的 个数 远远 小于 特征 的 维数 解决 标准 
的 稀疏 回归模型 可以 采用 greedy 算法 和 convex relaxation 
Greedy 算法 中 比较 有 代表性 的 是 OMP 要从 
稀疏 的 参数 重建 参数 需要 有 2个 条件 即 
irrepresentable 和 RIP 稀疏 模型 一个 代表性 的 问题 是 
Lasso 的 求解 老师 从 上面 2个 条件 介绍 了 
lasso 的 求解 Lasso 是 基于 L1 规则化 的 其它 
一些 比较 复杂 的 规则 项 对应 的 sparsity model 
有 比如 structured sparsity 比如说 group structure graphical model matrix 
  regularization . 这 又是 一堂 纯数学 的 课程 第 
13 课 Graphical modelGraphical model 是 一个 应用 比较 广泛 
的 模型 不过 比较复杂 因为 里面 涉及 到 了 很多 
概率 的 知识 但是 这 节 课 的 内容 还 
算 比较 表面 没有 过多 的 细节 主要 从3个/nr 方面 
介绍 graphical model 即 model 本身 推理方法 和 模型 的 
结构 学习 概率模型 中 一大 部分 就是 graphic model 而 
graphic model 中 又 分为 有向图 和 无向图 有向图 中 
比较 有 代表 的 是 贝叶斯 网络 无向图 中 比较 
有 代表 的 是 MRF 本节 内容 主要 是 讲 
的 有向图 任何 一个 复杂 的 贝叶斯 网络 都 可以 
由 causal chains common cause common effect 这 3 部分 
构成 Graphical model 应用 很广 比如说 常见 的 线性 回归 
问题 也 可以 转换成 graphical model 问题 如果 是 分段 
线性 回归 问题 还 可以 转换成 带有 隐 变量 的 
graphical model 贝叶斯 网络 中 的 推理 一般 是 给定 
一些 观测 数据 求出 在此 观测 数据 下 出现 某些 
中间 状态 的 概率 当 网络 是 简单 的 链 
或者 是 树状 时 推理 起来 比较 简单 当 模型 
含有 环状 结构 时 对应 的 推理 就 非常 复杂 
了 Graphical model 中 最后 一个 问题 是 模型 结构 
的 学习 可以 将其 看做 是 结构 的 搜索 问题 
对应 的 很多 AI 搜索算法 此时 也 可以 派上用场 结构 
学习 的 问题 主要 包括 发现 模型 中的 隐 变量 
因果 关系 直接 从 数据 中 学习 其 结构 第 
14 课   structured learning 结构 学习 的 方法 和 
理论 包括 结构 输入 结构 输出 和 结构 模型 其中 
结构 模型 分为 conditional model 和 generative model Generative model 
包括 HMM ＨＭＭ 有 观察 值 独立性 的 假设 为了 
解决 该 假设 带来 的 问题 后来 有 学长 提出 
了 MEMM 算法 不过 MEMM 本身 又 带来 了 标注 
偏置 问题 最 后面 的 改进 算法 ＣＲＦ 成功 的 
解决 了 标注 偏置 问题 CRF 模型 可以 看做 是 
logistic 回归 在 结构 学习 框架 下 的 扩展 . 
同理 M3N 可以 看做 是 SVM 在 结构化 框架 下 
的 扩展 最后 课堂 上 老师 比较 了 CRFs 和 
M3N 两种 算法 第 15 课 deep learning 这 节 
课 讲 的 内容 比较 容易 激发 人 的 兴趣 
一 是因为 deep learning 最近 非常 火热 二 是 因为 
用 deep learning 来做 一些 视觉 问题 其 效果 能 
提高 不少 本次 课程 没有 讲 具体 的 细节 主要 
是 介绍 了 一些 deep learning 的 概念 和 应用 
Deep learning 的 意思 是 可以 自动 来 学习 一些 
特征 比如说 在 视觉 的 分类 或者 识别 中 一般 
都是/nr 特征提取 + 分类器 设计 并且 提取 到 的 特征 
的 好坏 直接 影响 了 分类器 的 分类 效果 但是 
在 目前 的 计算机 视觉 领域 其 特征 的 提取 
都是/nr 我们 人工 设计 的 需要 针对 不同 的 应用 
场合 来 提取 不同 的 特征 余 老师 开玩笑 的 
说 计算机 视觉 最近 10年 的 最大 成就 就是 有了 
个 SIFT 特征 但是 它 是 基于 RGB 图像 提出 
的 而今 各种 传感器 比如 Kinect 等 我们 又 得去 
重新 设计 它 的 特征 难道 我们 还要 等 10年 
么 因此 可以 看出 一个 通用 的 特征提取 框架 需要 
给出 这 就是 deep learning 也叫做 feature learning 也 就是说 
给 了 很多 样本 系统 能够 自动 去 学习 这些 
样本 的 特征 而 不是 依靠 人工 来 设计 听 
起来 是 多么 的 诱人 这就 更 类似于 AI 了 
Deep learning 主要 是 确定 一个 算法 的 层次结构 这个 
层次结构 非常重要 它 的 想法 和 人体 大脑皮层 的 工作 
机制 类似 因为 人 大脑 在 识别 某些 东西 的 
时候 也 是 一个 层次 结构 的 课件 中 主要 
接 受了 multi scale models 和 hierarchical model structure spectrum 
等 但 没有 具体 展开 只是 做 了 一个 综述 
性 的 介绍 第 16 课 Transfer learning & Semi 
supervised learning 一方面 由于 有些 问题 的 训练 样本数据 非常少 
且 样本 的 获取 代价 非常 高 或者 是 模型 
的 训练 时间 特别 长 另一方面 由于 很多 问题 之 
间有 相似性 所以 TL transfer learning 就 产生 了 TL 
主要 是 把 多个 相似 的 task 放在 一起 来 
解决 它们 共享 同一 个 输入 空间 和 输出 空间 
TL 常见 的 例子 有 传感器 网络 预测 推荐 系统 
图像 分类 等 常见 的 用来 解决 TL 问题 有 
下面 几个 模型 HLM 层次 线性 模型 NN 回归 线性 
模型 这些 模型 本质上 都是 学校 一个 隐含 的 相同 
的 特征 空间 另外 老师 也 讲 到了 TL 和 
GP 高斯 过程 的 对比 高斯 过程 是 一个 贝叶斯 
核 机器 的 非线性 算法 通过 对 先验 样本 的 
采用 学习 可以 得到 尖锐 的 后验/nr 概率模型 它 是 
一种 非 参数 的 模型 TL 方法 主要 分为 4 
大类 样本 之间 的 迁移 特征 表达 的 迁移 模型 
的 迁移 和 相关 领域 知识 的 迁移 其中 特征 
表达 的 迁移 和 模型 的 迁移 在 数学 本质上 
是 类似 的 也是 学者 们 研究 的 重点 SSL 
Semi supervised learning 是 为了 达到 用 少量 标注 了 
的 样本 + 大量 没有 标注 的 样本 来 学习 
一个 比 单独 用 少量 标注 样本 效果 更好 的 
模型 老师 举 了 一个 混合 高斯分布 的 例子 来 
解释 SSL 学习 的 效果 通过 这个 例子 引出 了 
SSL 的 一个 通用 模型 本课 还 简单 的 介绍 
了 co training 方法 所谓 co training 就是 把 表 
组 好 的 数据 分成 几类 每 一类 都 train 
一个 model 然后 把 这些 model 作用 到 unlabel 的 
样本 上 通过 优化 方法 达到 输出 一致 的 效果 
最后 介绍 的 Graph Laplacian 以及 它 的 harmonic 解就/nr 
完全/ad 木/n 有/v 看懂/i 第 17 课 Recommendation y s 
t e m s R e c o m m 
e n d a t i o n Systems 一个 
简单 的 应用 就是 会 根据 用户 的 购买 历史 
来 退 算出 用户 可能 喜欢 的 产品 然后 推荐 
给 用户 目前 很多 互联网 公司 都 在做 这 方面 
的 研究 因为 可以 带来 大量 的 经济效益 Recommendation Systems 
是 一个 协同 滤波 问题 本 课程 主要 围绕 不同 
用户 给 不同 电影 评分 这个 例子 来 介绍 首先 
要 解决 的 是 历史数据 偏差 不同 的 问题 即要 
对 数据 做 预处理 实现 归一化 在对 Recommendation Systems 进行 
设计 的 一个 主流 方法 之一 是 将 Recommendation Systems 
问题 看做 是 一个 分 类 问题 即把 用户 i 
对 所有 电影 打分 看做 是 要 预测 的 标签 
而 其他 所有 人 对 电影 的 打分 看做 是 
特征 主要 采用 的 方法 是 朴素 贝叶斯 KNN 等 
其他 大 部分 的 分类 算法 都 可以 派上用场 Recommendation 
Systems 问题 的 另一 主流 方法 是 把 它 看成 
矩阵 分解 MF 问题 这在 实际 应用 中 是 效果 
最好 的 因为 我们 观察 到 的 数据 是 很 
稀疏 的 很多 位置 都是 missing 的 且 这些 数据 
之间 内部 是 存在 一个 简单 结构 的 因此 我们 
可以 把 需要 填充 的 矩阵 R 分解成 2个 低 
秩 矩阵 的 乘积 这 可以 采用 SVD 或者 SVD 
+ 一些 优化 的 方法 来 解决 由此 可以 看出 
Recommendation Systems 是 一个 典型 的 ML 问题 第 18 
课   computer vision 本课 简单 的 介绍 了 下 
computer vision 中 的 基本 问题 比如说 什么 事 computer 
vison computer vison 的 难点 computer vison 问题 的 分类 
特征 检测 边缘 检测 目标 检测 图像 分割 拼图 3D 
重建 计算机 图形学 目标 识别 等等 第 19 课 learning 
on the web 机器学习 在 web 上 的 应用 比较 
广泛 比如 前面 讲过 的 推荐 系统 另外 还有 一些 
搜索 结果 排序 分类 问题 社区 行为 分析 用户 行为 
模型 等等 本/r 课程/n 主要/b 从/p 分类/n 和/c 排序/n 做/v 
了/ul 一些/m 介绍/v 网络 上 存在 着 各种 垃圾 信息 
例如 垃圾邮件 垃圾 网页 垃圾 广告 等 分类 问题 就是 
采用 ML 的 方法 过滤 掉 这些 垃圾 信息 另外 
一个 比较 常见 的 分类 问题是 文本 分类 找出 文本 
描述 的 主题 其中 BOW 算法 既 简单 又 取得 
了 很好 的 效果 最后 老师 对 Web search 问题 
也 做了 个 简单 的 介绍 总之 本课 大概 介绍 
了 下 ML 在 web 上 的 简单 应用 和 
挑战 总结 ML/w 给/p 我/r 的/uj 感觉/n 就是/d 规则/n 项和/nr 
最优化/v 贯穿/v 了/ul 本次/r 课程/n 的/uj 所有/b 章节/n 参考资料 http 
/ / bigeye . au . tsinghua . edu . 
cn / DragonStar2012 / index . html 