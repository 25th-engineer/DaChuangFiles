这 节 课 的 题目 是 Deep learning 个人 以为 
说 的 跟 Deep learning 比较 浅 跟 autoencoder 和 
PCA 这块 内容 比较 紧密 林 介绍 了 deep learning 
近年来 受到 了 很大 的 关注 deep NNet 概念 很早 
就有 只是 受限于 硬件 的 计算 能力 和 参数 学习 
方法 近年来 深度 学习 长足 进步 的 原因 有 两个 
1 pre training 技术 获得 了 发展 2 regularization 的 
技术 获得 了 发展 接下来 林 开始 介绍 autoencoder 的 
motivation 每 过 一个 隐 层 可以 看做 是 做了 
一次 对 原始 输入 信息 的 转换 什么 是 一个 
好 的 转换 呢 就是 因为 这种 转换 而 丢失 
较多 的 信息 即 encoding 之后 甚至 可以 用 decoding 
的 过程 复原 因此 在 考虑 deep NNet 的 参数 
学习 的 时候 如果在 pre training 阶段 采用 类似 autoencoding 
的 方式 似乎 是 一个 不错 的 选择 如下 就是 
autoencoder 的 一个 示例 简单 来说 就是 经过/nr 如下 的 
单层 神经 网络结构 后 输出 跟 输出 十分 接近 这种 
autoencoder 对于 机器学习 来说 有 什么 作用 呢 1 对于 
supervised learning 来说 这种 information preserving NN 的 隐 层 
结构 + 权重 是 一种 对 原始 输入 合理 的 
转换 相当于 在 结构 中 学习 了 data 的 表达 
方式 2 对于 unsupervised learning 来说 可以 作为 density estimation 
或 outlier detection 这个 地方 没 太 理解 清 可能 
还是 缺少 例子 autoencoder 可以 看成 是 单层 的 NN 
可以 用 backprop 求解 这里 需要 多 加入 一个 正则化 
条件 wij 1 = wji 2 采用 上述 的 basic 
autoencoder 可以 作为 Deep NNet 的 pre training 方式 接下来 
林 开始 关注 Deep NNet 的 regularization 的 问题 之前 
提到 过 的 几种 regularization 方式 都 可以 用 structural 
constraints weight decay / elimination regularizers early stopping 下面 介绍 
一种 新的 regularization technique 这种 方式 是 adding noise to 
data 简单 来说 在 训练 autoencoder 的 时候 加入 高斯 
噪声 喂 进去 的 输出 端 还是 没有 加入 噪声 
的 data 这样 学 出来 的 autoencoder 就 具备 了 
抵抗 noise 的 能力 接下来 开始 引入 PCA 相关 的 
内容 之前 陈述 的 autoencoder 可以 归类 到 nonliner autoencoder 
因为 隐 层 输出 需要 经过 tanh 的 操作 所以 
是 nonlinear 的 那么 如果 是 linear autoencoder 呢 这里 
把 隐 层 的 bias 单元 去掉 最后 得到 的 
linear autoencoder 的 表达式 就是 h x = WW x 
由此 可以 写 出来 error function 这 是 一个 关于 
W 的 4 阶 的 多项式 analytic solution 不太好 整 
于是 林 给出 了 下面 的 一种 求解 思路 上述 
的 核心 在于 WW 是 实 对称 阵 实 对称 
阵 有 如下 的 性质 http / / wenku . 
baidu . com / view / 1 4 7 0 
f 0 e 8 8 5 6 a 5 6 
1 2 5 2 d 3 6 f 5 d 
. html 我们 注意 一下 W 这个 矩阵 W 是 
d × d 维度 的 矩阵 WW 是 d × 
d 维度 的 矩阵 这里 回顾 一下 矩阵 的 秩 
的 性质 因此 WW 的 秩 最大 就是 d 了 
d 代表 数据 的 原始 维度 d 代表 隐 层 
神经元 的 个数 一般 d ＜ d WW 的 秩 
最大 是 d 能 得到 这样 的 结论 WW 至 
多有 d 个 非零 特征值 → 对角 阵 gamma 对角 
线上 最多 有d/nr 个 非零 元素 这里 需要 复习 线性代数 
一个 概念 如果 矩阵 可以 对 角化 那么 非零 特征值 
的 个数 就 等于 矩阵 的 秩 如果 矩阵 不 
可以 对 角化 那么 这个 结论 就 不一定 成立 了 
这里 我们 说 的 WW 是 实 对称 阵 又 
因为 实 对称 阵 一定 可以 对 角化 因此 WW 
的 非零 特征值 特殊 就 等于 矩阵 的 秩 通过 
上述 的 内容 WW x 又 可以 看成 是 VgammaV 
x 1 V x 可以 看成 是 对 原始 输入 
rotate2 gamma 可以 看成 是 将 0 特征值 的 component 
的 部分 设成 0 并且 scale 其余 的 部分 3 
再 转 回来 因此 优化 目标函数 就 出来 了 这里 
可以 不用 管 前面 的 V 这是 正交变换 的 一个 
性质 正交变换 不 改变 两个 向量 的 内积 详情 见 
https / / zh . wikipedia . org / wiki 
/ 正交 这样一来 问题 就 简化 了 令 I gamma 
生出 很多 0 利用 gamma 对角线 元素 的 自由度 往 
gamma 里面 塞 1 最 多塞 d 个 1 剩下 
的 事情 交给 V 来 搞定 1 先把 最小化 转化 
为 等价 的 最大化 问题 2 用 只有 一个 非零 
特征值 的 情况 来 考虑 Σ v xx v   
s . t . v v = 13 在 上述 
最优化 问题 中 最好 的 v 要 满足 error function/w 
和/c constraints/w 在/p 最优/d 解的/nr 时候/n 他们 的 微分 要 
平行 4 再 仔细 观察 下 形式 Σ xx v 
= lambdav 这里 的 v 不 就是 XX 的 特征向量 
么 因此 最 优化 的 v 就是 特征值 最大 的 
XX 的 特征向量 需要 降到 多少 维 的 就 取 
前 多少 个 特征向量 林 最后 提了 一句 PCA 其实 
就是 在 进行 上述 步骤 之前 先 对 各个 维度 
的 向量 均值 化 下面 说 一下 PCA http / 
/ blog . codinglabs . org / articles / pca 
tutorial . html 上面 这篇 日志 非常好 基本 完全 解释 
了 PCA 的 来龙去脉 1 PCA 的 目的 是 对 
数据 降 维 之后 还能 尽量 保持数据 原有 的 信息 
分得开 方差 大 2 如果 对 原始数据 各个 维度 做 
均值 化 的 操作 之后 方差 & 协方差 只用 一个 
矩阵 就 表示 出来 了 上述 这段话 看 明白 了 
PCA 的 核心 就 有了 巧妙 地 把 原始 输入 
数据 各个 维度 均值 化 之后 方差/n 和/c 协方差/n 都/d 
放到/v 一个/m 矩阵/n 里/f 了/ul 优化 的 目标 是 方差 
要 大 协方差 要 小 这样 的 优化 目标 就 
等价 于把/nr 协方差 矩阵 对 角化 实 对称 阵 对 
角化 是 线性代数 的 基础 知识 http / / wenku 
. baidu . com / view / 1 4 7 
0 f 0 e 8 8 5 6 a 5 
6 1 2 5 2 d 3 6 f 5 
d . htmlOK PCA 就 大体上 搞定 了 中途 还 
看了 stanford 的 http / / ufldl . stanford . 
edu / wiki / index . php / PCA 脑子 
里 冒出 来 一个 想法 如果 协方差 矩阵 是 满秩的/nr 
并且 不 对 数据 降 维 原来 是 多少 维 
还是 多少 维 那么 变换 前 和 变换 后有 啥 
区别 呢 从 式子 上看 这种 变化 相当于 把 变换 
后的/nr 协方差 矩阵 搞成 对角 阵 了 如果 从 几何 
上 来看 比较 下面 两个 图 变换 前 变换 后 
直观 上看 就是 整体 给 放平 了 变化 前 x1 
越大 x2 也 越大 反之亦然 变换 后 由于 给 放 
平了 x1 的 大小 与 x2 的 大小 没关系 了 
因此 变换/v 后/f 这种/r 放平/i 就/d 消除/v 了/ul x1/i 和/c 
x2/i 的/uj 相关性/l 了/ul 也 就是 协方差 矩阵 的 非 
对角 元素 给 搞成 0 的 效果 