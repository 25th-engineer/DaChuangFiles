了解 更多 技术文章 请 点击 原文 链接 随着 科学技术 的 
发展 以及 硬件 计算 能力 的 大幅 提升 人工智能/n 已经/d 
从/p 几十/m 年的/nr 幕后/s 工作/vn 一下子/m 跃入/v 人们/n 眼帘/n 人工智能 
的 背后 源自于 大 数据 高性能 的 硬件 与 优秀 
的 算法 的 支持 2016年 深度 学习 已成为 Google 搜索 
的 热词 随着 最近一 两年 的 围棋 人机 大战 中 
阿 法狗/nr 完胜 世界 冠军 后 人们 感觉 到 再也 
无法 抵挡 住 AI 的 车轮 的 快速 驶来 在 
2017年 这 一年中 AI 已经 突破 天际 相关 产品 也 
出现 在 人们 的 生活 中 比如 智能 机器人 无人驾驶 
以及 语音 搜索 等 最近 世界 智能 大会 在 天津 
举办 成功 大会 上 许多 业内 行家 及 企业家 发表 
自己 对 未来 的 看法 可以 了解到 大多数 的 科技 
公司 及 研究 机构 都 非常 看好 人工智能 的 前景 
比如 百度 公司 将 自己 的 全部 身家 压在 人工智能 
上 不管 破釜沉舟 后是/nr 一举成名 还是 一败涂地 只要 不是 一无所获 
就行 为什么 突然 之间 深度 学习会 有 这么 大 的 
效应 与 热潮 呢 这 是因为 科技 改变 生活 很多 
的 职业 可能 在 今后 的 时间 里 慢慢 被 
人工智能 所 取代 全民 都在 热议 人工智能 与 深度 学习 
就连 Yann LeCun 大牛 都 感受到 了 人工智能 在 中国 
的 火热 言归正传 人工智能 的 背后 是 大 数据 优秀 
的 算法 以及 强大 运算 能力 的 硬件 支持 比如 
英伟达 公司 凭借 自己 的 强大 的 硬件 研发 能力 
以及 对 深度 学习 框架 的 支持 夺得 世 全球 
最 聪明 的 五十 家 公司 榜首 另外 优秀 的 
深度 学习 算法 有 很多 时不时 就 会 出现 一个 
新的 算法 真是 令人 眼花缭乱 但 大多 都是/nr 基于 经典 
的 算法 改进 而来 比如 卷积 神经网络 CNN 深度 信念 
网络 DBN 循环 神经网络 RNN 等等 本文 将 介绍 经典 
的 网络 之 循环 神经网络 RNN 这一 网络 也是 时序 
数据 的 首选 网络 当 涉及 某些 顺序 机器学习 任务 
时 RNN 可以 达到 很高 的 精度 没有 其他 算法 
可以 与之 一较高下 这 是 由于 传统 的 神经 网络 
只是 具有 一种 短期 记忆 而 RNN 具有 有限 的 
短期 记忆 的 优势 然而 第一代 RNNs 网络 并 没有 
引起 人们 着重 的 注意 这 是 由于 研究 人员 
在 利用 反向 传播 和 梯度 下降 算法 过程 中 
遭受 到 了 严重 的 梯度 消失 问题 阻碍 了 
RNN 几十年 的 发展 最后 于 90 年代 后期 出现 
了 重大 突破 导致 更加 准确 的 新一代 RNN 的 
问世 基于 这 一 突破 的 近 二十 年 直到 
Google Voice Search 和 Apple Siri 等 应用 程序 开始 
抢夺 其 关键 流程 开发人员 完善 和 优化 了 新一代 
的 RNN 现在 RNN 网络 遍布 各个 研究领域 并且 正在 
帮助 点燃 人工智能 的 复兴 之火 与 过去 有关 的 
神经 网络 RNN 大多数 人造 神经网络 如 前馈 神经网络 都 
没有 记忆 它们 刚刚 收到 的 输入 例如 如果 提供 
前馈 神经 网络 的 字符 WISDOM 当 它 到达 字符 
D 时 它 已经 忘记 了 它 刚刚 读过 字符 
S 这 是 一个 大 问题 无论 训练 该 网络 
是 多么 的 辛苦 总是 很难 猜出 下 一个 最 
有可能 的 字符 O 这 使得 它 成为 某些 任务 
的 一个 相当 无用 的 候选人 例如 在 语音 识别 
中 识别 的 好坏 在 很大 程度 上 受益 于 
预测 下 一个 字符 的 能力 另一方面 RNN 网络 确实 
记住 了 之前 的 输入 但是 处于 一个 非常 复杂 
的 水平 我们 再次 输入 WISDOM 并将 其 应用 到 
一个 复发性 网络 中 RNN 网络 中 的 单元 或 
人造 神经元 在 接收 到 D 时 也将 其 之前 
接 收到 的 字符 S 作为 其 输入 换句话说 就是 
把 刚刚 过去 的 事情 联合 现在 的 事情 作为 
输入 来 预测 接下来 会 发生 的 事情 这 给了 
它 有限 的 短期 记忆 的 优势 当 训练 时 
提供 足够 的 背景 下 可以 猜测 下 一个 字符 
最 有可能 是 O 调整 和 重新 调整 像 所有 
人工神经网络 一样 RNN 的 单元 为其 多个 输入 分配 一个 
权重 矩阵 这些 权重 代表 各 个 输入 在 网络层 
中 所占 的 比重 然后 对 这些 权重 应用 一个 
函数 来 确定 单个 输出 这个 函数 一般 被 称为 
损失 函数 代价 函数 限定 实际 输出 与 目标 输出 
之间 的 误差 然而 循环 神经 网络 不仅 对 当前 
输入 分配 权重 而且 还从 对过去 时刻 输入 分配 权重 
然后 通过 使得 损失 函数 最 下来 动态 的 调整 
分配给 当前 输入 和 过去 输入 的 权重 这个 过程 
涉及 到 两个 关键 概念 梯度 下降 和 反向 传播 
BPTT 梯度 下降 机器学习 中最 著名 的 算法 之一 就是 
梯度 下降 算法 它 的 主要 优点 在于 它 显着 
的 回避 了 维数 灾难 什么 是 维数 灾难 呢 
就是说 在 涉及 到 向量 的 计算 问题 中 随着 
维数 的 增加 计算 量 会 呈 指数 倍 增长 
这个 问题 困扰 着 诸多 神经 网络 系统 因为 太多 
的 变量 需要 计算 来 达到 最小 的 损失 函数 
然而 梯度 下降 算法 通过 放大 多维 误差 或 代价 
函数 的 局部 最小值 来 打破 维数 灾难 这 有助于 
系统 调整 分配 给 各个 单元 的 权重 值 以使 
网络 变得 更加 精确 通过 时间 的 反向 传播 RNN 
通过 反向 推理 微调 其 权重 来 训练 其 单元 
简单 的 说 就是 根据 单元 计算出 的 总 输出 
与 目标 输出 之间 的 误差 从 网络 的 最终 
输 出端 反向 逐层 回归 利用 损失 函数 的 偏 
导 调整 每个 单元 的 权重 这 就是 著名 的 
BP 算法 关于 BP 算法 可以 看 本 博主 之前 
的 相关 博客 而 RNN 网络 使用 的 是 类似 
的 一个 版本 称为 通过 时间 的 反向 传播 BPTT 
该 版本 扩展 了 调整 过程 包括 负责 前 一 
时刻 T 1 输入 值 对应 的 每个 单元 的 
记忆 的 权重 Yikes 梯度 消失 问题 尽管 在 梯度 
下降 算法 和 BPTT 的 帮助 下 享有 一些 初步 
的 成功 但是 许多 人造 神经网络 包括 第一代 RNNs 网络 
最终 都 遭受 了 严重 的 挫折 梯度 消失 问题 
什么 是 梯度 消失 问题 呢 其 基本 思想 其实 
很 简单 首先 来看 一个 梯度 的 概念 将 梯度 
视为 斜率 在 训练 深层 神经 网络 的 背景 中 
梯度 值 越 大代表 坡 度越 陡峭 系统 能够 越快 
地下 滑到 终点线 并 完成 训练 但这 也是 研究者 陷入 
困境 的 地方 当 斜坡 太 平坦 时 无法 进行 
快速 的 训练 这 对于 深层 网络 中 的 第一 
层 而言 特别 关键 因为 若 第一 层 的 梯度 
值 为零 说明 没有 了 调整 方向 无法 调整 相关 
的 权重 值 来 最下 化 损失 函数 这一 现象 
就是 消 梯度 失 随着 梯度 越来越 小 训练 时间 
也 会 越来越 长 类似于 物理学 中的 沿 直线运动 光滑 
表面 小球 会 一直 运动 下去 大 的 突破 长短期 
记忆 LSTM 在 九十 年代 后期 一个 重大 的 突破 
解决 了 上述 梯度 消失 问题 给 RNN 网络 发展 
带来 了 第二 次 研究 热潮 这种 大 突破 的 
中心 思想 是 引入 了 单元 长短期 记忆 LSTM LSTM 
的 引入 给 AI 领域 创造 了 一个 不同 的 
世界 这 是 由于 这些 新 单元 或 人造 神经元 
如 RNN 的 标准 短期 记忆 单元 从一/nr 开始 就 
记住 了 它们 的 输入 然而 与 标准 的 RNN 
单元 不同 LSTM 可以 挂 载在 它们 的 存储器 上 
这些 存储器 具有 类似 于 常规 计算机 中的 存储器 寄存器 
的 读 / 写 属性 另外 LSTM 是 模拟 的 
而 不是 数字 使得 它们 的 特征 可以 区分 换句话说 
它们 的 曲线 是 连续 的 可以 找到 它们 的 
斜坡 的 陡度 因此 LSTM/w 特别/d 适合/v 于/p 反向/v 传播/vn 
和/c 梯度/n 下降/v 中所/l 涉及/v 的/uj 偏/a 微积分/n 总而言之 LSTM 
不仅 可以 调整 其 权重 还 可以 根据 训练 的 
梯度 来 保留 删除 转换/v 和/c 控制/v 其/r 存储/l 数据/n 
的/uj 流入/v 和/c 流出/v 最 重要 的 是 LSTM 可以 
长时间 保存 重要 的 错误 信息 以使 梯度 相对 陡峭 
从而 网络 的 训练 时间 相对 较短 这 解决 了 
梯度 消失 的 问题 并 大大 提高 了 当今 基于 
LSTM 的 RNN 网络 的 准确性 由于 RNN 架构 的 
显著 改进 谷歌 苹果 及 许多 其他 先进 的 公司 
现在 正在 使用 RNN 为其 业务 中心 的 应用 提供 
推动力 总结 循环 神经网络 RNN 可以 记住 其 以前 的 
输入 当 涉及 到 连续 的 与 上下文 相关 的 
任务 如 语音识别 时 它 比 其他 人造 神经网络 具有 
更大 的 优势 关于 RNN 网络 的 发展 历程 第一代 
RNNs 通过 反向 传播 和 梯度 下降 算法 达到 了 
纠正 错误 的 能力 但 梯度 消失 问题 阻止 了 
RNN 的 发展 直到 1997年 引入 了 一个 基于 LSTM 
的 架构 后 取得 了 大 的 突破 新的 方法 
有效地 将 RNN 网络 中 的 每个 单元 转变 成 
一个 模拟 计算机 大大 提高 了 网络 精度 作者 信息 
Jason Roell 软件 工程师 热爱 深度 学习 及其 可 改变 
技术 的 应用 Linkedin http / / www . linkedin 
. com / in / jason roell 47830817 / 本文 
由 北邮 @ 爱 可可 爱 生活 老师 推荐 阿里云 
云栖 社区 组织 翻译 文章 原 标题 Understanding Recurrent Neural 
Networks The Preferred Neural Network for Time Series Data 作者 
Jason Roel 译者 海棠 审阅 袁虎/nr 附件/n 为/p 原文/n 的/uj 
pdf/w 文章/n 为/p 简译/nr 更为 详细 的 内容 请 查看 
原文 了解 更多 技术文章 请 点击 原文 链接 