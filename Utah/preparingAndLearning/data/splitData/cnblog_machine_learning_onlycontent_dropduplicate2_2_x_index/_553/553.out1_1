摘要 本文 首先 浅谈 了 自己 对 决策树 的 理解 
进而 通过 Python 一步步 构造 决策树 并 通过 Matplotlib 更 
直观 的 绘制 树形图 最后 选取 相应 的 数据集 对 
算法 进行 测试 最近 在看 机器学习 实战 这本书 因为 一直 
想 好好 了解 机器学习 方面 的 算法 加之 想学 Python 
就在 朋友 的 推荐 之下 选择 了 这本 同等 定位 
的 书 今天 就 来 学习 一下 决策树 所有 的 
代码 均 python3 . 4 实现 确实 与 2.7 有 
很多 不同 决策树 和 KNN 一样 都是 处理 分类 问题 
的 算法 对于 决策树 的 定义 不计其数 就 我 个人 
而言 首先 单看/nr 名字 就 想到 了 最小 生成树 猜想 
图解 的话 这个 算 法会 是 一棵树 在 机器学习 这个 
层面 将 所要 处理 的 数据 看做 是 树 的 
根 相应 的 选取 数据 的 特征 作为 一个 个 
节点 决策 点 每次 选 取 一个 节点 将 数据集 
分为 不同 的 数据 子集 可以 看 成对 树 进行 
分支 这里 体现 出 了 决策 直到 最后 无法 可分 
停止 也 就是 分支 上 的 数据 为 同一 类型 
可以想象 一次次 划分 之后 由 根 延 伸出 了 许多 
分支 形象 的 说 就是 一棵树 在 机器 学习 中 
决策树 是 一个 预测模型 它 代表 的 是 对象 属性 
与 对象 值 之间 的 一种 映射 关系 我们 可以 
利用 决策树 发现 数 据 内部 所 蕴含 的 知识 
比如 在 本文 的 最后 我们 选取 隐形眼镜 数据集 根据 
决策树 学习 到 眼科 医生 是 如何 判断 患者 佩戴 
眼镜片 的 过程 而 K 近邻 算法 虽 与 决策树 
同属 分类 却 无从 得知 数据 的 内在 形式 下面 
我们 就 一步步 的 学习 决策树 1 . 构造 决策树 
基于 之前 的 了解 在 构造 决策树 首先 需要 选取 
特征 将 原始数据 划分 为 几个 数据集 那么 第一 个 
问题 就是 当前 数据 的 哪个 特征 在 划分 数据 
分类 时起/nr 决定性 作用 所以 必须 评估 每个 特征 进而 
通过 特征 将 原始 数据 就 被 划分 为 几个 
数据 子集 这些 数据 子集 分布 在 第一个 决策 点 
的 所有 分支 上 如果 分支 上 的 所有 数据 
为 同一 类型 则 划分 停止 若 分支 上 的 
所有 数据 不是 同一 类型 则 还 需要 继续 划分 
直到 所有 具有 相同 类型 的 数据 均 在 一个 
数据 子 集中 在用 决策树 进行 划分 时 关键 是 
每次 划 分时 选取 哪个 特征 进行 划分 在 划分 
数据 时 我们 必须 采用 量化 的 方法 判断 如何 
划分 数据 1 信息 增益 划分 数据 时是/nr 根据 某 
一 原则 进行 划分 使得 划分 在 同一 集合 中 
的 数据 具有 共同 的 特征 据此 我们 可以 理解 
为 划分 数据 的 原则 就是 是 无序 的 数据 
变得 有序 当然 划分 数据 有 很多 种 方法 在此 
选用 信息论 度量 信息 划分 组织 杂乱无章 的 数据 信息论 
是 量化 处理 信息 的 分支 科学 可以 在 数据 
划分 之前 或之后 使用 信息论 量化 度量 信息 的 内容 
其中 在 划分 数据集 之前 之后 信息 发生 的 变化 
称为 信息 增益 计算 每个 特征值 划分 数据集 获得 的 
信息 增益 获得 信息 增益 最高 的 特征 就是 最好 
的 选择 首先 我们 需要 知道 怎么 计算 信息 增益 
集合 信息 的 度量 方式 称为 香农 熵 或者 简称为 
熵 熵 定义 为 信息 的 期望值 那么 信息 是 
什么 xi 的 信息 可 定义 为 L xi = 
log p xi 其中 p xi 是 选择 该 分类 
的 概率 熵 指 的 是 所有 类别 所有 可能 
值 包含 的 信息 期望值 可表示 为 熵 越高 表明 
混合 的 数据 越多 则 可以 在 数据 集中 添加 
更多 的 分类 基于 上述 的 分析 编程 计算 给定 
数据集 的 香农 熵 代码 如下 from math import log 
# # # 计算 香农 熵 为 float 类型 def 
calShang dataSet numEntries = len dataSet labelCounts = { } 
# # 创建 字典 for featVec in dataSet currentLabel = 
featVec 1 if currentLabel not in labelCounts . keys labelCounts 
currentLabel = 0 labelCounts currentLabel + = 1 shannonEnt = 
0.0 for key in labelCounts prob = float labelCounts key 
/ numEntries shannonEnt = prob * log prob 2 return 
shannonEnt 对此 我们 可以 输入 数据集 测试 def creatDataSet dataSet 
= 1 1 yes 1 1 yes 1 0 no 
0 1 no 0 1 no labels = no surfacing 
flippers return dataSet labels # 测试 myData labels = creatDataSet 
print 原 数据 为 myData print 标签 为 labels shang 
= calShang myData print 香农 熵 为 shang 得到 熵 
之后 我们 就 可以 按照 获取 最大 增益 的 办法 
划分 数据集 2 划分 数据集 基于 之前 的 分析 信息 
增益 表示 的 是 信息 的 变化 而 信息 可以 
用 熵 来 度量 所以 我们 可以 用 熵 的 
变化 来 表示 信息 增益 而 获得 最高 信息 增益 
的 特征 就是 最好 的 选择 故此 我们 可以 对 
所有 特征 遍历 得到 最高 信息 增益 的 特征 加以 
选择 首先 我们 按照 给定 特征 划分 数据集 并 进行 
简单 的 测试 # # # 划分 数据集 以 指定 
特征 将 数据 进行 划分 def splitDataSet dataSet feature value 
# # 传入 待 划分 的 数据集 划分 数据集 的 
特征 以及 需要 返回 的 特征 的 值 newDataSet = 
for featVec in dataSet if featVec feature = = value 
reducedFeatVec = featVec feature reducedFeatVec . extend featVec feature + 
1 newDataSet . append reducedFeatVec return newDataSet # 测试 myData 
labels = creatDataSet print 原 数据 为 myData print 标签 
为 labels split = splitDataSet myData 0 1 print 划分 
后的/nr 结果 为 split 接下来 我们 遍历 整个 数据集 循环 
计算 香农 熵 和 splitDataSet 函数 找到 最好 的 划分 
方式 并 简单 测试 # # 选择 最好 的 划分 
方式 选取 每个 特征 划分 数据集 从中 选取 信息 增益 
最大 的 作为 最优 划分 在 这里 体现 了 信息 
增益 的 概念 def chooseBest dataSet featNum = len dataSet 
0 1 baseEntropy = calShang dataSet bestInforGain = 0.0 bestFeat 
= 1 # # 表示 最好 划分 特征 的 下标 
for i in range featNum featList = example i for 
example in dataSet # 列表 uniqueFeat = set featList # 
# 得到 每个 特征 中 所含 的 不同 元素 newEntropy 
= 0.0 for value in uniqueFeat subDataSet = splitDataSet dataSet 
i value prob = len subDataSet / len dataSet newEntropy 
+ = prob * calShang subDataSet inforGain = baseEntropy newEntropy 
if inforGain bestInforGain bestInforGain = inforGain bestFeature = i # 
第 i 个 特征 是 最 有利于 划分 的 特征 
return bestFeature # # 测试 myData labels = creatDataSet best 
= chooseBest myData print best 3 递归 构建 决策树 基于 
之前 的 分析 我们 选取 划分 结果 最好 的 特征 
划分 数据集 由于 特征 很 可能 多 与 两个 因此 
可能 存在 大 于 两个 分支 的 数据集 划分 第一 
次 划分 之后 可以 将 划分 的 数据 继续 向下 
传递 如果 将 每一个 划分 的 数据 看成 是 原 
数据集 那么/r 之后/f 的/uj 每一次/i 划分/v 都/d 可以/c 看成/v 是/v 
和/c 第一次/m 划分/v 相同/d 的/uj 过程/n 据此 我们 可以 采用 
递归 的 原则 处理 数据集 递归 结束 的 条件 是 
程序 遍历 完 所有 划分 数据集 的 属性 或者 每个 
分支 下 的 所有 实例 都有 相同 的 分类 编程 
实现 # # 递归 构建 决策树 import operator # 返回 
出现 次数 最多 的 分类 名称 def majorClass classList classCount 
= { } for vote in classList if vote not 
in classCount . keys classCount vote = 0 classCount vote 
+ = 1 # 降序 排序 可以 指定 reverse = 
true sortedClassCount = sorted classcount . iteritems key = operator 
. itemgetter 1 reverse = true return sortedClassCount 0 0 
# 创 建树 def creatTree dataSet labels classList = example 
1 for example in dataSet if classList . count classList 
0 = = len classList return classList 0 if len 
dataSet 0 = = 1 return majorClass classList bestFeat = 
chooseBest dataSet bestFeatLabel = labels bestFeat myTree = { bestFeatLabel 
{ } } del labels bestFeat featValues = example bestFeat 
for example in dataSet uniqueVals = set featValues for value 
in uniqueVals subLabels = labels myTree bestFeatLabel value = creatTree 
splitDataSet dataSet bestFeat value subLabels return myTree # 测试 myData 
labels = creatDataSet mytree = creatTree myData labels print mytree 
2 . 使用 matplotlib 注解 绘制 树形图 之前 我们 已经 
从 数据 集中 成功 的 创建 了 决策树 但是 字典 
的 形式 非常 的 不 易于 理解 因此 本节 采用 
Matplotlib 库 创建 树形图 首先 使用 文本 注解 绘制 树节点 
# # 采用 matplotlib 绘制 树形图 import matplotlib . pyplot 
as plt decisionNode = dict boxstyle = sawtooth fc = 
0.8 leafNode = dict boxstyle = round4 fc = 0.8 
arrow _ args = dict arrowstyle = # 绘制 树节点 
def plotNode nodeTxt centerPt parentPt nodeType createPlot . ax1 . 
annotate nodeTxt xy = parentPt xycoords = axes fraction xytext 
= centerPt textcoords = axes fraction va = center ha 
= center bbox = nodeType arrowprops = arrow _ args 
获得/v 叶/nr 节点/n 的/uj 数目/n 和树的/nr 层数/n 并 进行 测试 
# # 获取 节点 的 数目 和树的/nr 层数 def getNumLeafs 
myTree numLeafs = 0 # firstStr = myTree . keys 
0 firstSides = list myTree . keys firstStr = firstSides 
0 # 找到 输入 的 第一 个 元素 secondDict = 
myTree firstStr for key in secondDict . keys if type 
secondDict key = = dict numLeafs + = getNumLeafs secondDict 
key else numLeafs + = 1 return numLeafs def getTreeDepth 
myTree maxDepth = 1 firstSides = list myTree . keys 
firstStr = firstSides 0 # 找到 输入 的 第一 个 
元素 # firstStr = myTree . keys 0 secondDict = 
myTree firstStr for key in secondDict . keys if type 
secondDict key = = dict thisDepth = 1 + getTreeDepth 
secondDict key else thisDepth = 1 if thisDepth maxDepth maxDepth 
= thisDepth return maxDepth def retrieveTree i listOfTrees = { 
no surfacing { 0 no 1 { flippers { 0 
no 1 yes } } } } { no surfacing 
{ 0 no 1 { flippers { 0 { head 
{ 0 no 1 yes } } 1 no } 
} } } return listOfTrees i # 测试 mytree = 
retrieveTree 0 print getNumLeafs mytree print getTreeDepth mytree 在此 我们 
说明 一下 Python2 . 7 和 3.4 在 实现 本段 
代码 的 区别 在 2.7中 找到 key 所 对应 的 
第一 个 元素 为 firstStr = myTree . keys 0 
这在 3.4 中运 行会 报错 dict _ keys object does 
not support indexing 这 是因为 python3 改变 了 dict . 
keys 返回 的 是 dict _ keys 对象 支持 iterable 
  但 不支持 indexable 我们 可以 将 其 明确 的 
转化 成 list 则 此项 功能 在 3中 应 这样 
实现 firstSides = list myTree . keys firstStr = firstSides 
0 # 找到 输入 的 第一 个 元素 绘制 树 
def plotNode nodeTxt centerPt parentPt nodeType createPlot . ax1 . 
annotate nodeTxt xy = parentPt xycoords = axes fraction xytext 
= centerPt textcoords = axes fraction va = center ha 
= center bbox = nodeType arrowprops = arrow _ args 
def plotMidText cntrPt parentPt txtString xMid = parentPt 0 cntrPt 
0 / 2.0 + cntrPt 0 yMid = parentPt 1 
cntrPt 1 / 2.0 + cntrPt 1 createPlot . ax1 
. text xMid yMid txtString va = center ha = 
center rotation = 30 def plotTree myTree parentPt nodeTxt numLeafs 
= getNumLeafs myTree depth = getTreeDepth myTree firstSides = list 
myTree . keys firstStr = firstSides 0 # 找到 输入 
的 第一 个 元素 cntrPt = plotTree . xOff + 
1.0 + float numLeafs / 2.0 / plotTree . totalW 
plotTree . yOff plotMidText cntrPt parentPt nodeTxt plotNode firstStr cntrPt 
parentPt decisionNode secondDict = myTree firstStr plotTree . yOff = 
plotTree . yOff 1.0 / plotTree . totalD for key 
in secondDict . keys if type secondDict key . _ 
_ name _ _ = = dict plotTree secondDict key 
cntrPt str key else plotTree . xOff = plotTree . 
xOff + 1.0 / plotTree . totalW plotNode secondDict key 
plotTree . xOff plotTree . yOff cntrPt leafNode plotMidText plotTree 
. xOff plotTree . yOff cntrPt str key plotTree . 
yOff = plotTree . yOff + 1.0 / plotTree . 
totalD def createPlot inTree fig = plt . figure 1 
facecolor = white fig . clf axprops = dict xticks 
= yticks = createPlot . ax1 = plt . subplot 
111 frameon = False * * axprops plotTree . totalW 
= float getNumLeafs inTree plotTree . totalD = float getTreeDepth 
inTree plotTree . xOff = 0.5 / plotTree . totalW 
plotTree . yOff = 1.0 plotTree inTree 0.5 1.0 plt 
. show # 测试 mytree = retrieveTree 0 print mytree 
createPlot mytree 测试 之后 结果 如下 这样 相比 于 字典 
形式 确实 清晰 了 很多 3 . 测试 算法 在 
本 章中 我们 首先 使用 决策树 对 实际 数据 进行 
分类 然后 使用 决策树 预测 隐形眼镜 类型 对 算法 进行 
验证 1 使用 决策树 执行 分类 在 使用 了 训练 
数据 构造 了 决策树 之后 我们 便 可以 将 它 
用于 实际 数据 的 分类 # # # 决策树 的 
分类 函数 返回 当前 节点 的 分类 标签 def classify 
inputTree featLabels testVec # # 传入 的 数据 为 dict 
类型 firstSides = list inputTree . keys firstStr = firstSides 
0 # 找到 输入 的 第一 个 元素 # # 
这里 表明 了 python3 和 python2 版本 的 差别 上述 
两行 代码 在 2.7 中为 firstStr = inputTree . key 
0 secondDict = inputTree firstStr # # 建 一个 dict 
# print secondDict featIndex = featLabels . index firstStr # 
找到 在 label 中 firstStr 的 下标 for i in 
secondDict . keys print i for key in secondDict . 
keys if testVec featIndex = = key if type secondDict 
key = = dict # # # 判断 一个 变量 
是否 为 dict 直接 type 就好 classLabel = classify secondDict 
key featLabels testVec else classLabel = secondDict key return classLabel 
# # 比较 测试数据 中的 值 和 树上 的 值 
最后 得到 节点 # 测试 myData labels = creatDataSet print 
labels mytree = retrieveTree 0 print mytree classify = classify 
mytree labels 1 0 print classify 2 使用 决策树 预测 
隐形眼镜 类型 基于 之前 的 分析 我们 知道 可以 根据 
决策树 学习 到 眼科 医生 是 如何 判断 患者 需要 
佩戴 的 眼镜片 据此 我们 可以 帮助 人们 判断 需要 
佩戴 的 镜片 类型 在此 从 UCI 数据库 中 选取 
隐形眼镜 数据集 lenses . txt 它 包含 了 很多 患者 
眼部 状况 的 观察 条件 以及 医生 推荐 的 隐形眼镜 
类型 我们 选取 此 数据集 结合 Matplotlib 绘制 树形图 进一步 
观察 决策树 是 如何 工作 的 具体 的 代码 如下 
fr = open lenses . txt lenses = inst . 
strip . split \ t for inst in fr . 
readlines lensesLabels = ages prescript astigmatic tearRate lensesTree = creatTree 
lenses lensesLabels print lensesTree createPlot lensesTree 得到 的 树形图 沿着 
决策树 的 不同 分支 我们 可以 得到 不同 患者 需要 
佩戴 的 隐形眼镜 类型 从 该图 中 我们 可以 得到 
只 需要 问 四个 问题 就 可以 确定 出 患者 
需要 佩戴 何种 隐形眼镜 本章 主要 使用 的 是 ID3 
算法 自身 也 存在 着 很多 不足 当然 还有 其它 
的 决策 树 构造 算法 比如 C 4.5 和 CART 
以后 有 机会 了 再 好好 看看 以上 是 我 
自己 的 一些 理解 与 总结 难免 有错 望 大家 
不吝指教 ~ 