写 在前面 本来 这篇 应该是 上周四 更新 但是 上周四 写了 
一篇 深度 学习 的 反向 传播 法的/nr 过程 就 推迟 
更新 了 本来 想 参考 PRML 来写 但是 发现 里面 
涉及 到 比较 多 的 数学 知识 写出来 可能 不 
好 理解 我 决定 还是 用 最 通俗 的 方法 
解释 PCA 并举 一个 实例 一步步 计算 然后 再 进行 
数学 推导 最后 再 介绍 一些 变种 以及 相应 的 
程序 数学 推导 及 变种 下次 再写 好了 正文 在 
数据 处理 中 经常 会 遇到 特征 维度 比 样本 
数量 多得多 的 情况 如果 拿 到 实际 工程 中去 
跑 效果 不 一定 好 一 是因为 冗余 的 特征 
会 带来 一些 噪音 影响 计算 的 结果 二 是因为 
无关 的 特征 会 加大 计算 量 耗费 时间 和 
资源 所以 我们 通常 会 对 数据 重新 变换 一下 
再 跑 模型 数据 变换 的 目的 不 仅仅 是 
降 维 还 可以 消除 特征 之间 的 相关性 并 
发现 一些 潜在 的 特征 变量 一 PCA 的 目的 
PCA 是 一种 在 尽可能 减少 信息 损失 的 情况 
下 找到 某种 方式 降低 数据 的 维度 的 方法 
通常 来说 我们 期望 得到 的 结果 是 把 原始数据 
的 特征 空间 n 个 d 维 样本 投影 到 
一个 小 一点 的 子空间 里 去 并 尽可能 表达 
的 很好 就是说 损失 信息 最少 常见 的 应用 在于 
模式识别 中 我们 可以 通过 减少 特征 空间 的 维度 
抽取 子空间 的 数据 来 最好 的 表达 我们 的 
数据 从而 减少 参数估计 的 误差 注意 主 成分 分析 
通常 会 得到 协方差 矩阵 和 相关矩阵 这些 矩阵 可以 
通过 原始数据 计算出来 协方差 矩阵 包含 平方和 与 向量积 的 
和 相关矩阵 与 协方差 矩阵 类似 但 是 第一 个 
变量 也 就是 第一 列 是 标准化 后的/nr 数据 如果 
变量 之间 的 方差 很大 或者 变量 的 量纲 不统一 
我们 必须 先 标准化 再 进行 主 成分 分析 二 
PCA VS MDA 提到 PCA 可能 有些 人 会 想到 
MDA Multiple Discriminate Analysis 多元 判别 分析法 这两者 都是 线性变换 
而且 很 相似 只不过 在 PCA 中 我们 是 找到 
一个 成分 方向 来 把 我们 的 数据 最大化 方差 
而在 MDA 中 我们 的 目标 是 最大化 不同 类别 
之间 的 差异 比如说 在 模式识别 问题 中 我们 的 
数据 包含 多个 类别 与 两个 主 成分 的 PCA 
相比 这就 忽略 了 类别 标签 换句话说 通过 PCA 我们 
把 整个 数据集 不含 类别 标签 投射 到 一个 不同 
的 子空间 中 在 MDA 中 我们 试图 决定 一个 
合适 的 子空间 来 区分 不同 类别 再 换 种 
方式 说 PCA 是 找到 数据 传播 最广 的 时候 
的 最大 方差 的 轴 axis MDA 是 最大化 类别 
与 类别 之间 的 区别 上文 我们 提到 了 子空间 
那么 怎么样 去 寻找 好 的 子空间 呢 假设 我们 
的 目标 是 减少 d 维 的 数据集 将其 投影 
到 k 维 的 子空间 上 看 k d 所以 
我们 如何 来 确定 k 呢 如何 知道 我们 选择 
的 特征 空间 能够 很好 的 表达 原始数据 呢 下文 
中 我们 会 计算 数据 中的 特征向量 主 成分 然后 
计算 散布 矩阵 scatter _ matrix 中 也 可以 从 
协方差 矩阵 中 计算 每个 特征向量 与 特征值 相关 即 
特征向量 的 长度 或 大小 如果 发现 每个 特征值 都 
很小 那就/nr 可以 说明 我们 的 原始数据 就 已经 是 
一个 好 的 空间 了 但是 如果 有些 特征值 比 
其他 值 要 大得多 我们 只 需要 关注 那些 特别 
大 的 特征值 因为 这些 值 包含 了 数据 分布 
情况 的 绝大部分 信息 反之 那些 接近于 0 的 特征值 
包含 的 信息 几乎 没有 在 新的 特征 空间 里 
我们 可以 忽略不计 三 PCA 的 过程 通常 来说 有 
以下 六步 1 . 去掉 数据 的 类别 特征 label 
将 去掉 后的d/nr 维 数据 作为 样本 2 . 计算 
d 维 的 均值 向量 即 所有 数据 的 每 
一维 向量 的 均值 3 . 计算所 有 数据 的 
散布 矩阵 或者 协方差 矩阵 4 . 计算 特征值 e1 
e2 . . . ed 以及 相应 的 特征向量 lambda1 
lambda2 . . . lambda d 5 . 按照 特征值 
的 大小 对 特征向量 降序 排序 选择 前 k 个 
最大 的 特征向量 组成 d * k 维 的 矩阵 
W 其中 每 一列 代表 一个 特征向量 6 . 运用 
d * K 的 特征向量 矩阵 W 将 样本数据 变换 
成 新的 子空间 用 数学 式子 表达 就是 其中 x 
是 d * 1 维 的 向量 代表 一个 样本 
y 是 K * 1 维 的 在 新的 子空间 
里 的 向量 四 具体步骤 1 . 数据 准备 生成 
三维 样本 向量 首先 随机 生成 40 * 3 维 
的 数据 符合 多元 高斯分布 假设 数据 被 分为 两类 
其中 一半 类别 为 w1 另一半 类别 为 w21 # 
coding utf 8 2 import numpy as np 3 4 
np . random . seed 4294967295 5 6 mu _ 
vec1 = np . array 0 0 0 7 cov 
_ mat1 = np . array 1 0 0 0 
1 0 0 0 1 8 class1 _ sample = 
np . random . multivariate _ normal mu _ vec1 
cov _ mat1 20 . T 9 assert class1 _ 
sample . shape = = 3 20 # 检验 数据 
的 维度 是否 为 3 * 20 若 不为 3 
* 20 则 抛出 异常 10 11 mu _ vec2 
= np . array 1 1 1 12 cov _ 
mat2 = np . array 1 0 0 0 1 
0 0 0 1 13 class2 _ sample = np 
. random . multivariate _ normal mu _ vec2 cov 
_ mat2 20 . T 14 assert class1 _ sample 
. shape = = 3 20 # 检验 数据 的 
维度 是否 为 3 * 20 若 不为 3 * 
20 则 抛出 异常 运行 这段 代码 后 我们 就 
生成 了 包含 两个 类别 的 样本数据 其中 每 一列 
都是 一个三维 的 向量 所有 数据 是 这样 的 矩阵 
结果 2 . 作图 查看 原始数据 的 分布 1 from 
matplotlib import pyplot as plt 2 from mpl _ toolkits 
. mplot3d import Axes3D 3 from mpl _ toolkits . 
mplot3d import proj3d 4 5 fig = plt . figure 
figsize = 8 8 6 ax = fig . add 
_ subplot 111 projection = 3d 7 plt . rcParams 
legend . fontsize = 10 8 ax . plot class1 
_ sample 0 class1 _ sample 1 class1 _ sample 
2 o markersize = 8 color = blue alpha = 
0.5 label = class1 9 ax . plot class2 _ 
sample 0 class2 _ sample 1 class2 _ sample 2 
^ markersize = 8 alpha = 0.5 color = red 
label = class2 10 11 plt . title Samples for 
class 1 and class 2 12 ax . legend loc 
= upper right 13 14 plt . show 结果 3 
. 去掉 数据 的 类别 特征 1 all _ samples 
= np . concatenate class1 _ sample class2 _ sample 
axis = 1 2 assert all _ samples . shape 
= = 3 40 # 检验 数据 的 维度 是否 
为 3 * 20 若 不为 3 * 20 则 
抛出 异常 4 . 计算 d 维 向量 均值 1 
mean _ x = np . mean all _ samples 
0 2 mean _ y = np . mean all 
_ samples 1 3 mean _ z = np . 
mean all _ samples 2 4 5 mean _ vector 
= np . array mean _ x mean _ y 
mean _ z 6 7 print Mean Vector \ n 
mean _ vector 结果 1 print Mean Vector \ n 
mean _ vector 2 Mean Vector 3 array 0.68047077 4 
0.52975093 5 0.43787182 5 . 计算 散步 矩阵 或者 协方差 
矩阵 a . 计算 散步 矩阵 散布 矩阵 公式 其中 
m 是 向量 的 均值 第 4步 已经 算 出来 
是 mean _ vector 1 scatter _ matrix = np 
. zeros 3 3 2 for i in range all 
_ samples . shape 1 3 scatter _ matrix + 
= all _ samples i . reshape 3 1 mean 
_ vector . dot all _ samples i . reshape 
3 1 mean _ vector . T 4 print Scatter 
Matrix \ n scatter _ matrix 结果 1 print Scatter 
Matrix \ n scatter _ matrix 2 Scatter Matrix 3 
array 46.81069724 13.95578062 27.08660175 4 13.95578062 48.28401947 11.32856266 5 27.08660175 
11.32856266 50.51724488 b . 计算 协方差 矩阵 如果 不 计算 
散布 矩阵 的话 也 可以 用 python 里 内置 的 
numpy . cov 函数 直接 计算 协方差 矩阵 因为 散步 
矩阵 和 协方差 矩阵 非常 类似 散布 矩阵 乘以 1 
/ N 1 就是 协方差 所以 他们 的 特征 空间 
是 完全 等价 的 特征向量 相同 特征值 用 一个 常数 
1 / N 1 这里 是 1/39 等价 缩 放了 
协方差 矩阵 如下 所示 1 cov _ mat = np 
. cov all _ samples 0 all _ samples 1 
all _ samples 2 2 print Covariance Matrix \ n 
cov _ mat 结果 1 print Covariance Matrix \ n 
cov _ mat 2 Covariance Matrix 3 array 1.20027429 0.35784053 
0.69452825 4 0.35784053 1.23805178 0.29047597 5 0.69452825 0.29047597 1.29531397 6 
. 计算 相应 的 特征向量 和 特征值 1 # 通过 
散布 矩阵 计算 特征值 和 特征向量 2 eig _ val 
_ sc eig _ vec _ sc = np . 
linalg . eig scatter _ matrix 3 4 # 通过 
协方差 矩阵 计算 特征值 和 特征向量 5 eig _ val 
_ cov eig _ vec _ cov = np . 
linalg . eig cov _ mat 6 7 for i 
in range len eig _ val _ sc 8 eigvec 
_ sc = eig _ vec _ sc i . 
reshape 1 3 . T 9 eigvec _ cov = 
eig _ vec _ cov i . reshape 1 3 
. T 10 assert eigvec _ sc . all = 
= eigvec _ cov . all 11 12 print Eigenvector 
{ } \ n { } . format i + 
1 eigvec _ sc 13 print Eigenvalue { } from 
scatter matrix { } . format i + 1 eig 
_ val _ sc i 14 print Eigenvalue { } 
from covariance matrix { } . format i + 1 
eig _ val _ cov i 15 print Scaling factor 
eig _ val _ sc i / eig _ val 
_ cov i 16 print 40 * 结果 1 Eigenvector 
1 2 0.84190486 3 0.39978877 4 0.36244329 5 Eigenvalue 1 
from scatter matrix 55 . 398855957302445 6 Eigenvalue 1 from 
covariance matrix 1 . 4204834860846791 7 Scaling factor 39.0 8 
9 Eigenvector 2 10 0.44565232 11 0.13637858 12 0.88475697 13 
Eigenvalue 2 from scatter matrix 32 . 42754801292286 14 Eigenvalue 
2 from covariance matrix 0 . 8314755900749456 15 Scaling factor 
39.0 16 17 Eigenvector 3 18 0.30428639 19 0.90640489 20 
0.29298458 21 Eigenvalue 3 from scatter matrix 34 . 65493432806495 
22 Eigenvalue 3 from covariance matrix 0 . 8885880596939733 23 
Scaling factor 39.0 24 其实 从 上面 的 结果 就 
可以 发现 通过 散布 矩阵 和 协方差 矩阵 计算 的 
特征 空间 相同 协方差 矩阵 的 特征值 * 39 = 
散布 矩阵 的 特征值 当然 我们 也 可以 快速 验证 
一下 特征值 特征向量 的 计算 是否 正确 是不是 满足 方程 
其中 为 协方差 矩阵 v 为 特征向量 lambda 为 特征值 
1 for i in range len eig _ val _ 
sc 2 eigv = eig _ vec _ sc i 
. reshape 1 3 . T 3 np . testing 
. assert _ array _ almost _ equal scatter _ 
matrix . dot eigv eig _ val _ sc i 
* eigv decimal = 6 err _ msg = verbose 
= True 得出 结果 未 返回 异常 证明 计算 正确 
注 np . testing . assert _ array _ almost 
_ equal 计算 得出 的 结果 不 一样 会 返回 
一下 结果 1 np . testing . assert _ array 
_ almost _ equal 1.0 2.33333 np . nan 2 
. . . 1.0 2.33339 np . nan decimal = 
5 3 . . . 4 type exceptions . AssertionError 
5 AssertionError 6 Arrays are not almost equal 7 8 
mismatch 50.0% 9 x array 1 . 2.33333 NaN 10 
y array 1 . 2.33339 NaN 可视化 特征向量 1 from 
matplotlib import pyplot as plt 2 from mpl _ toolkits 
. mplot3d import Axes3D 3 from mpl _ toolkits . 
mplot3d import proj3d 4 from matplotlib . patches import FancyArrowPatch 
5 6 7 class Arrow3D FancyArrowPatch 8 def _ _ 
init _ _ self xs ys zs * args * 
* kwargs 9 FancyArrowPatch . _ _ init _ _ 
self 0 0 0 0 * args * * kwargs 
10 self . _ verts3d = xs ys zs 11 
12 def draw self renderer 13 xs3d ys3d zs3d = 
self . _ verts3d 14 xs ys zs = proj3d 
. proj _ transform xs3d ys3d zs3d renderer . M 
15 self . set _ positions xs 0 ys 0 
xs 1 ys 1 16 FancyArrowPatch . draw self renderer 
17 18 fig = plt . figure figsize = 7 
7 19 ax = fig . add _ subplot 111 
projection = 3d 20 21 ax . plot all _ 
samples 0 all _ samples 1 all _ samples 2 
o markersize = 8 color = green alpha = 0.2 
22 ax . plot mean _ x mean _ y 
mean _ z o markersize = 10 color = red 
alpha = 0.5 23 for v in eig _ vec 
_ sc . T 24 a = Arrow3D mean _ 
x v 0 mean _ y v 1 mean _ 
z v 2 mutation _ scale = 20 lw = 
3 arrowstyle = | color = r 25 ax . 
add _ artist a 26 ax . set _ xlabel 
x _ values 27 ax . set _ ylabel y 
_ values 28 ax . set _ zlabel z _ 
values 29 30 plt . title Eigenvectors 31 32 plt 
. show 结果 7 . 根据 特征值 对 特征向量 降序 
排列 我们 的 目标 是 减少 特征 空间 的 维度 
即 通过 PCA 方法 将 特征 空间 投影 到 一个 
小 一点 的 子空间 里 其中 特征向量 将会 构成 新的 
特征 空间 的 轴 然而 特征向量 只会 决定 轴 的 
方向 他们 的 单位 长度 都为 1 可以 用 代码 
检验 一下 1 for ev in eig _ vec _ 
sc 2 numpy . testing . assert _ array _ 
almost _ equal 1.0 np . linalg . norm ev 
因此 对于 低维 的 子空间 来说 决定 丢掉 哪个 特征向量 
就必须 参考 特征向量 相应 的 特征值 通俗 来说 如果 一个 
特征向量 的 特征值 特别小 那它 所 包含 的 数据 分布 
的 信息 也 很少 那么 这个 特征向量 就 可以 忽略不计 
了 常用 的 方法 是 根据 特征值 对 特征向量 进行 
降序 排列 选出 前 k 个 特征向量 1 # 生成 
特征向量 特征值 元祖 2 eig _ pairs = np . 
abs eig _ val _ sc i eig _ vec 
_ sc i for i in range len eig _ 
val _ sc 3 4 # 对 特征向量 特征值 元祖 
按照 降序 排列 5 eig _ pairs . sort key 
= lambda x x 0 reverse = True 6 7 
# 输出 值 8 for i in eig _ pairs 
9 print i 0 结果 1 84.5729942896 2 39.811391232 3 
21.22757606828 . 选出 前 k 个 特征 值 最大 的 
特征向量 本文 的 例子 是 想把 三维 的 空间 降 
维 成二/nr 维空间 现在 我们 把 前 两个 最大 特征值 
的 特征向量 组合 起来 生成 d * k 维 的 
特征向量 矩阵 W1 matrix _ w = np . hstack 
eig _ pairs 0 1 . reshape 3 1 eig 
_ pairs 1 1 . reshape 3 1 2 print 
Matrix W \ n matrix _ w 结果 1 print 
Matrix W \ n matrix _ w 2 Matrix W 
3 array 0.62497663 0.2126888 4 0.44135959 0.88989795 5 0.643899 0.40354071 
9 . 将 样本 转化 为 新的 特征 空间 最后 
一步 把 2 * 3 维 的 特征向量 矩阵 W 
带到 公式 中 将 样本数据 转化 为 新的 特征 空间 
1 matrix _ w = np . hstack eig _ 
pairs 0 1 . reshape 3 1 eig _ pairs 
1 1 . reshape 3 1 2 print Matrix W 
\ n matrix _ w 3 4 5 transformed = 
matrix _ w . T . dot all _ samples 
6 assert transformed . shape = = 2 40 The 
matrix is not 2x40 dimensional . 7 8 9 plt 
. plot transformed 0 0 20 transformed 1 0 20 
o markersize = 7 color = blue alpha = 0.5 
label = class1 10 plt . plot transformed 0 20 
40 transformed 1 20 40 ^ markersize = 7 color 
= red alpha = 0.5 label = class2 11 plt 
. xlim 4 4 12 plt . ylim 4 4 
13 plt . xlabel x _ values 14 plt . 
ylabel y _ values 15 plt . legend 16 plt 
. title Transformed samples with class labels 17 18 plt 
. show 结果 到 这一步 PCA 的 过程 就 结束 
了 其实 python 里 有 已经 写好 的 模块 可以 
直接 拿来 用 但是 我 觉得 不管 什么 模块 都要 
懂得 它 的 原理 是 什么 matplotlib 有 matplotlib . 
mlab . PCA sklearn 也有 专门 一个 模块 Dimensionality reduction 
专门 讲 PCA 包括 传统 的 PCA 也 就是 我 
上文 写 的 以及 增量 PCA 核 PCA 等等 除了 
PCA 以外 还有 ZCA 白化 等等 在 图像 处理 中 
也 经常 会 用到 内容 太多 下次 再写 最后 推荐 
一个 博客 动态 展示 了 PCA 的 过程 http / 
/ setosa . io / ev / principal component analysis 
/   写 的 也很 清楚 可以 看 一下 再 
推荐 一个 维基 百科 的 讲 的 真的 是 详细 
啊 https / / en . wikipedia . org / 
wiki / Principal _ component _ analysis 本 博客 所有 
内容 以 学习 研究 和 分享 为主 如需 转载 请 
联系 本人 标明 作者 和 出处 并且 是 非 商业 
用途 谢谢 