先来看看自维基百科的定义
什么是人工智能？
人工智能（Artificial Intelligence, AI）亦称机器智能，是指由人制造出来的机器所表现出来的智能。通常人工智能是指通过普通电脑程式的手段实现的类人智能技术。该词同时也指研究这样的智能系统是否能够实现，以及如何实现的科学领域。
一般教材中的定义领域是“智能主体（intelligent agent）的研究与设计”，智能主体是指一个可以观察周遭环境并作出行动以达致目标的系统。约翰·麦卡锡于1955年的定义是“制造智能机器的科学与工程。”
人工智能的研究是高度技术性和专业的，各分支领域都是深入且各不相通的，因而涉及范围极广。人工智能的研究可以分为几个技术问题。其分支领域主要集中在解决具体问题，其中之一是，如何使用各种不同的工具完成特定的应用程序。
AI的核心问题包括建构能够跟人类似甚至超越的推理、知识、规划、学习、交流、感知、移动和操作物体的能力等。强人工智能目前仍然是该领域的长远目标。目前强人工智能已经有初步成果，甚至在一些影像辨识、语言分析、棋类游戏等等单方面的能力达到了超越人类的水平，而且人工智能的通用性代表着，能解决上述的问题的是一样的AI程式，无须重新开发算法就可以直接使用现有的AI完成任务，与人类的处理能力相同，但达到具备思考能力的统合强人工智能还需要时间研究，比较流行的方法包括统计方法，计算智能和传统意义的AI。目前有大量的工具应用了人工智能，其中包括搜索和数学优化、逻辑推演。而基于仿生学、认知心理学，以及基于概率论和经济学的算法等等也在逐步探索当中。
概论
人工智能的定义可以分为两部分，即“人工”和“智能”。“人工”比较好理解，争议性也不大。有时我们会要考虑什么是人力所能及制造的，或者人自身的智能程度有没有高到可以创造人工智能的地步，等等。但总括来说，“人工系统”就是通常意义下的人工系统。
关于什么是“智能”，就问题多多了。这涉及到其它诸如意识（consciousness）、自我（self）、心灵（mind），包括潜意识（unconscious mind）等等问题。人唯一了解的智能是人本身的智能，这是普遍认同的观点。但是我们对我们自身智能的理解都非常有限，对构成人的智能必要元素的了解也很有限，所以就很难定义什么是“人工”制造的“智能”了。因此人工智能的研究往往涉及对人智能本身的研究。其它关于动物或其它人造系统的智能也普遍被认为是人工智能相关的研究课题。
人工智能目前在计算机领域内，得到了愈加广泛的发挥。并在机器人、经济政治决策、控制系统、仿真系统中得到应用。
什么是机器学习？
机器学习是人工智能的一个分支。人工智能的研究是从以“推理”为重点到以“知识”为重点，再到以“学习”为重点，一条自然、清晰的脉络。显然，机器学习是实现人工智能的一个途径，即以机器学习为手段解决人工智能中的问题。机器学习在近30多年已发展为一门多领域交叉学科，涉及概率论、统计学、逼近论、、计算复杂性理论等多门学科。机器学习理论主要是设计和分析一些让计算机可以自动“学习”的算法。机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。因为学习算法中涉及了大量的统计学理论，机器学习与推断统计学联系尤为密切，也被称为统计学习理论。算法设计方面，机器学习理论关注可以实现的，行之有效的学习算法。很多推论问题属于无程序可循难度，所以部分的机器学习研究是开发容易处理的近似算法。
机器学习已广泛应用于数据挖掘、计算机视觉、自然语言处理、生物特征识别、搜索引擎、诊断、检测信用卡欺诈、证券市场分析、DNA序列测序、语音识别和手写识别识别、战略游戏和机器人等领域。
定义
机器学习有下面几种定义：
机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在经验学习中改善具体算法的性能。
机器学习是对能通过经验自动改进的计算机算法的研究。
机器学习是用数据或以往的经验，以此优化计算机程序的性能标准。
一种经常引用的英文定义是：A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.
分类
机器学习可以分成下面几种类别：
监督学习从给定的训练数据集中学习出一个函数，当新的数据到来时，可以根据这个函数预测结果。监督学习的训练集要求是包括输入和输出，也可以说是特征和目标。训练集中的目标是由人标注的。常见的监督学习算法包括回归分析和统计分类。
监督学习和非监督学习的差别就是训练集目标是否人标注。他们都有训练集 且都有输入和输出
非监督式学习与监督学习相比，训练集没有人为标注的结果。常见的无监督学习算法有聚类。
半监督学习介于监督学习与无监督学习之间。
强化学习通过观察来学习做成如何的动作。每个动作都会对环境有所影响，学习对象根据观察到的周围环境的反馈来做出判断。
算法
具体的机器学习算法有：
构造间隔理论分布：聚类分析和模式识别
人工神经网络
决策树
感知器
支持向量机
集成学习AdaBoost
降维与度量学习
聚类
朴素贝叶斯分类器
构造条件概率：回归分析和统计分类
Kriging
线性判别分析
最近邻居法
径向基函数核
通过再生模型构造概率密度函数：
最大期望算法
图模式：包括贝氏网路和Markov随机场
Generative Topographic Mapping
近似推断技术：
马尔可夫链
蒙特卡罗方法
变分法
最优化：大多数以上方法，直接或者间接使用最优化算法。
什么是深度学习？
深度学习（deep learning）是机器学习的分支，是一种试图使用包含复杂结构或由多重非线性变换构成的多个处理层对数据进行高层抽象的算法。
深度学习是机器学习中一种基于对数据进行表征学习的算法。观测值（例如一幅图像）可以使用多种方式来表示，如每个像素强度值的向量，或者更抽象地表示成一系列边、特定形状的区域尺度不变特征转换。而使用某些特定的表示方法更容易从实例中学习任务（例如，人脸识别或面部表情识别）。深度学习的好处是用非监督式学习或的特征学习和分层特征提取高效算法来替代手工获取。
表征学习的目标是寻求更好的表示方法并建立更好的模型来从大规模未标记数据中学习这些表示方法。表示方法来自神经科学，并松散地建立在类似神经系统中的信息处理和对通信模式的理解上，如神经编码，试图定义拉动神经元的反应之间的关系以及大脑中的神经元的电活动之间的关系。
至今已有数种深度学习框架，如深度学习#深度神经网络、卷积神经网络和和递归神经网络已被应用在计算机视觉、语音识别、自然语言处理、音频识别与生物信息学等领域并取得了极好的效果。
另外，“深度学习”已成为类似术语，或者说是神经网络的品牌重塑。
简介
深度学习框架，尤其是基于人工神经网络的框架可以追溯到1980年福岛邦彦提出的新认知机，而人工神经网络的历史更为久远。1989年，扬·勒丘恩（Yann LeCun）等人开始将1974年提出的标准反向传播算法应用于深度神经网络，这一网络被用于手写邮政编码识别。尽管算法可以成功执行，但计算代价非常巨大，神经网路的训练时间达到了3天，因而无法投入实际使用。许多因素导致了这一缓慢的训练过程，其中一种是由的学生于1991年提出的梯度消失问题。
最早的进行一般自然杂乱图像中自然物体识别的深度学习网络是翁巨扬（Juyang Weng）等在1991和1992发表的生长网（Cresceptron）。它也是第一个提出了后来很多实验广泛采用的一个方法：现在称为最大汇集（max-pooling)以用于处理大物体的变形等问题。生长网不仅直接从杂乱自然场景中学习老师指定的一般物体，还用网络反向分析的方法把图像内被识别了的物体从背景图像中分割出来。
2007年前后，杰弗里·辛顿和鲁斯兰·萨拉赫丁诺夫（Ruslan Salakhutdinov）提出了一种在前馈神经网络中进行有效训练的算法。这一算法将网络中的每一层视为无监督学习的受限玻尔兹曼机，再使用有监督的反向传播算法进行调优。在此之前的1992年，在更为普遍的情形下，施密德胡伯也曾在递归神经网络上提出一种类似的训练方法，并在实验中证明这一训练方法能够有效提高有监督学习的执行速度.
自深度学习出现以来，它已成为很多领域，尤其是在计算机视觉和语音识别中，成为各种领先系统的一部分。在通用的用于检验的数据集，例如语音识别中的TIMIT和图像识别中的ImageNet, Cifar10上的实验证明，深度学习能够提高识别的精度。与此同时，神经网络也受到了其他更加简单归类模型的挑战，支持向量机等模型在20世纪90年代到21世纪初成为过流行的机器学习算法。
硬件的进步也是深度学习重新获得关注的重要因素。高性能图形处理器的出现极大地提高了数值和矩阵运算的速度，使得机器学习算法的运行时间得到了显著的缩短。
由于脑科学方面的大量研究已表明人脑网络不是一个级联的结构，深度学习网络在2001年后正逐渐被更有潜力的基于脑模型的网络所替代。
基本概念
深度学习的基础是机器学习中的分散表示（distributed representation）。分散表示假定观测值是由不同因子相互作用生成。在此基础上，深度学习进一步假定这一相互作用的过程可分为多个层次，代表对观测值的多层抽象。不同的层数和层的规模可用于不同程度的抽象。
深度学习运用了这分层次抽象的思想，更高层次的概念从低层次的概念学习得到。这一分层结构常常使用贪婪算法逐层构建而成，并从中选取有助于机器学习的更有效的特征.
不少深度学习算法都以无监督学习的形式出现，因而这些算法能被应用于其他算法无法企及的无标签数据，这一类数据比有标签数据更丰富，也更容易获得。这一点也为深度学习赢得了重要的优势。
人工神经网络下的深度学习
一部分最成功的深度学习方法涉及到对人工神经网络的运用。人工神经网络受到了1959年由诺贝尔奖得主大卫·休伯尔（David H. Hubel）和托斯坦·威泽尔（Torsten Wiesel）提出的理论启发。休伯尔和威泽尔发现，在大脑的初级视觉皮层中存在两种细胞：简单细胞和复杂细胞，这两种细胞承担不同层次的视觉感知功能。受此启发，许多神经网络模型也被设计为不同节点之间的分层模型。
福岛邦彦提出的新认知机引入了使用无监督学习训练的卷积神经网络。扬·勒丘恩将有监督的反向传播算法应用于这一架构。事实上，从反向传播算法自20世纪70年代提出以来，不少研究者都曾试图将其应用于训练有监督的深度神经网络，但最初的尝试大都失败。在其博士论文中将失败的原因归结为梯度消失，这一现象同时在深度前馈神经网络和递归神经网络中出现，后者的训练过程类似深度网络。在分层训练的过程中，本应用于修正模型参数的误差随着层数的增加指数递减，这导致了模型训练的效率低下。
为了解决这一问题，研究者们提出了一些不同的方法。于1992年提出多层级网络，利用无监督学习训练深度神经网络的每一层，再使用反向传播算法进行调优。在这一模型中，神经网络中的每一层都代表观测变量的一种压缩表示，这一表示也被传递到下一层网络。
另一种方法是赛普·霍克赖特和于尔根·施密德胡伯提出的，LSTM）。2009年，在ICDAR 2009举办的连笔手写识别竞赛中，在没有任何先验知识的情况下，深度多维长短期记忆神经网络取得了其中三场比赛的胜利。
斯文·贝克提出了在训练时只依赖梯度符号的神经抽象金字塔模型，用以解决图像重建和人脸定位的问题
。
其他方法同样采用了无监督预训练来构建神经网络，用以发现有效的特征，此后再采用有监督的反向传播以区分有标签数据。辛顿等人于2006年提出的深度模型提出了使用多层隐变量学习高层表示的方法。这一方法使用斯摩棱斯基于1986年提出的受限玻尔兹曼机
对每一个包含高层特征的层进行建模。模型保证了数据的对数似然下界随着层数的提升而递增。当足够多的层数被学习完毕，这一深层结构成为一个生成模型，可以通过自上而下的采样重构整个数据集。辛顿声称这一模型在高维结构化数据上能够有效地提取特征。
吴恩达和杰夫·迪恩领导的谷歌大脑团队创建了一个仅通过YouTube视频学习高层概念（例如猫）的神经网络
。
其他方法依赖了现代电子计算机的强大计算能力，尤其是GPU。2010年，在于尔根·施密德胡伯位于瑞士人工智能实验室IDSIA的研究组中，丹·奇雷尚（Dan Ciresan）和他的同事展示了利用GPU直接执行反向传播算法而忽视梯度消失问题的存在。这一方法在扬·勒丘恩等人给出的手写识别MNIST数据集上战胜了已有的其他方法。
截止2011年，前馈神经网络深度学习中最新的方法是交替使用卷积层（convolutional layers）和最大值池化层（max-pooling layers）并加入单纯的分类层作为顶端。训练过程也无需引入无监督的预训练。从2011年起，这一方法的GPU实现和其他比赛。
这些深度学习算法也是最先在某些识别任务上达到和人类表现具备同等竞争力的算法D. C. Ciresan, U. Meier, J. Schmidhuber. Multi-column Deep Neural Networks for Image Classification. IEEE Conf. on Computer Vision and Pattern Recognition CVPR 2012.
总结下：
人工智能：为机器赋予人的智能
机器学习：一种实现人工智能的方法
深度学习：一种实现机器学习的技术
文中提到人工神经网络是一种机器学习的算法，希望有同样疑问的人也搞清楚了