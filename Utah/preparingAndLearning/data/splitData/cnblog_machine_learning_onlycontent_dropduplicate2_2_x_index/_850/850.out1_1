过拟合 在 进行 数据挖掘 或者 机器学习 模型 建立 的 时候 
因为 在 统计 学习 中 假设 数据 满足 独立 同 
分布 即 当前 已 产生 的 数据 可以 对 未来 
的 数据 进行 推测 与 模拟 因此 都是/nr 使用 历史数据 
建立 模型 即 使用 已经 产生 的 数据 去 训练 
然后 使用 该 模型 去 拟合 未来 的 数据 但是 
一般 独立 同 分布 的 假设 往往 不 成立 即 
数据 的 分布 可能会 发生 变化 distribution drift 并且 可能 
当前 的 数据 量 过少 不足以 对 整个 数据集 进行 
分布 估计 因此 往往 需要 防止 模型 过拟合 提高 模型 
泛化 能力 而 为了 达到 该 目的 的 最 常见 
方法 便是 正则化 即在 对模型 的 目标 函数 objective function 
或 代价 函数 cost function 加上 正则 项 在对 模型 
进行 训练 时 有 可能 遇到 训练 数据 不够 即 
训练 数据 无法 对 整个 数据 的 分布 进行 估计 
的 时候 或者 在 对模型 进行 过度 训练 overtraining 时 
常常 会 导致 模型 的 过拟合 overfitting 如下 图 所示 
通过 上图 可以 看出 随着 模型 训练 的 进行 模型 
的 复杂度 会 增加 此时 模型 在 训练 数据 集上 
的 训练 误差 会 逐渐 减小 但是 在 模型 的 
复杂度 达到 一定 程度 时 模型 在 验证 集上 的 
误差 反而 随着 模型 的 复杂度 增加 而 增大 此时 
便 发生 了 过拟合 即 模型 的 复杂度 升高 但是 
该 模型 在 除 训练 集 之外 的 数据 集上 
却不 work 方法 提前 终止 当 验证 集上 的 效果 
变差 的 时候 正则化 Regularization L1 正则化 L2 正则化 数据集 
扩增 Data augmentation Dropout1 提前 终止 对 模型 进行 训练 
的 过程 即是 对模型 的 参数 进行 学习 更新 的 
过程 这个 参数 学习 的 过程 往往会 用到 一些 迭代 
方法 如 梯度 下降 Gradient descent 学习 算法 Early stopping 
便 是 一种 迭代 次数 截断 的 方法 来 防止 
过拟合 的 方法 即在 模型 对 训练 数据集 迭代 收敛 
之前 停止 迭代 来 防止 过拟合 Early stopping 方法 的 
具体 做法 是 在 每一个 Epoch 结束时 一个 Epoch 集为 
对 所有 的 训练 数据 的 一轮 遍历 计算 validation 
data 的 accuracy 当 accuracy 不再 提高 时 就 停止 
训练 这种 做法 很 符合 直观 感受 因为 accurary 都 
不再 提高了 在 继续 训练 也是 无益 的 只会 提高 
训练 的 时间 那么 该 做法 的 一个 重点 便是 
怎样 才 认为 validation accurary 不再 提高了 呢 并 不是 
说 validation accuracy 一 降 下来 便 认为 不再 提高了 
因为 可能 经过 这个 Epoch 后 accuracy 降低 了 但是 
随后 的 Epoch 又 让 accuracy 又上 去了 所以 不能 
根据 一 两次 的 连续 降低 就 判断 不 再 
提高 一般 的 做法 是 在 训练 的 过程 中 
记录 到 目前 为止 最好 的 validation accuracy 当 连续 
10次 Epoch 或者 更多 次 没 达到 最佳 accuracy 时 
则 可以 认为 accuracy 不再 提高了 此时 便 可以 停止 
迭代 了 Early Stopping 这种 策略 也 称为 No improvement 
in n n 即 Epoch 的 次数 可以 根据 实际 
情况 取 如 10 20 30 2 数据集 扩增 在 
数据挖掘 领域 流行 着 这样 的 一句话 有时候 往往 拥有 
更多 的 数据 胜过 一个 好 的 模型 因为 我们 
在 使用 训练 数据 训练 模型 通过 这个 模型 对 
将来 的 数据 进行 拟合 而在 这 之间 又 一个 
假设 便是 训练 数据 与 将来 的 数据 是 独立 
同 分布 的 即使 用 当前 的 训练 数据 来 
对 将来 的 数据 进行 估计 与 模拟 而 更多 
的 数据 往往 估计 与 模拟 地 更 准确 因此 
更多 的 数据 有时候 更 优秀 但是 往往 条件 有限 
如 人力 物力 财力 的 不足 而不能 收集 到 更多 
的 数据 如 在 进行 分类 的 任务 中 需要 
对 数据 进行 打 标 并且 很多 情况下 都是/nr 人工 
得 进行 打 标 因此 一旦 需要 打 标 的 
数据 量 过多 就 会 导致 效率 低下 以及 可能 
出错 的 情况 所以 往往 在 这时候 需要 采取 一些 
计算 的 方式 与 策略 在 已有 的 数据 集上 
进行 手脚 以 得到 更多 的 数据 通俗 得 讲 
数据机 扩增 即 需要 得到 更多 的 符合 要求 的 
数据 即 和 已有 的 数据 是 独立 同 分布 
的 或者 近似 独立 同 分布 的 一般 有 以下 
方法 从 数据 源头 采集 更多 数据 复制 原有 数据 
并 加上 随机噪声 重 采样 根据 当前 数据集 估计 数据分布 
参数 使用 该 分布 产生 更多 数据 等 如 图像处理 
图像 平移 这种 方法 可以 使得 网络 学习 到 平移 
不变 的 特征 图像 旋转 学习 旋转 不变 的 特征 
有些 任务 里 目标 可能 有 多种 不同 的 姿态 
旋转 正好 可以 弥补 样本 中 姿态 较少 的 问题 
图像 镜像 和 旋转 的 功能 类似 图像 亮度 变化 
甚至 可以 用 直方图 均衡化 裁剪 缩放 图像 模糊 用 
不同 的 模板 卷积 产生 模糊 图像 3 正则化 正则化 
方法 是 指在 进行 目标函数 或 代价 函数 优 化时 
在 目标函数 或 代价 函数 后面 加 上一个 正则 项 
一般 有 L1 正则 与 L2 正则 等 3.1 L1 
正则 在 原始 的 代价 函数 后面 加 上一个 L1 
正则化 项 即 所有 权重 w 的 绝对值 的 和 
乘以 λ / n 这里 不像 L2 正则化 项 那样 
需要 再 乘以 1/2 同样 先 计算 导数 上式 中 
sgn w 表示 w 的 符号 那么 权重 w 的 
更新 规则 为 比 原始 的 更新 规则 多 出了 
η * λ * sgn w / n 这 一项 
当 w 为 正时 更新 后的w/nr 变小 当 w 为 
负 时 更新 后的w/nr 变大 因此 它 的 效果 就是 
让 w 往 0 靠 使 网络 中 的 权重 
尽可能 为 0 也就 相当于 减小 了 网络 复杂度 防止 
过拟合 另外 上面 没有 提到 一个 问题 当 w 为 
0时 怎么办 当 w 等于 0时 | W | 是 
不可导 的 所以 我们 只能 按照 原始 的 未经 正则化 
的 方法 去 更新 w 这就 相当于 去掉 η * 
λ * sgn w / n 这 一项 所以 我们 
可以 规定 sgn 0 = 0 这样 就把 w = 
0 的 情况 也 统一 进来 了 在 编程 的 
时候 令 sgn 0 = 0 sgn w 0 = 
1 sgn w 0 = 1 3.2 L2 正则化 L2 
正则化 就是 在 代价 函数 后面 再 加上 一个 正则化 
项 C0 代表 原始 的 代价 函数 后面 那 一项 
就是 L2 正则化 项 它 是 这样 来 的 所有 
参数 w 的 平方 的 和 除以 训练 集 的 
样本 大小 n λ 就是 正则 项 系数 权衡 正则 
项与C/nr 0项 的 比重 另外 还有 一个 系数 1/2 1/2 
经常 会 看到 主要 是 为了 后面 求导 的 结果 
方便 后面 那 一项 求导 会 产生 一个 2 与 
1/2 相乘 刚好 凑 整 L2 正则化 项是/nr 怎么 避免 
overfitting 的 呢 我们 推导 一下 看看 先 求导 可以 
发现 L2 正则化 项对b/nr 的 更新 没有 影响 但是 对于 
w 的 更新 有影响 在 不 使用 L2 正则 化时 
求导 结果 中 w 前 系数 为 1 现在 w 
前面 系数 为 1 − η λ / n 因为 
η λ n 都是 正 的 所以 1 − η 
λ / n 小于 1 它 的 效果 是 减小 
w 这 也 就是 权重 衰减 weight decay 的 由来 
当然 考虑 到 后面 的 导 数项 w 最终 的 
值 可能 增大 也可能 减小 另外 需要 提 一下 对于 
基于 mini batch 的 随机 梯度 下降 w 和b/nr 更新 
的 公式 跟 上面 给出 的 有点 不同 对比 上面 
w 的 更新 公式 可以 发现 后面 那 一项 变了 
变成 所有 导数 加 和 乘以 η 再 除以 m 
m 是 一个 mini batch 中 样本 的 个数 到 
目前 为止 我们/r 只是/c 解释/v 了/ul L2/i 正则化/i 项有让/nr w/w 
变小 的 效果 但是 还 没 解释 为什么 w 变小 
可以 防止 overfitting 一个 所谓 显而易见 的 解释 就是 更小 
的 权值 w 从 某种 意义 上 说 表示 网络 
的 复杂度 更低 对 数据 的 拟合 刚刚好 这个 法则 
也叫做 奥卡姆 剃刀 而在 实际 应用 中 也 验证 了 
这 一点 L2 正则化 的 效果 往往 好于 未经 正则化 
的 效果 当然 对于 很多 人 包括 我 来说 这个 
解释 似乎 不 那么 显而易见 所以 这里 添加 一个 稍微 
数学 一点 的 解释 引自 知乎 过拟合 的 时候 拟合 
函数 的 系数 往往 非常 大 为什么 如下 图 所示 
过拟合 就是 拟合 函数 需要 顾忌 每 一个 点 最终 
形成 的 拟合 函数 波动 很大 在 某些 很小 的 
区间 里 函数值 的 变化 很 剧烈 这 就 意味着 
函数 在 某些 小区 间里 的 导 数值 绝对值 非常大 
由于 自 变量值 可大可小 所以 只有 系数 足够 大 才能 
保证 导 数值 很大 而 正则化 是 通过 约束 参数 
的 范数 使其 不要 太大 所以 可以 在 一定 程度 
上 减少 过拟合 情况 4 DropoutL1 L2 正则化 是 通过 
修改 代价 函数 来 实现 的 而 Dropout 则 是 
通过 修改 神经 网络 本身 来 实现 的 它 是 
在 训练 网络 时用的/nr 一种 技巧 trike 它 的 流程 
如下 假设 我们 要 训练 上 图 这个 网络 在 
训练 开始时 我们 随机 地 删除 一半 的 隐 层 
单元 视 它们 为 不存在 得到 如下 的 网络 保持 
输入输出 层 不变 按照 BP 算法 更新 上图 神经网络 中的 
权值 虚线 连接 的 单元 不 更新 因为 它们 被 
临时 删除 了 以上 就是 一次 迭代 的 过程 在 
第二 次 迭代 中 也用 同样 的 方法 只不过 这次 
删除 的 那 一半 隐 层 单元 跟 上一次 删 
除掉 的 肯定 是 不 一样 的 因为 我们 每一次 
迭代 都是 随机 地 去 删掉 一半 第三 次 第四 
次 都是/nr 这样 直至 训练 结束 以上 就是 Dropout 它 
为什么 有助于 防止 过拟合 呢 可以 简单 地 这样 解释 
运用 了 dropout 的 训练 过程 相当于 训练 了 很多 
个 只有 半数 隐 层 单元 的 神经 网络 后面 
简称为 半数 网络 每 一个 这样 的 半数 网络 都 
可以 给 出 一个 分类 结果 这些 结果 有的 是 
正确 的 有的 是 错误 的 随着 训练 的 进行 
大部分 半数 网络 都 可以 给 出 正确 的 分类 
结果 那么 少数 的 错误 分类 结果 就 不会 对 
最终 结果 造成 大 的 影响 更加 深入 地 理解 
可以/c 看看/v Hinton/w 和/c Alex/w 两/m 牛/n 2012/m 的/uj 论文/nz 
ImageNet Classification with Deep Convolutional Neural Networks 参考 http / 
/ blog . csdn . net / heyongluoyao8 / article 
/ details / 49429629http / / blog . csdn . 
net / u012162613 / article / details / 44261657http / 
/ blog . csdn . net / thesby / article 
/ details / 53164257 