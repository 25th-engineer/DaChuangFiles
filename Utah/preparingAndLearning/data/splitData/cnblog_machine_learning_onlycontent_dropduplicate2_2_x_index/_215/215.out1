下表 为 是否 适合 打 垒球 的 决策表 预测 E 
= { 天气 = 晴 温度 = 适中 湿度 = 
正常 风速 = 弱 } 的 场合 是否 合适 中 
打 垒球 天气/n 温度/n 湿度/n 风速/n 活动/vn 晴/v 炎热/a 高弱/nr 
取消/v 晴/v 炎热/a 高强/nr 取消/v 阴/a 炎热/a 高弱/nr 进行/v 雨/n 
适/v 中高/ns 弱/a 进行/v 雨/n 寒冷/a 正常/d 弱/a 进行/v 雨/n 
寒冷/a 正常/d 强/a 取消/v 阴/a 寒冷/a 正常/d 强/a 进行/v 晴/v 
适/v 中高/ns 弱/a 取消/v 晴/v 寒冷/a 正常/d 弱/a 进行/v 雨/n 
适中/v 正常/d 弱/a 进行/v 晴/v 适中/v 正常/d 强/a 进行/v 阴/a 
适中/v 高强/nr 进行/v 阴/a 炎热/a 正常/d 弱/a 进行/v 雨/n 适中/v 
高强/nr 取消/v 如何/r 发现/v 这些/r 数据/n 之中/r 所/c 掩藏/v 的/uj 
规律/n 从而 较好 的 预测 在 给定 条件下 所 可能 
的 结果 决策树 是 一种 以 示例 为基础 的 归纳 
学习 方法 能够 较好 的 解决 这 类 问题 一个 
简单 的 例子 请 给出 布尔 函数 A * B 
+ C + 或 * 与 非 的 最小 体积 
或 结点 决策树 当 C 为 1时 AB/w 不/d 管取/i 
何值/nr 整个/b 表达式/n 都/d 为真/i 此时 这个 表达式 就 可以 
确定 真假 所以 选择 C 作为 头 结点 若 C 
为 0 表达式 无法 确定 真假 还需 进一步 看 AB 
的 取值 A 与非 B 是 与 的 关系 两者 
具有 相同 的 地位 所以 接下来 无论 取 A 还是 
B 都 可以 整个 决策树 构造 结果 如下 图 所示 
类似 于 这个 简单 例子 对于 打 垒球 这些 数据 
我们 可以 将 天气 温度 湿度 风速 可以 成为 属性 
或 特征 类比 成 布尔 函数 的 ABC 而 它们 
的 取值 如 天气 的 取值 可以 是 晴 雨 
阴/a 类比/v 成/n ABC/w 布尔/nr 取值/v 真假/n 那么 活动 的 
取消 或 进行 就 可以 类比 成 整个 布尔 表达式 
的 真 或 假 要 构造 一颗 最小 体积 决策树 
就要 每次 在 各个 属性 中找到 区分度 最大 的 属性 
来 作为 当前 决策树 的 节点 相关 名词 熵 通常 
熵 表示 事物 的 混乱 程度 熵 越大 表示 混乱 
程度 越大 越小 表示 混乱 程度 越小 对于 随机事件 如果 
我们 知道 它 有N种/nr 取值 情况 每种 情况 发生 的 
概论 为 那么 这件事 的 熵 就 定义 为 例如 
对于 打 垒球 的 例子 要求 活动 的 熵 H 
活动 在 活动 一栏 属性 中 发现 活动 的 取值 
有 两种 取消 5个 和 进行 9个 它们 所占 的 
比例 分别为 5/14 9/14 那么 H 活动 的 取值 为 
算出 的 结果 约为 0.94 对于 熵 的 理解 如果 
一件 事 发生 的 可能 是 1 不 发生 的 
肯为 0 那么 这件事 的 熵 为 = 0 这就 
表明 这件事 肯定 发生 没有 不 发生 的 情况 那么 
它 的 混乱 程度 是 最小 的 0 同 理当 
不 发生 的 可能 是 1 混乱 程度 也是 0 
当 发生 与 不发生 各占 一半 时 这件事 就越 不好 
确定 所以 此时 熵 为 最大 其 图像 如下 图 
所示 计算 熵 的 代码 如下 1 def calcShannonEnt dataSet 
# 计算 香农 熵 2 numEntries = len dataSet 3 
4 labelCounts = { } 5 for featVec in dataSet 
6 currentLabel = featVec 1 # 取得 最后 一列 数据 
计算 该 属性 取值 情况 有 多少 个 7 if 
currentLabel not in labelCounts . keys 8 labelCounts currentLabel = 
0 9 labelCounts currentLabel + = 1 10 11 # 
计算 熵 12 shannonEnt = 0.0 13 for key in 
labelCounts 14 prob = float labelCounts key / numEntries 15 
shannonEnt = prob * log prob 2 16 17 return 
shannonEntView Code 信息 增益 随机事件 未 按照 某个 属 划 
的 不同 取值 划 分时 的 熵 减去 按照 某个 
属性 的 不同 取值 划 分时 的 平均 熵 即 
前后 两次 熵 的 差值 还是 对于 打 垒球 的 
例子 未 按照 某个 属 划 的 不同 取值 划 
分时 的 熵 即 H 活动 已 算出 未 0.94 
现在 按 照 天气 属性 的 不同 取值 来 划分 
发现 天气 属性 有 3个 不同 取值 分别为 晴 阴 
雨 划 分好 后 如下 图 所示 天气/n 温度/n 湿度/n 
风速/n 活动/vn 晴/v 炎热/a 高弱/nr 取消/v 晴/v 炎热/a 高强/nr 取消/v 
晴/v 适/v 中高/ns 弱/a 取消/v 晴/v 寒冷/a 正常/d 弱/a 进行/v 
晴/v 适中/v 正常/d 强/a 进行/v 阴/a 炎热/a 高弱/nr 进行/v 阴/a 
寒冷/a 正常/d 强/a 进行/v 阴/a 适中/v 高强/nr 进行/v 阴/a 炎热/a 
正常/d 弱/a 进行/v 雨/n 寒冷/a 正常/d 强/a 取消/v 雨/n 适中/v 
高强/nr 取消/v 雨/n 适/v 中高/ns 弱/a 进行/v 雨/n 寒冷/a 正常/d 
弱/a 进行/v 雨/n 适中/v 正常/d 弱/a 进行/v 在/p 天气/n 为/p 
晴/v 时有/d 5种/mq 情况/n 发现 活动 取消 有 3种 进行 
有 2种 计算 现在 的 条件 熵 = 0.971 同理 
天气 为 阴 时有 4种 情况 活动 进行 的 有 
4种 则 条件 熵 为 = 0 同理 天气 为 
雨 时有 5种 情况 活动 取消 的 有 2种 进行 
的 有 3种 则 条件 熵 为 = 0.971 由于 
按 照 天气 属性 不同 取值 划 分时 天气 为 
晴 占 整个 情况 的 5/14 天气 为 阴占/nr 整个 
情况 的 4/14 天气 为 雨 占 整个 情况 的 
5/14 则 按照 天气 属性 不同 取值 划 分时 的 
带 权 平均值 熵 为 算出 的 结果 约为 0.693 
. 则 此时 的 信息 增益 Gain 活动 天气 = 
  H 活动   H 活动 | 天气   = 
0.94 0.693 = 0.246 同理 我们 可以 计算出 按照 温度 
属性 不同 取值 划分 后的/nr 信息 增益 Gain 活动 温度 
= H 活动 H 活动 | 温度 = 0.94 0.911 
= 0.029 按照 湿度 属性 不同 取值 划分 后的/nr 信息 
增益 Gain 活动 湿度 = H 活动 H 活动 | 
湿度 = 0.94 0.789 = 0.151 按照 风速 属性 不同 
取值 划分 后的/nr 信息 增益 Gain 活动 风速 = H 
活动 H 活动 | 风速 = 0.94 0.892 = 0.048 
对于 信息 增益 的 理解 信息 增益 就是 两个 熵 
的 差 当 差值 越大 说明 按照 此 划分 对于 
事件 的 混乱 程度 减少 越 有帮助 计算 各个 属性 
的 信息 增益 并 选择 信息 增益 最大 的 属性 
的 代码 如下 1 # 定义 按照 某个 特征 进行 
划分 的 函数 splitDataSet 2 # 输入 三个 变量 待 
划分 的 数据 集 特征 分类 值 3 # axis 
特征值 中 0 代表 no surfacing 1 代表 flippers 4 
# value 分类 值 中 0 代表 否 1 代表 
是 5 def splitDataSet dataSet axis value 6 retDataSet = 
7 for featVec in dataSet # 取 大 列表 中 
的 每个 小 列表 8 if featVec axis = = 
value 9 reduceFeatVec = featVec axis 10 reduceFeatVec . extend 
featVec axis + 1 11 retDataSet . append reduceFeatVec 12 
13 return retDataSet # 返回 不含 划分 特征 的 子集 
14 15 def c h o o s e B 
e s t F e a t u r e 
T o p l i t dataSet 16 numFeature = 
len dataSet 0 1 17 baseEntropy = calcShannonEnt dataSet 18 
bestInforGain = 0 19 bestFeature = 1 20 21 for 
i in range numFeature 22 featList = number i for 
number in dataSet # 得到 某个 特征 下 所有 值 
某 列 23 uniquelVals = set featList # set 无 
重复 的 属性 特征值 得到 所 有无 重复 的 属性 
取值 24 25 # 计算 每个 属性 i 的 概论 
熵 26 newEntropy = 0 27 for value in uniquelVals 
28 subDataSet = splitDataSet dataSet i value # 得到 i 
属性 下 取 i 属性 为 value 时的/nr 集合 29 
prob = len subDataSet / float len dataSet # 每个 
属性 取值 为 value 时 所占 比重 30 newEntropy + 
= prob * calcShannonEnt subDataSet 31 inforGain = baseEntropy newEntropy 
# 当前 属性 i 的 信息 增益 32 33 if 
inforGain bestInforGain 34 bestInforGain = inforGain 35 bestFeature = i 
36 37 return bestFeature # 返回 最大 信息 增益 属性 
下标 View Code 构造 决策树 决策树 的 构造 就是 要 
选择 当前 信息 增益 最大 的 属性 来 作为 当前 
决策树 的 节点 因此 我们 选择 天气 属性 来做 为 
决策 树根 节点 这时 天气 属性 有3/nr 取值 可能 晴 
阴 雨 我们 发现 当 天气 为 阴时/nr 活动 全为/nr 
进行 因此 这 件 事情 就 可以 确定 了 而 
天气 为 晴 或 雨 时 活动 中 有 进行 
的 也有 取消 的 事件 还 无法 确定 这时 就 
需要 在 当前 按照 天气 属性 划分 下 的 剩下 
的 属性 中 递归 再次 计算 活动 熵 和 信息 
增益 选择 信息 增益 最大 的 属性 来 作为 下 
一个 节点 直到 整个 事件 能够 确定 下来 例如 当 
天气 为 晴 时 得到/v 如下/t 表/v 所示/v 的/uj 事件/n 
天气/n 温度/n 湿度/n 风速/n 活动/vn 晴/v 炎热/a 高弱/nr 取消/v 晴/v 
炎热/a 高强/nr 取消/v 晴/v 适/v 中高/ns 弱/a 取消/v 晴/v 寒冷/a 
正常/d 弱/a 进行/v 晴/v 适中/v 正常/d 强/a 进行/v 我们/r 需要/v 
递归/v 处理/v 继续 在 温度 湿度 风速 这三个 属性 中找到 
信息 增益 最大 的 属性 来 做为 下 一个 节点 
首先 继续 计算 活动 熵 此时 有 5个 样例 活动 
取消 有 3个 进行 有 2个 则 活动 熵 为 
= 0.971 接着 计算 信息 增益 在 天气 为 晴 
的 前提 下 按照/p 温度/n 属性/n 的/uj 不同/a 取值/v 分类/n 
后/f 结果/n 如下/t 所示/v 天气/n 温度/n 湿度/n 风速/n 活动/vn 晴/v 
炎热/a 高弱/nr 取消/v 晴/v 炎热/a 高强/nr 取消/v 晴/v 适/v 中高/ns 
弱/a 取消/v 晴/v 寒冷/a 正常/d 弱/a 进行/v 晴/v 适中/v 正常/d 
强/a 进行/v 发现/v 湿度/n 为高/i 时有/d 3种/mq 情况/n 活动 取消 
有 3种 进行 有 0种 则 条件 熵 为 = 
0 湿 度正 常有 2种 情况 活动 取消 0种 进行 
2中 则 条件 熵 为 = 0 由于 按照 湿度 
属性 不同 取值 划 分时 湿度 为高 占 总 情况 
的 3/5 湿度 正常 占 总 情况 的 2/5 则 
按照 湿度 属性 不同 取值 划 分时 的 带 权 
平均值 熵 为 算出 的 结果 约为 0 所以 此时 
在 天气 为 晴 的 前提 下 按照 湿度 属性 
的 不同 取值 划分 的 信息 增益 为 Gain = 
  H 活动 | 天气 = 晴   H 活动 
| 天气 湿度   = 0.971 0 = 0.971 同理 
还需 继续 计算 在 天气 为 晴 的 前提 下 
按照 温度 风速 属性 的 不同 取值 划分 的 信息 
增益 找到 信息 增益 最大 的 作为 决策树 的 下 
一个 节点 递归 构造 决策树 的 代码 如下 1 # 
递归 创 建树 用于 找出 出现 次数 最多 的 分类 
名称 2 def majorityCnt classList 3 classCount = { } 
4 for vote in classList # 统计 当前 划分 下 
每 中 情况 的 个数 5 if vote not in 
classCount . keys 6 classCount vote = 0 7 classCount 
vote + = 1 8 sortedClassCount = sorted classCount . 
items key = operator . itemgetter 1 reversed = True 
# reversed = True 表示 由 大 到 小 排序 
9 # 对 字典 里 的 元素 按照 value 值 
由 大 到 小 排序 10 print * * * 
* * * * * * * * * * 
* * * 11 print sortedClassCount 0 0 12 return 
sortedClassCount 0 0 13 14 15 def createTree dataSet labels 
16 classList = example 1 for example in dataSet # 
创建 数组 存放 所有 标签 值 取 dataSet 里 最后 
一列 结果 17 # 类别 相同 停止 划分 18 if 
classList . count classList 1 = = len classList # 
判断 classList 里 是否 全是 一类 count 方法 用于 统计 
某个 元素 在 列表 中 出现 的 次数 19 return 
classList 1 # 当 全是 一类 时 停止 分割 20 
# 长度 为 1 返回 出现 次数 最多 的 类别 
21 if len classList 0 = = 1 # 当 
没有 更多 特征 时 停止 分割 即 分到 最后 一个 
特征 也 没有 把 数据 完全 分开 就 返回 多数 
的 那个 结果 22 return majorityCnt classList 23 # 按照 
信息 增益 最高 选取 分类 特征 属性 24 bestFeat = 
c h o o s e B e s t 
F e a t u r e T o p 
l i t dataSet # 返回 分类 的 特征 序号 
按照 最大熵 原则 进行 分类 25 bestFeatLable = labels bestFeat 
# 该 特征 的 label # 存储 分类 特征 的 
标签 26 27 myTree = { bestFeatLable { } } 
# 构 建树 的 字典 28 del labels bestFeat # 
从 labels 的 list 中 删除 该 label 29 30 
featValues = example bestFeat for example in dataSet 31 uniqueVals 
= set featValues 32 for value in uniqueVals 33 subLables 
= labels # 子集合 将 labels 赋 给 sublabels 此时 
的 labels 已经 删掉 了 用于 分类 的 特征 的 
标签 34 # 构建 数据 的 子集合 并 进行 递归 
35 myTree bestFeatLable value = createTree splitDataSet dataSet bestFeat value 
subLables 36 return myTreeView Code 最后 得到 的 决策 树 
如下 图 所示 整个 程序 如下 1 from math import 
log 2 from operator import * 3 4 def storeTree 
inputTree filename 5 import pickle 6 fw = open filename 
wb # pickle 默认 方式 是 二进制 需要 制定 wb 
7 pickle . dump inputTree fw 8 fw . close 
9 10 def grabTree filename 11 import pickle 12 fr 
= open filename rb # 需要 制定 rb 以 byte 
形式 读取 13 return pickle . load fr 14 15 
16 def createDataSet 17 18 dataSet = 1 1 yes 
1 1 yes 1 0 no 0 1 no 0 
1 no 19 labels = no surfacing flippers 20 21 
dataSet = sunny hot high weak no 22 sunny hot 
high strong no 23 overcast hot high weak yes 24 
rain mild high weak yes 25 rain cool normal weak 
yes 26 rain cool normal strong no 27 overcast cool 
normal strong yes 28 sunny mild high weak no 29 
sunny cool normal weak yes 30 rain mild normal weak 
yes 31 sunny mild normal strong yes 32 overcast mild 
high strong yes 33 overcast hot normal weak yes 34 
rain mild high strong no 35 labels = outlook temperature 
humidity wind 36 return dataSet labels 37 38 def calcShannonEnt 
dataSet # 计算 香农 熵 39 numEntries = len dataSet 
40 41 labelCounts = { } 42 for featVec in 
dataSet 43 currentLabel = featVec 1 # 取得 最后 一列 
数据 该 属性 取值 情况 有 多少 个 44 if 
currentLabel not in labelCounts . keys 45 labelCounts currentLabel = 
0 46 labelCounts currentLabel + = 1 47 48 # 
计算 熵 49 shannonEnt = 0.0 50 for key in 
labelCounts 51 prob = float labelCounts key / numEntries 52 
shannonEnt = prob * log prob 2 53 54 return 
shannonEnt 55 56 # 定义 按照 某个 特征 进行 划分 
的 函数 splitDataSet 57 # 输入 三个 变量 待 划分 
的 数据 集 特征 分类 值 58 # axis 特征值 
中 0 代表 no surfacing 1 代表 flippers 59 # 
value 分类 值 中 0 代表 否 1 代表 是 
60 def splitDataSet dataSet axis value 61 retDataSet = 62 
for featVec in dataSet # 取 大 列表 中 的 
每个 小 列表 63 if featVec axis = = value 
64 reduceFeatVec = featVec axis 65 reduceFeatVec . extend featVec 
axis + 1 66 retDataSet . append reduceFeatVec 67 68 
return retDataSet # 返回 不含 划分 特征 的 子集 69 
70 def c h o o s e B e 
s t F e a t u r e T 
o p l i t dataSet 71 numFeature = len 
dataSet 0 1 72 baseEntropy = calcShannonEnt dataSet 73 bestInforGain 
= 0 74 bestFeature = 1 75 76 for i 
in range numFeature 77 featList = number i for number 
in dataSet # 得到 某个 特征 下 所有 值 某 
列 78 uniquelVals = set featList # set 无 重复 
的 属性 特征值 得到 所 有无 重复 的 属性 取值 
79 80 # 计算 每个 属性 i 的 概论 熵 
81 newEntropy = 0 82 for value in uniquelVals 83 
subDataSet = splitDataSet dataSet i value # 得到 i 属性 
下 取 i 属性 为 value 时的/nr 集合 84 prob 
= len subDataSet / float len dataSet # 每个 属性 
取值 为 value 时 所占 比重 85 newEntropy + = 
prob * calcShannonEnt subDataSet 86 inforGain = baseEntropy newEntropy # 
当前 属性 i 的 信息 增益 87 88 if inforGain 
bestInforGain 89 bestInforGain = inforGain 90 bestFeature = i 91 
92 return bestFeature # 返回 最大 信息 增益 属性 下标 
93 94 # 递归 创 建树 用于 找出 出现 次数 
最多 的 分类 名称 95 def majorityCnt classList 96 classCount 
= { } 97 for vote in classList # 统计 
当前 划分 下 每 中 情况 的 个数 98 if 
vote not in classCount . keys 99 classCount vote = 
0 100 classCount vote + = 1 101 sortedClassCount = 
sorted classCount . items key = operator . itemgetter 1 
reversed = True # reversed = True 表示 由 大 
到 小 排序 102 # 对 字典 里 的 元素 
按照 value 值 由 大 到 小 排序 103 104 
return sortedClassCount 0 0 105 106 107 def createTree dataSet 
labels 108 classList = example 1 for example in dataSet 
# 创建 数组 存放 所有 标签 值 取 dataSet 里 
最后 一列 结果 109 # 类别 相同 停止 划分 110 
if classList . count classList 1 = = len classList 
# 判断 classList 里 是否 全是 一类 count 方法 用于 
统计 某个 元素 在 列表 中 出现 的 次数 111 
return classList 1 # 当 全是 一类 时 停止 分割 
112 # 长度 为 1 返回 出现 次数 最多 的 
类别 113 if len classList 0 = = 1 # 
当 没有 更多 特征 时 停止 分割 即 分到 最后 
一个 特征 也 没有 把 数据 完全 分开 就 返回 
多数 的 那个 结果 114 return majorityCnt classList 115 # 
按照 信息 增益 最高 选取 分类 特征 属性 116 bestFeat 
= c h o o s e B e s 
t F e a t u r e T o 
p l i t dataSet # 返回 分类 的 特征 
序号 按照 最大熵 原则 进行 分类 117 bestFeatLable = labels 
bestFeat # 该 特征 的 label # 存储 分类 特征 
的 标签 118 119 myTree = { bestFeatLable { } 
} # 构 建树 的 字典 120 del labels bestFeat 
# 从 labels 的 list 中 删除 该 label 121 
122 featValues = example bestFeat for example in dataSet 123 
uniqueVals = set featValues 124 for value in uniqueVals 125 
subLables = labels # 子集合 将 labels 赋 给 sublabels 
此时 的 labels 已经 删掉 了 用于 分类 的 特征 
的 标签 126 # 构建 数据 的 子集合 并 进行 
递归 127 myTree bestFeatLable value = createTree splitDataSet dataSet bestFeat 
value subLables 128 return myTree 129 130 131 if _ 
_ name _ _ = = _ _ main _ 
_ 132 my _ Data labels = createDataSet 133 134 
# print calcShannonEnt my _ Data 135 Mytree = createTree 
my _ Data labels 136 print Mytree View Code 