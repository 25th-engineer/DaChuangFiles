本 笔记 主要 记录 学习 机器学习 的 总结 体会 如有 
理解 不 到位 的 地方 欢迎 大家 指出 我会 努力 
改正 在 学习 机器学习 时 我 主要 是 通过 Andrew 
Ng 教授 在 mooc 上 提供 的 Machine Learning 课程 
不得 不说 Andrew Ng 老师 在 讲授 这门 课程 时 
真的 很 用心 特别 是 编程 练习 这门 课 真的 
很 nice 在此 谢谢 Andrew Ng 老师 的 付出 同时 
也 谢过 告知 这个 平台 的 小伙伴 本文 在 写 
的 过程 中 多有 借鉴 Andrew Ng 教授 在 mooc 
提供 的 资料 再次 感谢 转载 请 注明 出处 http 
/ / blog . csdn . net / u010278305 什么 
是 机器学习 我 认为 机器学习 就是 给定 一定 的 信息 
如 一间 房子 的 面子 一幅 图片 每个 点 的 
像素 值 等等 通过 对 这些 信息 进行 学习 得出 
一个 学习 模型 这个 模型 可以 在 有该/nr 类型 的 
信息 输入 时 输出 我们 感兴趣 的 结果 好比 我们 
如果 要 进行 手写 数字 的 识别 已经 给定 了 
一些 已知 信息 一些 图片 和 这些 图片 上 的 
手写 数字 是 多少 我们 可以 按 以下 步骤 进行 
学习 1 将 这些 图片 每个 点 的 像素 值 
与 每个 图片 的 手写 数字 值 输入 学习 系统 
2 通过 学习 过程 我们 得到 一个 学习 模型 这个 
模型 可以 在 有 新的 手写 数字 的 图片 输 
入时 给出 这张 图片 对应 手写 数字 的 合理 估计 
什么 是 线性 回归 我 的 理解 就是 用 一个 
线性函数 对 提供 的 已知 数据 进行 拟合 最终 得到 
一个 线性函数 使 这个 函数 满足 我们 的 要求 如 
具有 最小 平方差 随后 我们 将 定义 一个 代价 函数 
使 这个 目标 量化 之后 我们 可以 利用 这个 函数 
对 给定 的 输入 进行 预测 例如 给定 房屋 面积 
我们 预测 这个 房屋 的 价格 如下 图 所示 假设 
我们 最终 要 的 得到 的 假设 函数 具有 如下 
形式 其中 x 是 我们 的 输入 theta 是 我们 
要求 得 的 参数 代价 函数 如下 我们 的 目标 
是 使 得此 代价 函数 具有 最小值 为此 我们 还 
需要 求得 代价 函数 关于 参量 theta 的 导数 即 
梯度 具有 如下 形式 有了/nr 这些 信息 之后 我们 就 
可以 用 梯度 下降 算法 来 求得 theta 参数 过程 
如下 其实 为了 求得 theta 参数 有 更多 更好 的 
算法 可以 选择 我们 可以 通过 调用 matlab 的 fminunc 
函数 实现 而 我们 只需 求出 代价 与 梯度 供 
该 函数调用 即可 根据 以上 公式 我们 给出 代价 函数 
的 具体 实现 function J = computeCostMulti X y theta 
% COMPUTECOSTMULTI Compute cost for linear regression with multiple variables 
% J = COMPUTECOSTMULTI X y theta computes the cost 
of using theta as the % parameter for linear regression 
to fit the data points in X and y % 
Initialize some useful values m = length y % number 
of training examples % You need to return the following 
variables correctly J = 0 % Instructions Compute the cost 
of a particular choice of theta % You should set 
J to the cost . hThetaX = X * theta 
J = 1 / 2 * m * sum hThetaX 
y . ^ 2 end 什么 是 逻辑 回归 相比 
于 线性 回归 逻辑 回归 只会 输出 一些 离散 的 
特定 值 例如 判定 一封 邮件 是否 为 垃圾 邮件 
输出 只有 0 和1/nr 而且 对 假设 函数 进行 了 
处理 使得 输出 只在 0 和1/nr 之间 假设 函数 如下 
代价 函数 如下 梯度 函数 如下 观察 可知 形式 与 
线性 回归 时 一样 有了/nr 这些 信息 我们 就 可以 
通过 fminunc 求出 最优 的 theta 参数 我们 只需 给出 
代价 与 梯度 的 计算 方式 代码 如下 function J 
grad = costFunction theta X y % COSTFUNCTION Compute cost 
and gradient for logistic regression % J = COSTFUNCTION theta 
X y computes the cost of using theta as the 
% parameter for logistic regression and the gradient of the 
cost % w . r . t . to the 
parameters . % Initialize some useful values m = length 
y % number of training examples % You need to 
return the following variables correctly J = 0 grad = 
zeros size theta % Instructions Compute the cost of a 
particular choice of theta . % You should set J 
to the cost . % Compute the partial derivatives and 
set grad to the partial % derivatives of the cost 
w . r . t . each parameter in theta 
% % Note grad should have the same dimensions as 
theta % hThetaX = sigmoid X * theta J = 
1 / m * sum y . * log hThetaX 
1 y . * log 1 hThetaX grad = 1 
/ m * hThetaX y * X end 其中 sigmod 
函数 如下 function g = sigmoid z % SIGMOID Compute 
sigmoid functoon % J = SIGMOID z computes the sigmoid 
of z . % You need to return the following 
variables correctly g = zeros size z % Instructions Compute 
the sigmoid of each value of z z can be 
a matrix % vector or scalar . e = exp 
1 g = 1 . / 1 + e . 
^ z end 有时 会 出现 过拟合 的 情况 即 
求得 的 参数 能够 很好 的 拟合 训练 集中 的 
数据 但 在 进行 预 测时 明显 与 趋势 不符 
好比 下图 所示 此时 我们 需要 进行 正则化 处理 对 
参数 进行 惩罚 使得 除 theta 1 之外 的 theta 
值 均 保持 较小 值 进行 正则化 之后 的 代价 
函数 如下 进行 正则化 之后 的 梯度 如下 下面 给出 
正则化 之后 的 代价 与 梯度 值得 代码 function J 
grad = costFunctionReg theta X y lambda % COSTFUNCTIONREG Compute 
cost and gradient for logistic regression with regularization % J 
= COSTFUNCTIONREG theta X y lambda computes the cost of 
using % theta as the parameter for regularized logistic regression 
and the % gradient of the cost w . r 
. t . to the parameters . % Initialize some 
useful values m = length y % number of training 
examples % You need to return the following variables correctly 
J = 0 grad = zeros size theta % Instructions 
Compute the cost of a particular choice of theta . 
% You should set J to the cost . % 
Compute the partial derivatives and set grad to the partial 
% derivatives of the cost w . r . t 
. each parameter in theta hThetaX = sigmoid X * 
theta theta 1 = 0 J = 1 / m 
* sum y . * log hThetaX 1 y . 
* log 1 hThetaX + lambda / 2 * m 
* sum theta . ^ 2 grad = 1 / 
m * hThetaX y * X + lambda / m 
* theta end 对于 线性 回归 正则化 的 过程 基本 
类似 至于 如何 选择 正则 化时 的 常数 lambda 我们 
可以 将 数据 分为 训练 集 交叉 验证 集 和 
测试 集 三 部分 在 不同 lambda 下 先用 训练 
集 求出 参数 theta 之后 求出 训练 集 与 交叉 
验证 集 的 代价 通过 分析 得出 适合 的 lambda 
如下 图 所示 转载 请 注明 出处 http / / 
blog . csdn . net / u010278305 