最近 在看 机器学习 实战 这本书 因为 自己 本身 很想 深入 
的 了解 机器学习 算法 加之 想学 python 就在 朋友 的 
推荐 之下 选择 了 这 本书 进行 学习 在 写 
这篇文章 之前 对 FCM 有过 一定 的 了解 所以 对 
K 均值 算法 有 一种 莫名 的 亲切感 言归正传 今天 
我 和 大家 一起来 学习 K 均值 聚 类 算法 
一 K 均值 聚 类 K means 概述 1 . 
聚 类 类 指 的 是 具有 相似性 的 集合 
聚 类 是 指 将 数据集 划分为 若干类 使得 类 
内 之间 的 数据 最为 相似 各类 之间 的 数据 
相似 度 差别 尽可能 大 聚类分析 就是 以 相似性 为基础 
对 数据 集 进行 聚 类 划分 属于 无 监督 
学习 2 . 无 监督 学习 和 监督 学习上 一篇 
对 KNN 进行 了 验证 和 KNN 所 不同 K 
均值 聚 类 属于 无 监督 学习 那么 监督 学习 
和无/nr 监督 学习 的 区别 在 哪儿 呢 监督 学习 
知道 从 对象 数据 中 学习 什么 而无 监督 学习 
无需 知道 所要 搜寻 的 目标 它 是 根据 算法 
得到 数据 的 共同 特征 比如 用 分类 和聚类/nr 来说 
分类 事先 就 知道 所 要 得到 的 类别 而 
聚 类 则 不一样 只是 以 相似 度 为基础 将 
对象 分得 不同 的 簇 3 . K meansk means 
算法 是 一种 简单 的 迭 代型 聚 类 算法 
采用 距离 作为 相似性 指标 从而 发现 给 定 数据 
集中 的 K 个 类 且 每个 类 的 中心 
是 根据 类 中 所有 值 的 均值 得到 每个 
类 用 聚 类 中心 来 描述 对于 给定 的 
一个 包含 n 个 d 维数 据点 的 数据 集 
X 以及 要 分得 的 类别 K 选取 欧式 距离 
作为 相似 度 指标 聚 类 目标 是 使得 各类 
的 聚 类 平方和 最小 即 最小化 结合/v 最小二乘/i 法和/nr 
拉格朗日/i 原理/n 聚 类 中心 为 对应 类别 中 各数 
据点 的 平均值 同时 为了 使得 算法 收敛 在 迭代 
过程 中 应使/nr 最终 的 聚 类 中心 尽可能 的 
不变 4 . 算法 流程 K means 是 一个 反复 
迭代 的 过程 算法 分为 四 个 步骤 1 选取 
数据 空间 中的 K 个 对象 作为 初始 中心 每个 
对象 代表 一个 聚 类 中心 2 对于 样本 中的 
数据对象 根据 它们 与 这些 聚 类 中心 的 欧氏距离 
按 距离 最近 的 准则 将 它们 分到 距离 它们 
最近 的 聚 类 中心 最 相似 所 对应 的 
类 3 更新 聚 类 中心 将 每个 类别 中 
所有 对象 所 对应 的 均值 作为 该 类别 的 
聚 类 中心 计算 目标函数 的 值 4 判断 聚 
类 中心 和 目标 函数 的 值 是否 发生 改变 
若 不变 则 输出 结果 若 改变 则 返回 2 
用 以下 例子 加以 说明 给定 一个 数据集 根据 K 
= 5 初始化 聚 类 中心 保证 聚 类 中心 
处于 数据 空间 内 根据 计算 类 内 对象 和聚类/nr 
中心 之间 的 相似 度 指标 将 数据 进行 划分 
将 类 内 之间 数据 的 均值 作为 聚 类 
中心 更新 聚 类 中心 最后 判断 算法 结束 与否 
即可 目的 是 为了 保证 算法 的 收敛 二   
python 实现 首先 需要 说明 的 是 我 采用 的 
是 python2 . 7 直接 上 代码 # k means 
算法 的 实现 # * coding utf 8 * from 
numpy import * from math import sqrt import sys sys 
. path . append C / Users / Administrator / 
Desktop / k means 的 python 实现 def loadData fileName 
data = fr = open fileName for line in fr 
. readlines curline = line . strip . split \ 
t frline = map float curline data . append frline 
return data # test a = mat loadData C / 
Users / Administrator / Desktop / k means / testSet 
. txt print a # 计算 欧氏距离 def distElud vecA 
vecB return sqrt sum power vecA vecB 2 # 初始化 
聚 类 中心 def randCent dataSet k n = shape 
dataSet 1 center = mat zeros k n for j 
in range n rangeJ = float max dataSet j min 
dataSet j center j = min dataSet j + rangeJ 
* random . rand k 1 return center # test 
a = mat loadData C / Users / Administrator / 
Desktop / k means / testSet . txt n = 
3 b = randCent a 3 print b def kMeans 
dataSet k dist = distElud createCent = randCent m = 
shape dataSet 0 clusterAssment = mat zeros m 2 center 
= createCent dataSet k clusterChanged = True while clusterChanged clusterChanged 
= False for i in range m minDist = inf 
minIndex = 1 for j in range k distJI = 
dist dataSet i center j if distJI minDist minDist = 
distJI minIndex = j if clusterAssment i 0 = minIndex 
# 判断 是否 收敛 clusterChanged = True clusterAssment i = 
minIndex minDist * * 2 print center for cent in 
range k # 更新 聚 类 中心 dataCent = dataSet 
nonzero clusterAssment 0 . A = = cent 0 center 
cent = mean dataCent axis = 0 # axis 是 
普通 的 将 每 一列 相加 而 axis = 1 
表示 的 是 将 向量 的 每 一行 进行 相加 
return center clusterAssment # test dataSet = mat loadData C 
/ Users / Administrator / Desktop / k means / 
testSet . txt k = 4 a = kMeans dataSet 
k print a 三 MATLAB 实现 之前 用 MATLAB 做过 
一些 聚 类 算法 方面 的 优化 自然 使用 它 
相比 python 更 得心应手 一点 根据 算法 的 步骤 编程 
实现 直接 上 程序 % % % K means clear 
all clc % % 构造 随机 数据 mu1 = 0 
0 0 S1 = 0.23 0 0 0 0.87 0 
0 0 0.56 data1 = mvnrnd mu1 S1 100 % 
产生 高斯分布 数据 % % 第二类 数据 mu2 = 1.25 
1.25 1.25 S2 = 0.23 0 0 0 0.87 0 
0 0 0.56 data2 = mvnrnd mu2 S2 100 % 
第三 个 类 数据 mu3 = 1.25 1.25 1.25 S3 
= 0.23 0 0 0 0.87 0 0 0 0.56 
data3 = mvnrnd mu3 S3 100 mu4 = 1.5 1.5 
1.5 S4 = 0.23 0 0 0 0.87 0 0 
0 0.56 data4 = mvnrnd mu4 S4 100 % 显示 
数据 figure plot3 data1 1 data1 2 data1 3 + 
title 原始数据 hold on plot3 data2 1 data2 2 data2 
3 r + plot3 data3 1 data3 2 data3 3 
g + plot3 data4 1 data4 2 data3 3 y 
+ grid on data = data1 data2 data3 data4 row 
col = size data K = 4 max _ iter 
= 300 % % 迭代 次数 min _ impro = 
0.1 % % % % 最小 步长 display = 1 
% % % 判定 条件 center = zeros K col 
U = zeros K col % % 初始化 聚 类 
中心 mi = zeros col 1 ma = zeros col 
1 for i = 1 col mi i 1 = 
min data i ma i 1 = max data i 
center i = ma i 1 ma i 1 mi 
i 1 * rand K 1 end % % 开始 
迭代 for o = 1 max _ iter % % 
计算 欧氏距离 用 norm 函数 for i = 1 K 
dist { i } = for j = 1 row 
dist { i } = dist { i } data 
j center i end end minDis = zeros row K 
for i = 1 row tem = for j = 
1 K tem = tem norm dist { j } 
i end nmin index = min tem minDis i index 
= norm dist { index } i end % % 
更新 聚 类 中心 for i = 1 K for 
j = 1 col U i j = sum minDis 
i . * data j / sum minDis i end 
end % % 判定 if display end if o 1 
if max abs U center min _ impro break else 
center = U end end end % % 返回 所属 
的 类别 class = for i = 1 row dist 
= for j = 1 K dist = dist norm 
data i U j end nmin index = min dist 
class = class data i index end % % 显示 
最后 结果 m n = size class figure title 聚 
类 结果 hold on for i = 1 row if 
class i 4 = = 1 plot3 class i 1 
class i 2 class i 3 ro elseif class i 
4 = = 2 plot3 class i 1 class i 
2 class i 3 go elseif class i 4 = 
= 3 plot3 class i 1 class i 2 class 
i 3 bo else plot3 class i 1 class i 
2 class i 3 yo end end grid on 最终 
的 结果 如下 和 总结 在这次 程序 的 调试 中 
其实 出现 的 问题 还是 蛮 多 的 相似 度 
指标 依旧 选用 的 是 欧氏距离 在 之前 一直 是 
按照 公式 直接 计算 的 可 欧氏距离 其实 就是 2 
范数 啊 2 范数 属于 酉 不变 范数 因此 矩阵 
的 2 范数 就是 矩阵 的 最大 奇异 值 在 
求解 过程 中 可以 直接 采用 norm 函数 简化 上 
图中 的 结果 可以 清晰 的 看到 算法 具有 一定 
的 聚 类 效果 要 进一步 验证 的话 可以 采取 
MCR 或者 NMI 和 ARI 这些 常用 的 准则 进行 
衡量 聚 类 结果 的 优劣 在此 我 选取 MCR 
进行 验证 代码 如下 % % 采用 MCR 判 定聚 
类 效果 B = class 4 B = reshape B 
1 row A = ones 1 100 2 * ones 
1 100 3 * ones 1 100 4 * ones 
1 100 sum = 0 for i = 1 row 
if A 1 i ~ = B 1 i sum 
= sum + 1 end end MCR = sum / 
row fprintf MCR = % d \ n MCR 多次 
计算 平均 求得 的 MCR = 0.53 表明 误 分率 
还是 蛮 大 的 聚 类 效果 并 不是 很 
理想 究其原因 虽然 算法 收敛 但 算法 只是 收敛 到了 
局部 最小值 而 并非 全局 最小值 所以 可以 引入 二分 
K 均值 对 算法 进行 优化 除此之外 FCM 算法 在 
一定 程度 上 也是 对 算法 的 一个 优化 吧 
进而 导入 UCI 数据库 中的 wine 数据 进行 测试 结果 
甚是 不 理想 至于 原因 吧 算法 本身 的 性能 
是 占 一 部分 的 还有 可能 是 数据 的 
维数 相对 较多 . . . . . . 在此 
我 也 不敢 妄加 猜测 之后 慢慢 验证 吧 . 
. . . . . 