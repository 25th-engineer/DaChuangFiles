主 成分 分析 principal component analysis 是 一种 常见 的 
数据 降 维 方法 其 目的 是 在 信息 损失 
较小 的 前提 下 将 高维 的 数据 转换 到 
低维 从而 减小 计算 量 PCA 的 本质 就是 找 
一些 投影 方向 使得 数据 在 这些 投影 方 向上 
的 方差 最大 而且 这些 投影 方向 是 相互 正交 
的 这 其实 就是 找 新的 正交基 的 过程 计算 
原始数据 在 这些 正交基 上 投影 的 方差 方差 越大 
就 说明 在 对应 正交基 上 包含 了 更多 的 
信息量 后面 会 证明 原始数据 协方差 矩阵 的 特征值 越大 
对应 的 方差 越大 在 对应 的 特征 向量 上 
投影 的 信息量 就 越大 反之 如果 特征值 较小 则 
说明 数据 在 这些 特征向量 上 投影 的 信息量 很小 
可以 将 小 特征值 对应 方向 的 数据 删除 从而 
达到 了 降 维 的 目的 PCA 的 全部 工作 
简单 点 说 就是 对 原始 的 空间 中 顺序 
地 找 一组 相互 正交 的 坐标轴 第一 个 轴 
是 使得 方差 最大 的 第二个 轴 是 在与 第一个 
轴 正交 的 平面 中 使得 方差 最大 的 第三个 
轴 是 在与 第 1 2个 轴 正交 的 平面 
中 方差 最大 的 这样 假设在 N 维空间 中 我们 
可以 找到 N 个 这样 的 坐标轴 我们 取 前 
r 个 去 近似 这个 空间 这样 就 从 一个 
N 维 的 空间 压缩 到 r 维 的 空间 
了 但是 我们 选择 的 r 个 坐标轴 能够 使得 
空间 的 压缩 使得 数据 的 损失 最小 因此 关键点 
就 在于 如何 找到 新的 投影 方向 使得 原始数据 的 
信息量 损失 最少 1 . 样本 信息量 的 衡量 样本 
的 信息量 指 的 是 样本 在 特征 方向 上 
投影 的 方差 方差 越大 则 样本 在 该 特征 
上 的 差异 就 越大 因此 该 特征 就 越 
重要 以 机器学习 实战 上 的 图 说明 在 分类 
问题 里 样本 的 方差 越大 越 容易 将 不同 
类别 的 样本 区分开 图中 共有 3个 类别 的 数据 
很显然 方差 越大 越 容易 分开 不同 类别 的 点 
样本 在 X 轴上 的 投影 方差 较大 在 Y 
轴 的 投影 方差 较小 方差 最大 的 方向 应该 
是 中间 斜 向上 的 方向 图中 红线 方向 如果 
将 样本 按照 中间 斜 向上 的 方向 进行 映射 
则 只要 一维 的 数据 就 可以 对 其 进行 
分类 相比 二维 的 原 数据 就 相当 降了 一维 
在 原始 数据 更 多维 的 情况 下 先 得到 
一个 数据 变换 后 方差 最大 的 方向 然后 选择 
与 第一 个 方向 正交 的 方向 该 方向 是 
方差 次大 的 方向 如此 下去 直到 变 换出 与 
原 特征 个数 相同 的 新 特征 或者 变 换出 
前 N 个 特征 在 这 前 N 个 特征 
包含 了 数据 的 绝大部分 信息 简而言之 PCA 是 一个 
降 维 的 过程 将 数据 映射 到 新的 特征 
新 特征 是 原始 特征 的 线性组合 2 . 计算 
过程 因为 插入 公式 比较 麻烦 就 直接 采用 截图 
的 方式 3 . python 实现 # coding = utf 
8 from numpy import * 通过 方差 的 百分比 来 
计算 将 数据 降到 多少 维 是 比较 合适 的 
函数 传入 的 参数 是 特征值 和 百分比 percentage 返回 
需要 降到 的 维度 数 num def eigValPct eigVals percentage 
sortArray = sort eigVals # 使用 numpy 中的 sort 对 
特征值 按照 从小到大 排序 sortArray = sortArray 1 1 # 
特征值 从大到/nr 小 排序 arraySum = sum sortArray # 数据 
全部 的 方差 arraySum tempSum = 0 num = 0 
for i in sortArray tempSum + = i num + 
= 1 if tempsum = arraySum * percentage return num 
pca 函数 有 两个 参数 其中 dataMat 是 已经 转换 
成 矩阵 matrix 形式 的 数据 集 列 表示 特征 
其中 的 percentage 表示 取 前 多少 个 特征 需要 
达到 的 方差 占 比 默认 为 0.9 def pca 
dataMat percentage = 0.9 meanVals = mean dataMat axis = 
0 # 对 每 一列 求 平均值 因为 协方差 的 
计算 中 需要 减去 均值 meanRemoved = dataMat meanVals covMat 
= cov meanRemoved rowvar = 0 # cov 计算 方差 
eigVals eigVects = linalg . eig mat covMat # 利用 
numpy 中 寻找 特征值 和 特征向量 的 模块 linalg 中的 
eig 方法 k = eigValPct eigVals percentage # 要 达到 
方差 的 百分比 percentage 需要 前 k 个 向量 eigValInd 
= argsort eigVals # 对 特征值 eigVals 从小到大 排序 eigValInd 
= eigValInd k + 1 1 # 从 排好 序 
的 特征值 从后/nr 往前/t 取/v k/w 个/q 这样 就 实现 
了 特征值 的 从大到/nr 小 排列 redEigVects = eigVects eigValInd 
# 返回 排序 后 特征值 对应 的 特征向量 redEigVects 主 
成分 lowDDataMat = meanRemoved * redEigVects # 将 原始数据 投影 
到 主 成分 上 得到 新的 低维 数据 lowDDataMat reconMat 
= lowDDataMat * redEigVects . T + meanVals # 得到 
重构 数据 reconMat return lowDDataMat r e c o n 
M a t R e f e r e n 
c e 1 . Peter Harrington 机器学习 实战 人民邮电出版社 20132 
. http / / www . cnblogs . com / 
jerrylead / archive / 2011/04 / 18/2020209 . html 其中 
有 PCA 的 计算 实例 3 . 张 学工 模式识别 
第三版 清华大学出版社 2010 