决策树 是 机器 学习 的 常见 算法 分为 分类 树 
和 回归 树 当 对 一个 样本 的 分类 进行 
预测 时 使用 分类 树 当 对 样本 的 某一个 
值 进行 预测 时 使用 回归 树 本文 是 有关 
决策树 的 第一 部分 主要 介绍 分类 树 的 几种 
构建 方法 以及 如何 使用 分类 树 测试 分类 目录 
如下 1 分类 树 的 基本 概念 2 采用 数据集 
说明 3 划分 数据集 的 几种 方式 4 构造 分类 
树 5 使用 分类 树 测试 分类 6 写 在后面 
的话 一 分类 树 的 基本 概念 分类 树 classification 
tree 简单 地 说 就是 根据 训练 数据集 构造 一个 
类似 树形 的 分类 决策模型 然后 用 这个 模型 来 
辅助 决策 例如 下图 是 一个 简单 的 是否 举行 
某个 活动 的 决策 树 分类 树 我们 可以 通过 
上 面的 决策树 进行 预测 当 天气晴朗 交通 畅通 时 
我们 预测 该 活动 很 可能 要 举办 当 天下 
小雨 交通 拥挤 时 我们 预测 活动 很 可能 被 
取消 这 只是 一个 简单 的 小 例子 真实 中的 
决策树 方法 包括 以下 几个 步骤 1 收集 数据 可以 
使用 任何 方法 2 准备 数据 树 构造 算法 只 
适用 于 标称 型 数据 因此 数值 型 数据 必须 
离散化 3 选取 划分算法 根据 数据 的 特点 选取 合适 
的 划分算法 4 构造 决策树 使用 选取 的 划分算法 构造 
树形 的 决策模型 5 测试 算法 使用 经验 树 计算 
错误率 6 使用 算法 使用 决策树 模型 预测 决策 二 
采用 数据集 说明 UCI 数据集 是 机器学习 不错 的 数据 
集 网站 本文 选取 其中 的 Balloons 数据集 将其 内容 
用 中文 表示 如下 该 数据 主要 是 根据 几个 
因素 预测 气球 是否 会破 三 划分 数据集 的 几种 
方式 决策树 的 几种 经典 实现 方式 是 ID3 C 
4.5 和 CART 其中 C 4.5 是 对 ID3 的 
改进 C 4.5 和 ID3 都是 分类 树 CART 即可 
用作 分类 树 又可 用于 回归 树 当 CART 用作 
分类 时 使用 基尼指数 作为 划分 依据 当 CART 用作 
回归 时 使用 最小 方差 作为 划分 依据 信息熵 熵 
entropy 也即 信息熵 是 度量 样本 集合 纯度 的 一种 
指标 一个 数据集 的 熵 越大 则 说明 该 数据 
分类 的 纯度 越纯/nr D 表示 数据集 假设 D 共有 
m 个 类别 Pk 表示 第 k 个 类别 占 
样本 总数 的 比例 数据集 熵 的 公式 如下 如 
上面 给出 的 关于 气球 的 数据集 只有 会 和 
不会 两种 分类 会 有 7个 不会 有 9个 占 
比 分别 是 P1 = 7/16   P2 = 9/16 
按照 熵 的 公式 可得 Ent D = P1 * 
log2P1   P2 * log2P2 = 0.989 代码 实现 1 
from math import log 2 import operator 3 4 def 
calc _ entropy data _ set 5 计算 数据集 的 
熵 6 count = len data _ set 7 label 
_ counts = { } 8 9 # 统计 数据 
集中 每种 分类 的 个数 10 for row in data 
_ set 11 label = row 1 12 if label 
not in label _ counts . keys 13 label _ 
counts label = 1 14 else 15 label _ counts 
label + = 1 16 17 # 计算 熵 18 
entropy = 0.0 19 for key in label _ counts 
20 prob = float label _ counts key / count 
21 entropy = prob * log prob 2 22 return 
entropy 信息 增 益法 ID3 算法 使用 信息 增益 作为 
划分 数据集 的 依据 整个 数据集 的 熵 称作 原始 
熵 数据集 D 根据 某 个 特征 划分 之后 的 
熵 为 条件 熵 信息 增益 = 原始 熵 条件 
熵     用 信息 增益 划分 的 具体 做法 
是 计算 每 一类 特征 V 对应 的 信息 增益 
然后 挑选 信息 增益 最小 的 特征 进行 划分 信息 
增益 公式 为 其中 v 为特征 a 的 一个 分类 
pv 为 v 分类 占 特征 a 总 个数 的 
比例 Dv   根据 特征 a 的 v 分类 进行 
划分 之后 的 数据 集 代码 实现 1 def choose 
_ best _ feature _ 1 data _ set 2 
选取 信息 增益 最高 的 特征 3 feature _ count 
= len data _ set 0 1 4 # 数据集 
的 原始 熵 5 base _ entropy = calc _ 
entropy data _ set 6 # 最大 的 信息 增益 
7 best _ gain = 0.0 8 # 信息 增益 
最大 的 特征 9 best _ feature = 1 10 
11 # 遍历 计算 每个 特征 12 for i in 
range feature _ count 13 feature = example i for 
example in data _ set 14 feature _ value _ 
set = set feature 15 new _ entropy = 0.0 
16 17 # 计算 信息 增益 18 for value in 
feature _ value _ set 19 sub _ data _ 
set = split _ data _ set data _ set 
i value 20 prob = len sub _ data _ 
set / float len data _ set 21 new _ 
entropy + = prob * calc _ entropy sub _ 
data _ set 22 gain = base _ entropy new 
_ entropy 23 24 # 比较 得出 最大 的 信息 
增益 25 if gain best _ gain 26 best _ 
gain = gain 27 best _ feature = i 28 
29 return best/w _/i feature/w 增益/n 率/v 法/l ID3/i 所/c 
采用/v 的/uj 信息/n 增益/n 划分/v 数据集/i 是/v 可能/v 对/p 数目/n 
较多/i 的/uj 属性/n 有/v 偏好/d C 4.5 算法 避免 了 
这个 问题 使用 增益 率 来 选择 最 优化 分 
属性 信息 增益 率 公式 其中 Gain _ ratio D 
a 表示 根据 特征 a 划分 之后 的 信息 增益 
率 IV a 为 特征 a 的 固有 值 代码 
实现 1 def choose _ best _ feature _ 2 
data _ set 2 根据 增益 率 选取 划分 特征 
3 feature _ count = len data _ set 0 
1 4 # 数据集 的 原始 熵 5 base _ 
entropy = calc _ entropy data _ set 6 # 
最大 的 信息 增益 率 7 best _ gain _ 
ratio = 0.0 8 # 信息 增益 率 最大 的 
特征 9 best _ feature = 1 10 11 # 
遍历 计算 每个 特征 12 for i in range feature 
_ count 13 feature = example i for example in 
data _ set 14 feature _ value _ set = 
set feature 15 new _ entropy = 0.0 16 # 
固有 值 17 intrinsic _ value = 0.0 18 19 
# 计算 信息 增益 20 for value in feature _ 
value _ set 21 sub _ data _ set = 
split _ data _ set data _ set i value 
22 prob = len sub _ data _ set / 
float len data _ set 23 new _ entropy + 
= prob * calc _ entropy sub _ data _ 
set 24 intrinsic _ value = prob * log prob 
2 25 gain = base _ entropy new _ entropy 
26 gain _ ratio = gain / intrinsic _ value 
27 28 # 比较 得出 最大 的 信息 增益 率 
29 if gain _ ratio best _ gain _ ratio 
30 best _ gain _ ratio = gain _ ratio 
31 best _ feature = i 32 33 return best 
_ feature 基尼指数 法当/nr CART 用做 分类 树 时 使用 
基尼指数 来 选择 划分 特征 基尼指数 是 另外 一种 表示 
数据集 纯度 的 指标 基尼 值 基尼指数 代码 实现 1 
def calc _ gini data _ set 2 计算 数据集 
的 基尼 值 3 count = len data _ set 
4 label _ counts = { } 5 6 # 
统计 数据 集中 每种 分类 的 个数 7 for row 
in data _ set 8 label = row 1 9 
if label not in label _ counts . keys 10 
label _ counts label = 1 11 else 12 label 
_ counts label + = 1 13 14 # 计算 
基尼 值 15 gini = 1.0 16 for key in 
label _ counts 17 prob = float label _ counts 
key / count 18 gini = prob * prob 19 
return gini 20 21 22 def choose _ best _ 
feature _ 3 data _ set 23 根据 基尼指数 选择 
划分 特征 24 feature _ count = len data _ 
set 0 1 25 # 最小 基尼指数 26 min _ 
gini _ index = 0.0 27 # 基尼指数 最小 的 
特征 28 best _ feature = 1 29 30 # 
遍历 计算 每个 特征 31 for i in range feature 
_ count 32 feature = example i for example in 
data _ set 33 feature _ value _ set = 
set feature 34 35 # 基尼指数 36 gini _ index 
= 0.0 37 # 计算 基尼指数 38 for value in 
feature _ value _ set 39 sub _ data _ 
set = split _ data _ set data _ set 
i value 40 prob = len sub _ data _ 
set / float len data _ set 41 gini _ 
index + = prob * calc _ gini sub _ 
data _ set 42 43 # 比较 得出 最小 的 
基尼指数 44 if gini _ index min _ gini _ 
index or min _ gini _ index = = 0.0 
45 min _ gini _ index = gini _ index 
46 best _ feature = i 47 48 return best 
_ feature 四 构造 分类 树 每次 根据 划分算法 选出 
最佳 的 划分 特征 进行 划分 然后 对子 数据集 进行 
递归 划分 直到 所有 子集 的 纯度 都为 1 即 
构成 了 决策树 构造 决策树 代码 1 def create _ 
division _ tree data _ set labels 2 创建 决策树 
3 class _ list = example 1 for example in 
data _ set 4 5 # 所有 分类 相 同时 
返回 6 if class _ list . count class _ 
list 0 = = len class _ list 7 return 
class _ list 0 8 9 # 已经 遍历 完 
所有 特征 10 if len data _ set 0 = 
= 1 11 return get _ top _ class class 
_ list 12 13 # 选取 最好 的 特征 14 
best _ feat = choose _ best _ feature _ 
1 data _ set 15 best _ feat _ label 
= labels best _ feat 16 17 # 划分 18 
my _ tree = { best _ feat _ label 
{ } } 19 del labels best _ feat 20 
value _ set = set example best _ feat for 
example in data _ set 21 for value in value 
_ set 22 sub _ labels = labels 23 my 
_ tree best _ feat _ label value = create 
_ division _ tree split _ data _ set data 
_ set best _ feat value sub _ labels 24 
return my _ tree 构造 的 决策树 存在 在 Python 
字典 类型 中 不能 直观 地 看清 决策树 层次 这里 
我们 使用 Matplotlib 模块 提供 的 绘图 工具 绘制 出 
决策树 的 模型 如下 我们 可以 看到 使用 不同 的 
划分算法 构造 的 决策 树 是 不 一样 的 五 
使用 决策树 测试 分类 通过 决策树 预测 测试 样本 时 
就是 根据 测试 样本 的 特征 属性 从 决策 树根 
节点 开始 不断 向下 遍历 直到 叶子 节点 1 def 
classify division _ tree feat _ labels test _ vector 
2 遍历 决策树 对 测试数据 进行 分类 3 first _ 
key = list division _ tree . keys 0 4 
second _ dict = division _ tree first _ key 
5 6 feat _ index = feat _ labels . 
index first _ key 7 test _ key = test 
_ vector feat _ index 8 9 test _ value 
= second _ dict test _ key 10 11 if 
isinstance test _ value dict 12 class _ label = 
classify test _ value feat _ labels test _ vector 
13 else 14 class _ label = test _ value 
15 return class _ label 六 写 在后面 的话 本文 
完整 代码 见 https / / gitee . com / 
beiyan / machine _ learning / tree / master / 
decision _ tree 本文 只是 分类 树 方法 的 简单 
实现 关于 回归 树 的 介绍 以及 决策树 的 剪枝 
算法 数值 型 数据 离散 方法 等 将在 后序 文章 
中 介绍 