绘制 了 一张 导 图 有 不对 的 地方 欢迎 
指正 下载 地址 机器学习 中 特征 是 很 关键 的 
. 其中 包括 特征 的 提取 和 特征 的 选择 
. 他们 是 降 维 的 两种 方法 但又 有所不同 
特征 抽取 Feature Extraction Creatting a subset of new features 
by combinations of the exsiting features . 也就是说 特征 抽取 
后的新/nr 特征 是 原来 特征 的 一个 映射 特征选择 Feature 
Selection choosing a subset of all the features the ones 
more informative 也 就是说 特征选择 后的/nr 特征 是 原来 特征 
的 一个 子集 特征提取 最好 的 情况 下 当然 是 
有 专家 知道该 提取 什么样 的 特征 但是 在 不 
知道 的 前提 下 一般 的 降 维 方法 可以 
派上用场 from wiki Principal component a n a l y 
s i s e m i d e f i 
n i t e e m b e d d 
i n g M u l t i f a 
c t o r dimensionality r e d u c 
t i o n M u l t i l 
i n e a r subspace l e a r 
n i n g N o n l i n 
e a r dimensionality r e d u c t 
i o n I s o m a p K 
e r n e l PCAMultilinear PCALatent semantic analysisPartial least 
s q u a r e s I n d 
e p e n d e n t component a 
n a l y s i s A u t 
o e n c o d e r 1 Signal 
representation 信号 表示 The goal of the feature extraction mapping 
is to represent the samples   accurately in a   
low dimensional space . 也 就是说 特征 抽取 后的/nr 特征 
要 能够 精确 地 表示 样本 信息 使得 信息 丢失 
很小 对应 的 方法 是 PCA . 2 Signal classification 
信号 分类 The goal of the feature extraction mapping is 
toenhance the class discriminatory   information in a low dimensional 
space .   也 就是说 特征 抽取 后的/nr 特征 要 
使得 分类 后的/nr 准确率 很高 不能 比 原来 特征 进行 
分类 的 准确率 低 对 与 线性 来说 对应 的 
方法 是 LDA   . 在 图像 处理 方面 有 
广泛 的 应用 . 特征选择 主要 过程 1 产生 过程 
2 . 2.1 完全 搜索 完全 搜索 分为 穷举 搜索 
Exhaustive 与非 穷举 搜索 Non Exhaustive 两类 1 广度 优先 
搜索 Breadth First Search 算法 描述 广度 优先 遍历 特征 
子空间 算法 评价 枚举 了 所有 的 特征 组合 属于 
穷举 搜索 时间 复杂度 是 O 2n 实用性 不高 2 
分支 限界 搜索 Branch and Bound 算法 描述 在 穷举 
搜索 的 基础 上 加入 分支 限界 例如 若 断定 
某些 分支 不 可能 搜索 出 比 当前 找到 的 
最优 解 更优 的 解 则 可以 剪 掉 这些 
分支 3 定 向搜索 Beam Search 算法 描述 首先 选择 
N 个 得分 最高 的 特征 作 为特征 子集 将其 
加入 一个 限制 最大 长度 的 优先 队列 每次 从 
队列 中 取出 得分 最高 的 子集 然后/c 穷举/n 向该/nr 
子集/n 加入/v 1个/mq 特征/n 后/f 产生/n 的/uj 所有/b 特/d 征集/v 
将 这些 特征 集 加入 队列 4 最优 优先 搜索 
Best First Search 算法 描述 与 定向 搜索 类似 唯一 
的 不同 点 是 不 限制 优先 队列 的 长度 
2 . 2.2 启发式 搜索 1 序 列前 向 选择 
SFS Sequential Forward Selection 算法 描述 特征 子集 X 从 
空集 开始 每次 选择 一个 特征 x 加入 特征 子集 
X 使得 特征函数 J X 最优 简单 说 就是 每次 
都 选择 一个 使得 评价 函数 的 取值 达到 最优 
的 特征 加入 其实 就是 一种 简单 的 贪心 算法 
算法 评价 缺点 是 只能 加入 特征 而 不能 去除 
特征 例如 特征 A 完全 依赖 于 特征 B 与 
C 可以 认为 如果 加入 了 特征 B 与 C 
则 A 就是 多余 的 假设 序 列前 向 选择 
算法 首先 将 A 加入 特 征集 然后 又将 B 
与 C 加入 那么 特征 子集 中就 包含 了 多余 
的 特征 A 2 序列 后向 选择 SBS Sequential Backward 
Selection 算法 描述 从 特征 全集 O 开始 每次 从特/nr 
征集 O 中 剔除 一个 特征 x 使得 剔除 特征 
x 后 评价 函数值 达到 最优 算法 评价 序列 后向 
选择 与 序 列前 向 选择 正好 相反 它 的 
缺点 是 特征 只能 去 除 不能 加入 另外 SFS 
与 SBS 都 属于 贪心 算法 容易 陷入 局部 最优 
值 3 双 向搜索 BDS Bidirectional Search 算法 描述 使用 
序 列前 向 选择 SFS 从 空集 开始 同时 使用 
序列 后向 选择 SBS 从 全集 开始 搜索 当 两者 
搜索 到 一个 相同 的 特征 子集 C 时 停止 
搜索 双向 搜索 的 出发点 是 如下 图 所示 O 
点 代表 搜索 起点 A 点 代表 搜索 目标 灰色 
的 圆 代 表单 向搜索 可能 的 搜索 范围 绿色 
的 2个 圆 表示 某次 双向 搜索 的 搜索 范围 
容易 证明 绿色 的 面积 必定 要比 灰色 的 要 
小 4 增 L 去 R 选择 算法 LRS Plus 
L Minus R Selection 该 算法 有 两种 形式 1 
算法 从 空集 开始 每 轮 先 加入 L 个 
特征 然后 从中 去除 R 个 特征 使得 评价 函数值 
最优 L R 2 算法 从 全集 开始 每 轮 
先 去除 R 个 特征 然后 加入 L 个 特征 
使得 评价 函数值 最优 L R 算法 评价 增 L 
去 R 选择 算法 结合 了 序 列前 向 选择 
与 序列 后向 选择 思想 L 与 R 的 选择 
是 算法 的 关键 5 序列 浮动 选择 Sequential Floating 
Selection 算法 描述 序列 浮动 选择 由 增 L 去 
R 选择 算法 发展 而来 该 算法 与 增 L 
去 R 选择 算法 的 不同 之 处 在于 序列 
浮动 选择 的 L 与 R 不是 固定 的 而是 
浮动 的 也 就是 会 变化 的 序列 浮动 选择 
根据 搜索 方向 的 不同 有 以下 两种 变种 1 
序列 浮动 前 向 选择 SFFS Sequential Floating Forward Selection 
算法 描述 从 空集 开始 每 轮 在 未 选择 
的 特征 中 选择 一个 子集 x 使 加入 子集 
x 后 评价 函数 达到 最优 然后 在 已 选择 
的 特征 中 选择 子集 z 使 剔除 子集 z 
后 评价 函数 达到 最优 2 序列 浮动 后向 选择 
SFBS Sequential Floating Backward Selection 算法 描述 与 SFFS 类似 
不同之处 在于 SFBS 是从 全集 开始 每 轮 先 剔除 
特征 然后 加入 特征 算法 评价 序列 浮动 选择 结合 
了 序 列前 向 选择 序列 后向 选择 增 L 
去 R 选择 的 特点 并 弥补 了 它们 的 
缺点 6 决策树 Decision Tree Method DTM 算法 描述 在 
训练样本 集上 运行 C 4.5 或 其他 决策树 生成 算法 
待 决策树 充分 生长 后 再在 树上 运行 剪枝 算法 
则 最终 决策树 各 分支 处 的 特征 就是 选 
出来 的 特征 子 集了 决策树 方法 一般 使用 信息 
增益 作为 评价 函数 2 . 2.3 随机 算法 1 
随机 产生 序列 选择 算法 RGSS Random Generation plus Sequential 
Selection 算法 描述 随机 产生 一个 特征 子集 然后 在 
该 子集 上 执行 SFS 与 SBS 算法 算法 评价 
可 作为 SFS 与 SBS 的 补充 用于 跳出 局部 
最优 值 2 模拟 退火算法 SA Simulated Annealing 模拟 退火算法 
可 参考 大白话 解析 模拟 退火算法 算法 评价 模拟退火 一定 
程度 克服 了 序列 搜索算法 容易 陷入 局部 最优 值 
的 缺点 但是 若 最优 解的/nr 区域 太小 如 所谓 
的 高尔夫球 洞 地形 则 模拟退火 难以 求解 3 遗传算法 
GA   Genetic Algorithms 遗传算法 可 参考 遗传算法 入门 算法 
描述 首先 随机 产生 一批 特征 子集 并用 评价 函数 
给 这些 特征 子集 评分 然后 通过 交叉 突变 等 
操作 繁殖 出 下一代 的 特征 子集 并且 评分 越高 
的 特征 子集 被 选中 参加 繁殖 的 概率 越高 
这样/r 经过/p N/w 代的/nr 繁殖/v 和/c 优胜劣汰/l 后/f 种群 中就 
可能 产生 了 评价 函数值 最高 的 特征 子集 随机 
算法 的 共同 缺点 依赖于 随机因素 有 实验 结果 难以 
重现 2 评价 函数 1 相关性 Correlation filter 运用 相关性 
来 度量 特征 子集 的 好坏 是 基于 这样 一个 
假设 好 的 特征 子集 所 包含 的 特征 应该是 
与 分类 的 相关 度 较高 相关度 高 而 特征 
之间 相关度 较低 的 亢 余度 低 可以 使用 线性 
相关系数 correlation coefficient 来 衡量 向量 之间 线性 相关度 2 
距离 Distance Metrics filter 运用 距离 度量 进行 特征选择 是 
基于 这样 的 假设 好 的 特征 子集 应该 使得 
属于 同 一类 的 样本 距离 尽可能 小 属于 不 
同类 的 样本 之间 的 距离 尽可能 远 常用 的 
距离 度量 相似性 度量 包括 欧氏距离 标准化 欧氏距离 马氏 距离 
等 3 信息 增益 Information Gain filter 假设 存在 离散 
变量 Y Y 中的 取值 包括 { y1 y2 . 
. . . ym } yi 出现 的 概率 为 
Pi 则 Y 的 信息 熵 定义 为 信息熵 有 
如下 特性 若 集合 Y 的 元素 分布 越 纯 
则 其 信息熵 越小 若 Y 分布 越 紊乱 则 
其 信息熵 越大 在 极端 的 情况 下 若 Y 
只能 取 一个 值 即 P1 = 1 则 H 
Y 取 最小值 0 反之 若 各种 取值 出现 的 
概率 都 相等 即 都是 1 / m 则 H 
Y 取 最大值 log2m 在 附加 条件 另一个 变量 X 
而且 知道 X = xi 后 Y 的 条件 信息熵 
Conditional Entropy 表示 为 在 加入 条件 X 前后 的 
Y 的 信息 增益 定义 为 类似 的 分类 标记 
C 的 信息 熵 H C 可表示 为 将 特征 
Fj 用于 分类 后的/nr 分类 C 的 条件 信息熵 H 
C | Fj 表示 为 选用 特征 Fj 前后 的 
C 的 信息 熵 的 变化 成为 C 的 信息 
增益 Information Gain 用 表示 公式 为 假设 存在 特征 
子集 A 和 特征 子集 B 分类 变量 为 C 
若 IG C | A IG C | B 则 
认为 选用 特征 子集 A 的 分类 结果 比 B 
好 因此 倾向于 选用 特征 子集 A 4 一致性 Consistency 
filter 若 样本 1 与 样本 2 属于 不同 的 
分类 但在 特征 A B 上 的 取值 完全 一样 
那么 特征 子集 { A B } 不应该 选作 最终 
的 特征 集 5 分类器 错误率 Classifier error rate wrapper 
使用 特定 的 分类器 用 给定 的 特征 子集 对 
样本 集 进行 分类 用 分类 的 精度 来 衡量 
特征 子集 的 好坏 以上 5种 度量 方法 中 相关性 
距离 信息 增益 一致性 属于 筛选器 而 分类器 错误率 属于 
封装 器 3 停止 准则 4 验证 过程 主要 分 
3类 from wiki Filter Method 思想 与 模型 无关 . 
基于 一些 变 特征 的 衡量 标准 即 给 每一个 
特征 打分 表示 这个 特征 的 重要 程度 排序 后 
除去 那些 得分 较低 的 特征 . . 主要 方法 
1 . Chi squared test 卡方检验 2 . information gain 
信息 增益 或 信息 增益 率 3 . correlation coefficient 
scores 相关系数 优点 计算 时间 上 较 高效 对于 过拟合 
问题 具有 较高 的 鲁棒性 缺点 倾向 于 选择 冗余 
的 特征 因为 他们 不 考虑 特征 之间 的 相关性 
有 可能 某 一个 特征 的 分类 能力 很差 但是 
它 和 某些 其它 特征 组合 起来 会 得到 不错 
的 效果 Wrapper Method 思想 根据 不同 的 特征 集合 
所 获得 的 预测 效果 建立 一个 黑盒 学习 不断 
优化 . 通过 目标 学习 算法 来 评估 特征 集合 
假如有 p 个 特征 那么 就 会有 2p 种 特征 
组合 每种 组合 对应 了 一个 模型 Wrapper 类 方法 
的 思想 是 枚举 出 所有 可能 的 情况 从中 
选取 最好 的 特征 组合 这种 方式 的 问题 是 
由于 每种 特征 组合 都 需要 训练 一次 模型 而 
训练 模型 的 代价 实际上 是 很大 的 如果 p 
非常大 那么 上述 方式 显然 不 具有 可操作性 下面 介绍 
两种 优化 的 方法 forward search 前 向搜索 和 backward 
search 后 向搜索 forward search 初始 时 假设 已 选 
特征 的 集合 为 空集 算法 采取 贪心 的 方式 
逐步 扩充 该 集合 直到 该 集合 的 特征 数 
达到 一个 阈值 该 阈值 可以 预先 设定 也 可以 
通过 交叉 验证 获得 算法 的 伪 码 如下 对于 
算法 的 外 重 循环 当 F 中 包含 所有 
特征 时 或者 F 中的 特征 数 达到 了 阈值 
则 循环 结束 算法 最后 选出 在整个 搜索 过程 中 
最优 的 特征 集合 backward search 初始 时 假设 已 
选 特征 集合 F 为 特征 的 全集 算法 每次 
删除 一个 特征 直到 F 的 特征 数 达到 指定 
的 阈值 或者 F 被删 空 该/r 算法/n 在/p 选择/v 
删除/v 哪/r 一个/m 特征/n 时和/nr forward/w search 在 选择 一个 
特征 加入 F 时是/nr 一样 的 做法 将 子集 的 
选择 看作 是 一个 搜索 寻优 问题 生成 不同 的 
组合 对 组合 进行 评价 再 与 其他 的 组合 
进行 比较 这样 就将 子集 的 选择 看作 是 一个 
优化 问题 主要 方法 recursive feature elimination algorithm 递归 特征 
消除 算法 . 这里 有 很多 的 优化 算法 可以 
解决 尤其 是 一些 启发式 的 优化 算法 如 GA 
PSO DE ABC 等 详见 优化 算法 人工 蜂群 算法 
ABC 优化 算法 粒 子群 算法 PSO 优点 考虑到 特征 
与 特征 之间 的 关联性 缺点 1 . 当 观测 
数据 较 少时 容易 过拟合 2 . 当 特征 数量 
较 多时 计算 时间 增长 Embedded Method 折中 思想 旨在 
集合 filter 和 wrapper 方法 的 优点 时间 复杂度 较低 
并且 也 考虑 特征 之间 的 组合 关系 在 事先 
了解 什么 是 好 的 特征 的 的 前提 下 
才 可以 使用 该 方法 主要 方法 正则化 可以 见 
简单 易学 的 机器学习 算法 岭回归 Ridge Regression 岭回归 就是 
在 基本 线性 回归 的 过程 中 加入 了 正则 
项 我们 知道 L1 正则化 自带 特征选择 的 功能 它 
倾向于 留下 相关 特征 而 删除 无关 特征 比如 在 
文本 分类 中 我们 不再 需要 进行 显示 的 特征 
选择 这 一步 而是 直接 将 所有 特征 扔进 带有 
L1 正则化 的 模型 里 由 模型 的 训练 过程 
来 进行 特征 的 选择 优点 集合 了 前面 两种 
方法 的 优点 缺点 必须 事先 知道 什么 是 好 
的 选择 一般来说 特征选择 算法 的 选用 需要 考虑 下 
因素 1 分类器 的 性能 要 显著 提高 学习 算法 
的 性能 可以 采用 Wrapper 模型 例如 可以 选用 采用 
启发式 搜索 策略 的 SBS SLASH 算法 或 基于 遗传算法 
的 Wrapper 方法 GA 2 能否 去除 冗余 特征 如果 
只是 要 去除 不 相关 的 特征 可以 采用 Relief 
系列 算法 互信息 法 MI 或 Symmetric Uncertainty 这些 算法 
可以 有效 的 去除 和 类别 不 相关 的 特征 
但是 无法 去除 冗余 特征 若要 同时 除去 不相关的 和 
冗余 特征 可采用 CFS 算法 或 FCBF 另外 还 可以 
考虑 多种 算法 的 结合 例如 先用 Relief 算法 快速 
去除 不 相关 的 特征 然后 采用 一种 Wrapper 方法 
去除 冗余 特征 3 数据集 的 规模 对于 小 规模 
数据 可以 采用 使用 完全 搜索 策略 的 Filter 模型 
或 Wrapper 模型 例如 BB BFF Bobro 对于 大 规模 
数据 一般 采用 运行 速度快 的 Filter 模型 例如 Relief 
系列 算法 及 FCBF 4 类别 信息 目前 非 监督 
的 特征选择 算法 还 比较 少 在 样本 类别 未知 
的 情况 下 需要 选用 无 监督 的 特征选择 算法 
例如 Dash 等 提出 的 一种 基于 熵 的 Filter 
模型 5 数据集 的 数据 类型 Relief 系列 算法 可以 
处理 数值 的 numeric 或 名 词性 的 nominal 属性 
互信息 MI FCBF 在 处理 连续 的 数值 属性 时 
需要 预先 对 特征 离散化 BB BFF 及 Bobro 等 
则 不能 处理 名 词性 的 属性 Ref Guyon I 
. & Elisseeff A . 2003 . An introduction to 
variable and feature selection . The Journal of Machine Learning 
Research 3 1157 1182 . Hall M . A . 
1999 . Correlation based feature selection for machine learning Doctoral 
dissertation The University of Waikato . 第 3 \ 4 
章 Kohavi R . & John G . H . 
1997 . Wrappers for feature subset selection . Artificial intelligence 
97 1 273 324 . M . Dash H . 
Liu Feature Selection for Classification . In Intelligent Data Analysis 
1 1997 131 – 156 . Lei Yu Huan Liu 
Feature Selection for High Dimensional Data A Fast Correlation Based 
Filter SolutionRicardo Gutierrez Osuna Introduction to Pattern Analysis LECTURE 11 
Sequential Feature Selection http / / courses . cs . 
tamu . edu / rgutier / cpsc689 _ f08 / 
l11 . pdfhttp / / blog . csdn . net 
/ henryczj / article / details / 41043883http / / 
www . cnblogs . com / heaad / archive / 
2011/01 / 02/1924088 . html 