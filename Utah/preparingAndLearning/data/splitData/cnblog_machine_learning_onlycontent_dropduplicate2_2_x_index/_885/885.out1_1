首先 先 来讲 讲闲话 如果 让 你 现在 去 搞 
机器学习 你 会去 吗 不会 的话 是 因为 你 对 
这 方面 不 感兴趣 还是 因为 你 觉得 这 东西 
太 难了 自己 肯定 学 不来 如果 你 觉 的 
太难 了 很好 相信 看完 这篇文章 你 就会 有胆量 踏入 
机器学习 这一 领域 机器学习 Machine Learning 一个 在 才学 一年 
编程 的 人 看来 十分 高大 尚的/nr 东西 不知不觉 就 
接触 了 它 暑假 的 时候 表哥 给 我 布置 
了 任务 在 github 上有 一篇 D e e p 
L e a r n i n g F l 
a p p y B i r d 他 当时 
要 我 一天 之内 先让 这段 代码 跑起来 然后 第二天 
再 把 这段 代码 翻译成 C + + 的 . 
. . . . . . wtf 我 当时 一脸 
懵 逼 一个 才学 编程 一年 不到 的 C/w 和C+/nr 
+/i 的/uj 水平/n 也/d 就/d 差不多/l 是/v 大学/n 课堂/n 教完/v 
后的/nr 那种/r 跑起来 可能 还算 容易 但要 翻译   饶了 
我 吧   orz 当时 配了 一天 的 环境 到 
那天 晚上 快 12 点了 终于 跑 起来 了 然后 
到 了 第二天 我/r 先去/t 查了查/nr tensorflow/w opencv3 在 VS 
上 使用 发现 根本 搞不定 我 电脑 上装 的 是 
VS2013 里面 只有 32位 的 emmmm 得 下个 VS2017 看了 
看 电脑 空间 不够 了 好 的 放弃 以后 有 
空闲 得 慌了 再 来搞 吧 前几天 开学 了 还 
没什么 事 干 突然 就 想到 机器学习 了 其实 dalao 
提到 了 感知机 这 东西 就 试着 去 做了 做 
也就 当个 入门 吧 再来 聊聊 机器学习 机器学习 说白了 跟 
人类 学习 是 一样 的 只不过 机器 可以 无止境 的 
学习 并且 机器学习 时 完全 不 需要 懂 什么 原理 
只要 知道 怎么 去 实现 就行 就如 Alpha Go 你们 
觉得 它 懂得 棋 理 吗 我 觉得 肯定 不懂 
但 通过 对 无数 的 棋谱 的 学习 让人 觉得 
它 肯定 掌握 了 围棋 的 真谛 机器 学习 的 
标准 范式 表达 对于 一个 Task 及其 Performance 的 度量 
方法 给出 特定 的 Algorithm 能够 通过 利用 Experience Data 
不断 提高 在 该 Task 上 的 Performance 的 方法 
就 称为 机器学习 其实 这个 定义 就是 学习 的 全部 
含义 这个事情 按照 机器 的 思路 来做 就是 机器学习 说白了 
只要 算法 合理 每次 学习 都 能有 提高 机器 有 
足够 的 运算 能力 就 能将 机器 的 能力 达到 
极致 从而 超越 人类 机器 学习 的 本质 就是 建立 
数据 认知 关系 库 下面 就来 讲讲 感知 机先 扯扯 
定义 感知机 perceptron 是 二 分类 的 线性 分类 模型 
输入 为 实例 的 特征向量 输出 为 实例 的 类别 
取 + 1 和 1 感知机 对应 于 输入 空间 
中将 实例 划分为 两类 的 分离 超平面 感知机 旨在 求出 
该 超平面 为 求得 超平面 导入 了 基于 误 分类 
的 损失 函数 利用 梯度 下 降法 对 损失 函数 
进行 最 优化 最优化 感知机 的 学习 算法 具有 简单 
而 易于 实现 的 优点 分为 原始 形式 和 对偶 
形式 感知机 预测 是 用 学习 得到 的 感知机 模型 
对 新的 实例 进行 预测 的 因此 属于 判别 模型 
感知机 由 Rosenblatt 于 1957年 提出 的 是 神经 网络 
和 支持 向量 机 的 基础 拿 二维 平面 举例 
看 这张 图 很明显 直线 没有 将 红蓝 点完 全分 
开在 两个 区域 我们 可以 将 其 称为 错误 的 
直线 感知机 要做 的 就是 根据 各点 坐标 将 错误 
的 直线 纠正 为 正确 的 这样 得到 的 直线 
就是 训练 的 结果 说 了 这么 多 要 怎么 
实现 呢 看 下面 的 流程图 / * * 判断 
所 有点 的 位置 关系 进行 分类 * / public 
boolean classify { boolean flag = false while flag { 
for int i = 0 i arrayList . size i 
+ + { if Anwser arrayList . get i = 
0 { Update arrayList . get i break } if 
i + 1 = = arrayList . size { flag 
= true } } } return true } / * 
* 点乘 返回 sum * / private double Dot double 
w double x { double sum = 0 for int 
i = 0 i x . length i + + 
{ sum + = w i * x i } 
return sum } / * * 返回 函数 计算 的 
值 * / private double Anwser Point point { System 
. out . println Arrays . toString w System . 
out . println b return point . y * Dot 
w point . x + b } 如果 还 有疑问 
那 可能 就是 w 与 b 怎么 修 改了 我们 
会 定义 一个 变量 η 0 ≤ η ≤ 1 
作为 步长 在 统计学 是 中 成为 学习 速率 步长 
越大 梯度 下降 的 速度 越快 更 能接 近极 小点 
如果 步长 过大 有 可能 导致 跨过 极 小点 导致 
函数 发散 如果 步长 过小 有 可能 会 耗 很长 
时间 才能 达到 极 小点 默认 为 1 对于 wi 
  wi + = η * y * xi 对于 
b       b + = η * ypublic 
void Update Point point { for int i = 0 
i w . length i + + { w i 
+ = eta * point . y * point . 
x i } b + = eta * point . 
y return } 这样 我们 就 可以 完成 成w/nr b 
的 更新 了 将 上面 的 步骤 组合 起来 就 
可以 实现 感知机 了 谈谈 自己 的 想法 写 了 
这个 感知机 后 发现 机器学习 并 不是 想象 当中 那么 
难 其实 只要 知道 怎么 去 算 就 可以 了 
目前 比较 浅显 的 理解 可能 我 还没 接触到 难 
的 地方 而且 Python 中 有 关机器 学习 的 库 
可以 说是 很 完善 了 如果 熟悉 了 这些 库 
的 用法 即使 不懂 算法 也 时能 用 这些 库 
来做 开发 的 要知道 算法 岗 和 开发 岗 还是 
不 一样 的 这是 我 选取 的 测试用例 及 结果 
1 Point p1 = new Point new double { 0 
0 0 1 } 1 2 Point p2 = new 
Point new double { 1 0 0 0 } 1 
3 Point p3 = new Point new double { 2 
1 0 0 } 1 4 Point p4 = new 
Point new double { 2 1 0 1 } 1 
0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 
0.0 0.0 0.0 1.0 1.0 1.0 0.0 0.0 1.0 0.0 
1.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 1.0 0.0 
1.0 0.0 0.0 1.0 0.0 1.0 1.0 0.0 2.0 1.0 
1.0 1.0 0.0 2.0 1.0 0.0 1.0 0.0 2.0 0.0 
0.0 1.0 0.0 2.0 0.0 1.0 1.0 0.0 2.0 1.0 
1.0 1.0 0.0 2.0 1.0 1.0 1.0 0.0 2.0 1.0 
1.0 1.0 0.0 2.0 1.0 1.0 2.0 0.0 3.0 0.0 
1.0 2.0 0.0 3.0 0.0 0.0 2.0 0.0 3.0 1.0 
0.0 2.0 0.0 3.0 1.0 0.0 2.0 0.0 3.0 1.0 
2.0 1.0 0.0 3.0 2.0 2.0 1.0 0.0 3.0 2.0 
2.0 1.0 0.0 3.0 2.0 2.0 1.0 0.0 3.0 2.0 
0.0 2.0 0.0 4.0 1.0 0.0 2.0 0.0 4.0 1.0 
0.0 2.0 0.0 4.0 1.0 2.0 1.0 0.0 4.0 2.0 
2.0 1.0 0.0 4.0 2.0 2.0 1.0 0.0 4.0 2.0 
2.0 1.0 0.0 4.0 2.0 0.0 2.0 0.0 5.0 1.0 
0.0 2.0 0.0 5.0 1.0 0.0 2.0 0.0 5.0 1.0 
2.0 1.0 0.0 5.0 2.0 2.0 1.0 0.0 5.0 2.0 
2.0 1.0 0.0 5.0 2.0 2.0 1.0 0.0 5.0 2.0 
0.0 2.0 0.0 6.0 1.0 0.0 2.0 0.0 6.0 1.0 
0.0 2.0 0.0 6.0 1.0 2.0 1.0 0.0 6.0 2.0 
2.0 1.0 0.0 6.0 2.0 2.0 1.0 0.0 6.0 2.0 
2.0 1.0 0.0 6.0 2.0 