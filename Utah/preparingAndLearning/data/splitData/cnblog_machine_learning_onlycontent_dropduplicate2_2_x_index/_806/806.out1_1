课程 首页 Coursera Stanford Machine Learning 授课 教授 吴恩 达 
Andrew Ng Week 1 2018 . 10.10 I n t 
r o d u c t i o n L 
i n e a r Regression with One VariableLinear Algebra 
Review1 . What is Machine Learning Arthur Samuel the field 
of study that gives computers the ability to learn without 
being explicitly programmed . Tom Mitchell A computer program is 
said to learn from experience E with respect to some 
class of tasks T and performance measure P if its 
performance at tasks in T as measured by P improves 
with experience E . 举个 例子 在 机器学习 象棋 对弈 
中 E 就是 成千 上万次 的 象棋 对弈 实战 T 
就是 象棋 对弈 这一 任务 P 就是 机器 在 下次 
对弈 中 获胜 的 可能性 那么 上述 第二个 定义 就 
可以 表述 为 对于 象棋 对弈 这一 学习 任务 机器学习 
能从 不断 的 对弈 实战 经验 中 提升 自己 的 
对弈 获胜 概率 2 .   Supervised Learning &   
Unsupervised LearningIn supervised learning we are given a data set 
and already know what our correct output should look like 
having the idea that there is a relationship between the 
input and the output .   Supervised learning problems are 
categorized into regression and classification problems . Unsupervised learning allows 
us to approach problems with little or no idea what 
our results should look like . We can derive structure 
from data where we don t necessarily know the effect 
of the variables . 机器学习 问题 通常 可以 分为 两类 
监督性 学习 和无/nr 监督性 学习 二者 的 区别 就 在于 
Supervised 翻成 中文 监督性 似乎 有点 晦涩 其实 可以 理解 
为 是否 给 定了 正确 答案 或 方向 在 监督 
性 学习 中 数据集 的 每个 样本 均有 提供 正确 
答案 如 这张 照片 是 一名 35岁 的 女生 或 
这个 肿瘤 样本 是 良性 的 可 据此 进行 预测 
或 判定 例如 通过 人物 正面 照 预测 他 / 
她 的 年龄 也 称为 回归 问题 map input variables 
to some continuous function 或者 判定 病人 身上 的 肿瘤 
是 良性 还是 恶性 也 称为 分类 问题 map input 
variables into discrete categories 在 无 监督 性 问题 中 
只 给定 数据集 每个 样本 并 没有 给出 正确 答案 
要求 在 数据 中 自动 找出 某种 结构 进行 预测 
或 分类 其 结果 也 不 存在 反馈 例如 Google 
对 当日 新闻 进行 同 主题 自动 分类 或者 对 
相互 混杂 的 声音 应用 Cocktail Party Algorithm 进行 语音 
识别 3 .   Model and Cost FunctionModel Representation 给定 
训练 集 通过学习 算法 得到 假设 模型 那么 对 任意 
的 输出 可以 通过 假设 模型 得到 预测值 Training Set 
Learning Algorithm Hypothesis \ h \   =   \ 
Y = h X \ . Cost Function 用来 描述 
假设 函数 的 精确度 即 计算 预测值 与 实际 值 
的 距离 在 单 变量 线性 回归 问题 中 \ 
h _ θ x _ i = θ _ 0 
+ θ _ 1 x _ i \ 那么 $ 
$ J θ _ 0 θ _ 1 = \ 
frac { 1 } { 2m } \ sum _ 
{ i = 1 } ^ m \ hat { 
y } _ i y _ i ^ 2 = 
  \ frac { 1 } { 2m } \ 
sum _ { i = 1 } ^ m h 
_ θ x _ i y _ i ^ 2 
$ $ 我们 的 目标 就是 最小化 $ J θ 
_ 0 θ _ 1 $ 如果 进一步 简化 令 
$ θ _ 0 = 0 $ 那么 就是 最小化 
$ J θ _ 1 $ 这样 预测值 与 实际 
值 就 最接近 在 凸函数 中 就是 找到 cost function 
的 最低点 4 . Gradient Descent 梯度 下降 顾名思义 就是 
沿着 梯度 下降 最快 的 地方 最 容易 走到 最低点 
想象 你 站在 山坡上 要 去往 山脚 但却 不 知道 
路 你 可以 考察 前后左右 各个 方向 下降 最快 的 
地方 最 有可能 是 通往 山脚 的 路径 但 也 
有 可能 走到 一个 山谷 去 而 不是 真正 的 
山脚 也 就是 所谓 的 困在 了 局部 最小值 中 
算法 如下 1 随机 选择 两个 初始值 $ θ _ 
0 θ _ 1 $ 2 重复 执行 $ θ 
_ j =   θ _ j α \ frac 
{ \ partial } { \ partial θ _ j 
} J θ _ 0 θ _ 1   $ 
其中 $ j = 0 1 $ 直至 收敛 注 
$ θ _ 0 θ _ 1 $ 必须 同时 
更新 其中 $ α $ 称为 Learning Rate 学习 速率 
或 步长 如果 太小 的话 梯度 下降 会 很慢 如果 
太大 的话 又 可能 无法 收敛 甚至 发散 另外 如果在 
每一步 的 下降 中 都 选用 全部 的 样本 进行 
计算 那么 这 称为 Batch Gradient Descent 与之 对应 的 
有 Stochastic Gradient Descent 和 Mini batch Gradient Descent 后面 
会 介绍 5 . Linear Algebra Review 线性代数 的 复习 
不 赘述 需要 了解 的 概念 Matrices and Vectors element 
dimension 1 indexed scalar   Addition and Scalar Multiplication   
Matrix Vector Multiplication   Matrix Matrix Multiplication associative   not 
commutative Identity Matrix   Inverse and Transpose . Week 2 
2018 . 10.14 Linear Regression with Multiple VariablesOctave / Matlab 
Tutorial1 .   Multivariate Linear Regression 在 线性 回 归中 
如果 变量 / 特征 不止 一个 如 根据 样本 的 
面积 房间数 楼层 预测 房屋 价值 那么 就 称为 多元 
线性 回归 令 $ m $ 为 训练 样本数 $ 
n $ 为 变量 / 特征 数 $ x ^ 
{ i } $ 为 第 $ i $ 个 
训练样本 $ x _ j ^ { i } $ 
为 第 $ i $ 个 样本 的 第 $ 
j $ 个 变量 / 特征 那么 之前 的 假设 
模型 $ h _ θ x $ 则为 $ $ 
  h _ θ x = θ _ 0 + 
θ _ 1 x _ 1 + θ _ 2 
x _ 2 +   \ cdots +   θ 
_ n x _ n = \ left   \ 
begin { matrix }   θ _ 0 &   
θ _ 1 &   \ cdots &   θ 
_ n   \ end { matrix }   \ 
right \ left   \ begin { matrix }   
x _ 0 \ \ x _ 1 \ \ 
  \ vdots \ \ x _ n   \ 
end { matrix }   \ right   =   
θ ^ T x $ $ 其中 $ x _ 
0 ^ { i } = 1 $ 那么 多 
元 线性 回归 中的 梯度 下降 算 法则 修改 为 
1 选定 初始值 2 重复 执行 $ θ _ j 
=   θ _ j α \ frac { 1 
} { m } \ sum _ { i = 
1 } ^ m h _ θ x ^ { 
i } y ^ { i } \ cdot x 
_ j ^ { i }   $ 直至 收敛 
算法 的 实际 应用 中 由于 $ θ $ 对于 
范围 小 的 输入 变量 下降 较快 范围 大 的 
下降 慢 所以 当 变量 之间 不 平均 时 容易 
振荡 下降 性能 不佳 因此 可以 采用 特征 缩放 Feature 
Scaling 和 均值 归一化 处理 Mean Normalization 技巧 来 调整 
输入 变量 的 范围 以此 加快 收敛 减少 迭代 的 
次数 即 $ $ x _ i = \ frac 
{ x _ i μ _ i } { s 
_ i }   $ $ 其中 $ μ _ 
i $ 是 样本 中 所有 特征 的 均值 $ 
s _ i $ 是 特征值 的 范围 max min 
或 称为 标准差 举个 例子 如果 数据 集中 房屋 样本 
的 价格 范围 为 100 到 2000 平均 价格 为 
1000 那么 $ x _ i = price 1000 / 
1900 $ 而 对于 步长 $ α $ 如果 太小 
则 收敛 慢 如果 太大 则 可能 不 收敛 甚至 
发散 在 具体 调试 中 可以 做 $ J θ 
$ 随 迭代 次数 变化 的 图 如果 随着 迭代 
次数 增加 曲线 下降 那么 说明 梯度 下降 算法 正常工作 
如果 曲线 反而 上升 那么 可以 考虑 减小 $ α 
$ 判断 是否 收敛 可以 考察 $ J θ $ 
在 一次 迭代 中 是否 下降 小于 某个 小 值 
$ ε $ 如 $ 10 ^ { 3 } 
$ 此外 如果 可以 用 新的 一个 变量 代替 多个 
变量 的 组合 将 减少 算法 的 计算 量 例如 
在 房价 预测 中 对于 房屋 的 长 $ x 
_ 1 $ 和宽$/nr x _ 2 $ 可以 用 
面积 $ x _ 3 $ 来 代替 之前 的 
例子 中 我们 用 直线 去 拟合 数据 但 实际 
中 直线 不一定 能 很好 的 拟合 数据 因此 我们 
可以 采用 多项式 回归 Polynomial Regression 即 $ $ h 
_ θ x = θ _ 0 + θ _ 
1 x _ 1 + θ _ 2 x _ 
1 ^ 2 + \ cdots + θ _ n 
x _ 1 ^ n $ $ 不过 值得 注意 
的 是 在 这种 情况 下 特征 缩放 就 尤为重要 
因为 1 1000 范围 的 三次方 就 已经 是 1 
1000000000 2 .   Computing Parameters Analytically 除了 梯度 下 
降法 还 可以 使用 正规 方程 Normal Equation 来 求解 
$ θ $ 的 最优 值 假设 数据集 $ X 
$ 有$m/nr $ 个 样本 $ n $ 个 特征 
那么 $ $ X =   \ left   \ 
begin { matrix } 1 & x _ 1 ^ 
{ 1 } &   x _ 2 ^ { 
1 }   & \ cdots   & x _ 
n ^ { 1 } \ \   1 & 
x _ 1 ^ { 2 } &     
x _ 2 ^ { 2 } & \ cdots 
  & x _ n ^ { 2 }   
\ \   \ vdots & \ vdots &   
\ vdots & \ vdots   &   \ vdots 
\ \   1 & x _ 1 ^ { 
m } &     x _ 2 ^ { 
m } & \ cdots   & x _ n 
^ { m }   \ end { matrix } 
  \ right y = \ left   \ begin 
{ matrix } y ^ { 1 } \ \ 
y ^ { 2 } \ \ \ vdots   
\ \   y ^ { m } \ end 
{ matrix }   \ right     θ = 
  \ left   \ begin { matrix } θ 
_ 0 \ \ θ _ 1   \ \ 
\ vdots   \ \   θ _ n \ 
end { matrix }   \ right   $ $ 
则 可以 求出 $ θ = X ^ T X 
^ { 1 } X ^ Ty $ 相比 梯度 
下 降法 正规 方程 法的/nr 优点 在于 无需 选择 步长 
$ α $ 也 无需 迭代计算 也 无需 进行 特征 
缩放 缺点 在于 时间 复杂度 较高 为 $ O n 
^ 3 $ 同时 需要 计算 $ X ^ TX 
$ 的 逆 矩阵 当 $ n $ 很大 时 
超过 10000 算法 将会 变得 很慢 而 梯度 下 降法 
的 时间 复杂度 为 $ O kn ^ 2 $ 
当 $ n $ 很大 时 仍然 能 奏效 不过 
你 可能 会 质疑 $ X ^ TX $ 的 
逆 矩阵 是否 存在 逆 矩阵 存在 的 定理 请 
回顾 线性代数 课程 不过 请 放心 不 存在 的 情况 
是 极少 的 而且在 Octave 中 用 Pinv 命令 去求 
逆 矩阵 事实上 它 是 求 伪 逆 pseudo inverse 
操作 即便 矩阵 不可逆 也能 求解 如果 $ X ^ 
TX $ 真的 不可逆 那么 可以 考虑 是否 有 • 
  冗余 变量 Redundant Features 也 就是 两个 变量 相关性 
很高 如 线性 依赖 •   太多 变量 相 比较 
之下 样本数 不足 这种 情况 下 可以 删除 一些 特征 
或者 使用 正则化 regularization 3 . Octave / Matlab Tutorial 
语法 跟 MATLAB 差不多 找 本书 或 找 篇 相关 
文章 看看 应该 就能 上手 介绍 了 Basic Operations Moving 
Data Around Computing on Data Plotting Data Control Statements for 
while if Vectorization 建议 做 一下 第一课 的 编程 作业 
熟悉 下 Week 3 2018 . 10.17 Logistic R e 
g r e s s i o n R e 
g u l a r i z a t i 
o n 1 .   Classification and Representation 前面 主要 
介绍 了 回归 问题 在 分类 Classification 问题 中 预测值 
是 离散 的 如 判断 邮件 是否 是 垃圾 邮件 
肿瘤 是 良性 还是 恶性 即 常见 的 二元 分类 
问题 Binary Classification Problem $ y \ in \ { 
0 1 \ } $ 0 通常 被 称为 负 
类 1 是 正 类 也 可用 和 + 表示 
和 多元 分类 问题 对于 分类 问题 也 可以 借鉴 
之前 的 线性 回归 算法 不过 需要 变化 一下 由于 
$ y \ in \ { 0 1 \ } 
$ 所以 $ h _ θ x $ 也 满足 
这个 范围 我们 可以 采用 Sigmoid 函数 或称 Logistic 函数 
$ $ g z = \ frac { 1 } 
{ 1 + e ^ { z } } $ 
$ 它 的 函数 曲线 大致 如下 图 所示 可以 
看出 当 $ z ≥ 0 $ 时 $ g 
z ≥ 0.5 $ 当 $ z → ∞ $ 
时 $ g z = 1 $ 当 $ z 
  → ∞ $ 时 $ g z = 0 
$ 令 $ h _ θ x = g θ 
^ T x $ 它 给 出了 $ y = 
1 $ 的 概率 如 $ h _ θ x 
= 0.7 $ 意味着 有 70% 的 概率 输出 为 
1 或 30% 的 概率 为 0 如果 判定 $ 
h _ θ x   ≥ 0.5 $ 时 $ 
y = 1 $ $ h _ θ x   
0.5 $ 时 $ y = 0 $ 那么 决策 
边界 Decision Boundary 就是 $ h _ θ x   
= 0.5 = g θ ^ T x $ 也 
就是 $ θ ^ T x ≥ 0 $ 时 
$ y = 1 $ $ θ ^ T x 
0 $ 时 $ y = 0 $ 另外 $ 
z $ 可以 是 非线性 的 如 $ z = 
θ _ 0 +   θ _ 1 x _ 
1 ^ 2 + θ _ 2 x _ 2 
^ 2 $ 2 . Logistic Regression Model 对于 逻辑 
回归 之前 线性 回归 中的 Cost Function 可能 不 适用 
因为 代价 函数 有 可能 是 个 非 凸函数 有 
许多 局部 最 优点 因此 逻辑 回归 的 代价 函数 
修改 如下 $ $ J θ = \ frac { 
1 } { m } \ sum _ { i 
= 1 } ^ { m } Cost h _ 
θ x ^ { i } y ^ { i 
}     $ $ 其中 $ Cost h _ 
θ x ^ { i } y ^ { i 
} = log h _ θ x $ 当 $ 
y = 1 $ 时 $ Cost h _ θ 
x ^ { i } y ^ { i } 
= log 1 h _ θ x $ 当 $ 
y = 0 $ 时 注意到 当 $ h _ 
θ x = y $ 时 $ Cost h _ 
θ x ^ { i } y ^ { i 
} $ 为 0 当 $ y = 0 $ 
而 $ h _ θ x = 1 $ 时 
或者 $ y = 1 $ 而 $ h _ 
θ x = 0 $ 时 $ Cost h _ 
θ x ^ { i } y ^ { i 
} $ 为 无穷大 这是 符合 逻辑 的 同时 在 
这种 情况 下 保证 了 成本 函数 是 凸函数 我们 
可以 进一步 作 简化 令 $ Cost h _ θ 
x ^ { i } y ^ { i } 
= y log h _ θ x 1 y log 
1 h _ θ x $ 那么 代价 函数 为 
$ $ J θ = \ frac { 1 } 
{ m } \ sum _ { i = 1 
} ^ { m } y ^ { i } 
log h _ θ x ^ { i } + 
1 y ^ { i } log 1 h _ 
θ x ^ { i }   $ $ 对应 
的 向 量化 实现 方式 为 $ J θ = 
\ frac { 1 } { m } \ cdot 
\ left y ^ T log h 1 y ^ 
T log 1 h \ right $ 之前 线性 回归 
中的 梯度 下降 算法 在 逻辑 回归 中 同样 适用 
即 重复 执行 $ θ _ j =   θ 
_ j α \ frac { 1 } { m 
} \ sum _ { i = 1 } ^ 
m h _ θ x ^ { i } y 
^ { i } \ cdot x _ j ^ 
{ i }   $ 直至 收敛 此外 可以 运用 
共轭 梯度 Conjugate gradient BFGS L BFGS 等 更 高级 
的 算法 这些 算法 能够 更快 的 得出 最优 解 
不过 不 建议 自己 从头 写 直接 调用 已 有的 
库函数 即可 如 Octave 中的 fminunc 函数 3 . Multiclass 
Classification 在 多元 分类 问题 中 我们 采用 一对多 的 
方法 1 选择 一个 分类 将 剩下 的 所有 不论 
有几个 都 看成 第二 个 分类 然后 求出 对应 的 
假设 函数 $ h _ θ ^ { i } 
x $ 依此 对 所有 分类 进行 训练 2 对于 
需要 进行 分类 的 $ x $ 选出 $ h 
_ θ ^ { i } x $ 里 最大 
的 那一个 即为 分类 值 4 . Solving the Problem 
of Overfitting 在 一些 情况 下 直线 并 不能 很好 
的 拟合 数据 下 这 称为 欠 拟合 Underfitting 或 
high bias 通常 是 由于 预测 函数 太 简单 或者 
使用 特征值 太少 于是 我们 会 想用 曲线 去 拟合 
增加 一个 额外 的 特征 变量 $ x ^ 2 
$ 能够 取得 不错 的 效果 下 但是 特征 变量 
并 不是 越多越好 对 数据集 样本 更 精确 的 拟合 
并不 一定 能更/nr 准确 的 进行 预测 太 复杂 的 
函数 往往 带来 许多 不 必要 的 曲线 和 转角 
如下 这 称为 过拟合 Overfitting 对于 过拟合 通常 采用 如下 
两个 方式 处理 1 减少 特征 变量 数 手动 选择 
要 保留 的 特征 或 使用 模型 选择 算法 2 
正则化 Regularization 保留 所有 特征 但 减少 $ θ _ 
j $ 的 贡献 当 我们 有 许多 只有 些许 
用处 的 特征 但又 不想 舍弃 的 情况 下 正则化 
是 很好 的 选择 5 . Regularization 正则化 通过 增加 
部分 项的/nr 代价 以 减少 它们 的 权重 避免 过拟合 
现象 例如 有 假设 函数 为 $ θ _ 0 
+ θ _ 1 x   + θ _ 2 
x ^ 2   + θ _ 3 x ^ 
3   + θ _ 4 x ^ 4 $ 
如果 我们 想 要 减少 最后 两项 的 影响 那么 
可以 修改 代价 函数 为 $ min _ θ \ 
frac { 1 } { 2m } \ sum _ 
{ i = 1 } ^ { m } h 
_ θ x _ { i } y _ { 
i } ^ 2 + 1000 \ cdot θ _ 
3 ^ 2 + 1000 \ cdot θ _ 4 
^ 2   $ 这种 情况 下 为了 使 代价 
函数 接近 0 必须 减小 $ θ _ 3 $ 
和$θ/nr _ 4 $ 至 趋于 0 以此 更好 的 
拟合 曲线 减少 过拟合 推广 到 一般 情况 则 得到 
$ $ min _ θ \ frac { 1 } 
{ 2m } \ sum _ { i = 1 
} ^ { m }   h _ θ x 
_ { i } y _ { i } ^ 
2 + λ \ sum _ { j = 1 
} ^ { n } θ _ j ^ 2 
  $ $ 其中 $ λ $ 称为 正则化 参数 
它 对 曲线 起 平缓 作用 如果 它 选得 太大 
容易 造成 太 平滑 导致 欠 拟合 如果 太小 或 
等于 0 则 起 不到 正则化 的 效果 仍然 会有 
过拟合 的 现象 正则化 方法 在 线性 回归 和 逻辑 
回归 中 均可 应用 在 线性 回 归中 如果 使用 
梯度 下降 算法 那么 只 需要 关键步骤 为 重复 执行 
$ θ _ 0 =   θ _ 0 α 
\ frac { 1 } { m } \ sum 
_ { i = 1 } ^ m h _ 
θ x ^ { i } y ^ { i 
} x _ 0 ^ { i }   $ 
$ θ _ j =   θ _ j α 
\ left \ left \ frac { 1 } { 
m } \ sum _ { i = 1 } 
^ m h _ θ x ^ { i } 
y ^ { i } x _ j ^ { 
i } \ right + \ frac { λ } 
{ m } θ _ j \ right = θ 
_ j 1 α \ frac { λ } { 
m } α \ frac { 1 } { m 
} \ sum _ { i = 1 } ^ 
m h _ θ x ^ { i } y 
^ { i } x _ j ^ { i 
} \ qquad j \ in \ { 1 2 
\ cdots n \ } $ 直至 收敛 注意到 $ 
1   α \ frac { λ } { m 
} $ 永远 小于 1 可以 看成 在 每次 迭代 
中 减少 $ θ _ j $ 的 值 如果 
使用 正规 方程 那么 之前 的 求解 式 就 变成 
$ θ = X ^ T X + λ \ 
cdot L ^ { 1 } X ^ Ty $ 
其中 $ $ L =   \ left   \ 
begin { matrix } 0 & \ quad &   
\ quad   &   \ quad &   \ 
quad \ \   \ quad & 1 &   
\ quad   &   \ quad   &   
\ quad \ \   \ quad &   \ 
quad &   1 &   \ quad   & 
  \ quad \ \   \ quad   & 
  \ quad   &   \ quad &   
\ ddots   &   \ quad   \ \ 
  \ quad &   \ quad &   \ 
quad   & \ quad   & 1   \ 
end { matrix }   \ right $ $ 同时 
如果 m n 那么 $ X ^ T X $ 
是 不可逆的 但当 引入 了 λ 后 $ X ^ 
T X + λ \ cdot L $ 变为 一定 
可逆 在 逻辑 回 归中 类似 的 可以 修改 代价 
函数 为 $ $ J θ = \ frac { 
1 } { m } \ sum _ { i 
= 1 } ^ { m } y ^ { 
i } log h _ θ x ^ { i 
} + 1 y ^ { i } log 1 
h _ θ x ^ { i } + \ 
frac { λ } { 2m }   \ sum 
_ { j = 1 } ^ n \ theta 
_ j ^ 2 $ $ 即 可利用 梯度 下降 
算法 处理 Week 4 2018 . 10.29 Neural Networks Representation1 
. Why Neural Networks 之前 的 课程 中 已经 有 
提到 Non linear Hypotheses 例如 可以 对 特征 进行 多项式 
组合 这样 就 能 得到 更 完美 的 拟合 有时 
也 会 变成 过拟合 当 特征 数量 少时 是个 不错 
的 选择 但是 当 特征 逐渐 增多 时 计算 量 
将 急剧 递增 在 实际 的 机器 学习 问题 中 
并 不是 很好 的 选择 举个 例子 计算机 希望 识别 
出 图 片中 的 物体 是否 是 一辆车 对于 人眼 
来说 自然 很 容易 做到 但在 计算机 眼中 这 就是 
一幅 二维 矩阵 再 加上 RGB 三个 通道 即便 是 
100x100 大小 的 照片 也有 10000个 特征点 如果 考虑 $ 
x _ i x _ j $ 的 二阶 组合 
那么 将 组合 出 $ 5 \ times 10 ^ 
7 $ 个 特征 计算 量 特别 庞大 因此 我们 
需要 一种 新的 算法 神经 网络 在 解决 复杂 的 
非线性 分类 问题 上 被 证明 是 一种 很好 的 
算法 即使 对于 特征 维数 n 很大 的 情况 神经网络 
Neural Networks 在 80 年代 得到 广泛 应用 其 流 
行度 在 90 年代 有所 消退 但 近年来 随着 AI 
的 兴起 再度 受到 人们 的 关注 神经 网络 的 
思想 是 试图 模仿 大脑 的 信号 处理 方式 我们 
知道 人类 可以 通过 看 听 触 味觉 去 感知 
这个 世界 那么 大脑 是 如何 处理 这些 信号 而 
获得 对 外部 世界 的 认知 呢 如果 计算机 能 
学会 大脑 的 处理 方式 是不是 就 拥有 了 大脑 
的 学习 能力 2 . Neural Networks Model 一个 神经元 
有 很多 树突 dendrites 呈 放射状 是 接受 从 其它 
神经元 传来 的 信号 的 入口 如 触觉 和 听觉 
当 它 接收 的 刺激 足够 强 超出 一定 阈值 
就会 兴奋 并 向后 传导 也 就是 通过 轴突 axons 
将 当前 神经元 发出 的 信号 传递 给 下一个 神经元 
每个 神经元 有一个 轴突 两个 神经元 之间 接触 的 地方 
就是 突触 如果 用 一个 简单 的 逻辑 单元 来 
表示 神经 模型 那么 树突 就 相当于 输入 轴突 就是 
输出 神经元 的 细胞核 就是 处理函数 $ $ \ begin 
{ bmatrix }   x _ 0 \ \ x 
_ 1 \ \ x _ 2   \ end 
{ bmatrix } \ rightarrow \ quad \ rightarrow h 
_ \ theta x $ $ 其中 $ h _ 
\ theta x = \ frac { 1 } { 
1 + e ^ { \ theta ^ { T 
} x } } $ 称为 Sigmoid logistic 激活 函数 
activation function 推广 到 神经 网络 中 我们 会 有 
多个 输入 input layer 通过 多层 的 神经 传导 hidden 
layer 最后 输出 output layer 其中 hidden layer 中的 节点 
又 称为 激活 单元 activation units 具体 如下 $ $ 
\ begin { align * } & a _ i 
^ { j } = \ text { activation of 
unit $ i $ in layer $ j $ } 
\ newline & \ Theta ^ { j } = 
\ text { matrix of weights controlling function mapping from 
layer $ j $ to layer $ j + 1 
$ } \ end { align * } $ $ 
每个 激活 单元 可以 通过 如下 式子 得到 $ $ 
  \ begin { align * } a _ 1 
^ { 2 } = g \ Theta _ { 
10 } ^ { 1 } x _ 0 + 
\ Theta _ { 11 } ^ { 1 } 
x _ 1 + \ Theta _ { 12 } 
^ { 1 } x _ 2 + \ Theta 
_ { 13 } ^ { 1 } x _ 
3 \ newline a _ 2 ^ { 2 } 
= g \ Theta _ { 20 } ^ { 
1 } x _ 0 + \ Theta _ { 
21 } ^ { 1 } x _ 1 + 
\ Theta _ { 22 } ^ { 1 } 
x _ 2 + \ Theta _ { 23 } 
^ { 1 } x _ 3 \ newline a 
_ 3 ^ { 2 } = g \ Theta 
_ { 30 } ^ { 1 } x _ 
0 + \ Theta _ { 31 } ^ { 
1 } x _ 1 + \ Theta _ { 
32 } ^ { 1 } x _ 2 + 
\ Theta _ { 33 } ^ { 1 } 
x _ 3 \ newline h _ \ Theta x 
= a _ 1 ^ { 3 } = g 
\ Theta _ { 10 } ^ { 2 } 
a _ 0 ^ { 2 } + \ Theta 
_ { 11 } ^ { 2 } a _ 
1 ^ { 2 } + \ Theta _ { 
12 } ^ { 2 } a _ 2 ^ 
{ 2 } + \ Theta _ { 13 } 
^ { 2 } a _ 3 ^ { 2 
} \ newline \ end { align * } $ 
$ 也 就是说 如果 神经 网络 的 第 $ j 
$ 层 有$s/nr _ j $ 个 单元 第 $ 
j + 1 $ 层 有$s/nr _ { j + 
1 } $ 个 单元 那么 $ \ Theta ^ 
{ j } $ 的 维度 就是 $ s _ 
{ j + 1 } \ times s _ j 
+ 1 $ 其中 + 1 是 考虑 了 $ 
x _ 0 $ 和$\/nr Theta _ 0 ^ { 
j } $ 举个 例子 如果 第一 层 有 2个 
输入 节点 第二层 有 4个 激活 节点 那么 $ \ 
Theta ^ { 1 } $ 的 维度 就是 4x3 
对于 上式 的 例子 我们 可以 采用 向 量化 编程 
来 实现 我们 用 $ z _ k ^ { 
j } $ 来 代替 括号 内 的 式子 那么 
$ $ \ begin { align * } a _ 
1 ^ { 2 } = g z _ 1 
^ { 2 } \ newline a _ 2 ^ 
{ 2 } = g z _ 2 ^ { 
2 } \ newline a _ 3 ^ { 2 
} = g z _ 3 ^ { 2 } 
\ newline \ end { align * } $ $ 
也就是说 对于 第二层 j = 2 和 节点 k $ 
z _ k ^ { 2 } = \ Theta 
_ { k 0 } ^ { 1 } x 
_ 0 + \ Theta _ { k 1 } 
^ { 1 } x _ 1 + \ cdots 
+   \ Theta _ { k n } ^ 
{ 1 } x _ n $ 那么 $ x 
$ 和$z/nr ^ j $ 的 向量 表示 为 $ 
$ \ begin { align * } x = \ 
begin { bmatrix } x _ 0 \ newline x 
_ 1 \ newline \ cdots \ newline x _ 
n \ end { bmatrix } & z ^ { 
j } = \ begin { bmatrix } z _ 
1 ^ { j } \ newline z _ 2 
^ { j } \ newline \ cdots \ newline 
z _ n ^ { j } \ end { 
bmatrix } \ end { align * } $ $ 
令 $ x = a ^ { 1 } $ 
那么 $ z ^ { j } = \ Theta 
^ { j 1 } a ^ { j 1 
} $ 也 就是 $ h _ \ Theta x 
= a ^ { j + 1 } = g 
z ^ { j + 1 } $ 值得 注意 
的 是 最后 一步 其实 就 很像 逻辑 回归 3 
. Example and Intuitions 上面 的 公式 似乎 比较 难 
理解 举个 简单 的 例子 如果 我们 要 用 神经 
网络 来 预测 $ x _ 1 $ AND $ 
x _ 2 $ 那么 对应 的 描述 为 $ 
$ \ begin { align * } \ begin { 
bmatrix } x _ 0 \ newline x _ 1 
\ newline x _ 2 \ end { bmatrix } 
\ rightarrow \ begin { bmatrix } g z ^ 
{ 2 } \ end { bmatrix } \ rightarrow 
h _ \ Theta x \ end { align * 
} $ $ 其中 权 重为 $ \ Theta ^ 
{ 1 } = 30 \ quad 20 \ quad 
20 $ 那么 回忆 Sigmoid 函数 的 曲线 特征 有 
$ $   \ begin { align * } & 
h _ \ Theta x = g 30 + 20x 
_ 1 + 20x _ 2 \ newline \ newline 
& x _ 1 = 0 \ \ and \ 
\ x _ 2 = 0 \ \ then \ 
\ g 30 \ approx 0 \ newline & x 
_ 1 = 0 \ \ and \ \ x 
_ 2 = 1 \ \ then \ \ g 
10 \ approx 0 \ newline & x _ 1 
= 1 \ \ and \ \ x _ 2 
= 0 \ \ then \ \ g 10 \ 
approx 0 \ newline & x _ 1 = 1 
\ \ and \ \ x _ 2 = 1 
\ \ then \ \ g 10 \ approx 1 
\ end { align * } $ $ 也 就是 
通过 这种 方式 我们 构 造出 了 一个 与门 的 
神经网路 同样 的 只要 改变 下 权重 我们 可以 构 
造出 或 门 或 其它 逻辑门 那么 在 第 2层 
我们 可以 设置 多个 激活 单元 通过 不同 的 权重 
实现 不同 的 逻辑门 然后 送往 第 3层 通过 组合 
实现 更 复杂 的 输入输出 依次 类推 对于 更为 复杂 
的 非线性 方程 神经 网络 都 可以 很好 的 模拟 
那么 对于 多元 分类 不再 是 只有 一个 输出 而是 
多个 输出   每个 中间层 都 提供 了 新的 信息 
以供 最后 的 判断 Week 5 2018 . 12.04 Neural 
Networks Learning1 .   Backpropagation 在 神经 网络 中 首先 
我们 定义 $ L $ 网络 中的 总 层数 $ 
s _ l $ 第 $ l $ 层 中的 
节点 数量 不包括 偏差 节点 $ K $ 输出 节点 
/ 分类 数量 回忆 逻辑 回归 中的 代价 函数 $ 
$   J \ theta = \ frac { 1 
} { m } \ sum _ { i = 
1 } ^ m y ^ { i } \ 
\ log h _ \ theta x ^ { i 
} + 1 y ^ { i } \ \ 
log 1 h _ \ theta x ^ { i 
} + \ frac { \ lambda } { 2m 
} \ sum _ { j = 1 } ^ 
n \ theta _ j ^ 2 $ $ 那么 
神经 网络 中 的 代价 函数 可以 写成 $ $ 
  \ begin { gather * } J \ Theta 
= \ frac { 1 } { m } \ 
sum _ { i = 1 } ^ m \ 
sum _ { k = 1 } ^ K \ 
left y ^ { i } _ k \ log 
h _ \ Theta x ^ { i } _ 
k + 1 y ^ { i } _ k 
\ log 1 h _ \ Theta x ^ { 
i } _ k \ right + \ frac { 
\ lambda } { 2m } \ sum _ { 
l = 1 } ^ { L 1 } \ 
sum _ { i = 1 } ^ { s 
_ l } \ sum _ { j = 1 
} ^ { s _ { l + 1 } 
} \ Theta _ { j i } ^ { 
l } ^ 2 \ end { gather * } 
$ $ 看着 略 复杂 其实 相比 只是 加了 几个 
嵌套 求和 上式 的 第一 部分 在 方括号 前 加 
了 一个 额外 的 对 输出 节点 从1到/nr K 的 
循环 在 正则化 部分 我们 对 每 一层 的 $ 
\ Theta $ 都 进行 考虑 加权 即从 第 $ 
1 $ 层 到 $ L 1 $ 层 第 
$ l $ 层 的 每一个 节点 到 第 $ 
l + 1 $ 层 的 每一个 节点 对应 的 
$ \ Theta _ { j i } ^ { 
l } $ 定义 了 代价 函数 我们 的 目标 
当然 就是 $ \ min _ \ Theta J \ 
Theta $ 如果 利用 梯度 下 降法 我们 就 需要 
计算 $ \ dfrac { \ partial } { \ 
partial \ Theta _ { i j } ^ { 
l } } J \ Theta $ 这 就 需要 
引入 反向 传播 Backpropagation 算法 给定 训练 集 对于 所有 
的 $ l i j $ 设 $ \ Delta 
^ { l } _ { i j } = 
0 $ 对于 训练样本 t = 1 到 m 1 
. 设 $ a ^ { 1 } = x 
^ { t } $ 2 . 执行 前 向 
传播 计算 $ a ^ { l } $ 即 
$ a ^ { 1 } = x z ^ 
{ 2 } = \ Theta ^ { 1 } 
a ^ { 1 } a ^ { 2 } 
= g z ^ { 2 } z ^ { 
3 } = \ Theta ^ { 2 } a 
^ { 2 } \ cdots $ 3 . 利用 
$ y ^ { t } $ 计算 最后 一层 
的 误差 $ \ delta ^ { L } = 
a ^ { L } y ^ { t } 
$ 这样 就能 往前 计算 每 一层 的 误差 4 
. 利用 $ \ delta ^ { l } = 
\ Theta ^ { l } ^ T \ delta 
^ { l + 1 } \ . * \ 
a ^ { l } \ . * \ 1 
a ^ { l } $ 计算 之前 每 一层 
的 误差 其中 也 可用 $ g z ^ { 
l } = a ^ { l } \ . 
* \ 1 a ^ { l } $ 来 
简化 形式 5 . 更新 $ \ Delta ^ { 
l } _ { i j } = \ Delta 
^ { l } _ { i j } + 
a _ j ^ { l } \ delta _ 
i ^ { l + 1 } $ 那么 $ 
$ D ^ { l } _ { i j 
} = \ dfrac { 1 } { m } 
\ left \ Delta ^ { l } _ { 
i j } + \ lambda \ Theta ^ { 
l } _ { i j } \ right j 
\ ne 0 $ $ $ $ D ^ { 
l } _ { i j } = \ dfrac 
{ 1 } { m } \ Delta ^ { 
l } _ { i j } j = 0 
$ $ 这 就是 我们 要求 的 偏 导数 $ 
\ frac \ partial { \ partial \ Theta _ 
{ ij } ^ { l } } J \ 
Theta = D _ { ij } ^ { l 
} $ 这个 算法 乍 看上去 很 难理解 到底 是 
在做 些 什么 考虑 一个 简单 情况 非 多类 分类 
问题 k = 1 且 忽略 正则化 对于 单个 样本 
$ x ^ { t } y ^ { t 
} $ 代价 函数 为 $ cost t = y 
^ { t } \ \ log h _ \ 
Theta x ^ { t } + 1 y ^ 
{ t } \ \ log 1 h _ \ 
Theta x ^ { t } $ 直观 上 来说 
$ \ delta _ j ^ { l } $ 
是 $ a ^ { l } _ j $ 
的 误差 更 正式 来说 它 其实 是 代价 函数 
的 导数 $ \ delta _ j ^ { l 
} = \ dfrac { \ partial } { \ 
partial z _ j ^ { l } } cost 
t $ 所以 要求 梯度 下 降法 中的 导数 就要 
计算 误差值 然后 去 更新 权重 参数 而 计算 一个 
节点 的 误差 项 需要 先 计算 每个 与其 相连 
的 下一层 节点 的 误差 项 这就 要求 误差 项的/nr 
计算 顺序 必须 是从 输出 层 开始 然后 反向 依次 
计算 每个 隐藏 层 的 误差 项 直到 与 输入 
层 相连 的 那个 隐藏 层 还是 不懂 在 说 
些 什么 是 吧 其实 反向 传播 就 类似于 从 
错误 中 学习 一 开始 所有 的 权重 都是 随机 
分配 的 对于 训练 数据集 输入 神经网络 得到 一组 输出 
这个 输出 会 和 我们 已知 的 的 输出 作比较 
误差 会 传播 回 上 一层 依次 计算 每 一层 
的 误差 权重 也会 被 相应 的 调整 反复 执行 
该 流程 直到 输出 误差 足够 低 就 得到 了 
一个 训 练好 的 神经 网络 2 . Implementation Note 
在 神经 网络 的 具体 实施 过程 中 一些 技巧 
能够 让 我们 更好 的 实现 效果 例如 参数 展开 
Unrolling Parameters 对于 需要 处理 的 矩阵 $ \ begin 
{ align * } \ Theta ^ { 1 } 
\ Theta ^ { 2 } \ Theta ^ { 
3 } \ dots \ newline D ^ { 1 
} D ^ { 2 } D ^ { 3 
} \ dots \ end { align * } $ 
我们 可以 把 它们 展 开到 长 向量 中 thetaVector 
= Theta1 Theta2 Theta3 deltaVector = D1 D2 D3 当然 
也 可以 从长/nr 向量 中 求得 原始 的 值 假设 
Theta1 为 10x11 Theta2 为 10x11 Theta3 为 1x11 Theta1 
= reshape thetaVector 1 110 10 11 Theta2 = reshape 
thetaVector 111 220 10 11 Theta3 = reshape thetaVector 221 
231 1 11 这样 我们 就 可以 利用 Octave 中的 
fminunc 函数 来 实现 算法 fminunc @ costFunction initialTheta options 
另一个 技巧 就是 梯度 检验 Gradient Checking 在 算法 实现 
过程 中 如何 确认 你 的 代码 是否 正确 工作 
呢 梯度 检验 的 思路 就 是 通过 近似 估算 
当前 的 梯度 与 计算 得到 的 值 进行 比较 
如果 二者 相差 在 允许 误差 范围内 说明 代码运行 正常 
怎么 估算 呢 $ \ dfrac { \ partial } 
{ \ partial \ Theta } J \ Theta \ 
approx \ dfrac { J \ Theta + \ epsilon 
J \ Theta \ epsilon } { 2 \ epsilon 
} $ 这个 不难理解 在 切线 出 作 三角形 就 
可以 得到 上式 近似 多个 Theta 矩阵 的话 可以 用 
下式 估算 $ \ dfrac { \ partial } { 
\ partial \ Theta _ j } J \ Theta 
\ approx \ dfrac { J \ Theta _ 1 
\ Theta _ j + \ epsilon \ Theta _ 
n J \ Theta _ 1 \ Theta _ j 
\ epsilon   \ Theta _ n } { 2 
\ epsilon } $ 有了 估算 公式 在 具体 的 
实现 中 我们 利用 反向 传播 算法 来 计算 梯度 
向量 DVec 然后 利用 梯度 检验 方法 得到 gradApprox 如果 
二者 相近 则 关闭 梯度 检验 使用 反向 传播 继续 
学习 为什么 验证 后要/nr 关闭 梯度 检验 因为 在 每一次 
迭代 中都 计算 该 梯度 将 会使 代码 变得 很慢 
最后 随机 初始化 Random Initialization 是 有 必要 的 所有 
参数 初始 化为 0 是 不 合适 的 因为 在 
反向 传播 中 所有 节点 都会 更新 为 相同 值 
正确 的 做法 是 将 $ \ Theta ^ { 
l } _ { ij } $ 初始 化为 $ 
\ epsilon \ epsilon $ 之间 随机 值 3 . 
  Putting it Together 综上所述 神经 网络 的 算法 具体 
实现 可以 归纳 为 在 训练 之前 首先 必须 设计 
神经 网络 架构 包括 总共 多少 层 每层 多少 个 
节点 通常 输入 单元 的 数量 = 样本 特征 的 
维度 输出 单元 的 数量 = 分类 的 数量 隐藏 
层 单元 的 数量 = 通常 越多越好 但 必须 与 
计算 成本 相平衡 另外 如果 隐藏 层 超过 1层 通常 
设计 成 每个 隐藏 层 节点 数量 一致 然后 就 
可以 训练 神经网络 1 . 随机 初始化 权重 2 . 
实施 前 向 传播 得到 $ h _ \ Theta 
x ^ { i } $ 3 . 实施 代价 
函数 4 . 实施 反向 传播 计算 偏 导数 5 
. 使用 梯度 检验 来 确认 反向 传播 计算 正确 
然后 关闭 梯度 检验 6 . 使用 梯度 下降 或 
其它 优化 算法 最小化 代价 函数 理想 情况下 $ h 
_ \ Theta x ^ { i } \ approx 
y ^ { i } $ 使得 代价 函数 最小 
但是 由于 $ J \ Theta $ 不是 凸函数 所以 
我们 可能 得到 的 只是 局部 最小值 Week 6 2018 
. 12.05 Advice for Applying Machine LearningMachine Learning System Design1 
.   Evaluating a Learning Algorithm 如果 在 实施 正则化 
线性 回归 以 预测 房价 的 过程 中 你 发现 
预测值 有 很大 的 误差 那么 应该 怎么做 以下 有 
几种 思路 获得 更多 的 训练样本 尝试 更小 的 特征 
集 尝试 获取 额外 的 特征 尝试 特征 的 多项式 
组合 尝试 增加 或 减小 $ \ lambda $ 但 
如何 知道 哪种 方法 有效 逐一 尝试 显然 不是 最 
合适 的 方法 在 改进 我们 的 算法 之前 我们 
需要 做 的 是 诊断 Diagnostic 它 能让 你 深入 
了解 算法 中 哪些 起作用 哪些 不起作用 并 指导 你 
如何 最好 的 提升 算法 性能 尽管 有时候 诊断 需要 
花费 一些 时间 但这 是 值得 的 有的 时候 我们 
把 机器学习 算法 训练 得 很好 对于 训练样本 能够 很好 
的 预测 和 分类 但是 遇到 样 本集 以外 的 
新 样本 性能 就 急剧下降 为了 避免 这种 情况 在 
训练 过程 中 我们 会 把 训练样本 分为 训练 集 
train 通常 为 70% 和 测试 集 test 通常 为 
30% 那么 训练 过程 可以 改进 为 从 训练 集中学习 
参数 $ \ theta $ 最小化 训练 误差 $ J 
\ theta $ 计算 测试 集 误差 对于 线性 回归 
测试 集 误差 计算 如下 $ J _ { test 
} \ Theta = \ dfrac { 1 } { 
2m _ { test } } \ sum _ { 
i = 1 } ^ { m _ { test 
} } h _ \ Theta x ^ { i 
} _ { test } y ^ { i } 
_ { test } ^ 2 $ 对于 逻辑 回归 
测试 集 误差 计算 如下 $ J _ { test 
} \ theta = \ frac { 1 } { 
2m _ { test } } \ sum _ { 
i = 1 } ^ { m _ { test 
} } y ^ { i } _ { test 
} log h _ { \ theta } x ^ 
{ i } _ { test } + 1 y 
^ { i } _ { test } log 1 
h _ { \ theta } x ^ { i 
} _ { test } $ 如果 训练 集 误差 
很小 但是 测试 集 误差 很大 那很/nr 有可能 是 过拟合 
那么 如何 选择 合适 的 假设 模型 呢 以 多项式 
回归 为例 假设 函数 可以 是 n 阶 方程 那么 
怎么 知道 n 到底 是 多少 合适 测试 集 只能 
给 出 当前 模型 下 误差 因此/c 我们/r 有/v 必要/d 
从/p 训练样本/n 集中/v 再/d 分出/v 一部分/m 数据/n 作为/v 交叉/n 验证/v 
集/q cross validation 通常 比例 为 60% 20% 20% 那么 
便 可以 分别 计算 三 部分 的 误差值 以此 来 
评价 算法 性能 1 . 利用 训练样本 集 对于 每个 
多项式 假设 模型 计算 优化 参数 $ \ Theta $ 
2 . 利用 交叉 验证 集 得到 最小 误差 对应 
的 假设 模型 3 . 利用 测 试样 本集 计算 
该 算法 的 预测误差 得到 了 该 算法 的 误差 
指标 我们 并不 清楚 应该 如何 改进 我们 的 算法 
因此 有 必要 引入 另外 两个 特征 指标 偏差 bias 
和 方差 variance 通常 欠 拟合 表示 高 偏差 过拟合 
表示 高 方差 我们 的 目标 就是 找到 一个 模型 
让 二者 平衡 尽可能 都小/nr 以 下图 为例 在 多项式 
回 归中 我们 知道 通常 如果 只用 一 阶 函 
数去 拟合 曲线 误差 都会 比较 大 如果 阶数 越高 
对 训练 集 的 拟合 就 越好 误差 就 越小 
这 就是 $ J _ { training } \ Theta 
$ 随着 d 增加 而 下降 的 原因 但 并 
不是 阶数 越高 越好 容易 出现 过 拟合 的 问题 
所以 当 d 超过 某一个 值 后 $ J _ 
{ cross validation } \ Theta $ 反而 开始 转为 
上升 总结 来说 在 高 偏差 High bias underfitting 中 
$ J _ { train } \ Theta $ 和$J/nr 
_ { CV } \ Theta $ 都 很高 且 
二者 相近 在 高 方差 High variance overfitting 中 $ 
J _ { train } \ Theta $ 很低 而 
$ J _ { CV } \ Theta $ 远大于 
$ J _ { train } \ Theta $ 在 
之前 的 课程 中 我们 知道 正则化 可以 有效 减少 
过拟合 通常 $ \ lambda $ 很大 时 会 造成 
求解 代价 函数 最小值 的 过程 中 尽可能 的 减小 
$ \ theta _ 1 \ theta _ 2 \ 
cdots $ 因此 容易 造成 欠 拟合 即 高 偏差 
$ \ lambda $ 很 小时 正则化 几乎 不起作用 所以 
容易 造成 过拟合 即 高 方差 那么 怎样 的 $ 
\ lambda $ 才是 合适 的 呢 以下 是 常用 
的 做法 1 . 创建 一个 $ \ lambda $ 
列表 例如 λ ∈ { 0 0.01 0.02 0.04 0.08 
0.16 0.32 0.64 1.28 2.56 5.12 10.24 } 2 . 
创建 一个 带 不同 次数 或 任意 变量 的 模型 
集合 3 . 遍历 $ \ lambda s $ 对于 
每个 $ \ lambda $ 在 模型 集合 中 训练 
得到 $ \ Theta $ 4 . 不 使用 正则化 
或 $ \ lambda = 0 $ 利用 上面 得到 
的 $ \ Theta $ 计算 交叉 验证 误差 $ 
J _ { CV } \ Theta $ 5 . 
选择 令 交叉 验证 得到 的 误差 最低 的 最佳 
组合 6 . 使用 最佳 的 $ \ Theta $ 
和$\/nr lambda $ 应用于 $ J _ { test } 
\ Theta $ 测试 它 是否 有好 的 性能 学习曲线 
Learning Curves 是 另一种 评价 和 检查 算法 有效性 的 
指标 如果 我们 只有 几个 样本点 那么 很 容易 找到 
一个 曲线 来 完美 的 拟合 它 但是 随着 训练 
集 的 增大 误差 将会增大 而 误差 随着 训练 集 
的 不断 增大 会 趋于 平缓 如下 图 所示 从 
左图 可以 得到 如果 算法 当前 高 偏差 那么 增加 
样本 数量 会 使得 $ J _ { train } 
\ Theta $ 和$J/nr _ { CV } \ Theta 
$ 都 很高 且 相近 因此 并 没有 什么 帮助 
从 右图 可以 得出 如果 算法 当前 高 方差 那么 
增加 样本 数量 会 使得 $ J _ { train 
} \ Theta $ 增大 $ J _ { CV 
} \ Theta $ 减小 且 二者 之间 距离 较远 
因此 可以 起到 一定 作用 总结 一下 更多 训练样本 解决 
高 方差 一定 程度 上 减少 特 征集 解决 高 
方差 无谓 的 特征 太多 干扰 了 我们 的 拟合 
容易 过拟合 增加 特征 解决 高 偏差 特征 太 单调 
仅靠 面积 就想 预测 房价 肯定 有 很大 偏差 增加 
多项式 特征 解决 高 偏差 用 曲线 去 拟合 数据 
会比 直线 更 准 一些 减小 $ \ lambda $ 
解决 高 偏差 正则化 作用 减小 高次 因子 发挥 功能 
更能 精确 拟合 数据 增大 $ \ lambda $ 解决 
高 方差 正则化 起作用 降低 高次 因子 的 影响 不 
容易 过拟合 对于 神经网络 而言 越少 的 参数 越 容易 
欠 拟合 尽管 计算 成本 降低 越多 的 参数 越 
容易 过拟合 同时 计算 成本 也 很高 可以 使用 正则化 
减少 过拟合 一 开始 的 时候 应该 尽量 选择 一层 
的 隐藏 层 然后 使用 交叉 验证 集 训练 多个 
隐藏 层 的 神经 网络 从中 选择 一个 最优 的 
模型 的 复杂性 对 算法 的 影响 在于 低阶 多项式 
低 模型 复杂度 具有/v 高/a 偏差/n 和低/nr 方差/n 不能 很好 
的 拟合 数据 高阶 多项式 高 模型 复杂度 在 训练 
数据 中 表现 很好 在 测试 数据 中 有时 却 
很 糟糕 即 低 偏差 高 方差 因此 必须 选择 
一个 介于 二者 之间 的 模型 才能 更好 的 拟合 
数据 2 .   Building a Spam Classifier 举个 垃圾邮件 
分类器 的 例子 如何 判断 一封 邮件 是 不是 垃圾 
邮件 通常 如果 这封 邮件 语法 和 拼写 各种 出错 
或者 带有 明显 的 推销 和 广告 的 字眼 那么 
我们 就 认为 它 是 一封 垃圾邮件 以 关键字 为例 
我们 选择 最 常见 的 垃圾 邮件 关键字 100个 实际 
中 通常 会 选择 1万 到 5万 个 关键字 作 
为特征 $ x $ 构成 100x1 的 向量 如果 邮件 
中 包含 这个 关键字 则 对应 元素 置 1 $ 
y $ 表示 分类 结果 1 表示 垃圾邮件 0 表示 
正常 邮件 这样 我们 就 能 通过 训练 样 本集 
来 训练 我们 的 算法 检测 是否 垃圾邮件 在 这 
过程 中 如何 减小 分类 误差 你 可以 收集 大量 
的 数据 选用 更为 复杂 的 特征 选用 不同 的 
算法 以 不同 方式 处理 邮件 但是 很难说 哪种 方式 
最好 我们 必须 有 一套 误差 分析 Error Analysis 准则 
通常 推荐 的 做法 是 采用 一个 简单 的 算法 
快速 的 实施 尽早 用 交叉 验证 数据测试 绘制 出 
学习曲线 看看 更多 的 数据 或者 更多 的 特征 等等 
是否 有 帮助 手动 检查 交叉 验证 误差 尝试 找出 
是 什么 造成 误差 最大 例如 如果/c 我们/r 有/v 500/m 
封/q 邮件/n 而 我们 的 算法 误 分类 了 其中 
100 封 那/r 我们/r 可以/c 手动/n 检查/vn 这/r 100/m 封/q 
邮件/n 尝试 根据 它们 的 类型 分类 然后 分析 出 
新的 特征 可以 帮助 我们 准确 的 对 它们 进行 
分类 例如 如果 这些 邮件 都 企图 盗窃 你 的 
密码 那么 我们 可以 归纳 出 新的 特征 加到 我们 
的 模型 中 或者 在 模型 中 是否 要 考虑 
单词 的 大小写 词性 变化 等 要素 误差 分析 不 
一定 有用 但是 值得 一试 且 试 且 行 量化 
的 误差 分析 十分 重要 否则 的话 很难 去 评估 
算法 性能 例如 如果 你 在 算法 中 考虑 了 
词性 变化 误差 只有 3% 不 考虑 的 话 误差 
有 5% 那么 我们 当然 应该 把 这个 加到 算法 
里 但 如果 考虑 单词 大小写 和 不考虑 大小写 误差 
一个 是 3.2% 一个 是 3% 那就 没必要 加入 这个 
新 特征 3 . Handling Skewed Data 在 机器学习 算法 
的 实施 过程 中 有时候 样本数据 很 不平衡 存在 倾斜 
Skewed 例如 在 癌症 诊断 分类器 中 建立 逻辑 回归模型 
1 表示 有 癌症 0 表示 没有 最终 得到 算法 
准确率 有 99% 只有 1% 的 误差 听 起来 好像 
很 不错 但 如果 我们 发现 样本数据 中 只有 0.5% 
的 病人 有 癌症 那么 算法 的 准确度 就 不见得 
很不错 即便 在 你 的 算法 里 你 简单 粗暴 
地 判断 所有 的 样本 分类 值 都为 0 这个 
算法 也 只有 0.5% 的 误差 甚至 还 比 之前 
的 算法 好 所以 不能 简单 的 从 误差 上 
判断 算法 的 优劣 有 必要 引入 精确 率 Precision 
和 召回率 Recall Actual C l a s s P 
r e d i c t e d C l 
a s s 1 0 1 T r u e 
PositiveFalse Positive0False NegativeTrue Negative 精确 率 $ \ frac { 
True pos } { predicted pos } = \ frac 
{ True pos } { True pos + false pos 
} $ 即 对于 我们 预测 为 1 有 癌症 
的 病人 真正 有 癌症 的 占 几成 召回率 $ 
\ frac { True pos } { actual pos } 
= \ frac { True pos } { True pos 
+ false neg } $ 即 对于 真正 有 癌症 
的 病人 我们 准确 预测 出了 几成 通常 二者 需要 
达到 一个 平衡 对于 逻辑 回归 来说 我们 通常 会 
预测 $ h _ \ theta x $ 大于 0.5 
为 1 小于 0.5 为 0 如果/c 我们/r 想/v 要在/i 
十分/m 有/v 把握/v 的/uj 情况下/i 才/d 判断/v 这个/r 病人/n 有/v 
癌症/n 那就/nr 需要 把 阈值 调高 例如 调至 0.9 只有 
当 $ h _ \ theta x $ 大于 0.9 
的 时候 才 判断 为 1 这时 精确 率 将会 
提高 召回率 将会 降低 如果 我们 不想 错过 任何 有 
可能 的 癌症 患者 那就/nr 需要 把 阈值 调低 例如 
调至 0.1 只要 $ h _ \ theta x $ 
大于 0.1 就 判断 为 1 这时 精确 率 将会 
降低 召回率 将会 提高 直觉 上 不难 理解 怎么 比较 
两个 指标 呢 我们 引入 了 另一个 指标 F1 Score 
$ F = 2 \ frac { PR } { 
P + R } $ 当 P = 0 或者 
R = 0时 F1 为 0 当 P = 1 
且 R = 1时 F1 为 1 4 . Using 
Large Data Sets 在 一定 条件 下 数据 量 越大 
算法 的 训练 就 越好 也 就是 常说 的 It 
s not who has the best algorithms that wins it 
s who has the most data . 但 前提 条件 
是 什么 呢 1 . 特征 x 是否 包含 了 
足够 多 的 信息 来 准确 的 预测 y 单纯 
从 房屋 面积 预测 房价 显然 不合理 或者 给 定了 
x 人类 的 专家 能 自信 的 预测 出 y 
吗 2 . 使 用带 许多 参数 的 学习 算法 
例如 带 许多 参数 的 线性 / 逻辑 回归 或者 
许多 隐藏 层 的 神经 网络 Week 7 2018 . 
12.07 Support Vector Machines1 .   Large Margin Classification 回忆 
一下 逻辑 回归 $ h _ \ theta x = 
\ frac { 1 } { 1 + e ^ 
{ \ theta ^ T x } } $ 如果 
$ y = 1 $ 我们 希望 $ h _ 
\ theta x \ approx 1 $ 即 $ \ 
theta ^ T x 0 $ 如果 $ y = 
0 $ 我们 希望 $ h _ \ theta x 
  \ approx 0 $ 即 $ \ theta ^ 
T x 0 $ 代价 函数 为 $ J \ 
theta = \ frac { 1 } { m } 
\ sum _ { i = 1 } ^ m 
y ^ { i } logh _ { \ theta 
} x ^ { i } + 1 y ^ 
{ i } log 1 h _ \ theta x 
^ { i } + \ frac { \ lambda 
} { 2m } \ sum _ { j = 
1 } ^ { m } \ theta _ j 
^ 2 $ 回忆 y = 1 和y=/nr 0 的 
曲线 见 week3 是 上升 和 下降 的 曲线 如果 
我们 用 更 严格 的 直线 来 替换 这两条 曲线 
例如 y = 1 当 z 大于 某个 数 时 
代价 函数 为 0 如下 图 所示 那么 代价 函数 
可以 替换成 $ J \ theta = \ frac { 
1 } { m } \ sum _ { i 
= 1 } ^ m y ^ { i } 
Cost _ 1 \ theta ^ Tx ^ { i 
} + 1 y ^ { i } Cost _ 
0 \ theta ^ Tx ^ { i } + 
\ frac { \ lambda } { 2m } \ 
sum _ { j = 1 } ^ { m 
} \ theta _ j ^ 2 $ 不考虑 m 
因为 不 影响 最小化 结果 提取 $ \ lambda $ 
则 也 可以 写成 $ J \ theta = C 
\ sum _ { i = 1 } ^ m 
y ^ { i } Cost _ 1 \ theta 
^ Tx ^ { i } + 1 y ^ 
{ i } Cost _ 0 \ theta ^ Tx 
^ { i } + \ frac { 1 } 
{ 2 } \ sum _ { j = 1 
} ^ { m } \ theta _ j ^ 
2 $ 这 就是 被 称为 支持 向量 机 Support 
Vector Machines 的 代价 函数 它 与 逻辑 回归 不同 
的 地方 在于 如果 $ y = 1 $ 我们 
希望 $ \ theta ^ T x \ geq 1 
$ 而 不是 只 是 大于 等于 0 如果 $ 
y = 0 $ 我们 希望 $ \ theta ^ 
T x \ leq 1 $ 而 不是 只是 小于 
0 当 C 很大 时 也 被 称为 惩罚 系数 
就 要求 代价 函数 的 前 一部分 非常 小 即 
满足 上述 的 条件 此时 求 代价 函数 最小值 就 
转化 为求 $ \ frac { 1 } { 2 
} \ sum _ { i = 1 } ^ 
{ n } \ theta _ j ^ 2 $ 
的 最小值 这个 式子 也 可以 表示 为求 $ | 
| \ theta | | $ 最小值 那么 $ \ 
theta ^ T x $ 表示 什么 含义 呢 回忆 
一下 两个 向量 相乘 或 内积 $ u ^ Tv 
= p   \ cdot | | u | | 
$ 其中 $ p $ 是 $ v $ 在 
$ u $ 上 的 投影 长度 可 正可 负 
所以 $ \ theta ^ T x ^ { i 
} = p ^ { i } \ cdot | 
| \ theta | | $ 其中 $ p ^ 
{ i } $ 是 $ x ^ { i 
} $ 在 向量 $ \ theta $ 上 的 
投影 长度 所以 我们 希望 $ p ^ { i 
} $ 尽可能 的 大 即 $ x ^ { 
i } $ 在 $ \ theta $ 上 的 
投影 尽可能 大 这样 满足 了 约束条件 才能 转化 为求 
$ | | \ theta | | $ 最小值 以 
下图 左图 为例 红/a 色线/n 和/c 蓝色/n 线/n 都能/i 区分/n 
正负/v 集/q 但 很明显 我们 会 觉得 蓝色 的 决策 
边界 更为 合理 对于 红色 决策 边界 离 某些 样本点 
过于 近 垂直于 该 边界 的 法线 为 $ \ 
theta $ 的 方向 那么 作 这些 样本点 在法 线上 
的 投影 $ | | p ^ { i } 
| | $ 就会 比较 小 这样 为了 使 约束条件 
满足 我们 需要 $ | | \ theta | | 
$ 取值 很大 因此 这 条 直线 不算 非常好 的 
决策 边界 相反 的 对于 蓝色 决策 边界 样本 点在 
$ \ theta $ 上 的 投影 相对 更大 因此 
$ | | \ theta | | $ 可以 取值 
更小 其实 支持 向量 机 的 思想 就是 找到 正负 
样 本集 最 中间 的 分界线 也 可以 看成 如果 
想 在中间 修 一条 笔直 的 马路 怎么样 才能 让 
这条 马路 最 宽 而 马路 的 中间 就是 最佳 
的 决策 边界 这也 是 为什么 被 称为 最 大 
间距 分类 Large Margin Classification 为此 根据 代价 函数 就 
必须 找到 样本点 在 某个 向量 上 的 投影 $ 
p $ 最长 进而 转化 为求 $ | | \ 
theta | | $ 的 最小值 2 .   Kernels 
在 之前 的 课程 中 我们 遇到 过 非线性 拟合 
问题 当时 我们 采用 的 是 多项式 拟合 的 方法 
即 取 多项式 为 新的 特征 但是 这个 方法 的 
一个 缺点 在于 随着 多项式 次数 增高 计算 量 逐渐 
加大 有 没有 另外 的 构成 新 特征 的 方法 
这里 引入 核 函数 Kernel 方法 首先 取 几个 标记 
点 $ l ^ { i } $ 计算 样本 
和 标记 点 的 相似性 $ f _ i = 
similarity x l ^ { i } = exp \ 
frac { | | x l ^ { i } 
| | ^ 2 } { 2 \ delta ^ 
2 } $ 这里 的 $ exp $ 函数 为 
高斯 核 函数 当 $ x \ approx l ^ 
{ i } $ 时 $ f _ i \ 
approx 1 $ 当 $ x $ 离 $ l 
^ { i } $ 很远 时 $ f _ 
i \ approx 0 $ 也 就是 离 标记 点 
越近值/nr 越高 然后 将 计算 得到 的 $ f _ 
i $ 带入 Hypothesis 中 进行 预测 以 三个 标记 
点 为例 当 $ \ theta _ 0 + \ 
theta _ 1 f _ 1 + \ theta _ 
2 f _ 2 +   \ theta _ 3 
f _ 3 \ geq 0 $ 时 预测 为 
1 那么 标记 点 怎么 选择 呢 一种 方法 是 
取样 本集 的 点 为 标记 点 假设有 m 个 
样本 则有 m 个 标记 点 $ l $ 这样 
可以 计算出 m + 1个 特征 $ f $ 其中 
$ f _ 0 = 1 $ 当 $ \ 
theta ^ T f \ geq 0 $ 时 预测 
$ y = 1 $ 怎么 得到 参数 $ \ 
theta $ 呢 其实 就是 最小化 下式 $ J \ 
theta = C \ sum _ { i = 1 
} ^ m y ^ { i } Cost _ 
1 \ theta ^ Tf ^ { i } + 
1 y ^ { i } Cost _ 0 \ 
theta ^ Tf ^ { i } + \ frac 
{ 1 } { 2 } \ sum _ { 
j = 1 } ^ { m } \ theta 
_ j ^ 2 $ 这就是 带 核 函数 的 
SVM 最后 提 一下 SVM 中 参数 对 算法 的 
影响 C = 1 / λ 值 很大 λ 很小 
则 低 偏差 高 方差 值 很小 λ 很大 则 
高 偏差 低 方差 $ \ delta ^ 2 $ 
值 很大 则 特征 $ f _ i $ 变化 
平缓 即 高 偏差 低 方差 值 很小 则 特征 
$ f _ i $ 变化 不 平缓 即 低 
偏差 高 方差 3 .   Using an SVM 具体 
在 实践 中 怎么 使用 SVM 呢 直接 利用 SVM 
软件包 中的 函数 进行 机器学习 优化 参数 $ \ theta 
$ 你 需要 做 的 就是 确定 参数 C 以及 
确定 核 函数 如果 没有 核 函数 则 称为 线性 
核 函数 如果 选择 高斯 核 函数 就要 确定 $ 
\ delta ^ 2 $ 并 记得 先 进行 特征 
缩放 当然 也 有 许多 其它 核 函数 可以 选择 
但 不是 所有 的 相似性 函数 都是 有效 的 核 
函数 必须 满足 Mercer s Theorm 对于 多 分类 问题 
许多 SVM 包以 及 集成 了 分类 函数 直接 调用 
即可 使用 之前 介绍 过 的 one vs . all 
方法 也 是 可以 的 最后 何时 使用 逻辑 回归 
何时 使用 SVM 以下 是 一些 参考 1 . 特征 
维度 很大 可使用 逻辑 回归 或 不带 核 函数 的 
SVM 2 . 特征 维度 小 样本数 中等 则 使用 
带 高斯 核 函数 的 SVM 3 . 特征 维度 
小 样本数 大 则 增加 新 的 特征 然后 使用 
逻辑 回归 或 不带 核 函数 的 SVM 4 . 
神经网络 对于 以上 情况 均 表现 不错 就是 可能 训 
练起来 会很 花时间 Week 8 2018 . 12.08 Unsupervised L 
e a r n i n g D i m 
e n s i o n a l i t 
y Reduction1 . Clustering 这周 开始 介绍 无 监督性 学习 
unsupervised learning 这里 每 个 样本 并 没有 标记 值 
因此 需要 找到 一个 算法 能够 自动 地 将 它们 
分类 如下 图 所示 其中 一种 基本 的 算法 称为 
聚 类 算法 Clustering Algorithm 而 其中 最 简单 的 
则是 被称为 K means 算法 1 . 随机 选择 K 
个 聚 类 质心 点 cluster centroids 假设 要 分成 
K 类 $ \ mu _ 1 \ mu _ 
2 \ cdots \ mu _ k $ 2 . 
重复 执行 以下 步骤 直至 收敛 对于 每一个 样本 通过 
$ c _ { i } = arg min _ 
i | | x ^ { i } \ mu 
_ j | | ^ 2 $ 计算 离 哪个 
centroids 最近 则 标记 为 属于 哪 一类 对于 每一个 
质心 点 根据 上面 计算 后 归类 的 点 重新 
计算 均值 质心 点 $ \ mu _ k $ 
听 起来 有点 绕口 其实 看图 很 直观 其实 这个 
过程 也 可以 看出 是 一个 最 优化 问题 或者 
最小化 代价 函数 问题 只是 这里 的 代价 函数 为 
$ J c ^ { 1 } \ cdots c 
^ { m } \ mu _ 1 \ cdots 
\ mu _ k = \ frac { 1 } 
{ m } \ sum _ { i = 1 
} ^ { m } | | x ^ { 
i } \ mu _ { c ^ { i 
} } | | ^ 2 $ 在 K means 
算法 中 随机 挑选 初始值 可能 会 影响 最后 的 
结果 有 可能 就 会 找到 局部 最优 解 例如 
几个 质心 点 都在 同一个 簇 中 一种 比较 推荐 
的 初始化 方法 是从 1 到 m 中 随机 挑选 
k 个数 取 这 k 个数 对应 的 样本 值 
作为 初始 聚 类 质心 另外 可以 多次 随机 初始化 
运行 K means 算法 然后 计算 对应 代价 函数 选择 
最小 的 那个 另外 还有 一个 问题 怎么 知道 要 
分 几类 即 K 取 多少 一种 方法 是 Elbow 
method 即 计算 随着 K 的 增加 代价 函数 的 
变化 曲线 如果 在 某个 点 有 明显 的 转角 
类似 手肘 则 那个 点 一般 是 比较 合适 的 
K 值 但 如果 曲线 很 平滑 的话 那么 这种 
方法 就 失效 了 另一种 方法 是 根据 实际 的 
应用 场景 设置 例如 根据 需求 决定 T shirt 的 
尺寸 需要 分成 几类 2 . Principal Component Analysis 在 
实际 应用 中 数据 的 维度 往往 是 很大 的 
带来 的 计算 量 也 非常 庞大 因此 数据 降 
维 Data Dimensionality Reduction 是 很 有 必要 的 例如 
我们 可以 在 二维 坐标系 中 做 一条 直线 向量 
把 所有 二维 点 投影 到 该 向量 上 取 
这些 点在 该 向量 上 的 位置 作为 新的 特征值 
一维 又 或者 把 三维 坐标系 中的 点 投影 到 
一个 平面 上 即 完成 3 维 到 2 维 
的 降 维 除了 数据压缩 Data Compression 降 维 的 
另一个 需求 是 数据 可视化 Data Visualization 例如 我们 收集 
了 每个 国家 的 GDP 人均 GDP 人类 发展 指数 
预期 寿命 等等 很难 去 绘制 出 多 维度 的 
图 但是/c 如果/c 我们/r 能降维/nr 成/n 2个/mq 特征/n 那么 就 
可以 用 图 展示 出来 更加 直观 对于 实现 数据 
降 维 最 常用 的 算法 是 主 成分 分析法 
Principal Component Analysis PCA 它 的 算法 思想 是由 高维 
向 低维 使得 投影 误差 最小 投影 例如 从2维/nr 向1维/nr 
投影/n 就是 找到 一个 向量 使得 数据 投影 到 这个 
向量 后的/nr 投影 误差 最小 类似 的 从n维/nr 到 k 
维 投影 就是 找到 k 个 向量 使得 数据 投影 
到 这些 向量 的 投影 误差 最小 这个 似乎 和 
线性 回归 很像 在 线性 回 归中 也 似乎 是 
找到 一条 直线 最好 的 拟合 这些 点 不过 二者 
其实 不同 在 线性 回 归中 计算 的 是 预测 
值 与 实际 值 的 误差 以 二维 坐标 为例 
就是 计算 y 方 向上 的 误差 而在 PCA 中 
计算 的 是 数据 点到 向量 的 垂直距离 PCA 的 
具体 算法 如下 1 . 数据 预处理 进行 特征 缩放 
和 均值 归一化 处理 2 . 计算 样本 之间 的 
协方差 矩阵 covariance matrix $ Sigma = \ frac { 
1 } { m } \ sum _ { i 
= 1 } ^ { m } x ^ { 
i } x ^ { i } ^ T $ 
3 . 进行 奇异 值 分解 计算 特征向量 U S 
V = svd Sigma 4 . 提取 前 K 个 
主 成分 Ureduce = U 1 k 5 . 降 
维 后的/nr 数据 z = Ureduce * x 降 维 
之后 如果 要 恢复 的话 也 是 可以 的 x 
= Ureduce * z 这里 是 近似值 另外 如何 选择 
k 1 . 计算 平均 均方 投影 误差 Average Squared 
Projection Error $ \ frac { 1 } { m 
} \ sum _ { i = 1 } ^ 
{ m } | | x ^ { i } 
x _ { approx } ^ { i } | 
| ^ 2 $ 2 . 计算 数据 总 方差 
Total Variation $ \ frac { 1 } { m 
} \ sum _ { i = 1 } ^ 
{ m } | | x ^ { i } 
| | ^ 2 $ 3 . 选择 使得 上面 
两式 相除 后值/nr 最小 的 k 如果 值 小于 0.01 
则 我们 可以 说 PCA 算法 保留 了 原 数据 
99% 的 差异性 因此 在 实际 应用 中 我们 通常 
是 先 设定 好 阈值 如 我们 希望 保留 99% 
的 差异性 然后 从k=/nr 1 开始 尝试 如果 不 满足 
则 k 加 1 直至 满足 条件 的 最小 的 
k 出现 PCA 算法 可以 用于 加速 监督 学习 提取 
训练样本 的 输入 $ x ^ { i } y 
^ { i } $ 运用 PCA 降 维 如从 
10000 降到 1000 得到 新的 训练 样 本集 $ z 
^ { i } y ^ { i } $ 
用 其 进行 机器学习 求解 最优 参数 另外 PCA 算法 
只 用于 训练 样本 中 得到 映射 关系 后 可以 
直接 运用 与 交叉 验证 数据 和 测试数据 总结 一下 
1 . PCA 的 应用 场景 在于 1 . 数据压缩 
减少 存储空间 加速 学习 算法 2 . 数据 可视化 2 
. 人们 会 认为 PCA 减少 了 特征 的 数量 
因此 可以 减少 过拟合 这是 不 正确 的 正确 的 
应该 是 使用 正则化 3 . 在 设计 机器学习 系统 
时 不 应该 一上来 就用 PCA 算法 而是 应该 尝试 
用 原始 数据 进行 训练 学习 只有 当 算法 结果 
不 理想 时 才 需要 尝试 考虑 PCA 算法 Week 
9 2018 . 12.08 Anomaly D e t e c 
t i o n R e c o m m 
e n d e r Systems1 .   Anomaly Detection 
机器 学习 的 一个 应用 场景 就是 异常 检测 Anomaly 
Detection 什么 是 异常 检测 举个 例子 对于 航空 发动机 
有几个 特征参数 决定 它 工作 正 不正常 $ x _ 
1 $ 工作 产生 的 热量 $ x _ 2 
$ 震动 强度 等等 你 收集到 了 一些 数据 那么 
对于 一台 新的 发动机 你 需要 根据 之前 的 样本 
集 来 判断 它 是否 有 异常 通常 这些 样本点 
都会 围绕 一个 中心 点 分布 因此 如果 越 接近 
样本点 的 中心 正常 的 概率 越高 离 中心 越远 
异常 的 可能性 就 越高 因此 处理 此类 问题 的 
通常 做法 是 建立 一个 概率模型 $ p x $ 
它 以 样本 中心 为 峰值 向外 递减 类似于 等高线 
如果 测试数据 代入 这个 概率模型 中 求得 概率 小于 某个 
小 值 $ \ epsilon $ 则 判定 它 为 
异常 样本 这种 异常 检测 在 诸如 欺诈 检测 用户 
行为 监测 制造业 产品 QA 测试 数据中心 主机 监控 等 
方面 有 重要 应用 那么 如何 建立 概率模型 常用 的 
是 高斯 正 态 分布 这 是因为 自然界 中 许多 
事件 的 概率 都 服从 该 分布 其 概率 公式 
为 $ p x = \ frac { 1 } 
{ \ sqrt { 2 \ pi } \ sigma 
} \ exp \ left \ frac { x \ 
mu ^ 2 } { 2 \ sigma ^ 2 
} \ right $ 这个 大家 应该 很 熟悉 了 
就 不过 多 介绍 值得一提的是 独立 分布 事件 的 概率 
等于 事件 概率 的 乘积 有了 概率模型 异常 检测 的 
算法 就 可以 具体 为 1 . 选出 你 觉得 
可能 会 指示 出 异常 的 重要 特征 $ x 
_ i $ 2 . 计算 各 个 特征 的 
均值 与 方差 $ \ displaystyle \ mu _ j 
= \ frac { 1 } { m } \ 
sum _ { i = 1 } ^ m x 
_ j ^ { i } \ \ sigma _ 
j ^ 2 = \ frac { 1 } { 
m } \ sum _ { i = 1 } 
^ m x _ j ^ { i } \ 
mu _ j ^ 2 $ 得到 每种 特征 的 
概率 $ p x _ j \ mu _ j 
\ sigma _ j ^ 2 $ 3 . 对于 
一个 新 样本 计算 其 概率 是否 小于 $ \ 
epsilon $ 其中 $ p x $ 等于 各项 特征 
概率 的 乘积 有了 异常 检测 算法 我们 还 需要 
知道 如何 去 评估 异常 检测 系统 性能 最 常用 
的 方法 就是 跟 监督性 学习 一样 将 样 本集 
分成 Trainning Set Cross Validation Set 和 Test Set 通常 
为 60% 20% 20% 这样的话 就 可以 用 之前 的 
指标 去 评估 系统 性能 具体 如下 1 . 对 
训练 集 进行 学习 建立 概率模型 $ p x $ 
2 . 对 交叉 验证 集 进行 测试 概率 小于 
$ \ epsilon $ 的 预测 为 1 anomaly 然后 
利用 之前 介绍 过 的 Precision / Recall / F1 
score 来 评价 算法 也 可以 用 交叉 验证 集 
来 选择 参数 $ \ epsilon $ 有人 可能会 问 
这类 问题 如果 用 监督性 学习 来做 是否 也 可以 
同样 是 有带 标记 的 样本 集 训练 后 用来 
对 新 样本 进行 分类 0/1 二者 其实 是 有 
一些 区 别的 在 异常 检测 中 只有 少量 的 
正 样本 y = 1 通常 20个 以内 但 有 
大量 的 负 样本 同时 异常 的 种类 也 很多 
很难 通过 这些 正 样本 学习 到 所有 的 异常 
类型 之后 的 异常 类型 也 有可能 与 当前 的 
完全 不 一样 在 这种 情况 下 用 概率模型 去 
检测 是 比较 合理 的 做法 在 监督 学习 中 
有/v 许多/m 的/uj 正/d 样本/n 和负/nr 样本/n 包含 足够 多 
的 正 样本 让 算法 获取 正 样本 的 信息 
而 未来 的 正 样本 也 跟 当前 训练 集 
的 正 样本 类似 因此 二者 的 应用 场景 也 
有所不同 异常 检测 通常 应用于 欺诈 检测 工业 质检 数据 
中心 检测 等 监督 学习 通常 应用于 垃圾邮件 分类 天气 
预测 癌症 诊断 等 整个 异常 检测 算法 是 基于 
样本 中 的 特征 概率 符合 高斯分布 的 假设 那 
如果 样本 特征 不服从 高斯分布 怎么办 有时候 可以 做 一些 
简单 的 数学 转换 让其 更 服从 高斯分布 即可 如 
做 log 变换 或者 做 平方根 立方根 转换 另外 在 
算法 中 我们 希望 $ p x $ 对于 正常 
样 本值 很大 对于 异常 样 本值 很小 但 有时 
单个 特征 中 的 正常 和 异常 样本 的 $ 
p x $ 比较 相近 因此 如果 能 增加 一个 
特征 结合 一起 判断 准确度 会 更高 即 类似于 0.3 
x 0.3 = 0.09 最后 再 提 一点 之前 的 
异常 检测 系统 我们 都 考虑 特征 之间 是 彼此 
独立 不 相关 的 因此 概率 直接 相乘 就好 但是 
也许 两个 特征 是 有 相关性 的 呢 例如 在 
数据 中心 主机 监测 问题 中 CPU 的 温度 是 
一个 特征 网络流量 也 是 一个 特征 如果 不 考虑 
相关性 某个 时间 点 CPU 温度 很高 因此 极有可能 异常 
网络流量 也 很大 也 极有可能 异常 两者 概率 相乘 直接 
就 判定 为 主机 异常 但是 我们 都 知道 网络流量 
很大 时 CPU 的 负载 就会 很高 温度 自然 会 
比较 高 但 这时 主机 还是 在 正常 工作 状态 
的 所以 必须 考虑 样本 特征 之间 的 相关性 例如 
可以 选择 新 变量 为 CPU 温度 / 网络流量 另一种 
方法 就是 使用 多元 高斯分布 模型 Multivariate Gaussian Distribution 相比 
正常 的 高斯分布 呈 均匀 圆形 分布 多元 高斯分布 由于 
特征 之间 的 相关性 可能会 呈 椭圆形 分布 多元 高斯分布 
的 概率模型 为 $ p x \ mu \ sum 
= \ frac { 1 } { 2 \ pi 
^ { n / 2 } | \ sum | 
^ { 1/2 } } exp \ frac { 1 
} { 2 } x \ mu ^ T \ 
sum ^ { 1 } x \ mu $ 其中 
$ \ mu = \ frac { 1 } { 
m } \ sum _ { i = 1 } 
^ { m } x ^ { i } $ 
是 每个 正态分布 的 对称轴 $ \ sum = \ 
frac { 1 } { m } \ sum _ 
{ i = 1 } ^ { m } x 
^ { i } \ mu x ^ { i 
} \ mu ^ T $ 是 协方差 矩阵 关于 
这 两个 参数 对 图形 形状 的 影响 这里 不再 
赘述 因此 利用 多元 高斯分布 进行 异常 检测 的 算法 
如下 1 . 对于 训练样本 集 根据 上面 的 式子 
分布 计算 $ \ mu $ 和$\/nr sum $ 得到 
$ p x $ 2 . 对于 新 样本 计算 
其 $ p x $ 如果 小于 $ \ epsilon 
$ 则 判定 为 异常 对比 之前 的 异常 检测 
模型 多元 高斯分布 检测 模型 的 特点 在于 它 计算 
中 自动 包含 特征 间 的 相关性 但是 相对 的 
计算 成本 更高 同时 还 必须 要求 样本数 大于 特征 
数 不然的话 协方差 矩阵 不可逆 通常 要求 m ≥ 10n 
而 单元 告诉 分布 检测 模型 需要 手动 创建 新 
特征 以 减少 特征 相关性 带来 的 影响 不过 它 
的 计算 成本 比较 低 另外 在 样 本集 比 
较小 的 时候 也 能 适用 2 .   Recommender 
Systems 推荐 系统 Recommender Systems 目前 已 深入 到 我们 
生活 的 点点滴滴 今日 头条 淘宝 广告 豆瓣 主页 几乎/d 
每个/r App/w 都有/nr 自己/r 的/uj 推荐算法/i 根据 客户 之前 的 
浏览 和 评价 记录 推荐 可能 感兴趣 的 内容 定位 
更为 精准 虽然 有时候 也挺 烦 的 那么 如何 建立 
一套 推荐 系统 以 电影 评分 举例 假设有 5部 电影 
前 三部 是 爱情片 后 两部 是 动作片 有 4位 
用户 A B C D 的 评分 如下 55 00 
5 400 0 035 00 4 其中 表示 未 评分 
需要 我们 去 预测 根据 直观 的 判断 我们 推测 
A 和B/nr 比较喜欢 爱情片 讨厌 动作片 因此 A 对 第三 
部 电影 的 评分 大概会 是 4 或 5 B 
同理 C 的话 对 两部 爱情片 都评/nr 0分 因此 有 
理由 推测 他 对 第二 部 爱情片 评分 也会 很低 
这 就是 基于 内容 推荐 Content based recommender 的 基本 
简单 思想 具体 怎么 做 呢 用户数量 记为 $ n 
_ u $ 电影 数量 记为 $ n _ m 
$ $ r i j = 1 $ 表示 用户 
$ j $ 对 电影 $ i $ 已经 评 
分过 / rated $ y ^ { i j } 
$ 则 表示 对应 的 电影 评分 $ \ theta 
^ { j } $ 是 用户 $ j $ 
的 参数 向量 $ x ^ { i } $ 
是 电影 的 特征向量 $ m ^ { j } 
$ 是 用户 $ j $ 评 分过 的 电影 
数量 那么 $ y ^ { i j } = 
\ theta ^ { j } ^ T x ^ 
{ i } $ 举个 例子 如果 我们 已经 知道 
第三 部 电影 的 特征向量 为 $ x ^ { 
3 } = 1 0.99 0 ^ T $ 那么 
通过 算法 学习 得 到 第一 位 用户 的 $ 
\ theta ^ { 1 } = 0 5 0 
$ 那么 可以 求得 用户 1对 电影 3 的 评分 
为 $ \ theta ^ { 1 } ^ T 
x ^ { 3 } = 4.95 $ 这里 有 
两个 问题 一是 电影 特征向量 怎么 得到 当然 可以 人为 
的 取 特征 并 给出 值 如 取 爱情 动作 
为 特征 人为 给出 电影 3 的 特征 为 $ 
1 0.99 0 ^ T $ 表示 这部 电影 爱情 
成分 居多 毫无 动作 成分 也 可以 让 系统 自动 
归纳 这个 后面 再谈 二 是 如何 学习 $ \ 
theta $ 同 之前 代价 函数 一样 用户 $ j 
$ 所有 评 过分 的 电影 作 样 本集 最小化 
预测值 与 实际 值 并 采用 正则化 $ min _ 
{ \ theta ^ { j } } \ frac 
{ 1 } { 2 } \ sum _ { 
i r i j = 1 } \ theta ^ 
{ j } ^ T x ^ { i } 
y ^ { i j } ^ 2 + \ 
frac { \ lambda } { 2 } \ sum 
_ { k = 1 } ^ { n } 
\ theta _ k ^ { j } ^ 2 
$ 如果 要 学习 每个 用户 的 参数 则 $ 
min _ { \ theta ^ { 1 } \ 
cdots \ theta ^ { n _ u } } 
\ frac { 1 } { 2 } \ sum 
_ { j = 1 } { n _ u 
} \ sum _ { i r i j = 
1 } \ theta ^ { j } ^ T 
x ^ { i } y ^ { i j 
} ^ 2 + \ frac { \ lambda } 
{ 2 } \ sum _ { j = 1 
} ^ { n _ u } \ sum _ 
{ k = 1 } ^ { n } \ 
theta _ k ^ { j } ^ 2 $ 
这样 就 可以 使用 梯度 下 降法 去 优化 参数 
以上 的 基于 内容 推荐算法 是 基于 我们 得到 不同 
电影 的 特征向量 但是 实际 应用 中 我们 很难 确定 
所有 的 特征 以及 对应 的 特征 向量 因此 我们 
需要 一种 算法 能够 自动 地 学习 所要 的 特征 
这 就是 以下 要 介绍 的 协同 过滤 Collaborative Filtering 
如果 用户 能够 告诉 我们 他 有多/nr 喜欢 看 爱情片 
多 喜欢 看 动作片 那么 我们 就 能 反过来 根据 
他们 的 评分 反推 这部 电影 的 特征 也 就是 
通过 $ \ theta $ 学习 $ x $ 这 
就是 协同 的 含义 给定 $ x $ 可以 学习 
$ \ theta $ 给定 $ \ theta $ 可以 
学习 $ x $ 如此 反复 同时 最小化 两类 参数 
即 $ J x ^ { 1 } \ cdots 
x ^ { n _ m } \ theta ^ 
{ 1 } \ cdots \ theta ^ { n 
_ u } = \ frac { 1 } { 
2 } \ sum _ { i j r i 
j = 1 } \ theta ^ { j } 
^ T x ^ { i } y ^ { 
i j } ^ 2 + \ frac { \ 
lambda } { 2 } \ sum _ { i 
= 1 } ^ { n _ m } \ 
sum _ { k = 1 } ^ { n 
} x _ k ^ { i } ^ 2 
+ \ frac { \ lambda } { 2 } 
\ sum _ { j = 1 } ^ { 
n _ u } \ sum _ { k = 
1 } ^ { n } \ theta _ k 
^ { j } ^ 2 $ 协同 过滤 的 
算法 可以 归纳 如下 1 . 初始化 $ x ^ 
{ 1 } \ cdots x ^ { n _ 
m } \ theta ^ { 1 } \ cdots 
\ theta ^ { n _ u } $ 为 
很小 的 随机 值 2 . 使用 梯度 下降 算法 
或 其它 高级 优化 算法 最小化 代价 函数 求得 $ 
\ theta $ 和$x/nr $ 3 . 利用 $ \ 
theta ^ T x $ 预测 用户 对 电影 的 
评分 协同 过滤 的 向 量化 实现 方法 如下 令 
$ X = \ begin { bmatrix } & x 
^ { 1 } ^ T & \ \ & 
\ vdots & \ \ & x ^ { n 
_ m } & \ end { bmatrix } \ 
\ Theta = \ begin { bmatrix } & \ 
theta ^ { 1 } ^ T & \ \ 
& \ vdots & \ \ & \ theta ^ 
{ n _ u } & \ end { bmatrix 
} $ 则 $ X \ Theta ^ T = 
\ displaystyle \ begin { bmatrix } x ^ { 
1 } ^ T \ theta ^ { 1 } 
& \ ldots & x ^ { 1 } ^ 
T \ theta ^ { n _ u } \ 
\ \ vdots & \ ddots & \ vdots \ 
\ x ^ { n _ m } ^ T 
\ theta ^ { 1 } & \ ldots & 
x ^ { n _ m } ^ T \ 
theta ^ { n _ u } \ end { 
bmatrix } $ 也 就是 所谓 的 低 秩 矩阵 
分解 从 这个 过程 中 我们 也 可以 衡量 特征向量 
之间 的 距离 如果 两部 电影 的 特征 向量 之间 
距离 很小 $ small \ | | x ^ { 
i } x ^ { j } | | $ 
那么 就 表明 这 两部 电影 很 相似 最后 提 
一下 如果 有个/nr 用户 从来 没有 对 任何 电 影评 
分过 这 如何 预测 这时 可以 利用 均值 归一化 方法 
$   Y = \ begin { bmatrix }   
5 & 5 & 0 & 0 & \ \ 
    5 & & & 0 & \ \ 
    & 4 & 0 & & \ \ 
    0 & 0 & 5 & 4 & 
\ \     0 & 0 & 5 & 
0 &   \ end { bmatrix }   $ 
计算 前 四列 的 均值 $ \ mu $ 然后 
让 $ Y $ 减去 $ \ mu $ 带进 
算法 中 求解 后 再把 均值 加回来 即 $ \ 
theta ^ { j } ^ T x ^ { 
i } + \ mu _ i $ 对于 用户 
5 来说 由于 从来 没有 评 过分 所以 $ \ 
theta ^ { 5 } = 0 0 ^ T 
$ 如此 其实 预测值 就是 $ \ mu $ 好吧 
说了半天 其实 就是 如果 一个 用户 从来 没有 评 过分 
就 取 之前 所有 用户 对 电影 评分 的 均值 
Week 10 2018 . 12.09 Large Scale Machine Learning1 . 
  Gradient Descent with Large Datasets 记得 我们 之前 提到 
过 It s not who has the best algorithms that 
wins it s who has the most data . 近年来 
机器学习 再度 掀起 热潮 的 其中 一个 原因 在于 我们 
已经 拥有 了 处理 海量 数据 的 能力 使得 机器学习 
算法 的 性能 不断 提升 但 并不 是 增加 训练样本 
一定 会 提升 系统 性能 例如 之前 我们 提到 对于 
已经 高 偏差 的 学习 算法 增加 数据 量 并不 
一定 是 合适 的 方向 应该 尝试 增加 额外 的 
特征 另外 对于 大 数据 的 处理 仍然 对 系统 
压力 很大 在 保持 性能 的 情况 我们 还是 希望 
从 算法 上 减少 计算 量 以 线性 回归 的 
梯度 下 降法 为例 在 每次 的 迭代 过程 中 
我们 令 $ θ _ j =   θ _ 
j α \ frac { 1 } { m } 
\ sum _ { i = 1 } ^ m 
h _ θ x ^ { i } y ^ 
{ i } \ cdot x _ j ^ { 
i }   $ 这样 在 每次 迭代 中 我们 
都 需要 把 所有 样本 代入 计算 这 称为 Batch 
gradient descent 为了 减少 计算 量 我们 再 每次 迭代 
中 可以 随机 选择 一个 样本 计算 这 个 样本 
对应 的 梯度 即 Stochastic gradient descent 1 . 随机 
打乱 样 本集 这一步 很 重要 2 . 重复 执行 
以下 步骤 直至 收敛 对于 1 到 m 令 $ 
θ _ j =   θ _ j α h 
_ θ x ^ { i } y ^ { 
i } \ cdot x _ j ^ { i 
}   $ 当 训练 集 的 个数 m 很大 
时 随机 梯度 下降 算法 计算 快 很多 但是 并 
不是 每次 迭代 中 代价 函数 都会 减少 有时 甚至 
会 增大 但 总体 是 曲曲折折 地 下降 至 最低 
点 附近 此外 也许 你 会 觉得 每次 迭代 使用 
一个 样本 也 太少 因此 可以 折中 使用 Mini batch 
gradient descent 在 每次 迭代 中选 b 个 样本 进行 
计算 那么 在 运行 梯度 下降 算法 时 如何 才能 
确定 算法 正常 收敛 批量 梯度 下降 中 我们 画出 
代价 函数 随着 迭代 次数 增加 的 曲线图 确保 它 
每次 迭代 都在 下降 那么 在 随机 梯度 下降 算法 
中 我们 可以 每隔 1000次 迭代计算 一次 最近 1000个 样本 
的 平均 成本 曲线 有 可能 是 曲曲折折 下降 的 
这个 时候 可以 每 5000个 样本 计算 一次 会 得到 
更 平滑 的 下降 曲线 如果 你 发现 曲线 平缓 
也不 下降 那有/nr 可能/v 需要/v 改变/v $/i \/i alpha/w $/i 
或者/c 对/p 算法/n 进行/v 改进/v 如果 你 看到 曲线 反而 
是 上升 的 那 说明 算法 是 发散 的 你 
可以 尝试 使用 较小 的 $ \ alpha $ 通常 
$ \ alpha $ 是 保持 不变 的 但 有时 
为了 收敛 我们 可以 逐渐 的 减小 它 例如 可以 
令 $ \ alpha = const1 / iterationNumber + const2 
$ 2 .   Advanced Topics 来看 两个 大 数据 
学习 相关 的 主题 第一个 是 在线 学习 Online Learning 
在 之前 的 例子 中 我们 都是在/nr 本地 保存 了 
大量 的 训练样本 用来 训练 算法 那么 如果 你 经营 
一个 网站 有 连续 的 数据 流 需要 实时 的 
学习 并 更新 参数 那么 在线 学习 就 是 合适 
的 方法 例如 对于 快递 网站 用户 选择 出发地 和 
目的地 后 你 给出 包裹 报价 用户 会 选择 下单 
y = 1 或者 不 下单 y = 0 那么 
可以 构造 特征 用户 属性 出发地 目的地 等 我们 希望 
学习 $ p y = 1 | x \ theta 
$ 来 优化 报价 每一次 有 新用户 你 都能 得到 
一对 x y 然后 你 就 可以 用 这个 样本 
去 更新 $ \ theta $ 此外 在 网上 商城 
产品 推荐 新闻 文章 定制 化 推荐 等 地方 都 
可以 利用 在线 学习 在线 学习 的 另一个 好处 在于 
它 能够 适应 不断 变化 的 用户 偏好 例如 随着 
时间 推移 经济 形势 发生 转变 用户 价格 敏感度 降低 
愿意 支付 更高 价格 来 买 某个 产品 采用 在线 
学习 就 能 不断 更新 自己 的 参数 与时俱进 如果 
你 的 用户 类型 也 发生 了 变化 或者 有了 
新 类型 的 用户 这种 算法 也 能 不断 适应 
另 一个 是 Map reduce 我们 之所以 改进 批量 梯度 
下降 算法 就是 因为 我们 觉得 每次 迭代 中 计算 
所有 的 样本 太慢 那/r 如果/c 我们/r 有/v 4台/mq 机器/n 
我们 可以 把 求和 的 那 部分 式子 分成 4份 
分别 在 4台 机器 中 进行 计算 相加 得到 一个 
临 时值 最后 合到 一台 机器 上 更新 参数 如果 
不 考虑 网络 传输 速率 和 延迟 问题 那么 可以 
把 速度 提高 4倍 或者 如果 有 一台 具有 多 
内核 的 计算器 也 可以 采用 这种 方法 而且 这种 
情况下 不 容易 出现 网络 延迟 问题 目前 已经 有 
一些 不错 的 MapReduce 开源 工具 如 Hadoop 通常 情况下 
许多 学习 算法 都 包含 训练样本 某种 形式 的 和 
这种 情况下 就 可以 考虑 采用 MapReduce 方法 Week 11 
2018 . 12.09 Application Example Photo OCR 课程 的 最后 
来看 一个 机器学习 应用 的 实例 Photo OCR 从中 我们 
可以 看到 一个 复杂 的 机器学习 系统 是 如何 组合 
实现 的 也会 学习 到 机器学习 流水线 machine learning pipeline 
的 有关 概念 以及 如何 分配 你 的 资源 最后 
可以 了解 到 如何 将 机器学习 应用 于 计算机 视觉 
问题 以及 人工 数据 合成 artificial data synthesis 的 概念 
OCR 其实 就是 字符识别 简单 的 例子 就是 自动 抓拍 
超速 车牌 如果 给 计算机 一幅 图像 需要 计算机 识别 
出 图像 中 的 文字 需要 哪些 步骤 1 . 
文字 检测 Text detection 计算机 先要 识别 图像 中 哪些 
区域 是 包含 文字 的 把 这些 区域 提取 出来 
便于 处理 2 . 字符 分隔 Character segmentation 对于 一个 
单词 直接 识别 似乎 比较 难 我们 可以 先按 字符 
把 图像 区域 分隔 得到 只 包含 一 个字符 的 
小区域 3 . 字符 分类 Character classification 最后 就是 对 
提取 出来 的 字符 图像 做 字符 分类 以上 就是 
图像 OCR 流水线 Pipeline 在 每个 步骤 中都 需要 一定 
的 人力 和 算法 去 处理 首先 来看 文字 检测 
我们 先用 更 简单 的 行人 检测 作 例子 如果 
在 一幅 图像 中 有 多个 行人 怎么 识别 出 
他们 的 区域 这个 会比 文字 检测 简单 些 因为 
大部分 行人 都 比较 相似 因此 可以 用 一个 固定 
长 宽比 的 矩形 来做 检测 例如 取 图像 左上角 
82x36 的 图像 块 通过 机器学习 训练 到 的 分类器 
来 确定 是不是 有 行人 如果 没有 的话 分类器 应该 
输出 为 y = 0 然后 我们 把 窗口 向右 
移动 一点 再次 做 检测 依此 向右 移动 直到 达到 
右 边界 后 回到 最 左边 向下 移动 一点 然后 
重复 之前 的 过程 完成 对 整幅 图像 的 检测 
这 就是 滑动 窗 slide windows 回到 文字 检测 也是 
类似 的 过程 使用 滑动 窗 进行 检测 没有 文字 
的 区域 就 变黑 检测 到 文字 的 区域 变白 
方便 下 一步 提取 对 包含 文字 的 区域 进行 
文字 分隔 也是 同样 的 处理 过程 如果 滑动 窗 
中 只 包含 一 个字符 则 归类 为 y = 
0 如果 不是 单个 字符 就 归类 为 y = 
0 最后 就是 字符 分类 运用 之 前 介绍 过 
的 机器学习 算法 实现 即可 这套 系统 有 良好 性能 
需要 一个 前提 就是 有 训练 优异 的 分类器 怎么 
获得 一个 高效 的 机器学习 系统 呢 一种 方法 是 
先 选择 一个 低 偏差 的 学习 算法 通过 增加 
特征 或在 神经 网络 中 增加 隐藏 层 然后 用 
大量 的 训练 集 来 训练 它 那么 从哪/nr 获得 
大量 的 训练 集 呢 既然 没 办法 获得 那么多 
真实 数据 我们 可以 进行 人工 数据 合成 例如 在 
字符 分类 学习 中 为了 获得 更多 的 训练样本 可以 
采集 同一 字符 的 不同 字体 然后 加上 随机 背景 
这样 就 可以 获得 成千上万 的 训练样本 另一种 方法 是 
对 字符 图像 进行 扭曲 实现 各种各样 的 效果 此外 
还有 一种 人 为 标记 的 做法 其实 很多 人 
现在 就 从事 这样 的 工作 每天 的 工作 内容 
就是 给 AI 公司 标记 样本 当 样本 数量 足够 
多 后 AI 公司 的 产品 算法 才能 有 良好 
的 性能 不过 在 你 做 这些 事情 之前 你 
得 先 问问 自己 如果 要 获得 10倍 的 数据 
量 需要 花费 多少 时间 划 不划算 最后 来看 一下 
在 系统 搭建 的 过程 中 你 必须 知道 哪 
部分 是 你 值得 投入 资源 去 提高 的 资源 
是 有限 的 同时 并 不是 每 件 事情 努力 
了 都有 回报 的 这 就是 上限 分析 Ceiling analysis 
以上 面的 OCR 流水线 为例 如果 整 套 系统 的 
准确度 为 72% 手 动令 文字 检测 的 结果 为 
100% 正确 此时 评估 系统 准确度 为 89% 因此 我们 
说 改善 文字 检测 模块 系统 得到 17% 的 准确度 
提升 非常 值得 投入 资源 去做 如果 手 动令 文字 
分隔 的 结果 为 100% 得到 系统 准确度 为 90% 
因此 系统 准确度 只有 1% 的 提升 因此 没有 必要 
花费 额外 的 人力 物力 在 改善 这个 模块 上 
注 部分/n 图片/n 来源/n 于/p 课程/n 讲义/n 和/c 网络/n 