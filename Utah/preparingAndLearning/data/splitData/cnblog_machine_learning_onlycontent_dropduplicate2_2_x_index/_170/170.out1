一 引言 上 一章 我们 讲 的 kNN 算法 虽然 
可以 完成 很多 分类 任务 但 它 最大 的 缺点 
是 无法 给出 数据 的 内在 含义 而 决策树 的 
主要 优势 就 在于 数据 形式 非常 容易 理解 决策树 
算法 能够 读取数据 集合 决策树 的 一个 重要 任务 是 
为了 数据 所 蕴含 的 知识 信息 因此 决策树 可以 
使用 不 熟悉 的 数据 集合 并从 中 提取 一 
系列 规则 在 这些 机器 根据 数据集 创建 规则 是 
就是 机器 学习 的 过程 二 相关 知识 1 决策树 
算法 在 构造 决策树 时 第一 个 需要 解决 的 
问题 就是 如何 确定 出 哪个 特征 在 划分 数据 
分类 是 起 决定性 作用 或者说 使用 哪个 特征 分类 
能 实现 最好 的 分类 效果 这样 为了 找到 决定性 
的 特征 划分 川 最好 的 结果 我们 就 需要 
评估 每个 特征 当 找到 最优 特征 后 依此 特征 
数据集 就被 划分 为 几个 数据 子集 这些 数据 自己 
会 分布 在 该 决策 点 的 所有 分支 中 
此时 如果 某 个 分支 下 的 数据 属于 同一 
类型 则 该 分支 下 的 数据 分类 已经 完成 
无需 进行 下 一步 的 数据集 分类 如果 分支 下 
的 数据 子集 内 数据 不 属于 同一 类型 那么 
就 要 重复 划分 该 数据集 的 过程 按照 划分 
原始 数据集 相同 的 原则 确 定出 该 数据 子 
集中 的 最优 特征 继续 对 数据 子集 进行 分类 
直到 所有 的 特征 已经 遍历 完成 或者 所有 叶 
结点 分支 下 的 数据 具有 相同 的 分类 创建 
分支 的 伪代码 函数 createBranch 如下 检测 数据 集中 的 
每一个 子项 是否 属于 同一 分类 if so return 类 
标签 else 寻找 划分 数据集 的 最好 特征 划分 数据集 
创建 分支 结点 for 每个 分支 结点 调用函数 createBranch 并 
增加 返回 结 点到 分支 结 点中 / / 递归调用 
createBranch return 分支 结点 了解 了 如何 划分 数据集 后 
我们 可以 总结 出 决策树 的 一般 流程 1 收集 
数据 2 准备 数据 构造 树 算法 只 适用 于 
标称 型 数据 因此 数值 型 数据 必须 离散化 3 
分析 数据 4 训练 数据 上述 的 构造 树 过程 
构造 决策树 的 数据 结构 5 测试 算法 使用 经验 
树 计算 错误率 6 使用 算法 在 实际 中 更好 
地 理解 数据 内在 含义 2 最好 特征 选取 的 
规则 信息 增益 划分 数据集 的 大 原则 是 将 
无序 的 数据 变得 更加 有序 在 划分 数据集 前后 
信息 发生 的 变化 称为 信息 增益 如果 我们 知道 
如何 计算 信息 增益 就 可以 计算 每个 特征值 划分 
数据集 获得 的 信息 增益 而 获取 信息 增益 最高 
的 特征 就是 最好 的 特征 接下来 我们 讲 学习 
如何 计算 信息 增益 而提 到 信息 增益 我们 又 
不得不 提到 一个 概念 香农 熵 或者 简称 熵 熵 
定义 为 信息 的 期望值 如果 待 分类 的 事物 
可能 会 出现 多个 结果 x 则 第 i 个 
结果 xi 发生 的 概率 为 p xi 那么 我们 
可以 由此 计算 出 xi 的 信息 熵 为 l 
xi = p xi log 1 / p xi = 
p xi log p xi 那么 对于 所有 可能 出现 
的 结果 事物 所 包含 的 信息 希望 值 信息熵 
就为 H = Σ p xi log p xi i 
属于 所有 可能 的 结果 这样 假设 利用 数据 集中 
某一 特征 A 对 数据集 D D 的 分类 类别 
有n种/nr 进行 分类 而 特征 A 取值 有k种/nr 那么 此时 
利用 特征 A 对 数据 集 进行 分类 的 信息 
增益 为 信息 增益 H D A = 原始 数据集 
的 信息 熵 H D 特征 A 对 数据 集 
进行 划分 后 信息熵 H D / A 其中 H 
D / A = ∑ | Aj | / | 
D | * H Aj j 属于 A 的 k 
种 取值 | Aj | 和|D/nr | 分别 表示 特征 
A 第 j 种 取值 的 样本 数 占 所有 
取值 样本 总数 的 比例 以及 数据集 的 样本 总数 
三 构造 决策树 在 知道 了 如何 选取 划分 数据 
的 最优 特征 后 我们 就 可以 依 据此 来 
构建 决策树 了 1 由于 要 多次 使用 香农 熵 
的 公式 所以 我们 写出 计算 给定 数据集 的 熵 
的 公式 # 计算 给定 数据集 的 熵 # 导入 
log 运算符 from math import log def calEnt dataSet # 
获取 数据集 的 行数 numEntries = len dataSet # 设置 
字典 的 数据结构 labelCounts = { } # 提取 数据集 
的 每 一行 的 特征向量 for featVec in dataSet # 
获取 特征向量 的 最后 一 列 的 标签 currentLabel = 
featVec 1 # 检测 字典 的 关键字 key 中 是否 
存在 该 标签 # 如果 不 存在 keys 关键字 if 
currentLabel not in labelCounts . keys # 将 当前 标签 
/ 0 键值 对 存入 字典 中 labelCounts currentLabel = 
0 # 否则 将 当前 标签 对应 的 键值 加 
1 labelCounts currentLabel + = 1 # 初始化 熵 为 
0 Ent = 0.0 # 对于 数据 集中 所有 的 
分类 类别 for key in labelCounts # 计算 各 个 
类别 出现 的 频率 prob = float labelCounts key / 
numEntries # 计算 各 个 类别 信息 期望值 Ent = 
prob * log prob 2 # 返回 熵 return Ent2 
我们 当然 需要 构建 决策树 的 数据集 # 创建 一个 
简单 的 数据集 # 数据 集中 包含 两个 特征 no 
surfacing flippers # 数据 的 类 标签 有 两个 yes 
no def creatDataSet dataSet = 1 1 yes 1 1 
yes 1 0 no 0 1 no 0 1 no 
labels = no surfacing flippers # 返回 数据集 和类/nr 标签 
return dataSet labels 需要 说明 的 是 熵 越高 那么 
混合 的 数据 就 越多 如果 我们 在 数据 集中 
添加 更多 的 分类 会 导致 熵 结果 增大 3 
接下来 我们 就要 通过 上面 讲到 的 信息 增益 公式 
得到 划分 数据集 的 最有 特征 从而 划分 数据集 首先 
划分 数据集 的 代码 # 划分 数据集 按照 最优 特征 
划分 数据集 # @ dataSet 待 划分 的 数据 集 
# @ axis 划分 数据集 的 特征 # @ value 
特征 的 取值 def splitDataSet dataSet axis value # 需要 
说明 的 是 python 语言 传递 参数 列表 时 传递 
的 是 列表 的 引用 # 如果 在 函数 内部 
对 列表 对象 进行 修改 将 会 导致 列表 发生变化 
为了 # 不 修改 原始 数据集 创建 一个 新 的 
列表 对象 进行 操作 retDataSet = # 提取 数据集 的 
每 一行 的 特征向量 for featVec in dataSet # 针对 
axis 特征 不同 的 取值 将 数据集 划分 为 不同 
的 分支 # 如果 该 特征 的 取值 为 value 
if featVec axis = = value # 将 特征向量 的 
0 ~ axis 1列 存入 列表 reducedFeatVec reducedFeatVec = featVec 
axis # 将 特征向量 的 axis + 1 ~ 最后 
一列 存入 列表 reducedFeatVec # extend 是 将 另外 一个 
列表 中 的 元素 以 列表 中 元素 为 对象 
一一 添加 到 当前 列表 中 构成 一个 列表 # 
比如 a = 1 2 3 b = 4 5 
6 则 a . extend b = 1 2 3 
4 5 6 reducedFeatVec . extend featVec axis + 1 
# 简言之 就是 将 原始 数据集 去掉 当前 划分 数据 
的 特征 列 # append 是 将 另外 一个 列表 
以 列表 为 对象 添加 到 当前 列表 中 # 
# 比如 a = 1 2 3 b = 4 
5 6 则 a . extend b = 1 2 
3 4 5 6 retDataSet . append reducedFeatVec return retDataSet 
需要 说明 的 是 1 在 划分 数据集 函数 中 
传递 的 参数 dataSet 列表 的 引用 在 函数 内部 
对 该 列表 对象 进行 修改 会 导致 列表 内容 
发生 改变 于是 为了 消除 该 影响 我们 应该 在 
函数 中 创建 一个 新的 列表 对象 将对 列表 对象 
操作 后的/nr 数据集 存入 新的 列表 对象 中 2 需要 
区分 一下 append 函数 和 extend 函数 这 两种 方法 
的 功能 类似 都是 在 列表 末尾 添加 新元素 但是 
在 处理 多个 列表 时 处理结果 有所不同 比如 a = 
1 2 3 b = 4 5 6 那么 a 
. append b 的 结果 为 1 2 3 4 
5 6 即使 用 append 函数 会在 列表 末尾 添 
加人 新的 列表 对象 b 而 a . extend b 
的 结果 为 1 2 3 4 5 6 即使 
用 extend 函数 接下来 我们 再看 选取 最优 特征 的 
代码 # 如何 选择 最好 的 划分 数据集 的 特征 
# 使用 某 一 特征 划分 数据集 信息 增益 最大 
则 选择 该 特征 作为 最优 特征 def c h 
o o s e B e s t F e 
a t u r e T o p l i 
t dataSet # 获取 数据集 特征 的 数目 不 包含 
最后 一列 的 类 标签 numFeatures = len dataSet 0 
1 # 计算 未 进行 划分 的 信息 熵 baseEntropy 
= calEnt dataSet # 最优 信息 增益 最优 特征 bestInfoGain 
= 0.0 bestFeature = 1 # 利用 每 一个 特征 
分别 对 数据 集 进行 划分 计算 信息 增益 for 
i in range numFeatures # 得到 特征 i 的 特征值 
列表 featList = example i for example in dataSet # 
利用 set 集合 的 性质 元素 的 唯一性 得到 特征 
i 的 取值 uniqueVals = set featList # 信息 增益 
0.0 newEntropy = 0.0 # 对 特征 的 每一个 取值 
分别 构建 相应 的 分支 for value in uniqueVals # 
根据 特征 i 的 取值 将 数据 集 进行 划分 
为 不同 的 子集 # 利用 splitDataSet 获取 特征 取值 
Value 分支 包含 的 数据 集 subDataSet = splitDataSet dataSet 
i value # 计算 特征 取值 value 对应 子集 占 
数据集 的 比例 prob = len subDataSet / float len 
dataSet # 计算 占 比 * 当前 子集 的 信息 
熵 并 进行 累加 得到 总的 信息熵 newEntropy + = 
prob * calEnt subDataSet # 计算 按此 特征 划分 数据集 
的 信息 增益 # 公式 特征 A 数据集 D # 
则 H D A = H D H D / 
A infoGain = baseEntropy newEntropy # 比较 此 增益 与 
当前 保存 的 最大 的 信息 增益 if infoGain bestInfoGain 
# 保存信息 增益 的 最大值 bestInfoGain = infoGain # 相应 
地 保存 得 到此 最大 增益 的 特征 i bestFeature 
= i # 返回 最优 特征 return bestFeature 在 函数调用 
中 数据 必须 满足 一定 的 要求 首先 数据 必须 
是由 列表 元素 组成 的 列表 而且 所有 的 列表 
元素 具有 相同 的 数据 长度 其次 数据 的 最后 
一列 或者 每个 实例 的 最后 一个 元素 是 当前 
实例 的 类别 标签 这样 我们 才能 通过 程序 统一 
完成 数据集 的 划分 4 在 通过 以上 的 各个 
模块 学习 之后 我们 接下来 就要 真正 构建 决策树 构建 
决策树 的 工作 原理 为 首先 得到 原始 数据集 然后 
基于 最好 的 属性 划分 数据集 由于 特征值 可能 多 
于 两个 因此 可能 存在 大 于 两个 分支 的 
数据集 划分 第一 次 划分 之后 数据 将 向下 传递 
到 树 分支 的 下一个 结点 在 该 结 点上 
我们 可以 再次 划分 数据 因此 我们 可以 采用 递归 
的 方法 处理 数据集 完成 决策树 构造 递归 的 条件 
是 程序 遍历 完 所有 划分 数据集 的 属性 或者 
每个 分 之下 的 所有 实例 都 具有 相同 的 
分类 如果 所有 的 实例 具有 相同 的 分类 则 
得到 一个 叶子 结点 或者 终止 块 当然 我们 可能 
会 遇到 当 遍历 完 所有 的 特征 属性 但是 
某个 或 多个 分支 下 实例 类 标签 仍然 不 
唯一 此时 我们 需要 确定 出 如何 定义 该 叶子 
结点 在 这种 情况 下 通过 会 采取 多数 表决 
的 原则 选取 分支 下 实例 中 类 标签 种类 
最多 的 分类 作为 该 叶子 结点 的 分类 这样 
我们 就 需要 先 定义 一个 多数 表决 函数 majorityCnt 
# 当 遍历 完 所有 的 特征 属性 后 类 
标签 仍然 不 唯一 分支 下 仍 有 不同 分类 
的 实例 # 采用 多数 表决 的 方法 完成 分类 
def majorityCnt classList # 创建 一个 类 标签 的 字典 
classCount = { } # 遍历 类 标签 列表 中 
每 一个 元素 for vote in classList # 如果 元素 
不在 字典 中 if vote not in classCount . keys 
# 在 字典 中 添加 新的 键值 对 classCount vote 
= 0 # 否则 当前 键 对于 的 值 加 
1 classCount vote + = 1 # 对 字典 中的 
键 对应 的 值 所在 的 列 按照 又大 到 
小 进行 排序 # @ classCount . items 列表 对象 
# @ key = operator . itemgetter 1 获取 列表 
对象 的 第一 个 域 的 值 # @ reverse 
= true 降序 排序 默认 是 升序 排序 sortedClassCount = 
sorted classCount . items \ key = operator . itemgetter 
1 reverse = true # 返回 出现 次数 最多 的 
类 标签 return sortedClassCount 0 0 好了 考虑 了 这种 
情况 后 我们 就 可以 通过 递归 的 方式 写出 
决策树 的 构建 代码 了 # 创 建树 def createTree 
dataSet labels # 获取 数据 集中 的 最后 一列 的 
类 标签 存入 classList 列表 classList = example 1 for 
example in dataSet # 通过 count 函数 获取 类 标签 
列表 中 第一个 类 标签 的 数目 # 判断 数目 
是否 等于 列表 长度 相同 表面 所有 类 标签 相同 
属于 同 一类 if classList . count classList 0 = 
= len classList return classList 0 # 遍历 完 所有 
的 特征 属性 此时 数据集 的 列为 1 即 只有 
类 标签 列 if len dataSet 0 = = 1 
# 多数 表决 原则 确定 类 标签 return majorityCnt classList 
# 确定 出 当前 最优 的 分类 特征 bestFeat = 
c h o o s e B e s t 
F e a t u r e T o p 
l i t dataSet # 在 特征 标签 列表 中 
获取 该 特征 对应 的 值 bestFeatLabel = labels bestFeat 
# 采用 字典 嵌套 字典 的 方式 存储 分类 树 
信息 myTree = { bestFeatLabel { } } # # 
此 位置 书上 写 的 有误 书 上为 del labels 
bestFeat # # 相当于 操作 原始 列表 内容 导致 原始 
列表 内容 发生 改变 # # 按此 运行 程序 报错 
no surfacing is not in list # # 以下 代码 
已 改正 # 复制 当前 特征 标签 列表 防止 改变 
原始 列表 的 内容 subLabels = labels # 删除 属性 
列表 中 当前 分类 数据集 特征 del subLabels bestFeat # 
获取 数据 集中 最优 特征 所在 列 featValues = example 
bestFeat for example in dataSet # 采用 set 集合 性质 
获取 特征 的 所有 的 唯一 取值 uniqueVals = set 
featValues # 遍历 每 一个 特征 取值 for value in 
uniqueVals # 采用 递归 的 方法 利用 该 特征 对 
数据 集 进行 分类 # @ bestFeatLabel 分类 特征 的 
特征 标签 值 # @ dataSet 要 分类 的 数据 
集 # @ bestFeat 分类 特征 的 标称值 # @ 
value 标称 型 特征 的 取值 # @ subLabels 去除 
分类 特征 后的子/nr 特征 标签 列表 myTree bestFeatLabel value = 
createTree splitDataSet \ dataSet bestFeat value subLabels return myTree 需要 
说明 的 是 此时 参数 dataSet 为 列表 的 引用 
我们 不能 在 函数 中 直接 对 列表 进行 修改 
但是 在 书中 代码 中有 del labels bestFeat 的 删除 
列表 某 一列 的 操作 显然 不 可取 应该 创建 
新的 列表 对象 subLabels = labels 再 调用函数   del 
subLabels bestFeat 好了 接下来 运行 代码 5 接下来 我们 可以 
通过 决策树 进行 实际 的 分类 了 利用 构 建好 
的 决策树 输入 符合 要求 的 测试数据 比较 测试数据 与 
决策 树上 的 数值 递归 执行 该 过程 直到 叶子 
结点 最后 将 测试数据 定义 为 叶子 结点 所有 的 
分类 输出 分类 结果 决策树 分类 函数 代码 为 # 
测试 算法 # 完成 决策树 的 构造 后 采用 决策树 
实现 具体 应用 # @ intputTree 构 建好 的 决策树 
# @ featLabels 特征 标签 列表 # @ testVec 测试 
实例 def classify inputTree featLabels testVec # 找到 树 的 
第一 个 分类 特征 或者说 根 节点 no surfacing # 
注意 python2 . x 和3./nr x 区别 2 . x 
可 写成 firstStr = inputTree . keys 0 # 而 
不支持 3 . x firstStr = list inputTree . keys 
0 # 从树中/nr 得到 该 分类 特征 的 分支 有0和/nr 
1/m secondDict = inputTree firstStr # 根据 分类 特征 的 
索引 找到 对应 的 标称 型 数据 值 # no 
surfacing 对应 的 索 引为 0 featIndex = featLabels . 
index firstStr # 遍历 分类 特征 所有 的 取值 for 
key in secondDict . keys # 测试 实例 的 第 
0个 特征 取值 等于 第 key 个子 节点 if testVec 
featIndex = = key # type 函数 判断 该 子 
节点 是否 为 字典 类型 if type secondDict key . 
_ _ name _ _ = = dict # 子 
节点 为 字典 类型 则从 该 分支 树 开始 继续 
遍历 分类 classLabel = classify secondDict key featLabels testVec # 
如果 是 叶子 节点 则 返回 节点 取值 else classLabel 
= secondDict key return classLabel 输入 实例 通过 分类 函数 
得到 预测 结果 可以 与 实际 结果 比对 计算 错误率 
6   我们 说 一个 好 的 分类 算法 要 
能够 完成 实际 应用 的 需要 决策树 算法 也不例外 一个 
算法 好不好 还是 需要 实际 应用 的 检验 才行 接下来 
我们 会 通过 一个 实例 来 使用 决策树 预测 隐形眼镜 
的 类型 首先 我们 知道 构建 决策树 是 非常 耗时 
的 任务 即使 很小 的 数据 集 也 要花费 几秒 
的 时间 来 构建 决策树 这样 显然 耗费 计算 时间 
所以 我们 可以 将 构建 好 的 决策 树 保存 
在 磁 盘中 这样 当 我们 需要 的 时候 再从 
磁 盘中 读 取出 来 使用 即可 如何 进行 对象 
的 序列 化 操作 python 的 pickle 模块 足以 胜任 
该 任务 任何 对象 都 可以 通过 pickle 模块 执行 
序列化 操作 字典 也不例外 使用 pickle 模块 存储 和 读取 
决策树 文件 的 代码 如下 # 决策树 的 存储 python 
的 pickle 模块 序列化 决策树 对象 使 决策树 保存 在 
磁 盘中 # 在 需要 时 读取 即可 数据集 很大 
时 可以 节省 构造 树 的 时间 # pickle 模块 
存储 决策树 def storeTree inputTree filename # 导入 pickle 模块 
import pickle # 创建 一个 可以 写 的 文本文件 # 
这里 如果 按 树 中写 的 w 将会 报错 write 
argument must be str not bytes # 所以 这里 改为 
二进制 写入 wb fw = open filename wb # pickle 
的 dump 函数 将 决策树 写入 文件 中 pickle . 
dump inputTree fw # 写 完成 后 关闭 文件 fw 
. close # 取 决策树 操作 def grabTree filename import 
pickle # 对应 于 二进制 方式 写入 数据 rb 采用 
二进制 形式 读出 数据 fr = open filename rb return 
pickle . load fr 这里 文件 的 写入 操作 为 
wb 或 wb + 表示 以 byte 的 形式 写入 
数据 相应 rb 以 byte 形式 读入 数据 接下来 我们 
将 通过 隐形眼镜 数据集 构建 决策树 从而 预测 患者 需要 
佩戴 的 隐形眼镜 的 类型 步骤 如下 1 收集 数据 
文本 数据集 lenses . txt 2 准备 数据 解析 tab 
键 分 隔开 的 数据 行 3 分析 数据 快速 
检查数据 确保 正确地 解析 数据 内容 4 训练 算法 构建 
决策树 5 测试 算法 通过 构建 的 决策 树 比较 
准确 预测 出 分类 结果 6 算法 的 分类 准确 
类 满足要求 将 决策树 存储 下来 下次 需要 时 读取 
使用 其中 解析 文本 数据集 和 构建 隐形眼镜 类型 决策树 
的 函数 代码 如下 # 示例 使用 决策树 预测 隐形眼镜 
类型 def p r e d i c t L 
e n s e s T y p e filename 
# 打开 文本 数据 fr = open filename # 将 
文本 数据 的 每一个 数据 行 按照 tab 键 分割 
并 依次 存入 lenses lenses = inst . strip . 
split \ t for inst in fr . readlines # 
创建 并 存入 特征 标签 列表 lensesLabels = age prescript 
astigmatic tearRate # 根据 继续 文件 得到 的 数据集 和 
特征 标签 列表 创建 决策树 lensesTree = createTree lenses lensesLabels 
return lensesTree 当然 我们 还 可以 通过 python 的 matplotlib 
工具 绘制 出 决策树 的 树形图 由于 内容 太多 就不 
一一 讲解 啦 接下来 补充 一下 决策树 算 法可能 或 
出现 的 过度 匹配 过拟合 的 问题 当 决策树 的 
复杂度 较大 时 很 可能 会 造成 过拟合 问题 此时 
我们 可以 通过 裁剪 决策树 的 办法 降低 决策树 的 
复杂度 提高 决策树 的 泛化 能力 比如 如果 决策树 的 
某一 叶子 结点 只能 增加 很少 的 信息 那么 我们 
就 可 将该 节点 删掉 将其 并入 到 相邻 的 
结点 中去 这样 降低 了 决策树 的 复杂度 消除 过拟合 
问题 