By Kubi Code 文章 目录 1 . 有/v 监督/vn 学习/v 
和无/nr 监督/vn 学习/v 的/uj 区别/n 2/m ./i 正则化 3 . 
过拟合 3.1 . 产生 的 原因 3.2 . 解决 方法 
4 . 泛化 能力 5 . 生成 模型 和 判别 
模型 6 . 线性 分类器 与 非线性 分类器 的 区别 
以及 优劣 6.1 . 特征 比 数据量 还大 时 选择 
什么样 的 分类器 6.2 . 对于 维度 很高 的 特征 
你 是 选择 线性 还是 非线性 分类器 6.3 . 对于 
维度 极低 的 特征 你 是 选择 线性 还是 非线性 
分类器 7 . ill condition 病态 问题 8 . L1 
和 L2 正则 的 区别 如何 选择 L1 和 L2 
正则 9 . 特征向量 的 归一化 方法 10 . 特征向量 
的 异常值 处理 11 . 越小 的 参数 说明 模型 
越 简单 12 . svm 中 rbf 核 函数 与 
高斯 和 函数 的 比较 13 . KMeans 初始 类 
簇 中心点 的 选取 13.1 . 选择 批次 距离 尽可能 
远 的 K 个 点 13.2 . 选用 层次 聚 
类 或者 Canopy 算法 进行 初始 聚 类 14 . 
ROC AUC14 . 1 . ROC 曲线 14.2 . AUC14 
. 3 . 为什么 要 使用 ROC 和 AUC15 . 
测试 集 和 训练 集 的 区别 16 . 优化 
Kmeans17 . 数据挖掘 和 机器 学习 的 区别 18 . 
备注/v 有/v 监督/vn 学习/v 和无/nr 监督/vn 学习/v 的/uj 区别/n 有/v 
监督/vn 学习/v 对 具有 标记 的 训练 样本 进行 学习 
以 尽可能 对 训练样本 集外 的 数据 进行 分类 预测 
LR SVM BP RF GBRT 无 监督 学习 对 未 
标记 的 样本 进行 训练 学习 比 发现 这些 样本 
中 的 结构 知识 KMeans DL 正则化 正则化 是 针对 
过拟合 而 提出 的 以为 在 求解 模型 最优 的 
是 一般 优化 最小 的 经验 风险 现在 在 该 
经验 风险 上 加入 模型 复杂度 这 一项 正则化 项是/nr 
模型 参数 向量 的 范数 并 使用 一个 rate 比率 
来 权衡 模型 复杂度 与 以往 经验 风险 的 权重 
如果 模型 复杂度 越高 结构化 的 经验 风险 会 越大 
现在 的 目标 就 变为 了 结构 经验 风险 的 
最优化 可以 防止 模型 训练 过度 复杂 有效 的 降低 
过拟合 的 风险 奥卡姆 剃刀 原理 能够 很好 的 解释 
已知 数据 并且 十分 简单 才是 最好 的 模型 过拟合 
如果 一味 的 去 提高 训练 数据 的 预测 能力 
所选 模型 的 复杂度 往往 会 很高 这种 现象 称为 
过拟合 所 表现 的 就是 模型 训练 时候 的 误差 
很小 但在 测试 的 时候 误差 很大 产生 的 原因 
因为 参数 太多 会 导致 我们 的 模型 复杂度 上升 
容易 过拟合 权值 学习 迭代 次数 足够 多 Overtraining 拟合/v 
了/ul 训练/vn 数据/n 中/f 的/uj 噪声/n 和/c 训练/vn 样例/i 中/f 
没有/v 代表性/n 的/uj 特征/n ./i 解决/v 方法/n 交叉/n 验证法/n 减少/v 
特征/n 正则化/i 权值/i 衰减/v 验证/v 数据/n 泛化/v 能力/n 泛化/v 能力/n 
是/v 指/n 模型/n 对/p 未知/v 数据/n 的/uj 预测/vn 能力/n 生成/v 
模型/n 和/c 判别/v 模型/n 生成/v 模型/n 由 数据 学习 联合 
概率分布 P X Y 然后 求出 条件 概率分布 P Y 
| X 作为 预测 的 模型 即 生成 模型 P 
Y | X = P X Y / P X 
朴素 贝叶斯 生成 模型 可以 还原 联合 概率分布 p X 
Y 并且有 较快 的 学习 收敛 速度 还 可以 用于 
隐 变量 的 学习 判别 模型 由 数据 直接 学习 
决策函数 Y = f X 或者 条件 概率分布 P Y 
| X 作为 预测 的 模型 即 判别 模型 k 
近邻 决策树 直接 面对 预测 往往 准确率 较高 直接 对 
数据 在 各种 程度 上 的 抽象 所以 可以 简化 
模型 线性 分类器 与 非线性 分类器 的 区别 以及 优劣 
如果 模型 是 参数 的 线性函数 并且 存在 线性 分类 
面 那么 就是 线性 分类器 否则 不是 常见 的 线性 
分类器 有 LR 贝叶斯 分类 单层 感知机 线性 回归 常见 
的 非线性 分类器 决策树 RF GBDT 多层 感知机 SVM 两种 
都有 看 线性 核 还是 高斯 核 线性 分类器 速度快 
编程 方便 但是 可能 拟合 效果 不 会 很好 非线性 
分类器 编程 复杂 但是 效果 拟合 能力强 特征 比 数据量 
还大 时 选择 什么样 的 分类器 线性 分类器 因为 维度 
高的/nr 时候 数据 一般 在 维 度空间 里面 会 比较 
稀疏 很 有可能 线性 可分 对于 维度 很高 的 特征 
你 是 选择 线性 还是 非线性 分类器 理由 同上 对于 
维度 极低 的 特征 你 是 选择 线性 还是 非线性 
分类器 非线性 分类器 因为 低 维空间 可能 很多 特征 都 
跑到 一起 了 导致 线性 不可分 ill condition 病态 问题 
训练 完 的 模型 测试 样本 稍作 修改 就会 得到 
差别 很大 的 结果 就是 病态 问题 这 简直 是 
不能 用 啊 L1 和 L2 正则 的 区别 如何/r 
选择/v L1/i 和/c L2/i 正则/n 他们/r 都是/nr 可以/c 防止/v 过拟合/i 
降低 模型 复杂度 L1 是 在 loss function 后面 加上 
模型 参数 的 1 范数 也 就是 | xi | 
L2 是 在 loss function 后面 加上 模型 参数 的 
2 范数 也 就是 sigma xi ^ 2 注意 L2 
范数 的 定义 是 sqrt sigma xi ^ 2 在 
正则 项上/nr 没有 添加 sqrt 根号 是 为了 更加 容易 
优化 L1 会 产生 稀疏 的 特征 L2 会 产生 
更多 地 特征 但是 都会 接近于 0L1 会 趋向 于 
产生 少量 的 特征 而 其他 的 特征 都是 0 
而 L2 会 选择 更多 的 特征 这些 特征 都会 
接近于 0 L1 在 特征选择 时候 非常 有用 而 L2 
就 只是 一种 规则化 而已 特征向量 的 归一化 方法 线性函数 
转换 表达式 如下 y = x MinValue / MaxValue MinValue 
对数函数 转换 表达式 如下 y = log10 x 反 余切函数 
转换 表达式 如下 y = arctan x * 2 / 
PI 减去 均值 乘以 方差 y = x means / 
variance 特征向量 的 异常值 处理 用 均值 或者 其他 统计量 
代替 越小 的 参数 说明 模型 越 简单 过拟合 的 
拟合 会 经过 曲面 的 每个 点 也 就是说 在 
较小 的 区间 里面 可能会 有 较大 的 曲率 这里 
的 导数 就是 很大 线性 模型 里面 的 权值 就是 
导数 所以 越小 的 参数 说明 模型 越 简单 追加 
这个 其实 可以 看 VC 维 相关 的 东西 感觉 
更加 合适 svm 中 rbf 核 函数 与 高斯 和 
函数 的 比较 高斯 核 函数 好像 是 RBF 核 
的 一种 KMeans 初始 类 簇 中心点 的 选取 选择 
批次 距离 尽可能 远 的 K 个 点 首先 随机 
选取 一个 点 作为 初 始点 然后 选择 距离 与 
该点 最远 的 那个 点 作为 中心点 再 选择 距离 
与 前 两个 点 最远 的 店 作为 第三 个 
中心店 以此类推 直至 选取 大 k 个 选用 层次 聚 
类 或者 Canopy 算法 进行 初始 聚 类 ROC AUCROC 
和 AUC 通常 是 用来 评价 一个 二 值 分类器 
的 好坏 ROC 曲线 曲线 坐 标上 X 轴 是 
FPR 表示 假 阳率/nr 预测 结果 为 positive 但是 实际 
结果 为 negitive FP / N Y 轴式 TPR 表示 
真 阳率/nr 预测 结果 为 positive 而且 的确 真实 结果 
也为 positive 的 TP / P 那么 平面 的 上点 
X Y 0 1 表示 所有 的 positive 的 样本 
都 预测 出来 了 分类 效果 最好 0 0 表示 
预测 的 结果 全部 为 negitive 1 0 表示 预测 
的 错过 全 部分 错了 分类 效果 最差 1 1 
表示 预测 的 结果 全部 为 positive 针对 落在 x 
= y 上点 表示 是 采用 随机 猜测 出来 的 
结果 ROC 曲线 建立 一般 默认 预测 完成 之后 会 
有一个 概率 输出 p 这个 概率 越高 表示 它 对 
positive 的 概率 越大 现在 假设 我们 有一个 threshold 如果 
p threshold 那么 该 预测 结果 为 positive 否 则为 
negitive 按照 这个 思路 我们 多 设置 几个 threshold 那么 
我们 就 可以 得到 多组 positive 和 negitive 的 结果 
了 也 就是 我们 可以 得到 多组 FPR 和 TPR 
值了 将 这些 FPR TPR 点 投射 到 坐 标上 
再用 线 连接 起来 就是 ROC 曲线 了当 threshold 取 
1 和 0时 分别 得到 的 就是 0 0 和 
1 1 这 两个 点 threshold = 1 预测 的 
样本 全部 为 负 样本 threshold = 0 预测 的 
样本 全部 为 正 样本 AUCAUC Area Under Curve 被 
定义 为 ROC 曲 线下 的 面积 显然 这个 面积 
不会 大于 1 一般 情况 下 ROC 会在 x = 
y 的 上方 所以 0.5 AUC 1 . AUC 越大 
说明 分类 效果 越好 为什么 要 使用 ROC 和 AUC 
因为 当 测试 集中 的 正负 样本 发生 变化 时 
ROC 曲线 能 基本 保持 不变 但是 precision 和 recall 
可能 就 会有 较大 的 波动 http / / www 
. douban . com / note / 284051363 / type 
= like 测试 集 和 训练 集 的 区别 训练 
集 用于 建立 模型 测试 集 评估 模型 的 预测 
等 能力 优化 Kmeans 使用 kd 树 或者 ball tree 
这个 树 不懂 将 所有 的 观测 实例 构 建成 
一颗 kd 树 之前/f 每个/r 聚/v 类/q 中心/n 都是/nr 需要/v 
和/c 每个/r 观测点/n 做/v 依次/d 距离/n 计算/v 现在 这些 聚 
类 中心 根据 kd 树 只需要 计算 附近 的 一个 
局部 区域 即可 数据挖掘 和 机器 学习 的 区别 机器学习 
是 数据 挖掘 的 一个 重要 工具 但是 数据挖掘 不仅仅 
只有 机器学习 这一类 方法 还有 其他 很多 非 机器 学习 
的 方法 比 如图 挖掘 频繁 项 挖掘 等 感觉 
数据挖掘 是从 目的 而言 的 但是 机器学习 是从 方法 而言 
的 备注 题目 主要 来源于 网络 答案 主要 来源于 网络 
或者 统计 学习 方法 还有 自己 一 小 部分 的 
总结 如果 错误 之处 敬请 指出 如果 想 要 了解 
关于 常见 模型 的 东东 可以 看 这篇 机器学习 常见 
算法 个人 总结 面 试用 文章 