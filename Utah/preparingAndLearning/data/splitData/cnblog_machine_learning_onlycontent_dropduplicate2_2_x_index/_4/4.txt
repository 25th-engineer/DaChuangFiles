#对coursera上Andrew Ng老师开的机器学习课程的笔记和心得；
#注:此笔记是我自己认为本节课里比较重要、难理解或容易忘记的内容并做了些补充，并非是课堂详细笔记和要点；
#标记为<补充>的是我自己加的内容而非课堂内容，参考文献列于文末。博主能力有限，若有错误，恳请指正；
#---------------------------------------------------------------------------------#
多层神经网络模型：
，
<补充>:每一个单元有一定数量的实值输入，产生单一的实值输出(可以是其他很多单元的输入)；
符号标记：ai(j):activation of unit i in layer j ；Ɵ(j) :matrix of parameters controlling the function mapping from layer j to layer j+1；
#---------------------------------------------------------------------------------#
神经网络的cost function:
前一项的目的是使所有单元的误差和最小(采用对数损失函数)，后一项是regularization项，旨在控制模型复杂度，防止overfitting；
#---------------------------------------------------------------------------------#
forward propagation(前向传播)
<补充>：其实也就是通过神经网络，从输入参数到输出结果的计算过程(只计算一次)；
参数的计算如下：
，其中g(x)是sigmoid函数；
#---------------------------------------------------------------------------------#
Back propagation(反向传播)：与前向传播非常类似，从结果层倒推回输入层，计算每层δ的过程，δ为误差；
，其中：l指第几层，；
注:第一层是输入层，没有δ1项，最后一层(输出层)的δ不是按此式计算，见下例；
，
δ4 = a4 - y，δ3 = (Ɵ3)T δ4 . *(a3 . * (1 - a3))，δ2 = (Ɵ2)T δ3 . *(a2 . * (1 - a2))；
#---------------------------------------------------------------------------------#
Back propagation algorithm(反向传播算法)
<补充>：一个最优化问题，目的是在使cost function值最小(这里是通过偏导最小来实现)的情况下，训练出神经网络各个参数的权值；
算法如下:
1，给出训练集作为输入，，将delta值设为0，；
2，进行下列过程直至性能满足要求为止:
对于每一训练(采样)输入，
(a) 通过前向传播计算所得输出。
(b) 通过反向传播计算每层的δ值；
(c) 更新delta值：；
3，得到神经网络参数的权值:
,其中：;
#---------------------------------------------------------------------------------#
几则关于神经网络的问题和解决办法
1，Gradient checking：反向传播算法有很多细节，非常容易出错，Gradient checking有助于cost function J(Ɵ)的准确性；
原理：比较由反向传播计算得到的DVec和梯度计算得到的gradApprox两者是否相近似来判断；
<补充>：其实是用了微积分当中导数的概念，；
注:在训练数据时需要将Gradient checking代码注释掉，因为gradApprox的计算是很耗时的；
2，Random initialization：反向传播算法是局部收敛的，需多次选起始点训练来减少最终局部收敛的可能性；
#---------------------------------------------------------------------------------#
参考文献:
《machine learning》, by Tom Mitchell；
couresra课程: standford machine learning, by Andrew Ng；