Adaboost 提升 算法 是 机器 学习 中 很好用 的 两个 
算法 之一 另 一个 是 SVM 支持 向量 机 机器学习 
面试 中 也会 经常 提 问到 Adaboost 的 一些 原理 
另外 本文 还 介绍 了 一下 非平衡 分类 问题 的 
解决方案 这个 问题 在 面试 中 也 经常 被 提到 
比如 信用卡 数据 集中 失信 的 是 少数 5 10000 
的 情况 下 怎么 准确 分类 一 引言 1 元 
算法 集成 算法 多个 弱 分类器 的 组合 弱 分类器 
的 准确率 很低 50% 接近 随机 了 这种 组合 可 
以是 不同 算法 或 同一 算法 不同 配置 或是 数据集 
的 不同 部分 分配 给 不同 分类器 2 bagging 把 
原始 数据集 随机抽样 成个与/nr 原始 数据集 一样 大新 数据集 允许 
有 重复 值 然后 训练 个 分类器 最后 投票 结果 
集成 代表 随机 森林 3 boosting 关注 以后 分类器 错分 
的 数据 而 得到 新的 分类器 代表 adaboostbagging 和 boosting 
类似 都是 抽样 的 方式 构造 多个 数据集 特别 适用 
于 数据集 有限 的 时候 并且 多个 组合 分类器 的 
类型 都 相同 但 bagging 是 串行 的 下一个 分类器 
在 上一个 分类器 的 基础 上 继续 训练 得到 的 
权重 均等 而 boosting 关注 的 是 错分 的 数据 
错分 的 数据 权 重大 二 adaboost adaptive boost 自适应 
提升 算法 原理 为 每一个 样本 赋 均等 的 权重 
D = 1 / n 先用 这个 数据集 训练 第一个 
弱 分类器 计算 错误率 错误率 是 为了 计算 这个 分类器 
最后 投票 的 权重 alpha 见 公式 错分 的 样本 
权重 提升 对分 的 样本 权重 降低 然后 用 这个 
数据集 训练 第二个 若 分类器 迭代 到 弱 分类器 错误率 
为 0 或 迭代 指定 个数 的 弱 分类器 停止 
直观 如图 第一 个 分类器 每个 样本 权重 均等 最后 
根据 错误率 计算 alpha = 0.69 然后 调整 样本 权重 
错分 的 权重 增加 得 第二 个 分类器 的 alpha0 
. 97 同理 第三 个 分类器 的 alpha = 0.90 
最后 投票 总 的 结果 是 = 0.69 * D1 
+ 0.97 * D2 + 0.90 * D3 1 弱 
分类器 本文 采用 是 时 单层 分类器 又叫 树桩 分类器 
是 决策树 最 简单 的 一种 def stumpClassify dataMatrix dimen 
threshVal threshIneq # just classify the data retArray = ones 
shape dataMatrix 0 1 if threshIneq = = lt retArray 
dataMatrix dimen = threshVal = 1.0 else retArray dataMatrix dimen 
threshVal = 1.0 return retArray def buildStump dataArr classLabels D 
dataMatrix = mat dataArr labelMat = mat classLabels . T 
m n = shape dataMatrix numSteps = 10.0 bestStump = 
{ } bestClasEst = mat zeros m 1 minError = 
inf # init error sum to + infinity for i 
in range n # loop over all dimensions rangeMin = 
dataMatrix i . min rangeMax = dataMatrix i . max 
stepSize = rangeMax rangeMin / numSteps for j in range 
1 int numSteps + 1 # loop over all range 
in current dimension for inequal in lt gt # go 
over less than and greater than threshVal = rangeMin + 
float j * stepSize predictedVals = stumpClassify dataMatrix i threshVal 
inequal # call stump classify with i j lessThan errArr 
= mat ones m 1 errArr predictedVals = = labelMat 
= 0 weightedError = D . T * errArr # 
calc total error multiplied by D # print split dim 
% d thresh % . 2f thresh ineqal % s 
the weighted error is % . 3f % i threshVal 
inequal weightedError if weightedError minError minError = weightedError bestClasEst = 
predictedVals . copy bestStump dim = i bestStump thresh = 
threshVal bestStump ineq = inequal return bestStump minError bestClasEst 原理 
遍历 每个 属性 以 一定 步长 枚举 大于 和 小于 
找 一条 错误率 最小 的 与 垂直 坐标轴 的 直线 
分开 样本点 例如 ins = a b c 找到 的 
若 分类器 是 a = 1 or b = 2 
or c = 3 这样 的 垂直 坐标轴 的 直线 
2 adaboost 训练 分类器 的 代码 原理 如上 介绍 训练 
分类器 就是 为了 得到 若 分类器 的 参数 dim thresh 
ineq 和 alpha 前 三个 参数 dim thresh ineq 是 
弱 分类器 树桩 分类器 的 参数 最后 一个 alpha 是 
集合 多 弱 分类器 结果 的 权重 def adaBoostTrainDS dataArr 
classLabels numIt = 40 weakClassArr = m = shape dataArr 
0 D = mat ones m 1 / m # 
init D to all equal aggClassEst = mat zeros m 
1 for i in range numIt bestStump error classEst = 
buildStump dataArr classLabels D # build Stump print error error 
# print D D . T alpha = float 0.5 
* log 1.0 error / max error 1e 16 # 
calc alpha throw in max error eps to account for 
error = 0 bestStump alpha = alpha weakClassArr . append 
bestStump # store Stump Params in Array print classEst classEst 
. T expon = multiply 1 * alpha * mat 
classLabels . T classEst # exponent for D calc getting 
messy D = multiply D exp expon # Calc New 
D for next iteration D = D / D . 
sum print D D # calc training error of all 
classifiers if this is 0 quit for loop early use 
break aggClassEst + = alpha * classEst aggErrors = multiply 
sign aggClassEst = mat classLabels . T ones m 1 
errorRate = aggErrors . sum / m print total error 
errorRate if errorRate = = 0.0 break return weakClassArr aggClassEst 
3 测试 adaboost 代码 根据 弱 i 训练 分类器 得到 
的 参数 使用 设置 参数 的 弱 分类器 对 测试 
样本 进行 预测 最后 结果 通过 alpha 集成 def adaClassify 
datToClass classifierArr dataMatrix = mat datToClass # do stuff similar 
to last aggClassEst in adaBoostTrainDS m = shape dataMatrix 0 
aggClassEst = mat zeros m 1 for i in range 
len classifierArr classEst = stumpClassify dataMatrix classifierArr i dim \ 
classifierArr i thresh \ classifierArr i ineq # call stump 
classify aggClassEst + = classifierArr i alpha * classEst print 
aggClassEst return sign aggClassEst 三 病 马 数据集 实例 datArr 
labelArr = loadDataSet HorseTraining2 . txt classifierArr = adaBoostTrainDS datArr 
labelArr 9 testArr testLabelArr = loadDataSet HorseTraining2 . txt prediciton 
= adaClassify testArr classifierArr error = mat ones 67 1 
error prediciton = mat testLabelArr . T . sum 这个 
实例 就是 调用 了 上面 adaboost 的 接口 值得 注意 
的 是 这个/r 病/n 马的/nr 数据集/i 是/v 我们/r 在/p 上一/i 
篇文章/n logistics/w 算法/n 时/n 用到/v 的/uj 在 logistics 里 错误率 
是 0.3 因为 这个 数据集 有 很多 缺失 值 难 
预测 而 adaboost 的 50个 弱 分类器 的 错误 率 
只有 0.21 主意 弱 分类器 的 个数 太少 易欠/nr 拟合 
太多 易 过拟合 最好 的 是 适当 的 个数 就像 
一张 经典 的 图 横坐标 是 弱 分类器 的 个数 
训练样本 的 错误率 越来越低 测试 样本 的 错误率 是 对 
勾型/nr 取 拐点 处 个数 最好 了 既不 过拟合 也不 
欠 拟合 四 不平衡 分类 问题 不 平衡 问题 是 
正 例和 负 例 的 比例 相差 很大 比如 信用卡 
账户 是否 欠账 5个 正 例 5000个 负 例 1 
解决方案 1 预处理 级 过 采样 和欠/nr 采样 及 混合 
采样 抽样 过程 可以 通过 随机 或 制定 的 方式 
实现 1 过 采样 复制 正 例 样本 增加 样本 
个数 或者 增加 和正例/nr 样本 相似 的 样本 2 欠 
采样 删除 距离 边界 较远 负 例 样本 上例 中 
为了 平衡 需要 删除 4950个 负 例 3 混合 过 
采样 和欠/nr 采样 2 算法 级 代价 敏感 举个 例子 
说明 是 什么 代价 敏感 分类器 二 分类器 代价 矩阵 
真实 结果 | 预测 结果 + 1 1 + 1 
51 1500 根据 代价 矩阵 表 求出 最后 的 总的 
代价 选择 代价 最小 的 类 做为 左后 的 预测 
结果 2 AUC 计算 代码 就 不能 把 准确率 自己 
作为 不 平衡 问题 的 评价 指标 了 因为 在 
不 平衡 分类 中 100个 样本 90 正 例 10 
负 例 则 粗暴 的 把 100个 全 分为 正 
类 就 可以 达到 很高 的 100% 准确率 这 显然 
不是 我们 想要 的 结果 召回率 这时候 也 起到 了 
作用 正 类 中 分对 了 多少 90% AUC 是 
最为 理想 的 一个 指标 通过 正 例和 负 例 
pairs 的 排名 计算 def plotROC predStrengths classLabels import matplotlib 
. pyplot as plt cur = 1.0 1.0 # cursor 
ySum = 0.0 # variable to calculate AUC numPosClas = 
sum array classLabels = = 1.0 yStep = 1 / 
float numPosClas xStep = 1 / float len classLabels numPosClas 
sortedIndicies = predStrengths . argsort # get sorted index it 
s reverse fig = plt . figure fig . clf 
ax = plt . subplot 111 # loop through all 
the values drawing a line segment at each point for 
index in sortedIndicies . tolist 0 if classLabels index = 
= 1.0 delX = 0 delY = yStep else delX 
= xStep delY = 0 ySum + = cur 1 
# draw line from cur to cur 0 delX cur 
1 delY ax . plot cur 0 cur 0 delX 
cur 1 cur 1 delY c = b cur = 
cur 0 delX cur 1 delY ax . plot 0 
1 0 1 b plt . xlabel False positive rate 
plt . ylabel True positive rate plt . title ROC 
curve for AdaBoost horse colic detection system ax . axis 
0 1 0 1 plt . show print the Area 
Under the Curve is ySum * xStep 五 总结 优点 
准确度 较高 无 参数 调整 缺点 对 离散 值 敏感 
数据类型 数值 和 离散 型 