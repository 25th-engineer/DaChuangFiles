作者 Datartisan 链接 https / / zhuanlan . zhihu . 
com / p / 22833471 来源 知乎 著作权 归 作者 
所有 商业 转载 请 联系 作者 获得 授权 非商业 转载 
请 注明 出处 每个/r 数据/n 科学家/n 每天/r 都要/nr 处理/v 成吨的/nr 
数据/n 而/c 他们/r 60%/mf ~/i 70%/mf 的/uj 时间/n 都在/nr 进行/v 
数据/n 清洗/v 和/c 数据格式/n 调整/vn 将 原始数据 转变 为 可以 
用 机器 学习 所 识别 的 形式 本文 主要 集中 
在 数据 清洗 后的/nr 过程 也 就是 机器 学习 的 
通用 框架 这个 框架 是 我 在 参加 了 百余 
场 机器学习 竞 赛后 的 一个 总结 尽管 这个 框架 
是 非常 笼统 和 概括 的 但是 绝对 能 发挥 
强大 的 作用 仍然 可以 在 专业 人员 的 运用 
下 变成 复杂 高效 的 方法 整个 过程 使用 Python 
来 实现 数据 在 用 机器 学习 的 方法 之前 
我们 应该 先 把 数据 转变为 表格 的 形式 这个 
过程 是 最 耗时 最 复杂 的 我们 用 下图 
来 表示 这 一 过程 这一 过程 也 就是 将 
原始 数据 的 所有 的 变量 量化 进一步 转变 为 
含 数据 Data 和 标签 Labels 的 数据 框 形式 
这样 处理 过 的 数据 就 可以 用来 机器学习 建模 
了 数据 框 形式 的 数据 是 机器 学习 和 
数据 挖掘 中 最为 通用 的 数据 表现 形式 它 
的 行 是 数据 抽样 得到 的 样本 列 代表 
数据 的 标签 Y 和 特征 X 其中 标签 根据 
我们 要 研究 的 问题 不同 有 可能 是 一列 
或 多列 标签 的 类型 根据 我们 要 研究 的 
问题 标签 的 类型 也 不一 单列 0 1 值 
二分 类 问题 一个 样本 只 属于 一类 并且 一共 
只有 两类 单列 连续 值 单 回归 问题 要 预测 
的 值 只有 一个 多列 0 1 值 多分 类 
问题 同样 是 一个 样本 只 属于 一类 但是 一 
共有 多类 多列 连续 值 多 回归 问题 能够 预测 
多个 值 多 标签 多 标签 分类 问题 但是 一个 
样本 可以 属于 多类 评价 指标 对于 多个 机器学习 方法 
我们 必须 找到 一个 评价 指标 来 衡量 它们 的 
好坏 比如 一个二元 分类 的 问题 我们 一般 选用 AUC 
ROC 或者 仅仅 用 AUC 曲线 下面 的 面积 来 
衡量 在 多 标签 和 多分 类 问题 上 我们 
选择 交叉 熵 或 对数 损失 函数 在 回归 问题 
上 我们 选择 常用 的 均方 误差 MSE Python 库 
在 安装 机器 学习 的 几个 库 之前 应该 安装 
两个 基础 库 numpy 和 scipy Pandas 处理 数据 最 
强大 的 库 scikit learn 涵盖 机器学习 几乎 所有 方法 
的 库 xgboost 优化 了 传统 的 梯度 提升 算法 
keras 神经网络 matplotlib 用来 作图 的 库 tpdm 显示 过程 
机器学习 框架 2015年 我 想出 了 一个 自动式 机器学习 的 
框架 直到 今天 还 在 开发 阶段 但是 不久就 会 
发布 本文 就是 以 这个 框架 作为 基础 的 下图 
展示 了 这个 框架 上面 展示 的 这个 框架 里面 
粉红色 的 线 就是 一些 通用 的 步骤 在 处理 
完 数据 并 把 数据 转为 数据 框 格式 后 
我们 就 可以 进行 机器学习 过程 了 确定 问题 确定 
要 研究 的 问题 也 就是 通过 观察 标签 的 
类别 确定 究竟 是 分类 还是 回归 问题 划分 样本 
第二步 是 将 所有 的 样本 划分 为 训练 集 
training data 和 验证 集 validation data 过程 如下 划分 
样本 的 这一 过程 必须 要 根据 标签 来做 比如 
对于 一个 类别 不 平衡 的 分类 问题 必须 要 
用 分层抽样 的 方法 比如 每种 标签 抽 多少 这样 
才能 保证 抽 出来 的 两个 样本 子集 分布 类似 
在 Python 中 我们 可以 用 scikit learn 轻松 实现 
对于 回归 问题 那么 一个 简单 的 K 折 划分 
就 足够 了 但是/c 仍然/d 有/v 一些/m 复杂/a 的/uj 方法/n 
可以/c 使得/v 验证/v 集/q 和/c 训练/vn 集/q 标签/n 的/uj 分布/v 
接近/v 这个 问题 留给 读者 作为 练习 上面 我 用了 
样本 全集 中的 10% 作为 验证 集 的 规模 当然 
你 可以 根据 你 的 样本 量 做 相应 的 
调整 划分 完 样本 以后 我们 就 把 这些 数据 
放在 一边 接下来 我们 使用 的 任何 一种 机器 学习 
的 方法 都要 先在 训练 集上 使用 然后 再用 验证 
集 检验 效果 验证/v 集/q 和/c 训练/vn 集/q 永远/d 都/d 
不能/v 掺和/v 在/p 一起/m 这样 才能 得到 有效 的 评价 
得分 否则 将 会 导致 过拟合 的 问题 识别 特征 
一个 数据集 总是 带 有 很多 的 变量 variables 或者 
称之 为特征 features 他们 对 应着 数据 框 的 维度 
一般 特征 的 值 有三种 类型 数值 变量 属性 变量 
和 文字 变量 我们 用 经典 的 泰坦尼克号 数据集 来 
示例 在 这里 生还 survival 就是 标签 船舱 等级 pclass 
性别 sex 和 登船 港口 embarked 是 属性 变量 而 
像 年龄 age 船上 兄弟姐妹 数量 sibsp 船上 父母 孩子 
数量 parch 是 数值 变量 而 姓名 name 这种 文字 
变量 我们 认为 这 和 生还 与否 没什么 关系 所以 
我们 决定 不 考虑 首先 处理 数值 型 变量 这些 
变量 几乎 不 需要 任何 的 处理 常见 的 方式 
是 正规化 normalization 处理 属性 变量 通常 有 两步 把 
属性 变量 转变为 标签 把 标签 转变为 二元 数值 由于 
泰坦尼克号 数据集 没有 很好 的 文字 变量 来 示范 那么 
我们 就 制定 一个 通用 的 规则 来 处理 文字 
变量 把 所有 的 文字 变量 组合 到一起 然后 用 
某种 算法 来 处理 并 转变 为 数字 我们 可以 
用 CountVectorizer 或者 TfidfVectorizer 来 实现 一般来说 第二 种 方法 
往往 比较 优越 下面 代码 框 中所 展示 的 参数 
长期以来 都 取得 了 良好 的 效果 如果 你 对 
训练 集 数据 采用 了 上述 处理方式 那么 也 要 
保证 对 验证 及 数据 做 相同 处理 特征 融合 
特征 融合 是 指 将 不同 的 特征 融合 要 
区别 对待 密集型 变量 和 稀疏 型 变量 当 我们 
把 特征 融合 好 以后 可以 开始 机器 学习 的 
建模 过程 了 在 这里 我们 都是/nr 选择 以 决策树 
为 基 学习 器 的 集成 算法 主要 有 R 
a n d o m F o r e s 
t C l a s s i f i e 
r R a n d o m F o r 
e s t R e g r e s s 
o r E x t r a T r e 
e s C l a s s i f i 
e r E x t r a T r e 
e s R e g r e s s o 
r X G B C l a s s i 
f i e r X G B R e g 
r e s s o r 但是 不能 直接 把 
没有 经过 规范化 的 数值 变量 直接 用 线性 模型 
拟合 可以 用 scikitlearn 里面 的 规范化 Normalized 和 标准化 
StandardScaler 命令 分别 对 密集 和 稀疏 的 数据 进行 
相应 的 处理 特征 降 维和 特征选择 如果 以 上 
方式 处理 后的/nr 数据 可以 产生 一个 优秀 的 模型 
那就/nr 可以 直接 进行 参数 调整了 如果 不行 则 还要 
继续 进行 特征 降 维和 特征选择 降 维 的 方法 
有 以下 几种 简单 起见 这里 不 考虑 LDA 和 
QDA 对于 高维 数据 来说 PCA 是 常用 的 降 
维 方式 对于 图像 数据 一般 我们 选用 10 ~ 
15组 主 成分 当然 如果 模型 效果 会 提升 的话 
也 可以 选择 更多 的 主 成分 对于 其他 类型 
的 数据 我们 一般 选择 50 ~ 60个 主 成分 
文字 变量 转变为 稀疏 矩阵 后 进行 奇异 值 分解 
奇异 值 分解 对应 scikit learn 库 中的 TruncatedSVD 语句 
一般 在 TF IDF 中 SVD 主 成分 的 数目 
大约在 120 ~ 200 之间 但是 也 可以 采用 更多 
的 成分 但是 相应 的 计算 成本 也 会 增加 
在 特征 降 维 之后 我们 可以 进行 建模 的 
训练 过程 了 但是 有的 时候 如果 这样 降 维 
后的/nr 结果 仍 不 理想 可以 进行 特征选择 特征选择 也 
有 很多 实用 的 方法 比如说 常用 的 向前 或 
向后 搜索 那 就是 一个 接 一个 地 把 特征 
加入 模型 训练 如果 加入 一个 新 的 特征 后 
模型 效果 不好 那就 不 加入 这 一 特征 直到 
选出 最好 的 特征 子集 对于 这种 方法 有 一个 
提升 的 方式 是 用 AUC 作为 评价 指标 当然 
这个 提升 也 不是 尽善尽美 的 还是 需要 实际 应用 
进行 改善 和 调整 的 还有 一种 特征选择 的 方式 
是 在 建模 的 过程 中 就 得到 了 最佳 
特征 子集 比如 我们 可以 观察 logit 模型 的 系数 
或者 拟合 一个 随机 森林 模型 从而 直接 把 这些 
甄选 后的/nr 特征 用在 其它 模型 中 在 上面 的 
处理 中 应该 选择 一个 小 的 estimator 数目 这样 
不会 导致 过拟合 还 可以 用 梯度 提升 算法 来 
进行 特征选择 在 这里 我们 建议 用 xgboost 的 库 
而不是 sklearn 库 里面 的 梯度 提升 算法 因为 前者 
速度快 且 有着 更好 的 延展性 对于 稀疏 的 数据 
集 我们 可以 用 随机 森林 xgboost 或 卡方 等 
方式 来 进行 特征选择 下面 的 例子 中 我们 用了 
卡方 的 方法 选择 了 20个 特征 出来 当然 这个 
参数值 20 也 是 可以 进一步 优化 的 同样 以上 
我们 用 的 所有 方法 都要/nr 记录 储存 用以 交叉 
验证 模型 选择 和 参数 调整 一般而言 常用 的 机器学习 
模型 有 以下 几种 我们 将 在 这些 模型 中 
选择 最好 的 模型 分类 问题 随机 森林 梯度 提升 
算法 GBM Logistic 回归 朴素 贝叶斯 分类器 支持 向量 机 
k 临近 分类器 回归 问题 随机 森林 梯度 提升 算法 
GBM 线性 回归 岭回归 Lasso 支持 向量 回归 下 表中 
展示 了 每种 模型 分别 需要 优化 的 参数 这 
其中 包含 的 问题 太多 太多 了 究竟 参数 取 
什么 值 才 最优 很多 人 往往 有 经验 但是 
不会 甘愿 把 这些 秘密 分享 给 别人 但是 在 
这里 我 会把 我 的 经验 跟 大家 分享 RS 
* 是 指 没有 一个 确切 的 值 提供 给 
大家 在我看来 上面 的 这些 模型 基本 会 完爆 其他 
的 模型 当然 这 只是 我 的 一家之言 下面 是 
上述 过程 的 一个 总结 主要 是 强调 一下 要 
保留 训练 的 结果 用 来给 验证 集 验证 而 
不是 重新 用 验证 集 训练 在 我 长 时间 
的 实践 过程 中 我 发现 这些 总结 出来 的 
规则 和 框架 还是 很 有用 的 当然 在 一些 
极其 复杂 的 工作 中 这些 方法 还是 力 有不 
逮 生活 从来 不会 完美 我们 只能 尽 自身 所 
能去 优化 机器学习 也是 一样 原文 作者 Abhishek Thakur 原文 
链接 Approaching Almost Any Machine Learning Problem 译者 Cup 