机器学习 实战 这 本书 是 基于 python 的 如果 我们 
想 要 完成 python 开发 那么 python 的 开发 环境 
必不可少 1 python3 . 52 64位 这是 我 用 的 
python 版本 2 numpy 1 . 11.3 64位 这是 python 
的 科学计算 包 是 python 的 一个 矩阵 类型 包含 
数组 和 矩阵 提供 了 大量 的 矩阵 处理函数 使 
运算 更加 容易 执行 更加 迅速 3 matplotlib 1 . 
5.3 64位 在 下载 该 工具 时 一定 要 对应 
好 python 的 版本 处理器 版本 matplotlib 可以 认为 是 
python 的 一个 可视化 工具 好了 如果 你 已经 完成 
了 上述 的 环境 配置 下面 就 可以 开始 完成 
真正 的 算法 实 战了 一 k 近邻 算法 的 
工作 原理 存在 一个 样本 数据集 也 称作 训练 数据集 
并且 样本 集中 每个 数据 都 存在 标签 即 我们 
知道 样本 集中 每个 数据 与 所属 分类 的 对应 
关系 当 输入 没有 标签 的 新 数据 后 将 
新 数据 的 每个 特征 与 样本 集中 数据 对应 
的 特征 进行 比较 然后 算法 提取 样本 集中 特征 
最 相似 的 数据 的 分类 标签 一般 来水 我们 
只 选择 样本数据 集中 最 相似 的 k 个 数据 
通常 k 不大于 20 再 根据 多数 表决 原则 选择 
k 个 最 相似 数据 中 出现 次数 最多 的 
分类 作为 新 数据 的 分类 k 近邻 算法 的 
一般 流程 1 收集 数据 可以 采用 公开 的 数据源 
2 准备 数据 计算 距离 所 需要 的 数值 3 
分析 数据 剔除 垃圾 信息 4 测试 算法 计算 错误率 
5 使用 算法 运用 在 实际 中 对 实际 情况 
进行 预测 二 算法 具体 实施 过程 1 使用 python 
导入 数据 代码 解析 如下 # 1 准备 数据 # 
可以 采用 公开 的 数据 集 也 可以 利用 网络爬虫 
从 网站 上 抽取 数据 方式 不限 # 2 准备 
数据 # 确保 数据格式 符合要求 # 导入 科学计算 包 数组 
和 矩阵 from numpy import * from os import listdir 
# 导入 运算符 模块 import operator # 创建 符合 python 
格式 的 数据集 def createDataSet # 数据集 list 列表 形式 
group = array 1.0 1.1 1.0 1.0 0 0 0 
0.1 # 标签 labels = A A B B return 
group labels 2 我们 可以 使用 matplotlib 对 数据 进行 
分析 在 python 命令 环境 中 输入 如 下命令 当 
输入 如 下命令 时 # 导入 制图 工具 import matplotlib 
import matplotlib . pyplot as plt fig = plt . 
figure ax = fig . add _ subplot 111 ax 
. scatter datingDataMat 1 datingDataMat 2 plt . show 从 
上面 可以 看到 由于 没有 使用 样本 分类 的 特征值 
我们 很难 看到 比较 有用 的 数据 模式 信息 一般而言 
我们 会 采用 色彩 或 其他 几号 来 标记 不同 
样本 的 分类 以便 更好 的 理解 数据 重新 输入 
命令 # 导入 制图 工具 import matplotlib import matplotlib . 
pyplot as plt fig = plt . figure ax = 
fig . add _ subplot 111 # 记得 导入 array 
函数 from numpy import array # 色彩 不等 尺寸 不同 
ax . scatter datingDataMat 1 datingDataMat 2 15.0 * array 
datingLabels 15.0 * array datingLabels plt . show 3 实施 
kNN 算法 k 近邻 算法 对 未知 类别 属性 的 
数据 集中 每个 点 依次 执行 如下 步骤 1 计算 
已知 类别 数据 集中 的 点 与 当前 点 之间 
的 距离 2 按照 距离 递增 次序 排序 3 选取 
与 当前 点 距离 最小 的 k 个 点 4 
确定 前 k 个 点 所在 类别 的 出现 频率 
5 返回 前 k 个 点 出现 频率 最高 的 
类别 作为 当前 点 的 预测 分类 具体 代码 解析 
如下 # 构建 分类器 # KNN 算法 实施 # @ 
inX 测试 样本数据 # @ dataSet 训练 样本数据 # @ 
labels 测试 样本 标签 # @ k 选取 距离 最近 
的 k 个 点 def classify0 inX dataSet labels k 
# 获取 训练 数据集 的 行数 dataSetSize = dataSet . 
shape 0 # 欧氏距离 计算 # 各个 函数 均 是以 
矩阵 形式 保存 # tile inX 沿 各个 维度 的 
复制 次数 diffMat = tile inX dataSetSize 1 dataSet sqDiffMat 
= diffMat * * 2 # . sum 运行 加 
函数 参数 axis = 1 表示 矩阵 每 一行 的 
各个 值 相加 和 sqDistances = sqDiffMat . sum axis 
= 1 distances = sqDistances * * 0.5 # # 
获取 排序 有 小到 大 后的/nr 距离 值 的 索引 
序号 s o r t e d D i s 
t I n d i c i e s = 
distances . argsort # 字典 键值 对 结构 类似 于 
hash 表 classCount = { } for i in range 
k # 获取 该 索引 对应 的 训练 样本 的 
标签 voteIlabel = labels s o r t e d 
D i s t I n d i c i 
e s i # 累加 几类 标签 出现 的 次数 
构成 键值 对 key / values 并存 于 classCount 中 
classCount voteIlabel = classCount . get voteIlabel 0 + 1 
# 将 字典 列表 中 按照 第二列 也 就是 次数 
标签 反序 排序 由 大 到 小 排序 sortedClassCount = 
sorted classCount . items key = operator . itemgetter 1 
reverse = True # 返回 第一 个 元素 最 高频率 
标签 key return sortedClassCount 0 0 3 测试 分类器 下面 
以 两个 实例 对 分类器 效果 进行 测试 实例 1 
使用 kNN 改进 某 约会 网站 的 配对 效果 # 
knn 算法 实例 # 约会 网站 配对 # 1 将 
text 文本 数据 转化 为 分类器 可以 接受 的 格式 
def file2matrix filename # 打开 文件 fr = open filename 
# 读取 文件 每 一行 到 array0Lines 列表 # read 
读取 整个 文件 通常 将 文件 内容 放到 一个 字符串 
中 # readline 每次 读取 文件 一行 当 没有 足够 
内存 一次 读取 整个 文件 内容 时 使用 该 方法 
# readlines 读取 文件 的 每 一行 组成 一个 字符串 
列表 内存 足够 时 使用 array0Lines = fr . readlines 
# 获取 字符串 列表 行数 行数 numberOfLines = len array0Lines 
# 返回 的 特征 矩阵 大小 returnMat = zeros numberOfLines 
3 # list 存储 类 标签 classLabelVector = index = 
0 for line in array0Lines # 去掉 字符串 头尾 的 
空格 类似于 Java 的 trim line = line . strip 
# 将 整行 元素 按照 tab 分割 成 一个 元素 
列表 listFromLine = line . split \ t # 将 
listFromLine 的 前 三个 元素 依次 存入 returnmat 的 index 
行 的 三列 returnMat index = listFromLine 0 3 # 
python 可以 使用 负 索引 1 表示 列表 的 最后 
一 列 元素 从而 将 标签 存入 标签 向量 中 
# 使用 append 函数 每次 循环 在 list 尾部 添加 
一个 标签 值 classLabelVector . append int listFromLine 1 index 
+ = 1 return returnMat classLabelVector # 2 准备 数据 
归一化 # 计算 欧式 距离 时 如果 某一 特征 数值 
相对于 其他 特征 数值 较大 那么 该 特征 对于 结果 
影响 要 # 远 大于 其他 特征 然后 假设 特征 
都是/nr 同等 重要 即 等 权重 的 那么 可能 某一 
特征 对于 结果 存 # 在 严重 影响 def autoNorm 
dataSet # 找出 每 一列 的 最小值 minVals = dataSet 
. min 0 # 找出 每 一列 的 最大值 maxVals 
= dataSet . max 0 ranges = maxVals minVals # 
创建 与 dataSet 等 大小 的 归一化 矩阵 # shape 
获取 矩阵 的 大小 normDataSet = zeros shape dataSet # 
获取 dataSet 第一 维度 的 大小 m = dataSet . 
shape 0 # 将 dataSet 的 每 一行 的 对应 
列 减去 minVals 中 对 应列 的 最小值 normDataSet = 
dataSet tile minVals m 1 # 归一化 公式 newValue = 
value minvalue / maxVal minVal normDataSet = normDataSet / tile 
ranges m 1 return normDataSet ranges minVals # 3 测试 
算法 # 改变 测试 样本 占 比 k 值 等 
都会 对 最后 的 错误 率 产生影响 def datingClassTest # 
设定 用来 测试 的 样本 占 比 hoRatio = 0.10 
# 从 文本 中 提取 得到 数据 特征 及 对应 
的 标签 datingDataMat datingLabels = file2matrix datingTestSet2 . txt # 
对 数据 特征 进行 归一化 normMat ranges minVals = autoNorm 
datingDataMat # 得到 第一 维度 的 大小 m = normMat 
. shape 0 # 测试 样本 数量 numTestVecs = int 
hoRatio * m # 错误 数 初始化 errorCount = 0.0 
for i in range numTestVecs # 利用 分类 函数 classify0 
获取 测试 样本数据 分类 结果 classifierResult = classify0 normMat i 
normMat numTestVecs m \ datingLabels numTestVecs m 3 # 打印 
预测 结果 和 实际 标签 print the classifier came back 
with % d the real answer is % d \ 
% classifierResult datingLabels i # 如果 预测 输出 不 等于 
实际 标签 错误 数 增加 1.0 if classifierResult = datingLabels 
i errorCount + = 1.0 # 打印 最后 的 误差率 
print the total error rate is % f % errorCount 
/ float numTestVecs # 4 构建 可 手动 输入系统 # 
用户 输入 相关 数据 进行 预测 def classifyPerson # 定义 
预测 结果 resultList = not at all in small does 
in large does # 在 python3 . x 中 已经 
删除 raw _ input 取而代之 的 是 input percentTats = 
float input \ percentage of time spent playing video games 
ffMiles = float input frequent filer miles earned per year 
iceCream = float input liters of ice cream consumed per 
year datingDataMat datingLabels = file2matrix datingTestSet2 . txt normMat ranges 
minValues = autoNorm datingDataMat # 将 输入 的 数值 放在 
数组 中 inArr = array ffMiles percentTats iceCream classifierResult = 
classify0 inArr minValues / ranges normMat datingLabels 3 print you 
will probably like this person resultList classifierResult 1 实验 结果 
当然 用户 也 可以 自己 手动输入 进行 预测 实例 2 
手写 识别系统 # knn 算法 实例 # 手写 识别系统 # 
1 将 图像 转化 为 测试 向量 # 图像 大小 
32 * 32 转化 为 1024 的 向量 def img2vector 
filename returnVec = zeros 1 1024 fr = open filename 
for i in range 32 # 每次 读取 一行 lineStr 
= fr . readline for j in range 32 # 
通 俗讲 就是 根据 首 地址 位置 的 偏移量 计算 
出 当前 数据 存放 的 地址 位置 returnVec 0 32 
* i + j = int lineStr j return returnVec 
# 2 测试代码 def h a n d w r 
i t i n g C l a s s 
T e s t hwLabels = # 列出 给定 目录 
的 文件名 列表 使用 前 需 导入 from os import 
listdir trainingFileList = listdir knn / trainingDigits # 获取 列表 
的 长度 m = len trainingFileList # 创建 一个 m 
* 1024 的 矩阵 用于 存储 训练 数据 trainingMat = 
zeros m 1024 for i in range m # 获取 
当 前行 的 字符串 fileNameStr = trainingFileList i # 将 
字符串 按照 . 分开 并将 前 一部分 放于 fileStr fileStr 
= fileNameStr . split . 0 # 将 fileStr 按照 
_ 分开 并将 前 一部分 存于 classNumStr classNumStr = int 
fileStr . split _ 0 # 将 每个 标签 值 
全部 存入 一个 列表 中 hwLabels . append classNumStr # 
解析 目录 中 的 每一个 文件 将 图像 转化 为 
向量 最后 存入 训练 矩阵 中 trainingMat i = img2vector 
knn / trainingDigits / % s % fileNameStr # 读取 
测试数据 目录 中 的 文件 列表 testFileList = listdir knn 
/ testDigits errorCount = 0.0 mTest = len testFileList for 
i in range mTest # 获取 第 i 行 的 
文件名 fileNameStr = testFileList i # 将 字符串 按照 . 
分开 并将 前 一部分 放于 fileStr fileStr = fileNameStr . 
split . 0 # 将 fileStr 按照 _ 分开 并将 
前 一部分 存于 classNumStr classNumStr = int fileStr . split 
_ 0 # 解析 目录 中 的 每一个 文件 将 
图像 转化 为 向量 vectorUnderTest = img2vector knn / testDigits 
/ % s % fileNameStr # 分类 预测 classifierResult = 
classify0 vectorUnderTest trainingMat hwLabels 3 # 打印 预测 结果 和 
实际 结果 print the classifierResult came back with % d 
the real answer is % d % classifierResult classNumStr # 
预测 错误 错误 数 加 1次 if classifierResult = classNumStr 
errorCount + = 1.0 # 打印 错误 数 和 错误率 
print \ nthe total number of errors is % d 
% errorCount print \ nthe total error rate is % 
f % errorCount / float mTest 实验 结果 错误率 三 
算法 小结 1 如果 我们 改变 训练样本 的 数目 调整 
相应 的 k 值 都会 对 最后 的 预测 错误率 
产生影响 我们 可以 根据 错误率 的 情况 对 这些 变量 
进行 调整 从而 降低 预测 错误率 2 k 近邻 算法 
的 优缺点 k 近邻 算法 具有 精度高 对 异常值 不 
敏感 的 优点 k 近邻 算法 是 基于 实例 的 
学习 使用/v 算法/n 时/n 我们/r 必须/d 有/v 接近/v 实际/n 数据/n 
的/uj 训练/vn 样本数据/i k 近邻 算法 必须 保存 全部 数据集 
如果 训练 数据集 很大 必须 使用 大量 的 存储空间 此外 
由于 必须 对 数据 集中 的 每个 数据 计算 距离 
实际 使用 时也/nr 可能 会 非常 耗时 此外 k 近邻 
算法 无法 给出 数据 的 基础 结构 信息 因此 我们 
无法 知道 平均 实例 样本 和 典型 实例 样本 具有 
怎样 的 特征 