注 正则化 是 用来 防止 过拟合 的 方法 在最 开始 
学习 机器 学习 的 课程 时 只是 觉得 这个 方法 
就像 某种 魔法 一样 非常 神奇 的 改变 了 模型 
的 参数 但是 一直 也 无法 对 其 基本 原理 
有一个 透彻 直观 的 理解 直到 最近 再次 接触 到 
这个 概念 经过 一番 苦思冥想 后 终于 有了 我 自己 
的 理解 0 . 正则化 Regularization   前面 使用 多项式 
回归 如果 多项式 最 高次 项 比较 大 模型 就 
容易 出现 过 拟合 正则化 是 一种 常见 的 防止 
过拟合 的 方法 一般 原理 是 在 代价 函数 后面 
加上 一个 对 参数 的 约束 项 这个 约束 项被/nr 
叫做 正则化 项 regularizer 在 线性 回归模型 中 通常 有 
两种 不同 的 正则化 项 加上 所有 参数 不包括 $ 
\ theta _ 0 $ 的 绝对值 之和 即 $ 
l1 $ 范数 此时 叫做 Lasso 回归 加上 所有 参数 
不包括 $ \ theta _ 0 $ 的 平方和 即 
$ l2 $ 范数 此时 叫做 岭回归 . 看过 不少 
关于 正则化 原理 的 解释 但是 都 没有 获得 一个 
比较 直观 的 理解 下面 用 代价 函数 的 图像 
以及 正则化 项的/nr 图像 来 帮助 解释 正则化 之所以 起 
作用 的 原因 0.1 代价 函数 的 图像 为了 可视化 
选择 直线 方程 进行 优化 假设 一个 直线 方程 以及 
代价 函数 如下 $ \ hat { h } _ 
{ \ theta } = \ theta _ 0 + 
\ theta _ 1 x $ 该 方程 只有 一个 
特征 $ x $ 两个 参数 $ \ theta _ 
0 $ 和$\/nr theta _ 1 $ $ J \ 
theta = \ frac { 1 } { m } 
\ sum _ { i = 1 } ^ { 
m } { \ theta _ 0 + \ theta 
_ 1 x ^ { i } y ^ { 
i } ^ 2 } $ 该 代价 函数 为 
均方 误差函数 MSE 其中 $ m $ 表示 样本量 . 
为了 保持 简单 只取 一个 样本点 $ 1 1 $ 
代入 上面 的 代价 函数 方程 中 可得 $ J 
\ theta = \ theta _ 0 + \ theta 
_ 1 1 ^ 2 $ . 该 式 是 
一个 二元 一次方程 可以 在 3 维空间 中 作图 下面 
利用 网站 GeoGebra 画出 该 方程 的 图像 1 代入 
样本点 $ 1 1 $ 后的/nr 代价 函数 MSE 的 
图像 由于 多个 样本点 的 代价 函数 是 所有 样本点 
代价 函数 之和 且 不同 的 样本 点 只是 相当于 
改变 了 代价 函数 中 两个 变量 的 参数 此时 
$ \ theta _ 0 $ 和$\/nr theta _ 1 
$ 是 变量 样本点 的 取值 是 参数 因此 多 
样本 的 代价 函数 MSE 的 图像 只会在 1 上 
发生 缩放 和 平移 而 不会 发生 过 大 的 
形变 对于 坐标轴 表示 如下 使用 $ J $ 轴 
表示 蓝色 轴线 上方 为 正向 使用 $ \ theta 
_ 1 $ 表示 红色 轴线 左边 为 正向 使用 
$ \ theta _ 0 $ 表示 绿色 轴线 指向 
屏幕 外 的 方向 为 正向 . 此时 的 函数 
图像 相当于 一条 抛物线 沿着 平面 $ J = 0 
$ 上 直线 $ \ theta _ 0 = \ 
theta _ 1 $ 平移 后 形成 的 图像 0.2 
正则化 项的/nr 图像 这里 使用 $ L1 $ 范数 作为 
正则化 项 加上 正则化 项 之后 MSE 代价 函数 变成 
$ J \ theta =   \ frac { 1 
} { m } \ sum _ { i = 
1 } ^ { m } { \ theta _ 
0 + \ theta _ 1 x ^ { i 
} y ^ { i } ^ 2 }   
+ \ lambda | | \ theta _ 1 | 
| _ 1 $ 上式 中 $ \ lambda $ 
是 正则化 项的/nr 参数 为了 简化 取 $ \ lambda 
= 1 $ 由于/c 正则化/i 项中/nr 始终/d 不/d 包含/v 截距/v 
项$\/nr theta/w _/i 0/m $/i 此时 的 $ L1 $ 
范数 相当于 参数 $ \ theta _ 1 $ 的 
绝对值 函数 图像 如下 2 $ L1 $ 正则化 项的/nr 
图像 此时 的 函数 图像 相当于 一张 对折 后 半 
张开 的 纸 纸 的 折痕 与 平面 $ J 
= 0 $ 上 $ \ theta _ 0 $ 
轴 重叠 0.3 代价 函数 与 正则化 项 图像 的 
叠加 直接 将 这两个 图像 放在 一起 的 样子 3 
同时 显示 代价 函数 与 正则化 项的/nr 图像 将 两个 
方程 相加 之后 即 $ J \ theta = \ 
theta _ 0 + \ theta _ 1 1 ^ 
2 + | \ theta _ 1 | $ 做 
图 可以 得到 下面 的 图像 4 加入 正则化 项 
之后 代价 函数 的 图像 此时 的 图像 就像 是 
一个 圆锥体 被 捏 扁了 之后 立 在 坐标 原 
点上 观察 添加 正则化 项 前后 的 图像 我们 会 
发现 加上 正则化 项 之后 此时 损失 函数 就 分成 
了 两部分 第 1项 为 原来 的 MSE 函数 第 
2项 为 正则化 项 最终 的 结果 是 这 两部分 
的 线性组合 在 第 1项 的 值 非常 小 但在 
第 2项 的 值 非常大 的 区域 这些 值 会 
受到 正则化 项的/nr 巨大 影响 从而 使得 这些 区域 的 
值 变 的 与 正则化 项 近似 例如 原来 的 
损失 函数 沿 $ \ theta _ 0 = \ 
theta _ 1 $ $ J $ 轴 方 向上 
的 值 始终 为 0 但是 加入 正则化 项$J/nr = 
| \ theta _ 1 | $ 后 该 直线 
上 原来 为 0 的 点 都 变成 了 $ 
\ theta _ 1 $ 的 绝对值 这 就像 加权 
平均值 一样 哪 一项 的 权重 越大 对 最终 结果 
产生 的 影响 也 越大 如果 想象 一种 非常 极端 
的 情况 在 参数 的 整个 定义域 上 第 2项 
的 取值 都 远远 大于 第一项 的 取值 那么 最终 
的 损失 函数 几乎 100% 都 会由 第 2项 决定 
也 就是 整个 代价 函数 的 图像 会 非常 类似 
于$J/nr = | \ theta _ 1 | $ 2 
而 不是 原来 的 MSE 函数 的 图像 1 这时候 
就 相当于 $ \ lambda $ 的 取值 过大 的 
情况 最终 的 全局 最优 解 将会 是 坐标 原点 
这/r 就是/d 为什么/r 在/p 这种/r 情况/n 下/f 最终/d 得到/v 的/uj 
解全/nr 都为/i 0.1/mx ./i  /i 岭回归/i 岭回归/i 与/p 多项式/l 回归/v 
唯一/b 的/uj 不同/a 在于/v 代价/n 函/n 数上/i 的/uj 差别/d 岭回归 
的 代价 函数 如下 $ $ J \ theta = 
\ frac { 1 } { m } \ sum 
_ { i = 1 } ^ { m } 
{ y ^ { i } w x ^ { 
i } + b ^ 2 }   + \ 
lambda | | w | | _ 2 ^ 2 
= MSE \ theta + \ lambda \ sum _ 
{ i = 1 } ^ { n } { 
\ theta _ i ^ 2 } \ \ quad 
\ cdots \ 1 1 $ $ 为了 方便 计算 
导数 通常 也 写成 下面 的 形式 $ $ J 
\ theta = \ frac { 1 } { 2m 
} \ sum _ { i = 1 } ^ 
{ m } { y ^ { i } w 
x ^ { i } + b ^ 2 } 
  + \ frac { \ lambda } { 2 
} | | w | | _ 2 ^ 2 
= \ frac { 1 } { 2 } MSE 
\ theta + \ frac { \ lambda } { 
2 } \ sum _ { i = 1 } 
^ { n } { \ theta _ i ^ 
2 } \ \ quad \ cdots \ 1 2 
$ $ 上式 中的 $ w $ 是 长度 为 
$ n $ 的 向量 不包括 截距 项的/nr 系数 $ 
\ theta _ 0 $ $ \ theta $ 是 
长度 为 $ n + 1 $ 的 向量 包括 
截距 项的/nr 系数 $ \ theta _ 0 $ $ 
m $ 为 样本 数 $ n $ 为特征 数 
. 岭回归 的 代价 函数 仍然 是 一个 凸函数 因此 
可以 利用 梯度 等于 0 的 方式 求得 全局 最优 
解 正规 方程 $ $ \ theta = X ^ 
T X + \ lambda I ^ { 1 } 
X ^ T y $ $ 上述 正规 方程 与 
一般 线性 回归 的 正规 方程 相比 多了 一项 $ 
\ lambda I $ 其中 $ I $ 表示 单位矩阵 
假如 $ X ^ T X $ 是 一个 奇异 
矩阵 不满 秩 添加 这 一项 后 可以 保证 该项 
可逆 由于 单位矩阵 的 形状 是 对角线 上为 1 其他 
地方 都为 0 看起来 像 一条 山岭 因此 而 得名 
除了 上述 正规 方程 之外 还 可以 使用 梯度 下降 
的 方式 求解 求 梯度 的 过程 可以 参考 一般 
线性 回归 3 . 2.2 节 这里 采用 式子 $ 
1 2 $ 来 求导 $ $ \ nabla _ 
{ \ theta } J \ theta = \ frac 
{ 1 } { m } X ^ T \ 
cdot X \ cdot \ theta y     + 
\ lambda w \ \ quad \ cdots \ 1 
3 $ $ 因为 式子 $ 1 2 $ 中 
和式 第二项 不 包含 $ \ theta _ 0 $ 
因此 求 梯度 后 上式 第二项 中的 $ w $ 
本来 也 不 包含 $ \ theta _ 0 $ 
为了 计算 方便 添加 $ \ theta _ 0 = 
0 $ 到 $ w $ . 因此在 梯度 下降 
的 过程 中 参数 的 更新 可以 表示 成 下面 
的 公式 $ $ \ theta = \ theta \ 
frac { \ alpha } { m } X ^ 
T \ cdot X \ cdot \ theta y   
  + \ lambda w   \ \ quad \ 
cdots \ 1 4   $ $ 其中 $ \ 
alpha $ 为 学习率 $ \ lambda $ 为 正则化 
项的/nr 参数 1.1 数据 以及 相关 函数 1 import numpy 
as np 2 import matplotlib . pyplot as plt 3 
from sklearn . preprocessing import P o l y n 
o m i a l F e a t u 
r e s 4 from sklearn . metrics import mean 
_ squared _ error 5 6 data = np . 
array 2.95507616 10.94533252 7 0.44226119 2.96705822 8 2.13294087 6.57336839 9 
1.84990823 5.44244467 10 0.35139795 2.83533936 11 1.77443098 5.6800407 12 1.8657203 
6.34470814 13 1.61526823 4.77833358 14 2.38043687 8.51887713 15 1.40513866 4.18262786 
16 m = data . shape 0 # 样本 大小 
17 X = data 0 . reshape 1 1 # 
将 array 转换成 矩阵 18 y = data 1 . 
reshape 1 1 继续 使用 多项式 回 归中 的 数据 
1.2 岭回归 的 手动 实现 有了/nr 上面 的 理论 基础 
就 可以 自己 实现 岭回归 了 下面 是 Python 代码 
1 # 代价 函数 2 def L _ theta theta 
X _ x0 y lamb 3 4 lamb lambda the 
parameter of regularization 5 theta n + 1 1 matrix 
contains the parameter of x0 = 1 6 X _ 
x0 m n + 1 matrix plus x0 7 8 
h = np . dot X _ x0 theta # 
np . dot 表示 矩阵 乘法 9 theta _ without 
_ t0 = theta 1 10 L _ theta = 
0.5 * mean _ squared _ error h y + 
0.5 * lamb * np . sum np . square 
theta _ without _ t0 11 return L _ theta 
12 13 # 梯度 下降 14 def GD lamb X 
_ x0 theta y alpha 15 16 lamb lambda the 
parameter of regularization 17 alpha learning rate 18 X _ 
x0 m n + 1 plus x0 19 theta n 
+ 1 1 matrix contains the parameter of x0 = 
1 20 21 for i in range T 22 h 
= np . dot X _ x0 theta 23 theta 
_ with _ t0 _ 0 = np . r 
_ np . zeros 1 1 theta 1 # set 
theta 0 = 0 24 theta = alpha * 1 
/ m * np . dot X _ x0 . 
T h y + lamb * theta _ with _ 
t0 _ 0 # add the gradient of regularization term 
25 if i % 50000 = = 0 26 print 
L _ theta theta X _ x0 y lamb 27 
return theta 28 29 T = 1200000 # 迭代 次数 
30 degree = 11 31 theta = np . ones 
degree + 1 1 # 参数 的 初始化 degree = 
11 一个 12个 参数 32 alpha = 0.0000000006 # 学习率 
33 # alpha = 0.003 # 学习率 34 lamb = 
0.0001 35 # lamb = 0 36 poly _ features 
_ d = P o l y n o m 
i a l F e a t u r e 
s degree = degree include _ bias = False 37 
X _ poly _ d = poly _ features _ 
d . fit _ transform X 38 X _ x0 
= np . c _ np . ones m 1 
X _ poly _ d # ADD X0 = 1 
to each instance 39 theta = GD lamb = lamb 
X _ x0 = X _ x0 theta = theta 
y = y alpha = alpha 上面 第 10行 对应 
公式 $ 1 2 $ 第 24行 对应 公式 $ 
1 3 $ 由于 自由 度 比较 大 此时 利用 
梯度 下降 的 方法 训练 模型 比较 困难 学习率 稍微 
大 一点 就 会 出现 出现 损失 函数 的 值 
越过 最低点 不断 增长 的 情况 下面 是 训练 结束 
后的/nr 参数 以及 代价 函数值 1.00078848 e + 00 1.03862735 
e 05 3.85144400 e 05 3.77233288 e 05 1.28959318 e 
04 1.42449160 e 04 4.42760996 e 04 5.11518471 e 04 
1.42533716 e 03 1.40265037 e 03 3.13638870 e 03 1.21862016 
e 03 3.59934190413 从 上面 的 结果 看 截距 项的/nr 
参数 最大 高阶/nr 项的/nr 参数/n 都/d 比较/d 小/a 下面 是 
比较 原始 数据 和 训练 出来 的 模型 之间 的 
关系 1 X _ plot = np . linspace 2.99 
1.9 1000 . reshape 1 1 2 poly _ features 
_ d _ with _ bias = P o l 
y n o m i a l F e a 
t u r e s degree = degree include _ 
bias = True 3 X _ plot _ poly = 
poly _ features _ d _ with _ bias . 
fit _ transform X _ plot 4 y _ plot 
= np . dot X _ plot _ poly theta 
5 plt . plot X _ plot y _ plot 
r 6 plt . plot X y b . 7 
plt . xlabel x 8 plt . ylabel y 9 
plt . show 1 手动 实现 岭回归 的 效果图 中 
模型 与 原始 数据 的 匹配度 不是太好 但是 过拟合 的 
情况 极大 的 改善 了 模型 变 的 更 简单 
了 1.2 正规 方程 下面 使用 正规 方程 求解 其中 
$ \ lambda = 10 $ 1 theta2 = np 
. linalg . inv np . dot X _ x0 
. T X _ x0 + 10 * np . 
identity X _ x0 . shape 1 . dot X 
_ x0 . T . dot y 2 print theta2 
3 print L _ theta theta2 X _ x0 y 
lamb 4 5 X _ plot = np . linspace 
3 2 1000 . reshape 1 1 6 poly _ 
features _ d _ with _ bias = P o 
l y n o m i a l F e 
a t u r e s degree = degree include 
_ bias = True 7 X _ plot _ poly 
= poly _ features _ d _ with _ bias 
. fit _ transform X _ plot 8 y _ 
plot = np . dot X _ plot _ poly 
theta2 9 plt . plot X _ plot y _ 
plot r 10 plt . plot X y b . 
11 plt . xlabel x 12 plt . ylabel y 
13 plt . show 参数 即 代价 函数 的 值 
0.56502653 0.12459546 0.26772443 0.15642405 0.29249514 0.10084392 0.22791769 0.1648667 0.05686718 0.03906615 
0.00111673 0.00101724 0.604428719639 从 参数 来看 截距 项的/nr 系数 减小 
了 1 7/m 阶/n 都/d 有比/i 较大/a 的/uj 参数/n 都/d 
比较/d 大/a 后面 更 高阶 项的/nr 参数 越来越 小 下面 
是 函数 图像 2 使用 正规 方程 求解 从 图中 
可以 看到 虽然 模型 的 自由度 没变 还是 11 但是 
过拟合 的 程度 得到 了 改善 1.3 使用 scikit learnscikit 
learn 中有 专门 计算 岭回归 的 函数 而且 效果 要比 
上面 的 方法 好 使用 scikit learn 中的 岭回归 只 
需要 输入 以下 参数 alpha 上面 公式 中的 $ \ 
lambda $ 正则化 项的/nr 系数 solver 求解 方法 X 训练样本 
y 训练样本 的 标签 . 1 from sklearn . linear 
_ model import Ridge 2 3 # 代价 函数 4 
def L _ theta _ new intercept coef X y 
lamb 5 6 lamb lambda the parameter of regularization 7 
theta n + 1 1 matrix contains the parameter of 
x0 = 1 8 X _ x0 m n + 
1 matrix plus x0 9 10 h = np . 
dot X coef + intercept # np . dot 表示 
矩阵 乘法 11 L _ theta = 0.5 * mean 
_ squared _ error h y + 0.5 * lamb 
* np . sum np . square coef 12 return 
L _ theta 13 14 lamb = 10 15 ridge 
_ reg = Ridge alpha = lamb solver = cholesky 
16 ridge _ reg . fit X _ poly _ 
d y 17 print ridge _ reg . intercept _ 
ridge _ reg . coef _ 18 print L _ 
theta _ new intercept = ridge _ reg . intercept 
_ coef = ridge _ reg . coef _ . 
T X = X _ poly _ d y = 
y lamb = lamb 19 20 X _ plot = 
np . linspace 3 2 1000 . reshape 1 1 
21 X _ plot _ poly = poly _ features 
_ d . fit _ transform X _ plot 22 
h = np . dot X _ plot _ poly 
ridge _ reg . coef _ . T + ridge 
_ reg . intercept _ 23 plt . plot X 
_ plot h r 24 plt . plot X y 
b . 25 plt . show 训练 结束 后 得到 
的 参数 为 分别 表示 截距 特征 的 系数 代价 
函数 的 值 3.03698398 2.95619849 e 02 6.09137803 e 02 
4.93919290 e 02 1.10593684 e 01 4.65660197 e 02 1.06387336 
e 01 5.14340826 e 02 2.29460359 e 02 1.12705709 e 
02 1.73925386 e 05 2.79198986 e 04 0.213877232488 3 使用 
scikit learn 训练 岭回归 经过 与 前面 两种 方法 得到 
的 结果 比较 这里 得到 的 曲线 更加 平滑 不仅 
降低 了 过拟合 的 风险 代价 函数 的 值 也 
非常 低 2 . Lasso 回归 Lasso 回 归于 岭回归 
非常 相似 它们 的 差别 在于 使用 了 不同 的 
正则化 项 最终 都 实现 了 约束 参数 从而 防止 
过拟合 的 效果 但是 Lasso 之所以 重要 还有 另一个 原因 
是 Lasso 能够 将 一些 作用 比较 小 的 特征 
的 参数 训练 为 0 从而 获得 稀 疏解 也 
就是说 用 这种 方法 在 训练 模型 的 过程 中 
实现 了 降 维 特征 筛选 的 目的 Lasso 回归 
的 代价 函数 为 $ $ J \ theta = 
\ frac { 1 } { 2m } \ sum 
_ { i = 1 } ^ { m } 
{ y ^ { i } w x ^ { 
i } + b ^ 2 }   + \ 
lambda | | w | | _ 1 = \ 
frac { 1 } { 2 } MSE \ theta 
+ \ lambda \ sum _ { i = 1 
} ^ { n } { | \ theta _ 
i | } \ \ quad \ cdots \ 2 
1 $ $ 上式 中的 $ w $ 是 长度 
为 $ n $ 的 向量 不包括 截距 项的/nr 系数 
$ θ _ 0 $ $ θ $ 是 长度 
为 $ n + 1 $ 的 向量 包括 截距 
项的/nr 系数 $ θ _ 0 $ $ m $ 
为 样本 数 $ n $ 为特征 数 . $ 
| | w | | _ 1 $ 表示 参数 
$ w $ 的 $ l1 $ 范数 也 是 
一种 表示 距离 的 函数 加入 $ w $ 表示 
3 维空间 中 的 一个 点 $ x y z 
$ 那么 $ | | w | | _ 1 
= | x | + | y | + | 
z | $ 即 各个 方向 上 的 绝对值 长度 
之和 式子 $ 2 1 $ 的 梯度 为 $ 
$ \ nabla _ { \ theta } MSE \ 
theta + \ lambda \ begin { pmatrix } sign 
\ theta _ 1 \ \   sign \ theta 
_ 2 \ \ \ vdots \ \ sign \ 
theta _ n \ end { pmatrix } \ quad 
\ cdots \ 2 2 $ $ 其中 $ sign 
\ theta _ i $ 由 $ \ theta _ 
i $ 的 符号 决定 $ \ theta _ i 
0 sign \ theta _ i = 1 \ \ 
theta _ i = 0 sign \ theta _ i 
= 0 \ \ theta _ i 0 sign \ 
theta _ i = 1 $ . 2.1 Lasso 的 
实现 直接 使用 scikit learn 中的 函数 可以 参考 官方 
文档 http / / scikit learn . org / stable 
/ modules / generated / sklearn . linear _ model 
. Lasso . html 下面 模型 中 的 参数 alpha 
就是 公式 2 1 中的 参数 $ \ lambda $ 
是 正则化 项的/nr 系数 可以 取 大于 0 的 任意 
值 alpha 的 值 越大 对模型 中 参数 的 惩罚 
力度 越大 因此会 有 更多 的 参数 被 训练 为 
0 只对 线性 相关 的 参数 起作用 模型 也 就 
变得 更加 简单 了 1 from sklearn . linear _ 
model import Lasso 2 3 lamb = 0.025 4 lasso 
_ reg = Lasso alpha = lamb 5 lasso _ 
reg . fit X _ poly _ d y 6 
print lasso _ reg . intercept _ lasso _ reg 
. coef _ 7 print L _ theta _ new 
intercept = lasso _ reg . intercept _ coef = 
lasso _ reg . coef _ . T X = 
X _ poly _ d y = y lamb = 
lamb 8 9 X _ plot = np . linspace 
3 2 1000 . reshape 1 1 10 X _ 
plot _ poly = poly _ features _ d . 
fit _ transform X _ plot 11 h = np 
. dot X _ plot _ poly lasso _ reg 
. coef _ . T + lasso _ reg . 
intercept _ 12 plt . plot X _ plot h 
r 13 plt . plot X y b . 14 
plt . show 最终 获得 的 参数 以及 代价 函数 
的 值 为 其中 计算 代价 函数值 的 函数 L 
_ theta _ new 需要 修改 其中 的 L _ 
theta 为 L _ theta = 0.5 * mean _ 
squared _ error h y + lamb * np . 
sum np . abs coef 2.86435179 0.00000000 e + 00 
5.29099723 e 01 3.61182017 e 02 9.75614738 e 02 1.61971116 
e 03 3.42711766 e 03 2.78782527 e 04 1.63421713 e 
04 5.64291215 e 06 1.38933655 e 05 1.02036898 e 06 
0.0451291096773 从 结果 可以 看到 截距 项的值/nr 最大 一次 项的/nr 
系数 为 0 二次/m 项的/nr 系数/n 是/v 剩下/v 的/uj 所有/b 
项/n 中值/n 最大/a 的/uj 也 比较 符合 数据 的 真实 
来源 这里 也 可以 看 出来 更/d 高阶/nr 的/uj 项/n 
虽然/c 系数/n 都/d 非常/d 小/a 但/c 不为/c 0/m 这 是因为 
这些 项 之间 的 关系 是 非线性 的 无法 用 
线性组合 互相 表示 1 Lasso 回归 得到 的 图像 1 
是 目前 在 $ degree = 11 $ 的 情况 
下 得到 的 最好 模型 3 . 弹性 网络   
Elastic Net 弹性 网络 是 结合 了 岭回归 和 Lasso 
回归 由 两者 加权平均 所得 据 介绍 这种 方法 在 
特征 数 大于 训练 集 样本数 或 有些 特征 之间 
高度 相关 时比/nr Lasso 更加 稳定 其 代价 函数 为 
$ $ J \ theta = \ frac { 1 
} { 2 } MSE \ theta + r \ 
lambda \ sum _ { i = 1 } ^ 
{ n } { | \ theta _ i | 
} + \ frac { 1 r } { 2 
} \ lambda \ sum _ { i = 1 
} ^ { n } { \ theta _ i 
^ 2 } \ \ quad \ cdots \ 3 
1 $ $ 其中 $ r $ 表示 $ l1 
$ 所占 的 比例 使用 scikit learn 的 实现 1 
from sklearn . linear _ model import ElasticNet 2 3 
# 代价 函数 4 def L _ theta _ ee 
intercept coef X y lamb r 5 6 lamb lambda 
the parameter of regularization 7 theta n + 1 1 
matrix contains the parameter of x0 = 1 8 X 
_ x0 m n + 1 matrix plus x0 9 
10 h = np . dot X coef + intercept 
# np . dot 表示 矩阵 乘法 11 L _ 
theta = 0.5 * mean _ squared _ error h 
y + r * lamb * np . sum np 
. abs coef + 0.5 * 1 r * lamb 
* np . sum np . square coef 12 return 
L _ theta 13 14 elastic _ net = ElasticNet 
alpha = 0.5 l1 _ ratio = 0.8 15 elastic 
_ net . fit X _ poly _ d y 
16 print elastic _ net . intercept _ elastic _ 
net . coef _ 17 print L _ theta _ 
ee intercept = elastic _ net . intercept _ coef 
= elastic _ net . coef _ . T X 
= X _ poly _ d y = y lamb 
= 0.1 r = 0.8 18 19 X _ plot 
= np . linspace 3 2 1000 . reshape 1 
1 20 X _ plot _ poly = poly _ 
features _ d . fit _ transform X _ plot 
21 h = np . dot X _ plot _ 
poly elastic _ net . coef _ . T + 
elastic _ net . intercept _ 22 plt . plot 
X _ plot h r 23 plt . plot X 
y b . 24 plt . show 得到 的 结果 
为 3.31466833 0.00000000 e + 00 0.00000000 e + 00 
0.00000000 e + 00 1.99874040 e 01 1.21830209 e 02 
2.58040545 e 04 3.01117857 e 03 8.54952421 e 04 4.35227606 
e 05 2.84995639 e 06 8.36248799 e 06 0.0807738447192 该 
方法 中 得到 了 更多 的 0 当然 这也 跟 
参数 的 设置 有关 1 使用 elastic net 得到 的 
结果 4 . 正则化 项的/nr 使用 以及 l1 与 l2 
的 比较 根据 吴恩 达 老师 的 机器学习 公开课 建议 
使用 下面 的 步骤 来 确定 $ \ lambda $ 
的 值 创建 一个 $ \ lambda $ 值 的 
列表 例如 $ \ lambda \ in { 0 0.01 
0.02 0.04 0.08 0.16 0.32 0.64 1.28 2.56 5.12 10.24 
} $ 创建 不同 degree 的 模型 或 改变 其他 
变量 遍历 不同 的 模型 和 不同 的 $ \ 
lambda $ 值 使用 学习 到 的 参数 $ \ 
theta $ 包含 正则化 项 计算 验证 集上 的 误差 
计算误差 时不 包含 正则化 项 $ J _ { CV 
} \ theta $ 选择 在 验证 集上 误差 最小 
的 参数 组合 degree 和$\/nr lambda $ 使用 选 出来 
的 参数 和$\/nr lambda $ 在 测试 集上 测试 计算 
$ J _ { test } \ theta $ . 
下面 通过 一张 图像 来 比较 一下 岭回归 和 Lasso 
回归 1 Lasso 与 岭回归 的 比较 俯瞰图 上 图中 
左上方 表示 $ l1 $ 图中 菱形 图案 和 代价 
函数 图中 深色 椭 圆环 左下方 表示 $ l2 $ 
椭圆形 线圈 和 代价 函数 图中 深色 椭 圆环 同一条 
线上 或 同一 个 环 上 表示 对应 的 函数值 
相同 图案 中心 分别 表示 $ l1 l2 $ 范数 
以及 代价 函数 的 最小值 位置 右边 表示 代价 函数 
加上 对应 的 正则化 项 之后 的 图像 添加 正则化 
项 之后 会 影响 原来 的 代价 函数 的 最小值 
的 位置 以及 梯度 下降时 的 路线 如果 参数 调整 
合适 的话 最小值 应该 在 距离 原来 代价 函数 最小值 
附近 且 与 正则化 项的/nr 图像 相交 因为 此时 这两项 
在 相互 约束 的 情况下 都 取到 最小值 它们 的 
和也/nr 最小 右上图 显示 了 Lasso 回 归中 参数 的 
变化 情况 最终 停留 在 了 $ \ theta _ 
2 = 0 $ 这条 线上 右下方 的 取值 由于 
受到 了 $ l2 $ 范数 的 约束 也 产生 
了 位移 当 正则化 项的/nr 权重 非常大 的 时候 会 
产生 左侧 黄色 点 标识 的 路线 最终 所有 参数 
都为 0 但是 趋近 原点 的 方式 不同 这 是 
因为 对于 范数 来说 原点 是 它们 的 最小值 点 
Referencehttp / / scikit learn . org / stable / 
modules / generated / sklearn . linear _ model . 
Ridge . htmlG é ron A . Hands on machine 
learning with Scikit Learn and TensorFlow concepts tools and techniques 
to build intelligent systems M . O Reilly Media Inc 
. 2017 .   githubhttps / / www . coursera 
. org / learn / machine learningedx   UCSanDiegoX DSE220x 
Machine Learning Fundamentals 