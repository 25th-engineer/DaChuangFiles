文本分类的定义
文本分类是现在非常热门的一个研究领域，也是机器学习中最为重要最为基础的组成部分。文本分类有各种各样的方法，有些简单易懂，有些看上去非常复杂。其实只要搞清楚他们背后的原理，理解文本分类并不是一件很困难的事情。今天先从宏观上介绍一下文本分类，后续会在其他博文中分门别类对文本分类这一课题进行深入的分析，敬请关注。也希望各位高手们多提建议，毕竟我也是菜鸟一个。
文本分类就是将一篇文章归入已有的几个类别当中，这里注重强调2点：
1 要分类的类别必须是事先确定的，并且短时间内不会发生改变。
2 分类的类别并不一定唯一。
文本分类的方法
1 人工制订规则
这种方法最大的弊端就是需要牵扯太大的人力，成本极高。对人的要求也极高，很难通过文章抽象出属于某个分类的规则。而且这种方法灵活性太小，很难适应语言的发展，因此极少有人使用。
2 统计学方法
统计学习方法的基本思想就是：让机器像人类一样自己来通过对大量同类文档的观察来自己总结经验，作为今后分类的依据。统计学习方法需要一批由人工进行了准确分类的文档作为学习的材料（称为训练集，注意由人分类一批文档比从这些文档中总结出准确的规则成本要低得多），计算机从这些文档重挖掘出一些能够有效分类的规则，这个过程被形象的称为训练，而总结出的规则集合常常被称为分类器。训练完成之后，需要对计算机从来没有见过的文档进行分类时，便使用这些分类器来进行。
统计学习方法
之前已经提到过，这种方法就是让计算机自己去学习已经分类好的训练集，然而计算机不是人类，无法按人类理解文章那样来学习，这就给文本分类提出了一个重要的课题。因此如何表示一篇文章，让计算机能够理解就成了重中之重。我们知道，文章的语义信息是很难用计算机可以识别的形式所描述的，因此我们只能退而求其次，用文章中所包含的较低级别的词汇信息来表示文档。事实证明，这种做法的效果也是不错的。
进一步的，不光是包含哪些词很重要，这些词出现的次数对分类也很重要。
再确定如何表示文档之后，就要讨论如何让计算机去学习文档了，也就是我们说的训练。
在训练过程中，每一个实例被称为一个样本，这些样本是由人工进行分类处理过的文档集合，计算机认为这些数据的分类是绝对正确的，可以信赖的。之后，让计算机去观察学习这些样本，来猜出一个可能的分类规则，在机器学习中，这种猜测出来的规则称为假设。然后当遇到要分类的文档时，就是用我们的假设来进行判断，并为文档进行分类。
举个例子说明，人们评价一辆车是否是“好车”的时候，可以看作一个分类问题。我们也可以把一辆车的所有特征提取出来转化为向量形式。在这个问题中词典向量可以为： D=（价格，最高时速，外观得分，性价比，稀有程度）
保时捷：vp=（200万，320，9.5，3，9）
花冠： vt=（15万，220，6.0，8，3）
不同的人会有不同的评价标准，如果以性价比来看，很显然花冠是首选。如果考虑时速，外观和时尚，当然要选保时捷了。可见，对同一个分类问题，用同样的表示形式（同样的文档模型），但因为关注数据不同方面的特性而可能得到不同的结论。这种对文档数据不同方面侧重的不同导致了原理和实现方式都不尽相同的多种方法，每种方法也都对文本分类这个问题本身作了一些有利于自身的假设和简化，这些假设又接下来影响着依据这些方法而得到的分类器最终的表现。
常用的分类方法
分类方法可以说是机器学习领域人们研究的最多的一个部分，目前也有很多成熟的算法。比如决策树，Rocchio，朴素贝叶斯，神经网络，支持向量机，线性最小平方拟合，kNN，遗传算法，最大熵等等，下面就挑选几个跟大家简单介绍一下，以后的博文会陆续的对其中的一些方法做详细的阐述，希望大家常来看看，欢迎拍砖。
1 Rocchio算法
这个方法的思想就是把一个分类中的所有文档的向量去平均值，得到一个新的向量，相当于分类的质心。再有新文档需要判断的时候，比较新文档和质心有多么相像，也就是计算新文档和质心的距离。通过距离的远近判断是否属于该分类。另外有改进的Rocchio算法除了考虑所有正样本的质心之外，还考虑不属于该文档的所有文档的质心。这样一来，新文档应该接近正样本的质心，远离负样本的质心。
该算法有致命的缺陷：
首先该算法假设所有同类文档都是聚集在一个质心的周围，这显然没有任何依据，事实证明也并非如此。
另外一个弊端是该算法认为训练数据绝对正确，这在很多应用中是无法保证的。
2 朴素贝叶斯算法
贝叶斯算法关注的是文档属于某类别概率。文档属于某个类别的概率等于文档中每个词属于该类别的概率的综合表达式。而每个词属于该类别的概率又在一定程度上 可以用这个词在该类别训练文档中出现的次数（词频信息）来粗略估计，因而使得整个计算过程成为可行的。使用朴素贝叶斯算法时，在训练阶段的主要任务就是估 计这些值。
同样的，该方法也有一定的缺陷：
首先，P(d| Ci)之所以能展开成（式1）的连乘积形式，就是假设一篇文章中的各个词之间是彼此独立的，其中一个词的出现丝毫不受另一个词的影响。但这显然不对，即使不是语言学专家的我们也知道，词语之间有明显的所谓“共现”关系，在不同主题的文章中，可能共现的次数或频率有变化，但彼此间绝对谈不上独立。
其次，使用某个词在某个类别训练文档中出现的次数来估计P(wi|Ci)时，只在训练样本数量非常多的情况下才比较准确，而需要大量样本的要求不仅给前期人工分类的工作带来更高要求（从而成本上升），在后期由计算机处理的时候也对存储和计算资源提出了更高的要求。
3 kNN算法
kNN算法则又有所不同，在kNN算法看来，训练样本就代表了类别的准确信息，因此此算法产生的分类器也叫做“基于实例”的分类器，而不管样本是使用什么特征表示的。其基本思想是在给定新文档后，计算新文档特征向量和训练文档集中各个文档的向量的相似度，得到K篇与该新文档距离最近最相似的文档， 根据这K篇文档所属的类别判定新文档所属的类别（注意这也意味着kNN算法根本没有真正意义上的“训练”阶段）。这种判断方法很好的克服了Rocchio 算法中无法处理线性不可分问题的缺陷，也很适用于分类标准随时会产生变化的需求（只要删除旧训练文档，添加新训练文档，就改变了分类的准则）。
kNN唯一的也可以说最致命的缺点就是判断一篇新文档的类别时，需要把它与现存的所有训练文档全都比较一遍，这个计算代价并不是每个系统都能够承受的 （比如我将要构建的一个文本分类系统，上万个类，每个类即便只有20个训练样本，为了判断一个新文档的类别，也要做20万次的向量比较！）。一些基于 kNN的改良方法比如Generalized Instance Set就在试图解决这个问题。
特征选择的方法
大家可能会觉得奇怪，正说着文本分类方法，怎么突然就跳到特征选择上面了。这是因为特征选择在文本分类乃至机器学习中都起到了至关重要的作用。特征选择的好，既可以充分利用训练数据，又可以将数据量进行有效的精简，从而减少计算代价，有效的防治过拟合和欠拟合的发生。常用的特征选择算法有互信息，文档频率，信息增益，开方检验等。具体的特征选择方法我会在以后的博文中进行说明，大家一起期待吧。
今天就写到这里，大概介绍了文本分类的定义，分类和常用算法，以后会针对不同的分类进行单独详细的讲解。