第 4 章 基于 概率论 的 分类 方法 朴素 贝叶斯 
朴素 贝叶斯 概述 贝叶斯 分类 是 一类 分类 算法 的 
总称 这类 算法 均以 贝叶 斯定理 为基础 故 统称为 贝叶斯 
分类 本章 首先 介绍 贝叶斯 分类 算法 的 基础 贝叶 
斯定理 最后 我们 通过 实例 来 讨论 贝叶斯 分类 的 
中 最简单 的 一种 朴素 贝叶斯 分类 贝叶斯 理论 & 
条件概率 贝叶斯 理论 我们 现在 有一个 数据集 它 由 两类 
数据 组成 数据分布 如下 图 所示 我们 现在 用 p1 
x y 表示 数 据点 x y 属于 类别 1 
图 中用 圆点 表示 的 类别 的 概率 用 p2 
x y 表示 数 据点 x y 属于 类别 2 
图中 三角形 表示 的 类别 的 概率 那么 对于 一个 
新 数据 点 x y 可以 用 下面 的 规则 
来 判断 它 的 类别 如果 p1 x y p2 
x y 那么 类别 为 1 如果 p2 x y 
p1 x y 那么 类别 为 2 也就是说 我们 会 
选择 高 概率 对应 的 类别 这 就是 贝叶斯 决策 
理论 的 核心 思想 即 选择 具有 最高 概率 的 
决策 条件概率 如果 你 对 p x y | c1 
符号 很熟悉 那么 可以 跳过 本 小节 有 一个 装了 
7 块 石头 的 罐子 其中 3 块 是 白色 
的 4 块 是 黑色 的 如果 从 罐子 中 
随机 取出 一块 石头 那么 是 白色 石头 的 可能性 
是 多少 由于 取 石头 有 7 种 可能 其中 
3 种 为 白色 所以 取出 白色 石头 的 概率 
为 3/7 那么 取到 黑色 石头 的 概率 又是 多少 
呢 很显然 是 4/7 我们 使用 P white 来 表示 
取到 白色 石头 的 概率 其 概率值 可以 通过 白色 
石头 数目 除以 总 的 石头 数目 来 得到 如果 
这 7 块 石头 如下 图 所示 放在 两个 桶 
中 那么 上述 概率 应该 如何 计算 计算 P white 
或者 P black 如果 事先 我们 知道 石头 所在 桶 
的 信息 是 会 改变 结果 的 这 就是 所谓 
的 条件 概率 conditional probablity 假定 计算 的 是从 B 
桶 取到 白色 石头 的 概率 这个 概率 可以 记作 
P white | bucketB 我们 称之为 在 已知 石头 出自 
B 桶 的 条件 下 取出 白色 石头 的 概率 
很容易 得到 P white | bucketA 值 为 2/4 P 
white | bucketB 的 值 为 1/3 条件概率 的 计算 
公式 如下 P white | bucketB = P white and 
bucketB / P bucketB 首先 我们 用 B 桶 中 
白色 石头 的 个数 除以 两个 桶 中 总的 石 
头数 得到 P white and bucketB = 1/7 . 其次 
由于 B 桶 中有 3 块 石头 而 总 石 
头数 为 7 于是 P bucketB 就 等于 3/7 于是 
又 P white | bucketB = P white and bucketB 
/ P bucketB = 1/7 / 3/7 = 1/3 另外 
一种 有效 计算 条件概率 的 方法 称为 贝叶斯 准则 贝叶斯 
准则 告诉 我们 如何 交换 条件概率 中 的 条件 与 
结果 即 如果 已知 P x | c 要求 P 
c | x 那么 可以 使用 下面 的 计算 方法 
使用 条件概率 来 分类 上面 我们 提到 贝叶斯 决策理论 要求 
计算 两个 概率 p1 x y 和 p2 x y 
如果 p1 x y p2 x y 那么 属于 类别 
1 如果 p2 x y p1 X y 那么 属于 
类别 2 . 这 并 不是 贝叶斯 决策 理论 的 
所有 内容 使用 p1 和 p2 只是 为了 尽可能 简化 
描述 而 真正 需要 计算 和 比较 的 是 p 
c1 | x y 和 p c2 | x y 
. 这些 符号 所 代表 的 具体 意义 是 给定 
某个 由 x y 表示 的 数据 点 那么 该 
数据 点 来自 类别 c1 的 概率 是 多少 数 
据点 来自 类别 c2 的 概率 又是 多少 注意 这些 
概率 与 概率 p x y | c1 并不 一样 
不过 可以 使用 贝叶斯 准则 来 交换 概率 中 条件 
与 结果 具体 地 应用 贝叶斯 准则 得到 使用 上面 
这些 定义 可以 定义 贝叶斯 分类 准 则为 如果 P 
c1 | x y P c2 | x y 那么 
属于 类别 c1 如果 P c2 | x y P 
c1 | x y 那么 属于 类别 c2 . 在 
文档 分类 中 整个 文档 如一 封电子邮件 是 实例 而 
电子 邮件 中 的 某些 元素 则 构成 特征 我们 
可以 观察 文档 中 出现 的 词 并把 每个 词 
作为 一个 特征 而 每个 词 的 出现 或者 不 
出现 作为 该 特征 的 值 这样 得到 的 特征 
数目 就 会跟 词汇表 中的 词 的 数目 一样 多 
我们 假设 特征 之间   相互 独立   所谓   
独立 independence   指 的 是 统计 意义上 的 独立 
即/v 一个/m 特征/n 或者/c 单词/nr 出现/v 的/uj 可能性/n 与/p 它/r 
和/c 其他/r 单词/n 相邻/v 没有/v 关系/n 比如说 我们 中的 我 
和 们 出现 的 概率 与 这 两个 字 相邻 
没有 任何 关系 这个 假设 正是 朴素 贝叶斯 分类器 中 
朴素 naive 一 词 的 含义 朴素 贝叶斯 分类器 中的 
另一个 假设 是 每个 特征 同等 重要 Note   朴素 
贝叶斯 分类器 通常 有 两种 实现 方式 一种 基于 伯努利 
模型 实现 一种 基于 多项式 模型 实现 这里 采用 前一种 
实现 方式 该 实现 方式 中 并不 考虑 词 在 
文档 中 出现 的 次数 只 考虑 出 不 出现 
因此 在 这个 意义 上 相当于 假 设词 是 等 
权重 的 朴素 贝叶斯 场景 机器 学习 的 一个 重要 
应用 就是 文档 的 自动 分类 在 文档 分类 中 
整个 文档 如一 封电子邮件 是 实例 而 电子 邮件 中 
的 某些 元素 则 构成 特征 我们 可以 观察 文档 
中 出现 的 词 并把 每个 词 作为 一个 特征 
而 每个 词 的 出现 或者 不 出现 作为 该 
特征 的 值 这样 得到 的 特征 数目 就 会跟 
词汇表 中的 词 的 数目 一样 多 朴素 贝叶斯 是 
上面 介绍 的 贝叶斯 分类器 的 一个 扩展 是 用于 
文档 分类 的 常用 算法 下面 我们 会 进行 一些 
朴素 贝叶斯 分类 的 实践 项目 朴素 贝叶斯 原理 朴素 
贝叶斯 工作 原理 提取 所有 文档 中的 词条 并 进行 
去 重 获取 文档 的 所有 类别 计算 每个 类别 
中 的 文档 数目 对 每篇 训练 文档 对 每个 
类别 如果 词条 出现 在 文档 中 增加 该 词条 
的 计 数值 for 循环 或者 矩阵 相加 增加 所有 
词条 的 计 数值 此类 别下 词条 总数 对 每个 
类别 对 每个 词条 将该 词条 的 数目 除以 总 
词条 数目 得到 的 条件 概率 P 词条 | 类别 
返回 该 文档 属于 每个 类别 的 条件概率 P 类别 
| 文档 的 所有 词条 朴素 贝叶斯 开发 流程 收集 
数据 可以 使用 任何 方法 准备 数据 需要 数值 型 
或者 布尔 型 数据 分析 数据 有 大量 特征 时 
绘制 特征 作用 不大 此时 使用 直方图 效果 更好 训练 
算法 计算 不同 的 独立 特征 的 条件 概率 测试 
算法 计算 错误率 使用 算法 一个 常见 的 朴素 贝叶斯 
应用 是 文档 分类 可以 在 任意 的 分类 场景 
中 使用 朴素 贝叶斯 分类器 不一定 非 要是 文本 朴素 
贝叶斯 算法 特点 优点 在 数据 较少 的 情况 下 
仍然 有效 可以 处理 多 类别 问题 缺点 对于 输入 
数据 的 准备 方式 较为 敏感 适用 数据类型 标称 型 
数据 朴素 贝叶斯 项目 案例 项目 案例 1 屏蔽 社区 
留言板 的 侮辱性 言论 项目 概述 构建 一个 快速 过滤器 
来 屏蔽 在线 社区 留言板 上 的 侮辱性 言论 如果 
某 条 留言 使用 了 负面 或者 侮辱性 的 语言 
那么 就 将该 留言 标识 为 内容 不当 对此 问题 
建立 两个 类别 侮辱 类 和非/nr 侮辱 类 使用 1 
和 0 分别 表示 开发 流程 收集 数据 可以 使用 
任何 方法 准备 数据 从 文本 中 构建 词 向量 
分析 数据 检查 词条 确保 解析 的 正确性 训练 算法 
从词向/nr 量计算/l 概率/n 测试 算法 根据 现实 情况 修改 分类器 
使用 算法 对 社区 留言板 言论 进行 分类 收集 数据 
可以 使用 任何 方法 本例 是 我们 自己 构造 的 
词表 def loadDataSet 创建 数据集 return 单词 列表 postingList 所属 
类别 classVec postingList = my dog has flea problems help 
please # 0 0 1 1 1 . . . 
. . . maybe not take him to dog park 
stupid my dalmation is so cute I love him stop 
posting stupid worthless garbage mr licks ate my steak how 
to stop him quit buying worthless dog food stupid classVec 
= 0 1 0 1 0 1 # 1 is 
abusive 0 not return postingList classVec 准备 数据 从 文本 
中 构建 词 向量 def createVocabList dataSet 获取 所有 单词 
的 集合 param dataSet 数据集 return 所有 单词 的 集合 
即 不含 重复 元素 的 单词 列表 vocabSet = set 
# create empty set for document in dataSet # 操作符 
| 用于 求 两个 集合 的 并 集 vocabSet = 
vocabSet | set document # union of the two sets 
return list vocabSet def setOfWords2Vec vocabList inputSet 遍历 查看 该 
单词 是否 出现 出现 该 单词 则 将该 单词 置 
1 param vocabList 所有 单词 集合 列表 param inputSet 输入 
数据集 return 匹配 列表 0 1 0 1 . . 
. 其中 1 与 0 表示 词汇表 中的 单词 是否 
出现 在 输入 的 数据 集中 # 创建 一个 和 
词汇表 等长 的 向量 并 将其 元素 都 设置 为 
0 returnVec = 0 * len vocabList # 0 0 
. . . . . . # 遍历 文档 中 
的 所有 单词 如果 出现 了 词汇表 中的 单词 则将 
输出 的 文档 向量 中的 对应 值 设为 1 for 
word in inputSet if word in vocabList returnVec vocabList . 
index word = 1 else print the word % s 
is not in my Vocabulary % word return returnVec 分析 
数据 检查 词条 确保 解析 的 正确性 检查 函数 执行 
情况 检查 词表 不 出现 重复 单词 需要的话 可以 对 
其 进行 排序 listOPosts listClasses = bayes . loadDataSet myVocabList 
= bayes . createVocabList listOPosts myVocabList cute love help garbage 
quit I problems is park stop flea dalmation licks food 
not him buying posting has worthless ate to maybe please 
dog how stupid so take mr steak my 检查 函数 
有效性 例如 myVocabList 中 索 引为 2 的 元素 是 
什么 单词 应该 是 是 help 该 单词 在 第一 
篇 文档 中 出现 了 现在 检查一下 看看 它 是否 
出现 在 第四篇 文档 中 bayes . setOfWords2Vec myVocabList listOPosts 
0 0 0 1 0 0 0 1 0 0 
0 1 0 0 0 0 0 0 0 1 
0 0 0 0 1 1 0 0 0 0 
0 0 1 bayes . setOfWords2Vec myVocabList listOPosts 3 0 
0 0 1 0 0 0 0 0 1 0 
0 0 0 0 0 0 1 0 1 0 
0 0 0 0 0 1 0 0 0 0 
0 训练 算法 从词向/nr 量计算/l 概率/n 现在/t 已经/d 知道/v 了/ul 
一个/m 词/n 是否/v 出现/v 在/p 一篇/m 文档/n 中/f 也 知道 
该 文档 所属 的 类别 接下来 我们 重写 贝叶斯 准则 
将 之前 的 x y 替换 为   w . 
粗体 的   w   表示 这 是 一个 向量 
即 它 由 多个 值 组成 在 这个 例子 中 
数值 个数 与 词汇表 中的 词 个数 相同 我们 使用 
上述 公式 对 每个 类 计算 该 值 然后 比较 
这 两个 概率值 的 大小 首先 可以 通过 类别 i 
侮辱性 留言 或者 非 侮辱性 留言 中的 文档 数 除以 
总 的 文档 数来 计算 概率 p ci 接下来 计算 
p w   | ci 这里 就要 用到 朴素 贝叶斯 
假设 如果 将 w 展开 为 一个 个 独立 特征 
那么 就 可以 将 上述 概率 写作 p w0 w1 
w2 . . . wn | ci 这里 假设 所有 
词 都 互相 独立 该 假设 也 称作 条件 独立性 
假设 例如 A 和 B 两个人 抛 骰子 概率 是 
互不 影响 的 也 就是 相互 独立 的 A 抛 
2点 的 同时 B 抛 3 点 的 概率 就是 
1/6 * 1/6 它 意味着 可以 使用 p w0 | 
ci p w1 | ci p w2 | ci . 
. . p wn | ci 来 计算 上述 概率 
这样 就 极大 地 简化 了 计算 的 过程 朴素 
贝叶斯 分类器 训练 函数 def _ trainNB0 trainMatrix trainCategory 训练 
数据 原版 param trainMatrix 文件 单词 矩阵 1 0 1 
1 1 . . . . . . . param 
trainCategory 文件 对应 的 类别 0 1 1 0 . 
. . . 列表 长度 等于 单词 矩阵 数 其中 
的 1 代表 对应 的 文件 是 侮辱 性 文件 
0 代表 不是 侮辱性 矩阵 return # 文 件数 numTrainDocs 
= len trainMatrix # 单 词数 numWords = len trainMatrix 
0 # 侮辱性 文件 的 出现 概率 即 trainCategory 中 
所有 的 1 的 个数 # 代表 的 就是 多少 
个 侮辱性 文件 与 文件 的 总数 相除 就 得到 
了 侮辱 性 文件 的 出现 概率 pAbusive = sum 
trainCategory / float numTrainDocs # 构造 单词 出现 次数 列表 
p0Num = zeros numWords # 0 0 0 . . 
. . . p1Num = zeros numWords # 0 0 
0 . . . . . # 整个 数据集 单词 
出现 总数 p0Denom = 0.0 p1Denom = 0.0 for i 
in range numTrainDocs # 是否 是 侮辱 性 文件 if 
trainCategory i = = 1 # 如果 是 侮辱 性 
文件 对 侮辱性 文件 的 向量 进行 加 和 p1Num 
+ = trainMatrix i # 0 1 1 . . 
. . + 0 1 1 . . . . 
0 2 2 . . . # 对 向量 中 
的 所有 元素 进行 求和 也 就是 计算 所有 侮辱性 
文件 中 出现 的 单词 总数 p1Denom + = sum 
trainMatrix i else p0Num + = trainMatrix i p0Denom + 
= sum trainMatrix i # 类别 1 即 侮辱性 文档 
的 P F1 | C1 P F2 | C1 P 
F3 | C1 P F4 | C1 P F5 | 
C1 . . . . 列表 # 即 在 1类 
别下 每个 单词 出现 的 概率 p1Vect = p1Num / 
p1Denom # 1 2 3 5 / 90 1/90 . 
. . # 类别 0 即 正常 文档 的 P 
F1 | C0 P F2 | C0 P F3 | 
C0 P F4 | C0 P F5 | C0 . 
. . . 列表 # 即 在 0类 别下 每个 
单词 出现 的 概率 p0Vect = p0Num / p0Denom return 
p0Vect p1Vect pAbusive 测试 算法 根据 现实 情况 修改 分类器 
在 利用 贝叶斯 分类器 对 文档 进行 分类 时 要 
计算 多个 概率 的 乘积 以 获得 文档 属于 某个 
类别 的 概率 即 计算 p w0 | 1 * 
p w1 | 1 * p w2 | 1 如果 
其中 一个 概率值 为 0 那么 最后 的 乘积 也为 
0 为 降低 这种 影响 可以 将 所有 词 的 
出现 数 初始 化为 1 并将 分母 初始 化为 2 
取 1 或 2 的 目的 主要 是 为了 保证 
分子 和 分母 不为 0 大家 可以 根据 业务 需求 
进行 更改 另一个 遇到 的 问题 是 下 溢出 这 
是 由于 太多 很小 的 数 相乘 造成 的 当 
计算 乘积 p w0 | ci * p w1 | 
ci * p w2 | ci . . . p 
wn | ci 时 由于 大部分 因子 都 非常 小 
所以 程序 会下 溢出 或者 得到 不 正确 的 答案 
用 Python 尝试 相乘 许多 很小 的 数 最后 四舍五入 
后会 得到 0 一种 解决 办法 是 对 乘积 取 
自然对数 在 代数 中有 ln a * b = ln 
a + ln b 于是 通过 求 对数 可以 避免 
下 溢出 或者 浮点数 舍入 导致 的 错误 同时 采用 
自然对数 进行 处理 不会 有 任何 损失 下图 给 出了 
函数 f x 与 ln f x 的 曲线 可以 
看出 它们 在 相同 区域 内 同时 增加 或者 减少 
并且 在 相同 点上 取到 极值 它们 的 取值 虽然 
不同 但 不 影响 最终 结果 def trainNB0 trainMatrix trainCategory 
训练 数据 优化 版本 param trainMatrix 文件 单词 矩阵 param 
trainCategory 文件 对应 的 类别 return # 总 文 件数 
numTrainDocs = len trainMatrix # 总 单 词数 numWords = 
len trainMatrix 0 # 侮辱性 文件 的 出现 概率 pAbusive 
= sum trainCategory / float numTrainDocs # 构造 单词 出现 
次数 列表 # p0Num 正常 的 统计 # p1Num 侮辱 
的 统计 p0Num = ones numWords # 0 0 . 
. . . . . 1 1 1 1 1 
. . . . . p1Num = ones numWords # 
整个 数据集 单词 出现 总数 2.0 根据 样本 / 实际 
调查 结果 调整 分母 的 值 2 主要 是 避免 
分母 为 0 当然 值 可以 调整 # p0Denom 正常 
的 统计 # p1Denom 侮辱 的 统计 p0Denom = 2.0 
p1Denom = 2.0 for i in range numTrainDocs if trainCategory 
i = = 1 # 累加 辱骂 词 的 频次 
p1Num + = trainMatrix i # 对 每篇 文章 的 
辱骂 的 频次 进行 统计 汇总 p1Denom + = sum 
trainMatrix i else p0Num + = trainMatrix i p0Denom + 
= sum trainMatrix i # 类别 1 即 侮辱性 文档 
的 log P F1 | C1 log P F2 | 
C1 log P F3 | C1 log P F4 | 
C1 log P F5 | C1 . . . . 
列表 p1Vect = log p1Num / p1Denom # 类别 0 
即 正常 文档 的 log P F1 | C0 log 
P F2 | C0 log P F3 | C0 log 
P F4 | C0 log P F5 | C0 . 
. . . 列表 p0Vect = log p0Num / p0Denom 
return p0Vect p1Vect pAbusive 使用 算法 对 社区 留言板 言论 
进行 分类 朴素 贝叶斯 分类 函数 def classifyNB vec2Classify p0Vec 
p1Vec pClass1 使用 算法 # 将 乘法 转换 为 加法 
乘法 P C | F1F2 . . . Fn = 
P F1F2 . . . Fn | C P C 
/ P F1F2 . . . Fn 加法 P F1 
| C * P F2 | C . . . 
. P Fn | C P C log P F1 
| C + log P F2 | C + . 
. . . + log P Fn | C + 
log P C param vec2Classify 待 测数据 0 1 1 
1 1 . . . 即要 分类 的 向量 param 
p0Vec 类别 0 即 正常 文档 的 log P F1 
| C0 log P F2 | C0 log P F3 
| C0 log P F4 | C0 log P F5 
| C0 . . . . 列表 param p1Vec 类别 
1 即 侮辱性 文档 的 log P F1 | C1 
log P F2 | C1 log P F3 | C1 
log P F4 | C1 log P F5 | C1 
. . . . 列表 param pClass1 类别 1 侮辱性 
文件 的 出现 概率 return 类别 1 or 0 # 
计算公式 log P F1 | C + log P F2 
| C + . . . . + log P 
Fn | C + log P C # 大家 可能 
会 发现 上面 的 计算 公式 没有 除以 贝叶斯 准则 
的 公式 的 分母 也 就是 P w P w 
指 的 是 此 文档 在 所有 的 文档 中 
出现 的 概率 就 进行 概率 大小 的 比较 了 
# 因为 P w 针对 的 是 包含 侮辱 和非/nr 
侮辱 的 全部 文档 所以 P w 是 相同 的 
# 使用 NumPy 数组 来 计算 两个 向量 相乘 的 
结果 这里 的 相乘 是 指 对应 元素 相乘 即 
先将 两个 向量 中 的 第一 个 元素 相乘 然后 
将 第 2个 元素 相乘 以此类推 # 我 的 理解 
是 这里 的 vec2Classify * p1Vec 的 意思 就是 将 
每个 词 与其 对应 的 概率 相 关联 起来 p1 
= sum vec2Classify * p1Vec + log pClass1 # P 
w | c1 * P c1 即 贝叶斯 准则 的 
分子 p0 = sum vec2Classify * p0Vec + log 1.0 
pClass1 # P w | c0 * P c0 即 
贝叶斯 准则 的 分子 if p1 p0 return 1 else 
return 0 def testingNB 测试 朴素 贝叶斯 算法 # 1 
. 加载 数据集 listOPosts listClasses = loadDataSet # 2 . 
创建 单词 集合 myVocabList = createVocabList listOPosts # 3 . 
计算 单词 是否 出现 并 创建 数据 矩阵 trainMat = 
for postinDoc in listOPosts # 返回 m * len myVocabList 
的 矩阵 记录 的 都是 0 1 信息 trainMat . 
append setOfWords2Vec myVocabList postinDoc # 4 . 训练 数据 p0V 
p1V pAb = trainNB0 array trainMat array listClasses # 5 
. 测试数据 testEntry = love my dalmation thisDoc = array 
setOfWords2Vec myVocabList testEntry print testEntry classified as classifyNB thisDoc p0V 
p1V pAb testEntry = stupid garbage thisDoc = array setOfWords2Vec 
myVocabList testEntry print testEntry classified as classifyNB thisDoc p0V p1V 
pAb 完整 代码 地址   https / / github . 
com / apachecn / MachineLearning / blob / master / 
src / python / 4 . NaiveBayes / bayes . 
py 项目 案例 2 使用 朴素 贝叶斯 过滤 垃圾邮件 项目 
概述 完成 朴素 贝叶斯 的 一个 最 著名 的 应用 
电子邮件 垃圾 过滤 开发 流程 使用 朴素 贝叶斯 对 电子邮件 
进行 分类 收集 数据 提供 文本文件 准备 数据 将 文本文件 
解析 成 词条 向量 分析 数据 检查 词条 确保 解析 
的 正确性 训练 算法 使用 我们 之前 建立 的 trainNB 
函数 测试 算法 使用 朴素 贝叶斯 进行 交叉 验证 使用 
算法 构建 一个 完整 的 程序 对 一组 文档 进行 
分类 将 错分 的 文档 输出 到 屏幕 上 收集 
数据 提供 文本文件 文本文件 内容 如下 Hi Peter With Jose 
out of town do you want to meet once in 
a while to keep things going and do some interesting 
stuff Let me know Eugene 准备 数据 将 文本文件 解析 
成 词条 向量 使用 正则表达式 来 切分 文本 mySent = 
This book is the best book on Python or M 
. L . I have ever laid eyes upon . 
import re regEx = re . compile \ \ W 
* listOfTokens = regEx . split mySent listOfTokens This book 
is the best book on Python or M . L 
. I have ever laid eyes upon 分析 数据 检查 
词条 确保 解析 的 正确性 训练 算法 使用 我们 之前 
建立 的 trainNB0 函数 def trainNB0 trainMatrix trainCategory 训练 数据 
优化 版本 param trainMatrix 文件 单词 矩阵 param trainCategory 文件 
对应 的 类别 return # 总 文 件数 numTrainDocs = 
len trainMatrix # 总 单 词数 numWords = len trainMatrix 
0 # 侮辱性 文件 的 出现 概率 pAbusive = sum 
trainCategory / float numTrainDocs # 构造 单词 出现 次数 列表 
# p0Num 正常 的 统计 # p1Num 侮辱 的 统计 
p0Num = ones numWords # 0 0 . . . 
. . . 1 1 1 1 1 . . 
. . . p1Num = ones numWords # 整个 数据集 
单词 出现 总数 2.0 根据 样本 / 实际 调查 结果 
调整 分母 的 值 2 主要 是 避免 分母 为 
0 当然 值 可以 调整 # p0Denom 正常 的 统计 
# p1Denom 侮辱 的 统计 p0Denom = 2.0 p1Denom = 
2.0 for i in range numTrainDocs if trainCategory i = 
= 1 # 累加 辱骂 词 的 频次 p1Num + 
= trainMatrix i # 对 每篇 文章 的 辱骂 的 
频次 进行 统计 汇总 p1Denom + = sum trainMatrix i 
else p0Num + = trainMatrix i p0Denom + = sum 
trainMatrix i # 类别 1 即 侮辱性 文档 的 log 
P F1 | C1 log P F2 | C1 log 
P F3 | C1 log P F4 | C1 log 
P F5 | C1 . . . . 列表 p1Vect 
= log p1Num / p1Denom # 类别 0 即 正常 
文档 的 log P F1 | C0 log P F2 
| C0 log P F3 | C0 log P F4 
| C0 log P F5 | C0 . . . 
. 列表 p0Vect = log p0Num / p0Denom return p0Vect 
p1Vect pAbusive 测试 算法 使用 朴素 贝叶斯 进行 交叉 验证 
文件 解析 及 完整 的 垃圾 邮件 测试函数 # 切分 
文本 def textParse bigString Desc 接收 一个 大 字符串 并 
将其 解析 为 字符串 列表 Args bigString 大 字符串 Returns 
去掉 少于 2 个字符 的 字符串 并将 所有 字符串 转换 
为 小写 返回 字符串 列表 import re # 使用 正则表达式 
来 切分 句子 其中 分隔符 是 除 单词 数字 外 
的 任意 字符串 listOfTokens = re . split r \ 
W * bigString return tok . lower for tok in 
listOfTokens if len tok 2 def spamTest Desc 对 贝叶斯 
垃圾邮件 分类器 进行 自动化 处理 Args none Returns 对 测试 
集中 的 每封 邮件 进行 分类 若 邮件 分类 错误 
则 错误 数 加 1 最后 返回 总 的 错误 
百分比 docList = classList = fullText = for i in 
range 1 26 # 切分 解析 数据 并 归类 为 
1 类别 wordList = textParse open input / 4 . 
NaiveBayes / email / spam / % d . txt 
% i . read docList . append wordList classList . 
append 1 # 切分 解析 数据 并 归类 为 0 
类别 wordList = textParse open input / 4 . NaiveBayes 
/ email / ham / % d . txt % 
i . read docList . append wordList fullText . extend 
wordList classList . append 0 # 创建 词汇表 vocabList = 
createVocabList docList trainingSet = range 50 testSet = # 随机 
取 10 个 邮件 用来 测试 for i in range 
10 # random . uniform x y 随机 生成 一个 
范围 为 x y 的 实数 randIndex = int random 
. uniform 0 len trainingSet testSet . append trainingSet randIndex 
del trainingSet randIndex trainMat = trainClasses = for docIndex in 
trainingSet trainMat . append setOfWords2Vec vocabList docList docIndex trainClasses . 
append classList docIndex p0V p1V pSpam = trainNB0 array trainMat 
array trainClasses errorCount = 0 for docIndex in testSet wordVector 
= setOfWords2Vec vocabList docList docIndex if classifyNB array wordVector p0V 
p1V pSpam = classList docIndex errorCount + = 1 print 
the errorCount is errorCount print the testSet length is len 
testSet print the error rate is float errorCount / len 
testSet 使用 算法 构建 一个 完整 的 程序 对 一组 
文档 进行 分类 将 错分 的 文档 输出 到 屏幕 
上 完整 代码 地址   https / / github . 
com / apachecn / MachineLearning / blob / master / 
src / python / 4 . NaiveBayes / bayes . 
py 项目 案例 3 使用 朴素 贝叶斯 分类器 从 个人 
广告 中 获取 区域 倾向 项目 概述 广告商 往往 想 
知道 关于 一个人 的 一些 特定 人口 统计 信息 以便 
能 更好 地 定向 推销 广告 我们 将 分别 从 
美国 的 两个 城市 中 选取 一些 人 通过 分析 
这些 人 发布 的 信息 来 比较 这 两个 城市 
的 人们 在 广告 用词 上 是否 不同 如果 结论 
确实 不同 那么 他们 各自 常用 的 词 是 那些 
从 人们 的 用词 当中 我们 能否 对 不同 城市 
的 人 所 关心 的 内容 有所 了解 开发 流程 
收集 数据 从 RSS 源 收集 内容 这里 需要 对 
RSS 源 构建 一个 接口 准备 数据 将 文本文件 解析 
成 词条 向量 分析 数据 检查 词条 确保 解析 的 
正确性 训练 算法 使用 我们 之前 简历 的 trainNB0 函数 
测试 算法 观察 错误率 确保 分类器 可用 可以 修改 切分 
程序 以 降低 错误率 提高 分类 结果 使用 算法 构建 
一个 完整 的 程序 封装 所有 内容 给定 两个 RSS 
源 改 程序 会 显示 最 常用 的 公共 词 
收集 数据 从 RSS 源 收集 内容 这里 需要 对 
RSS 源 构建 一个 接口 也 就是 导入 RSS 源 
我们 使用 python 下载 文本 在 http / / code 
. google . com / p / feedparser /   
下 浏览 相关 文档 安装 feedparse 首先 解压 下载 的 
包 并将 当前目录 切换 到 解压 文件 所在 的 文件夹 
然后 在 python 提示符 下 输入 python setup . py 
install 准备 数据 将 文本文件 解析 成 词条 向量 文档 
词 袋 模型 我们 将 每个 词 的 出现 与否 
作为 一个 特征 这 可以 被 描述 为   词集 
模型 set of words model 如果 一个 词 在 文档 
中 出现 不止 一次 这 可能 意味着 包含 该词 是否 
出现 在 文档 中 所 不能 表达 的 某种 信息 
这种 方法 被 称为   词 袋 模型 bag of 
words model 在 词 袋中 每个 单词 可以 出现 多次 
而在 词 集中 每个 词 只能 出现 一次 为 适应 
词 袋 模型 需要 对 函数 setOfWords2Vec 稍加 修改 修改后 
的 函数 为 bagOfWords2Vec 如下 给出 了 基于 词 袋 
模型 的 朴素 贝叶斯 代码 它 与 函数 setOfWords2Vec 几乎 
完全 相同 唯一 不同 的 是 每当 遇到 一个 单词 
时 它 会 增加 词 向量 中的 对应 值 而 
不 只是 将 对应 的 数值 设为 1 def bagOfWords2VecMN 
vocaList inputSet returnVec = 0 * len vocabList for word 
in inputSet if word in inputSet returnVec vocabList . index 
word + = 1 return returnVec # 创建 一个 包含 
在 所有 文档 中 出现 的 不重 复词 的 列表 
def createVocabList dataSet vocabSet = set # 创建 一个 空集 
for document in dataSet vocabSet = vocabSet | set document 
# 创建 两个 集合 的 并 集 return list vocabSet 
def setOfWords2VecMN vocabList inputSet returnVec = 0 * len vocabList 
# 创建 一个 其中 所含 元素 都为 0 的 向量 
for word in inputSet if word in vocabList returnVec vocabList 
. index word + = 1 return returnVec # 文件 
解析 def textParse bigString import re listOfTokens = re . 
split r \ W * bigString return tok . lower 
for tok in listOfTokens if len tok 2 分析 数据 
检查 词条 确保 解析 的 正确性 训练 算法 使用 我们 
之前 简历 的 trainNB0 函数 def trainNB0 trainMatrix trainCategory 训练 
数据 优化 版本 param trainMatrix 文件 单词 矩阵 param trainCategory 
文件 对应 的 类别 return # 总 文 件数 numTrainDocs 
= len trainMatrix # 总 单 词数 numWords = len 
trainMatrix 0 # 侮辱性 文件 的 出现 概率 pAbusive = 
sum trainCategory / float numTrainDocs # 构造 单词 出现 次数 
列表 # p0Num 正常 的 统计 # p1Num 侮辱 的 
统计 # 避免 单词 列表 中 的 任何 一个 单词 
为 0 而 导致 最后 的 乘积 为 0 所以 
将 每个 单词 的 出现 次数 初始 化为 1 p0Num 
= ones numWords # 0 0 . . . . 
. . 1 1 1 1 1 . . . 
. . p1Num = ones numWords # 整个 数据集 单词 
出现 总数 2.0 根据 样本 / 实际 调查 结果 调整 
分母 的 值 2 主要 是 避免 分母 为 0 
当然 值 可以 调整 # p0Denom 正常 的 统计 # 
p1Denom 侮辱 的 统计 p0Denom = 2.0 p1Denom = 2.0 
for i in range numTrainDocs if trainCategory i = = 
1 # 累加 辱骂 词 的 频次 p1Num + = 
trainMatrix i # 对 每篇 文章 的 辱骂 的 频次 
进行 统计 汇总 p1Denom + = sum trainMatrix i else 
p0Num + = trainMatrix i p0Denom + = sum trainMatrix 
i # 类别 1 即 侮辱性 文档 的 log P 
F1 | C1 log P F2 | C1 log P 
F3 | C1 log P F4 | C1 log P 
F5 | C1 . . . . 列表 p1Vect = 
log p1Num / p1Denom # 类别 0 即 正常 文档 
的 log P F1 | C0 log P F2 | 
C0 log P F3 | C0 log P F4 | 
C0 log P F5 | C0 . . . . 
列表 p0Vect = log p0Num / p0Denom return p0Vect p1Vect 
pAbusive 测试 算法 观察 错误率 确保 分类器 可用 可以 修改 
切分 程序 以 降低 错误率 提高 分类 结果 # RSS 
源 分类器 及 高频词 去除 函数 def calcMostFreq vocabList fullText 
import operator freqDict = { } for token in vocabList 
# 遍历 词汇表 中的 每个 词 freqDict token = fullText 
. count token # 统计 每个 词 在 文本 中 
出现 的 次数 sortedFreq = sorted freqDict . iteritems key 
= operator . itemgetter 1 reverse = True # 根据 
每个 词 出现 的 次数 从高 到底 对 字典 进行 
排序 return sortedFreq 30 # 返回 出现 次数 最高 的 
30个 单词 def localWords feed1 feed0 import feedparser docList = 
classList = fullText = minLen = min len feed1 entries 
len feed0 entries for i in range minLen wordList = 
textParse feed1 entries i summary # 每 次访问 一条 RSS 
源 docList . append wordList fullText . extend wordList classList 
. append 1 wordList = textParse feed0 entries i summary 
docList . append wordList fullText . extend wordList classList . 
append 0 vocabList = createVocabList docList top30Words = calcMostFreq vocabList 
fullText for pairW in top30Words if pairW 0 in vocabList 
vocabList . remove pairW 0 # 去掉 出现 次数 最高 
的 那些 词 trainingSet = range 2 * minLen testSet 
= for i in range 20 randIndex = int random 
. uniform 0 len trainingSet testSet . append trainingSet randIndex 
del trainingSet randIndex trainMat = trainClasses = for docIndex in 
trainingSet trainMat . append bagOfWords2VecMN vocabList docList docIndex trainClasses . 
append classList docIndex p0V p1V pSpam = trainNBO array trainMat 
array trainClasses errorCount = 0 for docIndex in testSet wordVector 
= bagOfWords2VecMN vocabList docList docIndex if classifyNB array wordVector p0V 
p1V pSpam = classList docIndex errorCount + = 1 print 
the error rate is float errorCount / len testSet return 
vocabList p0V p1V # 朴素 贝叶斯 分类 函数 def classifyNB 
vec2Classify p0Vec p1Vec pClass1 p1 = sum vec2Classify * p1Vec 
+ log pClass1 p0 = sum vec2Classify * p0Vec + 
log 1.0 pClass1 if p1 p0 return 1 else return 
0 使用 算法 构建 一个 完整 的 程序 封装 所有 
内容 给定 两个 RSS 源 改 程序 会 显示 最 
常用 的 公共 词 函数 localWords 使用 了 两个 RSS 
源 作为 参数 RSS 源 要在 函数 外 导入 这样 
做 的 原因 是 RSS 源 会 随 时间 而 
改变 重新 加载 RSS 源 就会 得到 新的 数据 reload 
bayes module bayes from bayes . pyc import feedparser ny 
= feedparser . parse http / / newyork . craigslist 
. org / stp / index . rss sy = 
feedparser . parse http / / sfbay . craigslist . 
org / stp / index . rss vocabList pSF pNY 
= bayes . localWords ny sf the error rate is 
0.2 vocabList pSF pNY = bayes . localWords ny sf 
the error rate is 0.3 vocabList pSF pNY = bayes 
. localWords ny sf the error rate is 0.55 为了 
得到 错误率 的 精确 估计 应该 多次 进行 上述 实验 
然后 取 平均值 接下来 我们 要 分析 一下 数据 显示 
地域 相关 的 用词 可以 先 对 向量 pSF 与 
pNY 进行 排序 然后 按照 顺序 打印 出来 将 下面 
的 代码 添加 到 文件 中 # 最具 表征 性 
的 词汇 显示 函数 def getTopWords ny sf import operator 
vocabList p0V p1V = localWords ny sf topNY = topSF 
= for i in range len p0V if p0V i 
6.0 topSF . append vocabList i p0V i if p1V 
i 6.0 topNY . append vocabList i p1V i sortedSF 
= sorted topSF key = lambda pair pair 1 reverse 
= True print SF * * SF * * SF 
* * SF * * SF * * SF * 
* SF * * SF * * SF * * 
SF * * SF * * SF * * SF 
* * SF * * for item in sortedSF print 
item 0 sortedNY = sorted topNY key = lambda pair 
pair 1 reverse = True print NY * * NY 
* * NY * * NY * * NY * 
* NY * * NY * * NY * * 
NY * * NY * * NY * * NY 
* * NY * * NY * * for item 
in sortedNY print item 0 函数 getTopWords 使用 两个 RSS 
源 作为 输入 然后 训练 并 测试 朴素 贝叶斯 分类器 
返回 使用 的 概率值 然后 创建 两个 列表 用于 元组 
的 存储 与 之前 返回 排名 最高 的 X 个 
单词 不同 这里 可以 返回 大于 某个 阈值 的 所有 
词 这些 元组 会 按照 它们 的 条件概率 进行 排序 
保存 bayes . py 文件 在 python 提示符 下 输入 
reload bayes module bayes from bayes . pyc bayes . 
getTopWords ny sf the error rate is 0.55 SF * 
* SF * * SF * * SF * * 
SF * * SF * * SF * * SF 
* * SF * * SF * * SF * 
* SF * * SF * * SF * * 
how last man . . . veteran still ends late 
off own know NY * * NY * * NY 
* * NY * * NY * * NY * 
* NY * * NY * * NY * * 
NY * * NY * * NY * * NY 
* * NY * * someone meet . . . 
apparel recalled starting strings 当 注释 掉 用于 移除 高频词 
的 三行 代码 然后 比较 注释 前后 的 分类 性能 
去掉 这 几行 代码 之后 错误率 为 54% 而 保留 
这些 代码 得到 的 错误率 为 70% 这里 观察到 这些 
留言 中 出现 次数 最多 的 前 30个 词 涵盖 
了 所有 用词 的 30% vocabList 的 大小 约为 3000个 
词 也 就是说 词汇表 中的 一小部分 单词 却 占据 了 
所有 文本 用词 的 一大 部分 产生 这种 现象 的 
原因 是 因为 语言 中 大部分 都是 冗余 和 结构 
辅助性 内容 另一个 常用 的 方法 是 不仅 移除 高频词 
同时 从 某个 预定 高频 词中 移除 结构上 的 辅助 
词 该 词表 称为 停用 词表 最后 输出 的 单词 
可以 看出 程序 输出 了 大量 的 停用词 可以 移除 
固定 的 停用词 看看 结果 如何 这样 做 的 花 
分类 错误率 也会 降低 完整 代码 地址   https / 
/ github . com / apachecn / MachineLearning / blob 
/ master / src / python / 4 . NaiveBayes 
/ bayes . py 作者 羊三 /nr 小 瑶 GitHub 地址 
  https / / github . com / apachecn / 
MachineLearning 版权 声明 欢迎 转载 学习 = 请 标注 信息 
来源 于 /nr ApacheCN 