损失 函数 loss function 是 用来 估量 你 模型 的 
预测 值 f x 与 真实 值 Y 的 不一致 
程度 它 是 一个 非 负 实值函数 通常 使用 L 
Y f x 来 表示 损失 函数 越小 模型 的 
鲁棒性 就 越好 损失 函数 是 经验 风险 函数 的 
核心 部分 也是 结构 风险 函数 重要 组成部分 模型/n 的/uj 
结构/n 风险/n 函数/n 包括/v 了/ul 经验/n 风险/n 项和/nr 正则/n 项/n 
通常 可以 表示 成 如下 式子 其中 前面 的 均值 
函数 表示 的 是 经验 风险 函数 L 代表 的 
是 损失 函数 后面 的 Φ Φ 是 正则化 项 
regularizer 或者 叫 惩罚 项 penalty term 它 可以 是 
L1 也 可以 是 L2 或者 其他 的 正则 函数 
整个 式子 表示 的 意思 是 找到 使 目标函数 最 
小时 的 θ θ 值 下面 主要 列出 几种 常见 
的 损失 函数 一 log 对数 损失 函数 逻辑 回归 
有些 人 可能 觉得 逻辑 回归 的 损失 函数 就是 
平方 损失 其实 并 不是 平方 损失 函数 可以 通过 
线性 回归 在 假设 样本 是 高斯分布 的 条件 下 
推导 得到 而 逻辑 回归 得到 的 并 不是 平方 
损失 在 逻辑 回归 的 推导 中 它 假设 样本 
服从 伯努利 分布 0 1 分布 然后 求得 满足 该 
分布 的 似 然 函数 接着 取 对数 求 极值 
等等 而 逻辑 回归 并 没有 求 似 然 函数 
的 极值 而是 把 极大化 当做 是 一种 思想 进而 
推 导出 它 的 经验 风险 函数 为 最小化 负 
的 似 然 函数 即 max F y f x 
min F y f x 从 损失 函数 的 视角 
来看 它 就 成了 log 损失 函 数了 log 损失 
函数 的 标准 形式 L Y P Y | X 
= − logP Y | X L Y P Y 
| X = − log ⁡ P Y | X 
刚刚 说到 取 对数 是 为了 方便 计算 极大 似 
然 估计 因为 在 MLE 中 直接 求导 比较 困难 
所以 通常 都是 先取 对数 再 求导 找 极值 点 
损失 函数 L Y P Y | X 表达 的 
是 样本 X 在 分类 Y 的 情况 下 使 
概率 P Y | X 达到 最大值 换言之 就是 利用 
已知 的 样本分布 找到 最 有可能 即 最大 概率 导致 
这种 分布 的 参数值 或者 说 什么样 的 参数 才能 
使 我们 观测 到 目前 这 组 数据 的 概率 
最大 因为 log 函数 是 单调 递增 的 所以 logP 
Y | X 也会 达到 最大值 因此 在 前面 加上 
负号 之后 最大化 P Y | X 就 等价 于 
最小化 L 了 逻辑 回归 的 P Y = y 
| x 表达式 如下 P Y = y | x 
= 11 + exp − yf x P Y = 
y | x = 11 + exp − yf x 
将 它 带入 到 上式 通过 推导 可以 得到 logistic 
的 损失 函数 表达式 如下 L y P Y = 
y | x = log 1 + exp − yf 
x L y P Y = y | x = 
log ⁡ 1 + exp − yf x 逻辑 回归 
最后 得到 的 目标 式子 如下 如果 是 二 分类 
的话 则 m 值 等于 2 如果 是 多 分类 
m 就是 相应 的 类别 总 个数 这里 需要 解释 
一下 之所以 有人 认为 逻辑 回归 是 平方 损失 是 
因为 在 使用 梯度 下降 来 求 最优 解的/nr 时候 
它 的 迭代 式子 与 平方 损失 求导 后的/nr 式子 
非常 相似 从而 给 人 一种 直 观上 的 错觉 
这里 有个 PDF 可以 参考 一下 Lecture 6 logistic regression 
. pdf . 二 平方 损失 函数 最小二乘 法 Ordinary 
Least Squares 最小二乘 法是/nr 线性 回归 的 一种 OLS 将 
问题 转化成 了 一个 凸 优化 问题 在 线性 回 
归中 它/r 假设/vn 样本/n 和/c 噪声/n 都/d 服从/v 高斯分布/nr 为什么 
假 设成 高斯分布 呢 其实 这里 隐藏 了 一个 小 
知识 点 就是 中心 极限 定理 可以 参考 central limit 
theorem 最后 通过 极大 似 然 估计 MLE 可以 推 
导出 最小二乘 式子 最小二乘 的 基本 原则 是 最优 拟合 
直线 应该是 使 各点 到 回归 直线 的 距离 和 
最小 的 直线 即 平方和 最小 换言之 OLS 是 基于 
距离 的 而 这个 距离 就 是 我们 用 的 
最多 的 欧几里得 距离 为什么 它 会 选择 使用 欧式 
距离 作为 误差 度量 呢 即 Mean squared error MSE 
主要 有 以下 几个 原因 简单 计算 方便 欧氏距离 是 
一种 很好 的 相似性 度量 标准 在 不同 的 表示 
域 变换 后 特征 性质 不变 平方 损失 Square loss 
的 标准 形式 如下 L Y f X = Y 
− f X 2L Y f X = Y − 
f X 2 当 样本 个数 为 n 时 此时 
的 损失 函数 变为 Y f X 表示 的 是 
残差 整个 式子 表示 的 是 残差 的 平方和 而 
我们 的 目的 就是 最小化 这个 目标 函数值 注 该 
式子 未 加入 正则 项 也 就是 最小化 残差 的 
平方和 residual sum of squares RSS 而在 实际 应用 中 
通常 会 使用 均方差 MSE 作为 一项 衡量 指标 公式 
如下 MSE = 1n ∑ i = 1n Yi ~ 
− Yi 2MSE = 1n ∑ i = 1n Yi 
~ − Yi 2 上面 提到 了 线性 回归 这里 
额外 补充 一句 我们 通常 说 的 线性 有 两种 
情况 一种 是 因变量 y 是 自变量 x 的 线性函数 
一种 是 因变量 y 是 参数 α α 的 线性函数 
在 机器 学习 中 通常指 的 都是 后 一种 情况 
三 指数 损失 函数 Adaboost 学过 Adaboost 算法 的 人都 
知道 它 是 前 向 分步 加法 算法 的 特例 
是 一个 加 和 模型 损失 函数 就是 指数函数 在 
Adaboost 中 经过 m 此 迭代 之后 可以 得到 fm 
x fm x Adaboost 每次 迭 代时 的 目的 是 
为了 找到 最小化 下 列式 子时 的 参数 α α 
  和G/nr 而 指数 损失 函数 exp loss 的 标准 
形式 如下 可以 看出 Adaboost 的 目标 式子 就是 指数 
损失 在 给定 n 个 样本 的 情况 下 Adaboost 
的 损失 函数 为 关于 Adaboost 的 推导 可以 参考 
Wikipedia AdaBoost 或者 统计 学习 方法 P145 . 四 Hinge 
损失 函数 SVM 在 机器学习 算法 中 hinge 损失 函数 
和 SVM 是 息息相关 的 在 线性 支持 向量 机中 
最优化 问题 可以 等价 于 下列 式子 下面 来 对 
式子 做个 变形 令 于是 原式 就 变成 了 如若 
取 λ = 12C λ = 12C 式子 就 可以 
表示 成 可以 看出 该 式子 与 下式 非常 相似 
前半部 分 中的 ll 就是 hinge 损失 函数 而 后面 
相当于 L2 正则 项 Hinge 损失 函数 的 标准 形式 
L y = max 0 1 − yy ~ y 
= ± 1L y = max 0 1 − yy 
~ y = ± 1 可以 看出 当 | y 
| = 1时 L y = 0 更多 内容 参考 
Hinge loss 补充 一下 在 libsvm 中 一共 有4/nr 中核 
函数 可以 选择 对应 的 是 t 参数 分别 是 
0 线性 核 1 多项式 核 2 RBF 核 3 
sigmoid 核 五 其它 损失 函数 除了 以上 这 几种 
损失 函数 常用 的 还有 0 1 损失 函数 绝对值 
损失 函数 下面 来 看看 几种 损失 函数 的 可视化 
图像 对着 图 看看 横坐标 看看 纵坐标 再 看看 每条 
线 都 表示 什么 损失 函数 多看 几次 好好 消化 
消化 OK 暂时 先 写到 这里 休息 下 最后 需要 
记住 的 是 参数 越多 模型 越 复杂 而/c 越/d 
复杂/a 的/uj 模型/n 越/d 容易/a 过拟合/i 过拟合 就是说 模型 在 
训练 数据 上 的 效果 远远 好于 在 测试 集上 
的 性能 此时 可以 考虑 正则化 通过 设置 正则 项 
前面 的 hyper parameter 来 权衡 损失 函数 和 正则 
项 减小 参数 规模 达到 模型简化 的 目的 从而 使 
模型 具有 更好 的 泛化 能力 