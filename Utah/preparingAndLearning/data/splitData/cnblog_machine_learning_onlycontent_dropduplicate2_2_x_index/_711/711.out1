版权 声明 本文 由 LeftNotEasy 发布 于 http / / 
leftnoteasy . cnblogs . com 本文 可以 被 全部 的 
转载 或者 部分 使用 但 请 注明 出处 如果 有 
问题 请 联系 wheeleast @ gmail . com 前言 本 
来上 一章 的 结尾 提到 准备 写写 线性 分类 的 
问题 文章 都 已经 写 得 差不多 了 但是 突然 
听说 最近 Team 准备 做 一套 分布式 的 分类器 可能 
会 使用 Random Forest 来做 下了 几篇 论文 看了 看 
简单 的 random forest 还 比较 容易 弄懂 复杂 一点 
的 还 会与 boosting 等 算法 结合 参见 iccv09 对于 
boosting 也 不甚了解 所以 临时 抱佛脚 的 看了 看 说起 
boosting 强哥/nr 之前 实现 过 一套 Gradient Boosting Decision Tree 
GBDT 算法 正好 参考 一下 最近 看 的 一些 论文 
中 发现 了 模型 组合 的 好处 比如 GBDT 或者 
rf 都是 将 简单 的 模型 组合 起来 效果 比 
单个 更 复杂 的 模型 好 组合 的 方式 很多 
随机化 比如 random forest Boosting 比如 GBDT 都是/nr 其中 典型 
的 方法 今天 主要 谈谈 Gradient Boosting 方法 这个 与 
传统 的 Boosting 还有 一些 不同 的 一些 数学 基础 
有了/nr 这个 数学 基础 上面 的 应用 可 以看 Freidman 
的 Gradient Boosting Machine 本文 要求 读者 学过 基本 的 
大学 数学 另外 对 分类 回归 等 基本 的 机器学习 
概念 了解 本文 主要 参考 资料 是 prml 与 Gradient 
Boosting Machine Boosting 方法 Boosting 这 其实 思想 相当 的 
简单 大概 是 对 一份 数据 建立 M 个 模型 
比如 分类 一般 这种 模型 比较简单 称为 弱 分类器 weak 
learner 每次 分类 都将 上一次 分 错 的 数据 权重 
提高 一点 再 进行 分类 这样 最终 得到 的 分类器 
在 测试 数据 与 训练 数据 上 都 可以 得到 
比较 好 的 成绩 上图 图片 来自 prml p660 就是 
一个 Boosting 的 过程 绿色 的 线 表示 目前 取得 
的 模型 模型 是由 前 m 次 得到 的 模型 
合并 得到 的 虚线 表示 当前 这次 模型 每次 分类 
的 时候 会 更 关注 分 错 的 数据 上 
图中 红色 和 蓝色 的 点 就是 数据 点 越大 
表示 权重 越高 看看 右 下角 的 图片 当 m 
= 150 的 时候 获取 的 模型 已经 几乎 能够 
将 红色 和 蓝色 的 点 区 分开 了 Boosting 
可以 用 下面 的 公式 来 表示 训练 集中 一 
共有 n 个 点 我们 可以 为 里面 的 每一个 
点 赋 上一个 权重 Wi 0 = i n 表示 
这个 点 的 重要 程度 通过 依次 训练 模型 的 
过程 我们 对 点 的 权重 进行 修正 如果 分类 
正确 了 权重 降低 如果 分类 错了 则 权重 提高 
初始 的 时候 权重 都是 一样 的 上 图中 绿色 
的 线 就是 表示 依次 训练 模型 可以 想象 得到 
程序 越往后 执行 训 练出 的 模型 就 越会 在意 
那些 容易 分 错 权重 高 的 点 当 全部 
的 程序执行 完 后 会 得到 M 个 模型 分别 
对应 上图 的 y1 x yM x 通过 加权 的 
方式 组合 成 一个 最终 的 模型 YM x 我 
觉得 Boosting 更像 是 一个 人 学习 的 过程 开始 
学 一样 东西 的 时候 会去 做 一些 习题 但是 
常常 连 一些 简单 的 题目 都会 弄错 但是 越到/nr 
后面 简单 的 题目 已经 难不倒 他 了 就会 去做 
更 复杂 的 题目 等到 他 做 了 很多 的 
题目 后 不管 是 难题 还是 简单 的 题 都 
可以 解决 掉了 Gradient Boosting 方法 其实 Boosting 更像 是 
一种 思想 Gradient Boosting 是 一种 Boosting 的 方法 它 
主要 的 思想 是 每一次 建立 模型 是 在 之前 
建立 模型 损失 函数 的 梯度 下降 方向 这 句话 
有 一点 拗口 损失 函数 loss function 描述 的 是 
模型 的 不靠谱 程度 损失 函数 越大 则 说明 模型 
越 容易 出错 其实 这里 有 一个 方差 偏差 均衡 
的 问题 但是 这里 就 假设 损失 函数 越大 模型 
越 容易 出错 如果 我们 的 模型 能够 让 损失 
函数 持续 的 下降 则 说明 我们 的 模型 在 
不停 的 改进 而 最好 的 方式 就是 让 损失 
函数 在其 梯度 Gradient 的 方向 上 下降 下面 的 
内容 就是 用 数学 的 方式 来 描述 Gradient Boosting 
数学上 不算 太 复杂 只要 潜下 心 来看 就 能看懂 
可加 的 参数 的 梯度 表示 假设 我们 的 模型 
能够 用 下面 的 函数 来 表示 P 表示 参数 
可能 有 多个 参数 组成 P = { p0 p1 
p2 . } F x P 表示 以 P 为 
参数 的 x 的 函数 也 就是 我们 的 预测 
函数 我们 的 模型 是 由 多个 模型 加 起来 
的 β 表示 每个 模型 的 权重 α 表示 模型 
里面 的 参数 为了 优化 F 我们 就 可以 优化 
{ β α } 也就是 P 我们 还是 用 P 
来 表示 模型 的 参数 可以 得到 Φ P 表示 
P 的 likelihood 函数 也 就是 模型 F x P 
的 loss 函数 Φ P = 后面 的 一块 看起来 
很 复杂 只要 理解 成是/nr 一个 损失 函数 就行了 不要 
被 吓跑 了 既然 模型 F x P 是 可加 
的 对于 参数 P 我们 也 可以 得到 下面 的 
式子     这样 优化 P 的 过程 就 可以 
是 一个 梯度 下降 的 过程 了 假设 当前 已经 
得到 了 m 1个 模型 想要 得到 第 m 个 
模型 的 时候 我们 首先 对 前 m 1个 模型 
求 梯度 得到 最快 下降 的 方向 gm 就是 最快 
下降 的 方向 这里 有 一个 很 重要 的 假设 
对于 求出 的 前 m 1个 模型 我们 认为 是 
已知 的 了 不要 去 改变 它 而 我们 的 
目标 是 放在 之后 的 模型 建 立上 就像 做 
事情 的 时候 之前 做错 的 事 就 没有 后悔药 
吃了 只有 努力 在 之后 的 事情 上 别 犯错 
我们 得到 的 新的 模型 就是 它 就在 P 似 
然 函数 的 梯度方向 ρ 是 在 梯度方向 上 下降 
的 距离 我们 最终 可以 通过 优化 下面 的 式子 
来 得到 最优 的 ρ 可加 的 函数 的 梯度 
表示 上面 通过 参数 P 的 可加 性 得到 了 
参数 P 的 似 然 函数 的 梯度 下降 的 
方法 我们 可以 将 参数 P 的 可加 性 推广 
到 函数 空间 我们 可以 得到 下面 的 函数 此处 
的 fi x 类似于 上面 的 h x α 因为 
作者 的 文献 中 这样 使用 我 这里 就用 作者 
的 表达方法 同样 我们 可以 得到 函数 F x 的 
梯度 下降 方向 g x 最终 可以 得到 第 m 
个 模型 fm x 的 表达式 通用 的 Gradient Descent 
Boosting 的 框架 下面 我 将 推导 一下 Gradient Descent 
方法 的 通用 形式 之前 讨论 过 的 对于 模型 
的 参数 { β α } 我们 可以 用 下面 
的 式子 来 进行 表示 这个 式子 的 意思 是 
对于 N 个 样本点 xi yi 计算 其 在 模型 
F x α β 下 的 损失 函数 最优 的 
{ α β } 就是 能够 使得 这个 损失 函数 
最小 的 { α β } 表示 两个 m 维 
的 参数 写成 梯度 下降 的 方式 就是 下面 的 
形式 也 就是 我们 将 要 得到 的 模型 fm 
x 的 参数 { α m β m } 能够 
使得 fm 的 方向 是 之前 得到 的 模型 Fm 
1 x 的 损失 函数 下降 最快 的 方向 对于 
每 一个 数 据点 xi 都 可以 得到 一个 gm 
xi 最终 我们 可以 得到 一个 完整 梯度 下降 方向 
为了 使得 fm x 能够 在 gm x 的 方向 
上 我们 可以 优化 下面 的 式子 得到 可以 使用 
最 小二 乘法 得到 了 α 的 基础 上 然后 
可以 得到 β m           最终 
合并 到 模型 中 算法 的 流程图 如下 之后 作者 
还 说 了 这个 算法 在 其他 的 地方 的 
推广 其中 Multi class logistic regression and classification 就是 GBDT 
的 一种 实现 可以 看看 流程图 跟 上面 的 算法 
类似 的 这里 不 打算 继续 写 下去 再写 下去 
就成 论文 翻译 了 请 参考 文章 Greedy function Approximation 
– A Gradient Boosting Machine 作者 Freidman 总结 本文 主要 
谈了 谈 Boosting 与 Gradient Boosting 的 方法 Boosting 主要 
是 一种 思想 表示 知错就改 而 Gradient Boosting 是 在 
这个 思想 下 的 一种 函数 也 可以 说 是 
模型 的 优化 的 方法 首先 将 函数 分解为 可加 
的 形式 其实 所有 的 函数 都是 可加 的 只是 
是否 好 放在 这个 框架 中 以及 最终 的 效果 
如何 然后 进行 m 次 迭代 通过 使得 损失 函数 
在 梯度方向 上 减少 最终 得到 一个 优秀 的 模型 
值得一提的是 每次 模型 在 梯度 方 向上 的 减少 的 
部分 可以 认为 是 一个 小 的 或者 弱 的 
模型 最终 我们 会 通过 加权 也 就是 每次 在 
梯度方向 上 下降 的 距离 的 方式 将 这些 弱 
的 模型 合并 起来 形成 一个 更好 的 模型 有了/nr 
这个 Gradient Descent 这个 基础 还 可以 做 很多 的 
事情 也在 机器 学习 的 道路 上 更进一步 了 