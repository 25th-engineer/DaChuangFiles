http / / www . renwuyi . com / index 
. php action = artinfo & id = 19036 & 
cat _ id = 2 # top 文本 生成 是 
比较 学术 的 说法 通常 在 媒体 上 见到 的 
机器人 写作 人工智能 写作 自动 对话 生成 机器人 写 古诗 
等 都 属于 文本 生成 的 范畴 2016 年里 关于 
文本 生成 有 许多 的 新闻 事件 引起 了 学术界 
以外 对这 一 话题 的 广泛 关注 2016年 3月 3日 
MIT CSAIL 1 报道 了 MIT 计算机 科学 与 人工智能 
实验室 的 一位 博士 后 开发 了 一款 推特 机器人 
叫 DeepDrumpf 它 可以 模仿 当时 的 美国 总统 候选人 
Donald Trump 来 发文 2016年 3月 22日 日本 共同社 报道 
由 人工智能 创作 的 小说 作品 机器人 写 小说 的 
那一天 入围 了 第三 届 星新一 文学奖 的 初审 这一 
奖项 以 被誉为 日本 微型 小说 之父 的 科幻 作家 
星新一 命名 提交 小说 的 是 任性 的 人工智能 之 
我 是 作家 简称 我 是 作家 团队 2 2016年 
5月 美国 多家 媒体 3 4 报道 谷歌 的 人工智能 
项目 在 学习 了 上千 本 浪漫 小说 之后 写出 
后 现代 风格 的 诗歌 基于 人工智能 的 文本 生成 
真的 已经 达到 媒体 宣传 的 水平 了吗 这些 事件 
背后 是 怎样 的 人工智能 技术 关于 机器人 写 小说 
的 工作 我们 会 在 另一 篇 文章 会 有 
那么 一天 机器人 可以 写 小说 吗 里 进行 深入 
的 讨论 他们 的 工作 更多 的 是 基于 模板 
的 生成 在 这篇文章 里 我们 主要 想 通过 三 
篇 文章 介绍 另一 大类 方法 即 基于 统计 的 
文本 生成 令人 吃惊 的 Char RNN 关于 基于 深度 
学习 的 文本 生成 最 入门级 的 读物 包括 Andrej 
Karpathy 这篇 博客 5 他 使用 例子 生动 讲解 了 
Char RNN Character based Recurrent Neural Network 如何 用于 从 
文本 数据集 里 学习 然后 自动 生成 像模像样 的 文本 
图一 直观 展示 了 Char RNN 的 原理 以 要让 
模型 学习 写出 hello 为例 Char RNN 的 输入输出 层 
都 是以 字符 为 单位 输入 h 应该 输出 e 
输入 e 则 应该 输出 后续 的 l 输入 层 
我们 可以 用 只有 一个 元素 为 1 的 向量 
来 编码 不同 的 字符 例如 h 被 编码 为 
1000 e 被 编码 为 0100 而 l 被 编码 
为 0010 使用 RNN 的 学习 目标 是 可以 让 
生成 的 下一 个字符 尽量 与 训练 样 本里 的 
目标 输出 一致 在 图一 的 例子 中 根据 前 
两个 字符 产生 的 状态 和 第三 个 输入 l 
预测出 的 下一 个字符 的 向量 为 0.1 0.5 1.9 
1.1 最大 的 一维 是 第三 维 对应 的 字符 
则为 0010 正好 是 l 这 就是 一个 正确 的 
预测 但从 第一 个 h 得到 的 输出 向量 是 
第四维 最大 对应 的 并 不是 e 这样 就 产生 
代价 学习 的 过程 就是 不断 降低 这个 代价 学习 
到 的 模型 对 任何 输入 字符 可以 很好 地 
不断 预测 下 一个 字符 如此一来 就 能 生成 句子 
或 段落 Andrej Karpathy 还 共享 了 代码 6 感 
兴趣 的 同学 不妨 下载 来 试试 效果 会 让 
你 震惊 Andrej Karpathy 在底层 使用 的 RNN 的 具体 
实现 是 LSTM Long Short Term Memory 想 了解 LSTM 
可以 阅读 7 讲得 再 清楚 不过 研究 人员 用 
Char RNN 做 了 很多 有趣 的 尝试 例如 用 
莎士比亚 的 作品 来做 训练 模型 就 能 生成 出 
类似 莎士比亚 的 句子 利用 金庸 的 小说 来做 训练 
模型 就 能 生成 武侠小说 式 的 句子 利用 汪峰 
的 歌词 做 训练 模型 也 能 生成 类似 歌词 
的 句子 来 在 本文 一开始 提到 的 1 MIT 
计算机 科学 与 人工智能 实验室 的 博士后 Bradley Hayes 也 
正是 利用 类似 的 方法 开发 了 一款 模仿 候任 
美国总统 Donald Trump 的 推特 机器人 叫 DeepDrumpf 例如 图 
二中 这个 机器人 说 我 就是 伊斯兰 国不/nr 需要 的 
据 作者 介绍 他 受到 一篇 模拟 莎士比亚 的 论文 
启发 以 Donald Trump 的 演讲 和 辩论 时常 大约 
几个 小时 的 字幕 作为 训练 语料 使用 深度 神经 
网络 学习 去 训练 Trump 的 模型 他 也 声称 
因为 有 一篇 文章 调侃 Trump 的 发言 只有 小学 
四年级 的 水平 因而 想到 用 Trump 的 语料 可能 
是 最 容易 控制 的 这 是 一个 有趣 的 
应用 记者 评论 称 这个 机器人 也 并 不是 总 
能写 出好 的 句子 但至少 部分 是 通顺 的 其实 
风格 并 不是 很难 学到 只要 使用 的 训练 语料 
来自 同一 个人 而这 个人 的 写作 或者 发言 具有 
辨识度 高的/nr 特点 深度 学习 生成 对话 推荐 阅读 的 
第二 篇 文章 是 诺亚方舟 实验室 的 尚利 峰 吕/nr 
正东/ns 和/c 李航/i 在/p 2015年/tdq ACL/w 大会/n 上/f 发表/v 的/uj 
Neural Responding Machine for Short Text Conversation 9 大家 也许 
听说 过 微软 小冰 它 因为 开创性 的 主要 做 
闲聊 即 以 娱乐 为 目的 的 聊天 式 对话 
被 哈尔滨 工业 大学 的 刘挺/nr 教授 誉为 是 第二 
波 人机对话 的 浪潮 的 代表 8 小冰 的 出现 
也 影响 到 了 学术界 除了 原 来做 知识性 的 
问答 一些 研究 也 开始 关注 闲聊 让 机器人 和 
人类 搭话 这 方面 诺亚方舟 实验室 发表 了 一系列 有 
影响力 的 文章 今天 介绍 的 这篇文章 在 Arxiv . 
org 上 发布 短短 一年 时间 已经 有 67次 的 
引用 9 这篇文章 尝试 用 encoder decoder 编码 解码 的 
框架 解决 短 文本 对话 Short Text Conversation 缩写 为 
STC 的 问题 虽然 encoder decoder 框架 已经 被 成功 
应用 在 机器 翻译 的 任务 中 但是 对话 与 
翻译 不同 对应 一个 输入 文本 post 往往 有 多种 
不同 的 应答 responses 文 中举 了 一个 例子 一个人 
说 刚刚 我 吃 了 一个 吞拿鱼 三明治 不同 的 
应答 可以 是 天哪 才 早晨 11点 看起来 很 美味 
哟 或是 在 哪里 吃 的 这种 一对多 的 情况 
在 对话 中 很 普遍 也很 自然 的确 不同 的 
人 会对 同 一句话 做出 不同 的 反应 即使 是 
同一 个人 如果 每次 回答 都 一模一样 也是 很 无趣 
的 针对 这 一 特点 作者 们 提出 Neural Responding 
Machine 简称 NRM 见 图三 框架 来 解决 短 文本 
对话 的 问题 他们 尝试 了 全局 编码 和 局部 
编码 最终 发现 先 分别 训练 再 用图 四 的 
结构 来做 微调 训练 效果 最佳 全局 编码 的 优点 
是 能够 获得 全局 信息 同样 的 词 在 不同 
情境 下 会有 不同 的 意义 全局 信息 可以 部分 
解决 这 类 情况 缺点 是 它 供给 解码 的 
输入 比较 固定 局部 编码 利用 局部 信息 比较 灵活 
多样 刚好 可以 缓解 全局 编码 的 弱点 这篇 论文 
的 另一 大 贡献 是 构建 了 一个 比 较大 
的 数据 集 和 标注 来 评价 不同 的 方法 
通过 对比 所/c 提出/v 的/uj 混合/vn 全局/n 和/c 局部/n 的/uj 
方法/n 比/p 以往/t 基于/p 搜索/v 的/uj 方法/n 和/c 机器/n 翻译/v 
的/uj 方法/n 都/d 要好/i 很多/m 机器 翻译 的 方法 生成 
的 句子 往往 不 通顺 得分 最低 能比 基于 搜索 
的 方法 好 很多 也 非常 不 容易 因为 基于 
搜索 的 方法 得到 的 已经 是 人 使用 过 
的 应答 不会不 通顺 大家 可以 在 图 五 的 
实例 中 直接 感受 一下 生成 的 效果 NRM glo 
是 全局 编码 的 模型 NRM loc 是 局部 编码 
的 模型 NRM hyb 是 混合 了 全局 和 局部 
的 模型 Rtr . based 则是 基于 搜索 的 方法 
2015 到 2016年 这篇 论文 的 作者 组织 了 NTCIR 
12 STC 任务 10 公开 他们 的 数据 集 并 
提供 公共 评测 有 16个 大学 或 研究 机构 参加 
了 中文 短文 本 对话 任务 的 评测 2017年 他们 
将 会 继续 组织 NTCIR 13 STC 11 现已 开放注册 
12 除了 上 一届 的 基于 搜索 的 子 任务 
这一次 还 设立 了 生成 应答 的 子 任务 我们 
预计 今年 的 结果 会 更 精彩 被 媒体 误解 
的 谷歌 人工智能 写诗 第三 篇 文章 是 Samuel Bowman 
等 发表 在 Arxiv . org 上 的 名为 Generating 
Sentences from a Continuous Space 的 文章 13 作者 分别 
来自 斯坦福大学 马萨诸塞 大学 阿姆斯特 分校 以及 谷歌 大脑 部门 
工作 是 在 谷歌 完成 的 这一 工作 曾被/nr 媒体 
广泛 报道 但 我 发现 很多 报道 例如 3 4 
都对 论文 的 工作 有 一些 误解 一些 记者 将 
图 六 所示 的 文字 误认为 是 机器人 写 出来 
的 后 现代 风格 的 诗歌 其实不然 这 只是 作者 
在 展示 他们 的 方法 可以 让 句子 级别 的 
编码 解码 更 连续 具体 而言 在 他们 学习 到 
的 空间 中 每个 点 可以 对应 一个 句子 任意 
选定 两个 点 例如 在 图 六中 一对 点 对应 
的 句子 分别 是 i want to talk to you 
. 和 she didn t want to be with him 
两点 之间 的 连线 上 可以 找出 间隔 均匀 的 
几个 点 将 它们 也 解码 成 句子 会 发现 
这些 句子 好像 是 从 第一 句 逐渐 变化 成了 
最后 一句 得到 这样 的 结果 实属不易 在 文章 的 
一 开始 作者 就 给出 了 一个 例子 来 说明 
传统 的 自动 解码 并 不能 很好 地 编码 完整 
的 句子 如图 七 所示 从 句子 i went to 
the store to buy some groceries 到 句子 horses are 
my favorite animals 中间 取 的 点 经过 解码 得到 
的 句子 呈现 在 它们 之间 可以 发现 这些 句子 
未必 是 符合 语法 的 英文 句子 与之 相比 图 
六 呈现 的 句子 质量 要 好 很多 不仅 语法 
正确 主题 和 句法 也 一致 这 篇 文章 的 
想法 非常 有意思 他们 想 使用 VAE v a r 
a t i o n a l a u t 
o e n c o d e r 的 简称 
学习 到 一个 更 连续 的 句子 空间 如图 八 
所示 作者 使用 了 单层 的 LSTM 模型 作为 encoder 
编码器 和 decoder 解码器 并 使用 高斯 先验 作为 regularizer 
正规化 项 形成 一个 序列 的 自动 编码器 比起 一般 
的 编码 解码 框架 得到 的 句子 编码 往往 只 
会 记住 一些 孤立 的 点 VAE 框架 学到 的 
可以 想象 成是/nr 一个 椭圆形 区域 这样 可以 更好 地 
充满 整个 空间 我 的 理解 是 VAE 框架 将 
贝叶斯 理论 与 深度 神经网络 相结合 在 优化 生成 下 
一个 词 的 目标 的 同时 也 优化 了 跟 
先验 有关 的 一些 目标 例如 KL cost 和 crossentropy 
两项 细节 请 参考 论文 使对 一个 整句 的 表达 
更好 当然 为了 实现 这 一 想法 作者 做 了 
很多 尝试 首先 对 图 八所 展示 的 结构 做 
一些 变形 并 没有 带来 明显 的 区别 但在 优 
化时 使用 退火 的 技巧 来 降低 KL cost/w 和/c 
训练/vn 时把/nr 适当/a 比例/n 的/uj 词/n 变为/v 未知/v 词/n 即 
word dropout 这 两项 技术 就 非常 有效 作者 们 
通过 两个 有意思 的 实验 来 展示 了 他们 的 
结果 一个 是 做 填空题 如图 九 所示 隐藏 句子 
的 后 20% 让 模型 来 生成 后面 的 部分 
从 几个 例子 看 VAE 的 方法 比 RNN 语言 
模型 简称 RNNLM 更加 通顺 和有 信息量 第二 个 实验 
就是 在 两个 句子 之间 做 轮 移 Homotopy 也 
就是 线性插值 对比 图 六和 图 七 可以 看出 VAE 
给出 的 句子 更 平滑 而且 正确 这 一点 可以 
间接 说明 学习 到 的 句子 空间 更好 地 被 
充满 当然 作者 们 还给 出 了 一些 定量 的 
比较 结果 在 比较 填空 结果 时 他们 使用 了 
adversarial evaluation 对抗 评价 具体 的 做法 是 他们 取样 
50% 的 完整 句子 作为 正 例 再拿 50% 的 
由 模型 填空 完成 的 句子 作为 负 例 然后 
训练 一个 分类器 如果 一个 模型 填 的 越难与/nr 正 
例 分开 就 说明 这种 模型 的 生成 效果 更好 
更具 欺骗性 因此 可以 认为 这 一 模型 在 填空 
任务 上 更 出色 实验 的 结果 也 支持 VAE 
比 RNNLM 更好 问题 与 难点 人工智能 真的 会 创作 
吗 使用 深度 学习 技术 写出 的 文章 或者 对话 
的确 是 会 出现 训练 集合 里 未 见过 的 
句子 例如 一个 原 句 的 前半段 可能会 跟上 另一个 
原 句 的 后半段 也 可能 除了 词 搭配 组合 
都是 训练 集 里 没有 的 这 看起来 有些 创作 
的 意味 但是 细究 起来 往往 是 原 句 的 
部分 更为 通 顺和 有意义 目前 的 技术 可以 拼凑 
偶尔 出现 一两 个 好玩 的 点 但是 写得 长了 
读 起来 会 觉得 没头没脑 这 是因为 没有 统领 全篇 
的 精神 跟 人类 的 作家 比 当然 还是 相差 
很远 机器学习 到 的 还 只是 文字 表面 没有 具备 
人 要写 文章 的 内在 动因 人 写文章 表达 的 
是 自己 的 思想 和 感受 这是 机器 所 没有 
的 因此 即使 是 机器 写文章 具体 想要 表达 什么 
似乎 还要 由人 来 控制 但 如果 控制 得 太多 
看起来 又 不那么 智能 少了 些 趣味 我 认为 要想 
让 机器 更 自由 地 写出 合乎 逻辑 的 话来 
我们 还 需要 类似 VAE 那 篇文章 一样 更 深入 
的 研究 对 句子 甚至 段落 的 内在 逻辑 进行 
学习 另外 人在 写 一篇 文章 的 时候 很容易 自我 
衡量 语句 是否 通顺 思想 是否 表达 清楚 以及 文章 
的 结构 是否 清晰 有趣 机器 却 很难 做到 因此 
优化 的 目标 很难 与 真正 的 质量 相一致 目前 
的 自然 语言 理解 技术 对于 判断句 法语 法 是否 
正确 可能 还 有些 办法 但 要想 判断 内容 和 
逻辑 上 是否 顺畅 恐怕 还 需要 常识 和 推理 
的 帮助 这些 部分 暂时 还 比较 薄弱 但也 并非 
毫无办法 我 相信 未来 对 文本 生成 的 研究 一定 
会 涉及 这些 方面 