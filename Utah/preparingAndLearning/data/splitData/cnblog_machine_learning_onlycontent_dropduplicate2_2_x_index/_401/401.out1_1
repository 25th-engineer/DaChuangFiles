原文 http / / blog . csdn . net / 
zouxy09 / article / details / 48903179 一 概述 机器学习 
算法 在 近几年 大 数据 点燃 的 热火 熏陶 下 
已经 变得 被人 所 熟知 就算 不 懂得 其中 各 
算法 理论 叫 你 喊上 一两 个 著名 算法 的 
名字 你 也能 昂首挺胸 脱口而出 当然 了 算法 之 林虽大/nr 
但能 者 还是 有限 能适应 某些 环境 并 取得 较好 
效果 的 算法 会 脱颖而出 而 表现 平平 者 则 
被 历史 所 淡忘 随着 机器学习 社区 的 发展 和 
实践 验证 这 群 脱颖而出 者 也 逐渐 被人 所 
认可 和 青睐 同时 获得 了 更多 社区 力量 的 
支持 改进 和 推广 以 最 广泛 的 分类 算法 
为例 大致 可以 分为 线性 和 非线性 两 大 派别 
线性 算法 有 著名 的 逻辑 回归 朴素 贝叶斯 最大熵 
等 非线性 算法 有 随机 森林 决策树 神经网络 核 机器 
等等 线性 算法 举 的 大旗 是 训练 和 预测 
的 效率 比 较高 但 最终 效果 对 特征 的 
依赖 程度 较高 需要 数据 在 特征 层面 上 是 
线性 可分 的 因此 使用 线性 算法 需要 在 特征 
工程 上 下 不少 功夫 尽量 对 特征 进行 选择 
变换 或者 组合 等 使得 特征 具有 区分 性 而 
非线性 算 法则 牛逼 点 可以 建模 复杂 的 分类 
面 从而 能 更好 的 拟合 数据 那在/nr 我们 选择 
了 特征 的 基础 上 哪个 机器学习 算法 能 取得 
更好 的 效果 呢 谁也 不 知道 实践 是 检验 
哪个 好 的 不二 标准 那 难道 要 苦 逼到 
写 五六个 机器学习 的 代码 吗 No 机器学习 社区 的 
力量 是 强大 的 码 农界的/nr 共识 是 不 重复 
造 轮子 因此 对 某些 较为 成熟 的 算法 总有 
某些 优秀 的 库 可以 直接 使用 省去 了 大伙 
调研 的 大部分 时间 基于 目前 使用 python 较多 而 
python 界 中 远近闻名 的 机器学习 库 要数 scikit learn 
莫属 了 这个 库 优点 很多 简单易用 接口 抽象 得 
非常 好 而且 文档 支持 实在 感人 本文 中 我们 
可以 封装 其中 的 很多 机器学习 算法 然后 进行 一次性 
测试 从而 便于 分 析取 优 当然 了 针对 具体 
算法 超 参 调 优 也 非常 重要 二 Scikit 
learn 的 python 实践 2.1 Python 的 准备 工作 Python 
一个 备受 欢迎 的 点 是 社区 支持 很多 有 
非常 多 优秀 的 库 或者 模块 但是 某些 库 
之间 有时候 也 存在 依赖 所以 要 安装 这些 库 
也是 挺 繁琐 的 过程 但 总 有人 忍 受不了 
这种 繁琐 都会 开发 出 不少 自动化 的 工具 来 
节省 各位 客官 的 时间 其中 个人 总结 安装 一个 
python 的 库 有 以下 三 种方法 1 Anaconda 这 
是 一个 非常 齐全 的 python 发行 版本 最新 的 
版本 提供 了 多达 195个 流行 的 python 包 包含 
了 我们 常用 的 numpy scipy 等等 科学计算 的 包 
有了 它 妈妈 再也 不用 担心 我 焦头烂额 地 安装 
一个 又 一个 依赖 包了 Anaconda 在手 轻松 我 有 
下载 地址 如下 http / / www . continuum . 
io / downloads2 Pip 使用 过 Ubuntu 的 人 对 
apt get 的 爱 只有 自己 懂 其实 对 Python 
的 库 的 下载 和 安装 可以 借助 pip 工具 
的 需要 安装 什么 库 直接 下载 和 安装 一条龙 
服务 在 pip 官网 https / / pypi . python 
. org / pypi / pip 下载安装 即可 未来 的 
需求 就在 # pip install xx 中 3 源码/n 包/v 
如果/c 上述/b 两种/m 方法/n 都/d 没有/v 找到/v 你/r 的/uj 库/n 
那你/nr 直接 把 库 的 源码 下载 回来 解压 然后 
在 目录 中会 有个 setup . py 文件 执行 # 
python setup . py install 即可 把 这个 库 安装 
到 python 的 默认 库 目录 中 2.2 Scikit learn 
的 测试 scikit learn 已经 包含 在 Anaconda 中 也 
可以 在 官方 下载 源码 包 进行 安装 本文 代码 
里 封装 了 如下 机器学习 算法 我们 修改 数据 加载 
函数 即可 一键 测试 classifiers = { NB naive _ 
bayes _ classifier KNN knn _ classifier LR logistic _ 
regression _ classifier RF random _ forest _ classifier DT 
decision _ tree _ classifier SVM svm _ classifier SVMCV 
svm _ cross _ validation GBDT gradient _ boosting _ 
classifier } train _ test . py # usr / 
bin / env python # * coding utf 8 * 
import sys import os import time from sklearn import metrics 
import numpy as np import cPickle as pickle reload sys 
sys . s e t d e f a u 
l t e n c o d i n g 
utf8 # Multinomial Naive Bayes Classifier def naive _ bayes 
_ classifier train _ x train _ y from sklearn 
. naive _ bayes import MultinomialNB model = MultinomialNB alpha 
= 0.01 model . fit train _ x train _ 
y return model # KNN Classifier def knn _ classifier 
train _ x train _ y from sklearn . neighbors 
import K N e i g h b o r 
s C l a s s i f i e 
r model = K N e i g h b 
o r s C l a s s i f 
i e r model . fit train _ x train 
_ y return model # Logistic Regression Classifier def logistic 
_ regression _ classifier train _ x train _ y 
from sklearn . linear _ model import L o g 
i s t i c R e g r e 
s s i o n model = L o g 
i s t i c R e g r e 
s s i o n penalty = l2 model . 
fit train _ x train _ y return model # 
Random Forest Classifier def random _ forest _ classifier train 
_ x train _ y from sklearn . ensemble import 
R a n d o m F o r e 
s t C l a s s i f i 
e r model = R a n d o m 
F o r e s t C l a s 
s i f i e r n _ estimators = 
8 model . fit train _ x train _ y 
return model # Decision Tree Classifier def decision _ tree 
_ classifier train _ x train _ y from sklearn 
import tree model = tree . D e c i 
s i o n T r e e C l 
a s s i f i e r model . 
fit train _ x train _ y return model # 
GBDT Gradient Boosting Decision Tree Classifier def gradient _ boosting 
_ classifier train _ x train _ y from sklearn 
. ensemble import G r a d i e n 
t B o o s t i n g C 
l a s s i f i e r model 
= G r a d i e n t B 
o o s t i n g C l a 
s s i f i e r n _ estimators 
= 200 model . fit train _ x train _ 
y return model # SVM Classifier def svm _ classifier 
train _ x train _ y from sklearn . svm 
import SVC model = SVC kernel = rbf probability = 
True model . fit train _ x train _ y 
return model # SVM Classifier using cross validation def svm 
_ cross _ validation train _ x train _ y 
from sklearn . grid _ search import GridSearchCV from sklearn 
. svm import SVC model = SVC kernel = rbf 
probability = True param _ grid = { C 1e 
3 1e 2 1e 1 1 10 100 1000 gamma 
0.001 0.0001 } grid _ search = GridSearchCV model param 
_ grid n _ jobs = 1 verbose = 1 
grid _ search . fit train _ x train _ 
y best _ parameters = grid _ search . best 
_ estimator _ . get _ params for para val 
in best _ parameters . items print para val model 
= SVC kernel = rbf C = best _ parameters 
C gamma = best _ parameters gamma probability = True 
model . fit train _ x train _ y return 
model def read _ data data _ file import gzip 
f = gzip . open data _ file rb train 
val test = pickle . load f f . close 
train _ x = train 0 train _ y = 
train 1 test _ x = test 0 test _ 
y = test 1 return train _ x train _ 
y test _ x test _ y if _ _ 
name _ _ = = _ _ main _ _ 
data _ file = mnist . pkl . gz thresh 
= 0.5 model _ save _ file = None model 
_ save = { } test _ classifiers = NB 
KNN LR RF DT SVM GBDT classifiers = { NB 
naive _ bayes _ classifier KNN knn _ classifier LR 
logistic _ regression _ classifier RF random _ forest _ 
classifier DT decision _ tree _ classifier SVM svm _ 
classifier SVMCV svm _ cross _ validation GBDT gradient _ 
boosting _ classifier } print reading training and testing data 
. . . train _ x train _ y test 
_ x test _ y = read _ data data 
_ file num _ train num _ feat = train 
_ x . shape num _ test num _ feat 
= test _ x . shape is _ binary _ 
class = len np . unique train _ y = 
= 2 print * * * * * * * 
* * * * * * * * * * 
* * * Data Info * * * * * 
* * * * * * * * * * 
* * * * * * print # training data 
% d # testing _ data % d dimension % 
d % num _ train num _ test num _ 
feat for classifier in test _ classifiers print * * 
* * * * * * * * * * 
* * * * * * * % s * 
* * * * * * * * * * 
* * * * * * * * * % 
classifier start _ time = time . time model = 
classifiers classifier train _ x train _ y print training 
took % fs % time . time start _ time 
predict = model . predict test _ x if model 
_ save _ file = None model _ save classifier 
= model if is _ binary _ class precision = 
metrics . precision _ score test _ y predict recall 
= metrics . recall _ score test _ y predict 
print precision % . 2f % % recall % . 
2f % % % 100 * precision 100 * recall 
accuracy = metrics . accuracy _ score test _ y 
predict print accuracy % . 2f % % % 100 
* accuracy if model _ save _ file = None 
pickle . dump model _ save open model _ save 
_ file wb 四 测试 结果 本次 使用 mnist 手写体 
库 进行 实验 http / / deeplearning . net / 
data / mnist / mnist . pkl . gz 共 
5万 训练样本 和 1万 测试 样本 代码运行 结果 如下 reading 
training and testing data . . . * * * 
* * * * * * * * * * 
* * * * * * * Data Info * 
* * * * * * * * * * 
* * * * * * * * * * 
# training data 50000 # testing _ data 10000 dimension 
784 * * * * * * * * * 
* * * * * * * * * * 
NB * * * * * * * * * 
* * * * * * * * * * 
* training took 0.287000 s accuracy 83.69% * * * 
* * * * * * * * * * 
* * * * * * KNN * * * 
* * * * * * * * * * 
* * * * * * * training took 31.991000 
s accuracy 96.64% * * * * * * * 
* * * * * * * * * * 
* * LR * * * * * * * 
* * * * * * * * * * 
* * * training took 101.282000 s accuracy 91.99% * 
* * * * * * * * * * 
* * * * * * * * RF * 
* * * * * * * * * * 
* * * * * * * * * training 
took 5.442000 s accuracy 93.78% * * * * * 
* * * * * * * * * * 
* * * * DT * * * * * 
* * * * * * * * * * 
* * * * * training took 28.326000 s accuracy 
87.23% * * * * * * * * * 
* * * * * * * * * * 
SVM * * * * * * * * * 
* * * * * * * * * * 
* training took 3152.369000 s accuracy 94.35% * * * 
* * * * * * * * * * 
* * * * * * GBDT * * * 
* * * * * * * * * * 
* * * * * * * training took 7623.761000 
s accuracy 96.18% 在 这个 数据 集中 由于 数据分布 的 
团 簇 性 较好 如果 对 这个 数据库 了解 的话 
看 它 的 t SNE 映射 图 就 可以 看 
出来 由于 任务 简单 其 在 deep learning 界 已 
被 认为 是 toy dataset 因此 KNN 的 效果 不赖 
GBDT 是个 非常 不错 的 算法 在 kaggle 等 大 
数据 比 赛中 状元 探花 榜眼 之列 经常 能见 其 
身影 三个臭皮匠 赛过诸葛亮 还是 被 验证 有 道理 的 特别 
是 三个臭皮匠 还 能力 互补 的 时候 还有 一个 在 
实际 中 非常 有效 的 方法 就是 融合 这些 分类器 
再 进行 决策 例如 简单 的 投票 效果 都 非常 
不错 建议 在 实践 中 大家 都 可以 尝试 下 
