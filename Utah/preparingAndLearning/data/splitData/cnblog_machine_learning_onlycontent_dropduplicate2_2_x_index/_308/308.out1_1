今天 在 研究 点 云 分割 的 时候 终于 走完 
了 所有 的 传统 路子 走到 了 基于 机器 学习 
的 分割 与 传统 自底向上 分割 的 分界点 CRF 算法 
好吧 MIT 的 老 教授 说 的 对 其实 你 
很难 真正 绕过 某个 问题 数学 如是 人生 也 如是 
记 我 的 机器学习 之路 1 机器学习 在 之前 的 
学习 过程 中 机器学习 对 我 而言 实在 是 洪水猛兽 
般的 存在 很多 玄玄 乎乎 的 公式 算法 各种 算法 
的 名字 一看 就 比较 高级 如 黑箱 一般 的 
过程 摸不清 的 物理 意义 繁杂 的 公式 实在 是 
让人 头大 为了 能 更好 的 学习 PGM 我 决定 
放弃 由 从 神经 网络 或者 深度 学习 一类 的 
算法 入手 的 打算 改由 统计 概率 与 推断 入手 
来 学习 Learning . 实际上 如果 你 只看 中 利用 
机器学习 来 做什么 那么 完全 不 需要 头疼 它 到底 
是 怎么 Learning 的 Learning 已经 成了/nr 一个 抽象类 对 
所有 的 问题 都 可以 用 Learn graph _ learn 
new learn graph _ learn learn . train learn . 
app learn . result 的 方法 来 完成 至于 how 
to train how to extract result 显然 已经 被 各路 
大神 封装 的 妥妥 的 了 但是 如果 真的 想 
要从 机器学习 中 获得 启发 或者 是 将其 与 自己 
的 专业 结合 的 更 紧密 可能 还是 要 对 
这个 东西 开膛 破肚 吧 2 . 极大 似 然 
估计 估计 estimating 是 一种 手段 一种 对模型 参数 进行 
推测 的 手段 说白了 就是 利用 训练 数据 对 模型 
进行 calibration . 在 标定 之前 首先 需要 有 一个 
模型 模型 中 需要 有待 辨识 的 参数 极大 似 
然 估计 是 现象 原理 的 过程 这个 世界 上 
已经 发生 的 都 必然 会 发生 given by Kang 
. YH _ HUST 遵循 这个 原理 求解 模型 参数 
就成 了 一个 寻优 的 过程 当 给定 模型 与 
结果 时间 发生 的 概率 p 是 参数 a 的 
函数 其 原理 如下 f x1 . . . xn 
θ = ∏   f θ xj θ . L 
θ x1 . . . xn = f x1 . 
. . xn θ . θ = argmax θ L 
θ x1 . . . xn . 由于 模型 是 
已知 的 那么 对 给定 观测值 其 对应 概率 p 
是 可以 表达 的 根据 此 原理 可以 求解 以下 
两个 问题 1 . 如果 θ 是 二项分布 的 参数 
那么 在 给定 一组 结果 的 情况 下 似 然 
函数 L 可 表达 为 对 似 然 函数 求导 
可 得当 θ = h / n 时似然/nr 函数 取 
最大值 h 是 x = 1 的 次数 n 是 
实验 总数 2 . 如果 L 函数 是 线性 的 
那么 可以 对 L 函数 取 对数 取 完 对数 
后 结果 是 一样 的 3 . 条件 似 然 
估计 说到 条件概率 好像 事情 就 变得 有点 复杂 y 
| x 总是 给 人 一种 先有 x 后有 y 
的 感觉 但 其实 x y 并 不是 先有 鸡 
后有 蛋 的 关系 x y 应该 理解 为 y 
受到 x 的 约束 会 随着 x 的 改变 而 
改变 y 的 取值 条件 所 表达 的 意义 在于 
约束 而非 顺序   如果 用 约束 来 解释 这个 
关系 那么 接下来 的 事情 就 好 理解 了 1 
如果 y 受到 x 向量 的 约束 y 的 取值 
是 随着 x 变化 的 2 为 简化 约束 假定 
x 各个 分量 对 y 的 约束 是 线性组合 的 
不同 的 分量 有 不同 的 权重 上面 是 一个 
简单 粗暴 的 模型 x 的 各个 分量 可以 看作 
是 x 的 不同 特征 比如 点 云 颜色 点 
云 密度 点 云 曲率 都 可以 看作 是 对 
该 是否 该 出现 0 1 问题 有影响 那么 就 
可以 设计 基于 条件概率 的 点 云 滤波器 有了 上述 
概念 以后 就 需要 把 它 量化 有 两点 需要 
量化 1 . x 的 加权 和在 哪个 范围 2 
. 如何 让 y 的 概率 落在 0 ~ 1 
显然   logistic regression model 能够 很好 的 解决 量化 
问题 对 x 各个 部分 的 权重 而言 取值 可以 
从 负无穷 到 正无穷 对 y 的 概率 而言 取值 
用于 在 0 ~ 1 之间 这个 系统 中 需要 
辨识 的 参数 就 仅仅是 beta alpha 可以 看作 是 
beta0 x0 = 1 要 辨识 beta 并 不是 一件 
容易 的 事情 对 所有 给定 x 我 需要 算出 
它 对应 的 p 并把 表达式 相乘 再 优化 求解 
显然 这是 笨办法 先 对 L beta 函数 做些 处理 
才 是 上策 因为 对 Y 而言 其为 二项分布 丢 
硬币 要么 正面 要么 反面 但是 硬币 可以 有 bias 
这个 bias 可以 由 很多 事情 决定 二项分布 的 极大 
似 然 估计 是 已知 的 似 然 函数 取了 
对数 如果 等式 两项 同时 向 beta 求导 则有 p 
不仅仅 是 p p 还是 beta 的 函数 那么 则有 
OK 好像 令 上式 为 0 则可 求出 beta 对 
xij yij 的 表达式 然而 却 没有 那么 简单 3 
学习 机器 也 抄近 道 这个 东西 不 那么 简单 
的 原因 在于 pi 是 一个 非常 复杂 的 表达式 
虽然 任意 一个 beta 都是 独立 的 但是 要 把 
所有 的 xi 全部 代 进去 再 求 最小值 也没 
那么 简单 虽然 这个 最小值 肯定 存在 但 如果 没有 
beta 你 也算 不 出来 啊 陷入 了 该 先 
拿 身份证 还是 先 开锁 的 困境 中 所谓 的 
学习 training 本质上 是 一种 数值 求解 方法 一般 情况 
下 可以 表达 为 梯度 下 降法 它 是 这样 
的 在 已经 假 设了 beta 向量 的 初始值 情况下 
beta j 的 梯度 是 已经 得知 了 的 那么 
对 整个 训练 集 i 全部 遍历 一遍 就 可以 
算出 梯度 由 梯度 再 刷新 Beta _ j 伪代码 
大约 如下 while all _ beta = coverge for j 
= 1 end _ of _ featuresfor i = 1 
end _ of _ training _ setPartial _ Beta _ 
j + = yi pi xijend beta j = beta 
j + lamda * Partial _ Beta _ j endend 
显然 这 很 不科学 对 每个 beta j 我 都 
需要 遍历 整个 训练 集 最后 才 改变 了 一点点 
然后 我 又要 遍历 整个 训练 集 如果 训练 集 
很大 那 岂不 是 坑爹 所以 我们 需要 一个 抄近 
道 的 方法 成为 随机 梯度 下 降法 这个 方法 
的 原理 其实 很 简单 训练 集 包含 的 信息 
是 一致 的 不是 一样 的 是 一致 的 假设 
我们 能 找到 一个 随机变量 Z 使 Z 满足 那么 
只要 Z 的 取值 次数 够多 我们 就 可以 利用 
Zj 来 替代 E Zj 取 足够 多次 那么 他们 
最终 效果 的 相同 的 不妨 取 由于 n 是 
一个 常数 训练 集 的 规模 那么 Beta j 也就 
变成 了 终于 整个 训练 过程 被 简化 成了 1 
. for epcho = 1 3 2 . for i 
= 1 end _ of _ training _ set 3 
. for j = 1 end _ of _ features 
Partial _ Beta _ j + = yi pi xij 
beta j = beta j + lamda * Partial _ 
Beta _ j end end train _ set . rand 
_ rank end 其中 epcho 被 用作 强制 收敛 一般 
是 3 ~ 100 第二个 for 和 第三 个 for 
是 可以 对调 位置 的 据说 会 导致 收敛 变慢 
该 方法 叫做 cordianate ascent . Ok 学习 的 问题 
基本上 解决 好了 还抄 了点 近道 想想 就 有些 小 
开心 ~ 接下来 还要 解决 1个 扫尾 的 问题 搞定 
了 的话 我们 也 算 玩过 机器学习 啦啦 啦啦 ~ 
~ ~ 问题 如果 训练 集合 处处 点 云 密度 
为 1 且 只要 这个 特征 出现 点 云就被/nr 认为 
是 无效 的 那么 随着 训练 的 增加 beta 就会 
一直 上升 直到 无穷大 当然 当 训练 集 非常 合理 
特征 量化 也 非常 合理 的 时候 上面 的 那个 
问题 不会 出现 但 显然 不会 如此 十全十美 所以 简单 
粗暴 的 引入 一个 惩罚 因子 来 防止 beta 跑 
飞 . u 是 常数 当 beta 过大 或者 过小 
的 时候 惩罚 因子 能 起到 纠偏 的 作用 但 
结果 会 带来 误差 4 将 简单 的 机器学习 用于 
点 云 滤波 显然 上式 我们 已经 得到 了 beta 
关于 xij yi 的 表达式 接下来 要做 的 很简单 1 
. 量化 曲率 x1 颜色 x2 密度 x3 根据 经验 
都 要在 1 ~ 1 之间 方差 最好 为 1 
2 . 指定 一些 点 是 或者 不是 某 点 
云的点/nr y 0/1 3 . 训练 我们 就 得到 了 
一个 判断 某 点 是否 属于 该点 云的/nr 一个 简单 
判别 器 本质 上 就是 个 概率函数 当 遇到 某个 
点 时 可以 算 它 是 有效 点 还是 无效 
点 的 概率 其实 机器学习 没有 那么 难 ~ 也 
没有 黑 ~ ~ ~ ~ 