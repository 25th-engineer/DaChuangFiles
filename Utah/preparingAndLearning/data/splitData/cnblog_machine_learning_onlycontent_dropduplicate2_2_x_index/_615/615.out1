本节 使用 的 算法 称为 ID3 另一个 决策树 构造 算法 
CART 以后 讲解 一 概述 我们 经常 使用 决策树 处理 
分类 问题 它 的 过程 类似 二十 个 问题 的 
游戏 参与 游戏 的 一方 在 脑海 里 想 某个 
事物 其他 参与者 向他/nr 提出 问题 只允许 提 20个 问 
题 问题 的 答案 也 只能 用 对 或 错 
回答 问 问题 的 人 通过 推断 分解 逐步 缩小 
带 猜测 事物 的 范围 如 所示 的 流程 图 
就是 一个 决策树 长方形 代表 判断 模块 decision block 椭圆形 
代表 终止 模块 terminating block 表示 已经 得出结论 可以 终止 
运行 从 判断 模块 引出 的 左右 箭头 称作 分支 
branch 它 可以 到达 另一个 判断 模块 或 终止 模块 
图 1 构造 了 一个 假象 的 邮件 分类 系统 
它 首先 检测 发送 邮件 域名地址 如果 地址 为 myEmployer 
. com 则将 其 放在 分类 无聊 时 需要 阅读 
的 邮件 中 如 果 邮件 不是 来自 这个 域名 
则 检查 内容 是否 包括 单词 曲棍球 如果 包含 则将 
邮件 归类 到 需要 及时 处理 的 朋友 邮件 否则 
将 邮件 归类 到 无须 阅读 的 垃圾 邮件 二 
优缺点 优点 计算 复杂度 不高 输出 结果 易于 理解 对 
中间值 的 缺失 不 敏感 可以 处理 不 相关 特征 
数据 缺点 可能 会 产生 过度 匹配 问题 适用 数据类型 
离散 型 和 连续型 三 数学公式 如果 待 分类 的 
数据集 可能 划分 在 多个 分类 之中 则 符号 Xi 
定义 为 x 其中 p xi 是 选择 该 分类 
的 概率 其中 n = 数据集 分类 数 例如 数据集 
的 分类 为 lables = A B C B A 
B 则 P A = 2/6 = 0.3333 P B 
= 3/6 = 0.5 P C = 1/6 = 0.277 
数据集 的 熵 H = P A * log2P A 
+ P B * log2P B + P C * 
log2P C 四 树 的 构造 在 构造 决策树 时 
我们 需要 解决 的 第一 个 问题 就是 当前 数据集 
上 哪个 特征 在 划分 数据 分类 时起/nr 决定性 作用 
为了 找到 决定性 的 特征 划分 出 最好 的 结果 
我们 必须 评估 每个 特征 我们 假设 已经 根据 一定 
的 方法 选取 了 待 划分 的 特征 则 原始 
数据集 将 根据 这个 特征 被 划分 为 几个 数据 
子集 这 数据 子 集会 分布 在 决策 点 关键 
特征 的 所有 分支 上 如果 某 个 分支 下 
的 数据 属于 同一 类型 则 无需 进一步 对 数据 
集 进行 分割 如果 数据 子集 内 的 数据 不 
属于 同一 类型 则 需要 递归 地 重复 划分 数据 
子集 的 过程 直到 每个 数据 子集 内 的 数据 
类型 相同 如何 划分 子集 的 算法 和 划分 原始 
数据集 的 方法 相同 创建 分支 的 过程 用 伪代码 
表示 如下 检测 数据 集中 的 每个 子项 是否 属于 
同一 类型 If Yes return 类 标签 Else 寻找 划分 
数据集 的 最好 特征 划分 数据集 创建 分支 节点 for 
每个 划分 的 子集 递归调用 本 算法 并 添加 返回 
结果 到 分支 节 点中 这 是个 递归 return 分支 
节点 决策树 的 一般 流程 收集 数据 可以 使用 任何 
方法 准备 数据 树 构造 算法 只 适用 于 标称 
数据 因此 数值 型 数据 必须 离散化 分析 数据 可以 
使用 任何 方法 构造 树 完成 之后 我们 应该 检查 
图形 是否 符合 预期 训练 算法 构造 树 的 数据结构 
测试 算法 使用 经验 树 计算 错误率 使用 算法 此 
步骤 可以 适用 于 任何 监督 学习 算法 而 使用 
决策树 可以 更好 地 理解 数据 的 内在 含义 一些 
决策树 算法 使用 二分法 划分 数据 本书 将 使用 ID3 
算法 划分 数据集 该算 法 处理 如何 划分 数据集 何时 
停止 划分 数据集 每次 划分 数据集 我们 只 选取 一个 
特征 属性 那么 应该 选择 哪个 特征 作为 划分 的 
参考 属性 呢 表 1 的 数据 包含 5个 海洋 
动物 特征 包括 不 浮出水面 是否 可以 生存 以及 是否 
有脚噗/nr 我们 可以 将 这些 动物 分成 两类 鱼类 和非/nr 
鱼类 表 1 海洋生物 数据 不 浮出水面 是否 可以 生存 
是否 有 脚蹼 属于 鱼类 1 是 是 是 2 
是 是 是 3 是否 否 4 否 是否 5 
否 是否 五 信息 增益 划分 数据集 的 大 原则 
是 将 无序 的 数据 变得 更加 有序 我们 可以 
使用 多种 方法 划分 数据集 但是/c 每/zg 种/m 方法/n 都有/nr 
各自/r 的/uj 优缺点/n 组织 杂乱无章 数据 的 一种 方法 就是 
使用 信息论 度量 信息 信息论 是 量化 处理 信息 的 
分支 科学 我们 可以 在 划分 数据 之前 或之后 使用 
信息论 量化 度量 信息 的 内容 在 划分 数据集 之前 
之后 信息 发生 的 变化 成为 信息 增益 我们 可以 
计算 每个 特征 划分 数据集 获得 的 信息 增益 获得 
信息 增益 最高 的 特征 就是 最好 的 选择 集合 
信息 的 度量 方式 成为 香农 熵 或者 简称为 熵 
为了 计算 熵 我们 需要 计算 所有 类型 所有 可能 
值 包含 的 信息 的 期望值 通过 下面 的 公式 
得到 其中 n 是 分类 的 数目 下面 给出 计算 
信息熵 的 Python 函数 创建 名为 trees . py 文件 
添加 如下 代码 1 def createDataSet 2 dataSet = 1 
1 yes 3 1 1 yes 4 1 0 no 
5 0 1 no 6 0 1 no 7 labels 
= no surfacing flippers 8 # change to discrete values 
9 return dataSet labels 10 def calcShannonEnt dataSet 11 numEntries 
= len dataSet # 获取数据 行数 numEntries = 5 12 
labelCounts = { } 13 for featVec in dataSet # 
the the number of unique elements and their occurance 14 
currentLabel = featVec 1 15 if currentLabel not in labelCounts 
. keys labelCounts currentLabel = 0 16 labelCounts currentLabel + 
= 1 17 shannonEnt = 0.0 18 print labelCounts = 
labelCounts # labelCounts = { yes 2 no 3 } 
19 for key in labelCounts 20 prob = float labelCounts 
key / numEntries # prob 为 每个 值 出现 的 
概率 21 shannonEnt = prob * log prob 2 # 
数学公式 计算 熵 22 print shannonEnt = shannonEnt 23 return 
shannonEnt 测试代码 1 d l = trees . createDataSet 2 
d 3 1 1 yes 1 1 yes 1 0 
no 0 1 no 0 1 no 4 l 5 
no surfacing flippers 6 c = trees . calcShannonEnt d 
7 labelCounts = { yes 2 no 3 } 8 
shannonEnt = 0 . 9709505944546686 9 calcShannonEnt 返回 整个 数据集 
的 熵 上面 测试代码 数据集 的 熵 = 0 . 
9709505944546686 熵 值 越高 则 混合 的 数据 也 越多 
得到 熵 之后 我们 就 可以 按 最大 信息 增益 
的 方法 划分 数据集 六 划分 数据集 上面 我们 学习 
了 如何 度量 数据集 的 无序 程序 分类 算法 除了 
需要 测量 信息熵 还 需要 划分 数据集 度量 划分 数据集 
的 熵 以便 判断 当前 是否 正确 地 划分 了 
数据集 我们 将 对 每个 特征 划分 数据集 的 结果 
计算 一次 信息熵 然后 判断 按照 哪个 特征 划分 数据集市 
最好 的 划分 方法 按照 给定 的 特征 划分 数据集 
1 def splitDataSet dataSet axis value # 查找 数据集 dataSet 
第 axis 列 值 = = value 的 元素 再 
排除 第 axis 列 的 数据 组成 一个 新的 数据集 
2 retDataSet = 3 for featVec in dataSet 4 if 
featVec axis = = value 5 reducedFeatVec = featVec axis 
# chop out axis used for splitting 6 reducedFeatVec . 
extend featVec axis + 1 7 retDataSet . append reducedFeatVec 
8 return retDataSet 该 函数 使用 了 三个 输入 参数 
带 划分 的 数据 集 划分 数据集 的 特征 数据集 
第 几列 需要 返回 的 特征 的 值 按 哪个 
值 划分 函数 先 选取 数据 集中 第 axis 个 
特征值 为 value 的 数据 从这/nr 部分 数据 中 去除 
第 axis 个 特征 并 返回 测试 这个 函数 效果 
如下 1 myDat labels = trees . createDataSet 2 myDat 
3 1 1 yes 1 1 yes 1 0 no 
0 1 no 0 1 no 4 trees . splitDataSet 
myDat 0 1 # 查找 第 0列 值 = = 
1 的 元素 再 排除 第 0列 的 数据 组成 
一个 新的 数据集 5 1 yes 1 yes 0 no 
6 trees . splitDataSet myDat 0 0 7 1 no 
1 no 接下来 我们 将 遍历 整个 数据集 循环 计算 
香农 熵 和 splitDataSet 函数 找到 最好 的 特征 划分 
方式 熵 计算 将会 告诉 我们 如何 划分 数据集 是 
最好 的 数据 组织 方式 选择 最好 的 数据 集 
划分 方式 1 def c h o o s e 
B e s t F e a t u r 
e T o p l i t dataSet 2 numFeatures 
= len dataSet 0 1 # 这里 的 dataSet 最后 
一 列 是 分类 numFeatures = 2 我们 按 2列 
数据 进行 划分 3 baseEntropy = calcShannonEnt dataSet # 计算 
出 整个 数据 数据集 的 熵 4 bestInfoGain = 0.0 
bestFeature = 1 5 for i in range numFeatures # 
循环 每 一列 特征 6 featList = example i for 
example in dataSet # 创建 一个 新的 列表 存放 数据集 
第 i 列 的 数据 7 uniqueVals = set featList 
# 使用 集合 把 数据 去 重 8 newEntropy = 
0.0 # 以下 是 计算 每 一列 的 熵 求 
某 列 最大 的 熵 9 for value in uniqueVals 
# 循环 第 i 列 的 特征值 10 subDataSet = 
splitDataSet dataSet i value # 划分 数据集 11 prob = 
len subDataSet / float len dataSet # 子 数据集 所占 
的 比例 12 newEntropy + = prob * calcShannonEnt subDataSet 
# 子 数据集 的 熵 * 比例 13 infoGain = 
baseEntropy newEntropy # calculate the info gain ie reduction in 
entropy 14 if infoGain bestInfoGain # compare this to the 
best gain so far 15 bestInfoGain = infoGain # if 
better than current best set to best 16 bestFeature = 
i 17 return bestFeature # 返回 按 某 列 划分 
数据集 的 最大 熵/g 本/r 函数/n 使用/v 变量/vn bestInfoGain/w 和/c 
bestFeature/w 记录/n 最好/a 的/uj 信息/n 增益/n 和/c 对应/vn 的/uj 特征/n 
numFeatures 记录 特征 维数 依次 遍历 各个 特征 计算 该 
特征值 的 集合 uniqueVals 遍历 该 特征 计算 使用 该 
特征 划分 的 熵 newEntropy 据此 计算 新的 信息 增益 
infoGain 比较/d infoGain/w 和/c bestInfoGain/w 记录/n 信息/n 增益/n 的/uj 最大值/l 
和/c 对应/vn 特征/n 最终 返回 最大 的 信息 增益 对应 
特征 的 索引 测试 上面 代码 的 实际 输出 结果 
1 myData labels = trees . createDataSet 2 trees . 
c h o o s e B e s t 
F e a t u r e T o p 
l i t myData 3 labelCounts = { yes 2 
no 3 } 4 shannonEnt = 0 . 9709505944546686 5 
labelCounts = { no 2 } 6 shannonEnt = 0.0 
7 labelCounts = { yes 2 no 1 } 8 
shannonEnt = 0 . 9182958340544896 9 labelCounts = { no 
1 } 10 shannonEnt = 0.0 11 labelCounts = { 
yes 2 no 2 } 12 shannonEnt = 1.0 13 
0 七 递归 构建 决策树 构建 决策树 的 算法 流程 
如下 得到 原始 数据集 基于 最好 的 属性值 划分 数据集 
由于 特征值 可能 多 于 两个 因此 可能 存在 大 
于 两个 分支 的 数据集 划分 第一 次 划分 之后 
数据 将被 向下 传递 到 树 分支 的 下 一个 
节点 在 这个 节点 上 我们 可以 再次 划分 数据 
我们 可以 采用 递归 的 原则 处理 数据集 递归 结束 
的 条件 是 程序 遍历 完 所有 划分 数据集 的 
属性 或者 每个 分支 下 的 所有 实例 都 具有 
相同 的 分类 添加 下面 的 程序 代码 1 def 
majorityCnt classList # 返回 出现 次数 最多 的 类别 类似于 
K 近邻 算法 中 返回 前 K 中 类别 出现 
最多 次数 的 2 classCount = { } 3 for 
vote in classList 4 if vote not in classCount . 
keys classCount vote = 0 5 classCount vote + = 
1 6 sortedClassCount = sorted classCount . iteritems key = 
operator . itemgetter 1 reverse = True 7 return sortedClassCount 
0 0 8 9 def createTree dataSet labels 10 classList 
= example 1 for example in dataSet # 获取 数据集 
的 所有 类别 11 if classList . count classList 0 
= = len classList 12 return classList 0 # 如果 
数据集 的 所有 类别 都 相同 则 不 需要 划分 
13 if len dataSet 0 = = 1 # 使用 
完 了 所有 特征 仍然 不能 将 数据 划分 到 
某个 类别 上 的话 返回 出现 次数 最多 的 类别 
14 return majorityCnt classList 15 bestFeat = c h o 
o s e B e s t F e a 
t u r e T o p l i t 
dataSet # 获取 数据 集中 按 哪 一列 进行 划分 
16 bestFeatLabel = labels bestFeat # bestFeatLabel = 列 描述 
17 myTree = { bestFeatLabel { } } # 创建 
一个 字典 18 del labels bestFeat # 删除 已 计算 
过 的 列 19 featValues = example bestFeat for example 
in dataSet 20 uniqueVals = set featValues # 获取 某 
列 所有 不 重复 值 21 for value in uniqueVals 
22 subLabels = labels # copy all of labels so 
trees don t mess up existing labels 23 myTree bestFeatLabel 
value = createTree splitDataSet dataSet bestFeat value subLabels # 递归 
24 return myTree 25majorityCnt 函数 统计 classList 列表 中 每个 
类型 标签 出现 频率 返回 出现 次数 最多 的 分类 
名称 createTree 函数 使用 两 个 输入 参数 数据集 dataSet 
和 标签 列表 labels 标签 列表 包含 了 数据 集中 
所有 特征 的 标签 算法 本身 并 不需 要 这个 
变量 但是 为了 给出 数据 明确 的 含义 我们 将 
它 作为 一个 输入 参数 提供 上述 代码 首先 创建 
了 名为 classList 的 列表 变量 其中 包含 了 数据集 
的 所有 类 标签 列表 变量 classList 包含 了 数据集 
的 所有 类 标签 递归函数 的 第一 个 停止 条件 
是 所有 类 标签 完全相同 则 直接 返回 该类 标签 
递归函数 的 第二 个 停 止 条件 是 使用 完 
了 所有 特征 仍然 不能 将 数据集 划分 成仅/nr 包含 
唯一 类别 的 分组 这里 使用 majorityCnt 函数 挑选 出现 
次数 最多 的 类别 作为 返回值 下 一步 程序 开始 
创 建树 这里 直接 使用 Python 的 字典 类型 存储 
树 的 信息 字典 变量 myTree 存储 树 的 所有 
信息 当前 数据集 选取 的 最好 特征 存储 在 变量 
bestFeat 中 得到 列表 中 包含 的 所有 属性值 最后 
代码 遍历 当前 选择 特征 包含 的 所有 属性值 在 
每个 数据集 划分 上 递归 待用 函数 createTree 得到 的 
返回值 将被 插入 到 字典 变量 myTree 中 因此 函数 
终止 执行 时 字典 中将 会 嵌套 很多 代表 叶子 
节点 信息 的 字典 数据 注意 其中 的 subLabels = 
labels 复制 了 类 标签 因为 在 递归调用 createTree 函数 
中会 改变 标签 列表 的 值 测试 这些 函数 1 
myDat labels = trees . createDataSet 2 myTree = trees 
. createTree myDat labels 3 myTree 4 { no surfacing 
{ 0 no 1 { flippers { 0 no 1 
yes } } } } 变量 myTree 包含 了 很多 
代表 树结构 信息 的 嵌套 字典 从 左边 开始 第一 
个 关键字 no surfacing 是 第一 个 划分 数据集 特征 
的 名称 该 关键字 的 值 也 是 一个 字典 
八 测试 算法 使用 决策树 执行 分类 以及 决策树 的 
存储 依靠 训练 数据 构造 了 决策树 之后 我们 可以 
将 它 用于 实际 数据 的 分类 在 执行 数据 
分类 时 需要 决策树 以及 用于 决策树 的 标签 向量 
然后 程序 比较 测试数据 与 决策 树上 的 数值 递归 
执行 该 过程 直到 进入 叶子 结点 最后 将 测试数据 
定义 为 叶子 结点 所属 的 类型 使用 决策树 分类 
的 函数 1 def classify inputTree featLabels testVec 2 firstStr 
= inputTree . keys 0 3 secondDict = inputTree firstStr 
4 featIndex = featLabels . index firstStr 5 key = 
testVec featIndex 6 valueOfFeat = secondDict key 7 if isinstance 
valueOfFeat dict 8 classLabel = classify valueOfFeat featLabels testVec 9 
else classLabel = valueOfFeat 10 return classLabel 测试 上面 的 
分类 函数 myDat labels = trees . createDataSet myTree = 
trees . createTree myDat labels trees . classify myTree labels 
1 0 no trees . classify myTree labels 1 1 
yes 可以 使用 Python 模块 pickle 序列化 对象 参见 下面 
的 程序 序列化 对象 可以 在 磁盘 上 保存 对象 
并在 需要 的 时候 读 取出来 1 def storeTree inputTree 
filename 2 import pickle 3 fw = open filename w 
4 pickle . dump inputTree fw 5 fw . close 
6 7 def grabTree filename 8 import pickle 9 fr 
= open filename 10 return pickle . load fr 九 
示例 使用 决策树 预测 隐形眼镜 类型 fr = open lenses 
. txt lensens = inst . strip . split \ 
t for inst in fr . readlines lensensLabels = age 
prescript astigmatic tearRate lensesTree = trees . createTree lensens lensensLabels 
lensensTree 执行 结果 { tearRate { normal { astigmatic { 
yes { prescript { hyper { age { pre no 
lenses presbyopic no lenses young hard } } myope hard 
} } no { age { pre soft presbyopic { 
prescript { hyper soft myope no lenses } } young 
soft } } } } reduced no lenses } } 
十 总结 一下 计算 整个 数据集 的 熵 根据 第 
1处 计算 出来 的 熵 再 计算 数据集 按 哪 
一列 划分 最为 合适 计算 数据集 每 一列 的 熵 
根据 所有 列 计算 出来 的 熵 获取 最佳 列 
设 此处 计算 出 最佳 列为 I 获取 数据集 第 
I 列 所有 不 重复 值 的 集合 设 此处 
集 合为 Mfor v in M 循环 集合 M 根据 
I 列 和 v 值 划分 数据集 再 递归 运算 
