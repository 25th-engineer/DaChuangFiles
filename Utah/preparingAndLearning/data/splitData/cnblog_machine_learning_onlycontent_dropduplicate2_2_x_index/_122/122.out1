gbdt 又称 Gradient Boosted Decision Tree / Grdient Boosted Regression 
Tree 是 一种 迭代 的 决策树 算法 该 算法 由 
多个 决策树 组成 它 最早 见于 yahoo 后被 广泛 应用 
在 搜索 排序 点击率 预估 上 xgboost 是 陈天奇 大牛 
新 开发 的 Boosting 库 它 是 一个 大 规模 
分布式 的 通用 Gradient Boosting GBDT 库 它 在 Gradient 
Boosting 框架 下 实现 了 GBDT 和 一些 广义 的 
线性 机器学习 算法 本文 首先 讲解 了 gbdt 的 原理 
分析 了 代码 实现 随后 分析 了 xgboost 的 原理 
和 实现 逻辑 本文 的 目录 如下 一 GBDT1 . 
GBDT 简介 2 . GBDT 公式 推导 3 . 优缺点 
4 . 实现 分析 5 . 常用 参数 和调优/nr 二 
Xgboost1 . Xgboost 简介 2 . Xgboost 公式 推导 3 
. 优缺点 4 . 实现 分析 5 . 常用 参数 
和调优/nr 一 GBDT / GBRT1 . GBDT 简介 GBDT 是 
一个 基于 迭代 累加 的 决策树 算法 它 通过 构造 
一组 弱 的 学习 器 树 并把 多颗 决策树 的 
结果 累 加起来 作为 最终 的 预测 输出 树 模型 
也 分为 决策树 和 回归 树 决策树 常 用来 分类 
问题 回归 树 常 用来 预测 问题 决策树 常 用于 
分类 标签 值 比如 用户 性别 网页 是否 是 垃圾 
页面 用户 是不是 作弊 而 回归 树 常 用于 预测 
真 实数值 比如 用户 的 年龄 用户 点击 的 概率 
网页 相关 程度 等等 由于 GBDT 的 核心 在与 累加 
所有 树 的 结果 作为 最终 结果 而 分类 结果 
对于 预测 分类 并 不是 这么 的 容易 叠加 稍等 
后面 会 看到 其实 并 不是 简单 的 叠加 而是/c 
每一步/i 每/zg 一棵树/ns 拟合/v 的/uj 残差/n 和/c 选择/v 分/v 裂点/n 
评价/n 方式/n 都是/nr 经过/p 公式/n 推导/v 得到/v 的/uj 所以 GBDT 
中的 树 都是 回归 树 其实 回归 树 也能 用来 
做 分类 的 哈 同样 的 我们 经常 会把 RandomForest 
的 思想 引入 到 GBDT 里面 来 即 每 棵树 
建树 的 时候 我们 会对 特征 和 样本 同时 进行 
采样 然后 对 采样 的 样本 和 特征 进行 建树 
好啦 既然 每 棵树 拟合 的 值 预测值 分 裂点 
选取 都 不是 随便 选取 的 那么 到底 是 如何 
选择 的 呢 我们 先 进入 GBDT 的 公式 推导 
吧 2 . GBDT 公式 推导 我们 都 知道 LR 
的 映射函数 是 损失 函数 是 对于 分类 问题 来说 
我们 一般 选取 映射函数 构造 损失 函数 然后 逐步 求解 
使得 损失 函数 最小化 就行了 可是 对于 回归 问题 求解 
的 方式 就 有些 不同 了 那么 GBDT 的 目标 
函数 和 损失 函数 分别 又是 什么 的 呢 每个 
树 拟合 的 残差 是 什么 呢 首先 GBDT 和 
LR 是 不 一样 的 因为 GBDT 是 希望 组合 
一组 弱 的 学习 器 的 线性组合 于是 我们 有 
那么 它 的 目标 函数 就 如下 其中 如上 公式 
中 是 p 步长 而 h x am 是 第 
m 颗 树 的 预测 值 梯度方向 我们 可以 在 
函数 空间 上 形式 使用 梯度 下 降法 求解 首先 
固定 x 对 F x 求其 最优 解 下面 给出 
框架 流程 和 Logloss 下 的 推导 框架 流程 如下 
我们 需要 估计 g _ m x 这里 使用 决策树 
实现 去 逼近 g _ m x 使得 两者 之间 
的 距离 尽可能 的 近 而 距离 的 衡量 方式 
有 很多 种 比如 均方 误差 和 Logloss 误差 我 
在 这里 给出 Logloss 损失 函 数下 的 具体 推导 
GBDT 预测值 到 输出 概率 0 1 的 sigmoid 转换 
下面 我们 需要 首先 求解 F0 然后再 求解 每个 梯度 
Step 1 . 首先 求解 初始值 F0 令其 偏 导 
为 0 实 现时 是 第 1颗 树 需要 拟合 
的 残差 Step2 . 估计 g _ m x 并用 
决策树 对其 进行 拟合 g _ m x 是 梯度 
实 现时 是 第 m 颗 树 需要 拟合 的 
残差 Step3 .   使用 牛顿 法 求解 下降 方向 
步长 r _ jm 是 拟合 的 步长 实 现时 
是 每 棵树 的 预测 值 Step4 . 预 测时 
就 很简单 啦 把 每 棵树 的 预测 值 乘以 
缩放 因子 加到 一起 就 得到 预测值 啦 注意 如果 
需要 输出 的 区间 在 0 1 之间 我们 还 
需要 做 如下 转换 3 . 优缺点 GBDT 的 优点 
当然 很 明显 啦 它 的 非线性 变换 比较 多 
表达 能力 强 而且 不 需要 做 复杂 的 特征 
工程 和 特征 变换 GBDT 的 缺点 也 很明显 Boost 
是 一个 串行 过程 不好 并行 化 而且 计算 复杂度 
高 同时 不 太 适合 高维 洗漱 特征 4 . 
实现 分析 5 . 参数/n 和/c 模型/n 调/v 优/n GBDT/w 
常用/b 的/uj 参数/n 有/v 如下/t 几个/m 1 .   树 
个数 2 . 树 深度 3 . 缩放 因子 4 
. 损失 函数 5 . 数据 采样 比 6 . 
特征 采样 比 二 Xgboostxgboost 是 boosting Tree 的 一个 
很 牛的/nr 实现 它 在 最近 Kaggle 比赛 中 大放异彩 
它 有 以下 几 个 优良 的 特性 1 . 
显示 的 把 树 模型 复杂度 作为 正则 项 加到 
优化 目标 中 2 . 公式 推导 中用 到了 二阶 
导数 用了 二阶 泰勒 展开 GBDT 用 牛顿 法 貌似 
也是 二阶 信息 3 . 实现 了 分裂 点 寻找 
近似算法 4 . 利用 了 特征 的 稀疏 性 5 
. 数据 事先 排序 并且 以 block 形式 存储 有利于 
并行计算 6 . 基于 分布式 通信 框架 rabit 可以 运行 
在 MPI 和 yarn 上 最新 已经 不 基于 rabit 
了 7 . 实现 做 了 面向 体系 结构 的 
优化 针对 cache 和 内存 做了 性能 优化 在 项目 
实测 中 使用 发现 Xgboost 的 训练 速度 要 远远 
快于 传统 的 GBDT 实现 10倍 量级 1 . 原理 
在 有 监督 学习 中 我们 通常 会 构造 一个 
目标函数 和 一个 预测 函数 使用 训练 样本 对 目标函数 
最小化 学习 到 相关 的 参数 然后 用 预测 函数 
和 训练样本 得到 的 参数 来 对 未知 的 样本 
进行 分类 的 标注 或者 数值 的 预测 一般 目标函数 
是 如下 形式 的 我们 通过 对 目标函数 最小化 求解 
模型 参数 预测 函数 损失 函数 正则化 因子 在 不同 
模型 下 是 各不相同 的 其中 预测 函数 有 如下 
几 种 形式 1 . 普通 预测 函数 a . 
  线性 下 我们 的 预测 函数 为 b . 
逻辑 回归 下 我们 的 预测 函数 为 2 . 
损失 函数 a . 平方 损失 函数 b . Logistic 
损失 函数 3 . 正则化 a . L1   参数 
求和 b . L2   参数 平方 求和 其实 我 
个人 感觉 Boosting   Tree 的 求解 方式 和 以上 
略有不同 Boosting Tree 由于 是 回归 树 一般 是 构造 
树 来 拟合 残差 而 不是 最小化 损失 函数 且看 
GBDT 情况 下 我们 的 预测 函数 为 而 Xgboost 
引入 了 二阶 导 来 进行 求解 并且 引入 了 
节点 的 数目 参数 的 L2 正则 来 评估 模型 
的 复杂度 那么 Xgboost 是 如何 构造 和 预测 的 
呢 首先 我们 给 出 结果 Xgboost 的 预测 函数 
为 而 目标函数 为 那么 作者 是 如何 构思 得到 
这些 预测 函数 和 优化 目标 的 呢 它们 又 
如何 求解 得到 的 呢 答案 是 作者 巧妙 的 
利用 了 泰勒 二阶 展开 和 巧妙 的 定义 了 
正则 项 用 求解 到 的 数值 作为 树 的 
预测 值 我们 定义 正则化 项 可以 得到 目标函数 转化 
为 然后 就 可以 求解 得到 同样在 分 裂点 选择 
的 时候 也 以 目标函数 最小 化为 目标 2 . 
实现 分析 3 . 参数 调 优 a . 初阶 
参数 调 优 1 . booster2 . objective3 . eta4 
. gamma5 . min _ child _ weight6 . max 
_ depth7 . colsample _ bytree8 . subsample9 . num 
_ round10 . save _ period 参考文献 1 . xgboost 
导读 和 实践 http / / vdisk . weibo . 
com / s / vlQWp3erG2yo / 14316586792 . GBDT MART 
迭代 决策树 入门教程   http / / blog . csdn 
. net / w28971023 / article / details / 82407563 
.   Introduction to Boosted Trees   https / / 
homes . cs . washington . edu / ~ tqchen 
/ pdf / BoostedTree . pdf4 . xgboost   https 
/ / github . com / dmlc / xgboost 