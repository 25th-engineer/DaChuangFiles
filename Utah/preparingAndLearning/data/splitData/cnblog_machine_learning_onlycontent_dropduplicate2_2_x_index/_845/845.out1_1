转载 请 注明 出处 http / / www . cnblogs 
. com / ymingjingr / p / 4271742 . html 
目录 机器学习 基石 笔记 1 在 何时 可以 使用 机器学习 
1 机器学习 基石 笔记 2 在 何时 可以 使用 机器学习 
2 机器学习 基石 笔记 3 在 何时 可以 使用 机器学习 
3 修改版 机器学习 基石 笔记 4 在 何时 可以 使用 
机器学习 4 机器学习 基石 笔记 5 为什么 机器 可以 学习 
1 机器学习 基石 笔记 6 为什么 机器 可以 学习 2 
机器学习 基石 笔记 7 为什么 机器 可以 学习 3 机器学习 
基石 笔记 8 为什么 机器 可以 学习 4 机器学习 基石 
笔记 9 机器 可以 怎样 学习 1 机器学习 基石 笔记 
10 机器 可以 怎样 学习 2 机器学习 基石 笔记 11 
机器 可以 怎样 学习 3 机器学习 基石 笔记 12 机器 
可以 怎样 学习 4 机器学习 基石 笔记 13 机器 可以 
怎样 学 得 更好 1 机器学习 基石 笔记 14 机器 
可以 怎样 学 得 更好 2 机器学习 基石 笔记 15 
机器 可以 怎样 学 得 更好 3 机器学习 基石 笔记 
16 机器 可以 怎样 学 得 更好 4 四 Feasibility 
of Learning 机器 学习 的 可能性 4.1 Learning is Impossible 
学习 可能 是 做 不到 的 在 训练 样 本集 
in sample 中 可以 求得 一个 最佳 的 假设 g 
该 假设 最大 可能 的 接近 目标函数 f 但是 在 
训练 样 本集 之外 的 其他 样本 out of sample 
中 假设 g 和 目标函数 f 可能 差别 很远 4.2 
Probability to the Rescue 可能 的 补救 方式 通过 上一 
小节 我们 得到 一个 结论 机器学习 无法 求得 近似 目标函数 
f 的 假设 函数 g 回忆 在 以前 学过 的 
知识 中 有无 遇到 过 类似 的 问题 通过 少量 
的 已知 样本 推论 整 个样 本集 的 情况 是否 
想 到 一个 曾经 学过 的 知识 其实 就是 概率 
统计 中 的 知识 通过 一个 例子 来 复习 下 
该 知识 有 一个 罐子 这个/r 罐子/n 里/f 盛/v 放着/i 
橙色/n 和/c 绿色/n 两种/m 颜色/n 的/uj 小球/n 我们 如何 在 
不查 遍 所有 小球 的 情况 下 得知 罐子 中 
橙子 小球 所占 的 比例 呢 抽取 样本 如 1 
所示 1 抽取 样本 假设 罐子 中 橙色 小球 的 
概率 为 不 难得 出 绿色 小球 的 概率 为 
其中 为 未知 值 而 通过 抽样 查出 的 橙色 
小球 比例 为 绿色 小球 的 比例 为 是从 抽样 
数据 中 计算出 的 因此 为 已知 值 如何 通过 
已知 样本 求得 未知 的 样本 可以 想象 到 在 
很大 的 几率 上 接近 的 结果 因为 在 罐子 
里 的 小球 均匀 搅拌 过后 抽出 小球 中的 橙色 
小球 比例 很 有可能 接近 整个 罐子 中 橙色 小球 
的 比例 不难 想象 在 抽出 的 小球 数量 等于 
罐中 小球 数量 时 两者 完全一致 这 其中 不 了解 
的 是 到底有 多大 的 可能性 两者 接近 此处 使用 
数学 的 方式 给予 答案 如 公式 4 1 所示 
公式 4 1 该 公式 称之为 霍夫 丁 不等式 Hoeffding 
s Inequality 其中 P 为 概率 符号 表示 与 的 
接近 程度 为此 程度 的 下界 N 表示 样本 数量 
其中 不等式 左边 表示 与 之间 相差 大于 某 值 
时的/nr 概率 从该/nr 不等式 不难 得出 随着 样本量 的 增大 
与 相差 较大 的 概率 就 不断 变小 两者 相差 
越多 即 越大 该 概率 越低 就 意味着 与 相等 
的 结论 大概 近似 正确 probably approximately correct PAC 同时 
可以 得出 当 N 足够 大 时 能够 从 已知 
的 推导 出 未知 的 4.3 Connection to Learning 联系 
到 机器 学习上 上 一节 得出 的 结论 可 以 
扩展 到 其他 应用 场景 其中 包括 机器学习 为 方便 
理解 做一个 对比 表 如表 4 1 所示 表 4 
1 机器学习 与 统计 中 的 对比 罐子 小球 机器学习 
未知 的 橙色 小球 比例 某一 确定 的 假设 在整个 
X 输入 空间 中 输入 向量 x 满足条件 的 占 
整个 输入 空间 的 比例 抽取 的 小球 ∈ 整个 
罐子 中的 小球 训练 输入 样 本集 整个 数据集 X 
橙色 小球 假设 h 作用 于此 输入 向量 x 与 
给定 的 输出 不相等 绿色 小球 假设 h 作用 于此 
输入 向量 x 与 给定 的 输出 相等 小球 样本 
是从 罐子 中 独立 随机 抽取 的 输入 样本 x 
是从 整个 数据集 D 中 独立 随机 选择 的 更 
通俗 一点 的 解释 上表 表达 的 内容 训练 输入 
样 本集 类比 随机 抽取 的 小球 样本 此 样本 
集中 先 确定 一个 假设 函数 h 满足 条件 的 
输入 向量 x 占 整个 样本 的 比例 类比 于 
橙色 小球 在 随机 抽取 小球 样本 的 比例 写成 
公式 的 形式 可以 入 公式 4 2 所示 因此 
使用 上 一节 中的 PAC 可能 近似 正确 的 理论 
在整个 输入 空间 中 这个 固定 的 假设 函数 h 
同 目标函数 f 不相等 的 输入量 占 整个 输入 空间 
数量 的 概率 的 取值 如 公式 4 3 所示 
与 上述 随机样本 中 两个 函数 不相等 的 样本 数 
占 抽样 数 的 比例 相同 这一 结论 也 是 
大概 近似 正确 的 公式 4 2 公式 4 3 
其中 N 为 随机 独立 抽样 的 样本 数 X 
为 整个 输入 空间 满足 条件 为 1 否 则为 
0 E 为 取 期望值 对 1.4节 的 机器学习 流程图 
进行 扩展 得到 如 2 所示 2 引入 统计学 知识 
的 机器学习 流程图 其中 虚线 表示 未知 概率 P 对 
随机抽样 以及 概率 的 影响 实线 表示 已经 随机 抽出 
的 训练样本 及 某一 确定 的 假设 对 比例 的 
影响 得出 的 结论 如下 对 任意 已 确定 的 
假设 函数 h 都 可以 通过 已知 的 求出 未知 
的 以后 我们 将 使用 和 这种 专业 的 符号 
分别 表示 在 某一 确定 的 假设 函数 h 中 
随机抽样 得到 的 样本 错误率 和 整个 输入 空间 的 
错误率 同样 可以 使用 霍夫 丁 不等式 对 以上 得到 
的 结论 做出 相应 的 数学 表达 如 公式 4 
4 所示 公式 4 4 但是 我们 想 得到 的 
不是 给 定 一个 已 确定 的 假设 函数 h 
通过 样本 的 错误 比例 来 推断出 在整个 输入 空间 
上 的 错误 概率 而是 在 整个 输入 空间 上 
同 目标函数 f 最 接近 的 假设 函数 h 那 
如何 实现 最 接近 呢 说白 了 错误 率 最低 
只需 在 上述 结论 上 再加 一个 条件 即 错误 
比例 很小 即可 总 结下 在 结论 基础 之上 加上 
很小 可以 推出 也 很小 即 在整个 输入 空间 中 
h ≈ f 上面 说 了 那么 多 可能 很多 
人 已经 糊涂 了 因为 这 并不 是 一个 学习 
问题 而是 一个 固定 假设 函数 h 判断 该 假设 
函数 是否 满足 上述 性质 这 准确 的 讲 是 
一种 确认 Verification 确实 如此 这种 形式 不能 称为 学习 
如 3 所示 3 确认 流程图 4.4 Connection to Real 
Learning 联系 到 真正 的 学习 上 首先 我们 要 
再次 确认 下 我们 上一 小节 确定 的 概念 要 
寻找 的 是 一个 使得 很小 的 假设 函数 h 
这样 就 可以 使得 h 和 目标函数 f 在整个 输入 
空间 中 也很 接近 继续 以 丢 硬币 为例 形象 
的 观察 这种 学习 方法 有 无问题 如 4 所示 
4 丢 硬币 的 例子 假设有 150个 人 同时 丢 
五次 硬币 统计 其中 有 一个 人 丢出 五次 全部 
正面 向上 的 概率 是 多少 不 难得 出 一个 
人 丢出 五次 正面 向上 的 概率 为 则 150人 
中有 一人 丢出 全 正面 向上 的 概率 为 这 
其中 抛出 正面 类比 于 绿色 小球 的 概率 也 
就是 当然 从 选择 的 角度 肯定 要 选择 犯错 
最小 的 即 正面 尽可能 多 的 情况 此例 中 
不难 发现 存在 全部都 为 正面 的 概率 是 非常 
大 的 此处 应注意 选择 全为 正面 的 或者 说 
为 0 并不 正确 因为 想 得到 的 结果 是 
而 不是 99% 这一 结论 与 真实 的 情况 或者 
说 差 的 太远 我们 不 仅仅 要 满足 很小 
条件 同时 还 要 使得 与 不能 有 太大 差距 
因此 这种 不好 的 样本 的 存在 得到 了 很 
糟糕 的 结果 上面 介绍 了 坏 的 样例 bad 
sample 把 本来 很高 的 通过 一个 使得 的 坏 
抽样 样本 进行 了 错误 的 估计 到底 是 什么 
造成 了 这种 错误 要 深入 了解 我们 还 需要 
介绍 坏 的 数据 bad data 的 概念 这里 写 
一下 自己 的 理解 坏 的 样本 bad sample ∈ 
坏 的 数据 bad data 坏 的 数据 就是 使得 
与 相差 很大 时 抽样 到 的 N 个 输入 
样本 我 的 理解 不是 这 N 个 输入 样本 
都 不好 可能 只是 有 几个 不好 的 样本 导致 
该 次 抽样 的 数据 产生 不好 的 结果 但 
此次 抽样 的 数据 集 被 统一 叫 做坏 的 
数据 根据 霍夫 丁 不等式 这种 情况 很少 出现 但是 
并不 代表 没有 特别 是 当 进行 假设 函数 的 
选择 时 它 的 影响 会 被 放大 以下 进行 
一个 具体 的 说明 如表 4 2 所示 表 4 
2 单个/b 假设/vn 函数/n 时的/nr 霍夫/nr 丁/nr 不等式/l D1D2/i D1126 
D5678 霍夫/nr 丁/nr hBADBAD/w 计算所/n 有/v 不好/d 的/uj D/w 出现/v 
的/uj 概率/n 如/v 公式/n 4/m 5 所示 公式 4 5 
单一 假设 函数 中 不好 的 D 出现 的 概率 
其实 不 算高 但是 当 在做 选择 时 面对 的 
是从 整个 假设 空间 选出 的 无数 种 可能 的 
假设 因此 不好 D 的 计算 就 有所 改变 当然 
我们 先 讨论 假设 函数 是 有限 多种 的 情况 
如表 4 3 所示 表 4 3 M 个 假设 
情况下 的 霍夫 丁 不等式 D1D2 D1126 D5678 霍夫 丁BA/nr 
D B A D B A D B A D 
B A D B A D B A D B 
A D A L L B A D B A 
D B A D 这 其中 包含 了 M 个 
假设 而 不好 的 D 不是 由 单一 假设 就 
确定 的 而是 只要 有 一个 假设 在此 抽样 D 
上 表现 不好 则 该 抽样 被 标记 为 坏 
的 因此 霍夫 丁 不等式 如 公式 4 6 所示 
联合 上界 公式 4 6 因此 如果 | H | 
= M 的 这种 有限 情况 下 抽样 样本 N 
足够 大 时 可以 确保 假设 空间 中 每个 假设 
都 满足 如果 通过 算法 找出 的 g 满足 则 
通过 PAC 的 规则 可以 保证 