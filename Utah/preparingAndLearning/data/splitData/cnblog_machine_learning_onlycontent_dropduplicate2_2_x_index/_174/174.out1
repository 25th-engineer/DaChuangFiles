一 . 背景 5月 9号 到 北大 去 听 hulu 
的 讲座 推荐 系统 和 计算 广告 在 视频 行业 
应用 想到/v 能/v 见到/v 传说中/i 的/uj 项亮/nr 大神/n 特地 拿了 
本 推荐 系统 实践 求 签名 讲座 开始 主讲人 先 
问了 下 哪些 同学 有 机器学习 的 背景 我 恬不知耻 
的 毅然 举手 真是 惭愧 后来 主讲人 在 讲座 中 
提到 了 最小二乘 法 说 这个 是 机器学习 最 基础 
的 算法 神马 最 基础 我 咋 不 知道 呢 
看来 以后 还是 要 对 自己 有 清晰 认识 回来 
赶紧 上 百度 搜 了 下 什么 是 最小二乘 法 
先 看下 百度 百科 的 介绍 最小二乘 法 又称 最 
小平 方法 是 一种 数学 优化 技术 它 通过 最小化 
误差 的 平方 和 寻找 数据 的 最佳 函数 匹配 
利用 最小二乘 法 可以 简便 地 求得 未知 的 数据 
并 使得 这些 求得 的 数据 与 实际 数据 之间 
误差 的 平方和 为 最小 最小二乘 法 还可 用于 曲线拟合 
其他 一些 优化 问题 也 可通过 最小化 能量 或 最大化 
熵 用 最小二乘 法来/nr 表达 通过 这段 描述 可以 看 
出来 最小二乘 法也是/nr 一种 优化 方法 求得 目标函数 的 最优 
值 并且 也 可以 用于 曲线拟合 来 解决 回归 问题 
难怪 统计 学习 方法 中 提到 回归 学习 最 常用 
的 损失 函数 是 平方 损失 函数 在此 情况 下 
回归 问题 可以 著名 的 最小二乘 法来/nr 解决 看来/v 最小二乘/i 
法/l 果然/d 是/v 机器学习/i 领域/n 做/v 有名/a 和/c 有效/a 的/uj 
算法/n 之一/r 二 . 最小二乘 法 我们 以 最 简单 
的 一元 线性 模型 来 解释 最小二乘 法 什么 是 
一元 线性 模型 呢   监督 学习 中 如果 预测 
的 变量 是 离散 的 我们 称 其为 分类 如 
决策树 支持 向量 机 等 如果 预测 的 变量 是 
连续 的 我们 称 其为 回归 回归分析 中 如果 只 
包括 一个 自 变量 和 一个 因变量 且 二者 的 
关系 可 用 一条 直线 近似 表示 这种 回归分析 称为 
一元 线性 回归分析 如果 回归分析 中 包括 两个 或 两个 
以上 的 自变量 且 因变量 和 自变量 之间 是 线性关系 
则 称为 多元 线性 回归分析 对于 二 维空间 线性 是 
一条 直线 对于 三维空间 线性 是 一个 平面 对于 多维空间 
线性 是 一个 超平面 . . . 对于 一元 线性 
回归模型 假设 从 总体 中 获取 了 n 组 观察 
值 X1 Y1 X2 Y2 Xn Yn 对于 平面 中的 
这 n 个 点 可以 使用 无数条 曲线 来 拟合 
要求 样本 回归 函数 尽可能 好 地 拟合 这组 值 
综合 起来 看 这条 直线 处于 样本数据 的 中心 位置 
最 合理 选择 最佳 拟合 曲线 的 标准 可以 确定 
为 使 总的 拟合 误差 即 总 残差 达到 最小 
有 以下 三 个 标准 可以 选择 1 用 残差 
和 最小 确 定直线 位置 是 一个 途径 但 很快 
发现 计算 残差 和 存在 相互 抵消 的 问题 2 
用 残差 绝对值 和 最小 确 定直线 位置 也 是 
一个 途径 但 绝对值 的 计算 比较 麻烦 3 最小二乘 
法的/nr 原则 是以 残差 平方和 最小 确 定直线 位置 用 
最小二乘 法 除了 计算 比较 方便 外 得到 的 估计量 
还 具有 优良 特性 这种 方法 对 异常值 非常 敏感 
最 常用 的 是 普通 最 小二 乘法 Ordinary   
Least Square OLS 所 选择 的 回归模型 应该 使 所有 
观察 值 的 残差 平方和 达到 最小 Q 为 残差 
平方和 即 采用 平方 损失 函数 样本 回归模型 其中 ei 
为 样本 Xi   Yi 的 误差 平方 损失 函数 
则 通过 Q 最小 确定 这 条 直线 即 确定 
以为 变量 把 它们 看作 是 Q 的 函数 就 
变成 了 一个 求 极值 的 问题 可以 通过 求 
导数 得到 求 Q 对 两个 待 估 参数 的 
偏 导数 根据 数学知识 我们 知道 函数 的 极值 点 
为 偏 导 为 0 的 点 解得 这 就是 
最 小二 乘法 的 解法 就是 求得 平方 损失 函数 
的 极值 点 三 . C + + 实现 代码 
1 / * 2 最小二乘 法C+/nr + 实现 3 参数 
1 为 输入 文件 4 输入 x 5 输出 预测 
的 y 6 * / 7 # include iostream 8 
# include fstream 9 # include vector 10 using namespace 
std 11 12 class LeastSquare { 13 double a b 
14 public 15 LeastSquare const vector double & x const 
vector double & y 16 { 17 double t1 = 
0 t2 = 0 t3 = 0 t4 = 0 
18 for int i = 0 i x . size 
+ + i 19 { 20 t1 + = x 
i * x i 21 t2 + = x i 
22 t3 + = x i * y i 23 
t4 + = y i 24 } 25 a = 
t3 * x . size t2 * t4 / t1 
* x . size t2 * t2 / / 求得 
β 1 26 b = t1 * t4 t2 * 
t3 / t1 * x . size t2 * t2 
/ / 求得 β 2 27 } 28 29 double 
getY const double x const 30 { 31 return a 
* x + b 32 } 33 34 void print 
const 35 { 36 cout y = a x + 
b \ n 37 } 38 39 } 40 41 
int main int argc char * argv 42 { 43 
if argc = 2 44 { 45 cout Usage DataFile 
. txt endl 46 return 1 47 } 48 else 
49 { 50 vector double x 51 ifstream in argv 
1 52 for double d in d 53 x . 
push _ back d 54 int sz = x . 
size 55 vector double y x . begin + sz 
/ 2 x . end 56 x . resize sz 
/ 2 57 LeastSquare ls x y 58 ls . 
print 59 60 cout Input x \ n 61 double 
x0 62 while cin x0 63 { 64 cout y 
= ls . getY x0 endl 65 cout Input x 
\ n 66 } 67 } 68 } 四 . 
最小二乘/i 法与/nr 梯度/n 下/f 降法/n 最小二乘/i 法跟/nr 梯度/n 下/f 降法/n 
都是/i 通过/p 求导/v 来/v 求/v 损失/n 函数/n 的/uj 最小值/l 那/r 
它们/r 有/v 什么/r 区别/n 呢/y 相同 1 . 本质 相同 
两种 方法 都是 在 给定 已知 数据 independent & dependent 
variables 的 前提 下 对 dependent variables 算出 出 一个 
一般性 的 估值 函数 然后 对 给定 新 数据 的 
dependent variables 进行 估算 2 . 目标 相同 都是 在 
已知 数据 的 框架 内 使得 估算 值 与 实际 
值 的 总 平方差 尽量 更小 事实上 未必 一定 要 
使用 平方 估算 值 与 实际 值 的 总 平方差 
的 公式 为 其中 为 第 i 组 数据 的 
independent variable 为 第 i 组 数据 的 dependent variable 
为 系数 向量 不同 1 . 实现 方法 和 结果 
不同 最小二乘 法是/nr 直接 对 求导 找出 全局 最小 是非 
迭代法 而 梯度 下 降法 是 一种 迭代法 先给 定 
一个 然后 向 下降 最快 的 方向 调整 在 若干 
次 迭代 之后 找到 局部 最小 梯度 下 降法 的 
缺点 是 到 最 小点 的 时候 收敛 速度 变慢 
并且 对 初 始点 的 选择 极为 敏感 其 改进 
大多 是 在 这 两 方面 下功夫 参考   http 
/ / blog . csdn . net / qll125596718 / 
article / details / 8248249 