机器学习 实战 读书笔记 06 – SVM 支持 向量 机 前言 
最近 在看 Peter Harrington 写 的 机器学习 实战 这是 我 
的 学习 笔记 这次 是 第 6 章 SVM 支持 
向量 机 支持 向量 机 不是 很好 被 理解 主要 
是 因为 里面 涉及 到 了 许多 数学知识 需要 慢慢 
地 理解 我 也 是 通过 看 别人 的 博客 
理解 SVM 的 推荐 大家 看看 on2way 的 SVM 系列 
解密 SVM 系列 一 关于/p 拉格朗日/i 乘子/n 法和/nr KKT/w 条件/n 
解密/nr SVM/w 系列/q 二 SVM 的 理论 基础 解密 SVM 
系列 三 SMO 算法 原理 与 实战 求解 解密 SVM 
系列 四 SVM 非线性 分类 原理 实验 基本概念 SVM Support 
Vector Machine 支持 向量 机 其 含义 是 通过 支持 
向量 运算 的 分类器 其中 机 的 意思 是 机器 
可以 理解 为 分类器 什么 是 支持 向量 呢 在 
求解 的 过程 中 会 发现 只 根据 部分 数据 
就 可以 确定 分类器 这些 数据 称 为 支持 向量 
见 下图 在 一个 二维 环境 中 其 中点 R 
S G 点 和 其它 靠近 中间 黑线 的 点 
可以 看 作为 支持 向量 它们 可以 决定 分类器 也 
就是 黑线 的 具体 参数 分类器 就是 分类 函数 线性 
分类 可以 理解 为 在 2 维空间 中 可以 通过 
一条 直线 来 分类 在 p 维空间 中 可以 通过 
一个 p 1 维 的 超平面 来 分类 向量 有 
多个 属性 的 变量 在 多维空间 中 的 一个 点 
就是 一个 向量 比如 \ x = x _ 1 
x _ 2 . . . x _ n \ 
下面 的 \ w \ 也是 向量 约束条件 subject to 
在 求 一个 函数 的 最优 值 时 需要 满足 
的 约束条件 向量 相乘 \ xw ^ T = \ 
textstyle \ sum _ { i = 1 } ^ 
n w _ ix _ i \ 内积 \ \ 
langle x y \ rangle = \ textstyle \ sum 
_ { i = 1 } ^ n x _ 
iy _ i \ 解决 的 问题 线性 分类 在 
训练 数据 中 每个 数据 都有 n 个 的 属性 
和 一个 二类 类别 标志 我们 可以 认为 这些 数据 
在 一个 n 维空间 里 我们 的 目标 是 找到 
一个 n 1 维 的 超平面 hyperplane 这个 超平面 可以 
将 数据 分成 两部分 每 部分 数据 都 属于 同 
一个 类别 其实 这样 的 超平面 有 很多 我们 要 
找到 一个 最佳 的 因此 增加 一个 约束条件 这个 超平面 
到 每边 最近 数 据点 的 距离 是 最大 的 
也 成为 最大 间隔 超平面 maximum margin hyperplane 这个 分类器 
也 成为 最大 间隔 分类器 maximum margin classifier 支持 向量 
机 是 一个 二类 分类器 非线性 分类 SVM 的 一个 
优势 是 支持 非线性 分类 它/r 结合/v 使用/v 拉格朗日/i 乘子/n 
法和/nr KKT/w 条件/n 以及 核 函数 可以 产生 非线性 分类器 
分类器 1 线性 分类器 是 一个 线性函数 可以 用于 线性 
分类 一个 优势 是 不 需要 样本数据 classifier 1 \ 
f x = xw ^ T + b \ \ 
w \ 和 \ b \ 是 训练 数据 后 
产生 的 值 分类器 2 非线性 分类器 支持 线性 分类 
和 非线性 分类 需要 部分 样本数据 支持 向量 也 就是 
\ \ alpha _ i \ ne 0 \ 的 
数据 \ \ because \ \ w = \ textstyle 
\ sum _ { i = 1 } ^ n 
\ alpha _ iy _ ix _ i \ \ 
\ therefore \ classifier 2 \ f x = \ 
textstyle \ sum _ { i = 1 } ^ 
n \ alpha _ iy _ i K x _ 
i x + b \ \ \ text { here 
} \ \ \ qquad x _ i \ text 
{ training data i } \ \ \ qquad y 
_ i \ text { label value of training data 
i } \ \ \ qquad \ alpha _ i 
\ text { Lagrange multiplier of training data i } 
\ \ \ qquad K x _ 1 x _ 
2 = exp \ frac { \ lVert x _ 
1 x _ 2 \ rVert ^ 2 } { 
2 \ sigma ^ 2 } \ text { kernel 
function } \ \ \ \ \ alpha \ \ 
\ sigma \ 和 \ b \ 是 训练 数据 
后 产生 的 值 可以 通过 调节 \ \ sigma 
\ 来 匹配 维度 的 大小 \ \ sigma \ 
越大 维度 越低 核心思想 SVM 的 目的 是 要 找到 
一个 线性 分类 的 最佳 超平面 \ f x = 
xw ^ T + b = 0 \ 求 \ 
w \ 和 \ b \ 首先 通过 两个 分类 
的 最近 点 找到 \ f x \ 的 约束条件 
有了 约束条件 就/d 可以/c 通过/p 拉格朗日/i 乘子/n 法和/nr KKT/w 条件/n 
来/v 求解/v 这时 问题 变成 了 求 拉格朗日 乘子 \ 
\ alpha _ i \ 和 \ b \ 对于 
异常 点 的 情况 加入 松弛 变量 \ \ xi 
\ 来 处理 使用 SMO 来 求 拉格朗日 乘子 \ 
\ alpha _ i \ 和\/nr b \ 这时 我们 
会 发现 有些 \ \ alpha _ i = 0 
\ 这些 点 就 可以 不用 在 分类器 中 考虑 
了 惊喜 不用 求 \ w \ 了 可以 使用 
拉格朗日 乘子 \ \ alpha _ i \ 和\/nr b 
\ 作为 分类器 的 参数 非线性 分类 的 问题 映 
射到 高 维度 使用 核 函数 详解 线性 分类 及其 
约束条件 SVM 的 解决 问题 的 思路 是 找到 离 
超平面 的 最近 点 通过 其 约束条件 求出 最优 解 
对于 训练 数据集 T 其 数据 可以 分为 两类 C1 
和 C2 对于 函数 \ f x = xw ^ 
T + b \ 对于 C 1类 的 数据 \ 
xw ^ T + b \ geqslant 1 \ 其中 
至少 有 一个 点 \ x _ i \ \ 
f x _ i = 1 \ 这个 点 称之 
为最 近点 对于 C 2类 的 数据 \ xw ^ 
T + b \ leqslant 1 \ 其中 至少 有 
一个 点 \ x _ i \ \ f x 
_ i = 1 \ 这个 点 称 也是 最 
近点 上面 两个 约束条件 可以 合 并为 \ y _ 
if x _ i = y _ i x _ 
iw ^ T + b \ geqslant 1 \ \ 
y _ i \ 是 点 \ x _ i 
\ 对应 的 分类 值 1 或者 1 求 \ 
w \ 和\/nr b \ . 则 超平面 函数 是 
\ xw ^ T + b = 0 \ 为了 
求 最优 的 f x 期望 训练 数据 中 的 
每个 点到 超平面 的 距离 最大 解释 1 这里 需要 
理解 一个 事情 根据 上图 我们 可以 给 每个 点 
做 一条 平行 于 超平面 的 平行线 超 平行面 因此 
这个 最大化 相当于 求 最近 点到 超平面 距离 的 最大化 
总结 现在 我们 的 公式 是 Formula 6.1 \ f 
x = xw ^ T + b \ \ \ 
text { subject to } \ \ \ qquad y 
_ if x _ i = y _ i x 
_ iw ^ T + b \ geqslant 1 i 
= 1 . . . n \ 几个 训练 脑筋 
的 小问题 Q y 是否 可以 是 其它 非 { 
1 1 } 的 值 A 将 y 值 定义 
为 { 1 1 } 是 最 简化 的 方案 
你 的 分类 可以 是 cat 和 dog 只要 将 
cat 对应 到 1 dog 对应 到 1 就 可以 
了 你 也 可以 将 y 值 定义 为 其它 
数 比如 2 2 或者 2 3 之类 的 但是 
这样 就 需要 修改 超平面 函数 和 约束条件 增加 了 
没 必要 的 繁琐 实际上 和y值/nr 定义 为 { 1 
1 } 是 等价 的 Q 如果 两 组 数据 
里 的 太近 或者 太远 是不是 可能 就 找不到 \ 
xw ^ T + b = 1 \ 和\/nr xw 
^ T + b = 1 \ 的 这 两个 
点 A 不会 假设 可以 找到 \ x _ iw 
^ T + b = c \ 和 \ x 
_ jw ^ T + b = c \ . 
\ c 0 and c 1 \ 其 超平面 函数 
为 \ xw ^ T + b = 0 \ 
. 上面 公式 左右 同时 除以 c 则 \ x 
_ iw ^ T / c + b / c 
= 1 \ \ x _ jw ^ T / 
c + b / c = 1 \ 令 \ 
w = w / c \ \ b = b 
/ c \ 有 \ x _ iw ^ T 
+ b = 1 \ \ x _ jw ^ 
T + b = 1 \ 可以 找到 超平面 函数 
\ xw ^ T + b = 0 \ 因此 
总是 可以 找到 y 是 { 1 1 } 的 
超平面 如果 有的 话 最大 几何 间隔 geometrical margin \ 
f x \ 为 函数 间隔 \ \ gamma \ 
如果 求 \ \ text { max } yf x 
\ 有个/nr 问题 就是 w 和b/nr 可以 等比例 增大 导致 
\ yf x \ 的 间隔 可以 无限大 因此 需要 
变成 求 等价 的 最大 几何 间隔 \ \ bar 
{ \ gamma } = \ frac { yf x 
} { \ lVert w \ rVert } \ \ 
\ text { subject to } \ \ \ qquad 
y _ if x _ i = y _ i 
x _ iw ^ T + b \ geqslant 1 
i = 1 . . . n \ \ \ 
lVert w \ rVert \ 二阶 范数 也 就是 各 
项目 平方和 的 平方根 \ \ sqrt { \ textstyle 
\ sum _ { i = 1 } ^ n 
w _ i ^ 2 } \ 根据 上面 的 
解释 这个 问题 可以 转变 为 \ \ text { 
max } \ frac { 1 } { \ lVert 
w \ rVert } \ \ \ text { subject 
to } \ \ \ qquad y _ i x 
_ iw ^ T + b \ geqslant 1 i 
= 1 . . . n \ 再做 一次 等价 
转换 Formula 6.2 \ \ text { min } \ 
frac { 1 } { 2 } \ lVert w 
\ rVert ^ 2 \ \ \ text { subject 
to } \ \ \ qquad y _ i x 
_ iw ^ T + b \ geqslant 1 i 
= 1 . . . n \ 求解 问题 \ 
w b \ Leftrightarrow \ alpha _ i b \ 
我们/r 使用/v 拉格朗日/i 乘子/n 法和/nr KKT/w 条件/n 来/v 求/v \/i 
w \ 和\/nr b \ 一个 重要 原因 是 使用 
拉格朗日 乘子 法后/nr 还 可以 解决 非线性 划分 问题 拉格朗日/i 
乘子/n 法和/nr KKT/w 条件/n 可以/c 解决/v 下面/f 这个/r 问题/n 求 
一个 最 优化 问题 \ f x \ 刚好 对应 
我们 的 问题 \ min \ frac { 1 } 
{ 2 } \ lVert w \ rVert ^ 2 
\ 如果 存在 不等式 约束 \ g _ k x 
= 0 k = 1 q \ 对应 \ \ 
text { subject to } \ qquad 1 y _ 
i x _ iw ^ T + b = 0 
i = 1 . . . n \ F x 
必须 是 凸函数 这个 也 满足 SVM 的 问题 满足 
使用 拉格朗日 乘子 法的/nr 条件 因此 问题 变成 Formula 6.3 
\ \ underset { \ alpha } { max } 
\ text { } W \ alpha = \ mathcal 
{ L } w b \ alpha = \ frac 
{ 1 } { 2 } \ lVert w \ 
rVert ^ 2 \ textstyle \ sum _ { i 
= 1 } ^ n \ alpha _ i y 
_ i x _ iw ^ T + b 1 
\ \ \ text { subject to } \ \ 
\ qquad \ alpha _ i = 0 i = 
1 . . . n \ \ \ qquad \ 
textstyle \ sum _ { i = 1 } ^ 
n \ alpha _ iy _ i = 0 \ 
\ \ qquad 1 y _ i x _ iw 
^ T + b = 0 i = 1 . 
. . n \ \ \ qquad w = \ 
textstyle \ sum _ { i = 1 } ^ 
n \ alpha _ iy _ ix _ i \ 
\ \ text { here } \ \ \ qquad 
\ alpha _ i \ text { Lagrange multiplier of 
training data i } \ \ \ 消除 \ w 
\ 之后 变为 Formula 6.4 \ \ underset { \ 
alpha } { max } \ text { } W 
\ alpha = \ mathcal { L } w b 
\ alpha = \ textstyle \ sum _ { i 
= 1 } ^ n \ alpha _ i \ 
frac { 1 } { 2 } \ textstyle \ 
sum _ { i j = 1 } ^ n 
\ alpha _ i \ alpha _ jy _ iy 
_ jx _ i ^ Tx _ j \ \ 
\ text { subject to } \ \ \ qquad 
\ alpha _ i = 0 i = 1 . 
. . n \ \ \ qquad \ textstyle \ 
sum _ { i = 1 } ^ n \ 
alpha _ iy _ i = 0 \ \ \ 
qquad \ alpha _ i 1 y _ i \ 
textstyle \ sum _ { j = 1 } ^ 
n \ alpha _ jy _ j \ langle x 
_ j x _ i \ rangle + b = 
0 i = 1 . . . n \ \ 
\ langle x _ j x _ i \ rangle 
\ 是 \ x _ j \ 和 \ x 
_ i \ 的 内积 相当于 \ x _ ix 
_ j ^ T \ 可见/v 使用/v 拉格朗日/i 乘子/n 法和/nr 
KKT/w 条件/n 后/f 求 \ w b \ 的 问题 
变成 了 求 拉格朗日 乘子 \ \ alpha _ i 
\ 和\/nr b \ 的 问题 到 后面 更 有趣 
变成 了 不求 \ w \ 了 因为 \ \ 
alpha _ i \ 可以 直接 使用 到 分类器 中去 
并且 可以 使用 \ \ alpha _ i \ 支持 
非 线性 的 情况 \ xw ^ T + b 
\ 是 线性函数 支持 不了 非线性 的 情况 哦 以上 
的 具体 证明 请看 解密 SVM 系列 二 SVM/w 的/uj 
理论/n 基础/n 关于/p 拉格朗日/i 乘子/n 法和/nr KKT/w 条件/n 请看 深入 
理解 拉格朗日 乘子 法 Lagrange Multiplier 和 KKT 条件 处理 
异常 点 outliers 如上图 点 w 是 一个 异常 点 
导致 无法 找到 一个 合适 的 超平面 为了 解决 这个 
问题 我们 引入 松弛 变量 slack variable \ \ xi 
\ 修改 之间 的 约束 条件 为 \ x _ 
iw ^ T + b = 1 – \ xi 
_ i \ qquad \ text { for all i 
= 1 n } \ 则 运用 拉格朗日 乘子 法 
之后 的 公式 变为 Formula 6.5 \ \ underset { 
\ alpha } { max } \ text { } 
W \ alpha = \ mathcal { L } w 
b \ alpha = \ textstyle \ sum _ { 
i = 1 } ^ n \ alpha _ i 
\ frac { 1 } { 2 } \ textstyle 
\ sum _ { i j = 1 } ^ 
n \ alpha _ i \ alpha _ jy _ 
iy _ jx _ jx _ i ^ T \ 
\ \ text { subject to } \ \ \ 
qquad 0 \ leqslant \ alpha _ i \ leqslant 
C i = 1 . . . n \ \ 
\ qquad \ textstyle \ sum _ { i = 
1 } ^ n \ alpha _ iy _ i 
= 0 \ \ \ qquad \ alpha _ i 
1 y _ i \ textstyle \ sum _ { 
j = 1 } ^ n \ alpha _ jy 
_ j \ langle x _ j x _ i 
\ rangle + b = 0 i = 1 . 
. . n \ 输入 参数 参数 \ C \ 
越大 表明 影响 越 严重 \ C \ 应该 一个 
大于 0 值 其实 \ C \ 也 不能 太小 
太小 了 就 约束 \ \ alpha _ i \ 
了 比如 200 参数 \ \ xi \ 对 所有 
样本数据 起效 的 松弛 变量 比如 0.0001 具体 证明 请看 
解密 SVM 系列 二 SVM 的 理论 基础 求解 \ 
\ alpha \ 使用 SMO 方法 1996年 John Platt 发布 
了 一个 称为 SMO 的 强大 算法 用于 训练 SVM 
SMO 表示 序列 最小 优化 Sequential Minimal Optimization SMO 方法 
概要 SMO 方法 的 中心 思想 是 每次 取 一对 
\ \ alpha _ i \ 和\/nr \ alpha _ 
j \ 调整 这 两个 值 参数 训练 数据 / 
分类 数据 / \ C \ / \ \ xi 
\ / 最大 迭 代数 过程 初始化 \ \ alpha 
\ 为 0 在 每次 迭代 中 小于 等于 最大 
迭 代数 找到 第一个 不 满足 KKT 条件 的 训练 
数据 对应 的 \ \ alpha _ i \ 在 
其它 不满足 KKT 条件 的 训练 数据 中 找到 误差 
最大 的 x 对应 的 index 的 \ \ alpha 
_ j \ \ \ alpha _ i \ 和\/nr 
\ alpha _ j \ 组成 了 一对 根据 约束条件 
调整 \ \ alpha _ i \ \ \ alpha 
_ j \ 不满足 KKT 条件 的 公式 Formula 6.6 
\ \ text { 1 } y _ i u 
_ i y _ i \ leqslant \ xi \ 
text { and } \ alpha _ i C \ 
\ \ text { 2 } y _ i u 
_ i y _ i \ geqslant \ xi \ 
text { and } \ alpha _ i 0 \ 
\ here \ \ \ qquad u _ i = 
\ textstyle \ sum _ { j = 1 } 
^ n \ alpha _ jy _ j K x 
_ j x _ i + b \ \ \ 
qquad K x _ 1 x _ 2 = \ 
langle x _ 1 x _ 2 \ rangle \ 
\ \ qquad \ xi \ text { slack variable 
} \ 调整 公式 Formula 6.7 \ \ alpha _ 
2 ^ { new } = \ alpha _ 2 
^ { old } \ frac { y _ 2 
E _ 1 E _ 2 } { \ eta 
} \ \ \ alpha _ 1 ^ { new 
} = \ alpha _ 1 ^ { old } 
+ y _ 1y _ 2 \ alpha _ 2 
^ { old } \ alpha _ 2 ^ { 
new } \ \ b _ 1 = b ^ 
{ old } E _ 1 y _ 1 \ 
alpha _ 1 ^ { new } \ alpha _ 
1 ^ { old } K x _ 1 x 
_ 1 y _ 2 \ alpha _ 2 ^ 
{ new } \ alpha _ 2 ^ { old 
} K x _ 1 x _ 2 \ \ 
b _ 2 = b ^ { old } E 
_ 2 y _ 1 \ alpha _ 1 ^ 
{ new } \ alpha _ 1 ^ { old 
} K x _ 1 x _ 2 y _ 
2 \ alpha _ 2 ^ { new } \ 
alpha _ 2 ^ { old } K x _ 
2 x _ 2 \ \ b = \ begin 
{ cases } b _ 1 & \ text { 
if } 0 \ leqslant \ alpha _ 1 ^ 
{ new } \ leqslant C \ \ b _ 
2 & \ text { if } 0 \ leqslant 
\ alpha _ 2 ^ { new } \ leqslant 
C \ \ \ frac { b _ 1 + 
b _ 2 } { 2 } & \ text 
{ otherwise } \ end { cases } \ \ 
here \ \ \ qquad E _ i = u 
_ i y _ i \ \ \ qquad \ 
eta = 2K x _ 1 x _ 2 K 
x _ 1 x _ 1 K x _ 2 
x _ 2 \ \ \ qquad u _ i 
= \ textstyle \ sum _ { j = 1 
} ^ n \ alpha _ jy _ j K 
x _ j x _ i + b \ \ 
\ qquad K x _ 1 x _ 2 = 
\ langle x _ 1 x _ 2 \ rangle 
\ 具体 证明 请 参照 解密 SVM 系列 三 SMO 
算法 原理 与 实战 求解 最后 一步 解决 非线性 分类 
根据 机器 学习 的 理论 非线性 问题 可以 通过 映 
射到 高 维度 后 变成 一个 线 性问题 比如 二维 
下 的 一个 点 \ x1 x2 \ 可以 映射 
到 一个 5 维空间 这个 空间 的 5个 维度 分别 
是 \ x1 x2 x1x2 x1 ^ 2 x2 ^ 
2 \ 映 射到 高 维度 有 两个 问题 一个 
是 如何 映射 另外 一个 问题 是 计算 变得 更 
复杂 了 幸运 的 是 我们 可以 使用 核 函数 
Kernel function 来 解决 这个 问题 核 函数 kernel function 
也 称为 核 技巧 kernel trick 核 函数 的 思想 
是 仔细观察 Formula 6.6 和 Formula 6.7 就 会 发现 
关于 向量 \ x \ 的 计算 总是 在 计算 
两个 向量 的 内积 \ K x _ 1 x 
_ 2 = \ langle x _ 1 x _ 
2 \ rangle \ 因此 在 高维空间 里 公式 的 
变化 只有 计算 低 维空间 下 的 内积 \ \ 
langle x _ 1 x _ 2 \ rangle \ 
变成 了 计算 高维空间 下 的 内积 \ \ langle 
x _ 1 x _ 2 \ rangle \ 核 
函数 提供 了 一个 方法 通过 原始 空间 的 向 
量值 计算 高维空间 的 内积 而 不用 管 映射 的 
方式 我们 可以 用 核 函数 代替 \ K x 
_ 1 x _ 2 \ 核 函数 有 很多 
种 一般 可以 使用 高斯 核 径向 基 函数 radial 
basis function Formula 6.8 \ K x _ 1 x 
_ 2 = exp \ frac { \ lVert x 
_ 1 x _ 2 \ rVert ^ 2 } 
{ 2 \ sigma ^ 2 } \ 可以 通过 
调节 \ \ sigma \ 来 匹配 维度 的 大小 
\ \ sigma \ 越大 维度 越低 比如 10 可以 
参照 解密 SVM 系列 四 SVM 非线性 分类 原理 实验 
支持 向量 机 通俗 导论 理解 SVM 的 三层 境界 
如何 解决 多类 分类 问题 支持 向量 机 是 一个 
二类 分类器 基于 SVM 如何 构建 多类 分类器 建议 阅读 
C . W . Huset 等人 发表 的 一篇 论文 
A Comparison of Methods for Multiclass Support Vector Machines 需要 
对 代码 做 一些 修改 参照 Machine Learning in Action 
by Peter Harrington 解密 SVM 系列 一 关于/p 拉格朗日/i 乘子/n 
法和/nr KKT/w 条件/n 解密/nr SVM/w 系列/q 二 SVM 的 理论 
基础 解密 SVM 系列 三 SMO 算法 原理 与 实战 
求解 解密 SVM 系列 四 SVM 非线性 分类 原理 实验 
深入 理解 拉格朗日 乘子 法 Lagrange Multiplier 和 KKT 条件 
支持 向量 机 通俗 导论 理解 SVM 的 三层 境界 
https / / en . wikipedia . org / wiki 
/ Support _ vector _ machine 