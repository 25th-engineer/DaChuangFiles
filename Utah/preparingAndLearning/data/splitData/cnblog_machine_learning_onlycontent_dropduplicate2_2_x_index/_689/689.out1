欢迎 大家 前往 腾讯 云 社区 获取 更多 腾讯 海量 
技术 实践 干货 哦 ~ 作者 汪毅雄/nr 导语 本文 详细 
的 解释 了 机器 学习 中 经常 会 用到 数据 
清洗 与 特征 提取 的 方法 PCA 从 理论 数据 
代码 三个 层次 予以 分析 机器学习 这个 名词 大家 都 
耳熟能详 虽然 这个 概念 很早 就 被人 提 出来 了 
但是 鉴于 科技 水平 的 落后 一直 发展 的 比较 
缓慢 但是 近些年 随着 计算机硬件 能力 的 大幅度 提升 这一 
概念 慢慢 地 回到 我们 的 视野 而且 发展 速度 
之 快 令 很多 人 刮目相看 尤其 这两年 阿 法狗在/nr 
围棋 届 的 神勇 表现 给人 在此 领域 有了/nr 巨大 
的 遐想 空间 所谓 机器学习 一般 专业 一点 的 描述 
其 是 机器学习 Machine Learning ML 是 一门 多 领域 
交叉 学科 涉及 概率论 统计学 逼近 论 凸 分析 算法 
复杂度 理论 等 多门 学科 专门研究 计算机 怎样 模拟 或 
实现 人类 的 学习 行为 以 获取 新 的 知识 
或 技能 重新组织 已有 的 知识 结构 使 之 不断 
改善 自身 的 性能 机器学习 这门 技术 是 多种 技术 
的 结合 而 在 这个 结合 体中 如何 进行 数据 
分析 处理 是 个人 认为 最 核心 的 内容 通常 
在 机器 学习 中 我们 指 的 数据 分析 是 
从 一大堆 数据 中 筛选 出 一些 有 意义 的 
数据 推断 出 一个 潜在 的 可能 结论 得出 这个 
不 知道 正确 与否 的 结论 其 经过 的 步骤 
通常 是 1 预处理 把/p 数据/n 处理/v 成/n 一些/m 有/v 
意义/n 的/uj 特征/n 这 一步 的 目的 主要 是 为了 
降 维 2 建模 这 部分 主要 是 建立 模型 
通常 是 曲线 的 拟合 为 分类器 搭建 一个 可能 
的 边界 3 分类器 处理 根据 模型 把 数据 分类 
并 进行 数据 结论 的 预测 本文 讲 的 主要 
是 数据 的 预处理 降 维 而 这里 采用 的 
方式 是 PCA PCA 的 个人 理论 分析 假设 有 
一个 学生 信息 管理 系统 里面 需要 存储 人性 别的 
字段 我们 在 数据库 里 可以 有M/nr F 两个 字段 
用 1 0 分别 代表 是 否 当是 男 学生 
的 时候 其中 M 列为 1 F 列为 0 为 
女生 时M/nr 列为 0 F 列为 1 我们 发现 对 
任意 一条 记录 当 M 为 1 F 必然 为 
0 反之 也是 如此 因此 实际 过程 我们 把 M 
列 或 F 列 去掉 也不会 丢失 任何 信息 因为 
我们 可以 反 推出 结论 这种 情况下 的 M F 
列 的 关联 比 是 最高 的 是 100% 再举 
另外 一个 例子 小明 开了 家 店铺 他 每天 在 
统计 其 店铺 的 访问量 V 和 成交量 D 可以 
发现 往往 V 多 的 时候 D 通常 也 多 
D 少 的 时候 V 通常 也 很少 可以 猜到 
V 和D是/nr 有种 必然 的 联系 但又 没有 绝对 的 
联系 此时 小明 如果 想 根据 V D 来 衡量 
这一天 的 价值 往往 可以 根据 一些 历史 数据 来 
计算 出 V D 的 关联 比 拍 脑门 说 
一个 如果 关联 比 大于 80% 那么 可以 取 VD 
其中 任意 一个 即可 衡量 当天 价值 这样 就 达到 
了 降 维 的 效果 当然 降 维 并非 只能 
在 比如说 2 维 数据 V D 中 选取 其中 
的 1 维 V 作为 特征值 它 有可能 是 在 
V + D 的 情况 下 使得 对 V D 
的 关联 比 最大 但是 PCA 思想 就是 如此 简单 
点 说 假设有 x1 x2 x3 xn 维 数据 我们 
想 把 数据 降到 m 维 我们 可以 根据 这 
n 维 的 历史 数据 算 出 一个 与 x1 
xn 相关 m 维 数据 使得 这个 m 维 数据 
对 历史 数据 的 关联 比 达到 最大 数学分析 假设 
我们 有 一组 二维 数据 如果 我们 必须 使用 一维 
来 表示 这些 数据 又 希望 尽量 保留 原始 的 
信息 你 要 如何 选择 这个 问题 实际上 是 要在 
二维 平面 中 选择 一个 方向 将 所有 数据 都 
投影 到 这个 方向 所在 直线 上 用 投影 值 
表示 原始记录 这 是 一个 实际 的 二维 降到 一维 
的 问题 那么 如何 选择 这个 方向 才能 尽量 保留 
最多 的 原始 信息 呢 一种 直观 的 看法 是 
希望 投 影后 的 投影 值 尽可能 分散 这样 投影 
的 范围 越大 在做 分类 的 时候 也 就 更容易 
做 分类器 以 上图 为例 可以 看出 如果 向x轴/nr 投影 
那么 最 左边 的 两个 点 会 重叠 在 一起 
中间 的 两个 点 也会 重叠 在 一起 于是 本身 
四个 各不相同 的 二维 点 投 影后 只剩 下 两个 
不同 的 值了 这 是 一种 严重 的 信息 丢失 
同理 如果 向y轴/nr 投影 中间 的 三个点 都会 重叠 效果 
更糟 所以/c 看来/v x/w 和y轴/nr 都/d 不是/c 最好/a 的/uj 投影/n 
选择/v 直观 来看 如果/c 向/p 通过/p 第一象限/i 和/c 第三/m 象限/n 
的/uj 斜线/n 投影/n 则 五个 点在 投影 后 还是 可以 
区分 的 我们 希望 投 影后 投影 值 尽可能 分散 
那 什么 是 衡量 分散 程度 的 统计量 呢 显然 
可以 用 数学 上 的 方差 来 表述 通常 为了 
方便 计算 我们 会 把 每个 点 都 减去 均值 
这样 得到 的 点 的 均值 就 会为 0 . 
这个 过程 叫做 均一 化 均一 化 后 于是 上面 
的 问题 被 形式化 表述 为 寻找 一个 基 使得 
所有 数据 变换 为 这个 基 上 的 坐标 表示 
后 方差 值 最大 我们 跳出 刚才 的 例子 因为 
很 容易 把 刚才 的 结论 推广 到 任意 纬度 
要求 投影 点 的 方差 最大值 所 对应 的 基 
u 这时 有 两种 方法 来 求解 方法 一 假设 
有个 投影 A 显然 刚才 说 的 方差 V 可以 
用来 表示 而 投影 A = 原始数据 X . U 
这样 方差 可以 表示 为 求 这个 方差 的 最大值 
我们 可以 用 拉格朗日 插值法 来做 L u λ 为 
求导 L 令 导数 为 0 这样/r 问题/n 就/d 转换/v 
成求X/nr ./i XT/w 的/uj 特征值/n 和/c 特征向量/n 问题 就 迎刃而解 
了 同时 我们 可以 知道 特征值/n 和/c 特征向量/n 有/v 很多/m 
个/q 当 λ 最大 的 时候 所 对应 的 特征向量 
我们 把 它 叫 作主 成份 向量 如果 需要 将 
m 降 维 为 n 只 需要 去 前 n 
大 的 特征值 所 对应 的 特征向量 即可 方法 二 
对于 上面 二维 降成 一维 的 问题 来说 找到 那个 
使得 方差 最大 的 方向 就 可以 了 不过 对于 
更 高维 首先 我们 希望 找到 一个 方向 基 使得 
投 影后 方差 最大 当 我们 找 第二 个 方向 
基 的 时候 为了 最大 可能 还原 多 的 信息 
我们 显然 不 希望 第二 个 方向 与 第一 个 
方向 有 重复 的 信息 这个 从 向量 的 角度 
看 意味 这 一个 向量 在 另一个 向量 的 投影 
必须 为 0 . 这 就有 这时候 我们 思路 就很 
明了 将 一组 N 维 向量 降为 K 维 K 
大于 0 小于 N 其 目标 是 选择 K 个 
单位 模 为 1 正交基 使得 原始数据 变换 到 这组 
基 上 后 各 字段 两 两间 协方差 为 0 
而 字段 本身 的 方差 则 尽可能 大 还是 假设 
我们 原始数据 为 A 我们 做 一个 处理 A . 
AT 得到 我们 发现 要是 能 找到 一个 基 使得 
这个 矩阵 变成 一个 除了 斜对角 外 其余 全是 0 
的话 那 这个 基 就是 我们 需要 的 基 那么 
问题 就 转换 成 矩阵 的 对角 化了 先 说 
一个 先验 知识 在 线性代数 上 我们 可以 知道 实 
对称矩阵 不同 特征值 对应 的 特征向量 必然 正交 对 一个 
n 行 n 列 的 实 对称矩阵 一定 可以 找到 
n 个 单位 正交 特征向量 设 这 n 个 特征向量 
为 e1 e2 ⋯ en 组合成 矩阵 的 形式 如图 
由上 结论 又 有一个 新的 结论 就是 对于 实 对称矩阵 
A 它 的 特征向量 矩阵 为 E 必然 满足 有了/nr 
这个 先验 知识 我们 假设 原始数据 A 基 为 U 
投 影后 的 数据 为 Y 则有 Y = UA 
根据 上面 所说 的 要是 投 影后 的 矩阵 Y 
的 Y . YT 为 一个 对角 阵 那么 就有 
要是 Y . YT 为 对角 阵 那么 只 需要 
U 是 A . AT 的 特征向量 即可 那么 问题 
最终 还是 转换 为求 AAT 的 特征向量 代码 实现 刚才 
说 了 两种 PCA 的 计算 思路 我们 简单 看下 
代码 的 实现 吧 由于 matlab 自 带了 求 特征向量 
的 函数 这边 使用 matlab 进行 模拟 我们 用 测试数据 
试试 当 我们 只 保留 0.5 的 成分 时 newA 
从3维/nr 降到 1 维 当 进行 还原 时 准确性 也会 
稍微 差些 当 我们 保留 0.9 的 成分 时 newA 
从3维/nr 降到 2 维 当 进行 还原 时 还原 度 
会 稍微 好些 当 我们 保留 0.97 的 成分 时 
就 无法 降 维 了 这时候 就 可以 100% 还原 
了 总结 一下 我们 在 做 机器 学习 的 数据 
分析 的 时候 由于 数据集 的 维度 可能 很高 这时候 
我们 需要 对 数据 进行 降 维 本文 从 各个 
方向 介绍 了 一下 降 维 的 经典 方法 PCA 
也从 代码 的 角度 告诉 了 怎么 降 维 的 
过程 实际操作 可能会 比较 简单 但是 原理 个人 觉得 还是 
有 学习 的 地方 的 相关 阅读 机器 学习 之 
回归 原理 详述 一 机器学习 之 决策树 与 随机 森林 
模型 主流 机器学习 算法 简介 与其 优缺点 分析 此文 已由 
作者 授权 腾讯 云 技术 社区 发布 转载 请 注明 
原文 出处 