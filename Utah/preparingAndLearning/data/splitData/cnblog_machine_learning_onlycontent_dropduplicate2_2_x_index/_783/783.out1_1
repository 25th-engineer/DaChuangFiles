考虑 一个 二分 问题 即将 实例 分成 正 类 positive 
或 负 类 negative 对 一个 二分 问题 来说 会 
出现 四 种 情况 如果 一个 实例 是 正 类 
并且 也被 预测 成正类/nr 即为 真正 类 True positive 如果 
实例 是 负 类 被 预测 成正类/nr 称之为 假 正 
类 False positive 相应 地 如果 实例 是 负 类 
被 预测 成负类/nr 称之 为真 负 类 True negative 正 
类 被 预测 成负类/nr 则为 假 负 类 false negative 
TP 正确 肯定 的 数目 FN 漏报 没有 正确 找到 
的 匹配 的 数目 FP 误报 给出 的 匹配 是 
不 正确 的 TN 正确 拒绝 的 非 匹配 对数 
列联表 如下 表 所示 1 代表 正 类 0 代表 
负 类 预测 1 预测 0 实际 1True Positive TP 
False Negative FN 实际 0False Positive FP True Negative TN 
1 . TPR FPR & TNR 从 列联表 引入 两个 
新名词 其一 是 真正 类 率 true positive rate TPR 
计算公式 为 TPR = TP / TP +   FN 
刻画 的 是 分类器 所 识别 出 的 正 实例 
占 所有 正 实例 的 比例 另外 一个 是 负 
正 类 率 false positive rate   FPR 计算公式 为 
FPR = FP / FP + TN 计算 的 是 
分类器 错 认为 正 类 的 负 实例 占 所有 
负 实例 的 比例 还有 一个 真 负 类 率 
True Negative Rate TNR 也 称为 specificity 计算公式 为 TNR 
= TN / FP +   TN = 1 FPR2 
. 精确 率 Precision 召回率 Recall 和 F1 值 精确 
率 正确率 和/c 召回率/i 是/v 广泛/a 用于/v 信息检索/n 和/c 统计学/nt 
分类/n 领域/n 的/uj 两个/m 度/zg 量值/n 用来 评价 结果 的 
质量 其中 精度 是 检索 出 相关 文档 数 与 
检索 出 的 文档 总数 的 比率 衡量 的 是 
检索 系统 的 查准率 召回率 是 指 检索 出 的 
相关 文档 数 和 文档 库 中 所有 的 相关 
文档 数 的 比率 衡量 的 是 检索 系统 的 
查全率 一般来说 Precision 就是 检索 出来 的 条目 比如 文档 
网页 等 有 多少 是 准确 的 Recall 就是 所有 
准确 的 条目 有 多少 被 检索 出来了 两者 的 
定义 分别 如下 Precision = 提 取出 的 正确 信息 
条数 /   提 取出 的 信息 条数 Recall = 
提 取出 的 正确 信息 条数 /   样本 中 
的 信息 条数 为了 能够 评价 不同 算法 的 优劣 
在 Precision 和 Recall 的 基础 上 提出 了 F1 
值 的 概念 来 对 Precision 和 Recall 进行 整体 
评价 F1 的 定义 如下 F1 值   = 正确率 
* 召回率 * 2 / 正确率 + 召回率 不妨 举 
这样 一个 例子 某 池塘 有 1400条 鲤鱼 300只 虾 
300只 鳖 现在 以 捕 鲤鱼 为 目的 撒 一 
大网 逮 着 了 700条 鲤鱼 200只 虾 100只 鳖 
那么 这些 指标 分别 如下 正确率 = 700 / 700 
+ 200 + 100 = 70% 召回率 = 700 / 
1400 = 50% F1 值 = 70% * 50% * 
2 / 70% + 50% = 58.3% 不妨 看看 如果把 
池子 里 的 所有 的 鲤鱼 虾/n 和鳖都/nr 一网打尽/l 这些 
指标 又 有何 变化 正确率 =   1400 / 1400 
+   300 + 300 = 70% 召回率 =   
1400 / 1400 = 100% F1 值 = 70% * 
100% * 2 / 70% + 100% = 82.35% 由此可见 
正确率 是 评估 捕获 的 成果 中 目标 成果 所 
占得 比例 召回率 顾名思义 就是 从 关注 领域 中 召回 
目标 类别 的 比例 而 F 值 则是 综合 这 
二者 指标 的 评估 指标 用于 综合 反映 整体 的 
指标 当然 希望 检索 结果 Precision 越高 越好 同时 Recall 
也 越高 越好 但 事实上 这 两者 在 某些 情况下 
有 矛盾 的 比如 极端 情况 下 我们 只 搜索 
出 了 一个 结果 且 是 准确 的 那么 Precision 
就是 100% 但是 Recall 就 很低 而 如果 我们 把 
所有 结果 都 返回 那么 比如 Recall 是 100% 但是 
Precision 就会 很低 因此 在 不同 的 场合 中 需要 
自己 判断 希望 Precision 比较 高 或是 Recall 比较 高 
如果 是 做 实验 研究 可以 绘制 Precision Recall 曲线 
来 帮助 分析 3 . 综合 评价 指标 F measurePrecision 
和 Recall 指标 有时候 会 出现 的 矛盾 的 情况 
这样 就 需要 综合 考虑 他们 最 常见 的 方法 
就是 F Measure 又 称为 F Score F Measure 是 
Precision 和 Recall 加权 调和 平均 当 参数 α = 
1时 就是 最 常见 的 F1 因此 F1 综合 了 
P 和R的/nr 结果 当 F1 较高 时 则能 说明 试验 
方法 比较 有效 4 . ROC 曲线 和 AUC4 . 
1 为什么 引入 ROC 曲线 Motivation1 在 一个 二 分类 
模型 中 对于 所 得到 的 连续 结果 假设 已 
确定 一个 阀值 比如说 0.6 大于 这个 值 的 实例 
划归 为 正 类 小于 这个 值 则 划到 负 
类 中 如果 减小 阀值 减到 0.5 固然 能 识别 
出 更多 的 正 类 也 就是 提高 了 识别 
出 的 正 例 占 所有 正 例 的 比 
类 即 TPR 但 同时 也 将 更多 的 负 
实例 当作 了 正 实例 即 提高 了 FPR 为了 
形象化 这一 变化 引入 ROC ROC 曲线 可以 用于 评价 
一个 分类器 Motivation2 在 类 不 平衡 的 情况 下 
如 正 样本 90个 负 样本 10个 直接 把 所有 
样本 分类 为 正 样本 得到 识别率 为 90% 但这 
显然 是 没有 意义 的 单纯 根据 Precision 和 Recall 
来 衡量 算法 的 优劣 已经 不能 表征 这种 病态 
问题 4.2 什么 是 ROC 曲线 ROC Receiver Operating Characteristic 
翻译 为 接受者 操作 特性 曲线 曲线 由 两个 变量 
1 specificity 和 Sensitivity 绘制 . 1 specificity = FPR 
即 负 正 类 率 Sensitivity 即是 真正 类 率 
TPR True positive rate 反映 了 正 类 覆盖 程度 
这个 组合 以 1 specificity 对 sensitivity 即 是以 代价 
costs 对 收益 benefits 此外 ROC 曲线 还 可以 用来 
计算 均值 平均 精度 mean average precision 这是 当 你 
通过 改变 阈值 来 选择 最好 的 结果 时所/nr 得到 
的 平均 精度 PPV 为了 更好 地 理解 ROC 曲线 
我们 使用 具体 的 实例 来 说明 如在 医学 诊断 
中 判断 有病 的 样本 那么 尽量 把 有病 的 
揪 出来 是 主要 任务 也 就是 第一 个 指标 
TPR 要 越高 越好 而把 没病 的 样本 误诊 为 
有病 的 也 就是 第二 个 指标 FPR 要 越低 
越好 不难 发现 这 两个 指标 之间 是 相互 制约 
的 如果 某 个 医生 对于 有病 的 症状 比较 
敏感 稍微 的 小 症状 都 判断 为 有病 那么 
他 的 第一 个 指标 应该 会 很高 但是 第二 
个 指标 也 就 相应 地 变 高 最 极端 
的 情况 下 他 把 所有 的 样本 都 看做 
有病 那么 第一 个 指标 达到 1 第二 个 指标 
也为 1 我们 以 FPR 为 横轴 TPR 为 纵轴 
得到 如下 ROC 空间 我们 可以 看出 左上角 的 点 
TPR = 1 FPR = 0 为 完美 分类 也 
就是 这个 医生 医术 高明 诊断 全对 点 A TPR 
FPR 医生 A 的 判断 大体 是 正确 的 中 
线上 的 点 B TPR = FPR 也 就是 医生 
B 全都 是 蒙的/nr 蒙对 一半 蒙错/nr 一半 下半 平面 
的 点 C TPR FPR 这个 医生 说 你 有病 
那么 你 很 可能 没 有病 医生 C 的话 我们 
要 反 着 听 为真 庸医 上图 中 一个 阈值 
得到 一个 点 现在 我们 需要 一个 独立 于 阈值 
的 评价 指标 来 衡量 这个 医生 的 医术 如何 
也 就是 遍历 所有 的 阈值 得到 ROC 曲线 还是 
一 开始 的 那幅 图 假设 如下 就是 某 个 
医生 的 诊断 统计图 直线 代表 阈值 我们 遍历 所有 
的 阈值 能够 在 ROC 平面 上 得到 如下 的 
ROC 曲线 曲线 距离 左上角 越近/nr 证明 分类器 效果 越好 
如上 是 三条 ROC 曲线 在 0.23处 取 一条 直线 
那么 在 同样 的 低 FPR = 0.23 的 情况 
下 红色 分类器 得到 更高 的 PTR 也就 表明 ROC 
越 往上 分类器 效果 越好 我们 用 一个 标 量值 
AUC 来 量化 它 4.3 什么 是 AUC AUC 值 
为 ROC 曲线 所 覆盖 的 区域 面积 显然 AUC 
越大 分类器 分类 效果 越好 AUC = 1 是 完美 
分类器 采用 这个 预测模型 时 不管 设定 什么 阈值 都能 
得出 完美 预测 绝大多数 预测 的 场合 不 存在 完美 
分类器 0.5 AUC 1 优于 随机 猜测 这个 分类器 模型 
妥善 设定 阈值 的话 能有 预测 价值 AUC = 0.5 
跟 随机 猜测 一样 例 丢 铜板 模型 没有 预测 
价值 AUC 0.5 比 随机 猜测 还差 但 只要 总是 
反 预测 而行 就 优于 随机 猜测 AUC 的 物理 
意义 假设 分类器 的 输出 是 样本 属 于正 类 
的 socre 置信度 则 AUC 的 物理 意义 为 任取 
一对 正 负 样本 正 样本 的 score 大于 负 
样本 的 score 的 概率 4.4 怎样 计算 AUC 第 
一种 方法 AUC 为 ROC 曲 线下 的 面积 那 
我们 直接 计算 面积 可得 面积 为 一个 个 小 
的 梯形 面积 之和 计算 的 精度 与 阈值 的 
精度 有关 第二 种 方法 根据 AUC 的 物理 意义 
我们 计算 正 样本 score 大于 负 样本 的 score 
的 概率 取 N * M N 为 正 样本数 
M 为 负 样本数 个 二 元组 比较 score 最后 
得到 AUC 时间 复杂度 为 O N * M 第三 
种 方法 与 第二 种 方法 相似 直接 计算 正 
样本 score 大于 负 样本 的 概率 我们 首先 把 
所有 样本 按照 score 排序 依次 用 rank 表示 他们 
如 最大 score 的 样本 rank = n n = 
N + M 其次 为 n 1 那么 对于 正 
样本 中 rank 最大 的 样本 rank _ max 有M/nr 
1个 其他 正 样本 比 他 score 小 那么 就有 
rank _ max 1 M 1 个 负 样本 比 
他 score 小 其次 为 rank _ second 1 M 
2 最后 我们 得到 正 样本 大于 负 样本 的 
概率 为 时间 复杂度 为 O N + M 5 
. 参考 内容 1 . 机器学习 指标 大 汇总 http 
/ / www . 36dsj . com / archives / 
42271 