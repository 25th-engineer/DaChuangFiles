强化 学习 概况 正如 在 前面 所 提到 的 强化 
学习 是 指 一种 计算机 以 试错 的 方式 进行 
学习 通过 与 环境 进行 交互 获得 的 奖赏 指导 
行为 目标 是 使 程序 获得 最大 的 奖赏 强化 
学习 不同 于连 督 学习 区别 主要 表现 在 强化 
信号 上 强化 学习 中 由 环境 提供 的 强化 
信号 是 对 产生 动作 的 好坏 作 一种 评价 
通常 为 标量 信号 而 不是 告诉 强化 学习 系统 
如 何去 产生 正确 的 动作 唯一 的 目的 是 
最大化 效率 和/或/nr 性能 算法 对 正确 的 决策 给予 
奖励 对 错误 的 决策 给予 惩罚 如下 图 所示 
持续 的 训练 是 为了 不断 提高 效率 这里 的 
重点 是 性能 这 意味着 我们 需要 在看 不见 的 
数据 和 算法 已经 学过 的 东西 之间 找到 一种 
平衡 该 算法 将 一个 操作 应用到 它 的 环境 
中 根据 它 所做 的 行为 接受 奖励 或 惩罚 
不断 的 重复 这个 过程 等等 接下来 让 我们 看 
一个 程序 概念 是 相似 的 尽管 它 的 规模 
和 复杂性 很低 想象 一下 是 什么 让 自动 驾驶 
的 车辆 从 一个 地点 移动 到 了 另一个 点 
让 我们 看看 我们 的 应用 程序 在 这里 可以 
看到 我们 有 一个 非常 基本 的 地图 一个 没有 
障碍 但有 外部 限制 的 墙 黑 色块 start 是 
我们 的 对象 红 色块 stop 是 我们 的 目标 
在 这个 应用 程序 中 我们 的 目标 是 让 
我们 的 对象 在 墙壁 以内 到达 目标 位置 如果 
我们 的 下一步 把 我们 的 对象 放在 一个 白色 
的 方块 上 我们 的 算法 将 得到 奖励 如果 
我们 的 下 一步 行动 超出 墙壁 的 围地 范围 
我们 将 受到 惩罚 在 这个 例子 中 它 的 
路径 上 绝对 没有 障碍 所以 我们 的 对象 应该 
能够 到达 它 的 目的地 问题 是 它 能 多快 
学会 下面 是 另一个 比较 复杂 的 地图 示例 学习类型 
在 应用 程序 的 右边 是 我们 的 设置 如 
下面 的 屏幕截图 所示 我们 首先 看到 的 是 学习 
算法 在 这个 应用 中 我们 将 处理 两种 不同 
的 学习 算法 Q learning 和 state action reward state 
action SARSA 让 我们 简要 讨论 一下 这 两种 算法 
Q learningQ learning 可以 在 没有 完全 定义 的 环境 
模型 的 情况 下 识别 给定 状态下 的 最优 行为 
在 每个 状态 中 值 最高 的 行为 它 还 
擅长 处理 随机 转换 和 奖励 的 问题 而 不 
需要 调整 或 适应 以下 是 Q learning 的 数学 
表达式 如果 我们 提供 一个 非常 高级 的 抽象 示例 
可能 更 容易 理解 程序 从 状态 1 开始 然后 
它 执行 动作 1 并 获得 奖励 1 接下来 它 
四处寻找 状态 2中 某个 行为 的 最大 可能 奖励 是 
多少 然后 使用 它 来 更 新动作 1 的 值 
等等 SARSASARSA 的 工作 原理 是 这样 的 1 . 
程序 从 状态 1 开始 2 . 然后 它 执行 
动作 1 并 获得 奖励 1 3 . 接下来 它 
进入 状态 2 执行 动作 2 并 获得 奖励 2 
4 . 然后 程序 返回 并 更新 动作 1 的 
值 这 Q learning 算法 的 不同 之 处 在于 
找到 未来 奖励 的 方式 Q learning 使用 状态 2中 
奖励 最高 的 动作 的 值 而 SARSA 使用 实际 
动作 的 值 这是 SARSA 的 数学 表达式 运行 我们 
的 应用 程序 现在 让 我们 开始 使用 带有 默认 
参数 的 应用 程序 只需 点击 开始 按钮 学习 就 
开始 了 完成后 您 将 能够 单击 Show Solution 按钮 
学习 路径 将 从头到尾 播放 点击 Start 开始 学习 阶段 
一直 到 黑色 物体 达到目标 对于 每个 迭代 将 评估 
不同 的 对象 位置 以及 它们 的 操作 和 奖励 
一旦 学习 完成 我们 可以 单击 Show Solution 按钮 来 
重播 最终 的 解决方案 完成后 黑色 对象 将 位于 红色 
对象 之上 现在 让 我们 看看 应用 程序 中 的 
代码 有 两种 我们 之前 强调 过 的 学习 方法 
Q learning 是 这样 的 / / / summary / 
/ / Q Learning 线程 / / / / summary 
private void QLearningThread { / / 迭代 次数 int iteration 
= 0 T a b u e a r c 
h E x p l o r a t i 
o n tabuPolicy = T a b u e a 
r c h E x p l o r a 
t i o n qLearning . E x p l 
o r a t i o n P o l 
i c y E p s i l o n 
G r e e d y E x p l 
o r a t i o n e x p 
l o r a t i o n P o 
l i c y = E p s i l 
o n G r e e d y E x 
p l o r a t i o n tabuPolicy 
. BasePolicy while needToStop & & iteration l e a 
r n i n g I t e r a 
t i o n s { e x p l 
o r a t i o n P o l 
i c y . Epsilon = explorationRate double iteration / 
l e a r n i n g I t 
e r a t i o n s * explorationRate 
qLearning . LearningRate = learningRate double iteration / l e 
a r n i n g I t e r 
a t i o n s * learningRate tabuPolicy . 
ResetTabuList var agentCurrentX = agentStartX var agentCurrentY = agentStartY int 
steps = 0 while needToStop & & agentCurrentX = agentStopX 
| | agentCurrentY = agentStopY { steps + + int 
currentState = GetStateNumber agentCurrentX agentCurrentY int action = qLearning . 
GetAction currentState double reward = U p d a t 
e A g e n t P o s i 
t i o n ref agentCurrentX ref agentCurrentY action int 
nextState = GetStateNumber agentCurrentX agentCurrentY / / 更新 对象 的 
qLearning 以 设置 禁忌 行为 qLearning . UpdateState currentState action 
reward nextState tabuPolicy . SetTabuAction action + 2 % 4 
1 } System . Diagnostics . Debug . WriteLine steps 
iteration + + SetText iterationBox iteration . ToString } EnableControls 
true }/i SARSA/w 学习/v 有何/nr 不同/a 让 我们 来 看看 
SARSA 学习 的 while 循环 并 理解 它 / / 
/ summary / / / Sarsa 学习 线程 / / 
/ / summary private void SarsaThread { int iteration = 
0 T a b u e a r c h 
E x p l o r a t i o 
n tabuPolicy = T a b u e a r 
c h E x p l o r a t 
i o n sarsa . E x p l o 
r a t i o n P o l i 
c y E p s i l o n G 
r e e d y E x p l o 
r a t i o n e x p l 
o r a t i o n P o l 
i c y = E p s i l o 
n G r e e d y E x p 
l o r a t i o n tabuPolicy . 
BasePolicy while needToStop & & iteration l e a r 
n i n g I t e r a t 
i o n s { e x p l o 
r a t i o n P o l i 
c y . Epsilon = explorationRate double iteration / l 
e a r n i n g I t e 
r a t i o n s * explorationRate sarsa 
. LearningRate = learningRate double iteration / l e a 
r n i n g I t e r a 
t i o n s * learningRate tabuPolicy . ResetTabuList 
var agentCurrentX = agentStartX var agentCurrentY = agentStartY int steps 
= 1 int previousState = GetStateNumber agentCurrentX agentCurrentY int previousAction 
= sarsa . GetAction previousState double reward = U p 
d a t e A g e n t P 
o s i t i o n ref agentCurrentX ref 
agentCurrentY previousAction while needToStop & & agentCurrentX = agentStopX | 
| agentCurrentY = agentStopY { steps + + tabuPolicy . 
SetTabuAction previousAction + 2 % 4 1 int nextState = 
GetStateNumber agentCurrentX agentCurrentY int nextAction = sarsa . GetAction nextState 
sarsa . UpdateState previousState previousAction reward nextState nextAction reward = 
U p d a t e A g e n 
t P o s i t i o n ref 
agentCurrentX ref agentCurrentY nextAction previousState = nextState previousAction = nextAction 
} if needToStop { sarsa . UpdateState previousState previousAction reward 
} System . Diagnostics . Debug . WriteLine steps iteration 
+ + SetText iterationBox iteration . ToString } / / 
启用 设置 控件 EnableControls true } 最后 一步 看看 如何 
使 解决方案 具有 动画 效果 我们 需要 这样 才能 看到 
我们 的 算法 是否 实现 了 它 的 目标 代码 
如下 T a b u e a r c h 
E x p l o r a t i o 
n tabuPolicy if qLearning = null tabuPolicy = T a 
b u e a r c h E x p 
l o r a t i o n qLearning . 
E x p l o r a t i o 
n P o l i c y else if sarsa 
= null tabuPolicy = T a b u e a 
r c h E x p l o r a 
t i o n sarsa . E x p l 
o r a t i o n P o l 
i c y else throw new Exception var e x 
p l o r a t i o n P 
o l i c y = E p s i 
l o n G r e e d y E 
x p l o r a t i o n 
tabuPolicy . BasePolicy e x p l o r a 
t i o n P o l i c y 
. Epsilon = 0 tabuPolicy . ResetTabuList int agentCurrentX = 
agentStartX agentCurrentY = agentStartY Array . Copy map mapToDisplay mapWidth 
* mapHeight mapToDisplay agentStartY agentStartX = 2 mapToDisplay agentStopY agentStopX 
= 3 这 是 我们 的 while 循环 所有 神奇 
的 事情 都 发生 在 这里 while needToStop { cellWorld 
. Map = mapToDisplay Thread . Sleep 200 if agentCurrentX 
= = agentStopX & & agentCurrentY = = agentStopY { 
mapToDisplay agentStartY agentStartX = 2 mapToDisplay agentStopY agentStopX = 3 
agentCurrentX = agentStartX agentCurrentY = agentStartY cellWorld . Map = 
mapToDisplay Thread . Sleep 200 } mapToDisplay agentCurrentY agentCurrentX = 
0 int currentState = GetStateNumber agentCurrentX agentCurrentY int action = 
qLearning . GetAction currentState sarsa . GetAction currentState U p 
d a t e A g e n t P 
o s i t i o n ref agentCurrentX ref 
agentCurrentY action mapToDisplay agentCurrentY agentCurrentX = 2 } 让 我们 
把 它 分成 更容易 消化 的 部分 我们 要 做 
的 第一 件 事 就是 建立 禁忌 政策 如果 您 
不熟悉 tabu 搜索 请注意 它 的 目的 是 通过 放松 
其 规则 来 提高 本地 搜索 的 性能 在 每一步 
中 如果 没有 其他 选择 有 回报 的 行动 有时 
恶化 行动 是 可以 接受 的 此外 还 设置 了 
prohibition tabu 以 确保 算法 不 会 返回 到 以前 
访问 的 解决方案 T a b u e a r 
c h E x p l o r a t 
i o n tabuPolicy if qLearning = null tabuPolicy = 
T a b u e a r c h E 
x p l o r a t i o n 
qLearning . E x p l o r a t 
i o n P o l i c y else 
if sarsa = null tabuPolicy = T a b u 
e a r c h E x p l o 
r a t i o n sarsa . E x 
p l o r a t i o n P 
o l i c y else throw new Exception var 
e x p l o r a t i o 
n P o l i c y = E p 
s i l o n G r e e d 
y E x p l o r a t i 
o n tabuPolicy . BasePolicy e x p l o 
r a t i o n P o l i 
c y . Epsilon = 0 tabuPolicy . ResetTabuList 接下来 
我们 要 定位 我们 的 对象 并 准备 地图 int 
agentCurrentX = agentStartX agentCurrentY = agentStartY Array . Copy map 
mapToDisplay mapWidth * mapHeight mapToDisplay agentStartY agentStartX = 2 mapToDisplay 
agentStopY agentStopX = 3 下面 是 我们 的 主 执行 
循环 它 将以 动画 的 方式 显示 解决方案 while needToStop 
{ cellWorld . Map = mapToDisplay Thread . Sleep 200 
if agentCurrentX = = agentStopX & & agentCurrentY = = 
agentStopY { mapToDisplay agentStartY agentStartX = 2 mapToDisplay agentStopY agentStopX 
= 3 agentCurrentX = agentStartX agentCurrentY = agentStartY cellWorld . 
Map = mapToDisplay Thread . Sleep 200 } mapToDisplay agentCurrentY 
agentCurrentX = 0 int currentState = GetStateNumber agentCurrentX agentCurrentY int 
action = qLearning . GetAction currentState sarsa . GetAction currentState 
U p d a t e A g e n 
t P o s i t i o n ref 
agentCurrentX ref agentCurrentY action mapToDisplay agentCurrentY agentCurrentX = 2 } 
汉诺塔 游戏 河内 塔 由 三根 杆子 和最/nr 左边 的 
几个 按顺序 大小 排列 的 圆盘 组成 目标 是 用 
最少 的 移动 次数 将 所有 磁盘 从最/nr 左边 的 
棒子 移动 到 最 右边 的 棒子 你 必须 遵守 
的 两条 重要 规则 是 一次 只能 移动 一个 磁盘 
不能 把 大 磁盘 放在 小 磁 盘上 也 就是说 
在 任何 棒 中 磁盘 的 顺序 必须 始终 是从 
底部 最大 的 磁盘 到 顶部 最小 的 磁盘 如下 
所示 假设 我们 使用 三个 磁盘 如图所示 在 这种 情况 
下 有 33种 可能 的 状态 如下 图 所示 河内 
塔 谜题 中 所有 可能 状态 的 总数 是 3 
的 磁盘 数 次幂 其中 | | | | 是 
集合 状态 中 的 元素 个数 n 是 磁盘 的 
个数 在 我们 的 例子 中 我们 有3×/nr 3 × 
3 = 27个 圆 盘在 这 三根 棒 上 分布 
的 唯一 可能 状态 包括 空棒/nr 但是 两个 空棒/nr 可以 
处于 最 大 状态 定义 了 状态 总数 之后 下面 
是 我们 的 算法 从 一种 状态 移动 到 另一种 
状态 的 所有 可能 操作 这个 谜题 的 最小 可能 
步数 是 磁盘 的 数量 是 n Q learning 算法 
的 正式 定义 如下 在 这个 Q learning 算法 中 
我们 使用 了 以下 变量 Q 矩阵 一个二维 数组 首先 
对 所有 元素 填充 一个 固定值 通常 为 0 用于 
保存 所有 状态下 的 计算 策略 也 就是说 对于 每一个 
状态 它 持有 对 各自 可能 的 行动 的 奖励 
折扣 因子 决定 了 对象 如何 处理 奖励 的 政策 
当 贴现率 接近 0时 只 考虑 当前 的 报酬 会使 
对象 变得 贪婪 而 当 贴现率 接近 1时 会使 对象 
变得 更具 策略性 和 前瞻性 从而 在 长期 内 获得 
更好 的 报酬 R 矩阵 包含 初始 奖励 的 二维 
数组 允许 程序 确定 特定 状态 的 可能 操作 列表 
我们 应该 简要 介绍 一下 Q learning Class 的 一些 
方法 Init 生成 所有 可能 的 状态 以及 开始 学习 
过程 Learn 在 学习 过程 中 有 顺序 的 步骤 
InitRMatrix 这个 初始化 奖励 矩阵 的 值 如下 1 . 
0 在 这种 状态 下 我们 没有 奖励 2 . 
100 这 是 我们 在 最终 状态下 的 最大 奖励 
我们 想去 的 地方 3 . X 在 这种 情况下 
是 不 可能 采取 这种 行动 的 TrainQMatrix 包含 Q 
矩阵 的 实际 迭代 值更 新规则 完成后 我们 希望 得到 
一个 训练有素 的 对象 NormalizeQMatrix 使 Q 矩阵 的 值 
标准化 使 它们 成为 百分数 Test 提供 来自 用户 的 
文本 输入 并 显示 解决 此 难题 的 最佳 最短 
路径 让 我们 更 深入 地 研究 我们 的 TrainQMatrix 
的 代码 / / / summary / / / 训练 
Q 矩阵 / / / / summary / / / 
param name = _ StatesMaxCount 所有 可能 移动 的 个数 
/ param private void TrainQMatrix int _ StatesMaxCount { pickedActions 
= new Dictionary int int / / 可用 操作 列表 
基于 R 矩阵 其中 包含 从 某个 状态 开始 的 
允许 的 下一个 操作 在 数组 中为 0 List int 
nextActions = new List int int counter = 0 int 
rIndex = 0 / / 3 乘以 所有 可能 移动 
的 个数 就有 足够 的 集 来 训练 Q 矩阵 
while counter 3 * _ StatesMaxCount { var init = 
Utility . GetRandomNumber 0 _ StatesMaxCount do { / / 
获得 可用 的 动作 nextActions = GetNextActions _ StatesMaxCount init 
/ / 从 可用 动作 中 随机 选择 一个 动作 
if nextActions = null { var nextStep = Utility . 
GetRandomNumber 0 nextActions . Count nextStep = nextActions nextStep / 
/ 获得 可用 的 动作 nextActions = GetNextActions _ StatesMaxCount 
nextStep / / 设置 从该/nr 状态 采取 的 动作 的 
索引 for int i = 0 i 3 i + 
+ { if R = null & & R init 
i 1 = = nextStep rIndex = i } / 
/ 这是 值 迭代 更 新规则 折现系数 是 0.8 Q 
init nextStep = R init rIndex 0 + 0.8 * 
Utility . GetMax Q nextStep nextActions / / 将 下 
一步 设置 为 当前 步骤 init = nextStep } } 
while init = FinalStateIndex counter + + } } 使用 
三个 磁盘 运行 应用程序 使用 四个 磁盘 运行 应用程序 这里有 
7个 磁盘 最佳 移动 步数 是 127 所以 你 可以 
看到 解决 方案 可以 多 快地 乘以 可能 的 组合 
总结 这种 形式 的 强化 学习 更 正式 地 称为 
马尔可夫 决策 过程 Markov Decision Process MDP MDP 是 一个 
离散 时间 随机 控制 的 过程 这 意味着 在 每个 
时间 步 在 状态 x 下 决策 者 可以 选择 
任何 可用 的 行动 状态 这个 过程 将 在下 一步 
反应 随机 移动 到 一个 新 的 状态 给 决策 
者 一个 奖励 进程 进入 新 状态 的 概率 由 
所选 动作 决定 因此 下一个 状态 取决于 当前 状态 和 
决策者 的 行为 给定 状态 和 操作 下/f 一步/m 完全/ad 
独立/v 于/p 之前/f 的/uj 所有/b 状态/n 和/c 操作/v 