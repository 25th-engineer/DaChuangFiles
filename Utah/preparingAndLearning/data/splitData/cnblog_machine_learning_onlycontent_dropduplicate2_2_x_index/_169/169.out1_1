一 引言 本 程序 是 一个 完整 的 机器 学习 
过程 先 编写 基于 python 的 爬虫 脚本 爬 取 
目标 论坛 网站 的 评论 到 本地 存储 然后 使用 
贝叶斯 分类 模型 对 评论 进行 分类 预测 新 的 
评论 是否 为 垃圾 评论 如果 遇到 大 数据 量 
的 问题 可以 把 贝叶斯 算法 写成 mapreduce 模式 map 
负责 把 数据集 划分 成 键值 对 格式 类 序号 
为 key 属 性 向量 为 value reduce 进行 汇总 
每类 的 先验概率 和 条件概率 主 server 汇总 所有 类 
的 统计量 二 爬虫 脚本 1 编写 爬虫 脚本 爬 
取 目标 论坛 的 评论 其中 headers 是 必须 的 
因为 我们 需要 伪装成 浏览器 在 访问 论坛 的 服务器 
使用 requests 包 获取 指定 url 的 数据 流 mport 
requests from bs4 import BeautifulSoup import re import json import 
time headers = { Accept text / html application / 
xhtml + xml application / xml q = 0.9 * 
/ * q = 0.8 Accept Language zh CN zh 
q = 0.8 en US q = 0.5 en q 
= 0.3 Accept Encoding gzip deflate br Connection keep alive 
Cookie _ _ cfduid = d 6 5 3 b 
f 9 3 1 c b d e 1 0 
f 9 2 4 3 b 6 3 e 9 
9 1 f 7 0 d c 4 1 4 
6 6 7 7 8 5 8 5 loid = 
a 5 W U n H R H l l 
e K L 9 O R loidcreated = 2016 06 
24T14 % 3A29 % 3A45 . 413Z _ recent _ 
srs = t5 _ 2qu49 _ ga = GA1 . 
2.54465388 . 1466778724 pc = ne _ _ utma = 
55650728 . 54465388 . 1466778724 . 1466778728 . 1466843492.2 _ 
_ utmz = 55650728 . 1466778728 . 1.1 . utmcsr 
= direct | utmccn = direct | utmcmd = none 
_ _ utmb = 55650728.0 . 10.1466843492 _ _ utmc 
= 55650728 Host www . reddit . com User Agent 
Mozilla / 5.0 Windows NT 10.0 WOW64 rv 47.0 Gecko 
/ 20100101 Firefox / 47.0 } url = https / 
/ www . reddit . com / r / AskReddit 
/ comments / 4qfh01 / what _ are _ some 
_ classes _ you _ must _ take _ in 
/ r = requests . get url headers = headers 
r . encoding = r . apparent _ encoding2 使用 
BeautifulSoup 解析 爬 去 的 html 文件 css 定位 我们 
需要 的 字段 输出 到 本地 文件 comments . txt 
保存 即可 soup = BeautifulSoup r . text res = 
soup . select div . md comments = for item1 
in res 1 comments . append item1 . contents print 
comments fd = open comments . txt w + p 
_ soup = BeautifulSoup str comments res2 = p _ 
soup . findAll p for item2 in res2 ct = 
str item2 . contents . encode utf 8 print ct 
3 2 fd . write ct 3 2 + \ 
n fd . close 三 实战 1 文本 分类 应用 
过滤 恶意 留言 等 下面 是 二分 类 问题 文档 
只能 属于 0 和1/nr 两个 类别 1 载入 数据集 本地 
读取 文件 comments . txt 中 爬虫 爬 取 的 
评论 from numpy import * def loadDataSet fd = open 
comments . txt postingList = classVec = for line in 
fd . readlines tmp = line . split postingList . 
append tmp 1 classVec . append int tmp 0 return 
postingList classVec2 创建 词汇表 利用 集合 结构 内 元素 的 
唯一性 创建 一个 包含 所有 词汇 的 词表 def createVocabList 
dataSet vocabSet = set   # create empty setfor document 
in dataSet vocabSet = vocabSet | set document # union 
of the two setsreturn list vocabSet 3 把 输入 文本 
根据 词表 转化 为 计算机 可 处理 的 01 向量 
形式 eq 测试 文本 1 love my dalmation 词汇表 cute 
love help garbage quit I problems is park stop flea 
dalmation licks food not him buying posting has worthless ate 
to maybe please dog how stupid so take mr steak 
my 向 量化 结果 0 1 0 0 0 0 
0 0 0 0 0 1 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 
0 0 0 0 0 1 def setOfWords2Vec vocabList inputSet 
returnVec = 0 * len vocabList for word in inputSet 
if word in vocabList returnVec vocabList . index word = 
1else print the word % s is not in my 
Vocabulary % wordreturn returnVec4 训练 模型 在 训练 样本 中 
计算 先验概率 p Ci 和 条件概率   p x y 
| Ci 本/r 实例/n 有/v 0/m 和1/nr 两个/m 类别/n 所以 
返回 p x y | 0 p x y | 
1 和p/nr Ci 此处 有 两个 改进 的 地方 1 
若有 的 类别 没有 出现 其 概率 就是 0 会 
十分 影响 分类器 的 性能 所以 采取 各 类别 默认 
1次 累加 总 类别 两类 次数 2 这样 不 影响 
相对 大小 2 若 很小 是 数字 相乘 则 结果 
会 更小 再 四舍五入 存在 误差 而且 会 造成 下 
溢出 采取 取 log 乘法 变为 加法 并且 相对 大 
小趋势 不变 def trainNB0 trainMatrix trainCategory numTrainDocs = len trainMatrix 
numWords = len trainMatrix 0 pAbusive = sum trainCategory / 
float numTrainDocs p0Num = ones numWords p1Num = ones numWords 
          # change to ones p0Denom 
= 2.0 p1Denom = 2.0           
                    
                # change 
to 2 . 0for i in range numTrainDocs if trainCategory 
i = = 1 p1Num + = trainMatrix i p1Denom 
+ = sum trainMatrix i else p0Num + = trainMatrix 
i p0Denom + = sum trainMatrix i p1Vect = log 
p1Num / p1Denom               
    # change to log p0Vect = log p0Num 
/ p0Denom                 
  # change to log return p0Vect p1Vect pAbusive5 分类 
根据 计算 后 哪个 类别 的 概率 大 则 属于 
哪个 类别 def classifyNB vec2Classify p0Vec p1Vec pClass1 p1 = 
sum vec2Classify * p1Vec + log pClass1 # element wise 
mult p0 = sum vec2Classify * p0Vec + log 1.0 
pClass1 if p1 p0 return 1 else return 06 测试函数 
加载 数据集 + 提炼 词表 训练 模型 根据 六条 训练 
集 计算 先验概率 和 条件概率 测试 模型 对 训练 两条 
测试 文本 进行 分类 def testingNB listOPosts listClasses = loadDataSet 
myVocabList = createVocabList listOPosts trainMat = for postinDoc in listOPosts 
trainMat . append setOfWords2Vec myVocabList postinDoc p0V p1V pAb = 
trainNB0 array trainMat array listClasses testEntry = friends my take 
thisDoc = array setOfWords2Vec myVocabList testEntry print testEntry classified as 
classifyNB thisDoc p0V p1V pAb testEntry = stupid garbage thisDoc 
= array setOfWords2Vec myVocabList testEntry print testEntry classified as classifyNB 
thisDoc p0V p1V pAb 缺点 词表 只能 记录 词汇 是否 
出现 不能 体现 这个 词汇 出现 的 次数 改进 方法 
采 用词 袋 模型 见 下面 垃圾邮件 分类 实战 结果 
图 friends wish classes 被 分到 正常 评论 类 stupid 
garbage 被 分到 垃圾 评论 类 分类 结果 正确 四 
算法 的 MapReduce 形式 本人 正在 把 这个 贝叶斯 分类 
算法 转换成 分布式 算法 初步 思想 是 可以 把 贝叶斯 
算法 写成 mapreduce 模式 map 负责 把 数据集 划分 成 
键值 对 格式 类 序号 为 key 属 性 向量 
为 value reduce 进行 汇总 每类 的 先验概率 和 条件概率 
主 server 汇总 所有 类 的 统计量 1 mapper 程序 
if _ _ name _ _ = = _ _ 
main _ _ for line in sys . stdin line 
= line . strip word = line . split key 
= word 0 value = for item in word 1 
value + = item + print % s \ t 
% s % key value 2 本次 mapper 测试 结果 
键值 对 成功 分离   执行命令 $ cat data . 
txt | python bayes _ mapper . py1 0.270252528981 0.102916847315 
0 0.772917922479 0.182969066019 1 0.817848764874 0.743666751784 0 0.197846533796 0.835258987261 1 
0.174895157684 0.31219280438 1 0.16756664003 0.529593388634 1 0.56918073026 0.0624409762296 1 0.292857532814 
0.152683257148 0 0.971077138206 0.712432682621 0 0.775544377315 0.1631659099543 reducer 程序 正在 
编写 下次 更新 备注 数据集 生成 脚本 # / usr 
/ bin / python import sys import random if _ 
_ name _ _ = = _ _ main _ 
_ if len sys . argv 3 row = 5 
col = 5 else row = int sys . argv 
1 col = int sys . argv 2 for r 
in range 0 row tmp = str random . randint 
0 1 for c in range 0 col num = 
random . uniform 0 1 tmp + = + str 
num print tmp 生成 数据集 如下 1 0.270252528981 0.102916847315 0 
0.772917922479 0.182969066019 1 0.817848764874 0.743666751784 0 0.197846533796 0.835258987261 1 0.174895157684 
0.31219280438 1 0.16756664003 0.529593388634 1 0.56918073026 0.0624409762296 1 0.292857532814 0.152683257148 
0 0.971077138206 0.712432682621 0 0.775544377315 0.163165909954 