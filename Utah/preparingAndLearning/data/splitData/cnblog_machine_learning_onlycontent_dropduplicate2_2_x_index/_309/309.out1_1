今天 我们 聊聊 机器学习 中 出现 的 非常 频繁 的 
问题 过拟合 与 规则化 我们 先 简单 的 来 理解 
下 常用 的 L0 L1 L2 和核/nr 范数 规则化 最后 
聊 下 规则化 项 参数 的 选择 问题 这里 因为 
篇幅 比较 庞大 为了 不 吓到 大家 我 将 这个 
五 个 部分 分成 两 篇 博文 知识 有限 以下 
都是 我 一些 浅显 的 看法 如果 理解 存在 错误 
希望 大家 不吝指正 谢谢 监督 机器学习 问题 无非 就是 minimizeyour 
error while regularizing your parameters 也 就是 在 规则化 参数 
的 同时 最小化 误差 最小化 误差 是 为了 让 我们 
的 模型 拟合 我们 的 训练 数据 而 规则化 参数 
是 防止 我们 的 模型 过分 拟合 我们 的 训练 
数据 多么 简约 的 哲学 啊 因为 参数 太多 会 
导致 我们 的 模型 复杂度 上升 容易 过拟合 也 就是 
我们 的 训练 误 差会 很小 但 训练 误差 小 
并 不是 我们 的 最终 目标 我们 的 目标 是 
希望 模型 的 测试 误差 小 也 就是 能 准确 
的 预测 新的 样本 所以 我们 需要 保证 模型 简单 
的 基础 上 最小化 训练 误差 这样 得到 的 参数 
才具 有好 的 泛化 性能 也 就是 测试 误差 也 
小 而 模型 简单 就是 通过 规则 函数 来 实现 
的 另外 规则 项的/nr 使用 还 可以 约束 我们 的 
模型 的 特性 这样 就 可以 将 人 对 这个 
模型 的 先验 知识 融入 到 模型 的 学习 当中 
强行 地 让 学习 到 的 模型 具有 人 想要 
的 特性 例如 稀疏 低 秩 平滑 等等 要知道 有时候 
人 的 先验 是 非常 重要 的 前人 的 经验 
会 让 你 少 走 很多 弯路 这 就是 为什么 
我们 平时 学习 最好 找个 大牛 带带 的 原因 一句 
点拨 可以 为 我们 拨开 眼前 乌云 还 我们 一片 
晴空万里 醍醐灌顶 对 机器 学习 也 是 一样 如果 被 
我们 人 稍微 点拨 一下 它 肯定 能 更快 的 
学习 相应 的 任务 只是 由于 人 和 机器 的 
交流 目前 还 没有 那么 直接 的 方法 目前 这个 
媒介 只能 由 规则 项来/nr 担当 了 还有 几 种 
角度 来 看待 规则化 的 规则化 符合 奥卡姆 剃刀 Occam 
s razor 原理 这 名字 好 霸气 razor 不过 它 
的 思想 很 平易近人 在 所有 可能 选择 的 模型 
中 我们 应该 选择 能够 很好 地 解释 已知 数据 
并且 十分 简单 的 模型 从 贝叶斯 估计 的 角度 
来看 规则化/n 项/n 对应/vn 于/p 模型/n 的/uj 先验概率/l 民间 还有 
个 说法 就是 规则化 是 结构 风险 最小化 策略 的 
实现 是 在 经验 风险 上加 一个 正则化 项 regularizer 
或 惩罚 项 penalty term 一般来说 监督 学习 可以 看做 
最小化 下面 的 目标 函数 其中 第一项 L yi f 
xi w 衡量 我们 的 模型 分类 或者 回归 对 
第 i 个 样本 的 预测 值 f xi w 
和 真实 的 标签 yi 之前 的 误差 因为 我们 
的 模型 是 要 拟合 我们 的 训练 样本 的 
嘛 所以 我们 要求 这 一项 最小 也 就是 要求 
我们 的 模型 尽量 的 拟合 我们 的 训练 数据 
但 正如 上面 说 言 我们 不仅 要 保证 训练 
误差 最小 我们 更 希望 我们 的 模型 测试 误差 
小 所以 我们 需要 加 上 第二 项 也 就是 
对 参数 w 的 规则化 函数 Ω w 去 约束 
我们 的 模型 尽量 的 简单 OK 到 这里 如果 
你 在 机器学习 浴血奋战 多年 你 会 发现 哎哟哟 机器/n 
学习/v 的/uj 大部/n 分带/n 参/n 模型/n 都和/nr 这个/r 不但/c 形似/v 
而且 神似 是的 其实 大部分 无非 就是 变换 这两项 而已 
对于 第一项 Loss 函数 如果 是 Square loss 那 就是 
最 小二 乘了 如果 是 Hinge Loss 那 就是 著名 
的 SVM 了 如果 是 exp Loss 那 就是 牛逼 
的 Boosting 了 如果 是 log Loss 那 就是 Logistic 
Regression 了 还有 等等 不同 的 loss 函数 具有 不同 
的 拟合 特性 这个 也得 就 具体 问题 具体 分析 
的 但 这里 我们 先不 究 loss 函数 的 问题 
我们 把 目光 转向 规则 项Ω/nr w 规则化 函数 Ω 
w 也 有 很多 种 选择 一般 是 模型 复杂度 
的 单调 递 增函数 模型 越 复杂 规则化 值 就 
越大 比如 规则化 项 可以 是 模型 参数 向量 的 
范数 然而 不同 的 选择 对 参数 w 的 约束 
不同 取得 的 效果 也 不同 但 我们 在 论文 
中 常见 的 都 聚集 在 零 范数 一 范数 
二 范数 迹 范数 Frobenius 范数 和核/nr 范数 等等 这么 
多 范数 到底 它们 表达 啥意思 具 有啥 能力 什么 
时候 才能 用 什么 时候 需要 用 呢 不急 不急 
下面 我们 挑 几个 常见 的 娓娓道来 一 L0 范数 
与 L1 范数 L0 范数 是 指向 量 中非 0 
的 元素 的 个数 如果 我们 用 L0 范 数来 
规则化 一个 参数 矩阵 W 的话 就是 希望 W 的 
大部分 元素 都是 0 这太 直观 了 太 露骨 了吧 
换句话说 让 参数 W 是 稀疏 的 OK 看到 了 
稀疏 二字 大家/n 都/d 应该/v 从/p 当下/t 风风火火/l 的/uj 压缩感知 
和 稀疏 编码 中 醒悟 过来 原 来用 的 漫山遍野 
的 稀疏 就是 通过 这 玩意 来 实现 的 但 
你 又 开始 怀疑 了 是 这样 吗 看到 的 
papers 世界 中 稀疏 不是 都 通过 L1 范数 来 
实现 吗 脑海 里 是不是 到处 都是 | | W 
| | 1 影子 呀 几乎 是 抬头 不见 低头 
见 没错 这 就是 这 节 的 题目 把 L0 
和 L1 放在 一起 的 原因 因为 他们 有着 某种 
不 寻常 的 关系 那 我们 再 来 看看 L1 
范数 是 什么 它 为什么 可以 实现 稀疏 为什么 大家 
都用/nr L1 范数 去 实现 稀疏 而 不是 L0 范数 
呢 L1 范数 是 指向 量 中 各个 元素 绝对值 
之和 也 有个 美称 叫 稀疏 规则 算子 Lasso regularization 
现在 我们 来 分析 下 这个 价值 一个亿 的 问题 
为什么 L1 范数 会使 权值 稀疏 有人 可能 会 这样 
给 你 回答 它 是 L0 范数 的 最优 凸 
近似 实际上 还 存在 一个 更 美的 回答 任何 的 
规则化 算子 如果 他 在 Wi = 0 的 地方 
不 可微 并且 可以 分解 为 一个 求和 的 形式 
那么 这个 规则化 算子 就 可以 实现 稀疏 这 说是 
这么 说 W 的 L1 范数 是 绝对值 | w 
| 在 w = 0处 是 不可 微 但这 还是 
不够 直观 这里 因为 我们 需要 和 L2 范数 进行 
对比 分析 所以 关于 L1 范数 的 直观 理解 请 
待会 看看 第二节 对了 上面 还有 一个 问题 既然 L0 
可以 实现 稀疏 为什么 不用 L0 而 要用 L1 呢 
个人 理解 一 是因为 L0 范数 很难 优化 求解 NP 
难 问题 二 是 L1 范数 是 L0 范数 的 
最优 凸 近似 而且 它 比 L0 范数 要 容易 
优化 求解 所以/c 大家/n 才/d 把/p 目光/n 和/c 万千/m 宠爱/v 
转/v 于/p L1/i 范数/n OK 来个 一句话 总结 L1 范数 
和 L0 范数 可以 实现 稀疏 L1 因 具有 比 
L0 更好 的 优化 求解 特性 而被 广泛应用 好 到 
这里 我们 大概 知道 了 L1 可以 实现 稀疏 但 
我们 会 想 呀 为什么 要 稀疏 让 我们 的 
参数 稀疏 有 什么 好处 呢 这里 扯 两点 1 
特征选择 Feature Selection 大家 对 稀疏 规则化 趋之若鹜 的 一个 
关键 原因 在于 它 能 实现 特征 的 自动 选择 
一般来说 xi 的 大部分 元素 也 就是 特征 都是 和 
最终 的 输出 yi 没有 关系 或者 不 提供 任何 
信息 的 在 最小化 目标函数 的 时候 考虑 xi 这些 
额外 的 特征 虽然 可以 获得 更小 的 训练 误差 
但在 预测 新的 样本 时 这些 没用 的 信息 反而会 
被 考虑 从而 干扰 了 对 正确 yi 的 预测 
稀疏 规则化 算子 的 引入 就是 为了 完成 特征 自动 
选择 的 光荣 使命 它 会 学习 地 去掉 这些 
没有 信息 的 特征 也 就是 把 这些 特征 对应 
的 权重 置 为 0 2 可 解释性 Interpretability 另一个 
青睐 于 稀疏 的 理由 是 模型 更容易 解释 例如 
患 某 种病 的 概率 是 y 然后 我们 收集到 
的 数据 x 是 1000 维 的 也 就是 我们 
需要 寻找 这 1000种 因素 到底 是 怎么 影响 患上 
这 种病 的 概率 的 假设 我们 这个 是个 回归模型 
y = w1 * x1 + w2 * x2 + 
+ w1000 * x1000 + b 当然 了 为了 让 
y 限定 在 0 1 的 范围 一般 还得 加个 
Logistic 函数 通过学习 如果 最后 学习 到 的 w * 
就 只有 很少 的 非零 元素 例如 只有 5个 非零 
的 wi 那么 我们 就 有 理由 相信 这些 对应 
的 特征 在 患病 分析 上面 提供 的 信息 是 
巨大 的 决策性 的 也 就是说 患 不患 这种 病 
只和 这 5个 因素 有关 那 医生 就好 分析 多了 
但 如果 1000个 wi 都非0/nr 医生 面对 这 1000种 因素 
累 觉 不爱 二 L2 范数 除了 L1 范数 还有 
一种 更 受 宠幸 的 规则化 范数 是 L2 范数 
| | W | | 2 它 也 不逊于 L1 
范数 它 有 两个 美称 在 回归 里面 有人 把 
有它的/nr 回归 叫 岭回归 Ridge Regression 有人 也叫 它 权值 
衰减 weight decay 这 用 的 很多 吧 因为 它 
的 强大 功效 是 改善 机器学习 里面 一个 非常 重要 
的 问题 过拟合 至于 过拟合 是 什么 上面 也 解释 
了 就是 模型 训练 时候 的 误差 很小 但在 测试 
的 时候 误差 很大 也 就是 我们 的 模型 复杂 
到 可以 拟合 到 我们 的 所有 训练样本 了 但在 
实际 预测 新的 样本 的 时候 糟糕 的 一塌糊涂 通俗 
的 讲 就是 应试 能力 很强 实际 应用 能力 很差 
擅长 背诵 知识 却不 懂得 灵活 利用 知识 例如 下图 
所示 来自 Ng 的 course 上面 的 图 是 线性 
回归 下面 的 图 是 Logistic 回归 也 可以 说 
是 分类 的 情况 从左到右 分别 是 欠 拟合 underfitting 
也称 High bias 合适 的 拟合 和 过拟合 overfitting 也称 
High variance 三种 情况 可以 看到 如果 模型 复杂 可以 
拟合 任意 的 复杂 函数 它 可以 让 我们 的 
模型 拟合 所有 的 数据 点 也 就是 基本上 没有 
误差 对于 回归 来说 就是 我们 的 函数 曲线 通过 
了 所有 的 数据 点 如上 图右 对 分类 来说 
就是 我们 的 函数 曲线 要把 所有 的 数据 点 
都 分类 正确 如下 图右 这 两种 情况 很 明显 
过拟合 了 OK 那 现在 到 我们 非常 关键 的 
问题 了 为什么 L2 范数 可以 防止 过拟合 回答 这个 
问题 之前 我们 得 先 看看 L2 范数 是个 什么 
东西 L2 范数 是 指向 量 各 元素 的 平方和 
然后 求 平方根 我们 让 L2 范数 的 规则 项||/nr 
W | | 2 最小 可以 使得 W 的 每个 
元素 都 很小 都/d 接近/v 于0/nr 但 与 L1 范数 
不同 它 不会 让 它 等于 0 而是 接近于 0 
这里 是 有 很大 的 区别 的 哦 而 越小 
的 参数 说明 模型 越 简单 越/d 简单/a 的/uj 模型/n 
则/d 越不/nr 容易/a 产生/n 过拟合/i 现象/n 为什么 越小 的 参数 
说明 模型 越 简单 我 也 不懂 我 的 理解 
是 限制 了 参数 很小 实际上 就 限制 了 多项式 
某些 分量 的 影响 很小 看 上面 线性 回归 的 
模型 的 那个 拟合 的 图 这样 就 相当于 减少 
参数 个数 其实 我 也 不太 懂 希望 大家 可以 
指点 下 这里 也 一句话 总结 下 通过 L2 范数 
我们 可以 实现 了 对模型 空间 的 限制 从而 在 
一定 程度 上 避免 了 过拟合 L2 范数 的 好处 
是 什么 呢 这里 也 扯上 两点 1 学习 理论 
的 角度 从 学习 理论 的 角度 来说 L2 范数 
可以 防止 过拟合 提升 模型 的 泛化 能力 2 优化 
计算 的 角度 从 优化 或者 数值 计算 的 角度 
来说 L2 范数 有助于 处理 condition number 不好 的 情况 
下 矩阵 求 逆 很 困难 的 问题 哎 等等 
这 condition number 是 啥 我 先 google 一下 哈 
这里 我们 也 故作 高雅 的 来 聊聊 优化 问题 
优化 有两 大 难题 一是 局部 最小值 二 是 ill 
condition 病态 问题 前者 俺 就 不说 了 大家 都懂吧/nr 
我们 要 找 的 是 全局 最小值 如果 局部 最小值 
太多 那 我们 的 优化 算法 就 很容易 陷入 局部 
最小 而 不能自拔 这 很 明显 不是 观众 愿意 看到 
的 剧情 那 下面 我们 来 聊聊 ill condition ill 
condition 对应 的 是 well condition 那 他们 分别 代表 
什么 假设 我们 有个 方程组 AX = b 我们 需要 
求解 X 如果 A 或者 b 稍微 的 改变 会 
使得 X 的 解 发生 很大 的 改变 那么 这个 
方程组 系统 就是 ill condition 的 反之 就是 well condition 
的 我们 具体 举个 例子 吧 咱们 先看 左边 的 
那个 第一行 假设 是 我们 的 AX = b 第二行 
我们 稍微 改变 下 b 得到 的 x 和没/nr 改变 
前 的 差别 很大 看到 吧 第三行 我们 稍微 改变 
下 系数 矩阵 A 可以 看到 结果 的 变化 也 
很大 换句话 来说 这个 系统 的 解对 系数 矩阵 A 
或者 b 太 敏感 了 又 因为 一般 我们 的 
系数 矩阵 A 和b/nr 是从 实验 数据 里面 估计 得到 
的 所以 它 是 存在 误差 的 如果 我们 的 
系统 对 这个 误差 是 可以 容忍 的 就 还好 
但 系统 对 这个 误差 太 敏感 了 以至于 我们 
的 解的/nr 误差 更大 那/r 这个/r 解就/nr 太不/i 靠谱/i 了/ul 
所以 这个 方程组 系统 就是 ill conditioned 病态 的 不 
正常 的 不 稳定 的 有 问题 的 哈哈 这 
清楚 了吧 右边 那个 就叫 well condition 的 系统 了 
还是 再 啰嗦 一下吧 对于 一个 ill condition 的 系统 
我 的 输入 稍微 改变 下 输出 就 发生 很大 
的 改变 这不 好啊 这 表明 我们 的 系统 不能 
实用 啊 你 想想 看 例如 对于 一个 回归 问题 
y = f x 我们 是 用 训练样本 x 去 
训练 模型 f 使得 y 尽量 输出 我们 期待 的 
值 例如 0 那 假如 我们 遇到 一个 样本 x 
这个 样本 和 训练样本 x 差别 很小 面对 他 系统 
本 应该 输出 和 上面 的 y 差不多 的 值 
的 例如 0.00001 最后 却给 我 输出 了 一个 0.9999 
这 很 明显 不 对呀 就好像 你 很 熟悉 的 
一个 人 脸上 长了 个 青春痘 你 就不 认识 他 
了 那你/nr 大脑 就 太 差劲 了 哈哈 所以 如果 
一个 系统 是 ill conditioned 病态 的 我们 就 会对 
它 的 结果 产生 怀疑 那 到底 要 相信 它 
多少 呢 我们 得 找个 标准 来 衡量 吧 因为 
有些 系统 的 病 没 那么 重 它 的 结果 
还是 可以 相信 的 不能 一刀切 吧 终于 回来 了 
上面 的 condition number 就是 拿 来 衡量 ill condition 
系统 的 可信度 的 condition number 衡量 的 是 输入 
发生 微小 变化 的 时候 输出 会 发生 多大 的 
变化 也 就是 系统对 微小 变化 的 敏感度 condition number 
值 小 的 就是 well conditioned 的 大 的 就是 
ill conditioned 的 如果 方阵 A 是非 奇异 的 那么 
A 的 conditionnumber 定义 为 也 就是 矩阵 A 的 
norm 乘以 它 的 逆 的 norm 所以 具体 的 
值 是 多少 就要 看 你 选择 的 norm 是 
什么 了 如果 方阵 A 是 奇异 的 那么 A 
的 condition number 就是 正无穷 大了 实际上 每一个 可逆 方阵 
都 存在 一个 condition number 但 如果 要 计算 它 
我们 需要 先 知道 这 个 方阵 的 norm 范数 
和 Machine Epsilon 机器 的 精度 为什么 要 范数 范数 
就 相当于 衡量 一个 矩阵 的 大小 我们 知道 矩阵 
是 没有 大小 的 当 上面 不是 要 衡量 一个 
矩阵 A 或者 向量 b 变化 的 时候 我们 的 
解x/nr 变化 的 大小 吗 所以/c 肯定/v 得要/i 有/v 一个/m 
东西/ns 来/v 度量/n 矩阵/n 和/c 向量/n 的/uj 大小/b 吧/y 对了 
他 就是 范数 表示 矩阵 大 小 或者 向量 长度 
OK 经过 比较 简单 的 证明 对于 AX = b 
我们 可以 得到 以下 的 结论 也/d 就是/d 我们/r 的/uj 
解x的/nr 相对/d 变化/vn 和A/nr 或者/c b/w 的/uj 相对/d 变化/vn 是/v 
有像/nr 上面/f 那样/r 的/uj 关系/n 的/uj 其中 k A 的 
值 就 相当于 倍率 看到 了 吗 相当于 x 变化 
的 界 对 condition number 来个 一句话 总结 conditionnumber 是 
一个 矩阵 或者 它 所 描述 的 线性系统 的 稳定性 
或者 敏感度 的 度量 如果 一个 矩阵 的 condition number 
在 1 附近 那么 它 就是 well conditioned 的 如果 
远大于 1 那么 它 就是 ill conditioned 的 如果 一个 
系统 是 ill conditioned 的 它 的 输出 结果 就 
不要 太 相信 了 好了 对 这么 一个 东西 已经 
说 了 好多 了 对了 我们 为什么 聊到 这个 的 
了 回到 第一 句话 从 优化 或者 数值 计算 的 
角度 来说 L2 范数 有助于 处理 condition number 不好 的 
情况 下 矩阵 求 逆 很 困难 的 问题 因为 
目标函数 如果 是 二次 的 对于 线性 回归 来说 那/r 
实际上/d 是/v 有/v 解析/vn 解的/nr 求导 并 令 导数 等于零 
即可 得到 最优 解为/nr 然而 如果 当 我们 的 样本 
X 的 数目 比 每个 样本 的 维度 还要 小 
的 时候 矩阵 XTX 将会 不是 满秩的/nr 也 就是 XTX 
会 变得 不可逆 所以 w * 就 没办法 直接 计算 
出来 了 或者 更 确切 地 说 将会 有 无穷 
多个 解 因为 我们 方程组 的 个数 小于 未知数 的 
个数 也 就是说 我们 的 数据 不 足以 确定 一个 
解 如果/c 我们/r 从/p 所有/b 可行/v 解里/nr 随机/d 选/zg 一个/m 
的话/u 很 可能 并 不是 真正 好 的 解 总而言之 
我们 过拟合 了 但 如果 加上 L2 规则 项 就 
变成 了 下面 这种 情况 就 可以 直接 求 逆 
了 这 里面 专业点 的 描述 是 要 得到 这个 
解 我们 通常 并不 直接 求 矩阵 的 逆 而是 
通过 解 线性方程组 的 方式 例如 高斯消 元法 来 计算 
考虑 没有 规则 项的/nr 时候 也 就是 λ = 0 
的 情况 如果 矩阵 XTX 的 condition number 很大 的 
话 解 线性方程组 就 会在 数值 上 相当 不 稳定 
而 这个 规则 项的/nr 引入 则 可以 改善 condition number 
另外 如果 使用 迭代 优化 的 算法 condition number 太大 
仍然 会 导致 问题 它 会 拖慢 迭代 的 收敛 
速度 而/c 规则/n 项从/nr 优化/vn 的/uj 角度/n 来看/u 实际上 是 
将 目标函数 变成 λ strongly convex λ 强凸/nr 的 了 
哎哟哟 这里 又 出现 个 λ 强凸/nr 啥 叫 λ 
强凸呢/nr 当 f 满足 时 我们 称 f 为 λ 
stronglyconvex 函数 其中 参数 λ 0 当 λ = 0时 
退回 到 普通 convex 函数 的 定义 在 直观 的 
说明 强凸/nr 之前 我们 先 看看 普通 的 凸 是 
怎样 的 假设 我们 让 f 在 x 的 地方 
做 一 阶 泰勒 近似 一 阶 泰勒 展开 忘 
了吗 f x = f a + f a x 
a + o | | x a | | . 
直观 来讲 convex 性质 是 指 函数 曲线 位于 该点 
处 的 切线 也 就是 线性 近似 之上 而 strongly 
convex 则 进一步 要求 位于 该处 的 一个 二次函数 上方 
也 就是说 要 求函数 不要 太 平坦 而是 可以 保证 
有 一定 的 向上 弯曲 的 趋势 专业点 说 就是 
convex 可以 保证 函数 在 任意 一点 都 处于 它 
的 一 阶 泰勒 函数 之上 而 strongly convex 可以 
保证 函数 在 任意 一点 都 存在 一个 非常 漂亮 
的 二次 下界 quadratic lower bound 当然 这 是 一个 
很强 的 假设 但是 同时 也 是 非常 重要 的 
假设 可能 还 不好 理解 那 我们 画 个 图 
来 形象 的 理解 下 大家 一 看到 上面 这个 
图 就 全 明白 了吧 不用 我 啰嗦 了吧 还是 
啰嗦 一下吧 我们 取 我们 的 最优 解w*/nr 的 地方 
如果 我们 的 函数 f w 见 左图 也 就是 
红色 那个 函数 都会 位于 蓝色 虚线 的 那根 二次函数 
之上 这样 就算 wt 和w*/nr 离 的 比较 近 的 
时候 f wt 和f/nr w * 的 值 差别 还是 
挺大 的 也 就是 会 保证 在 我们 的 最优 
解w*/nr 附近 的 时候 还 存在 较大 的 梯度 值 
这样 我们 才 可以 在 比较 少 的 迭代 次数 
内 达到 w * 但 对于 右图 红色 的 函数 
f w 只 约束 在 一个 线性 的 蓝色 虚线 
之上 假设 是 如 右图 的 很 不幸 的 情况 
非常 平坦 那在/nr wt 还 离 我们 的 最优 点 
w * 很远 的 时候 我们 的 近似 梯度 f 
wt f w * / wt w * 就 已经 
非常 小了 在 wt 处 的 近似 梯度 ∂ f 
/ ∂ w 就 更 小了 这样 通过 梯度 下降 
wt + 1 = wt α * ∂ f / 
∂ w 我们 得到 的 结果 就是 w 的 变化 
非常 缓慢 像 蜗牛 一样 非常 缓慢 的 向 我们 
的 最优 点 w * 爬动 那在/nr 有限 的 迭代 
时间内 它 离 我们 的 最优 点 还是 很远 所以 
仅 仅靠 convex 性质 并 不能 保证 在 梯度 下降 
和 有限 的 迭代 次数 的 情况 下 得到 的 
点 w 会 是 一个 比 较好 的 全局 最 
小点 w * 的 近似 点 插 个 话 有 
地方 说 实际上 让 迭代 在 接近 最优 的 地方 
停止 也 是 一种 规则化 或者 提高 泛化 性能 的 
方法 正如 上面 分析 的 那样 如果 f w 在 
全局 最 小点 w * 周围 是 非常 平坦 的 
情况 的话 我们 有 可能 会 找到 一个 很远 的 
点 但 如果 我们 有 强凸/nr 的话 就能 对 情况 
做 一些 控制 我们 就 可以 得到 一个 更好 的 
近似解 至于 有多/nr 好嘛 这 里面 有 一个 bound 这个 
bound 的 好坏 也要 取决于 strongly convex 性质 中的 常数 
α 的 大小 看到 这里 不 知道 大家 学 聪明 
了 没有 如果 要 获得 strongly convex 怎么做 最 简单 
的 就是 往里面 加入 一项 α / 2 * | 
| w | | 2 呃 讲个 strongly convex 花了 
那么多 的 篇幅 实际上 在 梯度 下降 中 目标函数 收敛 
速率 的 上界 实际上 是 和 矩阵 XTX 的 condition 
number 有关 XTX 的 condition number 越小 上界 就 越小 
也 就是 收敛 速度 会 越快 这一个 优化 说 了 
那么 多 的 东西 还是 来个 一句话 总结 吧 L2 
范数 不但 可以 防止 过拟合 还 可以 让 我们 的 
优化 求解 变得 稳定 和 快速 好了 这里 兑现 上面 
的 承诺 来 直观 的 聊聊 L1 和 L2 的 
差别 为什么 一个 让 绝对值 最小 一个 让 平方 最小 
会 有 那么 大 的 差别 呢 我 看到 的 
有 两种 几何 上 直观 的 解析 1 下降 速度 
我们 知道 L1 和 L2 都是 规则化 的 方式 我们 
将 权 值参 数以 L1 或者 L2 的 方式 放到 
代价 函数 里面 去 然后 模型 就 会 尝试 去 
最小化 这些 权值 参数 而 这个 最小化 就像 一个 下坡 
的 过程 L1 和 L2 的 差别 就 在于 这个 
坡 不同 如 下图 L1 就是 按 绝对值 函数 的 
坡 下降 的 而 L2 是 按 二次函数 的 坡 
下降 所以 实际上 在 0 附近 L1 的 下降 速度 
比 L2 的 下降 速度 要快 所以 会 非常 快得 
降到 0 不过 我 觉得 这里 解释 的 不太 中肯 
当然 了 也 不 知道 是不是 自己 理解 的 问题 
L1 在 江湖上 人称 Lasso L2 人称 Ridge 不过 这 
两个 名字 还 挺 让人 迷糊 的 看 上面 的 
图片 Lasso 的 图 看起来 就像 ridge 而 ridge 的 
图 看起来 就像 lasso 2 模型 空间 的 限制 实际上 
对于 L1 和 L2 规则化 的 代价 函数 来说 我们 
可以 写成 以下 形式 也 就是说 我们 将 模型 空间 
限制 在 w 的 一个 L1 ball 中 为了 便于 
可视化 我们 考虑 两维 的 情况 在 w1 w2 平面 
上 可以 画出 目标函数 的 等高线 而 约束条件 则 成为 
平面 上 半径 为 C 的 一个 norm ball 等高线 
与 norm ball 首次 相交 的 地方 就是 最优 解 
可以 看到 L1 ball 与 L2 ball 的 不同 就 
在于 L1 在 和 每个 坐标轴 相交 的 地方 都有 
角 出现 而 目标 函数 的 测地线 除非 位置 摆得 
非常好 大部分 时候 都会 在 角 的 地方 相交 注意到 
在 角 的 位置 就 会 产生 稀疏 性 例 
如图 中的 相 交点 就有 w1 = 0 而 更 
高维 的 时候 想象 一下 三维 的 L1 ball 是 
什么样 的 除了 角 点 以外 还有 很 多边 的 
轮廓 也 是 既有 很大 的 概率 成为 第一 次 
相交 的 地方 又 会 产生 稀疏 性 相比之下 L2 
ball 就 没有 这样 的 性质 因为 没 有角 所以 
第一 次 相交 的 地方 出现 在 具有 稀疏 性 
的 位置 的 概率 就 变得 非常 小了 这 就从 
直观 上来 解释 了 为什么 L1 regularization 能 产生 稀疏 
性 而 L2 regularization 不行 的 原因 了 因此 一句话 
总结 就是 L1 会 趋向 于 产生 少量 的 特征 
而 其他 的 特征 都是 0 而 L2 会 选择 
更多 的 特征 这些 特征 都会 接近于 0 Lasso 在 
特征选择 时候 非常 有用 而 Ridge 就 只是 一种 规则化 
而已 OK 就 聊到 这里 下一/i 篇/q 博文/nr 我们/r 聊聊/v 
核/n 范数/n 和/c 规则化/n 项/n 参数/n 选择/v 的/uj 问题/n 全篇 
的 参考 资料 也 请见 下 一篇 博文 这里 不 
重复 列出 谢谢 