一 . 简介 首先 来看 百度 百科 对 最小二乘 法的/nr 
介绍 最小二乘 法 又称 最 小平 方法 是 一种 数学 
优化 技术 它 通过 最小化 误差 的 平方 和 寻找 
数据 的 最佳 函数 匹配 利用 最小二乘 法 可以 简便 
地 求得 未知 的 数据 并 使得 这些 求得 的 
数据 与 实际 数据 之间 误差 的 平方和 为 最小 
最小二乘 法 还可 用于 曲线拟合 其他 一些 优化 问题 也 
可通过 最小化 能量 或 最大化 熵 用 最小二乘 法来/nr 表达 
简而言之 最小二乘 法同/nr 梯度 下降 类似 都是/nr 一种 求解 无约束 
最优化 问题 的 常用 方法 并且 也 可以 用于 曲线拟合 
来 解决 回归 问题 最小二乘 法 实质 就是 最小化 均方 
误差 而 均方 误差 就是 残差 平方和 的 1 / 
m m 为 样本 数 同时 均方 误差 也是 回归 
任务 中 最 常用 的 性能 度量 二 . 对于 
一元 线性 模型 如果 以 最 简单 的 一元 线性 
模型 来 解释 最小二乘 法 回归分析 中 如果 只 包括 
一个 自 变量 和 一个 因变量 且 二者 的 关系 
可 用 一条 直线 近似 表示 这种 回归分析 称为 一元 
线性 回归分析 如果 回归分析 中 包括 两个 或 两个 以上 
的 自变量 且 因变量 和 自变量 之间 是 线性关系 则 
称为 多元 线性 回归分析 对于 二 维空间 线性 是 一条 
直线 对于 三维空间 线性 是 一个 平面 对于 多维空间 线性 
是 一个 超平面 . . . 对于 一元 线性 回归模型 
假设 从 总体 中 获取 了 m 组 观察 值 
X1 Y1 X2 Y2 Xm Ym 对于 平面 中的 这 
m 个 点 可以 使用 无数条 曲线 来 拟合 要求 
样本 回归 函数 尽可能 好 地 拟合 这组 值 综合 
起来 看 这条 直线 处于 样本数据 的 中心 位置 最 
合理 选择 最佳 拟合 曲线 的 标准 可以 确定 为 
使 总的 拟合 误差 即 总 残差 达到 最小 有 
以下 三 个 标准 可以 选择 1 用 残差 和 
最小 确 定直线 位置 是 一个 途径 但 可能 会 
出现 计算 残差 和 存在 相互 抵消 的 问题 2 
用 残差 绝对值 和 最小 确 定直线 位置 也 是 
一个 途径 但 绝对值 的 计算 比较 麻烦 3 最小二乘 
法的/nr 原则 是以 残差 平方和 最小 确 定直线 位置 用 
最小二乘 法 除了 计算 比较 方便 外 得到 的 估计量 
还 具有 优良 特性 这种 方法 对 异常值 非常 敏感 
最 常用 的 是 普通 最 小二 乘法 Ordinary   
Least Square OLS 所 选择 的 回归模型 应该 使 所有 
观察 值 的 残差 平方和 达到 最小 在 讲 最小二乘 
的 详情 之前 首先 明确 两点 1 . 我们 假设在 
测量 系统 中 不 存在 有 系统误差 只 存在 有纯/nr 
偶然误差 比如 体重计 或者 身高计 本身 有 问题 测量/vn 出来/v 
的/uj 数据/n 都/d 偏大/a 或者/c 都/d 偏小/i 这种 误差 是 
绝对 不 存在 的 或者说 这 不能 叫 误差 这叫 
错误 2 . 误差 是 符合 正态分布 的 因此 最后 
误差 的 均值 为 0 这 一点 很 重要 明确 
了 上面 两点 以后 重点 来了 为了 计算 β 0 
β 1 的 值 我们 采取 如下 规则 β 0 
β 1 应该 使 计算 出来 的 函数 曲线 与 
观察 值 的 差 的 平方和 最小 用 数学 公式 
描述 就是 其中 yie 表示 根据 y = β 0 
+ β 1x 估算 出来 的 值 yi 是 观察 
得到 的 真实 值 为什么 要 用 残差 的 平方和 
最小 用 差 的 绝对值 不行 么 以下 是 一个 
相对 靠谱 的 解释 我们 假设 直线 对于 坐标 Xi 
给出 的 预测 f Xi 是 最 靠谱 的 预测 
所有 纵坐标 偏离 f Xi 的 那些 数 据点 都 
含有 噪音 是 噪音 使得 它们 偏离 了 完美 的 
一条 直线 一个 合理 的 假设 就是 偏离 路线 越远 
的 概率 越小 具体 小 多少 可以 用 一个 正态分布 
曲线 来 模拟 这个 分布 曲线 以 直线 对 Xi 
给出 的 预测 f Xi 为 中心 实际 纵坐标 为 
Yi 的 点 Xi Yi 发生 的 概率 就 正比 
于 EXP Δ Yi ^ 2 EXP . . 代表 
以 常数 e 为 底 的 多少 次方 所以 我们 
在 前面 的 两点 里 提到 假设 误差 的 分布 
要为 一个 正态分布 原因 就 在 这里 了 另外 说 
一点 我 自己 的 理解 从 数学 处理 的 角度 
来说 绝对值 的 数学 处理过程 比 平方和 的 处理 要 
复杂 很多 搞过 机器 学习 的 同学 都 知道 L1 
正则 就是 绝对值 的 方式 而 L2 正 则是 平方和 
的 形式 L1 能 产生 稀疏 的 特征 这对 大 
规模 的 机器学习 灰常 灰常 重要 但是 L1 的 求解 
过程 实在 是 太过 蛋疼 所以 即使 L1 能 产生 
稀疏 特征 不到 万不得已 我们 也 还是 宁 可用 L2 
正则 因为 L2 正则 计算起来 方便 得多 明确 了 前面 
的 cost function 以后 后面 的 优化 求解 过程 反倒 
变得 s 容易 了 样本 的 回归模型 很容易 得出 现在 
需要 确定 β 0 β 1 使 cost function 最小 
学 过高 数 的 同志们 都 清楚 求导 就 OK 
对于 这种 形式 的 函数 求导 根据 数学知识 我们 知道 
函数 的 极值 点 为 偏 导 为 0 的 
点 将 这 两个 方程 稍微 整理 一下 使用 克莱姆 
法则 很容易 求解 得出 这 就是 最 小二 乘法 的 
解法 就是 求得 平方 损失 函数 的 极值 点 需要 
注意 的 一点 是 β 0 是 常数项 对应 的 
系数 此处 相当于 添加 了 一个 特征值 x0 且 x0 
恒 为 1 也 就是 目标函数 中的 β 0 可以 
看成 β 0x0 这样 的话 就 不同 单独 考虑 常数项 
了 在 后面 的 多元 线性 模型 就用 到了 该 
性质 三 . 对于 多元 线性 模型 如果 我们 推广 
到 更 一般 的 情况 假如 有 更多 的 模型 
变量 x1 x2 ⋯ xn 可以 用 线性函数 表示 如下 
对于 m 个 样本 来说 可以 用 如下 线性方程组 表示 
如果 将 样本 矩阵 xij 记为 矩阵 A 将 参数 
矩阵 记为 向量 β 真 实值 记为 向量 Y 上述 
线性方程组 可以 表示 为 对于 最小二乘 来说 最终 的 矩阵 
表达 形式 可以 表示 为 其中 m ≥ n 由于 
考虑到 了 常数项 故 属性值 个数 由 n 变为 n 
+ 1 关于 这个 方程 的 解法 具体 如下 其中 
倒数 第二 行 中的 中间 两项 为 标量 所以 二者 
相等 然后 利用 该 式 对 向量 β 求导 1 
由 矩阵 的 求导 法则 可知 1 式 的 结果 
为 令 上式 结果 等于 0 可得 2 上式 就是 
最 小二 乘法 的 解析 解 它 是 一个 全局 
最优 解 四 . 其他 一些 想法 1 . 最小二乘/i 
法和/nr 梯度/n 下降/v 乍一看/l 看/v β/i 的/uj 最终/d 结果/n 感觉 
很 面熟 仔细 一看 这不 就是 NG 的 ML 课程 
中所 讲到 的 正规 方程 嘛 实际上 NG 所说 的 
的 正规 方程 的 解法 就是 最 小二 乘法 求 
解析 解的/nr 解法 1 最小二乘/i 法和/nr 梯度/n 下/f 降法/n 在/p 
线性/n 回归/v 问题/n 中/f 的/uj 目标/n 函数/n 是/v 一样/r 的/uj 
或者说 本质 相同 都是/nr 通过 最小化 均方 误差 来 构建 
拟合 曲线 2 二者 的 不同 点 可见 下图 正规 
方程 就是 最 小二 乘法 需要/v 注意/v 的/uj 一点/m 是/v 
最小二乘/i 法只/nr 适用/v 于/p 线性/n 模型/n 这里 一般指 线性 回归 
而 梯度 下降 适用性 极强 一般而言 只要 是 凸函数 都 
可以 通过 梯度 下 降法 得到 全局 最优 值 对于 
非 凸函数 能够 得到 局部 最优 解 梯度 下 降法 
只要 保证 目标函数 存在 一 阶 连续 偏 导 就 
可以 使用 2/m ./i 最小二乘/i 法的/nr 一些/m 限制/v 和/c 解决/v 
方法/n 我们 由 第三 部分 2 式 可 知道 要 
保证 最小二乘 法 有解 就得 保证 ATA 是 一个 可逆 
阵 非 奇异 矩阵 那 如果 ATA 不可逆 怎么办 什么 
情况 下 ATA 不可逆 关于 ATA 在 什么 情况下 不可逆 
1 当 样本 的 数量 小于 参数 向量 即 β 
的 维度 时 此时 ATA 一定 是 不可逆的 例如 你 
有 1000个 特征 但 你 的 样本 数目 小于 1000 
的话 那么 构 造出 的 ATA 就是 不可逆的 2 在 
所有 特征 中 若 存在 一个 特征 与 另一个 特征 
线性相关 或 一个 特征 与 若干 个 特征 线性相关 时 
此时 ATA 也是 不可逆的 为什么 呢 具体来说 假设 A 是 
m * n 维 的 矩阵 若 存在 线性 相关 
的 特征 则 R A n R AT n R 
ATA n 所以 ATA 不可逆 如果 ATA 不可逆 应该 怎样 
解决 1 筛选出 线性 无关 的 特征 不 保留 相同 
的 特征 保证 不 存在 线性 相关 的 特征 2 
增加 样本量 3 采用 正则化 的 方法 对于 正则化 的 
方法 常见/a 的/uj 是/v L1/i 正则/n 项和/nr L2/i 正则/n 项/n 
L 1项 有助于 从 很多 特征 中 筛选 出 重要 
的 特征 而 使得 不 重要 的 特征 为 0 
所以 L1 正则 项 是个 不错 的 特征选择 方法 如果 
采用 L2 正则 项 的话 实际上 解析 解就/nr 变成 了 
如下 的 形式 λ 即 正则参数 是 一种 超 参数 
后面 的 矩阵 为 n + 1 * n + 
1 维 如果 不 考虑 常数项 的话 就是 一个 单位阵 
此时 括号 中的 矩阵 一定是 可逆 的 3 . 最小二乘 
法的/nr 改进 最 小二 乘法 由于 是 最小化 均方差 所以 
它 考虑 了 每个 样本 的 贡献 也 就是 每个 
样本 具有 相同 的 权重 由于 它 采用 距离 作为 
度量 使得 他 对 噪声 比较 敏感 最小二乘 法 假设 
噪声 服从 高斯分布 即 使得 他 它 对 异常 点 
比较 敏感 因此 人们 提出 了 加权 最小二乘 法 相当于 
给 每个 样本 设置 了 一个 权重 以此 来 反应 
样本 的 重要 程度 或者 对 解的/nr 影响 程度 参考 
NG 机器学习 矩阵分析 与 应用 http / / www . 
cnblogs . com / iamccme / archive / 2013/05 / 
15/3080737 . htmlhttp / / blog . csdn . net 
/ bitcarmanlee / article / details / 51589143 