感谢 中国 人民 大学 的 胡鹤/nr 老师 课程 容量 巨大 
收获 颇丰 之前 提到 的 CNN 模型 主要 用 到 
人类 的 视觉 中枢 但 其 有一/nr 劣势 无论 是 
人类 的 视觉 神经 还是 听觉 神经 所 接受 到 
的 都是/nr 一个 连续 的 序列 使用 CNN 相当于 割裂 
了 前后 的 联系 从而 诞生 了 专门 为 处理 
序列 的 Recurrent Neural Network RNN 每一个 神经元 除了 当前 
信息 的 输入 外 还有 之前 产生 的 记忆 信息 
保留 序列 依赖型 一 RNN 基本原理 如下 图 所示 有 
两种 表示 方法 每张 图片 左边 是 RNN 的 神经元 
称为 memory cell 右边 是 按 时间轴 展开 后的/nr 情况 
每次 输入 两个 信息 输出 两个 信息 每 轮 处理 
hidden state 把 同样 神经元 在 时间 上 展开 处理 
比 CNN 更加 节省 参数 是 一个 相当 高效 的 
表示 方法 可 参考 如下 公式 表示 最后 简化 后的/nr 
形式 同 一般 神经元 相同 输入 信息 乘 权重 加 
偏 值 由于 t 状态 的 t 由 t 1 
时候 决定 因而 具有 记忆 功能 也 叫作 memory cell 
或 cell 隐 状态 可由 如下 表示 隐 状态 是 
当前 t 时刻 的 状态 也由 t 1 时刻 决定 
简单 情况 下 hidden state 等同于 output y 但 大多 
较为 复杂 的 cell 中 它们 并不相同 如下 图 所示 
二 RNN 种类 1 . sequence to sequence 输入输出 都是/nr 
一个 序列 例如 股票 预测 中的 RNN 输入 是 前 
N 天 价格 输出 明天 的 股市 价格 2 . 
sequence to vector 输入 是 一个 序列 输出 单一 向量 
例如 输入 一个 电影 评价 序列 输 出 一个 分数 
表示 情感 趋势 喜欢 还是 讨厌 3 . vector to 
sequence 输入 单一 向量 输 出 一个 序列 4 . 
Encoder Decoder 输入 sequence to vector 称作 encoder 输出 vector 
to sequence 称作 decoder 这 是 一个 delay 模型 经过 
一段 延迟 即把 所有 输入 都 读取 后 在 decoder 
中 获取 输入 并 输出 一个 序列 这个 模型 在 
机器 翻译 中 使用 较 广泛 源语言 输 在 入 
放入 encoder 浓缩 在 状态 信息 中 生成 目标语言 时 
可以 生成 一个 不 长度 的 目标 语言 序列 三 
RNN 实例 1 . 手动 实现 以下 是 一个 手动 
实现 RNN 的 实例 n _ inputs = 3 # 
hidden state n _ neurons = 5 X0 = tf 
. placeholder tf . float32 None n _ inputs X1 
= tf . placeholder tf . float32 None n _ 
inputs # 由于 Wx 要和 X 相乘 故 低维 是 
n _ inputs Wx = tf . Variable tf . 
random _ normal shape = n _ inputs n _ 
neurons dtype = tf . float32 # 低维 高维 都是 
n _ neurons 为了 使得 输出 也是 hidden state 的 
深度 # 这样 下一次 才 可以 继续 运算 Wy = 
tf . Variable tf . random _ normal shape = 
n _ neurons n _ neurons dtype = tf . 
float32 b = tf . Variable tf . zeros 1 
n _ neurons dtype = tf . float32 # Y0 
初始 化为 0 初始 时 没有 记忆 Y0 = tf 
. tanh tf . matmul X0 Wx + b # 
把 上 一轮 输出 Y0 也 作为 输入 Y1 = 
tf . tanh tf . matmul Y0 Wy + tf 
. matmul X1 Wx + b init = tf . 
global _ variables _ initializer import numpy as np X0 
_ batch = np . array 0 1 2 3 
4 5 6 7 8 9 0 1 # t 
= 0 X1 _ batch = np . array 9 
8 7 0 0 0 6 5 4 3 2 
1 # t = 1 with tf . Session as 
sess init . run Y0 _ val Y1 _ val 
= sess . run Y0 Y1 feed _ dict = 
{ X0 X0 _ batch X1 X1 _ batch } 
# Y0 Y1 都是 4 * 5 大小 4 是 
mini batch 数目 5 是 输出 神经元 个数 TensorFlow 函数 
集成 后 实现 2 . static unrolling through timestatic _ 
rnn 是 使用 链式 cells 实现 一个 按 时间轴 展开 
的 RNN # 这种 和 上面 那种 手动 实现 的 
效果 相同 n _ inputs = 3 n _ neurons 
= 5 X0 = tf . placeholder tf . float32 
None n _ inputs X1 = tf . placeholder tf 
. float32 None n _ inputs basic _ cell = 
tf . contrib . rnn . BasicRNNCell num _ units 
= n _ neurons output _ seqs states = tf 
. contrib . rnn . static _ rnn basic _ 
cell X0 X1 dtype = tf . float32 Y0 Y1 
= output _ seqs # run 部分 init = tf 
. global _ variables _ initializer X0 _ batch = 
np . array 0 1 2 3 4 5 6 
7 8 9 0 1 X1 _ batch = np 
. array 9 8 7 0 0 0 6 5 
4 3 2 1 with tf . Session as sess 
init . run Y0 _ val Y1 _ val = 
sess . run Y0 Y1 feed _ dict = { 
X0 X0 _ batch X1 X1 _ batch } packing 
sequencen _ steps = 2 n _ inputs = 3 
n _ neurons = 5 # 输入 是 一个 三维 
tensor none 是 mini batch 大小 不限 n _ steps 
是 序列 长度 X = tf . placeholder tf . 
float32 None n _ steps n _ inputs # 把/p 
一个/m 高/a 维度/ns n/w 的/uj tensor/w 展开/v 成/n 一个/m n/w 
1 维 降 维 这里 是 3位 降到 2 维 
列表 # unstack 之前 要 做一个 1 2 维 转置 
相当于 构造 了 n _ steps 个数 的 列表 X 
_ seqs = tf . unstack tf . transpose X 
perm = 1 0 2 basic _ cell = tf 
. contrib . rnn . BasicRNNCell num _ units = 
n _ neurons # states 是 最新 状态 output _ 
seqs states = tf . contrib . rnn . static 
_ rnn basic _ cell X _ seqs dtype = 
tf . float32 # 再 做 一个 转置 和 输入 
对应 outputs = tf . transpose tf . stack output 
_ seqs perm = 1 0 2 # 输入 大小 
4 * 2 * 3 X _ batch = np 
. array # t = 0 t = 1 0 
1 2 9 8 7 # instance 0 3 4 
5 0 0 0 # instance 1 6 7 8 
6 5 4 # instance 2 9 0 1 3 
2 1 # instance 3 with tf . Session as 
sess init . run outputs _ val = outputs . 
eval feed _ dict = { X X _ batch 
} # output _ val 是 一个 4 * 2 
* 5 仅 输出 维度 神经元 个数 改变 3 . 
dynamic RNN 本身 支持 高维 tensor 输入 内嵌 一个 循环 
运行 足够 多 次数 的 cell 不 需要 unstack 步骤 
这个 内嵌 循环 while _ loop 在前 向 传播 中 
将 每次 迭代 的 tensor 值 存储 下来 以 便于 
反向 传播 过程 中 使用 其 计算 梯度 值 X 
= tf . placeholder tf . float32 None n _ 
steps n _ inputs # 动态 RNN 内部 封装 一个 
循环 # 根据 输入 动态 决定 自己 需要 展开 几次 
basic _ cell = tf . contrib . rnn . 
BasicRNNCell num _ units = n _ neurons outputs states 
= tf . nn . dynamic _ rnn basic _ 
cell X dtype = tf . float32 dynamicRNN 可以 动态 
规定 输入 大小 就像 句子 输入 n _ steps = 
2 n _ inputs = 3 n _ neurons = 
5 X = tf . placeholder tf . float32 None 
n _ steps n _ inputs basic _ cell = 
tf . contrib . rnn . BasicRNNCell num _ units 
= n _ neurons seq _ length = tf . 
placeholder tf . int32 None outputs states = tf . 
nn . dynamic _ rnn basic _ cell X dtype 
= tf . float32 sequence _ length = seq _ 
length init = tf . global _ variables _ initializer 
# X _ batch 的 大小 4 * 2 * 
3 X _ batch = np . array # step 
0 step 1 0 1 2 9 8 7 # 
instance 1 3 4 5 0 0 0 # instance 
2 padded with zero vectors 6 7 8 6 5 
4 # instance 3 9 0 1 3 2 1 
# instance 4 # 这里 设置 sequence 大小 一共 4个 
batch 第 二维 上 只取 第一个 seq _ length _ 
batch = np . array 2 1 2 2 with 
tf . Session as sess init . run outputs _ 
val states _ val = sess . run outputs states 
feed _ dict = { X X _ batch seq 
_ length seq _ length _ batch } 如果 事先 
不 知道 输出 序列 的 长度 就 需要 定义 一个 
end of sequence token eos token 无论是 课上 还是 网上 
相关 信息 都 很少 这里 就不 展开 了 四 RNN 
训练 1 . 拟合 分类 RNN 比较 难以 训练 单是如/nr 
下图 的 节点 中 cost function 就 包含 y2 y3 
y4 三个 输出 往回 回溯 以下 MINIST 中 使用 150个 
RNN 神经元 最后 加 一个 全 连接 层 得到 10个 
神经元 的 输出 分别 对应 0 9 最后 看 对 
应在 from tensorflow . contrib . layers import fully _ 
connected n _ steps = 28 n _ inputs = 
28 n _ neurons = 150 n _ outputs = 
10 learning _ rate = 0.001 X = tf . 
placeholder tf . float32 None n _ steps n _ 
inputs # 一维 输出 y = tf . placeholder tf 
. int32 None # 使用 最 简单 的 basicRNNcell basic 
_ cell = tf . contrib . rnn . BasicRNNCell 
num _ units = n _ neurons # 使用 dynamic 
_ rnn outputs states = tf . nn . dynamic 
_ rnn basic _ cell X dtype = tf . 
float32 # 原始 输出 logits = fully _ connected states 
n _ outputs activation _ fn = None # 计算 
和 真实 的 交叉 熵 xentropy = tf . nn 
. sparse _ softmax _ cross _ entropy _ with 
_ logits labels = y logits = logits loss = 
tf . reduce _ mean xentropy # 使用 AdamOptimizer optimizer 
= tf . train . AdamOptimizer learning _ rate = 
learning _ rate training _ op = optimizer . minimize 
loss # 计算 准确率 只有 等于 y 才是 对 的 
其他 都错/nr correct = tf . nn . in _ 
top _ k logits y 1 accuracy = tf . 
reduce _ mean tf . cast correct tf . float32 
init = tf . global _ variables _ initializer from 
tensorflow . examples . tutorials . mnist import input _ 
data mnist = input _ data . read _ data 
_ sets / tmp / data / # 转换 到 
合理 的 输入 shape X _ test = mnist . 
test . images . reshape 1 n _ steps n 
_ inputs y _ test = mnist . test . 
labels # run100 遍 每次 处理 150个 输入 n _ 
epochs = 100 batch _ size = 150 # 开始 
循环 with tf . Session as sess init . run 
for epoch in range n _ epochs for iteration in 
range mnist . train . num _ examples / / 
batch _ size # 读入 数据 并 reshape X _ 
batch y _ batch = mnist . train . next 
_ batch batch _ size X _ batch = X 
_ batch . reshape 1 n _ steps n _ 
inputs # X 大写 y 小写 sess . run training 
_ op feed _ dict = { X X _ 
batch y y _ batch } acc _ train = 
accuracy . eval feed _ dict = { X X 
_ batch y y _ batch } acc _ test 
= accuracy . eval feed _ dict = { X 
X _ test y y _ test } # 每次 
打印 一下 当前 信息 print epoch Train accuracy acc _ 
train Test accuracy acc _ test 以下 只用了 150个 参数 
做了 单层 就 可以 达到 非常 高的/nr 效果 可以 看出 
rnn 效果 非常 不错 序列 预测 前 20个 状态 作为 
输入 则 第 2个 到 21个 作为 输出 作为 训练 
集 # 输入 x0 x19 n _ steps = 20 
# 只 预测 一个 值 n _ inputs = 1 
# rnn 有 100个 n _ neurons = 100 n 
_ outputs = 1 # none 表示 min _ batch 
大小 这里 任意 X = tf . placeholder tf . 
float32 None n _ steps n _ inputs y = 
tf . placeholder tf . float32 None n _ steps 
n _ outputs cell = tf . contrib . rnn 
. BasicRNNCell num _ units = n _ neurons activation 
= tf . nn . relu outputs states = tf 
. nn . dynamic _ rnn cell X dtype = 
tf . float32 如上 代码 中 每次 输出 的 vector 
都是 100 维 的 加入 一个 output rejections 后 使得 
每次 只 输出 1个 值 output rejection 实现 代码 如下 
# 设置 输出 为 上面 设定 的 n _ outputs 
大小 cell = tf . contrib . rnn . O 
u t p u t P r o j e 
c t i o n W r a p p 
e r tf . contrib . rnn . BasicRNNCell num 
_ units = n _ neurons activation = tf . 
nn . relu output _ size = n _ outputs 
learning _ rate = 0.001 loss = tf . reduce 
_ mean tf . square outputs y optimizer = tf 
. train . AdamOptimizer learning _ rate = learning _ 
rate training _ op = optimizer . minimize loss init 
= tf . global _ variables _ initializer # 开始 
训练 n _ iterations = 10000 batch _ size = 
50 with tf . Session as sess init . run 
for iteration in range n _ iterations X _ batch 
y _ batch = . . . # fetch the 
next training batch sess . run training _ op feed 
_ dict = { X X _ batch y y 
_ batch } if iteration % 100 = = 0 
mse = loss . eval feed _ dict = { 
X X _ batch y y _ batch } print 
iteration \ tMSE mse 2 . 预测 当 一个 RNN 
训 练好 后 它 就 可以 生成 很多 新的 东西 
RNN 的 强大 的 生成 能力 非常 有 魅力 用 
很多 曲子 去 训练 它 它 就 可以 生成 新的 
曲子 用 很多 文章 训练 它 他 就 可以 生成 
新的 文章 如果 可以 训练 出 功能 非常 强的/nr RNN 
模型 就 有可能 代替人 的 工作 with tf . Session 
as sess # 导入 训 练好 的 模型 saver . 
restore sess . / my _ time _ series _ 
model # 生成 新的 曲线 sequence = 0 . * 
n _ steps for iteration in range 300 X _ 
batch = np . array sequence n _ steps . 
reshape 1 n _ steps 1 y _ pred = 
sess . run outputs feed _ dict = { X 
X _ batch } sequence . append y _ pred 
0 1 0 RNN 也 可以 不断 叠加 形成 很深 
的 网络 如下 图 所示 每 一层 输出 都 反馈 
到 当前 位置 的 输入 时间轴 展开 后 如 右边 
所示 n _ inputs = 2 n _ steps = 
5 X = tf . placeholder tf . float32 None 
n _ steps n _ inputs n _ neurons = 
100 n _ layers = 3 # 做了 3层 rnn 
# 模型 不是 越 复杂 越好 越 复杂 所需 数据 
量 越大 否则 会有 过拟合 的 风险 # 可以 加 
dropout 来 控制 layers = tf . contrib . rnn 
. BasicRNNCell num _ units = n _ neurons for 
layer in range n _ layers multi _ layer _ 
cell = tf . contrib . rnn . MultiRNNCell layers 
outputs states = tf . nn . dynamic _ rnn 
multi _ layer _ cell X dtype = tf . 
float32 init = tf . global _ variables _ initializer 
X _ batch = np . random . rand 2 
n _ steps n _ inputs with tf . Session 
as sess init . run outputs _ val states _ 
val = sess . run outputs states feed _ dict 
= { X X _ batch } 五 困难 及 
优化 反向 训练 时 对于 RNN 来说 要 横向 往 
前推 一直往前 推到 序列 开始 的 地方 当 序列 非常 
长时 梯度 消失 梯度 爆炸 都与 路径 长度 太长 有关 
前面 的 权重 都 基本 固定 不变 没有 训练 效果 
为了 解决 这个 困难 有了/nr 很多 更 复杂 RNN 模型 
的 提出 1 . LSTM Long Short Term Memory 97年 
提出 直到 深度 学习 提出 使用 LSTM 做出 具体 实 
事后 才 火 起来 或许 是 因为 现在 有大/nr 数据 
的 环境 以及 训练 能力 很强 的 硬件 这些 客观 
条件 得 具备 才能 真正 发挥 LSTM 的 威力 它 
把 训练 信息 分为 长期 记忆 c 和 短期 记忆 
h 上面 的 长期 记忆 信息 可以 穿到 很远 即使 
序列 长到 1000 也 可以 向前 传导 它 分 了 
很多 个 门 gate 输出 信息 趋 近于 0 门 
关闭 趋 近于 1 门 打开 i 是 输 入门 
控制 新 输入 加多少 到 长期 记忆 中 f 是 
forget 控制 是否 受 长期 记忆 的 影响 哪些 长期 
记忆 被 忘掉 o 是 输出 门 控制 哪些 长期 
记忆 可以 输出 并 作为 短期 记忆 ht 传递 下去 
通过 这 3个 门 控制 信息 的 流动 可以 保证 
长期 记忆 变换 的 缓慢 相对 稳定 可以 对 距离 
比 较远 的 序列 影响 ht 和 ht 1 可以 
看到 距离 也 比较 远 短期 记忆 ht 1 变化 
明显 # TensorFlow 中 LSTM 具体 实现 n _ steps 
= 28 n _ inputs = 28 n _ neurons 
= 150 n _ outputs = 10 n _ layers 
= 3 learning _ rate = 0.001 X = tf 
. placeholder tf . float32 None n _ steps n 
_ inputs y = tf . placeholder tf . int32 
None lstm _ cells = tf . contrib . rnn 
. BasicLSTMCell num _ units = n _ neurons for 
layer in range n _ layers multi _ cell = 
tf . contrib . rnn . MultiRNNCell lstm _ cells 
outputs states = tf . nn . dynamic _ rnn 
multi _ cell X dtype = tf . float32 top 
_ layer _ h _ state = states 1 1 
logits = tf . layers . dense top _ layer 
_ h _ state n _ outputs name = softmax 
xentropy = tf . nn . sparse _ softmax _ 
cross _ entropy _ with _ logits labels = y 
logits = logits loss = tf . reduce _ mean 
xentropy name = loss optimizer = tf . train . 
AdamOptimizer learning _ rate = learning _ rate training _ 
op = optimizer . minimize loss correct = tf . 
nn . in _ top _ k logits y 1 
accuracy = tf . reduce _ mean tf . cast 
correct tf . float32 init = tf . global _ 
variables _ initializer LSTM 还有 一点 改进 Peephole Connectionlstm _ 
cell = tf . contrib . rnn . LSTMCell num 
_ units = n _ neurons use _ peepholes = 
True 2 . GRU Gated recurrent unit GRU 是 对 
LSTM 简化 后的/nr 版本 去掉 了 长短期 记忆 的 区分 
都是 h 减少 了 几个 门 2014年 提出 从 参数 
上 来说 较 LSTM 简单 些 统一 用 update gate 
控制 原来 的 i 门 和f门/nr z 趋 近于 0 
就用 ht 1 来 更新 趋 近于 1 就 取 
当前 输入 比 LSTM 还少 一个 矩阵 乘法 实际 表现 
不比 LSTM 差 也 成为 现在 很多 研究 者 越来越 
看重 的 方法 调 用时 直接 调用 GRU cell 即可 
gru _ cell = tf . contrib . rnn . 
GRUCell num _ units = n _ neurons 六 RNN 
在 NLP natural language processing 中 的 应用 RNN 的 
输入 原本 是 one hot 的 表示 但 这样 会 
使得 输入 极其 稀疏 不好 训练 于是 将 高维空间 映 
射到 低维 如 100 维 空间 用 这个 低维 嵌入 
的 输入 做 训练 非常 有效 Word Embeddings 相同 含义 
的 词 在 低维 空间 中 距离 近 含义 差 
的 多 的 离得 远 # 把 50000 维 数据 
映 射到 150 维 数据 空间 上 vocabulary _ size 
= 50000 embedding _ size = 150 # 做 一个 
全 连接 embeddings = tf . Variable tf . random 
_ uniform vocabulary _ size embedding _ size 1.0 1.0 
train _ inputs = tf . placeholder tf . int32 
shape = None # from ids embed = tf . 
nn . embedding _ lookup embeddings train _ inputs # 
to embd 例如 下图 所示 把 要 翻译 的 英文 
句子 做 输入 用 训练 后的/nr 状态值 做 输入 和 
法语 作为 训练 集 的 作为 decoder 输入 第一位 放 
一个 起始信号 go 输出 和 输入 刚好 错 一位 最后 
一位 以 一个 结束 标识 eos 结束 这样 做 是 
为了 后继 应用 时 翻译 新 句子 没有 training label 
只有 英文 输入 把 英文 输入 放进来 后加 一个 go 
得到 第一个 je 输出 把 第一 个 词 放进 来 
得到 第二 个 输出 bois 最后 eos 翻译 结束 最后 
实际 应用 时 如下 输入 go 开始 翻译 