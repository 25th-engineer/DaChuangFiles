目录 信息熵 条件 熵 相对 熵 交叉 熵 总结 1 
信息熵 information entropy 熵 entropy 这一 词 最初 来源于 热力学 
1948年 克劳德 爱尔 伍德 香农 将 热力学 中的 熵 引入 
信息论 所以 也 被 称为 香农 熵 Shannon entropy 信息熵 
information entropy 本文 只 讨论 信息熵 首先 我们 先 来 
理解 一下 信息 这个 概念 信息 是 一个 很 抽象 
的 概念 百度 百科 将 它 定义 为 指 音讯 
消息 通讯 系统 传输 和 处理 的 对象 泛指 人类 
社会 传播 的 一切 内容 那 信息 可以 被 量化 
么 可以 的 香农 提出 的 信息熵 概念 解决 了 
这 一 问题 一条 信息 的 信息量 大 小和 它 
的 不确定性 有 直接 的 关系 我们 需要 搞 清楚 
一件 非常 非常 不 确定 的 事 或者 是 我们 
一无所知 的 事 就 需要 了解 大量 的 信息 相反 
如果 我们 对 某件 事 已经 有了/nr 较多 的 了解 
我们 就 不需要 太多 的 信息 就 能把 它 搞清楚 
所以 从 这个 角度 我们 可以 认为 信息量 的 度量 
就 等于 不确定性 的 多少 比如 有人 说 广东 下雪 
了 对于 这 句话 我们 是 十分 不 确定 的 
因为 广东 几十年 来 下雪 的 次数 寥寥无几 为了 搞 
清楚 我们 就要 去 看 天气 预报 新闻 询问 在 
广东 的 朋友 而这 就 需要 大量 的 信息 信息熵 
很高 再 比如 中国男足 进军 2022年 卡塔尔 世界杯 决赛圈 对于 
这 句话 因为 确定性 很高 几乎 不 需要 引入 信息 
信息熵 很低 考虑 一个 离散 的 随机变量 $ x $ 
由 上面 两个 例子 可知 信息 的 量度 应该 依赖于 
概率分布 $ p x $ 因此 我们 想 要 寻找 
一个 函数 $ I x $ 它 是 概率 $ 
p x $ 的 单调 函数 表达 了 信息 的 
内容 怎么 寻找 呢 如果 我们 有 两个 不 相关 
的 事件 $ x $ 和 $ y $ 那么/r 
观察/v 两个/m 事件/n 同时/c 发生/v 时/n 获得/v 的/uj 信息/n 量/n 
应该/v 等于/v 观察/v 到/v 事件/n 各自/r 发生/v 时/n 获得/v 的/uj 
信息/n 之和/n 即 $ I x y = I x 
+ I y $ 因为 两个 事件 是 独立 不相关的 
因此 $ p x y = p x p y 
$ 根据 这 两个 关系 很容易 看出   $ I 
x $ 一定 与 $ p x $ 的 对数 
有关   因为 对数 的 运算 法则 是 $ log 
_ a mn = log _ am + log _ 
an $ 因此 我们 有$I/nr x = logp x $ 
其中 负号 是 用来 保证 信息量 是 正数 或者 零 
而 $ log $ 函数 基 的 选择 是 任意 
的 信息论 中 基 常常 选择 为 2 因此 信息 
的 单位 为 比特 bits 而 机器学习 中 基 常常 
选择 为 自然常数 因此 单位 常常 被 称为 奈特 nats 
$ I x $ 也 被 称为 随机变量 $ x 
$ 的 自信 息 self information 描述 的 是 随机 
变量 的 某个 事件 发生 所 带来 的 信息量 图像 
如图 最后 我们 正式 引出 信息熵 现在 假设 一个 发送者 
想 传送 一个 随机 变量 的 值 给 接收者 那么 
在 这个 过程 中 他们 传输 的 平均 信息量 可以 
通过 求 $ I x = logp x $ 关于 
概率分布 $ p x $ 的 期望 得到 即 $ 
H X = \ displaystyle \ sum _ { x 
} p x logp x = \ sum _ { 
i = 1 } ^ { n } p x 
_ i logp x _ i $ $ H X 
$ 就 被 称为 随机变量 $ x $ 的 熵 
它 是 表示 随机变量 不 确定 的 度量 是 对 
所有 可能 发生 的 事件 产生 的 信息量 的 期望 
从 公式 可得 随机变量 的 取值 个数 越多 状态 数 
也就 越多 信息熵 就 越大 混乱 程度 就 越大 当 
随机分布 为 均匀分布 时 熵 最大 且 $ 0 \ 
le H X \ le logn $ 稍后 证明 将 
一维 随机变量 分布 推广 到 多维 随机变量 分布 则 其 
联合 熵 Joint entropy   为 $ H X Y 
= \ displaystyle \ sum _ { x y } 
p x y logp x y = \ sum _ 
{ i = 1 } ^ { n } \ 
sum _ { j = 1 } ^ { m 
} p x _ i y _ i logp x 
_ i y _ i $ 注意 点 1 熵 
只 依赖于 随机变量 的 分布 与 随机变量 取值 无关 所以 
也 可以 将 $ X $ 的 熵 记作 $ 
H p $ 2 令 0log0 = 0 因为 某个 
取值 概率 可能 为 0 那么 这些 定义 有着 什么样 
的 性质 呢 考虑 一个 随机变量 $ x $ 这个 
随机变量 有 4种 可能 的 状态 每个 状态 都是 等 
可能 的 为了 把   $ x $ 的 值 
传给 接收者 我们 需要 传输 2 比特 的 消息 $ 
H X = 4 \ times \ dfrac { 1 
} { 4 } log _ 2 \ dfrac { 
1 } { 4 } = 2 \ bits $ 
现在 考虑 一个 具有 4种 可能 的 状态 $ \ 
left \ { a b c d \ right \ 
} $ 的 随机变量 每个 状态 各自 的 概率 为 
$ \ left \ displaystyle \ frac { 1 } 
{ 2 } \ frac { 1 } { 4 
} \ frac { 1 } { 8 } \ 
frac { 1 } { 8 } \ right $ 
这种 情形 下 的 熵 为 $ H X = 
\ displaystyle \ frac { 1 } { 2 } 
log _ 2 \ frac { 1 } { 2 
} \ frac { 1 } { 4 } log 
_ 2 \ frac { 1 } { 4 } 
\ frac { 1 } { 8 } log _ 
2 \ frac { 1 } { 8 } \ 
frac { 1 } { 8 } log _ 2 
\ frac { 1 } { 8 } = 1.75 
\ bits $ 我们 可以 看到 非 均匀分布 比 均匀分布 
的 熵 要 小 现在 让 我们 考虑 如何 把 
变量 状态 的 类别 传递 给 接收者 与 之前 一样 
我们 可以 使用 一个 2 比特 的 数字 来 完成 
这 件 事情 然而 我们 可以 利用 非 均匀分布 这个 
特点 使用 更短 的 编码 来 描述 更 可能 的 
事件 使用 更长 的 编码 来 描述 不 太 可能 
的 事件 我们 希望 这样 做 能够 得到 一个 更短 
的 平均 编码 长度 我们 可以 使用 下面 的 编码 
串 哈夫曼 编码 0 10 110 111 来 表示 状态 
$ \ left \ { a b c d \ 
right \ } $ 传输 的 编码 的 平均 长度 
就是 average code length =   $ \ displaystyle \ 
frac { 1 } { 2 } \ times1 + 
\ frac { 1 } { 4 } \ times2 
+ 2 \ times \ frac { 1 } { 
8 } \ times3 = 1.75 \ bits $ 这个 
值 与 上方 的 随机 变量 的 熵 相等 熵 
和 最短 编码 长度 的 这种 关系 是 一种 普遍 
的 情形 Shannon 编码 定理 https / / baike . 
baidu . com / item / Shannon % 20% E 
7% BC % 96% E 7% A 0% 81% E 
5% AE % 9A % E 7% 90% 86/15585931 fr 
= aladdin   表明 熵 是 传输 一个 随机变量 状态值 
所需 的 比特 位 下界 最短 平均 编码 长度 因此 
信息熵 可以 应用 在 数据压缩 方面 这里 这篇文章 http / 
/ www . ruanyifeng . com / blog / 2014/09 
/ information entropy . html 讲 的 很 详细 了 
我 就不 赘述 了 证明 $ 0 \ le H 
X \ le logn $ 利用 拉格朗日 乘子 法 证明 
因为 $ p 1 + p 2 + \ dots 
+ p n = 1 $ 所以有 目标函数 $ f 
p 1 p 2 \ dots p n = p 
1 logp 1 + p 2 logp 2 + \ 
dots + p n logp n $ 约束条件 $ g 
p 1 p 2 \ dots p n \ lambda 
= p 1 + p 2 + \ dots + 
p n 1 = 0 $ 1 定义 拉格朗日 函数 
$ L p 1 p 2 \ dots p n 
\ lambda = p 1 logp 1 + p 2 
logp 2 + \ dots + p n logp n 
+ \ lambda p 1 + p 2 + \ 
dots + p n 1 $ 2 $ L p 
1 p 2 \ dots p n \ lambda $ 
分别 对 $ p 1 p 2 p n \ 
lambda $ 求 偏 导数 令 偏 导数 为 $ 
0 $ $ \ lambda log e \ cdot p 
1 = 0 $ $ \ lambda log e \ 
cdot p 2 = 0 $ $ \ dots \ 
dots $ $ \ lambda log e \ cdot p 
n = 0 $ $ p 1 + p 2 
+ \ dots + p n 1 = 0 $ 
3 求出 $ p 1 p 2 \ dots p 
n $ 的 值 解方程 得 $ p 1 = 
p 2 = \ dots = p n = \ 
displaystyle \ frac { 1 } { n } $ 
代入   $ f p 1 p 2 \ dots 
p n $ 中 得到 目标 函数 的 极值 为 
  $ f \ displaystyle \ frac { 1 } 
{ n } \ frac { 1 } { n 
} \ dots \ frac { 1 } { n 
} = \ frac { 1 } { n } 
log \ frac { 1 } { n } + 
\ frac { 1 } { n } log \ 
frac { 1 } { n } + \ dots 
+ \ frac { 1 } { n } log 
\ frac { 1 } { n } = log 
\ frac { 1 } { n } = logn 
$ 由此可证 $ logn $ 为 最大值 2 条件 熵 
Conditional entropy 条件 熵 $ H Y | X $ 
表示 在 已知 随机变量 $ X $ 的 条件 下 
随机变量   $ Y $ 的 不确定性 条件 熵 $ 
H Y | X $ 定义 为   $ X 
$ 给定 条件 下   $ Y $ 的 条件 
概率分布 的 熵 对     $ X $ 的 
数学期望 条件 熵 $ H Y | X $ 相当于 
联合 熵 $ H X Y $ 减去 单独 的 
熵 $ H X $ 即 $ H Y | 
X = H X Y H X $ 证明 如下 
举个 例子 比如 环境温度 是 低 还是 高 和我穿/nr 短袖 
还是 外套 这 两个 事件 可以 组成 联合 概率分布 $ 
H X Y $ 因为 两个 事件 加 起来 的 
信息量 肯定 是 大于 单一 事件 的 信息量 的 假设 
$ H X $ 对应 着 今天 环境温度 的 信息量 
由于 今天 环境温度 和 今天 我 穿 什么 衣服 这 
两个 事件 并 不是 独立 分布 的 所以在 已知 今天 
环境温度 的 情况 下 我 穿 什么 衣服 的 信息 
量 或者 说 不确定性 是 被 减少 了 当 已知 
$ H X $ 这个 信息量 的 时候 $ H 
X Y $ 剩下 的 信息 量 就是 条件 熵 
$ H Y | X = H X Y H 
X $ 因此 可以 这样 理解 描述 $ X $ 
和 $ Y $ 所需 的 信息 是 描述 $ 
X $ 自己 所需 的 信息 加上 给定     
$ X $ 的 条件 下 具体化     $ 
Y $ 所需 的 额外 信息 关于 条件 熵 的 
例子 可以 看 这篇文章 讲得 很详细 https / / zhuanlan 
. zhihu . com / p / 265517983 相对 熵 
Relative entropy 也称 KL 散度 Kullback – Leibler divergence 设 
$ p x $ $ q x $ 是 离散 
随机变量 $ X $ 中 取值 的 两个 概率分布 则 
$ p $ 对 $ q $ 的 相对 熵 
是 $ D _ { KL } p | | 
q = \ displaystyle \ sum _ { x } 
p x log \ frac { p x } { 
q x } = E _ { p x } 
log \ frac { p x } { q x 
} $ 性质 1 如果 $ p x $ 和 
$ q x $ 两个 分布 相同 那么 相对 熵 
等于 02 $ D _ { KL } p | 
| q \ not = D _ { KL } 
q | | p $ 相对 熵 具有 不对称性 大家 
可以 举个 简单 例子 算 一下 3 $ D _ 
{ KL } p | | q \ geq0 $ 
证明 如下 利用 Jensen 不等式 https / / en . 
wikipedia . org / wiki / Jensen % 27s _ 
inequality 因为 $ \ displaystyle \ sum _ { x 
} p x = 1 $ 所以 $ D _ 
{ KL } p | | q \ geq0 $ 
总结 相对 熵 可以 用来 衡量 两个 概率分布 之间 的 
差异 上面 公式 的 意义 就是 求 $ p $ 
与 $ q $ 之间 的 对数 差 在 $ 
p $ 上 的 期望值 4 交叉 熵 Cross entropy 
现在 有 关于 样 本集 的 两个 概率分布 $ p 
x $ 和 $ q x $ 其中     
$ p x $ 为 真实 分布   $ q 
x $ 非 真实 分布 如果 用 真实 分布   
$ p x $ 来 衡量 识别 别 一个样 本所 
需要 编码 长度 的 期望 平均 编码 长度 为 $ 
H p = \ displaystyle \ sum _ { x 
} p x log \ frac { 1 } { 
p x } $ 如果 使用 非 真实 分布 $ 
q x $ 来 表示 来自 真实 分布   $ 
p x $ 的 平均 编码 长度 则是 $ H 
p q = \ displaystyle \ sum   _ { 
x } p x log \ frac { 1 } 
{ q x } $ 因为 用 $ q x 
$ 来 编码 的 样本 来自 于 分布   $ 
q x $ 所以 $ H p q $ 中的 
概率 是 $ p x $ 此时 就将 $ H 
p q $ 称之为 交叉 熵 举个 例子 考虑 一个 
随机变量 $ x $ 真实 分布 $ p x = 
  \ left \ displaystyle \ frac { 1 } 
{ 2 } \ frac { 1 } { 4 
} \ frac { 1 } { 8 } \ 
frac { 1 } { 8 } \ right $ 
非 真实 分布 $ q x = \ left \ 
displaystyle \ frac { 1 } { 4 } \ 
frac { 1 } { 4 } \ frac { 
1 } { 4 } \ frac { 1 } 
{ 4 } \ right $   则 $ H 
p = 1.75 \ bits $ 最短 平均 码长 交叉 
熵 $ H p q = \ displaystyle \ frac 
{ 1 } { 2 } log _ 24 + 
\ frac { 1 } { 4 } log _ 
24 + \ frac { 1 } { 8 } 
log _ 24 + \ frac { 1 } { 
8 } log _ 24 = 2 \ bits $ 
由此 可以 看出 根据 非 真实 分布 $ q x 
$ 得到 的 平均 码长 大于 根据 真实 分布 $ 
p x $ 得到 的 平均 码长 我们 再 化简 
一下 相对 熵 的 公式 $ D _ { KL 
} p | | q = \ displaystyle \ sum 
_ { x } p x log \ frac { 
p x } { q x } = \ sum 
_ { x } p x logp x p x 
logq x $ 有 没有 发现 什么 熵 的 公式 
$ H p = \ displaystyle \ sum _ { 
x } p x logp x $ 交叉 熵 的 
公式   $ H p q = \ displaystyle \ 
sum   _ { x } p x log \ 
frac { 1 } { q x } = \ 
sum   _ { x } p x logq x 
$ 所以有 $ D _ { KL } p | 
| q = H p q H p $ 当用 
非 真实 分布 $ q x $ 得到 的 平均 
码长 比 真实 分布 $ p x $ 得到 的 
平均 码长 多出 的 比特 数 就是 相对 熵 又 
因为   $ D _ { KL } p | 
| q \ geq0 $ 所以 $ H p q 
\ geq H p $ 当 $ p x = 
q x $ 时取 等号 此时 交叉 熵 等于 信息熵 
并且 当 $ H p $ 为 常量 时 注 
在 机器 学习 中 训练 数据分布 是 固定 的 最小化 
相对 熵   $ D _ { KL } p 
| | q $ 等价 于 最小化 交叉 熵 $ 
H p q $ 也 等价 于 最大化 似 然 
估计 具体 参考 Deep Learning 5.5 在 机器 学习 中 
我们 希望 在 训练 数据 上 模型 学到 的 分布 
$ P model $ 和 真实 数据 的 分布   
  $ P real $ 越 接近 越好 所以 我们 
可以 使 其 相对 熵 最小 但是 我们 没有 真实 
数据 的 分布 所以 只能 希望 模型 学到 的 分布 
$ P model $ 和 训练 数据 的 分布 $ 
P train $ 尽量 相同 假设 训练 数据 是 从 
总体 中 独立 同 分布 采样 的 那么 我们 可以 
通过 最小化 训练 数据 的 经验 误差 来 降低 模型 
的 泛化 误差 即 希望 学到 的 模型 的 分布 
和 真实 分布 一致 $ P model \ simeq P 
real $ 但是 真实 分布 不 可知 假设 训练 数据 
是从 真实 数据 中 独立 同 分布 采样 的 $ 
P train \ simeq P real $ 因此 我们 希望 
学到 的 模型 分布 至少 和 训练 数据 的 分布 
一致 $ P train \ simeq P model $ 根据 
之前 的 描述 最小化 训练 数据 上 的 分布   
  $ P train $ 与 最小化 模型 分布   
$ P model $ 的 差异 等价 于 最小化 相对 
熵 即 $ D _ { KL } P train 
| | P model $ 此时   $ P train 
$ 就是 $ D _ { KL } p | 
| q $ 中的 $ p $ 即 真实 分布 
$ P model $ 就是 $ q $ 又 因为 
训练 数据 的 分布 $ p $ 是 给定 的 
所 以求     $ D _ { KL } 
p | | q $   等价 于求/nr $ H 
p q $ 得 证 交叉 熵 可以 用来 计算 
学习 模型 分布 与 训练 分布 之间 的 差异 交叉 
熵 广泛 用于 逻辑 回归 的 Sigmoid 和 Softmax 函数 
中 作为 损失 函数 使用 这 篇 文章 先 不说 
了 5 总结 信息熵 是 衡量 随机变量 分布 的 混乱 
程度 是 随机分布 各 事件 发生 的 信息量 的 期望值 
随机变量 的 取值 个数 越多 状态 数 也就 越多 信息熵 
就 越大 混乱 程度 就 越大 当 随机分布 为 均匀分布 
时 熵 最大 信息熵 推广 到 多维 领域 则 可得到 
联合 信息熵 条件 熵 表示 的 是 在 $ X 
$ 给定 条件 下 $ Y $ 的 条件 概率分布 
的 熵 对 $ X $ 的 期望 相对 熵 
可以 用来 衡量 两个 概率分布 之间 的 差异 交叉 熵 
可以 来 衡量 在 给定 的 真实 分 布下 使用 
非 真实 分布 所 指定 的 策略 消除 系统 的 
不确定性 所 需要 付出 的 努力 的 大小 或者 信息熵 
是 传输 一个 随机变量 状态值 所需 的 比特 位 下界 
最短 平均 编码 长度 相对 熵 是 指用   $ 
q $ 来 表示 分布   $ p $   
  额外 需要 的 编码 长度 交叉 熵 是 指用 
分布 $ q $ 来 表示 本来 表示 分布   
$ p $ 的 平均 编码 长度 6 参考 1 
吴军 数学 之美 2 李航 统计 学习 方法 3 马春 
鹏 模式识别 与 机器学习 3 https / / www . 
zhihu . com / question / 41252833   如何 通俗 
的 解释 交叉 熵 与 相对 熵 4 https / 
/ www . zhihu . com / question / 65288314 
/ answer / 244557337 为什么 交叉 熵 cross entropy 可以 
用 于 计算 代价 5 https / / baike . 
baidu . com / item / % E 4% BA 
% A 4% E 5% 8F % 89% E 7% 
86% B 5/8983241 fr = aladdin 交叉 熵 的 百度 
百科 解释 6 https / / blog . csdn . 
net / saltriver / article / details / 53056816 信息熵 
到底 是 什么 7 后记 本人 不是 大神 大牛 目前 
写 博客 是 为了 让 我 自己 更 深刻 地 
记忆 学过 的 知识 和对/nr 知识 进行 梳理 这篇 博客 
是 我 的 第一 篇 其中 借鉴 了 不少 其他 
博主 的 博客 里 的 分享 都有 标注 来源 如有 
遗忘 劳烦 提醒 衷心 感谢 他们 对 自己 所 掌握 
的 知识 的 分享 这篇 博客 可能 还 存在 着 
一些 错误 如有 发现 请求 斧正 谢谢 