机器学习 之 类别 不 平衡 问题 1 各种 评估 指标 
机器学习 之 类别 不 平衡 问题 2 ROC 和 PR 
曲线 机器学习 之 类别 不 平衡 问题 3 采 样方法 
前 两篇 主要 谈 类别 不 平衡 问题 的 评估 
方法 重心/a 放在/v 各类/r 评估/vn 指标/n 以及/c ROC/w 和/c PR/w 
曲/q 线上/i 只有 在 明确 了 这些 后 我们 才能 
据此 选择 具体 的 处理 类别 不 平衡 问题 的 
方法 本篇 介绍 的 采样 方法 是 其中 比较 常用 
的 方法 其 主要 目的 是 通过 改变 原有 的 
不平衡 样 本集 以期 获得 一个 平衡 的 样本分布 进而 
学习 出 合适 的 模型 采 样方法 大致 可 分为 
过 采样 oversampling 和欠/nr 采样 undersampling 虽然 过 采样 和降/nr 
采样 主题思想 简单 但 这些 年来/nr 研究 出 了 很多 
变种 本篇 挑 一些 来 具体 阐述 见下 思维导图 \ 
\ scriptsize { \ spadesuit } \ 过 采样 1 
. 随机 过 采样 随机 过 采样 顾名思义 就是 从 
样本 少 的 类别 中 随机 抽样 再将 抽样 得来 
的 样本 添加到 数据 集中 然而 这种 方法 如今 已经 
不大 使用 了 因为 重复 采样 往往 会 导致 严重 
的 过拟合 因而 现在 的 主流 过 采样 方法 是 
通过 某种 方式 人工合成 一些 少数类 样本 从而 达到 类别 
平衡 的 目的 而 这 其中 的 鼻祖 就是 SMOTE 
2 . SMOTESMOTE synthetic minority oversampling technique 的 思想 概括 
起来 就是 在 少数类 样本 之间 进行 插值 来 产生 
额外 的 样本 具体 地 对于 一个 少数类 样本 \ 
\ mathbf { x } _ i \ 使用 K 
近邻 法 k 值 需要 提前 指定 求 出离 \ 
\ mathbf { x } _ i \ 距离 最近 
的 k 个 少数类 样本 其 中距离 定义 为 样本 
之间 n 维 特征 空间 的 欧氏距离 然后 从k个/nr 近邻 
点中 随机 选取 一个 使用 下列 公式 生成 新 样本 
\ \ mathbf { x } _ { new } 
= \ mathbf { x } _ { i } 
+ \ mathbf { \ hat { x } } 
_ { i } \ mathbf { x } _ 
{ i } \ times \ delta \ tag { 
1.1 } \ 其中 \ \ mathbf { \ hat 
{ x } } \ 为 选出 的 k 近邻 
点 \ \ delta \ in 0 1 \ 是 
一个 随机数 下图 就是 一个 SMOTE 生成 样本 的 例子 
使用 的 是 3 近邻 可以 看出 SMOTE 生成 的 
样本 一般 就在 \ \ mathbf { x } _ 
{ i } \ 和\/nr \ mathbf { \ hat 
{ x } } _ { i } \ 相连 
的 直线 上 SMOTE 会 随机 选取 少数类 样本 用以 
合成 新 样本 而不 考虑 周边 样本 的 情况 这样 
容易 带来 两 个 问题 如果 选取 的 少数类 样本 
周围 也 都是 少数类 样本 则 新 合成 的 样本 
不会 提供 太多 有用 信息 这就 像 支持 向量 机中 
远离 margin 的 点 对 决策 边界 影响 不大 如果 
选取 的 少数类 样本 周围 都是/nr 多数 类 样本 这类 
的 样本 可能 是 噪音 则 新 合成 的 样本 
会 与 周围 的 多数 类 样本 产生 大 部分 
重叠 致使 分类 困难 总的来说 我们 希望 新 合成 的 
少数类 样 本能 处于 两个 类别 的 边界 附近 这样 
往往 能 提供 足够 的 信息 用 以 分类 而 
这 就是 下 面的 Border line SMOTE 算法 要做 的 
事情 3 . Border line SMOTE 这个 算 法会 先将 
所有 的 少数类 样本 分成 三类 如下 图 所示 noise 
所有 的 k 近邻 个 样本 都 属于 多数 类 
danger 超过 一半 的 k 近邻 样 本属于 多数 类 
safe 超过 一半 的 k 近邻 样 本属于 少数类 Border 
line SMOTE 算法 只会 从 处于 danger 状态 的 样本 
中 随机 选择 然后 用 SMOTE 算法 产生 新的 样本 
处于 danger 状态 的 样本 代表 靠近 边界 附近 的 
少数类 样本 而 处于 边界 附近 的 样本 往往 更容易 
被 误 分类 因而 Border line SMOTE 只对 那些 靠近 
边界 的 少数类 样本 进行 人工 合成 样本 而 SMOTE 
则 对 所有 少数类 样本 一视同仁 Border line SMOTE 分为 
两种 Borderline 1 SMOTE 和 Borderline 2 SMOTE Borderline 1 
SMOTE 在 合成 样本 时\/nr 1.1 \ 式 中的 \ 
\ mathbf { \ hat { x } } \ 
是 一个 少数类 样本 而 Borderline 2 SMOTE 中的 \ 
\ mathbf { \ hat { x } } \ 
则是 k 近邻 中的 任意 一个 样本 4 . ADASYNADASYN 
名为 自适应 合成 抽样 adaptive synthetic sampling 其 最大 的 
特点 是 采用 某种 机制 自动 决定 每个 少数类 样本 
需要 产生 多少 合成 样本 而 不是 像 SMOTE 那样 
对 每个 少数类 样本 合成 同 数量 的 样本 具体 
流程 如下 首先 计算 需要 合成 的 样本 总量 \ 
G = _ { maj } _ { min } 
\ times \ beta \ 其中 \ _ { maj 
} \ 为 多数 类 样本 数量 \ _ { 
min } \ 为 少数类 样本 数量 \ \ beta 
\ in 0 1 \ 为 系数 G 即为 总共 
想要 合成 的 少数类 样本 数量 如果 \ \ beta 
= 1 \ 则是 合成 后各/nr 类别 数目 相等 对于 
每个 少 类别 样本 \ \ mathbf { x } 
_ i \ 找出 其 K 近邻 个 点 并 
计算 \ \ Gamma _ i = \ frac { 
\ Delta _ i \ / \ K } { 
Z } \ 其中 \ \ Delta _ i \ 
为 K 近邻 个 点中 多数 类 样本 的 数量 
Z 为 规范化 因子 以 确保 \ \ Gamma \ 
构成 一个 分布 这样 若 一个 少数类 样本 \ \ 
mathbf { x } _ i \ 的 周围 多数 
类 样本 越多 则 其 \ \ Gamma _ i 
\ 也就 越高 最后 对 每个 少 类别 样本 \ 
\ mathbf { x } _ i \ 计算 需要 
合成 的 样本 数量 \ g _ i \ 再用 
SMOTE 算法 合成 新 样本 \ g _ i = 
\ Gamma _ i \ times G \ 可以 看到 
ADASYN 利用 分布 \ \ Gamma \ 来 自动 决定 
每个 少数类 样本 所 需要 合成 的 样本 数量 这等 
于是 给 每个 少数类 样本 施加 了 一个 权重 周围 
的 多数 类 样本 越 多则 权重 越高 ADASYN 的 
缺点 是 易受 离群 点 的 影响 如果 一个 少数类 
样本 的 K 近邻 都是 多数 类 样本 则 其 
权重 会 变得 相当 大 进而 会 在其 周围 生成 
较多 的 样本 下面 利用 sklearn 中的 make _ classification 
构造 了 一个 不 平衡 数据集 各 类别 比例 为 
{ 0 54 1 946 } 原始数据 SMOTE Borderline 1 
SMOTE Borderline 2 SMOTE 和 ADASYN 的 比较 见 下图 
左侧 为过 采样 后的/nr 决策 边界 右侧 为过 采样 后的/nr 
样本 分布 情况 可以/c 看到/v 过/ug 采样/v 后/f 原来/d 少数类/m 
的/uj 决策/n 边界/n 都/d 扩大/v 了/ul 导致 更多 的 多数 
类 样本 被 划为 少数类 了 从上 图 我们 也 
可以 比较 几 种过 采 样方法 各自 的 特点 用 
SMOTE 合成 的 样本分布 比较 平均 而 Border line SMOTE 
合成 的 样本 则 集中 在 类别 边界 处 ADASYN 
的 特性 是 一个 少数类 样本 周围 多数 类 样本 
越多 则 算法 会为 其 生成 越多 的 样本 从 
图中 也 可以 看到 生成 的 样本 大都 来自于 原来 
与 多数 类 比较 靠近 的 那些 少数类 样本 \ 
\ scriptsize { \ blacklozenge } \ 欠 采样 1 
. 随机 欠 采样 随机 欠 采样 的 思想 同样 
比较简单 就是 从 多数 类 样本 中 随机 选取 一些 
剔除 掉 这种 方法 的 缺点 是 被 剔除 的 
样本 可能 包含 着 一些 重要 信息 致使 学习 出来 
的 模型 效果 不好 2 . EasyEnsemble 和 B a 
l a n c e C a s c a 
d e E a s y E n s e 
m b l e 和 BalanceCascade 采用 集成 学习 机制 
来 处理 传统 随机 欠 采样 中的 信息 丢失 问题 
EasyEnsemble 将 多数 类 样本 随机 划分 成n个/nr 子集 每个 
子集 的 数量 等于 少数类 样本 的 数量 这 相当于 
欠 采样 接着 将 每个 子集 与 少数类 样本 结合 
起来 分别 训练 一个 模型 最后 将 n 个 模型 
集成 这样 虽然 每个 子集 的 样本 少于 总体 样本 
但 集成 后总/nr 信息量 并不 减少 如果说 EasyEnsemble 是 基于 
无 监督 的 方式 从 多数 类 样本 中 生成 
子集 进行 欠 采样 那么 BalanceCascade 则是 采用 了 有 
监督 结合 Boosting 的 方式 在 第 n 轮 训练 
中 将从 多数 类 样本 中 抽样 得来 的 子集 
与 少数类 样本 结合 起来 训练 一个 基 学习 器 
H 训练 完 后 多数 类 中 能被 H 正确 
分类 的 样本 会 被 剔除 在 接下来 的 第 
n + 1轮 中 从被/nr 剔除/v 后的/nr 多数/m 类/q 样本/n 
中/f 产生/n 子集/n 用于/v 与/p 少数类/m 样本/n 结合/v 起来/v 训练/vn 
最后 将 不同 的 基 学习 器 集成 起来 BalanceCascade 
的 有 监督表 现在 每 一轮 的 基 学习 器 
起到 了 在 多数 类 中 选择 样本 的 作用 
而其 Boosting 特点 则 体现 在 每 一轮 丢弃 被 
正确 分类 的 样本 进而 后续 基 学习 器 会 
更 注重 那些 之前 分类 错误 的 样本 3 . 
NearMissNearMiss 本质上 是 一种 原型 选择 prototype selection 方法 即从 
多数 类 样本 中 选取 最具 代表性 的 样本 用于 
训练 主要 是 为了 缓解 随机 欠 采样 中 的 
信息 丢失 问题 NearMiss 采用 一些 启发式 的 规则 来 
选择 样本 根据 规则 的 不同 可分为 3类 NearMiss 1 
选择 到 最近 的 K 个 少数类 样本 平均 距离 
最近 的 多数 类 样本 NearMiss 2 选择 到 最远 
的 K 个 少数类 样本 平均 距离 最近 的 多数 
类 样本 NearMiss 3 对于 每个 少数类 样本 选择 K 
个 最近 的 多数 类 样本 目的 是 保证 每个 
少数类 样本 都被 多数 类 样本 包围 NearMiss 1 和 
NearMiss 2 的 计算 开销 很大 因为 需要 计算 每个 
多 类别 样本 的 K 近邻 点 另外 NearMiss 1 
易受 离群 点 的 影响 如 下面 第二幅 图中 合理 
的 情况 是 处于 边界 附近 的 多数 类 样本 
会 被 选中 然而 由于 右下方 一些 少数类 离群 点 
的 存在 其 附近 的 多数 类 样本 就被 选择 
了 相比之下 NearMiss 2 和 NearMiss 3 不易 产生 这 
方面 的 问题 4 . 数据 清洗 方法 data cleaning 
tichniques 这类 方法 主要 通过 某种 规则 来 清洗 重叠 
的 数据 从而 达到 欠 采样 的 目的 而 这些 
规则 往往 也是 启发性 的 下面 进行 简要 阐述 Tomek 
Link Tomek Link 表示 不同 类别 之间 距离 最近 的 
一对 样本 即 这 两个 样本 互 为最 近邻 且 
分属 不同 类别 这样 如果 两个 样本 形成 了 一个 
Tomek Link 则 要么 其中 一个 是 噪音 要么 两个 
样本 都在/nr 边界 附近 这样 通过 移除 Tomek Link 就能 
清 洗掉 类 间 重叠 样本 使得 互 为最 近邻 
的 样本 皆 属于 同一 类别 从而 能 更好 地 
进行 分类 下 图左 上为 原始数据 右 上为 SMOTE 后的/nr 
数据 左下 虚线 标识 出 Tomek Link 右 下为 移除 
Tomek Link 后的/nr 数据集 可以 看到 不同 类别 之间 样本 
重叠 减少 了 很多 Edited Nearest Neighbours ENN 对于 属于 
多数 类 的 一个 样本 如果/c 其/r K/w 个/q 近邻/v 
点/m 有/v 超过/v 一半/m 都/d 不属于/i 多数/m 类/q 则 这个 
样 本会 被 剔除 这个 方法 的 另一个 变种 是 
所有 的 K 个 近邻 点 都 不属于 多数 类 
则 这个 样 本会 被 剔除 最后 数据 清洗 技术 
最大 的 缺点 是 无法 控制 欠 采样 的 数量 
由于 都在/nr 某种 程度 上 采用 了 K 近邻 法 
而 事实上 大 部分 多数 类 样本 周围 也 都是 
多数 类 因而 能 剔除 的 多数 类 样本 比较 
有限 \ \ scriptsize { \ clubsuit } \ 过 
采样 和欠/nr 采样 结合 上 文中 提到 SMOTE 算法 的 
缺点 是 生成 的 少数类 样本 容易 与 周围 的 
多数 类 样本 产生 重叠 难以 分类 而 数据 清洗 
技术 恰好 可以 处理 掉 重叠 样本 所以 可以 将 
二者 结合 起来 形成 一个 pipeline 先过 采样 再 进行 
数据 清洗 主要 的 方法 是 SMOTE + ENN 和 
SMOTE + Tomek 其中 SMOTE + ENN 通常 能 清除 
更多 的 重叠 样本 如 下图 综上 本文 简要 介绍 
了 几种 过 采样 和欠/nr 采样 的 方法 其实 还有 
更多 的 变种 可 参阅 imbalanced learn 最后 列出 的 
的 References \ \ scriptsize { \ bigstar } \ 
采样 方法 的 效果 最 后 当然 还有 一个 最 
重要 的 问题 本文 列举 了 多种 不同 的 采 
样方法 那么 哪种 方法 效果 好呢 下面 用 两个 数据集 
对 各类 方法 进行 比较 同样 结论 也 是 基于 
这 两个 数据集 本文 代码 主要 使用 了 imbalanced learn 
这个 库 算是 scikit learn 的 姐妹 项目 第一 个 
数据 集为 us _ crime 多数 类 样本 和 少数类 
样本 的 比例 为 12 1 us _ crime = 
fetch _ datasets us _ crime X _ train X 
_ test y _ train y _ test = train 
_ test _ split us _ crime . data us 
_ crime . target test _ size = 0.5 random 
_ state = 42 stratify = us _ crime . 
target # 分为 训练 集 和 测试 集 总共 用 
9个 模型 1个 base _ model 即 不 进行 采样 
的 模型 加上 8个 过 采样 和欠采/nr 样方法 imbalanced learn 
中 大部分 采样 方法 都 可以 使用 make _ pipeline 
将 采样 方法 和 分类 模型 连接起来 但 两种 集成 
方法 EasyEnsemble 和 BalanceCascade 无法 使用 make _ pipeline 因为 
本质上 是 集成 了 好几 个 分类 模型 所以 需要 
自 定义方法 sampling _ methods = Original SMOTE random _ 
state = 42 SMOTE random _ state = 42 kind 
= borderline1 ADASYN random _ state = 42 EasyEnsemble random 
_ state = 42 BalanceCascade random _ state = 42 
NearMiss version = 3 random _ state = 42 SMOTEENN 
random _ state = 42 SMOTETomek random _ state = 
42 names = Base model SMOTE Borderline SMOTE ADASYN EasyEnsemble 
BalanceCascade NearMiss SMOTE + ENN SMOTE + Tomek def ensemble 
_ method method # EasyEnsemble 和 BalanceCascade 的 方法 count 
= 0 xx yy = method . fit _ sample 
X _ train y _ train y _ pred y 
_ prob = np . zeros len X _ test 
np . zeros len X _ test for X _ 
ensemble y _ ensemble in zip xx yy model = 
L o g i s t i c R e 
g r e s s i o n model . 
fit X _ ensemble y _ ensemble y _ pred 
+ = model . predict X _ test y _ 
prob + = model . predict _ proba X _ 
test 1 count + = 1 return np . where 
y _ pred = 0 1 1 y _ prob 
/ count # 画 ROC 曲线 plt . figure figsize 
= 15 8 for name method in zip names sampling 
_ methods t0 = time . time if name = 
= EasyEnsemble or name = = BalanceCascade y _ pred 
y _ prob = ensemble _ method method else model 
= make _ pipeline method L o g i s 
t i c R e g r e s s 
i o n model . fit X _ train y 
_ train y _ pred = model . predict X 
_ test y _ prob = model . predict _ 
proba X _ test 1 fpr tpr thresholds = roc 
_ curve y _ test y _ prob pos _ 
label = 1 plt . plot fpr tpr lw = 
3 label = { } AUC = { . 2f 
} time = { . 2f } s . format 
name auc fpr tpr time . time t0 plt . 
xlabel FPR fontsize = 17 plt . ylabel TPR fontsize 
= 17 plt . legend fontsize = 14 # 画 
PR 曲线 plt . figure figsize = 15 8 for 
name method in zip names sampling _ methods t0 = 
time . time if name = = EasyEnsemble or name 
= = BalanceCascade y _ pred y _ prob = 
ensemble _ method method else model = make _ pipeline 
method L o g i s t i c R 
e g r e s s i o n model 
. fit X _ train y _ train y _ 
pred = model . predict X _ test y _ 
prob = model . predict _ proba X _ test 
1 precision recall thresholds = precision _ recall _ curve 
y _ test y _ prob pos _ label = 
1 plt . plot recall precision lw = 3 label 
= { } AUC = { . 2f } time 
= { . 2f } s . format name auc 
recall precision time . time t0 plt . xlabel Recall 
fontsize = 17 plt . ylabel Precision fontsize = 17 
plt . legend fontsize = 14 loc = upper right 
第二个 数据集 是 abalone 多数 类 样本 和 少数类 样本 
的 比例 为 130 1 非常 悬殊 abalone _ 19 
= fetch _ datasets abalone _ 19 X _ train 
X _ test y _ train y _ test = 
train _ test _ split abalone _ 19 . data 
abalone _ 19 . target test _ size = 0.5 
random _ state = 42 stratify = abalone _ 19 
. target # 画 ROC 曲线 和 PR 曲线 plt 
. figure figsize = 15 8 for name method in 
zip names sampling _ methods t0 = time . time 
if name = = EasyEnsemble or name = = BalanceCascade 
y _ pred y _ prob = ensemble _ method 
method else model = make _ pipeline method L o 
g i s t i c R e g r 
e s s i o n model . fit X 
_ train y _ train y _ pred = model 
. predict X _ test y _ prob = model 
. predict _ proba X _ test 1 fpr tpr 
thresholds = roc _ curve y _ test y _ 
prob pos _ label = 1 plt . plot fpr 
tpr lw = 3 label = { } AUC = 
{ . 2f } time = { . 2f } 
s . format name auc fpr tpr time . time 
t0 plt . xlabel FPR fontsize = 17 plt . 
ylabel TPR fontsize = 17 plt . legend fontsize = 
14 plt . figure figsize = 15 8 for name 
method in zip names sampling _ methods t0 = time 
. time if name = = EasyEnsemble or name = 
= BalanceCascade y _ pred y _ prob = ensemble 
_ method method else model = make _ pipeline method 
L o g i s t i c R e 
g r e s s i o n model . 
fit X _ train y _ train y _ pred 
= model . predict X _ test y _ prob 
= model . predict _ proba X _ test 1 
precision recall thresholds = precision _ recall _ curve y 
_ test y _ prob pos _ label = 1 
plt . plot recall precision lw = 3 label = 
{ } AUC = { . 2f } time = 
{ . 2f } s . format name auc recall 
precision time . time t0 plt . xlabel Recall fontsize 
= 17 plt . ylabel Precision fontsize = 17 plt 
. legend fontsize = 14 loc = best 从 以上 
几张 图 中 我们 可以 得出 一些 推论 就 时间 
开销 而言 BalanceCascade 以及 两 种过 采样 欠 采样 结合 
的 方法 SMOTE + ENN 和 SMOTE + Tomek 耗时 
最高 如果 追求 速度 的话 这几个 可能 并非 很好 的 
选择 第一 个 数据集 us _ crime 的 多数 类 
和 少数类 样本 比例 为 12 1 相差 不是 很 
悬殊 综合 ROC 曲线 和 PR 曲线 的 AUC 来看 
两种 集成 方法 EasyEnsemble 和 BalanceCascade 表现 较好 对于 第二 
个 数据集 abalone _ 19 来说 多数 类 和 少数类 
样本 比例 为 130 1 而且 少数类 样本 非常少 因而/c 
从/p 结果/n 来看/u 几/m 种过/v 采样/v 方/n 法如/i Borderline/w SMOTE 
SMOTE + Tomek 等 效果 较好 可见 在 类别 差异 
很大 的 情况 下 过 采样 能 一定 程度 上 
弥补 少数类 样本 的 极端 不足 然而 从 PR 曲线 
上 来看 其实 结果 都不/nr 尽如人意 对于 这种 极端 不 
平衡 的 数据 可能 比较 适合 异常 检测 的 方法 
以后 有 机会 详述 上 篇 文章 中 提到 ROC 
曲线 通常 会 呈现 一个 过分 乐观 的 效果 估计 
这里 再一次 得到 体现 第一 个 数据 集中 大 部分 
ROC 曲线 的 AUC 都在 0.9 左右 而 PR 曲线 
都在 0.5 左右 第二个 数据集 则 更 夸张 从/p PR/w 
曲线/n 来看/u 其实/d 模型/n 对于/p 少数类/m 的/uj 预测/vn 准确率/n 是/v 
无限/v 接近/v 于0了/nr 但在 ROC 曲 线上 却 很难 看出 
这 一点 如果/c 单纯/a 从/p ROC/w 曲线/n 和/c PR/w 曲线/n 
上/f 来看/u 表面 上 各种 采样 方法 和 base model 
差别 不大 但 实际上 这 其中 却是 暗流 涌动 下面 
来 看一下 us _ crime 数据 集中 各 方法 的 
classification report def ensemble _ method _ 2 method # 
定义 一个 简化 版 集成 方法 xx yy = method 
. fit _ sample X _ train y _ train 
y _ pred y _ prob = np . zeros 
len X _ test np . zeros len X _ 
test for X _ ensemble y _ ensemble in zip 
xx yy model = L o g i s t 
i c R e g r e s s i 
o n model . fit X _ ensemble y _ 
ensemble y _ pred + = model . predict X 
_ test return np . where y _ pred = 
0 1 1 us _ crime = fetch _ datasets 
us _ crime X _ train X _ test y 
_ train y _ test = train _ test _ 
split us _ crime . data us _ crime . 
target test _ size = 0.5 random _ state = 
42 stratify = us _ crime . target class _ 
names = majority class minority class model = L o 
g i s t i c R e g r 
e s s i o n model . fit X 
_ train y _ train y _ pred = model 
. predict X _ test print Base Model \ n 
classification _ report y _ test y _ pred target 
_ names = class _ names \ n model = 
make _ pipeline SMOTE random _ state = 42 L 
o g i s t i c R e g 
r e s s i o n model . fit 
X _ train y _ train y _ pred = 
model . predict X _ test print SMOTE \ n 
classification _ report y _ test y _ pred target 
_ names = class _ names \ n model = 
make _ pipeline NearMiss version = 2 random _ state 
= 42 L o g i s t i c 
R e g r e s s i o n 
model . fit X _ train y _ train y 
_ pred = model . predict X _ test print 
NearMiss \ n classification _ report y _ test y 
_ pred target _ names = class _ names \ 
n y _ pred = ensemble _ method _ 2 
EasyEnsemble random _ state = 42 class _ names = 
majority class minority class print EasyEnsemble \ n classification _ 
report y _ test y _ pred target _ names 
= class _ names \ n 输出 Base Model precision 
recall f1 score support majority class 0.95 0.98 0.97 922 
minority class 0.62 0.35 0.44 75 avg / total 0.92 
0.93 0.93 997 SMOTE precision recall f1 score support majority 
class 0.98 0.90 0.94 922 minority class 0.38 0.75 0.51 
75 avg / total 0.93 0.89 0.91 997 NearMiss precision 
recall f1 score support majority class 0.97 0.81 0.88 922 
minority class 0.24 0.73 0.36 75 avg / total 0.92 
0.80 0.84 997 EasyEnsemble precision recall f1 score support majority 
class 0.98 0.85 0.91 922 minority class 0.31 0.84 0.45 
75 avg / total 0.93 0.85 0.88 997 这里 我们 
主要 关注 少数类 样本 可以 看到 Base Model 的 特点 
是 precision 高 recall 低 而 几种 采样 方法 则 
相反 precision 低 recall 高 采 样方法 普遍 扩大 了 
少数类 样本 的 决策 边界 从上 文中 的 决策 边界 
图 就能 看出来 所以 把 很多 多数 类 样本 也 
划为 少数类 了 导致 precision 下降 而 recall 提升 当然 
这些 都是 分类 阈值 为 0.5 的 前提 下 得出 
的 结论 如果 进一步 调整 阈值 的话 能 得到 更好 
的 模型 策略 是 base model 的 阈值 往 下调 
采样 方法 的 阈值 往上调 def ensemble _ method _ 
3 method xx yy = method . fit _ sample 
X _ train y _ train y _ pred y 
_ prob = np . zeros len X _ test 
np . zeros len X _ test for X _ 
ensemble y _ ensemble in zip xx yy model = 
L o g i s t i c R e 
g r e s s i o n model . 
fit X _ ensemble y _ ensemble y _ pred 
+ = np . where model . predict _ proba 
X _ test 1 = 0.7 1 1 # 阈值 
0.7 return np . where y _ pred = 0 
1 1 us _ crime = fetch _ datasets us 
_ crime X _ train X _ test y _ 
train y _ test = train _ test _ split 
us _ crime . data us _ crime . target 
test _ size = 0.5 random _ state = 42 
stratify = us _ crime . target model = L 
o g i s t i c R e g 
r e s s i o n model . fit 
X _ train y _ train y _ pred = 
np . where model . predict _ proba X _ 
test 1 = 0.3 1 1 print Base Model threshold 
= 0.3 \ n classification _ report y _ test 
y _ pred target _ names = class _ names 
\ n model = make _ pipeline SMOTE random _ 
state = 42 L o g i s t i 
c R e g r e s s i o 
n model . fit X _ train y _ train 
y _ pred = np . where model . predict 
_ proba X _ test 1 = 0.9 1 1 
print SMOTE threshold = 0.9 \ n classification _ report 
y _ test y _ pred target _ names = 
class _ names \ n model = make _ pipeline 
NearMiss version = 2 random _ state = 42 L 
o g i s t i c R e g 
r e s s i o n model . fit 
X _ train y _ train y _ pred = 
np . where model . predict _ proba X _ 
test 1 = 0.7 1 1 print NearMiss threshold = 
0.7 \ n classification _ report y _ test y 
_ pred target _ names = class _ names \ 
n model = EasyEnsemble random _ state = 42 y 
_ pred = ensemble _ method _ 3 model class 
_ names = majority class minority class print EasyEnsemble threshold 
= 0.7 \ n classification _ report y _ test 
y _ pred target _ names = class _ names 
\ n 输出 Base Model threshold = 0.3 precision recall 
f1 score support majority class 0.96 0.96 0.96 922 minority 
class 0.53 0.53 0.53 75 avg / total 0.93 0.93 
0.93 997 SMOTE threshold = 0.9 precision recall f1 score 
support majority class 0.96 0.97 0.97 922 minority class 0.60 
0.49 0.54 75 avg / total 0.93 0.94 0.93 997 
NearMiss threshold = 0.7 precision recall f1 score support majority 
class 0.96 0.90 0.93 922 minority class 0.32 0.59 0.42 
75 avg / total 0.92 0.88 0.89 997 EasyEnsemble threshold 
= 0.7 precision recall f1 score support majority class 0.97 
0.92 0.95 922 minority class 0.42 0.69 0.53 75 avg 
/ total 0.93 0.91 0.92 997 在 经过 阈值 调整 
后 各/r 方法/n 的/uj 整体/n F1/i 分数/n 都有/nr 提高/v 可见 
很多 单 指标 如 precision recall 等 都会 受到 不同 
阈值 的 影响 所以 这 也是 为什么 在 类别 不 
平衡 问题 中用 ROC 和 PR 曲线 来 评估 非常 
流行 因为 它们 不受 特定 阈值 变化 的 影响 反映 
的 是 模型 的 整体 预测 能力 不过 在 这里 
我 不得不 得出 一个 比较 悲观 的 结论 就 这 
两个 数据集 的 结果 来看 如果 本身 数据偏斜 不是 很 
厉害 那么 采样 方法 的 提升 效果 很 细微 如果 
本身 数据偏斜 很厉害 采 样方法 纵使 比 base model 好 
很多 但 由于 base model 本身 的 少数类 预测 能力 
很差 所以 本质上 也不 尽如人意 这 就像 考试 原来 一直 
靠 10分 采样 了 之后 考了 30分 绝对 意义 上 
提升 很大 但 其实 还是 差得 远了 完 