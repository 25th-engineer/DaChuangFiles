机器学习 确定 最佳 聚 类 数目 的 10种 方法 在 
聚类分析 的 时候 确定 最佳 聚 类 数目 是 一个 
很 重要 的 问题 比如 kmeans 函数 就要 你 提供 
聚 类 数目 这个 参数 总不能 两眼一抹黑 乱 填 一个 
吧 之前 也 被 这个 问题 困扰 过 看 了 
很多 博客 大多 泛泛 带过 今天 把 看到 的 这么 
多 方法 进行 汇总 以及 代码 实现 并 尽量 弄清 
每个 方法 的 原理 数据集 选用 比较 出名 的 wine 
数据集 进行 分析 library gclus data wine head wine Loading 
required package cluster 因为 我们 要 找 一个 数据 集 
进行 聚类分析 所以 不 需要 第一列 的 种类 标签 信息 
因此 去掉 第一列 同时 注意到 每 一列 的 值 差别 
很大 从1到/nr 100多 都有 这样 会 造成 误差 所以 需要 
归一化 用 scale 函数 dataset wine 1 # 去除 分类 
标签 dataset scale dataset 去掉 标签 之后 就 可以 开始 
对 数据 集 进行 聚类分析 了 下面/f 就/d 一一/m 介绍/v 
各种/r 确定/v 最佳/z 聚/v 类/q 数目/n 的/uj 方法/n 判定/v 方法/n 
1/m ./i mclust/w 包/v mclust/w 包是/nr 聚类分析/l 非常/d 强大/a 的/uj 
一个/m 包/v 也是 上课 时 老师 给 我们 介绍 的 
一个 包 每次 导 入时 有一种 科技感 帮助 文档 非常 
详尽 可以 进行 聚 类 分类 密度 分析 Mclust 包 
方法 有点 暴力 聚 类 数目 自定义 比如 我 选取 
的 从1到/nr 20 然后 一共 14种 模型 每一种/i 模型/n 都/d 
计算/v 聚/v 类/q 数目/n 从1到/nr 20/m 的/uj BIC/w 值/n 最终 
确定 最佳 聚 类 数目 这种 方法 的 思想 很 
直接了当 但是 弊端 也就 显然 易 见了 时间 复杂度 太高 
效率 低 library mclust m _ clust Mclust as . 
matrix dataset G = 1 20 # 聚 类 数目 
从1/nr 一直 试 到 20 summary m _ clust Gaussian 
finite mixture model fitted by EM algorithm Mclust EVE ellipsoidal 
equal volume and orientation model with 3 components log . 
likelihood n df BIC ICL 3032.45 178 156 6873.257 6873.549 
Clustering table 1 2 363 51 64 可见 该函 数 
已经 把 数据 集聚 类 为 3种 类型 了 数目 
分别为 63 51 64 再 画出 14个 指标 随着 聚 
类 数目 变化 的 走势图 plot m _ clust BIC 
下表 是 这些 模型 的 意义 它们 应该 分别 代表 
着 相关性 完全 正 负相关 对角线 稍 强正/nr 负相关 椭圆 
无关 圆 等 参数 的 改变 对应 的 模型 研究 
清楚 这些 又 是 非常 复杂 的 问题 了 先 
按下不表 知道 BIC 值 越大 则 说明 所 选取 的 
变量 集合 拟合 效果 越好   上图 中 除了 两个 
模型 一直 递增 其他 的 12 模型 数 基本上 都是 
在 聚 类 数目 为 3 的 时候 达到 峰值 
所以 该 算法 由此 得出 最佳 聚 类 数目 为 
3 的 结论 mclust 包还/nr 可以 用于 分类 密度估计 等 
这个 包 值得 好好 把玩 注意 此 BIC 并 不是 
贝叶斯 信息 准则 最近 上课 老师 讲 金融 模型 时 
提到 了 BIC 值 说 BIC 值 越小 模型 效果 
越好 顿时 想起 这里 是 在 图中 BIC 极大值 为 
最佳 聚 类 数目 然后 和 老师 探讨 了 这个 
问题 之前 这里 误导 大家 了 Mclust 包 里面 的 
BIC 并 不是 贝叶斯 信息 准则 1 . 维基 上 
的 贝叶斯 信息 准则 定义 与 log likelihood 成反比 极大 
似 然 估计 是 值 越大 越好 那么 BIC 值 
确实是 越小 模型 效果 越好 2 . Mclust 包 中的 
BIC 定义 3 这是 Mclust 包 里面 作者 定义 的 
BIC 值 此 BIC 非 彼 BIC 这里 是 作者 
自己 定义 的 BIC 可以 看到 这里 的 BIC 与 
极大 似 然 估计 是 成正比 的 所以 这里 是 
BIC 值 越大 越好 与/p 贝叶斯/nr 信息/n 准则/n 值/n 越小/i 
模型/n 越好/d 的/uj 结论/n 并不/i 冲突/vn 2/m ./i Nbclust/w 包/v 
Nbclust/w 包是我/nr 在/p R 语言 实战 上 看到 的 一个 
包 思想/n 和/c mclust/w 包/v 比较/d 相近/v 也是 定义 了 
几十 个 评估 指标 然后 聚 类 数目 从2/nr 遍历 
到 15 自己 设定 然后 通过 这些 指标 看 分别 
在 聚 类 数 为 多少 时 达到 最优 最后 
选择 指标 支持 数 最多 的 聚 类 数目 就是 
最佳 聚 类 数目 library NbClust set . seed 1234 
# 因为 method 选择 的 是 kmeans 所以 如果 不 
设定 种子 每次 跑得 结果 可能 不同 nb _ clust 
NbClust dataset distance = euclidean min . nc = 2 
max . nc = 15 method = kmeans index = 
alllong alphaBeale = 0.1 * * * The Hubert index 
is a graphical method of determining the number of clusters 
. In the plot of Hubert index we seek a 
significant knee that corresponds to a significant increase of the 
value of the measure i . e the significant peak 
in Hubert index second differences plot . * * * 
The D index is a graphical method of determining the 
number of clusters . In the plot of D index 
we seek a significant knee the significant peak in Dindex 
second differences plot that corresponds to a significant increase of 
the value of the measure . * * * * 
* * * * * * * * * * 
* * * * * * * * * * 
* * * * * * * * * * 
* * * * * * * * * * 
* * * * * * * * * * 
* * * * * * * * * * 
* * * * Among all indices * 5 proposed 
2 as the best number of clusters * 16 proposed 
3 as the best number of clusters * 1 proposed 
10 as the best number of clusters * 1 proposed 
12 as the best number of clusters * 1 proposed 
14 as the best number of clusters * 3 proposed 
15 as the best number of clusters * * * 
* * Conclusion * * * * * * According 
to the majority rule the best number of clusters is 
3 * * * * * * * * * 
* * * * * * * * * * 
* * * * * * * * * * 
* * * * * * * * * * 
* * * * * * * * * * 
* * * * * * * * * * 
* * * * * * * * barplot table 
nb _ clust $ Best . nc 1 xlab = 
聚 类 数 ylab = 支持 指标 数 可以 看到 
有 16个 指标 支持 最佳 聚 类 数目 为 3 
5个 指标 支持 聚 类 数 为 2 所以 该 
方法 推荐 的 最佳 聚 类 数目 为 3.3 . 
组 内 平方 误差 和 拐点 图 想必 之前 动辄 
几十 个 指标 这里 就 用 一个 最 简单 的 
指标 sum of squared error SSE 组 内 平方 误差 
和来/nr 确定 最佳 聚 类 数目 这个 方法 也 是 
出于 R 语言 实战 自定义 的 一个 求 组 内 
误差 平方和 的 函数 wssplot function data nc = 15 
seed = 1234 { wss nrow data 1 * sum 
apply data 2 var for i in 2 nc { 
set . seed seed wss i sum kmeans data centers 
= i $ withinss } plot 1 nc wss type 
= b xlab = Number of Clusters ylab = Within 
groups sum of squares } wssplot dataset 随着 聚 类 
数目 增多 每 一个 类别 中 数量 越来越 少 距离 
越来越 近 因此 WSS 值 肯定 是 随着 聚 类 
数目 增多 而 减少 的 所以 关注 的 是 斜率 
的 变化 但 WWS 减少 得很 缓慢 时 就 认为 
进一步 增大 聚 类 数 效果 也 并不 能 增强 
存在 得 这个 肘 点 就是 最佳 聚 类 数目 
从 一类 到 三类 下降 得 很快 之后 下降 得 
很慢 所以 最佳 聚 类 个数 选为 三 另外 也 
有 现成 的 包 factoextra 可以 调用 library factoextra library 
ggplot2 set . seed 1234 fviz _ nbclust dataset kmeans 
method = wss + geom _ vline xintercept = 3 
linetype = 2 Loading required package ggplot2 选定 为 3类 
为 最佳 聚 类 数目 用 该 包下 的 fviz 
_ cluster 函数 可视化 一下 聚 类 结果 km . 
res kmeans dataset 3 fviz _ cluster km . res 
data = dataset 4 . PAM Partitioning Around Medoids 围绕 
中心点 的 分割 算法 k means 算法 取得 是 均值 
那么 对于 异常 点 其实 对其 的 影响 非常 大 
很可能 这种 孤立 的 点 就 聚 为 一类 一个 
改进 的 方法 就是 PAM 算法 也叫 k medoids clustering 
首先 通过 fpc 包 中的 pamk 函数 得到 最佳 聚 
类 数目 library fpc pamk . best pamk dataset pamk 
. best $ nc3pamk 函数 不 需要 提供 聚 类 
数目 也会 直接 自动 计算 出 最佳 聚 类 数 
这里 也 得到 为 3 得到 聚 类 数 提供给 
cluster 包下 的 pam 函数 并 进行 可视化 library cluster 
clusplot pam dataset pamk . best $ nc 5 . 
Calinsky criterion 这个 评估 标准 定义 5 如下 其中 k 
是 聚 类 数 N 是 样本 数 SSw 是 
我们 之前 提到 过 的 组 内 平方和 误差 SSb 
是 组 与 组 之间 的 平方和 误差 SSw 越小 
SSb 越大 聚 类 效果 越好 所以 Calinsky criterion 值 
一般来说 是 越大 聚 类 效果 越好 library vegan ca 
_ clust cascadeKM dataset 1 10 iter = 1000 ca/w 
_/i clust/w $/i results/w 可以/c 看到/v 该/r 函数/n 把/p 组/zg 
内/n 平方和/nr 误差/n 和/c Calinsky/w 都/d 计算/v 出来/v 了/ul 可以 
看到 calinski 在 聚 类 数 为 3时 达到 最大值 
calinski . best as . numeric which . max ca 
_ clust $ results 2 calinski . best3 画图 出来 
观察 一下 plot fit sortg = TRUE grpmts . plot 
= TRUE 注意 到 那个 红点 就是 对应 的 最大值 
自带 的 绘图 横轴 纵轴 取 的 可能 不 符合 
我们 的 直觉 把 数据 取出 来 自己 单独 画 
一下 calinski as . data . frame ca _ clust 
$ results 2 calinski $ cluster c 1 10 library 
ggplot2 ggplot calinski aes x = calinski 2 y = 
calinski 1 + geom _ line Warning message Removed 1 
rows containing missing values geom _ path . 这个 看上去 
直观 多了 这就 很 清晰 的 可以 看到 在 聚 
类 数目 为 3时 calinski 指标 达到 了 最大值 所以 
最佳 数目 为 36 . Affinity propagation AP clustering 这个 
本质上 是 类似 kmeans 或者 层次 聚 类 一样 是 
一种 聚 类 方法 因为 不 需要 像 kmeans 一样 
提供 聚 类 数 会 自动 算出 最佳 聚 类 
数 因此 也 放到 这里 作为 一种 计算 最佳 聚 
类 数目 的 方法 AP 算法 的 基本 思想 是 
将 全部 样本 看作 网络 的 节点 然后 通过 网络 
中 各条 边的/nr 消息 传递 计算出 各 样本 的 聚 
类 中心 聚 类 过程 中 共有 两种 消息 在各 
节点 间 传递 分别 是 吸引 度 responsibility 和 归属 
度 availability AP 算法 通过 迭代 过程 不断 更新 每 
一个 点 的 吸引 度 和 归属 度 值 直到 
产生 m 个 高 质量 的 Exemplar 类似于 质心 同时 
将 其余 的 数据 点 分配 到 相应 的 聚 
类 中 7 library apcluster ap _ clust apcluster negDistMat 
r = 2 dataset length ap _ clust @ clusters 
15 该 聚 类 方法 推荐 的 最佳 聚 类 
数目 为 15 再用 热 力图 可视化 一下 heatmap ap 
_ clust 选 x 或者 y 方向 看 对称 可以 
数 出来 叶子 节点 一共 15个 7 . 轮廓 系数 
Average silhouette method 轮廓 系数 是 类 的 密集 与 
分散 程度 的 评价 指标 a i 是 测量 组 
内 的 相似 度 b i 是 测量 组间 的 
相似 度 s i 范围 从 1 到 1 值 
越大 说明 组 内 吻合 越高 组间 距离 越远 也 
就是说 轮廓 系 数值 越大 聚 类 效果 越好 9 
require cluster library factoextra fviz _ nbclust dataset kmeans method 
= silhouette 可以 看到 也是 在 聚 类 数 为 
3时 轮廓 系数 达到 了 峰值 所以 最佳 聚 类 
数 为 38 . Gap Statistic 之前 我们 提到 了 
WSSE 组 内 平方和 误差 该种 方法 是 通过 找 
肘 点 来 找到 最佳 聚 类 数 肘 点 
的 选择 并 不是 那么 清晰 因此 斯坦福 大学 的 
Robert 等 教授 提出 了 Gap Statistic 方法 定义 的 
Gap 值 为 9 取 对数 的 原因 是 因为 
Wk 的 值 可能 很 大 通过 这个 式 子来 
找出 Wk 跌落 最快 的 点 Gap 最大值 对应 的 
k 值 就是 最佳 聚 类 数 library cluster set 
. seed 123 gap _ clust clusGap dataset kmeans 10 
B = 500 verbose = interactive gap _ clustClustering Gap 
statistic clusGap from call clusGap x = dataset FUNcluster = 
kmeans K . max = 10 B = 500 verbose 
= interactive B = 500 simulated reference sets k = 
1 . . 10 spaceH0 = scaledPCA Number of clusters 
method firstSEmax SE . factor = 1 3 logW E 
. logW gap SE . sim 1 5.377557 5.863690 0.4861333 
0.01273873 2 5.203502 5.758276 0.5547745 0.01420766 3 5.066921 5.697322 0.6304006 
0.01278909 4 5.023936 5.651618 0.6276814 0.01243239 5 4.993720 5.615174 0.6214536 
0.01251765 6 4.962933 5.584564 0.6216311 0.01165595 7 4.943241 5.556310 0.6130690 
0.01181831 8 4.915582 5.531834 0.6162518 0.01139207 9 4.881449 5.508514 0.6270646 
0.01169532 10 4.855837 5.487005 0.6311683 0 . 01198264library factoextra fviz 
_ gap _ stat gap _ clust 可以 看到 也是 
在 聚 类 数 为 3 的 时候 gap 值 
取 到了 最大值 所以 最佳 聚 类 数 为 39 
. 层次 聚 类 层次 聚 类 是 通过 可视化 
然后 人为 去 判断 大致 聚 为 几类 很 明显 
在 共同 父 节点 的 一颗 子树 可以 被 聚 
类 为 一个 类 h _ dist dist as . 
matrix dataset h _ clust hclust h _ dist plot 
h _ clust hang = 1 labels = FALSE rect 
. hclust h _ clust 3 10 . clustergram 最后 
一种 算法 是 Tal Galili 10 大牛 自己 定义 的 
一种 聚 类 可视化 的 展示 绘制 随着 聚 类 
数目 的 增加 所有 成员 是 如何 分配 到 各个 
类别 的 该 代码 没有 被 制作 成R包/nr 可以 去 
Galili 介绍 页面 里面 的 github 地址 找到 源代码 跑 
一遍 然后 就 可以 用 这个 函 数了 因为 源代码 
有点 长 我 就 不放 博客 里面 了 直接 放出 
运行 代码 的 截图 clustergram dataset k . range = 
2 8 line . width = 0.004 Loading required package 
colorspace Loading required package plyr 随着 K 的 增加 从最/nr 
开始 的 两类 到 最后 的 八类 图 肯定 是 
越到 后面 越 密集 通过 这个 图 判断 最佳 聚 
类 数目 的 方法 应该 是 看 随着 K 每 
增加 1 分 出来 的 线 越少 说明 在 该 
k 值 下 越 稳定 比如 k = 7 到 
k = 8 假设 k = 7 是 很好 的 
聚 类 数 那/r 分成/v 8类/mq 时/n 应该/v 可能/v 只是/c 
某/r 一类/m 分成/v 了/ul 两类/m 其他 6类 都每/nr 怎么 变 
反 应到 图中 应该是 有6簇/nr 平行线 有 一簇 分成 了 
两股 而 现在 可以 看到 从7到/nr 8 线 完全 乱了 
说明 k = 7时 效果 并 不好 按照 这个 分析 
k = 3 到 k = 4时 第一股 和 第三股 
几本 没变 就 第二股 拆 成了 2类 所以 k = 
3 是 最佳 聚 类 数目 方法 汇总 与 比较 
wine 数据集 我们 知道 其实 是 分为 3类 的 以上 
10种 判定 方法 中 层次 聚 类 和 clustergram 方法 
肘 点 图法 需要 人工 判定 虽然 可以 得出 大致 
的 最佳 聚 类 数 但 算法 本身 不会 给 
出 最佳 聚 类 数 除了 Affinity propagation AP clustering 
给出 最佳 聚 类 数 为 15 剩下 6种 全都 
是 给出 最佳 聚 类 数 为 3 选用 上次 
文本 挖掘 的 矩阵 进行 分析 667 * 1623 mclust 
效果 很差 14种/mq 模型/n 只有/c 6种/mq 有结果/i bclust/w 报错/v SSE/w 
可以/c 运行/v fpc/w 包/v 中的/i pamk/w 函数/n 聚/v 成/n 2类/mq 
明显 不行 Calinsky criterion 聚 成 2类 Affinity propagation AP 
clustering 聚 成 28类 相对 靠谱 轮廓 系数 Average silhouette 
聚 类 2类 gap Statistic 跑 不出 结果 可见 上述 
方法 中 有的 因为 数据 太大 不 能 运行 有的/nr 
结果 很 明显 不对 一个 可能 是 数据集 的 本身 
的 原因 缺失 值 太多 等 但是 也 告诉 了 
我们 在 确定 最佳 聚 类 数目 的 时候 需要 
多 尝试 几种 方法 并 没有 固定 的 套路 然后 
选择 一种 可信度 较高 的 聚 类 数目 最后 再 
把这 10 种方法 总结 一下 参考文献 1 R 语言 实战 
第二 版 2 Partitioning cluster analysis Quick start guide Unsupervised 
Machine Learning 3 BIC http / / www . stat 
. washington . edu / raftery / Research / PDF 
/ fraley1998 . pdf 4 Cluster analysis in R determine 
the optimal number of clusters 5 Calinski Harabasz Criterion Calinski 
Harabasz criterion clustering evaluation object 6 Determining the optimal number 
of clusters 3 must known methods Unsupervised Machine Learning 7 
  affinity propagation 聚 类 算法 Affinity Propagation AP 8 
轮廓 系数 https / / en . wikipedia . org 
/ wiki / Silhouette clustering 9 gap statistic Tibshirani R 
Walther G Hastie T . Estimating the number of clusters 
in a data set via the gap statistic J . 
Journal of the Royal Statistical Society Series B Statistical Methodology 
2001 63 2 411 423 . 10 C l u 
s t e r g r a m s C 
l u s t e r g r a m 
visualization and diagnostics for cluster analysis R code 