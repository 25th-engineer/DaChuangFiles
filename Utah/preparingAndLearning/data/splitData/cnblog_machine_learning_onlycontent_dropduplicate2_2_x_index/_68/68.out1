pre { margin top 0 max width 95% border 1px 
solid # ccc white space pre wrap } pre code 
{ display block padding 0 . 5em } code . 
r code . cpp { background color # F8F8F8 } 
table td th { border none } blockquote { color 
# 666666 margin 0 padding left 1em border left 0 
. 5em # EEE solid } hr { height 0px 
border bottom none border top width thin border top style 
dotted border top color # 999999 } @ media print 
{ * { background transparent important color black important filter 
none important ms filter none important } body { font 
size 12pt max width 100% } a a visited { 
text decoration underline } hr { visibility hidden page break 
before always } pre blockquote { padding right 1em page 
break inside avoid } tr img { page break inside 
avoid } img { max width 100% important } @ 
page left { margin 15mm 20mm 15mm 10mm } @ 
page right { margin 15mm 10mm 15mm 20mm } p 
h2 h3 { orphans 3 widows 3 } h2 h3 
{ page break after avoid } } pre . operator 
pre . paren { color rgb 104 118 135 } 
pre . literal { color rgb 88 72 246 } 
pre . number { color rgb 0 0 205 } 
pre . comment { color rgb 76 136 107 } 
pre . keyword { color rgb 0 0 255 } 
pre . identifier { color rgb 0 0 0 } 
pre . string { color rgb 3 106 7 } 
前言 近年来 Machine Learning 在 许多 领域 上 已然 取得 
了 可喜 的 成就 非常 火热 就 我 个人 来讲 
有意 将 业余 Sport Programming 的 范围 扩展 一下 譬如 
Topcoder Marathon 在 解决 实际 问题 中 方法 太 Naive 
往往 效果 不怎么样 依旧 需要 学习 一下 相关 的 基础 
知识 本 系列 文章 主要 基于 Coursera 的 Machine Learning 
我社 内部 Machine Learning 课 里 能说 的 一部分 wikipedia 
以及 一些 其他 的 读物 一些 概念 机器 学习 的 
定义 对于 某类 任务 T 和 性能 度量 P 如果 
一个 计算机 程序 在 T 上以 P 衡量 的 性能 
随着 经验 E 而 自我完善 那么 我们 称 这个 计算机程序 
从 经验 E 中 学习 这 是 一个 比较 严谨 
的 界定 机器学习 问题 的 Guideline 如果 有 什么 问题 
搞 不 清楚 是不是 这个 范畴 可以 尝试 套用 定义 
来 检查 任务 是 什么 性能 度量 是 什么 经验 
是 什么 性能 是否 由于 经验 而 提升 一些 机器学习 
应用 手写识别 Optical character recognition 文本 分类 识别 垃圾邮件 工口 
反动 内容 等 语音 识别 机器 翻译 等 图像识别 人脸识别 
识别 钓鱼 网站 机器人 无人机 等 一些 机器学习 问题 分类 
Classi ﬁ cation 给 每组 输入 打 一个 tag 譬如 
手写识别 实际上 相当于 对 一个 图像 进行 分类 回归 Regression 
对 每组 输入 预测 一个 实数值 譬如 预测 股市行情 Ranking 
将 输入 排序 譬 如对 搜索引擎 的 搜索 结果 推荐 
系统 等 聚 类 Clustering 将 输入 数据 分成 若干类 
降 维 Dimensionality reduction 寻找 输入 数据 的 低维 表示 
一些 定义 样例 Example 某个 实体 Features 实体 的 属性 
集合 通常用 向量 来 表示 Label 对于 分类 问题 就是 
样例 属于 哪 一类 对于 回归 问题 就是 实数值 训练 
集 Training Data 用来 训练 模型 Validation set 往往 用来 
调整 学习 的 参数 测试 集 Test Data 用来 评估 
模型 的 表现 监督 学习 从 给定 的 训练 数据 
集中 学习 出 一个 函数 当 新的 数据 到 来时 
可以 根据 这个 函数 预测 结果 非 监督 学习 训练 
集 没有人 为 标注 的 结果 区别 训练 数据 有 
没有 标注 机器学习 三要素 模型 策略 算法 模型 就是 所 
要 学习 条件 概率分布 或 决策函数 策略 按照 什么样 的 
准则 来 学习 或者 挑选 模型 算法 学习 模型 的 
具体 计算方法 即用 什么样 的 方法 来 求得 最优 解 
个人 理解 模型 代表 着 你 如何 看待 这个 问题 
譬如 识别 一个 东西 是 不是 汽车 如果 你 认为 
识别 的 依据 是 金属 壳 + 车灯 + 反光镜 
+ 车轮子 = 汽车 这个 思路 就 比较 接近 基于 
规则 决策树 贝叶斯 如果 你 考虑 这个 东西 和 见过 
的 什么 东西 比较 相似 就是 KNN 的 思路 之后 
我们 要 不断 学习 的 实际上 都是 模型 常见 的 
两个 策略 是 经验 风险 最小化 和 结构 风险 最小化 
经验 风险 最小化 意味着 我们 倾向于 对 训练 数据 取得 
精准 的 预测 这个 想法 很 直接 且 有 一定 
道理 模型 在 训练 数据 上 表现 不佳 更 无法 
指望 在 测试 数据 上 取得 好 结果 但是 在 
训练 集上 表现 好 的 模型 未必 在 测试 集上 
表现 好 通常 来讲 简单 的 模型 会 更有 通用性 
而 复杂 的 模型 往往会 有 一些 hardcode 了 训练 
数据 的 感觉 效果 反而 不 一定 好 结构 风险 
最小化 在 经验 风险 最小化 的 情况 下 加入 一些 
因 子来 限制 模型 的 复杂度 根据 策略 可以 列出 
一个 需要 最 优化 的 式子 算法 就是 求 这个 
式子 最优 或者 较优 解的/nr 方法 最 常见 的 方法 
是 梯度 下降 其他 技能 还 没有 get 就 暂不 
讨论 了 梯度 下 降法 Gradient Descent 暂时 忘记 机器学习 
现在 需要 优化 一个 形如 \ y = f \ 
theta \ 的 式子 求 \ x = argmax f 
\ theta \ 或 \ x = argmin f \ 
theta \ 有 什么 好 的 办法 么 梯度 下 
降法 基于 这样 的 观察 如果 实值函数 \ F x 
\ 在 点 a 处 可微 且有 定义 那么 函数 
\ F x \ 在 a 点 沿着 梯度 相反 
的 方向 \ F \ nabla a \ 下降 最快 
因此 如果 \ b = a \ gamma \ nabla 
F a \ 对于 \ \ gamma 0 \ 且为 
一个 够 小数 时 成立 那么 \ F a \ 
geq F b \ 换句话说 我们 给 出 一个 对 
极值 的 估计 a 不断 迭代 求 \ a = 
a \ gamma \ nabla F a \ 就能 取得 
一个 极值 用 一个 实际 例子 来 演示 一下 对 
二次函数 \ f y = x ^ { 2 } 
+ 2x + 10 \ 使用 梯度 下 降法 求 
\ min f x \ 和 \ argmin f x 
\ 函数 图像 如下 结论/n 无论是/c 从/p 图像/n 还是/c 初中/f 
数学/n 的/uj 角度/n 来看/u 都/d 很简单/i 我们 看看 梯度 下降 
算法 是 如何 进行 的 f function x { x 
^ 2 + 2 * x + 10 } df 
function x { 2 * x + 2 } x 
5 y f x learning . rate 0.3 plot f 
5 5 while TRUE { nx = x df x 
* learning . rate ny = f nx if abs 
x nx 0.01 break arrows x y nx ny col 
= red x = nx y = ny print c 
x ny } # # 1 1.40 14.76 # # 
1 0.040 9.922 # # 1 0.616 9.147 # # 
1 0.8464 9.0236 # # 1 0.9386 9.0038 # # 
1 0.9754 9.0006 # # 1 0.9902 9.0001 无论 是 
简单 问题 还是 复杂问题 参数 learning . rate 也 就是 
前 文中 提到 的 \ \ gamma \ 的 选择 
非常 重要 Learning rate 过小 则 需要 更多 的 迭代 
Learning rate 过大 则 会 出现 之 字 下降 甚至 
之 字 上升 看 一个 非 凸 多元 函数 的 
例子 Rosenbrock 函数 \ f x y = 1 x 
^ 2 + 100 y x ^ 2 ^ 2 
\ 很显然 x = y = 1 的 时候 可以 
取得 最优 解 但是 求解 过程 却是 很 坑 的 
咱们 把 x = y = 1 附近 的 图像 
画出来 再 研究 一下 x = 1 时的/nr 切面 大概 
能 看出来 这个/r 函数/n 在/p 解/v 附近/f 有个/nr 很大/a 的/uj 
很/zg 平的底/nr 贴 一段 代码 大家 可以 play 一下 f 
function x y { 1 x * * 2 + 
100 * y x * * 2 * * 2 
} df . dx function x y { x * 
2 2 400 * y + 400 * x * 
* 3 } df . dy function x y { 
200 * y 200 } x runif 1 0 2 
y runif 1 0 2 z = f x y 
learning . rate = 1E 6 eps 1E 10 while 
TRUE { new . x = x df . dx 
x y * learning . rate new . y = 
y df . dy x y * learning . rate 
new . z = f new . x new . 
y if abs new . z z eps break x 
= new . x y = new . y z 
= new . z print c x y z } 
可以 调整 一些 参数 譬如 learning . rate eps 去 
看看 某些 现象 我们 可以 看到 他 最后 几步 的 
收敛 极为 缓慢 如果 learning . rate 过大 还会 之 
字 上升 等等 总的来讲 选择 一个 合适 的 learning rate 
是 非常 重要 的 除去 经验性 的 技巧 往往 也 
只好 枚 举了 看看 cost function 的 变化 情况 如果 
下降 过慢 则 需要 增大 learning rate 如果 反而 增长 
了 则 需要 减少 learning rate 这 也 就是 为什么 
某些 时候 我们 需要 一个 比 较小 的 validate set 
我们 可以 定期 的 在 训练 中 的 模型 上 
跑 一下 validate set 看一下 cost function 的 变化 从而 
决定 learning rate 的 调整 一元 线性 回归 以 Stanford 
Machine Learning 为例 根据 房子 的 面积 预测 房价 咱们 
来 把 一些 概念 对上 号 Training Set m 个 
二元 对 \ x _ { i } y _ 
{ i } \ feature 房屋 面积 即 \ x 
_ { i } \ label 房价 即 \ y 
_ { i } \ 这是 监督 学习 因为 测试数据 
是 标注 过 的 这是 回归 问题 因为 label 是 
连续 值 模型 一元 线性 回归 这个 想法 很 显然 
策略 \ minimize J \ theta _ { 0 } 
\ theta _ { 1 } = \ sum _ 
{ i = 1 } ^ { m } \ 
theta _ { 0 } x _ { i } 
+ \ theta _ { 1 } y _ { 
i } ^ 2 \ 算法 注意 此 时 我们 
要 求解 的 是 \ \ theta _ { 0 
} \ theta _ { 1 } \ 而 \ 
x _ { i } y _ { i } 
\ 都是 已知 量 可以考虑 求 偏 导 然后 用 
梯度 下降 求解 这 就是 技能 范围 以内 的 东西 
了 因为 每次 迭代 用了 所有 的 Training Data 所以 
这个 做法 叫 Batch Gradient Descent 实际 应用 中 比较 
好 用 的 算法 是 Stochastic Gradient Descent Batch Gradient 
Descent 每次 迭代 对 \ \ sum _ { i 
= 1 } ^ { m } \ theta _ 
{ 0 } x _ { i } + \ 
theta _ { 1 } y _ { i } 
^ 2 \ 求导 相当于 \ \ theta _ { 
0 } = \ theta _ { 0 } 2 
\ alpha \ sum _ { i = 1 } 
^ { m } \ theta _ { 0 } 
x _ { i } + \ theta _ { 
1 } y _ { i } x _ { 
i } \ \ \ theta _ { 1 } 
= \ theta _ { 1 } 2 \ alpha 
\ sum _ { i = 1 } ^ { 
m } \ theta _ { 0 } x _ 
{ i } + \ theta _ { 1 } 
y _ { i } \ 而 Stochastic Gradient Descent 
相当 与 把 Batch Gradient Descent 的 1 次 迭代 
拆 成了 m 次 每次 对 \ \ theta _ 
{ 0 } x _ { i } + \ 
theta _ { 1 } y _ { i } 
^ 2 \ 求导 然后 \ \ theta _ { 
0 } = \ theta _ { 0 } 2 
\ alpha \ theta _ { 0 } x _ 
{ i } + \ theta _ { 1 } 
y _ { i } x _ { i } 
\ \ \ theta _ { 1 } = \ 
theta _ { 1 } 2 \ alpha \ theta 
_ { 0 } x _ { i } + 
\ theta _ { 1 } y _ { i 
} \ Batch Gradient Descent 可以 求得 更 精确 的 
解 但是 如果 模型 复杂 或者 数据 量大 就 很难 
直接 Batch Gradient Descent 了 