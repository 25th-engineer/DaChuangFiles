版权 声明 本文 为 博主 原创 文章 转载 请 注明 
出处 机器 学习 的 研究 领域 是 发明 计算机 算法 
把 数据 转变 为 智能 行为 机器 学习 和 数据 
挖掘 的 区别 可能 是 机器学习 侧重于 执行 一个 已知 
的 任务 而 数据 发掘 是 在 大 数据 中 
寻找 有 价值 的 东西 机器学习 一般 步骤 收集 数据 
将 数据 转化 为 适合 分析 的 电子 数据 探索 
和 准备 数据 机器学习 中 许多 时间 花费 在 数据 
探索 中 它 要 学习 更多 的 数据 信息 识别 
它们 的 微小 差异 基于 数据 训练 模型 根据 你 
要 学习 什么 的 设想 选择 你 要 使用 的 
一种 或 多种 算法 评价 模型 的 性能 需要 依据 
一定 的 检验 标准 改进 模型 的 性能 有时候 需要 
利用 更 高级 的 方法 有时候 需要 更换 模型 机器学习 
算法 有 监督 学习 算法 用于 分类 k 近邻 朴素 
贝叶斯 决策树 规则学习 神经网络 支持 向量 机 用于 数值 预测 
线性 回归 回归 树 模型 树 神经网络 支持 向量 机 
无 监督 学习 算法 用于 模式识别 数据 之间 联系 的 
紧密性 关联 规则 用于 聚 类 k 均值 聚 类 
R 语言 机器学习 算法 实现 kNN k Nearest Neighbors k 
近邻 原理 计算 距离 找到 测试数据 的 k 个 近邻 
根据 k 个 近邻 的 分类 预测 测试数据 的 分类 
应用 k 近邻 需要 将 各个 特征 转换 为 一个 
标准 的 范围 归一化 处理 可以 应用 min max 标准化 
所有 值 落在 0 ~ 1 范围 新 数据 = 
原 数据 最小值 / 最大值 最小值 也 可以 应用 z 
score 标准化 新 数据 = 原 数据 均值 / 标准差 
对于 名义 变量 表示 类别 可以 进行 哑 变量 编码 
其中 1 表示 一个 类别 0 表示 其它 类别 对于 
n 个 类别 的 名义 变量 可以 用 n 1个 
特征 进行 哑 变量 编码 比如 高 中 低 可以 
用 高 中 两类 的 哑 变量 表示 这 三类 
高 1 是 0 其它 中 1 是 0 其它 
优点 简单 且 有效 对 数据分布 没有 要求 训练 阶段 
很快 缺点 不 产生 模型 在 发现 特征 之间 的 
关系 上 的 能力 有限 分类 阶段 很慢 需要 大量 
的 内存 名义 变量 和 缺失 数据 需要 额外 处理 
R 代码 使用 class 包的/nr knn 函数 对于 测试数据 中的 
每 一个 实例 该 函数 使用 欧氏距离 标识 k 个 
近邻 然后 选出 k 个 近邻 中 大多数 所属 的 
那个 类 如果 票数 相等 测试 实例 会被 随机 分配 
dt _ pred knn train = dt _ train test 
= dt _ test class = dt _ train _ 
labels k = 3 # train 一个 包含 数值 型 
训练 数据 的 数据库 test 一个 包含 数值 型 测试数据 
的 数据 框 class 训练 数据 每 一行 分类 的 
一个 因子 变量 k 标识 最 近邻 数据 的 一个 
整数 通常 取 实 例数 的 平方根 该 函数 返回 
一个 向量 该 向量 含有 测试数据 框 中 每 一行 
的 预测 分类 尽管 kNN 是 并 没有 进行 任何 
学习 的 简单 算法 但是 却 能 处理 及 其 
复杂 的 任务 比如 识别 肿瘤细胞 的 肿块 对 R 
自带 iris 数据 用 kNN 进行 训练 预测 并 与 
实际 结果 对比 llibrary class library gmodels # prepare data 
set . seed 12345 # set random seed in order 
to repeat the result iris _ rand iris order runif 
150 iris _ z as . data . frame scale 
iris _ rand 5 # z score normalize train iris 
_ z 1 105 test iris _ z 106 150 
train . label iris _ rand 1 105 5 test 
. label iris _ rand 106 150 5 # kNN 
pred knn train test train . label k = 10 
# comfusion matrix CrossTable pred test . label prop . 
r = F prop . t = F prop . 
chisq = F 这个 结果 显示 kNN 对 测试数据 全部 
预测 正确 朴素 贝叶斯 分类 原理 基于 朴素 贝叶 斯定理 
根据 先验概率 计算 预测 实例 的 属于 不同 类别 的 
总 似 然 再将 某类 别的 似 然 除以 不同 
类别 似 然 的 和 得到 预测 实例 在某 类别 
的 概率 应用 朴素 贝叶斯 算法 每个 特征 必须 是 
分类 变量 对于 数值 型 变量 可以 将 数值 型 
特征 离散化 分段 可以 根据 直方图 查看 数据 明显 的 
分隔 点 如果 没有 明显 的 分隔 点 可以 使用 
三 分位数 四分位数 五 分位数 分段 太少 会把 重要 信息 
丢失 拉普拉斯 估计 对于 某些 从来 没有 出现 的 概率 
为 0 的 会 影响 概率 的 估计 拉普拉斯 估计 
本质上 是 在 概率 表 的 每个 计数 加上 一个 
较小 的 数 这样 保证 每 一类 中 每个 特征 
发生 的 概率 是 非零 的 优点 简单 快速 有效 
能/v 处理/v 噪声/n 数据/n 和/c 缺失/v 数据/n 需要用 来 训练 
的 例子 相对 较少 但 同样 能 处理 好 大量 
的 例子 很 容易 获得 一个 预测 的 估计 概率值 
缺点 依赖 于 一个 常用 的 错误 假设 即 一样 
的 重要性 和 独立 特征 应用 在 大量 数值 特征 
的 数据集 时 并不 理想 概率 的 估计值 相对于 预测 
的 类 而言 更加 不 可靠 R 代码 使用 维也纳 
理工大学 统计 系 开发 的 e1071 添加 包 中的 naiveBayesm 
naiveBayes train class laplace = 0 # train 数据 框 
或者 包含 训练 数据 的 矩阵 class 包含 训练 数据 
每 一行 的 分类 的 一个 因子 向量 laplace 控制 
拉普拉斯 估计 的 一个 数值 可以 进行 调节 看 是否 
会 提高 模型 性能 该 函数 返回 一个 朴素 贝叶斯 
模型 对象 该 对象 能够 用于 预测 p predict m 
test type = class # m 由 函数 naiveBays 训练 
的 一个 模型 test 数据 框 或者 包含 测试数据 的 
矩阵 包含 与 用来 建立 分类器 的 训练 数据 的 
相同 特征 type 值 为 class 或者 raw 标识 预测 
向量 最 可能 的 类别 值 或者 原始 预测 的 
概率值 library e1071 library gmodels set . seed 12345 # 
set random seed in order to repeat the result iris 
_ rand iris order runif 150 train iris _ rand 
1 105 5 test iris _ rand 106 150 5 
train . label iris _ rand 1 105 5 test 
. label iris _ rand 106 150 5 # tranform 
numerical variable to classified variable conver _ counts function x 
{ q quantile x sect1 which q 1 = x 
& x = q 2 sect2 which q 2 x 
& x = q 3 sect3 which q 3 x 
& x = q 4 sect4 which q 4 x 
& x = q 5 x sect1 1 x sect2 
2 x sect3 3 x sect4 4 return x } 
train apply train 2 conver _ counts # naiveBayes m 
naiveBayes train train . label laplace = 1 pred predict 
m test type = class # comfusion matrix CrossTable pred 
test . label prop . r = F prop . 
t = F prop . chisq = F 可见 对 
第一 类 setosa 分类上 预测 错误率 很高 这 可能 反映 
了 朴素 贝叶斯 算法 的 缺点 对于 处理 大量 数值 
特征 数据集 时 并不 理想 决策树 原理 以 树形 结构 
建立 模型 使用 一种 称为 递归 划分 的 探索 法 
这种方法 通常 称为 分而治之 因为 它 利用 特征 的 值 
将 数据 分解 为 具有 相似 类 的 较小 的 
子集 从 代表 整个 数据集 的 数据 结点 开始 该 
算法 选择 最 能 预测 目标 类 的 特征 然后 
这些 案例 将被 划分 到 这一 特征 的 不同 值 
的 组 中 这一 决定 形成 了 第一 组 树枝 
该 算法 继续 分而治之 其他 结点 每次 选择 最佳 的 
候选 特征 直到 达到 停止 的 标准 如果 一个 节点 
停止 它 可能 具有 下列 情况 节点 上 所有 几乎 
所有 的 案例 都 属于 同 一类 没有 剩余 的 
特征 来 分辩 案例 之间 的 区别 决策树 已经 达到 
了 预先 定义 的 大小 限制 C 5.0 算法 时最/nr 
知名 的 决策 树 算法 之一 单线程 版本 的 源代码 
是 公开 的 R 中有 编写 好 的 该 程序 
C 5.0 算法 已经成为 生成 决策树 的 行业 标准 因为 
它 适用 于 大多数 类型 的 问题 并且 可以 直接 
使用 与 其它 先进 的 机器学习 模型 神经 网络 和 
支持 向量 机 相比 一般 表现 的 几乎 一样 并且 
更 容易 理解 和 部署 选择 最佳 的 分割 需要 
确立 分割 的 标准 有 信息 增益 基尼系数 卡方 统计量 
和 增益 比 C 5.0 算法 使用 信息 增益 修剪 
决策树 如果 决策树 增长 过大 将会 使 决策 过于 具体 
模型 将 会 过度 拟合 训练 数据 解决 这个 问题 
的 一种 方法 是 一旦 达到 一定 数量 的 决策 
或者 决策 节点 仅 含有 少量 的 案例 我们 就 
停止 树 的 增长 这 叫做 提前 停止 法 或者 
预 剪枝 决策树 法 分为 预 剪枝 提前 规定 树 
的 大小 和后/nr 剪枝/n 一旦 树 生长 的 过大 就 
根据 节点 处 的 错误率 使用 修剪 准 则将 决策树 
减少 到 更 合适 的 大小 通常 比 预 剪枝 
更 有效 自适应 增强 算法 进行 许多 次 尝试 在 
决策树 中 是 建立 许多 决策树 然后 这些 决策树 通过 
投票 表决 的 方法 为 每个 案例 选择 最佳 的 
分类 优点 一个 适用 于 大多数 问题 的 通用 分类器 
高度 自动化 的 学习 过程 可以 处理 数值 型 数据 
名义 特征 以及 缺失 数据 只 使用 最 重要 的 
特征 可以 用于 只有 相对 较少 训练 案例 的 数据 
或者 有 相当 多 训练 案例 的 数据 没有 数学 
背景 也 可解释 一个 模型 的 结果 对于 比 较小 
的 树 比 其他 复杂 的 模型 更 有效 缺点 
决策树/n 模型/n 在/p 根据/p 具有/v 大量/n 水平/n 的/uj 特征/n 进行/v 
划分/v 时/n 往往/t 是/v 有偏的/nr 很容易 过度 拟合 或者 不能 
充分 拟合 模型 因为 依赖于 轴 平行 分割 所以在 对 
一些 关系 建立 模型 时会 有困难 训练 数据 中的 小 
变化 可能 导致 决策 逻辑 的 较大 的 变化 大 
的 决策树 可能 难以 理解 给出 的 决策 可能 看 
起来 违反 直觉 R 代码 使用 R 包 C50 的 
函数 C5 . 0m C 5.0 train class trials = 
1 costs = NULL # train 一个 包含 训练 数据 
的 数据 框 class 包含 训练 数据 每 一行 的 
分类 的 一个 因子 trials 为 一个 可选 数值 用于 
控制 自适应 增强 循环 的 次数 默认值 为 1 一般用 
10 因为 研究 标明 这 能 降低 关于 测试数据 大约 
25% 的 概率 costs 为 一个 可选 矩阵 用于 给出 
与 各种 类型 错误 相 对应 的 成本 和 混淆 
矩阵 稍微 不同 行 用来 表示 预测值 列 用来 表示 
实际 值 函数 返回 一个 C 5.0 模型 对象 该 
对象 能够 用于 预测 p predict m test type = 
class # m 有 函数 C 5.0 训练 的 一个 
模型 test 一个 包含 训练 数据 的 数据 框 该/r 
数据/n 框/v 和/c 用来/v 创建/v 分类/n 其/r 的/uj 数据/n 框/v 
有/v 同样/d 的/uj 特征/n type 取值 为 class 或者 prob 
表识 预测 是 最 可能 的 类别 值 或者 是 
原始 的 预测 概率 该 函数 返回 一个 向量 根据 
参数 type 的 取值 该 向量 含有 预测 的 类别 
值 或者 原始 预测 的 概率值 library C50 library gmodels 
set . seed 12345 # set random seed in order 
to repeat the result iris _ rand iris order runif 
150 train iris _ rand 1 105 5 test iris 
_ rand 106 150 5 train . label iris _ 
rand 1 105 5 test . label iris _ rand 
106 150 5 # C50 m C 5.0 train train 
. label trials = 10 pred predict m test type 
= class # comfusion matrix CrossTable pred test . label 
prop . r = F prop . t = F 
prop . chisq = F 规则学习 分类 原理 规则学习 算法 
使用 了 一种 称为 独立 而 治 之 的 探索 
法 这个 过程 包括 确定 训练 数据 中 覆盖 一个 
案例 子集 的 规则 然后 再从 剩余 的 数据 中 
分离出 该 分区 随着 规则 的 增加 更多 的 数据 
子集 会被 分离 直到 整个 数据集 都被 覆盖 不再 有 
案例 残留 独立 而 治 之和 决策树 的 分而治之 区别 
很小 决策树 的 每个 决策 节点 会 受到 过去 决策 
历史 的 影响 而 规则学习 不 存在 这样 的 沿袭 
随着 规则 的 增加 更多 的 数据 子集 会被 分离 
知道 整个 数据集 都被 覆盖 不再/d 有/v 案例/n 被/p 保留/v 
单/n 规则/n 1R 算法 ZeroR 一个 规则学习 算法 从 字面 
上 看 没有 规则学习 对于 一个 未 标记 的 案例 
不用 考虑 它 的 特征值 就 会把 它 预测 为 
最 常见 的 类 单 规则 算法 1R 或 OneR 
在 ZeroR 的 基础 上 添加 一个 规则 像 K 
近邻 一样 虽然 简单 但是 往往 表现 的 比 你 
预期 的 要好 优点 可以 生成 一个 单一 的 易于 
理解 的 人类 可读 的 经验法则 大拇指 法则 表现 往往 
出奇 的 好 可以 作为 更 复杂 算法 的 一个 
基准 缺点 只 使用 了 一个 单一 的 特征 可能会 
过于 简单 R 代码 使用 R 包 RWeka 中 OneR 
函数 来 实现 1R 算法 m OneR class ~ predictors 
data = mydata # class 是 mydata 数据 框 中 
需要 预测 的 那 一列 predictors 为 一个 公式 用来 
指定 mydata 数据 框 中 用来 进行 预测 的 特征 
data 为 包含 一个 class 和 predictors 所 要求 的 
数据 的 数据 框 该 函数 返回 一个 1R 模型 
对象 该 对象 能够 用于 预测 p predict m test 
# m 由 函数 OneR 训练 的 一个 模型 test 
一个 包含 测试数据 的 数据 框 该 数据 框 和 
用来 创建 分类器 的 训练 数据 有着 相同 的 特征 
该 函数 返回 一个 含有 预测 的 类别 的 向量 
library RWeka library gmodels set . seed 12345 # set 
random seed in order to repeat the result iris _ 
rand iris order runif 150 train iris _ rand 1 
105 test iris _ rand 106 150 5 test . 
label iris _ rand 106 150 5 m OneR Species 
~ . data = train pred predict m test CrossTable 
pred test . label prop . r = F prop 
. t = F prop . chisq = F 查看 
生成 的 规则 按照 Petal 的 宽度 分成 三类 正确 
分类 了 105个 里面 的 101个 对于 测试数据 的 混合 
矩阵 如下 可见 只 使用 了 一个 规则 也能 也 
做到 了 不错 的 效果 RIPPER 算法 对于 复杂 的 
任务 只 考虑 单个 规则 可能 过于 简单 考虑 多个 
因素 的 更 复杂 的 规则学习 算法 可能会 有用 但 
也 可能 因此 会 变得 更加 难以 理解 早期 的 
规则学习 算法 速度慢 并且 对于 噪声 数据 往往 不 准确 
后来 出现 增 量减少 误差 修剪 算法 IREP 使用/v 了/ul 
生成/v 复杂/a 规则/n 的/uj 预/v 剪枝/n 和后/nr 剪枝/n 方法/n 的/uj 
组合/v 并在 案例 从 全部 数据集 分离 之前 进行 修剪 
虽然 这 提高 了 性能 但是 还是 决策树 表现 的 
更好 直到 1995年 出现 了 重复 增量 修剪 算法 RIPPER 
它 对 IREP 算法 进行 改进 后再 生成 规则 它 
的 性能 与 决策树 相当 甚至 超过 决策树 原理 可以 
笼统 的 理解 为 一个 三步 过程 生长 修剪 优化 
生长 过程 利用 独立 而 治 之 技术 对 规则 
贪婪 地 添加 条件 直到 该 规则 能 完全 划分 
出 一个 数据 子集 或者 没有 属性 用于 分割 与 
决策树 类似 信息 增益 准则 可 用于 确定 下 一个 
分割 的 属性 当 增加 一个 特指 的 规则 而 
熵值 不再 减 少时 该 规则 需要 立即 修剪 重复 
第一步 和 第二 步 直到 达到 一个 停止 准则 然后 
使用 各种 探索 法对/nr 整套 的 规则 进行 优化 优点 
生成 易于 理解 的 人类 可读 的 规则 对 大 
数据集 和 噪声 数据 有效 通常 比 决策树 产生 的 
模型 更简单 缺点 可能 会 导致 违反常理 或 这 专家 
知识 的 规则 处理 数值 型 数据 可能 不 太 
理想 性能 有 可能 不如 复杂 的 模型 R 代码 
使用 R 包 RWeka 中 JRip 函数 是 基于 Java 
实现 的 RIPPER 规则学习 算法 m JRip class ~ predictors 
data = my data # class 是 mydata 数据 框 
中 需要 预测 的 那 一列 predictors 为 一个 R 
公式 用来 指定 mydata 数据 框 中 用来 进行 预测 
的 特征 data 为 包含 class 和 predictors 所 要求 
的 数据 的 数据 框 该 函数 返回 一个 RIPPER 
模型 对象 该 对象 能够 用于 预测 p predict m 
test # m 由 函数 JRip 训练 的 一个 模型 
test 一个 包含 测试数据 的 数据 框 该/r 数据/n 框/v 
和/c 用来/v 创建/v 分类器/n 的/uj 训练/vn 数据/n 有/v 同样/d 的/uj 
特征/n 该 函数 返回 一个 含有 预测 的 类别 值 
的 向量 library RWeka library gmodels set . seed 12345 
# set random seed in order to repeat the result 
iris _ rand iris order runif 150 train iris _ 
rand 1 105 test iris _ rand 106 150 5 
test . label iris _ rand 106 150 5 m 
JRip Species ~ . data = train pred predict m 
test CrossTable pred test . label prop . r = 
F prop . t = F prop . chisq = 
F 这次 使用 了 三个 规则 Petal . Width = 
1.8 为 virginica Petal . Length = 3 为 versicolor 
其它 为 setosa 可见 虽然 增加 了 规则 但是 并 
没有 提高 模型 的 性能 预测 数值 型 数据 线性 
回归 回归 主要 关注 一个 唯一 的 因变量 需要 预测 
的 值 和 一个 或 多个 数值 型 自变量 之间 
的 关系 如果 只有 一个 自变量 称为 一元 线性 回归 
或者 简单 线性 回归 否则 称为 多元回归 原理 对 线性 
参数 的 估计 使用 最小二乘 估计 广义 线性 回归 它们 
对 线性 模型 进行 了 两 方面 的 推广 通过 
设定 一个 连接 函数 将 响应 变量 的 期望 与 
线性 变量 相联系 以及对 误差 的 分布 给出 一个 误差函数 
这些 推广 允许 许多 线性 的 方法 能够 被 用于 
一般 的 问题 比如 逻辑 回归 可以 用来 对 二元 
分类 的 结果 建模 而 泊松 回归 可以 对 整型 
的 计数 数据 进行 建模 优点 迄今为止 它 是 数值 
型 数据 建模 最 常用 的 方法 可 适用 于 
几乎 所有 的 数据 提供 了 特征 变量 之间 关系 
的 强度 和 大小 的 估计 缺点 对 数据 作出 
了 很强 的 假设 该 模型 的 形式 必须 由 
使用者 事先 指定 不能 很好 地 处理 缺失 数据 只能 
处理 数值 特征 所以 分类 数据 需要 额外 的 处理 
需要 一些 统计 学 知识 来 理解 模型 LASSO 回归 
算法 LASSO 回归 的 特点 是 在 拟合 广义 线性 
模型 的 同时 进行 变量 筛选 只 选择 对 因变量 
有 显著 影响 的 自变量 和 复杂度 调整 通过 参数 
控制 模型 复杂度 避免 过度 拟合 它 通过 惩罚 最大 
似 然 来 拟合 广义 线性 模型 正则化 路径 是 
通过 正则化 参数 lambda 的 值 的 网格 上 计算 
lasso 或者 弹性 网络 惩戒 lambda 越大 对 变量 较多 
的 线性 模型 的 惩罚 力度 就 越大 从而 最终 
获得 一个 变量 较少 的 模型 R 代码 使用 R 
包 glmnet 中 glmnet 函数 拟合 LASSO 回归模型 glmnet x 
y family = c gaussian binomial poisson multinomial cox mgaussian 
weights offset = NULL alpha = 1 nlambda = 100 
lambda . min . ratio = ifelse nobs nvars 0.01 
0.0001 lambda = NULL standardize = TRUE intercept = TRUE 
thresh = 1e 07 dfmax = nvars + 1 pmax 
= min dfmax * 2 + 20 nvars exclude penalty 
. factor = rep 1 nvars lower . limits = 
Inf upper . limits = Inf maxit = 100000 type 
. gaussian = ifelse nvars 500 covariance naive type . 
logistic = c Newton modified . Newton standardize . response 
= FALSE type . multinomial = c ungrouped grouped x 
输入 矩阵 每 列 表示 变量 特征 每行 表示 一个 
观察 向量 也 支持 输入 稀疏 矩阵 Matrix 中的 稀疏 
矩阵 类 y 反应变量 对于 gaussian 或者 poisson 分布 族 
是 相应 的 量 对于 binomial 分布 族 要求 是 
两 水平 的 因子 或者 两列 的 矩阵 第一列 是 
计数 或者 是 比例 第二列 是 靶向 分类 对于 因子 
来说 最后 的 水平 是 按照 字母表 排序 的 分类 
对于 multinomial 分布 族 能有 超过 两 水平 的 因子 
无论 binomial 或者 是 multinomial 如果 y 是 向量 的话 
会 强制 转化 为 因子 对于 cox 分布 族 y 
要求 是 两列 分别 是 time 和 status 后者 是 
二进 制变 两 1 表示 死亡 0 表示 截尾 survival 
包带 的 Surv 函数 可以 产生 这样 的 矩阵 对于 
mgaussian 分布 族 y 是 量化 的 反应 变量 的 
矩阵 family 反应类型 参数 family 规定 了 回归 模型 的 
类型 family = gaussian 适用于 一维 连续 因变量 univariate family 
= mgaussian 适用于 多维 连续 因变量 multivariate family = poisson 
适用于 非 负 次数 因变量 count family = binomial 适用于 
二元 离散 因变量 binary family = multinomial 适用于 多元 离散 
因变量 category weights 权重 观察 的 权重 如果 反应变量 是 
比例 矩阵 的话 权重 是 总 计数 默认 每个 观察 
权重 都是 1 offset 包含 在 线性 预测 中的 和 
观察 向量 同样 长度 的 向量 在 poisson 分布 族 
中 使用 比如 log 后的/nr 暴露 时间 或者 是 对于 
已经 拟合 的 模型 的 重新 定义 将 旧 模型 
的 因变量 作为 向量 放入 offset 中 默认 是 NULL 
如果 提供 了 值 该 值 也 必须 提供 给 
predict 函数 alpha 弹性 网络 混合 参数 0 = a 
= 1 惩罚 定义 为 1 α / 2 | 
| β | | _ 2 ^ 2 + α 
| | β | | _ 1 . 其中 alpha 
等于 1 是 lasso 惩罚 alpha 等于 0 是 ridge 
岭回归 的 惩罚 nlambda lambda 值 个数 拟 合出 n 
个 系数 不同 的 模型 lambda . min . ratio 
lambda 的 最小值 lambda . max 的 比例 形式 比如 
全部 系数 都是 0 的 时候 的 最小值 默认值 依赖于 
观察 的 个数 和 特征 的 个数 如果 观察 个数 
大于 特征 个数 默认值 是 0.0001 接近 0 如果 观察 
个数 小于 特征 个数 默认值 是 0.01 在 观察 值 
个数 小于 特征 个数 的 情况 下 非常 小 的 
lambda . min . ratio 会 导致 过拟合 在 binominal 
和 multinomial 分布 族 性 这个 值 未定义 如果 解释 
变异 百分比 总是 1 的话 程序 会 自动 退出 lambda 
用户 提供 的 lambda 序列 一个 典型 的 用法 基于 
nlambada 和 lambda . min . ratio 来 计算 自身 
lambda 序列 如果 提供 lambda 序列 提供 的 lambda 序列 
会 覆盖 这个 需 谨慎 使用 不要 提供 单个 值 
给 lambda 对于 CV 步骤 后的/nr 预测 应 使用 predict 
函数 替代 glmnet 依赖于 缓慢 开始 并且 它 用于 拟合 
全 路径 比 计算 单个 拟合 更快 standardize 对于 x 
变量 是否 标准化 的 逻辑 标志 倾向于 拟合 模型 序列 
系数 总是 在 原有 规模 返回 默认 standardize = TRUE 
如果 变量 已经 是 同一 单位 你 可能 并 不能 
得到 想要 的 标准化 结果 intercept 是否 拟合 截距 默认 
TRUE 或者 设置 为 0 FALSE thresh 坐标 下降 的 
收敛 域 值 每个 内部 坐标 下降 一直 进行 循环 
直到 系数 更新 后的/nr 最大 改 变值 比 thresh 值乘 
以 默认 变异 要 小 默认 thresh 为 1E 7 
dfmax 在 模型 中 的 最大 变量 数 对于 大量 
的 变量 数 的 模型 但 我们 只 需要 部分 
变量 时 可以 起到 作用 pmax 限制 非零 变量 的 
最大 数目 exclude 要从 模型 中 排除 的 变量 的 
索引 等同 于 一个 无限 的 惩罚 因子 penalty . 
factor 惩罚 因子 分开 的 惩罚 因子 能够 应用 到 
每一个 系数 这 是 一个 数字 乘以 lambda 来 允许 
不同 的 收缩 对于 一些 变量 来说 可以 是 0 
意味着 无 收缩 默认 对 全部 变量 是 1 对于 
列 在 exlude 里面 的 变量 是 无限大 注意 惩罚 
因子 是 内部 对 nvars n 个 变量 的 和 
进行 重新 调整 并且 lambda 序列 将 会 影响 这个 
改变 lower . limits 对于 每个 系数 的 更低 限制 
的 向量 默认 是 无穷小 向量 的 每个 值 须非/nr 
正值 也 可以 以 单个 值 呈现 将会 重复 或者 
是 nvars 长度 upper . limit 对于 每个 系数 的 
更高 限制 的 向量 默认 是 无穷大 maxit 所有 lambda 
值 的 数据 最大 传递 数 type . gaussian 支持 
高斯分布 族 的 两种 算法 类型 默认 nvar 500 使用 
covariance 并且 保留 所有 内部 计算 的 结果 这种 方式 
比 naive 快 naive 通过 对 nobs n 个 观察 
进行 循环 每次 内部 计算 一个 结果 对于 nvar nobs 
或者 nvar 500 的 情况 下 后者 往往 更 高效 
type . logistic 如果 是 Newton 会 使用 准确 的 
hessian 矩阵 默认 当用 的 是 modified . Newton 时 
只 使用 hession 矩阵 的 上界 会 更快 standardize . 
response 这个 参数 时 对于 mgaussian 分布 族 来说 的 
允许 用户 标准化 应答 变量 type . multinomial 如果 是 
grouped 在 多项式 系数 的 变量 使用 分布 lasso 惩罚 
这样 能 确保 它们 完全 在 一起 默认 是 ungrouped 
glmnet 返回 3类 glmnet * * 可以 是 elnet lognet 
multnet fishnet poisson merlnetcall 产生 这个 对象 的 调用 a0 
截距 beta 对于 elnet lognet fishnet 和 coxnet 模型 返回 
稀疏 矩阵 格式 的 系数 矩阵 CsparseMatrix 对于 multnet 和 
mgaussian 模型 返回 列表 包括 每 一类 的 矩阵 lambda 
使用 的 lambda 值 的 实际 序列 当 alpha = 
0时 最大 的 lambda 值 并不 单单 等于 0 系数 
原则上 labda 等于 无穷大 相反 使用 alpha = 0.01 的 
lambda 由此 导出 lambda 值 dev . ratio 表示 由 
模型 解释 的 变异 的 百分比 对于 elnet 使用 R 
sqare 如果 存在 权重 变异 计算 会 加入 权重 变异 
定义 为 2x loglike _ sat loglike loglike _ sat 
是 饱和 模型 每个 观察 值 具有 自由 参数 的 
模型 的 log 似 然 因此 dev . ratio = 
1 dev / nulldev 越 接近 1 说明 模型 的 
表现 越好 nulldev NULL 变异 每个 观察 值 这个 定义 
为 2 * loglike _ sat loglike Null NULL 模型 
是 指 截距 模型 除了 Cox 0 模型 df 对于 
每个 lambda 的 非零 系数 的 数量 对于 multnet 这是 
对于 一些 类 的 变量 数目 dfmat 仅/d 适用/v 于/p 
multnet/w 和/c mrelnet/w 一个 包括 每 一类 的 非零 向量 
数目 的 矩阵 dim 系数 矩阵 的 维度 nobs 观察 
的 数量 npasses 全部 lambda 值 加 和的/nr 数据 的 
总的 通量 offset 逻辑 变量 显示 模型 中 是否 包含 
偏移 jerr 错误 标记 用来 警告 和 报错 很大 部分 
用于 内部 调 试验 而 直接 显示 的 结果 有 
三列 分别 是 df % Dev 就是 dev . ratio 
lambda 是 每个 模型 对应 的 λ 值 predict object 
newx s = NULL type = c link reponse coefficients 
nonzero class exact = FALSE offset . . . coef 
object s = NULL exact = FALSE object glmnet 返回 
的 对象 newx 用来 预测 的 矩阵 也 可以 是 
系数 矩阵 这个 参数 不能 用于 type = c coefficents 
nonzero s 惩罚 参数 lambda 的 值 默认 是 用来 
创建 模型 的 全部 lambda 值 type 预测值 的 类型 
link 类型 给 binomial multinomial poisson 或者 cov 模型 线性 
预测 的 值 对于 gaussian 模型 给 拟合 值 response 
类型 对于 binominal 和 multinomial 给 拟合 的 概率 对于 
poisson 给 拟合 的 均值 对于 cox 给 拟合 的 
相对 未及 对于 gaussion response 等同于 link 类型 coefficients 类型 
对 于 需求 的 s 值 计算 系数 注意 对于 
binomial 模型 来说 结果 仅 仅对 因子 应答 的 第二 
个 水平 的 类 返回 class 类型 仅仅 应用于 binomial 
和 multinomial 模型 返回 最大 可能性 的 分类 标签 nonzero 
类型 对 每个 s 中的 值 返回 一个 列表 其中 
包含 非 0 参数 的 索引 exact 这个 参数 仅仅 
对于 用于 预测 的 s lambda 值 不同于 原始 模型 
的 拟合 的 值 时 这个 参数 起到 作用 如果 
exact = FALSE 默认 预测 函数 使用 线性 解释 来 
对 给 的 s lambda 值 进行 预测 这时 一个 
非常 接近 的 结果 只是 稍微 有点 粗糙 如果 exact 
= TRUE 这些/r 不同/a 的/uj s/w 值/n 和/c 拟合/v 对象/n 
的/uj lambda/w 值/n 进行/v sorted/w 和/c merged/w 在 作出 预测 
之前 进行 模型 的 重新 拟合 在 这种 情况 下 
强烈建议 提供 原始 的 数据 x = 和y=/nr 作为 额外 
的 命名 参 数给 perdict 或者 coef predict . glmnet 
需要 升级 模型 并且 期望 用于 创建 接近 它 的 
数据 尽管 不 提供 这些 额外 的 参数 它 也会 
运行 的 很好 在 调用函数 中 使用 嵌套 序列 很 
可能 会 中断 offset 如果 使用 offset 参 数来 拟合 
必须 提供 一个 offset 参数 来作 预测 除了 类型 coefficients 
或者 nonzero . . . 可以 提供 参数 其它 参数 
的 机制 比如 x = when exact = TRUE seeexact 
参数 library glmnet library psych # dummy variable encoding iris 
$ issetosa ifelse iris $ Species = = setosa 1 
0 iris $ isversicolor ifelse iris $ Species = = 
versicolor 1 0 iris _ dt iris 5 pairs . 
panels iris _ dt # scatterplot matrixpairs . panel 画出 
散点图 矩阵 对角线 上方 显示 的 是 变量 之间 的 
相关 系数 每个 散点图 中 呈 椭圆形 的 对象 称为 
相关 椭圆 它 提供 一种 变量 之间 是 如何 密切 
相关 的 可视化 信息 位于 椭圆 中间 的 的 点 
表示 x 轴 变量 和y轴/nr 变量 的 均值 所 确定 
的 点 两个 变量 之间 的 相关性 由 椭圆 的 
形状 表示 椭圆 越被/nr 拉伸 其 相关性 就 越强 散点图 
中 绘制 的 曲线 称为 局部 回归 平滑 它 表示 
x 轴 和y轴/nr 变量 之间 的 一般 关系 iris/w 数据/n 
画出/i 的/uj 散点图/v 矩阵/n 中/f 的/uj 相关/v 系数/n 和/c 散点图/v 
曲线/n 都/d 可见/v Petal/w ./i Length/w 和/c Petal/w ./i Width/w 
有着/v 强的/nr 相关性/l 而从 散点图 曲线 也 可 看出 似乎 
Sepal . Length 超出 一定 阈值 后 Sepal . Length 
增加 Petal . Length 也 增加 并且 也 和 品种 
是 setosa 或者 versicolor 也 有关系 以 Petal . Width 
作为 因变量 作 线性 回归 library glmnet # dummy variable 
encoding iris $ issetosa ifelse iris $ Species = = 
setosa 1 0 iris $ isversicolor ifelse iris $ Species 
= = versicolor 1 0 # divided into training sets 
and test sets set . seed 12345 # set random 
seed in order to repeat the result iris _ rand 
iris order runif 150 train iris _ rand 1 105 
c 4 5 test iris _ rand 106 150 c 
4 5 train _ value iris _ rand 1 105 
4 test _ value iris _ rand 106 150 4 
# lasso m _ lasso glmnet as . matrix train 
train _ value family = gaussian plot data . frame 
df = m _ lasso $ df dev . ratio 
= m _ lasso $ dev . ratio type = 
b cex = 0.6 coef m _ lasso s = 
0.0497000 # min df 查看 变量 个数 与 模型 解释 
变异 百分比 的 点 图 发现 在 df = 1时 
已经 开始 平缓 已经 可以 解释 93% 的 变异 因此 
取 df = 1 的 可以 解释 最大 变异 的 
lambda 0.0452800 查看 系数 发现 使用 了 两个 特征 其中 
一个 系数 非常低 并 不是 我们 需要 的 因此 lambda 
改为 第二个 解释 最大 变异 的 lambda 0.0497000 . 用 
coef 取出 参数 如下 lambda = 0 . 0452800lambda = 
0.0497000 用 选出 的 lambda 值 进行 预测 pred predict 
m _ lasso newx = as . matrix test s 
= 0.0497000 summary pred summary test _ value cor test 
_ value pred MAE mean abs pred test _ value 
mean abs mean train _ value test _ value 发现/v 
预测值/n 和/c 真实/d 值/n 范围/n 非常/d 接近/v 相关/v 系数/n 高/a 
MAE/w 平均 绝对误差 反映 预测值 和 真实 值 的 差距 
仅为 0.1981803 如果 只是 拿 训练 集 的 均值 预测 
的话 MAE 高达 0.6551746 综合 以上 的 度量 标准 说明 
我们 的 模型 预测 的 不错 回归 树 和 模型 
树 决策树 用于 数值 预测 分为 两类 第一类 称为 回归 
树 第二类 称为 模型 树 回归 树 作为 分类 回归 
树 的 一部分 引入 回归 树 并 没有 使用 线性 
回归 的 办法 而是 基于 到达 节点 的 案例 的 
平均值 进行 预测 模型 树 比 回归 树 晚 几年 
引入 但是 或许 功能 更加 强大 模型 树 和 回归 
树 以 大致 相同 的 方式 生长 但是 在 每个 
叶 节点 根据 到达 该 节点 的 案例 建立 多元 
线性 回归模型 根据 叶 节点 的 数目 一棵 模型 树 
可能 会 建立 几十 个 甚至 几百 个 这样 的 
模型 这 可能会 使 模型 树 更加 难以 理解 但 
好处 是 它们 也许 能 建立 一个 更加 精确 的 
模型 优点 将 决策树 的 优点 与 数值 型 数据 
建立 模型 的 能力 相 结合 能 自动 选择 特征 
允许 该 方法 和 大量 特征 一起 使用 不 需要 
使用者 事先 指定 模型 拟合 某些 类型 的 数据 可能 
会比 线性 回归 好得多 不 要求 用 统计 的 知识 
来 解释 模型 缺点 不像 线性 回归 那样 常用 需要 
大量 的 训练 数据 难以确定 单个 特征 对于 结果 的 
总体 净 影响 可能 比 回归模型 更难 解释 原理 用于 
数值 预测 的 决策树 的 建立 方式 与 用于 分类 
的 决策树 的 建立 方式 大致相同 从根/nr 节点 开始 按照 
特征 使用 分而治之 的 策略 对 数据 进行 划分 在 
进行 一次 分割 后 将 会 导致 数据 最大化 的 
均匀 增长 而在 分类 决策树 中 一致性 均匀性 是由 熵值 
来 度量 的 而 对于 数值 型 的 数据 是 
未定义 的 对于 数值 型 决策树 一致性 可以 通过 统计量 
比如 方差 标准差 或者 平均 绝对 偏差 来 度量 不同 
的 决策树 生长 算法 一致性 度量 可能 会 有所不同 但 
原理 是 基本 相同 的 一种 常见 的 分割 标准 
是 标准偏差 减少 就是/d 原始/v 值/n 的/uj 标准差/n 减去/v 分割/v 
后不/nr 同类/n 的/uj 数据/n 加权/v 后的/nr 标准差/n 这里 的 加权 
就是 该类 的 数目 比上 总的 数目 决策树 停止 生长 
后 假如 一个 案例 使用 特征 B 进行 分割 落入 
某 一组 B 1中 那么 该 案例 的 预测 值 
将 取 B 1组 的 平均值 模型 树 要多 走 
一步 使用 落入 B 1组 的 训练 案例 和 落入 
B 2组 的 训练 案例 建立 一个 相对 于 其它 
特征 特征 A 的 线性 回归模型 R 代码 在 R 
包 rpart 递归 划分 中 提供 了 像 CART 分类 
回归 树 团队 中 所 描述 的 最 可靠 的 
回归 树 的 实现 m . rpart rpart dv ~ 
iv data = mydata # dv 是 mydata 数据 框 
中 需要 建模 的 因变量 iv 为 一个 R 公式 
用来 指定 mydata 数据 框 中的 自变量 data 为 包含 
变量 dv 和 变量 iv 的 数据 框 p predict 
m test type = c vector prob class matrix # 
m 是 有 函数 rpart 训练 的 一个 模型 test 
一个 包含 测试数据 的 数据 框 该 数据 框 和 
用来 建立 模型 的 数据 具有 相同 的 特征 type 
给定 返回 的 预测 值 的 类型 prob 返回 预测 
的 概率 matrix 返回 矩阵 的 形式 包括 各类 的 
概率 class 返回 树 的 分类 否则 返回 一个 向量 
的 结果 可以 使用 R 包 rpart . plot 中 
rpart . plot 函数 对 回归 树 结果 可视化 目前 
模型 树 中最 先进 的 算法 是 M5 算法 可以 
通过 R 包 Rweka 中 M5P 函数 实现 m M5P 
dv ~ iv data = mydata # dv 是 mydata 
数据 框 中 需要 建模 的 因变量 iv 为 一个 
R 公式 用来 指定 mydata 数据 框 中的 自变量 data 
为 包含 变量 dv 和 变量 iv 的 数据 框 
p predict m test # m 是 有 函数 rpart 
训练 的 一个 模型 test 一个 包含 测试数据 的 数据 
框 该 数据 框 和 用来 建立 模型 的 数据 
具有 相同 的 特征 library rpart library RWeka library rpart 
. plot # dummy variable encoding iris $ issetosa ifelse 
iris $ Species = = setosa 1 0 iris $ 
isversicolor ifelse iris $ Species = = versicolor 1 0 
# divided into training sets and test sets set . 
seed 12345 # set random seed in order to repeat 
the result iris _ rand iris order runif 150 train 
_ dt iris _ rand 1 105 5 test iris 
_ rand 106 150 c 4 5 test _ value 
iris _ rand 106 150 4 # rpart m . 
rpart rpart Petal . Width ~ Sepal . Length + 
Sepal . Width + Petal . Length + issetosa + 
isversicolor data = train _ dt summary m . rpart 
rpart . plot m . rpart pred predict m . 
rpart test cor test _ value pred mean abs pred 
test _ value # rpart MAE mean abs mean train 
_ dt $ Petal . Width test _ value # 
mean MAE # M5P m . M5P M5P Petal . 
Width ~ Sepal . Length + Sepal . Width + 
Petal . Length + issetosa + isversicolor data = train 
_ dt summary m . M5P pred predict m . 
M5P test cor test _ value pred mean abs pred 
test _ value # rpart MAE mean abs mean train 
_ dt $ Petal . Width test _ value # 
mean MAE 回归 树 的 结果 如下 rpart . plot 
结果 相关性 到达 0.9797762 回归 树 MAF0 . 1242998 明显 
比 直接 用 均值 预测 MAF0 . 7255238 更 接近 
于 真实 的 Petal . Width 模型 树 的 结果 
如下 相关系数 到达 0.9714331 MAF0 . 1410668 在 这个 模型 
树 中 只有 一个 根 节点 相应 建立 了 一个 
线性 模型 直接 用 Sepal . Length Sepal . Width 
Petal . Length 三个 特征 进行 预测 和 lasso 回归模型 
一样 特征 前面 的 系数 代表 该 特征 对 Petal 
. Width 的 静 影响 注意 这里 的 净 影响 
是 指在 当前 节点 这个 线性 模型 中的 净 影响 
在 这个 线性 模型 中 每 增加 一点 Sepal . 
Width 和 Petal . Length Petal . Width 都会 增加 
而 系数 小于 0 的 Sepal . Length 意味着 每 
增加 一点 Sepal . Length Petal . Width 就会 减少 
从 结果 可以 看出 在 这个 案例 中 模型 树 
没有 回归 树 的 效果 好 此处 模型 树 在 
没有 生成 多个 树节点 的 情况 下 只是 对 特征 
做了 线性 回归 MAF 达到 0.1410668 和 之前 对 数据 
作 线性 回归 的 lasso 模型 结果 MAF0 . 1981803 
相比 貌似 做 的 更好 但 其实 之前 的 lasso 
回归模型 我们 限制 了 特征值 个数 来 避免 过拟合 如果 
增加 特征值 数量 和 调整 labda 参数 一样 可以 达到 
比 较小 的 MAF 小结 本文 主要 讲 了 机器 
学习 的 一些 基本 概念 还有 部分 机器学习 方法 的 
基本 原理 及 R 语言 实现 包括 用于 分类 的 
机器 学习 方法 k 近邻 朴素 贝叶斯 决策树 规则学习 用于 
数值 预测 的 机器 学习 方法 lasso 回归 回归 树 
模型 树 它们 都 属于 监督 学习 下 篇 文章 
会 说到 监督 学习 中 的 神经 网络 和 支持 
向量 机 还有 其他 非 监督 学习 的 一些 方法 
本文 可以 作为 一个 速查 和 简单 的 入门 一些 
函 数只 列举 了 部分 重要 的 参数 具体 的 
使用 参数 可以 通过 查看 R 里面 的 帮助 获得 
另外 如果 要 用于 实践 还 需要 了解 一些 K 
折 交叉 检验 kappa 统计量 ROC 曲线 内容 以对 模型 
的 性能 进行 评价 和对/nr 不同 的 模型 进行 对比 
参考资料 Brett Lantz 机器学习 与 R 语言 薛毅 陈立 萍 
统计 建模 与 R 软件 下册 侯澄钧/nr 热门 数据挖掘 模型 
应用 入门 一 LASSO https / / cosx . org 
/ 2016/10 / data mining 1 lassoslade _ sha 的 
博客 Lasso 算法 理论 介绍 http / / blog . 
csdn . net / slade _ sha / article / 
details / 53164905 