1 . LDALDA 是 一种 三层 贝叶斯 模型 三层 分别为 
文档 层 主题 层 和词层/nr 该 模型 基于 如下 假设 
1 整个 文档 集合 中 存在 k 个 互相 独立 
的 主题 2 每 一个 主题 是 词 上 的 
多项 分布 3 每一个 文档 由 k 个 主题 随机 
混合 组成 4 每一个 文档 是 k 个 主题 上 
的 多项 分布 5 每一个 文档 的 主题 概率分布 的 
先验 分布 是 Dirichlet 分布 6 每 一个 主题 中词 
的 概率 分布 的 先验 分布 是 Dirichlet 分布 文档 
的 生成 过程 如下 1 对于 文档 集合 M 从 
参数 为 β 的 Dirichlet 分布 中 采样 topic 生成 
word 的 分布 参数 φ 2 对于 每个 M 中的 
文档 m 从 参数 为 α 的 Dirichlet 分布 中 
采样 doc 对 topic 的 分布 参数 θ 3 对于 
文档 m 中的 第 n 个 词语 W _ mn 
先 按照 θ 分布 采样 文档 m 的 一个 隐含 
的 主题 Z _ m 再 按照 φ 分布 采样 
主题 Z _ m 的 一个 词语 W _ mn 
因此 整个 模型 的 联合 分布 如下 对 联合 分布 
求积分 去掉 部分 隐 变量 后 用 间接 计算 转移 
概率 可以 消除 中间 参数 θ 和φ/nr 所以 主题 的 
转移 概率 化为 这样 我们 就 可以 通过 吉布斯 采样 
来 进行 每 轮 的 迭代 迭代 过程 即 首先 
产生 于 一个 均匀分布 的 随机数 然后 根据 上式 计算 
每个 转移 主题 的 概率 通过 累积 概率 判断 随机数 
落在 哪个 new topic 下 更新 参数 矩阵 如此 迭代 
直至 收敛 2 . CNN2 . 1 多层 感知器 基础 
单个 感知器 的 结构 示例 如下 其中 函数 f 为 
激活 函数 一般用 sigmoid 函数 将 多个 单元 组合 起来 
并 具有 分层 结构 时 就 形成 了 多层 感知器 
模型 神经网络 下图 是 一个 具有 一个 隐含 层 3个 
节点 和/c 一个/m 单/n 节点/n 输出/v 层/q 的/uj 神经/n 网络/n 
2.2 卷积 神经网络 2 . 2.1 结构特征 在 图像 处理 
中 往往 把 图像 表示 为 像素 的 向量 比如 
一个 1000 × 1000 的 图像 可以 表示 为 一个 
〖 10 〗 ^ 6 的 向量 在 上述 的 
神经 网络 中 如果 隐含 层 数目 与 输入 层 
一样 即 也是 〖 10 〗 ^ 6时 那么 输入 
层 到 隐含 层 的 参数 数据 为 〖 10 
〗 ^ 12 这样 就 太多 了 基本 没法 训练 
因此 需要 减少 网络 的 参数 卷积 网络 就是 为 
识别 二维 形状 而 特殊 设计 的 一个 多层 感知器 
这种 网络结构 对 平移 比例 缩放 倾斜 或者 共 他 
形式 的 变形 具有 高度 不变性 这些 良好 的 性能 
是 网络 在 有 监督 方式 下 学会 的 网络/n 
的/uj 结构/n 主要/b 有/v 稀疏/a 连接/v 和/c 权值/i 共享/v 两个/m 
特点/n 包括 如下 形式 的 约束 1 特征提取 每一个 神经元 
从上 一层 的 局部 接受 域 得到 输入 因而 迫使 
它 提取 局部 特征 一旦 一个 特征 被 提取 出来 
只要 它 相对于 其他 特征 的 位置 被 近似 地 
保留 下来 它 的 精确 位置 就 变得 没有 那么 
重要 了 2 特征 映射 网络 的 每一个 计算 层 
都是 由 多个 特征 映射 组成 的 每个 特征 映射 
都是 平面 形式 的 平面 中 单独 的 神经元 在 
约束 下 共享 相同 的 权值 集 3 子 抽样 
每个 卷积 层 后面 跟着 一个 实现 局部 平均 和子/nr 
抽样 的 计算 层 由此 特征 映射 的 分辨率 降低 
这种 操作 具 有使 特征 映射 的 输出 对 平移 
和 其他 形式 的 变形 的 敏感度 下降 的 作用 
在 一个 卷积 网络 的 所有 层 中的 所有 权值 
都是 通过 有 监督 训练 来 学习 的 此外 网络 
还能 自动 的 在 学习 过程 中 提取 特征 一个 
卷积 神经网络 一般 是由 卷积 层 和子/nr 抽样 层 交替 
组成 下图 是 一个 例子 输入 的 图片 经过 卷积 
层 子 抽样 层 卷积 层 子 抽样 层 之后 
再由 一个 全 连接成 得到 输出 2 . 2.2 卷积 
层 卷积 层 是 通过 权值 共享 实现 的 共享 
权值 的 单元 构成 一个 特征 映射 如下 图 所示 
在 图中 有 3个 隐 层 节点 他们 属于 同 
一个 特征 映射 同种 颜色 的 链接 的 权值 是 
相同 的 这里 仍然 可以 使用 梯度 下降 的 方法 
来 学习 这些 权值 只 需要 对 原始 算法 做 
一些 小 的 改动 共享 权值 的 梯度 是 所有 
共享 参数 的 梯度 的 总和 2 . 2.3 子 
抽样 层子 抽样 层 通过 局部 感知 实现 一般 认为 
人 对 外界 的 认知 是 从 局部 到 全局 
的 而 图像 的 空间 联系 也 是 局部 的 
像素 联系 较为 紧密 而 距离 较远 的 像素 相关性 
则 较弱 因而 每个 神经元 其实 没有 必要 对 全局 
图像 进行 感知 只 需要 对 局部 进行 感知 然后 
在 更 高层 将 局部 的 信息 综合 起来 就 
得到 了 全局 的 信息 如下 图 所示 左/m 图为/i 
全/a 连接/v 右图 为 局部 连接 3 . LR 线性 
回归模型 一般 表达 为 h _ θ x = θ 
^ T X 形式 输出 域 是 整个 实 数域 
可以 用来 进行 二 分类 任务 但 实际 应用 中 
对 二分 类 问题 人们 一般 都 希望 获 的 
一个 0 1 范围 的 概率 值 比如 生病 的 
概率 是 0.9 或者 0.1 sigmoid 函数 g z 可以 
满足 这 一 需求 将 线性 回归 的 输出 转换 
到 0 1 利用 g z 可以 获取 样本 x 
属于 类别 1 和 类别 0 的 概率 p y 
= 1 | x θ p y = 0 | 
x θ 变成 逻辑 回归 的 形式 取 分类 阈值 
为 0.5 相应 的 决策函数 为 取 不同 的 分类 
阈值 可以 得到 不同 的 分类 结果 如果 对正 例 
的 判别 准确性 要求 高 可以 选择 阈值 大一些 比如 
0.6 对正 例 的 召回 要求 高 则 可以 选择 
阈值 小 一些 比如 0.3 转换 后的/nr 分类 面 decision 
boundary 与 原来 的 线性 回归 是 等价 的 3.1 
参数 求解 模型 的 数学 形式 确定 后 剩下 就是 
如何 去 求解 模型 中 的 参数 统计学 中 常用 
的 一种 方法 是 最大 似 然 估计 即 找到 
一组 参数 使得 在 这组 参 数下 我们 的 数据 
的 似 然 值 概率 越大 在 逻辑 回归模型 中 
似 然 值 可表示 为 取 对数 可以 得到 对数 
似 然 值 另一方面 在 机器学习 领域 我们 更 经常 
遇到 的 是 损失 函数 的 概念 其 衡量 的 
是 模型 预测 的 误差 值 越小 说明 模型 预测 
越好 常用 的 损失 函数 有0/nr 1 损失 log 损失 
hinge 损失 等 其中 log 损失 在 单个 样本点 的 
定义 为 定义 整个 数据 集上 的 平均 log 损失 
我们 可以 得到 即在 逻辑 回归模型 中 最大化 似 然 
函数 和 最小化 log 损失 函数 实际上 是 等价 的 
对于 该 优化 问题 存在 多种 求解 方法 这里 以 
梯度 下降 的 为例 说明 梯度 下降 Gradient Descent 又叫 
最速 梯度 下降 是 一种 迭代 求解 的 方法 通过 
在 每一步 选取 使 目标函数 变化 最快 的 一个 方向 
调整 参数 的 值 来 逼近 最优 值 基本 步骤 
如下 选择 下降 方向 梯度方向   选择 步长 更新 参数 
重复 以上 两步 直到 满足 终止 条件 3.2 分类 边界 
知道 如何 求解 参数 后 我们 来看 一下 模型 得到 
的 最后 结果 是 什么样 的 很容易 可以 从 sigmoid 
函数 看出 取 0.5 作为 分类 阈值 当     
时 y = 1 否则 y = 0   是 
模型 隐含 的 分类 平面 在 高维空间 中 一般 叫做 
超平面 所以 说 逻辑 回归 本质 上 是 一个 线性 
模型 但这 不 意味着 只有 线性 可分 的 数据 能 
通过 LR 求解 实际上 可以 通过 特征 变换 的 方式 
把 低 维空间 转换 到 高维空间 而在 低维 空间 不 
可分 的 数据 到 高维空间 中 线性 可分 的 几率 
会 高 一些 下面 两个 图 的 对比 说明了 线性 
分类 曲线 和 非线性 分类 曲线 通过 特征 映射 左图 
是 一个 线性 可分 的 数据集 右图 在 原始 空间 
中 线性 不可分 但是 在 特征 转换 x1 x2 = 
x1 x2 x21 x22 x1x2 后的/nr 空间 是 线性 可分 
的 对应 的 原始 空间 中 分类 边界 为 一条 
类 椭圆曲线 3.3 Word2VecWord2Vec 有 两种 网络 模型 分别为 CBOW 
模型 Continuous Bag of Words Model 和 Sikp gram 模型 
Continuous Skip gram Model 两个 模型 都 包含 三层 输入 
层 投影 层 和 输出 层 其中 CBOW 模型 是 
在 已知 当前 词 w t 的 上下文 w t 
2 w t 1 w t + 1 w t 
+ 2 的 情况 下 来 预测 词 w t 
Skip gram 模型 则 恰恰相反 它 是 在 已知 当前 
词 w t 的 情况 下 来 预测 当前 词 
的 上下文 w t 2 w t 1 w t 
+ 1 w t + 2 例如 今天 / 天气 
/ 好 / 晴朗 而 当前 词 为 天气 CBOW 
模型 是 预测 今天 好 晴朗 之间 出现 天气 的 
概率 而 Skip gram 模型 是 预测 天气 的 周围 
出现 今天 好 晴朗 三个 词 的 概率 CBOW 模型 
通过 优化 如下 的 目标 函 数来 求解 目标函数 为 
一个 对数 似 然 函数 CBOW 的 输入 为 包含 
Context w 中 2c 个 词 的 词 向量 v 
w 这 2c 个 词 向量 在 投影 层 累加 
得到 输出 层 的 输出 记为 X _ w 输出 
层 采用 了 Hierarchical Softmax 的 技术 组织 成 一棵 
根据 训练样本 集 的 所有 词 的 词频 构建 的 
Huffman 树 实际 的 词 为 Huffman 树 的 叶子 
节点 通过 长度 为     的 路径     
可以 找到 词 w 路径/n 可以/c 表示/v 成由0/nr 和1/nr 组成/v 
的/uj 串/v 记为   Huffman/w 数/n 的/uj 每个/r 中间/f 节点/n 
都/d 类似/v 于/p 一个/m 逻辑/n 回归/v 的/uj 判别式/n 每个 中间 
节点 的 参数 记为     那么 对于 CBOW 模型 
来说 有 那么 目标函数 为 那么 通过 随机 梯度 下 
降法 更新 目标 函数 的 参数 θ 和X/nr 使得 目标函数 
的 值 最大 即可 与 CBOW 模型 类似 Skip gram 
通过 优化 如下 的 目标 函 数来 求解 其中 那么 
Skip gram 的 目标 函数 为 通过 随机 梯度 下 
降法 更新 目标 函数 的 参数 θ 和v/nr w 使得 
目标函数 的 值 最大 即可 