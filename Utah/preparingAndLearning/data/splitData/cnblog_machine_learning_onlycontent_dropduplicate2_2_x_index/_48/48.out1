1 L1 范式 和 L2 方式 的 区别 1 L1 
范式 是 对应 参数 向量 绝对值 之和 2 L1 范式 
具有 稀疏 性 3 L1 范式 可以 用来 作为 特征选择 
并且 可 解释 性 较强 这里 的 原理 是 在 
实际 Loss function 中都 需 要求 最小值 根据 L1 的 
定义 可知 L1 最小值 只有 0 故 可以 通过 这种 
方式 来 进行 特征选择 4 L2 范式 是 对应 参数 
向量 的 平方和 再 求 平方根 5 L2 范式 是 
为了 防止 机器 学习 的 过拟合 提升 模型 的 泛化 
能力 2 优化 算法 及其 优缺点 温馨提示 在 回答 面试官 
的 问题 的 时候 往往 将 问题 往 大 的 
方面 去 回答 这样 不会 陷于 小 的 技术 上 
死磕 最后 很容易 把 自己 嗑 死了 1 随即 梯度 
下降 优点 可以 一定 程度 上 解决 局部 最优 解的/nr 
问题 缺点 收敛 速度 较慢 2 批量 梯度 下降 优点 
容易 陷入 局部 最优 解 缺点 收敛 速度 较快 3 
mini _ batch 梯度 下降 综合 随即 梯度 下降 和 
批量 梯度 下降 的 优缺点 提取 的 一个 中和 的 
方法 4 牛顿/nr 法/l 牛顿/nr 法在/nr 迭代/v 的/uj 时候/n 需要 
计算 Hessian 矩阵 当 维度 较高 的 时候 计算 Hessian 
矩阵 比较 困难 5 拟/v 牛顿/nr 法拟/nr 牛顿/nr 法是/nr 为了/p 
改进/v 牛顿/nr 法在/nr 迭代/v 过程/n 中/f 计算 Hessian 矩阵 而 
提取 的 算法 它 采用 的 方式 是 通过 逼近 
Hessian 的 方式 来 进行 求解 6 共轭 梯度 7 
启发式 的 优化 算法 启发式 的 优化 算法 有 遗传算法 
粒 子群 算法 等 这类 算法 的 主要 思想 就是 
设定 一个 目标函数 每次 迭代 根据 相应 的 策略 优化 
种群 直到 满足 什么样 的 条件 为止 3 RF 与 
GBDT 之间 的 区别 1 相同点/n 都/d 是由/i 多/m 棵树/i 
组成/v 最终/d 的/uj 结果/n 都/d 是由/i 多/m 棵树/i 一起/m 决定/v 
2 不同 点 组成 随机 森林 的 树 可以 分类 
树 也 可以 是 回归 树 而 GBDT 只由 回归 
树 组成 组成 随机 森林 的 树 可以 并行 生成 
而 GBDT 是 串行 生成 随机 森林 的 结果 是 
多数 表决 表决 的 而 GBDT 则是 多 棵树 累加 
之和 随机 森林 对 异常值 不 敏感 而 GBDT 对 
异常 值 比较 敏感 随机 森林 是 通过 减少 模型 
的 方差 来 提高 性能 而 GBDT 是 减少 模型 
的 偏差 来 提高 性能 的 随机 森林 不 需要 
进行 数据 预处理 即 特征 归一化 而 GBDT 则 需要 
进行 特征 归一化 3 RF 优点 易于 理解 易于 可视化 
不需要 太多 的 数据 预处理 即 数据 归一化 不易 过拟合 
易于 并行 化 缺点 不适合 小 样本数据 只 适合 大 
样本数据 大多数 情况下 RF 的 精度 低于 GBDT 适合 决策 
边界 的 是 矩阵 不适合 对角 线型 4 GBDT 优点 
精度高 缺点 参数 较多 容易 过拟合 不易 并行 化 4 
SVM 的 模型 的 推导 5 SVM 与 树 模型 
之间 的 区别 1 SVMSVM 是 通过 核 函数 将 
样本 映 射到 高纬 空间 再通过 线性 的 SVM 方式 
求解 分界 面 进行 分类 对 缺失 值 比较 敏感 
可以 解决 高纬度 的 问题 可以 避免 局部 极小值 的 
问题 可以 解决 小 样本 机器学习 的 问题 2 树/v 
模型/n 可以/c 解决/v 大/a 样本/n 的/uj 问题/n 易于/v 理解/v 和/c 
解释/v 会/v 陷入/v 局部/n 最优/d 解易/nr 过拟合/i 6/m 梯度 消 
失和 梯度 膨胀 1 梯度 消失 p . p1 { 
margin 0 . 0px 0 . 0px 0 . 0px 
0 . 0px font 10 . 0px . SF NS 
Text } 根据 链式法则 如果 每 一层 神经元 对上 一层 
的 输出 的 偏 导 乘上 权重 结果 都 小于 
1 的话 那么 即使 这个 结果 是 0.99 在 经过 
足够 多层 传播 之后 误差 对 输入 层 的 偏 
导 会 趋于 0 p . p1 { margin 0 
. 0px 0 . 0px 0 . 0px 0 . 
0px font 10 . 0px . SF NS Text } 
可以 采用 ReLU 激活 函数 有效 的 解决 梯度 消失 
的 情况 2 梯度 膨胀 p . p1 { margin 
0 . 0px 0 . 0px 0 . 0px 0 
. 0px font 10 . 0px . SF NS Text 
} 根据 链式法则 如果 每 一层 神经元 对上 一层 的 
输出 的 偏 导 乘上 权重 结果 都 大于 1 
的话 在 经过 足够 多层 传播 之后 误差 对 输入 
层 的 偏 导 会 趋于 无穷大 可以 通过 激活 
函数 来 解决 7 LR 的 原理 和 Loss 的 
推导 p . p1 { margin 0 . 0px 0 
. 0px 0 . 0px 0 . 0px font 10 
. 0px . SF NS Text } 