1、L1范式和L2方式的区别
（1）L1范式是对应参数向量绝对值之和
（2）L1范式具有稀疏性
（3）L1范式可以用来作为特征选择，并且可解释性较强（这里的原理是在实际Loss function中都需要求最小值，根据L1的定义可知L1最小值只有0，故可以通过这种方式来进行特征选择）
（4）L2范式是对应参数向量的平方和，再求平方根
（5）L2范式是为了防止机器学习的过拟合，提升模型的泛化能力
2、优化算法及其优缺点
温馨提示：在回答面试官的问题的时候，往往将问题往大的方面去回答，这样不会陷于小的技术上死磕，最后很容易把自己嗑死了。
（1）随即梯度下降
优点：可以一定程度上解决局部最优解的问题
缺点：收敛速度较慢
（2）批量梯度下降
优点：容易陷入局部最优解
缺点：收敛速度较快
（3）mini_batch梯度下降
综合随即梯度下降和批量梯度下降的优缺点，提取的一个中和的方法。
（4）牛顿法
牛顿法在迭代的时候，需要计算Hessian矩阵，当维度较高的时候，计算Hessian矩阵比较困难。
（5）拟牛顿法
拟牛顿法是为了改进牛顿法在迭代过程中，计算Hessian矩阵而提取的算法，它采用的方式是通过逼近Hessian的方式来进行求解。
（6）共轭梯度
（7）启发式的优化算法
启发式的优化算法有遗传算法，粒子群算法等。这类算法的主要思想就是设定一个目标函数，每次迭代根据相应的策略优化种群。直到满足什么样的条件为止。
3、RF与GBDT之间的区别
（1）相同点
都是由多棵树组成
最终的结果都是由多棵树一起决定
（2）不同点
组成随机森林的树可以分类树也可以是回归树，而GBDT只由回归树组成
组成随机森林的树可以并行生成，而GBDT是串行生成
随机森林的结果是多数表决表决的，而GBDT则是多棵树累加之和
随机森林对异常值不敏感，而GBDT对异常值比较敏感
随机森林是通过减少模型的方差来提高性能，而GBDT是减少模型的偏差来提高性能的
随机森林不需要进行数据预处理，即特征归一化。而GBDT则需要进行特征归一化
（3）RF：
优点：
易于理解，易于可视化
不需要太多的数据预处理，即数据归一化
不易过拟合
易于并行化
缺点：
不适合小样本数据，只适合大样本数据
大多数情况下，RF的精度低于GBDT
适合决策边界的是矩阵，不适合对角线型
（4）GBDT
优点：
精度高
缺点：
参数较多，容易过拟合
不易并行化
4、SVM的模型的推导
5、SVM与树模型之间的区别
（1）SVM
SVM是通过核函数将样本映射到高纬空间，再通过线性的SVM方式求解分界面进行分类。
对缺失值比较敏感
可以解决高纬度的问题
可以避免局部极小值的问题
可以解决小样本机器学习的问题
（2）树模型
可以解决大样本的问题
易于理解和解释
会陷入局部最优解
易过拟合
6、梯度消失和梯度膨胀
（1）梯度消失：
<!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 10.0px '.SF NS Text'} -->
根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都小于1的话，那么即使这个结果是0.99，在经过足够多层传播之后，误差对输入层的偏导会趋于0
<!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 10.0px '.SF NS Text'} -->
可以采用ReLU激活函数有效的解决梯度消失的情况
（2）梯度膨胀
<!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 10.0px '.SF NS Text'} -->
根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都大于1的话，在经过足够多层传播之后，误差对输入层的偏导会趋于无穷大
可以通过激活函数来解决
7、LR的原理和Loss的推导
<!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 10.0px '.SF NS Text'} -->