1 . 简介 gbdt 全称 梯度 提升 决策树 在 传统 
机器学习 算法 里面 是 对 真实 分布 拟合 的 最好 
的 几种 算法 之一 在 前几年 深度 学习 还 没有 
大行其道 之前 gbdt 在 各种 竞赛 是 大放异彩 原因 大概 
有 几个 一是 效果 确实 挺 不错 二 是 即 
可以 用于 分类 也 可以 用于 回归 三 是 可以 
筛选 特征 这 三点 实在 是 太 吸引人 了 导致 
在 面试 的 时候 大家 也 非常 喜欢 问 这个 
算法   gbdt 的 面试 考核 点 大致 有 下面 
几个 gbdt 的 算法 的 流程 gbdt 如何 选择 特征 
gbdt 如何 构建 特征 gbdt 如何 用于 分类 gbdt 通过 
什么 方式 减少 误差 gbdt 的 效果 相比 于 传统 
的 LR SVM 效果 为什么 好一些 gbdt 如何 加速 训练 
gbdt 的 参数 有 哪些 如何 调 参 gbdt 实战 
当中 遇到 的 一些 问题 gbdt 的 优缺点 2 . 
正式 介绍 首先 gbdt 是 通过 采用 加法 模型 即 
基 函数 的 线性组合 以及 不断 减小 训练 过程 产生 
的 残差 来 达到 将 数据 分类 或者 回归 的 
算法 gbdt 的 训练 过程 我们 通过 一张 图片 图片 
来源 来 说明 gbdt 的 训练 过程 图 1 GBDT 
的 训练 过程 gbdt 通过 多轮 迭代 每 轮 迭代 
产生 一个 弱 分类器 每个 分类器 在上 一轮 分类器 的 
残差 基础 上 进行 训练 对 弱 分类器 的 要求 
一般 是 足够 简单 并且/c 是/v 低/a 方差/n 和高/nr 偏差/n 
的/uj 因为 训练 的 过程 是 通过 降低 偏差 来 
不断 提高 最终 分类器 的 精度 此处 是 可以 证明 
的 弱 分类器 一般 会 选择 为 CART TREE 也 
就是 分类 回归 树 由于/c 上述/b 高/a 偏差/n 和/c 简单/a 
的/uj 要求/v 每个 分类 回归 树 的 深度 不会 很深 
最终 的 总 分类器 是 将 每 轮 训练 得到 
的 弱 分类器 加权 求和 得到 的 也 就是 加法 
模型 模型 最终 可以 描述 为 $ $ F _ 
{ m } x = \ sum _ { m 
= 1 } ^ { M } T \ left 
x \ theta _ m \ right $ $ 模型 
一共 训练 M 轮 每 轮 产生 一个 弱 分类器 
$ T \ left x \ theta _ m \ 
right $ 弱 分类器 的 损失 函数 $ $ \ 
hat \ theta _ { m } = \ mathop 
{ \ arg \ min } _ { \ theta 
_ { m } } \ sum _ { i 
= 1 } ^ { N } L \ left 
y _ { i } F _ { m 1 
} x _ { i } + T x _ 
{ i } \ theta _ { m } \ 
right $ $ $ F _ { m 1 } 
x $   为 当前 的 模型 gbdt 通过 经验 
风险 极小 化 来 确定 下 一个 弱 分类器 的 
参数 具体 到 损失 函数 本身 的 选择 也 就是 
L 的 选择 有 平方 损失 函数 0 1 损失 
函数 对数 损失 函数 等等 如果 我们 选择 平方 损失 
函数 那么 这个 差值 其实 就是 我们 平常 所说 的 
残差 但是 其实 我们 真正 关注 的 1 . 是 
希望 损失 函数 能够 不断 的 减小 2 . 是 
希望 损失 函数 能够 尽可能 快 的 减小 所以 如何 
尽可能 快 的 减小 呢 让 损失 函数 沿着 梯度方向 
的 下降 这个 就是 gbdt 的 gb 的 核心 了 
利用 损失 函数 的 负 梯度 在 当前 模型 的 
值 作为 回归 问题 提升 树 算法 中的 残差 的 
近似值 去 拟合 一个 回归 树 gbdt 每 轮 迭代 
的 时候 都去 拟合 损失 函数 在 当前 模型 下 
的 负 梯度 这样 每 轮 训练 的 时候 都 
能够 让 损失 函数 尽可能 快 的 减小 尽快 的 
收敛 达到 局部 最优 解 或者 全局 最优 解 gbdt 
如何 选择 特征 gbdt 选择 特征 的 细节 其实 是 
想问 你 CART Tree 生成 的 过程 这里 有 一个 
前提 gbdt 的 弱 分类器 默认 选择 的 是 CART 
TREE 其实 也 可以 选择 其他 弱 分类器 的 选择/v 
的/uj 前提/n 是/v 低/a 方差/n 和高/nr 偏差/n 框架 服从 boosting 
框架 即可 下面 我们 具体 来说 CART TREE 是 一种 
二叉树 如何 生成 CART TREE 生成 的 过程 其实 就是 
一个 选择 特征 的 过程 假设 我们 目前 总共有 M 
个 特征 第一步 我们 需要 从中/nr 选择 出 一个 特征 
j 做为 二叉树 的 第一 个 节点 然后 对 特征 
j 的 值 选择 一个 切 分点 m . 一个 
样本 的 特征 j 的 值 如果 小于 m 则 
分为 一类 如果 大于 m 则 分为 另外 一类 如此 
便 构建 了 CART 树 的 一个 节点 其他 节点 
的 生成 过程 和 这个 是 一样 的 现在 的 
问题 是 在 每 轮 迭代 的 时候 如何 选择 
这个 特征 j 以及 如何 选择 特征 j 的 切 
分点 m 原始 的 gbdt 的 做法 非常 的 暴力 
首先 遍历 每个 特征 然后 对 每个 特征 遍历 它 
所有 可能 的 切 分点 找到 最优 特征 m 的 
最优 切 分点 j 如何 衡量 我们 找到 的 特征 
m 和切/nr 分点 j 是 最优 的 呢 我们 用 
定义 一个 函数 FindLossAndSplit 来 展示 一下 求解 过程 1 
def findLossAndSplit x y 2 # 我们 用 x 来 
表示 训练 数据 3 # 我们 用 y 来 表示 
训练 数据 的 label 4 # x i 表示 训练 
数据 的 第 i 个 特征 5 # x _ 
i 表示 第 i 个 训练样本 6 7 # minLoss 
表示 最小 的 损失 8 minLoss = Integet . max 
_ value 9 # feature 表示 是 训练 的 数据 
第几 纬度 的 特征 10 feature = 0 11 # 
split 表示 切 分点 的 个数 12 split = 0 
13 14 # M 表示 样本 x 的 特征 个数 
15 for j in range 0 M 16 # 该 
维 特征 下 特征值 的 每个 切 分点 这里 具体 
的 切分 方式 可以 自己 定义 17 for c in 
range 0 x j 18 L = 0 19 # 
第一类 20 R1 = { x | x j = 
c } 21 # 第二类 22 R2 = { x 
| x j c } 23 # 属于 第一 类 
样本 的 y 值 的 平均值 24 y1 = ave 
{ y | x 属于 R1 } 25 # 属于 
第二类 样本 的 y 值 的 平均值 26 y2 = 
ave { y | x 属于 R2 } 27 # 
遍历 所有 的 样本 找到 loss funtion 的 值 28 
for x _ 1 in all x 29 if x 
_ 1 属于 R1 30 L + = y _ 
1 y1 ^ 2 31 else 32 L + = 
y _ 1 y2 ^ 2 33 if L minLoss 
34 minLoss = L 35 feature = i 36 split 
= c 37 return minLoss feature split 如果 对 这段 
代码 不是 很 了解 的 可以 先 去 看看 李航 
第五 章中 对 CART TREE 算法 的 叙述 在 这里 
我们 先 遍历 训练样本 的 所有 的 特征 对于 特征 
j 我们 遍历 特征 j 所有 特征值 的 切 分点 
c 找到 可以 让 下面 这个 式子 最小 的 特征 
j 以及 切 分点 c . gbdt 如何 构建 特征 
其实 说 gbdt 能够 构建 特征 并非 很 准确 gbdt 
本身 是 不能 产生 特征 的 但是 我们 可以 利用 
gbdt 去 产生 特征 的 组合 在 CTR 预 估中 
工业界 一般 会 采用 逻辑 回归 去 进行 处理 在 
我 的 上一 篇 博文 当中 已经 说过 逻辑 回归 
本身 是 适合 处理 线性 可分 的 数据 如果 我们 
想 让 逻辑 回归 处理 非线性 的 数据 其中 一种 
方式 便是 组合 不同 特征 增强 逻辑 回归 对 非线性 
分布 的 拟合 能力 长久以来 我们 都是/nr 通过 人工 的 
先验 知识 或者 实验 来 获得 有效 的 组合 特征 
但是 很多 时候 使用 人工 经验 知识 来 组合 特征 
过于 耗费 人力 造成了 机器学习 当中 一个 很 奇特 的 
现象 有/v 多少/m 人工/n 就/d 有/v 多少/m 智能/n 关键 是 
这样 通过 人工 去 组合 特征 并不 一定 能够 提升 
模型 的 效果 所以 我们 的 从业者 或者 学 界 
一直 都 有一个 趋势 便是 通过 算法 自动 高效 的 
寻找 到 有效 的 特征 组合 Facebook 在 2014年 发表 
的 一篇 论文 便是 这种 尝试 下 的 产物 利用 
gbdt 去 产生 有效 的 特征 组合 以便 用于 逻辑 
回归 的 训练 提升 模型 最终 的 效果 图 2 
用 GBDT 构造 特征 如图 2 所示 我们 使用 GBDT 
生成 了 两棵树 两颗 树 一 共有 五个 叶子 节点 
我们 将 样本 X 输入 到 两颗 树 当中 去 
样本 X 落在 了 第一 棵树 的 第二个 叶子 节点 
第二颗 树 的 第一 个 叶子 节点 于是 我们 便 
可以 依次 构建 一个 五 纬 的 特征向量 每一个 纬度 
代表 了 一个 叶子 节点 样本 落 在 这个 叶子 
节点 上面 的话 那么 值 为 1 没有 落在 该 
叶子 节点 的话 那么 值 为 0 . 于是 对于 
该 样本 我们 可以 得到 一个 向量 0 1 0 
1 0 作为 该 样本 的 组合 特征 和 原来 
的 特征 一起 输入 到 逻辑 回归 当中 进行 训练 
实验 证明 这样 会 得到 比较 显著 的 效果 提升 
GBDT 如何 用于 分类 首先 明确 一点 gbdt 无论 用于 
分类 还是 回归 一直都 是 使用 的 CART 回归 树 
不会 因为 我们 所 选择 的 任务 是 分类 任务 
就 选用 分类 树 这 里面 的 核心 是 因为 
gbdt 每 轮 的 训练 是 在上 一轮 的 训练 
的 残差 基础 之上 进行 训练 的 这里 的 残差 
就是 当前 模型 的 负 梯度 值 这个 要求 每 
轮 迭代 的 时候 弱 分类器 的 输出 的 结果 
相减 是 有 意义 的 残差 相减 是 有 意义 
的 如果 选用 的 弱 分类器 是 分类 树 类别 
相减 是 没有 意义 的 上 一轮 输出 的 是 
样本 x 属于 A 类 本 一轮 训练 输出 的 
是 样本 x 属于 B 类 A 和 B 很多 
时候 甚至 都 没有 比较 的 意义 A 类 B 
类 是 没有 意义 的 我们 具体 到 分类 这个 
任务 上面 来 我们 假设 样本 X 总共有 K 类 
来 了 一个 样本 x 我们 需要 使用 gbdt 来 
判断 x 属于 样本 的 哪 一类 图三 gbdt 多 
分类 算法 流程 第一步 我们 在 训练 的 时候 是 
针对 样本 X 每个 可能 的 类 都 训练 一个 
分类 回归 树 举例说明 目前 样本 有 三类 也 就是 
K = 3 样本 x 属于 第二类 那么 针对 该 
样本 x 的 分类 结果 其实 我们 可以 用 一个 
三维 向量 0 1 0 来 表示 0 表示 样本 
不属于 该类 1 表示 样 本属于 该类 由于 样本 已经 
属于 第二类 了 所以 第二类 对应 的 向量 维度 为 
1 其他 位置 为 0 针对 样本 有 三类 的 
情况 我们 实质上 是 在 每 轮 的 训练 的 
时候 是 同时 训练 三颗 树 第一颗 树 针对 样本 
x 的 第一 类 输入 为 $ x 0 $ 
第二颗 树 输入 针对 样本 x 的 第二 类 输入 
为 $ x 1 $ 第三颗 树 针对 样本 x 
的 第三 类 输入 为 $ x 0 $ 在 
这里 每颗 树 的 训练 过程 其实 就是 就是 我们 
之前 已经 提到 过 的 CATR TREE 的 生成 过程 
在 此处 我们 参照 之前 的 生成树 的 程序 即 
可以 就 解出 三颗 树 以及 三颗 树 对 x 
类别 的 预测 值 $ f _ { 1 } 
x f _ { 2 } x f _ { 
3 } x $ 那么 在 此类 训练 中 我们 
仿照 多 分类 的 逻辑 回归 使用 softmax 来 产生 
概率 则 属于 类别 1 的 概率 $ $ p 
_ { 1 } = exp f _ { 1 
} { x } / \ sum _ { k 
= 1 } ^ { 3 } exp f _ 
{ k } { x } $ $ 并且 我们 
我们 可以 针对 类别 1 求出 残差 $ y _ 
{ 11 } x = 0 p _ { 1 
} x $ 类别 2 求出 残差 $ y _ 
{ 22 } x = 1 p _ 2 x 
$ 类别 3 求出 残差 $ y _ { 33 
} x = 0 p _ { 3 } x 
$ . 然后 开始 第二 轮 训练 针对 第一类 输入 
为 x $ y _ { 11 } x $ 
针对 第二类 输入 为 x $ y _ { 22 
} x $ 针对 第三类 输入 为 x $ y 
_ { 33 } x $ . 继续 训练 出 
三颗 树 一直 迭代 M 轮 每 轮 构建 3颗 
树 所以 当 K = 3 我们 其实 应该 有 
三个 式子 $ $ F _ { 1M } { 
x } = \ sum _ { m = 1 
} ^ { M } { \ hat { C 
_ { 1m } } I x \ epsilon R 
_ { 1m } } $ $ $ $ F 
_ { 2M } { x } = \ sum 
_ { m = 1 } ^ { M } 
{ \ hat { C _ { 2m } } 
I x \ epsilon R _ { 2m } } 
$ $ $ $ F _ { 3M } { 
x } = \ sum _ { m = 1 
} ^ { M } { \ hat { C 
_ { 3m } } I x \ epsilon R 
_ { 3m } } $ $ 当 训练 完毕 
以后 新来 一个 样本 x1 我们 需要 预测 该 样本 
的 类别 的 时候 便 可以 有 这三个 式子 产生 
三个 值 $ f _ { 1 } x f 
_ { 2 } x f _ { 3 } 
x $ 样 本属于 某个 类别 c 的 概率 为 
  $ $ p _ { c } = exp 
f _ { c } { x } / \ 
sum _ { k = 1 } ^ { 3 
} exp f _ { k } { x } 
$ $ GBDT 多 分类 举例说明 上面 的 理论 阐述 
可能 仍旧 过于 难懂 我们 下面 将 拿 Iris 数据 
集中 的 六个 数据 作为 例子 来 展示 gbdt 多 
分类 的 过程 样本 编号 花萼 长度 cm 花萼 宽度 
cm 花瓣 长度 cm 花瓣/n 宽度/n 花的/nr 种类/n 15/m ./i 
13.51/mx ./i 40.2/mx 山/n 鸢尾/n 24/m ./i 93.01/mx ./i 40.2/mx 
山/n 鸢尾/n 37/m ./i 03.24/mx ./i 71.4/mx 杂色/n 鸢尾/n 46/m 
./i 43.24/mx ./i 51.5/mx 杂色/n 鸢尾/n 56/m ./i 33.36/mx ./i 
02.5/mx 维吉尼亚/ns 鸢尾/n 65/m ./i 82.75/mx ./i 11.9/mx 维吉尼亚/ns 鸢尾/n 
图/n 四/m Iris 数据集 这 是 一个 有 6个 样本 
的 三分 类 问题 我们 需要 根据 这个 花的/nr 花萼 
长度 花萼 宽度 花瓣 长度 花瓣 宽度 来 判断 这个 
花属/nr 于山 鸢尾 杂色 鸢尾 还是 维吉尼亚 鸢尾 具体 应用 
到 gbdt 多 分类 算法 上面 我们 用 一个三维 向量 
来 标志 样本 的 label 1 0 0 表示 样 
本属 于山 鸢尾 0 1 0 表示 样 本属于 杂色 
鸢尾 0 0 1 表示 属于 维吉尼亚 鸢尾 gbdt 的 
多 分类 是 针对 每个 类 都 独立 训练 一个 
CART Tree 所以 这里 我们 将 针对 山 鸢尾 类别 
训练 一个 CART Tree 1 杂色 鸢尾 训练 一个 CART 
Tree 2 维吉尼亚 鸢尾 训练 一个 CART Tree 3 这三个 
树 相互 独立 我们 以 样本 1 为例 针对 CART 
Tree1 的 训练 样本 是 $ 5.1 3.5 1.4 0.2 
$ label 是 1 最终 输入 到 模型 当中 的 
为 $ 5.1 3.5 1.4 0.2 1 $ 针对 CART 
Tree2 的 训练 样本 也是 $ 5.1 3.5 1.4 0.2 
$ 但是 label 为 0 最终 输入 模型 的 为 
$ 5.1 3.5 1.4 0.2 0 $ . 针对 CART 
Tree 3 的 训练样本 也是 $ 5.1 3.5 1.4 0.2 
$ label 也为 0 最终 输入 模型 当中 的 为 
$ 5.1 3.5 1.4 0.2 0 $ . 下面 我们 
来看 CART Tree1 是 如何 生成 的 其他 树 CART 
Tree2 CART Tree 3 的 生成 方式 是 一样 的 
CART Tree 的 生成 过程 是从 这四个 特征 中 找 
一个 特征 做为 CART Tree1 的 节点 比如 花萼 长度 
做为 节点 6个 样本 当中 花萼 长度 大于 5.1 cm 
的 就是 A 类 小于 等于 5.1 cm 的 是 
B 类 生成 的 过程 其实 非常 简单 问题 1 
. 是 哪个 特征 最合适 2 . 是 这个 特征 
的 什么 特征值 作为 切 分点 即使 我们 已经 确定 
了 花萼 长度 做为 节点 花萼 长度 本身 也 有 
很多 值 在 这里 我们 的 方式 是 遍历 所有 
的 可能性 找到 一个 最好 的 特征 和它/nr 对应 的 
最优 特征值 可以 让 当前 式子 的 值 最小 我们 
以 第一 个 特征 的 第一 个 特征值 为例 R1 
为 所有 样本 中 花萼 长度 小于 5.1 cm 的 
样本 集合 R2 为 所有 样本 当中 花萼 长度 大于 
等于 5 . 1cm 的 样本 集合 所以 $ R1 
= \ left \ {   2 \ right \ 
} $ $ R2 = \ left \ { 1 
3 4 5 6 \ right \ } $ . 
图 5 节点 分裂 示意图 y1 为 R1 所有 样本 
的 label 的 均值 $ 1/1 = 1 $ y2 
为 R2 所有 样本 的 label 的 均值 $ 1 
+ 0 + 0 + 0 + 0 / 5 
= 0.2 $ 下面 便 开始 针对 所有 的 样本 
计算 这个 式子 的 值 样本 1 属于 R2 计算 
的 值 为 $ 1 0.2 ^ 2 $ 样本 
2 属于 R1 计算 的 值 为 $ 1 1 
^ 2 $ 样本 3 4 5 6 同理 都是 
属于 R2 的 所以 值 是 $ 0 0.2 ^ 
2 $ . 把这 六个 值 加起来 便是 山 鸢尾 
类型 在 特征 1 的 第一 个 特征值 的 损失 
值 这里 算出来 1 0.2 ^ 2 + 1 1 
^ 2 + 0 0.2 ^ 2 + 0 0.2 
^ 2 + 0 0.2 ^ 2 + 0 0.2 
^ 2 = 0.84 接着 我们 计算 第一 个 特征 
的 第二 个 特征值 计算 方式 同上 R1 为 所有 
样本 中 花萼 长度 小于 4.9 cm 的 样本 集合 
R2 为 所有 样本 当中 花萼 长度 大于 等于 4.9 
cm 的 样本 集合 . 所以 $ R1 = \ 
left \ {   \ right \ } $ $ 
R1 = \ left \ { 1 2 3 4 
5 6 \ right \ } $ .   y1 
为 R1 所有 样本 的 label 的 均值 = 0 
y2 为 R2 所有 样本 的 label 的 均值 $ 
1 + 1 + 0 + 0 + 0 + 
0 / 6 = 0.3333 $ 图 6 第一 个 
特征 的 第二 个 特侦 值 的 节点 分裂 情况 
我们 需要 针对 所有 的 样本 样本 1 属于 R2 
计算 的 值 为 $ 1 0.333 ^ 2 $ 
样本 2 属于 R2 计算 的 值 为 $ 1 
0.333 ^ 2 $ 样本 3 4 5 6 同理 
都是 属于 R2 的 所以 值 是 $ 0 0.333 
^ 2 $ . 把这 六个 值 加起来 山 鸢尾 
类型 在 特征 1 的 第二 个 特征值 的 损失 
值 这里 算出来 1 0.333 ^ 2 + 1 0.333 
^ 2 + 0 0.333 ^ 2 + 0 0.333 
^ 2 + 0 0.333 ^ 2 + 0 0.333 
^ 2 = 2.244189 . 这里 的 损失 值 大于 
特征 一 的 第一 个 特征值 的 损失 值 所以 
我们 不 取 这个 特征 的 特征值 图 7 所有 
情况 说明 这样 我们 可以 遍历 所有 特征 的 所有 
特征值 找到 让 这个 式子 最小 的 特征 以及 其 
对应 的 特征值 一 共有 24种 情况 4个 特征 * 
每个 特征 有 6个 特征值 在 这里 我们 算 出来 
让 这个 式子 最小 的 特征 花萼 长度 特征值 为 
5.1 cm 这个 时候 损失 函数 最小 为 0.8 于是 
我们 的 预测 函数 此时 也 可以 得到 $ $ 
f x = \ sum _ { x \ epsilon 
R _ { 1 } } y _ { 1 
} * I x \ epsilon R _ { 1 
} + \ sum _ { x \ epsilon R 
_ { 2 } } y _ { 2 } 
* I x \ epsilon R _ { 2 } 
$ $ 此处 R1 = { 2 } R2 = 
{ 1 3 4 5 6 } y1 = 1 
y2 = 0.2 训练 完 以后 的 最终 式子 为 
$ $ f _ { 1 } x = \ 
sum _ { x \ epsilon R _ { 1 
} } 1 * I x \ epsilon R _ 
{ 1 } + \ sum _ { x \ 
epsilon R _ { 2 } } 0.2 * I 
x \ epsilon R _ { 2 } $ $ 
借 由 这个 式子 我们 得到 对 样本 属于 类别 
1 的 预测 值 $ f _ { 1 } 
x = 1 + 0.2 * 5   = 2 
$ 同理 我们 可以 得到 对 样本 属于 类别 2 
3 的 预测 值 $ f _ { 2 } 
x $ $ f _ { 3 } x $ 
. 样 本属于 类别 1 的 概率 即为   $ 
$ p _ { 1 } = exp f _ 
{ 1 } { x } / \ sum _ 
{ k = 1 } ^ { 3 } exp 
f _ { k } { x } $ $ 
下面 我们 用 代码 来 实现 整个 找 特征 的 
过程 大家 可以 自己 再 对照 代码 看看 1 # 
定义 训练 数据 2 train _ data = 5.1 3.5 
1.4 0.2 4.9 3.0 1.4 0.2 7.0 3.2 4.7 1.4 
6.4 3.2 4.5 1.5 6.3 3.3 6.0 2.5 5.8 2.7 
5.1 1.9 3 4 # 定义 label 5 label _ 
data = 1 0 0 1 0 0 0 1 
0 0 1 0 0 0 1 0 0 1 
6 # index 表示 的 第 几类 7 def f 
i n d B e s t L o s 
s A n d p l i t train _ 
data label _ data index 8 sample _ numbers = 
len label _ data 9 feature _ numbers = len 
train _ data 0 10 current _ label = 11 
12 # define the minLoss 13 minLoss = 10000000 14 
15 # feature represents the dimensions of the feature 16 
feature = 0 17 18 # split represents the detail 
split value 19 split = 0 20 21 # get 
current label 22 for label _ index in range 0 
len label _ data 23 current _ label . append 
label _ data label _ index index 24 25 # 
trans all features 26 for feature _ index in range 
0 feature _ numbers 27 # # current feature value 
28 current _ value = 29 30 for sample _ 
index in range 0 sample _ numbers 31 current _ 
value . append train _ data sample _ index feature 
_ index 32 L = 0 33 # # different 
split value 34 print current _ value 35 for index 
in range 0 len current _ value 36 R1 = 
37 R2 = 38 y1 = 0 39 y2 = 
0 40 41 for index _ 1 in range 0 
len current _ value 42 if current _ value index 
_ 1 current _ value index 43 R1 . append 
index _ 1 44 else 45 R2 . append index 
_ 1 46 47 # # calculate the samples for 
first class 48 sum _ y = 0 49 for 
index _ R1 in R1 50 sum _ y + 
= current _ label index _ R1 51 if len 
R1 = 0 52 y1 = float sum _ y 
/ float len R1 53 else 54 y1 = 0 
55 56 # # calculate the samples for second class 
57 sum _ y = 0 58 for index _ 
R2 in R2 59 sum _ y + = current 
_ label index _ R2 60 if len R2 = 
0 61 y2 = float sum _ y / float 
len R2 62 else 63 y2 = 0 64 65 
# # trans all samples to find minium loss and 
best split 66 for index _ 2 in range 0 
len current _ value 67 if index _ 2 in 
R1 68 L + = float current _ label index 
_ 2 y1 * float current _ label index _ 
2 y1 69 else 70 L + = float current 
_ label index _ 2 y2 * float current _ 
label index _ 2 y2 71 72 if L minLoss 
73 feature = feature _ index 74 split = current 
_ value index 75 minLoss = L 76 print minLoss 
77 print minLoss 78 print split 79 print split 80 
print feature 81 print feature 82 return minLoss split feature 
83 84 f i n d B e s t 
L o s s A n d p l i 
t train _ data label _ data 0 3 总结 
目前 我们 总结 了 gbdt 的 算法 的 流程 gbdt 
如何 选择 特征 如何 产生 特征 的 组合 以及 gbdt 
如何 用于 分类 这个 目前 可以 认为 是 gbdt 最 
经常 问到 的 四个 部分 至于 剩余 的 问题 因为 
篇幅 的 问题 我们 准备 再 开 一个 篇幅 来 
进行 总结 也 欢迎 大家 关注 我 的 微信 公众 
号 ModifyAI 