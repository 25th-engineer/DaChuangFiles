朴素 贝叶斯 P A ∩ B = P A * 
P B | A = P B * P A 
| B 所以有 P A | B = P B 
| A * P A / P B 对于 给出 
的 待 分 类项 求解 在 此项 出现 的 条件 
下 各个 目标 类别 出现 的 概率 哪个 最大 就 
认为 此 待 分 类项 属于 哪个 类别 工作 原理 
假设 现在 有 样本 x = a1 a2 a3 an 
这个 待 分 类项 并 认为 x 里面 的 特征 
独立 再 假设 现在 有 分类 目标 Y = { 
y1 y2 y3 y4 . . yn } 那么 max 
P y1 | x P y2 | x P y3 
| x . . P yn | x 中 的 
最大 者 就是 最终 的 分类 类别 而 P yi 
| x = p x | yi * P yi 
/ P x 因为 x 对于 每个 分类 目标 来说 
都 一样 所以 就是 求 max P x | yi 
* p yi P x | yi * p yi 
= p yi * PI P ai | yi PI 
表示 连乘 而 具体 的 p ai | yi 和p/nr 
yi 都是 能从 训练样本 中 统计 出来 p ai | 
yi 表示 该类 别下 该 特征 出现 的 概率 p 
yi 表示 全部 类别 中 这个 这个 类别 出现 的 
概率 好 的 就是 这么 工作 的 ^ _ ^ 
工作 流程 准备 阶段 确定 特征 属性 并对 每个 特征 
属性 进行 适当 划分 然后 由 人工 对 一部分 待 
分类 项 进行 分类 形成 训练样本 训练 阶段 计算 每个 
类别 在 训练 样本 中 的 出现 频率 及 每个 
特征 属性 划分 对 每个 类别 的 条件概率 估计 应用 
阶段 使用 分类器 进行 分类 输入 是 分类器 和待/nr 分类 
样本 输出 是 样本 属于 的 分类 类别 属性 特征 
特征 为 离散 值 时 直接 统计 即可 表示 统计 
概率 特征 为 连续 值 的 时候 假定 特征 符合 
高斯分布 g x n u 那么 p ak | yi 
= g xk ni ui Laplace 校准 拉普拉斯 校验 当 
某个 类 别下 某个 特征 划分 没有 出现 时 会有 
P a | y = 0 就是 导致 分类器 质量 
降低 所以 此时 引入 Laplace 校验 就是 对 没 类 
别下 所有 划分 的 计数 加 1 遇到 特征 之间 
不 独立 问题 参考 改进 的 贝叶斯 网络 使用 DAG 
来 进行 概率 图 的 描述 优缺点 朴素 贝叶斯 的 
优点 对 小 规模 的 数据 表现 很好 适合 多 
分类 任务 适合 增量 式 训练 缺点 对 输入 数据 
的 表达 形式 很 敏感 离散 连续 值 极大 极小 
之类 的 http / / www . cnblogs . com 
/ leoo2sk / archive / 2010 / 09/17 / naive 
bayesian classifier . html 逻辑 回归 和 线性 回归 LR 
回归 是 一个 线性 的 二分 类 模型 主要 是 
计算 在 某个 样本 特征 下 事件 发生 的 概率 
比如 根据 用户 的 浏览 购买 情况 作 为特征 来 
计算 它 是否 会 购买 这 个 商品 抑或 是 
它 是否 会 点击 这个 商品 然后 LR 的 最终 
值 是 根据 一个 线性 和 函数 再 通过 一个 
sigmod 函 数来 求得 这个 线性 和 函数 权重 与 
特征值 的 累加 以及 加上 偏置 求 出来 的 所以/c 
在/p 训练/vn LR/w 时也/nr 就是/d 在/p 训练/vn 线性/n 和/c 函数/n 
的/uj 各个/r 权重/n 值/n w/w 关于 这个 权重 值 w 
一般 使用 最大 似 然 法来/nr 估计 比如 yi = 
1 的 概率 是 pi 则 yi = 0 的 
概率 是 1 pi 那么 观测 概率 为 p yi 
= pi ^ yi * 1 pi ^ 1 yi 
这个 这个 最大 似 然 函数 为 hw xi ^ 
yi * 1 hw xi ^ 1 yi 连乘 对 
这个 似 然 函数 取 对数 之后 就 会 得到 
的 表达式 L w = sigma yi * log hw 
xi 1 yi log 1 hw xi = sigma yi 
* w * xi log 1 + exp w * 
xi 估计 这个 L w 的 极大值 就 可以 得到 
w 的 估计值 所以 求解 问题 就 变成 了 这个 
最大 似 然 函数 的 最优 化 问题 这里 通常会 
采样 随机 梯度 下 降法 和拟/nr 牛顿 迭代法 来 进行 
优化 梯度 下 降法 如果 hw x = 1 / 
1 e ^ wx 则 cost function = 1 / 
m * sigma yi * log hw xi + 1 
yi * log 1 hw xi = j w 这里 
就 成了 就 min j w 所以 更新 w 的 
过程 为 w = w lamea * j w 求导 
w = w lamea * 1 / m \ * 
sigma m hw xi yi * xi 直到 j w 
不能 再 的 时候 停止 梯度 下 降法 的 最大 
问题 就是 会 陷入 局部 最优 并且 每次 在 对 
当前 样本 计算 cost 的 时候 都 需要 去 遍历 
全部 样本 才能 得到 cost 值 这样 计算速度 就会 慢 
很多 虽然 在 计算 的 时候 可以 转为 矩阵 乘法 
去 更新 整个 w 值 所以 现在 好多 框架 mahout 
中 一般 使用 随机 梯度 下 降法 它 在 计算 
cost 的 时候 只 计算 当前 的 代价 最终 cost 
是 在 全部 样本 迭代 一遍 之 求和 得出 还有 
他 在 更新 当前 的 参数 w 的 时候 并 
不是 依次 遍历 样本 而是 从 所有 的 样本 中 
随机 选择 一条 进行 计算 它 方法 收敛 速度快 一般 
是 使用 最大 迭代 次数 并且 还 可以 避免 局部 
最优 并且 还 很容易 并行 使用 参数 服务器 的 方式 
进行 并行 这里 SGD 可以 改进 的 地方 就是 使用 
动态 的 梯度 值 alpha = 0.04 * 1.0 + 
n + i + Rate 其他 优化 方法 拟 牛顿 
法 记得 是 需要 使用 Hessian 矩阵 和 cholesky 分解 
BFGSL BFGS 优缺点 无需 选择 学习率 α 更快 但是 更 
复杂 关于 LR 的 过拟合 问题 如果 我们 有 很多 
的 特性 在 训练 集上 拟合 得 很好 但是 在 
预测 集上 却 达 不到 这种 效果 1 . 减少 
feature 个数 人工 定义 留 多少 个 feature 算法 选取 
这些 feature 2 . 正则化 留下 所有 的 feature 但 
对于 部分 feature 定义 其 parameter 非常 小 在 cost 
上加 lamea sigma w ^ 2 同时 w 的 更新 
变为 w = w rate * 1 / m \ 
* sigma m hw xi yi * xi + lamea 
/ m * w 注意 这里 的 w0 不受 正则化 
影响 关于 LR 的 多 分类 softmaxsoftmax 假设 离散 型 
随机变量 Y 的 取值 集合 是 { 1 2 . 
. k } 则 多 分类 的 LR 为 P 
Y = a | x = exp wa * x 
/ 1 1 到 k 求和 wk * x 1 
a k 这里会 输出 当前 样本 下 属于 哪 一类 
的 概率 并且 满足 全部 概率 加起来 = 1 关于 
softmax 和k个/nr LR 的 选择 如果 类别 之间 是否 互斥 
比如 音乐 只能 属于 古典音乐 乡村音乐 摇滚 月 的 一种 
就用 softmax 否则 类别 之前 有 联系 比如 一 首歌曲 
可能 有 影视 原声 也 可能 包含 人声 或者 是 
舞曲 这个 时候 使用 k 个 LR 更为 合适 优缺点 
Logistic 回归 优点 实现 简单 分类 时 计算 量 非常 
小 速度 很快 存储资源 低 缺点 容易 欠 拟合 一般 
准确度 不 太高 只能 处理 两 分类 问题 在此 基础 
上 衍生 出来 的 softmax 可以 用于 多 分类 且 
必须 线性 可分 http / / www . cnblogs . 
com / biyeymyhjob / archive / 2012/07 / 18/2595410 . 
htmlhttp / / blog . csdn . net / abcjennifer 
/ article / details / 7716281http / / ufldl . 
stanford . edu / wiki / index . php / 
Softmax % E 5% 9B % 9E % E 5% 
BD % 92KNN 算法 给 一个 训练 数据集 和 一个 
新的 实例 在 训练 数据 集中 找出 与 这个 新 
实例 最近 的 k 个 训练 实例 然后 统计 最近 
的 k 个 训练 实例 中 所属 类别 计数 最多 
的 那个 类 就是 新 实例 的 类 三要素 k 
值 的 选择 距离 的 度量 常见 的 距离 度量 
有 欧式 距离 马氏 距离 等 分类 决策 规则 多数 
表决 规则 k 值 的 选择 k 值 越小 表明 
模型 越 复杂 更加 容易 过拟合 但是 k 值 越大 
模型 越 简单 如果 k = N 的 时候 就 
表明 无论什么 点 都是 训练 集中 类别 最多 的 那个 
类 所以 一般 k 会 取 一个 较小 的 值 
然后 用过 交叉 验证 来 确定 这里 所谓 的 交叉 
验证 就是 将 样本 划分 一部分 出来 为 预测 样本 
比如 95% 训练 5% 预测 然后 k 分别 取 1 
2 3 4 5 之类 的 进行 预测 计算 最后 
的 分类 误差 选择 误差 最小 的 kKNN 的 回归 
在 找到 最近 的 k 个 实例 之后 可以 计算 
这 k 个 实例 的 平均值 作为 预测值 或者 还 
可以 给 这 k 个 实例 添加 一个 权重 再 
求 平均值 这个 权重 与 度量 距离 成反比 越近/nr 权重 
越大 优缺点 KNN 算法 的 优点 思想 简单 理论 成熟 
既 可以 用来 做 分类 也 可以 用来 做 回归 
可 用于 非线性 分类 训练 时间 复杂度 为 O n 
准确度 高 对 数据 没有 假设 对 outlier 不 敏感 
缺点 计算 量大 样本 不 平衡 问题 即 有些 类别 
的 样本 数量 很多 而 其它 样本 的 数量 很少 
需要 大量 的 内存 KD 树 KD 树 是 一个 
二叉树 表示 对 K 维空间 的 一个 划分 可以 进行 
快速 检索 那/r KNN/w 计算/v 的/uj 时候/n 不/d 需要/v 对/p 
全/a 样本/n 进行/v 距离/n 的/uj 计算/v 了/ul 构造 KD 树 
在 k 维 的 空间 上 循环 找 子 区域 
的 中位数 进行 划分 的 过程 假设 现在 有K/nr 维空间 
的 数据集 T = { x1 x2 x3 xn } 
xi = { a1 a2 a3 . . ak } 
首先 构造 根 节点 以 坐标 a1 的 中位数 b 
为 切 分点 将 根 结点 对应 的 矩形 局域 
划分 为 两个 区域 区域 1中 a1b 构造 叶子 节点 
分别 以 上面 两个 区域 中 a2 的 中位数 作为 
切 分点 再次 将 他们 两两 划分 作为 深度 1 
的 叶子 节点 如果 a2 = 中位数 则 a2 的 
实例 落在 切分 面 不断 重复 2 的 操作 深度 
为 j 的 叶子 节点 划分 的 时候 索取 的 
ai 的 i = j % k + 1 直到/v 
两/m 个子/n 区域/n 没有/v 实例/n 时/n 停止/v KD/w 树/v 的/uj 
搜索/v 首先/d 从根/nr 节点/n 开始/v 递归/v 往下/t 找到/v 包含/v x/w 
的/uj 叶子/nr 节点/n 每 一层 都是 找 对应 的 xi 
将 这个 叶子 节点 认为 是 当前 的 近似 最 
近点 递归 向上 回退 如果 以 x 圆心 以 近似 
最 近点 为 半径 的 球 与 根 节点 的 
另一 半子 区域 边界 相交 则 说明 另一 半子 区域 
中 存在 与 x 更近 的 点 则 进入 另一 
个子 区域 中 查找 该点 并且 更新 近似 最 近点 
重复 3 的 步骤 直到 另一 子 区域 与 球体 
不 相交 或者 退回 根 节点 最后 更 新的 近似 
最 近点 与 x 真正 的 最近 点 KD 树 
进行 KNN 查找 通过 KD 树 的 搜索 找到 与 
搜索 目标 最近 的 点 这样 KNN 的 搜索 就 
可以 被 限制 在 空间 的 局部 区域 上了 可以 
大大 增加 效率 KD 树 搜索 的 复杂度 当 实例 
随机分布 的 时候 搜索 的 复杂度 为 log N N 
为 实例 的 个数 KD 树 更加 适用于 实例 数量 
远 大于 空间维度 的 KNN 搜索 如果 实例 的 空间维度 
与 实例 个数 差 不多时 它 的 效率 基于 等于 
线性 扫描 SVM SMO 对于 样本点 xi yi 以及 svm 
的 超平面 wix + b = 0 函数 间隔 yi 
wxi + b 几何 间隔 yi wxi + b / 
| | w | | 其中 | | w | 
| 为 w 的 L2 范数 几何 间隔 不会 因为 
参数 比例 的 改变 而 改变 svm 的 基本 想法 
就是 求解 能 正确 划分 训练样本 并且 其 几何 间隔 
最大化 的 超平面 线性 SVM 问题 yi wxi + b 
/ | | w | | = d 使用 几何 
间隔 求 max d 那么 假设 d = d | 
| w | | 则将 问题 转为 yi wxi + 
b = 1 max d / | | w | 
| 由于 d 的 成比例 增减 不会 影响 实际 间距 
所以 这里 的 取 d = 1 又 因为 max 
1 / | | w | | = min 1/2 
\ | | w | | ^ 2 所以 最终 
的 问题 就 变为 了 yi wxi + b = 
1 min 1/2 * | | w | | ^ 
2 这样 就 变成 了 一个 凸 的 二次 规划 
化 可以 将其 转换 为 拉格朗日 函数 然后 使用 对偶 
算法 来 求解 对偶 求解 L w b a = 
1/2 * | | w | | ^ 2 sigma 
ai * yi wxi + b + sigma ai 其中 
a = { a1 a2 . . an } 为 
拉格朗日 向量 根据 对偶 性质 原始 问题 就是 求 对偶 
问题 的 极大 极小 max a min w b L 
w b a 先 求 L 对 w b 的 
极小 再 求 对 a 的 极大 求 min w 
b L w b a L w = w sigma 
aiyixi = 0L b = sigma aiyi = 0 代入 
后 可得 min w b L w b a = 
1/2 * sigma sigma aiajyiyj xi xj + sigma ai 
求 min w b L w b a 对 a 
的 极大 max a 1/2 * sigma sigma aiajyiyj xi 
xj + sigma ai sigma aiyi = 0 转成 等价 
的 对偶 形式 就是 min a 1/2 * sigma sigma 
aiajyiyj xi xj sigma ai sigma aiyi = 0 假如 
求解 出来 的 a 为 a ^ = a1 a2 
an 则 得到 最优 的 w b 分别为 w ^ 
= sigma aiyixi b ^ = yj sigma aiyi xi 
xj 所以 最终 的 决策 分类 面为 f = sign 
sigma aiyi x xi + b ^ 也就是说 分类 决策函数 
只 依赖于 输入 x 与 训练 样本 的 输入 的 
内积 与 分离 超平面 最近 的 样本点 称为 支持 向量 
损失 函数 经验 损失 函数 sigma 1 yi wxi + 
b 注意 如果 该 值 小于 0时 直 接取 0 
即可 合页 损失 函数 sigma 1 yi wi + b 
+ leama | | w | | ^ 2 后面 
的 是 L2 正则 项 为什么 要 引入 对偶 算法 
对偶 问题 往往 更加 容易 求解 结合 拉格朗日 和 kkt 
条件 可以 很 自然 的 引用 核 函数 拉格朗日 表达式 
里面 有 内积 而 核 函数 也 是 通过 内积 
进行 映射 的 核 函数 将 输入 特征 x 线性 
不可分 映 射到 高维 特征 R 空间 可以 在 R 
空间 上 让 SVM 进行 线性 可以 变 这 就是 
核 函数 的 作用 多项式 核 函数 K x z 
= x * z + 1 ^ p 高斯 核 
函数 K x z = exp x z ^ 2 
/ a ^ 2 a 为 均值 字符串 核 函数 
好像 用于 文本 匹配 检索 之类 的 不懂 SVM 优缺点 
优点 使用 核 函数 可以向 高维空间 进行 映射 使用 核 
函数 可以 解决 非线性 的 分类 分类 思想 很 简单 
就是 将 样本 与 决策 面的 间隔 最大化 分类 效果 
较好 缺点 对 大 规模 数据 训练 比较 困难 因为 
它 是 用 二次 规划 来 求解 的 无法 直接 
支持 多 分类 但是 可以 使用 间接 的 方法 来做 
SMOSMO 是 用于 快速 求解 SVM 的 它 选择 凸 
二次 规划 的 两个 变量 其他 的 变量 保持 不变 
然后 根据 这 两个 变量 构建 一个 二 次 规划 
问题 这个 二 次 规划 关于 这 两个 变量 解会/nr 
更加 的 接近 原始 二次 规划 的 解 通过 这样 
的 子 问题 划分 可以 大大 增加 整个 算法 的 
计算速度 关于 这 两个 变量 其中 一个 是 严重 违反 
KKT 条件 的 一个 变量 另一个 变量 是 根据 自由 
约束 确定 好像 是 求 剩余 变量 的 最大化 来 
确定 的 SVM 多分 类 问题 直接 法 直接 在 
目标函数 上 进行 修改 将 多个 分类 面的 参数 求解 
合并 到 一个 最 优化 问题 中 通过 求解 该 
优化 就 可以 实现 多 分类 计算 复杂度 很高 实现 
起来 较为 困难 间 接法 一对多 其中 某个 类 为 
一类 其余 n 1个 类 为 另一个 类 比如 A 
B C D 四个 类 第一 次 A 为 一个 
类 { B C D } 为 一个 类 训练 
一个 分类器 第二 次 B 为 一个 类 { A 
C D } 为 另一个 类 按 这 方式 共 
需要 训练 4个 分类器 最后 在 测试 的 时候 将 
测试 样本 经过 这 4个 分类器 f1 x f2 x 
f3 x 和 f4 x 取其 最大值 为 分类器 这种 
方式 由于 是 1对 M 分类 会 存在 偏置 很 
不实用 一对一 libsvm 实现 的 方式 任意 两个 类 都 
训练 一个 分类器 那么 n 个 类 就需要 n * 
n 1 / 2 个 svm 分类器 还是 以 A 
B C D 为例 那么 需要 { A B } 
{ A C } { A D } { B 
C } { B D } { C D } 
为 目标 共 6个 分类器 然后 在 预测 的 将 
测试 样本 通过 这 6个 分类器 之后 进行 投票 选择 
最终 结果 这种方法 虽 好 但是 需要 n * n 
1 / 2 个 分类器 代价 太大 不过 有 好像 
使用 循环 图 来 进行 改进 决策树 决策树 是 一颗 
依托 决策 而 建立 起来 的 树 ID3 首先 是 
针对 当前 的 集合 计算 每个 特征 的 信息 增益 
然后 选择 信息 增益 最大 的 特征 作为 当前 节点 
的 决策 决策 特征 根据 特征 不同 的 类别 划分 
到 不同 的 子 节点 比如 年龄 特征 有 青年 
中年 老年 则 划分 到 3颗 子树 然后 继续 对子 
节点 进行 递归 直到 所有 特征 都被/nr 划分 C ai 
= sigma pilog pi 一个 属性 中 某个 类别 的 
熵 pi = P yi | ai pi 表示 ai 
情况 下 发生 yi 的 概率 也即 是 统计 概率 
C A = sigma P A = ai \ ai 
整个 属性 的 熵 为 各个 类别 的 比例 与 
各自 熵 的 加权 求和 Gain C A = C 
S C A 增益 表示 分类 目标 的 熵 减去 
当前 属性 的 熵 增益 越大 分类 能力 越强 这里 
前者 叫做 经验 熵 表示 数据集 分类 C 的 不确定性 
后者 就 是 经验 条件 熵 表示 在 给定 A 
的 条件 下 对 数据集 分类 C 的 不确定性 两者 
相减 叫做 互信息 决策树 的 增益 等价 于 互信息 比如说 
当前 属性 是 是否 拥有 房产 分类 是 是否 能 
偿还 债务 现在 有用 房产 为 7个 4个 能 偿还 
债务 3个 无法 偿还 债务 然后 无 房产 为 3个 
其中 1个 能 偿还 债务 2个 无法 偿还 债务 然后 
有 房产 = 4/7 * log4 / 7 + 3/7 
* log3 / 7 S 无 房产 = 1/3 * 
log1 / 3 + 2/3 * log2 / 3 其中 
分类 = 5/10 * log5 / 10 + 5/10 * 
log5 / 10 最终 的 增益 = 分类 7/10 * 
有 房产 + 3/10 * 无 房产 最大 越好 关于 
损失 函数 设 树 的 叶子 节点 个数 为 T 
t 为 其中 一个 叶子 节点 该 叶子 节点 有 
Nt 个 样本 其中 k 类 的 样本 有 Ntk 
个 H t 为 叶子 节点 上 的 经验 熵 
则 损失 函数 定义 为 Ct T = sigma Nt 
* H t + lamdba | T | 其中 H 
t = sigma Ntk / Nt * log Ntk / 
Nt 代入 可以 得到 Ct T = sigma sigma Ntk 
* log Ntk / Nt + lamdba | T | 
最终 有 Ct T = C T + lamdba | 
T | lamdba | T | 为 正则化 项 leama 
是 用于 调节 比率 决策树 的 生成 只 考虑 了 
信息 增益 C 4.5 它 是 ID3 的 一个 改进 
算法 使用 信息 增益 率 来 进行 属性 的 选择 
splitInformation S A = sigma | Si | / | 
| * log2 | Si | / | | GainRatio 
S A = Gain S A / splitInformation S A 
优缺点 准确率 高 但是 子 构造 树 的 过程 中 
需要 进行 多次 的 扫描 和 排序 所以 它 的 
运算 效率 较低 Cart 分类 回归 树 Classification And Regression 
Tree 是 一个 决策 二叉树 在 通过 递归 的 方式 
建立 每个/r 节/t 点在/i 分裂/v 的/uj 时候/n 都是/nr 希望/v 通过/p 
最好/a 的/uj 方式/n 将/d 剩余/v 的/uj 样本/n 划分/v 成/n 两类/m 
这里 的 分类 指标 分类 树 基尼指数 最小化 gini _ 
index 回归 树 平方 误差 最小化 分类 树 首先/d 是/v 
根据/p 当前/t 特征/n 计算/v 他们/r 的/uj 基尼/nz 增益/n 选择/v 基尼/nz 
增益/n 最小/a 的/uj 特征/n 作为/v 划分/v 特征/n 从该/nr 特征/n 中/f 
查找/v 基尼指数/i 最小/a 的/uj 分类/n 类别/n 作为/v 最优/d 划分/v 点将/n 
当前/t 样本/n 划分/v 成/n 两类/m 一类 是 划分 特征 的 
类别 等于 最优 划 分点 另一类 就是 不 等于 针对 
这 两类 递归 进行 上述 的 划分 工作 直达 所有 
叶子 指向 同一 样本 目标 或者 叶子 个数 小于 一定 
的 阈值 gini 用来 度量 分布 不 均匀性 或者说 不纯 
总体 的 类别 越 杂乱 GINI 指数 就 越大 跟 
熵 的 概念 很 相似 gini ai = 1 sigma 
pi ^ 2 pi 当前 数据 集中 第 i 类 
样本 的 比例 gini 越小 表示 样本分布 越 均匀 0 
的 时候 就 表示 只有 一类 了 越大 越 不均匀 
基尼 增益 gini _ gain = sigma Ni / N 
* gini ai 表示 当前 属性 的 一个 混乱 Ni 
/ N 表示 当前 类别 占 所有 类别 的 概率 
最终 Cart 选择 GiniGain 最小 的 特征 作为 划分 特征 
以 ID3 中的 贷款 的 那 棵树 为 样例 gini 
有 房产 = 1 3/7 ^ 2 + 4/7 ^ 
2 / / 基尼指数 gini 无 房产 = 1 1/3 
^ 2 + 2/3 ^ 2 gini _ gain = 
7/10 * gini 有 房产 + 3/10 * gini 无 
房产 / / 基尼 增益 回归 树 回归 树 是以 
平方 误差 最小化 的 准则 划分为 两块 区域 遍历 特征 
计算 最优 的 划分 点 s 使其 最小化 的 平方 
误差 是 min { min R1 . sigma yi c1 
^ 2 + min R2 . sigma yi c2 ^ 
2 } 计算 根据 s 划分 到 左侧 和 右侧 
子树 的 目标 值 与 预测 值 之差 的 平方和 
最小 这里 的 预测 值 是 两个 子树 上 输入 
xi 样本 对应 yi 的 均值 找到 最小 的 划分 
特征 j 以 及其 最优 的 划分 点 s 根据 
特征 j 以及 划 分点 s 将 现有 的 样本 
划分 为 两个 区域 一个 是 在 特征 j 上 
小于 等于 s 另 一个 在 在 特征 j 上 
大于 sR1 j = { x | x j = 
s } R2 j = { x | x j 
s } 进入 两 个子 区域 按 上述 方法 继续 
划分 直到 到达 停止 条件 这 里面 的 最小化 我 
记得 可以 使用 最 小二 乘法 来 求 关于 剪枝 
用 独立 的 验证 数据集 对 训练 集 生长 的 
树 进行 剪枝 事后 剪枝 停止/v 条件/n 直到/v 每个/r 叶子/nr 
节点/n 都/d 只有/c 一种/m 类型/n 的/uj 记录/n 时/n 停止/v 这种 
方式 很 容易 过拟合 另一种/i 时当/nr 叶子/nr 节点/n 的/uj 记录/n 
树/v 小于/v 一定/d 的/uj 阈值/n 或者/c 节点/n 的/uj 信息/n 增益/n 
小于/v 一定/d 的/uj 阈值/n 时/n 停止/v 关于/p 特征/n 与/p 目标值/n 
特征/n 离散/v 目标值 离散 可以 使用 ID3 cart 特征 连续 
目标值 离散 将 连续 的 特征 离散化 可以 使用 ID3 
cart 特征 离散 目标值 连续 决策树 的 分类 与 回归 
分类 树 输出 叶子 节 点中 所属 类别 最多 的 
那一类 回归 树 输出 叶子 节点 中 各个 样 本值 
的 平均值 理想 的 决策 树叶子 节 点数 尽量少 叶子 
节点 的 深度 尽 量小 太深 可能会 过拟合 解决 决策树 
的 过拟合 剪枝 前置 剪枝 在 分裂 节点 的 时候 
设计 比较 苛刻 的 条件 如 不满足 则 直接 停止 
分裂 这样 干 决策树 无法 到 最优 也 无法 得到 
比较 好 的 效果 后置 剪枝 在 树 建立 完 
之后 用 单个 节点 代替 子树 节点 的 分类 采用 
子树 中 主要 的 分类 这种 方法 比较 浪费 前面 
的 建立 过程 交叉 验证 随机 森林 优缺点 优点 计算 
量 简单 可解释 性强 比较 适合 处理 有 缺失 属性值 
的 样本 能够 处理 不 相关 的 特征 缺点 单颗/nr 
决策树 分类 能力 弱 并且 对 连续 值 变量 难以 
处理 容易 过拟合 后续 出现 了 随机 森林 减小 了 
过拟合 现象 随机 森林 RF 随机 森林 是 有 很多 
随机 得 决策树 构成 它们 之间 没有 关联 得到 RF 
以后 在 预测 时 分别 对 每一个 决策树 进行 判断 
最后 使用 Bagging 的 思想 进行 结果 的 输出 也 
就是 投票 的 思想 学习 过程 现在 有N个/nr 训练样本 每个 
样本 的 特征 为 M 个 需要 建 K 颗 
树 从N个/nr 训练样本 中有 放回 的 取 N 个 样本 
作为 一组 训练 集 其余 未 取到 的 样本 作为 
预测 分类 评估 其 误差 从M个/nr 特征 中 取 m 
个 特征 左右 子集 特征 m M 对 采样 的 
数据 使用 完全 分裂 的 方式 来 建立 决策树 这样 
的 决策树 每个 节点 要么 无法 分裂 要么 所有 的 
样本 都 指向 同一 个 分类 重复 2 的 过程 
K 次 即可 建立 森林 预测 过程 将 预测 样本 
输入 到 K 颗 树 分别 进行 预测 如果 是 
分类 问题 直接 使用 投票 的 方式 选择 分类 频次 
最高 的 类别 如果 是 回归 问题 使用 分类 之后 
的 均值 作为 结果 参数 问题 这里 的 一般 取 
m = sqrt M 关于 树 的 个数 K 一般 
都 需要 成百上千 但是 也 有 具体 的 样本 有关 
比如 特征 数量 树 的 最大 深度 太深 可能 可能 
导致 过拟合 节点 上 的 最小 样本数 最小 信息 增益 
泛化 误差 估计 使用 oob out of bag 进行 泛化 
误差 的 估计 将 各个 树 的 未 采样 样本 
作为 预测 样本 大约有 36.8% 使用 已经 建立 好 的 
森林 对 各个 预测 样本 进行 预测 预测 完 之后 
最 后 统计 误 分得 个数 占 总 预测 样本 
的 比率 作为 RF 的 oob 误 分率 学习 算法 
ID3 算法 处理 离散 值 的 量 C45 算法 处理 
连续 值 的 量 Cart 算法 离散 和 连续 两者 
都 合适 关于 CARTCart 可以 通过 特征 的 选择 迭代 
建立 一颗 分类 树 使得 每次 的 分类 平面 能 
最好 的 将 剩余 数据 分为 两类 gini = 1 
sigma pi ^ 2 表示 每个 类别 出现 的 概率 
和与1/nr 的 差值 分类 问题 argmax Gini GiniLeft GiniRight 回归 
问题 argmax Var VarLeft VarRight 查找 最佳 特征 f 已经 
最佳 属性 阈值 th 小于 th 的 在 左边 大于 
th 的 在 右边 子树 优缺点 能够 处理 大量 特征 
的 分类 并且/c 还/d 不用/v 做/v 特征选择/nr 在/p 训练/vn 完成/v 
之后/f 能/v 给出/v 哪些/r feature/w 的/uj 比较/d 重要/a 训练/vn 速度/n 
很快/d 很/zg 容易/a 并行/v 实现/v 相对来说/l 较为简单/z GBDTGBDT/w 的/uj 精髓/n 
在于/v 训练/vn 的/uj 时候/n 都是/nr 以上/f 一颗/m 树/v 的/uj 残差/n 
为/p 目标/n 这个 残差 就是 上 一个 树 的 预测 
值 与 真实 值 的 差值 比如 当前 样本 年龄 
是 18岁 那么 第一 颗 会去 按 18岁 来 训练 
但是 训练 完 之后 预测 的 年龄 为 12岁 差值 
为 6 所以 第二颗 树 的 会 以 6岁 来 
进行 训练 假如 训练 完 之后 预测 出来 Boosting 的 
好处 就是 每 一步 的 参加 就是 变相 了 增加 
了 分 错 instance 的 权重 而对 已经 对 的 
instance 趋向于 0 这样 后面 的 树 就 可以 更加 
关注 错分 的 instance 的 训练 了 h r i 
n k a g e h r i n k 
a g e 认为 每次 走 一 小步 逐步 逼近 
的 结果 要比 每次 迈 一大步 逼近 结果 更加 容易 
避免 过拟合 y 1 ~ i = y 1 ~ 
i 1 + step * yi 就像 我们 做 互联网 
总是 先 解决 60% 用户 的 需求 凑合着 再 解决 
35% 用户 的 需求 最后 才 关注 那 5% 人 
的 需求 这样 就 能 逐渐 把 产品 做好 . 
调 参 树 的 个数 100 ~ 10000 叶子 的 
深度 3 ~ 8 学习 速率 0.01 ~ 1 叶子 
上 最大 节点 树 20 训练 采样 比例 0.5 ~ 
1 训练 特征 采样 比例 sqrt num 优缺点 优点 精度高/n 
能/v 处理/v 非线性/b 数据/n 能/v 处理/v 多/m 特征/n 类型/n 适合/v 
低维/i 稠密/a 数据/n 缺点/n 并行 麻烦 因为 上下 两颗 树 
有联系 多 分类 的 时候 复杂度/n 很大/a BP/w 最小二乘/i 法/l 
最小二乘/i 法是/nr 一种/m 数学/n 的/uj 优化/vn 技术/n 通过 求 最小化 
平方 误差 来 寻找 最佳 的 函数 匹配 假设 现在 
有 二维 的 观测 数据 x1 y1 x2 y2 xn 
yn 求 y = a + bx 的 拟合 现 
设 yi = a + bxi + ki 如果 有a/nr 
b 能 得到 sigma | ki | 最小 则 该线 
比较 理想 所以 先 变 为求 min sigma ki 这个 
与 min sigma ki ^ 2 等价 而 ki = 
yi a + bxi 那么 现 设 f = sigma 
yi a + bxi ^ 2 求其 最小 即可 上述 
就是 最小二乘 原则 估计 a b 的 方法 称为 最 
小二 乘法 先 求 f 对 a b 的 偏 
导 f a = 2 * sigma yi a + 
bxi = 0f b = 2 * xi * sigma 
yi a + bxi = 0 现 设 X = 
sigma xi / n Y = sigma yi / 则 
代入 上述 偏 导 an + bnX = nYanX + 
b * sigma xi ^ 2 = sigma xi * 
yi 求 该 行列式 | n nX | | nX 
sigma xi ^ 2 | = n * sigma xi 
X = 0 所以/c 有/v 唯一/b 解最/nr 后记/n l xx 
= sigma xi X ^ 2 l yy = sigma 
yi Y ^ 2 l xy = sigma xi X 
yi Y 则 b = l xy / l xx 
a = Y bX 百度 文库 最小二乘 法 EMEM 用于 
隐含 变量 的 概率 模型 的 极大 似 然 估计 
它 一般 分为 两步 第一步 求 期望 E 第二步 求 
极大 M 如果 概率模型 的 变量 都是 观测 变量 那么 
给定 数据 之后 就 可以 直接 使用 极大 似 然 
法 或者 贝叶斯 估计 模型 参数 但是 当 模型 含有 
隐含 变量 的 时候 就 不能 简单 的 用 这些 
方法 来 估计 EM 就是 一种 含有 隐含 变量 的 
概率模型 参数 的 极大 似 然 估计法 应用到 的 地方 
混合 高斯 模型 混合 朴素 贝叶斯 模型 因子/n 分析模型/n Bagging/w 
从N/nr 样本/n 中有/i 放回/v 的/uj 采样/v N/w 个/q 样本/n 对这/i 
N/w 个/q 样本/n 在/p 全/a 属性/n 上/f 建立/v 分类器/n CART 
SVM 重复 上面 的 步骤 建立 m 个 分类器 预测 
的 时候 使用 投票 的 方法 得到 结果 Boostingboosting 在 
训练 的 时候 会给 样本 加 一个 权重 然后 使 
loss function 尽量 去 考虑 那些 分 错 类 的 
样本 比如 给分 错 类 的 样本 的 权重 值 
加大 凸 优化 在 机器 学习 中 往往 是 最终 
要 求解 某个 函数 的 最优 值 但是 一般 情况 
下 任意 一个 函数 的 最优 值 求解 比较 困难 
但是 对于 凸函数 来说 就 可以 有效 的 求 解出 
全局 最优 值 凸 集 一个 集合 C 是 当前 
仅 当 任意 x y 属于 C 且 0 = 
theta = 1 都有 theta * x + 1 theta 
* y 属于 C 用 通俗 的话 来说 C 集合 
线段 上 的 任意 两点 也在 C 集合 中 凸函数 
一个 函数 f 其 定义域 D f 是 凸 集 
并且 对 任意 x y 属于 D f 和0/nr = 
theta = 1 都有 f theta * x + 1 
theta * y = theta * f x + 1 
theta * f y 这个 貌似 叫做 jensen 不等式 用 
通俗 的话 来说 就是 曲线 上 任意 两点 的 割线 
都在 曲线 的 上方 常见 的 凸函数 有 指数函数 f 
x = a ^ x a 1 负 对数函数 logax 
a 1 x 0 开口 向上 的 二次函数 等 凸函数 
的 判定 如果 f 是 一 阶 可导 对于 任意 
数据 域内 的 x y 满足 f y = f 
x + f x y x 如果 f 是 二阶 
可导 凸 优化 应用 举例 SVM 其中 由 max | 
w | 转向 min 1/2 * | w | ^ 
2 最小二乘 法 LR 的 损失 函数 sigma yi * 
log hw x + 1 yi * log 1 hw 
x 