朴素贝叶斯
P(A∩B)=P(A)*P(B|A)=P(B)*P(A|B)
所以有：P(A|B)=P(B|A)*P(A)/P(B)
对于给出的待分类项，求解在此项出现的条件下各个目标类别出现的概率，哪个最大，就认为此待分类项属于哪个类别
工作原理
假设现在有样本x=(a1,a2,a3,…an)这个待分类项(并认为x里面的特征独立)
再假设现在有分类目标Y={y1,y2,y3,y4..yn}
那么max(P(y1|x),P(y2|x),P(y3|x)..P(yn|x))中的最大者就是最终的分类类别
而P(yi|x)=p(x|yi)*P(yi)/P(x)
因为x对于每个分类目标来说都一样，所以就是求max(P(x|yi)*p(yi))
P(x|yi)*p(yi)=p(yi)*PI(P(ai|yi)) (PI表示连乘)
而具体的p(ai|yi)和p(yi)都是能从训练样本中统计出来
p(ai|yi)表示该类别下该特征出现的概率
p(yi)表示全部类别中这个这个类别出现的概率
好的，就是这么工作的^_^
工作流程
准备阶段
确定特征属性，并对每个特征属性进行适当划分，然后由人工对一部分待分类项进行分类，形成训练样本。
训练阶段
计算每个类别在训练样本中的出现频率及每个特征属性划分对每个类别的条件概率估计
应用阶段
使用分类器进行分类，输入是分类器和待分类样本，输出是样本属于的分类类别
属性特征
特征为离散值时直接统计即可（表示统计概率）
特征为连续值的时候假定特征符合高斯分布:g(x,n,u)
那么p(ak|yi)=g(xk,ni,ui)
Laplace校准(拉普拉斯校验)
当某个类别下某个特征划分没有出现时，会有P(a|y)=0，就是导致分类器质量降低，所以此时引入Laplace校验，就是对没类别下所有划分的计数加1。
遇到特征之间不独立问题
参考改进的贝叶斯网络，使用DAG来进行概率图的描述
优缺点
朴素贝叶斯的优点：
对小规模的数据表现很好，适合多分类任务，适合增量式训练。
缺点：
对输入数据的表达形式很敏感（离散、连续，值极大极小之类的）。
http://www.cnblogs.com/leoo2sk/archive/2010/09/17/naive-bayesian-classifier.html
逻辑回归和线性回归
LR回归是一个线性的二分类模型，主要是计算在某个样本特征下事件发生的概率，比如根据用户的浏览购买情况作为特征来计算它是否会购买这个商品，抑或是它是否会点击这个商品。然后LR的最终值是根据一个线性和函数再通过一个sigmod函数来求得，这个线性和函数权重与特征值的累加以及加上偏置求出来的，所以在训练LR时也就是在训练线性和函数的各个权重值w。
关于这个权重值w一般使用最大似然法来估计，比如yi=1的概率是pi,则yi=0的概率是1-pi，那么观测概率为p(yi)=pi^yi*(1-pi)^(1-yi)这个这个最大似然函数为（hw(xi)^yi*(1-hw(xi))^(1-yi)）连乘，对这个似然函数取对数之后就会得到的表达式L(w)=sigma(yi*log(hw(xi))-(1-yi)log(1-hw(xi)))=sigma(yi*(w*xi)-log(1+exp(w*xi)))，估计这个L(w)的极大值就可以得到w的估计值。
所以求解问题就变成了这个最大似然函数的最优化问题，这里通常会采样随机梯度下降法和拟牛顿迭代法来进行优化
梯度下降法
如果hw(x)=1/(1-e^(-wx))，
则cost function=-1/m* sigma(yi*log(hw(xi)+(1-yi)*log(1-hw(xi)))=j(w)
这里就成了就min(j(w))
所以更新w的过程为
w:=w-lamea*j(w)’ (求导)
w:=w-lamea* 1/m\*sigma[m](hw(xi)-yi)*xi)
直到j(w)不能再的时候停止
梯度下降法的最大问题就是会陷入局部最优，并且每次在对当前样本计算cost的时候都需要去遍历全部样本才能得到cost值，这样计算速度就会慢很多（虽然在计算的时候可以转为矩阵乘法去更新整个w值）
所以现在好多框架（mahout）中一般使用随机梯度下降法，它在计算cost的时候只计算当前的代价，最终cost是在全部样本迭代一遍之求和得出，还有他在更新当前的参数w的时候并不是依次遍历样本，而是从所有的样本中随机选择一条进行计算，它方法收敛速度快（一般是使用最大迭代次数），并且还可以避免局部最优，并且还很容易并行（使用参数服务器的方式进行并行）
这里SGD可以改进的地方就是使用动态的梯度值alpha=0.04*(1.0+n+i)+Rate
其他优化方法
拟牛顿法（记得是需要使用Hessian矩阵和cholesky分解）
BFGS
L-BFGS
优缺点：无需选择学习率α，更快，但是更复杂
关于LR的过拟合问题：
如果我们有很多的特性，在训练集上拟合得很好，但是在预测集上却达不到这种效果
1. 减少feature个数（人工定义留多少个feature、算法选取这些feature）
2. 正则化（留下所有的feature，但对于部分feature定义其parameter非常小），在cost上加 lamea(sigma(w^2))，同时w的更新变为w:=w-rate* 1/m\*sigma[m](hw(xi)-yi)*xi+ （lamea/m)*w。注意：这里的w0不受正则化影响
关于LR的多分类：softmax
softmax:假设离散型随机变量Y的取值集合是{1,2,..,k},则多分类的LR为
P(Y=a|x)=exp(wa*x)/(1-1到k求和(wk*x)) 1<a<k
这里会输出当前样本下属于哪一类的概率，并且满足全部概率加起来=1
关于softmax和k个LR的选择
如果类别之间是否互斥（比如音乐只能属于古典音乐、乡村音乐、摇滚月的一种）就用softmax
否则类别之前有联系（比如一首歌曲可能有影视原声，也可能包含人声，或者是舞曲），这个时候使用k个LR更为合适
优缺点：
Logistic回归优点：
实现简单；
分类时计算量非常小，速度很快，存储资源低；
缺点：
容易欠拟合，一般准确度不太高
只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分；
http://www.cnblogs.com/biyeymyhjob/archive/2012/07/18/2595410.html
http://blog.csdn.net/abcjennifer/article/details/7716281
http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92
KNN算法
给一个训练数据集和一个新的实例，在训练数据集中找出与这个新实例最近的k个训练实例，然后统计最近的k个训练实例中所属类别计数最多的那个类，就是新实例的类
三要素：
k值的选择
距离的度量（常见的距离度量有欧式距离，马氏距离等）
分类决策规则 （多数表决规则）
k值的选择
k值越小表明模型越复杂，更加容易过拟合
但是k值越大，模型越简单，如果k=N的时候就表明无论什么点都是训练集中类别最多的那个类
所以一般k会取一个较小的值，然后用过交叉验证来确定
这里所谓的交叉验证就是将样本划分一部分出来为预测样本，比如95%训练，5%预测，然后k分别取1，2，3，4，5之类的，进行预测，计算最后的分类误差，选择误差最小的k
KNN的回归
在找到最近的k个实例之后，可以计算这k个实例的平均值作为预测值。或者还可以给这k个实例添加一个权重再求平均值，这个权重与度量距离成反比（越近权重越大）。
优缺点：
KNN算法的优点：
思想简单，理论成熟，既可以用来做分类也可以用来做回归；
可用于非线性分类；
训练时间复杂度为O(n)；
准确度高，对数据没有假设，对outlier不敏感；
缺点：
计算量大；
样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）；
需要大量的内存；
KD树
KD树是一个二叉树，表示对K维空间的一个划分，可以进行快速检索（那KNN计算的时候不需要对全样本进行距离的计算了）
构造KD树
在k维的空间上循环找子区域的中位数进行划分的过程。
假设现在有K维空间的数据集T={x1,x2,x3,…xn},xi={a1,a2,a3..ak}
首先构造根节点，以坐标a1的中位数b为切分点，将根结点对应的矩形局域划分为两个区域，区域1中a1b
构造叶子节点，分别以上面两个区域中a2的中位数作为切分点，再次将他们两两划分，作为深度1的叶子节点，（如果a2=中位数，则a2的实例落在切分面）
不断重复2的操作，深度为j的叶子节点划分的时候，索取的ai 的i=j%k+1，直到两个子区域没有实例时停止
KD树的搜索
首先从根节点开始递归往下找到包含x的叶子节点，每一层都是找对应的xi
将这个叶子节点认为是当前的“近似最近点”
递归向上回退，如果以x圆心，以“近似最近点”为半径的球与根节点的另一半子区域边界相交，则说明另一半子区域中存在与x更近的点，则进入另一个子区域中查找该点并且更新”近似最近点“
重复3的步骤，直到另一子区域与球体不相交或者退回根节点
最后更新的”近似最近点“与x真正的最近点
KD树进行KNN查找
通过KD树的搜索找到与搜索目标最近的点，这样KNN的搜索就可以被限制在空间的局部区域上了，可以大大增加效率。
KD树搜索的复杂度
当实例随机分布的时候，搜索的复杂度为log(N)，N为实例的个数，KD树更加适用于实例数量远大于空间维度的KNN搜索，如果实例的空间维度与实例个数差不多时，它的效率基于等于线性扫描。
SVM、SMO
对于样本点(xi,yi)以及svm的超平面：wix+b=0
函数间隔：yi(wxi+b)
几何间隔：yi(wxi+b)/||w||,其中||w||为w的L2范数，几何间隔不会因为参数比例的改变而改变
svm的基本想法就是求解能正确划分训练样本并且其几何间隔最大化的超平面。
线性SVM问题
yi(wxi+b)/||w||>=d （使用几何间隔）
求max(d)
那么假设d’=d||w||
则将问题转为：yi(wxi+b)>=1,max(d’/||w||)
由于d’的成比例增减不会影响实际间距，所以这里的取d’=1，又因为max(1/||w||)=min(1/2\||w||^2)
所以最终的问题就变为了
yi(wxi+b)>=1,min(1/2*||w||^2)
这样就变成了一个凸的二次规划化，可以将其转换为拉格朗日函数，然后使用对偶算法来求解
对偶求解
L(w,b,a)=1/2*||w||^2-sigma(ai*yi(wxi+b))+sigma(ai) 其中a={a1,a2..an}为拉格朗日向量
根据对偶性质 原始问题就是求对偶问题的极大极小max[a]min[w,b]L(w,b,a)
先求L对w,b的极小，再求对a的极大
求min[w,b]L(w,b,a)：
L’(w)=w-sigma(aiyixi)=0
L’(b)=sigma(aiyi)=0;
代入后可得min[w,b]L(w,b,a)=-1/2*sigma(sigma(aiajyiyj(xi·xj)))+sigma(ai)
求min[w,b]L(w,b,a)对a的极大
max[a] -1/2*sigma(sigma(aiajyiyj(xi·xj)))+sigma(ai)
sigma(aiyi)=0
转成等价的对偶形式就是
min[a] 1/2*sigma(sigma(aiajyiyj(xi·xj)))-sigma(ai)
sigma(aiyi)=0
假如求解出来的a为a^=(a1,a2,…an)
则得到最优的w,b分别为
w^=sigma(aiyixi)
b^=yj-sigma(aiyi(xi·xj))
所以，最终的决策分类面为
f=sign(sigma(aiyi(x·xi))+b^
也就是说，分类决策函数只依赖于输入x与训练样本的输入的内积
与分离超平面最近的样本点称为支持向量
损失函数
经验损失函数:sigma(1-yi(wxi+b)) (注意，如果该值小于0时直接取0即可)
合页损失函数：sigma(1-yi(wi+b)) + leama||w||^2 后面的是L2正则项
为什么要引入对偶算法
对偶问题往往更加容易求解(结合拉格朗日和kkt条件)
可以很自然的引用核函数（拉格朗日表达式里面有内积，而核函数也是通过内积进行映射的）
核函数
将输入特征x（线性不可分）映射到高维特征R空间，可以在R空间上让SVM进行线性可以变，这就是核函数的作用
多项式核函数:K(x,z)=(x*z+1)^p
高斯核函数:K(x,z)=exp(-(x-z)^2/a^2) a为均值
字符串核函数：好像用于文本匹配、检索之类的，不懂
SVM优缺点
优点：
使用核函数可以向高维空间进行映射
使用核函数可以解决非线性的分类
分类思想很简单，就是将样本与决策面的间隔最大化
分类效果较好
缺点：
对大规模数据训练比较困难，因为它是用二次规划来求解的
无法直接支持多分类，但是可以使用间接的方法来做
SMO
SMO是用于快速求解SVM的
它选择凸二次规划的两个变量，其他的变量保持不变，然后根据这两个变量构建一个二次规划问题，这个二次规划关于这两个变量解会更加的接近原始二次规划的解，通过这样的子问题划分可以大大增加整个算法的计算速度，关于这两个变量：
其中一个是严重违反KKT条件的一个变量
另一个变量是根据自由约束确定，好像是求剩余变量的最大化来确定的。
SVM多分类问题
直接法
直接在目标函数上进行修改，将多个分类面的参数求解合并到一个最优化问题中，通过求解该优化就可以实现多分类（计算复杂度很高，实现起来较为困难）
间接法
一对多
其中某个类为一类，其余n-1个类为另一个类，比如A,B,C,D四个类，第一次A为一个类，{B,C,D}为一个类训练一个分类器，第二次B为一个类,{A,C,D}为另一个类,按这方式共需要训练4个分类器，最后在测试的时候将测试样本经过这4个分类器f1(x),f2(x),f3(x)和f4(x),取其最大值为分类器(这种方式由于是1对M分类，会存在偏置，很不实用)
一对一(libsvm实现的方式)
任意两个类都训练一个分类器，那么n个类就需要n*(n-1)/2个svm分类器。
还是以A,B,C,D为例,那么需要{A,B},{A,C},{A,D},{B,C},{B,D},{C,D}为目标共6个分类器，然后在预测的将测试样本通过这6个分类器之后进行投票选择最终结果。（这种方法虽好，但是需要n*(n-1)/2个分类器代价太大，不过有好像使用循环图来进行改进）
决策树
决策树是一颗依托决策而建立起来的树。
ID3
首先是针对当前的集合，计算每个特征的信息增益
然后选择信息增益最大的特征作为当前节点的决策决策特征
根据特征不同的类别划分到不同的子节点（比如年龄特征有青年，中年，老年，则划分到3颗子树）
然后继续对子节点进行递归，直到所有特征都被划分
S(C,ai)=-sigma(pilog(pi)) 一个属性中某个类别的熵 pi=P(yi|ai) pi表示ai情况下发生yi的概率，也即是统计概率
S(C,A)=sigma(P(A=ai)\S(ai)) 整个属性的熵，为各个类别的比例与各自熵的加权求和
Gain(C,A)=S(C)-S(C,A) 增益表示分类目标的熵减去当前属性的熵，增益越大，分类能力越强
(这里前者叫做经验熵，表示数据集分类C的不确定性，后者就是经验条件熵，表示在给定A的条件下对数据集分类C的不确定性，两者相减叫做互信息，决策树的增益等价于互信息)
比如说当前属性是是否拥有房产，分类是是否能偿还债务
现在：
有用房产为7个，4个能偿还债务，3个无法偿还债务
然后无房产为3个，其中1个能偿还债务，2个无法偿还债务
然后S(有房产)=-(4/7*log4/7+3/7*log3/7)
S(无房产)=-(1/3*log1/3+2/3*log2/3)
其中S(分类)=-(5/10*log5/10+5/10*log5/10)
最终的增益=S(分类)-(7/10*S(有房产)+3/10*S(无房产)) 最大越好
关于损失函数
设树的叶子节点个数为T，t为其中一个叶子节点，该叶子节点有Nt个样本，其中k类的样本有Ntk个，H(t)为叶子节点上的经验熵，则损失函数定义为
Ct(T)=sigma(Nt*H(t))+ lamdba |T|
其中H(t)=sigma(Ntk/Nt*log(Ntk/Nt))
代入可以得到Ct(T)=sigma(sigma(Ntk*log(Ntk/Nt)))+lamdba|T|
最终有Ct(T)=C(T)+ lamdba|T|
lamdba|T|为正则化项，leama是用于调节比率
决策树的生成只考虑了信息增益
C4.5
它是ID3的一个改进算法，使用信息增益率来进行属性的选择
splitInformation(S,A)=-sigma(|Si|/|S|*log2(|Si|/|S|))
GainRatio(S,A)=Gain(S,A)/splitInformation(S,A)
优缺点：
准确率高，但是子构造树的过程中需要进行多次的扫描和排序，所以它的运算效率较低
Cart
分类回归树(Classification And Regression Tree)是一个决策二叉树，在通过递归的方式建立，每个节点在分裂的时候都是希望通过最好的方式将剩余的样本划分成两类，这里的分类指标：
分类树：基尼指数最小化(gini_index)
回归树：平方误差最小化
分类树：
首先是根据当前特征计算他们的基尼增益
选择基尼增益最小的特征作为划分特征
从该特征中查找基尼指数最小的分类类别作为最优划分点
将当前样本划分成两类，一类是划分特征的类别等于最优划分点，另一类就是不等于
针对这两类递归进行上述的划分工作，直达所有叶子指向同一样本目标或者叶子个数小于一定的阈值
gini用来度量分布不均匀性（或者说不纯），总体的类别越杂乱，GINI指数就越大（跟熵的概念很相似）
gini(ai)=1-sigma(pi^2) pi当前数据集中第i类样本的比例
gini越小，表示样本分布越均匀（0的时候就表示只有一类了），越大越不均匀
基尼增益gini_gain=sigma(Ni/N*gini(ai)) 表示当前属性的一个混乱 Ni/N表示当前类别占所有类别的概率
最终Cart选择GiniGain最小的特征作为划分特征
以ID3中的贷款的那棵树为样例：
gini(有房产)=1-((3/7)^2+(4/7)^2) //基尼指数
gini(无房产)=1-((1/3)^2+(2/3)^2)
gini_gain=7/10*gini(有房产)+3/10*gini(无房产) //基尼增益
回归树：
回归树是以平方误差最小化的准则划分为两块区域
遍历特征计算最优的划分点s，
使其最小化的平方误差是：min{min(R1.sigma((yi-c1)^2))+min(R2.sigma((yi-c2)^2))}
计算根据s划分到左侧和右侧子树的目标值与预测值之差的平方和最小，这里的预测值是两个子树上输入xi样本对应yi的均值
找到最小的划分特征j以及其最优的划分点s,根据特征j以及划分点s将现有的样本划分为两个区域，一个是在特征j上小于等于s，另一个在在特征j上大于s
R1(j)={x|x(j)<=s}、R2(j)={x|x(j)>s}
进入两个子区域按上述方法继续划分，直到到达停止条件
这里面的最小化我记得可以使用最小二乘法来求
关于剪枝：用独立的验证数据集对训练集生长的树进行剪枝（事后剪枝）。
停止条件
直到每个叶子节点都只有一种类型的记录时停止，（这种方式很容易过拟合）
另一种时当叶子节点的记录树小于一定的阈值或者节点的信息增益小于一定的阈值时停止
关于特征与目标值
特征离散 目标值离散：可以使用ID3，cart
特征连续 目标值离散：将连续的特征离散化 可以使用ID3，cart
特征离散 目标值连续
决策树的分类与回归
分类树
输出叶子节点中所属类别最多的那一类
回归树
输出叶子节点中各个样本值的平均值
理想的决策树
叶子节点数尽量少
叶子节点的深度尽量小(太深可能会过拟合)
解决决策树的过拟合
剪枝
前置剪枝：在分裂节点的时候设计比较苛刻的条件，如不满足则直接停止分裂（这样干决策树无法到最优，也无法得到比较好的效果）
后置剪枝：在树建立完之后，用单个节点代替子树，节点的分类采用子树中主要的分类（这种方法比较浪费前面的建立过程）
交叉验证
随机森林
优缺点
优点：
计算量简单，可解释性强，比较适合处理有缺失属性值的样本，能够处理不相关的特征；
缺点：
单颗决策树分类能力弱，并且对连续值变量难以处理；
容易过拟合（后续出现了随机森林，减小了过拟合现象）；
随机森林RF
随机森林是有很多随机得决策树构成，它们之间没有关联。得到RF以后，在预测时分别对每一个决策树进行判断，最后使用Bagging的思想进行结果的输出（也就是投票的思想）
学习过程
现在有N个训练样本，每个样本的特征为M个，需要建K颗树
从N个训练样本中有放回的取N个样本作为一组训练集（其余未取到的样本作为预测分类，评估其误差）
从M个特征中取m个特征左右子集特征(m<<M)
对采样的数据使用完全分裂的方式来建立决策树，这样的决策树每个节点要么无法分裂，要么所有的样本都指向同一个分类
重复2的过程K次，即可建立森林
预测过程
将预测样本输入到K颗树分别进行预测
如果是分类问题，直接使用投票的方式选择分类频次最高的类别
如果是回归问题，使用分类之后的均值作为结果
参数问题
这里的一般取m=sqrt(M)
关于树的个数K，一般都需要成百上千，但是也有具体的样本有关（比如特征数量）
树的最大深度，（太深可能可能导致过拟合？？）
节点上的最小样本数、最小信息增益
泛化误差估计
使用oob（out-of-bag）进行泛化误差的估计，将各个树的未采样样本作为预测样本（大约有36.8%），使用已经建立好的森林对各个预测样本进行预测，预测完之后最后统计误分得个数占总预测样本的比率作为RF的oob误分率。
学习算法
ID3算法：处理离散值的量
C45算法：处理连续值的量
Cart算法：离散和连续 两者都合适？
关于CART
Cart可以通过特征的选择迭代建立一颗分类树，使得每次的分类平面能最好的将剩余数据分为两类
gini=1-sigma(pi^2)，表示每个类别出现的概率和与1的差值，
分类问题：argmax（Gini-GiniLeft-GiniRight）
回归问题argmax(Var-VarLeft-VarRight)
查找最佳特征f已经最佳属性阈值th 小于th的在左边，大于th的在右边子树
优缺点
能够处理大量特征的分类，并且还不用做特征选择
在训练完成之后能给出哪些feature的比较重要
训练速度很快
很容易并行
实现相对来说较为简单
GBDT
GBDT的精髓在于训练的时候都是以上一颗树的残差为目标，这个残差就是上一个树的预测值与真实值的差值。
比如，当前样本年龄是18岁，那么第一颗会去按18岁来训练，但是训练完之后预测的年龄为12岁，差值为6，所以第二颗树的会以6岁来进行训练，假如训练完之后预测出来
Boosting的好处就是每一步的参加就是变相了增加了分错instance的权重，而对已经对的instance趋向于0，这样后面的树就可以更加关注错分的instance的训练了
Shrinkage
Shrinkage认为，每次走一小步逐步逼近的结果要比每次迈一大步逼近结果更加容易避免过拟合。
y(1 ~ i) = y(1 ~ i-1) + step * yi
就像我们做互联网，总是先解决60%用户的需求凑合着，再解决35%用户的需求，最后才关注那5%人的需求，这样就能逐渐把产品做好.
调参
树的个数 100~10000
叶子的深度 3~8
学习速率 0.01~1
叶子上最大节点树 20
训练采样比例 0.5~1
训练特征采样比例 sqrt(num)
优缺点：
优点：
精度高
能处理非线性数据
能处理多特征类型
适合低维稠密数据
缺点：
并行麻烦（因为上下两颗树有联系）
多分类的时候 复杂度很大
BP
最小二乘法
最小二乘法是一种数学的优化技术，通过求最小化平方误差来寻找最佳的函数匹配
假设现在有二维的观测数据(x1,y1),(x2,y2)…(xn,yn)，求y=a+bx的拟合。
现设yi=a+bxi+ki 如果有a,b能得到sigma(|ki|)最小，则该线比较理想
所以先变为求min(sigma(ki)) ，这个与min(sigma(ki^2))等价
而ki=yi-(a+bxi)
那么现设f=sigma((yi-(a+bxi))^2)求其最小即可
上述就是最小二乘原则，估计a,b的方法称为最小二乘法
先求f对a,b的偏导：
f’(a)=-2*sigma(yi-(a+bxi))=0
f’(b)=-2*xi*sigma(yi-(a+bxi))=0
现设：X=sigma(xi)/n Y=sigma(yi)/
则代入上述偏导：
an+bnX=nY
anX+b*sigma(xi^2)=sigma(xi*yi)
求该行列式:
|n ,nX |
|nX,sigma(xi^2)|
=n*sigma((xi-X))!=0 所以有唯一解
最后记：
l(xx)=sigma((xi-X)^2)
l(yy)=sigma((yi-Y)^2)
l(xy)=sigma((xi-X)(yi-Y))
则b=l(xy)/l(xx) a=Y-bX
百度文库-最小二乘法
EM
EM用于隐含变量的概率模型的极大似然估计，它一般分为两步：第一步求期望(E),第二步求极大(M)，
如果概率模型的变量都是观测变量，那么给定数据之后就可以直接使用极大似然法或者贝叶斯估计模型参数。
但是当模型含有隐含变量的时候就不能简单的用这些方法来估计，EM就是一种含有隐含变量的概率模型参数的极大似然估计法。
应用到的地方：混合高斯模型、混合朴素贝叶斯模型、因子分析模型
Bagging
从N样本中有放回的采样N个样本
对这N个样本在全属性上建立分类器(CART,SVM)
重复上面的步骤，建立m个分类器
预测的时候使用投票的方法得到结果
Boosting
boosting在训练的时候会给样本加一个权重，然后使loss function尽量去考虑那些分错类的样本（比如给分错类的样本的权重值加大）
凸优化
在机器学习中往往是最终要求解某个函数的最优值，但是一般情况下，任意一个函数的最优值求解比较困难，但是对于凸函数来说就可以有效的求解出全局最优值。
凸集
一个集合C是，当前仅当任意x,y属于C且0<=theta<=1，都有theta*x+(1-theta)*y属于C
用通俗的话来说C集合线段上的任意两点也在C集合中
凸函数
一个函数f其定义域(D(f))是凸集，并且对任意x,y属于D(f)和0<=theta<=1都有
f(theta*x+(1-theta)*y)<=theta*f(x)+(1-theta)*f(y) —这个貌似叫做jensen不等式
用通俗的话来说就是曲线上任意两点的割线都在曲线的上方
常见的凸函数有：
指数函数f(x)=a^x a>1
负对数函数-logax a>1,x>0
开口向上的二次函数等
凸函数的判定：
如果f是一阶可导，对于任意数据域内的x,y满足f(y)>=f(x)+f’(x)(y-x)
如果f是二阶可导，
凸优化应用举例
SVM：其中由max|w| 转向min(1/2*|w|^2)
最小二乘法？
LR的损失函数sigma(yi*log(hw(x))+(1-yi)*(log(1-hw(x))))