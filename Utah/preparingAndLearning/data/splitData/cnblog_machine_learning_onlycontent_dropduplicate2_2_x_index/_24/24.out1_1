1 . The Problem of Overfitting1 还是 来看 预测 房价 
的 这个 例子 我们 先 对 该 数据 做 线性 
回归 也 就是 左边 第一张 图 如果 这么 做 我们 
可以 获得 拟合 数据 的 这样 一条 直线 但是 实际上 
这 并不 是 一个 很好 的 模型 我们 看看 这些 
数据 很明显 随着 房子 面积 增大 住房/n 价格/n 的/uj 变化/vn 
趋于稳定/l 或者说/c 越/d 往右/n 越/d 平缓/a 因此 线性 回归 并 
没有 很好 拟合 训练 数据 我们 把 此类情况 称为 欠 
拟合 underfitting 或者 叫作 叫做 高 偏差 bias 这 两种 
说法 大致 相似 都 表示 没有 很好 地 拟合 训练 
数据 高 偏差 这个词 是 machine learning 的 研究 初期 
传 下来 的 一个 专业名词 具体 到 这个 问题 意思 
就是说 如果 用 线性 回归 这个 算法 去 拟合 训练 
数据 那么 该 算法 实际上 会 产生 一个 非常 大 
的 偏差 或者 说 存在 一个 很强 的 偏见 第二幅 
图 我们 在 中间 加入 一个 二次 项 也 就是说 
对于 这幅 数据 我们 用 二次函数 去 拟合 自然 可以 
拟合 出 一条 曲线 事实 也 证明 这个 拟合 效果 
很好 另 一个 极端 情况 是 如果 在 第三 幅 
图中 对于 该 数据集 用 一个 四次 多项式 来 拟合 
因此 在 这里 我们 有 五个 参数 θ 0 到 
θ 4 这样 我们 同样 可以 拟合 一条 曲线 通过 
我们 的 五个 训练样本 我们 可以 得到 如 右图 的 
一条 曲线 一方面 我们 似乎 对 训练 数据 做 了 
一个 很好 的 拟合 因为 这条 曲线 通过 了 所有 
的 训练 实例 但是 这 实际上 是 一条 很 扭曲 
的 曲线 它 不停 上下 波动 因此 事实上 我们 并不 
认为 它 是 一个 预测 房价 的 好 模型 所以 
我们 把 这类 情况 叫做 过拟合 overfitting 也叫 高 方差 
variance 与 高 偏差 一样 高 方差 同样 也 是 
一个 历史 上 的 叫法 从 第一 印象 上 来说 
如果 我们 拟合 一个 高阶 多项式 那么 这个 函数 能 
很好 的 拟合 训练 集 能 拟合 几乎 所有 的 
训练 数据 但这 也就 面临 函数 可能 太过 庞大 的 
问题 变量 太多 同时 如果 我们 没有 足够 的 数据 
集 训练 集 去 约束 这个 变量 过多 的 模型 
那么 就 会 发生 过 拟合 2 过度 拟合 的 
问题 通常 发生 在 变量 特征 过多 的 时候 这种 
情况 下 训练 出 的 方程 总是 能 很好 的 
拟合 训练 数据 也 就是说 我们 的 代价 函数 可能 
非常 接近 于 0 或者 就为 0 但是 这样 的 
曲线 千方百计 的 去 拟合 训练 数据 这样 会 导致 
它 无法 泛化 到 新的 数据 样本 中 以至于 无法 
预测 新 样本 价格 在 这里 术语 泛化 指 的 
是 一个 假设 模型 能够 应用到 新 样本 的 能力 
新 样本数据 是 指 没有 出现 在 训练 集中 的 
数据 之前 我们 看到 了 线性 回归 情况下 的 过拟合 
类似 的 情况 也 适用 于 逻辑 回归 3 那么 
如果 发生 了 过拟合 问题 我们 应该 如何 处理 过多 
的 变量 特征 同时 只有 非常少 的 训练 数据 会 
导致 出现 过度 拟合 的 问题 因此 为了 解决 过度 
拟合 有 以下 两个 办法 方法 一 尽量减少 选取 变量 
的 数量 具体 而言 我们 可以 人工 检查 每一项 变量 
并 以此 来 确定 哪些 变量 更为重要 然后 保留 那些 
更为 重要 的 特征 变量 至于 哪些 变量 应该 舍弃 
我们 以后 在 讨论 这会 涉及 到 模型 选择 算法 
这种 算法 是 可以 自动 选择 采用 哪些 特征 变量 
自动 舍弃 不 需要 的 变量 这类 做法 非常 有效 
但是 其 缺点 是 当 你 舍弃 一部分 特征 变量 
时 你 也 舍弃 了 问题 中 的 一些 信息 
例如 也许 所有 的 特征 变量 对于 预测 房价 都是 
有用 的 我们 实际上 并 不想 舍弃 一些 信息 或者 
说 舍弃 这些 特征 变量 方法 二 正则化 正则化 中 
我们 将 保留 所有 的 特征 变量 但是 会 减小 
特征 变量 的 数量级 参数 数值 的 大小 θ j 
这个 方法 非常 有效 当 我们 有 很多 特征 变量 
时 其中 每 一个 变量 都 能对 预测 产生 一点 
影响 正如 我们 在 房价 预测 的 例子 中 看到 
的 那样 我们 可以 有 很多 特征 变量 其中 每 
一个 变量 都是 有用 的 因此 我们 不 希望 把 
它们 删掉 这就 导致 了 正则化 概念 的 发生 接下来 
我们 会 讨论 怎样 应用 正则化 和 什么 叫做 正则化 
均值 然后 将 开始 讨论 怎样 使用 正则化 来使 学习 
算法 正常工作 并 避免 过拟合 2 . Cost Function1 在 
前面 的 介绍 中 我们 看到 了 如果 用 一个 
二次函数 来 拟合 这些 数据 那么 它 给 了 我们 
一个 对 数据 很好 的 拟合 然而 如果 我们 用 
一个 更 高次 的 多项式 去 拟合 最终 我们 可能 
会 得到 一个 曲线 它 能 很好 地 拟合 训练 
集 但却 并 不是 一个 好 的 结果 因为 它 
过度 拟合 了 数据 因此 一般性 并 不是 很好 让 
我们 考虑 下面 的 假设 我们 想 要 加上 惩罚 
项 从而 使 参数 θ 3 和 θ 4 足够 
的 小 这里 我 的 意思 就是 上图 的 式子 
是 我们 的 优化 目标 也 就是说 我们 需要 尽量减少 
代价 函数 的 均方 误差 对于 这个 函数 我们 对 
它 添加 一些 项 加上 1000 乘以 θ 3 的 
平方 再 加上 1000 乘以 θ 4 的 平方 1000 
只是 我 随便 写 的 某个 较大 的 数字 而已 
现在 如果 我们 要 最小化 这个 函数 那么 为了 最小化 
这个 新的 代价 函数 我们 要 让 θ 3 和 
θ 4 尽可能 小 因为 如果 你 在 原有 代价 
函数 的 基础上 加上 1000 乘以 θ 3 这 一项 
那么 这个 新 的 代价 函数 将 变得 很大 所以 
当 我们 最小化 这个 新的 代价 函数 时 我们 将 
使 θ 3 的 值 接近于 0 同样 θ 4 
的 值 也 接近 于 0 就像 我们 忽略 了 
这两个 值 一样 如果 我们 做到 这 一点 θ 3 
和 θ 4 接近 0 那么 我们 将 得到 一个 
近 似的 二次函数 因此 我们 最终 恰当 地 拟合 了 
数据 我们 所 使用 的 正是 二次函数 加上 一些 非常 
小 贡献 很 小项 因为 这些 项的/nr θ 3 θ 
4 非常 接近 于0/nr 显然 这 是 一个 更好 的 
假设 2 更 一般地 这里 给 出了 正规化 背后 的 
思路 这种 思路 就是 如果 我们 的 参数值 对应 一个 
较小 值 的话 参数值 比较 小 那么 往往 我们 会 
得到 一个 形式 更 简单 的 假设 在 我们 上面 
的 例子 中 我们 惩罚 的 只是 θ 3 和 
θ 4 使 这 两个 值 均 接近 于零/nr 从而 
我们 得到 了 一个 更 简单 的 假设 实际上 这个 
假设 大 抵上 是 一个 二次函数 但 更 一般地说 如果 
我们 像 惩罚 θ 3 和 θ 4 这样 惩罚 
其它 参数 那么 我们 往往 可以 得到 一个 相对 较为 
简单 的 假设 实际上 这些 参数 的 值 越小 通常 
对 应于 越 光滑 的 函数 也 就是 更加 简单 
的 函数 因此 就不 易发生 过拟合 的 问题 我 知道 
为什么 越小 的 参数 对应 于 一个 相对 较为 简单 
的 假设 对 你 来说 现在 不 一定 完全 理解 
但是 在 上面 的 例子 中使 θ 3 和 θ 
4 很小 并且 这样 做 能给/nr 我们 一个 更加 简单 
的 假设 这个 例子 至少 给 了 我们 一些 直观 
感受 来 让 我们 看看 具体 的 例子 对于 房屋 
价格 预测 我们 可能 有 上百 种 特征 与 刚刚 
所讲 的 多项式 例子 不同 我们 并不 知道 θ 3 
和 θ 4 是 高阶 多项式 的 项 所以 如果 
我们 有 一百 个 特征 我们 并不 知道 如何 选择 
关联度 更好 的 参数 如何 缩小 参数 的 数目 等等 
因此在 正则化 里 我们 要 做 的 事情 就是 把 
减小 我们 的 代价 函数 例子 中 是 线性 回归 
的 代价 函数 所有 的 参数值 因为 我们 并不 知道 
是 哪一个 或 哪几个 要去 缩小 因此 我们 需要 修改 
代价 函数 在 这 后面 添加 一项 就像 我们 在 
方括号 里 的 这项 当 我们 添加 一个 额外 的 
正则化 项的/nr 时候 我们 收缩 了 每个 参数 顺便 说 
一下 按照 惯例 我们 没有 去 惩罚 θ 0 因此 
θ 0 的 值 是 大 的 这 就是 一个 
约定 从 1 到 n 的 求和 而 不是 从 
0 到 n 的 求和 但 其实 在 实践 中 
这只 会有 非常 小 的 差异 无论 你 是否 包括 
这 θ 0 这项 但是 按照 惯例 通常 情况下 我们 
还是 只从 θ 1 到 θ n 进行 正则化 下面 
的 这项 就是 一个 正则化 项 并且 λ 在 这里 
我们 称做 正则化 参数 λ 要做 的 就是 控制 在 
两个 不同 的 目标 中 的 平衡 关系 第一 个 
目标 就是 我们 想要 训练 使 假设 更好 地 拟合 
训练 数据 我们 希望 假设 能够 很好 的 适应 训练 
集 而 第二 个 目标 是 我们 想 要 保持 
参数值 较小 通过 正则化 项 而 λ 这个 正则化 参数 
需要 控制 的 是 这 两者 之间 的 平衡 即 
平衡 拟合 训练 的 目标 和 保持 参数值 较小 的 
目标 从而 来 保持 假设 的 形式 相对 简单 来 
避免 过度 的 拟合 对于 我们 的 房屋 价格 预测 
来说 我们 之前 所用 的 非常 高的/nr 高阶 多项式 来 
拟合 我们 将 会 得到 一个 非常 弯曲 和 复杂 
的 曲线 函数 现在 我们 只 需要 使用 正则化 目标 
的 方法 那么 你 就 可以 得到 一个 更加 合适 
的 曲线 但 这个 曲线 不是 一个 真正 的 二次函数 
而是 更加 的 流畅 和 简单 的 一个 曲线 这样 
就 得到 了 对于 这个 数据 更好 的 假设 再一次 
说明 下 这 部分 内容 的确 有些 难以 明白 为什么 
加上 参数 的 影响 可以 具有 这种 效果 但 如果 
你 亲自 实现 了 正规化 你 将 能够 看到 这种 
影响 的 最 直观 的 感受 3 在 正则化 线性 
回 归中 如果 正则化 参数值 λ 被 设定 为 非常 
大 那么 将 会 发生 什么 呢 我们 将 会 
非常 大地 惩罚 参数 θ 1 θ 2 θ 3 
θ 4 也 就是说 我们 最终 惩罚 θ 1 θ 
2 θ 3 θ 4   在 一个 非常 大 
的 程度 那么 我们 会 使 所有 这些 参数 接 
近于零 如果 我们 这么 做 那么 就是 我们 的 假设 
中 相当于 去掉 了 这些 项 并且 使 我们 只是 
留下 了 一个 简单 的 假设 这个 假设 只能 表明 
房屋 价格 等于 θ 0 的 值 那 就是 类似于 
拟合 了 一条 水平 直线 对于 数据 来说 这 就是 
一个 欠 拟合 underfitting 这种 情况 下 这一 假设 它 
是 条 失败 的 直线 对于 训练 集 来说 这 
只是 一条 平滑 直线 它 没有 任何 趋势 它 不会 
去 趋向 大部分 训练样本 的 任何 值 这 句话 的 
另 一种 方式 来 表达 就是 这种 假设 有 过于 
强烈 的 偏见 或者 过高 的 偏差 bais 认为 预测 
的 价格 只是 等于 θ 0 对于 数据 来说 这 
只是 一条 水平线 因此 为了 使 正则化 运作 良好 我们 
应当 注意 一些 方面 应该 去 选择 一个 不错 的 
正则化 参数 λ 当 我们 以后 讲到 多重选择 时 我们 
将 讨论 一种 方法 来 自动 选择 正则化 参数 λ 
  为了 使用 正则化 接下来 我们 将 把 这些 概念 
应用 到到 线性 回归 和 逻辑 回归 中去 那么 我们 
就 可以 让 他们 避免 过度 拟合 了 3 . 
Regularized Linear Regression 之前 我们 已经 介绍 过 岭回归 的 
代价 函数 如下 对于 线性 回归 的 求解 我们 之前 
运用 了 两种 学习 算法 一种 基于 梯度 下降 一种 
基于 正规 方程 1 梯度 下降 如下 2 正规 方程 
如下 3 现在 考虑 M 即 样本量 比 N 即 
特征 的 数量 小 或 等于 N 通过 之前 的 
博文 我们 知道 如果 你 只有 较少 的 样本 导致 
特征 数量 大于 样本 数量 那么 矩阵 XTX 将 是 
不 可逆矩阵 或 奇异 singluar 矩阵 或者 用 另一种 说法 
是 这个 矩阵 是 退化 degenerate 的 那么 我们 就 
没有 办法 使用 正规 方程 来 求出 θ 幸运 的 
是 正规化 也 为 我们 解决 了 这个 问题 具体 
的 说 只要 正则参数 是 严格 大于 零 实际上 可以 
证明 如下 矩阵 将 是 可逆 的 因此 使用 正则 
还 可以 照顾 任何 XTX 不可逆 的 问题 所以 你 
现在 知道 如何 实现 岭回归 利用 它 你 就 可以 
避免 过度 拟合 即使 你 在 一个 相对 较小 的 
训练 集 里 有 很多 特征 这 应该 可以 让 
你 在 很多 问题 上 更好 的 运用 线性 回归 
在 接下来 的 视频 中 我们 将 把 这种 正则化 
的 想法 应用到 Logistic 回归 这样 我们 就 可以 让 
logistic 回归 也 避免 过度 拟合 从而 表现 的 更好 
4 . Regularized Logistic R e g r e s 
s i o n R e g u l a 
r i z e d Logistic Regression 实际上 与 Regularized 
Linear Regression 是 十分 相似 的 同样 使用 梯度 下降 
如果 在 高级 优化 算法 中 使用 正则化 技术 的话 
那么 对于 这类 算法 我们 需要 自己 定义 costFunction For 
those methods what we needed to do was to define 
the function that s called the cost function . 这个 
我们 自定义 的 costFunction 的 输入 为 向量 θ 返回值 
有 两项 分别 是 代价 函数 jVal 以及 梯度 gradient 
总之 我们 需要 的 就是 这个 自定义 函数 costFunction 针对 
Octave 而言 我们 可以 将 这个 函数 作为 参数 传入 
到 fminunc 系统函数 中 fminunc 用来 求函数 的 最小值 将 
@ costFunction 作为 参 数代 进去 注意 @ costFunction 类似于 
C 语言 中的 函数指针 fminunc 返回 的 是 函数 costFunction 
在 无约束 条件下 的 最小值 即 我们 提供 的 代价 
函数 jVal 的 最小值 当然 也 会 返回 向量 θ 
的 解 上述 方法 显然 对 正则化 逻辑 回归 是 
适用 的 5 . 尾声 通过 最近 的 几 篇文章 
我们 不难 发现 无论是 线性 回归 问题 还是 逻辑 回归 
问题 都 可以 通过 构造 多项式 来 解决 但是 你 
将 逐渐 发现 其实 还有 更为 强大 的 非线性 分类器 
可以 用来 解决 多项式 回归 问题 下 篇 文章 中 
我们 将 会 讨论 