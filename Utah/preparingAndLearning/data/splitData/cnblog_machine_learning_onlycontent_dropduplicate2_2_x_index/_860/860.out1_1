这是 一份 贝叶斯 机器学习 路线图 正在 不断 更新 中 . 
路线图 由 简短 的 介绍 配 以 相应 的 学习 
资源 组成 读者 不 一定 要 按顺序 学习 可以 直接 
定位 到 自己 需要 的 地方 . 很多 时候 我们 
希望 自学 某个 领域 的 知识 学习 能力 是 不差 
的 但 苦于 不知 该 学 哪些 从何 学起 看 
什么 书 / 视频 好 各个 概念 / 知识 点 
之间 是 怎样 的 联系 这份 路线图 是 为 解决 
以上 问题 而生 的 对于 学习 贝叶斯 机器学习 应该 十分 
有 帮助 . 若 您 发现 错漏 欢迎 评论 指正 
也 希望 有 更多 的 人 愿意 分享 自己 所在 
领域 的 学习 路线图 注意 文中 部分 资源 链接 需要 
* * * 方可 打开 本文 目录 结构 如下 核心 
主题 中心 问题 参数估计 模型 比较 非 贝叶斯 方法 最大 
似 然 正则化 EM 算法 基本 推断 算法 MAP 估计 
Gibbs 采样 马尔科夫 链 蒙特卡洛 MCMC 变分 推断 Variational inference 
模型 混合 高斯 因子分析 隐 马尔科夫 模型 HMM 贝叶斯 模型 
比较 贝叶斯 信息 准则 Bayesian information criterion 拉普拉斯 近似 Laplace 
approximation 进阶 主题 模型 逻辑 回归 Logistic regression 贝叶斯 网络 
Bayesian networks Latent Dirichlet allocation LDA 线性 动态 系统 Linear 
dynamical systems 稀疏 编码 Sparse coding 贝叶斯 非 参数 高斯 
过程 Gaussian processes Chinese restaurant process CRP Hierarchical Dirichlet processIndian 
buffet process IBP Dirichlet diffusion treesPitman Yor process 采样 算法 
折叠 Gibbs 采样 Collapsed Gibbs sampling 哈密尔顿 蒙特卡洛 Hamiltonian Monte 
Carlo HMC 切片 采样 Slice sampling 可逆 跳跃 MCMC reversible 
jump MCMC Sequential Monte Carlo SMC 粒子 滤波器 Particle filter 
退火 重要性 采样 Annealed importance sampling 变分 推断 变分 贝叶斯 
Variational Bayes 平均 场 近似 Mean field approximation 期望 传播 
expectation propagation 信念 传播 Belief propagation 树 结构图 模型 Sum 
product algorithmMax product algorithm 非 树 结构图 模型 循环 信念 
传播 Loopy belief propagation 连接 树 算法 Junction tree algorithm 
理论 无 信息 先验 uninformative priors Jeffreys prior 最大 似 
然 的 渐进 asymptotics of maximum likelihood 贝叶斯 统计 是 
统计 的 一个 分支 它 的 特点 是 把 我们 
感兴趣 的 量 比如 统计模型 的 参数 看作 随机变量 . 
给定 观察 数据 后 我们 对 这些 量 的 后验/nr 
分布 进行 分析 从而 得出结论 . 虽然 贝叶斯 统计 的 
核心 思想 已经 历经 很多年 了 但 贝叶斯 的 思想 
在 过去 近 20年 对 机器 学习 产生 了 重大 
影响 因为 它 在对 真实世界 现象 建立 结构化 模型 时 
提供 了 灵活性 . 算法 的 进步 和 日益 增长 
的 计算资源 使得 我们 可以 拟合 丰富 的 高度 结构化 
的 模型 而 这些 模型 在 过去 是 很 棘手 
的 . 这个 路线图 旨在 给出 贝叶斯 机器学习 中 许多 
关键 思想 的 指引 . 如果 您 正 考虑 在 
某些 问题 中 使用 贝叶斯 方法 您 需要 学习 核心 
主题 中 的 所有 内容 . 即使 您 只是 希望 
使用 诸如 BUGS Infer . NET 或 Stan 等 软件包 
这些 背景 知识 也对 您 很有帮助 . 如果 这些 软件包 
不能 马上 解决 您 的 问题 知道 模型 的 大致 
思想 可 帮助 您 找出 问题 所在 . 如果 您 
正 考虑 研究 贝叶斯 机器学习 那么 许多 论文 会 假设 
您 已经 掌握 了 核心 主题 的 内容 以及 部分 
进阶 主题 的 内容 而 不再 给 出 参考 文献 
. 阅 读本 路线图 时 我们 不 需要 按 顺序 
学习 希望 本文 可以 在 您 需要 时为 您 提供 
帮助 . 核心 主题 这 一章 覆盖 了 贝叶斯 机器学习 
的 核心 概念 . 如果 您 希望 使用 这些 工具 
建议您 学习 本章 的 所有 内容 . 中心 问题 什么 
是 贝叶斯 机器学习 一般来说 贝叶斯 方法 旨在 解决 下面 给出 
的 某一个 问题 参数估计 parameter estimation 假设 您 已经 建好 
了 一个 统计模型 并且 希望 用 它 来做 预测 . 
抑或 您 认为 模型 中 的 参数 很 有意义 所以 
希望 拟合 这些 参数 来 学习 到 某些 东西 . 
贝叶斯 方法 是 在 给定 观察 数据 后 去 计算 
或者 近似 这些 参数 的 后验/nr 分布 . 您 通常会 
希望 使用 训练 好 的 模型 来 作出 一些 决策 
行为 . 贝叶斯 决策理论 Bayesian decision theory 提供 了 选择 
行为 的 一个 框架 . 模型 比较 model comparison 您 
可能 有 许多 个 不同 的 候选 模型 那么 哪 
一个 是 最 贴切 给定 数据 的 呢 一种 常见 
的 情形 是 您 有 一些 形式 相同 但 复杂度 
不同 的 模型 并且 希望 在 复杂度 和 拟合度 间 
权衡 . 与 选择 单个 模型 相比 您 可以 先为 
模型 定义 先验 并且 根据 模型 的 后验对/nr 预测 进行 
平均 . 这 便是 贝叶斯 模型 平均 bayesian model averaging 
. 此外 贝叶斯 网络 Bayesian networks Bayes nets 的 基础 
知识 也 值得 一 学 因为 这些 符号 在 讨论 
贝叶斯 模型 时会 经常 用到 . 由于 贝叶斯 方法 把 
模型 参数 也 看作 随机变量 所以 我们 可以 把 贝叶斯 
推断 问题 本身 表达 为 贝叶斯 网络 . 阅读 本章 
内容 会 告诉 您 贝叶斯 方法 解决 什么 问题 但是 
没 告诉 您 一般 情况 下 如何 真正 地 解决 
这些 问题 . 这是 本 路线图 剩余 部分 将 讨论 
的 内容 . 非 贝叶斯 方法 Non Bayesian techniques 作为 
背景 知识 了解 如何 使用 非 贝叶斯 方法 拟合 生成 
模型 是 有助于 理解 的 . 这么做 的 其中 一个 
理由 是 这些 方法 更 易于 理解 并且 一般来说 结果 
已经 足够 好了 . 此外 贝叶斯 方法 跟 这些 方法 
存在 一些 相似性 学习 这些 方法 后 通过 类比 可以 
帮助 我们 学习 贝叶斯 方法 . 最 基础 的 您 
需要 明白 泛化 generalization 的 符号 或者 知道 一个 机器学习 
算法 在 未知 数据 上 表现 如何 . 这是 衡量 
机器学习 算法 的 基础 . 您 需要 理解 以下 方法 
最大 似 然 maximum likelihood 拟合 模型 参数 的 准则 
. 正则化 regularization 防止 过拟合 的 方法 . EM 算法 
the EM algorithm 为 每个 数 据点 都有 与之 相 
关联 的 潜在 变量 未 观测 变量 的 生成 模型 
拟合 参数 . 基本 推断 算法 一般来说 贝叶斯 推断 需要 
回答 的 问题 是 给定 观察 数据 后 推断 关于 
模型 参数 或 潜在 变量 latent variables 的 后验/nr 分布 
. 对于 一些 简单 模型 这些 问题 拥有 解析 解./nr 
然而 大多数 时候 我们 得不到 解析 解 所以 需要 计算 
近似解 . 如果 您 需要 实现 自己 的 贝叶斯 推断 
算法 以下 可能 是 最简单 的 选择 MAP 估计 MAP 
estimation 使用 最优 参数 的 点估计 来 近似 后验./nr 这把 
积分 问题 替换 为了 优化 问题 . 但这 并不 代表 
问题 就 很 简单 了 因为 优化 问题 本身 也 
常常 很 棘手 . 然而 这 通常 会 简化 问题 
因为 优化 软件包 比 采样 软件包 更 普适 general 也 
更 鲁棒 robust . 吉布斯 采样 Gibbs sampling 吉布斯 采样 
是 一种 迭代 的 采样 过程 每一个 随机变量 都从 给定 
其他 随机变量 的 条件 分布 中 采样 得到 . 采样/v 
的/uj 结果/n 很/zg 有/v 希望/v 是/v 后验/nr 分布/v 中/f 的/uj 
一个/m 近似/a 样本/n ./i 您/zg 还/d 应该/v 知道/v 下列/v 常用/b 
的/uj 方法/n ./i 他们 的 一般 公式 大多数 时候 都 
过于 宽泛 而 难以 使用 但是 在 很多 特殊 情形 
下 他们 还是 很 强大 的 马尔科夫 链 蒙特卡洛 Markov 
chain Monte Carlo 一类 基于 采样 的 算法 这些 算法 
基于 参数 的 马尔科夫 链 该 马尔科夫 链 的 稳态 
分布 是 后验/nr 分布 . 1 . 特别 的 Metropolis 
Hastings M H 算法 是 一类 实用 的 构建 有效 
MCMC 链 的 方法 . 吉布斯 采样 也是 M H 
算法 的 特例 . 变分 推断 Variational inference 尝试 用 
易于 处理 的 分布 去 近似 难以 处理 的 分布 
. 一般来说 易处理 分布 的 参数 通过 最小化 某种 度量 
指标 来 选择 这个 度量 指标 衡量 了 近似 分布 
和 真实 分布 之间 的 距离 . 模型 以下 是 
一些 简单 的 生成 模型 这些 模型 常常 运用 贝叶斯 
方法 . 混合 高斯 mixture of Gaussians 混合 高斯 模型 
中 每 个数 据点 属于 若干 簇 或者 群组 中的 
其中 一个 每个 簇 中的 数 据点 都 服从 高斯分布 
. 拟合 这样 一个 模型 可以 让 我们 推断出 数据 
中 有 意义 的 分组 情况 . 因子分析 factor analysis 
因子分析 中 每 个数 据点 被 更低 维度 的 线性函数 
近似 表达 . 我们 的 想法 是 潜在 空间 latent 
space 中 每个 维度 对应 一个 有 意义 的 因子 
或者 数据 中 变化 的 维度 . 隐 马尔科夫 模型 
hidden Markov models 隐 马尔科夫 模型 适用于 时间 序列 数据 
其中 有 一个 潜在 的 离散 状态 随着 时间 的 
推移 而 演变 . 虽然 贝叶斯 方法 大多数 时候 与 
生成 模型 相联系 但 它 也 可以 被 用于 判别 
模型 的 情况 . 这种 情形 下 我们 尝试 对 
已知 观测 数据 时 目标 变量 的 条件分布 直接进行 建模 
. 标准 的 例子 是 贝叶斯 线性 回归 Bayesian linear 
regression . 贝叶斯 模型 比较 推断 算法 的 小节 为 
我们 提供 了 近似 后验/nr 推断 的 工具 . 那么 
比较 模型 的 工具 是 什么 呢 不幸 的 是 
大多数 模型 比较 算法 相当 复杂 在 您 熟悉 下面 
描述 的 高级 推理 算法 前 您 可能 不想 自己 
实现 它们 . 然而 有 两个 相当 粗略 的 近似 
模型 比较 是 较为 容易 实现 的 . 贝叶斯 信息 
准则 Bayesian information criterion BIC 贝叶斯 信息 准则 简单 地 
使用 MAP 解并/nr 添加 一个 罚 项 该罚/i 项的/nr 大小/b 
正比/z 于/p 参数/n 的/uj 数量/n ./i 拉普拉斯/l 近似/a Laplace approximation 
使用/v 均值/n 与/p 真实/d 后验/nr 分布/v MAP/w 相同/d 的/uj 高斯分布/nr 
对/p 后验/nr 分布/v 进行/v 近似/a ./i 进阶/n 主题/n 本章/r 将/d 
讨论/v 贝叶斯/nr 机器学习/i 中/f 更/d 进阶/n 的/uj 主题/n ./i 您 
可以 以 任何 顺序 学习 以下内容 模型 在 核心 主题 
一章 中 我们 列出 了 一些 常用 的 生成 模型 
. 但是 大多数 的 数据 集并 不符合 那样 的 结构 
. 贝叶斯 建模 的 强大 之处 在于 其 在 处理 
不 同类型 的 数据 时 提供 了 灵活性 . 以下 
列出 更多 的 模型 模型 列出 的 顺序 没有 特殊 
意义 . 逻辑 回归 logistic regression 逻辑 回归 是 一个 
判别 模型 给定 输入 特征 后 对 二元 目标 变 
量 进行 预测 . 贝叶斯 网络 Bayesian networks Bayes nets 
. 概括地说 贝叶斯 网络 是 表示 不同 随机变量 间 概率 
依赖 关系 的 有向图 它 经常 被 用于 描述 不同 
变量 间 的 因果关系 . 尽管 贝叶斯 网络 可以 通过 
非 贝叶斯 方法 学习 但 贝叶斯 方法 可 被 用于 
学习 网络 的 参数 parameters 和 结构 structure 网络 中的 
边 线性 高斯 模型 Linear Gaussian models 是 网络 中的 
变量 都 服从 联合 高斯 的 重要 特殊情况 . 即使 
在 具有 相同 结构 的 离散 网络 难以 处理 的 
情况 下 这些/r 网络/n 的/uj 推论/v 都常/nr 易于/v 处理/v ./i 
latent/w Dirichlet allocation LDA LDA 模型 是 一个 主题 模型 
其 假定 一组 文档 例如 网页 由 一些 主题 组成 
比如 计算机 或 运动 . 相关 模型 包括 非 负 
矩阵 分解 nonnegative matrix factorization 和 概率 潜在 语义分析 probabilistic 
latent semantic analysis 线性 动态 系统 linear dynamical systems 一个时间 
序列 模型 . 其中 低维 高斯 潜在状态 随 时间 演变 
并且 观察 结果 是 潜在 状态 的 噪声 线性函数 . 
这 可以 被 认为 是 HMM 的 连续 版本 . 
可以 使用 卡尔曼 滤波器 Kalman filter 和 平滑 器 smoother 
来 精确 地 执行 该 模型 中 的 判断 . 
稀疏 编码 sparse coding 稀疏 编码 中 每 一个 数 
据点 被 建模 为从 较大 的 字典 中 抽取 的 
少量 元素 的 线性组合 . 当 该 模型 被 应用 
于 自然 图像 像素 时 学习 的 字典 类似于 主 
视觉 皮层 中的 神经元 的 接受 字段 . 此外 另 
一个 密切 相关 的 模型 称为 独立 成分 分析 independent 
component analysis . 贝叶斯 非 参数 上述 所有 模型 都是 
参数 化 的 因为 它们 是 以 固定 的 有限 
数量 的 参数 表示 的 . 这是 有 问题 的 
因为 这 意味着 我们 需要 预先指定 一些 参数 比如 聚 
类 中的 簇 的 数目 而 这些 参数 往往 是 
我们 事先 不 知道 的 . 这个 问题 可能 对 
上述 模型 看起来 并无 大碍 因为 对于 诸如 聚 类 
的 简单 模型 我们 通常 可以 使用 交叉 验证 来 
选择 好 的 参数 . 然而 许多 广泛 应用 的 
模型 是 更为 复杂 的 其中 涉及 许多 独立 的 
聚 类 问题 簇 的 数量 可能 是 少数 几个 
也 可能 是 数千个 . 贝叶斯 非 参数 是 机器 
学习 和 统计学 中 不断 研究 的 领域 通过 定义 
无限 复杂 的 模型 来 解决 这个 问题 . 当然 
我们 不能 明确 地 表示 无限 的 对象 . 但是 
关键 的 观点 是 对于 有限 数据集 我们 仍然 可以 
在 模型 中 执行 后验/nr 推断 而 仅仅 明确 地 
表示 它们 的 有限 部分 . 下面 给 出 一些 
重要 的 组成 贝叶斯 非 参数 模型 的 构建 模块 
高斯 过程 Gaussian processes 高斯 过程 是 函 数上 的 
先验 使得 在 任何 有限 集合点 处 采样 的 值 
是 服从 联合 高斯 的 . 在 许多 情况 下 
为 在 函 数上 赋予 先验 您 需要 假设 后验/nr 
推理 是 易于 处理 的 . Chinese restaurant process CRP 
CRP 是 无限 对象 集合 的 划分 的 先验 这 
常被 用于 聚 类 模型 使得 簇 的 数目 无需 
事先 指定 . 推理 算法 相当 简单 且 易于 理解 
所以 没有 理由 不 使用 CRP 模型 代替 有限 聚 
类 模型 . 这个 过程 可以 等价 于 Dirichlet process 
. Hierarchical Dirichlet process 包含 一组 共享 相同 base measure 
的 Dirichlet process baase measure 本身 也 是从 Dirichlet process 
中 选取 的 . Indian buffet process IBP IBP 无限 
二进制 矩阵 的 先验 使得 矩阵 的 每 一行 仅 
具有 有限 个 1 . 这是 在 每个 对象 可以 
拥有 多个 不同 属性 时最/nr 常用 的 模型 . 其中 
矩阵 的 行 对应 于 对象 列 对应 于 属性 
如果 对象 具有 某 属性 对 应列 的 元素 为 
1 . 最简单 的 例子 可能 是 IBP linear Gaussian 
model . 其中 观察到 的 数据 是 属性 的 线性函数 
. 还 可以 根据 beta process 来看 IBP 过程 . 
本质上 beta process 之于 IBP 正如 Dirichlet process 之于 CRP 
. Dirichlet diffusion trees 一个 分层 聚 类 模型 . 
其中 数 据点 以 不同 的 粒度 级别 聚 类 
. 即 可能 存在 一些 粗粒度 的 簇 但是 这些 
簇 又 可以 分解 成更/nr 细粒度 的 簇 . Pitman 
Yor process 类似于 CRP 但是 在 聚 类 大小 上有 
更重 尾 的 分布 比如 幂律 分布 . 这 说明 
您 希望 找到 一些 非常 庞大 的 簇 以及 大量 
的 小 簇 . 比起 CRP 选择 0 的 指数分布 
幂律 分布 对于 许多 真实 数据 有 更好 的 拟合 
效果 . 采样 算法 从 核心 主题 章节 您 已经 
学习 了 两个 采样 算法 Gibbs 采样 和 Metropolis Hastings 
M H 算法 . Gibbs 采样 涵盖 了 很多 简单 
的 情况 但 在 很多 模型 中 您 甚至 不能 
计算 更新 . 即使 对于 适用 的 模型 如果 不同 
的 变量 紧密 耦合 tightly coupled 采样 过程 也会 mix 
得 非常 缓慢 . M H 算法 是 更 一般 
的 但是 M H 算法 的 一般 公式 中 没有 
提供 关于 如何 选择 提议 分布 proposals 的 指导 并且 
为 实现 良好 的 mix 通常 需要 非常 仔细 地 
选择 提议 分布 . 下面 是 一些 更 先进 的 
MCMC 算法 这些 算法 在 特定 情形 中 表现 更为 
良好 collapsed Gibbs sampling 变量 的 一部分 在 理论上 被 
边缘化 marginalized 或 折叠 collapsed 掉 并在 剩下 的 变量 
上 进行 Gibbs 采样 . 例如 当 拟合 CRP 聚 
类 模型 时 我们 通常 将 聚 类 参数 边缘 
化掉 并对 聚 类 分配 执行 Gibbs 采样 . 这可以 
显著 地 改善 mix 因为 聚 类 分配 和簇/nr 参数 
是 紧密 耦合 的 . Hamiltonian Monte Carlo HMC 连续 
空间 中 M H 算法 的 实例 其 使用 对数 
概率 的 梯度 来 选择 更好 的 探索 方向 . 
这是 驱动 Stan 的 算法 . slice sampling 一种 从 
一维 分布 中 采样 的 辅助 变量 方法 . 其 
关键 卖点 是 算法 不 需要 指定 任何 参数 . 
因此 它 经常 与 其他 算法 例如 HMC 结合 否则 
将 需要 指定 步长 参数 . reversible jump MCMC 在 
不同 维度 的 空间 之间 构造 M H 提议 分布 
的 方式 . 最 常见 的 用 例 是 贝叶斯 
模型 平均 虽然 在 实践 中 使用 的 大多数 采样 
算法 是 MCMC 算法 但 Sequential Monte Carlo SMC 算法 
值得一提 . 这是 从 一系列 相关 分布 中 近似 采样 
的 另一 类 技术 . 最 常见 的 例子 可能 
是 粒子 滤波器 particle filter 通常 应用于 时间 序列 模型 
的 推理 算法 . 它 每次 一步 地 考虑 观察 
数据 并且 在 每个 步骤 中 用 一组 粒子 表示 
潜在状态 的 后验/nr 退火 重要性 采样 Annealed importance sampling AIS 
是 另一种 SMC 方法 其 通过 一 系列 中间 分布 
从 简单 的 初始 分布 比如 先验 到 难 处理 
的 目标 分布 例 如后 验 逐渐 退火 针对 每个 
中间 分布 执行 MCMC 转换 . 由于 在 初始 分布 
附近 mixing 通常 更快 这 应该 有助于 采样器 避免 困在 
局部 模式 中 . 算法 计算 一组 权重 这些 权重 
亦可 被 用于 估计 边际 似 然 estimate the marginal 
likelihood . 当 使用 了 足够 多 的 中间 分布 
时 权重 的 方 差会 很小 因此 产生 了 一个 
精确 的 边际 似 然 估计 . 变分 推断 Variational 
inference 变分 推断 是 基于 优化 而不是 采样 的 另一 
类 近似 推断 方法 . 其 基本 想法 是 用 
一个 易处理 的 近似 分布 来 逼近 难 处理 的 
后验/nr 分布 . 选择/v 近似/a 分布/v 的/uj 参数/n 以使/c 近似/a 
分布/v 和后验/nr 分布/v 之间/f 的/uj 距离/n 的/uj 某些/r 度量/n 通常 
使用 KL 散度 最小化 . 我们 很难 对 变分 推断 
和 采样 方法 之间 的 折中 作出 任何 一般性 的 
陈述 因为 这些 都是/nr 一个 广泛 的 类别 其中 包括 
了 许多 特殊 的 算法 既有 简单 的 又有 复杂 
的 . 然而 有 一些 一般 的 经验 规则 变分 
推断 算法 具有 与 采样 方法 不同 的 实现 困难 
变分 推断 算法 更难 因为 它们 需要 冗长 的 数学 
推导 来 确定 更 新规则 . 然而 一旦 实现 变分 
贝叶斯 方法 可以 更容易 地被 检验 因为 可以 对 优化 
代码 采用 标准 检查 梯度 检查 局部 最优 测试 等 
. 此外 大多数 变分 推断 算法 收敛 到 局部 最优 
解 这 消除 了 检查 收敛 诊断 的 需要 . 
大多数 变分 推理 分布 的 输出 是 一个 分布 而 
不是 样本 . 为了 回答 许多问题 例如 模型 参数 的 
期望 或者 方差 可以 简单 地 检查 变分 分布 . 
相比之下 采 样方法 通常 需要 收集 大量 采样 样本 这 
可能 需要 很大 的 开销 . 然而 使用 变分法 近似 
的 精度 受到 近似 分布 族 的 表达 能力 的 
限制 并且/c 近似/a 分布/v 与/p 后验/nr 分布/v 有多/nr 大不同/i 并不/i 
总是/c 那么/r 明显/a ./i 相反 如果 您 运行 一个 采样 
算法 足够 长 时间 最终 您 会 得到 较为 准确 
的 结果 . 这里 给 出 一些 变分 推断 算法 
的 重要 例子 变分 贝叶斯 variational Bayes 贝叶斯 模型 的 
变分 推断 应用 其中 参数 的 后验/nr 分布 不能 精确 
地 表示 如果 模型 还包括 潜在 变量 则 可以 使用 
变分 贝叶斯 EM 算法 variational Bayes EM 平均 场 近似 
mean field approximation 近似 分布 具有 特别 简单 的 形式 
假定 所有 变量 是 独立 的 . 平均 场 也 
可以 根据 凸 对偶性 convex duality 来 观察 这将 导出 
与 普通 解释 不同 的 拓展 期望 传播 expectation propagation 
对 循环 置信 传播 loopy belief propagation 的 一种 近似 
. 它 发送 近似 消息 这些 消息 仅代表 相关 变量 
的 充分 统计量 的 期望 . 下面 给 出 一些 
使用 变分 推断 方法 的 经典 例子 . 尽管 你 
可能 不会 直接 使用 这些 模型 但是 它们 给 出了 
变分 技巧 如何 更 一般 地 用于 贝叶斯 模型 的 
指引 线性 回归 linear regression 逻辑 回归 logistic regression 混合 
高斯 mixture of Gaussians 指数 族 模型 exponential family models 
信念 传播 Belief propagation 信念 传播 是 用于 如 贝叶斯 
网络 Bayes nets 和 马尔科夫 场 Markov random fields MRFs 
等 图 模型 的 另一 类 推断 算法 . 模型 
中的 变量 相互 传递 消息 它们 总结 了 关于 其他 
变量 的 联合 分布 的 信息 . 信念 传播 有 
两种 一般 形式 当 应用于 树 结构图 模型 时 BP 
执行 精确 的 后验/nr 推断 . 有 两种 特殊 的 
形式 the sum product algorithm 计算 每个 单独 变量 以及 
每 一对 相邻 变量 的 边际 分布 . the max 
product algorithm 计算 所有 变量 的 最 可能 的 联合 
分配 还 可以 在 不是 树结构 的 图中 应用 相同 
的 消息 传递 规则 . 这 没有 给 出 确切 
的 结果 事实上 甚至 缺少 基本 的 保证 例如 收敛 
到 固定点 但 通常 它 在 实践 中能 很 有效 
. 这 通常 被 称为 循环 信念 传播 loopy belief 
propagation 以 区别于 树 结构 的 版本 但 令人 困惑 
的 是 一些 研究 人员 简单 地 将其 称为 信念 
传播 Loopy BP 被 解释 为 一种 变分 推断 算法 
连接 树 算法 junction tree algorithm 给出 了 通过 定义 
粗糙 的 超 变量 super variables 来 对 非 树 
结构图 应用 精确 的 BP 的 方法 . 定义 超 
变量 后的图/nr 是 树结构 的 . 树上 的 BP 最 
常见 的 特殊 情况 是 HMMs 的 前 向 后向 
算法 forward backward algorithm . 卡尔曼 平滑 Kalman smoothing 也是 
前 向 后向 算法 的 一种 特例 因此 也 是 
一种 BP . BP 在 计算机 视觉 和 信息论 中被 
广泛 使用 在 这 两个 领域 中 推断 问题 往往 
具有 规则 的 结构 . 在 贝叶斯 机器学习 中 BP 
不常 被 单独 使用 但是 它 可以 是 基于 变分 
或 采样 的 算法 中的 强大 组成部分 . 理论 最后 
给出 贝叶斯 方法 中 的 一些 理论 问题 . 定义 
贝叶斯 模型 需要 指定 先验 . 如果 对于 参数 没有 
较大 的 先验 信念 我们 可能 希望 选择 无 信息 
先验 uninformative priors . 一个 常见 的 选择 是 Jeffreys 
prior . 准确 地 估计 模型 中 的 参数 需要 
多少 数据 最大 似 然 的 渐进 asymptotics of maximum 
likelihood 提供 了 对于 这个 问题 的 许多 洞见 因为 
对于 有限 模型 后验/nr 分布 具有 与 最大 似 然 
估计 的 分布 相似 的 渐进 行为 . 参考 自 
metacademy 