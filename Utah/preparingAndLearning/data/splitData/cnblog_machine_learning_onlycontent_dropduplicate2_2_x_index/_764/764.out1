第一章 mesos spark shellSPARK shell 1 修改 spark / conf 
/ spark env . sh 增加 以下内容 shell export MESOS 
_ NATIVE _ JAVA _ LIBRARY = / usr / 
local / mesos / lib / libmesos . so export 
SPARK _ EXECUTOR _ URI = 上传 spark 1 . 
6.0 . tar . gz 对应 的 hdfs URL 如果 
已经 把 spark 的 jar 包 放在 了 mesos agent 
机器 上 不用 这个 配置 2 运行 命令 shell . 
/ bin / spark shell master mesos / / host 
5050 3 代码 ` ` ` scala val lines = 
sc . textFile / root / README . md lines 
org . apache . spark . rdd . RDD String 
= / root / README . md M a p 
P a r t i t i o n s 
R D D 3 at textFile at 27scala lines . 
countres1 Long = 3 ` ` ` 4 web 页面 
4040 端口 可以 看到 上面 执行 的 count 操作 SPARK 
的 核心 概念 1 spark shell 其实 是 一个 driver 
programme 驱动器 程序 2 driver programme 包含 应用 的 main 
函数 定义 了 集群 上 的 分布式 数据集 用来 发起 
集群 上 的 各种 并行操作 3 由于 spark shell 在 
开启 时 制定 了 master 因此 driver programe 提交 这些 
任务 到 集群 上 操作 spark 文件 操作 第二章 RDD 
编程 操作过程 1 RDD 包含 两类 操作 transaction 和 action 
只有 action 会对 RDD 计算 出 一个 结果 2 RDD 
会在 每次 action 操作 时 重新 计算 可以 通过 presist 
对 RDD 持久化 在 第一次 对 持久化 的 RDD 计算 
后 spark 会把 RDD 内容 保存 到 内存 中 以 
分区 方式 存储 在 集群 的 每台 机器 上 这样 
之后 的 action 操作 就 可以 重 用 这些 RDD 
数据 默认 不会 把 RDD 缓 存在 内存 中 是因为 
大量 数据 不 应该 仅仅 以 不同 的 形式 多份 
存在 内存 中 scala val lines = sc . textFile 
/ root / README . md val rdd2 = lines 
. filter line = line . contains aa . count 
rdd2 . presist rdd2 . count 创建 RDD 的 两种 
途径 1 把 已有 集合 传给 SparkContext 的 parallelize 方法 
2 从外部 数据 创建 rddscala sc . parallelize List aaa 
bbb sc . textFile / root / README . md 
RDD 操作 1 transaction a . 转化 操作 只能 产生 
新的 RDD 而不能 改版 原先 RDD 的 数据 errorsRDD = 
inputRDD . filter lambda x error in x warningsRDD = 
inputRDD . filter lambda x warning in x badLinesRDD = 
errorsRDD . union warningsRDD b . main 函数 中 所有 
的 RDD 会用 谱系 图 记录 产生 依赖 关系 2 
action a . 获取 操作 take int n 获取 RDD 
的 前 n 行 数据 collect 获取 RDD 中 的 
所有 数据 b . 当 调用 一个 新 action 操作 
时 整个 RDD 都会 重新 计算 导致 行为 低效 用户 
可以 将 中间 结果 持久化 常见 算子 1 transaction ` 
` ` scalaval rdd1 = sc . parallelize List 1 
2 3 3 / / 1 2 3 3 scala 
rdd1 . map x = x + 1 . collect 
/ / Array 2 3 4 4 scala rdd1 . 
flatMap x = x . to 3 . collect / 
/ Array 1 2 3 2 3 3 3 scala 
rdd1 . filter x = x = 1 . collect 
/ / Array 2 3 3 scala rdd1 . distinct 
. collect / / Array 2 1 3 scala rdd1 
. sample false 0.5 . collect / / 随机取样 个数/n 
和/c 数值/n 每次/r 都/d 不一样/i 是否 替换 val rdd1 = 
sc . parallelize List 1 2 3 / / 1 
2 3 val rdd2 = sc . parallelize List 3 
4 5 / / 3 4 5 scala rdd1 . 
union rdd2 . collect / / Array 1 2 3 
3 4 5 scala rdd1 . intersection rdd2 . collect 
/ / Array 3 求 两个 RDD 共同 的 元素 
scala rdd1 . subtract rdd2 . collect / / Array 
2 1 求 两个 RDD 不同 的 部分 scala rdd1 
. cartesian rdd2 . collect / / 两个 RDD 求 
笛卡尔积 Array 1 3 1 4 1 5 2 3 
3 3 2 4 2 5 3 4 3 5 
* * 2 action * * scalaval rdd1 = sc 
. parallelize List 1 2 3 3 / home / 
lj / Documents / / 1 2 3 3 scala 
rdd1 . collect / / Array 1 2 3 3 
返回 RDD 所有 元素 scala rdd1 . count / / 
4 返回 RDD 的 元素 个数 scala rdd1 . countByValue 
/ / Map 2 1 1 1 3 2 返回 
键值 对 元素 值 元素 个数 scala rdd1 . take 
2 / / Array 1 2 返回 RDD 中的 n 
个 元素 scala rdd1 . top 2 / / Array 
3 3 返回 RDD 中的 前 2个 元素 ` ` 
` 持久化 1 RDD 持久 化时 计算出 RDD 的 节点 
会 分别 保存 他们 所 求出 的 分区 数据 2 
如果 一个 有 持久化 数据 的 节点 发生 故障 spark 
会在 用到 缓存数据 时 重算 丢失 的 数据 分区 如果 
想在 节点 故障 时不 拖累 执行 速度 也 可以 把 
数据备份 到 多个 节点 上 3 persist 默认 把 数据 
以 序列化 的 形式 缓 存在 jvm 堆 内存 中 
同时 也 可以 通过 调整 持久化 级别 把 数据 缓 
存到 磁盘 或 堆 外 缓存 上 ` ` ` 
scalascala rdd2 . persist StorageLevel . DISK _ ONLY / 
/ persist 不会 立刻 触发 缓存 而是 等到 第一 次 
action 操作 后 自动 缓存 这个 RDD 结果 res4 rdd2 
. type = M a p P a r t 
i t i o n s R D D 1 
at filter at 30scala rdd2 . countres5 Long = 1 
` ` ` 4 如果 缓存 的 RDD 数据 在 
节点 上 的 内存 放不下 了 spark 会 通过 LRU 
最近 最少 被 使用 原则 吧 老 数据 移除 内存 
存放 新 数据 因此 不论 只 缓 存到 内存 还是 
同时 缓 存到 内存 和 硬盘 都 不会 因为 缓存 
而 使得 作业 停止 但是 缓存 过多 不 必要 的 
数据 会 带来 更多 分区 重算 时间 第三章 键值 对 
RDD 一 . 普通 RDD 转换成 pair RDD 初始化 scala 
val rdd1 = sc . parallelize List 1 2 3 
4 3 9 rdd1 org . apache . spark . 
rdd . RDD Int Int = P a r a 
l l e l C o l l e c 
t i o n R D D 0 at parallelize 
at console 24 二 . pair RDD 转化 操作 聚合 
操作 组合 RDD 中 相同 key 的 value 1 reduceByKey 
接受 一个 函数 为 数据 集中 每个 键 进行 规约 
操作 每个 规约 操作 会将 键 相同 的 值 合并 
起来 2 foldByKey 将 key 相同 的 分在 一组 再 
对 组 内 的 value 进行 fold 操作 . 使用 
一个 零值 初始 进行 折叠 零值 与 另一个 元素 合并 
结果 仍 为 该 元素 ` ` ` scala / 
/ mapValues 与 reduceByKey 计算 每个 键 对应 的 均值 
scala val rdd1 = sc . parallelize Array panda 3 
pink 1 panda 6 pink 3 scala val rdd2 = 
rdd1 . mapValues x = x 1 . reduceByKey v1 
v2 = v1 . _ 1 + v2 . _ 
1 v1 . _ 2 + v2 . _ 2 
/ / Array panda 9 2 pink 4 2 scala 
val rdd3 = rdd2 . mapValues v = v . 
_ 1 * 1.0 / v . _ 2 / 
/ Array panda 4.5 pink 2.0 3 ` combineByKey createCombiner 
mergeValue mergeCombiners ` combineByKey 方法 的 三个 参数 分别 对应 
聚合 的 几个 阶段 在 遍历 所有 元素 时 每个 
元素 的 key 要么 没有 遇到 过 要么 与 之前 
的 某个 元素 的 key 相同 第一 个 参数 createCombiner 
将 每个 元素 的 value 映射 成 新的 value 相当于 
mapvalue 方法 第二个 参数 mergeValue 是 说 当 发现 该 
元素 的 key 与 之前 已经 映 射成 新 value 
的 元素 的 key 相 同时 这个 新 形势 的 
value 与 新 遍历 到 的 元素 的 旧 形式 
的 value 如何 组合 第三个 参数 mergeCombiners 当 每个 分区 
的 元素 都 已经 形成 了 新形势 的 k v 
此时 如何 对 相同 k 的 value 进行 组合 scala 
/ / combineValues 计算 每个 key 的 平均值 scala val 
rdd1 = sc . parallelize Array panda 3 pink 1 
panda 6 scala val rdd2 = rdd1 . combineByKey | 
v = v 1 | nValue Int Int oValue = 
nValue . _ 1 + oValue nValue . _ 2 
+ 1 | nValue1 Int Int nValue2 Int Int = 
nValue1 . _ 1 + nValue2 . _ 1 nValue1 
. _ 2 + nValue2 . _ 2 | scala 
val rdd3 = rdd2 . mapValues v = v . 
_ 1 * 1.0 / v . _ 2 / 
/ Array panda 4.5 pink 1.0 4 并行度 优化 在 
执行 分组 和聚/nr 合时 可以 指定 spark 的 分区 数 
scalasc . parallelize data . reduceByKey x y = x 
+ y 10 / / 指定 10个 分区 ` ` 
` 数据分组 1 groupByKey 把 相同 键值 的 RDD K 
V 经过 聚合 变成 RDD K Iterator V . 因此 
scala rdd . reduceByKey func = rdd . groupByKey . 
mapValues v = v . reduce func 连接 1 cogroup 
将 两个 pair rdd 合并 成 一个 rdd 形式 为 
RDD k Iterator v Iterator w 2 leftOuterJoin 和 rightOuterJoin 
分别 表示 左右 连接 ` ` ` scalascala val rdd1 
= sc . parallelize Array 1 30 2 29 4 
21 scala val rdd2 = sc . parallelize Array 1 
zhangsan 2 lisi 3 wangwu scala rdd1 . cogroup rdd2 
. collect / / cogroupres0 Array Int Iterable Int Iterable 
String = Array 4 CompactBuffer 21 CompactBuffer 2 CompactBuffer 29 
CompactBuffer lisi 1 CompactBuffer 30 CompactBuffer zhangsan 3 CompactBuffer CompactBuffer 
wangwu scala rdd1 . leftOuterJoin rdd2 . collect / / 
l e f t O u t e r J 
o i n r e s 1 Array Int Int 
Option String = Array 4 21 None 2 29 Some 
lisi 1 30 Some zhangsan scala rdd1 . rightOuterJoin rdd2 
. collect / / r i g h t O 
u t e r J o i n r e 
s 2 Array Int Option Int String = Array 2 
Some 29 lisi 1 Some 30 zhangsan 3 None wangwu 
` ` ` 数据 排序 1 sortByKey 默认 按照 升序 
排列 相同 key 的 value 让 rdd 有 顺序 的 
save 到 磁盘 或 展示 出来 ` ` ` scalascala 
val rdd1 = sc . parallelize Array panda 3 pink 
1 panda 6 scala def sortInt = new Ordering Int 
{ | override def compare a Int b Int = 
a . toString . compare b . toString | } 
scala rdd1 . sortByKey . collect / / Array panda 
3 panda 6 pink 1 ` ` ` Pair RDD 
的 action 操作 1 countByKey 统计 每个 key 出现 的 
个数 2 collectAsMap 把 RDD 输出 Map 3 lookup key 
返回 key 对应 的 所有 value ` ` ` scalascala 
val rdd1 = sc . parallelize List 1 2 3 
4 3 6 scala rdd1 . countByKey res5 scala . 
collection . Map Int Long = Map 1 1 3 
2 scala rdd1 . collectAsMapres6 scala . collection . Map 
Int Int = Map 1 2 3 6 scala rdd1 
. lookup 3 res7 Seq Int = WrappedArray 4 6 
` ` ` 数据 分区 1 分布式 程序 中 通信 
的 代价 很大 因此 控制 数据分布 来 减少 网络 传输 
可以 极大 提升 整体 性能 但 分区 并非 对 所有 
的 应用 都是 好 的 比如 如果 RDD 只需 被 
扫描 一次 就 完全 不必 预先 对其 分区 只有 当 
数据集 多次 在 诸如 连接 这样 的 基于 key 的 
操作 时 分区 才会 有用 2 所有 的 Pair RDD 
都能 进行 分区 系统 会 根据 一个 针对 key 的 
函数 对 元素 进行 分组 spark 没有 给出 显示 控制 
每个 key 具体 落在 哪个 工作 节点 上 的 方法 
其中 一个 原因 是 节点 失败 时让然/nr 可以 在 其他 
节点 进行 工作 但是 spark 可以 确保 同 一组 key 
出现 在 同一 个 节点 上 eg 通过 哈希 分区 
将 一个 RDD 分成 100个 分区 此时 key 的 哈希 
值 对 100 取 模 结果 相同 的 记录 都会 
被 放在 同 一个 节点 上 3 应用 举例 内存 
中 保存 着 一份 由 UserID UserInfo 对 组成 的 
RDD 表 其中 UserInfo 包含 用户 订阅 的 url 这张 
表 会 周期性 的 与 一个 小 文件 进行 组合 
小文件 存储 着 过去 五 分钟 用户 所 访问 的 
url 因此 现在 要 每 五分钟 对 用户 访问 其 
未 订阅 的 url 做 统计 ` ` ` scala 
/ / userid 和 userinfo 这个 表 一般 不变 val 
userData = sc . s e q u e n 
c e F i l e U s e r 
I d UserInfo . persist / / 周期性 的 调用 
该 方法 处 理过去 5 分钟 产生 的 事件 日志 
def processNewLogs logFileName String { val events = sc . 
s e q u e n c e F i 
l e U s e r I d UserInfo / 
/ 用户 点击 事件 日志 val joined = userData . 
join events / / Pair RDD of UserId UserInfo LinkInfo 
val offTopicVisits = joined . filter { case userid userinfo 
linkinfo = userinfo . topics . contains linkinfo . topic 
} . count } & nbsp & nbsp & nbsp 
& nbsp & nbsp & nbsp 上面 的 代码 可以 
运行 但是 效率 不高 原因 在于 默认 情况 下 join 
操作 会 把 两个 pair RDD 中 的 所有 key 
的 哈希 值 都求/nr 出来 再将 key 哈希 值 相同 
的 记录 通过 网络 传到 一个 机器 上 然后 在 
那台 机器 上 进行 连接 操作 因为 userData 这张 用户 
订阅 uel 表 远远 比 没 五分钟 出现 的 小 
表 大 所以 每 五 分钟 都 要对 userData 表 
进行 哈希 取值 然后 跨 节点 混洗 & nbsp & 
nbsp & nbsp & nbsp & nbsp & nbsp 因此 
我们 的 改进 方法 就是 先 对 userData 表 进行 
哈希 分区 之后 持久化 到 内存 中 让 每 五分钟 
出现 的 操作 只对 小 表 进行 hash 取值 scalaval 
userData = sc . s e q u e n 
c e F i l e U s e r 
I d UserInfo . partitionBy new HashPartitioner 100 / / 
分成 100个 分区 分区 个数 至少 和 集群 中 机器 
的 个数 相同 . persist / / partitionBy 只是 转化 
操作 需要 持久化 才能 避免 每次 引用 该 rdd 重新 
分区 4 scala 可以 通过 RDD 的 partitioner 属性 获取 
分区 信息 scalascala val pairs = sc . parallelize List 
1 1 2 2 3 3 scala pairs . partitioner 
/ / 无 分区 res8 Option org . apache . 
spark . Partitioner = Nonescala val partitioned = pairs . 
partitionBy new org . apache . spark . HashPartitioner 2 
scala partitioned . partitioner / / 有 分区 res9 Option 
org . apache . spark . Partitioner = Some org 
. apache . spark . HashPartitioner @ 2 5 合适 
手动 设置 分区 有 效益 用 partitioner 方法 对 RDD 
分区 会对 很多 操作 产生 益处 ` reduceBy ` ` 
cogroup ` ` groupwith ` ` join ` ` leftOuterJoin 
` ` rightOuterJoin ` ` groupByKey ` ` combineByKey ` 
` lookup ` 但是 对于 像 reduceByKey 这样 操作 单个 
RDD 的 方法 他们 只 会把 每个 key 对应 的 
value 在 本地 机器 上 进行 计算 最终 把 所有 
机器 上 的 结果 进行 规约 这种 操作 本身 就 
不会 产生 数据 跨 节点 混洗 但是 想 cogroup 和 
join 这样 操作 两个 RDD 的 方法 如果 对 这 
两个 RDD 采用 相同 的 办法 手动 分区 那么 相同 
key 的 项 都在 同 一台 机器 上 这样 就 
避免 了 产生 数据 的 跨界 点 混洗 br 6 
自定义 分区 方式 & nbsp & nbsp & nbsp & 
nbsp 继承 ` org . apache . spark . Partitioner 
` 类 实现 下列 三个 方法 & nbsp & nbsp 
& nbsp & nbsp a numPartitions 创建 的 总分 区 
数 & nbsp & nbsp & nbsp & nbsp b 
getPartition key Any 根据 key 返回 分区 编号 编号 从0到/nr 
numPartitions 1 & nbsp & nbsp & nbsp & nbsp 
c equals 该 方法 来 判断 你 的 分区 器 
对象 是否 和 其他 分区 器 实例 相同 scala / 
* * 创建 一个 基于 域名 的 分区 器 这个 
分区 器 只对 url 中的 域名 部分 求 哈希 * 
/ class D o m a i n N u 
m P a r t i t i o n 
e r numParts Int extends Partitioner { override def numPartitions 
Int = numPartsoverride def getPartition key Any Int = { 
val domain = new java . net . Url key 
. toString . getHost val code = domain . hashCode 
% numPartitions if code 0 code + n u m 
P a r t i t i o n s 
e l s e c o d e } override 
def equals other Any Boolean = other match { case 
dnp D o m a i n N u m 
P a r t i t i o n e 
r = dnp . numPartitions = = n u m 
P a r t i t i o n s 
c a s e _ = false } } ` 
` ` 第四章 spark 的 共享 变量 累加器 1 由于 
spark 的 任务 再多 个 节点 上 跑 驱动 节点 
上 的 普通 变量 不能 再 多个 节点 上 共享 
因此 为了 解决 共享 变量 的 问题 提出 了 累加器 
结果 聚合 和 广播 变量 广播 scala / / 计算 
文件 中 空行 的 行数 scala val file = sc 
. textFile / root / README . md scala val 
count = sc . accumulator 0 count org . apache 
. spark . Accumulator Int = 0 scala val call 
= file . map line = { if line = 
= count + = 1 } scala println count . 
value 5 这个 例子 中 使用 了 累加器 在 数据 
读取 时 进行 错误 统计 而 没有 对 rdd 进行 
filter 和 reduce 实现 2 累加器 是 一种 只写 变量 
操作 节点 不能 访问 累加器 的 值 必须 要 对 
每次 更新 操作 进行 复杂 的 通信 3 通过 value 
变量 获取 可 累加器 的 值 4 累加器 操作 应该 
写在 action 的 动作 中 比如 写在 forEach 算子 因为 
在 转化 算子 中 比如 如果 有 一个 分 区 
执行 map 操作 失败 了 spark 会在 另 一个 节点 
重新 运行 该 任务 即 使该 节点 没有 崩溃 只是 
处理 速度 比 别的 节点 慢 很多 spark 也 可以 
抢占 式 的 再 另 一个 节点 上 启动 一个 
任务 副本 谁先 结束 任务 就 取 谁 的 副本 
因此 这种 情况 会 导致 累加器 操作 重复 执行 多次 
广播 变量 1 广播 变量 是 一种 只读 变量 2 
虽然 spark 会把 闭包 中的 变量 发送 到 每个 工作 
结点 但 这种 方法 比 广播 变量 低效 得多 原因 
有二/nr a 广播 变量 再 变量 的 发送 上 对 
大 对象 有 网络 优化 b 如果 这个 变量 来自于 
读取 文件 不适用 广播 变量 会 导致 这个 文件 会被 
不同 工作 节点 读取 多次 3 使用 value 获取 广播 
变量 的 值 scala scala val words = sc . 
broadcast List fuck shit scala words . value res1 List 
String = List fuck shit 4 当 广播 变量 的 
数据 很大 时 应当 选择 一种 合适 的 序列 化机制 
分区 共享 连接池 等 资源 1 当 map 等 转换 
操作 中 包含 访问 数据库 等 操作 时 就 需要 
通过 数据库 连接池 的 方式 重用 连接 而 分布式 的 
代码 在 不同 分区 中 运行 简单 的 复用 连接池 
对象 无法 正常 工作 2 scala 提供 了 mapPartitions function 
算子 这个 function 中的 变量 会在 分区 之间 a 共享 
这个 function 输入 为 每个 分区 的 元素 迭代 器 
返回 一个 执行 结果 的 序列 迭代 器 ` ` 
` scalaobject Test1 extends App { def s u m 
o f e v e r y P a r 
t i t i o n in Iterator Int Int 
= { var sum = 0in . reduce + } 
val conf = new SparkConf . setAppName test111 . setMaster 
mesos / / base1 5050 val sc = new SparkContext 
conf val input = sc . parallelize List 1 2 
3 4 val result = input . mapPartitions / / 
partVal Iterator Int RDD 中的 元素 是 Int 的 partVal 
= Iterator s u m o f e v e 
r y P a r t i t i o 
n partVal result . collect . foreach print _ / 
/ 2个 结果 3 7sc . stop } ` ` 
` 数值 操作 1 spark 对 包含 数值数据 的 RDD 
提供 了 统计学 方法 | 方法 | 含义 | | 
| | | count long value | RDD 中 元素 
个数 | | mean | 元素 平均值 | | vaiance 
| 元素 方差 | | samoleVariance | 从 采样 中 
计算出 方差 | | stdev | 标准差 | | sampleStdev 
| 采样 的 标准差 | 2 通过 RDD 的 stats 
方法 返回 org . apache . spark . util . 
StatCounter 对象 该 对象 包含 mean 平均值 stdev 标准 差等 
数值 方法 scala scala val rdd1 = sc . parallelize 
List 1 2 3 4 scala val stats = rdd1 
. stats scala stats . mean 第五章 submit 提交 集群 
驱动器 节点 2个 职责 1 把 用户程序 转换成 分布式 任务 
所有 的 spark 程序 遵从 同一个 流程 把 输入 数据 
创建 一 系列 RDD 通过 转 化 操作 派生 出 
新的 RDD 最后 使用 行动 操作 收集 或 存储 结果 
RDD 中 的 数据 spark 程序 会 隐式 地 创建 
出 一个 由 操作 组成 的 有向 无 环 图 
当 驱动器 程序执行 时会 把 这个 逻辑图 转换 为 物理 
执行计划 2 为 执行器 节点 调度 任务 驱动器 程序 必须 
在各 执行器 进程 间 协调 任务调度 执行器 进程 启动 后 
会 向 驱动器 进程 注册 自己 执行器 节点 2个 作用 
spark 应用 启动时 执行器 节点 就被 同时 启动 1 执行 
组成 spark 应用 的 任务 并将 结果 返回 给 驱动器 
程序 1 通过 BlockManager 为 用户 程序 中 要求 缓存 
的 RDD 提供 内存 式 存储 集群 管理器 spark 依赖 
集群 管理器 启动 驱动器 节点 集群 管理器 是 spark 的 
可 插拔 式 组件 集群 管理器 用于 启动 执行器 节点 
而 驱动器 可以 被 集群 管理器 也 可以 不被 集群 
管理器 启动 spark submit 1 基本 格式 shell bin / 
spark submit options app jar | python file app options 
构建 程序包 1 build . sbt ` ` ` scalaimport 
AssemblyKeys . _ name = Simple Project version = 1.0 
organization = com . databricks scalaVersion = 2 . 11.8 
l i b r a r y D e p 
e n d e n c i e s + 
+ = Seq / / Spark 依赖 org . apache 
. spark % spark core _ 2.10 % 1 . 
2.0 % provided / / 第三 方库/nr net . sf 
. jopt simple % jopt simple % 4.3 joda time 
% joda time % 2.0 / / 这条 语句 打开 
了 assembly 插件 的 功能 assemblySettings / / 配置 assembly 
插件 所 使用 的 JARjarName in assembly = my project 
assembly . jar / / 一个 用来 把 Scala 本身 
排除 在 组合 JAR 包 之外 的 特殊 选项 因为 
Spark / / 已经 包含 了 c a l a 
a s s e m b l y O p 
t i o n in assembly = assemblyOption in assembly 
. value . copy includeScala = false 2 project / 
assembly . sbtshell # 显示 project / assembly . sbt 
的 内容 $ cat project / assembly . sbtaddSbtPlugin com 
. eed3si9n % sbt assembly % 0 . 11.2 $ 
sbt assembly ` ` ` submit 的 部署 模式 1 
客户端 模式 客户端 模式 下 驱动器 程序 会 在 执行 
spark submit 的 机器 上 此时 终端 可以 看到 驱动器 
程序 的 输出 但 要 保持 终端 始终 连接 且 
该 机器 与 执行 节点 需要 有很 快速 的 网络交换 
2 集群 模式 deploy mode cluster 该 模式 下 驱动器 
程序 本身 也会在 集群 中 申请 资源 运行 自己 的 
进程 这样 可以 在 程序 运行时 关闭 电脑 3 yarn 
管理 的 spark 集群 既有 客户端 模式 又有 集群 模式 
但是 mesos 管理 的 spark 集群 只有 客户端 模式 但是 
mesos 管理 下 的 任务 可以 动态分配 CPU 即 执行器 
进程 占用 的 cpu 个数 会在 他们 执行 的 过程 
中 动态变化 这种 默认 的 方式 成为 细粒度 模式 mesos 
也 支持 粗粒度 模式 一 开始 分配 固定 的 cpu 
内存 Spark 应用 的 spark . mesos . coarse 设置 
为 true 4 yarn 集群 和 mesos 集群 的 选择 
Mesos 相对于 YARN 和 独立 模式 的 一大 优点 在于 
其 细粒度 共享 的 选项 该 选项 可以 将 类似 
Spark shell 这样 的 交互式 应用 中 的 不同 命令 
分配 到 不同 的 CPU 上 因此 这 对于 多 
用户 同时 运行 交互式 shell 的 用 例 更有 用处 
除此之外 选择 使用 yarn 模式 更为 合适 第六章 Spark SQLDataSet 
与 DataFrame 1 DataSet 是 Spark1 . 6 以后 新 
加 的 分布式 数据集 比 RDD 有 诸多 好处 比如/v 
强/a 类型/n 和/c 提供/v 更/d 有力/n 的/uj 表达式/n 方法/n 适应 
sql 执行 引擎 2 DataFrame 是 包含 列名 的 DataSet 
` ` ` scala / / 1 . 构建 对象 
的 分布式 数据集 scala val spark = SparkSession . builder 
. appName test sql . config p1 v1 . getOrCreate 
scala case class People name String age Int scala val 
ds1 = Seq People Andy 32 . toDSscala ds1 . 
show + + + | name | age | + 
+ + | Andy | 32 | + + + 
scala ds1 . collectres2 Array People = Array People Andy 
32 / / 2 . 构建 一般 数据类型 的 分布式 
数据集 scala val primitiveDS = Seq 1 2 3 5 
7 9 . toDS primitiveDS org . apache . spark 
. sql . Dataset Int = value int scala primitiveDS 
. map _ + 1 . collectres3 Array Int = 
Array 2 3 4 6 8 10 / / 3 
. 把 文件 读成 对象 的 分布式 数据集 scala case 
class Person name String age BigInt defined class Personscala val 
peopleDS = spark . read . json examples / src 
/ main / resources / people . json . as 
Person / / 不加 as Person 只会 读成 res6 Array 
org . apache . spark . sql . Row = 
Array null Michael 30 Andy 19 Justin peopleDS org . 
apache . spark . sql . Dataset Person = age 
bigint name string scala peopleDS . collectres5 Array Person = 
Array Person Michael null Person Andy 30 Person Justin 19 
` ` ` 解析 json 文件 1 文件格式 文件 的 
每 一行 都是/nr 一个 json 串 每 一行 会被 转化 
为 一个 Row 对象 2 Spark sql 读取 文件 后 
把 整个 形成 一个 DataFrame 带有 列名 的 表 ` 
` ` scalascala import org . apache . spark . 
sql . p a r k e s s i 
o n s c a l a import spark . 
implicits . _ scala val spark = SparkSession . builder 
. appName test sql . config p1 v1 . getOrCreate 
WARN SparkSession $ Builder Using an existing SparkSession some configuration 
may not take effect . spark org . apache . 
spark . sql . SparkSession = org . apache . 
spark . sql . SparkSession @ 399ef33f scala val df 
= spark . read . json examples / src / 
main / resources / people . json df org . 
apache . spark . sql . DataFrame = age bigint 
name string scala df . select name . show + 
+ | name | + + | Michael | | 
Andy | | Justin | + + scala df . 
select $ name $ age + 1 . show + 
+ + | name | age + 1 | + 
+ + | Michael | null | | Andy | 
31 | | Justin | 20 | + + + 
` ` ` 用 sql 语句 查询 session 中的 视图 
session 中的 视图 只能 存在 于 这个 session 中 一旦 
session 结束 视图 消失 如果 想 在 所有 session 中 
共享 就要 使用 全局 视图 scala df . c r 
e a t e O r R e p l 
a c e T e m p V i e 
w people scala val sqlDF = spark . sql select 
* from people sqlDF org . apache . spark . 
sql . DataFrame = age bigint name string scala sqlDF 
. collect res8 Array org . apache . spark . 
sql . Row = Array null Michael 30 Andy 19 
Justin 全局 视图 全局 视图 保存 在 系统 数据库 global 
_ temp 中 使用 全局 视图 时 必须 加上 数据库 
的 名字 ` ` ` scalascala df . c r 
e a t e G l o b a l 
T e m p V i e w people scala 
spark . sql SELECT * FROM global _ temp . 
people . collectres11 Array org . apache . spark . 
sql . Row = Array null Michael 30 Andy 19 
Justin scala spark . newSession . sql SELECT * FROM 
global _ temp . people . collectres12 Array org . 
apache . spark . sql . Row = Array null 
Michael 30 Andy 19 Justin ` ` ` spark sql 
支持 的 文件 类型 spark sql 支持 的 文件 类型 
json parquet jdbc orc libsvm csv text ` ` ` 
scalascala val peopleDF = spark . read . format json 
. load examples / src / main / resources / 
people . json peopleDF org . apache . spark . 
sql . DataFrame = age bigint name string scala peopleDF 
. select name age . write . format parquet . 
save namesAndAges . parquet SLF4J Failed to load class org 
. slf4j . impl . t a t i c 
L o g g e r B i n d 
e r . SLF4J Defaulting to no operation NOP logger 
i m p l e m e n t a 
t i o n L F 4 J See http 
/ / www . slf4j . org / codes . 
html # t a t i c L o g 
g e r B i n d e r for 
further details . ` ` ` 第七章 spark streaming 一 
. 入门 例子 要求 1 从一 台 服务器 的 7777 
端口 上 接受 一个 以 换行符 分割 的 多行 文本 
从中 筛选出 包含 error 的 行 并 打印 出来 2 
使用 命令 模 拟向 端口 7777 发送 消息 shell $ 
nc lk localhost 7777 在此 输入 文本 streaming 代码 ` 
` ` scalaimport org . apache . spark . _ 
import org . apache . spark . streaming . _ 
object TestMain { def main args Array String Unit = 
{ val conf = new SparkConf val ssc = new 
StreamingContext conf Seconds 1 val lines = ssc . socketTextStream 
localhost 9999 val words = lines . flatMap . split 
val pairs = words . map x = x 1 
val wordcounts = pairs . reduceByKey + _ wordcounts . 
print ssc . start ssc . awaitTermination } } s 
c a l a l i b r a r 
y D e p e n d e n c 
i e s + + = Seq org . apache 
. spark % spark streaming _ 2.11 % 2 . 
1.0 org . apache . spark % spark core _ 
2.11 % 2 . 1.0 ` ` ` 提交 代码 
1 submit 提交 命令 shell spark submit class Test master 
local 4 sparkdemo _ 2.11 1.0 . jar 2 ide 
中 配置 的 提交 jvm 参数 Dspark . master = 
local 4 Dspark . app . name = mystreamingtest 二 
. 架构 与 抽象 离散化 流 的 概念 1 spark 
streaming 使用 微 批次 架构 把 流式 数据 当做 一 
系列 小规模 批处理 对待 新 批次 按照 均匀 时间 间隔 
创建 出来 2 streaming 的 编程 模型 是 离散化 流 
DStream 他 是 一个 RDD 序列 每个 RDD 代表 数据流 
中 一个 时间 片 内 的 数据 straming 在 驱动器 
和 执行 节点 的 执行 过程 1 spark streamng 为 
每个 输入 源 启动 接收器 接收器 以 任务 的 形式 
运行 在 执行器 中 2 接收器 从 输入 源 收集 
数据 并 保存为 RDD 他们 在 收到 输入 数据 后 
会把 数据 复制 到 另一个 执行器 进程 来 保障 容错性 
3 数据 被 保存 在 执行器 进程 的 内存 中 
和 缓存 RDD 的 方式 一样 4 streamingcontext 周期性 的 
运行 spark 任务 来 处理 这些 数据 把 数据 和 
之前 区间 的 RDD 整合 spark streaming 的 容错性 1 
streaming 对 DStream 提供 的 容错性 和 spark 为 RDD 
提供 的 容错性 一致 只要 数据 还在 就能 根据 RDD 
谱系 图 重算 出 任意 状态 的 数据 集 2 
默认 情况 下 数据 分别 存在 于 两个 节点 上 
这样 可以 保证 数据 容错性 但是/c 只/d 根据/p 谱系/n 图/n 
重算/i 所有/b 从/p 程序/n 启动/vn 就/d 接收/v 到/v 的/uj 数据/n 
可能/v 会/v 花/v 很长时间/i 因此 streaming 提供 检查 点来 保存 
数据 到 hdfs 中 一般 情况 下 每 处理 5 
10次 就 保存 一次 scala ssc . checkpoint hdfs / 
/ . . . / / 本地 开发 时 可以 
使用 本地 路径 三 . streaming 的 转化 操作 DStream 
无状态 转化 1 无状态 转化 操作 是 应用 到 每个 
时间 片 的 RDD 上 的 eg map flatMap filter 
repartition reduceByKey groupByKey 2 无状态 转化 操作 也 可 用于 
把 两个 同时间 片 内 的 DStream 连接 起来 有 
状态 转化 操作 滑动 窗口 有 状态 转化 操作 需要 
打 开 检查 点 机制 来 确保 容错性 1 基于 
窗口 的 操作 在 一个 比 streamingcontext 批次 更长 的 
时间 范围 内 通过 整合 更多 个 批次 的 结果 
计算 整个 窗口 的 结果 所以 通过 window 产生 的 
DStream 中 每个 RDD 会 包含 多个 批次 的 数据 
可以 对 这些 数据 进行 count transform 操作 ` ` 
` scalaobject TestMain { def main args Array String Unit 
= { val sparkConf = new SparkConf val ssc = 
new StreamingContext sparkConf Seconds 1 ssc . checkpoint . / 
checkpoints / / 设置 检查 点 / / 初始 消息 
RDD val initialRDD = ssc . sparkContext . parallelize List 
hello 1 world 1 / / 创建 sparkstreaming 环境 val 
lines = ssc . socketTextStream localhost 9999 val words = 
lines . flatMap _ . split val pairs = words 
. map x = x 1 / / pairRDD / 
* * * @ param reduceFunc reduce function * @ 
param windowDuration 窗口 宽度 一次 批处理 的 时间 长短 * 
@ param slideDuration 两次 窗口 滑动 间隔 * / pairs 
. r e d u c e B y K 
e y A n d W i n d o 
w a Int b Int = a + b Seconds 
15 Seconds 1 pairs . print ssc . start ssc 
. awaitTermination } } ` ` ` 有 状态 转化 
操作 u p d a t e t a t 
e s B y K e y 与 mapWithState 1 
这 两个 方法 都是/nr 操作 PairRdd 的 他们 要求 新 
消息 以 只读 的 形式 到来 key 是 新消息 value 
是 新消息 对应 的 状态 2 mapWithState 需要 传入 mappingfunc 
来 计算 消息 的 新 状态 KeyType Option ValueType State 
StateType = MappedType 3 updateStateByKey 需要 传入 updateFunc 来 更 
新消息 状态 输入 参数 Seq V Option S = Option 
S 4 用 这 两个 方法 实现 持续 统计 单词 
技术 ` ` ` scalaimport org . apache . spark 
. _ import org . apache . spark . streaming 
. _ object TestMain { def main args Array String 
Unit = { val sparkConf = new SparkConf val ssc 
= new StreamingContext sparkConf Seconds 5 ssc . checkpoint . 
/ / 设置 检查 点 / / 初始 消息 RDD 
val initialRDD = ssc . sparkContext . parallelize List hello 
1 world 1 / / 创建 sparkstreaming 环境 val lines 
= ssc . socketTextStream localhost 9999 val words = lines 
. flatMap _ . split val wordDstream = words . 
map x = x 1 / / pairRDD / / 
定义 状态 更新 函数 输入 key 新到 的 pairRDD 中 
value 值 已经 保存 的 key 的 状态值 返回 一个 
键值 对 key State val mappingFunc = word String one 
Option Int state State Int = { val sum = 
one . getOrElse 0 + state . getOption . getOrElse 
0 val output = word sum state . update sum 
output } / / values 是 新消息 pairRDD 的 value 
值 state 是以 保存 的 状态值 def updateFunc values Seq 
Int state Option Int Option Int = { val newcount 
= state . getOrElse 0 + values . size Some 
newcount } val stateDstream = wordDstream . mapWithState StateSpec . 
function mappingFunc . initialState initialRDD / / 通过 mapping 方法 
累积 所 有消息 状态 val stateDstream = wordDstream . updateStateByKey 
Int updateFunc _ / / 通过 update 方法 累积 所 
有消息 状态 stateDstream . print ssc . start ssc . 
awaitTermination } } ` ` ` 四 . 输出 操作 
print 1 Dstream 如果 没有 被 执行 输出 操作 则 
这些 DStream 不会 被 求值 若 StreamingContext 中 没有 定义 
输出 操作 整个 context 就 不会 启动 保存 文件 1 
s a v e A s T e x t 
F i l e s c a l a / 
/ output 1497685765000 . txt 文件 根据 streamingcontext 设置 的 
时间 间隔 执行 一次 wordcounts . saveAsTextFiles output txt 2 
saveAsHadoopFile 该 函数 接受 一种 Hadoop 输出 格式 作为 参数 
可以 用 这个 函数 将 DStream 保存 成eq/nr u e 
n c e F i l e s c a 
l a val pairs = words . map x = 
new Text x new LongWritable 1 val wordcounts = pairs 
. reduceByKey x LongWritable y LongWritable = new LongWritable x 
. get + y . get wordcounts . s a 
v e A s H a d o o p 
F i l e s e q u e n 
c e F i l e O u t p 
u t F o r m a t Text LongWritable 
outdir txt 存入 外部 存储系统 如 mysql 中 scala wordcounts 
. foreachRDD { / / DStream 中的 每个 RDD rdd 
= rdd . foreachPartition { / / 每台 机器 上 
的 RDD 都能 公用 一个 分区 item = pool . 
getConn . save item / / 保存 每 一条 数据 
} } 五 . 输入 源 每个 DStream 与 一个 
Receiver 对象 相关联 该 对象 从 数据源 接收数据 并 将其 
存储 到 spark 集群 的 内存 中 核心 数据源 1 
文件 流 监听 一个 hdfs 下 的 文件夹 一旦 有 
新文件 进入 就将 其 作为 输入 源 处理 成 DStream 
这种方式 文件 一旦 进入 该 文件夹 就 不能 再 修改 
ssc . fileStream KeyClass ValueClass InputFormatClass dir 2 自定义 一个 
接收器 acceptor 接收 akka 数据源 Custom Receiver Guide 3 RDD 
队列 模拟 输入 源 可以 把 一 系列 的 RDD 
作为 DStream 的 一批 数据 c ssc . queueStream queueOfRDDs 
附加 数据源 1 kafka 数据源 2 flume 数据源 多 数据源 
与 集群 规模 1 当 使用 类似 union 将 多个 
DStream 合并 时 使用 多个 接收器 用来 提高 聚合 操作 
中 的 数据 获取 吞吐量 一个 接收器 会 成为 系统 
的 性能 瓶颈 此外 有时 需要 用 不同 接收器 从 
不同 数据源 接受 各种 数据 此时 应用 分配 的 CPU 
个数 至少 为 数据源 个数 + 1 最后 一个 用来 
计算 这些 数据 六 24/7 不间断 运行 配置 检查点 机制 
检查点 是 streaming 中 容错性 的 主要 机制 streaming 可 
通过 转化 图 的 谱系 图 来 重算 状态 检查点 
机制 则 可以 控制 要在 转化 图中 回溯 多远 其次 
如果 是 驱动器 程序 崩溃 用户 在 重启 驱动器 程序 
并 让 驱动器 程序 从 检查 点 回复 则 streaming 
可以 读取 之前 运行 的 程序 处理 数据 进度 并从 
这里 继续 驱动器 程序 容错 让 驱动器 程序 重启 后 
先从 检查点 恢复 s p a r k s t 
r e a m i n g c o n 
t e x t 再 重新 创建 streamingcontext 保证 错误 
恢复 def c r e a t e t r 
e a m i n g C o n t 
e x t = { val sparkConf = new SparkConf 
val ssc = new StreamingContext sparkConf Seconds 1 ssc . 
checkpoint . / checkpoints / / 设置 检查 点 ssc 
} val ssc = StreamingContext . getOrCreate . / checkpoints 
c r e a t e t r e a 
m i n g C o n t e x 
t _ 工作 节点 容错 1 streaming 使用 与 spark 
相同 的 容错 机制 所有 从外部 数据源 中 收到 的 
数据 都会 在 多个 工作 节点 上 备份 所 偶有 
RDD 操作 都能 容忍 一个 工作 节点 的 失败 根据 
RDD 谱系 图 系统 就 能把 丢失 的 数据 从 
输入 数据备份 中 计算出来 2 工作 节点 上 的 接收器 
容错 接受 其 提供 如下 保障 a . 所有/b 从/p 
hdfs/w 中/f 读取/v 的/uj 数据/n 都是/nr 可靠/v 的/uj 因为 底层 
文件系统 有 备份 strreaming 会 记住 那些 数据 放到 了 
检查 点中 并在 应用 崩溃 后 从 检查 点 处 
继续 执行 b . 对于 像 kafka 这种 不 可靠 
数据源 spark 会把 数据 放到 hdfs 中 仍然 确保 不 
丢失 数据 七 . 性能 批次 和 窗口 大小 1 
streaming 可 使用 的 最小 批次 间隔 一般 为 500 
毫秒 2 这个 结果 是 从 一个 较大 的 时间 
窗口 10s 逐步 缩小 实验 而来 当 减小 时间 窗口 
后 如果 streaming 用户界面 现实 的 处理 时间 保持 不变 
就 可以 进一步 减小 批次 大小 如果 处理 时间 增大 
则 认为 达到 了 应用 极限 此外 滑动 步长 也对 
性能 有着 巨大 影响 当 计算 代价 巨大 并 成为 
系 瓶颈 就 应该 考虑 增加 滑动 步长 提高 并行度 
1 增加 接收器 数目 如果 记录 太多 导致 单台/nr 机器 
来不及 读入 并 分发 数据 接收器 就会 成为 系统 瓶颈 
此时 可以 通过 创建 多个 输入 DStream 来 增加 接收器 
的 数目 然后 使用 union 合并 为 一个 打的 数据源 
2 将 收到 的 数据 显 式 的 重新 分区 
如果 接收器 数目 无法 增加 可以 通过 使用 DStream repartition 
来 重新 分区 输 入流 从而 重新分配 收到 的 数据源 
3 提高 聚合 计算 的 并行度 对于 像 reduceByKeyy 这样 
的 操作 可以 再 第二个 参数 制定 并行度 垃圾 回收 
和 内存 使用 可以 通过 修改 gc 策略 使用 CMS 
策略 scala spark submit conf spark . executor . extraJavaOptions 
= XX + U s e C o n c 
M a r k w e e p G C 
第八章 spark 机器学习 第九章 spark 调试 与 调 优 一 
. 使用 SparkConf 配置 Sparkspark 每个 配置 项 都是 基于 
字符串 形式 的 键值 对 eg 通过 setAppName 设置 spark 
. app . namespark 允许 通过 spark submit 脚本 动态 
配置 配置 项 脚本 会把 这些 配置 项这/nr 知道 运行时 
环境 中 当 一个 新的 SparkConf 被 创建 出来时 这些 
环境变量 会被 检测 出来 并 自动 配置 好 因此 用户 
只 需要 创建 一个 空的/nr SparkConf 并 直接 传给 SparkContext 
即可 spark submit class com . example . MyApp name 
My app conf spark . ui . port = 36000 
myApp . jarspark submit 脚 本会 查找 conf / spark 
defaults . conf 文件 然后 尝试 读取 该 文件 中 
以 空格 隔开 的 键值 对 数据 也 可 通过 
properties File 自定义 文件 路径 # 提交 脚本 spark subnmit 
class com . example . MyApp properties File myconfig . 
conf MyApp . jar # myconfig . conf 内容 spark 
. master local 4 spark . app . name My 
App spark . ui . port 36000sparkconf 的 优先级 选择 
1 最高 用户 显示 调用 的 sparkconfig 的 set 方法 
设置 的 选项 2 其次 spark submit 传递 的 参数 
3 写在 配置文件 中的 值 二 . RDD 依赖 关系 
RDD 依赖 1 窄 依赖 父 RDD 的 每个 Partition 
最多 被子 RDD 的 1个 分区 所 使用 a 窄 
依赖 分为 两种 一对一 依赖 OneToOneDependcy 一对一 范围 依赖 RangeDependency 
2 宽 依赖 指 计算 中 会 产生 shuffle 操作 
的 RDD 依赖 表示 一个 父 RDD 的 Partition 会被 
多个 子 RDD 的 Partition 使用 a groupByKey 就是 常见 
的 宽 依赖 算子 DAG 生成 机制 1 DAG 生成 
过程 就是 对 计算 中 stage 的 划分 2 对于 
窄 依赖 RDD 之间 的 数据 不 需要 进行 shuffle 
这些 处理 操作 可以 在 同一 台 机器 的 内存 
中 完成 所以 窄 依赖 在 划分 中 被 分成 
一个 stage 3 对于 宽 依赖 由于 数据 之间 存在 
shuffle 必须 等到 父 RDD 所有 数据 shuffle 完成 之后 
才能 进行 后续 操作 所以 在 此处 进行 stage 划分 
RDD 检查点 1 checkpoint 也是 存储 RDD 结果 的 一种 
方式 它 不同 于 persist 将 数据 存储 在 本地 
磁盘 而是 把 结果 存储 在 HDFS 中 shell scala 
val wordcount = sc . textFile / root / README 
. txt . flatMap _ . split . map word 
= word 1 . reduceByKey _ + _ scala wordcount 
. checkpointRDD 容错 1 RDD 容错 分为 3个 层面 调度 
层 RDD 血统 层 Checkpoint 层 2 调度 层 容错 
分别/d 在/p Stage/w 输出/v 时/n 出错/v 与/p 计算/v 时/n 出错/v 
stage 输出 出错 上层 调度 器 DAGScheduler 会 进行 重试 
stage 计算 出错 时 该 task 会 自动 被 重新 
计算 4次 3 RDD LINEAGE 血统 层 容错 a 基于 
各 RDD 各项 Transaction 构成 了 compute chain 在 部分 
结果 丢失 的 时候 可以 根据 Lineage 重新 计算 b 
窄 依赖 中 数据 进行 的 流水线 处理 子 RDD 
的 分区 数据 同样在 父 RDD 的 分区 中 并不 
存在 冗余 计算 4 CheckPoint 层 容 错在 宽 依赖 
上 做 检查 点 可以 避免 Lineage 很长 重新 计算 
而 带来 的 冗余 计算 三 . Spark 执行 步骤 
作业 任务 步骤 关于 作业 1 rdd . toDebugString 方法 
查看 RDD 谱系 图 2 行动 操作 会 触发 生成 
一个 作业 这个 作业 包含 了 transaction 动作 产生 的 
多个 步骤 查找 作业 信息 1 4040 端口 展示 了 
作业 列表 里面 包含 stage 执行 的 详情 该 页面 
包含 了 一个 作业 的 性能 表现 若果 有些 步骤 
特别 慢 还 可以 点击 进去 查看 是 哪 段 
用户 代码 2 数据 倾斜 是 导致 性能 问题 的 
常见 原因 当 有 少量 任务 对于 其他 任务 需要 
花费 大量 时间 时 一般 就是 发生 了 数据 倾斜 
四 . 驱动器 日志 和 执行器 日志 spark 独立 模式 
下 所有 日志 再 主 节点 的 网页 中 直接 
显示 存储 于 spark 目 录下 的 work 目录 中 
Mesos 模式 下 日志 存储 在 Mesos 从 节点 的 
work 目录 中 可 通过 主 节点 用户界面 访问 YARN 
模式 下 当 作业 运行 完毕 可以 通过 yarn logs 
applicationId 来 打包 一个 应用 日志 如果 要 查看 运行 
再 YARN 上 的 应用 日志 可以/c 从/p 资源/n 管理器/n 
的/uj 用户界面/n 进入/v 从/p 节点/n 页面/n 浏览 特定 节点 容器 
的 日志 log4j 配置文件 的 示例 在 conf / log4j 
. properties . template 也 可 通过 spark submit Files 
添加 log4j . properties 文件 五 . 关键 性能 考量 
并行度 序列化 格式 内存管理 硬件 供给 