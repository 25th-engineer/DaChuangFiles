1 . 简介 决策树 Decision Tree 是 在 已知 各种 
情况 发生 概率 的 基础 上 通过 构成 决策树 来 
求取 净现值 的 期望值 大于 等于 零 的 概率 评价 
项目 风险 判断 其 可行性 的 决策 分析方法 是 直观 
运用 概率分析 的 一种 图解法 由于 这种 决策 分支 画成 
图形 很像 一棵树 的 枝干 故称 决策树 在 机器 学习 
中 决策树 是 一个 预测模型 他 代表 的 是 对象 
属性 与 对象 值 之间 的 一种 映射 关系 Entropy 
= 系统 的 凌乱 程度 使用 算法 ID3 C 4.5 
和C/nr 5.0 生成树 算法 使用 熵 这一 度量 是 基于 
信息学 理论 中 熵 的 概念 决策树 是 一种 树形 
结构 其中 每个 内部 节点 表示 一个 属性 上 的 
测试 每个 分支 代表 一个 测试 输出 每个 叶 节点 
代表 一种 类别 决策树 学习 通常 包括 3 个 步骤 
特征选择 决策树 的 生成 决策树 的 修剪 1.1 决策树 场景 
场景 一 二十 个 问题 有 一个 叫 二十 个 
问题 的 游戏 游戏规则 很 简单 参与 游戏 的 一方 
在 脑海 中 想 某个 事物 其他 参与者 向他/nr 提问 
只允许 提 20 个 问题 问题 的 答案 也 只能 
用 对 或 错 回答 问 问题 的 人 通过 
推断 分解 逐步 缩小 待 猜测 事物 的 范围 最后 
得到 游戏 的 答案 场景 二 邮件 分类 一 个 
邮件 分类 系统 大致 工作 流程 如下 首先 检测 发送 
邮件 域名地址 如果 地址 为 myEmployer . com 则将 其 
放在 分类 无聊 时 需要 阅读 的 邮件 中 如果 
邮件 不是 来自 这个 域名 则 检测 邮件 内容 里 
是否 包含 单词 曲棍球 如果 包含 则将 邮件 归类 到 
需要 及时 处理 的 朋友 邮件 如果 不 包含 则将 
邮件 归类 到 无需 阅读 的 垃圾 邮件 1.2 定义 
分类 决策树 模型 是 一种 描述 对 实例 进行 分类 
的 树形 结构 决策树 由 结点 node 和 有向 边 
directed edge 组成 结点 有 两种 类型 内部 结点 internal 
node 表示 一个 特征 或 属性 叶 结点 leaf node 
表示 一个 类 用 决策树 分类 从根/nr 节点 开始 对 
实例 的 某一 特征 进行 测试 根据 测试 结果 将 
实例 分配 到 其子 结点 这时 每一 个子 结点 对应 
着 该 特征 的 一个 取值 如此 递归 地 对 
实例 进行 测试 并 分配 直至 达到 叶 结点 最后 
将 实例 分配 到 叶 结点 的 类 中 2 
. 决策树 原理 熵 熵 entropy 指 的 是 体系 
的 混乱 的 程度 在 不同 的 学科 中 也有 
引申 出 的 更为 具体 的 定义 是 各 领域 
十分 重要 的 参量 信息熵 香农 熵 是 一种 信息 
的 度量 方式 表示 信息 的 混乱 程度 也 就是说 
信息 越 有序 信息熵 越低 例如 火柴 有序 放在 火柴 
盒里 熵值 很低 相反 熵值 很高 信息 增益 在 划分 
数据集 前后 信息 发生 的 变化 称为 信息 增益 2.1 
工作 原理 我们 使用 createBranch 方法 构造 一个 决策树 如下 
所示 检测 数据 集中 的 所有 数据 的 分类 标签 
是否 相同 If so return 类 标签 Else 寻找 划分 
数据集 的 最好 特征 划分 之后 信息熵 最小 也 就是 
信息 增益 最大 的 特征 划分 数据集 创建 分支 节点 
for 每个 划分 的 子集 调用函数 createBranch 创建 分支 的 
函数 并 增加 返回 结果 到 分支 节 点中 return 
分支 节点 2.2 决策树 开发 流程 1 . 收集 数据 
可以 使用 任何 方法 2 . 准备 数据 树 构造 
算法 只 适用 于 标称 型 数据 因此 数值 型 
数据 必须 离散化 3 . 分析 数据 可以 使用 任何 
方法 构造 树 完成 之后 我们 应该 检查 图形 是否 
符合 预期 4 . 训练 算法 构造 树 的 数据结构 
5 . 测试 算法 使用 经验 树 计算 错误率 经验 
树 没有 搜索 到 较好 的 资料 有兴趣 的 同学 
可以 来 补充 6 . 使用 算法 此 步骤 可以 
适用 于 任何 监督 学习 算法 而 使用 决策树 可以 
更好 地 理解 数据 的 内在 含义 2.3 决策树 算法 
特点 优点 计算 复杂度 不高 输出 结果 易于 理解 对 
中间值 的 缺失 不 敏感 可以 处理 不 相关 特征 
数据 缺点 可能 会 产生 过度 匹配 问题 适用 数据类型 
数值 型 和 标称 型 3 . 实战 案例 3.1 
项目 概述 根据 以下 2 个 特征 将 动物 分成 
两类 鱼类 和非/nr 鱼类 特征 不 浮出水面 是否 可以 生存 
是否 有 脚蹼 3.2 开发 流程 1 收集 数据 可以 
使用 任何 方法 我们 利用 createDataSet 函数 输入 数据 def 
createDataSet dataSet = 1 1 yes 1 1 yes 1 
0 no 0 1 no 0 1 no labels = 
no surfacing flippers return dataSet labels 2 准备 数据 树 
构造 算法 只 适用 于 标称 型 数据 因此 数值 
型 数据 必须 离散化 此处 由于 我们 输入 的 数据 
本身 就是 离散化 数据 所以 这 一步 就 省略 了 
3 分析 数据 可以 使用 任何 方法 构造 树 完成 
之后 我们 应该 检查 图形 是否 符合 预期 计算 给定 
数据集 的 香农 熵 的 函数 def calcShannonEnt dataSet # 
求 list 的 长度 表示 计算 参与 训练 的 数据 
量 numEntries = len dataSet # 计算 分类 标签 label 
出现 的 次数 labelCounts = { } # the the 
number of unique elements and their occurance for featVec in 
dataSet # 将 当前 实例 的 标签 存储 即 每 
一行 数据 的 最后 一个 数据 代表 的 是 标签 
currentLabel = featVec 1 # 为 所有 可能 的 分类 
创建 字典 如果 当前 的 键值 不 存在 则 扩展 
字典 并将 当前 键值 加入 字典 每个 键值 都 记录 
了 当前 类别 出现 的 次数 if currentLabel not in 
labelCounts . keys labelCounts currentLabel = 0 labelCounts currentLabel + 
= 1 # 对于 label 标签 的 占 比 求出 
label 标签 的 香农 熵 shannonEnt = 0.0 for key 
in labelCounts # 使用 所有 类 标签 的 发生 频率 
计算 类别 出现 的 概率 prob = float labelCounts key 
/ numEntries # 计算 香农 熵 以 2 为 底 
求 对数 shannonEnt = prob * log prob 2 return 
shannonEnt 按照 给定 特征 划分 数据集 将 指定 特征 的 
特征值 等于 value 的 行 剩下 列作 为子 数据集 def 
splitDataSet dataSet index value splitDataSet 通过 遍历 dataSet 数据集 求出 
index 对应 的 colnum 列 的 值 为 value 的 
行 就是 依据 index 列 进行 分类 如果 index 列 
的 数据 等于 value 的 时候 就要 将 index 划分 
到 我们 创建 的 新的 数据 集中 Args dataSet 数据集 
待 划分 的 数据 集 index 表示 每 一行 的 
index 列 划分 数据集 的 特征 value 表示 index 列 
对应 的 value 值 需要 返回 的 特征 的 值 
Returns index 列为 value 的 数据集 该 数据集 需要 排除 
index 列 retDataSet = for featVec in dataSet # index 
列为 value 的 数据集 该 数据集 需要 排除 index 列 
# 判断 index 列 的 值 是否 为 value if 
featVec index = = value # chop out index used 
for splitting # index 表示 前 index 行 即若 index 
为 2 就是 取 featVec 的 前 index 行 reducedFeatVec 
= featVec index 请 百度 查询 一下 extend 和 append 
的 区别 list . append object 向 列表 中 添加 
一个 对象 object list . extend sequence 把 一个 序列 
seq 的 内容 添加到 列表 中 1 使用 append 的 
时候 是 将 new _ media 看作 一个 对象 整体 
打包 添加到 music _ media 对象 中 2 使用 extend 
的 时候 是 将 new _ media 看作 一个 序列 
将 这个 序列 和 music _ media 序列 合并 并 
放在 其 后面 result = result . extend 1 2 
3 print result result . append 4 5 6 print 
result result . extend 7 8 9 print result 结果 
1 2 3 1 2 3 4 5 6 1 
2 3 4 5 6 7 8 9 reducedFeatVec . 
extend featVec index + 1 # index + 1 表示 
从 跳过 index 的 index + 1行 取 接下来 的 
数据 # 收集 结果 值 index 列为 value 的 行 
该行 需要 排除 index 列 retDataSet . append reducedFeatVec return 
retDataSet 选择 最好 的 数据集 划分 方式 def c h 
o o s e B e s t F e 
a t u r e T o p l i 
t dataSet c h o o s e B e 
s t F e a t u r e T 
o p l i t 选择 最好 的 特征 Args 
dataSet 数据集 Returns bestFeature 最优 的 特征 列 # 求 
第一行 有 多少 列 的 Feature 最后 一 列 是 
label 列 嘛 numFeatures = len dataSet 0 1 # 
数据集 的 原始 信息熵 baseEntropy = calcShannonEnt dataSet # 最优 
的 信息 增益值 和 最优 的 Featurn 编号 bestInfoGain bestFeature 
= 0.0 1 # iterate over all the features for 
i in range numFeatures # create a list of all 
the examples of this feature # 获取 对应 的 feature 
下 的 所有 数据 featList = example i for example 
in dataSet # get a set of unique values # 
获取 剔 重 后的/nr 集合 使用 set 对 list 数据 
进行 去 重 uniqueVals = set featList # 创建 一个 
临时 的 信息 熵 newEntropy = 0.0 # 遍历 某 
一列 的 value 集合 计算 该 列 的 信息 熵 
# 遍历 当前 特征 中 的 所有 唯一 属性值 对 
每个 唯一 属性值 划分 一次 数据集 计算 数据集 的 新 
熵值 并对 所有 唯一 特征值 得到 的 熵 求和 for 
value in uniqueVals subDataSet = splitDataSet dataSet i value # 
计算 概率 prob = len subDataSet / float len dataSet 
# 计算 信息熵 newEntropy + = prob * calcShannonEnt subDataSet 
# gain 信息 增益 划分 数据集 前后 的 信息 变化 
获取 信息熵 最大 的 值 # 信息 增益 是 熵 
的 减少 或者 是 数据 无序 度 的 减少 最后 
比较 所有 特征 中 的 信息 增益 返回 最好 特征 
划分 的 索引 值 infoGain = baseEntropy newEntropy print infoGain 
= infoGain bestFeature = i baseEntropy newEntropy if infoGain bestInfoGain 
bestInfoGain = infoGain bestFeature = i return bestFeatureQ 上面 的 
newEntropy 为什么 是 根据 子集 计算 的 呢 A 因为 
我们 在 根据 一个 特征 计算 香农 熵 的 时候 
该 特征 的 分类 值 是 相同 这个 特征 这个 
分类 的 香农 熵 为 0 这 就是 为什么 计算 
新的 香农 熵 的 时候 使用 的 是 子集 4 
训练 算法 构造 树 的 数据结构 创 建树 的 函数 
代码 如下 def createTree dataSet labels classList = example 1 
for example in dataSet # 如果 数据集 的 最后 一 
列 的 第一 个 值 出现 的 次数 = 整个 
集合 的 数量 也就 说 只有 一个 类别 就只 直接 
返回 结果 就行 # 第一 个 停止 条件 所有 的 
类 标签 完全相同 则 直接 返回 该类 标签 # count 
函数 是 统计 括号 中的 值 在 list 中 出现 
的 次数 if classList . count classList 0 = = 
len classList return classList 0 # 如果 数据集 只有 1列 
那么 最 初 出现 label 次数 最多 的 一类 作为 
结果 # 第二个 停止 条件 使用 完 了 所有 特征 
仍然 不能 将 数据集 划分 成仅/nr 包含 唯一 类别 的 
分组 if len dataSet 0 = = 1 return majorityCnt 
classList # 选择 最优 的 列 得到 最优 列 对应 
的 label 含义 bestFeat = c h o o s 
e B e s t F e a t u 
r e T o p l i t dataSet # 
获取 label 的 名称 bestFeatLabel = labels bestFeat # 初始化 
myTree myTree = { bestFeatLabel { } } # 注 
labels 列表 是 可变 对象 在 PYTHON 函数 中 作为 
参数 时传址/nr 引用 能够 被 全局 修改 # 所以 这行 
代码 导致 函数 外 的 同名 变量 被 删除 了 
元素 造成 例句 无法 执行 提示 no surfacing is not 
in list del labels bestFeat # 取出 最优 列 然后 
它 的 branch 做 分类 featValues = example bestFeat for 
example in dataSet uniqueVals = set featValues for value in 
uniqueVals # 求出 剩余 的 标签 label subLabels = labels 
# 遍历 当前 选择 特征 包含 的 所有 属性值 在 
每个 数据集 划分 上 递归 调用函数 createTree myTree bestFeatLabel value 
= createTree splitDataSet dataSet bestFeat value subLabels # print myTree 
value myTree return myTree 5 测试 算法 使用 决策树 执行 
分类 代码 如下 def classify inputTree featLabels testVec classify 给 
输入 的 节点 进行 分类 Args inputTree 决策树 模型 featLabels 
Feature 标签 对应 的 名称 testVec 测试 输入 的 数据 
Returns classLabel 分类 的 结果 值 需要 映射 label 才能 
知道 名称 # 获取 tree 的 根 节点 对于 的 
key 值 firstStr = inputTree . keys 0 # 通过 
key 得到 根 节点 对应 的 value secondDict = inputTree 
firstStr # 判 断根 节点 名称 获取 根 节 点在 
label 中的 先后顺序 这样 就 知道 输入 的 testVec 怎么 
开始 对照 树 来做 分类 featIndex = featLabels . index 
firstStr # 测试数据 找到 根 节点 对应 的 label 位置 
也 就 知道 从 输入 的 数据 的 第 几位 
来 开始 分类 key = testVec featIndex valueOfFeat = secondDict 
key print + + + firstStr xxx secondDict key valueOfFeat 
# 判断 分枝 是否 结束 判断 valueOfFeat 是否是 dict 类型 
if isinstance valueOfFeat dict classLabel = classify valueOfFeat featLabels testVec 
else classLabel = valueOfFeat return classLabel 6 使用 算法 此 
步骤 可以 适用 于 任何 监督 学习 算法 而 使用 
决策树 可以 更好 地 理解 数据 的 内在 含义 构造 
决策树 是 很 耗时 的 任务 即使 很小 的 数据 
集 也 要花费 几秒 如果 用 创 建好 的 决策树 
解决 分类 问题 就 可以 很快 完成 因此 为了 节省 
计算 时间 最好能 每次 执行 分类 时 调用 已经 构造 
好 的 决策树 为了 解决 这个 问题 需要 使用 Python 
模块 pickle 序列化 对象 序列化 对象 可以 在 磁盘 上 
保存 对象 并在 需要 的 时候 读 取出来 任何 对象 
都 可以 执行 序列化 包括 字典 对象 下面 代码 是 
使用 pickle 模块 存储 决策树 def storeTree inputTree filename impory 
pickle fw = open filename w pickle . dump inputTree 
fw fw . close def grabTree filename import pickle fr 
= open filename return pickle . load fr 通过 上面 
的 代码 我们 可以 把 分类器 存储 在 硬 盘上 
而 不用 每次 对 数据 分类 时 重新学习 一遍 这 
也是 决策树 的 优点 之一 + + K 近邻 算法 
就 无法 持久化 分类器 + + 1 决策树 维基百科 https 
/ / zh . wikipedia . org / wiki / 
% E 5% 86% B 3% E 7% AD % 
96% E 6% A 0% 91 2 机器学习 实战 Peter 
Harrington 3 机器学习 周志华 