转载 请 注明 出处 http / / www . cnblogs 
. com / ymingjingr / p / 4271742 . html 
目录 机器学习 基石 笔记 1 在 何时 可以 使用 机器学习 
1 机器学习 基石 笔记 2 在 何时 可以 使用 机器学习 
2 机器学习 基石 笔记 3 在 何时 可以 使用 机器学习 
3 修改版 机器学习 基石 笔记 4 在 何时 可以 使用 
机器学习 4 机器学习 基石 笔记 5 为什么 机器 可以 学习 
1 机器学习 基石 笔记 6 为什么 机器 可以 学习 2 
机器学习 基石 笔记 7 为什么 机器 可以 学习 3 机器学习 
基石 笔记 8 为什么 机器 可以 学习 4 机器学习 基石 
笔记 9 机器 可以 怎样 学习 1 机器学习 基石 笔记 
10 机器 可以 怎样 学习 2 机器学习 基石 笔记 11 
机器 可以 怎样 学习 3 机器学习 基石 笔记 12 机器 
可以 怎样 学习 4 机器学习 基石 笔记 13 机器 可以 
怎样 学 得 更好 1 机器学习 基石 笔记 14 机器 
可以 怎样 学 得 更好 2 机器学习 基石 笔记 15 
机器 可以 怎样 学 得 更好 3 机器学习 基石 笔记 
16 机器 可以 怎样 学 得 更好 4 十六 Three 
Learning Principles 三 学习 原则 16.1 Occam s Razor 奥卡姆 
剃刀 定律 entia non sunt multiplicanda praeter necessitatem 此处 是 
拉丁文 译为 英文 是 entities must not be multiplied beyond 
necessity 意思 是 如 无必要 勿 增 实体 出自 奥卡姆 
为了 纪念 此人 将 这 句话 叫做 奥卡姆 剃刀 Occam 
s Razor 将 奥卡姆 剃刀 定律 应用 在 机器 学习上 
意思 是 使用 的 模型 尽可能 的 简单 如 1 
所示 对于 同 一组 数据集 两种 不同 的 分类 模型 
应该 如何 选择 1 不同 模型 的 选择 通过 肉眼观察 
当然 会 选择 左边 的 图形 因为 它 简单 于是 
产生 了 两个 问题 什么情况 意味着 模型 是 简单 的 
如何 得知 简单 的 模型 会 有好 的 表现 先从 
第一 个 问题 着手 简单 意味着 什么 对于 一个 假设 
h 参数 越小 意味着 越 简单 如 1中 左图 只 
需要 极少 的 参数 如 圆心 和 半径 对于 一个 
假设 空间 H 有效 的 假设 数量 越少 则 意味着 
越 简单 两者之间 有何 联系 两者 是 密切 相关 的 
如 一个 假设 空间 H 的 假设 数量 为 则 
单一 的 假设 可用 bits 表示 因此 如果 假设 空间 
H 的 模型 是 简单 的 很小 则 处在 此 
假设 空间 中 的 假设 h 也是 简单 的 很小 
接着 使用 一个 直觉 上 的 解释 阐述 为什么 越 
简单 的 模型 会有 越好 的 效果 假设 一个 数据集 
的 规律性 很差 如 输入 样本 的 输出 标记 都是 
随便 标记 的 此种 情况 很少 有 甚至 没有 假设 
函数 能使 得该 样本 的 等于 0 如果 一个 数据集 
能被 某 模型 分开 则 该 数据集 的 规律性 不会 
特别 差 在 使用 简单 模型 将 某 数据集 大致 
区 分开 时 则 可以 确定 该 数据集 是 具有 
某种 规律性 的 如果 是 用 复杂 模型 将 某 
数据集 分开 则 无法 确定 是 数据集 具有 规律性 还是 
模型 足够 复杂 恰巧 将 混乱 的 数据 集 分离 
因此 在 运用 模型 时 先 使用 简单 的 模型 
一般 使用 最 简单 的 线性 模型 16.2 Sampling Bias 
抽样 偏差 如果 数据 的 抽样 出现 偏差 则 机器学习 
也 会 产生 偏差 此种 偏差 称为 抽样 偏差 sampling 
bias 对 上述 结论 用 一个 技术性 的 说明 在 
VC 理论 中 其中/r 一个/m 假设/vn 是/v 训练/vn 样本/n 和/c 
测试/vn 样本/n 以/p 同样/d 的/uj 概率/n 来自/v 于/p 同一/b 个/q 
数据分布/n 因此 在 训练 数据 来自 于 而 测试 样本 
的 概率 时 VC 理论 无法 适用 即 不成立 这就 
好比 当 你 数学 学得 好时 测试 你 的 英语 
你 不 可能 保证 你 英语 测试 也能 通过 因此/c 
训练样本/n 和/c 测试/vn 样本/n 要/v 都/d 独立/v 同/p 分布/v 的/uj 
来自/v 于/p 概率分布/n P/w 16.3 Data Snooping 数据 窥探 的 
危害 在 学习 过程 的 任何 一步 中 数据集 都 
可能 被 影响 假设有 8年 的 交易 数据 将 前 
6年 的 作为 训练 数据 后 2年 的 作为 测试数据 
期望 得到 通过 前 二十 天 的 数据 预测 出 
第 21天 交易 通过/p 有/v 偷窥/v 和/c 没有/v 偷窥/v 两种/m 
预测/vn 的/uj 收益/n 情况/n 作/v 对比/v 害/v 如 2 所示 
红色 部分 为 使用 8年 的 放缩 统计数据 建立 模型 
预测 后 两年 的 收益 情况 蓝色 部分 是 使用 
前 6年 的 数据 建立 模型 预测 后 两年 的 
收益 情况 从该/nr 图表 可知 即使 是 间接 的 偷窥 
了 统计 信息 的 模型 也比 完全 不 偷窥 的 
模型 表现 好处 很多 2 偷窥 与否 的 收益 对比 
图 当然 在做 机器学习 时 很难 做到 不 偷窥 只 
可能 做到 尽量 避免 比 保留 验证 数据 做 验证 
等 对 所有 的 情况 都 存在 质疑 16.4 Power 
of Three 三 的 威力 本节 是 对 整个 课程 
做 一次 总结 总结 中 发现 此 课程 介绍 的 
内容 很 巧 的 都与 数字 三 有关 本节 的 
题目 因此 得名 首先 本 课程 介绍 了 三种 与 
机器 学习 有关 的 领域 数据挖掘 人工智能 和 统计 三个 
理论 保证 霍夫 丁 不等式 单一 假设 确认 时 使用 
多箱 霍夫 丁 不等式 有限 多个 假设 验证 时 使用 
和 VC 限制 无限 多个 假设 训练 时 使用 三个 
模型 二元 分类 模型 包含 PLA 和 pocket 线性 回归 
和 logistic 回归 三种 重要 工具 特征 转换 正则化 和 
验证 三个 原则 奥克 姆 剃刀 抽样 偏差 和 数据 
窥探 未来 学习 的 方向 也 分为 三种 更多 的 
转换 方式 更多 的 正则化 和 没有 标记 的 情况 
