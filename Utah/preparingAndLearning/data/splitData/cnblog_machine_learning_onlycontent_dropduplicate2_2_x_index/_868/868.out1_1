逻辑 回归 先前 所讲 的 线性 回归 主要 是 一个 
预测 问题 根据 已知 的 数据 去 预测 接下来 的 
情况 线性 回归 中的 房价 的 例子 就 很好 地 
说明 了 这个 问题 然后 在 现实 世界 中 很多 
问题 不是 预测 问题 而 是 一个 分 类 问题 
如 邮件 是否 为 垃圾 邮件 金融 交易 是否 正常 
肿瘤 是否 是 良性 的 这 新 问题 都是/nr 一个 
分类 在 分类 问题 中 结果 一般 是 为 0 
和1/nr 0 称为 负 样本 或者 是 负 类 如 
良性肿瘤 1 称为 正 样本 或者 是 正 类 如 
恶性肿瘤 那么 是否 能够 使用 线性 回归 的 方式 来 
解决 分类 问题 呢 如下 是 一个 辨别 肿瘤 是 
良性 还是 恶性 的 例子 可以 看到 貌似 线性 回归 
是 可以 解决 分类 问题 的 但是 如果 是 下面 
这个 情况 当 多 了 一个 数据 之后 发现 线性 
回归 就 存在 很 明显 的 偏差 但是 这个 数据 
是 完全 无 意义 的 干扰 数据 线性 回归 为了 
拟合 数据 导致 最后 的 分类 出错 在 线性 回 
归中 还 存 一个 很 严重 的 问题 就是 在 
分类 问题 中 最终 的 结果 只有 0 和1/nr 但是 
在 线性 回 归中 会 出现 小于 1 和 大于 
0 的 结果 总之 线性 回归 是 不适合 处理 分类 
问题 的 线性 回归 问题 就 可以 考虑 使用 逻辑 
回归 来 解决 了 PS 逻辑 回归 的 叫法 是 
历史 原因 和 回归 并 没有 什么 关系 逻辑 回归 
表示 逻辑 回归 的 表示 用 下面 的 一张 图 
来 进行 说明 其中 x 表示 的 是 特征向量 g 
代表 逻辑 函数 Logistic function 是 一个 常用 的 曲线 
函数 Sigmoid function 表达式 为 函数 的 图像 就 如上 
图 所示 h 表示 的 就是 逻辑 回归 带入 到 
函数 g 中 最终 得到 的 表达式 就是 函数 h 
表示 的 就是 当 输入 特征 X 时 根据 输入 
的 特征 计算 输出 变量 Y = 1 的 可能性 
假设 h x = 0.7 表示 的 就是 患有 恶性肿瘤 
的 概率 为 0.7 判定 边界 Decision Boundary 判定 边界 
能够 让 我们 更好 地 理解 逻辑 回归 的 函数 
在 进行 分类 中 的 意义 上图 就是 逻辑 回归 
的 函数 表示 以 及 图像 在 逻辑 回 归中 
我们 预测 如果 当 h = 0.5时 y = 1 
当 h 0.5时 y = 0 当 y = 1时 
要求 h = 0.5 意味着 g z = 0.5 那么 
就 表示 z 0 最后 就 得到 了 θ tX 
= 0 同理 当 y = 0时 最后 得到 θ 
tX 0 下面 就 以 一个 例子 来 说明 问题 
其中 的 theta 的 参数 分别为 3 1 1 存在 
如上 图 所示 的 数据 以及 表示 函数 如果 要 
预测 y = 1 的 概率 最后 得到 的 表达式 
为 那么 最后 得到 的 方程 在 坐标轴 显示 的 
如下 其中 的 方程 就是 一个 判定 边界 通过 这条 
线 就 可以 分辨出 正 样本 和负/nr 样本 了 除 
了 这种 线性 的 判定 边界 之外 还有 一些 其他 
形状 的 判定 边界 如 圆形 除 了 这种 线性 
的 判定 边界 之外 还有 一些 其他 形状 的 判定 
边界 如 圆形 逻辑 回归 中的 代价 函数 在 将 
逻辑 回归 中的 代价 函数 之前 先来 回顾 一下 之前 
讲过 的 在线 性 回归 中的 代价 函数 上面 就是 
之前 讲过 的 线性 回归 中的 代价 函数 这个 代价 
函数 在 线性 回归 中 能够 很好 地 使用 但是 
在 逻辑 回 归中 却 会 出现 问题 因为 将 
逻辑 回归 的 表达式 带入 到 h 函数 中 得到 
的 是 一个 非 凸函数 的 图像 那么 就 会 
存在 多个 局部 最优 解 无法 像 凸函数 一样 得到 
全局 最优 解 示例 如下 那么 在 逻辑 回归 中就 
需要 重新 定义 代价 函 数了 逻辑 回归 中的 代价 
函数 为 其中 最后 得到 的 函数 h 和 Cost 
函数 之前 的 关系 如下 构建 一个 这样 的 函数 
的 好处 是 在于 当 y = 1时 h = 
1 如果 h 不为 1时 误差 随着 h 的 变 
小而 增大 同样 当 y = 0时 h = 0 
如果 h 不为 0时 误差 随着 h 的 变 大而 
增大 代价 函数 中的 梯度 下降 在上 一节 中的 逻辑 
回归 中的 代价 函数 中 给出 了 代价 函数 的 
定义 最后 可以 简化 为 最终 的 求解 问题 就是 
要求 回归 函数 的 值 最小 那么 同样 可以 使用 
在 线性 回归 中所 用到 的 梯度 函数 上图 就是 
逻辑 回归 的 梯度 求解 过程 虽然 看起来 和 线性 
回归 相似 但 实则 是 完全 不同 的 在 线性 
回 归中 h 函数 为 theta 的 转置 与 X 
的 乘积 但是 在 逻辑 回 归中 则 不是 这样 
就 导致 了 两者 在 运算 方面 和 优化 方面 
是 完全 不同 的 但是 在 运行 梯度 下降 算法 
之前 进行 特征 缩放 依旧 是 非常 重要 的 高级 
优化 优化 算法 除了 讲到 的 梯度 下降 算法 之外 
还有 一些 叫做 共轭 梯度 下降 算法 BFGS L BFGS 
使用 这些 共轭 梯度 下降 算法 的 好处 在于 不 
需要 手动 地 选择 学习率 a 这些 算法 会 自行 
尝试 选择 a 比 梯度 下降 算法 运算 更快 一般 
情况 下 在 常见 的 机器学习 算 法库 中都 带有 
这些 算法 不 需要 程序员 手动 实现 这些 算法 多 
类别 分类 问题 现实 世界 中 除了 二元 的 分类 
问题 还有 多元 的 分类 问题 如对 天气 的 分类 
是 晴天 多云 小雨 等等 天气 多元 分类 问题 与 
二元 分类 问题 的 区别 如下 多元 分类 的 思路 
与 二元 分类 问题 的 解决 思路 是 类似 的 
可以 将 多元 问题 变为 两元 问题 具体 如下 这样 
n 元的/nr 分类 问题 就会 进行 n 次 的 机器 
学习 的 分类 算法 对 每一次 的 分类 结果 即为 
h x 那么 经过 n 此 分类 之后 最后 得到 
的 结果 为 那么 当 输入 新的 训练 集 或者 
是 变量 X 只 需要 按照 上面 的 思路 进行 
分类 其中 的 h x 的 最大 值 就是 对应 
的 最后 的 分类 结果 为了 能到/nr 远方 脚下 的 
每一步 都 不能 少 