LeNet 原始 论 文中 的 版本 数据 集为 MNIST 输入 
\ 32 * 32 * 1 \ N a m 
e k e r n e l s t r 
i d e p a d I n p u 
t O u t p u t P a r 
a m e t e r NumberConv16 5 × 51032 
× 32 × 128 × 28 × 6 5 × 
5 × 1 + 1 × 6subsampling + sigmoid2 × 
22028 × 28 × 614 × 14 × 6 1 
+ 1 × 6Conv216 5 × 51014 × 14 × 
610 × 10 × 16 5 × 5 × 3 
+ 1 × 6 + 5 × 5 × 4 
+ 1 × 6 + 5 × 5 × 4 
+ 1 × 3 + 5 × 5 × 6 
+ 1 × 1subsampling + sigmoid2 × 22010 × 10 
× 165 × 5 × 16 1 + 1 × 
16Conv3120 5 × 5105 × 5 × 161 × 1 
× 120 5 × 5 × 16 + 1 × 
120FC4 + tanh 1 x 1 x 1 2 0 
8 4 1 x 1 x 1 2 0 x 
8 4 R B F 84100 下 采样 的 方式 
为 \ sigmoid w * \ sum _ { i 
= 0 } ^ { 1 } \ sum _ 
{ j = 0 } ^ { 1 } x 
_ { ij } + b \ 这里 \ w 
\ 和 \ b \ 是 可 学习 参数 conv2 
使用 了 包括 3层 4层 6层 三种 通 道数 不同 
的 filters 然后 将 它们 的 输出 拼接 在 一起 
作为 这 一层 的 输出 这里 使用 多组 不同 形式 
的 卷积 的 原因 不 完全 机制 限制 了 连接 
的 数量 减少 计算 量 破坏 了 网络 的 对称性 
最后 一层 是 RBF 虽是 全 连接 但 参数 W 
是 给 定值 输入 的 \ 84 \ 维 向量 
相当于 \ 12 * 7 \ 的 比 特图 输出 
的 每一个 值 代表 了 输入 与 输出 对应 的 
参数 权重 的 均方 误差 MSE 要求 损失 函数 可以 
使 正确 的 label 对应 的 输出 值 越小 越好 
Caffe 中的 实现 输入 的 batch size \ 64 \ 
归一化 scale \ 0.00390625 \ N a m e k 
e r n e l s t r i d 
e p a d I n p u t O 
u t p u t P a r a m 
e t e r NumberConv1 + ReLU20 5 × 51028 
× 28 × 124 × 24 × 6 5 × 
5 × 1 + 1 × 20Max Pooling2 × 22024 
× 24 × 2012 × 12 × 200Conv2 + ReLU50 
5 × 51012 × 12 × 208x8x50 5 × 5 
× 20 + 1 × 50Max Pooling2 × 2 2 
0 8 x 8 x 5 0 4 x 4 
x 5 0 0 F C 3 + ReLU 4x4x505004x4x50 
× 500FC4 + Softmax 50010500x10 损失 函数 为 SoftmaxWithLoss 细节 
权重 的 初始化 方式 为 xavier 偏移 值 的 初始化 
方式 为 constant 默认 设为 0 权重 的 学习率 为 
base _ learning _ rate 偏移 值 的 学习率 为 
base _ learning _ rate 的 两倍 AlexNet ImageNet Classification 
with Deep Convolutional Neural Networks 结构 N a m e 
k e r n e l s t r i 
d e p a d I n p u t 
O u t p u t P a r a 
m e t e r NumberConv1 + ReLU96 11 × 
1140227 × 227 × 355 × 55 × 96 11 
× 11 × 3 + 1 × 96Max P o 
o l i n g 3 x 3 2 0 
5 5 x 5 5 x 9 6 2 7 
x 2 7 x 9 6 0 C o n 
v 2 + ReLU256 5 x 5 1 2 2 
7 x 2 7 x 9 6 2 7 x 
2 7 x 2 5 6 5x5x96 + 1 x256Max 
P o o l i n g 3 x 3 
2 0 2 7 x 2 7 x 2 5 
6 1 3 x 1 3 x 2 5 6 
0 C o n v 3 + ReLU384 3 x 
3 1 1 1 3 x 1 3 x 2 
5 6 1 3 x 1 3 x 3 8 
4 3x3x256 + 1 x384Conv4 + ReLU384 3 x 3 
1 1 1 3 x 1 3 x 3 8 
4 1 3 x 1 3 x 3 8 4 
3x3x384 + 1 x384Conv5 + ReLU256 3 x 3 1 
1 1 3 x 1 3 x 3 8 4 
1 3 x 1 3 x 2 5 6 3x3x384 
+ 1 x256Max P o o l i n g 
3 x 3 2 0 1 3 x 1 3 
x 2 5 6 6 x 6 x 2 5 
6 0 F C 6 + ReLU + Dropout 6 
x 6 x 2 5 6 4 0 9 6 
6 x 6 x 2 5 6 x 4 0 
9 6 F C 7 + ReLU + Dropout 4 
0 9 6 4 0 9 6 4 0 9 
6 x 4 0 9 6 F C 8 + 
Softmax 4 0 9 6 1 0 0 0 4 
0 9 6 x 1 0 0 0 浮点 乘法 
运算量 \ 55 * 55 * 96 * 11 * 
11 * 3 + 27 * 27 * 256 * 
5 * 5 * 96 + 13 * 13 * 
384 * 3 * 3 * 256 + 13 * 
13 * 384 * 3 * 3 * 384 + 
13 * 13 * 256 * 3 * 3 * 
384 + 6 * 6 * 256 * 4096 + 
4096 * 4096 + 4096 * 1000 \ 参数 60 
Million MACs 1.1 Billion 实际 计算 量 比 这个 值 
小 因为 Conv 层 中 使用 了 group 第一 次 
引入 ReLU 并 使用 了 overlapping Max Pooling 在前 两个 
全 连接 层 使用 了 系数 为 0.5 的 Dropout 
因此 测试 时 需要 将 结果 乘以 0.5 在 论文 
中 还 引入 了 局部 响应 归一化 层 LRN 但 
后来 证明 没有 效果 提升 同时 overlapping Max Pooling 也 
没有 被 广泛 应用 训练 细节 batch size 128 momentum 
0.9 weight decay 5e 4 学习率 初始 为 0 每当 
error 停止 下降 就 除以 10 数据 增强 对于 训练 
集 随机 剪裁 对于 测试 集 将/d 原始/v 图片/n 和/c 
对应/vn 的/uj 水平/n 镜像/n 从/p 中间/f 和/c 四边/f 剪切/vn 然后 
将 这 十个 预测 结果 取 平均 PCA jittering 基于 
整个 训练 集 特征值 λ 和 特征向量 P 对于 每个 
epoch 的 每一张 图像 从 均值 0 方差 0.1 的 
高斯分布 随机 抽取 α 对 原始 图片 三维 通道 做 
λ * α * P 映射 ZFNet Visualizing and Understanding 
Convolutional Networks 引入 DeconvNet 来 可视化 某一 feature map 将该 
层 其余 feature maps 设置 为 0 然后 经过 一 
系列 的 i unpool ii rectify iii filters 映 射回 
像素 级别 其中 i unpool max pooling 同时 记录 位置 
信息 ii ReLU iii 原 卷积 对应 矩阵 的 转置 
论文 中 使用 可视化 方法 对于 某 一层 的 某个 
feature map 我们 在 验证 集中 寻找 使该 feature map 
的 response 最大 的 九张 图片 画出 这 九张 图片 
中的 该 feature map 反 卷积 映射 的 结果 并和 
原图 相 对应 的 patch 对比 特征 可视化 层数 越高 
提取 的 特征 越 复杂 不变性 越 明显 越 discrimination 
训练 过程 中 特征 收敛 过程 层数 越低 收敛 越早 
特征 不变性 1 图像 缩放 平移 对模型 第一层 影响 较大 
对 后面 基本 没有 影响 2 图像 旋转 后 特征 
不具有 不变性 通过 第一层 和 第二层 可视化 对 AlexNet 进行 
改造 得到 ZFNet 减小 感受 野 降低 步长 模型 对 
局部 特征 敏感 模型 具有特征 泛 化性 因此 可以 用于 
迁移 学习 Network in Network 引入 1x1 卷积 Original Conv 
3x3 ReLU PoolingMLPConv Conv 3x3 ReLU Conv 1x1 ReLU . 
. . Pooling 使用 average pooling 取代 FC 结构 MLPConv 
堆叠 + global average poolingVGG Very Deep Convolutional Networks for 
Large Scale Image Recognition 使用 了 统一 的 卷积 结构 
证明 了 深度 对模型 效果 的 影响 LRN 层 没有 
提升 效果 堆叠 多个 3x3 的 感受 野 可以获得 类似于 
更大 感受 野 的 效果 同时 多层 3x3 卷积 堆叠 
对应 的 参数 更少 减少 参数 相当于 正则化 效果 运用 
了 Network in Network 中 提出 的 1x1 卷积 训练 
方式 256 batch size 0.9 momentum 5e 4 weight decay 
0.5 dropout ratio learning rate 初始 1e 2 每当 停止 
提升 后 下降 为 之前 的 十分之一 数据 增强 颜色 
增强 color jittering PCA jittering 尺度 变换 训练 集 随机 
缩 放到 256 512 然后 随机 剪 切到 224x224 尺度 
变换 对应 的 测试 方法 1 随机 裁剪 取 平均 
类似 AlexNet 2 将 FC 转为 Conv 原始 图片 直接 
输入 模型 这时 输出 不再 是 1x1x1000 而是 NxNx1000 然后 
取 平均 GoogLeNet Going deeper with convolutions 论文 发表 之前 
相关 的 工作 当时 研究 者 关注 增加 层数 和 
filter 数目 可能 会 导致 小 样本 过拟合 并 需要 
大量 的 计算资源 并 通过 Dropout 防止 过拟合 尽管 有人 
认为 Max Pooling 造成 了 空间 信息 的 损失 但 
这种 结构 在 localization detection human pose estimation 中 均 
取得 很好 的 成绩 Network In Network 中 引入 1x1 
卷积 增加 了 神经 网络 的 表示 能力 representational power 
GoogLeNet 中的 Inception 结构 也 运用 了 1x1 卷积 来 
进行 降 维 处理 为/p 解决/v 过拟合/i 和/c 计算/v 代/q 
价高/n 的/uj 问题/n 使用 稀疏 网络 来 代替 全 连接 
网络 在 实际 中 即使 用 卷积 层 Inception 结构 
对于 输入 的 feature maps 分别 通过 1x1 卷积 3x3 
卷积 5x5 卷积 和 Max Pooling 层 并将 输出 的 
feature maps 连接起来 作为 Inception 的 输出 同时 获得 不同 
感受 野 的 信息 在 3x3 卷积 5x5/i 卷积/n 前面/f 
和/c 池化层/nr 后面/f 接/v 1x1/i 卷积/n 起降 维 的 作用 
Inception v2 v3 Rethinking the Inception Architecture for Computer VisionCNN 
的 通用 设计 思想 通常 随着 网络 层数 增加 空间 
特征 数 逐渐 降低 通 道数 逐渐增加 不要 过度 压缩 
损失 精度 信息 避免 表征 瓶颈 增加 非线性 特征 可以 
解 耦合 特征 卷积 的 实现 形式 为 空间 聚合 
spatial aggregation 1x1 卷积 降 维 不会 产生 严重 的 
影响 猜测 相邻 通道 之间 具有 强 相关性 卷积 前 
降 维 基本 不会 损失 信息 5x5 卷积 的 感受 
野 与 两个 3x3 卷积 堆叠 所 对应 的 感受 
野 相同 使用 后者 可以 大大 减少 网络 参数 7x7 
同理 此外 两个/m 3x3/i 卷积/n 后/f 各连/i 接/v 一个/m 非线性/b 
层/q 的/uj 效果/n 优于/v 仅在/i 最后/f 连接/nr 一个/m 非线性/b 层/q 
NxN/w 的/uj 卷积/n 可以用/i 1xN/i 与/p Nx1/i 的/uj 卷积/n 堆叠/v 
实现/v 使用 Label Smoothing 增加 网络 的 正则 能力 使用 
Batch Normalization 和 RMSPropResNet Deep Residual Learning for Image Recognition 
网络 过深 会 导致 退化 问题 通过 短路 连接 解决 
该 问题 通常 一个 residual unit 的 残差 部分 使用 
二 至 三层 的 函数 映射 或称 卷积 层 shortcut 
部分 与 残差 部分 进行 eltwise add 后再 连接 非线性 
层 补充 论文 Inception v4 Inception ResNet and the Impact 
of Residual Connections on Learning 相比 v3 Inception v4 的 
主要 变化 是 网络 的 加深 卷积 和的/nr 个数 也 
大大 增加 Inception ResNet 即将 ResNet 的 残差 结构 替换成 
了 简单 的 Inception 结构 文中 认为 ResNet 对 提高 
精度 的 帮助 较小 ResNet 论文 中 提到 ResNet 解决 
了 退化 问题 加快 过深 网络 的 训练 速度 是 
其 主要 优势 对于 特别 深 的 残差 网络 可以 
通过 在 残 层 结构 后接 一个 scaling 如 残 
层 结构 输出 的 一个 元素 乘以 0.2 来 提高 
模型 稳定性 Wide Residual Networks 很多 论文 在 ResNet 的 
网络 结构 基础 上 进行 了 细微 的 改动 主要 
的 观点 是 ResNet 存在 diminishing feature reuse 的 问题 
网络 过深 很多 残差 块 对 最终 结果 只 做出 
了 很少 的 贡献 甚至 有些 残差 块 没有 学到 
有用 的 信息 反而 在 之前 学到 的 feature representation 
中 加入 了 轻微 噪声 ResNet 提高 一点 精 度 
可能 需要 将 深度 增加一倍 而且 会 产生 diminishing feature 
reuse 问题 因此 提出 增加 残差 块 的 宽度 减少 
网络 深度 的 WRNs 文中 提到 论文 Deep Networks with 
Stochastic Depth 中 通过 使 残差 块 随机 失 活来 
降低 每次 训练 使 网络 的 深度 作者 做 了 
大量 的 实验 表明 两个 3x3 卷积 堆叠 的 残差 
块 的 效果 优于 其他 残差 块 结构 同时 增加 
深度 和 宽度 可以 提高 精度 但 需要 正则 增加 
宽度 比 增加 深度 更容易 训练 在 瘦长 和 矮胖 
的 网络 中 在 残差 块 中 的 两个 卷积 
间 增加 Dropout 层 均 有效果 在 不做 大量 的 
数据 增强 的 前提 下 Dropout 的 效果 比 Batch 
Normalization 更好 Aggregated Residual Transformations for Deep Neural Networks 指出 
Inception 过于 复杂 不易 迁移 到 其他 问题 上 ResNet 
存在 diminishing feature reuse 的 问题 提出 了 基数 的 
概念 残差 块 采用 split transform merge 的 策略 基数 
类似 group 表示 split 的 数目 这种 架构 可以 接近 
large and dense layers 的 表示 能力 但 只 需要 
很少 的 计算资源 ResNeXt 有/v 一种/m 基本/n 形式/n 和/c 两种/m 
变体/n 一种 类似 Inception ResNet 一种 使用 group 实现 Densely 
Connected Convolutional NetworksDenseNet 极大 地 增加 了 特征 重用 的 
能力 其 有 以下 优点 1 . 参数 少 通过 
向后 连接 的 方式 保留 学到 的 信息 2 . 
改进 了 前 向 反向 传播 更易 训练 3 . 
增加 了 监督 学习 的 能力 4 . 在 小 
数据 上 不易 过拟合 即 增加 了 正则化 的 能力 
Dense Block 中 对于 任意 一层 的 feature maps 一 
方面 会 通过 BN ReLU Conv 1x1 BN ReLU Conv 
3x3 得到 下 一层 的 feature maps 另一方面 会与 其后 
的 每 一层 feature maps 连接 在 一起 并 提出 
了 growth rate 的 概念 增长率 k 是 3x3 卷积 
层 输出 的 feature maps 数 而 1x1 卷积 层 
输出 的 feature maps 数 为 4k 在 ImageNet 比赛 
中 运用 的 模型 每 两个 Dense Block 中间 以 
Batch Normalization ReLU Conv 1x1 Aver Pooling 2x2 相连 Squeeze 
and Excitation N e t w o r k s 
q u e e z e N e t ShuffleNet 
MobileNet X c e p t i o n q 
u e e z e N e t 的 基本 
结构 是 File Module 输入 的 feature maps 先 经过 
1x1 卷积 降 维 然后 分别 通过 1x1 卷积 和 
3x3 卷积 并将 两个 输出 连接起来 作为 这个 模块 整体 
的 输出 SqueezeNet 的 结构 就 是 多个 File Module 
堆叠 而成 中间 夹杂着 max pooling 最后 用 deep compression 
压缩 MobileNet 的 基本 结构 是 3x3 depth wise Conv 
加 1x1 Conv 1x1 卷积 使得 输出 的 每一个 feature 
map 要 包含 输入 层 所有 feature maps 的 信息 
这种 结构 减少 了 网络 参数 的 同时 还 降低 
了 计算 量 整个 MobileNet 就是 这种 基本 结构 堆叠 
而成 其中 没有 池化层/nr 而是 将 部分 的 depth wise 
Conv 的 stride 设置 为 2 来 减小 feature map 
的 大小 ShuffleNet 认为 depth wise 会 带来 信息 流通 
不畅 的 问题 利用 group convolution 和 channel shuffle 这 
两个 操作 来 设计 卷积 神经网络 模型 以 减少 模型 
使用 的 参数 数量 同时 使用 了 ResNet 中的 短路 
连接 ShuffleNet 通过 多个 Shuffle Residual Blocks 堆叠 而成 Xception 
相对于 借鉴 了 depth wise 的 思想 简化 了 Inception 
v3 Xception 的 结构 是 输入 的 feature maps 先 
经过 一个 1x1 卷积 然后 将 输出 的 每一个 feature 
map 后面 连接 一个 3x3 的 卷积 再 逐 通道 
卷积 然后 将 这些 3x3 卷积 的 输出 连接起来 补充 
权重 初始化 方式 Xavier Understanding the difficulty of training deep 
feedforward neural networksMSRA Delving Deep into Rectifiers Surpassing Human Level 
Performance on ImageNet Classification 补充 激活 函数 补充 Dropout 层 
补充 Batch Normalization 层 LeNet 原始 论 文中 的 版本 数据 集为 MNIST 输入 
\ 32 * 32 * 1 \ N a m 
e k e r n e l s t r 
i d e p a d I n p u 
t O u t p u t P a r 
a m e t e r NumberConv16 5 × 51032 
× 32 × 128 × 28 × 6 5 × 
5 × 1 + 1 × 6subsampling + sigmoid2 × 
22028 × 28 × 614 × 14 × 6 1 
+ 1 × 6Conv216 5 × 51014 × 14 × 
610 × 10 × 16 5 × 5 × 3 
+ 1 × 6 + 5 × 5 × 4 
+ 1 × 6 + 5 × 5 × 4 
+ 1 × 3 + 5 × 5 × 6 
+ 1 × 1subsampling + sigmoid2 × 22010 × 10 
× 165 × 5 × 16 1 + 1 × 
16Conv3120 5 × 5105 × 5 × 161 × 1 
× 120 5 × 5 × 16 + 1 × 
120FC4 + tanh 1 x 1 x 1 2 0 
8 4 1 x 1 x 1 2 0 x 
8 4 R B F 84100 下 采样 的 方式 
为 \ sigmoid w * \ sum _ { i 
= 0 } ^ { 1 } \ sum _ 
{ j = 0 } ^ { 1 } x 
_ { ij } + b \ 这里 \ w 
\ 和 \ b \ 是 可 学习 参数 conv2 
使用 了 包括 3层 4层 6层 三种 通 道数 不同 
的 filters 然后 将 它们 的 输出 拼接 在 一起 
作为 这 一层 的 输出 这里 使用 多组 不同 形式 
的 卷积 的 原因 不 完全 机制 限制 了 连接 
的 数量 减少 计算 量 破坏 了 网络 的 对称性 
最后 一层 是 RBF 虽是 全 连接 但 参数 W 
是 给 定值 输入 的 \ 84 \ 维 向量 
相当于 \ 12 * 7 \ 的 比 特图 输出 
的 每一个 值 代表 了 输入 与 输出 对应 的 
参数 权重 的 均方 误差 MSE 要求 损失 函数 可以 
使 正确 的 label 对应 的 输出 值 越小 越好 
Caffe 中的 实现 输入 的 batch size \ 64 \ 
归一化 scale \ 0.00390625 \ N a m e k 
e r n e l s t r i d 
e p a d I n p u t O 
u t p u t P a r a m 
e t e r NumberConv1 + ReLU20 5 × 51028 
× 28 × 124 × 24 × 6 5 × 
5 × 1 + 1 × 20Max Pooling2 × 22024 
× 24 × 2012 × 12 × 200Conv2 + ReLU50 
5 × 51012 × 12 × 208x8x50 5 × 5 
× 20 + 1 × 50Max Pooling2 × 2 2 
0 8 x 8 x 5 0 4 x 4 
x 5 0 0 F C 3 + ReLU 4x4x505004x4x50 
× 500FC4 + Softmax 50010500x10 损失 函数 为 SoftmaxWithLoss 细节 
权重 的 初始化 方式 为 xavier 偏移 值 的 初始化 
方式 为 constant 默认 设为 0 权重 的 学习率 为 
base _ learning _ rate 偏移 值 的 学习率 为 
base _ learning _ rate 的 两倍 AlexNet ImageNet Classification 
with Deep Convolutional Neural Networks 结构 N a m e 
k e r n e l s t r i 
d e p a d I n p u t 
O u t p u t P a r a 
m e t e r NumberConv1 + ReLU96 11 × 
1140227 × 227 × 355 × 55 × 96 11 
× 11 × 3 + 1 × 96Max P o 
o l i n g 3 x 3 2 0 
5 5 x 5 5 x 9 6 2 7 
x 2 7 x 9 6 0 C o n 
v 2 + ReLU256 5 x 5 1 2 2 
7 x 2 7 x 9 6 2 7 x 
2 7 x 2 5 6 5x5x96 + 1 x256Max 
P o o l i n g 3 x 3 
2 0 2 7 x 2 7 x 2 5 
6 1 3 x 1 3 x 2 5 6 
0 C o n v 3 + ReLU384 3 x 
3 1 1 1 3 x 1 3 x 2 
5 6 1 3 x 1 3 x 3 8 
4 3x3x256 + 1 x384Conv4 + ReLU384 3 x 3 
1 1 1 3 x 1 3 x 3 8 
4 1 3 x 1 3 x 3 8 4 
3x3x384 + 1 x384Conv5 + ReLU256 3 x 3 1 
1 1 3 x 1 3 x 3 8 4 
1 3 x 1 3 x 2 5 6 3x3x384 
+ 1 x256Max P o o l i n g 
3 x 3 2 0 1 3 x 1 3 
x 2 5 6 6 x 6 x 2 5 
6 0 F C 6 + ReLU + Dropout 6 
x 6 x 2 5 6 4 0 9 6 
6 x 6 x 2 5 6 x 4 0 
9 6 F C 7 + ReLU + Dropout 4 
0 9 6 4 0 9 6 4 0 9 
6 x 4 0 9 6 F C 8 + 
Softmax 4 0 9 6 1 0 0 0 4 
0 9 6 x 1 0 0 0 浮点 乘法 
运算量 \ 55 * 55 * 96 * 11 * 
11 * 3 + 27 * 27 * 256 * 
5 * 5 * 96 + 13 * 13 * 
384 * 3 * 3 * 256 + 13 * 
13 * 384 * 3 * 3 * 384 + 
13 * 13 * 256 * 3 * 3 * 
384 + 6 * 6 * 256 * 4096 + 
4096 * 4096 + 4096 * 1000 \ 参数 60 
Million MACs 1.1 Billion 实际 计算 量 比 这个 值 
小 因为 Conv 层 中 使用 了 group 第一 次 
引入 ReLU 并 使用 了 overlapping Max Pooling 在前 两个 
全 连接 层 使用 了 系数 为 0.5 的 Dropout 
因此 测试 时 需要 将 结果 乘以 0.5 在 论文 
中 还 引入 了 局部 响应 归一化 层 LRN 但 
后来 证明 没有 效果 提升 同时 overlapping Max Pooling 也 
没有 被 广泛 应用 训练 细节 batch size 128 momentum 
0.9 weight decay 5e 4 学习率 初始 为 0 每当 
error 停止 下降 就 除以 10 数据 增强 对于 训练 
集 随机 剪裁 对于 测试 集 将/d 原始/v 图片/n 和/c 
对应/vn 的/uj 水平/n 镜像/n 从/p 中间/f 和/c 四边/f 剪切/vn 然后 
将 这 十个 预测 结果 取 平均 PCA jittering 基于 
整个 训练 集 特征值 λ 和 特征向量 P 对于 每个 
epoch 的 每一张 图像 从 均值 0 方差 0.1 的 
高斯分布 随机 抽取 α 对 原始 图片 三维 通道 做 
λ * α * P 映射 ZFNet Visualizing and Understanding 
Convolutional Networks 引入 DeconvNet 来 可视化 某一 feature map 将该 
层 其余 feature maps 设置 为 0 然后 经过 一 
系列 的 i unpool ii rectify iii filters 映 射回 
像素 级别 其中 i unpool max pooling 同时 记录 位置 
信息 ii ReLU iii 原 卷积 对应 矩阵 的 转置 
论文 中 使用 可视化 方法 对于 某 一层 的 某个 
feature map 我们 在 验证 集中 寻找 使该 feature map 
的 response 最大 的 九张 图片 画出 这 九张 图片 
中的 该 feature map 反 卷积 映射 的 结果 并和 
原图 相 对应 的 patch 对比 特征 可视化 层数 越高 
提取 的 特征 越 复杂 不变性 越 明显 越 discrimination 
训练 过程 中 特征 收敛 过程 层数 越低 收敛 越早 
特征 不变性 1 图像 缩放 平移 对模型 第一层 影响 较大 
对 后面 基本 没有 影响 2 图像 旋转 后 特征 
不具有 不变性 通过 第一层 和 第二层 可视化 对 AlexNet 进行 
改造 得到 ZFNet 减小 感受 野 降低 步长 模型 对 
局部 特征 敏感 模型 具有特征 泛 化性 因此 可以 用于 
迁移 学习 Network in Network 引入 1x1 卷积 Original Conv 
3x3 ReLU PoolingMLPConv Conv 3x3 ReLU Conv 1x1 ReLU . 
. . Pooling 使用 average pooling 取代 FC 结构 MLPConv 
堆叠 + global average poolingVGG Very Deep Convolutional Networks for 
Large Scale Image Recognition 使用 了 统一 的 卷积 结构 
证明 了 深度 对模型 效果 的 影响 LRN 层 没有 
提升 效果 堆叠 多个 3x3 的 感受 野 可以获得 类似于 
更大 感受 野 的 效果 同时 多层 3x3 卷积 堆叠 
对应 的 参数 更少 减少 参数 相当于 正则化 效果 运用 
了 Network in Network 中 提出 的 1x1 卷积 训练 
方式 256 batch size 0.9 momentum 5e 4 weight decay 
0.5 dropout ratio learning rate 初始 1e 2 每当 停止 
提升 后 下降 为 之前 的 十分之一 数据 增强 颜色 
增强 color jittering PCA jittering 尺度 变换 训练 集 随机 
缩 放到 256 512 然后 随机 剪 切到 224x224 尺度 
变换 对应 的 测试 方法 1 随机 裁剪 取 平均 
类似 AlexNet 2 将 FC 转为 Conv 原始 图片 直接 
输入 模型 这时 输出 不再 是 1x1x1000 而是 NxNx1000 然后 
取 平均 GoogLeNet Going deeper with convolutions 论文 发表 之前 
相关 的 工作 当时 研究 者 关注 增加 层数 和 
filter 数目 可能 会 导致 小 样本 过拟合 并 需要 
大量 的 计算资源 并 通过 Dropout 防止 过拟合 尽管 有人 
认为 Max Pooling 造成 了 空间 信息 的 损失 但 
这种 结构 在 localization detection human pose estimation 中 均 
取得 很好 的 成绩 Network In Network 中 引入 1x1 
卷积 增加 了 神经 网络 的 表示 能力 representational power 
GoogLeNet 中的 Inception 结构 也 运用 了 1x1 卷积 来 
进行 降 维 处理 为/p 解决/v 过拟合/i 和/c 计算/v 代/q 
价高/n 的/uj 问题/n 使用 稀疏 网络 来 代替 全 连接 
网络 在 实际 中 即使 用 卷积 层 Inception 结构 
对于 输入 的 feature maps 分别 通过 1x1 卷积 3x3 
卷积 5x5 卷积 和 Max Pooling 层 并将 输出 的 
feature maps 连接起来 作为 Inception 的 输出 同时 获得 不同 
感受 野 的 信息 在 3x3 卷积 5x5/i 卷积/n 前面/f 
和/c 池化层/nr 后面/f 接/v 1x1/i 卷积/n 起降 维 的 作用 
Inception v2 v3 Rethinking the Inception Architecture for Computer VisionCNN 
的 通用 设计 思想 通常 随着 网络 层数 增加 空间 
特征 数 逐渐 降低 通 道数 逐渐增加 不要 过度 压缩 
损失 精度 信息 避免 表征 瓶颈 增加 非线性 特征 可以 
解 耦合 特征 卷积 的 实现 形式 为 空间 聚合 
spatial aggregation 1x1 卷积 降 维 不会 产生 严重 的 
影响 猜测 相邻 通道 之间 具有 强 相关性 卷积 前 
降 维 基本 不会 损失 信息 5x5 卷积 的 感受 
野 与 两个 3x3 卷积 堆叠 所 对应 的 感受 
野 相同 使用 后者 可以 大大 减少 网络 参数 7x7 
同理 此外 两个/m 3x3/i 卷积/n 后/f 各连/i 接/v 一个/m 非线性/b 
层/q 的/uj 效果/n 优于/v 仅在/i 最后/f 连接/nr 一个/m 非线性/b 层/q 
NxN/w 的/uj 卷积/n 可以用/i 1xN/i 与/p Nx1/i 的/uj 卷积/n 堆叠/v 
实现/v 使用 Label Smoothing 增加 网络 的 正则 能力 使用 
Batch Normalization 和 RMSPropResNet Deep Residual Learning for Image Recognition 
网络 过深 会 导致 退化 问题 通过 短路 连接 解决 
该 问题 通常 一个 residual unit 的 残差 部分 使用 
二 至 三层 的 函数 映射 或称 卷积 层 shortcut 
部分 与 残差 部分 进行 eltwise add 后再 连接 非线性 
层 补充 论文 Inception v4 Inception ResNet and the Impact 
of Residual Connections on Learning 相比 v3 Inception v4 的 
主要 变化 是 网络 的 加深 卷积 和的/nr 个数 也 
大大 增加 Inception ResNet 即将 ResNet 的 残差 结构 替换成 
了 简单 的 Inception 结构 文中 认为 ResNet 对 提高 
精度 的 帮助 较小 ResNet 论文 中 提到 ResNet 解决 
了 退化 问题 加快 过深 网络 的 训练 速度 是 
其 主要 优势 对于 特别 深 的 残差 网络 可以 
通过 在 残 层 结构 后接 一个 scaling 如 残 
层 结构 输出 的 一个 元素 乘以 0.2 来 提高 
模型 稳定性 Wide Residual Networks 很多 论文 在 ResNet 的 
网络 结构 基础 上 进行 了 细微 的 改动 主要 
的 观点 是 ResNet 存在 diminishing feature reuse 的 问题 
网络 过深 很多 残差 块 对 最终 结果 只 做出 
了 很少 的 贡献 甚至 有些 残差 块 没有 学到 
有用 的 信息 反而 在 之前 学到 的 feature representation 
中 加入 了 轻微 噪声 ResNet 提高 一点 精 度 
可能 需要 将 深度 增加一倍 而且 会 产生 diminishing feature 
reuse 问题 因此 提出 增加 残差 块 的 宽度 减少 
网络 深度 的 WRNs 文中 提到 论文 Deep Networks with 
Stochastic Depth 中 通过 使 残差 块 随机 失 活来 
降低 每次 训练 使 网络 的 深度 作者 做 了 
大量 的 实验 表明 两个 3x3 卷积 堆叠 的 残差 
块 的 效果 优于 其他 残差 块 结构 同时 增加 
深度 和 宽度 可以 提高 精度 但 需要 正则 增加 
宽度 比 增加 深度 更容易 训练 在 瘦长 和 矮胖 
的 网络 中 在 残差 块 中 的 两个 卷积 
间 增加 Dropout 层 均 有效果 在 不做 大量 的 
数据 增强 的 前提 下 Dropout 的 效果 比 Batch 
Normalization 更好 Aggregated Residual Transformations for Deep Neural Networks 指出 
Inception 过于 复杂 不易 迁移 到 其他 问题 上 ResNet 
存在 diminishing feature reuse 的 问题 提出 了 基数 的 
概念 残差 块 采用 split transform merge 的 策略 基数 
类似 group 表示 split 的 数目 这种 架构 可以 接近 
large and dense layers 的 表示 能力 但 只 需要 
很少 的 计算资源 ResNeXt 有/v 一种/m 基本/n 形式/n 和/c 两种/m 
变体/n 一种 类似 Inception ResNet 一种 使用 group 实现 Densely 
Connected Convolutional NetworksDenseNet 极大 地 增加 了 特征 重用 的 
能力 其 有 以下 优点 1 . 参数 少 通过 
向后 连接 的 方式 保留 学到 的 信息 2 . 
改进 了 前 向 反向 传播 更易 训练 3 . 
增加 了 监督 学习 的 能力 4 . 在 小 
数据 上 不易 过拟合 即 增加 了 正则化 的 能力 
Dense Block 中 对于 任意 一层 的 feature maps 一 
方面 会 通过 BN ReLU Conv 1x1 BN ReLU Conv 
3x3 得到 下 一层 的 feature maps 另一方面 会与 其后 
的 每 一层 feature maps 连接 在 一起 并 提出 
了 growth rate 的 概念 增长率 k 是 3x3 卷积 
层 输出 的 feature maps 数 而 1x1 卷积 层 
输出 的 feature maps 数 为 4k 在 ImageNet 比赛 
中 运用 的 模型 每 两个 Dense Block 中间 以 
Batch Normalization ReLU Conv 1x1 Aver Pooling 2x2 相连 Squeeze 
and Excitation N e t w o r k s 
q u e e z e N e t ShuffleNet 
MobileNet X c e p t i o n q 
u e e z e N e t 的 基本 
结构 是 File Module 输入 的 feature maps 先 经过 
1x1 卷积 降 维 然后 分别 通过 1x1 卷积 和 
3x3 卷积 并将 两个 输出 连接起来 作为 这个 模块 整体 
的 输出 SqueezeNet 的 结构 就 是 多个 File Module 
堆叠 而成 中间 夹杂着 max pooling 最后 用 deep compression 
压缩 MobileNet 的 基本 结构 是 3x3 depth wise Conv 
加 1x1 Conv 1x1 卷积 使得 输出 的 每一个 feature 
map 要 包含 输入 层 所有 feature maps 的 信息 
这种 结构 减少 了 网络 参数 的 同时 还 降低 
了 计算 量 整个 MobileNet 就是 这种 基本 结构 堆叠 
而成 其中 没有 池化层/nr 而是 将 部分 的 depth wise 
Conv 的 stride 设置 为 2 来 减小 feature map 
的 大小 ShuffleNet 认为 depth wise 会 带来 信息 流通 
不畅 的 问题 利用 group convolution 和 channel shuffle 这 
两个 操作 来 设计 卷积 神经网络 模型 以 减少 模型 
使用 的 参数 数量 同时 使用 了 ResNet 中的 短路 
连接 ShuffleNet 通过 多个 Shuffle Residual Blocks 堆叠 而成 Xception 
相对于 借鉴 了 depth wise 的 思想 简化 了 Inception 
v3 Xception 的 结构 是 输入 的 feature maps 先 
经过 一个 1x1 卷积 然后 将 输出 的 每一个 feature 
map 后面 连接 一个 3x3 的 卷积 再 逐 通道 
卷积 然后 将 这些 3x3 卷积 的 输出 连接起来 补充 
权重 初始化 方式 Xavier Understanding the difficulty of training deep 
feedforward neural networksMSRA Delving Deep into Rectifiers Surpassing Human Level 
Performance on ImageNet Classification 补充 激活 函数 补充 Dropout 层 
补充 Batch Normalization 层 LeNet 原始 论 文中 的 版本 数据 集为 MNIST 输入 
\ 32 * 32 * 1 \ N a m 
e k e r n e l s t r 
i d e p a d I n p u 
t O u t p u t P a r 
a m e t e r NumberConv16 5 × 51032 
× 32 × 128 × 28 × 6 5 × 
5 × 1 + 1 × 6subsampling + sigmoid2 × 
22028 × 28 × 614 × 14 × 6 1 
+ 1 × 6Conv216 5 × 51014 × 14 × 
610 × 10 × 16 5 × 5 × 3 
+ 1 × 6 + 5 × 5 × 4 
+ 1 × 6 + 5 × 5 × 4 
+ 1 × 3 + 5 × 5 × 6 
+ 1 × 1subsampling + sigmoid2 × 22010 × 10 
× 165 × 5 × 16 1 + 1 × 
16Conv3120 5 × 5105 × 5 × 161 × 1 
× 120 5 × 5 × 16 + 1 × 
120FC4 + tanh 1 x 1 x 1 2 0 
8 4 1 x 1 x 1 2 0 x 
8 4 R B F 84100 下 采样 的 方式 
为 \ sigmoid w * \ sum _ { i 
= 0 } ^ { 1 } \ sum _ 
{ j = 0 } ^ { 1 } x 
_ { ij } + b \ 这里 \ w 
\ 和 \ b \ 是 可 学习 参数 conv2 
使用 了 包括 3层 4层 6层 三种 通 道数 不同 
的 filters 然后 将 它们 的 输出 拼接 在 一起 
作为 这 一层 的 输出 这里 使用 多组 不同 形式 
的 卷积 的 原因 不 完全 机制 限制 了 连接 
的 数量 减少 计算 量 破坏 了 网络 的 对称性 
最后 一层 是 RBF 虽是 全 连接 但 参数 W 
是 给 定值 输入 的 \ 84 \ 维 向量 
相当于 \ 12 * 7 \ 的 比 特图 输出 
的 每一个 值 代表 了 输入 与 输出 对应 的 
参数 权重 的 均方 误差 MSE 要求 损失 函数 可以 
使 正确 的 label 对应 的 输出 值 越小 越好 
Caffe 中的 实现 输入 的 batch size \ 64 \ 
归一化 scale \ 0.00390625 \ N a m e k 
e r n e l s t r i d 
e p a d I n p u t O 
u t p u t P a r a m 
e t e r NumberConv1 + ReLU20 5 × 51028 
× 28 × 124 × 24 × 6 5 × 
5 × 1 + 1 × 20Max Pooling2 × 22024 
× 24 × 2012 × 12 × 200Conv2 + ReLU50 
5 × 51012 × 12 × 208x8x50 5 × 5 
× 20 + 1 × 50Max Pooling2 × 2 2 
0 8 x 8 x 5 0 4 x 4 
x 5 0 0 F C 3 + ReLU 4x4x505004x4x50 
× 500FC4 + Softmax 50010500x10 损失 函数 为 SoftmaxWithLoss 细节 
权重 的 初始化 方式 为 xavier 偏移 值 的 初始化 
方式 为 constant 默认 设为 0 权重 的 学习率 为 
base _ learning _ rate 偏移 值 的 学习率 为 
base _ learning _ rate 的 两倍 AlexNet ImageNet Classification 
with Deep Convolutional Neural Networks 结构 N a m e 
k e r n e l s t r i 
d e p a d I n p u t 
O u t p u t P a r a 
m e t e r NumberConv1 + ReLU96 11 × 
1140227 × 227 × 355 × 55 × 96 11 
× 11 × 3 + 1 × 96Max P o 
o l i n g 3 x 3 2 0 
5 5 x 5 5 x 9 6 2 7 
x 2 7 x 9 6 0 C o n 
v 2 + ReLU256 5 x 5 1 2 2 
7 x 2 7 x 9 6 2 7 x 
2 7 x 2 5 6 5x5x96 + 1 x256Max 
P o o l i n g 3 x 3 
2 0 2 7 x 2 7 x 2 5 
6 1 3 x 1 3 x 2 5 6 
0 C o n v 3 + ReLU384 3 x 
3 1 1 1 3 x 1 3 x 2 
5 6 1 3 x 1 3 x 3 8 
4 3x3x256 + 1 x384Conv4 + ReLU384 3 x 3 
1 1 1 3 x 1 3 x 3 8 
4 1 3 x 1 3 x 3 8 4 
3x3x384 + 1 x384Conv5 + ReLU256 3 x 3 1 
1 1 3 x 1 3 x 3 8 4 
1 3 x 1 3 x 2 5 6 3x3x384 
+ 1 x256Max P o o l i n g 
3 x 3 2 0 1 3 x 1 3 
x 2 5 6 6 x 6 x 2 5 
6 0 F C 6 + ReLU + Dropout 6 
x 6 x 2 5 6 4 0 9 6 
6 x 6 x 2 5 6 x 4 0 
9 6 F C 7 + ReLU + Dropout 4 
0 9 6 4 0 9 6 4 0 9 
6 x 4 0 9 6 F C 8 + 
Softmax 4 0 9 6 1 0 0 0 4 
0 9 6 x 1 0 0 0 浮点 乘法 
运算量 \ 55 * 55 * 96 * 11 * 
11 * 3 + 27 * 27 * 256 * 
5 * 5 * 96 + 13 * 13 * 
384 * 3 * 3 * 256 + 13 * 
13 * 384 * 3 * 3 * 384 + 
13 * 13 * 256 * 3 * 3 * 
384 + 6 * 6 * 256 * 4096 + 
4096 * 4096 + 4096 * 1000 \ 参数 60 
Million MACs 1.1 Billion 实际 计算 量 比 这个 值 
小 因为 Conv 层 中 使用 了 group 第一 次 
引入 ReLU 并 使用 了 overlapping Max Pooling 在前 两个 
全 连接 层 使用 了 系数 为 0.5 的 Dropout 
因此 测试 时 需要 将 结果 乘以 0.5 在 论文 
中 还 引入 了 局部 响应 归一化 层 LRN 但 
后来 证明 没有 效果 提升 同时 overlapping Max Pooling 也 
没有 被 广泛 应用 训练 细节 batch size 128 momentum 
0.9 weight decay 5e 4 学习率 初始 为 0 每当 
error 停止 下降 就 除以 10 数据 增强 对于 训练 
集 随机 剪裁 对于 测试 集 将/d 原始/v 图片/n 和/c 
对应/vn 的/uj 水平/n 镜像/n 从/p 中间/f 和/c 四边/f 剪切/vn 然后 
将 这 十个 预测 结果 取 平均 PCA jittering 基于 
整个 训练 集 特征值 λ 和 特征向量 P 对于 每个 
epoch 的 每一张 图像 从 均值 0 方差 0.1 的 
高斯分布 随机 抽取 α 对 原始 图片 三维 通道 做 
λ * α * P 映射 ZFNet Visualizing and Understanding 
Convolutional Networks 引入 DeconvNet 来 可视化 某一 feature map 将该 
层 其余 feature maps 设置 为 0 然后 经过 一 
系列 的 i unpool ii rectify iii filters 映 射回 
像素 级别 其中 i unpool max pooling 同时 记录 位置 
信息 ii ReLU iii 原 卷积 对应 矩阵 的 转置 
论文 中 使用 可视化 方法 对于 某 一层 的 某个 
feature map 我们 在 验证 集中 寻找 使该 feature map 
的 response 最大 的 九张 图片 画出 这 九张 图片 
中的 该 feature map 反 卷积 映射 的 结果 并和 
原图 相 对应 的 patch 对比 特征 可视化 层数 越高 
提取 的 特征 越 复杂 不变性 越 明显 越 discrimination 
训练 过程 中 特征 收敛 过程 层数 越低 收敛 越早 
特征 不变性 1 图像 缩放 平移 对模型 第一层 影响 较大 
对 后面 基本 没有 影响 2 图像 旋转 后 特征 
不具有 不变性 通过 第一层 和 第二层 可视化 对 AlexNet 进行 
改造 得到 ZFNet 减小 感受 野 降低 步长 模型 对 
局部 特征 敏感 模型 具有特征 泛 化性 因此 可以 用于 
迁移 学习 Network in Network 引入 1x1 卷积 Original Conv 
3x3 ReLU PoolingMLPConv Conv 3x3 ReLU Conv 1x1 ReLU . 
. . Pooling 使用 average pooling 取代 FC 结构 MLPConv 
堆叠 + global average poolingVGG Very Deep Convolutional Networks for 
Large Scale Image Recognition 使用 了 统一 的 卷积 结构 
证明 了 深度 对模型 效果 的 影响 LRN 层 没有 
提升 效果 堆叠 多个 3x3 的 感受 野 可以获得 类似于 
更大 感受 野 的 效果 同时 多层 3x3 卷积 堆叠 
对应 的 参数 更少 减少 参数 相当于 正则化 效果 运用 
了 Network in Network 中 提出 的 1x1 卷积 训练 
方式 256 batch size 0.9 momentum 5e 4 weight decay 
0.5 dropout ratio learning rate 初始 1e 2 每当 停止 
提升 后 下降 为 之前 的 十分之一 数据 增强 颜色 
增强 color jittering PCA jittering 尺度 变换 训练 集 随机 
缩 放到 256 512 然后 随机 剪 切到 224x224 尺度 
变换 对应 的 测试 方法 1 随机 裁剪 取 平均 
类似 AlexNet 2 将 FC 转为 Conv 原始 图片 直接 
输入 模型 这时 输出 不再 是 1x1x1000 而是 NxNx1000 然后 
取 平均 GoogLeNet Going deeper with convolutions 论文 发表 之前 
相关 的 工作 当时 研究 者 关注 增加 层数 和 
filter 数目 可能 会 导致 小 样本 过拟合 并 需要 
大量 的 计算资源 并 通过 Dropout 防止 过拟合 尽管 有人 
认为 Max Pooling 造成 了 空间 信息 的 损失 但 
这种 结构 在 localization detection human pose estimation 中 均 
取得 很好 的 成绩 Network In Network 中 引入 1x1 
卷积 增加 了 神经 网络 的 表示 能力 representational power 
GoogLeNet 中的 Inception 结构 也 运用 了 1x1 卷积 来 
进行 降 维 处理 为/p 解决/v 过拟合/i 和/c 计算/v 代/q 
价高/n 的/uj 问题/n 使用 稀疏 网络 来 代替 全 连接 
网络 在 实际 中 即使 用 卷积 层 Inception 结构 
对于 输入 的 feature maps 分别 通过 1x1 卷积 3x3 
卷积 5x5 卷积 和 Max Pooling 层 并将 输出 的 
feature maps 连接起来 作为 Inception 的 输出 同时 获得 不同 
感受 野 的 信息 在 3x3 卷积 5x5/i 卷积/n 前面/f 
和/c 池化层/nr 后面/f 接/v 1x1/i 卷积/n 起降 维 的 作用 
Inception v2 v3 Rethinking the Inception Architecture for Computer VisionCNN 
的 通用 设计 思想 通常 随着 网络 层数 增加 空间 
特征 数 逐渐 降低 通 道数 逐渐增加 不要 过度 压缩 
损失 精度 信息 避免 表征 瓶颈 增加 非线性 特征 可以 
解 耦合 特征 卷积 的 实现 形式 为 空间 聚合 
spatial aggregation 1x1 卷积 降 维 不会 产生 严重 的 
影响 猜测 相邻 通道 之间 具有 强 相关性 卷积 前 
降 维 基本 不会 损失 信息 5x5 卷积 的 感受 
野 与 两个 3x3 卷积 堆叠 所 对应 的 感受 
野 相同 使用 后者 可以 大大 减少 网络 参数 7x7 
同理 此外 两个/m 3x3/i 卷积/n 后/f 各连/i 接/v 一个/m 非线性/b 
层/q 的/uj 效果/n 优于/v 仅在/i 最后/f 连接/nr 一个/m 非线性/b 层/q 
NxN/w 的/uj 卷积/n 可以用/i 1xN/i 与/p Nx1/i 的/uj 卷积/n 堆叠/v 
实现/v 使用 Label Smoothing 增加 网络 的 正则 能力 使用 
Batch Normalization 和 RMSPropResNet Deep Residual Learning for Image Recognition 
网络 过深 会 导致 退化 问题 通过 短路 连接 解决 
该 问题 通常 一个 residual unit 的 残差 部分 使用 
二 至 三层 的 函数 映射 或称 卷积 层 shortcut 
部分 与 残差 部分 进行 eltwise add 后再 连接 非线性 
层 补充 论文 Inception v4 Inception ResNet and the Impact 
of Residual Connections on Learning 相比 v3 Inception v4 的 
主要 变化 是 网络 的 加深 卷积 和的/nr 个数 也 
大大 增加 Inception ResNet 即将 ResNet 的 残差 结构 替换成 
了 简单 的 Inception 结构 文中 认为 ResNet 对 提高 
精度 的 帮助 较小 ResNet 论文 中 提到 ResNet 解决 
了 退化 问题 加快 过深 网络 的 训练 速度 是 
其 主要 优势 对于 特别 深 的 残差 网络 可以 
通过 在 残 层 结构 后接 一个 scaling 如 残 
层 结构 输出 的 一个 元素 乘以 0.2 来 提高 
模型 稳定性 Wide Residual Networks 很多 论文 在 ResNet 的 
网络 结构 基础 上 进行 了 细微 的 改动 主要 
的 观点 是 ResNet 存在 diminishing feature reuse 的 问题 
网络 过深 很多 残差 块 对 最终 结果 只 做出 
了 很少 的 贡献 甚至 有些 残差 块 没有 学到 
有用 的 信息 反而 在 之前 学到 的 feature representation 
中 加入 了 轻微 噪声 ResNet 提高 一点 精 度 
可能 需要 将 深度 增加一倍 而且 会 产生 diminishing feature 
reuse 问题 因此 提出 增加 残差 块 的 宽度 减少 
网络 深度 的 WRNs 文中 提到 论文 Deep Networks with 
Stochastic Depth 中 通过 使 残差 块 随机 失 活来 
降低 每次 训练 使 网络 的 深度 作者 做 了 
大量 的 实验 表明 两个 3x3 卷积 堆叠 的 残差 
块 的 效果 优于 其他 残差 块 结构 同时 增加 
深度 和 宽度 可以 提高 精度 但 需要 正则 增加 
宽度 比 增加 深度 更容易 训练 在 瘦长 和 矮胖 
的 网络 中 在 残差 块 中 的 两个 卷积 
间 增加 Dropout 层 均 有效果 在 不做 大量 的 
数据 增强 的 前提 下 Dropout 的 效果 比 Batch 
Normalization 更好 Aggregated Residual Transformations for Deep Neural Networks 指出 
Inception 过于 复杂 不易 迁移 到 其他 问题 上 ResNet 
存在 diminishing feature reuse 的 问题 提出 了 基数 的 
概念 残差 块 采用 split transform merge 的 策略 基数 
类似 group 表示 split 的 数目 这种 架构 可以 接近 
large and dense layers 的 表示 能力 但 只 需要 
很少 的 计算资源 ResNeXt 有/v 一种/m 基本/n 形式/n 和/c 两种/m 
变体/n 一种 类似 Inception ResNet 一种 使用 group 实现 Densely 
Connected Convolutional NetworksDenseNet 极大 地 增加 了 特征 重用 的 
能力 其 有 以下 优点 1 . 参数 少 通过 
向后 连接 的 方式 保留 学到 的 信息 2 . 
改进 了 前 向 反向 传播 更易 训练 3 . 
增加 了 监督 学习 的 能力 4 . 在 小 
数据 上 不易 过拟合 即 增加 了 正则化 的 能力 
Dense Block 中 对于 任意 一层 的 feature maps 一 
方面 会 通过 BN ReLU Conv 1x1 BN ReLU Conv 
3x3 得到 下 一层 的 feature maps 另一方面 会与 其后 
的 每 一层 feature maps 连接 在 一起 并 提出 
了 growth rate 的 概念 增长率 k 是 3x3 卷积 
层 输出 的 feature maps 数 而 1x1 卷积 层 
输出 的 feature maps 数 为 4k 在 ImageNet 比赛 
中 运用 的 模型 每 两个 Dense Block 中间 以 
Batch Normalization ReLU Conv 1x1 Aver Pooling 2x2 相连 Squeeze 
and Excitation N e t w o r k s 
q u e e z e N e t ShuffleNet 
MobileNet X c e p t i o n q 
u e e z e N e t 的 基本 
结构 是 File Module 输入 的 feature maps 先 经过 
1x1 卷积 降 维 然后 分别 通过 1x1 卷积 和 
3x3 卷积 并将 两个 输出 连接起来 作为 这个 模块 整体 
的 输出 SqueezeNet 的 结构 就 是 多个 File Module 
堆叠 而成 中间 夹杂着 max pooling 最后 用 deep compression 
压缩 MobileNet 的 基本 结构 是 3x3 depth wise Conv 
加 1x1 Conv 1x1 卷积 使得 输出 的 每一个 feature 
map 要 包含 输入 层 所有 feature maps 的 信息 
这种 结构 减少 了 网络 参数 的 同时 还 降低 
了 计算 量 整个 MobileNet 就是 这种 基本 结构 堆叠 
而成 其中 没有 池化层/nr 而是 将 部分 的 depth wise 
Conv 的 stride 设置 为 2 来 减小 feature map 
的 大小 ShuffleNet 认为 depth wise 会 带来 信息 流通 
不畅 的 问题 利用 group convolution 和 channel shuffle 这 
两个 操作 来 设计 卷积 神经网络 模型 以 减少 模型 
使用 的 参数 数量 同时 使用 了 ResNet 中的 短路 
连接 ShuffleNet 通过 多个 Shuffle Residual Blocks 堆叠 而成 Xception 
相对于 借鉴 了 depth wise 的 思想 简化 了 Inception 
v3 Xception 的 结构 是 输入 的 feature maps 先 
经过 一个 1x1 卷积 然后 将 输出 的 每一个 feature 
map 后面 连接 一个 3x3 的 卷积 再 逐 通道 
卷积 然后 将 这些 3x3 卷积 的 输出 连接起来 补充 
权重 初始化 方式 Xavier Understanding the difficulty of training deep 
feedforward neural networksMSRA Delving Deep into Rectifiers Surpassing Human Level 
Performance on ImageNet Classification 补充 激活 函数 补充 Dropout 层 
补充 Batch Normalization 层 