上世纪 60 年代 Marvin Minsky 在 MIT 让 他 的 
本科 学生 Gerald Jay Sussman 用 一个 暑假 的 时间 
完成 一个 有趣 的 Project link a camera to a 
computer and get the computer to describe what it saw 
从 那时 开始 特别 是 David Marr 教授 于 1977年 
正式 提出 视觉 计算 理论 计算机 视觉 已经 走过 了 
四十 多年 的 历史 可是 从 今天 看来 这个 已 
入 不惑之年 的 学科 依然 显得 如此 年轻 而 朝气蓬勃 
在 它 几十年 的 发展 历程 中 多种 流派 的 
方法 都曾 各领风骚 于 一时 最近二 十年 中 计算机 视觉 
发展 最 鲜明 的 特征 就是 机器学习 与 概率模型 的 
广泛 应用 在 这里 我 简单 回顾 一下 对 这个 
领域 产生 了 重要 影响 的 几个 里程碑 ● 1984年 
Stuart Geman 和 Donald Geman 发表 了 一篇 先驱 性 
的 论文 Stochastic Relaxation Gibbs Distributions and the Bayesian Restoration 
of Images . 在 这篇文章 里 两位 Geman 先生 引入 
了 一 系列 对 计算机 视觉 以后 的 发展 具有 
深远 影响 的 概念 和 方法 Markov Random Field MRF 
  Gibbs Sampling 以及 Maximum a Posteriori estimate MAP estimate 
这篇 论文 的 意义 是 超前 于 时代 的 它 
所 建立 的 这 一系列 方法 直到 90 年代 中后期 
才 开始 被 广泛 关注 ● 1991年 Matthew Turk 和 
Alex Pentland 使用 Eigenface 进行 人脸 分类 从此 以 矩阵 
的 代数 分解 为 基础 的 方法 在 视觉 分析 
中 被 大量 运用 其中 有 代表性 的 方法 包括 
PCA LDA 以及 ICA ● 1995年 Corinna Cortes 和 Vladimir 
Vapnik 提出 带有 soft margin 的 Support Vector Machine SVM 
以及 它 的 Kernel 版本 并用 它 对 手写 数字 
进行 分类 从此 SVM 大受欢迎 并 成为 各种 应用 中 
的 基准 分类器 ● 1996年 Bruno Olshausen 和 David Field 
提出 使用 Overcomplete basis 对 图像 进行 稀疏 编码 Sparse 
coding 这个 方向 在 初期 的 反响 并不 热烈 直到 
近些年 Compressed Sensing 在 信号 处理 领域 成为 炙手可热 的 
方向 Sparse coding 在 这一 热潮 的 带动 下 成为 
视觉 领域 一个 活跃 的 研究 方向 ● 90 年代 
末 Graphical Model 和 Variational Inference 逐步 发展 成熟 1998年 
MIT 出版社 出版 了 由 Michale Jordan 主编 的 文集 
Learning in Graphical Models 这部 书 总结 了 那 一时期 
关于 Graphical Model 的 建模 分析 和 推断 的 主要 
成果 这些 成果 为 Graphical Model 在 人工智能 的 各个 
领域 的 应用 提供 了 方法论 基础 进入 21 世纪 
Graphical Model 和 Bayesian 方法 在 视觉 研究 中 的 
运用 出现 了 井喷式 的 增长 ● 2001年 John Lafferty 
和 Andrew McCallum 等 提出 Conditional Random Field CRF CRF 
为 结构化 的 分类 和 预测 提供 了 一种 通用 
的 工具 此后 语义 结构 开始 被 运用 于 视觉 
场景 分析 ● 2003年 David Blei 等 提出 Latent Dirichlet 
Allocation 2004年 Yee Whye Teh 等 提出 Hierarchical Dirichlet Process 
各种 参数 化 或者 非 参数 化 的 Topic Model 
在此后 不久 被 广泛 用于 语义 层面 的 场景 分析 
● 虽然 Yahn Lecun 等 人在 1993年 已 提出 Convolutional 
Neural Network 但在 vision 中的 应用 效果 一直 欠佳 时至 
2006年 Geoffrey Hinton 等人 提出 Deep Belief Network 进行 layer 
wise 的 pretraining 应用 效果 取得 突破性 进展 其 与 
之后 Ruslan Salakhutdinov 提出 的 Deep Boltzmann Machine 重新 点燃 
了 视觉 领域 对于 Neural Network 和 Boltzmann Machine 的 
热情 时间 进入 2013年 Probabilistic Graphical Model 早已 成为 视觉 
领域 中 一种 基本 的 建模 工具 Probabilistic Graphical Model 
的 研究 涉及 非常多 的 方面 限于 篇幅 在 本文 
中 我 只能 简要 介绍 其 中 几个 重要 的 
方面 希望 能为/nr 大家 提供 一些 有用 的 参考 Graphical 
Model 的 基本 类型 基本 的 Graphical Model 可以 大致 
分为 两个 类别 贝叶斯 网络 Bayesian Network 和 马尔可夫 随 
机场 Markov Random Field 它们 的 主要 区别 在于 采用 
不同 类型 的 图 来 表达 变量 之间 的 关系 
贝叶斯 网络 采用 有向 无 环 图 Directed Acyclic Graph 
来 表达 因果关系 马尔可夫 随 机场 则 采用 无向图 Undirected 
Graph 来 表达 变量 间 的 相互作用 这种 结构上 的 
区别 导致 了 它们 在 建模 和 推断 方面 的 
一系列 微妙 的 差异 一般来说 贝叶斯/nr 网络/n 中/f 每一个/i 节点/n 
都/d 对应/vn 于/p 一个/m 先验/n 概率分布/n 或者/c 条件/n 概率分布/n 因此 
整体 的 联合 分布 可以 直接 分解 为 所有 单个 
节点 所 对应 的 分布 的 乘积 而 对于 马尔可夫 
场 由于 变量 之间 没有 明确 的 因果关系 它 的 
联合 概率分布 通常 会 表达 为 一系列 势函数 potential function 
的 乘积 通常 情况下 这些 乘积 的 积分 并不等于 1 
因此 还要 对 其 进行 归一化 才能 形成 一个 有效 
的 概率分布 这 一点 往往 在 实际 应用 中 给 
参数估计 造成 非常 大 的 困难 值得一提的是 贝叶斯/nr 网络/n 和/c 
马尔可夫/nr 随/v 机场/n 的/uj 分类/n 主要/b 是/v 为了/p 研究/vn 和/c 
学习/v 的/uj 便利/a 在 实际 应用 中 所 使用 的 
模型 在 很多 时候 是 它们 的 某种 形式 的 
结合 比如 一个 马尔可夫 随 机场 可以 作为 整体 成为 
一个 更大 的 贝叶斯 网络 的 节点 又 或者 多个 
贝叶斯 网络 可以 通过 马尔可夫 随 机场 联系起来 这种 混合型 
的 模型 提供 了 更 丰富 的 表达 结构 同时 
也 会给 模型 的 推断 和 估计 带来 新的 挑战 
Graphical Model 的 新 发展 方向 在 传统 的 Graphical 
Model 的 应用 中 模型 的 设计 者 需要 在 
设计 阶段 就 固定 整个 模型 的 结构 比如 它 
要 使用 哪些 节点 它们 相互之间 如何 关联 等等 但是 
在 实际 问题 中 选择 合适 的 模型 结构 往往 
是 非常 困难 的 因为 我们 在 很多 时候 其实 
并不 清楚 数据 的 实际 结构 为了 解决 这个 问题 
人们 开始 探索 一种 新 的 建立 概率模型 的 方式 
结构 学习 在 这种 方法 中 模型 的 结构 在 
设计 的 阶段 并 不完全 固定 设计 者 通常 只 
需要 设定 模型 结构 所 需要 遵循 的 约束 然后 
再从 模型 学习 的 过程 中 同时 推断出 模型 的 
实际 结构 结构 学习 直到 今天 仍然 是 机器 学习 
中 一个 极 具 挑战性 的 方向 结构 学习 并 
没有 固定 的 形式 不同 的 研究 者 往往 会 
采取 不同 的 途径 比如 结构 学习 中 一个 非常 
重要 的 问题 就是 如何 去 发现 变量 之间 的 
内部 关联 对于 这个 问题 人们 提出 了 多种 截然不同 
的 方法 比如 你 可以 先 建立 一个 完全 图 
连接 所有 的 变量 然后 选择 一个 子图 来 描述 
它们 的 实际 结构 又 或者 你 可以 引入 潜在 
节点 latent node 来 建立 变量 之间 的 关联 Probabilistic 
Graphical Model 的 另外 一个 重要 的 发展 方向 是非 
参数 化 与 传统 的 参数 化 方法 不同 非 
参数 化 方法 是 一种 更为 灵活 的 建模 方式 
非 参数 化 模型 的 大小 比如 节点 的 数量 
可以 随着 数据 的 变化 而 变化 一个 典型 的 
非 参数 化 模型 就 是 基于 狄利克 莱 过程 
Dirichlet Process 的 混合模型 这种 模型 引入 狄利克 莱 过程 
作为 部件 component 参数 的 先验 分布 从而 允许 混合体 
中 可以 有 任意 多个 部件 这 从 根本 上 
克服 了 传统 的 有限 混合模型 中 的 一个 难题 
就是 确定 部件 的 数量 在 近几年 的 文章 中 
非 参数 化 模型 开始 被 用于 特征 学习 在这方面 
比较 有 代表性 的 工作 就 是 基于 Hierarchical Beta 
Process 来 学习 不定 数量 的 特征 基于 Graphical Model 
的 统计 推断 Inference 完成 模型 的 设计 之后 下 
一步 就是 通过 一定 的 算法 从 数据 中去 估计 
模型 的 参数 或 推断 我们 感兴趣 的 其它 未知 
变量 的 值 在 贝叶斯 方法 中 模型 的 参数 
也 通常 被 视为 变量 它们 和 普通 的 变量 
并 没有 根本 的 区别 因此 参数估计 也 可以 被 
视为 是 统计 推断 的 一种 特例 除了 最 简单 
的 一些 模型 统计 推断 在 计算 上 是 非常 
困难 的 一般而言 确切 推断 exact inference 的 复杂度 取决于 
模型 的 tree width 对于 很多 实际 模型 这个 复杂 
度 可能 随着 问题 规模 增长 而 指数 增长 于是 
人们 退而求其次 转而 探索 具有 多项式 复杂度 的 近似 推断 
approximate inference 方法 主流 的 近似 推断 方法 有三种 1 
基于 平均 场 逼近 mean field approximation 的 variational inference 
这种方法 通常用于 由 Exponential family distribution 所 组成 的 贝叶斯 
网络 其 基本 思想 就是 引入 一个 computationally tractable 的 
upper bound 逼近 原 模型 的 log partition function 从而 
有效 地 降低 了 优化 的 复杂度 大家 所 熟悉 
的 EM 算法 就 属于 这 类型 算法 的 一种 
特例 2 Belief propagation 这种方法 最初 由 Judea Pearl 提出 
用于 树状 结构 的 统计 推断 后来 人们 直接 把 
这种 算法 用于 带环 的 模型 忽略 掉 它 本来 
对 树状 结构 的 要求 在 很多 情况 下 仍然 
取得 不错 的 实际 效果 这 就是 loop belief propagation 
在 进一步 的 探索 的 过程 中 人们 发现 了 
它 与 Bethe approximation 的 关系 并 由此 逐步 建立 
起 了 对 loopy belief propagation 的 理论 解释 以及 
刻画出 它 在 各种 设 定下 的 收敛 条件 值得一提的是 
由于 Judea Pearl 对 人工 智能 和 因果关系 推断 方法 
上 的 根本性 贡献 他 在 2011年 获得 了 计算机 
科学 领域 的 最高 奖 图灵奖 基于 message passing 的 
方法 在 最近 十年 有 很多 新的 发展 Martin Wainwright 
在 2003年 提出 Tree reweighted message passing 这种方法 采用 mixture 
of trees 来 逼近 任意 的 graphical model 并 利用 
mixture coefficient 和 edge probability 之间 的 对偶 关系 建立 
了 一种 新的 message passing 的 方法 这种 方法 是 
对 belief propagation 的 推广 Jason Johnson 等 人在 2005年 
建立 的 walk sum analysis 为 高斯 马尔可夫 随机 场上 
的 belief propagation 提供 了 系统 的 分析 方法 这种 
方法 成功 刻画 了 belief propagation 在 高斯 场上 的 
收敛 条件 也是 后来 提出 的 多种 改进型 的 belief 
propagation 的 理论 依据 Thomas Minka 在 他 PhD 期间 
所 建立 的 expectation propagation 也是 belief propagation 的 在 
一般 Graphical Model 上 的 重要 推广 3 蒙特卡罗 采样 
Monte Carlo sampling 与 基于 优化 的 方法 不同 蒙特卡罗 
方法 通过 对 概率模型 的 随机 模拟 运行 来 收集 
样本 然后 通过 收集 到 的 样本 来 估计 变量 
的 统计 特性 比如 均值 采样 方法 有三个/nr 方面 的 
重要 优点 第一 它 提供 了 一种 有 严谨 数学 
基础 的 方法 来 逼近 概率 计算 中 经常 出现 
的 积分 积分 计算 的 复杂度 随着 空间维度 的 提高 
呈几何 增长 第二 采样 过程 最终 获得 的 是 整个 
联合 分布 的 样本 集 而 不仅仅 是 对 某些 
参数 或者 变量值 的 最优 估计 这个 样 本集 近似 
地 提供 了 对 整个 分布 的 更 全面 的 
刻画 比如 你 可以 计算 任意 两个 变量 的 相关 
系数 第三 它 的 渐近 特性 通常 可以 被 严格 
证明 对于 复杂 的 模型 由 variational inference 或者 belief 
propagation 所 获得 的 解 一般 并 不能 保证 是 
对 问题 的 全局 最优 解 在 大 部分 情况 
下 甚至/d 无法/n 了解/v 它/r 和/c 最优/d 解的/nr 距离/n 有/v 
多远/i 如果 使用 采样 只要 时间 足够 长 是 可以 
任意 逼近 真实 的 分布 的 而且 采样 过程 的 
复杂度 往往 较为 容易 获得 理论上 的 保证 蒙特卡罗 方法 
本身 也 是 现代 统计学 中 一个 非常 重要 的 
分支 对 它 的 研究 在 过去 几十年 来 一直 
非常 活跃 在 机器学习 领域 中 常见 的 采样 方法 
包括 Gibbs Sampling Metropolis Hasting Sampling M H   Importance 
Sampling Slice Sampling 以及 Hamiltonian Monte Carlo 其中 Gibbs Sampling 
由于 可以 纳入 M H 方法 中 解释 而 通常 
被 视为 M H 的 特例 虽然 它们 最初 的 
motivation 是 不 一样 的 Graphical Model 以及 与 它 
相关 的 probabilistic inference 是 一个 非常 博大 的 领域 
远非 本文 所能 涵盖 在 这篇文章 中 我 只能 蜻蜓点水 
般 地 介绍 了 其中 一些 我 较为 熟悉 的 
方面 希望/v 能给在/nr 这/r 方面/n 有/v 兴趣/n 的/uj 朋友/n 一点/m 
参考/v ./i 非/h 参数/n 化/n 的/uj 模型/n 还有/v 其它/r 哪些/r 
优势/n 林达 华 老师 非 参数 化 模型 确实 引入 
了 其它 参数 比如 concentration parameter 但是 这个 参数 和 
component 的 个数 在 实用 中 是 有着 不同 的 
影响 的 concentration parameter 主要 传达 的 是 使用者 希望 
形成 的 聚 类 粒度 举个 简单 的 例子 比如 
一组 数据 存在 3个 大类 每个 大类 中有 3个 相对 
靠近 的 子类 这种 情况 下 聚 成 3类 或者 
9类 都是 合理 的 解 如果 concentration parameter 设得 比较 
大 最后 的 结果 可能 形成 9类 如果 设得 比较 
小 则 可能 形成 3类 但是 如果 人为地 固定 类 
数 则 很可能 导致 不 合理 的 结果 需要 强调 
的 是非 参数 化 贝叶斯 方法 是 一个 非常 博大 
的 方向 目前 的 研究 只 是 处于 起步 阶段 
而 Dirichlet Process mixture model 只 是非 参数 方法 的 
一个 具体 应用 事实上 DP 像 Gauss distribution 一样 都是/nr 
一种 有着 良好 数学 性质 的 过程 分布 但是 它们 
在 实用 中都 过于 理想 化了 目前 的 一个 新的 
研究 方向 就是 建立 更为 贴近 实际 的 非 参数 
化 过程 相比 于 传统 参数 化 方法 而言 非 
参数 化 方法 的 主要 优势 是 允许 模型 的 
结构 在 学习 的 过程 中 动态变化 而 不仅仅 是 
组件 的 数量 这种 灵活性 对于 描述 处于 不断 变化 
中 的 数据 非常 重要 当然 如何 在 更 复杂 
的 模型 中 应用 非 参数 化 方法 是 一个 
比较 新的 课题 有 很多 值得 进一步 探索 的 地方 
「 SIGVC BBS 」 文中 后面 提到 的 结构 学习 
是不是 这 两年 比较 火 的 Structured Output Prediction 呢 
他们 的 关系 如何 Structured Percepton 和 Structured SVM 应该 
就是 属于 这个 大类 吗 结构 学习 的 输出 是 
树结 构和 图 结构 吗 结构 学习 与 图像 的 
层次 分割 或者 层次 聚 类有 关系 吗 林达 华 
老师 Structured Prediction e . g . Structured SVM 其实 
属于 利用 结构 而 不是 我 在 文中 所指 结构 
学习 在 大部分 Structured Prediction 的 应用 中 结构 是 
预先 固定 的 比如 哪些 变量 要用 potential 联系 在 
一起 学习 的 过程 其实 只是 优化 待定 的 参数 
尽管如此 这些 工作 本身 是 非常 有 价值 的 在 
很多 问题 中 都 取得 了 不错 的 效果 我 
在 文中 所 提到 的 结构 学习 是 指 连 
结构 本身 都是 不 固定 的 需要 从 数据 中 
去 学习 一般 情况 下 学习 输出 的 是 图 
或者 树 的 结构 以及 相关 参数 这个 topic 其实 
历史 很 长了 早期 的 代表 性 工作 就是 chow 
liu tree 这 是 一种 利用 信息 量计算 寻找 最 
优树 结构 来 描述 数据 的 算法 Alan Willsky 的 
小组 近几年 在 这个 方向 取得 了 很多 进展 但是 
总体而言 这个 方向 仍旧 非常 困难 大 部分 工作 属于 
探索性 的 并不 特别 成熟 目前 在 Vision 中的 应用 
不是 特别 广泛 但是 我 相信 随着 一些 方法 逐步 
成熟 进入 实用阶段 它 的 应用 前景 是 非常 不错 
的 「 SIGVC BBS 」 文中 提到 了 Convolutional Deep 
Network Deep Belief Network Deep Boltzmann Machine 等 近年 炙手可热 
的 神经 网络 方法 那么 神经 网络 和 概率 图 
模型 是不是 本质 上 完全 是 一回 事 只是 观察 
角度 和 历史 发展 不同 感觉 它 们 很多 地方 
都很/nr 相似 深度/ns 学习/v 里/f RBM/w 学习/v 的/uj 训练/vn 算法/n 
与/p 概率/n 图/n 模型/n 的/uj 学习/v 推理/v 算法/n 有/v 什么/r 
联系/n 和/c 区别/n 吗/y 他们/r 的/uj 结构/n 模型/n 有/v 什么/r 
联系/n 和/c 区别/n 吗/y 林达 华 老师 这 两类 模型 
所 使用 的 数学 方法 是 非常 不同 的 Graphical 
model/w 的/uj 很多/m 推断/v 和/c 学习/v 方法/n 都有/nr 很深/i 的/uj 
数学/n 根基/n 通过 近 十 几年 的 努力 大家 已经 
逐步 建立 起 整套 的 方法论 体系 对 相关 算法 
进行 分析 Deep Learning 目前 并 没有 什么 有效 的 
分析 方法 Deep learning 取得 很好 的 性能 其中 很多 
技巧性 的 方法 trick 起到 了 重要 作用 至于 为什么 
这些 trick 能 导致 更好 的 性能 目前 还 未能 
有 一个 很好 的 解释 我 个人 看来 这些 技巧 
其实 是 很 有价值 的 一方面 它们 确实 在 实践 
中 提高 了 性能 另外 一方面 它们 为 理论上 的 
探索 提出 了 问题 但是 我 觉得 有效 回答 这些 
问题 需要 新的 数学 工具 新的 数学 分析方法 这 看来 
不是 近期内 能 做到 的 「 SIGVC BBS 」 在 
一些 论文 中 看到 采样 的 方法 如 Gibbs 采样 
也 有其 缺点 一个 是 计算 量 比较 大 computationally 
intensive 另 一个 是 收敛 检测 比较 难 不 知道 
这些 说法 是否 有 道理 或者 目前 这些 问题 是否 
有 得到解决 林达 华 老师 这里 提到 的 两个 问题 
确实 是 Sampling 的 两个 主要 的 困难 对于 这些 
问题 过去 几十 年 取得 了 很多 进展 提出 了 
很多 新的 采 样方法 但是 困难 仍然 很大 但是 采样 
能 提供 整个 分布 的 信息 而且有 渐近 asymptotic 的 
理论 保证 这 在 很多 情况下 是 一般 的 optimization 
方法 做 不到 的 最近/f 有/v 新的/i 研究/vn 尝试/vn 结合/v 
Sampling/w 和/c Optimization/w 在 特定 问题 上 有 一些 有趣 
的 结果 比如 George Papandreou 的 Perturb and MAP . 
「 SIGVC BBS 」 在 计算机 视觉 中 视觉 目标 
跟踪 问题 已经 用 到了 动态 贝叶斯 网络 方法 一些 
最近 发表 的 自然 图像 分割 方法 也 用到 LDA 
Latent Dirichlet Allocation 在 受限 的 理想 数据 条件下 这些 
方法 都 取得 了 较好 的 结果 但是 不得不 承认 
我们 在 研究 和 应用 的 过程 中 在 心理 
上 首先 对 应用 概率 图 模型 有所 畏惧 这里 
除 我们 已经 用 得 较多 较 熟悉 的 MRF 
CRF 和 Dynamic Bayesian network based visual tracking condensation 之外 
主要 的 解释 可能有 一方面 它 不象 很多 正则化 方法 
那样 其 细节 能被 自我 掌握 观测 和 控制 另一方面 
对于 一个 新的 问题 我们 需要 不停 地 问自己 什么样 
的 设计 图 是 最好 的 从而 在 很多 情况 
下 我们 更 愿意 选择 使用 那些 正则化 方法 比如 
对 小规模 人脸识别 我们 会 选择 PCA ＋ LAD SVM 
对 大 一点 的 规模 我们 会 考虑 特征选择 ＋ 
adaboost 框架 就 计算机 视觉 能否 从 实践 的 角度 
给 我们 一点 关于 使用 概率 图 模型 的 建议 
另外 在 计算机 视觉 中 什么样 的 问题 更 适合 
于 采用 概率 图 模型 方法 来 解决 林达 华 
老师 首先 Graphical model 和 其它 的 方法 一样 只是 
一种 数学 工具 对于 解决 问题 而言 最 重要 的 
是 选择 合适 的 工具 而不 一定 要 选 看上去 
高深 的 方法 对于 普通 的 分类 问题 传统 的 
SVM Boost 仍 不失为 最 有效 的 方法 Graphical model 
通常 应用 在 问题 本身 带有 多个 相互 联系 的 
变量 的 时候 这个 时候 Graphical model 提供 了 一种 
表达 方式 让 你 去 表达 这些 联系 我 觉得 
并不 必要 去 寻求 最优 的 设计图 事实上 没有 人 
知道 什么样 的 图 才是 最优 的 实践 中 我们 
通常 是 根据 问题 本身 建立 一个 能 比较 自然 
地 表达 问题 结构 的 图 然后 通过 实验 了 
验证 这个 图 是不是 合适 的 如果 不 合适 可以 
根据 结果 分析 原因 对 图 做出 修正 举个 具体 
的 例子 比如 对 一个 比赛 视频 进行 分析 那么 
可能 涉及 多个 变量 摄像机 的 角度 背景 运动员 的 
动作 等等 那么 这个 问题 可能 就 设计 多 个 
未知 变量 的 推断 这些 变量 间 可能 存在 各种 
联系 这个 时候 Graphical model 可能 就是 一种 合适 的 
选择 值得 注意 的 是 选择 合适 的 图 有时候 
也 需要 一些 经验 比如 分布 的 选择 上 要注意 
形成 conjugate 这样 往往 容易 得到 简易 的 推断 公式 
了解 各种 分布 的 特性 以及 它们 可能 对 最后 
结果 的 影响 也是 有 帮助 的 上世纪 60 年代 Marvin Minsky 在 MIT 让 他 的 
本科 学生 Gerald Jay Sussman 用 一个 暑假 的 时间 
完成 一个 有趣 的 Project link a camera to a 
computer and get the computer to describe what it saw 
从 那时 开始 特别 是 David Marr 教授 于 1977年 
正式 提出 视觉 计算 理论 计算机 视觉 已经 走过 了 
四十 多年 的 历史 可是 从 今天 看来 这个 已 
入 不惑之年 的 学科 依然 显得 如此 年轻 而 朝气蓬勃 
在 它 几十年 的 发展 历程 中 多种 流派 的 
方法 都曾 各领风骚 于 一时 最近二 十年 中 计算机 视觉 
发展 最 鲜明 的 特征 就是 机器学习 与 概率模型 的 
广泛 应用 在 这里 我 简单 回顾 一下 对 这个 
领域 产生 了 重要 影响 的 几个 里程碑 ● 1984年 
Stuart Geman 和 Donald Geman 发表 了 一篇 先驱 性 
的 论文 Stochastic Relaxation Gibbs Distributions and the Bayesian Restoration 
of Images . 在 这篇文章 里 两位 Geman 先生 引入 
了 一 系列 对 计算机 视觉 以后 的 发展 具有 
深远 影响 的 概念 和 方法 Markov Random Field MRF 
  Gibbs Sampling 以及 Maximum a Posteriori estimate MAP estimate 
这篇 论文 的 意义 是 超前 于 时代 的 它 
所 建立 的 这 一系列 方法 直到 90 年代 中后期 
才 开始 被 广泛 关注 ● 1991年 Matthew Turk 和 
Alex Pentland 使用 Eigenface 进行 人脸 分类 从此 以 矩阵 
的 代数 分解 为 基础 的 方法 在 视觉 分析 
中 被 大量 运用 其中 有 代表性 的 方法 包括 
PCA LDA 以及 ICA ● 1995年 Corinna Cortes 和 Vladimir 
Vapnik 提出 带有 soft margin 的 Support Vector Machine SVM 
以及 它 的 Kernel 版本 并用 它 对 手写 数字 
进行 分类 从此 SVM 大受欢迎 并 成为 各种 应用 中 
的 基准 分类器 ● 1996年 Bruno Olshausen 和 David Field 
提出 使用 Overcomplete basis 对 图像 进行 稀疏 编码 Sparse 
coding 这个 方向 在 初期 的 反响 并不 热烈 直到 
近些年 Compressed Sensing 在 信号 处理 领域 成为 炙手可热 的 
方向 Sparse coding 在 这一 热潮 的 带动 下 成为 
视觉 领域 一个 活跃 的 研究 方向 ● 90 年代 
末 Graphical Model 和 Variational Inference 逐步 发展 成熟 1998年 
MIT 出版社 出版 了 由 Michale Jordan 主编 的 文集 
Learning in Graphical Models 这部 书 总结 了 那 一时期 
关于 Graphical Model 的 建模 分析 和 推断 的 主要 
成果 这些 成果 为 Graphical Model 在 人工智能 的 各个 
领域 的 应用 提供 了 方法论 基础 进入 21 世纪 
Graphical Model 和 Bayesian 方法 在 视觉 研究 中 的 
运用 出现 了 井喷式 的 增长 ● 2001年 John Lafferty 
和 Andrew McCallum 等 提出 Conditional Random Field CRF CRF 
为 结构化 的 分类 和 预测 提供 了 一种 通用 
的 工具 此后 语义 结构 开始 被 运用 于 视觉 
场景 分析 ● 2003年 David Blei 等 提出 Latent Dirichlet 
Allocation 2004年 Yee Whye Teh 等 提出 Hierarchical Dirichlet Process 
各种 参数 化 或者 非 参数 化 的 Topic Model 
在此后 不久 被 广泛 用于 语义 层面 的 场景 分析 
● 虽然 Yahn Lecun 等 人在 1993年 已 提出 Convolutional 
Neural Network 但在 vision 中的 应用 效果 一直 欠佳 时至 
2006年 Geoffrey Hinton 等人 提出 Deep Belief Network 进行 layer 
wise 的 pretraining 应用 效果 取得 突破性 进展 其 与 
之后 Ruslan Salakhutdinov 提出 的 Deep Boltzmann Machine 重新 点燃 
了 视觉 领域 对于 Neural Network 和 Boltzmann Machine 的 
热情 时间 进入 2013年 Probabilistic Graphical Model 早已 成为 视觉 
领域 中 一种 基本 的 建模 工具 Probabilistic Graphical Model 
的 研究 涉及 非常多 的 方面 限于 篇幅 在 本文 
中 我 只能 简要 介绍 其 中 几个 重要 的 
方面 希望 能为/nr 大家 提供 一些 有用 的 参考 Graphical 
Model 的 基本 类型 基本 的 Graphical Model 可以 大致 
分为 两个 类别 贝叶斯 网络 Bayesian Network 和 马尔可夫 随 
机场 Markov Random Field 它们 的 主要 区别 在于 采用 
不同 类型 的 图 来 表达 变量 之间 的 关系 
贝叶斯 网络 采用 有向 无 环 图 Directed Acyclic Graph 
来 表达 因果关系 马尔可夫 随 机场 则 采用 无向图 Undirected 
Graph 来 表达 变量 间 的 相互作用 这种 结构上 的 
区别 导致 了 它们 在 建模 和 推断 方面 的 
一系列 微妙 的 差异 一般来说 贝叶斯/nr 网络/n 中/f 每一个/i 节点/n 
都/d 对应/vn 于/p 一个/m 先验/n 概率分布/n 或者/c 条件/n 概率分布/n 因此 
整体 的 联合 分布 可以 直接 分解 为 所有 单个 
节点 所 对应 的 分布 的 乘积 而 对于 马尔可夫 
场 由于 变量 之间 没有 明确 的 因果关系 它 的 
联合 概率分布 通常 会 表达 为 一系列 势函数 potential function 
的 乘积 通常 情况下 这些 乘积 的 积分 并不等于 1 
因此 还要 对 其 进行 归一化 才能 形成 一个 有效 
的 概率分布 这 一点 往往 在 实际 应用 中 给 
参数估计 造成 非常 大 的 困难 值得一提的是 贝叶斯/nr 网络/n 和/c 
马尔可夫/nr 随/v 机场/n 的/uj 分类/n 主要/b 是/v 为了/p 研究/vn 和/c 
学习/v 的/uj 便利/a 在 实际 应用 中 所 使用 的 
模型 在 很多 时候 是 它们 的 某种 形式 的 
结合 比如 一个 马尔可夫 随 机场 可以 作为 整体 成为 
一个 更大 的 贝叶斯 网络 的 节点 又 或者 多个 
贝叶斯 网络 可以 通过 马尔可夫 随 机场 联系起来 这种 混合型 
的 模型 提供 了 更 丰富 的 表达 结构 同时 
也 会给 模型 的 推断 和 估计 带来 新的 挑战 
Graphical Model 的 新 发展 方向 在 传统 的 Graphical 
Model 的 应用 中 模型 的 设计 者 需要 在 
设计 阶段 就 固定 整个 模型 的 结构 比如 它 
要 使用 哪些 节点 它们 相互之间 如何 关联 等等 但是 
在 实际 问题 中 选择 合适 的 模型 结构 往往 
是 非常 困难 的 因为 我们 在 很多 时候 其实 
并不 清楚 数据 的 实际 结构 为了 解决 这个 问题 
人们 开始 探索 一种 新 的 建立 概率模型 的 方式 
结构 学习 在 这种 方法 中 模型 的 结构 在 
设计 的 阶段 并 不完全 固定 设计 者 通常 只 
需要 设定 模型 结构 所 需要 遵循 的 约束 然后 
再从 模型 学习 的 过程 中 同时 推断出 模型 的 
实际 结构 结构 学习 直到 今天 仍然 是 机器 学习 
中 一个 极 具 挑战性 的 方向 结构 学习 并 
没有 固定 的 形式 不同 的 研究 者 往往 会 
采取 不同 的 途径 比如 结构 学习 中 一个 非常 
重要 的 问题 就是 如何 去 发现 变量 之间 的 
内部 关联 对于 这个 问题 人们 提出 了 多种 截然不同 
的 方法 比如 你 可以 先 建立 一个 完全 图 
连接 所有 的 变量 然后 选择 一个 子图 来 描述 
它们 的 实际 结构 又 或者 你 可以 引入 潜在 
节点 latent node 来 建立 变量 之间 的 关联 Probabilistic 
Graphical Model 的 另外 一个 重要 的 发展 方向 是非 
参数 化 与 传统 的 参数 化 方法 不同 非 
参数 化 方法 是 一种 更为 灵活 的 建模 方式 
非 参数 化 模型 的 大小 比如 节点 的 数量 
可以 随着 数据 的 变化 而 变化 一个 典型 的 
非 参数 化 模型 就 是 基于 狄利克 莱 过程 
Dirichlet Process 的 混合模型 这种 模型 引入 狄利克 莱 过程 
作为 部件 component 参数 的 先验 分布 从而 允许 混合体 
中 可以 有 任意 多个 部件 这 从 根本 上 
克服 了 传统 的 有限 混合模型 中 的 一个 难题 
就是 确定 部件 的 数量 在 近几年 的 文章 中 
非 参数 化 模型 开始 被 用于 特征 学习 在这方面 
比较 有 代表性 的 工作 就 是 基于 Hierarchical Beta 
Process 来 学习 不定 数量 的 特征 基于 Graphical Model 
的 统计 推断 Inference 完成 模型 的 设计 之后 下 
一步 就是 通过 一定 的 算法 从 数据 中去 估计 
模型 的 参数 或 推断 我们 感兴趣 的 其它 未知 
变量 的 值 在 贝叶斯 方法 中 模型 的 参数 
也 通常 被 视为 变量 它们 和 普通 的 变量 
并 没有 根本 的 区别 因此 参数估计 也 可以 被 
视为 是 统计 推断 的 一种 特例 除了 最 简单 
的 一些 模型 统计 推断 在 计算 上 是 非常 
困难 的 一般而言 确切 推断 exact inference 的 复杂度 取决于 
模型 的 tree width 对于 很多 实际 模型 这个 复杂 
度 可能 随着 问题 规模 增长 而 指数 增长 于是 
人们 退而求其次 转而 探索 具有 多项式 复杂度 的 近似 推断 
approximate inference 方法 主流 的 近似 推断 方法 有三种 1 
基于 平均 场 逼近 mean field approximation 的 variational inference 
这种方法 通常用于 由 Exponential family distribution 所 组成 的 贝叶斯 
网络 其 基本 思想 就是 引入 一个 computationally tractable 的 
upper bound 逼近 原 模型 的 log partition function 从而 
有效 地 降低 了 优化 的 复杂度 大家 所 熟悉 
的 EM 算法 就 属于 这 类型 算法 的 一种 
特例 2 Belief propagation 这种方法 最初 由 Judea Pearl 提出 
用于 树状 结构 的 统计 推断 后来 人们 直接 把 
这种 算法 用于 带环 的 模型 忽略 掉 它 本来 
对 树状 结构 的 要求 在 很多 情况 下 仍然 
取得 不错 的 实际 效果 这 就是 loop belief propagation 
在 进一步 的 探索 的 过程 中 人们 发现 了 
它 与 Bethe approximation 的 关系 并 由此 逐步 建立 
起 了 对 loopy belief propagation 的 理论 解释 以及 
刻画出 它 在 各种 设 定下 的 收敛 条件 值得一提的是 
由于 Judea Pearl 对 人工 智能 和 因果关系 推断 方法 
上 的 根本性 贡献 他 在 2011年 获得 了 计算机 
科学 领域 的 最高 奖 图灵奖 基于 message passing 的 
方法 在 最近 十年 有 很多 新的 发展 Martin Wainwright 
在 2003年 提出 Tree reweighted message passing 这种方法 采用 mixture 
of trees 来 逼近 任意 的 graphical model 并 利用 
mixture coefficient 和 edge probability 之间 的 对偶 关系 建立 
了 一种 新的 message passing 的 方法 这种 方法 是 
对 belief propagation 的 推广 Jason Johnson 等 人在 2005年 
建立 的 walk sum analysis 为 高斯 马尔可夫 随机 场上 
的 belief propagation 提供 了 系统 的 分析 方法 这种 
方法 成功 刻画 了 belief propagation 在 高斯 场上 的 
收敛 条件 也是 后来 提出 的 多种 改进型 的 belief 
propagation 的 理论 依据 Thomas Minka 在 他 PhD 期间 
所 建立 的 expectation propagation 也是 belief propagation 的 在 
一般 Graphical Model 上 的 重要 推广 3 蒙特卡罗 采样 
Monte Carlo sampling 与 基于 优化 的 方法 不同 蒙特卡罗 
方法 通过 对 概率模型 的 随机 模拟 运行 来 收集 
样本 然后 通过 收集 到 的 样本 来 估计 变量 
的 统计 特性 比如 均值 采样 方法 有三个/nr 方面 的 
重要 优点 第一 它 提供 了 一种 有 严谨 数学 
基础 的 方法 来 逼近 概率 计算 中 经常 出现 
的 积分 积分 计算 的 复杂度 随着 空间维度 的 提高 
呈几何 增长 第二 采样 过程 最终 获得 的 是 整个 
联合 分布 的 样本 集 而 不仅仅 是 对 某些 
参数 或者 变量值 的 最优 估计 这个 样 本集 近似 
地 提供 了 对 整个 分布 的 更 全面 的 
刻画 比如 你 可以 计算 任意 两个 变量 的 相关 
系数 第三 它 的 渐近 特性 通常 可以 被 严格 
证明 对于 复杂 的 模型 由 variational inference 或者 belief 
propagation 所 获得 的 解 一般 并 不能 保证 是 
对 问题 的 全局 最优 解 在 大 部分 情况 
下 甚至/d 无法/n 了解/v 它/r 和/c 最优/d 解的/nr 距离/n 有/v 
多远/i 如果 使用 采样 只要 时间 足够 长 是 可以 
任意 逼近 真实 的 分布 的 而且 采样 过程 的 
复杂度 往往 较为 容易 获得 理论上 的 保证 蒙特卡罗 方法 
本身 也 是 现代 统计学 中 一个 非常 重要 的 
分支 对 它 的 研究 在 过去 几十年 来 一直 
非常 活跃 在 机器学习 领域 中 常见 的 采样 方法 
包括 Gibbs Sampling Metropolis Hasting Sampling M H   Importance 
Sampling Slice Sampling 以及 Hamiltonian Monte Carlo 其中 Gibbs Sampling 
由于 可以 纳入 M H 方法 中 解释 而 通常 
被 视为 M H 的 特例 虽然 它们 最初 的 
motivation 是 不 一样 的 Graphical Model 以及 与 它 
相关 的 probabilistic inference 是 一个 非常 博大 的 领域 
远非 本文 所能 涵盖 在 这篇文章 中 我 只能 蜻蜓点水 
般 地 介绍 了 其中 一些 我 较为 熟悉 的 
方面 希望/v 能给在/nr 这/r 方面/n 有/v 兴趣/n 的/uj 朋友/n 一点/m 
参考/v ./i 非/h 参数/n 化/n 的/uj 模型/n 还有/v 其它/r 哪些/r 
优势/n 林达 华 老师 非 参数 化 模型 确实 引入 
了 其它 参数 比如 concentration parameter 但是 这个 参数 和 
component 的 个数 在 实用 中 是 有着 不同 的 
影响 的 concentration parameter 主要 传达 的 是 使用者 希望 
形成 的 聚 类 粒度 举个 简单 的 例子 比如 
一组 数据 存在 3个 大类 每个 大类 中有 3个 相对 
靠近 的 子类 这种 情况 下 聚 成 3类 或者 
9类 都是 合理 的 解 如果 concentration parameter 设得 比较 
大 最后 的 结果 可能 形成 9类 如果 设得 比较 
小 则 可能 形成 3类 但是 如果 人为地 固定 类 
数 则 很可能 导致 不 合理 的 结果 需要 强调 
的 是非 参数 化 贝叶斯 方法 是 一个 非常 博大 
的 方向 目前 的 研究 只 是 处于 起步 阶段 
而 Dirichlet Process mixture model 只 是非 参数 方法 的 
一个 具体 应用 事实上 DP 像 Gauss distribution 一样 都是/nr 
一种 有着 良好 数学 性质 的 过程 分布 但是 它们 
在 实用 中都 过于 理想 化了 目前 的 一个 新的 
研究 方向 就是 建立 更为 贴近 实际 的 非 参数 
化 过程 相比 于 传统 参数 化 方法 而言 非 
参数 化 方法 的 主要 优势 是 允许 模型 的 
结构 在 学习 的 过程 中 动态变化 而 不仅仅 是 
组件 的 数量 这种 灵活性 对于 描述 处于 不断 变化 
中 的 数据 非常 重要 当然 如何 在 更 复杂 
的 模型 中 应用 非 参数 化 方法 是 一个 
比较 新的 课题 有 很多 值得 进一步 探索 的 地方 
「 SIGVC BBS 」 文中 后面 提到 的 结构 学习 
是不是 这 两年 比较 火 的 Structured Output Prediction 呢 
他们 的 关系 如何 Structured Percepton 和 Structured SVM 应该 
就是 属于 这个 大类 吗 结构 学习 的 输出 是 
树结 构和 图 结构 吗 结构 学习 与 图像 的 
层次 分割 或者 层次 聚 类有 关系 吗 林达 华 
老师 Structured Prediction e . g . Structured SVM 其实 
属于 利用 结构 而 不是 我 在 文中 所指 结构 
学习 在 大部分 Structured Prediction 的 应用 中 结构 是 
预先 固定 的 比如 哪些 变量 要用 potential 联系 在 
一起 学习 的 过程 其实 只是 优化 待定 的 参数 
尽管如此 这些 工作 本身 是 非常 有 价值 的 在 
很多 问题 中 都 取得 了 不错 的 效果 我 
在 文中 所 提到 的 结构 学习 是 指 连 
结构 本身 都是 不 固定 的 需要 从 数据 中 
去 学习 一般 情况 下 学习 输出 的 是 图 
或者 树 的 结构 以及 相关 参数 这个 topic 其实 
历史 很 长了 早期 的 代表 性 工作 就是 chow 
liu tree 这 是 一种 利用 信息 量计算 寻找 最 
优树 结构 来 描述 数据 的 算法 Alan Willsky 的 
小组 近几年 在 这个 方向 取得 了 很多 进展 但是 
总体而言 这个 方向 仍旧 非常 困难 大 部分 工作 属于 
探索性 的 并不 特别 成熟 目前 在 Vision 中的 应用 
不是 特别 广泛 但是 我 相信 随着 一些 方法 逐步 
成熟 进入 实用阶段 它 的 应用 前景 是 非常 不错 
的 「 SIGVC BBS 」 文中 提到 了 Convolutional Deep 
Network Deep Belief Network Deep Boltzmann Machine 等 近年 炙手可热 
的 神经 网络 方法 那么 神经 网络 和 概率 图 
模型 是不是 本质 上 完全 是 一回 事 只是 观察 
角度 和 历史 发展 不同 感觉 它 们 很多 地方 
都很/nr 相似 深度/ns 学习/v 里/f RBM/w 学习/v 的/uj 训练/vn 算法/n 
与/p 概率/n 图/n 模型/n 的/uj 学习/v 推理/v 算法/n 有/v 什么/r 
联系/n 和/c 区别/n 吗/y 他们/r 的/uj 结构/n 模型/n 有/v 什么/r 
联系/n 和/c 区别/n 吗/y 林达 华 老师 这 两类 模型 
所 使用 的 数学 方法 是 非常 不同 的 Graphical 
model/w 的/uj 很多/m 推断/v 和/c 学习/v 方法/n 都有/nr 很深/i 的/uj 
数学/n 根基/n 通过 近 十 几年 的 努力 大家 已经 
逐步 建立 起 整套 的 方法论 体系 对 相关 算法 
进行 分析 Deep Learning 目前 并 没有 什么 有效 的 
分析 方法 Deep learning 取得 很好 的 性能 其中 很多 
技巧性 的 方法 trick 起到 了 重要 作用 至于 为什么 
这些 trick 能 导致 更好 的 性能 目前 还 未能 
有 一个 很好 的 解释 我 个人 看来 这些 技巧 
其实 是 很 有价值 的 一方面 它们 确实 在 实践 
中 提高 了 性能 另外 一方面 它们 为 理论上 的 
探索 提出 了 问题 但是 我 觉得 有效 回答 这些 
问题 需要 新的 数学 工具 新的 数学 分析方法 这 看来 
不是 近期内 能 做到 的 「 SIGVC BBS 」 在 
一些 论文 中 看到 采样 的 方法 如 Gibbs 采样 
也 有其 缺点 一个 是 计算 量 比较 大 computationally 
intensive 另 一个 是 收敛 检测 比较 难 不 知道 
这些 说法 是否 有 道理 或者 目前 这些 问题 是否 
有 得到解决 林达 华 老师 这里 提到 的 两个 问题 
确实 是 Sampling 的 两个 主要 的 困难 对于 这些 
问题 过去 几十 年 取得 了 很多 进展 提出 了 
很多 新的 采 样方法 但是 困难 仍然 很大 但是 采样 
能 提供 整个 分布 的 信息 而且有 渐近 asymptotic 的 
理论 保证 这 在 很多 情况下 是 一般 的 optimization 
方法 做 不到 的 最近/f 有/v 新的/i 研究/vn 尝试/vn 结合/v 
Sampling/w 和/c Optimization/w 在 特定 问题 上 有 一些 有趣 
的 结果 比如 George Papandreou 的 Perturb and MAP . 
「 SIGVC BBS 」 在 计算机 视觉 中 视觉 目标 
跟踪 问题 已经 用 到了 动态 贝叶斯 网络 方法 一些 
最近 发表 的 自然 图像 分割 方法 也 用到 LDA 
Latent Dirichlet Allocation 在 受限 的 理想 数据 条件下 这些 
方法 都 取得 了 较好 的 结果 但是 不得不 承认 
我们 在 研究 和 应用 的 过程 中 在 心理 
上 首先 对 应用 概率 图 模型 有所 畏惧 这里 
除 我们 已经 用 得 较多 较 熟悉 的 MRF 
CRF 和 Dynamic Bayesian network based visual tracking condensation 之外 
主要 的 解释 可能有 一方面 它 不象 很多 正则化 方法 
那样 其 细节 能被 自我 掌握 观测 和 控制 另一方面 
对于 一个 新的 问题 我们 需要 不停 地 问自己 什么样 
的 设计 图 是 最好 的 从而 在 很多 情况 
下 我们 更 愿意 选择 使用 那些 正则化 方法 比如 
对 小规模 人脸识别 我们 会 选择 PCA ＋ LAD SVM 
对 大 一点 的 规模 我们 会 考虑 特征选择 ＋ 
adaboost 框架 就 计算机 视觉 能否 从 实践 的 角度 
给 我们 一点 关于 使用 概率 图 模型 的 建议 
另外 在 计算机 视觉 中 什么样 的 问题 更 适合 
于 采用 概率 图 模型 方法 来 解决 林达 华 
老师 首先 Graphical model 和 其它 的 方法 一样 只是 
一种 数学 工具 对于 解决 问题 而言 最 重要 的 
是 选择 合适 的 工具 而不 一定 要 选 看上去 
高深 的 方法 对于 普通 的 分类 问题 传统 的 
SVM Boost 仍 不失为 最 有效 的 方法 Graphical model 
通常 应用 在 问题 本身 带有 多个 相互 联系 的 
变量 的 时候 这个 时候 Graphical model 提供 了 一种 
表达 方式 让 你 去 表达 这些 联系 我 觉得 
并不 必要 去 寻求 最优 的 设计图 事实上 没有 人 
知道 什么样 的 图 才是 最优 的 实践 中 我们 
通常 是 根据 问题 本身 建立 一个 能 比较 自然 
地 表达 问题 结构 的 图 然后 通过 实验 了 
验证 这个 图 是不是 合适 的 如果 不 合适 可以 
根据 结果 分析 原因 对 图 做出 修正 举个 具体 
的 例子 比如 对 一个 比赛 视频 进行 分析 那么 
可能 涉及 多个 变量 摄像机 的 角度 背景 运动员 的 
动作 等等 那么 这个 问题 可能 就 设计 多 个 
未知 变量 的 推断 这些 变量 间 可能 存在 各种 
联系 这个 时候 Graphical model 可能 就是 一种 合适 的 
选择 值得 注意 的 是 选择 合适 的 图 有时候 
也 需要 一些 经验 比如 分布 的 选择 上 要注意 
形成 conjugate 这样 往往 容易 得到 简易 的 推断 公式 
了解 各种 分布 的 特性 以及 它们 可能 对 最后 
结果 的 影响 也是 有 帮助 的 上世纪 60 年代 Marvin Minsky 在 MIT 让 他 的 
本科 学生 Gerald Jay Sussman 用 一个 暑假 的 时间 
完成 一个 有趣 的 Project link a camera to a 
computer and get the computer to describe what it saw 
从 那时 开始 特别 是 David Marr 教授 于 1977年 
正式 提出 视觉 计算 理论 计算机 视觉 已经 走过 了 
四十 多年 的 历史 可是 从 今天 看来 这个 已 
入 不惑之年 的 学科 依然 显得 如此 年轻 而 朝气蓬勃 
在 它 几十年 的 发展 历程 中 多种 流派 的 
方法 都曾 各领风骚 于 一时 最近二 十年 中 计算机 视觉 
发展 最 鲜明 的 特征 就是 机器学习 与 概率模型 的 
广泛 应用 在 这里 我 简单 回顾 一下 对 这个 
领域 产生 了 重要 影响 的 几个 里程碑 ● 1984年 
Stuart Geman 和 Donald Geman 发表 了 一篇 先驱 性 
的 论文 Stochastic Relaxation Gibbs Distributions and the Bayesian Restoration 
of Images . 在 这篇文章 里 两位 Geman 先生 引入 
了 一 系列 对 计算机 视觉 以后 的 发展 具有 
深远 影响 的 概念 和 方法 Markov Random Field MRF 
  Gibbs Sampling 以及 Maximum a Posteriori estimate MAP estimate 
这篇 论文 的 意义 是 超前 于 时代 的 它 
所 建立 的 这 一系列 方法 直到 90 年代 中后期 
才 开始 被 广泛 关注 ● 1991年 Matthew Turk 和 
Alex Pentland 使用 Eigenface 进行 人脸 分类 从此 以 矩阵 
的 代数 分解 为 基础 的 方法 在 视觉 分析 
中 被 大量 运用 其中 有 代表性 的 方法 包括 
PCA LDA 以及 ICA ● 1995年 Corinna Cortes 和 Vladimir 
Vapnik 提出 带有 soft margin 的 Support Vector Machine SVM 
以及 它 的 Kernel 版本 并用 它 对 手写 数字 
进行 分类 从此 SVM 大受欢迎 并 成为 各种 应用 中 
的 基准 分类器 ● 1996年 Bruno Olshausen 和 David Field 
提出 使用 Overcomplete basis 对 图像 进行 稀疏 编码 Sparse 
coding 这个 方向 在 初期 的 反响 并不 热烈 直到 
近些年 Compressed Sensing 在 信号 处理 领域 成为 炙手可热 的 
方向 Sparse coding 在 这一 热潮 的 带动 下 成为 
视觉 领域 一个 活跃 的 研究 方向 ● 90 年代 
末 Graphical Model 和 Variational Inference 逐步 发展 成熟 1998年 
MIT 出版社 出版 了 由 Michale Jordan 主编 的 文集 
Learning in Graphical Models 这部 书 总结 了 那 一时期 
关于 Graphical Model 的 建模 分析 和 推断 的 主要 
成果 这些 成果 为 Graphical Model 在 人工智能 的 各个 
领域 的 应用 提供 了 方法论 基础 进入 21 世纪 
Graphical Model 和 Bayesian 方法 在 视觉 研究 中 的 
运用 出现 了 井喷式 的 增长 ● 2001年 John Lafferty 
和 Andrew McCallum 等 提出 Conditional Random Field CRF CRF 
为 结构化 的 分类 和 预测 提供 了 一种 通用 
的 工具 此后 语义 结构 开始 被 运用 于 视觉 
场景 分析 ● 2003年 David Blei 等 提出 Latent Dirichlet 
Allocation 2004年 Yee Whye Teh 等 提出 Hierarchical Dirichlet Process 
各种 参数 化 或者 非 参数 化 的 Topic Model 
在此后 不久 被 广泛 用于 语义 层面 的 场景 分析 
● 虽然 Yahn Lecun 等 人在 1993年 已 提出 Convolutional 
Neural Network 但在 vision 中的 应用 效果 一直 欠佳 时至 
2006年 Geoffrey Hinton 等人 提出 Deep Belief Network 进行 layer 
wise 的 pretraining 应用 效果 取得 突破性 进展 其 与 
之后 Ruslan Salakhutdinov 提出 的 Deep Boltzmann Machine 重新 点燃 
了 视觉 领域 对于 Neural Network 和 Boltzmann Machine 的 
热情 时间 进入 2013年 Probabilistic Graphical Model 早已 成为 视觉 
领域 中 一种 基本 的 建模 工具 Probabilistic Graphical Model 
的 研究 涉及 非常多 的 方面 限于 篇幅 在 本文 
中 我 只能 简要 介绍 其 中 几个 重要 的 
方面 希望 能为/nr 大家 提供 一些 有用 的 参考 Graphical 
Model 的 基本 类型 基本 的 Graphical Model 可以 大致 
分为 两个 类别 贝叶斯 网络 Bayesian Network 和 马尔可夫 随 
机场 Markov Random Field 它们 的 主要 区别 在于 采用 
不同 类型 的 图 来 表达 变量 之间 的 关系 
贝叶斯 网络 采用 有向 无 环 图 Directed Acyclic Graph 
来 表达 因果关系 马尔可夫 随 机场 则 采用 无向图 Undirected 
Graph 来 表达 变量 间 的 相互作用 这种 结构上 的 
区别 导致 了 它们 在 建模 和 推断 方面 的 
一系列 微妙 的 差异 一般来说 贝叶斯/nr 网络/n 中/f 每一个/i 节点/n 
都/d 对应/vn 于/p 一个/m 先验/n 概率分布/n 或者/c 条件/n 概率分布/n 因此 
整体 的 联合 分布 可以 直接 分解 为 所有 单个 
节点 所 对应 的 分布 的 乘积 而 对于 马尔可夫 
场 由于 变量 之间 没有 明确 的 因果关系 它 的 
联合 概率分布 通常 会 表达 为 一系列 势函数 potential function 
的 乘积 通常 情况下 这些 乘积 的 积分 并不等于 1 
因此 还要 对 其 进行 归一化 才能 形成 一个 有效 
的 概率分布 这 一点 往往 在 实际 应用 中 给 
参数估计 造成 非常 大 的 困难 值得一提的是 贝叶斯/nr 网络/n 和/c 
马尔可夫/nr 随/v 机场/n 的/uj 分类/n 主要/b 是/v 为了/p 研究/vn 和/c 
学习/v 的/uj 便利/a 在 实际 应用 中 所 使用 的 
模型 在 很多 时候 是 它们 的 某种 形式 的 
结合 比如 一个 马尔可夫 随 机场 可以 作为 整体 成为 
一个 更大 的 贝叶斯 网络 的 节点 又 或者 多个 
贝叶斯 网络 可以 通过 马尔可夫 随 机场 联系起来 这种 混合型 
的 模型 提供 了 更 丰富 的 表达 结构 同时 
也 会给 模型 的 推断 和 估计 带来 新的 挑战 
Graphical Model 的 新 发展 方向 在 传统 的 Graphical 
Model 的 应用 中 模型 的 设计 者 需要 在 
设计 阶段 就 固定 整个 模型 的 结构 比如 它 
要 使用 哪些 节点 它们 相互之间 如何 关联 等等 但是 
在 实际 问题 中 选择 合适 的 模型 结构 往往 
是 非常 困难 的 因为 我们 在 很多 时候 其实 
并不 清楚 数据 的 实际 结构 为了 解决 这个 问题 
人们 开始 探索 一种 新 的 建立 概率模型 的 方式 
结构 学习 在 这种 方法 中 模型 的 结构 在 
设计 的 阶段 并 不完全 固定 设计 者 通常 只 
需要 设定 模型 结构 所 需要 遵循 的 约束 然后 
再从 模型 学习 的 过程 中 同时 推断出 模型 的 
实际 结构 结构 学习 直到 今天 仍然 是 机器 学习 
中 一个 极 具 挑战性 的 方向 结构 学习 并 
没有 固定 的 形式 不同 的 研究 者 往往 会 
采取 不同 的 途径 比如 结构 学习 中 一个 非常 
重要 的 问题 就是 如何 去 发现 变量 之间 的 
内部 关联 对于 这个 问题 人们 提出 了 多种 截然不同 
的 方法 比如 你 可以 先 建立 一个 完全 图 
连接 所有 的 变量 然后 选择 一个 子图 来 描述 
它们 的 实际 结构 又 或者 你 可以 引入 潜在 
节点 latent node 来 建立 变量 之间 的 关联 Probabilistic 
Graphical Model 的 另外 一个 重要 的 发展 方向 是非 
参数 化 与 传统 的 参数 化 方法 不同 非 
参数 化 方法 是 一种 更为 灵活 的 建模 方式 
非 参数 化 模型 的 大小 比如 节点 的 数量 
可以 随着 数据 的 变化 而 变化 一个 典型 的 
非 参数 化 模型 就 是 基于 狄利克 莱 过程 
Dirichlet Process 的 混合模型 这种 模型 引入 狄利克 莱 过程 
作为 部件 component 参数 的 先验 分布 从而 允许 混合体 
中 可以 有 任意 多个 部件 这 从 根本 上 
克服 了 传统 的 有限 混合模型 中 的 一个 难题 
就是 确定 部件 的 数量 在 近几年 的 文章 中 
非 参数 化 模型 开始 被 用于 特征 学习 在这方面 
比较 有 代表性 的 工作 就 是 基于 Hierarchical Beta 
Process 来 学习 不定 数量 的 特征 基于 Graphical Model 
的 统计 推断 Inference 完成 模型 的 设计 之后 下 
一步 就是 通过 一定 的 算法 从 数据 中去 估计 
模型 的 参数 或 推断 我们 感兴趣 的 其它 未知 
变量 的 值 在 贝叶斯 方法 中 模型 的 参数 
也 通常 被 视为 变量 它们 和 普通 的 变量 
并 没有 根本 的 区别 因此 参数估计 也 可以 被 
视为 是 统计 推断 的 一种 特例 除了 最 简单 
的 一些 模型 统计 推断 在 计算 上 是 非常 
困难 的 一般而言 确切 推断 exact inference 的 复杂度 取决于 
模型 的 tree width 对于 很多 实际 模型 这个 复杂 
度 可能 随着 问题 规模 增长 而 指数 增长 于是 
人们 退而求其次 转而 探索 具有 多项式 复杂度 的 近似 推断 
approximate inference 方法 主流 的 近似 推断 方法 有三种 1 
基于 平均 场 逼近 mean field approximation 的 variational inference 
这种方法 通常用于 由 Exponential family distribution 所 组成 的 贝叶斯 
网络 其 基本 思想 就是 引入 一个 computationally tractable 的 
upper bound 逼近 原 模型 的 log partition function 从而 
有效 地 降低 了 优化 的 复杂度 大家 所 熟悉 
的 EM 算法 就 属于 这 类型 算法 的 一种 
特例 2 Belief propagation 这种方法 最初 由 Judea Pearl 提出 
用于 树状 结构 的 统计 推断 后来 人们 直接 把 
这种 算法 用于 带环 的 模型 忽略 掉 它 本来 
对 树状 结构 的 要求 在 很多 情况 下 仍然 
取得 不错 的 实际 效果 这 就是 loop belief propagation 
在 进一步 的 探索 的 过程 中 人们 发现 了 
它 与 Bethe approximation 的 关系 并 由此 逐步 建立 
起 了 对 loopy belief propagation 的 理论 解释 以及 
刻画出 它 在 各种 设 定下 的 收敛 条件 值得一提的是 
由于 Judea Pearl 对 人工 智能 和 因果关系 推断 方法 
上 的 根本性 贡献 他 在 2011年 获得 了 计算机 
科学 领域 的 最高 奖 图灵奖 基于 message passing 的 
方法 在 最近 十年 有 很多 新的 发展 Martin Wainwright 
在 2003年 提出 Tree reweighted message passing 这种方法 采用 mixture 
of trees 来 逼近 任意 的 graphical model 并 利用 
mixture coefficient 和 edge probability 之间 的 对偶 关系 建立 
了 一种 新的 message passing 的 方法 这种 方法 是 
对 belief propagation 的 推广 Jason Johnson 等 人在 2005年 
建立 的 walk sum analysis 为 高斯 马尔可夫 随机 场上 
的 belief propagation 提供 了 系统 的 分析 方法 这种 
方法 成功 刻画 了 belief propagation 在 高斯 场上 的 
收敛 条件 也是 后来 提出 的 多种 改进型 的 belief 
propagation 的 理论 依据 Thomas Minka 在 他 PhD 期间 
所 建立 的 expectation propagation 也是 belief propagation 的 在 
一般 Graphical Model 上 的 重要 推广 3 蒙特卡罗 采样 
Monte Carlo sampling 与 基于 优化 的 方法 不同 蒙特卡罗 
方法 通过 对 概率模型 的 随机 模拟 运行 来 收集 
样本 然后 通过 收集 到 的 样本 来 估计 变量 
的 统计 特性 比如 均值 采样 方法 有三个/nr 方面 的 
重要 优点 第一 它 提供 了 一种 有 严谨 数学 
基础 的 方法 来 逼近 概率 计算 中 经常 出现 
的 积分 积分 计算 的 复杂度 随着 空间维度 的 提高 
呈几何 增长 第二 采样 过程 最终 获得 的 是 整个 
联合 分布 的 样本 集 而 不仅仅 是 对 某些 
参数 或者 变量值 的 最优 估计 这个 样 本集 近似 
地 提供 了 对 整个 分布 的 更 全面 的 
刻画 比如 你 可以 计算 任意 两个 变量 的 相关 
系数 第三 它 的 渐近 特性 通常 可以 被 严格 
证明 对于 复杂 的 模型 由 variational inference 或者 belief 
propagation 所 获得 的 解 一般 并 不能 保证 是 
对 问题 的 全局 最优 解 在 大 部分 情况 
下 甚至/d 无法/n 了解/v 它/r 和/c 最优/d 解的/nr 距离/n 有/v 
多远/i 如果 使用 采样 只要 时间 足够 长 是 可以 
任意 逼近 真实 的 分布 的 而且 采样 过程 的 
复杂度 往往 较为 容易 获得 理论上 的 保证 蒙特卡罗 方法 
本身 也 是 现代 统计学 中 一个 非常 重要 的 
分支 对 它 的 研究 在 过去 几十年 来 一直 
非常 活跃 在 机器学习 领域 中 常见 的 采样 方法 
包括 Gibbs Sampling Metropolis Hasting Sampling M H   Importance 
Sampling Slice Sampling 以及 Hamiltonian Monte Carlo 其中 Gibbs Sampling 
由于 可以 纳入 M H 方法 中 解释 而 通常 
被 视为 M H 的 特例 虽然 它们 最初 的 
motivation 是 不 一样 的 Graphical Model 以及 与 它 
相关 的 probabilistic inference 是 一个 非常 博大 的 领域 
远非 本文 所能 涵盖 在 这篇文章 中 我 只能 蜻蜓点水 
般 地 介绍 了 其中 一些 我 较为 熟悉 的 
方面 希望/v 能给在/nr 这/r 方面/n 有/v 兴趣/n 的/uj 朋友/n 一点/m 
参考/v ./i 非/h 参数/n 化/n 的/uj 模型/n 还有/v 其它/r 哪些/r 
优势/n 林达 华 老师 非 参数 化 模型 确实 引入 
了 其它 参数 比如 concentration parameter 但是 这个 参数 和 
component 的 个数 在 实用 中 是 有着 不同 的 
影响 的 concentration parameter 主要 传达 的 是 使用者 希望 
形成 的 聚 类 粒度 举个 简单 的 例子 比如 
一组 数据 存在 3个 大类 每个 大类 中有 3个 相对 
靠近 的 子类 这种 情况 下 聚 成 3类 或者 
9类 都是 合理 的 解 如果 concentration parameter 设得 比较 
大 最后 的 结果 可能 形成 9类 如果 设得 比较 
小 则 可能 形成 3类 但是 如果 人为地 固定 类 
数 则 很可能 导致 不 合理 的 结果 需要 强调 
的 是非 参数 化 贝叶斯 方法 是 一个 非常 博大 
的 方向 目前 的 研究 只 是 处于 起步 阶段 
而 Dirichlet Process mixture model 只 是非 参数 方法 的 
一个 具体 应用 事实上 DP 像 Gauss distribution 一样 都是/nr 
一种 有着 良好 数学 性质 的 过程 分布 但是 它们 
在 实用 中都 过于 理想 化了 目前 的 一个 新的 
研究 方向 就是 建立 更为 贴近 实际 的 非 参数 
化 过程 相比 于 传统 参数 化 方法 而言 非 
参数 化 方法 的 主要 优势 是 允许 模型 的 
结构 在 学习 的 过程 中 动态变化 而 不仅仅 是 
组件 的 数量 这种 灵活性 对于 描述 处于 不断 变化 
中 的 数据 非常 重要 当然 如何 在 更 复杂 
的 模型 中 应用 非 参数 化 方法 是 一个 
比较 新的 课题 有 很多 值得 进一步 探索 的 地方 
「 SIGVC BBS 」 文中 后面 提到 的 结构 学习 
是不是 这 两年 比较 火 的 Structured Output Prediction 呢 
他们 的 关系 如何 Structured Percepton 和 Structured SVM 应该 
就是 属于 这个 大类 吗 结构 学习 的 输出 是 
树结 构和 图 结构 吗 结构 学习 与 图像 的 
层次 分割 或者 层次 聚 类有 关系 吗 林达 华 
老师 Structured Prediction e . g . Structured SVM 其实 
属于 利用 结构 而 不是 我 在 文中 所指 结构 
学习 在 大部分 Structured Prediction 的 应用 中 结构 是 
预先 固定 的 比如 哪些 变量 要用 potential 联系 在 
一起 学习 的 过程 其实 只是 优化 待定 的 参数 
尽管如此 这些 工作 本身 是 非常 有 价值 的 在 
很多 问题 中 都 取得 了 不错 的 效果 我 
在 文中 所 提到 的 结构 学习 是 指 连 
结构 本身 都是 不 固定 的 需要 从 数据 中 
去 学习 一般 情况 下 学习 输出 的 是 图 
或者 树 的 结构 以及 相关 参数 这个 topic 其实 
历史 很 长了 早期 的 代表 性 工作 就是 chow 
liu tree 这 是 一种 利用 信息 量计算 寻找 最 
优树 结构 来 描述 数据 的 算法 Alan Willsky 的 
小组 近几年 在 这个 方向 取得 了 很多 进展 但是 
总体而言 这个 方向 仍旧 非常 困难 大 部分 工作 属于 
探索性 的 并不 特别 成熟 目前 在 Vision 中的 应用 
不是 特别 广泛 但是 我 相信 随着 一些 方法 逐步 
成熟 进入 实用阶段 它 的 应用 前景 是 非常 不错 
的 「 SIGVC BBS 」 文中 提到 了 Convolutional Deep 
Network Deep Belief Network Deep Boltzmann Machine 等 近年 炙手可热 
的 神经 网络 方法 那么 神经 网络 和 概率 图 
模型 是不是 本质 上 完全 是 一回 事 只是 观察 
角度 和 历史 发展 不同 感觉 它 们 很多 地方 
都很/nr 相似 深度/ns 学习/v 里/f RBM/w 学习/v 的/uj 训练/vn 算法/n 
与/p 概率/n 图/n 模型/n 的/uj 学习/v 推理/v 算法/n 有/v 什么/r 
联系/n 和/c 区别/n 吗/y 他们/r 的/uj 结构/n 模型/n 有/v 什么/r 
联系/n 和/c 区别/n 吗/y 林达 华 老师 这 两类 模型 
所 使用 的 数学 方法 是 非常 不同 的 Graphical 
model/w 的/uj 很多/m 推断/v 和/c 学习/v 方法/n 都有/nr 很深/i 的/uj 
数学/n 根基/n 通过 近 十 几年 的 努力 大家 已经 
逐步 建立 起 整套 的 方法论 体系 对 相关 算法 
进行 分析 Deep Learning 目前 并 没有 什么 有效 的 
分析 方法 Deep learning 取得 很好 的 性能 其中 很多 
技巧性 的 方法 trick 起到 了 重要 作用 至于 为什么 
这些 trick 能 导致 更好 的 性能 目前 还 未能 
有 一个 很好 的 解释 我 个人 看来 这些 技巧 
其实 是 很 有价值 的 一方面 它们 确实 在 实践 
中 提高 了 性能 另外 一方面 它们 为 理论上 的 
探索 提出 了 问题 但是 我 觉得 有效 回答 这些 
问题 需要 新的 数学 工具 新的 数学 分析方法 这 看来 
不是 近期内 能 做到 的 「 SIGVC BBS 」 在 
一些 论文 中 看到 采样 的 方法 如 Gibbs 采样 
也 有其 缺点 一个 是 计算 量 比较 大 computationally 
intensive 另 一个 是 收敛 检测 比较 难 不 知道 
这些 说法 是否 有 道理 或者 目前 这些 问题 是否 
有 得到解决 林达 华 老师 这里 提到 的 两个 问题 
确实 是 Sampling 的 两个 主要 的 困难 对于 这些 
问题 过去 几十 年 取得 了 很多 进展 提出 了 
很多 新的 采 样方法 但是 困难 仍然 很大 但是 采样 
能 提供 整个 分布 的 信息 而且有 渐近 asymptotic 的 
理论 保证 这 在 很多 情况下 是 一般 的 optimization 
方法 做 不到 的 最近/f 有/v 新的/i 研究/vn 尝试/vn 结合/v 
Sampling/w 和/c Optimization/w 在 特定 问题 上 有 一些 有趣 
的 结果 比如 George Papandreou 的 Perturb and MAP . 
「 SIGVC BBS 」 在 计算机 视觉 中 视觉 目标 
跟踪 问题 已经 用 到了 动态 贝叶斯 网络 方法 一些 
最近 发表 的 自然 图像 分割 方法 也 用到 LDA 
Latent Dirichlet Allocation 在 受限 的 理想 数据 条件下 这些 
方法 都 取得 了 较好 的 结果 但是 不得不 承认 
我们 在 研究 和 应用 的 过程 中 在 心理 
上 首先 对 应用 概率 图 模型 有所 畏惧 这里 
除 我们 已经 用 得 较多 较 熟悉 的 MRF 
CRF 和 Dynamic Bayesian network based visual tracking condensation 之外 
主要 的 解释 可能有 一方面 它 不象 很多 正则化 方法 
那样 其 细节 能被 自我 掌握 观测 和 控制 另一方面 
对于 一个 新的 问题 我们 需要 不停 地 问自己 什么样 
的 设计 图 是 最好 的 从而 在 很多 情况 
下 我们 更 愿意 选择 使用 那些 正则化 方法 比如 
对 小规模 人脸识别 我们 会 选择 PCA ＋ LAD SVM 
对 大 一点 的 规模 我们 会 考虑 特征选择 ＋ 
adaboost 框架 就 计算机 视觉 能否 从 实践 的 角度 
给 我们 一点 关于 使用 概率 图 模型 的 建议 
另外 在 计算机 视觉 中 什么样 的 问题 更 适合 
于 采用 概率 图 模型 方法 来 解决 林达 华 
老师 首先 Graphical model 和 其它 的 方法 一样 只是 
一种 数学 工具 对于 解决 问题 而言 最 重要 的 
是 选择 合适 的 工具 而不 一定 要 选 看上去 
高深 的 方法 对于 普通 的 分类 问题 传统 的 
SVM Boost 仍 不失为 最 有效 的 方法 Graphical model 
通常 应用 在 问题 本身 带有 多个 相互 联系 的 
变量 的 时候 这个 时候 Graphical model 提供 了 一种 
表达 方式 让 你 去 表达 这些 联系 我 觉得 
并不 必要 去 寻求 最优 的 设计图 事实上 没有 人 
知道 什么样 的 图 才是 最优 的 实践 中 我们 
通常 是 根据 问题 本身 建立 一个 能 比较 自然 
地 表达 问题 结构 的 图 然后 通过 实验 了 
验证 这个 图 是不是 合适 的 如果 不 合适 可以 
根据 结果 分析 原因 对 图 做出 修正 举个 具体 
的 例子 比如 对 一个 比赛 视频 进行 分析 那么 
可能 涉及 多个 变量 摄像机 的 角度 背景 运动员 的 
动作 等等 那么 这个 问题 可能 就 设计 多 个 
未知 变量 的 推断 这些 变量 间 可能 存在 各种 
联系 这个 时候 Graphical model 可能 就是 一种 合适 的 
选择 值得 注意 的 是 选择 合适 的 图 有时候 
也 需要 一些 经验 比如 分布 的 选择 上 要注意 
形成 conjugate 这样 往往 容易 得到 简易 的 推断 公式 
了解 各种 分布 的 特性 以及 它们 可能 对 最后 
结果 的 影响 也是 有 帮助 的 