1. 线性代数 (Linear Algebra)：我想国内的大学生都会学过这门课程，但是，未必每一位老师都能贯彻它的精要。这门学科对于Learning是必备的基础，对它的透彻掌握是必不可少的。我在科大一年级的时候就学习了这门课，后来到了香港后，又重新把线性代数读了一遍，所读的是Introduction to Linear Algebra (3rd Ed.)  by Gilbert Strang.这本书是MIT的线性代数课使用的教材，也是被很多其它大学选用的经典教材。它的难度适中，讲解清晰，重要的是对许多核心的概念讨论得比较透彻。我个人觉得，学习线性代数，最重要的不是去熟练矩阵运算和解方程的方法——这些在实际工作中MATLAB可以代劳，关键的是要深入理解几个基础而又重要的概念：子空间(Subspace)，正交(Orthogonality)，特征值和特征向量(Eigenvalues and eigenvectors)，和线性变换(Linear transform)。从我的角度看来，一本线代教科书的质量，就在于它能否给这些根本概念以足够的重视，能否把它们的联系讲清楚。Strang的这本书在这方面是做得很好的。而且，这本书有个得天独厚的优势。书的作者长期在MIT讲授线性代数课(18.06)，课程的video在MIT的Open courseware网站上有提供。有时间的朋友可以一边看着名师授课的录像，一边对照课本学习或者复习。http://ocw.mit.edu/OcwWeb/Mathematics/18-06Spring-2005/CourseHome/index.htm2. 概率和统计 (Probability and Statistics):概率论和统计的入门教科书很多，我目前也没有特别的推荐。我在这里想介绍的是一本关于多元统计的基础教科书：Applied Multivariate Statistical Analysis (5th Ed.)  by Richard A. Johnson and Dean W. Wichern这本书是我在刚接触向量统计的时候用于学习的，我在香港时做研究的基础就是从此打下了。实验室的一些同学也借用这本书学习向量统计。这本书没有特别追求数学上的深度，而是以通俗易懂的方式讲述主要的基本概念，读起来很舒服，内容也很实用。对于Linear regression, factor analysis, principal component analysis (PCA), and canonical component analysis (CCA)这些Learning中的基本方法也展开了初步的论述。之后就可以进一步深入学习贝叶斯统计和Graphical models。一本理想的书是Introduction to Graphical Models (draft version).  by M. Jordan and C. Bishop.我不知道这本书是不是已经出版了（不要和Learning in Graphical Models混淆，那是个论文集，不适合初学）。这本书从基本的贝叶斯统计模型出发一直深入到复杂的统计网络的估计和推断，深入浅出，statistical learning的许多重要方面都在此书有清楚论述和详细讲解。MIT内部可以access，至于外面，好像也是有电子版的。3. 分析 (Analysis)：我想大家基本都在大学就学过微积分或者数学分析，深度和广度则随各个学校而异了。这个领域是很多学科的基础，值得推荐的教科书莫过于Principles of Mathematical Analysis, by Walter Rudin有点老，但是绝对经典，深入透彻。缺点就是比较艰深——这是Rudin的书的一贯风格，适合于有一定基础后回头去看。在分析这个方向，接下来就是泛函分析(Functional Analysis)。Introductory Functional Analysis with Applications, by Erwin Kreyszig.适合作为泛函的基础教材，容易切入而不失全面。我特别喜欢它对于谱论和算子理论的特别关注，这对于做learning的研究是特别重要的。Rudin也有一本关于functional analysis的书，那本书在数学上可能更为深刻，但是不易于上手，所讲内容和learning的切合度不如此书。在分析这个方向，还有一个重要的学科是测度理论(Measure theory)，但是我看过的书里面目前还没有感觉有特别值得介绍的。4. 拓扑 (Topology)：在我读过的基本拓扑书各有特色，但是综合而言，我最推崇：Topology (2nd Ed.)  by James Munkres这本书是Munkres教授长期执教MIT拓扑课的心血所凝。对于一般拓扑学(General topology)有全面介绍，而对于代数拓扑(Algebraic topology)也有适度的探讨。此书不需要特别的数学知识就可以开始学习，由浅入深，从最基本的集合论概念（很多书不屑讲这个）到Nagata-Smirnov Theorem和Tychonoff theorem等较深的定理（很多书避开了这个）都覆盖了。讲述方式思想性很强，对于很多定理，除了给出证明过程和引导你思考其背后的原理脉络，很多令人赞叹的亮点——我常读得忘却饥饿，不愿释手。很多习题很有水平。5. 流形理论 (Manifold theory)：对于拓扑和分析有一定把握时，方可开始学习流形理论，否则所学只能流于浮浅。我所使用的书是Introduction to Smooth Manifolds.  by John M. Lee虽然书名有introduction这个单词，但是实际上此书涉入很深，除了讲授了基本的manifold, tangent space,bundle, sub-manifold等，还探讨了诸如纲理论(Category theory)，德拉姆上同调(De Rham cohomology)和积分流形等一些比较高级的专题。对于李群和李代数也有相当多的讨论。行文通俗而又不失严谨，不过对某些记号方式需要熟悉一下。虽然李群论是建基于平滑流形的概念之上，不过，也可能从矩阵出发直接学习李群和李代数——这种方法对于急需使用李群论解决问题的朋友可能更加实用。而且，对于一个问题从不同角度看待也利于加深理解。下面一本书就是这个方向的典范：Lie Groups, Lie Algebras, and Representations: An Elementary Introduction.  by Brian C. Hall此书从开始即从矩阵切入，从代数而非几何角度引入矩阵李群的概念。并通过定义运算的方式建立exponential mapping，并就此引入李代数。这种方式比起传统的通过“左不变向量场(Left-invariant vector field)“的方式定义李代数更容易为人所接受，也更容易揭示李代数的意义。最后，也有专门的论述把这种新的定义方式和传统方式联系起来。————————————————————————————无论是研究Vision, Learning还是其它别的学科，数学终究是根基所在。学好数学是做好研究的基石。学好数学的关键归根结底是自己的努力，但是选择一本好的书还是大有益处的。不同的人有不同的知识背景，思维习惯和研究方向，因此书的选择也因人而异，只求适合自己，不必强求一致。上面的书仅仅是从我个人角度的出发介绍的，我的阅读经历实在非常有限，很可能还有比它们更好的书（不妨也告知我一声，先说声谢谢了）。％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％％Learning中的代数结构的建立Learning是一个融会多种数学于一体的领域。说起与此有关的数学学科，我们可能会迅速联想到线性代数以及建立在向量空间基础上的统计模型——事实上，主流的论文中确实在很大程度上基于它们。R^n (n-维实向量空间) 是我们在paper中见到最多的空间，它确实非常重要和实用，但是，仅仅依靠它来描述我们的世界并不足够。事实上，数学家们给我们提供了丰富得多的工具。“空间”(space)，这是一个很有意思的名词，几乎出现在所有的数学分支的基础定义之中。归纳起来，所谓空间就是指一个集合以及在上面定义的某种数学结构。关于这个数学结构的定义或者公理，就成为这个数学分支的基础，一切由此而展开。还是从我们最熟悉的空间——R^n 说起吧。大家平常使用这个空间的时候，除了线性运算，其实还用到了别的数学结构，包括度量结构和内积结构。·                   第一，它是一个拓扑空间(Topological space)。而且从拓扑学的角度看，具有非常优良的性质：Normal (implying Hausdorff and Regular), Locally Compact, Paracompact, with Countable basis, Simply connected (implying connected and path connected),Metrizable.·                   第二，它是一个度量空间(Metric space)。我们可以计算上面任意两点的距离。·                   第三，它是一个有限维向量空间(Finite dimensional space)。因此，我们可以对里面的元素进行代数运算（加法和数乘），我们还可以赋予它一组有限的基，从而可以用有限维坐标表达每个元素。·                   第四，基于度量结构和线性运算结构，可以建立起分析(Analysis)体系。我们可以对连续函数进行微分，积分，建立和求解微分方程，以及进行傅立叶变换和小波分析。·                   第五，它是一个希尔伯特空间（也就是完备的内积空间）(Hilbert space, Complete inner product space)。它有一套很方便计算的内积(inner product)结构——这个空间的度量结构其实就是从其内积结构诱导出来。更重要的，它是完备的(Complete)——代表任何一个柯西序列(Cauchy sequence)都有极限——很多人有意无意中其实用到了这个特性，不过习惯性地认为是理所当然了。·                   第六，它上面的线性映射构成的算子空间仍旧是有限维的——一个非常重要的好处就是，所有的线性映射都可以用矩阵唯一表示。特别的，因为它是有限维完备空间，它的泛函空间和它本身是同构的，也是R^n。因而，它们的谱结构，也就可以通过矩阵的特征值和特征向量获得。·                   第七，它是一个测度空间——可以计算子集的大小（面积/体积）。正因为此，我们才可能在上面建立概率分布(distribution)——这是我们接触的绝大多数连续统计模型的基础。我们可以看到，这是一个非常完美的空间，为我们的应用在数学上提供了一切的方便，在上面，我们可以理所当然地认为它具有我们希望的各种良好性质，而无须特别的证明；我们可以直接使用它的各种运算结构，而不需要从头建立；而且很多本来不一样的概念在这里变成等价的了，我们因此不再需要辨明它们的区别。以此为界，Learning的主要工作分成两个大的范畴：1.    建立一种表达形式，让它处于上面讨论的R^n空间里面。2.    获得了有限维向量表达后，建立各种代数算法或者统计模型进行分析和处理。这里只讨论第一个范畴。先看看，目前用得比较广泛的一些方法：1.    直接基于原始数据建立表达。我们关心的最终目标是一个个现实世界中的对象：一幅图片，一段语音，一篇文章，一条交易记录，等等。这些东西大部分本身没有附着一个数值向量的。为了构造一个向量表达，我们可以把传感器中记录的数值，或者别的什么方式收集的数值数据按照一定的顺序罗列出来，就形成一个向量了。如果有n个数字，就认为它们在R^n里面。不过，这在数学上有一点小问题，在大部分情况下，根据数据产生的物理原理，这些向量的值域并不能充满整个空间。比如图像的像素值一般是正值，而且在一个有界闭集之中。这带来的问题是，对它们进行线性运算很可能得到的结果会溢出正常的范围——在大部分paper中，可能只是采用某些heuristics的手段进行简单处理，或者根本不管，很少见到在数学上对此进行深入探讨的——不过如果能解决实际问题，这也是无可厚非的，毕竟不是所有的工作都需要像纯数学那样追求严谨。2.    量化(quantization)。这是在处理连续信号时被广泛采用的方式。只是习以为常，一般不提名字而已。比如一个空间信号（Vision中的image）或者时间信号，它们的domain中的值是不可数无限大的(uncountably infinite)，不要说表示为有限维向量，即使表达为无限序列也是不可能的。在这种情况下，一般在有限域内，按照一定顺序每隔一定距离取一个点来代表其周围的点，从而形成有限维的表达。这就是信号在时域或空域的量化。这样做不可避免要丢失信息。但是，由于小邻域内信号的高度相关，信息丢失的程度往往并不显著。而且，从理论上说，这相当于在频域中的低通过率。对于有限能量的连续信号，不可能在无限高的频域中依然保持足够的强度，只要采样密度足够，丢失的东西可以任意的少。除了表示信号，对于几何形体的表达也经常使用量化，比如表示curve和surface。3.    找出有限个数充分表达一个对象也许不是最困难的。不过,在其上面建立数学结构却未必了。一般来说，我们要对其进行处理，首先需要一个拓扑结构用以描述空间上的点是如何联系在一起。直接建立拓扑结构在数学上往往非常困难，也未必实用。因此，绝大部分工作采取的方式是首先建立度量结构。一个度量空间，其度量会自然地诱导出一个拓扑结构——不过，很多情况下我们似乎会无视它的存在。最简单的情况，就是使用原始向量表达的欧氏距离(Euclidean distance)作为metric。不过，由于原始表达数值的不同特性，这种方式效果一般不是特别好，未必能有效表达实际对象的相似性（或者不相似性）。因此，很多工作会有再此基础上进行度量的二次建立。方式是多种多样的，一种是寻求一个映射，把原空间的元素变换到一个新的空间，在那里欧氏距离变得更加合适。这个映射发挥的作用包括对信息进行筛选，整合，对某些部分进行加强或者抑制。这就是大部分关于feature selection，feature extraction，或者subspace learning的文章所要做的。另外一种方式，就是直接调节距离的计算方式（有些文章称之为metric learning）。这两种方式未必是不同的。如果映射是单射，那么它相当于在原空间建立了一个不同的度量。反过来，通过改变距离计算方式建立的度量在特定的条件下对应于某种映射。4.    大家可能注意到，上面提到的度量建立方法，比如欧氏距离，它需要对元素进行代数运算。对于普通的向量空间，线性运算是天然赋予的，我们无须专门建立，所以可以直接进行度量的构造——这也是大部分工作的基础。可是，有些事物其原始表达不是一个n-tuple，它可能是一个set，一个graph，或者别的什么特别的object。怎么建立代数运算呢？一种方法是直接建立。就是给这些东西定义自己的加法和数乘。这往往不是那么直接（能很容易建立的线性运算结构早已经被建立好并广泛应用了），可能需要涉及很深的数学知识，并且要有对问题本身的深入了解和数学上的洞察力。不过，一个新的代数结构一旦建立起来，其它的数学结构，包括拓扑，度量，分析，以及内积结构也随之能被自然地诱导出来，我们也就具有了对这个对象空间进行各种数学运算和操作的基础。加法和数乘看上去简单，但是如果我们对于本来不知道如何进行加法和数乘的空间建立了这两样东西，其理论上的贡献是非常大的。（一个小问题：大家常用各种graphical model，但是，每次这些model都是分别formulate，然后推导出estimation和evaluation的步骤方法。是否可能对"the space of graphical model"或者它的某个特定子集建立某种代数结构呢？（不一定是线性空间，比如群，环，广群， etc）从而使得它们在代数意义上统一起来，而相应的estimation或者evaluation也可以用过代数运算derive。这不是我的研究范围，也超出了我目前的能力和知识水平，只是我相信它在理论上的重要意义，留作一个远景的问题。事实上，数学中确实有一个分支叫做 Algebraic statistics 可能在探讨类似的问题，不过我现在对此了解非常有限。）5.    回到我们的正题，除了直接建立运算定义，另外一种方式就是嵌入(embedding)到某个向量空间，从而继承其运算结构为我所用。当然这种嵌入也不是乱来，它需要保持原来这些对象的某种关系。最常见的就是保距嵌入(isometric embedding)，我们首先建立度量结构（绕过向量表达，直接对两个对象的距离通过某种方法进行计算），然后把这个空间嵌入到目标空间，通常是有限维向量空间，要求保持度量不变。“嵌入”是一种在数学上应用广泛的手段，其主要目标就是通过嵌入到一个属性良好，结构丰富的空间，从而利用其某种结构或者运算体系。在拓扑学中，嵌入到metric space是对某个拓扑空间建立度量的重要手段。而在这里，我们是已有度量的情况下，通过嵌入获取线性运算的结构。除此以来，还有一种就是前些年比较热的manifold embedding，这个是通过保持局部结构的嵌入，获取全局结构，后面还会提到。6.    接下来的一个重要的代数结构，就是内积(inner product)结构。内积结构一旦建立，会直接诱导出一种性质良好的度量，就是范数(norm)，并且进而诱导出拓扑结构。一般来说，内积需要建立在线性空间的基础上，否则连一个二元运算是否是内积都无法验证。不过，kernel理论指出，对于一个空间，只要定义一个正定核(positive kernel)——一个符合正定条件的二元运算，就必然存在一个希尔伯特空间，其内积运算等效于核运算。这个结论的重要意义在于，我们可以绕开线性空间，通过首先定义kernel的方式，诱导出一个线性空间(叫做再生核希尔伯特空间 Reproducing Kernel Hilbert Space)，从而我们就自然获得我们所需要的度量结构和线性运算结构。这是kernel theory的基础。在很多教科书中，以二次核为例子，把二维空间变成三维，然后告诉大家kernel用于升维。对于这种说法，我一直认为在一定程度上是误导的。事实上，kernel的最首要意义是内积的建立（或者改造），从而诱导出更利于表达的度量和运算结构。对于一个问题而言，选择一个切合问题的kernel比起关注“升维”来得更为重要。kernel被视为非线性化的重要手段，用于处理非高斯的数据分布。这是有道理的。通过nonlinear kernel改造的内积空间，其结构和原空间的结构确实不是线性关联，从这个意义上说，它实施了非线性化。不过，我们还应该明白，它的最终目标还是要回到线性空间，新的内积空间仍旧是一个线性空间，它一旦建立，其后的运算都是线性的，因此，kernel的使用就是为了寻求一个新的线性空间，使得线性运算更加合理——非线性化的改造最终仍旧是要为线性运算服务。值得一提的是，kernelization本质上说还是一种嵌入过程：对于一个空间先建立内积结构，并且以保持内积结构不变的方式嵌入到一个高维的线性空间，从而继承其线性运算体系。7.    上面说到的都是从全局的方式建立代数结构的过程，但是那必须以某种全局结构为基础（无论预先定义的是运算，度量还是内积，都必须适用于全空间。）但是，全局结构未必存在或者适合，而局部结构往往简单方便得多。这里就形成一种策略，以局部而达全局——这就是流形(manifold)的思想，而其则根源于拓扑学。从拓扑学的角度说，流形就是一个非常优良的拓扑空间：符合Hausdorff分离公理（任何不同的两点都可以通过不相交的邻域分离），符合第二可数公理（具有可数的拓扑基），并且更重要的是，局部同胚于R^n。因此，一个正则(Regular)流形基本就具有了各种最良好的拓扑特性。而局部同胚于R^n，代表了它至少在局部上可以继承R^n的各种结构，比如线性运算和内积，从而建立分析体系。事实上，拓扑流形继承这些结构后形成的体系，正是现代流形理论研究的重点。继承了分析体系的流形，就形成了微分流形(Differential manifold)，这是现代微分几何的核心。而微分流形各点上的切空间(Tangent Space)，则获得了线性运算的体系。而进一步继承了局部内积结构的流形，则形成黎曼流形(Riemann manifold)，而流形的全局度量体系——测地距离(geodesics)正是通过对局部度量的延伸来获得。进一步的，当流行本身的拓扑结构和切空间上的线性结构发生关系——也就获得一簇拓扑关联的线性空间——向量丛(Vector bundle)。虽然manifold theory作为现代几何学的核心，是一个博大精深的领域，但是它在learning中的应用则显得非常狭窄。事实上，对于manifold，很多做learning的朋友首先反应的是ISOMAP, LLE, eigenmap之类的算法。这些都属于embedding。当然，这确实是流形理论的一个重要方面。严格来说，这要求是从原空间到其映像的微分同胚映射，因此，嵌入后的空间在局部上具有相同的分析结构，同时也获得了各种好处——全局的线性运算和度量。不过，这个概念在learning的应用中被相当程度的放宽了——微分同胚并不能被完全保证，而整个分析结构也不能被完全保持。大家更关注的是保持局部结构中的某个方面——不过这在实际应用中的折衷方案也是可以理解的。事实表明，当原空间中的数据足够密集的情况下，这些算法工作良好。Learning中流形应用的真正问题在于它被过滥地运用于稀疏空间(Sparse space)，事实上在高维空间中撒进去几千乃至几十万点，即使最相邻的几点也难称为局部了，局部的范围和全局的范围其实已经没有了根本差别，连局部的概念都立不住脚的时候，后面基于其展开的一切工作也都没有太大的意义。事实上，稀疏空间有其本身的规律和法则，通过局部形成全局的流形思想从本质上是不适合于此的。虽然，流形是一种非常美的理论，但是再漂亮的理论也需要用得其所——它应该用于解决具有密集数据分布的低维空间。至于，一些paper所报告的在高维空间（比如人脸）运用流形方法获得性能提升，其实未必是因为“流形”本身所起的作用，而很可能是其它方面的因素。8.    流形在实际应用中起重要作用的还有两个方面：一个是研究几何形体的性质（我们暂且不谈这个），还有就是它和代数结构的结合形成的李群(Lie group)和李代数(Lie algebra)。当我们研究的对象是变换本身的时候，它们构成的空间是有其特殊性的，比如所有子空间投影形成了Grassmann流形，所有的可逆线性算子，或者仿射算子，也形成各自的流形。对他们的最重要操作是变换的结合，而不是加法数乘，因此，它们上面定义的更合适的代数结构应该是群和不是线性空间。而群和微分流形的结合体——李群则成为它们最合适的描述体系——而其切空间则构成了一种加强的线性空间：李代数，用于描述其局部变化特性。李代数和李群的关系是非常漂亮的。它把变换的微变化转换成了线性空间的代数运算，使得移植传统的基于线性空间的模型和算法到李空间变得可能。而且李代数中的矩阵比起变换本身的矩阵甚至更能反映变换的特性。几何变换的李代数矩阵的谱结构就能非常方便地用于分析变换的几何特性。最后，回头总结一下关于嵌入这个应用广泛的策略，在learning中的isometry, kernel和manifold embedding都属于此范畴，它们分别通过保持原空间的度量结构，内积结构和局部结构来获得到目标（通常是向量空间）的嵌入，从而获得全局的坐标表达，线性运算和度量，进而能被各种线性算法和模型所应用。在获得这一系列好处的同时，也有值得我们注意的地方。首先，嵌入只是一种数学手段，并不能取代对问题本身的研究和分析。一种不恰当的原始结构或者嵌入策略，很多时候甚至适得其反——比如稀疏空间的流形嵌入，或者选取不恰当的kernel。另外，嵌入适合于分析，而未必适合于重建或者合成。这是因为嵌入是一个单射(injection)，目标空间不是每一个点都和原空间能有效对应的。嵌入之后的运算往往就打破了原空间施加的限制。比如两个元素即使都是从原空间映射过来，它们的和却未必有原像，这时就不能直接地回到原空间了。当然可以考虑在原空间找一个点它的映射与之最近，不过这在实际中的有效性是值得商榷的。和Learning有关的数学世界是非常广博的，我随着学习和研究的深入，越来越发现在一些我平常不注意的数学分支中有着适合于问题的结构和方法。比如，广群(groupoid)和广代数(algebroid)能克服李群和李代数在表示连续变换过程中的一些困难——这些困难困扰了我很长时间。解决问题和建立数学模型是相辅相成的，一方面，一个清晰的问题将使我们有明确的目标去寻求合适的数学结构，另一方面，对数学结构的深入理解对于指导问题的解决也是有重要作用的。对于解决一个问题来说，数学工具的选择最重要的是适合，而不是高深，但是如果在现有数学方法陷入困难的时候，寻求更高级别的数学的帮助，往往能柳暗花明。数学家长时间的努力解决的很多问题，并不都是理论游戏，他们的解决方案中很多时候蕴含着我们需要的东西，而且可能导致对更多问题的解决——但是我们需要时间去学习和发现它们。拓扑：游走于直观与抽象之间近日来，抽空再读了一遍点集拓扑(Point Set Topology)，这是我第三次重新学习这个理论了。我看电视剧和小说，极少能有兴致看第二遍，但是，对于数学，每看一次都有新的启发和收获。代数，分析，和拓扑，被称为是现代数学的三大柱石。最初读拓扑，是在两三年前，由于学习流形理论的需要。可是，随着知识的积累，发现它是很多理论的根基。可以说，没有拓扑，就没有现代意义的分析与几何。我们在各种数学分支中接触到的最基本的概念，比如，极限，连续，距离（度量），边界，路径，在现代数学中，都源于拓扑。拓扑学是一门非常奇妙的学科，它把最直观的现象和最抽象的概念联系在一起了。拓扑描述的是普遍使用的概念（比如开集，闭集，连续），我们对这些概念习以为常，理所当然地使用着，可是，真要定义它，则需要对它们本质的最深刻的洞察。数学家们经过长时间的努力，得到了这些概念的现代定义。这里面很多第一眼看上去，会感觉惊奇——怎么会定义成这个样子。首先是开集。在学习初等数学时，我们都学习开区间 (a, b)。可是，这只是在一条线上的，怎么推广到二维空间，或者更高维空间，或者别的形体上呢？最直观的想法，就是“一个不包含边界的集合”。可是，问题来了，给一个集合，何谓“边界”？在拓扑学里面，开集(Open Set)是最根本的概念，它是定义在集合运算的基础上的。它要求开集符合这样的条件：开集的任意并集和有限交集仍为开集。我最初的时候，对于这样的定义方式，确实百思不解。不过，读下去，看了和做了很多证明后，发现，这样的定义一个很重要的意义在于：它保证了开集中每个点都有一个邻域包含在这个集合内——所有点都和外界（补集）保持距离。这样的理解应该比使用集合运算的定义有更明晰的几何意义。但是，直观的东西不容易直接形成严谨的定义，使用集合运算则更为严格。而集合运算定义中，任意并集的封闭性是对这个几何特点的内在保证。另外一个例子就是“连续函数”(Continuous Function)。在学微积分时，一个耳熟能详的定义是“对任意的epsilon > 0，存在delta > 0，使得。。。。”，背后最直观的意思就是“足够近的点保证映射到任意小的范围内”。可是，epsilon, delta都依赖于实空间，不在实空间的映射又怎么办呢？拓扑的定义是“如果一个映射的值域中任何开集的原象都是开集，那么它连续。”这里就没有epsilon什么事了。“开集的原象是开集”这里的关键在于，在拓扑学中，开集的最重要意义就是要传递“邻域”的意思——开集本身就是所含点的邻域。这样连续定义成这样就顺理成章了。稍微把说法调节一下，上面的定义就变成了“对于f(x)的任意邻域U，都有x的一个邻域V，使得V里面的点都映射到U中。”这里面，我们可以感受到为什么开集在拓扑学中有根本性的意义。既然开集传达“邻域”的意思，那么，它最重要的作用就是要表达哪些点靠得比较近。给出一个拓扑结构，就是要指出哪些是开集，从而指出哪些点靠得比较近，这样就形成了一个聚集结构——这就是拓扑。可是这也可以通过距离来描述，为什么要用开集呢，反而不直观了。某种意义上说，拓扑是“定性”的，距离度量是“定量”的。随着连续变形，距离会不断变化，但是靠近的点还是靠近，因此本身固有的拓扑特性不会改变。拓扑学研究的就是这种本质特性——连续变化中的不变性。在拓扑的基本概念中，最令人费解的，莫过于“紧性”(Compactness)。它描述一个空间或者一个集合“紧不紧”。正式的定义是“如果一个集合的任意开覆盖都有有限子覆盖，那么它是紧的”。乍一看，实在有点莫名其妙。它究竟想描述一个什么东西呢？和“紧”这个形容词又怎么扯上关系呢？一个直观一点的理解，几个集合是“紧”的，就是说，无限个点撒进去，不可能充分散开。无论邻域多么小，必然有一些邻域里面有无限个点。上面关于compactness的这个定义的玄机就在有限和无限的转换中。一个紧的集合，被无限多的小邻域覆盖着，但是，总能找到其中的有限个就能盖全。那么，后果是什么呢？无限个点撒进去，总有一个邻域包着无数个点。邻域们再怎么小都是这样——这就保证了无限序列中存在极限点。Compact这个概念虽然有点不那么直观，可是在分析中有着无比重要的作用。因为它关系到极限的存在性——这是数学分析的基础。了解泛函分析的朋友都知道，序列是否收敛，很多时候就看它了。微积分中，一个重要的定理——有界数列必然包含收敛子列，就是根源于此。在学习拓扑，或者其它现代数学理论之前，我们的数学一直都在有限维欧氏空间之中，那是一个完美的世界，具有一切良好的属性，Hausdorff, Locally compact, Simply connected，Completed，还有一套线性代数结构，还有良好定义的度量，范数，与内积。可是，随着研究的加深，终究还是要走出这个圈子。这个时候，本来理所当然的东西，变得不那么必然了。·                   两个点必然能分开？你要证明空间是Hausdorff的。·                   有界数列必然存在极限点？这只在locally compact的空间如此。·                   一个连续体内任意两点必然有路径连接？这可未必。一切看上去有悖常理，而又确实存在。从线性代数到一般的群，从有限维到无限维，从度量空间到拓扑空间，整个认识都需要重新清理。而且，这些绝非仅是数学家的概念游戏，因为我们的世界不是有限维向量能充分表达的。当我们研究一些不是向量能表达的东西的时候，度量，代数，以及分析的概念，都要重新建立，而起点就在拓扑。和机器学习和计算机视觉相关的数学（转载）（以下转自一位MIT牛人的空间文章，写得很实际：）作者：Dahua感觉数学似乎总是不够的。这些日子为了解决research中的一些问题，又在图书馆捧起了数学的教科书。从大学到现在，课堂上学的和自学的数学其实不算少了，可是在研究的过程中总是发现需要补充新的数学知识。Learning和Vision都是很多种数学的交汇场。看着不同的理论体系的交汇，对于一个researcher来说，往往是非常exciting的enjoyable的事情。不过，这也代表着要充分了解这个领域并且取得有意义的进展是很艰苦的。记得在两年前的一次blog里面，提到过和learning有关的数学。今天看来，我对于数学在这个领域的作用有了新的思考。对于Learning的研究，1、Linear Algebra (线性代数) 和 Statistics (统计学) 是最重要和不可缺少的。这代表了Machine Learning中最主流的两大类方法的基础。一种是以研究函数和变换为重点的代数方法，比如Dimension reduction，feature extraction，Kernel等，一种是以研究统计模型和样本分布为重点的统计方法，比如Graphical model, Information theoretical models等。它们侧重虽有不同，但是常常是共同使用的，对于代数方法，往往需要统计上的解释，对于统计模型，其具体计算则需要代数的帮助。以代数和统计为出发点，继续往深处走，我们会发现需要更多的数学。2、Calculus (微积分)，只是数学分析体系的基础。其基础性作用不言而喻。Learning研究的大部分问题是在连续的度量空间进行的，无论代数还是统计，在研究优化问题的时候，对一个映射的微分或者梯度的分析总是不可避免。而在统计学中，Marginalization和积分更是密不可分——不过，以解析形式把积分导出来的情况则不多见。3、Partial Differential Equation （偏微分方程)，这主要用于描述动态过程，或者仿动态过程。这个学科在Vision中用得比Learning多，主要用于描述连续场的运动或者扩散过程。比如Level set, Optical flow都是这方面的典型例子。4、Functional Analysis (泛函分析)，通俗地，可以理解为微积分从有限维空间到无限维空间的拓展——当然了，它实际上远不止于此。在这个地方，函数以及其所作用的对象之间存在的对偶关系扮演了非常重要的角色。Learning发展至今，也在向无限维延伸——从研究有限维向量的问题到以无限维的函数为研究对象。Kernel Learning 和 Gaussian Process 是其中典型的例子——其中的核心概念都是Kernel。很多做Learning的人把Kernel简单理解为Kernel trick的运用，这就把kernel的意义严重弱化了。在泛函里面，Kernel (Inner Product)是建立整个博大的代数体系的根本，从metric, transform到spectrum都根源于此。5、Measure Theory (测度理论)，这是和实分析关系非常密切的学科。但是测度理论并不限于此。从某种意义上说，Real Analysis可以从Lebesgue Measure（勒贝格测度）推演，不过其实还有很多别的测度体系——概率本身就是一种测度。测度理论对于Learning的意义是根本的，现代统计学整个就是建立在测度理论的基础之上——虽然初级的概率论教科书一般不这样引入。在看一些统计方面的文章的时候，你可能会发现，它们会把统计的公式改用测度来表达，这样做有两个好处：所有的推导和结论不用分别给连续分布和离散分布各自写一遍了，这两种东西都可以用同一的测度形式表达：连续分布的积分基于Lebesgue测度，离散分布的求和基于计数测度，而且还能推广到那种既不连续又不离散的分布中去（这种东西不是数学家的游戏，而是已经在实用的东西，在Dirchlet Process或者Pitman-Yor Process里面会经常看到)。而且，即使是连续积分，如果不是在欧氏空间进行，而是在更一般的拓扑空间（比如微分流形或者变换群），那么传统的黎曼积分（就是大学一年级在微积分课学的那种）就不work了，你可能需要它们的一些推广，比如Haar Measure或者Lebesgue-Stieltjes积分。6、Topology（拓扑学)，这是学术中很基础的学科。它一般不直接提供方法，但是它的很多概念和定理是其它数学分支的基石。看很多别的数学的时候，你会经常接触这样一些概念：Open set / Closed set，set basis，Hausdauf, continuous function，metric space, Cauchy sequence, neighborhood, compactness, connectivity。很多这些也许在大学一年级就学习过一些，当时是基于极限的概念获得的。如果，看过拓扑学之后，对这些概念的认识会有根本性的拓展。比如，连续函数，当时是由epison法定义的，就是无论取多小的正数epsilon，都存在xxx，使得xxx。这是需要一种metric去度量距离的，在general topology里面，对于连续函数的定义连坐标和距离都不需要——如果一个映射使得开集的原像是开集，它就是连续的——至于开集是基于集合论定义的，不是通常的开区间的意思。这只是最简单的例子。当然，我们研究learning也许不需要深究这些数学概念背后的公理体系，但是，打破原来定义的概念的局限在很多问题上是必须的——尤其是当你研究的东西它不是在欧氏空间里面的时候——正交矩阵，变换群，流形，概率分布的空间，都属于此。7、Differential Manifold (微分流形)，通俗地说它研究的是平滑的曲面。一个直接的印象是它是不是可以用来fitting一个surface什么的——当然这算是一种应用，但是这是非常初步的。本质上说，微分流形研究的是平滑的拓扑结构。一个空间构成微分流形的基本要素是局部平滑：从拓扑学来理解，就是它的任意局部都同胚于欧氏空间，从解析的角度来看，就是相容的局部坐标系统。当然，在全局上，它不要求和欧氏空间同胚。它除了可以用于刻画集合上的平滑曲面外，更重要的意义在于，它可以用于研究很多重要的集合。一个n-维线性空间的全部k-维子空间(k8、Lie Group Theory (李群论)，一般意义的群论在Learning中被运用的不是很多，群论在Learning中用得较多的是它的一个重要方向Lie group。定义在平滑流形上的群，并且其群运算是平滑的话，那么这就叫李群。因为Learning和编码不同，更多关注的是连续空间，因为Lie group在各种群中对于Learning特别重要。各种子空间，线性变换，非奇异矩阵都基于通常意义的矩阵乘法构成李群。在李群中的映射，变换，度量，划分等等都对于Learning中代数方法的研究有重要指导意义。9、Graph Theory（图论)，图，由于它在表述各种关系的强大能力以及优雅的理论，高效的算法，越来越受到Learning领域的欢迎。经典图论，在Learning中的一个最重要应用就是graphical models了，它被成功运用于分析统计网络的结构和规划统计推断的流程。Graphical model所取得的成功，图论可谓功不可没。在Vision里面，maxflow (graphcut)算法在图像分割，Stereo还有各种能量优化中也广受应用。另外一个重要的图论分支就是Algebraic graph theory (代数图论)，主要运用于图的谱分析，著名的应用包括Normalized Cut和Spectral Clustering。近年来在semi-supervised learning中受到特别关注。这是大牛们做的很好的综述啊！据说，是MIT一牛人对数学在机器学习中的作用给的评述!via:http://blog.sina.com.cn/s/blog_6833a4df0100nazk.html