机器人视觉中有一项重要人物就是从场景中提取物体的位置，姿态。图像处理算法借助Deep Learning 的东风已经在图像的物体标记领域耍的飞起了。而从三维场景中提取物体还有待研究。目前已有的思路是先提取关键点，再使用各种局部特征描述子对关键点进行描述，最后与待检测物体进行比对，得到点-点的匹配。个别文章在之后还采取了ICP对匹配结果进行优化。对于缺乏表面纹理信息，或局部曲率变化很小，或点云本身就非常稀疏的物体，采用局部特征描述子很难有效的提取到匹配对。所以就有了所谓基于Point Pair 的特征，该特征使用了一些全局的信息来进行匹配，更神奇的是，最终的位姿估计结果并不会陷入局部最小值。详细可参见论文：Model globally, match locally: Efficient and robust 3D object recognition. 与 Going further with point pair features。SLAM的重要研究方向object based Slam 也声称使用了Point Pair Feature进行匹配。为了更好的理解这种方法，而在pcl中也没有找到现成的算法，所以我自己用matlab实现了一遍。算法的思想很简单：0、ppf 特征为[d,<d,n1>,<d,n2>,<n1,n2>].1、针对目标模型，在两两点之间构造点对特征F，如果有N个点，那么就有N*N个特征（说明此算法是O(N2)的），N*N个特征形成特征集F_Set2、在场景中任意取1定点a,再任意取1动点b，构造ppf特征，并从F_set中寻找对应的，那么理想情况下，如果找到了完全匹配的特征，则可获得点云匹配的结果。3、此算法是一种投票算法，每次匹配都能得到一个旋转角度，如果m个b都投票给了某一旋转角度则可认为匹配成功这个算法最大的问题就是不停的采样会导致极大的计算量。不过算法本身确实可以匹配物体和场景。ppf 特征的构建1 function obj = ppf(point1,point2) 2 d = point1.Location - point2.Location; 3 d_unit = d/norm(d); 4 apha1 = acos(point1.Normal*d_unit'); 5 apha2 = acos(point2.Normal*d_unit'); 6 apha3 = acos(point1.Normal*point2.Normal'); 7 obj = [norm(d),apha1,apha2,apha3]; 8 endppf 特征集的构建1 classdef modelFeatureSet < handle 2 %MODELFEATURESET 此处显示有关此类的摘要 3 % 此处显示详细说明 4 5 properties 6 FeatureTree 7 ModelPointCloud 8 Pairs 9 end 10 11 methods 12 function obj = modelFeatureSet(pt) 13 obj.ModelPointCloud = copy(pt.removeInvalidPoints()); 14 end 15 function growTree(self) 16 self.ModelPointCloud = pcdownsample(self.ModelPointCloud,'GridAverage',.1); 17 pt_size = self.ModelPointCloud.Count; 18 idx = repmat(1:pt_size,pt_size,1); 19 tmp1 = reshape(idx,pt_size*pt_size,1); 20 tmp2 = reshape(idx',pt_size*pt_size,1); 21 pairs = [tmp1,tmp2]; 22 rnd = randseed(1,1000,1,1,pt_size*pt_size); 23 pairs = pairs(rnd,:); 24 Features = zeros(size(pairs,1),4); 25 for i = 1:size(pairs,1) 26 Features(i,:) = ppf(self.ModelPointCloud.select(pairs(i,1)),... 27 self.ModelPointCloud.select(pairs(i,2))); 28 end 29 self.FeatureTree = createns(Features); 30 self.Pairs = pairs; 31 end 32 end 33 endView Code