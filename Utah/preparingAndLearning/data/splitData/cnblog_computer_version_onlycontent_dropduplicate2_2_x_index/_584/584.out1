深度 学习 与 计算机 视觉 系列 2 _ 图像 分类 
与 KNN 作者   寒 小阳 时间 2015年 11月 出处 
http / / blog . csdn . net / han 
_ xiaoyang / article / details / 49949535 声明 版权所有 
转载 请 注明 出处 谢谢 1 . 图像 分类 问题 
这是 人 每天 自然而然 会做 的 事情 普 通到 大部分 
时候 我们 都 感知 不到 我们 在 完成 一个 个 
这样 的 任务 早晨 起床 洗漱 你 要 看看 洗漱 
台 一堆 东西 中 哪个 是 杯子 哪个 是 你 
的 牙刷 吃早餐 的 时候 你 要 分辨 食物 和 
碗碟 抽象 一下 对于 一张 输入 的 图片 要 判定 
它 属于 给定 的 一些 标签 / 类别 中的 哪一个 
看似 很 简单 的 一个 问题 这么 多年 却 一直 
是 计算机 视觉 的 一个 核心 问题 应用 场景 也 
很多 它 的 重要性 还 体现 在 其实 其他 的 
一些 计算机 视觉 的 问题 比如说 物体 定位 和 识别 
图像 内容 分割 等 都 可以 基于 它 去 完成 
咱们 举个 例子 从 机器 学习 的 角度 描述 一下 
这个 问题 ^ _ ^ 计算机 拿到 一张 图片 如下 
图 所示 然后 需要 给出 它 对应 { 猫 狗 
帽子 杯子 } 4类 的 概率 人类 是 灰常 牛逼 
的 生物 我们 一瞥 就 知道 这 货 是 猫 
然而 对 计算机 而言 他们 是 没 办法 像人 一样 
看 到 整张 图片 的 对 它 而言 这 是 
一个 3 维 的 大 矩阵 包含 248 * 400个 
像素点 每个 像素点 又有 红绿蓝 RGB 3个 颜色通道 的 值 
每个 值 在 0 黑 255 白 之间 计算机 就 
需要 根据 这 248 * 400 * 3 = 297600个 
数值 去 判定 这 货 是 猫 1.1 图像 识别 
的 难点 图像识别 看似 很 直接 但 实际上 包含 很多 
挑战 人类 可是 经过 数 亿年 的 进化 才 获得 
如此 强大 的 大脑 对于 各种 物体 有着 精准 的 
视觉 理解力 总体而言 我们 想 教 会 计算机 去 认识 
一类 图 会有 下面 这样 一些 困难 视角 不同 每个 
事物 旋转 或者 侧视 最后 的 构图 都 完全 不同 
尺寸 大小 不 统一 相同 内容 的 图片 也 可大可小 
变形 很多 东西 处于 特殊 的 情形 下 会有 特殊 
的 摆放 和 形状 光影 等 干扰 / 幻象 背景 
干扰 同类 内 的 差异 比如 椅子 有 靠椅 / 
吧椅 / 餐椅 / 躺椅 1.2 识别 的 途径 首先 
大家 想想 就 知道 这个 算法 并不 像 对 一个 
数组 排序 或者 求 有向图 的 最短 路径 我们 没 
办法 提前 制定 一个 流程 和 规则 去 解决 定义 
猫 这种 动物 本身 就是 一件 很难 的 事情 了 
更 不要 说 去 定义 一只猫 在 图像 上 的 
固定 表现形式 所以 我们 寄 希望 于 机器学习 使用 Data 
driven approach / 数据 驱动 法 来做 做 尝试 简单 
说来 就是 对于 每个 类别 我们 都找/nr 一定量 的 图片 
数据 喂 给 计算机 让 它 自己 去 学习 和 
总结 每 一类 的 图片 的 特点 对了 这个 过程 
和 小盆友 学习 新鲜 事物 是 一样 一样 的 喂 
给 计算机 学习 的 图片 数据 就和 下图 的 猫 
/ 狗 / 杯子 / 帽子 一样 1.3 机器学习 解决 
图像 分类 的 流程 / Pipeline 整体 的 流程 和 
普通 机器学习 完全一致 简单 说来 也就 下面 三步 输入 我们 
的 给定 K 个 类别 的 N 张 图片 作为 
计算机 学习 的 训练 集 学习 让 计算机 逐 张 
图片 地 观察 和 学习 评估 就像 我们 上 学学 
了 东西 要 考试 检测 一样 我们 也 得 考考 
计算机 学得 如何 于是 我们 给定 一些 计算机 不 知道 
类别 的 图片 让 它 判别 然后再 比 对 我们 
已知 的 正确 答案 2 . 最 近邻 分类器 Nearest 
Neighbor Classifier 先从 简单 的 方法 开始 说 先提 一提 
最 近邻 分类器 / Nearest Neighbor Classifier 不过 事先 申明 
它 和 深度 学习 中的 卷积 神经网 / Convolutional Neural 
Networks 其实 一点 关系 都 没有 我们 只是 从 基础 
到 前沿 一点一点 推进 最 近邻 是 图像 识别 一个 
相对 简单 和 基础 的 实现 方式 2.1 CIFAR 10CIFAR 
10 是 一个 非常 常用 的 图像 分类 数据集 数据集 
包含 60000张 32 * 32 像素 的 小 图片 每张/r 
图片/n 都有/nr 一个/m 类别/n 标注/v 总共有 10类 分成 了 50000张 
的 训练 集 和 10000张 的 测试 集 如下 是 
一些 图片 示例 上 图中 左边 是 十个 类别 和 
对应 的 一些 示例 图片 右边 是 给定 一张 图片 
后 根据 像素 距离 计算 出来 的 最近 的 10张 
图片 2.2 基于 最 近邻 的 简单 图像 类别 判定 
假如 现在 用 CIFAR 10 数据集 做 训练 集 判断 
一张 未知 的 图片 属于 CIFAR 10 中的 哪 一类 
应该 怎么做 呢 一个 很 直观 的 想法 就是 既然 
我们 现在 有 每个 像素点 的 值 那 我们 就 
根据 输入 图片 的 这些 值 计算 和 训练 集中 
的 图片 距离 找 最近 的 图片 的 类别 作为 
它 的 类别 不 就行 了吗 恩 想法 很 直接 
这 就是 最 近邻 的 思想 偷偷 说一句 这种 直接 
的 做法 在 图像 识别 中 其实 效果 并 不是 
特别 好 比 如上图 是 按照 这个 思想 找 的 
最近 邻 其实 只有 3个 图片 的 最近 邻 是 
正确 的 类目 即使 这样 作为 最 基础 的 方法 
还是 得 掌握 我们 来 简单 实现 一下吧 我们 需要 
一个 图像 距离 评定 准则 比如 最 简单 的 方式 
就是 比对 两个 图像 像素 向量 之间 的 l1 距离 
也叫 曼哈顿 距离 / cityblock 距离 公式 如下 d1 I1 
I2 = ∑ p ∣ ∣ Ip1 − Ip2 ∣ 
∣ 其实 就是 计算 了 所有 像素点 之间 的 差值 
然后 做了 加法 直观 的 理解 如 下图 我们 先把 
数据集 读进 内存 # / usr / bin / env 
python # coding = utf 8 import os import sys 
import numpy as np def load _ CIFAR _ batch 
filename cifar 10 数据集 是 分 batch 存储 的 这是 
载入 单个 batch @ 参数 filename cifar 文件名 @ r 
返回值 X Y cifar batch 中的 data 和 labels with 
open filename r as f datadict = pickle . load 
f X = datadict data Y = datadict labels X 
= X . reshape 10000 3 32 32 . transpose 
0 2 3 1 . astype float Y = np 
. array Y return X Y def load _ CIFAR10 
ROOT 读取 载入 整个 CIFAR 10 数据集 @ 参数 ROOT 
根 目录名 @ return X _ train Y _ train 
训练 集 data 和 labels X _ test Y _ 
test 测试 集 data 和 labels xs = ys = 
for b in range 1 6 f = os . 
path . join ROOT data _ batch _ % d 
% b X Y = load _ CIFAR _ batch 
f xs . append X ys . append Y X 
_ train = np . concatenate xs Y _ train 
= np . concatenate ys del X Y X _ 
test Y _ test = load _ CIFAR _ batch 
os . path . join ROOT test _ batch return 
X _ train Y _ train X _ test Y 
_ test # 载入 训练 和 测试 数据集 X _ 
train Y _ train X _ test Y _ test 
= load _ CIFAR10 data / cifar10 / # 把 
32 * 32 * 3 的 多维 数组 展平 Xtr 
_ rows = X _ train . reshape X _ 
train . shape 0 32 * 32 * 3 # 
Xtr _ rows 50000 x 3072 Xte _ rows = 
X _ test . reshape X _ test . shape 
0 32 * 32 * 3 # Xte _ rows 
10000 x 3 0 7 2 1 2 3 4 
5 6 7 8 9 1 0 1 1 1 
2 1 3 1 4 1 5 1 6 1 
7 1 8 1 9 2 0 2 1 2 
2 2 3 2 4 2 5 2 6 2 
7 2 8 2 9 3 0 3 1 3 
2 3 3 3 4 3 5 3 6 3 
7 3 8 3 9 4 0 4 1 4 
2 4 3 4 4 4 5 4 6 4 
7 4 8 4 9 5 0 5 1 5 
2 5 3 5 4 5 5 5 6 5 
7 5 8 1 2 3 4 5 6 7 
8 9 1 0 1 1 1 2 1 3 
1 4 1 5 1 6 1 7 1 8 
1 9 2 0 2 1 2 2 2 3 
2 4 2 5 2 6 2 7 2 8 
2 9 3 0 3 1 3 2 3 3 
3 4 3 5 3 6 3 7 3 8 
3 9 4 0 4 1 4 2 4 3 
4 4 4 5 4 6 4 7 4 8 
4 9 5 0 5 1 5 2 5 3 
5 4 5 5 5 6 5 7 5 8 
下面 我们 实现 最 近邻 的 思路 class NearestNeighbor def 
_ _ init _ _ self pass def train self 
X y 这个 地方 的 训练 其实 就是 把 所有 
的 已有 图片 读取 进来 _ | | # the 
nearest neighbor classifier simply remembers all the training data self 
. Xtr = X self . ytr = y def 
predict self X 所谓 的 预测 过程 其实 就是 扫描 
所有 训练 集中 的 图片 计算 距离 取 最小 的 
距离 对应 图片 的 类目 num _ test = X 
. shape 0 # 要 保证 维度 一致 哦 Ypred 
= np . zeros num _ test dtype = self 
. ytr . dtype # 把 训练 集 扫 一遍 
_ | | for i in xrange num _ test 
# 计算 l1 距离 并 找到 最近 的 图片 distances 
= np . sum np . abs self . Xtr 
X i axis = 1 min _ index = np 
. argmin distances # 取 最近 图片 的 下标 Ypred 
i = self . ytr min _ index # 记录 
下 label return Ypred nn = NearestNeighbor # 初始化 一个 
最 近邻 对象 nn . train Xtr _ rows Y 
_ train # 训练 . . . 其实 就是 读取 
训练 集 Yte _ predict = nn . predict Xte 
_ rows # 预测 # 比对 标准答案 计算 准确率 print 
accuracy % f % np . mean Yte _ predict 
= = Y _ test 1 2 3 4 5 
6 7 8 9 1 0 1 1 1 2 
1 3 1 4 1 5 1 6 1 7 
1 8 1 9 2 0 2 1 2 2 
2 3 2 4 2 5 2 6 2 7 
2 8 2 9 3 0 3 1 3 2 
3 3 3 4 1 2 3 4 5 6 
7 8 9 1 0 1 1 1 2 1 
3 1 4 1 5 1 6 1 7 1 
8 1 9 2 0 2 1 2 2 2 
3 2 4 2 5 2 6 2 7 2 
8 2 9 3 0 3 1 3 2 3 
3 3 4 最 近邻 的 思想 在 CIFAR 上 
得到 的 准确度 为 38.6% 我们 知道 10 各 类别 
我们 随机 猜测 的话 准确率 差不多 是 1/10 = 10% 
所以 说 还是 有 识别 效果 的 但是 显然 这 
距离 人 的 识别 准确率 94% 实在 是 低 太多 
了 不那么 实用 2.3 关于 最 近邻 的 距离 准则 
我们 这里 用 的 距离 准则 是 l1 距离 实际上 
除掉 l1 距离 我们 还有 很多 其他 的 距离 准则 
比如说 l2 距离 也 就是 大家 熟知 的 欧氏距离 的 
计算 准则 如下 d2 I1 I2 = ∑ p Ip1 
− Ip2 2 − − − − − − − 
− − − − − √ 比如 余弦 距离 计算 
准则 如下 1 − I1 ⋅ I2 | | I1 
| | ⋅ | | I2 | | 更多 的 
距离 准则 可以 参见 scipy 相关 计算 页面 . 3 
. K 最 近邻 分类器 K Nearest Neighbor Classifier 这是 
对 最近 邻 的 思想 的 一个 调整 其实 我们 
在 使用 最 近邻 分类器 分类 扫描 CIFAR 训练 集 
的 时候 会 发现 有时候 不 一定 距离 最近 的 
和 当前 图片 是 同类 但是/c 最近/f 的/uj 一些/m 里/f 
有/v 很多/m 和/c 当前/t 图片/n 是/v 同类/n 所以 我们 自然而然 
想到 把 最 近邻 扩展 为 最近 的 N 个 
临近 点 然后 统计 一下 这些 点 的 类目 分布 
取 最多 的 那个 类目 作为 自己 的 类别 恩 
这 就是 KNN 的 思想 KNN 其实 是 一种 特别 
常用 的 分类 算法 但是 有个/nr 问题 我们 的 K 
值 应该 取 多少 呢 换句话说 我们 找 多少 邻居 
来 投票 比较 靠谱 呢 3.1 交叉 验证 与 参数 
选择 在 现在 的 场景 下 假如 我们 确定 使用 
KNN 来 完成 图片 类别 识别 问题 我们 发现 有 
一些 参数 是 肯定 会 影响 最后 的 识别 结果 
的 比如 距离 的 选择 l1 l2 cos 等等 近邻 
个数 K 的 取值 每组/r 参/n 数下/m 其实/d 都能/nr 产生/n 
一个/m 新的/i model/w 所以 这 可以 视为 一个 模型 选择 
/ model selection 问题 而 对于 模型 选择 问题 最 
常用 的 办法 就是 在 交叉 验证 集上 实验 数据 
总量 就 那么 多 如果 我们 在 test data 上 
做 模型 参数 选择 又用 它 做 效果 评估 显然 
不是 那么 合理 因为 我们 的 模型 参数 很 有可能 
是 在 test data 上 过拟合 的 不能 很 公正 
地 评估 结果 所以 我们 通常 会 把 训练 数据 
分为 两个 部分 一大 部分 作为 训练 用 另外 一 
部分 就是 所谓 的 cross validation 数据集 用来 进行 模型 
参数 选择 的 比如说 我们 有 50000 训练 图片 我们 
可以 把 它 分为 49000 的 训练 集 和 1000 
的 交叉 验证 集 # 假定 已经 有 Xtr _ 
rows Ytr Xte _ rows Yte 了 其中 Xtr _ 
rows 为 50000 * 3072 矩阵 Xval _ rows = 
Xtr _ rows 1000 # 构建 1000 的 交叉 验证 
集 Yval = Ytr 1000 Xtr _ rows = Xtr 
_ rows 1000 # 保留 49000 的 训练 集 Ytr 
= Ytr 1000 # 设置 一些 k 值 用于 试验 
validation _ accuracies = for k in 1 3 5 
7 10 20 50 100 # 初始化 对象 nn = 
NearestNeighbor nn . train Xtr _ rows Ytr # 修改 
一下 predict 函数 接受 k 作为 参数 Yval _ predict 
= nn . predict Xval _ rows k = k 
acc = np . mean Yval _ predict = = 
Yval print accuracy % f % acc # 输出 结果 
validation _ accuracies . append k acc 1 2 3 
4 5 6 7 8 9 1 0 1 1 
1 2 1 3 1 4 1 5 1 6 
1 7 1 8 1 9 2 0 1 2 
3 4 5 6 7 8 9 1 0 1 
1 1 2 1 3 1 4 1 5 1 
6 1 7 1 8 1 9 2 0 这里 
提 一个 在 很多 地方 会 看到 的 概念 叫做 
k fold cross validation 意思 其实 就是 把 原始数据 分成 
k 份 轮流 使用 其中 k 1份 作为 训练 数据 
而 剩余 的 1份 作为 交叉 验证 数据 因此 其实 
对于 k fold cross validation 我们 会 得到 k 个 
accuracy 以下 是 5 fold cross validation 的 一个 示例 
以下 是 我们 使用 5 fold cross validation 取 不同 
的 k 值 时 得到 的 accuracy 曲线 补充 一下 
因为 是 5 fold cross validation 所以 在 每个 k 
值 上有 5个 取值 我们 取其 均值 作为 此时 的 
准确度 可以 看出 大概在 k = 7 左右 有 最佳 
的 准确度 3.2 最 近邻 方法 的 优缺点 K 最 
近邻 的 优点 大家 都看/nr 出来 了 思路 非常 简单 
清晰 而且 完全 不 需要 训练 不过 也 正 因为 
如此 最后 的 predict 过程 非常 耗时 因为 要 和 
全部 训练 集中 的 图片 比对 一遍 实际 应用 中 
我们 其实 更加 关心 实施 predict 所 消耗 的 时间 
如果 有 一个 图像识别 app 返回 结果 要 半 小时 
一 小时 你 一定 第一 时间 把 它 卸了 我们 
反倒 不 那么 在乎 训练 时长 训练 时间 稍微 长 
一点 没关系 只要 最后 应用 的 时候 识别 速度快 效果 
好 就很 赞 后面 会 提到 的 深度 神经 网络 
就是 这样 深度 神经 网络 解决 图像 问题 时 训练 
是 一个 相对 耗 时间 的 过程 但是 识别 的 
过程 非常 快 另外 不得 不多 说一句 的 是 优化 
计算 K 最 近邻 时间问题 实际上 依旧 到 现在 都是/nr 
一个 非常 热门 的 问题 Approximate Nearest Neighbor ANN 算法 
是 牺牲掉 一小部分 的 准确度 而 提高 很大 程度 的 
速度 能比 较快 地 找到 近似 的 K 最 近邻 
现在 已经 有 很多 这样 的 库 比如说 FLANN . 
最后 我们 用 一张 图 来 说明 一下 用 图片 
像素 级别 的 距离 来 实现 图像 类别 识别 有其 
不足之处 我们 用 一个 叫做 t SNE 的 技术 把 
CIFAR 10 的 所有 图片 按 两个 维度 平 铺出来 
靠 得越 近 的 图片 表示 其 像素 级别 的 
距离 越 接近 然而 我们 瞄 一眼 发现 其实 靠得 
最近 的 并不 一定 是 同类 别的 其实 观察 一下 
你 就会 发现 像素 级别 接近 的 图片 在 整张 
图 的 颜色 分布 上 有 很大 的 共性 然而 
在 图像 内容 上 有时候 也 只能 无奈 地 呵呵 
嗒 毕竟 颜色 分布 相同 的 不同 物体 也 是 
非常 多 的 参考 资料 与 原文 cs231n 图像 分类 
与 KNN 深度 学习 与 计算机 视觉 系列 2 _ 图像 分类 
与 KNN 作者   寒 小阳 时间 2015年 11月 出处 
http / / blog . csdn . net / han 
_ xiaoyang / article / details / 49949535 声明 版权所有 
转载 请 注明 出处 谢谢 1 . 图像 分类 问题 
这是 人 每天 自然而然 会做 的 事情 普 通到 大部分 
时候 我们 都 感知 不到 我们 在 完成 一个 个 
这样 的 任务 早晨 起床 洗漱 你 要 看看 洗漱 
台 一堆 东西 中 哪个 是 杯子 哪个 是 你 
的 牙刷 吃早餐 的 时候 你 要 分辨 食物 和 
碗碟 抽象 一下 对于 一张 输入 的 图片 要 判定 
它 属于 给定 的 一些 标签 / 类别 中的 哪一个 
看似 很 简单 的 一个 问题 这么 多年 却 一直 
是 计算机 视觉 的 一个 核心 问题 应用 场景 也 
很多 它 的 重要性 还 体现 在 其实 其他 的 
一些 计算机 视觉 的 问题 比如说 物体 定位 和 识别 
图像 内容 分割 等 都 可以 基于 它 去 完成 
咱们 举个 例子 从 机器 学习 的 角度 描述 一下 
这个 问题 ^ _ ^ 计算机 拿到 一张 图片 如下 
图 所示 然后 需要 给出 它 对应 { 猫 狗 
帽子 杯子 } 4类 的 概率 人类 是 灰常 牛逼 
的 生物 我们 一瞥 就 知道 这 货 是 猫 
然而 对 计算机 而言 他们 是 没 办法 像人 一样 
看 到 整张 图片 的 对 它 而言 这 是 
一个 3 维 的 大 矩阵 包含 248 * 400个 
像素点 每个 像素点 又有 红绿蓝 RGB 3个 颜色通道 的 值 
每个 值 在 0 黑 255 白 之间 计算机 就 
需要 根据 这 248 * 400 * 3 = 297600个 
数值 去 判定 这 货 是 猫 1.1 图像 识别 
的 难点 图像识别 看似 很 直接 但 实际上 包含 很多 
挑战 人类 可是 经过 数 亿年 的 进化 才 获得 
如此 强大 的 大脑 对于 各种 物体 有着 精准 的 
视觉 理解力 总体而言 我们 想 教 会 计算机 去 认识 
一类 图 会有 下面 这样 一些 困难 视角 不同 每个 
事物 旋转 或者 侧视 最后 的 构图 都 完全 不同 
尺寸 大小 不 统一 相同 内容 的 图片 也 可大可小 
变形 很多 东西 处于 特殊 的 情形 下 会有 特殊 
的 摆放 和 形状 光影 等 干扰 / 幻象 背景 
干扰 同类 内 的 差异 比如 椅子 有 靠椅 / 
吧椅 / 餐椅 / 躺椅 1.2 识别 的 途径 首先 
大家 想想 就 知道 这个 算法 并不 像 对 一个 
数组 排序 或者 求 有向图 的 最短 路径 我们 没 
办法 提前 制定 一个 流程 和 规则 去 解决 定义 
猫 这种 动物 本身 就是 一件 很难 的 事情 了 
更 不要 说 去 定义 一只猫 在 图像 上 的 
固定 表现形式 所以 我们 寄 希望 于 机器学习 使用 Data 
driven approach / 数据 驱动 法 来做 做 尝试 简单 
说来 就是 对于 每个 类别 我们 都找/nr 一定量 的 图片 
数据 喂 给 计算机 让 它 自己 去 学习 和 
总结 每 一类 的 图片 的 特点 对了 这个 过程 
和 小盆友 学习 新鲜 事物 是 一样 一样 的 喂 
给 计算机 学习 的 图片 数据 就和 下图 的 猫 
/ 狗 / 杯子 / 帽子 一样 1.3 机器学习 解决 
图像 分类 的 流程 / Pipeline 整体 的 流程 和 
普通 机器学习 完全一致 简单 说来 也就 下面 三步 输入 我们 
的 给定 K 个 类别 的 N 张 图片 作为 
计算机 学习 的 训练 集 学习 让 计算机 逐 张 
图片 地 观察 和 学习 评估 就像 我们 上 学学 
了 东西 要 考试 检测 一样 我们 也 得 考考 
计算机 学得 如何 于是 我们 给定 一些 计算机 不 知道 
类别 的 图片 让 它 判别 然后再 比 对 我们 
已知 的 正确 答案 2 . 最 近邻 分类器 Nearest 
Neighbor Classifier 先从 简单 的 方法 开始 说 先提 一提 
最 近邻 分类器 / Nearest Neighbor Classifier 不过 事先 申明 
它 和 深度 学习 中的 卷积 神经网 / Convolutional Neural 
Networks 其实 一点 关系 都 没有 我们 只是 从 基础 
到 前沿 一点一点 推进 最 近邻 是 图像 识别 一个 
相对 简单 和 基础 的 实现 方式 2.1 CIFAR 10CIFAR 
10 是 一个 非常 常用 的 图像 分类 数据集 数据集 
包含 60000张 32 * 32 像素 的 小 图片 每张/r 
图片/n 都有/nr 一个/m 类别/n 标注/v 总共有 10类 分成 了 50000张 
的 训练 集 和 10000张 的 测试 集 如下 是 
一些 图片 示例 上 图中 左边 是 十个 类别 和 
对应 的 一些 示例 图片 右边 是 给定 一张 图片 
后 根据 像素 距离 计算 出来 的 最近 的 10张 
图片 2.2 基于 最 近邻 的 简单 图像 类别 判定 
假如 现在 用 CIFAR 10 数据集 做 训练 集 判断 
一张 未知 的 图片 属于 CIFAR 10 中的 哪 一类 
应该 怎么做 呢 一个 很 直观 的 想法 就是 既然 
我们 现在 有 每个 像素点 的 值 那 我们 就 
根据 输入 图片 的 这些 值 计算 和 训练 集中 
的 图片 距离 找 最近 的 图片 的 类别 作为 
它 的 类别 不 就行 了吗 恩 想法 很 直接 
这 就是 最 近邻 的 思想 偷偷 说一句 这种 直接 
的 做法 在 图像 识别 中 其实 效果 并 不是 
特别 好 比 如上图 是 按照 这个 思想 找 的 
最近 邻 其实 只有 3个 图片 的 最近 邻 是 
正确 的 类目 即使 这样 作为 最 基础 的 方法 
还是 得 掌握 我们 来 简单 实现 一下吧 我们 需要 
一个 图像 距离 评定 准则 比如 最 简单 的 方式 
就是 比对 两个 图像 像素 向量 之间 的 l1 距离 
也叫 曼哈顿 距离 / cityblock 距离 公式 如下 d1 I1 
I2 = ∑ p ∣ ∣ Ip1 − Ip2 ∣ 
∣ 其实 就是 计算 了 所有 像素点 之间 的 差值 
然后 做了 加法 直观 的 理解 如 下图 我们 先把 
数据集 读进 内存 # / usr / bin / env 
python # coding = utf 8 import os import sys 
import numpy as np def load _ CIFAR _ batch 
filename cifar 10 数据集 是 分 batch 存储 的 这是 
载入 单个 batch @ 参数 filename cifar 文件名 @ r 
返回值 X Y cifar batch 中的 data 和 labels with 
open filename r as f datadict = pickle . load 
f X = datadict data Y = datadict labels X 
= X . reshape 10000 3 32 32 . transpose 
0 2 3 1 . astype float Y = np 
. array Y return X Y def load _ CIFAR10 
ROOT 读取 载入 整个 CIFAR 10 数据集 @ 参数 ROOT 
根 目录名 @ return X _ train Y _ train 
训练 集 data 和 labels X _ test Y _ 
test 测试 集 data 和 labels xs = ys = 
for b in range 1 6 f = os . 
path . join ROOT data _ batch _ % d 
% b X Y = load _ CIFAR _ batch 
f xs . append X ys . append Y X 
_ train = np . concatenate xs Y _ train 
= np . concatenate ys del X Y X _ 
test Y _ test = load _ CIFAR _ batch 
os . path . join ROOT test _ batch return 
X _ train Y _ train X _ test Y 
_ test # 载入 训练 和 测试 数据集 X _ 
train Y _ train X _ test Y _ test 
= load _ CIFAR10 data / cifar10 / # 把 
32 * 32 * 3 的 多维 数组 展平 Xtr 
_ rows = X _ train . reshape X _ 
train . shape 0 32 * 32 * 3 # 
Xtr _ rows 50000 x 3072 Xte _ rows = 
X _ test . reshape X _ test . shape 
0 32 * 32 * 3 # Xte _ rows 
10000 x 3 0 7 2 1 2 3 4 
5 6 7 8 9 1 0 1 1 1 
2 1 3 1 4 1 5 1 6 1 
7 1 8 1 9 2 0 2 1 2 
2 2 3 2 4 2 5 2 6 2 
7 2 8 2 9 3 0 3 1 3 
2 3 3 3 4 3 5 3 6 3 
7 3 8 3 9 4 0 4 1 4 
2 4 3 4 4 4 5 4 6 4 
7 4 8 4 9 5 0 5 1 5 
2 5 3 5 4 5 5 5 6 5 
7 5 8 1 2 3 4 5 6 7 
8 9 1 0 1 1 1 2 1 3 
1 4 1 5 1 6 1 7 1 8 
1 9 2 0 2 1 2 2 2 3 
2 4 2 5 2 6 2 7 2 8 
2 9 3 0 3 1 3 2 3 3 
3 4 3 5 3 6 3 7 3 8 
3 9 4 0 4 1 4 2 4 3 
4 4 4 5 4 6 4 7 4 8 
4 9 5 0 5 1 5 2 5 3 
5 4 5 5 5 6 5 7 5 8 
下面 我们 实现 最 近邻 的 思路 class NearestNeighbor def 
_ _ init _ _ self pass def train self 
X y 这个 地方 的 训练 其实 就是 把 所有 
的 已有 图片 读取 进来 _ | | # the 
nearest neighbor classifier simply remembers all the training data self 
. Xtr = X self . ytr = y def 
predict self X 所谓 的 预测 过程 其实 就是 扫描 
所有 训练 集中 的 图片 计算 距离 取 最小 的 
距离 对应 图片 的 类目 num _ test = X 
. shape 0 # 要 保证 维度 一致 哦 Ypred 
= np . zeros num _ test dtype = self 
. ytr . dtype # 把 训练 集 扫 一遍 
_ | | for i in xrange num _ test 
# 计算 l1 距离 并 找到 最近 的 图片 distances 
= np . sum np . abs self . Xtr 
X i axis = 1 min _ index = np 
. argmin distances # 取 最近 图片 的 下标 Ypred 
i = self . ytr min _ index # 记录 
下 label return Ypred nn = NearestNeighbor # 初始化 一个 
最 近邻 对象 nn . train Xtr _ rows Y 
_ train # 训练 . . . 其实 就是 读取 
训练 集 Yte _ predict = nn . predict Xte 
_ rows # 预测 # 比对 标准答案 计算 准确率 print 
accuracy % f % np . mean Yte _ predict 
= = Y _ test 1 2 3 4 5 
6 7 8 9 1 0 1 1 1 2 
1 3 1 4 1 5 1 6 1 7 
1 8 1 9 2 0 2 1 2 2 
2 3 2 4 2 5 2 6 2 7 
2 8 2 9 3 0 3 1 3 2 
3 3 3 4 1 2 3 4 5 6 
7 8 9 1 0 1 1 1 2 1 
3 1 4 1 5 1 6 1 7 1 
8 1 9 2 0 2 1 2 2 2 
3 2 4 2 5 2 6 2 7 2 
8 2 9 3 0 3 1 3 2 3 
3 3 4 最 近邻 的 思想 在 CIFAR 上 
得到 的 准确度 为 38.6% 我们 知道 10 各 类别 
我们 随机 猜测 的话 准确率 差不多 是 1/10 = 10% 
所以 说 还是 有 识别 效果 的 但是 显然 这 
距离 人 的 识别 准确率 94% 实在 是 低 太多 
了 不那么 实用 2.3 关于 最 近邻 的 距离 准则 
我们 这里 用 的 距离 准则 是 l1 距离 实际上 
除掉 l1 距离 我们 还有 很多 其他 的 距离 准则 
比如说 l2 距离 也 就是 大家 熟知 的 欧氏距离 的 
计算 准则 如下 d2 I1 I2 = ∑ p Ip1 
− Ip2 2 − − − − − − − 
− − − − − √ 比如 余弦 距离 计算 
准则 如下 1 − I1 ⋅ I2 | | I1 
| | ⋅ | | I2 | | 更多 的 
距离 准则 可以 参见 scipy 相关 计算 页面 . 3 
. K 最 近邻 分类器 K Nearest Neighbor Classifier 这是 
对 最近 邻 的 思想 的 一个 调整 其实 我们 
在 使用 最 近邻 分类器 分类 扫描 CIFAR 训练 集 
的 时候 会 发现 有时候 不 一定 距离 最近 的 
和 当前 图片 是 同类 但是/c 最近/f 的/uj 一些/m 里/f 
有/v 很多/m 和/c 当前/t 图片/n 是/v 同类/n 所以 我们 自然而然 
想到 把 最 近邻 扩展 为 最近 的 N 个 
临近 点 然后 统计 一下 这些 点 的 类目 分布 
取 最多 的 那个 类目 作为 自己 的 类别 恩 
这 就是 KNN 的 思想 KNN 其实 是 一种 特别 
常用 的 分类 算法 但是 有个/nr 问题 我们 的 K 
值 应该 取 多少 呢 换句话说 我们 找 多少 邻居 
来 投票 比较 靠谱 呢 3.1 交叉 验证 与 参数 
选择 在 现在 的 场景 下 假如 我们 确定 使用 
KNN 来 完成 图片 类别 识别 问题 我们 发现 有 
一些 参数 是 肯定 会 影响 最后 的 识别 结果 
的 比如 距离 的 选择 l1 l2 cos 等等 近邻 
个数 K 的 取值 每组/r 参/n 数下/m 其实/d 都能/nr 产生/n 
一个/m 新的/i model/w 所以 这 可以 视为 一个 模型 选择 
/ model selection 问题 而 对于 模型 选择 问题 最 
常用 的 办法 就是 在 交叉 验证 集上 实验 数据 
总量 就 那么 多 如果 我们 在 test data 上 
做 模型 参数 选择 又用 它 做 效果 评估 显然 
不是 那么 合理 因为 我们 的 模型 参数 很 有可能 
是 在 test data 上 过拟合 的 不能 很 公正 
地 评估 结果 所以 我们 通常 会 把 训练 数据 
分为 两个 部分 一大 部分 作为 训练 用 另外 一 
部分 就是 所谓 的 cross validation 数据集 用来 进行 模型 
参数 选择 的 比如说 我们 有 50000 训练 图片 我们 
可以 把 它 分为 49000 的 训练 集 和 1000 
的 交叉 验证 集 # 假定 已经 有 Xtr _ 
rows Ytr Xte _ rows Yte 了 其中 Xtr _ 
rows 为 50000 * 3072 矩阵 Xval _ rows = 
Xtr _ rows 1000 # 构建 1000 的 交叉 验证 
集 Yval = Ytr 1000 Xtr _ rows = Xtr 
_ rows 1000 # 保留 49000 的 训练 集 Ytr 
= Ytr 1000 # 设置 一些 k 值 用于 试验 
validation _ accuracies = for k in 1 3 5 
7 10 20 50 100 # 初始化 对象 nn = 
NearestNeighbor nn . train Xtr _ rows Ytr # 修改 
一下 predict 函数 接受 k 作为 参数 Yval _ predict 
= nn . predict Xval _ rows k = k 
acc = np . mean Yval _ predict = = 
Yval print accuracy % f % acc # 输出 结果 
validation _ accuracies . append k acc 1 2 3 
4 5 6 7 8 9 1 0 1 1 
1 2 1 3 1 4 1 5 1 6 
1 7 1 8 1 9 2 0 1 2 
3 4 5 6 7 8 9 1 0 1 
1 1 2 1 3 1 4 1 5 1 
6 1 7 1 8 1 9 2 0 这里 
提 一个 在 很多 地方 会 看到 的 概念 叫做 
k fold cross validation 意思 其实 就是 把 原始数据 分成 
k 份 轮流 使用 其中 k 1份 作为 训练 数据 
而 剩余 的 1份 作为 交叉 验证 数据 因此 其实 
对于 k fold cross validation 我们 会 得到 k 个 
accuracy 以下 是 5 fold cross validation 的 一个 示例 
以下 是 我们 使用 5 fold cross validation 取 不同 
的 k 值 时 得到 的 accuracy 曲线 补充 一下 
因为 是 5 fold cross validation 所以 在 每个 k 
值 上有 5个 取值 我们 取其 均值 作为 此时 的 
准确度 可以 看出 大概在 k = 7 左右 有 最佳 
的 准确度 3.2 最 近邻 方法 的 优缺点 K 最 
近邻 的 优点 大家 都看/nr 出来 了 思路 非常 简单 
清晰 而且 完全 不 需要 训练 不过 也 正 因为 
如此 最后 的 predict 过程 非常 耗时 因为 要 和 
全部 训练 集中 的 图片 比对 一遍 实际 应用 中 
我们 其实 更加 关心 实施 predict 所 消耗 的 时间 
如果 有 一个 图像识别 app 返回 结果 要 半 小时 
一 小时 你 一定 第一 时间 把 它 卸了 我们 
反倒 不 那么 在乎 训练 时长 训练 时间 稍微 长 
一点 没关系 只要 最后 应用 的 时候 识别 速度快 效果 
好 就很 赞 后面 会 提到 的 深度 神经 网络 
就是 这样 深度 神经 网络 解决 图像 问题 时 训练 
是 一个 相对 耗 时间 的 过程 但是 识别 的 
过程 非常 快 另外 不得 不多 说一句 的 是 优化 
计算 K 最 近邻 时间问题 实际上 依旧 到 现在 都是/nr 
一个 非常 热门 的 问题 Approximate Nearest Neighbor ANN 算法 
是 牺牲掉 一小部分 的 准确度 而 提高 很大 程度 的 
速度 能比 较快 地 找到 近似 的 K 最 近邻 
现在 已经 有 很多 这样 的 库 比如说 FLANN . 
最后 我们 用 一张 图 来 说明 一下 用 图片 
像素 级别 的 距离 来 实现 图像 类别 识别 有其 
不足之处 我们 用 一个 叫做 t SNE 的 技术 把 
CIFAR 10 的 所有 图片 按 两个 维度 平 铺出来 
靠 得越 近 的 图片 表示 其 像素 级别 的 
距离 越 接近 然而 我们 瞄 一眼 发现 其实 靠得 
最近 的 并不 一定 是 同类 别的 其实 观察 一下 
你 就会 发现 像素 级别 接近 的 图片 在 整张 
图 的 颜色 分布 上 有 很大 的 共性 然而 
在 图像 内容 上 有时候 也 只能 无奈 地 呵呵 
嗒 毕竟 颜色 分布 相同 的 不同 物体 也 是 
非常 多 的 参考 资料 与 原文 cs231n 图像 分类 
与 KNN 深度 学习 与 计算机 视觉 系列 2 _ 图像 分类 
与 KNN 作者   寒 小阳 时间 2015年 11月 出处 
http / / blog . csdn . net / han 
_ xiaoyang / article / details / 49949535 声明 版权所有 
转载 请 注明 出处 谢谢 1 . 图像 分类 问题 
这是 人 每天 自然而然 会做 的 事情 普 通到 大部分 
时候 我们 都 感知 不到 我们 在 完成 一个 个 
这样 的 任务 早晨 起床 洗漱 你 要 看看 洗漱 
台 一堆 东西 中 哪个 是 杯子 哪个 是 你 
的 牙刷 吃早餐 的 时候 你 要 分辨 食物 和 
碗碟 抽象 一下 对于 一张 输入 的 图片 要 判定 
它 属于 给定 的 一些 标签 / 类别 中的 哪一个 
看似 很 简单 的 一个 问题 这么 多年 却 一直 
是 计算机 视觉 的 一个 核心 问题 应用 场景 也 
很多 它 的 重要性 还 体现 在 其实 其他 的 
一些 计算机 视觉 的 问题 比如说 物体 定位 和 识别 
图像 内容 分割 等 都 可以 基于 它 去 完成 
咱们 举个 例子 从 机器 学习 的 角度 描述 一下 
这个 问题 ^ _ ^ 计算机 拿到 一张 图片 如下 
图 所示 然后 需要 给出 它 对应 { 猫 狗 
帽子 杯子 } 4类 的 概率 人类 是 灰常 牛逼 
的 生物 我们 一瞥 就 知道 这 货 是 猫 
然而 对 计算机 而言 他们 是 没 办法 像人 一样 
看 到 整张 图片 的 对 它 而言 这 是 
一个 3 维 的 大 矩阵 包含 248 * 400个 
像素点 每个 像素点 又有 红绿蓝 RGB 3个 颜色通道 的 值 
每个 值 在 0 黑 255 白 之间 计算机 就 
需要 根据 这 248 * 400 * 3 = 297600个 
数值 去 判定 这 货 是 猫 1.1 图像 识别 
的 难点 图像识别 看似 很 直接 但 实际上 包含 很多 
挑战 人类 可是 经过 数 亿年 的 进化 才 获得 
如此 强大 的 大脑 对于 各种 物体 有着 精准 的 
视觉 理解力 总体而言 我们 想 教 会 计算机 去 认识 
一类 图 会有 下面 这样 一些 困难 视角 不同 每个 
事物 旋转 或者 侧视 最后 的 构图 都 完全 不同 
尺寸 大小 不 统一 相同 内容 的 图片 也 可大可小 
变形 很多 东西 处于 特殊 的 情形 下 会有 特殊 
的 摆放 和 形状 光影 等 干扰 / 幻象 背景 
干扰 同类 内 的 差异 比如 椅子 有 靠椅 / 
吧椅 / 餐椅 / 躺椅 1.2 识别 的 途径 首先 
大家 想想 就 知道 这个 算法 并不 像 对 一个 
数组 排序 或者 求 有向图 的 最短 路径 我们 没 
办法 提前 制定 一个 流程 和 规则 去 解决 定义 
猫 这种 动物 本身 就是 一件 很难 的 事情 了 
更 不要 说 去 定义 一只猫 在 图像 上 的 
固定 表现形式 所以 我们 寄 希望 于 机器学习 使用 Data 
driven approach / 数据 驱动 法 来做 做 尝试 简单 
说来 就是 对于 每个 类别 我们 都找/nr 一定量 的 图片 
数据 喂 给 计算机 让 它 自己 去 学习 和 
总结 每 一类 的 图片 的 特点 对了 这个 过程 
和 小盆友 学习 新鲜 事物 是 一样 一样 的 喂 
给 计算机 学习 的 图片 数据 就和 下图 的 猫 
/ 狗 / 杯子 / 帽子 一样 1.3 机器学习 解决 
图像 分类 的 流程 / Pipeline 整体 的 流程 和 
普通 机器学习 完全一致 简单 说来 也就 下面 三步 输入 我们 
的 给定 K 个 类别 的 N 张 图片 作为 
计算机 学习 的 训练 集 学习 让 计算机 逐 张 
图片 地 观察 和 学习 评估 就像 我们 上 学学 
了 东西 要 考试 检测 一样 我们 也 得 考考 
计算机 学得 如何 于是 我们 给定 一些 计算机 不 知道 
类别 的 图片 让 它 判别 然后再 比 对 我们 
已知 的 正确 答案 2 . 最 近邻 分类器 Nearest 
Neighbor Classifier 先从 简单 的 方法 开始 说 先提 一提 
最 近邻 分类器 / Nearest Neighbor Classifier 不过 事先 申明 
它 和 深度 学习 中的 卷积 神经网 / Convolutional Neural 
Networks 其实 一点 关系 都 没有 我们 只是 从 基础 
到 前沿 一点一点 推进 最 近邻 是 图像 识别 一个 
相对 简单 和 基础 的 实现 方式 2.1 CIFAR 10CIFAR 
10 是 一个 非常 常用 的 图像 分类 数据集 数据集 
包含 60000张 32 * 32 像素 的 小 图片 每张/r 
图片/n 都有/nr 一个/m 类别/n 标注/v 总共有 10类 分成 了 50000张 
的 训练 集 和 10000张 的 测试 集 如下 是 
一些 图片 示例 上 图中 左边 是 十个 类别 和 
对应 的 一些 示例 图片 右边 是 给定 一张 图片 
后 根据 像素 距离 计算 出来 的 最近 的 10张 
图片 2.2 基于 最 近邻 的 简单 图像 类别 判定 
假如 现在 用 CIFAR 10 数据集 做 训练 集 判断 
一张 未知 的 图片 属于 CIFAR 10 中的 哪 一类 
应该 怎么做 呢 一个 很 直观 的 想法 就是 既然 
我们 现在 有 每个 像素点 的 值 那 我们 就 
根据 输入 图片 的 这些 值 计算 和 训练 集中 
的 图片 距离 找 最近 的 图片 的 类别 作为 
它 的 类别 不 就行 了吗 恩 想法 很 直接 
这 就是 最 近邻 的 思想 偷偷 说一句 这种 直接 
的 做法 在 图像 识别 中 其实 效果 并 不是 
特别 好 比 如上图 是 按照 这个 思想 找 的 
最近 邻 其实 只有 3个 图片 的 最近 邻 是 
正确 的 类目 即使 这样 作为 最 基础 的 方法 
还是 得 掌握 我们 来 简单 实现 一下吧 我们 需要 
一个 图像 距离 评定 准则 比如 最 简单 的 方式 
就是 比对 两个 图像 像素 向量 之间 的 l1 距离 
也叫 曼哈顿 距离 / cityblock 距离 公式 如下 d1 I1 
I2 = ∑ p ∣ ∣ Ip1 − Ip2 ∣ 
∣ 其实 就是 计算 了 所有 像素点 之间 的 差值 
然后 做了 加法 直观 的 理解 如 下图 我们 先把 
数据集 读进 内存 # / usr / bin / env 
python # coding = utf 8 import os import sys 
import numpy as np def load _ CIFAR _ batch 
filename cifar 10 数据集 是 分 batch 存储 的 这是 
载入 单个 batch @ 参数 filename cifar 文件名 @ r 
返回值 X Y cifar batch 中的 data 和 labels with 
open filename r as f datadict = pickle . load 
f X = datadict data Y = datadict labels X 
= X . reshape 10000 3 32 32 . transpose 
0 2 3 1 . astype float Y = np 
. array Y return X Y def load _ CIFAR10 
ROOT 读取 载入 整个 CIFAR 10 数据集 @ 参数 ROOT 
根 目录名 @ return X _ train Y _ train 
训练 集 data 和 labels X _ test Y _ 
test 测试 集 data 和 labels xs = ys = 
for b in range 1 6 f = os . 
path . join ROOT data _ batch _ % d 
% b X Y = load _ CIFAR _ batch 
f xs . append X ys . append Y X 
_ train = np . concatenate xs Y _ train 
= np . concatenate ys del X Y X _ 
test Y _ test = load _ CIFAR _ batch 
os . path . join ROOT test _ batch return 
X _ train Y _ train X _ test Y 
_ test # 载入 训练 和 测试 数据集 X _ 
train Y _ train X _ test Y _ test 
= load _ CIFAR10 data / cifar10 / # 把 
32 * 32 * 3 的 多维 数组 展平 Xtr 
_ rows = X _ train . reshape X _ 
train . shape 0 32 * 32 * 3 # 
Xtr _ rows 50000 x 3072 Xte _ rows = 
X _ test . reshape X _ test . shape 
0 32 * 32 * 3 # Xte _ rows 
10000 x 3 0 7 2 1 2 3 4 
5 6 7 8 9 1 0 1 1 1 
2 1 3 1 4 1 5 1 6 1 
7 1 8 1 9 2 0 2 1 2 
2 2 3 2 4 2 5 2 6 2 
7 2 8 2 9 3 0 3 1 3 
2 3 3 3 4 3 5 3 6 3 
7 3 8 3 9 4 0 4 1 4 
2 4 3 4 4 4 5 4 6 4 
7 4 8 4 9 5 0 5 1 5 
2 5 3 5 4 5 5 5 6 5 
7 5 8 1 2 3 4 5 6 7 
8 9 1 0 1 1 1 2 1 3 
1 4 1 5 1 6 1 7 1 8 
1 9 2 0 2 1 2 2 2 3 
2 4 2 5 2 6 2 7 2 8 
2 9 3 0 3 1 3 2 3 3 
3 4 3 5 3 6 3 7 3 8 
3 9 4 0 4 1 4 2 4 3 
4 4 4 5 4 6 4 7 4 8 
4 9 5 0 5 1 5 2 5 3 
5 4 5 5 5 6 5 7 5 8 
下面 我们 实现 最 近邻 的 思路 class NearestNeighbor def 
_ _ init _ _ self pass def train self 
X y 这个 地方 的 训练 其实 就是 把 所有 
的 已有 图片 读取 进来 _ | | # the 
nearest neighbor classifier simply remembers all the training data self 
. Xtr = X self . ytr = y def 
predict self X 所谓 的 预测 过程 其实 就是 扫描 
所有 训练 集中 的 图片 计算 距离 取 最小 的 
距离 对应 图片 的 类目 num _ test = X 
. shape 0 # 要 保证 维度 一致 哦 Ypred 
= np . zeros num _ test dtype = self 
. ytr . dtype # 把 训练 集 扫 一遍 
_ | | for i in xrange num _ test 
# 计算 l1 距离 并 找到 最近 的 图片 distances 
= np . sum np . abs self . Xtr 
X i axis = 1 min _ index = np 
. argmin distances # 取 最近 图片 的 下标 Ypred 
i = self . ytr min _ index # 记录 
下 label return Ypred nn = NearestNeighbor # 初始化 一个 
最 近邻 对象 nn . train Xtr _ rows Y 
_ train # 训练 . . . 其实 就是 读取 
训练 集 Yte _ predict = nn . predict Xte 
_ rows # 预测 # 比对 标准答案 计算 准确率 print 
accuracy % f % np . mean Yte _ predict 
= = Y _ test 1 2 3 4 5 
6 7 8 9 1 0 1 1 1 2 
1 3 1 4 1 5 1 6 1 7 
1 8 1 9 2 0 2 1 2 2 
2 3 2 4 2 5 2 6 2 7 
2 8 2 9 3 0 3 1 3 2 
3 3 3 4 1 2 3 4 5 6 
7 8 9 1 0 1 1 1 2 1 
3 1 4 1 5 1 6 1 7 1 
8 1 9 2 0 2 1 2 2 2 
3 2 4 2 5 2 6 2 7 2 
8 2 9 3 0 3 1 3 2 3 
3 3 4 最 近邻 的 思想 在 CIFAR 上 
得到 的 准确度 为 38.6% 我们 知道 10 各 类别 
我们 随机 猜测 的话 准确率 差不多 是 1/10 = 10% 
所以 说 还是 有 识别 效果 的 但是 显然 这 
距离 人 的 识别 准确率 94% 实在 是 低 太多 
了 不那么 实用 2.3 关于 最 近邻 的 距离 准则 
我们 这里 用 的 距离 准则 是 l1 距离 实际上 
除掉 l1 距离 我们 还有 很多 其他 的 距离 准则 
比如说 l2 距离 也 就是 大家 熟知 的 欧氏距离 的 
计算 准则 如下 d2 I1 I2 = ∑ p Ip1 
− Ip2 2 − − − − − − − 
− − − − − √ 比如 余弦 距离 计算 
准则 如下 1 − I1 ⋅ I2 | | I1 
| | ⋅ | | I2 | | 更多 的 
距离 准则 可以 参见 scipy 相关 计算 页面 . 3 
. K 最 近邻 分类器 K Nearest Neighbor Classifier 这是 
对 最近 邻 的 思想 的 一个 调整 其实 我们 
在 使用 最 近邻 分类器 分类 扫描 CIFAR 训练 集 
的 时候 会 发现 有时候 不 一定 距离 最近 的 
和 当前 图片 是 同类 但是/c 最近/f 的/uj 一些/m 里/f 
有/v 很多/m 和/c 当前/t 图片/n 是/v 同类/n 所以 我们 自然而然 
想到 把 最 近邻 扩展 为 最近 的 N 个 
临近 点 然后 统计 一下 这些 点 的 类目 分布 
取 最多 的 那个 类目 作为 自己 的 类别 恩 
这 就是 KNN 的 思想 KNN 其实 是 一种 特别 
常用 的 分类 算法 但是 有个/nr 问题 我们 的 K 
值 应该 取 多少 呢 换句话说 我们 找 多少 邻居 
来 投票 比较 靠谱 呢 3.1 交叉 验证 与 参数 
选择 在 现在 的 场景 下 假如 我们 确定 使用 
KNN 来 完成 图片 类别 识别 问题 我们 发现 有 
一些 参数 是 肯定 会 影响 最后 的 识别 结果 
的 比如 距离 的 选择 l1 l2 cos 等等 近邻 
个数 K 的 取值 每组/r 参/n 数下/m 其实/d 都能/nr 产生/n 
一个/m 新的/i model/w 所以 这 可以 视为 一个 模型 选择 
/ model selection 问题 而 对于 模型 选择 问题 最 
常用 的 办法 就是 在 交叉 验证 集上 实验 数据 
总量 就 那么 多 如果 我们 在 test data 上 
做 模型 参数 选择 又用 它 做 效果 评估 显然 
不是 那么 合理 因为 我们 的 模型 参数 很 有可能 
是 在 test data 上 过拟合 的 不能 很 公正 
地 评估 结果 所以 我们 通常 会 把 训练 数据 
分为 两个 部分 一大 部分 作为 训练 用 另外 一 
部分 就是 所谓 的 cross validation 数据集 用来 进行 模型 
参数 选择 的 比如说 我们 有 50000 训练 图片 我们 
可以 把 它 分为 49000 的 训练 集 和 1000 
的 交叉 验证 集 # 假定 已经 有 Xtr _ 
rows Ytr Xte _ rows Yte 了 其中 Xtr _ 
rows 为 50000 * 3072 矩阵 Xval _ rows = 
Xtr _ rows 1000 # 构建 1000 的 交叉 验证 
集 Yval = Ytr 1000 Xtr _ rows = Xtr 
_ rows 1000 # 保留 49000 的 训练 集 Ytr 
= Ytr 1000 # 设置 一些 k 值 用于 试验 
validation _ accuracies = for k in 1 3 5 
7 10 20 50 100 # 初始化 对象 nn = 
NearestNeighbor nn . train Xtr _ rows Ytr # 修改 
一下 predict 函数 接受 k 作为 参数 Yval _ predict 
= nn . predict Xval _ rows k = k 
acc = np . mean Yval _ predict = = 
Yval print accuracy % f % acc # 输出 结果 
validation _ accuracies . append k acc 1 2 3 
4 5 6 7 8 9 1 0 1 1 
1 2 1 3 1 4 1 5 1 6 
1 7 1 8 1 9 2 0 1 2 
3 4 5 6 7 8 9 1 0 1 
1 1 2 1 3 1 4 1 5 1 
6 1 7 1 8 1 9 2 0 这里 
提 一个 在 很多 地方 会 看到 的 概念 叫做 
k fold cross validation 意思 其实 就是 把 原始数据 分成 
k 份 轮流 使用 其中 k 1份 作为 训练 数据 
而 剩余 的 1份 作为 交叉 验证 数据 因此 其实 
对于 k fold cross validation 我们 会 得到 k 个 
accuracy 以下 是 5 fold cross validation 的 一个 示例 
以下 是 我们 使用 5 fold cross validation 取 不同 
的 k 值 时 得到 的 accuracy 曲线 补充 一下 
因为 是 5 fold cross validation 所以 在 每个 k 
值 上有 5个 取值 我们 取其 均值 作为 此时 的 
准确度 可以 看出 大概在 k = 7 左右 有 最佳 
的 准确度 3.2 最 近邻 方法 的 优缺点 K 最 
近邻 的 优点 大家 都看/nr 出来 了 思路 非常 简单 
清晰 而且 完全 不 需要 训练 不过 也 正 因为 
如此 最后 的 predict 过程 非常 耗时 因为 要 和 
全部 训练 集中 的 图片 比对 一遍 实际 应用 中 
我们 其实 更加 关心 实施 predict 所 消耗 的 时间 
如果 有 一个 图像识别 app 返回 结果 要 半 小时 
一 小时 你 一定 第一 时间 把 它 卸了 我们 
反倒 不 那么 在乎 训练 时长 训练 时间 稍微 长 
一点 没关系 只要 最后 应用 的 时候 识别 速度快 效果 
好 就很 赞 后面 会 提到 的 深度 神经 网络 
就是 这样 深度 神经 网络 解决 图像 问题 时 训练 
是 一个 相对 耗 时间 的 过程 但是 识别 的 
过程 非常 快 另外 不得 不多 说一句 的 是 优化 
计算 K 最 近邻 时间问题 实际上 依旧 到 现在 都是/nr 
一个 非常 热门 的 问题 Approximate Nearest Neighbor ANN 算法 
是 牺牲掉 一小部分 的 准确度 而 提高 很大 程度 的 
速度 能比 较快 地 找到 近似 的 K 最 近邻 
现在 已经 有 很多 这样 的 库 比如说 FLANN . 
最后 我们 用 一张 图 来 说明 一下 用 图片 
像素 级别 的 距离 来 实现 图像 类别 识别 有其 
不足之处 我们 用 一个 叫做 t SNE 的 技术 把 
CIFAR 10 的 所有 图片 按 两个 维度 平 铺出来 
靠 得越 近 的 图片 表示 其 像素 级别 的 
距离 越 接近 然而 我们 瞄 一眼 发现 其实 靠得 
最近 的 并不 一定 是 同类 别的 其实 观察 一下 
你 就会 发现 像素 级别 接近 的 图片 在 整张 
图 的 颜色 分布 上 有 很大 的 共性 然而 
在 图像 内容 上 有时候 也 只能 无奈 地 呵呵 
嗒 毕竟 颜色 分布 相同 的 不同 物体 也 是 
非常 多 的 参考 资料 与 原文 cs231n 图像 分类 
与 KNN 