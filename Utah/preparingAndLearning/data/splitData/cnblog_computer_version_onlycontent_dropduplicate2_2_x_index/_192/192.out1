下图 Github 地址 Mask _ RCNNMask _ RCNN _ KeyPoints 
计算机 视觉 Mask RCNN _ 论文 学习 计算机 视觉 Mask 
RCNN _ 项目 文档 翻译 计算机 视觉 Mask RCNN _ 
推断 网络 其一 总览 计算机 视觉 Mask RCNN _ 推断 
网络 其二 基于 ReNet101 的 FPN 共享 网络 计算机 视觉 
Mask RCNN _ 推断 网络 其三 RPN 锚 框 处理 
和 Proposal 生成 计算机 视觉 Mask RCNN _ 推断 网络 
其四 FPN 和 ROIAlign 的 耦合 计算机 视觉 Mask RCNN 
_ 推断 网络 其五 目标 检测 结果 精炼 计算机 视觉 
Mask RCNN _ 推断 网络 其 六 Mask 生成 计算机 
视觉 Mask RCNN _ 推断 网络 终篇 使用 detect 方法 
进行 推断 计算机 视觉 Mask RCNN _ 锚 框 生成 
计算机 视觉 Mask RCNN _ 训练 网络 其一 数据集 与 
Dataset 类 计算机 视觉 Mask RCNN _ 训练 网络 其二 
train 网络结构 & 损失 函数 计算机 视觉 Mask RCNN _ 
训练 网络 其三 训练 Model 原 论文 中 提到 过 
Mask _ RCNN 是 可以 进行 关键点 检测 的 不过 
我们 学习 的 这个 工程 并 没有 添加 关键点 检测 
分支 而 有人 基于 本 工程 进行 了 完善 Mask 
_ RCNN _ Humanpose 本文 我们 将 简要 的 了解 
如何 将 关键点 识别 分支 添 加进 模型 更进一步 的 
我们 将 尝试 使用 Mask _ RCNN 对 实际 数据 
进行 识别 零 配置 相关 import os import numpy as 
np import pandas as pd from PIL import Image import 
utils as utils import model as modellib from config import 
Config PART _ INDEX = { blouse 0 1 2 
3 4 5 6 9 10 11 12 13 14 
outwear 0 1 3 4 5 6 7 8 9 
10 11 12 13 14 dress 0 1 2 3 
4 5 6 7 8 9 10 11 12 17 
18 skirt 15 16 17 18 trousers 15 16 19 
20 21 22 23 } PART _ STR = neckline 
_ left neckline _ right center _ front shoulder _ 
left shoulder _ right armpit _ left armpit _ right 
waistline _ left waistline _ right cuff _ left _ 
in cuff _ left _ out cuff _ right _ 
in cuff _ right _ out top _ hem _ 
left top _ hem _ right waistband _ left waistband 
_ right hemline _ left hemline _ right crotch bottom 
_ left _ in bottom _ left _ out bottom 
_ right _ in bottom _ right _ out IMAGE 
_ CATEGORY = blouse outwear dress skirt trousers 0 class 
FIConfig Config Configuration for training on the toy shapes dataset 
. Derives from the base Config class and overrides values 
specific to the toy shapes dataset . # Give the 
configuration a recognizable name NAME = FI # 数据集 名 
# Train on 1 GPU and 8 images per GPU 
. We can put multiple images on each # GPU 
because the images are small . Batch size is 8 
GPUs * images / GPU . GPU _ COUNT = 
1 IMAGES _ PER _ GPU = 1 NUM _ 
KEYPOINTS = len PART _ INDEX IMAGE _ CATEGORY # 
关键点 数目 KEYPOINT _ MASK _ SHAPE = 56 56 
# Number of classes including background NUM _ CLASSES = 
1 + 1 RPN _ TRAIN _ ANCHORS _ PER 
_ IMAGE = 100 VALIDATION _ STPES = 100 STEPS 
_ PER _ EPOCH = 1000 MINI _ MASK _ 
SHAPE = 56 56 KEYPOINT _ MASK _ POOL _ 
SIZE = 7 # Pooled ROIs POOL _ SIZE = 
7 MASK _ POOL _ SIZE = 14 MASK _ 
SHAPE = 28 28 WEIGHT _ LOSS = True KEYPOINT 
_ THRESHOLD = 0.005 常量 配置 记录 数据 类 关键点 
类 数据 类 和 关键 点 类 的 对应 关系 
config 类 记录 的 大部分 为 model 设置 无需 改动 
注意 设置 一下 NAME NUM _ KEYPOINTS 匹 配上 数据集 
一 数据 类 建立 1 关键点 标注 形式 回顾 一下 
之前 的 数据 集 介绍 在 非 关键 点 检测 
任务 中 我们 需要 的 数据 有 两种 a 原始 
的 图片 文件 b 图片 上 每个 instance 的 掩码 
但是 由于 Mask _ RCNN 会对 掩码 进行 一次 加工 
获取 每个 instance 的 坐标 框 即 实际上 还 需要 
c 每个 instance 的 坐标 框 既然 这里 要 检测 
关键点 那 我们 就 需要 d 图像 的 关键 点 
标注 key _ points num _ keypoints coordinates and visibility 
x y v num _ person num _ keypoints 3 
of num _ person 首先 我们 需要 明确 keypoints 从属 
于 某个 instance 即 上面 的 num _ person 的 
由来 人体 关键点 检测 为例 一个 instance 就是 一个人 而 
一个 instance 有 num _ keypoints 个 关键 点 每 
一个 点 由 3个 值 组成 横坐标 纵坐标 状态 其中 
状态 有三种 该类 不 存在 此 关键 点 被 遮挡 
可见 对于 COCO 而言 0 表示 这个 关键 点 没有 
标注 这种 情况 下 x = y = v = 
0 1 表示 这个 关键 点 标注 了 但是 不 
可见 被 遮挡 了 2 表示 这个 关键 点 标注 
了 同时 也 可见 在 不同 的 数据 集上 可能 
有 不同 的 数字 来 表达 这 三个点 但是 在 
此 框架 训练 中 建议 统一 到 COCO 的 标准 
避免 过多 的 修改 model 代码 主要 是 避免 修改 
关键 点 损失 函数 中的 代码 带来 不 必要 的 
意外 2 服装 关键点 标注 有了/nr 这些 基础 我们 以 
天池 的 服饰 关键点 定位 数据 为例 看一看 如何 设计 
Dataset class 具体 数据 说明 自行 查阅 上面 说明 本节 
重点 在 介绍 Mask RCNN 关键点 加 测 思路 而非 
数据 本身 其 文档 如下 我们 设计 的 Dataset class 
见 计算机 视觉 Mask RCNN _ 训练 网络 其一 数据集 
与 Dataset 类 目的 就是 基于 文档 信息 为 网络结构 
输送 数据 a 服装 类别 和 Mask RCNN 值得 注意 
的 是 Mask RCNN 的 分类 检测 Mask 生成 任务 
都是 多 分类 但是 关键 点 识别 由于 其 本身 
难度 更高 一个 类别 有 众多 关键点 不同 类别 关键点 
类型 之间 关系 不大 甚至 完全 不同 所以 建议 每 
一个 大 类 单独 训练 一个 model 检测 其 关键 
点 实际上 pose 关键点 检测 对应 过来 就是 检测 person 
这一个 类 的 框 Mask 以及 每 一个 instance 每 
一个 人 的 不同 部位 的 关键 点 实际/n 的/uj 
class/w 分类/n 值/n 有/v person/w 和/c 背景/n 两个/m 类/q 对应 
到 服饰 数据集 我们 需要 训练 5次 对 框 应 
五种 服装 b 服装 检测 框 服装 数据 标注 仅有 
关键点 但是 检测 框 对于 Mask RCNN 来说 是 必要 
的 因为 RPN 网络 需要 它 RPN 之后 的 回归 
网络 分支 可以 注释 掉 但是 RPN 是 网络 的 
主干 部分 不能 注释 所以 我们 采取 Mask RCNN 工程 
的 检测 框 生成 思路 利用 关键点 生成 检测 框 
由于 关键点 未必 在 服装 边缘 一般 是 在 的 
我们 的 检测 框 取 大 一点 尽量 完全 包含 
服装 下面 的 函数 见 utils . py 脚本 暂不 
涉及 这个 函数 只是 说 到了 贴上来 而已 def extract 
_ keypoint _ bboxes keypoints image _ size param keypoints 
instances keypoints _ per _ instance 3 param image _ 
size w h return bboxes = np . zeros keypoints 
. shape 0 4 dtype = np . int32 for 
i in range keypoints . shape 0 x = keypoints 
i 0 keypoints i 0 0 y = keypoints i 
1 keypoints i 1 0 x1 = x . min 
10 if x . min 10 0 else 0 y1 
= y . min 10 if y . min 10 
0 else 0 x2 = x . max + 11 
if x . max + 11 image _ size 0 
else image _ size 0 y2 = y . max 
+ 11 if y . max + 11 image _ 
size 1 else image _ size 1 bboxes i = 
np . array y1 x1 y2 x2 np . int32 
return bboxesc Mask 说明 服装 数据 是 没有 Mask 信息 
的 按照 Mask RCNN 论文 的 说法 掩码 使用 关键 
点 位置 为 1 其他 位置 为 0 的 形式 
即可 感觉 不 太 靠谱 而在 COCO 数据集 里 即 
本文 参考 工程 Mask _ RCNN _ Humanpose 掩码 信息 
使用 的 是 人 的 掩码 见 下图 我 在 
Dataset class 中生 成了 掩码 信息 作为 演示 在 build 
网络 中 取消 了 Mask 分支 下图 摘自 李沐/nr 博士 
的 手动 学习 深度 学习 可以 很 直观 的 理解 
我们 为什么 可以 把 Mask 分 支取 消掉 3 class 
FIDataset 正如 Dataset 注释 所说 要想 运行 自己 的 数据集 
我们 首先 要 实现 一个 方法 load _ shapes 根据 
数据集 取名 即可 收集 原始 图像 类别 信息 然后 实现 
两个 方法 load _ image load _ mask 分别 实现 
获取 单张 图片 数据 获取 单张 图片 对应 的 objs 
的 masks 和 classes 这样 基本 完成 了 数据集 类 
的 构建 对于 本 数据集 我们 使用 load _ FI 
方法 代替 load _ shapes 调用 self . add _ 
class 和 self . add _ image 记录 图片 类别 
信息 父 类 的 load _ image 会去 读取 self 
. image _ info 中 每张 图片 的 path 路径 
载入 图片 我们 不必 重写 保证 在 load _ FI 
中录 入了 即可 load _ mask 被 load _ keupoints 
取代 Mask _ RCNN _ Humanpose 做 了 这个 改动 
并 已经 捋顺 了 相关 调用 其 注释 如下 我们 
不 需要 mask 信息 返回 None 占位 即可 后面 需要 
将 网络 中 有关 Mask 信息 的 调用 注释 处理 
掉 这里 先不 介绍 Returns key _ points num _ 
keypoints coordinates and visibility x y v num _ person 
num _ keypoints 3 of num _ personmasks A bool 
array of shape height width instance count withone mask per 
instance . class _ ids a 1D array of class 
IDs of the instance masks here is always equal to 
num _ person 1 至此 我们 介绍 了 Dataset class 
的 目的 下面 给出 实现 见 FI _ train . 
py 由于 训练 时 需要 验证 集 而 我 截至 
撰文 时 没有 实现 验证 集 划分 用 训练 集 
冒充 验证 集 所以 load _ FI 的 参数 train 
_ data 没有意义 更新 会在 github 上 进行 后续 本文 
不予 修改 class FIDataset utils . Dataset Generates the shapes 
synthetic dataset . The dataset consists of simple shapes triangles 
squares circles placed randomly on a blank surface . The 
images are generated on the fly . No file access 
required . def load _ FI self train _ data 
= True Generate the requested number of synthetic images . 
count number of images to generate . height width the 
size of the generated images . if train _ data 
csv _ data = pd . concat pd . read 
_ csv . . / keypoint _ data / train1 
. csv pd . read _ csv . . / 
keypoint _ data / train2 . csv axis = 0 
ignore _ index = True # 忽略 索引 表示 不会 
直接 拼接 索引 会 重新 计算 行数 索引 class _ 
data = csv _ data csv _ data . image 
_ category . isin blouse # Add classes self . 
add _ class source = FI class _ id = 
1 class _ name = blouse # Add images for 
i in range class _ data . shape 0 annotation 
= class _ data . iloc i img _ path 
= os . path . join . . / keypoint 
_ data annotation . image _ id keypoints = np 
. array p . split _ for p in class 
_ data . iloc i 2 dtype = int PART 
_ INDEX IMAGE _ CATEGORY keypoints 1 + = 1 
self . add _ image source = FI image _ 
id = i path = img _ path annotations = 
keypoints def load _ keypoints self image _ id with 
_ mask = True Returns key _ points num _ 
keypoints coordinates and visibility x y v num _ person 
num _ keypoints 3 of num _ person masks A 
bool array of shape height width instance count with one 
mask per instance . class _ ids a 1D array 
of class IDs of the instance masks here is always 
equal to num _ person 1 key _ points = 
np . expand _ dims self . image _ info 
image _ id annotations 0 # 已知 图中 仅有 一个 
对象 class _ ids = np . array 1 if 
with _ mask annotations = self . image _ info 
image _ id annotations w h = image _ size 
self . image _ info image _ id path mask 
= np . zeros w h dtype = int mask 
annotations 1 annotations 0 = 1 return key _ points 
. copy np . expand _ dims mask 1 class 
_ ids return key _ points . copy None class 
_ ids 二 数据 类 读取 为了 验证 数据 类 
构建 的 正确性 我们 可以 直接 调用 接口 model . 
py 中的 load _ image _ gt _ keypoints 获取 
original _ image image _ meta gt _ class _ 
id gt _ bbox gt _ keypoint 等 信息 实际上 
在 真正 的 训练 中 程序 也 是 通过 这个 
函数 完成 Dataset class 中 的 数据 到 model 模型 
之间 的 传递 def load _ image _ gt _ 
keypoints dataset config image _ id augment = True use 
_ mini _ mask = False Load and return ground 
truth data for an image image keypoint _ mask keypoint 
_ weight mask bounding boxes . augment If true apply 
random image augmentation . Currently onlyhorizontal flipping is offered . 
use _ mini _ mask If False returns full size 
masks and keypoints that are the same heightand width as 
the original image . These can be big for e 
x a m p l e 1 0 2 4 
x 1 0 2 4 x 1 0 0 for 
100 instances . Mini masks are smaller typically 224x224 and 
are generated by extracting the bounding box of theobject and 
resizing it to MINI _ MASK _ SHAPE . Returns 
image height width 3 shape the original shape of the 
image before resizing and cropping . keypoints num _ person 
num _ keypoint 3 x y v v value is 
as belows 0 not visible and without annotations1 not visible 
but with annotations2 visible and with annotationsclass _ ids instance 
_ count Integer class IDsbbox instance _ count y1 x1 
y2 x2 mask height width instance _ count . The 
height and width are thoseof the image unless use _ 
mini _ mask is True in which case they aredefined 
in MINI _ MASK _ SHAPE . 在 visualize . 
py 模块 中 函数 display _ keypoints 可以 对接 上面 
函数 的 输出 直接 可视化 Dataset class 经由 load _ 
image _ gt _ keypoints 提取 的 结果 当然 并 
不是 直接 提取 该 函数 实际上 进行 了 一系列 的 
图像 预处理 这也 增加 了 我们 可视化 验证 正确性 的 
必要 流程 代码 如下 见 FI _ train . py 
config = FIConfig import visualize from model import log dataset 
= FIDataset dataset . load _ FI dataset . prepare 
original _ image image _ meta gt _ class _ 
id gt _ bbox gt _ keypoint = \ modellib 
. load _ image _ gt _ keypoints dataset FIConfig 
0 log original _ image original _ image log image 
_ meta image _ meta log gt _ class _ 
id gt _ class _ id log gt _ bbox 
gt _ bbox log gt _ keypoint gt _ keypoint 
visualize . display _ keypoints original _ image gt _ 
bbox gt _ keypoint gt _ class _ id dataset 
. class _ names 输出 图片 见下 可以 明确 的 
看见 至少 进行 了 padding 个 flip 两个 预处理 并非 
重点 不提 实现 了 自己 的 Dataset class 之后 使用 
model . load _ image _ gt _ keypoints 和 
visualize . display _ keypoints 进行 验证 保证 Dataset class 
的 正确性 三 修改 及 运行 模型 1 运行 模型 
步骤 data _ tra = FIDataset data _ tra . 
load _ FI data _ tra . prepare data _ 
val = FIDataset data _ val . load _ FI 
data _ val . prepare model = modellib . MaskRCNN 
mode = training config = config model _ dir = 
. / model . load _ weights . / mask 
_ rcnn _ coco . h5 by _ name = 
True exclude = mrcnn _ class _ logits mrcnn _ 
bbox _ fc mrcnn _ bbox mrcnn _ mask model 
. train data _ tra data _ val learning _ 
rate = config . LEARNING _ RATE / 10 epochs 
= 400 layers = heads 2 网络 细节 修改 服装 
关键点 和 Humanpose 数据 最大 的 不同 就 在于 我们 
没有 mask 掩码 数据 所以 我们 需要 对 原 model 
进行 修改 取 消掉 设计 mask 的 分支 注 意指 
的 是 Humanpose 代码 而非 原版 的 Mask RCNN 那个 
改 动起来 变化 太大 1 需要 添加 keypoint 标注 数据 
的 整个 预处理 分支 2 需要 实现 model 有关 keypoint 
的 损失 函数 在内 的 全部 处理 步骤 下面 给 
出 修改 之后 的 build 方法 由于 Mask RCNN 将 
各个 分支 损失 函数 直接 相加 所以 我们 直接 注释 
掉 Mask 分支 即可 不会 影响 代码 逻辑 程序 可以 
直接 正常 运行 def build self mode config Build Mask 
R CNN architecture . input _ shape The shape of 
the input image . mode Either training or inference . 
The inputs and outputs of the model differ accordingly . 
assert mode in training inference # Image size must be 
dividable by 2 multiple times h w = config . 
IMAGE _ SHAPE 2 if h / 2 * * 
6 = int h / 2 * * 6 or 
w / 2 * * 6 = int w / 
2 * * 6 raise Exception Image size must be 
dividable by 2 at least 6 times to avoid fractions 
when downscaling and upscaling . For example use 256 320 
384 448 512 . . . etc . # Inputs 
input _ image = KL . Input shape = config 
. IMAGE _ SHAPE . tolist name = input _ 
image input _ image _ meta = KL . Input 
shape = None name = input _ image _ meta 
if mode = = training # RPN GT input _ 
rpn _ match = KL . Input shape = None 
1 name = input _ rpn _ match dtype = 
tf . int32 input _ rpn _ bbox = KL 
. Input shape = None 4 name = input _ 
rpn _ bbox dtype = tf . float32 # Detection 
GT class IDs bounding boxes and masks # 1 . 
GT Class IDs zero padded input _ gt _ class 
_ ids = KL . Input shape = None name 
= input _ gt _ class _ ids dtype = 
tf . int32 # 2 . GT Boxes in pixels 
zero padded # batch MAX _ GT _ INSTANCES y1 
x1 y2 x2 in image coordinates input _ gt _ 
boxes = KL . Input shape = None 4 name 
= input _ gt _ boxes dtype = tf . 
float32 # Normalize coordinates h w = K . shape 
input _ image 1 K . shape input _ image 
2 image _ scale = K . cast K . 
stack h w h w axis = 0 tf . 
float32 gt _ boxes = KL . Lambda lambda x 
x / image _ scale name = gt _ boxes 
input _ gt _ boxes keypoint _ scale = K 
. cast K . stack w h 1 axis = 
0 tf . float32 input _ gt _ keypoints = 
KL . Input shape = None config . NUM _ 
KEYPOINTS 3 gt _ keypoints = KL . Lambda lambda 
x x / keypoint _ scale name = gt _ 
keypoints input _ gt _ keypoints # 3 . GT 
Masks zero padded # batch height width MAX _ GT 
_ INSTANCES # if config . USE _ MINI _ 
MASK # input _ gt _ masks = KL . 
Input # shape = config . MINI _ MASK _ 
SHAPE 0 # config . MINI _ MASK _ SHAPE 
1 None # name = input _ gt _ masks 
dtype = bool # # input _ gt _ keypoint 
_ masks = KL . Input # # shape = 
config . MINI _ MASK _ SHAPE 0 # # 
config . MINI _ MASK _ SHAPE 1 None config 
. NUM _ KEYPOINTS # # name = input _ 
gt _ keypoint _ masks dtype = bool # else 
# input _ gt _ masks = KL . Input 
# shape = config . IMAGE _ SHAPE 0 config 
. IMAGE _ SHAPE 1 None # name = input 
_ gt _ masks dtype = bool # input _ 
gt _ keypoint _ masks = KL . Input # 
shape = config . IMAGE _ SHAPE 0 config . 
IMAGE _ SHAPE 1 None config . NUM _ KEYPOINTS 
# name = input _ gt _ keypoint _ masks 
dtype = bool # input _ gt _ keypoint _ 
weigths = KL . Input # shape = None config 
. NUM _ KEYPOINTS name = input _ gt _ 
keypoint _ weights dtype = tf . int32 # Build 
the shared convolutional layers . # Bottom up Layers # 
Returns a list of the last layers of each stage 
5 in total . # Don t create the thead 
stage 5 so we pick the 4th item in the 
list . _ C2 C3 C4 C5 = resnet _ 
graph input _ image resnet101 stage5 = True # Top 
down Layers # TODO add assert to varify feature map 
sizes match what s in config P5 = KL . 
Conv2D 256 1 1 name = fpn _ c5p5 C5 
P4 = KL . Add name = fpn _ p4add 
KL . UpSampling2D size = 2 2 name = fpn 
_ p5upsampled P5 KL . Conv2D 256 1 1 name 
= fpn _ c4p4 C4 P3 = KL . Add 
name = fpn _ p3add KL . UpSampling2D size = 
2 2 name = fpn _ p4upsampled P4 KL . 
Conv2D 256 1 1 name = fpn _ c3p3 C3 
P2 = KL . Add name = fpn _ p2add 
KL . UpSampling2D size = 2 2 name = fpn 
_ p3upsampled P3 KL . Conv2D 256 1 1 name 
= fpn _ c2p2 C2 # Attach 3x3 conv to 
all P layers to get the final feature maps . 
P2 = KL . Conv2D 256 3 3 padding = 
SAME name = fpn _ p2 P2 P3 = KL 
. Conv2D 256 3 3 padding = SAME name = 
fpn _ p3 P3 P4 = KL . Conv2D 256 
3 3 padding = SAME name = fpn _ p4 
P4 P5 = KL . Conv2D 256 3 3 padding 
= SAME name = fpn _ p5 P5 # P6 
is used for the 5th anchor scale in RPN . 
Generated by # subsampling from P5 with stride of 2 
. P6 = KL . MaxPooling2D pool _ size = 
1 1 strides = 2 name = fpn _ p6 
P5 # Note that P6 is used in RPN but 
not in the classifier heads . rpn _ feature _ 
maps = P2 P3 P4 P5 P6 mrcnn _ feature 
_ maps = P2 P3 P4 P5 # Generate Anchors 
self . anchors = utils . generate _ pyramid _ 
anchors config . RPN _ ANCHOR _ SCALES config . 
RPN _ ANCHOR _ RATIOS config . BACKBONE _ SHAPES 
config . BACKBONE _ STRIDES config . RPN _ ANCHOR 
_ STRIDE # RPN Model rpn = build _ rpn 
_ model config . RPN _ ANCHOR _ STRIDE len 
config . RPN _ ANCHOR _ RATIOS 256 # Loop 
through pyramid layers layer _ outputs = # list of 
lists for p in rpn _ feature _ maps layer 
_ outputs . append rpn p # Concatenate layer outputs 
# Convert from list of lists of level outputs to 
list of lists # of outputs across levels . # 
e . g . a1 b1 c1 a2 b2 c2 
= a1 a2 b1 b2 c1 c2 output _ names 
= rpn _ class _ logits rpn _ class rpn 
_ bbox outputs = list zip * layer _ outputs 
outputs = KL . Concatenate axis = 1 name = 
n list o for o n in zip outputs output 
_ names rpn _ class _ logits rpn _ class 
rpn _ bbox = outputs # Generate proposals # Proposals 
are batch N y1 x1 y2 x2 in normalized coordinates 
# and zero padded . proposal _ count = config 
. POST _ NMS _ ROIS _ TRAINING if mode 
= = training \ else config . POST _ NMS 
_ ROIS _ INFERENCE rpn _ rois = ProposalLayer proposal 
_ count = proposal _ count nms _ threshold = 
config . RPN _ NMS _ THRESHOLD name = ROI 
anchors = self . anchors config = config rpn _ 
class rpn _ bbox if mode = = training # 
Class ID mask to mark class IDs supported by the 
dataset the image # came from . _ _ _ 
active _ class _ ids = KL . Lambda lambda 
x parse _ image _ meta _ graph x mask 
= None None None None input _ image _ meta 
if not config . USE _ RPN _ ROIS # 
Ignore predicted ROIs and use ROIs provided as an input 
. input _ rois = KL . Input shape = 
config . POST _ NMS _ ROIS _ TRAINING 4 
name = input _ roi dtype = np . int32 
# Normalize coordinates to 0 1 range . target _ 
rois = KL . Lambda lambda x K . cast 
x tf . float32 / image _ scale 4 input 
_ rois else target _ rois = rpn _ rois 
# Generate detection targets # Subsamples proposals and generates target 
outputs for training # Note that proposal class IDs gt 
_ boxes and gt _ masks are zero # padded 
. Equally returned rois and targets are zero padded . 
# Every rois corresond to one target # rois target 
_ class _ ids target _ bbox target _ mask 
= \ # D e t e c t i 
o n T a r g e t L a 
y e r config name = proposal _ targets # 
target _ rois input _ gt _ class _ ids 
gt _ boxes input _ gt _ masks # Generate 
detection targets # Subsamples proposals and generates target outputs for 
training # Note that proposal class IDs gt _ boxes 
gt _ keypoint _ masks and gt _ keypoint _ 
weights are zero # padded . Equally returned rois and 
targets are zero padded . rois target _ class _ 
ids target _ bbox target _ keypoint target _ keypoint 
_ weight = \ D e t e c t 
i o n K e y p o i n 
t T a r g e t L a y 
e r config name = proposal _ targets \ target 
_ rois input _ gt _ class _ ids gt 
_ boxes gt _ keypoints # Network Heads # TODO 
verify that this handles zero padded ROIs mrcnn _ class 
_ logits mrcnn _ class mrcnn _ bbox = \ 
fpn _ classifier _ graph rois mrcnn _ feature _ 
maps config . IMAGE _ SHAPE config . POOL _ 
SIZE config . NUM _ CLASSES # mrcnn _ mask 
= build _ fpn _ mask _ graph rois mrcnn 
_ feature _ maps # config . IMAGE _ SHAPE 
# config . MASK _ POOL _ SIZE # config 
. NUM _ CLASSES # shape batch _ size num 
_ roi num _ keypoint 56 * 56 keypoint _ 
mrcnn _ mask = build _ fpn _ keypoint _ 
graph rois mrcnn _ feature _ maps config . IMAGE 
_ SHAPE config . KEYPOINT _ MASK _ POOL _ 
SIZE config . NUM _ KEYPOINTS # TODO clean up 
use tf . identify if necessary output _ rois = 
KL . Lambda lambda x x * 1 name = 
output _ rois rois # keypoint _ mrcnn _ mask 
= KL . Lambda lambda x x * 1 name 
= keypoint _ mrcnn _ mask keypoint _ mrcnn _ 
mask # Losses rpn _ class _ loss = KL 
. Lambda lambda x rpn _ class _ loss _ 
graph * x name = rpn _ class _ loss 
input _ rpn _ match rpn _ class _ logits 
rpn _ bbox _ loss = KL . Lambda lambda 
x rpn _ bbox _ loss _ graph config * 
x name = rpn _ bbox _ loss input _ 
rpn _ bbox input _ rpn _ match rpn _ 
bbox class _ loss = KL . Lambda lambda x 
mrcnn _ class _ loss _ graph * x name 
= mrcnn _ class _ loss target _ class _ 
ids mrcnn _ class _ logits active _ class _ 
ids bbox _ loss = KL . Lambda lambda x 
mrcnn _ bbox _ loss _ graph * x name 
= mrcnn _ bbox _ loss target _ bbox target 
_ class _ ids mrcnn _ bbox # mask _ 
loss = KL . Lambda lambda x mrcnn _ mask 
_ loss _ graph * x # name = mrcnn 
_ mask _ loss # target _ mask target _ 
class _ ids mrcnn _ mask keypoint _ loss = 
KL . Lambda lambda x keypoint _ mrcnn _ mask 
_ loss _ graph * x weight _ loss = 
config . WEIGHT _ LOSS name = keypoint _ mrcnn 
_ mask _ loss target _ keypoint target _ keypoint 
_ weight target _ class _ ids keypoint _ mrcnn 
_ mask target _ keypoints batch TRAIN _ ROIS _ 
PER _ IMAGE NUM _ KEYPOINTS Keypoint labels cropped to 
bbox boundaries and resized to neural network output size . 
Maps keypoints from the half open interval x1 x2 on 
continuous image coordinates to the closed interval 0 HEATMAP _ 
SIZE 1 target _ keypoint _ weights batch TRAIN _ 
ROIS _ PER _ IMAGE NUM _ KEYPOINTS bool type 
Keypoint _ weights 0 isn t visible 1 visilble # 
test _ target _ keypoint _ mask = test _ 
keypoint _ mrcnn _ mask _ loss _ graph target 
_ keypoint target _ keypoint _ weight # target _ 
class _ ids keypoint _ mrcnn _ mask # keypoint 
_ weight _ loss = KL . Lambda lambda x 
keypoint _ weight _ loss _ graph * x name 
= keypoint _ weight _ loss # target _ keypoint 
_ weight keypoint _ weight _ logits target _ class 
_ ids # Model generated # batch _ images batch 
_ image _ meta batch _ rpn _ match batch 
_ rpn _ bbox batch _ gt _ class _ 
ids \ # batch _ gt _ boxes batch _ 
gt _ keypoint batch _ gt _ masks inputs = 
input _ image input _ image _ meta input _ 
rpn _ match input _ rpn _ bbox input _ 
gt _ class _ ids input _ gt _ boxes 
input _ gt _ keypoints if not config . USE 
_ RPN _ ROIS inputs . append input _ rois 
# add test _ target _ keypoint _ mask in 
the output for test the keypoint loss function outputs = 
rpn _ class _ logits rpn _ class rpn _ 
bbox mrcnn _ class _ logits mrcnn _ class mrcnn 
_ bbox keypoint _ mrcnn _ mask rpn _ rois 
output _ rois rpn _ class _ loss rpn _ 
bbox _ loss class _ loss bbox _ loss keypoint 
_ loss # + test _ target _ keypoint _ 
mask for test the keypoint loss graph model = KM 
. Model inputs outputs name = mask _ keypoint _ 
mrcnn else # Network Heads # Proposal classifier and BBox 
regressor heads mrcnn _ class _ logits mrcnn _ class 
mrcnn _ bbox = \ fpn _ classifier _ graph 
rpn _ rois mrcnn _ feature _ maps config . 
IMAGE _ SHAPE config . POOL _ SIZE config . 
NUM _ CLASSES # Detections # output is # detections 
batch num _ detections y1 x1 y2 x2 class _ 
id score in image coordinates # keypoint _ weights batch 
num _ detections num _ keypoints detections = DetectionLayer config 
name = mrcnn _ detection rpn _ rois mrcnn _ 
class mrcnn _ bbox input _ image _ meta # 
Convert boxes to normalized coordinates # TODO let DetectionLayer return 
normalized coordinates to avoid # unnecessary conversions h w = 
config . IMAGE _ SHAPE 2 detection _ boxes = 
KL . Lambda lambda x x . . . 4 
/ np . array h w h w detections # 
Create masks for detections mrcnn _ mask = build _ 
fpn _ mask _ graph detection _ boxes mrcnn _ 
feature _ maps config . IMAGE _ SHAPE config . 
MASK _ POOL _ SIZE config . NUM _ CLASSES 
keypoint _ mrcnn = build _ fpn _ keypoint _ 
graph detection _ boxes mrcnn _ feature _ maps config 
. IMAGE _ SHAPE config . KEYPOINT _ MASK _ 
POOL _ SIZE config . NUM _ KEYPOINTS # shape 
Batch N _ ROI Number _ Keypoint height * width 
keypoint _ mcrcnn _ prob = KL . Activation softmax 
name = mrcnn _ prob keypoint _ mrcnn model = 
KM . Model input _ image input _ image _ 
meta detections mrcnn _ class mrcnn _ bbox rpn _ 
rois rpn _ class rpn _ bbox mrcnn _ mask 
keypoint _ mcrcnn _ prob name = keypoint _ mask 
_ rcnn # Add multi GPU support . if config 
. GPU _ COUNT 1 from parallel _ model import 
ParallelModel model = ParallelModel model config . GPU _ COUNT 
return model 在 model . compile 方法 中 我们 可以 
看到 有关 损失 函数 添加 的 细节 # Add Losses 
# First clear previously set losses to avoid duplication self 
. keras _ model . _ losses = self . 
keras _ model . _ per _ input _ losses 
= { } loss _ names = rpn _ class 
_ loss rpn _ bbox _ loss mrcnn _ class 
_ loss mrcnn _ bbox _ loss keypoint _ mrcnn 
_ mask _ loss for name in loss _ names 
layer = self . keras _ model . get _ 
layer name if layer . output in self . keras 
_ model . losses continue self . keras _ model 
. add _ loss tf . reduce _ mean layer 
. output keepdims = True # Add L2 Regularization # 
Skip gamma and beta weights of batch normalization layers . 
reg _ losses = keras . regularizers . l2 self 
. config . WEIGHT _ DECAY w / tf . 
cast tf . size w tf . float32 for w 
in self . keras _ model . trainable _ weights 
if gamma not in w . name and beta not 
in w . name self . keras _ model . 
add _ loss tf . add _ n reg _ 
losses 至此 keypoints 检测 分支 添加 完毕 直接 训练 即可 
3 keypoint 损失 函 数本 损失 函数 也是 原版 Mask 
RCNN 没有 实现 经由 Humanpose 工程 实现 的 我们 无需 
改动 其 原理 就是 将 true propose 的 目标 中 
的 可见 关键 点 进行 稀疏 交叉 熵 计算 之所以 
强调 是 稀疏 交叉 熵 因为 每 一个 关键 点 
其 使用 一个 56 * 56 的 向量 表示 大部分 
位置 为 0 仅 关键 点 位置 为 1 def 
keypoint _ mrcnn _ mask _ loss _ graph target 
_ keypoints target _ keypoint _ weights target _ class 
_ ids pred _ keypoints _ logit weight _ loss 
= True mask _ shape = 56 56 number _ 
point = 13 Mask softmax cross entropy loss for the 
keypoint head . 积极 区域 的 关键 点 才 参与 
loss 计算 真实 目标 类别 target _ class _ ids 
大于 0 的 位置 可见 点 才 参与 loss 运算 
真实 关键点 权重 target _ keypoint _ weights 为 1 
的 位置 target _ keypoints 真实 关键点 坐标 pred _ 
keypoints _ logit 预测出 关键点 生成 的 热 图 target 
_ keypoints batch TRAIN _ ROIS _ PER _ IMAGE 
NUM _ KEYPOINTS Keypoint labels cropped to bbox boundaries and 
resized to neural network output size . Maps keypoints from 
the half open interval x1 x2 on continuous image coordinates 
to the closed interval 0 HEATMAP _ SIZE 1 target 
_ keypoint _ weights batch TRAIN _ ROIS _ PER 
_ IMAGE NUM _ KEYPOINTS bool type Keypoint _ weights 
0 isn t visible 1 visilble target _ class _ 
ids batch TRAIN _ ROIS _ PER _ IMAGE . 
Integer class IDs . pred _ keypoints _ logit batch 
_ size num _ roi num _ keypoint 56 * 
56 # Reshape for simplicity . Merge first two dimensions 
into one . # shape N target _ class _ 
ids = K . reshape target _ class _ ids 
1 # Only positive person ROIs contribute to the loss 
. And only # the people specific mask of each 
ROI . positive _ people _ ix = tf . 
where target _ class _ ids 0 0 positive _ 
people _ ids = tf . cast tf . gather 
target _ class _ ids positive _ people _ ix 
tf . int64 # # # Step 1 Get the 
positive target and predict keypoint masks # reshape target _ 
keypoint _ weights to N num _ keypoints target _ 
keypoint _ weights = K . reshape target _ keypoint 
_ weights 1 number _ point # 点 的 可见度 
# reshape target _ keypoint _ masks to N num 
_ keypoints target _ keypoints = K . reshape target 
_ keypoints 1 number _ point # 点 的 坐标 
# reshape pred _ keypoint _ masks to N number 
_ point 56 * 56 pred _ keypoints _ logit 
= K . reshape pred _ keypoints _ logit 1 
number _ point mask _ shape 0 * mask _ 
shape 1 # 推荐 区 特征 图 # Gather the 
keypoint masks target and predict that contribute to loss # 
shape N _ positive number _ point positive _ target 
_ keypoints = tf . cast tf . gather target 
_ keypoints positive _ people _ ix tf . int32 
# shape N _ positive number _ point 56 * 
56 positive _ pred _ keypoints _ logit = tf 
. gather pred _ keypoints _ logit positive _ people 
_ ix # positive target _ keypoint _ weights to 
N _ positive number _ point positive _ keypoint _ 
weights = tf . cast tf . gather target _ 
keypoint _ weights positive _ people _ ix tf . 
float32 loss = K . switch tf . size positive 
_ target _ keypoints 0 lambda tf . nn . 
sparse _ softmax _ cross _ entropy _ with _ 
logits logits = positive _ pred _ keypoints _ logit 
labels = positive _ target _ keypoints lambda tf . 
constant 0.0 loss = loss * positive _ keypoint _ 
weights if weight _ loss loss = K . switch 
tf . reduce _ sum positive _ keypoint _ weights 
0 lambda tf . reduce _ sum loss / tf 
. reduce _ sum positive _ keypoint _ weights lambda 
tf . constant 0.0 else loss = K . mean 
loss loss = tf . reshape loss 1 1 return 
loss 我们 随机 选择 一张 图片 运行 demo _ detect 
. ipynb 脚本 查看 训练 效果 下图 Github 地址 Mask _ RCNNMask _ RCNN _ KeyPoints 
计算机 视觉 Mask RCNN _ 论文 学习 计算机 视觉 Mask 
RCNN _ 项目 文档 翻译 计算机 视觉 Mask RCNN _ 
推断 网络 其一 总览 计算机 视觉 Mask RCNN _ 推断 
网络 其二 基于 ReNet101 的 FPN 共享 网络 计算机 视觉 
Mask RCNN _ 推断 网络 其三 RPN 锚 框 处理 
和 Proposal 生成 计算机 视觉 Mask RCNN _ 推断 网络 
其四 FPN 和 ROIAlign 的 耦合 计算机 视觉 Mask RCNN 
_ 推断 网络 其五 目标 检测 结果 精炼 计算机 视觉 
Mask RCNN _ 推断 网络 其 六 Mask 生成 计算机 
视觉 Mask RCNN _ 推断 网络 终篇 使用 detect 方法 
进行 推断 计算机 视觉 Mask RCNN _ 锚 框 生成 
计算机 视觉 Mask RCNN _ 训练 网络 其一 数据集 与 
Dataset 类 计算机 视觉 Mask RCNN _ 训练 网络 其二 
train 网络结构 & 损失 函数 计算机 视觉 Mask RCNN _ 
训练 网络 其三 训练 Model 原 论文 中 提到 过 
Mask _ RCNN 是 可以 进行 关键点 检测 的 不过 
我们 学习 的 这个 工程 并 没有 添加 关键点 检测 
分支 而 有人 基于 本 工程 进行 了 完善 Mask 
_ RCNN _ Humanpose 本文 我们 将 简要 的 了解 
如何 将 关键点 识别 分支 添 加进 模型 更进一步 的 
我们 将 尝试 使用 Mask _ RCNN 对 实际 数据 
进行 识别 零 配置 相关 import os import numpy as 
np import pandas as pd from PIL import Image import 
utils as utils import model as modellib from config import 
Config PART _ INDEX = { blouse 0 1 2 
3 4 5 6 9 10 11 12 13 14 
outwear 0 1 3 4 5 6 7 8 9 
10 11 12 13 14 dress 0 1 2 3 
4 5 6 7 8 9 10 11 12 17 
18 skirt 15 16 17 18 trousers 15 16 19 
20 21 22 23 } PART _ STR = neckline 
_ left neckline _ right center _ front shoulder _ 
left shoulder _ right armpit _ left armpit _ right 
waistline _ left waistline _ right cuff _ left _ 
in cuff _ left _ out cuff _ right _ 
in cuff _ right _ out top _ hem _ 
left top _ hem _ right waistband _ left waistband 
_ right hemline _ left hemline _ right crotch bottom 
_ left _ in bottom _ left _ out bottom 
_ right _ in bottom _ right _ out IMAGE 
_ CATEGORY = blouse outwear dress skirt trousers 0 class 
FIConfig Config Configuration for training on the toy shapes dataset 
. Derives from the base Config class and overrides values 
specific to the toy shapes dataset . # Give the 
configuration a recognizable name NAME = FI # 数据集 名 
# Train on 1 GPU and 8 images per GPU 
. We can put multiple images on each # GPU 
because the images are small . Batch size is 8 
GPUs * images / GPU . GPU _ COUNT = 
1 IMAGES _ PER _ GPU = 1 NUM _ 
KEYPOINTS = len PART _ INDEX IMAGE _ CATEGORY # 
关键点 数目 KEYPOINT _ MASK _ SHAPE = 56 56 
# Number of classes including background NUM _ CLASSES = 
1 + 1 RPN _ TRAIN _ ANCHORS _ PER 
_ IMAGE = 100 VALIDATION _ STPES = 100 STEPS 
_ PER _ EPOCH = 1000 MINI _ MASK _ 
SHAPE = 56 56 KEYPOINT _ MASK _ POOL _ 
SIZE = 7 # Pooled ROIs POOL _ SIZE = 
7 MASK _ POOL _ SIZE = 14 MASK _ 
SHAPE = 28 28 WEIGHT _ LOSS = True KEYPOINT 
_ THRESHOLD = 0.005 常量 配置 记录 数据 类 关键点 
类 数据 类 和 关键 点 类 的 对应 关系 
config 类 记录 的 大部分 为 model 设置 无需 改动 
注意 设置 一下 NAME NUM _ KEYPOINTS 匹 配上 数据集 
一 数据 类 建立 1 关键点 标注 形式 回顾 一下 
之前 的 数据 集 介绍 在 非 关键 点 检测 
任务 中 我们 需要 的 数据 有 两种 a 原始 
的 图片 文件 b 图片 上 每个 instance 的 掩码 
但是 由于 Mask _ RCNN 会对 掩码 进行 一次 加工 
获取 每个 instance 的 坐标 框 即 实际上 还 需要 
c 每个 instance 的 坐标 框 既然 这里 要 检测 
关键点 那 我们 就 需要 d 图像 的 关键 点 
标注 key _ points num _ keypoints coordinates and visibility 
x y v num _ person num _ keypoints 3 
of num _ person 首先 我们 需要 明确 keypoints 从属 
于 某个 instance 即 上面 的 num _ person 的 
由来 人体 关键点 检测 为例 一个 instance 就是 一个人 而 
一个 instance 有 num _ keypoints 个 关键 点 每 
一个 点 由 3个 值 组成 横坐标 纵坐标 状态 其中 
状态 有三种 该类 不 存在 此 关键 点 被 遮挡 
可见 对于 COCO 而言 0 表示 这个 关键 点 没有 
标注 这种 情况 下 x = y = v = 
0 1 表示 这个 关键 点 标注 了 但是 不 
可见 被 遮挡 了 2 表示 这个 关键 点 标注 
了 同时 也 可见 在 不同 的 数据 集上 可能 
有 不同 的 数字 来 表达 这 三个点 但是 在 
此 框架 训练 中 建议 统一 到 COCO 的 标准 
避免 过多 的 修改 model 代码 主要 是 避免 修改 
关键 点 损失 函数 中的 代码 带来 不 必要 的 
意外 2 服装 关键点 标注 有了/nr 这些 基础 我们 以 
天池 的 服饰 关键点 定位 数据 为例 看一看 如何 设计 
Dataset class 具体 数据 说明 自行 查阅 上面 说明 本节 
重点 在 介绍 Mask RCNN 关键点 加 测 思路 而非 
数据 本身 其 文档 如下 我们 设计 的 Dataset class 
见 计算机 视觉 Mask RCNN _ 训练 网络 其一 数据集 
与 Dataset 类 目的 就是 基于 文档 信息 为 网络结构 
输送 数据 a 服装 类别 和 Mask RCNN 值得 注意 
的 是 Mask RCNN 的 分类 检测 Mask 生成 任务 
都是 多 分类 但是 关键 点 识别 由于 其 本身 
难度 更高 一个 类别 有 众多 关键点 不同 类别 关键点 
类型 之间 关系 不大 甚至 完全 不同 所以 建议 每 
一个 大 类 单独 训练 一个 model 检测 其 关键 
点 实际上 pose 关键点 检测 对应 过来 就是 检测 person 
这一个 类 的 框 Mask 以及 每 一个 instance 每 
一个 人 的 不同 部位 的 关键 点 实际/n 的/uj 
class/w 分类/n 值/n 有/v person/w 和/c 背景/n 两个/m 类/q 对应 
到 服饰 数据集 我们 需要 训练 5次 对 框 应 
五种 服装 b 服装 检测 框 服装 数据 标注 仅有 
关键点 但是 检测 框 对于 Mask RCNN 来说 是 必要 
的 因为 RPN 网络 需要 它 RPN 之后 的 回归 
网络 分支 可以 注释 掉 但是 RPN 是 网络 的 
主干 部分 不能 注释 所以 我们 采取 Mask RCNN 工程 
的 检测 框 生成 思路 利用 关键点 生成 检测 框 
由于 关键点 未必 在 服装 边缘 一般 是 在 的 
我们 的 检测 框 取 大 一点 尽量 完全 包含 
服装 下面 的 函数 见 utils . py 脚本 暂不 
涉及 这个 函数 只是 说 到了 贴上来 而已 def extract 
_ keypoint _ bboxes keypoints image _ size param keypoints 
instances keypoints _ per _ instance 3 param image _ 
size w h return bboxes = np . zeros keypoints 
. shape 0 4 dtype = np . int32 for 
i in range keypoints . shape 0 x = keypoints 
i 0 keypoints i 0 0 y = keypoints i 
1 keypoints i 1 0 x1 = x . min 
10 if x . min 10 0 else 0 y1 
= y . min 10 if y . min 10 
0 else 0 x2 = x . max + 11 
if x . max + 11 image _ size 0 
else image _ size 0 y2 = y . max 
+ 11 if y . max + 11 image _ 
size 1 else image _ size 1 bboxes i = 
np . array y1 x1 y2 x2 np . int32 
return bboxesc Mask 说明 服装 数据 是 没有 Mask 信息 
的 按照 Mask RCNN 论文 的 说法 掩码 使用 关键 
点 位置 为 1 其他 位置 为 0 的 形式 
即可 感觉 不 太 靠谱 而在 COCO 数据集 里 即 
本文 参考 工程 Mask _ RCNN _ Humanpose 掩码 信息 
使用 的 是 人 的 掩码 见 下图 我 在 
Dataset class 中生 成了 掩码 信息 作为 演示 在 build 
网络 中 取消 了 Mask 分支 下图 摘自 李沐/nr 博士 
的 手动 学习 深度 学习 可以 很 直观 的 理解 
我们 为什么 可以 把 Mask 分 支取 消掉 3 class 
FIDataset 正如 Dataset 注释 所说 要想 运行 自己 的 数据集 
我们 首先 要 实现 一个 方法 load _ shapes 根据 
数据集 取名 即可 收集 原始 图像 类别 信息 然后 实现 
两个 方法 load _ image load _ mask 分别 实现 
获取 单张 图片 数据 获取 单张 图片 对应 的 objs 
的 masks 和 classes 这样 基本 完成 了 数据集 类 
的 构建 对于 本 数据集 我们 使用 load _ FI 
方法 代替 load _ shapes 调用 self . add _ 
class 和 self . add _ image 记录 图片 类别 
信息 父 类 的 load _ image 会去 读取 self 
. image _ info 中 每张 图片 的 path 路径 
载入 图片 我们 不必 重写 保证 在 load _ FI 
中录 入了 即可 load _ mask 被 load _ keupoints 
取代 Mask _ RCNN _ Humanpose 做 了 这个 改动 
并 已经 捋顺 了 相关 调用 其 注释 如下 我们 
不 需要 mask 信息 返回 None 占位 即可 后面 需要 
将 网络 中 有关 Mask 信息 的 调用 注释 处理 
掉 这里 先不 介绍 Returns key _ points num _ 
keypoints coordinates and visibility x y v num _ person 
num _ keypoints 3 of num _ personmasks A bool 
array of shape height width instance count withone mask per 
instance . class _ ids a 1D array of class 
IDs of the instance masks here is always equal to 
num _ person 1 至此 我们 介绍 了 Dataset class 
的 目的 下面 给出 实现 见 FI _ train . 
py 由于 训练 时 需要 验证 集 而 我 截至 
撰文 时 没有 实现 验证 集 划分 用 训练 集 
冒充 验证 集 所以 load _ FI 的 参数 train 
_ data 没有意义 更新 会在 github 上 进行 后续 本文 
不予 修改 class FIDataset utils . Dataset Generates the shapes 
synthetic dataset . The dataset consists of simple shapes triangles 
squares circles placed randomly on a blank surface . The 
images are generated on the fly . No file access 
required . def load _ FI self train _ data 
= True Generate the requested number of synthetic images . 
count number of images to generate . height width the 
size of the generated images . if train _ data 
csv _ data = pd . concat pd . read 
_ csv . . / keypoint _ data / train1 
. csv pd . read _ csv . . / 
keypoint _ data / train2 . csv axis = 0 
ignore _ index = True # 忽略 索引 表示 不会 
直接 拼接 索引 会 重新 计算 行数 索引 class _ 
data = csv _ data csv _ data . image 
_ category . isin blouse # Add classes self . 
add _ class source = FI class _ id = 
1 class _ name = blouse # Add images for 
i in range class _ data . shape 0 annotation 
= class _ data . iloc i img _ path 
= os . path . join . . / keypoint 
_ data annotation . image _ id keypoints = np 
. array p . split _ for p in class 
_ data . iloc i 2 dtype = int PART 
_ INDEX IMAGE _ CATEGORY keypoints 1 + = 1 
self . add _ image source = FI image _ 
id = i path = img _ path annotations = 
keypoints def load _ keypoints self image _ id with 
_ mask = True Returns key _ points num _ 
keypoints coordinates and visibility x y v num _ person 
num _ keypoints 3 of num _ person masks A 
bool array of shape height width instance count with one 
mask per instance . class _ ids a 1D array 
of class IDs of the instance masks here is always 
equal to num _ person 1 key _ points = 
np . expand _ dims self . image _ info 
image _ id annotations 0 # 已知 图中 仅有 一个 
对象 class _ ids = np . array 1 if 
with _ mask annotations = self . image _ info 
image _ id annotations w h = image _ size 
self . image _ info image _ id path mask 
= np . zeros w h dtype = int mask 
annotations 1 annotations 0 = 1 return key _ points 
. copy np . expand _ dims mask 1 class 
_ ids return key _ points . copy None class 
_ ids 二 数据 类 读取 为了 验证 数据 类 
构建 的 正确性 我们 可以 直接 调用 接口 model . 
py 中的 load _ image _ gt _ keypoints 获取 
original _ image image _ meta gt _ class _ 
id gt _ bbox gt _ keypoint 等 信息 实际上 
在 真正 的 训练 中 程序 也 是 通过 这个 
函数 完成 Dataset class 中 的 数据 到 model 模型 
之间 的 传递 def load _ image _ gt _ 
keypoints dataset config image _ id augment = True use 
_ mini _ mask = False Load and return ground 
truth data for an image image keypoint _ mask keypoint 
_ weight mask bounding boxes . augment If true apply 
random image augmentation . Currently onlyhorizontal flipping is offered . 
use _ mini _ mask If False returns full size 
masks and keypoints that are the same heightand width as 
the original image . These can be big for e 
x a m p l e 1 0 2 4 
x 1 0 2 4 x 1 0 0 for 
100 instances . Mini masks are smaller typically 224x224 and 
are generated by extracting the bounding box of theobject and 
resizing it to MINI _ MASK _ SHAPE . Returns 
image height width 3 shape the original shape of the 
image before resizing and cropping . keypoints num _ person 
num _ keypoint 3 x y v v value is 
as belows 0 not visible and without annotations1 not visible 
but with annotations2 visible and with annotationsclass _ ids instance 
_ count Integer class IDsbbox instance _ count y1 x1 
y2 x2 mask height width instance _ count . The 
height and width are thoseof the image unless use _ 
mini _ mask is True in which case they aredefined 
in MINI _ MASK _ SHAPE . 在 visualize . 
py 模块 中 函数 display _ keypoints 可以 对接 上面 
函数 的 输出 直接 可视化 Dataset class 经由 load _ 
image _ gt _ keypoints 提取 的 结果 当然 并 
不是 直接 提取 该 函数 实际上 进行 了 一系列 的 
图像 预处理 这也 增加 了 我们 可视化 验证 正确性 的 
必要 流程 代码 如下 见 FI _ train . py 
config = FIConfig import visualize from model import log dataset 
= FIDataset dataset . load _ FI dataset . prepare 
original _ image image _ meta gt _ class _ 
id gt _ bbox gt _ keypoint = \ modellib 
. load _ image _ gt _ keypoints dataset FIConfig 
0 log original _ image original _ image log image 
_ meta image _ meta log gt _ class _ 
id gt _ class _ id log gt _ bbox 
gt _ bbox log gt _ keypoint gt _ keypoint 
visualize . display _ keypoints original _ image gt _ 
bbox gt _ keypoint gt _ class _ id dataset 
. class _ names 输出 图片 见下 可以 明确 的 
看见 至少 进行 了 padding 个 flip 两个 预处理 并非 
重点 不提 实现 了 自己 的 Dataset class 之后 使用 
model . load _ image _ gt _ keypoints 和 
visualize . display _ keypoints 进行 验证 保证 Dataset class 
的 正确性 三 修改 及 运行 模型 1 运行 模型 
步骤 data _ tra = FIDataset data _ tra . 
load _ FI data _ tra . prepare data _ 
val = FIDataset data _ val . load _ FI 
data _ val . prepare model = modellib . MaskRCNN 
mode = training config = config model _ dir = 
. / model . load _ weights . / mask 
_ rcnn _ coco . h5 by _ name = 
True exclude = mrcnn _ class _ logits mrcnn _ 
bbox _ fc mrcnn _ bbox mrcnn _ mask model 
. train data _ tra data _ val learning _ 
rate = config . LEARNING _ RATE / 10 epochs 
= 400 layers = heads 2 网络 细节 修改 服装 
关键点 和 Humanpose 数据 最大 的 不同 就 在于 我们 
没有 mask 掩码 数据 所以 我们 需要 对 原 model 
进行 修改 取 消掉 设计 mask 的 分支 注 意指 
的 是 Humanpose 代码 而非 原版 的 Mask RCNN 那个 
改 动起来 变化 太大 1 需要 添加 keypoint 标注 数据 
的 整个 预处理 分支 2 需要 实现 model 有关 keypoint 
的 损失 函数 在内 的 全部 处理 步骤 下面 给 
出 修改 之后 的 build 方法 由于 Mask RCNN 将 
各个 分支 损失 函数 直接 相加 所以 我们 直接 注释 
掉 Mask 分支 即可 不会 影响 代码 逻辑 程序 可以 
直接 正常 运行 def build self mode config Build Mask 
R CNN architecture . input _ shape The shape of 
the input image . mode Either training or inference . 
The inputs and outputs of the model differ accordingly . 
assert mode in training inference # Image size must be 
dividable by 2 multiple times h w = config . 
IMAGE _ SHAPE 2 if h / 2 * * 
6 = int h / 2 * * 6 or 
w / 2 * * 6 = int w / 
2 * * 6 raise Exception Image size must be 
dividable by 2 at least 6 times to avoid fractions 
when downscaling and upscaling . For example use 256 320 
384 448 512 . . . etc . # Inputs 
input _ image = KL . Input shape = config 
. IMAGE _ SHAPE . tolist name = input _ 
image input _ image _ meta = KL . Input 
shape = None name = input _ image _ meta 
if mode = = training # RPN GT input _ 
rpn _ match = KL . Input shape = None 
1 name = input _ rpn _ match dtype = 
tf . int32 input _ rpn _ bbox = KL 
. Input shape = None 4 name = input _ 
rpn _ bbox dtype = tf . float32 # Detection 
GT class IDs bounding boxes and masks # 1 . 
GT Class IDs zero padded input _ gt _ class 
_ ids = KL . Input shape = None name 
= input _ gt _ class _ ids dtype = 
tf . int32 # 2 . GT Boxes in pixels 
zero padded # batch MAX _ GT _ INSTANCES y1 
x1 y2 x2 in image coordinates input _ gt _ 
boxes = KL . Input shape = None 4 name 
= input _ gt _ boxes dtype = tf . 
float32 # Normalize coordinates h w = K . shape 
input _ image 1 K . shape input _ image 
2 image _ scale = K . cast K . 
stack h w h w axis = 0 tf . 
float32 gt _ boxes = KL . Lambda lambda x 
x / image _ scale name = gt _ boxes 
input _ gt _ boxes keypoint _ scale = K 
. cast K . stack w h 1 axis = 
0 tf . float32 input _ gt _ keypoints = 
KL . Input shape = None config . NUM _ 
KEYPOINTS 3 gt _ keypoints = KL . Lambda lambda 
x x / keypoint _ scale name = gt _ 
keypoints input _ gt _ keypoints # 3 . GT 
Masks zero padded # batch height width MAX _ GT 
_ INSTANCES # if config . USE _ MINI _ 
MASK # input _ gt _ masks = KL . 
Input # shape = config . MINI _ MASK _ 
SHAPE 0 # config . MINI _ MASK _ SHAPE 
1 None # name = input _ gt _ masks 
dtype = bool # # input _ gt _ keypoint 
_ masks = KL . Input # # shape = 
config . MINI _ MASK _ SHAPE 0 # # 
config . MINI _ MASK _ SHAPE 1 None config 
. NUM _ KEYPOINTS # # name = input _ 
gt _ keypoint _ masks dtype = bool # else 
# input _ gt _ masks = KL . Input 
# shape = config . IMAGE _ SHAPE 0 config 
. IMAGE _ SHAPE 1 None # name = input 
_ gt _ masks dtype = bool # input _ 
gt _ keypoint _ masks = KL . Input # 
shape = config . IMAGE _ SHAPE 0 config . 
IMAGE _ SHAPE 1 None config . NUM _ KEYPOINTS 
# name = input _ gt _ keypoint _ masks 
dtype = bool # input _ gt _ keypoint _ 
weigths = KL . Input # shape = None config 
. NUM _ KEYPOINTS name = input _ gt _ 
keypoint _ weights dtype = tf . int32 # Build 
the shared convolutional layers . # Bottom up Layers # 
Returns a list of the last layers of each stage 
5 in total . # Don t create the thead 
stage 5 so we pick the 4th item in the 
list . _ C2 C3 C4 C5 = resnet _ 
graph input _ image resnet101 stage5 = True # Top 
down Layers # TODO add assert to varify feature map 
sizes match what s in config P5 = KL . 
Conv2D 256 1 1 name = fpn _ c5p5 C5 
P4 = KL . Add name = fpn _ p4add 
KL . UpSampling2D size = 2 2 name = fpn 
_ p5upsampled P5 KL . Conv2D 256 1 1 name 
= fpn _ c4p4 C4 P3 = KL . Add 
name = fpn _ p3add KL . UpSampling2D size = 
2 2 name = fpn _ p4upsampled P4 KL . 
Conv2D 256 1 1 name = fpn _ c3p3 C3 
P2 = KL . Add name = fpn _ p2add 
KL . UpSampling2D size = 2 2 name = fpn 
_ p3upsampled P3 KL . Conv2D 256 1 1 name 
= fpn _ c2p2 C2 # Attach 3x3 conv to 
all P layers to get the final feature maps . 
P2 = KL . Conv2D 256 3 3 padding = 
SAME name = fpn _ p2 P2 P3 = KL 
. Conv2D 256 3 3 padding = SAME name = 
fpn _ p3 P3 P4 = KL . Conv2D 256 
3 3 padding = SAME name = fpn _ p4 
P4 P5 = KL . Conv2D 256 3 3 padding 
= SAME name = fpn _ p5 P5 # P6 
is used for the 5th anchor scale in RPN . 
Generated by # subsampling from P5 with stride of 2 
. P6 = KL . MaxPooling2D pool _ size = 
1 1 strides = 2 name = fpn _ p6 
P5 # Note that P6 is used in RPN but 
not in the classifier heads . rpn _ feature _ 
maps = P2 P3 P4 P5 P6 mrcnn _ feature 
_ maps = P2 P3 P4 P5 # Generate Anchors 
self . anchors = utils . generate _ pyramid _ 
anchors config . RPN _ ANCHOR _ SCALES config . 
RPN _ ANCHOR _ RATIOS config . BACKBONE _ SHAPES 
config . BACKBONE _ STRIDES config . RPN _ ANCHOR 
_ STRIDE # RPN Model rpn = build _ rpn 
_ model config . RPN _ ANCHOR _ STRIDE len 
config . RPN _ ANCHOR _ RATIOS 256 # Loop 
through pyramid layers layer _ outputs = # list of 
lists for p in rpn _ feature _ maps layer 
_ outputs . append rpn p # Concatenate layer outputs 
# Convert from list of lists of level outputs to 
list of lists # of outputs across levels . # 
e . g . a1 b1 c1 a2 b2 c2 
= a1 a2 b1 b2 c1 c2 output _ names 
= rpn _ class _ logits rpn _ class rpn 
_ bbox outputs = list zip * layer _ outputs 
outputs = KL . Concatenate axis = 1 name = 
n list o for o n in zip outputs output 
_ names rpn _ class _ logits rpn _ class 
rpn _ bbox = outputs # Generate proposals # Proposals 
are batch N y1 x1 y2 x2 in normalized coordinates 
# and zero padded . proposal _ count = config 
. POST _ NMS _ ROIS _ TRAINING if mode 
= = training \ else config . POST _ NMS 
_ ROIS _ INFERENCE rpn _ rois = ProposalLayer proposal 
_ count = proposal _ count nms _ threshold = 
config . RPN _ NMS _ THRESHOLD name = ROI 
anchors = self . anchors config = config rpn _ 
class rpn _ bbox if mode = = training # 
Class ID mask to mark class IDs supported by the 
dataset the image # came from . _ _ _ 
active _ class _ ids = KL . Lambda lambda 
x parse _ image _ meta _ graph x mask 
= None None None None input _ image _ meta 
if not config . USE _ RPN _ ROIS # 
Ignore predicted ROIs and use ROIs provided as an input 
. input _ rois = KL . Input shape = 
config . POST _ NMS _ ROIS _ TRAINING 4 
name = input _ roi dtype = np . int32 
# Normalize coordinates to 0 1 range . target _ 
rois = KL . Lambda lambda x K . cast 
x tf . float32 / image _ scale 4 input 
_ rois else target _ rois = rpn _ rois 
# Generate detection targets # Subsamples proposals and generates target 
outputs for training # Note that proposal class IDs gt 
_ boxes and gt _ masks are zero # padded 
. Equally returned rois and targets are zero padded . 
# Every rois corresond to one target # rois target 
_ class _ ids target _ bbox target _ mask 
= \ # D e t e c t i 
o n T a r g e t L a 
y e r config name = proposal _ targets # 
target _ rois input _ gt _ class _ ids 
gt _ boxes input _ gt _ masks # Generate 
detection targets # Subsamples proposals and generates target outputs for 
training # Note that proposal class IDs gt _ boxes 
gt _ keypoint _ masks and gt _ keypoint _ 
weights are zero # padded . Equally returned rois and 
targets are zero padded . rois target _ class _ 
ids target _ bbox target _ keypoint target _ keypoint 
_ weight = \ D e t e c t 
i o n K e y p o i n 
t T a r g e t L a y 
e r config name = proposal _ targets \ target 
_ rois input _ gt _ class _ ids gt 
_ boxes gt _ keypoints # Network Heads # TODO 
verify that this handles zero padded ROIs mrcnn _ class 
_ logits mrcnn _ class mrcnn _ bbox = \ 
fpn _ classifier _ graph rois mrcnn _ feature _ 
maps config . IMAGE _ SHAPE config . POOL _ 
SIZE config . NUM _ CLASSES # mrcnn _ mask 
= build _ fpn _ mask _ graph rois mrcnn 
_ feature _ maps # config . IMAGE _ SHAPE 
# config . MASK _ POOL _ SIZE # config 
. NUM _ CLASSES # shape batch _ size num 
_ roi num _ keypoint 56 * 56 keypoint _ 
mrcnn _ mask = build _ fpn _ keypoint _ 
graph rois mrcnn _ feature _ maps config . IMAGE 
_ SHAPE config . KEYPOINT _ MASK _ POOL _ 
SIZE config . NUM _ KEYPOINTS # TODO clean up 
use tf . identify if necessary output _ rois = 
KL . Lambda lambda x x * 1 name = 
output _ rois rois # keypoint _ mrcnn _ mask 
= KL . Lambda lambda x x * 1 name 
= keypoint _ mrcnn _ mask keypoint _ mrcnn _ 
mask # Losses rpn _ class _ loss = KL 
. Lambda lambda x rpn _ class _ loss _ 
graph * x name = rpn _ class _ loss 
input _ rpn _ match rpn _ class _ logits 
rpn _ bbox _ loss = KL . Lambda lambda 
x rpn _ bbox _ loss _ graph config * 
x name = rpn _ bbox _ loss input _ 
rpn _ bbox input _ rpn _ match rpn _ 
bbox class _ loss = KL . Lambda lambda x 
mrcnn _ class _ loss _ graph * x name 
= mrcnn _ class _ loss target _ class _ 
ids mrcnn _ class _ logits active _ class _ 
ids bbox _ loss = KL . Lambda lambda x 
mrcnn _ bbox _ loss _ graph * x name 
= mrcnn _ bbox _ loss target _ bbox target 
_ class _ ids mrcnn _ bbox # mask _ 
loss = KL . Lambda lambda x mrcnn _ mask 
_ loss _ graph * x # name = mrcnn 
_ mask _ loss # target _ mask target _ 
class _ ids mrcnn _ mask keypoint _ loss = 
KL . Lambda lambda x keypoint _ mrcnn _ mask 
_ loss _ graph * x weight _ loss = 
config . WEIGHT _ LOSS name = keypoint _ mrcnn 
_ mask _ loss target _ keypoint target _ keypoint 
_ weight target _ class _ ids keypoint _ mrcnn 
_ mask target _ keypoints batch TRAIN _ ROIS _ 
PER _ IMAGE NUM _ KEYPOINTS Keypoint labels cropped to 
bbox boundaries and resized to neural network output size . 
Maps keypoints from the half open interval x1 x2 on 
continuous image coordinates to the closed interval 0 HEATMAP _ 
SIZE 1 target _ keypoint _ weights batch TRAIN _ 
ROIS _ PER _ IMAGE NUM _ KEYPOINTS bool type 
Keypoint _ weights 0 isn t visible 1 visilble # 
test _ target _ keypoint _ mask = test _ 
keypoint _ mrcnn _ mask _ loss _ graph target 
_ keypoint target _ keypoint _ weight # target _ 
class _ ids keypoint _ mrcnn _ mask # keypoint 
_ weight _ loss = KL . Lambda lambda x 
keypoint _ weight _ loss _ graph * x name 
= keypoint _ weight _ loss # target _ keypoint 
_ weight keypoint _ weight _ logits target _ class 
_ ids # Model generated # batch _ images batch 
_ image _ meta batch _ rpn _ match batch 
_ rpn _ bbox batch _ gt _ class _ 
ids \ # batch _ gt _ boxes batch _ 
gt _ keypoint batch _ gt _ masks inputs = 
input _ image input _ image _ meta input _ 
rpn _ match input _ rpn _ bbox input _ 
gt _ class _ ids input _ gt _ boxes 
input _ gt _ keypoints if not config . USE 
_ RPN _ ROIS inputs . append input _ rois 
# add test _ target _ keypoint _ mask in 
the output for test the keypoint loss function outputs = 
rpn _ class _ logits rpn _ class rpn _ 
bbox mrcnn _ class _ logits mrcnn _ class mrcnn 
_ bbox keypoint _ mrcnn _ mask rpn _ rois 
output _ rois rpn _ class _ loss rpn _ 
bbox _ loss class _ loss bbox _ loss keypoint 
_ loss # + test _ target _ keypoint _ 
mask for test the keypoint loss graph model = KM 
. Model inputs outputs name = mask _ keypoint _ 
mrcnn else # Network Heads # Proposal classifier and BBox 
regressor heads mrcnn _ class _ logits mrcnn _ class 
mrcnn _ bbox = \ fpn _ classifier _ graph 
rpn _ rois mrcnn _ feature _ maps config . 
IMAGE _ SHAPE config . POOL _ SIZE config . 
NUM _ CLASSES # Detections # output is # detections 
batch num _ detections y1 x1 y2 x2 class _ 
id score in image coordinates # keypoint _ weights batch 
num _ detections num _ keypoints detections = DetectionLayer config 
name = mrcnn _ detection rpn _ rois mrcnn _ 
class mrcnn _ bbox input _ image _ meta # 
Convert boxes to normalized coordinates # TODO let DetectionLayer return 
normalized coordinates to avoid # unnecessary conversions h w = 
config . IMAGE _ SHAPE 2 detection _ boxes = 
KL . Lambda lambda x x . . . 4 
/ np . array h w h w detections # 
Create masks for detections mrcnn _ mask = build _ 
fpn _ mask _ graph detection _ boxes mrcnn _ 
feature _ maps config . IMAGE _ SHAPE config . 
MASK _ POOL _ SIZE config . NUM _ CLASSES 
keypoint _ mrcnn = build _ fpn _ keypoint _ 
graph detection _ boxes mrcnn _ feature _ maps config 
. IMAGE _ SHAPE config . KEYPOINT _ MASK _ 
POOL _ SIZE config . NUM _ KEYPOINTS # shape 
Batch N _ ROI Number _ Keypoint height * width 
keypoint _ mcrcnn _ prob = KL . Activation softmax 
name = mrcnn _ prob keypoint _ mrcnn model = 
KM . Model input _ image input _ image _ 
meta detections mrcnn _ class mrcnn _ bbox rpn _ 
rois rpn _ class rpn _ bbox mrcnn _ mask 
keypoint _ mcrcnn _ prob name = keypoint _ mask 
_ rcnn # Add multi GPU support . if config 
. GPU _ COUNT 1 from parallel _ model import 
ParallelModel model = ParallelModel model config . GPU _ COUNT 
return model 在 model . compile 方法 中 我们 可以 
看到 有关 损失 函数 添加 的 细节 # Add Losses 
# First clear previously set losses to avoid duplication self 
. keras _ model . _ losses = self . 
keras _ model . _ per _ input _ losses 
= { } loss _ names = rpn _ class 
_ loss rpn _ bbox _ loss mrcnn _ class 
_ loss mrcnn _ bbox _ loss keypoint _ mrcnn 
_ mask _ loss for name in loss _ names 
layer = self . keras _ model . get _ 
layer name if layer . output in self . keras 
_ model . losses continue self . keras _ model 
. add _ loss tf . reduce _ mean layer 
. output keepdims = True # Add L2 Regularization # 
Skip gamma and beta weights of batch normalization layers . 
reg _ losses = keras . regularizers . l2 self 
. config . WEIGHT _ DECAY w / tf . 
cast tf . size w tf . float32 for w 
in self . keras _ model . trainable _ weights 
if gamma not in w . name and beta not 
in w . name self . keras _ model . 
add _ loss tf . add _ n reg _ 
losses 至此 keypoints 检测 分支 添加 完毕 直接 训练 即可 
3 keypoint 损失 函 数本 损失 函数 也是 原版 Mask 
RCNN 没有 实现 经由 Humanpose 工程 实现 的 我们 无需 
改动 其 原理 就是 将 true propose 的 目标 中 
的 可见 关键 点 进行 稀疏 交叉 熵 计算 之所以 
强调 是 稀疏 交叉 熵 因为 每 一个 关键 点 
其 使用 一个 56 * 56 的 向量 表示 大部分 
位置 为 0 仅 关键 点 位置 为 1 def 
keypoint _ mrcnn _ mask _ loss _ graph target 
_ keypoints target _ keypoint _ weights target _ class 
_ ids pred _ keypoints _ logit weight _ loss 
= True mask _ shape = 56 56 number _ 
point = 13 Mask softmax cross entropy loss for the 
keypoint head . 积极 区域 的 关键 点 才 参与 
loss 计算 真实 目标 类别 target _ class _ ids 
大于 0 的 位置 可见 点 才 参与 loss 运算 
真实 关键点 权重 target _ keypoint _ weights 为 1 
的 位置 target _ keypoints 真实 关键点 坐标 pred _ 
keypoints _ logit 预测出 关键点 生成 的 热 图 target 
_ keypoints batch TRAIN _ ROIS _ PER _ IMAGE 
NUM _ KEYPOINTS Keypoint labels cropped to bbox boundaries and 
resized to neural network output size . Maps keypoints from 
the half open interval x1 x2 on continuous image coordinates 
to the closed interval 0 HEATMAP _ SIZE 1 target 
_ keypoint _ weights batch TRAIN _ ROIS _ PER 
_ IMAGE NUM _ KEYPOINTS bool type Keypoint _ weights 
0 isn t visible 1 visilble target _ class _ 
ids batch TRAIN _ ROIS _ PER _ IMAGE . 
Integer class IDs . pred _ keypoints _ logit batch 
_ size num _ roi num _ keypoint 56 * 
56 # Reshape for simplicity . Merge first two dimensions 
into one . # shape N target _ class _ 
ids = K . reshape target _ class _ ids 
1 # Only positive person ROIs contribute to the loss 
. And only # the people specific mask of each 
ROI . positive _ people _ ix = tf . 
where target _ class _ ids 0 0 positive _ 
people _ ids = tf . cast tf . gather 
target _ class _ ids positive _ people _ ix 
tf . int64 # # # Step 1 Get the 
positive target and predict keypoint masks # reshape target _ 
keypoint _ weights to N num _ keypoints target _ 
keypoint _ weights = K . reshape target _ keypoint 
_ weights 1 number _ point # 点 的 可见度 
# reshape target _ keypoint _ masks to N num 
_ keypoints target _ keypoints = K . reshape target 
_ keypoints 1 number _ point # 点 的 坐标 
# reshape pred _ keypoint _ masks to N number 
_ point 56 * 56 pred _ keypoints _ logit 
= K . reshape pred _ keypoints _ logit 1 
number _ point mask _ shape 0 * mask _ 
shape 1 # 推荐 区 特征 图 # Gather the 
keypoint masks target and predict that contribute to loss # 
shape N _ positive number _ point positive _ target 
_ keypoints = tf . cast tf . gather target 
_ keypoints positive _ people _ ix tf . int32 
# shape N _ positive number _ point 56 * 
56 positive _ pred _ keypoints _ logit = tf 
. gather pred _ keypoints _ logit positive _ people 
_ ix # positive target _ keypoint _ weights to 
N _ positive number _ point positive _ keypoint _ 
weights = tf . cast tf . gather target _ 
keypoint _ weights positive _ people _ ix tf . 
float32 loss = K . switch tf . size positive 
_ target _ keypoints 0 lambda tf . nn . 
sparse _ softmax _ cross _ entropy _ with _ 
logits logits = positive _ pred _ keypoints _ logit 
labels = positive _ target _ keypoints lambda tf . 
constant 0.0 loss = loss * positive _ keypoint _ 
weights if weight _ loss loss = K . switch 
tf . reduce _ sum positive _ keypoint _ weights 
0 lambda tf . reduce _ sum loss / tf 
. reduce _ sum positive _ keypoint _ weights lambda 
tf . constant 0.0 else loss = K . mean 
loss loss = tf . reshape loss 1 1 return 
loss 我们 随机 选择 一张 图片 运行 demo _ detect 
. ipynb 脚本 查看 训练 效果 下图 Github 地址 Mask _ RCNNMask _ RCNN _ KeyPoints 
计算机 视觉 Mask RCNN _ 论文 学习 计算机 视觉 Mask 
RCNN _ 项目 文档 翻译 计算机 视觉 Mask RCNN _ 
推断 网络 其一 总览 计算机 视觉 Mask RCNN _ 推断 
网络 其二 基于 ReNet101 的 FPN 共享 网络 计算机 视觉 
Mask RCNN _ 推断 网络 其三 RPN 锚 框 处理 
和 Proposal 生成 计算机 视觉 Mask RCNN _ 推断 网络 
其四 FPN 和 ROIAlign 的 耦合 计算机 视觉 Mask RCNN 
_ 推断 网络 其五 目标 检测 结果 精炼 计算机 视觉 
Mask RCNN _ 推断 网络 其 六 Mask 生成 计算机 
视觉 Mask RCNN _ 推断 网络 终篇 使用 detect 方法 
进行 推断 计算机 视觉 Mask RCNN _ 锚 框 生成 
计算机 视觉 Mask RCNN _ 训练 网络 其一 数据集 与 
Dataset 类 计算机 视觉 Mask RCNN _ 训练 网络 其二 
train 网络结构 & 损失 函数 计算机 视觉 Mask RCNN _ 
训练 网络 其三 训练 Model 原 论文 中 提到 过 
Mask _ RCNN 是 可以 进行 关键点 检测 的 不过 
我们 学习 的 这个 工程 并 没有 添加 关键点 检测 
分支 而 有人 基于 本 工程 进行 了 完善 Mask 
_ RCNN _ Humanpose 本文 我们 将 简要 的 了解 
如何 将 关键点 识别 分支 添 加进 模型 更进一步 的 
我们 将 尝试 使用 Mask _ RCNN 对 实际 数据 
进行 识别 零 配置 相关 import os import numpy as 
np import pandas as pd from PIL import Image import 
utils as utils import model as modellib from config import 
Config PART _ INDEX = { blouse 0 1 2 
3 4 5 6 9 10 11 12 13 14 
outwear 0 1 3 4 5 6 7 8 9 
10 11 12 13 14 dress 0 1 2 3 
4 5 6 7 8 9 10 11 12 17 
18 skirt 15 16 17 18 trousers 15 16 19 
20 21 22 23 } PART _ STR = neckline 
_ left neckline _ right center _ front shoulder _ 
left shoulder _ right armpit _ left armpit _ right 
waistline _ left waistline _ right cuff _ left _ 
in cuff _ left _ out cuff _ right _ 
in cuff _ right _ out top _ hem _ 
left top _ hem _ right waistband _ left waistband 
_ right hemline _ left hemline _ right crotch bottom 
_ left _ in bottom _ left _ out bottom 
_ right _ in bottom _ right _ out IMAGE 
_ CATEGORY = blouse outwear dress skirt trousers 0 class 
FIConfig Config Configuration for training on the toy shapes dataset 
. Derives from the base Config class and overrides values 
specific to the toy shapes dataset . # Give the 
configuration a recognizable name NAME = FI # 数据集 名 
# Train on 1 GPU and 8 images per GPU 
. We can put multiple images on each # GPU 
because the images are small . Batch size is 8 
GPUs * images / GPU . GPU _ COUNT = 
1 IMAGES _ PER _ GPU = 1 NUM _ 
KEYPOINTS = len PART _ INDEX IMAGE _ CATEGORY # 
关键点 数目 KEYPOINT _ MASK _ SHAPE = 56 56 
# Number of classes including background NUM _ CLASSES = 
1 + 1 RPN _ TRAIN _ ANCHORS _ PER 
_ IMAGE = 100 VALIDATION _ STPES = 100 STEPS 
_ PER _ EPOCH = 1000 MINI _ MASK _ 
SHAPE = 56 56 KEYPOINT _ MASK _ POOL _ 
SIZE = 7 # Pooled ROIs POOL _ SIZE = 
7 MASK _ POOL _ SIZE = 14 MASK _ 
SHAPE = 28 28 WEIGHT _ LOSS = True KEYPOINT 
_ THRESHOLD = 0.005 常量 配置 记录 数据 类 关键点 
类 数据 类 和 关键 点 类 的 对应 关系 
config 类 记录 的 大部分 为 model 设置 无需 改动 
注意 设置 一下 NAME NUM _ KEYPOINTS 匹 配上 数据集 
一 数据 类 建立 1 关键点 标注 形式 回顾 一下 
之前 的 数据 集 介绍 在 非 关键 点 检测 
任务 中 我们 需要 的 数据 有 两种 a 原始 
的 图片 文件 b 图片 上 每个 instance 的 掩码 
但是 由于 Mask _ RCNN 会对 掩码 进行 一次 加工 
获取 每个 instance 的 坐标 框 即 实际上 还 需要 
c 每个 instance 的 坐标 框 既然 这里 要 检测 
关键点 那 我们 就 需要 d 图像 的 关键 点 
标注 key _ points num _ keypoints coordinates and visibility 
x y v num _ person num _ keypoints 3 
of num _ person 首先 我们 需要 明确 keypoints 从属 
于 某个 instance 即 上面 的 num _ person 的 
由来 人体 关键点 检测 为例 一个 instance 就是 一个人 而 
一个 instance 有 num _ keypoints 个 关键 点 每 
一个 点 由 3个 值 组成 横坐标 纵坐标 状态 其中 
状态 有三种 该类 不 存在 此 关键 点 被 遮挡 
可见 对于 COCO 而言 0 表示 这个 关键 点 没有 
标注 这种 情况 下 x = y = v = 
0 1 表示 这个 关键 点 标注 了 但是 不 
可见 被 遮挡 了 2 表示 这个 关键 点 标注 
了 同时 也 可见 在 不同 的 数据 集上 可能 
有 不同 的 数字 来 表达 这 三个点 但是 在 
此 框架 训练 中 建议 统一 到 COCO 的 标准 
避免 过多 的 修改 model 代码 主要 是 避免 修改 
关键 点 损失 函数 中的 代码 带来 不 必要 的 
意外 2 服装 关键点 标注 有了/nr 这些 基础 我们 以 
天池 的 服饰 关键点 定位 数据 为例 看一看 如何 设计 
Dataset class 具体 数据 说明 自行 查阅 上面 说明 本节 
重点 在 介绍 Mask RCNN 关键点 加 测 思路 而非 
数据 本身 其 文档 如下 我们 设计 的 Dataset class 
见 计算机 视觉 Mask RCNN _ 训练 网络 其一 数据集 
与 Dataset 类 目的 就是 基于 文档 信息 为 网络结构 
输送 数据 a 服装 类别 和 Mask RCNN 值得 注意 
的 是 Mask RCNN 的 分类 检测 Mask 生成 任务 
都是 多 分类 但是 关键 点 识别 由于 其 本身 
难度 更高 一个 类别 有 众多 关键点 不同 类别 关键点 
类型 之间 关系 不大 甚至 完全 不同 所以 建议 每 
一个 大 类 单独 训练 一个 model 检测 其 关键 
点 实际上 pose 关键点 检测 对应 过来 就是 检测 person 
这一个 类 的 框 Mask 以及 每 一个 instance 每 
一个 人 的 不同 部位 的 关键 点 实际/n 的/uj 
class/w 分类/n 值/n 有/v person/w 和/c 背景/n 两个/m 类/q 对应 
到 服饰 数据集 我们 需要 训练 5次 对 框 应 
五种 服装 b 服装 检测 框 服装 数据 标注 仅有 
关键点 但是 检测 框 对于 Mask RCNN 来说 是 必要 
的 因为 RPN 网络 需要 它 RPN 之后 的 回归 
网络 分支 可以 注释 掉 但是 RPN 是 网络 的 
主干 部分 不能 注释 所以 我们 采取 Mask RCNN 工程 
的 检测 框 生成 思路 利用 关键点 生成 检测 框 
由于 关键点 未必 在 服装 边缘 一般 是 在 的 
我们 的 检测 框 取 大 一点 尽量 完全 包含 
服装 下面 的 函数 见 utils . py 脚本 暂不 
涉及 这个 函数 只是 说 到了 贴上来 而已 def extract 
_ keypoint _ bboxes keypoints image _ size param keypoints 
instances keypoints _ per _ instance 3 param image _ 
size w h return bboxes = np . zeros keypoints 
. shape 0 4 dtype = np . int32 for 
i in range keypoints . shape 0 x = keypoints 
i 0 keypoints i 0 0 y = keypoints i 
1 keypoints i 1 0 x1 = x . min 
10 if x . min 10 0 else 0 y1 
= y . min 10 if y . min 10 
0 else 0 x2 = x . max + 11 
if x . max + 11 image _ size 0 
else image _ size 0 y2 = y . max 
+ 11 if y . max + 11 image _ 
size 1 else image _ size 1 bboxes i = 
np . array y1 x1 y2 x2 np . int32 
return bboxesc Mask 说明 服装 数据 是 没有 Mask 信息 
的 按照 Mask RCNN 论文 的 说法 掩码 使用 关键 
点 位置 为 1 其他 位置 为 0 的 形式 
即可 感觉 不 太 靠谱 而在 COCO 数据集 里 即 
本文 参考 工程 Mask _ RCNN _ Humanpose 掩码 信息 
使用 的 是 人 的 掩码 见 下图 我 在 
Dataset class 中生 成了 掩码 信息 作为 演示 在 build 
网络 中 取消 了 Mask 分支 下图 摘自 李沐/nr 博士 
的 手动 学习 深度 学习 可以 很 直观 的 理解 
我们 为什么 可以 把 Mask 分 支取 消掉 3 class 
FIDataset 正如 Dataset 注释 所说 要想 运行 自己 的 数据集 
我们 首先 要 实现 一个 方法 load _ shapes 根据 
数据集 取名 即可 收集 原始 图像 类别 信息 然后 实现 
两个 方法 load _ image load _ mask 分别 实现 
获取 单张 图片 数据 获取 单张 图片 对应 的 objs 
的 masks 和 classes 这样 基本 完成 了 数据集 类 
的 构建 对于 本 数据集 我们 使用 load _ FI 
方法 代替 load _ shapes 调用 self . add _ 
class 和 self . add _ image 记录 图片 类别 
信息 父 类 的 load _ image 会去 读取 self 
. image _ info 中 每张 图片 的 path 路径 
载入 图片 我们 不必 重写 保证 在 load _ FI 
中录 入了 即可 load _ mask 被 load _ keupoints 
取代 Mask _ RCNN _ Humanpose 做 了 这个 改动 
并 已经 捋顺 了 相关 调用 其 注释 如下 我们 
不 需要 mask 信息 返回 None 占位 即可 后面 需要 
将 网络 中 有关 Mask 信息 的 调用 注释 处理 
掉 这里 先不 介绍 Returns key _ points num _ 
keypoints coordinates and visibility x y v num _ person 
num _ keypoints 3 of num _ personmasks A bool 
array of shape height width instance count withone mask per 
instance . class _ ids a 1D array of class 
IDs of the instance masks here is always equal to 
num _ person 1 至此 我们 介绍 了 Dataset class 
的 目的 下面 给出 实现 见 FI _ train . 
py 由于 训练 时 需要 验证 集 而 我 截至 
撰文 时 没有 实现 验证 集 划分 用 训练 集 
冒充 验证 集 所以 load _ FI 的 参数 train 
_ data 没有意义 更新 会在 github 上 进行 后续 本文 
不予 修改 class FIDataset utils . Dataset Generates the shapes 
synthetic dataset . The dataset consists of simple shapes triangles 
squares circles placed randomly on a blank surface . The 
images are generated on the fly . No file access 
required . def load _ FI self train _ data 
= True Generate the requested number of synthetic images . 
count number of images to generate . height width the 
size of the generated images . if train _ data 
csv _ data = pd . concat pd . read 
_ csv . . / keypoint _ data / train1 
. csv pd . read _ csv . . / 
keypoint _ data / train2 . csv axis = 0 
ignore _ index = True # 忽略 索引 表示 不会 
直接 拼接 索引 会 重新 计算 行数 索引 class _ 
data = csv _ data csv _ data . image 
_ category . isin blouse # Add classes self . 
add _ class source = FI class _ id = 
1 class _ name = blouse # Add images for 
i in range class _ data . shape 0 annotation 
= class _ data . iloc i img _ path 
= os . path . join . . / keypoint 
_ data annotation . image _ id keypoints = np 
. array p . split _ for p in class 
_ data . iloc i 2 dtype = int PART 
_ INDEX IMAGE _ CATEGORY keypoints 1 + = 1 
self . add _ image source = FI image _ 
id = i path = img _ path annotations = 
keypoints def load _ keypoints self image _ id with 
_ mask = True Returns key _ points num _ 
keypoints coordinates and visibility x y v num _ person 
num _ keypoints 3 of num _ person masks A 
bool array of shape height width instance count with one 
mask per instance . class _ ids a 1D array 
of class IDs of the instance masks here is always 
equal to num _ person 1 key _ points = 
np . expand _ dims self . image _ info 
image _ id annotations 0 # 已知 图中 仅有 一个 
对象 class _ ids = np . array 1 if 
with _ mask annotations = self . image _ info 
image _ id annotations w h = image _ size 
self . image _ info image _ id path mask 
= np . zeros w h dtype = int mask 
annotations 1 annotations 0 = 1 return key _ points 
. copy np . expand _ dims mask 1 class 
_ ids return key _ points . copy None class 
_ ids 二 数据 类 读取 为了 验证 数据 类 
构建 的 正确性 我们 可以 直接 调用 接口 model . 
py 中的 load _ image _ gt _ keypoints 获取 
original _ image image _ meta gt _ class _ 
id gt _ bbox gt _ keypoint 等 信息 实际上 
在 真正 的 训练 中 程序 也 是 通过 这个 
函数 完成 Dataset class 中 的 数据 到 model 模型 
之间 的 传递 def load _ image _ gt _ 
keypoints dataset config image _ id augment = True use 
_ mini _ mask = False Load and return ground 
truth data for an image image keypoint _ mask keypoint 
_ weight mask bounding boxes . augment If true apply 
random image augmentation . Currently onlyhorizontal flipping is offered . 
use _ mini _ mask If False returns full size 
masks and keypoints that are the same heightand width as 
the original image . These can be big for e 
x a m p l e 1 0 2 4 
x 1 0 2 4 x 1 0 0 for 
100 instances . Mini masks are smaller typically 224x224 and 
are generated by extracting the bounding box of theobject and 
resizing it to MINI _ MASK _ SHAPE . Returns 
image height width 3 shape the original shape of the 
image before resizing and cropping . keypoints num _ person 
num _ keypoint 3 x y v v value is 
as belows 0 not visible and without annotations1 not visible 
but with annotations2 visible and with annotationsclass _ ids instance 
_ count Integer class IDsbbox instance _ count y1 x1 
y2 x2 mask height width instance _ count . The 
height and width are thoseof the image unless use _ 
mini _ mask is True in which case they aredefined 
in MINI _ MASK _ SHAPE . 在 visualize . 
py 模块 中 函数 display _ keypoints 可以 对接 上面 
函数 的 输出 直接 可视化 Dataset class 经由 load _ 
image _ gt _ keypoints 提取 的 结果 当然 并 
不是 直接 提取 该 函数 实际上 进行 了 一系列 的 
图像 预处理 这也 增加 了 我们 可视化 验证 正确性 的 
必要 流程 代码 如下 见 FI _ train . py 
config = FIConfig import visualize from model import log dataset 
= FIDataset dataset . load _ FI dataset . prepare 
original _ image image _ meta gt _ class _ 
id gt _ bbox gt _ keypoint = \ modellib 
. load _ image _ gt _ keypoints dataset FIConfig 
0 log original _ image original _ image log image 
_ meta image _ meta log gt _ class _ 
id gt _ class _ id log gt _ bbox 
gt _ bbox log gt _ keypoint gt _ keypoint 
visualize . display _ keypoints original _ image gt _ 
bbox gt _ keypoint gt _ class _ id dataset 
. class _ names 输出 图片 见下 可以 明确 的 
看见 至少 进行 了 padding 个 flip 两个 预处理 并非 
重点 不提 实现 了 自己 的 Dataset class 之后 使用 
model . load _ image _ gt _ keypoints 和 
visualize . display _ keypoints 进行 验证 保证 Dataset class 
的 正确性 三 修改 及 运行 模型 1 运行 模型 
步骤 data _ tra = FIDataset data _ tra . 
load _ FI data _ tra . prepare data _ 
val = FIDataset data _ val . load _ FI 
data _ val . prepare model = modellib . MaskRCNN 
mode = training config = config model _ dir = 
. / model . load _ weights . / mask 
_ rcnn _ coco . h5 by _ name = 
True exclude = mrcnn _ class _ logits mrcnn _ 
bbox _ fc mrcnn _ bbox mrcnn _ mask model 
. train data _ tra data _ val learning _ 
rate = config . LEARNING _ RATE / 10 epochs 
= 400 layers = heads 2 网络 细节 修改 服装 
关键点 和 Humanpose 数据 最大 的 不同 就 在于 我们 
没有 mask 掩码 数据 所以 我们 需要 对 原 model 
进行 修改 取 消掉 设计 mask 的 分支 注 意指 
的 是 Humanpose 代码 而非 原版 的 Mask RCNN 那个 
改 动起来 变化 太大 1 需要 添加 keypoint 标注 数据 
的 整个 预处理 分支 2 需要 实现 model 有关 keypoint 
的 损失 函数 在内 的 全部 处理 步骤 下面 给 
出 修改 之后 的 build 方法 由于 Mask RCNN 将 
各个 分支 损失 函数 直接 相加 所以 我们 直接 注释 
掉 Mask 分支 即可 不会 影响 代码 逻辑 程序 可以 
直接 正常 运行 def build self mode config Build Mask 
R CNN architecture . input _ shape The shape of 
the input image . mode Either training or inference . 
The inputs and outputs of the model differ accordingly . 
assert mode in training inference # Image size must be 
dividable by 2 multiple times h w = config . 
IMAGE _ SHAPE 2 if h / 2 * * 
6 = int h / 2 * * 6 or 
w / 2 * * 6 = int w / 
2 * * 6 raise Exception Image size must be 
dividable by 2 at least 6 times to avoid fractions 
when downscaling and upscaling . For example use 256 320 
384 448 512 . . . etc . # Inputs 
input _ image = KL . Input shape = config 
. IMAGE _ SHAPE . tolist name = input _ 
image input _ image _ meta = KL . Input 
shape = None name = input _ image _ meta 
if mode = = training # RPN GT input _ 
rpn _ match = KL . Input shape = None 
1 name = input _ rpn _ match dtype = 
tf . int32 input _ rpn _ bbox = KL 
. Input shape = None 4 name = input _ 
rpn _ bbox dtype = tf . float32 # Detection 
GT class IDs bounding boxes and masks # 1 . 
GT Class IDs zero padded input _ gt _ class 
_ ids = KL . Input shape = None name 
= input _ gt _ class _ ids dtype = 
tf . int32 # 2 . GT Boxes in pixels 
zero padded # batch MAX _ GT _ INSTANCES y1 
x1 y2 x2 in image coordinates input _ gt _ 
boxes = KL . Input shape = None 4 name 
= input _ gt _ boxes dtype = tf . 
float32 # Normalize coordinates h w = K . shape 
input _ image 1 K . shape input _ image 
2 image _ scale = K . cast K . 
stack h w h w axis = 0 tf . 
float32 gt _ boxes = KL . Lambda lambda x 
x / image _ scale name = gt _ boxes 
input _ gt _ boxes keypoint _ scale = K 
. cast K . stack w h 1 axis = 
0 tf . float32 input _ gt _ keypoints = 
KL . Input shape = None config . NUM _ 
KEYPOINTS 3 gt _ keypoints = KL . Lambda lambda 
x x / keypoint _ scale name = gt _ 
keypoints input _ gt _ keypoints # 3 . GT 
Masks zero padded # batch height width MAX _ GT 
_ INSTANCES # if config . USE _ MINI _ 
MASK # input _ gt _ masks = KL . 
Input # shape = config . MINI _ MASK _ 
SHAPE 0 # config . MINI _ MASK _ SHAPE 
1 None # name = input _ gt _ masks 
dtype = bool # # input _ gt _ keypoint 
_ masks = KL . Input # # shape = 
config . MINI _ MASK _ SHAPE 0 # # 
config . MINI _ MASK _ SHAPE 1 None config 
. NUM _ KEYPOINTS # # name = input _ 
gt _ keypoint _ masks dtype = bool # else 
# input _ gt _ masks = KL . Input 
# shape = config . IMAGE _ SHAPE 0 config 
. IMAGE _ SHAPE 1 None # name = input 
_ gt _ masks dtype = bool # input _ 
gt _ keypoint _ masks = KL . Input # 
shape = config . IMAGE _ SHAPE 0 config . 
IMAGE _ SHAPE 1 None config . NUM _ KEYPOINTS 
# name = input _ gt _ keypoint _ masks 
dtype = bool # input _ gt _ keypoint _ 
weigths = KL . Input # shape = None config 
. NUM _ KEYPOINTS name = input _ gt _ 
keypoint _ weights dtype = tf . int32 # Build 
the shared convolutional layers . # Bottom up Layers # 
Returns a list of the last layers of each stage 
5 in total . # Don t create the thead 
stage 5 so we pick the 4th item in the 
list . _ C2 C3 C4 C5 = resnet _ 
graph input _ image resnet101 stage5 = True # Top 
down Layers # TODO add assert to varify feature map 
sizes match what s in config P5 = KL . 
Conv2D 256 1 1 name = fpn _ c5p5 C5 
P4 = KL . Add name = fpn _ p4add 
KL . UpSampling2D size = 2 2 name = fpn 
_ p5upsampled P5 KL . Conv2D 256 1 1 name 
= fpn _ c4p4 C4 P3 = KL . Add 
name = fpn _ p3add KL . UpSampling2D size = 
2 2 name = fpn _ p4upsampled P4 KL . 
Conv2D 256 1 1 name = fpn _ c3p3 C3 
P2 = KL . Add name = fpn _ p2add 
KL . UpSampling2D size = 2 2 name = fpn 
_ p3upsampled P3 KL . Conv2D 256 1 1 name 
= fpn _ c2p2 C2 # Attach 3x3 conv to 
all P layers to get the final feature maps . 
P2 = KL . Conv2D 256 3 3 padding = 
SAME name = fpn _ p2 P2 P3 = KL 
. Conv2D 256 3 3 padding = SAME name = 
fpn _ p3 P3 P4 = KL . Conv2D 256 
3 3 padding = SAME name = fpn _ p4 
P4 P5 = KL . Conv2D 256 3 3 padding 
= SAME name = fpn _ p5 P5 # P6 
is used for the 5th anchor scale in RPN . 
Generated by # subsampling from P5 with stride of 2 
. P6 = KL . MaxPooling2D pool _ size = 
1 1 strides = 2 name = fpn _ p6 
P5 # Note that P6 is used in RPN but 
not in the classifier heads . rpn _ feature _ 
maps = P2 P3 P4 P5 P6 mrcnn _ feature 
_ maps = P2 P3 P4 P5 # Generate Anchors 
self . anchors = utils . generate _ pyramid _ 
anchors config . RPN _ ANCHOR _ SCALES config . 
RPN _ ANCHOR _ RATIOS config . BACKBONE _ SHAPES 
config . BACKBONE _ STRIDES config . RPN _ ANCHOR 
_ STRIDE # RPN Model rpn = build _ rpn 
_ model config . RPN _ ANCHOR _ STRIDE len 
config . RPN _ ANCHOR _ RATIOS 256 # Loop 
through pyramid layers layer _ outputs = # list of 
lists for p in rpn _ feature _ maps layer 
_ outputs . append rpn p # Concatenate layer outputs 
# Convert from list of lists of level outputs to 
list of lists # of outputs across levels . # 
e . g . a1 b1 c1 a2 b2 c2 
= a1 a2 b1 b2 c1 c2 output _ names 
= rpn _ class _ logits rpn _ class rpn 
_ bbox outputs = list zip * layer _ outputs 
outputs = KL . Concatenate axis = 1 name = 
n list o for o n in zip outputs output 
_ names rpn _ class _ logits rpn _ class 
rpn _ bbox = outputs # Generate proposals # Proposals 
are batch N y1 x1 y2 x2 in normalized coordinates 
# and zero padded . proposal _ count = config 
. POST _ NMS _ ROIS _ TRAINING if mode 
= = training \ else config . POST _ NMS 
_ ROIS _ INFERENCE rpn _ rois = ProposalLayer proposal 
_ count = proposal _ count nms _ threshold = 
config . RPN _ NMS _ THRESHOLD name = ROI 
anchors = self . anchors config = config rpn _ 
class rpn _ bbox if mode = = training # 
Class ID mask to mark class IDs supported by the 
dataset the image # came from . _ _ _ 
active _ class _ ids = KL . Lambda lambda 
x parse _ image _ meta _ graph x mask 
= None None None None input _ image _ meta 
if not config . USE _ RPN _ ROIS # 
Ignore predicted ROIs and use ROIs provided as an input 
. input _ rois = KL . Input shape = 
config . POST _ NMS _ ROIS _ TRAINING 4 
name = input _ roi dtype = np . int32 
# Normalize coordinates to 0 1 range . target _ 
rois = KL . Lambda lambda x K . cast 
x tf . float32 / image _ scale 4 input 
_ rois else target _ rois = rpn _ rois 
# Generate detection targets # Subsamples proposals and generates target 
outputs for training # Note that proposal class IDs gt 
_ boxes and gt _ masks are zero # padded 
. Equally returned rois and targets are zero padded . 
# Every rois corresond to one target # rois target 
_ class _ ids target _ bbox target _ mask 
= \ # D e t e c t i 
o n T a r g e t L a 
y e r config name = proposal _ targets # 
target _ rois input _ gt _ class _ ids 
gt _ boxes input _ gt _ masks # Generate 
detection targets # Subsamples proposals and generates target outputs for 
training # Note that proposal class IDs gt _ boxes 
gt _ keypoint _ masks and gt _ keypoint _ 
weights are zero # padded . Equally returned rois and 
targets are zero padded . rois target _ class _ 
ids target _ bbox target _ keypoint target _ keypoint 
_ weight = \ D e t e c t 
i o n K e y p o i n 
t T a r g e t L a y 
e r config name = proposal _ targets \ target 
_ rois input _ gt _ class _ ids gt 
_ boxes gt _ keypoints # Network Heads # TODO 
verify that this handles zero padded ROIs mrcnn _ class 
_ logits mrcnn _ class mrcnn _ bbox = \ 
fpn _ classifier _ graph rois mrcnn _ feature _ 
maps config . IMAGE _ SHAPE config . POOL _ 
SIZE config . NUM _ CLASSES # mrcnn _ mask 
= build _ fpn _ mask _ graph rois mrcnn 
_ feature _ maps # config . IMAGE _ SHAPE 
# config . MASK _ POOL _ SIZE # config 
. NUM _ CLASSES # shape batch _ size num 
_ roi num _ keypoint 56 * 56 keypoint _ 
mrcnn _ mask = build _ fpn _ keypoint _ 
graph rois mrcnn _ feature _ maps config . IMAGE 
_ SHAPE config . KEYPOINT _ MASK _ POOL _ 
SIZE config . NUM _ KEYPOINTS # TODO clean up 
use tf . identify if necessary output _ rois = 
KL . Lambda lambda x x * 1 name = 
output _ rois rois # keypoint _ mrcnn _ mask 
= KL . Lambda lambda x x * 1 name 
= keypoint _ mrcnn _ mask keypoint _ mrcnn _ 
mask # Losses rpn _ class _ loss = KL 
. Lambda lambda x rpn _ class _ loss _ 
graph * x name = rpn _ class _ loss 
input _ rpn _ match rpn _ class _ logits 
rpn _ bbox _ loss = KL . Lambda lambda 
x rpn _ bbox _ loss _ graph config * 
x name = rpn _ bbox _ loss input _ 
rpn _ bbox input _ rpn _ match rpn _ 
bbox class _ loss = KL . Lambda lambda x 
mrcnn _ class _ loss _ graph * x name 
= mrcnn _ class _ loss target _ class _ 
ids mrcnn _ class _ logits active _ class _ 
ids bbox _ loss = KL . Lambda lambda x 
mrcnn _ bbox _ loss _ graph * x name 
= mrcnn _ bbox _ loss target _ bbox target 
_ class _ ids mrcnn _ bbox # mask _ 
loss = KL . Lambda lambda x mrcnn _ mask 
_ loss _ graph * x # name = mrcnn 
_ mask _ loss # target _ mask target _ 
class _ ids mrcnn _ mask keypoint _ loss = 
KL . Lambda lambda x keypoint _ mrcnn _ mask 
_ loss _ graph * x weight _ loss = 
config . WEIGHT _ LOSS name = keypoint _ mrcnn 
_ mask _ loss target _ keypoint target _ keypoint 
_ weight target _ class _ ids keypoint _ mrcnn 
_ mask target _ keypoints batch TRAIN _ ROIS _ 
PER _ IMAGE NUM _ KEYPOINTS Keypoint labels cropped to 
bbox boundaries and resized to neural network output size . 
Maps keypoints from the half open interval x1 x2 on 
continuous image coordinates to the closed interval 0 HEATMAP _ 
SIZE 1 target _ keypoint _ weights batch TRAIN _ 
ROIS _ PER _ IMAGE NUM _ KEYPOINTS bool type 
Keypoint _ weights 0 isn t visible 1 visilble # 
test _ target _ keypoint _ mask = test _ 
keypoint _ mrcnn _ mask _ loss _ graph target 
_ keypoint target _ keypoint _ weight # target _ 
class _ ids keypoint _ mrcnn _ mask # keypoint 
_ weight _ loss = KL . Lambda lambda x 
keypoint _ weight _ loss _ graph * x name 
= keypoint _ weight _ loss # target _ keypoint 
_ weight keypoint _ weight _ logits target _ class 
_ ids # Model generated # batch _ images batch 
_ image _ meta batch _ rpn _ match batch 
_ rpn _ bbox batch _ gt _ class _ 
ids \ # batch _ gt _ boxes batch _ 
gt _ keypoint batch _ gt _ masks inputs = 
input _ image input _ image _ meta input _ 
rpn _ match input _ rpn _ bbox input _ 
gt _ class _ ids input _ gt _ boxes 
input _ gt _ keypoints if not config . USE 
_ RPN _ ROIS inputs . append input _ rois 
# add test _ target _ keypoint _ mask in 
the output for test the keypoint loss function outputs = 
rpn _ class _ logits rpn _ class rpn _ 
bbox mrcnn _ class _ logits mrcnn _ class mrcnn 
_ bbox keypoint _ mrcnn _ mask rpn _ rois 
output _ rois rpn _ class _ loss rpn _ 
bbox _ loss class _ loss bbox _ loss keypoint 
_ loss # + test _ target _ keypoint _ 
mask for test the keypoint loss graph model = KM 
. Model inputs outputs name = mask _ keypoint _ 
mrcnn else # Network Heads # Proposal classifier and BBox 
regressor heads mrcnn _ class _ logits mrcnn _ class 
mrcnn _ bbox = \ fpn _ classifier _ graph 
rpn _ rois mrcnn _ feature _ maps config . 
IMAGE _ SHAPE config . POOL _ SIZE config . 
NUM _ CLASSES # Detections # output is # detections 
batch num _ detections y1 x1 y2 x2 class _ 
id score in image coordinates # keypoint _ weights batch 
num _ detections num _ keypoints detections = DetectionLayer config 
name = mrcnn _ detection rpn _ rois mrcnn _ 
class mrcnn _ bbox input _ image _ meta # 
Convert boxes to normalized coordinates # TODO let DetectionLayer return 
normalized coordinates to avoid # unnecessary conversions h w = 
config . IMAGE _ SHAPE 2 detection _ boxes = 
KL . Lambda lambda x x . . . 4 
/ np . array h w h w detections # 
Create masks for detections mrcnn _ mask = build _ 
fpn _ mask _ graph detection _ boxes mrcnn _ 
feature _ maps config . IMAGE _ SHAPE config . 
MASK _ POOL _ SIZE config . NUM _ CLASSES 
keypoint _ mrcnn = build _ fpn _ keypoint _ 
graph detection _ boxes mrcnn _ feature _ maps config 
. IMAGE _ SHAPE config . KEYPOINT _ MASK _ 
POOL _ SIZE config . NUM _ KEYPOINTS # shape 
Batch N _ ROI Number _ Keypoint height * width 
keypoint _ mcrcnn _ prob = KL . Activation softmax 
name = mrcnn _ prob keypoint _ mrcnn model = 
KM . Model input _ image input _ image _ 
meta detections mrcnn _ class mrcnn _ bbox rpn _ 
rois rpn _ class rpn _ bbox mrcnn _ mask 
keypoint _ mcrcnn _ prob name = keypoint _ mask 
_ rcnn # Add multi GPU support . if config 
. GPU _ COUNT 1 from parallel _ model import 
ParallelModel model = ParallelModel model config . GPU _ COUNT 
return model 在 model . compile 方法 中 我们 可以 
看到 有关 损失 函数 添加 的 细节 # Add Losses 
# First clear previously set losses to avoid duplication self 
. keras _ model . _ losses = self . 
keras _ model . _ per _ input _ losses 
= { } loss _ names = rpn _ class 
_ loss rpn _ bbox _ loss mrcnn _ class 
_ loss mrcnn _ bbox _ loss keypoint _ mrcnn 
_ mask _ loss for name in loss _ names 
layer = self . keras _ model . get _ 
layer name if layer . output in self . keras 
_ model . losses continue self . keras _ model 
. add _ loss tf . reduce _ mean layer 
. output keepdims = True # Add L2 Regularization # 
Skip gamma and beta weights of batch normalization layers . 
reg _ losses = keras . regularizers . l2 self 
. config . WEIGHT _ DECAY w / tf . 
cast tf . size w tf . float32 for w 
in self . keras _ model . trainable _ weights 
if gamma not in w . name and beta not 
in w . name self . keras _ model . 
add _ loss tf . add _ n reg _ 
losses 至此 keypoints 检测 分支 添加 完毕 直接 训练 即可 
3 keypoint 损失 函 数本 损失 函数 也是 原版 Mask 
RCNN 没有 实现 经由 Humanpose 工程 实现 的 我们 无需 
改动 其 原理 就是 将 true propose 的 目标 中 
的 可见 关键 点 进行 稀疏 交叉 熵 计算 之所以 
强调 是 稀疏 交叉 熵 因为 每 一个 关键 点 
其 使用 一个 56 * 56 的 向量 表示 大部分 
位置 为 0 仅 关键 点 位置 为 1 def 
keypoint _ mrcnn _ mask _ loss _ graph target 
_ keypoints target _ keypoint _ weights target _ class 
_ ids pred _ keypoints _ logit weight _ loss 
= True mask _ shape = 56 56 number _ 
point = 13 Mask softmax cross entropy loss for the 
keypoint head . 积极 区域 的 关键 点 才 参与 
loss 计算 真实 目标 类别 target _ class _ ids 
大于 0 的 位置 可见 点 才 参与 loss 运算 
真实 关键点 权重 target _ keypoint _ weights 为 1 
的 位置 target _ keypoints 真实 关键点 坐标 pred _ 
keypoints _ logit 预测出 关键点 生成 的 热 图 target 
_ keypoints batch TRAIN _ ROIS _ PER _ IMAGE 
NUM _ KEYPOINTS Keypoint labels cropped to bbox boundaries and 
resized to neural network output size . Maps keypoints from 
the half open interval x1 x2 on continuous image coordinates 
to the closed interval 0 HEATMAP _ SIZE 1 target 
_ keypoint _ weights batch TRAIN _ ROIS _ PER 
_ IMAGE NUM _ KEYPOINTS bool type Keypoint _ weights 
0 isn t visible 1 visilble target _ class _ 
ids batch TRAIN _ ROIS _ PER _ IMAGE . 
Integer class IDs . pred _ keypoints _ logit batch 
_ size num _ roi num _ keypoint 56 * 
56 # Reshape for simplicity . Merge first two dimensions 
into one . # shape N target _ class _ 
ids = K . reshape target _ class _ ids 
1 # Only positive person ROIs contribute to the loss 
. And only # the people specific mask of each 
ROI . positive _ people _ ix = tf . 
where target _ class _ ids 0 0 positive _ 
people _ ids = tf . cast tf . gather 
target _ class _ ids positive _ people _ ix 
tf . int64 # # # Step 1 Get the 
positive target and predict keypoint masks # reshape target _ 
keypoint _ weights to N num _ keypoints target _ 
keypoint _ weights = K . reshape target _ keypoint 
_ weights 1 number _ point # 点 的 可见度 
# reshape target _ keypoint _ masks to N num 
_ keypoints target _ keypoints = K . reshape target 
_ keypoints 1 number _ point # 点 的 坐标 
# reshape pred _ keypoint _ masks to N number 
_ point 56 * 56 pred _ keypoints _ logit 
= K . reshape pred _ keypoints _ logit 1 
number _ point mask _ shape 0 * mask _ 
shape 1 # 推荐 区 特征 图 # Gather the 
keypoint masks target and predict that contribute to loss # 
shape N _ positive number _ point positive _ target 
_ keypoints = tf . cast tf . gather target 
_ keypoints positive _ people _ ix tf . int32 
# shape N _ positive number _ point 56 * 
56 positive _ pred _ keypoints _ logit = tf 
. gather pred _ keypoints _ logit positive _ people 
_ ix # positive target _ keypoint _ weights to 
N _ positive number _ point positive _ keypoint _ 
weights = tf . cast tf . gather target _ 
keypoint _ weights positive _ people _ ix tf . 
float32 loss = K . switch tf . size positive 
_ target _ keypoints 0 lambda tf . nn . 
sparse _ softmax _ cross _ entropy _ with _ 
logits logits = positive _ pred _ keypoints _ logit 
labels = positive _ target _ keypoints lambda tf . 
constant 0.0 loss = loss * positive _ keypoint _ 
weights if weight _ loss loss = K . switch 
tf . reduce _ sum positive _ keypoint _ weights 
0 lambda tf . reduce _ sum loss / tf 
. reduce _ sum positive _ keypoint _ weights lambda 
tf . constant 0.0 else loss = K . mean 
loss loss = tf . reshape loss 1 1 return 
loss 我们 随机 选择 一张 图片 运行 demo _ detect 
. ipynb 脚本 查看 训练 效果 