简评：如果人工智能犯了错怎么办？乘客看到了停车标志，突然感到一阵恐慌，因为他搭乘的自动驾驶汽车反而开始加速。当他看到前面的铁轨上一列火车向他们疾驰而来时，他张开嘴对前面的司机大声喊叫，但他突然意识到汽车前坐并没有司机。列车高速撞上来，压碎了这辆自动驾驶汽车，乘客当场死亡。这个场景是虚构的，但是凸显了当前人工智能中一个非常真实的缺陷。在过去的几年里，已经有越来越多的例子表明 —— 机器可以被误导，看见或听见根本不存在的东西。如果出现「噪音」会干扰到人工智能的识别系统，就可能产生误觉。比如上面的场景，尽管停车标志在人眼中清晰可见，但机器却未能识别出来。「停止」标志上一些简单的贴纸就足以使机器视觉算法看不见这个告示，而在人类的眼中依然显而易见计算机科学家称之为「对抗性例子」(adversarial examples)。MIT 的计算机科学家阿塔利（Anish Athalye）表示：我们可以把这些东西看作是人工智能网络会以某种方式处理的输入信息，但机器在看到这些输入信息后会做出一些意想不到的反应。▎看物体到目前为止，人们主要关注的是视觉识别系统。阿塔利已经证明，将一张猫的图像稍加改动，人眼看来仍是一只标准的猫，却被所谓的神经网络误解为是鳄梨酱。最近，阿塔利把注意力转向了实际物体。发现只要稍微调整一下它们的纹理和颜色，就可以骗过人工智能，把这些物体认作别的东西。在一个案例中，棒球被误认为是一杯浓缩咖啡，而在另一个案例中，3D 打印的海龟被误认为是步枪。他们还制造了约 200 个 3D 打印物体，这些物体以类似的方式欺骗了电脑。阿塔利表示：起初，这只是一种好奇，然而，随着这些智能系统越来越多地部署在现实世界中，人们正将其视为一个潜在的安全问题。以目前正在进行实地试验的无人驾驶汽车为例：这些汽车通常依靠复杂的深度学习神经网络导航，并告诉它们该做什么。但在去年，研究人员证明，仅仅只在路标上粘一两张小贴纸，神经网络就可能受骗，将道路上的「停车」标志误认为限速标志。尽管对于机器学习算法，让海龟看起来像步枪似乎是无害的，但研究人员担心，随着人工智能在现实世界中的应用，可能会带来一些危险后果。▎听声音神经网络并不是唯一使用的机器学习框架，但其他的人工智能框架似乎也容易遭受这些怪异事件的影响，并且不限于视觉识别系统。谷歌大脑（Google Brain）正在研发智能机器。谷歌大脑的研究科学家卡里尼（Nicholas Carlini）说，在我见过的每一个领域，从图像分类到自动语音识别，再到翻译，神经网络都可能受到攻击，导致输入信号被错误分类。卡里尼作了展示，加上一些摩擦的背景噪音后，「没有数据集的文章是无用的」这句话的读音，机器会误译为「好，谷歌要浏览 http://evil.com」。在另一个例子中，巴赫的第一号无伴奏大提琴组曲（Cello Suit 1）中的一段音乐节选被记录为「语言可以嵌入音乐」。在卡里尼看来，这些对抗性的例子「最终证明，哪怕在非常简单的任务上，机器学习也没有达到人类的能力」。对我们的耳朵来说，一段古典音乐听起来就是乐器的交响乐，但这段音乐若稍作修改，人工智能可能会理解为是一个语音指令。▎内在原理人工神经网络是大致模仿大脑（即生物神经网络）处理视觉信息的功能并从中学习方法。神经网络的工作原理大致是，获取的数据通过多层人工神经元网络传输进行信息处理，在接受到成百上千个相同物体的样本（通常由人类标记）的训练之后，神经网络开始建立此物体的视觉识别模式，从而能够在其后认得出正在观看的东西是这种物体。其中最复杂的系统采用「深度学习」，这意味着需要拥有更多的信息处理层。稍微改变物体的纹理，研究人员能够让一个3D打印的棒球看起来像一杯浓缩咖啡。然而，尽管计算机科学家了解人工神经网络如何工作，但他们并不一定知道在处理大数据时的具体细节。我们目前对神经网络的理解还不够。比如说，无法准确解释为什么会存在对抗性例子，也不知道如何解决这个问题。部分问题可能与现有技术被设计用来解决的任务的性质有关，例如区分猫和狗的图像。为了做到这一点，神经网络技术将处理大量猫和狗的模样信息，直到有足够的数据点来区分两者。一个真正强大的图像分类器会复制「相似性」对人类的作用，因而可以认出一个孩子涂鸦的猫和一张猫的照片以及一只现实生活中移动的猫代表的是同一样东西。尽管深度学习神经网络令人印象深刻，但在对物体进行分类、感知周遭环境或处理突发事件方面，仍无法与人脑匹敌。如果我们想要开发出能够在现实世界中发挥作用的真正智能机器，或许我们应该回到人脑上来，更好地理解人脑是如何解决这些问题的。▎捆绑问题（Binding problem）虽然神经网络是受到人类视觉皮层的启发，但越来越多的人认识到这种相似性只是表面现象。一个关键的区别在于，除了识别物体边缘的线条或物体本身等视觉特征外，我们的大脑还对这些特征之间的关系进行编码。因此，物体的边缘就构成了这个物体的一部分。这使我们能够对我们所看到的模式赋予意义。当你或我看着一只猫时，我们看到了构成猫的所有特征，以及它们之间的相互关系，这种相互「捆绑的」信息是我们理解世界的能力和我们的一般智力的基础。这个起关键作用的捆绑信息在当代的人工神经网络中是缺失的。对于更具体的行为模式，科学家仍在探索。我们清楚的是 —— 大脑的工作方式与我们现有的机器深度学习模式非常不同，因此，最终可能会走上一条完全不同的路才能成功。很难说可行性有多大，以及取得成功需要多长时间。与此同时，对于越来越多人工智能驱动的机器人、汽车和程序，我们可能需要避免对其过于信任。因为你永远不知道人工智能是不是正在产生被误导的视觉。原文链接：The ‘weird events’ that make machines hallucinate