一 空洞 卷积 的 提出 空洞 卷积 atrous convolutions 又名 
扩张 卷积 dilated convolutions 向 卷积 层 引入 了 一个 
称为 扩张 率 dilation rate 的 新 参数 该 参数 
定义 了 卷积 核 处理 数据 时各值/nr 的 间距 该 
结构 的 目的 是 在 不用 pooling pooling 层 会 
导致 信息 损失 且 计算 量 相当 的 情况 下 
提供 更大 的 感受 野 顺便 一提 卷积 结构 的 
主要 问题 如下 池化层/nr 不 可学 内部 数据结构 丢失 空间 
层级 化 信息 丢失 小物体 信息 无法 重建 假设有 四个 
pooling layer 则 任何 小于 2 ^ 4 = 16 
pixel 的 物体 信息 将 理论 上 无法 重建 而 
空洞 卷积 就有 内部 数据结构 的 保留 和 避免 使用 
down sampling 这样 的 特性 优点 明显 二 空洞 卷积 
原理 如下 如 卷积 核 没有 红点 标记 位置 为 
0 红点 标记 位置 同 正常 卷积 核 假设 原始 
特征 为 feat0 首先 使用 扩张 率 为 1 的 
空洞 卷积 生成 feat1 feat1 上 一点 相对 feat0 感受 
野 为 3 * 3 如图 a 然后 使用 扩张 
率 为 2 的 空洞 卷积 处理 feat1 生成 feat2 
如图 b 使 第一 次 空洞 卷积 的 卷积 核 
大小 等于 第二次 空洞 卷积 的 一个 像素点 的 感受 
野 图 b 即 feat1 上 一个 点 综合 了 
图 a 即 feat0 上 3 * 3 区域 的 
信息 则 生成 的 feat2 感受 野 为 7 * 
7 即 整个 图 b 深色 区域 第三 次 处理 
同上 第二 次 空洞 卷积 的 整个 卷积 核 大小 
等 于 第三 次 空洞 卷积 的 一个 像素点 的 
感受 野 图 c 即 feat2 上 每个 点 综合 
了 feat0 上 7 * 7 的 信息 感受 野 
则 采用 扩张 率 为 3 的 空洞 卷积 生成 
的 feat3 每 一个 点 感受 野 为 15 * 
15 相 比较 之下 使用 stride 为 1 的 普通 
3 * 3 卷积 三层 之后 感受 野 仅仅 为 
kernel 1 * layer + 1 = 7 三 空洞 
卷积 问题 感受 野 跳跃 我们 对 同一 张图/nr 连续 
三 次 使用 扩张 率 为 1 的 空洞 卷积 
观察 整张 图 的 中心点 的 感受 野 如 下图 
很明显 感受 野 不连续 我们 上一 小结 的 例子 就 
没 这个 问题 所以 空洞 卷积 依赖 网络设计 小尺度 物体 
检测 类似 第一 个 问题 仍然 需要 调整 扩张 率 
的 组合 来 解决 这个 问题 四 网络设计 研究 第一 
个 特性 是 叠加 卷积 的 dilation rate 不能 有 
大于 1 的 公约数 比如 2 4 6 则 不是 
一个 好 的 三层 卷积 依然 会 出现 gridding effect 
第二个 特性 是 我们 将 dilation rate 设计 成 锯齿状 
结构 例如 1 2 5 1 2 5 循环 结构 
第三个 特性 是 我们 需要 满足 一下 这个 式子 其中 
是 i 层 的 dilation rate 而 是 指在 i 
层 的 最大 dilation rate 那么 假设 总共有 n 层 
的话 默认 假设 我们 应用 于 kernel 为 k x 
k 的话 我们 的 目标 则是 这样 我们 至少 可以 
用 dilation rate 1 即 standard convolution 的 方式 来 
覆盖 掉 所有 洞 一个 简单 的 例子 dilation rate 
1 2 5 with 3 x 3 kernel 可行 的 
方案 而 这样 的 锯齿状 本身 的 性质 就 比较 
好 的 来 同时 满足 小物体 大 物体 的 分割 
要求 小 dilation rate 来 关心 近距离 信息 大 dilation 
rate 来 关心 远距离 信息 单 分支 设计 的 研究 
通向 标准化 设计 Hybrid Dilated Convolution HDC 可以 很好 的 
满足 分割 需要 如下 图 所示 多 分支 研究 解决 
多 尺度 分割 仅仅 在 一个 卷积 分支 网络 下 
使用 dilated convolution 去 抓取 多 尺度 物体 是 一个 
不 正统 的 方法 比方说 我们 用 一个 HDC 的 
方法 来 获取 一个 大 近 车辆 的 信息 然而 
对于 一个 小 远 车辆 的 信息 都 不再 受用 
假设 我们 再去 用 小 dilated convolution 的 方法 重新 
获取 小 车辆 的 信息 则 这么 做 非常 的 
冗余 基于 港 中文 和 商汤 组 的 PSPNet 里 
的 Pooling module 其 网络 同样 获得 当年 的 SOTA 
结果 ASPP 则 在 网络 decoder 上 对于 不同 尺度 
上 用 不同 大小 的 dilation rate 来 抓去 多 
尺度 信息 每个 尺度 则为 一个 独立 的 分支 在 
网络 最后 把 他 合并 起来 再 接 一个 卷积 
层 输出 预测 label 这样 的 设计 则 有效 避免 
了 在 encoder 上 冗余 的 信息 的 获取 直接 
关注 与 物体 之间 之内 的 相关性 五 常用 框架 
API 介绍 TensorFlow 接口 tf . nn . atrous _ 
conv2d value filters rate padding name = None value 指 
需要 做 卷积 的 输入 图像 要求 是 一个 4 
维 Tensor 具有 batch height width channels 这样 的 shape 
具体 含义 是 训练 时 一个 batch 的 图片 数量 
图片 高度 图片 宽度 图像 通 道数 filters 相当于 CNN 
中的 卷积 核 要求 是 一个 4 维 Tensor 具有 
filter _ height filter _ width channels out _ channels 
这样 的 shape 具体 含义 是 卷积 核 的 高度 
卷积 核 的 宽度 图像 通 道数 卷积 核 个数 
同理 这里 第 三维 channels 就是 参数 value 的 第四维 
rate 要求 是 一个 int 型 的 正数 正常 的 
卷积 操作 应该 会有 stride 即 卷积 核 的 滑动 
步长 但是 空洞 卷积 是 没有 stride 参数 的 这 
一点 尤其 要 注意 取而代之 它 使用 了 新的 rate 
参数 那么 rate 参数 有 什么 用 呢 它 定义 
为 我们 在 输入 图像 上 卷积 时的/nr 采样 间隔 
你 可以 理解 为 卷积 核 当中 穿插 了 rate 
1 数量 的 0 把 原来 的 卷积 核 插 
出 了 很多 洞洞 这样 做 卷积 时就/nr 相当于 对 
原 图像 的 采样 间隔 变 大了 具体 怎么 插 
得 可以 看 后面 更加 详细 的 描述 此时 我们 
很 容易 得出 rate = 1时 就 没有 0 插入 
此时 这个 函数 就 变成 了 普通 卷积 padding string 
类型 的 量 只能 是 SAME VALID 其中之一 这个 值 
决定 了 不同 边缘 填充 方式 函数 默认 stride = 
1 无法 改变 结果 返回 一个 Tensor 填充 方式 为 
VALID 时 返回 batch height 2 * filter _ width 
1 width 2 * filter _ height 1 out _ 
channels 的 Tensor 填充 方式 为 SAME 时 返回 batch 
height width out _ channels 的 Tensor 测试代码 如下 img 
= tf . constant value = 1 2 3 4 
1 2 3 4 1 2 3 4 1 2 
3 4 dtype = tf . float32 img = tf 
. concat values = img img axis = 3 filter 
= tf . constant value = 1 shape = 3 
3 2 5 dtype = tf . float32 out _ 
img1 = tf . nn . atrous _ conv2d value 
= img filters = filter rate = 1 padding = 
SAME out _ img2 = tf . nn . atrous 
_ conv2d value = img filters = filter rate = 
1 padding = VALID out _ img3 = tf . 
nn . atrous _ conv2d value = img filters = 
filter rate = 2 padding = SAME # error # 
out _ img4 = tf . nn . atrous _ 
conv2d value = img filters = filter rate = 2 
padding = VALID with tf . Session as sess print 
rate = 1 SAME mode result print sess . run 
out _ img1 print rate = 1 VALID mode result 
print sess . run out _ img2 print rate = 
2 SAME mode result print sess . run out _ 
img3 # error # print rate = 2 VALID mode 
result # print sess . run out _ img4 扩张 
率 为 1时 空洞 卷积 等价 于 普通 卷积 对于 
SAME 和 VALID 模式 计算 方式 如 下图 所示 扩张 
率 为 2 的 VALID 模式 计算 过程 扩张 率 
为 2 的 VALID 模式 会 报错 此时 卷积 核 
大于 图片 无法 卷积 MXNet 接口 MXNet 卷积 操作 自带 
扩张 率 参数 详见 文档 MXNet 的 通道 存储 与 
TensorFlow 不太 一致 所以 我 们 打印 一下 对比 上 
面的 图 可以 体会 到 为什么 除了 tf 外 大多 
框架 把 通道 放在 第 二维 import mxnet as mx 
import mxnet . ndarray as nd img = nd . 
array 1 2 3 4 1 2 3 4 1 
2 3 4 1 2 3 4 img = nd 
. concat img img dim = 1 img = nd 
. transpose img axes = 0 3 1 2 w 
= nd . ones 5 2 3 3 b = 
nd . array 0 for _ in range 5 img 
1 . 2 . 3 . 4 . 1 . 
2 . 3 . 4 . 1 . 2 . 
3 . 4 . 1 . 2 . 3 . 
4 . 1 . 2 . 3 . 4 . 
1 . 2 . 3 . 4 . 1 . 
2 . 3 . 4 . 1 . 2 . 
3 . 4 . NDArray 1x2x4x4 @ cpu 0 nd 
. Convolution img w b kernel = w . shape 
2 num _ filter = w . shape 0 stride 
= 1 1 pad = 1 1 dilate = 1 
1 12 . 24 . 36 . 28 . 18 
. 36 . 54 . 42 . 18 . 36 
. 54 . 42 . 12 . 24 . 36 
. 28 . 12 . 24 . 36 . 28 
. 18 . 36 . 54 . 42 . 18 
. 36 . 54 . 42 . 12 . 24 
. 36 . 28 . 12 . 24 . 36 
. 28 . 18 . 36 . 54 . 42 
. 18 . 36 . 54 . 42 . 12 
. 24 . 36 . 28 . 12 . 24 
. 36 . 28 . 18 . 36 . 54 
. 42 . 18 . 36 . 54 . 42 
. 12 . 24 . 36 . 28 . 12 
. 24 . 36 . 28 . 18 . 36 
. 54 . 42 . 18 . 36 . 54 
. 42 . 12 . 24 . 36 . 28 
. NDArray 1x5x4x4 @ cpu 0 nd . Convolution img 
w b kernel = w . shape 2 num _ 
filter = w . shape 0 stride = 1 1 
pad = 2 2 dilate = 2 2 16 . 
24 . 16 . 24 . 16 . 24 . 
16 . 24 . 16 . 24 . 16 . 
24 . 16 . 24 . 16 . 24 . 
16 . 24 . 16 . 24 . 16 . 
24 . 16 . 24 . 16 . 24 . 
16 . 24 . 16 . 24 . 16 . 
24 . 16 . 24 . 16 . 24 . 
16 . 24 . 16 . 24 . 16 . 
24 . 16 . 24 . 16 . 24 . 
16 . 24 . 16 . 24 . 16 . 
24 . 16 . 24 . 16 . 24 . 
16 . 24 . 16 . 24 . 16 . 
24 . 16 . 24 . 16 . 24 . 
16 . 24 . 16 . 24 . 16 . 
24 . 16 . 24 . 16 . 24 . 
16 . 24 . 16 . 24 . NDArray 1x5x4x4 
@ cpu 0 六 参考 来源 Multi scale Context Aggregation 
by Dilated Convolutions Tensorflow tf . nn . atrous _ 
conv2d 如何 实现 空洞 卷积 如何 理解 空洞 卷积 dilated 
convolution 一 空洞 卷积 的 提出 空洞 卷积 atrous convolutions 又名 
扩张 卷积 dilated convolutions 向 卷积 层 引入 了 一个 
称为 扩张 率 dilation rate 的 新 参数 该 参数 
定义 了 卷积 核 处理 数据 时各值/nr 的 间距 该 
结构 的 目的 是 在 不用 pooling pooling 层 会 
导致 信息 损失 且 计算 量 相当 的 情况 下 
提供 更大 的 感受 野 顺便 一提 卷积 结构 的 
主要 问题 如下 池化层/nr 不 可学 内部 数据结构 丢失 空间 
层级 化 信息 丢失 小物体 信息 无法 重建 假设有 四个 
pooling layer 则 任何 小于 2 ^ 4 = 16 
pixel 的 物体 信息 将 理论 上 无法 重建 而 
空洞 卷积 就有 内部 数据结构 的 保留 和 避免 使用 
down sampling 这样 的 特性 优点 明显 二 空洞 卷积 
原理 如下 如 卷积 核 没有 红点 标记 位置 为 
0 红点 标记 位置 同 正常 卷积 核 假设 原始 
特征 为 feat0 首先 使用 扩张 率 为 1 的 
空洞 卷积 生成 feat1 feat1 上 一点 相对 feat0 感受 
野 为 3 * 3 如图 a 然后 使用 扩张 
率 为 2 的 空洞 卷积 处理 feat1 生成 feat2 
如图 b 使 第一 次 空洞 卷积 的 卷积 核 
大小 等于 第二次 空洞 卷积 的 一个 像素点 的 感受 
野 图 b 即 feat1 上 一个 点 综合 了 
图 a 即 feat0 上 3 * 3 区域 的 
信息 则 生成 的 feat2 感受 野 为 7 * 
7 即 整个 图 b 深色 区域 第三 次 处理 
同上 第二 次 空洞 卷积 的 整个 卷积 核 大小 
等 于 第三 次 空洞 卷积 的 一个 像素点 的 
感受 野 图 c 即 feat2 上 每个 点 综合 
了 feat0 上 7 * 7 的 信息 感受 野 
则 采用 扩张 率 为 3 的 空洞 卷积 生成 
的 feat3 每 一个 点 感受 野 为 15 * 
15 相 比较 之下 使用 stride 为 1 的 普通 
3 * 3 卷积 三层 之后 感受 野 仅仅 为 
kernel 1 * layer + 1 = 7 三 空洞 
卷积 问题 感受 野 跳跃 我们 对 同一 张图/nr 连续 
三 次 使用 扩张 率 为 1 的 空洞 卷积 
观察 整张 图 的 中心点 的 感受 野 如 下图 
很明显 感受 野 不连续 我们 上一 小结 的 例子 就 
没 这个 问题 所以 空洞 卷积 依赖 网络设计 小尺度 物体 
检测 类似 第一 个 问题 仍然 需要 调整 扩张 率 
的 组合 来 解决 这个 问题 四 网络设计 研究 第一 
个 特性 是 叠加 卷积 的 dilation rate 不能 有 
大于 1 的 公约数 比如 2 4 6 则 不是 
一个 好 的 三层 卷积 依然 会 出现 gridding effect 
第二个 特性 是 我们 将 dilation rate 设计 成 锯齿状 
结构 例如 1 2 5 1 2 5 循环 结构 
第三个 特性 是 我们 需要 满足 一下 这个 式子 其中 
是 i 层 的 dilation rate 而 是 指在 i 
层 的 最大 dilation rate 那么 假设 总共有 n 层 
的话 默认 假设 我们 应用 于 kernel 为 k x 
k 的话 我们 的 目标 则是 这样 我们 至少 可以 
用 dilation rate 1 即 standard convolution 的 方式 来 
覆盖 掉 所有 洞 一个 简单 的 例子 dilation rate 
1 2 5 with 3 x 3 kernel 可行 的 
方案 而 这样 的 锯齿状 本身 的 性质 就 比较 
好 的 来 同时 满足 小物体 大 物体 的 分割 
要求 小 dilation rate 来 关心 近距离 信息 大 dilation 
rate 来 关心 远距离 信息 单 分支 设计 的 研究 
通向 标准化 设计 Hybrid Dilated Convolution HDC 可以 很好 的 
满足 分割 需要 如下 图 所示 多 分支 研究 解决 
多 尺度 分割 仅仅 在 一个 卷积 分支 网络 下 
使用 dilated convolution 去 抓取 多 尺度 物体 是 一个 
不 正统 的 方法 比方说 我们 用 一个 HDC 的 
方法 来 获取 一个 大 近 车辆 的 信息 然而 
对于 一个 小 远 车辆 的 信息 都 不再 受用 
假设 我们 再去 用 小 dilated convolution 的 方法 重新 
获取 小 车辆 的 信息 则 这么 做 非常 的 
冗余 基于 港 中文 和 商汤 组 的 PSPNet 里 
的 Pooling module 其 网络 同样 获得 当年 的 SOTA 
结果 ASPP 则 在 网络 decoder 上 对于 不同 尺度 
上 用 不同 大小 的 dilation rate 来 抓去 多 
尺度 信息 每个 尺度 则为 一个 独立 的 分支 在 
网络 最后 把 他 合并 起来 再 接 一个 卷积 
层 输出 预测 label 这样 的 设计 则 有效 避免 
了 在 encoder 上 冗余 的 信息 的 获取 直接 
关注 与 物体 之间 之内 的 相关性 五 常用 框架 
API 介绍 TensorFlow 接口 tf . nn . atrous _ 
conv2d value filters rate padding name = None value 指 
需要 做 卷积 的 输入 图像 要求 是 一个 4 
维 Tensor 具有 batch height width channels 这样 的 shape 
具体 含义 是 训练 时 一个 batch 的 图片 数量 
图片 高度 图片 宽度 图像 通 道数 filters 相当于 CNN 
中的 卷积 核 要求 是 一个 4 维 Tensor 具有 
filter _ height filter _ width channels out _ channels 
这样 的 shape 具体 含义 是 卷积 核 的 高度 
卷积 核 的 宽度 图像 通 道数 卷积 核 个数 
同理 这里 第 三维 channels 就是 参数 value 的 第四维 
rate 要求 是 一个 int 型 的 正数 正常 的 
卷积 操作 应该 会有 stride 即 卷积 核 的 滑动 
步长 但是 空洞 卷积 是 没有 stride 参数 的 这 
一点 尤其 要 注意 取而代之 它 使用 了 新的 rate 
参数 那么 rate 参数 有 什么 用 呢 它 定义 
为 我们 在 输入 图像 上 卷积 时的/nr 采样 间隔 
你 可以 理解 为 卷积 核 当中 穿插 了 rate 
1 数量 的 0 把 原来 的 卷积 核 插 
出 了 很多 洞洞 这样 做 卷积 时就/nr 相当于 对 
原 图像 的 采样 间隔 变 大了 具体 怎么 插 
得 可以 看 后面 更加 详细 的 描述 此时 我们 
很 容易 得出 rate = 1时 就 没有 0 插入 
此时 这个 函数 就 变成 了 普通 卷积 padding string 
类型 的 量 只能 是 SAME VALID 其中之一 这个 值 
决定 了 不同 边缘 填充 方式 函数 默认 stride = 
1 无法 改变 结果 返回 一个 Tensor 填充 方式 为 
VALID 时 返回 batch height 2 * filter _ width 
1 width 2 * filter _ height 1 out _ 
channels 的 Tensor 填充 方式 为 SAME 时 返回 batch 
height width out _ channels 的 Tensor 测试代码 如下 img 
= tf . constant value = 1 2 3 4 
1 2 3 4 1 2 3 4 1 2 
3 4 dtype = tf . float32 img = tf 
. concat values = img img axis = 3 filter 
= tf . constant value = 1 shape = 3 
3 2 5 dtype = tf . float32 out _ 
img1 = tf . nn . atrous _ conv2d value 
= img filters = filter rate = 1 padding = 
SAME out _ img2 = tf . nn . atrous 
_ conv2d value = img filters = filter rate = 
1 padding = VALID out _ img3 = tf . 
nn . atrous _ conv2d value = img filters = 
filter rate = 2 padding = SAME # error # 
out _ img4 = tf . nn . atrous _ 
conv2d value = img filters = filter rate = 2 
padding = VALID with tf . Session as sess print 
rate = 1 SAME mode result print sess . run 
out _ img1 print rate = 1 VALID mode result 
print sess . run out _ img2 print rate = 
2 SAME mode result print sess . run out _ 
img3 # error # print rate = 2 VALID mode 
result # print sess . run out _ img4 扩张 
率 为 1时 空洞 卷积 等价 于 普通 卷积 对于 
SAME 和 VALID 模式 计算 方式 如 下图 所示 扩张 
率 为 2 的 VALID 模式 计算 过程 扩张 率 
为 2 的 VALID 模式 会 报错 此时 卷积 核 
大于 图片 无法 卷积 MXNet 接口 MXNet 卷积 操作 自带 
扩张 率 参数 详见 文档 MXNet 的 通道 存储 与 
TensorFlow 不太 一致 所以 我 们 打印 一下 对比 上 
面的 图 可以 体会 到 为什么 除了 tf 外 大多 
框架 把 通道 放在 第 二维 import mxnet as mx 
import mxnet . ndarray as nd img = nd . 
array 1 2 3 4 1 2 3 4 1 
2 3 4 1 2 3 4 img = nd 
. concat img img dim = 1 img = nd 
. transpose img axes = 0 3 1 2 w 
= nd . ones 5 2 3 3 b = 
nd . array 0 for _ in range 5 img 
1 . 2 . 3 . 4 . 1 . 
2 . 3 . 4 . 1 . 2 . 
3 . 4 . 1 . 2 . 3 . 
4 . 1 . 2 . 3 . 4 . 
1 . 2 . 3 . 4 . 1 . 
2 . 3 . 4 . 1 . 2 . 
3 . 4 . NDArray 1x2x4x4 @ cpu 0 nd 
. Convolution img w b kernel = w . shape 
2 num _ filter = w . shape 0 stride 
= 1 1 pad = 1 1 dilate = 1 
1 12 . 24 . 36 . 28 . 18 
. 36 . 54 . 42 . 18 . 36 
. 54 . 42 . 12 . 24 . 36 
. 28 . 12 . 24 . 36 . 28 
. 18 . 36 . 54 . 42 . 18 
. 36 . 54 . 42 . 12 . 24 
. 36 . 28 . 12 . 24 . 36 
. 28 . 18 . 36 . 54 . 42 
. 18 . 36 . 54 . 42 . 12 
. 24 . 36 . 28 . 12 . 24 
. 36 . 28 . 18 . 36 . 54 
. 42 . 18 . 36 . 54 . 42 
. 12 . 24 . 36 . 28 . 12 
. 24 . 36 . 28 . 18 . 36 
. 54 . 42 . 18 . 36 . 54 
. 42 . 12 . 24 . 36 . 28 
. NDArray 1x5x4x4 @ cpu 0 nd . Convolution img 
w b kernel = w . shape 2 num _ 
filter = w . shape 0 stride = 1 1 
pad = 2 2 dilate = 2 2 16 . 
24 . 16 . 24 . 16 . 24 . 
16 . 24 . 16 . 24 . 16 . 
24 . 16 . 24 . 16 . 24 . 
16 . 24 . 16 . 24 . 16 . 
24 . 16 . 24 . 16 . 24 . 
16 . 24 . 16 . 24 . 16 . 
24 . 16 . 24 . 16 . 24 . 
16 . 24 . 16 . 24 . 16 . 
24 . 16 . 24 . 16 . 24 . 
16 . 24 . 16 . 24 . 16 . 
24 . 16 . 24 . 16 . 24 . 
16 . 24 . 16 . 24 . 16 . 
24 . 16 . 24 . 16 . 24 . 
16 . 24 . 16 . 24 . 16 . 
24 . 16 . 24 . 16 . 24 . 
16 . 24 . 16 . 24 . NDArray 1x5x4x4 
@ cpu 0 六 参考 来源 Multi scale Context Aggregation 
by Dilated Convolutions Tensorflow tf . nn . atrous _ 
conv2d 如何 实现 空洞 卷积 如何 理解 空洞 卷积 dilated 
convolution 一 空洞 卷积 的 提出 空洞 卷积 atrous convolutions 又名 
扩张 卷积 dilated convolutions 向 卷积 层 引入 了 一个 
称为 扩张 率 dilation rate 的 新 参数 该 参数 
定义 了 卷积 核 处理 数据 时各值/nr 的 间距 该 
结构 的 目的 是 在 不用 pooling pooling 层 会 
导致 信息 损失 且 计算 量 相当 的 情况 下 
提供 更大 的 感受 野 顺便 一提 卷积 结构 的 
主要 问题 如下 池化层/nr 不 可学 内部 数据结构 丢失 空间 
层级 化 信息 丢失 小物体 信息 无法 重建 假设有 四个 
pooling layer 则 任何 小于 2 ^ 4 = 16 
pixel 的 物体 信息 将 理论 上 无法 重建 而 
空洞 卷积 就有 内部 数据结构 的 保留 和 避免 使用 
down sampling 这样 的 特性 优点 明显 二 空洞 卷积 
原理 如下 如 卷积 核 没有 红点 标记 位置 为 
0 红点 标记 位置 同 正常 卷积 核 假设 原始 
特征 为 feat0 首先 使用 扩张 率 为 1 的 
空洞 卷积 生成 feat1 feat1 上 一点 相对 feat0 感受 
野 为 3 * 3 如图 a 然后 使用 扩张 
率 为 2 的 空洞 卷积 处理 feat1 生成 feat2 
如图 b 使 第一 次 空洞 卷积 的 卷积 核 
大小 等于 第二次 空洞 卷积 的 一个 像素点 的 感受 
野 图 b 即 feat1 上 一个 点 综合 了 
图 a 即 feat0 上 3 * 3 区域 的 
信息 则 生成 的 feat2 感受 野 为 7 * 
7 即 整个 图 b 深色 区域 第三 次 处理 
同上 第二 次 空洞 卷积 的 整个 卷积 核 大小 
等 于 第三 次 空洞 卷积 的 一个 像素点 的 
感受 野 图 c 即 feat2 上 每个 点 综合 
了 feat0 上 7 * 7 的 信息 感受 野 
则 采用 扩张 率 为 3 的 空洞 卷积 生成 
的 feat3 每 一个 点 感受 野 为 15 * 
15 相 比较 之下 使用 stride 为 1 的 普通 
3 * 3 卷积 三层 之后 感受 野 仅仅 为 
kernel 1 * layer + 1 = 7 三 空洞 
卷积 问题 感受 野 跳跃 我们 对 同一 张图/nr 连续 
三 次 使用 扩张 率 为 1 的 空洞 卷积 
观察 整张 图 的 中心点 的 感受 野 如 下图 
很明显 感受 野 不连续 我们 上一 小结 的 例子 就 
没 这个 问题 所以 空洞 卷积 依赖 网络设计 小尺度 物体 
检测 类似 第一 个 问题 仍然 需要 调整 扩张 率 
的 组合 来 解决 这个 问题 四 网络设计 研究 第一 
个 特性 是 叠加 卷积 的 dilation rate 不能 有 
大于 1 的 公约数 比如 2 4 6 则 不是 
一个 好 的 三层 卷积 依然 会 出现 gridding effect 
第二个 特性 是 我们 将 dilation rate 设计 成 锯齿状 
结构 例如 1 2 5 1 2 5 循环 结构 
第三个 特性 是 我们 需要 满足 一下 这个 式子 其中 
是 i 层 的 dilation rate 而 是 指在 i 
层 的 最大 dilation rate 那么 假设 总共有 n 层 
的话 默认 假设 我们 应用 于 kernel 为 k x 
k 的话 我们 的 目标 则是 这样 我们 至少 可以 
用 dilation rate 1 即 standard convolution 的 方式 来 
覆盖 掉 所有 洞 一个 简单 的 例子 dilation rate 
1 2 5 with 3 x 3 kernel 可行 的 
方案 而 这样 的 锯齿状 本身 的 性质 就 比较 
好 的 来 同时 满足 小物体 大 物体 的 分割 
要求 小 dilation rate 来 关心 近距离 信息 大 dilation 
rate 来 关心 远距离 信息 单 分支 设计 的 研究 
通向 标准化 设计 Hybrid Dilated Convolution HDC 可以 很好 的 
满足 分割 需要 如下 图 所示 多 分支 研究 解决 
多 尺度 分割 仅仅 在 一个 卷积 分支 网络 下 
使用 dilated convolution 去 抓取 多 尺度 物体 是 一个 
不 正统 的 方法 比方说 我们 用 一个 HDC 的 
方法 来 获取 一个 大 近 车辆 的 信息 然而 
对于 一个 小 远 车辆 的 信息 都 不再 受用 
假设 我们 再去 用 小 dilated convolution 的 方法 重新 
获取 小 车辆 的 信息 则 这么 做 非常 的 
冗余 基于 港 中文 和 商汤 组 的 PSPNet 里 
的 Pooling module 其 网络 同样 获得 当年 的 SOTA 
结果 ASPP 则 在 网络 decoder 上 对于 不同 尺度 
上 用 不同 大小 的 dilation rate 来 抓去 多 
尺度 信息 每个 尺度 则为 一个 独立 的 分支 在 
网络 最后 把 他 合并 起来 再 接 一个 卷积 
层 输出 预测 label 这样 的 设计 则 有效 避免 
了 在 encoder 上 冗余 的 信息 的 获取 直接 
关注 与 物体 之间 之内 的 相关性 五 常用 框架 
API 介绍 TensorFlow 接口 tf . nn . atrous _ 
conv2d value filters rate padding name = None value 指 
需要 做 卷积 的 输入 图像 要求 是 一个 4 
维 Tensor 具有 batch height width channels 这样 的 shape 
具体 含义 是 训练 时 一个 batch 的 图片 数量 
图片 高度 图片 宽度 图像 通 道数 filters 相当于 CNN 
中的 卷积 核 要求 是 一个 4 维 Tensor 具有 
filter _ height filter _ width channels out _ channels 
这样 的 shape 具体 含义 是 卷积 核 的 高度 
卷积 核 的 宽度 图像 通 道数 卷积 核 个数 
同理 这里 第 三维 channels 就是 参数 value 的 第四维 
rate 要求 是 一个 int 型 的 正数 正常 的 
卷积 操作 应该 会有 stride 即 卷积 核 的 滑动 
步长 但是 空洞 卷积 是 没有 stride 参数 的 这 
一点 尤其 要 注意 取而代之 它 使用 了 新的 rate 
参数 那么 rate 参数 有 什么 用 呢 它 定义 
为 我们 在 输入 图像 上 卷积 时的/nr 采样 间隔 
你 可以 理解 为 卷积 核 当中 穿插 了 rate 
1 数量 的 0 把 原来 的 卷积 核 插 
出 了 很多 洞洞 这样 做 卷积 时就/nr 相当于 对 
原 图像 的 采样 间隔 变 大了 具体 怎么 插 
得 可以 看 后面 更加 详细 的 描述 此时 我们 
很 容易 得出 rate = 1时 就 没有 0 插入 
此时 这个 函数 就 变成 了 普通 卷积 padding string 
类型 的 量 只能 是 SAME VALID 其中之一 这个 值 
决定 了 不同 边缘 填充 方式 函数 默认 stride = 
1 无法 改变 结果 返回 一个 Tensor 填充 方式 为 
VALID 时 返回 batch height 2 * filter _ width 
1 width 2 * filter _ height 1 out _ 
channels 的 Tensor 填充 方式 为 SAME 时 返回 batch 
height width out _ channels 的 Tensor 测试代码 如下 img 
= tf . constant value = 1 2 3 4 
1 2 3 4 1 2 3 4 1 2 
3 4 dtype = tf . float32 img = tf 
. concat values = img img axis = 3 filter 
= tf . constant value = 1 shape = 3 
3 2 5 dtype = tf . float32 out _ 
img1 = tf . nn . atrous _ conv2d value 
= img filters = filter rate = 1 padding = 
SAME out _ img2 = tf . nn . atrous 
_ conv2d value = img filters = filter rate = 
1 padding = VALID out _ img3 = tf . 
nn . atrous _ conv2d value = img filters = 
filter rate = 2 padding = SAME # error # 
out _ img4 = tf . nn . atrous _ 
conv2d value = img filters = filter rate = 2 
padding = VALID with tf . Session as sess print 
rate = 1 SAME mode result print sess . run 
out _ img1 print rate = 1 VALID mode result 
print sess . run out _ img2 print rate = 
2 SAME mode result print sess . run out _ 
img3 # error # print rate = 2 VALID mode 
result # print sess . run out _ img4 扩张 
率 为 1时 空洞 卷积 等价 于 普通 卷积 对于 
SAME 和 VALID 模式 计算 方式 如 下图 所示 扩张 
率 为 2 的 VALID 模式 计算 过程 扩张 率 
为 2 的 VALID 模式 会 报错 此时 卷积 核 
大于 图片 无法 卷积 MXNet 接口 MXNet 卷积 操作 自带 
扩张 率 参数 详见 文档 MXNet 的 通道 存储 与 
TensorFlow 不太 一致 所以 我 们 打印 一下 对比 上 
面的 图 可以 体会 到 为什么 除了 tf 外 大多 
框架 把 通道 放在 第 二维 import mxnet as mx 
import mxnet . ndarray as nd img = nd . 
array 1 2 3 4 1 2 3 4 1 
2 3 4 1 2 3 4 img = nd 
. concat img img dim = 1 img = nd 
. transpose img axes = 0 3 1 2 w 
= nd . ones 5 2 3 3 b = 
nd . array 0 for _ in range 5 img 
1 . 2 . 3 . 4 . 1 . 
2 . 3 . 4 . 1 . 2 . 
3 . 4 . 1 . 2 . 3 . 
4 . 1 . 2 . 3 . 4 . 
1 . 2 . 3 . 4 . 1 . 
2 . 3 . 4 . 1 . 2 . 
3 . 4 . NDArray 1x2x4x4 @ cpu 0 nd 
. Convolution img w b kernel = w . shape 
2 num _ filter = w . shape 0 stride 
= 1 1 pad = 1 1 dilate = 1 
1 12 . 24 . 36 . 28 . 18 
. 36 . 54 . 42 . 18 . 36 
. 54 . 42 . 12 . 24 . 36 
. 28 . 12 . 24 . 36 . 28 
. 18 . 36 . 54 . 42 . 18 
. 36 . 54 . 42 . 12 . 24 
. 36 . 28 . 12 . 24 . 36 
. 28 . 18 . 36 . 54 . 42 
. 18 . 36 . 54 . 42 . 12 
. 24 . 36 . 28 . 12 . 24 
. 36 . 28 . 18 . 36 . 54 
. 42 . 18 . 36 . 54 . 42 
. 12 . 24 . 36 . 28 . 12 
. 24 . 36 . 28 . 18 . 36 
. 54 . 42 . 18 . 36 . 54 
. 42 . 12 . 24 . 36 . 28 
. NDArray 1x5x4x4 @ cpu 0 nd . Convolution img 
w b kernel = w . shape 2 num _ 
filter = w . shape 0 stride = 1 1 
pad = 2 2 dilate = 2 2 16 . 
24 . 16 . 24 . 16 . 24 . 
16 . 24 . 16 . 24 . 16 . 
24 . 16 . 24 . 16 . 24 . 
16 . 24 . 16 . 24 . 16 . 
24 . 16 . 24 . 16 . 24 . 
16 . 24 . 16 . 24 . 16 . 
24 . 16 . 24 . 16 . 24 . 
16 . 24 . 16 . 24 . 16 . 
24 . 16 . 24 . 16 . 24 . 
16 . 24 . 16 . 24 . 16 . 
24 . 16 . 24 . 16 . 24 . 
16 . 24 . 16 . 24 . 16 . 
24 . 16 . 24 . 16 . 24 . 
16 . 24 . 16 . 24 . 16 . 
24 . 16 . 24 . 16 . 24 . 
16 . 24 . 16 . 24 . NDArray 1x5x4x4 
@ cpu 0 六 参考 来源 Multi scale Context Aggregation 
by Dilated Convolutions Tensorflow tf . nn . atrous _ 
conv2d 如何 实现 空洞 卷积 如何 理解 空洞 卷积 dilated 
convolution 