在 前面 两 篇 文章 介绍 了 深度 学习 的 
一些 基本 概念 本文 则 使用 Python 实现 一个 简单 
的 深度 神经网络 并 使用 MNIST 数据库 进行 测试 神经 
网络 的 实现 包括 以下 内容 神经网络 权值 的 初始化 
正向 传播 误差 评估 反向 传播 更新 权值 主要 是 
根据 反向 传播 的 4个 基本 方程 利用 Python 实现 
神经 网络 的 反向 传播 初始化 首先 定义 代表 神经 
网络 的 类 NeuralNetwork class NeuralNetwork def _ _ init 
_ _ self layers alpha = 0.1 self . W 
= self . layers = layers self . alpha = 
alpha 有 三个 属性 W 存储 各个 层 之间 的 
权值 矩阵 也是 神经网络 要 更新 学习 的 layers 神经 
网络 的 结构 例如 2 2 1 表示 输入 层 
有 2个 神经元 隐藏 层 2个 神经元 输出 层 只有 
1个 神经元 alpha 学习 速率 接下来 初始化 各个 层 之间 
的 权值 矩阵 for i in np . arange 0 
len layers 2 w = np . random . randn 
layers i + 1 layers i + 1 + 1 
self . W . append w / np . sqrt 
layers i 注意 上面 生成 权值 矩阵 的 大小 layers 
i + 1 layers i + 1 + 1 都 
加了 1 这是 将 神经元 的 偏置 和 权值 统一 
的 放到 了 权值 矩阵 里面 \ \ left \ 
begin { array } { c } w _ { 
11 } & w _ { 12 } \ \ 
w _ { 21 } & w _ { 22 
} \ end { array } \ right \ cdot 
\ left \ begin { array } { c } 
x _ 1 \ \ x _ 2 \ end 
{ array } \ right + \ left \ begin 
{ array } { c } b _ 1 \ 
\ b _ 2 \ end { array } \ 
right = \ left \ begin { array } { 
c } w _ { 11 } x _ 1 
+ w { 12 } x _ 2 + b 
_ 1 \ \ w _ { 21 } x 
_ 1 + w _ { 22 } x _ 
2 + b _ 2 \ end { array } 
\ right \ 可以 将 上式 写成 齐次 的 形式 
\ \ left \ begin { array } { c 
} w _ { 11 } & w _ { 
12 } & b _ 1 \ \ w _ 
{ 21 } & w _ { 22 } & 
b _ 2 \ end { array } \ right 
\ cdot \ left \ begin { array } { 
c } x _ 1 \ \ x _ 2 
\ \ 1 \ end { array } \ right 
\ 使用 统一 的 矩阵 运算 在 正向 反向 传播 
的 时候 更 方便 在 输出 层 的 神经元 并 
没有 偏置 所以 要 单独 初始化 输出 层 的 权值 
矩阵 w = np . random . randn layers 2 
+ 1 layers 1 self . W . append w 
/ np . sqrt layers 2 下面 实现 Python 的 
magic function _ _ repr _ _ 输出 神经 网络结构 
def _ _ repr _ _ self return NeuralNetWork { 
} . format . join str l for l in 
self . layers 激活 函数 在 神经 网络 中 使用 
sigmoid 作为 激活 函数 实现 sigmoid 及其 导数 def sigmoid 
self x return 1.0 / 1 + np . exp 
x def sigmoid _ deriv self x return x * 
1 x 正向 反向 传播 这 一部分 是 神经 的 
网络 的 核心 了 下面 实现 fit 方法 在 方法 
中 完成 神经网络 权值 更新 训练 的 过程 def fit 
self X y epochs = 1000 displayUpdate = 100 X 
= np . c _ X np . ones X 
. shape 0 for epoch in np . arange 0 
epochs for x target in zip X y self . 
fit _ partial x target # check to see if 
we should display a training update if epoch = = 
0 or epoch + 1 % displayUpdate = = 0 
loss = self . calculate _ loss X y print 
INFO epoch = { } loss = { . 7f 
} . format epoch + 1 loss 该 函数 有 
4个 参数 X 是 输入 的 样本数据 y 是 样本 
的 真是 值 epochs 训练 的 轮数 displayUpdate 输出 训练 
的 loss 值 X = np . c _ X 
np . ones X . shape 0 将 输入 训练 
的 样本 表示 为 齐次 向量 也 就是 在 末尾 
添 1 fit _ partial 是 对 输入 的 每个 
样本 进行 训练 包括 正向 传播 反向 传播 以及 权值 
的 更新 def fit _ partial self x y A 
= np . atleast _ 2d x # 正向 传播 
# 层层 之间 的 数据 传递 for layer in np 
. arange 0 len self . W # 输入 经过 
加权 以及 偏 置后 的 值 net = A layer 
. dot self . W layer # 神经元 的 输出 
out = self . sigmoid net # 保存 下来 反向 
传播 的 时候 使用 A . append out 上面 完成 
了 神经 玩过 的 正向 传播 过程 下面 根据 反向 
传播 的 4个 基本 方程 进行 反向 传播 首先 根据 
\ BP1 \ \ \ delta ^ L = \ 
frac { \ partial e } { \ partial a 
^ L } \ odot \ sigma z ^ L 
\ tag { BP1 } \ 计算 输出 层 的 
误差 \ \ delta ^ L \ error = A 
1 y # 输出 层 的 误差 均值 方差 作为 
损失 函数 D = error * self . sigmoid _ 
deriv A 1 得到 输出 层 的 误差 D 后 
根据 \ BP2 \ 计算 各个 层 的 误差 \ 
\ delta ^ { L 1 } = W ^ 
L ^ T \ delta ^ L \ odot \ 
sigma z ^ { L 1 } \ tag { 
BP2 } \ for layer in np . arange len 
A 2 0 1 delta = D 1 . dot 
self . W layer . T delta = delta * 
self . sigmoid _ deriv A layer D . append 
delta D = D 1 将 D 反转 和 各个 
层 的 索引 对应 起来 下面 根据 \ BP3 BP4 
\ 计算 权值 矩阵 和 偏置 的 导数 \ \ 
frac { \ partial e } { b _ j 
^ l } = \ delta _ j ^ l 
\ tag { BP3 } \ \ \ frac { 
\ partial e } { w _ { jk } 
^ l } = \ delta _ j ^ l 
a _ k ^ { l 1 } \ tag 
{ BP4 } \ for layer in np . arange 
0 len self . W self . W layer + 
= self . alpha * A layer . T . 
dot D layer 首先 求得 权值 和 偏置 的 导数 
权值 和 偏置 统一 到 同一个 矩阵 中 A layer 
. T . dot D layer 然后 将 梯度 乘以 
学习 速率 alpha 每次 权值 减小 的 步长 上述 就 
完成 利用 反向 传播 算法 更新 权值 的 过程 关于 
反向 传播 四个 基本 方程 的 推导 过程 可以 参考 
文章 深度 学习 与 计算机 视觉 搞懂 反向 传播 算法 
的 四个 基本 方程 误差 评估 上面 代码 已经 实现 
了 深度 学习 的 训练 过程 下面 实现 predict 输出 
使用 训练 好 的 模型 预测 的 结果 calculate _ 
loss 评估 训练 后 模型 的 评估 def predict self 
X addBias = True p = np . atleast _ 
2d X if addBias p = np . c _ 
p np . ones p . shape 0 for layer 
in np . arange 0 len self . W p 
= self . sigmoid np . dot p self . 
W layer return p def calculate _ loss self X 
targets targets = np . atleast _ 2d targets predictions 
= self . predict X addBias = False loss = 
0.5 * np . sum predictions targets * * 2 
return lossMNIST 分类 识别 使用 上面 实现 的 深度 神经 
网络 对 MNIST 手写体 进行 识别 首先 导入 必要 的 
包 import NeuralNetwork from sklearn . preprocessing import LabelBinarizer from 
sklearn . model _ selection import train _ test _ 
split from sklearn . metrics import classification _ report from 
sklearn import datasets 需要 使用 sklearn 包中的/nr 一些 工具 进行 
数据 的 处理 # load MNIST 数据集 并 使用 min 
/ max 对 数据 进行 归一化 digits = datasets . 
load _ digits data = digits . data . astype 
float data = data data . min / data . 
max data . min print INFO samples { } dim 
{ } . format data . shape 0 data . 
shape 1 将 数据 拆分 为 训练 集 和 测试 
集 并对 MNIST 的 类别 进行 编码 trainX testX trainY 
testY = train _ test _ split data digits . 
target test _ size = 0.25 # convert the labels 
from integers to vectors trainY = LabelBinarizer . fit _ 
transform trainY testY = LabelBinarizer . fit _ transform testY 
下面 构建 神经 网络结构 并 使用 训练 集 进行 训练 
nn = NeuralNetwork data . shape 1 32 16 10 
print INFO { } . format nn nn . fit 
trainX trainY epochs = 1000 神经网络 结构 为 64 32 
16 10 其中 64 为 输入 数据 的 大小 10 
输出 类别 的 个数 最后 评估 训练 得到 的 模型 
predictions = nn . predict testX print classification _ report 
testY . argmax axis = 1 predictions . argmax axis 
= 1 最终 的 输出 结果 INFO loading MNIST sample 
dataset . . . INFO samples 1797 dim 64 INFO 
training network . . . INFO NeuralNetWork 64 32 16 
10 INFO epoch = 1 loss = 607.1711647 INFO epoch 
= 100 loss = 7.1082795 INFO epoch = 200 loss 
= 4.0731690 INFO epoch = 300 loss = 3.1401868 INFO 
epoch = 400 loss = 2.8801101 INFO epoch = 500 
loss = 1.8738122 INFO epoch = 600 loss = 1.7461474 
INFO epoch = 700 loss = 1.6624043 INFO epoch = 
800 loss = 1.1852884 INFO epoch = 900 loss = 
0.6710255 INFO epoch = 1000 loss = 0.6336826 INFO evaluating 
network . . . precision recall f1 score support 0 
1.00 0.95 0.97 39 1 0.84 1.00 0.92 38 2 
1.00 0.98 0.99 41 3 0.93 0.98 0.95 52 4 
0.91 0.97 0.94 40 5 0.98 0.98 0.98 41 6 
1.00 0.96 0.98 51 7 1.00 0.98 0.99 48 8 
0.98 0.89 0.93 55 9 0.98 0.93 0.95 45 micro 
avg 0.96 0.96 0.96 450 macro avg 0.96 0.96 0.96 
450 weighted avg 0.96 0.96 0.96 450 如上 测试 结果 
在 测试 集 的 上 表现 还 算 不错 总结 
本文 使用 Python 简单 的 实现 了 一个 神经 网络 
主要 是 利用 反向 传播 的 4个 基本 方程 实现 
反向 传播 算法 更新 各 个 神经元 的 权值 最后 
使用 该 网络 对 MNIST 数据 进行 识别 分类 上面 
实现 的 神经 网络 只是 玩具 用以 加深 对 深度 
学习 的 训练 过程 以及 反向 传播 算法 的 理解 
后面 将 使用 Keras 和 PyTorch 来 构建 神经网络 本文 
代码 在 git 库 https / / github . com 
/ brookicv / m a c h i n e 
L e a r n i n g a m 
p l e 在 前面 两 篇 文章 介绍 了 深度 学习 的 
一些 基本 概念 本文 则 使用 Python 实现 一个 简单 
的 深度 神经网络 并 使用 MNIST 数据库 进行 测试 神经 
网络 的 实现 包括 以下 内容 神经网络 权值 的 初始化 
正向 传播 误差 评估 反向 传播 更新 权值 主要 是 
根据 反向 传播 的 4个 基本 方程 利用 Python 实现 
神经 网络 的 反向 传播 初始化 首先 定义 代表 神经 
网络 的 类 NeuralNetwork class NeuralNetwork def _ _ init 
_ _ self layers alpha = 0.1 self . W 
= self . layers = layers self . alpha = 
alpha 有 三个 属性 W 存储 各个 层 之间 的 
权值 矩阵 也是 神经网络 要 更新 学习 的 layers 神经 
网络 的 结构 例如 2 2 1 表示 输入 层 
有 2个 神经元 隐藏 层 2个 神经元 输出 层 只有 
1个 神经元 alpha 学习 速率 接下来 初始化 各个 层 之间 
的 权值 矩阵 for i in np . arange 0 
len layers 2 w = np . random . randn 
layers i + 1 layers i + 1 + 1 
self . W . append w / np . sqrt 
layers i 注意 上面 生成 权值 矩阵 的 大小 layers 
i + 1 layers i + 1 + 1 都 
加了 1 这是 将 神经元 的 偏置 和 权值 统一 
的 放到 了 权值 矩阵 里面 \ \ left \ 
begin { array } { c } w _ { 
11 } & w _ { 12 } \ \ 
w _ { 21 } & w _ { 22 
} \ end { array } \ right \ cdot 
\ left \ begin { array } { c } 
x _ 1 \ \ x _ 2 \ end 
{ array } \ right + \ left \ begin 
{ array } { c } b _ 1 \ 
\ b _ 2 \ end { array } \ 
right = \ left \ begin { array } { 
c } w _ { 11 } x _ 1 
+ w { 12 } x _ 2 + b 
_ 1 \ \ w _ { 21 } x 
_ 1 + w _ { 22 } x _ 
2 + b _ 2 \ end { array } 
\ right \ 可以 将 上式 写成 齐次 的 形式 
\ \ left \ begin { array } { c 
} w _ { 11 } & w _ { 
12 } & b _ 1 \ \ w _ 
{ 21 } & w _ { 22 } & 
b _ 2 \ end { array } \ right 
\ cdot \ left \ begin { array } { 
c } x _ 1 \ \ x _ 2 
\ \ 1 \ end { array } \ right 
\ 使用 统一 的 矩阵 运算 在 正向 反向 传播 
的 时候 更 方便 在 输出 层 的 神经元 并 
没有 偏置 所以 要 单独 初始化 输出 层 的 权值 
矩阵 w = np . random . randn layers 2 
+ 1 layers 1 self . W . append w 
/ np . sqrt layers 2 下面 实现 Python 的 
magic function _ _ repr _ _ 输出 神经 网络结构 
def _ _ repr _ _ self return NeuralNetWork { 
} . format . join str l for l in 
self . layers 激活 函数 在 神经 网络 中 使用 
sigmoid 作为 激活 函数 实现 sigmoid 及其 导数 def sigmoid 
self x return 1.0 / 1 + np . exp 
x def sigmoid _ deriv self x return x * 
1 x 正向 反向 传播 这 一部分 是 神经 的 
网络 的 核心 了 下面 实现 fit 方法 在 方法 
中 完成 神经网络 权值 更新 训练 的 过程 def fit 
self X y epochs = 1000 displayUpdate = 100 X 
= np . c _ X np . ones X 
. shape 0 for epoch in np . arange 0 
epochs for x target in zip X y self . 
fit _ partial x target # check to see if 
we should display a training update if epoch = = 
0 or epoch + 1 % displayUpdate = = 0 
loss = self . calculate _ loss X y print 
INFO epoch = { } loss = { . 7f 
} . format epoch + 1 loss 该 函数 有 
4个 参数 X 是 输入 的 样本数据 y 是 样本 
的 真是 值 epochs 训练 的 轮数 displayUpdate 输出 训练 
的 loss 值 X = np . c _ X 
np . ones X . shape 0 将 输入 训练 
的 样本 表示 为 齐次 向量 也 就是 在 末尾 
添 1 fit _ partial 是 对 输入 的 每个 
样本 进行 训练 包括 正向 传播 反向 传播 以及 权值 
的 更新 def fit _ partial self x y A 
= np . atleast _ 2d x # 正向 传播 
# 层层 之间 的 数据 传递 for layer in np 
. arange 0 len self . W # 输入 经过 
加权 以及 偏 置后 的 值 net = A layer 
. dot self . W layer # 神经元 的 输出 
out = self . sigmoid net # 保存 下来 反向 
传播 的 时候 使用 A . append out 上面 完成 
了 神经 玩过 的 正向 传播 过程 下面 根据 反向 
传播 的 4个 基本 方程 进行 反向 传播 首先 根据 
\ BP1 \ \ \ delta ^ L = \ 
frac { \ partial e } { \ partial a 
^ L } \ odot \ sigma z ^ L 
\ tag { BP1 } \ 计算 输出 层 的 
误差 \ \ delta ^ L \ error = A 
1 y # 输出 层 的 误差 均值 方差 作为 
损失 函数 D = error * self . sigmoid _ 
deriv A 1 得到 输出 层 的 误差 D 后 
根据 \ BP2 \ 计算 各个 层 的 误差 \ 
\ delta ^ { L 1 } = W ^ 
L ^ T \ delta ^ L \ odot \ 
sigma z ^ { L 1 } \ tag { 
BP2 } \ for layer in np . arange len 
A 2 0 1 delta = D 1 . dot 
self . W layer . T delta = delta * 
self . sigmoid _ deriv A layer D . append 
delta D = D 1 将 D 反转 和 各个 
层 的 索引 对应 起来 下面 根据 \ BP3 BP4 
\ 计算 权值 矩阵 和 偏置 的 导数 \ \ 
frac { \ partial e } { b _ j 
^ l } = \ delta _ j ^ l 
\ tag { BP3 } \ \ \ frac { 
\ partial e } { w _ { jk } 
^ l } = \ delta _ j ^ l 
a _ k ^ { l 1 } \ tag 
{ BP4 } \ for layer in np . arange 
0 len self . W self . W layer + 
= self . alpha * A layer . T . 
dot D layer 首先 求得 权值 和 偏置 的 导数 
权值 和 偏置 统一 到 同一个 矩阵 中 A layer 
. T . dot D layer 然后 将 梯度 乘以 
学习 速率 alpha 每次 权值 减小 的 步长 上述 就 
完成 利用 反向 传播 算法 更新 权值 的 过程 关于 
反向 传播 四个 基本 方程 的 推导 过程 可以 参考 
文章 深度 学习 与 计算机 视觉 搞懂 反向 传播 算法 
的 四个 基本 方程 误差 评估 上面 代码 已经 实现 
了 深度 学习 的 训练 过程 下面 实现 predict 输出 
使用 训练 好 的 模型 预测 的 结果 calculate _ 
loss 评估 训练 后 模型 的 评估 def predict self 
X addBias = True p = np . atleast _ 
2d X if addBias p = np . c _ 
p np . ones p . shape 0 for layer 
in np . arange 0 len self . W p 
= self . sigmoid np . dot p self . 
W layer return p def calculate _ loss self X 
targets targets = np . atleast _ 2d targets predictions 
= self . predict X addBias = False loss = 
0.5 * np . sum predictions targets * * 2 
return lossMNIST 分类 识别 使用 上面 实现 的 深度 神经 
网络 对 MNIST 手写体 进行 识别 首先 导入 必要 的 
包 import NeuralNetwork from sklearn . preprocessing import LabelBinarizer from 
sklearn . model _ selection import train _ test _ 
split from sklearn . metrics import classification _ report from 
sklearn import datasets 需要 使用 sklearn 包中的/nr 一些 工具 进行 
数据 的 处理 # load MNIST 数据集 并 使用 min 
/ max 对 数据 进行 归一化 digits = datasets . 
load _ digits data = digits . data . astype 
float data = data data . min / data . 
max data . min print INFO samples { } dim 
{ } . format data . shape 0 data . 
shape 1 将 数据 拆分 为 训练 集 和 测试 
集 并对 MNIST 的 类别 进行 编码 trainX testX trainY 
testY = train _ test _ split data digits . 
target test _ size = 0.25 # convert the labels 
from integers to vectors trainY = LabelBinarizer . fit _ 
transform trainY testY = LabelBinarizer . fit _ transform testY 
下面 构建 神经 网络结构 并 使用 训练 集 进行 训练 
nn = NeuralNetwork data . shape 1 32 16 10 
print INFO { } . format nn nn . fit 
trainX trainY epochs = 1000 神经网络 结构 为 64 32 
16 10 其中 64 为 输入 数据 的 大小 10 
输出 类别 的 个数 最后 评估 训练 得到 的 模型 
predictions = nn . predict testX print classification _ report 
testY . argmax axis = 1 predictions . argmax axis 
= 1 最终 的 输出 结果 INFO loading MNIST sample 
dataset . . . INFO samples 1797 dim 64 INFO 
training network . . . INFO NeuralNetWork 64 32 16 
10 INFO epoch = 1 loss = 607.1711647 INFO epoch 
= 100 loss = 7.1082795 INFO epoch = 200 loss 
= 4.0731690 INFO epoch = 300 loss = 3.1401868 INFO 
epoch = 400 loss = 2.8801101 INFO epoch = 500 
loss = 1.8738122 INFO epoch = 600 loss = 1.7461474 
INFO epoch = 700 loss = 1.6624043 INFO epoch = 
800 loss = 1.1852884 INFO epoch = 900 loss = 
0.6710255 INFO epoch = 1000 loss = 0.6336826 INFO evaluating 
network . . . precision recall f1 score support 0 
1.00 0.95 0.97 39 1 0.84 1.00 0.92 38 2 
1.00 0.98 0.99 41 3 0.93 0.98 0.95 52 4 
0.91 0.97 0.94 40 5 0.98 0.98 0.98 41 6 
1.00 0.96 0.98 51 7 1.00 0.98 0.99 48 8 
0.98 0.89 0.93 55 9 0.98 0.93 0.95 45 micro 
avg 0.96 0.96 0.96 450 macro avg 0.96 0.96 0.96 
450 weighted avg 0.96 0.96 0.96 450 如上 测试 结果 
在 测试 集 的 上 表现 还 算 不错 总结 
本文 使用 Python 简单 的 实现 了 一个 神经 网络 
主要 是 利用 反向 传播 的 4个 基本 方程 实现 
反向 传播 算法 更新 各 个 神经元 的 权值 最后 
使用 该 网络 对 MNIST 数据 进行 识别 分类 上面 
实现 的 神经 网络 只是 玩具 用以 加深 对 深度 
学习 的 训练 过程 以及 反向 传播 算法 的 理解 
后面 将 使用 Keras 和 PyTorch 来 构建 神经网络 本文 
代码 在 git 库 https / / github . com 
/ brookicv / m a c h i n e 
L e a r n i n g a m 
p l e 在 前面 两 篇 文章 介绍 了 深度 学习 的 
一些 基本 概念 本文 则 使用 Python 实现 一个 简单 
的 深度 神经网络 并 使用 MNIST 数据库 进行 测试 神经 
网络 的 实现 包括 以下 内容 神经网络 权值 的 初始化 
正向 传播 误差 评估 反向 传播 更新 权值 主要 是 
根据 反向 传播 的 4个 基本 方程 利用 Python 实现 
神经 网络 的 反向 传播 初始化 首先 定义 代表 神经 
网络 的 类 NeuralNetwork class NeuralNetwork def _ _ init 
_ _ self layers alpha = 0.1 self . W 
= self . layers = layers self . alpha = 
alpha 有 三个 属性 W 存储 各个 层 之间 的 
权值 矩阵 也是 神经网络 要 更新 学习 的 layers 神经 
网络 的 结构 例如 2 2 1 表示 输入 层 
有 2个 神经元 隐藏 层 2个 神经元 输出 层 只有 
1个 神经元 alpha 学习 速率 接下来 初始化 各个 层 之间 
的 权值 矩阵 for i in np . arange 0 
len layers 2 w = np . random . randn 
layers i + 1 layers i + 1 + 1 
self . W . append w / np . sqrt 
layers i 注意 上面 生成 权值 矩阵 的 大小 layers 
i + 1 layers i + 1 + 1 都 
加了 1 这是 将 神经元 的 偏置 和 权值 统一 
的 放到 了 权值 矩阵 里面 \ \ left \ 
begin { array } { c } w _ { 
11 } & w _ { 12 } \ \ 
w _ { 21 } & w _ { 22 
} \ end { array } \ right \ cdot 
\ left \ begin { array } { c } 
x _ 1 \ \ x _ 2 \ end 
{ array } \ right + \ left \ begin 
{ array } { c } b _ 1 \ 
\ b _ 2 \ end { array } \ 
right = \ left \ begin { array } { 
c } w _ { 11 } x _ 1 
+ w { 12 } x _ 2 + b 
_ 1 \ \ w _ { 21 } x 
_ 1 + w _ { 22 } x _ 
2 + b _ 2 \ end { array } 
\ right \ 可以 将 上式 写成 齐次 的 形式 
\ \ left \ begin { array } { c 
} w _ { 11 } & w _ { 
12 } & b _ 1 \ \ w _ 
{ 21 } & w _ { 22 } & 
b _ 2 \ end { array } \ right 
\ cdot \ left \ begin { array } { 
c } x _ 1 \ \ x _ 2 
\ \ 1 \ end { array } \ right 
\ 使用 统一 的 矩阵 运算 在 正向 反向 传播 
的 时候 更 方便 在 输出 层 的 神经元 并 
没有 偏置 所以 要 单独 初始化 输出 层 的 权值 
矩阵 w = np . random . randn layers 2 
+ 1 layers 1 self . W . append w 
/ np . sqrt layers 2 下面 实现 Python 的 
magic function _ _ repr _ _ 输出 神经 网络结构 
def _ _ repr _ _ self return NeuralNetWork { 
} . format . join str l for l in 
self . layers 激活 函数 在 神经 网络 中 使用 
sigmoid 作为 激活 函数 实现 sigmoid 及其 导数 def sigmoid 
self x return 1.0 / 1 + np . exp 
x def sigmoid _ deriv self x return x * 
1 x 正向 反向 传播 这 一部分 是 神经 的 
网络 的 核心 了 下面 实现 fit 方法 在 方法 
中 完成 神经网络 权值 更新 训练 的 过程 def fit 
self X y epochs = 1000 displayUpdate = 100 X 
= np . c _ X np . ones X 
. shape 0 for epoch in np . arange 0 
epochs for x target in zip X y self . 
fit _ partial x target # check to see if 
we should display a training update if epoch = = 
0 or epoch + 1 % displayUpdate = = 0 
loss = self . calculate _ loss X y print 
INFO epoch = { } loss = { . 7f 
} . format epoch + 1 loss 该 函数 有 
4个 参数 X 是 输入 的 样本数据 y 是 样本 
的 真是 值 epochs 训练 的 轮数 displayUpdate 输出 训练 
的 loss 值 X = np . c _ X 
np . ones X . shape 0 将 输入 训练 
的 样本 表示 为 齐次 向量 也 就是 在 末尾 
添 1 fit _ partial 是 对 输入 的 每个 
样本 进行 训练 包括 正向 传播 反向 传播 以及 权值 
的 更新 def fit _ partial self x y A 
= np . atleast _ 2d x # 正向 传播 
# 层层 之间 的 数据 传递 for layer in np 
. arange 0 len self . W # 输入 经过 
加权 以及 偏 置后 的 值 net = A layer 
. dot self . W layer # 神经元 的 输出 
out = self . sigmoid net # 保存 下来 反向 
传播 的 时候 使用 A . append out 上面 完成 
了 神经 玩过 的 正向 传播 过程 下面 根据 反向 
传播 的 4个 基本 方程 进行 反向 传播 首先 根据 
\ BP1 \ \ \ delta ^ L = \ 
frac { \ partial e } { \ partial a 
^ L } \ odot \ sigma z ^ L 
\ tag { BP1 } \ 计算 输出 层 的 
误差 \ \ delta ^ L \ error = A 
1 y # 输出 层 的 误差 均值 方差 作为 
损失 函数 D = error * self . sigmoid _ 
deriv A 1 得到 输出 层 的 误差 D 后 
根据 \ BP2 \ 计算 各个 层 的 误差 \ 
\ delta ^ { L 1 } = W ^ 
L ^ T \ delta ^ L \ odot \ 
sigma z ^ { L 1 } \ tag { 
BP2 } \ for layer in np . arange len 
A 2 0 1 delta = D 1 . dot 
self . W layer . T delta = delta * 
self . sigmoid _ deriv A layer D . append 
delta D = D 1 将 D 反转 和 各个 
层 的 索引 对应 起来 下面 根据 \ BP3 BP4 
\ 计算 权值 矩阵 和 偏置 的 导数 \ \ 
frac { \ partial e } { b _ j 
^ l } = \ delta _ j ^ l 
\ tag { BP3 } \ \ \ frac { 
\ partial e } { w _ { jk } 
^ l } = \ delta _ j ^ l 
a _ k ^ { l 1 } \ tag 
{ BP4 } \ for layer in np . arange 
0 len self . W self . W layer + 
= self . alpha * A layer . T . 
dot D layer 首先 求得 权值 和 偏置 的 导数 
权值 和 偏置 统一 到 同一个 矩阵 中 A layer 
. T . dot D layer 然后 将 梯度 乘以 
学习 速率 alpha 每次 权值 减小 的 步长 上述 就 
完成 利用 反向 传播 算法 更新 权值 的 过程 关于 
反向 传播 四个 基本 方程 的 推导 过程 可以 参考 
文章 深度 学习 与 计算机 视觉 搞懂 反向 传播 算法 
的 四个 基本 方程 误差 评估 上面 代码 已经 实现 
了 深度 学习 的 训练 过程 下面 实现 predict 输出 
使用 训练 好 的 模型 预测 的 结果 calculate _ 
loss 评估 训练 后 模型 的 评估 def predict self 
X addBias = True p = np . atleast _ 
2d X if addBias p = np . c _ 
p np . ones p . shape 0 for layer 
in np . arange 0 len self . W p 
= self . sigmoid np . dot p self . 
W layer return p def calculate _ loss self X 
targets targets = np . atleast _ 2d targets predictions 
= self . predict X addBias = False loss = 
0.5 * np . sum predictions targets * * 2 
return lossMNIST 分类 识别 使用 上面 实现 的 深度 神经 
网络 对 MNIST 手写体 进行 识别 首先 导入 必要 的 
包 import NeuralNetwork from sklearn . preprocessing import LabelBinarizer from 
sklearn . model _ selection import train _ test _ 
split from sklearn . metrics import classification _ report from 
sklearn import datasets 需要 使用 sklearn 包中的/nr 一些 工具 进行 
数据 的 处理 # load MNIST 数据集 并 使用 min 
/ max 对 数据 进行 归一化 digits = datasets . 
load _ digits data = digits . data . astype 
float data = data data . min / data . 
max data . min print INFO samples { } dim 
{ } . format data . shape 0 data . 
shape 1 将 数据 拆分 为 训练 集 和 测试 
集 并对 MNIST 的 类别 进行 编码 trainX testX trainY 
testY = train _ test _ split data digits . 
target test _ size = 0.25 # convert the labels 
from integers to vectors trainY = LabelBinarizer . fit _ 
transform trainY testY = LabelBinarizer . fit _ transform testY 
下面 构建 神经 网络结构 并 使用 训练 集 进行 训练 
nn = NeuralNetwork data . shape 1 32 16 10 
print INFO { } . format nn nn . fit 
trainX trainY epochs = 1000 神经网络 结构 为 64 32 
16 10 其中 64 为 输入 数据 的 大小 10 
输出 类别 的 个数 最后 评估 训练 得到 的 模型 
predictions = nn . predict testX print classification _ report 
testY . argmax axis = 1 predictions . argmax axis 
= 1 最终 的 输出 结果 INFO loading MNIST sample 
dataset . . . INFO samples 1797 dim 64 INFO 
training network . . . INFO NeuralNetWork 64 32 16 
10 INFO epoch = 1 loss = 607.1711647 INFO epoch 
= 100 loss = 7.1082795 INFO epoch = 200 loss 
= 4.0731690 INFO epoch = 300 loss = 3.1401868 INFO 
epoch = 400 loss = 2.8801101 INFO epoch = 500 
loss = 1.8738122 INFO epoch = 600 loss = 1.7461474 
INFO epoch = 700 loss = 1.6624043 INFO epoch = 
800 loss = 1.1852884 INFO epoch = 900 loss = 
0.6710255 INFO epoch = 1000 loss = 0.6336826 INFO evaluating 
network . . . precision recall f1 score support 0 
1.00 0.95 0.97 39 1 0.84 1.00 0.92 38 2 
1.00 0.98 0.99 41 3 0.93 0.98 0.95 52 4 
0.91 0.97 0.94 40 5 0.98 0.98 0.98 41 6 
1.00 0.96 0.98 51 7 1.00 0.98 0.99 48 8 
0.98 0.89 0.93 55 9 0.98 0.93 0.95 45 micro 
avg 0.96 0.96 0.96 450 macro avg 0.96 0.96 0.96 
450 weighted avg 0.96 0.96 0.96 450 如上 测试 结果 
在 测试 集 的 上 表现 还 算 不错 总结 
本文 使用 Python 简单 的 实现 了 一个 神经 网络 
主要 是 利用 反向 传播 的 4个 基本 方程 实现 
反向 传播 算法 更新 各 个 神经元 的 权值 最后 
使用 该 网络 对 MNIST 数据 进行 识别 分类 上面 
实现 的 神经 网络 只是 玩具 用以 加深 对 深度 
学习 的 训练 过程 以及 反向 传播 算法 的 理解 
后面 将 使用 Keras 和 PyTorch 来 构建 神经网络 本文 
代码 在 git 库 https / / github . com 
/ brookicv / m a c h i n e 
L e a r n i n g a m 
p l e 