求职 季 真的 会 让 一个 人 变得 有些 不 
一样 吧 比如 我 对于 一个 菜鸟 来说 最近 一段 
时间 焦虑不安 外加 有点 自闭 . . . 前段 时间 
在 校内 网上 看到 了 陌陌 科技 内 推 计算机 
视觉 算法 工程师 和 机器学习 算法 工程师 的 消息 抱着 
试试 的 心态 整理 了 一份 简历 按照 提供 的 
邮箱 投 出去 了 我 想 这次 应该 又是 石沉大海 
了吧 谁想 在 一周 前 闷热 的 一天 在 嘈杂 
的 餐厅 接到 了 陌陌 科技 HR 的 电话 一周 
后的/nr 周五 下午 4点 在 西安 的 一家 咖啡馆 参加 
面试 我 问清 了 时间 地点 并 道 谢了 HR 
后并/nr 挂了 电话 继续 吃饭 好吧 这周/r 每天/r 都有/nr 各个/r 
公司/n 的/uj 笔试/n 外加 这周 周五 上午 的 组 会 
轮到 我 做 组 会 汇报 我 心里 预估 了 
一下 时间 安排 确实 没 时间 来 准备 陌陌 的 
面试 心想 就 这样 吧 面 挂了 就当 积累 经验 
吧 . . . 时间 很快 就 来到 了 周四 
晚上 当 我 9点 做完 招商 银行 的 网上 笔试 
后 打来 之前 没有 写完 的 明天 组 会 汇报 
的 ppt 接着 写了 起来 前 两天 已经 连续 凌晨 
2点 回 宿舍 了 今晚 不知 何时 能回/nr 我 主要 
给 大家 汇报 一下 近期 的 工作 以及 一篇 临时 
看 的 发表 在 ICB2018 上 的 使用 GAN 来 
完成 从热/nr 红外 到 可见光 的 跨 频谱 人脸 匹配 
的 文献 时间 来到 11 点半 ppt 算是 写得 差不多 
了 但是 文献 中 还是 有 很多 细节 问题 因为 
时间 关系 没有 搞懂 这篇 文献 里 最大 创新 点 
也 就是 所 提出 的 损失 函数 理解 得 云里雾里 
我 是 继续 加班 搞懂 才回 宿舍 呢 还是 就 
这样 将 这个 问题 放在 组 会上 大家 一起 讨论 
可是 明天 还得 早起 啊 有 那么 一瞬间 我 感觉 
呼吸 不 太 顺畅 身体 超负荷 运转 已经 吃不消 了 
我 选择 回 宿舍 休息 就 这样 吧 . . 
. 昨晚 还是 没有 睡好 已经 很久 没有 睡好 觉 
了 不过 相 对于 前 两天 已经 很 不错 了 
9点 组 会 开始 疲倦 始终 围绕 着 我 不出所料 
这次 组 会 因为 各种 因素 我 算是 搞 砸了 
. . . 组 会 完 后 回到 实验室 给 
手机 充电 打印 了 一份 简历 吃完 午饭 回来 打开 
百度 地图 搜索 那家 咖啡馆 的 地理 位置 并 做好 
时间 路线 规划 等待/v 手机/n 充/v 满电/nz 后我便/nr 出发/v 了/ul 
下午 3点 我 提前 一个 小时 到了 那家 咖啡馆 我 
微信 上 给 HR 发 消息 说 我 到了 进门 
之后 怎么走 那位 帅气 的 HR 带 我 进了 咖啡馆 
问了 下 我 的 姓名 和 求职 岗位 带我去 签完 
到 后给我/nr 找 一个 位置 并 了 给了 我 一张 
A4 纸 然后 就是 用 自己 熟悉 的 语言 实现 
两道 算法 题 我 周围 的 人都 是 今天 来 
面试 陌陌 的 他们 都在/nr 认真 的 低着头 写 代码 
第 一题 是 实现 输出 一个 长度 为 n 的 
无序 数组 中的 前 k 个 最小值 我 能 想到 
的 就是 先 通过 各种 排序算法 将 数组 排序 然后 
输出 前 k 个 最小值 就行了 但是 这 并 不是 
最好 的 方式 会 造成 复杂度 比较 高 因为 只 
需要 输出 前 k 个 最小值 剩下 的 n k 
个 数值 不 需要 考虑 那么 通过 排序算法 只需要 排 
前 k 个 数值 ok 了 不同 的 排序算法 时间 
复杂度 都是 不 一样 的 比如 比较 容易 实现 的 
选择 排序 和 冒泡排序 平均 情况 下 都是 O n2 
若 只 需要 找到 前面 的 k 个 值 复杂度 
也要 O n * k 若 使用 快速排序 复杂度 近似 
O n 如果 使用 堆排序 复杂度 近似 O nlogk 下面 
基于 堆排序 给出 解题 思路 以及 python3 代码 方法 是 
维护 k 个 元素 的 最大 堆 即用 容量 为 
k 的 最大 堆 存储 最先 遍历 到 的 k 
个数 并 假设 它们 即是 最小 的 k 个数 建 
堆 费时 O k 后 有 k1 k2 . . 
. kmax kmax 设为 大顶 堆 中 最大 元素 继续 
遍历 数列 每次 遍历 一个 元素 x 与 堆 顶 
元素 比较 x kmax 更新 堆 用时 logk 否则 不 
更新 堆 这样 下来 总 费时 O k + n 
k * logk = O n * logk 此 方法 
得益于 在 堆 中 查找 等 各项 操作 时间 复杂度 
均为 logk python3 代码 如下 1 # * coding utf 
8 * 2 3 Created on Sun Sep 2 17 
16 36 2018 4 5 @ author aoanng 6 7 
8 def create _ heap lyst 9 # 创建 数组 
中前 k 个数 的 最大 堆 10 for start in 
range len lyst 2 / / 2 1 1 11 
sift _ down lyst start len lyst 1 12 13 
14 return lyst 15 16 # 堆排序 对于 本 问题 
用不着 17 def heapSort lyst 18 # 堆排序 19 for 
end in range len lyst 1 0 1 20 lyst 
0 lyst end = lyst end lyst 0 21 sift 
_ down lyst 0 end 1 22 return lyst 23 
24 # 最 大堆 调整 25 def sift _ down 
lst start end 26 root = start 27 while True 
28 child = 2 * root + 1 29 if 
child end 30 break 31 if child + 1 = 
end and lst child lst child + 1 32 child 
+ = 1 33 if lst root lst child 34 
lst root lst child = lst child lst root 35 
root = child 36 else 37 break 38 39 40 
# 测试 41 if _ _ name _ _ = 
= _ _ main _ _ 42 list1 = 50 
45 40 20 25 35 30 10 15 43 k 
= 4 # 设置 需要 输出 的 前 k 个 
最小值 44 list _ n _ k = list1 k 
45 heap _ k = create _ heap list1 k 
# 将 数组 前 k 个数 创建 最 大堆 并 
假设 它们 是 最小 的 k 个数 46 for i 
in range len list _ n _ k 47 if 
list _ n _ k i heap _ k 0 
48 heap _ k 0 = list _ n _ 
k i 49 heap _ k = create _ heap 
heap _ k # 更新 堆 50 print heap _ 
k # 输出 前 k 个 未 排序 的 最小值 
51 52 # 若 需要 则 可以 对 堆 进行 
排序 53 heap _ k _ sort = heapSort heap 
_ k 54 print heap _ k _ sort 第 
二题 是 给出 一个 n * n 的 矩阵 将其 
逆时针 旋转 90度 但是 不能 开辟 新的 内存空间 这 题 
的 前提 是 必须 在 原 数组 上 进行 旋转 
操作 只要 搞清楚 矩阵 中 元素 旋转 的 规律 就 
容易 求 解了 那 就是 当前 元素 ai j 经过 
逆时针 旋转 90度 后有 ai j = aj n i 
的 关系 写完 这 两道 算法 题 后 我 检查 
了 两遍 并 在那儿 扣 手机 hr 看见 我 写完 
之后 过来 收 走了 我 的 作业 让 我 等待 
一会儿 大概 20 分钟 后 一位 技术 面试官 带着 我 
的 简历 以及 之前 写好 的 算法 题 过来 找 
我 第一 轮 技术 面试 便 开始 了 我 先 
简单 的 自我 介绍 后 面试官 看着 我 的 写 
的 那 两道 算法 题 聊了 起来 让 我 说说 
我 的 思路 我 说 第一 题 其实 就是 一个 
排序算法 问题 然后 输出 前 k 个 值 就好 看见 
我 用 的 选择 排序算法 面试官 指出 了 疑问 说 
这样 时间 复杂度 会 比较 高 然后 问 我 有 
没有 其他 的 思路 我 说 可以 只 需要 排 
前 k 个 值 将 时间 复杂度 降到 O n 
* k 面试官 最后 逐步 的 引导 我 说 用 
最大 堆 会 比较 好 总之 面试 官人 很 nice 
问到 我 不会 的 总是 在 引导 我 然后 就是 
介绍 我 简历 中 做过 的 几个 项目 项目/n 当中/s 
有/v 用到/v 深度/ns 学习/v 平台/n tensorflow/w 和/c CNN/w 网络/n 架构/n 
以及/c 一些/m 机器学习/i 算法/n 面试官 逐个 问 我 比如 在 
ttensorflow 中 怎样 构建 一个 cnn 网络 防止 过拟合 的 
一些 tips Dropout 是 怎样 工作 的 等等 然后 让 
我 手 写在 tensorflow 中 怎样 保存 模型 和 加载 
模型 tf . get _ variable 和 tf . Variable 
的 区别 tf/w ./i variable/w _/i scope/w 和/c tf/w ./i 
name/w _/i scope/w 的/uj 用法/n 和/c 区别/n 这些 其实 我 
平时 项目 中 也有 用到 平时 也 关注 过 这个 
问题 只是 没有 上心 当时 没有 回答 上来 然后 面试官 
大致 的 给 我 讲了 一下 原理 就 跳过 这个 
问题 了 回来/v 后我又/nr 在/p 网上/s 查了/nr 一下/m 资料/n 总结 
如下 tf . variable _ scope 和 tf . name 
_ scope 的 用法 tf . variable _ scope 可以 
让 变量 有 相同 的 命名 包括 tf . get 
_ variable 得到 的 变量 还有 tf . Variable 的 
变量 tf . name _ scope 可以 让 变量 有 
相同 的 命名 只是 限于 tf . Variable 的 变量 
例如 1 import tensorflow as tf 2 3 with tf 
. variable _ scope V1 4 a1 = tf . 
get _ variable name = a1 shape = 1 initializer 
= tf . constant _ initializer 1 5 a2 = 
tf . Variable tf . random _ normal shape = 
2 3 mean = 0 stddev = 1 name = 
a2 6 with tf . variable _ scope V2 7 
a3 = tf . get _ variable name = a1 
shape = 1 initializer = tf . constant _ initializer 
1 8 a4 = tf . Variable tf . random 
_ normal shape = 2 3 mean = 0 stddev 
= 1 name = a2 9 10 with tf . 
Session as sess 11 sess . run tf . global 
_ variables _ initializer 12 print a1 . name 13 
print a2 . name 14 print a3 . name 15 
print a4 . name 16 17 # 输出 18 19 
V1 / a1 0 20 V1 / a2 0 21 
V2 / a1 0 22 V2 / a2 0 23 
如果 将 上边 的 tf . variable _ scope 换成 
tf . name _ scope 将会 报错 Variable a1 already 
exists disallowed . Did you mean to set reuse = 
True or reuse = tf . AUTO _ REUSE in 
VarScope . . . 改成 如下 这样 就 ok 了 
1 import tensorflow as tf 2 3 with tf . 
name _ scope V1 4 # a1 = tf . 
get _ variable name = a1 shape = 1 initializer 
= tf . constant _ initializer 1 5 a2 = 
tf . Variable tf . random _ normal shape = 
2 3 mean = 0 stddev = 1 name = 
a2 6 with tf . name _ scope V2 7 
# a3 = tf . get _ variable name = 
a1 shape = 1 initializer = tf . constant _ 
initializer 1 8 a4 = tf . Variable tf . 
random _ normal shape = 2 3 mean = 0 
stddev = 1 name = a2 9 10 with tf 
. Session as sess 11 sess . run tf . 
global _ variables _ initializer 12 # print a1 . 
name 13 print a2 . name 14 # print a3 
. name 15 print a4 . name 16 17 # 
输出 18 19 V1 / a2 0 20 V2 / 
a2 0 21 接下 来 看看 tf . Variable 和 
tf . get _ variable 的 区别 在 tensorflow 中 
tf . Variable 和 tf . get _ variable 两个 
op 分别 用来 创建 变量 tf . Variable 总是 创建 
新的 变量 返回 一个 variable 可以 定义 名字 相同 的 
变量 若 给出 的 name 已经 存在 会 自动 修改 
name 生成 个 新的 1 import tensorflow as tf 2 
w _ 1 = tf . Variable 3 name = 
w _ 1 3 w _ 2 = tf . 
Variable 1 name = w _ 1 4 print w 
_ 1 . name 5 print w _ 2 . 
name 6 # 输出 7 # w _ 1 0 
8 # w _ 1 _ 1 0tf . get 
_ variable 不可以 定义 名字 相同 的 变量 tf . 
get _ variable 函数 拥有 一个 变量 检查 机制 会 
检测 已经 存在 的 变量 是否 设置 为 共享 变量 
如果 已经 存在 的 变量 没有 设置 为 共享 变量 
TensorFlow 运行 到 第二 个 拥有 相同 名字 的 变量 
的 时候 就会 报错 不同 的 变量 之间 不能 有 
相同 的 名字 除非 你 定义 了 variable _ scope 
这样 才 可以 有 相同 的 名字 1 import tensorflow 
as tf 2 3 w _ 1 = tf . 
get _ variable name = w _ 1 initializer = 
1 4 w _ 2 = tf . get _ 
variable name = w _ 1 initializer = 2 5 
# 错误信息 6 # ValueError Variable w _ 1 already 
exists disallowed . Did 7 # you mean to set 
reuse = True in VarScope tf . get _ variable 
一般 和 tf . variable _ scope 配合 使用 用于 
在 同一个 的 变量 域中 共享 同一个 变量 如何 在 
tensorflow 中 保存 和 加载 模型 呢 构建 网络 中 
加入 saver = tf . train . Saver 然后 在 
session 会 话中 saver . save sess . / model 
/ model . ckpt 加载 模型 构建 网络 中 需要 
和 之前 一样 然后 在 session 会 话中 加载 模型 
saver . restore sess . / model / model . 
ckpt 然后 和 面试官 讨论 一些 机器学习 算法 的 问题 
诸如 LR 和 SVM 的 区别 随机 森林 和 GBDT 
区别 xgboost 以及 最 优化 算法 的 原理 等等 很快 
一个 多 小时 就 过去 啦 感觉 自己 表现 得 
不是太好 但是 和 面试官 还是 挺 聊 得来 的 面试 
的 最后 面试官 问 我 对 一些 经典 的 数据结构 
熟悉 不 我 说 还 可以 然后 他 让 我 
现场 写 一个 单链表 的 逆序 很 简单 的 问题 
我 却没 写出来 我 曾经 看过 java 版本 和c/nr 版本 
的 数据 结构 与 算法 前 不久 也 看过 用 
python 实现 的 数据 结构 与 算法 但是 这个 时候 
我 却 卡住 了 10/m 分钟/q 后/f 面试官/n 看/v 我/r 
连/nr 这个/r 简单/a 的/uj 问题/n 都没/i 写出来/i 笑着 对 我 
说 该不该 给 我 第二 轮 技术 面 的 机会 
然后 第一 轮 技术 面 就 这样 结束 了 让 
我 在 旁边 的 椅子 上 稍等一下 . . . 
等 的 过程 中 我 拿出 手机 百度 了 下 
单链表 的 逆序 如何 实现 恍然大悟 的 同时 也 有点 
懊恼 参考 网上 的 答案 实现 如下 循环 反转 单链表 
1 # 定义 一个 单链表 节点 2 class ListNode 3 
def _ _ init _ _ self x 4 self 
. data = x 5 self . next = None 
6 7 def nonrecurse head # 循环 的 方法 反转 
链表 8 if head is None or head . next 
is None 9 return head 10 pre = None 11 
cur = head 12 h = head 13 while cur 
14 h = cur 15 tmp = cur . next 
16 cur . next = pre 17 pre = cur 
18 cur = tmp 19 return h 20 21 head 
= ListNode 1 # 测试代码 22 p1 = ListNode 2 
# 建立 链表 1 2 3 4 None 23 p2 
= ListNode 3 # head p1 p2 p3 None 24 
p3 = ListNode 4 25 head . next = p1 
26 p1 . next = p2 27 p2 . next 
= p3 28 29 p = nonrecurse head # 输出 
链表 4 3 2 1 None 30 while p 31 
print p . data 32 p = p . next 
递归 实现 单链表 反转 1 class ListNode 2 def _ 
_ init _ _ self x 3 self . val 
= x 4 self . next = None 5 6 
7 def recurse head newhead # 递归 head 为 原 
链表 的 头 结点 newhead 为 反转 后 链表 的 
头 结点 8 if head is None 9 return 10 
if head . next is None 11 newhead = head 
12 else 13 newhead = recurse head . next newhead 
14 head . next . next = head 15 head 
. next = None 16 return newhead 17 18 head 
= ListNode 1 # 测试代码 19 p1 = ListNode 2 
# 建立 链表 1 2 3 4 None 20 p2 
= ListNode 3 21 p3 = ListNode 4 22 head 
. next = p1 23 p1 . next = p2 
24 p2 . next = p3 25 newhead = None 
26 p = recurse head newhead # 输出 链表 4 
3 2 1 None 27 while p 28 print p 
. val 29 p = p . next 接下来 就是 
技术 第二面 了 不说 了 说 多了 都是 泪 . 
. . 参考 窥探 算法 之 美妙 寻找 数组 中 
最小 的 K 个数 & python 中 巧用 最 大堆 
程序员 编程 艺术 第三章 寻找 最小 的 k 个数 tf 
. variable _ scope 和 tf . name _ scope 
的 用法 tf . Variable 与 tf . get _ 
variable 与 不同之处 TensorFlow 模型 保存 和 加载 方法 单链表 
反转 python 实现 求职 季 真的 会 让 一个 人 变得 有些 不 
一样 吧 比如 我 对于 一个 菜鸟 来说 最近 一段 
时间 焦虑不安 外加 有点 自闭 . . . 前段 时间 
在 校内 网上 看到 了 陌陌 科技 内 推 计算机 
视觉 算法 工程师 和 机器学习 算法 工程师 的 消息 抱着 
试试 的 心态 整理 了 一份 简历 按照 提供 的 
邮箱 投 出去 了 我 想 这次 应该 又是 石沉大海 
了吧 谁想 在 一周 前 闷热 的 一天 在 嘈杂 
的 餐厅 接到 了 陌陌 科技 HR 的 电话 一周 
后的/nr 周五 下午 4点 在 西安 的 一家 咖啡馆 参加 
面试 我 问清 了 时间 地点 并 道 谢了 HR 
后并/nr 挂了 电话 继续 吃饭 好吧 这周/r 每天/r 都有/nr 各个/r 
公司/n 的/uj 笔试/n 外加 这周 周五 上午 的 组 会 
轮到 我 做 组 会 汇报 我 心里 预估 了 
一下 时间 安排 确实 没 时间 来 准备 陌陌 的 
面试 心想 就 这样 吧 面 挂了 就当 积累 经验 
吧 . . . 时间 很快 就 来到 了 周四 
晚上 当 我 9点 做完 招商 银行 的 网上 笔试 
后 打来 之前 没有 写完 的 明天 组 会 汇报 
的 ppt 接着 写了 起来 前 两天 已经 连续 凌晨 
2点 回 宿舍 了 今晚 不知 何时 能回/nr 我 主要 
给 大家 汇报 一下 近期 的 工作 以及 一篇 临时 
看 的 发表 在 ICB2018 上 的 使用 GAN 来 
完成 从热/nr 红外 到 可见光 的 跨 频谱 人脸 匹配 
的 文献 时间 来到 11 点半 ppt 算是 写得 差不多 
了 但是 文献 中 还是 有 很多 细节 问题 因为 
时间 关系 没有 搞懂 这篇 文献 里 最大 创新 点 
也 就是 所 提出 的 损失 函数 理解 得 云里雾里 
我 是 继续 加班 搞懂 才回 宿舍 呢 还是 就 
这样 将 这个 问题 放在 组 会上 大家 一起 讨论 
可是 明天 还得 早起 啊 有 那么 一瞬间 我 感觉 
呼吸 不 太 顺畅 身体 超负荷 运转 已经 吃不消 了 
我 选择 回 宿舍 休息 就 这样 吧 . . 
. 昨晚 还是 没有 睡好 已经 很久 没有 睡好 觉 
了 不过 相 对于 前 两天 已经 很 不错 了 
9点 组 会 开始 疲倦 始终 围绕 着 我 不出所料 
这次 组 会 因为 各种 因素 我 算是 搞 砸了 
. . . 组 会 完 后 回到 实验室 给 
手机 充电 打印 了 一份 简历 吃完 午饭 回来 打开 
百度 地图 搜索 那家 咖啡馆 的 地理 位置 并 做好 
时间 路线 规划 等待/v 手机/n 充/v 满电/nz 后我便/nr 出发/v 了/ul 
下午 3点 我 提前 一个 小时 到了 那家 咖啡馆 我 
微信 上 给 HR 发 消息 说 我 到了 进门 
之后 怎么走 那位 帅气 的 HR 带 我 进了 咖啡馆 
问了 下 我 的 姓名 和 求职 岗位 带我去 签完 
到 后给我/nr 找 一个 位置 并 了 给了 我 一张 
A4 纸 然后 就是 用 自己 熟悉 的 语言 实现 
两道 算法 题 我 周围 的 人都 是 今天 来 
面试 陌陌 的 他们 都在/nr 认真 的 低着头 写 代码 
第 一题 是 实现 输出 一个 长度 为 n 的 
无序 数组 中的 前 k 个 最小值 我 能 想到 
的 就是 先 通过 各种 排序算法 将 数组 排序 然后 
输出 前 k 个 最小值 就行了 但是 这 并 不是 
最好 的 方式 会 造成 复杂度 比较 高 因为 只 
需要 输出 前 k 个 最小值 剩下 的 n k 
个 数值 不 需要 考虑 那么 通过 排序算法 只需要 排 
前 k 个 数值 ok 了 不同 的 排序算法 时间 
复杂度 都是 不 一样 的 比如 比较 容易 实现 的 
选择 排序 和 冒泡排序 平均 情况 下 都是 O n2 
若 只 需要 找到 前面 的 k 个 值 复杂度 
也要 O n * k 若 使用 快速排序 复杂度 近似 
O n 如果 使用 堆排序 复杂度 近似 O nlogk 下面 
基于 堆排序 给出 解题 思路 以及 python3 代码 方法 是 
维护 k 个 元素 的 最大 堆 即用 容量 为 
k 的 最大 堆 存储 最先 遍历 到 的 k 
个数 并 假设 它们 即是 最小 的 k 个数 建 
堆 费时 O k 后 有 k1 k2 . . 
. kmax kmax 设为 大顶 堆 中 最大 元素 继续 
遍历 数列 每次 遍历 一个 元素 x 与 堆 顶 
元素 比较 x kmax 更新 堆 用时 logk 否则 不 
更新 堆 这样 下来 总 费时 O k + n 
k * logk = O n * logk 此 方法 
得益于 在 堆 中 查找 等 各项 操作 时间 复杂度 
均为 logk python3 代码 如下 1 # * coding utf 
8 * 2 3 Created on Sun Sep 2 17 
16 36 2018 4 5 @ author aoanng 6 7 
8 def create _ heap lyst 9 # 创建 数组 
中前 k 个数 的 最大 堆 10 for start in 
range len lyst 2 / / 2 1 1 11 
sift _ down lyst start len lyst 1 12 13 
14 return lyst 15 16 # 堆排序 对于 本 问题 
用不着 17 def heapSort lyst 18 # 堆排序 19 for 
end in range len lyst 1 0 1 20 lyst 
0 lyst end = lyst end lyst 0 21 sift 
_ down lyst 0 end 1 22 return lyst 23 
24 # 最 大堆 调整 25 def sift _ down 
lst start end 26 root = start 27 while True 
28 child = 2 * root + 1 29 if 
child end 30 break 31 if child + 1 = 
end and lst child lst child + 1 32 child 
+ = 1 33 if lst root lst child 34 
lst root lst child = lst child lst root 35 
root = child 36 else 37 break 38 39 40 
# 测试 41 if _ _ name _ _ = 
= _ _ main _ _ 42 list1 = 50 
45 40 20 25 35 30 10 15 43 k 
= 4 # 设置 需要 输出 的 前 k 个 
最小值 44 list _ n _ k = list1 k 
45 heap _ k = create _ heap list1 k 
# 将 数组 前 k 个数 创建 最 大堆 并 
假设 它们 是 最小 的 k 个数 46 for i 
in range len list _ n _ k 47 if 
list _ n _ k i heap _ k 0 
48 heap _ k 0 = list _ n _ 
k i 49 heap _ k = create _ heap 
heap _ k # 更新 堆 50 print heap _ 
k # 输出 前 k 个 未 排序 的 最小值 
51 52 # 若 需要 则 可以 对 堆 进行 
排序 53 heap _ k _ sort = heapSort heap 
_ k 54 print heap _ k _ sort 第 
二题 是 给出 一个 n * n 的 矩阵 将其 
逆时针 旋转 90度 但是 不能 开辟 新的 内存空间 这 题 
的 前提 是 必须 在 原 数组 上 进行 旋转 
操作 只要 搞清楚 矩阵 中 元素 旋转 的 规律 就 
容易 求 解了 那 就是 当前 元素 ai j 经过 
逆时针 旋转 90度 后有 ai j = aj n i 
的 关系 写完 这 两道 算法 题 后 我 检查 
了 两遍 并 在那儿 扣 手机 hr 看见 我 写完 
之后 过来 收 走了 我 的 作业 让 我 等待 
一会儿 大概 20 分钟 后 一位 技术 面试官 带着 我 
的 简历 以及 之前 写好 的 算法 题 过来 找 
我 第一 轮 技术 面试 便 开始 了 我 先 
简单 的 自我 介绍 后 面试官 看着 我 的 写 
的 那 两道 算法 题 聊了 起来 让 我 说说 
我 的 思路 我 说 第一 题 其实 就是 一个 
排序算法 问题 然后 输出 前 k 个 值 就好 看见 
我 用 的 选择 排序算法 面试官 指出 了 疑问 说 
这样 时间 复杂度 会 比较 高 然后 问 我 有 
没有 其他 的 思路 我 说 可以 只 需要 排 
前 k 个 值 将 时间 复杂度 降到 O n 
* k 面试官 最后 逐步 的 引导 我 说 用 
最大 堆 会 比较 好 总之 面试 官人 很 nice 
问到 我 不会 的 总是 在 引导 我 然后 就是 
介绍 我 简历 中 做过 的 几个 项目 项目/n 当中/s 
有/v 用到/v 深度/ns 学习/v 平台/n tensorflow/w 和/c CNN/w 网络/n 架构/n 
以及/c 一些/m 机器学习/i 算法/n 面试官 逐个 问 我 比如 在 
ttensorflow 中 怎样 构建 一个 cnn 网络 防止 过拟合 的 
一些 tips Dropout 是 怎样 工作 的 等等 然后 让 
我 手 写在 tensorflow 中 怎样 保存 模型 和 加载 
模型 tf . get _ variable 和 tf . Variable 
的 区别 tf/w ./i variable/w _/i scope/w 和/c tf/w ./i 
name/w _/i scope/w 的/uj 用法/n 和/c 区别/n 这些 其实 我 
平时 项目 中 也有 用到 平时 也 关注 过 这个 
问题 只是 没有 上心 当时 没有 回答 上来 然后 面试官 
大致 的 给 我 讲了 一下 原理 就 跳过 这个 
问题 了 回来/v 后我又/nr 在/p 网上/s 查了/nr 一下/m 资料/n 总结 
如下 tf . variable _ scope 和 tf . name 
_ scope 的 用法 tf . variable _ scope 可以 
让 变量 有 相同 的 命名 包括 tf . get 
_ variable 得到 的 变量 还有 tf . Variable 的 
变量 tf . name _ scope 可以 让 变量 有 
相同 的 命名 只是 限于 tf . Variable 的 变量 
例如 1 import tensorflow as tf 2 3 with tf 
. variable _ scope V1 4 a1 = tf . 
get _ variable name = a1 shape = 1 initializer 
= tf . constant _ initializer 1 5 a2 = 
tf . Variable tf . random _ normal shape = 
2 3 mean = 0 stddev = 1 name = 
a2 6 with tf . variable _ scope V2 7 
a3 = tf . get _ variable name = a1 
shape = 1 initializer = tf . constant _ initializer 
1 8 a4 = tf . Variable tf . random 
_ normal shape = 2 3 mean = 0 stddev 
= 1 name = a2 9 10 with tf . 
Session as sess 11 sess . run tf . global 
_ variables _ initializer 12 print a1 . name 13 
print a2 . name 14 print a3 . name 15 
print a4 . name 16 17 # 输出 18 19 
V1 / a1 0 20 V1 / a2 0 21 
V2 / a1 0 22 V2 / a2 0 23 
如果 将 上边 的 tf . variable _ scope 换成 
tf . name _ scope 将会 报错 Variable a1 already 
exists disallowed . Did you mean to set reuse = 
True or reuse = tf . AUTO _ REUSE in 
VarScope . . . 改成 如下 这样 就 ok 了 
1 import tensorflow as tf 2 3 with tf . 
name _ scope V1 4 # a1 = tf . 
get _ variable name = a1 shape = 1 initializer 
= tf . constant _ initializer 1 5 a2 = 
tf . Variable tf . random _ normal shape = 
2 3 mean = 0 stddev = 1 name = 
a2 6 with tf . name _ scope V2 7 
# a3 = tf . get _ variable name = 
a1 shape = 1 initializer = tf . constant _ 
initializer 1 8 a4 = tf . Variable tf . 
random _ normal shape = 2 3 mean = 0 
stddev = 1 name = a2 9 10 with tf 
. Session as sess 11 sess . run tf . 
global _ variables _ initializer 12 # print a1 . 
name 13 print a2 . name 14 # print a3 
. name 15 print a4 . name 16 17 # 
输出 18 19 V1 / a2 0 20 V2 / 
a2 0 21 接下 来 看看 tf . Variable 和 
tf . get _ variable 的 区别 在 tensorflow 中 
tf . Variable 和 tf . get _ variable 两个 
op 分别 用来 创建 变量 tf . Variable 总是 创建 
新的 变量 返回 一个 variable 可以 定义 名字 相同 的 
变量 若 给出 的 name 已经 存在 会 自动 修改 
name 生成 个 新的 1 import tensorflow as tf 2 
w _ 1 = tf . Variable 3 name = 
w _ 1 3 w _ 2 = tf . 
Variable 1 name = w _ 1 4 print w 
_ 1 . name 5 print w _ 2 . 
name 6 # 输出 7 # w _ 1 0 
8 # w _ 1 _ 1 0tf . get 
_ variable 不可以 定义 名字 相同 的 变量 tf . 
get _ variable 函数 拥有 一个 变量 检查 机制 会 
检测 已经 存在 的 变量 是否 设置 为 共享 变量 
如果 已经 存在 的 变量 没有 设置 为 共享 变量 
TensorFlow 运行 到 第二 个 拥有 相同 名字 的 变量 
的 时候 就会 报错 不同 的 变量 之间 不能 有 
相同 的 名字 除非 你 定义 了 variable _ scope 
这样 才 可以 有 相同 的 名字 1 import tensorflow 
as tf 2 3 w _ 1 = tf . 
get _ variable name = w _ 1 initializer = 
1 4 w _ 2 = tf . get _ 
variable name = w _ 1 initializer = 2 5 
# 错误信息 6 # ValueError Variable w _ 1 already 
exists disallowed . Did 7 # you mean to set 
reuse = True in VarScope tf . get _ variable 
一般 和 tf . variable _ scope 配合 使用 用于 
在 同一个 的 变量 域中 共享 同一个 变量 如何 在 
tensorflow 中 保存 和 加载 模型 呢 构建 网络 中 
加入 saver = tf . train . Saver 然后 在 
session 会 话中 saver . save sess . / model 
/ model . ckpt 加载 模型 构建 网络 中 需要 
和 之前 一样 然后 在 session 会 话中 加载 模型 
saver . restore sess . / model / model . 
ckpt 然后 和 面试官 讨论 一些 机器学习 算法 的 问题 
诸如 LR 和 SVM 的 区别 随机 森林 和 GBDT 
区别 xgboost 以及 最 优化 算法 的 原理 等等 很快 
一个 多 小时 就 过去 啦 感觉 自己 表现 得 
不是太好 但是 和 面试官 还是 挺 聊 得来 的 面试 
的 最后 面试官 问 我 对 一些 经典 的 数据结构 
熟悉 不 我 说 还 可以 然后 他 让 我 
现场 写 一个 单链表 的 逆序 很 简单 的 问题 
我 却没 写出来 我 曾经 看过 java 版本 和c/nr 版本 
的 数据 结构 与 算法 前 不久 也 看过 用 
python 实现 的 数据 结构 与 算法 但是 这个 时候 
我 却 卡住 了 10/m 分钟/q 后/f 面试官/n 看/v 我/r 
连/nr 这个/r 简单/a 的/uj 问题/n 都没/i 写出来/i 笑着 对 我 
说 该不该 给 我 第二 轮 技术 面 的 机会 
然后 第一 轮 技术 面 就 这样 结束 了 让 
我 在 旁边 的 椅子 上 稍等一下 . . . 
等 的 过程 中 我 拿出 手机 百度 了 下 
单链表 的 逆序 如何 实现 恍然大悟 的 同时 也 有点 
懊恼 参考 网上 的 答案 实现 如下 循环 反转 单链表 
1 # 定义 一个 单链表 节点 2 class ListNode 3 
def _ _ init _ _ self x 4 self 
. data = x 5 self . next = None 
6 7 def nonrecurse head # 循环 的 方法 反转 
链表 8 if head is None or head . next 
is None 9 return head 10 pre = None 11 
cur = head 12 h = head 13 while cur 
14 h = cur 15 tmp = cur . next 
16 cur . next = pre 17 pre = cur 
18 cur = tmp 19 return h 20 21 head 
= ListNode 1 # 测试代码 22 p1 = ListNode 2 
# 建立 链表 1 2 3 4 None 23 p2 
= ListNode 3 # head p1 p2 p3 None 24 
p3 = ListNode 4 25 head . next = p1 
26 p1 . next = p2 27 p2 . next 
= p3 28 29 p = nonrecurse head # 输出 
链表 4 3 2 1 None 30 while p 31 
print p . data 32 p = p . next 
递归 实现 单链表 反转 1 class ListNode 2 def _ 
_ init _ _ self x 3 self . val 
= x 4 self . next = None 5 6 
7 def recurse head newhead # 递归 head 为 原 
链表 的 头 结点 newhead 为 反转 后 链表 的 
头 结点 8 if head is None 9 return 10 
if head . next is None 11 newhead = head 
12 else 13 newhead = recurse head . next newhead 
14 head . next . next = head 15 head 
. next = None 16 return newhead 17 18 head 
= ListNode 1 # 测试代码 19 p1 = ListNode 2 
# 建立 链表 1 2 3 4 None 20 p2 
= ListNode 3 21 p3 = ListNode 4 22 head 
. next = p1 23 p1 . next = p2 
24 p2 . next = p3 25 newhead = None 
26 p = recurse head newhead # 输出 链表 4 
3 2 1 None 27 while p 28 print p 
. val 29 p = p . next 接下来 就是 
技术 第二面 了 不说 了 说 多了 都是 泪 . 
. . 参考 窥探 算法 之 美妙 寻找 数组 中 
最小 的 K 个数 & python 中 巧用 最 大堆 
程序员 编程 艺术 第三章 寻找 最小 的 k 个数 tf 
. variable _ scope 和 tf . name _ scope 
的 用法 tf . Variable 与 tf . get _ 
variable 与 不同之处 TensorFlow 模型 保存 和 加载 方法 单链表 
反转 python 实现 求职 季 真的 会 让 一个 人 变得 有些 不 
一样 吧 比如 我 对于 一个 菜鸟 来说 最近 一段 
时间 焦虑不安 外加 有点 自闭 . . . 前段 时间 
在 校内 网上 看到 了 陌陌 科技 内 推 计算机 
视觉 算法 工程师 和 机器学习 算法 工程师 的 消息 抱着 
试试 的 心态 整理 了 一份 简历 按照 提供 的 
邮箱 投 出去 了 我 想 这次 应该 又是 石沉大海 
了吧 谁想 在 一周 前 闷热 的 一天 在 嘈杂 
的 餐厅 接到 了 陌陌 科技 HR 的 电话 一周 
后的/nr 周五 下午 4点 在 西安 的 一家 咖啡馆 参加 
面试 我 问清 了 时间 地点 并 道 谢了 HR 
后并/nr 挂了 电话 继续 吃饭 好吧 这周/r 每天/r 都有/nr 各个/r 
公司/n 的/uj 笔试/n 外加 这周 周五 上午 的 组 会 
轮到 我 做 组 会 汇报 我 心里 预估 了 
一下 时间 安排 确实 没 时间 来 准备 陌陌 的 
面试 心想 就 这样 吧 面 挂了 就当 积累 经验 
吧 . . . 时间 很快 就 来到 了 周四 
晚上 当 我 9点 做完 招商 银行 的 网上 笔试 
后 打来 之前 没有 写完 的 明天 组 会 汇报 
的 ppt 接着 写了 起来 前 两天 已经 连续 凌晨 
2点 回 宿舍 了 今晚 不知 何时 能回/nr 我 主要 
给 大家 汇报 一下 近期 的 工作 以及 一篇 临时 
看 的 发表 在 ICB2018 上 的 使用 GAN 来 
完成 从热/nr 红外 到 可见光 的 跨 频谱 人脸 匹配 
的 文献 时间 来到 11 点半 ppt 算是 写得 差不多 
了 但是 文献 中 还是 有 很多 细节 问题 因为 
时间 关系 没有 搞懂 这篇 文献 里 最大 创新 点 
也 就是 所 提出 的 损失 函数 理解 得 云里雾里 
我 是 继续 加班 搞懂 才回 宿舍 呢 还是 就 
这样 将 这个 问题 放在 组 会上 大家 一起 讨论 
可是 明天 还得 早起 啊 有 那么 一瞬间 我 感觉 
呼吸 不 太 顺畅 身体 超负荷 运转 已经 吃不消 了 
我 选择 回 宿舍 休息 就 这样 吧 . . 
. 昨晚 还是 没有 睡好 已经 很久 没有 睡好 觉 
了 不过 相 对于 前 两天 已经 很 不错 了 
9点 组 会 开始 疲倦 始终 围绕 着 我 不出所料 
这次 组 会 因为 各种 因素 我 算是 搞 砸了 
. . . 组 会 完 后 回到 实验室 给 
手机 充电 打印 了 一份 简历 吃完 午饭 回来 打开 
百度 地图 搜索 那家 咖啡馆 的 地理 位置 并 做好 
时间 路线 规划 等待/v 手机/n 充/v 满电/nz 后我便/nr 出发/v 了/ul 
下午 3点 我 提前 一个 小时 到了 那家 咖啡馆 我 
微信 上 给 HR 发 消息 说 我 到了 进门 
之后 怎么走 那位 帅气 的 HR 带 我 进了 咖啡馆 
问了 下 我 的 姓名 和 求职 岗位 带我去 签完 
到 后给我/nr 找 一个 位置 并 了 给了 我 一张 
A4 纸 然后 就是 用 自己 熟悉 的 语言 实现 
两道 算法 题 我 周围 的 人都 是 今天 来 
面试 陌陌 的 他们 都在/nr 认真 的 低着头 写 代码 
第 一题 是 实现 输出 一个 长度 为 n 的 
无序 数组 中的 前 k 个 最小值 我 能 想到 
的 就是 先 通过 各种 排序算法 将 数组 排序 然后 
输出 前 k 个 最小值 就行了 但是 这 并 不是 
最好 的 方式 会 造成 复杂度 比较 高 因为 只 
需要 输出 前 k 个 最小值 剩下 的 n k 
个 数值 不 需要 考虑 那么 通过 排序算法 只需要 排 
前 k 个 数值 ok 了 不同 的 排序算法 时间 
复杂度 都是 不 一样 的 比如 比较 容易 实现 的 
选择 排序 和 冒泡排序 平均 情况 下 都是 O n2 
若 只 需要 找到 前面 的 k 个 值 复杂度 
也要 O n * k 若 使用 快速排序 复杂度 近似 
O n 如果 使用 堆排序 复杂度 近似 O nlogk 下面 
基于 堆排序 给出 解题 思路 以及 python3 代码 方法 是 
维护 k 个 元素 的 最大 堆 即用 容量 为 
k 的 最大 堆 存储 最先 遍历 到 的 k 
个数 并 假设 它们 即是 最小 的 k 个数 建 
堆 费时 O k 后 有 k1 k2 . . 
. kmax kmax 设为 大顶 堆 中 最大 元素 继续 
遍历 数列 每次 遍历 一个 元素 x 与 堆 顶 
元素 比较 x kmax 更新 堆 用时 logk 否则 不 
更新 堆 这样 下来 总 费时 O k + n 
k * logk = O n * logk 此 方法 
得益于 在 堆 中 查找 等 各项 操作 时间 复杂度 
均为 logk python3 代码 如下 1 # * coding utf 
8 * 2 3 Created on Sun Sep 2 17 
16 36 2018 4 5 @ author aoanng 6 7 
8 def create _ heap lyst 9 # 创建 数组 
中前 k 个数 的 最大 堆 10 for start in 
range len lyst 2 / / 2 1 1 11 
sift _ down lyst start len lyst 1 12 13 
14 return lyst 15 16 # 堆排序 对于 本 问题 
用不着 17 def heapSort lyst 18 # 堆排序 19 for 
end in range len lyst 1 0 1 20 lyst 
0 lyst end = lyst end lyst 0 21 sift 
_ down lyst 0 end 1 22 return lyst 23 
24 # 最 大堆 调整 25 def sift _ down 
lst start end 26 root = start 27 while True 
28 child = 2 * root + 1 29 if 
child end 30 break 31 if child + 1 = 
end and lst child lst child + 1 32 child 
+ = 1 33 if lst root lst child 34 
lst root lst child = lst child lst root 35 
root = child 36 else 37 break 38 39 40 
# 测试 41 if _ _ name _ _ = 
= _ _ main _ _ 42 list1 = 50 
45 40 20 25 35 30 10 15 43 k 
= 4 # 设置 需要 输出 的 前 k 个 
最小值 44 list _ n _ k = list1 k 
45 heap _ k = create _ heap list1 k 
# 将 数组 前 k 个数 创建 最 大堆 并 
假设 它们 是 最小 的 k 个数 46 for i 
in range len list _ n _ k 47 if 
list _ n _ k i heap _ k 0 
48 heap _ k 0 = list _ n _ 
k i 49 heap _ k = create _ heap 
heap _ k # 更新 堆 50 print heap _ 
k # 输出 前 k 个 未 排序 的 最小值 
51 52 # 若 需要 则 可以 对 堆 进行 
排序 53 heap _ k _ sort = heapSort heap 
_ k 54 print heap _ k _ sort 第 
二题 是 给出 一个 n * n 的 矩阵 将其 
逆时针 旋转 90度 但是 不能 开辟 新的 内存空间 这 题 
的 前提 是 必须 在 原 数组 上 进行 旋转 
操作 只要 搞清楚 矩阵 中 元素 旋转 的 规律 就 
容易 求 解了 那 就是 当前 元素 ai j 经过 
逆时针 旋转 90度 后有 ai j = aj n i 
的 关系 写完 这 两道 算法 题 后 我 检查 
了 两遍 并 在那儿 扣 手机 hr 看见 我 写完 
之后 过来 收 走了 我 的 作业 让 我 等待 
一会儿 大概 20 分钟 后 一位 技术 面试官 带着 我 
的 简历 以及 之前 写好 的 算法 题 过来 找 
我 第一 轮 技术 面试 便 开始 了 我 先 
简单 的 自我 介绍 后 面试官 看着 我 的 写 
的 那 两道 算法 题 聊了 起来 让 我 说说 
我 的 思路 我 说 第一 题 其实 就是 一个 
排序算法 问题 然后 输出 前 k 个 值 就好 看见 
我 用 的 选择 排序算法 面试官 指出 了 疑问 说 
这样 时间 复杂度 会 比较 高 然后 问 我 有 
没有 其他 的 思路 我 说 可以 只 需要 排 
前 k 个 值 将 时间 复杂度 降到 O n 
* k 面试官 最后 逐步 的 引导 我 说 用 
最大 堆 会 比较 好 总之 面试 官人 很 nice 
问到 我 不会 的 总是 在 引导 我 然后 就是 
介绍 我 简历 中 做过 的 几个 项目 项目/n 当中/s 
有/v 用到/v 深度/ns 学习/v 平台/n tensorflow/w 和/c CNN/w 网络/n 架构/n 
以及/c 一些/m 机器学习/i 算法/n 面试官 逐个 问 我 比如 在 
ttensorflow 中 怎样 构建 一个 cnn 网络 防止 过拟合 的 
一些 tips Dropout 是 怎样 工作 的 等等 然后 让 
我 手 写在 tensorflow 中 怎样 保存 模型 和 加载 
模型 tf . get _ variable 和 tf . Variable 
的 区别 tf/w ./i variable/w _/i scope/w 和/c tf/w ./i 
name/w _/i scope/w 的/uj 用法/n 和/c 区别/n 这些 其实 我 
平时 项目 中 也有 用到 平时 也 关注 过 这个 
问题 只是 没有 上心 当时 没有 回答 上来 然后 面试官 
大致 的 给 我 讲了 一下 原理 就 跳过 这个 
问题 了 回来/v 后我又/nr 在/p 网上/s 查了/nr 一下/m 资料/n 总结 
如下 tf . variable _ scope 和 tf . name 
_ scope 的 用法 tf . variable _ scope 可以 
让 变量 有 相同 的 命名 包括 tf . get 
_ variable 得到 的 变量 还有 tf . Variable 的 
变量 tf . name _ scope 可以 让 变量 有 
相同 的 命名 只是 限于 tf . Variable 的 变量 
例如 1 import tensorflow as tf 2 3 with tf 
. variable _ scope V1 4 a1 = tf . 
get _ variable name = a1 shape = 1 initializer 
= tf . constant _ initializer 1 5 a2 = 
tf . Variable tf . random _ normal shape = 
2 3 mean = 0 stddev = 1 name = 
a2 6 with tf . variable _ scope V2 7 
a3 = tf . get _ variable name = a1 
shape = 1 initializer = tf . constant _ initializer 
1 8 a4 = tf . Variable tf . random 
_ normal shape = 2 3 mean = 0 stddev 
= 1 name = a2 9 10 with tf . 
Session as sess 11 sess . run tf . global 
_ variables _ initializer 12 print a1 . name 13 
print a2 . name 14 print a3 . name 15 
print a4 . name 16 17 # 输出 18 19 
V1 / a1 0 20 V1 / a2 0 21 
V2 / a1 0 22 V2 / a2 0 23 
如果 将 上边 的 tf . variable _ scope 换成 
tf . name _ scope 将会 报错 Variable a1 already 
exists disallowed . Did you mean to set reuse = 
True or reuse = tf . AUTO _ REUSE in 
VarScope . . . 改成 如下 这样 就 ok 了 
1 import tensorflow as tf 2 3 with tf . 
name _ scope V1 4 # a1 = tf . 
get _ variable name = a1 shape = 1 initializer 
= tf . constant _ initializer 1 5 a2 = 
tf . Variable tf . random _ normal shape = 
2 3 mean = 0 stddev = 1 name = 
a2 6 with tf . name _ scope V2 7 
# a3 = tf . get _ variable name = 
a1 shape = 1 initializer = tf . constant _ 
initializer 1 8 a4 = tf . Variable tf . 
random _ normal shape = 2 3 mean = 0 
stddev = 1 name = a2 9 10 with tf 
. Session as sess 11 sess . run tf . 
global _ variables _ initializer 12 # print a1 . 
name 13 print a2 . name 14 # print a3 
. name 15 print a4 . name 16 17 # 
输出 18 19 V1 / a2 0 20 V2 / 
a2 0 21 接下 来 看看 tf . Variable 和 
tf . get _ variable 的 区别 在 tensorflow 中 
tf . Variable 和 tf . get _ variable 两个 
op 分别 用来 创建 变量 tf . Variable 总是 创建 
新的 变量 返回 一个 variable 可以 定义 名字 相同 的 
变量 若 给出 的 name 已经 存在 会 自动 修改 
name 生成 个 新的 1 import tensorflow as tf 2 
w _ 1 = tf . Variable 3 name = 
w _ 1 3 w _ 2 = tf . 
Variable 1 name = w _ 1 4 print w 
_ 1 . name 5 print w _ 2 . 
name 6 # 输出 7 # w _ 1 0 
8 # w _ 1 _ 1 0tf . get 
_ variable 不可以 定义 名字 相同 的 变量 tf . 
get _ variable 函数 拥有 一个 变量 检查 机制 会 
检测 已经 存在 的 变量 是否 设置 为 共享 变量 
如果 已经 存在 的 变量 没有 设置 为 共享 变量 
TensorFlow 运行 到 第二 个 拥有 相同 名字 的 变量 
的 时候 就会 报错 不同 的 变量 之间 不能 有 
相同 的 名字 除非 你 定义 了 variable _ scope 
这样 才 可以 有 相同 的 名字 1 import tensorflow 
as tf 2 3 w _ 1 = tf . 
get _ variable name = w _ 1 initializer = 
1 4 w _ 2 = tf . get _ 
variable name = w _ 1 initializer = 2 5 
# 错误信息 6 # ValueError Variable w _ 1 already 
exists disallowed . Did 7 # you mean to set 
reuse = True in VarScope tf . get _ variable 
一般 和 tf . variable _ scope 配合 使用 用于 
在 同一个 的 变量 域中 共享 同一个 变量 如何 在 
tensorflow 中 保存 和 加载 模型 呢 构建 网络 中 
加入 saver = tf . train . Saver 然后 在 
session 会 话中 saver . save sess . / model 
/ model . ckpt 加载 模型 构建 网络 中 需要 
和 之前 一样 然后 在 session 会 话中 加载 模型 
saver . restore sess . / model / model . 
ckpt 然后 和 面试官 讨论 一些 机器学习 算法 的 问题 
诸如 LR 和 SVM 的 区别 随机 森林 和 GBDT 
区别 xgboost 以及 最 优化 算法 的 原理 等等 很快 
一个 多 小时 就 过去 啦 感觉 自己 表现 得 
不是太好 但是 和 面试官 还是 挺 聊 得来 的 面试 
的 最后 面试官 问 我 对 一些 经典 的 数据结构 
熟悉 不 我 说 还 可以 然后 他 让 我 
现场 写 一个 单链表 的 逆序 很 简单 的 问题 
我 却没 写出来 我 曾经 看过 java 版本 和c/nr 版本 
的 数据 结构 与 算法 前 不久 也 看过 用 
python 实现 的 数据 结构 与 算法 但是 这个 时候 
我 却 卡住 了 10/m 分钟/q 后/f 面试官/n 看/v 我/r 
连/nr 这个/r 简单/a 的/uj 问题/n 都没/i 写出来/i 笑着 对 我 
说 该不该 给 我 第二 轮 技术 面 的 机会 
然后 第一 轮 技术 面 就 这样 结束 了 让 
我 在 旁边 的 椅子 上 稍等一下 . . . 
等 的 过程 中 我 拿出 手机 百度 了 下 
单链表 的 逆序 如何 实现 恍然大悟 的 同时 也 有点 
懊恼 参考 网上 的 答案 实现 如下 循环 反转 单链表 
1 # 定义 一个 单链表 节点 2 class ListNode 3 
def _ _ init _ _ self x 4 self 
. data = x 5 self . next = None 
6 7 def nonrecurse head # 循环 的 方法 反转 
链表 8 if head is None or head . next 
is None 9 return head 10 pre = None 11 
cur = head 12 h = head 13 while cur 
14 h = cur 15 tmp = cur . next 
16 cur . next = pre 17 pre = cur 
18 cur = tmp 19 return h 20 21 head 
= ListNode 1 # 测试代码 22 p1 = ListNode 2 
# 建立 链表 1 2 3 4 None 23 p2 
= ListNode 3 # head p1 p2 p3 None 24 
p3 = ListNode 4 25 head . next = p1 
26 p1 . next = p2 27 p2 . next 
= p3 28 29 p = nonrecurse head # 输出 
链表 4 3 2 1 None 30 while p 31 
print p . data 32 p = p . next 
递归 实现 单链表 反转 1 class ListNode 2 def _ 
_ init _ _ self x 3 self . val 
= x 4 self . next = None 5 6 
7 def recurse head newhead # 递归 head 为 原 
链表 的 头 结点 newhead 为 反转 后 链表 的 
头 结点 8 if head is None 9 return 10 
if head . next is None 11 newhead = head 
12 else 13 newhead = recurse head . next newhead 
14 head . next . next = head 15 head 
. next = None 16 return newhead 17 18 head 
= ListNode 1 # 测试代码 19 p1 = ListNode 2 
# 建立 链表 1 2 3 4 None 20 p2 
= ListNode 3 21 p3 = ListNode 4 22 head 
. next = p1 23 p1 . next = p2 
24 p2 . next = p3 25 newhead = None 
26 p = recurse head newhead # 输出 链表 4 
3 2 1 None 27 while p 28 print p 
. val 29 p = p . next 接下来 就是 
技术 第二面 了 不说 了 说 多了 都是 泪 . 
. . 参考 窥探 算法 之 美妙 寻找 数组 中 
最小 的 K 个数 & python 中 巧用 最 大堆 
程序员 编程 艺术 第三章 寻找 最小 的 k 个数 tf 
. variable _ scope 和 tf . name _ scope 
的 用法 tf . Variable 与 tf . get _ 
variable 与 不同之处 TensorFlow 模型 保存 和 加载 方法 单链表 
反转 python 实现 