1 . 线性 分类器 在 深度 学习 与 计算机 视觉 
系列 2 我们 提到 了 图像 识别 的 问题 同时 
提出 了 一种 简单 的 解决 方法 KNN 然后 我们 
也 看到 了 KNN 在 解决 这个 问题 的 时候 
虽然 实现 起来 非常 简单 但是 有 很大 的 弊端 
分类器 必须 记住 全部 的 训练 数据 因为 要 遍历 
找 近邻 啊 而在 任何 实际 的 图像 训练 集上 
数据量 很 可能 非常 大 那么 一次性 载入 内存 不管 
是 速度 还是 对 硬件 的 要求 都是/nr 一个 极大 
的 挑战 分类 的 时候 要 遍历 所有 的 训练 
图片 这 是 一个 相当 相当 相当 耗时 的 过程 
这个 部分 我们 介绍 一类 新的 分类器 方法 而 对其 
的 改进 和 启发 也能 帮助 我们 自然而然 地 过渡 
到 深度 学习 中的 卷积 神经网 有 两个 重要 的 
概念 得分 函数 / score function 将 原始数据 映射 到 
每个 类 的 打分 的 函数 损失 函数 / loss 
function 用于 量化 模型 预测 结果 和 实际 结果 之间 
吻 合度 的 函数 在 我们 得到 损失 函数 之后 
我们 就 将 问题 转化 成为 一个 最 优化 的 
问题 目标 是 得到 让 我们 的 损失 函数 取值 
最小 的 一组 参数 2 . 得分 函数 / score 
function 首先 我们 定义 一个 有 原始 的 图片 像素 
值 映射 到最后 类目 得分 的 函数 也 就是 这里 
提到 的 得分 函数 先 笼统 解释一下 一会儿 我们 给 
个 具体 的 实例 来 说明 假设 我们 的 训练 
数据 为 对应 的 标签 yi 这里 i = 1 
N 表示 N 个 样本 yi ∈ 1 K 表示 
K 类 图片 比如 CIFAR 10 数据 集中 N = 
50000 而 D = 32x32x3 = 3072 像素 K = 
10 因为 这 时候 我们 有 10个 不同 的 类别 
狗 / 猫 / 车 我们 实际上 要 定义 一个 
将 原始 像素 映 射到 得分 上 函数 2.1 线性 
分类器 我们 先 丢出 一个 简单 的 线性映射 在 这个 
公式 里 我们/r 假定/v 图片/n 的/uj 像素/n 都平/nr 展为/i D 
x 1 的 向量 然后 我们 有 两个 参数 W 
是 K x D 的 矩阵 而 向量 b 为 
K x 1 的 在 CIFAR 10中 每张 图片 平 
展开 得到 一个 3072 x 1 的 向量 那W就/nr 应该 
是 一个 10 x 3072 的 矩阵 b 为 10 
x 1 的 向量 这样 以 我们 的 线性代数 知识 
我们 知道 这个 函数 接受 3072 个数 作为 输入 同时 
输出 10 个数 作为 类目 得分 我们 把 W 叫做 
权重 b 叫做 偏移 向量 说明 几个 点 我们 知道 
一次 矩阵 运算 我们 就 可以 借助 W 把 原始数据 
映射 为 10个 类别 的 得分 其实 我们 的 输入 
xi yi 其实 是 固定 的 我们 现在 要做 的 
事情 是 我们 要 调整 W b 使得 我们 的 
得分 结果 和 实际 的 类目 结果 最为 吻合 我们 
可以 想象 到 这样 一种 分类 解决方案 的 优势 是 
一旦 我们 找到 合适 的 参数 那么 我们 最后 的 
模型 可以 简化 到 只有 保留 W b 即可 而 
所有 原始 的 训练 数据 我们 都 可以 不管 了 
识别 阶段 我们 需要 做 的 事情 仅仅 是 一次 
矩阵 乘法 和 一次 加法 这个 计算 量 相对 之前 
不要 小 太多 好么 提前 剧透 一下 其实 卷积 神经网 
做 的 事情 也 是 类似 的 将 原始 输入 
的 像素 映 射成 类目 得分 只不过 它 的 中间 
映射 更加 复杂 参数 更多 而已 2.2 理解 线性 分类器 
我们 想想 其实 线性 分类器 在做 的 事情 是 对 
每个 像素点 的 三个 颜色通道 做 计算 咱们 拟人化 一下 
帮助 我们 理解 可以 认为 设定 的 参数 / 权重 
不同 会 影响 分类器 的 性格 从而 使得 分类器 对 
特定 位置 的 颜色 会 有 自己 的 喜好 举个 
例子 假如 说 我们 的 分类器 要 识别 船只 那么 
它 可能 会 喜欢 图片 的 四周 都是/nr 蓝色 通常 
船 只是 在 水里 海里 吧 我们 用 一个 实际 
的 例子 来 表示 这个 得分 映射 的 过程 大概 
就是 下 图 这个 样子 原始 像素点 向量 xi 经过 
W 和b/nr 映射 为 对应 结果 类别 的 得分 不过 
上面 这组 参数 其实 给 的 是 不太 恰当 的 
因为 我们 看到 在 这组 参 数下 图片 属于 狗狗 
的 得分 最高 _ | | 2 . 2.1 划分 
的 第 1种 理解 图片 被 平 展开 之后 向量 
维度 很高 高维空间 比较 难 想象 我们 简化 一下 假如 
把 图片 像素 输入 看做 可以 压缩 到 二 维空间 
之中 的 点 那 我们 想想 分类器 实际上 在做 的 
事情 就 如下 图 所示 W 中的 每 一列 对应 
类别 中的 每 一类 而 当 我们 改变 W 中的 
值 的 时候 图上 的 线 的 方向 会 跟着 
改变 那么 b 呢 对 b 是 一个 偏移量 它 
表示 当 我们 的 直线 方向 确定 以后 我们 可以 
适当 平移 直线 到 合适 的 位置 没有 b 会 
怎么样 呢 如果 直线 没有 偏移量 那 意味着 所有 的 
直线 都要 通过 原点 这种 强 限制 条件 下 显然 
不能 保证 很好 的 平面 类别 分割 2 . 2.2 
划分 的 第 2种 理解 对 W 第二种 理解 方式 
是 W 的 每 一行 可以 看做 是 其中 一个 
类别 的 模板 而 我们 输入 图片 相对 这个 类别 
的 得分 实际上 是 像素点 和 模板 匹配度 通过 内积 
运算 获得 而 类目 识别 实际上 就是 在 匹配 图像 
和 所有 类别 的 模板 找到 匹配度 最高 的 那个 
是不是 感觉 和 KNN 有点 类似 的 意思 是 有 
那么 点 相近 但是 这里 我们 不再 比对 所有 图片 
而是 比对 类别 的 模板 这样 比对 次数 只和 类 
目数 K 有关系 所以 自然 计算 量 要 小 很多 
同时 比对 的 时候 用 的 不再 是 l1 或者 
l2 距离 而是 内积 计算 我们 提前 透 露一下 CIFAR 
10 上 学习 到 的 模板 的 样子 你 看 
和 我们 设想 的 很 接近 ship 类别 的 周边 
有 大量 的 蓝色 而 car 的 旁边 是 土地 
的 颜色 2 . 2.3 关于 偏移量 的 处理 我们 
先回到 如下 的 公式 公式 中有 W 和b/nr 两个 参数 
我们 知道 调节 两个 参数 总归 比 调节 一个 参数 
要 麻烦 所以 我们 用 一点 小 技巧 来 把 
他们 组合 在 一起 放到 一个 参数 中 我们 现在 
要做 的 运算 是 矩阵 乘法 再加 偏移量 最 常用 
的 合并 方法 就是 想 办法 把 b 合并 成W的/nr 
一部分 我们 仔细 看看 下面 这 张 图片 我们 给 
输入 的 像素 矩阵 加上 一个 1 从而 把 b 
拼 接到 W 里 变成 一个 变量 依旧 拿 CIFAR 
10 举例 原本 是 3072 x 1 的 像素 向量 
我们 添上 最后 那个 1 变成 3073 x 1 的 
向量 而 W 变成 W b 2 . 2.4 关于 
数据 的 预处理 插播 一段 实际 应用 中 我们 很多 
时候 并 不是 把 原始 的 像素 矩阵 作为 输入 
而是 会 预先 做 一些 处理 比如说 有 一个 很 
重要 的 处理 叫做 去 均值 他 做 的 事情 
是 对于 训练 集 我们 求得 所有 图片 像素 矩阵 
的 均值 作为 中心 然后 输入 的 图片 先减 掉 
这个 均值 再做 后续 的 操作 有时候 我们 甚至 要 
对 图片 的 幅度 归一化 / scaling 去 均值 是 
一个 非常 重要 的 步骤 原因 我们 在 后续 的 
梯度 下降 里 会 提到 2.3 损失 函数 我们 已经 
通过 参数 W 完成 了 由 像素 映 射到 类目 
得分 的 过程 同时 我们 知道 我们 的 训练 数据 
xi yi 是 给定 的 我们 可以 调整 的 是 
参数 / 权重 W 使得 这个 映射 的 结果 和 
实际 类别 是 吻合 的 我们 回到 最 上面 的 
图片 中 预测 猫 / 狗 / 船 得分 的 
例子 里 这个 图片 中 给定 的 W 显然 不是 
一个 合理 的 值 预测/vn 的/uj 结果/n 和/c 实际/n 情况/n 
有/v 很大/a 的/uj 偏差/n 于是 我们 现在 要 想 办法 
去 把 这个 偏差 表示出来 拟人 一点 说 就是 我们 
希望 我们 的 模型 在 训练 的 过程 中 能够 
对 输出 的 结果 计算 并 知道 自己 做 的 
好坏 而能 帮助 我们 完成 这 件 事情 的 工具 
叫做 损失 函数 / loss function 其实 它 还有 很多 
其他 的 名字 比如说 你 说不定 在 其他 的 地方 
听人 把 它 叫做 代价 函数 / cost function 或者 
客观 度 / objective 直观 一点 说 就是 我们 输出 
的 结果 和 实际 情况 偏差 很大 的 时候 损失 
/ 代价 就会 很大 2 . 3.1 多 类别 支持 
向量 机 损失 / Multiclass Support Vector Machine loss 腻 
害 的 大神 们 定义 出了 好些 损失 函数 我们 
这里 首先 要 介绍 一种 极其 常用 的 叫 做多 
类别 支持 向量 机 损失 Multiclass SVM loss 如果 要 
用 一句 精简 的话 来 描述 它 就是 它 SVM 
希望 正确 的 类别 结果 获得 的 得分 比 不 
正确 的 类别 至少 要 高上 一个 固定 的 大小 
Δ 我们 先 解释 一下 这 句话 一会儿 再 举个 
例子 说明 一下 对于 训练 集中 的 第 i 张 
图片 数据 xi 我们 的 得分 函数 在 参数 W 
下 会 计算 出 一个 所有 类 得分 结果 其中 
第 j 类 得分 我们 记作 该 图片 的 实际 
类别 为 yi 则 对于 第 i 张 样本 图片 
我们 的 损失 函数 是 如下 定义 的 看 公式 
容易 看 瞎 译者 也 经常 深深地 为 自己 智商 
感到 捉急 我们 举 个 例子 来 解释 一下 这个 
公式 假如 我们 现在 有 三个 类别 而 得分 函数 
计算 某 张 图片 的 得分 为 而 实际 的 
结果 是 第一 类 yi = 0 假设 Δ = 
10 这个 参数 一会儿 会 介绍 上面 的 公式 把 
错误类别 j ≠ yi 都 遍历 了 一遍 求值 加 
和 仔细 看看 上述 的 两项 左边 项 10 和0/nr 
中的 最大值 为 0 因此 取值 是 零 其实 这里 
的 含义 是 实际 的 类别 得分 13 要比 第二类 
得分 7 高出 20 超过 了 我们 设定 的 正确 
类目 和 错误 类目 之间 的 最小 margin   Δ 
= 10 因此 第二类 的 结果 我们 认为 是 满意 
的 并不 带来 loss 所以 值 为 0 而 第三 
类 得分 11 仅 比 13 小 2 没有 大于 
Δ = 10 因此 我们 认为 他 是 有 损失 
/ loss 的 而 损失 就 是 当前 距离 2 
距离 设定 的 最小 距离 Δ 的 差距 8 注意 
到 我们 的 得分 函数 是 输入 像素 值 的 
一个 线性函数 因此 公式 又 可以 简化 为 其中 wj 
是 W 的 第 j 行 我们 还 需要 提 
一下 的 是 关于 损失 函数 中 max 0 的 
这种 形式 我们 也 把 它 叫做 hinge loss / 
铰链 型 损失 有时候 你 会 看到 squared hinge loss 
SVM 也叫 L2 SVM 它 用到 的 是 这个 损失 
函数 惩罚 那些 在 设定 Δ 距离 之内 的 错误 
类别 的 惩罚 度 更高 两种 损失 函数 标准 在 
特定 的 场景 下 效果 各有 优劣 要 判定 用 
哪个 还是 得 借助于 交叉 验证 / cross validation 对于 
损失 函数 的 理解 可以 参照 下图 . 3.2 正则化 
如果 大家 仔细 想想 会 发现 使用 上述 的 loss 
function 会 有一个 bug 如果 参数 W 能够 正确 地 
识别 训练 集中 所有 的 图片 损失 函数 为 0 
那么 我们 对 M 做 一些 变换 可以 得到 无 
数组 也能 满足 loss function = 0 的 参数 W 
举个 例子 对于 λ 1 的 所有 λ W 原来 
的 错误类别 和 正确 类别 之间 的 距离 已经 大于 
Δ 现在 乘以 λ 更大 了 显然 也 能 满足 
loss 为 0 于是 我们 得 想 办法 把 W 
参数 的 这种 不确定性 去 除掉 啊 这 就是 我们 
要 提到 的 正则化 我们 需要 在 原来 的 损失 
函 数上 再加 上 一项 正则化 项 regularization penalty   
R W 最 常见 的 正则化 项是/nr L2 范数 它 
会对 幅度 很大 的 特征 权重 给 很高 的 惩罚 
根据 公式 可以 看到 这个 表达式 R W 把 所有 
W 的 元素 的 平方 项求/nr 和了 而且 它 和 
数据 本身 无关 只和 特征 权重 有关系 我们 把 两 
部分组 数据 损失 / data loss 和 正则化 损失 / 
regularization loss 在 一起 得到 完整 的 多 类别 SVM 
损失 权重 如下 也 可以 展开 得到 更 具体 的 
完整 形式 其中 N 是 训练 样本数 我们 给 正则化 
项 一个 参数 λ 但是 这个 参数 的 设定 只有 
通过 实验 确定 对 还是 得 交叉 验证 / cross 
validation 关于/p 设定/v 这样/r 一个/m 正则化/i 惩罚/vn 项/n 为什么/r 能/v 
解决/v W/w 的/uj 不确定性/n 我们 在 之后 的 系列 里 
会 提到 这里 我们 举 个 例子 简单 看看 这个 
项是/nr 怎么 起到 作用 的 假定 我们 的 输入 图片 
像素 矩阵 是 x = 1 1 1 1 而 
现在 我们 有 两组 不同 的 W 权重 参数 中 
对应 的 向量 w1 = 1 0 0 0 w2 
= 0.25 0.25 0.25 0.25 那 我们 很 容易 知道 
wT1x = wT2x = 1 所以 不加 正则 项的/nr 时候 
这俩 得到 的 结果 是 完全 一样 的 也 就 
意味着 它们 是 等价 的 但是 加了 正则 项 之后 
我们 发现 w2 总体 的 损失 函数 结果 更小 因为 
4 * 0.25 ^ 2 1 于是 我们 的 系统 
会 选择 w2 这也 就 意味着 系统 更 喜欢 权重 
分布 均匀 的 参数 而 不是 某些 特征 权重 明显 
高于 其他 权重 占据 绝对 主导 作用 的 参数 之后 
的 系列 里 会 提到 这样 一个 平滑 的 操作 
实际上 也 会 提高 系统 的 泛化 能力 让其 具备 
更高 的 通用性 而不 至于 在 训练 集上 过拟合 另外 
我们 在 讨论 过拟合 的 这个 部分 的 时候 并 
没有 提到 b 这个 参数 这 是因为 它 并不 具备 
像 W 一样 的 控制 输入 特征 的 某个 维度 
影响力 的 能力 还 需要 说 一下 的 是 因为 
正则 项的/nr 引入 训练 集上 的 准确度 是 会 有 
一定 程度 的 下降 的 我们 永远 也 不 可能 
让 损失 达到 零 了 因为 这 意味着 正则化 项为0/nr 
也 就是 W = 0 下面 是 简单 的 计算 
损失 函数 没 加上 正则化 项 的 代码 有/v 未向/i 
量化/v 和向/nr 量化/v 两种/m 形式/n def L _ i x 
y W 未向 量化 版本 . 对 给定 的 单个 
样本 x y 计算 multiclass svm loss . x 代表 
图片 像素 输入 的 向量 例如 CIFAR 10中 是 3073 
x 1 因为 添加 了 bias 项 对应 的 1 
到 x 中 y 图片 对应 的 类别 编号 比如 
CIFAR 10中 是 0 9 W 权重 矩阵 例如 CIFAR 
10中 是 10 x 3073 delta = 1.0 # 设定 
delta scores = W . dot x # 内积 计算 
得分 correct _ class _ score = scores y D 
= W . shape 0 # 类 别数 例如 10 
loss _ i = 0.0 for j in xrange D 
# 遍历 所有 错误 的 类别 if j = = 
y # 跳过 正确 类别 continue # 对 第 i 
个 样本 累加 loss loss _ i + = max 
0 scores j correct _ class _ score + delta 
return loss _ i def L _ i _ vectorized 
x y W 半 向 量化 的 版本 速度 更快 
之所以 说是 半 向 量化 是 因为 这个 函数 外层 
要用 for 循环 遍历 整个 训练 集 _ | | 
delta = 1.0 scores = W . dot x # 
矩阵 一次性 计算 margins = np . maximum 0 scores 
scores y + delta margins y = 0 loss _ 
i = np . sum margins return loss _ i 
def L X y W 全向 量化 实现 X 包含 
所有 训练样本 中 数据 例如 CIFAR 10 是 3073 x 
50000 y 所有 的 类别 结果 例如 50000 x 1 
的 向量 W 权重 矩阵 例如 10 x 3073 # 
待 完成 . . . 说到 这里 其实 我们 的 
损失 函数 是 提供 给 我们 一个 数值 型 的 
表示 来 衡量 我们 的 预测 结果 和 实际 结果 
的 差别 而 要 提高 预测 的 准确性 要做 的 
事情 是 想办法 最小化 这个 loss 2.4 一些 现实 的 
考虑 点 2 . 4.1 设定 Delta 我们 在 计算 
Multi SVM loss 的 时候 Δ 是 我们 提前 设定 
的 一个 参数 这个 值 咋 设定 莫不是 也 需要 
交叉 验证 其实 基本上 大部分 的 场合 下 我们 设定 
Δ = 1.0 都是/nr 一个 安全 的 设定 我们 看 
公式 中的 参数 Δ 和λ/nr 似乎 是 两个 截然不同 的 
参数 实际上 他俩 做 的 事情 比较 类似 都是 尽量 
让 模型 贴近 标准 预测 结果 的 时候 在 数据 
损失 / data loss 和 正则化 损失 / regularization loss 
之间 做 一个 交换 和 平衡 在 损失 函数 计算公式 
里 可以 看出 权重 W 的 幅度 对 类别 得分 
有最/nr 直接 的 影响 我们 减小 W 最后 的 得分 
就 会 减少 我们 增大 W 最后 的 得分 就 
增大 从 这个 角度 看 Δ 这个 参数 的 设定 
Δ = 1 或者 Δ = 100 其实 无法 限定 
W 的 伸缩 而 真正 可以 做到 这点 的 是 
正则化 项 λ 的 大小 实际上 控制 着 权重 可以 
增长 和 膨胀 的 空间 2 . 4.2 关于 二元 
/ Binary 支持 向量 机 如果 大家 之前 接触 过 
Binary SVM 我们 知道 它 的 公式 如下 我们 可以 
理解 为 类别 yi ∈ − 1 1 它 是 
我们 的 多 类别 识别 的 一个 特殊 情况 而 
这里 的 C 和λ是/nr 一样 的 作用 只不过 他们 的 
大小 对 结果 的 影响 是 相反 的 也 就是 
C ∝ 1 / λ 2 . 4.3 关于 非 
线性 的 SVM 如果 对 机器 学习 有 了解 你 
可能 会 了解 很多 其他 关于 SVM 的 术语 kernel 
dual SMO 算法 等等 在 这个 系列 里面 我们 只 
讨论 最 基本 的 线性 形式 当然 其实 从 本质 
上 来说 这些 方法 都是/nr 类似 的 2.5 Softmax 分类器 
话说 其实 有 两种 特别 常见 的 分类器 前面 提 
的 SVM 是 其中 的 一种 而 另外 一种 就是 
Softmax 分类器 它 有着 截然不同 的 损失 函数 如果 你 
听说 过 逻辑 回归 二 分类器 那么 Softmax 分类器 是 
它 泛化 到 多 分类 的 情形 不像 SVM 这种 
直接 给 类目 打分 f xi W 并 作为 输出 
Softmax 分类器 从新 的 角度 做了 不 一样 的 处理 
我们 依旧 需要 将 输入 的 像素 向量 映射 为 
得分 f x _ i W = W x _ 
i 只不过 我们 还 需要 将 得分 映 射到 概率 
域 我们 也 不再 使用 hinge loss 了 而是 使用 
交叉 熵 损失 / cross entropy loss 形式 如下 我们 
使用 fj 来 代表 得 分向量 f 的 第 j 
个 元素 值 和 前面 提到 的 一样 总体 的 
损失 / loss 也是 Li 遍历 训练 集 之后 的 
均值 再 加上 正则化 项R/nr W 而 函数 被 称之为 
softmax 函数 它 的 输入 值 是 一个 实数 向量 
z 然后 在 指数 域 做 了 一个 归一化 以 
保证 之和 为 1 映射 为 概率 2 . 5.1 
信息论 角度 的 理解 对于 两个 概率分布 p 真实 的 
概率分布 和 估测 的 概率分布 q 估测 的 属于 每个 
类 的 概率 它们 的 互 熵 定义 为 如下 
形式 而 Softmax 分类器 要做 的 事情 就是 要 最小化 
预测 类别 的 概率分布 之前 看到 了 是 与 实际 
类别 概率分布 p = 0 1 0 只在 结 果类 
目上 是 1 其余 都为 0 两个 概率分布 的 交叉 
熵 另外 因为 互 熵 可以用 熵 加上 KL 距离 
/ Kullback Leibler Divergence 也叫 相对 熵 / Relative Entropy 
来 表示 即 而 p 的 熵 为 0 这 
是 一个 确定 事件 无 随机性 所以 互 熵 最小化 
等同于 最小化 两个 分布 之间 的 KL 距离 换句话说 交叉 
熵 想 要从 给定 的 分布 q 上 预测 结果 
分布 p 2 . 5.2 概率 角度 的 理解 我们 
再 来 看看 以下 表达式 其实 可以 看做 给定 图片 
数据 xi 和 类别 yi 以及 参数 W 之后 的 
归一化 概率 在 概率 的 角度 理解 我们 在 做 
的 事情 就是 最小化 错误类别 的 负 log 似 然 
概率 也 可以 理解 为 进行 最大 似 然 估计 
/ Maximum Likelihood Estimation MLE 这个 理解 角度 还有 一个 
好处 这个 时候 我们 的 正则化 项R/nr W 有 很好 
的 解释性 可以 理解 为 整个 损失 函数 在 权重 
矩阵 W 上 的 一个 高斯 先验 所以 其实 这 
时候 是 在做 一个 最大 后验/nr 估计 / Maximum a 
posteriori MAP 2 . 5.3 实际 工程 上 的 注意 
点 数据 稳定性 在 我们 要 写 代码 工程 实现 
Softmax 函数 的 时候 计算 的 中间 项 因为 指数 
运算 可能 变得 非常 大 除法 的 结果 非常 不 
稳定 所以 这里 需要 一个 小 技巧 注意到 如果 我们 
在 分子 分母 前 都 乘以 常数 C 然后 整 
理到 指 数上 我们 会 得到 下面 的 公式 C 
的 取值 由 我们 而定 不 影响 最后 的 结果 
但是 对于 实际 计算 过程 中 的 稳定性 有 很大 
的 帮助 一个 最 常见 的 C 取值 为 这 
表明 我们 应该 平移 向量 f 中的 值 使得 最大值 
为 0 以下 的 代码 是 它 的 一个 实现 
f = np . array 123 456 789 # 3个 
类别 的 预测 示例 p = np . exp f 
/ np . sum np . exp f # 直接 
运算 数值 稳定性 不太好 # 我们 先 对 数据 做 
一个 平移 所以 输入 的 最大 值 为 0 f 
= np . max f # f 变成 666 333 
0 p = np . exp f / np . 
sum np . exp f # 结果 正确 同时 解决 
数值 不 稳定 问题 2 . 5.4 关于 softmax 这个 
名字 的 一点 说明 准确 地 说 SVM 分类器 使用 
hinge loss 有时候 也叫 max margin loss 而 Softmax 分类器 
使用 交叉 熵 损失 / cross entropy loss Softmax 分类器 
从 softmax 函数 恩 其实 做 的 事情 就是 把 
一列 原始 的 类别 得分 归一化 到 一列 和为1/nr 的 
正数 表示 概率 得到 softmax 函数 使得 交叉 熵 损失 
可以 用 起来 而 实际上 我们 并 没有 softmax loss 
这个 概念 因为 softmax 实质上 就是 一个 函数 有时候 我们 
图 方便 就 随口 称呼 softmax loss 2.6 SVM 与 
Softmax 这个 比较 很 有意思 就 像在 用到 分类 算法 
的 时候 就 会想 SVM 还是 logistic regression 呢 一样 
我们 先 用 一张 图 来 表示 从 输入端 到 
分类 结果 SVM/w 和/c Softmax/w 都/d 做了/i 啥/r 区别 就是 
拿 到 原始 像素 数据 映射 得到 的 得分 之后 
的 处理 而 正因为 处理 方式 不同 我们 定义 不同 
的 损失 函数 有 不同 的 优化 方法 2 . 
6.1 另外 的 差别 SVM 下 我们 能 完成 类别 
的 判定 但是 实际上 我们 得到 的 类别 得分 大小 
顺序 表示 着 所属 类别 的 排序 但是 得分 的 
绝对值 大小 并 没有 特别 明显 的 物理 含义 Softmax 
分类器 中 结果 的 绝对值 大小 表征 属于 该 类别 
的 概率 举个 例子 说 SVM 可能 拿 到 对应 
猫 / 狗 / 船 的 得分 12.5 0.6 23.0 
同 一个 问题 Softmax 分类器 拿到 0.9 0.09 0.01 这样在 
SVM 结果 下 我们 只 知道 猫 是 正确 答案 
而在 Softmax 分类器 的 结果 中 我们 可以 知道 属于 
每个 类别 的 概率 但是 Softmax 中 拿到 的 概率 
其实/d 和/c 正则化/i 参数/n λ/i 有/v 很大/a 的/uj 关系/n 因为 
λ 实际上 在 控制 着 W 的 伸缩 程度 所以 
也 控制 着 最后 得分 的 scale 这会 直接 影响 
最后 概率 向量 中 概率 的 分散度 比如说 某个 λ 
下 我们 得到 的 得分 和 概率 可能 如下 而 
我们 加大 λ 提高 其 约束 能力 后 很可能 得分 
变为 原来 的 一半 大小 这时候 如下 因为 λ 的 
不同 使得 最后 得到 的 结果 概率 分散度 有 很大 
的 差别 在 上面 的 结果 中 猫 有着 统治性 
的 概率 大小 而在 下面 的 结果 中 它 和 
船只 的 概率 差距 被 缩小 2 . 6.2 际 
应用 中的 SVM 与 Softmax 分类器 实际 应用 中 两类 
分类器 的 表现 是 相当 的 当然 每个人/i 都有/nr 自己/r 
的/uj 喜好/v 和/c 倾向性/n 习惯 用 某类 分类器 一定 要 
对比 一下 的话 SVM 其实 并不 在乎 每个 类别 得到 
的 绝对 得 分大小 举个 例子 说 我们 现在 对 
三个 类别 算得 的 得分 是 10 2 3 实际 
第一类 是 正确 结果 而 设定 Δ = 1 那么 
10 3 = 7 已经 比 1 要 大 很多 
了 那对/nr SVM 而言 它 觉得 这 已经 是 一个 
很 标准 的 答案 了 完全 满足 要求 了 不 
需要 再 做 其他 事情 了 结果 是 10 100 
100 或者 10 9 9 它 都是 满意 的 然而 
对于 Softmax 而言 不是 这样 的 10 100 100 和 
10 9 9 映 射到 概率 域 计算 得到 的 
交叉 熵 损失 是 有 很大 差别 的 所以 Softmax 
是 一个 永远 不会 满足 的 分类器 在 每个 得分 
计算 到 的 概率 基础 上 它 总是 觉得 可以 
让 概率分布 更 接近 标准 结果 一些 交叉 熵 损失 
更 小 一些 有 兴趣 的话 W 与 得分 预测 
结果 demo 是 一个 可以 手动 调整 和 观察 二维 
数据 上 的 分类 问题 随 W 变化 结果 变化 
的 demo 可以 动手 调调 看看 参考 资料 与 原文 
1 . 线性 分类器 在 深度 学习 与 计算机 视觉 
系列 2 我们 提到 了 图像 识别 的 问题 同时 
提出 了 一种 简单 的 解决 方法 KNN 然后 我们 
也 看到 了 KNN 在 解决 这个 问题 的 时候 
虽然 实现 起来 非常 简单 但是 有 很大 的 弊端 
分类器 必须 记住 全部 的 训练 数据 因为 要 遍历 
找 近邻 啊 而在 任何 实际 的 图像 训练 集上 
数据量 很 可能 非常 大 那么 一次性 载入 内存 不管 
是 速度 还是 对 硬件 的 要求 都是/nr 一个 极大 
的 挑战 分类 的 时候 要 遍历 所有 的 训练 
图片 这 是 一个 相当 相当 相当 耗时 的 过程 
这个 部分 我们 介绍 一类 新的 分类器 方法 而 对其 
的 改进 和 启发 也能 帮助 我们 自然而然 地 过渡 
到 深度 学习 中的 卷积 神经网 有 两个 重要 的 
概念 得分 函数 / score function 将 原始数据 映射 到 
每个 类 的 打分 的 函数 损失 函数 / loss 
function 用于 量化 模型 预测 结果 和 实际 结果 之间 
吻 合度 的 函数 在 我们 得到 损失 函数 之后 
我们 就 将 问题 转化 成为 一个 最 优化 的 
问题 目标 是 得到 让 我们 的 损失 函数 取值 
最小 的 一组 参数 2 . 得分 函数 / score 
function 首先 我们 定义 一个 有 原始 的 图片 像素 
值 映射 到最后 类目 得分 的 函数 也 就是 这里 
提到 的 得分 函数 先 笼统 解释一下 一会儿 我们 给 
个 具体 的 实例 来 说明 假设 我们 的 训练 
数据 为 对应 的 标签 yi 这里 i = 1 
N 表示 N 个 样本 yi ∈ 1 K 表示 
K 类 图片 比如 CIFAR 10 数据 集中 N = 
50000 而 D = 32x32x3 = 3072 像素 K = 
10 因为 这 时候 我们 有 10个 不同 的 类别 
狗 / 猫 / 车 我们 实际上 要 定义 一个 
将 原始 像素 映 射到 得分 上 函数 2.1 线性 
分类器 我们 先 丢出 一个 简单 的 线性映射 在 这个 
公式 里 我们/r 假定/v 图片/n 的/uj 像素/n 都平/nr 展为/i D 
x 1 的 向量 然后 我们 有 两个 参数 W 
是 K x D 的 矩阵 而 向量 b 为 
K x 1 的 在 CIFAR 10中 每张 图片 平 
展开 得到 一个 3072 x 1 的 向量 那W就/nr 应该 
是 一个 10 x 3072 的 矩阵 b 为 10 
x 1 的 向量 这样 以 我们 的 线性代数 知识 
我们 知道 这个 函数 接受 3072 个数 作为 输入 同时 
输出 10 个数 作为 类目 得分 我们 把 W 叫做 
权重 b 叫做 偏移 向量 说明 几个 点 我们 知道 
一次 矩阵 运算 我们 就 可以 借助 W 把 原始数据 
映射 为 10个 类别 的 得分 其实 我们 的 输入 
xi yi 其实 是 固定 的 我们 现在 要做 的 
事情 是 我们 要 调整 W b 使得 我们 的 
得分 结果 和 实际 的 类目 结果 最为 吻合 我们 
可以 想象 到 这样 一种 分类 解决方案 的 优势 是 
一旦 我们 找到 合适 的 参数 那么 我们 最后 的 
模型 可以 简化 到 只有 保留 W b 即可 而 
所有 原始 的 训练 数据 我们 都 可以 不管 了 
识别 阶段 我们 需要 做 的 事情 仅仅 是 一次 
矩阵 乘法 和 一次 加法 这个 计算 量 相对 之前 
不要 小 太多 好么 提前 剧透 一下 其实 卷积 神经网 
做 的 事情 也 是 类似 的 将 原始 输入 
的 像素 映 射成 类目 得分 只不过 它 的 中间 
映射 更加 复杂 参数 更多 而已 2.2 理解 线性 分类器 
我们 想想 其实 线性 分类器 在做 的 事情 是 对 
每个 像素点 的 三个 颜色通道 做 计算 咱们 拟人化 一下 
帮助 我们 理解 可以 认为 设定 的 参数 / 权重 
不同 会 影响 分类器 的 性格 从而 使得 分类器 对 
特定 位置 的 颜色 会 有 自己 的 喜好 举个 
例子 假如 说 我们 的 分类器 要 识别 船只 那么 
它 可能 会 喜欢 图片 的 四周 都是/nr 蓝色 通常 
船 只是 在 水里 海里 吧 我们 用 一个 实际 
的 例子 来 表示 这个 得分 映射 的 过程 大概 
就是 下 图 这个 样子 原始 像素点 向量 xi 经过 
W 和b/nr 映射 为 对应 结果 类别 的 得分 不过 
上面 这组 参数 其实 给 的 是 不太 恰当 的 
因为 我们 看到 在 这组 参 数下 图片 属于 狗狗 
的 得分 最高 _ | | 2 . 2.1 划分 
的 第 1种 理解 图片 被 平 展开 之后 向量 
维度 很高 高维空间 比较 难 想象 我们 简化 一下 假如 
把 图片 像素 输入 看做 可以 压缩 到 二 维空间 
之中 的 点 那 我们 想想 分类器 实际上 在做 的 
事情 就 如下 图 所示 W 中的 每 一列 对应 
类别 中的 每 一类 而 当 我们 改变 W 中的 
值 的 时候 图上 的 线 的 方向 会 跟着 
改变 那么 b 呢 对 b 是 一个 偏移量 它 
表示 当 我们 的 直线 方向 确定 以后 我们 可以 
适当 平移 直线 到 合适 的 位置 没有 b 会 
怎么样 呢 如果 直线 没有 偏移量 那 意味着 所有 的 
直线 都要 通过 原点 这种 强 限制 条件 下 显然 
不能 保证 很好 的 平面 类别 分割 2 . 2.2 
划分 的 第 2种 理解 对 W 第二种 理解 方式 
是 W 的 每 一行 可以 看做 是 其中 一个 
类别 的 模板 而 我们 输入 图片 相对 这个 类别 
的 得分 实际上 是 像素点 和 模板 匹配度 通过 内积 
运算 获得 而 类目 识别 实际上 就是 在 匹配 图像 
和 所有 类别 的 模板 找到 匹配度 最高 的 那个 
是不是 感觉 和 KNN 有点 类似 的 意思 是 有 
那么 点 相近 但是 这里 我们 不再 比对 所有 图片 
而是 比对 类别 的 模板 这样 比对 次数 只和 类 
目数 K 有关系 所以 自然 计算 量 要 小 很多 
同时 比对 的 时候 用 的 不再 是 l1 或者 
l2 距离 而是 内积 计算 我们 提前 透 露一下 CIFAR 
10 上 学习 到 的 模板 的 样子 你 看 
和 我们 设想 的 很 接近 ship 类别 的 周边 
有 大量 的 蓝色 而 car 的 旁边 是 土地 
的 颜色 2 . 2.3 关于 偏移量 的 处理 我们 
先回到 如下 的 公式 公式 中有 W 和b/nr 两个 参数 
我们 知道 调节 两个 参数 总归 比 调节 一个 参数 
要 麻烦 所以 我们 用 一点 小 技巧 来 把 
他们 组合 在 一起 放到 一个 参数 中 我们 现在 
要做 的 运算 是 矩阵 乘法 再加 偏移量 最 常用 
的 合并 方法 就是 想 办法 把 b 合并 成W的/nr 
一部分 我们 仔细 看看 下面 这 张 图片 我们 给 
输入 的 像素 矩阵 加上 一个 1 从而 把 b 
拼 接到 W 里 变成 一个 变量 依旧 拿 CIFAR 
10 举例 原本 是 3072 x 1 的 像素 向量 
我们 添上 最后 那个 1 变成 3073 x 1 的 
向量 而 W 变成 W b 2 . 2.4 关于 
数据 的 预处理 插播 一段 实际 应用 中 我们 很多 
时候 并 不是 把 原始 的 像素 矩阵 作为 输入 
而是 会 预先 做 一些 处理 比如说 有 一个 很 
重要 的 处理 叫做 去 均值 他 做 的 事情 
是 对于 训练 集 我们 求得 所有 图片 像素 矩阵 
的 均值 作为 中心 然后 输入 的 图片 先减 掉 
这个 均值 再做 后续 的 操作 有时候 我们 甚至 要 
对 图片 的 幅度 归一化 / scaling 去 均值 是 
一个 非常 重要 的 步骤 原因 我们 在 后续 的 
梯度 下降 里 会 提到 2.3 损失 函数 我们 已经 
通过 参数 W 完成 了 由 像素 映 射到 类目 
得分 的 过程 同时 我们 知道 我们 的 训练 数据 
xi yi 是 给定 的 我们 可以 调整 的 是 
参数 / 权重 W 使得 这个 映射 的 结果 和 
实际 类别 是 吻合 的 我们 回到 最 上面 的 
图片 中 预测 猫 / 狗 / 船 得分 的 
例子 里 这个 图片 中 给定 的 W 显然 不是 
一个 合理 的 值 预测/vn 的/uj 结果/n 和/c 实际/n 情况/n 
有/v 很大/a 的/uj 偏差/n 于是 我们 现在 要 想 办法 
去 把 这个 偏差 表示出来 拟人 一点 说 就是 我们 
希望 我们 的 模型 在 训练 的 过程 中 能够 
对 输出 的 结果 计算 并 知道 自己 做 的 
好坏 而能 帮助 我们 完成 这 件 事情 的 工具 
叫做 损失 函数 / loss function 其实 它 还有 很多 
其他 的 名字 比如说 你 说不定 在 其他 的 地方 
听人 把 它 叫做 代价 函数 / cost function 或者 
客观 度 / objective 直观 一点 说 就是 我们 输出 
的 结果 和 实际 情况 偏差 很大 的 时候 损失 
/ 代价 就会 很大 2 . 3.1 多 类别 支持 
向量 机 损失 / Multiclass Support Vector Machine loss 腻 
害 的 大神 们 定义 出了 好些 损失 函数 我们 
这里 首先 要 介绍 一种 极其 常用 的 叫 做多 
类别 支持 向量 机 损失 Multiclass SVM loss 如果 要 
用 一句 精简 的话 来 描述 它 就是 它 SVM 
希望 正确 的 类别 结果 获得 的 得分 比 不 
正确 的 类别 至少 要 高上 一个 固定 的 大小 
Δ 我们 先 解释 一下 这 句话 一会儿 再 举个 
例子 说明 一下 对于 训练 集中 的 第 i 张 
图片 数据 xi 我们 的 得分 函数 在 参数 W 
下 会 计算 出 一个 所有 类 得分 结果 其中 
第 j 类 得分 我们 记作 该 图片 的 实际 
类别 为 yi 则 对于 第 i 张 样本 图片 
我们 的 损失 函数 是 如下 定义 的 看 公式 
容易 看 瞎 译者 也 经常 深深地 为 自己 智商 
感到 捉急 我们 举 个 例子 来 解释 一下 这个 
公式 假如 我们 现在 有 三个 类别 而 得分 函数 
计算 某 张 图片 的 得分 为 而 实际 的 
结果 是 第一 类 yi = 0 假设 Δ = 
10 这个 参数 一会儿 会 介绍 上面 的 公式 把 
错误类别 j ≠ yi 都 遍历 了 一遍 求值 加 
和 仔细 看看 上述 的 两项 左边 项 10 和0/nr 
中的 最大值 为 0 因此 取值 是 零 其实 这里 
的 含义 是 实际 的 类别 得分 13 要比 第二类 
得分 7 高出 20 超过 了 我们 设定 的 正确 
类目 和 错误 类目 之间 的 最小 margin   Δ 
= 10 因此 第二类 的 结果 我们 认为 是 满意 
的 并不 带来 loss 所以 值 为 0 而 第三 
类 得分 11 仅 比 13 小 2 没有 大于 
Δ = 10 因此 我们 认为 他 是 有 损失 
/ loss 的 而 损失 就 是 当前 距离 2 
距离 设定 的 最小 距离 Δ 的 差距 8 注意 
到 我们 的 得分 函数 是 输入 像素 值 的 
一个 线性函数 因此 公式 又 可以 简化 为 其中 wj 
是 W 的 第 j 行 我们 还 需要 提 
一下 的 是 关于 损失 函数 中 max 0 的 
这种 形式 我们 也 把 它 叫做 hinge loss / 
铰链 型 损失 有时候 你 会 看到 squared hinge loss 
SVM 也叫 L2 SVM 它 用到 的 是 这个 损失 
函数 惩罚 那些 在 设定 Δ 距离 之内 的 错误 
类别 的 惩罚 度 更高 两种 损失 函数 标准 在 
特定 的 场景 下 效果 各有 优劣 要 判定 用 
哪个 还是 得 借助于 交叉 验证 / cross validation 对于 
损失 函数 的 理解 可以 参照 下图 . 3.2 正则化 
如果 大家 仔细 想想 会 发现 使用 上述 的 loss 
function 会 有一个 bug 如果 参数 W 能够 正确 地 
识别 训练 集中 所有 的 图片 损失 函数 为 0 
那么 我们 对 M 做 一些 变换 可以 得到 无 
数组 也能 满足 loss function = 0 的 参数 W 
举个 例子 对于 λ 1 的 所有 λ W 原来 
的 错误类别 和 正确 类别 之间 的 距离 已经 大于 
Δ 现在 乘以 λ 更大 了 显然 也 能 满足 
loss 为 0 于是 我们 得 想 办法 把 W 
参数 的 这种 不确定性 去 除掉 啊 这 就是 我们 
要 提到 的 正则化 我们 需要 在 原来 的 损失 
函 数上 再加 上 一项 正则化 项 regularization penalty   
R W 最 常见 的 正则化 项是/nr L2 范数 它 
会对 幅度 很大 的 特征 权重 给 很高 的 惩罚 
根据 公式 可以 看到 这个 表达式 R W 把 所有 
W 的 元素 的 平方 项求/nr 和了 而且 它 和 
数据 本身 无关 只和 特征 权重 有关系 我们 把 两 
部分组 数据 损失 / data loss 和 正则化 损失 / 
regularization loss 在 一起 得到 完整 的 多 类别 SVM 
损失 权重 如下 也 可以 展开 得到 更 具体 的 
完整 形式 其中 N 是 训练 样本数 我们 给 正则化 
项 一个 参数 λ 但是 这个 参数 的 设定 只有 
通过 实验 确定 对 还是 得 交叉 验证 / cross 
validation 关于/p 设定/v 这样/r 一个/m 正则化/i 惩罚/vn 项/n 为什么/r 能/v 
解决/v W/w 的/uj 不确定性/n 我们 在 之后 的 系列 里 
会 提到 这里 我们 举 个 例子 简单 看看 这个 
项是/nr 怎么 起到 作用 的 假定 我们 的 输入 图片 
像素 矩阵 是 x = 1 1 1 1 而 
现在 我们 有 两组 不同 的 W 权重 参数 中 
对应 的 向量 w1 = 1 0 0 0 w2 
= 0.25 0.25 0.25 0.25 那 我们 很 容易 知道 
wT1x = wT2x = 1 所以 不加 正则 项的/nr 时候 
这俩 得到 的 结果 是 完全 一样 的 也 就 
意味着 它们 是 等价 的 但是 加了 正则 项 之后 
我们 发现 w2 总体 的 损失 函数 结果 更小 因为 
4 * 0.25 ^ 2 1 于是 我们 的 系统 
会 选择 w2 这也 就 意味着 系统 更 喜欢 权重 
分布 均匀 的 参数 而 不是 某些 特征 权重 明显 
高于 其他 权重 占据 绝对 主导 作用 的 参数 之后 
的 系列 里 会 提到 这样 一个 平滑 的 操作 
实际上 也 会 提高 系统 的 泛化 能力 让其 具备 
更高 的 通用性 而不 至于 在 训练 集上 过拟合 另外 
我们 在 讨论 过拟合 的 这个 部分 的 时候 并 
没有 提到 b 这个 参数 这 是因为 它 并不 具备 
像 W 一样 的 控制 输入 特征 的 某个 维度 
影响力 的 能力 还 需要 说 一下 的 是 因为 
正则 项的/nr 引入 训练 集上 的 准确度 是 会 有 
一定 程度 的 下降 的 我们 永远 也 不 可能 
让 损失 达到 零 了 因为 这 意味着 正则化 项为0/nr 
也 就是 W = 0 下面 是 简单 的 计算 
损失 函数 没 加上 正则化 项 的 代码 有/v 未向/i 
量化/v 和向/nr 量化/v 两种/m 形式/n def L _ i x 
y W 未向 量化 版本 . 对 给定 的 单个 
样本 x y 计算 multiclass svm loss . x 代表 
图片 像素 输入 的 向量 例如 CIFAR 10中 是 3073 
x 1 因为 添加 了 bias 项 对应 的 1 
到 x 中 y 图片 对应 的 类别 编号 比如 
CIFAR 10中 是 0 9 W 权重 矩阵 例如 CIFAR 
10中 是 10 x 3073 delta = 1.0 # 设定 
delta scores = W . dot x # 内积 计算 
得分 correct _ class _ score = scores y D 
= W . shape 0 # 类 别数 例如 10 
loss _ i = 0.0 for j in xrange D 
# 遍历 所有 错误 的 类别 if j = = 
y # 跳过 正确 类别 continue # 对 第 i 
个 样本 累加 loss loss _ i + = max 
0 scores j correct _ class _ score + delta 
return loss _ i def L _ i _ vectorized 
x y W 半 向 量化 的 版本 速度 更快 
之所以 说是 半 向 量化 是 因为 这个 函数 外层 
要用 for 循环 遍历 整个 训练 集 _ | | 
delta = 1.0 scores = W . dot x # 
矩阵 一次性 计算 margins = np . maximum 0 scores 
scores y + delta margins y = 0 loss _ 
i = np . sum margins return loss _ i 
def L X y W 全向 量化 实现 X 包含 
所有 训练样本 中 数据 例如 CIFAR 10 是 3073 x 
50000 y 所有 的 类别 结果 例如 50000 x 1 
的 向量 W 权重 矩阵 例如 10 x 3073 # 
待 完成 . . . 说到 这里 其实 我们 的 
损失 函数 是 提供 给 我们 一个 数值 型 的 
表示 来 衡量 我们 的 预测 结果 和 实际 结果 
的 差别 而 要 提高 预测 的 准确性 要做 的 
事情 是 想办法 最小化 这个 loss 2.4 一些 现实 的 
考虑 点 2 . 4.1 设定 Delta 我们 在 计算 
Multi SVM loss 的 时候 Δ 是 我们 提前 设定 
的 一个 参数 这个 值 咋 设定 莫不是 也 需要 
交叉 验证 其实 基本上 大部分 的 场合 下 我们 设定 
Δ = 1.0 都是/nr 一个 安全 的 设定 我们 看 
公式 中的 参数 Δ 和λ/nr 似乎 是 两个 截然不同 的 
参数 实际上 他俩 做 的 事情 比较 类似 都是 尽量 
让 模型 贴近 标准 预测 结果 的 时候 在 数据 
损失 / data loss 和 正则化 损失 / regularization loss 
之间 做 一个 交换 和 平衡 在 损失 函数 计算公式 
里 可以 看出 权重 W 的 幅度 对 类别 得分 
有最/nr 直接 的 影响 我们 减小 W 最后 的 得分 
就 会 减少 我们 增大 W 最后 的 得分 就 
增大 从 这个 角度 看 Δ 这个 参数 的 设定 
Δ = 1 或者 Δ = 100 其实 无法 限定 
W 的 伸缩 而 真正 可以 做到 这点 的 是 
正则化 项 λ 的 大小 实际上 控制 着 权重 可以 
增长 和 膨胀 的 空间 2 . 4.2 关于 二元 
/ Binary 支持 向量 机 如果 大家 之前 接触 过 
Binary SVM 我们 知道 它 的 公式 如下 我们 可以 
理解 为 类别 yi ∈ − 1 1 它 是 
我们 的 多 类别 识别 的 一个 特殊 情况 而 
这里 的 C 和λ是/nr 一样 的 作用 只不过 他们 的 
大小 对 结果 的 影响 是 相反 的 也 就是 
C ∝ 1 / λ 2 . 4.3 关于 非 
线性 的 SVM 如果 对 机器 学习 有 了解 你 
可能 会 了解 很多 其他 关于 SVM 的 术语 kernel 
dual SMO 算法 等等 在 这个 系列 里面 我们 只 
讨论 最 基本 的 线性 形式 当然 其实 从 本质 
上 来说 这些 方法 都是/nr 类似 的 2.5 Softmax 分类器 
话说 其实 有 两种 特别 常见 的 分类器 前面 提 
的 SVM 是 其中 的 一种 而 另外 一种 就是 
Softmax 分类器 它 有着 截然不同 的 损失 函数 如果 你 
听说 过 逻辑 回归 二 分类器 那么 Softmax 分类器 是 
它 泛化 到 多 分类 的 情形 不像 SVM 这种 
直接 给 类目 打分 f xi W 并 作为 输出 
Softmax 分类器 从新 的 角度 做了 不 一样 的 处理 
我们 依旧 需要 将 输入 的 像素 向量 映射 为 
得分 f x _ i W = W x _ 
i 只不过 我们 还 需要 将 得分 映 射到 概率 
域 我们 也 不再 使用 hinge loss 了 而是 使用 
交叉 熵 损失 / cross entropy loss 形式 如下 我们 
使用 fj 来 代表 得 分向量 f 的 第 j 
个 元素 值 和 前面 提到 的 一样 总体 的 
损失 / loss 也是 Li 遍历 训练 集 之后 的 
均值 再 加上 正则化 项R/nr W 而 函数 被 称之为 
softmax 函数 它 的 输入 值 是 一个 实数 向量 
z 然后 在 指数 域 做 了 一个 归一化 以 
保证 之和 为 1 映射 为 概率 2 . 5.1 
信息论 角度 的 理解 对于 两个 概率分布 p 真实 的 
概率分布 和 估测 的 概率分布 q 估测 的 属于 每个 
类 的 概率 它们 的 互 熵 定义 为 如下 
形式 而 Softmax 分类器 要做 的 事情 就是 要 最小化 
预测 类别 的 概率分布 之前 看到 了 是 与 实际 
类别 概率分布 p = 0 1 0 只在 结 果类 
目上 是 1 其余 都为 0 两个 概率分布 的 交叉 
熵 另外 因为 互 熵 可以用 熵 加上 KL 距离 
/ Kullback Leibler Divergence 也叫 相对 熵 / Relative Entropy 
来 表示 即 而 p 的 熵 为 0 这 
是 一个 确定 事件 无 随机性 所以 互 熵 最小化 
等同于 最小化 两个 分布 之间 的 KL 距离 换句话说 交叉 
熵 想 要从 给定 的 分布 q 上 预测 结果 
分布 p 2 . 5.2 概率 角度 的 理解 我们 
再 来 看看 以下 表达式 其实 可以 看做 给定 图片 
数据 xi 和 类别 yi 以及 参数 W 之后 的 
归一化 概率 在 概率 的 角度 理解 我们 在 做 
的 事情 就是 最小化 错误类别 的 负 log 似 然 
概率 也 可以 理解 为 进行 最大 似 然 估计 
/ Maximum Likelihood Estimation MLE 这个 理解 角度 还有 一个 
好处 这个 时候 我们 的 正则化 项R/nr W 有 很好 
的 解释性 可以 理解 为 整个 损失 函数 在 权重 
矩阵 W 上 的 一个 高斯 先验 所以 其实 这 
时候 是 在做 一个 最大 后验/nr 估计 / Maximum a 
posteriori MAP 2 . 5.3 实际 工程 上 的 注意 
点 数据 稳定性 在 我们 要 写 代码 工程 实现 
Softmax 函数 的 时候 计算 的 中间 项 因为 指数 
运算 可能 变得 非常 大 除法 的 结果 非常 不 
稳定 所以 这里 需要 一个 小 技巧 注意到 如果 我们 
在 分子 分母 前 都 乘以 常数 C 然后 整 
理到 指 数上 我们 会 得到 下面 的 公式 C 
的 取值 由 我们 而定 不 影响 最后 的 结果 
但是 对于 实际 计算 过程 中 的 稳定性 有 很大 
的 帮助 一个 最 常见 的 C 取值 为 这 
表明 我们 应该 平移 向量 f 中的 值 使得 最大值 
为 0 以下 的 代码 是 它 的 一个 实现 
f = np . array 123 456 789 # 3个 
类别 的 预测 示例 p = np . exp f 
/ np . sum np . exp f # 直接 
运算 数值 稳定性 不太好 # 我们 先 对 数据 做 
一个 平移 所以 输入 的 最大 值 为 0 f 
= np . max f # f 变成 666 333 
0 p = np . exp f / np . 
sum np . exp f # 结果 正确 同时 解决 
数值 不 稳定 问题 2 . 5.4 关于 softmax 这个 
名字 的 一点 说明 准确 地 说 SVM 分类器 使用 
hinge loss 有时候 也叫 max margin loss 而 Softmax 分类器 
使用 交叉 熵 损失 / cross entropy loss Softmax 分类器 
从 softmax 函数 恩 其实 做 的 事情 就是 把 
一列 原始 的 类别 得分 归一化 到 一列 和为1/nr 的 
正数 表示 概率 得到 softmax 函数 使得 交叉 熵 损失 
可以 用 起来 而 实际上 我们 并 没有 softmax loss 
这个 概念 因为 softmax 实质上 就是 一个 函数 有时候 我们 
图 方便 就 随口 称呼 softmax loss 2.6 SVM 与 
Softmax 这个 比较 很 有意思 就 像在 用到 分类 算法 
的 时候 就 会想 SVM 还是 logistic regression 呢 一样 
我们 先 用 一张 图 来 表示 从 输入端 到 
分类 结果 SVM/w 和/c Softmax/w 都/d 做了/i 啥/r 区别 就是 
拿 到 原始 像素 数据 映射 得到 的 得分 之后 
的 处理 而 正因为 处理 方式 不同 我们 定义 不同 
的 损失 函数 有 不同 的 优化 方法 2 . 
6.1 另外 的 差别 SVM 下 我们 能 完成 类别 
的 判定 但是 实际上 我们 得到 的 类别 得分 大小 
顺序 表示 着 所属 类别 的 排序 但是 得分 的 
绝对值 大小 并 没有 特别 明显 的 物理 含义 Softmax 
分类器 中 结果 的 绝对值 大小 表征 属于 该 类别 
的 概率 举个 例子 说 SVM 可能 拿 到 对应 
猫 / 狗 / 船 的 得分 12.5 0.6 23.0 
同 一个 问题 Softmax 分类器 拿到 0.9 0.09 0.01 这样在 
SVM 结果 下 我们 只 知道 猫 是 正确 答案 
而在 Softmax 分类器 的 结果 中 我们 可以 知道 属于 
每个 类别 的 概率 但是 Softmax 中 拿到 的 概率 
其实/d 和/c 正则化/i 参数/n λ/i 有/v 很大/a 的/uj 关系/n 因为 
λ 实际上 在 控制 着 W 的 伸缩 程度 所以 
也 控制 着 最后 得分 的 scale 这会 直接 影响 
最后 概率 向量 中 概率 的 分散度 比如说 某个 λ 
下 我们 得到 的 得分 和 概率 可能 如下 而 
我们 加大 λ 提高 其 约束 能力 后 很可能 得分 
变为 原来 的 一半 大小 这时候 如下 因为 λ 的 
不同 使得 最后 得到 的 结果 概率 分散度 有 很大 
的 差别 在 上面 的 结果 中 猫 有着 统治性 
的 概率 大小 而在 下面 的 结果 中 它 和 
船只 的 概率 差距 被 缩小 2 . 6.2 际 
应用 中的 SVM 与 Softmax 分类器 实际 应用 中 两类 
分类器 的 表现 是 相当 的 当然 每个人/i 都有/nr 自己/r 
的/uj 喜好/v 和/c 倾向性/n 习惯 用 某类 分类器 一定 要 
对比 一下 的话 SVM 其实 并不 在乎 每个 类别 得到 
的 绝对 得 分大小 举个 例子 说 我们 现在 对 
三个 类别 算得 的 得分 是 10 2 3 实际 
第一类 是 正确 结果 而 设定 Δ = 1 那么 
10 3 = 7 已经 比 1 要 大 很多 
了 那对/nr SVM 而言 它 觉得 这 已经 是 一个 
很 标准 的 答案 了 完全 满足 要求 了 不 
需要 再 做 其他 事情 了 结果 是 10 100 
100 或者 10 9 9 它 都是 满意 的 然而 
对于 Softmax 而言 不是 这样 的 10 100 100 和 
10 9 9 映 射到 概率 域 计算 得到 的 
交叉 熵 损失 是 有 很大 差别 的 所以 Softmax 
是 一个 永远 不会 满足 的 分类器 在 每个 得分 
计算 到 的 概率 基础 上 它 总是 觉得 可以 
让 概率分布 更 接近 标准 结果 一些 交叉 熵 损失 
更 小 一些 有 兴趣 的话 W 与 得分 预测 
结果 demo 是 一个 可以 手动 调整 和 观察 二维 
数据 上 的 分类 问题 随 W 变化 结果 变化 
的 demo 可以 动手 调调 看看 参考 资料 与 原文 
1 . 线性 分类器 在 深度 学习 与 计算机 视觉 
系列 2 我们 提到 了 图像 识别 的 问题 同时 
提出 了 一种 简单 的 解决 方法 KNN 然后 我们 
也 看到 了 KNN 在 解决 这个 问题 的 时候 
虽然 实现 起来 非常 简单 但是 有 很大 的 弊端 
分类器 必须 记住 全部 的 训练 数据 因为 要 遍历 
找 近邻 啊 而在 任何 实际 的 图像 训练 集上 
数据量 很 可能 非常 大 那么 一次性 载入 内存 不管 
是 速度 还是 对 硬件 的 要求 都是/nr 一个 极大 
的 挑战 分类 的 时候 要 遍历 所有 的 训练 
图片 这 是 一个 相当 相当 相当 耗时 的 过程 
这个 部分 我们 介绍 一类 新的 分类器 方法 而 对其 
的 改进 和 启发 也能 帮助 我们 自然而然 地 过渡 
到 深度 学习 中的 卷积 神经网 有 两个 重要 的 
概念 得分 函数 / score function 将 原始数据 映射 到 
每个 类 的 打分 的 函数 损失 函数 / loss 
function 用于 量化 模型 预测 结果 和 实际 结果 之间 
吻 合度 的 函数 在 我们 得到 损失 函数 之后 
我们 就 将 问题 转化 成为 一个 最 优化 的 
问题 目标 是 得到 让 我们 的 损失 函数 取值 
最小 的 一组 参数 2 . 得分 函数 / score 
function 首先 我们 定义 一个 有 原始 的 图片 像素 
值 映射 到最后 类目 得分 的 函数 也 就是 这里 
提到 的 得分 函数 先 笼统 解释一下 一会儿 我们 给 
个 具体 的 实例 来 说明 假设 我们 的 训练 
数据 为 对应 的 标签 yi 这里 i = 1 
N 表示 N 个 样本 yi ∈ 1 K 表示 
K 类 图片 比如 CIFAR 10 数据 集中 N = 
50000 而 D = 32x32x3 = 3072 像素 K = 
10 因为 这 时候 我们 有 10个 不同 的 类别 
狗 / 猫 / 车 我们 实际上 要 定义 一个 
将 原始 像素 映 射到 得分 上 函数 2.1 线性 
分类器 我们 先 丢出 一个 简单 的 线性映射 在 这个 
公式 里 我们/r 假定/v 图片/n 的/uj 像素/n 都平/nr 展为/i D 
x 1 的 向量 然后 我们 有 两个 参数 W 
是 K x D 的 矩阵 而 向量 b 为 
K x 1 的 在 CIFAR 10中 每张 图片 平 
展开 得到 一个 3072 x 1 的 向量 那W就/nr 应该 
是 一个 10 x 3072 的 矩阵 b 为 10 
x 1 的 向量 这样 以 我们 的 线性代数 知识 
我们 知道 这个 函数 接受 3072 个数 作为 输入 同时 
输出 10 个数 作为 类目 得分 我们 把 W 叫做 
权重 b 叫做 偏移 向量 说明 几个 点 我们 知道 
一次 矩阵 运算 我们 就 可以 借助 W 把 原始数据 
映射 为 10个 类别 的 得分 其实 我们 的 输入 
xi yi 其实 是 固定 的 我们 现在 要做 的 
事情 是 我们 要 调整 W b 使得 我们 的 
得分 结果 和 实际 的 类目 结果 最为 吻合 我们 
可以 想象 到 这样 一种 分类 解决方案 的 优势 是 
一旦 我们 找到 合适 的 参数 那么 我们 最后 的 
模型 可以 简化 到 只有 保留 W b 即可 而 
所有 原始 的 训练 数据 我们 都 可以 不管 了 
识别 阶段 我们 需要 做 的 事情 仅仅 是 一次 
矩阵 乘法 和 一次 加法 这个 计算 量 相对 之前 
不要 小 太多 好么 提前 剧透 一下 其实 卷积 神经网 
做 的 事情 也 是 类似 的 将 原始 输入 
的 像素 映 射成 类目 得分 只不过 它 的 中间 
映射 更加 复杂 参数 更多 而已 2.2 理解 线性 分类器 
我们 想想 其实 线性 分类器 在做 的 事情 是 对 
每个 像素点 的 三个 颜色通道 做 计算 咱们 拟人化 一下 
帮助 我们 理解 可以 认为 设定 的 参数 / 权重 
不同 会 影响 分类器 的 性格 从而 使得 分类器 对 
特定 位置 的 颜色 会 有 自己 的 喜好 举个 
例子 假如 说 我们 的 分类器 要 识别 船只 那么 
它 可能 会 喜欢 图片 的 四周 都是/nr 蓝色 通常 
船 只是 在 水里 海里 吧 我们 用 一个 实际 
的 例子 来 表示 这个 得分 映射 的 过程 大概 
就是 下 图 这个 样子 原始 像素点 向量 xi 经过 
W 和b/nr 映射 为 对应 结果 类别 的 得分 不过 
上面 这组 参数 其实 给 的 是 不太 恰当 的 
因为 我们 看到 在 这组 参 数下 图片 属于 狗狗 
的 得分 最高 _ | | 2 . 2.1 划分 
的 第 1种 理解 图片 被 平 展开 之后 向量 
维度 很高 高维空间 比较 难 想象 我们 简化 一下 假如 
把 图片 像素 输入 看做 可以 压缩 到 二 维空间 
之中 的 点 那 我们 想想 分类器 实际上 在做 的 
事情 就 如下 图 所示 W 中的 每 一列 对应 
类别 中的 每 一类 而 当 我们 改变 W 中的 
值 的 时候 图上 的 线 的 方向 会 跟着 
改变 那么 b 呢 对 b 是 一个 偏移量 它 
表示 当 我们 的 直线 方向 确定 以后 我们 可以 
适当 平移 直线 到 合适 的 位置 没有 b 会 
怎么样 呢 如果 直线 没有 偏移量 那 意味着 所有 的 
直线 都要 通过 原点 这种 强 限制 条件 下 显然 
不能 保证 很好 的 平面 类别 分割 2 . 2.2 
划分 的 第 2种 理解 对 W 第二种 理解 方式 
是 W 的 每 一行 可以 看做 是 其中 一个 
类别 的 模板 而 我们 输入 图片 相对 这个 类别 
的 得分 实际上 是 像素点 和 模板 匹配度 通过 内积 
运算 获得 而 类目 识别 实际上 就是 在 匹配 图像 
和 所有 类别 的 模板 找到 匹配度 最高 的 那个 
是不是 感觉 和 KNN 有点 类似 的 意思 是 有 
那么 点 相近 但是 这里 我们 不再 比对 所有 图片 
而是 比对 类别 的 模板 这样 比对 次数 只和 类 
目数 K 有关系 所以 自然 计算 量 要 小 很多 
同时 比对 的 时候 用 的 不再 是 l1 或者 
l2 距离 而是 内积 计算 我们 提前 透 露一下 CIFAR 
10 上 学习 到 的 模板 的 样子 你 看 
和 我们 设想 的 很 接近 ship 类别 的 周边 
有 大量 的 蓝色 而 car 的 旁边 是 土地 
的 颜色 2 . 2.3 关于 偏移量 的 处理 我们 
先回到 如下 的 公式 公式 中有 W 和b/nr 两个 参数 
我们 知道 调节 两个 参数 总归 比 调节 一个 参数 
要 麻烦 所以 我们 用 一点 小 技巧 来 把 
他们 组合 在 一起 放到 一个 参数 中 我们 现在 
要做 的 运算 是 矩阵 乘法 再加 偏移量 最 常用 
的 合并 方法 就是 想 办法 把 b 合并 成W的/nr 
一部分 我们 仔细 看看 下面 这 张 图片 我们 给 
输入 的 像素 矩阵 加上 一个 1 从而 把 b 
拼 接到 W 里 变成 一个 变量 依旧 拿 CIFAR 
10 举例 原本 是 3072 x 1 的 像素 向量 
我们 添上 最后 那个 1 变成 3073 x 1 的 
向量 而 W 变成 W b 2 . 2.4 关于 
数据 的 预处理 插播 一段 实际 应用 中 我们 很多 
时候 并 不是 把 原始 的 像素 矩阵 作为 输入 
而是 会 预先 做 一些 处理 比如说 有 一个 很 
重要 的 处理 叫做 去 均值 他 做 的 事情 
是 对于 训练 集 我们 求得 所有 图片 像素 矩阵 
的 均值 作为 中心 然后 输入 的 图片 先减 掉 
这个 均值 再做 后续 的 操作 有时候 我们 甚至 要 
对 图片 的 幅度 归一化 / scaling 去 均值 是 
一个 非常 重要 的 步骤 原因 我们 在 后续 的 
梯度 下降 里 会 提到 2.3 损失 函数 我们 已经 
通过 参数 W 完成 了 由 像素 映 射到 类目 
得分 的 过程 同时 我们 知道 我们 的 训练 数据 
xi yi 是 给定 的 我们 可以 调整 的 是 
参数 / 权重 W 使得 这个 映射 的 结果 和 
实际 类别 是 吻合 的 我们 回到 最 上面 的 
图片 中 预测 猫 / 狗 / 船 得分 的 
例子 里 这个 图片 中 给定 的 W 显然 不是 
一个 合理 的 值 预测/vn 的/uj 结果/n 和/c 实际/n 情况/n 
有/v 很大/a 的/uj 偏差/n 于是 我们 现在 要 想 办法 
去 把 这个 偏差 表示出来 拟人 一点 说 就是 我们 
希望 我们 的 模型 在 训练 的 过程 中 能够 
对 输出 的 结果 计算 并 知道 自己 做 的 
好坏 而能 帮助 我们 完成 这 件 事情 的 工具 
叫做 损失 函数 / loss function 其实 它 还有 很多 
其他 的 名字 比如说 你 说不定 在 其他 的 地方 
听人 把 它 叫做 代价 函数 / cost function 或者 
客观 度 / objective 直观 一点 说 就是 我们 输出 
的 结果 和 实际 情况 偏差 很大 的 时候 损失 
/ 代价 就会 很大 2 . 3.1 多 类别 支持 
向量 机 损失 / Multiclass Support Vector Machine loss 腻 
害 的 大神 们 定义 出了 好些 损失 函数 我们 
这里 首先 要 介绍 一种 极其 常用 的 叫 做多 
类别 支持 向量 机 损失 Multiclass SVM loss 如果 要 
用 一句 精简 的话 来 描述 它 就是 它 SVM 
希望 正确 的 类别 结果 获得 的 得分 比 不 
正确 的 类别 至少 要 高上 一个 固定 的 大小 
Δ 我们 先 解释 一下 这 句话 一会儿 再 举个 
例子 说明 一下 对于 训练 集中 的 第 i 张 
图片 数据 xi 我们 的 得分 函数 在 参数 W 
下 会 计算 出 一个 所有 类 得分 结果 其中 
第 j 类 得分 我们 记作 该 图片 的 实际 
类别 为 yi 则 对于 第 i 张 样本 图片 
我们 的 损失 函数 是 如下 定义 的 看 公式 
容易 看 瞎 译者 也 经常 深深地 为 自己 智商 
感到 捉急 我们 举 个 例子 来 解释 一下 这个 
公式 假如 我们 现在 有 三个 类别 而 得分 函数 
计算 某 张 图片 的 得分 为 而 实际 的 
结果 是 第一 类 yi = 0 假设 Δ = 
10 这个 参数 一会儿 会 介绍 上面 的 公式 把 
错误类别 j ≠ yi 都 遍历 了 一遍 求值 加 
和 仔细 看看 上述 的 两项 左边 项 10 和0/nr 
中的 最大值 为 0 因此 取值 是 零 其实 这里 
的 含义 是 实际 的 类别 得分 13 要比 第二类 
得分 7 高出 20 超过 了 我们 设定 的 正确 
类目 和 错误 类目 之间 的 最小 margin   Δ 
= 10 因此 第二类 的 结果 我们 认为 是 满意 
的 并不 带来 loss 所以 值 为 0 而 第三 
类 得分 11 仅 比 13 小 2 没有 大于 
Δ = 10 因此 我们 认为 他 是 有 损失 
/ loss 的 而 损失 就 是 当前 距离 2 
距离 设定 的 最小 距离 Δ 的 差距 8 注意 
到 我们 的 得分 函数 是 输入 像素 值 的 
一个 线性函数 因此 公式 又 可以 简化 为 其中 wj 
是 W 的 第 j 行 我们 还 需要 提 
一下 的 是 关于 损失 函数 中 max 0 的 
这种 形式 我们 也 把 它 叫做 hinge loss / 
铰链 型 损失 有时候 你 会 看到 squared hinge loss 
SVM 也叫 L2 SVM 它 用到 的 是 这个 损失 
函数 惩罚 那些 在 设定 Δ 距离 之内 的 错误 
类别 的 惩罚 度 更高 两种 损失 函数 标准 在 
特定 的 场景 下 效果 各有 优劣 要 判定 用 
哪个 还是 得 借助于 交叉 验证 / cross validation 对于 
损失 函数 的 理解 可以 参照 下图 . 3.2 正则化 
如果 大家 仔细 想想 会 发现 使用 上述 的 loss 
function 会 有一个 bug 如果 参数 W 能够 正确 地 
识别 训练 集中 所有 的 图片 损失 函数 为 0 
那么 我们 对 M 做 一些 变换 可以 得到 无 
数组 也能 满足 loss function = 0 的 参数 W 
举个 例子 对于 λ 1 的 所有 λ W 原来 
的 错误类别 和 正确 类别 之间 的 距离 已经 大于 
Δ 现在 乘以 λ 更大 了 显然 也 能 满足 
loss 为 0 于是 我们 得 想 办法 把 W 
参数 的 这种 不确定性 去 除掉 啊 这 就是 我们 
要 提到 的 正则化 我们 需要 在 原来 的 损失 
函 数上 再加 上 一项 正则化 项 regularization penalty   
R W 最 常见 的 正则化 项是/nr L2 范数 它 
会对 幅度 很大 的 特征 权重 给 很高 的 惩罚 
根据 公式 可以 看到 这个 表达式 R W 把 所有 
W 的 元素 的 平方 项求/nr 和了 而且 它 和 
数据 本身 无关 只和 特征 权重 有关系 我们 把 两 
部分组 数据 损失 / data loss 和 正则化 损失 / 
regularization loss 在 一起 得到 完整 的 多 类别 SVM 
损失 权重 如下 也 可以 展开 得到 更 具体 的 
完整 形式 其中 N 是 训练 样本数 我们 给 正则化 
项 一个 参数 λ 但是 这个 参数 的 设定 只有 
通过 实验 确定 对 还是 得 交叉 验证 / cross 
validation 关于/p 设定/v 这样/r 一个/m 正则化/i 惩罚/vn 项/n 为什么/r 能/v 
解决/v W/w 的/uj 不确定性/n 我们 在 之后 的 系列 里 
会 提到 这里 我们 举 个 例子 简单 看看 这个 
项是/nr 怎么 起到 作用 的 假定 我们 的 输入 图片 
像素 矩阵 是 x = 1 1 1 1 而 
现在 我们 有 两组 不同 的 W 权重 参数 中 
对应 的 向量 w1 = 1 0 0 0 w2 
= 0.25 0.25 0.25 0.25 那 我们 很 容易 知道 
wT1x = wT2x = 1 所以 不加 正则 项的/nr 时候 
这俩 得到 的 结果 是 完全 一样 的 也 就 
意味着 它们 是 等价 的 但是 加了 正则 项 之后 
我们 发现 w2 总体 的 损失 函数 结果 更小 因为 
4 * 0.25 ^ 2 1 于是 我们 的 系统 
会 选择 w2 这也 就 意味着 系统 更 喜欢 权重 
分布 均匀 的 参数 而 不是 某些 特征 权重 明显 
高于 其他 权重 占据 绝对 主导 作用 的 参数 之后 
的 系列 里 会 提到 这样 一个 平滑 的 操作 
实际上 也 会 提高 系统 的 泛化 能力 让其 具备 
更高 的 通用性 而不 至于 在 训练 集上 过拟合 另外 
我们 在 讨论 过拟合 的 这个 部分 的 时候 并 
没有 提到 b 这个 参数 这 是因为 它 并不 具备 
像 W 一样 的 控制 输入 特征 的 某个 维度 
影响力 的 能力 还 需要 说 一下 的 是 因为 
正则 项的/nr 引入 训练 集上 的 准确度 是 会 有 
一定 程度 的 下降 的 我们 永远 也 不 可能 
让 损失 达到 零 了 因为 这 意味着 正则化 项为0/nr 
也 就是 W = 0 下面 是 简单 的 计算 
损失 函数 没 加上 正则化 项 的 代码 有/v 未向/i 
量化/v 和向/nr 量化/v 两种/m 形式/n def L _ i x 
y W 未向 量化 版本 . 对 给定 的 单个 
样本 x y 计算 multiclass svm loss . x 代表 
图片 像素 输入 的 向量 例如 CIFAR 10中 是 3073 
x 1 因为 添加 了 bias 项 对应 的 1 
到 x 中 y 图片 对应 的 类别 编号 比如 
CIFAR 10中 是 0 9 W 权重 矩阵 例如 CIFAR 
10中 是 10 x 3073 delta = 1.0 # 设定 
delta scores = W . dot x # 内积 计算 
得分 correct _ class _ score = scores y D 
= W . shape 0 # 类 别数 例如 10 
loss _ i = 0.0 for j in xrange D 
# 遍历 所有 错误 的 类别 if j = = 
y # 跳过 正确 类别 continue # 对 第 i 
个 样本 累加 loss loss _ i + = max 
0 scores j correct _ class _ score + delta 
return loss _ i def L _ i _ vectorized 
x y W 半 向 量化 的 版本 速度 更快 
之所以 说是 半 向 量化 是 因为 这个 函数 外层 
要用 for 循环 遍历 整个 训练 集 _ | | 
delta = 1.0 scores = W . dot x # 
矩阵 一次性 计算 margins = np . maximum 0 scores 
scores y + delta margins y = 0 loss _ 
i = np . sum margins return loss _ i 
def L X y W 全向 量化 实现 X 包含 
所有 训练样本 中 数据 例如 CIFAR 10 是 3073 x 
50000 y 所有 的 类别 结果 例如 50000 x 1 
的 向量 W 权重 矩阵 例如 10 x 3073 # 
待 完成 . . . 说到 这里 其实 我们 的 
损失 函数 是 提供 给 我们 一个 数值 型 的 
表示 来 衡量 我们 的 预测 结果 和 实际 结果 
的 差别 而 要 提高 预测 的 准确性 要做 的 
事情 是 想办法 最小化 这个 loss 2.4 一些 现实 的 
考虑 点 2 . 4.1 设定 Delta 我们 在 计算 
Multi SVM loss 的 时候 Δ 是 我们 提前 设定 
的 一个 参数 这个 值 咋 设定 莫不是 也 需要 
交叉 验证 其实 基本上 大部分 的 场合 下 我们 设定 
Δ = 1.0 都是/nr 一个 安全 的 设定 我们 看 
公式 中的 参数 Δ 和λ/nr 似乎 是 两个 截然不同 的 
参数 实际上 他俩 做 的 事情 比较 类似 都是 尽量 
让 模型 贴近 标准 预测 结果 的 时候 在 数据 
损失 / data loss 和 正则化 损失 / regularization loss 
之间 做 一个 交换 和 平衡 在 损失 函数 计算公式 
里 可以 看出 权重 W 的 幅度 对 类别 得分 
有最/nr 直接 的 影响 我们 减小 W 最后 的 得分 
就 会 减少 我们 增大 W 最后 的 得分 就 
增大 从 这个 角度 看 Δ 这个 参数 的 设定 
Δ = 1 或者 Δ = 100 其实 无法 限定 
W 的 伸缩 而 真正 可以 做到 这点 的 是 
正则化 项 λ 的 大小 实际上 控制 着 权重 可以 
增长 和 膨胀 的 空间 2 . 4.2 关于 二元 
/ Binary 支持 向量 机 如果 大家 之前 接触 过 
Binary SVM 我们 知道 它 的 公式 如下 我们 可以 
理解 为 类别 yi ∈ − 1 1 它 是 
我们 的 多 类别 识别 的 一个 特殊 情况 而 
这里 的 C 和λ是/nr 一样 的 作用 只不过 他们 的 
大小 对 结果 的 影响 是 相反 的 也 就是 
C ∝ 1 / λ 2 . 4.3 关于 非 
线性 的 SVM 如果 对 机器 学习 有 了解 你 
可能 会 了解 很多 其他 关于 SVM 的 术语 kernel 
dual SMO 算法 等等 在 这个 系列 里面 我们 只 
讨论 最 基本 的 线性 形式 当然 其实 从 本质 
上 来说 这些 方法 都是/nr 类似 的 2.5 Softmax 分类器 
话说 其实 有 两种 特别 常见 的 分类器 前面 提 
的 SVM 是 其中 的 一种 而 另外 一种 就是 
Softmax 分类器 它 有着 截然不同 的 损失 函数 如果 你 
听说 过 逻辑 回归 二 分类器 那么 Softmax 分类器 是 
它 泛化 到 多 分类 的 情形 不像 SVM 这种 
直接 给 类目 打分 f xi W 并 作为 输出 
Softmax 分类器 从新 的 角度 做了 不 一样 的 处理 
我们 依旧 需要 将 输入 的 像素 向量 映射 为 
得分 f x _ i W = W x _ 
i 只不过 我们 还 需要 将 得分 映 射到 概率 
域 我们 也 不再 使用 hinge loss 了 而是 使用 
交叉 熵 损失 / cross entropy loss 形式 如下 我们 
使用 fj 来 代表 得 分向量 f 的 第 j 
个 元素 值 和 前面 提到 的 一样 总体 的 
损失 / loss 也是 Li 遍历 训练 集 之后 的 
均值 再 加上 正则化 项R/nr W 而 函数 被 称之为 
softmax 函数 它 的 输入 值 是 一个 实数 向量 
z 然后 在 指数 域 做 了 一个 归一化 以 
保证 之和 为 1 映射 为 概率 2 . 5.1 
信息论 角度 的 理解 对于 两个 概率分布 p 真实 的 
概率分布 和 估测 的 概率分布 q 估测 的 属于 每个 
类 的 概率 它们 的 互 熵 定义 为 如下 
形式 而 Softmax 分类器 要做 的 事情 就是 要 最小化 
预测 类别 的 概率分布 之前 看到 了 是 与 实际 
类别 概率分布 p = 0 1 0 只在 结 果类 
目上 是 1 其余 都为 0 两个 概率分布 的 交叉 
熵 另外 因为 互 熵 可以用 熵 加上 KL 距离 
/ Kullback Leibler Divergence 也叫 相对 熵 / Relative Entropy 
来 表示 即 而 p 的 熵 为 0 这 
是 一个 确定 事件 无 随机性 所以 互 熵 最小化 
等同于 最小化 两个 分布 之间 的 KL 距离 换句话说 交叉 
熵 想 要从 给定 的 分布 q 上 预测 结果 
分布 p 2 . 5.2 概率 角度 的 理解 我们 
再 来 看看 以下 表达式 其实 可以 看做 给定 图片 
数据 xi 和 类别 yi 以及 参数 W 之后 的 
归一化 概率 在 概率 的 角度 理解 我们 在 做 
的 事情 就是 最小化 错误类别 的 负 log 似 然 
概率 也 可以 理解 为 进行 最大 似 然 估计 
/ Maximum Likelihood Estimation MLE 这个 理解 角度 还有 一个 
好处 这个 时候 我们 的 正则化 项R/nr W 有 很好 
的 解释性 可以 理解 为 整个 损失 函数 在 权重 
矩阵 W 上 的 一个 高斯 先验 所以 其实 这 
时候 是 在做 一个 最大 后验/nr 估计 / Maximum a 
posteriori MAP 2 . 5.3 实际 工程 上 的 注意 
点 数据 稳定性 在 我们 要 写 代码 工程 实现 
Softmax 函数 的 时候 计算 的 中间 项 因为 指数 
运算 可能 变得 非常 大 除法 的 结果 非常 不 
稳定 所以 这里 需要 一个 小 技巧 注意到 如果 我们 
在 分子 分母 前 都 乘以 常数 C 然后 整 
理到 指 数上 我们 会 得到 下面 的 公式 C 
的 取值 由 我们 而定 不 影响 最后 的 结果 
但是 对于 实际 计算 过程 中 的 稳定性 有 很大 
的 帮助 一个 最 常见 的 C 取值 为 这 
表明 我们 应该 平移 向量 f 中的 值 使得 最大值 
为 0 以下 的 代码 是 它 的 一个 实现 
f = np . array 123 456 789 # 3个 
类别 的 预测 示例 p = np . exp f 
/ np . sum np . exp f # 直接 
运算 数值 稳定性 不太好 # 我们 先 对 数据 做 
一个 平移 所以 输入 的 最大 值 为 0 f 
= np . max f # f 变成 666 333 
0 p = np . exp f / np . 
sum np . exp f # 结果 正确 同时 解决 
数值 不 稳定 问题 2 . 5.4 关于 softmax 这个 
名字 的 一点 说明 准确 地 说 SVM 分类器 使用 
hinge loss 有时候 也叫 max margin loss 而 Softmax 分类器 
使用 交叉 熵 损失 / cross entropy loss Softmax 分类器 
从 softmax 函数 恩 其实 做 的 事情 就是 把 
一列 原始 的 类别 得分 归一化 到 一列 和为1/nr 的 
正数 表示 概率 得到 softmax 函数 使得 交叉 熵 损失 
可以 用 起来 而 实际上 我们 并 没有 softmax loss 
这个 概念 因为 softmax 实质上 就是 一个 函数 有时候 我们 
图 方便 就 随口 称呼 softmax loss 2.6 SVM 与 
Softmax 这个 比较 很 有意思 就 像在 用到 分类 算法 
的 时候 就 会想 SVM 还是 logistic regression 呢 一样 
我们 先 用 一张 图 来 表示 从 输入端 到 
分类 结果 SVM/w 和/c Softmax/w 都/d 做了/i 啥/r 区别 就是 
拿 到 原始 像素 数据 映射 得到 的 得分 之后 
的 处理 而 正因为 处理 方式 不同 我们 定义 不同 
的 损失 函数 有 不同 的 优化 方法 2 . 
6.1 另外 的 差别 SVM 下 我们 能 完成 类别 
的 判定 但是 实际上 我们 得到 的 类别 得分 大小 
顺序 表示 着 所属 类别 的 排序 但是 得分 的 
绝对值 大小 并 没有 特别 明显 的 物理 含义 Softmax 
分类器 中 结果 的 绝对值 大小 表征 属于 该 类别 
的 概率 举个 例子 说 SVM 可能 拿 到 对应 
猫 / 狗 / 船 的 得分 12.5 0.6 23.0 
同 一个 问题 Softmax 分类器 拿到 0.9 0.09 0.01 这样在 
SVM 结果 下 我们 只 知道 猫 是 正确 答案 
而在 Softmax 分类器 的 结果 中 我们 可以 知道 属于 
每个 类别 的 概率 但是 Softmax 中 拿到 的 概率 
其实/d 和/c 正则化/i 参数/n λ/i 有/v 很大/a 的/uj 关系/n 因为 
λ 实际上 在 控制 着 W 的 伸缩 程度 所以 
也 控制 着 最后 得分 的 scale 这会 直接 影响 
最后 概率 向量 中 概率 的 分散度 比如说 某个 λ 
下 我们 得到 的 得分 和 概率 可能 如下 而 
我们 加大 λ 提高 其 约束 能力 后 很可能 得分 
变为 原来 的 一半 大小 这时候 如下 因为 λ 的 
不同 使得 最后 得到 的 结果 概率 分散度 有 很大 
的 差别 在 上面 的 结果 中 猫 有着 统治性 
的 概率 大小 而在 下面 的 结果 中 它 和 
船只 的 概率 差距 被 缩小 2 . 6.2 际 
应用 中的 SVM 与 Softmax 分类器 实际 应用 中 两类 
分类器 的 表现 是 相当 的 当然 每个人/i 都有/nr 自己/r 
的/uj 喜好/v 和/c 倾向性/n 习惯 用 某类 分类器 一定 要 
对比 一下 的话 SVM 其实 并不 在乎 每个 类别 得到 
的 绝对 得 分大小 举个 例子 说 我们 现在 对 
三个 类别 算得 的 得分 是 10 2 3 实际 
第一类 是 正确 结果 而 设定 Δ = 1 那么 
10 3 = 7 已经 比 1 要 大 很多 
了 那对/nr SVM 而言 它 觉得 这 已经 是 一个 
很 标准 的 答案 了 完全 满足 要求 了 不 
需要 再 做 其他 事情 了 结果 是 10 100 
100 或者 10 9 9 它 都是 满意 的 然而 
对于 Softmax 而言 不是 这样 的 10 100 100 和 
10 9 9 映 射到 概率 域 计算 得到 的 
交叉 熵 损失 是 有 很大 差别 的 所以 Softmax 
是 一个 永远 不会 满足 的 分类器 在 每个 得分 
计算 到 的 概率 基础 上 它 总是 觉得 可以 
让 概率分布 更 接近 标准 结果 一些 交叉 熵 损失 
更 小 一些 有 兴趣 的话 W 与 得分 预测 
结果 demo 是 一个 可以 手动 调整 和 观察 二维 
数据 上 的 分类 问题 随 W 变化 结果 变化 
的 demo 可以 动手 调调 看看 参考 资料 与 原文 
