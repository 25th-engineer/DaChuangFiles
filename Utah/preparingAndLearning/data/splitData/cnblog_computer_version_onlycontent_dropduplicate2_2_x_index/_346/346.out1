作者 寒 小阳 时间 2016年 1月 出处 http / / 
blog . csdn . net / han _ xiaoyang / 
article / details / 50542880 声明 版权 全部 转载 请 
联系 作者 并 注明 出处 1 . 前言 前面 九 
讲对 神经 网络 的 结构 组件 训练方法 原理 等 做了 
介绍 如今 我们 回到 本 系列 的 核心 计算机 视觉 
神经 网络 中 的 一种 特殊 版本号 在 计算机 视觉 
中 使用 最为 广泛 这 就是 大家 都 知道 的 
卷积 神经网络 卷积 神经 网络 和 普通 的 神经 网络 
一样 由 神经元 按 层级 结构 组成 其间 的 权重 
和 偏移量 都是 可 训练 得到 的 相/v 同是/c 输入/v 
的/uj 数据/n 和权/nr 重做/i 运算/vn 输出 结果 输入 激励 神经元 
输出 结果 从 整体 上 看来 整个 神经 网络 做 
的 事情 依然 是 对于 像素 级别 输入 的 图像 
数据 用 得分 函数 计算 最后 各个 类别 的 得分 
然后 我们 通过 最小化 损失 函数 来 得到 最优 的 
权重 之前 的 博文 中 介绍 的 各种 技巧 和 
训练 方法 以及 注意事项 在 这个 特殊 版本号 的 神经 
网络 上 依然 好使 既然 提到 卷积 神经 网络 了 
我们 就 来说 说 它 的 特殊 之处 首先 这里 
的 卷积 神经网络 一般 假定 输入 就是 图片 数据 也 
正是 由于 输入 是 图片 数据 我们 能够 利用 它 
的 像素 结构 特性 去做 一些 假 设来 简化 神经 
网络 的 训练 复杂度 降低 训练 參 数 个数 2 
. 卷积 神经网 整体 结构 一览 我们 前面 讲过 的 
神经 网络结构 都比 較 一致 输入 层 和 输出 层 
中间 夹着 数层 隐藏 层 每 一层 都由 多个 神经元 
组成 层/q 和层/nr 之间/f 是/v 全/a 连接/v 的/uj 结构/n 同 
一层 的 神经 元 之间 没有 连接 卷积 神经 网络 
是 上述 结构 的 一种 特殊 化 处理 由于 对于 
图像 这种 数据 而言 上面 这种 结构 实际 应用 起来 
有 较大 的 困难 就拿 CIFAR 10 举例 吧 图片 
已经 非常 小了 是 32 * 32 * 3 长宽 
各 32 像素 3个 颜色通道 的 那么 在 神经网络 当中 
我们 仅仅 看 隐藏 层 中 的 一个 神经元 就 
应该 有 32 * 32 * 3 = 3072个 权重 
假设 大家 认为 这个 权重 个数 的 量 还行 的话 
再 设想 一下 当 这 是 一个 包括 多 个 
神经元 的 多层 神经网 假设 n 个 再 比方 图像 
的 质量 好 一点 比方 是 200 * 200 * 
3 的 那 将有 200 * 200 * 3 * 
n = 120000n 个 权重 须要 训练 结果 是 拉着 
这么多 參 数 训练 基本 跑不动 跑得 起来 也是 气喘吁吁 
当然 最 关键 的 是 这么 多 參 数 的 
情况 下 分分钟 模型 就 过拟合 了 别急 别急 一会儿 
我们 会 提到 卷积 神经 网络 的 想法 和 简化 
之处 卷积 神经 网络结构 比 較 固定 的 原因 之中 
的 一个 是 图片 数据 本身 的 合理 结构 类 
图像 结构 200 * 200 * 3 我们 也 把 
卷积 神经 网络 的 神经元 排布 成 width * height 
* depth 的 结构 也 就是说 这 一层 总 共同 
拥有 width * height * depth 个 神经元 例 如以 
下图 所 看到 的 举个 样例 说 CIFAR 10 的 
输出 层 就是 1 * 1 * 10 维 的 
另外 我们 后 面会 说到 每 一层 的 神经元 事实上 
仅仅 和上/nr 一层 里 某些 小区域 进行 连接 而/c 不是/c 
和上/nr 一层/m 每一个/i 神经元/nz 全/a 连接/v 3 . 卷积 神经 
网络 的 组成 层 在 卷积 神经 网络 中 有 
3种 最 基本 的 层 卷积 运算 层 pooling 层 
全 连接 层 一个 完整 的 神经 网络 就是 由 
这三种 层 叠加 组成 的 结构 演示 样例 我们 继续 
拿 CIFAR 10 数据集 举例 一个 典型 的 该 数据 
集上 的 卷积 神经网络 分类器 应该有 INPUT CONV RELU POOL 
FC 的 结构 详细 说来 是 这种 INPUT 32 * 
32 * 3 包括 原始 图片 数据 中 的 全部 
像素 长宽 都是 32 有 RGB 3个 颜色通道 CONV 卷积 
层 中 没 个 神经元 会 和上/nr 一层 的 若干 
小区域 连接 计算 权 重和 小区域 像素 的 内积 举个 
样例 可能 产出 的 结果 数据 是 32 * 32 
* 12 的 RELU 层 就是 神经元 激励 层 基本 
的 计算 就是 max 0 x 结果 数据 依然 是 
32 * 32 * 12 POOLing 层 做 的 事情 
能够 理解 成 一个 下 採 样 可能 得到 的 
结果 维度 就 变为 16 * 16 * 12 了 
全 连接 层 一般 用于 最后 计算 类别 得分 得到 
的 结果 为 1 * 1 * 10 的 当中 
的 10 相应 10个 不同 的 类别 和 名字 一样 
这/r 一层/m 的/uj 全部/n 神经元/nz 会/v 和上/nr 一层/m 的/uj 全部/n 
神经元/nz 有/v 连接/v 这样 卷积 神经网络 作为 一个 中间 的 
通道 就 一步步 把 原始 的 图像 数据 转成 最后 
的 类别 得 分了 有 一个 点 我们 要 提 
一下 刚才 说 到了 有 几种 不同 的 神经 网络层 
当中 有 一些 层 是 有待 训练 參 数 的 
另外 一些 没有 详细 一点 说 卷积/n 层/q 和全/nr 连接/v 
层/q 包括/v 权/n 重和/i 偏移/v 的/uj 而 RELU 和 POOLing 
层 仅仅 是 一个 固定 的 函数 运算 是 不包括 
权 重和 偏移 參 数 的 只是 POOLing 层 包括 
了 我们 手动 指定 的 超 參 数 这个 我们 
之后 会 提到 总结 一下 一个 卷积 神经网络 由 多种 
不同 类型 的 层 卷 几层 / 全 连接 层 
/ RELU 层 / POOLing 层 等 叠加 而成 每 
一层 的 输入 结构 是 3 维 的 数据 计算 
完 输出 依然 是 3 维 的 数据 卷积/n 层/q 
和全/nr 连接/v 层/q 包括/v 训练/vn 參/zg 数/n RELU 和 POOLing 
层 不包括 卷积 层 全/a 连接/v 层/q 和/c POOLing/w 层/q 
包括/v 超/v 參/zg 数/n RELU 层 没有 下图 为 CIFAR 
10 数据集 构建 的 一个 卷积 神经 网络结构 示意图 既然 
有 这么 多 不同 的 层级 结构 那 我们 就 
展开来 讲讲 3.1 卷积 层 说 起来 这是 卷积 神经 
网络 的 核心层 从 名字 就 能够 看出来 对吧 _ 
| | 3 . 1.1 卷积 层 综述 直观 看来 
卷积 层 的 參 数 事实上 能够 看做 一 系列 
的 可 训练 / 学习 的 过滤器 在前 向 计算 
过程 中 我们 输入 一定 区域 大小 width * height 
的 数据 和/c 过滤器/n 点乘/i 后/f 等到/v 新的/i 二维/m 数据/n 
然后 滑过 一个个 滤波器 组成 新的 3 维 输出 数据 
而/c 我们/r 能够/v 理解/v 成每/nr 一个/m 过滤器/n 都/d 仅仅/d 关心/n 
过滤/v 数据/n 小平面/n 内/n 的/uj 部分/n 特征/n 当 出现 它 
学习 到 的 特征 的 时候 就会 呈现 激活 / 
activate 态 局部 关联度 这是 卷积 神经 网络 的 独特 
之处 当中 之中 的 一个 我们 知道 在 高维 数据 
比方 图片 中 用 全 连接 的 神经 网络 实际 
project 中 基本 是 不 可行 的 卷积 神经 网络 
中 每 一层 的 神经元 仅仅 会 和上/nr 一层 的 
一些 局部 区域 相连 这 就是 所谓 的 局部 连接性 
你 能够 想象 成 上 一层 的 数据 区 有 
一个 滑动 的 窗体 仅仅/d 有/v 这个/r 窗/s 体内/s 的/uj 
数据/n 会/v 和/c 下一层/i 神经元/nz 有/v 关联/ns 当然 这个 做法 
就 要求 我们 手动 敲定 一个 超 參 数 窗体 
大小 通常 情况下 这个 窗体 的 长 和宽是/nr 相等 的 
我们 把 长 x 宽 叫做 receptive field 实际 的 
计算 中 这个 窗体 是 会 滑动 的 会 近似 
覆盖 图片 的 全部 小区域 举个 实例 CIFAR 10中 的 
图片 输入 数据 为 32 * 32 * 3 的 
假设 我们 把 receptive field 设为 5 * 5 那 
receptive field 的 data 都会 和下/nr 一层 的 神经元 关联 
所以 共同 拥有 5 * 5 * 3 = 75个 
权重 注意 到 最后 的 3 依然 代表 着 RGB 
3个 颜色通道 假设 不是 输入 数据 层 中间层 的 data 
格式 可能 是 16 * 16 * 20 的 假如 
我们 取 3 * 3 的 receptive field 那 单个 
神经元 的 权重 为 3 * 3 * 20 = 
180 局部 关联 细节 我们 刚才 说到 卷积 层 的 
局部 关联 问题 这个 地方 有 一个 receptive field 也 
就是 我们 直观 理解 上 的 滑动 数据 窗体 从 
输入 的 数据 到 输出 数据 有 三个 超 參 
数 会 决定 输出 数据 的 维度 各自 是 深度 
/ depth 步长 / stride 和 填 充值 / zero 
padding 所谓 深度 / depth 简单 说来 指 的 就是 
卷积 层 中和 上 一层 同一 个 输入 区域 连接 
的 神经元 个数 这部分 神经元 会在 遇到 输入 中的 不同 
feature 时 呈现 activate 状态 举个 样例 假设 这是 第一个 
卷积 层 那 输入 到 它 的 数据 实际上 是 
像素 值 不同 的 神经元 可能 对 图像 的 边缘 
轮廓 或者 颜色 会 敏感 所谓 步长 / stride 是 
指 的 窗体 从 当前 位置 到下 一个 位置 跳过 
的 中间 数据 个数 比 方从 图像 数据 层 输入 
到 卷积 层 的 情况 下 或许 窗体 初始 位置 
在 第 1个 像素 第二 个 位置 在 第 5个 
像素 那么 stride = 5 1 = 4 . 所谓 
zero padding 是 在 原始 数据 的 周边 补上 0 
值 的 圈数 以 下第 2张 图中 的 样子 这么 
解释 可能 理解 起来 还是 会 有困难 我们 找 两张 
图 来 相应 一下 这 三 个量 这 是 解决 
ImageNet 分类 问题 用到 的 卷积 神经 网络 的 一部分 
我们 看到 卷积 层 直接 和最/nr 前面 的 图像 层 
连接 图像 层 的 维度 为 227 * 227 * 
3 而 receptive field 设为 11 * 11 图上 未标明 
可是 滑动 窗体 的 步长 stride 设为 4 深度 depth 
为 48 + 48 = 96 这是 双 GPU 并行 
设置 边缘 没有 补 0 因此 zero padding 为 0 
因此 窗体 滑完/nr 一行 总共 停留 次数 为 data _ 
len receptive _ field _ len + 2 * zero 
padding / stride + 1 = 227 11 + 2 
* 0 / 4 + 1 = 55 由于 图像 
的 长宽 相等 因此 纵向 窗体 数 也是 55 最后 
得到 的 输出 数据 维度 为 55 * 55 * 
96 维 这是 一张 动态 的 卷积 层 计 算图 
图上 的 zero padding 为 1 所以 大家 能够 看到 
数据 左右 各 补了 一行 0 窗体 的 长宽 为 
3 滑动 步长 stride 为 2 关于 zero padding 补 
0 这个 操作 产生 的 根本 原因 是 为了 保证 
窗体 的 滑动 能 从头 刚好 到尾 举个 样例 说 
上 2 图中 的 上面 一幅 图 由于 data _ 
len receptive _ field _ len + 2 * zero 
padding / stride 刚好 能够 整除 所以 窗体 左側/nr 贴着 
数据 開始 位置 滑到 尾部 刚好 窗体 右 側 能够 
贴着 数据 尾部 位置 因此 是 不 须要 补 0 
的 而在 以下 那幅 图中 假设 滑动 步长 设为 4 
你 会 发现 第一 次 计算 之后 窗体 就 无法 
滑动 了 而 尾部 的 数据 是 没有 被 窗体 
看到过 的 因此 补 0 能够 解决 问题 关于 窗体 
滑动 步长 大家 能够 发现 一点 窗体 滑动 步长 设定 
越小 两次 滑动 取得 的 数据 重叠 部分 越多 可是 
窗体 停留 的 次数 也 会 越多 运算 律 大一些 
窗体 滑动 步长 设定 越长 两次 滑动 取得 的 数据 
重叠 部分 越少 窗体 停留 次数 也 越少 运算 量小 
可是 从 一定 程度 上 说 数据 信息 不如 上面 
丰富 了 3 . 1.2 卷积 层 的 參 数 
共享 首先 得 说 卷积 层 的 參 数 共享 
是 一个 非常 赞 的 处理 方式 它 使得 卷积 
神经 网络 的 训练 计算 复杂度 和參数/nr 个数 降低 非常 
非常 多 就拿 实际 解决 ImageNet 分类 问题 的 卷积 
神经 网络结构 来说 我们 知道 输出 结果 有 55 * 
55 * 96 = 290400个 神经元 而 每 一个 神经元 
由于 和窗/nr 体内 数据 的 连接 有/v 11/m */i 11/m 
*/i 3/m =/i 363个/mq 权/n 重和/i 1个/mq 偏移量/n 所以 总 
共同 拥有 290400 * 364 = 105705600个 权重 然后 恩 
训练 要 累 挂了 因此 我们 做 了 一个 大胆 
的 假设 我们 刚才 提到 了 每一个 神经元 能够 看做 
一个 filter 对 图片 中 的 数据 窗 区域 做 
过滤 那 既然 是 filter 我们 干脆 就 假设 这个 
神经元 用于 连接 数据 窗 的 权重 是 固定 的 
这 意味着 对 同一 个 神经元 而言 不论 上 一层 
数据 窗体 停留 在 哪个 位置 连接 两者 之间 的 
权重 都是 同 一组 数 那 代表 着 上面 的 
样例 中的 卷积 层 我们 仅仅 须要 神经元 个数 * 
数据 窗体 维度 = 96 * 11 * 11 * 
3 = 34848个 权重 假设 相应 每一个 神经元 的 权重 
是 固定 的 那么 整个 计算 的 过程 就 能够 
看做 一组 固定 的 权重 和 不同 的 数据 窗体 
数据 做 内积 的 过程 这在 数学上 刚好 相应 卷积 
操作 这 也 就是 卷积 神经网 的 名字 来源 另外 
由于 每 一个 神经元 的 权重 固定 它 能够 看做 
一个 恒定 的 filter 比方 上面 96个 神经元 作为 filter 
可视化 之后 是 例如 以下 的 样子 须要 说明 的 
一点 是 參 数 共享 这个 策略 并非 每 一个 
场景 下 都 合适 的 有 一些 特定 的 场合 
我们 不能 把 图片 上 的 这些 窗体 数据 都 
视作 作用 等同 的 一个 非常 典型 的 样例 就是 
人脸识别 一般人 的 面部 都 集中 在 图像 的 中央 
因此 我们 希望 数据 窗体 滑过 这块 区域 的 时候 
权重 和 其它 边缘 区域 是 不同 的 我们 有 
一种 特殊 的 层 相应 这种 功能 叫做 局部 连接 
层 / Locally Connected Layer3 . 1.3 卷积 层 的 
简单 numpy 实现 我们 假定 输入 到 卷积 层 的 
数据 为 X 增加 X 的 维度 为 X . 
shape 11 11 4 假定 我们 的 zero padding 为 
0 也 就是 左 右上 下不 补充 0 数据 数据 
窗体 大小 为 5 窗体 滑动 步 长为 2 那 
输出 数据 的 长宽 应该 为 11 5 / 2 
+ 1 = 4 假定/v 第一/m 个/q 神经元/nz 相应/v 的/uj 
权重/n 和/c 偏移量/n 分别为/i W0/i 和/c b0/i 那/r 我们/r 就/d 
能/v 算得/v 在 第一 行 数据 窗体 停留 的 4个 
位置 得到 的 结果 值 分别为 V 0 0 0 
= np . sum X 5 5 * W0 + 
b0V 1 0 0 = np . sum X 2 
7 5 * W0 + b0V 2 0 0 = 
np . sum X 4 9 5 * W0 + 
b0V 3 0 0 = np . sum X 6 
11 5 * W0 + b0 注意 上述 计算 过程 
中 * 运算符 是 对 两个 向量 进行 点乘 的 
因此 W0 应该 维度 为 5 5 4 相同 你 
能够 计算 其它 位置 的 计算 输出 值 V 0 
0 1 = np . sum X 5 5 * 
W1 + b1V 1 0 1 = np . sum 
X 2 7 5 * W1 + b1V 2 0 
1 = np . sum X 4 9 5 * 
W1 + b1V 3 0 1 = np . sum 
X 6 11 5 * W1 + b1 每一个 神经元 
相应 不同 的 一组 W 和b/nr 在 每 一个 数据 
窗体 停留 的 位置 得到 一个 输出 值 我们 之前 
提到 了 卷积 层 在做 的 事情 是/v 不断/d 做/v 
权/n 重和/i 窗体/n 数据/n 的/uj 点乘/i 和/c 求和/v 因此 我们 
也 能够 把 这个 过程 整理 成 一个 大 的 
矩阵 乘法 看看 数据 端 我们 能够 做 一个 操作 
im2col 将 数据 转成 一个 可 直接 供 神经元 filter 
计算 的 大 矩阵 举个 样例 说 输入 是 227 
* 227 * 3 的 图片 而 神经元 权 重为 
11 * 11 * 3 同一 时候 窗体 移动 步 
长为 4 那 我们 知道 数据 窗体 滑动 过程 中 
总共 产生 227 11 / 4 + 1 * 227 
11 / 4 + 1 = 55 * 55 = 
3025个 局部 数据 区域 又 每 一个 区域 包括 11 
* 11 * 3 = 363个 数据 值 因此/c 我们/r 
想/v 办法/n 把/p 原始数据/n 反复/v 和/c 扩充/v 成/n 一个/m 363 
* 3025 的 数据 矩阵 X _ col 就 能够 
直接 和 filter 进 行运 算了 对于 filter 端 卷积 
层 假如 厚度 为 96 有/v 96个/mq 不同/a 权/n 重组/vn 
的/uj filter/w 每一个 filter 的 权重 为 11 * 11 
* 3 因此 filter 矩阵 W _ row 维度 为 
96 * 363 在 得到 上述 两个 矩阵 后 我们 
的 输出 结果 即 能够 通过 np . dot W 
_ row X _ col 计算 得到 结果 数据 为 
96 * 3025 维 的 这个 实现 的 弊端 是 
由于 数据 窗体 的 滑动 过程 中 有 重叠 因此 
我们 出现 了 非常 多 反复 数据 占用 内存 较大 
优点 是 实际 计算 过程 非常 easy 假设 我们 用 
相似 BLAS 这种 库 计算 将 非常 迅速 另外 在 
反向 传播 过程 中 事实上 卷积 相应 的 操作 还是 
卷积 因此 实现 起来 也 非常 方便 3.2 Pooling 层 
简单 说来 在 卷积 神经 网络 中 Pooling 层 是 
夹在 连续 的 卷积 层 中间 的 层 它 的 
作用 也 非常 easy 就是 * * 逐步 地 压缩 
/ 降低 数据 和參数/nr 的 量 也 在 一定 程度 
上 减小 过拟合 的 现象 * * Pooling 层 做 
的 操作 也 非常 easy 就是 将 原 数据 上 
的 区域 压缩 成 一个 值 区域 最大值 / MAX 
或者 平均值 / AVERAGE 最 常见 的 Pooling 设定 是 
将 原 数据 切成 2 * 2 的 小块 每块 
里面 取 最大值 作为 输出 这样 我们 就 自然而然 降低 
了 75% 的 数据 量 须要 提到 的 是 除掉 
MAX 和 AVERAGE 的 Pooling 方式 事实上 我们 也 能够 
设定 别的 pooling 方式 比方 L2 范数 pooling 说 起来 
历史 上 average pooling 用 的 非常 多 可是 近些年 
热度 降 了 不少 project 师们在/nr 实践 中 发现 max 
pooling 的 效果 相对 好 一些 一个 对 Pooling 层 
和它的/nr 操作 直观 理解 的 示意 图为 上 图为 Pooling 
层 的 一个 直观 演示 样例 相当于 对 厚度 为 
64 的 data 每一个 切片 做 了 一个 下 採 
样 下图 为 Pooling 操作 的 实际 max 操作 Pooling 
层 假定 是 MAX Pooling 在 反向 传播 中 的 
计算 也是 非常 easy 的 大家 都 知道 怎样 去求 
max x y 函数 的 偏 导 也是 分段 的 
3.3 归一化 层 Normalization Layer 卷积 神经网络 里面 有时候 会 
用到 各种各样 的 归一化 层 尤其 是 早期 的 研究 
常常 能 见到 它们 的 身影 只是 近些年来 的 研究 
表明 似乎 这个 层级 对 最后 结果 的 帮助 非常 
小 所以 后来 大多数 时候 就 干脆 拿 掉了 3.4 
全 连接 层 这 是 我们 在 介绍 神经 网络 
的 时候 最 标准 的 形式 不论什么/l 神经/n 元和/n 上/f 
一层/m 的/uj 不论什么/l 神经元/nz 之间/f 都有/nr 关联/ns 然后 矩阵 运算 
也 非常 easy 和 直接 如今 的 非常 多 卷积 
神经 网络结构 末 层 会 採 用 全连 接去 学习 
很多 其它 的 信息 4 . 搭建 卷积 神经网 结构 
从 上面 的 内容 我们 知道 卷积 神经网络 一般 由 
3种 层 搭建 而成 卷积 层 POOLing 层 我们 直接 
指定 用 MAX Pooling 和全/nr 连接/v 层/q 然后 我们 一般 
选用 最 常见 的 神经元 ReLU 我们 来 看看 有 
这些 组件 之后 怎么 拼 出 一个 合理 的 卷积 
神经网 4.1 层 和层/nr 怎么 排 最 常见 的 组合 
方式 是 用 ReLU 神经元 的 卷积 层 组 一个 
神经 网络 同一 时候 在 卷积 层 和 卷积 层 
之间 插入 Pooling 层 经过 多次 的 卷积 层 = 
Pooling 层 叠加 之后 数据 的 整体 量级 就 不大 
了 这个 时候 我们 能够 放 一层 全 连接 层 
然后/c 最后/f 一层/m 和/c output/w 层/q 之间/f 是/v 一个/m 全/a 
连接/v 层/q 所以 总结 一下 最 常见 的 卷积 神经网 
结构 为 输入 层 = ReLU 卷积 层 * N 
= Pooling 层 * M = ReLU 全 连接 层 
* K = 全 连接 层 解释一下 当中 \ * 
操作 代表 能够 叠加 非常 多层 而 Pooling 层 表示 
Pooling 层 事实上 是 可选 的 可有可无 N 和M是/nr 详细 
层数 比方说 输入 层 ReLU 卷积 层 = ReLU 卷积 
层 = Pooling 层 * 3 ReLU 全 连接 层 
* 2 全 连接 层 就是 一个 合理 的 深层 
的 卷积 神经网 在 相同 的 视野 范围内 选择 多层 
叠加 的 卷积 层 而 不是 一个 大 的 卷积 
层 这句话 非常 拗口 但这 是 实际 设计 卷积 神经网络 
时候 的 经验 我们 找个 样例 来 解释 一下 这 
句话 假设 你 设计 的 卷积 神经网 在 数据 层 
有 3层 连续 的 卷积 层 同一 时候 每 一层 
滑动 数据 窗体 为 3 * 3 第一层 每 一个 
神经元 能够 同一 时候 看到 3 * 3 的 原始数据 
层 那 第二层 每一个 神经元 能够 间接 看到 1 + 
3 + 1 * 1 + 3 + 1 = 
5 * 5 的 数据 层 内容 第三层 每 一个 
神经元 能够 间接 看到 1 + 5 + 1 * 
1 + 5 + 1 = 7 * 7 的 
数据 层 内容 那从最/nr 表层/n 看/v 还 不如 直接 设定 
滑动 数据 窗体 为 7 * 7 的 为啥 要 
这么 设计 呢 我们 来 分析 一下 优劣 尽管 第三 
层 对 数据 层 的 视野 范围 是 一致 的 
可是 单层 卷积 层 加 7 * 7 的 上层 
滑动 数据 窗体 结果 是 这 7个 位置 的 数据 
都是 线性组合 后 得到 最后 结果 的 而 3层 卷积 
层 加 3 * 3 的 滑动 数据 窗体 得到 
的 结果 是 原 数据 上 7 * 7 的 
视野 内 数据 多层 非 线性组合 因此 这种 特征 也 
会 具备 更高 的 表达 能力 假设 我们 假设 全部 
层 的 厚度 / channel 数 是 一致 的 为 
C 那7*/nr 7 的 卷积 层 会 得到 C × 
7 × 7 × C = 49C2 个 參 数 
而 3层 叠加 的 3 * 3 卷积 层 仅仅有 
3 × C × 3 × 3 × C = 
27C2 个 參 数 在 计算 量 上 后者 显然是 
有 优势 的 同上 一点 我们 知道 为了 反向 传播 
方便 实际 计算 过程 中 我们/r 会/v 在前/i 向/p 计算/v 
时/n 保留/v 非常多/i 中间/f 梯度/n 3层 叠加 的 3 * 
3 卷积 层 须要 保持 的 中间 梯度 要 小于 
前 一种 情况 这在 project 实现 上 是 非常 有 
优点 的 4.2 层 大小 的 设定 话说 层级 结构 
确定 了 也得 知道 每 一层 大概 什么 规模 啊 
如今 我们 就 来 聊聊 这个 说 起来 每 一层 
的 大小 神经元 个数 和 排布 并 没有 严格 的 
数字 规则 可是/c 我们/r 有/v 一些/m 通用/v 的/uj project/w 实践/v 
经验/n 和/c 系数/n 对于 输入 层 图像 层 我们 一般 
把 数据 归一 化成 2 的 次方 的 长宽 像素 
值 比方 CIFAR 10 是 32 * 32 * 3 
STL 10 数据集 是 64 * 64 * 3 而 
ImageNet 是 224 * 224 * 3 或者 512 * 
512 * 3 卷积 层 一般 会 把 每一个 滤 
子 / filter / 神经元 相应 的 上层 滑动 数据 
窗体 设为 3 * 3 或者 5 * 5 滑动 
步长 stride 设为 1 project 实践 结果表明 stride 设为 1 
尽管 比 較 密集 可是 效果 比 較 好 步长 
拉 太大 easy 损失 太多 信息 zero padding 就 不用 
了 Pooling 层 一般 採 用 max pooling 同一 时候 
设定 採 样 窗体 为 2 * 2 偶尔 会 
见到 设定 更大 的 採 样 窗体 可是那 意味着 损失 
掉 比 較 多 的 信息 了 比 較 重要 
的 是 我们 得 预估 一下 内存 然后 依据 内存 
的 情况 去 设定 合理 的 值 我们 举个 样例 
在 ImageNet 分类 问题 中 图片 是 224 * 224 
* 3 的 我们 跟 在 数据 层 后面 3个 
3 * 3 视野 窗 的 卷积 层 每 一层 
64个 filter / 神经元 把 padding 设为 1 那么 最后 
每一个 卷积 层 的 output 都是 224 * 224 * 
64 大概 须要 1000 万次 对 output 的 激励 计算 
非线性 activation 大概 花费 72MB 内存 而 project 实践 里 
一般 训练 都在 GPU 上 进行 GPU 的 内存 比 
CPU 要 吃紧 的 多 所以 或许 我们 要 略微 
调动 一下 參 数 比方 AlexNet 用 的 是 11 
* 11 的 的 视野 窗 滑动 步 长为 4 
4.3 典型 的 工业 界 在用 卷积 神经网络 几个 有名 
的 卷积 神经网络 例如 以下 LeNet 这是 最早 用 起来 
的 卷积 神经网络 Yann LeCun 在 论文 LeNet 提到 AlexNet 
2012 ILSVRC 比赛 远超 第 2名 的 卷积 神经网络 和 
LeNet 的 结构 比 較 像 仅仅 是 更深 同一 
时候 用 多层 小 卷积 层 叠加 提到 大 卷积 
层 ZF Net 2013 ILSVRC 比赛 冠军 能够 參 考 
论文 ZF NetGoogLeNet 2014 ILSVRC 比赛 冠军 Google 发表 的 
论文 Going Deeper with Convolutions 有 详细 介绍 VGGNet 也是 
2014 ILSVRC 比赛 中 的 模型 有意思 的 是 即使 
这个 模型 当时 在 分类 问题 上 的 效果 略 
差 于 google 的 GoogLeNet 可是 在 非常 多 图像 
转化 学习 问题 比方 object detection 上 效果 奇好 它 
也 证明 卷积 神经网 的 深度 对于 最后 的 效果 
有 至关 关键 的 数据 预 训 练好 的 模型 
在 pretrained model site 能够 下载 详细 一点 说来 VGGNet 
的 层级 结构 和 花费 的 内存 例如 以下 INPUT 
224x224x3 memory 224 * 224 * 3 = 150K weights 
0 CONV3 64 224x224x64 memory 224 * 224 * 64 
= 3.2 M weights 3 * 3 * 3 * 
64 = 1 728 CONV3 64 224x224x64 memory 224 * 
224 * 64 = 3.2 M weights 3 * 3 
* 64 * 64 = 36 864 POOL2 112x112x64 memory 
112 * 112 * 64 = 800K weights 0 CONV3 
128 112x112x128 memory 112 * 112 * 128 = 1.6 
M weights 3 * 3 * 64 * 128 = 
73 728 CONV3 128 112x112x128 memory 112 * 112 * 
128 = 1.6 M weights 3 * 3 * 128 
* 128 = 147 456 POOL2 56x56x128 memory 56 * 
56 * 128 = 400K weights 0 CONV3 256 56x56x256 
memory 56 * 56 * 256 = 800K weights 3 
* 3 * 128 * 256 = 294 912 CONV3 
256 56x56x256 memory 56 * 56 * 256 = 800K 
weights 3 * 3 * 256 * 256 = 589 
824 CONV3 256 56x56x256 memory 56 * 56 * 256 
= 800K weights 3 * 3 * 256 * 256 
= 589 824 POOL2 28x28x256 memory 28 * 28 * 
256 = 200K weights 0 CONV3 512 28x28x512 memory 28 
* 28 * 512 = 400K weights 3 * 3 
* 256 * 512 = 1 179 648 CONV3 512 
28x28x512 memory 28 * 28 * 512 = 400K weights 
3 * 3 * 512 * 512 = 2 359 
296 CONV3 512 28x28x512 memory 28 * 28 * 512 
= 400K weights 3 * 3 * 512 * 512 
= 2 359 296 POOL2 14x14x512 memory 14 * 14 
* 512 = 100K weights 0 CONV3 512 14x14x512 memory 
14 * 14 * 512 = 100K weights 3 * 
3 * 512 * 512 = 2 359 296 CONV3 
512 14x14x512 memory 14 * 14 * 512 = 100K 
weights 3 * 3 * 512 * 512 = 2 
359 296 CONV3 512 14x14x512 memory 14 * 14 * 
512 = 100K weights 3 * 3 * 512 * 
512 = 2 359 296 POOL2 7x7x512 memory 7 * 
7 * 512 = 25K weights 0 FC 1x1x4096 memory 
4096 weights 7 * 7 * 512 * 4096 = 
102 760 448 FC 1x1x4096 memory 4096 weights 4096 * 
4096 = 16 777 216 FC 1x1x1000 memory 1000 weights 
4096 * 1000 = 4 096 000 TOTAL memory 24M 
* 4 bytes ~ = 93MB / image only forward 
~ * 2 for bwd TOTAL params 138M parameters 有意思 
的 是 大家 会 注意 到 在 VGGNet 这样 一个 
神经 网络 里 大多数 的 内存 消耗 在 前面 的 
卷积 层 而 大多数 须要 训练 的 參 数 却 
集中 在 最后 的 全 连接 层 比方 上上 面的 
样例 里 全/a 连接/v 层/q 有/v 1亿/mq 权重/n 參/zg 数/n 
总共 神经 网里 也就 1.4亿 权重 參 数 4.4 考虑 
点 组 一个 实际 可用 的 卷积 神经网络 最大 的 
瓶颈 是 GPU 的 内存 毕竟 如今 非常多 GPU 仅仅有 
3/4 / 6GB 的 内存 最大 的 GPU 也就 12G 
内存 所以 我们 应该 在 设计 卷积 神经网 的 时候 
多 加 考虑 非常大 的 一部分 内存 开销 来源于 卷积 
层 的 激励函数 个数 和 保存 的 梯度 数量 保存 
的 权重 參 数 也是 内存 的 主要 消耗 处 
包括 反向 传播 要 用到 的 梯度 以及 你 用 
momentum Adagrad or RMSProp 这些 算法 时候 的 中间 存储 
值 数据 batch 以及 其它 的 相似 版本号 信息 或者 
来源 信息 等 也会 消耗 一部分 内存 5 . 很多 
其它 的 卷积 神经网络 參 考 资料 DeepLearning . net 
tutorial 是 一个 用 Theano 完整 实现 卷积 神经网 的 
教程 cuda convnet2 是 多 GPU 并行 化 的 实现 
ConvNetJS CIFAR 10 demo 同意 你 手动 设定 參 数 
然后 直接 在 浏览器 看 卷积 神经 网络 的 结果 
Caffe 主流 卷积 神经网络 开源 库 之中 的 一个 Example 
Torch 7 ConvNet 在 CIFAR 10 上 错误率 仅仅有 7% 
的 卷积 神经 网络 实现 Ben Graham s Sparse ConvNet 
CIFAR 10 上 错误率 仅仅有 4% 的 实现 Face recognition 
for right whales using deep learning Kaggle 看图 识别 濒临灭绝 
右 鲸 比赛 的 冠军 队伍 卷积 神经网络 參 考 
资料 与 原文 cs231n 卷积 神经网络 作者 寒 小阳 时间 2016年 1月 出处 http / / 
blog . csdn . net / han _ xiaoyang / 
article / details / 50542880 声明 版权 全部 转载 请 
联系 作者 并 注明 出处 1 . 前言 前面 九 
讲对 神经 网络 的 结构 组件 训练方法 原理 等 做了 
介绍 如今 我们 回到 本 系列 的 核心 计算机 视觉 
神经 网络 中 的 一种 特殊 版本号 在 计算机 视觉 
中 使用 最为 广泛 这 就是 大家 都 知道 的 
卷积 神经网络 卷积 神经 网络 和 普通 的 神经 网络 
一样 由 神经元 按 层级 结构 组成 其间 的 权重 
和 偏移量 都是 可 训练 得到 的 相/v 同是/c 输入/v 
的/uj 数据/n 和权/nr 重做/i 运算/vn 输出 结果 输入 激励 神经元 
输出 结果 从 整体 上 看来 整个 神经 网络 做 
的 事情 依然 是 对于 像素 级别 输入 的 图像 
数据 用 得分 函数 计算 最后 各个 类别 的 得分 
然后 我们 通过 最小化 损失 函数 来 得到 最优 的 
权重 之前 的 博文 中 介绍 的 各种 技巧 和 
训练 方法 以及 注意事项 在 这个 特殊 版本号 的 神经 
网络 上 依然 好使 既然 提到 卷积 神经 网络 了 
我们 就 来说 说 它 的 特殊 之处 首先 这里 
的 卷积 神经网络 一般 假定 输入 就是 图片 数据 也 
正是 由于 输入 是 图片 数据 我们 能够 利用 它 
的 像素 结构 特性 去做 一些 假 设来 简化 神经 
网络 的 训练 复杂度 降低 训练 參 数 个数 2 
. 卷积 神经网 整体 结构 一览 我们 前面 讲过 的 
神经 网络结构 都比 較 一致 输入 层 和 输出 层 
中间 夹着 数层 隐藏 层 每 一层 都由 多个 神经元 
组成 层/q 和层/nr 之间/f 是/v 全/a 连接/v 的/uj 结构/n 同 
一层 的 神经 元 之间 没有 连接 卷积 神经 网络 
是 上述 结构 的 一种 特殊 化 处理 由于 对于 
图像 这种 数据 而言 上面 这种 结构 实际 应用 起来 
有 较大 的 困难 就拿 CIFAR 10 举例 吧 图片 
已经 非常 小了 是 32 * 32 * 3 长宽 
各 32 像素 3个 颜色通道 的 那么 在 神经网络 当中 
我们 仅仅 看 隐藏 层 中 的 一个 神经元 就 
应该 有 32 * 32 * 3 = 3072个 权重 
假设 大家 认为 这个 权重 个数 的 量 还行 的话 
再 设想 一下 当 这 是 一个 包括 多 个 
神经元 的 多层 神经网 假设 n 个 再 比方 图像 
的 质量 好 一点 比方 是 200 * 200 * 
3 的 那 将有 200 * 200 * 3 * 
n = 120000n 个 权重 须要 训练 结果 是 拉着 
这么多 參 数 训练 基本 跑不动 跑得 起来 也是 气喘吁吁 
当然 最 关键 的 是 这么 多 參 数 的 
情况 下 分分钟 模型 就 过拟合 了 别急 别急 一会儿 
我们 会 提到 卷积 神经 网络 的 想法 和 简化 
之处 卷积 神经 网络结构 比 較 固定 的 原因 之中 
的 一个 是 图片 数据 本身 的 合理 结构 类 
图像 结构 200 * 200 * 3 我们 也 把 
卷积 神经 网络 的 神经元 排布 成 width * height 
* depth 的 结构 也 就是说 这 一层 总 共同 
拥有 width * height * depth 个 神经元 例 如以 
下图 所 看到 的 举个 样例 说 CIFAR 10 的 
输出 层 就是 1 * 1 * 10 维 的 
另外 我们 后 面会 说到 每 一层 的 神经元 事实上 
仅仅 和上/nr 一层 里 某些 小区域 进行 连接 而/c 不是/c 
和上/nr 一层/m 每一个/i 神经元/nz 全/a 连接/v 3 . 卷积 神经 
网络 的 组成 层 在 卷积 神经 网络 中 有 
3种 最 基本 的 层 卷积 运算 层 pooling 层 
全 连接 层 一个 完整 的 神经 网络 就是 由 
这三种 层 叠加 组成 的 结构 演示 样例 我们 继续 
拿 CIFAR 10 数据集 举例 一个 典型 的 该 数据 
集上 的 卷积 神经网络 分类器 应该有 INPUT CONV RELU POOL 
FC 的 结构 详细 说来 是 这种 INPUT 32 * 
32 * 3 包括 原始 图片 数据 中 的 全部 
像素 长宽 都是 32 有 RGB 3个 颜色通道 CONV 卷积 
层 中 没 个 神经元 会 和上/nr 一层 的 若干 
小区域 连接 计算 权 重和 小区域 像素 的 内积 举个 
样例 可能 产出 的 结果 数据 是 32 * 32 
* 12 的 RELU 层 就是 神经元 激励 层 基本 
的 计算 就是 max 0 x 结果 数据 依然 是 
32 * 32 * 12 POOLing 层 做 的 事情 
能够 理解 成 一个 下 採 样 可能 得到 的 
结果 维度 就 变为 16 * 16 * 12 了 
全 连接 层 一般 用于 最后 计算 类别 得分 得到 
的 结果 为 1 * 1 * 10 的 当中 
的 10 相应 10个 不同 的 类别 和 名字 一样 
这/r 一层/m 的/uj 全部/n 神经元/nz 会/v 和上/nr 一层/m 的/uj 全部/n 
神经元/nz 有/v 连接/v 这样 卷积 神经网络 作为 一个 中间 的 
通道 就 一步步 把 原始 的 图像 数据 转成 最后 
的 类别 得 分了 有 一个 点 我们 要 提 
一下 刚才 说 到了 有 几种 不同 的 神经 网络层 
当中 有 一些 层 是 有待 训练 參 数 的 
另外 一些 没有 详细 一点 说 卷积/n 层/q 和全/nr 连接/v 
层/q 包括/v 权/n 重和/i 偏移/v 的/uj 而 RELU 和 POOLing 
层 仅仅 是 一个 固定 的 函数 运算 是 不包括 
权 重和 偏移 參 数 的 只是 POOLing 层 包括 
了 我们 手动 指定 的 超 參 数 这个 我们 
之后 会 提到 总结 一下 一个 卷积 神经网络 由 多种 
不同 类型 的 层 卷 几层 / 全 连接 层 
/ RELU 层 / POOLing 层 等 叠加 而成 每 
一层 的 输入 结构 是 3 维 的 数据 计算 
完 输出 依然 是 3 维 的 数据 卷积/n 层/q 
和全/nr 连接/v 层/q 包括/v 训练/vn 參/zg 数/n RELU 和 POOLing 
层 不包括 卷积 层 全/a 连接/v 层/q 和/c POOLing/w 层/q 
包括/v 超/v 參/zg 数/n RELU 层 没有 下图 为 CIFAR 
10 数据集 构建 的 一个 卷积 神经 网络结构 示意图 既然 
有 这么 多 不同 的 层级 结构 那 我们 就 
展开来 讲讲 3.1 卷积 层 说 起来 这是 卷积 神经 
网络 的 核心层 从 名字 就 能够 看出来 对吧 _ 
| | 3 . 1.1 卷积 层 综述 直观 看来 
卷积 层 的 參 数 事实上 能够 看做 一 系列 
的 可 训练 / 学习 的 过滤器 在前 向 计算 
过程 中 我们 输入 一定 区域 大小 width * height 
的 数据 和/c 过滤器/n 点乘/i 后/f 等到/v 新的/i 二维/m 数据/n 
然后 滑过 一个个 滤波器 组成 新的 3 维 输出 数据 
而/c 我们/r 能够/v 理解/v 成每/nr 一个/m 过滤器/n 都/d 仅仅/d 关心/n 
过滤/v 数据/n 小平面/n 内/n 的/uj 部分/n 特征/n 当 出现 它 
学习 到 的 特征 的 时候 就会 呈现 激活 / 
activate 态 局部 关联度 这是 卷积 神经 网络 的 独特 
之处 当中 之中 的 一个 我们 知道 在 高维 数据 
比方 图片 中 用 全 连接 的 神经 网络 实际 
project 中 基本 是 不 可行 的 卷积 神经 网络 
中 每 一层 的 神经元 仅仅 会 和上/nr 一层 的 
一些 局部 区域 相连 这 就是 所谓 的 局部 连接性 
你 能够 想象 成 上 一层 的 数据 区 有 
一个 滑动 的 窗体 仅仅/d 有/v 这个/r 窗/s 体内/s 的/uj 
数据/n 会/v 和/c 下一层/i 神经元/nz 有/v 关联/ns 当然 这个 做法 
就 要求 我们 手动 敲定 一个 超 參 数 窗体 
大小 通常 情况下 这个 窗体 的 长 和宽是/nr 相等 的 
我们 把 长 x 宽 叫做 receptive field 实际 的 
计算 中 这个 窗体 是 会 滑动 的 会 近似 
覆盖 图片 的 全部 小区域 举个 实例 CIFAR 10中 的 
图片 输入 数据 为 32 * 32 * 3 的 
假设 我们 把 receptive field 设为 5 * 5 那 
receptive field 的 data 都会 和下/nr 一层 的 神经元 关联 
所以 共同 拥有 5 * 5 * 3 = 75个 
权重 注意 到 最后 的 3 依然 代表 着 RGB 
3个 颜色通道 假设 不是 输入 数据 层 中间层 的 data 
格式 可能 是 16 * 16 * 20 的 假如 
我们 取 3 * 3 的 receptive field 那 单个 
神经元 的 权重 为 3 * 3 * 20 = 
180 局部 关联 细节 我们 刚才 说到 卷积 层 的 
局部 关联 问题 这个 地方 有 一个 receptive field 也 
就是 我们 直观 理解 上 的 滑动 数据 窗体 从 
输入 的 数据 到 输出 数据 有 三个 超 參 
数 会 决定 输出 数据 的 维度 各自 是 深度 
/ depth 步长 / stride 和 填 充值 / zero 
padding 所谓 深度 / depth 简单 说来 指 的 就是 
卷积 层 中和 上 一层 同一 个 输入 区域 连接 
的 神经元 个数 这部分 神经元 会在 遇到 输入 中的 不同 
feature 时 呈现 activate 状态 举个 样例 假设 这是 第一个 
卷积 层 那 输入 到 它 的 数据 实际上 是 
像素 值 不同 的 神经元 可能 对 图像 的 边缘 
轮廓 或者 颜色 会 敏感 所谓 步长 / stride 是 
指 的 窗体 从 当前 位置 到下 一个 位置 跳过 
的 中间 数据 个数 比 方从 图像 数据 层 输入 
到 卷积 层 的 情况 下 或许 窗体 初始 位置 
在 第 1个 像素 第二 个 位置 在 第 5个 
像素 那么 stride = 5 1 = 4 . 所谓 
zero padding 是 在 原始 数据 的 周边 补上 0 
值 的 圈数 以 下第 2张 图中 的 样子 这么 
解释 可能 理解 起来 还是 会 有困难 我们 找 两张 
图 来 相应 一下 这 三 个量 这 是 解决 
ImageNet 分类 问题 用到 的 卷积 神经 网络 的 一部分 
我们 看到 卷积 层 直接 和最/nr 前面 的 图像 层 
连接 图像 层 的 维度 为 227 * 227 * 
3 而 receptive field 设为 11 * 11 图上 未标明 
可是 滑动 窗体 的 步长 stride 设为 4 深度 depth 
为 48 + 48 = 96 这是 双 GPU 并行 
设置 边缘 没有 补 0 因此 zero padding 为 0 
因此 窗体 滑完/nr 一行 总共 停留 次数 为 data _ 
len receptive _ field _ len + 2 * zero 
padding / stride + 1 = 227 11 + 2 
* 0 / 4 + 1 = 55 由于 图像 
的 长宽 相等 因此 纵向 窗体 数 也是 55 最后 
得到 的 输出 数据 维度 为 55 * 55 * 
96 维 这是 一张 动态 的 卷积 层 计 算图 
图上 的 zero padding 为 1 所以 大家 能够 看到 
数据 左右 各 补了 一行 0 窗体 的 长宽 为 
3 滑动 步长 stride 为 2 关于 zero padding 补 
0 这个 操作 产生 的 根本 原因 是 为了 保证 
窗体 的 滑动 能 从头 刚好 到尾 举个 样例 说 
上 2 图中 的 上面 一幅 图 由于 data _ 
len receptive _ field _ len + 2 * zero 
padding / stride 刚好 能够 整除 所以 窗体 左側/nr 贴着 
数据 開始 位置 滑到 尾部 刚好 窗体 右 側 能够 
贴着 数据 尾部 位置 因此 是 不 须要 补 0 
的 而在 以下 那幅 图中 假设 滑动 步长 设为 4 
你 会 发现 第一 次 计算 之后 窗体 就 无法 
滑动 了 而 尾部 的 数据 是 没有 被 窗体 
看到过 的 因此 补 0 能够 解决 问题 关于 窗体 
滑动 步长 大家 能够 发现 一点 窗体 滑动 步长 设定 
越小 两次 滑动 取得 的 数据 重叠 部分 越多 可是 
窗体 停留 的 次数 也 会 越多 运算 律 大一些 
窗体 滑动 步长 设定 越长 两次 滑动 取得 的 数据 
重叠 部分 越少 窗体 停留 次数 也 越少 运算 量小 
可是 从 一定 程度 上 说 数据 信息 不如 上面 
丰富 了 3 . 1.2 卷积 层 的 參 数 
共享 首先 得 说 卷积 层 的 參 数 共享 
是 一个 非常 赞 的 处理 方式 它 使得 卷积 
神经 网络 的 训练 计算 复杂度 和參数/nr 个数 降低 非常 
非常 多 就拿 实际 解决 ImageNet 分类 问题 的 卷积 
神经 网络结构 来说 我们 知道 输出 结果 有 55 * 
55 * 96 = 290400个 神经元 而 每 一个 神经元 
由于 和窗/nr 体内 数据 的 连接 有/v 11/m */i 11/m 
*/i 3/m =/i 363个/mq 权/n 重和/i 1个/mq 偏移量/n 所以 总 
共同 拥有 290400 * 364 = 105705600个 权重 然后 恩 
训练 要 累 挂了 因此 我们 做 了 一个 大胆 
的 假设 我们 刚才 提到 了 每一个 神经元 能够 看做 
一个 filter 对 图片 中 的 数据 窗 区域 做 
过滤 那 既然 是 filter 我们 干脆 就 假设 这个 
神经元 用于 连接 数据 窗 的 权重 是 固定 的 
这 意味着 对 同一 个 神经元 而言 不论 上 一层 
数据 窗体 停留 在 哪个 位置 连接 两者 之间 的 
权重 都是 同 一组 数 那 代表 着 上面 的 
样例 中的 卷积 层 我们 仅仅 须要 神经元 个数 * 
数据 窗体 维度 = 96 * 11 * 11 * 
3 = 34848个 权重 假设 相应 每一个 神经元 的 权重 
是 固定 的 那么 整个 计算 的 过程 就 能够 
看做 一组 固定 的 权重 和 不同 的 数据 窗体 
数据 做 内积 的 过程 这在 数学上 刚好 相应 卷积 
操作 这 也 就是 卷积 神经网 的 名字 来源 另外 
由于 每 一个 神经元 的 权重 固定 它 能够 看做 
一个 恒定 的 filter 比方 上面 96个 神经元 作为 filter 
可视化 之后 是 例如 以下 的 样子 须要 说明 的 
一点 是 參 数 共享 这个 策略 并非 每 一个 
场景 下 都 合适 的 有 一些 特定 的 场合 
我们 不能 把 图片 上 的 这些 窗体 数据 都 
视作 作用 等同 的 一个 非常 典型 的 样例 就是 
人脸识别 一般人 的 面部 都 集中 在 图像 的 中央 
因此 我们 希望 数据 窗体 滑过 这块 区域 的 时候 
权重 和 其它 边缘 区域 是 不同 的 我们 有 
一种 特殊 的 层 相应 这种 功能 叫做 局部 连接 
层 / Locally Connected Layer3 . 1.3 卷积 层 的 
简单 numpy 实现 我们 假定 输入 到 卷积 层 的 
数据 为 X 增加 X 的 维度 为 X . 
shape 11 11 4 假定 我们 的 zero padding 为 
0 也 就是 左 右上 下不 补充 0 数据 数据 
窗体 大小 为 5 窗体 滑动 步 长为 2 那 
输出 数据 的 长宽 应该 为 11 5 / 2 
+ 1 = 4 假定/v 第一/m 个/q 神经元/nz 相应/v 的/uj 
权重/n 和/c 偏移量/n 分别为/i W0/i 和/c b0/i 那/r 我们/r 就/d 
能/v 算得/v 在 第一 行 数据 窗体 停留 的 4个 
位置 得到 的 结果 值 分别为 V 0 0 0 
= np . sum X 5 5 * W0 + 
b0V 1 0 0 = np . sum X 2 
7 5 * W0 + b0V 2 0 0 = 
np . sum X 4 9 5 * W0 + 
b0V 3 0 0 = np . sum X 6 
11 5 * W0 + b0 注意 上述 计算 过程 
中 * 运算符 是 对 两个 向量 进行 点乘 的 
因此 W0 应该 维度 为 5 5 4 相同 你 
能够 计算 其它 位置 的 计算 输出 值 V 0 
0 1 = np . sum X 5 5 * 
W1 + b1V 1 0 1 = np . sum 
X 2 7 5 * W1 + b1V 2 0 
1 = np . sum X 4 9 5 * 
W1 + b1V 3 0 1 = np . sum 
X 6 11 5 * W1 + b1 每一个 神经元 
相应 不同 的 一组 W 和b/nr 在 每 一个 数据 
窗体 停留 的 位置 得到 一个 输出 值 我们 之前 
提到 了 卷积 层 在做 的 事情 是/v 不断/d 做/v 
权/n 重和/i 窗体/n 数据/n 的/uj 点乘/i 和/c 求和/v 因此 我们 
也 能够 把 这个 过程 整理 成 一个 大 的 
矩阵 乘法 看看 数据 端 我们 能够 做 一个 操作 
im2col 将 数据 转成 一个 可 直接 供 神经元 filter 
计算 的 大 矩阵 举个 样例 说 输入 是 227 
* 227 * 3 的 图片 而 神经元 权 重为 
11 * 11 * 3 同一 时候 窗体 移动 步 
长为 4 那 我们 知道 数据 窗体 滑动 过程 中 
总共 产生 227 11 / 4 + 1 * 227 
11 / 4 + 1 = 55 * 55 = 
3025个 局部 数据 区域 又 每 一个 区域 包括 11 
* 11 * 3 = 363个 数据 值 因此/c 我们/r 
想/v 办法/n 把/p 原始数据/n 反复/v 和/c 扩充/v 成/n 一个/m 363 
* 3025 的 数据 矩阵 X _ col 就 能够 
直接 和 filter 进 行运 算了 对于 filter 端 卷积 
层 假如 厚度 为 96 有/v 96个/mq 不同/a 权/n 重组/vn 
的/uj filter/w 每一个 filter 的 权重 为 11 * 11 
* 3 因此 filter 矩阵 W _ row 维度 为 
96 * 363 在 得到 上述 两个 矩阵 后 我们 
的 输出 结果 即 能够 通过 np . dot W 
_ row X _ col 计算 得到 结果 数据 为 
96 * 3025 维 的 这个 实现 的 弊端 是 
由于 数据 窗体 的 滑动 过程 中 有 重叠 因此 
我们 出现 了 非常 多 反复 数据 占用 内存 较大 
优点 是 实际 计算 过程 非常 easy 假设 我们 用 
相似 BLAS 这种 库 计算 将 非常 迅速 另外 在 
反向 传播 过程 中 事实上 卷积 相应 的 操作 还是 
卷积 因此 实现 起来 也 非常 方便 3.2 Pooling 层 
简单 说来 在 卷积 神经 网络 中 Pooling 层 是 
夹在 连续 的 卷积 层 中间 的 层 它 的 
作用 也 非常 easy 就是 * * 逐步 地 压缩 
/ 降低 数据 和參数/nr 的 量 也 在 一定 程度 
上 减小 过拟合 的 现象 * * Pooling 层 做 
的 操作 也 非常 easy 就是 将 原 数据 上 
的 区域 压缩 成 一个 值 区域 最大值 / MAX 
或者 平均值 / AVERAGE 最 常见 的 Pooling 设定 是 
将 原 数据 切成 2 * 2 的 小块 每块 
里面 取 最大值 作为 输出 这样 我们 就 自然而然 降低 
了 75% 的 数据 量 须要 提到 的 是 除掉 
MAX 和 AVERAGE 的 Pooling 方式 事实上 我们 也 能够 
设定 别的 pooling 方式 比方 L2 范数 pooling 说 起来 
历史 上 average pooling 用 的 非常 多 可是 近些年 
热度 降 了 不少 project 师们在/nr 实践 中 发现 max 
pooling 的 效果 相对 好 一些 一个 对 Pooling 层 
和它的/nr 操作 直观 理解 的 示意 图为 上 图为 Pooling 
层 的 一个 直观 演示 样例 相当于 对 厚度 为 
64 的 data 每一个 切片 做 了 一个 下 採 
样 下图 为 Pooling 操作 的 实际 max 操作 Pooling 
层 假定 是 MAX Pooling 在 反向 传播 中 的 
计算 也是 非常 easy 的 大家 都 知道 怎样 去求 
max x y 函数 的 偏 导 也是 分段 的 
3.3 归一化 层 Normalization Layer 卷积 神经网络 里面 有时候 会 
用到 各种各样 的 归一化 层 尤其 是 早期 的 研究 
常常 能 见到 它们 的 身影 只是 近些年来 的 研究 
表明 似乎 这个 层级 对 最后 结果 的 帮助 非常 
小 所以 后来 大多数 时候 就 干脆 拿 掉了 3.4 
全 连接 层 这 是 我们 在 介绍 神经 网络 
的 时候 最 标准 的 形式 不论什么/l 神经/n 元和/n 上/f 
一层/m 的/uj 不论什么/l 神经元/nz 之间/f 都有/nr 关联/ns 然后 矩阵 运算 
也 非常 easy 和 直接 如今 的 非常 多 卷积 
神经 网络结构 末 层 会 採 用 全连 接去 学习 
很多 其它 的 信息 4 . 搭建 卷积 神经网 结构 
从 上面 的 内容 我们 知道 卷积 神经网络 一般 由 
3种 层 搭建 而成 卷积 层 POOLing 层 我们 直接 
指定 用 MAX Pooling 和全/nr 连接/v 层/q 然后 我们 一般 
选用 最 常见 的 神经元 ReLU 我们 来 看看 有 
这些 组件 之后 怎么 拼 出 一个 合理 的 卷积 
神经网 4.1 层 和层/nr 怎么 排 最 常见 的 组合 
方式 是 用 ReLU 神经元 的 卷积 层 组 一个 
神经 网络 同一 时候 在 卷积 层 和 卷积 层 
之间 插入 Pooling 层 经过 多次 的 卷积 层 = 
Pooling 层 叠加 之后 数据 的 整体 量级 就 不大 
了 这个 时候 我们 能够 放 一层 全 连接 层 
然后/c 最后/f 一层/m 和/c output/w 层/q 之间/f 是/v 一个/m 全/a 
连接/v 层/q 所以 总结 一下 最 常见 的 卷积 神经网 
结构 为 输入 层 = ReLU 卷积 层 * N 
= Pooling 层 * M = ReLU 全 连接 层 
* K = 全 连接 层 解释一下 当中 \ * 
操作 代表 能够 叠加 非常 多层 而 Pooling 层 表示 
Pooling 层 事实上 是 可选 的 可有可无 N 和M是/nr 详细 
层数 比方说 输入 层 ReLU 卷积 层 = ReLU 卷积 
层 = Pooling 层 * 3 ReLU 全 连接 层 
* 2 全 连接 层 就是 一个 合理 的 深层 
的 卷积 神经网 在 相同 的 视野 范围内 选择 多层 
叠加 的 卷积 层 而 不是 一个 大 的 卷积 
层 这句话 非常 拗口 但这 是 实际 设计 卷积 神经网络 
时候 的 经验 我们 找个 样例 来 解释 一下 这 
句话 假设 你 设计 的 卷积 神经网 在 数据 层 
有 3层 连续 的 卷积 层 同一 时候 每 一层 
滑动 数据 窗体 为 3 * 3 第一层 每 一个 
神经元 能够 同一 时候 看到 3 * 3 的 原始数据 
层 那 第二层 每一个 神经元 能够 间接 看到 1 + 
3 + 1 * 1 + 3 + 1 = 
5 * 5 的 数据 层 内容 第三层 每 一个 
神经元 能够 间接 看到 1 + 5 + 1 * 
1 + 5 + 1 = 7 * 7 的 
数据 层 内容 那从最/nr 表层/n 看/v 还 不如 直接 设定 
滑动 数据 窗体 为 7 * 7 的 为啥 要 
这么 设计 呢 我们 来 分析 一下 优劣 尽管 第三 
层 对 数据 层 的 视野 范围 是 一致 的 
可是 单层 卷积 层 加 7 * 7 的 上层 
滑动 数据 窗体 结果 是 这 7个 位置 的 数据 
都是 线性组合 后 得到 最后 结果 的 而 3层 卷积 
层 加 3 * 3 的 滑动 数据 窗体 得到 
的 结果 是 原 数据 上 7 * 7 的 
视野 内 数据 多层 非 线性组合 因此 这种 特征 也 
会 具备 更高 的 表达 能力 假设 我们 假设 全部 
层 的 厚度 / channel 数 是 一致 的 为 
C 那7*/nr 7 的 卷积 层 会 得到 C × 
7 × 7 × C = 49C2 个 參 数 
而 3层 叠加 的 3 * 3 卷积 层 仅仅有 
3 × C × 3 × 3 × C = 
27C2 个 參 数 在 计算 量 上 后者 显然是 
有 优势 的 同上 一点 我们 知道 为了 反向 传播 
方便 实际 计算 过程 中 我们/r 会/v 在前/i 向/p 计算/v 
时/n 保留/v 非常多/i 中间/f 梯度/n 3层 叠加 的 3 * 
3 卷积 层 须要 保持 的 中间 梯度 要 小于 
前 一种 情况 这在 project 实现 上 是 非常 有 
优点 的 4.2 层 大小 的 设定 话说 层级 结构 
确定 了 也得 知道 每 一层 大概 什么 规模 啊 
如今 我们 就 来 聊聊 这个 说 起来 每 一层 
的 大小 神经元 个数 和 排布 并 没有 严格 的 
数字 规则 可是/c 我们/r 有/v 一些/m 通用/v 的/uj project/w 实践/v 
经验/n 和/c 系数/n 对于 输入 层 图像 层 我们 一般 
把 数据 归一 化成 2 的 次方 的 长宽 像素 
值 比方 CIFAR 10 是 32 * 32 * 3 
STL 10 数据集 是 64 * 64 * 3 而 
ImageNet 是 224 * 224 * 3 或者 512 * 
512 * 3 卷积 层 一般 会 把 每一个 滤 
子 / filter / 神经元 相应 的 上层 滑动 数据 
窗体 设为 3 * 3 或者 5 * 5 滑动 
步长 stride 设为 1 project 实践 结果表明 stride 设为 1 
尽管 比 較 密集 可是 效果 比 較 好 步长 
拉 太大 easy 损失 太多 信息 zero padding 就 不用 
了 Pooling 层 一般 採 用 max pooling 同一 时候 
设定 採 样 窗体 为 2 * 2 偶尔 会 
见到 设定 更大 的 採 样 窗体 可是那 意味着 损失 
掉 比 較 多 的 信息 了 比 較 重要 
的 是 我们 得 预估 一下 内存 然后 依据 内存 
的 情况 去 设定 合理 的 值 我们 举个 样例 
在 ImageNet 分类 问题 中 图片 是 224 * 224 
* 3 的 我们 跟 在 数据 层 后面 3个 
3 * 3 视野 窗 的 卷积 层 每 一层 
64个 filter / 神经元 把 padding 设为 1 那么 最后 
每一个 卷积 层 的 output 都是 224 * 224 * 
64 大概 须要 1000 万次 对 output 的 激励 计算 
非线性 activation 大概 花费 72MB 内存 而 project 实践 里 
一般 训练 都在 GPU 上 进行 GPU 的 内存 比 
CPU 要 吃紧 的 多 所以 或许 我们 要 略微 
调动 一下 參 数 比方 AlexNet 用 的 是 11 
* 11 的 的 视野 窗 滑动 步 长为 4 
4.3 典型 的 工业 界 在用 卷积 神经网络 几个 有名 
的 卷积 神经网络 例如 以下 LeNet 这是 最早 用 起来 
的 卷积 神经网络 Yann LeCun 在 论文 LeNet 提到 AlexNet 
2012 ILSVRC 比赛 远超 第 2名 的 卷积 神经网络 和 
LeNet 的 结构 比 較 像 仅仅 是 更深 同一 
时候 用 多层 小 卷积 层 叠加 提到 大 卷积 
层 ZF Net 2013 ILSVRC 比赛 冠军 能够 參 考 
论文 ZF NetGoogLeNet 2014 ILSVRC 比赛 冠军 Google 发表 的 
论文 Going Deeper with Convolutions 有 详细 介绍 VGGNet 也是 
2014 ILSVRC 比赛 中 的 模型 有意思 的 是 即使 
这个 模型 当时 在 分类 问题 上 的 效果 略 
差 于 google 的 GoogLeNet 可是 在 非常 多 图像 
转化 学习 问题 比方 object detection 上 效果 奇好 它 
也 证明 卷积 神经网 的 深度 对于 最后 的 效果 
有 至关 关键 的 数据 预 训 练好 的 模型 
在 pretrained model site 能够 下载 详细 一点 说来 VGGNet 
的 层级 结构 和 花费 的 内存 例如 以下 INPUT 
224x224x3 memory 224 * 224 * 3 = 150K weights 
0 CONV3 64 224x224x64 memory 224 * 224 * 64 
= 3.2 M weights 3 * 3 * 3 * 
64 = 1 728 CONV3 64 224x224x64 memory 224 * 
224 * 64 = 3.2 M weights 3 * 3 
* 64 * 64 = 36 864 POOL2 112x112x64 memory 
112 * 112 * 64 = 800K weights 0 CONV3 
128 112x112x128 memory 112 * 112 * 128 = 1.6 
M weights 3 * 3 * 64 * 128 = 
73 728 CONV3 128 112x112x128 memory 112 * 112 * 
128 = 1.6 M weights 3 * 3 * 128 
* 128 = 147 456 POOL2 56x56x128 memory 56 * 
56 * 128 = 400K weights 0 CONV3 256 56x56x256 
memory 56 * 56 * 256 = 800K weights 3 
* 3 * 128 * 256 = 294 912 CONV3 
256 56x56x256 memory 56 * 56 * 256 = 800K 
weights 3 * 3 * 256 * 256 = 589 
824 CONV3 256 56x56x256 memory 56 * 56 * 256 
= 800K weights 3 * 3 * 256 * 256 
= 589 824 POOL2 28x28x256 memory 28 * 28 * 
256 = 200K weights 0 CONV3 512 28x28x512 memory 28 
* 28 * 512 = 400K weights 3 * 3 
* 256 * 512 = 1 179 648 CONV3 512 
28x28x512 memory 28 * 28 * 512 = 400K weights 
3 * 3 * 512 * 512 = 2 359 
296 CONV3 512 28x28x512 memory 28 * 28 * 512 
= 400K weights 3 * 3 * 512 * 512 
= 2 359 296 POOL2 14x14x512 memory 14 * 14 
* 512 = 100K weights 0 CONV3 512 14x14x512 memory 
14 * 14 * 512 = 100K weights 3 * 
3 * 512 * 512 = 2 359 296 CONV3 
512 14x14x512 memory 14 * 14 * 512 = 100K 
weights 3 * 3 * 512 * 512 = 2 
359 296 CONV3 512 14x14x512 memory 14 * 14 * 
512 = 100K weights 3 * 3 * 512 * 
512 = 2 359 296 POOL2 7x7x512 memory 7 * 
7 * 512 = 25K weights 0 FC 1x1x4096 memory 
4096 weights 7 * 7 * 512 * 4096 = 
102 760 448 FC 1x1x4096 memory 4096 weights 4096 * 
4096 = 16 777 216 FC 1x1x1000 memory 1000 weights 
4096 * 1000 = 4 096 000 TOTAL memory 24M 
* 4 bytes ~ = 93MB / image only forward 
~ * 2 for bwd TOTAL params 138M parameters 有意思 
的 是 大家 会 注意 到 在 VGGNet 这样 一个 
神经 网络 里 大多数 的 内存 消耗 在 前面 的 
卷积 层 而 大多数 须要 训练 的 參 数 却 
集中 在 最后 的 全 连接 层 比方 上上 面的 
样例 里 全/a 连接/v 层/q 有/v 1亿/mq 权重/n 參/zg 数/n 
总共 神经 网里 也就 1.4亿 权重 參 数 4.4 考虑 
点 组 一个 实际 可用 的 卷积 神经网络 最大 的 
瓶颈 是 GPU 的 内存 毕竟 如今 非常多 GPU 仅仅有 
3/4 / 6GB 的 内存 最大 的 GPU 也就 12G 
内存 所以 我们 应该 在 设计 卷积 神经网 的 时候 
多 加 考虑 非常大 的 一部分 内存 开销 来源于 卷积 
层 的 激励函数 个数 和 保存 的 梯度 数量 保存 
的 权重 參 数 也是 内存 的 主要 消耗 处 
包括 反向 传播 要 用到 的 梯度 以及 你 用 
momentum Adagrad or RMSProp 这些 算法 时候 的 中间 存储 
值 数据 batch 以及 其它 的 相似 版本号 信息 或者 
来源 信息 等 也会 消耗 一部分 内存 5 . 很多 
其它 的 卷积 神经网络 參 考 资料 DeepLearning . net 
tutorial 是 一个 用 Theano 完整 实现 卷积 神经网 的 
教程 cuda convnet2 是 多 GPU 并行 化 的 实现 
ConvNetJS CIFAR 10 demo 同意 你 手动 设定 參 数 
然后 直接 在 浏览器 看 卷积 神经 网络 的 结果 
Caffe 主流 卷积 神经网络 开源 库 之中 的 一个 Example 
Torch 7 ConvNet 在 CIFAR 10 上 错误率 仅仅有 7% 
的 卷积 神经 网络 实现 Ben Graham s Sparse ConvNet 
CIFAR 10 上 错误率 仅仅有 4% 的 实现 Face recognition 
for right whales using deep learning Kaggle 看图 识别 濒临灭绝 
右 鲸 比赛 的 冠军 队伍 卷积 神经网络 參 考 
资料 与 原文 cs231n 卷积 神经网络 作者 寒 小阳 时间 2016年 1月 出处 http / / 
blog . csdn . net / han _ xiaoyang / 
article / details / 50542880 声明 版权 全部 转载 请 
联系 作者 并 注明 出处 1 . 前言 前面 九 
讲对 神经 网络 的 结构 组件 训练方法 原理 等 做了 
介绍 如今 我们 回到 本 系列 的 核心 计算机 视觉 
神经 网络 中 的 一种 特殊 版本号 在 计算机 视觉 
中 使用 最为 广泛 这 就是 大家 都 知道 的 
卷积 神经网络 卷积 神经 网络 和 普通 的 神经 网络 
一样 由 神经元 按 层级 结构 组成 其间 的 权重 
和 偏移量 都是 可 训练 得到 的 相/v 同是/c 输入/v 
的/uj 数据/n 和权/nr 重做/i 运算/vn 输出 结果 输入 激励 神经元 
输出 结果 从 整体 上 看来 整个 神经 网络 做 
的 事情 依然 是 对于 像素 级别 输入 的 图像 
数据 用 得分 函数 计算 最后 各个 类别 的 得分 
然后 我们 通过 最小化 损失 函数 来 得到 最优 的 
权重 之前 的 博文 中 介绍 的 各种 技巧 和 
训练 方法 以及 注意事项 在 这个 特殊 版本号 的 神经 
网络 上 依然 好使 既然 提到 卷积 神经 网络 了 
我们 就 来说 说 它 的 特殊 之处 首先 这里 
的 卷积 神经网络 一般 假定 输入 就是 图片 数据 也 
正是 由于 输入 是 图片 数据 我们 能够 利用 它 
的 像素 结构 特性 去做 一些 假 设来 简化 神经 
网络 的 训练 复杂度 降低 训练 參 数 个数 2 
. 卷积 神经网 整体 结构 一览 我们 前面 讲过 的 
神经 网络结构 都比 較 一致 输入 层 和 输出 层 
中间 夹着 数层 隐藏 层 每 一层 都由 多个 神经元 
组成 层/q 和层/nr 之间/f 是/v 全/a 连接/v 的/uj 结构/n 同 
一层 的 神经 元 之间 没有 连接 卷积 神经 网络 
是 上述 结构 的 一种 特殊 化 处理 由于 对于 
图像 这种 数据 而言 上面 这种 结构 实际 应用 起来 
有 较大 的 困难 就拿 CIFAR 10 举例 吧 图片 
已经 非常 小了 是 32 * 32 * 3 长宽 
各 32 像素 3个 颜色通道 的 那么 在 神经网络 当中 
我们 仅仅 看 隐藏 层 中 的 一个 神经元 就 
应该 有 32 * 32 * 3 = 3072个 权重 
假设 大家 认为 这个 权重 个数 的 量 还行 的话 
再 设想 一下 当 这 是 一个 包括 多 个 
神经元 的 多层 神经网 假设 n 个 再 比方 图像 
的 质量 好 一点 比方 是 200 * 200 * 
3 的 那 将有 200 * 200 * 3 * 
n = 120000n 个 权重 须要 训练 结果 是 拉着 
这么多 參 数 训练 基本 跑不动 跑得 起来 也是 气喘吁吁 
当然 最 关键 的 是 这么 多 參 数 的 
情况 下 分分钟 模型 就 过拟合 了 别急 别急 一会儿 
我们 会 提到 卷积 神经 网络 的 想法 和 简化 
之处 卷积 神经 网络结构 比 較 固定 的 原因 之中 
的 一个 是 图片 数据 本身 的 合理 结构 类 
图像 结构 200 * 200 * 3 我们 也 把 
卷积 神经 网络 的 神经元 排布 成 width * height 
* depth 的 结构 也 就是说 这 一层 总 共同 
拥有 width * height * depth 个 神经元 例 如以 
下图 所 看到 的 举个 样例 说 CIFAR 10 的 
输出 层 就是 1 * 1 * 10 维 的 
另外 我们 后 面会 说到 每 一层 的 神经元 事实上 
仅仅 和上/nr 一层 里 某些 小区域 进行 连接 而/c 不是/c 
和上/nr 一层/m 每一个/i 神经元/nz 全/a 连接/v 3 . 卷积 神经 
网络 的 组成 层 在 卷积 神经 网络 中 有 
3种 最 基本 的 层 卷积 运算 层 pooling 层 
全 连接 层 一个 完整 的 神经 网络 就是 由 
这三种 层 叠加 组成 的 结构 演示 样例 我们 继续 
拿 CIFAR 10 数据集 举例 一个 典型 的 该 数据 
集上 的 卷积 神经网络 分类器 应该有 INPUT CONV RELU POOL 
FC 的 结构 详细 说来 是 这种 INPUT 32 * 
32 * 3 包括 原始 图片 数据 中 的 全部 
像素 长宽 都是 32 有 RGB 3个 颜色通道 CONV 卷积 
层 中 没 个 神经元 会 和上/nr 一层 的 若干 
小区域 连接 计算 权 重和 小区域 像素 的 内积 举个 
样例 可能 产出 的 结果 数据 是 32 * 32 
* 12 的 RELU 层 就是 神经元 激励 层 基本 
的 计算 就是 max 0 x 结果 数据 依然 是 
32 * 32 * 12 POOLing 层 做 的 事情 
能够 理解 成 一个 下 採 样 可能 得到 的 
结果 维度 就 变为 16 * 16 * 12 了 
全 连接 层 一般 用于 最后 计算 类别 得分 得到 
的 结果 为 1 * 1 * 10 的 当中 
的 10 相应 10个 不同 的 类别 和 名字 一样 
这/r 一层/m 的/uj 全部/n 神经元/nz 会/v 和上/nr 一层/m 的/uj 全部/n 
神经元/nz 有/v 连接/v 这样 卷积 神经网络 作为 一个 中间 的 
通道 就 一步步 把 原始 的 图像 数据 转成 最后 
的 类别 得 分了 有 一个 点 我们 要 提 
一下 刚才 说 到了 有 几种 不同 的 神经 网络层 
当中 有 一些 层 是 有待 训练 參 数 的 
另外 一些 没有 详细 一点 说 卷积/n 层/q 和全/nr 连接/v 
层/q 包括/v 权/n 重和/i 偏移/v 的/uj 而 RELU 和 POOLing 
层 仅仅 是 一个 固定 的 函数 运算 是 不包括 
权 重和 偏移 參 数 的 只是 POOLing 层 包括 
了 我们 手动 指定 的 超 參 数 这个 我们 
之后 会 提到 总结 一下 一个 卷积 神经网络 由 多种 
不同 类型 的 层 卷 几层 / 全 连接 层 
/ RELU 层 / POOLing 层 等 叠加 而成 每 
一层 的 输入 结构 是 3 维 的 数据 计算 
完 输出 依然 是 3 维 的 数据 卷积/n 层/q 
和全/nr 连接/v 层/q 包括/v 训练/vn 參/zg 数/n RELU 和 POOLing 
层 不包括 卷积 层 全/a 连接/v 层/q 和/c POOLing/w 层/q 
包括/v 超/v 參/zg 数/n RELU 层 没有 下图 为 CIFAR 
10 数据集 构建 的 一个 卷积 神经 网络结构 示意图 既然 
有 这么 多 不同 的 层级 结构 那 我们 就 
展开来 讲讲 3.1 卷积 层 说 起来 这是 卷积 神经 
网络 的 核心层 从 名字 就 能够 看出来 对吧 _ 
| | 3 . 1.1 卷积 层 综述 直观 看来 
卷积 层 的 參 数 事实上 能够 看做 一 系列 
的 可 训练 / 学习 的 过滤器 在前 向 计算 
过程 中 我们 输入 一定 区域 大小 width * height 
的 数据 和/c 过滤器/n 点乘/i 后/f 等到/v 新的/i 二维/m 数据/n 
然后 滑过 一个个 滤波器 组成 新的 3 维 输出 数据 
而/c 我们/r 能够/v 理解/v 成每/nr 一个/m 过滤器/n 都/d 仅仅/d 关心/n 
过滤/v 数据/n 小平面/n 内/n 的/uj 部分/n 特征/n 当 出现 它 
学习 到 的 特征 的 时候 就会 呈现 激活 / 
activate 态 局部 关联度 这是 卷积 神经 网络 的 独特 
之处 当中 之中 的 一个 我们 知道 在 高维 数据 
比方 图片 中 用 全 连接 的 神经 网络 实际 
project 中 基本 是 不 可行 的 卷积 神经 网络 
中 每 一层 的 神经元 仅仅 会 和上/nr 一层 的 
一些 局部 区域 相连 这 就是 所谓 的 局部 连接性 
你 能够 想象 成 上 一层 的 数据 区 有 
一个 滑动 的 窗体 仅仅/d 有/v 这个/r 窗/s 体内/s 的/uj 
数据/n 会/v 和/c 下一层/i 神经元/nz 有/v 关联/ns 当然 这个 做法 
就 要求 我们 手动 敲定 一个 超 參 数 窗体 
大小 通常 情况下 这个 窗体 的 长 和宽是/nr 相等 的 
我们 把 长 x 宽 叫做 receptive field 实际 的 
计算 中 这个 窗体 是 会 滑动 的 会 近似 
覆盖 图片 的 全部 小区域 举个 实例 CIFAR 10中 的 
图片 输入 数据 为 32 * 32 * 3 的 
假设 我们 把 receptive field 设为 5 * 5 那 
receptive field 的 data 都会 和下/nr 一层 的 神经元 关联 
所以 共同 拥有 5 * 5 * 3 = 75个 
权重 注意 到 最后 的 3 依然 代表 着 RGB 
3个 颜色通道 假设 不是 输入 数据 层 中间层 的 data 
格式 可能 是 16 * 16 * 20 的 假如 
我们 取 3 * 3 的 receptive field 那 单个 
神经元 的 权重 为 3 * 3 * 20 = 
180 局部 关联 细节 我们 刚才 说到 卷积 层 的 
局部 关联 问题 这个 地方 有 一个 receptive field 也 
就是 我们 直观 理解 上 的 滑动 数据 窗体 从 
输入 的 数据 到 输出 数据 有 三个 超 參 
数 会 决定 输出 数据 的 维度 各自 是 深度 
/ depth 步长 / stride 和 填 充值 / zero 
padding 所谓 深度 / depth 简单 说来 指 的 就是 
卷积 层 中和 上 一层 同一 个 输入 区域 连接 
的 神经元 个数 这部分 神经元 会在 遇到 输入 中的 不同 
feature 时 呈现 activate 状态 举个 样例 假设 这是 第一个 
卷积 层 那 输入 到 它 的 数据 实际上 是 
像素 值 不同 的 神经元 可能 对 图像 的 边缘 
轮廓 或者 颜色 会 敏感 所谓 步长 / stride 是 
指 的 窗体 从 当前 位置 到下 一个 位置 跳过 
的 中间 数据 个数 比 方从 图像 数据 层 输入 
到 卷积 层 的 情况 下 或许 窗体 初始 位置 
在 第 1个 像素 第二 个 位置 在 第 5个 
像素 那么 stride = 5 1 = 4 . 所谓 
zero padding 是 在 原始 数据 的 周边 补上 0 
值 的 圈数 以 下第 2张 图中 的 样子 这么 
解释 可能 理解 起来 还是 会 有困难 我们 找 两张 
图 来 相应 一下 这 三 个量 这 是 解决 
ImageNet 分类 问题 用到 的 卷积 神经 网络 的 一部分 
我们 看到 卷积 层 直接 和最/nr 前面 的 图像 层 
连接 图像 层 的 维度 为 227 * 227 * 
3 而 receptive field 设为 11 * 11 图上 未标明 
可是 滑动 窗体 的 步长 stride 设为 4 深度 depth 
为 48 + 48 = 96 这是 双 GPU 并行 
设置 边缘 没有 补 0 因此 zero padding 为 0 
因此 窗体 滑完/nr 一行 总共 停留 次数 为 data _ 
len receptive _ field _ len + 2 * zero 
padding / stride + 1 = 227 11 + 2 
* 0 / 4 + 1 = 55 由于 图像 
的 长宽 相等 因此 纵向 窗体 数 也是 55 最后 
得到 的 输出 数据 维度 为 55 * 55 * 
96 维 这是 一张 动态 的 卷积 层 计 算图 
图上 的 zero padding 为 1 所以 大家 能够 看到 
数据 左右 各 补了 一行 0 窗体 的 长宽 为 
3 滑动 步长 stride 为 2 关于 zero padding 补 
0 这个 操作 产生 的 根本 原因 是 为了 保证 
窗体 的 滑动 能 从头 刚好 到尾 举个 样例 说 
上 2 图中 的 上面 一幅 图 由于 data _ 
len receptive _ field _ len + 2 * zero 
padding / stride 刚好 能够 整除 所以 窗体 左側/nr 贴着 
数据 開始 位置 滑到 尾部 刚好 窗体 右 側 能够 
贴着 数据 尾部 位置 因此 是 不 须要 补 0 
的 而在 以下 那幅 图中 假设 滑动 步长 设为 4 
你 会 发现 第一 次 计算 之后 窗体 就 无法 
滑动 了 而 尾部 的 数据 是 没有 被 窗体 
看到过 的 因此 补 0 能够 解决 问题 关于 窗体 
滑动 步长 大家 能够 发现 一点 窗体 滑动 步长 设定 
越小 两次 滑动 取得 的 数据 重叠 部分 越多 可是 
窗体 停留 的 次数 也 会 越多 运算 律 大一些 
窗体 滑动 步长 设定 越长 两次 滑动 取得 的 数据 
重叠 部分 越少 窗体 停留 次数 也 越少 运算 量小 
可是 从 一定 程度 上 说 数据 信息 不如 上面 
丰富 了 3 . 1.2 卷积 层 的 參 数 
共享 首先 得 说 卷积 层 的 參 数 共享 
是 一个 非常 赞 的 处理 方式 它 使得 卷积 
神经 网络 的 训练 计算 复杂度 和參数/nr 个数 降低 非常 
非常 多 就拿 实际 解决 ImageNet 分类 问题 的 卷积 
神经 网络结构 来说 我们 知道 输出 结果 有 55 * 
55 * 96 = 290400个 神经元 而 每 一个 神经元 
由于 和窗/nr 体内 数据 的 连接 有/v 11/m */i 11/m 
*/i 3/m =/i 363个/mq 权/n 重和/i 1个/mq 偏移量/n 所以 总 
共同 拥有 290400 * 364 = 105705600个 权重 然后 恩 
训练 要 累 挂了 因此 我们 做 了 一个 大胆 
的 假设 我们 刚才 提到 了 每一个 神经元 能够 看做 
一个 filter 对 图片 中 的 数据 窗 区域 做 
过滤 那 既然 是 filter 我们 干脆 就 假设 这个 
神经元 用于 连接 数据 窗 的 权重 是 固定 的 
这 意味着 对 同一 个 神经元 而言 不论 上 一层 
数据 窗体 停留 在 哪个 位置 连接 两者 之间 的 
权重 都是 同 一组 数 那 代表 着 上面 的 
样例 中的 卷积 层 我们 仅仅 须要 神经元 个数 * 
数据 窗体 维度 = 96 * 11 * 11 * 
3 = 34848个 权重 假设 相应 每一个 神经元 的 权重 
是 固定 的 那么 整个 计算 的 过程 就 能够 
看做 一组 固定 的 权重 和 不同 的 数据 窗体 
数据 做 内积 的 过程 这在 数学上 刚好 相应 卷积 
操作 这 也 就是 卷积 神经网 的 名字 来源 另外 
由于 每 一个 神经元 的 权重 固定 它 能够 看做 
一个 恒定 的 filter 比方 上面 96个 神经元 作为 filter 
可视化 之后 是 例如 以下 的 样子 须要 说明 的 
一点 是 參 数 共享 这个 策略 并非 每 一个 
场景 下 都 合适 的 有 一些 特定 的 场合 
我们 不能 把 图片 上 的 这些 窗体 数据 都 
视作 作用 等同 的 一个 非常 典型 的 样例 就是 
人脸识别 一般人 的 面部 都 集中 在 图像 的 中央 
因此 我们 希望 数据 窗体 滑过 这块 区域 的 时候 
权重 和 其它 边缘 区域 是 不同 的 我们 有 
一种 特殊 的 层 相应 这种 功能 叫做 局部 连接 
层 / Locally Connected Layer3 . 1.3 卷积 层 的 
简单 numpy 实现 我们 假定 输入 到 卷积 层 的 
数据 为 X 增加 X 的 维度 为 X . 
shape 11 11 4 假定 我们 的 zero padding 为 
0 也 就是 左 右上 下不 补充 0 数据 数据 
窗体 大小 为 5 窗体 滑动 步 长为 2 那 
输出 数据 的 长宽 应该 为 11 5 / 2 
+ 1 = 4 假定/v 第一/m 个/q 神经元/nz 相应/v 的/uj 
权重/n 和/c 偏移量/n 分别为/i W0/i 和/c b0/i 那/r 我们/r 就/d 
能/v 算得/v 在 第一 行 数据 窗体 停留 的 4个 
位置 得到 的 结果 值 分别为 V 0 0 0 
= np . sum X 5 5 * W0 + 
b0V 1 0 0 = np . sum X 2 
7 5 * W0 + b0V 2 0 0 = 
np . sum X 4 9 5 * W0 + 
b0V 3 0 0 = np . sum X 6 
11 5 * W0 + b0 注意 上述 计算 过程 
中 * 运算符 是 对 两个 向量 进行 点乘 的 
因此 W0 应该 维度 为 5 5 4 相同 你 
能够 计算 其它 位置 的 计算 输出 值 V 0 
0 1 = np . sum X 5 5 * 
W1 + b1V 1 0 1 = np . sum 
X 2 7 5 * W1 + b1V 2 0 
1 = np . sum X 4 9 5 * 
W1 + b1V 3 0 1 = np . sum 
X 6 11 5 * W1 + b1 每一个 神经元 
相应 不同 的 一组 W 和b/nr 在 每 一个 数据 
窗体 停留 的 位置 得到 一个 输出 值 我们 之前 
提到 了 卷积 层 在做 的 事情 是/v 不断/d 做/v 
权/n 重和/i 窗体/n 数据/n 的/uj 点乘/i 和/c 求和/v 因此 我们 
也 能够 把 这个 过程 整理 成 一个 大 的 
矩阵 乘法 看看 数据 端 我们 能够 做 一个 操作 
im2col 将 数据 转成 一个 可 直接 供 神经元 filter 
计算 的 大 矩阵 举个 样例 说 输入 是 227 
* 227 * 3 的 图片 而 神经元 权 重为 
11 * 11 * 3 同一 时候 窗体 移动 步 
长为 4 那 我们 知道 数据 窗体 滑动 过程 中 
总共 产生 227 11 / 4 + 1 * 227 
11 / 4 + 1 = 55 * 55 = 
3025个 局部 数据 区域 又 每 一个 区域 包括 11 
* 11 * 3 = 363个 数据 值 因此/c 我们/r 
想/v 办法/n 把/p 原始数据/n 反复/v 和/c 扩充/v 成/n 一个/m 363 
* 3025 的 数据 矩阵 X _ col 就 能够 
直接 和 filter 进 行运 算了 对于 filter 端 卷积 
层 假如 厚度 为 96 有/v 96个/mq 不同/a 权/n 重组/vn 
的/uj filter/w 每一个 filter 的 权重 为 11 * 11 
* 3 因此 filter 矩阵 W _ row 维度 为 
96 * 363 在 得到 上述 两个 矩阵 后 我们 
的 输出 结果 即 能够 通过 np . dot W 
_ row X _ col 计算 得到 结果 数据 为 
96 * 3025 维 的 这个 实现 的 弊端 是 
由于 数据 窗体 的 滑动 过程 中 有 重叠 因此 
我们 出现 了 非常 多 反复 数据 占用 内存 较大 
优点 是 实际 计算 过程 非常 easy 假设 我们 用 
相似 BLAS 这种 库 计算 将 非常 迅速 另外 在 
反向 传播 过程 中 事实上 卷积 相应 的 操作 还是 
卷积 因此 实现 起来 也 非常 方便 3.2 Pooling 层 
简单 说来 在 卷积 神经 网络 中 Pooling 层 是 
夹在 连续 的 卷积 层 中间 的 层 它 的 
作用 也 非常 easy 就是 * * 逐步 地 压缩 
/ 降低 数据 和參数/nr 的 量 也 在 一定 程度 
上 减小 过拟合 的 现象 * * Pooling 层 做 
的 操作 也 非常 easy 就是 将 原 数据 上 
的 区域 压缩 成 一个 值 区域 最大值 / MAX 
或者 平均值 / AVERAGE 最 常见 的 Pooling 设定 是 
将 原 数据 切成 2 * 2 的 小块 每块 
里面 取 最大值 作为 输出 这样 我们 就 自然而然 降低 
了 75% 的 数据 量 须要 提到 的 是 除掉 
MAX 和 AVERAGE 的 Pooling 方式 事实上 我们 也 能够 
设定 别的 pooling 方式 比方 L2 范数 pooling 说 起来 
历史 上 average pooling 用 的 非常 多 可是 近些年 
热度 降 了 不少 project 师们在/nr 实践 中 发现 max 
pooling 的 效果 相对 好 一些 一个 对 Pooling 层 
和它的/nr 操作 直观 理解 的 示意 图为 上 图为 Pooling 
层 的 一个 直观 演示 样例 相当于 对 厚度 为 
64 的 data 每一个 切片 做 了 一个 下 採 
样 下图 为 Pooling 操作 的 实际 max 操作 Pooling 
层 假定 是 MAX Pooling 在 反向 传播 中 的 
计算 也是 非常 easy 的 大家 都 知道 怎样 去求 
max x y 函数 的 偏 导 也是 分段 的 
3.3 归一化 层 Normalization Layer 卷积 神经网络 里面 有时候 会 
用到 各种各样 的 归一化 层 尤其 是 早期 的 研究 
常常 能 见到 它们 的 身影 只是 近些年来 的 研究 
表明 似乎 这个 层级 对 最后 结果 的 帮助 非常 
小 所以 后来 大多数 时候 就 干脆 拿 掉了 3.4 
全 连接 层 这 是 我们 在 介绍 神经 网络 
的 时候 最 标准 的 形式 不论什么/l 神经/n 元和/n 上/f 
一层/m 的/uj 不论什么/l 神经元/nz 之间/f 都有/nr 关联/ns 然后 矩阵 运算 
也 非常 easy 和 直接 如今 的 非常 多 卷积 
神经 网络结构 末 层 会 採 用 全连 接去 学习 
很多 其它 的 信息 4 . 搭建 卷积 神经网 结构 
从 上面 的 内容 我们 知道 卷积 神经网络 一般 由 
3种 层 搭建 而成 卷积 层 POOLing 层 我们 直接 
指定 用 MAX Pooling 和全/nr 连接/v 层/q 然后 我们 一般 
选用 最 常见 的 神经元 ReLU 我们 来 看看 有 
这些 组件 之后 怎么 拼 出 一个 合理 的 卷积 
神经网 4.1 层 和层/nr 怎么 排 最 常见 的 组合 
方式 是 用 ReLU 神经元 的 卷积 层 组 一个 
神经 网络 同一 时候 在 卷积 层 和 卷积 层 
之间 插入 Pooling 层 经过 多次 的 卷积 层 = 
Pooling 层 叠加 之后 数据 的 整体 量级 就 不大 
了 这个 时候 我们 能够 放 一层 全 连接 层 
然后/c 最后/f 一层/m 和/c output/w 层/q 之间/f 是/v 一个/m 全/a 
连接/v 层/q 所以 总结 一下 最 常见 的 卷积 神经网 
结构 为 输入 层 = ReLU 卷积 层 * N 
= Pooling 层 * M = ReLU 全 连接 层 
* K = 全 连接 层 解释一下 当中 \ * 
操作 代表 能够 叠加 非常 多层 而 Pooling 层 表示 
Pooling 层 事实上 是 可选 的 可有可无 N 和M是/nr 详细 
层数 比方说 输入 层 ReLU 卷积 层 = ReLU 卷积 
层 = Pooling 层 * 3 ReLU 全 连接 层 
* 2 全 连接 层 就是 一个 合理 的 深层 
的 卷积 神经网 在 相同 的 视野 范围内 选择 多层 
叠加 的 卷积 层 而 不是 一个 大 的 卷积 
层 这句话 非常 拗口 但这 是 实际 设计 卷积 神经网络 
时候 的 经验 我们 找个 样例 来 解释 一下 这 
句话 假设 你 设计 的 卷积 神经网 在 数据 层 
有 3层 连续 的 卷积 层 同一 时候 每 一层 
滑动 数据 窗体 为 3 * 3 第一层 每 一个 
神经元 能够 同一 时候 看到 3 * 3 的 原始数据 
层 那 第二层 每一个 神经元 能够 间接 看到 1 + 
3 + 1 * 1 + 3 + 1 = 
5 * 5 的 数据 层 内容 第三层 每 一个 
神经元 能够 间接 看到 1 + 5 + 1 * 
1 + 5 + 1 = 7 * 7 的 
数据 层 内容 那从最/nr 表层/n 看/v 还 不如 直接 设定 
滑动 数据 窗体 为 7 * 7 的 为啥 要 
这么 设计 呢 我们 来 分析 一下 优劣 尽管 第三 
层 对 数据 层 的 视野 范围 是 一致 的 
可是 单层 卷积 层 加 7 * 7 的 上层 
滑动 数据 窗体 结果 是 这 7个 位置 的 数据 
都是 线性组合 后 得到 最后 结果 的 而 3层 卷积 
层 加 3 * 3 的 滑动 数据 窗体 得到 
的 结果 是 原 数据 上 7 * 7 的 
视野 内 数据 多层 非 线性组合 因此 这种 特征 也 
会 具备 更高 的 表达 能力 假设 我们 假设 全部 
层 的 厚度 / channel 数 是 一致 的 为 
C 那7*/nr 7 的 卷积 层 会 得到 C × 
7 × 7 × C = 49C2 个 參 数 
而 3层 叠加 的 3 * 3 卷积 层 仅仅有 
3 × C × 3 × 3 × C = 
27C2 个 參 数 在 计算 量 上 后者 显然是 
有 优势 的 同上 一点 我们 知道 为了 反向 传播 
方便 实际 计算 过程 中 我们/r 会/v 在前/i 向/p 计算/v 
时/n 保留/v 非常多/i 中间/f 梯度/n 3层 叠加 的 3 * 
3 卷积 层 须要 保持 的 中间 梯度 要 小于 
前 一种 情况 这在 project 实现 上 是 非常 有 
优点 的 4.2 层 大小 的 设定 话说 层级 结构 
确定 了 也得 知道 每 一层 大概 什么 规模 啊 
如今 我们 就 来 聊聊 这个 说 起来 每 一层 
的 大小 神经元 个数 和 排布 并 没有 严格 的 
数字 规则 可是/c 我们/r 有/v 一些/m 通用/v 的/uj project/w 实践/v 
经验/n 和/c 系数/n 对于 输入 层 图像 层 我们 一般 
把 数据 归一 化成 2 的 次方 的 长宽 像素 
值 比方 CIFAR 10 是 32 * 32 * 3 
STL 10 数据集 是 64 * 64 * 3 而 
ImageNet 是 224 * 224 * 3 或者 512 * 
512 * 3 卷积 层 一般 会 把 每一个 滤 
子 / filter / 神经元 相应 的 上层 滑动 数据 
窗体 设为 3 * 3 或者 5 * 5 滑动 
步长 stride 设为 1 project 实践 结果表明 stride 设为 1 
尽管 比 較 密集 可是 效果 比 較 好 步长 
拉 太大 easy 损失 太多 信息 zero padding 就 不用 
了 Pooling 层 一般 採 用 max pooling 同一 时候 
设定 採 样 窗体 为 2 * 2 偶尔 会 
见到 设定 更大 的 採 样 窗体 可是那 意味着 损失 
掉 比 較 多 的 信息 了 比 較 重要 
的 是 我们 得 预估 一下 内存 然后 依据 内存 
的 情况 去 设定 合理 的 值 我们 举个 样例 
在 ImageNet 分类 问题 中 图片 是 224 * 224 
* 3 的 我们 跟 在 数据 层 后面 3个 
3 * 3 视野 窗 的 卷积 层 每 一层 
64个 filter / 神经元 把 padding 设为 1 那么 最后 
每一个 卷积 层 的 output 都是 224 * 224 * 
64 大概 须要 1000 万次 对 output 的 激励 计算 
非线性 activation 大概 花费 72MB 内存 而 project 实践 里 
一般 训练 都在 GPU 上 进行 GPU 的 内存 比 
CPU 要 吃紧 的 多 所以 或许 我们 要 略微 
调动 一下 參 数 比方 AlexNet 用 的 是 11 
* 11 的 的 视野 窗 滑动 步 长为 4 
4.3 典型 的 工业 界 在用 卷积 神经网络 几个 有名 
的 卷积 神经网络 例如 以下 LeNet 这是 最早 用 起来 
的 卷积 神经网络 Yann LeCun 在 论文 LeNet 提到 AlexNet 
2012 ILSVRC 比赛 远超 第 2名 的 卷积 神经网络 和 
LeNet 的 结构 比 較 像 仅仅 是 更深 同一 
时候 用 多层 小 卷积 层 叠加 提到 大 卷积 
层 ZF Net 2013 ILSVRC 比赛 冠军 能够 參 考 
论文 ZF NetGoogLeNet 2014 ILSVRC 比赛 冠军 Google 发表 的 
论文 Going Deeper with Convolutions 有 详细 介绍 VGGNet 也是 
2014 ILSVRC 比赛 中 的 模型 有意思 的 是 即使 
这个 模型 当时 在 分类 问题 上 的 效果 略 
差 于 google 的 GoogLeNet 可是 在 非常 多 图像 
转化 学习 问题 比方 object detection 上 效果 奇好 它 
也 证明 卷积 神经网 的 深度 对于 最后 的 效果 
有 至关 关键 的 数据 预 训 练好 的 模型 
在 pretrained model site 能够 下载 详细 一点 说来 VGGNet 
的 层级 结构 和 花费 的 内存 例如 以下 INPUT 
224x224x3 memory 224 * 224 * 3 = 150K weights 
0 CONV3 64 224x224x64 memory 224 * 224 * 64 
= 3.2 M weights 3 * 3 * 3 * 
64 = 1 728 CONV3 64 224x224x64 memory 224 * 
224 * 64 = 3.2 M weights 3 * 3 
* 64 * 64 = 36 864 POOL2 112x112x64 memory 
112 * 112 * 64 = 800K weights 0 CONV3 
128 112x112x128 memory 112 * 112 * 128 = 1.6 
M weights 3 * 3 * 64 * 128 = 
73 728 CONV3 128 112x112x128 memory 112 * 112 * 
128 = 1.6 M weights 3 * 3 * 128 
* 128 = 147 456 POOL2 56x56x128 memory 56 * 
56 * 128 = 400K weights 0 CONV3 256 56x56x256 
memory 56 * 56 * 256 = 800K weights 3 
* 3 * 128 * 256 = 294 912 CONV3 
256 56x56x256 memory 56 * 56 * 256 = 800K 
weights 3 * 3 * 256 * 256 = 589 
824 CONV3 256 56x56x256 memory 56 * 56 * 256 
= 800K weights 3 * 3 * 256 * 256 
= 589 824 POOL2 28x28x256 memory 28 * 28 * 
256 = 200K weights 0 CONV3 512 28x28x512 memory 28 
* 28 * 512 = 400K weights 3 * 3 
* 256 * 512 = 1 179 648 CONV3 512 
28x28x512 memory 28 * 28 * 512 = 400K weights 
3 * 3 * 512 * 512 = 2 359 
296 CONV3 512 28x28x512 memory 28 * 28 * 512 
= 400K weights 3 * 3 * 512 * 512 
= 2 359 296 POOL2 14x14x512 memory 14 * 14 
* 512 = 100K weights 0 CONV3 512 14x14x512 memory 
14 * 14 * 512 = 100K weights 3 * 
3 * 512 * 512 = 2 359 296 CONV3 
512 14x14x512 memory 14 * 14 * 512 = 100K 
weights 3 * 3 * 512 * 512 = 2 
359 296 CONV3 512 14x14x512 memory 14 * 14 * 
512 = 100K weights 3 * 3 * 512 * 
512 = 2 359 296 POOL2 7x7x512 memory 7 * 
7 * 512 = 25K weights 0 FC 1x1x4096 memory 
4096 weights 7 * 7 * 512 * 4096 = 
102 760 448 FC 1x1x4096 memory 4096 weights 4096 * 
4096 = 16 777 216 FC 1x1x1000 memory 1000 weights 
4096 * 1000 = 4 096 000 TOTAL memory 24M 
* 4 bytes ~ = 93MB / image only forward 
~ * 2 for bwd TOTAL params 138M parameters 有意思 
的 是 大家 会 注意 到 在 VGGNet 这样 一个 
神经 网络 里 大多数 的 内存 消耗 在 前面 的 
卷积 层 而 大多数 须要 训练 的 參 数 却 
集中 在 最后 的 全 连接 层 比方 上上 面的 
样例 里 全/a 连接/v 层/q 有/v 1亿/mq 权重/n 參/zg 数/n 
总共 神经 网里 也就 1.4亿 权重 參 数 4.4 考虑 
点 组 一个 实际 可用 的 卷积 神经网络 最大 的 
瓶颈 是 GPU 的 内存 毕竟 如今 非常多 GPU 仅仅有 
3/4 / 6GB 的 内存 最大 的 GPU 也就 12G 
内存 所以 我们 应该 在 设计 卷积 神经网 的 时候 
多 加 考虑 非常大 的 一部分 内存 开销 来源于 卷积 
层 的 激励函数 个数 和 保存 的 梯度 数量 保存 
的 权重 參 数 也是 内存 的 主要 消耗 处 
包括 反向 传播 要 用到 的 梯度 以及 你 用 
momentum Adagrad or RMSProp 这些 算法 时候 的 中间 存储 
值 数据 batch 以及 其它 的 相似 版本号 信息 或者 
来源 信息 等 也会 消耗 一部分 内存 5 . 很多 
其它 的 卷积 神经网络 參 考 资料 DeepLearning . net 
tutorial 是 一个 用 Theano 完整 实现 卷积 神经网 的 
教程 cuda convnet2 是 多 GPU 并行 化 的 实现 
ConvNetJS CIFAR 10 demo 同意 你 手动 设定 參 数 
然后 直接 在 浏览器 看 卷积 神经 网络 的 结果 
Caffe 主流 卷积 神经网络 开源 库 之中 的 一个 Example 
Torch 7 ConvNet 在 CIFAR 10 上 错误率 仅仅有 7% 
的 卷积 神经 网络 实现 Ben Graham s Sparse ConvNet 
CIFAR 10 上 错误率 仅仅有 4% 的 实现 Face recognition 
for right whales using deep learning Kaggle 看图 识别 濒临灭绝 
右 鲸 比赛 的 冠军 队伍 卷积 神经网络 參 考 
资料 与 原文 cs231n 卷积 神经网络 