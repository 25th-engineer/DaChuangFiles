{"content2":"什么是人工智能？\n人工智能是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学，是认知、决策、反馈的过程。\n人工智能技术的细分领域有哪些？\n人工智能技术应用的细分领域：深度学习、计算机视觉、智能机器人、虚拟个人助理、自然语言处理—语音识别、自然语言处理—通用、实时语音翻译、情境感知计算、手势控制、视觉内容自动识别、推荐引擎等。\n下面，我们就每个细分领域，从概述和技术原理角度稍微做一下展开，供大家拓展一下知识。\n1、深度学习\n深度学习作为人工智能领域的一个重要应用领域。说到深度学习，大家第一个想到的肯定是AlphaGo，通过一次又一次的学习、更新算法，最终在人机大战中打败围棋大师。\n对于一个智能系统来讲，深度学习的能力大小，决定着它在多大程度上能达到用户对它的期待。。\n深度学习的技术原理：\n1.构建一个网络并且随机初始化所有连接的权重； 2.将大量的数据情况输出到这个网络中； 3.网络处理这些动作并且进行学习； 4.如果这个动作符合指定的动作，将会增强权重，如果不符合，将会降低权重； 5.系统通过如上过程调整权重； 6.在成千上万次的学习之后，超过人类的表现；\n2、计算机视觉\n计算机视觉是指计算机从图像中识别出物体、场景和活动的能力。计算机视觉有着广泛的细分应用，其中包括，医疗领域成像分析、人脸识别、公关安全、安防监控等等。\n计算机视觉\n计算机视觉的技术原理：\n计算机视觉技术运用由图像处理操作及其他技术所组成的序列来将图像分析任务分解为便于管理的小块任务。\n3、语音识别\n语音识别，是把语音转化为文字，并对其进行识别、认知和处理。语音识别的主要应用包括电话外呼、医疗领域听写、语音书写、电脑系统声控、电话客服等。\n语音识别\n语音识别技术原理：\n1、 对声音进行处理，使用移动函数对声音进行分帧； 2、 声音被分帧后，变为很多波形，需要将波形做声学体征提取； 3、 声音特征提取之后，声音就变成了一个矩阵。然后通过音素组合成单词；\n4、虚拟个人助理\n苹果手机的Siri，以及小米手机上的小爱，都算是虚拟个人助理的应用。\n虚拟个人助理技术原理：（以小爱为例）\n1、用户对着小爱说话后，语音将立即被编码，并转换成一个压缩数字文件，该文件包含了用户语音的相关信息； 2、由于用户手机处于开机状态，语音信号将被转入用户所使用移动运营商的基站当中，然后再通过一系列固定电 线发送至用户的互联网服务供应商(ISP)，该ISP拥有云计算服务器； 3、该服务器中的内置系列模块，将通过技术手段来识别用户刚才说过的内容。\n5、自然语言处理\n自然语言处理（NLP），像计算机视觉技术一样，将各种有助于实现目标的多种技术进行了融合，实现人机间自然语言的通信。\nNLP\n自然语言处理技术原理：\n1、汉字编码词法分析； 2、句法分析； 3、语义分析； 4、文本生成； 5、语音识别；\n6、智能机器人\n智能机器人在生活中随处可见，扫地机器人、陪伴机器人……这些机器人不管是跟人语音聊天，还是自主定位导航行走、安防监控等，都离不开人工智能技术的支持。\n智能机器人技术原理：\n人工智能技术把机器视觉、自动规划等认知技术、各种传感器整合到机器人身上，使得机器人拥有判断、决策的能力，能在各种不同的环境中处理不同的任务。智能穿戴设备、智能家电、智能出行或者无人机设备其实都是类似的原理。\n7、引擎推荐\n淘宝、京东等商城，以及36氪等资讯网站，会根据你之前浏览过的商品、页面、搜索过的关键字推送给你一些相关的产品、或网站内容。这其实就是引擎推荐技术的一种表现。\nGoogle为什么会做免费搜索引擎，目的就是为了搜集大量的自然搜索数据，丰富他的大数据数据库，为后面的人工智能数据库做准备。\n引擎推荐技术原理：\n推荐引擎是基于用户的行为、属性（用户浏览行为产生的数据），通过算法分析和处理，主动发现用户当前或潜在需求，并主动推送信息给用户的浏览页面。\n未来人工智能应用领域的展望\n除了上面的应用之外，人工智能技术肯定会朝着越来越多的分支领域发展。医疗、教育、金融、衣食住行等等涉及人类生活的各个方面都会有所渗透。\n未来已来，顺应未来发展大势，让自己的企业具备应对未来发展环境的能力，这就是你最应该做的事情。希望大家能在读了本片文章后，能激发出更大的兴趣去了解、学习人工智能的知识，也许，下一步可以从《人工智能简史》开始。"}
{"content2":"人工智能在医疗领域究竟要怎么玩？\nhttp://news.hc3i.cn/art/201708/40125.htm\n2017-08-14 15:04\n作者:石晨露\n来源：中国数字医疗网\n7月20日，国务院发布《新一代人工智能发展规划》，明确人工智能作为未来国家重要的发展战略，将人工智能再次送上发展快车道。同时，更大胆预测人工智能核心产业的规模将在三年内超过1500亿元，到2030年超过1万亿元。由中国医学装备协会数字医疗技术分会、HC3i中国数字医疗网、无界进修主办的2017中国医学装备协会数字医疗分会年会暨“互联网与新医疗”高峰学术交流会期间，相关行业专家及企业领导以“医疗人工智能，云计算的机遇与挑战”为主题进行了圆桌讨论，场面十分激烈。\n产业链如何看待人工智能和云计算？\n本次讨论对话环节，邀请到上海瑞金医院副院长胡伟国，中国医学装备协会数字医疗分会副会长、南京军区福州总医院陈金雄，金沙江创投董事总经理丁健，北京市医师协会副会长许朔，荣之联医疗事业部技术总监田鑫辉，埃森哲医疗事业部总监李信仪，生命奇点联合创始人、首席科学官佟崴嵬，百度研究院大数据实验室高级产品经理王涛等各个不同领域的相关行业专家。不论从学术研究，还是从应用实践，甚至是投资方面，人工智能和云计算到底面临怎样的机遇和挑战，嘉宾各抒己见，分享了自身的看法和经验。中国医学装备协会数字医疗分会副会长、中山大学医学院计算机中心/产学研合作处周毅主持本次讨论对话环节。\n一、医院视角\n作为一名临床医生，上海瑞金医院副院长胡伟国表示，信息化产业跟医疗的合作可以分为三个阶段：第一，数字医疗。将医院所有的临床资料进行数字化运转，以及现在各类数字化产品的发展；第二，智慧医疗。医院临床数据资料进行数字化之后，进行互联互通建设；第三，认知医疗。就像如今的沃森，沃森毕竟是美国的医生，那么如何进行汉化，绝不仅仅只是文字上的汉化，以及将文献统一等等。胡伟国强调，未来人工智能也好，智慧医疗也罢，认知医疗是未来的一个发展趋势，如何将人工智能作为医生决策的辅助，这是非常重要的。\n同样作为医院的代表，来自中国医学装备协会数字医疗分会副会长、南京军区福州总医院的陈金雄则从宏观和微观角度分享了对于人工智能的看法。宏观层面来说，中国医疗最大的问题就是资源不够，依靠目前的生产方式是无法解决医疗问题的，而是需要借助“互联网+智能医疗”解决。微观层面来说，我们既要仰望星空，更要脚踏实地，踏踏实实把小数据做好，同时做到数据共享、信息共享，为人工智能发展增添更多的燃料。\n二、投资视角\n年被称为人工智能发展元年，作为投资人，自然已经嗅到人工智能市场的“躁动”。投资人如何看待人工智能在医疗行业的发展？更青睐于投资哪些项目？不妨从金沙江创投董事总经理丁健的分享中得到答案。\n丁健表示，目前大家都在用人工智能做大数据处理、自然语言处理（NLP）。丁健认为，虽然自然语言处理在人工智能领域看上去很简单，但是实际上是有误导性的，语音识别和自然语言处理有着本质区别，语音识别只是说，而自然语言处理的真正最高境界是要理解字的含义。对于这方面的投资，目前资方还是处于观望状态，但是对于图像领域比较看好，而且随着数据量的增加，特别是高质量、标签数据的增加，深度学习在这个角度上会有更高的突破。\n与此同时，医疗行业目前最核心的问题在于关于数据的共享和数据的数据化问题。其实不仅仅是医疗界，很多行业都是如此，都觉得大数据是块宝，拼命将自己的数据保护起来。对此，丁健这样比喻到，大数据更像是武功秘籍，每人拿一页根本做不成事情，只有将武功秘籍拼凑在一起，才能发挥它的功效。随着人工智能的发展，特别是深度学习方面，对于数据的质量和完整性将会提出更高的要求，所以数据的共享十分重要。\n三、协会视角\n“众所周知，临床最重要的就是患者安全，只要能够保证安全，医疗互联网也好，人工智能也好，都会发展得很好。”北京医师协会副会长许朔这样说道。那么如何做到保障患者安全，第一要积极，第二要规范。\n许朔表示，人工智能的确改变了人们的生活、就医行为等各方面，同时人工智能对于行业的冲击也是大范围的，任何行业都无法避免。许朔还用两个实例分享了人工智能和大数据在临床的应用，不论是在硅谷考察大大小小的公司，看到很小的沃顿商学院的公司在做临床急救方面的事情，还是旧金山的未来诊所快速进行患者诊断，这些都为我们提供了很好的借鉴。人工智能已经把医疗做成了一个围城，那么怎么突出重围，值得每一个医疗工作者思考。\n四、技术视角\n人工智能的发展，离不开底层技术的支持。荣之联医疗事业部技术总监田鑫辉从技术视角分享了人工智能在医疗行业的发展趋势。包括文字结构化处理、可穿戴设备，甚至包括移动医疗等等这些方面，人工智能的应用都十分广泛，但是这些仅仅只是上层的应用，必须需要底层强大的数据支持。因为采集的数据量的多少以及准确度都会对最终结果产生重大影响。田鑫辉表示，由于这个行业的特殊性，数据的采集要求十分高，这将是一个不断精细化的过程，除了数据共享，区域型数据中心建立、信息标准建立，以及数据的安全性等方面，都是未来真正迈向人工智能时代，需要做的关键工作。\n未来医院真的不需要医生？\n未来医院是否真的不需要医生？胡伟国这样解释道：爱迪生预言没有人使用交流电，比尔盖茨认为16K对人是足够的，如今这些猜想都是错误的，马云所预言的未来医院不需要医生，同样是错误的，为什么？因为医生除了医技、医能以外还有人文关怀，这是对于患者来说不可或缺的人对人的关怀，所以从人文角度来讲，马云的预言是错误的。与此同时，陈金雄用赋能、部分取代、转型三个词表达了自己的看法。\n同样，作为企业方，埃森哲医疗事业部总监李信仪也认为从智能技术和医生合作来看，人工智能是不可能取代医生的，只能是以一种辅助工具的形式帮助医生。\n生命奇点、百度等众多的企业都在布局人工智能，希望可以通过人工智能、云计算技术给医疗行业的发展带来更大的颠覆，但是颠覆更多的理解是创新，用一种新的方式革新整个医疗行业，最终实现全民健康的目标，这也是每一位医疗人始终践行的宗旨！"}
{"content2":"腾讯人工智能AI开放平台上提供了很多免费的人工智能API，开发人员只需要一个QQ号就可以登录进去使用。\n腾讯人工智能AI开放平台的地址：https://ai.qq.com/\n里面的好东西很多，以自然语言处理的人工智能API为例。\n假设我们有一个句子：腾讯AI人工智能开放平台。我们希望用腾讯的人工智能开放平台里提供的自然语言处理API对这个句子进行智能分词。\n用您的QQ号登录腾讯人工智能开放平台，创建一个新的应用：\nhttps://ai.qq.com/\n根据您的实际需要选择自然语言处理的具体类别：\n文本朗读（Text to speech）/语音合成（Speech synthesis）\n语音识别（Speech recognition）\n中文自动分词（Chinese word segmentation）\n词性标注（Part-of-speech tagging）\n句法分析（Parsing）\n自然语言生成（Natural language generation）\n文本分类（Text categorization）\n信息检索（Information retrieval）\n信息抽取（Information extraction）\n文字校对（Text-proofing）\n问答系统（Question answering）\n机器翻译（Machine translation）\n自动摘要（Automatic summarization）\n文字蕴涵（Textual entailment）\n创建应用之后生成的app id和app key要记下来，在代码里要使用。\n新建一个js文件，输入如下代码：\nvar md5 = require('md5'); var app_id = \"2107823355\"; var time_stamp = Date.now() / 1000; var nonce_str = Date.now(); var text = \"腾讯AI人工智能开放平台\"; var app_key = \"LHGNH0usjUTRRRSA\"; var input = \"app_id=\" + app_id + \"&nonce_str=\" + nonce_str + \"&text=\" + encodeURI(text) + \"&time_stamp=\" + time_stamp + \"&app_key=\" + app_key; var upper = md5(input).toUpperCase(); console.log(upper); input = input + \"&sign=\" + upper; var request = require('request'); var oOptions = { url: \"https://api.ai.qq.com/fcgi-bin/nlp/nlp_wordseg\", method: \"POST\", headers: { \"content-type\": \"application/x-www-form-urlencoded\", }, body: input }; console.log(\"request sent: \" + oOptions.body); var action = new Promise(function(resolve,reject){ request(oOptions,function(error,response,body){ console.log(\"response: \" + body); }); // end of request });\n通过nodejs里的request组件, 使用HTTP POST调用https://api.ai.qq.com/fcgi-bin/nlp/nlp_wordseg去消费腾讯人工智能开放平台的自然语言处理的分词API：\n这些代码的详细解释，我已经在我之前的NLP版本里介绍过了：\n使用命令行 node nlp.js即可消费该API并查看结果：\n要获取更多Jerry的原创技术文章，请关注公众号\"汪子熙\"或者扫描下面二维码:"}
{"content2":"以下内容均来自： https://ptorch.com/news/11.html\nword embedding也叫做word2vec简单来说就是语料中每一个单词对应的其相应的词向量，目前训练词向量的方式最常使用的应该是word2vec（参考 http://www.cnblogs.com/bamtercelboo/p/7181899.html）\nWord Embedding\n在自然语言处理中词向量是很重要的，首先介绍一下词向量。\n之前做分类问题的时候大家应该都还记得我们会使用one-hot编码，比如一共有5类，那么属于第二类的话，它的编码就是(0, 1, 0, 0, 0)，对于分类问题，这样当然特别简明，但是对于单词，这样做就不行了，比如有1000个不同的词，那么使用one-hot这样的方法效率就很低了，所以我们必须要使用另外一种方式去定义每一个单词，这就引出了word embedding。\n我们可以先举三个例子，比如\nThe cat likes playing ball.\nThe kitty likes playing wool.\nThe dog likes playing ball.\nThe boy likes playing ball.\n假如我们使用一个二维向量(a, b)来定义一个词，其中a，b分别代表这个词的一种属性，比如a代表是否喜欢玩飞盘，b代表是否喜欢玩毛线，并且这个数值越大表示越喜欢，这样我们就可以区分这三个词了，为什么呢？\n比如对于cat，它的词向量就是(-1, 4)，对于kitty，它的词向量就是(-2, 5)，对于dog，它的词向量就是(3, -2)，对于boy，它的词向量就是(-2, -3)，我们怎么去定义他们之间的相似度呢，我们可以通过他们之间的夹角来定义他们的相似度。\n上面这张图就显示出了不同的词之间的夹角，我们可以发现kitty和cat是非常相似的，而dog和boy是不相似的。\n而对于一个词，我们自己去想它的属性不是很困难吗，所以这个时候就可以交给神经网络了，我们只需要定义我们想要的维度，比如100，然后通过神经网络去学习它的每一个属性的大小，而我们并不用关心到底这个属性代表着什么，我们只需要知道词向量的夹角越小，表示他们之间的语义更加接近。\n下面我们使用pytorch来实现一个word embedding\n代码\n在pytorch里面实现word embedding是通过一个函数来实现的:nn.Embedding\n# -*- coding: utf-8 -*- import numpy as np import torch import torch.nn as nn import torch.nn.functional as F from torch.autograd import Variable word_to_ix = {'hello': 0, 'world': 1} embeds = nn.Embedding(2, 5) hello_idx = torch.LongTensor([word_to_ix['hello']]) hello_idx = Variable(hello_idx) hello_embed = embeds(hello_idx) print(hello_embed)\n这就是我们输出的hello这个词的word embedding，代码会输出如下内容，接下来我们解析一下代码：\nVariable containing: 0.4606 0.6847 -1.9592 0.9434 0.2316 [torch.FloatTensor of size 1x5]\n首先我们需要word_to_ix = {'hello': 0, 'world': 1}，每个单词我们需要用一个数字去表示他，这样我们需要hello的时候，就用0来表示它。\n接着就是word embedding的定义nn.Embedding(2, 5)，这里的2表示有2个词，5表示5维度，其实也就是一个2x5的矩阵，所以如果你有1000个词，每个词希望是100维，你就可以这样建立一个word embedding，nn.Embedding(1000, 100)。如何访问每一个词的词向量是下面两行的代码，注意这里的词向量的建立只是初始的词向量，并没有经过任何修改优化，我们需要建立神经网络通过learning的办法修改word embedding里面的参数使得word embedding每一个词向量能够表示每一个不同的词。\nhello_idx = torch.LongTensor([word_to_ix['hello']]) hello_idx = Variable(hello_idx)\n接着这两行代码表示得到一个Variable，它的值是hello这个词的index，也就是0。这里要特别注意一下我们需要Variable，因为我们需要访问nn.Embedding里面定义的元素，并且word embeding算是神经网络里面的参数，所以我们需要定义Variable。\nhello_embed = embeds(hello_idx)这一行表示得到word embedding里面关于hello这个词的初始词向量，最后我们就可以print出来。"}
{"content2":"实验环境：Windows 7 / Python 3.6.1 / CoreNLP 3.7.0\n一、下载 CoreNLP\n在 Stanford NLP 官网 下载最新的模型文件：\nCoreNLP 完整包 stanford-corenlp-full-2016-10-31.zip：下载后解压到工作目录。\n中文模型stanford-chinese-corenlp-2016-10-31-models.jar：下载后复制到上述工作目录。\n二、安装 stanza\nstanza 是 Stanford CoreNLP 官方最新开发的 Python 接口。\n根据 StanfordNLPHelp 在 stackoverflow 上的解释，推荐 Python 用户使用 stanza 而非 nltk 的接口。\nIf you want to use our tools in Python, I would recommend using the Stanford CoreNLP 3.7.0 server and making small server requests (or using the stanza library).\nIf you use nltk what I believe happens is Python just calls our Java code with subprocess and this can actually be very inefficient since distinct calls reload all of the models.\n注意 stanza\\setup.py 文件临近结尾部分，有一行是\npackages=['stanza', 'stanza.text', 'stanza.monitoring', 'stanza.util'],\n这样安装后缺少模块，需要手动修改为\npackages=['stanza', 'stanza.text', 'stanza.monitoring', 'stanza.util', 'stanza.corenlp', 'stanza.ml', 'stanza.cluster', 'stanza.research'],\n三、测试\n在CoreNLP工作目录中，打开cmd窗口，启动服务器：\n如果处理英文，输入\njava -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000\n如果处理中文，输入\njava -Xmx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -serverProperties StanfordCoreNLP-chinese.properties -port 9000 -timeout 15000\n注意stanford-chinese-corenlp-2016-10-31-models.jar应当位于工作目录下。\n可在浏览器中键入 http://localhost:9000/ 或 corenlp.run 进行直观测试。\nPython示例代码：\nfrom stanza.nlp.corenlp import CoreNLPClient client = CoreNLPClient(server='http://localhost:9000', default_annotators=['ssplit', 'lemma', 'tokenize', 'pos', 'ner']) # 注意在以前的版本中，中文分词为 segment，新版已经和其他语言统一为 tokenize # 分词和词性标注测试 test1 = \"深蓝的天空中挂着一轮金黄的圆月，下面是海边的沙地，都种着一望无际的碧绿的西瓜，其间有一个十一二岁的少年，项带银圈，手捏一柄钢叉，向一匹猹尽力的刺去，那猹却将身一扭，反从他的胯下逃走了。\" annotated = client.annotate(test1) for sentence in annotated.sentences: for token in sentence: print(token.word, token.pos) # 命名实体识别测试 test2 = \"大概是物以希为贵罢。北京的白菜运往浙江，便用红头绳系住菜根，倒挂在水果店头，尊为胶菜；福建野生着的芦荟，一到北京就请进温室，且美其名曰龙舌兰。我到仙台也颇受了这样的优待……\" annotated = client.annotate(test2) for sentence in annotated.sentences: for token in sentence: if token.ner != 'O': print(token.word, token.ner)"}
{"content2":"从去年开始，陆陆续续学习了大半年的机器学习，现在是时候做个总结了。\n在以往的编程经验里面，我们需要对于输入有一个精确的，可控制的，可以说明的输出。例如，将1 + 1作为输入，其结果就是一个精确的输出 2 。并且不论怎么调整参数，都希望结果是2，并且能够很清楚的说明，为什么结果是2，不是3。这样的理念在传统的IT界，非常重要，所有的东西就像时钟一般精确，一切都是黑白分明的。由于这种严格的输入输出，衍生出很多对于程序的自动测试工具，你的程序无论怎么运行，都应该在相同输入情况下，得到相同的，准确的，精确的输出。\n但是，如果你进入机器学习的世界，则一切都是基于一个准确率。换句话说，你的模型，允许是不完美的，1 + 1，结果可以是 2.01，也可以是1.98。有时候，如果你的模型要追求完美，则可能出现过拟合的可能性。也就是说，由于你的模型太过于完美，使得模型可以很好的匹配训练用数据，反而失去了通用性，在数据发生变化的时候，发生错误。\n举个例子来说吧，如果一个男孩子说喜欢某个女孩子，这个女孩子身高178，籍贯是辽宁抚顺，专业是计算机。如果机器学习发生过拟合的时候，它就会输出这样一个模型\n如果 身高 = 178 ，籍贯 = 抚顺 ，专业 = 计算机 则喜欢。\n这个模型如果用来匹配一个个例，则这个模型是完美的！\n但是，如果这个女孩子身高是179呢，这个模型会告诉你，这个男孩子不喜欢她。其实，对于男孩子来说，178和179其实没有什么很大的区别。但是由于计算机想精确给出男孩子喜欢女孩子的模型，所以，计算机做出了过拟合的模型。\n当然，一般来说，计算机的模型应该是有弹性的。\n身高在 【175，185】之间\n籍贯是 东北\n专业是 IT相关的\n这样的话，模型虽然会把一些男孩子不喜欢的女孩子也错误的标识出来，但是大部分的样本还是可以比较好的预测出来的。\n机器学习追求的不是100%的正确，而是一个可以容忍的正确率。\n当然，在某些时候，还需要一些风险策略的，例如，在人工智能判断一个用户是否能够发给信用卡的时候，并不是说，这个人51%的可能性是一个讲信用的人，就发卡，而是这个人95%是讲信用的人的时候，才发卡的。机器给出的只是一个估计值，最后还是要人工控制风险的。\n机器学习，很多人认为是一个高科技的IT技能，其实，一个好的机器学习模型，领域里的业务知识还是很需要的。而且现在很多工具可以帮助大家建立程序，完全不需要什么编程的技能，只需要给机器“喂”数据，调节参数，就可以获得结果了。\n给机器“喂”什么数据，那些数据的特征值是有用的，那些特征值没有价值，这个就是领域专家思考的问题了。\n男孩子喜欢女孩子，这时候 颜值，身材，脾气 可能是比较关键的特征值，喜欢可口可乐还是百事可乐则变得基本没有什么价值。如果你的数据里面，都是女孩子喜欢那个牌子的可乐，这样的数据训练出来的模型没有任何意义。当然，如果你有很多特征值，还是有一些自动化的计算帮你挑选用那些特征值的（主成因分析）。\n在机器学习中，有一些复杂的概念，往往都是由一个简单的概念扩展开来的。\n卷积神经网络为首的一些神经网络的概念，都是从感知机这个小家伙来的。\n感知机的输出，是由输入和权重决定的，在监督学习中，输入和输出是已知的，然后机器学习通过不停的调整权重，使得感知机的输出（模型）和实际的输出（样本）尽量一致。这个过程中，学习结果就是这些权重，权重知道了，模型就定下来了。一个最简单的感知机的应用就是线性单元。\n零基础入门深度学习(1) - 感知器\n零基础入门深度学习(2) - 线性单元和梯度下降\n单个感知机是弱小的，但是，如果感知机有成千上万个，然后一层一层一层叠加起来呢。。这些小家伙就变成强大的神经网络了\n贝叶斯，马尔科夫同志则共享了很多关于概率的机器学习。\n贝叶斯最大贡献如下。\n在“你家隔壁住着老王（B）”的前提下，“你的孩子长得像隔壁老王（A）”的概率\n等于“你的孩子长得像隔壁老王（A）”的前提下，“你家隔壁住着老王（B）”\n乘以：“你的孩子长得像隔壁老王（A）”的概率（和隔壁是否住着老王无关）\n除以：“你家隔壁住着老王（B）”的概率\n当然这个正统说法要牵涉到先验概率，后验概率。\n从最简单的伯努利分布，到关于分布的分布的变态级别的狄利克雷分布，很多机器学习都在追求模型最符合抽样的分布概率。换句话说，就是希望从概率学上看，我们做出来的模型，和我们看到的样本之间，看上去是最相似。（最大似然）\n例如，我们要做一个模型，表示抛一枚硬币有多大概率正面向上。如果我们的样本告诉我们，10次里面，有7次正面向上，则我们说这枚硬币70%会出现正面向上。这个模型的结论和样本之间，从概率学上看是最有可能的。\n我们做的模型，就是追求和实际样本的结果，在概率学上看，是最有可能发生的情况。\n最快梯度下降则几乎出现在所有的迭代算法中。\n为什么梯度下降特别重要，因为大部分的算法都是尽可能将损失函数降低，怎么才能将损失函数降低，就是不停调整参数（权重），权重调整的方向，和梯度下降的方向是一致的。当然，最快梯度下降有可能不会收敛到全局最低点。（能否收敛到全局最低点，和初始位置有关）\n机器学习和自然语言处理也是密不可分的。在很多自然语言处理中，将大量使用机器学习的概念。马尔可夫链和条件随机场，狄利克雷分布这些都是自然语言处理的基础理论。\n关注公众号 TensorFlow教室 深度学习，机器学习，自然语言处理。"}
{"content2":"https://mp.weixin.qq.com/s/trkCGvpW6aCgnFwLxrGmvQ\n撰稿 & 整理｜Debra 编辑｜Debra\n导读：在 2018 云栖人工智能峰会上，阿里巴巴推出的人工智能产品和相关服务真不少，包括一款天猫精灵人机交流车载系统，两款搭载天猫精灵系统的移动机器人太空蛋、太空梭，汽车战略重大升级，推出车路协同系统以及首款 L4 车辆协同自动驾驶新能源车。AI 前线对这些产品介绍做了整理，希望可以帮助大家了解阿里这段时间在智能语音、自动驾驶方面的最新研究和进展。\n天猫精灵人机交流车载系统\n阿里巴巴自从 2017 年 7 月推出天猫精灵品牌之后，已经陆续推出了多款 AI 智能产品，其中大家最熟悉的可能是智能音箱天猫精灵。据达摩院人工智能实验室产品总负责人杜海涛介绍，目前，智能音箱天猫精灵销量已破 500W 台，具备 700 多项能力，连接了近 7000 万可用家庭电器，每天调用峰值达 4000 万次，每天陪伴在人身边的时长达 1 小时。数据显示，2018 年第一季度，天猫精灵已经以 110 万的出货量占据了中国智能音箱市场 59% 的市场份额，并做到了中国第一、全球第三的位置，成为阿里人工智能落地产品中的代表，引人注目。\n在今天的人工智能峰会上，天猫精灵的另一项新发布，同样吸引人的眼球，那就是天猫精灵人机交流车载系统。据介绍，它包括在阿里的天猫精灵汽车 AI＋计划之内。\n据介绍，这个系统主要围绕导航、娱乐、通讯需求而开发，采用了阵列增强技术，在车内拥有 10dB 以上的干扰消除能力，语音唤醒日常环境下准确率达到 95%，语音识别率达到了 93%；搭载声纹识别技术，全球首用智能语音支付场景，其语音合成技术可以贴合人声自然度 90% 以上。\n天猫精灵人机交流车载系统具有车内人机交互、人车互动、娱乐服务和家车互联四项主要功能。其中声纹技术迁移到车载系统，在人机交互中可以发挥抗噪的功能；人车交互包括语音调用查询车辆状态等功能；娱乐服务包括听新闻、音乐和电台，也可查询天气、股票订餐和票务等信息；家车互联旨在将家和车辆信息打通。官方称，阿里的家车互联已经支持 164 个智能平台，并且联合了 300 多个品牌，支持 1000 多款设备。\n早在今年 6 月，阿里巴巴天猫精灵就已经与沃尔沃、宝马、奔驰、奥迪四家车企达成合作，具有联网功能的车辆均可以在未来搭载天猫精灵人机交流车载系统。阿里透露，明年将与沃尔沃全线车辆达成合作，落地内置天猫精灵。\n阿里将开未来酒店，用上天猫精灵太空蛋\n在峰会上，天猫精灵发布了新系统：Aligenie 3.0。它具备听、说、看、行动的能力，可以实现精准定位、自助导航、环境感知、传感器融合、人机交互、多机器人协同，实现语音、视觉、多模态交互功能。\n此外，天猫精灵家族再添两名新成员：太空蛋和太空梭，将分别用于未来酒店和医院等设施。天猫精灵太空蛋可以接收天猫精灵的指令，乘坐自动电梯进行物品快送，也可自动去储备仓。官方表示，太空蛋将会用于第一家阿里未来酒店。\n另外一款机器人太空梭将会用于医院等设施，它内置了 60 个独立药仓，可用语音控制进行非接触式无菌操作，乘坐自动电梯进行物品快送。这两款产品都内嵌了 Aligenie 3.0 系统。\n首创智能感知基站，车路协同系统\n峰会上，阿里巴巴集团宣布升级汽车战略：由车向路延展，利用车路协同技术打造全新的“智能高速公路”。这一战略将由 AliOS 联合阿里云、达摩院、高德、支付宝、千寻位置、斑马网络等共同完成，旨在探索未来二十年的路。达摩院人工智能实验室首席科学家王刚对车路协同系统做了详细介绍。\n他指出，自动驾驶其实早已不是一个新的研究课题，但是为什么经过长时间研究到现在也没有完全实现自动驾驶和商业化呢？开发人员也许应该反思他们的技术路径和方法是否可以优化。\n在他看来，其中一个原因就在于过去几十年自动驾驶领域专注于单车智能优化，但单车智能系统即使智能程度达到非常高的水平，也会面临很多问题，如感知盲区、死角、障碍物会导致真实环境中的安全隐患；车载传感器不购灵敏，即使是最好的激光雷达能感知到的行人距离仅有几十米，而且系统极度依赖高精度地图，一旦发生特殊情况，系统处理就会出问题。这是单车智能系统面临的非常困难或者根本不可能克服的难题；单车智能系统的另一个问题是成本，据统计，2018 年自动驾驶车辆均价为 20 万美元，高昂的成本阻碍了自动驾驶技术的发展。\n所以，阿里认为打造安全可靠、成本降低的自动驾驶车辆，需要将车辆自身和道路设施结合起来，利用“聪明”的道路和道路设施来解决问题，道路数据协同共享还可以降低成本。基于此，阿里推出了车路协同系统。协同智能系统能够提高安全性能，在“看”、“想”、“做”（分别对应着自动驾驶车辆感知、决策、控制三个部分）三个方面提高安全性能、降低成本。王刚重点介绍了车路协同系统在感知上的功能，他表示，车路协同智能系统中的感知基站，可以做到无死角、精准识别路面状况、互联互通车辆、全覆盖感知，而没有感知距离的限制；在决策上，相对单车智能的局部最优，协同智能可以做到全局最优。\n目前，阿里已经和交通运输部公路研究院成立了车路协同的实验室，未来会将技术开放给产业。\n首款 L4 车路协同自动驾驶新能源车\n峰会上，阿里还发布了首款 L4 新能源车，搭载了协同智能系统，能够实现“车端 - 路端 - 云端”三位一体的车路协同智能。\n据介绍，阿里发布的自动驾驶智慧物流车前后和两侧使用 Velodyne 的 16 线激光雷达，车顶安装一个 Velodyne 32 线激光雷达，一个双目摄像头，5 个单目摄像头，其他的传感器，如 RTK、超声波雷达等则隐藏在车身中。\n该车在城市道路中的行驶速度在 30 到 40 公里左右，载重在几吨的级别，定位精度在 20 厘米以内。\n阿里巴巴人工智能实验室表示，这款定位于 L4 级别的自动驾驶智慧物流车从去年开始研发，目前仍处于测试阶段，离量产还需要一些时间。\n除此之外，阿里巴巴还喜提杭州市第一张自动驾驶路测牌照，王刚代表阿里“无人车”团队，接过了这张车牌号为“浙 A4390 测”的牌照。据悉，这是继此前阿里曝光无人车、车路协同技术方案后，在自动驾驶方面的最新进展。\n不只消费级产品，阿里AI Lab走向何方？\n2017 年，阿里巴巴将马云投入上亿美元的 Pepper 机器人项目中止，把人员队伍拆分划进人工智能实验室（阿里 AI Labs）。自此，主导智能音箱项目的阿里巴巴人工智能实验室潜伏地下，默默攻关有半年之久，致力于打造“阿里巴巴人工智能实验室首款消费级人工智能产品”。\n2017 年 8 月，阿里人工智能实验室推出了天猫精灵 X1 智能音箱和智能语音系统 AliGenie，正式入局智能音箱市场。\n2018 年 3 月 22 日，阿里巴巴人工智能实验室在北京召开新品发布会上公布了几项新产品：新版交互引擎 AliGenie 2.0：在第一代中文语音交互的基础上，引入了「听觉」、「视觉」、「触觉」及「情感反馈」的多模态交互能力；「精灵火眼」+ XHolder：天猫精灵手机 APP 中新增「精灵火眼」功能，搭配连接硬件 XHolder，便可将智能手机秒变音箱显示屏幕，同时为天猫精灵增加视觉能力；天猫精灵曲奇版：一款 Mini 智能音箱，同样搭载 AliGenie 语音系统，可用于播放音乐、电台，也可购物、控制能家居，售价为 299 元；天猫魔屏：一款 3D 智能投影仪，覆盖华数、优酷、土豆等多个内容平台，可支持天猫精灵语音操控。\n截至当时，天猫精灵的总销量已经突破 200 万台，累计回答了超过 1 亿个问题，执行了 9 亿次任务。此外，天猫精灵在智能家居生态上也进一步扩容，目前可连接 4500 万台家用电器，并联合联发科发布了蓝牙 Mesh 5.0 协议。\n直到 2017 年 10 月，长于自然语言理解、实体挖掘的聂再清担任 AI Labs 北京研发中心总负责人，擅长计算机视觉的李名杨任 AI Labs 机器视觉杰出科学家。两位专家的加入，预示着 AI Labs 未来的产品方向除了天猫精灵这一语音交互产品外，很可能会推出基于视觉交互，甚至“机器人”类型的人工智能产品，重新恢复对机器人的研发。\n果不其然，在云栖大会上，AI Labs 发布了两款面向 B 端企业用户的服务型机器人，一款室内送货机器人、一款室内补货机器人，其操作流程与天猫精灵语音系统全面打通，而支付环节则打通了支付宝。而这与去年 7 月该实验室发布智能音箱时给自己设立的「专注于消费级 AI 产品」的定位，实际上已经有所偏差。\n在团队组成上，阿里人工智能实验室过去一年里也发生了一些变化。\n其中，浅雪（本名陈丽娟）已由 2017 年发布天猫精灵 X1 智能音箱和智能语音系统 AliGenie 时的智能生活事业部总经理，变为人工智能实验室负责人。杜海涛也由原来的高级产品专家升为人工智能实验室产品总负责人。而王刚则带领无人驾驶研发团队独立出来，组建了达摩院智慧交通实验室。\n从今天阿里在峰会上公布的几项重大发布中，我们不难发现阿里对于“车”的重视，从天猫精灵人机交流车载系统、车路智能协同系统、首款搭载车路智能协同系统 L4 新能源汽车，到智慧物流车，无一不体现着阿里对自动驾驶的野心。另一方面，阿里人工智能实验室的定位，也由原来的专注于“消费级 AI 产品”，开始向机器人、智能货运等方向拓展。\n未来，它将在这几个方向如何发力？如何在 AI 实验室遍地开花的时代下脱颖而出？我们拭目以待。"}
{"content2":"如火如荼的人工智能行业正在世界范围内蓬勃发展，特别是谷歌公司的Alpha Go程序战胜了围棋界的顶尖高手，令世人看到了计算机在人类擅长的领域也有一定的思维演绎能力。伴随计算机运算能力的飞速提升和人工智能领域的算法的不断改进，之前处在实验室阶段的Demo，正逐步变得实用，有些工作岗位甚至被淘汰，但同时也衍生出了人工智能在各行业交叉学科的工作岗位。本文从几个方面浅谈人工智能在医疗行业的应用。\n人工智能在医疗电子病历系统中的应用\n对于国内的医疗机构来说，医生在患者就诊过程中必须填写检查诊断信息，这对接诊量大的医生来说是一个非常繁重的劳动，以至于有的医生很反感写病历，天书病历也由此诞生。随着电子病历的推广，使得医生在规范化方面更进一步，但是这要求医生熟练使用相应的电子病历软件，对于年轻的医生来说不算太重的负担，这些医生都是伴随中国互联网发展成长起来的新一代，但是对于老一辈的医生，对电子病历系统很生疏，经常是医院的制度要求迫使老医生花费大量时间耗费在电子病历的输入上。有些年轻的医生为了应付差事，将其他患者的病历拷贝过来，稍作修改为下一个患者的病历诊断信息，这表面上提高了工作效率，但是也存在着极大的隐患，有的医生在对拷贝的信息进行修改了，往往遗漏了很多关键信息，甚至患者的姓名和性别都忘记修改。为了进一步减轻医生在电子病历上耗费的时间，电子病历厂商研发出了各种科室的输入模板，并且禁止两个患者之间的病历信息拷贝，从源头上堵住了医生随意复制粘贴电子病历的可能。从原理来说，电子病历厂商的目的是解决了医生的病历填写规范性的问题，但是医生从手写大量的文字转移到了选择大量结构化的菜单，有的病历模板选择项目非常多，医生的劳动强度并没有因此减轻。\n传统的软件厂商往往为了解决医生的一个问题，引入了另外的一个问题，整个软件系统臃肿不堪，叠床架屋式的解决方案只能使得问题复杂化。病历数据必须准确、清晰和完整，但同时医生填写病历的劳动强度必须减轻，所以语音录入系统由此诞生了。语音录入系统仅仅需要医生口述患者病情，即可将语音转换成文字填写到病历系统中。语音输入的速度比打字的速度快很多，平均每分钟可以录入200以上的汉字。语音录入系统也面临着两个问题：首先，使用者的口音问题和环境噪声对语音识别的准确率有非常大的影响，如果先用语音录入填写完成一份病历，然后再用键盘方式进行修订，总共所花费的时间比只用键盘方式录入花费的时间还长；另外一个问题是医疗术语的复杂性，有的医疗术语没有统一的发音，尤其是遇到中英文混排时，情况变得异常复杂。好在现在的深度学习和自然语言处理混合处理技术能够比较好地解决上述两个问题。在北京协和医院，北京云知声地语音录入系统已经在全院推广，在某些科室反馈还不错。\n2. 人工智能在医疗图像识别中的应用\n现代医疗科技的一个显著特点就是引入了其它领域的技术作为辅助的诊疗手段，例如：X光，超声检查、核磁共振检查等等，这些检查手段有一个共同特点，都需要医生根据仪器采集到的图像数据判断患者的病兆情况。国内的三甲医院面对众多的患者，需要医生在段时间内对每一个患者的影像数据作出快速、准确的判断,这不仅考验医生的眼力，对快速诊疗技术的要求也越来越高。目前在医疗行业的图像识别主要集中在静态图片的判断推理上，这需要大量的医疗图片和医疗标准，前期的准备工作量非常大，国内目前还处于起步阶段，离实用化还有一段距离。另外，最关键的动图识别，人工智能在这方面也处于积累数据阶段。\n3. 人工智能在医疗信息系统自动化中的应用\n医院是一个相对封闭的系统，所有的医疗数据都是内部孤岛共享，亟待人工智能技术进行发掘，医院内部采用的各种信息系统由多家厂商开发完成，很难进行集成。语音识别技术让医生可以通过语音命令调用信息系统中的菜单，进行各种输入和操作，这特别适用于医院需要多人协助的操作，比如超声科、外科手术等操作环境，尤其是超声科。通常在超声科室，主治医生在操作设备时，一边查看实时的图像信息，一边播报看到的图像信息，由另外一名助手完成关键信息的录入。如果通过全语音操控，则可以节省人力成本，提高处理效率。\n以上几点就是人工智能技术在医疗行业遇到的常见解决方案，后续还会再补充。"}
{"content2":"1.2016年 alphaGO 下围棋的关键技术分为两类：一个是决策神经网络，一个是评估神经网络\n2.1997年  IBM公司的深蓝打败了国际象棋冠军\n3.人工智能是指：使机器拥有人类智能行为的计算机科学\n机器学习：是一种随着经验积累，自动提高性能，完成特定任务的计算机程序\n人工神经网络：是一种模仿生物神经网络行为特征，进行信息处理的数学模型\n深度学习：是隐藏层更多的人工神经网络模型\n4.神经网络发展的三次浪潮：\n（1）1950年代中期，第一次浪潮，感知机，罗森布拉特，被称为神经网络创立者\n（2）1980年中期，辛顿，BP（反向传播算法）\n应用：手写体识别，YangLecun\n自动驾驶\n（3）2000年，辛顿，深度学习\n应用：人脸识别、自然语言处理\n总结：BP算法是神经网络发展历程中里程碑式的算法\n链式法则求导能够清晰地表述‘’‘反向传播’含义\n5.人工智能学派简介：\n（1）符号主义：（逻辑主义）心里学派、计算机学派。其原理是物理符号系统假设和有限合理性原理\n符号主义是人工智能的主流派别\n（2）连接主义：仿生学派，生理学派\n主要原理是神经网络及神经网络间连接机制与学习算法，例slphaGO\n（3）行为主义：进化主义、控制论学派\n人工智能的三大巨头：辛顿、YangLecun、BenGio\n6.深度学习的机理：输入——基础特征提取——多层复杂特征提取——权重学习——预测结果"}
{"content2":"郑昀@玩聚SD  20080708\n正如你所知，08年7月1日，微软1亿美金收购了Powerset，合并入LiveSearch。1亿美金对于微软不算什么大钱，按照微软08年第一季度财报，一天的净利润就差不多快7千万美金。但毕竟Powerset是语义世界中的最领先者之一，风头最劲，那么Powerset收进来之后，到底给谁炮火支援呢？微软只是为了要Powerset的人才吗？就像之前声称并购 Yahoo!主要是为了他们的人才或搜索团队？\n这里有三个答案。\n一个是，针对微软整个互联网体系的广告主的。简单地说，就是利用Powerset的语义技术加强Contextual Advertising(粗糙的翻译就是：上下文广告)，提高微软互联网体系中的广告转化率。众所周知，Google拥有地球上最强的关联广告引擎，非 Yahoo!所能望其项背，并与之合作。不光是微软Live Search需要高转化率的Contextual Advertising，MSN、Live等都需要有这个武器，才能在未来的广告世界中勉强站住脚。想想看，Google最近的Google Trends流量分析、Ad Planner、雅虎谷歌广告合作等一系列大的小的布局，用不了几年工夫，估计就赢者通吃整个生物链了。\n另一个是，针对搜索用户的。即Powerset的语义搜索是Google关键词搜索进化的下一步，理应提供给用户更高的用户搜索体验。这里，Powerset往往强调的是，它们能够更精准地体会用户搜索的意图，给出更符合用户需求的搜索结果。\n这里容易被质疑的是，统计表明，目前搜索引擎的用户搜索只有5%的查询是自然语言查询，需要什么样的神奇效果才能让这个比例提高，是一个大问题。\n第三个是，微软惯用的那招，捆绑到它的软件上。我们用的Word等Office软件中都自带搜索，IE中也默认的是LiveSearch。那么，假设后面统统换成Powerset的语义搜索和关联，会是什么情况？你浏览的网页，变得像Wikipedia一样富含信息量，而且是准确的、多媒体的，会不会削减你搜索的次数呢？搜索本身不挣钱，广告才挣钱。不要把技术局限于搜索结果的展示上。\n会是哪个答案？\n我们回过头来，看看Powerset的历史和Vision(愿景)。\n1.Powerset的自然语言处理技术来自于施乐PARC实验室，而且签订的是排他性协议(exclusive license)。2007年2月9日，施乐PARC宣布将大量的专利与技术授权给了一家资金充裕的初创公司，这家公司雄心勃勃：准备开发出一种有朝一日能够超越Google的搜索引擎。而在此之前，Powerset已经获取1250万美金的风险投资。\nSteve Newcomb，Powerset的创始人之一评价PARC这些技术说：“这是已知的最尖端的自然语言技术(the most sophisticated natural language technology known to man)。”\n而在06年11月的访谈中，Google搜索与用户体验部门副总裁Marissa Mayer说：“自然语言真的很难。我认为，未来5年，自然语言搜索不会出现。”\n2.更早些时候。\n三年前，Powerset首席执行官Barney Pell还在一家风投时，一直在寻找搜索的未来是什么。他之前在人工智能(Artificial Intelligence，AI)方面有很深的背景，在实验室掌握了高级AI技术，也在NASA混过。\n他看到，一方面，庞大的计算能力已经变成现实，另一方面，AI和自然语言处理，也似乎在可商用性上准备就绪，这两个趋势是可以聚焦在一起的，像是一个完美风暴的中心。他看到这个让人怦然心动的趋势之后，就开始行动了。\n他先考察了不同团队、研究机构的自然语言处理技术，确定处理大规模计算所需要的特性，自然语言搜索将是什么特性，经济形态是什么样。Barney Pell发现他所需要的技术，在施乐PARC，已经研发了三十年，已经到了能够走向世界、建立商业形态的临界点了，而且也能够应付大规模计算。于是他和 PARC，和Ron Kaplan谈判。\n与此同时，他还在富士施乐的Palo Alto labs也找到了另外一组人，目标和基础都差不多。于是，这票人马有着相同的vision和认知，接下来就是跟施乐PARC的漫长谈判。\n最终，两年后，也就是08年5月，他们推出了第一个beta产品，主要是基于Wikipidea的数据展示了他们的语义搜索能力，以后会扩展到更大范围的计算。扩展到全网，他们面临的困难主要是计算能力上，语义引擎分析并索引一个网页可能需要数秒到20秒，计算量可想而知，1250万美金真不够花的。不过，并入微软之后，钱和计算能力倒不用担心了。\n从他们的Vision来看，似乎是答案2。\n但我认为微软的答案是1+3。你呢？\n资料：\n1：《Can natural language search bring down Google?》\n2：《Microsoft buys Powerset, gets foot in semantic search door》\n3：《Playing with Powerset: search shows promise, has challenges》\n4：《MSFT’s acquisition of Powerset is not about search》\n5：《Interview With Barney Pell and Ramez Naam About Microsoft’s Powerset Acquisition: Integration By End Of Year》\n6：《The Empire Strikes Back: Our Analysis Of Microsoft Live Search Cashback》\n7：《Powerset 加入 Live Search》\n8：《深度：施乐30年技术自然语言挑战Google》"}
{"content2":"2019年机器学习：追踪人工智能发展之路\nhttps://mp.weixin.qq.com/s/HvAlEohfSEJMzRkH3zZtlw\n【导读】“智能助理”的时代已经到来了。机器学习已经成为全球数字化转型的关键要素之一 ,在企业领域，机器学习用例的增长在过去几年中也是显著的。预计机器学习工具和解决方案的企业级采用率将在本十年结束前达到65％ - 并且支出将达到460亿美元（根据IDC报告）。平均而言，55％的企业CIO已将机器学习视为业务加速的核心优先事项之一。在这里，我们将重点介绍2019年机器学习将如何继续发展。\n作者｜Hussain Fakhruddin\n编译｜专知\n整理｜Yingying，李大囧\n机器学习的新用例即将出现\n今年早些时候，美国陆军宣布将使用定制的机器学习软件工具用于战斗车辆的预测性维护。换句话说，机器学习将能够预测车辆可能需要何时以及何种类型的维修服务。另一个有趣的机器学习用例是根据之前股票收益的记录预测股市波动。最近的一项研究表明，用机器学习预测股票市场具有60％以上的准确度。在医疗健康领域，机器学习模型被用于估计一个人的死亡概率（在这种情况下的准确率远远超过90％）。零售，营销和销售以及工业/制造业场景也常有机器学习的用例出现。 “阅读”和“解释”过去的数据并预测未来 - 这是机器学习的本质而技术肯定会越来越精致。\n注意：人工智能应用程序和机器学习工具的概念不再局限于机器人。相反，它们已成为业务工作流程和日常应用程序的自然扩展。\n采用'针对机器学习优化的硬件'将会出现\n2019年很可能是特别准备的硅芯片 - 具有定制人工智能和机器学习功能 - 成为主流，至少对于企业而言。在可预见的未来，人工智能优化硬件市场将继续快速增长。一系列新的，功能强大的处理设备将会出现 - 我们还可以看到高端CPU和GPU。总而言之，这些工具和平台将大大增强机器学习硬件的可用性。\n云计算与机器学习结合\n到2020年，全球云计算市场的年增长率约为25％。企业中机器学习的日益普及是推动这一激增的关键因素。为了成功实施“机器学习文化”，企业必须比以往更加关注创新 - 特别强调改进的云托管和基础设施参数。随着时间的推移，越来越多的“人工智能专用工具和系统”必须存储在云上 - 后者需要具有足够的安全性和可用性标准。强大，可扩展的云支持将帮助企业从机器学习无缝转移到深度学习，为最终用户提供更大价值，并提高他们的ROI数据。\n注意：从2019年开始，一般用户将开始更清楚地了解人工智能和机器学习流程的工作原理 。鉴于人工智能正在其存在的领域（例如：医学科学）的关键性质，人们想要知道技术如何得出其结论/预测是很自然的。\n继续推进胶囊网络\n神经网络的优点是，它们通常不考虑选择对象的相对方向或位置。因此，可能会出现“信息差距”。而胶囊网络就是为了而生的。它们很可能在2019年及以后取代许多传统的神经网络。在性能方面，这些胶囊网络比传统的神经网络系统更具优势 - 具有更准确的模式检测功能，而且在少量数据时，错误概率也大大降低。更重要的是 - 胶囊网络也不需要重复训练迭代，以“理解”变化。\n注意：基于机器学习算法的高级医疗保健模块，用于比较患者的医学图像和其他医疗图像，已经在使用。生物制药公司阿斯利康（AstraZeneca）计划广泛使用机器人和机器学习 - 用于在中国开发智能诊断系统。\n人工智能助手的兴起和崛起\nSiri和Google智能助理以及Alexa已经成为我们日常生活的一部分，而更重要的是，每个顶级“智能助手”都在逐年变得更加聪明（基于5000个一般性问题，Siri设法回答了大约31％，其中近80％是正确答案;在同一项调查中，Google智能助理回答了超过67％的问题，准确度低于88％。随着机器学习范围的扩大，人工智能助手已准备好超越智能家居。从明年开始，现代和起亚将开始在其新车型中提供内置的人工智能虚拟助手系统。这些助手将能够执行无数的任务 - 从远程家庭和汽车控制功能（通过语音）到目的地建议（基于先前的偏好）和导航指南。在所有生活范围内，具有机器学习功能的“智能助手”将使生活变得前所未有的简单。\n注意：智能聊天机器人（具有人工智能）也正在迅速崛起。但是，有必要保持警惕 - 因为训练数据集中的偏差会对用户体验造成严重损害。微软的'Tay'聊天机器人就是这种失败的典型例子。\n开发人员将专注于使用机器学习解决更多“真正的问题”\n当涉及到诸如人工智能（多用途无人机和自动监控摄像头以及自动驾驶汽车等）等技术时，它很容易过火。然而，重要的是要意识到 - 虽然所有这些事情都可以成为现实 - 但是，成熟的数据驱动型生态系统的步骤必须是渐进和系统化的。在2019年，应用程序开发人员和人工智能专家将关注使用机器学习来成功解决真正的重要需求（个人和业务） - 而不是简单地制作新的深度学习工具原型。换句话说，开发人员必须明白人工智能和机器学习不仅仅是几个技术流行语 - 如果实施得当，他们的潜力可能是无穷无尽的。目前还有许多其他技术正在争夺注意力（如4d打印），除非人工智能的发展解决了实际问题，否则投资者可能会开始寻找其他地方。将“人工智能 overhype”与“人工智能事实”分开是至关重要的，并根据后者采取行动。\n注意：在最近的一项研究中，发现89％的CIO计划在其业务中实施机器学习工具和应用程序。\n机器人的世界？\n智能机器人在工作场所的作用正逐渐增加 - 而机器学习的改进是其主要原因。在日本，到2025年，人工智能机器人将提供四分之三的老年人护理服务 - 取代人类照顾者。天元服装 - 一家中国的T恤公司 - 计划在其阿肯色州工厂使用“缝纫机器人”。一般而言，许多劳动密集型任务（特别是不需要太多专业技能的重复性活动）将在不久的将来由“智能机器人”执行。除了使工作流程更智能，提高可用性和可靠性以及缩短产品上市时间外，机器学习驱动的机器人还可以显着降低运营成本（以及外包成本，如果有的话）。提高生产率应该是工作场所全面采用人工智能的直接结果。\n注意：机器学习也可以在精准农业中发挥重要作用。用于农业的智能电杆，具有深根传感器和专用机器学习模块，可以帮助农民做出更明智的决策。\n语音技术脱颖而出\nComScore是否预测到2020年将有50％的搜索活动由语音提供支持，这一点还有待观察 - 但是，语音识别（以及基于此的交互）已经成为一个重要的事实是无法摆脱的机器学习的要素。与早期的语音技术不同，现今的语音识别误码率低于5％ - 这比可用的更多。交互式语音应答（IVR）系统变得比以往任何时候都更加智能 - 由于迭代学习，基于语音的机器学习系统能够转录各种语言/口音。开发人员推出支持语音技术的移动应用程序的趋势预计也将在2019年获得进一步的发展势头。亚马逊Alexa和Google Home等助手已经理解了我们的语音命令 - 他们正在为更多此类平台铺平道路。进入市场。\n注意：传统的，适合的客户服务主管也逐渐被虚拟角色所取代。后者提供更快速的响应 - 并且由于对话是智能的（虚拟代理从之前的对话中学习），因此个人触摸不会丢失。\n美国和中国的人工智能市场 - 大战？\n就人工智能研究和收养而言，北美传统上一直是领跑者。然而，这种束缚正在变得越来越弱 - 中国市场正在成为一股强有力的力量。 2017年，人工智能创业公司在中国的股权融资份额高于美国同行（48％对38％）。中国的人工智能启动场景是整体的（不像北美市场的轻微碎片） - 重点是物流，智慧城市项目，零售，医疗保健，智能农业和其他领域。在深度学习方面，中国显然正在削弱它 - 发布的患者数量比美国多6倍。根据报告，中国希望到2020年与美国人工智能相媲美，并在十年内成为无可争议的机器学习技术领导者。看看美国与中国在未来几年内争夺全球人工智能 / 机器学习霸权的竞争将会非常有趣。\n注意：开发人员不再依赖第三方API，而是越来越多地转向为机器学习应用程序制作自己的API。有许多开发人员友好的装配工具包和移动SDK，以提供必要的帮助。\n更多的机器学习平台（还有更好的平台？）\n像TensorFlow，H2O，人工智能-one和Torch这样的平台已经在如何在不同场景中部署机器学习功能方面发挥重要作用。在即将到来的这一年中，我们可以期待更强大的机器学习平台 - 具有顶尖的分析，分类和预测功能。这些平台的容量与其他API一起使用，大数据也将继续改进。机器学习的不断发展为计算机和移动设备提供了更快“学习”和更好地“解释/分析”数据的机会。\n注意：人工智能 / 机器学习应用程序也在促进自动化决策管理实践。 Informatica和UiPath就是很好的例子。\n彻底改变人类与技术互动的方式\n他们目前可能只出现在少数几个地方- 但'无收银员亚马逊Go'商店正在彻底改变购物的概念。事实上，到2021年，仅在美国就有2000多家“亚马逊Go”商店。我们与智能事物（特别是）和技术打交道，互动，生活的方式（总的来说）正在被人工智能＆机器学习革命所塑造。无论是企业还是社会或智能家居 - 深度学习都将扰乱我们的生活，确保全面提升效率。通过人工智能，科幻电影和我们的想象力似乎已经成为可能。这里的关键是该技术对不同类型用例的适应性。 机器学习正在解决问题并提供价值 - 而这正是它越来越受欢迎的原因。\n注意：用于战争的“杀手机器人”的开发可能是令人震惊的。最近的一份报告预测，人工智能在军事应用方面不断增加的投资很可能导致2040年至2050年之间的核战争。\nNLP变得更加微妙\n作为人工智能的子领域，自然语言处理（NLP）的重要性在过去几年中显着增加。自然语言生成主要用于将数据转换为文本，是许多深度学习系统的关键特征 - 并且用于编写详细的市场摘要或报告 - NLP非常方便。自然语言处理的准确度也不断提高，并且自动化系统能够以无缝方式传达思想。 Cambridge Semantics和Attivio是一些着名的提供NLP服务的公司。\n注意：NLP模块通常需要分析三个方面：语法，语义和上下文。随着机器学习领域的更多进展和新的应用领域被挖掘出来，人工智能专家（而不是技术通才）的需求将继续增长。有一些灰色地带 - 比如大规模失业的前景和可能会进行侵入性监视 - 但可以肯定地说，2019年将成为机器学习的重要一年。 AI-as-a-Service的时代已经到了！\n原文链接：\nhttp://teks.co.in/site/blog/machine-learning-in-2019-tracing-the-artificial-intelligence-growth-path/\n-END-"}
{"content2":"人工智能的神奇之处，在于它能被应用在医疗保健、交通运输和环境保护等方方面面，为复杂的社会问题探寻解决方案。如今，在人工智能的协助下，人们得以探索全新的研究领域，开发创新的产品，让数以百万计的用户从中获益。\n为了使人工智能触手可及，并推动机器学习领域的研究，Google 在 2015 年将机器学习框架 TensorFlow 开源。至今，TensorFlow 已成为世界上最受欢迎的开源人工智能框架，并在GitHub上获得了超过 60,000 个评星 (Stars)。在广受欢迎的 Python 编程语言在线软件知识库 PyPi 上，TensorFlow 的下载次数超过了 90 万，其中 15% 来自中国。\n今天，Google 在中国的开发者网站上提供数百份实用资源，让中国的开发者们能够更便捷地使用 TensorFlow 打造人工智能应用。所有对 TensorFlow 感兴趣的开发者、研究人员或公司都可以在 tensorflow.google.cn 上获取使用指南，在知识库中搜索现有的 API（应用程序编程接口），或是构建模型的方法。如果你是一家希望打造人工智能服务的初创公司，可以在这里清楚地了解到 TensorFlow 的使用方式，并通过实践教程进行学习，而这些资料和教程都完全免费。\nTensorFlow 是一个人人都触手可及的完全开源的机器学习框架，适用于众多领域，在研究工作以及实际产品开发中都能发挥重要的作用。我们还赋予了 TensorFlow 很快的运行速度，并使其能适应更多的应用领域。举例来说，清华大学自然语言处理实验室的研究人员已通过 TensorFlow 来加强他们对语言理解的研究工作；而像京东、小米等众多中国企业也在使用 TensorFlow。与此同时，在中国的初创公司当中，也有不少 TensorFlow 的使用者，CastBox 就是其中之一。CastBox 开发的播客应用程序利用 TensorFlow 对他们的用户偏好进行分析，并向听众提供个性化的推荐。\n尽管人工智能已被越来越多的中国创新者应用于前沿研究，或产品及服务的开发当中，但这仅仅是人工智能无限的潜力的冰山一角。通过今天的更新，Google希望为每一位中国开发者及研究人员带来触手可及的丰富资源，从而让大家可以更加便利地通过 TensorFlow 打造优质的应用，令人工智能真正的造福于人类。\n原文链接：https://mp.weixin.qq.com/s/v4nrNDvI9eZ0nlcoOK02Xg\n更多 TensorFlow 教程：http://www.buluo360.com/"}
{"content2":"NLP汉语自然语言处理入门基础知识介绍\n自然语言处理定义：\n自然语言处理是一门计算机科学、人工智能以及语言学的交叉学科。虽然语言只是人工智能的一部分（人工智能还包括计算机视觉等），但它是非常独特的一部分。这个星球上有许多生物拥有超过人类的视觉系统，但只有人类才拥有这么高级的语言。\n自然语言处理的目标是让计算机处理或说“理解”自然语言，以完成有意义的任务，比如订机票购物或QA等。完全理解和表达语言是极其困难的，完美的语言理解等效于实现人工智能。\n自然语言处理涉及的几个层次：\n作为输入一共有两个来源，语音与文本。所以第一级是语音识别和OCR或分词（事实上，跳过分词虽然理所当然地不能做句法分析，但字符级也可以直接做不少应用）。接下来是形态学，援引《统计自然语言处理》中的定义：形态学（morphology）：形态学（又称“词汇形态学”或“词法”）是语言学的一个分支，研究词的内部结构，包括屈折变化和构词法两个部分。由于词具有语音特征、句法特征和语义特征，形态学处于音位学、句法学和语义学的结合部位，所以形态学是每个语言学家都要关注的一门学科。\nHanlp自然语言处理开发包：\n从事大数据方面工作的人对自然语言处理必然都是不陌生的，在Github上用户量最多的开源汉语自然语言处理工具是HanLP。HanLP的初始版本是在2014年初开发的，3月份的时候开始在Github上开源。2015年的时候集成在了大快搜索的DKNLP中，目前大快已经把DKNLP技术成果已经开源，并且整体装如HanLP项目，HanLP的版本已经到了V1.50。\nHanlp自然语言处理技术优势：\n支持中文分词（N-最短路分词、CRF分词、索引分词、用户自定义词调、词性标注），命名实体识别（中国人民、音译人民、日本人民，地名，实体机构名识别），关键词提取，自动摘要，短语提取，拼音转换，简繁转换，文本推荐，依存句法分析（MaxEnt依存句法分析、神经网络依存句法分析）。提供Lucene查件，兼容Solr和ElasticSearch。\nHanlp自然语言处理应用领域：\nHanlp已经被广泛应用于Lucene、Solr、ElasticSearch、hadoop、android、Resin等平台，有大量开源作者开发各种查件与拓展，并且被包装或移植到Python、C#、R、JavaScript等语言上去。"}
{"content2":"阅读目录\n活动介绍\n报名地址\n时间地点\n会议流程\n精彩预告\n赞助社区\n特别感谢\n【活动介绍】\n微软爱开源，已是尽人皆知的事实。自从收购全球最大的开源社区 GitHub 之后，微软依旧使 GitHub 保持独立运营，并且通过此项举措，微软本身已经成为最大的社区服务者。\n.Net Core 开源后取得了更加快速的发展，目前越活跃用户高达400万人，每月新增开发者45万，在 GitHub 上的月度增长达到15%。目前有来自超过3,700家企业的1.9万开发者在为 .NET Core 做贡献。\n基于微软对.NET开源社区的支持，因此在福州举办首届.NET开源社区线下技术交流会。此次交流会邀请到了三位资深专家作为分享嘉宾，将从Azure云服务、DDD、微服务与大家交流：\n《Azure介绍及应用》\n《我在网龙的DDD实践之路》\n《Hello Microservice》\n活动须知：\n1、此次线下交流会纯公益，免费，感兴趣的同学抓紧时间报名吧。\n2、为了保证活动的质量，使话题能够深入展开，我们将限定参会人数，请您务必提前报名。\n【报名地址】\n报名网址：http://dwr23tij34gntad1.mikecrm.com/XO5VomQ\n报名二维码：报名成功后请加入社区微信群。\n【时间地点】\n时间：2018年11月10日 周六下午 13:30 -- 17:00\n活动地点：蒲公英创新工场(泊寓万象城店)\n具体地址：福州市台江区西二环路荷泽小区荷泽巷32号\n【会议流程】\n13:30 – 13:55 签到\n14:00 – 14:15 郭联钰 开场 《首届福州.NET开源社区启动及介绍》\n14:15 – 14:45 谢鸿凯 《Azure介绍及应用》\n14:50 – 15:30 林靖 《我在网龙的DDD实践之路》\n15:40 – 16:20 马坚 《Hello Microservice》\n16:25 – 16:40 会后总结及合影留念\n16:40 – 17:00 自由讨论\n【精彩预告】\n开场《首届福州.NET开源社区启动及介绍》\n主持人：郭联钰，网龙高级软件工程师,福州.NET开源社区发起者,擅长.NET Core微服务架构及容器运用，具有多年的软件研发经验。\n话题一《Azure介绍及应用》\n分享人：谢鸿凯，福建知鱼科技，云部门客户成功经理，云服务与企业服务领域专家，拥有多年的云服务应用和项目管理经验。\n话题简介：应用产生数据，数据蕴含智慧，如何应用人工智能从大数据挖掘出有价值的知识信息并提供智能化的应用是一个比较有挑战力的话题，在大数据和人工智能领域，dotNET应用并不广泛，基于项目实践经验为大家分享.NET在大数据和人智能领域之中诸如数据采集、自然语言处理、机器学习、结果展示和整体架构设计方面的应用。\n话题二《我在网龙的DDD实践之路》\n分享人：林靖，网龙高级架构师，资深.NET、Kubernetes和容器化解决方案专家，超过10年的软件研发和项目管理经验。\n话题简介：应用产生数据，数据蕴含智慧，如何应用人工智能从大数据挖掘出有价值的知识信息并提供智能化的应用是一个比较有挑战力的话题，在大数据和人工智能领域，dotNET应用并不广泛，基于项目实践经验为大家分享.NET在大数据和人智能领域之中诸如数据采集、自然语言处理、机器学习、结果展示和整体架构设计方面的应用。\n话题三《Hello Microservice》\n分享人：马坚，健康之路高级架构师，微软最有价值专家Microsoft MVP，.NET开源社区贡献者以及福州.NET开源社区发起者，具有多年软件研发项目管理经验。\n话题简介：应用产生数据，数据蕴含智慧，如何应用人工智能从大数据挖掘出有价值的知识信息并提供智能化的应用是一个比较有挑战力的话题，在大数据和人工智能领域，dotNET应用并不广泛，基于项目实践经验为大家分享.NET在大数据和人智能领域之中诸如数据采集、自然语言处理、机器学习、结果展示和整体架构设计方面的应用。\n【赞助社区】\n【特别感谢】\n泊寓万象城店主打活动、社交、资源共享，这里聚集了一群有趣的灵魂，给在城市漂泊的青年一个家。\n泊寓万象城店招租热线15659125561，现在签约立减500。\nasp.net core 交流群：787464275 欢迎加群交流\n如果您认为这篇文章还不错或者有所收获，您可以点击右下角的【推荐】按钮精神支持，因为这种支持是我继续写作，分享的最大动力！\n作者：LouieGuo\n声明：原创博客请在转载时保留原文链接或者在文章开头加上本人博客地址，如发现错误，欢迎批评指正。凡是转载于本人的文章，不能设置打赏功能，如有特殊需求请与本人联系！\n微信公众号：欢迎关注                                                 QQ技术交流群： 欢迎加群"}
{"content2":"1、 你认为什么是人工智能？\n人工智能是一门研究模拟人类智能，实现机器智能的一门科学，研究人员希望机器人不止能够做一些繁重繁琐的工业任务或者数理计算，而是希望机器人能够有独立思考的能力，也就是有自我。通过图像识别，动作识别，逻辑判断，自然语言的处理和反馈以及深层次的数学以及理论思考来体现人工智能的意义。我们现在处于启蒙阶段，并没有真正的制造出可以自我思考能力的机器人，但是在这个方面已经有了初步的探索，比如神经网络的提出和发展，生物计算机的建设，当然，人工智能还需要一段路要走，在不久的将来，真正的人工智能一定能够实现！\n2、 简述推理、学习、存储，三者之间的联系！\n首先，我们先看一下推理、学习、存储在人工智能领域里的说明：推理是人工智能中的最基础问题之一，所谓推理是按照某种策略，从已知事实出发，利用知识推出所需结论的过程；\n学习是获取知识的根本途径，是机器具有智能的重要标志；存储就是根据不同的应用环境通过采取合理、安全、有效的方式将数据保存到某些介质上并能保证有效的访问，总的来讲可以包含两个方面的含义：一方面它是数据临时或长期驻留的物理媒介；另一方面，它是保证数据完整安全存放的方式或行为。存储就是把这两个方面结合起来，向客户提供一套数据存放解决方案。\n由此可知，机器通过学习将知识、动作、方案等存储起来，在解决问题时，通过对问题的分析推理，从而找出最优解决方案。\n3、   “警卫和囚犯”问题的过河方案，使用语义网络进行问题求解。模仿示例画出你的求解方案，并给出一共需要多少步可以成功过河？\n答：警卫用0表示、囚犯要1表示，如图如示：\n初始状态\n000\n111\n第一步：在船里载11到对岸\n000\n11\n11\n第二步：在对岸1再载船返回\n000\n11\n1\n第三步：在船里载11到对岸\n000\n111\n第四步:在对岸1载船回来\n000\n1\n11\n第五步：00载船过去对岸\n0\n00\n1\n11\n第六步：01载船返回\n00\n0\n11\n1\n第七步：00载船到对岸\n000\n11\n1\n第八步：1载船返回\n000\n111\n第九步：11载船到对岸\n000\n1\n11\n第十步：1载船返回\n000\n11\n1\n第十一步：11载船到对岸，过河结束\n000\n111\n总而言之，载船前往返回最少十一次才能完成渡河任务。"}
{"content2":"什么是解析？\n在自然语言的学习过程，个人一定都学过语法，比如句子能够用主语、谓语、宾语来表示。在自然语言的处理过程中。有很多应用场景都须要考虑句子的语法，因此研究语法解析变得很重要。\n语法解析有两个基本的问题，其一是句子语法在计算机中的表达与存储方法。以及语料数据集；其二是语法解析的算法。\n对于第一个问题，我们能够用树状结构图来表示，例如以下图所看到的。S表示句子；NP、VP、PP是名词、动词、介词短语（短语级别）；N、V、P各自是名词、动词、介词。\n实际存储的时候上述的树能够表示为(S (NP (N Boeing)) (VP (V is) (VP (V located) (PP (P in) (NP (N Seattle))))))。互联网上已经有成熟的、手工标注的语料数据集。比如The Penn Treebank Project （Penn Treebank II Constituent Tags）。\n对于第二个问题，我们须要有合适的算法来处理。这也是我们本章将要讨论的内容。\n上下文无关语法（Context-Free Grammer）\n为了生成句子的语法树，我们能够定义例如以下的一套上下文无关语法。\n1）N表示一组非叶子节点的标注。比如{S、NP、VP、N...}\n2）Σ表示一组叶子结点的标注。比如{boeing、is...}\n3）R表示一组规则，每条规则能够表示为X->Y1Y2...Yn，X∈N。Yi∈(N∪Σ)\n4）S表示语法树開始的标注\n举例来说，语法的一个语法子集能够表示为下图所看到的。当给定一个句子时，我们便能够依照从左到右的顺序来解析语法。比如。句子the man sleeps就能够表示为(S (NP (DT the) (NN man)) (VP sleeps))。\n这种上下文无关的语法能够非常easy的推导出一个句子的语法结构，可是缺点是推导出的结构可能存在二义性。比如以下两张图中的语法树都能够表示同一个句子。\n常见的二义性问题有：1）单词的不同词性，如can一般表示“能够”这个情态动词。有时表示罐子；2）介词短语的作用范围。如VP PP PP这种结构，第二个介词短语可能形容VP，也可能形容第一个PP；3）连续的名字，如NN NN NN。\n概率分布的上下文无关语法（Probabilistic Context-Free Grammar）\n因为语法的解析存在二义性，我们就须要找到一种方法从多种可能的语法树种找出最可能的一棵树。一种常见的方法既是PCFG （Probabilistic Context-Free Grammar）。例如以下图所看到的。除了常规的语法规则以外，我们还对每一条规则赋予了一个概率。\n对于每一棵生成的语法树，我们将当中所以规则的概率的乘积作为语法树的出现概率。\n综上所述，当我们或得多颗语法树时，我们能够分别计算每颗语法树的概率p(t)。出现概率最大的那颗语法树就是我们希望得到的结果，即arg max p(t)。\n训练算法\n我们已经定义了语法解析的算法，而这个算法依赖于CFG中对于N、Σ、R、S的定义以及PCFG中的p(x)。上文中我们提到了Penn Treebank通过手工的方法已经提供了一个很大的语料数据集，我们的任务就是从语料库中训练出PCFG所须要的參数。\n1）统计出语料库中全部的N与Σ；\n2）利用语料库中的全部规则作为R；\n3）针对每一个规则A -> B。从语料库中估算p(x) = p(A -> B) / p(A)；\n在CFG的定义的基础上，我们又一次定义一种叫Chomsky的语法格式。\n这样的格式要求每条规则仅仅能是X -> Y1 Y2或者X -> Y的格式。实际上Chomsky语法格式保证生产的语法树总是二叉树的格式，同一时候随意一棵语法树总是可以转化成Chomsky语法格式。\n语法树预測算法\n如果我们已经有一个PCFG的模型，包括N、Σ、R、S、p(x)等參数，而且语法树总数Chomsky语法格式。当输入一个句子x1, x2, ... , xn时。我们要怎样计算句子相应的语法树呢？\n第一种方法是暴力遍历的方法。每一个单词x可能有m = len(N)种取值，句子长度是n，每种情况至少存在n个规则，所以在时间复杂度O(m*n*n)的情况下。我们能够推断出全部可能的语法树并计算出最佳的那个。\n另外一种方法当然是动态规划，我们定义w[i, j, X]是第i个单词至第j个单词由标注X来表示的最大概率。直观来讲。比如xi, xi+1, ... , xj，当X=PP时。子树可能是多种解释方式。如(P NP)或者(PP PP)。可是w[i, j, PP]代表的是继续往上一层递归时，我们仅仅选择当前概率最大的组合方式。特殊情况下。w[i, i, X] = p(X -> xi)。因此，动态规划的方程能够表示为w[i, j, X] = max (p(X -> Y Z) * w(i, s, Y) * w(s+1, j, Z))。关于动态规划方法。leetcode里有不少案例能够说明。\n语法解析依照上述的算法过程便完毕了。虽说PCFG也有一些缺点，比如：1）缺乏词法信息。2）连续短语（如名词、介词）的处理等。但整体来讲它给语法解析提供了一种很有效的实现方法。\n版权声明：本文博主原创文章。博客，未经同意不得转载。"}
{"content2":"一 安装与介绍\n1.1 概述\nSnowNLP是一个python写的类库，可以方便的处理中文文本内容，是受到了TextBlob的启发而写的，由于现在大部分的自然语言处理库基本都是针对英文的，于是写了一个方便处理中文的类库，并且和TextBlob不同的是，这里没有用NLTK，所有的算法都是自己实现的，并且自带了一些训练好的字典。注意本程序都是处理的unicode编码，所以使用时请自行decode成unicode。\n1.2 特点\n# s as SnowNLP(text) 1) s.words 词语 2) s.sentences 句子/分句 3) s.sentiments 情感偏向,0-1之间的浮点数，越靠近1越积极(正面) 4) s.pinyin 转为拼音 5) s.han 转为简体 6) s.keywords(n) 提取关键字,n默认为5 7) s.summary(n) 提取摘要,n默认为5 8) s.tf 计算term frequency词频 9) s.idf 计算inverse document frequency逆向文件频率 10) s.sim(doc,index) 计算相似度\n1.3 安装\npip install snownlp\n二 模块解析\n2.1 seg [分词模块]\n分词库仍以jieba的中文分词效果最佳。\n个人认为：jieba(多种分词模式/用户可自定义领域词汇) / pynlpir (二者可结合) >> snownlp\nfrom snownlp import seg from snownlp import SnowNLP s = SnowNLP(u\"今天我很快乐。你怎么样呀？\"); print(\"[words]\",s.words); # 分词 seg.train(trainDataFileOfPath); # 训练用户提供的自定义的新的训练分词词典的数据集 seg.save(targetDir+'seg2.marshal'); #保存训练后的模型 print(seg.data_path) # 查看 or 设置snownlp提供的默认分词的词典的路径 # [output] [words] ['今天', '我', '很', '快乐', '。', '你', '怎么样', '呀', '？'] D:\\Program Files (x86)\\Anaconda3\\lib\\site-packages\\snownlp\\seg\\seg.marshal\n# 打开其目录seg中的data.txt： 迈/b 向/e 充/b 满/e 希/b 望/e 的/s 新/s 世/b 纪/e 中/b 共/m 中/m 央/e 总/b 书/m 记/e # /b代表begin，/m代表middle，/e代表end，分别代表一个词语的开始，中间词和结尾，/s代表single，一个字是一个词的意思\n2.2 sentiment [情感分析]\nfrom snownlp import sentiment s = SnowNLP(u\"今天我很快乐。你怎么样呀？\"); print(\"[sentiments]\",s.sentiments); #情感性分析 sentiment.train(neg1.txt,pos1.txt); # 训练用户提供的自定义的新的训练分词词典的负面情感数据集和正面情感数据集 sentiment.save('sentiment2.marshal'); #保存训练后的模型 print(sentiment.data_path) # 查看 or 设置snownlp提供的默认情感分析模型的路径 # [output] [sentiments] 0.971889316039116 #有博客博友推荐，设定 value>0.6：正面; value < 20%：负面; 反之：中性 D:\\Program Files (x86)\\Anaconda3\\lib\\site-packages\\snownlp\\sentiment\\sentiment.marshal\n2.3 summary [文本摘要]\n算法基于TextRank。\nfrom snownlp import summary text=u\"https://news.ifeng.com/c/7kdJkIAg 2OO2月26日，四川大学华西医院化妆品评价中心在微信公众号上发布了一条招募信息，希望招募有脱发困扰的试用者，来测试一款防脱育发液产品，该消息很快引发网友关注，北青报记者27日上午从华西医院工作人员处了解到，发布该消息仅一天时间，就已经有8000多人报名，但是医院实际上只需要30名试用者即可。据四川大学华西医院化妆品评价中心发布的消息显示，这次试用者招募有几项目要求，要求试用者年龄为18到60周岁，性别不限，有脱发困扰，目前未参加中心的其他项目。该招募信息称，试用者3月1日到中心领取育发液，回家试用，产品试用期为28天，每周五到中心回访，连续四周，同时，该实验还有相应的报酬。“现在被脱发困扰的人很多，又因为这条信息里有‘脱发’字样，所以很快引起了大家的关注，还上了微博热搜。”华西大学工作人员告诉北青报记者，“我27日上午了解了一下，到目前为止已经有8000多人报名了。”该工作人员表示，这次招募是医院的化妆品评价中心需要为一款产品做测试，需要30名左右的试用者，所以现在有这么多人报名，肯定是需要进行筛选的，“但是现在还无法统计报名的人主要集中地年龄段，不过现在脱发已经有年轻化的趋势，所以应该从18到60岁的报名者都有。这次入选的人会给发放一定的报酬，但是金额应该不会太多，主要是为了解决试用者来医院的交通费用等。”医院工作人员告诉北青报记者，华西医院化妆品评价中心经常会发布一些试用者招募活动，之前这些招募可能并未引起太多人的关注，“这次招募防脱育发液试用者，也让我们的其他招募受到了关注，很多项目都增加了很多报名的试用者。”北青报记者看到，医院的化妆品评价中心之前确实发不过许多试用者招募信息，例如“法国进口改善皮肤暗沉提高皮肤含水量产品招募试用者”、“皮肤封闭式斑贴实验项目志愿者招募”、“男士护肤品免费试用啦”等，对此，有网友表示，“找到了一条免费使用化妆品之路”。\"; s = SnowNLP(text); print(\"[summary]\",s.summary(3)); # [output] [summary] ['华西医院化妆品评价中心经常会发布一些试用者招募活动', '医院的化妆品评价中心之前确实发不过许多试用者招募信息', '希望招募有脱发困扰的试用者']\n三 快速示例教程\n# [code] import os from snownlp import SnowNLP from snownlp import sentiment from snownlp import seg # snownlp - demo text = [u\"今天我很快乐。你怎么样呀？\",u\"苏宁易购，是谁给你们下架OV的勇气\",u\"恐怖\",u\"质量不太好\"]; s = SnowNLP(text[2]) #载入文本 print(\"[words]\",s.words); # 分词 print(\"[sentiments]\",s.sentiments); #情感性分析 for sentence in s.sentences :#分句 print(\"[sentence]\",sentence); pass; #sentiment.train('./neg.txt', './pos.txt');# 重新训练语料模型 #sentiment.save('sentiment.marshal'); # 保存好新训练的词典模型 print(\"[seg.data_path]\",seg.data_path); # 查看seg子模块的词典位置 print(\"[sentiment.data_path]\",sentiment.data_path); # 查看sentiment子模块的词典位置 #seg.data_path = os.path.join(os.path.dirname(os.path.abspath(__file__)),'sentiment.marshal') # 设置词典位置\n# [output] [words] ['恐怖'] [sentiments] 0.293103448275862 [sentence] 恐怖 [seg.data_path] D:\\Program Files (x86)\\Anaconda3\\lib\\site-packages\\snownlp\\seg\\seg.marshal [sentiment.data_path] D:\\Program Files (x86)\\Anaconda3\\lib\\site-packages\\snownlp\\sentiment\\sentiment.marshal\n四 推荐文献\nNLP之淘宝商品评论情感分析 - 基于SnowNLP - 推荐\nNLP从入门到喜欢之jieba分词器 - 推荐\nSnowNLP 中文文本分析器基本用法\nsnownlp 0.12.3 - PYPI - 推荐\n利用目前的三个分词工具(jieba、snownlp、pynlpir)简单的实现了短文本的分词效果\n使用pynlpir增强jieba分词的准确度"}
{"content2":"看到这个题目，估计程序猿们要笑了，大家都懂的嘛。毕竟大家已经被“21天学会XXX”坑了不少回了。所以，这个题目自然是开玩笑的了，怎么可能21天搞定聊天机器人呢。\n虽然2016年号称人工智能元年，各种牛逼的人工智能技术如雨后春笋冒出来，但是要打造一个智能聊天机器人谈何容易，目前还没有一款聊天机器人可以通过“图灵测试”。最近大热的《西部世界》中的阿诺德提到了人工智能的四层金字塔模型，从下往上依次为\"记忆\",\"即兴\",\"私利\",\"?\", (最上面一层剧中尚没有透露), 这也恰恰就是描述我们人类的智商层次。\nOK，言归正传。今天想聊的只是在做聊天机器人(或者叫语音助手)的过程中需要面对的一个小问题，叫做所谓的命名实体识别(NER: Named Entity Recognition)。NER是自然语言处理中的一个课题，比如识别文本中的人名，地名，日期等这些词语。\n比如对于如下一句话：\n“Siri，打电话给Abby\"\n作为Siri来说，她除了要明白你的这句话是让她打电话，同时要知道究竟是打电话给谁。在这句话中，很显然是要打电话给Abby这个人。那自然要从这句话中识别出Abby作为打电话的目标人。那么怎么来实现从一句话红提取出人名这中命名实体呢？ 当然对于人名这种典型的命名实体，已经有很多自然语言处理的工具实现了这个功能，比如NLTK, Stanford NER等。\n但是，今天我想讲的是借助机器学习和简单的NLP来实现定制化的NER。OK，怎么搞呢？既然提到机器学习，第一反应就是“分类”，毕竟机器学习一个很大的应用领域就在于分类。那么对于NER怎么来用“分类”算法做呢？既然我们的目标是从一个句子中定位出相关的\"词\"或\"词组\"，那么很自然这个句子中的每个词或字就可以划分成两个分类：人名，和非人名。\n对上面的这个例子来说，就可以分为下面两个类：\nSiri, 打电话给 ---> 非人名分类\nAbby ---> 人名分类\n因此这个问题就变成了分类问题了！ 只要我们准备足够的语料库（包含可能的说法，和常见的人名）就可以来训练我们的分类模型了。\n具体流程可以参考下面示意图：\n现在你的聊天机器人应该知道怎么打电话了, 是不是很赞。哈哈，其实这种方法可以推广，可以用来训练识别其他的Concept, 不仅仅是人名。\nOK，今天先聊到这里了。\n欢迎扫描二维码，了解更多。"}
{"content2":"整理到第四天越来越心虚了，因为发现好多专业的技术都不知道，比如关于语义的分析提取之类的，后面打算专门针对这些专业性的“玩法”整理下。那么今天，就继续整理下人机问答的原理实现吧！\n更多内容参考：\n自然语言处理扫盲·第一天——自然语言处理的背景、应用、推荐资料\n自然语言处理扫盲·第二天——白话机器翻译原理\n自然语言处理扫盲·第三天——白话情感分析原理\n背景\n在很多的领域其实都需要这样一个问答系统，问答系统有很多种交互的方式。先来目睹一下问答系统的风采吧：\n比如在线的聊天机器人\n想要体验一下的话，可以点击传送门\n比如智能的搜索平台\n关于这种问答系统，好坏的评测很简单，直接人工肉眼就能判断出来回答的是否是自己想要的，因此小白的体验用户也可以直观的评测一个问答系统效果是否足够好。\n基于问答库的实现方法\n首先可以设想一下：\n如果即没有任何高大上的机器学习算法，也没有任何复杂的公式，怎么设计一个问答系统？\n如果现在只使用java和mysql(想要个页面也可以加上js\\html....)，那么只要我们的数据库里问答的内容足够多，就能支撑这样的问答系统。不过数据支持的搜索过于简单，只支持普通的%的模糊搜索。\n接下来，我们可以引入一些分词工具（lucene），或者直接借助全文检索的系统（solr、elasticsearch）。这样我们输入的内容就可以以全文检索的形式进行查询了。不过基于全文检索，即TF-IDF词频逆文档频率算法，效果仍然有限，比如一些词语的位置无法识别、同义词等无法识别。\n为了解决这个问题，可以再引入一些自然语言处理的技术手段，比如实现一些同义词的识别、词性的转换等等。\n总结起来，大致的流程就是：\n判断类型的问题，用于锁定问题的精准范围（可以采用一些分类的机器学习算法）\n信息提取：提取关键词\n基于关键词的搜索，可以直接基于TF-IDF算法搜索，也可以基于word2vec转变空间向量使用相似词进行搜索\n结合问题的主题等信息对候选集的答案进行打分\n返回得分最高的TopN候选答案\n基于知识图谱的实现方法\n基于问答库的做法，已经能实现一个基本的问答系统了。但是他其实并不智能，因为所有的答案都需要事先录入好。没有准备的问题，就无法找到对应的答案....试想一下，人类说话的方式多种多样，稍微变一下，就可能得不到想要的答案，这得是多么烂的系统啊。\n随着最近知识图谱的兴起，人机问答系统又迎来了一波春天....关于知识图谱可以简单的说一下，知识图谱里面有这样几种类型：\n实体，比如刘德华、北京、奔驰\n属性，比如年龄、地址、性别\n关系，比如夫妻、父子、前任\n有了这样的定义，就可以基于我们自己拥有的知识库建立知识图谱了。\n举个例子，在电商领域阿迪、耐克、三楼、白色都是实体，阿迪和白色之间是一种属性的关系，比如颜色,因此可以得到这样的图：\n阿\n迪与耐克之间又有一些关系，比如竞争:\n有了这样的知识图谱之后，我们就可以进行一些智能的搜索了。\n在问答系统系统中一般有这样几种方法：\n基于语义解析\n语义解析一听就是跟语言学有关的东西，由于我缺少这方面的背景，所以只能讲个大概，也许还会有很多纰漏。我们在小学学习语文的时候，老师就教过主谓宾定状补，现在不知道大家还记不记得：\n主语：一个句子的主体，也是整个句子的核心，所有的操作行为的发起者\n谓语：主体做的动作\n宾语：动作的承受者\n定语：描述一些程度\n状语：时间地点\n补语：做一些数量、状态的补充\n比如明明（主语）在不二心（状语）吃（谓语）了六个（补语）超好吃（定语）的肉包子（宾语），而在英文里面词性又不一样了，英文中不是主谓宾定状补而是名词``代次``形容词``副词``动词...一般来说为了统一表示，在自然语言处理里面，都是使用统一的代号进行处理的，比如:\nS表示句子；NP、VP、PP是名词、动词、介词短语（短语级别）；N、V、P分别是名词、动词、介词。\n比如下面两个语法树：\n那么有了这样的语法树，就可以基于语法树形成特定的查询语言，直接基于知识库进行查询了，比如转化成SPARQL进行查询，比如:\nNumber of dramas starring Tom Cruise?\n可以转化成查询语言：\n基于信息提取\n信息提取的方式，跟之前的语义树查询的方式稍有不同。对于这样的问题：\nwhat is the name of Justin Bieber brother?\n会形成语法的依存树，然后根据依存树再生成问题图：\n基于这样的问题图，我们很容易能到知识图谱里面查询到相关的实体，然后得到基于这个实体的子图。得到子图后，就可以从这个实体出发，把所有的关系对应的实体都做为回答的候选集。当然不仅仅是一层的关系，也可以收集两层的图作为候选集。有了这样的候选集就剩下寻找最优答案的问题了，寻找答案的方法有很多，比如抽取答案的特征与问题的特征两两组合，基于朴素贝叶斯算法选取概率最大的候选答案。\n基于word embedding\n说完基于特征的信息提取，就要说说这个word embedding，word embedding是一种概念，即区别于普通的One hot编码。\none hot：一位有效编码，一般都是该维有N个值，那么就会用N-1位来表示该维度的数据，从而方便机器学习进行模型的训练和计算。然而one hot有个缺点就是无法识别词之间的关系，比如父亲和母亲、北京和上海\nword embedding：也叫做词嵌入，常用的方法有word2vec。这种方法是基于大量的语料库去分析词语被表达的向量，中间的过程暂且不谈，最终的效果就是父亲-母亲 = 男-女，也就是说在用夹角余弦计算相似度的时候，同种类型的词语往往相似度都比较高。\n那么在问答系统中，就是对问题进行特征的提取，然后转变成word embedding的表示方法；再对知识库中的答案提取特征，形成word embedding，最终基于计算两者的关系，来寻找最匹配的答案。这种基于word embedding的表达方式寻找最佳的匹配，也被应用到了推荐系统的场景，因此还是值得好好研究一下的。\n基于深度学习\n由于word embedding其实也是基于特征词的，因此在提取特征词的时候都会忽略掉位置等等的因素。比如，谢霆锋的爸爸是谁？和谢霆锋是谁的爸爸?就是两个完全不同的问题。但是在深度学习中，如果使用一些循环神经网络，就可以把每个词作为下一个词分析的输入，也就变相的考虑了词语的顺序特征。具体的就不详谈了，能力有限...\n对深度学习如何在问答系统中应用，可以参考知乎专栏——揭开知识库问答KB-QA的面纱\n参考\n基于KB-QA知识库的问答系统：https://zhuanlan.zhihu.com/p/25735572\n什么是知识图谱：http://www.jianshu.com/p/ae871569af02\n图灵机器人体验处：http://www.tuling123.com/experience/exp_virtual_robot.jhtml?nav=exp\n人机问答系统的工作原理：http://www.csdn.net/article/2014-08-13/2821188\nQA问答系统中深度学习的技术实现：http://www.52nlp.cn/qa%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E5%AE%9E%E7%8E%B0\n百度问答系统：https://www.leiphone.com/news/201702/LDdGVnuiyP9HiPXa.html?viewType=weixin"}
{"content2":"许多语言处理任务都涉及模式匹配。以前我们使用‘stsrtswith（str）’或者‘endswith（str）’来寻找特定的单词。但是下面引入正则表达式，正则表达式是一个强大的模块，他不属于哪一种特定的语言，是一个强大的语言处理工具。\n在Python中使用正则表达式需要使用import re来导入re模块。还需要用于搜索的词汇链表。这里我们再次使用前面使用过的语料库，对它进行预处理消除某些名称。\n>>>import re >>>wordlist=[w for w in nltk.corpus.words.words('en') if w.islower()]\n1、使用基本的元字符\n使用正则表达式《ed$》查找以ed结尾的词汇。使用函数re.search(p,s)检查字符串s中是否有模式p。使用美元符号，在正则表达式中用来匹配单词的末尾。\n>>>print([w for w in wordlist if re.search('ed$',w)]) [['abaissed', 'abandoned', 'abased', 'abashed', 'abatised', 'abed', 'aborted', 'abridged', 'abscessed', ...]\n通配符‘.’用来匹配任何单个字符。假设有一个8个字符组成的字谜，j是第三个字母，t是第六个字母。\n>>>print([w for w in wordlist if re.search('^..t..t..$',w)]) ['abjectly', 'adjuster', 'dejected', 'dejectly', 'injector', 'majestic', 'objectee', 'objector', 'rejecter',...]\n插入字符‘^’匹配字符串的开始。\n2、范围和闭包\n在手机输入法联想提示，九宫格，输入序列4633可以得到hole和golf，还可以产生哪些字符？使用下面正则表达式进行判断：\n>>>print([w for w in wordlist if re.search('^[ghi][mno][jlk][def]$',w)]) ['gold', 'golf', 'hold', 'hole']\n正则表达式中‘+’号表示‘前面项目的一个或者多个实例’。‘*’表示‘前面的项目的零个或者多个实例’。当‘^’出现在方括号内的第一个字符位置时有其他的功能。例如\"[^aeiou]\"匹配除元音字母以外的所有字母。\n下面是另外一些正则表达式的例子。使用一些新的符号：|、{}、和|\n>>> wsj = sorted(set(nltk.corpus.treebank.words())) >>> [w for w in wsj if re.search('^[0-9]+\\.[0-9]+$', w)] ['0.0085', '0.05', '0.1', '0.16', '0.2', '0.25', '0.28', '0.3', '0.4', '0.5', '0.50', '0.54', '0.56', '0.60', '0.7', '0.82', '0.84', '0.9', '0.95', '0.99', '1.01', '1.1', '1.125', '1.14', '1.1650', '1.17', '1.18', '1.19', '1.2', ...] >>> [w for w in wsj if re.search('^[A-Z]+\\$$', w)] ['C$', 'US$'] >>> [w for w in wsj if re.search('^[0-9]{4}$', w)] ['1614', '1637', '1787', '1901', '1903', '1917', '1925', '1929', '1933', ...] >>> [w for w in wsj if re.search('^[0-9]+-[a-z]{3,5}$', w)] ['10-day', '10-lap', '10-year', '100-share', '12-point', '12-year', ...] >>> [w for w in wsj if re.search('^[a-z]{5,}-[a-z]{2,3}-[a-z]{,6}$', w)] ['black-and-white', 'bread-and-butter', 'father-in-law', 'machine-gun-toting', 'savings-and-loan'] >>> [w for w in wsj if re.search('(ed|ing)$', w)] ['62%-owned', 'Absorbed', 'According', 'Adopting', 'Advanced', 'Advancing', ...]\n正则表达式总结如下：\nTable 3-3. Basic regular expression metacharacters, including wildcards, ranges, and closures\nOperator\nBehavior\n.\n通配符，匹配所有字符\n^abc\n匹配以abc开始的字符串\nabc$\n匹配以abc结尾的字符串\n[abc]\n匹配字符集合\n[A-Z0-9]\n匹配字符范围\ned|ing|s\n匹配指定的字符串\n*\n前面的项目的零个或多个（Kleene闭包）\n+\n前面的项目的一个或多个\n?\n前面的项目的一个或零个（可选）\n{n}\n重复n次，n为非负整数\n{n,}\n至少重复n次\n{,n}\n至多重复n次\n{m,n}\n重复多于m次不多于n次\na(b|c)+\n括号表示操作符的范围\n我们在使用re正则表达式时候要可以使用在字符串加一个前缀‘r’表示一个原始字符串。"}
{"content2":"全文检索技术\n全文检索\n是一种面向全文和提供全文的检索技术, 其核心技术是将文档中所有基本元素的出现信息记录到索引库中, 检索时允许用户采用自然语言表达其检索需求, 并借助截词、邻词等匹配方法直接查阅文献原文信息, 最后将检索结果按相关度排序返回给用户。因而索引数据库的建立是全文检索系统实现的基础, 它以特定的结构存储了数据资源的全文信息, 从而为全文检索系统提供可检索的数据对象。\n自然语言处理研究内容的基础部分。\n词法分析、句法分析、语义分析、语用分析、语境分析\n自然语言处理技术在中文全文检索中的应用\n文献信息处理,对文献进行分析, 提取关键信息, 建立转换文档及数据库;\n提问处理, 解释查询提问;\n问题匹配, 将查询提问与转换文档及数据库进行匹配; 对查询结果进行排序处理。\n索引包括：\n字索引\n词索引\n短语索引\n文摘自动生成：\n常用的文摘自动生成的方法是基于统计的方法, 这种方法的基本思想是, 首先对全文进行自动分词, 然后统计文章中各个词出现的频率和权重, 并按照某种准则确定出关键词,将关键词所在的语句抽取出来, 依据各种句子权重指标计算句子综合权重, 选出一组最能代表文献主题内容的句子, 并对句子进行排序作为文摘句, 最后生成文摘 。\n文本分类\n包括自动聚类\n自动归类\n两者的主要区别就是自动聚类不需要事先定义好分类体系, 而自动归类则需要确定好类别体系, 并且要为每个类别提供一批预先分好的对象作为训练文集。\n基于自然语言处理技术的中文全文检索技术的局限\n自然语言处理技术在中文全文检索中的应用深度不够。目前, 自然语言处理技术在全文检索中使用较多的是词法和句法分析, 而在语音、语义和语用方面的应用很少, 即对文本和查询仍停留在对语言结构的分析, 还没有达到概念语义的层面。因而在分析文本的过程中, 系统不能借助上下文语言环境, 正确地推断和选择词汇的含义。\n检索效率不高\n返回的信息过多\n返回重复的信息。\n中文全文检索技术的未来发展方向\n文献信息的深度处理\n未来的标引是按照一定的格式, 建立词法、句法/语义层次的深度标引。\n匹配机制的进一步优化\n未来的匹配机制将达到真正意义上的概念匹配, 匹配在语义上相同、相近、相包含的词语, 使检索更接近人的智能程度, 以减少误检和漏检.\n智能化知识检索\n从内容上真正地理解文献所论述的主题;\n能使用适当的知识表示方法来充分体现各主题概念和标识之间的分、属、交叉等复杂关系;\n能准确在分析用户用各种方式表达的查询要求, 理解用户的真正意图\n具有基于内容的相似性检索、自动分类(自动聚类)和自动摘要、以及知识压缩和去重功能\n跟踪和分析用户的检索行为, 并与用户进行相关反馈, 为用户提供个性化信息服务;\n检索结果自动聚类, 提高检索结果的相关度。\n笔记：主要看看缺陷和发展方向，为写论文做准备。不过这篇文章是07年的。。。看到了几个词：歧义处理、语料库中没有的新词、语义、查询结果相关性差，冗余信息多。全文检索要提升查准率、查全率、查询速度。我有个小思路是：根据查询语句中的关键词，和文章中与此关键词相关的词语做更好的排序。ps：仅是yy而已。"}
{"content2":"word2vec 本来就是用来解决自然语言处理问题的，它在 NLP 中的应用是显然的。\n比如，你可以直接用它来寻找相关词、发现新词、命名实体识别、信息索引、情感分析等；你也可以将词向量作为其他模型的输入，用于诸如文本分类、聚类等各种自然语言处理问题。\n事实上，word2vec 的思想和工具，还可以应用于自然语言处理之外的其他领域。一个词，无非就是个符号；句子是词的序列，无非也就是个符号序列。如果我们能够在其他的应用场景中，构造出一些符号，还有这些符号形成的序列，那我们就可以试一把 word2vec。\n下面是，根据网络上的资料，整理的 word2vec 在自然语言处理领域之外的一些应用。\n【社交网络】\n应用场景：在社交网络中，给当前用户推荐 他/她 可能关注的大V\n映射关系：每一个大V 就是一个词；将每个用户关注的大V，按照关注的顺序排列，形成文章\n【App 商店】\n应用场景：App 商店中，向用户推荐感兴趣的 App\n映射关系：每个 App 就是一个词；将每个用户下载的 App，按照下载的顺序排列，形成文章\n【广告系统】\n应用场景：广告主在媒体网站上打广告，媒体网站提供一个后台管理系统，可以让广告主自行决定要将广告推荐给哪些目标人群。\n映射关系：每一个页面就是一个词；将每个用户浏览的页面，按照浏览的顺序排列，形成文章。\n这样，根据训练后的词向量，就可以计算出页面之间的相关程度。\n那目标用户怎么计算呢？浏览与广告主的广告页 相关的页面 的用户 就是广告主潜在的 目标用户。把这些用户推荐给广告主就可以了。\n应用场景：广告系统中广告主上线了一支新广告，如何估算用户对新广告的 CTR（Click-Through-Rate），即点击通过率。\n映射关系：和上面给广告主推荐目标用户一样的做法，可以计算出每个广告页对应的向量\n然后，对这些广告页做一个聚类，把相似的广告页聚在一个簇中。用新广告所在簇的 CTR 来近似新广告的 CTR。\n【向量快速检索】\n综合以上各种应用，将各种文档转换成向量之后，常见一个基本操作就是输入一个文档（对应的向量），寻找和它最相关的 top k 个文档（对应的向量）。如果要所有文档都比对一遍的话，那时间复杂度就是 O(n)。这在实际的工程应用中就太慢了。因此，需要借助 redis，或者引入 kd-tree, simhash, 聚类等算法来加速检索。\n【 原文链接】http://www.ipaomi.com/2017/09/22/word2vec-在-非-自然语言处理-nlp-领域的应用/\n参考：\nword2vec在工业界的应用场景\n深度学习word2vec笔记之应用篇\nword2vec有什么应用\nA non-NLP application of Word2Vec\nWord2Vec with Non-Textual Data"}
{"content2":"了解对自然语言处理的卷积神经网络\n当我们听到卷积神经网络（CNN）的时候，我们通常会想到计算机视觉。 CNN负责图像分类的重大突破，是当今大多数计算视觉系统的核心，从Facebook的自动照片标签到自动驾驶。\n最近我们也开始将CNN应用于自然语言处理中的问题，并获得了一些有趣的结果。 在这篇文章中，我将尝试总结一下CNN是什么，以及它们如何在NLP中使用。 对于“计算机视觉”用例来说，CNN背后的直觉更容易理解，所以我将从那里开始，然后慢慢地向NLP移动。\nUnderstanding Convolutional Neural Networks for NLP\nWhen we hear about Convolutional Neural Network (CNNs), we typically think of Computer Vision. CNNs were responsible for major breakthroughs in Image Classification and are the core of most Computer Vision systems today, from Facebook’s automated photo tagging to self-driving cars.\nMore recently we’ve also started to apply CNNs to problems in Natural Language Processing and gotten some interesting results. In this post I’ll try to summarize what CNNs are, and how they’re used in NLP. The intuitions behind CNNs are somewhat easier to understand for the Computer Vision use case, so I’ll start there, and then slowly move towards NLP.\n原文链接：http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/\n更多教程：http://www.tensorflownews.com/"}
{"content2":"人工智能浪潮下的思考\n摘要\n这次的选题是以最近比较火的人工智能作为选题，人工智能大概是在2016年开始迎来了它的第一股热浪。人工智能是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。\n人工智能从诞生以来，理论和技术日益成熟，应用领域也不断扩大，可以设想，未来人工智能带来的科技产品，将会是人类智慧的“容器”。人工智能可以对人的意识、思维的信息过程的模拟\n我们耳熟能详的暗知识就是机器通过大量的训练尝试所获得的知识，那些知识是人类目前都没有完全掌握的。这就是深度学习，机器通过试错所学习的知识已经不仅仅是我们人类目前所掌握的知识了，这个时候的智能已经不是人的智能了。\n再说一下人工智能应用的领域：图像处理、语音的识别、无人驾驶、医学(机器看病)、智能家居(像国内的小爱)、无人零售。人工智能相信带给我们的惊喜远远不仅如此，之后会随着越来越多的人加入到人工智能的研究中人工智能会再一次打破我们传统的生活方式给我们带来更多的便捷。\n关键字：人工智能; 智能家居; 智能控制; 智能医疗;\nThinking under the tide of artificial intelligence\nAbstract\nThis selection of topic is based on the recently popular artificial can only be selected as the topic, artificial intelligence probably began to usher in its first heat wave in 2016. Artificial intelligence (ai) is a new technical science that researches and develops theories, methods, technologies and application systems for simulating, extending and extending human intelligence.\nSince the birth of artificial intelligence, theories and technologies have become increasingly mature and the application field has been expanding. It can be assumed that the scientific and technological products brought by artificial intelligence in the future will be the \"container\" of human intelligence. Artificial intelligence can simulate the information process of human consciousness and thinking\nThe dark knowledge that we are all familiar with is the knowledge that machines have acquired through numerous training attempts, which humans have not yet fully mastered. This is deep learning. The knowledge learned by machines through trial and error is not only the knowledge we humans have at present. At this time, intelligence is no longer human intelligence.\nLet's talk about the application areas of artificial intelligence: image processing, speech recognition, unmanned driving, medicine (medical treatment by machine), intelligent home (like domestic little love), unmanned retail. Artificial intelligence is believed to bring us more surprises than that. Later, as more and more people join in the research of artificial intelligence, artificial intelligence will once again break our traditional way of life and bring us more convenience.\n目录\n人工智能浪潮下的思考 1\n摘要 1\n第1章 人工智能的发展历史 3\n1.1人工智能的起源 3\n1.2人工智能的第一次高峰 4\n1.3人工智能第一次低谷 4\n1.4人工智能的崛起 4\n1.5人工智能的今天 5\n第2章 人工智能的定义 5\n2.1我眼中的人工智能 5\n2.2人工智能(60年代) 6\n2.3人工智能(80年代) 6\n2.4人工智能的分类 6\n2.4.1强人工智能 6\n2.4.2弱人工智能 6\n第3章 人工智能的应用领域 6\n3.1深度学习 6\n1. 构建一个网络并且随机初始化所有连接的权重； 6\n2. 将大量的数据情况输出到这个网络中； 7\n3.网络处理这些动作并且进行学习； 7\n4.如果这个动作符合指定的动作，将会增强权重，如果不符合，将会降低权重； 7\n5.系统通过如上过程调整权重； 7\n6.在成千上万次的学习之后，超过人类的表现； 7\n3.2语音识别 7\n3.3计算机视觉 7\n3.4智能机器人 7\n3.5医疗方面 7\nAI＋医疗影像、机器人／机械臂辅助手术、自动化的工作流程助理 7\n第4章 我对于人工智能的思考 7\n4.1人工智能的发展浅谈 7\n4.2人工智能的前景 8\n4.3人工智能是把双刃剑 8\n参考文献.................................................................10\n第1章 人工智能的发展历史\n1.1人工智能的起源\n其实，人工智能早在上世纪中叶就已经诞生。1950年，一位名叫马文·明斯基(后被人称为“人工智能之父”)的大四学生与他的同学邓恩·埃德蒙一起，建造了世界上第一台神经网络计算机。这也被看做是人工智能的一个起点。巧合的是，同样是在1950年，被称为“计算机之父”的阿兰·图灵提出了一个举世瞩目的想法——图灵测试。按照图灵的设想：如果一台机器能够与人类开展对话而不能被辨别出机器身份，那么这台机器就具有智能。而就在这一年，图灵还大胆预言了真正具备智能机器的可行性。\n1956年，在由达特茅斯学院举办的一次会议上，计算机专家约翰·麦卡锡提出了“人工智能”一词。后来，这被人们看做是人工智能正式诞生的标志。就在这次会议后不久，麦卡锡从达特茅斯搬到了MIT。同年，明斯基也搬到了这里，之后两人共同创建了世界上第一座人工智能实验室——MIT AI LAB实验室。\n1.2人工智能的第一次高峰\n在1956年的这次会议之后，人工智能迎来了属于它的第一段Happy Time。在这段长达十余年的时间里，计算机被广泛应用于数学和自然语言领域，用来解决代数、几何和英语问题。这让很多研究学者看到了机器向人工智能发展的信心。甚至在当时，有很多学者认为：“二十年内，机器将能完成人能做到的一切。”\n1.3人工智能第一次低谷\n时间来到了70年代，人工智能进入了一段痛苦而艰难岁月。由于科研人员在人工智能的研究中对项目难度预估不足，不仅导致与美国国防高级研究计划署的合作计划失败，还让大家对人工智能的前景蒙上了一层阴影。与此同时，社会舆论的压力也开始慢慢压向人工智能这边,导致很多研究经费被转移到了其他项目上。\n在当时，人工智能面临的技术瓶颈主要是三个方面，第一计算机性能不足，导致早期很多程序无法在人工智能领域得到应用；第二，问题的复杂性，早期人工智能程序主要是解决特定的问题，因为特定的问题对象少，复杂性低，可一旦问题上升维度，程序立马就不堪重负了；第三，数据量严重缺失，在当时不可能找到足够大的数据库来支撑程序进行深度学习，这很容易导致机器无法读取足够量的数据进行智能化。\n因此，人工智能项目停滞不前，但却让一些人有机可乘,1973年Lighthill针对英国AI研究状况的报告。批评了AI在实现“宏伟目标”上的失败。由此，人工智能遭遇了长达6年的科研深渊。\n1.4人工智能的崛起\n1980年，卡内基梅隆大学为数字设备公司设计了一套名为XCON的“专家系统”。这是一种，采用人工智能程序的系统，可以简单的理解为“知识库+推理机”的组合，XCON是一套具有完整专业知识和经验的计算机智能系统。这套系统在1986年之前能为公司每年节省下来超过四千美元经费。有了这种商业模式后，衍生出了像Symbolics、Lisp Machines等和IntelliCorp、Aion等这样的硬件，软件公司。在这个时期，仅专家系统产业的价值就高达5亿美元。\n1.5人工智能的今天\n把时间拨回到现在，2019年。我们回顾了人工智能接近70多年的发展历程。在这段漫长是时间里，科研技术人员不断突破阻碍，让我们可以看到今天人工智能所取得的辉煌成果，比如在1997年，IBM的深蓝战胜国际象棋世界冠军卡斯帕罗夫；2009年，螺丝联邦理工学院发起的蓝脑计划，生成已经成功模拟了部分鼠脑；2014年5月28日谷歌推出新产品——无人驾驶汽车；2016年，围棋人工智能程序AlphaGo以4:1的成绩战胜围棋世界冠军李世石；2017年，AlphaGo化身Master，再次出战横扫棋坛，让人类见识到了人工智能的强大。\n第2章 人工智能的定义\n对于人工智能的定义有许多，不同的人给出的也都不尽相同，这这里我列举一些著名人士给出的定义以及我的简单看法。\n2.1我眼中的人工智能\n我认为人工智能就是人类智慧的延申和另一种表现形式。人工智能是服务于人类而存在的，仅仅的只能是没有任何意义的，只有当它的价值体现和应用在人类身上时人工智能才有存在的必要。人工智能不能替代人类只是一种工具，所谓是工具就会存在利弊，不能因为某种不安全或者不确定的因素去随便否定人工智能，不能够以偏概全。\n2.2人工智能(60年代)\n在60年代，AI研究人员认为人工智能是一台通用机器人，它拥有模仿智能的特征，懂得使用语言，懂得形成抽象概念，能够对自己的行为进行推理，它可以解决人类现存问题。由于理念、技术和数据的限制，人工智能在模式识别、信息表示、问题解决和自然语言处理等不同领域发展缓慢。\n2.3人工智能(80年代)\n80年代，AI研究人员转移方向，认为人工智能对事物的推理能力比抽象能力更重要，机器为了获得真正的智能，机器必须具有躯体，它需要感知、移动、生存，与这个世界交互。为了积累更多推理能力，AI研究人员开发出专家系统，它能够依据一组从专门知识中推演出的逻辑规则在某一特定领域回答或解决问题。\n2.4人工智能的分类\n2.4.1强人工智能\n普通群众所遐想的人工智能属于强人工智能，它属于通用型机器人，也就是60年代AI研究人员提出的理念。它能够和人类一样对世界进行感知和交互，通过自我学习的方式对所有领域进行记忆、推理和解决问题。\n2.4.2弱人工智能\n不能制造出真正地推理（Reasoning）和解决问题（Problem_solving）的智能机器，这些机器只不过看起来像是智能的，但是并不真正拥有智能，也不会有自主意识。\n第3章 人工智能的应用领域\n3.1深度学习\n深度学习的技术原理：\n构建一个网络并且随机初始化所有连接的权重；\n将大量的数据情况输出到这个网络中；\n3.网络处理这些动作并且进行学习；\n4.如果这个动作符合指定的动作，将会增强权重，如果不符合，将会降低权重；\n5.系统通过如上过程调整权重；\n6.在成千上万次的学习之后，超过人类的表现；\n3.2语音识别\n语音识别，是把语音转化为文字，并对其进行识别、认知和处理。语音识别的主要应用包括电话外呼、医疗领域听写、语音书写、电脑系统声控、电话客服等。\n3.3计算机视觉\n计算机视觉是指计算机从图像中识别出物体、场景和活动的能力。计算机视觉有着广泛的细分应用，其中包括，医疗领域成像分析、人脸识别、公关安全、安防监控等等。\n3.4智能机器人\n智能机器人在生活中随处可见，扫地机器人、陪伴机器人……这些机器人不管是跟人语音聊天，还是自主定位导航行走、安防监控等，都离不开人工智能技术的支持。\n3.5医疗方面\nAI＋医疗影像、机器人／机械臂辅助手术、自动化的工作流程助理\n第4章 我对于人工智能的思考\n4.1人工智能的发展浅谈\n人工智能这个词在很大概是在60年代就被提出来，当时的人们也是有着很高的热情去投身于人工智能的算法设计和研究当中，人们当时认为20年及其就可以完成人类所完成的一切工作。在现在看来是很可笑的一件事，事实也确实如此，由于当时数据量的不足，当时的人工智能根本就无法进行深度的学习，说的前浅显一点就是机器就无法完成自我的对弈，这就导致人工智能的提升十分的有限。当时人们去给人工智能的设计的思考模式是按照人类的神经进行模拟的，但是人的神经分了成千上万的的级数，机器一旦应用这种模型就会存在级数一多没有相对应的管理算法，这也是当时人工智能第一次低谷的原因。但是并不是所有人都放弃了对于人工只能的研究，Geoffrey Hinton当时在研究神经网络的算法，他坚信人工智能一定会再一次出现在公众的视野当中。终于在80年代人工智能又再一次亮相，因为数学的发展、生物学的发展还有那些一直默默奉献的人们为人工智能的再次出现打下了坚实的基础。随后就是我们熟知的人工智能的事件：2016年，围棋人工智能程序AlphaGo以4:1的成绩战胜围棋世界冠军李世石；2017年，AlphaGo化身Master，再次出战横扫棋坛，让人类见识到了人工智能的强大。\n4.2人工智能的前景\n人工智能总是能够带给我们想象不到的惊喜，目前炒的很热的就是华为鸿蒙系统。据爆料华为OS将打通手机、电脑、平板、电视、汽车、智能穿戴，(将这些设备)统一成一个操作系统。\n人工智能在医疗方面也是有了雏形，现在越来越多的人们都喜欢带智能手环，从中你不仅可以看到自己的睡眠情况还可以看到自己的心率，我相信未来的手环很有可能像身份证一样人手一个，通过智能手环就可以诊断一些常见的病，还有就是现在越来越多的医院的设施都和电脑有了联系，相信之后的联席会越来越紧密地，像一些远程医疗、人工智能辅助设施都会成为发展的一个方向。\n4.3人工智能是把双刃剑\n我们都知道人工智能可以方便我们的生活，让我们在方方面面都享受到人工智能所带给我们的便捷。但是是工具都会有利和弊两个方面存在，我们都大概知道强人工智能会在深度学习之后会存在一些自我的意识，而我们人与机器最大的区别就是我们与自我意识而机器没有。举一个简单的例子：一个小孩是没有自我意识的因为它完全不知道什么是对什么是错，他并不会因为做了一件我们认为错的事情而有任何情绪上的变化，而大人就不同了，因为在长大的过程当中他会慢慢了解到对与错的概念，会开始自己的想法和思维。想一想你如果正在享受按摩机器人的服务而下一秒它就会威胁到你的生命你作何感想，机器的主观意识一直是一个无法回避的一个问题。但是我们想想喝口水都有可能被呛着的风险，难道我们就不喝水了吗？人工智能存在的一些问题我们无法回避，但是我们可以通过一些途径来解决而不是一味的停滞不前。\n参考文献\nwww.baidu.com\n李开复所著的《人工智能》"}
{"content2":"自然语言处理地位\n信息时代最重要的技术之一， 理解复杂的语言表达方式也是人工智能重要的一部分。\n自然语言处理应用\n自然语言处理应用无处不再，因为人们基本上所有的事情都是通过语言进行交流。\n常见的应用：\nweb搜索\n广告\nEmail\n客户服务\n语言翻译\n等等...\n自然语言处理常用技术\n在自然语言处理技术背后有大量的基础技术任务和机器学习模型。\n当前，深度学习方法在很多不同的自然语言处理应用中取得了非常好的性能。 在深度学习技术中，这些模型常常只要通过单个的端到端的模型训练即可。而不需要传统的，特定任务的特征工程。\n后续通过对神经网络模型的实现，训练，调试，可视化来加深对神经网络技术的理解。以及当前先进的深度学习应用到自然语言处理。\n在模型方面，后续介绍词矢量表示，基于窗的神经网络， Recurrent Neural Networks，Long-Short-Term-Memory 模型，Recursive Neural Networks， Convolutional Neural Networks以及最近的涉及到内存组件的模型。"}
{"content2":"发表于 2009年04月29号 由 52nlp\n自然语言处理：最大熵和对数线性模型\nNatural Language Processing: Maximum Entropy and Log-linear Models\n作者：Regina Barzilay（MIT,EECS Department, October 1, 2004)\n译者：我爱自然语言处理（www.52nlp.cn ，2009年4月29日）\n一、 词性标注（POS tagging）：\nc) 特征向量表示（Feature Vector Representation）\ni. 一个特征就是一个函数f（A feature is a function f ）：\nii. 我们有m个特征fk，k = 1…m（We have m features fk for k =1…m）\nd) 词性表示（POS Representation）\ni. 对于所有的单纯/标记对的单词/标记特征，（Word/tag features for all word/tag pairs）：\nii. 对于所有特定长度的前缀/后缀的拼写特征（Spelling features for all prefixes/suffixes of certain length）：\niii. 上下文特征（Contextual features）：\niv. 对于一个给定的“历史”x ∈ X ，每一个γ中的标记都被映射到一个不同的特征向量（For a given history x ∈ X, each label in γ is mapped to a different feature vector）：\nv. 目标（Goal）：学习一个条件概率P(tag|history)（learn a conditional probability P(tag|history)\n二、 最大熵（Maximum Entropy）：\na) 例子（Motivating Example）：\ni. 给定约束条件：p(x, 0)+p(y, 0)=0.6，a ∈{x, y}且b ∈0, 1，估计概率分布p(a, b)（Estimate probability distribution p(a, b), given the constraint: p(x, 0) + p(y, 0) =0.6, where a ∈{x, y}and b ∈0, 1））：\nii. 满足约束条件的一种分布（One Way To Satisfy Constraints）：\niii. 满足约束条件的另一种分布（Another Way To Satisfy Constraints）：\nb) 最大熵模型(Maximum Entropy Modeling)\ni. 给定一个训练样本集，我们希望寻找一个分布符合如下两个条件(Given a set of training examples, we wish to find a distribution which)：\n1. 满足已知的约束条件（satisfies the input constraints）\n2. 最大化其不确定性（maximizes the uncertainty）\nii. 补充：\n最大熵原理是在1957 年由E.T.Jaynes 提出的，其主要思想是，在只掌握关于未知分布的部分知识时，应该选取符合这些知识但熵值最大的概率分布。因为在这种情况下，符合已知知识的概率分布可能不止一个。我们知道，熵定义的实际上是一个随机变量的不确定性，熵最大的时侯，说明随机变量最不确定，换句话说，也就是随机变量最随机，对其行为做准确预测最困难。从这个意义上讲，那么最大熵原理的实质就是，在已知部分知识的前提下，关于未知分布最合理的推断就是符合已知知识最不确定或最随机的推断，这是我们可以作出的唯一不偏不倚的选择，任何其它的选择都意味着我们增加了其它的约束和假设，这些约束和假设根据我们掌握的信息无法做出。（这一段转自北大常宝宝老师的《自然语言处理的最大熵模型》）\n附：课程及课件pdf下载MIT英文网页地址：\nhttp://people.csail.mit.edu/regina/6881/"}
{"content2":"阿里BCG重磅报告《人工智能，未来致胜之道》\n阿里云研究中心、波士顿咨询公司以及Alibaba Innovation Ventures合作共同推出的《人工智能：未来制胜之道》这份报告对人工智能产业链价值，以及商业模式的剖析，对于创业公司的价值非常大。\n重点摘要：\n人工智能具备“快速处理”和“自主学习”两种能力。\n人工智能在图像识别、语言识别和自然语言处理，以及人机交互、机器视觉、自动驾驶等方面都已经成功应用。\n海量、精准、高质量的数据为训练人工智能提供了原材料。\n框架层：TensorFlow，Caffe，Theano，Torch，DMTK，DTPAR，ROS等框架或操作系统。\n通用技术层：语音识别、图像识别、人脸识别、NLP、SLAM、传感器融合、路径规划等技术或中间件。\n利用人工智能构建新的竞争优势，传统企业需要携手互联网企业，探索新的商业模式。\n--------------------------------------\n1.人工智能的准确定义与内涵：\n人工智能是研究、开发用于模拟、延伸和扩展人类智能的理论、方法、技术及应用系统的一门新的技术科学。\n根据人工智能的应用，人工智能可以分为专有人工智能、通用人工智能、超级人工智能。\n根据人工智能的内涵，人工智能可以分为类人行为（模拟行为结果）、类人思维（模拟大脑运作）、泛（不再局限于模拟人）智能。\n人工智能的驱动因素：算法/技术驱动、数据/计算、场景和颠覆性商业模式驱动。\n人工智能的承载方式：\n技术承载方式：单机智能、平行运算/多核智能、高度分散/群体智能。\n表现方式：云智能、端智能、云端融合。\n人工智能与人的关系：机器主导、人主导、人机融合。\n2.人工智能会做什么？\n人工智能具备“快速处理”和“自主学习”两种能力。\n人工智能实现了学习、决策和行动的快速处理。计算机处理信息、沟通信息、并行计算和线性计算的速度都快于人类。\n人工智能可以更灵活地自主学习和管理知识，支持知识的“产生——存储——应用——更新”的体系化管理。\n3.人工智能用在哪？\n人工智能在图像识别、语言识别和自然语言处理，以及人机交互、机器视觉、自动驾驶等方面都已经成功应用。\n人工智能更易于解决符合以下特点的商业问题：\n（1）行业存在持续痛点；\n（2）商业流程本身具有数字化的信息输入，问题可以细分并清晰的界定，商业流程存在重复，且获得的结果的沟通以书面沟通或单相沟通为主；\n（3）商业流程较少受整体商业环境的复杂影响。\n4.大数据是战略性竞争优势。\n海量、精准、高质量的数据为训练人工智能提供了原材料。\n人工智能的三种主要技术，都需要专有类型的数据。\n机器学习需要大量的标签样本数据。\n模式识别偏重于信号、图像、语音、文字、指纹等非直观数据。\n人机交互则需要积累大量的用户数据。\n5.现阶段，特别对于创业公司而言，数据的来源主要有三种。\n方式一，自筹数据，即从零开始，投入大量人力采集数据。\n方式二，公共数据。例如美国、英国、加拿大、新西兰，以及我国的香港、上海、北京、武汉、无锡、佛山和南海等城市都有自己的线上数据平台。\n方式三，产业数据协同，即下游创业公司或行业公司和产业链上游的数据或平台型公司建立合作，连接对双方均有利的产品或数据。\n6.从人工智能的技术突破和应用价值两维度分析，未来人工智能将会出现三个阶段：\n情景一：未来3-5年，仍以服务智能为主。在仍工智能及有技术的基础上，技术取得边际进步，机器始终作为人的辅助；在应用层面，人工智能拓展、整合多个垂直行业应用，丰富实用场景。随着数据和场景的增加，人工智能创造的价值呈现指数增长。\n情景二：中长期将出现显著科技突破。人工智能技术取得显著突破，如自然语言处理技术可以及时完全理解类人对话，甚至预测出“潜台词”。在技术创新的领域，现有的应用向纵深拓展，价值创造限制在技术取得突破的领域。\n情景三：长期可能出现超级智能。人工智能的技术取得显著突破，应用范围显著拓宽，人机完全融合，人工智能全面超过人类，无所不在，且颠覆各个行业和领域，价值创造极高。\n7.人工智能产业链\n人工智能产业链根据技术层级从上到下，分为基础层、技术层和应用层。基础层最靠近“云”，应用层最靠近“端”。\n基础层（按技术层级从上到下，下同）\n计算能力层：大数据、云计算、GPU/FPGA等硬件加速、神经网络芯片等计算能力提供商。\n数据层：身份信息、医疗、购物、交通出行等各行业、各场景的一手数据。\n技术层\n框架层：TensorFlow，Caffe，Theano，Torch，DMTK，DTPAR，ROS等框架或操作系统。\n算法层：机器学习、深度学习、增强学习等各种算法。\n通用技术层：语音识别、图像识别、人脸识别、NLP、SLAM、传感器融合、路径规划等技术或中间件。\n应用层\n应用平台层：行业应用分发和运营平台，机器人运营平台。\n解决方案层：智能广告、智能诊断、自动写作、身份识别、智能投资顾问、智能助理、无人车、机器人等场景应用。\n8.人工智能产业链价值分析：\n人工智能产业链中，基础层是构建生态的基础，价值最高，需要长期投入进行战略布局；通用技术层是构建技术护城河的基础，需要中长期进行布局；解决方案层直戳行业痛点，变现能力最强。\n9.未来人工智能竞争格局：\n在人工智能平台化的趋势下，未来人工智能将呈现若干主导平台加广泛场景应用的竞争格局，生态构建者将成为其中最重要的一类模式。\n模式一：生态构建着——全产业链生态+场景应用作为突破口。\n关键成功因素：大量计算能力投入，积累海量优质多维数据，建立算法平台、通用技术平台和应用平台，以场景应用为入口，积累用户。\n模式二：技术算法驱动者——技术层+场景应用作为突破口。\n关键成功因素：深耕算法和通用技术，建立技术优势，同时以场景应用为入口，积累用户。\n模式三：应用聚焦者——场景应用。\n关键成功因素：掌握细分市场数据，选择合适的场景构建应用，建立大量多维度的场景应用，抓住用户；同时，与互联网公司合作，有效结合传统商业模式和人工智能。\n模式四：垂直领域先行者——杀手级应用+逐渐构建垂直领域生态。\n关键成功因素：在应用较广泛且有海量数据的场景能率先推出杀手级应用，从而积累用户，成为该垂直行业的主导者；通过积累海量数据，逐步向应用平台、通用技术、基础算法拓展。\n模式五：基础设施提供者——从基础设施切入，并向产业链下游拓展。\n关键成功因素：开发具有智能计算能力的新型芯片，如图像、语音识别芯片等、拓展芯片的应用场景；在移动智能设备、大型服务器、无人机（车），机器人等设备、设施上广泛集成运用，提供更加高效、低成本的运算能力、服务，与相关行业进行深度整合。\n10.人工智能对企业的启示：\n传统企业的竞争优势主要来自两个方面：\n其一，在企业布局上，企业有专有的固定资产、品牌、知识产权等资源，在所在领域取得规模经济和范围经济，并通过门店和经销商网络建立了稳定的客户关系；\n其二，在企业自身的能力上，企业积累独特的人力资源和技能，并在流程上尽可能精简。\n人工智能时代，企业竞争优势转变为算法和数据资产，建立学习网络和数据生态，360度洞察消费者，通过人工智能不断地学习产生新的知识，同时在数据驱动下，进行即时自动决策。\n利用人工智能构建新的竞争优势，传统企业需要携手互联网企业，探索新的商业模式。\n11.人工智能对政府的启示：\n为加快人工智能产业发展，政府应从以下三个维度加强对人工智能产业的政策支持：\n（1）开放政府及公共领域数据，打造国家级人工智能资源平台。数据是人工智能的基础。为鼓励人工智能产业发展，应开放公共数据，并优化数据质量，建立系统化结构化的数据库平台，为人工智能的发展提供资源。\n（2）建立企业主导、高校研发、国家投入的人工智能产业一体化发展模式。人工智能在未来数年内将以服务智能为主，因此需要树立企业在人工智能行业的主导地位，鼓励企业积极开发人工智能的场景应用，以将人工智能科研成果转变为商业价值。同时，鼓励高校研发、增加国家科研投入，为长期人工智能基础科技突破做准备。\n（3）以产业基金、专项基金等激励人工智能创新，提供针对人工智能创业企业的税收优惠，以人才为导向，配套全球人工智能人才安家政策，提供宽松的人工智能法律法规环境。\n结语\n总体而言，阿里的人工智能体现于对于自身的产品与业务进行功能试水。当然，类似的方式也是腾讯和华为的人工智能战略逻辑。\n但其对人工智能产业链价值，以及商业模式的剖析，对于创业公司的价值非常大。指出机会在哪里，有助于创业公司在巨头的夹缝中找到自身优势，回避短板，获得更好的发展。"}
{"content2":"实际应用\n机器视觉，指纹识别，人脸识别，视网膜识别，虹膜识别，掌纹识别，专家系统，自动规划，智能搜索，定理证明，博弈，自动程序设计，智能控制，机器人学，语言和图像理解，遗传编程等\n主要成果\n人机对弈\n模式识别\n采用 $模式识别引擎，分支有2D识别引擎 ，3D识别引擎，驻波识别引擎以及多维识别引擎\n2D识别引擎已推出指纹识别，人像识别 ，文字识别，图像识别 ，车牌识别；驻波识别引擎已推出语音识别；3D识别引擎已推出指纹识别玉带林中挂（玩游智能版1.25）\n自动工程\n自动驾驶（OSO系统）\n印钞工厂（￥流水线）\n猎鹰系统（YOD绘图）\n知识工程\n以知识本身为处理对象，研究如何运用人工智能和软件技术，设计、构造和维护知识系统\n专家系统\n智能搜索引擎\n计算机视觉和图像处理\n机器翻译和自然语言理解\n数据挖掘和知识发现\n强弱人工智能\n强人工智能(BOTTOM-UP AI)与弱人工智能(TOP-DOWN AI)->超人工智能\n强人工智能观点认为有可能制造出真正能推理（REASONING）和解决问题（PROBLEM_SOLVING）的智能机器，并且，这样的机器能将被认为是有知觉的，有自我意识的\n弱人工智能观点认为不可能制造出能真正地推理（REASONING）和解决问题（PROBLEM_SOLVING）的智能机器，这些机器只不过看起来像是智能的，但是并不真正拥有智能，也不会有自主意识。\n主流科研集中在弱人工智能上，并且一般认为这一研究领域已经取得可观的成就。强人工智能的研究则处于停滞不前的状态下。"}
{"content2":"1.\n>>> s[:4]+'u'+s[-5:]\n2.\n>>> word1='dish-es' >>> word1 'dish-es' >>> word2='running' >>> 'undo'[:2] 'un' >>> 'pre-heat'[:3] 'pre' 3. >>> word1[-2] 'e' >>> word1[-8]\nTraceback (most recent call last): File \"<pyshell#100>\", line 1, in <module> word1[-8] IndexError: string index out of range >>> word1[7] Traceback (most recent call last): File \"<pyshell#101>\", line 1, in <module> word1[7] IndexError: string index out of range >>>\n4.\n>>> monty='Monty Python' >>> monty[6:11:2] 'Pto' >>> monty[10:5:-2] 'otP'\n5.\n逆序输出\n6.\na. [a-zA-Z]+           字母字符串\nb. [A-Z][a-z]*         开头大写后小字母不限\nc. p[aeiou]{,2}t        p开头t结尾中间有<=2个元音字幕\nd. \\d+(\\.\\d+)?          .数字可有可没有但是一定要有一个或多个数字。整数或者带小数的整数\ne. ([^aeiou][aeiou][^aeiou])*   pot类似的\nf. \\w+|[^\\w\\s]+          字母一个多个或者不是字母空格的一个多个\n>>> nltk.re_show('([^aeiou][aeiou][^aeiou])* ','poat',left='{',right='}') >>> nltk.re_show('([^aeiou][aeiou][^aeiou])*',out,left='{',right='}')\n{}.{}T{hef}a{milyofDas}h{}w{}o{}o{}d{hadlon}g{}b{}e{}e{}n{set}t{led}i{}n{Sussex}.{}\n>>> nltk.re_show('\\w+|[^\\w\\s]+',out,left='{',right='}')\n{.}{ThefamilyofDashwoodhadlongbeensettledinSussex}{.}\n7.\n1）.\n>>> nltk.re_show('an?|the','thesisiaishihsthean',left='{',right='}')\n{the}sisi{a}ishihs{the}{an}\n2）.\n>>> nltk.re_show('\\d+\\*\\d+\\+\\d+','2*3+8',left='{',right='}')\n{2*3+8}\n8.\n>>> from urllib import urlopen >>> def dealhtml(url): html=urlopen(url).read() raw=nltk.clean_html(html) tokens=nltk.word_tokenize(raw) text=nltk.Text(tokens) return text >>>url=\"http://news.baidu.com/z/2012europe/zhuanti.html\"\n9.\n>>> def load(name): f=open(name) raw=f.read() return raw >>> print load('corpus.txt')\na.\n>>> pattern = r'''(?x) # set flag to allow verbose regexps [][.,;\"'?():-_`] # these are separate tokens ''' >>> nltk.regexp_tokenize(text, pattern)\nb.\n>>> pattern =r'''(?x) # set flag to allow verbose regexps ([A-Z]\\.)+ # abbreviations, e.g. U.S.A. | [A-Z][a-z]*\\s[A-Z][a-z]* # words with optional internal | \\$?\\d+(\\.\\d+)?%? # currency and percentages, e.g. $12.40, 82% | \\d+-\\d+-\\d+ ''' >>> nltk.regexp_tokenize(text, pattern)\n10.\n>>> sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper'] >>> result=[] >>> for word in sent: result.append(format%(word,len(word))) >>> result\n['(The,3),', '(dog,3),', '(gave,4),', '(John,4),', '(the,3),', '(newspaper,9),']\n11.\n>>> raw='Good muffins cost $3.88\\\\nin New York.Please buy me\\\\ntwo of them.\\\\n\\\\nThankes.' >>> raw.split('s')\n['Good muffin', ' co', 't $3.88\\\\nin New York.Plea', 'e buy me\\\\ntwo of them.\\\\n\\\\nThanke', '.']\n12.\n>>> len(raw)\n76\n>>> for i in range(len(raw)): print raw[i]+\"\\n\",\n13.\n>>> text.split() ['Good', 'muffi', 'ns', 'cost', '$3.88\\\\nin', 'New', 'York.Please', 'buy', 'me\\\\ntwo', 'of', 'them.\\\\n\\\\nThankes.'] >>> text.split(' ') ['Good', 'muffi', '', '', 'ns', 'cost', '$3.88\\\\nin', 'New', 'York.Please', 'buy', 'me\\\\ntwo', 'of', 'them.\\\\n\\\\nThankes.'] >>> >>> text='Good muffi \\t ns co\\ts' >>> text.split(' ') ['Good', 'muffi', '', '\\t', 'ns', 'co\\ts'] >>> text.split() ['Good', 'muffi', 'ns', 'co', 's'] >>>\n14.\n汗一个，没发现有什么区别\n15.\n>>> \"3\"*7 '3333333' >>> 3*7 21 >>> '3'*7 '3333333' >>> >>> int(\"3\") 3 >>> str(3) '3' >>>\n相当于强制转换类型\n16.\n我的不成功\n17.\n%6s与%-6s的区别在于-是左对齐\n>>> '%6s' %'dog' ' dog' >>> '%6s'%'sdasdasdsds' 'sdasdasdsds' >>> '%-6s'%'sdasdasdsds' 'sdasdasdsds' >>> '%-6s' %'dog'' 'dog ' >>>\n18.\n>>> def load(name): f=open(name) raw=f.read() return raw >>> text=load('corpus.txt') >>> pattern =r'''(?x) # set flag to allow verbose regexps [Ww][Hh]\\S+ # abbreviations, e.g. U.S.A. ''' >>> nltk.regexp_tokenize(text, pattern) ['What', 'while', 'who', 'who', 'who', 'who', 'who', 'when', 'when', 'who', 'which']\n19.\n>>> f=open('freq.txt').readlines() >>> f\n['fuzzy 53\\n', 'absent 46\\n', 'lost 33\\n', 'dead 17\\n', 'over 32']\n>>> words=[] >>> for str in f: words.append(str.split()) >>> result=[] >>> for [str,num] in words: word=str; intnum=int(num); result.append([word,intnum]) >>> print result\n20.\n>>> from urllib import urlopen >>> def dealhtml(url): html=urlopen(url).read() raw=nltk.clean_html(html) tokens=nltk.word_tokenize(raw) text=nltk.Text(tokens) return text >>> url=\"http://www.weather.com.cn/weather/101020100.shtml\" >>> text=dealhtml(url) >>> print text >>> output_file = open('output.txt', 'w') >>> for word in text: output_file.write(word+'\\s')\n有个中文编码的问题。这个是个问题暂时不会等一会没事再研究一下\n21.\n>>> from urllib import urlopen >>> import nltk,re >>> url=\"http://www.bbc.co.uk/news/world-middle-east-18650775\" >>> wordsres=[] >>> def unknown(url): html=urlopen(url).read() raw=nltk.clean_html(html) words=re.findall(r'[a-z]+',raw) wordlist=[w for w in nltk.corpus.words.words('en') if w.islower()] for word in words: if word not in wordlist: wordsres.append(word) return wordsres >>> wordsres=[] >>> wordsres=unknown(url)\n23.\n因为这个有问题n't|\\w因为你正则表达式是r'  '，这样就结束了r｛'n'｝t|\\w‘\n>>> text=\"who is don't you know\" >>> re.split('',text) [\"who is don't you know\"] >>> re.split(' ',text) ['who', 'is', \"don't\", 'you', 'know'] >>> re.findall(r'n\\'t|\\w+',text) ['who', 'is', 'don', 't', 'you', 'know'] >>> re.findall('n\\'t'|r'\\w+',text) Traceback (most recent call last): File \"<pyshell#17>\", line 1, in <module> re.findall('n\\'t'|r'\\w+',text) TypeError: unsupported operand type(s) for |: 'str' and 'str' >>>\n24.\n>>>result=[] >>>text='say what your classment' >>> words=text.split() >>> result=[] >>> for word in words: if 'e' in word: result.append(word.replace('e','3')) if 'ate' in word: result.append(word.replace('ate','8')) if '.' in word: result.append(word.replace('.','5w33t!')) if '1' in word: result.append(word.replace('1','|')) if 'o' in word: result.append(word.replace('o','0')) if word in re.findall(r'[^s\\s]\\w+s+\\w+',text): result.append(word.replace('s','5')) if word.startswith('s'): result.append(word.replace(word[0],'$')) >>>result\n但是这种方法会得到重复的值因此不合适要改\n>>> for i in range(len(words)): if words[i] in re.findall(r'[^s\\s]+s+\\w+',text): words[i]=words[i].replace('s','5') if words[i].startswith('s'): words[i]=words[i].replace(words[i][0],'$') if 'e' in words[i]: words[i]=words[i].replace('e','3') if 'ate' in words[i]: words[i]=words[i].replace('ate','8') if '.' in words[i]: words[i]=words[i].replace('.','5w33t!') if '1' in words[i]: words[i]=words[i].replace('1','|') if 'o' in words[i]: words[i]=words[i].replace('o','0')\n修改以后这个就可以了！终于对了\n25.\na.\n>>> def pig_word(word): for i in range(len(word)): if word[i] in '[AEIOUaeiou]': pigword=[word[i:],word[:i],'ay'] result=''.join(pigword) return result >>> >>> pig_word(word) 'ingstr'\nb.\n>>> re=[] >>> for str in text: re.append(pig_word(str)) >>> print re ['eTh', 'amilyf', 'of', 'ashwoodD', 'adh', 'ongl', 'eenb', 'ettleds', 'in', 'ussexS', None] >>>\nC.\n>>> def qupig_word(word): for i in range(len(word)): if word[i] in '[AEIOUaeiou]': pigword=[word[i+1:],word[:i+1],'ay'] result=''.join(pigword) return result >>> words=[\"yellow\",\"happy\",\"quiet\"] >>> for word in words: if word.startswith('y'): re.append(pig_word(word)) if 'qu' in word.lower(): re.append(qupig_word(word)) else: re.append(pig_word(word)) >>> re\n26.这个我不会。。真心不会，求解答。哪位好心人告诉我怎么办\n27.\n>>> str=[] >>> for i in range(500): str.append(random.choice(\"aehh \")) >>> str >>> ''.join(str) >>> word=str.split() >>> str=''.join(word) >>> str\n28.\n>>>import nltk >>>from nltk.corpus import brown >>>words1=[len(word) for word in nltk.corpus.brown.words(categories='lore')] >>>sents1=[len(sent) for sent in nltk.corpus.brown.sents(categories='lore')] >>>words2=[len(word) for word in nltk.corpus.brown.words(categories='learned')] >>>sents2=[len(sent) for sent in nltk.corpus.brown.sents(categories='learned')] >>> wordsum=0; >>> for wlength in words1: wordsum+=int(wlength) >>> for slength in sents1: sentsum+=slength >>> from __future__ import division >>> def ARI(uw,us): return 4.71*uw+0.5*us-21.43 >>> uw=wordsum/len(words1) >>> us=sentsum/len(sents1) >>> us 22.59762343782012 >>> ARI(uw,us) 10.254756197101155\n30.\n>>> saying=['After', 'all', 'is', 'said','and', 'done', ',', 'more','is', 'said', 'than', 'done', '.'] >>> porter=nltk.PorterStemmer() >>> lancaster=nltk.LancasterStemmer() >>> [porter.stem(word) for word in saying] ['After', 'all', 'is', 'said', 'and', 'done', ',', 'more', 'is', 'said', 'than', 'done', '.'] >>> [lancaster.stem(word) for word in saying] ['aft', 'al', 'is', 'said', 'and', 'don', ',', 'mor', 'is', 'said', 'than', 'don', '.'] >>>\n31.\n>>> saying=['After', 'all', 'is', 'said','and', 'done', ',', 'more', 'is', 'said', 'than', 'done', '.'] >>> length=[] >>> for say in saying: length.append(len(say)) >>> length\n32.\na.\n>>> silly='newly formed bland ideas are inexpressible in an infuriating way' >>> bland=silly.split() >>> bland\n['newly', 'formed', 'bland', 'ideas', 'are', 'inexpressible', 'in', 'an', 'infuriating', 'way']\nB.\n>>> silly2=[] >>> for word in bland: silly2.append(word[1]) >>> silly2 ['e', 'o', 'l', 'd', 'r', 'n', 'n', 'n', 'n', 'a'] >>> ''.join(silly2)\n'eoldrnnnna'\nC.\n>>> newsilly=' '.join(bland) >>> newsilly\n'newly formed bland ideas are inexpressible in an infuriating way'\n>>>\nD.\n>>> for word in bland: print word,'\\n',\n33.\na.\n>>> 'inexpressible'.index('re')\n5\nb.\n>>> bland.index('way')\n9\nC.\n>>> silly.index('in ')\n43\n>>> phrase=[] >>> phrase=silly[:silly.index('in ')].split() >>> phrase\n['newly', 'formed', 'bland', 'ideas', 'are', 'inexpressible']\n34.\n我把维基百科上面的表存到了country.txt这个文件里，这样就很好办了（某些有乱码）\n>>> import nltk,re >>> f=open(\"countryname.txt\") >>> f <open file 'countryname.txt', mode 'r' at 0x0000000005687930> >>> n=raw.split('\\n') >>> def CTadjton(countryadj): for country in n: if re.findall(countryadj,country): countrypt=country.split() return countrypt[0] >>> countryadj='Canadian' >>> CTadjton(countryadj)\n'Canada'\n35.\n>>>from nltk.corpus import brown >>>text=''.join(brown.words()) >>> re.findall(r'as best \\w+ can',text) >>> re.findall(r'as best as \\w+ can',text)\n36.\n喵星语搞不了、\n37.\n在Python根目录下见了一个test.html内容为\n“\n<html> this is a test </html>\n”\n>>> import re >>> f=open(\"test.html\") >>> raw=f.read() >>> pattern=r'''(?x) <html> |</html> ''' >>> re.sub(pattern,'',raw)\n'this is a test'"}
{"content2":"原文：Python将是人工智能时代的最佳编程语言\nPython将是人工智能时代的最佳编程语言\n移动互联网取代PC互联网领跑在互联网时代的最前沿，Android和iOS一度成为移动互联网应用平台的两大霸主，成为移动开发者首选的两门技术，HTML5以其跨平台的优势在移动互联网应用平台占据重要位置，可以说是后来者居上。  由于技术的限制难以催生出更多的新应用，互联网+的产品日渐饱和，移动互联网从巅峰时代逐渐趋于平缓发展，下一个时代谁是主场？下一门应用技术谁来掌门？\n在第三届互联网大会中百度CEO李彦宏曾表述：靠移动互联网的风口已经没有可能再出现独角兽了，因为市场已经进入了一个相对平稳的发展阶段，互联网人口渗透率已经超过了50%。而未来的机会在人工智能。的确互联网巨头公司在人工智能领域投入明显增大，都力争做人工智能时代的“带头大哥”。\n特点\nPython作为一门编程语言，其魅力远超C#，Java,C,C++，它被昵称为“胶水语言”，更被热爱它的程序员誉为“最美丽的”编程语言。从云端、客户端，到物联网终端，python应用无处不在，同时也是人工智能首先的编程语言。\n在人工智能上使用Python编程语言的优势\n1.优质的文档\n2.平台无关，可以在现在每一个*nix版本上使用\n3.和其他面向对象编程语言比学习更加简单快速\n4.Python有许多图像加强库像Python Imaging Libary,VTK和Maya 3D可视化工具包，Numeric Python, Scientific Python和其他很多可用工具可以于数值和科学应用。\n5.Python的设计非常好，快速，坚固，可移植，可扩展。很明显这些对于人工智能应用来说都是非常重要的因素。\n6.对于科学用途的广泛编程任务都很有用，无论从小的shell脚本还是整个网站应用。\n7.最后，它是开源的。可以得到相同的社区支持。\nPython库\n1.总体的AI库\nAIMA：Python实现了从Russell到Norvigs的“人工智能：一种现代的方法”的算法\npyDatalog：Python中的逻辑编程引擎\nSimpleAI：Python实现在“人工智能：一种现代的方法”这本书中描述过的人工智能的算法。它专注于提供一个易于使用，有良好文档和测试的库。\nEasyAI：一个双人AI游戏的python引擎（负极大值，置换表、游戏解决）\n2.机器学习库\nPyBrain 一个灵活，简单而有效的针对机器学习任务的算法，它是模块化的Python机器学习库。它也提供了多种预定义好的环境来测试和比较你的算法。\nPyML 一个用Python写的双边框架，重点研究SVM和其他内核方法。它支持Linux和Mac OS X。\nscikit-learn 旨在提供简单而强大的解决方案，可以在不同的上下文中重用：机器学习作为科学和工程的一个多功能工具。它是python的一个模块，集成了经典的机器学习的算法，这些算法是和python科学包（numpy,scipy.matplotlib）紧密联系在一起的。\nMDP-Toolkit 这是一个Python数据处理的框架，可以很容易的进行扩展。它海收集了有监管和没有监管的学习算法和其他数据处理单元，可以组合成数据处理序列或者更复杂的前馈网络结构。新算法的实现是简单和直观的。可用的算法是在不断的稳定增加的，包括信号处理方法（主成分分析、独立成分分析、慢特征分析），流型学习方法（局部线性嵌入），集中分类，概率方法（因子分析，RBM),数据预处理方法等等。\n3.自然语言和文本处理库\nNLTK 开源的Python模块，语言学数据和文档，用来研究和开发自然语言处理和文本分析。有windows,Mac OSX和Linux版本。\nPython势必成为人工智能时代的新宠儿，Python这门学科也将引入大量的学习者，任何行业的成功人士当属那些先行者，人工智能的浪潮还未席卷，选择Python这门学科就是有先见之明。在适合的时期选择适合的培训机构是至关重要的。\n保质量，求真实，能学会，可就业，拿高薪的培训机构才是最佳选项。在培训机构中常见低价聘请新手Python开发者做讲师、常见其他学科讲师现学Python充当讲师，耽误无数学生！千锋Python教学部特聘请尹老师担任教学总监，毕业于清华大学，微软全球最具价值专家，资深软件架构师，CSDN著名技术专家，微软-清华大学联合实验室技术顾问，清华大学Oracle-java创始人，清华大学Google技术俱乐部创始人，清华大学Linux 技术俱乐部创始人。精通Python，C/C++，对于移动3G、语音技术、javaEE、信息安全、大数据高并发都有丰富的开发经验，拥有多年世界顶尖IT企业工作经验。不用坐在教室看1个讲师全国同步视频授课，不用在毫无学习氛围的教室看在线直播授课。千锋采用100%全程面授，名师一点胜庸师百万。"}
{"content2":"这个小程序旨在通过自然语言对话查询快递、身份证、天气、诗歌、词典等等的功能。\n自然语言对话，即使用中文语言直接对程序下命令，比如：‘查一下天气’，“帮我查一下123456这个运单号吧”，“我想听李白的静夜思”等等。\n如果还是不明白，请阅读博客 https://i.cnblogs.com/EditPosts.aspx?postid=7203097&update=1  的前言部分。\n1. 小程序功能介绍\n如果希望直接体验小程序，请直接扫描下面的二维码，这样更直观。\n但是发布的小程序里没有身份证查询，因为个人的公众号不能提供政务查询功能（相当遗憾）。但是这里提供的代码里有身份证信息查询.\n代码下载：  智能查询代码下载\n欧拉蜜语法文件下载： 身份证、快递、词典语法文件下载\n**   首页\n点击首页中的任何选项和图片，都会进入相应的查询界面：\n** 帮助页面\n帮助页面提供各种功能的简单介绍和自然语言理解、技术交流的联系方式。\n\n** 子页面\n每个子页面里都提供例句和切换例句的功能，可以先点击例句试试看。每个子页面也都有输出结果显示，如果内容过多，需要触摸滚动显示。\n\n**快递查询\n快递查询会列出你要查询的快递公司、运单号和详细信息。\n*词典查询\n词库大约有50万数据，支持近义词、反义词、出处、含义单独查询。\n**身份证查询\n身份证前6位仅能查到行政位置信息，身份证号码可以查到除了姓名之外的信息。\n**天气查询\n天气支持今天前后五天的查询，也支持温度、风力、风向、指数的查询，口语化做的比较好，比如“明天冷不冷”，“明天上海会下雨吗”\n**诗歌背诵\n诗歌支持诗歌名称查询，另外支持作者作品、诗词上下句、诗词出处等的查询。\n**菜谱查询\n菜谱支持菜名、菜系、菜类型查询。\n**其余功能\n剩余功能不再做详细介绍，可以查看小程序的帮助页面。\n2.   代码解析\n2.1 代码结构\n------图中app.json定义了所有的页面，以及标题栏和导航栏的样式，包括“首页”和“帮助”两个tabBar.\n-------index表示“首页”page\n-------express表示快递查询页面\n-------identify表示身份证查询页面\n-------dict表示词典查询页面\n-------mine表示帮助页面\n-------general表示其他页面，这些页面同意使用欧拉蜜官网提供的内置语法模块，所以使用统一页面代码。\n但需要根据首页不同的选择传入对应的参数。\n2.2 代码中用到的API接口\n由于微信小程序仅支持https访问，因此接口必须支持https访问。\n------所有自然语言的解析，包括输入框中输入的语句和例句，均调用欧拉蜜人工智能开放平台中的自然语言语义理解API接口，详细使用方法可以参考我的另外一些博客：\n* 用欧拉蜜语言开发平台实现智能客服开发攻略\n*  告诉你如何使用OLAMI自然语言理解开放平台API制作自己的智能对话助手\n-----快递查询接口\n这里使用的是 快递鸟即时查询接口，免费使用，请自行到官网（）申请APPkey和BusinessID, 请填入util的queryExpress.js中对应的位置：\n------词典API\n极速数据的汉语词典，申请获取的APPkey填入dict.js的相应代码：\n--------身份证查询ＡＰＩ\n极速数据的 身份证查询ＡＰＩ　，申请APPkey之后填入identify.js相应代码：\n-------天气、诗歌、计算、菜谱、笑话等其他模块的输出数据均由欧拉蜜的自然语言理解接口提供结果，\n相关代码未general.js 的 function parseCorpus(corpus,object) {}函数处理。\n3.  调试\n3.1 下载代码并在小程序开发工具中调试\n代码下载：  智能查询代码下载\n注意： 在没有申请相应的API接口之前，快递、词典、身份证查询不能正常使用，其他模块可以正常测试。\n欧拉蜜语法文件下载： 身份证、快递、词典语法文件下载\n3.2   扫码小程序或者关注公众号直接测试\n小程序二维码：\n微信公众号二维码：\n4. 技术交流\n关于自然语言理解使用有不懂的地方可以扫描下面QQ群进行技术交流："}
{"content2":"3.8 Segmentation   分割\nThis section discusses more advanced concepts, which you may prefer to skip on the first time through this chapter. Tokenization is an instance of a more general problem of segmentation. In this section, we will look at two other instances of this problem, which use radically（根本上）different techniques to the ones we have seen so far in this chapter.\nSentence Segmentation 断句\nManipulating texts at the level of individual words often presupposes（假定） the ability to divide a text into individual sentences. As we have seen, some corpora already provide access at the sentence level. In the following example, we compute the average number of words per sentence in the Brown Corpus:\n>>> len(nltk.corpus.brown.words()) / len(nltk.corpus.brown.sents())\n20.250994070456922\nIn other cases, the text is available only as a stream of characters. Before tokenizing the text into words, we need to segment it into sentences. NLTK facilitates this by including the Punkt sentence segmenter (Kiss & Strunk, 2006). Here is an example of its use in segmenting the text of a novel. (Note that if the segmenter’s internal data has been updated by the time you read this, you will see different output.)\n>>> sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n>>> text = nltk.corpus.gutenberg.raw('chesterton-thursday.txt')\n>>> sents = sent_tokenizer.tokenize(text)\n>>> pprint.pprint(sents[171:181])\n['\"Nonsense!',\n'\" said Gregory, who was very rational when anyone else\\nattempted paradox.',\n'\"Why do all the clerks and navvies in the\\nrailway trains look so sad and tired,...',\n'I will\\ntell you.',\n'It is because they know that the train is going right.',\n'It\\nis because they know that whatever place they have taken a ticket\\nfor that ...',\n'It is because after they have\\npassed Sloane Square they know that the next stat...',\n'Oh, their wild rapture!',\n'oh,\\ntheir eyes like stars and their souls again in Eden, if the next\\nstation w...'\n'\"\\n\\n\"It is you who are unpoetical,\" replied the poet Syme.']\nNotice that this example is really a single sentence, reporting the speech of Mr. Lucian Gregory. However, the quoted speech contains several sentences, and these have been split into individual strings. This is reasonable behavior for most applications.\nSentence segmentation is difficult because a period is used to mark abbreviations, and some periods simultaneously（同时）mark an abbreviation and terminate a sentence, as often happens with acronyms like U.S.A. For another approach to sentence segmentation, see Section 6.2.\nWord Segmentation 断词\nFor some writing systems, tokenizing text is made more difficult by the fact that there is no visual representation of word boundaries. For example, in Chinese, the three-character string: 爱国人 (ai4 “love” [verb], guo3 “country”, ren2 “person”) could be tokenized as 爱国 / 人, “country-loving person,” or as 爱 / 国人, “love country-person.”\nA similar problem arises in the processing of spoken language, where the hearer must segment a continuous speech stream into individual words. A particularly challenging version of this problem arises when we don’t know the words in advance. This is the problem faced by a language learner, such as a child hearing utterances（说话） from a parent.\nConsider the following artificial example, where word boundaries have been removed:\n(1) a. doyouseethekitty\nb. seethedoggy\nc. doyoulikethekitty\nd. likethedoggy\nOur first challenge is simply to represent the problem: we need to find a way to separate text content from the segmentation. We can do this by annotating each character with a boolean value to indicate whether or not a word-break appears after the character (an idea that will be used heavily for “chunking” in Chapter 7). Let’s assume that the learner is given the utterance breaks, since these often correspond to extended pauses. Here is a possible representation, including the initial and target segmentations:\n>>> text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n>>> seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n>>> seg2 = \"0100100100100001001001000010100100010010000100010010000\"\nObserve that the segmentation strings consist of zeros and ones. They are one character shorter than the source text, since a text of length n can be broken up in only n–1 places. The segment() function in Example 3-2 demonstrates that we can get back to the original segmented text from its representation.\nExample 3-2. Reconstruct segmented text from string representation: seg1 and seg2 represent the initial and final segmentations of some hypothetical（假设）child-directed speech; the segment() function can use them to reproduce the segmented text.\ndef segment(text, segs):\nwords = []\nlast = 0\nfor i in range(len(segs)):\nif segs[i] == '1':\nwords.append(text[last:i+1])\nlast = i+1\nwords.append(text[last:])\nreturn words\n>>> text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n>>> seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n>>> seg2 = \"0100100100100001001001000010100100010010000100010010000\"\n>>> segment(text, seg1)\n['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n>>> segment(text, seg2)\n['do', 'you', 'see', 'the', 'kitty', 'see', 'the', 'doggy', 'do', 'you',\n'like', 'the', kitty', 'like', 'the', 'doggy']\nNow the segmentation task becomes a search problem: find the bit string that causes the text string to be correctly segmented into words. We assume the learner is acquiring words and storing them in an internal lexicon. Given a suitable lexicon, it is possible to reconstruct the source text as a sequence of lexical items. Following (Brent & Cart-wright, 1995), we can define an objective function（目标函数）, a scoring function whose value we will try to optimize, based on the size of the lexicon and the amount of information needed to reconstruct the source text from the lexicon. We illustrate this in Figure 3-6.\nFigure 3-6. Calculation of objective function: Given a hypothetical segmentation of the source text (on the left), derive a lexicon and a derivation table（推导表） that permit the source text to be reconstructed, then total up（合计） the number of characters used by each lexical item (including a boundary marker 界标) and each derivation, to serve as a score of the quality of the segmentation; smaller values of the score indicate a better segmentation（得分值与分割性能成反比）.（词汇的分数是按长度+界标，推导是由分割的数量）\nIt is a simple matter to implement this objective function, as shown in Example 3-3.\nExample 3-3. Computing the cost of storing the lexicon and reconstructing the source text.\ndef evaluate(text, segs):\nwords = segment(text, segs)\ntext_size = len(words)\nlexicon_size = len(' '.join(list(set(words))))\nreturn text_size + lexicon_size\n>>> text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n>>> seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n>>> seg2 = \"0100100100100001001001000010100100010010000100010010000\"\n>>> seg3 = \"0000100100000011001000000110000100010000001100010000001\"\n>>> segment(text, seg3)\n['doyou', 'see', 'thekitt', 'y', 'see', 'thedogg', 'y', 'doyou', 'like',\n'thekitt', 'y', 'like', 'thedogg', 'y']\n>>> evaluate(text, seg3)\n46\n>>> evaluate(text, seg2)\n47\n>>> evaluate(text, seg1)\n63\nThe final step is to search for the pattern of zeros and ones that maximizes this objective function, shown in Example 3-4. Notice that the best segmentation includes “words” like thekitty, since there’s not enough evidence in the data to split this any further.\nExample 3-4. Non-deterministic search using simulated annealing（模拟退火算法）: Begin searching with phrase segmentations only; randomly perturb（扰乱） the zeros and ones proportional to the “temperature”; with each iteration the temperature is lowered and the perturbation（扰乱） of boundaries is reduced.\n刚开始仅搜索短语分词；随机地扰乱0和1，与“temperature”成比例；每次迭代温度降低并且边界的扰乱减少了。\nfrom random import randint\ndef flip(segs, pos):\nreturn segs[:pos] + str(1-int(segs[pos])) + segs[pos+1:]\ndef flip_n(segs, n):\nfor i in range(n):\nsegs = flip(segs, randint(0,len(segs)-1))\nreturn segs\ndef anneal(text, segs, iterations, cooling_rate):\ntemperature = float(len(segs))\nwhile temperature > 0.5:\nbest_segs, best = segs, evaluate(text, segs)\nfor i in range(iterations):\nguess = flip_n(segs, int(round(temperature)))\nscore = evaluate(text, guess)\nif score < best:\nbest, best_segs = score, guess\nscore, segs = best, best_segs\ntemperature = temperature / cooling_rate\nprint evaluate(text, segs), segment(text, segs)\nprint\nreturn segs\n>>> text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n>>> seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n>>> anneal(text, seg1, 5000, 1.2)\n60 ['doyouseetheki', 'tty', 'see', 'thedoggy', 'doyouliketh', 'ekittylike', 'thedoggy']\n58 ['doy', 'ouseetheki', 'ttysee', 'thedoggy', 'doy', 'o', 'ulikethekittylike', 'thedoggy']\n56 ['doyou', 'seetheki', 'ttysee', 'thedoggy', 'doyou', 'liketh', 'ekittylike', 'thedoggy']\n54 ['doyou', 'seethekit', 'tysee', 'thedoggy', 'doyou', 'likethekittylike', 'thedoggy']\n53 ['doyou', 'seethekit', 'tysee', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n51 ['doyou', 'seethekittysee', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n42 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n'0000100100000001001000000010000100010000000100010000000'\nWith enough data, it is possible to automatically segment text into words with a reasonable degree of accuracy. Such methods can be applied to tokenization for writing systems that don’t have any visual representation of word boundaries."}
{"content2":"人工分词产生不一致性的原因主要在于人们对词的颗粒度的认知问题。在汉语里，词是表达意最基本的意思，再小意思就变了。在机器翻译中会有一种颗粒度比另外一种颗粒度更好的情况，颗粒度大的翻译效果好。\n为了解决词语的多义性的问题，维护一个词语对等列表是有必要的。例如“中国银联”=“银联”，这时候主键应该保存为“中国银联”，同样很多中英文系统的互译也应该将其保存在对等列表中\n[补充]维护的应该是词语相关性表（应该设定关联性>p)，一些词和其他词的关联性，从而拓展两种可能：1.词义相同的两个词可以以较高的关联性进行识别，从而提高答案的准确性；2.词义较为相近的两个词关联，从而提高相近答案的输出（建议的形式输出，例如我们没有发现XXX的答案，建议查看YYY的答案），提高用户对于会话智能的认可\nAt Infermedica we've developed a diagnostic engine that collects symptoms, asks diagnostic questions and presents likely health issues underlying this evidence. The engine uses a complex probabilistic model built on top of a knowledge base curated by medical professionals and enriched by machine learning. It is available via API and has provided helpful information to over 1M patients through a number of symptom checker apps, intelligent patient intake forms and other applications. Symptomate Bot is our attempt at building a conversational interface to the engine that will work as a symptom checker chatbot.\n客服机器人最主要的还是问答引擎，收集用户的问题，并进一步收集全面和明确用户的信息（例如机器人主动提问），引擎需要采用匹配模型来讲问题和知识库进行搭配（目前考虑使用标签），回答用户的知识，并收集反馈进行下一轮会话使用\nChallenge 1: Scalable architecture for handling messages\n客服机器人设计的会话峰值应该考虑？是否需要排队机制？\n会话机制的建立（新会话/老会话未结束），随时保存，状态转移的。\n会话处理的流程：收集消息，获取与会话关联的客服模型状态，处理消息，回复用户，并将更新状态推回客服模型。\n会话的保存应该适用灵活的结构和快速的读写访问，应该是NoSQL数据库，考虑MongoDB和redis。\nChallenge 2: A conversational agent must converse\n应该使用一些问答模板来代替“回复1”、“输入1”这样的问答,这样显得你的系统很傻。\n这些简答模板包括：再次确认/纠正歧义/获取更多信息/…\nChallenge 3:Users will get frustrated\n大部分用户会被误解，我们只为真正的顾客提供服务（基于场景的开发），就是说不正常的用户输入在我们的系统中将不会得到有效的答复。\n失望的原因包括：\n１.很多用户是因为存在问题或者发生故障来寻找客服服务的，本身带有消极的情绪，例如愤怒、着急、失望等。\n会话机器人理解问题的规模取决于知识库的大小，如果是询问银联总裁是不会存在任何误解的可能，但是客户机器人使用的知识库往往是落后于实际的业务开展情况的，而且客户很有可能问不存在与知识库的知识。\n另外，我们提供三个方法来解决“失望的用户”问题：\n1.感知用户的情绪并给予安慰；\n2.自动开出跟踪服务单引入人工客服的机制来解决问题；\n3.定期数据统计，跟踪出现用户失望的会话，并从设计和数据方面进行改善。\n消息队列的使用\n即使能够写出涵盖所有自然语言现象的语法规则集合，也很难用计算机来解析。描述自然语言的文法和计算机高级程序语言的文法不同。自然语言在延边过程中，产生了词义和上下文相关的特性。\n——《自然之美》p22\n我们的会话机器人要基于当前会话上下文相关进行。在同一个会话中，用户提交的信息能在后续的会话中进行思考。应该有好的算法和模型来处理上下文会话理解的难题。Context Dependent Grammar.解决多义性。\n只能回答“是什么”，但是没有计算机能够回答“为什么”和“怎么做”这一类的难题。\n目前我们的Chatbot只需要考虑是什么就行了。\n模型训练中一个重要的问题就是训练书怒，语料库的选取。 p38\n如何寻找会话机器人的语料库？训练数据库越多越好，模型参数估计的越准确。\nBots简单理解，就是智能对话机器人，是基于自然语言理解的智能对话系统，它应用了智能识别技术（如语言、文字、图像），还有机器学习、知识表示、云计算、大数据等。在美剧【西部世界】中，机器人居民与人类的沟通，就是用自然语言完成。一个Bot能部署到微信、QQ、网站、微博等渠道，用户通过任何数字入口都可以唤醒，并使用它提供服务，同时，Bot与Bot之间可以通过自然语言相互进行调用，实现跨平台操作。未来，Bots可对接万物，让从虚拟到实体的应用都能与人类自然对话。所以，Bots被界定为后App时代的全新人机交互方式。苹果Siri，微软找冰，小i机器人都属于这个领域的代表。\n通用的会话场景是无穷尽的\n但是客服的会话场景是可以进行穷尽的，是否可以基于场景进行会话机器人设计？\n问答系统六个层次：基础搜索、词联想、本体知识库，短程关系、长程关系、基于上下文的自由问答。从知识提取，知识存储，知识表达，知识检索，到人机交互、知识库，不知道多少个小零件要逐一打造。\n机器善于做短程关系的查找（lookup），一层，罕见的情况下可以做两层。长程关系的发现（discovery）是机器做不好的，只能由人来写，最后变成规则机器执行。那些Siri里有趣的回答，都是人写的，和机器智能无关。\n客服机器人做多少层比较好？三层？两层？\n客服机器人的本质是通过问答的方式提供客服服务，所以关键的动作还是问和答。\n如何准确的捕捉问的操作呢？关键词（为什么？是什么？怎么？）\n人工智能理论体系的系统梳理，会话机器人理论体系的系统梳理；目前会话机器人的学术研究最新进展，偏向国外，偏向博士论文，偏向技术方面（最新技术/主流技术）\n人工智能理论体系介绍\n会话机器人理论体系\n最新学术研究进展：\n知识库的构建 用AIML可以做吗？\n自然语言处理很困难：（1）语言是不完全有规律的，规律是错综复杂的。有一定的规律，也有很多例外。因为语言是经过上万年的时间发明的，这一过程类似于建立维基百科。因此，一定会出现功能冗余、逻辑不一致等现象。但是语言依旧有一定的规律，若不遵循一定的规范，交流会比较困难；（2）语言是可以组合的。语言的重要特点是能够将词语组合起来形成句子，能够组成复杂的语言表达；（3）语言是一个开放的集合。我们可以任意地发明创造一些新的表达。比如，微信中“潜水”的表达就是一种比喻。一旦形成之后，大家都会使用，形成固定说法。语言本质的发明创造就是通过比喻扩展出来的；（4）语言需要联系到实践知识；（5）语言的使用要基于环境。在人与人之间的互动中被使用。如果在外语的语言环境里去学习外语，人们就会学习得非常快，理解得非常深。\n基于这个设定，我们的会话机器人目前是否只针对无歧义的语句进行准确展出，有歧义的由客户进行选择呢？即在用户的帮助下解决语言的歧义问题\n所有的自然语言处理的问题都可以分类成为五大统计自然语言处理的方法或者模型，即分类、匹配、翻译、结构预测，马尔可夫决策过程。各种各样的自然语言处理的应用，都可以模型化为这五大基本问题，基本能够涵盖自然语言处理相当一部分或者大部分的技术。主要采用统计机器学习的方法来解决。第一是分类，就是你给我一个字符串，我给你一个标签，这个字符串可以是一个文本，一句话或者其他的自然语言单元；其次是匹配，两个字符串，两句话或者两段文章去做一个匹配，判断这两个字符串的相关度是多少；第三就是翻译，即更广义的翻译或者转换，把一个字符串转换成另外一个字符串；第四是结构预测，即找到字符串里面的一定结构；第五是马可夫决策过程，在处理一些事情的时候有很多状态，基于现在的状态，来决定采取什么样的行动，然后去判断下一个状态。\n分类：For 客服机器人，分类是指从答案中抽取一个或多个标签，并有效评估答案和标签的关联紧密程度\n匹配：For 客服机器人，如何把用户输入的问题与知识库中的标签进行关联，并匹配标签对应的知识点 and 解决未匹配的情况\n翻译：即如何将一个文本认为是标签的另外一个翻译\n结构预测：寻找字符串里面的结构\n马尔科夫决策：基于当前的状态太采取什么样的活动\n分类主要有文本分类和情感分类，匹配主要有搜索、问题回答、对话（主要是单轮对话）；翻译主要有机器翻译，语音识别，手写识别，单轮对话；结构预测主要有专门识别，词性标注，句法分析，文本的语义分析；马可夫决策过程可以用于多轮对话。\n自然语言处理，在一定程度上需要考虑技术上界和性能下界的关系。现在的自然语言处理，最本质是用数据驱动的方法去模拟人，通过人工智能闭环去逼近人的语言使用能力。\n在设计之前我们必须对于客服机器人有一个使用上的认识，我们的产品是原型产品，但是也可以满足用户的正常使用，后期随着技术资源的投入在准确性和响应及时性上会有更多的提升，但是基本的准确性和响应及时性也应该有一个保证，所以我们必须对准确性、响应及时性等指标有个下界的评估\n参考这个图，似乎我们可以用SVD来解决匹配问题啊"}
{"content2":"人工智能（artificial intelligence，AI）是科技研究中最热门的方向之一。像 IBM、谷歌、微软、Facebook 和亚马逊等公司都在研发上投入大量的资金、或者收购那些在机器学习、神经网络、自然语言和图像处理等领域取得了进展的初创公司。考虑到人们对此感兴趣的程度，我们将不会惊讶于斯坦福的专家在人工智能报告中得出的结论：“越来越强大的人工智能应用，可能会对我们的社会和经济产生深远的积极影响，这将出现在从现在到 2030 年的时间段里。”\n在最近的一篇文章中，我们概述了 45 个十分有趣或有前途的人工智能项目。在本文中，我们将聚焦于开源的人工智能工具，详细的了解下最著名的 15 个开源人工智能项目。\n以下这些开源人工智能应用都处于人工智能研究的最前沿。\n1. Caffe\n它是由贾扬清在加州大学伯克利分校的读博时创造的，Caffe 是一个基于表达体系结构和可扩展代码的深度学习框架。使它声名鹊起的是它的速度，这让它受到研究人员和企业用户的欢迎。根据其网站所言，它可以在一天之内只用一个 NVIDIA K40 GPU 处理 6000 万多个图像。它是由伯克利视野和学习中心（BVLC）管理的，并且由 NVIDIA 和亚马逊等公司资助来支持它的发展。\n2. CNTK\n它是计算网络工具包（Computational Network Toolkit）的缩写，CNTK 是一个微软的开源人工智能工具。不论是在单个 CPU、单个 GPU、多个 GPU 或是拥有多个 GPU 的多台机器上它都有优异的表现。微软主要用它做语音识别的研究，但是它在机器翻译、图像识别、图像字幕、文本处理、语言理解和语言建模方面都有着良好的应用。\n3. Deeplearning4j\nDeeplearning4j 是一个 java 虚拟机（JVM）的开源深度学习库。它运行在分布式环境并且集成在 Hadoop 和 Apache Spark 中。这使它可以配置深度神经网络，并且它与 Java、Scala 和 其他 JVM 语言兼容。\n这个项目是由一个叫做 Skymind 的商业公司管理的，它为这个项目提供支持、培训和一个企业的发行版。\n4. DMTK\nDMTK 是分布式机器学习工具（Distributed Machine Learning Toolkit）的缩写，和 CNTK 一样，是微软的开源人工智能工具。作为设计用于大数据的应用程序，它的目标是更快的训练人工智能系统。它包括三个主要组件：DMTK 框架、LightLDA 主题模型算法和分布式（多义）字嵌入算法。为了证明它的速度，微软声称在一个八集群的机器上，它能够“用 100 万个主题和 1000 万个单词的词汇表（总共 10 万亿参数）训练一个主题模型，在一个文档中收集 1000 亿个符号，”。这一成绩是别的工具无法比拟的。\n5. H20\n相比起科研，H2O 更注重将 AI 服务于企业用户，因此 H2O 有着大量的公司客户，比如第一资本金融公司、思科、Nielsen Catalina、PayPal 和泛美都是它的用户。它声称任何人都可以利用机器学习和预测分析的力量来解决业务难题。它可以用于预测建模、风险和欺诈分析、保险分析、广告技术、医疗保健和客户情报。\n它有两种开源版本：标准版 H2O 和 Sparking Water 版，它被集成在 Apache Spark 中。也有付费的企业用户支持。\n6. Mahout\n它是 Apache 基金会项目，Mahout 是一个开源机器学习框架。根据它的网站所言，它有着三个主要的特性：一个构建可扩展算法的编程环境、像 Spark 和 H2O 一样的预制算法工具和一个叫 Samsara 的矢量数学实验环境。使用 Mahout 的公司有 Adobe、埃森哲咨询公司、Foursquare、英特尔、领英、Twitter、雅虎和其他许多公司。其网站列了出第三方的专业支持。\n7. MLlib\n由于其速度，Apache Spark 成为一个最流行的大数据处理工具。MLlib 是 Spark 的可扩展机器学习库。它集成了 Hadoop 并可以与 NumPy 和 R 进行交互操作。它包括了许多机器学习算法如分类、回归、决策树、推荐、集群、主题建模、功能转换、模型评价、ML 管道架构、ML 持久、生存分析、频繁项集和序列模式挖掘、分布式线性代数和统计。\n8. NuPIC\n由 Numenta 公司管理的 NuPIC 是一个基于分层暂时记忆（Hierarchical Temporal Memory，HTM）理论的开源人工智能项目。从本质上讲，HTM 试图创建一个计算机系统来模仿人类大脑皮层。他们的目标是创造一个 “在许多认知任务上接近或者超越人类认知能力” 的机器。\n除了开源许可，Numenta 还提供 NuPic 的商业许可协议，并且它还提供技术专利的许可证。"}
{"content2":"课程：\n6.891 (Fall 2003): Machine Learning Approaches for Natural Language Processing\nhttp://www.ai.mit.edu/courses/6.891-nlp/\nCS 276 / LING 286 Information Retrieval and Web Search Spring 2012\nhttp://www.stanford.edu/class/cs276/index.html\nCS 224d DL for NLP\nhttps://cs224d.stanford.edu/\nhttps://mp.weixin.qq.com/s/ywGWUvLyjDFyyg6wPlzZjw\n阅读列表：\n清华nlp小组整理的一个机器翻译的阅读列表，分类整理了机器翻译（尤其是神经机器翻译）\n<!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 14.0px 'Helvetica Neue'; color: #00a2ff} -->\nhttps://github.com/THUNLP-MT/MT-Reading-List\n资源：\nInformation Retrieval Resources\nhttp://nlp.stanford.edu/IR-book/information-retrieval.html\nStatistical natural language processing and corpus-based computational linguistics: An annotated list of resources\nhttp://www-nlp.stanford.edu/links/statnlp.html\nSpeech and Language Processing\nhttps://web.stanford.edu/~jurafsky/slp3/\nMachine Learning for Natural Language Processing (ML-for-NLP)\nhttps://wiki.inf.ed.ac.uk/MLforNLP\nNLTK 2.0 documentation\nhttp://nltk.org/\nNatural Language Processing with Python\n--- Analyzing Text with the Natural Language Toolkit\nhttp://nltk.org/book/\nPYTHON自然语言处理中文翻译 NLTK Natural Language Processing with Python 中文版.pdf\nhttp://vdisk.weibo.com/s/4ffue/1334656530\nProject Gutenberg - free ebooks\nhttp://www.gutenberg.org/\n医学搜索引擎\n<!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 14.0px 'Helvetica Neue'; color: #00a2ff} -->\nwww.pubmed.com\n博客文章：\n百度朱凯华：智能搜索和对话式OS最新技术全面解读（65PPT）\nhttps://wx.abbao.cn/a/11210-7bb27d2a59c0f19c.html\nhttps://mp.weixin.qq.com/s/oMoPk8FoVm9xMQpQISgq6A\n“搜你所想”之用户搜索意图识别\nhttp://www.infoq.com/cn/articles/user-search-intention-recognition\n百度副总裁王海峰：百度在NLP领域都做了什么？\nhttps://mp.weixin.qq.com/s/vjYv6zWn4OIQo18HA2vHqw\n《搜索与推荐中的深度学习匹配》之搜索篇\nhttps://zhuanlan.zhihu.com/p/38296950\n<!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 14.0px '.PingFang SC Light'} -->\n<!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 14.0px '.PingFang SC Light'} -->"}
{"content2":"这本书是在博客园里乱转时发现的，报着对数学的强烈兴趣就买下了。由于书中的一些数学算法还是有相当难度的，且与我的实际工作相关性不太大，按照Action笔记的思想，只记录一些对我有启发的Action。\n第1章 文字和语言 vs 数字和信息\n罗塞塔石碑记录了3种语言，难怪我用的一款非常不错的Rosetta软件是学语言用的。\n看看《从一到无穷大》这本书。从新浪上找到了，一本古老的书，抽时间看看。\n第2章 自然语言处理 — 从规则到统计\n图灵测试(Turing Test)----让人和机器进行交流，如果人无法判断自己交流的对象是人还是机器时，就说明这个机器有智能了。\n自然语言处理采用句法分析和语义理解竟然走不通，全面地转向统计模型方法了。难道自然语言经过了几千年的演变，已经变得相当复杂，许多语法并不是确定性的而是模糊的？已经不能用计算机来处理？还是句法分析的算法需要一场革命性的变革？\n第3章 统计语言模型\n条件概率P(w2|w1)：已经第一个词w1的前提下，第二个词w2出现的概率。\nP(w2|w1) ≈ #(w1, w2) / #(w1)\n其中：#(w1, w2)是w1,w2两个词连续出现的次数，#(w1)是w1出现的次数，上述公式是在统计数量非常大的情况下成立。\n零概率问题的处理：古德-图灵估计(Good-Turing Estimate)\n第4章 谈谈中文分词\n用统计模型可以很好地解决中文分词问题。\n第5章 隐含马尔可夫模型\n这一章与概率论、随机过程联系起来了，想想大学时学的一点点概率论的课程，几乎没有什么印象了。\n第6章 信息的度量和作用\n以前做数据压缩时也遇到过Shannon给熵做的定义，给定随机变量X，fX为X的概率密度函数，则其熵h(X)为：\n一个离散化的公式，对于离散信号X＝{x1, x2, …, xk}, P(X=xi)表示xi出现的概率，则该信号的信息熵为：\n第7章 贾里尼克和现代语言处理\n这一章里谈到少年时的教育问题，有两个观点比较赞同。中学阶段花了很长时间比同伴多读的课程，在大学以后可以用非常短的时间读完，因为大学时期的理解力比中学时要强许多倍。学习（和教育）是一个人一辈子的过程，中国的孩子由于中学阶段读了大量的教科书，在大学之后往往厌倦了读书。\n第8章 简单之美\n这里提到了做好搜索的一个经验，最基本的要求就是每天分析10-20个不好的搜索结果。对于人工智能中的机器博弈来说，每天做10-20个不好的局面评估的分析是不是也是一种提高的办法？\n第9章 图论和网络爬虫\n这里提到了人工智能中问题求解算法中常用的广度优先搜索BFS和深度优先搜索DFS。\n第10章 PageRank — Google的民主表决式网络排名技术\n没看懂，以后再说。\n第11章 如何确定网页和查询的相关性\nTF-IDF\n第12章 地图和本地搜索的最基本技术-有限状态机和动态规划\n第13章 Google AK-47 的设计者\n第14章 余弦定理和新闻的分类\n原来余弦定理中计算向量的夹角，还可以用来比较两组特征的相似度，这是第一次翻看本书时让我感兴趣的事。一个中学时学到的知识，实际上在实际中是有许多应用的。如果中学老师知道这些广泛的应用，可能会激发学生的无限兴趣，当你知道空间中的夹角实际上可以映射为现实生活中的其它属性，展开了这种想象后，数学知识才能真的发挥其作用。\ncos(A) = (b*b + c*c – a*a) / (2*b*c)\n第15章 矩阵运算和文本处理中的两个分类问题\n这里提到了矩阵的奇异值分解，在大学时我一直没搞明白矩阵的一大堆运算是干什么用的。\n第16章 信息指纹及其应用\n这里提到了更好的随机数生成算法----梅森旋转算法Mersenne Twister，正好我的Zobrist HASH算法中需要用到随机数，想试试这更好的随机数能不能减少一些HASH冲突？从网上搜索了一下找到了相关C语言代码，在我的Visual Studio 2010中出现链接错误，还没找到原因。\n这里提到的信息指纹与中国象棋局面表示中的Zobrist HASH算法是类似的。\n第17章 由电视剧《暗算》所想到的 — 谈谈密码学的数学原理\n关于大素数分解的加密算法\n第18章 闪光的不一定是金子 — 谈谈搜索引擎的反作弊问题\n不关心。\n后面的内容实在读不下去了，也不关心了，只有布隆过滤器可能对我还有些用，以后再说吧，把章节的名称放在这里吧。\n第19章 谈谈数学模型的重要性\n第20章 不要把鸡蛋放到一个篮子里 — 谈谈最大熵模型\n第21章 拼音输入法的数学原理\n第22章 自然语言处理的教父马库斯和他的优秀弟子们\n第23章 布隆过滤器\n第24章 马尔可夫链的扩展 — 贝叶斯网络\n第25章 条件随机场和句法分析\n第26章 维特比和他的维特比算法\n第27章 再谈文本自动分类问题 — 期望最大化算法\n第28章 逻辑回归和搜索广告\n第29章 各个击破算法和Google 云计算的基础"}
{"content2":"最近把一些在网上见到的自然语言处理的资源整理了一下，包括论文列表、软件资源和一些实验室主页、个人主页等，希望能对NLP研究者有所帮助，由于个人视野有限，目前只整理了这些，以后会持续更新。在此也感谢这些资源的提供者和维护者。\n转载请标明出处（http://blog.csdn.net/xuh5156/article/details/7437475）\n论文、博客\n1.       Google在研究博客中总结了他们2011年的精彩论文《Excellent Papers for 2011》，包括社会网络、机器学习、人机交互、信息检索、自然语言处理、多媒体、系统等各个领域，很精彩的论文集锦。http://googleresearch.blogspot.com/2012/03/excellent-papers-for-2011.html\n或者zibuyu的BLOG http://blog.sina.com.cn/s/blog_574a437f0100y6zy.html\n2.       Best paper awards for AAAI,ACL, CHI, CIKM, FOCS, ICML, IJCAI, KDD, OSDI, SIGIR, SIGMOD, SOSP, STOC, UIST,VLDB, WWWhttp://jeffhuang.com/best_paper_awards.html\n3.       IBM R&D Journal 刚发布了关于Watson的专刊《This is Watson》。总共17篇论文。http://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=6177717&punumber=5288520\n4.       Web Data Mining作者刘兵维护的一个专题资源：Opinion Mining,Sentiment Analysis, and Opinion Spam Detection 。http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html\n5.       Statistical Machine Translationhttp://www.statmt.org/\nStatistical Machine TranslationTutorial Readinghttp://cseweb.ucsd.edu/~dkauchak/mt-tutorial/\nPhilipp Koehn主页http://homepages.inf.ed.ac.uk/pkoehn/\n6.       Profile Hidden Markov ModelResourceshttp://webdocs.cs.ualberta.ca/~colinc/cmput606/\nHidden Markov Model (HMM) Toolbox forMatlabhttp://www.cs.ubc.ca/~murphyk/Software/HMM/hmm.html\n7.       CRF http://www.inference.phy.cam.ac.uk/hmw26/crf/\nConditional Random Field (CRF)Toolbox for Matlabhttp://www.cs.ubc.ca/~murphyk/Software/CRF/crf.html\nFlexCRFs: Flexible Conditional RandomFieldshttp://flexcrfs.sourceforge.net/\n8.       Transfer Learning 包含papers、talks、software等http://www.cse.ust.hk/TL/index.html\n9.       Topic Model，Topic Modeling Bibliographyhttp://www.cs.princeton.edu/~mimno/topics.html\nDavid M. Blei的主页http://www.cs.princeton.edu/~blei/publications.htmlMatlab Topic Modeling Toolbox 1.4http://psiexp.ss.uci.edu/research/programs_data/toolbox.htm\nLDA GIBBS Java源码http://arbylon.net/resources.html\nGibbsLDA++: A C/C++ Implementation ofLatent Dirichlet Allocationhttp://gibbslda.sourceforge.net/\n10.   科学网—推荐系统的循序进阶读物（从入门到精通） - 张子柯的博文http://blog.sciencenet.cn/home.php?mod=space&uid=210641&do=blog&id=508634\n11.   SVM入门http://www.blogjava.net/zhenandaci/archive/2009/02/13/254519.html\n12.   斯坦福大学自然语言处理实验室整理的NLP资源http://www-nlp.stanford.edu/links/statnlp.html\n13.   Stanford University InformationRetrieval Resourceshttp://nlp.stanford.edu/IR-book/information-retrieval.html\n14.   Software Tools for NLP http://www-a2k.is.tokushima-u.ac.jp/member/kita/NLP/nlp_tools.html\n实验室主页\n1.       The Stanford NLP Group http://nlp.stanford.edu\n2.       The Berkeley Natural LanguageProcessing Grouphttp://nlp.cs.berkeley.edu\n3.       University of Tokyo TsujiiLaboratory http://www.nactem.ac.uk/tsujii/publications.cgi?lang=en\n4.       Korea University NLP http://nlp.korea.ac.kr/ http://nlp.korea.ac.kr/new/\n5.       中国科学院计算技术研究所自然语言处理研究组http://nlp.ict.ac.cn/new/\n6.       清华大学自然语言处理组 http://nlp.csai.tsinghua.edu.cn/site2/\n7.       HIT-SCIR http://ir.hit.edu.cn/\n8.       苏州大学自然语言处理实验室http://nlp.suda.edu.cn/\n个人主页\n1.       David M. Blei， (Princeton) LDA，http://www.cs.princeton.edu/~blei/publications.html\n2.       Noah Smith, (CMU),以自然语言处理、机器学习为基础研究computationalsocial science。http://www.cs.cmu.edu/~nasmith/\n3.       Philipp Koehn (University ofEdinburgh)http://homepages.inf.ed.ac.uk/pkoehn/\n4.       Dekang Lin (University ofAlberta)http://webdocs.cs.ualberta.ca/~lindek/\n5.       Michael Collins(ColumbiaUniversity)http://www.cs.columbia.edu/~mcollins/\n6.       Dekai WU(HKUST) http://www.cs.ust.hk/~dekai/\n7.       Pascale Fung (HKUST) http://www.ee.ust.hk/~pascale/\n8.       Alessandro Moschitti (Universityof Trento)http://disi.unitn.it/moschitti/\n9.       Xiaojin (Jerry) Zhu (Universityof Wisconsin-Madison)http://pages.cs.wisc.edu/~jerryzhu/\n10.   Eugene Charniak (BrownUniversity)http://www.cs.brown.edu/~ec/\n转自：http://blog.csdn.net/xuh5156/article/details/7437475"}
{"content2":"Chapter7\nExtracting Information from Text   从文本提取信息\nFor any given question, it's likely that someone has written the answer down somewhere. The amount of natural language text that is available in electronic form is truly staggering（令人惊愕的）, and is increasing every day. However, the complexity of natural language can make it very difficult to access the information in that text. The state of the art（目前的技术水平） in NLP is still a long way from being able to build general-purpose（多种目的） representations of meaning from unrestricted text. If we instead focus our efforts on a limited set of questions or \"entity relations,\" such as \"where are different facilities located,\" or \"who is employed by what company,\" we can make significant progress. The goal of this chapter is to answer the following questions:\n1.        How can we build a system that extracts structured data, such as tables, from unstructured text?\n我们如何构建一个系统从非结构化的文本中来抽取结构化数据，例如表\n2.        What are some robust methods for identifying the entities and relationships described in a text?\n有哪些强健的方法来识别文中描述的实体和关系？\n3.        Which corpora are appropriate for this work, and how do we use them for training and evaluating our models?\n哪个语料库适合这项工作,并且我们如何使用它们来训练和评价我们的            模型？\nAlong the way, we'll apply techniques from the last two chapters to the problems of chunking and named-entity recognition.\n沿着这种方式，我们将应用最后两章中的技术来解决分块和命名实体识别。\n7.1   Information Extraction 信息抽取\nInformation comes in many shapes and sizes. One important form is structured data（结构化数据）, where there is a regular and predictable organization of entities and relationships. For example, we might be interested in the relation between companies and locations. Given a particular company, we would like to be able to identify the locations where it does business; conversely, given a location, we would like to discover which companies do business in that location. If our data is in tabular form, such as the example in Table 7.1, then answering these queries is straightforward.\nOrgName\nLocationName\nOmnicom\nNew York\nDDB Needham\nNew York\nKaplan Thaler Group\nNew York\nBBDO South\nAtlanta\nGeorgia-Pacific\nAtlanta\nIf this location data was stored in Python as a list of tuples (entity, relation, entity), then the question \"Which organizations operate in Atlanta?\" could be translated as follows:\n>>> print [org for (e1, rel, e2) if rel=='IN' and e2=='Atlanta']\n['BBDO South', 'Georgia-Pacific']\nThings are more tricky（棘手的） if we try to get similar information out of text. For example, consider the following snippet (from nltk.corpus.ieer, for fileid NYT19980315.0085).\n(1)\nThe fourth Wells account moving to another agency is the packaged paper-products division of Georgia-Pacific Corp., which arrived at Wells only last fall. Like Hertz and the History Channel, it is also leaving for an Omnicom-owned agency, the BBDO South unit of BBDO Worldwide. BBDO South in Atlanta, which handles corporate advertising for Georgia-Pacific, will assume additional duties for brands like Angel Soft toilet tissue and Sparkle paper towels, said Ken Haldin, a spokesman for Georgia-Pacific in Atlanta.\nIf you read through (1), you will glean（收集） the information required to answer the example question. But how do we get a machine to understand enough about (1) to return the answers in Table 7.2? This is obviously a much harder task. Unlike Table 7.1, (1) contains no structure that links organization names with location names.\nOne approach to this problem involves building a very general representation of meaning (Chapter 10). In this chapter we take a different approach, deciding in advance that we will only look for very specific kinds of information in text, such as the relation between organizations and locations. Rather than trying to use text like (1) to answer the question directly, we first convert the unstructured data of natural language sentences into the structured data of Table 7.1. Then we reap the benefits of （获得益处）powerful query tools such as SQL. This method of getting meaning from text is called Information Extraction（信息提取）.\nInformation Extraction has many applications, including business intelligence, resume harvesting, media analysis, sentiment detection（情感检测）, patent search（专利检索）, and email scanning. A particularly important area of current research involves the attempt to extract structured data out of electronically-available scientific literature, especially in the domain of biology and medicine.\nInformation Extraction Architecture 信息提取结构\nFigure 7.1 shows the architecture for a simple information extraction system. It begins by processing a document using several of the procedures discussed in Chapter 3 and Chapter 5: first, the raw text of the document is split into sentences using a sentence segmenter, and each sentence is further subdivided into words using a tokenizer. Next, each sentence is tagged with part-of-speech tags, which will prove very helpful in the next step, named entity detection（命名实体检测）. In this step, we search for mentions of（提及） potentially interesting entities in each sentence. Finally, we use relation detection（关系检测） to search for likely relations between different entities in the text.\nFigure 7.1: Simple Pipeline Architecture for an Information Extraction System. This system takes the raw text of a document as its input, and generates a list of (entity, relation, entity) tuples as its output. For example, given a document that indicates that the company Georgia-Pacific is located in Atlanta, it might generate the tuple ([ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']).\nTo perform the first three tasks, we can define a simple function that simply connects together NLTK's default sentence segmenter, word tokenizer, and part-of-speech tagger:\n>>> def ie_preprocess(document):\n...    sentences = nltk.sent_tokenize(document)\n...    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n...    sentences = [nltk.pos_tag(sent) for sent in sentences]\nNote\nRemember that our program samples assume you begin your interactive session or your program with: import nltk, re, pprint\nNext, in named entity detection, we segment and label the entities that might participate in interesting relations with one another. Typically, these will be definite noun phrases such as the knights who say \"ni\", or proper names such as Monty Python. In some tasks it is useful to also consider indefinite（不明确的） nouns or noun chunks, such as every student or cats, and these do not necessarily refer to entities in the same way as definite NPs and proper names.\nFinally, in relation extraction, we search for specific patterns between pairs of entities that occur near one another in the text, and use those patterns to build tuples recording the relationships between the entities."}
{"content2":"这是《人工智能系列笔记》的第二篇，我利用周六下午完成课程学习。这一方面是因为内容属于入门级，并且之前我已经对认知服务和机器人框架比较熟悉。\n如有兴趣，请关注该系列 https://aka.ms/learningAI\n但是学习这门课程还是很有收获，这篇笔记时特别加了\"探秘\"两个字，这是因为他不仅仅是介绍了微软的认知服务和机器人框架及其如何快速开始工作，更重要的是也做了很多铺垫，例如在讲文本分析服务（Text Analytics）之前，课程用了相当长的篇幅介绍了文本处理的一些技术原理，毕竟无论是微软的认知服务，还是其他厂商的服务，或者你自己尝试去实现，其内部的原理都是类似的。\n我将给大家分享三个部分的内容\n文本理解和沟通\n计算机视觉\n对话机器人\n第一部分：文本理解和沟通\n现在人工智能很火，花样也很多，可能大家不会想到，很早之前人类对于机器智能的研究，最主要就是在文本理解和处理这个部分，科学家们想要实现的场景主要如下\n这跟人类本身的学习及成长是类似的，一旦机器掌握这些能力，其实就相当于具备了\"听说读写\"的能力。我据说微软二十年前创立研究院之处，主要的研究范围也是在这个领域，二十年过去了还在继续投资，不断优化这方面的能力，可见其作为人工智能的重要性。\n其实这里提到的大部分过程，可以理解为通常意义上的自然语言处理（Natual Language Processing——NLP）的研究范畴。\n本次课程中使用python进行讲解，提到了一个关键的package：NLTK（Natual Language Toolkit），以及它的几个更加具体的库：freqdist 用来做字（词）频分析，stem用来做词干提取等等。\n下面是一些基本的用法\n也就是说，其实你用NLTK能做出绝大部分文本理解和处理的场景，当然如果你用微软的认知服务（Cognitive Service），则可以省去很多基础性的工作，而是直接专注在业务问题上。\n前面三种服务都相对简单，通常你只需要开通，并且调用相关的API 即可，例如 Text Analytics 可用来检测文本语言，识别其中的实体，关键信息，以及情感分析。\n而Language understanding 则相对更加复杂一点，它的全称是Language understanding intelligence service （Luis），是有一套完整的定义、训练、发布的流程。换言之，Luis允许你自定义模型，而前面三者则是利用微软已经训练好的模型立即开始工作。申请Luis服务是在Azure的门户中完成的，而要进行模型定义和训练，则需要通过 https://luis.ai 这个网站来完成。\n下面是我用来测试的一个模型的其中一个Intent （Luis能同时支持多种语言，甚至也能做到中英文混合文本的理解）\nLuis最大的一个使用场合可能是结合本文最后面提到的对话机器人来实现智能问答。\n第二部分：计算机视觉\n如果说文本智能是尝试学习人类的\"听说读写\"的能力，那么计算机视觉则是尝试模拟人类的眼睛，来实现\"看\"的能力。\n图像分析其实就是好比人类看到一个物体（或者其影像），脑电波反射过来信号，使得你意识到你看到的是什么。\n这个能力用到了预先训练好的模型。这个可以通过认知服务中的Computer Vision这个组件实现。\n但是，即便是上面的模型已经包含了数以百万计的照片，但相对而言还是很小的一个集合。所以，如果你想实现自己的图像识别，可以使用认知服务中提供的Custom vision这个能力来实现。\nCustom vision拥有一个同样很酷的主页：https://customvision.ai/ ，通过这个网站，你可以上传你预先收集好的照片，并且为其进行标记，通常情况下，每个标记至少需要5张照片，然后通过训练即可发布你的服务，并且用于后续的图像识别检测（例如某个图像是不是汽车，或者香蕉之类的）。\n人脸识别，则是特定领域的图像识别，这个应用也是目前在人工智能领域最火的一个，而也因为脸是如此重要，所以在认知服务中，有一个专门的API，叫Face API。\n使用这套API，可以做出来很有意思的应用，例如\n从技术上说，图像（Image）是由一个一个有颜色的数据点构成的，这些数据点通常用RGB值表示。而视频（Video）则是由一幅一幅的图像（Image，此时称为帧）构成的。所以，计算机视觉既然能做到图像的识别和理解（虽然可能会有偏差），那么从技术上说，它也就具备了对视频进行识别和理解的能力，如果再加上之前提到的文本智能，它就能至少实现如下的场景：\n识别视频中出现的人脸，以及他们出现的时间轴。如果是名人，也会自动识别出来，如果不是，支持标记，下次也能识别出来。\n识别视频中的情感，例如从人脸看出来的高兴还是悲伤，以及欢呼声等环境音。\n文本识别（OCR）——根据图像生成文字。\n自动生成字幕，并支持翻译成其他语言。\n第三部分：对话机器人\n我记得是在2016年的Build大会上，微软CEO Sayta 提出了一个新的概念：Conversation as a Platform, 简称CaaP，其具体的表现形式就是聊天机器人（chatbot）。\n当时的报道，请参考 https://www.businessinsider.sg/microsoft-ceo-satya-nadella-on-conversations-as-a-platform-and-chatbots-2016-3/?r=US&IR=T\n对话机器人这个单元，讲的就是这块内容。与人脸识别技术类似，机器人这个技术在这几年得到了长足的发展和广泛的应用，甚至到了妇孺皆知的地步。这里谈到的机器人，特指通过对话形式与用户进行交互，并且提供服务的一类机器人，广泛地应用于智能客服、聊天与陪伴、常见问题解答等场合。\n创建一个对话机器人真的很简单，如果你有一个Azure订阅的话。微软在早些时候已经将机器人框架（Bot Framework）完全地整合到了Azure平台。\n做一个机器人（Bot）其实真的不难，但要真的实现比较智能的体验，还真的要下一番功夫。目前比较常见的做法是，前端用Bot Framework定义和开发Bot（用来与用户交互），后台会连接Luis服务或QnA maker服务来实现智能体验，如下图所示。\n我在11月份的Microsoft 365 DevDays（开发者大会）上面专门讲解了机器人开发，有兴趣可以参考 https://github.com/chenxizhang/devdays2018-beijing 的资料。\n机器人框架 （Bot Framework）的一个强大之处在于，你可以实现编写一次，处处运行，它通过频道（Channel）来分发服务。目前支持的频道至少有16种。\n我自己之前用过Web Chat，Microsoft Teams，以及Direct Line和Skype for Business等四种。一直对Cortana这个场景比较感兴趣，这次通过学习，终于把这个做成功了，还是挺有意思的。\n这项功能，还有一个名称：Cortana Skills，目前需要用Microsoft Account注册这个Bot）。\n请通过 https://aka.ms/learningAI 或者扫描下面的二维码关注本系列文章《人工智能学习笔记》"}
{"content2":"不久前，高盛发布的名为《中国在人工智能领域崛起》的研究报告，报告中，高盛认为中国已经成为AI领域的主要竞争者，中国政府建设“智慧型经济”和“智慧社会”的目标将有可能推动中国未来GDP的增长。\n在这份报告中显示，中国的AI发展在人才、数据、基础设施和计算能力4个方面具有优势，在人才、数据和基础设施等方面已经具备了全力发展AI的实力。而今年3月，AI出现在政府工作报告中，7月中国公布首个国家战略层面的AI发展计划，都让全世界对中国的AI发展充满信心。\n在人工智能领域，美国比中国早发展5年，美国从1991年开始萌芽，而中国在1996年才诞生人工智能企业。然而因为有BAT这样众多互联网公司，中国在AI领域高速发展。\n一、中国AI领域风险投资奋起直追\n根据腾讯发布的《2017中美人工智能创投现状与趋势研究报告》中的数据，美国AI领域累计风险投资978亿元，中国累计风险投资635亿元，虽然在总金额上有差距，但是中国超过1亿美元的大型投资有22笔，累计353.5亿元，美国超过1亿美元的大型投资有11比，总计417.3亿元。\n在2017年前3个月，国内AI领域获得投资的企业有36个，半数以上融资金额超过千万，深醒科技、纵目科技、中译语通等数家企业获得超过1亿元人民币以上的融资，中国在人工智能领域的投资不断增长。\n二、中国AI领域企业数量差距缩短\n截止到2017年6月，全球人工智能企业总数达到2542家，美国1078家，中国592家。但是，美国的人工智能企业从1991年开始创建，到2013年达到峰值，而中国人工智能企业从1996年开始创建，到2015年达到峰值。\n起步期早于中国5年，发展期早于中国6年，爆发期和平缓期都只早于中国2年，从企业数量来看，中国已追平美国3-4年的时间差距，而中国的发展势头仍高于美国。\n三、中国AI企业更关注应用层\n美国AI创业公司中，自然语言处理、机器学习应用及计算机视觉与图像等3个领域的企业数量最多，而中国在计算机视觉与图像、智能机器人及自然语言处理等3个领域的企业数量最多。两者差别主要在机器学习和智能机器人2个方面，机器学习属于企业或个人辅助工具，各个行业均有涉及，而智能机器人更多专注于医疗、家居等专业领域的机器人，与实际应用关联度更高。\n中国的人工智能企业更关注应用层，而美国更看好基础层。在处理器、芯片等领域的创业企业中，美国有33家，中国仅有14家，这也是高盛发布的报告中指出的，中国AI领域唯一短板在于芯片领域。\n四、中国AI行业有数据方面的天然优势\n数据是中国AI产业发展的最大优势，中国拥有14亿人口，每年产生的数字信息约占全球的13%，各大AI创业企业依靠海量数据来进行更精准的分析。比如滴滴出行平均每天处理超过4500T数据，收到超过200亿次路线请求并处理超过2000万个订单。对滴滴在深度学习、人机交互、机器视觉及智能驾驶技术方面开展的研发活动带来帮助。\n五、中国技术型企业推送AI行业发展\n根据统计，2015年全球发表的顶级AI论文中，有43%的论文作者中有一名及一名以上的中国研究人员，到2016年10月份，中国在AI领域有超过1.6万个专利。\n而在AI研发投入上，百度的研发投入占营收14.4%，接近谷歌的15.5%和微软的14.5%。而在研发人员占比方面，腾讯研发人员占比51%，阿里巴巴占比45%，百度占比43%，都超过了谷歌（38%）和微软（32%）的研发人员占比。\n在招揽高级人才方面，百度为机器学习科学家提供的基本年薪为12.6万美元，加上每年的分红和权益，达到22万美元/年，仅次于Facebook的27.3万美元和微软的24.4万美元。\n六、中国技术企业的重心向AI转移\n百度提出“一切以AI为先”的思维，在全球建立三个实验室，有超过2000名AI研发工程师，开放了人工智能对话平台DuerOS和自动驾驶项目Apollo。\n在中国市场中，阿里云一枝独秀，市场份额达到40.7%，到6月份拥有100万个付费用户，覆盖媒体、互联网、金融、政府等领域，三分之一的500强企业和三分之二的独角兽企业都在使用阿里云。\n而阿里云的飞天Apsara处理系统每秒可处理数十万的并行事务，2016年双十一每秒处理17.5万次交易和12万次支付。\n并且，高盛十分看好阿里旗下的蚂蚁金服，蚂蚁金服覆盖全球32亿人口，付款数据覆盖7.17亿消费者，高盛提出如何阿里继续提高算法，那么阿里中国零售收入除以中国零售GMV的比例可达到5%。\n而腾讯后来居上，2016年成立人工智能实验室，2017年成立西雅图AI实验室，对计算机视觉、语音识别、自然语言处理和机器学习四个方面进行研究。2017年8月，腾讯联合数个领域的专家结成联盟，支持自动驾驶的研究和未来相关产品的制造。\n七、中国的人工智能行业是大势所趋\n中国人工智能行业已成为未来最大的风口，注重于应用层的AI企业将通过良性循环改变人们的生活生产方式，带来生活便利性。\n在2017年TOP100全球软件案例研究峰会上，专注于“城市计算”领域的微软亚洲研究院首席研究院郑宇担任联席主席，给予中国人工智能企业更多建议。\n第六届TOP100全球软件案例研究峰会将于11月9-12日在北京国家会议中心举办，甄选100个本年度最具行业代表性的软件研发案例，现场解读其解决方案和背后的技术逻辑，帮助研发团队快速提高效能。\n更多TOP100案例信息及日程请前往[官网]查阅。4天时间集中分享2017年最值得学习的100个研发案例实践。本平台共送出10张开幕式单天免费体验票，登录TOP100summit官网即可申请，数量有限，先到先得。"}
{"content2":"一、信息提取模型\n信息提取的步骤共分为五步，原始数据为未经处理的字符串，\n第一步：分句，用nltk.sent_tokenize(text)实现,得到一个list of strings\n第二步：分词，[nltk.word_tokenize(sent) for sent in sentences]实现，得到list of lists of strings\n第三步：标记词性，[nltk.pos_tag(sent) for sent in sentences]实现得到一个list of lists of tuples\n前三步可以定义在一个函数中：\n>>> def ie_preprocess(document): ... sentences = nltk.sent_tokenize(document) ... sentences = [nltk.word_tokenize(sent) for sent in sentences] ... sentences = [nltk.pos_tag(sent) for sent in sentences]\n第四步：实体识别（entity detection）在这一步，既要识别已定义的实体（指那些约定成俗的习语和专有名词），也要识别未定义的实体，得到一个树的列表\n第五步：关系识别（relation detection）寻找实体之间的关系，并用tuple标记，最后得到一个tuple列表\n二、分块（chunking）\n分块是第四步entity detection的基础，本文只介绍一种块noun phrase chunking即NP-chunking，这种块通常比完整的名词词组小，例如：the market for system-management software是一个名词词组，但是它会被分为两个NP-chunking——the market 和 system-management software。任何介词短语和从句都不会包含在NP-chunking中，因为它们内部总是会包含其他的名词词组。\n从一个句子中提取分块需要用到正则表达式，先给出示例代码：\ngrammar = r\"\"\" NP: {<DT|PP\\$>?<JJ>*<NN>} # chunk determiner/possessive, adjectives and noun {<NNP>+} # chunk sequences of proper nouns \"\"\" cp = nltk.RegexpParser(grammar) sentence = [(\"Rapunzel\", \"NNP\"), (\"let\", \"VBD\"), (\"down\", \"RP\"), (\"her\", \"PP$\"), (\"long\", \"JJ\"), (\"golden\", \"JJ\"), (\"hair\", \"NN\")] >>> print(cp.parse(sentence)) (S (NP Rapunzel/NNP) let/VBD down/RP (NP her/PP$ long/JJ golden/JJ hair/NN))\n正则表达式的格式为\"\"\"块名：{<表达式>...<>}\n{...}”\"\"\n如：\ngrammar = r\"\"\" NP: {<DT|PP\\$>?<JJ>*<NN>} # chunk determiner/possessive, adjectives and noun {<NNP>+} # chunk sequences of proper nouns \"\"\"\n大括号内为分块规则（chunking rule），可以有一个或多个，当rule不止一个时，RegexpParser会依次调用各个规则，并不断更新分块结果，直到所有的rule都被调用。nltk.RegexpParser(grammar)用于依照chunking rule创建一个chunk分析器，cp.parse()则在目标句子中运行分析器，最后的结果是一个树结构，我们可以用print打印它，或者用result.draw()将其画出。\n在chunking rule中还用一种表达式chink，用于定义chunk中我们不想要的模式，这种表达式的格式为：‘  }表达式{  ’ 使用chink的结果一般有三种，一、chink定义的表达式和整个chunk都匹配，则将整个chunk删除；二、匹配的序列在chunk中间，则chunk分裂为两个小chunk；三、在chunk的边缘，则chunk会变小。使用方法如下：\ngrammar = r\"\"\" NP: {<.*>+} # Chunk everything }<VBD|IN>+{ # Chink sequences of VBD and IN \"\"\" sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"), (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"), (\"the\", \"DT\"), (\"cat\", \"NN\")] cp = nltk.RegexpParser(grammar) >>> print(cp.parse(sentence)) (S (NP the/DT little/JJ yellow/JJ dog/NN) barked/VBD at/IN (NP the/DT cat/NN))"}
{"content2":"自然语言处理的预备知识\n熟练掌握Python\n微积分，线性代数 （MATH 51, CME 100）\n基本的概率论和统计（CS109）\n机器学习基础（CS229）\n自然语言处理参考书\nDan Jurafsky and James H. Martin. Speech and Language Processing (3rd ed. draft) [link]\nYoav Goldberg. A Primer on Neural Network Models for Natural Language Processing [link]\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press. [link]\n课程链接：\nhttp://web.stanford.edu/class/cs224n/"}
{"content2":"http://www.cnblogs.com/yuxc/archive/2011/08/29/2157415.html#2189237\n关于Python自然语言处理\n关于该书的简介：\n《Python自然语言处理》提供了非常易学的自然语言处理入门介绍，该领域涵盖从文本和电子邮件预测过滤，到自动总结和翻译等多种语言处理技术。在《Python自然语言处理(影印版)》 中，你将学会编写Python程序处理大量非结构化文本。你还将通过使用综合语言数据结构访问含有丰富注释的数据集，理解用于分析书面通信内容和结构的主 要算法。\n《Python自然语言处理》准备了充足的示例和练习，可以帮助你：\n从非结构化文本中抽取信息，甚至猜测主题或识别“命名实体”；\n分析文本语言结构，包括解析和语义分析；\n访问流行的语言学数据库，包括WordNet和树库(treebank)；\n从多种语言学和人工智能领域中提取的整合技巧。\n《Python自然语言处理(影印版)》将帮助你学习运用Python编程语言和自然语言工具包(NLTK)获得实用的自然语言处理技能。如果对于开发 Web应用、分析多语言新闻源或记录濒危语言感兴趣——即便只是想从程序员视角观察人类语言如何运作，你将发现《Python自然语言处理》是一本令人着 迷且极为有用的好书。\n关于学习笔记...\n这是我在阅读Python自然语言处理时写的一些学习笔记，因为受时间的约束(papers&find job...blablabla)，学习笔记采用注释和讨论的形式。 由于初学，翻译可能不准确，希望看到的童鞋能够指出，学习过程中我也会回头来修改翻译不恰当的地方。\n目前还没读完这本书，有些初步的感想与大家分享：\n优点：\n这本书即涉及到了语料库的操作，也对传统的基于规则的方法有所涉及。全书包括了分词（tokenization）、词性标注（POS）、语块（Chunk）标注、句法剖析与语义剖析等方面，是一本实用的自然语言处理教程，即使你不想全面地学习自然语言处理，也会对其中的分析美国历届总统的演讲、对抓取的网页进行解析提取文本，使用正则表达式处理字符串、新闻信息检索等等感兴趣的。此外，Python简洁优雅，适合上手，而且我很喜爱这门语言。\n缺点：\n(1)实用性很强，但对理论性介绍不足，需要自己去查找相关资料，我考虑把相关资料都搜集起来\n(2)毕竟是E文的语言背景，对中文涉及很少，中文与英文尤其在分词上有很大的不同，等我积累了一些心得，我打算把中文分词写成单独的一章。\nAnyway，我把下面这句话送给有兴趣的童鞋。\nNow is better than never.\n——摘自The Zen of Python\n目录索引以及持续更新信息的传送门在此，下面也贴出了目前的目录索引。\nUpdate日志\n创建日期：2011.6.27\n翻译修正：2011.7.20\n修改了已知的翻译错误\n更新1st:  2011.8.5\n为目录添加了相关笔记链接\n更新2nd:2011.8.28\n修正了部分翻译错误\n更新3rd:2011.9.7\n修正了Chapter7的部分翻译错误\n目前施工进度：Chapter7\nTable of Contents     目录\nPreface\n前言\n1. Language Processing and Python\nPython和语言处理\n1.1 Computing with Language: Texts and Words\n语言计算：文本和单词\n1.2 A Closer Look at Python: Texts as Lists of Words\n进一步学习Python：将文本视作单词列表\n1.3 Computing with Language: Simple Statistics\n语言计算：简单的统计\n1.4 Back to Python: Making Decisions and Taking Control\n回到Python:决策和控制\n1.5 Automatic Natural Language Understanding\n自动理解自然语言\n1.6 Summary\n小结\n1.7 Further Reading\n深入阅读\n1.8 Exercises\n练习\n2. Accessing Text Corpora and Lexical Resources\n访问文本语料库和词汇资源\n2.1 Accessing Text Corpora\n访问文本语料库\n2.2 Conditional Frequency Distributions\n条件频率分布\n2.3 More Python: Reusing Code\nMore Python:代码重用\n2.4 Lexical Resources\n词汇资源\n2.5 WordNet\nWordNet词典\n2.6 Summary\n小结\n2.7 Further Reading\n深入阅读\n2.8 Exercises\n练习\n3. Processing Raw Text\n处理原始文本\n3.1 Accessing Text from the Web and from Disk\n从Web和磁盘获得文本\n3.2 Strings: Text Processing at the Lowest Level\n字符串：最底层的文本处理\n3.3 Text Processing with Unicode\n使用Unicode处理文本\n3.4 Regular Expressions for Detecting Word Patterns\n使用正则表达式检测词组\n3.5 Useful Applications of Regular Expressions\n正则表示式的有益应用\n3.6 Normalizing Text\n规格化文本\n3.7 Regular Expressions for Tokenizing Text\n正则表达式用于本文分词\n3.8 Segmentation\n分割\n3.9 Formatting: From Lists to Strings\n格式设定：从列表到字符串\n3.10 Summary\n小结\n3.11 Further Reading\n深入阅读\n3.12 Exercises\n练习\n4. Writing Structured Programs\n编写结构化程序\n4.1 Back to the Basics\n回到基础\n4.2 Sequences\n序列\n4.3 Questions of Style\n关于风格\n4.4 Functions: The Foundation of Structured Programming\n函数：结构化编程的基础\n4.5 Doing More with Functions\n关于函数的更多使用\n4.6 Program Development\n程序开发\n4.7 Algorithm Design\n算法设计\n4.8 A Sample of Python Libraries\nPython库的样本\n4.9 Summary\n小结\n4.10 Further Reading\n深入阅读\n4.11 Exercises\n练习\n5. Categorizing and Tagging Words\n分类和标注单词\n5.1 Using a Tagger\n使用标注器\n5.2 Tagged Corpora\n标记语料库\n5.3 Mapping Words to Properties Using Python Dictionaries\n使用Python字典把单词映射到属性\n5.4 Automatic Tagging\n自动标注\n5.5 N-Gram Tagging\nN-Gram标注\n5.6 Transformation-Based Tagging\n基于转换的标注\n5.7 How to Determine the Category of a Word\n如何决定一个词的类别\n5.8 Summary\n小结\n5.9 Further Reading\n深入阅读\n5.10 Exercises\n练习\n6. Learning to Classify Text\n学习本文分类\n6.1 Supervised Classification\n监督式分类\n6.2 Further Examples of Supervised Classification\n监督式分类的更多例子\n6.3 Evaluation\n评分\n6.4 Decision Trees\n决策树\n6.5 Naive Bayes Classifiers\n朴素贝叶斯分类器\n6.6 Maximum Entropy Classifiers\n最大熵分类器\n6.7 Modeling Linguistic Patterns\n建模语言模式\n6.8 Summary\n小结\n6.9 Further Reading\n深入阅读\n6.10 Exercises\n练习\n7. Extracting Information from Text\n从文本提取信息\n7.1 Information Extraction\n信息抽取\n7.2 Chunking\n分块\n7.3 Developing and Evaluating Chunkers\n分块器开发和求值\n7.4 Recursion in Linguistic Structure\n语言结构中的递归\n7.5 Named Entity Recognition\n命名实体识别\n7.6 Relation Extraction\n关系提取\n7.7 Summary\n小结\n7.8 Further Reading\n深入阅读\n7.9 Exercises\n练习\n8. Analyzing Sentence Structure\n句子结构分析\n8.1 Some Grammatical Dilemmas\n一些语法困惑\n8.2 What’s the Use of Syntax?\n语法有什么用处？\n8.3 Context-Free Grammar\n上下文无关语法\n8.4 Parsing with Context-Free Grammar\n使用上下文无关语法进行解析\n8.5 Dependencies and Dependency Grammar\n相关性和相关性语法\n8.6 Grammar Development\n语法的发展\n8.7 Summary\n小结\n8.8 Further Reading\n深入阅读\n8.9 Exercises\n练习\n9. Building Feature-Based Grammars\n构建基于特征的语法\n9.1 Grammatical Features\n语法特征\n9.2 Processing Feature Structures\n处理特征结构\n9.3 Extending a Feature-Based Grammar\n扩展基于特征的语法\n9.4 Summary\n小结\n9.5 Further Reading\n深入扩展\n9.6 Exercises\n练习\n10. Analyzing the Meaning of Sentences\n分析句子的意义\n10.1 Natural Language Understanding\n自然语言的理解\n10.2 Propositional Logic\n命题逻辑\n10.3 First-Order Logic\n一阶逻辑\n10.4 The Semantics of English Sentences\n英文句子的语义\n10.5 Discourse Semantics\n语段语义\n10.6 Summary\n小结\n10.7 Further Reading\n深入阅读\n10.8 Exercises\n练习\n11. Managing Linguistic Data\n语料管理\n11.1 Corpus Structure: A Case Study\n语料库结构：案例研究\n11.2 The Life Cycle of a Corpus\n语料库的生命周期\n11.3 Acquiring Data\n获取数据\n11.4 Working with XML\n处理XML\n11.5 Working with Toolbox Data\n处理Toolbox Data\n11.6 Describing Language Resources Using OLAC Metadata\n使用OLAC元数据描述语言资源\n11.7 Summary\n小结\n11.8 Further Reading\n深入阅读\n11.9 Exercises\n练习\nAfterword: The Language Challenge\n后记：语言的挑战\nBibliography\n参考文献\nNLTK Index\nNLTK索引\nGeneral Index\n一般索引\n知识共享署名、非商业性使用、禁止演绎创作许可证3.0\n以上章节内容均来自Natural Language Processing with Python，由Steven Bird, Ewan Klein 和Edward Loper共同的辛勤劳动，Copyright © 2009，本内容并随NLTK共同发布，网址：http://www.nltk.org/ 。文章和相关资料遵循Creative Commons Attribution-Noncommercial-No Derivative Works 3.0创作许可证。"}
{"content2":"pypinyin\n百度AI的语音识别并不能很好的识别中文同音字,比如'圆圆','媛媛','园园'等,它是根据用户搜索关键字的热度来识别的,那么遇到中文同音字的问题要怎么处理呢?\n现在就要用到Python强大的三方库了,叫 pypinyin\n举个小例子:\nfrom pypinyin import TONE,TONE2,TONE3,lazy_pinyin a = '我叫媛媛' res = lazy_pinyin(a,TONE) res2 = lazy_pinyin(a,TONE2) res3 = lazy_pinyin(a,TONE3) print(res) print(res2) print(res3)\n结果:\n['wǒ', 'jiào', 'yuàn', 'yuàn'] ['wo3', 'jia4o', 'yua4n', 'yua4n'] ['wo3', 'jiao4', 'yuan4', 'yuan4']\n建议大家使用TONE2或者TONE3, 对ASCII码的检索速度更快一点,不包含特殊字符,检索深度低.\njieba 分词\n好的,中文同音字的问题解决了,那么,新的问题又有了,中华语言博大精深,一个问题,N多种问法,比如: '你是谁?',可以是'你叫什么名字?','你的名字叫什么?'等等,那么,怎么才能把这么多问法都指向同一个问题呢?这就引出了人工智能中的另一项技术:\n自然语言处理(NLP) : 大概意思就是 让计算机明白一句话要表达的意思,NLP就相当于计算机在思考你说的话,让计算机知道\"你是谁\",\"你叫啥\",\"你叫什么名字\"是一个意思\n这就要做 : 语义相似度\n接下来我们用Python大法来实现一个简单的自然语言处理\n现在又要用到Python强大的三方库了\n第一个是将中文字符串进行分词的库叫 jieba\npip install jieba\n我们通常把这个库叫做 结巴分词 确实是结巴分词,而且这个词库是 made in china , 基本用一下这个结巴分词:\nimport jieba key_word = \"小白兔,白又白,两只耳朵竖起来\" # 定义一句话,基于这句话进行分词 # jieba.add_word('白又白') #添加关键字,添加的关键字就不会被拆分,比如现在的结果就是: ['白兔', '小白兔', ',', '白又白', ',', '两只', '耳朵', '竖起', '来'] cut_word = jieba.cut_for_search(key_word) # 使用结巴分词中的cut方法进行分词,cut_for_search是基于某搜索引擎进行分词的,比cut分得更细微 print(cut_word) # 结果是个生成器 cut_word_list = list(cut_word) print(cut_word_list) # ['白兔', '小白兔', ',', '白', '又', '白', ',', '两只', '耳朵', '竖起', '来']\ngensim\n分词之后,就是开始下一步了.\n第二个是一个语言训练库叫 gensim\npip install gensim\n这个训练库很厉害, 里面封装很多机器学习的算法, 是目前人工智能的主流应用库,这个不是很好理解, 需要一定的Python数据处理的功底\nGensim是一款开源的第三方Python工具包，用于从原始的非结构化的文本中，无监督地学习到文本隐层的主题向量表达。\n它支持包括TF-IDF，LSI，LDA，和word2vec在内的多种主题模型算法，\n支持流式训练，并提供了诸如相似度计算，信息检索等一些常用任务的API接口\nimport jieba import gensim from gensim import corpora from gensim import models from gensim import similarities l1 = [\"你的名字是什么\", \"你今年几岁了\", \"你有多高你胸多大\", \"你胸多大\"] a = \"你今年多大了\" all_doc_list = [] for doc in l1: doc_list = [word for word in jieba.cut(doc)] all_doc_list.append(doc_list) print(all_doc_list) doc_test_list = [word for word in jieba.cut(a)] # 制作语料库 dictionary = corpora.Dictionary(all_doc_list) # 制作词袋 # 词袋的理解 # 词袋就是将很多很多的词,进行排列形成一个 词(key) 与一个 标志位(value) 的字典 # 例如: {'什么': 0, '你': 1, '名字': 2, '是': 3, '的': 4, '了': 5, '今年': 6, '几岁': 7, '多': 8, '有': 9, '胸多大': 10, '高': 11} # 至于它是做什么用的,带着问题往下看 print(\"token2id\", dictionary.token2id) print(\"dictionary\", dictionary, type(dictionary)) corpus = [dictionary.doc2bow(doc) for doc in all_doc_list] # 语料库: # 这里是将all_doc_list 中的每一个列表中的词语 与 dictionary 中的Key进行匹配 # 得到一个匹配后的结果,例如['你', '今年', '几岁', '了'] # 就可以得到 [(1, 1), (5, 1), (6, 1), (7, 1)] # 1代表的的是 你 1代表出现一次, 5代表的是 了 1代表出现了一次, 以此类推 6 = 今年 , 7 = 几岁 print(\"corpus\", corpus, type(corpus)) # 将需要寻找相似度的分词列表 做成 语料库 doc_test_vec doc_test_vec = dictionary.doc2bow(doc_test_list) print(\"doc_test_vec\", doc_test_vec, type(doc_test_vec)) # 将corpus语料库(初识语料库) 使用Lsi模型进行训练 lsi = models.LsiModel(corpus) # 这里的只是需要学习Lsi模型来了解的,这里不做阐述 print(\"lsi\", lsi, type(lsi)) # 语料库corpus的训练结果 print(\"lsi[corpus]\", lsi[corpus]) # 获得语料库doc_test_vec 在 语料库corpus的训练结果 中的 向量表示 print(\"lsi[doc_test_vec]\", lsi[doc_test_vec]) # 文本相似度 # 稀疏矩阵相似度 将 主 语料库corpus的训练结果 作为初始值 index = similarities.SparseMatrixSimilarity(lsi[corpus], num_features=len(dictionary.keys())) print(\"index\", index, type(index)) # 将 语料库doc_test_vec 在 语料库corpus的训练结果 中的 向量表示 与 语料库corpus的 向量表示 做矩阵相似度计算 sim = index[lsi[doc_test_vec]] print(\"sim\", sim, type(sim)) # 对下标和相似度结果进行一个排序,拿出相似度最高的结果 # cc = sorted(enumerate(sim), key=lambda item: item[1],reverse=True) cc = sorted(enumerate(sim), key=lambda item: -item[1]) print(cc) text = l1[cc[0][0]] print(a,text)"}
{"content2":"机器学习的定义\n专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。\n机器学习是用数据或以往的经验，以此优化计算机程序的性能标准。\n机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在经验学习中改善具体算法的性能。\n对于某类任务T和性能度量P，如果一个计算机程序在T上以P衡量的性能随着经验E而自我完善，那么我们称这个计算机程序在从经验E学习。\n机器学习的分类（根据反馈的不同）\n监督学习：主要特点是要在训练模型时提供给学习系统训练样本以及样本对应的类别标签，因此又称为有导师学习。典型的监督学习方法：决策树、支持向量机（SVM）、监督式神经网络等分类算法和线性回归等回归算法。\n无监督学习：主要特点是训练时只提供给学习系统训练样本，而没有样本对应的类别标签信息。典型的无监督学习方法：聚类学习、自组织神经网络学习\n强化学习：主要特点是通过试错来发现最优行为策略而不是带有标签的样本学习。\n形象化解释：http://blog.csdn.net/by4_Luminous/article/details/53341334\n机器学习的应用方面\n图像处理\\识别（人脸识别、图片分类）\n自然语言处理\n网络安全（垃圾邮件检测、恶意程序\\流量检测）\n自动驾驶\n机器人\n医疗拟合预测\n神经网络\n金融高频交易\n互联网数据挖掘/关联推荐\n机器学习安全分类体系\n1、按照对分类器的影响：\n(1) 诱发性攻击\n(2) 探索性攻击\n2、按照安全损害类型\n(1) 完整性攻击\n(2) 可用性攻击\n(3) 隐私窃取攻击\n3、按照攻击的专一性\n(1) 针对性攻击\n(2) 非针对性攻击\n机器学习敌手模型\n1、敌手目标\n敌手目标可以从两个角度描述，即攻击者期望造成的安全破坏程度（完整性、可用性或隐私性）和攻击的专一性（针对性、非针对性）。例如：攻击者的目标可以是产生一个非针对性的破坏完整性的攻击,来最大化分类器的错误率；也可以产生针对性的窃取隐私的攻击，来从分类器中获得具体的客户隐私信息。\n2、敌手知识\n敌手的知 识可以从分类器的具体组成来考虑，从敌手是否知 道分类器的训练数据、特征集合、学习算法和决策 函数的种类及其参数、分类器中可用的反馈信息 （敌手通过输入数据得到系统返回的标签信息）等 方面将敌手知识划分为有限的知识和完全的知识。\n3、敌手能力\n敌手的知 识主要是指攻击者对训练数据和测试数据的控制 能力。可以从以下几个方面定义：第一是攻击对分 类器造成的影响是探索性的还是诱发性的；第二是 敌手控制训练数据或者测试数据的程度；第三是敌手操纵的特征的内容及具体程度。\n4、攻击策略\n敌手的攻击策略 是指攻击者为了最优化其攻击目的会对训练数据 和测试数据进行的修改措施。具体包括：攻击哪些 样本类型；如何修改类别信息；如何操纵特征等。\n安全性问题汇总\n1、垃圾邮件检测系统和恶意流量检测系统等发现针对系统模型特点来逃避检测的问题\n2、针对面部识别系统缺陷来模仿受害者身份的非法认证危害\n3、针对医疗数据、人物图片数据的隐私窃取危害\n4、针对自动驾驶汽车、语音控制系统的恶意控制危害"}
{"content2":"AI（人工智能）为应用开发者开创了一个全新的可能性。通过利用机器学习或深度学习，您可以生成更好的用户配置文件、个性化设置和推荐，或者整合更智能的搜索、语音界面或智能助手，或者以其他数种方式改进您的应用。你甚至可以构建看得懂、听得懂，并与人类互动的应用。准备学习AI的你，知不知道选择哪种编程语言合适呢？以下列举的五种编程语言，被认为是最适合用来学习ＡＩ。大家可以参考一下。\n1. PYTHON\n第一名毫无疑问是 Python。尽管 Python 有些特性令人不爽（whitespace、Python 2.x 和 Python 3.x 之间的巨大差异、五种不同的包机制都在不同程度上有缺陷）但如果你正在从事 AI 工作，你几乎肯定会在某些时候用到 Python。\nPython 中可用库的数量是其他语言所无法企及的。NumPy 已经变得如此普遍，以至于几乎成为了张量运算的标准 API，Pandas 将 R 的强大而灵活的数据帧带入 Python。对于自然语言处理（NLP），您可以使用久负盛名的 NLTK 和快如闪电的 SpaCy。对于机器学习，有经过实战检验的 Scikit-learn。当谈到深度学习时，当前所有的库（TensorFlow，PyTorch，Chainer，Apache MXNet，Theano 等）都是在 Python 上首先实现的项目。\n(在LiveEdu上，一位德国的AI开发者教大家如何使用Python开发两个简单的机器学习模型）\n如果您正在阅读关于 arXiv 的顶尖深度学习研究，那么几乎可以肯定您会在 Python 中找到源代码。 此外，Python 生态系统中还有其他部分。虽然 IPython 已经改名为 Jupyter Notebook，看上去不再以 Python 为中心，但您仍然会发现绝大多数 Jupyter Notebook 用户以及大多数在线共享笔记本都使用 Python。\nPython 是人工智能研究的前沿语言，这是拥有最多机器学习和深度学习框架的语言，也是 AI 研究者几乎都掌握的语言。由于这些原因，尽管笔者每天都要咒骂一次 whitespace 问题，Python 仍然是人工智能编程语言之王，您没法绕过它。\n2. JAVA 和相关语言\nJVM 系列语言（Java，Scala，Kotlin，Clojure 等）也是 AI 应用开发的绝佳选择。无论是自然语言处理（CoreNLP）、张量运算（ND4J）还是完整的 GPU 加速深度学习堆栈（DL4J），您都可以使用大量的库来管理流水线的各个部分。另外，您还可以轻松访问 Apache Spark 和 Apache Hadoop 等大数据平台。\nJava 是大多数企业的通用语言，在 Java 8 和 Java 9 中提供了新的语言结构，这使得编写 Java 代码的体验不再像我们过去所记得的那样糟糕。使用 Java 编写人工智能应用可能会让人觉得无聊，但它确实能完成工作，并且您可以使用所有现成的 Java 基础架构来开发、部署和监视。\n3. C/C++\n在开发 AI 应用时，C / C ++ 不太可能成为您的首选，但如果您在嵌入式环境中工作，并且无法承受 Java 虚拟机或 Python 解释器的开销，那么 C / C ++ 就是最好的解决方案。当你需要榨干系统的每一滴性能时，你就得面对可怕的指针世界。\n幸运的是，现代 C / C ++ 写起来体验还不错（实话实说！）。您可以从下列方法中选择一个最适合的：您可以一头扎进堆栈底部，使用 CUDA 等库来编写自己的代码，这些代码将直接在 GPU 上运行；您也可以使用 TensorFlow 或 Caffe 以访问灵活的高级 API。后者还允许您导入数据科学家用 Python 写的模型，然后以 C / C ++ 级别的速度在生产环境中运行它们。\n在未来一年中，请密切留意 Rust 在 AI 领域的一些动作。结合 C / C ++ 级别的速度与类型和数据安全性，Rust 是实现产品级性能却不会造成安全问题的最佳选择。并且它现在已经可以与 TensorFlow 绑定了。\n4. JAVASCRIPT\n蛤？！JavaScript？我没听错吧？其实，谷歌最近发布了 TensorFlow.js，这是一个 WebGL 加速库，允许您在 Web 浏览器中训练和运行机器学习模型。它还包括 Keras API 以及加载和使用在常规 TensorFlow 中训练过的模型的功能。这可能会吸引大量的 JS 开发者涌入 AI 领域。虽然 JavaScript 目前能够访问的机器学习库与其他语言相比有所局限，但在不久的将来，开发者在网页中添加神经网络就和添加 React 组件或 CSS 属性一样简单。这听上去既强大又恐怖。\nTensorFlow.js 仍处于早期阶段。目前它可在浏览器中运行，但不适用于 Node.js。它还没有实现完整的 TensorFlow API。不过，我预计到 2018 年底，这两个问题都将基本得到解决，并且JavaScript 将在不久之后大举进军 AI 界。\n5. R\nR 在这份榜单中排名最末，并且看上去将会越来越没落。R 是数据科学家喜欢的语言。但是，其他程序员在第一次接触 R 时会感到有些困惑，因为它采用了以数据帧为中心的方法。如果您有一组专门的 R 开发者，那么将 R 与 TensorFlow、Keras 或 H2O 搭配使用，进行研究、原型设计和实验是有意义的。但基于性能和操作方面的考虑，我不愿意推荐将 R 用于生产。虽然您可以写出能在生产服务器上部署的高性能 R 代码，但将这种用 R 语言编写的原型重新编码为 Java 或 Python 肯定会更容易。\n本文为转载，原文出处：《AI 开发，究竟哪种语言强？》"}
{"content2":"一、人工智能与机器学习\n说到人工智能，就不得不提图灵测试。图灵测试是阿兰图灵在1950年提出的一个关于机器是否能够思考的著名实验，测试某机器是否能表现出与人等价或无法区分的智能。主要内容是：测试者与被测试者（一个人和一台机器）隔开的情况下，通过一些装置（如键盘）向被测试者随意提问。 进行多次测试后，如果测试者不能确定出被测试者是人还是机器，那么这台机器就通过了测试，并被认为具有人类智能。\n通常我们认为一个智能系统需要具有以下几个能力：\n语言能力--自然语言处理：能成功地用自然语言交流\n记忆能力--知识表示：存储它知道的或听到的信息\n推理能力--自动推理：运用存储的信息来回答问题并推出新结论\n学习能力--机器学习：适应新情况并检测和预测模式\n感知能力--计算机视觉：感知物体\n规划能力--自动规划：根据资源制定执行策略\n由此我们可以看出，机器学习是人工智能的一个分支，它是人工智能研究发展到一定阶段的必然产物。\n二、机器学习的发展\n二十世纪五十年代到七十年代初，人工智能研究处于”推理期“，人们认为只要能赋予机器逻辑推理能力，机器就具有智能。\n随着研究向前发展，人们逐渐认识到，仅仅具有逻辑推理能力是远远实现不了人工智能的。要使机器具有智能，必须设法使机器拥有知识。\n二十世纪七十年代中期开始，人工智能研究进入了“知识期”，但人们又认识到，由人来把知识总结出来再教给计算机是非常困难的。有人想到，如果机器能够自己学习知识就好了。\n二十世纪八十年代是机器学习成为一个独立的学科领域、各种机器学习技术百花初绽的时期。（实际上最早图灵在1950年图灵测试的文章中就曾提到机器学习的可能，五十年代到七十年代有一些机器学习的研究如基于神经网络的连接主义学习、感知机、基于逻辑表示的符号主义学习、以决策理论为基础的学习技术、强化学习等。但机器学习独立成为一个学科领域是在八十年代。）\n这时候，人们把机器学习划分为“机械学习”，“示教学习”，“类比学习”，“归纳学习”。\n机械学习\n死记硬背式学习，把外界输入的信息全部记录下来，需要时原封不动地取出来使用，实际上没有真正的学习，仅仅在进行信息存储与检索。\n示教学习\n从指令中学习。\n类比学习\n通过观察和发现学习。\n归纳学习\n从样例中学习，即从训练样例中归纳出学习结果。归纳学习是被研究最多，应用最广的，涵盖了监督学习、无监督学习等。\n二十世纪八十年代，归纳学习的主流是符号主义学习，其代表包括决策树学习和基于逻辑的学习。典型的决策树学习以信息论为基础，以信息熵的最小化为目标，直接模拟了人类对概念进行判定的树形流程。基于逻辑的学习的著名代表是归纳逻辑程序设计，可以看作机器学习与逻辑程序设计的交叉，使用一阶逻辑来进行知识表示，通过修改和扩充逻辑表达式来完成对数据的归纳。\n二十世纪九十年代，归纳学习的主流是基于神经网络的连接主义学习。\n二十世纪九十年代中期，归纳学习的主流是统计学习，其代表是支持向量机(SVM)以及更一般的“核方法(kernel methods)”。\n二十一世纪初，连接主义学习又卷土重来，掀起以“深度学习”为名的热潮。深度学习是指深层神经网络，它在语音、图像等复杂对象的应用中表现很好，性能优越。深度学习的流行一是因为大数据的发展，数据多了；二是因为计算机硬件的发展，计算能力强了。\n三、机器学习的分类\n机器学习中，有个定理叫“没有免费午餐“定理(No Free Lunch Theorem，简称NFL定理)。内容是无论学习算法a多聪明，学习算法b多笨拙，它们的期望性能是相同的。简而言之，就是没有一个通用算法可以完美解决所有问题，我们要根据具体问题来选择合适的算法。\n根据训练数据是否带有标签(label)信息，把训练数据中带有标签信息的学习算法称为监督学习，训练数据中不带标签信息的学习算法称为无监督学习。\n监督学习的代表是分类和回归，常见算法有线性回归、logistic回归、决策树、贝叶斯分类、支持向量机、神经网络等。\n无监督学习的代表是聚类，常见算法有主成分分析(PCA)、K均值聚类(K-Means)等。\n在后面的文章中会一一介绍这些算法和它们的代码实现，尽量给出不调包只用python代码实现和使用流行的机器学习框架实现两种实现方案。因为前者可以更好地理解算法，后者可以更快更方便地使用算法。 当然除了这些算法，还有数据预处理、模型的性能度量、超参数的调整等等。"}
{"content2":"自然语言处理 (NLP)问题都是序列化的。前馈神经网络，在单次前馈中对到来数据处理，假定所有输入独立，模式丢失。循环神经网络(recurrent neural network,RNN)对时间显式建模神经网络。RNN神经元可接收其他神经元加权输入。RNN神经元可与更高层建立连接，也可与更低层建立连接。隐含活性值在同一序列相邻输入间被记忆。2006年 LSTM。语音识别、语音合成、手写连体字识别、时间序列预测、图像标题生成、端到端机器翻译。\nRNN由神经元和连接权值构成任意有向图。输入神经元(input neuron)拥有“到来”连接，活性值由输入数据设置。输出神经元(output neuron)是数据流图一组可读取预测结果神经元。所有其他神经元为隐含神经元(hidden neuron)。每个时间步，通过设置输入神经元为网络提供输入序列下一帧。隐含活性值作为下一个时间步附加输入。RNN当前隐含活性值为状态。序列最开始设置值0空状态。RNN状态依赖当前输入和上一状态。状态与序列所有前输入都间接相关，工作记忆(woring memory)。RNN权值矩阵定义执行程序，决定隐含活性值输入，不同活性值整合新活性值输出。sigmoid激活函数 RNN 2006年被证明图录完备(Turing-complete)。给定正确权值，RNN可完成任意计算程序相同计算。不存在找到完美权值方法，可用梯度下降法得到次好结果。\n优化RNN，沿时间输展开，用优化前馈网络相同方式优化。复制序列神经元，连接在副本传递，移除循环边接而不改计算语义。相邻时间步权值相同强度。随时间反向传播(Back-Propagation Through Time,BPTT)，返回时间相关误差对权值(包括联结相邻副本权值)偏导。联结权值(tied weight)梯度相加。\n循环神经网络常见映射：序列分类、序列生成、序列标注、序列翻译。序列标注(sequential labelling)，序列作为输入，训练网络为每帧数据产生正确输出，一个序列到另一个序列等长映射。序列分类(sequential classification)，每个序列输入对应一个类别标签，可仅选择上一帧输出训练RNN，更新权值时误差流经所有时间步收集集成有用信息。序列生成(sequential generation)，给定一个类别标签，输出反馈给网络作为下一步输入，生成序列。单个向量视为信息稠密表示。序列翻译(sequential translation)，域中序列编码，最后隐含活性值解码为另一个域中序列。输入输出概念层次有差异，两个不同RNN，第一个模型最后活性值初始化第二个模型。单个网络，序列后传入特殊符号输入，通知网络停止编码，开始解码。\n带输出投影RNN网络结构，全连接隐含单元，映射输入输出。所有隐含单元都为输出，堆叠前馈层。隐含单元和输出单元不同激活函数。\nTensorFlow支持RNN各种变体，tf.nn.rnn_cell。tensor flow.models.rnn中tf.nn.dynamic_rnn实现RNN动力学。接收循环网络定义，输入序列批数据。所有序列等长。返回保存每个时间步输出和隐含状态两个张量。从tensor flow.models.rnn导入rnn_cell和rnn。输入数据维数为batch_size*sequence_length*frame_size。不希望限制批次大小，第1维尺寸可以设None。rnn_cell.BasicRNNCell 创建基础RNN。rnn.dynamic_rnn 定义sequence_length步模拟RNN运算。定义RNN，沿时间轴展开，加载数据，选择TensorFlow优化器训练网络，tf.train.RMSPropOptimizer、tf.train.AdamOptimizer。\n长时依赖性，网络记住含有许多后续不相关帧的序列第一帧。长序列，基础RNN展开网络深度非常大，层数非常多，每一层反向传播算法将来自网络上一层误差乘以局部偏导。如果大多数局部偏导远小于1,梯度每层变小，指数衰减，最终消失。如果很多偏导大于1,梯度值急剧增大。误差项包含相乘项权值矩阵转置。RNN相邻时间步联结一起，权值局部偏导都小于1或大于1,RNN每个权值都向相同方向缩放，梯度消失、爆炸问题突出。数值优化，浮点精度对梯度值产生影响。长短时记忆网络(long-short term memory,LSTM)RNN架构解决方案。\nLSTM专门解决梯度消失、爆炸问题。学习长时依赖关系的RNN事实标准。将RNN普通神经元替换为内部拥有少量记忆LSTM单元(LSTM Cell)。联结一起，内部状态记忆时间步误差。LSTM内部状态有固定权值为1自连接，线性激活函数，局部偏导始终为1。反向传播，常量误差传输子(constant error carousel)在时间步携带误差不发生梯度消失或爆炸。内部状态随时间步传递误差，LSTM环绕门(surrounding gates)负责学习，非线性激活函数(sigmoid)。原始LSTM单元，一种门学习对到来活性值缩放，另一种门学习输出活性值缩放，学习包含或忽略新输入，学习给其他单元传递特征。单元输入送入不同权值门。可以把循环神经网络用为规模更大网络架构组成部分。\nLSTMCell类可替换BasicRNNCell类，一个完整的LSTM层。输入非线性->输入门->状态->输出非线性->输出门。\nLSTM流行变种，添加对内部循环连接比例缩放遗忘门(forget gate)，网络学会遗忘，内部循环连接局部偏导变成遗忘门活性值，可取非1值。当上下文重要，遗忘门保持关闭状态。输入非线性->输入门->状态->遗忘门->输出非线性->输出门。\n添加窥视孔连接(peephole connection)，门能看到单元状态。当精确时间选择和间隔时有益。TensorFlow LSTM层传入use-peepholes=Trues标记激活窥视孔连接。\n门限循环单元(Gated Recurrent Unit,GRU)，架构简单，更少计算量，没有输出门，输入和遗忘门整合单独更新门(update gate)。更新门决定内部状态与候选活性值融合比例。重置门(reset gate)和新输入确定部分隐含状态计算得到候选活性值。TensorFlow GRU层对应GRUCell类。只需要单元数目参数。输入->候选活性值->更新门->重置门->状态。\n全连接隐含单元RNN，训练期间把不需要权值置0。最常见做法，两层或多层全连接RNN相互堆叠，信息只能在两层之间向上流动，多层RNN权值数目少，学习到更多抽象特征。\n参考资料：\n《面向机器智能的TensorFlow实践》\n欢迎加我微信交流：qingxingfengzi\n我的微信公众号:qingxingfengzigz\n我老婆张幸清的微信公众号：qingqingfeifangz"}
{"content2":"http://www.52nlp.cn/resources\n资源\n这里提供一些52nlp博客的一些系列文章以及收集的自然语言处理相关书籍及其他资源的下载，陆续整理中!如有不妥，我会做删除处理！\n特别推荐系列：\n1、HMM学习最佳范例全文文档，百度网盘链接: http://pan.baidu.com/s/1pJoMA2B 密码: f7az\n2、无约束最优化全文文档 -by @朱鉴 ，百度网盘链接：链接:http://pan.baidu.com/s/1hqEJtT6 密码: qng0\n3、PYTHON自然语言处理中文翻译-NLTK Natural Language Processing with Python 中文版，陈涛sean 无偿翻译。链接: http://pan.baidu.com/s/1i3DvwFV 密码: oxne\n4、正态分布的前世今生(pdf 版) – by @rickjin\n5、LDA-math-汇总 LDA数学八卦 – by @rickjin\n6、如何计算两个文档的相似度全文文档\n7、中文分词入门之字标注法全文文档\n一、书籍：\n1、《自然语言处理综论》英文版第二版\n2、《统计自然语言处理基础》英文版\n3、《用Python进行自然语言处理》，NLTK配套书\n4、《Learning Python第三版》，Python入门经典书籍，详细而不厌其烦\n5、《自然语言处理中的模式识别》\n6、《EM算法及其扩展》\n7、《统计学习基础》\n8、《自然语言理解》英文版（似乎只有前9章）\n9、《Fundamentals of Speech Recognition》，质量不太好，不过第6章关于HMM的部分比较详细，作者之一便是Lawrence Rabiner；\n10、概率统计经典入门书：《概率论及其应用》（英文版，威廉*费勒著）\n第一卷　　第二卷　　DjVuLibre阅读器（阅读前两卷书需要）\n11、一本利用Perl和Prolog进行自然语言处理的介绍书籍：《An Introduction to Language Processing with Perl and Prolog》\n12、国外机器学习书籍之：\n1) “Programming Collective Intelligence“，中文译名《集体智慧编程》，机器学习&数据挖掘领域”近年出的入门好书，培养兴趣是最重要的一环，一上来看大部头很容易被吓走的”\n2) “Machine Learning“,机器学习领域无可争议的经典书籍，下载完毕将后缀改为pdf即可。豆瓣评论 by 王宁）：老书，牛人。现在看来内容并不算深，很多章节有点到为止的感觉，但是很适合新手（当然，不能”新”到连算法和概率都不知道）入门。比如决策树部分就很精彩，并且这几年没有特别大的进展，所以并不过时。另外，这本书算是对97年前数十年机器学习工作的大综述，参考文献列表极有价值。国内有翻译和影印版，不知道绝版否。\n3) “Introduction to Machine Learning”\n13、国外数据挖掘书籍之：\n1) “Data.Mining.Concepts.and.Techniques.2nd“，数据挖掘经典书籍 作者 : Jiawei Han/Micheline Kamber 出版社 : Morgan Kaufmann 评语 : 华裔科学家写的书，相当深入浅出。\n2) Data Mining:Practical Machine Learning Tools and Techniques\n3) Beautiful Data: The Stories Behind Elegant Data Solutions（ Toby Segaran, Jeff Hammerbacher）\n14、国外模式识别书籍之：\n1）“Pattern Recognition”\n2）“Pattern Recongnition Technologies and Applications”\n3）“An Introduction to Pattern Recognition”\n4）“Introduction to Statistical Pattern Recognition”\n5）“Statistical Pattern Recognition 2nd Edition”\n6）“Supervised and Unsupervised Pattern Recognition”\n7）“Support Vector Machines for Pattern Classification”\n15、国外人工智能书籍之：\n1）Artificial Intelligence: A Modern Approach (2nd Edition) 人工智能领域无争议的经典。\n2）“Paradigms of Artificial Intelligence Programming: Case Studies in Common LISP”\n16、其他相关书籍：\n1）Programming the Semantic Web，Toby Segaran , Colin Evans, Jamie Taylor\n2）Learning.Python第四版，英文\n二、课件：\n1、哈工大刘挺老师的“统计自然语言处理”课件；\n2、哈工大刘秉权老师的“自然语言处理”课件；\n3、中科院计算所刘群老师的“计算语言学讲义“课件；\n4、中科院自动化所宗成庆老师的“自然语言理解”课件；\n5、北大常宝宝老师的“计算语言学”课件；\n6、北大詹卫东老师的“中文信息处理基础”的课件及相关代码；\n7、MIT Regina Barzilay教授的“自然语言处理”课件，52nlp上翻译了前5章；\n8、MIT大牛Michael Collins的“Machine Learning Approaches for Natural Language Processing(面向自然语言处理的机器学习方法)”课件；\n9、Michael Collins的“Machine Learning （机器学习）”课件；\n10、SMT牛人Philipp Koehn “Advanced Natural Language Processing（高级自然语言处理）”课件；\n11、Philipp Koehn “Empirical Methods in Natural Language Processing”课件；\n12、Philipp Koehn“Machine Translation（机器翻译）”课件；\n三、语言资源和开源工具：\n1、Brown语料库：\na) XML格式的brown语料库，带词性标注；\nb) 普通文本格式的brown语料库，带词性标注；\nc) 合并并去除空行、行首空格，用于词性标注训练：browntest.zip\n2、NLTK官方提供的语料库资源列表\n3、OpenNLP上的开源自然语言处理工具列表\n4、斯坦福大学自然语言处理组维护的“统计自然语言处理及基于语料库的计算语言学资源列表”\n5、LDC上免费的中文信息处理资源\n6、中文分词相关工具：\n1）Java版本的MMSEG：mmseg-v0.3.zip，作者为solol，详情可参见：《中文分词入门之篇外》\n2）张华平老师的ICTCLAS2010，该版本非商用免费一年，下载地址：\nhttp://cid-51de2738d3ea0fdd.skydrive.live.com/self.aspx/.Public/ICTCLAS2010-packet-release.rar\n7、热心读者“finallyliuyu”提供的一批新闻语料库，包括腾讯，新浪，网易，凤凰等，目前放在CSDN上：http://finallyliuyu.download.csdn.net/\n另外finalllyliuyu在2010年9月又提供了一批文本文类语料，详情见：献给热衷于自然语言处理的业余爱好者的中文新闻分类语料库之二\n四、文献：\n1、ACL-IJCNLP 2009论文全集：\na) 大会论文Full Paper第一卷\nb) 大会论文Full Paper第二卷\nc) 大会论文Short Paper合集\nd) ACL09之EMNLP-2009合集\ne) ACL09 所有workshop论文合集\n《资源》有 62 条评论\nPingback 引用通告： 推荐张华平老师的中文分词工具ICTCLAS2010 : 我爱自然语言处理"}
{"content2":"（一）语言分析的必要性：\n假如你的公司发布了一款全新的手机产品。 新产品的发布带来了来自不同媒体的相关报道、用户反馈。 面对这些数据，你可能希望了解\n大家关注的是这款手机的哪些特性\n大家对这款手机的评价如何\n有哪些用户表达了购买的意愿\n在面对海量数据的情况下，使用人力分析这些数据显然是不切实际的。 这种场景下，语言分析就派上了用场。\n让机器代替人来完成这些分析工作正是语言分析要做的工作。\n（二）语言分析常用操作：\n（1）分词\n中文分词 (Word Segmentation, WS) 指的是将汉字序列切分成词序列。 因为在汉语中，词是承载语义的最基本的单元。分词是信息检索、文本分类、情感分析等多项中文自然语言处理任务的基础。\n例如，句子\n国务院总理李克强调研上海外高桥时提出，支持上海积极探索新机制。\n正确分词的结果是\n国务院/ 总理/ 李克强/ 调研/ 上海/ 外高桥/ 时/ 提出/ ，/ 支持/ 上海/ 积极/ 探索/ 新/ 机制/ 。\n如果分词系统给出的切分结果是\n国务院/ 总理/ 李克/ 强调/ 研/ 上海 …\n因为强调也是一个常见的词，所以很可能出现这种分词结果。 那么，如果想要搜索和李克强相关的信息时，搜索引擎就很难检索到该文档了。\n切分歧义是分词任务中的主要难题。\n（2）词性标注\n词性标注(Part-of-speech Tagging, POS)是给句子中每个词一个词性类别的任务。 这里的词性类别可能是名词、动词、形容词或其他。 下面的句子是一个词性标注的例子。 其中，v代表动词、n代表名词、c代表连词、d代表副词、wp代表标点符号。\n词性标注不同的语料库采用的是不同的规范，此处以哈工大的语言云为例据理解释：\n国务院/ni 总理/n 李克强/nh 调研/v 上海/ns 外高桥/ns 时/n 提出/v ，/wp 支持/v 上海/ns 积极/a 探索/v 新/a 机制/n 。/wp\n词性标记集：LTP中采用863词性标注集，其各个词性含义如下表：\n（3）命名实体识别\n命名实体识别 (Named Entity Recognition, NER) 是在句子的词序列中定位并识别人名、地名、机构名等实体的任务。\n如之前的例子，命名实体识别的结果是：\n国务院 (机构名) 总理李克强 (人名) 调研上海外高桥 (地名) 时提出，支持上海 (地名) 积极探索新机制。\n命名实体识别对于挖掘文本中的实体进而对其进行分析有很重要的作用。\n命名实体识别的类型一般是根据任务确定的。LTP提供最基本的三种实体类型人名、地名、机构名的识别。\n用户可以很容易将实体类型拓展成品牌名、软件名等实体类型。\n（4）依存句法分析\n依存语法 (Dependency Parsing, DP) 通过分析语言单位内成分之间的依存关系揭示其句法结构。\n直观来讲，依存句法分析识别句子中的“主谓宾”、“定状补”这些语法成分，并分析各成分之间的关 系。仍然是上面的例子，其分析结果为：\n从分析结果中我们可以看到，句子的核心谓词为“提出”，主语是“李克强”，提出的宾语是“支持上海…”，“调研…时”是“提出”的 (时间) 状语，“李克强”的修饰语是“国务院总理”，“支持”的宾语是“探索 新机制”。有了上面的句法分析结果，我们就可以比较容易的看到，“提出者”是“李克强”，而不是“上海”或“外高桥”，即使它们都是名词，而且距离“提出”更近。\n依存句法分析标注关系 (共15种) 及含义如下：\n（5）语义角色标注\n语义角色标注 (Semantic Role Labeling, SRL) 是一种浅层的语义分析技术，标注句子中某些短语为给定谓词的论元 (语义角色) ，如施事、受事、时间和地点等。其能够对问答系统、信息抽取和机器翻译等应用产生推动作用。 仍然是上面的例子，语义角色标注的结果为：\n其中有三个谓词提出，调研和探索。以探索为例，积极是它的方式（一般用ADV表示），而新机制则是它的受事（一般用A1表示）\n核心的语义角色为 A0-5 六种，A0 通常表示动作的施事，A1通常表示动作的影响等，A2-5 根据谓语动词不同会有不同的语义含义。其余的15个语义角色为附加语义角色，如LOC 表示地点，TMP 表示时间等。附加语义角色列表如下：\n（6）语义依存分析\n语义依存分析 (Semantic Dependency Parsing, SDP)，分析句子各个语言单位之间的语义关联，并将语义关联以依存结构呈现。 使用语义依存刻画句子语义，好处在于不需要去抽象词汇本身，而是通过词汇所承受的语义框架来描述该词汇，而论元的数目相对词汇来说数量总是少了很多的。语义依存分析目标是跨越句子表层句法结构的束缚，直接获取深层的语义信息。 例如以下三个句子，用不同的表达方式表达了同一个语义信息，即张三实施了一个吃的动作，吃的动作是对苹果实施的。\n语义依存分析不受句法结构的影响，将具有直接语义关联的语言单元直接连接依存弧并标记上相应的语义关系。\n这也是语义依存分析与句法依存分析的重要区别。\n如上例对比了句法依存和语义分析的结果，可以看到两者存在两个显著差别。第一，句法依存某种程度上更重视非实词（如介词）在句子结构分析中的作用，而语义依存更倾向在具有直接语义关联的实词之间建立直接依存弧，非实词作为辅助标记存在。 第二，两者依存弧上标记的语义关系完全不同，语义依存关系是由论元关系引申归纳而来，可以用于回答问题，如我在哪里喝汤，我在用什么喝汤，谁在喝汤，我在喝什么。但是句法依存却没有这个能力。\n语义依存与语义角色标注之间也存在关联，语义角色标注只关注句子主要谓词的论元及谓词与论元之间的关系，而语义依存不仅关注谓词与论元的关系，还关注谓词与谓词之间、论元与论元之间、论元内部的语义关系。语义依存对句子语义信息的刻画更加完整全面。\n语义依存关系分为三类，分别是主要语义角色，每一种语义角色对应存在一个嵌套关系和反关系；事件关系，描述两个事件间的关系；语义依附标记，标记说话者语气等依附性信息。\n关系类型\nTag\nDescription\nExample\n施事关系\nAgt\nAgent\n我送她一束花 (我 <-- 送)\n当事关系\nExp\nExperiencer\n我跑得快 (跑 --> 我)\n感事关系\nAft\nAffection\n我思念家乡 (思念 --> 我)\n领事关系\nPoss\nPossessor\n他有一本好读 (他 <-- 有)\n受事关系\nPat\nPatient\n他打了小明 (打 --> 小明)\n客事关系\nCont\nContent\n他听到鞭炮声 (听 --> 鞭炮声)\n成事关系\nProd\nProduct\n他写了本小说 (写 --> 小说)\n源事关系\nOrig\nOrigin\n我军缴获敌人四辆坦克 (缴获 --> 坦克)\n涉事关系\nDatv\nDative\n他告诉我个秘密 ( 告诉 --> 我 )\n比较角色\nComp\nComitative\n他成绩比我好 (他 --> 我)\n属事角色\nBelg\nBelongings\n老赵有俩女儿 (老赵 <-- 有)\n类事角色\nClas\nClassification\n他是中学生 (是 --> 中学生)\n依据角色\nAccd\nAccording\n本庭依法宣判 (依法 <-- 宣判)\n缘故角色\nReas\nReason\n他在愁女儿婚事 (愁 --> 婚事)\n意图角色\nInt\nIntention\n为了金牌他拼命努力 (金牌 <-- 努力)\n结局角色\nCons\nConsequence\n他跑了满头大汗 (跑 --> 满头大汗)\n方式角色\nMann\nManner\n球慢慢滚进空门 (慢慢 <-- 滚)\n工具角色\nTool\nTool\n她用砂锅熬粥 (砂锅 <-- 熬粥)\n材料角色\nMalt\nMaterial\n她用小米熬粥 (小米 <-- 熬粥)\n时间角色\nTime\nTime\n唐朝有个李白 (唐朝 <-- 有)\n空间角色\nLoc\nLocation\n这房子朝南 (朝 --> 南)\n历程角色\nProc\nProcess\n火车正在过长江大桥 (过 --> 大桥)\n趋向角色\nDir\nDirection\n部队奔向南方 (奔 --> 南)\n范围角色\nSco\nScope\n产品应该比质量 (比 --> 质量)\n数量角色\nQuan\nQuantity\n一年有365天 (有 --> 天)\n数量数组\nQp\nQuantity-phrase\n三本书 (三 --> 本)\n频率角色\nFreq\nFrequency\n他每天看书 (每天 <-- 看)\n顺序角色\nSeq\nSequence\n他跑第一 (跑 --> 第一)\n描写角色\nDesc(Feat)\nDescription\n他长得胖 (长 --> 胖)\n宿主角色\nHost\nHost\n住房面积 (住房 <-- 面积)\n名字修饰角色\nNmod\nName-modifier\n果戈里大街 (果戈里 <-- 大街)\n时间修饰角色\nTmod\nTime-modifier\n星期一上午 (星期一 <-- 上午)\n反角色\nr + main role\n打篮球的小姑娘 (打篮球 <-- 姑娘)\n嵌套角色\nd + main role\n爷爷看见孙子在跑 (看见 --> 跑)\n并列关系\neCoo\nevent Coordination\n我喜欢唱歌和跳舞 (唱歌 --> 跳舞)\n选择关系\neSelt\nevent Selection\n您是喝茶还是喝咖啡 (茶 --> 咖啡)\n等同关系\neEqu\nevent Equivalent\n他们三个人一起走 (他们 --> 三个人)\n先行关系\nePrec\nevent Precedent\n首先，先\n顺承关系\neSucc\nevent Successor\n随后，然后\n递进关系\neProg\nevent Progression\n况且，并且\n转折关系\neAdvt\nevent adversative\n却，然而\n原因关系\neCau\nevent Cause\n因为，既然\n结果关系\neResu\nevent Result\n因此，以致\n推论关系\neInf\nevent Inference\n才，则\n条件关系\neCond\nevent Condition\n只要，除非\n假设关系\neSupp\nevent Supposition\n如果，要是\n让步关系\neConc\nevent Concession\n纵使，哪怕\n手段关系\neMetd\nevent Method\n目的关系\nePurp\nevent Purpose\n为了，以便\n割舍关系\neAban\nevent Abandonment\n与其，也不\n选取关系\nePref\nevent Preference\n不如，宁愿\n总括关系\neSum\nevent Summary\n总而言之\n分叙关系\neRect\nevent Recount\n例如，比方说\n连词标记\nmConj\nRecount Marker\n和，或\n的字标记\nmAux\nAuxiliary\n的，地，得\n介词标记\nmPrep\nPreposition\n把，被\n语气标记\nmTone\nTone\n吗，呢\n时间标记\nmTime\nTime\n才，曾经\n范围标记\nmRang\nRange\n都，到处\n程度标记\nmDegr\nDegree\n很，稍微\n频率标记\nmFreq\nFrequency Marker\n再，常常\n趋向标记\nmDir\nDirection Marker\n上去，下来\n插入语标记\nmPars\nParenthesis Marker\n总的来说，众所周知\n否定标记\nmNeg\nNegation Marker\n不，没，未\n情态标记\nmMod\nModal Marker\n幸亏，会，能\n标点标记\nmPunc\nPunctuation Marker\n，。！\n重复标记\nmPept\nRepetition Marker\n走啊走 (走 --> 走)\n多数标记\nmMaj\nMajority Marker\n们，等\n实词虚化标记\nmVain\nVain Marker\n离合标记\nmSepa\nSeperation Marker\n吃了个饭 (吃 --> 饭) 洗了个澡 (洗 --> 澡)\n根节点\nRoot\nRoot\n全句核心节点\n以上资料整理于哈工大的语言云\n20180503 于求是园"}
{"content2":"1.1 人工智能的概念定义\n1.1 人工智能的概念定义\n1.2 人工智能、机器学习和深度学习的关系\n1.3 人工智能的两大主要特征\n1.4 人工智能的发展简史\n1.5 人工智能的生态格局-市场篇\n1.6 人工智能产品经理的能力模型\n1.7 人工智能产品经理能力层次模型\n2.1 人工智能系统的技术架构\n2.2 人工智能系统的应用架构\n3.1 人工智能交互过程模型\n3.2.1 语音交互的技术框架\n3.2.2 语音交互的痛点\n3.2.3 语音交互的技术演进：语音识别模型\n3.2.3 语音交互的技术演进：语音识别错误率\n3.2.4 语音交互的开源框架\n3.3.1 自然语言处理的技术框架\n3.3.2 自然语言处理的痛点\n3.3.3 自然语言处理的技术演进\n3.3.4 自然语言处理：CHMM中文词法分析框架\n3.3.5 自然语言处理的开源框架\n3.4.1 机器问答的工作机制\n3.4.2 机器问答的人工智能标记语言（AIML）\n3.4.3 机器问答的开源框架\n4.1 人工智能发展的三个阶段：全景视图\n4.2 机器智能发展的四个阶段：技术视图\n4.3 人工智能发展的现状\n4.4 人工智能发展的瓶颈\n5.1 人工智能的战略框架\n5.2 基础：技术切入\n5.3 重点：数据融合\n5.4 核心：场景驱动\n5.5 关键：流量变现"}
{"content2":"前面了解过机器翻译的一些内容，对于文本的信息挖掘应该有了一定的了解，今天再来扫盲一下情感分析吧~\n更多内容参考：\n自然语言处理扫盲·第一天——自然语言处理的背景、应用、推荐资料\n自然语言处理扫盲·第二天——白话机器翻译原理\n引言\n情感分析在很多点上领域有很多的应用场景：\n比如，酒店网站需要提取用户对酒店的评价，然后策略性的进行显示，比如把负面的评价排的稍微往后面一点，总不能上来满屏都是脏乱差吧！\n比如，一些电商类的网站根据情感分析提取正负面的评价关键词，形成商品的标签。基于这些标签，用户可以快速知道大众对这个商品的看法\n比如，一些新闻类的网站，根据新闻的评论可以知道这个新闻的热点情况，是积极导向，还是消极导向，从而进行舆论新闻的有效控制。\n首先我们先来看看大厂们的效果：\n携程旅游\n这里把各种评价进行了归类，然后通过类别标签可以索引到目标评论。\n京东\n虽然有提取一些情感主题，但是不支持跳转\n天猫\n天猫做的就不错了，主题提取出来了，还支持跳转。\n有人对情感分析并不看好，首先是因为机器做情感分析毕竟没有一些主观的因素，难以还原用户当时的心情；其次是再好的情感分析也不如打星星来的精准，直接废话不多说，5颗星好评，1颗星差评...简单粗暴。\n不过我还是比较看好这个方向的，因为星星毕竟只是一个简单粗暴的情感分析，直接划分粒度太粗。\n所以回归正题，还是继续说说情感分析的一些实现方法吧！基本上情感分析有两种套路，一种是基于情感词的；另一种是基于机器学习的，我们下面就仔细的来看看每一种的实现方法。\n基于情感词典的情感分析\n这种分析方法简单粗暴，并不需要有太多复杂的知识，但是要求有尽量庞大完备的词库，而且这种词库必须是某一个领域背景下的。至于为什么不能通用稍后再说....\n首先需要这样几个词典：\n停顿词词典\n的 和 得 之间 ....\n正面评价词\n价格便宜 干净 美丽 物美价廉...\n负面评价词\n埋汰 脏 差 坏 ...\n程度词\n还行 0.8 非常好 3.0 凑合 0.5 一般 0.5 特别 2.0\n否定词\n不 难道 非 ...\n这些词典基本每个领域都不一样，比如声音大这个词，在音响的领域里面表示正面评价；但是在空调的领域里面就是负面评价了。因此每个领域最好有自己专业的词库，这个词库可以基于爬虫也可以基于人工搜集整理。网上有很多可以下载到的词库，不过都是比较通用的。\n然后就可以按照下面的步骤计算情感取向了:\n获取全部的用户评价内容\n先进行分词\n根据每个词计算总体的情感分值，公式如:-1^(否定词的个数)*程度词的分值*评价词的分值\n然后根据正负判断情感走向。\n比如，难道非得让我说差么？中，难道和非都算否定词，这样分值就是(-1)^2*1*-1 = -1，结论是负面评价\n再比如，难道这样不好吗?中，难道和不都是否定词，分值为(-1)^2*1*1=1，结论是正面评价\n虽然说有上面这些规则，在一些特定语境里面情感分析还是会出现误差。而且词语的位置也是一个很重要的因素，在词典这种机制里面，是忽略掉位置的。下面我们再看看基于机器学习的分析方法吧！\n基于机器学习的情感分析方法\n定义问题\n在情感分析中应用机器学习，首先第一步是定义问题，即先要判断情感分析是一个回归问题还是分类问题，还是聚类问题。由于用户基本上就是正面评价和负面评价，因此我们可以把它定义成二分类的问题。问题定义完，就可以考虑使用什么分类器的方法，比如逻辑回归、支持向量机、神经网络...都可以尝试。\n准备数据\n有机器学习背景的同学都应该知道，分类的问题属于有监督的学习问题，因此是需要提前准备一些标注数据的（标注的意思就是我们想要知道的结果）。比如现在有这样一波数:\n评价语1 正面评价 评价语2 负面评价 评价语3 正面评价 评价语4 正面评价\n其中评价属于最后我们想要的结果，即Label；评价语则是原始的数据，需要给变成可以计算的数值（方法有几种：词袋、TF-IDF、word2vec这个以后在详细说明，可以简单的理解为就是把一些评价文字，变成了 01010101的数值作为特征）\n然后我们就形成了这样的数据:\n(0 1 1 0 1),1 (1 1 0 0 1),0 (0 1 0 0 0),1 (0 1 1 1 0),1\n接下来就需要准备训练集和测试集，训练集用来训练模型；测试集用来测试模型是否正确。训练集在选取时，需要注意正负两个label的比例。试想一下，如果你的训练集里面90%都是负面评价，那么这个模型直接就写死只返回负面评价的结果，那么如果测试集也是同样的数据分布比例，那么正确率也会高达90%。这样显然是不合理的，因此要保证样本中正负评价数据的均衡。关于评测，手段有很多比如RMSE,MSE等等，有兴趣可以多了解下。\n训练模型\n然后就是利用各种算法训练模型，训练之后对比一下，选一个正确率最高的即可。最后把模型保存下面，之后可以直接使用。\n应用\n这里就直接应用模型，传入响应评价对应的特征参数即可。\n总结\n如果想要快速实现一个情感分析系统，最快的方式就是找到对应的情感词库，直接基于词典来做。如果考虑到未来的优化...可以再尝试使用机器学习的方式。\n参考\n基于情感词典的代码示例：http://www.aidnoob.com/ai/python/qinggan1/\nPython做文本情感分析之情感极性分析：http://www.jianshu.com/p/4cfcf1610a73"}
{"content2":"自动作文评分与自然语言处理\n前些天一个学弟发邮件咨询有关自动作文评分的问题，在了解了这是他们导师布置的一个任务后，出于做统计机器翻译的惯性思维，我马上想到的是利用语言模型对作文进行流利度方面的打分，但也意识到这是一个粗糙的甚至是错误的评分系统，因为它连最基本的作文长度都没有考虑。\n于是找了一些这方面的中英文材料看了一下，才发现自动作文评分系统在国外研究的很多很热甚至都已应用到真实的考试任务中去，而国内的研究寥寥，至少说明这个学弟选了一个很有应用前景和挑战性很强的方向。\n后来，我又与这个学弟在QQ上进一步做了交流，对于这个任务的界定清楚了一些。首先，他们将任务定为：4、6级考试的自动作文评分系统。有了明确的任务，就可以讨论一些具体的方法，这方面我也不懂，但是有一点基本达成了共识：自动作文评分可以纳入到文本分类方法的范畴中，所以学弟应该关注一下文本分类的方法学习；如果采用文本分类的方法做这套自动作文评分系统，首先要收集一套已经评过分的4、6级作文素材。至此，我能提供的建议就仅限于此了，如果哪位读者对这方面比较在行，不妨给这位学弟提点建议？这里先谢过了！\n关于自动作文评分，陈潇潇和葛诗利于2008年9月发表在《解放军外国语学院学报》的《自动作文评分研究综述》对于想初步了解自动作文评分的读者来说是一个不错的阅读素材，这篇文章对国外成熟的6大自动作文评分系统进行了不同程度的描述，而纵观这这些系统，无不与自然语言处理的相关技术紧密相连，以下是相关系统的一些简介：\n1、Project Essay Grade ( PEG)\nPEG是Ellis Page于1966年应美国大学委员会的请求而研发的, 其目的就是为了使大规模作文评分更加实际而高效。PEG完全依靠对文章的浅层语言学特征的分析对作文进行评分, 根本没有涉及内容。它使用代理量度标准(proxy measures) 来衡量作文的内在质量以模拟人对作文的评分。作文评分本应该直接针对作文的内在质量进行评判。但内在质量, 如写作的流畅性、句子结构的复杂度、文章措辞的情况等难以用计算机直接测量。于是PEG采取了间接测量写作构念分项指标的方法, 即所谓的代理量度标准。比如: 作文长度代表了写作的流畅性; 介词、关系代词等表明了句子结构的复杂度; 词长的变化表明了文章措辞的情况(因为非常用词一般都较长)。\nPEG由于其对语义方面的忽视和更多地注重表面结构而遭受指责。由于对作文内容相关方面的忽视, 该系统不能够给出对学生有指导意义的反馈。另外, 该系统最大的问题, 就是对写作技巧的间接测量很容易被写作者利用, 如写出文理不通的长文以获取流畅性方面的高分, 欺骗计算机。\n2、Intelligent Essay Assessor ( IEA)\nIEA是上世纪90 年代末由Pearson Knowledge Analysis Technology 公司在潜在语义分析( latent semantic analysis) 技术的基础上开发的。潜在语义分析本来是一个用于文本索引和信息提取的复杂统计技术, 其定义为“一个单词用法的统计模型, 该模型允许对片断文本包含的信息之间的语义相似性进行比较”。其核心思想就是一个段落的意义, 在很大程度上取决于该段落所包含的词汇的意义, 即使只改动一个单词, 也可能使这个段落的意义发生改变。该思想可以总结为“词汇1的意义+词汇2的意义+ ⋯⋯词汇n的意义=段落的意义”。另一方面, 两段由不同词汇构成的段落, 其意义也可能非常相似。通过大量文本的数学计算可以发现, 当某些不同的单词以较高的频率出现于相同或相似的语境时, 可以推算出这些词汇意义的相近。而由不相同但意义相近的单词构成的段落, 其意义也可能非常相似。\n在自动作文评分中, 该技术能够将学生的作文按照它所包含的单词投射成为能够代表作文意义(内容) 的数学形式, 然后在概念相关度和相关内容的含量两个方面与已知写作质量的参考文本进行比较, 从而得出学生作文的评分。\n3、Electronic Essay Rater (E-rater)\nE-rater是由Educational Testing Service ( ETS)的Burstein 等人在上世纪90 年代末开发的。目前ETS 正利用该系统对GMAT中Analytical Writing Assessment (AWA ) 部分进行评分, 并于2005年开始应用于托福考试的作文评分。在E-rater 付诸应用之前, GMAT 的AWA由两名评卷员在6分的范围内做出整体评分,如果两名评卷员的评分差异超过1分, 就需要第三名评卷员来处理。E-rater从1999 年2 月应用于AWA的评分。试卷的最终得分由E-rater和一名评卷员决定。同先前由两名评卷员共同阅卷的情况类似, 如果E2rater跟评卷员的评分差异超过1分, 第二名评卷员就参与解决这个问题。据Burstein 讲,自从E-rater应用于GMAT的AWA 的评分, E-rater与评卷员的分歧率一直低于3% , 这并不高于两名评卷员的分歧, 因此完全可以用于各种标准化考试的作文评分。E-rater系统采用基于微软自然语言处理的工具包来分析文章, 包括词性标注器为文本中每一个单词赋予词性; 句法分析器分析文本中的句法结构; 篇章分析器分析文本的篇章结构。采用词汇相似性度量器, 以统计技术中的简单关键词分析法分析文本中的词汇使用。另外, 采用了基于语料库的方法建模。使用统计与自然语言处理技术来提取待评分文章的语言学特征, 然后对照人工评分的标准作文集进行评分。评分过程主要由5个独立模块来进行。3个用来识别作为评分标准的特征, 包括: 句法模块、篇章模块和主题分析模块。这3个模块分别用来提取作文的句法多样性、思想的组织和词汇的使用方面的67个文本特征的特征值。第4个模块, 即模型构建模块, 用来选择和加权对作文评分具有预测力的特征。即把前3 个模块提取的数据作为自变量,人工评分的分数作为因变量进行逐步线性回归, 在67个变量中进行筛选, 建立回归方程。第5个模块用来计算待评分文章的最后得分, 即提取作文显著特征的特征值, 代入回归方程计算得分。\n4、IntelliMetricTM\nIntelliMetricTM是由Vantage Learning开发的, 第一套基于人工智能(AI) 的作文评分系统。它能够模仿人工评卷, 在1到4或者1到6的分值范围内对作文的内容、形式、组织和写作习惯进行评分。它集中了人工智能、自然语言处理和统计技术的长处, 是一种能够内化专家级评卷员集体智慧的学习机。其核心技术是Vantage Learning的CogniSearchTM和Quantum ReasoningTM 。前者是专门为IntelliMetricTM开发, 用来理解自然语言以支持作文的评分, 如它能分析词性和句法关系, 这使得IntelliMetricTM能够依据英语标准书面语的主要特征来评判作文。二者结合使得IntelliMetricTM能够内化作文中与某些特征相关的每一个得分点, 并用于接下来的作文自动评分。\nIntelliMetricTM需要采用专家级评卷员已经评好分数的作文集进行训练。在评分过程中, 系统采用了多个步骤。首先, 根据已评分数的训练集进行内化训练, 构建模型; 然后用较小的测试集检测模型的效度和概括度。两项都得到确认后, 便可用于待评分作文的评判了。一旦根据标准美式英语或者先前训练得到的标准, 某些作文被评估为不正常, 系统会自动做出标注。\nIntelliMetricTM评估了作文中语义、句法、篇章3个层次的300多项特征。在性能方面据称能够跟专家级评卷员给出的分数一样准确, 与评卷员的一致率达到了97%至99%。另外, IntelliMetricTM能够评阅多种语言的作文, 如英语、西班牙语、以色列语和印度尼西亚语。对荷兰语、法语、葡萄牙语、德语、意大利语、阿拉伯语以及日语等多种语言文本的评价现在也能够做到了。\n5、Bayesian Essay Test Scoring sYstem(BETSY)\nBETSY是由美国教育部投资, 由马里兰大学College Park的LawrenceM. Rudner开发的, 以概率论为指导, 基于训练语料对文本进行分类的程序(Valenti, et al. , 2003) 。该系统使用了包括内容与形式等多方面的一个大型特征集, 根据4点类型尺度(优、良、合格、不合格) 把一篇作文划分到一个最合适的集合中去。(Rudner & L iang, 2002) 文本分类所采用的底层模型是多元伯努利模型(MBM) 和伯努利模型(BM) , 两者都属于朴素贝叶斯模型,因为它们都以条件独立假设为前提。BETSY的计算量非常大, 但据其开发者声称, 由于该系统使用的方法能够整合PEG、LSA 和E2rater的最佳特征,“再加上本身所特有的长处, 使它具有以下特点:能够用于短文评测, 易于使用, 适用的内容范围宽广, 能够产生诊断性结果, 能够调节以用于多种技能的分类, 以及容易使非统计人员明白其中的道理”。值得一提的是,BETSY是作文自动评分领域唯一可免费下载使用的软件。\n6、Larkey的系统\n最早把文本统计分类方法用于作文自动评分的Larkey 以及Croft 在这个领域也做出了很大贡献。在他们的研究中, 采用了贝叶斯独立分类方法和最近邻分类方法( k-nearest-neighbor, 简称kNN) , 并提取11个文本复杂性特征用于线性的回归计算。在他们的实验中, 单独的贝叶斯独立分类方法有着稳定而良好的表现。然而, 加入文本复杂性特征和最近邻分类方法后, 系统性能并没有得到显著的改善。在这种评分方法中, 作文长度的重要性不像其他自动评分系统那样明显。"}
{"content2":"一文读懂AI简史：当年各国烧钱许下的愿，有些至今仍未实现\n导读：近日，马云、马化腾、李彦宏等互联网大佬纷纷亮相2018世界人工智能大会，并登台演讲。关于人工智能的现状与未来，他们提出了各自的观点，也引发网友热议。有人认为大佬们的分享干货满满，也有人有不同观点，认为我们并没有真正搞懂人工智能，更无法预测未来。\n如果回溯历史，你会发现，人工智能一直是国内外计算机、互联网大佬们喜欢的话题。他们的观点和预言，有些已成为今天的生活常态，有些却依然没有实现。本文就带你回顾人工智能发展过程中重要的历史阶段，以及大佬们那些精彩的言论。\n作者：钱纲\n本文摘编自《硅谷简史：通往人工智能之路》，如需转载请联系我们\n在我们回顾人工智能的历史之前，先来看看流行的人工智能的定义。目前，最被认可的人工智能定义为：能像人一样理性地思考和理性地行动的机器。行动被广义地理解为采取行动、制定行动的决策，并非肢体动作。\n人工智能分强弱两类。强人工智能为能推理（Reasoning）和解决问题（Problem solving）的智能机器，是有知觉、有自我意识的机器。强人工智能分两类：类人人工智能，即能像人一样思考和推理的机器；非类人人工智能，即具有和人不同的知觉和意识的机器，其推理方式和人类不同。持弱人工智能观点的人认为不可能制造出能真正地推理和解决问题的智能机器，那些看起来像智能的人工智能，既不是真正的智能，也不具有自主意识。\n人工智能的核心问题是使机器和软件具有与人类似的知识、学习、推理等能力。具体地说，就是让人工智能在一些以推理和分析为主的工作中达到或超过人类的水准和效率。目前，计算机硬件水平已经具有过去无法想象的能力了，为人工智能的实现铺平了道路。其实，在数学问题的证明上，在棋类竞赛上，甚至在股票投资上，人工智能都已经超过了人类。\n目前用来研究人工智能的主要手段及实现人工智能的机器是计算机, 人工智能的历史和计算机科学与技术史联系在一起。不过，人工智能还涉及信息论、控制论、自动化、仿生学、生物学、心理学、数理逻辑、语言学、医学和哲学等多门学科。\n01 早期的人工智能\n最早的人工智能是在先民的神话传说中。无论是古希腊还是我们中国古代都有把智能赋予机械装置的故事。1863年，塞缪尔·巴特勒（Samuel Butler）的论文《机器中的达尔文》探讨了机械装置器通过自然选择进化出智能的可能性。\n人工智能有一个基本假设，即人类可以用机械模拟人类的思考过程。这是一种形式化推理。在古代，亚里士多德的形式逻辑和欧几里得《几何原本》是形式化推理的典范。\n17世纪时，欧洲哲学家、数学家莱布尼茨和笛卡儿尝试过将思考形式转化为数学。莱布尼兹提出过一种用于推理的普适语言，它使推理成为计算，于是，哲学家之间的争论，就能用逻辑来判断其真伪。这些早期的哲学家已经知道，形式化推理依赖于形式语言系统。\n20世纪初，数理逻辑取得了长足的进步。希尔伯特提出了一个基础性问题：“能否将所有的数学推理形式化？”很快这个问题就被哥德尔的不完备定理解决了，他的答案是：任何形式语言系统都是不完备的。哥德尔也指出了：任何形式的数学推理都能在一些限制条件下简化成机械化步骤。\n1936年，24岁的英国数学家阿兰·图灵在他的论文中，提出了著名的图灵机模型，一个完整的形式语言系统。1945年，他发表了一系列论文论述了电子数字计算机的设计思想。\n随着计算机技术的提高，人们开始有了实现人工智能的技术手段。最早期的人工智能是用电子网络模拟人类的神经元，这一网络的激励电平只有“1”和“0”两种状态，没有中间状态。维纳的控制论很好地描述了电子网络的控制和稳定性。克劳德·香农提出的信息论则描述了如何用数字信号来实现逻辑功能。图灵的计算机理论证明二进制的数字信号足以描述任何形式的计算。这一切为人工智能打下了坚实的基础。\n最早提出神经元网络的学者是沃尔特·皮茨（Walter Pitts）和沃伦·麦卡洛克（Warren McCulloch），他们分析了理想的人工神经元网络，并给出了利用它们进行简单逻辑运算的机制。人工智能理论的奠基人之一的马文·明斯基（Marvin Minsky）当时只有24岁，是他们的学生。1951年明斯基与迪安·埃德蒙兹（Dean Edmonds）造出了第一台神经元网络机SNARC。\n1950年，图灵发表了一篇划时代论文《计算机器与智能》论文指出制造具有真正智能的机器是可能的。同时，图灵给出了智能的确切定义，即能够通过图灵测试的智能机器。图灵测试是这样的：如果一台机器与人类对话时，能不被辨别出其机器身份，那么该机器就具有智能。图灵的工作为人工智能奠定了坚实的基础。\n1951年，克里斯托夫·斯特雷奇（Christopher Strachey）写出了第一个跳棋（checkers）程序；很快就有人写出了国际象棋程序。20世纪50年代中期的国际象棋程序已经具有业余爱好者的水平了。游戏中的人工智能一直是评价人工智能进展的一类标准。\n20世纪50年代中期，科学家们开始用机器进行符号操作了。1955年，艾伦·纽厄尔（Allen Newell）和赫伯特·西蒙（Hervert Simon）开发了逻辑理论家（Logic Theorist）程序。该程序证明了《数学原理》中前52个定理中的38个，该程序的一些证明方法，比原著还好。\n1956年，第一次关于人工智能的学术会在达特茅斯学院召开，会议由明斯基、约翰·麦卡锡（John McCarthy）、香农等人发起，麦卡锡在会议上提出了人工智能一词。与会者有雷·所罗门诺夫（Ray Solomonoff）、奥利弗·塞尔弗里奇（Oliver Selfridge）、阿瑟·塞缪尔（Arthur Samuel）、纽厄尔和西蒙等人，他们后来在人工智能研究中做出了重要贡献。这次会议是人工智能被确立为一门学科的标志。\n人工智能之父和LISP语言发明人麦卡锡在达特茅斯会议上第一次为人工智能下了定义：“人工智能就是要让机器的行为看起来就像是人所表现出的智能行为一样。”麦卡锡的人工智能定义尽管很流行，但不全面。\n02 第一次高潮\n达特茅斯会议后的几年是人工智能飞速发展的时代。这一阶段，人们开发出了一些智能程序：解代数应用题，证明几何定理，学习和使用英语。当时，绝大多数人都无法相信机器能够解决这些智能问题。开发出这些程序的学者们相信，在未来的20年内，将出现具有完全智能的机器。国防部高级研究计划署为这些项目拨出了大笔科研经费。\n这些早期的人工智能程序中最有影响的是，搜索式推理、纽厄尔和西蒙的通用解题程序和赫伯特·格伦特尔（Herbert Gelernter）的几何定理证明程序等。\n人工智能的一个重要目标是使计算机能够通过自然语言和人类进行交流。早期的成功范例是丹尼尔·博布罗（Daniel Bobrow）的程序STUDENT，它还能解高中代数应用题。\n很快，有人就开发出了一个会说英语的聊天程序。与它聊天的用户有的会认为自己是在和人类在交谈。实际上该程序并不了解自己在说什么。它是按固定套路和语法在作答。\n1958年，纽厄尔和西蒙指出：“10年之内，数字计算机将成为国际象棋世界冠军。”“10年之内，数字计算机将发现并证明一个重要的数学定理。”\n1965年，西蒙称：“20年内，机器将能完成人能做到的一切工作。”\n当时的美国政府为人工智能提供的研究经费几乎是无条件的。麻省理工、卡内基梅隆大学、斯坦福大学和英国的爱丁堡大学是当时人工智能的研究中心。\n03 艰难的20世纪70年代\n20世纪70年代初，人工智能遭遇了瓶颈。当时最好的人工智能程序也只能解决一些最简单的问题，在很多人的眼里人工智能只是“玩具”而已。\n1976年，汉斯·莫拉维克（Hans Moravec）提出了著名的莫拉维克悖论：那些对于人类来说比较困难的问题，像证明定理这类问题对计算机程序而言相对容易；而一些对人类来说极其简单的任务，如人脸识别，却很难由计算机程序来实现。在莫拉维克悖论面前，当时的人工智能专家们一筹莫展。\n计算机的运算能力也是人工智能的瓶颈。当时的计算机内存和速度都不足以解决任何实际的人工智能问题。莫拉维克指出，计算机的能力离人工智能的要求还差上百万倍。\n1972年理查德·卡普（Richard Karp）证明了一个令人沮丧的结论，许多问题的计算时间与输入规模的幂成正比。除了最简单的情况，解决的时间接近无限长。也就是说人工智能恐怕永远也不会有实用价值。\n由于缺乏实质性进展，政府对人工智能的研究逐渐停止了资助。1974年，人工智能项目已经很难找到政府资助了。\n另外，来自其他领域的专家们也开始反对人工智能了。有哲学家称，哥德尔不完备定理证明了形式系统（例如计算机程序）无法判断某些陈述的真理性，而人类可以；还有些人认为人类推理实际上仅涉及少量的“符号处理”，而大多是具体的、直觉的、下意识的；还有人指出，程序并不理解它使用的符号，即意向性问题，如果符号对于机器没有意义，那机器就不是在思考。\n人工智能专家们不太把来自其他领域的批评当回事，但计算复杂性和“让程序具有常识”这些问题则是他们必须认真面对的。\n1976年约瑟夫·维森鲍姆（Joseph Weizeubaum）出版了专著《计算机的力量与人类的推理》（Computer Power and Human Reason），书中表示人工智能的滥用可能损害人类生命的价值。\n早在1958年，麦卡锡就提出了将逻辑学引入人工智能的构想。1963年，J.艾伦·鲁滨逊（J.Alan Robinson）发现了在计算机上实现推理的算法：归结（resolution）与合一（unification）算法。20世纪60年代末，麦卡锡发现用这一想法来直接实现逻辑推理的计算复杂度极高：即使证明很简单的定理也需要天文数字的步骤。20世纪70年代，罗伯特·科瓦尔斯基（Robert Kowalsky）、阿兰·科摩罗（Alain Colmerauer）和菲利普·鲁塞尔（Phillipe Roussel）在爱丁堡大学开发出了逻辑编程语言Prolog。\n04 繁荣的20世纪80年代\n20世纪80年代，人工智能中的专家系统程序开始被企业采纳。很快，知识处理就成了人工智能的主流。专家系统是一个能够依据一组从专门知识中推演出的逻辑规则在某一特定领域回答或解决问题的程序。最早的专家系统程序是由爱德华·费根鲍姆（Edward Feigenbaum）小组开发出来的。1965年的DENDRAL专家系统能根据分光计读数分辨混合物；1972年的MYCIN能够诊断血液传染病。\n专家系统是在很小的知识领域内的应用，避免了常识问题；因其简单又能容易地实现或修改，于是有了广泛的应用。但是，人们从专家系统中看到了这类程序的实用性。人工智能终于变得实用了。\n1980年，卡内基梅隆大学为数字设备公司设计了专家系统XCON，获得巨大成功。XCON每年为公司省下了4000万美元。于是，全世界的公司都开始研发和应用专家系统。到1985年，人工智能得到了各大企业超过10亿美元的投资。于是，为专家系统提供支持的产业应运而生，硬件公司有Symbolics、LISP Machines等，软件公司以IntelliCorp、Aion为主。\n专家系统的能力基于其存储的专业知识。20世纪70年代的经验告诉人们，智能行为与知识处理有着密切关系。知识库系统和知识工程是20世纪80年代人工智能研究的主要方向。\n1981年，日本经济产业省拨款8.5亿美元资助第五代计算机的研发。目标是造出能够与人对话，翻译语言，解释图像，并像人一样推理的计算机。他们选用Prolog作为该项目的主要编程语言。\n其他国家也纷纷响应。英国发起了耗资3.5亿英镑的Alvey工程。美国企业协会组织了微电子与计算机技术集团（Microelectronics and Computer Technology Corporation，MCC），向人工智能和信息技术的大规模项目提供资助。DARPA组织了战略计算促进会（Strategic Computing Initiative），开始向人工智能大量投资。\n1982年，物理学家霍普菲尔德（Hopfield）证明了一种新型的神经网络能用一种全新的方式学习和处理信息。同时，罗姆尔海特（Rumelhart）推广了一种神经网络训练方法。这些发现使神经网络在20世纪90年代获得了商业上的成功，它们被广泛地应用于光字符识别和语音识别软件。\n05 再次跌入低谷\n1987年，人工智能硬件市场需求突然下跌，而个人计算机的性能不断提升，其性能已经超过了Symbolics和其他厂家生产的昂贵的LISP机。人工智能硬件厂商失去了存在的理由，一个价值5亿美元的产业顷刻间土崩瓦解。\n一些曾经大获成功的专家系统的维护费居高不下。它们难以升级，难以使用，成了以前已经暴露的各种各样的问题的牺牲品。专家系统的实用性仅仅局限于某些特定情景。\n20世纪80年代晚期，战略计算促进会大幅削减对人工智能的资助。DARPA的新领导认为人工智能不是“下一个浪潮”，拨款将倾向于一些看起来容易出成果的项目。\n“第五代计算机工程”一直到1991年也没有实现。其中一些目标，比如“与人类展开交谈”，直到2010年也没有实现。\n06 再度繁荣\n20世纪80年代后期，一些学者提出了一种全新的人工智能方案。他们认为，为了获得真正的智能，机器必须能够感知、移动、生存，并与这个世界交互。他们认为这些感知运动技能对于常识等高层次技能至关重要，相反抽象推理则是人类最不重要、最无趣的技能。他们提倡“自底向上”地创造智能。\n20世纪90年代，人工智能终于实现了它最初的一些目标。它被成功地用在许多技术产业中。这些成就主要归功于计算机性能的提升。\n1997年5月11日，深蓝战胜了国际象棋世界冠军卡斯帕罗夫（Kasparov）。\n2005年，斯坦福大学开发的一台机器人在一条沙漠小径上成功地自动行驶了131英里，赢得了DARPA挑战大赛头奖。\n2009年，蓝脑计划成功地模拟了部分鼠脑。\n2011年，IBM的沃森参加《危险边缘》节目，在最后一集中打败了人类选手。\n但是，这些成就不是因为范式上的革命。它们只是工程技术的复杂应用。人们广泛地认识到，许多人工智能需要解决的问题已成为数学和运筹学领域的课题了。数学语言的共享使人工智能与其他学科在更高层次上进行了合作，其研究结果则更易于评估和证明。今天，人工智能已成为一门非常严格的科学分支。\n今天，计算机的计算能力已经提高到了前所未有的地步。在理论上，人工智能发展几乎是没有限制的，因此它必将像今天的互联网一样深远地影响我们的日常生活和价值观。\n关于作者：钱纲，现就职于美国德州仪器公司，从事半导体工艺及半导体器件的开发与研究工作。科学网人气作者，其作品在线获得超过千万人次的浏览量。钱纲的作品以涉及历史、科技的杂文、随笔为主。主要作品有美国历史及人物纪事《美国往事》，硅谷历史《硅谷简史》等。\n本文摘编自《硅谷简史：通往人工智能之路》，经出版方授权发布。\nhttps://mp.weixin.qq.com/s/PnhwSKV7_6C2xPjwIt6cKQ"}
{"content2":"《立委随笔：机器学习和自然语言处理》\n作者：李维\n有脚客介绍人工智能（AI）现状 ( http://rl.rockiestech.com/node/636 )，认为由于机器学习（ML）技术的长足进步，人工智能正进入繁荣期，并且开始成功用于自然语言处理（NLP). 除了调子过分乐观了一些，这是个不错的介绍。下面的随笔是根据我自己的经验和体会而来。\nAI, ML and NLP\nNLP 中过分强调 AI 曾经是斜途，其实现在我认为也还是斜途, 我很久以前就有过这个看法，现在觉得并没过时：\n机器翻译的另一极是建立在充分理解基础上, 毋须转换的自动翻译, 这是从实质上对人的翻译过程的模拟。这时候, 源语分析才是真正的自然语言理解, 机器翻译才真正属于人工智能。然而, 这里遇到两个难题: 一是知识处理问题; 二是所谓元语言问题。\n考察人的翻译活动, 可以发现, 人是靠丰富的知识在理解的基础上从事翻译的。这些知识既包括语言知识, 也包括世界知识(常识、专业知识等)。如何组织这些包罗万象的百科全书一样的知识, 以便适应机器处理和运用的需要, 是人工智能所面临的根本性课题。\n……\n总之, 虽然机器翻译的最终出路在于人工智能的理论和技术的突破, 但在条件不成熟的时候过份强调机器翻译的人工智能性质, 一味追求基于知识和理解的自动翻译,对于应用型机器翻译系统的研制, 往往没有益处。\n摘自【立委科普：机器翻译】: http://www.starlakeporch.net/bbs/read.php?45,18361\nAI 里面调子最高的一派是 Doug Lenat，他的 cyc 项目进行了多年，获得了政府和许多 high profile sponsors 的多年资助，一直无法实用，尽管他自己10年前就宣扬已经接近应用前夜了。对于 Doug Lenat，我打心底钦佩，这种基于常识推理的 AI 需要苦功夫，是对人的智能（一个侧面）的逼真模拟。\n多数学者对此不以为然，对这种 “纯粹AI” 不看好，大家大都转向以统计为基础的机器学习（ML）。基本上是把人的智能看成黑箱，不再试图从本质上模拟人脑的过程，包括逻辑推理，而是把每一个具体的智能活动定义为一个任务，一个从输入转换成所求的输出的任务，而这是可以客观度量的。只要机器能够训练成尽可能逼近所需的输出，人的智能就局部实现了。\nML 和 NLP\n如今，NLP（包括机器翻译MT）也基本上已经被搞机器学习的人统治了，传统的规则方法只能打边鼓。他们也确实弄出一些名堂来，尤其是语音处理，分类(classification)，和知识习得(knowledge acquisition) 方面。\n目前的情况是，有指导的学习（supervised learning） 比较成熟，但遭遇知识瓶颈，就是需要大数据量的 labeled data 的问题。如果问题单纯，features 选取容易，又有海量数据，学习的结果真地可以很接近人工水平。我们曾经做过一项研究（碰巧的是，IBM 也大体同时做了这项研究，不如我们深入，但大同小异，结果也类似），找到了一个很好的应用领域做大小写恢复工作（Case Restoration），效果奇好。过去很多档案文字的电子版本是全大写的，网络上现在还有很多文件也是不分大小写的（譬如很多语音识别出来的材料，标题，还有论坛和电子邮件的非正式文字，等等），这就给自然语言处理和信息抽取造成困难，因为多数语言处理系统 assume 的 input 是正常大小写夹杂的文字，一旦输入文件没有大小写的区别，一切就乱套了。连最基础的词类区分（POS: Part-of-Speech tagging）和专名识别（NE: named entity tagging）都寸步难行（因为最重要的一个识别专名边界的clue就是大写）。为了解决这个问题，以前的研究者就设计两套系统，比如BBN就把大小写的features统统弃置重新训练一套NE系统来对付没有大小写的input, 除了 overhead, 系统性能也下降很多。我们想，如果我们先把大小写恢复，然后再做 NLP 不就成了。这个恢复大小写的任务相对比较单纯，训练文本几乎是无限的，因为网上文字大多是区分大小写的。我们利用这些现成的 “labeled” data, 用最简单的HMM算法，学出了一个高效能的系统，解决了这个问题，结果超出预料地好。（Niu, C., W. Li, J. Ding, and R. Rohini. 2004. Orthographic Case Restoration Using Supervised Learning Without Manual Annotation. International Journal of Artificial Intelligence Tools, Vol. 13, No. 1, 2004.）\n不过，这样讨巧的事并不多 (一个类似可以讨巧的是某些classification的任务：比如想训练一个给评语分类的系统，就可以上网找到很多客户回馈的记录，这些记录除了文字外，常常还有星号标识，以1个星号表示很差，5星表示很好)。多数任务会遇到 lebeling data 的瓶颈。统计界的共识之一就是，data, data and data. 很多时候，算法的优劣是其次的，主要还是要足够多的 data 和合适的 feature design. 数据量大了，学习的效果自然就好了。所以，labeled data 是 supervised learning 的真正知识瓶颈。我就见过这样的系统，本来是指望随时重新训练以适应新情况的，结果 data 跟不上，成了一个只训练一次的死系统，任何后续的改进都不是经过增加数据重新训练，而是在系统外部打各种补丁。机器学习的优势就失去了。\n无须指导的学习（Unsupervised learning） 因此引起学者的兴趣，成为热点，因为所需的训练材料无须标注。在网络世界，有的是 raw data. 对某个对象进行 clustering 就可以用 unsupervised leaning, 出了很多有意思的结果。Clustering 有别于 classification, 前者没有预定一个目标，而是根据features，只要长得象的就归在一起，后者是有预定的 tag set 作为分类的目标。只要设计者心中有个大致的目标，features 选取得当，可以控制 clustering 的结果的粗细，然后去现实世界或使用者中印证clustering的合理性和含义。反正是 unsupervised learning, 不妨多来几次，选取最好的结果作为方向，这样就可以把 clustering 转化成具有广泛应用的 classification. （在人类智能活动中，分类是最常用的技能，也是应用最广泛，相对单纯，比较易于机器学习和模拟成功的任务。大千世界，林林总总，为了把握它，人类第一个要做的就是分类。分类以后，才好缩小范围，集中到某个子领域，钻进去仔细分析。）\n正如自如所述，目前很多研究者对所谓 weakly supervised learning 情有独衷，觉得这是一个具有突破性的研究方向。传统的 supervised learning 有知识瓶颈而为人诟病，完全没有指导的学习效率不高，因此尝试利用有限 labeled data 作为种子（seeds）, 怎样引导学习程序一步一步向指定方向去，这是一个充满魅力的路子。这方面的成果令人鼓舞，但总体还在探索阶段，只有少部分课题已经接近临床实用，譬如分类和词典习得（lexicon acqusition）.\n机器学习的缺点和局限等有时间再接着谈。先说一点，任务一复杂，ML 就麻烦。遇到复杂的难以分解的任务，基本是没戏，譬如 自然语言的深度结构分析（deep parsing）。而任务相对单纯的浅层分析（shallow parsing），ML 的效果就很好，可以媲美人工系统。\n来源： <http://blog.sciencenet.cn/blog-362400-294037.html>\n来自为知笔记(Wiz)"}
{"content2":"近年来，深度学习的研究越来越深入，在各个领域也都获得了不少突破性的进展。基于注意力（attention）机制的神经网络成为了最近神经网络研究的一个热点，下面是一些基于attention机制的神经网络在自然语言处理（NLP）领域的论文，现在来对attention在NLP中的应用进行一个总结，和大家一起分享。\n1 Attention研究进展\nAttention机制最早是在视觉图像领域提出来的，应该是在九几年思想就提出来了，但是真正火起来应该算是google mind团队的这篇论文《Recurrent Models of Visual Attention》[14]，他们在RNN模型上使用了attention机制来进行图像分类。随后，Bahdanau等人在论文《Neural Machine Translation by Jointly Learning to Align and Translate》 [1]中，使用类似attention的机制在机器翻译任务上将翻译和对齐同时进行，他们的工作算是是第一个提出attention机制应用到NLP领域中。接着类似的基于attention机制的RNN模型扩展开始应用到各种NLP任务中。最近，如何在CNN中使用attention机制也成为了大家的研究热点。下图表示了attention研究进展的大概趋势。\n2 Recurrent Models of Visual Attention\n在介绍NLP中的Attention之前，我想大致说一下图像中使用attention的思想。就具代表性的这篇论文《Recurrent Models of Visual Attention》 [14]，他们研究的动机其实也是受到人类注意力机制的启发。人们在进行观察图像的时候，其实并不是一次就把整幅图像的每个位置像素都看过，大多是根据需求将注意力集中到图像的特定部分。而且人类会根据之前观察的图像学习到未来要观察图像注意力应该集中的位置。下图是这篇论文的核心模型示意图。\n该模型是在传统的RNN上加入了attention机制（即红圈圈出来的部分），通过attention去学习一幅图像要处理的部分，每次当前状态，都会根据前一个状态学习得到的要关注的位置l和当前输入的图像，去处理注意力部分像素，而不是图像的全部像素。这样的好处就是更少的像素需要处理，减少了任务的复杂度。可以看到图像中应用attention和人类的注意力机制是很类似的，接下来我们看看在NLP中使用的attention。\n3 Attention-based RNN in NLP\n3.1 Neural Machine Translation by Jointly Learning to Align and Translate [1]\n这篇论文算是在NLP中第一个使用attention机制的工作。他们把attention机制用到了神经网络机器翻译（NMT）上，NMT其实就是一个典型的sequence to sequence模型，也就是一个encoder to decoder模型，传统的NMT使用两个RNN，一个RNN对源语言进行编码，将源语言编码到一个固定维度的中间向量，然后在使用一个RNN进行解码翻译到目标语言，传统的模型如下图：\n这篇论文提出了基于attention机制的NMT，模型大致如下图：\n图中我并没有把解码器中的所有连线画玩，只画了前两个词，后面的词其实都一样。可以看到基于attention的NMT在传统的基础上，它把源语言端的每个词学到的表达（传统的只有最后一个词后学到的表达）和当前要预测翻译的词联系了起来，这样的联系就是通过他们设计的attention进行的，在模型训练好后，根据attention矩阵，我们就可以得到源语言和目标语言的对齐矩阵了。具体论文的attention设计部分如下：\n可以看到他们是使用一个感知机公式来将目标语言和源语言的每个词联系了起来，然后通过soft函数将其归一化得到一个概率分布，就是attention矩阵。\n从结果来看相比传统的NMT（RNNsearch是attention NMT，RNNenc是传统NMT）效果提升了不少，最大的特点还在于它可以可视化对齐，并且在长句的处理上更有优势。\n3.2 Effective Approaches to Attention-based Neural Machine Translation [2]\n这篇论文是继上一篇论文后，一篇很具代表性的论文，他们的工作告诉了大家attention在RNN中可以如何进行扩展，这篇论文对后续各种基于attention的模型在NLP应用起到了很大的促进作用。在论文中他们提出了两种attention机制，一种是全局（global）机制，一种是局部（local）机制。\n首先我们来看看global机制的attention，其实这和上一篇论文提出的attention的思路是一样的，它都是对源语言对所有词进行处理，不同的是在计算attention矩阵值的时候，他提出了几种简单的扩展版本。\n在他们最后的实验中general的计算方法效果是最好的。\n我们再来看一下他们提出的local版本。主要思路是为了减少attention计算时的耗费，作者在计算attention时并不是去考虑源语言端的所有词，而是根据一个预测函数，先预测当前解码时要对齐的源语言端的位置Pt，然后通过上下文窗口，仅考虑窗口内的词。\n里面给出了两种预测方法，local-m和local-p，再计算最后的attention矩阵时，在原来的基础上去乘了一个pt位置相关的高斯分布。作者的实验结果是局部的比全局的attention效果好。\n这篇论文最大的贡献我觉得是首先告诉了我们可以如何扩展attention的计算方式，还有就是局部的attention方法。\n4 Attention-based CNN in NLP\n随后基于Attention的RNN模型开始在NLP中广泛应用，不仅仅是序列到序列模型，各种分类问题都可以使用这样的模型。那么在深度学习中与RNN同样流行的卷积神经网络CNN是否也可以使用attention机制呢？《ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs》 [13]这篇论文就提出了3中在CNN中使用attention的方法，是attention在CNN中较早的探索性工作。\n传统的CNN在构建句对模型时如上图，通过每个单通道处理一个句子，然后学习句子表达，最后一起输入到分类器中。这样的模型在输入分类器前句对间是没有相互联系的，作者们就想通过设计attention机制将不同cnn通道的句对联系起来。\n第一种方法ABCNN0-1是在卷积前进行attention，通过attention矩阵计算出相应句对的attention feature map，然后连同原来的feature map一起输入到卷积层。具体的计算方法如下。\n第二种方法ABCNN-2是在池化时进行attention，通过attention对卷积后的表达重新加权，然后再进行池化，原理如下图。\n第三种就是把前两种方法一起用到CNN中，如下图\n这篇论文提供了我们在CNN中使用attention的思路。现在也有不少使用基于attention的CNN工作，并取得了不错的效果。\n5 总结\n最后进行一下总结。Attention在NLP中其实我觉得可以看成是一种自动加权，它可以把两个你想要联系起来的不同模块，通过加权的形式进行联系。目前主流的计算公式有以下几种：\n通过设计一个函数将目标模块mt和源模块ms联系起来，然后通过一个soft函数将其归一化得到概率分布。\n目前Attention在NLP中已经有广泛的应用。它有一个很大的优点就是可以可视化attention矩阵来告诉大家神经网络在进行任务时关注了哪些部分。\n不过在NLP中的attention机制和人类的attention机制还是有所区别，它基本还是需要计算所有要处理的对象，并额外用一个矩阵去存储其权重，其实增加了开销。而不是像人类一样可以忽略不想关注的部分，只去处理关注的部分。\n参考文献\n[1] Bahdanau, D., Cho, K. & Bengio, Y. Neural Machine Translation by Jointly Learning to Align and Translate. Iclr 2015 1–15 (2014).\n[2] Luong, M. & Manning, C. D. Effective Approaches to Attention-based Neural Machine Translation. 1412–1421 (2015).\n[3] Rush, A. M. & Weston, J. A Neural Attention Model for Abstractive Sentence Summarization. EMNLP (2015).\n[4] Allamanis, M., Peng, H. & Sutton, C. A Convolutional Attention Network for Extreme Summarization of Source Code. Arxiv (2016).\n[5] Hermann, K. M. et al. Teaching Machines to Read and Comprehend. arXiv 1–13 (2015).\n[6] Yin, W., Ebert, S. & Schütze, H. Attention-Based Convolutional Neural Network for Machine Comprehension. 7 (2016).\n[7] Kadlec, R., Schmid, M., Bajgar, O. & Kleindienst, J. Text Understanding with the Attention Sum Reader Network. arXiv:1603.01547v1 [cs.CL] (2016).\n[8] Dhingra, B., Liu, H., Cohen, W. W. & Salakhutdinov, R. Gated-Attention Readers for Text Comprehension. (2016).\n[9] Vinyals, O. et al. Grammar as a Foreign Language. arXiv 1–10 (2015).\n[10] Wang, L., Cao, Z., De Melo, G. & Liu, Z. Relation Classification via Multi-Level Attention CNNs. Acl 1298–1307 (2016).\n[11] Zhou, P. et al. Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification. Proc. 54th Annu. Meet. Assoc. Comput. Linguist. (Volume 2 Short Pap. 207–212 (2016).\n[12] Yang, Z. et al. Hierarchical Attention Networks for Document Classification. Naacl (2016).\n[13] Yin W, Schütze H, Xiang B, et al. Abcnn: Attention-based convolutional neural network for modeling sentence pairs. arXiv preprint arXiv:1512.05193, 2015.\n[14] Mnih V, Heess N, Graves A. Recurrent models of visual attention[C]//Advances in Neural Information Processing Systems. 2014: 2204-2212."}
{"content2":"本文扩展了Herman Kamper和我在2018年深度学习Indaba组织的自然语言处理前沿课程。整个课程的幻灯片都可以在这里找到，这篇文章将主要讨论NLP中基于神经网络方法的近期进展。\n免责声明：本文尝试将大约15年NLP的发展历程浓缩为今天最相关的八个里程碑，因此遗漏了许多相关和重要的发展。特别是，它严重偏向于当前的神经方法，这可能给人留下此期间没有其他有影响力方法的错误影响。\n2001年-神经语言模型\n语言建模是在给定前面的单词的情况下预测文本中的下一个单词的任务。 它可能是最简单的语言处理任务，具有实际应用，如智能键盘和电子邮件响应建议（Kannan et al.,2016）。语言建模有着丰富的历史。基于n-gram的经典方法采用平滑处理看不见的n-gram（Kneser＆Ney,1995）。Bengio等人于2001年提出了第一种神经语言模型，一种前馈神经网络，如下所示。\n该模型把n个可以在表C中查找的先前单词向量表示作为输入。现在，这种向量被称为词嵌入。这些词嵌入被连接并送入隐藏层，然后将其输出提供给softmax层。想要了解更多该模型的信息，请查看此文章。\n最近，前馈神经网络已被用于语言建模的递归神经网络（RNN; Mikolov等人，2010）和长短期记忆网络（LSTM; Graves，2013）所取代。近年来已经提出了许多经典LSTM的新语言扩展模型（请参阅此页面以获得概述）。尽管有这些发展，但经典的LSTM仍然是一个强大的基线（Melis等，2018）。即使Bengio等人的经典前馈神经网络在某些环境中也与更复杂的模型竞争，但这些通常只学会考虑最近的词（Daniluk等，2017）。如何理解这些语言模型捕获的信息是一个活跃的研究领域（Kuncoro等，2018; Blevins等，2018）。\n语言建模通常是应用RNN时的首选训练场，并成功捕捉到了想象力，许多人通过Andrej的博客文章开始了解。语言建模是无监督学习的一种形式，Yann LeCun也将预测性学习作为获取常识的先决条件（参见NIPS 2016的Cake幻灯片）。 关于语言建模最显著的方面可能是，尽管它很简单，但它是本文讨论的许多后期进展的核心：\n词嵌入：word2vec的目标是简化语言建模；\n序列到序列模型：这种模型通过一次预测一个词来生成输出序列；\n预训练语言模型：这些方法使用语言模型中的表示来进行迁移学习；\n这反过来意味着NLP中许多最重要的最新进展减少为一种语言建模形式。 为了做“真正的”自然语言理解，仅仅从原始形式的文本中学习可能是不够的，我们将需要新的方法和模型。\n2008-多任务学习\n多任务学习是在多个任务上训练的模型之间共享参数的一般方法。在神经网络中，这可以通过绑定不同层的权重来轻松实现。多任务学习的想法在1993年由Rich Caruana首次提出，并应用于道路跟踪和肺炎预测（Caruana，1998）。直观地说，多任务学习鼓励模型学习对许多任务有用的表示。特别对于学习一般的低级表示，集中模型的注意力或在有限量的训练数据的设置中特别有用。有关多任务学习的更全面概述，请查看此文章。\nCollobert和Weston于2008年首次将多任务学习应用于NLP的神经网络。 在他们的模型中，查找表（或词嵌入矩阵）在两个在不同任务上训练的模型之间共享，如下面的所示。\n共享词嵌入使模型能够在词嵌入矩阵中协作和共享一般的低级信息，这通常构成模型中最大数量的参数。Collobert和Weston在2008年的论文中证明了它在多任务学习中的应用，它引领了诸如预训练词嵌入和使用卷积神经网络（CNN）之类的思想，这些思想仅在过去几年中被广泛采用。它赢得了ICML 2018的时间考验奖（参见此时的时间考验奖论文）。\n多任务学习现在用于各种NLP任务，并且利用现有或“人工”任务已成为NLP指令集中的有用工具。有关不同附加任务的概述，请查看此文章。虽然通常预先定义参数的共享，但是在优化过程期间也可以学习不同的共享模式（Ruder等，2017）。随着模型越来越多地在多项任务中被评估来评估其泛化能力，多任务学习越来越重要，最近提出了多任务学习的专用基准（Wang et al，2018; McCann et al，2018）。\n2013-词嵌入\n文本的稀疏向量表示，即所谓的词袋模型，在NLP中具有悠久的历史。正如我们在上面所看到的，早在2001年就已经使用了词或词嵌入的密集向量表示。 Mikolov等人在2013年提出的主要创新，是通过移动隐藏层和近似目标来使这些词嵌入的训练更有效率。虽然这些变化本质上很简单，但它们与高效的word2vec一起实现了大规模的词嵌入训练。\nWord2vec有两种模式，可以在下面的中看到：连续的词袋（CBOW）和skip-gram。它们的目标不同：一个基于周围的词预测中心词，而另一个则相反。\n虽然这些嵌入在概念上与使用前馈神经网络学习的嵌入技术没有什么不同，但是对非常大的语料库的训练使它们能够捕获诸如性别，动词时态和国家–首都关系之类的词之间的某些关系，由可知：\n这些关系及其背后的意义引发了对嵌入词的初步兴趣，许多研究调查了这些线性关系的起源（Arora等，2016; Mimno＆Thompson，2017; Antoniak＆Mimno，2018; Wendlandt等，2018））。然而，使用预训练嵌入作为初始化的固定词嵌入，把它作为当前NLP的主要内容被证明可以提高各种下游任务的性能。\n虽然捕获的关系word2vec具有直观且几乎神奇的性能，但后来的研究表明word2vec没有任何固有的特殊性：通过矩阵分解也可以学习词嵌入（Pennington等，2014; Levy＆Goldberg，2014）和通过适当的调整，经典的矩阵分解方法（如SVD和LSA）可以获得类似的结果（Levy等，2015）。\n从那时起，许多工作已经开始探索词嵌入的不同方面，可以通过这篇文章了解一些趋势和未来方向。尽管有许多发展，但word2ve仍然是如今被广泛使用的一种流行的选择。Word2vec的范围甚至超出了词级别：带有负抽样的skip-gram，一个基于本地环境学习嵌入的方便目标，已被应用于学习句子的表示（Mikolov＆Le，2014; Kiros et al.，2015）-甚至超越NLP，应用到网络（Grover＆Leskovec，2016）和生物序列（Asgari＆Mofrad，2015）等。\n一个特别令人兴奋的方向是将不同语言的词嵌入投影到同一空间中以实现（零射击）跨语言转移。越来越有可能以完全无监督的方式（至少对于类似语言）学习良好的投影，这开启了低资源语言和无监督机器翻译的应用（Lample等，2018; Artetxe等，2018）。请查看（Ruder等，2018）的概述。\n2013年-NLP的神经网络\n2013年和2014年是神经网络模型开始应用于NLP的标志年份。三种主要类型的神经网络被广泛使用：递归神经网络、卷积神经网络、循环神经网络。\n递归神经网络（RNN）是处理NLP中普遍存在的动态输入序列问题的明显选择。 Vanilla RNNs（Elman，1990）很快被经典的长短期记忆网络（Hochreiter＆Schmidhuber，1997）所取代，后者证明其对消失和爆炸梯度问题更具弹性。在2013年之前，RNN仍然被认为很难训练，Ilya Sutskever的博士论文是改变这种现状的一个关键例子。LSTM细胞可视化可以在下面的中看到。双向LSTM（Graves等，2013）通常用于处理左右上下文。\n随着卷积神经网络（CNN）被广泛用于计算机视觉，它们也开始应用于文本（Kalchbrenner等，2014; Kim等，2014）。用于文本的卷积神经网络仅在两个维度上操作，其中滤波器仅需要沿时间维度移动。下面的显示了NLP中使用的典型CNN。\n卷积神经网络的一个优点是它们比RNN更可并行化，因为每个时间步的状态仅取决于本地环境（通过卷积运算）而不是像RNN取决过去所有状态。CNN可以使用扩张卷积扩展到更宽的感受野，以捕捉更广泛的背景（Kalchbrenner等2016）。 CNN和LSTM也可以组合和堆叠，并且可以使用卷积来加速LSTM。\nRNN和CNN都将语言视为一个序列。然而，从语言学的角度来看，语言本质上是等级的：单词被组成高阶短语和子句它们本身可以根据一组生产规则递归地组合。将句子视为树而不是序列的语言启发思想产生了递归神经网络，这可以在下面的中看到：\n与从左到右或从右到左处理句子的RNN相比，递归神经网络从下到上构建序列的表示。在树的每个节点处，通过组合子节点的表示来计算新表示。由于树也可以被视为在RNN上施加不同的处理顺序，因此LSTM自然地扩展到树。\nRNN和LSTM不仅仅可以被扩展来使用分层结构，而且不仅可以根据本地语言学习词嵌入，而且可以基于语法背景来学习词嵌入（Levy＆Goldberg，2014）；语言模型可以基于句法堆栈生成单词（Dyer et al。，2016）; 图形卷积神经网络可以在树上运行（Bastings等，2017）。\n原文链接\n本文为云栖社区原创内容，未经允许不得转载。"}
{"content2":"前言：\n由于工作上的需要开始进入NLP领域。关于什么是NLP童鞋们可以自己百度学习这里不做解释。我的学习开发环境是Mac，但团队小伙伴有很多Windows的操作系统，所以使用虚拟机方式统一开放环境。我们将使用VMware＋CentOS7做为我们的我们的学习开发环境。使用Python语言。因为笔者也是边学边做会有一些踩坑的情况还望共同学习的童鞋们多多谅解。更希望大家可以共同探讨交流给出建议意见。\n工具准备:\n1.VMware 虚拟机工具\n官网 http://www.vmware.com/cn.html 现在对应自己系统的版本安装。\nCentOS7 操作系统 64位\n2.官网 https://www.centos.org\n注意：CentOS7 因为后面需要用到集成开发环境，所以CentOS 下载DVD ISO有桌面环境的。\n开放环境安装：\n1.VMware的安装。 安装说明\n注意：VMware的版本大同小异不要太纠结。\n2.在虚拟机中安装CentOS7。安装说明\n环境确认：\n1.启动CentOS7虚拟机环境\n2.进入CentOS7 控制台\n* 在桌面空白处单击鼠标右键->在终端中打开\n为了我们开放学习开发的方便我们。（主要是在CentOS下安装各种SDK，第三方包等）我们在控制台将统一使用root权限账户。\n* 在控制台窗口输入如下命令->su root 然后输入对应的密码\n3.检测语言环境\n* 在控制台输入->Python\n查看Python版本信息如上。默认情况CentOS7集成了Python环境，版本是2.7.x。试试打印一下hello word。如果执行无误说明基础环境已经具备。\n备注：\nCentOS虚机最好做一下系统升级。以保证软件的版本。笔者会保持最新升级。\n参考资料：\nPython自然语言处理 O'REILLY 人民邮电出版社 [美]Steven Bird Ewan Klein Edward Loper\nPython基础教程(第2版·修订版) 人民邮电出版社 [挪]Magnus Lie Hetland"}
{"content2":"MIT自然语言处理第三讲：概率语言模型（第二部分）\n发表于 2009年01月17号 由 52nlp\n自然语言处理：概率语言模型\nNatural Language Processing: Probabilistic Language Modeling\n作者：Regina Barzilay（MIT,EECS Department, November 15, 2004)\n译者：我爱自然语言处理（www.52nlp.cn ，2009年1月17日）\n二、语言模型构造\na) 语言模型问题提出（The Language Modeling Problem）\ni. 始于一些词汇集合（Start with some vocabulary）:\nν= {the, a, doctorate, candidate, Professors, grill, cook, ask, …}\nii. 得到一个与词汇集合v关的训练样本（Get a training sample of v）:\nGrill doctorate candidate.\nCook Professors.\nAsk Professors.\n……\niii. 假设（Assumption）:训练样本是由一些隐藏的分布P刻画的（training sample is drawn from some underlying distribution P）\niv. 目标（Goal）: 学习一个概率分布P prime尽可能的与P近似（learn a probability distribution P prime “as close” to P as possible）\nsum{x in v}{}{P prime (x)}=1, P prime (x) >=0\nP prime (candidates)=10^{-5}\n{P prime (ask~candidates)}=10^{-8}\nb) 获得语言模型（Deriving Language Model）\ni. 向一组单词序列w_{1}w_{2}…w_{n}赋予概率（Assign probability to a sequencew_{1}w_{2}…w_{n} ）\nii. 应用链式法则（Apply chain rule）:\n1. P(w1w2…wn)= P(w1|S)∗P(w2|S,w1)∗P(w3|S,w1,w2)…P(E|S,w1,w2,…,wn)\n2. 基于“历史”的模型(History-based model): 我们从过去的事件中预测未来的事件（we predict following things from past things）\n3. 我们需要考虑多大范围的上下文（How much context do we need to take into account）?\nc) 马尔科夫假设（Markov Assumption）\ni. 对于任意长度的单词序列P(wi|w(i-n) …w(i−1))是比较难预测的（For arbitrary long contexts P(wi|w(i-n) …w(i−1))difficult to estimate）\nii. 马尔科夫假设（Markov Assumption）: 第i个单词wi仅依赖于前n个单词（wi depends only on n preceding words）\niii. 三元语法模型（又称二阶马尔科夫模型）（Trigrams (second order)）:\n1. P(wi|START,w1,w2,…,w(i−1）)=P(wi|w(i−1),w(i−2))\n2. P(w1w2…wn)= 　P(w1|S)∗P(w2|S,w1)∗P(w3|w1,w2)∗…P(E|w(n−1),wn)\nd) 一种语言计算模型（A Computational Model of Language）\ni. 一种有用的概念和练习装置（A useful conceptual and practical device）:“抛硬币”模型（coin-flipping models）\n1. 由随机算法生成句子（A sentence is generated by a randomized algorithm）\n——生成器可以是许多“状态”中的一个（The generator can be one of several “states”）\n——抛硬币决定下一个状态（Flip coins to choose the next state）\n——抛其他硬币决定哪一个字母或单词输出（Flip other coins to decide which letter or word to output）\nii. 香农（Shannon）: “The states will correspond to the“residue of influence” from preceding letters”\ne) 基于单词的逼近（Word-Based Approximations）\n注：以下是用莎士比亚作品训练后随机生成的句子，可参考《自然语言处理综论》\ni. 一元语法逼近（这里MIT课件有误，不是一阶逼近（First-order approximation））\n1. To him swallowed confess hear both. which. OF save\n2. on trail for are ay device and rote life have\n3. Every enter now severally so, let\n4. Hill he late speaks; or! a more to leg less first you\n5. enter\nii. 三元语法逼近（这里课件有误，不是三阶逼近（Third-order approximation））\n1. King Henry. What! I will go seek the traitor Gloucester.\n2. Exeunt some of the watch. A great banquet serv’s in;\n3. Will you tell me how I am?\n4. It cannot be but so.\n附：课程及课件pdf下载MIT英文网页地址：\nhttp://people.csail.mit.edu/regina/6881/"}
{"content2":"版本：0.1\n本文是知识的综述，内容基本来源于网络和以前学到的东西。欢迎补充和更正。\n对于标题的三个定义，大部分情况下不需要分清楚。但当我们提到强人工智能时，有必要把相关的定义解释清楚，以便更准确的理解和交流。其实随着人工智能的不断发展，每个定义也在不停地变化。\n人工智能\n人工智能是一个基本的概念。从广义上来说，计算机实现的都可以算作人工智能，因为计算、记忆等就是智能的一部分。最开始，人类还没有计算机的时候，其实很多人就在考虑意识和智能到底是什么了，当时这还算是个哲学问题。到了机械制造较发达以后，有些人就觉得大脑就是一堆齿轮。当然还有把大脑想成别的东西的。从有了一定的制造能力后，人们还是很想制造出智能的，这时候应该是人工智能这个概念产生的时候。顺便提一句，人类也是最近一两百年才理解大脑才是思维的中心，而不是心脏。\n后来，计算机的产生与发展来源于军事的解密，后来造原子弹之类的。当时的计算机就是纯粹完成一些人类的需要几个月，一堆人才能做出来的数学题。（参考《暗算》的场景，中国的人海战术）这时候的机器有了记忆和计算的能力。\n再后来，随着计算机的硬件和软件的越来越通用，科学家们开始考虑如何将人类的知识和判断用计算机来实现，甚至实现和人一样的智能。如果这个目标实现了，理所当然可以叫做“人工智能”。当然最初的时候，大家以为写一堆if...else...就能把医学专家，石油勘探专家复制出来了，或者让计算机能够理解自然语言。这个过程中有很多成功的例子，可能有些现在还在解决实际的问题。但这个方向之算是一个捷径吧，有些专业的问题在人的逻辑认识复杂度之内的，都可以用这个方法来解决。但是很多模糊的，或者太庞杂的问题，就很难写出足够的if...else来解决了。这样的系统也无法解决通用问题。\n神经网络和机器学习\n差不多在专家系统发展的同时，现在所称的人工智能就出现了。一个方向是研究神经细胞，并模仿神经细胞的方式来解决问题，叫做人工神经网络（可简称神经网络）；另外一个方向是建模，并用统计的方法来解决问题，叫做机器学习。这时候，基本上专家系统就被排除在人工智能的概念之外了。因为一堆硬编码的if...else...已经不能体现出计算机系统的优越性了（半开玩笑，专家系统不容易解决更多通用的问题，开发成本也相对较高）。\n神经网络和机器学习这两个概念其实是相互重叠的，但又不完全相同。神经网络是从其结构来说的，类似于人类的神经结构，有神经细胞，树突和轴突，它们之间还会形成突触。另外还有整合、激发等特点。而机器学习是从其功能上来说的。主要是指计算机通过已知数据的训练，对逻辑有了统计上的认知，从而能够对需要解决的问题给出答案，甚至是未知问题。这里的“未知问题”不表示所有的未知问题，而是很多我们看起来是未知问题，但其抽象的结果还是一个已知问题，也可叫做元问题。\n现在很多机器学习的算法都是基于神经网络的，也有一些不是神经网络的算法用于机器学习。（我读书少，就不列举了。）大部分神经网络（也许所有）的算法都可以转化为矩阵。无外乎是节点及其之间的联系。有些神经网络几乎不需要学习，所以也不能管它们叫做机器学习。但大部分这类算法只能解决有限类型的问题，实际应用范围比较窄。\n当今的人工智能一般是灰盒的，知道输入和输出，但不一定能看懂里面的逻辑。其实我们日常的生活大部分都是这样的，告知需求，然后等待结果。这是人类活动和合作的模式。比如，一个软件项目，用户只关心功能方面的问题，不关心里面的代码和具体写出来的逻辑；理发的时候我们关心发型，不关心哪里要先剪，具体要剪多少。我们希望人工智能也是这样的，也只有这样才像个智能。但实际情况是，虽然解决了从输入到输出不需要对中间的逻辑过分关心的问题。但需要花大量时间来调整输入，以及整个模型的参数，直到给出的输出在一定输入范围内的答案满意为止。换句话说，虽然有了一个通用的模型可以解决很多问题，但还是要在每个具体的的问题上花很多时间去创建一个特定的模型。和人相比，计算机的优势是一旦模型创建出来，就能很稳定，且大规模、快速的解决问题。\n强人工智能\n大概从上世纪80年代的人工智能热以来，人工智能虽然有了很大的发展，发明了BP这样的能够处理复杂网络的算法，也解决了很多问题。但其智能仍然完全无法和人，甚至简单的虫子相提并论。（最近好像虫子项目已经能实现虫子级的智能了，等学了之后来更新。）所以，需要用个词语来区别于现有的人工智能。强人工智能和人工智能差了一个“强”字，以前我给这个概念用的“真”字。“强”当然是指的比现在的人工智能要强，其也是一个模糊的概念，到底是要强到像人这样呢，还是比人还强？这个概念随着人工智能的发展，会进一步变化。“真”指的就是和人一样的，真正的智能。（好吧，这也是相对的，没准儿某个智力发达的物种不认同我们是有真智能的物种。）有些地方还会用“超”人工智能的概念，指的是比人类还高的智能。\n现在的人工智能已经在各行各业广泛的应用起来，解决了很多实际的问题，也大大的解放了生产力。貌似人类也不需要更强人工智能。在没有电、没有手机之前，估计很多人也没觉得需要什么新东西。关于为什么需要强人工智能的问题，下一篇再讨论。"}
{"content2":"《自然语言处理综论》 2015-02-11 15:53\n想学习一下机器学习等一些高大上的技术。就从自然语言处理开始了，原来以为这具很容易入门。现在建议和我一样的初学者。还是从机器学习算法开始学习吧。不过这本书提前看，其实坏处也不大，大概的了解一下自然语言处理的内容。也增加学习机器学习的动机。由于这本书里课的自然语言处理是针对英语的，所以母语是汉语的我，英文又不行，从这本书里所能获取到的东西真有限。将来要是有幸真能从事自然语言处理的工作，再看一遍也罢。不打算再看自然语言处理的书了，除非是中文的处理。"}
{"content2":"Information Retrieval这个术语产生于Calvin Mooers1948年在MIT的硕士论文。\nInformation Retrieval(IR)：从文档集合中返回满足用户需求的相关信息的过程。作为一门学科，是研究信息的获取(acquisition)、表示(representation)、存储(storage)、组织(organization)和访问(access)的一门学问。\n信息检索可以看成计算机科学(Computer Science)和图书情报学(Library & Info. Science)的交叉学科。以计算机为手段，处理信息对象。和其他学科也融合：语言学、认知科学、\n检索来自英文单词Retrieval，有些人把它翻译成获取。其本义是“获得与输入要求相匹配的输出”。和我们平时所理解的搜索意义上的检索不一样。\nIR不仅仅是搜索，IR系统也不仅仅是搜索引擎。\n例1：返回与信息检索相关的网页􀃆搜索引擎(Search Engine, SE)\n例2：毛主席的生日是哪天？􀃆问答系统(Question Answering, QA)\n例3：返回联想PC的型号、配置、价格等信息􀃆信息抽取(Information Extraction, IE)\n例4：订阅有关NBA的新闻􀃆信息过滤(Information Filtering)、信息推荐(Information Recommending)\n也可以这样说，狭义的IR通常是指Information Search，而广义的IR包含非常多的内容(SE, QA, IE, …)\n形式上说，信息检索中的相关度是一个函数R，输入是查询Q、文档D和文档集合C，返回的是一个实数值R=f(Q,D,C)，信息检索就是给定一个查询Q，从文档集合C中计算每篇文档D与Q的相关度并排序(Ranking)。相关度通常只有相对意义，对一个Q，不同文档的相关度可以比较，而对于不同的Q的相关度不便比较。相关度的输入信息可以更多，比如用户的背景信息、用户的查询历史等等。现代信息检索中相关度不是唯一度量，如还有：重要度、权威度、新颖度等度量。或者说这些因子都影响“相关度”。Google中据说用了上百种排名因子\n信息检索的两种研究方式：1）以计算机为中心：IR的工作主要是建立索引、对用户查询进行处理、排序算法等等。2）以用户为中心：IR的主要工作是考察用户的行为、理解用户的需求、这些行为和需求如何影响检索系统的组织\n1948:\nC. N. Mooers在其MIT的硕士论文中第一次创造了“Information Retrieval”这个术语。\n1960－70年代：\n人们开始使用计算机为一些小规模科技和商业文献的摘要建立文本检索系统。产生了布尔模型(Boolean Model)、向量空间模型(Vector Space Model)和概率检索模型(Probabilistic Model)。\n康奈尔大学的Salton领导的研究小组是该领域研究的佼佼者。伦敦城市大学的Robertson及剑桥大学的SparckJones是概率模型的倡导者。\n1980年代：\n出现了一些商用的较大规模数据库检索系统：Lexis-Nexis，Dialog，MEDLINE\n1990’s:\n第一个网络搜索工具：1990年加拿大蒙特利尔McGill大学开发的FTP搜索工具Archie\n第一个WEB搜索引擎：1994年美国CMU开发的Lycos\n1995：斯坦福大学博士生开发的Yahoo\n1998：斯坦福大学博士生开发的Google，提出PageRank计算公式。\n1998：基于语言模型的IR模型提出。\n1990年代的其他重要事件:\n评测会议：NIST TREC\n推荐系统的出现：Ringo,Amazon,NetPerceptions\n文本分类和聚类的使用\n2000’s\n信息抽取:Whizbang,Fetch,Burning Glass\n问答系统:TREC Q/A track\n2001年，百度成立\n2000以来的其他重要事件：\n多媒体IR:Image,Video,Audio and music\n跨语言IR:DARPA Tides\n文本摘要:DUC评测\n图书情报学(Library & Info. Science)\n数据库管理(Database Management)\n人工智能(Artificial Intelligence)\n自然语言处理(Natural Language Processing)\n机器学习(Machine Learning)\nAI关注知识的表示、推理和智能行为。AI中知识的形式化表示:1)一阶谓词逻辑(First Order Predicate Logic).2)贝叶斯网络(Bayesian Networks).近年来Web本体及智能信息Agent方面研究使得IR和AI相互融合。\nNLP关注自然语言文本的语法(syntactic) 、语义(semantic)及语用(pragmatic)分析。NLP可以分析短语结构和语义，使得IR可以在短语上、或者从语义上进行处理，而不是仅仅基于单个关键词。\nNLP和IR天生就是融合的。通过上下文词义消歧(word sense disambiguation)来确定一个词在某个特定上下文的语义。通过一些NLP方法来获得文档中的一个语言片断(information extraction).通过NLP方法可以从文档集合中返回一些问题的答案(question answering)\nML关注通过对经验的学习来提高计算机系统的性能。1)从标注好的例子中学习相关概念，然后进行自动分类(有监督的学习，supervised learning).2)将未标注的例子自动聚集到有意义的不同集合中(无监督的学习，unsupervised learning).\n文本分类(Text Categorization):自动层次分类(如Yahoo目录),自适应过滤或推荐(Adaptive filtering/recommending),垃圾过滤(Spam filtering)\n文本聚类(Text Clustering):IR结果的自动聚类,层次型类别体系的自动构建(如Yahoo!目录)\nIR的两种模式：pull 或者push (filtering).Pull: 用户是主动的发起请求，在一个相对稳定的数据集合上进行查询。Push:用户事先定义自己的兴趣，系统在不断到来的流动数据上进行操作，将满足用户兴趣的数据推送给用户\n文本处理(Text Operations)：对查询和文本进行的预处理操作，中文分词(Chinese Word Segmentation)，词干还原(Stemming)，停用词消除(Stopwordremoval)\n查询处理(Query operations)：对经过文本处理后的查询进行进一步处理，得到查询的内部表示Query Representation)\n查询扩展(Query Expansion)：利用同义词或者近义词对查询进行扩展\n查询重构(Query Reconstruction)：利用用户的相关反馈信息对查询进行修改\n文本标引(Indexing)：对经过文本处理后的文本进行进一步处理，得到文本的内部表示(Text Representation)，通常基于标引项(Term)来表示。向量化、概率计算。组成成倒排表进行存储\n搜索(Searching)：从文本中查找包含查询中标引项的文本\n排序(Ranking)：对搜索出的文本按照某种方式来计算其相关度\nLogical View：指的是查询或者文本的表示，通常采用一些关键词或者标引项(index term)来表示一段查询或者文本。"}
{"content2":"1、首先需要构建自然语言处理的LTP的框架\n(1)需要下载LTP的源码包即c++程序(https://github.com/HIT-SCIR/ltp)下载完解压缩之后的文件为ltp-master\n(2)需要下载LTP4j的封装包(https://github.com/HIT-SCIR/ltp4j)，下载完解压缩之后的文件为ltp4j-master\n(3)需要下载cmake并且安装\n(4)需要下载ant用来编译LTP4j,将LTP4j文件编译成ltp.jar文件，最后在myeclipse中引用它\n2、首先编译ltp4j-master\n直接进入ltp4j-master的文件夹中，运行ant命令，然后就会生成一个\noutput的文件夹，里边是\njar包就在jar文件夹下。\n3、编译ltp-master的c++文件源码\n这个不会编译实在太复杂了，所以直接上网找了个编译好的，下了下来。大家可以上以下这个地址中去找去，下载64位已经编译好的动态链接库\nhttp://download.csdn.net/index.php/mobile/source/download/sv2008337/9471357\n4、构建java项目，然后进行分词测试\n构建的java项目如上图所示，ltp4j.jar文件就是第2步中编译出来的jar包。\nedu.hit.ir.ltp4j是从ltp4j-master文件夹中直接拷过来的。\nltp_data文件是分词的词库，据我们经理说现在都是用字典分词来着，所以肯定要有词库的，这个词库在这里是从下边这里拷进来的。\n好吧是我从网上下的，这里骗了大家了。\n下面来说卡了我时间最长的一步，就是通过jni来调用dlll动态链接库，因为这需要引入动态链接库的library\n就是下边这样，在这里配置你的dlll文件库在哪里，在我的电脑中是在，下下张图上边\n上面这张图也就是我从第3步中从网上下下来的dlll库，直接在java文件中配置就可以了\n5、分词测试\n经过了上面的准备工作我们就真的可以开始进行分词测试了\n(1)分词功能测试\n/** * 1，分词功能测试 */ @Test public void test1(){ if(Segmentor.create(\"ltp_data/cws.model\")<0){ System.err.println(\"load failed\"); return; } String sent = \"我是中国人\"; List<String> words = new ArrayList<String>(); int size = Segmentor.segment(sent,words); for(int i = 0; i<size; i++) { System.out.print(words.get(i)); if(i==size-1) { System.out.println(); } else{ System.out.print(\"\\t\"); } } Segmentor.release(); }\n结果如下所示：\n(2)词性标注功能测试\n/** * 词性标注功能测试 */ @Test public void testPosTag(){ if(Postagger.create(\"ltp_data//pos.model\")<0){ System.err.println(\"加载失败!\"); return; } List<String> words = new ArrayList<String>(); words.add(\"我\"); words.add(\"从事\"); words.add(\"自然\"); words.add(\"语言\"); words.add(\"处理\"); words.add(\"方面\"); words.add(\"的\"); words.add(\"工作\"); words.add(\"!\"); List<String> postags = new ArrayList<String>(); int size = Postagger.postag(words, postags); for (int i = 0; i < size; i++) { System.out.println(words.get(i)+\"_\"+postags.get(i)); if(i==size-1){ System.out.println(); }else{ System.out.println(\"|\"); } } Postagger.release(); }\n结果如下所示:"}
{"content2":"人工智能：计算机视觉、图像处理、模式识别、机器学习之间的关系\n什么是人工智能呢？人工智能，是由人类设计并在计算机环境下实现的模拟或再现某些人智能行为的技术。一般认为，人类智能活动可以分为两类：感知行为与思维活动。模拟感知行为的人工智能研究的一些例子包括语音识别、话者识别等与人类的听觉功能有关的“计算机听觉”，物体三维表现的形状知识、距离、速度感知等与人类视觉有关的“计算机视觉”，等等。模拟思维活动的人工智能研究的例子包括符号推理、模糊推理、定理证明等与人类思维有关的“计算机思维”，等等。\n人工智能领域：机器学习 深度学习 图像算法 图像处理 语音识别 图像识别 算法研究\n从图像处理和模式识别发展起来的计算机视觉研究对象之一是如何利用二维投影图像恢复三维景物世界。计算机视觉使用的理论方法主要是基于几何、概率和运动学计算与三维重构的视觉计算理论，它的基础包括射影几何学、刚体运动力学、概率论与随机过程、图像处理、人工智能等理论。计算机视觉要达到的基本目的有以下几个：\n(1) 根据一幅或多幅二维投影图像计算出观察点到目标物体的距离；\n(2) 根据一幅或多幅二维投影图像计算出目标物体的运动参数；\n(3) 根据一幅或多幅二维投影图像计算出目标物体的表面物理特性；\n(4) 根据多幅二维投影图像恢复出更大空间区域的投影图像。\n计算机视觉要达到的最终目的是实现利用计算机对于三维景物世界的理解，即实现人的视觉系统的某些功能。\n在计算机视觉领域里，医学图像分析、光学文字识别对模式识别的要求需要提到一定高度。又如模式识别中的预处理和特征抽取环节应用图像处理的技术；图像处理中的图像分析也应用模式识别的技术。在计算机视觉的大多数实际应用当中，计算机被预设为解决特定的任务，然而基于机器学习的方法正日渐普及，一旦机器学习的研究进一步发展，未来“泛用型”的电脑视觉应用或许可以成真。\n人工智能所研究的一个主要问题是：如何让系统具备“计划”和“决策能力”？从而使之完成特定的技术动作（例如：移动一个机器人通过某种特定环境）。这一问题便与计算机视觉问题息息相关。在这里，计算机视觉系统作为一个感知器，为决策提供信息。另外一些研究方向包括模式识别和机器学习（这也隶属于人工智能领域，但与计算机视觉有着重要联系），也由此，计算机视觉时常被看作人工智能与计算机科学的一个分支。\n机器学习是研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演译。\n为了达到计算机视觉的目的，有两种技术途径可以考虑。第一种是仿生学方法，即从分析人类视觉的过程入手，利用大自然提供给我们的最好参考系——人类视觉系统，建立起视觉过程的计算模型，然后用计算机系统实现之。第二种是工程方法，即脱离人类视觉系统框框的约束，利用一切可行和实用的技术手段实现视觉功能。此方法的一般做法是，将人类视觉系统作为一个黑盒子对待，实现时只关心对于某种输入，视觉系统将给出何种输出。这两种方法理论上都是可以使用的，但面临的困难是，人类视觉系统对应某种输入的输出到底是什么，这是无法直接测得的。而且由于人的智能活动是一个多功能系统综合作用的结果，即使是得到了一个输入输出对，也很难肯定它是仅由当前的输入视觉刺激所产生的响应，而不是一个与历史状态综合作用的结果。\n不难理解，计算机视觉的研究具有双重意义。其一，是为了满足人工智能应用的需要，即用计算机实现人工的视觉系统的需要。这些成果可以安装在计算机和各种机器上，使计算机和机器人能够具有“看”的能力。其二，视觉计算模型的研究结果反过来对于我们进一步认识和研究人类视觉系统本身的机理，甚至人脑的机理，也同样具有相当大的参考意义。\n在我的理解里，要实现计算机视觉必须有图像处理的帮助，而图像处理倚仗与模式识别的有效运用，而模式识别是人工智能领域的一个重要分支，人工智能与机器学习密不可分。纵观一切关系，发现计算机视觉的应用服务于机器学习。各个环节缺一不可，相辅相成。\n计算机视觉（computer vision），用计算机来模拟人的视觉机理获取和处理信息的能力。就是是指用摄影机和电脑代替人眼对目标进行识别、跟踪和测量等机器视觉，并进一步做图形处理，用电脑处理成为更适合人眼观察或传送给仪器检测的图像。计算机视觉研究相关的理论和技术，试图建立能够从图像或者多维数据中获取‘信息’的人工智能系统。计算机视觉的挑战是要为计算机和机器人开发具有与人类水平相当的视觉能力。机器视觉需要图象信号，纹理和颜色建模，几何处理和推理，以及物体建模。一个有能力的视觉系统应该把所有这些处理都紧密地集成在一起。\n图像处理（image processing），用计算机对图像进行分析，以达到所需结果的技术。又称影像处理。基本内容图像处理一般指数字图像处理。数字图像是指用数字摄像机、扫描仪等设备经过采样和数字化得到的一个大的二维数组，该数组的元素称为像素，其值为一整数，称为灰度值。图像处理技术的主要内容包括图像压缩，增强和复原，匹配、描述和识别3个部分。常见的处理有图像数字化、图像编码、图像增强、图像复原、图像分割和图像分析等。图像处理一般指数字图像处理。\n模式识别(Pattern Recognition)是指对表征事物或现象的各种形式的(数值的、文字的和逻辑关系的)信息进行处理和分析,以对事物或现象进行描述、辨认、分类和解释的过程,是信息科学和人工智能的重要组成部分。模式识别又常称作模式分类，从处理问题的性质和解决问题的方法等角度，模式识别分为有监督的分类（Supervised Classification）和无监督的分类(Unsupervised Classification)两种。模式还可分成抽象的和具体的两种形式。前者如意识、思想、议论等,属于概念识别研究的范畴,是人工智能的另一研究分支。我们所指的模式识别主要是对语音波形、地震波、心电图、脑电图、图片、照片、文字、符号、生物传感器等对象的具体模式进行辨识和分类。模式识别研究主要集中在两方面,一是研究生物体(包括人)是如何感知对象的，属于认识科学的范畴,二是在给定的任务下,如何用计算机实现模式识别的理论和方法。应用计算机对一组事件或过程进行辨识和分类，所识别的事件或过程可以是文字、声音、图像等具体对象，也可以是状态、程度等抽象对象。这些对象与数字形式的信息相区别，称为模式信息。模式识别与统计学、心理学、语言学、计算机科学、生物学、控制论等都有关系。它与人工智能、图像处理的研究有交叉关系。\n机器学习(Machine Learning)是研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演绎。机器学习在人工智能的研究中具有十分重要的地位。一个不具有学习能力的智能系统难以称得上是一个真正的智能系统，但是以往的智能系统都普遍缺少学习的能力。随着人工智能的深入发展，这些局限性表现得愈加突出。正是在这种情形下，机器学习逐渐成为人工智能研究的核心之一。它的应用已遍及人工智能的各个分支，如专家系统、自动推理、自然语言理解、模式识别、计算机视觉、智能机器人等领域。机器学习的研究是根据生理学、认知科学等对人类学习机理的了解，建立人类学习过程的计算模型或认识模型，发展各种学习理论和学习方法，研究通用的学习算法并进行理论上的分析，建立面向任务的具有特定应用的学习系统。这些研究目标相互影响相互促进。\n人类研究计算机的目的，是为了提高社会生产力水平，提高生活质量，把人从单调复杂甚至危险的工作中解救出来。今天的计算机在计算速度上已经远远超过了人，然而在很多方面，特别是在人类智能活动有关的方面例如在视觉功能、听觉功能、嗅觉功能、自然语言理解能力功能等等方面，还不如人。\n这种现状无法满足一些高级应用的要求。例如，我们希望计算机能够及早地发现路上的可疑情况并提醒汽车驾驶员以避免发生事故，我们更希望计算机能帮助我们进行自动驾驶，目前的技术还不足以满足诸如此类高级应用的要求，还需要更多的人工智能研究成果和系统实现的经验。\n转载时请以超链接形式标明文章原始出处和作者信息及本声明http://www.blogbus.com/shijuanfeng-logs/216968430.html"}
{"content2":"使用 js 实现文本过多时隐藏部分文本\n情景描述： 有时候我们需要显示部分文字，就像 QQ 空间这样，先显示部分文字，加一个【查看全文】，让用户选择是否查看全部\n解决方法：\n第一步：在一个 id 为 fullText 的 div 标签中存放所有文本，在另一个名为 subText 的div标签中存放部分文本（20个字为例）的文本，在一个 a 标签中实现点击效果。页面初识加载时，将要显示的文本放入一个 text 变量中，将fullText 的 div 的文本置为空，判断 text 长度是否大于20，若大于20，在名为subText 的 div 标签中显示前20个字的文本+“…查看全文”。若不大于20，在subText 显示全部文本。\n第二步：点击a标签时，判断 a 标签的文本内容，若为“…查看全文”，将 text 变量中的值赋给 subText，并将 a 标签改为”收起”。若a标签中的内容为”收起”，将 text 变量的前9个值赋给 subText 并将 a 标签改为“…查看全文”。\nhideText.html 源代码：\n<!DOCTYPE html> <html> <head> <meta charset=\"UTF-8\"> <title>隐藏功能</title> </head> <body> <div id=\"d1\"> <div id=\"fullText\"> 人工智能，英文缩写为AI。它是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。 人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。人工智能从诞生以来，理论和技术日益成熟，应用领域也不断扩大，可以设想，未来人工智能带来的科技产品，将会是人类智慧的“容器”。人工智能可以对人的意识、思维的信息过程的模拟。人工智能不是人的智能，但能像人那样思考、也可能超过人的智能。 </div> <div id=\"subText\"></div><a id=\"btn\" onclick=\"change()\" style=\"color: blue;\"></a> </div> <script> var text; function show() { text = document.getElementById(\"fullText\").innerHTML; document.getElementById(\"fullText\").innerHTML = \"\"; document.getElementById(\"subText\").style.float = \"left\"; document.getElementById(\"btn\").style.float = \"left\"; if(text.length > 20) { document.getElementById(\"subText\").innerHTML = text.substring(0, 20); document.getElementById(\"btn\").innerHTML = \"...查看全文\"; } else { document.getElementById(\"subText\").innerHTML = text; document.getElementById(\"btn\").innerHTML = \"\"; } } function change() { var t = document.getElementById(\"btn\"); var tt = document.getElementById(\"subText\"); if(t.innerHTML == \"...查看全文\") { tt.innerHTML = text; t.innerHTML = \"收起\" } else { tt.innerHTML = text.substring(0, 20); t.innerHTML = \"...查看全文\" } } show(); </script> </body> </html>"}
{"content2":"一方面学习基础理论，一方面进行实际操作，可以快速掌握Tensorflow。\n推荐阅读《TensorFlow深度学习算法原理与编程实战》，并根据实际代码进行测试。\n《TensorFlow深度学习算法原理与编程实战》以基础理论讲解和实践操作相结合的形式，详细介绍了深度学习的相关知识，能对使用TensorFlow进行深度学习算法设计的过程有更深入的理解。\n学习参考：\n《TensorFlow深度学习算法原理与编程实战》PDF，564页，带书签目录，文字可以复制。\n配套源代码。作者：蒋子阳\n下载: https://pan.baidu.com/s/1dAPvGY6jYOYSKrlfahwVNg\n提取码: djrj\n《TensorFlow深度学习算法原理与编程实战》共14章，主要内容有：人工智能、大数据、机器学习和深度学习概述；深度学习及TensorFlow框架的相关背景；\nTensorFlow的安装；TensorFlow编程策略；深度前馈神经网络；优化网络的方法；全连神经网络的经典实践；卷积神经网络的基础知识；\n经典卷积神经网络的TensorFlow实现；循环神经网络及其应用；\n深度强化学习概述；TensorFlow读取数据的API；\nTensorFlow持久化模型的API；可视化工具TensorBoard的使用；\nTensorFlow使用多GPU或并行的方式加速计算等。\n另推荐学习：《深入理解TensorFlow架构设计与实现原理》\n《深入理解TensorFlow架构设计与实现原理》PDF，375页，带书签目录，文字可以复制。\n作者：彭靖田,林健,白小龙\n下载: https://pan.baidu.com/s/1ZjznggugS3S_ivrLqeydMg\n提取码: wp4w\n《深入理解TensorFlow架构设计与实现原理》分为五大部分。\n第一部分为基础篇（第1～3章），简单介绍了TensorFlow设计目标、基本架构、环境准备和基础概念，包括数据流图的设计与使用，以及TensorFlow运行环境和训练机制，帮助读者快速入门TensorFlow，迅速上手使用。\n第二部分为关键模块篇（第4～7章），着重讲解了使用TensorFlow端到端解决人工智能问题涉及的关键模块，包括数据处理、编程框架、可视化工具和模型托管工具，帮助读者进一步提升开发效率，快速落地模型应用。\n第三部分为算法模型篇（第8～11章），在读者熟练掌握TensorFlow后，该部分将深度学习与TensorFlow有机结合，系统介绍了深度学习的发展历史与应用场景，并结合理论与代码实现深入讲解了CNN、GAN和RNN等经典模型。\n第四部分为核心揭秘篇（第12～14章），深入剖析了TensorFlow运行时核心、通信原理和数据流图计算的原理与实现，聚焦C++ 核心层的揭秘，帮助读者进一步理解TensorFlow底层的设计思想与实现细节，TensorFlow二次开发人员需重点关注这部分内容。\n第五部分为生态发展篇（第15章），全面介绍了TensorFlow生态系统发展，并重点介绍了Keras深度学习算法库，以及TensorFlow与云原生社区Kubernetes生态的结合、与大数据社区Spark生态的结合，并介绍了TensorFlow通信优化技术、TPU及NNVM模块化深度学习技术，帮助读者进一步全面了解深度学习生态发展的现状。\n另推荐：林大贵编写的《Tensorflow+Keras深度学习人工智能实践应用》\n《Tensorflow+Keras-深度学习人工智能实践应用》PDF，329页，带书签目录，文字可以复制。\n配套源代码。作者: 林大贵\n下载：https://pan.baidu.com/s/1NwhEBvB7gwvO81e8PhH75g\n《Tensorflow+Keras-深度学习人工智能实践应用》提供安装、上机操作指南，同时辅以大量范例程序介绍TensorFlow + Keras深度学习方面的知识。\n《Tensorflow+Keras-深度学习人工智能实践应用》分9部分，共21章，内容主要包括基本概念介绍、TensorFlow 与 Keras 的安装、Keras MNIST手写数字识别、Keras CIFAR-10照片图像物体识别、Keras多层感知器预测泰坦尼克号上旅客的生存概率、使用Keras MLP、RNN、LSTM进行IMDb自然语言处理与情感分析、以TensorFlow张量运算仿真神经网络的运行、TensorFlow MNIST手写数字识别、使用GPU大幅加快深度学习训练。\n另推荐: 顾思宇等所写的《TensorFlow实战Google深度学习框架(第2版)》\n《TensorFlow实战Google深度学习框架(第2版)》PDF，363页，带书签目录，文字可以复制。\n配套源代码。\n作者: 顾思宇 / 梁博文 / 郑泽宇\n下载：https://pan.baidu.com/s/1hltIZdjkET3CNxKWSixtZA\n《TensorFlow：实战Google深度学习框架（第2版）》为TensorFlow入门参考书，快速、有效的方式上手TensorFlow和深度学习。省略了烦琐的数学模型推导，从实际应用问题出发，通过具体的TensorFlow示例 介绍如何使用深度学习解决实际问题。包含深度学习的入门知识和大量实践经验。\n第2版将书中所有示例代码基于TensorFlow 1.4.0，新增两章分别介绍TensorFlow高层封装和深度学习在自 然语言领域应用的内容。\n很好入门，方便快速理解深度学习的整体流程架构，理论与tensorflow实践兼并，还能作为实践代码的参 照。\n我想作者的目的已经达到，强调整体的架构流程，忽略掉部分非常细函数的具体细节，不过作为阅读且止止步于一本书，细节入门方面推荐《深度学习入门：基于Python的理论与实现》 确实全程无理解难点。\n《深度学习入门：基于Python的理论与实现》中文PDF+代码\n下载：https://pan.baidu.com/s/1TaTZzete0hY8mK_jSqFsww\n《21个项目玩转深度学习》PDF+源代码\n下载：https://pan.baidu.com/s/1NYYpsxbWBvMn9U7jvj6XSw"}
{"content2":"作者：朱建平 腾讯云技术总监，腾讯TEG架构平台部专家工程师\n1.关于人工智能的若干个错误认知\n人工智能是AI工程师的事情，跟我没有什么关系\n大数据和机器学习(AI) 是解决问题的一种途径和手段，具有通用性，是一个基础的技能。当前我们工作中还有很多决策，是基于经验和预定的规则，未来这部分决策可以通过AI让我们做得更合理更好一些。\n人工智能太厉害了，未来会取代人类\n随着人工智能的发展，特别去年谷歌的AlphaGo围棋战胜代表人类的顶级棋手李世石，更是引爆了整个互联网。于是，网上不少人开始了很多担忧：机器人取代人类，有些人甚至在孩子高考填志愿时要求孩子填报艺术创作类似的方向，以避开未来与机器人或人工智能的竞争。\n实际上，虽然目前人工智能在语音识别，图片识别近年来取得了突破，但人工智能还远未完善: 数学理论尚不完备，“智能”的取得建立在大量的人工前期工作基础上，缺乏无监督学习。\n2. 传统开发 转行AI工程师的障碍\n2.1 急于求成的心态\nLR, SVM, 决策树，DNN,CNN, AlexNet, GoogleNet, Caffee,TensorFlow, 智能驾驶，AlphaGo, 个性化推荐, 智能语音，GPU, FPGA....\n晕了没？ 没晕再来一波。。。。\n这里面的水很深，不要太急躁很快能搞懂，事实上由于数学理论不完备，有些东西还真解释不清楚，比如在图像识别上ResNet 比GoogleNet识别率更高，ResNet是怎么推导出来的？\n梳理好这些概念，结合实际应用，化整为零逐步理解和吸收，有的放矢，不可操之过急。\n2.2 自底往上的学习方法，想要从基本概念学习\n建议结合应用场景先动手实践，再逐步细化。\n推荐《机器学习》 周志华 清华大学出版社\n3.AI工程师的知识结构-机器学习的基础知识\n3.1 人工智能<--> 机器学习<---->深度学习 的关系\n这是现在大家经常混淆的概念，什么叫做人工智能？什么叫做机器学习？什么叫做深度学习？人工智能是最大的范畴，只要你用计算机做了一点智能的事情都可以称为做了人工智能的工作。真正的人工智能应该是让机器拥有人的智能，让机器跟人一样能看、能听、能说，能用自然语言跟人进行交流。这个涉及到计算机视觉、语音识别、自然语言处理、人机交互、语音合成等等，是常规的我们研究讨论的人工智能的主要发力点，在互联网公司有着广阔应用场景的。\n机器学习可能是人工智能目前最火的领域，深度学习可能又是机器学习最火的子领域。什么时候需要用人工智能？直觉上来讲数据越复杂，深度学习越可能起作用；数据很简单很明确，深度学习可能就不怎么起作用了。比如搜索领域，目前只有Google宣称他们用深度学习double了用户点击率，是指他们将深度学习运用在用户浏览过、搜索过的信息上，那是非常庞大非常复杂的数据。\n3.2 机器学习解决问题的基本步骤\n一般应用机器学习解决实际问题分为4个步骤：\n1）定义目标问题\n目前还没看到有一个机器学习模型适用于解决所有问题，不同问题有各自适用的模型，如图像相关问题有深度学习、推荐相关问题有专门的推荐算法、安全相关问题有异常检测模型等。脱离具体问题去讨论模型是没有意义的。\n2)收集数据和特征工程\n机器学习是面向数据编程，数据是机器学习的基础。训练模型时，一般会把样本数据拆成两部分，其中大部分(约7成）数据用于训练模型，称其为训练集；另外少部分数据用于测试“模型的好坏”（也称“泛化能力”），称其为测试集。\n同一个机器学习算法，好的数据能让其表现更好，差的数据会让模型毫无用处。什么是“好的数据”？并没有统一定义，从结果看，能让模型表现良好的数据就是“好的数据”。一个可行的办法是想象“人”在解决该问题时，会依据哪些数据和特征做决策，然后挑选这些数据和特征作为机器学习模型的输入数据，这个过程就是特征工程。在应用机器学习时，可能需要反复做多次特征工程，特征工程是个试错的过程。\n3)训练模型和评估模型效果\n利用标注数据，训练模型数据，而一般的步骤是：\na. 从底层存储读取数据\nb. 对训练数据进行前向计算\nc. 计算训练误差\nd. 反向计算梯度，更新网络参数\ne. 重复a - d 步，直到模型收敛。\n测试模型效果，一般测试数据集远小于训练集，这里主要是快速前向计算，一般合并在第一步中。\n4)线上应用和持续优化\n模型在训练集上性能达标，但在线上环境性能不达标，这一现象被称为“过拟合”。通常的原因是用于训练模型的数据中特征的分布与线上数据偏差太大，此时需提取更具代表性的数据重新训练模型。\n模型在线上应用后，需持续跟踪模型的性能表现，机器学习是面向数据编程，如果线上系统上的数据出现了不包含在训练集中的新特征，需要补充新样本，重新训练迭代模型以保证预测效果。\n3.3 机器学习的相关概念\n模型用途：分类、回归、聚类\n主要区分在于output的描述是什么性质:分类是指output是整数（即多个类别标签）；回归是指output是一个实数，例如预测股票的走势，input是时间，output就是股票价格；聚类一般都是应用于非监督的状态下，对output完全不知道，只能对input数据本身进行统计分析，比如用户画像，通过数据之间的关系如关联程度将数据分成好几簇。\n训练过程: 监督、半监督和非监督\n机器学习是一个用数据训练的过程；监督是指input的每个数据样本，我们明确知道它的output（如类别标签）是什么；半监督是指我们只知道input数据样本中一小部分的output，另外大部分不知道；非监督是指所有input的数据样本，我们完全不知道它们的output是什么。\n学习模型：LR/SVM/决策树（传统的分类和聚类）DNN(深度神经网络）CNN（卷积神经网络）\n常用CNN模型：AlexNet, GoogleNet, ResNet\n浅层和深层，以前的机器学习方法大都是浅层，浅层学习模型是从六十年代发展到现在；深层学习模型过去不怎么work，自2010年迄今有了非常大的突破，深层模型在大量（至少百万级别）的有标签的数据驱动下将input端到output端之间的映射做的更深更完善。\n开源框架&平台：Caffee, TensorFlow（Google）,Torch (Facebook)\n为什么有这么多深度学习框架，参考《Deep Learning System Design Concepts》http://mxnet.io/architecture/index.html#deep-learning-system-design-concepts\n4.入门成为AI工程师的可行路径\n虽然从垂直领域讲有语音识别，图像视觉，个性化推荐等业务领域的AI工程师，但从其所从事的研发内容来看，从事AI研发的工程师主要分为3类：\n1)AI算法研究\n这类人大都有博士学历，在学校中积累了较好的理论和数学基础积累，对最新的学术成果能较快理解和吸收。这里的理论是指比如语音处理，计算机视觉等专业知识。\nAI算法研究的人主要研究内容有 样本特征，模型设计和优化，模型训练。样本特征是指如何从给定的数据中构建样本，定义样本的特征，这在个性化推荐领域中就非常重要。模型设计和优化是设计新的网络模型，或基于已有的模型机型迭代优化，比如CNN网络模型中AlexNet, GoogleNet v1/v2/v3, ResNet等新模型的不断出现，另外就是比如模型剪枝，在损失5%计算精度情况下，减少80%计算量，以实现移动终端的边缘计算等等。模型训练是指训练网络，如何防止过拟合以及快速收敛。\n2）AI工程实现\n这类人主要提供将计算逻辑，硬件封装打包起来，方便模型的训练和预测。比如：\n精通Caffee/TensorFlow等训练框架源码，能熟练使用并做针对性优化；\n构建机器学习平台，降低使用门槛，通过页面操作提供样本和模型就能启动训练；\n通过FPGA实行硬件加速，实现更低延时和成本的模型预测；\n在新模型验证完成后，实现在线平滑的模型切换；\n3）AI应用\n侧重验证好的模型在业务上的应用，常见语音识别，图像视觉，个性化推荐。当然这也包括更多结合业务场景的应用，比如终端网络传输带宽的预测，图片转码中参数的预测等等。\n5.DNN 和 CNN网络\n5.1 DNN原理\nDNN深度神经网络是模拟人脑的神经元工作机制构建的计算处理模型。\n激活函数常用的有：sigmoid，ReLU等,比如 典型的sigmoid函数\n多个神经元分层组织起来构成了一个网络，早期神经元网络仅能做到浅层，在训练方法和计算能力获得突破后，深层神经网络DNN得到了更广泛研究和应用。\n5.2 CNN原理\n简化的计算过程：\n上图展示了一次卷积计算: 一个66的图片I 使用卷积核F进行卷积，得到输出图片O。输入图片中在patch范围内的元素和卷积核中对应的元素相乘，最后乘积结果相加。\n真实的计算过程：\n上图是三维卷积的展示，对于第一层来说卷积核是11x11x3，在输入立方体227x227x3上进行滑动，对应图表 2中的k=11，N=227，D=3.卷积算法就是卷积核11x11x3和立方体227x227x3的重叠的每个值做乘运算，再把乘的结果做累加，最后得到一个值，数学公式为 y = x[0]k[0]+ x[1]k[1]+…+x[362]k[362]，因为卷积核11x11x3共有363个值，所以我们可以看成一个1x363的矩阵乘以363x1矩阵。\n6.AI开发的典型场景\n训练采用caffee 单机框架，单机2卡K80 GPU，为充分发挥GPU，采用了数据并行，一次一个batch 256张图片输入，alexnet网络分为前5层卷积层，后3层为全连接层，主要的计算在卷积计算，我们将其用FPGA实现，全连接层采用CPU实现。\n海量准确的样本也是个细致活，需要不断运营。\n腾讯云GPU云客户案例\n香港理工项目\n该实验项目是基于美国气象局提供的10年的气象数据，包括温度，湿度，风向，风速，降雨量，云层厚度，云图，空气浑浊度，日照等数据，对未来一段时间的天气进行预测。在该项目中，我们使用Google进行基于神经网络深度学习的Tensorflow框架，用Python2.7进行开发，并且在GPU上对深度神经网络进行训练。\n7.CPU、GPU、FPGA区别\n大多数人可能有个大致的认识: 训练用GPU, 预测用CPU 或者 FPGA\nCPU 开发门槛低，未来主要承载 高性能网络，计算分拆出来的逻辑复杂，不适合并行计算的部分。\nGPGPU 最新的P40,P100系列，采用16nm工艺，因其Cuda开发环境比较成熟，学习成本低，灵活性高，将继续在AI的模型训练阶段发挥关键作用。\nFPGA 最新的YV9P(16nm) FPGA，之前强调节能，单FPGA在数据中心的部署也是一个全新的课题，未来可能会加强 HBM2片上DDR内存容量和带宽的增长。未来在在线模型预测方面发挥重要作用，但IP不足，开发周期长是一个瓶颈。 FPGA卡，驱动开发，IP实现导致使用门槛较高，未来会在FPGA云上消除这些应用障碍。\n8. 腾讯云的GPU云、FPGA云进展\n8.1 腾讯云 Skylake CPU\n2017年2月，腾讯云宣布在国内率先使用英特尔下一代至强®处理器（代号Skylake），推出国内最新一代云服务器。新一代云服务器在计算性能、内存带宽、网络时延等方面拥有显著优势，最高可提供96 vCPU，可满足企业对云服务器高规格高配置的广泛需求，尤其在人工智能等高性能计算领域将发挥更大价值。据介绍，目前腾讯云官网已开放新一代云服务器的试用申请，客户将花费更低的购买价格，享受到更高性能计算服务。\n与过往采用至强系列处理器的云服务器相比，内置Skylake至强®处理器的新一代云服务器具有更高计算性能、更大内存带宽、更强存储I/O性能、更低网络时延等优势，能满足游戏行业、视频行业、金融行业等领域的更高计算需求。具体而言，Skylake至强®处理器具备的更优特性主要包括：\nSkylake至强®处理器支持AVX-512指令, 可支持更大数据宽度处理，能加速多媒体编解码、加解密数值运算，在机器学习、科学计算和金融分析等需要顶尖的浮点运算功能的场景提供更优质的处理性能。\nSkylake至强®处理器支持Omni-Path 互联架构，有助于提供更快的数据访问效率、更低的延时服务。\n8.2 腾讯云GPU\n腾讯云推出基于NVIDIA最新企业级产品（M40和P40）的云产品GPU云服务器和GPU黑石服务器，其中，基于M40的GPU云服务器已于2016年底正式上线。今年上半年，腾讯云还将推出1机8卡的GPU云服务器，单机多卡整机性能更高，可以满足超大数据量超大规模机器学习算法需求，最大化提升计算效率。\nG2实例最多可提供 2 个 NVIDIA M40 GPU、56 个 vCPU 和 120GB 主机内存，以及双卡 48GB 的GDDR5 显存。GPU云服务器拥有高达6144个加速核心、单机峰值计算能力突破14T Flops单精度浮点运算，0.4T Flops双精度浮点运算。\n在视频渲染、虚拟化桌面、深度学习等对计算能力要求极高的场景中，腾讯云GPU云服务器以及GPU黑石服务器都有广泛的应用前景，同时还能满足图形数据库、高性能数据库、计算流体动力学、计算金融、地震分析、分子建模、基因组学、渲染等领域对基础设施的高要求，且极具性价比。\n目前GPU云服务器已开放官网申请，腾讯GPU云申请\n8.3 腾讯云FPGA\n腾讯云在年前宣布推出国内首款高性能异构计算基础设施“FPGA云服务器”。已于2017年1月内测上线，以云服务方式将大型公司才能长期支付使用的FPGA推广到更多企业。 腾讯云和业界厂商有良好的关系，提供了基于 Intel和Xilinx 两家的硬件平台和软件开发工具，方便开发者选择自己熟悉的开发模式，避免切换平台。\n1）硬件平台\n腾讯云即将发布基于 Intel和Xilinx的单机4卡FPGA云服务器，推出多种规格的FPGA实例供您选择。单机多卡整机性能更高，可以满足超大数据量超大规模机器学习算法需求。也可选择单卡可节省计算效率，轻资产开发，降低项目研发期间的投入成本。\n2） 腾讯云官方FPGA IP\nAlexnet网络模型预测加速（已上线）--->用于图片鉴黄的粗筛\nGooglenetv1 网络模型预测加速（今年上半年） ---->用于图片鉴黄的精选\n同步开放的还有 内部使用的图片压缩IP。\n3）FPGA 生态建设\n我们通过IP市场，以开放合作的心态引入更多第三方成熟的AI IP进来，为FPGA生态的发展注入新的生机。\nQ&A机器学习对于模仿人的思考是怎么做到的？\n现在机器学习模仿人的思考做的比较原始。目前主要还是提取人做某项决策时考虑的主要因素，在机器学习中我们叫样本特征来告诉模型，当遇到类似特征时应该输出什么。\n相关阅读：\n腾讯GPU云申请\n人人都可以做深度学习应用：入门篇\n此文已由作者授权腾讯云技术社区发布，转载请注明文章出处，获取更多云计算技术干货，可请前往腾讯云技术社区\n欢迎大家关注腾讯云技术社区-博客园官方主页，我们将持续在博客园为大家推荐技术精品文章哦~"}
{"content2":"一． NLTK的几个常用函数\n1. Concordance\n实例如下：\n>>> text1.concordance(\"monstrous\") Displaying 11 of 11 matches: ong the former , one was of a most monstrous size . ... This came towards us , ON OF THE PSALMS . \" Touching that monstrous bulk of the whale or ork we have r ll over with a heathenish array of monstrous clubs and spears . Some were thick d as you gazed , and wondered what monstrous cannibal and savage could ever hav that has survived the flood ; most monstrous and most mountainous ! That Himmal they might scout at Moby Dick as a monstrous fable , or still worse and more de th of Radney .'\" CHAPTER 55 Of the Monstrous Pictures of Whales . I shall ere l ing Scenes . In connexion with the monstrous pictures of whales , I am strongly ere to enter upon those still more monstrous stories of them which are to be fo ght have been rummaged out of this monstrous cabinet there is no telling . But of Whale - Bones ; for Whales of a monstrous size are oftentimes cast up dead u >>>\n这个函数就是用来搜索单词word在text 中出现多的情况,包括出现的那一行,重点强调上下文。从输出来看 concordance 将要查询的单词,基本显示在一列,这样容易观察其上下文.\n2. Similar\n实例：\n>>> text1.similar(\"monstrous\") modifies horrible singular mouldy contemptible determined tyrannical candid wise lamentable pitiable fearless loving maddens domineering careful true mystifying part passing >>>\n这个函数的作用则是根据word 的上下文的单词的情况,来查找具有相似的上下文的单词. 比如monstrous 在上面可以看到,有这样的用法:\nmost monstrous size\nthe monstrous pictures\nthis monstrous cabinet\n等等, similar() 函数会在文本中 搜索具有类似结构的其他单词, 不过貌似这个函数只会考虑一些简单的指标,来作为相似度,比如上下文的词性,更多的完整匹配, 不会涉及到语义.\n3. Common_contexts\n实例：\n>>> text1.common_contexts([\"monstrous\", \"very\"]) No common contexts were found >>> text2.common_contexts([\"monstrous\", \"very\"]) a_pretty a_lucky am_glad be_glad is_pretty >>>\n这个函数跟simailar() 有点类似,也是在根据上下文搜索的.\n不同的是,这个函数是用来搜索 共用 参数中的列表中的所有单词,的上下文.即: word1,word2 相同的上下文.\n4. Dispersion_plot\n实例：\n>>> text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"Americ a\"])\n这个函数是用离散图 表示 语料中word 出现的位置序列表示. 效果如下：\n其中横坐标表示文本的单词位置.纵坐标表示查询的单词, 坐标里面的就是,单词出现的位置.就是 单词的分布情况。\n5. generate\n实例：\n>>> text3.generate() Traceback (most recent call last): File \"<stdin>\", line 1, in <module> TypeError: generate() missing 1 required positional argument: 'words' >>>\n产生一些与text3风格类似的随机文本。但在本机上却出错，原因是我使用的是nltk3.2.4和Python3.4.4，该版本下generate函数被注释了，所以无法使用。而《python自然语言处理时》书中用的是NLTK2.0版本。\n6. _future_模块\n_future_模块使得在Python2.x的版本下能够兼容更多的Python3.x的特性。把下一个新版本的特性导入到当前版本，于是我们就可以在当前版本中测试一些新版本的特性。所以Python3.x以后的版本中都不含有该模块。"}
{"content2":"前期准备\n使用文本向量化的前提是要对文章进行分词，分词可以参考前一篇文章。然后将分好的词进行向量化处理，以便计算机能够识别文本。常见的文本向量化技术有词频统计技术、TF-IDF技术等。\n词频统计技术\n词频统计技术是很直观的，文本被分词之后。 用每一个词作为维度key，有单词对应的位置为1，其他为0，向量长度和词典大小相同。然后给每个维度使用词频当作权值。词频统计技术默认出现频率越高的词权重越大。\n举例说明：\n原文：\n句子A：我喜欢看电视，不喜欢看电影。\n句子B：我不喜欢看电视，也不喜欢看电影。\n分词结果：\n句子A：我/喜欢/看/电视，不/喜欢/看/电影。\n句子B：我/不/喜欢/看/电视，也/不/喜欢/看/电影。\n列出维度：我，喜欢，看，电视，电影，不，也.\n统计词频：\n句子A：我 1，喜欢 2，看 2，电视 1，电影 1，不 1，也 0。\n句子B：我 1，喜欢 2，看 2，电视 1，电影 1，不 2，也 1。\n转换为向量：\n句子A：[1, 2, 2, 1, 1, 1, 0]\n句子B：[1, 2, 2, 1, 1, 2, 1]\n可以看出：词频统计技术直观、简单。但是有明显的缺陷：中文中有的词汇，如：“我”，“的”出现频率很高，因此会赋予较高的权值，但是这些词汇本身无意义。因此若要使用词频统计技术，必须要引入停用词将这些无意义的词汇进行过滤。\nTF-IDF技术\nTF-IDF技术就是为了克服词频统计技术的缺陷而产生的，它引入了“逆文档频率”概念，它衡量了一个词的常见程度，TF-IDF的假设是：如果某个词或短语在一篇文章中出现的频率高，并且在其他文章中很少出，那么它很可能就反映了这篇文章的特性，因此要提高它的权值。\nTF-IDF技术的需要维护一个语料库或文件集用于计算每个词的出现频率，频率越高的逆文档频率越小。语料库可以是整个铁路规章制度的集合，也可以是某个规章制度的全文。实践证明，TF-IDF在分词的时候，也需要剔除掉明显的停用词，这样效果会比较好。\n例如对于铁路规章制度而言，文本中“列车”一次的词频必然会非常高，但是在其语料库中出现的频率会非常高，因此其权重反而会降低。"}
{"content2":"原文转载：http://www.hankcs.com/nlp/cs224n-introduction-to-nlp-and-deep-learning.html\n什么是自然语言处理\n这是一门计算机科学、人工智能以及语言学的交叉学科。虽然语言只是人工智能的一部分（人工智能还包括计算机视觉等），但它是非常独特的一部分。这个星球上有许多生物拥有超过人类的视觉系统，但只有人类才拥有这么高级的语言。\n自然语言处理的目标是让计算机处理或说“理解”自然语言，以完成有意义的任务，比如订机票购物或QA等。完全理解和表达语言是极其困难的，完美的语言理解等效于实现人工智能。（曾经有人问我，是不是光自然语言处理的知识都够一本书的量，无知可笑。另一方面，市面上似乎又有点野鸡书/教程/培训班泛滥的趋势）\n自然语言处理涉及的几个层次\n作为输入一共有两个来源，语音与文本。所以第一级是语音识别和OCR或分词（事实上，跳过分词虽然理所当然地不能做句法分析，但字符级也可以直接做不少应用）。接下来是形态学，援引《统计自然语言处理》中的定义：\n形态学（morphology）：形态学（又称“词汇形态学”或“词法”）是语言学的一个分支，研究词的内部结构，包括屈折变化和构词法两个部分。由于词具有语音特征、句\n法特征和语义特征，形态学处于音位学、句法学和语义学的结合部位，所以形态学是每个语言学家都要关注的一门学科［Matthews,2000］。\n下面的是句法分析和语义分析，最后面的在中文中似乎翻译做“对话分析”，需要根据上文语境理解下文。\n这门课主要关注画圈的三个部分，其中中间的两个是重中之重，虽然深度学习在语音识别上的发力最大。\n自然语言处理应用\n一个小子集，从简单到复杂有：\n拼写检查、关键词检索……\n文本挖掘（产品价格、日期、时间、地点、人名、公司名）\n文本分类\n机器翻译\n客服系统\n复杂对话系统\n在工业界从搜索到广告投放、自动\\辅助翻译、情感舆情分析、语音识别、聊天机器人\\管家等等五花八门。\n人类语言的特殊之处\n与信号处理、数据挖掘不同，自然语言的随机性小而目的性强；语言是用来传输有意义的信息的，这种传输连小孩子都能很快学会。人类语言是离散的、明确的符号系统。但又允许出现各种变种，比如颜文字，随意的错误拼写“I loooove it”。这种自由性可能是因为语言的可靠性（赘余性）。所以说语言文字绝对不是形式逻辑或传统AI的产物。\n语言符号有多种形式（声音、手势、书写），在这些不同的形式中，其意义保持不变：\n虽然人类语言是明确的符号系统，但符号传输到大脑的过程是通过连续的声学光学信号，大脑编码似乎是连续的激活值上的模式。另外巨大的词表也导致数据稀疏，不利于机器学习。这构成一种动机，是不是应该用连续的信号而不是离散的符号去处理语言。\n什么是深度学习\n这是机器学习的一个子集。传统机器学习中，人类需要对专业问题理解非常透彻，才能手工设计特征。比如地名和机构名识别的特征模板：\n然后把特征交给某个机器学习算法，比如线性分类器。机器为这些特征调整找到合适的权值，将误差优化到最小。\n在这个过程中一直在学习的其实是人类，而不是机器。机器仅仅做了一道数值优化的题目而已。\n下面这张图很好地展示了这个过程中的比例：\n而深度学习是表示学习的一部分，用来学习原始输入的多层特征表示：\n“深度学习”的历史\n虽然这个术语大部分时候指代利用各种各样多层的神经网络进行表示学习，有时候也有一些概率图模型参与。统计学家会说，哦，不过是一些逻辑斯谛回归单元的堆砌而已。也许的确如此，但这还是以偏概全的说法（电子计算机还是一堆半导体的堆砌呢，大脑还是一堆神经元的堆砌呢）。这门课不会回顾历史（像Hinton老爷子那样博古通今），而只会专注当前在NLP领域大放异彩的方法。\n为什么需要研究深度学习\n手工特征耗时耗力，还不易拓展\n自动特征学习快，方便拓展\n深度学习提供了一种通用的学习框架，可用来表示世界、视觉和语言学信息\n深度学习既可以无监督学习，也可以监督学习\n深度学习可追溯到八九十年代，但在2010年左右才崛起（最先是语音与图像，后来才是NLP），那之前为什么没有呢？\n与Hinton介绍的一样，无非是以前数据量不够，计算力太弱。当然，最近也的确有许多新模型，新算法。\n语音识别中的深度学习\n突破性研究来自Hinton老爷子的学生，具体参考：http://www.hankcs.com/ml/hinton-deep-neural-nets-with-generative-pre-training.html#h3-11\n计算机视觉中的深度学习\n还是来自Hinton的学生。\n课程相关\n有4次编程练习，会用到TensorFlow。\n为什么NLP难\n人类语言是充满歧义的，不像编程语言那样明确。编程语言中有各种变量名，但人类语言中只有少数几个代词可以用，你得思考到底指代的是谁……\n人类语言的解读依赖于现实世界、常识以及上下文。由于说话速度书写速度阅读速度的限制，人类语言非常简练，省略了大量背景知识。\n接下来是几个英文的歧义例子，对native speaker而言很有趣。为了完整性只看一个：\nThe Pope’s baby steps on gays\n主要歧义发生在baby上面，可以理解为“教皇的孩子踩了基佬”，也可以理解为“教皇在同性恋问题上裹足不前”。\n旧版CS224d里面还有个更直观的例子，推特上关于电影明星“海瑟薇”的评论影响了保险公司哈撒韦的股价，因为两者拼写是一样的。\n说明某些“舆情系统”没做好命名实体识别。\nDeep NLP = Deep Learning + NLP\n将自然语言处理的思想与表示学习结合起来，用深度学习的手法解决NLP目标。这提高了许多方面的效果：\n层次：语音、词汇、语法、语义\n工具：词性标注、命名实体识别、句法\\语义分析\n应用：机器翻译、情感分析、客服系统、问答系统\n深度学习的一个魅力之处是，它提供了一套“宇宙通用”的框架解决了各种问题。虽然工具就那么几个，但在各行各业都适用。\nword vector\n老生常谈了。略过。听说接下来两课都在讲这个，希望有些更深入的收获。\nNLP表示层次：形态级别\n传统方法在形态级别的表示是词素：\n深度学习中把词素也作为向量：\n多个词素向量构成相同纬度语义更丰富的词向量。\nNLP工具：句法分析\n我在《基于神经网络的高性能依存句法分析器》中分析并移植的LTP句法分析器，参考的就是这里介绍的Danqi Chen的A Fast and Accurate Dependency Parser using Neural Networks.pdf。原来她是这门课的TA：\nNLP语义层面的表示\n传统方法是手写大量的规则函数，叫做Lambda calculus：\n在深度学习中，每个句子、短语和逻辑表述都是向量。神经网络负责它们的合并。\n情感分析\n传统方法是请一两百个工人，手工搜集“情感极性词典”在词袋模型上做分类器。\n深度学习复用了RNN来解决这个问题，它可以识别“反话”的情感极性：\n注意这只是为了方便理解的示意图，并不是RNN的工作流程。私以为这张图放在这里不合适，可能会误导一部分人，以为神经网络就是这样的基于规则的“决策树”模型。\nQA\n传统方法是手工编写大量的逻辑规则，比如正则表达式之类：\n我见过这类QA系统的实体，挺没意思的。\n深度学习依然使用了类似的学习框架，把事实储存在向量里：\n客服系统\n最著名的例子得数GMail的自动回复：\n图源\n这是Neural Language Models的又一次成功应用，Neural Language Models是基于RNN的：\n机器翻译\n传统方法在许多层级上做了尝试，词语、语法、语义之类。这类方法试图找到一种世界通用的“国际语”（Interlingua）来作为原文和译文的桥梁。\n而Neural Machine Translation将原文映射为向量，由向量构建译文。也许可以说Neural Machine Translation的“国际语”是向量。\n结论：所有层级的表示都是向量\n这可能是因为向量是最灵活的形式，它的维度是自由的，它可以组合成矩阵，或者更高维的Tensor。事实上，在实践的时候向量和矩阵没什么本质区别，经常看到为了效率或单纯的美观而pack成矩阵unroll成向量的操作。\n旧版视频中Socher还顺便广告了一下他的创业公司MetaMind（已被收购，人生赢家）：\n这个demo让我非常惊讶，因为普通NLP演示页面都是让人手工选择要执行的任务的。而这个demo竟然支持用一句话表示自己要执行的意图。不光可以执行情感分析、句法分析之类的常规任务，还可以输入一段话做推理任务。更让我惊讶的是，据说后台所有任务用的都是同一种模型，真乃神机也。据说这种模型是Dynamic Memory Network。另外，他们又发了篇A Joint Many-Task Model:Growing a Neural Network for Multiple NLP Tasks，不知道两者有什么联系没有。\n下面两次课会详细地讲解向量表示，希望能带来新的体会。"}
{"content2":"简单的问答已经实现了,那么问题也跟着出现了,我不能确定问题一定是\"你叫什么名字\",也有可能是\"你是谁\",\"你叫啥\"之类的,这就引出了人工智能中的另一项技术:\n自然语言处理(NLP) : 大概意思就是 让计算机明白一句话要表达的意思,NLP就相当于计算机在思考你说的话,让计算机知道\"你是谁\",\"你叫啥\",\"你叫什么名字\"是一个意思\n这就要做 : 语义相似度\n接下来我们用Python大法来实现一个简单的自然语言处理\n现在又要用到Python强大的三方库了\n第一个是将中文字符串进行分词的库叫 jieba\npip install jieba\n我们通常把这个库叫做 结巴分词 确实是结巴分词,而且这个词库是 made in china , 基本用一下这个结巴分词:\nimport jieba key_word = \"你叫什么名字\" # 定义一句话,基于这句话进行分词 cut_word = jieba.cut(key_word) # 使用结巴分词中的cut方法对\"你叫什么名字\" 进行分词 print(cut_word) # <generator object Tokenizer.cut at 0x03676390> 不懂生成器的话,就忽略这里 cut_word_list = list(cut_word) # 如果不明白生成器的话,这里要记得把生成器对象做成列表 print(cut_word_list) # ['你', '叫', '什么', '名字']\n测试代码就很明显了,它很清晰的把咱们的中文字符串转为列表存储起来了\n第二个是一个语言训练库叫 gensim\npip install gensim\n这个训练库很厉害, 里面封装很多机器学习的算法, 是目前人工智能的主流应用库,这个不是很好理解, 需要一定的Python数据处理的功底\nimport jieba import gensim from gensim import corpora from gensim import models from gensim import similarities l1 = [\"你的名字是什么\", \"你今年几岁了\", \"你有多高你胸多大\", \"你胸多大\"] a = \"你今年多大了\" all_doc_list = [] for doc in l1: doc_list = [word for word in jieba.cut(doc)] all_doc_list.append(doc_list) print(all_doc_list) doc_test_list = [word for word in jieba.cut(a)] # 制作语料库 dictionary = corpora.Dictionary(all_doc_list) # 制作词袋 # 词袋的理解 # 词袋就是将很多很多的词,进行排列形成一个 词(key) 与一个 标志位(value) 的字典 # 例如: {'什么': 0, '你': 1, '名字': 2, '是': 3, '的': 4, '了': 5, '今年': 6, '几岁': 7, '多': 8, '有': 9, '胸多大': 10, '高': 11} # 至于它是做什么用的,带着问题往下看 print(\"token2id\", dictionary.token2id) print(\"dictionary\", dictionary, type(dictionary)) corpus = [dictionary.doc2bow(doc) for doc in all_doc_list] # 语料库: # 这里是将all_doc_list 中的每一个列表中的词语 与 dictionary 中的Key进行匹配 # 得到一个匹配后的结果,例如['你', '今年', '几岁', '了'] # 就可以得到 [(1, 1), (5, 1), (6, 1), (7, 1)] # 1代表的的是 你 1代表出现一次, 5代表的是 了 1代表出现了一次, 以此类推 6 = 今年 , 7 = 几岁 print(\"corpus\", corpus, type(corpus)) # 将需要寻找相似度的分词列表 做成 语料库 doc_test_vec doc_test_vec = dictionary.doc2bow(doc_test_list) print(\"doc_test_vec\", doc_test_vec, type(doc_test_vec)) # 将corpus语料库(初识语料库) 使用Lsi模型进行训练 lsi = models.LsiModel(corpus) # 这里的只是需要学习Lsi模型来了解的,这里不做阐述 print(\"lsi\", lsi, type(lsi)) # 语料库corpus的训练结果 print(\"lsi[corpus]\", lsi[corpus]) # 获得语料库doc_test_vec 在 语料库corpus的训练结果 中的 向量表示 print(\"lsi[doc_test_vec]\", lsi[doc_test_vec]) # 文本相似度 # 稀疏矩阵相似度 将 主 语料库corpus的训练结果 作为初始值 index = similarities.SparseMatrixSimilarity(lsi[corpus], num_features=len(dictionary.keys())) print(\"index\", index, type(index)) # 将 语料库doc_test_vec 在 语料库corpus的训练结果 中的 向量表示 与 语料库corpus的 向量表示 做矩阵相似度计算 sim = index[lsi[doc_test_vec]] print(\"sim\", sim, type(sim)) # 对下标和相似度结果进行一个排序,拿出相似度最高的结果 # cc = sorted(enumerate(sim), key=lambda item: item[1],reverse=True) cc = sorted(enumerate(sim), key=lambda item: -item[1]) print(cc) text = l1[cc[0][0]] print(a,text)\n前方高能"}
{"content2":"是在去年（2011）知道这个online class的，那时的域名是www.ml-class.org，只有machine learning的课程。记得去年的某天，我在某处（应该是网易公开课）看了一集机器学习的公开课。很感兴趣，于是找到了Andrew Ng教授的主页，最后发现了这么一个网上公开课。\n在寒假时，我把公开课里没有任何字幕的Lecture下了下来，硬着头皮看了几个章节，结果是一知半解。现在工作之余，又想好好地把这个课程学习一下，于是决定通过写学习笔记的方式来督促自己去深究。\n现在ml-class已经成为了coursera.org的一份子，各路大牛在这开设online class，涉及各类学科，都是些难得的资源。ML class的页面链接：https://class.coursera.org/ml-2012-002/class/index，现在的版本已经能够下载字幕了。\n1. Welcome | 欢迎\n在这一节中，主要介绍了机器学习技术的基本概况，它的魅力以及它的应用实例。\n虽然我们一直以来不知道机器学习为何物，但是在生活中却经常使用与这个技术相关的服务，如搜索服务、相片识别、垃圾邮件分类等等。由此可见机器学习的魅力之大。Andrew Ng说\"For me one of the reasons I'm excited is the AI dream of someday building machines as intelligent as you or me.\"这么一个AI dream，很多人会为之动心吧！\n机器学习在人工智能中起着重要的作用，它随着人工智能的发展而得以发展，可以认为它使得计算机获得了一种强大的全新的能力。应用机器学习技术的领域有：数据挖掘（Data Mining）、手写识别（Handwriting Recognition）、自然语言处理（Natural Language Processing, NLP）以及计算机视觉（Computer Vision）。\n2. What is machine learning | 机器学习是什么\n对于“什么是机器学习，什么不是”，虽然很难给出一个能够被普遍接受的定义，但是还是有些定义值得研究的。\nArthur Samuel的定义是：Field of study that gives computers the ability to learn without being explicitly programmed. 其含义为，机器学习就是研究如何不通过明确地编写程序而实现给予计算机学习能力。\nTom M. Mitchell的定义：A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E. 对于某类任务T和性能度量P，如果一个计算机程序在T上以P衡量的性能随着经验E而自我完善，那么我们称这个计算机程序在从经验E中学习。（翻译来源：《机器学习》，Tom M. Mitchell，曾华军等译，机械工业出版社）\n用上述第二个定义来定义手写识别学习问题：\n任务T：识别和分类图像中的手写文字\n性能度量P：分类的正确率\n训练经验E：已知分类的手写文字数据库\n将要介绍的机器学习算法主要可分为有监督学习（Supervised Learning）和无监督学习（Unsupervised Learning）。在有监督学习中，我们将用训练样本去训练计算机，相当于教它做某事，而在无监督学习中，我们放手让它自己去学习。除此之外，还有增强学习（Reinforcement Learning）和推荐系统（Recommender System）将会被介绍。这个课程还会介绍有关学习算法实际应用的建议。\n未完待续……"}
{"content2":"手记实用系列文章：\n1 结巴分词和自然语言处理HanLP处理手记\n2 Python中文语料批量预处理手记\n3 自然语言处理手记\n4 Python中调用自然语言处理工具HanLP手记\n5 Python中结巴分词使用手记\nHanLP方法封装类：\n# -*- coding:utf-8 -*- # Filename: main.py from jpype import * startJVM(getDefaultJVMPath(), \"-Djava.class.path=C:\\hanlp\\hanlp-1.3.2.jar;C:\\hanlp\", \"-Xms1g\", \"-Xmx1g\") # 启动JVM，Linux需替换分号;为冒号: print(\"=\"*30+\"HanLP分词\"+\"=\"*30) HanLP = JClass('com.hankcs.hanlp.HanLP') # 中文分词 print(HanLP.segment('你好，欢迎在Python中调用HanLP的API')) print(\"-\"*70) print(\"=\"*30+\"标准分词\"+\"=\"*30) StandardTokenizer = JClass('com.hankcs.hanlp.tokenizer.StandardTokenizer') print(StandardTokenizer.segment('你好，欢迎在Python中调用HanLP的API')) print(\"-\"*70) # NLP分词NLPTokenizer会执行全部命名实体识别和词性标注 print(\"=\"*30+\"NLP分词\"+\"=\"*30) NLPTokenizer = JClass('com.hankcs.hanlp.tokenizer.NLPTokenizer') print(NLPTokenizer.segment('中国科学院计算技术研究所的宗成庆教授正在教授自然语言处理课程')) print(\"-\"*70) print(\"=\"*30+\"索引分词\"+\"=\"*30) IndexTokenizer = JClass('com.hankcs.hanlp.tokenizer.IndexTokenizer') termList= IndexTokenizer.segment(\"主副食品\"); for term in termList : print(str(term) + \" [\" + str(term.offset) + \":\" + str(term.offset + len(term.word)) + \"]\") print(\"-\"*70) print(\"=\"*30+\" N-最短路径分词\"+\"=\"*30) # CRFSegment = JClass('com.hankcs.hanlp.seg.CRF.CRFSegment') # segment=CRFSegment() # testCase =\"今天，刘志军案的关键人物,山西女商人丁书苗在市二中院出庭受审。\" # print(segment.seg(\"你看过穆赫兰道吗\")) print(\"-\"*70) print(\"=\"*30+\" CRF分词\"+\"=\"*30) print(\"-\"*70) print(\"=\"*30+\" 极速词典分词\"+\"=\"*30) SpeedTokenizer = JClass('com.hankcs.hanlp.tokenizer.SpeedTokenizer') print(NLPTokenizer.segment('江西鄱阳湖干枯，中国最大淡水湖变成大草原')) print(\"-\"*70) print(\"=\"*30+\" 自定义分词\"+\"=\"*30) CustomDictionary = JClass('com.hankcs.hanlp.dictionary.CustomDictionary') CustomDictionary.add('攻城狮') CustomDictionary.add('单身狗') HanLP = JClass('com.hankcs.hanlp.HanLP') print(HanLP.segment('攻城狮逆袭单身狗，迎娶白富美，走上人生巅峰')) print(\"-\"*70) print(\"=\"*20+\"命名实体识别与词性标注\"+\"=\"*30) NLPTokenizer = JClass('com.hankcs.hanlp.tokenizer.NLPTokenizer') print(NLPTokenizer.segment('中国科学院计算技术研究所的宗成庆教授正在教授自然语言处理课程')) print(\"-\"*70) document = \"水利部水资源司司长陈明忠9月29日在国务院新闻办举行的新闻发布会上透露，\" \\ \"根据刚刚完成了水资源管理制度的考核，有部分省接近了红线的指标，\" \\ \"有部分省超过红线的指标。对一些超过红线的地方，陈明忠表示，对一些取用水项目进行区域的限批，\" \\ \"严格地进行水资源论证和取水许可的批准。\" print(\"=\"*30+\"关键词提取\"+\"=\"*30) print(HanLP.extractKeyword(document, 8)) print(\"-\"*70) print(\"=\"*30+\"自动摘要\"+\"=\"*30) print(HanLP.extractSummary(document, 3)) print(\"-\"*70) # print(\"=\"*30+\"地名识别\"+\"=\"*30) # HanLP = JClass('com.hankcs.hanlp.HanLP') # segment = HanLP.newSegment().enablePlaceRecognize(true) # testCase=[\"武胜县新学乡政府大楼门前锣鼓喧天\", # \"蓝翔给宁夏固原市彭阳县红河镇黑牛沟村捐赠了挖掘机\"] # for sentence in testCase : # print(HanLP.segment(sentence)) # print(\"-\"*70) # print(\"=\"*30+\"依存句法分析\"+\"=\"*30) # print(HanLP.parseDependency(\"徐先生还具体帮助他确定了把画雄鹰、松鼠和麻雀作为主攻目标。\")) # print(\"-\"*70) text =r\"算法工程师\\n 算法（Algorithm）是一系列解决问题的清晰指令，也就是说，能够对一定规范的输入，在有限时间内获得所要求的输出。如果一个算法有缺陷，或不适合于某个问题，执行这个算法将不会解决这个问题。不同的算法可能用不同的时间、空间或效率来完成同样的任务。一个算法的优劣可以用空间复杂度与时间复杂度来衡量。算法工程师就是利用算法处理事物的人。\\n \\n 1职位简介\\n 算法工程师是一个非常高端的职位；\\n 专业要求：计算机、电子、通信、数学等相关专业；\\n 学历要求：本科及其以上的学历，大多数是硕士学历及其以上；\\n 语言要求：英语要求是熟练，基本上能阅读国外专业书刊；\\n 必须掌握计算机相关知识，熟练使用仿真工具MATLAB等，必须会一门编程语言。\\n\\n2研究方向\\n 视频算法工程师、图像处理算法工程师、音频算法工程师 通信基带算法工程师\\n \\n 3目前国内外状况\\n 目前国内从事算法研究的工程师不少，但是高级算法工程师却很少，是一个非常紧缺的专业工程师。算法工程师根据研究领域来分主要有音频/视频算法处理、图像技术方面的二维信息算法处理和通信物理层、雷达信号处理、生物医学信号处理等领域的一维信息算法处理。\\n 在计算机音视频和图形图像技术等二维信息算法处理方面目前比较先进的视频处理算法：机器视觉成为此类算法研究的核心；另外还有2D转3D算法(2D-to-3D conversion)，去隔行算法(de-interlacing)，运动估计运动补偿算法(Motion estimation/Motion Compensation)，去噪算法(Noise Reduction)，缩放算法(scaling)，锐化处理算法(Sharpness)，超分辨率算法(Super Resolution),手势识别(gesture recognition),人脸识别(face recognition)。\\n 在通信物理层等一维信息领域目前常用的算法：无线领域的RRM、RTT，传送领域的调制解调、信道均衡、信号检测、网络优化、信号分解等。\\n 另外数据挖掘、互联网搜索算法也成为当今的热门方向。\\n\" print(\"=\"*30+\"短语提取\"+\"=\"*30) print(HanLP.extractPhrase(text, 10)) print(\"-\"*70) shutdownJVM()\nHanLP运行结果：\npython调用HanLP的jar包\n链接: https://pan.baidu.com/s/1miDrWHq 密码: bmy6"}
{"content2":"MIT自然语言处理第四讲：标注（第四部分）\n发表于 2009年03月26号 由 52nlp\n自然语言处理：标注\nNatural Language Processing: Tagging\n作者：Regina Barzilay（MIT,EECS Department, November 15, 2004)\n译者：我爱自然语言处理（www.52nlp.cn ，2009年3月26日）\n三、 马尔科夫模型（Markov Model）\ng) 有效标注（Efficient Tagging）\ni. 对于一个单词序列，如何寻找最可能的标记序列（How to find the most likely a sequence of tags for a sequence of words）?\n1. 盲目搜索的方法是可怕的（The brute force search is dreadful）— 对于N个标记和W个单词计算代价是N^W.for N tags and W words, the cost is NW\n2. 主意（Idea）: 使用备忘录（Viterbi算法）（use memoization (the Viterbi Algorithm)）\n——结束于相同标记的序列可以压缩在一起，因为下一个标记仅依赖于此序列的当前标记（Sequences that end in the same tag can be collapsed together since the next tag depends only on the current tag of the sequence）\n图示如下：\nh) Viterbi 算法（The Viterbi Algorithm）\ni. 初始情况（Base case）:\npi delim{[}{0, START}{]} = log 1 = 0\npi delim{[}{0, t_{-1}}{]} = log 0 = infty\n对所有其他的t_{-1}(for all other t_{-1})\nii. 递归情况（Recursive case）:\n1. 对于i = 1…S.length及对于所有的t_{-1} in T:\npi delim{[}{i, t_{-1}}{]} = {max}under{t in T union START}{ pi delim{[}{i-1, t}{]} + log P(t_{-1}delim{|}{t}{}) + log P(S_i delim{|}{t_{-1}}{})}\n2. 回朔指针允许我们找出最大概率序列（Backpointers allow us to recover the max probability sequence）:\nBP delim{[}{i, t_{-1}}{]} = {argmax}under{t in T union START}{ pi delim{[}{i-1, t}{]} + log P(t_{-1}delim{|}{t}{}) + log P(S_i delim{|}{t_{-1}}{})}\ni) 性能（Performance）\ni. HMM标注器对于训练非常简单（HMM taggers are very simple to train）\nii. 表现相对很好（Perform relatively well） (over 90% performance on named entities)\niii. 最大的困难是对p(单词|标记)建模（Main difficulty is modeling of p(word|tag)）\n四、 结论（Conclusions）\na) 标注是一个相对比较简单的任务，至少在一个监督框架下对于英语来说（Tagging is relatively easy task (at least, in a supervised framework, and for English)）\nb) 影响标注器性能的因素包括（Factors that impact tagger performance include）:\ni. 训练集数量（The amount of training data available）\nii. 标记集（The tag set）\niii. 训练集和测试集的词汇差异（The difference in vocabulary between the training and the testing）\niv. 未登录词（Unknown words）\nc) TBL和HMM框架可用于其他自然语言处理任务（TBL and HMM framework can be used for other tasks）\n第四讲结束！\n附：课程及课件pdf下载MIT英文网页地址：\nhttp://people.csail.mit.edu/regina/6881/"}
{"content2":"核心提示：微软在 Office365、Azure 云、Dynamics365 上进行人工智能技术的部署，野心不小。 微软在2016年9月宣布组建自己的 AI 研究小组。该小组汇集了超过 5000 名计算机科学家和工程师，加上微软内部研究部门，将共同挖掘 AI 技术。 与此同时，亚马逊，Facebook，Google，IBM 还有微软联合宣\n而巨头们也纷纷拿出了自己的看家本领，Apple 的 Siri 利用自然语言处理来识别语音命令；Facebook 的深度学习面部识别算法能够快速准确地识别出人脸；Google 的「大脑」可能更为聪明，而且能够安装在数百万安卓系统的设备上。而隐藏在 CRM 领域的对手之一 Salesforce 也声称推出了一款全新的人工智能平台。还有 Amazon、Netflix 和 Spotify 都在强调使用机器学习了解如何与客户建立联系。\n最近的人工智能领域如此炙手可热，就连微软的首席执行官纳德拉也提到，「将 AI 覆盖到所有领域，是因为微软想要实现「AI 技术民主化」，从而解决全球最紧迫的挑战。」\n据悉，微软将从以下四个细分方向实现「AI 技术民主化」：\nAgents. Harness AI to fundamentally change human and computer interaction through agents such as Microsoft』s digital personal assistant Cortana\n1、助手。利用 AI 从根本上改变人机交互过程，如微软的数字个人助理 Cortana。\nApplications. Infuse every application, from the photo app on people』s phones to Skype and Office 365, with intelligence\n2、应用平台。从移动端的照片 app 到 Skype，再到 Office365，每一个应用平台融入了 AI 技术。\nServices. Make these same intelligent capabilities that are infused in Microsoft』s apps—cognitive capabilities such as vision and speech, and machine analytics—available to every application developer in the world\n3、服务。让全球每位应用开发者都能获取同样的 AI 技术支持。如微软 app 中视觉与语音识别能力，机器分析能力。\nInfrastructure. Build the world』s most powerful AI supercomputer with Azure and make it available to anyone, to enable people and organizations to harness its power\n4、基础架构。以 Azure 云为平台，构建强大的 AI 超级计算机，为企业和个人提供该项服务。\n那么，微软近期推出的几款人工智能产品看似「姗姗来迟」，但在 AI 领域的重量可不能小视，那么，微软在如何布局 AI 呢？其旗下几大产品又都与 AI 有着怎样的关系？\n推出人工智能版 Dynamics365\n提升销售转化率，更好地理解客户行为，是如今移动、社交、云三者融合时代的大背景下企业需要具备的重要能力。\n过去几周，CRM 领域频频传来人工智能利好的消息。如今，微软也向世人透露：在下月 1 日，将推出人工智能版 Dynamics365，为销售人员提供云端商务应用解决方案。\n该款产品将 ERP 与 CRM 进行整合，打造成为一个单一的云端解决方案。\n此次产品的推出主要针对的就是 CRM 领域占主导地位的 Salesforce，而此前不久，Salesforce 刚刚推出了一款名为「爱因斯坦」的人工智能云平台，能够为企业客户提供相应服务。\nMicrosoft has built in a couple of intelligence features into the release designed specifically for sales and service personnel. First, there is Customer Insights, a stand-alone cloud service, which enables users to bring in a variety of internal and external data sources. Companies can integrate all of this data with internal metrics (KPIs) to drive automated actions based on the data. The solution includes partner data from the likes of Facebook and Trip Advisor (proving you don』t need to own an external data source to take advantage of it).\n微软推出的 Dynamics365 能够为销售人员提供以下几类服务：一是客户洞察（Customer Insights），一款单机云服务，帮助用户收集各类内外部数据信息。企业能够将所有数据与内部指标（KPIs）进行集成，基于数据进行自动化行为的驱动。这套解决方案还包括了合作伙伴如 Facebook 和 Trip Advisor 上产生的大量数据，不过，这也说明了用户无需借外部获取数据。\nIt』s been designed as a stand-alone service that can work with any of the Dynamics 365 CRM components—sales, customer service or field service—and can also work with any external CRM tool with open APIs. This last point is particularly telling because it』s giving customers who might not be using Dynamics 365 (but are using other Microsoft tools like Outlook) access to this feature.\n一方面，单机服务能够与 Dynamics365 CRM 的组成部件进行很好地兼容，包括销售、客服或者现场服务；另一方面，还能与任何一个带有开放 API 接口的外部 CRM 工具进行兼容。此外，单机服务还能为并未使用 Dynamics365，而是微软其他工具如 Outlook 的客户，提供该项服务的渠道。\nThe second piece is called Relationship Insights, which as the name suggests gives sales people information about the status of their customer relationships at any given moment. It』s built on the on the Cortana Intelligence Suite, which Microsoft introduced in 2015 and uses tools like sentiment analysis to check on the likelihood of the deal closing and the next best action to take.\n二是关系洞察（Relationship Insights），为销售人员即时提供客户关系信息。该服务基于人工智能助手小娜的平台进行提供。这个智能助手于去年在全球推出，通过情感分析检测交易结束的可能性，并判断下一步最佳实践方案。\nFor now, know that Microsoft has consolidated its artificial intelligence tools into a single, coherent division and just about every vendor—not just those selling CRM—is trying to build some level of intelligence into its products. Dynamics 365 is just the latest manifestation.\n总的来说，微软已将人工智能工具嵌入一个单独连贯的区域，而且不只是 CRM 领域的供应商，基本上每个供应商都在试图将某种程度的智能技术嵌入到其产品中。"}
{"content2":"一、概念整体介绍\n人工智能（Artificial Intelligence）\n机器学习（Machine Learning）：一种实现人工智能的方法\n深度学习（Deep Learning）：一种实现机器学习的技术\n三者的关系图\n人工智能分类：\n强人工智能：强人工智能观点认为有可能制造出真正能推理（Reasoning）和解决问题（Problem_solving）的智能机器，并且这样的机器将被认为是有知觉的，有自我意识的。可以独立思考问题\n并制定解决问题的最优方案，有自己的价值观和世界观体系。有和生物一样的各种本能，比如生存和安全需求。\n弱人工智能：弱人工智能是指不能制造出真正地推理（Reasoning）和解决问题（Problem_solving）的智能机器，这些机器只不过看起来像是智能的，但是并不真正拥有智能，也不会有自主意识。\n人工智能是计算机学科的一个分支，二十世纪七十年代以来被称为世界三大尖端技术之一（空间技术、能源技术、人工智能）。\n也被认为是二十一世纪三大尖端技术（基因工程、纳米科学、人工智能）之一。\n这是因为近三十年来它获得了迅速的发展，在很多学科领域都获得了广泛应用，并取得了丰硕的成果，人工智能已逐步成为一个独立的分支，无论在理论和实践上都已自成一个系统。\n人工智能的研究分支\n人工智能的发展历程\n各种概念关系\n相关链接：\n一张图解释人工智能、机器学习、深度学习三者关系：https://baijiahao.baidu.com/s?id=1588563162916669654&wfr=spider&for=pc\n一篇文章讲清楚人工智能、机器学习和深度学习的区别和联系：https://www.cnblogs.com/bokeyuan-dlam/articles/7928135.html\n科普一下：机器学习和深度学习的区别和关系：http://www.elecfans.com/rengongzhineng/691751.html\n人工智能的三个分支：认知、机器学习、深度学习：https://blog.csdn.net/testcs_dn/article/details/81185750\n还纠结选机器学习还是深度学习？看完你就有数了：https://www.baidu.com/link?url=rVRgTtwZ11xkY1lcq4rRgilW9PwQEBXf737ESjE_H8RySv47Fwe-LyD69FJhFxeSqhQYPAL3kArqxR_nfWPSQAqRsrxsLaUqwm6EPUym6XK&wd=&eqid=edac1d7c00058498000000065cac9dc7\n=====================================================\n二、人工智能应用领域\n关键词：\n自然语言生成、语音识别、虚拟助理、机器学习平台、人工智能硬件优化、决策管理、深度学习平台、生物信息、\n图像识别、情绪识别、P2P网络、内容创作、网络防御、AI建模/数字孪生、机器处理自动化、文本分析和自然语言处理\n游戏 ：人工智能在国际象棋，扑克，围棋等游戏中起着至关重要的作用，机器可以根据启发式知识来思考大量可能的位置并计算出最优的下棋落子。\n自然语言处理 ： 可以与理解人类自然语言的计算机进行交互。比如常见机器翻译系统、人机对话系统。\n专家系统 ： 有一些应用程序集成了机器，软件和特殊信息，以传授推理和建议。它们为用户提供解释和建议。比如分析股票行情，进行量化交易。\n视觉系统 ： 它系统理解，解释计算机上的视觉输入。例如，间谍飞机拍摄照片，用于计算空间信息或区域地图。医生使用临床专家系统来诊断患者。警方使用的计算机软件可以识别数据库里面存储的肖像，从而识别犯罪者的脸部。还有我们最常用的车牌识别等。\n语音识别 ：智能系统能够与人类对话，通过句子及其含义来听取和理解人的语言。它可以处理不同的重音，俚语，背景噪音，不同人的的声调变化等。\n手写识别 ： 手写识别软件通过笔在屏幕上写的文本可以识别字母的形状并将其转换为可编辑的文本。\n智能机器人 ： 机器人能够执行人类给出的任务。它们具有传感器，检测到来自现实世界的光，热，温度，运动，声音，碰撞和压力等数据。他拥有高效的处理器，多个传感器和巨大的内存，以展示它的智能，并且能够从错误中吸取教训来适应新的环境。\n相关链接：http://www.qianjia.com/html/2018-08/23_302917.html\n=====================================================\n三、机器学习\n机器学习最基本的做法，是使用算法来解析数据、从中学习，然后对真实世界中的事件做出决策和预测。\n与传统的为解决特定任务、硬编码的软件程序不同，机器学习是用大量的数据来“训练”，通过各种算法从数据中学习如何完成任务。\n机器学习直接来源于早期的人工智能领域，传统的算法包括决策树、聚类、贝叶斯分类、支持向量机、EM、Adaboost等等。\n从学习方法上来分，机器学习算法可以分为监督学习（如分类问题）、无监督学习（如聚类问题）、半监督学习、集成学习、深度学习和强化学习。\n机器学习过程使用以下步骤进行定义：\n1. 确定相关数据集并准备进行分析。\n2. 选择要使用的算法类型。\n3. 根据所使用的算法构建分析模型。\n4. 立足测试数据集进行模型训练，并根据需要进行模型修改。\n5. 运行模型以生成测试评分。\n传统的机器学习算法在指纹识别、基于Haar的人脸检测、基于HoG特征的物体检测等领域的应用基本达到了商业化的要求或者特定场景的商业化水平，但每前进一步都异常艰难，直到深度学习算法的出现。\n=====================================================\n四、深度学习\n深度学习属于机器学习的一个子域，其相关算法受到大脑结构与功能（即人工神经网络）的启发。\n深度学习如今的全部价值皆通过监督式学习或经过标记的数据及算法实现。\n深度学习中的每种算法皆经过相同的学习过程。\n深度学习包含输入内容的非近线变换层级结构，可用于创建统计模型并输出对应结果。\n机器学习与深度学习间的区别：\n1、数据量：机器学习能够适应各种数据量，特别是数据量较小的场景。在另一方面，如果数据量迅速增加，那么深度学习的效果将更为突出。下图展示了不同数据量下机器学习与深度学习的效能水平。\n2、硬件依赖性：与传统机器学习算法相反，深度学习算法在设计上高度依赖于高端设备。深度学习算法需要执行大量矩阵乘法运算，因此需要充足的硬件资源作为支持。\n3、特征工程：特征工程是将特定领域知识放入指定特征的过程，旨在减少数据复杂性水平并生成可用于学习算法的模式。  示例：传统的机器学习模式专注于特征工程中所需要找像素及其他属性。\n深度学习算法则专注于数据的其他高级特征，因此能够降低处理每个新问题时特征提取器的实际工作量。\n4、问题解决方法：传统机器学习算法遵循标准程序以解决问题。它将问题拆分成数个部分，对其进行分别解决，而后再将结果结合起来以获得所需的答案。深度学习则以集中方式解决问题，而无需进行问题拆分。\n5、执行时间：执行时间是指训练算法所需要的时间量。深度学习需要大量时间进行训练，因为其中包含更多参数，因此训练的时间投入也更为可观。相对而言，机器学习算法的执行时间则相对较短。\n6、可解释性：可解释性是机器学习与深度学习算法间的主要区别之一——深度学习算法往往不具备可解释性。也正因为如此，业界在使用深度学习之前总会再三考量。\n机器学习与深度学习的实际应用：\n1. 通过指纹实现出勤打卡、人脸识别或者通过扫描车牌识别牌照号码的计算机视觉技术。\n2. 搜索引擎中的信息检索功能，例如文本搜索与图像搜索。\n3. 自动电子邮件营销与特定目标识别。\n4. 癌症肿瘤医学诊断或其他慢性疾病异常状态识别。\n5. 自然语言处理应用程序，例如照片标记。Facebook就提供此类功能以提升用户体验。\n6. 在线广告。\n未来发展趋势：\n1. 随着业界越来越多地使用数据科学与机器学习技术，对各个组织而言，最重要的是将机器学习方案引入其现有业务流程。\n2. 深度学习的重要程度正逐步超越机器学习。事实已经证明，深度学习是目前最先进且实际效能最出色的技术方案之一。\n3. 机器学习与深度学习将在研究与学术领域证明自身蕴藏的巨大能量。"}
{"content2":"Journals\nACM Transactions on Information Systems (TOIS) 影响因子 5.059(2006)\nIEEE Transactions on Knowledge and Data Engineering(TKDE),影响因子：2.063\nInformation Retrieval (KLUWER ACADEMIC PUBL),影响因子: 1.744 (2006)\nInformation Processing & Management (IP&M) (Elsevier), 影响因子：1.546 (2006)\nACM Transactions on Asian Language Information Processing (TALIP)\nACM Transactions on Knowledge Discovery from Data (TKDD)\nJCST\n计算机研究与发展\n计算机学报\n软件学报\n《中国科学》\n《Journal of Chinese Language and Computing》（中文与东方语言信息处理学会学报，新加坡）\n《Transactions on Asian Language Information Processing》 (TALIP,ACM,香港)\n《Computational Linguistics and Chinese Language Processing》（中文计算语言学，台湾）\nSCI检索（ISI主页）\nACL Anthology\nACM SIGIR\n计算机学报\n软件学报\n计算机研究与发展\n中文信息学报\n情报学报\nConferences\nIR: SIGIR/WWW/TREC/CIKM\nDM: VLDB/SIGMOD/ICDE\nML: ICML/ICDM/SIGKDD\nNLP: ACL/Coling/EMNLP\n1  ICCV: IEEE International Conference on Computer Vision  领域顶级国际会议，录取率20%左右，2年一次         计算机视觉，模式识别，多媒体计算\n2  CVPR: IEEE Conf on Comp Vision and Pattern Recognition      领域顶级国际会议，录取率25%左右，每年一次        模式识别，计算机视觉，多媒体计算\n3  ECCV: European Conference on Computer Vision      领域顶级国际会议，录取率25%左右，2年一次         模式识别，计算机视觉，多媒体计算\n4  ICML: International Conference on Machine Learning 领域顶级国际会议，录取率25%左右，2年一次         机器学习，模式识别\n5  NIPS: Neural Information Processing Systems      领域顶级国际会议，录取率20%左右，每年一次        神经计算，机器学习\n6  ACL: The Association for Computational Linguistics    国际计算语言学会年会，每年举办一次       计算语言学，自然语言处理\n7  COLING: International Conference on Computational Linguistics         计算语言学会议，两年一次        计算语言学，自然语言处理\n8  IJCNLP: International Joint Conference on Natural Language Processing 自然语言处理亚洲联盟主办的国际会议，是自然语言处理领域亚洲区域最有影响的学术会议，基本是每年举办一次   自然语言处理\n9  ACM SIGIR: The ACM Conference on Research and Development in Information Retrieval   信息检索方面最好的会议, ACM 主办, 每年开。19％左右       信息检索技术\n10         ACM SIGKDD: The ACM Conference on Knowledge Discovery in Databases and Data Mining    数据挖掘方面最好的会议, ACM 主办, 每年开。18％左右\n11         WWW: The ACM International World Wide Web Conference       应用和媒体领域顶级国际会议    万维网\n12         ACM SIGMOD: ACM SIGMOD Conf on Management of Data    数据库领域顶级国际 数据管理\n13         CIKM: The ACM Conference on Information and Knowledge Management       数据库领域知名国际会议   数据管理\n14         IEEE ICDM: International Conference on Data Mining          数据挖掘领域顶级国际会议\n15         IJCAI: International Joint Conference on Artificial Intelligence     人工智能领域顶级国际会议，论文接受率18％左右    人工智能\n16         VLDB: The ACM International Conference on Very Large Data Bases      数据库领域顶级国际 数据库\n17         AAAI: American Association for Artificial Intelligence   美国人工智能学会AAAI的年会，使该领域的顶级会议        人工智能\n18         CPM: Combinatorial Pattern Matching Symposium      组合模式匹配年会，是字符串匹配、模式匹配较好的会议。        模式匹配\n19     IEEE SPIRE: The IEEE International Symposium on String Processing and Information Retrieval    字符串处理与信息检索的年会，字符串匹配的主要进展都可以在这个会议上找到。    字符串处理信息检索\nBooks\nIntroduction to Information Retrieval. C.D. Manning, P. Raghavan, H. Schütze. Cambridge UP, 2007. Draft. NLP大拿Manning最新出的一本书，中文版已经上市，内容比较新，有语言模型、机器学习及XML检索，有很多实现方面的内容。\nInformation Retrieval: Algorithms and Heuristics. D.A. Grossman, O. Frieder. Springer, 2004. 非常好的教材, 例子非常多，在组织上和传统的教材不太一样，关注Ad Hoc 检索。据说在Amazon上卖的很好。\nModern Information Retrieval. R. Baeza-Yates, B. Ribeiro-Neto. Addison-Wesley, 1999. 用的较广泛的教材，非常全，但是感觉有点过时，实现方面的内容也有所欠缺。\nReadings in Information Retrieval. K. Sparck Jones, P. Willett. Morgan Kaufmann, 1997. 许多经典论文的珍藏本。\nManaging Gigabytes. I.H. Witten, A. Moffat, T.C. Bell. Morgan Kaufmann, 1999.    关于文本和图像处理中的索引及压缩的数据结构及算法\nInformation Retrieval: Data Structures and Algorithms(2nd edition), William B. Frakes and Ricardo Baeza-Yates, Prentice Hall PTR, 1992. 也是从数据结构和算法方面介绍IR的书，偏实现，但是确实有些内容过时了。\nMining the Web: Analysis of Hypertext and Semi Structured Data. S. Chakrabarti. Morgan Kaufmann, 2002. 有关WEB IR的书，感觉公式有点多\nFinding Out About: A Cognitive Perspective on Search Engine Technology and the WWW. R. Belew. Cambridge UP, 2001.\nGoogle's PageRank and Beyond: The Science of Search Engine Rankings, Amy N. Langville and Carl D. Meyer, 有关PageRank原理及实现的一本全集。\nWeb Data Mining: Exploring Hyperlinks, Contents, and Usage Data (Data-Centric Systems and Applications), Bing Liu, Springer, 2006. 只看了看目录，觉得挺全挺新的。中文翻译版已经面世。\nLucene in Action, Otis Gospodnetic and Erik Hatcher. Manning Publications. 2004. 不用说了，著名开源索引检索工具Lucene的大全。\nProgramming Collective Intelligence: Building Smart Web 2.0 Applications, Toby Segaran, O'Reilly Media, Inc. 2007.这本书给了很多在真实环境下用Python语言写IR/Machine Learning算法的例子。值得一读。网上有电子版。\nTeams\n北大计算语言学研究所\n中科院计算所自然语言处理课题组\n中科院计算所信息检索课题组\n知网（hownet）\n东北大学中文信息处理实验室\n纳讯--中文信息处理技术站点\n北邮模式识别与智能系统试验室\n自然语言处理机构站点收集\n中文自然语言处理开放平台\n微软研究院\nComputists International AI Association\n太平洋邻里协会（PNC,Pacific Neighborhood Consortium）\nComputational Linguistics Tools\nBow: A Toolkit for Statistical Language Modeling, Text Retrieval, Classification and Clustering\nGate -- General Architecture for Text Engineering\nUMass\nCMU\nUIUC\nGlasgow University\nUniversity of Montreal\nMicrosoft Research Cambridge\nMicrosoft Research Asia\nHIT\nTSinghua\nPKU\nACL Anthology\nACL Anthology Network\nACL Wiki\nCLSP\n<a< div=\"\">\nTAG: 导航 网址 信息检索 自然语言处理"}
{"content2":"2006年8月23日 下午 11:22:00\n发表者：吴军，Google 研究员\n我 在数学之美系列中一直强调的一个好方法就是简单。但是，事实上，自然语言处理中也有一些特例，比如有些学者将一个问题研究到极致，执著追求完善甚至可以说 完美的程度。他们的工作对同行有很大的参考价值，因此我们在科研中很需要这样的学者。在自然语言处理方面新一代的顶级人物麦克尔 · 柯林斯 (Michael Collins) 就是这样的人。\n柯林斯：追求完美\n柯 林斯从师于自然语言处理大师马库斯 (Mitch Marcus)（我们以后还会多次提到马库斯），从宾夕法利亚大学获得博士学位，现任麻省理工学院 (MIT) 副教授（别看他是副教授，他的水平在当今自然语言处理领域是数一数二的），在作博士期间，柯林斯写了一个后来以他名字命名的自然语言文法分析器 (sentence parser)，可以将书面语的每一句话准确地进行文法分析。文法分析是很多自然语言应用的基础。虽然柯林斯的师兄布莱尔 (Eric Brill) 和 Ratnaparkhi 以及师弟 Eisnar 都完成了相当不错的语言文法分析器，但是柯林斯却将它做到了极致，使它在相当长一段时间内成为世界上最好的文法分析器。柯林斯成功的关键在于将文法分析的 每一个细节都研究得很仔细。柯林斯用的数学模型也很漂亮，整个工作可以用完美来形容。我曾因为研究的需要，找柯林斯要过他文法分析器的源程序，他很爽快地 给了我。我试图将他的程序修改一下来满足我特定应用的要求，但后来发现，他的程序细节太多以至于很难进一步优化。柯林斯的博士论文堪称是自然语言处理领域的范文。它像一本优秀的小说，把所有事情的来龙去脉介绍的清清楚楚，对于任何有一点计算机和自然语言处理知识的人，都可以轻而易举地读懂他复杂的方法。\n柯 林斯毕业后，在 AT&T 实验室度过了三年快乐的时光。在那里柯林斯完成了许多世界一流的研究工作诸如隐含马尔科夫模型的区别性训练方法，卷积核在自然语言处理中的应用等等。三年 后，AT&T 停止了自然语言处理方面的研究，柯林斯幸运地在 MIT 找到了教职。在 MIT 的短短几年间，柯林斯多次在国际会议上获得最佳论文奖。相比其他同行，这种成就是独一无二的。柯林斯的特点就是把事情做到极致。如果说有人喜欢\"繁琐哲学 \"，柯林斯就是一个。\n布莱尔：简单才美\n在研究方法上，站在柯林斯对立面的典型是他的师兄艾里克 · 布莱尔 (Eric Brill) 和雅让斯基，后者我们已经介绍过了，这里就不再重复。与柯林斯从工业界到学术界相反，布莱尔职业路径是从学术界走到工业界。与柯里斯的研究方法相反，布莱 尔总是试图寻找简单得不能再简单的方法。布莱尔的成名作是基于变换规则的机器学习方法 (transformation rule based machine learning)。这个方法名称虽然很复杂，其实非常简单。我们以拼音转换字为例来说明它：\n第一步，我们把每个拼音对应的汉字中最常见的找出来作为第一遍变换的结果，当然结果有不少错误。比如，\"常识\"可能被转换成\"长识\"；\n第二步，可以说是\"去伪存真\"，我们用计算机根据上下文，列举所有的同音字替换的规则，比如，如果 chang 被标识成\"长\"，但是后面的汉字是\"识\"，则将\"长\"改成\"常\"；\n第三步，应该就是\"去粗取精\"，将所有的规则用到事先标识好的语料中，挑出有用的，删掉无用的。然后重复二三步，直到找不到有用的为止。\n布 莱尔就靠这么简单的方法，在很多自然语言研究领域，得到了几乎最好的结果。由于他的方法再简单不过了，许许多多的人都跟着学。布莱尔可以算是我在美国的第 一个业师，我们俩就用这么简单的方法作词性标注 (part of speech tagging)，也就是把句子中的词标成名词动词，很多年内无人能超越。（最后超越我们的是后来加入 Google 的一名荷兰工程师，用的是同样的方法，但是做得细致很多）布莱尔离开学术界后去了微软研究院。在那里的第一年，他一人一年完成的工作比组里其他所有人许多 年做的工作的总和还多。后来，布莱尔又加入了一个新的组，依然是高产科学家。据说，他的工作真正被微软重视要感谢 Google，因为有了 Google，微软才对他从人力物力上给于了巨大的支持，使得布莱尔成为微软搜索研究的领军人物之一。在研究方面，布莱尔有时不一定能马上找到应该怎么 做，但是能马上否定掉一种不可能的方案。这和他追求简单的研究方法有关，他能在短时间内大致摸清每种方法的好坏。\n由于布莱尔总是找简单有 效的方法，而又从不隐瞒自己的方法，所以他总是很容易被包括作者我自己在内的很多人赶上和超过。好在布莱尔很喜欢别人追赶他，因为，当人们在一个研究方向 超过他时，他已经调转船头驶向它方了。一次，艾里克对我说，有一件事我永远追不上他，那就是他比我先有了第二个孩子 ：）\n在接下来了系列里，我们还会介绍一个繁与简结合的例子。"}
{"content2":"果然不愧是百度，算法岗位的题目在广度和深度方面都超过了我之前做的几家，在这里放出部分我记得的笔试题，如有侵权，请联系作者删除\n题目类型：30选择+1道问答+1道系统设计+2道编程，题量很大\n一、选择题\n选择题面非常宽，不建议花费太多时间，涉及到的知识点有C++、python、数据库、分页式存储管理、机器学习、自然语言处理、数据结构、操作系统，基本是应有尽有，估计这么设计就是为了让大家快点蒙完，然后做后面的题目\n二、问答题\n问答题问了自然语言处理的相关知识\n什么是统计语言模型？\n统计语言模型中的ngram模型是什么概念？相关的公式推导是？\n如何估计ngram模型中的参数\n答案可以参考 https://www.cnblogs.com/yehui-mmd/p/8082877.html\nhttps://blog.csdn.net/ed_new/article/details/51924535\n三、系统设计题\n要求设计一个推荐系统，我个人认为推荐系统还算是一个比较常考的问题，建议笔试之前认真准备一下，否则很难回答上来。\n四、编程题\n（1）连分数比大小\n连分数可以表示为（a0；a1,a2,……，an）,这样直观的数学表示为：\n输入\nn，之后是n+1个数字,分别表示a0,a1,……,an\nm，之后是m+1个数字,分别表示b0,b1,……，bn\n要求比较以上两个连分数（分别记为x和y）的大小，n和m不超过100000\n输出\n若x>y,输出“>”，若x<y，输出“<”,否则输出“=”\n分析：这盗图并不是很难，但是由于我最后时间不够，导致写的代码没时间上传编译，我自己测试了几组数据，问题不大。我的思路是先比较两个数字是否相等，之后把连分数化为小数进行比较。使用的数据结构为vector，代码如下：\n1 #include<iostream> 2 #include <vector> 3 #include <algorithm> 4 using namespace std; 5 int main() 6 { 7 vector<int> x(100100); 8 vector<int> y(100100); 9 int numX, numY; 10 //double resX = 0.0,resY = 0.0; 11 cin >> numX; 12 for (int i = 0; i < numX + 1; i++) 13 cin >> x[i]; 14 cin >> numY; 15 for (int i = 0; i < numY + 1; i++) 16 cin >> y[i]; 17 if (x == y) 18 cout << '=' << endl; 19 else 20 { 21 double tempX = x[numX]; 22 for (int i = numX-1; i >= 0; i--) 23 { 24 tempX = x[i] + 1.0 / tempX; 25 //resX += 1.0 / x[i] + x[i - 1]; 26 } 27 double tempY = y[numY]; 28 for (int i = numY - 1; i >= 0; i--) 29 tempY = y[i] + 1.0 / tempY; 30 31 //cout << tempX << endl;// << \" \" << tempY << endl; 32 if (tempX > tempY) 33 cout << '>'; 34 else 35 cout << '<'; 36 // cout << temp << endl; 37 } 38 return 0; 39 }\n（2）区域划分"}
{"content2":"认知计算代表一种全新的计算模式，它包含信息分析，自然语言处理和机器学习领域的大量创新技术。\nCognnitive computing refers to systems that learn at scale, reason with purpose and interact with humans nautally.\n认知计算是一种学习系统，有规模、有原因、有目的地学会和人类交互。\n认知计算主要分为三个阶段：\n打孔机时代-> 编程时代 -> 认知时代\n代表性的项目：\nIBM watson (2006)，Google大脑(2011)，百度大脑(2011)\n代表人物：\n吴恩达\n和人工智能（Artificial Intelligence）比较：\n人工智能：\nArtificial, Explicity Programmed, Deterministic, Human not involved, Measured by Turing Test or minic humans, 会给出一个确切的结果，是或否。\n认知计算：\nCognitive, Learn and reason, Interactions with human and enviornments, Measured in more practical way, 会通过学习给出一个概率。\n适合的领域：\nLarge Scale, Complex, Interactive, Unstructured data, Probability.\n生命科学，金融，教育，政府，商业智能BI，交通。\n包含的知识：\nNatural Language Processing 自然语言处理\nQuestion Answering Technolog 问答技术\nHigh Performance Compting 高性能计算\nKnowledge Representation and Reasoning 知识表达和理解\nMachine Learning 机器学习\nUnstructured Information Management 非结构化信息管理\n和生物学关联：\n可视化、心理学、神经网络、深度学习"}
{"content2":"转自：http://www.52nlp.cn/resources\n这里提供一些52nlp博客的一些系列文章以及收集的自然语言处理相关书籍及其他资源的下载，陆续整理中!如有不妥，我会做删除处理！\n特别推荐系列：\n1、HMM学习最佳范例全文文档，百度网盘链接: http://pan.baidu.com/s/1pJoMA2B 密码: f7az\n2、无约束最优化全文文档 -by @朱鉴 ，百度网盘链接：链接: http://pan.baidu.com/s/1hqEJtT6 密码: qng0\n3、PYTHON自然语言处理中文翻译-NLTK Natural Language Processing with Python 中文版，陈涛sean 无偿翻译。链接: http://pan.baidu.com/s/1i3DvwFV 密码: oxne\n4、正态分布的前世今生(pdf 版) – by @rickjin\n5、LDA-math-汇总 LDA数学八卦 – by @rickjin\n6、如何计算两个文档的相似度全文文档\n7、中文分词入门之字标注法全文文档\n一、书籍：\n1、《自然语言处理综论》英文版第二版\n2、《统计自然语言处理基础》英文版\n3、《用Python进行自然语言处理》，NLTK配套书\n4、《Learning Python第三版》，Python入门经典书籍，详细而不厌其烦\n5、《自然语言处理中的模式识别》\n6、《EM算法及其扩展》\n7、《统计学习基础》\n8、《自然语言理解》英文版（似乎只有前9章）\n9、《Fundamentals of Speech Recognition》，质量不太好，不过第6章关于HMM的部分比较详细，作者之一便是Lawrence Rabiner；\n10、概率统计经典入门书：《概率论及其应用》（英文版，威廉*费勒著）\n第一卷　　第二卷　　DjVuLibre阅读器（阅读前两卷书需要）\n11、一本利用Perl和Prolog进行自然语言处理的介绍书籍：《An Introduction to Language Processing with Perl and Prolog》\n12、国外机器学习书籍之：\n1) “Programming Collective Intelligence“，中文译名《集体智慧编程》，机器学习&数据挖掘领域”近年出的入门好书，培养兴趣是最重要的一环，一上来看大部头很容易被吓走的”\n2) “Machine Learning“,机器学习领域无可争议的经典书籍，下载完毕将后缀改为pdf即可。豆瓣评论 by 王宁）：老书，牛人。现在看来内容并不算深，很多章节有点到为止的感觉，但是很适合新手（当然，不能”新”到连算法和概率都不知道）入门。比如决策树部分就很精彩，并且这几年没有特别大的进展，所以并不过时。另外，这本书算是对97年前数十年机器学习工作的大综述，参考文献列表极有价值。国内有翻译和影印版，不知道绝版否。\n3) “Introduction to Machine Learning”\n13、国外数据挖掘书籍之：\n1) “Data.Mining.Concepts.and.Techniques.2nd“，数据挖掘经典书籍 作者 : Jiawei Han/Micheline Kamber 出版社 : Morgan Kaufmann 评语 : 华裔科学家写的书，相当深入浅出。\n2) Data Mining:Practical Machine Learning Tools and Techniques\n3) Beautiful Data: The Stories Behind Elegant Data Solutions（ Toby Segaran, Jeff Hammerbacher）\n14、国外模式识别书籍之：\n1）“Pattern Recognition”\n2）“Pattern Recongnition Technologies and Applications”\n3）“An Introduction to Pattern Recognition”\n4）“Introduction to Statistical Pattern Recognition”\n5）“Statistical Pattern Recognition 2nd Edition”\n6）“Supervised and Unsupervised Pattern Recognition”\n7）“Support Vector Machines for Pattern Classification”\n15、国外人工智能书籍之：\n1）Artificial Intelligence: A Modern Approach (2nd Edition) 人工智能领域无争议的经典。\n2）“Paradigms of Artificial Intelligence Programming: Case Studies in Common LISP”\n16、其他相关书籍：\n1）Programming the Semantic Web，Toby Segaran , Colin Evans, Jamie Taylor\n2）Learning.Python第四版，英文\n二、课件：\n1、哈工大刘挺老师的“统计自然语言处理”课件；\n2、哈工大刘秉权老师的“自然语言处理”课件；\n3、中科院计算所刘群老师的“计算语言学讲义“课件；\n4、中科院自动化所宗成庆老师的“自然语言理解”课件；\n5、北大常宝宝老师的“计算语言学”课件；\n6、北大詹卫东老师的“中文信息处理基础”的课件及相关代码；\n7、MIT Regina Barzilay教授的“自然语言处理”课件，52nlp上翻译了前5章；\n8、MIT大牛Michael Collins的“Machine Learning Approaches for Natural Language Processing(面向自然语言处理的机器学习方法)”课件；\n9、Michael Collins的“Machine Learning （机器学习）”课件；\n10、SMT牛人Philipp Koehn “Advanced Natural Language Processing（高级自然语言处理）”课件；\n11、Philipp Koehn “Empirical Methods in Natural Language Processing”课件；\n12、Philipp Koehn“Machine Translation（机器翻译）”课件；\n三、语言资源和开源工具：\n1、Brown语料库：\na) XML格式的brown语料库，带词性标注；\nb) 普通文本格式的brown语料库，带词性标注；\nc) 合并并去除空行、行首空格，用于词性标注训练：browntest.zip\n2、NLTK官方提供的语料库资源列表\n3、OpenNLP上的开源自然语言处理工具列表\n4、斯坦福大学自然语言处理组维护的“统计自然语言处理及基于语料库的计算语言学资源列表”\n5、LDC上免费的中文信息处理资源\n6、中文分词相关工具：\n1）Java版本的MMSEG：mmseg-v0.3.zip，作者为solol，详情可参见：《中文分词入门之篇外》\n2）张华平老师的ICTCLAS2010，该版本非商用免费一年，下载地址：\nhttp://cid-51de2738d3ea0fdd.skydrive.live.com/self.aspx/.Public/ICTCLAS2010-packet-release.rar\n7、热心读者“finallyliuyu”提供的一批新闻语料库，包括腾讯，新浪，网易，凤凰等，目前放在CSDN上：http://finallyliuyu.download.csdn.net/\n另外finalllyliuyu在2010年9月又提供了一批文本文类语料，详情见：献给热衷于自然语言处理的业余爱好者的中文新闻分类语料库之二\n四、文献：\n1、ACL-IJCNLP 2009论文全集：\na) 大会论文Full Paper第一卷\nb) 大会论文Full Paper第二卷\nc) 大会论文Short Paper合集\nd) ACL09之EMNLP-2009合集\ne) ACL09 所有workshop论文合集"}
{"content2":"SnowNLP是一个python写的类库，可以方便的处理中文文本内容，是受到了TextBlob的启发而写的，由于现在大部分的自然语言处理库基本都是针对英文的，于是写了一个方便处理中文的类库，并且和TextBlob不同的是，这里没有用NLTK，所有的算法都是自己实现的，并且自带了一些训练好的字典。注意本程序都是处理的unicode编码，所以使用时请自行decode成unicode。\nfrom snownlp import SnowNLP s = SnowNLP(u'这个东西真心很赞') s.words # [u'这个', u'东西', u'真心', # u'很', u'赞'] s.tags # [(u'这个', u'r'), (u'东西', u'n'), # (u'真心', u'd'), (u'很', u'd'), # (u'赞', u'Vg')] s.sentiments # 0.9769663402895832 positive的概率 s.pinyin # [u'zhe', u'ge', u'dong', u'xi', # u'zhen', u'xin', u'hen', u'zan'] s = SnowNLP(u'「繁體字」「繁體中文」的叫法在臺灣亦很常見。') s.han # u'「繁体字」「繁体中文」的叫法 # 在台湾亦很常见。' text = u''' 自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。 它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。 自然语言处理是一门融语言学、计算机科学、数学于一体的科学。 因此，这一领域的研究将涉及自然语言，即人们日常使用的语言， 所以它与语言学的研究有着密切的联系，但又有重要的区别。 自然语言处理并不是一般地研究自然语言， 而在于研制能有效地实现自然语言通信的计算机系统， 特别是其中的软件系统。因而它是计算机科学的一部分。 ''' s = SnowNLP(text) s.keywords(3) # [u'语言', u'自然', u'计算机'] s.summary(3) # [u'因而它是计算机科学的一部分', # u'自然语言处理是一门融语言学、计算机科学、 # 数学于一体的科学', # u'自然语言处理是计算机科学领域与人工智能 # 领域中的一个重要方向'] s.sentences s = SnowNLP([[u'这篇', u'文章'], [u'那篇', u'论文'], [u'这个']]) s.tf s.idf s.sim([u'文章'])# [0.3756070762985226, 0, 0]\nFeatures\n中文分词（Character-Based Generative Model）\n词性标注（TnT 3-gram 隐马）\n情感分析（现在训练数据主要是买卖东西时的评价，所以对其他的一些可能效果不是很好，待解决）\n文本分类（Naive Bayes）\n转换成拼音（Trie树实现的最大匹配）\n繁体转简体（Trie树实现的最大匹配）\n提取文本关键词（TextRank算法）\n提取文本摘要（TextRank算法）\ntf，idf\nTokenization（分割成句子）\n文本相似（BM25）\n支持python3（感谢erning）\nGet It now\n$ pip install snownlp\n关于训练\n现在提供训练的包括分词，词性标注，情感分析，而且都提供了我用来训练的原始文件 以分词为例 分词在snownlp/seg目录下\nfrom snownlp import seg seg.train('data.txt') seg.save('seg.marshal') #from snownlp import tag #tag.train('199801.txt') #tag.save('tag.marshal') #from snownlp import sentiment #sentiment.train('neg.txt', 'pos.txt') #sentiment.save('sentiment.marshal')\n这样训练好的文件就存储为seg.marshal了，之后修改snownlp/seg/__init__.py里的data_path指向刚训练好的文件即可"}
{"content2":"1、分词的简单方法：\n在空格字符处分割文本是文本分词最简单的方法。考虑一下摘自《爱丽丝梦游仙境》中的文本。\n>>> raw = \"\"\"'When I'M a Duchess,' she said to herself, (not in a very hopeful tone ... though), 'I won't have any pepper in my kitchen AT ALL. Soup does very ... well without--Maybe it's always pepper that makes people hot-tempered,'...\"\"\"\n可以使用raw.split()在空格符处分割原始文本。使用正则表达式能做同样的事情，匹配字符串中的所有空白符是远远不够的，因为这会导致结果中包含'\\n'换行符。需要同时匹配任何数量的空格符、制表符或者换行符。\n>>>import re >>>re.split(r' ',raw) [\"'When\", \"I'M\", 'a', \"Duchess,'\", 'she', 'said', 'to', 'herself,', '(not', 'in', 'a', 'very', 'hopeful', 'tone\\nthough),', \"'I\", \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 'very\\nwell', 'without--Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', \"hot-tempered,'...\"] >>>re.split(r'[ \\t\\n]+',raw) [\"'When\", \"I'M\", 'a', \"Duchess,'\", 'she', 'said', 'to', 'herself,', '(not', 'in', 'a', 'very', 'hopeful', 'tone', 'though),', \"'I\", \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 'very', 'well', 'without--Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', \"hot-tempered,'...\"]\n正则表达式\"[ \\t\\n]+\"匹配一个多个空格、制表符或者换行符。其他的空白字符如回车和换行符也应该包含在内。于是re库内置了缩写'\\s'，表示匹配所有的空白字符。所有前面这个例子第二条语句可以改写为re.split(r'\\s',raw)\n在空格符出分割文本可得到如\"(not\"和\"herself,\"这样的标识符。另一种方法是使用Python提供给我们的字符类\"\\w\"匹配所有的字符，相当于[0-9a-zA-Z].定义这个类的补充部分\"\\W\"即所有的字母，数字以外的字符。还可以在一个简单的正则表达式中用\\W来分割所有单词以外的输入。\n>>> re.split(r'\\W+', raw) ['', 'When', 'I', 'M', 'a', 'Duchess', 'she', 'said', 'to', 'herself', 'not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', 'I', 'won', 't', 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', 'Soup', 'does', 'very', 'well', 'without', 'Maybe', 'it', 's', 'always', 'pepper', 'that', 'makes', 'people', 'hot', 'tempered', '']\n可以看到，在开始和结尾处都有一个空字符串，通过re.findall(r'\\w+',raw)使用模式匹配词汇而不是空白符号，得到相同的标识符。但是没有空字符串。\n正则表达式\"\\w+|\\S\\w*\"会首先尝试匹配词中字符的所有序列，如果没有找到匹配的，会尝试匹配后面跟着词中字符的任何非空白符。这意味着标点会很跟在后面的字母如’s在一起，但是两个或两个以上标点字符会被分割。\n>>> re.findall(r'\\w+|\\S\\w*', raw) [\"'When\", 'I', \"'M\", 'a', 'Duchess', ',', \"'\", 'she', 'said', 'to', 'herself', ',', '(not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', ')', ',', \"'I\", 'won', \"'t\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', '.', 'Soup', 'does', 'very', 'well', 'without', '-', '-Maybe', 'it', \"'s\", 'always', 'pepper', 'that', 'makes', 'people', 'hot', '-tempered', ',', \"'\", '.', '.', '.']\n扩展前面正则表达式中的“\\w+”，允许连字符和撇号“\\w+(-')\\w+”,他会匹配如hot-dog和it's这样的单词。还需要添加一个模式来匹配引号字符使他们与他们包含的文字分开。\n>>> print(re.findall(r\"\\w+(?:[-']\\w+)*|'|[-.(]+|\\S\\w*\", raw)) [\"'\", 'When', \"I'M\", 'a', 'Duchess', ',', \"'\", 'she', 'said', 'to', 'herself', ',', '(', 'not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', ')', ',', \"'\", 'I', \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', '.', 'Soup', 'does', 'very', 'well', 'without', '--', 'Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', 'hot-tempered', ',', \"'\", '...']\nNLTK的正则表达式分词器\n函数nltk.regexp_tokenize()和re.findall()类型，但是nltk.regexp_tokenize()分词效率更高，避免了括号的特殊处理的需要。为了增加可读性，将正则表达式分为几行写，每一行添加一个解释。（？x）‘Verbose’标志告诉Python去掉嵌入的注释和空格\n>>> text = 'That U.S.A. poster-print costs $12.40...' >>> pattern = r'''(?x) # set flag to allow verbose regexps ... ([A-Z]\\.)+ # abbreviations, e.g. U.S.A. ... | \\w+(-\\w+)* # words with optional internal hyphens ... | \\$?\\d+(\\.\\d+)?%? # currency and percentages, e.g. $12.40, 82% ... | \\.\\.\\. # ellipsis ... | [][.,;\"'?():-_`] # these are separate tokens; includes ], [ ... ''' >>> nltk.regexp_tokenize(text, pattern) ['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']\n使用verbose标志时，可以不使用' '来匹配空格字符，而是使用‘\\s’代替。regexp_tokenize()有一个可选参数gaps。设置为True时，正则表达式指定标识符之间的距离。就行re.split()一样。"}
{"content2":"深度学习实战篇-基于RNN的中文分词探索\n近年来，深度学习在人工智能的多个领域取得了显著成绩。微软使用的152层深度神经网络在ImageNet的比赛上斩获多项第一，同时在图像识别中超过了人类的识别水平。百度在中文语音识别上取得了97%的准确率，已经超过了人类的识别能力。\n随着深度学习在越来越多的领域中取得了突破性进展，自然语言处理这一人工智能的重要领域吸引了大批的研究者的注意力。最近谷歌发布了基于深度学习的机器翻译(GNMT)，和基于短语的机器翻译相比，错误率降低了55%-85%以上，从而又引发了深度学习在自然语言处理上的热潮。\n自然语言处理是人工智能和语言学的交叉学科，在搜索引擎，问答系统，机器翻译等人工智能领域发挥着重要作用。分词，词性标注，命名实体识别作为自然语言处理的基础领域，同样吸引着大批研究者的注意力，本篇就结合我们近期的一些探索从中文分词角度探索深度学习在自然语言处理中的应用。\n中文分词是将自然语言文本划分成词语序列，目前主流方法为序列标注，即用BMES这个四个标签去标注句子中的每一个字(B是词首，M是词中，E是词尾，S是单字词)。\n对于 { 京东 搜索 与 大数据 平台 数据挖掘 算法部 }\n其标注为{ BE BE S BME BE BMME BME }\n使用Keras实现了基于RNN的中文分词，接下来就分别介绍一下Keras和中文分词实战。\n1. Keras介绍\nKeras 是一个高度模块化的深度学习框架，使用python编写，封装了大量的函数接口，可支持CPU和GPU训练。Keras提供了一系列模块，这样我们在实验模型的时候只需要调用这些模块就可以完成模型搭建，不用自己在去实现各层网络。\n主要模块有Optimizers(优化函数模块)，常用的有SGD(随机梯度下降)、 Adam;Objectives(目标函数模块)，常用的有均方误差，交差熵;Activations(激活函数模块)，sigmoid用来做二分类、relu可解决梯度消失问题、softmax常用来做多分类;Layers(网络层模块)，提供了CNN(卷积层)、RNN(LSTM、GRU)、embeddings;Initializations(初始化模块)，主要用于模型参数的初始化，常用的有均匀分布初始化，高斯分布初始化。\n使用Keras进行模型试验，可分为四个步骤(数据准备，模型构建, 模型训练, 模型测试),本文也将基于这四个步骤探索基于RNN的中文分词。\n2. 中文分词实战\n数据准备\n训练数据使用的是bakeoff2005中的北京大学的标注语料，train作为训练样本，test作为测试样本。\n统计训练样本中出现的字符并生成字典，同时将训练样本中字符全部映射成对应的字典索引(为了处理未登录词，将出现次数低于2次的字符映射为未登录词)。在序列标注分词中，一个字的标签受到上下文的影响，因而取该字的前后3个字作为特征。\n模型构建\n本文模型由一层Embedding层，2层LSTM，一个Dense输出层构成，目标函数为交叉熵，优化函数选择adam。为了验证方法的有效性，本文未采用外部语料预训练词向量，而是在训练数据上训练词向量。\nEmbedding层完成从词典索引到词向量的映射过程，即输入一个词典索引，输出该索引对应的词向量，第一层LSTM层输入为词向量序列，输出为隐层输出序列，第二层LSTM层输入为前一层输出序列，输出为隐层个数，Dense输出层输入为第二层LSTM输出，输出为类别数。\n在这里embeddingDim设为128，RNN序列长度设为7，LSTM隐层个数设为100，outputDims设为4，batch_size设为128。\n模型训练\n分别在CPU和GPU(使用单卡)上进行模型训练，使用单卡GPU的速度为CPU的4.7倍，未来还会测试单机多卡和多机多卡的性能。\nCPU\nGPU\n模型测试\n使用北京大学test进行测试，并使用bakeoff2005的测试脚本进行测试，结果如下所示：\n3.总结和展望\n深度学习的优点是可以自动发现特征，大大减少了特征工程的工作量。目前深度学习已经在语音和图像等领域取得重大进展，自然语言与语音、图像不同，是抽象符号，因而如何将深度学习应用于自然语言处理需要进行更多的研究和探索：针对不同任务的不同字词表示、将先验知识和深度学习相结合、Attention机制的运用。同时相对于标注数据，未标注数据量更多，如何运用未标注数据也是目前自然语言处理探索的热点和难点。"}
{"content2":"为何人工智能(AI)首选Python？当你读完这篇文章就会明白了。为何人工智能(AI)首选Python？读完这篇文章你就知道了。我们看谷歌的TensorFlow基本上所有的代码都是C++和Python，其他语言一般只有几千行 。如果讲运行速度的部分，用C++，如果讲开发效率，用Python，谁会用Java这种高不成低不就的语言搞人工智能呢？Python虽然是脚本语言，但是因为容易学，迅速成为科学家的工具（MATLAB也能搞科学计算，但是软件要钱，且很贵），从而积累了大量的工具库、架构，人工智能涉及大量的数据计算，用Python是很自然的，简单高效。Python有非常多优秀的深度学习库可用，现在大部分深度学习框架都支持Python，不用Python用谁？人生苦短，就用Python。\npython现在的确已经很火了，这已是一个不需要争论的问题。如果说三年前，Matlab、Scala、R、Java 和 还各有机会，局面尚且不清楚，那么三年之后，趋势已经非常明确了，特别是前两天 Facebook 开源了 PyTorch 之后，Python 作为 AI 时代头牌语言的位置基本确立，未来的悬念仅仅是谁能坐稳第二把交椅。\n不过市场上还有一些杂音。最近一个有意学习数据科学的朋友跟我说，她的一个朋友建议她从 Java 入手，因为 Hadoop 等大数据基础设施是用 Java 写的。\n在这里我要明确表个态，对于希望加入到 AI 和大数据行业的开发人员来说，把鸡蛋放在 Python 这个篮子里不但是安全的，而且是必须的。或者换个方式说，如果你将来想在这个行业混，什么都不用想，先闭着眼睛把 Python 学会了。当然，Python不是没有它的问题和短处，你可以也应该有另外一种甚至几种语言与 Python 形成搭配，但是Python 将坐稳数据分析和 AI 第一语言的位置，这一点毫无疑问。\n我甚至认为，由于 Python 坐稳了这个位置，由于这个行业未来需要大批的从业者，更由于Python正在迅速成为全球大中小学编程入门课程的首选教学语言，这种开源动态脚本语言非常有机会在不久的将来成为第一种真正意义上的编程世界语。下面分享一个python实现人工智能的代码的脚本，进行AI人工智能python实现人机对话：\n【实现思路】\nAIML\nAIML由Richard Wallace发明。他设计了一个名为 A.L.I.C.E. （Artificial Linguistics Internet Computer Entity 人工语言网计算机实体） 的机器人，并获得了多项人工智能大奖。有趣的是，图灵测试的其中一项就在寻找这样的人工智能：人与机器人通过文本界面展开数分钟的交流，以此查看机器人是否会被当作人类。\n本文就使用了Python语言调用AIML库进行智能机器人的开发。\n本系统的运作方式是使用Python搭建服务端后台接口，供各平台可以直接调用。然后客户端进行对智能对话api接口的调用，服务端分析参数数据，进行语句的分析，最终返回应答结果。\n当前系统前端使用HTML进行简单地聊天室的设计与编写，使用异步请求的方式渲染数据。\n【开发及部署环境】\n开发环境：Windows 7 ×64 英文版\nJetBrains PyCharm 2017.1.3 x64\n测试环境：Windows 7 ×64 英文版\n部分代码：\n这是部分代码展示，想学习的，欢迎跟小编交流。如果十五年之后，所有40岁以下的知识工作者，无分中外，从医生到建筑工程师，从办公室秘书到电影导演，从作曲家到销售，都能使用同一种编程语言进行基本的数据处理，调用云上的人工智能 API，操纵智能机器人，进而相互沟通想法，那么这一普遍编程的协作网络，其意义将远远超越任何编程语言之争。目前看来，Python 最有希望担任这个角色。\nPython 已经是数据分析和 AI的第一语言，网络攻防的第一黑客语言，正在成为编程入门教学的第一语言，云计算系统管理第一语言。\nPython 也早就成为Web 开发、游戏脚本、计算机视觉、物联网管理和机器人开发的主流语言之一，随着 Python 用户可以预期的增长，它还有机会在多个领域里登顶。\n如果要从科技领域找出最大的变化和革新，那么我们很难不说到“人工智能”这个关键词。人工智能催生了大量新技术、新企业和新业态，为个人、企业、国家乃至全球提供了新的经济增长点，上到谷歌、苹果、百度等巨头，下到各类创业公司，人工智能已成为一个现象级的风口。短短几年时间，图片自动归类、人脸识别已经成为非常通用的功能，自然语言作为一种交互方式正在被各种语音助理广泛运用，无人车驾驶突飞猛进，AlphaGo战胜围棋冠军，仿生机器人的技术迭代，未来几十年的城市交通和人类的生活方式都将会被人工智能所改变。\nPython作为人工智能首选编程语言，随着人工智能时代的到来，Python开发效率非常高，Python有非常强大的第三方库，基本上你想通过计算机实现任何功能，Python官方库里都有相应的模块进行支持，直接下载调用后，在基础库的基础上再进行开发，大大降低开发周期，避免重复造轮子，还有python的是可移植性、可扩展性、可嵌入性、少量代码可以做很多事，这就是为何人工智能(AI)首选Python。好了，今天就分享到这里，欢迎学习python加群571799375与小编交流，最后祝在学习的您，早日实现您的目标！\n本文来自网络，如有侵权，请联系小编删除！"}
{"content2":"常常会听到有人说，自然语言处理（NLP）是人工智能技术（AI）皇冠上的明珠。那么，从这句话上就能够看到，目前我们常常说的NLP其实是AI技术的一个分支，而且是较难的那一个分支。\n那么，到底什么是NLP呢？大概这个问题会在我们从事NLP研究和学习的过程中会一直存在，当你处于不同的阶段，应该会有不同的理解。小Dream也只能说一说在目前这个阶段，对NLP的一些理解。\n所谓“自然语言”，是相对于计算机语言（计算机能够理解的语言，如汇编、C语言等）而言的，也就是人类日常使用的语言。因为计算机设计之初主要是用于计算，或者完成某一项有明确规则任务。所以计算机语言与人类日常使用的语言存在很大的差异，计算机不能直接理解人类语言的含义，不能直接同人类进行沟通。要想实现计算机的智能化，不能够理解人类语言，是必须要跨越的一道障碍。因为，人类语言承载了人的思考、文化等，是每个人对外沟通的最主要手段。NLP就是为了解决这样一个问题而出现的技术。他希望能够使得计算机能够理解人类的语言，甚至是语言背后的文化与意图。例如说，当你说“我饿了”，计算机能够明白你肚子饿了，并且提出帮你定外卖；当你说“保护好你的菊花”，计算机能够结合当时的情境，明白你说的菊花到底是哪个菊花。\n1. NLP技术的主要范畴\n1）语音识别\n顾名思义，就是将语音转化为文字。这一部分，小Dream还没机会涉及到，短时间应该不会涉及，感兴趣的小伙伴可以出门左转，百度一下。\n2）分词、实体识别等序列标注问题\n分词和实体识别是自然语言处理比较基础的部分，但是非常的重要。小Dream之前涉及过实体识别相关的工作，所以会在下一篇NLP系列文章中介绍相关的工作，敬请关注啦。\n3）机器翻译\n这是NLP领域比较早的一个工作，谷歌的机器翻译已经能够做到较好的水平。谷歌在2018年，利用attention技术构建的Transformer模型，是目前在这一领域比较大的进展。\n4）对话系统\n智能客服，聊天机器人等都需要用到这一项NLP技术。小Dream未来一段时间都会从事这项工作的研究及开发，所这一方面应该做一个较长的连载了。\n5）文本摘要\n6）自然语言生成\n最近，OPENAI发布的GPT2.0具有很高的自然语言生成能力，预期会对自然语言处理技术产生不小的影响，感兴趣的同学可以参见如下链接：https://github.com/openai/gpt-2\n7）文字蕴含\n8）其他方面\n2. NLP技术的难点\n1）单词的边界界定\n在口语中，词与词之间通常是连贯的，而界定字词边界通常使用的办法是取用能让给定的上下文最为通顺且在文法上无误的一种最佳组合。在书写上，汉语也没有词与词之间的边界。\n2）词义的消歧\n在各国的语言中，多义词的现象都比较多。NLP常常需要根据词语的上下文决定目前的语境环境下，该词的具体含义。\n3）句法的模糊性\n自然语言的文法通常是模棱两可的，针对一个句子通常可能会剖析出多棵剖析树，而我们必须要仰赖语意及前后文的资讯才能在其中选择一棵最为适合的剖析树。\n4）语言的概括性\n句子常常并不只是字面上的意思；例如，“你能把盐递过来吗”，一个好的回答应当是把盐递过去；在大多数上下文环境中，“能”将是糟糕的回答，虽说回答“不”或者“太远了我拿不到”也是可以接受的。再者，如果一门课程去年没开设，对于提问“这门课程去年有多少学生没通过？”回答“去年没开这门课”要比回答“没人没通过”好。\n---------------------------------------------------------------------------------------------------------------------------------\n常常在想，生活的意义是什么？挣钱，买房，开豪车或许都很重要。但是真正能让内心平静的往往是亲近之人的笑容、获取知识的满足感。\n更多的AI知识，关注“Dream看AI”，用简单、朴实的语言介绍AI技术。"}
{"content2":"原文\n先mark，后续尝试。\n1.NLTK\nNLTK 在用 Python 处理自然语言的工具中处于领先的地位。它提供了 WordNet 这种方便处理词汇资源的借口，还有分类、分词、除茎、标注、语法分析、语义推理等类库。\n网站\nhttp://www.nltk.org/\n安装\n安装 NLTK:\nsudo pip install -U nltk\n安装 Numpy (可选):\nsudo pip install -U numpy\n安装测试:\npython then type import nltk\n2.Pattern\nPattern 的自然语言处理工具有词性标注工具(Part-Of-Speech Tagger)，N元搜索(n-gram search)，情感分析(sentiment analysis)，WordNet。支持机器学习的向量空间模型，聚类，向量机。\n网站:\nhttps://github.com/clips/pattern\n安装:\npip install pattern\n3.TextBlob\nTextBlob 是一个处理文本数据的 Python 库。提供了一些简单的api解决一些自然语言处理的任务，例如词性标注、名词短语抽取、情感分析、分类、翻译等等。\n网站：\nhttp://textblob.readthedocs.org/en/dev/\n安装：\npip install -U textblob\n4.Gensim\nGensim 提供了对大型语料库的主题建模、文件索引、相似度检索的功能。它可以处理大于RAM内存的数据。作者说它是“实现无干预从纯文本语义建模的最强大、最高效、最无障碍的软件。”\n网站：\nhttps://github.com/piskvorky/gensim\n安装：\npip install -U gensim\n5.PyNLPI\n它的全称是：Python自然语言处理库（Python Natural Language Processing Library，音发作: pineapple） 这是一个各种自然语言处理任务的集合，PyNLPI可以用来处理N元搜索，计算频率表和分布，建立语言模型。他还可以处理向优先队列这种更加复杂的数据结构，或者像 Beam 搜索这种更加复杂的算法。\n安装：\nLInux:\nsudo apt-get install pymol\nFedora:\nyum install pymol\n6.spaCy\n这是一个商业的开源软件。结合Python和Cython，它的自然语言处理能力达到了工业强度。是速度最快，领域内最先进的自然语言处理工具。\n网站：\nhttps://github.com/proycon/pynlpl\n安装：\npip install spacy\n7.Polyglot\nPolyglot 支持对海量文本和多语言的处理。它支持对165种语言的分词，对196中语言的辨识，40种语言的专有名词识别，16种语言的词性标注，136种语言的情感分析，137种语言的嵌入，135种语言的形态分析，以及69中语言的翻译。\n网站：\nhttps://pypi.python.org/pypi/polyglot\n安装\npip install polyglot\n8.MontyLingua\nMontyLingua 是一个自由的、训练有素的、端到端的英文处理工具。输入原始英文文本到 MontyLingua ，就会得到这段文本的语义解释。适合用来进行信息检索和提取，问题处理，回答问题等任务。从英文文本中，它能提取出主动宾元组，形容词、名词和动词短语，人名、地名、事件，日期和时间，等语义信息。\n网站：\nhttp://web.media.mit.edu/~hugo/montylingua/\n9.BLLIP Parser\nBLLIP Parser（也叫做Charniak-Johnson parser）是一个集成了产生成分分析和最大熵排序的统计自然语言工具。包括 命令行 和 python接口 。\n10.Quepy\nQuepy是一个Python框架，提供将自然语言转换成为数据库查询语言。可以轻松地实现不同类型的自然语言和数据库查询语言的转化。所以，通过Quepy，仅仅修改几行代码，就可以实现你自己的自然语言查询数据库系统。"}
{"content2":"智能金融的基石——自然语言处理（NLP）和知识图谱\n金融行业因其与数据的高度相关性，成为人工智能最先应用的行业之一，而自然语言处理（NLP）与知识图谱作为人工智能技术的重要研究方向与组成部分，正在快速进入金融领域，并日益成为智能金融的基石。\n一般的金融科技公司只会集中在其中的某些业务方向，只要能深入掌握两到三种能力，就能具有相当的竞争力。在这些业务场景中，自然语言处理（NLP）和知识图谱技术往往需要共同应用，才能发挥出最大的效能。同时，一种核心能力可以在多个智能金融应用场景中得到应用，这些应用场景包括：智能投研、智能投顾、智能风控、智能客服、智能监管、智能运营等。\n金融语义应用场景概念框 01智能问答和语义搜索 智能问答和语义搜索是自然语言处理（NLP）的关键技术，目的是让用户以自然语言形式提出问题，深入进行语义分析，以更好理解用户意图，快速准确获取知识库中的信息。在用户界面上，既可以表现为问答机器人的形式（智能问答），也可以为搜索引擎的形式（语义搜索）。智能问答系统一般包括问句理解、信息检索、答案生成三个环节。智能问答系统与金融知识图谱密切相关，知识图谱在语义层面提供知识的表示、存储和推理，智能问答则从语义层面提供知识检索的入口。基于知识图谱的智能问答相比基于文本的问答更能满足金融业务实际需求。 智能问答和语义搜索的价值在金融领域越来越被重视。它主要应用的场景包括智能投研、智能投顾和智能客服。在智能投研领域，投研人员日常工作需要通过多种渠道搜索大量相关信息。而有了金融问答和语义搜索的帮助，信息获取途径将是“Just ask a question”。并且，语义搜索返回的结果不仅是平面化的网页信息，而是能把各方面的相关信息组织起来的立体化信息，还能提供一定的分析预测结论。在智能客服和智能投顾领域，智能问答系统的应用主要是机器人客服。机器人客服目前的作用还只是辅助人工客服回答一些常见问题，但已能较大地节省客服部门的人力成本。 典型应用案例如美国Alphasense公司为投研人员整合碎片化信息，提供专业金融知识访问工具。AlphaSense公司的产品可以说是新一代的金融知识引擎。它从新闻、财报、研报各种行业网站等获取大量数据、信息、知识形式的“素材”，通过语义分析构建成知识图谱，并提供高级语义搜索引擎、智能问答、交互式知识管理系统、文档（知识）协作系统，以对金融知识进行更加有效的管理、搜索、使用。 02资讯与舆情分析 金融资讯信息非常丰富，例如公司新闻（公告、重要事件、财务状况等）、金融产品资料（股票、证券等）、宏观经济（通货膨胀、失业率等）、政策法规（宏观政策、税收政策等）、社交媒体评论等。 金融资讯每天产生的数量非常庞大，要从浩如烟海的资讯库中准确找到相关文章，还要阅读分析每篇重要内容，是费时费力的工作。如果有一个工具帮助人工快速迅捷获取资讯信息，将大大提高工作效率。资讯舆情分析的主要功能包括资讯分类标签（按公司、产品、行业、概念板块等）、情感正负面分析（文章、公司或产品的情感）、自动文摘（文章的主要内容）、资讯个性化推荐、舆情监测预警（热点热度、云图、负面预警等）。在这个场景中，金融知识图谱提供的金融知识有助于更好理解资讯内容，更准确地进行资讯舆情分析。 资讯舆情分析的应用主要在智能投研和智能监管这两个场景。目前市场上的辅助投研工具中，资讯舆情分析是必不可少的重要部分。资讯舆情分析作为通用工具更多是对海量定性数据进行摘要、归纳、缩简，以更加快捷方便地为投研人员提供信息，支持他们进行决策，而非直接给出决策结论。在智能监管领域，通过资讯舆情分析，对金融舆情进行监控，发现违规非法活动进行预警。 03金融预测和分析 基于语义的金融预测即利用金融文本中包含的信息预测各种金融市场波动，它是以NLP等人工智能技术与量化金融技术的结合。 利用金融文本数据帮助改善金融交易预测模型的想法早已有之。本世纪初，美国就有人利用新闻和股价的历史数据来预测股价波动。2010年后，社交媒体产生了大量数据，基于Twitter、Facebook来预测股市的研究项目很多。最近，深度学习被大量应用在预测模型中。金融文本数据提供的信息是定性的（qualitative），而通常数字形式的数据是定量的（quantitative）。定性分析比定量分析更难，定性信息包含的信息量更大。有分析表明，投资决策人员在进行决策时，更多依赖于新闻、事件甚至流言等定性信息，而非定量数据。因此，可期待基于语义的金融预测分析大有潜力可挖。这个场景中涉及的关键NLP技术包括事件抽取和情感分析技术。金融知识图谱在金融预测分析中具有重要的作用，它是进行事件推理的基础。例如在中兴事件中，可根据产业链图谱推导受影响的公司。 基于语义的金融预测和分析在金融应用的主要场景包括智能投研和智能投顾。它的理想目标是能代替投资人员做投资预测，进行自动交易，但目前还只是作为投资人员的投资参考。将不同来源的多维度数据进行关联分析，特别是对非结构化数据的分析，比如邮件、社交网络信息、网络日志信息。从而挖掘和展现出未知的相关关系，为决策提供依据。典型的应用案例如美国Palantir公司提供基于知识图谱的大数据分析平台。其金融领域产品Metropolis，通过整合多源异构数据，构建金融领域知识图谱。特点是：对非结构化数据的分析能力、将人的洞察和逻辑与高效的机器辅助手段相结合起来。另一个例子如Kensho公司利用金融知识图谱进行预测分析。在英国脱欧期间，交易员成功运用Kensho了解到退欧选举造成当地货币贬值；曾准确分析了美国总统任期的前100天内股票涨跌情况。 04文档信息抽取 信息抽取是NLP的一种基础技术，是NLP进一步进行数据挖掘分析的基础，也是知识图谱中知识抽取的基础。采用的方法包括基于规则模板的槽填充的方法、基于机器学习或深度学习的方法。按抽取内容分可以分为实体抽取、属性抽取、关系抽取、规则抽取、事件抽取等。 在这里的文档信息抽取特指一种金融应用场景。指从金融文档（如公告研报）等抽取指定的关键信息，如公司名称、人名、指标名称、数值等。文档格式可能是格式化文档（word, pdf, html等）或纯文本。对格式化文本进行抽取时需要处理并利用表格、标题等格式信息。文档信息抽取的应用场景主要是智能投研和智能数据，促进数据生产自动化或辅助人工进行数据生产、文档复核等。 05自动文档生成 自动文档生成指根据一定的数据来源自动产生各类金融文档。常见的需要生成的金融文档如信息披露公告（债券评级、股转书等）、各种研究报告。 自动报告生成属于生成型NLP应用。它的数据来源可能是结构化数据，也可能是从非结构化数据用信息抽取技术取得的，也可能是在金融预测分析场景中获得的结论。简单的报告生成方法是根据预定义的模板，把关键数据填充进去得到报告。进一步的自动报告生成需要比较深入的NLG技术，它可以把数据和分析结论转换成流畅的自然语言文本。 自动文档生成的应用场景包括智能投研、智能投顾等。它的典型应用案例如美国的Narrative Science，它从结构化数据中进行数据挖掘，并把结果用简短的文字或依据模板产生报告内容。又如Automated Insights，它为美联社自动写出了10亿多篇文章与报告。 06风险评估与反欺诈 风险评估是大数据、互联网时代的传统应用场景，应用时间较早，应用行业广泛。它是通过大数据、机器学习技术对用户行为数据分析后，进行用户画像，并进行信用和风险评估。 NLP技术在风控场景中的作用是理解分析相关文本内容，为待评估对象打标签，为风控模型增加更多的评估因子。引入知识图谱技术以后，可以通过人员关系图谱的分析，发现人员关系的不一致性或者短时间内变动较大，从而侦测欺诈行为。利用大数据风控技术，在事前能够预警，过滤掉带恶意欺诈目的人群；在事中进行监控，发生欺诈攻击时及时发现；在事后进行分析，挖掘到欺诈者的关联信息，降低以后的风险。 在金融行业，风险评估与反欺诈的应用场景首先是智能风控。利用NLP和知识图谱技术改善风险模型以减少模型风险，提高欺诈监测能力。其次，还可以应用在智能监管领域，以加强监管者和各部门的信息交流，跟踪合规需求变化。通过对通信、邮件、会议记录、电话的文本进行分析，发现不一致和欺诈文本。例如欺诈文本有些固定模式：如用负面情感词，减少第一人称使用等。通过有效的数据聚合分析可大大减少风险报告和审计过程的资源成本。从事此类业务的Finctech公司很多，如Palantir最初从事的金融业务就是反欺诈。其他如Digital Reasoning、Rapid Miner、Lexalytics、Prattle等。 07客户洞察 客户关系管理（CRM）也是在互联网和大数据时代中发展起来，市场相对成熟，应用比较广泛，许多Fintech公司都以此为主要业务方向。现代交易越来越多是在线上而不是线下当面完成，因此如何掌握客户兴趣和客户情绪，越来越需要通过对客户行为数据进行分析来完成。 NLP技术在客户关系管理中的应用，是通过把客户的文本类数据（客服反馈信息、社交媒体上的客户评价、客户调查反馈等）解析文本语义内涵，打上客户标签，建立用户画像。同时，结合知识图谱技术，通过建立客户关系图谱，以获得更好的客户洞察。这包括客户兴趣洞察（产品兴趣），以进行个性化产品推荐、精准营销等。以及客户态度洞察（对公司和服务满意度、改进意见等），以快速响应客户问题，改善客户体验，加强客户联系，提高客户忠诚度。 客户洞察在金融行业的应用场景主要包括智能客服和智能运营。例如在智能客服中，通过客户洞察分析，可以改善客户服务质量，实现智能质检。在智能运营（智能CRM）中，根据客户兴趣洞察，实现个性化精准营销。国外从事这个业务方向的Fintech公司很多，如Inmoment，Medallia，NetBase等。\n自然语言处理（NLP）和知识图谱两种技术本身都还在发展成长过程中，因此在金融落地过程中势必也还会面临许多新的课题和挑战，任重而道远。一方面，人工智能必须与金融的具体业务场景切合，找到金融企业需求痛点，真正提升客户生产效率，给客户带来价值；另一方面，人工智能是基础技术学科，技术难度大，人才要求高，在核心技术和关键算法上需要有突破有优势，才能不断提升市场竞争力。所以，场景驱动和技术研发需要相辅相成、紧密结合。相信金融智能语义技术的应用将会有广阔的发展空间，推动智能金融迈向一个新的台阶。"}
{"content2":"随着.NET Core的发布和开源，.NET又重新回到了人们的视野。除了开源、跨平台、高性能以及优秀的语言特性，越来越多的第三方开源库也出现在了github上——包括ML.NET机器学习、Xamarin移动开发平台、基于Actor模型的分布式框架Orleans以及分布式开发及部署平台Service Fabric等等。\n西安.NET社区组织发起了此次“拥抱开源, 又见.NET”线下交流活动，邀请了三位资深.NET开发者作为分享讲师，他们将从架构、原理、语言出发，与大家一起分享交流：\n.NET在大数据及人工智能项目的设计及架构\nASP.NET Core应用程序的工程实践\n.NET家族的函数式语言从C#到F#\n免费技术交流，感兴趣的同学抓紧时间报名吧！\n【  时 间 地 点  】\n2018年09月15日（周六）1:00PM—5:30PM\n西安市高新区天谷八路环普产业园E座5楼 ThoughtWorks办公室（请从E6进入）\n【  活 动 安 排  】\n12:50 PM  签到\n01:20 PM  开场介绍\n01:30 PM  话题一《.NET在大数据和人工智能项目中的应用实践》\n02:40 PM  话题二《ASP.NET Core生命周期指北》\n03:50 PM  话题三《从C#到F#》\n05:10 PM  反馈抽奖&合影留念\n【  精 彩 预 告  】\n话题一《.NET在大数据和人工智能项目中的应用实践》\n话题简介：应用产生数据，数据蕴含智慧，如何应用人工智能从大数据挖掘出有价值的知识信息并提供智能化的应用是一个比较有挑战力的话题，在大数据和人工智能领域，dotNET应用并不广泛，基于项目实践经验为大家分享.NET在大数据和人智能领域之中诸如数据采集、自然语言处理、机器学习、结果展示和整体架构设计方面的应用。\n分享人：\n魏 琼 东\n易特科首席架构师\n从事软件研发工作17年，2004年开始使用C#，结合多年的研发管理经验，开发了AgileEAS.NET SOA 中间件平台并在医疗卫生、铁路、物流、制造等行业广泛应用，擅长企业软件过程改进、系统分析与架构设计，目前主要致力于大数据和人工智能与医疗健康领域的结合。\n话题二《ASP.NET Core生命周期指北》\n话题简介：通过对ASP.NET Core应用程序生命周期的剖析，让.NET Core开发人员能够能够更直观的掌握应用程序结构和设计思想，轻松的进行故障排除和调试。首先，通过对启动的分解，帮助理解Dependency Injection和Middleware的行为本质，接下来通过对HTTP请求和服务器响应的重演，来熟悉框架中的各个组件，以及它们之间是如何协同和交互的。\n分享人：\n张 文 清\nThoughtWorks 高级.NET开发\n热爱钻研技术本质，挑战新事物和推动潮流技术发展。曾供职于西安葡萄城信息技术有限公司，积累超过10年.NET控件和工具的设计以及开发经验，主导和实施多个云应用架构开发和DevOps实践。\n提示：如果希望现场做代码尝试的话，需要准备以下环境： .NET Core 2.1.400 2. IDE，推荐Visual Studio或者Jetbrains Rider。\n话题三《从C#到F#》\n话题简介：如果你已经精通了C#或者其他面向对象的语言，你也许会寻找另一个值得学习的语言。得益于F#强大的类型推导系统，你可以写出跟Python一样简洁而带有类型安全的代码；F#作为一门函数式语言，你可以很轻松的通过部分应用以及函数组合来创建新的函数，通过分享函数式编程模式以及monadic风格的实例让你了解函数式编程的基本思想；另外F#还内置了异步编程、消息队列，type provider等高生产力的组件及特性，跟.NET技术栈的无缝集成意味着你还可以使用所有.NET技术栈的类库和工具。\n分享人：\n张 阳\nThoughtWorks 高级.NET开发\n有着多年基于MessageBus以及Event Sourcing的CQRS企业级开发经验，对基于消息传递的微服务架构有着深刻的理解和实践，同时也是DDD的爱好者和布道师，博客园推荐作者以及.NET西安社区发起者。\n【  活 动 福 利  】\n参与者除了能够与众多NET开发师现场交流外，还将有机会获得由ThoughtWorks提供的精美礼品，如: 水杯、笔记本等实用小礼物。\n【  报 名 须 知  】\n1、本次活动为免费技术交流，对.NET开发感兴趣的小伙伴均可报名。\n2、为了保证活动的质量，使话题能够深入展开，我们将限定参会人数，请您务必提前报名。\n3、报名的小伙伴们如果提供的邮件地址和手机号码无误，我们会在9月12日（活动开始前3天）对通过报名筛选的小伙伴发送短信和邮件确认。请确认自己填写的电话、邮件地址无误。\n【报名地址】\n点我\n【  .NET西安社区介绍  】\n.Net 西安社区正在使用.NET Core / Azure / Xamarin等技术开发基于互联网和移动应用平台上的各种新产品和商业服务。我们的目标是通过举办各种分享活动，交流开发心得和经验来推动.NET技术栈在西安乃至西北地区的发展。我们是一个开放和自由的社区，欢迎您加入.NET西安社区！"}
{"content2":"公众号100天了，是个值得一提的日子！\n我从2017年10月31日开始做这个公众号，到今天2018年2月7日，差不多100天时间 。虽然公众号很早就申请了，但直到去年10月31日，我才有真正把这个公众号搞好的打算。\n其实刚开始也没有想到做什么主题的公众号，但我个人认为开公众号就需要做一些有价值的事情，而不是随便写点东西，一开始我就给自己定下了这样的价值观。\n公众号立意\n经常在微信群里看到大家在聊Python ，而且平时我也是关注很多Python方面的技术文章以及公众号。 我认为用Python做一些爬虫，数据分析，机器学习,自然语言处理以及人工智能是一件非常有意思的事情，这也是我想了解Python的动机。 我在新榜(一个提供微信公众号运营数据平台)搜索Python，我发现了一些比较好的Python公众号.\n它们大致就是这几种类型：\n有的是培训机构弄的，看了一部分就是要收费，没有办法满足部分小伙伴的学习需求\n有的公众号文章起点很高,不适合新入门的小白,没有办法去学习\n有的公众号文章太基础又没有进阶，可能是没有继续更新\n有的公众号技术不是很纯粹除了Python还有其它与技术主题无关的东西。\n我没有发现一个公众号是从入门到进阶，然后循序渐进式学习Python的，于是我就想做与他们不一样的公众号。\n我本身就是想学习Python,开公众号一方面能监督自己学习及回顾学习的东西，再者还能做一些技术传播,打造个人品牌. 所以我就萌发了做一个Python技术公众号的想法.并且很专注的去做这件事情。我的公众号文章都是原创，除了一些合作与推文，再就是自己觉得非常有意思的文章我才会去考虑转载。 我的观点就是文章尽量自己去创作，这才是自己的价值体现。100天以来也是这么坚持下来的,往后也会坚持这样的价值观。\n取得成绩\n100天学了不少新东西，也认识了很多新朋友。公众号现在平均每篇文章的阅读量都在900以上，活跃粉丝是8000多（新榜给的数据比我实际粉丝虚高一点)，创建了一个关于Python的微信群，差不多400号人，人员也算是基本稳定，无闲聊，一个很好的学术环境. 虽然这些成绩和一些大佬相比还是有较大的差距，但这达到了我的期望值,这就够了。\n新榜数据\n新榜数据\n未来展望\n对未来的展望我是打算继续把公众号做下去，把Python技术传播出去，并坚定不移的走自己的路线: 爬虫，数据分析，机器学习，自然语言处理，人工智能.发现要做的事情挺多的,也挺复杂的，但我认为做这些东西很有意思，值得去做,这就够了!\n推荐阅读：\nPython爬虫：使用Python动态爬取冯大辉老师微博，再用词云分析\nPython爬虫：动态爬取QQ说说并生成词云，分析朋友状况\nPython爬虫：学习Selenium并使用Selenium模拟登录知乎\nPython爬取大量数据时，如何防止IP被封\nPython爬虫：现学现用xpath爬取豆瓣音乐\n关注微信公众号(Python绿色通道)即可领取5T学习资料： 数据结构, 小程序, 前端, PHP, Java ,Android, Python, 大数据, 人工智能等！"}
{"content2":"<!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 18.0px 'Courier New'; color: #000000; background-color: #ffffff} p.p2 {margin: 0.0px 0.0px 0.0px 0.0px; font: 18.0px 'Courier New'; color: #afad24; background-color: #ffffff} span.s1 {font-variant-ligatures: no-common-ligatures} -->\n参考书籍《Python自然语言处理》，书籍中的版本是Python2和NLTK2，我使用的版本是Python3和NLTK3\n实验环境Windows8.1，已有Python3.4，并安装了NumPy, Matplotlib，参考：http://blog.csdn.net/monkey131499/article/details/50734183\n安装NLTK3，Natural Language Toolkit，自然语言工具包，地址：http://www.nltk.org/\n安装命令：pip install nltk\n代码:\nSaintKings-Mac-mini:~ saintking$ sudo pip install nltk\nPassword:\nThe directory '/Users/saintking/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\nThe directory '/Users/saintking/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\nCollecting nltk\nRequirement already satisfied: six in /Library/Python/2.7/site-packages (from nltk)\nInstalling collected packages: nltk\nSuccessfully installed nltk-3.2.5\nSaintKings-Mac-mini:~ saintking$\n安装完成后测试：import nltk\n<!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 18.0px 'Courier New'; color: #000000; background-color: #ffffff} span.s1 {font-variant-ligatures: no-common-ligatures} -->\nSaintKings-Mac-mini:~ saintking$ python\nPython 2.7.10 (default, Jul 30 2016, 18:31:42)\n[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.34)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import nltk\n>>> nltk.download()\nshowing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n没有报错即表明安装成功。\nNLTK包含大量的软件、数据和文档，可以进行文本分析和语言结构分析等。数据资源可以自行下载使用。地址：http://www.nltk.org/data.html，数据列表：http://www.nltk.org/nltk_data/\n下载NLTK-Data，在Python中输入命令：\n>>>import nltk\n>>>nltk.download()\n弹出新的窗口，用于选择下载的资源\n双击行后安装.\n<!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 18.0px 'Courier New'; color: #000000; background-color: #ffffff} span.s1 {font-variant-ligatures: no-common-ligatures} -->\n>>> import nltk\n>>> nltk.download()\nshowing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\nTrue\n>>>\n点击File可以更改下载安装的路径。all表示全部数据集合，all-corpora表示只有语料库和没有语法或训练的模型，book表示只有书籍中例子或练习的数据。需要注意一点，就是数据的保存路径，要么在C盘中，要么在Python的根目录下，否则后面程序调用数据的时候会因为找不到而报错。\n【注意：软件安装需求：Python、NLTK、NLTK-Data必须安装，NumPy和Matplotlin推荐安装，NetworkX和Prover9可选安装】\n简单测试NLTK分词功能：\n---\n下面看一下NLTK数据的几种方法：\n1.加载数据\n<!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 18.0px 'Courier New'; color: #000000; background-color: #ffffff} span.s1 {font-variant-ligatures: no-common-ligatures} -->\n>>> from nltk.book import *\n*** Introductory Examples for the NLTK Book ***\nLoading text1, ..., text9 and sent1, ..., sent9\nType the name of the text or sentence to view it.\nType: 'texts()' or 'sents()' to list the materials.\ntext1: Moby Dick by Herman Melville 1851\ntext2: Sense and Sensibility by Jane Austen 1811\ntext3: The Book of Genesis\ntext4: Inaugural Address Corpus\ntext5: Chat Corpus\ntext6: Monty Python and the Holy Grail\ntext7: Wall Street Journal\ntext8: Personals Corpus\ntext9: The Man Who Was Thursday by G . K . Chesterton 1908\n>>>\n2.搜索文本\n<!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 18.0px 'Courier New'; color: #000000; background-color: #ffffff} span.s1 {font-variant-ligatures: no-common-ligatures} -->\n>>> print(text1.concordance('monstrous'))\nDisplaying 11 of 11 matches:\nong the former , one was of a most monstrous size . ... This came towards us ,\nON OF THE PSALMS . \" Touching that monstrous bulk of the whale or ork we have r\nll over with a heathenish array of monstrous clubs and spears . Some were thick\nd as you gazed , and wondered what monstrous cannibal and savage could ever hav\nthat has survived the flood ; most monstrous and most mountainous ! That Himmal\nthey might scout at Moby Dick as a monstrous fable , or still worse and more de\nth of Radney .'\" CHAPTER 55 Of the Monstrous Pictures of Whales . I shall ere l\ning Scenes . In connexion with the monstrous pictures of whales , I am strongly\nere to enter upon those still more monstrous stories of them which are to be fo\nght have been rummaged out of this monstrous cabinet there is no telling . But\nof Whale - Bones ; for Whales of a monstrous size are oftentimes cast up dead u\nNone\n>>>\n3.相似文本\n<!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 18.0px 'Courier New'; color: #000000; background-color: #ffffff} span.s1 {font-variant-ligatures: no-common-ligatures} -->\n>>> print(text1.similar('monstrous'))\nimperial subtly impalpable pitiable curious abundant perilous\ntrustworthy untoward singular lamentable few determined maddens\nhorrible tyrannical lazy mystifying christian exasperate\nNone\n>>>\n4.共用词汇的上下文\n<!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 18.0px 'Courier New'; color: #000000; background-color: #ffffff} span.s1 {font-variant-ligatures: no-common-ligatures} -->\n>>> print(text2.common_contexts(['monstrous','very']))\na_pretty is_pretty a_lucky am_glad be_glad\nNone\n>>>\n5.词汇分布图\n<!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 18.0px 'Courier New'; color: #000000; background-color: #ffffff} span.s1 {font-variant-ligatures: no-common-ligatures} -->\n>>> text4.dispersion_plot(['citizens','democracy','freedom','duties','America'])\n6.词汇统计\n#encoding=utf-8 import nltk from nltk.book import * print('~~~~~~~~~~~~~~~~~~~~~~~~~') print('文档text3的长度：',len(text3)) print('文档text3词汇和标识符排序：',sorted(set(text3))) print('文档text3词汇和标识符总数：',len(set(text3))) print('单个词汇平均使用次数：',len(text3)*1.0/len(set(text3))) print('单词 Abram在text3中使用次数：',text3.count('Abram')) print('单词Abram在text3中使用百分率：',text3.count('Abram')*100/len(text3))"}
{"content2":"Python自然语言处理学习笔记(64)： 7.5 命名实体识别\n7.5   Named Entity Recognition  命名实体识别\nAt the start of this chapter, we briefly introduced named entities (NEs). Named entities are definite（确定的） noun phrases that refer to specific types of individuals, such as organizations, persons, dates, and so on（命名实体是明确的名词短语，指的是个体的具体类型，例如组织，个人，日期等等）. Table 7.4 lists some of the more commonly used types of NEs. These should be self-explanatory（不言自明的）, except for \"Facility\": human-made artifacts（人工产品） in the domains of architecture and civil engineering（土木工程）; and \"GPE\": geo-political entities（地缘政治实体） such as city, state/province, and country.\nNE Type\nExamples\nORGANIZATION\nGeorgia-Pacific Corp., WHO\nPERSON\nEddy Bonte, President Obama\nLOCATION\nMurray River, Mount Everest\nDATE\nJune, 2008-06-29\nTIME\ntwo fifty a m, 1:30 p.m.\nMONEY\n175 million Canadian Dollars, GBP 10.40\nPERCENT\ntwenty pct, 18.75 %\nFACILITY\nWashington Monument, Stonehenge\nGPE\nSouth East Asia, MidlothianTable 7.4:\nCommonly Used Types of Named Entity\nThe goal of a named entity recognition (NER) system is to identify all textual mentions of the named entities. This can be broken down into（分解成） two sub-tasks: identifying the boundaries of the NE, and identifying its type（识别NE的边界和它们的类型）. While named entity recognition is frequently a prelude（序曲） to identifying relations in Information Extraction, it can also contribute to other tasks. For example, in Question Answering (QA), we try to improve the precision of Information Retrieval by recovering not whole pages, but just those parts which contain an answer to the user's question. Most QA systems take the documents returned by standard Information Retrieval, and then attempt to isolate the minimal text snippet in the document containing the answer. Now suppose the question was Who was the first President of the US?, and one of the documents that was retrieved contained the following passage:\n(5)\nThe Washington Monument is the most prominent structure in Washington, D.C. and one of the city's early attractions. It was built in honor of George Washington, who led the country to independence and then became its first President.\nAnalysis of the question leads us to expect that an answer should be of the form X was the first President of the US, where X is not only a noun phrase, but also refers to a named entity of type PER. This should allow us to ignore the first sentence in the passage. While it contains two occurrences of Washington, named entity recognition should tell us that neither of them has the correct type.\nHow do we go about identifying named entities? One option would be to look up each word in an appropriate list of names. For example, in the case of locations, we could use a gazetteer（地名词典）, or geographical dictionary, such as the Alexandria Gazetteer or the Getty Gazetteer. However, doing this blindly runs into problems, as shown in Figure 7.12.\nFigure 7.12: Location Detection by Simple Lookup for a News Story: Looking up every word in a gazetteer is error-prone（易错配的）; case distinctions may help, but these are not always present.\nObserve that the gazetteer has good coverage of locations in many countries, and incorrectly finds locations like Sanchez in the Dominican Republic and On in Vietnam. Of course we could omit such locations from the gazetteer, but then we won't be able to identify them when they do appear in a document.\nIt gets even harder in the case of names for people or organizations. Any list of such names will probably have poor coverage. New organizations come into existence every day, so if we are trying to deal with contemporary（当代的） newswire or blog entries, it is unlikely that we will be able to recognize many of the entities using gazetteer lookup.\nAnother major source of difficulty is caused by the fact that many named entity terms（措辞） are ambiguous. Thus May and North are likely to be parts of named entities for DATE and LOCATION, respectively, but could both be part of a PERSON; conversely Christian Dior looks like a PERSON but is more likely to be of type ORGANIZATION. A term like Yankee will be ordinary modifier in some contexts, but will be marked as an entity of type ORGANIZATION in the phrase Yankee infielders.\nFurther challenges are posed by multi-word names like Stanford University, and by names that contain other names such as Cecil H. Green Library andEscondido Village Conference Service Center. In named entity recognition, therefore, we need to be able to identify the beginning and end of multi-token sequences.\nNamed entity recognition is a task that is well-suited（适当的） to the type of classifier-based approach that we saw for noun phrase chunking. In particular, we can build a tagger that labels each word in a sentence using the IOB format, where chunks are labeled by their appropriate type. Here is part of the CONLL 2002 (conll2002) Dutch training data:\nEddy N B-PER\nBonte N I-PER\nis V O\nwoordvoerder N O\nvan Prep O\ndiezelfde Pron O\nHogeschool N B-ORG\n. Punc O\nIn this representation, there is one token per line, each with its part-of-speech tag and its named entity tag. Based on this training corpus, we can construct a tagger that can be used to label new sentences; and use the nltk.chunk.conlltags2tree() function to convert the tag sequences into a chunk tree.\nNLTK provides a classifier that has already been trained to recognize named entities, accessed with the function nltk.ne_chunk(). If we set the parameter binary=True, then named entities are just tagged as NE; otherwise, the classifier adds category labels such as PERSON, ORGANIZATION, and GPE.\n>>> sent = nltk.corpus.treebank.tagged_sents()[22]\n>>> print nltk.ne_chunk(sent, binary=True)\n(S\nThe/DT\n(NE U.S./NNP)\nis/VBZ\none/CD\n...\naccording/VBG\nto/TO\n(NE Brooke/NNP T./NNP Mossman/NNP)\n...)\n>>> print nltk.ne_chunk(sent)\n(S\nThe/DT\n(GPE U.S./NNP)\nis/VBZ\none/CD\n...\naccording/VBG\nto/TO\n(PERSON Brooke/NNP T./NNP Mossman/NNP)\n...) 7.5   Named Entity Recognition  命名实体识别\nAt the start of this chapter, we briefly introduced named entities (NEs). Named entities are definite（确定的） noun phrases that refer to specific types of individuals, such as organizations, persons, dates, and so on（命名实体是明确的名词短语，指的是个体的具体类型，例如组织，个人，日期等等）. Table 7.4 lists some of the more commonly used types of NEs. These should be self-explanatory（不言自明的）, except for \"Facility\": human-made artifacts（人工产品） in the domains of architecture and civil engineering（土木工程）; and \"GPE\": geo-political entities（地缘政治实体） such as city, state/province, and country.\nNE Type\nExamples\nORGANIZATION\nGeorgia-Pacific Corp., WHO\nPERSON\nEddy Bonte, President Obama\nLOCATION\nMurray River, Mount Everest\nDATE\nJune, 2008-06-29\nTIME\ntwo fifty a m, 1:30 p.m.\nMONEY\n175 million Canadian Dollars, GBP 10.40\nPERCENT\ntwenty pct, 18.75 %\nFACILITY\nWashington Monument, Stonehenge\nGPE\nSouth East Asia, MidlothianTable 7.4:\nCommonly Used Types of Named Entity\nThe goal of a named entity recognition (NER) system is to identify all textual mentions of the named entities. This can be broken down into（分解成） two sub-tasks: identifying the boundaries of the NE, and identifying its type（识别NE的边界和它们的类型）. While named entity recognition is frequently a prelude（序曲） to identifying relations in Information Extraction, it can also contribute to other tasks. For example, in Question Answering (QA), we try to improve the precision of Information Retrieval by recovering not whole pages, but just those parts which contain an answer to the user's question. Most QA systems take the documents returned by standard Information Retrieval, and then attempt to isolate the minimal text snippet in the document containing the answer. Now suppose the question was Who was the first President of the US?, and one of the documents that was retrieved contained the following passage:\n(5)\nThe Washington Monument is the most prominent structure in Washington, D.C. and one of the city's early attractions. It was built in honor of George Washington, who led the country to independence and then became its first President.\nAnalysis of the question leads us to expect that an answer should be of the form X was the first President of the US, where X is not only a noun phrase, but also refers to a named entity of type PER. This should allow us to ignore the first sentence in the passage. While it contains two occurrences of Washington, named entity recognition should tell us that neither of them has the correct type.\nHow do we go about identifying named entities? One option would be to look up each word in an appropriate list of names. For example, in the case of locations, we could use a gazetteer（地名词典）, or geographical dictionary, such as the Alexandria Gazetteer or the Getty Gazetteer. However, doing this blindly runs into problems, as shown in Figure 7.12.\nFigure 7.12: Location Detection by Simple Lookup for a News Story: Looking up every word in a gazetteer is error-prone（易错配的）; case distinctions may help, but these are not always present.\nObserve that the gazetteer has good coverage of locations in many countries, and incorrectly finds locations like Sanchez in the Dominican Republic and On in Vietnam. Of course we could omit such locations from the gazetteer, but then we won't be able to identify them when they do appear in a document.\nIt gets even harder in the case of names for people or organizations. Any list of such names will probably have poor coverage. New organizations come into existence every day, so if we are trying to deal with contemporary（当代的） newswire or blog entries, it is unlikely that we will be able to recognize many of the entities using gazetteer lookup.\nAnother major source of difficulty is caused by the fact that many named entity terms（措辞） are ambiguous. Thus May and North are likely to be parts of named entities for DATE and LOCATION, respectively, but could both be part of a PERSON; conversely Christian Dior looks like a PERSON but is more likely to be of type ORGANIZATION. A term like Yankee will be ordinary modifier in some contexts, but will be marked as an entity of type ORGANIZATION in the phrase Yankee infielders.\nFurther challenges are posed by multi-word names like Stanford University, and by names that contain other names such as Cecil H. Green Library andEscondido Village Conference Service Center. In named entity recognition, therefore, we need to be able to identify the beginning and end of multi-token sequences.\nNamed entity recognition is a task that is well-suited（适当的） to the type of classifier-based approach that we saw for noun phrase chunking. In particular, we can build a tagger that labels each word in a sentence using the IOB format, where chunks are labeled by their appropriate type. Here is part of the CONLL 2002 (conll2002) Dutch training data:\nEddy N B-PER\nBonte N I-PER\nis V O\nwoordvoerder N O\nvan Prep O\ndiezelfde Pron O\nHogeschool N B-ORG\n. Punc O\nIn this representation, there is one token per line, each with its part-of-speech tag and its named entity tag. Based on this training corpus, we can construct a tagger that can be used to label new sentences; and use the nltk.chunk.conlltags2tree() function to convert the tag sequences into a chunk tree.\nNLTK provides a classifier that has already been trained to recognize named entities, accessed with the function nltk.ne_chunk(). If we set the parameter binary=True, then named entities are just tagged as NE; otherwise, the classifier adds category labels such as PERSON, ORGANIZATION, and GPE.\n>>> sent = nltk.corpus.treebank.tagged_sents()[22]\n>>> print nltk.ne_chunk(sent, binary=True)\n(S\nThe/DT\n(NE U.S./NNP)\nis/VBZ\none/CD\n...\naccording/VBG\nto/TO\n(NE Brooke/NNP T./NNP Mossman/NNP)\n...)\n>>> print nltk.ne_chunk(sent)\n(S\nThe/DT\n(GPE U.S./NNP)\nis/VBZ\none/CD\n...\naccording/VBG\nto/TO\n(PERSON Brooke/NNP T./NNP Mossman/NNP)\n...)"}
{"content2":"郑捷2017年电子工业出版社出版的图书《NLP汉语自然语言处理原理与实践》\n第1章 中文语言的机器处理 1\n1.1 历史回顾 2\n1.1.1 从科幻到现实 2\n1.1.2 早期的探索 3\n1.1.3 规则派还是统计派 3\n1.1.4 从机器学习到认知计算 5\n1.2 现代自然语言系统简介 6\n1.2.1 NLP流程与开源框架 6\n1.2.2 哈工大NLP平台及其演示环境 9\n1.2.3 StanfordNLP团队及其演示环境 11\n1.2.4 NLTK开发环境 13\n1.3 整合中文分词模块 16\n1.3.1 安装Ltp Python组件 17\n1.3.2 使用Ltp 3.3进行中文分词 18\n1.3.3 使用结巴分词模块 20\n1.4 整合词性标注模块 22\n1.4.1 Ltp 3.3词性标注 23\n1.4.2 安装StanfordNLP并编写Python接口类 24\n1.4.3 执行Stanford词性标注 28\n1.5 整合命名实体识别模块 29\n1.5.1 Ltp 3.3命名实体识别 29\n1.5.2 Stanford命名实体识别 30\n1.6 整合句法解析模块 32\n1.6.1 Ltp 3.3句法依存树 33\n1.6.2 StanfordParser类 35\n1.6.3 Stanford短语结构树 36\n1.6.4 Stanford依存句法树 37\n1.7 整合语义角色标注模块 38\n1.8 结语 40\n第2章 汉语语言学研究回顾 42\n2.1 文字符号的起源 42\n2.1.1 从记事谈起 43\n2.1.2 古文字的形成 47\n2.2 六书及其他 48\n2.2.1 象形 48\n2.2.2 指事 50\n2.2.3 会意 51\n2.2.4 形声 53\n2.2.5 转注 54\n2.2.6 假借 55\n2.3 字形的流变 56\n2.3.1 笔与墨的形成与变革 56\n2.3.2 隶变的方式 58\n2.3.3 汉字的符号化与结构 61\n2.4 汉语的发展 67\n2.4.1 完整语义的基本形式——句子 68\n2.4.2 语言的初始形态与文言文 71\n2.4.3 白话文与复音词 73\n2.4.4 白话文与句法研究 78\n2.5 三个平面中的语义研究 80\n2.5.1 词汇与本体论 81\n2.5.2 格语法及其框架 84\n2.6 结语 86\n第3章 词汇与分词技术 88\n3.1 中文分词 89\n3.1.1 什么是词与分词规范 90\n3.1.2 两种分词标准 93\n3.1.3 歧义、机械分词、语言模型 94\n3.1.4 词汇的构成与未登录词 97\n3.2 系统总体流程与词典结构 98\n3.2.1 概述 98\n3.2.2 中文分词流程 99\n3.2.3 分词词典结构 103\n3.2.4 命名实体的词典结构 105\n3.2.5 词典的存储结构 108\n3.3 算法部分源码解析 111\n3.3.1 系统配置 112\n3.3.2 Main方法与例句 113\n3.3.3 句子切分 113\n3.3.4 分词流程 117\n3.3.5 一元词网 118\n3.3.6 二元词图 125\n3.3.7 NShort算法原理 130\n3.3.8 后处理规则集 136\n3.3.9 命名实体识别 137\n3.3.10 细分阶段与最短路径 140\n3.4 结语 142\n第4章 NLP中的概率图模型 143\n4.1 概率论回顾 143\n4.1.1 多元概率论的几个基本概念 144\n4.1.2 贝叶斯与朴素贝叶斯算法 146\n4.1.3 文本分类 148\n4.1.4 文本分类的实现 151\n4.2 信息熵 154\n4.2.1 信息量与信息熵 154\n4.2.2 互信息、联合熵、条件熵 156\n4.2.3 交叉熵和KL散度 158\n4.2.4 信息熵的NLP的意义 159\n4.3 NLP与概率图模型 160\n4.3.1 概率图模型的几个基本问题 161\n4.3.2 产生式模型和判别式模型 162\n4.3.3 统计语言模型与NLP算法设计 164\n4.3.4 极大似然估计 167\n4.4 隐马尔科夫模型简介 169\n4.4.1 马尔科夫链 169\n4.4.2 隐马尔科夫模型 170\n4.4.3 HMMs的一个实例 171\n4.4.4 Viterbi算法的实现 176\n4.5 最大熵模型 179\n4.5.1 从词性标注谈起 179\n4.5.2 特征和约束 181\n4.5.3 最大熵原理 183\n4.5.4 公式推导 185\n4.5.5 对偶问题的极大似然估计 186\n4.5.6 GIS实现 188\n4.6 条件随机场模型 193\n4.6.1 随机场 193\n4.6.2 无向图的团(Clique)与因子分解 194\n4.6.3 线性链条件随机场 195\n4.6.4 CRF的概率计算 198\n4.6.5 CRF的参数学习 199\n4.6.6 CRF预测标签 200\n4.7 结语 201\n第5章 词性、语块与命名实体识别 202\n5.1 汉语词性标注 203\n5.1.1 汉语的词性 203\n5.1.2 宾州树库的词性标注规范 205\n5.1.3stanfordNLP标注词性 210\n5.1.4 训练模型文件 213\n5.2 语义组块标注 219\n5.2.1 语义组块的种类 220\n5.2.2 细说NP 221\n5.2.3 细说VP 223\n5.2.4 其他语义块 227\n5.2.5 语义块的抽取 229\n5.2.6 CRF的使用 232\n5.3 命名实体识别 240\n5.3.1 命名实体 241\n5.3.2 分词架构与专名词典 243\n5.3.3 算法的策略——词典与统计相结合 245\n5.3.4 算法的策略——层叠式架构 252\n5.4 结语 259\n第6章 句法理论与自动分析 260\n6.1 转换生成语法 261\n6.1.1 乔姆斯基的语言观 261\n6.1.2 短语结构文法 263\n6.1.3 汉语句类 269\n6.1.4 谓词论元与空范畴 274\n6.1.5 轻动词分析理论 279\n6.1.6 NLTK操作句法树 280\n6.2 依存句法理论 283\n6.2.1 配价理论 283\n6.2.2 配价词典 285\n6.2.3 依存理论概述 287\n6.2.4 Ltp依存分析介绍 290\n6.2.5 Stanford依存转换、解析 293\n6.3 PCFG短语结构句法分析 298\n6.3.1 PCFG短语结构 298\n6.3.2 内向算法和外向算法 301\n6.3.3 Viterbi算法 303\n6.3.4 参数估计 304\n6.3.5 Stanford的PCFG算法训练 305\n6.4 结语 310\n第7章 建设语言资源库 311\n7.1 语料库概述 311\n7.1.1 语料库的简史 312\n7.1.2 语言资源库的分类 314\n7.1.3 语料库的设计实例：国家语委语料库 315\n7.1.4 语料库的层次加工 321\n7.2 语法语料库 323\n7.2.1 中文分词语料库 323\n7.2.2 中文分词的测评 326\n7.2.3 宾州大学CTB简介 327\n7.3 语义知识库 333\n7.3.1 知识库与HowNet简介 333\n7.3.2 发掘义原 334\n7.3.3 语义角色 336\n7.3.4 分类原则与事件分类 344\n7.3.5 实体分类 347\n7.3.6 属性与分类 352\n7.3.7 相似度计算与实例 353\n7.4 语义网与百科知识库 360\n7.4.1 语义网理论介绍 360\n7.4.2 维基百科知识库 364\n7.4.3 DBpedia抽取原理 365\n7.5 结语 368\n第8章 语义与认知 370\n8.1 回顾现代语义学 371\n8.1.1 语义三角论 371\n8.1.2 语义场论 373\n8.1.3 基于逻辑的语义学 376\n8.2 认知语言学概述 377\n8.2.1 象似性原理 379\n8.2.2 顺序象似性 380\n8.2.3 距离象似性 380\n8.2.4 重叠象似性 381\n8.3 意象图式的构成 383\n8.3.1 主观性与焦点 383\n8.3.2 范畴化：概念的认知 385\n8.3.3 主体与背景 390\n8.3.4 意象图式 392\n8.3.5 社交中的图式 396\n8.3.6 完形：压缩与省略 398\n8.4 隐喻与转喻 401\n8.4.1 隐喻的结构 402\n8.4.2 隐喻的认知本质 403\n8.4.3 隐喻计算的系统架构 405\n8.4.4 隐喻计算的实现 408\n8.5 构式语法 412\n8.5.1 构式的概念 413\n8.5.2 句法与构式 415\n8.5.3 构式知识库 417\n8.6 结语 420\n第9章 NLP中的深度学习 422\n9.1 神经网络回顾 422\n9.1.1 神经网络框架 423\n9.1.2 梯度下降法推导 425\n9.1.3 梯度下降法的实现 427\n9.1.4 BP神经网络介绍和推导 430\n9.2 Word2Vec简介 433\n9.2.1 词向量及其表达 434\n9.2.2 Word2Vec的算法原理 436\n9.2.3 训练词向量 439\n9.2.4 大规模上下位关系的自动识别 443\n9.3 NLP与RNN 448\n9.3.1Simple-RNN 449\n9.3.2 LSTM原理 454\n9.3.3 LSTM的Python实现 460\n9.4 深度学习框架与应用 467\n9.4.1 Keras框架介绍 467\n9.4.2 Keras序列标注 471\n9.4.3 依存句法的算法原理 478\n9.4.4 Stanford依存解析的训练过程 483\n9.5 结语 488\n第10章 语义计算的架构 490\n10.1 句子的语义和语法预处理 490\n10.1.1 长句切分和融合 491\n10.1.2 共指消解 496\n10.2 语义角色 502\n10.2.1 谓词论元与语义角色 502\n10.2.2PropBank简介 505\n10.2.3 CPB中的特殊句式 506\n10.2.4 名词性谓词的语义角色 509\n10.2.5PropBank展开 512\n10.3 句子的语义解析 517\n10.3.1 语义依存 517\n10.3.2 完整架构 524\n10.3.3 实体关系抽取 527\n10.4 结语 531 [29]"}
{"content2":"重磅|大快搜索上榜2018中国大数据企业50强\n——斩获“2018中国大数据产业生态大会”多项大奖，成年度最大黑马\n近日，由工信部中国电子信息产业发展研究院主办，中国大数据产业生态联盟承办的2018（第三届）中国大数据产业生态大会在北京盛大召开，工信部党组成员、总工程师张锋、工信部原副部长、北京大学教授杨学山等中央及地方主管领导、行业专家、企业领袖、知名投资人近千人汇聚一堂，深入探讨深挖数据智能、助推数字经济发展的有效路径。青岛大数据企业大快搜索受邀出席，并斩获多项大奖。\n入选“2018中国大数据企业50强”\n“2018中国大数据企业50强”从企业规模（15%）、研发投入（15%）、创新能力（20%）、应用案例（20%）、产品及方案成熟度（15%）、投资及发展潜力（15%）等不同维度设置评价指标体系。同时，结合《2018中国大数据产业生态地图暨中国大数据产业发展白皮书》的调研工作，经过由政府主管部门的相关领导、大数据产业资深专家、行业知名用户CIO、行业媒体总编以及第三方测评机构专家组成的专家评审组的严格评审，由最初的初步筛选，到近1000家企业成功入围，到200家优秀企业进入专家评审，最终评审出“2018中国大数据企业50强”。大快搜索凭借在大数据、人工智能、自然语言处理等领域的卓越能力和发展潜力入选“2018中国大数据企业50强”。\n大快搜索跻身“2018中国大数据50强”\n获评“中国最具投资价值大数据企业排行榜·TOP100”\n会上，主办方重磅发布的《2018中国大数据产业发展白皮书》重点呈现：2018中国大数据产业生态地图、中国最具投资价值大数据企业排行榜·TOP100及中国大数据产业应用十大爆发领域等。\n其中，在2018中国大数据产业生态地图中，大快搜索全面覆盖基础支撑、数据服务及融合应用三大生态环节，涉及大数据基础平台、数据采集及预处理及政府大数据解决方案等；同时，经过企业估值/市值、营收状况、创新投入、专利数量、产品竞争力、企业发展潜力、领导层能力等多维度的定量与定性评比及专家打分，大快搜索获“中国最具投资价值大数据企业排行榜·TOP100”。\n大快搜索获“中国最具投资价值大数据企业排行榜·TOP100”\n总工程师汤连杰荣获“2018中国数据大工匠\n2016年3月，政府工作报告中提出要“培育精益求精的工匠精神”，工匠精神列入政府工作报告。工匠精神是指精雕细琢，精益求精的精神理念。对于大数据企业的产品、解决方案以及服务进行精雕细琢，坚持精耕细作，精益求精，就是大数据领域的工匠精神。本次评选推选出兼具技术特长、影响力、领导力、贡献力、实践力等综合能力的中国数据大工匠，为培育国际领先的大数据核心龙头企业储备能量。大快搜索总工程师汤连杰凭借多年在Hadoop国产化、易用性的卓越贡献，荣获“2018中国数据大工匠”。\n获奖人：汤连杰——青岛大快搜索计算技术股份有限公司总工程师\n颁奖词：由汤连杰主持开发的DKHadoop解决了Hadoop技术不透明、数据安全存在隐患的痛点，并已得到了市场的广泛认可。在汤连杰的带领下，大快搜索为我国大数据软件市场提供了安全、可控的国产化大数据基础软件。\n大数据公共资源管理（管控）平台、石化预警预判系统项目入选《2018中国大数据应用典型案例集》\n《2018中国大数据应用典型案例集》重点展示充分体现技术先进性/实用性、探索大数据新型商业服务模式、引领行业转型升级、促进大数据新技术/新业态/新模式产业化、优化大数据产业发展环境、带动产业蓬勃发展等的优秀产品、服务平台及应用解决方案。由大快搜索研发的大数据公共资源管理（管控）平台、石化预警预判系统项目凭借技术领先性和引领行业转型升级，成功入选《2018中国大数据应用典型案例集》\nHanLP获评“2018中国数据星技术”\nHanLP是大快搜索高级研究员何晗（Hankcs）主持开发的面向象形文字的自然语言处理技术。具备功能完善、性能高效、架构清晰、语料时新、可自定义的特点。目前HanLP在GitHub上的star数已达到8000，高于老牌的NLTK（6.4k）和斯坦福大学开源的CoreNLP（4.3k）以及哈尔滨工业大学开源的LTP（1.4k），是目前全球开发者用户最多、最受欢迎、活跃度最高的开源自然语言处理项目。凭借技术领先性和用户活跃度，HanLP获评《2018中国数据星技术》\nHanLP获评“2018中国数据星技术”\n获评：“2018中国大数据基础软件领域创新企业&最佳产品”\n大快搜索自成立以来，始终致力于大数据与人工智能核心技术自主研发，在大数据和人工智能基础软件、自然语言处理技术、大数据标准开发库等领域均取得了傲人的成绩。目前大快的合作客户已超过200余家，其中：上市企业 43 家，中国软件百强 23 家。大快的核心技术，帮助众多大数据与人工智能项目，实现更高的计算性能提升。大快深度研发、整合的DK.ES、DK.Sqoop Pro等产品也应用在了大量的项目中，并取得了良好的市场反馈。本次大会，获评“2018中国大数据基础软件领域创新企业&最佳产品”\n大快搜索获评“2018中国大数据基础软件领域创新企业&最佳产品”\n此次大会获得的荣誉既是对以往工作的肯定，也是对大快未来成绩的一种期冀。大快将继续以核心技术自主可控为发展方向，为更多合作伙伴提供更高性能、更具可用性、更安全的国产化大数据与人工智能核心技术，助推中国大数据生态构建、为政府、社会提供自主可控、高效易用的大数据底层技术。"}
{"content2":"作为一名Android开发工程师，身边总有些同行很焦虑，看着人工智能越来越火，总是担心Android要不行了，所以，我们需要转行么？Android还能走多久？其实，无论是对于Android还是iOS开发者而言，我们更应该做的是稳固提升自己的技术，活到老学到老，时刻与不断发展的框架、标准和范式保持同步。同时，还要能活学活用，在工作中使用最合适的工具，以提高工作效率。随着机器学习在越来越多的应用程序中寻得了一席之地，越来越多的程序员加入AI领域，那么，入行AI领域需要哪些技能呢？\n人工智能到底有多火\n我相信大家之所以能来看这篇文章，也间接说明了人工智能这几年的火爆。在开发一个App越来越方便的今天，聊天可以用环信SDK，推送可以用Jpush，地图可以用百度和高德，集成社群公会系统可以用GangSDK……开发的门槛越来越低，多方位SDK的集成已经给我们省去了大量的时间人力成本，拔高和学习，我们需要从哪些方面入手？\n自从基于深度学习技术的算法2012年在ImageNet比赛中获得冠军以来，深度学习先是席卷了整个学术界，后又在工业界传播开来，一瞬间各大企业如果没有AI部门都不好意思对外宣传了。BAT中，百度宣布“All In AI”，阿里建立了达摩院及AI实验室，腾讯也在前不久会议上宣布“Ai In All”，并具有腾讯优图、AI Lab和微信AI实验室。2017年7月20日，国务院发布《新一代人工智能发展规划》，将人工智能上升为国家战略，为中国人工智能产业做出战略部署，对重点任务做出明确解析，抢抓重大机遇，构筑我国人工智能发展的先发优势。\n技术的发展往往遵循一个可预期的模式，先是萌芽，然后炒作，而后幻灭，接着才是技术成熟后的稳步爬升，最后到达应用高峰。研究分析机构Gartner每年都会推出这样一个分析新兴技术发展趋势的技术炒作周期报告。前段时间，Gartner发布了2017年的新兴技术炒作周期报告，报告聚焦了前端、后端与平台发展的三大趋势，提出了AI将无所不在（人工智能），体验将透明化和沉浸式（AR、VR），以及平台全面数字化（区块链）的观点，建议企业架构师和对技术创新有追求的人员应该积极探索和关注这三大趋势，从而了解掌握这三大趋势对自己公司和自己职业发展的未来影响。简单来说这三大趋势分别对应于括号中我备注的大家平时耳熟能详的词语。从曲线图可以看出，物联网、虚拟助手、深度学习、机器学习、无人车、纳米电子、认知计算以及区块链正处在炒作的高峰。实际上AR、VR属于计算机视觉，也可以归于人工智能范畴，因此总体上来说，未来人工智能将无处不在。\nGartner把深度学习、强化学习、常规人工智能、无人车、认知计算、无人机、会话式用户接口、机器学习、智能微尘、智能机器人、智能工作环境等均列为AI技术范畴。在人机大战等吸引眼球的活动助推下，很多AI技术目前正处在炒作的高峰期。比如深度学习、机器学习、认知计算以及无人车等。对比2016年的炒作周期曲线可以发现，有些太过超前的概念仍然不愠不火，比如智能微尘。有些概念因为炒作过高已经迅速进入到了幻灭期，比如商用无人机去年还处在触发期，今年就已经接近幻灭期边缘了。相对而言，正处在炒作高峰的深度学习和机器学习技术有望在2到5年内达到技术成熟和模式成熟。\n除了人工智能这么火之外，对于软件工程师，尤其是移动端开发工程师，有一点我们更要关注，那就是移动端深度学习逐渐成为新的深度学习研究趋势。未来会有越来越多的基于深度学习的移动端应用出现，作为开发者的我们了解深度学习更有助于我们开发出优秀的应用，同时提升自身能力，积极抓住机会，应对未来各种变化。\n什么是机器学习（Machine Learning，ML）？\n深度学习的基础是机器学习，事实上深度学习只是机器学习的一个分支。因此我们要入门深度学习就要先了解一些机器学习的基础知识。机器学习最基本的做法，是使用算法来解析数据、从中学习，然后对真实世界中的事件做出决策和预测。与传统的为解决特定任务、硬编码的软件程序不同，机器学习是用大量的数据来“训练”，通过各种算法从数据中学习如何完成任务。\n有人曾举过一个例子，很形象生动，当你使用手机的语音识别进行唤醒时，有没有想过实现这一功能的全部内部流程呢？我们日常交互的大部分计算机程序，都可以使用最基本的命令来实现，但是基于机器学习的程序却没有那么简单，想象下如何写一个程序来回应唤醒词，例如“Okay，Google”，“Siri”，和“Alexa”。如果在一个只有你自己和代码编辑器的房间里，仅使用最基本的指令编写这个程序，你该怎么做？不妨思考一下……这个问题非常困难。你可能会想像下面的程序：\nifinput_command=='Okey,Google':\nrun_voice_assistant()\n但实际上，你能拿到的只有麦克风里采集到的原始语音信号，可能是每秒44,000个样本点。怎样才能识别出语音内容？或者简单点，判断这些信号中是否包含唤醒词。\n如果你被这个问题难住了，不用担心。这就是我们为什么需要机器学习。\n虽然我们不知道怎么告诉机器去把语音信号转成对应的字符串，但我们自己可以。换句话说，就算你不清楚怎么编写程序，好让机器识别出唤醒词“Alexa”，你自己完全能够 识别出“Alexa”这个词。由此，我们可以收集一个巨大的数据集（dataset），里面包含了大量语音信号，以及每个语音型号是否对应我们需要的唤醒词。使用机器学习的解决方式，我们并非直接设计一个系统去准确地辨别唤醒词，而是写一个灵活的程序，并带有大量的参数（parameters）。通过调整这些参数，我们能够改变程序的行为。我们将这样的程序称为模型。总体上看，我们的模型仅仅是一个机器，通过某种方式，将输入转换为输出。在上面的例子中，这个模型的输入是一段语音信号，它的输出则是一个回答{yes, no}，告诉我们这段语音信号是否包含了唤醒词。\n如果我们选择了正确的模型，必然有一组参数设定，每当它听见“Alexa”时，都能触发yes的回答；也会有另一组参数，针对“Apricot”触发yes。我们希望这个模型既可以辨别“Alexa”，也可以辨别“Apricot”，因为它们是类似的任务。这时候你大概能猜到了，如果我们随机地设定这些参数，模型可能无法辨别“Alexa”，“Apricot”，甚至任何英文单词。在而大多数的深度学习中，学习就是指在训练过程中更新模型的行为（通过调整参数）。\n换言之，我们需要用数据训练机器学习模型，其过程通常如下：\n1.初始化一个几乎什么也不能做的模型；\n2.抓一些有标注的数据集（例如音频段落及其是否为唤醒词的标注）；\n3.修改模型使得它在抓取的数据集上能够更准确执行任务；\n4.重复以上步骤2和3，直到模型看起来不错。\n什么是机器学习算法？从本质上讲，机器学习采用了可以从数据中学习和预测数据的算法。这些算法通常来自于统计学，从简单的回归算法到决策树等等。\n什么是机器学习模型？一般来说，它是指在训练机器学习算法后创建的模型构件。一旦有了一个经过训练的机器学习模型，你就可以用它来根据新的输入进行预测。机器学习的目的是正确训练机器学习算法来创建这样的模型。\n机器学习已广泛应用于数据挖掘、计算机视觉、自然语言处理、生物特征识别、搜索引擎、医学诊断、检测信用卡欺诈、证券市场分析、DNA序列测序、语音和手写识别、战略游戏和机器人等领域。虽然深度学习技术的发展，也促进了语音和文本领域的发展，但变化最显著的还是属于计算机视觉领域。而且由于作者是做计算机视觉的，因此这里也没法深入介绍语音和自然语言处理领域的过多细节，就简要介绍下计算机视觉领域的技术发展和相关的应用，后续的实验环节，大部分也会是基于深度学习的图像应用为主。"}
{"content2":"斯坦福大学CS224n（全称：深度学习与自然语言处理）是自然语言处理领域很受欢迎的课程，由 Chris Manning 和 Richard Socher 主讲\n自然语言处理是理解复杂人类语言交际的关键人工智能技术。本系列讲座全面介绍了应用于NLP的前沿研究，这种方法最近在许多不同的NLP任务中获得了很高的性能，包括问答和机器翻译。\n官网课件：http://web.stanford.edu/class/cs224n/syllabus.html\nB站视频：https://space.bilibili.com/23852932/#!/channel/detail?cid=11177\nYouTube视频：https://www.youtube.com/playlist?list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6\n学习笔记：http://www.hankcs.com/nlp/cs224n-introduction-to-nlp-and-deep-learning.html\n官方笔记：https://github.com/stanfordnlp/cs224n-winter17-notes\n▍课程表\n简介\nWord2Vec词向量表示\nGloVe词向量表示\n词窗分类与神经网络\n反向传播\n依存解析\nTensorFlow入门\nRNN与语言模型\n机器翻译 Advanced Recurrent LSTMs & GRUs\n期中回顾\n神经机器翻译与注意模型\nGRU & NMT进阶\n端到端语音处理模型\n卷积神经网络及其变种\n树递归神经网络和句法分析\n共指消解\n动态神经网络自动问答\nNLP的问题及可能的解决框架\n聚焦深度学习NLP局限性\n内容简介\n第1讲：自然语言处理与深度学习\n第1讲介绍了自然语言处理（NLP）的概念和当前的NLP研究面临的问题，然后介绍将词表示为数字向量的概念，以及设计词向量的流行方法。\n关键词：自然语言处理、词向量、奇异值分解、skip-gram模型、 连续词袋模型（Continuous Bag-Of-Words，CBOW）、负采样、 Hierarchical Softmax、Word2Vec\n第2讲：词向量表示：word2vec\n第2讲继续讨论词向量的概念，以及流行的设计词向量的方法。\n第3讲 | GloVe：用于词汇表示的全局向量\n第3讲介绍了用于训练词向量的 GloVe 模型。本讲接着通过观察如何评估词向量（又称词嵌入）来拓展对它的理解。作为评估技术的一种，接着我们讨论了词的类比问题（word analogies），以及如何利用类比来调整词嵌入。然后我们讨论了训练模型的权重/参数，以及外部任务的词向量。最后，我们鼓励用人工神经网络作为自然语言处理任务的一类模型。\n关键词：GloVe、内部和外部评估、超参数对类比评估任务的影响、人类判断与词向量距离的相关性、使用上下文处理歧义、窗口分类\n第4讲：词窗口分类和神经网络\n第4讲介绍了单层和多层神经网络，以及如何它们进行分类任务。\n关键词：神经网络、前向计算、反向传播、神经元单元、最大边界损失、梯度检查、Xavier参数初始化、学习率、Adagrad\n第5讲：反向传播与项目建议\n第5讲讨论了如何使用反向传播这一分布式梯度下降技术来训练神经网络。\n第6讲：依存分析\n第6讲讨论依存分析，这是给定一个输入句子S，分析其句法依存结构的任务。依存分析器的输出是一个依存关系树，其中输入句子的词汇与依存关系类型相关联。\n关键词：依存分析\n第7讲：TensorFlow简介\n第7讲介绍了TensorFlow。TensorFlow是一个开源软件库，用于使用数据流图（data flow graphs）进行数值计算。它最初由谷歌大脑团队开发，用于进行机器学习和深度神经网络研究。\n关键词：TensorFlow\n第8讲：循环神经网络和语言模型\n第8讲介绍传统语言模型、RNN，以及RNN语言模型。本讲还回顾了一些重要的训练问题和技巧，用于其他序列任务的RNN，以及双向RNN（bidirectional RNNs）和deep RNNs。\n第9讲：机器翻译、LSTM和GRU\n第9讲回顾了前部分课程的重要概念，机器翻译的概念，以及用RNN模型处理机器翻译。\n关键词：语言模型、RNN、双向RNN、deep RNN、GRU、LSTM\n第10讲：神经机器翻译和注意力模型\n第10讲介绍了翻译、机器翻译和神经机器翻译，重点介绍谷歌的新 NMT模型，以及基于注意力的序列模型和序列模型解码器。\n第11讲：门控循环单元和NMT\n第11讲介绍了GRAT / LSTM之类的门控循环单元，然后介绍机器翻译的评估，处理大量词汇输出，以及 sub-word 模型和 character-based 模型。\n关键词：Seq2Seq、注意力机制、神经机器翻译，语音处理\n第12讲：语音处理的端到端模型\n第12讲介绍传统语音识别系统和端到端模型，包括CTC模型（ Connectionist Temporal Classification）和LAS（Listen Attend and Spell），这是一种用于语音识别的序列到序列模型。\n第13讲：卷积神经网络\n第13讲提供了Azure和GPU的一个小教程，然后介绍“Character-Aware Neural Language Models”。本讲还提到CNN的一些变体，以及比较了 BoV、RNN、CNN这些句子模型。\n第14讲：树递归神经网络和parsing分析\n第14讲介绍了语义合成性（compositionality）和结构预测，利用一个简单的树RNN：parsing。重点介绍“对话生成的深度强化学习”。\n关键词：RNN、递归神经网络、MV-RNN、RNTN\n第15讲：指代消解（Coreference Resolution）\n第15讲通过一个实例来介绍什么是指代（coreference），涉及的研究是“Summarizing Source Code”，这一研究介绍了指代消解和神经网络指代消解。\n第16讲：用于问题回答的动态神经网络\n第16讲介绍了“是否所有NLP任务都可以被视为问答问题”这一问题。\n关键词：指代消解、动态神经网络\n第17讲：NLP中的重要问题和NLP的架构\n第17讲介绍了高效树递归模型SPINN和SNLI，重点介绍“Learning to compose neural networks for QA”这一研究。\n第18讲：NLP中深度学习的局限性\n第18讲讨论了NLP中深度学习的局限，提供了一些presentations。"}
{"content2":"自然语言处理和图像处理不同，作为人类抽象出来的高级表达形式，它和图像、声音不同，图像和声音十分直觉，比如图像的像素的颜色表达可以直接量化成数字输入到神经网络中，当然如果是经过压缩的格式jpeg等必须还要经过一个解码的过程才能变成像素的高阶矩阵的形式，而自然语言则不同，自然语言和数字之间没有那么直接的相关关系，也就不是那么容易作为特征输入到神经网络中去了，所以，用神经网络处理自然语言，不可避免的在数据预处理方面更加繁琐，也更加细致！自然语言处理的另外一个不同之处在于语言之间的相关关系，举一个最简单的例子，在做智能助理机器人的时候，一句“我将在上午十点到达北京”和“我将在上午十点离开北京” 如果你只考虑每个独立的词汇的话，那么“北京”到底是作为始发地还是目的地是不得而知的，也就是说你必须得联系上下文才能够更好的理解整个句子的意思！这也是自然语言的特别之处！当然针对语言的这种特性，也有相应的用于处理的网络——RNN（recurrent neural network）循环神经网络！\n作为分类到Tensorflow编程实战里的一篇博客，当然以解释代码为主，具体的理论部分这里不过多解释，开门见山，本次处理的数据集是PTB数据集，它是目前语言模型学习中使用的最为广泛的数据集，PTB数据集的下载地址是： http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n将数据集解压之后将会得到好几个文件夹，重点看/data文件夹下的三个文件，ptb.test.txt,   ptb.train.txt,   ptb.valid.txt 这三个数据文件已经经过了预处理，相邻的单词之间用空格隔开，以ptb.train.txt为例，看一下它里面的具体内容：\nno it was n't black monday  but while the new york stock exchange did n't fall apart friday as the dow jones industrial average plunged N points most of it in the final hour it barely managed to stay this side of chaos\nsome circuit breakers installed after the october N crash failed their first test traders say unable to cool the selling panic in both stocks and futures\nthe N stock specialist firms on the big board floor\n数据集中包含了9998个词汇，加上稀有词语的特殊符号 <unk>和语句的结束标记，一共是10000个！ 为了能够将数据输入到神经网络，我们首先需要做的就是这10000个词汇按照其出现的频率进行编号，形成词汇表，然后按照词汇表的对应关系，将train/test/valid数据集分别转换成编号的形式！\n按照单词出现的频率进行编号的代码如下：\n1 import codecs 2 import collections 3 from operator import itemgetter 4 5 RAW_DATA = \"C:\\\\Users\\\\Yang\\\\Desktop\\\\nlp\\\\ptb.train.txt\" #数据的输入路径/ 6 VOCAB_OUTPUT =\"ptb.vocab\" #词汇表的输出路径 7 8 counter = collections.Counter() #counter 顾名思义是一个计数器 9 10 with codecs.open(RAW_DATA,\"r\",\"utf-8\") as f: 以read的方式，utf-8编码的形式打开上述路径 11 for line in f: #读行 12 for word in line.strip().split(): #以空格作为划分，将文件里面的每一个词汇都切开！ strip()默认用于去掉收尾的空格和换行符 13 counter[word] += 1 #统计单词出现的次数 14 15 sorted_word_to_cnt = sorted(counter.items(),key=itemgetter(1),reverse=True) 按照itemgetter(1)属性进行排序，应该就是按照单词出现的次数排序，例如：and:981 16 sorted_words = [x[0] for x in sorted_word_to_cnt] #之所以取x[0]是因为格式是 the:356 这种形式，x[0]就是为了将单词取出来 排好序的word 17 18 sorted_words = [\"<eos>\"] + sorted_words #将句子的结束符添加到排好序的list中 19 #因为PTB数据中已经将低频词汇替换成了<\"unk\">所以下面这步没必要 <\"eos\">是句子结束符 <\"sos\">是句子开始符 <\"unk\">是句子的低频词汇 20 #sorted_words = [\"<unk>\",\"<sos>\",\"<eos>\"] + sorted_words 21 #if len(sorted_words) >10000: 22 #sorted_words = sorted_words[:10000] 标红的这三句没必要出现！ 23 24 with codecs.open(VOCAB_OUTPUT,\"w\",'utf-8') as file_output : 25 26 for word in sorted_words: 27 file_output.write(word + \"\\n\") #将排序好的词汇再写回输出的文件目录里面，那么就算完成了词汇对应表的构建\n写完了词汇表的构建代码之后，我们还需要一个将ptb.train/test/valid.txt文件转换成对应的编号的过程！ 每一个词汇对应的编号就是其在词汇表里面对应的行号！ 注意到之前每一个word的输出后面都跟着\"\\n\"\n下面就来实现将单词转换成对应的编号的部分的代码：\n1 import codecs 2 import sys 3 4 RAW_DATA = 'C:\\\\Users\\\\Yang\\\\Desktop\\\\nlp\\\\data\\\\ptb.valid.txt' #这里是待转换的文件的目录/相应的改成ptb.test.txt/ptb.train.txt可以用于其他的转换 5 VOCAB = 'ptb.vocab' #词汇表的目录 6 OUTPUT_DATA = 'ptb.valid' 用于输出的目录 7 8 9 with codecs.open(VOCAB,'r','utf-8') as f_vocab: #首先就是将词汇表以read和utf-8编码的方式打开 10 vocab = [w.strip() for w in f_vocab.readlines()] #我怎么感觉这里只是读到了一行然后进行收尾空格换行符去掉的处理？？？ 11 #哦，我明白了 因为VOCAB中词汇的存储格式就是一个单词占一行，所以出来的就是一个个单词而不需要.split() 12 word_to_id = {k:v for (k,v) in zip(vocab,range(len(vocab)))} #转换成了单词序号的字典形式 这里是完成了词汇和其对应编号的对应！ 13 14 #将词汇转换成了对应的行号序号 15 def get_id(word): 16 return word_to_id[word] if word in word_to_id else word_to_id[\"<unk>\"] #如果是在词汇表里面的就将它转换成对应的编号，否则的话就转换成unk对应的编号 17 18 fin = codecs.open(RAW_DATA,\"r\",\"utf-8\") #打开待转换文件 19 fout = codecs.open(OUTPUT_DATA,\"w\",'utf-8') #输出文件目录 20 21 for line in fin: 22 words = line.strip().split() + [\"<eos>\"] #打开的文件读取每一行然后首尾去除换行符和空格之后对空格进行切分，在末尾加上eos! 23 out_line = ' '.join([str(get_id(w)) for w in words]) +'\\n' 对其的行进行转换 24 fout.write(out_line) 然后写入到对应的输出文件里面 25 26 fin.close() 27 fout.close()\n我们来看一下运行完这个代码之后我们的ptb.train.txt变成了什么样子：\n9994 9996 9974 9999 9972 9978 9981 9993 9977 9973 9985 9998 9992 9971 9997 9990 9995 9970 9989 9987 9988 9975 9980 9986 0\n没错，全部变成了词汇表对应的行号编号的形式！\n进行完上述两步处理之后，我们已经完成了数据预处理部分的一半了，接下来我们需要考虑的问题就是，处理后的编码数据不可能直接输入到网络里面去，网络接受的一般都是许多batch，但是在将数据打包成batch\n的时候，又有一个十分严峻的问题需要我们考虑，那就是到底以多长的长度来打包数据呢？ 每条句子的长度是不一样的，该如何打包呢？ 有两种比较常见的做法，一种是根据batch_size的大小，选取里面最长的句子为\n参考，剩下的padding成统一的长度！长度统一之后就可以用于batch了！\n而PTB数据是一整段一整段文本，如果一句一句的进行切分的话，会破坏掉数据之间的上下文的关联，而语言模型为了利用上下文信息，必须将前面的信息传递到后面的句子中去，所以PTB通常采用的是另外一种Batch的方法！\n该方法将长序列切割成固定长度的子序列，然后循环神经网络处理完这个子序列之后将最后一个隐藏层的结果复制到下一个序列中作为初始值，这样在前向计算的过程中效果就等同于一次性的读取完了全部的文档！\n但是如果这样做的话会面临另外一个问题，整个数据流都是串行流动的，这对于可以并行计算的tensorflow来说简直太浪费了，所以，兼顾二者的做法是，根据batch_size的大小将整个文档划分成batch_size部分！然后让batch_size的每一个位置负责一部分数据，如果处理不完，继续横向移动到下一个batch中去，也就是说，对于batch来讲数据之间横向才是连贯的，纵向其实是互不相干的！这样既能够保证数据之间的上下文关系，也能够保证tensorflow可以并行处理数据！\n画一个示意图的话，大概是下面这样："}
{"content2":"按键精灵是一款模拟鼠标键盘动作的软件。通过制作脚本，可以让按键精灵代替双手，自动执行一系列鼠标键盘动作。\n人工智能（Artificial Intelligence），英文缩写为AI。它是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。 人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。人工智能是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。人工智能从诞生以来，理论和技术日益成熟，应用领域也不断扩大，但没有一个统一的定义。\n人工智能是对人的意识、思维的信息过程的模拟。人工智能不是人的智能，但能像人那样思考、也可能超过人的智能。但是这种会自我思考的高级人工智能还需要科学理论和工程上的突破。\n人工智能是一门极富挑战性的科学，从事这项工作的人必须懂得计算机知识，心理学和哲学。人工智能是包括十分广泛的科学，它由不同的领域组成，如机器学习，计算机视觉等等，总的说来，人工智能研究的一个主要目标是使机器能够胜任一些通常需要人类智能才能完成的复杂工作。但不同的时代、不同的人对这种“复杂工作”的理解是不同的。"}
{"content2":"说明：这个贴用于收集笔者能力范围内收集收藏并认为有用的资料，方便各方参考，免去到处找寻之苦，提升信息的交叉引用价值。仅供参考，不作为必然的推荐倾向。如涉及版权等问题请相关人员联系笔者，谢谢。\n|博客|\n龙心尘的博客(http://blog.csdn.net/longxinchen_ml)\n寒小阳的博客(http://blog.csdn.net/han_xiaoyang)\nwepon(http://2hwp.com/)\n面包包包包包包（http://blog.csdn.net/breada）\n仆居（http://blog.csdn.net/kkk584520）\n|人工智能|机器学习|数据挖掘|神经网络|\n手把手入门神经网络系列-2篇-有图有码\n机器学习系列-7篇-有图有码\nNLP(自然语言处理)系列-5篇-有图有码\n利用 Python 练习数据挖掘\n[建议：适合入门；]\n[简介：围绕1个例子；完整的步骤；少量错误；]\n[扩展：关于IRIS数据集的Python分析 (太初）]\n[扩展：IRIS数据集的PCA分析和3D展现]\n用Python做科学计算-基础篇\n[简介：不错的教程]\nscikit-learn的主要模块和基本使用\n[简介：内容精要]\n浅谈神经网络[入门] | 统计与计算[入门]\n|人脸识别|指纹掌纹识别|生物识别|计算机视觉（CV）\nPCA+SVM人脸识别\n深度学习与计算机视觉系列-10篇-有图有码\nPython和OPENVC分析烤箱状态\n从特征描述符到深度学习：计算机视觉发展20年\nPython计算机视觉编程（10章+附录ABC）\n|算法|模型|\n| 朴素贝叶斯分类器 | 朴素贝叶斯分类 | 朴素贝叶斯的三个常用模型(高斯/多项式/伯努利) |\n| 判别模型/生成模型与朴素贝叶斯方法 | 聚类算法k-mean | 聚类算法k-mean | 白化 | PCA(1,2,3,4,5) |\n| 线性回归 | 中文分词软件(14款开源) |  |\n|实用连接|\n| numpy.zeros函数用法 | SCIPY.ORG |numpy中的ndarray方法和属性 | 多维数组ndarray及切片\n| plot绘图 | numpy.linspace | numpy.matrix |  numpy.arange | scipy |\n| sklearn官网API | 大量wheel | Anaconda[简介:极好的PythonIDE!](安装使用) |\n|平台|网站|竞赛|\nAminer（Open Science Platform)  | Kaggle(体验,简介) | DataCastle | 阿里天池 (体会,总结) |\n神经网络实验室(所见即所得的测试谷歌tensorflow)极好！ | 谷歌大脑Google Brain |\n|实用连接|\n|Python数据图表工具 | Python写爬虫查询 |\n[END]"}
{"content2":"好程序员大数据技术盘点 你都知道吗，大数据的概念，指的是无法在一定时间内用常规软件工具对其内容进行抓取、管理和处理的数据集合。而大数据技术，是指从各种各样类型的数据中，快速获得有价值信息的能力。\n第一，数据采集\nETL工具负责将分布的、异构数据源中的数据如关系数据、平面数据文件等抽取到临时中间层后进行清洗、转换、集成，最后加载到数据仓库或数据集市中，成为联机分析处理、数据挖掘的基础。\n第二，数据存取\n关系数据库、NOSQL、SQL等。\n第三，基础架构\n云存储、分布式文件存储等。\n第四，数据处理\n自然语言处理(NLP，Natural Language Processing)是研究人与计算机交互的语言问题的一门学科。处理自然语言的关键是要让计算机\"理解\"自然语言，所以自然语言处理又叫做自然语言理解(NLU，Natural Language Understanding)，也称为计算语言学(Computational Linguistics。一方面它是语言信息处理的一个分支，另一方面它是人工智能(AI, Artificial Intelligence)的核心课题之一。\n第五，统计分析\n假设检验、显著性检验、差异分析、相关分析、T检验、方差分析、卡方分析、偏相关分析、距离分析、回归分析、简单回归分析、多元回归分析、逐步回归、回归预测与残差分析、岭回归、logistic回归分析、曲线估计、因子分析、聚类分析、主成分分析、因子分析、快速聚类法与聚类法、判别分析、对应分析、多元对应分析(最优尺度分析)、bootstrap技术等等。\n第六，数据挖掘\n分类 (Classification)、估计(Estimation)、预测(Prediction)、相关性分组或关联规则(Affinity grouping or association rules)、聚类(Clustering)、描述和可视化、Description and Visualization)、复杂数据类型挖掘(Text, Web ,图形图像，视频，音频等)。\n第七，模型预测\n预测模型、机器学习、建模仿真。\n第八，结果呈现\n云计算、标签云、关系图等。\n其实，关于大数据的技术内容远不止以上内容，欢迎继续关注。"}
{"content2":"文本挖掘或者文档挖掘是一个从非结构化文本信息中获取用户感兴趣或者有用的模式的过程文本挖掘涵盖多种技术,包括信息抽取,信息检索,自然语言处理和数据挖掘技术。它的主要用途是从原本未经使用的文本中提取出未知的知识，但是文本挖掘也是一项非常困难的工作,因为它必须处理那些本来就模糊而且非结构化的文本数据,所以它是一个多学科混杂的领域,涵盖了信息技术、文本分析、模式识别、统计学、数据可视化、数据库技术、机器学习以及数据挖掘等技术。\nNlpir Parser智能语义分析系统是灵玖软件经过多年的研发成果，针对互联网内容处理的需要，融合了自然语言理解、网络搜索和文本挖掘的技术，提供了用于技术二次开发的基础工具集。开发平台由多个中间件组成，各个中间件API可以无缝地融合到客户的各类复杂应用系统之中。\nNlpir Parser智能语义分析系统是一套专门针对原始文本集进行处理和加工的软件，提供了中间件处理效果的可视化展示，也可以作为小规模数据的处理加工工具。用户可以使用该软件对自己的数据进行处理。Nlpir Parser系统在传统的技术的基础上，重点在一下技术方法上做了重大升级，使文本挖掘效率得到了很大的提高：\n1、可训练的自然语言处理和理解方法\n自然语言处理(NLP)是一个计算量非常大的领域,而且目前还没有发现它在文本挖掘中能起到什么巨大的作用.然而,复杂的NLP方法将是有效地进行信息抽取的方法,而信息抽取是文本挖掘中的重要环节.\n2、神经网络方法\n人工智能神经网络能够很好地在自然语言文本中识别高维度结构。神经网络极适合使用在含有噪声、并且有着难以理解的结构和不断变化的属性的数据上,而这些正是文本信息中普遍存在的现象.神经网络方法使用层次化自组织图来对文本进行关键信息抽取.一个自组织图(SOM)是一个无监督的人工神经网络。由许多层神经元构成,开始于输入层,向输出层推进.因为SOM是无监督的,它们拥有自学习的能力.神经网络特别适合用于文本分类.作为以神经网络为基础的系统,不再需要样本的训练。该方法表明:有许多人工智能的数据挖掘的技术可以被结合起来应用于文本的挖掘.\n文本挖掘最大的动机是来自于潜藏于电子形式中的大量的文本数据。利用数据挖掘技术处理公司大量的文本数据,将给企业带来巨大的商业价值。另外人们对于文本挖掘的感兴趣的原因还在于:人们有时候并不知道他们到底要找什么,而挖掘能够从数据库中抽取出许多有用的信息。"}
{"content2":"(转https://blog.csdn.net/gzmfxy/article/details/78994396)\n中文分词是中文文本处理的一个基础步骤，也是中文人机自然语言交互的基础模块，在进行中文自然语言处理时，通常需要先进行分词。本文详细介绍现在非常流行的且开源的分词器结巴jieba分词器，并使用python实战介绍。\njieba分词算法使用了基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能生成词情况所构成的有向无环图(DAG), 再采用了动态规划查找最大概率路径，找出基于词频的最大切分组合，对于未登录词，采用了基于汉字成词能力的HMM模型，使用了Viterbi算法。（这里面很多概念小修之前的文章都有陆续讲过哦）\njieba分词支持三种分词模式：\n1. 精确模式, 试图将句子最精确地切开，适合文本分析：\n2. 全模式，把句子中所有的可以成词的词语都扫描出来，速度非常快，但是不能解决歧义；\n3. 搜索引擎模式，在精确模式的基础上，对长词再词切分，提高召回率，适合用于搜索引擎分词。\njiaba分词还支持繁体分词和支持自定义分词。\n1jieba分词器安装\n在python2.x和python3.x均兼容，有以下三种：\n1. 全自动安装：easy_install jieba 或者 pip install jieba / pip3 install jieba\n2. 半自动安装: 先下载，网址为: http://pypi.python.org/pypi/jieba， 解压后运行: python setup.py install\n3. 手动安装: 将jieba目录放置于当前目录或者site-packages目录，\njieba分词可以通过import jieba 来引用\n2jieba分词主要功能\n先介绍主要的使用功能，再展示代码输出。jieba分词的主要功能有如下几种：\n1. jieba.cut：该方法接受三个输入参数：需要分词的字符串; cut_all 参数用来控制是否采用全模式；HMM参数用来控制是否适用HMM模型\n2. jieba.cut_for_search：该方法接受两个参数：需要分词的字符串；是否使用HMM模型，该方法适用于搜索引擎构建倒排索引的分词，粒度比较细。\n3. 待分词的字符串可以是unicode或者UTF－8字符串，GBK字符串。注意不建议直接输入GBK字符串，可能无法预料的误解码成UTF－8，\n4. jieba.cut 以及jieba.cut_for_search返回的结构都是可以得到的generator(生成器), 可以使用for循环来获取分词后得到的每一个词语或者使用\n5. jieb.lcut 以及 jieba.lcut_for_search 直接返回list\n6. jieba.Tokenizer(dictionary=DEFUALT_DICT) 新建自定义分词器，可用于同时使用不同字典，jieba.dt为默认分词器，所有全局分词相关函数都是该分词器的映射。\n代码演示：\n其中下面的是输出结果。\n3jieba分词器添加自定义词典\njieba分词器还有一个方便的地方是开发者可以指定自己的自定义词典，以便包含词库中没有的词，虽然jieba分词有新词识别能力，但是自行添加新词可以保证更高的正确率。\n使用命令：\njieba.load_userdict(filename) # filename为自定义词典的路径\n在使用的时候，词典的格式和jieba分词器本身的分词器中的词典格式必须保持一致，一个词占一行，每一行分成三部分，一部分为词语，一部分为词频，最后为词性（可以省略），用空格隔开。下面其中userdict.txt中的内容为小修添加的词典，而第二部分为小修没有添加字典之后对text文档进行分词得到的结果，第三部分为小修添加字典之后分词的效果。\n4利用jieba进行关键词抽取\n这里介绍基于TF－IDF算法的关键词抽取(干货｜详解自然语言处理之TF-IDF模型和python实现), 只有关键词抽取并且进行词向量化之后，才好进行下一步的文本分析，可以说这一步是自然语言处理技术中文本处理最基础的一步。\njieba分词中含有analyse模块，在进行关键词提取时可以使用下列代码\n当然也可以使用基于TextRank算法的关键词抽取:\n这里举一个例子，分别使用两种方法对同一文本进行关键词抽取，并且显示相应的权重值。\n5jieba分词的词性标注\njieba分词还可以进行词性标注，标注句子分词后每个词的词性，采用和ictclas兼容的标记法，这里知识简单的句一个列子。\n6jieba分词并行分词\njieba分词器如果是对于大的文本进行分词会比较慢，因此可以使用jieba自带的并行分词功能进行分词，其采用的原理是将目标文本按照行分割后，把各行文本分配到多个Python进程并行分词，然后归并结果，从而获得分词速度可观的提升。\n该过程需要基于python自带的multiprocessing模块，而且目前暂时不支持windows. 在使用的时候，只需要在使用jieba分词导入包的时候同时加上下面任意一个命令：\n在第五步进行关键词抽取并且计算相应的TF－iDF就可以进行后续的分类或者预测，推荐的相关步骤，后面小修会陆续介绍。\n参考内容:\n[1] jieba分词github介绍文档：https://github.com/fxsjy/jieba"}
{"content2":"人工智能从谷歌的阿尔法围棋（AlphaGo）赢了韩国围棋冠军李世石之后，开始火了。到了2017年5月，AlphaGo在赢了世界第一的柯洁之后，全世界都在讨论人工智能可能会给人类带来的影响。有人悲观地认为机器人会取代人类，也有人乐观地认为人类可以通过人工智能过上更幸福的生活。事实上人工智能已经在不断地改变人类的生活，比如我们经常使用的Cortana，或者是车牌人脸自动识别，乃至自动聊天客服机器人，都是人工智能的领域。\n人工智能当前的发展，主要分成下面几个子领域，包括视觉影像的自动处理，语音的自动处理，自然语言处理，知识管理等等。在每个领域都可以极大地提高处理能力，也让很多传统的职业消失。\n1， 视觉影像\n视觉是人类获取信息最大的渠道，也是人工智能最主要的研究方向，这些年包括人脸识别，车牌识别，场景识别，文字识别等等，都是这个领域中最重要的内容。从场景的角度来说，我们可以通过人脸识别来进行身份验证，通缉犯的抓捕，门禁系统管理甚至人的情感识别。这些信息的快速获得和处理，能够取代很多传统的工作，比如银行柜员，保安，一些警察的工作等等。在国内，大量的车牌自动识别系统已经部署在不同的停车场，加上移动支付的全面普及，停车场收费这个工作岗位也基本上要消失了。文字识别也是被广泛地应用在了翻译领域，以前我们旅游的时候要翻字典，现在可以直接用手机摄像头对着不明白的文字，自动翻译成想要的语言。\n当前计算机视觉处理的能力已经超过了普通人，除去计算机处理速度不谈，在分辨物体细微差别和存在干扰的图像中，计算机图像处理的错误率也已经低于5%，超过人眼和大脑的处理能力。\n2.语音\n当前在语音领域的最大应用就是速记和同声传译，另外还有说话人识别的功能。以前我们有专业的速记来记录现场的会议纪要，说话的内容等等。高级别的会议还需要专业的翻译人员提供现场的同声传译，不过人工智能如果发展到一定程度，会把这两部分的工作完全取代掉，毕竟人不是机器，需要休息，还会有出错的情况，而机器则完全不会出错。\n语音有一个另外的功能就是智能音响的唤醒词。Amazon，微软，谷歌以及国内的阿里，京东都先后推出了智能音箱，通过一个唤醒词，比如Hi Siri, Alex, Dingdong Dingdong来激活音响，并通过语音识别和人直接进行交互，比如询问天气，新闻，控制家电等等。这种弱人工智能的应用，会在最近的几年快速地进入千家万户。\n3.语言\n语言类的人工智能主要是解决机器对自然语言的理解，比如理解语言的观点，倾向性和主题等等。语言理解的应用主要集中在智能聊天机器人，自动摘要等。比如当前我们很多银行，电子商务购物网站的客服，其实都是由自动聊天机器人充当的。很多报社，新闻网站的新闻，其实也是通过自动摘要机器人直接写出来的。另外很多案件的案例查找，类似案例的判例查找，都是通过语言理解的机器人来自动实现。造成的后果就是很多初级的客服人员，初级的报社编辑甚至初级律师的失业。\n4. 知识\n人工智能在知识领域的进展是真正颠覆人类的部分，比如在一开头我们提到的AlphaGo，在下棋的时候下出了一些传统棋谱中完全没有出现的招式，最后赢得胜利。自动聊天机器人逐步发展出各种种族歧视，Facebook的聊天机器人还发展出一些人类不能理解的语言。凡此种种，都让悲观主义者觉得人工智能不可控。不过当前我们也不必杞人忧天，杯弓蛇影。人工智能的发展还远远达不到能够颠覆人类的地步，知识领域的人工智能也实现特定领域的分析。\n在医疗领域，人工智能可以通过分析大量的数据研究病人的MRI图像，对病人的情况进行诊断。在金融领域，人工智能的智能决策可以决定对一只股票是买入还是卖出，从而获得收益。在生物科学，医药领域，人工智能的智能学习算法能够帮助科研人员选择研究的方向。在社交领域，对各种社交媒体的文本和相关内容进行情感分析，能够了解一群人对某一些事情的倾向性。在这些领域中，人工智能已经实实在在地开始改变人类的发展方向。\n快速实现自己的人工智能\n讲了那么多，其实当前人工智能的能力还集中在弱人工智能的阶段，即解决简单的智能问题，计算机实现的智能还完全无法和人类相比。随着云计算的出现，大量的人工智能问题其实可以非常简单地通过云计算平台解决。\n比如我们刚才提到的视觉图像处理，在微软Microsoft Azure平台上，可以直接用分析图像的API获得每个人脸的位置，性别，表情等等信息。调用起来也非常简单，直接调用https://[location].api.cognitive.microsoft.com/vision/v1.0/analyze[?visualFeatures][&details][&language] 通过使用Http Post以及一些相应的参数，包括图像的分类，是否要识别人脸，是否识别成人或种族内容，以及识别名人。\n调用完这个API以后，如果成功，我们会获得200的返回，同时加上一段Json格式的字符串，里面描述了所有的相应内容，包括人脸的位置，颜色，是否为成人内容等等。具体的内容可以参考https://azure.microsoft.com/zh-cn/services/cognitive-services/computer-vision/\n再举个例子，如果我们想做翻译软件，以前是非常复杂的，需要处理各种数据。当有了云计算以后，我们就可以直接使用云计算提供的翻译API实现自己想要的内容。通过微软的翻译服务https://docs.microsofttranslator.com/text-translate.html#!/default/get_Translate 我们可以通过Get方法调用https://api.microsofttranslator.com/V2/Http.svc/Translate，同时提供源语言和目标语言获得专业的翻译结果。\n微软提供的人工智能API服务\n当前，微软Azure平台提供5大类30小类人工智能的API，包括影响，语音，语言，知识和搜索五大类。影响里的算法可以帮助自动审查内容，通过返回人脸、图像和情绪等智能见解构建更人性化的应用。包括计算机影像 API，人脸 API，内容审查器，情感 API，视频 API，自定义影像服务以及视频索引器。语音算法包括处理应用程序中的口述语言，例如翻译工具语音 API，说话人识别 API，必应语音 API和自定义语音服务。语言API包括了处理自然语言、评估观点和主题，并了解如何识别用户需求的功能，包括语言理解智能服务，文本分析 API，必应拼写检查 API，翻译工具文本 API，Web 语言模型 API和语言分析 API。而知识的人工智能领域则可以完成规划复杂的信息和数据，以解决智能推荐和语义搜索等任务，包括建议 API，学术知识 API，知识探索服务，QnA Maker API，实体链接智能服务 API和自定义决策服务，最后搜索则是调用了大量必应搜索的API，比如必应自动推荐 API，必应图像搜索 API，必应新闻搜索 API，必应视频搜索 API，必应 Web 搜索 API，必应自定义搜索，必应实体搜索 API。\n值得一提的是，微软的Azure云服务平台还有一个基于人工智能的云计算实验室项目，对于一些对领先技术有兴趣的读者，可以尝试这些不同的应用并进行反馈。主要有：\n• 布拉格项目 基于手势的控制\n• 约翰内斯堡项目 物流路线\n• 阿布达比项目 距离矩阵\n• Project Nanjing 等时线计算\n• 库斯科项目 与维基百科条目相关的活动\n• 伍伦贡项目 位置见解\n比如布拉格项目就可以通过Kinect的摄像头和不同手势来控制对象的变化和行为。\n我们可以用短短的几行代码，就可以自定义手势并且实现相应的功能，比如下面这段代码就可以实现识别手势旋转和让对象旋转的功能。\n我们可以访问https://azure.microsoft.com/zh-cn/services/cognitive-services/ 获得所有完整的的信息。\n结语\n我们并无法预期何时机器可以超过人类，不过机器的智能发展速度要远远快于人类进化的速度。随着奇点的到来，人类在很多智力上会大大落后于机器。云计算就是人工智能的发动机。作为开发者和技术人员，可以通过云的能力，把人工智能集成到自己的应用中去，帮助人类利用好人工智能的能力，让我们的生活越来越美好。"}
{"content2":"发表于 2009年05月14号 由 52nlp\n自然语言处理：最大熵和对数线性模型\nNatural Language Processing: Maximum Entropy and Log-linear Models\n作者：Regina Barzilay（MIT,EECS Department, October 1, 2004)\n译者：我爱自然语言处理（www.52nlp.cn ，2009年5月14日）\n三、 最大熵模型详述\ng) GIS算法（Generative Iterative Scaling）\ni. 背景：\n最原始的最大熵模型的训练方法是一种称为通用迭代算法GIS (generalized iterative scaling) 的迭代算法。GIS 的原理并不复杂，大致可以概括为以下几个步骤：\n1. 假定第零次迭代的初始模型为等概率的均匀分布。\n2. 用第 N 次迭代的模型来估算每种信息特征在训练数据中的分布，如果超过了实际的，就把相应的模型参数变小；否则，将它们变大。\n3. 重复步骤 2 直到收敛。\nGIS 最早是由 Darroch 和 Ratcliff 在七十年代提出的。但是，这两人没有能对这种算法的物理含义进行很好地解释。后来是由数学家希萨（Csiszar) 解释清楚的，因此，人们在谈到这个算法时，总是同时引用 Darroch 和Ratcliff 以及希萨的两篇论文。GIS 算法每次迭代的时间都很长，需要迭代很多次才能收敛，而且不太稳定，即使在 64 位计算机上都会出现溢出。因此，在实际应用中很少有人真正使用 GIS。大家只是通过它来了解最大熵模型的算法。\n八十年代，很有天才的孪生兄弟的达拉皮垂(Della Pietra)在 IBM 对 GIS 算法进行了两方面的改进，提出了改进迭代算法 IIS（improved iterative scaling）。这使得最大熵模型的训练时间缩短了一到两个数量级。这样最大熵模型才有可能变得实用。即使如此，在当时也只有 IBM 有条件是用最大熵模型。（以上摘自Google吴军《数学之美系列16》）\nii. 目标（Goal）：寻找遵循如下约束条件的此种形式pi prod{j=1}{k}{{alpha_j}^{f_j}(x)}的分布（Find distribution of the form pi prod{j=1}{k}{{alpha_j}^{f_j}(x)}that obeys the following constraints）：\nE_p f_j = E_{p prime}{f_j}\niii. GIS约束条件（GIS constraints）：\n1、\n其中C是一个常数（where C is a constant (add correctional feature)）\n2、\niv. 定理（Theorem）：下面的过程将收敛到p*∈P∩Q（The following procedure will converge to p*∈P∩Q）：\nv. 计算量（Computation）\n其中S={(a1,b1),…,(aN,bN)}是训练样本（where S is a training sample）\n因为有太多可能的(a,b)，为了减少计算量，因而采用下面的公式近似计算：\n时间复杂度（Running time）：O(NPA)\n其中N训练集规模，P是预期数，A是对于给定事件(a,b)活跃特征的平均数（where N is the training set size, P is the number of predictions, and A is the average number of features that are active for a given event (a,b)）\n四、 最大熵分类器（ME classifiers）\na) 可以处理很多特征（Can handle lots of features）\nb) 存在数据稀疏问题（Sparsity is an issue）\ni. 应用平滑算法和特征选择方法解决（apply smoothing and feature selection）\nc) 特征交互（Feature interaction）？\ni. 最大熵分类器并没有假设特征是独立的（ME classifiers do not assume feature independence）\nii. 然而，它们也没有明显的模型特征交互（However, they do not explicitly model feature interaction）\n五、 总结（Summary）\na) 条件概率建模与对数线性模型（Modeling conditional probabilities with log-linear models）\nb) 对数线性模型的最大熵性质（Maximum-entropy properties of log-linear models）\nc) 通过迭代缩放进行优化（Optimization via iterative scaling）\n一些实现的最大熵工具（Some implementations）：\nhttp://nlp.stanford.edu/downloads/classifier.shtml\nhttp://maxent.sourceforge.net\n第五讲结束！"}
{"content2":"Atitit 自然语言处理原理与实现 attilax总结\n1.1. 中文分词原理与实现 111\n1.2. 英文分析 1941\n1.3. 第6章　信息提取 2711\n1.4. 第7章　自动摘要 3041\n1.5. 第8章　文本分类 3191\n1.6. 第9章　文本倾向性分析 3641\n1.7. 第10章　问答系统 3741\n1.8. 第11章　语音识别 4131\n1.1. 中文分词原理与实现 11\n2.2 查找词典算法 13\n2.2.1 标准Trie树 14\n2.2.2 三叉Trie树 18\n2.2.3 词典格式 26\n1.2. 英文分析 194\n1.3. 第6章　信息提取 271\n1.4. 第7章　自动摘要 304\n1.5. 第8章　文本分类 319\n1.6. 第9章　文本倾向性分析 364\n1.7. 第10章　问答系统 374\n1.8. 第11章　语音识别 413\n作者::  ★(attilax)>>>   绰号:老哇的爪子 （ 全名：：Attilax Akbar Al Rapanui 阿提拉克斯 阿克巴 阿尔 拉帕努伊 ） 汉字名：艾龙，  EMAIL:1466519819@qq.com\n转载请注明来源： http://www.cnblogs.com/attilax/\n参考资料\n推荐新书《自然语言处理原理与技术实现》 - 自然语言处理-炼数成金-Dataguru专业数据分析社区.html"}
{"content2":"大家好，我禅师的助理兼人工智能排版住手助手条子。可能非常多人都不知道我。由于我真的难得露面一次，天天给禅师做底层工作。\n今天条子最终也熬到这一天！\n最终也有机会来为大家写文章了！\n激动的我啊。都忘了9月17号中午和禅师在我厂门口兰州料理吃饭。禅师要了一碗牛拉+一瓶可乐+一碟凉菜，总共30元。让我结账至今还没还钱的事儿了。真的，激动的我一点儿都想不起来了。\n国庆长假就要開始了，作为人工智能头条的读者。相信大家国庆的日程已经是计划好了的。\n第一天。学习。第二天。学习；第三天。学习……\n国庆哪儿哪儿都人多，还不如宅在家里好好学习呢！所以从今天開始，条子每天为大家推荐一本人工智能优秀书籍。第一本，是由 Keras 之父、现任 Google 人工智能研究员 Francois Chollet 大作：《Python 深度学习》。\n本书详尽介绍了用 Python 和 Keras 进行深度学习的探索实践。包括计算机视觉、自然语言处理、产生式模型等应用。\n书中包括30多个代码演示样例。步骤解说具体透彻。由于本书立足于人工智能的可达性和大众化。读者无须具备机器学习相关背景知识就可以展开阅读。\n学习完本书后。读者将具备搭建自己的深度学习环境、建立图像识别模型、生成图像和文字等能力。\n看到图片上有个二维码了吧？像不像30块钱？\n可是由于这个二维码条子被禅师骂了好久，说拉低了整个号的审美品位。\n品位能值几个钱？能帮大家省钱吗？能给大家优惠吗？不能省钱、没有优惠，条子怎么腆着脸给大家推荐书？\n所以。条子拼了老脸，跟 GitChat 要来以下这张百年好合色的、价值10元的、全场通用优惠券送给大家。\n老规矩：先领券。再购物。\n优惠券有效期：2018.09.28 - 2018.10.01。\n大家赶紧领了赶紧花出去，千万别亏了！"}
{"content2":"2016年，Alpha Go战胜韩国围棋选手李世乭再次引爆了全球对于人工智能的讨论和关注。计算机视觉作为人工智能技术的基础，受到深度学习的成功影响在近几年内取得了突破性的进展，巨头纷纷布局，市场也吸引了越来越多的人才创业参与其中。计算机视觉正在成为人工智能最火热的细分领域之一。\n本报告将针对计算机视觉技术发展的关键节点、市场现状及应用场景进行分析和研究。\n一、技术发展及市场现状分析\n1.人工智能是一场从终极概念到分级落地的技术演变\n2.人工智能所依赖的基础设施已经就位，但当前仍属于早期阶段\n人工智能正在像婴儿一样成长，机器不再只是通过特定的编程完成任务，而是可以通过不断地学习来掌握本领，这主要依赖高效的模型算法进行大量的数据训练，其背后需要具有高性能计算能力的软硬件作为支撑。\n数据量：2000年至今互联网及移动互联网的高速发展使得数据实现了量的积累，据IDC预测，2020年全球的大数据总量将为40ZB，其中有七成将会以图片和视频的形式进行存储，这为人工智能的发展提供了丰厚的土壤。\n深度学习算法：多伦多大学教授Geoffrey Hinton（致力于神经网络和深度学习研究）的学生在业内知名的图像识别比赛ImageNet中利用深度学习的算法将识别错误率一举降低了10%，甚至超过了谷歌，深度学习进而名声大噪。2015年，微软亚洲研究院视觉计算组在该项比赛中夺冠，将系统错误率降低至3.57%，已经超过了人眼。\n高性能计算：GPU响应速度快、对能源需求低，可以平行处理大量琐碎信息，并在高速状态下分析海量数据，有效满足人工智能发展的需求。\n基础设施成本：云计算的普及和GPU的广泛使用，极大提升了运算效率，也在一定程度上降低了运营成本。IDC报告显示，数据基础设施成本正在迅速下降，从2010年的每单位9美元下降到了2015年的0.2美元。\n3. 当前国内人工智能领域产业格局尚未成熟，上中下游均蕴含着不俗的创业空间，但进入门槛较高\n目前国内人工智能领域的产业发展还较为青涩，核心基础设施层面较为依赖国外市场，但也因市场变革期而存在大量弯道超车的机会，出现了地平线机器人、Cista、图灵机器人等创业型公司；\n技术服务层面多以创业公司为主，且有能力与大厂商一同探索推进AI技术的研究升级，其中以深度学习、计算机视觉、自然语言处理等最为火热，这是倒逼基础设施升级与拓展行业应用场景的关键环节（本报告将重点关注计算机视觉技术的发展与影响）；\n行业应用层则多点开花，既有致力于无人驾驶、无人机等创新产品研发的企业，也有将人工智能技术与传统行业结合，影响行业变革（诸如安全、医疗、金融等）的企业。\n4.计算机视觉是机器认知世界的基础，也是最主要的人工智能技术之一。\n人类认识了解世界的信息中91%来自视觉，同样计算机视觉成为机器认知世界的基础，终极目的是使得计算机能够像人一样“看懂世界”。目前计算机视觉主要应用在人脸识别、图像识别方面（包括静态、动态两类信息）。\n5. 计算机识别准确度和识别类型多寡是影响计算机视觉技术应用发展的基础因素\n提升计算机识别的准确度以及扩大计算机的识别范围一直是学术界和工业界努力的方向，并热衷于参加国际主流的计算机视觉比赛以此来验证研究成果。至今，斯坦福大学视觉实验室ImageNet通过众包的方式收集了1千多万张图片，共计2万多个标签类别，成为全球最大的图像识别数据库，其举办的ILSVRC也成为最受关注的大赛。\n2012年，ImageNet ILSVRC比赛中，冠军团队使用深度学习算法将识别错误率一举降低了10%，成为影响人工智能进程的里程碑事件，深度学习从此进入了广泛应用期。2015年，冠军团队的最新测试结果显示已经超过人类。\n6. 受技术发展影响，计算机视觉正在带动全球新一轮的市场热潮\n7．国内计算机视觉创业热度递增且深入行业，但处于早期阶段\n8. 国外巨头自研和收购双管齐下布局国外巨头自研和收购双管齐下布局，将视觉技术广泛应用于自身产品升级，并基于自身基因打造技术服务平台和新品类持续提升影响力。\n9. 国内巨头百度相对激进，阿里巴巴、腾讯基于自身产品进行功能试水综上可以看出，创业公司以多点垂直化企业服务为切入点，国内外巨头则一方面利用资源优势积极进行底层架构建设，并将技术广泛应用到已有的产品升级中，另一方面利用资金优势大量收购优秀的技术和数据创业公司，迅速弥补技术短板、数据短板和人才短板。Analysys易观认为，国内技术主要沿袭国外，但创业环境和应用场景更为宽松，市场空间不容小觑。\n二、技术应用场景及典型厂商分析\n1. 计算机视觉技术已应用于传统行业和前沿创新，安全/娱乐/营销成最抢先落地的商业化领域\n计算机视觉技术已经步入应用早期阶段，不仅渗透到传统领域的升级过程中，还作为最重要的基础人工智能技术参与到前沿创新的研究中。计算机对静态内容的识别应用主要体现在搜索变革和照片管理等基础服务层面，意在提升产品体验；伴随内容形式的变迁（文字→图片→视频），动态内容识别的需求愈加旺盛，安全、娱乐、营销成为最先落地的商业化领域。\nAnalysys易观认为，这三类领域均有一定的产业痛点，且均是视频内容产出的重地，数据体量巨大，适合利用深度学习的方式予以改进。与此同时，行业潜在的商业变现空间也是吸引创业者参与的重要原因。另一方面，当前计算机视觉主要应用于二维信息的识别，研究者们还在积极探索计算机对三维空间的感知能力，以提高识别深度。\n2. 计算机视觉的应用从软硬件两个层面优化安防人员的作业效率和深度\n安防是环境最为复杂的应用领域，通常的应用场景以识别犯罪嫌疑人、目标车辆（含套牌车/假牌车）以及真实环境中的异常为主。\n传统安防产品主要功能在于录像收录，只能为安防人员在事后取证的环节提供可能的线索，且需要人工进行反复地逐帧排查，耗时耗力；智能安防则是将视频内容结构化处理，通过大数据分析平台进行智能识别搜索，大大简化了工作难度，提高工作效率。\nAnalysys易观认为，计算机视觉的应用从行业痛点出发，以软硬件的方式大大优化了安防人员的作业效率与参考深度，是顺应行业升级的利好。不过，在实际应用过程中，对公安、交警、金融等常见安防需求方而言，更强的视觉识别效果往往意味着更多基础成本（存储、带宽等）的投入，安防厂商的未来将不只以技术高低作为唯一衡量标准，产品的实用性能与性价比的平衡才是进行突围、实现量产的根本，因此市场除了有巨大的应用空间外，还会引发一定的底层创新。\n3. 计算机视觉的应用有效迎合直播平台的前端用户体验和后端监管要求\n直播平台的爆红丰富了网民的娱乐生活方式，产生的海量内容也为平台的监管造成了巨大的压力，传统人工审核效果不稳定，基于深度学习的图像识别平台可以有效缓解这一痛点，同时也可对前端的内容运营进行优化，提高用户体验和活跃度。\nAnalysys易观认为，直播平台为计算机视觉创造了新的应用场景，伴随行业的发展，除刚需之外，平台定制化、差异化的需求也会为计算机视觉应用提供更多的增量空间。另外，与直播的UGC性质类似，其他的应用场景还有长短视频平台、社交平台、云存储平台、CDN以及社区平台。\n4.计算机视觉的应用促进视频环境中广告主和用户间交互闭环的落地\n挖掘视频中广告位及视频电商购物一直是视频平台作为中间方探索营销创收的新方式，此前以人工贴标、投放的方式实现，因效率低下仅作为小范围试水。\nAnalysys易观认为，计算机视觉将加速这一探索的进程，除技术成熟度之外，用户体验和用户习惯（从PC端向移动端迁移）也是对应用程度比较重要的影响因素，需要合理设计及长期培育。\n5.计算机视觉还将逐步拓展服务和工商业等多重应用场景\n6.计算机视觉作为基础人工智能技术，与其他技术融合共同推动创新型行业应用的发展7. 典型厂商分析——格灵深瞳\n8.典型厂商分析——商汤科技\n9. 典型厂商分析——图普科技\n10. 典型厂商分析——衣+商\n丁选"}
{"content2":"自然语言处理（简称NLP），是研究计算机处理人类语言的一门技术，包括：\n1.句法语义分析：对于给定的句子，进行分词、词性标记、命名实体识别和链接、句法分析、语义角色识别和多义词消歧。\n2.信息抽取：从给定文本中抽取重要的信息，比如，时间、地点、人物、事件、原因、结果、数字、日期、货币、专有名词等等。通俗说来，就是要了解谁在什么时候、什么原因、对谁、做了什么事、有什么结果。涉及到实体识别、时间抽取、因果关系抽取等关键技术。\n3.文本挖掘（或者文本数据挖掘）：包括文本聚类、分类、信息抽取、摘要、情感分析以及对挖掘的信息和知识的可视化、交互式的表达界面。目前主流的技术都是基于统计机器学习的。\n4.机器翻译：把输入的源语言文本通过自动翻译获得另外一种语言的文本。根据输入媒介不同，可以细分为文本翻译、语音翻译、手语翻译、图形翻译等。机器翻译从最早的基于规则的方法到二十年前的基于统计的方法，再到今天的基于神经网络（编码-解码）的方法，逐渐形成了一套比较严谨的方法体系。\n5.信息检索：对大规模的文档进行索引。可简单对文档中的词汇，赋之以不同的权重来建立索引，也可利用1，2，3的技术来建立更加深层的索引。在查询的时候，对输入的查询表达式比如一个检索词或者一个句子进行分析，然后在索引里面查找匹配的候选文档，再根据一个排序机制把候选文档排序，最后输出排序得分最高的文档。\n6.问答系统： 对一个自然语言表达的问题，由问答系统给出一个精准的答案。需要对自然语言查询语句进行某种程度的语义分析，包括实体链接、关系识别，形成逻辑表达式，然后到知识库中查找可能的候选答案并通过一个排序机制找出最佳的答案。\n7.对话系统：系统通过一系列的对话，跟用户进行聊天、回答、完成某一项任务。涉及到用户意图理解、通用聊天引擎、问答引擎、对话管理等技术。此外，为了体现上下文相关，要具备多轮对话能力。同时，为了体现个性化，要开发用户画像以及基于用户画像的个性化回复。"}
{"content2":"1、 你认为什么是人工智能？\n答：人工智能是一门研究模拟人类智能，实现机器智能的一门科学，研究人员希望机器人不止能够做一些繁重繁琐的工业任务或者数理计算，而是希望机器人能够有独立思考的能力，也就是有自我。通过图像识别，动作识别，逻辑判断，自然语言的处理和反馈以及深层次的数学以及理论思考来体现人工智能的意义。我们现在处于启蒙阶段，并没有真正的制造出可以自我思考能力的机器人，但是在这个方面已经有了初步的探索，比如神经网络的提出和发展，生物计算机的建设，当然，人工智能还需要一段路要走，在不久的将来，真正的人工智能一定能够实现！\n2、 简述推理、学习、存储，三者之间的联系。\n答：推理，逻辑学指思维的基本形式之一，是有一个或几个已知的判断推出新判断的过程。学习包括知识的学习，行为学习和技术的学习，以及抽象逻辑的理解和空间思维的想象。存储就是根据不同的应用环境通过采取合理、安全、有效方式将数据保存。学习包括了推理且存储同时在进行，当一个人在学习的时候就发生在头脑里存储，实质是人。换一个角度从人工智能来说，计算机上存储时是通过人的学习和设计间接反映出来人的学习和推理，保存在计算机中的方法。学习和推理可以先在人的思维进行，再次放到计算机里存储的过程，也可以在计算机里按着人的设计在推出新的存储。推理、学习、存储本质需要人的思想来设计。\n3、  “警卫和囚犯”问题的过河方案，使用语义网络进行问题求解。模仿示例画出你的求解方案，并给出一共需要多少步可以成功过河？\n答：用数字1代表警察，0代表囚犯，则过河的过程可以有以下步骤：\n第一步：00到达对岸，状态如下图：\n111\n0\n00\n第二步：0划船回去，状态如下图：\n111\n00\n0\n第三步：00到达对岸，状态如下图：\n111\n000\n第四步：0划船回去，状态如下图：\n111\n0\n00\n第五步：11到达对岸，状态如下图：\n1\n11\n0\n00\n第六步：10划船回去，状态如下图：\n11\n1\n00\n0\n第七步：11到达对岸，状态如下图：\n111\n00\n0\n第八步：0划船回去，状态如下图：\n111\n000\n第九步：00到达对岸，状态如下图：\n111\n0\n00\n第十步：0划船回去，状态如下图：\n111\n00\n0\n第十一步：00到达对岸，状态如下图：\n111\n000\n通过以上的分析可知，到达对岸一共需要11步。"}
{"content2":"一、 自然语言理解技术的背景\n随着社会的日益信息化，人们越来越强烈地希望用自然语言同计算机交流。自然语言理解是计算机科学中的一个引人入胜的、富有挑战性的课题。从计算机科学特别是从人工智能的观点看，自然语言理解的任务是建立一种计算机模型，这种计算机模型能够给出象人那样理解、分析并回答自然语言（即人们日常使用的各种通俗语言）的结果。现在的计算机的智能还远远没有达到能够象人一样理解自然语言的水平，而且在可预见的将来也达不到这样的水平。因此，关于计算机对自然语言的理解一般是从实用的角度进行评判的。自然语言理解系统可以用作专家系统、知识工程、情报检索、办公室自动化的自然语言人机接口，有很大的实用价值。\n然而，当前的自然语言理解技术往往面临着诸多的挑战。一方面，自然语言理解技术涉及的语言语义知识过浅，生成的结果往往差强人意，而且可扩展性很差；另外一方面，融入了过多语言专家知识的系统，需要分析的层次过深，则导致计算特别费时费力。\n灵玖软件 依托国际领先的自然语言理解专家团队，基于十余年在中文、英文与日文的自然语言计算处理的研究成果，专注于提供自然语言理解的解决方案，已经取得了长足的进步。\n二、 我们能为您解决什么 （白皮书下载、咨询及在线测试）\n灵玖软件通过几年的技术积累，结合当前的自然语言理解的需求，推出了LJParser自然语言理解中间件。经过近五年的应用和发展，灵玖LJParser产品已经可以在 不同操作系统、不同应用系统以及各种复杂的网络应用下高效运转。\n如果您遇到以下问题，那么请联系我们，我们将为您提供一整套的解决方案。\n1. 中日英等语言词语层面的分析\n灵玖软件基于条件随机场（Conditional Random Field,简称CRF）模型，研制了中文分词系统，系统切分速度可以达到每分钟一千万汉字，切词准确率高达98%，词性标注准确率高达98%。日语切分准确率高达99.2%。英文的词法分析系统正确率几乎100%。\n2. 不同领域专业词汇的自动抽取\nLJParser拥有专业词汇自动抽取系统，客户只需要提供各种自然语言文本，LJParser将自动抽取出各类专业词汇，500MB文本不到10分钟即可生成近10万专业词条，并给出计算依据和语用环境特征。抽查前1万条数据，准确率可超过90%。\n3. 文本关键语义分析\nLJParser可以自动抽取一篇文章的关键语义，并采用5-10个关键词来表达文章的语义；同时计算出文章的摘要。\n4. 海量文本集的自动分类聚类\n灵玖软件可以针对TB级别的海量文本集，按照用户的需要，智能地分类聚类。\n5. 网络多语言多编码多格式文档的正文标准化解析\n当前网络文档编码格式多样化，主要包括：Unicode，UTF-8，GBK，BIG5等；语种包括：英语、西班牙语、法语等拉丁语系；日语、藏语、维吾尔语等汉藏多语种；文档格式包括：txt，html，pdf，doc，exce，ppt等，灵玖LJParser自然语言理解中间件提供标准化套件，实现编码转换，多语种处理，并解析各种文档格式，去除广告导航等噪音数据，提取正文文本，最终用于进一步的文本计算。\n三、 LJParser的核心技术优势\n1. 核心算法全部经过了GB级别的网络真实数据测试，吸收了国内外最新的研究成果，性能高效，健壮性强，适合于实际网络业务应用。\n2. 支持Windows，Linux等操作系统，支持C/C++;Java; php等多种语言开发接口；\n3. 支持多数据源多格式多编码处理；\n四、 LJParser 典型案例\n1. 跨国企业：NCR，IBA；\n2. 政府行业客户：国家广电总局，北京市外事办\n3. 事业单位客户：中国科学技术信息研究所\n4. 互联网行业客户：口碑网，布谷网。"}
{"content2":"机器智能公司一览图\n对于人工智能，目前有两种态度。一种是担心，Elon Musk等人担心先进的人工智能会对人类造成威胁，Nicholas Carr担心自动化会抢人饭碗、让人变蠢；一种是放心，Google主席Eric Schmidt认为先进的技术智慧让人类变得越来越好，况且现在的人工智能还非常原始。那么，现在的机器智能现状到底是怎样的呢？Bloomberg Beta 的 VC Shivon Zilis 历时 3 个月，分析了 2529 家人工智能、机器学习及数据相关的初创企业后为我们做出解读。\n什么是机器智能？\n所谓机器智能，是机器学习与人工智能的统称。计算机正在学习如何思考以及读写。也正在获得人类的感觉功能，包括视觉、听觉，以及触觉、味觉和嗅觉（对后三者的关注略少）。机器智能技术涉及多种不同的问题类型（分类、聚类、自然语言处理、计算机视觉等）和方法（支持向量机、深度信念网络等）。这些都包含在机器智能版图当中。\n机器智能离不开大数据，数据是机器学习和人工智能的基础。尽管如此，出于篇幅和专注于人工智能方法的原因，这份版图并没有把大数据放进去。\n公司分类\n从事机器智能的公司很多，但是版图幅面有限。所以机器智能方法被当作关键技术的公司才可以列入。然后入选的公司再分为三大类，一类是专注于机器智能核心技术创新的核心技术类公司；一类就是应用型公司，应用型按照应用对象又可以分为面向企业、面向行业以及面向人 / 人机交互（HCI）这三类；第三类则是支撑技术，包括硬件、数据准备、数据采集等。\n如果你打算开一家相关的公司，可以利用这张图找出合适的核心技术和支持技术，然后打包成新颖的行业应用。尽管人人都想解决一些吸引人的问题，但是在许多不那么性感的行业还存在着大量的商机值得挖掘（如通过 Watson Developer Cloud、AlchemyAPI 等），所以未必要紧紧盯住热门的领域。\n版图思考\nKevin Kelly（K.K）认为，廉价的并行计算、大型的数据集，以及更好的算法推动了机器智能的发展，从而给企业、产业和人类带来了变革。这张版图的应用划分正是受到了这种观点的启发。正如 K.K 所言，“接下来的 10000 个初创企业的商业计划很容易预测，做 X 然后增加 AI。”有时候甚至连 X 都可以不要，因为机器智能本身就有可能创造出全新的行业。\n机器智能的前景非常可观。目前这个领域的初创企业被收购率已达到有 10%，Zilis 认为到 2015 年年底可能还会有另 10% 被收购掉。买家共有 15 个，其中 Google 是机器智能领域的头号买家。\n大公司具有压倒性优势，尤其是开发有消费者产品的那些。搜索（Google、百度）、社交网络（Facebook、LinkedIn、Pinterest）、内容（Netflix、Yahoo！）、移动（苹果）及电子商务巨头（Amazon）处在非常领先的位置。因为这些公司拥有大量的数据，且可以通过不断与消费者交互，从而形成算法调整的反馈回环，再加上网络效应，所以是最容易从中收获机器智能成效的公司。\n一流的个性化和推荐算法促进了这些公司的成功。在移动的新战场上，机器智能也不可或缺：如自然语言接口（苹果 Siri）、可视化搜索（Amazon 的 FireFly）、直接提供答案而非链接的动态问题回答。而 IBM 和微软在这个领域也取得了很大进展，但是主要集中在面向大型行业数据集的知识表征任务（因为缺乏上述公司类似的面向人的需求），比方说 IBM 的 Watson 就用到了辅助医生诊断上面。\n人才垄断\n过去 20 年里，人工智能领域最好的人才都在学术界。这些人发明了许多新的机器智能方法，但是能带来商业价值的却没几个。但现在像深度信念网（deep belief nets）和阶层式神经网络（hierarchical neural network）这样复杂的机器智能方法开始解决一些现实问题了。而象牙塔上的那些学者也开始走进企业。比方说Facebook招了纽约大学的Yann LeCun教授和 Rob Fergus，Google聘了多伦多大学的 Geoffrey Hinton，而百度则有吴恩达（Andrew Ng），这些都是机器智能领域“教父”级的任务。不过这些人并不是完全跟学术脱离了关系，不少的时间和精力还是要贡献给学校的。\n高薪和好设施当然是吸引这些顶尖学术人才的因素之一，但是最重要的却是另一个东西：数据。Facebook、Google、百度等拥有庞大的计算资源，还垄断了海量的数据，必然会吸引越来越多的人才加入，这就是大公司形成压倒性优势的原因。\n和平红利\n如上所述，大公司拥有固有优势，而赢得机器智能战者未来还会更加强大。幸运的是，对于其他公司来说，大公司开发出来的核心技术会迅速涌入到其他领域—通过大公司离职人员以及公开发表的研究的方式。\n此外，跟大数据革命类似，技术巨头也会把一些突破性的技术贡献给社区，然后由其他人来做应用层面的创新。\n创业机遇\n我的公司从事 X 的深度学习\n要想让你的公司明年火，可以用上面这句话做广告词。当然，前提是你的确是。\n深度学习是机器智能的热门方法。虽然可能有点炒作过头，但 Google、Facebook、百度这样的巨头，以及 Enlitic 等很初创企业，在视觉和语言处理方面采用这种办法的确取得了不俗的成绩。\n深度学习最令人兴奋的是，如果处置得当的话，其自动学习功能可以替代部分领域专家的直觉。在很多情况下，这有望改写许多领域的解决方案。\n人才收购作为商业模式\n我们在讨论大数据的时候往往提到数据科学家的短缺。但是由于此前机器智能仅限于学术研究，机器智能专家更是短缺中的短缺。这一现状并不会很快改变。\n这种短缺对于真正理解机器智能的创始人来说却是一种福利。这一领域的许多初创企业能够获得种子轮融资，往往就因为一个原因—机器智能人才收购的价格是一半技术人才收购价格的 5 倍以上（比方说 Deep Mind 人均收购价格为 500 到 1000 万美元）。作为有悟性的创始人，你甚至可以网罗一批机器智能人才然后就成立公司，说不定就有人会收购你了—好吧，这是个玩笑，但是这的确反映出“人工”智能的价值。"}
{"content2":"原作者：https://blog.csdn.net/u011734144/article/details/79559295\n转的\nimport math import jieba from utils import utils # 测试文本 text = ''' 自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。 它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。 自然语言处理是一门融语言学、计算机科学、数学于一体的科学。 因此，这一领域的研究将涉及自然语言，即人们日常使用的语言， 所以它与语言学的研究有着密切的联系，但又有重要的区别。 自然语言处理并不是一般地研究自然语言， 而在于研制能有效地实现自然语言通信的计算机系统， 特别是其中的软件系统。因而它是计算机科学的一部分。 ''' class BM25(object): def __init__(self, docs): self.D = len(docs) self.avgdl = sum([len(doc)+0.0 for doc in docs]) / self.D self.docs = docs self.f = [] # 列表的每一个元素是一个dict，dict存储着一个文档中每个词的出现次数 self.df = {} # 存储每个词及出现了该词的文档数量 self.idf = {} # 存储每个词的idf值 self.k1 = 1.5 self.b = 0.75 self.init() def init(self): for doc in self.docs: tmp = {} for word in doc: tmp[word] = tmp.get(word, 0) + 1 # 存储每个文档中每个词的出现次数 self.f.append(tmp) for k in tmp.keys(): self.df[k] = self.df.get(k, 0) + 1 for k, v in self.df.items(): self.idf[k] = math.log(self.D-v+0.5)-math.log(v+0.5) def sim(self, doc, index): score = 0 for word in doc: if word not in self.f[index]: continue d = len(self.docs[index]) score += (self.idf[word]*self.f[index][word]*(self.k1+1) / (self.f[index][word]+self.k1*(1-self.b+self.b*d / self.avgdl))) return score def simall(self, doc): scores = [] for index in range(self.D): score = self.sim(doc, index) scores.append(score) return scores if __name__ == '__main__': sents = utils.get_sentences(text) doc = [] for sent in sents: words = list(jieba.cut(sent)) words = utils.filter_stop(words) doc.append(words) print(doc) s = BM25(doc) print(s.f) print(s.idf) print(s.simall(['自然语言', '计算机科学', '领域', '人工智能', '领域']))"}
{"content2":"自然语言处理的基本流程：\n1）分词\n2）命名实体识别，主要有人名，地名，机构名等\n3）词性标注，对分词后的词语进行语义标注\n4）句法分析，主要是要构建语法树，标注单词，短语，句子的语法\n5）语义分析，包括两部分：语义消歧，主要是针对多义词在文中的意思；语义角色标注，主要是要标出一个句子中主谓宾状语等。语义角色标注过于依赖句法分析的准确性，不过目前自动句法分析的准确性并不是特别高，因此语义角色标注的效果也一般。\n6）篇章分析，在一篇文章中段落的顺序、句子的顺序都是很重要的，按照正确的表达序列排序才能构成完整的篇章。在篇章分析中可以划分的结构有篇章-段落-句子-词、篇章-句子-词、篇章-词等。\n上面的六步只不过是自然语言处理中的中间步骤，并不是自然语言处理的最终目标\n应用一：机器翻译\n应用二：语音翻译\n应用三：文本分类与情感分类\n应用四：信息检索与问答系统\n应用五：自动文摘与信息抽取\n应用六：口语信息处理与人机对话系统"}
{"content2":"自然语言处理（NLP）中的很多问题，都需要给文档中的词语一个定量化的权重值，进而可以完后词语重要性的排序，相似度的计算，相关性的排序，等等。本文就目前流行的权重计算方案进行了一个列举。\n1. TF-IDF\nwij=log(fij) x log(N/nj)\nwij是词语j在文档i中的权重， fij是词语j在文档i中出现的频率（TF）， N是所有的文档数，文章后面含义同此。\n主要思想：如果一个词语在一篇文章中出现的频率TF高，并且在其他文档中很少出现，则认为此词语具有很好的区分能力。对区分文档最有意义的词语应该是那些在文档中出现频率高而在整个文档集合中出现频率低的词语。考虑到每个词语区分不同类别的能力，TF-IDF认为一个词语出现的文档频率越小，它区分不同类别文档的能力就越大。\n2. MI （互信息）\n这里的N是所有文档中所有词语频率的和，而不是文档数。上面公式中，分子表示的是词语j在文档i中出现的概率；分母的前一项词语j在所有文档出现的概率，后一项是文档i出现的概率。\n互信息的意义：\n在某个特定文档出现频率高，但在其他文档出现频率比较低的词语与该文档的互信息比较大。通常用互信息作为特征词语和文档之间的相关度测量，如果特征词属于该文档，则他们的互信息量最大。\n3. ATC\n4. Okapi\n5. LTU\nnj是词语j至少出现过一次的文档， nj/N 是词语j的文档频率（DF）， 那么N/nj 就是逆向文档频率（IDF）， max_f是词语在所有文档中的最大频率， dl是文档长度，avg_dl是所有文档的平均长度。\n这三种权重方案都是TF-IDF的变种，是在其的基础上引入了其他的因素。ATC 引入了所有文档中的词语的最大频率，同时使用了欧几里德距离作为文档长度归一化考虑。Okapi和LTU使用了类似的方式\n来考虑文档长度（文档越长，那么相对来说，词语的频率也就越高，为了平衡，需要对长文档做出一定的惩罚，但又不能惩罚太厉害，所以引入了dl/avg_dl），但他们采用不同的方式来处理词语的频率。\nLTU使用的是log(fij),而Okapi使用的是fij/(fij+2).\n一般这几种方案没有绝对的优劣之分，根据具体情况选择合适的方案即可。"}
{"content2":"在语言理解中，词是最小的能够独立活动的有意义的语言成分。将词确定下来是理解自然语言的第一步，只有跨越了这一步，计算机才能准确的进行短语划分、实体识别、概念抽取、文本摘要等基本的自然语言理解任务。\n尽管，现在用jieba python库就能够比较方便的获得较好的分词结果，但是分词的基本原理和发展历史，是每一个NLP工作者都应该掌握的。\n1分词简介？\n词的概念在不同的语系中可能会有不同的定义，本文就以我们熟悉的中文为例来进行讲述。“词”这个概念一直是汉语语言学界纠缠不清又绕不开的话题。在汉语中，词以字为基本单位，但是一篇文章的语义表达却是以词为基础的。所以，在进行语言处理及理解时，分词是前提和必要的基础。\n分词的过程，就是将句子转化为词的表示的过程。分词的过程可以描述为通过计算机自动识别出句子中的词，在词间加入边界标记符，分隔出各个词汇。从中可以看出，分词也是一个序列标注的过程。\n有人也许会想，找一堆人统计出所有用的词汇，然后构建一个中文词表，分词的任务根据这个词表不就很容易就能够得到解决了吗？这样的想法未免有点太天真了。一方面，随着互联网的发展，词汇永远在不断的更新，这个词表很难去维护。另外一个方面，也是分词的主要困难就是分词歧义。举个例子，“各国有企业相继蓬勃发展”，这个句子可以这样去分“各国/有/企业/相继/蓬勃发展”，也可以这样去分“各/国有企业/相继/蓬勃发展”。\n2.分词算法\n分词算法主要包括规则分词和统计分词，还有基于这两种的混合分词。\n2.1规则分词\n分词的HMM理论大致就是这样了，大家会不会觉得其实很简单，有一种酣畅淋漓的感觉？如果没有，那请看代码，哈哈。代码引用了《Python自然语言处理实战：核心技术与算法》。\nclass HMM(object): def __init__(self): import os # 主要是用于存取算法中间结果，不用每次都训练模型 self.model_file = './data/hmm_model.pkl' # 状态值集合 self.state_list = ['B', 'M', 'E', 'S'] # 参数加载,用于判断是否需要重新加载model_file self.load_para = False # 用于加载已计算的中间结果，当需要重新训练时，需初始化清空结果 def try_load_model(self, trained): if trained: import pickle with open(self.model_file, 'rb') as f: self.A_dic = pickle.load(f) self.B_dic = pickle.load(f) self.Pi_dic = pickle.load(f) self.load_para = True else: # 状态转移概率（状态->状态的条件概率） self.A_dic = {} # 发射概率（状态->词语的条件概率） self.B_dic = {} # 状态的初始概率 self.Pi_dic = {} self.load_para = False # 计算转移概率、发射概率以及初始概率 def train(self, path): # 重置几个概率矩阵 self.try_load_model(False) # 统计状态出现次数，求p(o) Count_dic = {} # 初始化参数 def init_parameters(): for state in self.state_list: self.A_dic[state] = {s: 0.0 for s in self.state_list} self.Pi_dic[state] = 0.0 self.B_dic[state] = {} Count_dic[state] = 0 def makeLabel(text): out_text = [] if len(text) == 1: out_text.append('S') else: out_text += ['B'] + ['M'] * (len(text) - 2) + ['E'] return out_text init_parameters() line_num = -1 # 观察者集合，主要是字以及标点等 words = set() with open(path, encoding='utf8') as f: for line in f: line_num += 1 line = line.strip() if not line: continue word_list = [i for i in line if i != ' '] words |= set(word_list) # 更新字的集合 linelist = line.split() line_state = [] for w in linelist: line_state.extend(makeLabel(w)) assert len(word_list) == len(line_state) for k, v in enumerate(line_state): Count_dic[v] += 1 if k == 0: self.Pi_dic[v] += 1 # 每个句子的第一个字的状态，用于计算初始状态概率 else: self.A_dic[line_state[k - 1]][v] += 1 # 计算转移概率 self.B_dic[line_state[k]][word_list[k]] = \\ self.B_dic[line_state[k]].get(word_list[k], 0) + 1.0 # 计算发射概率 self.Pi_dic = {k: v * 1.0 / line_num for k, v in self.Pi_dic.items()} self.A_dic = {k: {k1: v1 / Count_dic[k] for k1, v1 in v.items()} for k, v in self.A_dic.items()} #加1平滑 self.B_dic = {k: {k1: (v1 + 1) / Count_dic[k] for k1, v1 in v.items()} for k, v in self.B_dic.items()} #序列化 import pickle with open(self.model_file, 'wb') as f: pickle.dump(self.A_dic, f) pickle.dump(self.B_dic, f) pickle.dump(self.Pi_dic, f) return self def viterbi(self, text, states, start_p, trans_p, emit_p): V = [{}] path = {} for y in states: V[0][y] = start_p[y] * emit_p[y].get(text[0], 0) path[y] = [y] for t in range(1, len(text)): V.append({}) newpath = {} #检验训练的发射概率矩阵中是否有该字 neverSeen = text[t] not in emit_p['S'].keys() and \\ text[t] not in emit_p['M'].keys() and \\ text[t] not in emit_p['E'].keys() and \\ text[t] not in emit_p['B'].keys() for y in states: emitP = emit_p[y].get(text[t], 0) if not neverSeen else 1.0 #设置未知字单独成词 (prob, state) = max( [(V[t - 1][y0] * trans_p[y0].get(y, 0) * emitP, y0) for y0 in states if V[t - 1][y0] > 0]) V[t][y] = prob newpath[y] = path[state] + [y] path = newpath if emit_p['M'].get(text[-1], 0)> emit_p['S'].get(text[-1], 0): (prob, state) = max([(V[len(text) - 1][y], y) for y in ('E','M')]) else: (prob, state) = max([(V[len(text) - 1][y], y) for y in states]) return (prob, path[state]) def cut(self, text): import os if not self.load_para: self.try_load_model(os.path.exists(self.model_file)) prob, pos_list = self.viterbi(text, self.state_list, self.Pi_dic, self.A_dic, self.B_dic) begin, next = 0, 0 for i, char in enumerate(text): pos = pos_list[i] if pos == 'B': begin = i elif pos == 'E': yield text[begin: i+1] next = i+1 elif pos == 'S': yield char next = i+1 if next < len(text): yield text[next:]\n了解分词的同学，可能都知道，通过jieba包能够非常的方便的进行分词。不了解的同学百度一下，很容易也能够学会的。所以为什么我们还要费这么大一堆来讲分词的原理呢？首先，jieba分词效果再你应用的领域未必是好的，有可能你也需要开发自己的分词系统；其次，通过对分词技术的了解你可以熟悉到序列标注，HMM等一系列的机器学习的知识，对你后续工作是有很大帮助的。\n因为微信公众号不能输入公式，我只有把有公式的部分转成图片再发上公众号，各位看客忍耐以下，以后再想办法解决。\n-----------------------------------------------------------------------\n梦想是自由的，但是实现梦想、度过幸福一生的人，实在是少之又少。但是每个人都有自己的路，选择了就走下去吧。最后还是说一句没用的，梦想都成真。"}
{"content2":"1. 前言\n自然语言处理是关毅老师的研究生课程。\n本博客仅对噪声信道模型、n元文法（N-gram语言模型）、维特比算法详细介绍。\n其他的重点知识还包括概率上文无关文法（PCFG）、HMM形式化定义、词网格分词等等，比较简单，不做赘述。\n2. 噪声信道模型\n2.1 噪声信道模型原理\n噪声信道模型的示意图如下所示：\n该模型的目标是通过有噪声的输出信号试图恢复输入信号，依据贝叶斯公式，其计算公式如下所示：\n\\[I = \\arg \\max _ { I } P ( I | O ) = \\arg \\max _ { I } \\frac { P ( O | I ) P ( I ) } { P ( O ) } = \\arg \\max _ { I } P ( O | I ) P ( I )\\]\n\\(I\\)指输入信号，\\(O\\)指输出信号。\n噪声模型的优点是具有普适性，通过修改噪声信道的定义，可以将很多常见的应用纳入到这一模型的框架之中，相关介绍见2.1。\n2.2 噪声信道模型的应用\n2.2.1 语音识别\n语音识别的目的是通过声学信号，找到与其对应的置信度最大的语言文本。\n计算公式与上文相同，此时的\\(I\\)为语言文本，\\(O\\)为声学信号。\n代码实现过程中，有一个信息源以概率\\(P(I)\\)生成语言文本，噪声信道以概率分布\\(P(O|I)\\)将语言文本转换为声学信号。\n模型通过贝叶斯公式对后验概率\\(P(I|O)\\)进行计算。\n2.2.2 其他应用\n手写汉字识别\n文本 -> 书写 -> 图像\n文本校错\n文本 -> 输入编辑 -> 带有错误的文本\n音字转换\n文本 -> 字音转换 -> 拼音编码\n词性标注\n词性标注序列 -> 词性词串替换 -> 词串\n3. N-gram语言模型\n3.1 N-gram语言模型原理\nN-gram语言模型基于马尔可夫假设，即下一个词的出现仅仅依赖于他前面的N个词，公式如下：\n\\[P ( S ) = P \\left( w _ { 1 } w _ { 2 } \\dots w _ { n } \\right) = p \\left( w _ { 1 } \\right) p \\left( w _ { 2 } | w _ { 1 } \\right) p \\left( w _ { 3 } | w _ { 1 } w _ { 2 } \\right) \\ldots p \\left( w _ { n } | w _ { 1 } w _ { 2 } \\dots w _ { n - 1 } \\right)\\]\n实践中，往往采用最大似然估计的方式进行计算：\n\\[P \\left( w _ { n } | w _ { 1 } w _ { 2 } \\dots w _ { n - 1 } \\right) = \\frac { C \\left( w _ { 1 } w _ { 2 } \\ldots w _ { n } \\right) } { C \\left( w _ { 1 } w _ { 2 } \\dots w _ { n - 1 } \\right) }\\]\n在训练语料库中统计获得字串的频度信息。\nn越大: 对下一个词出现的约束性信息更多，更大的辨别力\nn越小: 在训练语料库中出现的次数更多，更可靠的统计结果，更高的可靠性\n3.2 平滑处理\n如果不进行平滑处理，会面临数据稀疏的问题，这会使联合概率的其中一项值为0，从而导致句子的整体概率值为0。\n3.2.1 加一平滑法（拉普拉斯定律）\n公式如下：\n\\[P _ { L a p } \\left( w _ { 1 } w _ { 2 } , \\ldots w _ { n } \\right) = \\frac { C \\left( w _ { 1 } w _ { 2 } \\dots w _ { n } \\right) + 1 } { N + B } , \\left( B = | V | ^ { n } \\right)\\]\n实际运算时，\\(N\\)为条件概率中先验字串的频度。\n3.2.2 其他平滑方法\nLidstone定律\nGood-Turing估计\nBack-off平滑\n4. 维特比算法\n4.1 维特比算法原理\n维特比算法用于解决HMM三大问题中的解码问题，即给定一个输出字符序列和HMM模型参数，如何确定模型产生这一序列概率最大的状态序列。\n\\[\\arg \\max _ { X } P ( X | O ) = \\arg \\max _ { X } \\frac { P ( X , O ) } { P ( O ) } = \\arg \\max _ { X } P ( X , O )\\]\n\\(O\\)是输出字符序列，\\(X\\)是状态序列。\n维特比算法迭代过程如下：\n初始化\n\\[\\begin{array} { l } { \\delta _ { 1 } ( i ) = \\pi _ { i } b _ { i } \\left( o _ { 1 } \\right) } \\\\ { \\psi _ { 1 } ( i ) = 0 } \\end{array}\\]\n递归\n\\[\\begin{array} { c } { \\delta _ { t + 1 } ( j ) = \\underset { 1 \\leq i \\leq N } \\max \\delta _ { t } ( i ) a _ { i j } b _ { j } \\left( o _ { t + 1 } \\right) } \\\\ { \\psi _ { t + 1 } ( j ) = \\underset { 1 \\leq i \\leq N } { \\arg \\max } \\delta _ { t } ( i ) a _ { i j } b _ { j } \\left( o _ { t + 1 } \\right) } \\end{array}\\]\n结束\n\\[\\begin{array} { c } { P ^ { * } = \\max _ { 1 \\leq i \\leq N } \\delta _ { T } ( i ) } \\\\ { q _ { T } ^ { * } = \\underset { 1 \\leq i \\leq N } { \\arg \\max } \\delta _ { T } ( i ) } \\end{array}\\]\n最优路径（状态序列）\n\\[q _ { t } ^ { * } = \\psi _ { t + 1 } \\left( q _ { t + 1 } ^ { * } \\right) , \\quad t = T - 1 , \\ldots , 1\\]\n上述迭代过程，\\(a\\)状态转移矩阵，\\(b\\)是状态-输出发射矩阵。\n4.2 维特比算法例子\n例子：\n计算过程：\n第一次迭代（此时的输出字符为A）：\n\\[\\delta _ { 1 } ( 0 ) = 0.5*0.5=0.25 \\]\n\\[\\delta _ { 1 } ( 1 ) = 0.5*0.3=0.15 \\]\n\\[\\delta _ { 1 } ( 2 ) = 0*0.2=0 \\]\n第二次迭代（此时的输出字符为B）：\n\\[\\delta _ { 2 } ( 0 ) = max(0.25*0.3*0.3, 0, 0)=0.0225\\]\n\\[\\delta _ { 2 } ( 1 ) =max(0.25*0.2*0.4, 0.15*0.4*0.4, 0)=0.024\\]\n\\[\\delta _ { 2 } ( 2 ) = max(0.25*0.5*0.3, 0.15*0.6*0.3, 0)=0.0375\\]\n第三次迭代（此时的输出字符为C）：\n\\[\\delta _ { 3 } ( 0 ) = max(0.0225*0.3*0.2, 0, 0)=0.00135\\]\n\\[\\delta _ { 3 } ( 1 ) =max(0.0225*0.2*0.3, 0.024*0.4*0.3, 0)=0.00288 \\]\n\\[\\delta _ { 3 } ( 2 ) =max(0.0225*0.5*0.5, 0.024*0.6*0.5, 0)=0.0072\\]\n最终答案：\n选择最优路径的时候从后往前选，选择最后一列最大的概率值为最终结果。\n即\\(0.0072\\)）。\n接着寻找上一步中生成该概率值（\\(0.0072\\)）的数作为前一步结果。\n即\\(0.024\\)，因为\\(0.024*0.6*0.5=0.0072\\)。\n以此类推。\n4.3 维特比算法应用\n4.3.1 基于HMM的词性标注\nHMM的状态集合：词性标记集合\n\\(t_i\\)为为词性标记集合中的第\\(i\\)个词性标记。\nHMM的输出字符集合：词汇集合\n\\(\\pi _ { \\mathrm { i } }\\)：词性标记\\(t_i\\)初始概率\n\\(a_{ij}\\)：从词性标记\\(t_i\\)到\\(t_j\\)的状态转移概率\n\\(b_{jk}\\)：词性标记\\(t_j\\)对应的词\\(w_k\\)的发射概率"}
{"content2":"1. 什么是NLP\n所谓NLP就是自然语言处理，即计算机识别人的自然沟通语言，将人的语言转换成表达含义相同的文字。因为NLP的目的是将人和计算机通过自然语言沟通成为可能，而人最方便的沟通是通过语音发声，计算机只能识别二进制串，或者将可以同等转化为二进制的文字。所以NLP的目的是将语言发声和同等含义的文字进行相互转换后并进行人机交互。\n技术发展现状：\n但人工智能在很多方面，如语言理解、视觉场景理解、决策分析等，仍然举步维艰。一个关键的问题就是，机器必须要掌握大量的知识，特别是常识知识才能实现真正类人的智能。这也说明当前随着大数据红利的消失殆尽，以深度学习为代表的感知智能水平日益接近其“天花板”，而以知识为中心的认知智能将是下一代人工智能技术的关键方向。\n弱人工智能，强人工智能，超人工智能：\n弱人工智能：\n这种弱人工智能应用的非常广泛，但是因为比较“弱”，所以很多人没有意识到它们就是人工智能。就好像现在手机当中的自动拦截骚扰电话、邮箱的自动过滤、还有在象棋方面打败人类的机器人。\n强人工智能：\n能够有自己的思考方式，能够进行推理然后制作计划，最后进行执行，并且拥有一定的学习能力，能够在实践当中不断进步。\n超人工智能：\n智慧程度比人类还要高，在大部分领域当中都超越人类的人工智能，目前这种研发这种人工智能的程度非常之大，但是未来也并不是没有可能的，毕竟现在人工智能领域的发展速度的确是非常快速的\n2. NLP的工作原理\n（1）NLP的工作步骤：\n首先，将语音转换为文字\n其次，对文字进行拆分\n再者，词性理解\n最后，语义理解\n（2）各步骤\n3. NLP的框架有哪些，及这些框架的组成，特点\ntensorflow的插件SyntaxNet\n4. NLP的具体应用有哪些\n5. 写一个NLP具体的例子，带程序流程图，实例代码\n7. 四大强人工智能开放平台\n自动驾驶，城市大脑，医疗影像，智能语音"}
{"content2":"原文地址：http://bbs.pinggu.org/bigdata/\n大数据概念\n\"大数据\"是一个体量特别大，数据类别特别大的数据集，并且这样的数据集无法用传统数据库工具对其内容进行抓取、管理和处理。\n\"大数据\"首先是指数据体量(volumes)?大，指代大型数据集，一般在10TB?规模左右，但在实际应用中，很多企业用户把多个数据集放在一起，已经形成了PB级的数据量；\n其次是指数据类别(variety)大，数据来自多种数据源，数据种类和格式日渐丰富，已冲破了以前所限定的结构化数据范畴，囊括了半结构化和非结构化数据。\n接着是数据处理速度（Velocity）快，在数据量非常庞大的情况下，也能够做到数据的实时处理。\n最后一个特点是指数据真实性（Veracity）高，随着社交数据、企业内容、交易与应用数据等新数据源的兴趣，传统数据源的局限被打破，企业愈发需要有效的信息之力以确保其真实性及安全性。\n\"大数据\"是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。从数据的类别上看，\"大数据\"指的是无法使用传统流程或工具处理或分析的信息。它定义了那些超出正常处理范围和大小、迫使用户采用非传统处理方法的数据集。\n大数据的分析\n从所周知，大数据已经不简简单单是数据大的事实了，而最重要的现实是对大数据进行分析，只有通过分析才能获取很多智能的，深入的，有价值的信息。那么越来越多的应用涉及到大数据，而这些大数据的属性，包括数量，速度，多样性等等都是呈现了大数据不断增长的复杂性，所以大数据的分析方法在大数据领域就显得尤为重要，可以说是决定最终信息是否有价值的决定性因素。基于如此的认识，大数据分析普遍存在的方法理论有哪些呢？\n大数据分析的五个基本方面：\n1、可视化分析（Analytic Visualizations）\n大数据分析的使用者有大数据分析专家，同时还有普通用户，但是他们二者对于大数据分析最基本的要求就是可视化分析，因为可视化分析能够直观的呈现大数据特点，同时能够非常容易被读者所接受，就如同看图说话一样简单明了。\n2、数据挖掘算法（Data Mining Algorithms）\n大数据分析的理论核心就是数据挖掘算法，各种数据挖掘的算法基于不同的数据类型和格式才能更加科学的呈现出数据本身具备的特点，也正是因为这些被全世界统计学家所公认的各种统计方法（可以称之为真理）才能深入数据内部，挖掘出公认的价值。另外一个方面也是因为有这些数据挖掘的算法才能更快速的处理大数据，如果一个算法得花上好几年才能得出结论，那大数据的价值也就无从说起了。\n3、预测性分析能力（Predictive Analytic Capabilities）\n大数据分析最终要的应用领域之一就是预测性分析，从大数据中挖掘出特点，通过科学的建立模型，之后便可以通过模型带入新的数据，从而预测未来的数据。\n4、语义引擎（Semantic Engines）\n大数据分析广泛应用于网络数据挖掘，可从用户的搜索关键词、标签关键词、或其他输入语义，分析，判断用户需求，从而实现更好的用户体验和广告匹配。\n5、数据质量和数据管理（Data Quality and Master Data Management）\n大数据分析离不开数据质量和数据管理，高质量的数据和有效的数据管理，无论是在学术研究还是在商业应用领域，都能够保证分析结果的真实和有价值。 大数据分析的基础就是以上五个方面，当然更加深入大数据分析的话，还有很多很多更加有特点的、更加深入的、更加专业的大数据分析方法。\n大数据技术\n数据采集：ETL工具负责将分布的、异构数据源中的数据如关系数据、平面数据文件等抽取到临时中间层后进行清洗、转换、集成，最后加载到数据仓库或数据集市中，成为联机分析处理、数据挖掘的基础。\n数据存取：关系数据库、NOSQL、SQL等。\n基础架构：云存储、分布式文件存储等。\n数据处理：自然语言处理(NLP，NaturalLanguageProcessing)是研究人与计算机交互的语言问题的一门学科。处理自然语言的关键是要让计算机\"理解\"自然语言，所以自然语言处理又叫做自然语言理解(NLU，NaturalLanguage Understanding)，也称为计算语言学(Computational Linguistics。一方面它是语言信息处理的一个分支，另一方面它是人工智能(AI, Artificial Intelligence)的核心课题之一。\n统计分析：假设检验、显著性检验、差异分析、相关分析、T检验、方差分析、卡方分析、偏相关分析、距离分析、回归分析、简单回归分析、多元回归分析、逐步回归、回归预测与残差分析、岭回归、logistic回归分析、曲线估计、因子分析、聚类分析、主成分分析、因子分析、快速聚类法与聚类法、判别分析、对应分析、多元对应分析（最优尺度分析）、bootstrap技术等等。\n数据挖掘：分类 （Classification）、估计（Estimation）、预测（Prediction）、相关性分组或关联规则（Affinity grouping or association rules）、聚类（Clustering）、描述和可视化、Description and Visualization）、复杂数据类型挖掘(Text, Web ,图形图像，视频，音频等)\n模型预测：预测模型、机器学习、建模仿真。\n结果呈现：云计算、标签云、关系图等。\n大数据特点\n要理解大数据这一概念，首先要从\"大\"入手，\"大\"是指数据规模，大数据一般指在10TB(1TB=1024GB)规模以上的数据量。大数据同过去的海量数据有所区别，其基本特征可以用4个V来总结(Vol-ume、Variety、Value和Veloc-ity)，即体量大、多样性、价值密度低、速度快。\n第一，数据体量巨大。从TB级别，跃升到PB级别。\n第二，数据类型繁多，如前文提到的网络日志、视频、图片、地理位置信息，等等。\n第三，价值密度低。以视频为例，连续不间断监控过程中，可能有用的数据仅仅有一两秒。\n第四，处理速度快。1秒定律。最后这一点也是和传统的数据挖掘技术有着本质的不同。物联网、云计算、移动互联网、车联网、手机、平板电脑、PC以及遍布地球各个角落的各种各样的传感器，无一不是数据来源或者承载的方式。\n大数据技术是指从各种各样类型的巨量数据中，快速获得有价值信息的技术。解决大数据问题的核心是大数据技术。目前所说的\"大数据\"不仅指数据本身的规模，也包括采集数据的工具、平台和数据分析系统。大数据研发目的是发展大数据技术并将其应用到相关领域，通过解决巨量数据处理问题促进其突破性发展。因此，大数据时代带来的挑战不仅体现在如何处理巨量数据从中获取有价值的信息，也体现在如何加强大数据技术研发，抢占时代发展的前沿。\n当下我国大数据研发建设应在以下四个方面着力\n一是建立一套运行机制。大数据建设是一项有序的、动态的、可持续发展的系统工程，必须建立良好的运行机制，以促进建设过程中各个环节的正规有序，实现统合，搞好顶层设计。\n二是规范一套建设标准。没有标准就没有系统。应建立面向不同主题、覆盖各个领域、不断动态更新的大数据建设标准，为实现各级各类信息系统的网络互连、信息互通、资源共享奠定基础。\n三是搭建一个共享平台。数据只有不断流动和充分共享，才有生命力。应在各专用数据库建设的基础上，通过数据集成，实现各级各类指挥信息系统的数据交换和数据共享。\n四是培养一支专业队伍。大数据建设的每个环节都需要依靠专业人员完成，因此，必须培养和造就一支懂指挥、懂技术、懂管理的大数据建设专业队伍。\n大数据处理\n周涛：大数据处理数据时代理念的三大转变：要全体不要抽样，要效率不要绝对精确，要相关不要因果。\n具体的大数据处理方法确实有很多，但是根据笔者长时间的实践，总结了一个普遍适用的大数据处理流程，并且这个流程应该能够对大家理顺大数据的处理有所帮助。整个处理流程可以概括为四步，分别是采集、导入和预处理、统计和分析，最后是数据挖掘。\n大数据处理之一：采集\n大数据的采集是指利用多个数据库来接收发自客户端（Web、App或者传感器形式等）的数据，并且用户可以通过这些数据库来进行简单的查询和处理工作。比如，电商会使用传统的关系型数据库MySQL和Oracle等来存储每一笔事务数据，除此之外，Redis和MongoDB这样的NoSQL数据库也常用于数据的采集。\n在大数据的采集过程中，其主要特点和挑战是并发数高，因为同时有可能会有成千上万的用户来进行访问和操作，比如火车票售票网站和淘宝，它们并发的访问量在峰值时达到上百万，所以需要在采集端部署大量数据库才能支撑。并且如何在这些数据库之间进行负载均衡和分片的确是需要深入的思考和设计。\n大数据处理之二：导入/预处理\n虽然采集端本身会有很多数据库，但是如果要对这些海量数据进行有效的分析，还是应该将这些来自前端的数据导入到一个集中的大型分布式数据库，或者分布式存储集群，并且可以在导入基础上做一些简单的清洗和预处理工作。也有一些用户会在导入时使用来自Twitter的Storm来对数据进行流式计算，来满足部分业务的实时计算需求。\n导入与预处理过程的特点和挑战主要是导入的数据量大，每秒钟的导入量经常会达到百兆，甚至千兆级别。\n大数据处理之三：统计/分析\n统计与分析主要利用分布式数据库，或者分布式计算集群来对存储于其内的海量数据进行普通的分析和分类汇总等，以满足大多数常见的分析需求，在这方面，一些实时性需求会用到EMC的GreenPlum、Oracle的Exadata，以及基于MySQL的列式存储Infobright等，而一些批处理，或者基于半结构化数据的需求可以使用Hadoop。\n统计与分析这部分的主要特点和挑战是分析涉及的数据量大，其对系统资源，特别是I/O会有极大的占用。\n大数据处理之四：挖掘\n与前面统计和分析过程不同的是，数据挖掘一般没有什么预先设定好的主题，主要是在现有数据上面进行基于各种算法的计算，从而起到预测（Predict）的效果，从而实现一些高级别数据分析的需求。比较典型算法有用于聚类的Kmeans、用于统计学习的SVM和用于分类的NaiveBayes，主要使用的工具有Hadoop的Mahout等。该过程的特点和挑战主要是用于挖掘的算法很复杂，并且计算涉及的数据量和计算量都很大，常用数据挖掘算法都以单线程为主。\n整个大数据处理的普遍流程至少应该满足这四个方面的步骤，才能算得上是一个比较完整的大数据处理。"}
{"content2":"工业界\n腾讯人工智能实验室（Tencent AI Lab）\n百度自然语言处理（Baidu NLP）：对外提供了百度AI开放平台，王海峰（现任百度副总裁，AI技术平台体系AIG总负责人）\n微软亚洲研究院自然语言计算组（Natural Language Computing - Microsoft Research）、在NLP方向与哈工大、清华有联合实验室\n科大讯飞（与哈工大的语言认知计算联合实验室）\n搜狗实验室（Sogou Labs）\n头条人工智能实验室（Toutiao AI Lab）\n华为诺亚方舟实验室（Noah's Ark Lab）\n学术界\n哈工大SCIR实验室（哈尔滨工业大学社会计算与信息检索研究中心）：刘挺\n复旦大学自然语言处理研究组（Fudan Natural Language Processing Group）\n清华大学自然语言处理与社会人文计算实验室：清华计算机系前院长孙茂松教授是他们的leader\n北京大学计算语言学研究所：北大计算机学科比较有实力的一个研究方向之一\n中科院计算所自然语言处理组：尤其专长在机器翻译领域，组长为刘群研究员，大家常使用的中文分词工具ICTCLAS就是他们参与开发的\n苏州大学的自然语言处理实验室（Natural Language Processing Lab）、人类语言技术研究所（Institute of Human Language Technology）\n东北大学自然语言处理实验室（Natural Language Processing Laboratory at Northeastern University）\n南京大学自然语言处理研究组\n厦门大学智能科学与技术系自然语言处理实验室\n参考：\n国内有哪些自然语言处理的牛人或团队？ - 知乎\n国内外自然语言处理(NLP)研究组\n欢迎留言完善～"}
{"content2":"flatten()函数用法\nflatten是numpy.ndarray.flatten的一个函数，即返回一个一维数组。\nflatten只能适用于numpy对象，即array或者mat，普通的list列表不适用！。\na.flatten()：a是个数组，a.flatten()就是把a降到一维，默认是按行的方向降 。\na.flatten().A：a是个矩阵，降维后还是个矩阵，矩阵.A（等效于矩阵.getA()）变成了数组。具体看下面的例子：\n1、用于array（数组）对象\n?\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n>>> from numpy import *\n>>> a=array([[1,2],[3,4],[5,6]])\n>>> a\narray([[1, 2],\n[3, 4],\n[5, 6]])\n>>> a.flatten() #默认按行的方向降维\narray([1, 2, 3, 4, 5, 6])\n>>> a.flatten('F') #按列降维\narray([1, 3, 5, 2, 4, 6])\n>>> a.flatten('A') #按行降维\narray([1, 2, 3, 4, 5, 6])\n>>>\n2、用于mat（矩阵）对象\n?\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n>>> a=mat([[1,2,3],[4,5,6]])\n>>> a\nmatrix([[1, 2, 3],\n[4, 5, 6]])\n>>> a.flatten()\nmatrix([[1, 2, 3, 4, 5, 6]])\n>>> a=mat([[1,2,3],[4,5,6]])\n>>> a\nmatrix([[1, 2, 3],\n[4, 5, 6]])\n>>> a.flatten()\nmatrix([[1, 2, 3, 4, 5, 6]])\n>>> y=a.flatten().A\n>>> shape(y)\n(1L, 6L)\n>>> shape(y[0])\n(6L,)\n>>> a.flatten().A[0]\narray([1, 2, 3, 4, 5, 6])\n>>>\n从中可以看出matrix.A的用法和矩阵发生的变化。\n3、但是该方法不能用于list对象，想要list达到同样的效果可以使用列表表达式：\n?\n1\n2\n3\n4\n5\n>>> a=array([[1,2],[3,4],[5,6]])\n>>> [y for x in a for y in x]\n[1, 2, 3, 4, 5, 6]\n>>>\n！\n下面看下Python中flatten用法\n一、用在数组\n?\n1\n2\n3\n4\n>>> a = [[1,3],[2,4],[3,5]]\n>>> a = array(a)\n>>> a.flatten()\narray([1, 3, 2, 4, 3, 5])\n二、用在列表\n如果直接用flatten函数会出错\n?\n1\n2\n3\n4\n5\n6\n7\n>>> a = [[1,3],[2,4],[3,5]]\n>>> a.flatten()\nTraceback (most recent call last):\nFile \"<pyshell#10>\", line 1, in <module>\na.flatten()\nAttributeError: 'list' object has no attribute 'flatten'\n正确的用法\n?\n1\n2\n3\n4\n>>> a = [[1,3],[2,4],[3,5],[\"abc\",\"def\"]]\n>>> a1 = [y for x in a for y in x]\n>>> a1\n[1, 3, 2, 4, 3, 5, 'abc', 'def']\n或者（不理解）\n?\n1\n2\n3\n4\n>>> a = [[1,3],[2,4],[3,5],[\"abc\",\"def\"]]\n>>> flatten = lambda x: [y for l in x for y in flatten(l)] if type(x) is list else [x]\n>>> flatten(a)\n[1, 3, 2, 4, 3, 5, 'abc', 'def']\n三、用在矩阵\n?\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n>>> a = [[1,3],[2,4],[3,5]]\n>>> a = mat(a)\n>>> y = a.flatten()\n>>> y\nmatrix([[1, 3, 2, 4, 3, 5]])\n>>> y = a.flatten().A\n>>> y\narray([[1, 3, 2, 4, 3, 5]])\n>>> shape(y)\n(1, 6)\n>>> shape(y[0])\n(6,)\n>>> y = a.flatten().A[0]\n>>> y\narray([1, 3, 2, 4, 3, 5])\n2017年老男孩全栈python第2期视频教程\n55G老男孩python全栈开发全套视频教程 包含:基础篇,前端篇,web框架篇,项目实战篇\n2018全栈开发Flask Python Web网站编程价值2400\n2018Python Flask打造一个视频网站实战视频教程   ...2\n2018小象学院《分布式爬虫实战》第二期视频教程\nPython高级编程技巧实战 基于Python项目与面试题讲解  ...2\nPython分布式爬虫打造搜索引擎网站（价值388元）\n小象最新Python机器学习升级版视频学习教程\n麦子学院Python自动化开发 类Zabbix监控项目开发与实战视频教程\n2017AI人工智能时代基础实战python机器学习深度学习算法全套视频教程\nAI人工智能时代基础实战python机器学习深度学习算法视频教程  ...2\n2017年最新Python3.6网络爬虫实战案例基础+实战+框架+分布式高清视频教程\nPython数据分析基础与实践 Python数据分析实践课程 Python视频教程\n2017AI人工智能基础实战python机器深度学习算法视频教程  ...2\nPython Matplotlib实战视频教程 莫烦老师Python课程+麻省理工MIT Python大牛公开课\nPython自动化开发-类Zabbix监控项目开发与实战 老男孩教学总监Alex主讲\n2017最新python数据分析入门与实战   ...2\n2017最新python入门+进阶+实战课堂教学管理系统开发全套完整版  ...2\n2017年最新Python3.6网络爬虫实战案例基础+实战+框架+分布式高清视频教程\n高清中文PDF：数据处理、分析、科学计算、自然语言处理  ...2"}
{"content2":"摘要：随着机器学习和深度学习的热潮，各种图书层出不穷。然而多数是基础理论知识介绍，缺乏实现的深入理解。本系列文章是作者结合视频学习和书籍基础的笔记所得。本系列文章将采用理论结合实践方式编写。首先介绍机器学习和深度学习的范畴，然后介绍关于训练集、测试集等介绍。接着分别介绍机器学习常用算法，分别是监督学习之分类（决策树、临近取样、支持向量机、神经网络算法）监督学习之回归（线性回归、非线性回归）非监督学习（K-means聚类、Hierarchical聚类）。本文采用各个算法理论知识介绍，然后结合python具体实现源码和案例分析的方式（本文原创编著，转载注明出处:机器学习及其基础概念简介(2)）\n目录\n【Machine Learning】Python开发工具：Anaconda+Sublime(1)\n【Machine Learning】机器学习及其基础概念简介(2)\n【Machine Learning】决策树在商品购买力能力预测案例中的算法实现(3)\n【Machine Learning】KNN算法虹膜图片识别实战(4)\n1 机器学习简介\n机器学习 （Machine Learning, ML) ：\n概念：多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。\n学科定位：人工智能(Artificial Intelligence, AI）的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演绎。\n定义：探究和开发一系列算法来如何使计算机不需要通过外部明显的指示，而可以自己通过数据来学习，建模，并且利用建好的模型和新的输入来进行预测的学科。\n发展：\nArthur Samuel (1959): 一门不需要通过外部程序指示而让计算机有能力自我学习的学科\nLangley（1996) ： “机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在经验学习中改善具体算法的性能”\nTom Michell (1997):  “机器学习是对能通过经验自动改进的计算机算法的研究”\n学习：针对经验E (experience) 和一系列的任务 T (tasks) 和一定表现的衡量 P，如果随之经验E的积累，针对定义好的任务T可以提高表现P，就说计算机具有学习能力\n例子： 人脸识别、无人驾驶汽车、下棋、语音识别、电商推荐系统等\n应用：语音识别、自动驾驶、语言翻译、计算机视觉、推荐系统、无人机、识别垃圾邮件\n机器学习就业需求：LinkedIn所有职业技能需求量第一：机器学习，数据挖掘和统计分析人才 http://blog.linkedin.com/2014/12/17/the-25-hottest-skills-that-got-people-hired-in-2014/\n2 深度学习(Deep Learning)\n深度学习(Deep Learning)：\n深度学习是基于机器学习延伸出来的一个新的领域，由以人大脑结构为启发的神经网络算法为起源加之模型结构深度的增加发展，并伴随大数据和计算能力的提高而产生的一系列新的算法。\n深度学习发展：\n其概念由著名科学家Geoffrey Hinton等人在2006年和2007年在《Sciences》等上发表的文章被提出和兴起。\n学习能用来干什么？为什么近年来引起如此广泛的关注？\n深度学习，作为机器学习中延伸出来的一个领域，被应用在图像处理与计算机视觉，自然语言处理以及语音识别等领域。自2006年至今，学术界和工业界合作在深度学习方面的研究与应用在以上领域取得了突破性的进展。以ImageNet为数据库的经典图像中的物体识别竞赛为例，击了所有传统算法，取得了前所未有的精确度。\n深度学习目前有哪些代表性的学术机构和公司走在前沿？人才需要如何？\n学校以多伦多大学，纽约大学，斯坦福大学为代表，工业界以Google, Facebook, 和百度为代表走在深度学习研究与应用的前沿。Google挖走了Hinton，Facebook挖走了LeCun，百度硅谷的实验室挖走了Andrew Ng，Google去年4月份以超过5亿美金收购了专门研究深度学习的初创公司DeepMind, 深度学习方因技术的发展与人才的稀有造成的人才抢夺战达到了前所未有激烈的程度。诸多的大大小小(如阿里巴巴，雅虎）等公司也都在跟进，开始涉足深度学习领域，深度学习人才需求量会持续快速增长。\n深度学习如今和未来将对我们生活造成怎样的影响？\n目前我们使用的Android手机中google的语音识别，百度识图，google的图片搜索，都已经使用到了深度学习技术。Facebook在去年名为DeepFace的项目中对人脸识别的准备率第一次接近人类肉眼（97.25% vs 97.5%)。大数据时代，结合深度学习的发展在未来对我们生活的影响无法估量。保守而言，很多目前人类从事的活动都将因为深度学习和相关技术的发展被机器取代，如自动汽车驾驶，无人飞机，以及更加职能的机器人等。深度学习的发展让我们第一次看到并接近人工智能的终极目标。\n深度学习的应用展示：\n无人驾驶汽车中的路标识别\nGoogle Now中的语音识别\n百度识图\n针对图片，自动生成文字的描述\n图片文字识别结果：“A person riding a motorcycle on a dirt road,”\n3 机器学习相关概念介绍\n基本概念：训练集，测试集，特征值，监督学习，非监督学习，半监督学习，分类，回归\n概念学习：人类学习概念：鸟，车，计算机\n定义：概念学习是指从有关某个布尔函数的输入输出训练样例中推断出该布尔函数\n例子：学习 “享受运动\" 这一概念：\n小明进行水上运动，是否享受运动取决于很多因素\n样例\n天气\n温度\n湿度\n风力\n水温\n预报\n享受运动\n1\n晴\n暖\n普通\n强\n暖\n一样\n是\n2\n晴\n暖\n大\n强\n暖\n一样\n是\n3\n雨\n冷\n大\n强\n暖\n变化\n否\n4\n晴\n暖\n大\n强\n冷\n变化\n是\n天气：晴，阴，雨\n温度：暖，冷\n湿度：普通，大\n风力：强，弱\n水温：暖，冷\n预报：一样，变化\n享受运动：是，否\n概念定义在实例(instance)集合之上，这个集合表示为X。（X：所有可能的日子，每个日子的值由 天气，温度，湿度，风力，水温，预 报6个属性表示。待学习的概念或目标函数成为目标概念（target concept), 记做c。c(x) = 1, 当享受运动时， c(x) = 0 当不享受运动时，c(x)也可叫做y\nx: 每一个实例\nX: 样例, 所有实例的集合\n学习目标：f: X -> Y\n训练集(training set/data)/训练样例（training examples): 用来进行训练，也就是产生模型或者算法的数据集\n测试集(testing set/data)/测试样例 (testing examples)：用来专门进行测试已经学习好的模型或者算法的数据集\n特征向量(features/feature vector)：属性的集合，通常用一个向量来表示，附属于一个实例\n标记(label): c(x), 实例类别的标记\n正例(positive example)\n反例(negative example)\n例子：研究美国硅谷房价\n影响房价的两个重要因素：面积(平方米），学区（评分1-10）\n样例\n面积（平方米）\n学区 （11.2 深度学习(Deep Learning)介绍-10）\n房价 （1000$)\n1\n100\n8\n1000\n2\n120\n9\n1300\n3\n60\n6\n800\n4\n80\n9\n1100\n5\n95\n5\n850\n分类 (classification): 目标标记为类别型数据(category)\n回归(regression): 目标标记为连续性数值 (continuous numeric value)\n例子：研究肿瘤良性，恶性于尺寸，颜色的关系\n特征值：肿瘤尺寸，颜色\n标记：良性/恶性\n有监督学习(supervised learning)： 训练集有类别标记(class label)\n无监督学习(unsupervised learning)： 无类别标记(class label)\n半监督学习（semi-supervised learning)：有类别标记的训练集 + 无标记的训练集\n4 机器学习步骤框架\n把数据拆分为训练集和测试集\n用训练集和训练集的特征向量来训练算法\n用学习来的算法运用在测试集上来评估算法 （可能要设计到调整参数（parameter tuning), 用验证集（validation set）\n例如：\n100 天： 训练集\n10天：测试集 （不知道是否 ” 享受运动“， 知道6个属性，来预测每一天是否享受运动）"}
{"content2":"什么是认知服务Cognitive Service？\n认知服务是由微软在IBM认知计算[^1]的基础上提出来的，简单来讲，认知服务是基于文本分析、语音理解、以及视觉输入等形式经过人工智能网络分析后所提供的一种服务形式。微软认知服务的前身就是其大名鼎鼎的牛津计划[^2]项目，2015年火爆朋友圈的How-Old.net[^3]应用就是利用图片识别和情感分析等技术来判断一张图片中所有人的年龄，在很大程度上分析一个人的年龄基于一些固定的算法或者由于机器无法“理解”人类的“欺骗”技巧，所以可以对图片进行特殊处理使识别出来的年龄不同于真实年龄，以达到和朋友互动娱乐的效果。其实这还只是认知服务的冰山一角。\n目前认知服务的历史及发展状况？各大厂商提供了那些功能接口？如微软、IBM等。\n认知服务最早起源于IBM的Waston计划[^4]，早期Waston是一个基于自然语言的第一个“智能”问答系统，曾在2011年的综艺节目《危机边缘》[^5]中战胜了人类选手。而现如今Waston更是以认知商业服务的形式展现，从最初的语音文本识别，到今天的图片、图像识别；从简单的专家问答系统，到今天的辅助医疗诊断[^5]，认知服务的范围在不断扩大。在认知服务领域，IBM和微软共同领导行业的发展，尤其在图片识别分支领域微软拥有更多优势[^6]，所提供的API服务也大体相似。下面以微软提供的认知服务为例：\n微软认知服务分为5大类：\n1. 视觉类：计算机视觉，情感识别，人脸识别，视频检测。\n2. 语音类：自定义智能语音识别服务，声纹识别，语音识别。\n3. 语言类：必应拼写检查，语言理解智能服务，语言分析，文本分析，网络及语言模型。\n4. 知识类：学术知识，实体链接智能服务，知识探索服务，推荐。\n5. 搜索类：必应自动推荐，必应图片搜索，必应新闻搜索，必应视频搜索 API，必应网页搜索。\n其中前三类是属于输入部分，人类也好，计算机也好都需要一个确定的输入，然后由核心计算网络对数据进行加工、分解、运算。这三类输入奠定了人工智能理解人类世界的能力，也为她打开了一扇通往人类社会的大门。后面的两部分主要是基于搜索引擎的内容和图片搜索，就像谷歌一样，作为一个搜索引擎对我们来说再熟悉不过，个人觉得并没有太多亮点。接下来重点介绍几个比较有趣的功能。\n首先，让我们来看一下计算机视觉API。计算视觉API主要围绕对图像内容的定位和识别，然后通过分析对图片进行分类。\n上传一张图片，视觉API能对图片进行分析，识别事物类型，例如游泳，爬山等。\n{ \"requestId\": \"9d8398c3-c3df-4966-992e-cd81756f8d32\", \"metadata\": { \"height\": 1155, \"width\": 1500, \"format\": \"Jpeg\" }, \"imageType\": { \"clipArtType\": 0, \"lineDrawingType\": 0 }, \"color\": { \"accentColor\": \"19A4B2\", \"dominantColorForeground\": \"Grey\", \"dominantColorBackground\": \"White\", \"dominantColors\": [ \"White\" ], \"isBWImg\": false }, \"adult\": { \"isAdultContent\": false, \"isRacyContent\": false, \"adultScore\": 0.14750830829143524, \"racyScore\": 0.12601403892040253 }, \"categories\": [ { \"name\": \"people_swimming\", \"score\": 0.98046875 } ], \"faces\": [ { \"age\": 29, \"gender\": \"Male\", \"faceRectangle\": { \"width\": 304, \"height\": 304, \"left\": 748, \"top\": 336 } } ], \"tags\": [ { \"name\": \"water\", \"confidence\": 0.99964427947998047 }, { \"name\": \"sport\", \"confidence\": 0.95049923658370972 }, { \"name\": \"swimming\", \"confidence\": 0.90628170967102051, \"hint\": \"sport\" }, { \"name\": \"pool\", \"confidence\": 0.87875890731811523 }, { \"name\": \"water sport\", \"confidence\": 0.631849467754364, \"hint\": \"sport\" } ], \"description\": { \"type\": 0, \"captions\": [ { \"text\": \"a man swimming in a pool of water”, \"confidence\": 0.78501086930930186 } ] } }\n识别后返回一个Json格式的数据结构，对识别出的类型标签，还会给出一个置信度的值，接近1为最可信。如果娱乐精神强一点的话，完全可以设定很低的置信度比如0.6，如果涉及安全等领域，对识别精度要求较高时，就要适当的提升置信度。\n其次，让我们来看一下微软的另一个招牌服务—-情绪识别。情绪识别API采用图片中的人脸表情作为输入项，为图片中每张人脸返回一组情感置信评分，同时用人脸识别API为人脸范围做出标记。如果用户已经调用了人脸识别API，也可以将人脸方框作为一个可选输入项提交。  可检测到的情感有愤怒、轻蔑、厌恶、恐惧、快乐、无表情、悲伤以及惊讶[^7 ]。这些情感具有特定的面部表情，被认为是可进行跨文化和普适交流的。\n返回的数据还是比较清晰的数据就结构。\n[ { \"faceRectangle\": { \"left\": 479, \"top\": 190, \"width\": 158, \"height\": 158 }, \"scores\": { \"anger\": 0.00001619889, \"contempt\": 0.000121588469, \"disgust\": 0.0000216889184, \"fear\": 0.00138592813, \"happiness\": 0.00001577913, \"neutral\": 0.002224847, \"sadness\": 0.00000300440252, \"surprise\": 0.996211 } }, { \"faceRectangle\": { \"left\": 289, \"top\": 209, \"width\": 117, \"height\": 117 }, \"scores\": { \"anger\": 0.0003364322, \"contempt\": 0.008513732, \"disgust\": 0.000237169676, \"fear\": 0.001399079, \"happiness\": 0.03209325, \"neutral\": 0.1274486, \"sadness\": 0.000152969456, \"surprise\": 0.8298188 } } ]\n是不是很有趣？微软亦或IBM、Google等[^8]大公司在人工智能方面提供了很丰富的API接口，我们所需要做的仅是展开我们丰富的想象力，实现有实用价值并且有趣又好玩的应用App或者Web程序，这也许就是我们作为软件工程师的长处。\n将来会怎么样？\n关于认知服务我们能做些什么：\n微软的工程师们设计出一个结合IOT和人脸识别服务的项目，目的是通过人脸识别服务去控制门禁系统[^9]，能现实生活中找到实际应用的案例。也许这个项目并没有什么新鲜的想法或创新，不过其真正的意义在于人脸识别的准确率在实际操作上的可行性，正如DNA检测的准确率被广泛应用一样[^10]。其实我们还可以结合多个API去组合完成一些比较有意思的事情，比如我们可以上传一张自己或者朋友的图片，然后系统会自动依据人物特征推荐出最匹配的名人，并且可以分享到社交平台，有些类似于TwinsOrNot.net[^11]项目。更好的想法还得大家共同探索，希望有更多、更新奇的应用能展现出来。\n关于认知服务的一些假设：\n当我们谈及人工智能时不可回避的一个问题是究竟人工智能是天使还是魔鬼。而认知服务的主要任务是让机器理解人类世界，并能按照人类的思维方式或者超出普通人的思考能力去替代人类完成某些的判断或决策，以替代人类大脑。一旦人工智能拥有“思考能力”之后，人类很难预测她的行为究竟是善意的行为，还是恶意的陷阱（本文中的思考能力未有完整的定义，部分科学家并不相信机器会拥有“智能”，相关内容可参考图灵测试[^12]等内容）。今天的认知服务是否开启了人工智能的潘多拉魔盒我们还未尝可知，未来的一切还有待我们来探索。\n参考引用：\n1. ^ https://en.wikipedia.org/wiki/Cognitive_computing\n2. ^ http://www.msra.cn/zh-cn/research/project-oxford\n3. ^ http://www.how-old.net/\n4. ^ http://www-31.ibm.com/ibm/cn/cognitive/outthink/\n5. ^ https://en.wikipedia.org/wiki/Watson_(computer)\n6. ^ http://www.image-net.org/challenges/LSVRC/2015/results\n7. ^ https://www.azure.cn/cognitive-services/zh-cn/emotion-api\n8. ^http://www.programmableweb.com/news/top-10-machine-learning-apis-att-speech-ibm-watson-google-prediction/analysis/2015/08/03\n9. ^https://www.azure.cn/cognitive-services/zh-cn/WindowsIoTFacialRecognitionDoor\n10. ^https://en.wikipedia.org/wiki/Genealogical_DNA_test\n11. ^https://www.twinsornot.net\n12. ^https://en.wikipedia.org/wiki/Turing_test"}
{"content2":"最近一直在考虑如何选择自己上研究生后的方向，因为对计算机科学类的研究方向也不是很了解，最近一直在阅读关于不同的研究方向和它们的应用的书籍。最后终于确定了自己的研究方向，总结一下自己的思考吧。\n我认为自己不属于那种埋头研究型和论文型的人才，我更希望能把知识应用到软件的开发中，做出一些实用的激动人心的软件来，所以理论研究的路线我就不考虑了。从实际的应用出发，我认为软件粗略地可以分为三类----专用型软件，企业级软件，个人应用软件。当然这三类之间界限不是非常的清晰。\n首先，专用型软件，类似Photoshop，一些图形图像工作站中应用的软件，MATLAB之类的。人们对于这类软件的需求是比较固定的，变化相对较少。这类软件的市场相对成熟，一些巨型公司不断应用更新的技术去满足用户的这些需求。\n其次，企业级软件。这种针对大型企业，多用户，对软硬件需求都较高。这种软件又可以分为两类，一种是通用性企业软件，是基本每个企业都需要的，如财务，人事管理，企业运作，ERP之类。这类软件发展已经很完善，SAP、用友、金蝶等大公司已经占据了市场，从某个层面上讲，这些企业不是在卖软件，而是在卖他们的企业管理思想。另一类就是专门针对企业的业务流程的定制化软件，这类软件发展得前景很大，越来越多的企业将对其业务流程进行电子化。但这种软件的需求需要较强的背景知识，目前国内的软件团队往往往往缺乏这些领域知识，所以开发前需要大量人力物力投入到了解业务需求以及和客户沟通，而最终结果往往不能很好满足客户需要，这种高成本高风险的开发方式最终会被取代。随着需求的发展，应该会有更多针对单个领域进行软件开发的团队，他们都是这一领域的专家，代码可以极大程度上复用，和客户沟通更容易。同时，针对市场预测，数据存储和计算的需求也会相应增强，针对专业领域的技术如计算机视觉，以及高性能计算将会有很大的用武之地。\n最后，个人应用软件。一类是走向专业化的个人应用，比如游戏机，独立的硬件平台，专门为这一应用进行过优化。另一种软件则在向云端进发，谷歌的设想会慢慢地变为现实。个人pc的地位逐渐被更小巧，便携，拥有网络接入的手机，PDA以及上网本所取代。这些网络接入设备将越来越流行，无线网络覆盖和网络带宽将不断提高。但目前平台非常多样，软件之间的移植比较困难，往往是针对一个平台进行开发，由app store集中管理，而开发者很多是个人或小型的团体，未来应该会出现更多的专业平台应用开发商。因特网上的数据越来越多。有人形容因特网实际是一个巨大的数据库，只不过里面的数据以非形式化的方式以自然语言存储。那不必说，数据库技术必然会大有前途。对这些海量数据的管理、运算、抽取知识也将是一个发展的热点，就像谷歌做的那样，这需要数据挖掘、搜索、机器学习、人工智能方面的技术，来处理这些数据。相应高性能计算也会有强劲需求。软件的因特网化对信息安全也会有巨大需求。\n最近在看一本叫做《集体智慧编程》的书，里面介绍了在web2.0时代，利用海量的世界各地用户制造的数据进行处理的技术，如推荐系统，搜索，文件过滤等等。世界各地大量的人产生了海量的看似杂乱无章的数据，如何从这些数据中提取出有用的信息是一项非常有挑战性的难题。谷歌利用复杂的算法，从因特网中找到用户需求的网页，成就了如今的辉煌。\n分析完这些技术的前景，再分析我自己吧。我一直对软件工程有一定的兴趣，但没有强烈到让我下定决心一直把这一领域作为我的方向，我感觉软件工程的知识是每个开发团队都必需的，不管哪一领域，软件工程对于开发都是必需品。老师给我的建议是，选自己的方向首先要不讨厌它，在你比较有兴趣的领域中再去选择一个更有前景的。综上，我决定开始了解数据挖掘、和机器学习以及人工智能关于集体智慧方面的知识，争取在研究生之前先对这些有初步的了解。\n个人了解非常有限，片面的分析了一下软件的方向，希望大家能多多批评指正：）"}
{"content2":"转载自https://mp.weixin.qq.com/s/MD2-xMWWXx7rpfWzd5XDxA\n非常不错的总结\n根据这几年的积累，整理了一份国内外学术界和工业界的牛人和大牛团队，供大家申请硕士、博士、博士后和找工作参考。\n学校（排名不分先后）：\n哈工大社会计算与信息检索实验室：刘挺老师坐镇，教师包括：秦兵、张宇、车万翔、赵妍妍、刘铭、张伟男、丁效等老师，实验室共7个组，另外王海峰老师也是实验室兼职博导。\n哈工大智能技术与自然语言处理实验室：王晓龙老师坐镇，教师包括刘秉权、刘远超 、孙承杰等老师\n哈工大机器智能与翻译研究室：赵铁军老师坐镇，教师包括杨沐昀、郑德权、徐冰老师等，另外周明老师是实验室兼职博导。\n哈工大深圳智能计算研究中心：王晓龙老师坐镇，包括陈清才、汤步洲、徐睿峰、刘滨等老师，实力很强。\n哈工大深圳人类语言技术组：徐睿峰老师坐镇，情感原因发现做的比较好。\n哈工大另外做NLP的老师包括：关毅、王轩等。\n清华大学自然语言处理与社会人文计算实验室：孙茂松老师坐镇，包括刘洋、刘知远等老师。论文发的非常多。\n清华大学交互式人工智能（CoAI）课题组：朱小燕老师坐镇，包括黄民烈等老师。Dialogue System做的非常好，论文非常多。\n清华大学智能技术与系统国家重点实验室信息检索课题组：马少平老师坐镇，包括张敏、刘奕群等老师。信息检索做的非常好，论文非常多，前段时间刚拿了CIKM唯一的最佳论文（因为一作是学生，跟最佳学生论文合二为一了）。\n清华大学另外做NLP的老师还有李涓子、唐杰、朱军等老师，李老师知识图谱做得好，唐老师数据挖掘（尤其是学者画像）做得好，朱老师偏向机器学习和贝叶斯等做的很好。\n北京大学计算语言学教育部重点实验室：教师包括：王厚峰、万小军、常宝宝、李素建、孙栩、严睿、穗志方、吴云芳等（包含其他实验室的老师）。万老师、李老师、常老师等发论文很多。\n北京大学语言计算与互联网挖掘研究组：万小军老师、孙薇薇老师。万老师主要做自动摘要、文本生成、情感分析与计算等，论文非常多。\n中科院NLP组主要集中在自动化所模式识别国家重点实验室下属的中文信息处理研究组，另外计算所有刘群老师组和软件所也有孙乐老师做。具体老师包括刘群、宗成庆、赵军、孙乐、王斌、徐君、张家俊、刘康、韩先培、何世柱等老师。论文非常多。\n复旦大学自然语言处理组：黄萱菁、邱锡鹏等老师，发论文很多。\n复旦大学知识工场：肖仰华老师知识图谱做的非常好，论文发的很多。\n苏州大学自然语言处理组：做机器翻译、情感分析、信息抽取等，论文发的很多。教师包括张民、周国栋、姚建民、李正华、熊得意、李军辉、洪宇、陈文亮等老师。其中张老师、姚老师、李老师都是哈工大毕业的，张老师也是哈工大的兼职博导，论文很多。\n东北大学自然语言处理实验室：机器翻译做的非常好，还成立了自己的公司，对外合作很多。姚天顺老师是创始人，朱靖波老师坐镇，教师包括肖桐、任飞亮、张春良等老师。\n浙江大学：陈华钧、赵洲等老师，陈老师知识图谱做的很厉害。\n中国人民大学：文继荣、赵鑫、徐君、窦志成等老师。文老师现在是院长，之前在MSRA，信息检索非常厉害。\n上海交大：赵海老师，主要做机器翻译、句法分析等。\n东南大学：漆桂林老师，知识图谱做的很厉害。\n大连理工信息检索实验室：林鸿飞老师坐镇，包括杨志豪、王健、张绍武、孙媛媛、张冬瑜、杨亮等老师。主要做信息检索，隐喻、幽默等语料库做的非常好。\n西湖大学：张岳老师，之前在新加坡，论文发的非常非常非常多，剑桥2016年统计的全世界发论文的数量好像排第二。\n南京大学自然语言处理研究组：包括陈家俊、戴新宇、黄书剑等老师。\n天津大学：张鹏老老师，信息检索做的很好。\n北京理工大学：黄河燕老师、张华平老师。黄老师是北京理工大学计算机学院院长，主要研究机器翻译，担任好几个副理事长，享受国务院特殊津贴。中科院的自然语言处理工具包就是张老师做的，另外跟刘群老师合作发了不少论文，我之前工作时实习生开发的NER就是借鉴的他的层叠马尔可夫模型而二次开发和优化的。\n武汉大学语言与信息研究中心：姬东鸿等老师。\n厦门大学智能科学与技术系自然语言处理实验室：包括史晓东等老师，主要做机器翻译、知识图谱、信息抽取等。\n山东大学：聂礼强老师，信息检索做的非常好，论文很多。之前在新加坡，新加坡发SIGIR太多了。。。\n南开大学：杨征路老师，主要做信息检索。\n北京邮电大学：王小捷老师。\n北京语言大学：于东老师，主要做机器翻译、人机对话等。\n华东师范大学：吴苑斌老师，记得应该是复旦大学黄萱菁老师的学生。\n山西大学：李茹老师，山西大学计算机学院副院长。\n郑州大学自然语言处理实验室：实验室网页最近打不开了。\n黑龙江大学：付国宏、张梅山等老师。\n以下是港澳台地区的高校（排名不分先后）：\n台湾大学自然语言处理实验室：主要研究方向包括知识图谱、机器翻译、问答、自动摘要、信息检索等，论文非常多。\n香港科技大学人类语言技术中心：论文非常多，牛人也不少。\n香港中文大学文本挖掘组：主要研究方向包括文本挖掘和信息检索。\n澳门大学自然语言处理与中葡翻译实验室：主要做机器翻译，做的非常好，论文也非常多。\n香港理工大学社会媒体挖掘组：主要研究方向包括社会影响力建模、社会媒体分析、观点摘要、观点追踪、跨语言情感分析等，这个实验室对外合作很多，比如北大李素建老师、MSRA的韦福如老师等。\n国内工业界（排名不分先后）：\n百度王海峰老师以及带领的自然语言处理部+百度研究院做NLP的一些组，内部NLPC平台集成了几十个NLP算子，一些算子每天调用量都能上亿次（不要问我为啥知道这么详细，因为15-16年我参与开发过2个算子，被加到开发者组了，经常有群邮件告知各算子的调用情况），几乎涵盖所有的NLP任务，部分技术在百度AI开放平台-全球领先的人工智能服务平台-百度AI开放平台开放API，少量技术在github开源代码；\nMSRA周明老师带领的NLC组，组内论文发的非常多，尤其是几个高级研究员、主管研究员；\n哈工大和科大讯飞联合实验室：实验室主任是刘挺老师，阅读理解做的非常好；\n华为诺亚方舟的刘群老师以及带领的团队；\n小米：王斌老师坐镇，王老师翻译的书相信大部分人都看过吧。\n今日头条的李航老师；\n阿里巴巴、腾讯NLP做的也不少，只是个人感觉都是很多团队在做，比较分散，没有集中到一起。如果有人总结比较好的话，欢迎告知。\n其他很多创业公司也有大牛坐镇，只是太多、太分散了，不再赘述，感兴趣的可以私聊。\n国外学术界：\n麻省理工学院：Regina Barzilay, Tommi S. Jaakkola。\n卡内基梅隆大学：Jaime Carbonell, Justine Cassell, William Cohen（主要做信息抽取）, Chris Dyer（主要做机器翻译）, Scott Fahlman,（主要做只是表示和知识推理），Robert Frederking, Eduard Hovy, Alon Lavie, Lori Levin, Brian MacWhinney,（做的比较杂），Teruko Mitamura,（主要做QA），Tom Mitchell, Eric Nyberg，Kemal Oflazer, Carolyn Penstein Rosé,（主要做聊天），Roni Rosenfeld, Noah Smith, Eric Xing。\n约翰·霍普金斯大学：Andreas Andreou，Raman Arora，Jason Eisner, Sanjeev Khudanpur, David Yarowsky, Hynek Hermansky，Mark Dredze, Tom Lippincott，Philipp Koehn，Najim Dehak，Ben van Durme。绝对的NLP领域顶级牛校，研究几乎涵盖所有NLP任务，而且做的都非常好，如果非要说主要研究内容的话：句法分析、机器翻译。\n普林斯顿大学：Sanjeev Arora，Karthik Narasimhan。\n斯坦福大学： Christopher Manning, Daniel Jurafsky, Percy Liang，这几个人不用赘述了吧，实验室做的很广泛，句法分析和词性标注的工具很有名。\n哈弗大学：Stuart Shieber，Alexander Rush，主要做MT、自动摘要和文本生成。\n剑桥大学：Edward J. Briscoe，Ann Copestake，Simone Teufel，Paula Buttery，Andreas Vlachos，摘要、文本生成、NLU、句法分析、IR做的都不错。\n牛津大学：Stephen Pulman，Phil Blunsom（MT非常非常厉害）。\n加州大学伯克利分校：Dan Klein（主要做IE和MT）。\n伊利诺伊大学香槟分校：Margaret M. Fleck (CS)，Roxana Girju (Linguistics)，Mark Hasegawa-Johnson (ECE)，Julia Hockenmaier (CS)， Dan Roth (CS)，ChengXiang Zhai (CS)。\n宾夕法尼亚大学：Mitch Marcus，Dan Roth，Lyle Ungar，Ani Nenkova，Chris Callison-Burch，句法分析做的非常屌，LTAG、Penn Treebank不用过多解释了吧。\n芝加哥大学：John Lafferty（CRF发明人，机器翻译做的也不错）, John Goldsmith\n哥伦比亚大学：Kathy McKeown, Julia Hirschberg，Owen Rambow。\n康奈尔大学：Lillian Lee（主要做SA（情感分析））, Thorsten Joachims（深入学习SVM的话应该知道他）, Claire Cardie, Yoav Artzi。\n俄亥俄州立大学（OSU）：Eric Fosler-Lussier(我是因为做对话知道的他), Michael White（主要做NLG）, William Schuler（主要做句法分析和MT）, Micha Elsner, Alan Ritter, Wei Xu（社交媒体）。\n匹兹堡大学：Ashley Kevin， Brusilovsky Peter, Lewis Michael。\n多伦多大学：Graeme Hirst, Gerald Penn，Frank Rudzic，Suzanne Stevenson，主要做句法分析、语义分析。\n麦吉尔大学：Doina Precup，Jackie Chi Kit Cheung，Joelle Pineau， Prakash Panangaden\n蒙特利尔大学：Yoshua Bengio，不过多解释。\n佐治亚理工：Eric Gilbert（社会计算领域很有名）。\n南加州大学：Jerry Hobbs，Ron Artstein，David DeVault，Kallirroi Georgila，Panayiotis (Panos) Georgiou， Andrew Gordon，Jerry Hobbs，Khalil Iskarous，Kevin Knight，Sungbok Lee， Anton Leuski，Jonathan May，Prem Natarajan，MT、IE、关系挖掘、对话做的都不错。\n华盛顿大学：Tim Althoff，Jeffrey Bilmes，Yejin Choi，Pedro Domingos，Oren Etzioni， Hannaneh Hajishirzi，Noah Smith，Daniel S. Weld，Luke Zettlemoyer，主要做句法分析、MT、对话、IR等。\n爱丁堡大学：Shay Cohen（句法分析），Sharon Goldwater，Kenneth Heafield（MT），Frank Keller（句法分析），Mirella Lapata（NLU、NLG），Adam Lopez，Walid Magdy（IR、DM、社会计算），Rico Sennrich（句法分析、MT），Mark Steedman（对话），Ivan Titov（句法分析、NLU），Bonnie Webber（QA）。\n新加坡国立大学：NG Hwee Tou（主要做MT和句法纠错）。\n马里兰大学：Philip Resnik, Naomi Feldman，Marine Carpuat,Hal Daumé, 主要做MT和IR。\n东北大学：David A. Smith, Byron Wallace, Lu Wang。\n加州大学伯克利分校：Dan Klein，主要做NLP和ML交叉研究。\n加州大学圣巴巴拉分校：William Wang, 主要做IE（信息抽取）和ML。\n加州大学圣克鲁兹分校：Marilyn Walker，主要做dialogue。\n纽约市立学院（CUNY）：Martin Chodorow，Liang huang，Andrew Rosenberg，William Sakas，Virginia Teller。\nUniversity of Massachusetts Amherst：Andrew McCallum（CRF、主题模型）、Bruce Croft、James Allan（IR做的非常屌）。\n纽约大学：Sam Bowman, Kyunghyun Cho，NLU做的非常好。\n北卡罗来纳大学教堂山分校（UNC）：Mohit Bansal, Tamara Berg,主要做句法分析、多模态对话。\n罗切斯特大学：Len Schubert, James Allen（篇章分析、对话做的很好），Dan Gildea（句法分析、MT）。\n谢菲尔德大学：Rob Gaizauskas (Head of Group），Mark Hepple，Lucia Specia（MT很厉害），Mark Stevenson（主要做IR和IE），Yorick Wilks（ACL终身成就奖）。\n还有很多学校、很多学术界大佬没整理了，后续再补充吧。另外美国工业界的NLP大牛也很多，比如google、facebook、microsoft、amazon、IBM等公司。"}
{"content2":"人工智能、机器学习、模式识别、计算机视觉、数据挖掘、信息检索、自然语言处理等作为计算机科学重要的研究分支，不论是学术界还是工业界，有关这方面的研究都在如火如荼地进行着，学习这些方面的内容有一些经典书籍，现总结如下，方便自己和大家以后学习研究：\n人工智能：\n《Artificial Intelligence: A Modern Approach》，第三版，Russell著，权威、经典的人工智能教材，阐述了人工智能的核心内容，反映了人工智能最近10年来的新进展。\n《ProgrammingCollective Intelligence》，Toby Segaran著，本书将带你进入机器学习和统计学的世界，对算法的描述简明清晰，很对代码都可以直接拿去实际应用。\n数据挖掘：\n《DataMining, Concepts and Techniques》，第三版，Han著，数据挖掘领域最具里程碑意义的经典著作。\n《DataMining: Practical Machine Learning Tools and Techniques》,第二版，Witten著，介绍了机器学习的基本理论和实践方法，并提供了一个公开的数据挖掘工作平台Weka，算法部分介绍得很详细。\n信息检索：\n《An Introductionto Information Retrieval》,Manning著，这是一本介绍信息检索的入门书籍，书中对信息检索的基本概念和基本算法做了介绍，适合初学者。\n《Search Engines Information Retrieval in Practice》,Croft著，这本书讲述了搜索引擎的构造方法，通过实际代码展示了搜索引擎的工作原理，对于学生和从事相关领域的工程师，本书都值得一看。\n《Managing Gigabytes》，《Mining the Web -Discovering Knowledge from Hypertext Data》\n《Information Theory：Inference and Learning Algorithms》。\n模式识别和机器学习：\n《Pattern Classification 》，第二版，Duda著，模式识别的奠基之作，但对SVM、Boosting几乎没提，有挂一漏万之嫌。\n《Pattern Recognition and Machine Learning》,Bishop著，侧重概率模型，详细介绍了Bayesian方法、有向图、无向图理论等，体系完备。\n《Kernel Methods for Pattern Analysis》,John Shawe-Taylor著，SVM等统计学的诸多工具里都用到了核方法，可以将将低维非线性空间映射到高维的线性空间中，但同时会引入高维数据的难题。\n计算机视觉：\n《Computer Vision: A Modern Approach》，第二版，Forsyth著，一本不错的计算机视觉教材，全书理论联系实际，并加入了计算机视觉领域的最新研究成果。\n《Computer Vision: Algorithms and Applications》,Richard Szeliski的大作，《数字图像处理》课程老师推荐的一本书籍，这本书我还没有看完，书中对计算机视觉领域最新的一些算法进行了汇编，包括图像分割，特征检测和匹配，运动检测，图像缝合，3D重建，对象识别等图像处理的诸多方面，借助本书我们可以对最新主流图像处理算法有个全局把握。\n线性代数：\n《Linear Algebra and Its Applications》Fourth Edition, Gilbert Strang的著作，本书详细介绍了向量空间、线性变换、本征值和本征向量等线性代数的重要基本概念，把抽象的线性空间形象地表达出来，适合初学者。\n《Introduction to Probability Models》第10版，Ross著，一本书能够发行到第十版，你说是不是很经典呢？\n离散数学：\n《Discrete Mathematics and Its Applications》，第六版，Rosen著，本书囊括了离散数学推导、组合分析、算法及其应用、计算理论等多方面的内容，适合初学者。\n矩阵数学：\n《Matrix Analysis》,Horn著,本书无疑是矩阵论领域的经典著作了，风行几十年了。\n概率论与数理统计：\n《All Of Statistics》,Wasserman著，一本数理统计的简介读本。\n《Introductionto Mathematical Statistics》，第六版，Hogg著，本书介绍了概率统计的基本概念以及各种分布，以及ML，Bayesian方法等内容。\n《Statistical Learning Theory》Vapnik的大作，统计学界的权威，本书将理论上升到了哲学层面，他的另一本书《The Nature ofStatistical Learning Theory》也是统计学习研究不可多得的好书，但是这两本书都比较深入，适合有一定基础的读者。\n《统计学习方法》，李航著，国内很多大学都在用这本书，本书从具体问题入手，由浅入深，简明地介绍了统计学习的主要方法，适合初学者而又想对统计学习理论有一个全局理解的学生。\n《The Elements of Statistical Learning-Data Mining, Inference, and Prediction》,第二版，Trevor Hastie著，机器学习方面非常优秀的一本书，较PC和PRML,此书更加深入，对工程人员的价值也许更大一点。\n《AnIntroduction to Probabilistic Graphical Models》,Jordan著，本书介绍了条件独立、分解、混合、条件混合等图模型中的基本概念，对隐变量（潜在变量）也做了详细介绍，相信大家在隐马尔科夫链和用Gaussian混合模型来实现EM算法时遇到过这个概念。\n《Probabilistic Graphical Models-Principles and Techniques》，Koller著，一本很厚很全面的书，理论性很强，可以作为参考书使用。\n最优化方法：\n《Convex Optimization》，Boyd的经典书籍，被引用次数超过14000次，面向实际应用，并且有配套代码，是一本不可多得的好书，网址http://www.stanford.edu/~boyd/cvxbook/。\n《Numerical Optimization》，第二版，Nocedal著，非常适合非数值专业的学生和工程师参考，算法流程清晰详细，原理清楚。\n另外推荐几个博客和网站：\nhttps://www.coursera.org/，这是一个由世界顶级大学联合创办的网上在线视频公开课网站，里面有stanford, MIT,CMU等计算机科学一流大学提供的免费教学视频，内容全面，计算机科学方面的资源较网易视频公开课网站（http://open.163.com/）内容要新、要全。\nhttp://blog.csdn.net/pongba/article/details/2915005，本文的部分内容就是借鉴刘未鹏大神的博客而来的，也正是看过他的那个书单后，我才决定写一个总结归纳性的文章，这样可以方便大家学习，更可以勉励自己多看些有益的经典书籍。\nhttp://blog.pluskid.org/，这是浙大学生张驰原的博客网站，现在他去了MIT，博客里面的很多资源都值得一看，博文的很大一部分都是关于机器学习的，加入了作者自己的理解，深入浅出。\nhttp://blog.csdn.net/ffeng271/article/details/7164498，林达华推荐的基本数学书，转自MIT大牛博客。"}
{"content2":"来源商业新知网，原标题：清华AMiner：2018自然语言处理研究报告\nAMiner发布研究报告《2018自然语言处理研究报告》。自然语言处理是现代技术最重要的组成部分之一，而最近清华大学和中国工程院知识智能联合实验室发布一份非常全面的 NLP 报告。该报告从 NLP 的概念介绍、研究与应用情况、专家学者概要以及发展趋势这 5 个方向纵览了这一领域的当下与未来，机器之心简要介绍了该报的概要信息，但读者可以从这些方面纵览 NLP 的发展面貌。分析师们主要从以下五个方向六大章节梳理自然语言处理的发展状况：\n自然语言处理概念。首先对自然语言处理进行定义，接着对自然语言的发展历程进行了梳理，对我国自然语言处理现状进行了简单介绍，对自然语言处理业界情况进行介绍。\n自然语言处理研究情况。依据 2016 年中文信息学会发布的中文信息处理发展报告对自然语言处理研究中的重要技术进行介绍。\n自然语言处理领域专家介绍。利用 AMiner 大数据对自然语言处理领域专家进行深入挖掘，对国内外自然语言处理知名实验室及其主要负责人进行介绍。\n自然语言处理的应用及趋势预测。自然语言处理在现实生活中应用广泛，目前的应用集中在语言学、数据处理、认知科学以及语言工程等领域，在介绍相关应用的基础上，对机器翻译未来的发展趋势做出了相应的预测。\n自然语言处理业界发展\n1. Google\nGoogle 是最早开始研究自然语言处理技术的团队之一，作为一个以搜索为核心的公司，Google 对自然语言处理更为重视。Google 拥有着海量数据，可以搭建丰富庞大的数据库，可以为其研究提供强大的数据支撑。Google 对自然语言处理的研究侧重于应用规模、跨语言和跨领域的算法，其成果在 Google 的许多方面都被使用，提升了用户在搜索、移动、应用、广告、翻译等方面的体验。\n2. 百度\n百度自然语言处理部是百度最早成立的部门之一，研究涉及深度问答、阅读理解、智能 写作、对话系统、机器翻译、语义计算、语言分析、知识挖掘、个性化、反馈学习等。其中，百度自然语言处理在深度问答方向经过多年打磨，积累了问句理解、答案抽取、观点分析与 聚合等方面的一整套技术方案，目前已经在搜索、度秘等多个产品中实现应用。篇章理解通过篇章结构分析、主体分析、内容标签、情感分析等关键技术实现对文本内容的理解，目前，篇章理解的关键技术已经在搜索、资讯流、糯米等产品中实现应用。百度翻译目前支持全球 28 种语言，覆盖 756 个翻译方向，支持文本、语音、图像等翻译功能，并提供精准人工翻 译服务，满足不同场景下的翻译需求，在多项翻译技术取得重大突破，发布了世界上首个线 上神经网络翻译系统。\n3. 阿里巴巴\n阿里自然语言处理为其产品服务，在电商平台中构建知识图谱实现智能导购，同时进行全网用户兴趣挖掘，在客服场景中也运用自然语言处理技术打造机器人客服，例如蚂蚁金融智能小宝、淘宝卖家的辅助工具千牛插件等，同时进行语音识别以及后续分析。阿里的机器翻译主要与其国家化电商的规划相联系，可以进行商品信息翻译、广告关键词翻译、买家采 购需求以及即时通信翻译等，语种覆盖中文、荷兰语、希伯来语等语种，2017 年初阿里正式 上线了自主开发的神经网络翻译系统，进一步提升了其翻译质量。\n4. 腾讯\nAI Lab 是腾讯的人工智能实验室，研究领域包括计算机视觉、语音识别、自然语言处理、机器学习等。其研发的腾讯文智自然语言处理基于并行计算、分布式爬虫系统，结合独特的语义分析技术，可满足自然语言处理、转码、抽取、数据抓取等需求，同时，基于文智 API 还可以实现搜索、推荐、舆情、挖掘等功能。在机器翻译方面，2017 年腾讯宣布翻译君 上线「同声传译」新功能，用户边说边翻的需求得到满足，语音识别+NMT 等技术的应用保证了边说边翻的速度与精准性。\n除此之外，该报告还介绍了微软亚洲研究院、Facebook、京东和科大讯飞等在 NLP 方面有非常多研究与应用的机构。（订阅精选研究报告，关注微信号：知识库 Useitcomcn ）"}
{"content2":"环境：python2.7.10\n首先安装pip\n在https://pip.pypa.io/en/stable/installing/ 下载get-pip.py\n然后执行 python get-pip.py 将自动安装pip\nnltk是python的一个扩展包，提供自然语言处理工具集\n安装nltk\nsudo pip install -U nltk\nimport nltk\n然后下载语料库\nnltk.download()\n弹出窗口如下，按需下载，我选择的是book\n、\n下载完成查看目录：\n其中corpora为语料库，也可以下载自己需要的语料库：http://www.nltk.org/nltk_data/\ncorpora中有好多文章和字典，如古腾堡，路透社等文章，wordnet是面向语义的词典，names里包含了好多名字，stopwords包含了可忽略的语法上的高频词汇，words包含平时常用的单词可用来做拼写检查，\n还有city_database，webtext，unicode_samples等语料。\n使用时需要import，如：\nfrom nltk.corpus import gutenberg\nfrom nltk.corpus import stopwords\nwordnet语料库：\nwordnet是普林斯顿大学创建的语义词典，特点是其中包含了大量的单词间的联系，可以看作是一个巨大的词汇网络。\n词与词之间的关系可以为同义，反义，上下位（水果－苹果），整体部分（汽车－轮胎）。建立关系是大脑学习的首要过程，知识的脉络必定可达，孤立点会被遗忘。\n引入wordnet\nfrom nltk.corpus import wordnet as wn\nwordnet API：http://www.nltk.org/howto/wordnet.html\nsynsets()用来查询一个单词，返回结果是Synset数组，一个Synset由 单词－词性－序号 组成："}
{"content2":"不多说，直接上干货！\n为了进一步打造提升（大数据躺过的坑）本微信公众平台的博文高质量水平，特邀请善于分享、主动、敢于专研尝试新技术新领域的您，一起共同维护好我们的知识小天地。目前涉及领域有：大数据领域：Hadoop、Hive、HBase、Zookeeper、Flume、Sqoop、Kafka、Spark、Storm、Zeppelin、Oozie、Azkaban、Flink等。编程语言和脚本语言：Java、Scala、Python、Shell等。机器学习领域：分类、聚类、推荐、回归、优化、降维、神经网络、深度学习等。以及还有人工智能、自然语言处理... ...\n欢迎您的加入！\n打开百度App，扫码，精彩文章每天更新！欢迎关注我的百家号： 九月哥快讯\n大数据躺过的坑  （总群）：   161156071\nhadoop开发   ：       276519852\n大数据零基础入门 ：  416348910\nspark零基础入门： 285025652\nhadoop零基础入门：  541092360\n大数据开发 ：  207591869\n大数据手把手交流 ：  201590535\n大数据零基础收徒咨询  ：   132603465\n大数据内部收徒咨询群  ：  469185229\n第一步：\nhttps://mp.weixin.qq.com/cgi-bin/loginpage\n第二步：\n然后，进入个人的基本信息进行填写。\n这里大家随便选择一个邮箱。我这里以我的163邮箱进行注册。\n你好! 感谢你注册微信公众平台。 你的登录邮箱为：***@163.com。请回填如下6位验证码： ****\n这里，选择个人，然后进行个人信息的填写。包括个人的身份证号码和照片等详细信息。\n这里，使用自己的微信账号进行扫描其二维码。\n由此，立马，在自己的微信账号里收到：公众号管理员身份确认。 选择我确认并遵从协议。\n人生苦短，我愿分享。本公众号将秉持活到老学到老学习无休止的交流分享开源精神，汇聚于互联网和个人学习工作的精华干货知识，一切来于互联网，反馈回互联网。 目前研究领域：大数据、机器学习、深度学习、人工智能、数据挖掘、数据分析。\n大家，可以对自己的个人微信公众号的头像进行初步修改，当然以后也可以更改\n以后怎么来登录自己的个人微信公众号？\n我这里，因为是采用的是自己的163邮箱和密码，来注册。同时，是加上自己的微信号来作为管理员绑定。\n也许，后期会改名。"}
{"content2":"摘要\n这一章将进入机器人语音交互的学习，让机器人能跟人进行语音对话交流。这是一件很酷的事情，本章将涉及到语音识别、语音合成、自然语言处理方面的知识。本章内容：\n1.语音交互相关技术\n2.机器人语音交互实现\n3.自然语言处理云计算引擎\n2.机器人语音交互实现\n其实要自己做一款语音对话机器人还是很容易的，我们只需要选择好语音识别、语音合成、自然语言处理的技术，就可以在一款树莓派3开发板上实现了。由于语音交互系统的核心技术是云端自然语言处理技术，所以我们可以选择网上免费提供的语音识别、语音合成等现有方案，将主要精力用在云端自然语言处理技术的研发上。语音识别与语音合成SDK有：科大讯飞、百度语音、Google…，对于我们墙内玩家…(Google头疼)。经过我自己的实测，发现比较好用的免费SDK是科大讯飞家的，所以强烈推荐。为了测试方便，我先推荐图灵机器人API作为云端自然语言处理技术。等大家将整个语音交互系统的工作原理学会后，随时可以将图灵机器人API替换成自己的云端服务器，从而将主要精力转移到云端自然语言处理技术的研发上。说了这么多，我们先来看看咱们的机器人语音交互软硬件实现的真容吧，如。\n（）机器人语音交互软硬件实现\nUSB麦克风拾取声音，USB声卡和音响播放声音，树莓派3开发板上运行语音识别、语音合成、QA及NLP请求。其中，语音识别和语音合成采用科大讯飞的SDK，QA及NLP请求调用图灵机器人的API接口。\n这里特别说明一下，为什么选用USB声卡而不用树莓派自带AV声卡的原因。你可以直接将耳机插口插入树莓派的AV接口试试，肯定很酸爽！杂音太大。这里就需要硬件支持。杂音原因： 因为树莓派3的AV接口是音频和视频合并输出的，这个接口是美标接口，而在中国是国标的，接口的接地和音频是相反的，这就导致根本不能用了。另外对播放器的支持并不完善。\n2.1.获取科大讯飞的SDK\n科大讯飞提供用于研究用途的语音识别、语音合成的免费SDK，科大讯飞分发该SDK的形式是库文件（libmsc.so）+库授权码（APPID），库文件libmsc.so与库授权码APPID是绑定在一起的，这也是大多说商业软件分发的方式。\n注册科大讯飞账号：\n首先，前往讯飞开放平台（https://www.xfyun.cn），注册科大讯飞账号，注册好后，就可以进入自己的控制台进行设置了，如。\n（）注册科大讯飞账号及登录\n创建应用：\n我们要在科大讯飞的开放平台创建我们需要的应用，这样讯飞就根据应用类型给我们生成对应的SDK库。\n进入讯飞开放平台的控制台后，找到左侧栏的[创建应用]，按要求填写各个选项，注意[应用平台]一栏填Linux，因为我们用的树莓派3开发板装的是Linux系统，如。\n（）创建应用\n创建应用完成后，就要给该应用添加相应的AI技能了，由于我们需要讯飞的在线语音合成、在线语音识别（也就是语音听写），所以添加这两个服务就行了。如。\n（）添加语音合成与识别服务\n申请树莓派3平台对应的Linux SDK库：\n由于科大讯飞开放平台默认只提供PC端x86架构的Linux库，所以如果我们想在树莓派3（树莓派3为ARM架构）上使用科大讯飞的Linux SDK库，就需要另外申请。其实申请方法也很简单，进入科大讯飞中我的语音云页面：\nhttp://www.xfyun.cn/index.php/mycloud/app/linuxCrossCompile\n进行树莓派Linux平台库文件交叉编译申请，选择应用（必须是linux平台的应用），按照默认勾选全部在线服务，平台架构ARM硬件型号Broadcom BCM2837（树莓派3代b型，即树莓派3的SOC，其余版本树莓派，树莓派2为BroadcomBCM2836，更早的版本为BroadcomBCM2835），处理器位数32，运行内存填了1GB。最后记得填上自己的邮箱，提交后，如填写无误正确，你的邮箱将收到可下载库的链接，下载解压后得到libmsc.so，这个库文件就是我们申请的树莓派3平台对应的Linux SDK库了。如。关于交叉编译器和编译脚本，从这里http://pan.baidu.com/s/1pLFPTYr下载，具体交叉可以参考这一篇\nhttp://bbs.xfyun.cn/forum.php?mod=viewthread&tid=32028&highlight=\n（）申请树莓派3平台对应的Linux SDK库\n关于这个库文件对应的库授权码APPID，可以在[我的应用]界面查看，如。\n（）查看库文件对应的库授权码APPID\n2.2.编译安装讯飞语音交互实例ROS版DEMO\n利用科大讯飞提供的SDK库文件和官方API说明文档，我们就可以开发出自己的语音交互实例程序，当然也可以开发对应的ROS程序。在我们的miiboo机器人上开发的语音交互ROS功能包叫miiboo_asr。miiboo_asr功能包文件组织结构，如。其中lib文件夹下存放科大讯飞提供的libmsc.so库文件，iat.cpp是语音识别节点源文件，tts.cpp是语音合成节点源文件，qa_nlp.cpp是QA&NLP逻辑处理节点源文件，其他的文件我们可以不用关心。\n（）miiboo_asr功能包文件组织结构\n了解了miiboo_asr功能包的基本情况后，我们就开始编译安装吧。首先，将miiboo_asr包拷贝到~/catkin_ws_apps/src/目录下。然后将上面申请到的树莓派3平台对应的Linux SDK库libmsc.so文件拷贝到miiboo_asr/lib/中，并将miiboo_asr/CMakeLists.txt文件中有关libmsc.so的路径替换为你存放该libmsc.so的实际路径。如。\n（）CMakeLists.txt文件中有关libmsc.so的路径修改\n接着我们需要将miiboo_asr/launch/xf.launch文件中的各个appid、声卡硬件地址、麦克风硬件地址设置成自己实际的值。关于与libmsc.so库绑定的appid上面已经介绍了查看方法，而声卡硬件地址、麦克风硬件地址的查询也很简单。\n麦克风硬件地址的查询直接使用命令arecord -l，如。\n（）麦克风硬件地址的查询\n在这里麦克风录制设备处于卡1，设备0，于是我们的麦克风硬件地址就是“plughw::CameraB409241”。\n声卡硬件地址的查询直接使用命令aplay -l，如。\n（）声卡硬件地址的查询\n在这里声卡播放设备有三个，卡0中的设备0为3.5音频输出，卡0设备1为HDMI音频输出，卡2设备0为USB声卡输出。这里我推荐使用USB声卡输出，所以我们的声卡硬件地址就是“plughw:DAC”。\n在编译miiboo_asr前，我们还需要安装一些依赖项，其实就是麦克风录音和音乐播放工具，安装命令如下：\nsudo apt-get update sudo apt-get install libasound2-dev sudo apt-get install mplayer\n现在可以编译miiboo_asr了，编译命令如下：\ncd ~/catkin_ws_apps/ catkin_make -DCATKIN_WHITELIST_PACKAGES=”miiboo_asr”\n编译完成后，就可以运行语音交互节点来实现语音对话了，温馨提醒，请确保树莓派已连接网络，因为语音交互节点运行时需要访问网络。启动语音交互各个节点很简单，直接一条命令：\nroslaunch miiboo_asr xf.launch\n节点都运行起来后，会听到欢迎语句“你好，欢迎使用miiboo机器人语音控制系统”，之后就可以对着麦克风说出自己的指令，经语音识别被转换为文本，文本经图灵机器人得到应答，并通过语音合成使我们能听到回答的声音。这样一个语音交互的聊天机器人就诞生了，尽情享受和机器人聊天的乐趣吧^_^\n这里说明一下，如果你使用我们的miiboo机器人，那么miiboo机器人上已经安装编译好了miiboo_asr功能包，所以只需要上面roslaunch miiboo_asr xf.launch这条启动命令，就可以开始机器人聊天之旅。但是，miiboo机器人上安装的miiboo_asr功能包的libmsc.so的访问次数和频率是有限制的，只能供学习使用。如果大家需要将miiboo_asr功能包用来二次开发或实际应用，就需要按照上面的步骤去科大讯飞官网申请自己的SDK库了。\n后记\n------SLAM+语音机器人DIY系列【目录】快速导览------\n第1章：Linux基础\n1.Linux简介\n2.安装Linux发行版ubuntu系统\n3.Linux命令行基础操作\n第2章：ROS入门\n1.ROS是什么\n2.ROS系统整体架构\n3.在ubuntu16.04中安装ROS kinetic\n4.如何编写ROS的第一个程序hello_world\n5.编写简单的消息发布器和订阅器\n6.编写简单的service和client\n7.理解tf的原理\n8.理解roslaunch在大型项目中的作用\n9.熟练使用rviz\n10.在实际机器人上运行ROS高级功能预览\n第3章：感知与大脑\n1.ydlidar-x4激光雷达\n2.带自校准九轴数据融合IMU惯性传感器\n3.轮式里程计与运动控制\n4.音响麦克风与摄像头\n5.机器人大脑嵌入式主板性能对比\n6.做一个能走路和对话的机器人\n第4章：差分底盘设计\n1.stm32主控硬件设计\n2.stm32主控软件设计\n3.底盘通信协议\n4.底盘ROS驱动开发\n5.底盘PID控制参数整定\n6.底盘里程计标\n第5章：树莓派3开发环境搭建\n1.安装系统ubuntu_mate_16.04\n2.安装ros-kinetic\n3.装机后一些实用软件安装和系统设置\n4.PC端与robot端ROS网络通信\n5.Android手机端与robot端ROS网络通信\n6.树莓派USB与tty串口号绑定\n7.开机自启动ROS节点\n第6章：SLAM建图与自主避障导航\n1.在机器人上使用传感器\n2.google-cartographer机器人SLAM建图\n3.ros-navigation机器人自主避障导航\n4.多目标点导航及任务调度\n5.机器人巡航与现场监控\n第7章：语音交互与自然语言处理\n1.语音交互相关技术\n2.机器人语音交互实现\n3.自然语言处理云计算引擎\n第8章：高阶拓展\n1.miiboo机器人安卓手机APP开发\n2.centos7下部署Django(nginx+uwsgi+django+python3)\n----------------文章将持续更新，敬请关注-----------------\n如果大家对博文的相关类容感兴趣，或有什么技术疑问，欢迎加入下面的《SLAM+语音机器人DIY》QQ技术交流群，一起讨论学习^_^"}
{"content2":"参考 Applying Deep Learning To Answer Selection: A Study And An Open Task\nfollow: http://www.52nlp.cn/qa%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E5%AE%9E%E7%8E%B0\n网络结构：\nQ&A共用一个网络，网络中包括HL，CNN，P+T和Cosine_Similarity\nHL是一个g(W*X+b)的非线性变换\nCNN卷积神经网络\nP是max_pooling\nT是激活函数Tanh\nCosine_Similarity表示将Q&A输出的语义表示向量进行相似度计算。\n来自为知笔记(Wiz)"}
{"content2":"算法：一套计算机要遵循的指令。一个算法可以是一个简单的单步程序也可以是一个复杂的神经网络，但是通常被用来指一个模型。\n人工智能：这是一个统称。广义上说，软件意味着模仿或取代人类智能的各个方面。人工智能软件可以从图像或文本、经验、进化或其他研究人员的发明等数据中学习。\n计算机视觉：人工智能研究探索图像和视频识别和理解的领域。这个领域从了解苹果的外观，到苹果的功能用途，以及与之相关的理念。它是被用作自动驾驶汽车、谷歌图像搜索以及Facebook上自动贴标签的主要技术。\n深度学习：一个神经网络被分层来理解数据中的复杂模式和关系的领域。当一个神经网络的输出成为另一个神经网络的输入时，有效地将它们叠加起来，由此产生的神经网络就是“深度”了。\n普通智力：有时被称为“强人工智能”，一般智能将能够在不同的任务中学习和应用不同的想法。\n生成式对抗网络：这是一个包含两个神经网络的系统，一个是用来生成输出的，另一个是用来检验这个输出的质量是否是想要的输出的神经网络。例如，当试图生成一个苹果的图片时，生成器将生成一个图像，而另一个(称为鉴别器)如果不能识别图像中的一个苹果，会使生成器再次尝试生成。\n机器学习：机器学习(ML)常常与术语人工智能结合在一起，是使用算法从数据中学习的惯例。\n模型：模型是一种机器学习算法，它可以建立自己对某一主题的理解，或者它自己的世界模型。\n自然语言处理：用于理解语言中思想的意图和关系的软件。\n神经网络：通过连接起来的数学方程式的网络，模拟大脑处理信息的方式以建立起来的算法。提供给神经网络的数据被分解成更小的块并根据网络的复杂性分析其基础模式成千上万次。当一个神经网络的输出被输入到另一个神经网络的输入时，这两个神经网络就会链接到一起成为分层，成为一个深层的神经网络。通常，深度神经网络的层会分析越来越高的抽象层的数据，这意味着，在得到最简单和最准确的数据表示之前，它们会将有用数据从没有必要的数据中提取出来。\n卷积神经网络：一个主要用来识别和理解图像、视频和音频数据的神经网络，因为它能够处理密集的数据，比如数百万像素的图像或数千个音频文件样本。\n递归神经网络：一种用于自然语言处理的神经网络，它可以周期性地、连续地分析数据，这意味着它可以处理像单词或句子这样的数据，同时在句子中保持它们的顺序和上下文。\n长短期记忆网络：一种周期性的神经网络的变体，它的是用来根据数据来保留结构化的信息。例如，RNN可以识别句子中的所有名词和形容词，检查它们是否被正确使用，但LSTM可以记住一本书的情节。\n强化学习：一种能够从经验中学习的深度学习算法。是可以控制环境的某些方面的算法，比如视频游戏的角色，然后通过反复试验和错误来学习。由于它们是高度可重复的，作为三维世界的模型，并且已经在电脑上玩了，许多强化学习的突破都来自于玩视频游戏的算法。在DeepMind的AlphaGo中，RL是机器学习的主要类型之一，它在围棋中击败了世界冠军Lee Sedol。在现实世界中，在网络安全等领域已经证明了这一点，软件学会了欺骗反病毒软件，使其认为恶意文件是安全的。\n超级智能：比人脑还要更强大的人工智能。很难定义它因为我们仍然无法客观地衡量人类的大脑能做什么。\n监督式学习：在被训练的过程中，给其提供的数据是已经组织好的、已经被贴好标签的机器学习。如果你正在建立一种监督式的学习算法来识别猫，你就可以在1000张猫的图片上训练这个算法。\n训练：通过提供数据来让算法学习的过程。\n无监督学习：机器学习算法的一种，没有给出任何关于它应该如何对数据进行分类的信息，并且必须找到它们之间的关系的算法。像Facebook LeCun这样的人工智能研究人员将无人监督的学习视为人工智能研究的圣杯，因为它与人类自然学习的方式非常相似。“在无人监督的学习中，大脑比我们的模型好得多”，LeCun告诉IEEE光谱，“这就意味着我们的人工学习系统缺少了一些非常基本的生物学习原理”。"}
{"content2":"一、前述\n视觉问答（Visual Question Answering，VQA），是一种涉及计算机视觉和自然语言处理的学习任务。这一任务的定义如下： A VQA system takes as input an image and a free-form, open-ended, natural-language question about the image and produces a natural-language answer as the output[1]。 翻译为中文：一个VQA系统以一张图片和一个关于这张图片形式自由、开放式的自然语言问题作为输入，以生成一条自然语言答案作为输出。简单来说，VQA就是给定的图片进行问答。\nVQA系统需要将图片和问题作为输入，结合这两部分信息，产生一条人类语言作为输出。针对一张特定的图片，如果想要机器以自然语言来回答关于该图片的某一个特定问题，我们需要让机器对图片的内容、问题的含义和意图以及相关的常识有一定的理解。VQA涉及到多方面的AI技术（）：细粒度识别（这位女士是白种人吗？）、 物体识别（图中有几个香蕉？）、行为识别（这位女士在哭吗？）和对问题所包含文本的理解（NLP）。综上所述，VQA是一项涉及了计算机视觉（CV）和自然语言处理（NLP）两大领域的学习任务。它的主要目标就是让计算机根据输入的图片和问题输出一个符合自然语言规则且内容合理的答案。\n二、具体步骤\n2.1  第一步，生成答案\n2.2  第二步，处理输⼊源数据\n2.2.1 处理输⼊源数据：图⽚\n卷积CNN结合VGG-16模型\nVGG-16的标准构造 (keras)\ndef VGG_16(weights_path=None): model = Sequential() model.add(ZeroPadding2D((1,1),input_shape=(3,224,224))) model.add(Convolution2D(64, 3, 3, activation='relu')) model.add(ZeroPadding2D((1,1))) model.add(Convolution2D(64, 3, 3, activation='relu')) model.add(MaxPooling2D((2,2), strides=(2,2))) model.add(ZeroPadding2D((1,1))) model.add(Convolution2D(128, 3, 3, activation='relu')) model.add(ZeroPadding2D((1,1))) model.add(Convolution2D(128, 3, 3, activation='relu')) model.add(MaxPooling2D((2,2), strides=(2,2))) model.add(ZeroPadding2D((1,1))) model.add(Convolution2D(256, 3, 3, activation='relu')) model.add(ZeroPadding2D((1,1))) model.add(Convolution2D(256, 3, 3, activation='relu')) model.add(ZeroPadding2D((1,1))) model.add(Convolution2D(256, 3, 3, activation='relu')) model.add(MaxPooling2D((2,2), strides=(2,2))) model.add(ZeroPadding2D((1,1))) model.add(Convolution2D(512, 3, 3, activation='relu')) model.add(ZeroPadding2D((1,1))) model.add(Convolution2D(512, 3, 3, activation='relu')) model.add(ZeroPadding2D((1,1))) model.add(Convolution2D(512, 3, 3, activation='relu')) model.add(MaxPooling2D((2,2), strides=(2,2))) model.add(ZeroPadding2D((1,1))) model.add(Convolution2D(512, 3, 3, activation='relu')) model.add(ZeroPadding2D((1,1))) model.add(Convolution2D(512, 3, 3, activation='relu')) model.add(ZeroPadding2D((1,1))) model.add(Convolution2D(512, 3, 3, activation='relu')) model.add(MaxPooling2D((2,2), strides=(2,2))) model.add(Flatten()) model.add(Dense(4096, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(4096, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(1000, activation='softmax')) if weights_path: model.load_weights(weights_path) return model\n2.2.2 处理输⼊源数据：⽂字\n2.3  第三步， 选取VQA模型-MLP\n2.3.1 选取VQA模型-MLP\n2.3.2 选取VQA模型-LSTM"}
{"content2":"3.6 Normalizing Text   规格化文本\nIn earlier program examples we have often converted text to lowercase before doing anything with its words, e.g., set(w.lower() for w in text). By using lower(), we have normalized the text to lowercase so that the distinction between The and the is ignored. Often we want to go further than this and strip off any affixes（词缀）, a task known as stemming（提取词干）. A further step is to make sure that the resulting form is a known word in a dictionary, a task known as lemmatization（词元化）. We discuss each of these in turn. First, we need to define the data we will use in this section:\n>>> raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n... is no basis for a system of government. Supreme executive power derives from\n... a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n>>> tokens = nltk.word_tokenize(raw)\nStemmers   词干器\nNLTK includes several off-the-shelf stemmers, and if you ever need a stemmer, you should use one of these in preference to crafting（制作） your own using regular expressions, since NLTK’s stemmers handle a wide range of irregular cases. The Porter and Lancaster stemmers follow their own rules for stripping affixes. Observe that the Porter stemmer correctly handles the word lying (mapping it to lie), whereas the Lancaster stemmer does not.（我觉得basis两个分得都不好）\n>>> porter = nltk.PorterStemmer()\n>>> lancaster = nltk.LancasterStemmer()\n>>> [porter.stem(t) for t in tokens]\n['DENNI', ':', 'Listen', ',', 'strang', 'women', 'lie', 'in', 'pond',\n'distribut', 'sword', 'is', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern',\n'.', 'Suprem', 'execut', 'power', 'deriv', 'from', 'a', 'mandat', 'from',\n'the', 'mass', ',', 'not', 'from', 'some', 'farcic', 'aquat', 'ceremoni', '.']\n>>> [lancaster.stem(t) for t in tokens]\n['den', ':', 'list', ',', 'strange', 'wom', 'lying', 'in', 'pond', 'distribut',\n'sword', 'is', 'no', 'bas', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem',\n'execut', 'pow', 'der', 'from', 'a', 'mand', 'from', 'the', 'mass', ',', 'not',\n'from', 'som', 'farc', 'aqu', 'ceremony', '.']\nStemming is not a well-defined process, and we typically pick the stemmer that best suits the application we have in mind. The Porter Stemmer is a good choice if you are indexing some texts and want to support search using alternative forms of words (illustrated in Example 3-1, which uses object-oriented programming techniques that are outside the scope of this book, string formatting techniques to be covered in Section 3.9, and the enumerate() function to be explained in Section 4.2).\nExample 3-1. Indexing a text using a stemmer.\nclass IndexedText(object):\ndef __init__(self, stemmer, text):\nself._text = text\nself._stemmer = stemmer\nself._index = nltk.Index((self._stem(word), i)\nfor (i, word) in enumerate(text))\ndef concordance(self, word, width=40):\nkey = self._stem(word)\nwc = width/4                # words of context\nfor i in self._index[key]:\nlcontext = ' '.join(self._text[i-wc:i])\nrcontext = ' '.join(self._text[i:i+wc])\nldisplay = '%*s' % (width, lcontext[-width:])\nrdisplay = '%-*s' % (width, rcontext[:width])\nprint ldisplay, rdisplay\ndef _stem(self, word):\nreturn self._stemmer.stem(word).lower()\n>>> porter = nltk.PorterStemmer()\n>>> grail = nltk.corpus.webtext.words('grail.txt')\n>>> text = IndexedText(porter, grail)\n>>> text.concordance('lie')\nr king ! DENNIS : Listen , strange women lying in ponds distributing swords is no\nbeat a very brave retreat . ROBIN : All lies ! MINSTREL : [ singing ] Bravest of\nNay . Nay . Come . Come . You may lie here . Oh , but you are wounded !\ndoctors immediately ! No , no , please ! Lie down . [ clap clap ] PIGLET : Well\nere is much danger , for beyond the cave lies the Gorge of Eternal Peril , which\nyou . Oh ... TIM : To the north there lies a cave -- the cave of Caerbannog --\nh it and lived ! Bones of full fifty men lie strewn about its lair . So , brave k\nnot stop our fight ' til each one of you lies dead , and the Holy Grail returns t\nLemmatization 词元化\nThe WordNet lemmatizer removes affixes only if the resulting word is in its dictionary. This additional checking process makes the lemmatizer slower than the stemmers just mentioned. Notice that it doesn’t handle lying, but it converts women to woman.\n>>> wnl = nltk.WordNetLemmatizer()\n>>> [wnl.lemmatize(t) for t in tokens]\n['DENNIS', ':', 'Listen', ',', 'strange', 'woman', 'lying', 'in', 'pond',\n'distributing', 'sword', 'is', 'no', 'basis', 'for', 'a', 'system', 'of',\n'government', '.', 'Supreme', 'executive', 'power', 'derives', 'from', 'a',\n'mandate', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcical',\n'aquatic', 'ceremony', '.']\nThe WordNet lemmatizer is a good choice if you want to compile the vocabulary of some texts and want a list of valid lemmas (or lexicon headwords（中心词）).\nAnother normalization task involves identifying non-standard words, including numbers, abbreviations, and dates, and mapping any such tokens to a special vocabulary. For example, every decimal number could be mapped to a single token 0.0, and every acronym（首字母缩写词） could be mapped to AAA. This keeps the vocabulary small and improves the accuracy of many language modeling tasks."}
{"content2":"■引言\n自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。自然语言处理是一门融语言学、计算机科学、数学于一体的科学。因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，所以它与语言学的研究有着密切的联系，但又有重要的区别。自然语言处理并不是一般地研究自然语言，而在于研制能有效地实现自然语言通信的计算机系统，特别是其中的软件系统。因而它是计算机科学的一部分。\n■定义\n自然语言处理（NLP）是计算机科学，人工智能，语言学关注计算机和人类（自然）语言之间的相互作用的领域。 ———————————摘自百度百科\n■理解\n以上是对自然语言的官方定义，当然一看到这么一大段话，我想许多人都会有一些抵触，当然我也并不喜欢看那么一大段话，但是总结一段话就是，用我们平常的文字语言描述事物的特性、状态等。\n首先，我从信息安全方面着手和自然语言处理的联系，我们先来了解一段信息安全方面的背景，我想很多人都看过了需要的加密、解密等方面的电影、电视剧，有时候我们常常惊叹到剧中的人好高深，为了保证信息的安全性，用了各种手段去加密。我们最常了解到的就是凯撒密码就是典型的加密解密问题。当然最著名的还是《模仿游戏》里图灵破解英格玛机的精彩，至今我也能够清晰记得。但是加密、解密只是信息安全的一个分支，信息安全涵盖的范围真的太广了，以至于无法一言一语就能讲清楚的。有了这些背景以后我们可以开始了解这两者的关系，以及自然语言处理在信息安全中的重要性。\n信息安全，我想在近几年可谓是炙手可热，上至国家、下至学校都在不停的关注和完善我们信息安全、网络安全等方面，前几天还发布了“网络安全等级保护制度2.0标准”，可想而知国家也正在向着网络、信息更安全，更保密的方向发展，当然也需要许多人一起来构建，并不是一个人能完成的大事。\n我认为自然语言是自然界涵盖信息量最大的语言了，因为我们能够从各种文字里推出各种信息，当然信息和数据是一个不同的概念，数据要经过加工、分析、处理后才能得到信息。这也是自然语言没有被计算机语言替代的可能原因吧，因为自然语言里面涵盖的信息很多很多，我们下面通过一个例子来看：\n范例：自然语言描述\n下雨天留客,天留我不留.\n(下雨天时留住客人,但主人自白心声,天虽要留人,我不愿留客人.）\n.下雨天留客,天留我?不留.\n（下雨天时留住客人,客人自问天意要留住自己吗?客人的决定是不留.）\n通过一句话能够得出两种或者更多的信息，这是不同的人有不同的理解，当然也表明了自然语言若是处理不好，便可产生歧义性，这在信息安全方面也是。但是这并不能说它不是最好的语言，毕竟人无完人。\n于是我们可以利用自然语言中的歧义性用于信息安全方面，当然这在信息安全里面有一个专业的术语叫混淆，从这个简单的范例我们可以得知，自然语言是信息安全方面离不开关系。有了信息之后才有信息安全的市场，如果每个人都不想为自己的隐私保护，信息安全也就可以推出市场了。但是每个人都有一些不想让别人了解的事情。这恰恰使安全有了市场。\n通过上面我们可以得知自然语言处理与信息安全的关系了，下面是我对这两者的看法以及这两者以后的发展。\n■个人看法\n自然语言处理计算在信息安全方面可谓是矛与盾的关系，使用好了就是盾，使用不好就是矛。首先，自然语言处理在加密的方面有着一定的作用，毕竟从海量的数据里找到有用的信息不是那么容易的事。\n所以我认为自然语言处理能够在加密方面帮助到信息安全，使之信息能够更加透明、更加保密、更加完整。\n其次，自然语言在人工智能方面也有发展，这也间接加紧了信息安全与自然语言等方面的联系。\n因为人工智能能够智能检测出某人的行为是否越出了平常的行为，若是和平常的行为差距较大，我们可以认为这个人的信息可能已经泄露了，或者用于不好的方面了。\n然后，我认为信息安全是依赖于自然语言发展起来的，因为若是没有信息的重要性，我们便不用去保护信息的完整性、保密性等等，所以自然语言在日后一定会更加和信息安全紧密相连，这是不可争执的事实，也是历史发展的趋势，因为现在每个人都不喜欢自己的隐私被泄露。然而在这个信息开放的时代我们能够做的就是保护自己的信息，使自己能够在网络更加透明，当然这个透明是对于计算机来说，就是我们看不到，不是别人全看到的意思。所以我认为信息安全一定能够和自然语言走得更近，更远。这两者是不可分的关系，一旦没有安全，信息的保密性不存在；而没有信息，则安全也没有用途，所以这两者的关系我们不可简单的去分开讨论，这两者的关系是非常的密切。\n未来信息安全一定能越来越受欢迎，当人们认识到它的重要性的时候，我们已经泄露了太多太多的信息。当然亡羊补牢为时不晚。到那时候我们一定能够理解自然语言处理在信息安全方面的重要性，不可缺失。\n我相信信息安全一定能够是和自然语言处理走得最近的了，其重要性和联系我在以上已有阐述，这里不再过多阐述。当看到信息安全能够深入到每一个人的脑海中时，我认为这是一个会保护隐私的时代。\n以上是我对信息安全与自然语言处理关系的理解，当然由于本人的水平有限，有时候词语可能用的较为绝对或者用得不是那么恰当。"}
{"content2":"国人写的两本书，个人感觉还是不错的，一方面学习深度学习的理论，另一方面可以使用tensorflow进行测试，收效较快。\n《深入理解TensorFlow架构设计与实现原理》从基本概念、内部实现和实践等方面深入剖析了TensorFlow。\n《深入理解TensorFlow架构设计与实现原理》PDF，375页，带书签目录，文字可以复制。\n作者：彭靖田,林健,白小龙\n下载: https://pan.baidu.com/s/1ZjznggugS3S_ivrLqeydMg\n提取码: wp4w\n《深入理解TensorFlow架构设计与实现原理》分为五大部分，这五个部分，我认为最有用的是第4部分，能够学会对底层进行改进。\n第一部分为基础篇（第1～3章），简单介绍了TensorFlow设计目标、基本架构、环境准备和基础概念，包括数据流图的设计与使用，以及TensorFlow运行环境和训练机制，帮助读者快速入门TensorFlow，迅速上手使用。\n第二部分为关键模块篇（第4～7章），着重讲解了使用TensorFlow端到端解决人工智能问题涉及的关键模块，包括数据处理、编程框架、可视化工具和模型托管工具，帮助读者进一步提升开发效率，快速落地模型应用。\n第三部分为算法模型篇（第8～11章），在读者熟练掌握TensorFlow后，该部分将深度学习与TensorFlow有机结合，系统介绍了深度学习的发展历史与应用场景，并结合理论与代码实现深入讲解了CNN、GAN和RNN等经典模型。\n第四部分为核心揭秘篇（第12～14章），深入剖析了TensorFlow运行时核心、通信原理和数据流图计算的原理与实现，聚焦C++ 核心层的揭秘，帮助读者进一步理解TensorFlow底层的设计思想与实现细节，TensorFlow二次开发人员需重点关注这部分内容。\n第五部分为生态发展篇（第15章），全面介绍了TensorFlow生态系统发展，并重点介绍了Keras深度学习算法库，以及TensorFlow与云原生社区Kubernetes生态的结合、与大数据社区Spark生态的结合，并介绍了TensorFlow通信优化技术、TPU及NNVM模块化深度学习技术，帮助读者进一步全面了解深度学习生态发展的现状。\n学了理论后，我们可以通过《Tensorflow+Keras-深度学习人工智能实践应用》进行实践。\n《Tensorflow+Keras-深度学习人工智能实践应用》PDF，329页，带书签目录，文字可以复制。\n配套源代码。\n下载：https://pan.baidu.com/s/1NwhEBvB7gwvO81e8PhH75g\n通过学习《Tensorflow+Keras-深度学习人工智能实践应用》，我学会了安装、上机操作，也测试了大部分范例程序。\n分9部分，共21章，内容主要包括基本概念介绍、TensorFlow 与 Keras 的安装、Keras MNIST手写数字识别、Keras CIFAR-10照片图像物体识别、Keras多层感知器预测泰坦尼克号上旅客的生存概率、使用Keras MLP、RNN、LSTM进行IMDb自然语言处理与情感分析、以TensorFlow张量运算仿真神经网络的运行、TensorFlow MNIST手写数字识别、使用GPU大幅加快深度学习训练。\nTensorFlow + Keras深度学习方面的知识不需要具备高等数学模型、算法等专业知识，只需要具备基本的Python程序设计 能力，按照步骤循序渐进地学习，就可以了解深度学习的基本概念，进而实际运用深度学习的技术。\n在学习过程中，也会遇到这样那样的问题，入门阶段可以参考《白话深度学习与TensorFlow》\n《白话深度学习与TensorFlow》中文版PDF，带书签目录，文字可以复制，322页。\n下载：https://pan.baidu.com/s/1sOIhiaFUP00Azg-85tov_g\n基础篇（第1～3章），讲解了机器学习、深度学习与实践的上下文知识，如基本的机器学习与深度学习算法，TensorFlow框架的安全与配置，简单的深度学习实践。该篇是阅读和实践的基石。\n原理与实践篇（第4～8章），介绍“老牌”的深度学习网络的数学原理和工程实现原理，尤其是第4章，如 果能基本读懂，后面的网络实现层面的问题基本都可以迎刃而解。涵盖BP网络、CNN、RNN的结构、思路\n、训练与使用，以及一些常见的综合性问题。该篇是学习深度学习的重点和难点，作者通过大量示例、 推理与实现，帮读者*大化降低学习曲线。\n扩展篇（第9～13章），介绍一些网络的变种和一些较新的网络特性，涵盖深度残差网络、受限玻尔兹曼机、强化学习、对抗学习，这是读者进一步学习与实践思路的钥匙。最后给出了一些有趣的深度学习应 用：人脸识别、作诗姬、大师风图像处理，有趣又有用。\n个人感觉，学习过程中，需要配合好的资料，并且随时进行整理，这样才能收到好的学习效果。"}
{"content2":"--------------------------------------------------------概念定义-----------------------------------------------------------\n参考以下链接：http://www.cnblogs.com/biyeymyhjob/archive/2012/07/17/2595249.html\nTF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。\n在一份给定的文件里，词频 (term frequency, TF) 指的是某一个给定的词语在该文件中出现的次数。这个数字通常会被归一化（分子一般小于分母 区别于IDF），以防止它偏向长的文件。（同一个词语在长文件里可能会比短文件有更高的词频，而不管该词语重要与否。）\n逆向文件频率 (inverse document frequency, IDF) 是一个词语普遍重要性的度量。某一特定词语的IDF，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到。\n某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语\n--------------------------------------------------------计算方法------------------------------------------------------------------\n参考以下链接：http://blog.csdn.net/baimafujinji/article/details/51476117\n一些基于自然语言处理的预处理过程也会在本文中出现。如果你对NLTK和Scikit-Learn两个库还很陌生可以参考如下文章：\nNLTK\n自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。自然语言工具箱（NLTK，Natural Language Toolkit）是一个基于Python语言的类库\n当然本文涉及的主要任务都是自然语言处理中最常用，最基础的pre-processing过程，\n可以利用NLTK：1 进行分句 2 进行分词  等我想说《Python 自然语言处理》是一本很好的书。"}
{"content2":"3 月 18 日，由李飞飞担任所长之一的「以人为本人工智能研究所」（HAI）自启动以来不短的时间后，终于完成了正式成立的高光时刻。而正式上线的官网日前也更新了两条博文，一篇是详尽介绍 HAI 的文章；另一篇则是今天要给大家介绍的斯坦福 HAI 版人工智能简史图，图中涵盖的信息包括：人工智能大事记、全球人工智能初创公司布局、人工智能的普及度进展、人工智能的研究和教育进展以及斯坦福的人工智能突破性成果和人工智能课程。\n而之所以给此图加上「斯坦福 HAI 版」的前缀，则是因为本图笼罩着满满的「斯坦福」光环。本图虽然不够详尽，但是作为大家一窥人工智能历史以及目前相关进展的精简版素材，还是足够的。\n下面就一起逐个看看吧~\n人工智能大事记\n1955 年，在达特矛斯会议上，当时备受尊敬的计算机科学家约翰·麦卡锡首度提出「人工智能」这一概念。之后，本次会议也被视作人工智能正式诞生的标志，而提出者约翰·麦卡锡也被誉为「人工智能之父」。\n1963 年，「人工智能之父」约翰·麦卡锡创建了斯坦福人工智能实验室（SAIL，Stanford Artificial Intelligence Laboratory）。该实验室拥有多个领域的专家，涉及机器人技术、计算机视觉、机器学习、图像处理、自然语言处理等多个领域，代表人物包括一大批在人工智能领域闻名遐迩的人物，如 Christopher Manning 、吴恩达、李飞飞等。\n1966 年，MIT 计算机科学家 Joseph Weizenbaum 开发出首个自然语言处理程序——ELIZA，它是一个模拟罗杰斯心理治疗的聊天机器人。\n1967 年，斯坦福大学 E.A. Feigenbaum 领导开发出第一个「专家系统」——DENRAL， 使得人工智能的研究以推理算法为主转变为以知识为主。\n1969 年，阿瑟·布莱森（Arthur Bryson）和何毓琦（Yu-Chi Ho）提出机器学习领域最重要的算法——反向传播算法（Backpropagation）。这种算法可用于多层人工神经网络，2000 年至今深度学习的发展都离不开它的启发。\n1973 年，美国斯坦福国际研究所（Stanford Research Institute, SRI）研制出首台采用了人工智能学的移动机器人——Shakey。\n1979 年，汉斯·摩拉维克（Hans Moravec）在斯坦福大学就读研究生时期发明的 Stanford Cart，在无人干预的情况下自动穿过摆满椅子的房间并前后行驶了 5 小时。Stanford Cart 相当于早期无人驾驶汽车。\n1988 年，IBM 沃森研究中心发表《机器翻译的统计方法》（A STATISTICAL APPROACH TO LANGUAGE TRANSLATION），预示着基于规则的机器翻译方法开始转变为基于概率的方法，并反映了一个更为广泛的转变：从「理解」眼前的任务的「机器学习」方法转变为基于已知例子的统计分析方法。\n1991 年，蒂姆·伯纳斯－李（Tim Berners-Lee）发明的万维网首次上线。\n1997 年，IBM 研发的「深蓝」（Deep Blue）成为第一个击败人类象棋冠军 Garry Kasparov 的电脑程序。,\n1998 年，斯坦福大学教授肯尼斯·萨里斯伯里（Kenneth Salisbury）公开外科机器人（robotic surgery）专利。\n2005 年，斯坦福大学教授 Sebastian Thrun 联合斯坦福大学 AI 实验室发明的第一辆自动驾驶汽车完成了 132 英里的 Mojava 沙漠路线，在 DARPA 超级挑战赛（DARPA Grand Challenge）上一举夺冠。\n2009 年，李飞飞主导的 ImageNet 项目诞生了一个含有 1500 万张照片的数据库，涵盖了 22000 种物品。这个项目以及后来的一系列工作影响了整个计算机视觉领域发展。\n2010 年，苹果公司推出一款内建在苹果 iOS 系统中的人工智能助理软件 Siri。\n2011 年，IBM 开发的自然语言问答计算机沃森在美国老牌益智节目「危险边缘」（Jeopardy！）中击败人类。\n2012 年，杰夫·迪恩（Jeff Dean）和吴恩达（Andrew Ng）发布一份实验报告，他们给一个大型神经网络展示 1000 万张未标记的网络图片，发现神经网络能够识别出猫的形象。\n2014 年，亚马逊推出了智能音箱 Echo 以及智能语音助手 Alexa。\n2016 年，谷歌 DeepMind 研发的 AlphaGo 击败围棋世界冠军李世石。\n全球人工智能初创公司布局\nHAI 援引全球知名创投研究机构 Asgard CB Insights 的调查数据， 目前全球共涌现了 3600 余家人工智能初创公司，其中美国有 1393 家，占比 40％ 左右，排名第一；中国有 383 家，占比 11％ 左右，排名第二；而欧洲诸国中拥有 AI 公司数量最多的是英国，其有 245 家，占比 7% 左右；以色列有 362 家，占比 10％ 左右；加拿大仅有 131 家，占比 4% 左右。\n同时，Asgard CB Insights 还收集了美国各城市所拥有人工智能初创数量的数据，其中旧金山有 596 家，排名第一；纽约有 180 家，排名第二；波士顿有 102 家，排名第三；之后依次是洛杉矶、华盛顿、奥斯汀、西雅图，分别为 73 家、36 家、36 家以及 35 家。另外据 Pricewaterhousecoopers，2018 年美国初创公司从投资公司募集到的融资总额为 93 亿美元。\n另外，针对人工智能对全球经济的贡献，高德纳（Gartner）发布的报告预测，至 2030 年，人工智能将为全球经济贡献 15.7 万亿美元，这一数字将超过中国与印度两个国家目前的经济总量之和。\n人工智能的普及度进展\n针对人工智能的普及度，HAI 主要收集了美国国会提及「人工智能」的次数以及上市公司投资者所提及的科技术语次数两个方面的数据。\n随着人工智能的发展，美国国会提到「人工智能」的频率也日益增高，2017 年以前，美国国会提及「人工智能」的次数微乎其微，最高也不到 20 次，然而在 2017 年提及的次数竟飙升至近 100 次，这也从侧面展示了各国已经将人工智能布局提升到了国家战略布局的高度。\n而上市公司投资者所提及的科技术语次数方面，在 2011 年前，被提及最多的术语是「云计算」；2011 年至 2016 年是「大数据」；2016 年至 2017 年，「机器学习」与「人工智能」二者不相上下；而 2017 年后则是「人工智能」被提及的次数最多了。这些数据从侧面反映出了科技领域的发展趋势。\n人工智能的研究和教育进展\n在人工智能的研究和教育方面，HAI 援引 AI index 统计的学术论文发表数量和登记为「人工智能入门课程」的大学课程数量。其中学术论文发表数量方面，自 1996 年以来，关于「人工智能」主题的学术论文发表数量翻了 8 倍以上；而 2012 年至 2017 年，大学课程中登记为「人工智能入门课程」的课程数量的增长量高达 500%。这无疑彰显了学术界对于「人工智能」研究的极高热情。\n斯坦福的人工智能突破性成果\n而对于人工智能领域所取得的突破性成果，HAI 则重点列举了在人工智能领域具有代表性意义的高校——斯坦福大学的研究成果，包括：\n斯坦福大学 AI 实验室发明的自动驾驶汽车 Stanley 获得了 DARPA 超级挑战赛（DARPA Grand Challenge）冠军；\n斯坦福大学教授李飞飞主导的 ImageNet 改变了机器学习/AI 的发展路线，并引领了深度学习时代；\n斯坦福大学 AI 实验室共有 18 位成员获得 ACM 图灵奖；\n斯坦福大学联合斯坦福医学院开发出第一个用于医疗 AI 的超级计算机；\n斯坦福国际研究所（Stanford Research Institute, SRI）研制出第一台真正意义上的移动机器人；\n斯坦福大学于 2010 年发布领先的开源自然语言处理工具包——Stanford CoreNLP；\n斯坦福大学研制出人形潜水机器人——OceanOne，该机器人具有触觉反馈功能，可以高保真地探索海底世界。\n斯坦福人工智能课程\n在 2018 年，斯坦福开设的人工智能相关课程涵盖了 54 个研究主题，包括计算机视觉、自然语言处理、高级机器人以及计算基因组学等。其中最受欢迎的则要数吴恩达推出的《CS 221： 人工智能原理与技术》课程。雷锋网雷锋网\n本文转自：https://www.linuxprobe.com/stanford-hai-detailed.html"}
{"content2":"一、自然语言处理与深度学习\n自然语言处理应用\n深度学习模型\n为什么需要用深度学习来处理呢\n二、语言模型\n1、语言模型实例:\n机器翻译\n拼写纠错                                                                     智能问答\n1）机器翻译，比如要翻译高价，可能 P(high price) > P(large price)，然后得到的结果就是high price\n2）拼写纠错，比如 fifteen minutes，P(about fifteen minutes from) > P(about fifteenminuets from)，一般时分开写的，如果合在一起则会纠正为分开书写\n3）语言模型举例\n我 今天 下午 打 篮球\np(S) = p(w1,w2,w3,w4,w5,...,wn)\n= p(w1)p(w2|w1)p(w3|w1,w2) ... p(wn|w1,w2,...,wn-1)\n上式中wi表示每个词\np(S)被称为语言模型，即用来计算一个句子概率的模型\n2、语言模型存在哪些问题呢?\np(wi|w1,w2,...,wi-1) = p(w1,w2,...,wi-1,wi) / p(w1,w2,...,wi-1)\n1）数据过于稀疏\n2）参数空间太大\n三、N-gram模型\n假设下一个词的出现依赖它前面的一个词:\np(S)=p(w1)p(w2|w1)p(w3|w1,w2)...p(wn|w1,w2,...,wn-1)\n=p(w1)p(w2|w1)p(w3|w2)...p(wn|wn-1)\n假设下一个词的出现依赖它前面的两个词:\np(S)=p(w1)p(w2|w1)p(w3|w1,w2)...p(wn|w1,w2,...,wn-1)\n=p(w1)p(w2|w1)p(w3|w1,w2)...p(wn|wn-1,wn-2)\n举例：\nI want english food\np( I want chinese food ) = P( want|I ) × P( chinese|want ) × P( food|chinese )\n假设词典的大小是N，则模型参数的量级是\n四、词向量\n五、神经网络模型\n训练样本:  ，包括前n-1个词分别的向量,假定每个词向量大小m\n投影层:(n-1)*m 首尾拼接起来的大向量\n输出:\n表示上下文为 时，下一个词恰好为词典中第i个词的概率\n归一化:\n神经网络模型的优势\nS1 = ‘’我 今天 去 网咖’’       出现了1000次\nS2 = ‘’我 今天 去 网吧’’      出现了10次\n对于S1和S2两句话其实表达的意思差不多的，但\n对于N-gram模型:  P(S1) >> P(S2)，一般会表述为S1\n而神经网络模型计算的  P(S1) ≈ P(S2)\n对于如下：\n在神经网络中，只要语料库中出现其中一个，其他句子的概率也会相应的增大\n六、Hierarchical Softmax\nHierarchical Softmax有两种模型，CBOW，Skip-gram\n1、CBOW\nCBOW 是 Continuous Bag-of-Words Model 的缩写，是一种根据上下文的词语预测当前词语的出现概率的模型\n2、哈夫曼树\n3、Logistic回归\n4、CBOW模型推导\n输入层是上下文的词语的词向量,在训练CBOW模型,词向量只是个副产品,确切来说,是CBOW模型的一个参数。训练开始的时候,词向量是个随机值,随着训练的进行不断被更新)。\n投影层对其求和,所谓求和,就是简单的向量加法。\n输出层输出最可能的w。由于语料库中词汇量是固定的|C|个,所以上述过程其实可以看做一个多分类问题。给定特征,从|C|个分类中挑一个。\n5、Skip-gram模型\n1）输入层不再是多个词向量，而是一个词向量\n2）投影层其实什么事都没干，直接将输入层的词向量传递给输出层\n七、负采样模型（Negative Sampling）\n负样本那么多该如何选择呢？\n对于一个给定的正样本（Context(w), w)，我们希望最大化\n一般大多采用负采样模型来求解，因为Hierarchical softmax模型太过于复杂。"}
{"content2":"方法一\n《用Python 进行自然语言处理》（东南大学出版社）\ncs181.1伯克利人工智能课程（edX）\n斯坦福机器学习课程（Coursera）\ncs229 斯坦福机器学习讲义\n一个文本分类的project\n《统计学习方法》（李航，清华大学出版社）\nPattern Recognition And Machine Learning (PRML)\ncs224d 斯坦福自然语言处理课（YouTube 视频）\n方法二：\nGoogle在Udacity推出免费的深度学习课程 https://www.udacity.com/course/deep-learning--ud730\nCoursera上Andrew Ng的机器学习课程 https://zh.coursera.org/learn/machine-learning\n廖君同学整理的机器学习资料：https://github.com/ty4z2008/Qix/blob/master/dl.md\nMIT出版社出版发行《深度学习》，三位来自深度学习领域最顶级的科学家（谷歌大脑团队）：Ian Goodfellow、Yoshua Bengio和Aaron Courville撰写。"}
{"content2":"自然语言处理在很多APP中都有实际应用的场景，比如在电商软件中，客服问答系统、评论情感分析、带有语义识别的搜索、商品自动分类、用户画像等等。那么本篇作为自然语言处理浅学的第一篇，就着重来讲一下背景知识。\n背景知识\n自然语言处理，英文是natural language process, NLP，说白了就是利用计算机去对文本进行分析的加工。\n由于自然语言处理是一门交叉的学科，因此学习它的话，需要了解很多其他学科的知识。比如：\n概率论：需要了解概率、条件概率、贝叶斯法则；二项分布、期望、方差；最大似然估计、梯度下降等等\n统计学：建模、数据稀疏问题、回退方法等\n机器学习：分类、感知器、支持向量机\n语言学：构词、词类、句法、语义；语料库和知识库等等\n因此可以看到，想要了解自然语言处理，还是需要浓厚的背景基础的。\n应用场景\n机器翻译\n这个是自然语言处理最为人知的场景，也是现在没有什么明确商业化的场景。一般都是拿这种机器翻译来作为某个应用的组成部分，比入跨语言的搜索引流等等。\n国内外对于机器翻译都有比较成熟的产品，比如百度翻译、有道翻译、Google 翻译等等。\n机器翻译由于涉及到语义分析、上下文环境等面临很多挑战。\n体验频道：\n百度在线翻译：http://fanyi.baidu.com/?aldtype=16047#auto/zh\n有道在线翻译：http://fanyi.youdao.com/\n情感分析\n情感分析在一些评论机制的app中比较有用，比如某酒店网站，下面会有居住过的客人的评价，如果评论有几千条，满满的都是脏乱差，那谁还想住呢！\n所以可以通过情感分析，分析用户评论是积极的还是消极的，根据一定的排序规则和显示比例，在评论区显示。\n同样这个场景也适用于电商网站的商品评价。\n智能问答\n问答系统在一些电商网站也很有实际价值，比如充当客服角色。有很多基本的问题，其实并不需要真的联系人工客服来解决。通过这种智能的问答系统，就可以排除掉大量的用户问题，比如商品的质量投诉啊、商品的基本信息查询啊之类的。\n这样可以省去大量的人工成本。\n体验频道：\n图灵机器人：http://www.tuling123.com/experience/exp_virtual_robot.jhtml?nav=exp\n京东客服jimi：http://jimi1.jd.com/\n信息提取\n在很多搜索引擎的公司，都会采集各种数据，然后进行信息的提取分析。比如新闻的自动分类，就需要针对文本提取关键信息，然后应用一些tf-idf的算法，进行主要的主题分析，从而进行自动的分类。\n语音输入\n说到这个就不得不提科大讯飞了，前一阵新闻推广了讯飞的翻译器——他能根据的中文同声传译翻译成英文，也就是说习大大以后再也不需要带着一名翻译官出访其他国家了（话说，这跟我们有什么关系），以后可以去国外爽歪歪的旅游了（再也不用点餐的时候this this this ok了，咱们直接点宫保鸡丁、麻婆豆腐）。\n体验频道：\n翻译器视频：http://www.shidi.org/sf_DEB72250E26D4F96A9CB2857439BE6ED_277_xhat.html\n舆论分析\n说到这个舆论分析，最著名的就是奥巴马的总统竞选了。比如通过数据分析筹集资金，改变广告的投放策略，制作有效的拉票推荐等等。\n参考：http://tech.qq.com/a/20121108/000182.htm\n这个如果是那些明星的公关工作室，应该会很有用吧。比如最近《那年花开月正圆》挺火，孙俪成了舆论焦点，这个时候相关的明星发一些贴边的新闻，就能炒作一下！\n语言生成（新闻、篮球解说、文本摘要）\n再语言生成方面目前也有很多的应用场景，比如体育类节目的ai解说：\nhttp://www.techweb.com.cn/it/2016-08-17/2376291_4.shtml\n由于这个在电商领域应用的方向比较少，因此就不太关注了。\n知识图谱（略）\n知识图谱不知道啥时候突然就火了，也许是因为它能够描述复杂的关联关系。看到知识图谱的兴起，我还是有一些兴奋的，因为再读研期间搞的就是偏复杂网络的方向。复杂网络是一种复杂的图结构，图的点与线都有复杂的描述关系。在知识图谱中也是如此，不过知识图谱是面向业务领域的，比如百度描述明星关系的案例。\nhttp://tupu.baidu.com/xiaoyuan/\n有了这种知识图谱的基础，随便搜索黄晓明的老婆是谁？ 就很容易回答出来了。\n那么再电商领域，也许可以根据商品的关系或者产地、供应商等建立知识图谱，然后进行捆绑销售、促销活动、精准营销等活动，带动商品的曝光和销量。\n推荐书籍\n《数学之美》\n《自然语言处理综论》\n《统计自然语言处理》\n《统计学习方法》\n《机器学习实战》\n《集体智慧编程》\n参考\n1 百度在NLP领域都做了什么？https://www.leiphone.com/news/201702/LDdGVnuiyP9HiPXa.html\n2 机器翻译原理：https://www.zhihu.com/question/24588198"}
{"content2":"一、java开发\n（1） 应用开发，即Java SE开发，不属于java的优势所在，所以市场占有率很低，前途也不被看好。\n（2） web开发，即Java Web开发，主要是基于自有或第三方成熟框架的系统开发，如ssh、springMvc、springside、nutz、，面向各自不同的领域，像OA、金融、教育等有非常成熟案例，这是目前最大的市场所在，故人称“java为web而生”。但目前看它的缺点入门不高，所以待遇相对中等，上升空间很有限且缓慢。\n（3）移动开发(Android)，是目前的大趋势，但移动端往往只能充当客户端的角色，其技术的难度与复杂度要相对弱很多，因为一时的火热其技术市价被高抬，但长久看来其发展空间有限，但比pc web端开发要增快一些。\n二、网络爬虫\n也叫spider,始于也发展于百度、谷歌。但随者近几年大数据的兴起，爬虫应用被提升到前所未有的高度。就大数据而言，其实自有数据或用户产生数据平台很有限，只有像电商、微博类这样的平台才能免强自给自足，像很多数据分析挖掘公司多以网络爬虫的方式得到不同来元的数据集合，最后为其所用，构建属于自己的大数据综合平台。其中，像舆情类、金融股票分析类、广告数据挖掘类等属于此种。 下面技术层面描述之。\n（1） 传统爬虫，像nutch、hetriex之类的，以爬取简单页面为好，即没有复杂请求的页面。但随着web2.0的兴起，越来越多的网站采用很多动态交互技术如ajax之类的来提升用户体验、需用户登陆才可访问的页面等，它们就无能为力了，或者说需要二次开发的开发成本太高，很多人放弃用它们。\n（2） 定制爬虫，针对一些大数据平台，如微博、电商、点评网之类的，页面交互复杂、用户登陆后方可访问，往往是需要自定义定制开发一些爬虫项目，如专门针对微博的微博爬虫，针对大众点评网的定制爬虫，针对豆辩书评的评论爬虫，都属于典型的定制爬虫，其难度要大于传统爬虫，需要相应的定制分析工具与能力，并且要具备很扎实的程序设计功底，优化效率，克服验证码、拒绝服务等反爬措施，方可做出高效的该类爬虫。现在主流依然是基于httpclient+jsoup来搞定网络下载与页面解析。\n（3） 新型爬虫，结合一些成熟的第三方工具，如c/c++实现的webkit、htmlunit、phantomjs、casper等工具，其共同点即最大限度的去模拟人为操作浏览器的方式去解决用（1）、（2）所不易解决的问题，如模拟登陆、复杂参数的获取、复杂页面交互等问题。往往采用如上的工具可以轻松搞定这些问题，其最大的缺点是由于基于真实浏览器的操作，故效率比较低，所以往往需要和httpclient相结合，才能达到高效实用的目的。基于phantomjs做的百度元搜索抓取也证明了这一点，下一步可以结合它去完成微博类爬虫的模拟登陆获取cookies部分，之后采用httpclient+jsoup解决海量数据的抓取，是非常好的微博爬虫解决方案。\n因为其需要的知识面相对要较多，故其待遇要高于web开发，且上升幅度与速度都远高于web开发。\n三、自然语言处理\n即NLP,nature language process的简称，它也是很多其它名称的简称，很多人在此有误解。其主要包括典型的三部分，分词、词性标注、句法分析。\n（1）分词：主流的包括开源的ansj分词、ICTCLAS、哈工大的ltp、海量分词、fudan分词等，在借鉴ansj分词的基础上我也重构并开发了天亮分词，并已加入ansj主持的中国自然语言处理开源组织nlpchina的https://github.com/NLPchina/中。\n（2）词性标注：以前较主流的标注体系是ICTCLAS和北大标注体系，现在有出现了如哈工大ltp平台、大连理工自然语方处理实验室的标注体系，大同小异。\n（3）句法分析：这块相对前两者较难一些，目前国内我知道的是哈工大的ltp做的中文句法分析不错，像stanford的parser对英文句法还可以接受，但对中文句法分析就比较多。\n因为这一领域相对比较专，难度和工作量也较大，但由于开源分词比较多且实用性不错，所以专门搞这块的人员往往都在大公司或比较牛的个人，当然待遇比上述一二也要高。\n四、数据挖掘\n即datamining，这是现在的大趋势，它往往是基于nlp为基础的，再结合一些典型的数据挖掘算法，像分类、聚类、神经网络相关等算法，从而达到数据挖掘应用开发与产品的目\n（1）自行研发相关挖掘算法：也就是在一定数学和计算机基础之上，做一些自主研发相关算法与调优，难度比较大，往往是一些牛人或算法研发工程师去搞。\n（2） 引用第三方开源组件，像weka、mahout、libSvm等都提供了很多封装好的各种不同数据挖掘算法的组件，供上层开发人员直接调用，只要学习好其API，并按说明input、output就可以。\n五、四者之间的关系\njavaweb开发方面可以说是一个门户，可以让用户更好、更直接的了解后台的东西。\n网络爬虫，是大数据获取的途径，为nlp、datamining做准备。\nnlp,是衔接网络spider的数据和datamining的中间件。\ndatamining,是终极目标，也是得以变现的核心所在。\n这四者是一个顺序承接的关系，若能四者皆具，则谓之大才。\n写的比较随意，不当之处欢迎交流。"}
{"content2":"人工智能深度学习Caffe框架介绍，优秀的深度学习架构\n人工智能深度学习Caffe框架介绍，优秀的深度学习架构\n在深度学习领域，Caffe框架是人们无法绕过的一座山。这不仅是因为它无论在结构、性能上，还是在代码质量上，都称得上一款十分出色的开源框架。更重要的是，它将深度学习的每一个细节都原原本本地展现出来，大大降低了人们学习研究和开发的难度。\n一、从Caffe的开发中了解到的用户需求：\n深度学习的框架总会不断改变，Caffe也会有被新框架代替的一天。但是在开发Caffe的过程中，贾扬清发现大家喜欢的框架其实有着很多相似的地方，这些闪光点拥有很长的生命周期，发现并掌握人们这些共同偏好将为以后开发新的框架积累经验。\nCaffe之所以广受欢迎可能是因为有以下的四个特点：\n1、 稳定的模型架构\nCaffe通过Protobuf来定义一个网络的结构，而这个由Google开源的库具有优秀的版本兼容性。随着Caffe的框架源码的不断更新迭代，之前定义的网络结构依然能兼容解析，模型仍然能正确加载运行。\n2、较好的设备抽象\n合理的设备抽象能够精简代码，提高框架适用性。在这方面Caffe做了比较好的尝试，模型的训练和使用与不同的平台耦合比较低，只要平台能解析网络结构并读取二进制的模型参数，就能运行该模型。这样大大拓展了框架的应用范围，自然更加符合用户的使用需求。\n3、清晰的说明教程\n如何让首次接触到框架的人通过说明教程就能最快地熟悉运用，这对于一个新面世的框架来说尤为重要。以Caffe为例，用户只需要将官方文档的例子跑一遍，基本就能清楚Caffe的操作过程和细节，这给它的广泛传播提供了最坚实的基础。\n4、开放的模型仓库\n目前Caffe还维护了一个Model Zoo， 许多论文的作者会将模型发布到这里，其它用户可以利用这些材料轻松地将模型复现，还可以在github上参与开发讨论，从而更深入地学习实践。\n二、现有深度学习框架的侧重点：\n1、人们的需求多种多样，目前，还没有任何一种深度学习的框架能够满足人们所有的需求。对于工业界而言，从业者看重的是框架的稳定性强、数据量大、速度快、容易进行数据整合等。\n2、对于学术界来说，学者们更希望框架容易调试、灵活性要强、迭代要快。因此，比照现有深度学习框架的特点，Theano、Torch可能会更加适合学术界，而D4J等可能就要更适合工业界一些，至于Caffe、Tensorflow等为代表的框架则是介于二者之间。\n三、对未来机器学习框架设计的一些思考：\n1、使用计算图 (computation graph)\nCaffe框架在实现网络的forward, backward, update时，是通过Solver, Net, Layer之间递进地逐步回调对应的forward, backward, update来实现的，在加入并行化之后，为了用计算来覆盖数据传输的时间，这些回调的运用会变得相对复杂。\n因此，目前许多框架都在往computation graph的风格上偏移。这种基于computation graph的深度学习框架不仅在网络的更新方面更加直接，而且在并行方面，无论是数据并行方式还是模型并行方式，都能做到接近线性的提速。大家未来也可以在这个方向做些尝试。\n2、高效方便地输入数据\n对于Caffe用户而言，首要问题便是如何导入数据。尤其是在算法比较简单时，保证数据输入的高效性将成为制约模型的首要因素。之前进行的某个项目里，在8个GPU(Titan X)上训练AlexNet，需要达到每秒钟处理1600张图片（3.14GB/s）的要求。甚至对于另外一些模型而言，还需要更多的吞吐量。如果数据接口没有做好，是绝对无法达到这样的要求的。\n3、更快的速度\n网络结构实现需要在灵活性和速度上进行权衡，这种权衡可以体现在框架设计的粗细粒度上。例如一个Inception的结构，是做成像Caffe这样通过各个层的累积来形成，还是直接由Conv2D, BiasAdd, Relu这样的基本计算来直接构造一个Inception结构。这样不同粗细粒度的构建方法体现了整个框架对速度或是灵活性的权衡。\n另外，对于底层的实现，最好用硬件供应商提供的数值计算库，比如CuDNN, MKL-DNN,Accelerate,Eigen,NNPack等。\n4、 可移植性\n要提升框架的实用价值，就必须提升其训练出的模型的可移植性。换句话说，也就是要让框架训练出的模型具有平台无关性，包括了系统层面（windows、linux、android、iOS、OS X等）及硬件层面（CPU、GPU、FPGA、ARM等）。这就必须使得设计出的模型更加轻量。\n-----------------------------------------\n人工智能范畴及深度学习主流框架\n工业机器人，家用机器人这些只是人工智能的一个细分应用而已。\n图像识别，语音识别，推荐算法，NLP自然语言，广告算法，预测算法，数据挖掘，无人驾驶、医疗咨询机器人、聊天机器人，这些都属于人工智能的范畴。\n人工智能现在用到的基础算法是深度学习里面的神经网络算法，具体应用场景有不同的专业算法\n实际上很多细分领域的，差别还是很多的\n机器人的对运动控制算法，图像识别算法要求比较高\n像alphaGo，推荐算法，语音识别这些就主要靠深度学习算法和大数据训练了\n深度学习的开源框架现在主流的有：caffeonspark（微软）, tensorflow（google），Theano，dl4j, Torch，Keras\ncaffeonspark用在视觉图片识别上比较好，dl4j用在NLP上做类似问答搜索的比较多，tensorflow用在学习新的算法上比较好\n可以看本人另外一篇博客：\n人工智能范畴及深度学习主流框架，IBM Watson认知计算领域IntelligentBehavior介绍 - 流风，飘然的风 - 博客园\nhttp://www.cnblogs.com/zdz8207/p/DeepLearning-framework-ibm-watson.html\n流风，飘然的风，本人博客网站：http://www.ssqhm.com\n分类: 人工智能"}
{"content2":"最近把一些在网上见到的自然语言处理的资源整理了一下，包括论文列表、软件资源和一些实验室主页、个人主页等，希望能对NLP研究者有所帮助，由于个人视野有限，目前只整理了这些，以后会持续更新。在此也感谢这些资源的提供者和维护者。\n转载请标明出处（http://blog.csdn.net/xuh5156/article/details/7437475）\n论文、博客\n1.       Google在研究博客中总结了他们2011年的精彩论文《Excellent Papers for 2011》，包括社会网络、机器学习、人机交互、信息检索、自然语言处理、多媒体、系统等各个领域，很精彩的论文集锦。http://googleresearch.blogspot.com/2012/03/excellent-papers-for-2011.html\n或者zibuyu的BLOG http://blog.sina.com.cn/s/blog_574a437f0100y6zy.html\n2.       Best paper awards for AAAI,ACL, CHI, CIKM, FOCS, ICML, IJCAI, KDD, OSDI, SIGIR, SIGMOD, SOSP, STOC, UIST,VLDB, WWWhttp://jeffhuang.com/best_paper_awards.html\n3.       IBM R&D Journal 刚发布了关于Watson的专刊《This is Watson》。总共17篇论文。http://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=6177717&punumber=5288520\n4.       Web Data Mining作者刘兵维护的一个专题资源：Opinion Mining,Sentiment Analysis, and Opinion Spam Detection 。http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html\n5.       Statistical Machine Translationhttp://www.statmt.org/\nStatistical Machine TranslationTutorial Readinghttp://cseweb.ucsd.edu/~dkauchak/mt-tutorial/\nPhilipp Koehn主页http://homepages.inf.ed.ac.uk/pkoehn/\n6.       Profile Hidden Markov ModelResourceshttp://webdocs.cs.ualberta.ca/~colinc/cmput606/\nHidden Markov Model (HMM) Toolbox forMatlabhttp://www.cs.ubc.ca/~murphyk/Software/HMM/hmm.html\n7.       CRF http://www.inference.phy.cam.ac.uk/hmw26/crf/\nConditional Random Field (CRF)Toolbox for Matlabhttp://www.cs.ubc.ca/~murphyk/Software/CRF/crf.html\nFlexCRFs: Flexible Conditional RandomFieldshttp://flexcrfs.sourceforge.net/\n8.       Transfer Learning 包含papers、talks、software等http://www.cse.ust.hk/TL/index.html\n9.       Topic Model，Topic Modeling Bibliographyhttp://www.cs.princeton.edu/~mimno/topics.html\nDavid M. Blei的主页http://www.cs.princeton.edu/~blei/publications.htmlMatlab Topic Modeling Toolbox 1.4http://psiexp.ss.uci.edu/research/programs_data/toolbox.htm\nLDA GIBBS Java源码http://arbylon.net/resources.html\nGibbsLDA++: A C/C++ Implementation ofLatent Dirichlet Allocationhttp://gibbslda.sourceforge.net/\n10.   科学网—推荐系统的循序进阶读物（从入门到精通） - 张子柯的博文http://blog.sciencenet.cn/home.php?mod=space&uid=210641&do=blog&id=508634\n11.   SVM入门http://www.blogjava.net/zhenandaci/archive/2009/02/13/254519.html\n12.   斯坦福大学自然语言处理实验室整理的NLP资源http://www-nlp.stanford.edu/links/statnlp.html\n13.   Stanford University InformationRetrieval Resourceshttp://nlp.stanford.edu/IR-book/information-retrieval.html\n14.   Software Tools for NLP http://www-a2k.is.tokushima-u.ac.jp/member/kita/NLP/nlp_tools.html\n实验室主页\n1.       The Stanford NLP Group http://nlp.stanford.edu\n2.       The Berkeley Natural LanguageProcessing Grouphttp://nlp.cs.berkeley.edu\n3.       University of Tokyo TsujiiLaboratory http://www.nactem.ac.uk/tsujii/publications.cgi?lang=en\n4.       Korea University NLP http://nlp.korea.ac.kr/ http://nlp.korea.ac.kr/new/\n5.       中国科学院计算技术研究所自然语言处理研究组http://nlp.ict.ac.cn/new/\n6.       清华大学自然语言处理组 http://nlp.csai.tsinghua.edu.cn/site2/\n7.       HIT-SCIR http://ir.hit.edu.cn/\n8.       苏州大学自然语言处理实验室http://nlp.suda.edu.cn/\n个人主页\n1.       David M. Blei， (Princeton) LDA，http://www.cs.princeton.edu/~blei/publications.html\n2.       Noah Smith, (CMU),以自然语言处理、机器学习为基础研究computationalsocial science。http://www.cs.cmu.edu/~nasmith/\n3.       Philipp Koehn (University ofEdinburgh)http://homepages.inf.ed.ac.uk/pkoehn/\n4.       Dekang Lin (University ofAlberta)http://webdocs.cs.ualberta.ca/~lindek/\n5.       Michael Collins(ColumbiaUniversity)http://www.cs.columbia.edu/~mcollins/\n6.       Dekai WU(HKUST) http://www.cs.ust.hk/~dekai/\n7.       Pascale Fung (HKUST) http://www.ee.ust.hk/~pascale/\n8.       Alessandro Moschitti (Universityof Trento)http://disi.unitn.it/moschitti/\n9.       Xiaojin (Jerry) Zhu (Universityof Wisconsin-Madison)http://pages.cs.wisc.edu/~jerryzhu/\n10.   Eugene Charniak (BrownUniversity)http://www.cs.brown.edu/~ec/"}
{"content2":"人工智能是什么？什么是人工智能？人工智能是未来发展的必然趋势吗？以后人工智能技术真的能达到电影里机器人的智能水平吗？如果技术成熟的那一天，人类文明的胜利果实会不会被机器人给夺走？如果真有那么一天，各式各样的机器人将会被衍生出来。\n大家不妨试想一下，在一个只有机器人的国度里，“物竞天择、适者生存”这样的法则在机器人界里还适用吗？ 会有种族信仰吗？（原谅我在这天马行空一下~）\n这些问题的产生和提出都可以归结于人工智能技术的高速发展。\n作为创新型人工智能技术领域的典型代表——机器人，当仁不让的成为创新技术产业里的掌上明珠，也是信息时代转向智能时代的标志和结点。而随着工业4.0的引入以及“互联网+”不断对传统行业的渗透，已对整个人工智能领域起着推波助澜的作用。\n我们知道，机器人从电影银屏中走进现实生活里，一定程度上反映了当前市场的供需关系，另一方面则说明大众在生活质量方面提出了更高的要求，希望能有越来越多的智能工具来帮助自己完成一些事情。比如扫地机器人、陪伴机器人等等。这两点都直接影响着人工智能的发展。\n而我们知道，人工智能机器人一般分为工业机器人和服务机器人。其实很好理解，工业机器人就是工厂中常见的类似机械臂，能够代替工人更高效的完成简单且重复率高的流水线上的工作。\n说到第二类服务机器人时，先给大家科普一下，当前人工智能大致分为三个阶段：弱人工智能阶段、强人工智能阶段、超强人工智能阶段。\n怎么评定机器人处于什么阶段呢，一般都以机器人的“智商度”作为衡量标准。这其实也和“人工智能”领域本身很好的衔接对应了。\n回到文章题目本身，什么是人工智能？“人工智能”是不是可以简单的理解为“人工”与“智能”的完美结合呢？关于这个问题，仁者见仁智者见智。\n现在继续刚才的话题，以目前服务机器人的智商度来看的话，当前人工智能虽然在不断高速前进，但还是处于弱人工智能向强人工智能过渡的阶段。阻碍前行的因素很多，要攻克的技术难点也很多，但这些问题在人工智能领域的专家来看，技术的积累都只是时间问题，对人工智能技术做更进一步剖析的话，其实就是“算法”+“海量数据”。更通俗一点就是：在海量的数据中通过不断优化的算法来组建适合的数据群组。单从这一点就可以看出，数据的大量积累也是需要经过长时间才能做到的，这也是为什么人工智能领域只有大公司、企业以及科研机构才能有时间、有金钱和精力去投入。\n说了那么多关于人工智能机器人的发展，我们来看看当前市面上可以看到的落地化机器人有哪些？，如：法国的人形机器人NAO，软银公司的带有情绪的机器人Pepper等等，大家会发现，这些机器人的硬件都做得足够的好，尤其是机器人NAO，可以说代表了全世界机器人的硬件制造水平。但是在智商度这块，也被人们习惯称之为机器人大脑的这部分其实还是很弱的，由此也就出现了软件和硬件不平衡的现象。大部分的机器人公司都在硬件上狠下功夫，其机器人拥有一个聪明的大脑便显得尤为重要。直到现在，人们已经意识到了这个问题，在机器人软硬件的天平上，已经慢慢从硬件一边倒的状态往趋于平衡的状态靠近。当然，当所有的人都把目光投放在硬件水平的提高上，也有很少一部分的公司专注于机器人大脑的开发，比如图灵机器人团队，他们就致力于人机交互层面的软件技术研发，植入图灵机器人大脑之后的机器人拥有的智商可以和一个七到八岁孩子匹敌。据了解，图灵机器人团队已在人工智能自然语言处理领域长达十数年的研究。\n个人认为，随着人工智能的不断发展和技术积累沉淀，机器人一定会呈井喷式爆发，到时会出现各种各样的机器人，机器人也会变得越来越聪明，而机器人的发展也最会从强人工智能阶段不断向超强人工智能阶段靠近。对于未来，会不会出现机器人屠杀整个人类的场景呢。关于这个，我和大家一样，拭目以待吧。"}
{"content2":"WordNet是面向语义的英语词典，与传统辞典类似，但结构更丰富。nltk中包括英语WordNet，共有155287个单词和117659个同义词。\n1.寻找同义词\n这里以motorcar为例，寻找它的同义词集。\n1 >>> from nltk.corpus import wordnet as wn 2 >>> wn.synsets('motorcar') //找到同义词集 3 [Synset('car.n.01')] 4 >>> wn.synset('car.n.01').lemma_names 5 <bound method Synset.lemma_names of Synset('car.n.01')> 6 >>> wn.synset('car.n.01').lemma_names() //访问同义词集 7 ['car', 'auto', 'automobile', 'machine', 'motorcar'] 8 >>>\n1 >>> wn.synset('car.n.01').definition() //获取该词在该词集的定义 2 'a motor vehicle with four wheels; usually propelled by an internal combustion engine' 3 >>> wn.synset('car.n.01').examples() //获取该词在该词集下的例句 4 ['he needs a car to get to work'] 5 >>> wn.synset('car.n.01').lemmas() 6 [Lemma('car.n.01.car'), Lemma('car.n.01.auto'), Lemma('car.n.01.automobile'), Lemma('car.n.01.machine'), Lemma('car.n.01.motorcar')] 7 >>> wn.lemma('car.n.01.automobile') 8 Lemma('car.n.01.automobile') 9 >>> wn.lemma('car.n.01.automobile').synset() 10 Synset('car.n.01') 11 >>> wn.lemma('car.n.01.automobile').name() 12 'automobile' 13 >>> wn.synsets('car') 14 [Synset('car.n.01'), Synset('car.n.02'), Synset('car.n.03'), Synset('car.n.04'), Synset('cable_car.n.01')] 15 >>> for synset in wn.synsets('car'): 16 ... print (synset.lemma_names()) 17 ... 18 ['car', 'auto', 'automobile', 'machine', 'motorcar'] 19 ['car', 'railcar', 'railway_car', 'railroad_car'] 20 ['car', 'gondola'] 21 ['car', 'elevator_car'] 22 ['cable_car', 'car'] 23 >>> wn.lemmas('car') //访问所有包含词car的词条 24 [Lemma('car.n.01.car'), Lemma('car.n.02.car'), Lemma('car.n.03.car'), Lemma('car.n.04.car'), Lemma('cable_car.n.01.car')] 25 >>>\nView Code\n2.WordNet的层次结构\nWordNet的同义词集相当于抽象的概念，它们并不总是有对应的英语词汇。这些概念在层次结构中相互联系在一起。\n如上图，是WordNet概念的层次片段。每个节点对应一个同义词集，边表示上位词/下位词关系，即上级概念与从属概念的关系。\n1 >>> motorcar=wn.synset('car.n.01') 2 >>> types_of_motorcar=motorcar.hyponyms() 3 >>> types_of_motorcar[26] 4 Synset('stanley_steamer.n.01') 5 >>> sorted( 6 ... [lemma.name() 7 ... for synset in types_of_motorcar 8 ... for lemma in synset.lemmas()]) 9 ['Model_T', 'S.U.V.', 'SUV', 'Stanley_Steamer', 'ambulance', 'beach_waggon', 'beach_wagon', 'bus', 'cab', 'compact', 'compact_car', 'convert 10 ible', 'coupe', 'cruiser', 'electric', 'electric_automobile', 'electric_car', 'estate_car', 'gas_guzzler', 'hack', 'hardtop', 'hatchback', ' 11 heap', 'horseless_carriage', 'hot-rod', 'hot_rod', 'jalopy', 'jeep', 'landrover', 'limo', 'limousine', 'loaner', 'minicar', 'minivan', 'pace 12 _car', 'patrol_car', 'phaeton', 'police_car', 'police_cruiser', 'prowl_car', 'race_car', 'racer', 'racing_car', 'roadster', 'runabout', 'sal 13 oon', 'secondhand_car', 'sedan', 'sport_car', 'sport_utility', 'sport_utility_vehicle', 'sports_car', 'squad_car', 'station_waggon', 'statio 14 n_wagon', 'stock_car', 'subcompact', 'subcompact_car', 'taxi', 'taxicab', 'tourer', 'touring_car', 'two-seater', 'used-car', 'waggon', 'wago 15 n'] 16 >>> motorcar.hypernyms() 17 [Synset('motor_vehicle.n.01')] 18 >>> paths=motorcar.hypernym_paths() 19 >>> len(paths) 20 2 21 >>> [synset.name for synset in paths[0]] 22 [<bound method Synset.name of Synset('entity.n.01')>, <bound method Synset.name of Synset('physical_entity.n.01')>, <bound method Synset.nam 23 e of Synset('object.n.01')>, <bound method Synset.name of Synset('whole.n.02')>, <bound method Synset.name of Synset('artifact.n.01')>, <bou 24 nd method Synset.name of Synset('instrumentality.n.03')>, <bound method Synset.name of Synset('container.n.01')>, <bound method Synset.name 25 of Synset('wheeled_vehicle.n.01')>, <bound method Synset.name of Synset('self-propelled_vehicle.n.01')>, <bound method Synset.name of Synset 26 ('motor_vehicle.n.01')>, <bound method Synset.name of Synset('car.n.01')>] 27 >>> [synset.name() for synset in paths[0]] 28 ['entity.n.01', 'physical_entity.n.01', 'object.n.01', 'whole.n.02', 'artifact.n.01', 'instrumentality.n.03', 'container.n.01', 'wheeled_veh 29 icle.n.01', 'self-propelled_vehicle.n.01', 'motor_vehicle.n.01', 'car.n.01'] 30 >>> [synset.name() for synset in paths[1]] 31 ['entity.n.01', 'physical_entity.n.01', 'object.n.01', 'whole.n.02', 'artifact.n.01', 'instrumentality.n.03', 'conveyance.n.03', 'vehicle.n. 32 01', 'wheeled_vehicle.n.01', 'self-propelled_vehicle.n.01', 'motor_vehicle.n.01', 'car.n.01'] 33 >>> motorcar.root_hypernyms() 34 [Synset('entity.n.01')] 35 >>>\nView Code\n3.更多的词汇关系\n上位词和下位词被称为词汇关系，因为它们是同义集之间的关系。这两者的关系为上下定位“is-a”层次。WordNet网络另一个重要的定位方式是从条目到它们的部件（部分）或到包含它们的东西（整体）。\n1）部分-整体关系\n1 >>> wn.synset('tree.n.01').part_meronyms() 2 [Synset('burl.n.02'), Synset('crown.n.07'), Synset('limb.n.02'), Synset('stump.n.01'), Synset('trunk.n.01')] 3 >>> wn.synset('tree.n.01').substance_meronyms() 4 [Synset('heartwood.n.01'), Synset('sapwood.n.01')] 5 >>> wn.synset('tree.n.01').member_holonyms() 6 [Synset('forest.n.01')] 7 >>> for synset in wn.synsets('mint', wn.NOUN): 8 ... print(\"%s : %s\" % (synset.name(), synset.definition()) 9 ... 10 ... 11 ... ) 12 ... 13 batch.n.02 : (often followed by `of') a large number or amount or extent 14 mint.n.02 : any north temperate plant of the genus Mentha with aromatic leaves and small mauve flowers 15 mint.n.03 : any member of the mint family of plants 16 mint.n.04 : the leaves of a mint plant used fresh or candied 17 mint.n.05 : a candy that is flavored with a mint oil 18 mint.n.06 : a plant where money is coined by authority of the government 19 >>> wn.synset('mint.n.04').part_holonyms() 20 [Synset('mint.n.02')] 21 >>> wn.synset('mint.n.04').substance_holonyms() 22 [Synset('mint.n.05')]\n2）蕴涵关系\n1 >>> wn.synset('walk.v.01').entailments() 2 [Synset('step.v.01')] 3 >>> wn.synset('eat.v.01').entailments() 4 [Synset('chew.v.01'), Synset('swallow.v.01')] 5 >>> wn.synset('tease.v.03').entailments() 6 [Synset('arouse.v.07'), Synset('disappoint.v.01')]\n3）反义词\n1 >>> wn.lemma('supply.n.02.supply').antonyms() 2 [Lemma('demand.n.02.demand')] 3 >>> wn.lemma('rush.v.01.rush').antonyms() 4 [Lemma('linger.v.04.linger')] 5 >>> wn.lemma('horizontal.a.01.horizontal').antonyms() 6 [Lemma('inclined.a.02.inclined'), Lemma('vertical.a.01.vertical')] 7 >>> wn.lemma('staccato.r.01.staccato').antonyms() 8 [Lemma('legato.r.01.legato')] 9 >>>\n4. 语义相似度\n同义词集是由复杂的词汇关系网络所连接起来的。给定一个同义词集，可以遍历WordNet网络来查找相关含义的同义词集。每个同义词集都有一个或多个上位词路径连接到一个根上位词。连接到同一个根的两个同义词集可能有一些共同的上位词。如果两个同义词集共用一个特定的上位词——在上位词层次结构中处于较底层——它们一定有密切的联系。"}
{"content2":"一、什么是自然语言处理呢？\n自然语言处理是计算机科学家提出的名字，本质上与计算机语言学是同义的，它跨越了计算机学、语言学以及人工智能学科。\n自然语言处理是人工智能的一个分支，在计算机研究领域中，也有其他的分支，例如计算机视觉、机器人技术、知识表达和推理等。\n目标：让计算机能够理解人类语言来完成有意义的任务，例买东西或者是更高级的目标等。\n下图是人对语言层次的传统描述：\n从输入开始，而输入部分通常是语音输入，接着大脑就会进行语音和音义分析。也有部分是文字输入，而文字输入基本上和语言学没多大关系，OCR对文本进行文字识别操作。\n自然语言处理应用的领域：\n1.拼写检查或者是手机上的自动填写功能属于初级的语义理解任务\n2.在线搜索时，联想到的同义词，例如搜索某家公司名字就会出现一大堆的推荐，也是属于语言处理方面。\n3.让计算机能够阅读文字，提取信息，从而充分理解文本，或者也可以处理更高难度的任务，例如判定文档的阅读难度或者是目标受众群体等。\n4.机器翻译\n5.构建口语对话系统\n二、什么是深度学习？\n深度学习是机器学习的一个分支，总的来说，就是让计算机自动学习，而不是人工教授，手工代码告诉它想要做什么，类似于传统的编程。\n深度学习不同于以往年代的机器学习，例如80年代、90年代或者是00年代的机器学习。\n核心区别：对于大多数的机器学习而言，都是围绕着决策树、逻辑回归、朴素贝叶斯、支持向量机等概念。\n本质区别：由人类来审视一个特定的问题，找出解决该类问题的关键要素，然后涉及出与该问题相关的重要特征要素。通常使用python代码来识别这些特征。\n例如下图，显示了一些实体识别系统的特征：、\n机器学习和深度学习的区别？\n上图中可以发现，机器学习在实际应用中，大约90%的工作是人类研究如何描述数据，总结出重要特征，只有约10%的工作是大脑运行这一个数值优化算法。\n深度学习是表征学习的一个分支， 表征学习的理念就是只向电脑提供来自外界的原始信号，无论是视觉还是语言信号，然后电脑自动得出好的中间表征，来很好地去完成任务。从某种意义上来说，就是自己定义特征，和以往人类定义特征类似的方式。\n深度学习的真正含义是：得到了多层的习得表征，可以打败其他的学习方法。\n深度学习主要的两个突破：自然语言处理和计算机视觉。\n三、Deep NLP=Deep Learning + NLP\n一方面深度学习应用到各种不同层次的语言学上，例如词汇学、句法学、语义学。应用于各种不同类型的工具和算法的自然语言处理，例如为单词标注词性、识别人物姓名和结构名字、找出句子的句法结构。此外还被应用在其他的语言应用程序，结合各部分功能，例如机器翻译、情感分析的聊天助手等。\n深度学习模式运用同样一套工具和技术，非常统一的方法来处理各个领域的问题。\n参考资源：斯坦福大学 自然语言处理课程"}
{"content2":"原文地址:http://www.cnblogs.com/cyruszhu/p/5496913.html\n未经允许，请勿用于商业用途！相关请求，请联系作者:yunruizhu@126.com\n转载请附上原文链接，谢谢。\n机器学习/深度学习/自然语言处理学习路线\n1 基础\nl  Andrew NG 的 Machine Learning视频。\n连接：主页，资料。\nl  2.2008年Andrew Ng CS229 机器学习\n当然基本方法没有太大变化，所以课件PDF可下载是优点。\n中文字幕视频@网易公开课，英文版视频@youtube，课件PDF@Stanford\nl  3.Tom Mitchell 的机器学习视频\n他的《机器学习》在很多课程上被选做教材，有中文版。\n2 进阶\nl  3. 林軒田 (HT Lin) 老师的两门课。\n机器学习基石(Machine Learning Foundations)：\nMOOC，all handout slides ，free youtube videos\n机器学习技法(Machine Learning Techniques)：\nMOOC，all handout slides，free youtube videos\nl  4.2013年Yaser Abu-Mostafa (Caltech) Learning from Data\n内容更适合进阶，课程视频,课件PDF@Caltech\nYaser Abu-Mostafa是林軒田 (HT Lin)的老师，林的课内容安排和这个课相似。\nl  5. 2012年余凯(百度)张潼(Rutgers) 机器学习公开课\n内容更适合进阶，课程主页@百度文库，课件PDF@龙星计划\nl  PRML/机器学习导论/矩阵分析(计算)/神经网络与机器学习\n3 方向\n3.1 深度神经网络\nl  大致了解：\nA Deep Learning Tutorial: From Perceptrons to Algorithms\nIntroduction to Deep Learning Algorithms\nDeep learning from the bottom up\nYann LeCun, Yoshua Bengio & Geoffrey Hinton，Deep learning[J],Nature.\nl  UFLDL：Deep Learning Tutorial from Stanford，中文版。\nStanford计算机系的官方tutorial，Andrew Ng执笔。要想了解DL的原理，这个最好用了。\nl  Deep Learning，Ian Goodfellow，Yoshua Bengio，Aaron Courville。目前最权威的DL教材了。\nl  Neural Networks for Machine Learning。\nGeoffrey Hinton，Department of Computer Science，辛顿是反向传播算法和对比散度算法的发明人之一，也是深度学习的积极推动者。有视频和材料。\nl  Oxford Deep Learning\nNando de Freitas 在 Oxford 开设的深度学习课程，有全套视频。\nl  吴立德，复旦大学教授。优酷视频：《深度学习课程》，讲的很有大师风范。\n其他参考：\nl  Neural networks class，Hugo Larochelle from Université de Sherbrooke\nl  Deep Learning Course， CILVR lab @ NYU\n3.2机器视觉\nl  Fei-Fei Li ：CS231n: Convolutional Neural Networks for Visual Recognition。\nhttp://cs231n.stanford.edu/，英文字幕\nCS231n课程笔记翻译：Python Numpy教程 ，@杜客组织知乎的几个牛人翻译的，表示感谢。\nl  William Hoff,  Computer vision, 视频和课件都有，无字幕\nl  CAP 5415 - Computer Vision, 无字幕\n3.3自然语言处理\nl  Richard Socher：CS224d: Deep Learning for Natural Language Processing\nhttp://cs224d.stanford.edu/syllabus.html，video.\nl  Dan Jurafsky和Christopher Manning，在coursera上的NLP课程链接。自然语言处理。\nl  Michael Collins，哥伦比亚大学，Natural Language Processing ，Coursera课程。\nl  High quality video of the 2013 NAACL tutorial version are up here: video\n课程对应的主页。ACL 2012 + NAACL 2013 Tutorial: Deep Learning for NLP (without Magic)，链接。\nl  统计学习方法，李航。很出名，擅长自然语言处理，该本书也是按照自然语言处理来写的。\n3.4杂货\n作者：郭小贤\n链接：https://www.zhihu.com/question/26006703/answer/63572833\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n另外建议看看大神Yoshua Bengio的推荐（左边的链接是论文，右边的是代码），有理论有应用（主要应用于CV和NLP）\nPage on Toronto, Home Page of Geoffrey Hinton\nPage on Toronto, Home Page of Ruslan R Salakhutdinov\nPage on Wustl, ynd/cae.py · GitHub\nPage on Icml, https://github.com/lisa-lab/pyle...\nPage on Jmlr, pylearn2)\nOn the difficulty of training recurrent neural networks, trainingRNNs\nImageNet Classification with Deep Convolutional Neural Networks, cuda-convnet - High-performance C++/CUDA implementation of convolutional neural networks - Google Project Hosting\nLinguistic Regularities in Continuous Space Word Representations, word2vec - Tool for computing continuous distributed representations of words. - Google Project Hosting\n作者：专业主义\n链接：https://www.zhihu.com/question/26006703/answer/90969591\n来源：知乎\n《Deep Learning for Natural Language Processing and Related Applications》\n介绍:这份文档来自微软研究院,精髓很多。如果需要完全理解，需要一定的机器学习基础。不过有些地方会让人眼前一亮,毛塞顿开。\nUnderstanding Convolutions\n介绍:这是一篇介绍图像卷积运算的文章，讲的已经算比较详细的了\n《Deep Learning and Shallow Learning》\n介绍:对比 Deep Learning 和 Shallow Learning 的好文，来着浙大毕业、MIT 读博的 Chiyuan Zhang 的博客。\n《Java Machine Learning》\n介绍：Java机器学习相关平台和开源的机器学习库，按照大数据、NLP、计算机视觉和Deep Learning分类进行了整理。看起来挺全的，Java爱好者值得收藏。\n《机器学习经典论文/survey合集》\n介绍：看题目你已经知道了是什么内容,没错。里面有很多经典的机器学习论文值得仔细与反复的阅读。\n《机器学习经典书籍》\n介绍：总结了机器学习的经典书籍，包括数学基础和算法理论的书籍，可做为入门参考书单。\n《Deep Learning 101》\n介绍:因为近两年来，深度学习在媒体界被炒作很厉害（就像大数据）。其实很多人都还不知道什么是深度学习。这篇文章由浅入深。告诉你深度学究竟是什么！\n《Underactuated Robotics》\n介绍:MIT的Underactuated Robotics于 2014年10月1日开课，该课属于MIT研究生级别的课程，对机器人和非线性动力系统感兴趣的朋友不妨可以挑战一下这门课程！\n作者：肖凯\n链接：https://www.zhihu.com/question/31785984/answer/72180444\n来源：知乎\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\nVideo and Lectures\nHow To Create A Mind By Ray Kurzweil - Is a inspiring talk\nDeep Learning, Self-Taught Learning and Unsupervised Feature Learning By Andrew Ng\nRecent Developments in Deep Learning By Geoff Hinton\nThe Unreasonable Effectiveness of Deep Learning by Yann LeCun\nDeep Learning of Representations by Yoshua bengio\nPrinciples of Hierarchical Temporal Memory by Jeff Hawkins\nMachine Learning Discussion Group - Deep Learning w/ Stanford AI Lab by Adam Coates\nMaking Sense of the World with Deep Learning By Adam Coates\nDemystifying Unsupervised Feature Learning By Adam Coates\nVisual Perception with Deep Learning By Yann LeCun"}
{"content2":"昨天实验室一位刚进组的同学发邮件来问我如何查找学术论文，这让我想起自己刚读研究生时茫然四顾的情形：看着学长们高谈阔论领域动态，却不知如何入门。经过研究生几年的耳濡目染，现在终于能自信地知道去哪儿了解最新科研动态了。我想这可能是初学者们共通的困惑，与其只告诉一个人知道，不如将这些Folk Knowledge写下来，来减少更多人的麻烦吧。当然，这个总结不过是一家之谈，只盼有人能从中获得一点点益处，受个人认知所限，难免挂一漏万，还望大家海涵指正。\n1.      国际学术组织、学术会议与学术论文\n自然语言处理（natural language processing，NLP）在很大程度上与计算语言学（computational linguistics，CL）重合。与其他计算机学科类似，NLP/CL有一个属于自己的最权威的国际专业学会，叫做The Association for Computational Linguistics（ACL，URL：http://aclweb.org/），这个协会主办了NLP/CL领域最权威的国际会议，即ACL年会，ACL学会还会在北美和欧洲召开分年会，分别称为NAACL和EACL。除此之外，ACL学会下设多个特殊兴趣小组（special interest groups，SIGs），聚集了NLP/CL不同子领域的学者，性质类似一个大学校园的兴趣社团。其中比较有名的诸如SIGDAT（Linguistic data and corpus-based approaches to NLP）、SIGNLL（Natural Language Learning）等。这些SIGs也会召开一些国际学术会议，其中比较有名的就是SIGDAT组织的EMNLP（Conference on Empirical Methods on Natural Language Processing）和SIGNLL组织的CoNLL（Conference on Natural Language Learning）。此外还有一个International Committee on Computational Linguistics的老牌NLP/CL学术组织，它每两年组织一个称为International Conference on Computational Linguistics (COLING)的国际会议，也是NLP/CL的重要学术会议。NLP/CL的主要学术论文就分布在这些会议上。\n作为NLP/CL领域的学者最大的幸福在于，ACL学会网站建立了称作ACL Anthology的页面（URL：http://aclweb.org/anthology-new/），支持该领域绝大部分国际学术会议论文的免费下载，甚至包含了其他组织主办的学术会议，例如COLING、IJCNLP等，并支持基于Google的全文检索功能，可谓一站在手，NLP论文我有。由于这个论文集合非常庞大，并且可以开放获取，很多学者也基于它开展研究，提供了更丰富的检索支持，具体入口可以参考ACL Anthology页面上方搜索框右侧的不同检索按钮。\n与大部分计算机学科类似，由于技术发展迅速，NLP/CL领域更重视发表学术会议论文，原因是发表周期短，并可以通过会议进行交流。当然NLP/CL也有自己的旗舰学术期刊，发表过很多经典学术论文，那就是Computational Linguistics（URL：http://www.mitpressjournals.org/loi/coli）。该期刊每期只有几篇文章，平均质量高于会议论文，时间允许的话值得及时追踪。此外，ACL学会为了提高学术影响力，也刚刚创办了Transactions of ACL（TACL，URL：http://www.transacl.org/），值得关注。值得一提的是这两份期刊也都是开放获取的。此外也有一些与NLP/CL有关的期刊，如ACM Transactions on Speech and Language Processing，ACM Transactions on Asian Language Information Processing，Journal of Quantitative Linguistics等等。\n根据Google Scholar Metrics 2013年对NLP/CL学术期刊和会议的评价，ACL、EMNLP、NAACL、COLING、LREC、Computational Linguistics位于前5位，基本反映了本领域学者的关注程度。\nNLP/CL作为交叉学科，其相关领域也值得关注。主要包括以下几个方面：（1）信息检索和数据挖掘领域。相关学术会议主要由美国计算机学会（ACM）主办，包括SIGIR、WWW、WSDM等；（2）人工智能领域。相关学术会议主要包括AAAI和IJCAI等，相关学术期刊主要包括Artificial Intelligence和Journal of AI Research；（3）机器学习领域，相关学术会议主要包括ICML，NIPS，AISTATS，UAI等，相关学术期刊主要包括Journal of Machine Learning Research（JMLR）和Machine Learning（ML）等。例如最近兴起的knowledge graph研究论文，就有相当一部分发表在人工智能和信息检索领域的会议和期刊上。实际上国内计算机学会（CCF）制定了“中国计算机学会推荐国际学术会议和期刊目录”（http://www.ccf.org.cn/sites/ccf/aboutpm.jsp?contentId=2567814757463），通过这个列表，可以迅速了解每个领域的主要期刊与学术会议。\n最后，值得一提的是，美国Hal Daumé III维护了一个natural language processing的博客（http://nlpers.blogspot.com/），经常评论最新学术动态，值得关注。我经常看他关于ACL、NAACL等学术会议的参会感想和对论文的点评，很有启发。另外，ACL学会维护了一个Wiki页面（http://aclweb.org/aclwiki/），包含了大量NLP/CL的相关信息，如著名研究机构、历届会议录用率，等等，都是居家必备之良品，值得深挖。\n2.      国内学术组织、学术会议与学术论文\n与国际上相似，国内也有一个与NLP/CL相关的学会，叫做中国中文信息学会（URL：http://www.cipsc.org.cn/）。通过学会的理事名单（http://www.cipsc.org.cn/lingdao.php）基本可以了解国内从事NLP/CL的主要单位和学者。学会每年组织很多学术会议，例如全国计算语言学学术会议（CCL）、全国青年计算语言学研讨会（YCCL）、全国信息检索学术会议（CCIR）、全国机器翻译研讨会（CWMT），等等，是国内NLP/CL学者进行学术交流的重要平台。尤其值得一提的是，全国青年计算语言学研讨会是专门面向国内NLP/CL研究生的学术会议，从组织到审稿都由该领域研究生担任，非常有特色，也是NLP/CL同学们学术交流、快速成长的好去处。值得一提的是，2010年在北京召开的COLING以及2015年即将在北京召开的ACL，学会都是主要承办者，这也一定程度上反映了学会在国内NLP/CL领域的重要地位。此外，计算机学会中文信息技术专委会组织的自然语言处理与中文计算会议（NLP&CC）也是最近崛起的重要学术会议。中文信息学会主编了一份历史悠久的《中文信息学报》，是国内该领域的重要学术期刊，发表过很多篇重量级论文。此外，国内著名的《计算机学报》、《软件学报》等期刊上也经常有NLP/CL论文发表，值得关注。\n过去几年，在水木社区BBS上开设的AI、NLP版面曾经是国内NLP/CL领域在线交流讨论的重要平台。这几年随着社会媒体的发展，越来越多学者转战新浪微博，有浓厚的交流氛围。如何找到这些学者呢，一个简单的方法就是在新浪微博搜索的“找人”功能中检索“自然语言处理”、 “计算语言学”、“信息检索”、“机器学习”等字样，马上就能跟过去只在论文中看到名字的老师同学们近距离交流了。还有一种办法，清华大学梁斌开发的“微博寻人”系统（http://xunren.thuir.org/）可以检索每个领域的有影响力人士，因此也可以用来寻找NLP/CL领域的重要学者。值得一提的是，很多在国外任教的老师和求学的同学也活跃在新浪微博上，例如王威廉（http://weibo.com/u/1657470871）、李沐（http://weibo.com/mli65）等，经常爆料业内新闻，值得关注。还有，国内NLP/CL的著名博客是52nlp（http://www.52nlp.cn/），影响力比较大。总之，学术研究既需要苦练内功，也需要与人交流。所谓言者无意、听者有心，也许其他人的一句话就能点醒你苦思良久的问题。无疑，博客微博等提供了很好的交流平台，当然也注意不要沉迷哦。\n3.      如何快速了解某个领域研究进展\n最后简单说一下快速了解某领域研究进展的经验。你会发现，搜索引擎是查阅文献的重要工具，尤其是谷歌提供的Google Scholar，由于其庞大的索引量，将是我们披荆斩棘的利器。\n当需要了解某个领域，如果能找到一篇该领域的最新研究综述，就省劲多了。最方便的方法还是在Google Scholar中搜索“领域名称 + survey / review / tutorial / 综述”来查找。也有一些出版社专门出版各领域的综述文章，例如NOW Publisher出版的Foundations and Trends系列，Morgan & Claypool Publisher出版的Synthesis Lectures on Human Language Technologies系列等。它们发表了很多热门方向的综述，如文档摘要、情感分析和意见挖掘、学习排序、语言模型等。\n如果方向太新还没有相关综述，一般还可以查找该方向发表的最新论文，阅读它们的“相关工作”章节，顺着列出的参考文献，就基本能够了解相关研究脉络了。当然，还有很多其他办法，例如去videolectures.net上看著名学者在各大学术会议或暑期学校上做的tutorial报告，去直接咨询这个领域的研究者，等等。"}
{"content2":"转自：https://linux.cn/article-8582-1.html\n编译自：https://opensource.com/article/17/5/python-machine-learning-introduction 作者： Michael J. Garbade\n原创：LCTT https://linux.cn/article-8582-1.html 译者： ucasFL\n本文地址：https://linux.cn/article-8582-1.html\n2017-06-07 09:12    收藏: 1\n本文导航\n-提高你的 Python 技能21%\n-安装 Anaconda30%\n-基本的机器学习技能34%\n-学习更多的 Python 库48%\n-探索机器学习66%\n机器学习是你的简历中必需的一门技能。我们简要概括一下使用 Python 来进行机器学习的一些步骤。\n你想知道如何开始机器学习吗？在这篇文章中，我将简要概括一下使用 Python 来开始机器学习的一些步骤。Python 是一门流行的开源程序设计语言，也是在人工智能及其它相关科学领域中最常用的语言之一。机器学习简称 ML，是人工智能的一个分支，它是利用算法从数据中进行学习，然后作出预测。机器学习有助于帮助我们预测我们周围的世界。\n从无人驾驶汽车到股市预测，再到在线学习，机器学习通过预测来进行自我提高的方法几乎被用在了每一个领域。由于机器学习的实际运用，目前它已经成为就业市场上最有需求的技能之一。另外，使用 Python 来开始机器学习很简单，因为有大量的在线资源，以及许多可用的 Python 机器学习库。\n你需要如何开始使用 Python 进行机器学习呢？让我们来总结一下这个过程。\n提高你的 Python 技能\n由于 Python 在工业界和科学界都非常受欢迎，因此你不难找到 Python 的学习资源。如果你是一个从未接触过 Python 的新手，你可以利用在线资源，比如课程、书籍和视频来学习 Python。比如下面列举的一些资源：\nPython 学习之路\nGoogle 开发者 Python 课程（视频）\nGoogle 的 Python 课堂\n安装 Anaconda\n下一步是安装 Anacona。有了 Anaconda ，你将可以开始使用 Python 来探索机器学习的世界了。Anaconda 的默认安装库包含了进行机器学习所需要的工具。\n基本的机器学习技能\n有了一些基本的 Python 编程技能，你就可以开始学习一些基本的机器学习技能了。一个实用的学习方法是学到一定技能便开始进行练习。然而，如果你想深入学习这个领域，那么你需要准备投入更多的学习时间。\n一个获取技能的有效方法是在线课程。吴恩达的 Coursera 机器学习课程 是一个不错的选择。其它有用的在线训练包括：\nPython 机器学习： Scikit-Learn 教程\nPython 实用机器学习教程\n你也可以在 LiveEdu.tv 上观看机器学习视频，从而进一步了解这个领域。\n学习更多的 Python 库\n当你对 Python 和机器学习有一个好的感觉之后，可以开始学习一些开源的 Python 库。科学的 Python 库将会使完成一些简单的机器学习任务变得很简单。然而，选择什么库是完全主观的，并且在业界内许多人有很大的争论。\n一些实用的 Python 库包括：\nScikit-learn ：一个优雅的机器学习算法库，可用于数据挖掘和数据分析任务。\nTensorflow ：一个易于使用的神经网络库。\nTheano ： 一个强大的机器学习库，可以帮助你轻松的评估数学表达式。\nPattern ： 可以帮助你进行自然语言处理、数据挖掘以及更多的工作。\nNilearn ：基于 Scikit-learn，它可以帮助你进行简单快速的统计学习。\n探索机器学习\n对基本的 Python、机器学习技能和 Python 库有了一定理解之后，就可以开始探索机器学习了。接下来，尝试探索一下 Scikit-learn 库。一个不错的教程是 Jake VanderPlas 写的 Scikit-learn 简介。\n然后，进入中级主题，比如 K-均值聚类算法简介、线性回归、决策树和逻辑回归。\n最后，深入高级机器学习主题，比如向量机和复杂数据转换。\n就像学习任何新技能一样，练习得越多，就会学得越好。你可以通过练习不同的算法，使用不同的数据集来更好的理解机器学习，并提高解决问题的整体能力。\n使用 Python 进行机器学习是对你的技能的一个很好的补充，并且有大量免费和低成本的在线资源可以帮助你。你已经掌握机器学习技能了吗？可以在下面留下你的评论，或者提交一篇文章来分享你的故事。\n（题图：opensource.com）\n作者简介：\nMichael J. Garbade 博士是旧金山 LiveEdu Inc（Livecoding.tv）的创始人兼首席执行官。Livecoding.tv 是世界上观看工程师直播编代码最先进的直播平台。你可以通过观看工程师们写网站、移动应用和游戏，来将你的技能提升到一个新的水平。MichaelJ. Garbade 博士拥有金融学博士学位，并且是一名自学成才的工程师，他喜欢 Python、Django、Sencha Touch 和视频流。"}
{"content2":"NLP自然语言处理：\n百度AI的 NLP自然语言处理python语言--pythonSDK文档：\nhttps://ai.baidu.com/docs#/NLP-Python-SDK/top\n第三方模块：pip install baidu-aip\nNLP_test.py\nfrom aip import AipNlp \"\"\" 你的 APPID AK SK \"\"\" APP_ID = '16815394' API_KEY = 'jM4b8GIG9gzrzySTRq3szK2E' SECRET_KEY = 'iE626cEpjT1iAVwh24XV5h1QFuR8FPD2' client = AipNlp(APP_ID, API_KEY, SECRET_KEY) text1 = \"你叫什么名字？\" text2 = \"你是谁？\" \"\"\" 调用短文本相似度 \"\"\" result=client.simnet(text1, text2) print(result) score=result.get('score') print(score)\n通过短文本相识度的比较，得出相似程度score，一般认为score>=0.58即为相似！\n百度AI的NLP自然语言处理python语言--pythonSDK文档：https://ai.baidu.com/docs#/NLP-Python-SDK/top"}
{"content2":"随着科技的发展和网络的普及，人们可获得的数据量越来越多，这些数据多数是以文本形式存在的。而这些文本数据大多是比较繁杂的，这就导致了数据量大但信息却比较匮乏的状况。如何从这些繁杂的文本数据中获得有用的信息越来越受到人们的关注。“在文本文档中发现有意义或有用的模式的过程\"的文本挖掘技术为解决这一问题提供了一个有效的途径。\n知识发现与数据挖掘是人工智能、机器学习和数据库相结合的产物。随着科学数据的大量积累和各种数据库的广泛使用，人们又逐步认识到海量数据的利用十分困难、效率低下，而且很难从中获得有价值的指导性意见。在这种情况下，数据挖掘技术应运而生。\n数据挖掘包括许多步骤：从大规模数据库中(或从其他来源)取得数据;选择合适的特征属性;挑选合适的样本策略;剔除数据中不正常的数据并补足不够的部分;用恰当的降维、变换使数据挖掘过程与数据模型相适合或相匹配;辨别所得到的是否是知识则需将得到的结果信息化或可视化，然后与现有的知识相结合比较。这些步骤是从数据到知识的必由之路。每一步骤都可能是成功的关键或失败的开始。在一般的定义中数据挖掘是知识获取的一部分。\n文本挖掘作为数据挖掘的一个新主题 引起了人们的极大兴趣，同时它也是一个富于争议的研究方向。文本挖掘不但要处理大量的结构化和非结构化的文档数据，而且还要处理其中复杂的语义关系，因此，现有的数据挖掘技术无法直接应用于其上。对于非结构化问题，一条途径是发展全新的数据挖掘算法直接对非结构化数据进行挖掘，对于数据非常复杂，导致这种算法的复杂性很高;另一条途径就是将非结构化问题结构化，利用现有的数据挖掘技术进行挖掘，目前的文本挖掘一般采用该途径进行。对于语义关系，则需要集成计算语言学和自然语言处理等成果进行分析。\n灵玖软件NLPIR大数据语义智能分析平台针对中文数据挖掘的综合需求,融合了网络精准采集、自然语言理解、文本挖掘和语义搜索的研究成果,先后历时十八年,服务了全球四十万家机构用户,是大时代语义智能分析的一大利器。\nNLPIR大数据语义智能分析平台平台针对互联网内容处理的全技术链条的共享开发平台。15年专业研究与工程积累，提供应用软件及各平台下的二次开发包。提供了用于技术二次开发的基础工具集。开发平台由多个中间件组成，各个中间件API可以无缝地融合到客户的各类复杂应用系统之中。\nNLPIR能够全方位多角度满足应用者对大数据文本的处理需求，包括大数据完整的技术链条：网络采集、正文提取、中英文分词、词性标注、实体抽取、词频统计、关键词提取、语义信息抽取、文本分类、情感分析、语义深度扩展、繁简编码转换、自动注音、文本聚类等。\n中文数据挖掘技术应时代的要求应运而生，在很大程度上满足了人们对自然语言处理的需要，解决了人和计算机交流中的一些障碍;但中文数据挖掘技术也存在很多困难，NLPIR大数据语义智能技术将对中文数据挖掘技术进行深入研究，必将提供出高质量、多功能的中文数据挖掘算法并促进自然语言理解系统的广泛应用。"}
{"content2":"1、基本概念\n模拟退火算法（Simulated Annealing，SA）是一种模拟固体降温过程的最优化算法。其模拟的过程是首先将固体加温至某一温度，固体内部的粒子随温度上升慢慢变为无序的状态，内能增大，然后让其慢慢冷却，温度下降时，内部的粒子慢慢趋于有序，达到一种平衡态，最后达到常温时成为基态，此时内能减为最小，算法模拟这样一个过程期望能达到最优化的目的。\n模拟退火算法最早是由kirkpatrick等人应用于组合优化领域，它是基于Monte-Carlo迭代求解策略的一种随机寻优算法。算法从某一较高温度开始，不断下降温度参数，结合概率跳变性在解空间中随机的寻找目标函数的全局最优解，算法的关键点是其能在降温过程中达到局部最优解的情况下以一定的概率跳出局部最优解并最终趋于全局最优。模拟退火算法是一种通用的优化算法，理论上以概率1收敛于全局最优，在工程中有比较广泛的应用，如VLSI、生成调度、控制工程、机器学习、神经网络、信号处理等领域。\n2、算法原理\n模拟退火算法分为三部分：初始解、解空间以及目标函数，分别对应物理退火过程中的初始温度、降温以及最终温度。\n1)初始解：初始解释算法迭代的起点，试验表明，模拟退火算法是健壮的，即最终解的求得最优解并不十分依赖初始解的选取，从而可任意选择一个初始解。当然，如果初始解选择得当可以加快找到全局最优解。\n2)解空间：一般是离散的可行解的集合。\n3)目标函数：对优化目标的量化描述，是解空间到某个数集的一个映射，通常表示为若干优化目标的一个和式，应正确体现问题的整体优化要求且较易计算，当解空间包含不可行解时还应包括罚函数。\n算法从初始解或称为初始状态开始，在解空间中进行启发式搜索（通常是随机搜索的方式），最终搜索到最优的目标值。\n算法的基本过程如下：\n初始化：设定初始温度T，初始解状态S，终止温度T0；\n降温过程：如果T>T0，则循环执行3至6步；\n在解空间中随机搜索一个新解S’;\n计算增量ΔE=E(S′)-E(S)，其中C(S)为解S对应的目标函数值或称为评价函数；\n若ΔE<0则接受S′作为新的当前解，否则以概率exp(-ΔE/T)接受S′作为新的当前解；\n如果满足终止条件则输出当前解作为最优解，结束程序。终止条件有两种情况，一是温度已经达到了最低温度T0；二是在连续的取若干个新解都没有跳出当前最优解\n算法流程图如下：\n3、退火方式\n模拟退火算法中，退火方式对算法有很大影响。如果温度下降过慢，算法的收敛速度会大大降低。如果温度下降过快，可能会丢失极值点。为了提高模拟退火算法的性能，许多学者提出了退火的各种方式，比较有代表性的有以下几种：\n1）\n该方式的特点是温度下降缓慢，算法收敛速度也较慢，但是最终达到全局最优的可能性是最高的\n2）\n式中a为可调参数，可以改善退火曲线的形态。其特点是高温区温度下降比较快，低温区下降比较慢，这种退火方式主要期望在低温区收敛到全局最优。\n3）\n式中a为可调参数，其特点是温度下降很快，算法的收敛速度快，但是带来的损失是可能不能充分的收敛到全局最优\n以上三种退火方式各有优缺点以及适用的场景，需针对具体的应用进行选择。\n4、模拟退火算法在英文分词中的应用\n（1）问题描述\n英文通常不需要分词，因为其本身就是由一个个的英文单词组成，我们考虑一种场景，比如在口语语言处理中，听者必须将连续的语音流分割成单个的词汇，考虑由机器进行分词，这个问题就变得具有挑战性了，考虑下面人为构造的例子，单词边界已被人为去除：\na：doyouseethekitty\nb：seethedoggy\nc：doyoulikethekitty\nd：likethedoggy\n如果是由机器来识别单词边界，则需要找到一种算法来对每个句子进行分词。我们可以给每个字符标注一个布尔值来指示这个字符后面是否有一个分词标志，让我们假设说话人会给语言学习者（机器）一个说话时的停顿，这往往是对应一个延长的暂停，分别对应了abcd四个句子块。现在算法的任务就是对这个句子块进行分词，使得每个句子都能有对应的意思。\n经过处理后，四个句子变为如下的形式：\ntext = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\nseg1= \"0000000000000001000000000010000000000000000100000000000\"\nseg2= \"0100100100100001001001000010100100010010000100010010000\"\n其中seg1是初始解，seg2是全局最优的目标解，算法采用模拟退火算法通过若干轮迭代达到全局最优。\n（2）模拟退火算法引入\n初始解：初始解即为上面提到的seg1串。\n解空间：解空间是离散的，分别对应的是串中每位为1的情况，每个块（a、b、c、d）都有2的n次幂的分词方法，n为块的长度。\n目标函数：目标函数是一个打分函数，该打分函数能够体现分词的一个效果，也就是说分词效果越接近全局最优，分数应越低或越高。\n模拟退火算法自身需要考虑三个与温度相关的量：\n初始温度：初始温度可设为一个比较高的温度，使得算法有足够的时间收敛到全局最优\n降温方式：降温方式采用线性方式，T’=a*T，a是一个小于1的参数，可设为0.99，充分降温，使得搜索解空间更加充分。\n终止温度：终止温度与初始温度对应，当初始温度降到终止温度时，算法终止，因此终止温度的选择也与算法达到全局最优解密切相关\n（3）算法设计\n搜索解空间\n搜索解空间就是是寻找最大化目标函数值的0和1的模式，最好的分词包括像“thekitty”这样的“词”，因为数据中没有足够的证据进一步分割这个词。使用模拟退火算法的非确定性搜索即随机搜索：一开始仅搜索短语分词；随机扰动0和1，它们与“温度”成比例；每次迭代温度都会降低，扰动边界会减少\n目标函数或代价函数\n目标函数是一个打分函数，我们将基于词典的大小和从词典中重构源文本所需的信息量尽力优化它的值。\n给定一个假设的源文本的分词（左），推导出一个词典和推导表，它能让源文本重构，然后合计每个词项（包括边界标志）与推导表的字符数，作为分词质量的得分；得分值越小表明分词越好。\n如上图示的一种分词情况，目标得分包括两个部分，一个分词得分LEXICON，一个是分块得分DERIVATION。LEXICON对分好的每个唯一的词进行算分，分数即为单词长度加上边界1，如doyou的分词得分为单词长度5加上边界1即为6，其他词计算方法类似；分块得分就是买个块包含的单词数量，如第一个1|2|4|6，其得分为4，依次类推，最终得到分词得分为33，分块得分为14，两者相加即为总得目标得分，该目标得分越小则分词效果就越好，也就越接近我们人工识别的目标。\n（3）算法伪代码\n@text:待分词文本\n@segs：初始解，为01串\n@iterations：每个温度的迭代次数，目的是在在每次降温时都能找到一个当前最优的解，\n这个量是当目标达到一个局部最优时，能够使得算法有一定的概率跳出当前局部最优解，是模拟退火算法中以一定概率接受较差解的一种实现方式，随着温度的降低，这种概率会减小，具体实现是flip_n函数，其跳变的位数与温度正相关。\n@a：退火因子\nstring anneal(text, segs, iterations, a)\nT=float(length(segs));\nwhile T > T0\nscore=evaluate(text, segs)\nbest_segs=segs\nfor i=1 to iterations\n//flip_n是随机选择n位位进行跳变，这里n为int(round(T))，其与当前温//度相关，温度下降时，跳变的位数会减小，具体实现原理是保证温度下//降时，解跳变的概率会减小，最后返回跳变后的串\nguess = flip_n(segs, int(round(T)))\n//evalue即是我们上面讨论的目标函数或代价函数\nscore = evalue(text, guess)\nif score < best\nbest = score\nbest_segs=guess\nscore = best\nsegs = best_segs\nT = T*a//降温\n//打印每一步的优化结果\nprint evalue(text, segs), segment(text, segs)\n经过程序实际运行，打印过程为，最左边一列是其对应分数\n60     ['doyouseetheki', 'tty', 'see', 'thedoggy', 'doyouliketh', 'ekittylike', 'thedoggy']\n58     ['doy', 'ouseetheki', 'ttysee', 'thedoggy', 'doy', 'o', 'ulikethekittylike', 'thedoggy']\n56     ['doyou', 'seetheki', 'ttysee', 'thedoggy', 'doyou', 'liketh', 'ekittylike', 'thedoggy']\n54     ['doyou', 'seethekit', 'tysee', 'thedoggy', 'doyou', 'likethekittylike', 'thedoggy']\n53     ['doyou', 'seethekit', 'tysee', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n51     ['doyou', 'seethekittysee', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n42     ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n42分是最优结果，对应全局最优的01串是：\n'0000100100000001001000000010000100010000000100010000000'"}
{"content2":"编者：本文来自华为诺亚方舟实验室资深专家刘晓华在携程技术中心主办的深度学习Meetup中的主题演讲，介绍了华为诺亚面向语音语义的深度学习进展。关注“携程技术中心”微信公号（ctriptech），可获知更多技术分享信息哦。\n本次演讲简要回顾了深度学习近十年进展，重点介绍华为诺亚方舟实验室最近两年内和深度学习相关的研究成果，并探讨了深度学习的未来趋势。\n一、深度学习的近十年进展\n深度学习为什么现在这么火？大数据，算法突破和计算能力。算法上有什么样的突破？第一点，对多层神经网络做预训练。第二点，大量标注数据驱动的监督学习和防过拟合技术的结合，例如drop out。第三点，注意力模型。特别是2012年来，深度学习在语音、图片、视频识别，包括自然语言处理方面取得重大突破等。\n语音识别方面：基于深度神经网络的声音模型替隐马尔可夫框架下的基于混合高斯分布的升学模型，使语音识别取得了突破性进展。目前完全融合了声学模型、语言模型和声学词典的基于深度学习的端到端的语音识别系统也开始出现，并有可能演进为下一代的语音识别系统。\n图像识别：2011年，基于深度卷积神经网络的图像识别系统在ImageNet数据集上取得极大成功，并开始有成功的商业化应用。\n自然语言处理：自2014年始，深度学习在语法分析、机器翻译、对话等领域都取得了一系列重要成果。\n符号人工智能：深度学习开始进入知识表达和推理等传统人工智能的领域，并取得了初步成功。\n控制/强化学习：深度学习和强化学习结合催生了深度强化学习技术，该技术在控制领域，如游戏操作和机器人的控制的端到端学习（end-to-end learning），都有了令人瞩目的成功。\n二、诺亚语音语义方面深度学习相关研究\n华为诺亚方舟实验室已经成为中国在深度自然语言处理研究（deep learning for NLP）领域最好的实验室之一。\n深度语义匹配：对待匹配的两个目标对象的各个方面的匹配关系用深度神经网络进行建模。应用之一是基于自然语言的图片搜索：采用文本和图像深度匹配模型做手机上的图像搜索。\n自然语言对话：我们提出了第一个基于序列编码-解码算法的神经网络的对话模型。它能够理解你所说并产生合适的应答”。目前这一成果被业界广泛引用。\n机器翻译：传统的基于统计的机器翻译，是从大量的平行语料库中学习大量的翻译规则，然后基于翻译规则来做翻译。它的翻译结果会比较忠实原文，但往往比较生硬。其特点可以用信而不达来概括。2014年基于基于编码-解码算法的端-到-端的翻译系统被提出来了。它先会把源语言句子做语义编码，相当于先理解原句，然后根据源端的语义编码再生成目标句子。为了生成更好的目标句子，它会引入一个“注意力模型“，这个注意力模型会建议生成下一个目标翻译词时主要考虑源句端的哪些词。我们最近做的一个有意思的工作是把“覆盖率模型“这个传统统计机器翻译中常用的技术叠加到了”注意力模型“上，使得在生成下一个目标翻译词的时候，把关注重点能更多的放到那些还没有充分翻译的源句端的词上，从而一定程度缓解过译和漏译问题。这一成果已经在今年国际计算语言学协会大会（ACL 2016）上发表了。\n问答系统：我们的问答系统整体基于端到端的编码解码框架，但把知识库也融合进来了，是业界第一个基于深度学习的基于自然语言的问答系统。在生成答案的下一个目标词的时候，它会判断是否把注意力放到知识库，以及知识库中哪个条目，并从中选出答案作为下一个目标词。\n基于自然语言的推理：这一块相关研究我们刚刚起步，我们当前的方法也非常“朴素”。我们目前研究的聚焦在下面的场景：给定若干个事实，以及问题，输出答案。限定答案是某个分类标签，我们把这个任务转化为一个分类问题。\n三、深度学习的未来趋势\n深度学习还能有哪些突破？1.. 自然语言处理，人看起来简单自然的符号，能否与传统人工智能融合。2. 能否将举一反三的能力融入到深度学习中？3，无监督学习是否能有所突破。\n更加灵活的表示方式。代表性的工作包括神经图灵机（Neural Turing Machine）、记忆网络（memory network）, 以及诺亚最近的Neural Transformation Machine和Neural Reasoner。\n更加复杂的端到端学习系统。不再拘泥于一个简单的模型，而是多个不同功能的神经网络耦合而成的系统，这个系统可以和现实世界完成对接和交互,能够接受延迟的和曲折的监督信号（和增强学习的结合），是“可微的”，或者至少是可以被优化的（譬如基于抽样的优化）。\n和传统人工智能的融合。试图去解决传统符号人工智能任务，推理、知识表达等，可以将符号人工智能的强大的逻辑能力和神经网络的灵活性结合。\n和知识库的结合。在对话等需要真实世界知识的场景，我们需要建立一个可以高效动态的访问知识库的神经网络系统。大量需要解决的问题：知识库的表示问题，访问的方式和效率问题，End-to-end 训练的问题，和神经网络内部的“知识”的融合问题。\n受教式人工智能：数据+知识驱动的深度学习。面向特定场景，接受教育，自我成长，个性化的系统。\n（本文由携程技术中心童兰利整理）\n演讲PPT下载：\n诺亚面向语音语义的深度学习研究进展\n深度学习Meetup系列：\n深度学习在携程攻略社区的应用\n深度学习在搜狗无线搜索广告中的应用\n知识库上的问答系统：实体、文本及系统观点\n用户在线广告点击行为预测的深度学习模型\n知识图谱中的推理技术及其在高考机器人中的应用\n诺亚面向语音语义的深度学习研究进展"}
{"content2":"【前言】\n因为修学分的要求，选了人工智能原理课程。这门课在本科阶段就学过，不过当时只学了教材的前9章，忘得也差不多了。我对人工智能一直很感兴趣，决心认真学习这门课程，希望能运用到课题研究中。课程使用的教材是《人工智能：一种现代的方法（第3版）》，Stuart J. Russell, Peter Norvig著。每周一次课，大概上两章的内容。在这个过程中，我会总结每章学习的知识，发布到这个博客上。\n一、人工智能实现的四种途径\n1. Act like human 像人一样行动\n如何判断？图灵测试\n计算机需要的技术：自然语言处理、知识表示、自动推理、机器学习、计算机视觉、机器人学。\n2. Think like human 像人一样思考\n途径：认知建模、认知科学\n3. Think rationally 合理地思考\n逻辑主义流派，实际上难以实现。\n4. Act rationally 合理地行动\n实现：合理Agent\n这是本书所讲的内容，其优势在于：\n（1）正确推理只是实现合理性的几种可能之一，合理行动包括了正确推理的方法；\n（2）比基于人类行为/思维的途径更经得起科学发展的检验，也就是说这种思路能够用科学的方法研究和实现。\n二、理论基础\n1. 哲学：\n亚里士多德：三段论\n头脑：物理系统\n建立知识的来源：经验主义\n2. 数学：\n逻辑、算法、概率\n3. 经济学：\n效用、决策理论（博弈论、运筹学）、满意度\n4. 神经科学：\n神经元、人类的认识还不完全\n5. 心理学：\n行为主义、认知心理学、认知科学\n6. 计算机工程：\n建造高效的计算机\n7. 控制论：\n现代控制论，特别是被称为随机优化控制的分支，其目标是设计能随时最大化目标函数的系统。主要利用数学？\n8. 语言学：\n计算语言学、自然语言处理、知识表示。\n三、历史演变\n弱方法：串联基本的推理步骤来寻找完全解\n知识密集系统：专业知识来自于大量的专用规则\n专家系统\n神经网络\n数据挖掘\n贝叶斯网络\n极大数据集的可用性\n四、最新发展\n机器人汽车、语音识别、自主规划与调度、博弈、垃圾信息过滤、后勤规划、机器人技术、机器翻译"}
{"content2":"NLP（Natural Language Processing ）自然语言处理：是计算机科学，人工智能和语言学的交叉领域。目标是让计算机处理或“理解”自然语言，以执行语言翻译和问题回答等任务。\nNLU  (Natural Language Understanding ) 自然语言理解：将人的语言形式转化为机器可理解的、结构化的、完整的语义表示，通俗来讲就是让计算机能够理解和生成人类语言。\nNLG (Natural Language Generation) 自然语言生成：旨在让机器根据确定的结构化数据、文本、音视频等生成人类可以理解的自然语言形式的文本。\nNLP\n如上图：NLP 由两个主要的技术领域构成：自然语言理解（NLU)和自然语言生成(NLG)。主要包含的技术的技术领域如下图：\nNLU 旨在让机器理解自然语言形式的文本内容。从 NLU 处理的文本单元来讲，可以分为词(term)、句子(sentence)、文档(document)三种不同的类型\nNLG旨在让机器根据确定的结构化数据、文本、音视频等生成人类可以理解的自然语言形式的文本。根据数据源的类型，NLG可以分为三类：\nText to text NLG，主要是对输入的自然语言文本进行进一步的处理和加工；\nData to text NLG，主要是根据输入的结构化数据生成易读易理解的自然语言文本；\nVision to text NLG，主要是给定一张图片或一段视频，生成可以准确描述图片或视频（其实是连续的图片序列）语义信息的自然语言文本，同时 text to vision 的自动生成近几年也有一些有趣的进展。\n语议理解\n自然语言理解的结果，就是要获得一个语义表示（semantic representation），语义表示主要有三种方式：\n分布语义（Distributional semantics）：就是把语义表示成一个向量，它的理论基础来自于Harris的分布假设：语义相似的词出现在相似的语境中。具体的计算方法有多种，比如LSA（Latent Semantic Analysis）、LDA（Latent Dirichlet Allocation）及各种神经网络模型（如LSTM）等\n框架语义（Frame semantics）：把语义用一个frame表示出来\n模型论语义（Model-theoretic semantics）：把自然语言映射成逻辑表达式（logic form）\n目前采用的语义表示是frame semantics表示的一种变形：采用领域（domain）、意图（intent）和属性槽（slots）来表示语义结果。 如下图：\n领域：是指同一类型的数据或者资源，以及围绕这些数据或资源提供的服务，比如“餐厅”，“酒店”，“飞机票”、“火车票”、“电话黄页”等；\n意图：是指对于领域数据的操作，一般以动宾短语来命名，比如飞机票领域中，有“购票”、“退票”等意图；\n属性槽：用来存放领域的属性，比如飞机票领域有“时间”“出发地”“目的地”等\nchatbot 架构示例"}
{"content2":"今年7月29日，在新一代操作系统Win 10公布之后，微软所面临的最大考验是：怎样支撑、管理、执行好随同Win10发放出出去的数亿套智能机器人？大家知道，与操作系统本身不同，智能机器人不能全然独立于“母体”（即微软公司）而独立执行，它们须要随时向server存取数据。\n7月31日，微软市场部门主管迈赫迪(Yusuf Mehdi)对媒体表示，随着用户接收到推送通知。将有很多其它的用户会升级至新系统。当前，超过1,400万台设备已经用上最新的Windows 10操作系统。据悉，微软为了可以让本次推送顺利进行，事先预留了带宽高达40Tbps，眼下使用已超过10Tbps。这一数字还在不断增长。\n近日。微软表示，到今年年底。Win10装机量将达到3亿台。\n也就是说，到时将有数亿个智能机器人在“服役”，操着多种不同的语言。如今，我们要追问：微软放出的机器人是不是“智能机器人”？凭什么说它们是“智能”的？依据是什么？\n回想历史，2007年3月14日，微软投入8亿美元的巨资，兼并了“TellmeNetworks”公司(创立于1999年），从而拥有了自然语言处理的核心技术（NLP），当中包含”自然语言理解“（NLU）。让机器听懂人的”自然语言“（即自然语言理解），也就是说。首先要把自然语言转换成”一阶逻辑“语言。才干让机器处理。\n2009年，微软柯塔娜（Cortana）项目上马，从此微软開始走向人工智能的征途。\n我们说。柯塔娜的核心技术就是自然语言理解（NLU），据此，柯塔娜具备了与人类沟通的能力，有了智慧的”生命“，其它都是表面的”废话“。\n当前，微软所面临的问题是：怎样确保数亿台Win10设备上的智能机器人不出问题，顺利执行，为大众服务。实际上，这批智能机器人须要更快的更新速度，远比操作系统本身更难”侍奉“。我相信，微软要是没有”金刚钻“（云计算数据中心），就不会揽这个”瓷器活“（指人工智能AI）。\n袁萌 8月1日"}
{"content2":"课程简介:\n人工智能（Artificial Intelligence），英文缩写为AI。它是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。\n人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。人工智能从诞生以来，理论和技术日益成熟，应用领域也不断扩大，可以设想，未来人工智能带来的科技产品，将会是人类智慧的“容器”。人工智能可以对人的意识、思维的信息过程的模拟。人工智能不是人的智能，但能像人那样思考、也可能超过人的智能。\n人工智能是一门极富挑战性的科学，从事这项工作的人必须懂得计算机知识，心理学和哲学。人工智能是包括十分广泛的科学，它由不同的领域组成，如机器学习，计算机视觉等等，总的说来，人工智能研究的一个主要目标是使机器能够胜任一些通常需要人类智能才能完成的复杂工作。但不同的时代、不同的人对这种“复杂工作”的理解是不同的。 2017年12月，人工智能入选“2017年%\n下载地址：百度网盘下载"}
{"content2":"前言：\n用Python对自然语言处理有很好的库。它叫NLTK。下面就是对NLTK的第一尝试。\n安装：\n1.安装Pip\n比较简单，得益于CentOS7自带的easy_install。执行一行命令就可以搞定。\n*在终端控制台->easy_install pip\n2.检验Pip是否可用\nPip是Python的包管理工具。我们运行Pip确定CentOS下可用。\n*在终端控制台->pip -V 注意参数大小写\n3.使用Pip安装NLTK\n*在终端控制台->pip install -U nltk\nNLTK使用：\n完成安装后我们可以在Python的解释器里试验一下。当然在命令行形式（command line）下的解析器里编写Python确实有些不爽。下一节会推荐好用的Python IDE （集成开发环境）给大家。稍安勿躁哦。我们还是先来第一个小实验吧。\n＊在终端控制台->Python 进入Python解释器（command line）->print(\"hello python\")\n*继续输入->import nltk->nltk.download()\n*下载我们需要的预料库按l键来浏览列表（回车进行翻页）。我们需要下载的是book标记的预料库作为我们的第一个小实验的数据。\n*下载book语料库数据。按d键然后输入book回车。等待下载，下载完成可以按l键看看都安装了那些数据。后按q键退出。\n按L键，看看那些预料被安装了。回车翻页。\n第一个小实验搜索\n现在可以可以开始第一个小实验了，按照书上的范例我们搜索《白鲸记》中的词monstrous。当然这本书已经包含在我们刚才下载的预料数据里了。\n*导入nltk.book的预料库->from nltk.book import *->text1 便打印出了《白鲸记》\n*找出monstrous这个词很简单只需要使用concordance这个函数就可以了。是不是很简单。\n一共出现在了11处位置，并且显示了出现这个词的上下文。到现在为止我们已经开启了NLP学习的大门。是不是很激动呢。让我们一起努力吧。"}
{"content2":"首先申明本人的英语很搓，看英文非常吃力，只能用这种笨办法来方便下次阅读。有理解错误的地方，请别喷我。\n什么是卷积和什么是卷积神经网络就不讲了，自行google。从在自然语言处理的应用开始(SO, HOW DOES ANY OF THIS APPLY TO NLP?)。\n和图像像素不同的是，在自然语言处理中用矩阵来代表一句话或者一段话作为输入，矩阵的每一行代表一个token，可以是词，也可以是字符。这样每一行是一个向量，这个向量可以是词向量像word2vec或者GloVe。也可以是one-hot向量。如果一句话有10个词，每个词是100维的词向量，那么得到10*100的矩阵，这就相当于图像识别中的图像（input）。\n在图像中，过滤器是在图像的部分滑动，而在NLP中过滤器在整行上滑动。意思是过滤器的宽度和输入矩阵的宽度是一致地。（就是说过滤器的宽度等于词向量的维度。）在高度上常常是开2-5个词的滑动窗口。总结起来，一个在NLP上的CNN长这样：\n这里写图片描述\n这里有3种过滤器，滑动窗口为2、3、4，每种有2个。后面阐述了CNN在NLP上的不足（没看明白）。表示RNN更符合语言的理解习惯。后面又说模型跟实现的理解有偏差，但是CNN在NLP上的表现是不错的。同时也吐槽了词袋模型也一样。（原因鬼知道）\nCNN的另一个优势是快，这里用N-Gram模型做对比。我们都知道在VSM模型中采用3-gram的维度就很恐怖了，文中说google也处理不了超过5-gram的模型。这是CNN模型的优势，同时在CNN的输入层采用n-size的滑动窗口和n-gram处理是相似的。（不能同意再多，个人认为部分功劳在word embeddings上。当然不全是，因为即使采用one-hot，维度也不会随着窗口的size变化。而在n-gram中是随着n的变化爆发性增加的。）\n（干货，对理解模型和代码都非常必要。）\n对于窄卷积来说，是从第一个点开始做卷积，每次窗口滑动固定步幅。比如下图左部分为窄卷积。那么注意到越在边缘的位置被卷积的次数越少。于是有了宽卷积的方法，可以看作在卷积之前在边缘用0补充，常见有两种情况，一个是全补充，入下图右部分，这样输出大于输入的维度。另一种常用的方法是补充一部0值，使得输出和输入的维度一致。这里文中给了一个公式 。这里npadding在全补充里是filter-1，在输入输出相等时，就要主要奇偶性了，注意到卷积核常为奇数，这里应该有原因之一。（思考下为什么）\n这里写图片描述\n这个参数很简单，就是卷积核移动的步长。下面两幅图左边的步长为1，右边的步长为2。（看出卷积核是啥了吗）\n这里写图片描述\n这里说步幅常设置为1，在一些更接近于RNN的模型中会设置更大的stride。\n一般在卷积层后会有汇聚层。最常用的是max-pooling（就是取最大的那个）。stride的大小一般和max-pooling的窗口大小一致。（在NLP中代表性的操作是在整个输出上作汇聚，每个过滤器只输出一个值。） 为啥要做汇聚？讲了两个原因：一是可以提供确定的输出，对于后面做全连接有用。二是可以在保存大部分信息的前提下降维（希望是这样）。这里说这样的做法相当于某个词是否在句子中出现，而不关心这个词在句子的哪个位置出现。这和词袋模型的思想相同。不同的是在局部信息中，“not amazing”和”amazing not“在模型中会有很大的不同。（这里得好好想想，mark下）\n这没啥好说的，就是输入有几层。在图像中一般有1、3层（分别灰度图和RGB图）。在NLP中也可以有多个通道，比如说使用不同词向量化方式，甚至不同的语言等\n这里说CNN在NLP中常应用到文本分类中，比如情感分析、垃圾信息识别、主题分类中。由于卷积的汇聚操作会遗失一些词的位置信息，所以较难应用到词性标注和实体抽取中。但是也不是不可以做，你需要把位置信息加入到特征里。下面是作者看的CNN在NLP方面的论文。\n这里举了论文[1]中的例子，模型很简单。输入层是由word2vec词向量表示的句子，后面跟着是卷基础，然后是max-pooling层，最后是全连接的softmax分类器。同时论文中还实验了使用两个通道，一个静态一个动态，一个会在训练中变化（词向量变化？参数谁不会变化，mark）。在论文[2][6]还有多加入一层来实现“情感聚类”。\n这里写图片描述\n[4]中就没有像word2vec这样还要先训练，直接简单粗暴的使用one-hot向量。[5]的作者表示他的模型在长文本中表现非常好。总结了下，词向量这种在短文本中比长文本表现更好。\n构建CNN模型要做些啥：1、输入的向量化表示。2、卷积核的大小和数量的设置。3、汇聚层类型的选择。4、激活函数的选择。一个好的模型的建立需要多次的实验，这里作者表示如果没能力建立更好的模型，效仿他就足够了。另外有几点经验：1、max-pooling好于average-pooling。2、过滤器的大小很重要。3、正则并没有卵用。4、警告最好文本的长度都差不多。\n剩下的论文就不说了。\n[1] Kim, Y. (2014). Convolutional Neural Networks for Sentence Classification. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014), 1746–1751. [2] Kalchbrenner, N., Grefenstette, E., & Blunsom, P. (2014). A Convolutional Neural Network for Modelling Sentences. Acl, 655–665. [3] Santos, C. N. dos, & Gatti, M. (2014). Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts. In COLING-2014 (pp. 69–78). [4] Johnson, R., & Zhang, T. (2015). Effective Use of Word Order for Text Categorization with Convolutional Neural Networks. To Appear: NAACL-2015, (2011). [5] Johnson, R., & Zhang, T. (2015). Semi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding. [6] Wang, P., Xu, J., Xu, B., Liu, C., Zhang, H., Wang, F., & Hao, H. (2015). Semantic Clustering and Convolutional Neural Network for Short Text Categorization. Proceedings ACL 2015, 352–357. [7] Zhang, Y., & Wallace, B. (2015). A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification, [8] Nguyen, T. H., & Grishman, R. (2015). Relation Extraction: Perspective from Convolutional Neural Networks. Workshop on Vector Modeling for NLP, 39–48. [9] Sun, Y., Lin, L., Tang, D., Yang, N., Ji, Z., & Wang, X. (2015). Modeling Mention , Context and Entity with Neural Networks for Entity Disambiguation, (Ijcai), 1333–1339. [10] Zeng, D., Liu, K., Lai, S., Zhou, G., & Zhao, J. (2014). Relation Classification via Convolutional Deep Neural Network. Coling, (2011), 2335–2344. [11] Gao, J., Pantel, P., Gamon, M., He, X., & Deng, L. (2014). Modeling Interestingness with Deep Neural Networks. [12] Shen, Y., He, X., Gao, J., Deng, L., & Mesnil, G. (2014). A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval. Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management – CIKM ’14, 101–110. [13] Weston, J., & Adams, K. (2014). # T AG S PACE : Semantic Embeddings from Hashtags, 1822–1827. [14] Santos, C., & Zadrozny, B. (2014). Learning Character-level Representations for Part-of-Speech Tagging. Proceedings of the 31st International Conference on Machine Learning, ICML-14(2011), 1818–1826. [15] Zhang, X., Zhao, J., & LeCun, Y. (2015). Character-level Convolutional Networks for Text Classification, 1–9. [16] Zhang, X., & LeCun, Y. (2015). Text Understanding from Scratch. arXiv E-Prints, 3, 011102. [17] Kim, Y., Jernite, Y., Sontag, D., & Rush, A. M. (2015). Character-Aware Neural Language Models."}
{"content2":"Python有一个自然语言处理的工具包，叫做NLTK（Natural Language ToolKit），可以帮助你实现自然语言挖掘，语言建模等等工作。但是没有NLTK，也一样可以实现简单的词类统计。\n假如有一段文字：\na = 'Return a list of the words in the string S, using sep as the delimiter string. If maxsplit is given, at most maxsplit splits are done. If sep is not specified or is None, any whitespace string is a separator and empty strings are removed from the result.'\n单词个数查询：我想查这段文字有多少个单词，那么可以用下面这段代码：\ndef words(text): return text.split()\n-->  words(a)\n['Return', 'a', 'list', 'of', 'the', 'words', 'in', 'the', 'string', 'S,', 'using', 'sep', 'as', 'the', 'delimiter', 'string.', 'If', 'maxsplit', 'is', 'given,', 'at', 'most', 'maxsplit', 'splits', 'are', 'done.', 'If', 'sep', 'is', 'not', 'specified', 'or', 'is', 'None,', 'any', 'whitespace', 'string', 'is', 'a', 'separator', 'and', 'empty', 'strings', 'are', 'removed', 'from', 'the', 'result.']\n这样我就知道这段话有多少个词。\n单词数量查询：然后我又想知道这段话中用来多少个词，相当于对这段话中的词汇做一个dicstinct，可以这么做：\n-->print set(words(a)) set(['and', 'sep', 'is', 'in', 'as', 'at', 'S,', 'done.', 'any', 'given,', 'string.', 'Return', 'whitespace', 'specified', 'empty', 'from', 'string', 'result.', 'most', 'words', 'not', 'using', 'removed', 'a', 'None,', 'splits', 'of', 'maxsplit', 'list', 'strings', 'delimiter', 'separator', 'the', 'If', 'or', 'are'])\n个别单词数量查询：那如果我想知道这段话中包含多少个'string'呢。\n-->c= a.count('string') -->print c 4\n个别单词数所占百分比：想要知道某个单词在单词总数中占到的百分比，那就像下面以下样：\n-->from __future__ import division #引入浮点型除法 -->d = a.count('string') / len(words(a))*100 -->print d 8.33333333333"}
{"content2":"5.7 How to Determine the Category of a Word  如何判断词的分类\nNow that we have examined word classes in detail, we turn to a more basic question: how do we decide what category a word belongs to in the first place? In general, linguists use morphological（形态学的）, syntactic（语法的）, and semantic clues to determine the category of a word.\nMorphological Clues   形态线索\nThe internal structure of a word may give useful clues as to the word’s category. For example, -ness is a suffix that combines with an adjective to produce a noun, e.g., happy → happiness, ill → illness. So if we encounter a word that ends in -ness, this is very likely to be a noun. Similarly, -ment is a suffix that combines with some verbs to produce a noun, e.g., govern → government and establish → establishment.\nEnglish verbs can also be morphologically complex. For instance, the present participle of a verb ends in -ing, and expresses the idea of ongoing, incomplete action (e.g., falling, eating). The -ing suffix also appears on nouns derived from verbs, e.g., the falling of the leaves (this is known as the gerund 动名词).\nSyntactic Clues   语法线索\nAnother source of information is the typical contexts in which a word can occur. For example, assume that we have already determined the category of nouns. Then we might say that a syntactic criterion for an adjective in English is that it can occur immediately before a noun, or immediately following the words be or very. According to these tests, near should be categorized as an adjective:\n(2)\na. the near window\nb. The end is (very) near.\nSemantic Clues 语义线索\nFinally, the meaning of a word is a useful clue as to its lexical category. For example, the best-known definition of a noun is semantic: “the name of a person, place, or thing.” Within modern linguistics, semantic criteria for word classes are treated with suspicion（怀疑）, mainly because they are hard to formalize. Nevertheless, semantic criteria underpin（巩固） many of our intuitions about word classes, and enable us to make a good guess about the categorization of words in languages with which we are unfamiliar. For example, if all we know about the Dutch word verjaardag is that it means the same as the English word birthday, then we can guess that verjaardag is a noun in Dutch. However, some care is needed: although we might translate zij is vandaag jarig as it’s her birthday today, the word jarig is in fact an adjective in Dutch, and has no exact equivalent in English.\nNew Words 新词\nAll languages acquire new lexical items. A list of words recently added to the Oxford Dictionary of English includes cyberslacker, fatoush, blamestorm, SARS, cantopop, bupkis, noughties, muggle, and robata. Notice that all these new words are nouns, and this is reflected in calling nouns an open class（开放类）. By contrast, prepositions are regarded as a closed class（封闭类）. That is, there is a limited set of words belonging to the class (e.g., above, along, at, below, beside, between, during, for, from, in, near, on, outside, over, past, through, towards, under, up, with), and membership of the set only changes very gradually over time.\nMorphology in Part-of-Speech Tagsets 词性标注集合中的词态学\nCommon tagsets often capture some morphosyntactic information, that is, information about the kind of morphological markings that words receive by virtue of（借助） their syntactic role. Consider, for example, the selection of distinct grammatical forms of the word go illustrated in the following sentences:\n(3)  a. Go away!\nb. He sometimes goes to the cafe.\nc. All the cakes have gone.\nd. We went on the excursion（旅行）.\nEach of these forms—go, goes, gone, and went—is morphologically distinct from the others（go的词态上是不同滴）. Consider the form goes. This occurs in a restricted set of grammatical contexts, and requires a third person singular subject. Thus, the following sentences are ungrammatical.\n(4) a. *They sometimes goes to the cafe.\nb. *I sometimes goes to the cafe.\nBy contrast, gone is the past participle form; it is required after have (and cannot be replaced in this context by goes), and cannot occur as the main verb of a clause.\n(5) a. *All the cakes have goes.\nb. *He sometimes gone to the café\nWe can easily imagine a tagset in which the four distinct grammatical forms just discussed were all tagged as VB. Although this would be adequate for some purposes, a more fine-grained tagset provides useful information about these forms that can help other processors that try to detect patterns in tag sequences（细粒度的标记集能偶提供有用的信息）. The Brown tagset captures these distinctions, as summarized in Table 5-7.\nTable 5-7. Some morphosyntactic distinctions in the Brown tagset\nForm            Category          Tag\ngo                  base                VB\ngoes      third singular present VBZ\ngone      past participle          VBN\ngoing            gerund              VBG\nwent            simple past         VBD\nIn addition to this set of verb tags, the various forms of the verb to be have special tags: be/BE, being/BEG, am/BEM, are/BER, is/BEZ, been/BEN, were/BED, and was/BEDZ (plus extra tags for negative forms（否定形式？） of the verb). All told（总共）, this fine-grained tagging of verbs means that an automatic tagger that uses this tagset is effectively carrying out a limited amount of morphological analysis.\nMost part-of-speech tagsets make use of the same basic categories, such as noun, verb, adjective, and preposition. However, tagsets differ both in how finely they divide words into categories, and in how they define their categories. For example, is might be tagged simply as a verb in one tagset, but as a distinct form of the lexeme be in another tagset (as in the Brown Corpus). This variation in tagsets is unavoidable, since part-of-speech tags are used in different ways for different tasks. In other words, there is no one “right way” to assign tags, only more or less useful ways depending on one’s goals."}
{"content2":"简要描述\n各行各业都将悄悄迎来机器人同事，不必过于乐观，也不必过于悲观。还是比尔·盖茨的那句名言：“我们总是高估未来两年内将发生的变化和低估未来十年内将发生的变化。”人工智能最大的威胁不在于机器学习这项技术本身，而在于人们对这项技术的错误使用，以及在没有人类监督的情况下过度相信人工智能可以达成某些目标。\n岗位要求：\n结合数据挖掘传统算法，机器学习浅层算法，深度学习深层算法在人工智能领域的产品中进行分类和预测； 以上算法在图像识别、自然语言处理、舆情分析、语音识别等人工智能方面的应用；\n精通人工智能基础理论和常用算法，对以下至少一个领域有深入研究或者资深的工作经验\na) 深度学习\nb) 自然语言处理\nc) 搜索／推荐算法\nd) 图像识别\ne) 统计机器学习\nf)知识表示及推理、问答系统\nf)熟练使用JAVA/C++/Python等至少一门语言，熟练使用数据结构和常用算法，有较强的算法设计和实现能力\n熟悉Hadoop、Spark、Caffe、Tensorflow等开源社区\n1. 负责电商行业数据建模并利用算法预测趋势；\n2. 负责电商部门商品销量等数据预测算法研究开发；\n3. 负责仓储优化相关运筹规划的算法研究开发；"}
{"content2":"考虑一个语音识别系统，假设用户说了这么一句话：“I have a gun”，因为发音的相似，该语音识别系统发现如下几句话都是可能的候选：1、I have a gun. 2、I have a gull. 3、I have a gub. 那么问题来了，到底哪一个是正确答案呢？\n一般的解决方法是采用统计的方法。即比较上面的1、2和3这三句话哪一句在英语中出现的概率最高，哪句概率最高就把哪句返回给用户。那么如何计算一个句子出现的概率呢？说白了就是“数数”的方法。但是即使是“数数”也有很多种数法，其中，最简单的策略如下：\n给定一个语料库，数出其中所有的长度为4的句子的个数，设为N，然后再看在这N个长度为4的句子中，“I have a gun”出现了多少次，不妨设为N0，那么句子“I have a gun”的概率就是N0/N。其它两个句子的概率也这么计算。\n上述的这种数数方法，从逻辑上讲是完全OK的，但是因为自然语言的灵活多变性，以及语料库的规模总是有限的，对于一个稍长一点的句子，很可能语料库中根本就没有。比如说下面这个句子：“I am looking for a restaurant to eat breakfast”，直观上看，这句话在语料库中应该出现次数很多吧？但是如果把这句话输入到Google的搜索框中，点击搜索，你会发现返回的结果中根本就没有完全匹配上的。所以，我们需要提出更加有效的“数数”方法。\n为了把事情说清楚，需要引入一些简单的数学符号。\n1、word序列：w1, w2, w3, … , wn\n2、链式规则：P(w1, w2, w3, … , wn)=P(w1)P(w2|w1)P(w3|w1w2)P(w4|w1w2w3)…P(wn|w1w2…wn-1)\n好了，我们想要计算“I have a gun”的概率，也就是计算P(I,have,a,gun)，按照链式规则，则有：\nP(I,have,a,gun)=P(I)P(have|I)P(a|I,have)P(gun|I,have,a)\n但是事情并没有得到简化，例如要计算P(gun|I,have,a)，按照条件概率公式展开：\nP(gun|I,have,a) = P(I,have,a,gun)/P(I,have,a)\n发现了什么？为了计算P(gun|I,have,a)，我们需要先计算P(I,have,a,gun)和P(I,have,a)。哎？P(I,have,a,gun)不就是我们一开始想要计算的值吗？所以绕了一圈，我们又回到了原地？\n好了，现在我们来整理一下思路。\n对于一个句子，其可以表示为一个word序列：w1, w2, w3, … , wn。我们现在想要计算句子出现的概率，也就是计算P(w1, w2, w3, … , wn)。这个概率我们可以直接用数数的方法求解，但是效果并不好，所以我们利用链式规则，把计算P(w1, w2, w3, … , wn)转化为计算一系列的乘积：P(w1)P(w2|w1)P(w3|w1w2)P(w4|w1w2w3)…P(wn|w1w2…wn-1)。但是转化之后，问题并没有变得简单。怎么办？\nN-gram这时候就派上用场了。\n对于1-gram，其假设是P(wn|w1w2…wn-1)≈P(wn|wn-1)\n对于2-gram，其假设是P(wn|w1w2…wn-1)≈P(wn|wn-1,wn-2)\n对于3-gram，其假设是P(wn|w1w2…wn-1)≈P(wn|wn-1,wn-2,wn-3)\n依次类推。\n所以：\n在1-gram模型下：\nP(w1, w2, w3, … , wn)=P(w1)P(w2|w1)P(w3|w1w2)P(w4|w1w2w3)…P(wn|w1w2…wn-1)\n≈P(w1)P(w2|w1)P(w3|w2)P(w4|w3)…P(wn|wn-1)\n在2-gram模型下：\nP(w1, w2, w3, … , wn)=P(w1)P(w2|w1)P(w3|w1w2)P(w4|w1w2w3)…P(wn|w1w2…wn-1)\n≈P(w1)P(w2|w1)P(w3|w1w2)P(w4|w2w3)…P(wn|wn-2wn-1)\n在3-gram模型下：\nP(w1, w2, w3, … , wn)=P(w1)P(w2|w1)P(w3|w1w2)P(w4|w1w2w3)…P(wn|w1w2…wn-1)\n≈P(w1)P(w2|w1)P(w3|w1w2)P(w4|w1w2w3)…P(wn|wn-3wn-2wn-1)\n假设我们采用的是1-gram模型，那么：\nP(I,have,a,gun)=P(I)P(have|I)P(a|have)P(gun|a).\n然后，我们再用“数数”的方法求P(I)和其他的三个条件概率：\nP(I)=语料库中I出现的次数 / 语料库中的总词数\nP(have|I) = 语料库中I和have一起出现的次数 / 语料库中I出现的次数。\n总结，本文只是对N-gram做了非常简单的介绍，目的在于简单易懂，但是不够严谨。感兴趣的同学可以进一步查阅相关的资料。在任何一本关于自然语言处理的书上都能够找到N-gram的内容。"}
{"content2":"一、两种分词标准：\n1. 粗粒度。\n将词作为最小基本单位。比如：浙江大学。\n主要用于自然语言处理的各种应用。\n2. 细粒度。\n不仅对词汇继续切分，也对词汇内部的语素进行切分。比如：浙江/大学。\n主要用于搜索引擎。一种常用方案是：\n索引的时候使用细粒度的分词以保证召回，比如浙江/大学\n询的时候使用粗粒度的分词以保证精度\n二、歧义\n1.分类：\n交集型切分歧义。对于AJB，AJ和JB都成词\n组合型切分歧义。对于AB，A、B、AB都成词\n多义组合型切分歧义。对于AB，（1）A、B、AB同时为词；（2）文本中至少存在一个上下文语境c，在c的约束下，A、B在语法和语义上都成立\nPS：语法与语义\n语法：语言符号之间的关系\n语义：语言符号与所指事物之间的关系\n2. 解决方法（分词算法）\n机械分词系统\n基于最大匹配方法MM（The Maximum Matching Method）\n设词典中的最长词条为L，每次先取L个词尝试匹配，若失败，就去掉最后一个字，取前L-1个词尝试匹配，以此类推\n双向匹配法\nMM的改进算法，分为正向最佳匹配法和逆向最佳匹配法\n两个方向得到的结果必然不同\n缺陷：只能正向或逆向得找出最长的词，而不能找出所有的候选词条\n双向扫描法\n以上的改进算法，能更快速的检测出歧义产生的位置\n整体缺点：没有考虑词汇上下文相关性，分词准确度不高\n机械分词系统揭示了一个语言规律：\n一个词汇的出现与其上下文环境中出现的词汇序列存在着紧密的联系\n上下文相关性：\n文本中第n个词的出现与其前后n-m和n+m个词有高度相关性，这个范围[-m,m]称为窗口范围\n计算：Markov假设+最大似然估计，看笔记\n三、未登录词识别（Named Entity Recognition, NER)\n未登录词中，九成是专有名词，其余为通用新词或专业术语。所以未登录词识别就是包括中国人名、译名、日本人名、地理位置名称、组织机构等专有名词的识别。\n在NLP中，通常将上述专有名词和数字、日期等词称为命名实体。\n算法\n基于构词编码的方法\n缺点：只适用于狭窄的专门领域等，在处理大规模不同领域的未登录词上存在很大的障碍\n基于语义的方法\n认为：不同语义类下的未登录词，在统计学规律上具有相似性。\n算法：基于半监督的条件随机场算法（semi-CRF）"}
{"content2":"因为想学python自然语言处理就想在mac上重新配置一下python。\n在网上找了很久才找到两篇有用的教程http://765i.cn/%E5%9C%A8macosx%E4%B8%8A%E9%87%8D%E6%96%B0%E5%AE%89%E8%A3%85python-10-8/\nhttp://woodpecker.org.cn/diveintopython3/installing-python.html\n第一篇文章基本没有问题，但是使用了下发现，还是少了一些命令。\n第一步, 下载最新的Python版本\n下载的地址就在http://python.org， 最新的版本是Python 2.7.3 Mac OS X 64-bit/32-bit x86-64/i386 Installer (for Mac OS X 10.6 and 10.7)，大约是18.8Mb。DMG的格式，下载完成，双击即可安装，不需要任何设置，默认的安装路径是 /Library/Frameworks/Python.framework，不过这和系统原有的路径不一样。因为系统之前是存在Python的，所以安装成功后，在Terminal里运行Python，看到的依旧是2.7.2的版本号。接下来，我们要做的就是把原来的系统版本移除，让新的版本正常工作\n第二步, 把Python挪到正确的位置去\n如第二步删除系统原有的/System/Library/Frameworks/Python.framework/Versions/文件夹下的python\nsudo rm -R /System/Library/Frameworks/Python.framework/Versions/2.7\n使用完该命令以后要再系统文件下重新建立文件夹Version，sudo mkdir Version,反之无法继续下面命令\nsudo mv /Library/Frameworks/Python.framework/Versions/2.7 /System/Library/Frameworks/Python.framework/Versions\n来移动新的python到原来的系统路径下。\n第三步，修改文件所属的Group\n设置Group为wheel，原来系统自带的就是这样的。／／这个很重要自己设置的时候没注意\nsudo chown -R root:wheel /System/Library/Frameworks/Python.framework/Versions/2.7\n但是其实我的电脑做到这一步就可以了，下面两步感觉不是必须的。\n第四步，更新一下Current的Link\n在Versions的目录里有一个Current的link，是指向当前的Python版本，原始是指向系统自带的Python2.7.2，我们把它删除后，link就失效了，所以需要重新链一下这里的Current文件夹同样需要重新设置。\nsudo rm /System/Library/Frameworks/Python.framework/Versions/Current sudo ln -s /System/Library/Frameworks/Python.framework/Versions/2.7 /System/Library/Frameworks/Python.framework/Versions/Current\n第五步，重新链接可执行文件\n1) 先把系统原来的执行文件删掉\nsudo rm /usr/bin/pydoc sudo rm /usr/bin/python sudo rm /usr/bin/pythonw sudo rm /usr/bin/python-config\n2) 建立新的链接\nsudo ln -s /System/Library/Frameworks/Python.framework/Versions/2.7/bin/pydoc /usr/bin/pydoc sudo ln -s /System/Library/Frameworks/Python.framework/Versions/2.7/bin/python /usr/bin/python sudo ln -s /System/Library/Frameworks/Python.framework/Versions/2.7/bin/pythonw /usr/bin/pythonw sudo ln -s /System/Library/Frameworks/Python.framework/Versions/2.7/bin/python-config /usr/bin/python-config\n最后，更新一下.bash_profile文件\ncd ~\nvim .bash_profile (只要能编辑就行)\n插入新的Python路径\n# Setting PATH for Python 2.7 # The orginal version is saved in .bash_profile.pysave PATH=\"/System/Library/Frameworks/Python.framework/Versions/2.7/bin:${PATH}\" export PATH\n之后打命令python -V就可以看到版本是2.7.3"}
{"content2":"一、课程介绍\n斯坦福大学于2012年3月在Coursera启动了在线自然语言处理课程，由NLP领域大牛Dan Jurafsky 和 Chirs Manning教授授课：\nhttps://class.coursera.org/nlp/\n以下是本课程的学习笔记，以课程PPT/PDF为主，其他参考资料为辅，融入个人拓展、注解，抛砖引玉，欢迎大家在“我爱公开课”上一起探讨学习。\n课件汇总下载地址：斯坦福大学自然语言处理公开课课件汇总\n二、自然语言处理概览——什么是自然语言处理（NLP)\n1）相关技术与应用\n自动问答（Question Answering，QA）：它是一套可以理解复杂问题，并以充分的准确度、可信度和速度给出答案的计算系统，以IBM‘s Waston为代表；\n信息抽取（Information Extraction，IE）：其目的是将非结构化或半结构化的自然语言描述文本转化结构化的数据，如自动根据邮件内容生成Calendar；\n情感分析（Sentiment Analysis，SA）：又称倾向性分析和意见挖掘，它是对带有情感色彩的主观性文本进行分析、处理、归纳和推理的过程，如从大量网页文本中分析用户对“数码相机”的“变焦、价格、大小、重量、闪光、易用性”等属性的情感倾向；\n机器翻译（Machine Translation，MT）：将文本从一种语言转成另一种语言，如中英机器翻译。\n... ...\n2）发展现状\n基本解决：词性标注、命名实体识别、Spam识别\n取得长足进展：情感分析、共指消解、词义消歧、句法分析、机器翻译、信息抽取\n挑战：自动问答、复述、文摘、会话机器人\n3）NLP主要难点——歧义问题\n词法分析歧义\n分词，如“严守一把手机关了”，可能的分词结果“严守一/ 把/ 手机/ 关/  了” 和“严守/ 一把手/ 机关/  了”\n词性标注，如“计划”在不同上下文中有不同的词性：“我/ 计划/v 考/ 研/”和“我/ 完成/ 了/ 计划/n”\n语法分析歧义\n“那只狼咬死了猎人的狗”\n”咬死了猎人的狗失踪了”\n语义分析歧义\n机器翻译：句子“At last, a computer that understands you like your mother”可以有多种含义，如下：\n计算机会像你的母亲那样很好的理解你（的语言）\n计算机理解你喜欢你的母亲\n计算机会像很好的理解你的母亲那样理解你\nNLP应用中的歧义\n音字转换：拼音串“ji qi fan yi ji qi ying yong ji qi le ren men ji qi nong hou de xing qu”中的“ji qi”如何转换成正确的词条\n4）为什么自然语言理解如此困难？\n用户生成内容中存在大量口语化、成语、方言等非标准的语言描述\n分词问题\n新词不断产生\n基本常识与上下文知识\n各式各样的实体词\n... ...\n为了解决以上难题，我们需要掌握较多的语言学知识，构建知识库资源，并找到一种融合各种知识、资源的方法，目前使用较多是概率模型（probabilistic model）或称为统计模型（statistical model），或者称为“经验主义模型”，其建模过程基于大规模真实语料库，从中各级语言单位上的统计信息，并且，依据较低级语言单位上的统计信息，运行相关的统计、推理等技术计算较高级语言单位上的统计信息。与其相对的“理想主义模型”，即基于Chomsky形式语言的确定性语言模型，它建立在人脑中先天存在语法规则这一假设基础上，认为语言是人脑语言能力推导出来的，建立语言模型就是通过建立人工编辑的语言规则集来模拟这种先天的语言能力。\n本课程主要侧重于基于统计的NLP技术，如Viterbi、贝叶斯和最大熵分类器、N-gram语言模型等等。\n三、参考资料\nLecture Slides：Introduction\nhttp://en.wikipedia.org\n关毅，统计自然语言处理基础 课程PPT\n赵妍研，文本情感分析综述\n刘群、王海峰、王惠临、宗成庆、赵铁军、史晓东、朱靖波、陈家俊、张民，机器翻译技术的进展与展望，中文信息学会成立三十周年学术会议，2011年12月4-5日，北京\n时间: 2012年 4月 29日 分类:自然语言处理 作者: fandywang (2,110 基本)\n编辑 2012年 5月 1日 作者:fandywang\n转自 http://52opencourse.com/"}
{"content2":"中文分词是中文文本处理的一个基础步骤，也是中文人机自然语言交互的基础模块，不同于英文的是，中文句子中没有词的界限，因此在进行中文自然语言处理时，通常需要先进行分词，分词效果将直接影响词性，句法树等模块的效果，当然分词只是一个工具，场景不同，要求也不同。在人机自然语言交互中，成熟的中文分词算法能够达到更好的自然语言处理效果，帮助计算机理解复杂的中文语言。\n基于词典分词算法\n基于词典分词算法，也称为字符串匹配分词算法。该算法是按照一定的策略将待匹配的字符串和一个已经建立好的\"充分大的\"词典中的词进行匹配，若找到某个词条，则说明匹配成功，识别了该词。常见的基于词典的分词算法为一下几种：正向最大匹配算法，逆向最大匹配法，最少切分法和双向匹配分词法等。\n基于词典的分词算法是应用最广泛，分词速度最快的，很长一段时间内研究者在对对基于字符串匹配方法进行优化，比如最大长度设定，字符串存储和查找方法以及对于词表的组织结构，比如采用TRIE索引树，哈希索引等。\n这类算法的优点：速度快，都是O(n)的时间复杂度，实现简单，效果尚可，\n算法的缺点：对歧义和未登录的词处理不好。\n基于理解的分词方法\n这种分词方法是通过让计算机模拟人对句子的理解，达到识别词的效果，其基本思想就是在分词的同时进行句法、语义分析，利用句法信息和语义信息来处理歧义现象，它通常包含三个部分：分词系统，句法语义子系统，总控部分，在总控部分的协调下，分词系统可以获得有关词，句子等的句法和语义信息来对分词歧义进行判断，它模拟来人对句子的理解过程，这种分词方法需要大量的语言知识和信息，由于汉语言知识的笼统、复杂性，难以将各种语言信息组成及其可以直接读取的形式，因此目前基于理解的分词系统还在试验阶段。\n基于统计的机器学习算法\n这类目前常用的算法是HMM，CRF，SVM，深度学习等算法，比如stanford，Hanlp分词工具是基于CRF算法。以CRF为例，基本思路是对汉字进行标注训练，不仅考虑了词语出现的频率，还考虑上下文，具备良好的学习能力，因此对歧义词和未登录词的识别都具有良好的效果。\nNianwen Xue在其论文中《Combining Classifier for Chinese Word Segmentation》中首次提出对每个字符进行标注，通过机器学习算法训练分类器进行分词，在论文《Chinese word segmentation as character tagging》中较为详细地阐述了基于字标注的分词法。\n算法优点：能很好处理歧义和未登录词问题，效果比前一类效果好\n算法缺点: 需要大量的人工标注数据，以及较慢的分词速度\n现行常见的中文词分类器\n常见的分词器都是使用机器学习算法和词典相结合的算法，一方面能够提高分词准确率，另一方面能够改善领域适应性。\n随着深度学习的兴起，也出现了基于神经网络的分词器，例如有研究人员尝试使用双向LSTM＋CRF实现分词器，其本质上是序列标注，所以有通用性，命名实体识别等都可以使用该模型，据报道其分词器字符准确率可以高达97.5%,算法框架的思路与论文《Neural Architectures for Named Entity Recogintion》类似，利用该框架可以实现中文分词，如下图所示\n首先对语料进行字符嵌入，将得到的特征输入给双向的LSTM，然后加一个CRF就得到标注结果。\n分词器当前存在问题\n目前中文分词难点主要有三个：\n1. 分词标准：比如人名，在哈工大的标准中姓和名是分开的，但是在Hanlp中是合在一起的，这需要根据不同的需求制定不同的分词标准。\n2. 歧义：对于同一个待切分字符串存在多个分词结果。\n歧义又分为组合歧义，交集型歧义和真歧义三种分类。\n1）组合型歧义：分词是有不同的粒度的，指某个词条中的一部分也可以切分未一个独立的词条，比如“中华人民共和国”，粗粒度的分词就是“中华人民共和国”，细粒度的分词可能是“中华/人民/共和国”\n2）交集型歧义：在“郑州天和服装厂”中，“天和”是厂名，是一个专有名词，“和服”也是一个词，它们共用了“和”字\n3）真歧义：本身的语法和语义都没有问题，即便采用人工切分也会产生同样的歧义，只有通过上下文的语义环境才能给出正确的切分结果，例如：对于句子“美国会通过对台售武法案”，既可以切分成“美国/会/通过...”也可以切分成“美/国会/通过...”\n一般在搜索引擎中,构建索引时和查询时会使用不同的分词算法，常用的方案是，在索引的时候，使用细粒度的分词以保证召回，在查询的时候使用粗粒度的分词以保证精度。\n3. 新词：也称未被词典收录的词，该问题的解决依赖于人们对分词技术和汉语语言结构进一步认识。\n部分分词器的简单说明：\n哈工大的分词器：主页上给过调用接口，每秒请求的次数有限制。\n清华大学THULAC：目前已经有Java、Python和C++版本，并且代码开源。\n斯坦福分词器：作为众多斯坦福自然语言处理中的一个包，目前最新版本3.7.0， Java实现的CRF算法。可以直接使用训练好的模型，也提供训练模型接口。\nHanlp分词：求解的是最短路径。优点：开源、有人维护、可以解答。原始模型用的训练语料是人民日报的语料，当然如果你有足够的语料也可以自己训练。\n结巴分词工具：基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG)；采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合；对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法。\n字嵌入+Bi-LSTM+CRF分词器：本质上是序列标注，这个分词器用人民日报的80万语料，据说按照字符正确率评估标准能达到97.5%的准确率，各位感兴趣可以去看看。\nZPar分词器：新加坡科技设计大学开发的中文分词器，包括分词、词性标注和Parser，支持多语言，据说效果是公开的分词器中最好的，C++语言编写。\n关于速度\n由于分词是基础组件，其性能也是关键的考量因素。通常，分词速度跟系统的软硬件环境有相关外，还与词典的结构设计和算法复杂度相关。比如我们之前跑过字嵌入+Bi-LSTM+CRF分词器，其速度相对较慢。\n作者：lovive"}
{"content2":"序号\n著作或期刊名称\n作者或出版者\n适用对象及要求\n1\nAn Introduction to Corpus Linguistics(语料库语言学入门)\nGraeme Kennedy\n硕士必读\n2\nAspects of the Theory of Syntax(语法理论方面)\nChomsky\n硕士选读\n3\nComprehension: A Paradigm for Cognition(理解： 一个 范式 的 认知)\nKintsch\n博士选读\n4\nComputational Linguistics(计算 语言学)\nThe Association for Computational Linguistics\n硕士选读\n博士必读\n5\nConceptual Information Processing(信息处理的概念)\nSchank\n博士选读\n6\nCorpus Linguistics\nDouglas Biber, etc.\n硕士选读\n7\nDynamic Memory(Dynamic Memory）”理论)http://blog.csdn.net/xifeng_2008/article/details/3043638\nSchank\n博士选读\n8\nForm and Style: Research Papers, Reports and Theses (Tenth edition), （如何写研究论文与学术报告）\nCarole Slade\n硕士、博士必读\n9\nFoundations of Statistical Natural Language Processing (统计自然语言处理基础)\nChristopher D. Manning, Hinrich Schütze (苑春法等译)\n硕士选读\n博士必读\n10\nHNC(概念层次网络)理论导论\n苗传江\n硕士必读\n11\nHNC(概念层次网络)理论——计算机理解语言研究的新思路\n黄曾阳\n硕士、博士必读\n12\nKnowledge of Language: Its Nature, Origins and Use\nChomsky\n硕士、博士选读\n13\nLectures on Government and Binding\nChomsky\n博士选读\n14\nLinguistic Semantics: An Introduction\nJohn Lyons\n硕士、博士必读\n15\nNatural Language Understanding\n(自然语言理解)\nJames Allen\n(刘群等译)\n硕士、博士必读\n16\nPragmatics: An Introduction\nJacob L. Mey\n硕士、博士选读\n17\nScripts, Plans, Goals, and Understanding\nSchank and Abelson\n博士选读\n18\nSemantics\nJohn I. Saeed\n硕士、博士选读\n19\nSemantics\nJohn Lyons\n硕士、博士选读\n20\nSpeech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition (自然语言处理综论)\nDaniel Jurafsky, James H. Martin (冯志伟、孙乐译)\n硕士、博士必读\n21\nStatistics in Language Studies （语言研究中的统计方法）\nAnthony Woods, etc. （陈小荷、徐娟译）\n硕士必读\n22\nThe Case for Case (格辨)\nFillmore (胡明扬译)\n硕士、博士必读\n23\nThe Logical Structure of Linguistics Theory\nChomsky\n博士选读\n24\n词汇语义和计算语言学\n林杏光\n硕士必读\n25\n汉语计算语义学——关系、关系语义场和形式分析\n吴蔚天\n博士选读\n26\n汉语认知研究\n彭聃龄\n硕士、博士选读\n27\n汉语语法的意合网络\n鲁川\n硕士选读\n28\n汉语语法分析问题\n吕叔湘\n硕士选读\n29\n计算语言学导论\n翁富良、王野翊\n硕士选读\n30\n计算语言学概论\n俞士汶\n硕士选读\n31\n计算语言学基础\n冯志伟\n硕士必读\n32\n计算语言学视窗\n靳光瑾\n硕士选读\n33\n逻辑语义学\n方立\n硕士、博士选读\n34\n面向计算机的现代汉语词汇研究概论\n许嘉璐\n硕士、博士必读\n35\n普通语言学教程\n索绪尔\n硕士、博士必读\n36\n全国计算语言学联合学术会议论文集\n中国中文信息学会\n硕士、博士选读\n37\n人工智能及其应用\n蔡自兴、徐光祐\n博士选读\n38\n实验心理语言学纲要——语言的感知、理解和产生\n桂诗春\n硕士、博士选读\n39\n西方语言学名著选读\n胡明扬\n硕士选读\n40\n现代汉语八百词\n吕叔湘\n硕士选读\n41\n现代汉语动词语义计算理论\n靳光瑾\n博士选读\n42\n现代汉语自动分析——Visual C++实现\n陈小荷\n硕士必读\n43\n新编心理语言学\n桂诗春\n硕士、博士选读\n44\n形式语义学引论\n蒋严、潘海华\n硕士、博士选读\n45\n训诂学基础\n陈绂\n硕士、博士选读\n46\n应用语言学\n桂诗春\n硕士选读\n47\n应用语言学导论\nS.皮特·科德\n硕士选读\n48\n应用语言学概论\n于根元\n硕士选读\n49\n应用语言学纲要\n齐沪扬、陈昌来\n硕士选读\n50\n应用语言学综论\n冯志伟\n硕士选读\n51\n语法答问\n朱德熙\n硕士选读\n52\n语言概念空间的基本定理和数学物理表示式\n黄曾阳\n硕士、博士必读\n53\n语言文字学及其应用研究\n许嘉璐\n硕士选读\n54\n语言文字应用\n教育部语言文字应用研究所\n硕士、博士必读\n55\n语义学\n徐烈炯\n硕士、博士必读\n56\n语义学导论\n伍谦光\n硕士选读\n57\n语义学教程\n李福印\n硕士、博士必读\n58\n语用学概要\n何兆熊\n硕士、博士选读\n59\n中国现代应用语言学史纲\n于根元\n硕士、博士选读\n60\n中文文本自动分词和标注\n刘开瑛\n硕士选读\n61\n中文信息处理应用平台工程\n陈力为、袁琦\n硕士、博士选读\n62\n中文信息学报\n中国中文信息学会\n硕士、博士必读\n63\n自然语言处理\n刘开瑛、郭炳炎\n硕士选读\n64\n自然语言处理技术基础\n王小捷、常宝宝\n硕士选读\n65\n自然语言理解——一种让机器懂得人类语言的研究\n姚天顺等\n硕士、博士选读\n66\n自然语言逻辑研究\n邹崇理\n博士选读"}
{"content2":"对人工智能的认识\n人工智能是计算机科学的一个研究分支，不过，这个分支下面又包含很多的研究方向，这个分支提出的时间很早，中间也热闹过几次，自从2012年以来又热闹了，所以看了一点综述文章对此进行简单的记录。\n1、人工智能如果从程序层面看其实是一些列的算法，或者包括了算法的sdk或者api，它需要集成到系统中去才能发挥作用，比如做一个具有智能的手机app、具有智能的web app、具有智能的设备（比如搜狗的翻译宝，儿童手表，会自动泊车的汽车等等）。\n2、人工智能包括六个方面：\n一、计算机视觉（算上模式识别、图像处理等问题）\n二、自然语言理解与交流（包括语音识别、合成等问题）\n三、认知与推理（包含各种物理和社会常识）\n四、机器人学（机械、控制、设计、运动规划、任务规划等）\n五、博弈与伦理\n六、机器学习（各种统计的建模、分析工具和计算的方法）\n3、人工智能目前还特别幼稚，基本没有啥智能，但是现在科技发展到了这个关口，科学家面临着必须向人工智能进军的问题，所以人工智能成为现实是必然的，只不过这个过程可能会比较漫长，也会充满曲折与挫折，需要大家共同面对、携手前进。\n4、给机器赋予智能这事一般是一个趋势，不以个人意志为转移的，它必然会来，不是你回避、不承认，它就不来，也不是你拥抱、欢迎，它就会更快来临，它回来，但是在变为现实的过程中也是艰辛和曲折的，因此这件事要不得机会主义，也要不得逃避主义，不能右，也不能左，还是那句话前途是光明的，道路是曲折的。越努力越幸福。\n5、人工智能的发展历史是从1956年开始的，20世纪80年代初有一股热潮，20世纪80年代末90年代初又有一股热潮，2012年由于深度学习的热潮而又引起一股人工智能的热潮。\n参考资料：\n1、谢耘的演讲\n2、朱松纯老师的文章——浅谈人工智能：现状、任务、架构和统一，正本清源"}
